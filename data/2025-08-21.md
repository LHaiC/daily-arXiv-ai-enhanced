<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 73]
- [cs.CL](#cs.CL) [Total: 42]
- [physics.app-ph](#physics.app-ph) [Total: 4]
- [cs.RO](#cs.RO) [Total: 23]
- [cs.LG](#cs.LG) [Total: 92]
- [quant-ph](#quant-ph) [Total: 46]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.DS](#cs.DS) [Total: 7]
- [cs.LO](#cs.LO) [Total: 6]
- [cs.DC](#cs.DC) [Total: 11]
- [cs.GR](#cs.GR) [Total: 3]
- [cs.MA](#cs.MA) [Total: 1]
- [cs.AI](#cs.AI) [Total: 10]
- [eess.SP](#eess.SP) [Total: 8]
- [eess.SY](#eess.SY) [Total: 15]
- [cs.AR](#cs.AR) [Total: 9]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 9]
- [cs.GT](#cs.GT) [Total: 4]
- [cs.SI](#cs.SI) [Total: 2]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 16]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [A comparative study of some wavelet and sampling operators on various features of an image](https://arxiv.org/abs/2508.14043)
*Digvijay Singh,Rahul Shukla,Karunesh Kumar Singh*

Main category: cs.CV

TL;DR: 本研究探讨了采样 Kantorovich (SK) 算子及其在图像处理中的收敛性，并分析了不同算子在处理图像特征时的性能差异。


<details>
  <summary>Details</summary>
Motivation: 为了研究采样 Kantorovich (SK) 算子及其在图像处理中的收敛性质，特别是它们在处理具有不同特征的图像时的性能。

Method: 通过引入采样 Kantorovich (SK) 算子，并结合高斯、双边和小波算子，对局部和全局近似性质进行了分析。研究中还涉及了误差度量，如均方误差 (MSE)，以及斑点噪声相关的参数，如斑点指数 (SI)、斑点抑制指数 (SSI) 和等效观察次数 (ENL)。

Result: 研究通过数值示例验证了基本逼近定理 (FTA)，并评估了不同算子在处理 Shepp-Logan 幻像图像时的表现。结果表明，在非理想条件下，各种算子在处理图像的不同特征时表现出不同的效果，有些算子表现更好，而有些则不然。

Conclusion: 该研究评估了采样 Kantorovich (SK) 算子在图像处理中的应用，并分析了其在不同图像特征下的收敛性和近似性质。研究表明，不同的算子在处理非均匀图像的特定特征时各有优劣。

Abstract: This research includes the study of some positive sampling Kantorovich
operators (SK operators) and their convergence properties. A comprehensive
analysis of both local and global approximation properties is presented using
sampling Kantorovich (SK), Gaussian, Bilateral and the thresholding
wavelet-based operators in the framework of SK-operators. Explicitly, we start
the article by introducing the basic terminology and state the fundamental
theorem of approximation (FTA) by imposing the various required conditions
corresponding to the various defined operators. We measure the error and study
the other mathematical parameters such as the mean square error (MSE), the
speckle index (SI), the speckle suppression index (SSI), the speckle mean
preservation index (SMPI), and the equivalent number of looks (ENL) at various
levels of resolution parameters. The nature of these operators are demonstrated
via an example under ideal conditions in tabulated form at a certain level of
samples. Eventually, another numerical example is illustrated to discuss the
region of interest (ROI) via SI, SSI and SMPI of 2D Shepp-Logan Phantom taken
slice from the 3D image, which gives the justification of the fundamental
theorem of approximation (FTA). At the end of the derivation and illustrations
we observe that the various operators have their own significance while
studying the various features of the image because of the uneven nature of an
image (non-ideal condition). Therefore, to some extent, some operators work
well and some do not for some specific features of the image.

</details>


### [2] [Federated Action Recognition for Smart Worker Assistance Using FastPose](https://arxiv.org/abs/2508.14113)
*Vinit Hegiste,Vidit Goyal,Tatjana Legler,Martin Ruskowski*

Main category: cs.CV

TL;DR: 该研究提出了一种用于智能制造的联邦学习（FL）人类活动识别（HAR）框架，解决了数据隐私问题并提高了模型性能，特别是在处理新用户和不同环境方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 在智能制造环境中，准确、实时的工人动作识别对于生产力、安全和人机协作至关重要。现有的基于骨骼的HAR方法依赖于中心化数据集，这在注重隐私的工业场景中不切实际。

Method: 提出了一种基于姿态的HAR联邦学习框架，使用了包含八种工业相关上身手势的定制骨骼数据集，并通过改进的FastPose模型处理。在LSTM和Transformer两种时间骨干网络下，在集中式、本地化、带权重的联邦平均（FedAvg）和联邦集成学习（FedEnsemble）四种模式下进行了训练和评估。

Result: 在全局测试集上，FL Transformer比集中式训练提高了+12.4个百分点，FedEnsemble提高了+16.3个百分点。在未知的外部客户端上，FL和FedEnsemble分别比集中式准确率提高了+52.6和+58.3个百分点。

Conclusion: 联邦学习（FL）在保护隐私的同时，能够显著提高跨用户泛化能力，是工业环境中可扩展、注重隐私的人类活动识别（HAR）的实用解决方案。

Abstract: In smart manufacturing environments, accurate and real-time recognition of
worker actions is essential for productivity, safety, and human-machine
collaboration. While skeleton-based human activity recognition (HAR) offers
robustness to lighting, viewpoint, and background variations, most existing
approaches rely on centralized datasets, which are impractical in
privacy-sensitive industrial scenarios. This paper presents a federated
learning (FL) framework for pose-based HAR using a custom skeletal dataset of
eight industrially relevant upper-body gestures, captured from five
participants and processed using a modified FastPose model. Two temporal
backbones, an LSTM and a Transformer encoder, are trained and evaluated under
four paradigms: centralized, local (per-client), FL with weighted federated
averaging (FedAvg), and federated ensemble learning (FedEnsemble). On the
global test set, the FL Transformer improves over centralized training by +12.4
percentage points, with FedEnsemble delivering a +16.3 percentage points gain.
On an unseen external client, FL and FedEnsemble exceed centralized accuracy by
+52.6 and +58.3 percentage points, respectively. These results demonstrate that
FL not only preserves privacy but also substantially enhances cross-user
generalization, establishing it as a practical solution for scalable,
privacy-aware HAR in heterogeneous industrial settings.

</details>


### [3] [LENS: Learning to Segment Anything with Unified Reinforced Reasoning](https://arxiv.org/abs/2508.14153)
*Lianghui Zhu,Bin Ouyang,Yuxuan Zhang,Tianheng Cheng,Rui Hu,Haocheng Shen,Longjin Ran,Xiaoxin Chen,Li Yu,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: LENS is a new framework that uses reinforcement learning to improve text-prompted image segmentation by incorporating chain-of-thought reasoning. It outperforms existing methods and offers a path toward more generalizable segmentation models.


<details>
  <summary>Details</summary>
Motivation: Existing supervised fine-tuning methods for text-prompted image segmentation ignore explicit chain-of-thought (CoT) reasoning at test time, limiting their ability to generalize to unseen prompts and domains.

Method: LENS, a scalable reinforcement-learning framework that jointly optimizes the reasoning process and segmentation in an end-to-end manner. It uses unified reinforcement-learning rewards that span sentence-, box-, and segment-level cues to encourage the model to generate informative CoT rationales while refining mask quality.

Result: LENS achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg benchmarks, outperforming the fine-tuned method GLaMM by up to 5.6%.

Conclusion: RL-driven CoT reasoning is a robust prior for text-prompted segmentation, offering a practical path toward more generalizable Segment Anything models.

Abstract: Text-prompted image segmentation enables fine-grained visual understanding
and is critical for applications such as human-computer interaction and
robotics. However, existing supervised fine-tuning methods typically ignore
explicit chain-of-thought (CoT) reasoning at test time, which limits their
ability to generalize to unseen prompts and domains. To address this issue, we
introduce LENS, a scalable reinforcement-learning framework that jointly
optimizes the reasoning process and segmentation in an end-to-end manner. We
propose unified reinforcement-learning rewards that span sentence-, box-, and
segment-level cues, encouraging the model to generate informative CoT
rationales while refining mask quality. Using a publicly available
3-billion-parameter vision-language model, i.e., Qwen2.5-VL-3B-Instruct, LENS
achieves an average cIoU of 81.2% on the RefCOCO, RefCOCO+, and RefCOCOg
benchmarks, outperforming the strong fine-tuned method, i.e., GLaMM, by up to
5.6%. These results demonstrate that RL-driven CoT reasoning serves as a robust
prior for text-prompted segmentation and offers a practical path toward more
generalizable Segment Anything models. Code is available at
https://github.com/hustvl/LENS.

</details>


### [4] [Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer](https://arxiv.org/abs/2508.14187)
*Md Ashiqur Rahman,Chiao-An Yang,Michael N. Cheng,Lim Jun Hao,Jeremiah Jiang,Teck-Yian Lim,Raymond A. Yeh*

Main category: cs.CV

TL;DR: DEC是一种用于提高计算机视觉模型局部尺度等变性的新方法，可提升现有模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了有效处理计算机视觉中物体尺度变化带来的挑战。

Method: 提出了一种深度均衡规范化器（DEC）来提高模型的局部尺度等变性。

Result: DEC可以轻松地集成到现有网络架构中，并可适应预训练模型，在ImageNet基准测试中，DEC能够提升ViT、DeiT、Swin和BEiT等四个主流预训练深度网络的模型性能和局部尺度一致性。

Conclusion: DEC可以轻松地集成到现有网络架构中，并可适应预训练模型，在ImageNet基准测试中，DEC能够提升ViT、DeiT、Swin和BEiT等四个主流预训练深度网络的模型性能和局部尺度一致性。

Abstract: Scale variation is a fundamental challenge in computer vision. Objects of the
same class can have different sizes, and their perceived size is further
affected by the distance from the camera. These variations are local to the
objects, i.e., different object sizes may change differently within the same
image. To effectively handle scale variations, we present a deep equilibrium
canonicalizer (DEC) to improve the local scale equivariance of a model. DEC can
be easily incorporated into existing network architectures and can be adapted
to a pre-trained model. Notably, we show that on the competitive ImageNet
benchmark, DEC improves both model performance and local scale consistency
across four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our
code is available at https://github.com/ashiq24/local-scale-equivariance.

</details>


### [5] [RynnEC: Bringing MLLMs into Embodied World](https://arxiv.org/abs/2508.14160)
*Ronghao Dang,Yuqian Yuan,Yunxuan Mao,Kehan Li,Jiangpin Liu,Zhikai Wang,Xin Li,Fan Wang,Deli Zhao*

Main category: cs.CV

TL;DR: RynnEC是一个用于具身认知的视频多模态大语言模型，它通过区域编码器和掩码解码器实现了灵活的区域级视频交互。该模型在物体属性理解、物体分割和空间推理方面取得了最先进的性能。此外，还提出了一种用于生成具身认知数据的流水线和相应的基准测试RynnEC-Bench。


<details>
  <summary>Details</summary>
Motivation: 为了实现具身智能体的通用认知，需要对物理世界进行细粒度的感知并实现更精确的交互。现有的方法在区域级视频交互方面存在局限性。

Method: RynnEC是一个基于通用视觉语言基础模型的视频多模态大语言模型，它包含一个区域编码器和一个掩码解码器，实现了灵活的区域级视频交互。为了缓解带注释3D数据集的稀缺性，我们提出了一种基于自我中心的视频生成流水线来创建具身认知数据。此外，我们还引入了一个名为RynnEC-Bench的区域中心化基准测试，用于评估具身认知能力。

Result: RynnEC在物体属性理解、物体分割和空间推理方面取得了最先进的性能。

Conclusion: RynnEC有望推动通用认知核心在具身智能体中的发展，并促进跨不同具身任务的泛化。

Abstract: We introduce RynnEC, a video multimodal large language model designed for
embodied cognition. Built upon a general-purpose vision-language foundation
model, RynnEC incorporates a region encoder and a mask decoder, enabling
flexible region-level video interaction. Despite its compact architecture,
RynnEC achieves state-of-the-art performance in object property understanding,
object segmentation, and spatial reasoning. Conceptually, it offers a
region-centric video paradigm for the brain of embodied agents, providing
fine-grained perception of the physical world and enabling more precise
interactions. To mitigate the scarcity of annotated 3D datasets, we propose an
egocentric video based pipeline for generating embodied cognition data.
Furthermore, we introduce RynnEC-Bench, a region-centered benchmark for
evaluating embodied cognitive capabilities. We anticipate that RynnEC will
advance the development of general-purpose cognitive cores for embodied agents
and facilitate generalization across diverse embodied tasks. The code, model
checkpoints, and benchmark are available at:
https://github.com/alibaba-damo-academy/RynnEC

</details>


### [6] [CLIPSym: Delving into Symmetry Detection with CLIP](https://arxiv.org/abs/2508.14197)
*Tinghan Yang,Md Ashiqur Rahman,Raymond A. Yeh*

Main category: cs.CV

TL;DR: CLIPSym是一种新颖的对称性检测方法，它利用CLIP模型的图像和语言理解能力，并通过SAPG技术增强了语义线索的整合，在多个数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 利用预训练的CLIP模型通过整合自然图像描述中的对称性线索来辅助对称性检测。

Method: CLIPSym利用CLIP的图像和语言编码器，以及基于Transformer和G-Convolution混合的旋转等变解码器来检测旋转和反射对称性。提出了一种名为“语义感知提示分组”（SAPG）的新颖提示技术，用于充分利用CLIP的语言编码器，该技术聚合了多样化的、频繁出现的基于对象的提示，以更好地整合语义线索以进行对称性检测。

Result: CLIPSym在三个标准对称性检测数据集（DENDI、SDRW和LDRS）上取得了最先进的性能。消融实验验证了CLIP预训练、等变解码器和SAPG技术的有效性。

Conclusion: CLIPSym在三个标准对称性检测数据集（DENDI、SDRW和LDRS）上均优于当前最先进的模型。

Abstract: Symmetry is one of the most fundamental geometric cues in computer vision,
and detecting it has been an ongoing challenge. With the recent advances in
vision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP
model can aid symmetry detection by leveraging the additional symmetry cues
found in the natural image descriptions. We propose CLIPSym, which leverages
CLIP's image and language encoders and a rotation-equivariant decoder based on
a hybrid of Transformer and $G$-Convolution to detect rotation and reflection
symmetries. To fully utilize CLIP's language encoder, we have developed a novel
prompting technique called Semantic-Aware Prompt Grouping (SAPG), which
aggregates a diverse set of frequent object-based prompts to better integrate
the semantic cues for symmetry detection. Empirically, we show that CLIPSym
outperforms the current state-of-the-art on three standard symmetry detection
datasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations
verifying the benefits of CLIP's pre-training, the proposed equivariant
decoder, and the SAPG technique. The code is available at
https://github.com/timyoung2333/CLIPSym.

</details>


### [7] [A Survey on Video Anomaly Detection via Deep Learning: Human, Vehicle, and Environment](https://arxiv.org/abs/2508.14203)
*Ghazal Alinezhad Noghre,Armin Danesh Pazho,Hamed Tabkhi*

Main category: cs.CV

TL;DR: 本综述全面梳理了视频异常检测（VAD）领域的研究，重点关注不同学习范式和应用场景，旨在为该领域的研究和实际应用提供一个结构化的基础，并强调了未来的挑战。


<details>
  <summary>Details</summary>
Motivation: 视频异常检测（VAD）作为计算机视觉中的一个关键任务，在多个领域具有广泛的应用。尽管深度学习的最新进展推动了VAD领域的显著进步，但该领域在不同领域和学习范式之间仍然存在碎片化的问题。因此，本研究旨在提供一个全面的视角，系统地组织相关文献，整合各子领域的见解，为研究社区提供一个结构化的基础，以促进VAD系统理论理解和实际应用的发展，并强调该领域面临的公开挑战。

Method: 本研究通过对视频异常检测（VAD）领域的文献进行系统性组织和分析，涵盖了不同的监督级别（如无监督、半监督、监督学习）和自适应学习方法（如在线学习、主动学习、持续学习）。研究考察了人类中心、车辆中心和环境中心这三个主要的应用场景，并分析了各自独特的挑战和设计考量。

Result: 本研究系统性地组织了视频异常检测（VAD）领域的文献，涵盖了不同的监督级别和自适应学习方法，并考察了人类中心、车辆中心和环境中心的应用场景。通过识别现有方法的基本贡献和局限性，为VAD领域的研究和实际应用提供了结构化的基础和有用的参考，并指出了该领域面临的挑战。

Conclusion: 本篇论文旨在通过系统性地组织视频异常检测（VAD）的文献，涵盖不同的监督级别和自适应学习方法（如在线、主动和持续学习），并考察人类、车辆和环境中心的三种主要应用场景，为VAD领域提供一个全面的视角。通过整合各子领域的见解，论文旨在为研究社区提供一个结构化的基础，以促进VAD系统理论理解和实际应用的发展，并强调该领域面临的公开挑战，包括基础研究问题和实际部署障碍。

Abstract: Video Anomaly Detection (VAD) has emerged as a pivotal task in computer
vision, with broad relevance across multiple fields. Recent advances in deep
learning have driven significant progress in this area, yet the field remains
fragmented across domains and learning paradigms. This survey offers a
comprehensive perspective on VAD, systematically organizing the literature
across various supervision levels, as well as adaptive learning methods such as
online, active, and continual learning. We examine the state of VAD across
three major application categories: human-centric, vehicle-centric, and
environment-centric scenarios, each with distinct challenges and design
considerations. In doing so, we identify fundamental contributions and
limitations of current methodologies. By consolidating insights from subfields,
we aim to provide the community with a structured foundation for advancing both
theoretical understanding and real-world applicability of VAD systems. This
survey aims to support researchers by providing a useful reference, while also
drawing attention to the broader set of open challenges in anomaly detection,
including both fundamental research questions and practical obstacles to
real-world deployment.

</details>


### [8] [Accelerating Image Classification with Graph Convolutional Neural Networks using Voronoi Diagrams](https://arxiv.org/abs/2508.14218)
*Mustafa Mohammadi Gharasuie,Luis Rueda*

Main category: cs.CV

TL;DR: 本研究提出了一种结合GCN和Voronoi图的新图像分类方法，通过图结构表示图像，并引入了更快的NVGCN模型，在复杂场景和细粒度分类任务上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了应对图像分类中处理复杂数据结构的需求，并借鉴GCN在处理关系数据方面的能力，提出了一种新的图像分类方法。

Method: 提出了一种利用GCN和Voronoi图进行图像分类的新框架，将图像表示为图结构（像素或区域作为顶点，Delaunay三角剖分作为图的简化形式），并引入了一种名为NVGCN的改进GCN模型。

Result: 该模型在预处理时间和分类准确性方面均有显著提升，在多个基准数据集上超越了现有最先进的模型，尤其在复杂场景和细粒度类别方面表现突出。

Conclusion: 该研究通过结合GCN和Voronoi图，在图像分类任务上取得了显著的改进，特别是在处理复杂场景和细粒度类别时。提出的NVGCN模型比常规GCN更快，为图学习方法在计算机视觉和其他领域开辟了新的可能性。

Abstract: Recent advances in image classification have been significantly propelled by
the integration of Graph Convolutional Networks (GCNs), offering a novel
paradigm for handling complex data structures. This study introduces an
innovative framework that employs GCNs in conjunction with Voronoi diagrams to
peform image classification, leveraging their exceptional capability to model
relational data. Unlike conventional convolutional neural networks, our
approach utilizes a graph-based representation of images, where pixels or
regions are treated as vertices of a graph, which are then simplified in the
form of the corresponding Delaunay triangulations. Our model yields significant
improvement in pre-processing time and classification accuracy on several
benchmark datasets, surpassing existing state-of-the-art models, especially in
scenarios that involve complex scenes and fine-grained categories. The
experimental results, validated via cross-validation, underscore the potential
of integrating GCNs with Voronoi diagrams in advancing image classification
tasks. This research contributes to the field by introducing a novel approach
to image classification, while opening new avenues for developing graph-based
learning paradigms in other domains of computer vision and non-structured data.
In particular, we have proposed a new version of the GCN in this paper, namely
normalized Voronoi Graph Convolution Network (NVGCN), which is faster than the
regular GCN.

</details>


### [9] [Directed-Tokens: A Robust Multi-Modality Alignment Approach to Large Language-Vision Models](https://arxiv.org/abs/2508.14264)
*Thanh-Dat Truong,Huu-Thien Tran,Tran Thai Son,Bhiksha Raj,Khoa Luu*

Main category: cs.CV

TL;DR: 提出了一种新的学习机制，通过重构图像和文本顺序以及定向-token和图像到响应的引导损失，提高了大型多模态模型的鲁棒性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的LMM在鲁棒性和泛化性方面存在固有的局限性，这与视觉和文本特征之间的对齐和相关性有关。

Method: 通过引入两个新任务（重构图像顺序和文本顺序）来改进LMM的预训练和微调，以及提出一种新的定向-token方法来捕获视觉和文本知识，并引入图像到响应的引导损失来进一步提高LMM的视觉理解能力。

Result: 所提出的方法通过解决混淆问题，提高了视觉和文本模态之间鲁棒对齐的效率，从而提高了推理能力、视觉理解能力和跨模态对齐能力。

Conclusion: 该方法在学术任务导向和指令遵循的LMM基准上持续 achieves state-of-the-art (SoTA) 性能，优于之前的LMM。

Abstract: Large multimodal models (LMMs) have gained impressive performance due to
their outstanding capability in various understanding tasks. However, these
models still suffer from some fundamental limitations related to robustness and
generalization due to the alignment and correlation between visual and textual
features. In this paper, we introduce a simple but efficient learning mechanism
for improving the robust alignment between visual and textual modalities by
solving shuffling problems. In particular, the proposed approach can improve
reasoning capability, visual understanding, and cross-modality alignment by
introducing two new tasks: reconstructing the image order and the text order
into the LMM's pre-training and fine-tuning phases. In addition, we propose a
new directed-token approach to capture visual and textual knowledge, enabling
the capability to reconstruct the correct order of visual inputs. Then, we
introduce a new Image-to-Response Guided loss to further improve the visual
understanding of the LMM in its responses. The proposed approach consistently
achieves state-of-the-art (SoTA) performance compared with prior LMMs on
academic task-oriented and instruction-following LMM benchmarks.

</details>


### [10] [Effect of Data Augmentation on Conformal Prediction for Diabetic Retinopathy](https://arxiv.org/abs/2508.14266)
*Rizwan Ahamed,Annahita Amireskandari,Joel Palko,Carol Laxson,Binod Bhattarai,Prashnna Gyawali*

Main category: cs.CV

TL;DR: Deep learning models for medical tasks need reliable uncertainty estimates. This study tested how data augmentation affects conformal prediction (a method for uncertainty). Mixup and CutMix augmentations improved both accuracy and reliability, while CLAHE hurt reliability. Careful selection of augmentation is key for trustworthy AI in medical imaging.


<details>
  <summary>Details</summary>
Motivation: The clinical deployment of deep learning models for high-stakes tasks like diabetic retinopathy grading requires reliable uncertainty quantification, which is often lacking in current models. Conformal prediction (CP) provides a framework for this, but its interaction with standard training practices like data augmentation is not well understood.

Method: The study systematically investigated the effects of different data augmentation strategies (no augmentation, geometric transforms, CLAHE, Mixup, and CutMix) on the performance of conformal predictors for diabetic retinopathy (DR) grading using the DDR dataset and two backbone architectures (ResNet-50 and CoaT). Conformal metrics such as empirical coverage, average prediction set size, and correct efficiency were analyzed.

Result: The results demonstrate that Mixup and CutMix yield more reliable and efficient uncertainty estimates, alongside improved predictive accuracy. CLAHE, however, was found to negatively impact model certainty.

Conclusion: The study highlights the need to co-design data augmentation strategies with downstream uncertainty quantification to build trustworthy AI systems for medical imaging, as sample-mixing strategies like Mixup and CutMix improve both predictive accuracy and uncertainty estimates, while methods like CLAHE can negatively impact model certainty.

Abstract: The clinical deployment of deep learning models for high-stakes tasks such as
diabetic retinopathy (DR) grading requires demonstrable reliability. While
models achieve high accuracy, their clinical utility is limited by a lack of
robust uncertainty quantification. Conformal prediction (CP) offers a
distribution-free framework to generate prediction sets with statistical
guarantees of coverage. However, the interaction between standard training
practices like data augmentation and the validity of these guarantees is not
well understood. In this study, we systematically investigate how different
data augmentation strategies affect the performance of conformal predictors for
DR grading. Using the DDR dataset, we evaluate two backbone architectures --
ResNet-50 and a Co-Scale Conv-Attentional Transformer (CoaT) -- trained under
five augmentation regimes: no augmentation, standard geometric transforms,
CLAHE, Mixup, and CutMix. We analyze the downstream effects on conformal
metrics, including empirical coverage, average prediction set size, and correct
efficiency. Our results demonstrate that sample-mixing strategies like Mixup
and CutMix not only improve predictive accuracy but also yield more reliable
and efficient uncertainty estimates. Conversely, methods like CLAHE can
negatively impact model certainty. These findings highlight the need to
co-design augmentation strategies with downstream uncertainty quantification in
mind to build genuinely trustworthy AI systems for medical imaging.

</details>


### [11] [Tooth-Diffusion: Guided 3D CBCT Synthesis with Fine-Grained Tooth Conditioning](https://arxiv.org/abs/2508.14276)
*Said Djafar Said,Torkan Gholamalizadeh,Mostafa Mehdipour Ghazi*

Main category: cs.CV

TL;DR: 提出了一种用于3D牙科CBCT扫描生成的条件扩散框架，可以精确控制牙齿的存在和配置，实现了逼真的修改，并在各种任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 尽管牙科CBCT扫描在诊断和治疗规划方面的重要性日益增长，但在医学图像合成中生成具有精细控制的解剖结构逼真的扫描仍然是一个挑战。

Method: 提出了一种新颖的条件扩散框架，用于3D牙科容积生成，该框架由牙齿级别的二值属性引导，并结合了基于小波的去噪扩散、FiLM条件和掩码损失函数，以将学习集中在相关的解剖结构上。

Result: 在牙齿添加、移除和全牙列合成等多样化任务上进行了评估，模型在未见过的扫描上表现出强大的保真度和泛化能力，FID得分低，修复性能稳健，SSIM值高于0.91。

Conclusion: 生成具有精细控制的解剖结构逼真的牙科CBCT扫描仍然是一个挑战，但本研究提出的条件扩散框架通过牙齿级别的二值属性引导，实现了牙齿存在和配置的精确控制，为牙科AI工作流程中的手术规划、患者沟通和目标数据增强创造了机会。

Abstract: Despite the growing importance of dental CBCT scans for diagnosis and
treatment planning, generating anatomically realistic scans with fine-grained
control remains a challenge in medical image synthesis. In this work, we
propose a novel conditional diffusion framework for 3D dental volume
generation, guided by tooth-level binary attributes that allow precise control
over tooth presence and configuration. Our approach integrates wavelet-based
denoising diffusion, FiLM conditioning, and masked loss functions to focus
learning on relevant anatomical structures. We evaluate the model across
diverse tasks, such as tooth addition, removal, and full dentition synthesis,
using both paired and distributional similarity metrics. Results show strong
fidelity and generalization with low FID scores, robust inpainting performance,
and SSIM values above 0.91 even on unseen scans. By enabling realistic,
localized modification of dentition without rescanning, this work opens
opportunities for surgical planning, patient communication, and targeted data
augmentation in dental AI workflows. The codes are available at:
https://github.com/djafar1/tooth-diffusion.

</details>


### [12] [GALA: Guided Attention with Language Alignment for Open Vocabulary Gaussian Splatting](https://arxiv.org/abs/2508.14278)
*Elena Alegret Regalado,Kunyi Li,Sen Wang,Siyun Liang,Michael Niemeyer,Stefano Gasperini,Nassir Navab,Federico Tombari*

Main category: cs.CV

TL;DR: GALA是一个新颖的框架，利用3D高斯泼溅（3DGS）进行开放词汇表3D场景理解。它通过对比学习和跨注意力模块来学习视图无关的语义嵌入，实现了高效且准确的2D和3D场景理解。


<details>
  <summary>Details</summary>
Motivation: 现有方法在从2D图像捕获细粒度的、与语言相关的3D表示方面仍然存在挑战。

Method: GALA通过自监督对比学习提炼场景特定的3D实例特征场，并引入了一个核心的跨注意力模块，该模块包含两个可学习的代码本，用于编码与视图无关的语义嵌入，从而实现了无缝的2D和3D开放词汇表查询，并减少了内存消耗。

Result: GALA在真实世界数据集上的广泛实验证明了其在2D和3D开放词汇表任务上的卓越性能。

Conclusion: GALA在2D和3D的开放词汇表性能方面表现出色。

Abstract: 3D scene reconstruction and understanding have gained increasing popularity,
yet existing methods still struggle to capture fine-grained, language-aware 3D
representations from 2D images. In this paper, we present GALA, a novel
framework for open-vocabulary 3D scene understanding with 3D Gaussian Splatting
(3DGS). GALA distills a scene-specific 3D instance feature field via
self-supervised contrastive learning. To extend to generalized language feature
fields, we introduce the core contribution of GALA, a cross-attention module
with two learnable codebooks that encode view-independent semantic embeddings.
This design not only ensures intra-instance feature similarity but also
supports seamless 2D and 3D open-vocabulary queries. It reduces memory
consumption by avoiding per-Gaussian high-dimensional feature learning.
Extensive experiments on real-world datasets demonstrate GALA's remarkable
open-vocabulary performance on both 2D and 3D.

</details>


### [13] [Multi-Rationale Explainable Object Recognition via Contrastive Conditional Inference](https://arxiv.org/abs/2508.14280)
*Ali Rasekh,Sepehr Kazemi Ranjbar,Simon Gottschalk*

Main category: cs.CV

TL;DR: 本研究通过引入新的基准测试和对比条件推理（CCI）框架，改进了可解释目标识别方法，实现了更准确的类别预测和更高质量的释义，特别是在处理多个释义和零样本场景下表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于提示词调节，受限于CLIP的文本编码器，并且对解释结构的作用较弱。此外，先前的数据集通常仅限于单一且常常包含噪声的释义，未能捕捉判别性图像特征的全部多样性。

Method: 提出对比条件推理（CCI）框架，显式地对图像嵌入、类别标签和释义之间的概率关系进行建模。该框架无需任何训练，即可实现更有效的释义条件，以预测准确的目标类别。

Result: 本研究引入了一个多释义可解释目标识别基准测试，其中包含每个图像都带有多个真实释义标注的数据集，以及旨在更全面地表示任务的评估指标。

Conclusion: 本研究提出的对比条件推理（CCI）框架在多释义可解释目标识别基准测试中取得了最先进的成果，包括强大的零样本性能，并在分类准确性和释义质量方面树立了新的标准。本研究连同基准测试提供了一个更完整的框架，用于评估未来可解释目标识别模型。

Abstract: Explainable object recognition using vision-language models such as CLIP
involves predicting accurate category labels supported by rationales that
justify the decision-making process. Existing methods typically rely on
prompt-based conditioning, which suffers from limitations in CLIP's text
encoder and provides weak conditioning on explanatory structures. Additionally,
prior datasets are often restricted to single, and frequently noisy, rationales
that fail to capture the full diversity of discriminative image features. In
this work, we introduce a multi-rationale explainable object recognition
benchmark comprising datasets in which each image is annotated with multiple
ground-truth rationales, along with evaluation metrics designed to offer a more
comprehensive representation of the task. To overcome the limitations of
previous approaches, we propose a contrastive conditional inference (CCI)
framework that explicitly models the probabilistic relationships among image
embeddings, category labels, and rationales. Without requiring any training,
our framework enables more effective conditioning on rationales to predict
accurate object categories. Our approach achieves state-of-the-art results on
the multi-rationale explainable object recognition benchmark, including strong
zero-shot performance, and sets a new standard for both classification accuracy
and rationale quality. Together with the benchmark, this work provides a more
complete framework for evaluating future models in explainable object
recognition. The code will be made available online.

</details>


### [14] [OccluNet: Spatio-Temporal Deep Learning for Occlusion Detection on DSA](https://arxiv.org/abs/2508.14286)
*Anushka A. Kore,Frank G. te Nijenhuis,Matthijs van der Sluijs,Wim van Zwam,Charles Majoie,Geert Lycklama à Nijeholt,Danny Ruijters,Frans Vos,Sandra Cornelissen,Ruisheng Su,Theo van Walsum*

Main category: cs.CV

TL;DR: OccluNet是一个用于在DSA序列中自动检测血管闭塞的时空深度学习模型，在精确率和召回率方面均优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 在急性缺血性卒中（AIS）的血管内取栓（EVT）过程中，准确检测血管闭塞至关重要，但数字减影血管造影（DSA）序列的解读因解剖复杂性和时间限制而面临挑战。

Method: 提出了一种名为OccluNet的时空深度学习模型，该模型整合了YOLOX目标检测器和基于Transformer的 temporal attention 机制，用于自动检测DSA序列中的血管闭塞。探索了纯时间注意力和分离式时空注意力两种模型变体。

Result: OccluNet 能够捕捉时间上一致的特征，精确率和召回率分别为89.02%和74.87%，显著优于仅基于单个DSA帧或最小强度投影训练的YOLOv11基线模型。两种时空注意力变体均达到了相似的性能。

Conclusion: OccluNet 在 DSA序列中自动检测闭塞方面表现出色，显著优于基线模型，实现了89.02%的精确率和74.87%的召回率。

Abstract: Accurate detection of vascular occlusions during endovascular thrombectomy
(EVT) is critical in acute ischemic stroke (AIS). Interpretation of digital
subtraction angiography (DSA) sequences poses challenges due to anatomical
complexity and time constraints. This work proposes OccluNet, a spatio-temporal
deep learning model that integrates YOLOX, a single-stage object detector, with
transformer-based temporal attention mechanisms to automate occlusion detection
in DSA sequences. We compared OccluNet with a YOLOv11 baseline trained on
either individual DSA frames or minimum intensity projections. Two
spatio-temporal variants were explored for OccluNet: pure temporal attention
and divided space-time attention. Evaluation on DSA images from the MR CLEAN
Registry revealed the model's capability to capture temporally consistent
features, achieving precision and recall of 89.02% and 74.87%, respectively.
OccluNet significantly outperformed the baseline models, and both attention
variants attained similar performance. Source code is available at
https://github.com/anushka-kore/OccluNet.git

</details>


### [15] [Pixels to Play: A Foundation Model for 3D Gameplay](https://arxiv.org/abs/2508.14295)
*Yuguang Yue,Chris Green,Samuel Hunt,Irakli Salia,Wenzhe Shi,Jonathan J Hunt*

Main category: cs.CV

TL;DR: Pixels2Play-0.1是一个基础模型，它学习玩各种3D视频游戏，并具有可识别的人类行为。该模型通过行为克隆进行端到端训练，结合了标记的人类游戏演示和公共视频，并使用逆动力学模型进行动作推断。初步结果表明，该模型在Roblox和MS-DOS游戏中表现出 kompetent 的游戏能力。


<details>
  <summary>Details</summary>
Motivation: 新兴的消费者和开发人员用例，例如AI队友、可控制的NPC、个性化的直播流和辅助测试人员，这些用例都需要一个智能体能够依赖玩家可用的相同像素流，并以最少游戏特定的工程泛化到新游戏。

Method: 通过行为克隆进行端到端训练，结合了标记的人类游戏演示和公共视频，并使用逆动力学模型进行动作推断。使用解码器Transformer和自回归动作输出来处理大的动作空间，同时在单个消费级GPU上具有低延迟。

Result: 在Roblox和经典的MS-DOS游戏中表现出 kompetent 的游戏能力，进行了关于无标签数据的消融研究，并概述了达到专家级别、文本条件控制所需的扩展和评估步骤。

Conclusion: Pixels2Play-0.1 (P2P0.1)是一个基础模型，它学习玩各种3D视频游戏，并具有可识别的人类行为。该模型通过行为克隆进行端到端训练，结合了标记的人类游戏演示和公共视频，并使用逆动力学模型进行动作推断。P2P0.1使用解码器Transformer和自回归动作输出来处理大的动作空间，同时在单个消费级GPU上具有低延迟。初步结果表明，该模型在Roblox和MS-DOS游戏中表现出 kompetent 的游戏能力，并进行了关于无标签数据的消融研究，同时概述了达到专家级别、文本条件控制所需的扩展和评估步骤。

Abstract: We introduce Pixels2Play-0.1 (P2P0.1), a foundation model that learns to play
a wide range of 3D video games with recognizable human-like behavior. Motivated
by emerging consumer and developer use cases - AI teammates, controllable NPCs,
personalized live-streamers, assistive testers - we argue that an agent must
rely on the same pixel stream available to players and generalize to new titles
with minimal game-specific engineering. P2P0.1 is trained end-to-end with
behavior cloning: labeled demonstrations collected from instrumented human
game-play are complemented by unlabeled public videos, to which we impute
actions via an inverse-dynamics model. A decoder-only transformer with
auto-regressive action output handles the large action space while remaining
latency-friendly on a single consumer GPU. We report qualitative results
showing competent play across simple Roblox and classic MS-DOS titles,
ablations on unlabeled data, and outline the scaling and evaluation steps
required to reach expert-level, text-conditioned control.

</details>


### [16] [MoVieDrive: Multi-Modal Multi-View Urban Scene Video Generation](https://arxiv.org/abs/2508.14327)
*Guile Wu,David Huang,Dongfeng Bai,Bingbing Liu*

Main category: cs.CV

TL;DR: 提出了一种统一的扩散 Transformer 模型，用于生成多模态多视角自动驾驶视频，解决了现有方法在多模态生成和模型部署方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有的视频生成方法主要关注 RGB 视频生成，缺乏生成多模态视频（如深度图和语义图）的能力，而这些多模态数据对于整体城市场景理解至关重要。

Method: 提出了一种新颖的多模态多视角视频生成方法，该方法构建了一个统一的扩散 Transformer 模型，该模型由模态共享组件和模态特定组件组成，并利用了多样的条件输入来编码可控场景结构和内容线索。

Result: 在具有挑战性的真实世界自动驾驶数据集 nuScenes 上进行实验，证明了该方法在生成多模态多视角城市场景视频方面的高保真度和可控性。

Conclusion: 该方法能够生成高保真度和可控性的多模态多视角驾驶场景视频，并且优于最先进的方法。

Abstract: Video generation has recently shown superiority in urban scene synthesis for
autonomous driving. Existing video generation approaches to autonomous driving
primarily focus on RGB video generation and lack the ability to support
multi-modal video generation. However, multi-modal data, such as depth maps and
semantic maps, are crucial for holistic urban scene understanding in autonomous
driving. Although it is feasible to use multiple models to generate different
modalities, this increases the difficulty of model deployment and does not
leverage complementary cues for multi-modal data generation. To address this
problem, in this work, we propose a novel multi-modal multi-view video
generation approach to autonomous driving. Specifically, we construct a unified
diffusion transformer model composed of modal-shared components and
modal-specific components. Then, we leverage diverse conditioning inputs to
encode controllable scene structure and content cues into the unified diffusion
model for multi-modal multi-view video generation. In this way, our approach is
capable of generating multi-modal multi-view driving scene videos in a unified
framework. Our experiments on the challenging real-world autonomous driving
dataset, nuScenes, show that our approach can generate multi-modal multi-view
urban scene videos with high fidelity and controllability, surpassing the
state-of-the-art methods.

</details>


### [17] [Inter-Class Relational Loss for Small Object Detection: A Case Study on License Plates](https://arxiv.org/abs/2508.14343)
*Dian Ning,Dong Seog Han*

Main category: cs.CV

TL;DR: 该研究提出了一种类别间关系损失函数，解决了小目标检测中的梯度更新问题，并在车牌检测任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有的基于IoU的损失函数在处理小目标时存在梯度过于平坦的问题，导致训练收敛不稳定，特别是当需要同时更新多个目标时，小目标的梯度更新会受到更大的影响。因此，需要一种新的方法来有效更新小目标的梯度，同时不牺牲其他目标的学习效率。

Method: 提出了一种类别间关系损失函数（inter-class relational loss），通过计算目标（如车牌）与其相关联的大目标（如车辆）之间的空间关系来惩罚错误预测，从而为小目标提供更有效的梯度更新。该损失函数可以方便地集成到现有的基于IoU的损失函数中。

Result: 所提出的类别间关系损失函数（ICR loss）能够有效地更新小目标的梯度。在车辆车牌检测任务中，将ICR loss集成到YOLOv12-T和UAV-DETR模型后，mAP$^{	ext{test}}_{50}$分别提升了10.3%和1.6%，且无需额外的超参数调整。此外，还发布了一个新的小型车辆多车牌数据集（SVMLP）。

Conclusion: 研究者提出了一个新颖的类别间关系损失函数（inter-class relational loss），该函数通过利用目标间的空间关系来优化小目标检测的梯度更新问题，同时不影响其他目标的学习效率。该方法在车辆车牌检测任务中进行了验证，并取得了显著的性能提升，例如在YOLOv12-T和UAV-DETR模型上分别提升了10.3%和1.6%的mAP指标。此外，研究者还构建了一个新的小型车辆多车牌数据集（SVMLP）。

Abstract: In one-stage multi-object detection tasks, various intersection over union
(IoU)-based solutions aim at smooth and stable convergence near the targets
during training. However, IoU-based losses fail to correctly update the
gradient of small objects due to an extremely flat gradient. During the update
of multiple objects, the learning of small objects' gradients suffers more
because of insufficient gradient updates. Therefore, we propose an inter-class
relational loss to efficiently update the gradient of small objects while not
sacrificing the learning efficiency of other objects based on the simple fact
that an object has a spatial relationship to another object (e.g., a car plate
is attached to a car in a similar position). When the predicted car plate's
bounding box is not within its car, a loss punishment is added to guide the
learning, which is inversely proportional to the overlapped area of the car's
and predicted car plate's bounding box. By leveraging the spatial relationship
at the inter-class level, the loss guides small object predictions using larger
objects and enhances latent information in deeper feature maps. In this paper,
we present twofold contributions using license plate detection as a case study:
(1) a new small vehicle multi-license plate dataset (SVMLP), featuring diverse
real-world scenarios with high-quality annotations; and (2) a novel inter-class
relational loss function designed to promote effective detection performance.
We highlight the proposed ICR loss penalty can be easily added to existing
IoU-based losses and enhance the performance. These contributions improve the
standard mean Average Precision (mAP) metric, achieving gains of 10.3% and 1.6%
in mAP$^{\text{test}}_{50}$ for YOLOv12-T and UAV-DETR, respectively, without
any additional hyperparameter tuning. Code and dataset will be available soon.

</details>


### [18] [HandCraft: Dynamic Sign Generation for Synthetic Data Augmentation](https://arxiv.org/abs/2508.14345)
*Gaston Gustavo Rios*

Main category: cs.CV

TL;DR: 由于数据不足，手语识别（SLR）模型面临性能挑战。本文提出了一种新颖的基于CMLPe的信号生成模型和合成数据预训练方法，以提高SLR性能。该方法在LSFB和DiSPLaY数据集上取得了最先进的成果，并且在某些情况下优于传统的数据增强方法。


<details>
  <summary>Details</summary>
Motivation: 手语识别（SLR）模型由于训练数据不足而面临显著的性能限制。

Method: 提出了一种新颖、轻量级的基于CMLPe的信号生成模型，并结合了合成数据预训练方法。

Result: 所提出的模型与合成数据预训练方法相结合，能够持续提高识别准确率，在LSFB和DiSPLaY数据集上使用Mamba-SL和Transformer-SL分类器取得了新的最先进结果。研究结果表明，在某些情况下，合成数据预训练优于传统的增强方法，并且与它们结合使用时会产生互补的优势。

Conclusion: 通过提供计算高效的方法，该研究为SLR（手语识别）的信号生成和合成数据预训练提供了民主化途径，在不同的数据集上实现了显著的性能提升。

Abstract: Sign Language Recognition (SLR) models face significant performance
limitations due to insufficient training data availability. In this article, we
address the challenge of limited data in SLR by introducing a novel and
lightweight sign generation model based on CMLPe. This model, coupled with a
synthetic data pretraining approach, consistently improves recognition
accuracy, establishing new state-of-the-art results for the LSFB and DiSPLaY
datasets using our Mamba-SL and Transformer-SL classifiers. Our findings reveal
that synthetic data pretraining outperforms traditional augmentation methods in
some cases and yields complementary benefits when implemented alongside them.
Our approach democratizes sign generation and synthetic data pretraining for
SLR by providing computationally efficient methods that achieve significant
performance improvements across diverse datasets.

</details>


### [19] [Deep Learning for Taxol Exposure Analysis: A New Cell Image Dataset and Attention-Based Baseline Model](https://arxiv.org/abs/2508.14349)
*Sean Fletcher,Gabby Scott,Douglas Currie,Xin Zhang,Yuqi Song,Bruce MacLeod*

Main category: cs.CV

TL;DR: 开发了一个新的数据集和名为ResAttention-KNN的模型，用于通过显微图像自动分析化疗药物Taxol对细胞的影响，解决了现有方法成本高、通量低的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Taxol化疗药物细胞水平效应检测方法需要专门的设备、技术人员和大量的样本准备，成本高、劳动强度大，不适合高通量或实时分析。为了解决自动化形态学分析的Taxol响应数据缺失问题，本研究旨在创建一个新的显微图像数据集，并提出一个有效的模型来进行Taxol浓度分类。

Method: 提出了一种名为ResAttention-KNN的基线模型，该模型结合了ResNet-50和卷积块注意模块（CBAM），并利用K近邻（KNN）分类器在学习到的嵌入空间中进行分类。该模型集成了基于注意力的细化和非参数分类，以增强鲁棒性和可解释性。

Result: 成功构建了一个包含经Taxol处理的C6胶质瘤细胞的显微图像新数据集，并提出了ResAttention-KNN基线模型。该模型在Taxol浓度分类任务上表现出有效性，并为未来在该数据集上的研究建立了基准。

Conclusion: 该研究提出了一个名为ResAttention-KNN的基线模型，结合了ResNet-50、卷积块注意模块和K近邻分类器，用于对暴露于Taxol的细胞进行形态学分析，并在公开的数据集上进行了验证，旨在为未来的研究提供一个基准。

Abstract: Monitoring the effects of the chemotherapeutic agent Taxol at the cellular
level is critical for both clinical evaluation and biomedical research.
However, existing detection methods require specialized equipment, skilled
personnel, and extensive sample preparation, making them expensive,
labor-intensive, and unsuitable for high-throughput or real-time analysis. Deep
learning approaches have shown great promise in medical and biological image
analysis, enabling automated, high-throughput assessment of cellular
morphology. Yet, no publicly available dataset currently exists for automated
morphological analysis of cellular responses to Taxol exposure. To address this
gap, we introduce a new microscopy image dataset capturing C6 glioma cells
treated with varying concentrations of Taxol. To provide an effective solution
for Taxol concentration classification and establish a benchmark for future
studies on this dataset, we propose a baseline model named ResAttention-KNN,
which combines a ResNet-50 with Convolutional Block Attention Modules and uses
a k-Nearest Neighbors classifier in the learned embedding space. This model
integrates attention-based refinement and non-parametric classification to
enhance robustness and interpretability. Both the dataset and implementation
are publicly released to support reproducibility and facilitate future research
in vision-based biomedical analysis.

</details>


### [20] [Learning Point Cloud Representations with Pose Continuity for Depth-Based Category-Level 6D Object Pose Estimation](https://arxiv.org/abs/2508.14358)
*Zhujun Li,Shuo Zhang,Ioannis Stamos*

Main category: cs.CV

TL;DR: HRC-Pose 是一种新的深度学习方法，用于物体位姿估计，通过对比学习来解决现有方法的不足，并在多个基准测试中取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖 6D 位姿作为监督信号，未能显式捕捉位姿的内在连续性，导致预测不一致和对未见位姿的泛化能力降低。

Method: 提出了一种新颖的仅深度估计的框架 HRC-Pose，利用对比学习来学习保留 6D 位姿连续性的点云表示。HRC-Pose 将物体位姿解耦为旋转和平移分量，并在网络中分别进行编码和利用。具体来说，基于提出的 6D 位姿感知的分层排名方案，在多任务、多类别场景下引入了对比学习策略，通过考虑旋转和平移差异以及类别信息来对比多个类别的点云。进一步设计了分别处理学习到的旋转感知和翻译感知嵌入的位姿估计模块。

Result: 实验表明，HRC-Pose 成功学习了连续的特征空间。在 REAL275 和 CAMERA25 基准测试上，HRC-Pose 的性能始终优于现有的仅深度估计的最先进方法，并能实时运行。

Conclusion: HRC-Pose 成功学习了连续的特征空间，在 REAL275 和 CAMERA25 基准测试中，其性能始终优于现有的仅深度估计的最先进方法，并能实时运行，证明了其在实际应用中的有效性和潜力。

Abstract: Category-level object pose estimation aims to predict the 6D pose and 3D size
of objects within given categories. Existing approaches for this task rely
solely on 6D poses as supervisory signals without explicitly capturing the
intrinsic continuity of poses, leading to inconsistencies in predictions and
reduced generalization to unseen poses. To address this limitation, we propose
HRC-Pose, a novel depth-only framework for category-level object pose
estimation, which leverages contrastive learning to learn point cloud
representations that preserve the continuity of 6D poses. HRC-Pose decouples
object pose into rotation and translation components, which are separately
encoded and leveraged throughout the network. Specifically, we introduce a
contrastive learning strategy for multi-task, multi-category scenarios based on
our 6D pose-aware hierarchical ranking scheme, which contrasts point clouds
from multiple categories by considering rotational and translational
differences as well as categorical information. We further design pose
estimation modules that separately process the learned rotation-aware and
translation-aware embeddings. Our experiments demonstrate that HRC-Pose
successfully learns continuous feature spaces. Results on REAL275 and CAMERA25
benchmarks show that our method consistently outperforms existing depth-only
state-of-the-art methods and runs in real-time, demonstrating its effectiveness
and potential for real-world applications. Our code is at
https://github.com/zhujunli1993/HRC-Pose.

</details>


### [21] [Taming Transformer for Emotion-Controllable Talking Face Generation](https://arxiv.org/abs/2508.14359)
*Ziqi Zhang,Cheng Deng*

Main category: cs.CV

TL;DR: A novel method for emotion-controllable talking face generation using disentangled audio, quantized visual tokens with an emotion-anchor representation, and an autoregressive transformer to synthesize videos. Achieves superior qualitative and quantitative results.


<details>
  <summary>Details</summary>
Motivation: To fulfill emotion-controllable talking face generation by effectively modeling multimodal relationships related to specific emotions and synthesizing identity-preserving emotional videos.

Method: The method involves pre-training strategies to disentangle audio and quantize videos into tokens, proposing an emotion-anchor (EA) representation, and using an autoregressive transformer to predict visual token sequences for video synthesis.

Result: Experimental results on the MEAD dataset show the superiority of the proposed method in controlling video emotions based on multiple emotional audios.

Conclusion: The proposed method effectively models multimodal relationships for emotion-controllable talking face generation and demonstrates superiority both qualitatively and quantitatively on the MEAD dataset.

Abstract: Talking face generation is a novel and challenging generation task, aiming at
synthesizing a vivid speaking-face video given a specific audio. To fulfill
emotion-controllable talking face generation, current methods need to overcome
two challenges: One is how to effectively model the multimodal relationship
related to the specific emotion, and the other is how to leverage this
relationship to synthesize identity preserving emotional videos. In this paper,
we propose a novel method to tackle the emotion-controllable talking face
generation task discretely. Specifically, we employ two pre-training strategies
to disentangle audio into independent components and quantize videos into
combinations of visual tokens. Subsequently, we propose the emotion-anchor (EA)
representation that integrates the emotional information into visual tokens.
Finally, we introduce an autoregressive transformer to model the global
distribution of the visual tokens under the given conditions and further
predict the index sequence for synthesizing the manipulated videos. We conduct
experiments on the MEAD dataset that controls the emotion of videos conditioned
on multiple emotional audios. Extensive experiments demonstrate the
superiorities of our method both qualitatively and quantitatively.

</details>


### [22] [FastTracker: Real-Time and Accurate Visual Tracking](https://arxiv.org/abs/2508.14370)
*Hamidreza Hashempoor,Yu Dong Hwang*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Conventional multi-object tracking (MOT) systems are predominantly designed
for pedestrian tracking and often exhibit limited generalization to other
object categories. This paper presents a generalized tracking framework capable
of handling multiple object types, with a particular emphasis on vehicle
tracking in complex traffic scenes. The proposed method incorporates two key
components: (1) an occlusion-aware re-identification mechanism that enhances
identity preservation for heavily occluded objects, and (2) a
road-structure-aware tracklet refinement strategy that utilizes semantic scene
priors such as lane directions, crosswalks, and road boundaries to improve
trajectory continuity and accuracy. In addition, we introduce a new benchmark
dataset comprising diverse vehicle classes with frame-level tracking
annotations, specifically curated to support evaluation of vehicle-focused
tracking methods. Extensive experimental results demonstrate that the proposed
approach achieves robust performance on both the newly introduced dataset and
several public benchmarks, highlighting its effectiveness in general-purpose
object tracking. While our framework is designed for generalized multi-class
tracking, it also achieves strong performance on conventional benchmarks, with
HOTA scores of 66.4 on MOT17 and 65.7 on MOT20 test sets. Code and Benchmark
are available: github.com/Hamidreza-Hashempoor/FastTracker,
huggingface.co/datasets/Hamidreza-Hashemp/FastTracker-Benchmark.

</details>


### [23] [TCFNet: Bidirectional face-bone transformation via a Transformer-based coarse-to-fine point movement network](https://arxiv.org/abs/2508.14373)
*Runshi Zhang,Bimeng Jie,Yang He,Junchen Wang*

Main category: cs.CV

TL;DR: TCFNet 是一种用于面骨点云转换的 Transformer 协同网络，它通过粗到精的方法和局部信息聚合来提高准确性，并具有一个利用专家知识的辅助损失。


<details>
  <summary>Details</summary>
Motivation: 传统的生物力学模拟方法在计算时间消耗、劳动密集型数据处理策略和准确性方面存在局限性。深度学习方法无法处理大规模点云，感受野有限，并且需要复杂的注册操作。这些缺点限制了这些方法的性能和广泛适用性。

Method: 提出了一种基于 Transformer 的粗到精点移动网络（TCFNet），该网络在第一和第二阶段分别采用基于 Transformer 的网络和局部信息聚合网络（LIA-Net），两者相互加强，以生成精确的点移动路径。LIA-Net 通过对局部几何结构（边、方向和相对位置特征）进行建模，有效补偿了基于 Transformer 的网络在邻域精度上的损失。先前おのグローバル特徴被用于通过门控循环单元指导局部位移。受可变形医学图像配准的启发，提出了一种利用专家知识重建关键器官的辅助损失。

Result: TCFNet 在收集的数据集上与现有最先进（SOTA）方法相比，在评估指标和可视化结果方面均取得了优异的成绩。

Conclusion: TCFNet 在收集的数据集上与现有最先进（SOTA）方法相比，在评估指标和可视化结果方面均取得了优异的成绩。

Abstract: Computer-aided surgical simulation is a critical component of orthognathic
surgical planning, where accurately simulating face-bone shape transformations
is significant. The traditional biomechanical simulation methods are limited by
their computational time consumption levels, labor-intensive data processing
strategies and low accuracy. Recently, deep learning-based simulation methods
have been proposed to view this problem as a point-to-point transformation
between skeletal and facial point clouds. However, these approaches cannot
process large-scale points, have limited receptive fields that lead to noisy
points, and employ complex preprocessing and postprocessing operations based on
registration. These shortcomings limit the performance and widespread
applicability of such methods. Therefore, we propose a Transformer-based
coarse-to-fine point movement network (TCFNet) to learn unique, complicated
correspondences at the patch and point levels for dense face-bone point cloud
transformations. This end-to-end framework adopts a Transformer-based network
and a local information aggregation network (LIA-Net) in the first and second
stages, respectively, which reinforce each other to generate precise point
movement paths. LIA-Net can effectively compensate for the neighborhood
precision loss of the Transformer-based network by modeling local geometric
structures (edges, orientations and relative position features). The previous
global features are employed to guide the local displacement using a gated
recurrent unit. Inspired by deformable medical image registration, we propose
an auxiliary loss that can utilize expert knowledge for reconstructing critical
organs.Compared with the existing state-of-the-art (SOTA) methods on gathered
datasets, TCFNet achieves outstanding evaluation metrics and visualization
results. The code is available at https://github.com/Runshi-Zhang/TCFNet.

</details>


### [24] [QuadINR: Hardware-Efficient Implicit Neural Representations Through Quadratic Activation](https://arxiv.org/abs/2508.14374)
*Wenyong Zhou,Boyu Li,Jiachen Ren,Taiqiang Wu,Zhilin Ai,Zhengwu Liu,Ngai Wong*

Main category: cs.CV

TL;DR: QuadINR是一种硬件高效的INR，使用分段二次激活函数，在降低硬件成本的同时提高了性能，并在FPGA和ASIC上进行了实现。


<details>
  <summary>Details</summary>
Motivation: 现有的隐式神经表示（INRs）方法虽然能将离散信号编码为连续表示，但会受到激活函数（AFs）引起的频谱偏差问题。为了解决这个问题，以往的方法采用了复杂的激活函数，但这会带来显著的硬件开销。因此，需要一种在保持性能的同时降低硬件成本的解决方案。

Method: 提出了一种名为QuadINR的硬件高效隐式神经表示方法，该方法使用分段二次激活函数来解决频谱偏差问题。通过神经网络切线核（NTK）分析验证了二次函数在傅里叶级数中包含丰富的谐波内容，能够增强高频信号的表现力。开发了一个统一的N阶段流水线框架，用于实现不同激活函数的INRs。

Result: QuadINR在图像和视频实验中，实现了高达2.06dB的PSNR提升，硬件面积仅为1914μm²，动态功耗为6.14mW，与现有基线相比，资源和功耗消耗降低高达97%，延迟改善高达93%。

Conclusion: QuadINR利用分段二次激活函数，在实现更高性能的同时，显著降低了硬件开销，并在FPGA和ASIC上进行了实现和验证。

Abstract: Implicit Neural Representations (INRs) encode discrete signals continuously
while addressing spectral bias through activation functions (AFs). Previous
approaches mitigate this bias by employing complex AFs, which often incur
significant hardware overhead. To tackle this challenge, we introduce QuadINR,
a hardware-efficient INR that utilizes piecewise quadratic AFs to achieve
superior performance with dramatic reductions in hardware consumption. The
quadratic functions encompass rich harmonic content in their Fourier series,
delivering enhanced expressivity for high-frequency signals, as verified
through Neural Tangent Kernel (NTK) analysis. We develop a unified $N$-stage
pipeline framework that facilitates efficient hardware implementation of
various AFs in INRs. We demonstrate FPGA implementations on the VCU128 platform
and an ASIC implementation in a 28nm process. Experiments across images and
videos show that QuadINR achieves up to 2.06dB PSNR improvement over prior
work, with an area of only 1914$\mu$m$^2$ and a dynamic power of 6.14mW,
reducing resource and power consumption by up to 97\% and improving latency by
up to 93\% vs existing baselines.

</details>


### [25] [Img2ST-Net: Efficient High-Resolution Spatial Omics Prediction from Whole Slide Histology Images via Fully Convolutional Image-to-Image Learning](https://arxiv.org/abs/2508.14393)
*Junchao Zhu,Ruining Deng,Junlin Guo,Tianyuan Yao,Juming Xiong,Chongyu Qu,Mengmeng Yin,Yu Wang,Shilin Zhao,Haichun Yang,Daguang Xu,Yucheng Tang,Yuankai Huo*

Main category: cs.CV

TL;DR: Img2ST-Net 是一个新颖的框架，可以通过组织学图像高效、并行地预测高分辨率空间转录组学 (ST) 数据，解决了计算和稀疏性挑战，并引入了新的评估指标 SSIM-ST。


<details>
  <summary>Details</summary>
Motivation: 现有的空间转录组学 (ST) 数据获取成本高昂且耗时。虽然多模态 AI 有潜力从组织学图像生成 ST 数据，但高分辨率 ST（如 Visium HD）带来的计算和建模挑战（例如，点对点回归框架的效率和稳定性问题，以及数据稀疏性）阻碍了其应用。

Method: Img2ST-Net 采用全卷积架构，将高分辨率 ST 数据建模为超像素表示，实现了密集基因表达图的并行生成。SSIM-ST 作为一种结构相似性评估指标，专门用于高分辨率 ST 分析。

Result: Img2ST-Net 实现了高效且并行的全卷积高分辨率 ST 预测，提高了计算效率，更好地保留了空间组织，并能在稀疏表达模式下保持鲁棒性。

Conclusion: Img2ST-Net 提供了一个高效且准确的预测高分辨率空间转录组学 (ST) 数据的框架，解决了现有方法在大规模、高分辨率数据上面临的计算和建模挑战。通过采用全卷积架构进行并行预测，并将任务重新定义为超像素内容生成问题，Img2ST-Net 提高了计算效率并更好地保留了空间组织信息。SSIM-ST 评估指标的引入增强了在稀疏表达模式下的鲁棒性。

Abstract: Recent advances in multi-modal AI have demonstrated promising potential for
generating the currently expensive spatial transcriptomics (ST) data directly
from routine histology images, offering a means to reduce the high cost and
time-intensive nature of ST data acquisition. However, the increasing
resolution of ST, particularly with platforms such as Visium HD achieving 8um
or finer, introduces significant computational and modeling challenges.
Conventional spot-by-spot sequential regression frameworks become inefficient
and unstable at this scale, while the inherent extreme sparsity and low
expression levels of high-resolution ST further complicate both prediction and
evaluation. To address these limitations, we propose Img2ST-Net, a novel
histology-to-ST generation framework for efficient and parallel high-resolution
ST prediction. Unlike conventional spot-by-spot inference methods, Img2ST-Net
employs a fully convolutional architecture to generate dense, HD gene
expression maps in a parallelized manner. By modeling HD ST data as super-pixel
representations, the task is reformulated from image-to-omics inference into a
super-content image generation problem with hundreds or thousands of output
channels. This design not only improves computational efficiency but also
better preserves the spatial organization intrinsic to spatial omics data. To
enhance robustness under sparse expression patterns, we further introduce
SSIM-ST, a structural-similarity-based evaluation metric tailored for
high-resolution ST analysis. We present a scalable, biologically coherent
framework for high-resolution ST prediction. Img2ST-Net offers a principled
solution for efficient and accurate ST inference at scale. Our contributions
lay the groundwork for next-generation ST modeling that is robust and
resolution-aware. The source code has been made publicly available at
https://github.com/hrlblab/Img2ST-Net.

</details>


### [26] [CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities](https://arxiv.org/abs/2508.14405)
*Yue Gong,Shanyuan Liu,Liuzhuozheng Li,Jian Zhu,Bo Cheng,Liebucha Wu,Xiaoyu Wu,Yuhang Ma,Dawei Leng,Yuhui Yin*

Main category: cs.CV

TL;DR: CTA-Flux是一种适配中文文本到Flux模型的创新方法，通过MMDiT增强了模型对中文的理解和生成质量，同时保持了与现有插件的兼容性。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像（TTI）模型（如Flux）主要基于英语语料库训练，在处理非英语（特别是中文）提示时表现不佳，存在语言和文化偏差，导致图像生成质量和文化真实性不高。现有的解决方案（如翻译或双语映射微调）未能充分解决文化语义问题。

Method: 提出了一种名为CTA-Flux的适配方法，该方法利用多模态扩散Transformer（MMDiT）直接控制Flux模型，以适配中文文本输入，解决了现有方法在处理非英语提示时存在的语言和文化偏差问题。

Result: CTA-Flux支持中文和英文提示，在图像生成质量、视觉真实性和中文语义的忠实度方面均优于现有方法，并且参数量显著减少。

Conclusion: CTA-Flux通过利用MMDiT直接控制Flux骨干网络，显著减少了参数量，增强了模型对中文语义的理解，提高了生成图像的质量和文化真实性，并能与LoRA、IP-Adapter和ControlNet等现有模型插件兼容。

Abstract: We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method
fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative
model initially trained on the English corpus. Despite the notable image
generation ability conditioned on English text inputs, Flux performs poorly
when processing non-English prompts, particularly due to linguistic and
cultural biases inherent in predominantly English-centric training datasets.
Existing approaches, such as translating non-English prompts into English or
finetuning models for bilingual mappings, inadequately address culturally
specific semantics, compromising image authenticity and quality. To address
this issue, we introduce a novel method to bridge Chinese semantic
understanding with compatibility in English-centric TTI model communities.
Existing approaches relying on ControlNet-like architectures typically require
a massive parameter scale and lack direct control over Chinese semantics. In
comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to
control the Flux backbone directly, significantly reducing the number of
parameters while enhancing the model's understanding of Chinese semantics. This
integration significantly improves the generation quality and cultural
authenticity without extensive retraining of the entire model, thus maintaining
compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and
ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese
and English prompts and achieves superior image generation quality, visual
realism, and faithful depiction of Chinese semantics.

</details>


### [27] [MoCHA-former: Moiré-Conditioned Hybrid Adaptive Transformer for Video Demoiréing](https://arxiv.org/abs/2508.14423)
*Jeahun Sung,Changhyun Roh,Chanho Eom,Jihyong Oh*

Main category: cs.CV

TL;DR: MoCHA-former 是一种用于去除便携式成像中相机捕获的屏幕照片和视频中的 Moiré 伪影的 Transformer 模型。它通过 DMAD 和 STA 模块解决现有方法的局限性，并在各种评估指标上取得了优于先前方法的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有去 Moiré 方法在处理空间变化的伪影强度、大规模和全局扩散结构、通道依赖性统计和跨帧的快速时间波动等问题上的局限性，我们提出了 MoCHA-former。

Method: MoCHA-former 包含两个关键组件：解耦的 Moiré 自适应去 Moiré (DMAD) 和时空自适应去 Moiré (STAD)。DMAD 通过 Moiré 解耦块 (MDB) 和细节解耦块 (DDB) 分离 Moiré 和内容，然后使用 Moiré 条件块 (MCB) 生产 Moiré 自适应特征以进行目标恢复。STAD 引入了一个具有窗口注意力的空间融合块 (SFB) 来捕获大规模结构，并引入了一个特征通道注意力 (FCA) 来模拟 RAW 帧中的通道依赖性。为了确保时间一致性，MoCHA-former 在没有显式对齐模块的情况下进行隐式帧对齐。

Result: 对 Moiré 特征进行了定性和定量研究，并在覆盖 RAW 和 sRGB 域的两个视频数据集上进行了评估。

Conclusion: MoCHA-former 在 PSNR、SSIM 和 LPIPS 等指标上始终优于先前的方法。

Abstract: Recent advances in portable imaging have made camera-based screen capture
ubiquitous. Unfortunately, frequency aliasing between the camera's color filter
array (CFA) and the display's sub-pixels induces moir\'e patterns that severely
degrade captured photos and videos. Although various demoir\'eing models have
been proposed to remove such moir\'e patterns, these approaches still suffer
from several limitations: (i) spatially varying artifact strength within a
frame, (ii) large-scale and globally spreading structures, (iii)
channel-dependent statistics and (iv) rapid temporal fluctuations across
frames. We address these issues with the Moir\'e Conditioned Hybrid Adaptive
Transformer (MoCHA-former), which comprises two key components: Decoupled
Moir\'e Adaptive Demoir\'eing (DMAD) and Spatio-Temporal Adaptive Demoir\'eing
(STAD). DMAD separates moir\'e and content via a Moir\'e Decoupling Block (MDB)
and a Detail Decoupling Block (DDB), then produces moir\'e-adaptive features
using a Moir\'e Conditioning Block (MCB) for targeted restoration. STAD
introduces a Spatial Fusion Block (SFB) with window attention to capture
large-scale structures, and a Feature Channel Attention (FCA) to model channel
dependence in RAW frames. To ensure temporal consistency, MoCHA-former performs
implicit frame alignment without any explicit alignment module. We analyze
moir\'e characteristics through qualitative and quantitative studies, and
evaluate on two video datasets covering RAW and sRGB domains. MoCHA-former
consistently surpasses prior methods across PSNR, SSIM, and LPIPS.

</details>


### [28] [Making Pose Representations More Expressive and Disentangled via Residual Vector Quantization](https://arxiv.org/abs/2508.14561)
*Sukhyun Jeong,Hong-Gi Shin,Yong-Hoon Choi*

Main category: cs.CV

TL;DR: 一种通过结合 RVQ 增强的连续运动特征来改进文本到运动生成的方法，以捕捉细微的运动细节并提高可控性。


<details>
  <summary>Details</summary>
Motivation: 现有的可控运动生成（CoMo）方法通常依赖于姿势码表示，但这种表示无法捕捉细微的运动细节，限制了表现力。本研究旨在克服这一限制。

Method: 提出了一种使用残差向量量化（RVQ）将连续运动特征与基于姿势码的潜在表示相结合的方法。

Result: 实验表明，该模型将 FID 从 0.041 降低到 0.015，并将 Top-1 R-Precision 从 0.508 提高到 0.510。此外，对姿势码之间成对方向相似性的定性分析证实了该模型在运动编辑方面具有可控性。

Conclusion: 该模型通过结合 RVQ 增强的连续运动特征来改进基于姿势码的文本到运动生成，从而在保留可解释性的同时捕捉细微的运动细节。

Abstract: Recent progress in text-to-motion has advanced both 3D human motion
generation and text-based motion control. Controllable motion generation
(CoMo), which enables intuitive control, typically relies on pose code
representations, but discrete pose codes alone cannot capture fine-grained
motion details, limiting expressiveness. To overcome this, we propose a method
that augments pose code-based latent representations with continuous motion
features using residual vector quantization (RVQ). This design preserves the
interpretability and manipulability of pose codes while effectively capturing
subtle motion characteristics such as high-frequency details. Experiments on
the HumanML3D dataset show that our model reduces Frechet inception distance
(FID) from 0.041 to 0.015 and improves Top-1 R-Precision from 0.508 to 0.510.
Qualitative analysis of pairwise direction similarity between pose codes
further confirms the model's controllability for motion editing.

</details>


### [29] [HyperDiff: Hypergraph Guided Diffusion Model for 3D Human Pose Estimation](https://arxiv.org/abs/2508.14431)
*Bing Han,Yuhua Huang,Pan Gao*

Main category: cs.CV

TL;DR: HyperDiff是一种新颖的3D人体姿态估计方法，它结合了扩散模型和HyperGCN，解决了深度模糊、遮挡和多尺度骨架特征利用不足的问题，并在多个数据集上取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决单目3D人体姿态估计（HPE）在2D到3D提升过程中遇到的深度模糊和遮挡等挑战，以及传统方法在利用骨架结构信息时可能忽略多尺度骨架特征的问题。

Method: 本研究提出的HyperDiff方法整合了扩散模型和HyperGCN。扩散模型用于捕捉数据不确定性，以缓解深度模糊和遮挡问题。HyperGCN作为去噪器，利用多粒度结构来精确建模关节点之间的高阶相关性，从而提升模型在复杂姿态下的去噪能力。

Result: 实验结果表明，HyperDiff在Human3.6M和MPI-INF-3DHP数据集上取得了最先进的性能，并且能够灵活适应不同的计算资源，实现了性能和效率的平衡。

Conclusion: HyperDiff在Human3.6M和MPI-INF-3DHP数据集上实现了最先进的性能，并且能够灵活适应不同的计算资源以平衡性能和效率。

Abstract: Monocular 3D human pose estimation (HPE) often encounters challenges such as
depth ambiguity and occlusion during the 2D-to-3D lifting process.
Additionally, traditional methods may overlook multi-scale skeleton features
when utilizing skeleton structure information, which can negatively impact the
accuracy of pose estimation. To address these challenges, this paper introduces
a novel 3D pose estimation method, HyperDiff, which integrates diffusion models
with HyperGCN. The diffusion model effectively captures data uncertainty,
alleviating depth ambiguity and occlusion. Meanwhile, HyperGCN, serving as a
denoiser, employs multi-granularity structures to accurately model high-order
correlations between joints. This improves the model's denoising capability
especially for complex poses. Experimental results demonstrate that HyperDiff
achieves state-of-the-art performance on the Human3.6M and MPI-INF-3DHP
datasets and can flexibly adapt to varying computational resources to balance
performance and efficiency.

</details>


### [30] [Fusing Monocular RGB Images with AIS Data to Create a 6D Pose Estimation Dataset for Marine Vessels](https://arxiv.org/abs/2508.14767)
*Fabian Holst,Emre Gülsoylu,Simone Frintrop*

Main category: cs.CV

TL;DR: 该研究提出了一种通过融合 RGB 图像和 AIS 数据来生成船只 6D 位姿的新技术，并创建了一个名为 BONK-pose 的数据集，以解决现有数据集中手动注释的不足和 AIS 数据本身的局限性。


<details>
  <summary>Details</summary>
Motivation: 为了克服仅依赖 AIS 数据的局限性，如设备可靠性、数据篡改和传输延迟问题，并创建无需手动注释的 6D 位姿估计数据集。

Method: 通过融合单目 RGB 图像和自动识别系统 (AIS) 数据，利用 YOLOX-X 对象检测网络进行对象检测，并结合 PnP 方法将 AIS 数据与图像坐标对齐，来生成 6D 位姿。

Result: YOLOX-X 模型在 IoU 阈值为 0.5 时，相关船只类别的平均精度 (mAP) 为 0.80。PnP 方法实现了比基于单应性的方法更低的投影误差。成功创建了一个包含 3753 张图像和 3D 边界框注释的公开数据集 BONK-pose。

Conclusion: 该方法表明，无需手动注释即可创建 6D 位姿估计数据集，并且 PnP 方法比之前使用的基于单应性的方法实现了更低的投影误差。

Abstract: The paper presents a novel technique for creating a 6D pose estimation
dataset for marine vessels by fusing monocular RGB images with Automatic
Identification System (AIS) data. The proposed technique addresses the
limitations of relying purely on AIS for location information, caused by issues
like equipment reliability, data manipulation, and transmission delays. By
combining vessel detections from monocular RGB images, obtained using an object
detection network (YOLOX-X), with AIS messages, the technique generates 3D
bounding boxes that represent the vessels' 6D poses, i.e. spatial and
rotational dimensions. The paper evaluates different object detection models to
locate vessels in image space. We also compare two transformation methods
(homography and Perspective-n-Point) for aligning AIS data with image
coordinates. The results of our work demonstrate that the Perspective-n-Point
(PnP) method achieves a significantly lower projection error compared to
homography-based approaches used before, and the YOLOX-X model achieves a mean
Average Precision (mAP) of 0.80 at an Intersection over Union (IoU) threshold
of 0.5 for relevant vessel classes. We show indication that our approach allows
the creation of a 6D pose estimation dataset without needing manual annotation.
Additionally, we introduce the Boats on Nordelbe Kehrwieder (BONK-pose), a
publicly available dataset comprising 3753 images with 3D bounding box
annotations for pose estimation, created by our data fusion approach. This
dataset can be used for training and evaluating 6D pose estimation networks. In
addition we introduce a set of 1000 images with 2D bounding box annotations for
ship detection from the same scene.

</details>


### [31] [FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation](https://arxiv.org/abs/2508.14437)
*Gabriel Tjio,Jie Zhang,Xulei Yang,Yun Xing,Nhat Chung,Xiaofeng Cao,Ivor W. Tsang,Chee Keong Kwoh,Qing Guo*

Main category: cs.CV

TL;DR: FOCUS是一种新颖的频率条件化方法，通过扩散模型来适应模型自适应中的领域迁移问题，能有效防止知识遗忘，并在各种基准测试中取得领先成果。


<details>
  <summary>Details</summary>
Motivation: 在模型自适应过程中，如何在保持原有知识和适应领域迁移之间取得平衡是一个挑战，因为适应领域迁移可能会导致任务相关知识的遗忘。为了解决这个问题，我们提出FOCUS。

Method: FOCUS是一种新颖的基于频率的条件化方法，嵌套在由扩散驱动的输入自适应框架中。它利用了学习到的、空间自适应的频率先验，在扩散驱动的去噪过程中对逆向步骤进行条件化，以保留用于密集预测的任务相关语义信息。FOCUS使用了一个经过训练的、轻量级的Y形频率预测网络（Y-FPN），该网络将高频和低频信息从噪声图像中分离出来。Y-FPN通过一种名为FrequencyMix的新型数据增强方法进行训练，该方法会跨不同的频带来扰动图像，从而提高了方法的鲁棒性。

Result: FOCUS在语义分割和单目深度估计任务的15种不同损坏类型和3个数据集上证明了其有效性，平均性能达到了最先进水平。此外，FOCUS还能作为现有模型自适应方法的补充，通过FOCUS去噪图像生成伪标签以提供额外监督，即使在伪标签的有限、间歇性监督下，FOCUS也能有效缓解模型自适应方法的灾难性遗忘问题。

Conclusion: FOCUS能够有效缓解灾难性遗忘，并且可以为现有的模型自适应方法提供额外的监督，在各种噪声和领域迁移场景下都能取得最先进的平均性能。

Abstract: Test-time adaptation enables models to adapt to evolving domains. However,
balancing the tradeoff between preserving knowledge and adapting to domain
shifts remains challenging for model adaptation methods, since adapting to
domain shifts can induce forgetting of task-relevant knowledge. To address this
problem, we propose FOCUS, a novel frequency-based conditioning approach within
a diffusion-driven input-adaptation framework. Utilising learned, spatially
adaptive frequency priors, our approach conditions the reverse steps during
diffusion-driven denoising to preserve task-relevant semantic information for
dense prediction.
  FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network
(Y-FPN) that disentangles high and low frequency information from noisy images.
This minimizes the computational costs involved in implementing our approach in
a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data
augmentation method that perturbs the images across diverse frequency bands,
which improves the robustness of our approach to diverse corruptions.
  We demonstrate the effectiveness of FOCUS for semantic segmentation and
monocular depth estimation across 15 corruption types and three datasets,
achieving state-of-the-art averaged performance. In addition to improving
standalone performance, FOCUS complements existing model adaptation methods
since we can derive pseudo labels from FOCUS-denoised images for additional
supervision. Even under limited, intermittent supervision with the pseudo
labels derived from the FOCUS denoised images, we show that FOCUS mitigates
catastrophic forgetting for recent model adaptation methods.

</details>


### [32] [Virtual Community: An Open World for Humans, Robots, and Society](https://arxiv.org/abs/2508.14893)
*Qinhong Zhou,Hongxin Zhang,Xiangye Lin,Zheyuan Zhang,Yutian Chen,Wenjun Liu,Zunzhe Zhang,Sunli Chen,Lixing Fang,Qiushi Lyu,Xinyu Sun,Jincheng Yang,Zeyuan Wang,Bao Chi Dang,Zhehuan Chen,Daksha Ladia,Jiageng Liu,Chuang Gan*

Main category: cs.CV

TL;DR: 该研究提出了Virtual Community，一个用于研究人机共存的开放世界平台。该平台包括一个物理模拟器和一个社区生成流水线。研究人员还提出了两个挑战：社区规划挑战和社区机器人挑战，以评估开放世界环境中的多代理推理和协作能力。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能和机器人技术的快速发展，人类和机器人在共享社区中共存，这预示着深刻的社会变革，带来了机遇和挑战。本研究旨在探索人机共存的未来。

Method: 该研究提出了Virtual Community，一个建立在通用物理引擎之上并以真实3D场景为基础的开放世界平台，用于人类、机器人和社会。Virtual Community包括一个开源的多代理物理模拟器和一个大规模、真实世界对齐的社区生成流程，支持各种代理及其在社会中的交互。此外，还提出了两个新颖的挑战：社区规划挑战和社区机器人挑战，用于评估开放世界设置下的多代理推理、规划能力以及异构机器人在解决复杂开放世界任务方面的协作能力。

Result: 该研究提出了Virtual Community平台，并提出了两个新颖的挑战：社区规划挑战和社区机器人挑战。通过对这些挑战中的各种基线进行评估，研究证明了在高层次开放世界任务规划和低层次协作控制方面存在的挑战。

Conclusion: Virtual Community平台旨在促进对体现的社会智能的大规模研究，并希望进一步推动对开放世界环境中人机共存的研究。

Abstract: The rapid progress in AI and Robotics may lead to a profound societal
transformation, as humans and robots begin to coexist within shared
communities, introducing both opportunities and challenges. To explore this
future, we present Virtual Community-an open-world platform for humans, robots,
and society-built on a universal physics engine and grounded in real-world 3D
scenes. With Virtual Community, we aim to study embodied social intelligence at
scale: 1) How robots can intelligently cooperate or compete; 2) How humans
develop social relations and build community; 3) More importantly, how
intelligent robots and humans can co-exist in an open world. To support these,
Virtual Community features: 1) An open-source multi-agent physics simulator
that supports robots, humans, and their interactions within a society; 2) A
large-scale, real-world aligned community generation pipeline, including vast
outdoor space, diverse indoor scenes, and a community of grounded agents with
rich characters and appearances. Leveraging Virtual Community, we propose two
novel challenges. The Community Planning Challenge evaluates multi-agent
reasoning and planning ability in open-world settings, such as cooperating to
help agents with daily activities and efficiently connecting other agents. The
Community Robot Challenge requires multiple heterogeneous robots to collaborate
in solving complex open-world tasks. We evaluate various baselines on these
tasks and demonstrate the challenges in both high-level open-world task
planning and low-level cooperation controls. We hope that Virtual Community
will unlock further study of human-robot coexistence within open-world
environments.

</details>


### [33] [MUSE: Multi-Subject Unified Synthesis via Explicit Layout Semantic Expansion](https://arxiv.org/abs/2508.14440)
*Fei Peng,Junqiang Wu,Yan Li,Tingting Gao,Di Zhang,Huiyuan Fu*

Main category: cs.CV

TL;DR: MUSE框架通过CCA和渐进式训练策略，解决了文本到图像生成中多主体合成和空间控制的挑战，提高了图像的空间准确性和身份一致性。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像扩散模型在生成高质量图像方面表现出色，但在实现具有精确空间控制的多主体合成方面仍然面临挑战。现有方法难以同时满足此复合任务中空间精度和身份保持的双重要求。

Method: 提出了一种名为MUSE的统一合成框架，该框架采用连接交叉注意力（CCA）机制，通过显式的语义空间扩展，将布局规范与文本引导无缝集成。CCA机制实现了空间约束和文本描述之间的双向模态对齐，且没有干扰。此外，设计了一种渐进式两阶段训练策略，将LMS任务分解为可学习的子目标以进行有效优化。

Result: MUSE在零样本端到端生成方面取得了优于现有解决方案的性能，在空间准确性和身份一致性方面表现更佳。

Conclusion: MUSE通过结合CCA和渐进式两阶段训练策略，实现了零样本端到端生成，在空间准确性和身份一致性方面优于现有方法，推动了可控图像合成的边界。

Abstract: Existing text-to-image diffusion models have demonstrated remarkable
capabilities in generating high-quality images guided by textual prompts.
However, achieving multi-subject compositional synthesis with precise spatial
control remains a significant challenge. In this work, we address the task of
layout-controllable multi-subject synthesis (LMS), which requires both faithful
reconstruction of reference subjects and their accurate placement in specified
regions within a unified image. While recent advancements have separately
improved layout control and subject synthesis, existing approaches struggle to
simultaneously satisfy the dual requirements of spatial precision and identity
preservation in this composite task. To bridge this gap, we propose MUSE, a
unified synthesis framework that employs concatenated cross-attention (CCA) to
seamlessly integrate layout specifications with textual guidance through
explicit semantic space expansion. The proposed CCA mechanism enables
bidirectional modality alignment between spatial constraints and textual
descriptions without interference. Furthermore, we design a progressive
two-stage training strategy that decomposes the LMS task into learnable
sub-objectives for effective optimization. Extensive experiments demonstrate
that MUSE achieves zero-shot end-to-end generation with superior spatial
accuracy and identity consistency compared to existing solutions, advancing the
frontier of controllable image synthesis. Our code and model are available at
https://github.com/pf0607/MUSE.

</details>


### [34] [Reconstruction Using the Invisible: Intuition from NIR and Metadata for Enhanced 3D Gaussian Splatting](https://arxiv.org/abs/2508.14443)
*Gyusam Chang,Tuan-Anh Vu,Vivek Alumootil,Harris Song,Deanna Pham,Sangpil Kim,M. Khalid Jawed*

Main category: cs.CV

TL;DR: NIRSplat是一种新的多模态高斯喷涂方法，可以提高农业场景中的3D重建精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决3DGS在农业应用中遇到的挑战，例如光照不均、遮挡和视野有限，并利用近红外（NIR）数据和文本元数据来增强植物的理解。

Method: 提出了一种新颖的多模态高斯喷涂架构NIRSplat，该架构结合了交叉注意机制和基于3D点的 positional encoding，以提供鲁棒的几何先验。

Result: NIRSplat在具有挑战性的农业场景中表现出色，并在NIRPlant数据集上进行了验证，该数据集包含NIR图像、RGB图像、文本元数据、深度和LiDAR数据。

Conclusion: NIRSplat在具有挑战性的农业场景中优于现有的技术，包括3DGS、CoR-GS和Instant-Splat。

Abstract: While 3D Gaussian Splatting (3DGS) has rapidly advanced, its application in
agriculture remains underexplored. Agricultural scenes present unique
challenges for 3D reconstruction methods, particularly due to uneven
illumination, occlusions, and a limited field of view. To address these
limitations, we introduce \textbf{NIRPlant}, a novel multimodal dataset
encompassing Near-Infrared (NIR) imagery, RGB imagery, textual metadata, Depth,
and LiDAR data collected under varied indoor and outdoor lighting conditions.
By integrating NIR data, our approach enhances robustness and provides crucial
botanical insights that extend beyond the visible spectrum. Additionally, we
leverage text-based metadata derived from vegetation indices, such as NDVI,
NDWI, and the chlorophyll index, which significantly enriches the contextual
understanding of complex agricultural environments. To fully exploit these
modalities, we propose \textbf{NIRSplat}, an effective multimodal Gaussian
splatting architecture employing a cross-attention mechanism combined with 3D
point-based positional encoding, providing robust geometric priors.
Comprehensive experiments demonstrate that \textbf{NIRSplat} outperforms
existing landmark methods, including 3DGS, CoR-GS, and InstantSplat,
highlighting its effectiveness in challenging agricultural scenarios. The code
and dataset are publicly available at:
https://github.com/StructuresComp/3D-Reconstruction-NIR

</details>


### [35] [Generalizable Engagement Estimation in Conversation via Domain Prompting and Parallel Attention](https://arxiv.org/abs/2508.14448)
*Yangche Yu,Yin Chen,Jia Li,Peng Jia,Yu Zhang,Li Dai,Zhenzhen Hu,Meng Wang,Richang Hong*

Main category: cs.CV

TL;DR: DAPA是一个新框架，通过领域提示和并行交叉注意力来提高对话参与度估计的泛化能力和准确性，并在多个基准测试中取得了最先进的成果。


<details>
  <summary>Details</summary>
Motivation: 准确的参与度估计对于自适应人机交互系统至关重要，但由于在不同领域的泛化能力差以及对复杂交互动态建模的挑战，鲁棒部署受到阻碍。

Method: DAPA（Domain-Adaptive Parallel Attention）框架，通过在输入前添加可学习的领域特定向量（领域提示机制）来显式地将模型条件化到数据的来源，以促进面向领域的适应，同时保留可泛化的参与度表示。为了捕捉交互同步性，该框架还包含一个并行交叉注意力模块，以显式地对齐参与者之间反应性（前向BiLSTM）和预期性（后向BiLSTM）状态。

Result: DAPA框架在多个跨文化和跨语言基准上确立了新的最先进性能，在NoXi-J测试集上，与强基线相比，一致性相关系数（CCC）提高了0.45。该方法的优越性也通过在MultiMediate'25的多领域参与度估计挑战赛中获得第一名得到证实。

Conclusion: DAPA框架在多个跨文化和跨语言基准上确立了新的最先进性能，在NoXi-J测试集上，与强基线相比，一致性相关系数（CCC）提高了0.45。该方法在MultiMediate'25的多领域参与度估计挑战赛中获得第一名。

Abstract: Accurate engagement estimation is essential for adaptive human-computer
interaction systems, yet robust deployment is hindered by poor generalizability
across diverse domains and challenges in modeling complex interaction
dynamics.To tackle these issues, we propose DAPA (Domain-Adaptive Parallel
Attention), a novel framework for generalizable conversational engagement
modeling. DAPA introduces a Domain Prompting mechanism by prepending learnable
domain-specific vectors to the input, explicitly conditioning the model on the
data's origin to facilitate domain-aware adaptation while preserving
generalizable engagement representations. To capture interactional synchrony,
the framework also incorporates a Parallel Cross-Attention module that
explicitly aligns reactive (forward BiLSTM) and anticipatory (backward BiLSTM)
states between participants.Extensive experiments demonstrate that DAPA
establishes a new state-of-the-art performance on several cross-cultural and
cross-linguistic benchmarks, notably achieving an absolute improvement of 0.45
in Concordance Correlation Coefficient (CCC) over a strong baseline on the
NoXi-J test set. The superiority of our method was also confirmed by winning
the first place in the Multi-Domain Engagement Estimation Challenge at
MultiMediate'25.

</details>


### [36] [D^3-Talker: Dual-Branch Decoupled Deformation Fields for Few-Shot 3D Talking Head Synthesis](https://arxiv.org/abs/2508.14449)
*Yuhang Guo,Kaijun Deng,Siyang Song,Jindong Xie,Wenhui Ma,Linlin Shen*

Main category: cs.CV

TL;DR: D^3-Talker improves 3D talking head synthesis by decoupling audio-driven lip movements from identity-specific facial features using a novel 3D Gaussian attribute field and contrastive loss, leading to better synchronization and image quality with less training data.


<details>
  <summary>Details</summary>
Motivation: Existing methods struggle with lip synchronization and image quality in 3D talking head synthesis when trained on limited data, often due to audio containing irrelevant information for lip motion and difficulties in mapping audio to realistic lip behaviors.

Method: D^3-Talker utilizes a static 3D Gaussian attribute field and independently controls two distinct Gaussian attribute deformation fields using audio and facial motion signals, effectively decoupling general and personalized deformations. It incorporates a similarity contrastive loss function during pre-training for thorough decoupling and a Coarse-to-Fine module to refine rendered images, mitigating blurriness and enhancing quality.

Result: The proposed D^3-Talker achieves high-fidelity rendering and accurate audio-lip synchronization, outperforming previous methods, especially with limited training data.

Conclusion: D^3-Talker outperforms state-of-the-art methods in both high-fidelity rendering and accurate audio-lip synchronization with limited training data.

Abstract: A key challenge in 3D talking head synthesis lies in the reliance on a
long-duration talking head video to train a new model for each target identity
from scratch. Recent methods have attempted to address this issue by extracting
general features from audio through pre-training models. However, since audio
contains information irrelevant to lip motion, existing approaches typically
struggle to map the given audio to realistic lip behaviors in the target face
when trained on only a few frames, causing poor lip synchronization and talking
head image quality. This paper proposes D^3-Talker, a novel approach that
constructs a static 3D Gaussian attribute field and employs audio and Facial
Motion signals to independently control two distinct Gaussian attribute
deformation fields, effectively decoupling the predictions of general and
personalized deformations. We design a novel similarity contrastive loss
function during pre-training to achieve more thorough decoupling. Furthermore,
we integrate a Coarse-to-Fine module to refine the rendered images, alleviating
blurriness caused by head movements and enhancing overall image quality.
Extensive experiments demonstrate that D^3-Talker outperforms state-of-the-art
methods in both high-fidelity rendering and accurate audio-lip synchronization
with limited training data. Our code will be provided upon acceptance.

</details>


### [37] [Ouroboros: Single-step Diffusion Models for Cycle-consistent Forward and Inverse Rendering](https://arxiv.org/abs/2508.14461)
*Shanlin Sun,Yifan Wang,Hanwen Zhang,Yifeng Xiong,Qin Ren,Ruogu Fang,Xiaohui Xie,Chenyu You*

Main category: cs.CV

TL;DR: Ouroboros是一个由两个单步扩散模型组成的框架，用于前向和逆向渲染，解决了现有方法的周期不一致和推理速度慢的问题。它在室内外场景中表现出色，并能应用于视频分解。


<details>
  <summary>Details</summary>
Motivation: 现有的多步扩散模型在处理前向和逆向渲染时通常是独立进行的，这会导致周期不一致和推理速度慢的问题。

Method: 提出了一种名为Ouroboros的框架，该框架由两个单步扩散模型组成，能够处理前向和逆向渲染，并通过相互增强来解决这两个问题。该方法将内部分解扩展到室内和室外场景，并引入了周期一致性机制，确保前向和逆向渲染输出之间的一致性。

Result: 实验结果表明，Ouroboros在各种场景下均取得了最先进的性能，并且推理速度比其他基于扩散的方法快得多。该框架还可以以无需训练的方式迁移到视频分解，减少了视频序列的时间不一致性，同时保持了高质量的逐帧逆渲染。

Conclusion: Ouroboros框架在室内外场景的逆渲染任务上取得了最先进的性能，并显著提高了推理速度。此外，该框架无需额外训练即可应用于视频分解，减少了视频序列的时间不一致性，同时保持了高质量的逐帧逆渲染。

Abstract: While multi-step diffusion models have advanced both forward and inverse
rendering, existing approaches often treat these problems independently,
leading to cycle inconsistency and slow inference speed. In this work, we
present Ouroboros, a framework composed of two single-step diffusion models
that handle forward and inverse rendering with mutual reinforcement. Our
approach extends intrinsic decomposition to both indoor and outdoor scenes and
introduces a cycle consistency mechanism that ensures coherence between forward
and inverse rendering outputs. Experimental results demonstrate
state-of-the-art performance across diverse scenes while achieving
substantially faster inference speed compared to other diffusion-based methods.
We also demonstrate that Ouroboros can transfer to video decomposition in a
training-free manner, reducing temporal inconsistency in video sequences while
maintaining high-quality per-frame inverse rendering.

</details>


### [38] [DreamSwapV: Mask-guided Subject Swapping for Any Customized Video Editing](https://arxiv.org/abs/2508.14465)
*Weitao Wang,Zichen Wang,Hongdeng Shen,Yulei Lu,Xirui Fan,Suhui Wu,Jun Zhang,Haoqian Wang,Hao Zhang*

Main category: cs.CV

TL;DR: DreamSwapV is a new framework for video subject swapping that offers better customization and fidelity than previous methods, using mask guidance and improved condition integration.


<details>
  <summary>Details</summary>
Motivation: The demand for customized video editing, specifically subject swapping, is growing, but existing methods are limited to narrow domains or rely on indirect/ambiguous guidance, compromising fidelity.

Method: DreamSwapV is a mask-guided, subject-agnostic, end-to-end framework that uses multiple conditions and a dedicated condition fusion module for fine-grained guidance. It also incorporates an adaptive mask strategy for subjects of varying scales and attributes.

Result: DreamSwapV achieves superior performance compared to existing methods, as demonstrated by comprehensive experiments on VBench indicators and the newly introduced DreamSwapV-Benchmark.

Conclusion: DreamSwapV outperforms existing methods through its mask-guided, subject-agnostic framework, validated by experiments on VBench and a new benchmark.

Abstract: With the rapid progress of video generation, demand for customized video
editing is surging, where subject swapping constitutes a key component yet
remains under-explored. Prevailing swapping approaches either specialize in
narrow domains--such as human-body animation or hand-object interaction--or
rely on some indirect editing paradigm or ambiguous text prompts that
compromise final fidelity. In this paper, we propose DreamSwapV, a mask-guided,
subject-agnostic, end-to-end framework that swaps any subject in any video for
customization with a user-specified mask and reference image. To inject
fine-grained guidance, we introduce multiple conditions and a dedicated
condition fusion module that integrates them efficiently. In addition, an
adaptive mask strategy is designed to accommodate subjects of varying scales
and attributes, further improving interactions between the swapped subject and
its surrounding context. Through our elaborate two-phase dataset construction
and training scheme, our DreamSwapV outperforms existing methods, as validated
by comprehensive experiments on VBench indicators and our first introduced
DreamSwapV-Benchmark.

</details>


### [39] [LookOut: Real-World Humanoid Egocentric Navigation](https://arxiv.org/abs/2508.14466)
*Boxiao Pan,Adam W. Harley,C. Karen Liu,Leonidas J. Guibas*

Main category: cs.CV

TL;DR: The paper presents a new method and dataset for predicting future head poses from egocentric videos. The method uses a framework that reasons over temporally aggregated 3D latent features, and the dataset (AND) consists of 4 hours of real-world egocentric navigation data. The results show that the method learns human-like navigation behaviors and generalizes to new environments.


<details>
  <summary>Details</summary>
Motivation: Predicting collision-free future trajectories from egocentric observations is crucial in applications such as humanoid robotics, VR/AR, and assistive navigation. The paper addresses the challenging problem of predicting a sequence of future 6D head poses from an egocentric video, including both head translations and rotations to learn active information-gathering behavior through head-turning events. The lack of training data in this space also motivated the creation of a new dataset.

Method: A framework that reasons over temporally aggregated 3D latent features, which models the geometric and semantic constraints for both the static and dynamic parts of the environment. A data collection pipeline using Project Aria glasses was also developed.

Result: Extensive experiments show that the model learns human-like navigation behaviors and generalizes to unseen environments. The paper also presents the Aria Navigation Dataset (AND), a 4-hour dataset of real-world egocentric navigation scenarios.

Conclusion: The proposed framework learns human-like navigation behaviors such as waiting/slowing down, rerouting, and looking around for traffic while generalizing to unseen environments.

Abstract: The ability to predict collision-free future trajectories from egocentric
observations is crucial in applications such as humanoid robotics, VR / AR, and
assistive navigation. In this work, we introduce the challenging problem of
predicting a sequence of future 6D head poses from an egocentric video. In
particular, we predict both head translations and rotations to learn the active
information-gathering behavior expressed through head-turning events. To solve
this task, we propose a framework that reasons over temporally aggregated 3D
latent features, which models the geometric and semantic constraints for both
the static and dynamic parts of the environment. Motivated by the lack of
training data in this space, we further contribute a data collection pipeline
using the Project Aria glasses, and present a dataset collected through this
approach. Our dataset, dubbed Aria Navigation Dataset (AND), consists of 4
hours of recording of users navigating in real-world scenarios. It includes
diverse situations and navigation behaviors, providing a valuable resource for
learning real-world egocentric navigation policies. Extensive experiments show
that our model learns human-like navigation behaviors such as waiting / slowing
down, rerouting, and looking around for traffic while generalizing to unseen
environments. Check out our project webpage at
https://sites.google.com/stanford.edu/lookout.

</details>


### [40] [Vivid-VR: Distilling Concepts from Text-to-Video Diffusion Transformer for Photorealistic Video Restoration](https://arxiv.org/abs/2508.14483)
*Haoran Bai,Xiaoxu Chen,Canqian Yang,Zongyao He,Sibin Deng,Ying Chen*

Main category: cs.CV

TL;DR: Vivid-VR是一种基于DiT的视频修复方法，通过概念蒸馏和改进的控制架构来解决分布漂移问题，以提高纹理真实感和时间连贯性。


<details>
  <summary>Details</summary>
Motivation: 解决可控视频生成管道的微调常因多模态对齐不完美而出现分布漂移，导致纹理真实感和时间连贯性受损的问题。

Method: 提出了一种概念蒸馏训练策略，利用预训练的T2V模型合成包含文本概念的训练样本，以保留纹理和时间质量。重新设计了控制架构，包括一个控制特征投影仪和一个采用双分支设计的ControlNet连接器，该连接器结合了基于MLP的特征映射和交叉注意力机制，以实现内容保留和自适应控制信号调制。

Result: Vivid-VR在纹理真实感、视觉生动性和时间一致性方面表现出色。

Conclusion: Vivid-VR表现优于现有方法，在合成和真实世界基准以及AIGC视频上均取得了优异的纹理真实感、视觉生动性和时间一致性。

Abstract: We present Vivid-VR, a DiT-based generative video restoration method built
upon an advanced T2V foundation model, where ControlNet is leveraged to control
the generation process, ensuring content consistency. However, conventional
fine-tuning of such controllable pipelines frequently suffers from distribution
drift due to limitations in imperfect multimodal alignment, resulting in
compromised texture realism and temporal coherence. To tackle this challenge,
we propose a concept distillation training strategy that utilizes the
pretrained T2V model to synthesize training samples with embedded textual
concepts, thereby distilling its conceptual understanding to preserve texture
and temporal quality. To enhance generation controllability, we redesign the
control architecture with two key components: 1) a control feature projector
that filters degradation artifacts from input video latents to minimize their
propagation through the generation pipeline, and 2) a new ControlNet connector
employing a dual-branch design. This connector synergistically combines
MLP-based feature mapping with cross-attention mechanism for dynamic control
feature retrieval, enabling both content preservation and adaptive control
signal modulation. Extensive experiments show that Vivid-VR performs favorably
against existing approaches on both synthetic and real-world benchmarks, as
well as AIGC videos, achieving impressive texture realism, visual vividness,
and temporal consistency. The codes and checkpoints are publicly available at
https://github.com/csbhr/Vivid-VR.

</details>


### [41] [WeedSense: Multi-Task Learning for Weed Segmentation, Height Estimation, and Growth Stage Classification](https://arxiv.org/abs/2508.14486)
*Toqi Tahamid Sarker,Khaled R Ahmed,Taminul Islam,Cristiana Bernardi Rankrape,Karla Gage*

Main category: cs.CV

TL;DR: WeedSense 是一种用于杂草分析的新型多任务学习架构，可同时进行分割、高度估计和生长阶段分类。它使用独特的数据集，并在分割、高度估计和分类方面优于其他模型，同时实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 为了实现可持续农业实践和位点特定的管理方法，需要有效的杂草监测和分析策略。此研究旨在解决农业中杂草管理带来的重大挑战，该挑战会影响作物产量并消耗大量控制资源。

Method: WeedSense 采用一种新颖的多任务学习架构，结合了通用倒置瓶颈块的双路径编码器和基于 Transformer 的特征融合的多任务分叉解码器，以同时执行语义分割、高度估计和生长阶段分类。

Result: WeedSense 在多任务数据集上实现了 89.78% 的平均交并比（mIoU）用于分割，1.67cm 的平均绝对误差（MAE）用于高度估计，以及 99.99% 的生长阶段分类准确率。模型推理速度为 160 FPS。

Conclusion: WeedSense 在分割、高度估计和生长阶段分类方面均优于最先进的模型，同时保持实时推理速度。该模型实现了 mIoU 89.78%、平均绝对误差 1.67cm 和 99.99% 的准确率，并且比顺序单任务执行快 3 倍，参数减少 32.4%。

Abstract: Weed management represents a critical challenge in agriculture, significantly
impacting crop yields and requiring substantial resources for control.
Effective weed monitoring and analysis strategies are crucial for implementing
sustainable agricultural practices and site-specific management approaches. We
introduce WeedSense, a novel multi-task learning architecture for comprehensive
weed analysis that jointly performs semantic segmentation, height estimation,
and growth stage classification. We present a unique dataset capturing 16 weed
species over an 11-week growth cycle with pixel-level annotations, height
measurements, and temporal labels. WeedSense leverages a dual-path encoder
incorporating Universal Inverted Bottleneck blocks and a Multi-Task Bifurcated
Decoder with transformer-based feature fusion to generate multi-scale features
and enable simultaneous prediction across multiple tasks. WeedSense outperforms
other state-of-the-art models on our comprehensive evaluation. On our
multi-task dataset, WeedSense achieves mIoU of 89.78% for segmentation, 1.67cm
MAE for height estimation, and 99.99% accuracy for growth stage classification
while maintaining real-time inference at 160 FPS. Our multitask approach
achieves 3$\times$ faster inference than sequential single-task execution and
uses 32.4% fewer parameters. Please see our project page at
weedsense.github.io.

</details>


### [42] [SATURN: Autoregressive Image Generation Guided by Scene Graphs](https://arxiv.org/abs/2508.14502)
*Thanh-Nhan Vo,Trong-Thuan Nguyen,Tam V. Nguyen,Minh-Triet Tran*

Main category: cs.CV

TL;DR: SATURN是一种用于文本到图像生成的轻量级方法，它利用场景图来改进布局和对象关系，同时保持高保真度和速度。


<details>
  <summary>Details</summary>
Motivation: 尽管最先进的文本到图像模型在照片级真实感渲染方面表现出色，但它们往往难以捕捉复杂提示所暗示的布局和对象关系。场景图提供了自然结构先验，但以往的图引导方法通常依赖于沉重的GAN或扩散管道，在速度和保真度方面都落后于现代自回归架构。

Method: SATURN是一种轻量级扩展，适用于VAR-CLIP，它将场景图转换为一个优先排序的标记序列，使预先训练好的CLIP-VQ-VAE主干能够解释图结构，同时只对VAR transformer进行微调。

Result: SATURN在Visual Genome数据集上将FID从56.45%降低到21.62%，并将Inception Score从16.03提高到24.78，在不需要额外模块或多阶段训练的情况下，其性能优于SG2IM和SGDiff等先前方法。

Conclusion: SATURN通过将场景图转换为优先排序的标记序列，有效结合了结构感知和最先进的自回归保真度，并且在物体数量保真度和空间关系准确性方面取得了改进。

Abstract: State-of-the-art text-to-image models excel at photorealistic rendering but
often struggle to capture the layout and object relationships implied by
complex prompts. Scene graphs provide a natural structural prior, yet previous
graph-guided approaches have typically relied on heavy GAN or diffusion
pipelines, which lag behind modern autoregressive architectures in both speed
and fidelity. We introduce SATURN (Structured Arrangement of Triplets for
Unified Rendering Networks), a lightweight extension to VAR-CLIP that
translates a scene graph into a salience-ordered token sequence, enabling a
frozen CLIP-VQ-VAE backbone to interpret graph structure while fine-tuning only
the VAR transformer. On the Visual Genome dataset, SATURN reduces FID from
56.45% to 21.62% and increases the Inception Score from 16.03 to 24.78,
outperforming prior methods such as SG2IM and SGDiff without requiring extra
modules or multi-stage training. Qualitative results further confirm
improvements in object count fidelity and spatial relation accuracy, showing
that SATURN effectively combines structural awareness with state-of-the-art
autoregressive fidelity.

</details>


### [43] [PB-IAD: Utilizing multimodal foundation models for semantic industrial anomaly detection in dynamic manufacturing environments](https://arxiv.org/abs/2508.14504)
*Bernd Hofmann,Albert Scheck,Joerg Franke,Patrick Bruendl*

Main category: cs.CV

TL;DR: PB-IAD是一个利用基础模型（如GPT-4.1）进行工业异常检测的框架，它通过提示模板和预处理模块解决了数据稀疏、敏捷适应和领域用户中心化的问题，并在各种场景和与最先进方法（如PatchCore）的比较中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了确保产品质量和识别过程偏差，在制造过程中检测异常至关重要。然而，传统的统计和数据驱动方法在适应性和可用性方面受到限制，因为它们依赖于大量的标注数据，并且在动态生产条件下的灵活性有限。基础模型的感知能力为适应这一下游任务提供了机会。

Method: 提出了一种名为PB-IAD（Prompt-based Industrial Anomaly Detection）的新颖框架，该框架利用了基础模型的模态和推理能力来进行工业异常检测。该框架包含一个专门用于迭代实现领域特定过程知识的提示模板，以及一个将领域用户输入转换为有效系统提示的预处理模块。

Result: 使用GPT-4.1在三种不同的制造场景、两种数据模态以及一项消融研究中对PB-IAD进行了评估，系统地评估了语义指令的贡献。此外，PB-IAD与PatchCore等最先进的异常检测方法进行了基准测试。

Conclusion: PB-IAD框架在数据稀疏和少样本设置下表现出卓越的性能，且仅通过语义指令即可实现。

Abstract: The detection of anomalies in manufacturing processes is crucial to ensure
product quality and identify process deviations. Statistical and data-driven
approaches remain the standard in industrial anomaly detection, yet their
adaptability and usability are constrained by the dependence on extensive
annotated datasets and limited flexibility under dynamic production conditions.
Recent advances in the perception capabilities of foundation models provide
promising opportunities for their adaptation to this downstream task. This
paper presents PB-IAD (Prompt-based Industrial Anomaly Detection), a novel
framework that leverages the multimodal and reasoning capabilities of
foundation models for industrial anomaly detection. Specifically, PB-IAD
addresses three key requirements of dynamic production environments: data
sparsity, agile adaptability, and domain user centricity. In addition to the
anomaly detection, the framework includes a prompt template that is
specifically designed for iteratively implementing domain-specific process
knowledge, as well as a pre-processing module that translates domain user
inputs into effective system prompts. This user-centric design allows domain
experts to customise the system flexibly without requiring data science
expertise. The proposed framework is evaluated by utilizing GPT-4.1 across
three distinct manufacturing scenarios, two data modalities, and an ablation
study to systematically assess the contribution of semantic instructions.
Furthermore, PB-IAD is benchmarked to state-of-the-art methods for anomaly
detection such as PatchCore. The results demonstrate superior performance,
particularly in data-sparse scenarios and low-shot settings, achieved solely
through semantic instructions.

</details>


### [44] [Adversarial Generation and Collaborative Evolution of Safety-Critical Scenarios for Autonomous Vehicles](https://arxiv.org/abs/2508.14527)
*Jiangfan Liu,Yongkang Guo,Fangzhi Zhong,Tianyuan Zhang,Zonglei Jing,Siyuan Liang,Jiakai Wang,Mingchuan Zhang,Aishan Liu,Xianglong Liu*

Main category: cs.CV

TL;DR: ScenGE是一个生成安全关键场景的框架，通过引入新颖的对抗性案例并结合复杂的交通流来放大威胁，有效提高了自动驾驶车辆的安全评估能力和模型鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有自动驾驶车辆安全评估方法依赖预定义的威胁模式或基于规则的策略，难以暴露多样化和不可预见的故障模式。需要更有效的方法来生成安全关键场景。

Method: ScenGE框架包含元场景生成（利用大型语言模型推断具有挑战性的恶意行为者）和复杂场景演化（利用背景车辆和对手协作者图来放大核心威胁并减少自车机动空间）两个主要部分。

Result: ScenGE框架能生成更严重的碰撞案例（平均提高31.96%），可应用于不同模拟器和大型模型自动驾驶系统，并能通过对抗性训练提高模型鲁棒性。

Conclusion: ScenGE框架通过生成新颖的对抗性案例并结合复杂的交通流来生成丰富的安全关键场景，能够发现比现有方法更严重的碰撞案例，并提高自动驾驶车辆模型的鲁棒性，已通过真实世界车辆测试和人类评估得到验证。

Abstract: The generation of safety-critical scenarios in simulation has become
increasingly crucial for safety evaluation in autonomous vehicles prior to road
deployment in society. However, current approaches largely rely on predefined
threat patterns or rule-based strategies, which limit their ability to expose
diverse and unforeseen failure modes. To overcome these, we propose ScenGE, a
framework that can generate plentiful safety-critical scenarios by reasoning
novel adversarial cases and then amplifying them with complex traffic flows.
Given a simple prompt of a benign scene, it first performs Meta-Scenario
Generation, where a large language model, grounded in structured driving
knowledge, infers an adversarial agent whose behavior poses a threat that is
both plausible and deliberately challenging. This meta-scenario is then
specified in executable code for precise in-simulator control. Subsequently,
Complex Scenario Evolution uses background vehicles to amplify the core threat
introduced by Meta-Scenario. It builds an adversarial collaborator graph to
identify key agent trajectories for optimization. These perturbations are
designed to simultaneously reduce the ego vehicle's maneuvering space and
create critical occlusions. Extensive experiments conducted on multiple
reinforcement learning based AV models show that ScenGE uncovers more severe
collision cases (+31.96%) on average than SoTA baselines. Additionally, our
ScenGE can be applied to large model based AV systems and deployed on different
simulators; we further observe that adversarial training on our scenarios
improves the model robustness. Finally, we validate our framework through
real-world vehicle tests and human evaluation, confirming that the generated
scenarios are both plausible and critical. We hope our paper can build up a
critical step towards building public trust and ensuring their safe deployment.

</details>


### [45] [WISE-FUSE: Efficient Whole Slide Image Encoding via Coarse-to-Fine Patch Selection with VLM and LLM Knowledge Fusion](https://arxiv.org/abs/2508.14537)
*Yonghan Shin,SeungKyu Kim,Won-Ki Jeong*

Main category: cs.CV

TL;DR: WISE-FUSE通过利用病理学领域的视觉-语言模型和大型语言模型，选择性地处理有诊断意义的区域，解决了全切片图像（WSIs）在计算病理学（CPath）中因数据量巨大而带来的计算瓶颈问题，实现了更快的编码速度和相当的诊断精度。


<details>
  <summary>Details</summary>
Motivation: 计算病理学中全切片图像（WSIs）的千兆像素规模带来了巨大的计算挑战，需要对每张幻灯片处理成千上万个高分辨率补丁，导致了高昂的编码成本和过长的预处理及训练时间，使得WSI编码成为实际部署中最主要的瓶颈。

Method: WISE-FUSE框架首先利用知识蒸馏机制计算低分辨率补丁与类别特定文本描述之间的相似度得分，以保留细粒度的诊断特征。然后，基于这些相似度得分，为目标任务选择信息量大的小部分区域，从而在粗略层面快速排除无关补丁。最后，对相应的高分辨率补丁进行选择性编码，并与文本嵌入融合以加强诊断背景。

Result: 实验证明，WISE-FUSE将WSI编码时间缩短了三倍以上，同时诊断性能与详尽补丁处理相当或更优。

Conclusion: WISE-FUSE通过选择性处理诊断相关区域，将WSI编码时间缩短了三倍以上，同时在诊断性能上达到或超过了详尽的补丁处理水平，为计算病理学提供了可扩展且实用的解决方案。

Abstract: Whole slide images (WSIs) in computational pathology (CPath) pose a major
computational challenge due to their gigapixel scale, often requiring the
processing of tens to hundreds of thousands of high-resolution patches per
slide. This results in prohibitive encoding costs, with preprocessing and
training times extending to days or even weeks-making WSI encoding the most
significant bottleneck in real-world deployment. In this work, we propose
WISE-FUSE, an adaptive WSI encoding framework that leverages pathology-domain
vision-language models and large language models to address this challenge by
selectively processing diagnostically relevant regions. WISE-FUSE first
computes similarity scores between low-resolution patches and class-specific
textual descriptions using a knowledge distillation mechanism that preserves
fine-grained diagnostic features. Based on these similarity scores, we select a
small subset of informative regions for the target task, which quickly
eliminates irrelevant patches at the coarse level. The corresponding
high-resolution patches are then selectively encoded and fused with textual
embeddings to reinforce diagnostic context. Extensive experiments demonstrate
that WISE-FUSE reduces WSI encoding time by over threefold while achieving
diagnostic performance comparable to or surpassing that of exhaustive patch
processing, offering a scalable and practical solution for CPath.

</details>


### [46] [Improving OCR using internal document redundancy](https://arxiv.org/abs/2508.14557)
*Diego Belzarena,Seginus Mowlavi,Aitor Artola,Camilo Mariño,Marina Gardella,Ignacio Ramírez,Antoine Tadros,Roy He,Natalia Bottaioli,Boshra Rajaei,Gregory Randall,Jean-Michel Morel*

Main category: cs.CV

TL;DR: 提出了一种利用文档内字符形状冗余的无监督OCR改进方法，使用扩展的高斯混合模型来纠正错误输出，并在降级文档上展示了改进效果。


<details>
  <summary>Details</summary>
Motivation: 当前OCR系统在识别低质量数据时可能存在困难，尤其是在印刷文档中，其域内数据变异性低，但域间数据变异性高。现有OCR方法未能充分利用每个文档的冗余。

Method: 提出了一种无监督方法，利用文档中字符形状的冗余来纠正给定OCR系统的错误输出，并通过引入扩展的高斯混合模型（GMM），以期望最大化（EM）算法交替进行内部聚类调整过程和正态性统计检验。

Result: 改进了OCR系统在各种降级文档上的表现，包括恢复乌拉圭军事档案以及17世纪至20世纪中叶的欧洲报纸。

Conclusion: 该方法通过利用文档中字符形状的冗余来纠正给定OCR系统的错误输出来改进OCR

Abstract: Current OCR systems are based on deep learning models trained on large
amounts of data. Although they have shown some ability to generalize to unseen
data, especially in detection tasks, they can struggle with recognizing
low-quality data. This is particularly evident for printed documents, where
intra-domain data variability is typically low, but inter-domain data
variability is high. In that context, current OCR methods do not fully exploit
each document's redundancy. We propose an unsupervised method by leveraging the
redundancy of character shapes within a document to correct imperfect outputs
of a given OCR system and suggest better clustering. To this aim, we introduce
an extended Gaussian Mixture Model (GMM) by alternating an
Expectation-Maximization (EM) algorithm with an intra-cluster realignment
process and normality statistical testing. We demonstrate improvements in
documents with various levels of degradation, including recovered Uruguayan
military archives and 17th to mid-20th century European newspapers.

</details>


### [47] [A Comprehensive Review of Agricultural Parcel and Boundary Delineation from Remote Sensing Images: Recent Progress and Future Perspectives](https://arxiv.org/abs/2508.14558)
*Juepeng Zheng,Zi Ye,Yibin Wen,Jianxi Huang,Zhiwei Zhang,Qingmei Li,Qiong Hu,Baodong Xu,Lingyuan Zhao,Haohuan Fu*

Main category: cs.CV

TL;DR: 本综述对遥感影像中的农田地块和边界描绘（APBD）方法进行了全面的回顾和分析，重点关注深度学习方法，并对未来研究方向进行了展望。


<details>
  <summary>Details</summary>
Motivation: 为了提供一个清晰的现有APBD研究知识图谱，并帮助该领域的研究人员跟踪其发展和趋势。

Method: 通过对近期APBD论文进行元数据分析，对算法、研究地点、作物类型、传感器类型、评估方法等进行分类和总结。将方法分为传统图像处理、传统机器学习和深度学习三类，并重点讨论了深度学习方法。

Result: 对APBD方法进行了分类和深入讨论，包括传统方法和深度学习方法（如语义分割、目标检测、Transformer等）。此外，还讨论了多传感器数据、单任务与多任务学习、不同算法和任务的比较等五个APBD相关问题。

Conclusion: 本篇综述对遥感影像中的农田地块和边界描绘（APBD）进行了系统性回顾，对现有方法进行了分类和讨论，并对未来的研究方向进行了展望。

Abstract: Powered by advances in multiple remote sensing sensors, the production of
high spatial resolution images provides great potential to achieve
cost-efficient and high-accuracy agricultural inventory and analysis in an
automated way. Lots of studies that aim at providing an inventory of the level
of each agricultural parcel have generated many methods for Agricultural Parcel
and Boundary Delineation (APBD). This review covers APBD methods for detecting
and delineating agricultural parcels and systematically reviews the past and
present of APBD-related research applied to remote sensing images. With the
goal to provide a clear knowledge map of existing APBD efforts, we conduct a
comprehensive review of recent APBD papers to build a meta-data analysis,
including the algorithm, the study site, the crop type, the sensor type, the
evaluation method, etc. We categorize the methods into three classes: (1)
traditional image processing methods (including pixel-based, edge-based and
region-based); (2) traditional machine learning methods (such as random forest,
decision tree); and (3) deep learning-based methods. With deep
learning-oriented approaches contributing to a majority, we further discuss
deep learning-based methods like semantic segmentation-based, object
detection-based and Transformer-based methods. In addition, we discuss five
APBD-related issues to further comprehend the APBD domain using remote sensing
data, such as multi-sensor data in APBD task, comparisons between single-task
learning and multi-task learning in the APBD domain, comparisons among
different algorithms and different APBD tasks, etc. Finally, this review
proposes some APBD-related applications and a few exciting prospects and
potential hot topics in future APBD research. We hope this review help
researchers who involved in APBD domain to keep track of its development and
tendency.

</details>


### [48] [Locality-aware Concept Bottleneck Model](https://arxiv.org/abs/2508.14562)
*Sujin Jeon,Hyundo Lee,Eungseo Kim,Sanghack Lee,Byoung-Tak Zhang,Inwoo Hwang*

Main category: cs.CV

TL;DR: LCBM通过原型学习解决了标签-free CBMs中的概念定位问题，提高了概念识别的空间定位精度，同时保持了分类性能。


<details>
  <summary>Details</summary>
Motivation: 由于获得具有人类标签的密集概念注释要求高且成本高，因此最近的方法利用基础模型来确定图像中存在的概念。然而，这种无标签的CBMs通常无法将概念定位在相关区域，在预测概念存在时会关注视觉上无关的区域。

Method: LCBM利用来自基础模型的丰富信息，并采用原型学习来确保概念的准确空间定位。具体来说，我们为每个概念分配一个原型，以代表该概念的原型图像特征。通过鼓励这些原型编码相似的局部区域，并利用基础模型确保每个原型与其相关概念的相关性来学习这些原型。然后，我们使用这些原型来促进识别每个概念应被预测的适当局部区域的学习过程。

Result: 实验结果表明，LCBM有效地识别了图像中存在的概念，并提高了定位能力，同时保持了可比的分类性能。

Conclusion: LCBM有效地识别了图像中存在的概念，并提高了定位能力，同时保持了可比的分类性能。

Abstract: Concept bottleneck models (CBMs) are inherently interpretable models that
make predictions based on human-understandable visual cues, referred to as
concepts. As obtaining dense concept annotations with human labeling is
demanding and costly, recent approaches utilize foundation models to determine
the concepts existing in the images. However, such label-free CBMs often fail
to localize concepts in relevant regions, attending to visually unrelated
regions when predicting concept presence. To this end, we propose a framework,
coined Locality-aware Concept Bottleneck Model (LCBM), which utilizes rich
information from foundation models and adopts prototype learning to ensure
accurate spatial localization of the concepts. Specifically, we assign one
prototype to each concept, promoted to represent a prototypical image feature
of that concept. These prototypes are learned by encouraging them to encode
similar local regions, leveraging foundation models to assure the relevance of
each prototype to its associated concept. Then we use the prototypes to
facilitate the learning process of identifying the proper local region from
which each concept should be predicted. Experimental results demonstrate that
LCBM effectively identifies present concepts in the images and exhibits
improved localization while maintaining comparable classification performance.

</details>


### [49] [GOGS: High-Fidelity Geometry and Relighting for Glossy Objects via Gaussian Surfels](https://arxiv.org/abs/2508.14563)
*Xingyuan Yang,Min Wei*

Main category: cs.CV

TL;DR: GOGS是一个新颖的两阶段框架，通过结合物理渲染、几何先验和蒙特卡洛采样，解决了现有逆渲染方法在处理镜面反射和计算效率上的局限性，实现了高质量的几何重建、材质分离和光照重构。


<details>
  <summary>Details</summary>
Motivation: 现有的NeRF方法计算成本高昂，而3D高斯泼溅方法在处理镜面反射时存在局限性，导致多视图不一致、表面噪声、结构伪影以及不合理的重光照结果。为了解决这些问题，需要一种更有效且能处理镜面反射的方法。

Method: 提出了一种新颖的两阶段框架GOGS，基于2D高斯曲面元。第一阶段通过基于物理的渲染（使用分体求和近似）和基础模型的几何先验进行鲁棒的表面重建。第二阶段通过蒙特卡洛重要性采样完整渲染方程进行材质分解，并利用球形mipmap定向编码来建模间接光照和优化高频镜面细节。

Result: GOGS在几何重建、材质分离和光照重构方面取得了最先进的性能。

Conclusion: GOGS框架在几何重建、材质分离和光照重构方面取得了最先进的性能，优于现有的逆渲染方法。

Abstract: Inverse rendering of glossy objects from RGB imagery remains fundamentally
limited by inherent ambiguity. Although NeRF-based methods achieve
high-fidelity reconstruction via dense-ray sampling, their computational cost
is prohibitive. Recent 3D Gaussian Splatting achieves high reconstruction
efficiency but exhibits limitations under specular reflections. Multi-view
inconsistencies introduce high-frequency surface noise and structural
artifacts, while simplified rendering equations obscure material properties,
leading to implausible relighting results. To address these issues, we propose
GOGS, a novel two-stage framework based on 2D Gaussian surfels. First, we
establish robust surface reconstruction through physics-based rendering with
split-sum approximation, enhanced by geometric priors from foundation models.
Second, we perform material decomposition by leveraging Monte Carlo importance
sampling of the full rendering equation, modeling indirect illumination via
differentiable 2D Gaussian ray tracing and refining high-frequency specular
details through spherical mipmap-based directional encoding that captures
anisotropic highlights. Extensive experiments demonstrate state-of-the-art
performance in geometry reconstruction, material separation, and photorealistic
relighting under novel illuminations, outperforming existing inverse rendering
approaches.

</details>


### [50] [Safety-Critical Learning for Long-Tail Events: The TUM Traffic Accident Dataset](https://arxiv.org/abs/2508.14567)
*Walter Zimmer,Ross Greer,Xingcheng Zhou,Rui Song,Marc Pavel,Daniel Lehmberg,Ahmed Ghita,Akshay Gopalkrishnan,Mohan Trivedi,Alois Knoll*

Main category: cs.CV

TL;DR: 该研究发布了TUMTraf-A数据集，并提出了Accid3nD模型，用于检测交通事故。


<details>
  <summary>Details</summary>
Motivation: 尽管交通网络安全已有大量研究，事故仍时有发生，被视为交通网络的固有风险。因此，需要更好地理解和检测这些事故。

Method: 提出了一种名为Accid3nD的事故检测模型，该模型结合了基于规则和基于学习的方法。

Result: 创建并发布了TUMTraf-A数据集，包含真实世界的高速公路事故数据，并提出了Accid3nD模型，在数据集上进行了鲁棒性实验和消融研究。

Conclusion: 该研究提出了Accid3nD模型，通过结合基于规则和基于学习的方法来检测事故，并在TUMTraf-A数据集上进行了鲁棒性验证。

Abstract: Even though a significant amount of work has been done to increase the safety
of transportation networks, accidents still occur regularly. They must be
understood as an unavoidable and sporadic outcome of traffic networks. We
present the TUM Traffic Accident (TUMTraf-A) dataset, a collection of
real-world highway accidents. It contains ten sequences of vehicle crashes at
high-speed driving with 294,924 labeled 2D and 93,012 labeled 3D boxes and
track IDs within 48,144 labeled frames recorded from four roadside cameras and
LiDARs at 10 Hz. The dataset contains ten object classes and is provided in the
OpenLABEL format. We propose Accid3nD, an accident detection model that
combines a rule-based approach with a learning-based one. Experiments and
ablation studies on our dataset show the robustness of our proposed method. The
dataset, model, and code are available on our project website:
https://tum-traffic-dataset.github.io/tumtraf-a.

</details>


### [51] [Controllable Latent Space Augmentation for Digital Pathology](https://arxiv.org/abs/2508.14588)
*Sofiène Boutaj,Marin Scalbert,Pierre Marza,Florent Couzinie-Devy,Maria Vakalopoulou,Stergios Christodoulidis*

Main category: cs.CV

TL;DR: HistAug是一种用于数字病理学中WSI分析的生成模型，通过在潜在空间中进行可控的增强来提高MIL模型的性能，尤其是在数据稀疏的情况下。


<details>
  <summary>Details</summary>
Motivation: 数字病理学中的全切片图像（WSI）分析由于其巨大的分辨率和稀疏的监督信号而面临挑战。尽管数据增强技术可以提高模型鲁棒性，但传统方法成本高昂且缺乏语义控制。因此，需要一种有效的数据增强方法。

Method: HistAug是一种快速高效的生成模型，通过以显式的块级变换（如色调、侵蚀）作为条件，在潜在空间中生成可控的增强。该方法可以高效地一次处理大量图像块，同时保持初始的语义信息。

Result: 实验表明，HistAug在多个基于切片级别的任务和不同器官的数据集上都优于现有方法，尤其是在数据量较少的情况下。消融研究证实了学习到的变换优于基于噪声的扰动，并强调了均匀的WSI级别增强的重要性。

Conclusion: HistAug通过在潜在空间中进行可控的生成来提高数字病理学中MIL模型的性能，特别是在数据稀疏的情况下，并且优于现有方法。

Abstract: Whole slide image (WSI) analysis in digital pathology presents unique
challenges due to the gigapixel resolution of WSIs and the scarcity of dense
supervision signals. While Multiple Instance Learning (MIL) is a natural fit
for slide-level tasks, training robust models requires large and diverse
datasets. Even though image augmentation techniques could be utilized to
increase data variability and reduce overfitting, implementing them effectively
is not a trivial task. Traditional patch-level augmentation is prohibitively
expensive due to the large number of patches extracted from each WSI, and
existing feature-level augmentation methods lack control over transformation
semantics. We introduce HistAug, a fast and efficient generative model for
controllable augmentations in the latent space for digital pathology. By
conditioning on explicit patch-level transformations (e.g., hue, erosion),
HistAug generates realistic augmented embeddings while preserving initial
semantic information. Our method allows the processing of a large number of
patches in a single forward pass efficiently, while at the same time
consistently improving MIL model performance. Experiments across multiple
slide-level tasks and diverse organs show that HistAug outperforms existing
methods, particularly in low-data regimes. Ablation studies confirm the
benefits of learned transformations over noise-based perturbations and
highlight the importance of uniform WSI-wise augmentation. Code is available at
https://github.com/MICS-Lab/HistAug.

</details>


### [52] [Reliable Smoke Detection via Optical Flow-Guided Feature Fusion and Transformer-Based Uncertainty Modeling](https://arxiv.org/abs/2508.14597)
*Nitish Kumar Mahala,Muzammil Khan,Pushpendra Kumar*

Main category: cs.CV

TL;DR: 该研究提出了一种创新的烟雾检测框架，结合了图像特征和不确定性估计，提高了早期火灾预警的准确性和可靠性，尤其适用于复杂的现实场景。


<details>
  <summary>Details</summary>
Motivation: 火灾爆发对人类生命和基础设施构成严重威胁，需要高保真度的早期预警系统来检测烟雾等燃烧前兆。然而，烟雾羽流的时空动态复杂多变，受光照变化、流动动力学和环境噪声的影响，降低了传统检测器的可靠性。为了在不引入多传感器阵列的复杂性的情况下解决这些挑战，提出了一种信息融合框架。

Method: 提出了一种名为“双阶段不确定性感知移窗Transformer”的模型，该模型利用新构建的基于光流运动编码的烟雾分割数据集。光流估计采用受四色定理启发的双阶段水平集分数阶变分模型，以保留运动不连续性。然后，通过高斯混合模型将颜色编码的光流图与外观线索进行融合，生成烟雾区域的二值分割掩模。这些融合后的表示被输入到Shifted-Windows Transformer中，并通过多尺度不确定性估计头进行增强，在为期两阶段的学习方案下进行训练。第一学习阶段优化烟雾检测准确性，第二阶段模型通过联合建模偶然不确定性和认知不确定性来学习估计其预测的合理性置信度。

Result: 通过广泛的实验、多个评估指标以及与最先进方法的比较分析，证明了该框架具有优越的泛化性和鲁棒性，能够为监控、工业安全和自主监控应用提供可靠的早期火灾检测解决方案。

Conclusion: 所提出的信息融合框架通过整合来自单目图像的烟雾特征表示，能够稳健且可靠地进行烟雾检测，并通过多尺度不确定性估计来提高预测的可信度，在多个评估指标和与最先进方法的比较分析中展示了优越的泛化性和鲁棒性，为监控、工业安全和自主监控应用中的早期火灾检测提供了可靠的解决方案。

Abstract: Fire outbreaks pose critical threats to human life and infrastructure,
necessitating high-fidelity early-warning systems that detect combustion
precursors such as smoke. However, smoke plumes exhibit complex spatiotemporal
dynamics influenced by illumination variability, flow kinematics, and
environmental noise, undermining the reliability of traditional detectors. To
address these challenges without the logistical complexity of multi-sensor
arrays, we propose an information-fusion framework by integrating smoke feature
representations extracted from monocular imagery. Specifically, a Two-Phase
Uncertainty-Aware Shifted Windows Transformer for robust and reliable smoke
detection, leveraging a novel smoke segmentation dataset, constructed via
optical flow-based motion encoding, is proposed. The optical flow estimation is
performed with a four-color-theorem-inspired dual-phase level-set
fractional-order variational model, which preserves motion discontinuities. The
resulting color-encoded optical flow maps are fused with appearance cues via a
Gaussian Mixture Model to generate binary segmentation masks of the smoke
regions. These fused representations are fed into the novel Shifted-Windows
Transformer, which is augmented with a multi-scale uncertainty estimation head
and trained under a two-phase learning regimen. First learning phase optimizes
smoke detection accuracy, while during the second phase, the model learns to
estimate plausibility confidence in its predictions by jointly modeling
aleatoric and epistemic uncertainties. Extensive experiments using multiple
evaluation metrics and comparative analysis with state-of-the-art approaches
demonstrate superior generalization and robustness, offering a reliable
solution for early fire detection in surveillance, industrial safety, and
autonomous monitoring applications.

</details>


### [53] [Incremental Object Detection with Prompt-based Methods](https://arxiv.org/abs/2508.14599)
*Matthias Neuwirth-Trapp,Maarten Bieshaar,Danda Pani Paudel,Luc Van Gool*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Visual prompt-based methods have seen growing interest in incremental
learning (IL) for image classification. These approaches learn additional
embedding vectors while keeping the model frozen, making them efficient to
train. However, no prior work has applied such methods to incremental object
detection (IOD), leaving their generalizability unclear. In this paper, we
analyze three different prompt-based methods under a complex domain-incremental
learning setting. We additionally provide a wide range of reference baselines
for comparison. Empirically, we show that the prompt-based approaches we tested
underperform in this setting. However, a strong yet practical method, combining
visual prompts with replaying a small portion of previous data, achieves the
best results. Together with additional experiments on prompt length and
initialization, our findings offer valuable insights for advancing prompt-based
IL in IOD.

</details>


### [54] [UST-SSM: Unified Spatio-Temporal State Space Models for Point Cloud Video Modeling](https://arxiv.org/abs/2508.14604)
*Peiming Li,Ziyi Wang,Yulin Yuan,Hong Liu,Xiangming Meng,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 点云视频识别新方法UST-SSM，通过STSS、STSA和TIS解决了点云视频的时空无序性和信息缺失问题，在多个数据集上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 点云视频具有捕捉动态3D运动并减少光照和视角变化的优势，但其时空无序性阻碍了直接将其展开为1D序列的单向建模。现有模型（SSMs）虽然在序列建模方面表现良好，但未能有效处理点云视频的这一特性。

Method: 提出了一种统一时空状态空间模型（UST-SSM），通过空间-时间选择扫描（STSS）来处理无序点云，通过时空结构聚合（STSA）来弥补缺失的几何和运动细节，并通过时间交互采样（TIS）来增强时间依赖性。

Result: UST-SSM 能够有效利用空间和时间上遥远但相似的点，聚合时空特征并进行补偿，增强了精细的时间依赖性。

Conclusion:  UST-SSM 在 MSR-Action3D、NTU RGB+D 和 Synthia 4D 数据集上进行了实验验证，证明了其有效性。

Abstract: Point cloud videos capture dynamic 3D motion while reducing the effects of
lighting and viewpoint variations, making them highly effective for recognizing
subtle and continuous human actions. Although Selective State Space Models
(SSMs) have shown good performance in sequence modeling with linear complexity,
the spatio-temporal disorder of point cloud videos hinders their unidirectional
modeling when directly unfolding the point cloud video into a 1D sequence
through temporally sequential scanning. To address this challenge, we propose
the Unified Spatio-Temporal State Space Model (UST-SSM), which extends the
latest advancements in SSMs to point cloud videos. Specifically, we introduce
Spatial-Temporal Selection Scanning (STSS), which reorganizes unordered points
into semantic-aware sequences through prompt-guided clustering, thereby
enabling the effective utilization of points that are spatially and temporally
distant yet similar within the sequence. For missing 4D geometric and motion
details, Spatio-Temporal Structure Aggregation (STSA) aggregates
spatio-temporal features and compensates. To improve temporal interaction
within the sampled sequence, Temporal Interaction Sampling (TIS) enhances
fine-grained temporal dependencies through non-anchor frame utilization and
expanded receptive fields. Experimental results on the MSR-Action3D, NTU RGB+D,
and Synthia 4D datasets validate the effectiveness of our method. Our code is
available at https://github.com/wangzy01/UST-SSM.

</details>


### [55] [SMTrack: End-to-End Trained Spiking Neural Networks for Multi-Object Tracking in RGB Videos](https://arxiv.org/abs/2508.14607)
*Pengzhi Zhong,Xinzhe Wang,Dan Zeng,Qihua Zhou,Feixiang He,Shuiwang Li*

Main category: cs.CV

TL;DR: SMTrack是首个直接训练的深度SNN框架，用于在标准RGB视频上进行端到端的MOT。通过引入Asa-NWDLoss和TrackTrack身份模块，SMTrack在MOT任务上取得了与领先的ANN方法相当的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管SNN在低功耗计算方面有潜力，但其在视觉任务中的应用主要限于图像分类、目标检测和事件驱动跟踪。直接训练的SNN在复杂时间任务（如MOT）方面的潜力尚未得到充分探索，特别是对于传统的RGB视频流。

Method: SMTrack是一个直接训练的深度SNN框架，用于在标准RGB视频上进行端到端的MOT。它引入了自适应且尺度感知的归一化Wasserstein距离损失（Asa-NWDLoss）来处理不同尺度的对象，并通过TrackTrack身份模块来维持对象轨迹。

Result: SMTrack在MOT任务中取得了与领先的ANN方法相当的性能，并且通过Asa-NWDLoss提高了对小对象的检测和定位性能。

Conclusion: SMTrack在BEE24、MOT17、MOT20和DanceTrack等数据集上实现了与领先的ANN（人工神经网络）MOT方法相当的性能，从而在复杂场景中推动了基于SNN（脉冲神经网络）的鲁棒且准确的跟踪。

Abstract: Brain-inspired Spiking Neural Networks (SNNs) exhibit significant potential
for low-power computation, yet their application in visual tasks remains
largely confined to image classification, object detection, and event-based
tracking. In contrast, real-world vision systems still widely use conventional
RGB video streams, where the potential of directly-trained SNNs for complex
temporal tasks such as multi-object tracking (MOT) remains underexplored. To
address this challenge, we propose SMTrack-the first directly trained deep SNN
framework for end-to-end multi-object tracking on standard RGB videos. SMTrack
introduces an adaptive and scale-aware Normalized Wasserstein Distance loss
(Asa-NWDLoss) to improve detection and localization performance under varying
object scales and densities. Specifically, the method computes the average
object size within each training batch and dynamically adjusts the
normalization factor, thereby enhancing sensitivity to small objects. For the
association stage, we incorporate the TrackTrack identity module to maintain
robust and consistent object trajectories. Extensive evaluations on BEE24,
MOT17, MOT20, and DanceTrack show that SMTrack achieves performance on par with
leading ANN-based MOT methods, advancing robust and accurate SNN-based tracking
in complex scenarios.

</details>


### [56] [AnchorSync: Global Consistency Optimization for Long Video Editing](https://arxiv.org/abs/2508.14609)
*Zichi Liu,Yinggui Wang,Tao Wei,Chao Ma*

Main category: cs.CV

TL;DR: AnchorSync是一个新颖的、基于扩散的框架，通过分离稀疏关键帧编辑和中间帧平滑插值，实现了高质量的长时视频编辑。


<details>
  <summary>Details</summary>
Motivation: 解决长视频编辑中保持全局一致性和时间连贯性的挑战，以及现有方法在长序列中出现的结构漂移或时间伪影问题。

Method: AnchorSync框架将长视频编辑任务分解为稀疏关键帧编辑和中间帧平滑插值两个子任务，通过渐进式去噪过程强制执行结构一致性，并通过多模态引导保留时间动态。

Result: AnchorSync在长视频编辑任务中表现出色，能够生成高质量、长时序的视频编辑内容，并且在视觉质量和时间稳定性方面优于现有方法。

Conclusion: AnchorSync能够生成连贯、高保真的视频编辑内容，在视觉质量和时间稳定性方面优于现有方法。

Abstract: Editing long videos remains a challenging task due to the need for
maintaining both global consistency and temporal coherence across thousands of
frames. Existing methods often suffer from structural drift or temporal
artifacts, particularly in minute-long sequences. We introduce AnchorSync, a
novel diffusion-based framework that enables high-quality, long-term video
editing by decoupling the task into sparse anchor frame editing and smooth
intermediate frame interpolation. Our approach enforces structural consistency
through a progressive denoising process and preserves temporal dynamics via
multimodal guidance. Extensive experiments show that AnchorSync produces
coherent, high-fidelity edits, surpassing prior methods in visual quality and
temporal stability.

</details>


### [57] [Towards PerSense++: Advancing Training-Free Personalized Instance Segmentation in Dense Images](https://arxiv.org/abs/2508.14660)
*Muhammad Ibraheem Siddiqui,Muhammad Umer Sheikh,Hassan Abid,Kevin Henry,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: PerSense 框架通过 IDM 和 PPSM 模块，利用密度图进行个性化实例分割，PerSense++ 进一步提升了在密集场景下的鲁棒性和性能，并推出了专门的基准测试集。


<details>
  <summary>Details</summary>
Motivation: 解决密集视觉场景中由于遮挡、背景混乱和尺度变化导致的分割挑战。

Method: PerSense 框架包括实例检测模块（IDM）和点提示选择模块（PPSM），通过密度图（DMs）生成候选点提示，并进行过滤和优化。PerSense++ 增加了多样性感知示例选择策略、结合轮廓和峰值提示的混合 IDM 以及不相关的掩码拒绝模块（IMRM）。

Result: PerSense++ 在密集场景下相比现有方法具有更优越的性能，并引入了 PerSense-D 基准测试集。

Conclusion: PerSense++ 在密集场景的个性化分割任务中表现优于现有方法。

Abstract: Segmentation in dense visual scenes poses significant challenges due to
occlusions, background clutter, and scale variations. To address this, we
introduce PerSense, an end-to-end, training-free, and model-agnostic one-shot
framework for Personalized instance Segmentation in dense images. PerSense
employs a novel Instance Detection Module (IDM) that leverages density maps
(DMs) to generate instance-level candidate point prompts, followed by a Point
Prompt Selection Module (PPSM) that filters false positives via adaptive
thresholding and spatial gating. A feedback mechanism further enhances
segmentation by automatically selecting effective exemplars to improve DM
quality. We additionally present PerSense++, an enhanced variant that
incorporates three additional components to improve robustness in cluttered
scenes: (i) a diversity-aware exemplar selection strategy that leverages
feature and scale diversity for better DM generation; (ii) a hybrid IDM
combining contour and peak-based prompt generation for improved instance
separation within complex density patterns; and (iii) an Irrelevant Mask
Rejection Module (IMRM) that discards spatially inconsistent masks using
outlier analysis. Finally, to support this underexplored task, we introduce
PerSense-D, a dedicated benchmark for personalized segmentation in dense
images. Extensive experiments across multiple benchmarks demonstrate that
PerSense++ outperforms existing methods in dense settings.

</details>


### [58] [GeMS: Efficient Gaussian Splatting for Extreme Motion Blur](https://arxiv.org/abs/2508.14682)
*Gopi Raju Matta,Trisha Reddypalli,Vemunuri Divya Madhuri,Kaushik Mitra*

Main category: cs.CV

TL;DR: 本研究提出了GeMS，一个可以直接从严重运动模糊图像重建3D场景的3D高斯喷溅框架。它集成了VGGSfM进行姿态估计和点云生成，3DGS-MCMC进行场景初始化，以及联合优化。增强版本GeMS-E进一步利用事件数据进行去模糊和细化，以提高重建质量。GeMS和GeMS-E在处理极端运动模糊方面取得了最先进的效果。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯喷溅（3DGS）方法和去模糊方法（如ExBluRF、Deblur-GS、BAD-Gaussians）在处理严重运动模糊的图像时存在局限性，它们通常假设可以获得清晰图像用于相机姿态估计和点云生成，或者在模糊图像中由于不可靠的特征对应而失败。本研究旨在提出一个能够直接从严重运动模糊图像中进行3D场景重建的框架。

Method: GeMS框架包含三个主要组成部分：1) VGGSfM，一个基于深度学习的运动恢复结构（SfM）流程，可以直接从模糊输入估计姿态和生成点云；2) 3DGS-MCMC，通过将高斯视为概率分布的采样来实现鲁棒的场景初始化，无需启发式稠密化和剪枝；3) 联合优化相机轨迹和高斯参数以实现稳定的重建。GeMS-E在此基础上增加了事件驱动的渐进式细化步骤，即EDI去模糊，以恢复更清晰的图像。

Result: GeMS和GeMS-E在合成和真实世界的数据集上都达到了最先进的性能，能够直接从严重模糊的输入中进行3D高斯喷溅。

Conclusion: GeMS及其增强版本GeMS-E在处理严重运动模糊的3D高斯喷溅（3DGS）方面取得了最先进的性能，能够直接从严重模糊的输入中进行场景重建，解决了现有方法依赖于清晰图像进行相机姿态估计和点云生成的局限性。GeMS-E通过整合基于事件的双重积分（EDI）去模糊技术，进一步提高了重建质量。

Abstract: We introduce GeMS, a framework for 3D Gaussian Splatting (3DGS) designed to
handle severely motion-blurred images. State-of-the-art deblurring methods for
extreme blur, such as ExBluRF, as well as Gaussian Splatting-based approaches
like Deblur-GS, typically assume access to sharp images for camera pose
estimation and point cloud generation, an unrealistic assumption. Methods
relying on COLMAP initialization, such as BAD-Gaussians, also fail due to
unreliable feature correspondences under severe blur. To address these
challenges, we propose GeMS, a 3DGS framework that reconstructs scenes directly
from extremely blurred images. GeMS integrates: (1) VGGSfM, a deep
learning-based Structure-from-Motion pipeline that estimates poses and
generates point clouds directly from blurred inputs; (2) 3DGS-MCMC, which
enables robust scene initialization by treating Gaussians as samples from a
probability distribution, eliminating heuristic densification and pruning; and
(3) joint optimization of camera trajectories and Gaussian parameters for
stable reconstruction. While this pipeline produces strong results,
inaccuracies may remain when all inputs are severely blurred. To mitigate this,
we propose GeMS-E, which integrates a progressive refinement step using events:
(4) Event-based Double Integral (EDI) deblurring restores sharper images that
are then fed into GeMS, improving pose estimation, point cloud generation, and
overall reconstruction. Both GeMS and GeMS-E achieve state-of-the-art
performance on synthetic and real-world datasets. To our knowledge, this is the
first framework to address extreme motion blur within 3DGS directly from
severely blurred inputs.

</details>


### [59] [Seeing Further on the Shoulders of Giants: Knowledge Inheritance for Vision Foundation Models](https://arxiv.org/abs/2508.14707)
*Jiabo Huang,Chen Chen,Lingjuan Lyu*

Main category: cs.CV

TL;DR: 通过融合多个预训练模型，在不依赖大量标签数据的情况下，构建了一个强大的视觉基础模型（VFM），并在多个视觉任务上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉基础模型（VFMs）主要依赖于数据驱动的方法，需要大量高质量的标签数据和高端GPU，这对大多数机构来说是一个挑战。然而，许多开源的预训练模型虽然包含了领域特定的知识，但并未得到充分利用来构建通用的VFMs。

Method: 本文提出了一种新的模型驱动方法，通过联合知识转移和保存来训练视觉基础模型（VFMs）。该方法将多个预训练的教师模型统一到共享的潜在空间中，以缓解由其分布差异引起的问题。此外，本文还引入了一种知识保存策略，利用一个通用的教师模型作为知识库，通过一个适配器模块整合来自其他特定任务的教师模型的知识。

Result: 该模型不仅提供了可泛化的视觉特征，还内置支持多种下游任务。

Conclusion: 该模型能够继承教师模型的专业知识，并且无需大量标记数据即可进行训练。实验结果表明，该模型在图像分类、目标检测、语义分割和实例分割四个基本视觉任务上优于现有的以数据为中心的方法。

Abstract: Vision foundation models (VFMs) are predominantly developed using
data-centric methods. These methods require training on vast amounts of data
usually with high-quality labels, which poses a bottleneck for most
institutions that lack both large-scale data and high-end GPUs. On the other
hand, many open-source vision models have been pretrained on domain-specific
data, enabling them to distill and represent core knowledge in a form that is
transferable across diverse applications. Even though these models are highly
valuable assets, they remain largely under-explored in empowering the
development of a general-purpose VFM. In this paper, we presents a new
model-driven approach for training VFMs through joint knowledge transfer and
preservation. Our method unifies multiple pre-trained teacher models in a
shared latent space to mitigate the ``imbalanced transfer'' issue caused by
their distributional gaps. Besides, we introduce a knowledge preservation
strategy to take a general-purpose teacher as a knowledge base for integrating
knowledge from the remaining purpose-specific teachers using an adapter module.
By unifying and aggregating existing models, we build a powerful VFM to inherit
teachers' expertise without needing to train on a large amount of labeled data.
Our model not only provides generalizable visual features, but also inherently
supports multiple downstream tasks. Extensive experiments demonstrate that our
VFM outperforms existing data-centric models across four fundamental vision
tasks, including image classification, object detection, semantic and instance
segmentation.

</details>


### [60] [GSFix3D: Diffusion-Guided Repair of Novel Views in Gaussian Splatting](https://arxiv.org/abs/2508.14717)
*Jiaxin Wei,Stefan Leutenegger,Simon Schaefer*

Main category: cs.CV

TL;DR: GSFix3D通过融合3D高斯样元和扩散模型，提高了3D场景在新视图和部分缺失区域的渲染质量，达到了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D高斯样元技术在从极端新视图或部分观察区域生成高质量渲染方面仍然存在挑战。同时，扩散模型虽然具有强大的生成能力，但缺乏对特定场景信息的感知，这限制了其在3D重建任务中的应用。为了解决这些问题，需要一个能够结合两者优点并提高3D重建质量的框架。

Method: GSFix3D是一个新颖的框架，它利用GSFixer（一个自定义微调协议获得的潜在扩散模型）来增强3D高斯样元在极端新视图或部分观察区域的渲染质量。GSFixer能够同时处理网格和3D高斯，并适应各种环境和伪影类型。此外，还提出了一种随机掩码增强策略，用于修复缺失区域。

Result: GSFix3D和GSFixer在具有挑战性的基准测试中取得了最先进的性能，并且只需要最少量的场景特定微调。实验还证实了其对潜在姿态错误的鲁韧性。

Conclusion: GSFix3D框架通过将扩散模型的先验知识提炼到3D表示中，并保留与观察到的场景细节的一致性，从而提高了在约束不足区域的视觉保真度。GSFixer（一个潜在扩散模型）能够利用网格和3D高斯来适应预训练的生成模型，以适应各种环境和来自不同重建方法的伪影类型，从而实现对未见过的相机位姿的鲁棒的新视图修复。此外，提出的随机掩码增强策略使GSFixer能够合理地修复缺失的区域。GSFix3D和GSFixer在具有挑战性的基准测试中取得了最先进的性能，并且在捕获的数据上只需要最少量的场景特定微调。实验还证实了其对潜在姿态错误的鲁韧性。

Abstract: Recent developments in 3D Gaussian Splatting have significantly enhanced
novel view synthesis, yet generating high-quality renderings from extreme novel
viewpoints or partially observed regions remains challenging. Meanwhile,
diffusion models exhibit strong generative capabilities, but their reliance on
text prompts and lack of awareness of specific scene information hinder
accurate 3D reconstruction tasks. To address these limitations, we introduce
GSFix3D, a novel framework that improves the visual fidelity in
under-constrained regions by distilling prior knowledge from diffusion models
into 3D representations, while preserving consistency with observed scene
details. At its core is GSFixer, a latent diffusion model obtained via our
customized fine-tuning protocol that can leverage both mesh and 3D Gaussians to
adapt pretrained generative models to a variety of environments and artifact
types from different reconstruction methods, enabling robust novel view repair
for unseen camera poses. Moreover, we propose a random mask augmentation
strategy that empowers GSFixer to plausibly inpaint missing regions.
Experiments on challenging benchmarks demonstrate that our GSFix3D and GSFixer
achieve state-of-the-art performance, requiring only minimal scene-specific
fine-tuning on captured data. Real-world test further confirms its resilience
to potential pose errors. Our code and data will be made publicly available.
Project page: https://gsfix3d.github.io.

</details>


### [61] [Multiscale Video Transformers for Class Agnostic Segmentation in Autonomous Driving](https://arxiv.org/abs/2508.14729)
*Leila Cheshmi,Mennatullah Siam*

Main category: cs.CV

TL;DR: 开发了一种仅使用运动线索的多尺度视频 Transformer，用于检测未知物体，实现了高效的类不可知分割，无需光流，并在多个基准测试中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 为了解决自动驾驶中处理未知物体和不可预见驾驶场景的安全挑战，以及现有视频语义和全景分割方法在处理新类别时的局限性，并降低视觉基础大语言模型在像素级输出时的计算成本。

Method: 提出了一种高效的视频 Transformer，采用多阶段多尺度查询-内存解码和特定尺度的随机丢弃标记，实现了无需光流的端到端类不可知分割。

Result: 在DAVIS'16、KITTI和Cityscapes数据集上进行了评估，结果一致优于多尺度基线方法，同时在GPU内存和运行时间方面效率更高。

Conclusion: 该方法为实时、鲁棒的自动驾驶安全关键应用提供了有前景的实时密集预测方向。

Abstract: Ensuring safety in autonomous driving is a complex challenge requiring
handling unknown objects and unforeseen driving scenarios. We develop
multiscale video transformers capable of detecting unknown objects using only
motion cues. Video semantic and panoptic segmentation often relies on known
classes seen during training, overlooking novel categories. Recent visual
grounding with large language models is computationally expensive, especially
for pixel-level output. We propose an efficient video transformer trained
end-to-end for class-agnostic segmentation without optical flow. Our method
uses multi-stage multiscale query-memory decoding and a scale-specific random
drop-token to ensure efficiency and accuracy, maintaining detailed
spatiotemporal features with a shared, learnable memory module. Unlike
conventional decoders that compress features, our memory-centric design
preserves high-resolution information at multiple scales. We evaluate on
DAVIS'16, KITTI, and Cityscapes. Our method consistently outperforms multiscale
baselines while being efficient in GPU memory and run-time, demonstrating a
promising direction for real-time, robust dense prediction in safety-critical
robotics.

</details>


### [62] [Improved Mapping Between Illuminations and Sensors for RAW Images](https://arxiv.org/abs/2508.14730)
*Abhijith Punnappurath,Luxi Zhao,Hoang Le,Abdelrahman Abdelhamed,SaiKiran Kumar Tedla,Michael S. Brown*

Main category: cs.CV

TL;DR: 该研究提出了一种新的数据集和轻量级神经网路方法，用于解决RAW图像的光照和传感器依赖性问题，并在实践中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了应对RAW图像固有的传感器和光照依赖性，以及深度学习方法捕捉数据集的挑战，研究旨在实现光照增强和跨传感器映射，以减轻数据采集负担。

Method: 提出了一种轻量级神经网路方法，用于光照和传感器映射，并使用包含390种光照、四种摄像头和18个场景的数据集进行训练和评估。

Result: 所提出的轻量级神经网路方法在光照和传感器映射任务上优于现有方法，并在训练神经ISP的下游任务中展示了其实用性。

Conclusion: 该研究通过引入一个包含390种光照、四种摄像头和18个场景的数据集，并提出了一种轻量级神经网路方法，实现了光照和传感器映射，并在下游任务的神经ISP训练中展现了优越性能。

Abstract: RAW images are unprocessed camera sensor output with sensor-specific RGB
values based on the sensor's color filter spectral sensitivities. RAW images
also incur strong color casts due to the sensor's response to the spectral
properties of scene illumination. The sensor- and illumination-specific nature
of RAW images makes it challenging to capture RAW datasets for deep learning
methods, as scenes need to be captured for each sensor and under a wide range
of illumination. Methods for illumination augmentation for a given sensor and
the ability to map RAW images between sensors are important for reducing the
burden of data capture. To explore this problem, we introduce the
first-of-its-kind dataset comprising carefully captured scenes under a wide
range of illumination. Specifically, we use a customized lightbox with tunable
illumination spectra to capture several scenes with different cameras. Our
illumination and sensor mapping dataset has 390 illuminations, four cameras,
and 18 scenes. Using this dataset, we introduce a lightweight neural network
approach for illumination and sensor mapping that outperforms competing
methods. We demonstrate the utility of our approach on the downstream task of
training a neural ISP. Link to project page:
https://github.com/SamsungLabs/illum-sensor-mapping.

</details>


### [63] [6-DoF Object Tracking with Event-based Optical Flow and Frames](https://arxiv.org/abs/2508.14776)
*Zhichao Li,Arren Glover,Chiara Bartolozzi,Lorenzo Natale*

Main category: cs.CV

TL;DR: 通过结合事件相机（用于高速运动测量）和RGB相机（用于姿态估计），提出了一种新的6-DoF物体姿态跟踪方法，在高速运动时效果显著。


<details>
  <summary>Details</summary>
Motivation: 高速运动的物体由于传统相机的帧率限制和运动模糊，其位置和方向（6-DoF）的实时跟踪变得更具挑战性。事件相机具有高时间分辨率、低延迟和高动态范围等特点，可以克服运动模糊的影响。而RGB相机提供丰富的视觉信息，更适合单次曝光物体姿态估计。因此，有必要结合两者的优势来解决高速运动物体姿态跟踪问题。

Method: 提出了一种事件驱动的光流算法来测量物体运动，并将其与基于RGB的全局物体姿态估计器相结合，以实现高速运动物体的6-DoF姿态跟踪。具体来说，通过整合跟踪到的物体6-DoF速度和低频估计姿态，该方法能够应对高速运动。

Result: 该算法在合成数据和真实世界数据上进行了测试和验证，证明了其有效性，尤其在高速运动场景下表现出色。

Conclusion: 该方法通过结合事件相机和RGB相机，利用事件驱动的光流和全局姿态估计器，在高速运动场景下实现了有效的6-DoF物体姿态跟踪。

Abstract: Tracking the position and orientation of objects in space (i.e., in 6-DoF) in
real time is a fundamental problem in robotics for environment interaction. It
becomes more challenging when objects move at high-speed due to frame rate
limitations in conventional cameras and motion blur. Event cameras are
characterized by high temporal resolution, low latency and high dynamic range,
that can potentially overcome the impacts of motion blur. Traditional RGB
cameras provide rich visual information that is more suitable for the
challenging task of single-shot object pose estimation. In this work, we
propose using event-based optical flow combined with an RGB based global object
pose estimator for 6-DoF pose tracking of objects at high-speed, exploiting the
core advantages of both types of vision sensors. Specifically, we propose an
event-based optical flow algorithm for object motion measurement to implement
an object 6-DoF velocity tracker. By integrating the tracked object 6-DoF
velocity with low frequency estimated pose from the global pose estimator, the
method can track pose when objects move at high-speed. The proposed algorithm
is tested and validated on both synthetic and real world data, demonstrating
its effectiveness, especially in high-speed motion scenarios.

</details>


### [64] [Adversarial Hospital-Invariant Feature Learning for WSI Patch Classification](https://arxiv.org/abs/2508.14779)
*Mengliang Zhang,Jacob M. Luber*

Main category: cs.CV

TL;DR: 该研究通过一种新的对抗性方法解决了病理基础模型中的医院域偏差问题，有效提高了模型在不同医院数据上的泛化能力和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了解决病理基础模型（PFMs）在临床部署中可能因不同医院的扫描硬件和预处理风格差异而学到医院特定特征，导致域偏差的风险。

Method: 通过构建量化域偏差的流程、评估模型性能，并提出了一种轻量级对抗框架，该框架利用可训练适配器和梯度反转层（GRL）连接的域分类器，在不修改编码器的情况下移除潜在的医院特定特征，学习任务区分性但域不变的表示。

Result: 所提出的方法显著降低了域可预测性，同时维持甚至提高了疾病分类性能，尤其是在未见过医院的域外场景中。进一步的分析确认了该方法在减轻医院偏见方面的有效性。

Conclusion: 该研究提出了首个系统性研究病理基础模型（PFMs）中源自医院特征的域偏差问题，并通过实验验证了其有效性。

Abstract: Pathology foundation models (PFMs) have demonstrated remarkable potential in
whole-slide image (WSI) diagnosis. However, pathology images from different
hospitals often vary due to differences in scanning hardware and preprocessing
styles, which may lead PFMs to inadvertently learn hospital-specific features,
posing risks for clinical deployment. In this work, we present the first
systematic study of domain bias in PFMs arising from hospital source
characteristics. Specifically, we (1) construct a pipeline for quantifying
domain bias in PFMs, (2) evaluate and compare the performance of multiple
models, and (3) propose a lightweight adversarial framework that removes latent
hospital-specific features from frozen representations without modifying the
encoder itself. By introducing a trainable adapter and a domain classifier
connected through a gradient reversal layer (GRL), our method learns
task-discriminative yet domain-invariant representations. Experiments on
multi-center histopathology datasets demonstrate that our approach
substantially reduces domain predictability while maintaining or even improving
disease classification performance, particularly in out-of-domain (unseen
hospital) scenarios. Further analyses, including hospital detection and feature
space visualization, confirm the effectiveness of our method in mitigating
hospital bias. We will provide our code based on acceptance.

</details>


### [65] [MF-LPR$^2$: Multi-Frame License Plate Image Restoration and Recognition using Optical Flow](https://arxiv.org/abs/2508.14797)
*Kihyun Na,Junseok Oh,Youngkwan Cho,Bumjin Kim,Sungmin Cho,Jinyoung Choi,Injung Kim*

Main category: cs.CV

TL;DR: 提出了一种新颖的多帧车牌恢复和识别框架MF-LPR$^2$，通过对相邻帧进行对齐和聚合来解决低质量车牌图像的恢复和识别问题，无需依赖预训练知识，并在实验中取得了显著优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型在恢复低质量车牌图像时，由于依赖预训练先验知识，常常会引入严重的伪影和失真，无法可靠地恢复低分辨率、运动模糊和眩光等问题。为解决此问题，提出一种新的多帧车牌恢复和识别框架。

Method: 提出了一种新颖的多帧车牌恢复和识别框架MF-LPR$^2$，通过最先进的光流估计器结合精心设计的算法来检测和纠正错误的光流估计，以实现准确的帧对齐。该方法通过利用车牌图像序列中固有的时空一致性来解决低质量图像中的模糊问题。

Result: MF-LPR$^2$在PSNR、SSIM和LPIPS方面显著优于八种现有的恢复模型，识别准确率达到86.44%，优于最佳单帧LPR（14.04%）和多帧LPR（82.55%）。消融研究结果证实了过滤和精炼算法对性能提升的贡献。

Conclusion: MF-LPR$^2$通过对相邻帧进行对齐和聚合来解决低质量图像中的模糊问题，无需依赖预训练知识。该方法在图像质量和识别准确性方面均有所提升，并保留了输入图像的证据内容。实验结果表明，MF-LPR$^2$在PSNR、SSIM和LPIPS方面显著优于八种现有的恢复模型，在识别准确率方面达到了86.44%，远超基线模型。

Abstract: License plate recognition (LPR) is important for traffic law enforcement,
crime investigation, and surveillance. However, license plate areas in dash cam
images often suffer from low resolution, motion blur, and glare, which make
accurate recognition challenging. Existing generative models that rely on
pretrained priors cannot reliably restore such poor-quality images, frequently
introducing severe artifacts and distortions. To address this issue, we propose
a novel multi-frame license plate restoration and recognition framework,
MF-LPR$^2$, which addresses ambiguities in poor-quality images by aligning and
aggregating neighboring frames instead of relying on pretrained knowledge. To
achieve accurate frame alignment, we employ a state-of-the-art optical flow
estimator in conjunction with carefully designed algorithms that detect and
correct erroneous optical flow estimations by leveraging the spatio-temporal
consistency inherent in license plate image sequences. Our approach enhances
both image quality and recognition accuracy while preserving the evidential
content of the input images. In addition, we constructed a novel Realistic LPR
(RLPR) dataset to evaluate MF-LPR$^2$. The RLPR dataset contains 200 pairs of
low-quality license plate image sequences and high-quality pseudo ground-truth
images, reflecting the complexities of real-world scenarios. In experiments,
MF-LPR$^2$ outperformed eight recent restoration models in terms of PSNR, SSIM,
and LPIPS by significant margins. In recognition, MF-LPR$^2$ achieved an
accuracy of 86.44%, outperforming both the best single-frame LPR (14.04%) and
the multi-frame LPR (82.55%) among the eleven baseline models. The results of
ablation studies confirm that our filtering and refinement algorithms
significantly contribute to these improvements.

</details>


### [66] [DINOv3 with Test-Time Training for Medical Image Registration](https://arxiv.org/abs/2508.14809)
*Shansong Wang,Mojtaba Safari,Mingzhe Hu,Qiang Li,Chih-Wei Chang,Richard LJ Qiu,Xiaofeng Yang*

Main category: cs.CV

TL;DR: 一种无需训练的医学图像配准方法，利用冻结的 DINOv3 编码器和特征空间中的测试时优化，在精度和正则化方面均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有医学图像配准方法（尤其是基于学习的方法）需要大量训练数据，限制了临床应用的问题。

Method: 提出了一种无需训练的管线，该管线依赖于冻结的 DINOv3 编码器和特征空间中的形变场测试时优化。

Result: 在 Abdomen MR-CT 和 ACDC 心脏 MRI 基准测试中，该方法均表现出高精度和良好的正则化形变。在 Abdomen MR-CT 上，DSC 达到 0.790，HD95 和 SDLogJ 均为最低。在 ACDC 心脏 MRI 上，DSC 提高到 0.769，SDLogJ 和 HD95 均有所降低。

Conclusion: 在紧凑的、冻结的特征空间中进行测试时优化，为临床配准提供了一种无需额外训练的实用且通用的解决方案。

Abstract: Prior medical image registration approaches, particularly learning-based
methods, often require large amounts of training data, which constrains
clinical adoption. To overcome this limitation, we propose a training-free
pipeline that relies on a frozen DINOv3 encoder and test-time optimization of
the deformation field in feature space. Across two representative benchmarks,
the method is accurate and yields regular deformations. On Abdomen MR-CT, it
attained the best mean Dice score (DSC) of 0.790 together with the lowest 95th
percentile Hausdorff Distance (HD95) of 4.9+-5.0 and the lowest standard
deviation of Log-Jacobian (SDLogJ) of 0.08+-0.02. On ACDC cardiac MRI, it
improves mean DSC to 0.769 and reduces SDLogJ to 0.11 and HD95 to 4.8, a marked
gain over the initial alignment. The results indicate that operating in a
compact foundation feature space at test time offers a practical and general
solution for clinical registration without additional training.

</details>


### [67] [Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From Sparse Inputs without Per-Scene Optimization](https://arxiv.org/abs/2508.14811)
*Canyu Zhao,Xiaoman Li,Tianjian Feng,Zhiyue Zhao,Hao Chen,Chunhua Shen*

Main category: cs.CV

TL;DR: Tinker是一个通用的三维编辑框架，无需任何场景微调，就能从一到两张图像实现高保真、多视角一致性编辑。它利用预训练的扩散模型，并包含一个参考多视角编辑器和一个任意视角到视频合成器，大大降低了三维内容创作的门槛。


<details>
  <summary>Details</summary>
Motivation: 现有的三维编辑技术通常需要大量的每场景优化来确保多视角一致性或生成大量一致的编辑输入视图，这限制了三维编辑的便捷性和可扩展性。本文旨在开发一种无需每场景微调即可进行高保真三维编辑的通用框架。

Method: Tinker框架通过重新利用预训练的扩散模型，利用其潜在的三维感知能力，实现了无需每场景微调即可进行高保真三维编辑。其核心组件包括：1. 参考多视角编辑器，用于实现精确、由参考驱动且在所有视点上保持一致的编辑。2. 任意视角到视频合成器，利用视频扩散的空间时间先验，即使在输入稀疏的情况下也能进行高质量场景补全和新视角生成。

Result: Tinker框架能够从一到两张图像开始，提供稳健的多视角一致性编辑，并且在编辑、新视角合成和渲染增强任务上取得了最先进的性能。

Conclusion: Tinker框架显著降低了可推广的三维内容创作门槛，在编辑、新视角合成和渲染增强任务上取得了最先进的性能，是实现真正可扩展、零样本三维编辑的关键一步。

Abstract: We introduce Tinker, a versatile framework for high-fidelity 3D editing that
operates in both one-shot and few-shot regimes without any per-scene
finetuning. Unlike prior techniques that demand extensive per-scene
optimization to ensure multi-view consistency or to produce dozens of
consistent edited input views, Tinker delivers robust, multi-view consistent
edits from as few as one or two images. This capability stems from repurposing
pretrained diffusion models, which unlocks their latent 3D awareness. To drive
research in this space, we curate the first large-scale multi-view editing
dataset and data pipeline, spanning diverse scenes and styles. Building on this
dataset, we develop our framework capable of generating multi-view consistent
edited views without per-scene training, which consists of two novel
components: (1) Referring multi-view editor: Enables precise, reference-driven
edits that remain coherent across all viewpoints. (2) Any-view-to-video
synthesizer: Leverages spatial-temporal priors from video diffusion to perform
high-quality scene completion and novel-view generation even from sparse
inputs. Through extensive experiments, Tinker significantly reduces the barrier
to generalizable 3D content creation, achieving state-of-the-art performance on
editing, novel-view synthesis, and rendering enhancement tasks. We believe that
Tinker represents a key step towards truly scalable, zero-shot 3D editing.
Project webpage: https://aim-uofa.github.io/Tinker

</details>


### [68] [Repeating Words for Video-Language Retrieval with Coarse-to-Fine Objectives](https://arxiv.org/abs/2508.14812)
*Haoyu Zhao,Jiaxi Gu,Shicong Wang,Xing Zhang,Hang Xu,Zuxuan Wu,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 为了解决视频语言检索的准确率和训练成本问题，提出了一种新的框架和推理管线。该框架利用细粒度特征学习和粗粒度到细粒度的目标，并通过粒度感知表示模块获取数据。推理管线通过投票机制和匹配熵指标提高了性能，无需额外训练。


<details>
  <summary>Details</summary>
Motivation: 视频流的爆发式增长在实现高准确率和低训练成本的视频-语言检索方面带来了挑战。现有方法依赖于大规模预训练来提高视频检索性能，但这会导致显著的计算需求。此外，视频和文本中的细粒度信息仍有待探索。

Method: 提出了一种新颖的框架来学习细粒度特征以实现更好的对齐，并引入了一个不需额外训练即可提高性能的推理管线。具体而言，采用粗粒度到细粒度的目标来理解视频-文本对的语义信息，包括对比学习和匹配学习。通过基于视频帧和标题中单词之间相似性分析设计的粒度感知表示模块来获取用于训练的细粒度数据。此外，通过引入包含投票机制和新的匹配熵指标的推理管线，在无需额外预训练的情况下提高了检索性能。

Result: 实验结果表明，所提出的方法在四个基准测试中均优于先前的方法。

Conclusion: 提出的方法在四个基准测试中均优于先前的方法，并且所提出的推理管线在MSR-VTT数据集上实现了2.1%的Recall@1提升，在DiDeMo数据集上实现了1.6%的Recall@1提升。

Abstract: The explosive growth of video streaming presents challenges in achieving high
accuracy and low training costs for video-language retrieval. However, existing
methods rely on large-scale pre-training to improve video retrieval
performance, resulting in significant computational demands. Additionally, the
fine-grained information in videos and texts remains underexplored. To
alleviate these problems, we propose a novel framework to learn fine-grained
features for better alignment and introduce an inference pipeline to improve
performance without additional training. Specifically, we employ coarse-to-fine
objectives to understand the semantic information of video-text pairs,
including contrastive and matching learning. The fine-grained data used for
training is obtained through the Granularity-Aware Representation module, which
is designed based on similarity analysis between video frames and words in
captions. Furthermore, we observe that the repetition of keywords in the
original captions, referred to as "Repetition", can enhance retrieval
performance and improve alignment between video and text. Based on this
insight, we propose a novel and effective inference pipeline that incorporates
a voting mechanism and a new Matching Entropy metric to achieve better
retrieval performance without requiring additional pre-training. Experimental
results on four benchmarks demonstrate that the proposed method outperforms
previous approaches. Additionally, our inference pipeline achieves significant
performance improvements, with a 2.1% increase in Recall@1 on the MSR-VTT
dataset and a 1.6% increase on the DiDeMo dataset.

</details>


### [69] [TransLight: Image-Guided Customized Lighting Control with Generative Decoupling](https://arxiv.org/abs/2508.14814)
*Zongming Li,Lianghui Zhu,Haocheng Shen,Longjin Ran,Wenyu Liu,Xinggang Wang*

Main category: cs.CV

TL;DR: TransLight 通过生成解耦技术，首次实现了跨图像的光照效果转移和定制化控制。


<details>
  <summary>Details</summary>
Motivation: 解决现有光照编辑方法在提供定制化光照控制和保持内容完整性方面存在不足的问题，尤其是在将复杂光照效果从参考图像转移到目标图像的任务中。

Method: 提出了一种名为 TransLight 的新颖框架，该框架通过生成解耦（使用两个微调的扩散模型来分离图像内容和光照效果）和 IC-Light（一种以光照图像作为附加条件信号进行训练的生成模型）来实现高保真和高自由度的光照效果转移。

Result: TransLight 能够实现多样化光照效果的定制化和自然转移，提供高度灵活的光照控制。

Conclusion: TransLight 成功地将光照效果转移到不同的图像，提供比现有技术更定制化的光照控制，并为光照协调和编辑的研究开辟了新方向。

Abstract: Most existing illumination-editing approaches fail to simultaneously provide
customized control of light effects and preserve content integrity. This makes
them less effective for practical lighting stylization requirements, especially
in the challenging task of transferring complex light effects from a reference
image to a user-specified target image. To address this problem, we propose
TransLight, a novel framework that enables high-fidelity and high-freedom
transfer of light effects. Extracting the light effect from the reference image
is the most critical and challenging step in our method. The difficulty lies in
the complex geometric structure features embedded in light effects that are
highly coupled with content in real-world scenarios. To achieve this, we first
present Generative Decoupling, where two fine-tuned diffusion models are used
to accurately separate image content and light effects, generating a newly
curated, million-scale dataset of image-content-light triplets. Then, we employ
IC-Light as the generative model and train our model with our triplets,
injecting the reference lighting image as an additional conditioning signal.
The resulting TransLight model enables customized and natural transfer of
diverse light effects. Notably, by thoroughly disentangling light effects from
reference images, our generative decoupling strategy endows TransLight with
highly flexible illumination control. Experimental results establish TransLight
as the first method to successfully transfer light effects across disparate
images, delivering more customized illumination control than existing
techniques and charting new directions for research in illumination
harmonization and editing.

</details>


### [70] [EventSSEG: Event-driven Self-Supervised Segmentation with Probabilistic Attention](https://arxiv.org/abs/2508.14856)
*Lakshmi Annamalai,Chetan Singh Thakur*

Main category: cs.CV

TL;DR: EventSSEG是一种用于道路分割的新方法，它使用事件相机和自监督学习，在不需要大量标注数据的情况下实现了先进的性能。


<details>
  <summary>Details</summary>
Motivation: 为自动驾驶车辆在低延迟和低计算的场景下，利用事件相机的低功耗特性实现道路分割，并解决事件驱动方法在迁移预训练权重和缺乏标签数据方面的挑战。

Method: EventSSEG方法利用事件相机，采用仅事件计算和概率性注意力机制进行道路分割，并结合事件驱动的自监督学习策略以解决标签数据稀缺的问题。

Result: 在DSEC-Semantic和DDD17数据集上的实验证明，EventSSEG方法实现了最先进的性能，并且仅需极少量的标签事件。

Conclusion: EventSSEG方法通过事件驱动的自监督学习克服了有监督学习对标签数据的依赖，实现了在DSEC-Semantic和DDD17数据集上的最先进性能，同时显著减少了对标签事件的需求，充分发挥了事件相机的潜力。

Abstract: Road segmentation is pivotal for autonomous vehicles, yet achieving low
latency and low compute solutions using frame based cameras remains a
challenge. Event cameras offer a promising alternative. To leverage their low
power sensing, we introduce EventSSEG, a method for road segmentation that uses
event only computing and a probabilistic attention mechanism. Event only
computing poses a challenge in transferring pretrained weights from the
conventional camera domain, requiring abundant labeled data, which is scarce.
To overcome this, EventSSEG employs event-based self supervised learning,
eliminating the need for extensive labeled data. Experiments on DSEC-Semantic
and DDD17 show that EventSSEG achieves state of the art performance with
minimal labeled events. This approach maximizes event cameras capabilities and
addresses the lack of labeled events.

</details>


### [71] [Lifespan Pancreas Morphology for Control vs Type 2 Diabetes using AI on Largescale Clinical Imaging](https://arxiv.org/abs/2508.14878)
*Lucas W. Remedios,Chloe Cho,Trent M. Schwartz,Dingjie Su,Gaurav Rudravaram,Chenyu Gao,Aravind R. Krishnan,Adam M. Saunders,Michael E. Kim,Shunxing Bao,Thomas A. Lasko,Alvin C. Powers,Bennett A. Landman,John Virostko*

Main category: cs.CV

TL;DR: 这项研究分析了2533名患者的数据，使用CT和MRI扫描来研究胰腺的大小和形状随年龄的变化。研究发现，与非糖尿病患者相比，2型糖尿病患者的胰腺在形态上存在显著差异，并且通常较小。该研究还比较了CT和MRI在胰腺测量中的一致性，并为不同年龄段人群的胰腺形态提供了参考。


<details>
  <summary>Details</summary>
Motivation: 了解胰腺如何变化对于检测2型糖尿病和其他胰腺疾病的异常至关重要。我们使用从0岁到90岁年龄段的形态学测量来测量胰腺的大小和形状。我们的目标是1)识别用于基于人工智能的胰腺测量的可靠临床成像模态，2)建立正常的形态学衰老趋势，以及3)检测2型糖尿病的潜在偏差。

Method: 我们分析了一个包含2533名患者的临床获取数据集，这些患者使用了腹部CT或MRI进行成像。我们将扫描重采样为3毫米各向同性分辨率，使用自动化方法对胰腺进行分割，并在整个生命周期中提取了13个形态学胰腺特征。首先，我们评估了CT和MRI测量结果，以确定哪些成像模态能提供一致的生命史趋势。其次，我们表征了按年龄组和性别分层的正常形态模式的分布。第三，我们使用GAMLSS回归模型对1350名根据年龄、性别和2型糖尿病状况进行匹配的患者的胰腺形态趋势进行建模，以识别与2型糖尿病相关的正常衰老模式的任何偏差。

Result: 在调整了混杂因素后，13个形态学特征中有10个的衰老趋势在2型糖尿病患者和非糖尿病对照者之间存在显著差异（在多次比较校正后，p < 0.05）。此外，使用我们基于人工智能的方法，MRI产生的胰腺测量结果似乎与CT不同。

Conclusion: 我们提供了生活史趋势，证明2型糖尿病患者的胰腺大小和形状发生了改变（使用了675名对照患者和675名糖尿病患者）。此外，我们的研究结果也证实了2型糖尿病患者的胰腺较小。我们还为临床环境中大量的非糖尿病对照患者提供了生活史胰腺形态的参考。

Abstract: Purpose: Understanding how the pancreas changes is critical for detecting
deviations in type 2 diabetes and other pancreatic disease. We measure pancreas
size and shape using morphological measurements from ages 0 to 90. Our goals
are to 1) identify reliable clinical imaging modalities for AI-based pancreas
measurement, 2) establish normative morphological aging trends, and 3) detect
potential deviations in type 2 diabetes.
  Approach: We analyzed a clinically acquired dataset of 2533 patients imaged
with abdominal CT or MRI. We resampled the scans to 3mm isotropic resolution,
segmented the pancreas using automated methods, and extracted 13 morphological
pancreas features across the lifespan. First, we assessed CT and MRI
measurements to determine which modalities provide consistent lifespan trends.
Second, we characterized distributions of normative morphological patterns
stratified by age group and sex. Third, we used GAMLSS regression to model
pancreas morphology trends in 1350 patients matched for age, sex, and type 2
diabetes status to identify any deviations from normative aging associated with
type 2 diabetes.
  Results: When adjusting for confounders, the aging trends for 10 of 13
morphological features were significantly different between patients with type
2 diabetes and non-diabetic controls (p < 0.05 after multiple comparisons
corrections). Additionally, MRI appeared to yield different pancreas
measurements than CT using our AI-based method.
  Conclusions: We provide lifespan trends demonstrating that the size and shape
of the pancreas is altered in type 2 diabetes using 675 control patients and
675 diabetes patients. Moreover, our findings reinforce that the pancreas is
smaller in type 2 diabetes. Additionally, we contribute a reference of lifespan
pancreas morphology from a large cohort of non-diabetic control patients in a
clinical setting.

</details>


### [72] [MS-CLR: Multi-Skeleton Contrastive Learning for Human Action Recognition](https://arxiv.org/abs/2508.14889)
*Mert Kiray,Alvaro Ritter,Nassir Navab,Benjamin Busam*

Main category: cs.CV

TL;DR: 提出了一种多骨架对比学习（MS-CLR）框架，通过整合来自同一序列的多种骨架表示，提高了骨架表示的泛化能力和表达能力，并在NTU RGB+D 60和120数据集上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的骨架表示学习方法依赖于单一骨架约定，这限制了它们在具有不同关节结构和解剖覆盖范围的数据集上的泛化能力。

Method: 提出了一种名为多骨架对比学习（MS-CLR）的通用自监督框架，该框架通过统一的表示方案，适配ST-GCN架构以处理具有不同关节布局和尺度的骨架。

Result: MS-CLR框架学习了结构不变性并捕获了多样化的解剖线索，从而获得更具表达力和泛化能力的特征。实验证明，MS-CLR相比于强大的单骨架对比学习基线方法，在NTU RGB+D 60和120数据集上始终提高了性能。

Conclusion: MS-CLR框架在NTU RGB+D 60和120数据集上持续提升了性能，并通过多骨架集成进一步提高了性能，在两个数据集上均达到了新的最先进水平。

Abstract: Contrastive learning has gained significant attention in skeleton-based
action recognition for its ability to learn robust representations from
unlabeled data. However, existing methods rely on a single skeleton convention,
which limits their ability to generalize across datasets with diverse joint
structures and anatomical coverage. We propose Multi-Skeleton Contrastive
Learning (MS-CLR), a general self-supervised framework that aligns pose
representations across multiple skeleton conventions extracted from the same
sequence. This encourages the model to learn structural invariances and capture
diverse anatomical cues, resulting in more expressive and generalizable
features. To support this, we adapt the ST-GCN architecture to handle skeletons
with varying joint layouts and scales through a unified representation scheme.
Experiments on the NTU RGB+D 60 and 120 datasets demonstrate that MS-CLR
consistently improves performance over strong single-skeleton contrastive
learning baselines. A multi-skeleton ensemble further boosts performance,
setting new state-of-the-art results on both datasets.

</details>


### [73] [GaussianArt: Unified Modeling of Geometry and Motion for Articulated Objects](https://arxiv.org/abs/2508.14891)
*Licheng Shen,Saining Zhang,Honghan Li,Peilin Yang,Zihao Huang,Zongzheng Zhang,Hao Zhao*

Main category: cs.CV

TL;DR: 提出了一种统一的表示方法，使用可伸缩的3D高斯联合建模几何和运动，以应对关节物体重建的挑战，并在MPArt-90基准上进行了评估。


<details>
  <summary>Details</summary>
Motivation: 为了构建交互式环境的数字孪生，需要对关节物体进行重建。然而，先前的方法通常将几何与运动分离，通过在不同状态下重建物体形状，然后通过事后对齐来估计关节，这使得重建流程复杂化，并限制了可伸缩性，特别是对于具有复杂、多部分关节的物体。

Method: 提出了一种使用可伸缩的3D高斯联合建模几何和运动的统一表示方法。

Result: 在MPArt-90基准上进行的大量实验表明，该方法在部件级几何重建和运动估计方面始终如一地实现了卓越的准确性，并且对机器人模拟和人景交互建模等下游任务具有适用性。

Conclusion: 该方法通过联合建模几何和运动的统一表示，显著优于先前的方法，并且可以处理高达20个部件的关节物体，而先前的方法通常在2-3个部件后就会遇到困难。

Abstract: Reconstructing articulated objects is essential for building digital twins of
interactive environments. However, prior methods typically decouple geometry
and motion by first reconstructing object shape in distinct states and then
estimating articulation through post-hoc alignment. This separation complicates
the reconstruction pipeline and restricts scalability, especially for objects
with complex, multi-part articulation. We introduce a unified representation
that jointly models geometry and motion using articulated 3D Gaussians. This
formulation improves robustness in motion decomposition and supports
articulated objects with up to 20 parts, significantly outperforming prior
approaches that often struggle beyond 2--3 parts due to brittle initialization.
To systematically assess scalability and generalization, we propose MPArt-90, a
new benchmark consisting of 90 articulated objects across 20 categories, each
with diverse part counts and motion configurations. Extensive experiments show
that our method consistently achieves superior accuracy in part-level geometry
reconstruction and motion estimation across a broad range of object types. We
further demonstrate applicability to downstream tasks such as robotic
simulation and human-scene interaction modeling, highlighting the potential of
unified articulated representations in scalable physical modeling.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [74] [From Image Captioning to Visual Storytelling](https://arxiv.org/abs/2508.14045)
*Admitos Passadakis,Yingjin Song,Albert Gatt*

Main category: cs.CL

TL;DR: 视觉故事生成是一个模态间的任务，需要生成连贯的故事情节。本研究将视觉故事生成视为图像描述的超集，并利用语言到语言的方法来生成故事，以平衡图像与语言。最后，提出了一种新的评估指标 ideality。


<details>
  <summary>Details</summary>
Motivation: 视觉故事生成是视觉和语言之间的一项具有挑战性的多模态任务，目的是为图像流生成故事。其难点在于故事既要以图像序列为基础，又要具有叙事性和连贯性。这项工作的目的在于平衡这些方面。

Method: 首先采用视觉到语言模型获取输入图像的字幕，然后使用语言到语言方法将这些字幕转换为连贯的叙述。

Result: 与之前的研究相比，该方法加速了训练时间，并使框架易于重复使用和重现。

Conclusion: 该方法将视觉故事生成视为图像描述的超集，并在生成故事时平衡了图像与语言。此外，我们提出了一个名为 ideality 的新指标/工具，用于评估模型与理想情况的差距，并应用它来模拟视觉故事生成中的类似人类的交互。

Abstract: Visual Storytelling is a challenging multimodal task between Vision &
Language, where the purpose is to generate a story for a stream of images. Its
difficulty lies on the fact that the story should be both grounded to the image
sequence but also narrative and coherent. The aim of this work is to balance
between these aspects, by treating Visual Storytelling as a superset of Image
Captioning, an approach quite different compared to most of prior relevant
studies. This means that we firstly employ a vision-to-language model for
obtaining captions of the input images, and then, these captions are
transformed into coherent narratives using language-to-language methods. Our
multifarious evaluation shows that integrating captioning and storytelling
under a unified framework, has a positive impact on the quality of the produced
stories. In addition, compared to numerous previous studies, this approach
accelerates training time and makes our framework readily reusable and
reproducible by anyone interested. Lastly, we propose a new metric/tool, named
ideality, that can be used to simulate how far some results are from an oracle
model, and we apply it to emulate human-likeness in visual storytelling.

</details>


### [75] [Benchmarking Sociolinguistic Diversity in Swahili NLP: A Taxonomy-Guided Approach](https://arxiv.org/abs/2508.14051)
*Kezia Oketch,John P. Lalor,Ahmed Abbasi*

Main category: cs.CL

TL;DR: This paper presents the first taxonomy-guided evaluation of Swahili NLP using a dataset of 2,170 responses from Kenyan speakers, revealing the impact of sociolinguistic variation on language model performance.


<details>
  <summary>Details</summary>
Motivation: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing gaps in sociolinguistic diversity.

Method: We develop a structured taxonomy and use it as a lens for examining model prediction errors across pre-trained and instruction-tuned language models.

Result: We collect a dataset of 2,170 free-text responses from Kenyan speakers. The data exhibits tribal influences, urban vernacular, code-mixing, and loanwords.

Conclusion: The findings advance culturally grounded evaluation frameworks and highlight the role of sociolinguistic variation in shaping model performance.

Abstract: We introduce the first taxonomy-guided evaluation of Swahili NLP, addressing
gaps in sociolinguistic diversity. Drawing on health-related psychometric
tasks, we collect a dataset of 2,170 free-text responses from Kenyan speakers.
The data exhibits tribal influences, urban vernacular, code-mixing, and
loanwords. We develop a structured taxonomy and use it as a lens for examining
model prediction errors across pre-trained and instruction-tuned language
models. Our findings advance culturally grounded evaluation frameworks and
highlight the role of sociolinguistic variation in shaping model performance.

</details>


### [76] [Contrastive Analysis of Constituent Order Preferences Within Adverbial Roles in English and Chinese News: A Large-Language-Model-Driven Approach](https://arxiv.org/abs/2508.14054)
*Yiran Rex Ma*

Main category: cs.CL

TL;DR: 本研究使用LLM标注的英汉新闻语料，分析了具有副词功能的“功能块”的语序差异。结果发现，英文新闻偏好核心信息优先、功能块后置；中文新闻偏好背景信息优先、功能块前置。两种语言在SVO结构和功能块并存时均有语序灵活性，但中文前置倾向更明显。研究揭示了语序的系统偏好与动态适应性，为英汉对比研究提供新证据。


<details>
  <summary>Details</summary>
Motivation: 探究英汉新闻语序的差异，特别是从具有副词功能的“功能块”角度进行分析，并阐述其典型的位置偏好和分布模式。

Method: 基于LLM对英汉新闻语料进行标注，从具有副词功能的“功能块”视角出发，探讨英汉新闻语序差异、位置偏好及分布模式。

Result: 1. 英文新闻倾向于线性叙事（核心信息优先），功能块多后置；中文新闻倾向于整体呈现（背景信息优先），功能块多前置。
2. 在主谓宾（SVO）结构中，英汉新闻的功能块分布存在差异，中文前置倾向更显著，英文后置倾向相对温和。
3. 当功能块并存时，英汉新闻均表现出高度灵活性，语序调整受信息和语用目的驱动。

Conclusion: 英文和中文新闻的语序既有系统性偏好，也具有动态适应性，这为英汉信息结构对比研究提供了新的实证支持。

Abstract: Based on comparable English-Chinese news corpora annotated by Large Language
Model (LLM), this paper attempts to explore the differences in constituent
order of English-Chinese news from the perspective of functional chunks with
adverbial roles, and analyze their typical positional preferences and
distribution patterns. It is found that: (1) English news prefers linear
narrative of core information first, and functional chunks are mostly
post-positioned, while Chinese news prefers overall presentation mode of
background first, and functional chunks are often pre-positioned; (2) In SVO
structure, both English and Chinese news show differences in the distribution
of functional chunks, but the tendency of Chinese pre-positioning is more
significant, while that of English post-positioning is relatively mild; (3)
When function blocks are co-occurring, both English and Chinese news show high
flexibility, and the order adjustment is driven by information and pragmatic
purposes. The study reveals that word order has both systematic preference and
dynamic adaptability, providing new empirical support for contrastive study of
English-Chinese information structure.

</details>


### [77] [T-REX: Table -- Refute or Entail eXplainer](https://arxiv.org/abs/2508.14055)
*Tim Luka Horstmann,Baptiste Geisenberger,Mehwish Alam*

Main category: cs.CL

TL;DR: T-REX 是一个用户友好的工具，利用 LLM 来验证关于表格的事实，使非专家能够进行事实核查。


<details>
  <summary>Details</summary>
Motivation: 验证文本声明与结构化表格数据是自然语言处理中的一项关键但具有挑战性的任务，具有广泛的实际影响。虽然 LLM 的最新进展在表格事实核查方面取得了重大进展，但目前的解决方案对于非专家来说仍然难以获得。

Method: T-REX（T-REX：表格--反驳或蕴含解释器）是一个使用最先进的指令调整推理 LLM 来验证多模态、多语言表格声明的实时、交互式工具。

Result: T-REX 提供了对先进的事实核查技术的访问，使非专家能够访问该技术。该系统可在网上公开获取。

Conclusion: T-REX 是首个用于通过最先进的指令调整推理 LLM 来验证多模态、多语言表格的声明的实时、交互式工具，旨在提高准确性和透明度，使非专家能够访问先进的事实核查技术。

Abstract: Verifying textual claims against structured tabular data is a critical yet
challenging task in Natural Language Processing with broad real-world impact.
While recent advances in Large Language Models (LLMs) have enabled significant
progress in table fact-checking, current solutions remain inaccessible to
non-experts. We introduce T-REX (T-REX: Table -- Refute or Entail eXplainer),
the first live, interactive tool for claim verification over multimodal,
multilingual tables using state-of-the-art instruction-tuned reasoning LLMs.
Designed for accuracy and transparency, T-REX empowers non-experts by providing
access to advanced fact-checking technology. The system is openly available
online.

</details>


### [78] [Confidence Estimation for Text-to-SQL in Large Language Models](https://arxiv.org/abs/2508.14056)
*Sepideh Entezari Maleki,Mohammadreza Pourreza,Davood Rafiei*

Main category: cs.CL

TL;DR: 在text-to-SQL的置信度估计中，黑盒模型（如一致性方法）和白盒模型（如SQL语法感知方法）均有良好表现，而执行结果可作为辅助信号提升效果。


<details>
  <summary>Details</summary>
Motivation: 旨在解决text-to-SQL模型生成SQL查询时的可靠性评估问题，特别是在LLM模型权重和梯度受限的情况下。

Method: 研究了黑盒和白盒两种置信度估计策略，并评估了它们在跨域text-to-SQL基准上的表现。

Result: 一致性方法在黑盒模型中表现更好，SQL语法感知方法在白盒模型中更具优势，执行结果可以提升两者效果。

Conclusion: 基于一致性和SQL语法感知的模型在置信度估计方面表现优异，并且执行结果可以作为有价值的补充信号。

Abstract: Confidence estimation for text-to-SQL aims to assess the reliability of
model-generated SQL queries without having access to gold answers. We study
this problem in the context of large language models (LLMs), where access to
model weights and gradients is often constrained. We explore both black-box and
white-box confidence estimation strategies, evaluating their effectiveness on
cross-domain text-to-SQL benchmarks. Our evaluation highlights the superior
performance of consistency-based methods among black-box models and the
advantage of SQL-syntax-aware approaches for interpreting LLM logits in
white-box settings. Furthermore, we show that execution-based grounding of
queries provides a valuable supplementary signal, improving the effectiveness
of both approaches.

</details>


### [79] [Assessing and Mitigating Data Memorization Risks in Fine-Tuned Large Language Models](https://arxiv.org/abs/2508.14062)
*Badrinath Ramakrishnan,Akshaya Balaji*

Main category: cs.CL

TL;DR: LLMs微调存在隐私风险，本研究提出的多层隐私保护框架能有效解决此问题，在保护隐私的同时保持模型效用。


<details>
  <summary>Details</summary>
Motivation: LLMs在微调过程中容易记忆训练数据，带来隐私风险，需要有效的隐私保护措施。

Method: 通过在GPT-2、Phi-3和Gemma-2等模型上进行控制实验，分析数据记忆情况，并提出并评估了语义数据去重、生成过程中的差分隐私、基于熵的过滤和基于模式的内容过滤四种隐私保护方法。

Result: 微调过程中的重复敏感数据会使隐私泄露率从基线水平的0-5%提高到60-75%。所提出的四种隐私保护方法能将数据泄露率降至0%，同时保持94.7%的模型效用。

Conclusion: 该研究展示了LLMs在微调过程中存在数据记忆和隐私泄露的风险，并提出了一种多层隐私保护框架。实验证明，该框架能有效降低隐私泄露率至0%，同时保持94.7%的模型效用。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities across
diverse natural language processing tasks, but their tendency to memorize
training data poses significant privacy risks, particularly during fine-tuning
processes. This paper presents a comprehensive empirical analysis of data
memorization in fine-tuned LLMs and introduces a novel multi-layered privacy
protection framework. Through controlled experiments on modern LLM
architectures including GPT-2, Phi-3, and Gemma-2, we demonstrate that
fine-tuning with repeated sensitive data increases privacy leakage rates from
baseline levels of 0-5% to 60-75%, representing a 64.2% average increase across
tested models. We propose and rigorously evaluate four complementary privacy
protection methods: semantic data deduplication, differential privacy during
generation, entropy-based filtering, and pattern-based content filtering. Our
experimental results show that these techniques can reduce data leakage to 0%
while maintaining 94.7% of original model utility.

</details>


### [80] [Punctuation and Predicates in Language Models](https://arxiv.org/abs/2508.14067)
*Sonakshi Chauhan,Maheep Chaudhary,Koby Choy,Samuel Nellessen,Nandi Schoots*

Main category: cs.CL

TL;DR: LLM如何处理标点和推理？研究发现，GPT-2重视标点，Gemma不重视；条件语句和全称量化处理方式也不同。这些发现有助于理解LLM内部运作。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示大型语言模型（LLMs）在信息收集和传播方面的内部机制，特别是关注标点符号作为“注意力汇聚点”和“记忆辅助”的计算重要性，以及LLMs如何处理不同的推理规则（如条件语句和全称量化），为LLM的可解释性研究提供新的见解。

Method: 本研究采用干预（intervention-based techniques）和层交换（layer-swapping experiments）等技术，评估了标点符号在GPT-2、DeepSeek和Gemma模型不同层级中的必要性和充分性。此外，还通过对比分析不同输入成分（如主语、形容词、标点、句子）和推理规则（条件语句、全称量化）的处理方式，来理解LLM的内部运作。

Result: 研究发现，标点符号在GPT-2模型的多层级中既是必需的也是充分的，但在DeepSeek和Gemma模型中的情况则不那么明显，尤其是在Gemma中几乎不适用。此外，研究表明LLMs处理条件语句和全称量化的方式存在显著差异，这暗示了模型在处理不同推理规则时具有不同的内部机制。

Conclusion: 本研究深入探究了LLM内部信息收集和传播机制，特别是标点符号和不同推理规则（如条件语句和全称量化）的作用。研究结果揭示了模型之间在处理这些信息上的显著差异，为理解LLM的内部机制和提高可解释性提供了新的视角。

Abstract: In this paper we explore where information is collected and how it is
propagated throughout layers in large language models (LLMs). We begin by
examining the surprising computational importance of punctuation tokens which
previous work has identified as attention sinks and memory aids. Using
intervention-based techniques, we evaluate the necessity and sufficiency (for
preserving model performance) of punctuation tokens across layers in GPT-2,
DeepSeek, and Gemma. Our results show stark model-specific differences: for
GPT-2, punctuation is both necessary and sufficient in multiple layers, while
this holds far less in DeepSeek and not at all in Gemma. Extending beyond
punctuation, we ask whether LLMs process different components of input (e.g.,
subjects, adjectives, punctuation, full sentences) by forming early static
summaries reused across the network, or if the model remains sensitive to
changes in these components across layers. Extending beyond punctuation, we
investigate whether different reasoning rules are processed differently by
LLMs. In particular, through interchange intervention and layer-swapping
experiments, we find that conditional statements (if, then), and universal
quantification (for all) are processed very differently. Our findings offer new
insight into the internal mechanisms of punctuation usage and reasoning in LLMs
and have implications for interpretability.

</details>


### [81] [DLLMQuant: Quantizing Diffusion-based Large Language Models](https://arxiv.org/abs/2508.14090)
*Chen Xu,Dawei Yang*

Main category: cs.CL

TL;DR: DLLMQuant 是一个针对 DLLMs 的 PTQ 框架，通过 TMAS、IA-AQ 和 CGQ 三种技术解决了量化带来的准确率下降和效率问题。


<details>
  <summary>Details</summary>
Motivation: 现有 PTQ 方法直接应用于 DLLMs 时，准确率会严重下降且泛化能力减弱。DLLMs 的关键机制（动态掩码、迭代生成、双向注意力）与量化存在冲突，具体表现为：1. 迭代生成和动态掩码比例导致不同解码步数的 token 分布存在差异，现有 PTQ 校准方法无法充分捕捉。2. 量化误差在 DLLMs 的迭代过程中会累积和放大，导致量化模型随解码步数增加性能下降。3. 未掩码 token 稳定而掩码 token 保持概率性，整体特征分布与现有 PTQ 方法不兼容。

Method: 1. 时间掩码自适应采样（TMAS）：一种考虑时间步和掩码因子、能够捕获时间步分布的校准方法。 2. 交互感知激活量化（IA-AQ）：利用双向注意力交互信号动态分配量化资源。 3. 确定性引导量化（CGQ）：将掩码状态和 token 分数作为关键权重标准集成到误差补偿中，使权重量化更适合 DLLMs。

Result: DLLMQuant 在量化 DLLMs 方面取得了显著的性能提升和效率增强。

Conclusion: DLLMQuant 框架通过时间掩码自适应采样（TMAS）、交互感知激活量化（IA-AQ）和确定性引导量化（CGQ）这三种新颖技术，有效地解决了现有 PTQ 方法在 DLLMs 上遇到的准确率下降和泛化能力减弱的问题，实现了显著的性能提升和效率增强。

Abstract: Diffusion-based large language models (DLLMs) have shown promise for
non-autoregressive text generation, but their deployment is constrained by
large model sizes and heavy computational costs. Post-training quantization
(PTQ), a widely used method for compressing and accelerating Large Language
Models (LLMs), suffers from severe accuracy degradation and reduced
generalization performance when directly applied to DLLMs (e.g., AWQ suffers a
16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key
mechanisms - dynamic masking, iterative generation, bidirectional attention -
clash with quantization. We identify three core issues: 1) Iterative generation
and dynamic masking ratios lead to distinct token distributions across decoding
steps, which are not adequately captured by existing PTQ calibration methods;
2) Quantization errors are accumulated and amplified progressively during
iteration in DLLMs, causing quantized models to perform worse as decoding steps
progress; 3) Unmasked tokens stabilize while masked remain probabilistic,
making overall feature distribution incompatible with existing PTQ methods. To
address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs,
which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling
(TMAS), a calibration method that accounts for both time and mask factors, with
the capacity to capture distributions across timesteps. 2) Interaction-Aware
Activation Quantization (IA-AQ), which utilizes bidirectional attention's
interaction signals to dynamically allocate quantization resources. 3)
Certainty-Guided Quantization (CGQ), which integrates mask status and token
scores as key weighting criteria into error compensation, making weight
quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves
significant performance gains while enhancing efficiency.

</details>


### [82] [MMReview: A Multidisciplinary and Multimodal Benchmark for LLM-Based Peer Review Automation](https://arxiv.org/abs/2508.14146)
*Xian Gao,Jiacheng Ruan,Zongyun Zhang,Jingsheng Gao,Ting Liu,Yuzhuo Fu*

Main category: cs.CL

TL;DR: MMReview 是一个多模态同行评审评估基准，用于评估大型语言模型在学术评审任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着学术出版物的快速增长，同行评审已成为研究界一项必不可少但耗时的工作。目前的 LLM 同行评审任务缺乏统一的评估基准来严格评估模型在生成全面、准确和符合人类期望的评估方面的能力，尤其是在涉及如图表等多模态内容的场景中。

Method: 提出了 MMReview 基准，包含跨学科和模态的内容，并设计了 13 项任务，涵盖了分步评审生成、结果制定、与人类偏好对齐以及对抗性输入操纵的鲁棒性等方面，并在 16 个开源模型和 5 个闭源模型上进行了广泛的实验。

Result: 通过在 16 个开源模型和 5 个先进的闭源模型上进行的大量实验，证明了该基准的全面性。

Conclusion: MMReview 是一个全面的基准，旨在为自动化同行评审系统的发展奠定标准化基础。

Abstract: With the rapid growth of academic publications, peer review has become an
essential yet time-consuming responsibility within the research community.
Large Language Models (LLMs) have increasingly been adopted to assist in the
generation of review comments; however, current LLM-based review tasks lack a
unified evaluation benchmark to rigorously assess the models' ability to
produce comprehensive, accurate, and human-aligned assessments, particularly in
scenarios involving multimodal content such as figures and tables. To address
this gap, we propose \textbf{MMReview}, a comprehensive benchmark that spans
multiple disciplines and modalities. MMReview includes multimodal content and
expert-written review comments for 240 papers across 17 research domains within
four major academic disciplines: Artificial Intelligence, Natural Sciences,
Engineering Sciences, and Social Sciences. We design a total of 13 tasks
grouped into four core categories, aimed at evaluating the performance of LLMs
and Multimodal LLMs (MLLMs) in step-wise review generation, outcome
formulation, alignment with human preferences, and robustness to adversarial
input manipulation. Extensive experiments conducted on 16 open-source models
and 5 advanced closed-source models demonstrate the thoroughness of the
benchmark. We envision MMReview as a critical step toward establishing a
standardized foundation for the development of automated peer review systems.

</details>


### [83] [DPad: Efficient Diffusion Language Models with Suffix Dropout](https://arxiv.org/abs/2508.14148)
*Xinhua Chen,Sitao Huang,Cong Guo,Chiyue Wei,Yintao He,Jianyi Zhang,Hai "Hellen" Li,Yiran Chen*

Main category: cs.CL

TL;DR: DPad 是一种训练方法，通过限制对附近后缀标记的注意力来提高 dLLMs 的效率，可实现高达 61.4 倍的速度提升，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: DPad 旨在解决 Diffusion-based Large Language Models (dLLMs) 在文本生成时的高计算开销问题，dLLMs 通过将解码视为一个去噪过程来实现文本生成并行化，但由于在每一步预测所有未来的后缀标记只保留一小部分，因此计算开销很高。

Method: DPad 整合了两种策略：(i) 滑动窗口，维护固定长度的后缀窗口；(ii) 距离衰减丢弃，在注意力计算之前确定性地移除远处的后缀标记。

Result: DPad 在 LLaDA-1.5 和 Dream 模型上的多项基准测试表明，DPad 的速度比标准的 dLLMs 快 61.4 倍，同时保持了可比的准确性。

Conclusion: DPad 通过限制注意力到一小组邻近的后缀标记，在保持保真度的同时消除了冗余，与现有的优化（如前缀缓存）兼容，并且只需要几行代码即可实现。

Abstract: Diffusion-based Large Language Models (dLLMs) parallelize text generation by
framing decoding as a denoising process, but suffer from high computational
overhead since they predict all future suffix tokens at each step while
retaining only a small fraction. We propose Diffusion Scratchpad (DPad), a
training-free method that restricts attention to a small set of nearby suffix
tokens, preserving fidelity while eliminating redundancy. DPad integrates two
strategies: (i) a sliding window, which maintains a fixed-length suffix window,
and (ii) distance-decay dropout, which deterministically removes distant suffix
tokens before attention computation. This simple design is compatible with
existing optimizations such as prefix caching and can be implemented with only
a few lines of code. Comprehensive evaluations across multiple benchmarks on
LLaDA-1.5 and Dream models demonstrate that DPad delivers up to
$\mathbf{61.4\times}$ speedup over vanilla dLLMs while maintaining comparable
accuracy, highlighting its potential for efficient and scalable long-sequence
inference. Our code is available at https://github.com/Crys-Chen/DPad.

</details>


### [84] [Comparing energy consumption and accuracy in text classification inference](https://arxiv.org/abs/2508.14170)
*Johannes Zschache,Tilman Hartwig*

Main category: cs.CL

TL;DR: LLMs在文本分类推理中的能耗和准确性分析表明，最佳模型可能兼具高准确率和高能效，而大型LLM能耗高且准确率低。能耗受模型和硬件影响，运行时间可作为能耗代理。


<details>
  <summary>Details</summary>
Motivation: LLMs在NLP任务中部署增加，引发对能耗和可持续性的担忧，但推理阶段受关注较少。

Method: 通过不同模型架构和硬件配置，系统评估文本分类推理中模型准确性与能耗的权衡。

Result: 最佳模型在准确率和能耗上均表现优异，大型LLM能耗高且准确率低。推理能耗存在较大差异，与模型类型、大小和硬件相关。推理能耗与运行时间强相关。

Conclusion: LLMs在文本分类推理中，准确性和能耗之间存在权衡。最佳模型不一定能耗最高，大型LLM能耗高且准确率低。模型类型、大小和硬件规格会影响推理能耗，能耗与运行时间强相关，可作为能耗的代理指标。

Abstract: The increasing deployment of large language models (LLMs) in natural language
processing (NLP) tasks raises concerns about energy efficiency and
sustainability. While prior research has largely focused on energy consumption
during model training, the inference phase has received comparatively less
attention. This study systematically evaluates the trade-offs between model
accuracy and energy consumption in text classification inference across various
model architectures and hardware configurations. Our empirical analysis shows
that the best-performing model in terms of accuracy can also be
energy-efficient, while larger LLMs tend to consume significantly more energy
with lower classification accuracy. We observe substantial variability in
inference energy consumption ($<$mWh to $>$kWh), influenced by model type,
model size, and hardware specifications. Additionally, we find a strong
correlation between inference energy consumption and model runtime, indicating
that execution time can serve as a practical proxy for energy usage in settings
where direct measurement is not feasible. These findings have implications for
sustainable AI development, providing actionable insights for researchers,
industry practitioners, and policymakers seeking to balance performance and
resource efficiency in NLP applications.

</details>


### [85] [Let's Use ChatGPT To Write Our Paper! Benchmarking LLMs To Write the Introduction of a Research Paper](https://arxiv.org/abs/2508.14273)
*Krishna Garg,Firoz Shaikh,Sambaran Bandyopadhyay,Cornelia Caragea*

Main category: cs.CL

TL;DR: 该研究提出了科学论文摘要生成（SciIG）任务，评估大型语言模型根据标题、摘要和相关工作生成连贯摘要的能力。通过对 NAACL 2025 和 ICLR 2025 论文的新数据集进行评估，研究发现 LLaMA-4 Maverick 表现最佳，并强调了三样本提示的有效性。


<details>
  <summary>Details</summary>
Motivation: 随着研究人员越来越多地采用大型语言模型（LLMs）作为写作助手，生成高质量的研究论文摘要仍然具有挑战性且至关重要。

Method: 评估了五种最先进的模型（包括开源的 DeepSeek-v3、Gemma-3-12B、LLaMA 4-Maverick、MistralAI Small 3.1 和闭源的 GPT-4o），在词汇重叠、语义相似性、内容覆盖、忠实度、一致性、引用正确性和叙事质量等多个维度上进行了评估。评估框架结合了自动指标和 LLM 作为裁判的评估。

Result: LLaMA-4 Maverick 在大多数指标上表现出卓越的性能，尤其是在语义相似性和忠实度方面。此外，三样本提示的性能优于少样本方法。

Conclusion: LLaMA-4 Maverick 在大多数指标上表现出卓越的性能，尤其是在语义相似性和忠实度方面。此外，三样本提示的性能优于少样本方法。这些发现为开发有效的研习写作助手提供了实用的见解，并为 LLM 辅助学术写作设定了实际的期望。

Abstract: As researchers increasingly adopt LLMs as writing assistants, generating
high-quality research paper introductions remains both challenging and
essential. We introduce Scientific Introduction Generation (SciIG), a task that
evaluates LLMs' ability to produce coherent introductions from titles,
abstracts, and related works. Curating new datasets from NAACL 2025 and ICLR
2025 papers, we assess five state-of-the-art models, including both open-source
(DeepSeek-v3, Gemma-3-12B, LLaMA 4-Maverick, MistralAI Small 3.1) and
closed-source GPT-4o systems, across multiple dimensions: lexical overlap,
semantic similarity, content coverage, faithfulness, consistency, citation
correctness, and narrative quality. Our comprehensive framework combines
automated metrics with LLM-as-a-judge evaluations. Results demonstrate LLaMA-4
Maverick's superior performance on most metrics, particularly in semantic
similarity and faithfulness. Moreover, three-shot prompting consistently
outperforms fewer-shot approaches. These findings provide practical insights
into developing effective research writing assistants and set realistic
expectations for LLM-assisted academic writing. To foster reproducibility and
future research, we will publicly release all code and datasets.

</details>


### [86] [Disentangling concept semantics via multilingual averaging in Sparse Autoencoders](https://arxiv.org/abs/2508.14275)
*Cliff O'Reilly,Ernesto Jimenez-Ruiz,Tillman Weyde*

Main category: cs.CL

TL;DR: 提出一种通过平均多语言激活来分离大语言模型中概念语义的新方法，实验表明该方法能更准确地反映类别间的真实关系。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLMs）在连接形式知识表示和推理时存在的不足，即嵌入和稀疏自编码器表示的语义与句法和语言相关信息纠缠不清的问题。

Method: 利用稀疏自编码器（Sparse Autoencoders）提取大语言模型（LLMs）中概念的激活值，通过平均不同语言（英语、法语、中文）的激活值来获得概念平均值，并将其与本体类别的真实映射进行关联分析。

Result: 平均不同语言的激活值所得出的概念平均值，与本体类别的真实映射之间的相关性，相比仅使用单一语言的激活值，具有更强的指示作用，表明该方法能够更准确地实现对内部网络状态的机械解释。

Conclusion: 本文提出的方法通过平均不同语言的激活来获得概念平均值，并在与真实类别映射进行比较时，显示出与真实类别关系更强的一致性，优于单一语言表示。

Abstract: Connecting LLMs with formal knowledge representation and reasoning is a
promising approach to address their shortcomings. Embeddings and sparse
autoencoders are widely used to represent textual content, but the semantics
are entangled with syntactic and language-specific information. We propose a
method that isolates concept semantics in Large Langue Models by averaging
concept activations derived via Sparse Autoencoders. We create English text
representations from OWL ontology classes, translate the English into French
and Chinese and then pass these texts as prompts to the Gemma 2B LLM. Using the
open source Gemma Scope suite of Sparse Autoencoders, we obtain concept
activations for each class and language version. We average the different
language activations to derive a conceptual average. We then correlate the
conceptual averages with a ground truth mapping between ontology classes. Our
results give a strong indication that the conceptual average aligns to the true
relationship between classes when compared with a single language by itself.
The result hints at a new technique which enables mechanistic interpretation of
internal network states with higher accuracy.

</details>


### [87] [GRILE: A Benchmark for Grammar Reasoning and Explanation in Romanian LLMs](https://arxiv.org/abs/2508.14279)
*Adrian-Marius Dumitran,Alexandra-Mihaela Danila,Angela-Liliana Dumitran*

Main category: cs.CL

TL;DR: 本研究提出了GRILE，一个用于评估LLMs在罗马尼亚语（一种低资源语言）语法能力和解释生成能力的基准。结果表明，尽管先进模型表现出一定潜力，但开源模型在准确性和解释质量方面仍有待提高，特别是在形态学和正字法方面。该研究为低资源语言的教育NLP领域的研究提供了重要资源和方向。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在低资源语言（以罗马尼亚语为例）的教育应用潜力，特别是它们在语法推理和提供准确的语言学解释方面的能力。

Method: 开发并使用了一个包含1151个罗马尼亚语多项选择题的开放基准GRILE，用于评估七个先进的多语言和特定罗马尼亚语LLMs在选择正确答案和生成语言学解释方面的能力。通过专家评审对LLMs的解释进行了错误分析。

Result: Gemini 2.5 Pro在选择正确答案方面达到83%的准确率，而大多数其他开源模型准确率低于65%。在解释方面，近一半的开源模型解释存在事实或教学上的缺陷。错误分析显示，模型在形态学和应用最新的正字法规范方面存在系统性弱点。

Conclusion: LLMs在低资源语言的教学应用方面仍面临挑战，尤其是在语法准确性和解释可靠性方面。GRILE基准的建立为评估和改进LLMs在这些领域的表现提供了新的途径。

Abstract: LLMs (Large language models) have revolutionized NLP (Natural Language
Processing), yet their pedagogical value for low-resource languages remains
unclear. We present GRILE (Grammar Romanian Inference and Language
Explanations) , the first open benchmark of 1,151 multiple-choice questions
harvested from Romanian high-stakes exams (National Evaluation, Baccalaureate,
university admissions). GRILE enables us to probe two complementary abilities
of seven state-of-the-art multilingual and Romanian-specific LLMs: (i)
selecting the correct answer, and (ii) producing linguistically accurate
explanations. While Gemini 2.5 Pro reaches 83% accuracy, most open-weight
models stay below 65%, and 48% of their explanations contain factual or
pedagogical flaws according to expert review. A detailed error analysis
pinpoints systematic weaknesses in morphology and in applying the latest DOOM3
orthographic norms. All data, code and a public web demo are released to
catalyze future research. Our findings expose open challenges for trustworthy
educational NLP in low-resource settings and establish GRILE as a new test-bed
for controllable explanation generation and evaluation.

</details>


### [88] [Tokens with Meaning: A Hybrid Tokenization Approach for NLP](https://arxiv.org/abs/2508.14292)
*M. Ali Bayram,Ali Arda Fincan,Ahmet Semih Gümüş,Sercan Karakaş,Banu Diri,Savaş Yıldırım,Demircan Çelik*

Main category: cs.CL

TL;DR: 提出了一种结合规则和统计方法的混合分词框架，用于处理形态丰富的语言，并在土耳其语上取得了优于现有方法的成果，同时具有语言无关性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的子词分词方法（如BPE和WordPiece）在处理形态丰富的粘着语时效率不高，因为它们依赖于频率而不是语言结构。

Method: 提出了一种结合基于规则的形态分析和统计子词分割的混合分词框架。该方法使用语音规范化、词根-附加词词典以及一种平衡词缀保留和词汇效率的新算法。它为语音变体附加词（例如 -ler 和 -lar）和改变的词根形式（例如 kitap vs. kitab{') 分配共享标识符，从而在保持语义完整性的同时减少冗余。添加了用于空格和大小写的特殊标记，包括一个UPPERCASE标记，以避免大写引起的词汇膨胀。BPE被整合用于处理未登录词，同时不损害形态连贯性。

Result: 该分词器在TR-MMLU基准测试中实现了最高的土耳其语标记化比例（90.29%）和纯标记化比例（85.8%）。与LLaMA、Gemma和GPT的分词器相比，该分词器生成的标记更具语言学意义和连贯性。

Conclusion: 该方法在TR-MMLU基准测试中实现了最高的土耳其语标记化比例（90.29%）和纯标记化比例（85.8%），并且生成的标记比LLaMA、Gemma和GPT的标记更具语言学意义和连贯性。该方法虽然在土耳其语上进行了演示，但它是语言无关的，并且可以适应其他语言，为构建更具可解释性和有效性的多语言NLP系统提供了实用的途径。

Abstract: Tokenization plays a pivotal role in natural language processing (NLP),
shaping how text is segmented and interpreted by language models. While subword
methods such as Byte Pair Encoding (BPE) and WordPiece have been effective,
they often struggle with morphologically rich and agglutinative languages
because they rely on frequency rather than linguistic structure. We introduce a
hybrid tokenization framework that combines rule-based morphological analysis
with statistical subword segmentation. The method uses phonological
normalization, root-affix dictionaries, and a novel algorithm that balances
morpheme preservation with vocabulary efficiency. It assigns shared identifiers
to phonologically variant affixes (e.g., -ler and -lar) and altered root forms
(e.g., kitap vs. kitab{\i}), reducing redundancy while maintaining semantic
integrity. Special tokens are added for whitespace and case, including an
UPPERCASE marker to avoid vocabulary inflation from capitalization. BPE is
integrated for out-of-vocabulary coverage without harming morphological
coherence. On the TR-MMLU benchmark, the tokenizer achieves the highest Turkish
Token Percentage (90.29\%) and Pure Token Percentage (85.8\%). Comparisons with
tokenizers from LLaMA, Gemma, and GPT show more linguistically meaningful and
coherent tokens. Although demonstrated on Turkish, the approach is
language-independent and adaptable to other languages, offering a practical
path toward more interpretable and effective multilingual NLP systems.

</details>


### [89] [A Joint Multitask Model for Morpho-Syntactic Parsing](https://arxiv.org/abs/2508.14307)
*Demian Inostroza,Mel Mistica,Ekaterina Vylomova,Chris Guest,Kemal Kurniawan*

Main category: cs.CL

TL;DR: 一个联合多任务模型，使用XLM-RoBERTa编码器和三个专门解码器，在UniDive 2025形态句法分析任务中取得最佳成绩。


<details>
  <summary>Details</summary>
Motivation: 为UniDive 2025形态句法分析共享任务开发一个联合多任务模型，预测形态和句法分析。

Method: 使用共享XLM-RoBERTa编码器和三个专门的解码器，分别用于内容词识别、依存句法分析和形态句法特征预测。

Result: 在涵盖九种类型学上多样化语言的共享任务排行榜上，实现了最佳总体性能。

Conclusion: 本模型在UniDive 2025形态句法分析共享任务中取得了最佳总体性能，平均MSLAS得分为78.7%，LAS为80.1%，Feats F1为90.3%。

Abstract: We present a joint multitask model for the UniDive 2025 Morpho-Syntactic
Parsing shared task, where systems predict both morphological and syntactic
analyses following novel UD annotation scheme. Our system uses a shared
XLM-RoBERTa encoder with three specialized decoders for content word
identification, dependency parsing, and morphosyntactic feature prediction. Our
model achieves the best overall performance on the shared task's leaderboard
covering nine typologically diverse languages, with an average MSLAS score of
78.7 percent, LAS of 80.1 percent, and Feats F1 of 90.3 percent. Our ablation
studies show that matching the task's gold tokenization and content word
identification are crucial to model performance. Error analysis reveals that
our model struggles with core grammatical cases (particularly Nom-Acc) and
nominal features across languages.

</details>


### [90] [Zero-knowledge LLM hallucination detection and mitigation through fine-grained cross-model consistency](https://arxiv.org/abs/2508.14314)
*Aman Goel,Daniel Schwartz,Yanjun Qi*

Main category: cs.CL

TL;DR: Finch-Zk is a framework that uses multiple LLMs to check for factual consistency and correct inaccuracies, improving reliability without external data.


<details>
  <summary>Details</summary>
Motivation: To address the issue of hallucinations (factual inaccuracies) in LLM outputs without relying on external knowledge sources.

Method: Finch-Zk employs a cross-model consistency checking strategy using semantically-equivalent prompts and a targeted mitigation technique for precise corrections.

Result: Finch-Zk improves hallucination detection F1 scores by 6-39% on the FELM dataset and achieves 7-8 absolute percentage points improvement in answer accuracy on the GPQA-diamond dataset when applied to state-of-the-art models.

Conclusion: Finch-Zk provides a practical, deployment-ready safeguard for enhancing factual reliability in production LLM systems, showing significant improvements in hallucination detection and mitigation.

Abstract: Large language models (LLMs) have demonstrated impressive capabilities across
diverse tasks, but they remain susceptible to hallucinations--generating
content that appears plausible but contains factual inaccuracies. We present
Finch-Zk, a black-box framework that leverages FINe-grained Cross-model
consistency to detect and mitigate Hallucinations in LLM outputs without
requiring external knowledge sources. Finch-Zk introduces two key innovations:
1) a cross-model consistency checking strategy that reveals fine-grained
inaccuracies by comparing responses generated by diverse models from
semantically-equivalent prompts, and 2) a targeted mitigation technique that
applies precise corrections to problematic segments while preserving accurate
content. Experiments on the FELM dataset show Finch-Zk improves hallucination
detection F1 scores by 6-39\% compared to existing approaches. For mitigation,
Finch-Zk achieves 7-8 absolute percentage points improvement in answer accuracy
on the GPQA-diamond dataset when applied to state-of-the-art models like Llama
4 Maverick and Claude 4 Sonnet. Extensive evaluation across multiple models
demonstrates that Finch-Zk provides a practical, deployment-ready safeguard for
enhancing factual reliability in production LLM systems.

</details>


### [91] [SurveyGen-I: Consistent Scientific Survey Generation with Evolving Plans and Memory-Guided Writing](https://arxiv.org/abs/2508.14317)
*Jing Chen,Zhiheng Yang,Yixian Shen,Jie Liu,Adam Belloum,Chrysa Papagainni,Paola Grosso*

Main category: cs.CL

TL;DR: SurveyGen-I is an LLM framework that automates survey generation, improving coherence and citation coverage using a coarse-to-fine retrieval, adaptive planning, and memory-guided approach.


<details>
  <summary>Details</summary>
Motivation: Existing LLM-based approaches struggle with maintaining coherence across long surveys and providing comprehensive citation coverage. This work addresses these limitations.

Method: SurveyGen-I uses a framework combining coarse-to-fine retrieval, adaptive planning, and memory-guided generation. It starts with survey-level retrieval for an outline and plan, then refines these dynamically during generation using a memory mechanism. Fine-grained retrieval is triggered when context is insufficient, and the memory mechanism ensures subsection coherence.

Result: SurveyGen-I outperforms previous works in content quality, consistency, and citation coverage.

Conclusion: SurveyGen-I consistently outperforms previous works in content quality, consistency, and citation coverage across four scientific domains.

Abstract: Survey papers play a critical role in scientific communication by
consolidating progress across a field. Recent advances in Large Language Models
(LLMs) offer a promising solution by automating key steps in the
survey-generation pipeline, such as retrieval, structuring, and summarization.
However, existing LLM-based approaches often struggle with maintaining
coherence across long, multi-section surveys and providing comprehensive
citation coverage. To address these limitations, we introduce SurveyGen-I, an
automatic survey generation framework that combines coarse-to-fine retrieval,
adaptive planning, and memory-guided generation. SurveyGen-I first performs
survey-level retrieval to construct the initial outline and writing plan, and
then dynamically refines both during generation through a memory mechanism that
stores previously written content and terminology, ensuring coherence across
subsections. When the system detects insufficient context, it triggers
fine-grained subsection-level retrieval. During generation, SurveyGen-I
leverages this memory mechanism to maintain coherence across subsections.
Experiments across four scientific domains demonstrate that SurveyGen-I
consistently outperforms previous works in content quality, consistency, and
citation coverage.

</details>


### [92] [Beyond Semantic Similarity: Reducing Unnecessary API Calls via Behavior-Aligned Retriever](https://arxiv.org/abs/2508.14323)
*Yixin Chen,Ying Xiong,Shangyu Wu,Yufei Cui,Xue Liu,Nan Guan,Chun Jason Xue*

Main category: cs.CL

TL;DR: 该研究提出了一种名为BAR的方法，通过提供行为一致的示例来提高工具增强LLM的函数调用准确性，从而降低成本和提高效率。


<details>
  <summary>Details</summary>
Motivation: 现有方法在提高LLM的函数调用准确性方面存在训练开销高和样本不一致的问题，BAR旨在解决这些问题。

Method: 训练了一个行为对齐检索器（BAR），使用对比学习框架和自定义的正/负样本对以及双负对比损失来检索行为一致的示例。

Result: 实验表明，该方法显著减少了错误的函数调用，同时保持了高任务性能。

Conclusion: 该方法通过检索行为一致的示例来提高工具增强LLM的函数调用准确性，显著减少了错误调用，同时保持了任务性能，为工具增强LLM提供了一种经济高效的解决方案。

Abstract: Tool-augmented large language models (LLMs) leverage external functions to
extend their capabilities, but inaccurate function calls can lead to
inefficiencies and increased costs.Existing methods address this challenge by
fine-tuning LLMs or using demonstration-based prompting, yet they often suffer
from high training overhead and fail to account for inconsistent demonstration
samples, which misguide the model's invocation behavior. In this paper, we
trained a behavior-aligned retriever (BAR), which provides behaviorally
consistent demonstrations to help LLMs make more accurate tool-using decisions.
To train the BAR, we construct a corpus including different function-calling
behaviors, i.e., calling or non-calling.We use the contrastive learning
framework to train the BAR with customized positive/negative pairs and a
dual-negative contrastive loss, ensuring robust retrieval of behaviorally
consistent examples.Experiments demonstrate that our approach significantly
reduces erroneous function calls while maintaining high task performance,
offering a cost-effective and efficient solution for tool-augmented LLMs.

</details>


### [93] [ISCA: A Framework for Interview-Style Conversational Agents](https://arxiv.org/abs/2508.14344)
*Charles Welch,Allison Lahnala,Vasudha Varadarajan,Lucie Flek,Rada Mihalcea,J. Lomax Boyd,João Sedoc*

Main category: cs.CL

TL;DR: 提出了一种低计算、非生成性的对话代理系统，用于通过受控交互和定量分析来收集定性数据。该系统易于使用和自定义，并已通过两个案例研究进行了演示。


<details>
  <summary>Details</summary>
Motivation: 为了在定性和定量研究中收集数据，需要一种低计算的、非生成性的、易于访问的系统来促进对话代理。

Method: 提出了一种用于实现面试式对话代理的低计算非生成系统，该系统可以通过受控交互和定量分析来促进定性数据收集。

Result: 该系统可以通过在线管理面板进行调整，以创建新的访谈。报告了两个案例研究：一个用于 COVID-19 的表达性访谈系统，以及一个用于调查公众对新兴神经技术的看法的半结构化访谈。

Conclusion: 该系统易于通过在线管理面板进行调整，无需编码即可创建新访谈，并且可以构建在开源代码之上以实现附加功能。

Abstract: We present a low-compute non-generative system for implementing
interview-style conversational agents which can be used to facilitate
qualitative data collection through controlled interactions and quantitative
analysis. Use cases include applications to tracking attitude formation or
behavior change, where control or standardization over the conversational flow
is desired. We show how our system can be easily adjusted through an online
administrative panel to create new interviews, making the tool accessible
without coding. Two case studies are presented as example applications, one
regarding the Expressive Interviewing system for COVID-19 and the other a
semi-structured interview to survey public opinion on emerging neurotechnology.
Our code is open-source, allowing others to build off of our work and develop
extensions for additional functionality.

</details>


### [94] [ZPD-SCA: Unveiling the Blind Spots of LLMs in Assessing Students' Cognitive Abilities](https://arxiv.org/abs/2508.14377)
*Wenhan Dong,Zhen Sun,Yuemeng Zhao,Zifan Peng,Jun Wu,Jingyi Zheng,Yule Liu,Xinlei He,Yu Wang,Ruiming Wang,Xinyi Huang,Lei Mo*

Main category: cs.CL

TL;DR: LLM在中国阅读理解评估方面有潜力但需改进，零样本能力弱，上下文学习可提升。ZPD-SCA基准可用于评估和改进LLM在教育应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究未能充分探索LLM在评估阅读材料与学生发展阶段的认知一致性方面的能力，尤其是在中国语言教育背景下，这与“最近发展区”（ZPD）的教育原则相悖。

Method: 提出了一个新的基准测试ZPD-SCA，用于评估LLM在中国语言教育中根据学生认知能力（SCA）评估阅读材料难度。该基准由60位特级教师（全国排名前0.15%）进行标注。

Result: 在零样本学习场景下，LLM表现不佳，甚至低于随机猜测。通过提供上下文示例，LLM的性能显著提高，准确率接近翻倍。然而，即使是表现最好的模型也存在系统性的偏差，并且在不同体裁上的表现存在显著差异。

Conclusion: LLM在评估阅读材料的认知难度方面具有潜力，但目前仍存在局限性，尤其是在零样本学习场景和跨不同体裁的表现上。需要进一步的训练和优化来提升其在教育场景下的表现。

Abstract: Large language models (LLMs) have demonstrated potential in educational
applications, yet their capacity to accurately assess the cognitive alignment
of reading materials with students' developmental stages remains insufficiently
explored. This gap is particularly critical given the foundational educational
principle of the Zone of Proximal Development (ZPD), which emphasizes the need
to match learning resources with Students' Cognitive Abilities (SCA). Despite
the importance of this alignment, there is a notable absence of comprehensive
studies investigating LLMs' ability to evaluate reading comprehension
difficulty across different student age groups, especially in the context of
Chinese language education. To fill this gap, we introduce ZPD-SCA, a novel
benchmark specifically designed to assess stage-level Chinese reading
comprehension difficulty. The benchmark is annotated by 60 Special Grade
teachers, a group that represents the top 0.15% of all in-service teachers
nationwide. Experimental results reveal that LLMs perform poorly in zero-shot
learning scenarios, with Qwen-max and GLM even falling below the probability of
random guessing. When provided with in-context examples, LLMs performance
improves substantially, with some models achieving nearly double the accuracy
of their zero-shot baselines. These results reveal that LLMs possess emerging
abilities to assess reading difficulty, while also exposing limitations in
their current training for educationally aligned judgment. Notably, even the
best-performing models display systematic directional biases, suggesting
difficulties in accurately aligning material difficulty with SCA. Furthermore,
significant variations in model performance across different genres underscore
the complexity of task. We envision that ZPD-SCA can provide a foundation for
evaluating and improving LLMs in cognitively aligned educational applications.

</details>


### [95] [Credence Calibration Game? Calibrating Large Language Models through Structured Play](https://arxiv.org/abs/2508.14390)
*Ke Fang,Tianyi Zhao,Lu Cheng*

Main category: cs.CL

TL;DR: LLM在关键领域的部署需要准确的置信度估计。本研究提出了一种新颖的基于“信念校准游戏”的提示框架，通过反馈驱动的交互和性能摘要来动态改进LLM校准，无需额外的监督或参数更新，并在实验中取得了持续的改进。


<details>
  <summary>Details</summary>
Motivation: 随着LLM越来越多地应用于决策关键领域，确保其置信度估计与其实际正确性相符变得至关重要。现有的校准方法大多侧重于事后调整或辅助模型训练，但这些方法通常需要额外的监督或参数更新。

Method: 提出了一种受“信念校准游戏”启发的、基于提示的校准框架。该框架通过结构化的交互循环，根据LLM预测置信度与正确性的一致性来提供反馈，并通过反馈驱动的提示和先前性能的自然语言摘要来动态改进模型校准。

Result: 在跨模型和游戏配置的广泛实验中，该框架在评估指标上显示出持续的改进，证明了基于游戏的提示作为LLM校准的有效策略的潜力。

Conclusion: 该研究提出了一个新颖的基于提示的校准框架，该框架受“信念校准游戏”的启发，并通过反馈驱动的提示和先前性能的自然语言摘要来动态改进模型校准。实验证明了该方法在LLM校准方面的有效性。

Abstract: As Large Language Models (LLMs) are increasingly deployed in
decision-critical domains, it becomes essential to ensure that their confidence
estimates faithfully correspond to their actual correctness. Existing
calibration methods have primarily focused on post-hoc adjustments or auxiliary
model training; however, many of these approaches necessitate additional
supervision or parameter updates. In this work, we propose a novel prompt-based
calibration framework inspired by the Credence Calibration Game. Our method
establishes a structured interaction loop wherein LLMs receive feedback based
on the alignment of their predicted confidence with correctness. Through
feedback-driven prompting and natural language summaries of prior performance,
our framework dynamically improves model calibration. Extensive experiments
across models and game configurations demonstrate consistent improvements in
evaluation metrics. Our results highlight the potential of game-based prompting
as an effective strategy for LLM calibration. Code and data are available at
https://anonymous.4open.science/r/LLM-Calibration/.

</details>


### [96] [DEPTH: Hallucination-Free Relation Extraction via Dependency-Aware Sentence Simplification and Two-tiered Hierarchical Refinement](https://arxiv.org/abs/2508.14391)
*Yupei Yang,Fan Feng,Lin Yang,Wanxi Deng,Lin Qu,Biwei Huang,Shikui Tu,Lei Xu*

Main category: cs.CL

TL;DR:  DEPTH框架通过句子简化和分层细化来解决大型语言模型在关系提取中的幻觉问题，显著提高了准确性和F1分数。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型在关系提取中存在幻觉问题，尤其是在处理复杂句子结构或语义时，会导致错误的预测，损害结构化知识的完整性。为了解决这些挑战，需要一个能够提高关系提取准确性和可靠性的框架。

Method:  DEPTH框架通过两个阶段进行关系提取：1. 提炼模块利用实体对的依赖路径提取关系，将句子简化为保留关键语义的最小关系上下文，以减少句法噪声；2. 精炼模块整合局部预测，并基于对句子的整体理解进行修正，以纠正遗漏和不一致之处。此外，还引入了因果驱动的奖励模型，通过解开虚假关联来缓解奖励攻击，从而通过人类反馈的强化学习实现稳健的微调。

Result:  DEPTH框架将平均幻觉率降低到7.0%，并将平均F1分数提高了17.2%，在六个基准测试中均优于现有最先进的基线。

Conclusion:  DEPTH框架通过整合依赖感知句子简化和两阶段分层细化，成功解决了大型语言模型在关系提取中的幻觉问题，将平均幻觉率降低至7.0%，并将平均F1分数提高了17.2%，在六个基准测试中均优于现有最先进的方法。

Abstract: Relation extraction enables the construction of structured knowledge for many
downstream applications. While large language models (LLMs) have shown great
promise in this domain, most existing methods concentrate on relation
classification, which predicts the semantic relation type between a related
entity pair. However, we observe that LLMs often struggle to reliably determine
whether a relation exists, especially in cases involving complex sentence
structures or intricate semantics, which leads to spurious predictions. Such
hallucinations can introduce noisy edges in knowledge graphs, compromising the
integrity of structured knowledge and downstream reliability. To address these
challenges, we propose DEPTH, a framework that integrates Dependency-aware
sEntence simPlification and Two-tiered Hierarchical refinement into the
relation extraction pipeline. Given a sentence and its candidate entity pairs,
DEPTH operates in two stages: (1) the Grounding module extracts relations for
each pair by leveraging their shortest dependency path, distilling the sentence
into a minimal yet coherent relational context that reduces syntactic noise
while preserving key semantics; (2) the Refinement module aggregates all local
predictions and revises them based on a holistic understanding of the sentence,
correcting omissions and inconsistencies. We further introduce a
causality-driven reward model that mitigates reward hacking by disentangling
spurious correlations, enabling robust fine-tuning via reinforcement learning
with human feedback. Experiments on six benchmarks demonstrate that DEPTH
reduces the average hallucination rate to 7.0\% while achieving a 17.2\%
improvement in average F1 score over state-of-the-art baselines.

</details>


### [97] [Cognitive Surgery: The Awakening of Implicit Territorial Awareness in LLMs](https://arxiv.org/abs/2508.14408)
*Yinghan Zhou,Weifeng Zhu,Juan Wen,Wanli Peng,Zhengxian Wu,Yiming Xue*

Main category: cs.CL

TL;DR: 该研究提出了一种名为“认知手术”的方法，通过唤醒大型语言模型（LLM）的“隐式地盘意识”，显著提高了其在单独判断文本来源时的准确率，平均准确率超过80%。


<details>
  <summary>Details</summary>
Motivation: 旨在系统性地分析在个体呈现范式（IPP）下，大型语言模型（LLM）在区分自身生成文本和他人生成文本时表现不佳的原因，并提出一种解决方案来提高其在该场景下的表现。

Method: 提出了一种名为认知手术（CoSur）的新框架，该框架包含四个主要模块：表示提取、地盘构建、作者身份区分和认知编辑，用于唤醒大型语言模型（LLM）的隐式地盘意识（ITA），以解决在个体呈现范式（IPP）下区分自身与其他生成文本的困难。

Result: 所提出的认知手术（CoSur）框架成功提高了三个不同LLM在个体呈现范式（IPP）下的性能，平均准确率分别达到83.25%、66.19%和88.01%。

Conclusion: 研究表明，通过认知手术（CoSur）框架可以唤醒大型语言模型（LLM）的隐式地盘意识（ITA），从而在个体呈现范式（IPP）下提高其区分自身与其他生成文本的能力，实验结果在三个不同的LLM上分别达到了83.25%、66.19%和88.01%的平均准确率。

Abstract: Large language models (LLMs) have been shown to possess a degree of
self-recognition capability-the ability to identify whether a given text was
generated by themselves. Prior work has demonstrated that this capability is
reliably expressed under the Pair Presentation Paradigm (PPP), where the model
is presented with two texts and asked to choose which one it authored. However,
performance deteriorates sharply under the Individual Presentation Paradigm
(IPP), where the model is given a single text to judge authorship. Although
this phenomenon has been observed, its underlying causes have not been
systematically analyzed. In this paper, we first replicate existing findings to
confirm that LLMs struggle to distinguish self- from other-generated text under
IPP. We then investigate the reasons for this failure and attribute it to a
phenomenon we term Implicit Territorial Awareness (ITA)-the model's latent
ability to distinguish self- and other-texts in representational space, which
remains unexpressed in its output behavior. To awaken the ITA of LLMs, we
propose Cognitive Surgery (CoSur), a novel framework comprising four main
modules: representation extraction, territory construction, authorship
discrimination and cognitive editing. Experimental results demonstrate that our
proposed method improves the performance of three different LLMs in the IPP
scenario, achieving average accuracies of 83.25%, 66.19%, and 88.01%,
respectively.

</details>


### [98] [Knowledge Graph-Infused Fine-Tuning for Structured Reasoning in Large Language Models](https://arxiv.org/abs/2508.14427)
*Wuyang Zhang,Yexin Tian,Xiandong Meng,Mengjie Wang,Junliang Du*

Main category: cs.CL

TL;DR: 通过知识图谱注入和图神经网络，并结合门控机制，提出了一种结构感知微调框架，有效解决了大型语言模型在结构化知识任务中的推理和语义理解问题，提升了模型性能和稳定性。


<details>
  <summary>Details</summary>
Motivation: 旨在解决大型语言模型在处理需要结构化知识的任务时，存在的推理链缺失和实体级语义理解不足的问题。

Method: 提出了一种基于知识图谱注入的微调算法框架，该框架结合了预训练语言模型和图神经网络，通过图谱嵌入和语言模型上下文表示的融合机制，并引入门控机制来动态平衡语言语义和结构知识的贡献，最后通过联合损失函数进行训练。

Result: 实验结果表明，该方法在实体识别、问答和语言生成等任务上表现出色，能够有效提高模型表示复杂语义单元、保持语义一致性和进行上下文逻辑建模的能力。

Conclusion: 该研究提出的结构感知微调框架通过知识图谱注入，显著提高了大型语言模型在处理结构化知识任务时的表现，增强了模型表示复杂语义单元、保持语义一致性和进行上下文逻辑建模的能力。

Abstract: This paper addresses the problems of missing reasoning chains and
insufficient entity-level semantic understanding in large language models when
dealing with tasks that require structured knowledge. It proposes a fine-tuning
algorithm framework based on knowledge graph injection. The method builds on
pretrained language models and introduces structured graph information for
auxiliary learning. A graph neural network is used to encode entities and their
relations, constructing a graph-based semantic representation. A fusion
mechanism is then designed to jointly model the knowledge graph embeddings with
the contextual representations from the language model. To enhance the
robustness of knowledge integration, a gating mechanism is introduced to
dynamically balance the contributions of linguistic semantics and structural
knowledge. This effectively mitigates conflicts between different
representational spaces. During training, a joint loss function is constructed
to account for both task performance and structural alignment objectives. This
helps improve the accuracy of entity prediction and semantic reasoning. The
study also includes a series of systematic sensitivity experiments. It
evaluates the effects of learning rate, graph coverage, and structural
perturbations on model performance. The results further validate the
effectiveness and stability of the proposed method across tasks such as entity
recognition, question answering, and language generation. Experimental findings
show that the proposed structure-aware fine-tuning framework significantly
enhances the model's ability to represent complex semantic units. It
demonstrates better semantic consistency and contextual logic modeling in
scenarios involving structural reasoning and entity extraction.

</details>


### [99] [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)
*NVIDIA,:,Aarti Basant,Abhijit Khairnar,Abhijit Paithankar,Abhinav Khattar,Adi Renduchintala,Adithya Renduchintala,Aditya Malte,Akhiad Bercovich,Akshay Hazare,Alejandra Rico,Aleksander Ficek,Alex Kondratenko,Alex Shaposhnikov,Ali Taghibakhshi,Amelia Barton,Ameya Sunil Mahabaleshwarkar,Amy Shen,Andrew Tao,Ann Guan,Anna Shors,Anubhav Mandarwal,Arham Mehta,Arun Venkatesan,Ashton Sharabiani,Ashwath Aithal,Ashwin Poojary,Ayush Dattagupta,Balaram Buddharaju,Banghua Zhu,Barnaby Simkin,Bilal Kartal,Bita Darvish Rouhani,Bobby Chen,Boris Ginsburg,Brandon Norick,Brian Yu,Bryan Catanzaro,Charles Wang,Charlie Truong,Chetan Mungekar,Chintan Patel,Chris Alexiuk,Christian Munley,Christopher Parisien,Dan Su,Daniel Afrimi,Daniel Korzekwa,Daniel Rohrer,Daria Gitman,David Mosallanezhad,Deepak Narayanan,Dima Rekesh,Dina Yared,Dmytro Pykhtar,Dong Ahn,Duncan Riach,Eileen Long,Elliott Ning,Eric Chung,Erick Galinkin,Evelina Bakhturina,Gargi Prasad,Gerald Shen,Haim Elisha,Harsh Sharma,Hayley Ross,Helen Ngo,Herman Sahota,Hexin Wang,Hoo Chang Shin,Hua Huang,Iain Cunningham,Igor Gitman,Ivan Moshkov,Jaehun Jung,Jan Kautz,Jane Polak Scowcroft,Jared Casper,Jimmy Zhang,Jinze Xue,Jocelyn Huang,Joey Conway,John Kamalu,Jonathan Cohen,Joseph Jennings,Julien Veron Vialard,Junkeun Yi,Jupinder Parmar,Kari Briski,Katherine Cheung,Katherine Luna,Keith Wyss,Keshav Santhanam,Kezhi Kong,Krzysztof Pawelec,Kumar Anik,Kunlun Li,Kushan Ahmadian,Lawrence McAfee,Laya Sleiman,Leon Derczynski,Luis Vega,Maer Rodrigues de Melo,Makesh Narsimhan Sreedhar,Marcin Chochowski,Mark Cai,Markus Kliegl,Marta Stepniewska-Dziubinska,Matvei Novikov,Mehrzad Samadi,Meredith Price,Meriem Boubdir,Michael Boone,Michael Evans,Michal Bien,Michal Zawalski,Miguel Martinez,Mike Chrzanowski,Mohammad Shoeybi,Mostofa Patwary,Namit Dhameja,Nave Assaf,Negar Habibi,Nidhi Bhatia,Nikki Pope,Nima Tajbakhsh,Nirmal Kumar Juluru,Oleg Rybakov,Oleksii Hrinchuk,Oleksii Kuchaiev,Oluwatobi Olabiyi,Pablo Ribalta,Padmavathy Subramanian,Parth Chadha,Pavlo Molchanov,Peter Dykas,Peter Jin,Piotr Bialecki,Piotr Januszewski,Pradeep Thalasta,Prashant Gaikwad,Prasoon Varshney,Pritam Gundecha,Przemek Tredak,Rabeeh Karimi Mahabadi,Rajen Patel,Ran El-Yaniv,Ranjit Rajan,Ria Cheruvu,Rima Shahbazyan,Ritika Borkar,Ritu Gala,Roger Waleffe,Ruoxi Zhang,Russell J. Hewett,Ryan Prenger,Sahil Jain,Samuel Kriman,Sanjeev Satheesh,Saori Kaji,Sarah Yurick,Saurav Muralidharan,Sean Narenthiran,Seonmyeong Bak,Sepehr Sameni,Seungju Han,Shanmugam Ramasamy,Shaona Ghosh,Sharath Turuvekere Sreenivas,Shelby Thomas,Shizhe Diao,Shreya Gopal,Shrimai Prabhumoye,Shubham Toshniwal,Shuoyang Ding,Siddharth Singh,Siddhartha Jain,Somshubra Majumdar,Stefania Alborghetti,Syeda Nahida Akter,Terry Kong,Tim Moon,Tomasz Hliwiak,Tomer Asida,Tony Wang,Twinkle Vashishth,Tyler Poon,Udi Karpas,Vahid Noroozi,Venkat Srinivasan,Vijay Korthikanti,Vikram Fugro,Vineeth Kalluru,Vitaly Kurin,Vitaly Lavrukhin,Wasi Uddin Ahmad,Wei Du,Wonmin Byeon,Ximing Lu,Xin Dong,Yashaswi Karnati,Yejin Choi,Yian Zhang,Ying Lin,Yonggan Fu,Yoshi Suhara,Zhen Dong,Zhiyu Li,Zhongbo Zhu,Zijia Chen*

Main category: cs.CL

TL;DR: Nemotron-Nano-9B-v2是一个混合Mamba-Transformer模型，通过用Mamba-2层替换Transformer的自注意力层，提高了推理吞吐量，并在推理任务中达到了最先进的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了提高语言模型在推理任务上的吞吐量，同时保持与同等规模模型相当的准确性。

Method: 本研究介绍了Nemotron-Nano-9B-v2，一个混合Mamba-Transformer语言模型。该模型在Nemotron-H架构的基础上，用Mamba-2层替换了大部分Transformer的自注意力层，以提高推理速度，特别是在生成长推理链时。研究人员首先在20万亿个token上预训练了一个120亿参数的模型（Nemotron-Nano-12B-v2-Base），采用了FP8训练方法。在对齐该模型后，研究人员使用Minitron策略对其进行压缩和蒸馏，目标是能在单个NVIDIA A10G GPU（22GiB内存，bfloat16精度）上处理高达128k的token。

Result: Nemotron-Nano-9B-v2模型实现了更高的推理吞吐量（在特定场景下高达6倍），同时在推理基准测试中与同等规模模型（如Qwen3-8B）相比，准确性相当或更好。

Conclusion: Nemotron-Nano-9B-v2在推理基准测试中取得了与同等规模模型相当或更好的准确性，同时在8k输入和16k输出令牌的推理场景下实现了高达6倍的吞吐量提升。

Abstract: We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model
designed to increase throughput for reasoning workloads while achieving
state-of-the-art accuracy compared to similarly-sized models.
Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the
majority of the self-attention layers in the common Transformer architecture
are replaced with Mamba-2 layers, to achieve improved inference speed when
generating the long thinking traces needed for reasoning. We create
Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model
(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.
After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to
compress and distill the model with the goal of enabling inference on up to
128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).
Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that
Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks
while achieving up to 6x higher inference throughput in reasoning settings like
8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,
Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with
the majority of our pre- and post-training datasets on Hugging Face.

</details>


### [100] [In2x at WMT25 Translation Task](https://arxiv.org/abs/2508.14472)
*Lei Pang,Hanyi Mao,Quanjia Xiao,HaiXiao Liu,Xiangyi Li*

Main category: cs.CL

TL;DR: In2x团队为WMT25任务提交了一个开放系统，重点是日本语翻译，旨在为低资源语言的机器翻译开发一个通用的LLM扩展范式。


<details>
  <summary>Details</summary>
Motivation: 探索将大型语言模型（LLMs）扩展到其他语言（特别是低资源或不常用语言）的可推广范式。

Method: 本文提出了一种可扩展的范式，包括数据构建方法和奖励模型设计，以将大型语言模型（LLMs）应用于其他语言。

Result: 本文是In2x研究团队为WMT25通用机器翻译共享任务提交的开放系统。

Conclusion: 该研究旨在为低资源或不常用语言的机器翻译提供一个可扩展的通用范式，重点是日本语相关任务。

Abstract: This paper presents the open-system submission by the In2x research team for
the WMT25 General Machine Translation Shared Task. Our submission focuses on
Japanese-related translation tasks, aiming to explore a generalizable paradigm
for extending large language models (LLMs) to other languages. This paradigm
encompasses aspects such as data construction methods and reward model design.
The ultimate goal is to enable large language model systems to achieve
exceptional performance in low-resource or less commonly spoken languages.

</details>


### [101] [Reasoning is about giving reasons](https://arxiv.org/abs/2508.14488)
*Krunal Shah,Dan Roth*

Main category: cs.CL

TL;DR: Transformers struggle to explain logical arguments. This paper introduces a Representation of the Logical Structure (RLS) that captures argument components, enabling better reasoning and explanations. The method achieves high accuracy on multiple datasets.


<details>
  <summary>Details</summary>
Motivation: Current transformer models struggle to articulate the 'reasons' behind logical arguments, lack interpretability, and are limited in their ability to handle different reasoning tasks like abduction or contradiction detection.

Method: The method involves identifying an intermediate representation called the Representation of the Logical Structure (RLS) that captures the logical atoms and rules of an argument. This RLS is then used to perform various reasoning tasks.

Result: The approach achieves high accuracy in identifying and extracting the logical structure of arguments from three popular reasoning datasets, significantly improving explanation generation and reasoning capabilities.

Conclusion: The paper proposes a new approach to represent the logical structure of natural language arguments, which allows for more accurate reasoning and explanation generation.

Abstract: Convincing someone of the truth value of a premise requires understanding and
articulating the core logical structure of the argument which proves or
disproves the premise. Understanding the logical structure of an argument
refers to understanding the underlying "reasons" which make up the proof or
disproof of the premise - as a function of the "logical atoms" in the argument.
While it has been shown that transformers can "chain" rules to derive simple
arguments, the challenge of articulating the "reasons" remains. Not only do
current approaches to chaining rules suffer in terms of their interpretability,
they are also quite constrained in their ability to accommodate extensions to
theoretically equivalent reasoning tasks - a model trained to chain rules
cannot support abduction or identify contradictions. In this work we suggest
addressing these shortcomings by identifying an intermediate representation
(which we call the Representation of the Logical Structure (RLS) of the
argument) that possesses an understanding of the logical structure of a natural
language argument - the logical atoms in the argument and the rules
incorporating them. Given the logical structure, reasoning is deterministic and
easy to compute. Therefore, our approach supports all forms of reasoning that
depend on the logical structure of the natural language argument, including
arbitrary depths of reasoning, on-the-fly mistake rectification and interactive
discussion with respect to an argument. We show that we can identify and
extract the logical structure of natural language arguments in three popular
reasoning datasets with high accuracies, thus supporting explanation generation
and extending the reasoning capabilities significantly.

</details>


### [102] [EmoTale: An Enacted Speech-emotion Dataset in Danish](https://arxiv.org/abs/2508.14548)
*Maja J. Hjuler,Harald V. Skat-Rørdam,Line H. Clemmensen,Sneha Das*

Main category: cs.CL

TL;DR: A new emotional speech corpus (EmoTale) for Danish and English was created. SER models using SSLM embeddings outperformed hand-crafted features, achieving 64.1% UAR.


<details>
  <summary>Details</summary>
Motivation: There is a lack of functional emotional speech datasets for smaller languages like Danish. The EmoTale corpus was created to address this gap and provide a resource for speech emotion recognition research.

Method: The study created the EmoTale corpus with Danish and English speech recordings and emotion annotations. Speech emotion recognition (SER) models were developed using self-supervised speech model (SSLM) embeddings and the openSMILE feature extractor. The models were evaluated using leave-one-speaker-out cross-validation.

Result: The best SER model achieved an unweighted average recall (UAR) of 64.1% on the EmoTale corpus, demonstrating the predictive power of the dataset and indicating performance comparable to the existing Danish Emotional Speech (DES) corpus.

Conclusion: self-supervised speech model embeddings show superior performance compared to hand-crafted features for speech emotion recognition in Danish and English.

Abstract: While multiple emotional speech corpora exist for commonly spoken languages,
there is a lack of functional datasets for smaller (spoken) languages, such as
Danish. To our knowledge, Danish Emotional Speech (DES), published in 1997, is
the only other database of Danish emotional speech. We present EmoTale; a
corpus comprising Danish and English speech recordings with their associated
enacted emotion annotations. We demonstrate the validity of the dataset by
investigating and presenting its predictive power using speech emotion
recognition (SER) models. We develop SER models for EmoTale and the reference
datasets using self-supervised speech model (SSLM) embeddings and the openSMILE
feature extractor. We find the embeddings superior to the hand-crafted
features. The best model achieves an unweighted average recall (UAR) of 64.1%
on the EmoTale corpus using leave-one-speaker-out cross-validation, comparable
to the performance on DES.

</details>


### [103] [Towards Skeletal and Signer Noise Reduction in Sign Language Production via Quaternion-Based Pose Encoding and Contrastive Learning](https://arxiv.org/abs/2508.14574)
*Guilhem Fauré,Mostafa Sadeghi,Sam Bigeard,Slim Ouni*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: One of the main challenges in neural sign language production (SLP) lies in
the high intra-class variability of signs, arising from signer morphology and
stylistic variety in the training data. To improve robustness to such
variations, we propose two enhancements to the standard Progressive
Transformers (PT) architecture (Saunders et al., 2020). First, we encode poses
using bone rotations in quaternion space and train with a geodesic loss to
improve the accuracy and clarity of angular joint movements. Second, we
introduce a contrastive loss to structure decoder embeddings by semantic
similarity, using either gloss overlap or SBERT-based sentence similarity,
aiming to filter out anatomical and stylistic features that do not convey
relevant semantic information. On the Phoenix14T dataset, the contrastive loss
alone yields a 16% improvement in Probability of Correct Keypoint over the PT
baseline. When combined with quaternion-based pose encoding, the model achieves
a 6% reduction in Mean Bone Angle Error. These results point to the benefit of
incorporating skeletal structure modeling and semantically guided contrastive
objectives on sign pose representations into the training of Transformer-based
SLP models.

</details>


### [104] [Filling the Gap for Uzbek: Creating Translation Resources for Southern Uzbek](https://arxiv.org/abs/2508.14586)
*Mukhammadsaid Mamasaidov,Azizullah Aral,Abror Shopulatov,Mironshoh Inomjonov*

Main category: cs.CL

TL;DR: 南方乌兹别克语（uzs）在阿富汗被500万人使用，但NLP研究不足。我们发布了包括FLORES+开发集、平行语料库和微调NLLB-200模型（lutfiy）在内的新资源，并提出了一种改进形态边界处理的后处理方法。


<details>
  <summary>Details</summary>
Motivation: 南方乌兹别克语（uzs）是一种有500万使用者但未被充分研究的土耳其语方言，该研究旨在为南方乌兹别克语的机器翻译提供新的资源。

Method: 提出了一个后处理方法，用于恢复阿拉伯文半空格符，以改善词法边界的处理。

Result: 构建了一个包含997个句子FLORES+开发集、39,994个平行句（来自词典、文学和网络）的数据集，并微调了NLLB-200模型（lutfiy）。提出的后处理方法提高了对词法边界的处理。

Conclusion: 该研究发布了南方乌兹别克语的机器翻译资源，包括FLORES+开发集、平行语料库和微调后的NLLB-200模型（lutfiy），并提出了一种后处理方法来恢复阿拉伯文半空格符，以改善词法边界处理。所有资源均公开，以支持南方乌兹别克语及其他低资源语言的未来研究。

Abstract: Southern Uzbek (uzs) is a Turkic language variety spoken by around 5 million
people in Afghanistan and differs significantly from Northern Uzbek (uzn) in
phonology, lexicon, and orthography. Despite the large number of speakers,
Southern Uzbek is underrepresented in natural language processing. We present
new resources for Southern Uzbek machine translation, including a 997-sentence
FLORES+ dev set, 39,994 parallel sentences from dictionary, literary, and web
sources, and a fine-tuned NLLB-200 model (lutfiy). We also propose a
post-processing method for restoring Arabic-script half-space characters, which
improves handling of morphological boundaries. All datasets, models, and tools
are released publicly to support future work on Southern Uzbek and other
low-resource languages.

</details>


### [105] [Continuous sentiment scores for literary and multilingual contexts](https://arxiv.org/abs/2508.14620)
*Laurits Lyngbaek,Pascale Feldkamp,Yuri Bizzoni,Kristoffer Nielbo,Kenneth Enevoldsen*

Main category: cs.CL

TL;DR: 提出了一种新颖的连续情感评分方法，该方法基于概念向量投影，并在多语言文学数据上进行训练，以克服传统方法在文学作品情感分析中的局限性，并在英语和丹麦语文本上取得了优于现有工具的结果。


<details>
  <summary>Details</summary>
Motivation: 传统的基于词典的工具通常表现不佳，尤其是在低资源语言中，而 transformer 模型通常输出粗糙的分类标签，限制了细粒度分析。然而，文学作品中的比喻语言、风格模糊性和情感唤起策略给情感分析带来了独特的挑战。

Method: 提出了一种新颖的基于概念向量投影的连续情感评分方法，并在多语言文学数据上进行训练。

Result: 所提出的方法能够更有效地捕捉跨越体裁、语言和历史时期的细微情感表达。

Conclusion: 该方法在英语和丹麦语文本上优于现有工具，生成的情感分数分布与人类评分非常匹配，能够实现更准确的文学作品分析和情感弧建模。

Abstract: Sentiment Analysis is widely used to quantify sentiment in text, but its
application to literary texts poses unique challenges due to figurative
language, stylistic ambiguity, as well as sentiment evocation strategies.
Traditional dictionary-based tools often underperform, especially for
low-resource languages, and transformer models, while promising, typically
output coarse categorical labels that limit fine-grained analysis. We introduce
a novel continuous sentiment scoring method based on concept vector projection,
trained on multilingual literary data, which more effectively captures nuanced
sentiment expressions across genres, languages, and historical periods. Our
approach outperforms existing tools on English and Danish texts, producing
sentiment scores whose distribution closely matches human ratings, enabling
more accurate analysis and sentiment arc modeling in literature.

</details>


### [106] [Improving in-context learning with a better scoring function](https://arxiv.org/abs/2508.14685)
*Omar Naim,Swarnadeep Bhar,Jérôme Bolte,Nicholas Asher*

Main category: cs.CL

TL;DR: A new method called SSA improves LLMs' ability to learn by analogy, especially on complex reasoning tasks.


<details>
  <summary>Details</summary>
Motivation: LLMs exhibit in-context learning (ICL), but recent studies show limitations on tasks with first-order quantifiers and linear functions. This paper examines these limitations.

Method: The paper identifies Softmax as a contributing factor to ICL limitations and proposes scaled signed averaging (SSA) as a novel alternative.

Result: SSA dramatically improves performance on the target tasks.

Conclusion: SSA matches or exceeds Softmax-based counterparts across a variety of linguistic probing tasks for both encoder-only and decoder-only transformers.

Abstract: Large language models (LLMs) exhibit a remarkable capacity to learn by
analogy, known as in-context learning (ICL). However, recent studies have
revealed limitations in this ability. In this paper, we examine these
limitations on tasks involving first-order quantifiers such as {\em all} and
{\em some}, as well as on ICL with linear functions. We identify Softmax, the
scoring function in attention mechanism, as a contributing factor to these
constraints. To address this, we propose \textbf{scaled signed averaging
(SSA)}, a novel alternative to Softmax. Empirical results show that SSA
dramatically improves performance on our target tasks. Furthermore, we evaluate
both encoder-only and decoder-only transformers models with SSA, demonstrating
that they match or exceed their Softmax-based counterparts across a variety of
linguistic probing tasks.

</details>


### [107] [ShizhenGPT: Towards Multimodal LLMs for Traditional Chinese Medicine](https://arxiv.org/abs/2508.14706)
*Junying Chen,Zhenyang Cai,Zhiheng Liu,Yunjin Yang,Rongsheng Wang,Qingying Xiao,Xiangyi Feng,Zhan Su,Jing Guo,Xiang Wan,Guangjun Yu,Haizhou Li,Benyou Wang*

Main category: cs.CL

TL;DR: ShizhenGPT是第一个针对传统中医（TCM）的多模态大语言模型，它通过整合大量文本和多模态数据（图像、音频、生理信号）解决了数据稀缺和多模态诊断的挑战。该模型在TCM知识和多模态推理方面表现出色，并在TCM视觉理解方面超越了现有模型，为实现整体多模态感知和诊断铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 由于高质量TCM数据的稀缺以及TCM诊断固有的、涉及视觉、听觉、嗅觉和脉诊等多模态的性质（这些感官丰富的模式超出了传统LLM的范围），大语言模型（LLM）在TCM领域的潜力在很大程度上仍未被探索。

Method: 提出了一种名为ShizhenGPT的多模态大语言模型，用于中医（TCM）领域。为了解决数据稀缺问题，我们整理了迄今为止最大的TCM数据集，包含100GB以上的文本和200GB以上的、包括1.2M图像、200小时音频和生理信号的多模态数据。ShizhenGPT经过预训练和指令调整，以实现深层TCM知识和多模态推理。

Result: 实验证明，ShizhenGPT在TCM视觉理解方面领先于现有的多模态LLM，并展示了跨越声音、脉搏、气味和视觉等模态的统一感知能力。

Conclusion: ShizhenGPT在TCM领域表现出色，超越了同等规模的LLM，并与大型专有模型相媲美。它在TCM视觉理解方面领先于现有的多模态LLM，并展示了跨越声音、脉搏、气味和视觉等模态的统一感知能力，为TCM中整体多模态感知和诊断铺平了道路。

Abstract: Despite the success of large language models (LLMs) in various domains, their
potential in Traditional Chinese Medicine (TCM) remains largely underexplored
due to two critical barriers: (1) the scarcity of high-quality TCM data and (2)
the inherently multimodal nature of TCM diagnostics, which involve looking,
listening, smelling, and pulse-taking. These sensory-rich modalities are beyond
the scope of conventional LLMs. To address these challenges, we present
ShizhenGPT, the first multimodal LLM tailored for TCM. To overcome data
scarcity, we curate the largest TCM dataset to date, comprising 100GB+ of text
and 200GB+ of multimodal data, including 1.2M images, 200 hours of audio, and
physiological signals. ShizhenGPT is pretrained and instruction-tuned to
achieve deep TCM knowledge and multimodal reasoning. For evaluation, we collect
recent national TCM qualification exams and build a visual benchmark for
Medicinal Recognition and Visual Diagnosis. Experiments demonstrate that
ShizhenGPT outperforms comparable-scale LLMs and competes with larger
proprietary models. Moreover, it leads in TCM visual understanding among
existing multimodal LLMs and demonstrates unified perception across modalities
like sound, pulse, smell, and vision, paving the way toward holistic multimodal
perception and diagnosis in TCM. Datasets, models, and code are publicly
available. We hope this work will inspire further exploration in this field.

</details>


### [108] [The Digital Sous Chef -- A Comparative Study on Fine-Tuning Language Models for Recipe Generation](https://arxiv.org/abs/2508.14718)
*Shubham Pundhir,Ganesh Bagler*

Main category: cs.CL

TL;DR: 我们提出了一种新的食谱生成方法，通过定制标记化策略提升了大型Transformer模型（GPT-2 774M）的表现，相比传统模型在准确性和流畅性上均有显著提升，并为未来研究指明了方向。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有通用标记化器在处理食谱文本时丢失关键结构和数值精度的问题，我们提出了一种新的标记化策略，以提高文本生成任务的准确性和领域特异性。

Method: 通过引入包含23个分数标记和自定义结构标记的定制标记化策略，增强了GPT-2大模型（774M）在RecipeDB的5-cuisine语料库上的性能，并与GPT-2小模型及LSTM/RNN基线进行了比较。

Result: 大型Transformer模型（GPT-2 774M）相比最佳循环基线模型，BERTScore（F1）提高了超过20%（0.92 vs 0.72），困惑度降低了69.8%。

Conclusion: 大型Transformer模型在食谱生成任务上表现出显著优势，但仍需解决事实准确性等问题，为未来研究指明方向。

Abstract: We established a rigorous benchmark for text-based recipe generation, a
fundamental task in natural language generation. We present a comprehensive
comparative study contrasting a fine-tuned GPT-2 large (774M) model against the
GPT-2 small (124M) model and traditional LSTM/RNN baselines on the 5-cuisine
corpus from RecipeDB. Our key contribution is a targeted tokenization strategy
that augments the vocabulary with 23 common fraction tokens and custom
structural markers. This approach addresses a critical limitation of generic
tokenizers by preserving essential recipe structures and precise numerical
quantities, thereby enhancing domain specificity. Performance is evaluated
using a comprehensive suite of seven automatic metrics spanning fluency
(BLEU-4, METEOR), coherence (ROUGE-L), semantic relevance (BERTScore), and
diversity. Our experiments show that the large transformer-based approach
yields a >20% relative improvement in BERTScore (F1) (0.92 vs 0.72) over the
best recurrent baseline, while reducing perplexity by 69.8%. We conclude with a
discussion of remaining challenges, particularly regarding factual accuracy,
and outline how this foundational study paves the way for integrating
real-world constraints and multi-modal inputs in advanced recipe generation
research.

</details>


### [109] [Transplant Then Regenerate: A New Paradigm for Text Data Augmentation](https://arxiv.org/abs/2508.14723)
*Guangzhan Wang,Hongyu Zhang,Beijun Shen,Xiaodong Gu*

Main category: cs.CL

TL;DR: LMTransplant是一种新的文本增强范式，利用LLM来生成更多样化和更具创造性的内容级变体，同时保留原始文本的核心属性。


<details>
  <summary>Details</summary>
Motivation: 由于LLMs的“知识涌现”能力，它们在文本增强方面得到了提升，但控制这些输出的风格和结构仍然是一个挑战，需要细致的提示工程。

Method: LMTransplant提出了一种新颖的文本增强范式，其核心思想是“移植-然后-再生”：将种子文本整合到LLM扩展的上下文中，并要求LLM根据扩展后的上下文重新生成变体。

Result: LMTransplant在各种文本相关任务的评估中，展示了其优于现有文本增强方法的性能，并且在增强数据量增长时表现出卓越的可扩展性。

Conclusion: LMTransplant可以通过将种子文本整合到LLM扩展的上下文中，并要求LLM根据扩展后的上下文重新生成变体，从而在文本增强方面取得优于现有方法的性能，并具有出色的可扩展性。

Abstract: Data augmentation is a critical technique in deep learning. Traditional
methods like Back-translation typically focus on lexical-level rephrasing,
which primarily produces variations with the same semantics. While large
language models (LLMs) have enhanced text augmentation by their "knowledge
emergence" capability, controlling the style and structure of these outputs
remains challenging and requires meticulous prompt engineering. In this paper,
we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs.
The core idea of LMTransplant is transplant-then-regenerate: incorporating seed
text into a context expanded by LLM, and asking the LLM to regenerate a variant
based on the expanded context. This strategy allows the model to create more
diverse and creative content-level variants by fully leveraging the knowledge
embedded in LLMs, while preserving the core attributes of the original text. We
evaluate LMTransplant across various text-related tasks, demonstrating its
superior performance over existing text augmentation methods. Moreover,
LMTransplant demonstrates exceptional scalability as the size of augmented data
grows.

</details>


### [110] [Evaluating Multilingual and Code-Switched Alignment in LLMs via Synthetic Natural Language Inference](https://arxiv.org/abs/2508.14735)
*Samir Abdaljalil,Erchin Serpedin,Khalid Qaraqe,Hasan Kurban*

Main category: cs.CL

TL;DR: LLMs在多语言NLI中，混合语言（代码转换）可能通过词汇变化提升模型性能，这为增强多语言模型鲁棒性提供了新思路。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在多语言环境下应用广泛，但其跨语言的逻辑一致性和对齐能力仍需深入研究。评估LLMs在不同语言和代码转换场景下的自然语言推断能力，以理解其跨语言推理的潜力和局限性。

Method: 提出一个受控的多语言自然语言推断（NLI）评估框架，生成合成的、基于逻辑的“前提-假设”对，并将其翻译成多种类型的语言。该框架允许在单一语言和混合语言（代码转换）条件下进行测试，并通过基于嵌入的相似性分析和跨语言对齐可视化来验证语义的保留。

Result: 发现代码转换不仅不会降低LLMs在多语言NLI任务中的性能，反而可能通过引入词汇变化作为正则化信号来提升模型性能。验证了翻译后数据的语义保持度，并指出代码转换是提升多语言模型鲁棒性的一个有前景的方法。

Conclusion: LLMs在多语言NLI任务中，混合语言（代码转换）不仅不降低性能，反而可能通过词汇变化作为一种正则化信号来提升鲁棒性。这揭示了当前LLM跨语言推理的潜力和局限性，并指出了混合语言在提升多语言模型鲁棒性方面的潜力。

Abstract: Large language models (LLMs) are increasingly applied in multilingual
contexts, yet their capacity for consistent, logically grounded alignment
across languages remains underexplored. We present a controlled evaluation
framework for multilingual natural language inference (NLI) that generates
synthetic, logic-based premise-hypothesis pairs and translates them into a
typologically diverse set of languages. This design enables precise control
over semantic relations and allows testing in both monolingual and
mixed-language (code-switched) conditions. Surprisingly, code-switching does
not degrade, and can even improve, performance, suggesting that
translation-induced lexical variation may serve as a regularization signal. We
validate semantic preservation through embedding-based similarity analyses and
cross-lingual alignment visualizations, confirming the fidelity of translated
pairs. Our findings expose both the potential and the brittleness of current
LLM cross-lingual reasoning, and identify code-switching as a promising lever
for improving multilingual robustness. Code available at:
https://github.com/KurbanIntelligenceLab/nli-stress-testing

</details>


### [111] [TransLLM: A Unified Multi-Task Foundation Framework for Urban Transportation via Learnable Prompting](https://arxiv.org/abs/2508.14782)
*Jiaming Leng,Yunying Bi,Chuan Qin,Bing Yin,Yanyong Zhang,Chao Wang*

Main category: cs.CL

TL;DR: TransLLM 是一个创新的框架，它将时空建模与大型语言模型相结合，并通过动态生成的提示来提高城市交通任务的预测准确性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有方法在城市交通领域（如交通预测、电动汽车充电需求预测和出租车调度）中，小型深度学习模型任务特定且数据需求大、泛化性有限，以及大型语言模型难以处理结构化时空数据和数值推理的局限性。

Method: TransLLM 是一个统一的基础框架，通过可学习的提示组合将时空建模与大型语言模型相结合。它包括一个轻量级时空编码器（利用扩张时间卷积和双邻接图注意力网络捕获复杂依赖关系）以及一个通过强化学习训练的新颖实例级提示路由机制，该机制可根据输入特征动态地个性化提示。

Result: TransLLM 框架通过将时空模式编码为上下文表示，动态地组合个性化提示以指导 LLM 推理，并通过专门的输出层投影结果表示来生成特定任务的预测。实验表明，TransLLM 在七个数据集和三个任务上的表现优于现有模型。

Conclusion: TransLLM 在监督和零样本设置中都表现出卓越的有效性，在回归和规划问题上均优于十个基线模型，展示了强大的泛化能力和跨任务适应性。

Abstract: Urban transportation systems encounter diverse challenges across multiple
tasks, such as traffic forecasting, electric vehicle (EV) charging demand
prediction, and taxi dispatch. Existing approaches suffer from two key
limitations: small-scale deep learning models are task-specific and
data-hungry, limiting their generalizability across diverse scenarios, while
large language models (LLMs), despite offering flexibility through natural
language interfaces, struggle with structured spatiotemporal data and numerical
reasoning in transportation domains. To address these limitations, we propose
TransLLM, a unified foundation framework that integrates spatiotemporal
modeling with large language models through learnable prompt composition. Our
approach features a lightweight spatiotemporal encoder that captures complex
dependencies via dilated temporal convolutions and dual-adjacency graph
attention networks, seamlessly interfacing with LLMs through structured
embeddings. A novel instance-level prompt routing mechanism, trained via
reinforcement learning, dynamically personalizes prompts based on input
characteristics, moving beyond fixed task-specific templates. The framework
operates by encoding spatiotemporal patterns into contextual representations,
dynamically composing personalized prompts to guide LLM reasoning, and
projecting the resulting representations through specialized output layers to
generate task-specific predictions. Experiments across seven datasets and three
tasks demonstrate the exceptional effectiveness of TransLLM in both supervised
and zero-shot settings. Compared to ten baseline models, it delivers
competitive performance on both regression and planning problems, showing
strong generalization and cross-task adaptability. Our code is available at
https://github.com/BiYunying/TransLLM.

</details>


### [112] [Evaluating Retrieval-Augmented Generation vs. Long-Context Input for Clinical Reasoning over EHRs](https://arxiv.org/abs/2508.14817)
*Skatje Myers,Dmitriy Dligach,Timothy A. Miller,Samantha Barr,Yanjun Gao,Matthew Churpek,Anoop Mayampurath,Majid Afshar*

Main category: cs.CL

TL;DR: 该研究探索了使用检索增强生成（RAG）技术来处理电子健康记录（EHR）的有效性。研究人员提出了三个临床任务，并测试了不同的语言模型（LLM）和上下文输入策略（RAG vs. 近期记录 vs. 全部上下文）。结果表明，RAG在效率和性能上均表现出色，是处理长临床记录的有效方法。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录（EHR）由于其冗长、嘈杂和冗余的特性，给临床医生带来了巨大的导航挑战。大型语言模型（LLM）虽然有望解决这一问题，但临床记录的长度常常超出当前最先进模型（即使是具有扩展上下文窗口的模型）的处理能力。检索增强生成（RAG）通过从整个EHR中检索与任务相关的片段，为解决这一问题提供了一种替代方案，有望减少所需的输入令牌数量。

Method: 本研究提出了三个可跨健康系统轻松复制的临床任务：1）提取影像学检查；2）生成抗生素使用时间线；3）识别关键诊断。研究人员利用真实住院患者的电子健康记录，测试了三种具有不同上下文长度的先进语言模型（LLM），分别采用了定向文本检索（RAG）或仅使用近期临床记录的方法。

Result: 研究发现，与仅使用近期记录的方法相比，RAG在三个临床任务上的表现相当或更优；与使用模型全部上下文的方法相比，RAG的表现接近，但所需的输入令牌数量大大减少。

Conclusion: RAG（检索增强生成）技术在处理冗长、嘈杂且冗余的电子健康记录（EHR）方面，能够匹配甚至超越仅使用近期记录的方法，并且在仅使用全部上下文的方法的性能范围内，显著减少所需的输入令牌数量。这表明，即使在模型能够处理更长文本的情况下，RAG仍然是一种具有竞争力的、高效的方法。

Abstract: Electronic health records (EHRs) are long, noisy, and often redundant, posing
a major challenge for the clinicians who must navigate them. Large language
models (LLMs) offer a promising solution for extracting and reasoning over this
unstructured text, but the length of clinical notes often exceeds even
state-of-the-art models' extended context windows. Retrieval-augmented
generation (RAG) offers an alternative by retrieving task-relevant passages
from across the entire EHR, potentially reducing the amount of required input
tokens. In this work, we propose three clinical tasks designed to be replicable
across health systems with minimal effort: 1) extracting imaging procedures, 2)
generating timelines of antibiotic use, and 3) identifying key diagnoses. Using
EHRs from actual hospitalized patients, we test three state-of-the-art LLMs
with varying amounts of provided context, using either targeted text retrieval
or the most recent clinical notes. We find that RAG closely matches or exceeds
the performance of using recent notes, and approaches the performance of using
the models' full context while requiring drastically fewer input tokens. Our
results suggest that RAG remains a competitive and efficient approach even as
newer models become capable of handling increasingly longer amounts of text.

</details>


### [113] [Long Chain-of-Thought Reasoning Across Languages](https://arxiv.org/abs/2508.14828)
*Josh Barua,Seun Eisape,Kayo Yin,Alane Suhr*

Main category: cs.CL

TL;DR: 本研究发现，在多语言环境下，长链条思考（CoT）的有效性取决于语言和数据集。英语作为中间语言的效果不一，模型预训练和微调能改善多语言性能，但跨语言差距仍存。数据集质量和规模的选择也需因语言而异。研究提供了多语言数据集，以促进公平的多语言推理研究。


<details>
  <summary>Details</summary>
Motivation: 长链条思考（CoTs）虽然提升了大型语言模型（LLMs）的推理能力，但其推理过程几乎完全以英语为中心。本研究旨在探索和改善多语言环境下的长CoT生成能力，以促进公平的多语言推理研究。

Method: 本研究构建了两个流行的英语推理数据集的翻译版本，并对Qwen 2.5 (7B) 和 Qwen 3 (8B) 模型进行了微调，系统研究了长链条思考（CoT）在法语、日语、拉脱维亚语和斯瓦希里语中的生成情况。

Result: 1. 英语作为中间语言的有效性因语言而异：对法语无益，对日语和拉脱维亚语有益（作为推理语言），对斯瓦希里语不足（任务理解和推理均表现不佳）。 2. Qwen 3的多语言预训练缩小了跨语言性能差距，但未完全消除；轻量级微调（1k条数据）可使斯瓦希里语性能提升30%以上。 3. 数据质量与规模的权衡因语言而异：英语和法语适合小型精选数据集，而斯瓦希里语和拉脱维亚语则更适合更大但更嘈杂的数据集。

Conclusion: 本研究通过构建翻译的推理数据集，系统研究了长链条思考（CoT）在法语、日语、拉脱维亚语和斯瓦希里语中的生成情况，并对Qwen 2.5和Qwen 3模型进行了微调。研究结果表明，英语作为中间语言的有效性因语言而异，对法语无益，对日语和拉脱维亚语有益，而对斯瓦希里语则不足。Qwen 3的多语言预训练缩小了跨语言性能差距，但未能完全消除。轻量级微调（仅1k条数据）能显著提升斯瓦希里语性能。数据质量与规模的权衡也因语言而异，精选的小型数据集适用于英语和法语，而更大但更嘈杂的数据集则更适用于斯瓦希里语和拉脱维亚语。这些发现阐明了长CoT跨语言迁移的条件和原因，并提供了多语言数据集以促进公平的多语言推理研究。

Abstract: Scaling inference through long chains-of-thought (CoTs) has unlocked
impressive reasoning capabilities in large language models (LLMs), yet the
reasoning process remains almost exclusively English-centric. We construct
translated versions of two popular English reasoning datasets, fine-tune Qwen
2.5 (7B) and Qwen 3 (8B) models, and present a systematic study of long CoT
generation across French, Japanese, Latvian, and Swahili. Our experiments
reveal three key findings. First, the efficacy of using English as a pivot
language varies by language: it provides no benefit for French, improves
performance when used as the reasoning language for Japanese and Latvian, and
proves insufficient for Swahili where both task comprehension and reasoning
remain poor. Second, extensive multilingual pretraining in Qwen 3 narrows but
does not eliminate the cross-lingual performance gap. A lightweight fine-tune
using only 1k traces still improves performance by over 30\% in Swahili. Third,
data quality versus scale trade-offs are language dependent: small, carefully
curated datasets suffice for English and French, whereas larger but noisier
corpora prove more effective for Swahili and Latvian. Together, these results
clarify when and why long CoTs transfer across languages and provide translated
datasets to foster equitable multilingual reasoning research.

</details>


### [114] [MedReseacher-R1: Expert-Level Medical Deep Researcher via A Knowledge-Informed Trajectory Synthesis Framework](https://arxiv.org/abs/2508.14880)
*Ailing Yu,Lan Yao,Jingnan Liu,Zhe Chen,Jiajun Yin,Yuan Wang,Xinhao Liao,Zhiling Ye,Ji Li,Yun Yue,Hansong Xiao,Hualei Zhou,Chunxiao Guo,Peng Wei,Jinjie Gu*

Main category: cs.CL

TL;DR: A new medical agent, MedResearcher-R1-32B, outperforms larger proprietary systems on medical benchmarks by using a novel data synthesis framework with medical knowledge graphs and integrating a custom medical retrieval engine. It achieves state-of-the-art results through a specialized training approach.


<details>
  <summary>Details</summary>
Motivation: LLM-based agents struggle with medical domain challenges due to insufficient dense medical knowledge for clinical reasoning and the absence of specialized retrieval tools tailored for medical contexts. Leading proprietary systems achieve limited accuracy on complex medical benchmarks.

Method: 1. Developed a novel data synthesis framework using medical knowledge graphs, extracting the longest chains from subgraphs around rare medical entities to generate complex multi-hop question-answer pairs. 2. Integrated a custom-built private medical retrieval engine alongside general-purpose tools, enabling accurate medical information synthesis. 3. Employed a two-stage training paradigm combining supervised fine-tuning and online reinforcement learning with composite rewards.

Result: The MedResearcher-R1-32B model generates 2100+ diverse trajectories across 12 medical specialties, averaging 4.2 tool interactions. It establishes new state-of-the-art results on medical benchmarks while maintaining competitive performance on general deep research tasks.

Conclusion: Strategic domain-specific innovations in architecture, tool design, and training data construction can enable smaller open-source models to outperform much larger proprietary systems in specialized domains.

Abstract: Recent developments in Large Language Model (LLM)-based agents have shown
impressive capabilities spanning multiple domains, exemplified by deep research
systems that demonstrate superior performance on complex information-seeking
and synthesis tasks. While general-purpose deep research agents have shown
impressive capabilities, they struggle significantly with medical domain
challenges, as evidenced by leading proprietary systems achieving limited
accuracy on complex medical benchmarks. The key limitations are: (1) the model
lacks sufficient dense medical knowledge for clinical reasoning, and (2) the
framework is constrained by the absence of specialized retrieval tools tailored
for medical contexts.We present a medical deep research agent that addresses
these challenges through two core innovations. First, we develop a novel data
synthesis framework using medical knowledge graphs, extracting the longest
chains from subgraphs around rare medical entities to generate complex
multi-hop question-answer pairs. Second, we integrate a custom-built private
medical retrieval engine alongside general-purpose tools, enabling accurate
medical information synthesis. Our approach generates 2100+ diverse
trajectories across 12 medical specialties, each averaging 4.2 tool
interactions.Through a two-stage training paradigm combining supervised
fine-tuning and online reinforcement learning with composite rewards, our
MedResearcher-R1-32B model demonstrates exceptional performance, establishing
new state-of-the-art results on medical benchmarks while maintaining
competitive performance on general deep research tasks. Our work demonstrates
that strategic domain-specific innovations in architecture, tool design, and
training data construction can enable smaller open-source models to outperform
much larger proprietary systems in specialized domains.

</details>


### [115] [Quantization Meets dLLMs: A Systematic Study of Post-training Quantization for Diffusion LLMs](https://arxiv.org/abs/2508.14896)
*Haokun Lin,Haobo Xu,Yichen Wu,Ziyu Guo,Renrui Zhang,Zhichao Lu,Ying Wei,Qingfu Zhang,Zhenan Sun*

Main category: cs.CL

TL;DR:  本文首次系统研究了 dLLMs 的量化问题，识别了激活离群值挑战，并评估了多种 PTQ 方法。研究结果为 dLLMs 在边缘设备的部署提供了实践见解。


<details>
  <summary>Details</summary>
Motivation:  尽管 dLLMs 在自然语言生成任务中展现出巨大潜力，但其巨大的参数规模和高资源需求限制了其在边缘设备的部署。然而，目前对于如何应用 PTQ 等压缩技术于 dLLMs 的研究尚不充分。

Method:  本研究首先识别了激活离群值问题，这是低比特量化的关键挑战。随后，研究人员实现了最先进的后训练量化（PTQ）方法，并从比特宽度、量化方法、任务类别和模型类型四个关键维度进行了全面的评估。

Result:  研究通过多角度评估，揭示了 dLLMs 在不同配置下的量化行为，为未来的研究提供了实践指导。

Conclusion:  本研究首次系统地研究了扩散语言模型（dLLMs）的量化问题，并提出了有效的解决方案，为 dLLMs 在边缘设备的部署奠定了基础。

Abstract: Recent advances in diffusion large language models (dLLMs) have introduced a
promising alternative to autoregressive (AR) LLMs for natural language
generation tasks, leveraging full attention and denoising-based decoding
strategies. However, the deployment of these models on edge devices remains
challenging due to their massive parameter scale and high resource demands.
While post-training quantization (PTQ) has emerged as a widely adopted
technique for compressing AR LLMs, its applicability to dLLMs remains largely
unexplored. In this work, we present the first systematic study on quantizing
diffusion-based language models. We begin by identifying the presence of
activation outliers, characterized by abnormally large activation values that
dominate the dynamic range. These outliers pose a key challenge to low-bit
quantization, as they make it difficult to preserve precision for the majority
of values. More importantly, we implement state-of-the-art PTQ methods and
conduct a comprehensive evaluation across multiple task types and model
variants. Our analysis is structured along four key dimensions: bit-width,
quantization method, task category, and model type. Through this
multi-perspective evaluation, we offer practical insights into the quantization
behavior of dLLMs under different configurations. We hope our findings provide
a foundation for future research in efficient dLLM deployment. All codes and
experimental setups will be released to support the community.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [116] [Nonreciprocal parametric amplification of elastic waves in supersonic space-time modulated media](https://arxiv.org/abs/2508.14220)
*Yingrui Ye,Chunxia Liu,Xiaopeng Wang,Antonio Palermo*

Main category: physics.app-ph

TL;DR: 本文研究了超声速调制弹性介质中的弹性波传播，首次揭示了超声速调制引起方向性波数带隙和非互易参数放大/频率转换现象，这为设计新型非互易弹性波器件提供了基础。


<details>
  <summary>Details</summary>
Motivation: 为了探索先前未被充分研究的超声速调制弹性介质中的波传播现象，以及解决超声速调制下非互易现象的挑战，本文旨在揭示其动力学响应、波数带隙以及由此产生的非互易性放大和频率转换效应。

Method: 本文采用基于模式耦合理论的理论框架，并结合数值模拟来研究超声速调制弹性介质中弹性纵波的传播特性。

Result: 研究表明，超声速调制会在色散图中产生方向性波数带隙，并引起波的振幅调制和频率转换，导致非互易参数放大和频率转换，这些现象在先前有界的弹性介质中未有报道。参数放大的产生机制是由于反向传播的Floquet-Bloch模式在空间界面的干涉。

Conclusion: 本文研究了超声速调制弹性介质中弹性纵波的动力学响应，揭示了超声速调制引起方向性波数带隙，并通过模式耦合理论解释了振幅调制和频率转换。研究发现，参数放大源于反向传播的Floquet-Bloch模式在空间界面的干涉，而非波数的虚部。数值模拟证实了超声速调制可以实现非互易放大和频率转换，为设计具有高级功能（如信号处理、能量放大和频率转换）的非互易弹性波器件提供了新的途径。

Abstract: Space-time modulated elastic media, whose material properties vary in both
space and time, have attracted significant attention as a promising strategy
for achieving nonreciprocal propagation of elastic waves. To date, most studies
have focused on systems with subsonic modulation, where the phase velocity of
the modulation wave is lower than that of the guided elastic waves. In
contrast, wave propagation under supersonic modulation remains largely
unexplored, and the associated nonreciprocal phenomena present an open
challenge. In this work, we investigate the dynamic response of elastic
longitudinal waves propagating through a spatially bounded medium subjected to
supersonic modulation of its elastic properties. We show that supersonic
modulation gives rise to directional wavenumber bandgaps in the dispersion
diagram, characterized by vanishing wavenumbers. Using a theoretical framework
based on mode-coupling theory, we reveal how supersonic modulation governs
amplitude modulation and frequency conversion in wave transmission and
reflection at spatial interfaces. This leads to nonreciprocal parametric
amplification and frequency conversion, phenomena not previously reported in
bounded elastic media. Remarkably, the observed parametric amplification is not
driven by an imaginary component of the wavenumber, but instead arises from the
interference between counter-propagating Floquet-Bloch modes at spatial
interfaces. Numerical simulations corroborate the theoretical predictions and
illustrate nonreciprocal amplification and frequency conversion effects. Our
findings pave the way for the design of nonreciprocal elastic wave devices with
advanced functionalities such as signal processing, energy amplification, and
frequency conversion.

</details>


### [117] [Non-Hermitian funneling in anisotropic media](https://arxiv.org/abs/2508.14495)
*Yuan Tian,Nankun Gao,Xiujuan Zhang,Ming-Hui Lu,Yan-Feng Chen*

Main category: physics.app-ph

TL;DR: 本研究在均匀介质中实现了无需精细调谐的非厄米奇异态（NHSE），通过声学各向异性超材料成功引导波能量聚焦于特定边界（非厄米波聚焦），并发现了能将波聚焦于角落的二阶NHSE。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有非厄米奇异态（NHSE）仅限于需要精细调谐的离散模型（如格点上的特定增益/损耗或非对称耦合）的限制，本研究旨在探索在均匀介质中实现NHSE的新方法。

Method: 通过利用非厄米密度张量的各向异性，在均匀介质中实现了非厄米奇异态（NHSE）。实验上利用声学各向异性超材料进行了验证。

Result: 实现了在均匀介质中，利用非厄米密度张量的各向异性，成功展示了非厄米奇异态（NHSE）。通过声学各向异性超材料实验证明，NHSE能够引导波能量聚焦于特定边界，并呈现出宽带和宽角特性，这一现象被称为“非厄米波聚焦”。此外，还发现了能够将波聚焦到角落的二阶NHSE。

Conclusion: 该研究实现了均匀介质中的新型非厄米奇异态 (NHSE)，通过利用非厄米密度张量的各向异性，实现了将波能量定向收集到特定边界的波聚焦现象，并且不依赖于精细调谐，具有宽带和宽角特性。此外，还识别出一种能够将波聚焦到角落的二阶NHSE。该工作为在均匀介质中探索NHSE提供了新范例，深化了对非厄米物理学的理解，并为超材料或天然材料中的非厄米调控提供了新机制。

Abstract: Non-Hermitian skin effect (NHSE) has emerged as a distinctive phenomenon
enabling non-Bloch wave manipulation. However, it has been limited to discrete
lattices requiring fine-tuned onsite gain/loss or asymmetric couplings. Here,
moving beyond these discrete models, we realize novel NHSE in uniform media by
leveraging anisotropy of non-Hermitian density tensors. Experiments based on an
acoustic anisotropic metamaterial demonstrate that enabled by the NHSE, wave
energy can be directed toward and collected at specific boundaries, exhibiting
broadband and wide-angle characteristics. This intriguing phenomenon is termed
non-Hermitian wave funneling, which, remarkably, occurs under uniform
non-Hermitian modulations, free of fine-tuning. Furthermore, we identify a
second-order NHSE, enabling wave funneling toward corners. Our work establishes
a paradigm for exploring NHSE in uniform media, advancing the fundamental
understanding of non-Hermitian physics and providing novel mechanisms for
non-Bloch wave control in metamaterials or even natural materials without
delicate tuning.

</details>


### [118] [Synchronization driven reciprocity breaking](https://arxiv.org/abs/2508.14810)
*Alexander K. Stoychev,Ulrich Kuhl,Nicolas Noiray*

Main category: physics.app-ph

TL;DR: 通过利用自激振荡器同步，本研究提出了一种打破波传输互易性的新方法，该方法具有内在能量补偿特性，并且在实验中得到了验证。


<details>
  <summary>Details</summary>
Motivation: 为了打破波传输互易性，并提供一种不同于传统共振方法的、具有内在能量补偿特性的新途径。

Method: 本研究提出了一种利用自激振荡器同步来打破波传输互易性的新方法，并通过实验展示了空气动力学腔的非互易性行为。该系统可以用修正的时间耦合模式理论来量化描述。

Result: 实验结果表明，该系统表现出鲁棒的非互易性行为，并且该方法可以拓宽工作带宽。

Conclusion: 本研究提出的基于同步方法是一种实现波传输非互易性的新途径，它不同于传统的基于共振的方法，并且具有内在的能量补偿特性。

Abstract: This study introduces a novel method to break wave transmission reciprocity
by leveraging the synchronization of self-oscillators. An experimental
demonstration with aeroacoustic cavities is presented. They behave as weakly
nonlinear limit cycles when driven by a constant airflow, leading to
self-oscillations which can couple to the surrounding waveguides via two ports.
Incident waves from one port trigger anti-phase synchronization, causing
destructive interference and low transmission, while waves from the opposite
port induce in-phase synchronization, resulting in high transmission. This
directional dependence effectively breaks reciprocity, where the operational
bandwidth is defined by the synchronization region (Arnold tongue), and can be
broader than resonance based methods. Experimental results show robust
nonreciprocal behavior w.r.t. parameter changes. Moreover, a modified temporal
coupled mode theory is proposed, explaining the system's nonlinear dynamics and
scattering properties in a quantitative manner. This synchronization-based
approach offers a new avenue for directional wave control, complementing
traditional reciprocity breaking techniques, and offering an intrinsic
loss-compensation emanating from the self-oscillation of meta-atoms.

</details>


### [119] [Synchronization driven acoustics: The nonlinear scattering of a self-oscillating meta-atom](https://arxiv.org/abs/2508.14819)
*Alexander K. Stoychev,Xinxin Guo,Ulrich Kuhl,Nicolas Noiray*

Main category: physics.app-ph

TL;DR: 本研究提出了一种能放大声波的自振荡声学超构原子，通过控制声波开关状态，并利用同步现象进行扰动滤波和稳定声功率。研究结果可用非线性振荡器模型定量描述，为声学超材料研究开辟新方向。


<details>
  <summary>Details</summary>
Motivation: 该研究旨在探索一种新颖的声学超构原子功能，并为声学超材料研究提供新的方向。

Method: 本研究利用自振荡和同步过程，将非线性动力学和复杂系统中的关键概念与主动超材料设计相结合，提出了一种广泛适用的场无关波操纵机制。

Result: 实验测量结果定量描述了非线性 Li'enard 型振荡器，该振荡器具有饱和增益和线性损耗，并且其关键参数可通过独立测量进行估算。

Conclusion: 该研究展示了一种自振荡声学超构原子，可作为放大晶体管，利用稳定的外部流作为控制信号，在反射（关断状态）和透射（开启状态）状态之间切换。在开启状态下，声学极限环与入射声波同步。该过程决定了能量在设备中的传输，其传输带宽由参数空间中的同步区域（阿诺德舌）决定。实验测量揭示了非线性依赖于入射波幅度，从而能够进行扰动滤波并稳定下游声功率。所有实验观察到的现象均可通过具有饱和增益和线性损耗的非线性 Lienard 型振荡器进行定量描述，其关键参数可通过独立测量进行估计。

Abstract: In this study we demonstrate a self-oscillating acoustic meta-atom
functioning as an amplifying transistor, where a steady external flow serves as
a control signal to switch between reflective (off-state) and transmissive
(on-state) regimes. In the on-state, an acoustic limit cycle synchronizes with
incident sound waves. This process governs the energy transfer across the
device, with a transmission bandwidth dictated by the synchronization region in
parameter space (Arnold tongue). Our experimental measurements reveal nonlinear
dependence on the incident wave amplitude, enabling perturbation filtering
therein and stabilizing downstream acoustic power. All experimentally observed
phenomena are quantitatively described by a nonlinear Li\'enard-type oscillator
featuring saturable gain and linear loss, where the essential parameters can be
estimated by independent measurements. This work may offer a paradigm shift in
acoustic metamaterials research by leveraging self-oscillation and
synchronization processes. Bridging those key concepts from nonlinear dynamics
and complex systems with active metamaterial design in acoustics and related
disciplines, may establish a broadly applicable framework of field-independent
mechanisms for wave manipulation.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [120] [Research on UAV Applications in Public Administration: Based on an Improved RRT Algorithm](https://arxiv.org/abs/2508.14096)
*Zhanxi Xie,Baili Lu,Yanzhao Gu,Zikun Li,Junhao Wei,Ngai Cheong*

Main category: cs.RO

TL;DR: 无人机路径规划新算法dRRT在城市环境中表现优异，成功率100%，速度快，路径优，适用于公共管理，但计算开销大。


<details>
  <summary>Details</summary>
Motivation: 随着无人机在低空经济政策和智慧城市需求驱动下，从“技术工具”向“治理基础设施”转变，优化其路径规划以应对能源消耗、避障和空域限制等挑战变得至关重要。

Method: 研究提出了一种增强的快速探索随机树（dRRT）算法，该算法集成了目标偏向、动态步长、绕路优先和B样条平滑四种策略，用于优化无人机在公共管理场景中的路径规划。

Result: dRRT算法在500立方米的城市环境中进行了仿真测试，结果显示其成功率为100%，平均运行时间为0.01468秒，路径更短，航点更少，轨迹更平滑（最大偏航角小于45度），性能优于RRT、A*和ACO算法。

Conclusion: 该研究提出了增强的快速探索随机树（dRRT）算法，通过结合目标偏向、动态步长、绕路优先和B样条平滑等策略，在城市环境中优化无人机路径规划。仿真结果表明，dRRT在成功率、运行时间、路径长度、航点数量和轨迹平滑度方面优于传统RRT、A*和蚁群优化（ACO）算法，在紧急响应和交通监控等公共管理场景中具有应用潜力，但也存在计算开销增加和潜在局部最优等局限性，未来需与实时避障框架集成。

Abstract: This study investigates the application of unmanned aerial vehicles (UAVs) in
public management, focusing on optimizing path planning to address challenges
such as energy consumption, obstacle avoidance, and airspace constraints. As
UAVs transition from 'technical tools' to 'governance infrastructure', driven
by advancements in low-altitude economy policies and smart city demands,
efficient path planning becomes critical. The research proposes an enhanced
Rapidly-exploring Random Tree algorithm (dRRT), incorporating four strategies:
Target Bias (to accelerate convergence), Dynamic Step Size (to balance
exploration and obstacle navigation), Detour Priority (to prioritize horizontal
detours over vertical ascents), and B-spline smoothing (to enhance path
smoothness). Simulations in a 500 m3 urban environment with randomized
buildings demonstrate dRRT's superiority over traditional RRT, A*, and Ant
Colony Optimization (ACO). Results show dRRT achieves a 100\% success rate with
an average runtime of 0.01468s, shorter path lengths, fewer waypoints, and
smoother trajectories (maximum yaw angles <45{\deg}). Despite improvements,
limitations include increased computational overhead from added mechanisms and
potential local optima due to goal biasing. The study highlights dRRT's
potential for efficient UAV deployment in public management scenarios like
emergency response and traffic monitoring, while underscoring the need for
integration with real-time obstacle avoidance frameworks. This work contributes
to interdisciplinary advancements in urban governance, robotics, and
computational optimization.

</details>


### [121] [No More Marching: Learning Humanoid Locomotion for Short-Range SE(2) Targets](https://arxiv.org/abs/2508.14098)
*Pranay Dugar,Mohitvishnu S. Gadde,Jonah Siekmann,Yesh Godse,Aayam Shrestha,Alan Fern*

Main category: cs.RO

TL;DR: 本文提出了一种新的强化学习方法，通过特定设计的奖励函数，使人形机器人能够更高效、自然地完成短距离姿态到达任务，克服了现有方法的不足，并在实际应用中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于学习的机器人运动方法大多优化速度跟踪而非直接姿态到达，导致在执行短距离任务时效率低下，呈行军式运动。本文旨在解决此问题，使人形机器人的短距离运动更实用、高效。

Method: 提出了一种直接优化人形机器人SE(2)目标的强化学习方法，核心是设计了一种新的基于星座的奖励函数，以促进自然高效的定向运动。

Result: 所提出的方法在能耗、到达时间（time-to-target）和步数（footstep count）方面持续优于标准方法，并成功实现了从模拟到硬件的迁移。

Conclusion: 该研究提出了一种直接优化人形机器人SE(2)目标的强化学习方法，通过基于星座的奖励函数鼓励自然高效的定向运动，并在基准测试框架中证明其在能耗、到达时间、步数等方面优于标准方法，并成功从模拟迁移到硬件。

Abstract: Humanoids operating in real-world workspaces must frequently execute
task-driven, short-range movements to SE(2) target poses. To be practical,
these transitions must be fast, robust, and energy efficient. While
learning-based locomotion has made significant progress, most existing methods
optimize for velocity-tracking rather than direct pose reaching, resulting in
inefficient, marching-style behavior when applied to short-range tasks. In this
work, we develop a reinforcement learning approach that directly optimizes
humanoid locomotion for SE(2) targets. Central to this approach is a new
constellation-based reward function that encourages natural and efficient
target-oriented movement. To evaluate performance, we introduce a benchmarking
framework that measures energy consumption, time-to-target, and footstep count
on a distribution of SE(2) goals. Our results show that the proposed approach
consistently outperforms standard methods and enables successful transfer from
simulation to hardware, highlighting the importance of targeted reward design
for practical short-range humanoid locomotion.

</details>


### [122] [Task and Motion Planning for Humanoid Loco-manipulation](https://arxiv.org/abs/2508.14099)
*Michal Ciebielski,Victor Dhédin,Majid Khadiv*

Main category: cs.RO

TL;DR: 提出了一种新的TAMP框架，通过共享接触模式表示来统一机器人的运动和操作规划，并成功应用于人形机器人平台，实现了复杂的运动规划行为。


<details>
  <summary>Details</summary>
Motivation: 统一机器人运动和操作的规划，以处理复杂的长期行为。

Method: 提出了一种基于优化的任务与运动规划（TAMP）框架，该框架通过共享的接触模式表示来统一运动和操作的规划。将符号动作定义为接触模式的变化，从而将高级规划与低级运动相结合。这使得能够在一个统一的搜索中跨越任务、接触和运动规划，同时纳入全身动力学以及机器人、被操纵物体和环境之间的所有约束。

Result: 在人形平台上生成广泛的、物理上一致的运动规划行为，这些行为需要复杂的推理。

Conclusion: 这项工作首次实现了包含完整无环规划和考虑驱动约束的全身动力学的人形机器人任务与运动规划（TAMP）问题。

Abstract: This work presents an optimization-based task and motion planning (TAMP)
framework that unifies planning for locomotion and manipulation through a
shared representation of contact modes. We define symbolic actions as contact
mode changes, grounding high-level planning in low-level motion. This enables a
unified search that spans task, contact, and motion planning while
incorporating whole-body dynamics, as well as all constraints between the
robot, the manipulated object, and the environment. Results on a humanoid
platform show that our method can generate a broad range of physically
consistent loco-manipulation behaviors over long action sequences requiring
complex reasoning. To the best of our knowledge, this is the first work that
enables the resolution of an integrated TAMP formulation with fully acyclic
planning and whole body dynamics with actuation constraints for the humanoid
loco-manipulation problem.

</details>


### [123] [Domain Translation of a Soft Robotic Arm using Conditional Cycle Generative Adversarial Network](https://arxiv.org/abs/2508.14100)
*Nilay Kushawaha,Carlo Alessi,Lorenzo Fruzzetti,Egidio Falotico*

Main category: cs.RO

TL;DR: 提出了一种基于CCGAN的域翻译框架，用于软体机器人的跨域知识转移。该方法通过动态学习将控制器从一个域适配到另一个具有不同物理特性的域，并通过实验验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度学习为模拟软体机器人动力学提供了强大的方法，但传统分析方法需要精确的机器人结构、材料特性等信息，这对于复杂、非线性的软体机器人系统来说难以提取。此外，在一个域中学到的模型无法直接迁移到具有不同物理特性的域，这对于材料会随时间降解的软体机器人尤其重要。

Method: 提出了一种基于条件循环生成对抗网络（CCGAN）的域翻译框架，以实现从源域到目标域的知识转移。采用动态学习方法，将训练好的位姿控制器从标准模拟环境适配到粘度增加十倍的域。模型从输入压力信号（以双域中相应的末端执行器位置和方向为条件）中学习。

Result: 通过在五个不同形状上的轨迹跟踪实验评估了该方法。此外，还在噪声扰动和周期性测试下评估了其鲁棒性。结果表明，CCGAN-GP能够有效地促进跨域技能转移。

Conclusion: CCGAN-GP能够有效地促进跨域技能转移，为更具适应性和通用性的软体机器人控制器铺平道路。

Abstract: Deep learning provides a powerful method for modeling the dynamics of soft
robots, offering advantages over traditional analytical approaches that require
precise knowledge of the robot's structure, material properties, and other
physical characteristics. Given the inherent complexity and non-linearity of
these systems, extracting such details can be challenging. The mappings learned
in one domain cannot be directly transferred to another domain with different
physical properties. This challenge is particularly relevant for soft robots,
as their materials gradually degrade over time. In this paper, we introduce a
domain translation framework based on a conditional cycle generative
adversarial network (CCGAN) to enable knowledge transfer from a source domain
to a target domain. Specifically, we employ a dynamic learning approach to
adapt a pose controller trained in a standard simulation environment to a
domain with tenfold increased viscosity. Our model learns from input pressure
signals conditioned on corresponding end-effector positions and orientations in
both domains. We evaluate our approach through trajectory-tracking experiments
across five distinct shapes and further assess its robustness under noise
perturbations and periodicity tests. The results demonstrate that CCGAN-GP
effectively facilitates cross-domain skill transfer, paving the way for more
adaptable and generalizable soft robotic controllers.

</details>


### [124] [Efficient Environment Design for Multi-Robot Navigation via Continuous Control](https://arxiv.org/abs/2508.14105)
*Jahid Chowdhury Choton,John Woods,William Hsu*

Main category: cs.RO

TL;DR: 本研究提出了一个用于多机器人导航的强化学习环境，旨在通过MDP和多种RL方法解决路径规划问题，并在模拟环境中验证了其有效性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 深度强化学习（RL）在解决多机器人导航和路径规划问题方面虽然备受欢迎，但在样本效率和训练时间方面存在局限性，并且现有研究缺乏形式化保证。因此，本研究旨在开发一个能够解决这些问题的环境，以提高RL在多机器人导航任务中的效率和可靠性。

Method: 本文介绍了一个高效且高度可定制的连续控制多机器人导航环境，该环境将多机器人导航任务建模为马尔可夫决策过程（MDP），并将其表述为优化问题，然后使用A2C、PPO、TRPO、TQC、CrossQ和ARS等梯度和非梯度强化学习（RL）方法来寻找最优策略。此外，该环境还被部署到CoppeliaSim机器人模拟器中的3D农业领域，以评估其在真实世界应用中的鲁棒性。

Result: 研究人员构建了该导航环境的多个变体，并使用多种RL方法（A2C、PPO、TRPO、TQC、CrossQ和ARS）进行了性能评估。通过在CoppeliaSim机器人模拟器中的3D农业领域部署该环境，并对学习模型进行推理，验证了其在真实世界应用中的可行性和鲁棒性。

Conclusion: 该研究提出了一个用于连续控制的多机器人导航环境，并将其与马尔可夫决策过程（MDP）相关联，旨在为机器人规划访问一系列感兴趣区域（ROI）的最短路径。

Abstract: Multi-robot navigation and path planning in continuous state and action
spaces with uncertain environments remains an open challenge. Deep
Reinforcement Learning (RL) is one of the most popular paradigms for solving
this task, but its real-world application has been limited due to sample
inefficiency and long training periods. Moreover, the existing works using RL
for multi-robot navigation lack formal guarantees while designing the
environment. In this paper, we introduce an efficient and highly customizable
environment for continuous-control multi-robot navigation, where the robots
must visit a set of regions of interest (ROIs) by following the shortest paths.
The task is formally modeled as a Markov Decision Process (MDP). We describe
the multi-robot navigation task as an optimization problem and relate it to
finding an optimal policy for the MDP. We crafted several variations of the
environment and measured the performance using both gradient and non-gradient
based RL methods: A2C, PPO, TRPO, TQC, CrossQ and ARS. To show real-world
applicability, we deployed our environment to a 3-D agricultural field with
uncertainties using the CoppeliaSim robot simulator and measured the robustness
by running inference on the learned models. We believe our work will guide the
researchers on how to develop MDP-based environments that are applicable to
real-world systems and solve them using the existing state-of-the-art RL
methods with limited resources and within reasonable time periods.

</details>


### [125] [SimGenHOI: Physically Realistic Whole-Body Humanoid-Object Interaction via Generative Modeling and Reinforcement Learning](https://arxiv.org/abs/2508.14120)
*Yuhang Lin,Yijia Xie,Jiahong Xie,Yuehao Huang,Ruoyu Wang,Jiajun Lv,Yukai Ma,Xingxing Zuo*

Main category: cs.RO

TL;DR: SimGenHOI是一个结合了生成模型和强化学习的框架，用于生成逼真、物理上可行的长时程人机交互，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有HOI生成方法中存在的伪影（如不合理的接触、穿透和不切实际的全身动作）问题，该研究提出了SimGenHOI。

Method: SimGenHOI框架整合了基于Diffusion Transformer（DiT）的生成模型和强化学习驱动的感知-闭环控制器，以生成可控且物理上可行的、具有挑战性的和长时程的人机交互。

Result: SimGenHOI能够生成逼真、多样化且物理上可行的HOI，显著提高了跟踪成功率，并实现了长时程的操纵任务。

Conclusion: SimGenHOI通过结合生成模型和强化学习，能够生成逼真、多样化且物理上可行的、具有挑战性的和长时程的人机交互。

Abstract: Generating physically realistic humanoid-object interactions (HOI) is a
fundamental challenge in robotics. Existing HOI generation approaches, such as
diffusion-based models, often suffer from artifacts such as implausible
contacts, penetrations, and unrealistic whole-body actions, which hinder
successful execution in physical environments. To address these challenges, we
introduce SimGenHOI, a unified framework that combines the strengths of
generative modeling and reinforcement learning to produce controllable and
physically plausible HOI. Our HOI generative model, based on Diffusion
Transformers (DiT), predicts a set of key actions conditioned on text prompts,
object geometry, sparse object waypoints, and the initial humanoid pose. These
key actions capture essential interaction dynamics and are interpolated into
smooth motion trajectories, naturally supporting long-horizon generation. To
ensure physical realism, we design a contact-aware whole-body control policy
trained with reinforcement learning, which tracks the generated motions while
correcting artifacts such as penetration and foot sliding. Furthermore, we
introduce a mutual fine-tuning strategy, where the generative model and the
control policy iteratively refine each other, improving both motion realism and
tracking robustness. Extensive experiments demonstrate that SimGenHOI generates
realistic, diverse, and physically plausible humanoid-object interactions,
achieving significantly higher tracking success rates in simulation and
enabling long-horizon manipulation tasks. Code will be released upon acceptance
on our project page: https://xingxingzuo.github.io/simgen_hoi.

</details>


### [126] [Lightweight Tracking Control for Computationally Constrained Aerial Systems with the Newton-Raphson Method](https://arxiv.org/abs/2508.14185)
*Evanns Morales-Cuadrado,Luke Baird,Yorai Wardi,Samuel Coogan*

Main category: cs.RO

TL;DR: 本文评估了一种轻量级牛顿-拉夫逊流跟踪控制器在飞艇和四旋翼上的实际飞行性能，结果显示其跟踪精度与现有方法相当，但计算和能耗更低。


<details>
  <summary>Details</summary>
Motivation: 本文旨在通过实际飞行实验，评估该跟踪技术在面临实际部署和板载计算限制的空中硬件平台上的性能，并与成熟的控制框架（如反馈线性化和非线性模型预测控制）进行比较。

Method: 本文研究了一种基于牛顿-拉夫逊流方法的轻量级跟踪控制器，并将其应用于微型飞艇和中型四旋翼飞行器。

Result: 实验结果表明，与基线方法相比，所提出的牛顿-拉夫逊流控制器在跟踪精度方面相当或更优，并且在计算时间和 CPU 能源消耗方面表现出显著优势。

Conclusion: 研究表明，基于牛顿-拉夫逊流方法的轻量级跟踪控制器在实际空中硬件平台上实现了与基线方法相当或更优的跟踪性能，同时显著降低了计算时间和能源消耗。

Abstract: We investigate the performance of a lightweight tracking controller, based on
a flow version of the Newton-Raphson method, applied to a miniature blimp and a
mid-size quadrotor. This tracking technique has been shown to enjoy theoretical
guarantees of performance and has been applied with success in simulation
studies and on mobile robots with simple motion models. This paper investigates
the technique through real-world flight experiments on aerial hardware
platforms subject to realistic deployment and onboard computational
constraints. The technique's performance is assessed in comparison with the
established control frameworks of feedback linearization for the blimp, and
nonlinear model predictive control for both quadrotor and blimp. The
performance metrics under consideration are (i) root mean square error of
flight trajectories with respect to target trajectories, (ii) algorithms'
computation times, and (iii) CPU energy consumption associated with the control
algorithms. The experimental findings show that the Newton-Raphson flow-based
tracking controller achieves comparable or superior tracking performance to the
baseline methods with substantially reduced computation time and energy
expenditure.

</details>


### [127] [SLAM-based Safe Indoor Exploration Strategy](https://arxiv.org/abs/2508.14235)
*Omar Mostafa,Nikolaos Evangeliou,Anthony Tzes*

Main category: cs.RO

TL;DR: 提出了一种用于经典移动机器人的二维探索策略，优先考虑安全避障和空间探索。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在为具有圆形足迹且无法瞬时调整姿态的经典移动机器人开发一种二维探索策略，以应对充满障碍物的平面空间。

Method: 本研究使用集成IMU、LiDAR和RGB-D摄像头的双轮差速驱动移动机器人。机器人利用RTAB-SLAM算法进行SLAM，并采用基于骨架的安全探索方法，优先考虑机器人安全和未知空间探索。

Result: 实验结果表明，该路径规划策略能够有效地在复杂环境中安全探索和导航。

Conclusion: 该研究提出了一种用于避障和空间探索的移动机器人路径规划策略。

Abstract: This paper suggests a 2D exploration strategy for a planar space cluttered
with obstacles. Rather than using point robots capable of adjusting their
position and altitude instantly, this research is tailored to classical agents
with circular footprints that cannot control instantly their pose. Inhere, a
self-balanced dual-wheeled differential drive system is used to explore the
place. The system is equipped with linear accelerometers and angular
gyroscopes, a 3D-LiDAR, and a forward-facing RGB-D camera. The system performs
RTAB-SLAM using the IMU and the LiDAR, while the camera is used for loop
closures. The mobile agent explores the planar space using a safe skeleton
approach that places the agent as far as possible from the static obstacles.
During the exploration strategy, the heading is towards any offered openings of
the space. This space exploration strategy has as its highest priority the
agent's safety in avoiding the obstacles followed by the exploration of
undetected space. Experimental studies with a ROS-enabled mobile agent are
presented indicating the path planning strategy while exploring the space.

</details>


### [128] [Adapting Biological Reflexes for Dynamic Reorientation in Space Manipulator Systems](https://arxiv.org/abs/2508.14258)
*Daegyun Choi,Alhim Vera,Donghoon Kim*

Main category: cs.RO

TL;DR: 受蜥蜴空中翻正能力的启发，本研究探索了将这些生物学原理应用于太空机械臂控制的方法，以解决微重力下的动态耦合问题，并取得了积极的成果。


<details>
  <summary>Details</summary>
Motivation: 微重力下空间机械臂系统（SMS）的控制是一个重大挑战，因为机械臂和航天器基座之间存在动态耦合。本研究旨在探索利用生物学原理，特别是动物的空中翻正反射，来解决这个问题。

Method: 通过计算机视觉技术提取蜥蜴等动物的空中翻正运动轨迹，并利用多目标优化框架分析这些轨迹以确定关键行为目标及其相对重要性。然后将这些轨迹作为参考轨迹，并使用基线控制器进行跟踪。

Result: 研究结果表明，将动物的运动行为转化为可解释、自适应的空间机器人控制策略是可行的，为未来太空任务的机动性和鲁棒性提供了改进方向。

Conclusion: 该研究将动物的空中翻正反射应用于空间机械臂系统（SMS）的控制，以提高其在微重力下的机动性和鲁棒性。

Abstract: Robotic arms mounted on spacecraft, known as space manipulator systems
(SMSs), are critical for enabling on-orbit assembly, satellite servicing, and
debris removal. However, controlling these systems in microgravity remains a
significant challenge due to the dynamic coupling between the manipulator and
the spacecraft base. This study explores the potential of using biological
inspiration to address this issue, focusing on animals, particularly lizards,
that exhibit mid-air righting reflexes. Based on similarities between SMSs and
these animals in terms of behavior, morphology, and environment, their
air-righting motion trajectories are extracted from high-speed video recordings
using computer vision techniques. These trajectories are analyzed within a
multi-objective optimization framework to identify the key behavioral goals and
assess their relative importance. The resulting motion profiles are then
applied as reference trajectories for SMS control, with baseline controllers
used to track them. The findings provide a step toward translating evolved
animal behaviors into interpretable, adaptive control strategies for space
robotics, with implications for improving maneuverability and robustness in
future missions.

</details>


### [129] [D$^2$-LIO: Enhanced Optimization for LiDAR-IMU Odometry Considering Directional Degeneracy](https://arxiv.org/abs/2508.14355)
*Guodong Yao,Hao Wang,Qing Chang*

Main category: cs.RO

TL;DR: 提出了一种新的LIO框架，通过自适应离群点去除和扫描到子图配准策略，解决了LiDAR特征退化的问题，提高了在复杂环境下的定位鲁棒性和准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决LiDAR特征退化对可靠状态估计带来的挑战，提高LiDAR-inertial odometry (LIO)在复杂环境下的定位和建图精度。

Method: 提出了一种增强的LIO框架，该框架集成了自适应的离群点容忍对应和扫描到子图配准策略。核心在于自适应离群点去除阈值，该阈值根据点到传感器距离和平台运动幅度动态调整。此外，还引入了一种利用IMU数据进行扫描到子图配准的方法，以及一种融合IMU预积分协方差和扫描到子图过程退化度量的新型加权矩阵。

Result: 实验证明，该方法在室内外特征稀疏或退化的环境中，相比现有方法在鲁棒性和准确性方面均有提升。

Conclusion: 该方法在室内外环境中，特别是在特征稀疏或退化的场景下，相比现有方法在鲁棒性和准确性方面均有提升。

Abstract: LiDAR-inertial odometry (LIO) plays a vital role in achieving accurate
localization and mapping, especially in complex environments. However, the
presence of LiDAR feature degeneracy poses a major challenge to reliable state
estimation. To overcome this issue, we propose an enhanced LIO framework that
integrates adaptive outlier-tolerant correspondence with a scan-to-submap
registration strategy. The core contribution lies in an adaptive outlier
removal threshold, which dynamically adjusts based on point-to-sensor distance
and the motion amplitude of platform. This mechanism improves the robustness of
feature matching in varying conditions. Moreover, we introduce a flexible
scan-to-submap registration method that leverages IMU data to refine pose
estimation, particularly in degenerate geometric configurations. To further
enhance localization accuracy, we design a novel weighting matrix that fuses
IMU preintegration covariance with a degeneration metric derived from the
scan-to-submap process. Extensive experiments conducted in both indoor and
outdoor environments-characterized by sparse or degenerate features-demonstrate
that our method consistently outperforms state-of-the-art approaches in terms
of both robustness and accuracy.

</details>


### [130] [Action-Constrained Imitation Learning](https://arxiv.org/abs/2508.14379)
*Chia-Han Yeh,Tse-Sheng Nan,Risto Vuorio,Wei Hung,Hung-Yen Wu,Shao-Hua Sun,Ping-Chun Hsieh*

Main category: cs.RO

TL;DR: DTWIL是一种新的动作约束模仿学习（ACIL）方法，通过轨迹对齐解决了专家和模仿器之间因动作约束引起的不匹配问题，提高了学习性能和样本效率。


<details>
  <summary>Details</summary>
Motivation: 策略学习在确保机器人控制和资源分配中的安全行为起着至关重要的作用，而动作约束下的策略学习尤为重要。ACIL问题旨在解决动作约束模仿器从具有更大动作空间的专家那里学习的问题。

Method: DTWIL通过将轨迹对齐视为一个规划问题，并利用基于动态时间规整（DTW）距离的约束来解决。

Result: 通过DTWIL生成的的数据集进行学习，在多个机器人控制任务中显著提高了性能，并在样本效率方面优于各种基准模仿学习算法。

Conclusion: DTWIL通过轨迹对齐显著提高了策略学习在多个机器人控制任务中的性能，并且在样本效率方面优于各种基准模仿学习算法。

Abstract: Policy learning under action constraints plays a central role in ensuring
safe behaviors in various robot control and resource allocation applications.
In this paper, we study a new problem setting termed Action-Constrained
Imitation Learning (ACIL), where an action-constrained imitator aims to learn
from a demonstrative expert with larger action space. The fundamental challenge
of ACIL lies in the unavoidable mismatch of occupancy measure between the
expert and the imitator caused by the action constraints. We tackle this
mismatch through \textit{trajectory alignment} and propose DTWIL, which
replaces the original expert demonstrations with a surrogate dataset that
follows similar state trajectories while adhering to the action constraints.
Specifically, we recast trajectory alignment as a planning problem and solve it
via Model Predictive Control, which aligns the surrogate trajectories with the
expert trajectories based on the Dynamic Time Warping (DTW) distance. Through
extensive experiments, we demonstrate that learning from the dataset generated
by DTWIL significantly enhances performance across multiple robot control tasks
and outperforms various benchmark imitation learning algorithms in terms of
sample efficiency. Our code is publicly available at
https://github.com/NYCU-RL-Bandits-Lab/ACRL-Baselines.

</details>


### [131] [Fair-CoPlan: Negotiated Flight Planning with Fair Deconfliction for Urban Air Mobility](https://arxiv.org/abs/2508.14380)
*Nicole Fronda,Phil Smith,Bardh Hoxha,Yash Pant,Houssam Abbas*

Main category: cs.RO

TL;DR: 城市空中交通（UAM）中的无人机（UAS）需要共享空域，但不同的运营商有不同的目标。我们提出了 Fair-CoPlan，一种协商式、半分布式的飞行规划器，可以公平地优化无人机的飞行路径长度。Fair-CoPlan 通过 PSU（城市空中交通服务提供商）施加限制，运营商独立规划，最后由 PSU 解决冲突，以实现公平性。仿真结果表明，Fair-CoPlan 在公平性方面优于非公平规划器，但会带来轻微的延迟。


<details>
  <summary>Details</summary>
Motivation: 为了解决城市空中交通（UAM）中无人驾驶航空器（UAS）在共享空域时，由于运营商目标不同（有时相互竞争），可能导致部分 UAS 的飞行路径不成比例地短，而牺牲其他 UAS 的问题。

Method: 提出了一种协商式、半分布式飞行规划方法，称为 Fair-CoPlan。该方法包括三个步骤：1. PSU 根据起降点（vertiports）及其周边的容量限制起降选择。2. 各运营商在这些限制下独立规划。3. PSU 解决冲突路径，优化路径长度公平性。

Result: Fair-CoPlan 能够公平地分配冲突解决方案的成本，鼓励更广泛的参与，确保空域安全，并促进运营商的灵活性。仿真结果表明，与非公平规划器相比，Fair-CoPlan 能够实现更公平的分配，但会带来轻微的延迟。

Conclusion: Fair-CoPlan 通过仿真实验证明了其有效性，与非公平规划器相比，在公平性方面取得了更好的结果，但 M 略有延迟。

Abstract: Urban Air Mobility (UAM) is an emerging transportation paradigm in which
Uncrewed Aerial Systems (UAS) autonomously transport passengers and goods in
cities. The UAS have different operators with different, sometimes competing
goals, yet must share the airspace. We propose a negotiated, semi-distributed
flight planner that optimizes UAS' flight lengths {\em in a fair manner}.
Current flight planners might result in some UAS being given disproportionately
shorter flight paths at the expense of others. We introduce Fair-CoPlan, a
planner in which operators and a Provider of Service to the UAM (PSU) together
compute \emph{fair} flight paths. Fair-CoPlan has three steps: First, the PSU
constrains take-off and landing choices for flights based on capacity at and
around vertiports. Then, operators plan independently under these constraints.
Finally, the PSU resolves any conflicting paths, optimizing for path length
fairness. By fairly spreading the cost of deconfliction Fair-CoPlan encourages
wider participation in UAM, ensures safety of the airspace and the areas below
it, and promotes greater operator flexibility. We demonstrate Fair-CoPlan
through simulation experiments and find fairer outcomes than a non-fair planner
with minor delays as a trade-off.

</details>


### [132] [FiReFly: Fair Distributed Receding Horizon Planning for Multiple UAVs](https://arxiv.org/abs/2508.14381)
*Nicole Fronda,Bardh Hoxha,Houssam Abbas*

Main category: cs.RO

TL;DR: 提出 FiReFly 算法，在多机器人运动规划中引入公平性，优化能量消耗公平分配，提高任务成功率，并实现了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 当机器人存在竞争利益时，优化其资源使用中的公平性很重要，特别是公平地分配它们的能量消耗，同时保持任务成功。

Method: 提出了一种名为 FiReFly 的分布式公平运动规划算法，并将其与安全控制器集成。

Result: FiReFly 产生了更公平的轨迹，并提高了任务成功率。

Conclusion: FiReFly 在模拟的 reach-avoid 任务中，能够生成更公平的轨迹并提高任务成功率。实时性能可扩展到 15 架无人机，扩展到 50 架无人机则需要在运行时和公平性改进之间进行权衡。

Abstract: We propose injecting notions of fairness into multi-robot motion planning.
When robots have competing interests, it is important to optimize for some kind
of fairness in their usage of resources. In this work, we explore how the
robots' energy expenditures might be fairly distributed among them, while
maintaining mission success. We formulate a distributed fair motion planner and
integrate it with safe controllers in a algorithm called FiReFly. For simulated
reach-avoid missions, FiReFly produces fairer trajectories and improves mission
success rates over a non-fair planner. We find that real-time performance is
achievable up to 15 UAVs, and that scaling up to 50 UAVs is possible with
trade-offs between runtime and fairness improvements.

</details>


### [133] [Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations](https://arxiv.org/abs/2508.14383)
*Haitong Ma,Bo Dai,Zhaolin Ren,Yebin Wang,Na Li*

Main category: cs.RO

TL;DR: 通过预训练动力学表示，可以用少量数据（甚至一条轨迹）实现离线模仿学习。


<details>
  <summary>Details</summary>
Motivation: 离线模仿学习（IL）面临数据量有限的挑战。本研究旨在通过预训练动力学表示来解决这一瓶颈，利用大规模非专家数据和少量专家数据来提升IL性能。

Method: 提出了一种预训练阶段学习动力学表示的方法，该表示源于状态转移动力学的分解。通过对比学习的损失函数进行预训练，将决策变量映射到表示空间，从而减少下游模仿学习的参数量。

Result: 在MuJoCo的实验表明，该算法仅用一条轨迹即可模仿专家策略。在真实四足机器人的实验表明，可以利用模拟器数据预训练的动力学表示，从少量真实世界演示中学习行走。

Conclusion: 该方法通过引入预训练阶段来学习动力学表示，可以显著提高数据量有限的离线模仿学习性能。该方法能够利用大规模的非专家数据，并能从少量数据中学习到接近专家水平的策略。

Abstract: Limited data has become a major bottleneck in scaling up offline imitation
learning (IL). In this paper, we propose enhancing IL performance under limited
expert data by introducing a pre-training stage that learns dynamics
representations, derived from factorizations of the transition dynamics. We
first theoretically justify that the optimal decision variable of offline IL
lies in the representation space, significantly reducing the parameters to
learn in the downstream IL. Moreover, the dynamics representations can be
learned from arbitrary data collected with the same dynamics, allowing the
reuse of massive non-expert data and mitigating the limited data issues. We
present a tractable loss function inspired by noise contrastive estimation to
learn the dynamics representations at the pre-training stage. Experiments on
MuJoCo demonstrate that our proposed algorithm can mimic expert policies with
as few as a single trajectory. Experiments on real quadrupeds show that we can
leverage pre-trained dynamics representations from simulator data to learn to
walk from a few real-world demonstrations.

</details>


### [134] [DEXTER-LLM: Dynamic and Explainable Coordination of Multi-Robot Systems in Unknown Environments via Large Language Models](https://arxiv.org/abs/2508.14387)
*Yuxiao Zhu,Junfeng Chen,Xintong Zhang,Meng Guo,Zhongkui Li*

Main category: cs.RO

TL;DR: DEXTER-LLM框架通过结合LLM和基于模型的优化方法，解决了多机器人系统在未知环境中动态任务规划的适应性和可解释性问题，实验结果显示出显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在线协调开放和未知环境中多机器人系统时，特别是当语义特征动态触发新任务时，面临重大挑战。现有基于LLM的方法主要关注已知环境中的单次端到端解决方案，缺乏动态适应能力和规划过程的可解释性。

Method: 提出了一种新颖的DEXTER-LLM框架，包含四个模块：(i) 任务理解模块，用于解析自然语言或线性时序逻辑（LTL）公式指定的任务部分排序；(ii) 基于LLM的在线子任务生成模块，通过多阶段推理提高任务分解的准确性和可解释性；(iii) 最优子任务分配和调度模块，通过基于搜索的优化将子任务分配给机器人；(iv) 动态适应和人机循环验证模块，用于实现子任务及其分配的多速率、事件驱动更新，以应对在线检测到的新特征和任务。

Result: 实验评估表明，DEXTER-LLM框架在所有场景中均取得了100%的成功率，平均完成了160个任务和480个子任务（是基线的3倍），在适应过程中LLM查询减少了62%，并且在复合任务方面的计划质量提高了2倍。

Conclusion: DEXTER-LLM框架结合了LLM的开放世界推理能力和基于模型的分配方法的优化，解决了在线适应性和可解释性的关键问题。实验评估表明，在所有场景中成功率均为100%，平均完成160个任务和480个子任务（是基线的3倍），适应过程中LLM查询减少62%，复杂任务的计划质量提高2倍。

Abstract: Online coordination of multi-robot systems in open and unknown environments
faces significant challenges, particularly when semantic features detected
during operation dynamically trigger new tasks. Recent large language model
(LLMs)-based approaches for scene reasoning and planning primarily focus on
one-shot, end-to-end solutions in known environments, lacking both dynamic
adaptation capabilities for online operation and explainability in the
processes of planning. To address these issues, a novel framework (DEXTER-LLM)
for dynamic task planning in unknown environments, integrates four modules: (i)
a mission comprehension module that resolves partial ordering of tasks
specified by natural languages or linear temporal logic formulas (LTL); (ii) an
online subtask generator based on LLMs that improves the accuracy and
explainability of task decomposition via multi-stage reasoning; (iii) an
optimal subtask assigner and scheduler that allocates subtasks to robots via
search-based optimization; and (iv) a dynamic adaptation and human-in-the-loop
verification module that implements multi-rate, event-based updates for both
subtasks and their assignments, to cope with new features and tasks detected
online. The framework effectively combines LLMs' open-world reasoning
capabilities with the optimality of model-based assignment methods,
simultaneously addressing the critical issue of online adaptability and
explainability. Experimental evaluations demonstrate exceptional performances,
with 100% success rates across all scenarios, 160 tasks and 480 subtasks
completed on average (3 times the baselines), 62% less queries to LLMs during
adaptation, and superior plan quality (2 times higher) for compound tasks.
Project page at https://tcxm.github.io/DEXTER-LLM/

</details>


### [135] [FBI: Learning Dexterous In-hand Manipulation with Dynamic Visuotactile Shortcut Policy](https://arxiv.org/abs/2508.14441)
*Yijin Chen,Wenqiang Xu,Zhenjun Yu,Tutian Tang,Yutong Li,Siqiong Yao,Cewu Lu*

Main category: cs.RO

TL;DR: Robots struggle with dexterous in-hand manipulation because they often rely on only vision or touch, not both. This paper introduces FBI, a new framework that combines vision and touch using motion dynamics. FBI trains a one-step diffusion policy for real-time execution and shows better results than previous methods in simulations and real-world tests.


<details>
  <summary>Details</summary>
Motivation: Dexterous in-hand manipulation is a challenge for robots due to complex contact dynamics and partial observability. Current robotic approaches often rely on a single modality (vision or touch), which limits adaptability. This paper aims to improve robotic manipulation by synergizing vision and touch.

Method: Flow Before Imitation (FBI) is a visuotactile imitation learning framework that dynamically fuses tactile interactions with visual observations through motion dynamics. It establishes a causal link between tactile signals and object motion via a dynamics-aware latent model. FBI employs a transformer-based interaction module to fuse flow-derived tactile features with visual inputs, training a one-step diffusion policy for real-time execution.

Result: Extensive experiments demonstrate that FBI outperforms baseline methods in both simulation and the real world on two customized in-hand manipulation tasks and three standard dexterous manipulation tasks.

Conclusion: Dexterous in-hand manipulation is a long-standing challenge in robotics due to complex contact dynamics and partial observability. Humans synergize vision and touch for such tasks, but robotic approaches often prioritize one modality, limiting adaptability. This paper introduces Flow Before Imitation (FBI), a visuotactile imitation learning framework that dynamically fuses tactile interactions with visual observations through motion dynamics. Unlike prior static fusion methods, FBI establishes a causal link between tactile signals and object motion via a dynamics-aware latent model. FBI employs a transformer-based interaction module to fuse flow-derived tactile features with visual inputs, training a one-step diffusion policy for real-time execution. Extensive experiments demonstrate that the proposed method outperforms baseline methods in both simulation and the real world on two customized in-hand manipulation tasks and three standard dexterous manipulation tasks.

Abstract: Dexterous in-hand manipulation is a long-standing challenge in robotics due
to complex contact dynamics and partial observability. While humans synergize
vision and touch for such tasks, robotic approaches often prioritize one
modality, therefore limiting adaptability. This paper introduces Flow Before
Imitation (FBI), a visuotactile imitation learning framework that dynamically
fuses tactile interactions with visual observations through motion dynamics.
Unlike prior static fusion methods, FBI establishes a causal link between
tactile signals and object motion via a dynamics-aware latent model. FBI
employs a transformer-based interaction module to fuse flow-derived tactile
features with visual inputs, training a one-step diffusion policy for real-time
execution. Extensive experiments demonstrate that the proposed method
outperforms the baseline methods in both simulation and the real world on two
customized in-hand manipulation tasks and three standard dexterous manipulation
tasks. Code, models, and more results are available in the website
https://sites.google.com/view/dex-fbi.

</details>


### [136] [Taming VR Teleoperation and Learning from Demonstration for Multi-Task Bimanual Table Service Manipulation](https://arxiv.org/abs/2508.14542)
*Weize Li,Zhengxiao Han,Lixin Xu,Xiangyu Chen,Harrison Bounds,Chenrui Zhang,Yifan Xu*

Main category: cs.RO

TL;DR: 该技术报告介绍了ICRA 2025比赛WBCD赛项的服务机器人解决方案，该方案结合VR远程操作和演示学习，成功完成了铺桌布、放披萨、开关餐盒等任务，并最终夺冠。


<details>
  <summary>Details</summary>
Motivation: 在ICRA 2025比赛中，为了应对在速度、精度和可靠性方面有严格要求的餐桌服务任务，包括铺桌布、放置披萨和开关食品容器。

Method: 该解决方案结合了基于VR的远程操作和演示学习（LfD），其中大部分子任务通过高保真远程操作完成，而披萨放置任务则使用了基于ACT的策略，该策略是从100次随机初始配置的远程操作演示中训练出来的。

Result: 该解决方案在比赛中获得了第一名，成功应对了铺桌布、放置披萨和开关食品容器等任务。

Conclusion: 该解决方案通过结合VR远程操作和演示学习（LfD）取得了高效率和高可靠性，赢得了ICRA 2025比赛的冠军。

Abstract: This technical report presents the champion solution of the Table Service
Track in the ICRA 2025 What Bimanuals Can Do (WBCD) competition. We tackled a
series of demanding tasks under strict requirements for speed, precision, and
reliability: unfolding a tablecloth (deformable-object manipulation), placing a
pizza onto the table (pick-and-place), and opening and closing a food container
with the lid. Our solution combines VR-based teleoperation and Learning from
Demonstrations (LfD) to balance robustness and autonomy. Most subtasks were
executed through high-fidelity remote teleoperation, while the pizza placement
was handled by an ACT-based policy trained from 100 in-person teleoperated
demonstrations with randomized initial configurations. By carefully integrating
scoring rules, task characteristics, and current technical capabilities, our
approach achieved both high efficiency and reliability, ultimately securing the
first place in the competition.

</details>


### [137] [EAROL: Environmental Augmented Perception-Aware Planning and Robust Odometry via Downward-Mounted Tilted LiDAR](https://arxiv.org/abs/2508.14554)
*Xinkai Liang,Yigu Ge,Yangxi Shi,Haoyu Yang,Xu Cao,Hao Fang*

Main category: cs.RO

TL;DR: 提出EAROL框架，通过倾斜激光雷达和LIO系统，结合分层轨迹优化，解决了无人机在开放场景下的定位和规划问题，并在实验中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决无人机在开放场景（如倒塌的建筑物、无顶迷宫）中运行时的定位漂移和感知规划耦合问题。

Method: 提出了一种名为EAROL的新型框架，采用向下倾斜的激光雷达配置（20度倾斜），集成了激光雷达-惯性里程计（LIO）系统和分层轨迹偏航优化算法。通过迭代误差状态卡尔曼滤波器（IESKF）和动态运动补偿实现高精度6-DoF定位，并对规划器进行了增强。

Result: 物理实验表明，跟踪误差减少了81%，感知覆盖率提高了22%，垂直漂移接近于零。该框架能够进行动态障碍物检测，并平衡环境探索、目标跟踪精度和能源效率。

Conclusion: 该工作提出了一种软硬件协同设计范式，为灾后搜救任务中的无人机自主性提供了稳健的解决方案。

Abstract: To address the challenges of localization drift and perception-planning
coupling in unmanned aerial vehicles (UAVs) operating in open-top scenarios
(e.g., collapsed buildings, roofless mazes), this paper proposes EAROL, a novel
framework with a downward-mounted tilted LiDAR configuration (20{\deg}
inclination), integrating a LiDAR-Inertial Odometry (LIO) system and a
hierarchical trajectory-yaw optimization algorithm. The hardware innovation
enables constraint enhancement via dense ground point cloud acquisition and
forward environmental awareness for dynamic obstacle detection. A
tightly-coupled LIO system, empowered by an Iterative Error-State Kalman Filter
(IESKF) with dynamic motion compensation, achieves high level 6-DoF
localization accuracy in feature-sparse environments. The planner, augmented by
environment, balancing environmental exploration, target tracking precision,
and energy efficiency. Physical experiments demonstrate 81% tracking error
reduction, 22% improvement in perceptual coverage, and near-zero vertical drift
across indoor maze and 60-meter-scale outdoor scenarios. This work proposes a
hardware-algorithm co-design paradigm, offering a robust solution for UAV
autonomy in post-disaster search and rescue missions. We will release our
software and hardware as an open-source package for the community. Video:
https://youtu.be/7av2ueLSiYw.

</details>


### [138] [TRUST-Planner: Topology-guided Robust Trajectory Planner for AAVs with Uncertain Obstacle Spatial-temporal Avoidance](https://arxiv.org/abs/2508.14610)
*Junzhi Li,Teng Long,Jingliang Sun,Jianxin Zhong*

Main category: cs.RO

TL;DR: TRUST-Planner is a new motion planning framework for AAVs that uses topology-guided hierarchical planning with DEV-PRM and UTF-MINCO with DDF to overcome local minima and deadlock issues, achieving high success rates and efficiency in complex environments.


<details>
  <summary>Details</summary>
Motivation: Existing motion planning frameworks for autonomous aerial vehicles (AAVs) face challenges with local minima and deadlock in complex dynamic environments, increasing collision risks. TRUST-Planner aims to address these challenges for robust spatial-temporal obstacle avoidance.

Method: TRUST-Planner is a topology-guided hierarchical planning framework. It uses a dynamic enhanced visible probabilistic roadmap (DEV-PRM) in the frontend for topological path exploration and a uniform terminal-free minimum control polynomial (UTF-MINCO) and dynamic distance field (DDF) in the backend for obstacle avoidance and parallel computation. It also includes an incremental multi-branch trajectory management framework for spatio-temporal decision-making and efficient replanning.

Result: TRUST-Planner achieves a 96% success rate and millisecond-level computation efficiency in complex environments, outperforming baseline competitors. Its feasibility and practicality are validated by real-world experiments.

Conclusion: TRUST-Planner outperformed baseline competitors, achieving a 96% success rate and millisecond-level computation efficiency in tested complex environments. Real-world experiments further validate the feasibility and practicality of the proposed method.

Abstract: Despite extensive developments in motion planning of autonomous aerial
vehicles (AAVs), existing frameworks faces the challenges of local minima and
deadlock in complex dynamic environments, leading to increased collision risks.
To address these challenges, we present TRUST-Planner, a topology-guided
hierarchical planning framework for robust spatial-temporal obstacle avoidance.
In the frontend, a dynamic enhanced visible probabilistic roadmap (DEV-PRM) is
proposed to rapidly explore topological paths for global guidance. The backend
utilizes a uniform terminal-free minimum control polynomial (UTF-MINCO) and
dynamic distance field (DDF) to enable efficient predictive obstacle avoidance
and fast parallel computation. Furthermore, an incremental multi-branch
trajectory management framework is introduced to enable spatio-temporal
topological decision-making, while efficiently leveraging historical
information to reduce replanning time. Simulation results show that
TRUST-Planner outperforms baseline competitors, achieving a 96\% success rate
and millisecond-level computation efficiency in tested complex environments.
Real-world experiments further validate the feasibility and practicality of the
proposed method.

</details>


### [139] [Can LLM Agents Solve Collaborative Tasks? A Study on Urgency-Aware Planning and Coordination](https://arxiv.org/abs/2508.14635)
*João Vitor de Carvalho Silva,Douglas G. Macharet*

Main category: cs.RO

TL;DR: LLM agents were tested on a victim rescue task. They showed potential but also coordination issues. More research is needed.


<details>
  <summary>Details</summary>
Motivation: To investigate the potential of Large Language Models (LLMs) to support effective collaboration in multi-agent settings, specifically in a structured victim rescue task requiring coordination.

Method: The study evaluates LLM agents in a graph-based environment for a structured victim rescue task, using metrics such as task success rate, redundant actions, room conflicts, and urgency-weighted efficiency.

Result: The paper systematically evaluates the performance of LLM agents in a victim rescue scenario, providing insights into their strengths and weaknesses in physically grounded multi-agent collaboration.

Conclusion: LLM agents show potential in solving structured multi-agent tasks like victim rescue, but failure modes in coordination need further investigation for future benchmarks and architectural improvements.

Abstract: The ability to coordinate actions across multiple agents is critical for
solving complex, real-world problems. Large Language Models (LLMs) have shown
strong capabilities in communication, planning, and reasoning, raising the
question of whether they can also support effective collaboration in
multi-agent settings. In this work, we investigate the use of LLM agents to
solve a structured victim rescue task that requires division of labor,
prioritization, and cooperative planning. Agents operate in a fully known
graph-based environment and must allocate resources to victims with varying
needs and urgency levels. We systematically evaluate their performance using a
suite of coordination-sensitive metrics, including task success rate, redundant
actions, room conflicts, and urgency-weighted efficiency. This study offers new
insights into the strengths and failure modes of LLMs in physically grounded
multi-agent collaboration tasks, contributing to future benchmarks and
architectural improvements.

</details>


### [140] [An Informative Planning Framework for Target Tracking and Active Mapping in Dynamic Environments with ASVs](https://arxiv.org/abs/2508.14636)
*Sanjeev Ramkumar Sudha,Marija Popović,Erlend M. Coates*

Main category: cs.RO

TL;DR: 提出一种信息路径规划框架，用于在动态环境中跟踪未知移动目标。通过时空预测网络预测目标位置，并使用自适应规划目标进行跟踪，提高了跟踪性能。


<details>
  <summary>Details</summary>
Motivation: 移动机器人平台在环境监测等信息收集任务中的应用日益广泛，而在动态环境中实现高效的目标跟踪对于搜索救援和污染物清理等应用至关重要。然而，在风和洋流等环境扰动下对漂浮目标进行主动绘制是一个具有挑战性的问题，因为它涉及到由于不断变化的情况而导致的地图空间和时间变化预测。

Method: 提出了一种信息路径规划框架，利用时空预测网络来预测目标随时间的位置分布，并结合自适应规划目标进行目标跟踪。

Result: 与仅考虑熵减少作为规划目标的现有方法相比，所提出的规划目标提高了目标跟踪性能。

Conclusion: 该方法在真实世界的监测场景中能够有效地跟踪目标。

Abstract: Mobile robot platforms are increasingly being used to automate information
gathering tasks such as environmental monitoring. Efficient target tracking in
dynamic environments is critical for applications such as search and rescue and
pollutant cleanups. In this letter, we study active mapping of floating targets
that drift due to environmental disturbances such as wind and currents. This is
a challenging problem as it involves predicting both spatial and temporal
variations in the map due to changing conditions. We propose an informative
path planning framework to map an arbitrary number of moving targets with
initially unknown positions in dynamic environments. A key component of our
approach is a spatiotemporal prediction network that predicts target position
distributions over time. We propose an adaptive planning objective for target
tracking that leverages these predictions. Simulation experiments show that our
proposed planning objective improves target tracking performance compared to
existing methods that consider only entropy reduction as the planning
objective. Finally, we validate our approach in field tests using an autonomous
surface vehicle, showcasing its ability to track targets in real-world
monitoring scenarios.

</details>


### [141] [Consistent Pose Estimation of Unmanned Ground Vehicles through Terrain-Aided Multi-Sensor Fusion on Geometric Manifolds](https://arxiv.org/abs/2508.14661)
*Alexander Raab,Stephan Weiss,Alessandro Fornasier,Christian Brommer,Abdalrahman Ibrahim*

Main category: cs.RO

TL;DR: This paper introduces the Manifold Error State Extended Kalman Filter (M-ESEKF) to improve the accuracy and consistency of Kalman Filters for vehicle localization by representing robot pose in a lower-dimensional space and incorporating a novel correction scheme.


<details>
  <summary>Details</summary>
Motivation: Aiming to enhance the consistency and thus long-term accuracy of Extended Kalman Filters for terrestrial vehicle localization.

Method: The paper introduces the Manifold Error State Extended Kalman Filter (M-ESEKF) by representing the robot

Result: Extensive Monte Carlo simulations across diverse scenarios and dynamic sensor configurations show that the M-ESEKF outperforms classical filter formulations in terms of consistency and stability.

Conclusion: M-ESEKF outperforms classical filter formulations in terms of consistency and stability, and eliminates the need for scenario-specific parameter tuning, enabling its application in a variety of real-world settings.

Abstract: Aiming to enhance the consistency and thus long-term accuracy of Extended
Kalman Filters for terrestrial vehicle localization, this paper introduces the
Manifold Error State Extended Kalman Filter (M-ESEKF). By representing the
robot's pose in a space with reduced dimensionality, the approach ensures
feasible estimates on generic smooth surfaces, without introducing artificial
constraints or simplifications that may degrade a filter's performance. The
accompanying measurement models are compatible with common loosely- and
tightly-coupled sensor modalities and also implicitly account for the ground
geometry. We extend the formulation by introducing a novel correction scheme
that embeds additional domain knowledge into the sensor data, giving more
accurate uncertainty approximations and further enhancing filter consistency.
The proposed estimator is seamlessly integrated into a validated modular state
estimation framework, demonstrating compatibility with existing
implementations. Extensive Monte Carlo simulations across diverse scenarios and
dynamic sensor configurations show that the M-ESEKF outperforms classical
filter formulations in terms of consistency and stability. Moreover, it
eliminates the need for scenario-specific parameter tuning, enabling its
application in a variety of real-world settings.

</details>


### [142] [Safe and Transparent Robots for Human-in-the-Loop Meat Processing](https://arxiv.org/abs/2508.14763)
*Sagar Parekh,Casey Grothoff,Ryan Wright,Robin White,Dylan P. Losey*

Main category: cs.RO

TL;DR: Due to labor shortages, this paper presents a general-purpose robot framework for meat processing that works alongside humans. It includes safety features like hand detection and an instrumented knife, and transparency features like an LED interface for robot uncertainty and a graphical interface for planning and feedback. A user study validated the framework.


<details>
  <summary>Details</summary>
Motivation: Labor shortages in the meat processing sector necessitate the development of automated technology. Existing automation is specialized, inflexible, and costly. The objective is to create general-purpose robotic systems that collaborate with humans on multiple tasks.

Method: The paper introduces a safety and transparency framework for general-purpose meat processing robots. Safety is ensured through a hand-detection system that halts the robot when humans are too close and an instrumented knife with a force sensor to differentiate contact with meat, bone, or fixtures. Transparency is achieved via a method to detect and communicate robot uncertainty using an LED interface, and a graphical interface displaying robot plans and allowing human feedback.

Result: The framework addresses safety by implementing a hand-detection system and an instrumented knife. It enhances transparency through an uncertainty detection method with an LED interface and a graphical interface for robot plans and human feedback. The effectiveness was validated through a user study.

Conclusion: The developed framework ensures safe operation and keeps human workers informed about the robot's actions, as validated by a user study.

Abstract: Labor shortages have severely affected the meat processing sector. Automated
technology has the potential to support the meat industry, assist workers, and
enhance job quality. However, existing automation in meat processing is highly
specialized, inflexible, and cost intensive. Instead of forcing manufacturers
to buy a separate device for each step of the process, our objective is to
develop general-purpose robotic systems that work alongside humans to perform
multiple meat processing tasks. Through a recently conducted survey of industry
experts, we identified two main challenges associated with integrating these
collaborative robots alongside human workers. First, there must be measures to
ensure the safety of human coworkers; second, the coworkers need to understand
what the robot is doing. This paper addresses both challenges by introducing a
safety and transparency framework for general-purpose meat processing robots.
For safety, we implement a hand-detection system that continuously monitors
nearby humans. This system can halt the robot in situations where the human
comes into close proximity of the operating robot. We also develop an
instrumented knife equipped with a force sensor that can differentiate contact
between objects such as meat, bone, or fixtures. For transparency, we introduce
a method that detects the robot's uncertainty about its performance and uses an
LED interface to communicate that uncertainty to the human. Additionally, we
design a graphical interface that displays the robot's plans and allows the
human to provide feedback on the planned cut. Overall, our framework can ensure
safe operation while keeping human workers in-the-loop about the robot's
actions which we validate through a user study.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [143] [STAS: Spatio-Temporal Adaptive Computation Time for Spiking Transformers](https://arxiv.org/abs/2508.14138)
*Donghwa Kang,Doohyun Kim,Sang-Ki Ko,Jinkyu Lee,Brent ByungHoon Kang,Hyeongboo Baek*

Main category: cs.LG

TL;DR: STAS框架通过I-SPS模块解决SNN-ViT的时间稳定性问题，并利用A-SSA模块进行二维令牌修剪，从而在降低能耗的同时提高准确性。


<details>
  <summary>Details</summary>
Motivation: 传统的SNN动态计算方法存在碎片化问题。ACT虽然提供了一个统一的方法基础，但在SNN-ViT上的应用受到时间相似性先决条件被违反和静态架构不适合其原理的阻碍。因此，需要一个能够解决这些挑战的框架。

Method: STAS框架通过共设计静态架构和动态计算策略来解决SNN-ViT中的ACT应用问题。具体包括：1.集成脉冲块分割（I-SPS）模块：通过创建统一的输入表示来建立时间稳定性，解决时间不相似性问题。2.自适应脉冲自注意力（A-SSA）模块：在时间稳定性基础上，实现跨空间和时间轴的二维令牌修剪。

Result: 在CIFAR-10、CIFAR-100和ImageNet上，STAS框架将能耗分别降低了45.9%、43.8%和30.1%，同时提高了准确性，优于现有SOTA模型。

Conclusion: STAS通过集成脉冲块分割（I-SPS）模块和自适应脉冲自注意力（A-SSA）模块，解决了SNN-ViT中ACT应用的两个核心问题：时间相似性先决条件被违反和静态架构不适合其原理。I-SPS模块通过创建统一的输入表示来建立时间稳定性，而A-SSA模块则能在空间和时间轴上进行二维令牌修剪。该框架在CIFAR-10、CIFAR-100和ImageNet上进行了验证，并在降低能耗的同时提高了准确性，实现了高达45.9%、43.8%和30.1%的能耗降低。

Abstract: Spiking neural networks (SNNs) offer energy efficiency over artificial neural
networks (ANNs) but suffer from high latency and computational overhead due to
their multi-timestep operational nature. While various dynamic computation
methods have been developed to mitigate this by targeting spatial, temporal, or
architecture-specific redundancies, they remain fragmented. While the
principles of adaptive computation time (ACT) offer a robust foundation for a
unified approach, its application to SNN-based vision Transformers (ViTs) is
hindered by two core issues: the violation of its temporal similarity
prerequisite and a static architecture fundamentally unsuited for its
principles. To address these challenges, we propose STAS (Spatio-Temporal
Adaptive computation time for Spiking transformers), a framework that
co-designs the static architecture and dynamic computation policy. STAS
introduces an integrated spike patch splitting (I-SPS) module to establish
temporal stability by creating a unified input representation, thereby solving
the architectural problem of temporal dissimilarity. This stability, in turn,
allows our adaptive spiking self-attention (A-SSA) module to perform
two-dimensional token pruning across both spatial and temporal axes.
Implemented on spiking Transformer architectures and validated on CIFAR-10,
CIFAR-100, and ImageNet, STAS reduces energy consumption by up to 45.9%, 43.8%,
and 30.1%, respectively, while simultaneously improving accuracy over SOTA
models.

</details>


### [144] [Deep Learning for School Dropout Detection: A Comparison of Tabular and Graph-Based Models for Predicting At-Risk Students](https://arxiv.org/abs/2508.14057)
*Pablo G. Almeida,Guilherme A. L. Silva,Valéria Santos,Gladston Moreira,Pedro Silva,Eduardo Luz*

Main category: cs.LG

TL;DR: GNNs can improve student dropout prediction, but success depends on how you build the graph from the data and which GNN you use. GraphSAGE worked best when the data was prepared using PCA and K-Means.


<details>
  <summary>Details</summary>
Motivation: Student dropout is a significant issue with considerable social and economic costs. Predicting students at risk allows for timely interventions. While traditional Machine Learning (ML) models on tabular data have been used, Graph Neural Networks (GNNs) offer a potential advantage by capturing complex relationships in student data when structured as graphs. This research aims to explore whether transforming tabular data into graphs enhances dropout prediction accuracy.

Method: This paper investigates the use of Graph Neural Networks (GNNs), including a custom Graph Convolutional Network (GCN) and GraphSAGE, for student dropout prediction. Tabular student data was transformed into graph structures using clustering techniques (K-Means, HDBSCAN) and dimensionality reduction (PCA, UMAP). The performance of these GNNs was compared against traditional tabular models such as Random Forest (RF), XGBoost, and TabNet using a real-world student dataset.

Result: A specific GNN configuration, GraphSAGE on a graph constructed using PCA followed by K-Means clustering, achieved superior performance. This configuration improved the macro F1-score by approximately 7 percentage points and accuracy by nearly 2 percentage points compared to the best tabular baseline (XGBoost). However, other GNN configurations and graph construction methods did not consistently yield better results than tabular models.

Conclusion: GNNs, specifically GraphSAGE on a graph derived from PCA-KMeans clustering, show superior performance in student dropout prediction compared to traditional tabular models like XGBoost, improving macro F1-score by approximately 7 percentage points and accuracy by nearly 2 percentage points. However, the effectiveness of GNNs is highly dependent on the graph generation strategy and GNN architecture selection, as not all GNN configurations consistently outperformed tabular models.

Abstract: Student dropout is a significant challenge in educational systems worldwide,
leading to substantial social and economic costs. Predicting students at risk
of dropout allows for timely interventions. While traditional Machine Learning
(ML) models operating on tabular data have shown promise, Graph Neural Networks
(GNNs) offer a potential advantage by capturing complex relationships inherent
in student data if structured as graphs. This paper investigates whether
transforming tabular student data into graph structures, primarily using
clustering techniques, enhances dropout prediction accuracy. We compare the
performance of GNNs (a custom Graph Convolutional Network (GCN) and GraphSAGE)
on these generated graphs against established tabular models (Random Forest
(RF), XGBoost, and TabNet) using a real-world student dataset. Our experiments
explore various graph construction strategies based on different clustering
algorithms (K-Means, HDBSCAN) and dimensionality reduction techniques
(Principal Component Analysis (PCA), Uniform Manifold Approximation and
Projection (UMAP)). Our findings demonstrate that a specific GNN configuration,
GraphSAGE on a graph derived from PCA-KMeans clustering, achieved superior
performance, notably improving the macro F1-score by approximately 7 percentage
points and accuracy by nearly 2 percentage points over the strongest tabular
baseline (XGBoost). However, other GNN configurations and graph construction
methods did not consistently surpass tabular models, emphasizing the critical
role of the graph generation strategy and GNN architecture selection. This
highlights both the potential of GNNs and the challenges in optimally
transforming tabular data for graph-based learning in this domain.

</details>


### [145] [Load Forecasting on A Highly Sparse Electrical Load Dataset Using Gaussian Interpolation](https://arxiv.org/abs/2508.14069)
*Chinmoy Biswas,Nafis Faisal,Vivek Chowdhury,Abrar Al-Shadid Abir,Sabir Mahmud,Mithon Rahman,Shaikh Anowarul Fattah,Hafiz Imtiaz*

Main category: cs.LG

TL;DR: 研究表明，高斯插值可用于处理电力线负载预测中的稀疏数据（高达62%），并且LSTM模型在该任务上表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决现实生活中稀疏数据集（特别是含有62%缺失值的电力线负载数据）在电力线负载预测中的应用挑战。

Method: 本研究通过对稀疏数据集进行统计分析，并训练包括机器学习和深度学习在内的多种模型，来评估不同插值方法（特别是高斯插值）在电力线负载预测中的有效性。

Result: 与传统方法相比，使用高斯插值增强的稀疏数据集能够实现准确的电力线负载预测，并且在所测试的模型中，LSTM模型取得了最优性能。

Conclusion: 高斯插值是一种处理电力线负载预测中数据稀疏性问题的有效方法，其中基于长短期记忆（LSTM）的神经网络模型表现最佳。

Abstract: Sparsity, defined as the presence of missing or zero values in a dataset,
often poses a major challenge while operating on real-life datasets. Sparsity
in features or target data of the training dataset can be handled using various
interpolation methods, such as linear or polynomial interpolation, spline,
moving average, or can be simply imputed. Interpolation methods usually perform
well with Strict Sense Stationary (SSS) data. In this study, we show that an
approximately 62\% sparse dataset with hourly load data of a power plant can be
utilized for load forecasting assuming the data is Wide Sense Stationary (WSS),
if augmented with Gaussian interpolation. More specifically, we perform
statistical analysis on the data, and train multiple machine learning and deep
learning models on the dataset. By comparing the performance of these models,
we empirically demonstrate that Gaussian interpolation is a suitable option for
dealing with load forecasting problems. Additionally, we demonstrate that Long
Short-term Memory (LSTM)-based neural network model offers the best performance
among a diverse set of classical and neural network-based models.

</details>


### [146] [Generative AI Against Poaching: Latent Composite Flow Matching for Wildlife Conservation](https://arxiv.org/abs/2508.14342)
*Lingkai Kong,Haichuan Wang,Charles A. Emogor,Vincent Börsch-Supan,Lily Xu,Milind Tambe*

Main category: cs.LG

TL;DR: 该研究提出了一种新的盗猎预测方法，结合了流匹配和占用率模型，并使用线性模型进行初始化，以解决数据不精确和稀疏的问题，并在实际应用中提高了预测精度。


<details>
  <summary>Details</summary>
Motivation: 为了减少盗猎行为，预测盗猎者的行为至关重要，现有方法（线性模型或决策树）在捕捉复杂的时空模式方面存在局限性。生成模型，特别是流匹配，提供了一种更灵活的替代方案，但面临检测不精确和数据有限的挑战。

Method: 提出了一种结合流匹配和基于占用率的检测模型的方法，并在潜在空间中进行训练，以推断潜在的占用状态。为了缓解数据稀疏性问题，采用了从线性模型预测初始化的复合流。

Result: 在乌干达两个国家公园的数据集上进行的评估显示，预测精度得到了一致的提高。

Conclusion: 该研究通过整合基于占用率的检测模型和流匹配，并采用从线性模型预测初始化的复合流，在预测精度方面取得了持续的提升。

Abstract: Poaching poses significant threats to wildlife and biodiversity. A valuable
step in reducing poaching is to forecast poacher behavior, which can inform
patrol planning and other conservation interventions. Existing poaching
prediction methods based on linear models or decision trees lack the
expressivity to capture complex, nonlinear spatiotemporal patterns. Recent
advances in generative modeling, particularly flow matching, offer a more
flexible alternative. However, training such models on real-world poaching data
faces two central obstacles: imperfect detection of poaching events and limited
data. To address imperfect detection, we integrate flow matching with an
occupancy-based detection model and train the flow in latent space to infer the
underlying occupancy state. To mitigate data scarcity, we adopt a composite
flow initialized from a linear-model prediction rather than random noise which
is the standard in diffusion models, injecting prior knowledge and improving
generalization. Evaluations on datasets from two national parks in Uganda show
consistent gains in predictive accuracy.

</details>


### [147] [CoBAD: Modeling Collective Behaviors for Human Mobility Anomaly Detection](https://arxiv.org/abs/2508.14088)
*Haomin Wen,Shurui Cao,Leman Akoglu*

Main category: cs.LG

TL;DR: CoBAD是一种新颖的模型，用于检测人类移动中的集体异常，通过模拟个体间的时空依赖性来识别意外的共现和缺失异常。


<details>
  <summary>Details</summary>
Motivation: 传统异常检测方法主要关注个体移动模式，而集体异常检测（识别个体间集体移动行为的异常）是一个未被充分探索的挑战，需要模拟个体间的时空依赖性。

Method: CoBAD采用基于共现事件图的集体事件序列（CES）的无监督学习方法，并利用两阶段注意力机制来模拟个体移动模式和个体间的相互作用。

Result: CoBAD能够检测到两种集体异常：意外的共现异常和缺失异常，其中缺失异常在先前的工作中在很大程度上被忽视了。他们在AUCROC和AUCPR方面取得了显著的改进。

Conclusion: CoBAD在大型移动数据集上的广泛实验证明，其性能显著优于现有的异常检测基线，在AUCROC方面提高了13%-18%，在AUCPR方面提高了19%-70%。

Abstract: Detecting anomalies in human mobility is essential for applications such as
public safety and urban planning. While traditional anomaly detection methods
primarily focus on individual movement patterns (e.g., a child should stay at
home at night), collective anomaly detection aims to identify irregularities in
collective mobility behaviors across individuals (e.g., a child is at home
alone while the parents are elsewhere) and remains an underexplored challenge.
Unlike individual anomalies, collective anomalies require modeling
spatiotemporal dependencies between individuals, introducing additional
complexity. To address this gap, we propose CoBAD, a novel model designed to
capture Collective Behaviors for human mobility Anomaly Detection. We first
formulate the problem as unsupervised learning over Collective Event Sequences
(CES) with a co-occurrence event graph, where CES represents the event
sequences of related individuals. CoBAD then employs a two-stage attention
mechanism to model both the individual mobility patterns and the interactions
across multiple individuals. Pre-trained on large-scale collective behavior
data through masked event and link reconstruction tasks, CoBAD is able to
detect two types of collective anomalies: unexpected co-occurrence anomalies
and absence anomalies, the latter of which has been largely overlooked in prior
work. Extensive experiments on large-scale mobility datasets demonstrate that
CoBAD significantly outperforms existing anomaly detection baselines, achieving
an improvement of 13%-18% in AUCROC and 19%-70% in AUCPR. All source code is
available at https://github.com/wenhaomin/CoBAD.

</details>


### [148] [Edge-Selector Model Applied for Local Search Neighborhood for Solving Vehicle Routing Problems](https://arxiv.org/abs/2508.14071)
*Bachtiar Herdianto,Romain Billot,Flavien Lucas,Marc Sevaux,Daniele Vigo*

Main category: cs.LG

TL;DR: 提出了一种结合机器学习（表格分类器和GNN）与元启发式算法的混合方法来解决车辆路径问题，实现了性能提升和良好的可扩展性、泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决车辆路径问题（VRP），并提高现有元启发式算法的性能。

Method: 提出了一种混合机器学习和元启发式机制来解决车辆路径问题（VRP）。核心是一个边解选择器模型，它对解边进行分类以识别局部搜索中的禁止移动，从而指导元启发式基线内的搜索过程。该选择器采用梯度提升树和前馈神经网络作为基线算法的表格二元分类器，并调整决策阈值来处理类别不平衡问题。另一种机制使用图神经网络（GNN）利用图结构进行直接解边预测，通过预测禁止移动来指导局部搜索。

Result: 该混合机制在包括CVRP和CVRPTW在内的不同问题变体和规模上，在不同基线元启发式算法上均实现了性能提升，并且在多达30,000个客户节点的大规模数据集上进行了验证。

Conclusion: 该混合机制在基准数据集上展示了良好的可扩展性和泛化能力，在不同的基线元启发式算法、各种问题规模和变体（包括标准车辆路径问题（CVRP）和带时间窗的CVRP（CVRPTW））上均实现了性能提升。

Abstract: This research proposes a hybrid Machine Learning and metaheuristic mechanism
that is designed to solve Vehicle Routing Problems (VRPs). The main of our
method is an edge solution selector model, which classifies solution edges to
identify prohibited moves during the local search, hence guiding the search
process within metaheuristic baselines. Two learning-based mechanisms are used
to develop the edge selector: a simple tabular binary classifier and a Graph
Neural Network (GNN). The tabular classifier employs Gradient Boosting Trees
and Feedforward Neural Network as the baseline algorithms. Adjustments to the
decision threshold are also applied to handle the class imbalance in the
problem instance. An alternative mechanism employs the GNN to utilize graph
structure for direct solution edge prediction, with the objective of guiding
local search by predicting prohibited moves. These hybrid mechanisms are then
applied in state-fo-the-art metaheuristic baselines. Our method demonstrates
both scalability and generalizability, achieving performance improvements
across different baseline metaheuristics, various problem sizes and variants,
including the Capacitated Vehicle Routing Problem (CVRP) and CVRP with Time
Windows (CVRPTW). Experimental evaluations on benchmark datasets up to 30,000
customer nodes, supported by pair-wise statistical analysis, verify the
observed improvements.

</details>


### [149] [Multi-Objective Bayesian Optimization with Independent Tanimoto Kernel Gaussian Processes for Diverse Pareto Front Exploration](https://arxiv.org/abs/2508.14072)
*Anabel Yong*

Main category: cs.LG

TL;DR: GP-MOBO是一种新的多目标贝叶斯优化算法，它通过高效处理分子指纹的全部维度来改进分子优化，并在化学空间探索和帕累托前沿识别方面优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 提出一种能有效处理稀疏分子指纹全部维度且计算资源需求少的多目标贝叶斯优化算法，以改进分子优化。

Method: GP-MOBO是一种新的多目标贝叶斯优化算法，它集成了精确高斯过程（GPs）的快速最小包，能够高效处理稀疏分子指纹的全部维度，而无需大量计算资源。

Result: GP-MOBO在分子优化方面优于现有技术，通过充分利用指纹维度来识别高质量、有效的SMILES，并在化学搜索空间中实现更广泛的探索，在所有测试场景中都更接近帕累托前沿。

Conclusion: GP-MOBO在所有测试场景中均优于传统方法，并在DockSTRING数据集上实现了更高的几何平均值，证明了其在解决复杂多目标优化挑战方面的有效性、效率和较低的计算开销。

Abstract: We present GP-MOBO, a novel multi-objective Bayesian Optimization algorithm
that advances the state-of-the-art in molecular optimization. Our approach
integrates a fast minimal package for Exact Gaussian Processes (GPs) capable of
efficiently handling the full dimensionality of sparse molecular fingerprints
without the need for extensive computational resources. GP-MOBO consistently
outperforms traditional methods like GP-BO by fully leveraging fingerprint
dimensionality, leading to the identification of higher-quality and valid
SMILES. Moreover, our model achieves a broader exploration of the chemical
search space, as demonstrated by its superior proximity to the Pareto front in
all tested scenarios. Empirical results from the DockSTRING dataset reveal that
GP-MOBO yields higher geometric mean values across 20 Bayesian optimization
iterations, underscoring its effectiveness and efficiency in addressing complex
multi-objective optimization challenges with minimal computational overhead.

</details>


### [150] [MCLPD:Multi-view Contrastive Learning for EEG-based PD Detection Across Datasets](https://arxiv.org/abs/2508.14073)
*Qian Zhanga,Ruilin Zhang,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: MCLPD是一个半监督学习框架，通过对比预训练和少量监督微调，提高了EEG帕金森病检测的跨数据集性能，并减少了对标签数据的需求。


<details>
  <summary>Details</summary>
Motivation: EEG数据标注成本高导致数据集规模有限，且不同数据集在采集协议和受试者人口统计学方面存在差异，这严重阻碍了模型在跨数据集检测场景下的鲁棒性和泛化能力。因此，需要一种能增强跨数据集检测性能并减少对标签数据依赖的框架。

Method: 提出了一种名为MCLPD的半监督学习框架，该框架整合了多视图对比预训练和轻量级监督微调。在预训练阶段，利用UNM数据集的无标签数据进行自监督学习，并通过时域和频域的双重增强来构建对比对，融合时频信息。在微调阶段，仅使用少量来自UI和UC数据集的标签数据进行监督优化。

Result: MCLPD在使用1%标签数据时，在UI数据集上达到0.91 F1分数，在UC数据集上达到0.81 F1分数；当使用5%标签数据时，F1分数分别提高到0.97和0.87。与现有方法相比，MCLPD显著提高了跨数据集泛化能力，并减少了对标签数据的依赖。

Conclusion: MCLPD框架通过结合多视图对比预训练和轻量级监督微调，有效提高了跨数据集帕金森病检测的性能，同时减少了对标签数据的依赖。

Abstract: Electroencephalography has been validated as an effective technique for
detecting Parkinson's disease,particularly in its early stages.However,the high
cost of EEG data annotation often results in limited dataset size and
considerable discrepancies across datasets,including differences in acquisition
protocols and subject demographics,significantly hinder the robustness and
generalizability of models in cross-dataset detection scenarios.To address such
challenges,this paper proposes a semi-supervised learning framework named
MCLPD,which integrates multi-view contrastive pre-training with lightweight
supervised fine-tuning to enhance cross-dataset PD detection performance.During
pre-training,MCLPD uses self-supervised learning on the unlabeled UNM
dataset.To build contrastive pairs,it applies dual augmentations in both time
and frequency domains,which enrich the data and naturally fuse time-frequency
information.In the fine-tuning phase,only a small proportion of labeled data
from another two datasets (UI and UC)is used for supervised
optimization.Experimental results show that MCLPD achieves F1 scores of 0.91 on
UI and 0.81 on UC using only 1%of labeled data,which further improve to 0.97
and 0.87,respectively,when 5%of labeled data is used.Compared to existing
methods,MCLPD substantially improves cross-dataset generalization while
reducing the dependency on labeled data,demonstrating the effectiveness of the
proposed framework.

</details>


### [151] [GEPD:GAN-Enhanced Generalizable Model for EEG-Based Detection of Parkinson's Disease](https://arxiv.org/abs/2508.14074)
*Qian Zhang,Ruilin Zhang,Biaokai Zhu,Xun Han,Jun Xiao,Yifan Liu,Zhe Wang*

Main category: cs.LG

TL;DR: 提出了一种名为GEPD的生成对抗网络（GAN）增强的泛化模型，用于基于EEG的帕金森病跨数据集分类。该模型通过生成融合EEG数据和采用多卷积神经网络分类器来提高泛化能力，并在跨数据集任务中取得了84.3%的准确率和84.0%的F1分数。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有EEG数据集规模小且检测方法在不同数据集间变异性大的问题，从而训练一个适用于跨数据集场景的泛化模型。

Method: 首先，设计了一个生成网络，通过控制生成数据与真实数据的分布相似性来融合EEG数据，并设计了一个EEG信号质量评估模型来保证生成数据的质量。其次，设计了一个分类网络，结合了多个卷积神经网络来捕捉EEG信号的时频特征，同时保持了泛化结构并确保易于收敛。

Result: 评估结果表明，所提出的GEPD模型在跨数据集设置下表现与最先进模型相当，准确率达到84.3%，F1分数达到84.0%。

Conclusion: 所提出的GEPD模型在跨数据集设置下表现与最先进模型相当，准确率达到84.3%，F1分数达到84.0%，证明了该模型的泛化能力。

Abstract: Electroencephalography has been established as an effective method for
detecting Parkinson's disease, typically diagnosed early.Current Parkinson's
disease detection methods have shown significant success within individual
datasets, however, the variability in detection methods across different EEG
datasets and the small size of each dataset pose challenges for training a
generalizable model for cross-dataset scenarios. To address these issues, this
paper proposes a GAN-enhanced generalizable model, named GEPD, specifically for
EEG-based cross-dataset classification of Parkinson's disease.First, we design
a generative network that creates fusion EEG data by controlling the
distribution similarity between generated data and real data.In addition, an
EEG signal quality assessment model is designed to ensure the quality of
generated data great.Second, we design a classification network that utilizes a
combination of multiple convolutional neural networks to effectively capture
the time-frequency characteristics of EEG signals, while maintaining a
generalizable structure and ensuring easy convergence.This work is dedicated to
utilizing intelligent methods to study pathological manifestations, aiming to
facilitate the diagnosis and monitoring of neurological diseases.The evaluation
results demonstrate that our model performs comparably to state-of-the-art
models in cross-dataset settings, achieving an accuracy of 84.3% and an
F1-score of 84.0%, showcasing the generalizability of the proposed model.

</details>


### [152] [Logical Expressivity and Explanations for Monotonic GNNs with Scoring Functions](https://arxiv.org/abs/2508.14091)
*Matthew Morris,David J. Tena Cucala,Bernardo Cuenca Grau*

Main category: cs.LG

TL;DR: This paper addresses the lack of explainability in GNNs for link prediction by adapting GNNs and scoring functions to be monotonic, allowing for the extraction of sound Datalog rules. Experiments show this approach performs well and yields explainable rules.


<details>
  <summary>Details</summary>
Motivation: To address the lack of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs with provable correspondence guarantees. However, these works address only a form of link prediction based on a restricted, low-expressivity graph encoding/decoding method. We consider a more general and popular approach for link prediction where a scoring function is used to decode the GNN output into fact predictions.

Method: We show how GNNs and scoring functions can be adapted to be monotonic and use the monotonicity to extract sound rules for explaining predictions, and leverage existing results about the kind of rules that scoring functions can capture.

Result: Our experiments show that, on link prediction benchmarks, monotonic GNNs and scoring functions perform well in practice and yield many sound rules.

Conclusion: We define procedures for obtaining equivalent Datalog programs for certain classes of monotonic GNNs with scoring functions, and our experiments show that monotonic GNNs and scoring functions perform well in practice and yield many sound rules.

Abstract: Graph neural networks (GNNs) are often used for the task of link prediction:
predicting missing binary facts in knowledge graphs (KGs). To address the lack
of explainability of GNNs on KGs, recent works extract Datalog rules from GNNs
with provable correspondence guarantees. The extracted rules can be used to
explain the GNN's predictions; furthermore, they can help characterise the
expressive power of various GNN models. However, these works address only a
form of link prediction based on a restricted, low-expressivity graph
encoding/decoding method. In this paper, we consider a more general and popular
approach for link prediction where a scoring function is used to decode the GNN
output into fact predictions. We show how GNNs and scoring functions can be
adapted to be monotonic, use the monotonicity to extract sound rules for
explaining predictions, and leverage existing results about the kind of rules
that scoring functions can capture. We also define procedures for obtaining
equivalent Datalog programs for certain classes of monotonic GNNs with scoring
functions. Our experiments show that, on link prediction benchmarks, monotonic
GNNs and scoring functions perform well in practice and yield many sound rules.

</details>


### [153] [Explainable Graph Spectral Clustering For Text Embeddings](https://arxiv.org/abs/2508.14075)
*Mieczysław A. Kłopotek,Sławomir T. Wierzchoń,Bartłomiej Starosta,Piotr Borkowski,Dariusz Czerski,Eryk Laskowski*

Main category: cs.LG

TL;DR: This paper extends explainability methods for graph spectral clustering of text documents to GloVe embeddings, moving beyond cosine similarity.


<details>
  <summary>Details</summary>
Motivation: The motivation is to generalize the previous work on explaining Graph Spectral Clustering results for textual documents, which was limited to cosine similarity in term vector space, to a broader range of document embeddings.

Method: The method involves generalizing the explainability of Graph Spectral Clustering by considering other document embeddings, specifically those based on the GloVe embedding idea.

Result: The paper considers other embeddings of documents, in particular, based on the GloVe embedding idea, generalizing the explainability of Graph Spectral Clustering.

Conclusion: The paper generalizes the explainability of Graph Spectral Clustering results for textual documents to other embeddings, particularly those based on GloVe.

Abstract: In a previous paper, we proposed an introduction to the explainability of
Graph Spectral Clustering results for textual documents, given that document
similarity is computed as cosine similarity in term vector space.
  In this paper, we generalize this idea by considering other embeddings of
documents, in particular, based on the GloVe embedding idea.

</details>


### [154] [PersRM-R1: Enhance Personalized Reward Modeling with Reinforcement Learning](https://arxiv.org/abs/2508.14076)
*Mengdi Li,Guanqiao Chen,Xufeng Zhao,Haochen Wen,Shu Yang,Di Wang*

Main category: cs.LG

TL;DR: PersRM-R1是首个能够从少量样本中学习个性化偏好的奖励建模框架，解决了现有奖励模型在处理用户特定偏好时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的奖励模型（RM）难以捕捉细致的、用户特定的偏好，尤其是在数据有限和跨领域多样的情况下，需要一种能够处理数据稀疏性和跨领域泛化性挑战的个性化奖励建模方法。

Method: PersRM-R1是一个基于推理的奖励建模框架，它结合了合成数据生成和监督微调、强化微调两阶段训练流程，能够从少量个人样本中识别和表示个人因素。

Result: 实验结果表明，PersRM-R1在准确性和泛化性方面均优于现有同类模型，并能与更大规模的模型相媲美。

Conclusion: PersRM-R1在准确性和泛化性方面优于现有模型，并能匹配更大模型的性能，为更有效的个性化LLM铺平了道路。

Abstract: Reward models (RMs), which are central to existing post-training methods, aim
to align LLM outputs with human values by providing feedback signals during
fine-tuning. However, existing RMs struggle to capture nuanced, user-specific
preferences, especially under limited data and across diverse domains. Thus, we
introduce PersRM-R1, the first reasoning-based reward modeling framework
specifically designed to identify and represent personal factors from only one
or a few personal exemplars. To address challenges including limited data
availability and the requirement for robust generalization, our approach
combines synthetic data generation with a two-stage training pipeline
consisting of supervised fine-tuning followed by reinforcement fine-tuning.
Experimental results demonstrate that PersRM-R1 outperforms existing models of
similar size and matches the performance of much larger models in both accuracy
and generalizability, paving the way for more effective personalized LLMs.

</details>


### [155] [Label Smoothing is a Pragmatic Information Bottleneck](https://arxiv.org/abs/2508.14077)
*Sota Kudo*

Main category: cs.LG

TL;DR: 标签平滑是一种实用的信息瓶颈方法，其性能在理论和实验中都得到了证实。


<details>
  <summary>Details</summary>
Motivation: 重新审视标签平滑，并将其与信息瓶颈联系起来，以提供新的理解和解释。

Method: 通过理论和实验证明标签平滑是信息瓶颈最优解的探索，并将标签平滑视为信息瓶颈的一种实用方法。

Result: 标签平滑可以被解释为信息瓶颈的一种实用方法，并且在实验中也表明它对不包含目标信息或在给定另一个变量时未提供额外信息因素不敏感。

Conclusion: 这项研究通过信息瓶颈的某种形式重新审视了标签平滑。在模型足够灵活且没有相同输入的冲突标签的假设下，我们通过理论和实验证明，通过标签平滑获得的模型输出来探索信息瓶颈的最优解。基于此，标签平滑可以被理解为信息瓶颈的一种实用方法，易于实现。

Abstract: This study revisits label smoothing via a form of information bottleneck.
Under the assumption of sufficient model flexibility and no conflicting labels
for the same input, we theoretically and experimentally demonstrate that the
model output obtained through label smoothing explores the optimal solution of
the information bottleneck. Based on this, label smoothing can be interpreted
as a practical approach to the information bottleneck, enabling simple
implementation. As an information bottleneck method, we experimentally show
that label smoothing also exhibits the property of being insensitive to factors
that do not contain information about the target, or to factors that provide no
additional information about it when conditioned on another variable.

</details>


### [156] [Out-of-Sample Hydrocarbon Production Forecasting: Time Series Machine Learning using Productivity Index-Driven Features and Inductive Conformal Prediction](https://arxiv.org/abs/2508.14078)
*Mohamed Hassan Abdalla Idris,Jakub Marek Cebula,Jebraeel Gholinezhad,Shamsul Masum,Hongjie Ma*

Main category: cs.LG

TL;DR: 本研究提出了一种结合PI特征选择和ICP不确定性量化的机器学习框架，用于提高油气产量预测的准确性和可靠性。LSTM模型在该框架下表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了提高油气产量预测的鲁棒性，特别是处理多元时间序列分析。

Method: 本研究采用了一种新的机器学习框架，集成了生产指数（PI）驱动的特征选择和归纳式共形预测（ICP）不确定性量化。研究了LSTM、BiLSTM、GRU和XGBoost等预测算法，并使用MAE、预测偏差和预测方向准确度（PDA）等指标评估了模型性能。

Result: PI特征选择有效降低了输入维度。ICP框架提供了有效的预测区间。LSTM模型在NY. PF14油井的预测中表现最佳，MAE最低（测试数据为19.468，实际样本外预测数据为29.638），并在NY. E1H油井上得到了验证。

Conclusion: 将领域特定知识与先进的机器学习技术相结合，可以提高油气产量预测的可靠性。LSTM模型在测试和实际样本外预测数据方面表现出卓越的性能，在NY. E1H油井的预测中也得到了验证。

Abstract: This research introduces a new ML framework designed to enhance the
robustness of out-of-sample hydrocarbon production forecasting, specifically
addressing multivariate time series analysis. The proposed methodology
integrates Productivity Index (PI)-driven feature selection, a concept derived
from reservoir engineering, with Inductive Conformal Prediction (ICP) for
rigorous uncertainty quantification. Utilizing historical data from the Volve
(wells PF14, PF12) and Norne (well E1H) oil fields, this study investigates the
efficacy of various predictive algorithms-namely Long Short-Term Memory (LSTM),
Bidirectional LSTM (BiLSTM), Gated Recurrent Unit (GRU), and eXtreme Gradient
Boosting (XGBoost) - in forecasting historical oil production rates (OPR_H).
All the models achieved "out-of-sample" production forecasts for an upcoming
future timeframe. Model performance was comprehensively evaluated using
traditional error metrics (e.g., MAE) supplemented by Forecast Bias and
Prediction Direction Accuracy (PDA) to assess bias and trend-capturing
capabilities. The PI-based feature selection effectively reduced input
dimensionality compared to conventional numerical simulation workflows. The
uncertainty quantification was addressed using the ICP framework, a
distribution-free approach that guarantees valid prediction intervals (e.g.,
95% coverage) without reliance on distributional assumptions, offering a
distinct advantage over traditional confidence intervals, particularly for
complex, non-normal data. Results demonstrated the superior performance of the
LSTM model, achieving the lowest MAE on test (19.468) and genuine out-of-sample
forecast data (29.638) for well PF14, with subsequent validation on Norne well
E1H. These findings highlight the significant potential of combining
domain-specific knowledge with advanced ML techniques to improve the
reliability of hydrocarbon production forecasts.

</details>


### [157] [DualNILM: Energy Injection Identification Enabled Disaggregation with Deep Multi-Task Learning](https://arxiv.org/abs/2508.14600)
*Xudong Wang,Guoming Tang,Junyu Xue,Srinivasan Keshav,Tongxin Li,Chris Ding*

Main category: cs.LG

TL;DR: DualNILM是一种基于Transformer的多任务深度学习框架，可同时识别电器状态和计量点后能源注入，解决了传统NILM方法在处理这些数据时的性能下降问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决由于太阳能电池板和电池存储等计量点后能源的采用增加，而传统NILM方法仅依赖于计量点数据所带来的挑战。这些计量点后能源的注入会模糊单个电器的功率特征，导致NILM性能显著下降。

Method: 提出了一种名为DualNILM的深度多任务学习框架，采用基于Transformer的架构，结合序列到点和序列到序列策略，以捕捉聚合功耗模式中的多尺度时间依赖性，从而实现电器状态识别和能源注入识别的双重任务。

Result: DualNILM框架在自我收集和合成的开放NILM数据集上进行了验证，这些数据集包含电器级能耗和能源注入信息。实验结果表明，DualNILM在双重任务上表现出色，显著优于传统方法。

Conclusion: DualNILM框架在包含注入能源的NILM任务中表现优于传统方法，在识别电器状态和注入能源方面均保持出色性能。

Abstract: Non-Intrusive Load Monitoring (NILM) offers a cost-effective method to obtain
fine-grained appliance-level energy consumption in smart homes and building
applications. However, the increasing adoption of behind-the-meter energy
sources, such as solar panels and battery storage, poses new challenges for
conventional NILM methods that rely solely on at-the-meter data. The injected
energy from the behind-the-meter sources can obscure the power signatures of
individual appliances, leading to a significant decline in NILM performance. To
address this challenge, we present DualNILM, a deep multi-task learning
framework designed for the dual tasks of appliance state recognition and
injected energy identification in NILM. By integrating sequence-to-point and
sequence-to-sequence strategies within a Transformer-based architecture,
DualNILM can effectively capture multi-scale temporal dependencies in the
aggregate power consumption patterns, allowing for accurate appliance state
recognition and energy injection identification. We conduct validation of
DualNILM using both self-collected and synthesized open NILM datasets that
include both appliance-level energy consumption and energy injection. Extensive
experimental results demonstrate that DualNILM maintains an excellent
performance for the dual tasks in NILM, much outperforming conventional
methods.

</details>


### [158] [A Guide to Robust Generalization: The Impact of Architecture, Pre-training, and Optimization Strategy](https://arxiv.org/abs/2508.14079)
*Maxime Heuillet,Rishika Bhagwatkar,Jonas Ngnawé,Yann Pequignot,Alexandre Larouche,Christian Gagné,Irina Rish,Ola Ahmad,Audrey Durand*

Main category: cs.LG

TL;DR: 对鲁棒微调进行了迄今为止最多样化、最全面的基准测试，发现在大型数据集上进行监督预训练的卷积神经网络通常表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了理解设计选择（模型更新协议、专门损失目标、架构类型和大小、预训练表示）如何影响模型在面对测试时新的、未知的扰动时保持性能的鲁棒泛化能力。

Method: 通过在6个数据集、40种预训练架构、2种专门损失和3种适应协议上进行实证研究，共生成1440种训练配置和跨五种扰动类型的7200次鲁棒性测量。

Result: 虽然基于注意力的架构和鲁棒的预训练表示越来越受欢迎，但研究发现，在大型数据集上以监督方式预训练的卷积神经网络通常表现最佳。

Conclusion: 研究确认并挑战了先前的设计假设，指明了有希望的研究方向，并提供了实用的指导。

Abstract: Deep learning models operating in the image domain are vulnerable to small
input perturbations. For years, robustness to such perturbations was pursued by
training models from scratch (i.e., with random initializations) using
specialized loss objectives. Recently, robust fine-tuning has emerged as a more
efficient alternative: instead of training from scratch, pretrained models are
adapted to maximize predictive performance and robustness. To conduct robust
fine-tuning, practitioners design an optimization strategy that includes the
model update protocol (e.g., full or partial) and the specialized loss
objective. Additional design choices include the architecture type and size,
and the pretrained representation. These design choices affect robust
generalization, which is the model's ability to maintain performance when
exposed to new and unseen perturbations at test time. Understanding how these
design choices influence generalization remains an open question with
significant practical implications. In response, we present an empirical study
spanning 6 datasets, 40 pretrained architectures, 2 specialized losses, and 3
adaptation protocols, yielding 1,440 training configurations and 7,200
robustness measurements across five perturbation types. To our knowledge, this
is the most diverse and comprehensive benchmark of robust fine-tuning to date.
While attention-based architectures and robust pretrained representations are
increasingly popular, we find that convolutional neural networks pretrained in
a supervised manner on large datasets often perform best. Our analysis both
confirms and challenges prior design assumptions, highlighting promising
research directions and offering practical guidance.

</details>


### [159] [KnowDR-REC: A Benchmark for Referring Expression Comprehension with Real-World Knowledge](https://arxiv.org/abs/2508.14080)
*Guanghao Jin,Jingpei Wu,Tianpei Guo,Yiyi Niu,Weidong Zhou,Guoyang Liu*

Main category: cs.LG

TL;DR: 本研究提出了KnowDR-REC基准，以解决现有Referring Expression Comprehension (REC) 任务基准在评估多模态大型语言模型（MLLMs）的推理能力方面的不足。该基准基于真实世界知识，包含精心设计的负样本，并引入新的评估指标。实验结果表明，现有MLLMs在知识驱动的视觉基础任务上表现不佳，且存在文本理解与视觉基础脱节的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的Referring Expression Comprehension (REC) 任务基准在评估多模态大型语言模型（MLLMs）的推理能力方面存在不足，因为它们要么仅依赖于图像内部线索，要么缺乏足够的细粒度实例注释。

Method: 提出一个名为KnowDR-REC的新基准，该基准具有三个主要特点：1. 基于真实世界的知识，需要跨越文本和图像的细粒度多模态推理；2. 包含通过细粒度表达式编辑精心构建的负样本，用于评估模型的鲁棒性和抗干扰能力；3. 引入三个新的评估指标，用于系统地探索模型的内部推理过程。在KnowDR-REC上评估了16个最先进的多模态模型。

Result: 在KnowDR-REC基准上，现有MLLMs在知识驱动的视觉基础任务上表现不佳。研究观察到MLLMs在文本理解和视觉基础之间存在脱节，许多模型受到记忆捷径相关性的显著影响，这严重影响了它们在该基准上的行为，并阻碍了真正的多模态推理。

Conclusion: 现有的大型多模态语言模型（MLLMs）在处理知识驱动的视觉基础任务时仍面临挑战，它们在文本理解和视觉基础之间存在脱节，并且容易受到记忆捷径相关性的影响，这严重阻碍了真正的多模态推理能力。

Abstract: Referring Expression Comprehension (REC) is a popular multimodal task that
aims to accurately detect target objects within a single image based on a given
textual expression. However, due to the limitations of earlier models,
traditional REC benchmarks either rely solely on intra-image cues or lack
sufficiently fine-grained instance annotations, making them inadequate for
evaluating the reasoning capabilities of Multi-modal Large Language Models
(MLLMs). To address this gap, we propose a new benchmark, KnowDR-REC,
characterized by three key features: Firstly, it is built upon real-world
knowledge, requiring fine-grained multimodal reasoning across text and image.
Secondly, the dataset includes elaborately constructed negative samples via
fine-grained expression editing, designed to evaluate a model's robustness and
anti-hallucination ability. Lastly, we introduce three novel evaluation metrics
to systematically explore the model's internal reasoning process. We evaluate
16 state-of-the-art multimodal models on KnowDR-REC, with experimental results
showing that existing MLLMs still struggle with knowledge-driven visual
grounding tasks. Furthermore, we observe a decoupling between textual
understanding and visual grounding in MLLMs, where many models are
significantly influenced by memorized shortcut correlations, which severely
affect their behavior on our benchmark and hinder genuine multimodal reasoning.
We anticipate that the proposed benchmark will inspire future research towards
developing more robust, interpretable, and knowledge-intensive visual grounding
frameworks, driving the development of more reliable and robust multimodal
systems for complex real-world scenarios.

</details>


### [160] [Toward Lifelong Learning in Equilibrium Propagation: Sleep-like and Awake Rehearsal for Enhanced Stability](https://arxiv.org/abs/2508.14081)
*Yoshimasa Kubo,Jean Erik Delanois,Maxim Bazhenov*

Main category: cs.LG

TL;DR: 该研究提出了一种名为SRC的睡眠类似重放巩固算法，用于解决循环神经网络（RNN）在连续学习中遇到的灾难性遗忘问题。实验证明，SRC能有效提高RNN的知识保留能力，并且在多种数据集上的表现优于传统方法，甚至能与基于BPTT的模型相媲美。结合排练（清醒重放）更能进一步提升长期知识保留效果。这表明将类似人类的睡眠学习机制应用于RNN是可行的，并为构建更强大的人工神经网络提供了新的思路。


<details>
  <summary>Details</summary>
Motivation: 为了解决循环神经网络（RNN）在连续学习中面临的灾难性遗忘问题，即在学习新任务时，先前获得的知识会被覆盖。

Method: 提出了一种用于EP训练的RNN的睡眠类似重放巩固（SRC）算法，并在MNIST、Fashion MNIST、Kuzushiji-MNIST、CIFAR10和ImageNet数据集上进行了实验。

Result: SRC显著提高了RNN在连续学习场景中抵抗灾难性遗忘的能力。在SRC应用于每个新任务训练后的类别增量学习中，EP训练的多层RNN模型（MRNN-EP）表现显著优于结合了多种成熟正则化技术的前馈网络。在MNIST数据上，MRNN-EP与使用反向传播通过时间（BPTT）训练并结合SRC的MRNN表现相当；而在Fashion MNIST、Kuzushiji-MNIST、CIFAR10和ImageNet数据集上，其表现优于基于BPTT的模型。结合SRC和被称为“清醒重放”的排练，进一步增强了网络在继续学习新任务的同时保留长期知识的能力。

Conclusion: 该研究揭示了睡眠类似重放技术在循环神经网络（RNN）中的适用性，并强调了将类似人类的学习行为整合到人工神经网络（ANN）中的潜力。

Abstract: Recurrent neural networks (RNNs) trained using Equilibrium Propagation (EP),
a biologically plausible training algorithm, have demonstrated strong
performance in various tasks such as image classification and reinforcement
learning. However, these networks face a critical challenge in continuous
learning: catastrophic forgetting, where previously acquired knowledge is
overwritten when new tasks are learned. This limitation contrasts with the
human brain's ability to retain and integrate both old and new knowledge, aided
by processes like memory consolidation during sleep through the replay of
learned information. To address this challenge in RNNs, here we propose a
sleep-like replay consolidation (SRC) algorithm for EP-trained RNNs. We found
that SRC significantly improves RNN's resilience to catastrophic forgetting in
continuous learning scenarios. In class-incremental learning with SRC
implemented after each new task training, the EP-trained multilayer RNN model
(MRNN-EP) performed significantly better compared to feedforward networks
incorporating several well-established regularization techniques. The MRNN-EP
performed on par with MRNN trained using Backpropagation Through Time (BPTT)
when both were equipped with SRC on MNIST data and surpassed BPTT-based models
on the Fashion MNIST, Kuzushiji-MNIST, CIFAR10, and ImageNet datasets.
Combining SRC with rehearsal, also known as "awake replay", further boosted the
network's ability to retain long-term knowledge while continuing to learn new
tasks. Our study reveals the applicability of sleep-like replay techniques to
RNNs and highlights the potential for integrating human-like learning behaviors
into artificial neural networks (ANNs).

</details>


### [161] [FedEve: On Bridging the Client Drift and Period Drift for Cross-device Federated Learning](https://arxiv.org/abs/2508.14539)
*Tao Shen,Zexi Li,Didi Zhu,Ziyu Zhao,Chao Wu,Fei Wu*

Main category: cs.LG

TL;DR: 本文研究了联邦学习中由部分客户端参与引起的周期漂移及其与客户漂移的相互作用，并提出了一种名为FedEve的方法来缓解这些漂移对模型性能的影响，实验证明其有效性。


<details>
  <summary>Details</summary>
Motivation: 跨设备联邦学习中的数据异质性是一个关键挑战，周期漂移（由部分客户端参与引起）和客户漂移（由多次本地更新引起）都会导致模型性能下降。本文研究了这两种漂移的相互作用及其对模型的影响。

Method: 提出了一种预测-观察框架，并实例化为FedEve方法，以补偿周期漂移和客户漂移。

Result: 理论上证明了所提出的方法可以减少模型更新的方差。实验结果表明，FedEve在非独立同分布的跨设备设置中表现优于其他方法。

Conclusion: 提出了一种名为FedEve的预测-观察框架，通过补偿周期漂移和客户漂移来减轻它们对跨设备联邦学习的影响，并在非独立同分布数据上进行了广泛的实验验证，结果优于其他方法。

Abstract: Federated learning (FL) is a machine learning paradigm that allows multiple
clients to collaboratively train a shared model without exposing their private
data. Data heterogeneity is a fundamental challenge in FL, which can result in
poor convergence and performance degradation. Client drift has been recognized
as one of the factors contributing to this issue resulting from the multiple
local updates in FedAvg. However, in cross-device FL, a different form of drift
arises due to the partial client participation, but it has not been studied
well. This drift, we referred as period drift, occurs as participating clients
at each communication round may exhibit distinct data distribution that
deviates from that of all clients. It could be more harmful than client drift
since the optimization objective shifts with every round.
  In this paper, we investigate the interaction between period drift and client
drift, finding that period drift can have a particularly detrimental effect on
cross-device FL as the degree of data heterogeneity increases. To tackle these
issues, we propose a predict-observe framework and present an instantiated
method, FedEve, where these two types of drift can compensate each other to
mitigate their overall impact. We provide theoretical evidence that our
approach can reduce the variance of model updates. Extensive experiments
demonstrate that our method outperforms alternatives on non-iid data in
cross-device settings.

</details>


### [162] [Toward Generalist Semi-supervised Regression via Decoupled Representation Distillation](https://arxiv.org/abs/2508.14082)
*Ye Su,Hezhe Qiao,Wei Huang,Lin Chen*

Main category: cs.LG

TL;DR: DRILL是一个新的半监督回归框架，通过将回归任务转化为离散分布估计任务，并使用解耦分布对齐来提高模型的泛化能力和鲁棒性，实验证明其效果优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督回归方法在生成伪标签时，过度依赖伪标签的质量，并且直接回归容易导致过拟合，无法学习到标签的分布。为解决这些挑战，提出DRILL框架。

Method: 该研究提出了一种端到端的解耦表示蒸馏框架（DRILL），将半监督回归任务转化为离散分布估计（DDE）任务，并采用解耦分布对齐（DDA）来对齐教师和学生之间的目标桶和非目标桶的分布。

Result: DRILL框架能够更好地捕捉潜在的标签分布，并减轻直接回归的过拟合风险，在多个领域的实验中表现优于现有方法。

Conclusion: DRILL在来自不同领域的的数据集上进行了广泛的实验，证明了其具有很强的泛化能力，并且优于现有方法。

Abstract: Semi-supervised regression (SSR), which aims to predict continuous scores of
samples while reducing reliance on a large amount of labeled data, has recently
received considerable attention across various applications, including computer
vision, natural language processing, and audio and medical analysis. Existing
semi-supervised methods typically apply consistency regularization on the
general regression task by generating pseudo-labels. However, these methods
heavily rely on the quality of pseudo-labels, and direct regression fails to
learn the label distribution and can easily lead to overfitting. To address
these challenges, we introduce an end-to-end Decoupled Representation
distillation framework (DRILL) which is specially designed for the
semi-supervised regression task where we transform the general regression task
into a Discrete Distribution Estimation (DDE) task over multiple buckets to
better capture the underlying label distribution and mitigate the risk of
overfitting associated with direct regression. Then we employ the Decoupled
Distribution Alignment (DDA) to align the target bucket and non-target bucket
between teacher and student on the distribution of buckets, encouraging the
student to learn more robust and generalized knowledge from the teacher.
Extensive experiments conducted on datasets from diverse domains demonstrate
that the proposed DRILL has strong generalization and outperforms the competing
methods.

</details>


### [163] [Cooperative SGD with Dynamic Mixing Matrices](https://arxiv.org/abs/2508.14565)
*Soumya Sarkar,Shweta Jain*

Main category: cs.LG

TL;DR: 提出了一种新的分布式 SGD 训练框架，该框架通过动态调整通信拓扑和节点贡献来提高模型性能，并具有更好的理论收敛保证。


<details>
  <summary>Details</summary>
Motivation: 传统的分布式 SGD 训练算法通常假设固定的通信拓扑和均匀的节点贡献，而实验表明，动态拓扑和非均匀聚合策略可以显著提高模型性能。

Method: 提出一个统一的框架，涵盖了几种具有动态拓扑的本地更新 SGD 分布式算法，并提供了收敛性理论保证。

Result: 该框架提供了改进的或与现有工作相匹配的收敛性理论保证。

Conclusion: 该研究提出了一个统一的框架，涵盖了几种具有动态拓扑的本地更新 SGD 分布式算法，并提供了比现有工作更好或相当的收敛性理论保证。

Abstract: One of the most common methods to train machine learning algorithms today is
the stochastic gradient descent (SGD). In a distributed setting, SGD-based
algorithms have been shown to converge theoretically under specific
circumstances. A substantial number of works in the distributed SGD setting
assume a fixed topology for the edge devices. These papers also assume that the
contribution of nodes to the global model is uniform. However, experiments have
shown that such assumptions are suboptimal and a non uniform aggregation
strategy coupled with a dynamically shifting topology and client selection can
significantly improve the performance of such models. This paper details a
unified framework that covers several Local-Update SGD-based distributed
algorithms with dynamic topologies and provides improved or matching
theoretical guarantees on convergence compared to existing work.

</details>


### [164] [GeoMAE: Masking Representation Learning for Spatio-Temporal Graph Forecasting with Missing Values](https://arxiv.org/abs/2508.14083)
*Songyu Ke,Chenyu Wu,Yuxuan Liang,Xiuwen Yi,Yanping Sun,Junbo Zhang,Yu Zheng*

Main category: cs.LG

TL;DR: CSST 框架通过将人群流推断问题作为自监督属性图表示学习任务，并利用对比学习来处理稀缺标签、复杂时空依赖和 GPS 数据关联等挑战，从而提高了人群流推断的准确性，并在真实世界数据集上取得了优于基线方法的性能。


<details>
  <summary>Details</summary>
Motivation: 准确获取景点的ولهflow对于有效的交通管理、公共服务和城市规划至关重要。然而，由于城市传感技术的限制，大多数来源的数据质量不足以监测每个景点的ولهflow。因此，从低质量数据中推断出准确的ولهflow是一项关键且具有挑战性的任务，其复杂性因三个关键因素而加剧：1) 标记数据的稀缺性，2) POI 之间复杂的时空依赖性，以及 3) 精确的 لهflow 与 GPS 报告之间的海量关联。

Method: 提出了一种新颖的对比自我学习框架 (CSST)，将人群流推断问题重新构建为自监督属性图表示学习任务。该框架首先构建基于 POI 及其距离的空间邻近图，然后利用对比学习技术利用大量的未标记时空数据，并通过交换预测方法来预测目标子图的表示。

Result: 在两个真实世界数据集上进行的实验表明，CSST 模型在大量噪声数据上进行预训练后，其性能始终优于从头开始训练的模型。

Conclusion: 该模型在两个真实世界数据集上的实验表明，在大量噪声数据上预训练的 CSST 模型始终优于从头开始训练的模型。

Abstract: Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
  To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.

</details>


### [165] [Beyond Fixed Morphologies: Learning Graph Policies with Trust Region Compensation in Variable Action Spaces](https://arxiv.org/abs/2508.14102)
*Thomas Gallien*

Main category: cs.LG

TL;DR: Trust region optimization methods like TRPO/PPO are foundational for RL in continuous control but their behavior with changing action space dimensions (for morphological generalization) is unclear. This paper theoretically analyzes TRPO/PPO under these conditions and empirically tests them on the Swimmer environment.


<details>
  <summary>Details</summary>
Motivation: There is a growing demand for scalable and reusable control policies that exhibit morphological generalization, but the behavior of trust region methods under varying action space dimensionality is not well understood.

Method: The paper theoretically analyzes TRPO and PPO under varying action space dimensionality, focusing on KL-divergence and clipping penalties. It also includes an empirical evaluation using the Gymnasium Swimmer environment to study morphological generalization.

Result: The theoretical analysis aims to demonstrate how varying action space dimensionality influences the optimization landscape under KL-divergence or clipping penalties. The empirical evaluation uses the Gymnasium Swimmer environment to assess morphological generalization.

Conclusion: The study analyzes the impact of varying action space dimensionality on trust region optimization methods like TRPO and PPO, particularly concerning KL-divergence and clipping penalties, and empirically validates findings using the Gymnasium Swimmer environment to assess morphological generalization.

Abstract: Trust region-based optimization methods have become foundational
reinforcement learning algorithms that offer stability and strong empirical
performance in continuous control tasks. Growing interest in scalable and
reusable control policies translate also in a demand for morphological
generalization, the ability of control policies to cope with different
kinematic structures. Graph-based policy architectures provide a natural and
effective mechanism to encode such structural differences. However, while these
architectures accommodate variable morphologies, the behavior of trust region
methods under varying action space dimensionality remains poorly understood. To
this end, we conduct a theoretical analysis of trust region-based policy
optimization methods, focusing on both Trust Region Policy Optimization (TRPO)
and its widely used first-order approximation, Proximal Policy Optimization
(PPO). The goal is to demonstrate how varying action space dimensionality
influence the optimization landscape, particularly under the constraints
imposed by KL-divergence or policy clipping penalties. Complementing the
theoretical insights, an empirical evaluation under morphological variation is
carried out using the Gymnasium Swimmer environment. This benchmark offers a
systematically controlled setting for varying the kinematic structure without
altering the underlying task, making it particularly well-suited to study
morphological generalization.

</details>


### [166] [Federated Distillation on Edge Devices: Efficient Client-Side Filtering for Non-IID Data](https://arxiv.org/abs/2508.14769)
*Ahmed Mujtaba,Gleb Radchenko,Radu Prodan,Marc Masana*

Main category: cs.LG

TL;DR: EdgeFD通过KMeans简化了联邦蒸馏中的数据过滤，提高了效率和精度，尤其适合边缘设备。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦蒸馏方法需要复杂的选择性知识共享策略，客户端需要通过计算成本高昂的统计密度比估计器来识别in-distribution代理数据，并且服务器端过滤模糊知识会引入延迟。

Method: 提出了一种名为EdgeFD的资源高效方法，使用KMeans作为密度比估计器，在客户端过滤数据，无需服务器端过滤或预训练教师模型。

Result: EdgeFD在强非IID、弱非IID和IID数据分布下均优于现有方法，精度接近IID场景，计算开销显著降低，适合边缘设备部署。

Conclusion: EdgeFD方法通过使用高效的KMeans作为密度比估计器，简化了客户端的密度比估计，并消除了服务器端过滤的需要，在强非IID、弱非IID和IID数据分布下均表现出色，精度接近IID场景，计算开销显著降低，适合资源受限的边缘设备部署，提高了联邦蒸馏的可扩展性和实际应用性。

Abstract: Federated distillation has emerged as a promising collaborative machine
learning approach, offering enhanced privacy protection and reduced
communication compared to traditional federated learning by exchanging model
outputs (soft logits) rather than full model parameters. However, existing
methods employ complex selective knowledge-sharing strategies that require
clients to identify in-distribution proxy data through computationally
expensive statistical density ratio estimators. Additionally, server-side
filtering of ambiguous knowledge introduces latency to the process. To address
these challenges, we propose a robust, resource-efficient EdgeFD method that
reduces the complexity of the client-side density ratio estimation and removes
the need for server-side filtering. EdgeFD introduces an efficient KMeans-based
density ratio estimator for effectively filtering both in-distribution and
out-of-distribution proxy data on clients, significantly improving the quality
of knowledge sharing. We evaluate EdgeFD across diverse practical scenarios,
including strong non-IID, weak non-IID, and IID data distributions on clients,
without requiring a pre-trained teacher model on the server for knowledge
distillation. Experimental results demonstrate that EdgeFD outperforms
state-of-the-art methods, consistently achieving accuracy levels close to IID
scenarios even under heterogeneous and challenging conditions. The
significantly reduced computational overhead of the KMeans-based estimator is
suitable for deployment on resource-constrained edge devices, thereby enhancing
the scalability and real-world applicability of federated distillation. The
code is available online for reproducibility.

</details>


### [167] [Parameter-Aware Ensemble SINDy for Interpretable Symbolic SGS Closure](https://arxiv.org/abs/2508.14085)
*Hanseul Kang,Shervin Karimkashi,Ville Vuorinen*

Main category: cs.LG

TL;DR: 提出了一种新的稀疏回归框架，用于从数据中发现偏微分方程和亚网格尺度闭包，该框架能够处理多参数数据，并且在Burgers方程等基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了发现可解释的偏微分方程和亚网格尺度闭包，特别是在存在多参数模拟数据的情况下。

Method: 提出了一种可扩展的、参数感知的稀疏回归框架，用于从多参数模拟数据中发现可解释的偏微分方程和亚网格尺度闭包。该方法建立在SINDy的基础上，通过四项创新解决了关键限制：1. 符号参数化，允许物理参数在统一的回归中变化；2. 降维相似性滤波器，在减少候选库的同时强制单位一致性；3. 内存高效的格ram矩阵累积，实现批量处理；4. 集成共识和系数稳定性分析，用于稳健的模型识别。

Result: 在标准的一维基准测试上进行的验证表明，该框架能够可靠地恢复跨越参数范围的控制方程。将该框架应用于滤波后的Burgers数据集，成功发现了SGS闭包$	au_{	extrm{SGS}} = 0.1603
uhoar{S}_{ij}ar{S}^{ij}$，其Smagorinsky常数约为0.4004。该模型在不同滤波尺度上的$R^2$值为0.886，并且预测精度优于经典的闭包模型。

Conclusion: 该框架能够识别有物理意义的SGS形式并校准系数，为现有的湍流模型方法提供了补充，并为数据驱动的闭包发现这一新兴领域做出了贡献。

Abstract: We present a scalable, parameter-aware sparse regression framework for
discovering interpretable partial differential equations and subgrid-scale
closures from multi-parameter simulation data. Building on SINDy (Sparse
Identification of Nonlinear Dynamics), our approach addresses key limitations
through four innovations: symbolic parameterisation enabling physical
parameters to vary within unified regression; Dimensional Similarity Filter
enforcing unit-consistency whilst reducing candidate libraries;
memory-efficient Gram-matrix accumulation enabling batch processing; and
ensemble consensus with coefficient stability analysis for robust model
identification.
  Validation on canonical one-dimensional benchmarks demonstrates reliable
recovery of governing equations across parameter ranges. Applied to filtered
Burgers datasets, the framework discovers an SGS closure $\tau_{\mathrm{SGS}} =
0.1603\cdot\Delta^2\left(\frac{\partial \bar{u}}{\partial x}\right)^2$,
corresponding to a Smagorinsky constant of approximately 0.4004. This
represents autonomous discovery of Smagorinsky-type closure structure from data
without prior theoretical assumptions.
  The discovered model achieves $R^2 = 0.886$ across filter scales and
demonstrates improved prediction accuracy compared to classical closures. The
framework's ability to identify physically meaningful SGS forms and calibrate
coefficients offers a complementary approach to existing turbulence modelling
methods, contributing to the growing field of data-driven closure discovery.

</details>


### [168] [Comparison of derivative-free and gradient-based minimization for multi-objective compositional design of shape memory alloys](https://arxiv.org/abs/2508.14127)
*S. Josyula,Y. Noiman,E. J. Payton,T. Giovannelli*

Main category: cs.LG

TL;DR: 该研究通过结合物理信息数据、机器学习模型和优化算法来优化形状记忆合金的成分，以在满足特定性能目标的同时降低成本。研究发现，结合神经网络和梯度优化器的组合比基于树的模型和无梯度优化器更能有效地找到满足目标的合金成分。


<details>
  <summary>Details</summary>
Motivation: 设计满足性能目标且价格合理且可持续的形状记忆合金（SMA）是一项复杂的挑战。本工作侧重于优化SMA成分以实现所需的马氏体起始温度（Ms），同时最大限度地降低成本。

Method: 本工作使用机器学习模型作为代理预测器，并应用数值优化方法来搜索合适的合金组合。训练了两种机器学习模型：基于树的集成模型和神经网络。基于树的模型与无导数优化器（COBYLA）一起使用，而提供梯度信息的神经网络则与基于梯度的优化器（TRUST-CONSTR）配对。

Result: 结果表明，虽然两种模型在Ms预测方面具有相似的准确性，但与神经网络配对的优化器能够更一致地找到更好的解决方案。COBYLA 经常收敛于次优结果，特别是当初始猜测远离目标时。TRUST-CONSTR 方法表现出更稳定的行为，并且在达到同时满足两个目标的合金成分方面表现更好。

Conclusion: 该研究展示了一种结合物理信息数据、机器学习模型和优化算法来探索新型SMA组合物的实用方法。虽然数据集的规模小于基于模拟的研究，但实验数据的 Yet is used improves the reliability of the predictions. The approach can be extended to other materials where design trade-offs must be made with limited data.

Abstract: Designing shape memory alloys (SMAs) that meet performance targets while
remaining affordable and sustainable is a complex challenge. In this work, we
focus on optimizing SMA compositions to achieve a desired martensitic start
temperature (Ms) while minimizing cost. To do this, we use machine learning
models as surrogate predictors and apply numerical optimization methods to
search for suitable alloy combinations. We trained two types of machine
learning models, a tree-based ensemble and a neural network, using a dataset of
experimentally characterized alloys and physics-informed features. The
tree-based model was used with a derivative-free optimizer (COBYLA), while the
neural network, which provides gradient information, was paired with a
gradient-based optimizer (TRUST-CONSTR). Our results show that while both
models predict Ms with similar accuracy, the optimizer paired with the neural
network finds better solutions more consistently. COBYLA often converged to
suboptimal results, especially when the starting guess was far from the target.
The TRUST-CONSTR method showed more stable behavior and was better at reaching
alloy compositions that met both objectives. This study demonstrates a
practical approach to exploring new SMA compositions by combining
physics-informed data, machine learning models, and optimization algorithms.
Although the scale of our dataset is smaller than simulation-based efforts, the
use of experimental data improves the reliability of the predictions. The
approach can be extended to other materials where design trade-offs must be
made with limited data.

</details>


### [169] [EEGDM: EEG Representation Learning via Generative Diffusion Model](https://arxiv.org/abs/2508.14086)
*Jia Hong Puah,Sim Kuan Goh,Ziwei Zhang,Zixuan Ye,Chow Khuen Chan,Kheng Seang Lim,Si Lei Fong,Kok Sin Woon*

Main category: cs.LG

TL;DR: EEGDM 是一种新的 EEG 表示学习框架，它使用生成扩散模型和结构化状态空间模型，在提高性能的同时显著降低了计算成本，为 EEG 分析提供了一种更轻量级的替代方案。


<details>
  <summary>Details</summary>
Motivation: 由于标注有限和信号变异性高，从原始 EEG 信号中学习有意义的表示仍然是一个挑战。虽然基于 Transformer 和自监督学习的 EEG 基础模型显示出潜力，但它们通常会产生高昂的计算成本，并且在模型尺寸增大时性能提升有限。

Method: 提出了一种名为 EEGDM 的 EEG 表示学习框架，该框架结合了生成扩散模型。它利用了用于扩散预训练的结构化状态空间模型 (SSMDP) 来捕捉 EEG 信号的时间动态，并使用去噪扩散概率模型进行训练。然后，使用潜在融合 Transformer (LFT) 对学习到的潜在 EEG 表示进行下游分类任务。

Result: EEGDM 在 Temple University EEG Event Corpus 数据集上进行了评估，在下游分类任务中优于现有的最先进方法（包括 EEG 基础模型），同时计算量减少了约 19 倍。

Conclusion: EEGDM 提供了一个有前景的替代方案，可与当前最先进的方法相媲美，同时显著减轻了计算负担。

Abstract: While electroencephalogram (EEG) has been a crucial tool for monitoring the
brain and diagnosing neurological disorders (e.g., epilepsy), learning
meaningful representations from raw EEG signals remains challenging due to
limited annotations and high signal variability. Recently, EEG foundation
models (FMs) have shown promising potential by adopting transformer
architectures and self-supervised pre-training methods from large language
models (e.g., masked prediction) to learn representations from diverse EEG
data, followed by fine-tuning on specific EEG tasks. Nonetheless, these large
models often incurred high computational costs during both training and
inference, with only marginal performance improvements as model size increases.
In this work, we proposed EEG representation learning framework building upon
Generative Diffusion Model (EEGDM). Specifically, we developed structured
state-space model for diffusion pretraining (SSMDP) to better capture the
temporal dynamics of EEG signals and trained the architecture using a Denoising
Diffusion Probabilistic Model. The resulting latent EEG representations were
then used for downstream classification tasks via our proposed latent fusion
transformer (LFT). To evaluate our method, we used the multi-event Temple
University EEG Event Corpus and compared EEGDM with current state-of-the-art
approaches, including EEG FMs. Empirical results showed that our method
outperformed existing methods while being approximately 19x more lightweight.
These findings suggested that EEGDM offered a promising alternative to current
FMs. Our code is available at: https://github.com/jhpuah/EEGDM.

</details>


### [170] [Reliability comparison of vessel trajectory prediction models via Probability of Detection](https://arxiv.org/abs/2508.14198)
*Zahra Rastin,Kathrin Donandt,Dirk Söffker*

Main category: cs.LG

TL;DR: 本研究评估了不同的深度学习方法在船舶轨迹预测中的性能和可靠性，并提出了一种新的量化可靠性的方法。


<details>
  <summary>Details</summary>
Motivation: 先前的VTP模型忽略了特定的交通情况复杂性，并且缺乏可靠性评估，因此本研究旨在评估不同深度学习方法在VTP中的性能，并比较这些方法的可靠性。

Method: 对不同的基于深度学习的VTP方法进行了评估，并使用概率检测分析来量化模型在不同交通复杂性下的可靠性。

Result: 所有模型都在根据预测范围内的交通情况进行分类的测试样本上进行了评估，并获得了每个类别的性能指标和可靠性估计。

Conclusion: 该研究通过概率检测分析来量化不同交通场景下的模型可靠性，并评估了不同深度学习方法在船舶轨迹预测（VTP）中的性能。结果为开发更可靠的VTP方法提供了信息，以提高内河航道的安全性和效率。

Abstract: This contribution addresses vessel trajectory prediction (VTP), focusing on
the evaluation of different deep learning-based approaches. The objective is to
assess model performance in diverse traffic complexities and compare the
reliability of the approaches. While previous VTP models overlook the specific
traffic situation complexity and lack reliability assessments, this research
uses a probability of detection analysis to quantify model reliability in
varying traffic scenarios, thus going beyond common error distribution
analyses. All models are evaluated on test samples categorized according to
their traffic situation during the prediction horizon, with performance metrics
and reliability estimates obtained for each category. The results of this
comprehensive evaluation provide a deeper understanding of the strengths and
weaknesses of the different prediction approaches, along with their reliability
in terms of the prediction horizon lengths for which safe forecasts can be
guaranteed. These findings can inform the development of more reliable vessel
trajectory prediction approaches, enhancing safety and efficiency in future
inland waterways navigation.

</details>


### [171] [FM4NPP: A Scaling Foundation Model for Nuclear and Particle Physics](https://arxiv.org/abs/2508.14087)
*David Park,Shuhang Li,Yi Huang,Xihaier Luo,Haiwang Yu,Yeonju Go,Christopher Pinkenburg,Yuewei Lin,Shinjae Yoo,Joseph Osborn,Jin Huang,Yihui Ren*

Main category: cs.LG

TL;DR: 该研究提出了一个用于粒子物理的自监督学习基础模型，在处理探测器数据方面取得了显著进展，并在多项任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了探索将大型语言模型范式应用于粒子物理领域的可行性，克服粒子探测器数据与自然语言数据在稀疏性和空间分布上的差异，并验证粒子物理基础模型的可扩展性和泛化能力。

Method: 提出了一种新颖的自监督学习方法，用于处理粒子探测器数据，并构建了一个包含超过1100万个粒子碰撞事件的新数据集及一系列下游任务和评估标签。模型规模扩展至1.88亿个参数，并使用冻结权重和特定任务的适配器进行微调。

Result: 该基础模型在所有下游任务中持续优于基线模型，性能表现出强大的数据效率适应性。模型提取的表征具有任务无关性，但可以通过单一线性映射进行特定任务的专门化。

Conclusion: 通过对超过1100万个粒子碰撞事件的新数据集进行训练，并采用新颖的自监督学习方法，该粒子物理基础模型在各种下游任务中表现优于基线模型，并且能够高效地进行数据适应性调整。通过单次线性映射，该模型可以针对不同下游任务进行专门化。

Abstract: Large language models have revolutionized artificial intelligence by enabling
large, generalizable models trained through self-supervision. This paradigm has
inspired the development of scientific foundation models (FMs). However,
applying this capability to experimental particle physics is challenging due to
the sparse, spatially distributed nature of detector data, which differs
dramatically from natural language. This work addresses if an FM for particle
physics can scale and generalize across diverse tasks. We introduce a new
dataset with more than 11 million particle collision events and a suite of
downstream tasks and labeled data for evaluation. We propose a novel
self-supervised training method for detector data and demonstrate its neural
scalability with models that feature up to 188 million parameters. With frozen
weights and task-specific adapters, this FM consistently outperforms baseline
models across all downstream tasks. The performance also exhibits robust
data-efficient adaptation. Further analysis reveals that the representations
extracted by the FM are task-agnostic but can be specialized via a single
linear mapping for different downstream tasks.

</details>


### [172] [Online Incident Response Planning under Model Misspecification through Bayesian Learning and Belief Quantization](https://arxiv.org/abs/2508.14385)
*Kim Hammar,Tao Li*

Main category: cs.LG

TL;DR: MOBAL is a new online method for cyberattack incident response planning that handles incomplete or inaccurate information by using Bayesian learning to adapt its model, outperforming existing methods in adaptability and robustness.


<details>
  <summary>Details</summary>
Motivation: Most decision-support frameworks for incident response rely on detailed system models, restricting their practical utility, especially when information about the attack is incomplete or inaccurate. MOBAL addresses this limitation.

Method: MOBAL (Misspecified Online Bayesian Learning) is an online method for incident response planning under model misspecification. It iteratively refines a conjecture about the model through Bayesian learning as new information becomes available, facilitating model adaptation. It quantizes the conjectured model into a finite Markov model for efficient response planning using dynamic programming.

Result: Bayesian learning is asymptotically consistent with information feedback. Bounds on misspecification and quantization errors are established. Experiments on the CAGE-2 benchmark show MOBAL outperforms the state of the art.

Conclusion: MOBAL outperforms the state of the art in terms of adaptability and robustness to model misspecification.

Abstract: Effective responses to cyberattacks require fast decisions, even when
information about the attack is incomplete or inaccurate. However, most
decision-support frameworks for incident response rely on a detailed system
model that describes the incident, which restricts their practical utility. In
this paper, we address this limitation and present an online method for
incident response planning under model misspecification, which we call MOBAL:
Misspecified Online Bayesian Learning. MOBAL iteratively refines a conjecture
about the model through Bayesian learning as new information becomes available,
which facilitates model adaptation as the incident unfolds. To determine
effective responses online, we quantize the conjectured model into a finite
Markov model, which enables efficient response planning through dynamic
programming. We prove that Bayesian learning is asymptotically consistent with
respect to the information feedback. Additionally, we establish bounds on
misspecification and quantization errors. Experiments on the CAGE-2 benchmark
show that MOBAL outperforms the state of the art in terms of adaptability and
robustness to model misspecification.

</details>


### [173] [Physics-Informed Reward Machines](https://arxiv.org/abs/2508.14093)
*Daniel Ajeleye,Ashutosh Trivedi,Majid Zamani*

Main category: cs.LG

TL;DR: Physics-informed reward machines (pRMs) enhance reinforcement learning by providing a structured way to define complex rewards, improving learning speed and efficiency in physical environments through techniques like counterfactual experiences and reward shaping.


<details>
  <summary>Details</summary>
Motivation: Reward machines (RMs) provide a structured way to specify non-Markovian rewards in RL, improving expressiveness and programmability by separating known environmental aspects (reward mechanism) from unknown ones. This supports techniques like counterfactual experience generation and reward shaping to reduce sample complexity and speed up learning. pRMs aim to enhance these benefits.

Method: Introduce physics-informed reward machines (pRMs), a symbolic machine designed to express complex learning objectives and reward structures for RL agents. Present RL algorithms capable of exploiting pRMs via counterfactual experiences and reward shaping.

Result: Experimental results show that techniques exploiting pRMs accelerate reward acquisition during RL training. pRMs demonstrate expressiveness and effectiveness in physical environments, significantly improving learning efficiency across several control tasks.

Conclusion: pRMs significantly improve learning efficiency across several control tasks in both finite and continuous physical environments by accelerating reward acquisition during RL training phases.

Abstract: Reward machines (RMs) provide a structured way to specify non-Markovian
rewards in reinforcement learning (RL), thereby improving both expressiveness
and programmability. Viewed more broadly, they separate what is known about the
environment, captured by the reward mechanism, from what remains unknown and
must be discovered through sampling. This separation supports techniques such
as counterfactual experience generation and reward shaping, which reduce sample
complexity and speed up learning. We introduce physics-informed reward machines
(pRMs), a symbolic machine designed to express complex learning objectives and
reward structures for RL agents, thereby enabling more programmable,
expressive, and efficient learning. We present RL algorithms capable of
exploiting pRMs via counterfactual experiences and reward shaping. Our
experimental results show that these techniques accelerate reward acquisition
during the training phases of RL. We demonstrate the expressiveness and
effectiveness of pRMs through experiments in both finite and continuous
physical environments, illustrating that incorporating pRMs significantly
improves learning efficiency across several control tasks.

</details>


### [174] [Hard Examples Are All You Need: Maximizing GRPO Post-Training Under Annotation Budgets](https://arxiv.org/abs/2508.14094)
*Benjamin Pikus,Pratyush Ranjan Tiwari,Burton Ye*

Main category: cs.LG

TL;DR: 在预算有限的情况下，应优先选择最难的样本进行GRPO微调，以获得最大的性能提升。


<details>
  <summary>Details</summary>
Motivation: 在资源有限的情况下，研究如何选择训练样本以优化语言模型微调的性能。

Method: 通过多样本评估获得的基础模型难度估计，对同一未标记池中的样本进行难度排序，并研究了四种不同的子集选择策略（易、中、难、随机）。

Result: 在GRPO微调中，选择最难的样本可以带来最大的性能提升（高达47%），而选择最容易的样本提升最小。

Conclusion: 在预算有限的情况下，应优先选择最难的样本进行GRPO训练，因为这能带来最大的性能提升，可达47%。

Abstract: Collecting high-quality training examples for language model fine-tuning is
expensive, with practical budgets limiting the amount of data that can be
procured. We investigate a critical question for resource-constrained
alignment: under a fixed acquisition budget, should practitioners prioritize
examples that are easy, medium, hard, or of random difficulty? We study Group
Relative Policy Optimization (GRPO) fine-tuning across different model sizes
and families, comparing four subset selection policies chosen from the same
unlabeled pool using base-model difficulty estimates obtained via multi-sample
evaluation. Our experiments reveal that training on the hardest examples yields
the largest performance gains, up to 47%, while training on easy examples yield
the smallest gains. Analysis reveals that this effect arises from harder
examples providing more learnable opportunities during GRPO training. These
findings provide practical guidance for budget-constrained post-training:
prioritizing hard examples yields substantial performance gains on reasoning
tasks when using GRPO.

</details>


### [175] [Implicit Hypergraph Neural Network](https://arxiv.org/abs/2508.14101)
*Akash Choudhuri,Yongjian Zhong,Bijaya Adhikari*

Main category: cs.LG

TL;DR: IHNN框架通过联合学习节点和超边的固定点表示，解决了现有超图神经网络在捕获长距离依赖性时性能下降的问题，并在节点分类任务上取得了优于现有技术的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的超图神经网络（HGNN）通过少量消息传递来学习表示，这导致它们只能捕获局部信息，而忽略了长距离高阶依赖性。然而，盲目增加消息传递轮数来捕获长距离依赖性也会损害HGNN的性能。因此，有必要研究解决超图神经网络长距离依赖性问题的有效方法。

Method: 提出了一种名为隐式超图神经网络（IHNN）的新框架，该框架联合以端到端的方式学习节点和超边的固定点表示，以解决超图神经网络在捕获长距离依赖性时性能下降的问题。通过利用隐式微分，提出了一种可行的投影梯度下降方法来有效地训练模型。

Result: IHNN在真实世界超图的节点分类任务上进行了广泛的实验，结果表明IHNN在大多数情况下优于最接近的先前工作，并在超图学习领域建立了新的最先进水平。

Conclusion: IHNN通过联合学习节点和超边的固定点表示，利用隐式微分和投影梯度下降进行高效训练，从而解决了现有超图神经网络在聚合更多信息以捕获长距离依赖性时会丢失预测能力的问题。在真实世界超图的节点分类任务上的广泛实验表明，IHNN在大多数情况下优于现有技术，并在超图学习领域建立了新的最先进水平。

Abstract: Hypergraphs offer a generalized framework for capturing high-order
relationships between entities and have been widely applied in various domains,
including healthcare, social networks, and bioinformatics. Hypergraph neural
networks, which rely on message-passing between nodes over hyperedges to learn
latent representations, have emerged as the method of choice for predictive
tasks in many of these domains. These approaches typically perform only a small
number of message-passing rounds to learn the representations, which they then
utilize for predictions. The small number of message-passing rounds comes at a
cost, as the representations only capture local information and forego
long-range high-order dependencies. However, as we demonstrate, blindly
increasing the message-passing rounds to capture long-range dependency also
degrades the performance of hyper-graph neural networks.
  Recent works have demonstrated that implicit graph neural networks capture
long-range dependencies in standard graphs while maintaining performance.
Despite their popularity, prior work has not studied long-range dependency
issues on hypergraph neural networks. Here, we first demonstrate that existing
hypergraph neural networks lose predictive power when aggregating more
information to capture long-range dependency. We then propose Implicit
Hypergraph Neural Network (IHNN), a novel framework that jointly learns
fixed-point representations for both nodes and hyperedges in an end-to-end
manner to alleviate this issue. Leveraging implicit differentiation, we
introduce a tractable projected gradient descent approach to train the model
efficiently. Extensive experiments on real-world hypergraphs for node
classification demonstrate that IHNN outperforms the closest prior works in
most settings, establishing a new state-of-the-art in hypergraph learning.

</details>


### [176] [From AI for Science to Agentic Science: A Survey on Autonomous Scientific Discovery](https://arxiv.org/abs/2508.14111)
*Jiaqi Wei,Yuejin Yang,Xiang Zhang,Yuhan Chen,Xiang Zhuang,Zhangyang Gao,Dongzhan Zhou,Guangshuai Wang,Zhiqiang Gao,Juntai Cao,Zijie Qiu,Xuming He,Qiang Zhang,Chenyu You,Shuangjia Zheng,Ning Ding,Wanli Ouyang,Nanqing Dong,Yu Cheng,Siqi Sun,Lei Bai,Bowen Zhou*

Main category: cs.LG

TL;DR: AI驱动的科学发现正迈向自主新阶段，即Agentic Science。该研究提出了一个新框架，梳理了AI在科学各领域的自主发现能力，并展望了未来发展方向。


<details>
  <summary>Details</summary>
Motivation: AI正在重塑科学发现，从专业的计算工具演变为自主的研究伙伴，提出Agentic Science以理解和推进这一演变。

Method: 通过一个综合框架，连接了基础能力、核心过程和领域特定实现，对生命科学、化学、材料科学和物理学领域的自主科学发现进行了领域导向的综述。

Result: 该工作建立了自主科学发现的领域导向综合，并将Agentic Science定位为推进AI驱动研究的结构化范式，识别了支撑科学自主性的五项核心能力，并模拟了将发现视为一个动态的四阶段工作流程。

Conclusion: 本文提出了Agentic Science的概念，将其定位为AI for Science范式中的关键阶段，强调AI系统从部分协助发展到完全的科学自主性。通过一个综合框架，连接了基础能力、核心过程和领域特定实现，对生命科学、化学、材料科学和物理学领域的自主科学发现进行了领域导向的综述。

Abstract: Artificial intelligence (AI) is reshaping scientific discovery, evolving from
specialized computational tools into autonomous research partners. We position
Agentic Science as a pivotal stage within the broader AI for Science paradigm,
where AI systems progress from partial assistance to full scientific agency.
Enabled by large language models (LLMs), multimodal systems, and integrated
research platforms, agentic AI shows capabilities in hypothesis generation,
experimental design, execution, analysis, and iterative refinement -- behaviors
once regarded as uniquely human. This survey provides a domain-oriented review
of autonomous scientific discovery across life sciences, chemistry, materials
science, and physics. We unify three previously fragmented perspectives --
process-oriented, autonomy-oriented, and mechanism-oriented -- through a
comprehensive framework that connects foundational capabilities, core
processes, and domain-specific realizations. Building on this framework, we (i)
trace the evolution of AI for Science, (ii) identify five core capabilities
underpinning scientific agency, (iii) model discovery as a dynamic four-stage
workflow, (iv) review applications across the above domains, and (v) synthesize
key challenges and future opportunities. This work establishes a
domain-oriented synthesis of autonomous scientific discovery and positions
Agentic Science as a structured paradigm for advancing AI-driven research.

</details>


### [177] [A Cost-Effective Framework for Predicting Parking Availability Using Geospatial Data and Machine Learning](https://arxiv.org/abs/2508.14125)
*Madyan Bagosher,Tala Mustafa,Mohammad Alsmirat,Amal Al-Ali,Isam Mashhour Al Jawarneh*

Main category: cs.LG

TL;DR: A smart framework using location services data and various forecasting models (including Random Forest Regression and LSTM) was developed to help university students find parking. Random Forest Regression performed best among the tested models, but LSTM might offer better performance with more data.


<details>
  <summary>Details</summary>
Motivation: Urban populations are growing, leading to challenges in managing parking and determining occupancy, especially on university campuses where students need to find vacant parking spots quickly and conveniently during class timings. Efficient systems are necessary to allocate vacant parking spots effectively.

Method: A smart framework integrating street maps, mobility, and meteorological data through a spatial join operation to capture parking behavior and vehicle movement patterns. Data collected using location services, without requiring installed sensing tools. Evaluated forecasting models: Linear Regression, Support Vector Regression (SVR), Random Forest Regression (RFR), and Long Short-Term Memory (LSTM). Hyperparameter tuning using grid search. Model performance assessed using Root Mean Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of Determination (R2).

Result: Random Forest Regression achieved the lowest RMSE of 0.142 and highest R2 of 0.582.

Conclusion: Random Forest Regression achieved the lowest RMSE of 0.142 and highest R2 of 0.582. However, given the time-series nature of the task, an LSTM model may perform better with additional data and longer timesteps.

Abstract: As urban populations continue to grow, cities face numerous challenges in
managing parking and determining occupancy. This issue is particularly
pronounced in university campuses, where students need to find vacant parking
spots quickly and conveniently during class timings. The limited availability
of parking spaces on campuses underscores the necessity of implementing
efficient systems to allocate vacant parking spots effectively. We propose a
smart framework that integrates multiple data sources, including street maps,
mobility, and meteorological data, through a spatial join operation to capture
parking behavior and vehicle movement patterns over the span of 3 consecutive
days with an hourly duration between 7AM till 3PM. The system will not require
any sensing tools to be installed in the street or in the parking area to
provide its services since all the data needed will be collected using location
services. The framework will use the expected parking entrance and time to
specify a suitable parking area. Several forecasting models, namely, Linear
Regression, Support Vector Regression (SVR), Random Forest Regression (RFR),
and Long Short-Term Memory (LSTM), are evaluated. Hyperparameter tuning was
employed using grid search, and model performance is assessed using Root Mean
Squared Error (RMSE), Mean Absolute Error (MAE) and Coefficient of
Determination (R2). Random Forest Regression achieved the lowest RMSE of 0.142
and highest R2 of 0.582. However, given the time-series nature of the task, an
LSTM model may perform better with additional data and longer timesteps.

</details>


### [178] [ERIS: An Energy-Guided Feature Disentanglement Framework for Out-of-Distribution Time Series Classification](https://arxiv.org/abs/2508.14134)
*Xin Wu,Fei Teng,Ji Zhang,Xingwang Li,Yuxuan Liang*

Main category: cs.LG

TL;DR: ERIS是一个时间序列分类框架，通过引导式特征解耦解决分布外泛化问题，并取得显著的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类（TSC）模型在从分布外（OOD）数据中捕获不变表示方面存在困难，因为模型将特定领域和标签相关特征交织在一起，导致虚假相关性。现有的特征解耦方法缺乏将真正通用的特征分离所需的语义指导。

Method: ERIS框架通过以下机制实现特征解耦：1. 能量引导校准机制：为特征分离提供关键的语义指导，使模型能够自我校准。2. 权重级正交性策略：强制领域特定特征和标签相关特征之间的结构独立性，以减轻它们的干扰。3. 辅助对抗性训练机制：通过注入结构化扰动来增强鲁棒性。

Result: ERIS框架在四个基准测试中平均提高了4.04%的准确率，优于最先进的基线方法。

Conclusion: ERIS框架通过能量引导校准、权重级正交性和辅助对抗性训练等机制，实现了受引导且可靠的特征解耦，在四个基准测试中平均提高了4.04%的准确率，优于最先进的方法。

Abstract: An ideal time series classification (TSC) should be able to capture invariant
representations, but achieving reliable performance on out-of-distribution
(OOD) data remains a core obstacle. This obstacle arises from the way models
inherently entangle domain-specific and label-relevant features, resulting in
spurious correlations. While feature disentanglement aims to solve this,
current methods are largely unguided, lacking the semantic direction required
to isolate truly universal features. To address this, we propose an end-to-end
Energy-Regularized Information for Shift-Robustness (\textbf{ERIS}) framework
to enable guided and reliable feature disentanglement. The core idea is that
effective disentanglement requires not only mathematical constraints but also
semantic guidance to anchor the separation process. ERIS incorporates three key
mechanisms to achieve this goal. Specifically, we first introduce an
energy-guided calibration mechanism, which provides crucial semantic guidance
for the separation, enabling the model to self-calibrate. Additionally, a
weight-level orthogonality strategy enforces structural independence between
domain-specific and label-relevant features, thereby mitigating their
interference. Moreover, an auxiliary adversarial training mechanism enhances
robustness by injecting structured perturbations. Experiments demonstrate that
ERIS improves upon state-of-the-art baselines by an average of 4.04% accuracy
across four benchmarks.

</details>


### [179] [Towards Agent-based Test Support Systems: An Unsupervised Environment Design Approach](https://arxiv.org/abs/2508.14135)
*Collins O. Ogbodo,Timothy J. Rogers,Mattia Dal Borgo,David J. Wagg*

Main category: cs.LG

TL;DR: 提出一种基于智能体的自适应传感器放置框架，用于解决模态测试中的动态变化问题，通过强化学习优化传感器位置，并在案例研究中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统模态测试设计方法僵化，无法适应动态变化的测试环境和参数（如传感器配置）影响的问题。

Method: 该框架使用不完全指定的马尔可夫决策过程对问题进行公式化，并采用双课程学习策略来训练通用强化学习智能体。

Result: 该框架能够优化传感器在频率段上的位置，并在钢悬臂结构案例研究中得到验证。

Conclusion: 该研究提出了一种用于动态变化的模态测试环境中的自适应传感器放置的基于智能体的决策支持框架，并通过在钢悬臂结构上的详细案例研究证明了其有效性、稳健性和实际适用性。

Abstract: Modal testing plays a critical role in structural analysis by providing
essential insights into dynamic behaviour across a wide range of engineering
industries. In practice, designing an effective modal test campaign involves
complex experimental planning, comprising a series of interdependent decisions
that significantly influence the final test outcome. Traditional approaches to
test design are typically static-focusing only on global tests without
accounting for evolving test campaign parameters or the impact of such changes
on previously established decisions, such as sensor configurations, which have
been found to significantly influence test outcomes. These rigid methodologies
often compromise test accuracy and adaptability. To address these limitations,
this study introduces an agent-based decision support framework for adaptive
sensor placement across dynamically changing modal test environments. The
framework formulates the problem using an underspecified partially observable
Markov decision process, enabling the training of a generalist reinforcement
learning agent through a dual-curriculum learning strategy. A detailed case
study on a steel cantilever structure demonstrates the efficacy of the proposed
method in optimising sensor locations across frequency segments, validating its
robustness and real-world applicability in experimental settings.

</details>


### [180] [Topological Data Analysis for Unsupervised Anomaly Detection and Customer Segmentation on Banking Data](https://arxiv.org/abs/2508.14136)
*Leonardo Aldo Alejandro Barberi,Linda Maria De Cave*

Main category: cs.LG

TL;DR: This paper uses topology (Mapper algorithm, persistent homology) for unsupervised banking customer analysis (anomaly detection, segmentation), providing practical industry insights.


<details>
  <summary>Details</summary>
Motivation: To apply advanced topological data analysis techniques for unsupervised anomaly detection and customer segmentation in banking data.

Method: Utilizing the Mapper algorithm and persistent homology for unsupervised procedures in topological data analysis.

Result: Uncovered meaningful patterns in customers' banking data by exploiting topological information.

Conclusion: The presented framework effectively combines topology with real-world banking use cases for customer segmentation and anomaly detection, yielding actionable insights.

Abstract: This paper introduces advanced techniques of Topological Data Analysis (TDA)
for unsupervised anomaly detection and customer segmentation in banking data.
Using the Mapper algorithm and persistent homology, we develop unsupervised
procedures that uncover meaningful patterns in customers' banking data by
exploiting topological information. The framework we present in this paper
yields actionable insights that combine the abstract mathematical subject of
topology with real-life use cases that are useful in industry.

</details>


### [181] [Learning to Learn the Macroscopic Fundamental Diagram using Physics-Informed and meta Machine Learning techniques](https://arxiv.org/abs/2508.14137)
*Amalie Roark,Serio Agriesti,Francisco Camara Pereira,Guido Cantelmo*

Main category: cs.LG

TL;DR: 宏观基本图（MFD）的估计通常需要大量数据，而此研究提出了一种利用元学习的方法，通过在多个城市的数据上进行训练，来提高在数据有限的城市中估计 MFD 的性能。


<details>
  <summary>Details</summary>
Motivation: 宏观基本图（MFD）是描述交通动态的流行工具，但估计给定网络的 MFD 需要大量的环形检测器，这在实践中并非总是可用。

Method: 提出了一种利用元学习（机器学习的一个子类，训练模型自行理解和适应新任务）的框架，以缓解数据稀疏的挑战。所开发的模型通过利用来自多个城市的数据进行训练和测试，并利用这些数据来模拟具有不同探测器份额和拓扑结构的城市 MFD。

Result: 结果显示，在流量预测方面，平均 MSE 提高了约 17500 到 36000（取决于测试的环形检测器子集）。

Conclusion: 该元学习框架成功推广到不同的城市环境，并提高了数据有限的城市的性能，证明了在检测器数量有限的情况下使用元学习的潜力。最后，将该框架与传统的迁移学习方法进行了验证，并与文献中的非参数模型 FitFun 进行了测试，以证明其可转移性。

Abstract: The Macroscopic Fundamental Diagram is a popular tool used to describe
traffic dynamics in an aggregated way, with applications ranging from traffic
control to incident analysis. However, estimating the MFD for a given network
requires large numbers of loop detectors, which is not always available in
practice. This article proposes a framework harnessing meta-learning, a
subcategory of machine learning that trains models to understand and adapt to
new tasks on their own, to alleviate the data scarcity challenge. The developed
model is trained and tested by leveraging data from multiple cities and
exploiting it to model the MFD of other cities with different shares of
detectors and topological structures. The proposed meta-learning framework is
applied to an ad-hoc Multi-Task Physics-Informed Neural Network, specifically
designed to estimate the MFD. Results show an average MSE improvement in flow
prediction ranging between ~ 17500 and 36000 (depending on the subset of loop
detectors tested). The meta-learning framework thus successfully generalizes
across diverse urban settings and improves performance on cities with limited
data, demonstrating the potential of using meta-learning when a limited number
of detectors is available. Finally, the proposed framework is validated against
traditional transfer learning approaches and tested with FitFun, a
non-parametric model from the literature, to prove its transferability.

</details>


### [182] [Neuro-inspired Ensemble-to-Ensemble Communication Primitives for Sparse and Efficient ANNs](https://arxiv.org/abs/2508.14140)
*Orestis Konstantaropoulos,Stelios Manolis Smirnakis,Maria Papadopouli*

Main category: cs.LG

TL;DR: 受小鼠视觉皮层连接模式启发，提出G2GNet网络架构，通过稀疏、模块化连接和动态稀疏训练，在提升准确率的同时大幅减少计算量。


<details>
  <summary>Details</summary>
Motivation: 受到系统神经科学关于小鼠视觉皮层功能连接模式（特别是群体到群体通信）的发现的启发，旨在将这些生物学原理应用于人工神经网络（ANN）的设计，以提高效率和性能。

Method: 提出了一种名为G2GNet的新型网络架构，该架构在前馈层施加了稀疏、模块化的连接，并采用动态稀疏训练（DST）机制进行训练，该机制在训练过程中会修剪和重新生长连接。此外，还提出了一种基于激活相关性的、受生物可塑性启发的赫布学习重塑规则。

Result: G2GNet在Fashion-MNIST、CIFAR-10和CIFAR-100等标准视觉基准测试中，在显著减少参数量和计算量的同时，取得了优于全连接模型的准确率，最高稀疏度达到75%，准确率提升最高达4.3%。

Conclusion: G2GNet通过引入稀疏、模块化的前馈层连接，并结合动态稀疏训练（DST）机制和基于激活相关性的赫布学习重塑规则，实现了高达75%的稀疏度，并在Fashion-MNIST、CIFAR-10和CIFAR-100等基准测试中提高了高达4.3%的准确率，同时显著减少了计算量，优于密集基线模型。

Abstract: The structure of biological neural circuits-modular, hierarchical, and
sparsely interconnected-reflects an efficient trade-off between wiring cost,
functional specialization, and robustness. These principles offer valuable
insights for artificial neural network (ANN) design, especially as networks
grow in depth and scale. Sparsity, in particular, has been widely explored for
reducing memory and computation, improving speed, and enhancing generalization.
Motivated by systems neuroscience findings, we explore how patterns of
functional connectivity in the mouse visual cortex-specifically,
ensemble-to-ensemble communication, can inform ANN design. We introduce G2GNet,
a novel architecture that imposes sparse, modular connectivity across
feedforward layers. Despite having significantly fewer parameters than fully
connected models, G2GNet achieves superior accuracy on standard vision
benchmarks. To our knowledge, this is the first architecture to incorporate
biologically observed functional connectivity patterns as a structural bias in
ANN design. We complement this static bias with a dynamic sparse training (DST)
mechanism that prunes and regrows edges during training. We also propose a
Hebbian-inspired rewiring rule based on activation correlations, drawing on
principles of biological plasticity. G2GNet achieves up to 75% sparsity while
improving accuracy by up to 4.3% on benchmarks, including Fashion-MNIST,
CIFAR-10, and CIFAR-100, outperforming dense baselines with far fewer
computations.

</details>


### [183] [Beyond Turing: Memory-Amortized Inference as a Foundation for Cognitive Computation](https://arxiv.org/abs/2508.14143)
*Xin Li*

Main category: cs.LG

TL;DR: MAI是一种新的认知框架，通过重用记忆中的结构来推理，而不是重新计算，这使得推理更有效率，并可能为AGI铺平道路。


<details>
  <summary>Details</summary>
Motivation: 智能本质上是非遍历的，它不是来自均匀采样或从头开始优化，而是来自结构化地重用先前的推理轨迹。

Method: MAI框架将认知建模为在记忆的潜在周期上进行推理，而不是通过梯度下降进行重新计算。我们通过 delta-同调来展示MAI为Mountcastle的通用皮层算法提供了一个原则性基础，将每个皮层柱建模为在周期一致的记忆状态上进行局部推理的算子。

Result: MAI通过编码归纳偏差来最小化熵，并实现上下文感知、结构保持的推理。它将认知系统重新定义为在约束潜在流形上进行导航的导航器，而不是遍历采样器，并由持久的拓扑记忆指导。MAI与强化学习之间的时间反演对偶性，使得MAI能够从记忆中向后重建潜在原因，而RL则是从奖励向前传播价值。这种逆转为能源效率低下提供了途径，并解决了现代AI面临的计算瓶颈。

Conclusion: MAI提供了一个统一的、以生物为基础的、基于结构、重用和记忆的智能理论，并对实现通用人工智能（AGI）具有深远的影响。

Abstract: Intelligence is fundamentally non-ergodic: it emerges not from uniform
sampling or optimization from scratch, but from the structured reuse of prior
inference trajectories. We introduce Memory-Amortized Inference (MAI) as a
formal framework in which cognition is modeled as inference over latent cycles
in memory, rather than recomputation through gradient descent. MAI systems
encode inductive biases via structural reuse, minimizing entropy and enabling
context-aware, structure-preserving inference. This approach reframes cognitive
systems not as ergodic samplers, but as navigators over constrained latent
manifolds, guided by persistent topological memory. Through the lens of
delta-homology, we show that MAI provides a principled foundation for
Mountcastle's Universal Cortical Algorithm, modeling each cortical column as a
local inference operator over cycle-consistent memory states. Furthermore, we
establish a time-reversal duality between MAI and reinforcement learning:
whereas RL propagates value forward from reward, MAI reconstructs latent causes
backward from memory. This inversion paves a path toward energy-efficient
inference and addresses the computational bottlenecks facing modern AI. MAI
thus offers a unified, biologically grounded theory of intelligence based on
structure, reuse, and memory. We also briefly discuss the profound implications
of MAI for achieving artificial general intelligence (AGI).

</details>


### [184] [Noise Robust One-Class Intrusion Detection on Dynamic Graphs](https://arxiv.org/abs/2508.14192)
*Aleksei Liuliakov,Alexander Schulz,Luca Hermes,Barbara Hammer*

Main category: cs.LG

TL;DR: 该研究提出了一种概率TGN-SVDD模型，通过预测高斯分布参数来提高网络入侵检测中对噪声数据的鲁棒性，并在实验中取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 网络入侵检测中，处理被污染和有噪声的数据输入是一个关键挑战。

Method: 提出了一种概率TGN-SVDD模型，通过为每个网络事件预测高斯分布的参数来处理噪声数据。

Result: 在包含合成噪声的CIC-IDS2017数据集上进行实验，结果表明与基线TGN-SVDD模型相比，该模型在检测性能上有了显著提升，尤其是在噪声水平增加时。

Conclusion: 该研究通过引入高斯分布参数预测，成功提高了TGN-SVDD模型在存在输入噪声时的鲁棒性和检测准确性，尤其是在噪声水平较高的情况下。

Abstract: In the domain of network intrusion detection, robustness against contaminated
and noisy data inputs remains a critical challenge. This study introduces a
probabilistic version of the Temporal Graph Network Support Vector Data
Description (TGN-SVDD) model, designed to enhance detection accuracy in the
presence of input noise. By predicting parameters of a Gaussian distribution
for each network event, our model is able to naturally address noisy
adversarials and improve robustness compared to a baseline model. Our
experiments on a modified CIC-IDS2017 data set with synthetic noise demonstrate
significant improvements in detection performance compared to the baseline
TGN-SVDD model, especially as noise levels increase.

</details>


### [185] [Graph Concept Bottleneck Models](https://arxiv.org/abs/2508.14255)
*Haotian Xu,Tsui-Wei Weng,Lam M. Nguyen,Tengfei Ma*

Main category: cs.LG

TL;DR: GraphCBMs通过引入概念图来解决现有CBMs中概念之间被假设为独立和隔离的问题，从而提高了模型性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的CBMs假设概念在给定标签的条件下是条件独立的，并且彼此隔离，忽略了概念之间隐藏的关系。然而，CBM中的概念集通常具有内在结构，概念之间普遍存在相关性：改变一个概念会对其相关概念产生影响。

Method: 提出了一种名为GraphCBMs的新型CBM变体，通过构建潜在概念图来促进概念关系，并将其与CBM结合使用。

Result: 实验结果表明，GraphCBMs在图像分类任务中表现优越，提供了更多的概念结构信息以增强可解释性，能够利用潜在概念图进行更有效的干预，并且在不同的训练和架构设置中表现稳健。

Conclusion: GraphCBMs能够利用潜在概念图来增强模型性能，同时保持其可解释性，并在图像分类任务中表现出优越的性能，提供更多的概念结构信息，能够更有效地进行干预，并且在不同的训练和架构设置中表现出稳健的性能。

Abstract: Concept Bottleneck Models (CBMs) provide explicit interpretations for deep
neural networks through concepts and allow intervention with concepts to adjust
final predictions. Existing CBMs assume concepts are conditionally independent
given labels and isolated from each other, ignoring the hidden relationships
among concepts. However, the set of concepts in CBMs often has an intrinsic
structure where concepts are generally correlated: changing one concept will
inherently impact its related concepts. To mitigate this limitation, we propose
GraphCBMs: a new variant of CBM that facilitates concept relationships by
constructing latent concept graphs, which can be combined with CBMs to enhance
model performance while retaining their interpretability. Our experiment
results on real-world image classification tasks demonstrate Graph CBMs offer
the following benefits: (1) superior in image classification tasks while
providing more concept structure information for interpretability; (2) able to
utilize latent concept graphs for more effective interventions; and (3) robust
in performance across different training and architecture settings.

</details>


### [186] [Amortized Bayesian Meta-Learning for Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2508.14285)
*Liyi Zhang,Jake Snell,Thomas L. Griffiths*

Main category: cs.LG

TL;DR: ABMLL 是一种高效的 LLM 微调方法，通过优化 LoRA 参数提升泛化能力和不确定性量化，在多个基准测试中表现优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有 LLM 微调方法（如使用上下文提示或元学习）在泛化能力评估和计算成本方面存在的问题，提出 ABMLL 方法。

Method: ABMLL 通过重构 LoRA 中的任务特定参数和全局参数，并引入新的超参数来平衡重建准确性和任务特定参数与全局参数的保真度，从而实现对 LLM 的高效泛化和不确定性量化。

Result: ABMLL 在 Unified-QA 和 CrossFit 数据集上展示了比现有方法更好的性能，尤其在准确性和预期校准误差方面表现更优，并且能够扩展到 Llama3-8B 等大型模型。

Conclusion: Amortized Bayesian Meta-Learning for LoRA (ABMLL) 能够有效提升 LLM 在未见数据集上的泛化能力，同时保持计算效率，并在不确定性量化方面有所改进。在 Unified-QA 和 CrossFit 数据集上的测试结果表明，ABMLL 在准确性和预期校准误差方面均优于现有方法。

Abstract: Fine-tuning large language models (LLMs) with low-rank adaptaion (LoRA) is a
cost-effective way to incorporate information from a specific dataset. However,
it is often unclear how well the fine-tuned LLM will generalize, i.e., how well
it will perform on unseen datasets. Methods have been proposed to improve
generalization by optimizing with in-context prompts, or by using meta-learning
to fine-tune LLMs. However, these methods are expensive in memory and
computation, requiring either long-context prompts or saving copies of
parameters and using second-order gradient updates. To address these
challenges, we propose Amortized Bayesian Meta-Learning for LoRA (ABMLL). This
method builds on amortized Bayesian meta-learning for smaller models, adapting
this approach to LLMs while maintaining its computational efficiency. We
reframe task-specific and global parameters in the context of LoRA and use a
set of new hyperparameters to balance reconstruction accuracy and the fidelity
of task-specific parameters to the global ones. ABMLL provides effective
generalization and scales to large models such as Llama3-8B. Furthermore, as a
result of using a Bayesian framework, ABMLL provides improved uncertainty
quantification. We test ABMLL on Unified-QA and CrossFit datasets and find that
it outperforms existing methods on these benchmarks in terms of both accuracy
and expected calibration error.

</details>


### [187] [GLASS: Test-Time Acceleration for LLMs via Global-Local Neural Importance Aggregation](https://arxiv.org/abs/2508.14302)
*Amirmohsen Sattarifard,Sepehr Lavasani,Ehsan Imani,Kunlin Zhang,Hanlin Xu,Fengyu Sun,Negar Hassanpour,Chao Gao*

Main category: cs.LG

TL;DR: A/I-GLASS dynamically prunes LLMs for edge deployment using neuron statistics, outperforming prior methods in long-form generation without extra overhead.


<details>
  <summary>Details</summary>
Motivation: LLMs require prompt-aware dynamic pruning for deployment on edge hardware to reduce computation without sacrificing quality. Existing static or predictor-based methods have limitations, and previous zero-shot methods fail in certain scenarios.

Method: A/I-GLASS employs two training-free methods that dynamically select Feed-Forward Network (FFN) units by aggregating prompt-local and model-intrinsic global neuron statistics.

Result: A/I-GLASS significantly outperforms prior training-free methods across multiple LLMs and benchmarks, particularly in challenging long-form generation scenarios.

Conclusion: A/I-GLASS has been shown to significantly outperform previous training-free methods, especially in long-form generation scenarios, without requiring auxiliary predictors or increasing inference overhead.

Abstract: Deploying Large Language Models (LLMs) on edge hardware demands aggressive,
prompt-aware dynamic pruning to reduce computation without degrading quality.
Static or predictor-based schemes either lock in a single sparsity pattern or
incur extra runtime overhead, and recent zero-shot methods that rely on
statistics from a single prompt fail on short prompt and/or long generation
scenarios. We introduce A/I-GLASS: Activation- and Impact-based Global-Local
neural importance Aggregation for feed-forward network SparSification, two
training-free methods that dynamically select FFN units using a
rank-aggregation of prompt local and model-intrinsic global neuron statistics.
Empirical results across multiple LLMs and benchmarks demonstrate that GLASS
significantly outperforms prior training-free methods, particularly in
challenging long-form generation scenarios, without relying on auxiliary
predictors or adding any inference overhead.

</details>


### [188] [Learning Time-Varying Convexifications of Multiple Fairness Measures](https://arxiv.org/abs/2508.14311)
*Quan Zhou,Jakub Marecek,Robert Shorten*

Main category: cs.LG

TL;DR: Learning dynamic fairness measures with graph feedback.


<details>
  <summary>Details</summary>
Motivation: The need to consider multiple measures of fairness, where the relative weights of fairness regularisers are a priori unknown, may be time varying, and need to be learned on the fly.

Method: Learning time-varying convexifications of multiple fairness measures with limited graph-structured feedback.

Result: The paper addresses the challenge of learning fairness measures in dynamic environments.

Conclusion: This paper considers learning time-varying convexifications of multiple fairness measures with limited graph-structured feedback.

Abstract: There is an increasing appreciation that one may need to consider multiple
measures of fairness, e.g., considering multiple group and individual fairness
notions. The relative weights of the fairness regularisers are a priori
unknown, may be time varying, and need to be learned on the fly. We consider
the learning of time-varying convexifications of multiple fairness measures
with limited graph-structured feedback.

</details>


### [189] [Your Reward Function for RL is Your Best PRM for Search: Unifying RL and Search-Based TTS](https://arxiv.org/abs/2508.14313)
*Can Jin,Yang Zhou,Qixin Zhang,Hongwu Peng,Di Zhang,Marco Pavone,Ligong Han,Zhang-Wei Hong,Tong Che,Dimitris N. Metaxas*

Main category: cs.LG

TL;DR: AIRL-S 统一了基于强化学习和搜索的测试时缩放（TTS）技术，通过从正确的推理轨迹中学习一个动态的奖励模型（PRM），无需标记数据，从而提高了 LLM 的推理能力，并在各种任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时缩放（TTS）技术要么依赖于强化学习（RL）方法，这些方法存在不稳定性且样本效率低下，要么依赖于需要昂贵标签的搜索技术，这些技术在分布变化下表现不佳。

Method: AIRL-S 结合了对抗性逆强化学习（AIRL）和群组相对策略优化（GRPO），直接从正确的推理轨迹中学习一个密集的、动态的 PRM，该 PRM 在推理时既用作 RL của 评论者，也用作引导搜索过程的启发式方法。

Result: AIRL-S 在八个基准测试（包括数学、科学推理和代码生成）中的平均性能比基础模型提高了 9%，与 GPT-4o 相当。此外，当集成到多个搜索算法中时，AIRL-S 的 PRM 始终优于所有使用标记数据训练的基线 PRM。

Conclusion: AIRL-S 成功地统一了基于强化学习（RL）和基于搜索的技术，通过学习一个动态的、与过程相关的奖励模型（PRM），消除了对标记中间过程数据的需求，并在八个基准测试中平均提高了 9% 的性能，与 GPT-4o 相当，同时在多个搜索算法中优于所有基线 PRM。

Abstract: Test-time scaling (TTS) for large language models (LLMs) has thus far fallen
into two largely separate paradigms: (1) reinforcement learning (RL) methods
that optimize sparse outcome-based rewards, yet suffer from instability and low
sample efficiency; and (2) search-based techniques guided by independently
trained, static process reward models (PRMs), which require expensive human- or
LLM-generated labels and often degrade under distribution shifts. In this
paper, we introduce AIRL-S, the first natural unification of RL-based and
search-based TTS. Central to AIRL-S is the insight that the reward function
learned during RL training inherently represents the ideal PRM for guiding
downstream search. Specifically, we leverage adversarial inverse reinforcement
learning (AIRL) combined with group relative policy optimization (GRPO) to
learn a dense, dynamic PRM directly from correct reasoning traces, entirely
eliminating the need for labeled intermediate process data. At inference, the
resulting PRM simultaneously serves as the critic for RL rollouts and as a
heuristic to effectively guide search procedures, facilitating robust reasoning
chain extension, mitigating reward hacking, and enhancing cross-task
generalization. Experimental results across eight benchmarks, including
mathematics, scientific reasoning, and code generation, demonstrate that our
unified approach improves performance by 9 % on average over the base model,
matching GPT-4o. Furthermore, when integrated into multiple search algorithms,
our PRM consistently outperforms all baseline PRMs trained with labeled data.
These results underscore that, indeed, your reward function for RL is your best
PRM for search, providing a robust and cost-effective solution to complex
reasoning tasks in LLMs.

</details>


### [190] [FedRAIN-Lite: Federated Reinforcement Algorithms for Improving Idealised Numerical Weather and Climate Models](https://arxiv.org/abs/2508.14315)
*Pritthijit Nath,Sebastian Schemm,Henry Moss,Peter Haynes,Emily Shuckburgh,Mark Webb*

Main category: cs.LG

TL;DR: 介绍了一种名为FedRAIN-Lite的联邦强化学习框架，用于气候模型中的子网格参数化，并证明了DDPG算法在该框架下的优越性。


<details>
  <summary>Details</summary>
Motivation: 气候模型中的子网格参数化通常是静态的，并且是离线调整的，这限制了它们适应不断变化的状态的能力。

Method: 使用从单代理基线（ebm-v1）到多代理集成（ebm-v2）和类GCM（ebm-v3）设置的能量平衡气候模型层次结构，在不同的FedRL配置下对三种RL算法进行了基准测试。

Result: 与静态和单代理基线相比，深度确定性策略梯度（DDPG）在收敛速度和热带及中纬度地区的面积加权RMSE方面持续表现更好。

Conclusion: FedRAIN-Lite框架通过将代理分配到纬度带，实现了GCM中的空间分解，从而实现了本地参数学习和定期的全局聚合。DDPG算法在收敛速度和RMSE方面优于静态和单代理基线，表明其在地理自适应参数学习方面的潜力。

Abstract: Sub-grid parameterisations in climate models are traditionally static and
tuned offline, limiting adaptability to evolving states. This work introduces
FedRAIN-Lite, a federated reinforcement learning (FedRL) framework that mirrors
the spatial decomposition used in general circulation models (GCMs) by
assigning agents to latitude bands, enabling local parameter learning with
periodic global aggregation. Using a hierarchy of simplified energy-balance
climate models, from a single-agent baseline (ebm-v1) to multi-agent ensemble
(ebm-v2) and GCM-like (ebm-v3) setups, we benchmark three RL algorithms under
different FedRL configurations. Results show that Deep Deterministic Policy
Gradient (DDPG) consistently outperforms both static and single-agent
baselines, with faster convergence and lower area-weighted RMSE in tropical and
mid-latitude zones across both ebm-v2 and ebm-v3 setups. DDPG's ability to
transfer across hyperparameters and low computational cost make it well-suited
for geographically adaptive parameter learning. This capability offers a
scalable pathway towards high-complexity GCMs and provides a prototype for
physically aligned, online-learning climate models that can evolve with a
changing climate. Code accessible at
https://github.com/p3jitnath/climate-rl-fedrl.

</details>


### [191] [Multi-view Graph Condensation via Tensor Decomposition](https://arxiv.org/abs/2508.14330)
*Nícolas Roque dos Santos,Dawon Ahn,Diego Minatel,Alneu de Andrade Lopes,Evangelos E. Papalexakis*

Main category: cs.LG

TL;DR: GCTD利用张量分解技术，通过学习一个小的、信息丰富的合成图来解决图神经网络（GNN）在大规模图上的训练挑战，该方法比现有的图凝聚方法更具可解释性且计算效率更高。


<details>
  <summary>Details</summary>
Motivation: 当前的图凝聚方法依赖于计算密集型双层优化，并且无法保持合成节点和原始节点之间的映射，限制了模型决策的可解释性。研究张量分解技术在图凝聚中的应用，以提供一种更透明、资源消耗更少的方法。

Method: 提出了一种称为多视图图凝聚（GCTD）的新方法，该方法利用张量分解技术来合成信息量更小的图。

Result: GCTD有效减小了图的大小，同时保持了GNN的性能，在6个真实世界数据集上的实验表明，其中3个数据集的准确率提高了4.0%，并且在大图上与现有方法相比具有竞争力。

Conclusion: GCTD通过张量分解有效减小了图的大小，同时保持了GNN的性能，在6个数据集中有3个数据集的准确率提高了4.0%，并且在大图上与现有方法相比具有竞争力。

Abstract: Graph Neural Networks (GNNs) have demonstrated remarkable results in various
real-world applications, including drug discovery, object detection, social
media analysis, recommender systems, and text classification. In contrast to
their vast potential, training them on large-scale graphs presents significant
computational challenges due to the resources required for their storage and
processing. Graph Condensation has emerged as a promising solution to reduce
these demands by learning a synthetic compact graph that preserves the
essential information of the original one while maintaining the GNN's
predictive performance. Despite their efficacy, current graph condensation
approaches frequently rely on a computationally intensive bi-level
optimization. Moreover, they fail to maintain a mapping between synthetic and
original nodes, limiting the interpretability of the model's decisions. In this
sense, a wide range of decomposition techniques have been applied to learn
linear or multi-linear functions from graph data, offering a more transparent
and less resource-intensive alternative. However, their applicability to graph
condensation remains unexplored. This paper addresses this gap and proposes a
novel method called Multi-view Graph Condensation via Tensor Decomposition
(GCTD) to investigate to what extent such techniques can synthesize an
informative smaller graph and achieve comparable downstream task performance.
Extensive experiments on six real-world datasets demonstrate that GCTD
effectively reduces graph size while preserving GNN performance, achieving up
to a 4.0\ improvement in accuracy on three out of six datasets and competitive
performance on large graphs compared to existing approaches. Our code is
available at https://anonymous.4open.science/r/gctd-345A.

</details>


### [192] [DuPO: Enabling Reliable LLM Self-Verification via Dual Preference Optimization](https://arxiv.org/abs/2508.14460)
*Shuaijie She,Yu Bao,Yu Lu,Lu Xu,Tao Li,Wenhao Zhu,Shujian Huang,Shanbo Cheng,Lu Lu,Yuxuan Wang*

Main category: cs.LG

TL;DR: DuPO是一种基于双重学习的偏好优化框架，通过广义对偶生成无标注反馈，解决了RLVR和传统双重学习的局限性，并能处理不可逆任务，在翻译、数学推理和重排任务上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 解决RLVR依赖昂贵标签和适用性受限于可验证任务的局限性，以及传统对偶学习仅限于严格对偶任务对（如翻译和反向翻译）的限制。

Method: DuPO通过广义对偶生成无标注反馈，将原始任务输入分解为已知和未知部分，并构建对偶任务来重建未知部分，利用重建质量作为自我监督奖励来优化原始任务。

Result: DuPO在多种任务上实现了显著的提升：在756个方向上平均提高了2.13 COMET的翻译质量，在三个具有挑战性的基准上平均提高了6.4个百分点的数学推理准确率，并在推理时作为重排器提升了9.3个百分点的性能。

Conclusion: DuPO是一个可扩展、通用且无需标注的语言模型优化范式。

Abstract: We present DuPO, a dual learning-based preference optimization framework that
generates annotation-free feedback via a generalized duality. DuPO addresses
two key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s
reliance on costly labels and applicability restricted to verifiable tasks, and
traditional dual learning's restriction to strictly dual task pairs (e.g.,
translation and back-translation). Specifically, DuPO decomposes a primal
task's input into known and unknown components, then constructs its dual task
to reconstruct the unknown part using the primal output and known information
(e.g., reversing math solutions to recover hidden variables), broadening
applicability to non-invertible tasks. The quality of this reconstruction
serves as a self-supervised reward to optimize the primal task, synergizing
with LLMs' ability to instantiate both tasks via a single model. Empirically,
DuPO achieves substantial gains across diverse tasks: it enhances the average
translation quality by 2.13 COMET over 756 directions, boosts the mathematical
reasoning accuracy by an average of 6.4 points on three challenge benchmarks,
and enhances performance by 9.3 points as an inference-time reranker (trading
computation for accuracy). These results position DuPO as a scalable, general,
and annotation-free paradigm for LLM optimization.

</details>


### [193] [NeRC: Neural Ranging Correction through Differentiable Moving Horizon Location Estimation](https://arxiv.org/abs/2508.14336)
*Xu Weng,K. V. Ling,Haochen Liu,Bingheng Wang,Kun Cao*

Main category: cs.LG

TL;DR: 提出了一种名为NeRC的端到端框架，通过利用可微分MHE和EDF成本图，无需测距误差标注即可在城市环境的GNSS定位中提高准确性，并能在移动设备上实时运行。


<details>
  <summary>Details</summary>
Motivation: 城市环境中GNSS定位的准确性受限于信号传播和硬件质量导致的测距误差，而现有方法需要繁琐的测距误差标注。

Method: 提出了一种端到端的神经范围校正（NeRC）框架，使用可微分移动视线定位（MHE）处理测量数据并进行反向传播训练。此外，还提出了一种使用欧氏距离场（EDF）成本图的新训练范式，以减少对标记位置的需求。

Result: 在公开基准和收集的数据集上进行了评估，NeRC在定位准确性方面表现出显著的改进，并在边缘设备上验证了其实时性能。

Conclusion: GNSS定位的准确性通过NeRC框架得到了显著提升，该框架能够在边缘设备上进行实时部署。

Abstract: GNSS localization using everyday mobile devices is challenging in urban
environments, as ranging errors caused by the complex propagation of satellite
signals and low-quality onboard GNSS hardware are blamed for undermining
positioning accuracy. Researchers have pinned their hopes on data-driven
methods to regress such ranging errors from raw measurements. However, the
grueling annotation of ranging errors impedes their pace. This paper presents a
robust end-to-end Neural Ranging Correction (NeRC) framework, where
localization-related metrics serve as the task objective for training the
neural modules. Instead of seeking impractical ranging error labels, we train
the neural network using ground-truth locations that are relatively easy to
obtain. This functionality is supported by differentiable moving horizon
location estimation (MHE) that handles a horizon of measurements for
positioning and backpropagates the gradients for training. Even better, as a
blessing of end-to-end learning, we propose a new training paradigm using
Euclidean Distance Field (EDF) cost maps, which alleviates the demands on
labeled locations. We evaluate the proposed NeRC on public benchmarks and our
collected datasets, demonstrating its distinguished improvement in positioning
accuracy. We also deploy NeRC on the edge to verify its real-time performance
for mobile devices.

</details>


### [194] [On the Interplay between Graph Structure and Learning Algorithms in Graph Neural Networks](https://arxiv.org/abs/2508.14338)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: 本研究通过理论分析和实证研究，深入探讨了图结构、GNN 模型和学习算法之间的相互作用，尤其关注了在存在噪声的泛化场景下，图结构如何影响 GNN 的泛化性能。研究结果表明，图的连接特性（如规则性或幂律分布）会显著影响 SGD 和 Ridge 回归等算法的表现。此外，通过分析多层 GNN，研究还揭示了其在泛化过程中的非各向异性效应，这为理解和解决 GNN 的过平滑问题提供了新的视角。最终，本研究强调了这三者之间的耦合关系，并为实际应用中的 GNN 模型选择和设计提供了有价值的参考。


<details>
  <summary>Details</summary>
Motivation: 现有 GNN 理论研究主要关注插值模型（无噪声）下学习算法的收敛速度，但未能充分揭示学习动态与图结构之间的联系。本研究旨在弥合这一差距，通过在泛化模型（有噪声）中研究 GNN 学习算法的过量风险（泛化性能），来探究图结构对 GNN 泛化性能的影响。

Method: 本研究将学习理论中的常规设置扩展到 GNN 领域，并研究了随机梯度下降 (SGD) 和 Ridge 回归等学习算法的过量风险（泛化性能）。通过谱图理论将这些风险剖面与图结构联系起来，并分析了不同图结构（规则与幂律）对算法性能的影响。此外，还将分析扩展到多层线性 GNN，揭示了非各向异性效应对过量风险剖面的影响，并从学习算法的角度深入探讨了 GNN 的过平滑问题。

Result: 本研究推导了 GNN 中 SGD 和 Ridge 回归的过量风险剖面，并通过谱图理论将其与图结构联系起来。比较分析表明，不同图结构（规则与幂律）对这些算法的性能有不同影响。此外，研究还发现多层线性 GNN 中的非各向异性效应对过量风险剖面有增强作用，并从学习算法的角度揭示了 GNN 的过平滑问题。

Conclusion: 本研究揭示了图结构、GNN 和学习算法之间的耦合关系，为 GNN 算法的设计和选择提供了实践指导。

Abstract: This paper studies the interplay between learning algorithms and graph
structure for graph neural networks (GNNs). Existing theoretical studies on the
learning dynamics of GNNs primarily focus on the convergence rates of learning
algorithms under the interpolation regime (noise-free) and offer only a crude
connection between these dynamics and the actual graph structure (e.g., maximum
degree). This paper aims to bridge this gap by investigating the excessive risk
(generalization performance) of learning algorithms in GNNs within the
generalization regime (with noise). Specifically, we extend the conventional
settings from the learning theory literature to the context of GNNs and examine
how graph structure influences the performance of learning algorithms such as
stochastic gradient descent (SGD) and Ridge regression. Our study makes several
key contributions toward understanding the interplay between graph structure
and learning in GNNs. First, we derive the excess risk profiles of SGD and
Ridge regression in GNNs and connect these profiles to the graph structure
through spectral graph theory. With this established framework, we further
explore how different graph structures (regular vs. power-law) impact the
performance of these algorithms through comparative analysis. Additionally, we
extend our analysis to multi-layer linear GNNs, revealing an increasing
non-isotropic effect on the excess risk profile, thereby offering new insights
into the over-smoothing issue in GNNs from the perspective of learning
algorithms. Our empirical results align with our theoretical predictions,
\emph{collectively showcasing a coupling relation among graph structure, GNNs
and learning algorithms, and providing insights on GNN algorithm design and
selection in practice.}

</details>


### [195] [A Comparative Evaluation of Teacher-Guided Reinforcement Learning Techniques for Autonomous Cyber Operations](https://arxiv.org/abs/2508.14340)
*Konur Tholl,Mariam El Mezouar,Ranwa Al Mallah*

Main category: cs.LG

TL;DR: Teacher guidance improves ACO training by speeding up learning and enhancing early performance.


<details>
  <summary>Details</summary>
Motivation: Existing Autonomous Cyber Operations (ACO) applications require agents to learn from scratch, leading to slow convergence and poor early-stage performance. Teacher-guided techniques have not yet been applied to ACO.

Method: The study implemented four distinct teacher-guided techniques in the simulated CybORG environment and conducted a comparative evaluation.

Result: Teacher integration significantly improved training efficiency, showing better early-stage policy performance and faster convergence compared to learning from scratch.

Conclusion: Teacher integration can significantly improve training efficiency in terms of early policy performance and convergence speed, highlighting its potential benefits for autonomous cybersecurity.

Abstract: Autonomous Cyber Operations (ACO) rely on Reinforcement Learning (RL) to
train agents to make effective decisions in the cybersecurity domain. However,
existing ACO applications require agents to learn from scratch, leading to slow
convergence and poor early-stage performance. While teacher-guided techniques
have demonstrated promise in other domains, they have not yet been applied to
ACO. In this study, we implement four distinct teacher-guided techniques in the
simulated CybORG environment and conduct a comparative evaluation. Our results
demonstrate that teacher integration can significantly improve training
efficiency in terms of early policy performance and convergence speed,
highlighting its potential benefits for autonomous cybersecurity.

</details>


### [196] [A Non-Asymptotic Convergent Analysis for Scored-Based Graph Generative Model via a System of Stochastic Differential Equations](https://arxiv.org/abs/2508.14351)
*Junwei Su,Chuan Wu*

Main category: cs.LG

TL;DR: SGGMs在AI for Science领域潜力巨大，但收敛性分析不足。本文首次提供了SGGMs的非渐近收敛性分析，发现了影响收敛的图拓扑因素，并给出了超参数选择和模型改进的建议。实验验证了理论结果。


<details>
  <summary>Details</summary>
Motivation: 现有的评分基础图生成模型（SGGMs）在药物发现和蛋白质合成等领域表现出色，但其理论行为，特别是收敛性，尚未得到充分研究。由于SGGMs涉及耦合的随机微分方程（SDE）系统，与仅涉及单一SDE的普通SGM不同，现有的收敛性分析方法不适用于SGGMs。

Method: 本文提出了首个针对SGGMs（评分基础图生成模型）的非渐近收敛性分析，并针对三种图生成范式（固定图结构特征生成、固定节点特征图结构生成、联合生成）进行了收敛性界限（生成误差风险）的分析。

Result: 分析揭示了影响SGGMs收敛性界限的独特因素（如图的拓扑属性），为超参数选择（如采样步数和扩散长度）提供了理论见解，并提出使用归一化等技术来改善收敛性。通过在合成图模型上进行的受控实证研究，结果与理论预测一致。

Conclusion: 该研究首次对SGGMs进行了非渐近收敛性分析，发现了影响收敛性的特定于SGGMs的因素，如图结构的拓扑属性，并为超参数选择和模型改进提供了理论指导，实验结果验证了理论预测。

Abstract: Score-based graph generative models (SGGMs) have proven effective in critical
applications such as drug discovery and protein synthesis. However, their
theoretical behavior, particularly regarding convergence, remains
underexplored. Unlike common score-based generative models (SGMs), which are
governed by a single stochastic differential equation (SDE), SGGMs involve a
system of coupled SDEs. In SGGMs, the graph structure and node features are
governed by separate but interdependent SDEs. This distinction makes existing
convergence analyses from SGMs inapplicable for SGGMs. In this work, we present
the first non-asymptotic convergence analysis for SGGMs, focusing on the
convergence bound (the risk of generative error) across three key graph
generation paradigms: (1) feature generation with a fixed graph structure, (2)
graph structure generation with fixed node features, and (3) joint generation
of both graph structure and node features. Our analysis reveals several unique
factors specific to SGGMs (e.g., the topological properties of the graph
structure) which affect the convergence bound. Additionally, we offer
theoretical insights into the selection of hyperparameters (e.g., sampling
steps and diffusion length) and advocate for techniques like normalization to
improve convergence. To validate our theoretical findings, we conduct a
controlled empirical study using synthetic graph models, and the results align
with our theoretical predictions. This work deepens the theoretical
understanding of SGGMs, demonstrates their applicability in critical domains,
and provides practical guidance for designing effective models.

</details>


### [197] [SBGD: Improving Graph Diffusion Generative Model via Stochastic Block Diffusion](https://arxiv.org/abs/2508.14352)
*Junwei Su,Shan Wu*

Main category: cs.LG

TL;DR: SBGD是一种新的图扩散生成模型，通过将图映射到块图空间来解决可扩展性和尺寸泛化问题，实现了显著的内存改进和更好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有的图扩散生成模型（GDGM）在可扩展性和尺寸泛化方面面临挑战。它们通常在整个图空间中操作，需要将整个图存储在内存中，这限制了它们在大规模真实世界图上的应用。此外，GDGM在生成与训练数据尺寸不同的图时泛化能力有限。

Method: SBGD模型将图表示映射到块图空间，利用基于真实世界图模式的结构先验来降低内存复杂度，从而实现向大型图的可扩展性。这种块表示也通过捕捉基本的图结构来提高尺寸泛化能力。

Result: SBGD模型实现了显著的内存改进（高达6倍），同时保持了与最先进方法相当或更优的图生成性能。实验还表明，SBGD在泛化到未见的图尺寸方面表现更佳。

Conclusion: SBGD模型成功解决了图扩散生成模型（GDGM）在可扩展性和尺寸泛化方面存在的挑战。通过将图表示映射到块图空间，SBGD显著降低了内存需求，实现了向大型图的可扩展性，并提高了对不同尺寸图的泛化能力。实验证明，SBGD在内存使用方面有高达6倍的提升，同时保持了与最先进方法相当甚至更优的图生成性能，并且在泛化到未见过的图尺寸方面表现更佳。SBGD的意义不仅在于其作为一种可扩展且有效的GDGM，更在于它体现了生成模型中模块化的原则，为通过分解复杂任务为更易于管理的部分来探索生成模型开辟了新途径。

Abstract: Graph diffusion generative models (GDGMs) have emerged as powerful tools for
generating high-quality graphs. However, their broader adoption faces
challenges in \emph{scalability and size generalization}. GDGMs struggle to
scale to large graphs due to their high memory requirements, as they typically
operate in the full graph space, requiring the entire graph to be stored in
memory during training and inference. This constraint limits their feasibility
for large-scale real-world graphs. GDGMs also exhibit poor size generalization,
with limited ability to generate graphs of sizes different from those in the
training data, restricting their adaptability across diverse applications. To
address these challenges, we propose the stochastic block graph diffusion
(SBGD) model, which refines graph representations into a block graph space.
This space incorporates structural priors based on real-world graph patterns,
significantly reducing memory complexity and enabling scalability to large
graphs. The block representation also improves size generalization by capturing
fundamental graph structures. Empirical results show that SBGD achieves
significant memory improvements (up to 6$\times$) while maintaining comparable
or even superior graph generation performance relative to state-of-the-art
methods. Furthermore, experiments demonstrate that SBGD better generalizes to
unseen graph sizes. The significance of SBGD extends beyond being a scalable
and effective GDGM; it also exemplifies the principle of modularization in
generative modeling, offering a new avenue for exploring generative models by
decomposing complex tasks into more manageable components.

</details>


### [198] [Organ-Agents: Virtual Human Physiology Simulator via LLMs](https://arxiv.org/abs/2508.14357)
*Rihao Chang,He Jiao,Weizhi Nie,Honglin Guo,Keliang Xie,Zhenhua Wu,Lina Zhao,Yunpeng Bai,Yongtao Ma,Lanjun Wang,Yuting Su,Xi Gao,Weijie Wang,Nicu Sebe,Bruno Lepri,Bingwei Sun*

Main category: cs.LG

TL;DR: Organ-Agents是一个利用大型语言模型模拟人类生理系统的框架，在模拟准确性、鲁棒性和生理合理性方面表现出色，并可用于治疗模拟和预警任务。


<details>
  <summary>Details</summary>
Motivation: 旨在利用大型语言模型的最新进展来模拟复杂生理系统，以应对危重监护中的挑战。

Method: 该研究引入了一个名为Organ-Agents的多代理框架，利用大型语言模型（LLMs）驱动的代理来模拟人类生理系统。每个代理模拟一个特定的系统，如心血管、肾脏或免疫系统。通过在系统特定的时间序列数据上进行监督微调，然后使用动态参考选择和错误纠正进行强化引导的协调来训练模型。

Result: Organ-Agents在模拟人类生理系统方面表现出色，对4,509名患者的模拟准确性高，系统均方误差（MSE）低于0.16，并且在不同严重程度的患者中表现出鲁棒性。该框架还能准确重现关键的多系统事件，并得到了重症监护医生的认可。此外，Organ-Agents在支持反事实模拟和下游早期预警任务方面也显示出潜力。

Conclusion: Organ-Agents是一个可信、可解释且可泛化的数字孪生，可用于精准诊断、治疗模拟和假设检验，在模拟危重监护中的生理系统方面展示了高准确性、鲁棒性和生理学上的合理性。

Abstract: Recent advances in large language models (LLMs) have enabled new
possibilities in simulating complex physiological systems. We introduce
Organ-Agents, a multi-agent framework that simulates human physiology via
LLM-driven agents. Each Simulator models a specific system (e.g.,
cardiovascular, renal, immune). Training consists of supervised fine-tuning on
system-specific time-series data, followed by reinforcement-guided coordination
using dynamic reference selection and error correction. We curated data from
7,134 sepsis patients and 7,895 controls, generating high-resolution
trajectories across 9 systems and 125 variables. Organ-Agents achieved high
simulation accuracy on 4,509 held-out patients, with per-system MSEs <0.16 and
robustness across SOFA-based severity strata. External validation on 22,689 ICU
patients from two hospitals showed moderate degradation under distribution
shifts with stable simulation. Organ-Agents faithfully reproduces critical
multi-system events (e.g., hypotension, hyperlactatemia, hypoxemia) with
coherent timing and phase progression. Evaluation by 15 critical care
physicians confirmed realism and physiological plausibility (mean Likert
ratings 3.9 and 3.7). Organ-Agents also enables counterfactual simulations
under alternative sepsis treatment strategies, generating trajectories and
APACHE II scores aligned with matched real-world patients. In downstream early
warning tasks, classifiers trained on synthetic data showed minimal AUROC drops
(<0.04), indicating preserved decision-relevant patterns. These results
position Organ-Agents as a credible, interpretable, and generalizable digital
twin for precision diagnosis, treatment simulation, and hypothesis testing in
critical care.

</details>


### [199] [Disentanglement in T-space for Faster and Distributed Training of Diffusion Models with Fewer Latent-states](https://arxiv.org/abs/2508.14413)
*Samarth Gupta,Raghudeep Gadde,Rui Chen,Aleix M. Martinez*

Main category: cs.LG

TL;DR: 研究质疑了扩散模型需要大量潜在状态的假设，提出了一种使用较少潜在状态（甚至单个状态）的扩散模型，并实现了更快的收敛速度和高质量的样本生成。


<details>
  <summary>Details</summary>
Motivation: 质疑了扩散模型需要大量潜在状态或时间步长才能使反向生成过程接近高斯分布的基本假设。

Method: 通过仔细选择噪声调度，使用较少数量的潜在状态（例如 T≈32）训练的扩散模型可以匹配使用大量潜在状态（例如 T≈1000）训练的模型的性能。此外，研究将潜在状态数量的限制推至单个潜在状态，实现了T空间中的完全解耦，并通过组合多个独立训练的单潜在状态模型来生成高质量样本。

Result: 所提出的解耦模型在多个指标上实现了4-6倍的加速收敛，并在两个不同的数据集上生成高质量的样本。

Conclusion: 提出的模型在多个指标上实现了4-6倍的加速收敛，并且可以在两个不同的数据集上生成高质量的样本。

Abstract: We challenge a fundamental assumption of diffusion models, namely, that a
large number of latent-states or time-steps is required for training so that
the reverse generative process is close to a Gaussian. We first show that with
careful selection of a noise schedule, diffusion models trained over a small
number of latent states (i.e. $T \sim 32$) match the performance of models
trained over a much large number of latent states ($T \sim 1,000$). Second, we
push this limit (on the minimum number of latent states required) to a single
latent-state, which we refer to as complete disentanglement in T-space. We show
that high quality samples can be easily generated by the disentangled model
obtained by combining several independently trained single latent-state models.
We provide extensive experiments to show that the proposed disentangled model
provides 4-6$\times$ faster convergence measured across a variety of metrics on
two different datasets.

</details>


### [200] [Personalized Counterfactual Framework: Generating Potential Outcomes from Wearable Data](https://arxiv.org/abs/2508.14432)
*Ajan Subramanian,Amir M. Rahmani*

Main category: cs.LG

TL;DR: 本篇论文提出了一个框架，可以从多元可穿戴数据中学习个性化的反事实模型，从而能够探索“假设”情景，以了解生活方式选择对个体可能产生的影响。


<details>
  <summary>Details</summary>
Motivation: 可穿戴传感器数据为个性化健康监测提供了机会，但要从其复杂、纵向的数据流中提取可操作的见解具有挑战性。

Method: 本研究首先通过多模态相似性分析，利用相似患者的数据扩充个体数据集。然后，我们使用时间PC（Peter-Clark）算法的适应版本来发现预测关系，为变量在t-1时刻如何影响t时刻的生理变化建模。在这些发现的关系上训练梯度提升机，以量化个体特异性效应。这些模型驱动一个反事实引擎，用于预测假设干预（例如，活动或睡眠变化）下的生理轨迹。

Result: 评估显示了合理的预测准确性（例如，平均心率MAE为4.71 bpm）和高的反事实合理性（中位数为0.9643）。这些干预措施凸显了在响应假设生活方式改变方面显著的个体间差异，显示了该框架在个性化见解方面的潜力。

Conclusion: 该框架为探索个性化健康动态和生成关于个体对生活方式改变的反应的假设提供了工具。

Abstract: Wearable sensor data offer opportunities for personalized health monitoring,
yet deriving actionable insights from their complex, longitudinal data streams
is challenging. This paper introduces a framework to learn personalized
counterfactual models from multivariate wearable data. This enables exploring
what-if scenarios to understand potential individual-specific outcomes of
lifestyle choices. Our approach first augments individual datasets with data
from similar patients via multi-modal similarity analysis. We then use a
temporal PC (Peter-Clark) algorithm adaptation to discover predictive
relationships, modeling how variables at time t-1 influence physiological
changes at time t. Gradient Boosting Machines are trained on these discovered
relationships to quantify individual-specific effects. These models drive a
counterfactual engine projecting physiological trajectories under hypothetical
interventions (e.g., activity or sleep changes). We evaluate the framework via
one-step-ahead predictive validation and by assessing the plausibility and
impact of interventions. Evaluation showed reasonable predictive accuracy
(e.g., mean heart rate MAE 4.71 bpm) and high counterfactual plausibility
(median 0.9643). Crucially, these interventions highlighted significant
inter-individual variability in response to hypothetical lifestyle changes,
showing the framework's potential for personalized insights. This work provides
a tool to explore personalized health dynamics and generate hypotheses on
individual responses to lifestyle changes.

</details>


### [201] [Fast Symbolic Regression Benchmarking](https://arxiv.org/abs/2508.14481)
*Viktor Martinek*

Main category: cs.LG

TL;DR: 改进了符号回归基准测试方法，提高了重发现率并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于强调单一表达式形式的恢复，或仅依赖计算机代数系统进行评估，并且在找到目标表达式后仍继续搜索。

Method: 引入了可接受表达式的列表和提前终止的回调机制，并使用SRSD基准问题测试了SymbolicRegression.jl和TiSR两个包。

Result: SymbolicRegression.jl的重发现率从26.7%提高到44.7%，计算成本降低41.2%。TiSR的重发现率为69.4%，计算成本节省63%。

Conclusion: 该研究通过引入可接受表达式的列表和提前终止的回调机制，改进了符号回归基准测试的评估方法，提高了重发现率并降低了计算成本。

Abstract: Symbolic regression (SR) uncovers mathematical models from data. Several
benchmarks have been proposed to compare the performance of SR algorithms.
However, existing ground-truth rediscovery benchmarks overemphasize the
recovery of "the one" expression form or rely solely on computer algebra
systems (such as SymPy) to assess success. Furthermore, existing benchmarks
continue the expression search even after its discovery. We improve upon these
issues by introducing curated lists of acceptable expressions, and a callback
mechanism for early termination. As a starting point, we use the symbolic
regression for scientific discovery (SRSD) benchmark problems proposed by
Yoshitomo et al., and benchmark the two SR packages SymbolicRegression.jl and
TiSR. The new benchmarking method increases the rediscovery rate of
SymbolicRegression.jl from 26.7%, as reported by Yoshitomo et at., to 44.7%.
Performing the benchmark takes 41.2% less computational expense. TiSR's
rediscovery rate is 69.4%, while performing the benchmark saves 63% time.

</details>


### [202] [On the notion of missingness for path attribution explainability methods in medical settings: Guiding the selection of medically meaningful baselines](https://arxiv.org/abs/2508.14482)
*Alexander Geiger,Lars Wagner,Daniel Rueckert,Dirk Wilhelm,Alissa Jell*

Main category: cs.LG

TL;DR: Deep learning explainability is hard in medicine. Current methods use bad baselines. We propose using counterfactuals as baselines, which are better representations of 'missingness' in medical data. Our method works well on medical datasets.


<details>
  <summary>Details</summary>
Motivation: The explainability of deep learning models remains a significant challenge, particularly in the medical domain where interpretable outputs are critical for clinical trust and transparency. Path attribution methods such as Integrated Gradients rely on a baseline input representing the absence of relevant features ("missingness"). Commonly used baselines, such as all-zero inputs, are often semantically meaningless, especially in medical contexts where missingness can itself be informative. While alternative baseline choices have been explored, existing methods lack a principled approach to dynamically select baselines tailored to each input.

Method: We use a Variational Autoencoder to generate counterfactual baselines, though our concept is generative-model-agnostic and can be applied with any suitable counterfactual method.

Result: Counterfactual baselines yield more faithful and medically relevant attributions compared to standard baseline choices.

Conclusion: In this work, we examine the notion of missingness in the medical setting, analyze its implications for baseline selection, and introduce a counterfactual-guided approach to address the limitations of conventional baselines. We argue that a clinically normal but input-close counterfactual represents a more accurate representation of a meaningful absence of features in medical data. We evaluate the approach on three distinct medical data sets and empirically demonstrate that counterfactual baselines yield more faithful and medically relevant attributions compared to standard baseline choices.

Abstract: The explainability of deep learning models remains a significant challenge,
particularly in the medical domain where interpretable outputs are critical for
clinical trust and transparency. Path attribution methods such as Integrated
Gradients rely on a baseline input representing the absence of relevant
features ("missingness"). Commonly used baselines, such as all-zero inputs, are
often semantically meaningless, especially in medical contexts where
missingness can itself be informative. While alternative baseline choices have
been explored, existing methods lack a principled approach to dynamically
select baselines tailored to each input. In this work, we examine the notion of
missingness in the medical setting, analyze its implications for baseline
selection, and introduce a counterfactual-guided approach to address the
limitations of conventional baselines. We argue that a clinically normal but
input-close counterfactual represents a more accurate representation of a
meaningful absence of features in medical data. To implement this, we use a
Variational Autoencoder to generate counterfactual baselines, though our
concept is generative-model-agnostic and can be applied with any suitable
counterfactual method. We evaluate the approach on three distinct medical data
sets and empirically demonstrate that counterfactual baselines yield more
faithful and medically relevant attributions compared to standard baseline
choices.

</details>


### [203] [Semantic Energy: Detecting LLM Hallucination Beyond Entropy](https://arxiv.org/abs/2508.14496)
*Huan Ma,Jiadong Pan,Jing Liu,Yan Chen,Joey Tianyi Zhou,Guangyu Wang,Qinghua Hu,Hua Wu,Changqing Zhang,Haifeng Wang*

Main category: cs.LG

TL;DR: 提出 Semantic Energy 框架，改进 LLMs 的不确定性估计和幻觉检测。


<details>
  <summary>Details</summary>
Motivation: 现有的语义熵方法依赖于后 softmax 概率，未能捕捉模型的不确定性，在某些场景下效果不佳。

Method: 提出了一种名为 Semantic Energy 的新颖不确定性估计框架，该框架利用 LLMs 的内在置信度，直接作用于倒数第二层的 logits。

Result: 实验结果表明，Semantic Energy 在幻觉检测和不确定性估计方面显著优于现有方法，为下游应用提供了更可靠的信号。

Conclusion: Semantic Energy 框架通过直接作用于倒数第二层的 logits 并结合语义聚类和受玻尔兹曼启发的能量分布，能更好地捕捉不确定性，解决了现有语义熵方法在某些场景下失效的问题。

Abstract: Large Language Models (LLMs) are being increasingly deployed in real-world
applications, but they remain susceptible to hallucinations, which produce
fluent yet incorrect responses and lead to erroneous decision-making.
Uncertainty estimation is a feasible approach to detect such hallucinations.
For example, semantic entropy estimates uncertainty by considering the semantic
diversity across multiple sampled responses, thus identifying hallucinations.
However, semantic entropy relies on post-softmax probabilities and fails to
capture the model's inherent uncertainty, causing it to be ineffective in
certain scenarios. To address this issue, we introduce Semantic Energy, a novel
uncertainty estimation framework that leverages the inherent confidence of LLMs
by operating directly on logits of penultimate layer. By combining semantic
clustering with a Boltzmann-inspired energy distribution, our method better
captures uncertainty in cases where semantic entropy fails. Experiments across
multiple benchmarks show that Semantic Energy significantly improves
hallucination detection and uncertainty estimation, offering more reliable
signals for downstream applications such as hallucination detection.

</details>


### [204] [Exact Shapley Attributions in Quadratic-time for FANOVA Gaussian Processes](https://arxiv.org/abs/2508.14499)
*Majid Mohammadi,Krikamol Muandet,Ilaria Tiddi,Annette Ten Teije,Siu Lun Chau*

Main category: cs.LG

TL;DR: 精确计算Shapley值成本高，尤其是在GP等概率模型中。本研究提出了一种在二次时间内为FANOVA GP计算精确Shapley值的方法，能够处理局部和全局解释，并考虑不确定性，提升了可解释AI的能力。


<details>
  <summary>Details</summary>
Motivation: 精确计算Shapley值的时间复杂度随特征数量呈指数增长，限制了其在机器学习中的实际应用。特别是在高斯过程（GP）等概率模型中，由于输出是随机变量，计算Shapley值需要额外的计算来处理高阶矩，挑战更大。

Method: 本研究提出了一种利用Möbius表示和受牛顿恒等式启发的递归算法，在二次时间复杂度内计算FANOVA GP的Shapley值。具体来说，对于局部解释，定义了一个关于函数分量的随机合作博弈，计算精确的随机Shapley值；对于全局解释，引入了一个确定的、基于方差的价值函数，计算量化特征对模型整体敏感性的精确Shapley值。

Result: 本研究成功地证明了对于FANOVA GP，可以在二次时间复杂度内精确计算Shapley值。所提出的方法能够同时捕捉贡献的期望值和不确定性，并量化特征对模型整体敏感性的贡献，在经验研究中展示了其在提供更具可扩展性、符合公理且考虑不确定性的解释方面的优势。

Conclusion: 本研究为FANOVA GP这一重要的高斯过程（GP）模型提供了精确的Shapley值计算方法，实现了在二次时间复杂度内得到局部和全局解释。通过定义随机合作博弈和引入基于方差的价值函数，并利用Möbius表示和递归算法，有效解决了Shapley值计算的计算成本问题，并能同时考虑不确定性，从而提升了可解释AI在结构化概率模型中的应用价值。

Abstract: Shapley values are widely recognized as a principled method for attributing
importance to input features in machine learning. However, the exact
computation of Shapley values scales exponentially with the number of features,
severely limiting the practical application of this powerful approach. The
challenge is further compounded when the predictive model is probabilistic - as
in Gaussian processes (GPs) - where the outputs are random variables rather
than point estimates, necessitating additional computational effort in modeling
higher-order moments. In this work, we demonstrate that for an important class
of GPs known as FANOVA GP, which explicitly models all main effects and
interactions, *exact* Shapley attributions for both local and global
explanations can be computed in *quadratic time*. For local, instance-wise
explanations, we define a stochastic cooperative game over function components
and compute the exact stochastic Shapley value in quadratic time only,
capturing both the expected contribution and uncertainty. For global
explanations, we introduce a deterministic, variance-based value function and
compute exact Shapley values that quantify each feature's contribution to the
model's overall sensitivity. Our methods leverage a closed-form (stochastic)
M\"{o}bius representation of the FANOVA decomposition and introduce recursive
algorithms, inspired by Newton's identities, to efficiently compute the mean
and variance of Shapley values. Our work enhances the utility of explainable
AI, as demonstrated by empirical studies, by providing more scalable,
axiomatically sound, and uncertainty-aware explanations for predictions
generated by structured probabilistic models.

</details>


### [205] [Artificial Intelligence-Based Multiscale Temporal Modeling for Anomaly Detection in Cloud Services](https://arxiv.org/abs/2508.14503)
*Lian Lian,Yilin Li,Song Han,Renzi Meng,Sibo Wang,Ming Wang*

Main category: cs.LG

TL;DR: 一种新的基于Transformer的多尺度特征融合异常检测方法，在云服务异常检测任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 旨在解决云服务环境中异常检测在时序建模和尺度感知特征表示方面的局限性。

Method: 本研究提出了一种基于Transformer架构并集成多尺度特征感知的方法。首先，利用改进的Transformer模块进行高维监控数据的时序建模，通过自注意力机制捕捉长距离依赖和上下文语义。然后，引入多尺度特征构建路径，通过降采样和并行编码提取不同粒度的时序特征。设计了注意力加权融合模块，动态调整各尺度的贡献以增强模型在异常模式建模中的鲁棒性。在输入建模阶段，构建了标准化的多维时间序列，并使用位置编码增强模型的时间感知能力。

Result: 实验结果表明，所提出的方法在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持了强大的稳定性和检测性能，证明了其在复杂云环境中的优越能力。

Conclusion: 提出的基于Transformer的多尺度特征融合异常检测方法在云服务环境中表现出色，在精确率、召回率、AUC和F1分数等关键指标上优于主流基线模型，并在各种扰动条件下保持了良好的稳定性和检测性能。

Abstract: This study proposes an anomaly detection method based on the Transformer
architecture with integrated multiscale feature perception, aiming to address
the limitations of temporal modeling and scale-aware feature representation in
cloud service environments. The method first employs an improved Transformer
module to perform temporal modeling on high-dimensional monitoring data, using
a self-attention mechanism to capture long-range dependencies and contextual
semantics. Then, a multiscale feature construction path is introduced to
extract temporal features at different granularities through downsampling and
parallel encoding. An attention-weighted fusion module is designed to
dynamically adjust the contribution of each scale to the final decision,
enhancing the model's robustness in anomaly pattern modeling. In the input
modeling stage, standardized multidimensional time series are constructed,
covering core signals such as CPU utilization, memory usage, and task
scheduling states, while positional encoding is used to strengthen the model's
temporal awareness. A systematic experimental setup is designed to evaluate
performance, including comparative experiments and hyperparameter sensitivity
analysis, focusing on the impact of optimizers, learning rates, anomaly ratios,
and noise levels. Experimental results show that the proposed method
outperforms mainstream baseline models in key metrics, including precision,
recall, AUC, and F1-score, and maintains strong stability and detection
performance under various perturbation conditions, demonstrating its superior
capability in complex cloud environments.

</details>


### [206] [Great GATsBi: Hybrid, Multimodal, Trajectory Forecasting for Bicycles using Anticipation Mechanism](https://arxiv.org/abs/2508.14523)
*Kevin Riehl,Shaimaa K. El-Baklish,Anastasios Kouvelas,Michail A. Makridis*

Main category: cs.LG

TL;DR: A new framework called Great GATsBi improves bicycle trajectory prediction by combining physics and social models, outperforming existing methods and validated by real-world experiments.


<details>
  <summary>Details</summary>
Motivation: Accurate prediction of road user movement is crucial for applications like advanced driver assistance systems and autonomous driving, particularly for road safety. Bicycles, despite accounting for a significant portion of traffic accident fatalities, have received less attention than pedestrians and motorized vehicles in previous research.

Method: The framework combines physics-based modeling (for short-term prediction) and social-based modeling (using a graph attention network for long-term prediction), incorporating historical and anticipated trajectory data of surrounding traffic participants.

Result: The proposed ensemble of physics and social models outperforms state-of-the-art methods, with physics models excelling in short-term predictions and social models in long-term predictions. A controlled experiment demonstrated the framework's ability to forecast bicycle trajectories and model social interactions.

Conclusion: Great GATsBi, a hybrid, multimodal trajectory prediction framework for bicycles, shows state-of-the-art performance by integrating physics-based and social-based models. Controlled experiments validate its effectiveness in forecasting bicycle trajectories and modeling social interactions.

Abstract: Accurate prediction of road user movement is increasingly required by many
applications ranging from advanced driver assistance systems to autonomous
driving, and especially crucial for road safety. Even though most traffic
accident fatalities account to bicycles, they have received little attention,
as previous work focused mainly on pedestrians and motorized vehicles. In this
work, we present the Great GATsBi, a domain-knowledge-based, hybrid, multimodal
trajectory prediction framework for bicycles. The model incorporates both
physics-based modeling (inspired by motorized vehicles) and social-based
modeling (inspired by pedestrian movements) to explicitly account for the dual
nature of bicycle movement. The social interactions are modeled with a graph
attention network, and include decayed historical, but also anticipated, future
trajectory data of a bicycles neighborhood, following recent insights from
psychological and social studies. The results indicate that the proposed
ensemble of physics models -- performing well in the short-term predictions --
and social models -- performing well in the long-term predictions -- exceeds
state-of-the-art performance. We also conducted a controlled mass-cycling
experiment to demonstrate the framework's performance when forecasting bicycle
trajectories and modeling social interactions with road users.

</details>


### [207] [Beyond ReLU: Chebyshev-DQN for Enhanced Deep Q-Networks](https://arxiv.org/abs/2508.14536)
*Saman Yazdannik,Morteza Tayefi,Shamim Sanisales*

Main category: cs.LG

TL;DR: Chebyshev-DQN (Ch-DQN) 通过整合 Chebyshev 展开式基函数来改进 DQN 的特征表示，从而在 CartPole-v1 任务上实现了显著的性能提升，但需要仔细调整多项式次数以避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 深度 Q 网络 (DQN) 的性能在很大程度上取决于其底层神经网络逼近动作值函数的能力，而传统函数逼近器在表示复杂值函数方面可能存在困难。

Method: 提出了一种新颖的 Chebyshev-DQN (Ch-DQN) 架构，该架构将 Chebyshev 展开式基函数整合到 DQN 框架中，以创建更有效的特征表示。

Result: 与具有相似参数数量的标准 DQN 相比，具有适度多项式次数 (N=4) 的 Ch-DQN 在 CartPole-v1 基准测试中实现了显著更高的渐近性能，表现优于基线约 39%。然而，高次数 (N=8) 可能对学习产生不利影响。

Conclusion: Chebyshev-DQN (Ch-DQN) 整合了 Chebyshev 及其的 潜在应用，并通过实验证明了其在深度强化学习中的有效性，同时也指出了模型复杂性带来的挑战。

Abstract: The performance of Deep Q-Networks (DQN) is critically dependent on the
ability of its underlying neural network to accurately approximate the
action-value function. Standard function approximators, such as multi-layer
perceptrons, may struggle to efficiently represent the complex value landscapes
inherent in many reinforcement learning problems. This paper introduces a novel
architecture, the Chebyshev-DQN (Ch-DQN), which integrates a Chebyshev
polynomial basis into the DQN framework to create a more effective feature
representation. By leveraging the powerful function approximation properties of
Chebyshev polynomials, we hypothesize that the Ch-DQN can learn more
efficiently and achieve higher performance. We evaluate our proposed model on
the CartPole-v1 benchmark and compare it against a standard DQN with a
comparable number of parameters. Our results demonstrate that the Ch-DQN with a
moderate polynomial degree (N=4) achieves significantly better asymptotic
performance, outperforming the baseline by approximately 39\%. However, we also
find that the choice of polynomial degree is a critical hyperparameter, as a
high degree (N=8) can be detrimental to learning. This work validates the
potential of using orthogonal polynomial bases in deep reinforcement learning
while also highlighting the trade-offs involved in model complexity.

</details>


### [208] [Adaptively Robust LLM Inference Optimization under Prediction Uncertainty](https://arxiv.org/abs/2508.14544)
*Zixi Chen,Yinyu Ye,Zijie Zhou*

Main category: cs.LG

TL;DR: 针对 LLM 推理调度中的输出长度不确定性问题，提出了一种基于机器学习预测和自适应调整的 $\mathcal{A}_{\min}$ 算法，该算法在效率和鲁棒性方面表现优异。


<details>
  <summary>Details</summary>
Motivation: 为了优化大型语言模型（LLM）的推理调度，以最小化总延迟和降低能耗，尤其是在面对大量输入的请求时。主要挑战在于输出长度未知，而这会影响内存使用和处理时间。

Method: 提出了一种名为 $\mathcal{A}_{\min}$ 的自适应算法，该算法利用机器学习预测输出长度的区间（最小-最大范围），并动态调整估计值。设计了一个保守算法 $\mathcal{A}_{\max}$ 作为对比，它基于预测输出长度的上限进行调度。

Result: $	ext{A}_{\min}$ 算法被证明具有对数级别的竞争比，并通过数值模拟展示了其性能接近事后最优调度。

Conclusion: $	ext{A}_{\min}$ 算法通过动态调整预测的输出长度，在实践中表现接近事后优化，证明了其效率和鲁棒性。该算法仅依赖预测间隔的下界，提高了预测准确性。

Abstract: We study the problem of optimizing Large Language Model (LLM) inference
scheduling to minimize total latency. LLM inference is an online and multi-task
service process and also heavily energy consuming by which a pre-trained LLM
processes input requests and generates output tokens sequentially. Therefore,
it is vital to improve its scheduling efficiency and reduce the power
consumption while a great amount of prompt requests are arriving. A key
challenge in LLM inference scheduling is that while the prompt length is known
upon arrival, the output length, which critically impacts memory usage and
processing time, is unknown. To address this uncertainty, we propose algorithms
that leverage machine learning to predict output lengths, assuming the
prediction provides an interval classification (min-max range) for each
request.
  We first design a conservative algorithm, $\mathcal{A}_{\max}$, which
schedules requests based on the upper bound of predicted output lengths to
prevent memory overflow. However, this approach is overly conservative: as
prediction accuracy decreases, performance degrades significantly due to
potential overestimation. To overcome this limitation, we propose
$\mathcal{A}_{\min}$, an adaptive algorithm that initially treats the predicted
lower bound as the output length and dynamically refines this estimate during
inferencing. We prove that $\mathcal{A}_{\min}$ achieves a log-scale
competitive ratio. Through numerical simulations, we demonstrate that
$\mathcal{A}_{\min}$ often performs nearly as well as the hindsight scheduler,
highlighting both its efficiency and robustness in practical scenarios.
Moreover, $\mathcal{A}_{\min}$ relies solely on the lower bound of the
prediction interval--an advantageous design choice since upper bounds on output
length are typically more challenging to predict accurately.

</details>


### [209] [A Comprehensive Evaluation of the Sensitivity of Density-Ratio Estimation Based Fairness Measurement in Regression](https://arxiv.org/abs/2508.14576)
*Abdalwahab Almajed,Maryam Tabar,Peyman Najafirad*

Main category: cs.LG

TL;DR: 本研究发现，用于衡量回归公平性的密度比估计方法在选择不同的估计算法核心时，可能会产生显著不同的结果，甚至是不一致的。这表明该方法存在问题，需要进一步研究以提高其可靠性。


<details>
  <summary>Details</summary>
Motivation: 算法偏见在机器学习驱动的方法中普遍存在，这促使人们对测量和减轻机器学习领域的偏见进行了越来越多的研究。因此，以往的研究探讨了如何在回归中衡量公平性，这是一个复杂的问题。特别是，最近有研究提出将其表述为密度比估计问题，并依靠基于逻辑回归驱动的概率分类器的方法来解决。

Method: 本研究提出了一系列包含不同密度比估计核心的公平性测量方法，并详细研究了不同核心如何影响所达到的公平性水平。

Result: 实验结果表明，密度比估计核心的选择会显著影响公平性测量方法的输出，甚至可能导致不同算法的相对公平性结果不一致。

Conclusion: 该研究表明，基于密度比估计的回归公平性测量方法存在重大问题，需要进一步研究以提高其可靠性。

Abstract: The prevalence of algorithmic bias in Machine Learning (ML)-driven approaches
has inspired growing research on measuring and mitigating bias in the ML
domain. Accordingly, prior research studied how to measure fairness in
regression which is a complex problem. In particular, recent research proposed
to formulate it as a density-ratio estimation problem and relied on a Logistic
Regression-driven probabilistic classifier-based approach to solve it. However,
there are several other methods to estimate a density ratio, and to the best of
our knowledge, prior work did not study the sensitivity of such fairness
measurement methods to the choice of underlying density ratio estimation
algorithm. To fill this gap, this paper develops a set of fairness measurement
methods with various density-ratio estimation cores and thoroughly investigates
how different cores would affect the achieved level of fairness. Our
experimental results show that the choice of density-ratio estimation core
could significantly affect the outcome of fairness measurement method, and
even, generate inconsistent results with respect to the relative fairness of
various algorithms. These observations suggest major issues with density-ratio
estimation based fairness measurement in regression and a need for further
research to enhance their reliability.

</details>


### [210] [Measuring IIA Violations in Similarity Choices with Bayesian Models](https://arxiv.org/abs/2508.14615)
*Hugo Sales Corrêa,Suryanarayana Sankagiri,Daniel Ratton Figueiredo,Matthias Grossglauser*

Main category: cs.LG

TL;DR: 该研究发现了相似选择中的IIA违反，并提出了一种新的贝叶斯方法来衡量这种违反。研究结果表明，上下文效应是造成IIA违反的原因，并建议开发新的模型来处理这些效应。


<details>
  <summary>Details</summary>
Motivation: 传统的基于度量模型在相似选择中假设IIA，但IIA违反在相似选择设置中受到关注不足，因为目标依赖性使IIA测试复杂化。

Method: 提出了一种新的贝叶斯方法，包括一个基于后验预测检查（PPC）的框架，用于测试IIA并量化其违反程度。此外，还提出了一种新的PPC测试来检验群体同质性。

Result: 在两个数据集上都证实了显著的IIA违反，违反程度相当。研究还发现群体是同质的，表明IIA违反是由上下文效应（选择集内的交互）驱动的。

Conclusion: 该研究证明了在相似选择设置中存在不相关的替代品（IIA）的违反，并提出了一种新的贝叶斯方法来量化这种违反的程度。研究结果表明，IIA的违反是由上下文效应驱动的，并强调了开发考虑这些效应的新相似选择模型的必要性。

Abstract: Similarity choice data occur when humans make choices among alternatives
based on their similarity to a target, e.g., in the context of information
retrieval and in embedding learning settings. Classical metric-based models of
similarity choice assume independence of irrelevant alternatives (IIA), a
property that allows for a simpler formulation. While IIA violations have been
detected in many discrete choice settings, the similarity choice setting has
received scant attention. This is because the target-dependent nature of the
choice complicates IIA testing. We propose two statistical methods to test for
IIA: a classical goodness-of-fit test and a Bayesian counterpart based on the
framework of Posterior Predictive Checks (PPC). This Bayesian approach, our
main technical contribution, quantifies the degree of IIA violation beyond its
mere significance. We curate two datasets: one with choice sets designed to
elicit IIA violations, and another with randomly generated choice sets from the
same item universe. Our tests confirmed significant IIA violations on both
datasets, and notably, we find a comparable degree of violation between them.
Further, we devise a new PPC test for population homogeneity. Results show that
the population is indeed homogenous, suggesting that the IIA violations are
driven by context effects -- specifically, interactions within the choice sets.
These results highlight the need for new similarity choice models that account
for such context effects.

</details>


### [211] [A Fuzzy-Enhanced Explainable AI Framework for Flight Continuous Descent Operations Classification](https://arxiv.org/abs/2508.14618)
*Amin Noroozi,Sandaruwan K. Sethunge,Elham Norouzi,Phat T. Phan,Kavinda U. Waduge,Md. Arafatur Rahman*

Main category: cs.LG

TL;DR: 该研究提出了一种名为FEXAI的新方法，结合了模糊逻辑、机器学习和SHAP，以提高CDO性能的可解释性。该方法在分类准确率超过90%的同时，能识别出影响CDO的关键因素，并生成人类可读的规则，有助于航空领域的决策支持。


<details>
  <summary>Details</summary>
Motivation: 尽管CDO具有操作和环境效益，但影响其性能的因素研究有限，且现有方法缺乏航空领域所需的透明度和可解释性。

Method: 该研究收集了1094次航班的ADS-B数据，包含11个运行特征和18个天气特征。研究应用了机器学习模型和SHAP分析来对航班的CDO遵守水平进行分类和特征排序。随后，利用SHAP确定的三个最重要特征构建了一个基于模糊规则的分类器，以提取可解释的模糊规则。

Result: 所有模型实现了90%以上分类准确率，FEXAI提供了有意义的、人类可读的规则。研究表明，到达航线内的平均下降率、下降段数量和下降过程中的平均航向变化是CDO性能的最强预测因子。

Conclusion: 该研究提出了一个模糊增强的可解释人工智能（FEXAI）框架，该框架将模糊逻辑与机器学习和SHAP分析相结合，以对连续下降操作（CDO）的遵守情况进行分类，并识别影响CDO性能的关键因素。

Abstract: Continuous Descent Operations (CDO) involve smooth, idle-thrust descents that
avoid level-offs, reducing fuel burn, emissions, and noise while improving
efficiency and passenger comfort. Despite its operational and environmental
benefits, limited research has systematically examined the factors influencing
CDO performance. Moreover, many existing methods in related areas, such as
trajectory optimization, lack the transparency required in aviation, where
explainability is critical for safety and stakeholder trust. This study
addresses these gaps by proposing a Fuzzy-Enhanced Explainable AI (FEXAI)
framework that integrates fuzzy logic with machine learning and SHapley
Additive exPlanations (SHAP) analysis. For this purpose, a comprehensive
dataset of 29 features, including 11 operational and 18 weather-related
features, was collected from 1,094 flights using Automatic Dependent
Surveillance-Broadcast (ADS-B) data. Machine learning models and SHAP were then
applied to classify flights' CDO adherence levels and rank features by
importance. The three most influential features, as identified by SHAP scores,
were then used to construct a fuzzy rule-based classifier, enabling the
extraction of interpretable fuzzy rules. All models achieved classification
accuracies above 90%, with FEXAI providing meaningful, human-readable rules for
operational users. Results indicated that the average descent rate within the
arrival route, the number of descent segments, and the average change in
directional heading during descent were the strongest predictors of CDO
performance. The FEXAI method proposed in this study presents a novel pathway
for operational decision support and could be integrated into aviation tools to
enable real-time advisories that maintain CDO adherence under varying
operational conditions.

</details>


### [212] [Clinical semantics for lung cancer prediction](https://arxiv.org/abs/2508.14627)
*Luis H. John,Jan A. Kors,Jenna M. Reps,Peter R. Rijnbeek,Egill A. Fridgeirsson*

Main category: cs.LG

TL;DR: 通过将SNOMED医学术语层次结构映射到低维双曲空间并整合到深度学习模型中，可以提高肺癌发病率的预测。


<details>
  <summary>Details</summary>
Motivation: 现有的临床预测模型使用的特征忽略了临床概念之间的语义关系，本研究旨在通过将SNOMED医学术语层次结构映射到低维双曲空间来整合特定领域的语义信息，以提高肺癌发病率的预测。

Method: 使用回顾性队列，通过黎曼随机梯度下降法生成了SNOMED词汇的庞加莱嵌入。这些嵌入被整合到ResNet和Transformer模型中，并与使用随机初始化的欧氏嵌入的基线模型进行比较。

Result: 与使用随机初始化的欧氏嵌入的基线模型相比，整合预训练的庞加莱嵌入在辨别性能方面带来了适度且一致的改进。ResNet模型，特别是使用10维庞加莱嵌入的模型，显示出改进的校准性能，而Transformer模型在各种配置下保持了稳定的校准。

Conclusion: 将临床知识图谱嵌入双曲空间并整合到深度学习模型中，通过保留用于预测的临床术语的层次结构，可以改善肺癌发病率的预测。这种方法证明了一种结合数据驱动的特征提取和已建立的临床知识的可行方法。

Abstract: Background: Existing clinical prediction models often represent patient data
using features that ignore the semantic relationships between clinical
concepts. This study integrates domain-specific semantic information by mapping
the SNOMED medical term hierarchy into a low-dimensional hyperbolic space using
Poincar\'e embeddings, with the aim of improving lung cancer onset prediction.
  Methods: Using a retrospective cohort from the Optum EHR dataset, we derived
a clinical knowledge graph from the SNOMED taxonomy and generated Poincar\'e
embeddings via Riemannian stochastic gradient descent. These embeddings were
then incorporated into two deep learning architectures, a ResNet and a
Transformer model. Models were evaluated for discrimination (area under the
receiver operating characteristic curve) and calibration (average absolute
difference between observed and predicted probabilities) performance.
  Results: Incorporating pre-trained Poincar\'e embeddings resulted in modest
and consistent improvements in discrimination performance compared to baseline
models using randomly initialized Euclidean embeddings. ResNet models,
particularly those using a 10-dimensional Poincar\'e embedding, showed enhanced
calibration, whereas Transformer models maintained stable calibration across
configurations.
  Discussion: Embedding clinical knowledge graphs into hyperbolic space and
integrating these representations into deep learning models can improve lung
cancer onset prediction by preserving the hierarchical structure of clinical
terminologies used for prediction. This approach demonstrates a feasible method
for combining data-driven feature extraction with established clinical
knowledge.

</details>


### [213] [Understanding Data Influence with Differential Approximation](https://arxiv.org/abs/2508.14648)
*Haoru Tan,Sitong Wu,Xiuzhe Wu,Wang Wang,Bo Zhao,Zeke Xie,Gui-Song Xia,Xiaojuan Qi*

Main category: cs.LG

TL;DR: Diff-In是一种新的数据分析方法，通过累积样本影响力的变化来提高准确性，即使在非凸模型下也能高效运行，并在多项任务中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有数据分析工具在准确性方面存在不足，例如假设损失函数为凸函数，这限制了当前方法的有效实现。因此，需要一种新的方法来准确量化数据影响，即使在非凸模型下也能有效工作。

Method: 提出了一种名为Diff-In的新方法，通过累积连续学习步骤中样本影响力的变化来近似样本影响力。该方法利用二阶近似来准确计算差异项，无需模型凸性假设，并通过有限差分高效计算Hessian-gradient乘积，使其具有与一阶方法相当的计算复杂度和可扩展性。

Result: 理论分析表明，Diff-In的近似误差显著低于现有的影响力估计器。实验结果在多个基准数据集上的数据清洗、数据删除和coreset选择任务中证实了其优越性能。特别是在大规模视觉-语言预训练的数据剪枝实验中，Diff-In能够扩展到数百万个数据点，并优于其他基线方法。

Conclusion: Diff-In通过累积连续学习步骤之间的影响差异来近似样本影响，即使在非凸模型下也能实现高精度和可扩展性，优于现有方法，并在数据清洗、删除和coreset选择等任务中表现出色。

Abstract: Data plays a pivotal role in the groundbreaking advancements in artificial
intelligence. The quantitative analysis of data significantly contributes to
model training, enhancing both the efficiency and quality of data utilization.
However, existing data analysis tools often lag in accuracy. For instance, many
of these tools even assume that the loss function of neural networks is convex.
These limitations make it challenging to implement current methods effectively.
In this paper, we introduce a new formulation to approximate a sample's
influence by accumulating the differences in influence between consecutive
learning steps, which we term Diff-In. Specifically, we formulate the
sample-wise influence as the cumulative sum of its changes/differences across
successive training iterations. By employing second-order approximations, we
approximate these difference terms with high accuracy while eliminating the
need for model convexity required by existing methods. Despite being a
second-order method, Diff-In maintains computational complexity comparable to
that of first-order methods and remains scalable. This efficiency is achieved
by computing the product of the Hessian and gradient, which can be efficiently
approximated using finite differences of first-order gradients. We assess the
approximation accuracy of Diff-In both theoretically and empirically. Our
theoretical analysis demonstrates that Diff-In achieves significantly lower
approximation error compared to existing influence estimators. Extensive
experiments further confirm its superior performance across multiple benchmark
datasets in three data-centric tasks: data cleaning, data deletion, and coreset
selection. Notably, our experiments on data pruning for large-scale
vision-language pre-training show that Diff-In can scale to millions of data
points and outperforms strong baselines.

</details>


### [214] [ELATE: Evolutionary Language model for Automated Time-series Engineering](https://arxiv.org/abs/2508.14667)
*Andrew Murray,Danial Dervovic,Michael Cashmore*

Main category: cs.LG

TL;DR: ELATE是一个利用语言模型和进化框架自动进行时间序列特征工程的方法，可以提高预测准确率。


<details>
  <summary>Details</summary>
Motivation: 手动进行特征工程耗时耗力，现有的自动化方法计算成本高且缺乏领域特定见解。

Method: ELATE利用时间序列统计量和特征重要性指标来指导和修剪特征，同时语言模型提出新的、与上下文相关的特征转换。

Result: ELATE在多个领域中将预测准确率平均提高了8.4%。

Conclusion: ELATE通过在进化框架中利用语言模型来自动进行时间序列数据的特征工程，并在多个领域中将预测准确率平均提高了8.4%。

Abstract: Time-series prediction involves forecasting future values using machine
learning models. Feature engineering, whereby existing features are transformed
to make new ones, is critical for enhancing model performance, but is often
manual and time-intensive. Existing automation attempts rely on exhaustive
enumeration, which can be computationally costly and lacks domain-specific
insights. We introduce ELATE (Evolutionary Language model for Automated
Time-series Engineering), which leverages a language model within an
evolutionary framework to automate feature engineering for time-series data.
ELATE employs time-series statistical measures and feature importance metrics
to guide and prune features, while the language model proposes new,
contextually relevant feature transformations. Our experiments demonstrate that
ELATE improves forecasting accuracy by an average of 8.4% across various
domains.

</details>


### [215] [Improving Fairness in Graph Neural Networks via Counterfactual Debiasing](https://arxiv.org/abs/2508.14683)
*Zengyi Wo,Chang Liu,Yumeng Wang,Minglai Shao,Wenjun Wang*

Main category: cs.LG

TL;DR: Fair-ICD 通过反事实数据增强来解决 GNN 中的偏见问题，它在增强公平性的同时保持了准确性。


<details>
  <summary>Details</summary>
Motivation: GNNs 在处理图结构数据方面取得了成功，但它们可能会根据种族和性别等属性产生预测偏见，而图结构和消息传递机制可能会加剧这种偏见。

Method: 该方法包括在消息传递之前使用反事实来创建多样化的邻域，然后使用对抗性判别器来消除 GNN 分类器预测中的偏见。

Result: 实验表明，Fair-ICD 在标准的基准数据集上，使用三种 GNN 主干，能够显著提高公平性指标，同时保持高预测性能。

Conclusion: Fair-ICD 是一种利用反事实数据增强来减轻 GNN 偏见的新方法，可在保持高预测性能的同时提高公平性指标。

Abstract: Graph Neural Networks (GNNs) have been successful in modeling
graph-structured data. However, similar to other machine learning models, GNNs
can exhibit bias in predictions based on attributes like race and gender.
Moreover, bias in GNNs can be exacerbated by the graph structure and
message-passing mechanisms. Recent cutting-edge methods propose mitigating bias
by filtering out sensitive information from input or representations, like edge
dropping or feature masking. Yet, we argue that such strategies may
unintentionally eliminate non-sensitive features, leading to a compromised
balance between predictive accuracy and fairness. To tackle this challenge, we
present a novel approach utilizing counterfactual data augmentation for bias
mitigation. This method involves creating diverse neighborhoods using
counterfactuals before message passing, facilitating unbiased node
representations learning from the augmented graph. Subsequently, an adversarial
discriminator is employed to diminish bias in predictions by conventional GNN
classifiers. Our proposed technique, Fair-ICD, ensures the fairness of GNNs
under moderate conditions. Experiments on standard datasets using three GNN
backbones demonstrate that Fair-ICD notably enhances fairness metrics while
preserving high predictive performance.

</details>


### [216] [Addressing Graph Anomaly Detection via Causal Edge Separation and Spectrum](https://arxiv.org/abs/2508.14684)
*Zengyi Wo,Wenjun Wang,Minglai Shao,Chang Liu,Yumeng Wang,Yueheng Sun*

Main category: cs.LG

TL;DR: 提出了一种名为CES2-GAD的谱神经网络，用于检测异质图中的异常。该方法通过分离图中的同质和异质边，并结合混合谱滤波器来捕捉信号，从而解决了现有GNN方法在处理异质图时的不足。实验结果表明该方法是有效的。


<details>
  <summary>Details</summary>
Motivation: 现有的图神经网络（GNN）方法在处理异质图时存在不足，特别是在节点结构编码、节点特征及其上下文环境之间复杂关系的研究以及谱域异质性问题上。现实世界中的异常实体常常在隐藏与其他异常实体的直接联系的同时，增加与其他正常实体的连接，导致异质图的结构。

Method: 提出了一种基于因果边分离（CES2-GAD）的谱神经网络，用于检测异质图中的异常。该方法首先利用因果干预将原始图分离为同质边和异质边，然后使用混合谱滤波器从分割后的图中捕获信号，最后将多信号的表示连接起来输入分类器以预测异常。

Result: 发现异常节点的异质性会导致谱能量从低频转移到高频。

Conclusion: 实验证明了该方法在真实世界数据集上的有效性。

Abstract: In the real world, anomalous entities often add more legitimate connections
while hiding direct links with other anomalous entities, leading to
heterophilic structures in anomalous networks that most GNN-based techniques
fail to address. Several works have been proposed to tackle this issue in the
spatial domain. However, these methods overlook the complex relationships
between node structure encoding, node features, and their contextual
environment and rely on principled guidance, research on solving spectral
domain heterophilic problems remains limited. This study analyzes the spectral
distribution of nodes with different heterophilic degrees and discovers that
the heterophily of anomalous nodes causes the spectral energy to shift from low
to high frequencies. To address the above challenges, we propose a spectral
neural network CES2-GAD based on causal edge separation for anomaly detection
on heterophilic graphs. Firstly, CES2-GAD will separate the original graph into
homophilic and heterophilic edges using causal interventions. Subsequently,
various hybrid-spectrum filters are used to capture signals from the segmented
graphs. Finally, representations from multiple signals are concatenated and
input into a classifier to predict anomalies. Extensive experiments with
real-world datasets have proven the effectiveness of the method we proposed.

</details>


### [217] [AFABench: A Generic Framework for Benchmarking Active Feature Acquisition](https://arxiv.org/abs/2508.14734)
*Valter Schütz,Han Wu,Reza Rezvan,Linus Aronsson,Morteza Haghir Chehreghani*

Main category: cs.LG

TL;DR: The paper introduces AFABench, the first benchmark for Active Feature Acquisition (AFA), addressing the challenge of expensive feature acquisition. It provides diverse datasets and evaluates various AFA methods, offering insights into their trade-offs and limitations.


<details>
  <summary>Details</summary>
Motivation: Acquiring all features of a data instance can be expensive or impractical due to cost, latency, or privacy concerns. Active Feature Acquisition (AFA) aims to address this by dynamically selecting informative features to balance predictive performance and acquisition cost. However, the lack of standardized benchmarks hinders fair and systematic evaluation of AFA methods.

Method: The paper introduces AFABench, a benchmark framework for Active Feature Acquisition (AFA). It includes synthetic and real-world datasets, supports various acquisition policies, and provides a modular design for easy integration of new methods. Representative algorithms from static, greedy, and reinforcement learning categories are implemented and evaluated. A novel synthetic dataset, AFAContext, is introduced to test lookahead capabilities.

Result: The paper implements and evaluates representative AFA algorithms, including static, greedy, and reinforcement learning-based approaches. The novel AFAContext dataset exposes limitations of greedy selection. Results highlight key trade-offs between different AFA strategies and offer actionable insights for future research.

Conclusion: AFABench is the first benchmark framework for AFA, including diverse datasets and supporting various acquisition policies. It enables systematic evaluation and highlights trade-offs between different AFA strategies, providing insights for future research.

Abstract: In many real-world scenarios, acquiring all features of a data instance can
be expensive or impractical due to monetary cost, latency, or privacy concerns.
Active Feature Acquisition (AFA) addresses this challenge by dynamically
selecting a subset of informative features for each data instance, trading
predictive performance against acquisition cost. While numerous methods have
been proposed for AFA, ranging from greedy information-theoretic strategies to
non-myopic reinforcement learning approaches, fair and systematic evaluation of
these methods has been hindered by the lack of standardized benchmarks. In this
paper, we introduce AFABench, the first benchmark framework for AFA. Our
benchmark includes a diverse set of synthetic and real-world datasets, supports
a wide range of acquisition policies, and provides a modular design that
enables easy integration of new methods and tasks. We implement and evaluate
representative algorithms from all major categories, including static, greedy,
and reinforcement learning-based approaches. To test the lookahead capabilities
of AFA policies, we introduce a novel synthetic dataset, AFAContext, designed
to expose the limitations of greedy selection. Our results highlight key
trade-offs between different AFA strategies and provide actionable insights for
future research. The benchmark code is available at:
https://github.com/Linusaronsson/AFA-Benchmark.

</details>


### [218] [CaTE Data Curation for Trustworthy AI](https://arxiv.org/abs/2508.14741)
*Mary Versa Clemens-Sewall,Christopher Cervantes,Emma Rafkin,J. Neil Otte,Tom Magelinski,Libby Lewis,Michelle Liu,Dana Udwin,Monique Kirkman-Bey*

Main category: cs.LG

TL;DR: 本报告为人工智能系统的开发提供了数据整理阶段的可信度指南，并列出了具体的步骤、工具和方法。


<details>
  <summary>Details</summary>
Motivation: 本报告旨在为设计或开发人工智能系统的团队提供实际指导，以在开发的数据整理阶段促进可信度。

Method: 本报告首先定义了数据、数据整理阶段和可信度。然后，我们描述了一系列步骤，开发团队（尤其是数据科学家）可以采取这些步骤来构建可信的人工智能系统。我们枚举了核心步骤的顺序，并追踪了存在替代方案的并行路径。这些步骤的描述包括优势、劣势、先决条件、结果和相关的开源软件工具实现。

Result: 该报告是相关学术文献中数据整理工具和方法的综合，旨在为读者提供一套多样但连贯的实践，以提高人工智能的可信度。

Conclusion: 该报告旨在为设计或开发人工智能系统的团队提供实际指导，以在开发的数据整理阶段促进可信度。

Abstract: This report provides practical guidance to teams designing or developing
AI-enabled systems for how to promote trustworthiness during the data curation
phase of development. In this report, the authors first define data, the data
curation phase, and trustworthiness. We then describe a series of steps that
the development team, especially data scientists, can take to build a
trustworthy AI-enabled system. We enumerate the sequence of core steps and
trace parallel paths where alternatives exist. The descriptions of these steps
include strengths, weaknesses, preconditions, outcomes, and relevant
open-source software tool implementations. In total, this report is a synthesis
of data curation tools and approaches from relevant academic literature, and
our goal is to equip readers with a diverse yet coherent set of practices for
improving AI trustworthiness.

</details>


### [219] [MissionHD: Data-Driven Refinement of Reasoning Graph Structure through Hyperdimensional Causal Path Encoding and Decoding](https://arxiv.org/abs/2508.14746)
*Sanggeon Yun,Raheeb Hassan,Ryozo Masukawa,Mohsen Imani*

Main category: cs.LG

TL;DR: LLM推理图与下游视觉任务不对齐？D-GSR通过MissionHD框架优化图结构，提升视频异常检测等任务性能。


<details>
  <summary>Details</summary>
Motivation: 现有的图结构细化（GSR）方法不适用于大型语言模型（LLM）生成的、与下游视觉任务（如视频异常检测）不对齐的新型、无数据集图。

Method: 提出了一种名为Data-driven GSR（D-GSR）的新范式，并用MissionHD（一个高维计算框架）来实现。MissionHD采用高效的编码-解码过程，并以下游任务信号为指导来优化图结构。

Result: 在具有挑战性的VAD和VAR基准测试中，使用细化后的图可以带来显著的性能提升，证明了该方法作为有效预处理步骤的有效性。

Conclusion: 数据驱动的图结构细化（D-GSR）通过优化图结构来解决LLM推理图与下游视觉任务（如视频异常检测）的对齐问题，并通过MissionHD框架（基于高维计算）实现。实验证明，D-GSR显著提高了VAD和VAR基准的性能。

Abstract: Reasoning graphs from Large Language Models (LLMs) are often misaligned with
downstream visual tasks such as video anomaly detection (VAD). Existing Graph
Structure Refinement (GSR) methods are ill-suited for these novel, dataset-less
graphs. We introduce Data-driven GSR (D-GSR), a new paradigm that directly
optimizes graph structure using downstream task data, and propose MissionHD, a
hyperdimensional computing (HDC) framework to operationalize it. MissionHD uses
an efficient encode-decode process to refine the graph, guided by the
downstream task signal. Experiments on challenging VAD and VAR benchmarks show
significant performance improvements when using our refined graphs, validating
our approach as an effective pre-processing step.

</details>


### [220] [Cross-Modality Controlled Molecule Generation with Diffusion Language Model](https://arxiv.org/abs/2508.14748)
*Yunzhe Zhang,Yifei Wang,Khanh Vinh Nguyen,Pengyu Hong*

Main category: cs.LG

TL;DR: CMCM-DLM是一种新的扩散语言模型，用于分子生成。它通过引入结构控制模块（SCM）和属性控制模块（PCM），实现了对分子结构和化学属性的多模态约束，并且无需重新训练即可适应新约束。该方法在药物发现等领域具有重要应用价值。


<details>
  <summary>Details</summary>
Motivation: 现实世界的应用通常涉及跨越不同模态的多个约束，并且在研究过程中可能会出现新的约束。这带来了一个挑战：如何在不重新训练的情况下，将预训练的扩散模型扩展到支持跨模态约束，并整合新的约束。

Method: 提出了一种名为CMCM-DLM（Cross-Modality Controlled Molecule Generation with Diffusion Language Model）的方法，该方法建立在预训练的扩散模型之上，并引入了两个可训练的模块：结构控制模块（SCM）和属性控制模块（PCM）。模型在生成过程中分两个阶段进行：第一阶段使用SCM注入结构约束，锚定分子骨架；第二阶段引入PCM，通过指导推理过程的后期阶段来优化生成分子，以匹配目标化学属性。

Result: 实验结果表明，CMCM-DLM在多个数据集上展现了其效率和适应性，能够有效地处理分子结构和化学属性这两种不同的跨模态约束。

Conclusion: CMCM-DLM在分子生成方面取得了显著进展，尤其在药物发现应用中，能够有效地适应并结合多模态约束，而无需从头开始重新训练模型。

Abstract: Current SMILES-based diffusion models for molecule generation typically
support only unimodal constraint. They inject conditioning signals at the start
of the training process and require retraining a new model from scratch
whenever the constraint changes. However, real-world applications often involve
multiple constraints across different modalities, and additional constraints
may emerge over the course of a study. This raises a challenge: how to extend a
pre-trained diffusion model not only to support cross-modality constraints but
also to incorporate new ones without retraining. To tackle this problem, we
propose the Cross-Modality Controlled Molecule Generation with Diffusion
Language Model (CMCM-DLM), demonstrated by two distinct cross modalities:
molecular structure and chemical properties. Our approach builds upon a
pre-trained diffusion model, incorporating two trainable modules, the Structure
Control Module (SCM) and the Property Control Module (PCM), and operates in two
distinct phases during the generation process. In Phase I, we employs the SCM
to inject structural constraints during the early diffusion steps, effectively
anchoring the molecular backbone. Phase II builds on this by further
introducing PCM to guide the later stages of inference to refine the generated
molecules, ensuring their chemical properties match the specified targets.
Experimental results on multiple datasets demonstrate the efficiency and
adaptability of our approach, highlighting CMCM-DLM's significant advancement
in molecular generation for drug discovery applications.

</details>


### [221] [HERAKLES: Hierarchical Skill Compilation for Open-ended LLM Agents](https://arxiv.org/abs/2508.14751)
*Thomas Carta,Clément Romac,Loris Gaven,Pierre-Yves Oudeyer,Olivier Sigaud,Sylvain Lamprier*

Main category: cs.LG

TL;DR: HERAKLES是一个用于开放式AI代理的框架，通过LLM控制和技能编译来管理复杂目标，在Crafter环境中表现出良好的扩展性、样本效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 为了解决开放式AI代理在终身学习过程中，如何高效学习日益增长的复杂性、抽象性和异构性目标，并控制相关样本和计算复杂性增长的挑战。

Method: HERAKLES框架利用大型语言模型（LLM）作为高级控制器，结合其在目标分解和泛化方面的优势，在不断变化的子目标空间中有效运行，并将已掌握的目标编译为低级策略。

Result: HERAKLES在开放式Crafter环境中进行了训练，结果表明该框架能有效应对目标复杂性的增长，通过技能编译提高样本效率，并使代理能够随着时间的推移稳健地适应新的挑战。

Conclusion: HERAKLES框架通过两层级联的自定目标代理，将已掌握的目标持续编译成由小型、快速神经网络执行的低级策略，并动态扩展了可供高级策略使用的子目标集。

Abstract: Open-ended AI agents need to be able to learn efficiently goals of increasing
complexity, abstraction and heterogeneity over their lifetime. Beyond sampling
efficiently their own goals, autotelic agents specifically need to be able to
keep the growing complexity of goals under control, limiting the associated
growth in sample and computational complexity. To adress this challenge, recent
approaches have leveraged hierarchical reinforcement learning (HRL) and
language, capitalizing on its compositional and combinatorial generalization
capabilities to acquire temporally extended reusable behaviours. Existing
approaches use expert defined spaces of subgoals over which they instantiate a
hierarchy, and often assume pre-trained associated low-level policies. Such
designs are inadequate in open-ended scenarios, where goal spaces naturally
diversify across a broad spectrum of difficulties. We introduce HERAKLES, a
framework that enables a two-level hierarchical autotelic agent to continuously
compile mastered goals into the low-level policy, executed by a small, fast
neural network, dynamically expanding the set of subgoals available to the
high-level policy. We train a Large Language Model (LLM) to serve as the
high-level controller, exploiting its strengths in goal decomposition and
generalization to operate effectively over this evolving subgoal space. We
evaluate HERAKLES in the open-ended Crafter environment and show that it scales
effectively with goal complexity, improves sample efficiency through skill
compilation, and enables the agent to adapt robustly to novel challenges over
time.

</details>


### [222] [PepThink-R1: LLM for Interpretable Cyclic Peptide Optimization with CoT SFT and Reinforcement Learning](https://arxiv.org/abs/2508.14765)
*Ruheng Wang,Hang Zhang,Trieu Nguyen,Shasha Feng,Hao-Wei Pang,Xiang Yu,Li Xiao,Peter Zhiping Zhang*

Main category: cs.LG

TL;DR: PepThink-R1是一个新的框架，它使用大型语言模型和强化学习来设计更好的治疗性肽，并能解释其设计过程。


<details>
  <summary>Details</summary>
Motivation: 为解决当前生成模型在设计具有定制化性质的治疗性肽时面临的序列空间广阔、实验数据有限和可解释性差等挑战。

Method: PepThink-R1框架集成了大型语言模型（LLMs）、思维链（CoT）监督微调和强化学习（RL），在序列生成过程中显式地对单体级别的修饰进行推理，以实现可解释的设计选择并优化多种药理学性质。

Result: PepThink-R1生成的环肽在疏水性、稳定性和暴露性方面显著增强，并且在优化成功率和可解释性方面均优于现有的通用LLM（如GPT-5）和领域特定的基线模型。

Conclusion: PepThink-R1是首个结合了显式推理和RL驱动属性控制的基于LLM的肽设计框架，显著提高了疏水性、稳定性和暴露性，并在优化成功率和可解释性方面优于现有模型，为治疗性发现的可靠和透明肽优化迈出了重要一步。

Abstract: Designing therapeutic peptides with tailored properties is hindered by the
vastness of sequence space, limited experimental data, and poor
interpretability of current generative models. To address these challenges, we
introduce PepThink-R1, a generative framework that integrates large language
models (LLMs) with chain-of-thought (CoT) supervised fine-tuning and
reinforcement learning (RL). Unlike prior approaches, PepThink-R1 explicitly
reasons about monomer-level modifications during sequence generation, enabling
interpretable design choices while optimizing for multiple pharmacological
properties. Guided by a tailored reward function balancing chemical validity
and property improvements, the model autonomously explores diverse sequence
variants. We demonstrate that PepThink-R1 generates cyclic peptides with
significantly enhanced lipophilicity, stability, and exposure, outperforming
existing general LLMs (e.g., GPT-5) and domain-specific baseline in both
optimization success and interpretability. To our knowledge, this is the first
LLM-based peptide design framework that combines explicit reasoning with
RL-driven property control, marking a step toward reliable and transparent
peptide optimization for therapeutic discovery.

</details>


### [223] [Squeezed Diffusion Models](https://arxiv.org/abs/2508.14871)
*Jyotirmai Singh,Samar Khanna,James Burgess*

Main category: cs.LG

TL;DR: Diffusion models can be improved by scaling noise based on data structure, inspired by quantum mechanics. This 'Squeezed Diffusion Model' approach enhances generative performance.


<details>
  <summary>Details</summary>
Motivation: Motivated by quantum squeezed states redistributing uncertainty, the authors hypothesize that data-dependent noise scaling can improve the learning of important data features in diffusion models, similar to how squeezing enhances the signal-to-noise ratio in physics.

Method: The paper introduces Squeezed Diffusion Models (SDM), which adaptively scale noise based on the principal components of the training data distribution. Two configurations are studied: a Heisenberg diffusion model and a standard SDM variant. These models deviate from the typical injection of isotropic Gaussian noise.

Result: Experiments on CIFAR-10/100 and CelebA-64 showed that mild antisqueezing (increasing variance along the principal axis) consistently improved FID by up to 15% and advanced the precision-recall frontier towards higher recall.

Conclusion: Squeezed Diffusion Models (SDM) demonstrate that data-aware noise shaping can improve generative models without architectural changes, with mild antisqueezing consistently enhancing FID and shifting the precision-recall frontier.

Abstract: Diffusion models typically inject isotropic Gaussian noise, disregarding
structure in the data. Motivated by the way quantum squeezed states
redistribute uncertainty according to the Heisenberg uncertainty principle, we
introduce Squeezed Diffusion Models (SDM), which scale noise anisotropically
along the principal component of the training distribution. As squeezing
enhances the signal-to-noise ratio in physics, we hypothesize that scaling
noise in a data-dependent manner can better assist diffusion models in learning
important data features. We study two configurations: (i) a Heisenberg
diffusion model that compensates the scaling on the principal axis with inverse
scaling on orthogonal directions and (ii) a standard SDM variant that scales
only the principal axis. Counterintuitively, on CIFAR-10/100 and CelebA-64,
mild antisqueezing - i.e. increasing variance on the principal axis -
consistently improves FID by up to 15% and shifts the precision-recall frontier
toward higher recall. Our results demonstrate that simple, data-aware noise
shaping can deliver robust generative gains without architectural changes.

</details>


### [224] [Context Steering: A New Paradigm for Compression-based Embeddings by Synthesizing Relevant Information Features](https://arxiv.org/abs/2508.14780)
*Guillermo Sarasa Durán,Ana Granados Fontecha,Francisco de Borja Rodríguez Ortíz*

Main category: cs.LG

TL;DR: We introduce 'context steering,' a novel methodology that actively guides the feature-shaping process. Instead of passively accepting the emergent data structure (typically a hierarchy derived from clustering CDs), our approach 'steers' the process by systematically analyzing how each object influences the relational context within a clustering framework. This process generates a custom-tailored embedding that isolates and amplifies class-distinctive information. We validate the capabilities of this strategy using Normalized Compression Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical clustering, providing an effective alternative to common transductive methods. Experimental results across heterogeneous datasets-from text to real-world audio-validate the robustness and generality of context steering, marking a fundamental shift in their application: from merely discovering inherent data structures to actively shaping a feature space tailored to a specific objective.


<details>
  <summary>Details</summary>
Motivation: Compression-based distances (CD) offer a flexible and domain-agnostic means of measuring similarity by identifying implicit information through redundancies between data objects. However, as similarity features are derived from the data, rather than defined as an input, it often proves difficult to align with the task at hand, particularly in complex clustering or classification settings.

Method: Instead of passively accepting the emergent data structure (typically a hierarchy derived from clustering CDs), our approach 'steers' the process by systematically analyzing how each object influences the relational context within a clustering framework. This process generates a custom-tailored embedding that isolates and amplifies class-distinctive information. We validate the capabilities of this strategy using Normalized Compression Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical clustering, providing an effective alternative to common transductive methods.

Result: Experimental results across heterogeneous datasets-from text to real-world audio-validate the robustness and generality of context steering, marking a fundamental shift in their application: from merely discovering inherent data structures to actively shaping a feature space tailored to a specific objective.

Conclusion: Compression-based distances (CD) offer a flexible and domain-agnostic means of measuring similarity by identifying implicit information through redundancies between data objects. However, as similarity features are derived from the data, rather than defined as an input, it often proves difficult to align with the task at hand, particularly in complex clustering or classification settings. To address this issue, we introduce 'context steering,' a novel methodology that actively guides the feature-shaping process. Instead of passively accepting the emergent data structure (typically a hierarchy derived from clustering CDs), our approach 'steers' the process by systematically analyzing how each object influences the relational context within a clustering framework. This process generates a custom-tailored embedding that isolates and amplifies class-distinctive information. We validate the capabilities of this strategy using Normalized Compression Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical clustering, providing an effective alternative to common transductive methods. Experimental results across heterogeneous datasets-from text to real-world audio-validate the robustness and generality of context steering, marking a fundamental shift in their application: from merely discovering inherent data structures to actively shaping a feature space tailored to a specific objective.

Abstract: Compression-based distances (CD) offer a flexible and domain-agnostic means
of measuring similarity by identifying implicit information through
redundancies between data objects. However, as similarity features are derived
from the data, rather than defined as an input, it often proves difficult to
align with the task at hand, particularly in complex clustering or
classification settings. To address this issue, we introduce "context
steering," a novel methodology that actively guides the feature-shaping
process. Instead of passively accepting the emergent data structure (typically
a hierarchy derived from clustering CDs), our approach "steers" the process by
systematically analyzing how each object influences the relational context
within a clustering framework. This process generates a custom-tailored
embedding that isolates and amplifies class-distinctive information. We
validate the capabilities of this strategy using Normalized Compression
Distance (NCD) and Relative Compression Distance (NRC) with common hierarchical
clustering, providing an effective alternative to common transductive methods.
Experimental results across heterogeneous datasets-from text to real-world
audio-validate the robustness and generality of context steering, marking a
fundamental shift in their application: from merely discovering inherent data
structures to actively shaping a feature space tailored to a specific
objective.

</details>


### [225] [Synthetic Adaptive Guided Embeddings (SAGE): A Novel Knowledge Distillation Method](https://arxiv.org/abs/2508.14783)
*Suleyman Olcay Polat,Poli A. Nemkova,Mark V. Albert*

Main category: cs.LG

TL;DR: 该研究提出了一种自适应蒸馏框架，通过动态增加难例数据和优化师生接口，实现了高效的模型压缩，在NLP任务上取得了优异的性能。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统模型蒸馏方法计算开销大和泛化能力有限的问题，实现将大型模型知识转移到小型模型以适应资源受限环境的需求。

Method: 提出了一种新颖的自适应蒸馏框架，通过UMAP降维和最近邻采样动态地在学生模型损失较高的区域增加训练数据，并引入了一个轻量级的师生接口，直接在向量化表示上进行蒸馏。

Result: 在标准的NLP基准测试中，参数量为66M的学生模型在QNLI上达到91.2%，在SST-2上达到92.3%，表现与现有基线相当或更优，且训练轮数更少。

Conclusion: 所提出的自适应蒸馏框架通过损失感知的数据增强和向量化蒸馏，在模型压缩方面展现出高效性和有效性。

Abstract: Model distillation enables the transfer of knowledge from large-scale models
to compact student models, facilitating deployment in resource-constrained
environments. However, conventional distillation approaches often suffer from
computational overhead and limited generalization. We propose a novel adaptive
distillation framework that dynamically augments training data in regions of
high student model loss. Using UMAP-based dimensionality reduction and nearest
neighbor sampling, our method identifies underperforming regions in the
embedding space and generates targeted synthetic examples to guide student
learning. To further improve efficiency, we introduce a lightweight
teacher-student interface that bypasses the teacher's input layer, enabling
direct distillation on vectorized representations. Experiments across standard
NLP benchmarks demonstrate that our 66M-parameter student model consistently
matches or surpasses established baselines, achieving 91.2% on QNLI and 92.3%
on SST-2, while training with fewer epochs. These results highlight the promise
of loss-aware data augmentation and vectorized distillation for efficient and
effective model compression.

</details>


### [226] [A Guide for Manual Annotation of Scientific Imagery: How to Prepare for Large Projects](https://arxiv.org/abs/2508.14801)
*Azim Ahmadzadeh,Rohan Adhyapak,Armin Iraji,Kartik Chaurasiya,V Aparna,Petrus C. Martens*

Main category: cs.LG

TL;DR: 本研究提供了关于如何管理手动标注项目的指南，重点关注科学图像，并提出应进一步研究以降低相关成本。


<details>
  <summary>Details</summary>
Motivation: 手动标注图像数据的需求很高，但管理复杂的、成本高昂的标注项目仍然讨论不足，因为这些项目涉及的挑战通常超出了特定领域专家的知识范围，导致实践指南稀缺。

Method: 提供了一份领域无关的标注项目准备指南，重点关注科学图像，并讨论了各种人为偏见以及提高标注质量和效率的工具和技术。

Result: 该论文根据作者在管理大型手动标注项目方面的丰富经验，解决了包括成功指标、标注对象、项目目标、数据可用性和关键团队角色在内的基本概念。

Conclusion: 本研究旨在鼓励对降低手动标注项目成本的进一步研究和框架，以创建全面的知识库。

Abstract: Despite the high demand for manually annotated image data, managing complex
and costly annotation projects remains under-discussed. This is partly due to
the fact that leading such projects requires dealing with a set of diverse and
interconnected challenges which often fall outside the expertise of specific
domain experts, leaving practical guidelines scarce. These challenges range
widely from data collection to resource allocation and recruitment, from
mitigation of biases to effective training of the annotators. This paper
provides a domain-agnostic preparation guide for annotation projects, with a
focus on scientific imagery. Drawing from the authors' extensive experience in
managing a large manual annotation project, it addresses fundamental concepts
including success measures, annotation subjects, project goals, data
availability, and essential team roles. Additionally, it discusses various
human biases and recommends tools and technologies to improve annotation
quality and efficiency. The goal is to encourage further research and
frameworks for creating a comprehensive knowledge base to reduce the costs of
manual annotation projects across various fields.

</details>


### [227] [Source-Guided Flow Matching](https://arxiv.org/abs/2508.14807)
*Zifan Wang,Alice Harting,Matthieu Barreau,Michael M. Zavlanos,Karl H. Johansson*

Main category: cs.LG

TL;DR: 提出源引导流匹配（SGFM）框架，直接修改源分布以实现生成模型引导，比传统方法更灵活且效果好。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的引导方法，直接修改源分布，保持预训练向量场不变，以简化引导问题并提高灵活性。

Method: 提出源引导流匹配（SGFM）框架，修改源分布而不改变预训练的向量场，将引导问题转化为从源分布采样的过程。

Result: 理论上证明SGFM可以精确恢复目标分布，并提供了使用近似采样器和近似向量场时的Wasserstein误差界限。实验结果表明了该框架在合成2D基准、图像数据集和物理信息生成任务上的有效性和灵活性。

Conclusion: SGFM框架通过直接修改源分布并保持预训练的向量场不变，将引导问题转化为从源分布采样的良定义问题，能够精确恢复目标分布，并为近似采样器和近似向量场提供了Wasserstein误差界限。该框架允许用户灵活选择采样方法，与最优流匹配模型兼容，并在各种基准测试中证明了其有效性和灵活性。

Abstract: Guidance of generative models is typically achieved by modifying the
probability flow vector field through the addition of a guidance field. In this
paper, we instead propose the Source-Guided Flow Matching (SGFM) framework,
which modifies the source distribution directly while keeping the pre-trained
vector field intact. This reduces the guidance problem to a well-defined
problem of sampling from the source distribution. We theoretically show that
SGFM recovers the desired target distribution exactly. Furthermore, we provide
bounds on the Wasserstein error for the generated distribution when using an
approximate sampler of the source distribution and an approximate vector field.
The key benefit of our approach is that it allows the user to flexibly choose
the sampling method depending on their specific problem. To illustrate this, we
systematically compare different sampling methods and discuss conditions for
asymptotically exact guidance. Moreover, our framework integrates well with
optimal flow matching models since the straight transport map generated by the
vector field is preserved. Experimental results on synthetic 2D benchmarks,
image datasets, and physics-informed generative tasks demonstrate the
effectiveness and flexibility of the proposed framework.

</details>


### [228] [Enhancing Contrastive Link Prediction With Edge Balancing Augmentation](https://arxiv.org/abs/2508.14808)
*Chen-Hao Chang,Hui-Ju Hung,Chia-Hsun Lu,Chih-Ya Shen*

Main category: cs.LG

TL;DR: This paper analyzes contrastive learning for link prediction, addressing theoretical gaps and node degree considerations. It introduces Edge Balancing Augmentation (EBA) and a new model, CoEBA, which significantly improves performance on benchmark datasets.


<details>
  <summary>Details</summary>
Motivation: The study is motivated by the weaknesses observed in recent studies leveraging contrastive learning for link prediction: the lack of theoretical analysis and inadequate consideration of node degrees.

Method: The paper proposes a new graph augmentation approach called Edge Balancing Augmentation (EBA) to adjust node degrees and integrates it with new contrastive losses into a model named Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA).

Result: The proposed CoEBA model shows significant improvements compared to existing state-of-the-art link prediction models.

Conclusion: CoEBA significantly outperforms other state-of-the-art link prediction models on 8 benchmark datasets.

Abstract: Link prediction is one of the most fundamental tasks in graph mining, which
motivates the recent studies of leveraging contrastive learning to enhance the
performance. However, we observe two major weaknesses of these studies: i) the
lack of theoretical analysis for contrastive learning on link prediction, and
ii) inadequate consideration of node degrees in contrastive learning. To
address the above weaknesses, we provide the first formal theoretical analysis
for contrastive learning on link prediction, where our analysis results can
generalize to the autoencoder-based link prediction models with contrastive
learning. Motivated by our analysis results, we propose a new graph
augmentation approach, Edge Balancing Augmentation (EBA), which adjusts the
node degrees in the graph as the augmentation. We then propose a new approach,
named Contrastive Link Prediction with Edge Balancing Augmentation (CoEBA),
that integrates the proposed EBA and the proposed new contrastive losses to
improve the model performance. We conduct experiments on 8 benchmark datasets.
The results demonstrate that our proposed CoEBA significantly outperforms the
other state-of-the-art link prediction models.

</details>


### [229] [Successive Halving with Learning Curve Prediction via Latent Kronecker Gaussian Processes](https://arxiv.org/abs/2508.14818)
*Jihao Andreas Lin,Nicolas Mayoraz,Steffen Rendle,Dima Kuzmin,Emil Praun,Berivan Isik*

Main category: cs.LG

TL;DR: 通过结合学习曲线预测来改进 Successive Halving 算法，但存在数据要求和帕累托最优性的限制。


<details>
  <summary>Details</summary>
Motivation: 为了解决 Successive Halving 算法可能过早剪枝表现不佳但最终可能表现最佳的候选者的问题。

Method: 将基于潜在克罗内克高斯过程的学习曲线预测与 Successive Halving 算法相结合，并与仅基于当前性能值的标准方法进行比较。

Result: 在涉及不同神经网络架构和点击预测数据集的大规模实证研究中，基于预测的方法取得了有竞争力的性能，但与投入更多资源到标准方法相比，并非帕累托最优，因为它需要完全观察到的学习曲线作为训练数据。

Conclusion: Successive Halving 算法可以通过结合基于潜在克罗内克高斯过程的学习曲线预测来改进，但需要完全观察到的学习曲线作为训练数据，这限制了其在实践中的应用。然而，可以利用现有的学习曲线数据来缓解这一限制。

Abstract: Successive Halving is a popular algorithm for hyperparameter optimization
which allocates exponentially more resources to promising candidates. However,
the algorithm typically relies on intermediate performance values to make
resource allocation decisions, which can cause it to prematurely prune slow
starters that would eventually become the best candidate. We investigate
whether guiding Successive Halving with learning curve predictions based on
Latent Kronecker Gaussian Processes can overcome this limitation. In a
large-scale empirical study involving different neural network architectures
and a click prediction dataset, we compare this predictive approach to the
standard approach based on current performance values. Our experiments show
that, although the predictive approach achieves competitive performance, it is
not Pareto optimal compared to investing more resources into the standard
approach, because it requires fully observed learning curves as training data.
However, this downside could be mitigated by leveraging existing learning curve
data.

</details>


### [230] [On Defining Neural Averaging](https://arxiv.org/abs/2508.14832)
*Su Hyeong Lee,Richard Ngo*

Main category: cs.LG

TL;DR: 本文提出了一种名为“非线性模型集成”（AME）的数据无关方法，用于平均神经网络。该方法将模型差异视为伪梯度来指导权重更新，优于现有方法，特别是在分布外设置中。


<details>
  <summary>Details</summary>
Motivation: 本文旨在解决如何从一组预训练模型（每个模型都在不相交的数据分片上训练）中综合出单个神经网络的问题，并且只使用它们的最终权重，不访问训练数据。

Method: AME将模型差异视为伪梯度来指导神经网络权重更新，这是一种数据无关的元优化方法。

Result: AME不仅可以恢复模型汤，还可以实现更具表现力和适应性的集成策略。实验结果表明，AME产生的平均神经网络解决方案优于单独的专家模型和模型汤基线，尤其是在分布外设置中。

Conclusion: AME提供了一种新的数据无关模型权重聚合方法，并对如何进行神经网络平均进行了定义。

Abstract: What does it even mean to average neural networks? We investigate the problem
of synthesizing a single neural network from a collection of pretrained models,
each trained on disjoint data shards, using only their final weights and no
access to training data. In forming a definition of neural averaging, we take
insight from model soup, which appears to aggregate multiple models into a
singular model while enhancing generalization performance. In this work, we
reinterpret model souping as a special case of a broader framework: Amortized
Model Ensembling (AME) for neural averaging, a data-free meta-optimization
approach that treats model differences as pseudogradients to guide neural
weight updates. We show that this perspective not only recovers model soup but
enables more expressive and adaptive ensembling strategies. Empirically, AME
produces averaged neural solutions that outperform both individual experts and
model soup baselines, especially in out-of-distribution settings. Our results
suggest a principled and generalizable notion of data-free model weight
aggregation and defines, in one sense, how to perform neural averaging.

</details>


### [231] [Graph Structure Learning with Temporal Graph Information Bottleneck for Inductive Representation Learning](https://arxiv.org/abs/2508.14859)
*Jiafeng Xiong,Rizos Sakellariou*

Main category: cs.LG

TL;DR: GTGIB is a new framework for temporal graph learning that improves node representation and reduces noise using graph structure learning and an information bottleneck approach, outperforming existing methods in link prediction.


<details>
  <summary>Details</summary>
Motivation: Temporal graph learning faces challenges in representing unseen nodes and mitigating noisy or redundant graph information in dynamic networks.

Method: GTGIB integrates Graph Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). It uses a two-step GSL-based structural enhancer to optimize node neighborhoods and extends the information bottleneck principle to temporal graphs by regularizing edges and features via a tractable TGIB objective function and variational approximation.

Result: GTGIB models show significant and consistent improvement over existing methods in both inductive and transductive settings for link prediction on four real-world datasets.

Conclusion: GTGIB-based models outperform existing methods in predicting links on four real-world datasets in both inductive and transductive settings.

Abstract: Temporal graph learning is crucial for dynamic networks where nodes and edges
evolve over time and new nodes continuously join the system. Inductive
representation learning in such settings faces two major challenges:
effectively representing unseen nodes and mitigating noisy or redundant graph
information. We propose GTGIB, a versatile framework that integrates Graph
Structure Learning (GSL) with Temporal Graph Information Bottleneck (TGIB). We
design a novel two-step GSL-based structural enhancer to enrich and optimize
node neighborhoods and demonstrate its effectiveness and efficiency through
theoretical proofs and experiments. The TGIB refines the optimized graph by
extending the information bottleneck principle to temporal graphs, regularizing
both edges and features based on our derived tractable TGIB objective function
via variational approximation, enabling stable and efficient optimization.
GTGIB-based models are evaluated to predict links on four real-world datasets;
they outperform existing methods in all datasets under the inductive setting,
with significant and consistent improvement in the transductive setting.

</details>


### [232] [Multimodal Quantum Vision Transformer for Enzyme Commission Classification from Biochemical Representations](https://arxiv.org/abs/2508.14844)
*Murat Isik,Mandeep Kaur Saggi,Humaira Gowher,Sabre Kais*

Main category: cs.LG

TL;DR: 本研究提出了一种新的多模态量子机器学习框架，通过整合四种生化模态（序列、量子电子描述符、图结构、2D图像），使用量子视觉变换器（QVT）提高了酶功能预测的准确性，达到了85.1%的准确率。


<details>
  <summary>Details</summary>
Motivation: 准确预测酶的功能是计算生物学中的一个主要挑战，特别是对于结构注释或序列同源性有限的酶。

Method: 该研究采用了一种新的多模态量子机器学习框架，该框架利用量子视觉变换器（QVT）作为骨干，并结合了特定于模态的编码器和一个统一的交叉注意力融合模块，集成了四种生化模态：蛋白质序列嵌入、量子电子描述符、分子图结构和2D分子图像表示。

Result: 实验结果表明，该多模态QVT模型达到了85.1%的top-1准确率，显著优于仅基于序列的基线方法和其他量子机器学习模型。

Conclusion: 该研究提出了一个多模态量子机器学习框架，通过整合蛋白质序列嵌入、量子电子描述符、分子图结构和2D分子图像表示，提高了酶的功能分类精度。实验结果表明，该框架的准确率达到85.1%，优于仅使用序列的方法和其他量子机器学习模型。

Abstract: Accurately predicting enzyme functionality remains one of the major
challenges in computational biology, particularly for enzymes with limited
structural annotations or sequence homology. We present a novel multimodal
Quantum Machine Learning (QML) framework that enhances Enzyme Commission (EC)
classification by integrating four complementary biochemical modalities:
protein sequence embeddings, quantum-derived electronic descriptors, molecular
graph structures, and 2D molecular image representations. Quantum Vision
Transformer (QVT) backbone equipped with modality-specific encoders and a
unified cross-attention fusion module. By integrating graph features and
spatial patterns, our method captures key stereoelectronic interactions behind
enzyme function. Experimental results demonstrate that our multimodal QVT model
achieves a top-1 accuracy of 85.1%, outperforming sequence-only baselines by a
substantial margin and achieving better performance results compared to other
QML models.

</details>


### [233] [Universal and Transferable Adversarial Attack on Large Language Models Using Exponentiated Gradient Descent](https://arxiv.org/abs/2508.14853)
*Sajib Biswas,Mao Nishino,Samuel Jacob Chacko,Xiuwen Liu*

Main category: cs.LG

TL;DR: 为了解决LLM的越狱攻击问题，我们提出了一种新的内在优化方法，通过优化的单热编码后缀来提高攻击效率和成功率，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在关键应用中的部署，确保其鲁棒性和安全对齐仍然是一个重大挑战。现有的对齐技术（如RLHF）在面对由精心设计的对抗性触发器实现的越狱攻击时效果不佳。现有的越狱方法在离散令牌空间搜索或连续嵌入直接优化方面效率低下，且后者对专有模型不可行，前者存在复杂性和有效性降低的问题。

Method: 提出了一种内在优化方法，利用指数梯度下降和Bregman投影直接优化松弛单热编码的对抗性后缀令牌，确保令牌的优化单热编码始终在概率单纯形内。

Result: 该方法在五种开源LLM和四个用于评估越狱方法的对抗行为数据集上进行了评估，取得了比三种最先进基线更高的成功率和更快的收敛速度。此外，研究还成功生成了对多个提示有效的通用对抗性后缀，并证明了优化后缀在不同LLM之间具有可转移性。

Conclusion: 该研究提出了一种内在优化方法，通过指数梯度下降和Bregman投影直接优化松弛单热编码的对抗性后缀令牌，以应对大型语言模型（LLM）的越狱攻击。该方法能有效攻破多个广泛使用的LLM，并在成功率和收敛速度上优于现有基线方法。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, ensuring their robustness and safety alignment remains a major
challenge. Despite the overall success of alignment techniques such as
reinforcement learning from human feedback (RLHF) on typical prompts, LLMs
remain vulnerable to jailbreak attacks enabled by crafted adversarial triggers
appended to user prompts. Most existing jailbreak methods either rely on
inefficient searches over discrete token spaces or direct optimization of
continuous embeddings. While continuous embeddings can be given directly to
selected open-source models as input, doing so is not feasible for proprietary
models. On the other hand, projecting these embeddings back into valid discrete
tokens introduces additional complexity and often reduces attack effectiveness.
We propose an intrinsic optimization method which directly optimizes relaxed
one-hot encodings of the adversarial suffix tokens using exponentiated gradient
descent coupled with Bregman projection, ensuring that the optimized one-hot
encoding of each token always remains within the probability simplex. We
provide theoretical proof of convergence for our proposed method and implement
an efficient algorithm that effectively jailbreaks several widely used LLMs.
Our method achieves higher success rates and faster convergence compared to
three state-of-the-art baselines, evaluated on five open-source LLMs and four
adversarial behavior datasets curated for evaluating jailbreak methods. In
addition to individual prompt attacks, we also generate universal adversarial
suffixes effective across multiple prompts and demonstrate transferability of
optimized suffixes to different LLMs.

</details>


### [234] [Compute-Optimal Scaling for Value-Based Deep RL](https://arxiv.org/abs/2508.14881)
*Preston Fu,Oleh Rybkin,Zhiyuan Zhou,Michal Nauman,Pieter Abbeel,Sergey Levine,Aviral Kumar*

Main category: cs.LG

TL;DR: 在深度强化学习中，为了在有限的计算资源下实现最优的样本效率，需要仔细权衡模型容量和更新-数据（UTD）比率。研究发现，与小模型不同，大模型在面对“TD过拟合”现象时表现出更强的鲁棒性，能够有效利用更大的批处理大小，这为优化大规模强化学习训练提供了关键见解。


<details>
  <summary>Details</summary>
Motivation: 随着模型增大和训练成本的增加，在扩大训练规模时，如何以计算最优的方式（即每单位计算量获得最大性能）变得日益重要。尽管语言模型在此方面已有充分研究，但强化学习（RL）领域的研究相对较少。

Method: 本文通过实证研究，分析了在线、基于值的深度强化学习中计算扩展的两个主要方面：模型容量和更新-数据（UTD）比率。在固定的计算预算下，研究了资源如何在模型大小和UTD之间分配以最大化样本效率，并识别了“TD过拟合”现象。

Result: 研究结果表明，模型大小、批处理大小和UTD之间存在复杂的相互作用。特别是，文章识别出“TD过拟合”现象：增加批处理大小会损害小模型的Q函数准确性，但在大模型中不存在此问题，从而允许在大规模训练中有效利用大批处理大小。根据这些发现，论文提出了选择批处理大小和UTD以优化计算使用的指导方针。

Conclusion: 本研究为深度强化学习中的计算最优扩展提供了基于实证分析的指导原则，解决了监督学习领域已有研究但强化学习领域关注不足的问题。

Abstract: As models grow larger and training them becomes expensive, it becomes
increasingly important to scale training recipes not just to larger models and
more data, but to do so in a compute-optimal manner that extracts maximal
performance per unit of compute. While such scaling has been well studied for
language modeling, reinforcement learning (RL) has received less attention in
this regard. In this paper, we investigate compute scaling for online,
value-based deep RL. These methods present two primary axes for compute
allocation: model capacity and the update-to-data (UTD) ratio. Given a fixed
compute budget, we ask: how should resources be partitioned across these axes
to maximize sample efficiency? Our analysis reveals a nuanced interplay between
model size, batch size, and UTD. In particular, we identify a phenomenon we
call TD-overfitting: increasing the batch quickly harms Q-function accuracy for
small models, but this effect is absent in large models, enabling effective use
of large batch size at scale. We provide a mental model for understanding this
phenomenon and build guidelines for choosing batch size and UTD to optimize
compute usage. Our findings provide a grounded starting point for
compute-optimal scaling in deep RL, mirroring studies in supervised learning
but adapted to TD learning.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [235] [Using Frame Randomization to Mitigate Errors in Quantum Optimization](https://arxiv.org/abs/2508.14142)
*Rachel E. Johnson,Joshua A. Job,Steve Adachi*

Main category: quant-ph

TL;DR: 框架随机化是一种有前途的量子设备错误缓解技术。


<details>
  <summary>Details</summary>
Motivation: 对近期的含噪声量子设备至关重要，其中一种有前途的技术是框架随机化。

Method: 将随机旋转门插入量子电路以在保持酉性和深度的同时减少误差。

Result: 与无随机化（2.63±0.068）和无噪声模拟器（5.676±0.006）相比，随机编译和 Pauli 帧随机化的预期极值能量分别为 5.25±0.145 和 4.08±0.36。

Conclusion: 两种随机化方法均可用于纠错，其中 Pauli 帧随机化在能量计算方面表现更好。

Abstract: Error mitigation is essential for near-term quantum devices, and one
promising technique is frame randomization. This method inserts random twirling
gates into a circuit to reduce errors while preserving unitarity and depth. We
apply frame randomization to the quantum approximate optimization algorithm
(QAOA) with $p=1$ on a superconducting quantum circuit system, demonstrating
its potential to improve energy calculations. Specifically, we investigate the
use of QAOA to calculate the lowest energy state of a frustrated Ising ring
system and compare the results of randomized circuits generated using two
different randomized techniques. Our results show that both methods can
mitigate errors, with expected extremal energy values of $5.25\pm0.145$ and
$4.08\pm0.36$, for Randomized Compilation and Pauli frame randomization
respectively, compared to $2.63\pm0.068$ without randomization and
$5.676\pm0.006$ with a noiseless simulator.

</details>


### [236] [Trace-Based Reconstruction of Quantum Circuit Dataflow in Surface Codes](https://arxiv.org/abs/2508.14533)
*Theodoros Trochatos,Christopher Kang,Andrew Wang,Frederic T. Chong,Jakub Szefer*

Main category: quant-ph

TL;DR: 本研究提出了一种名为 TraceQ 的新框架，它使用量子计算中的“访问轨迹”来离线重建量子电路。研究发现，这些轨迹可以准确地揭示程序的结构，并且只需一次执行即可恢复整个程序。


<details>
  <summary>Details</summary>
Motivation: 量子计算的实际应用依赖于采用纠错措施的容错设备。表面码是一种有前途的量子纠错码，适用于大规模量子计算。然而，表面码的容错量子计算 (FTQC) 仍有改进空间。

Method: 本研究提出了一种名为 TraceQ 的基于跟踪的重建框架，该框架通过观察每个跟踪条目处的补丁活动来重建量子电路数据流。该框架支持处理跟踪中固有的模糊性的启发式方法，并在各种合成容错量子基准上展示了其有效性。

Result: 本研究引入了访问轨迹的概念，即表面码中量子比特操作的时空模式。研究发现，访问轨迹可以揭示逻辑量子比特的交互时间、地点和方式。TraceQ 框架能够仅通过观察补丁活动来重建量子电路数据流。此外，研究证明，最小访问轨迹可用于以极高的准确度恢复子程序甚至整个量子程序。

Conclusion: 该研究表明，访问轨迹可以准确地恢复量子程序，只需单次程序执行的轨迹，并且可以在完全离线的情况下进行处理。

Abstract: Practical applications of quantum computing depend on fault-tolerant devices
that employ error correction. A promising quantum error-correcting code for
large-scale quantum computing is the surface code. For this code,
Fault-Tolerant Quantum Computing (FTQC) can be performed via lattice surgery,
i.e. merging and splitting of encoded qubit patches on a 2D grid. Lattice
surgery operations result in space-time patterns of activity that are defined
in this work as access traces. This work demonstrates that the access traces
reveal when, where, and how logical qubits interact. Leveraging this
formulation, this work further introduces TraceQ, a trace-based reconstruction
framework that is able to reconstruct the quantum circuit dataflow just by
observing the patch activity at each trace entry. The framework is supported by
heuristics for handling inherent ambiguity in the traces, and demonstrates its
effectiveness on a range of synthetic fault-tolerant quantum benchmarks. The
access traces can have applications in a wide range of scenarios, enabling
analysis and profiling of execution of quantum programs and the hardware they
run on. As one example use of TraceQ, this work investigates whether such
traces can act as a side channel through which an observer can recover the
circuit's structure and identify known subroutines in a larger program or even
whole programs. The findings show that indeed the minimal access traces can be
used to recover subroutines or even whole quantum programs with very high
accuracy. Only a single trace per program execution is needed and the
processing can be done fully offline. Along with the custom heuristics,
advanced subgraph matching algorithms used in this work enable a high rate of
locating the subroutines while executing in minimal time.

</details>


### [237] [Importance of Correlations for Neural Quantum States](https://arxiv.org/abs/2508.14152)
*Fabian Döschl,Annabelle Bohrdt*

Main category: quant-ph

TL;DR: 本研究使用可解释的神经网络和布尔函数理论，揭示了神经量子态（NQS）表示量子态需要高阶相关性，并分析了不同因素对相关性需求的影响，从而加深了对NQS内部结构的理解。


<details>
  <summary>Details</summary>
Motivation: 为了更好地理解神经量子态（NQS）的内部机制，特别是它们在表示量子多体波函数时对相关性的依赖性。

Method: 使用基于相关性的可解释神经网络架构，并基于布尔函数理论证明观察结果。

Result: 相关性神经网络表明，即使对于简单的乘积态，也需要高至所有系统大小的相关性阶数才能忠实地表示量子态。研究还揭示了相关性基是内部NQS结构的有效基，解释了对高阶相关性的需求，以及在约束希尔伯特空间中的潜在线性依赖性，并阐述了自旋基旋转与相关性基之间的联系。

Conclusion: 该研究通过使用基于相关性的可解释神经网络架构并利用布尔函数理论证明了我们的观察结果，从而研究了相关性在神经量子态（NQS）等量子态表示中的作用。研究结果表明，即使对于简单的乘积态，也需要高至所有系统大小的相关性阶数才能忠实地表示量子态。此外，研究还分析了激活函数、网络架构和参考基的选择如何影响相关性要求。

Abstract: Neural quantum states (NQS) have emerged as a powerful variational ansatz for
representing quantum many-body wave functions. Their internal mechanisms,
however, remain poorly understood. We investigate the role of correlations for
NQS-like quantum state representation by employing a correlation-based
interpretable neural network architecture and thereafter proving our
observations based on Boolean function theory. The correlator neural network
demonstrates that, even for simple product states, up to all system-size
correlation orders in the chosen computational basis are required to represent
a quantum state faithfully. We explain these observations using the Fourier
expansion, which reveals the correlator basis as the effective basis of the
internal NQS structure, the resulting necessity for high-order correlations,
potential linear dependencies in constrained Hilbert spaces, and connections
between spin basis-rotations and the correlator basis. Furthermore, we analyze
how activation functions, network architectures, and choice of reference basis
influence correlation requirements. Our results provide new insights and a
better understanding of the internal structure and requirements of NQS,
enabling a more systematic use of NQS in future research.

</details>


### [238] [Relativistic Quantum Thermal Machine: Harnessing Relativistic Effects to Surpass Carnot Efficiency](https://arxiv.org/abs/2508.14183)
*Tanmoy Pandit,Pritam Chattopadhyay,Kaustav Chatterjee,Varinder Singh*

Main category: quant-ph

TL;DR: 相对论性运动可以提升量子热机的效率，甚至在无温差时也能提取功。


<details>
  <summary>Details</summary>
Motivation: 探索在相对论性运动影响下，量子热机的行为，特别是其效率和功率输出，以及与传统热力学限制的关系。

Method: 研究了一个三能级马氏热机，其中系统-库相互作用通过Unruh-DeWitt耦合模型化，一个或两个库相对于工作介质进行相对论性运动。

Result: 运动诱导的相对论性运动导致了多普勒重塑，改变了能量交换速率，使得在有限功率下超越卡诺效率成为可能。同时，也改变了热机和制冷机运行之间的界限，即使在没有温度梯度的请况下，也能提取正功。

Conclusion: 相对论性运动可以作为一种真正的热力学资源，它能改变热机的运行界限，使其在没有温度梯度的情况下也能提取正功。

Abstract: We investigate a three-level maser quantum thermal machine in which the
system-reservoir interaction is modeled via Unruh-DeWitt type coupling, with
one or both reservoirs undergoing relativistic motion relative to the working
medium. Motion induces Doppler reshaping of the reservoir spectra, modifying
energy-exchange rates and enabling operation beyond the Carnot efficiency at
finite power. We numerically analyze families of efficiency-power curves and
extract the analytic form of a generalized Carnot bound, which recovers the
Carnot limit. In addition, Doppler reshaping alters the boundaries between
heat-engine and refrigerator operation, making it possible to extract positive
work even in the absence of a temperature gradient. These findings establish
relativistic motion as a genuine thermodynamic resource.

</details>


### [239] [Flag at origin: a modular fault-tolerant preparation for CSS codes](https://arxiv.org/abs/2508.14200)
*Diego Forlivesi,David Amaro*

Main category: quant-ph

TL;DR: 该研究提出了一种更有效的容错量子态制备方法，通过模块化构造和优化的标志性小工具，显著降低了资源消耗和量子电路规模，并在实验中取得了优于现有技术的成果。


<details>
  <summary>Details</summary>
Motivation: 现有容错制备电路存在资源消耗大和量子电路规模庞大的问题，因此需要更有效率的构造方法。

Method: 通过附加X和Z错误检测标志性小工具到二分图CX制备CSS状态的电路中，实现了容错制备。设计了一种发现任意距离的最优（或接近最优）标志性小工具的算法，并使用子集采样蒙特卡洛模拟和近似最大似然查找表解码来估计逻辑状态制备错误率。

Result: 所提出的模块化构造方法显著提高了资源效率，特别是在处理大规模量子纠错码时。在H2-1设备上的实验结果表明，该方法能够实现较低的逻辑SPAM错误率。

Conclusion: 该研究提出了一种用于CSS码的容错制备电路的模块化构造方法，特别是在距离任意大的情况下，能够生成比先前方法更具资源效率的电路，并且提出的标志性小工具可以跨不同的量子纠错码和容错子程序重复使用。在Quantinuum H2-1设备上，使用[[23,1,7]] Golay码制备$vertar{0}angle$态，实现了$3.3_{-2.4}^{+8.6} 	imes 10^{-4}$的逻辑SPAM错误率，优于先前报道的逻辑状态制备和物理$vert 0angle$态的SPAM错误率。

Abstract: Fault-tolerant (FT) preparation of diverse logical stabilizer states in
quantum error-correcting (QEC) codes is essential for FT computation. Existing
constructions of these FT circuits are often constrained by classical
computational resources or result in unnecessarily large quantum circuits. This
work introduces a modular construction for FT preparation circuits in CSS codes
of arbitrary distance, yielding significantly more resource-efficient circuits
than previous approaches, especially for the largest codes studied. The key
insight is that in bipartite CX circuits used to prepare CSS states, $X$ errors
propagate in one direction across the qubit partition, while $Z$ errors
propagate in the opposite direction. By appending $X$-detecting flag gadgets to
the first partition and $Z$-detecting flag gadgets to the second, the circuit
becomes FT. To manage the associated overhead, we propose an algorithm that
discovers optimal (or near-optimal) flag gadgets at any distance. These gadgets
are reusable across different QEC codes and FT subroutines, such as flag-based
QEC. We estimate the logical state preparation error using subset-sampling
Monte Carlo simulations at the circuit level, combined with approximate
maximum-likelihood look-up table decoding. On Quantinuum's H2-1 device,
preparation of the $\lvert\bar{0}\rangle$ state in the [[23,1,7]] Golay code
achieves a logical SPAM error rate of $3.3_{-2.4}^{+8.6} \times 10^{-4}$ with
an acceptance rate of $47.23(86)\%$. This surpasses (within $95\%$ confidence
intervals) the minimum SPAM error rate of $6.0(1.6) \times 10^{-4}$ for a
physical $\lvert 0\rangle$, as well as the best previously demonstrated logical
state preparations.

</details>


### [240] [Logical Error Rates for the Surface Code Under a Mixed Coherent and Stochastic Circuit-Level Noise Model Inspired by Trapped Ions](https://arxiv.org/abs/2508.14227)
*Tyler LeBlond,Peter Groszkowski,Justin G. Lietz,Christopher M. Seck,Ryan S. Bennink*

Main category: quant-ph

TL;DR: 在离子阱量子计算机上，表面代码的容错量子计算（FTQC）的逻辑错误率。


<details>
  <summary>Details</summary>
Motivation: 随着容错量子计算（FTQC）的出现，理解量子纠错码（QECC）的合理硬件实现中逻辑错误的来源至关重要。

Method: 通过蒙特卡洛技术从我们独立以相位敏感方式模拟的克利福德电路的潜在拟概率分布中进行采样，以估计包含非克利福德噪声源的硬件电路之后的逻辑泡利可观测量的期望值。

Result: 在接近和低于当前一代离子阱量子计算机的相干去相干率下，代码距离d=11的错误抑制得到验证，并且发现在此区域中逻辑错误率与类似的全随机模拟一致。在d=3-5处探索更高的去相干率，发现了关于所有三个逻辑泡利轴的增长相干旋转、相对于随机模拟的对角逻辑错误过程矩阵元素的增加以及去相干率阈值的降低的证据。

Conclusion: 该工作为早期容错量子计算（FTQC）过程的真实硬件仿真铺平了道路，例如FTQC指令集中的成员。

Abstract: With fault-tolerant quantum computing (FTQC) on the horizon, it is critical
to understand sources of logical error in plausible hardware implementations of
quantum error-correcting codes (QECC). In this work, we consider logical error
rates for the surface code implemented on a hypothetical grid-based trapped-ion
quantum charge-coupled device (QCCD) architecture. Specifically, we construct
logical channels for the idling surface code and examine its diamond error
under a mixed coherent and stochastic circuit-level noise model inspired by
trapped ions. We include the coherent dephasing noise that is known to
accumulate during physical qubit idling and transport in these systems,
determining idling and transport durations using the time-resolved output of
the trapped-ion surface code compiler (TISCC). To estimate expectation values
of logical Pauli observables following hardware circuits containing
non-Clifford sources of noise, we utilize a Monte Carlo technique to sample
from an underlying quasi-probability distribution of Clifford circuits that we
independently simulate in a phase-sensitive fashion. We verify error
suppression up to code distance $d=11$ at coherent dephasing rates near and
below those of current-generation trapped-ion quantum computers and find that
logical error rates align with those of analogous fully stochastic simulations
in this regime. Exploring higher dephasing rates at $d=3-5$, we find evidence
for growing coherent rotations about all three logical Pauli axes, increased
diagonal logical error process matrix elements relative to those of stochastic
simulations, and a reduced dephasing rate threshold. Overall, our work paves a
way toward realistic hardware emulation of small fault-tolerant quantum
processes, e.g., members of a FTQC instruction set.

</details>


### [241] [Excitonic Coupling and Photon Antibunching in Venus Yellow Fluorescent Protein Dimers: A Lindblad Master Equation Approach](https://arxiv.org/abs/2508.14233)
*Ian T. Abrahams*

Main category: quant-ph

TL;DR: Venus dimers exhibit strong excitonic coupling and photon antibunching, explained by rapid decoherence preventing long-lived coherence from affecting photon emission. This work provides a platform for studying protein photophysics and suggests potential for fluorescent protein-based qubits with cryogenic cooling, highlighting a "bioexciton motif" linking quantum properties and protein structure.


<details>
  <summary>Details</summary>
Motivation: To explain the coexistence of strong excitonic coupling and photon antibunching in Venus yellow fluorescent protein (YFP) dimers, which is a cryptic combination under prevailing theoretical models.

Method: The population dynamics of Venus dimers are modeled within a Lindblad master equation framework, justified by the separation of characteristic coupling, dephasing, and thermal relaxation rates.

Result: Simulations predict rapid decoherence, consistent with antibunching, indicating that coherence in both the excitonic and site bases is too short-lived to affect photon emission statistics.

Conclusion: Despite the absence of long-lived coherence, Venus dimers provide a tractable platform for probing evolutionary pressures on fluorescent protein photophysics and quantum dynamics. Cryogenic cooling could extend coherence lifetimes into the regime required for quantum gate operations, suggesting a route toward fluorescent protein-based qubits. The results highlight a structural design principle -- a "bioexciton motif" -- that links excitonic coupling, decoherence, and protein architecture, pointing to general rules by which biology both constrains and inspires quantum technologies.

Abstract: Strong excitonic coupling and photon antibunching have been observed together
in Venus yellow fluorescent protein (YFP) dimers -- a cryptic combination under
prevailing theoretical models. In 2019, Kim et al. demonstrated Davydov
splitting in Venus dimer circular dichroism (CD) spectra, revealing large
negative dimer coupling energy, while antibunching was confirmed by
antibunching-fluorescence correlation spectroscopy (AB/FCS fingerprinting). To
explain this coexistence, Venus dimer population dynamics are modeled here
within a Lindblad master equation framework, justified by the separation of
characteristic coupling, dephasing, and thermal relaxation rates. Simulations
predict rapid decoherence, consistent with antibunching, indicating that
coherence in both the excitonic and site bases is too short-lived to affect
photon emission statistics. Despite the absence of long-lived coherence, Venus
dimers provide a tractable platform for probing evolutionary pressures on
fluorescent protein photophysics and quantum dynamics. Cryogenic cooling could
extend coherence lifetimes into the regime required for quantum gate
operations, suggesting a route toward fluorescent protein-based qubits. More
broadly, the results highlight a structural design principle -- a "bioexciton
motif" -- that links excitonic coupling, decoherence, and protein architecture,
pointing to general rules by which biology both constrains and inspires quantum
technologies.

</details>


### [242] [A high-temperature limit penalizing high-frequency quantum fluctuations](https://arxiv.org/abs/2508.14262)
*Graeme Pleasance,Erik Aurell,Francesco Petruccione*

Main category: quant-ph

TL;DR: 本文在高频高温极限下，发现了量子布朗运动的退相干新机制。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索量子布朗运动模型在高频下的行为，并寻找新的经典化机制。

Method: 本文在新的高温极限下，利用欧姆谱密度，重新审视了Caldeira-Leggett量子布朗运动模型，并推导了退相干核的附加贡献。

Result: 我们发现了一个附加的退相干核贡献，它导致了一个马尔可夫主方程，该方程具有确定的Lindblad形式，揭示了一种新颖的高频量子涨落经典化机制。

Conclusion: 本文揭示了一种新的高频量子涨落经典化机制，并推导出一种马尔可夫主方程，该方程具有确定的Lindblad形式。

Abstract: We revisit the Caldeira-Leggett model of quantum Brownian motion with Ohmic
spectral density, and derive an additional contribution to the decoherence
kernel in a new high-temperature limit at arbitrarily large cut-off frequency.
This contribution reveals a novel mechanism for the classicalization of
high-frequency quantum fluctuations. We further demonstrate that it leads to a
Markovian master equation that is in guaranteed Lindblad form. Our approach
considers in detail the behavior of the decoherence kernel at both the initial
and final times of the process on the time scale of the bath memory.

</details>


### [243] [Inserting Planar-Measured Qubits into MBQC Patterns while Preserving Flow](https://arxiv.org/abs/2508.14671)
*Miriam Backens,Thomas Perez*

Main category: quant-ph

TL;DR: 本文研究了在测量基元量子计算（MBQC）中插入平面测量（如 YZ 测量）以保持计算的因果流。研究扩展了因果流的定义，推导了 YZ 插入保持因果流的条件，并展示了其与现有重写规则的关系，旨在实现更优化的 MBQC。


<details>
  <summary>Details</summary>
Motivation: 为了优化、混淆或路由 MBQC 模式，需要能够进行保持因果流的重写。之前的研究主要集中在减少量子比特数量或引入 Pauli 测量的重写上。本文旨在研究插入平面测量（特别是 YZ 测量）以保持因果流的重写方法，这对于实现通用 MBQC 是必需的。

Method: 本文扩展了因果流的定义，以支持 YZ 测量，并推导了 YZ 插入保持因果流的条件。此外，还推导了 YZ 插入到具有 gflow 或 Pauli 流的模式中的条件，并证明了“顶点分裂”或“邻居融合”规则可以从 YZ 插入和枢轴推导出来。

Result: 本文成功扩展了因果流的定义以包含 YZ 测量，并给出了 YZ 插入保持因果流的条件。研究还表明，YZ 插入可以推导出“顶点分裂”或“邻居融合”规则，并可推广到 XZ 插入。这些结果为 MBQC 和 ZX-演算中更有效的优化、混淆或路由提供了基础。

Conclusion: 本文研究了在测量基元量子计算（MBQC）中插入平面测量（ Bloch 球上由一对 Pauli 算子张成的平面中的任意测量）以保持计算的因果流。研究扩展了先前仅限于 XY 测量的因果流定义，以允许 YZ 测量，并推导了 YZ 插入保持因果流的条件。此外，还推导了 YZ 插入到具有 gflow 或 Pauli 流的模式中的条件，并展示了如何从 YZ 插入和枢轴推导出“顶点分裂”或“邻居融合”规则。这项工作有助于理解 MBQC 和 ZX-演算中保持流的重写的广泛性质，并能实现更有效的优化、混淆或路由。

Abstract: In the one-way model of measurement-based quantum computation (MBQC),
computation proceeds via single-qubit measurements on a resource state. Flow
conditions ensure that the overall computation is deterministic in a suitable
sense, and are required for efficient translation into quantum circuits.
Procedures that rewrite MBQC patterns -- e.g. for optimisation, or adapting to
hardware constraints -- thus need to preserve the existence of flow. Most
previous work has focused on rewrites that reduce the number of qubits in the
computation, or that introduce new Pauli-measured qubits. Here, we consider the
insertion of planar-measured qubits into MBQC patterns, i.e. arbitrary
measurements in a plane of the Bloch sphere spanned by a pair of Pauli
operators; such measurements are necessary for universal MBQC. We extend the
definition of causal flow, previously restricted to XY -measurements only, to
also permit YZ-measurements and derive the conditions under which a
YZ-insertion preserves causal flow. Then we derive conditions for YZ-insertion
into patterns with gflow or Pauli flow, in which case the argument
straightforwardly extends to XZ-insertions as well. We also show that the
'vertex splitting' or 'neighbour unfusion' rule previously used in the
literature can be derived from YZ-insertion and pivoting. This work contributes
to understanding the broad properties of flow-preserving rewriting in MBQC and
in the ZX-calculus more broadly, and it will enable more efficient
optimisation, obfuscation, or routing.

</details>


### [244] [Strong Confinement of a Nanoparticle in a Needle Paul Trap: Towards Matter-Wave Interferometry with Nanodiamonds](https://arxiv.org/abs/2508.14272)
*Peter Skakunenko,Daniel Folman,Yaniv Bar-Haim,Ron Folman*

Main category: quant-ph

TL;DR: 本研究提出了一种用于实现宏观粒子量子干涉的实验方案，重点是使用纳米金刚石和改进的针式保罗囚阱。该方案旨在探索量子力学和广义相对论的交叉领域。研究团队成功实现了 40 kHz 的高囚阱频率，为未来的量子引力实验奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 为了检验量子力学（QM）和广义相对论（GR）这两大物理学支柱，特别是量子力学的空间叠加原理以及量子力学与广义相对论的接口（例如引力的量子化）。

Method: 利用嵌入自旋的纳米金刚石作为粒子，并通过 Stern-Gerlach 力实现时空的闭合回路。

Result: 实现了对被囚禁粒子的强约束，该约束对于角约束、精确聚焦至关重要，并可能有利于深度冷却。囚阱频率达到了 40 kHz，显著优于现有技术。

Conclusion: 该论文设计了一种新型针式保罗囚阱，并结合有效的充电方法（静电喷雾），实现了高达 40 kHz 的囚阱频率，该频率是现有技术水平的两倍以上。

Abstract: Quantum mechanics (QM) and General relativity (GR), also known as the theory
of gravity, are the two pillars of modern physics. A matter-wave interferometer
with a massive particle, can test numerous fundamental ideas, including the
spatial superposition principle - a foundational concept in QM - in completely
new regimes, as well as the interface between QM and GR, e.g., testing the
quantization of gravity. Consequently, there exists an intensive effort to
realize such an interferometer. While several paths are being pursued, we focus
on utilizing nanodiamonds as our particle, and a spin embedded in the
nanodiamond together with Stern-Gerlach forces, to achieve a closed loop in
space-time. There is a growing community of groups pursuing this path [1]. We
are posting this technical note (as part of a series of seven such notes), to
highlight our plans and solutions concerning various challenges in this
ambitious endeavor, hoping this will support this growing community. In this
work, we achieve strong confinement of a levitated particle, which is crucial
for angular confinement, precise positioning, and perhaps also advantageous for
deep cooling. We designed a needle Paul trap with a controllable distance
between the electrodes, giving rise to a strong electric gradient. By combining
it with an effective charging method - electrospray - we reach a trap frequency
of up to 40 kHz, which is more than twice the state of the art. We believe that
the designed trap could become a significant tool in the hands of the community
working towards massive matter-wave interferometry. We would be happy to make
more details available upon request.

</details>


### [245] [Non-representable quantum measures](https://arxiv.org/abs/2508.14326)
*Alexandru Chirvasitu*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Grade-$d$ measures on a $\sigma$-algebra $\mathcal{A}\subseteq 2^X$ over a
set $X$ are generalizations of measures satisfying one of a hierarchy of weak
additivity-type conditions initially introduced as interference operators in
quantum mechanics. Every signed polymeasure $\lambda$ on $(X,\mathcal{A})^d$
produces a grade-$d$ measure as its diagonal
$\widetilde{\lambda}(A):=\lambda(A,\cdots,A)$, and we prove that as soon as
$d\ge 2$ measures (as opposed to polymeasures) do not suffice: the separate
$\sigma$-additivity of a $\lambda$ producing $\mu=\widetilde{\lambda}$ cannot,
generally, be amplified to global $\sigma$-additivity. This amends a result in
the literature, asserting the contrary in case $d=2$.

</details>


### [246] [Free-Particle State Realized via Optimal Filtering in Optomechanics: Implications for Gravity-Induced Entanglement](https://arxiv.org/abs/2508.14337)
*Ryotaro Fukuzumi,Kosei Hatakeyama,Daisuke Miki,Kazuhiro Yamamoto*

Main category: quant-ph

TL;DR: 通过光学力学系统中的动量压缩和量子滤波，实现了超越标准量子极限的腔体状态，并显著增强了引力诱导纠缠信号，为验证引力的量子性质提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 研究在光学力学系统中，通过连续测量、反馈控制和量子滤波来分析机械反射镜的条件量子态，特别关注动量压缩和引力诱导纠缠。

Method: 通过连续测量、反馈控制和量子滤波分析光学力学系统中机械反射镜的条件量子态，识别出动量被压缩到标准量子极限之外的状态。

Result: 在特定条件下，动量压缩可以超越标准量子极限，最优滤波可实现自由粒子状态。在双光学力学系统中，动量压缩增强了共模与差模之间的差异，并增加了位置不确定性，从而扩大了叠加态的空间范围，显著提高了引力诱导纠缠的信号。

Conclusion: 本文提出的方法为利用光学力学平台验证引力的量子本质提供了新的策略，通过动量压缩和最优滤波实现自由粒子性质的腔体状态，并显著增强了由引力诱导的纠缠信号。

Abstract: We analyze the conditional quantum state of a mechanical mirror in an
optomechanical system under continuous measurement, feedback control, and
quantum filtering. Our study identifies a regime in which the mirror's momentum
is squeezed beyond the standard quantum limit achievable through appropriate
tuning of the homodyne detection angle. We show that in this regime, optimal
filtering effectively realizes a free-particle-like state. Notably, when
applied to a setup involving two optomechanical systems, this phenomenon
significantly boosts the signal of gravity-induced entanglement (GIE) because
the momentum squeezing not only accentuates the difference between the common
mode and the differential mode but, when the purity is high, also increases the
position uncertainty due to the uncertainty principle, thereby enlarging the
spatial extent of the superposition. Our results provide new insights into
strategies for verifying the quantum nature of gravity using optomechanical
platforms.

</details>


### [247] [From Chiral Topological Dynamics to Chiral Topological Amplification: Real vs Imaginary Parameters in a Hermitian Bosonic Chain](https://arxiv.org/abs/2508.14560)
*Kiran Babasaheb Estake,T. R. Vishnu,Dibyendu Roy*

Main category: quant-ph

TL;DR: 提出QBH模型，发现实数参数下存在莫比乌斯相，纯虚数参数下则表现出चiral放大效应，揭示了拓扑与动力学行为的关联。


<details>
  <summary>Details</summary>
Motivation: 探索新的量子模型及其拓扑和动力学特性，特别是研究参数性质（实数或纯虚数）对模型行为的影响，并揭示拓扑相位与放大行为之间的联系。

Method: 通过分析厄米二次玻色子模型（QBH）的动力学矩阵，研究其在实数和纯虚数参数下的拓扑和动力学现象。分析方法包括解析计算Loschmidt振幅和计算动力学拓扑序参量，并与nSSH2和nSSH1模型进行比较。

Result: 在实数参数下，QBH模型展现出莫比乌斯相，具有分数缠绕数，且其动力学可自然复现非厄米时间演化。在纯虚数参数下，莫比乌斯相消失，但存在亚格相关的चiral放大效应，该效应源于动力学矩阵的非平凡拓扑。

Conclusion: 该研究提出了一个厄米二次玻色子模型（QBH），并探讨了其在实数和纯虚数参数下的动力学特性。在实数参数下，QBH模型与四个独立的nSSH2模型等效，继承了其拓扑相和能谱，包括无厄米对应物的莫比乌斯相。在纯虚数参数下，QBH模型与nSSH1模型等效，仅支持平凡和非平凡两种拓扑相，莫比乌斯相消失，但会出现亚格相关的चiral放大效应。

Abstract: We propose a Hermitian quadratic bosonic model (QBH) whose dynamical matrix
exhibits distinct topological and dynamical phenomena depending on whether the
hopping and pairing amplitudes are real or purely imaginary. In the
real-parameter regime, the dynamical matrix is unitarily equivalent to four
decoupled copies of the sublattice-symmetric non-Hermitian Su-Schrieffer-Heeger
(nSSH2) model, thereby inheriting its topological phases and energy
spectrum-including the M\"obius phase, a gapless topological phase with
fractional winding number, having no Hermitian counterpart. We show that the
dynamics generated by the QBH Hamiltonian naturally reproduce non-Hermitian
time evolution, without invoking nonlinear Schr\"odinger dynamics or ad hoc
normalization. It is demonstrated by analytically calculating the Loschmidt
amplitude and computing the dynamical topological order parameter under
periodic boundary conditions, which displays a distinct chiral response in the
M\"obius phase. In contrast, when the hopping and pairing terms are taken to be
purely imaginary, the dynamical matrix becomes unitarily equivalent to a
different version of the sublattice-symmetric non-Hermitian
Su-Schrieffer-Heeger (nSSH1) model that supports only two topological phases:
trivial and non-trivial, and the M\"obius phase disappears. The latter system
exhibits sublattice-dependent chiral amplification under open boundary
conditions. We show that this amplification arises from the non-trivial
topology of the dynamical matrix, establishing a clear link between topological
phase and amplification behavior in the imaginary-parameter regime.

</details>


### [248] [Quasiprobability Thermodynamic Uncertainty Relation](https://arxiv.org/abs/2508.14354)
*Kohei Yoshimura,Ryusuke Hamazaki*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We derive a quantum extension of the thermodynamic uncertainty relation where
dynamical fluctuations are quantified by the Terletsky-Margenau-Hill
quasiprobability, a quantum generalization of the classical joint probability.
The obtained inequality plays a complementary role to existing quantum
thermodynamic uncertainty relations, focusing on observables' change rather
than exchange of charges through jumps and respecting initial coherence.
Quasiprobabilities show anomalous behaviors that are forbidden in classical
systems, such as negativity; we reveal that such behaviors are necessary to
reduce dissipation beyond classical limitations and show that they are stronger
requirements than that the state has quantum coherence. To illustrate these
statements, we employ a model that can exhibit a dissipationless heat current,
which would be prohibited in classical systems; we construct a state that has
much coherence but does not lead to a dissipationless current due to the
absence of anomalous behaviors in quasiprobabilities.

</details>


### [249] [Stoquasticity is not enough: towards a sharper diagnostic for Quantum Monte Carlo simulability](https://arxiv.org/abs/2508.14382)
*Arman Babakhani,Armen Karakashian*

Main category: quant-ph

TL;DR: 提出使用消失几何相位（VGP）作为诊断量子蒙特卡洛（QMC）模拟符号问题的标准，并提供了一系列量化符号问题严重性的工具。


<details>
  <summary>Details</summary>
Motivation: 为了克服量子蒙特卡洛（QMC）方法在模拟量子多体系统时面临的符号问题限制。

Method: 通过消失几何相位（VGP）来诊断QMC的可模拟性，并分析了识别VGP哈密顿量类别的复杂性。此外，还提出了一系列VGP启发的诊断方法，并用于分析单位变换下平均符号的缩放行为。

Result: 识别出VGP哈密顿量类别，确定了识别该类别的复杂性（包含困难和易于识别的情况），并展示了VGP相对于随机性作为诊断工具的优势。提出的VGP启发式诊断方法在分析平均符号的缩放行为方面显示了其数学上的有效性。

Conclusion: 该研究提出了使用消失几何相位（VGP）作为诊断量子蒙特卡洛（QMC）模拟可处理性的新标准，并提出了一系列VGP启发的诊断方法来量化符号问题的严重性。

Abstract: Quantum Monte Carlo (QMC) methods are powerful tools for simulating quantum
many-body systems, yet their applicability is limited by the infamous sign
problem. We approach this challenge through the lens of Vanishing Geometric
Phases (VGP) \cite{Hen_2021}, introducing it as a `geometric' criterion for
diagnosing QMC simulability. We characterize the class of VGP Hamiltonians, and
analyze the complexity of recognizing this class, identifying both hard and
efficiently identifiable cases. We further highlight the practical advantage of
the VGP criterion by exhibiting specific Hamiltonians that are readily
identified as sign-problem-free through VGP, yet whose stoquasticity is
difficult to ascertain. These examples underscore the efficiency and sharpness
of VGP as a diagnostic tool compared to stoquasticity-based heuristics. Beyond
classification, we propose a family of VGP-inspired diagnostics that serve as
quantitative indicators of sign problem severity. While exact evaluation of
these quantities is generically intractable, we demonstrate their mathematical
power in performing scaling analysis for the average sign under unitary
transformations. Our results provide both a conceptual foundation and practical
tools for understanding and mitigating the sign problem.

</details>


### [250] [Finite-Dimensional Quantum Systems under the Fourth Law of Thermodynamics](https://arxiv.org/abs/2508.14389)
*Rohit Kishan Ray*

Main category: quant-ph

TL;DR: 这项研究介绍了固定拉格朗日乘子（FLM）方法，用于解决最陡熵增（SEA）方程的动力学问题，该方法在分析量子系统（包括两量子比特系统）的演化以及确保非线性量子理论中的无信号条件方面表现出有效性。


<details>
  <summary>Details</summary>
Motivation: SEA（最陡熵增）ansatz被认为是热力学第四定律，支配着系统从非平衡态到最大熵平衡态的不可逆演化。SEA在第二定律的基础上统一了力学和热力学。然而，由于SEA的非线性性质，其运动方程的精确解稀少。

Method: 开发了固定拉格朗日乘子（FLM）方法作为一种近似分析工具，适用于两能级和更高维度的量子系统。使用量子行走，一种通用的计算模型，将FLM应用于单分量N能级系统的SEA动力学分析和求解。提供了N能级布洛赫矢量参数化的通用框架，包括N=3的解析根和N=4的完整参数化，以及在此表示中解析计算算子迹的方法。

Result: FLM方法的近似解与全数值模拟高度一致，尤其是在最大熵产生区域，这与SEA的预测一致。N=3的解析根和N=4的完整参数化为扩展SEA分析到复合系统（特别是两量子比特系统）提供了基础。SEA框架固有的无信号特性使其在非线性量子理论中保持局部性。

Conclusion: SEA框架在非线性量子理论中满足无信号条件，并且SEA的运动方程对于可分离和纠缠复合系统（例如贝尔态）都得到保持，证明SEA具有局部性，并为开放和封闭量子系统中的退相干建模提供了稳健的基础。

Abstract: The Steepest Entropy Ascent (SEA) ansatz, recently recognized as the fourth
law of thermodynamics, governs the irreversible evolution of a system from a
non-equilibrium state toward a unique maximum-entropy equilibrium. SEA builds
upon the second law to unify mechanics and thermodynamics. Due to its nonlinear
nature, exact solutions to the SEA equation of motion are scarce. To address
this, the Fixed Lagrange Multiplier (FLM) method is developed as an approximate
analytical tool, applicable to both two-level and higher-dimensional quantum
systems. Using quantum walks, a universal computation model, the study applies
FLM to analyze and solve the SEA dynamics for single-component $N-$level
systems. The approximate FLM solutions show strong agreement with full
numerical simulations, particularly in regions of maximum entropy production
consistent with SEA predictions. To extend SEA analysis to composite systems,
especially two-qubit systems, the work provides a general framework for
$N-$level Bloch vector parametrization. It includes analytical roots for $N=3$
and a complete parametrization for $N=4$, along with a method for analytically
computing operator traces in this representation. Finally, the study examines
the no-signaling condition in nonlinear quantum theories. While nonlinearity
often implies faster-than-light signaling, the SEA framework inherently
respects no-signaling. The equation of motion for both separable and entangled
(e.g., Bell-diagonal) composites confirms that SEA maintains locality and
provides a robust foundation for modeling decoherence in both open and closed
quantum systems. (Abridged for ArXiv)

</details>


### [251] [Non-Equilibrium Criticality-Enhanced Quantum Sensing with Superconducting Qubits](https://arxiv.org/abs/2508.14409)
*Hao Li,Yaoling Yang,Yun-Hao Shi,Zheng-An Wang,Ziting Wang,Jintao Li,Yipeng Zhang,Kui Zhao,Yue-Shan Xu,Cheng-Lin Deng,Yu Liu,Wei-Guo Ma,Tian-Ming Li,Jia-Chi Zhang,Cai-Ping Fang,Jia-Cheng Song,Hao-Tian Liu,Si-Yun Zhou,Zheng-He Liu,Bing-Jie Chen,Gui-Han Liang,Xiaohui Song,Zhongcheng Xiang,Kai Xu,Kaixuan Huang,Abolfazl Bayat,Heng Fan*

Main category: quant-ph

TL;DR: Quantum sensing can be enhanced using Stark-Wannier systems, which combine quantum criticality and non-equilibrium dynamics for improved precision across various parameters, even with basic measurements. This was shown on a 9-qubit superconducting device.


<details>
  <summary>Details</summary>
Motivation: To overcome limitations of existing quantum-enhanced precision methods (complex probe preparation/measurement for quantum criticality, narrow parameter regimes for non-equilibrium probes) by unifying these approaches.

Method: The study unifies quantum-enhanced precision approaches using a Stark-Wannier localization platform, implemented on a 9-qubit superconducting quantum device. The researchers explored its performance in different phases (extended, critical, localized) using both single- and double-excitation subspaces. They achieved near-Heisenberg-limited precision by combining outcomes from distinct evolution times, despite using only computational-basis measurements.

Result: The study demonstrated near-Heisenberg-limited precision using a Stark-Wannier system on a superconducting device. The probe's performance was significantly better in the extended phase compared to the localized regime, and the approach achieved enhanced precision over a wide parameter range with simple measurement requirements.

Conclusion: Stark-Wannier systems serve as versatile platforms for quantum sensing, offering enhanced precision over a wide range of parameters by combining criticality and non-equilibrium dynamics, even with simple measurement requirements.

Abstract: Exploiting quantum features allows for estimating external parameters with
precisions well beyond the capacity of classical sensors, a phenomenon known as
quantum-enhanced precision. Quantum criticality has been identified as a
resource for achieving such enhancements with respect to the probe size.
However, they demand complex probe preparation and measurement and the
achievable enhancement is ultimately restricted to narrow parameter regimes. On
the other hand, non-equilibrium probes harness dynamics, enabling
quantum-enhanced precision with respect to time over a wide range of parameters
through simple probe initialization. Here, we unify these approaches through a
Stark-Wannier localization platform, where competition between a linear
gradient field and particle tunneling enables quantum-enhanced sensitivity
across an extended parameter regime. The probe is implemented on a 9-qubit
superconducting quantum device, in both single- and double-excitation
subspaces, where we explore its performance in the extended phase, the critical
point and the localized phase. Despite employing only computational-basis
measurements we have been able to achieve near-Heisenberg-limited precision by
combining outcomes at distinct evolution times. In addition, we demonstrate
that the performance of the probe in the entire extended phase is significantly
outperforming the performance in the localized regime. Our results highlight
Stark-Wannier systems as versatile platforms for quantum sensing, where the
combination of criticality and non-equilibrium dynamics enhances precision over
a wide range of parameters without stringent measurement requirements.

</details>


### [252] [Adaptive Interpolating Quantum Transform: A Quantum-Native Framework for Efficient Transform Learning](https://arxiv.org/abs/2508.14418)
*Gekko Budiutama,Shunsuke Daimon,Hirofumi Nishi,Ryui Kaneko,Tomi Ohtsuki,Yu-ichiro Matsushita*

Main category: quant-ph

TL;DR: AIQT is a new framework for quantum machine learning that uses a trainable unitary to interpolate between quantum transforms, allowing for efficient learning with fewer parameters and inheriting advantages from constituent transforms.


<details>
  <summary>Details</summary>
Motivation: Deep variational quantum circuits require a large number of trainable parameters that grows with both qubit count and circuit depth, often rendering training infeasible.

Method: AIQT defines a trainable unitary that interpolates between quantum transforms, such as the Hadamard and quantum Fourier transforms.

Result: AIQT achieves high performance with minimal parameter count, offering a scalable and interpretable alternative to deep variational circuits.

Conclusion: AIQT achieves high performance with minimal parameter count, offering a scalable and interpretable alternative to deep variational circuits.

Abstract: Machine learning on quantum computers has attracted attention for its
potential to deliver computational speedups in different tasks. However, deep
variational quantum circuits require a large number of trainable parameters
that grows with both qubit count and circuit depth, often rendering training
infeasible. In this study, we introduce the Adaptive Interpolating Quantum
Transform (AIQT), a quantum-native framework for flexible and efficient
learning. AIQT defines a trainable unitary that interpolates between quantum
transforms, such as the Hadamard and quantum Fourier transforms. This approach
enables expressive quantum state manipulation while controlling parameter
overhead. It also allows AIQT to inherit any quantum advantages present in its
constituent transforms. Our results show that AIQT achieves high performance
with minimal parameter count, offering a scalable and interpretable alternative
to deep variational circuits.

</details>


### [253] [All incompatible sets of measurements can generate Buscemi nonlocality](https://arxiv.org/abs/2508.14421)
*Andrés F. Ducuara,Patryk Lipka-Bartosik,Cristian E. Susa,Paul Skrzypczyk*

Main category: quant-ph

TL;DR: 不兼容的测量集在有量子输入的情况下也能产生非局域性，其程度受不兼容度限制。


<details>
  <summary>Details</summary>
Motivation: 研究不兼容测量集与贝尔非局域性之间的关系，特别是探索在何种条件下不兼容测量集能够产生非局域性。

Method: 提出了一种称为“广义测量集”的新对象，用于统一研究包含量子输入的任何场景。

Result: 证明了在包含量子测量的扩展贝尔场景中，所有不兼容测量集都能产生非局域性，并发现其最大非局域性受不兼容程度的限制。

Conclusion: 在扩展的贝尔场景下，所有不兼容的测量集都能产生非局域性，即使是那些在标准场景下只能产生局域相关性的测量集。这种非局域性受测量集不兼容程度的限制。

Abstract: The presence of Bell-nonlocality in the correlations arising from measuring
spatially-separated systems guarantees that the sets of measurements used are
necessarily incompatible. Not all sets of incompatible measurements can however
lead to Bell-nonlocality, as there exist incompatible sets of measurements
which can only produce local correlations. In this work we prove that all sets
of incompatible measurements are nevertheless able to generate nonlocality in
an extended Bell scenario where quantum, instead of classical, measurement
inputs are considered. In particular, this holds true for all
incompatible-local sets of measurements and, consequently, shows that these
sets of measurements posses a form of hidden nonlocality which can be revealed
in such a scenario. We furthermore prove that the maximum amount of nonlocality
that can be extracted in such a way is limited by the degree of incompatibility
of the given set of measurements, thus effectively establishing an achievable
upper bound. In order to obtain these results, we introduce a new object which
we term a \emph{generalised set of measurements}, which provides a unifying way
to study any scenario involving quantum inputs.

</details>


### [254] [Multi-player conflict avoidance through entangled quantum walks](https://arxiv.org/abs/2508.14456)
*Honoka Shiratori,Tomoki Yamagami,Etsuo Segawa,Takatomo Mihana,André Röhm,Ryoichi Horisaki*

Main category: quant-ph

TL;DR: 本研究利用量子行走（QW）提出新方法，有效解决了集体决策中的三方冲突问题。


<details>
  <summary>Details</summary>
Motivation: 集体决策制定中存在决策冲突问题，现有方法难以解决三方及以上冲突。

Method: 提出了一种利用量子行走（QW）的新方法来消除决策冲突。

Result: 成功消除了三方决策冲突。

Conclusion: 本研究提出了一种利用量子行走（QW）完全消除三方决策冲突的新方法，并证明了其在集体决策制定中的有效性。

Abstract: Quantum computing has the potential to solve complex problems faster and more
efficiently than classical computing. It can achieve speedups by leveraging
quantum phenomena like superposition, entanglement, and tunneling. Quantum
walks (QWs) form the foundation for many quantum algorithms. Unlike classical
random walks, QWs exhibit quantum interference, leading to unique behaviors
such as linear spreading and localization. These properties make QWs valuable
for various applications, including universal computation, time series
prediction, encryption, and quantum hash functions. One emerging application of
QWs is decision making. Previous research has used QWs to model human decision
processes and solve multi-armed bandit problems. This paper extends QWs to
collective decision making, focusing on minimizing decision-conflict cases
where multiple agents choose the same option, leading to inefficiencies like
traffic congestion or overloaded servers. Prior research using quantum
interference has addressed two-player conflict avoidance but struggled with
three-player scenarios. This paper proposes a novel method using QWs to
entirely eliminate decision conflicts in three-player cases, demonstrating its
effectiveness in collective decision making.

</details>


### [255] [Preparation of Hamming-Weight-Preserving Quantum States with Log-Depth Quantum Circuits](https://arxiv.org/abs/2508.14470)
*Yu Li,Guojing Tian,Xiaoyu He,Xiaoming Sun*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Quantum state preparation is a critical task in quantum computing,
particularly in fields such as quantum machine learning, Hamiltonian
simulation, and quantum algorithm design. The depth of preparation circuit for
the most general state has been optimized to approximately optimal, but the
log-depth appears only when the number of ancillary qubits reaches exponential.
Actually, few log-depth preparation algorithms assisted by polynomial ancillary
qubits have been come up with even for a certain kind of non-uniform state. We
focus on the Hamming-Weight-preserving states, defined as
$|\psi_{\text{H}}\rangle = \sum_{\text{HW}(x)=k} \alpha_x |x\rangle$, which
have leveraged their strength in quantum machine learning. Especially when
$k=2$, such Hamming-Weight-preserving states correspond to simple undirected
graphs and will be called graph-structured states. Firstly, for the $n$-qubit
general graph-structured states with $m$ edges, we propose an algorithm to
build the preparation circuit of $O(\log n)$-depth with $O(m)$ ancillary
qubits. Specifically for the $n$-qubit tree-structured and grid-structured
states, the number of ancillary qubits in the corresponding preparation
circuits can be optimized to zero. Next we move to the preparation for the HWP
states with $k\geq 3$, and it can be solved in $O(\log{{n \choose k}})$-depth
using $O\left({n \choose k}\right)$ ancillary qubits, while the size keeps
$O\big( {n \choose k} \big)$. These depth and size complexities, for any $k
\geq 2$, exactly coincide with the lower bounds of $\Omega (\log{{n \choose
k}})$-depth and $\Omega ({n \choose k})$-size that we prove lastly, which
confirms the near-optimal efficiency of our algorithms.

</details>


### [256] [Application of resource theory based on free Clifford+kT computation to early fault-tolerant quantum computing](https://arxiv.org/abs/2508.14546)
*Yuya O. Nakagawa,Yasunori Lee*

Main category: quant-ph

TL;DR: 本研究介绍了Clifford+kT鲁棒性，用于评估量子计算资源使用效率，尤其是在FTQC早期阶段，并提供了优化量子比特和T门使用的实用建议。


<details>
  <summary>Details</summary>
Motivation: 在FTQC早期，可用的逻辑量子比特和T门数量有限，因此优化量子资源使用至关重要。本研究旨在研究在仅使用k个T门和无限量Clifford门约束下，模拟通用量子态的成本。

Method: 提出Clifford+kT鲁棒性概念，并推导了其下界，通过数值计算评估了其对魔态张量积等关键资源态的鲁棒性。

Result: 推导了Clifford+kT鲁棒性的下界，评估了其对魔态张量积等关键资源态的鲁棒性，并展示了Clifford+kT态相比于稳定器态在降低采样成本方面的效率。

Conclusion: 本研究提出了Clifford+kT鲁棒性概念，用于评估使用Clifford+kT态的量子线路模拟效率，并推导了其下界，为早期容错量子计算（FTQC）中的量子资源优化提供了实际指导。

Abstract: Recent advances in quantum hardware are bringing fault-tolerant quantum
computing (FTQC) closer to reality. In the early stage of FTQC, however, the
numbers of available logical qubits and high-fidelity $T$ gates remain limited,
making it crucial to optimize the quantum resource usage. In this work, we aim
to study the simulation cost of general quantum states under the constraint
that only $k$ $T$ gates can be used, alongside an unlimited number of Clifford
gates. Inspired by the notion of robustness of magic (RoM) which quantifies the
cost of quantum-circuit simulation using stabilizer states ($k = 0$), we
introduce its generalization, which we call Clifford+$kT$ robustness, treating
Clifford+$kT$ states as free resources. We explore theoretical properties of
Clifford$+kT$ robustness and in particular derive a lower bound that reveals
the (in)efficiency of quantum-circuit simulation using Clifford$+kT$ states.
Through numerical computations, we also evaluate Clifford+$kT$ robustness for
key resource states for universal quantum computation, such as tensor products
of the magic states. Our results allow to assess the sampling-cost reduction
achieved by the use of Clifford+$kT$ states instead of stabilizer states,
providing practical guidance for efficient resource usage in the early-FTQC
era.

</details>


### [257] [Polaronic Effect in High-Harmonic Generation](https://arxiv.org/abs/2508.14633)
*Gabriel Caceres-Aravena,Dieter Bauer*

Main category: quant-ph

TL;DR: 电子-声子耦合通过在SSH链中引入新态和增加跃迁来增强高谐波产生。


<details>
  <summary>Details</summary>
Motivation: 研究电子-声子耦合对SSH链中高谐波产生的影响。

Method: 使用紧束缚近似和量子谐振子模拟电子-声子耦合的SSH链中的高谐波产生。

Result: 电子-声子耦合修改了本征能量谱，在现有能隙中引入了新态，并通过增加允许的跃迁来提高谐波产额。

Conclusion: 电子-声子耦合通过引入新态和增加允许跃迁来增强高谐波产生。

Abstract: We investigate High-Harmonic Generation (HHG) in the Su-Schrieffer-Heeger
(SSH) chain with electron-phonon coupling modeled via the Holstein interaction.
The system dynamics are simulated using the tight-binding approximation, with
local phonons approximated as quantum harmonic oscillators. Phononic degrees of
freedom significantly expand the Hilbert space dimension. This interaction
modifies the eigenenergy spectrum by introducing new states within previously
existing gaps, enhancing the harmonic yield through additional allowed
transitions.

</details>


### [258] [On spurious fixed points in iterative maximum likelihood reconstruction for quantum tomography](https://arxiv.org/abs/2508.14549)
*Florian Oberender*

Main category: quant-ph

TL;DR: Maximum likelihood iteration for quantum tomography might not always converge to the correct solution. A new check is proposed, and the method is linked to factorized gradient descent.


<details>
  <summary>Details</summary>
Motivation: To address the practical observation that maximum likelihood iteration in quantum tomography converges reliably, despite theoretical doubts about guaranteed convergence to a true solution.

Method: The paper constructs examples of spurious fixed points to demonstrate the lack of guaranteed convergence. It then proposes a criterion based on first-order optimality conditions and generalizes the algorithm.

Result: Demonstrated that maximum likelihood iteration does not guarantee convergence to the true solution in general, provided a criterion for checking convergence, and showed the algorithm is equivalent to factorized gradient descent.

Conclusion: The paper shows that maximum likelihood iteration in quantum tomography does not guarantee convergence to the true solution in general. It provides a criterion based on first-order optimality conditions to check for convergence and generalizes the algorithm, showing its equivalence to factorized gradient descent.

Abstract: Maximum likelihood iteration is one of the most commonly used reconstruction
algorithms in quantum tomography. The main appeal of the method is that it is
easy to implement and that it converges reliably to a physically meaningful
density matrix in practice. Contradicting these practical observations, we will
show that convergence to a true solution is not guaranteed in general by
constructing examples for spurious fixed points. To deal with this newly found
problem, we then provide a criterion based on first order optimality conditions
to check if the result of the algorithm is indeed the desired solution.
Furthermore, we generalize the algorithm and show that it is equivalent to
factorized gradient descent.

</details>


### [259] [Electrically pumped ultrabright entangled photons on chip](https://arxiv.org/abs/2508.14566)
*Xu-Feng Jiao,Ming-Yang Zheng,Yi-Hang Chen,Bo Cao,Xina Wang,Yang Liu,Cheng-Ao Yang,Xiu-Ping Xie,Chao-Yang Lu,Zhi-Chuan Niu,Qiang Zhang,Jian-Wei Pan*

Main category: quant-ph

TL;DR: 研究人员开发了一种集成的电泵浦偏振纠缠光子源，实现了高生成率和高保真度，可用于高速量子密钥分发、卫星量子通信和量子计量等应用。


<details>
  <summary>Details</summary>
Motivation: 尽管在集成光学平台（如薄膜铌酸锂）方面取得了进展，但可扩展、高性能的芯片级EPS仍然难以实现。

Method: 通过混合集成分布式反馈激光器和集成有周期性极化铌酸锂波导、分束器和偏振旋转器组合器的薄膜铌酸锂芯片。

Result: 实现了高带宽（73 nm）和高纠缠对产生率（4.5*10^10 对/秒/毫瓦）的EPS，在频率相关模式下，偏振纠缠的贝尔态保真度高于96%。

Conclusion: 该研究展示了一种新型的电泵浦、无需后选择的偏振纠缠光子源（EPS），通过混合集成分布式反馈激光器和薄膜铌酸锂芯片实现，该芯片集成了周期性极化铌酸锂波导、分束器和偏振旋转器组合器。

Abstract: Entangled photon sources (EPS) are essential for quantum science and
technology. Despite advancements in integrated optical platforms like thin-film
lithium niobate, a scalable, high-performance, chip-scale EPS has remained
elusive. We address this by demonstrating an electrically pumped,
post-selection-free polarization-EPS, achieved through hybrid integration of a
distributed feedback laser with thin-film lithium niobate chip which integrates
periodically poled lithium niobate waveguides, beam splitter, and polarization
rotator combiner. By injecting current into the chip, we realize a
high-performance EPS with a bandwidth of 73 nm and an entanglement pair
generation rate of 4.5*10^10 pairs/s/mW. The polarization entanglement shows
Bell-state fidelities above 96% across frequency-correlated modes. This
compact, integrated EPS enables key applications, including high-speed quantum
key distribution via wavelength division multiplexing, satellite-based quantum
communication, and entanglement-based quantum metrology.

</details>


### [260] [Dirac, Schroedinger, and Maxwell equations in scalar and vector field quantum mechanics](https://arxiv.org/abs/2508.14583)
*Boris Chichkov*

Main category: quant-ph

TL;DR: This paper re-examines relativistic quantum mechanics using a photon-like dispersion relation. It offers a new derivation of the Dirac equation and proposes that wave-particle duality can be viewed as electromagnetic wave-particle duality, with de Broglie waves analogous to electromagnetic waves.


<details>
  <summary>Details</summary>
Motivation: The paper aims to reconsider the quantum theory of relativistic particles using a photon-like dispersion relation, offering a new derivation of the Dirac equation and a redefinition of wave-particle duality.

Method: The paper uses a first quantization technique, similar to Schroedinger and Dirac, based on a photon-like dispersion relation derived from Einstein's special relativity's energy conservation equation. It first discusses scalar quantum mechanics and then introduces vector field quantum mechanics.

Result: The paper provides a very simple new derivation of the Dirac equation and derives basic equations for vector-field quantum mechanics, similar to the source-free Maxwell equations. It suggests that a particle's de Broglie wave can be considered as a transversal electromagnetic wave.

Conclusion: The paper redefines wave-particle duality as electromagnetic wave-particle duality and analyzes the relationships between scalar and vector field quantum mechanics.

Abstract: The quantum theory of relativistic particles, based on the first quantization
technique similar to that used by Schroedinger and Dirac in formulating quantum
mechanics, is reconsidered on the basis of a photon-like dispersion relation
corresponding to the energy conservation equation of Einstein's special
relativity. First, scalar quantum mechanics of particles operating with their
wave functions is discussed. Using the first quantization of the photon-like
dispersion relation, very simple new derivation of the Dirac equation is given.
Then, vector field quantum mechanics is introduced, which defines vector fields
associated with the relativistic particle. Basic equations for the vector-field
quantum mechanics, similar to the source-free Maxwell equations, are derived.
Following these equations, the particle's de Broglie wave can be considered as
the transversal electromagnetic wave. Therefore, the wave-particle duality can
be redefined as the electromagnetic wave-particle duality. Relationships
between the scalar and vector field quantum mechanics are analyzed.

</details>


### [261] [Entanglement-enhanced imaging through scattering media](https://arxiv.org/abs/2508.14616)
*Chloé Vernière,Raphaël Guitter,Baptiste Courme,Hugo Defienne*

Main category: quant-ph

TL;DR: 本研究提出了一种新颖的量子纠缠成像技术，能够克服复杂介质的散射效应，成功传输和重建图像，而无需进行复杂的逆散射过程。该技术通过利用量子纠缠的光子相关性，在相同条件下优于传统成像方法。


<details>
  <summary>Details</summary>
Motivation: 复杂介质（如散射层）会扰乱光线，模糊图像，影响从天文学到显微镜学的各种应用。传统的计算算法和波前整形方法虽然可以逆转这种混合，但会受到许多实际因素的限制，例如可控模式数量有限、噪声、损耗以及前向模型不准确等。因此，这些方法仅在严格控制的条件下有效，限制了它们在现实世界成像中的应用。

Method: 本研究利用量子纠缠的特性，特别是纠缠光子在多个测量基之间保持相关性的能力，通过定制散射层来访问其中一个基，从而在复杂介质中传输图像。通过在输出端进行符合检测来重建图像。

Result: 本研究提出了一种利用量子纠缠传输图像通过复杂介质的方法，该方法通过定制散射层诱导的激光无序来访问一个特定的测量基，并通过符合检测在输出端重建图像。实验结果表明，该方法能够成功传输并重建图像，而经典的成像方法在这种条件下只能产生散斑图样，无法提取任何对象信息。这证明了该量子纠缠方法在成像方面的优越性。

Conclusion: 本研究提出了一种基于量子纠缠的方法，利用纠缠光子在复杂介质中传输图像，无需进行逆散射过程。该方法通过定制散射层诱导的激光无序来实现，并通过符合检测在输出端重建隐藏在介质后的任意物体的纠缠编码图像。与经典成像方法不同，该方法在相同条件下能够成功成像，而经典成像只能产生毫无对象信息的散斑图样。本研究开辟了在复杂环境中进行量子增强成像和通信的新途径。

Abstract: Scattering in complex media scrambles light, obscuring images and hindering
applications ranging from astronomy to microscopy. While computational
algorithms and wavefront shaping can, in principle, reverse this mixing by
exploiting the near-linear nature of light propagation, in practice the
inversion process is highly susceptible to limitations such as the finite
number of controllable modes, noise, losses, and inaccuracies in the forward
model. Consequently, these approaches are only effective under tightly
controlled conditions, limiting their impact on real-world imaging. Here, we
present a quantum entanglement-based approach that transmits images through
complex media without inverting the scattering process. This method exploits a
fundamental property of quantum entanglement: the preservation of photon
correlations across multiple measurement bases. By tailoring the optical
disorder induced by a scattering layer to access one such basis,
entanglement-encoded images of arbitrary objects hidden behind the medium can
be reconstructed via coincidence detection at the output. In contrast,
classical imaging under identical conditions fails, producing only speckle
patterns with no object information. Our work introduces a fundamentally new
approach to imaging through complex media by leveraging the unique properties
of quantum entanglement beyond simple optical correlations. This opens up new
avenues for quantum-enhanced imaging and communication in challenging
environments.

</details>


### [262] [Quantum reservoir computing induced by controllable damping](https://arxiv.org/abs/2508.14621)
*Emanuele Ricci,Francesco Monzani,Luca Nigro,Enrico Prati*

Main category: quant-ph

TL;DR: 提出了一种通过阻尼算法实现可调且稳定的非幺正演化，用于量子回声状态网络，从而在量子计算机上实现稳健且可扩展的随机计算。


<details>
  <summary>Details</summary>
Motivation: 量子水库计算作为一种有前途的机器学习范式，在近期的量子设备上处理时间序列数据，但缺乏确保可调且稳定的非幺正演化的方法。

Method: 提出了一种通过对水库中的每个量子比特应用受控旋转来诱导阻尼的算法，以实现可调、电路级别的零状态幅度放大，并防止因重复的中间电路测量而导致的信息丢失。

Result: 该算法在容错量子硬件上实现了稳健且可扩展的量子随机计算，并且量子比特间的量子相关性在内存保持方面提供了改进。

Conclusion: 量子门控回声状态网络通过引入阻尼实现可调且稳定的非幺正演化，从而在容错量子硬件上实现稳健且可扩展的量子随机计算。

Abstract: Quantum reservoir computing has emerged as a promising machine learning
paradigm for processing temporal data on near-term quantum devices, as it
allows for exploiting the large computational capacity of the qubits without
suffering from typical issues that occur when training a variational quantum
circuit. In particular, quantum gate-based echo state networks have proven
effective for learning when the evolution of the reservoir circuit is
non-unital. Nonetheless, a method for ensuring a tunable and stable non-unital
evolution of the circuit was still lacking. We propose an algorithm for
inducing damping by applying a controlled rotation to each qubit in the
reservoir. It enables tunable, circuit-level amplitude amplification of the
zero state, maintaining the system away from the maximally mixed state and
preventing information loss caused by repeated mid-circuit measurements. The
algorithm is inherently stable over time as it can, in principle, process
arbitrarily long input sequences, well beyond the coherence time of individual
qubits, by inducing an arbitrary damping on each qubit. Moreover, we show that
quantum correlations between qubits provide an improvement in terms of memory
retention, underscoring the potential utility of employing a quantum system as
a computational reservoir. We demonstrate, through typical benchmarks for
reservoir computing, that such an algorithm enables robust and scalable quantum
random computing on fault-tolerant quantum hardware.

</details>


### [263] [High-fidelity realisation of CNOT gate in Majorana-based optical platform](https://arxiv.org/abs/2508.14641)
*Jia-Kun Li,Kai Sun,Ze-Yan Hao,Jia-He Liang,Jiannis K. Pachos,Lucy Byles,Jin-Shi Xu,Yong-Jian Han,Chuan-Feng Li,Guang-Can Guo*

Main category: quant-ph

TL;DR: 该研究在光子平台上利用马约拉纳零模成功实现了高保真度 CNOT 量子门，提高了量子计算的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 量子计算在可扩展性方面面临噪声和退相干的挑战，而拓扑量子计算提供了一种潜在的解决方案。本研究旨在利用马约拉纳零模的拓扑特性，在光子平台上实现高保真度的 CNOT 量子门。

Method: 利用三个支持马约拉纳零模的 Kitaev 链对两个逻辑量子比特进行编码，并通过链内和链间编织操作来实现 CNOT 门。

Result: 实验成功实现了 CNOT 量子门，保真度超过 0.992。尽管在非相互作用的光子环境中，拓扑编码无法提供完全的拓扑保护，但它对实验中主要的噪声和退相干效应表现出自然的鲁棒性。

Conclusion: 该实验实现了使用光子平台模拟的马约拉纳零模的鲁棒 CNOT 量子门，保真度超过 0.992，显著提高了量子门性能，是实现可扩展量子计算的关键一步，并凸显了光子平台在实现高保真度量子门方面的潜力。

Abstract: We present the experimental realisation of a robust CNOT quantum gate using
Majorana zero modes simulated on a photonic platform. Three Kitaev chains
supporting Majorana zero modes at their endpoints are used to encode two
logical qubits, and both intra-chain and inter-chain braiding operations are
performed to implement the CNOT gate. While the topological encoding of quantum
information in Majorana fermions does not offer full topological protection in
our non-interacting photonic setting, it nevertheless exhibits a natural
resilience to the dominant noise and decoherence effects present in the
experiment. Consequently, the fidelity of the CNOT gate is significantly
enhanced, surpassing 0.992 and addressing a key limitation in the path toward
scalable quantum computation. These results represent a major advancement in
topological quantum computing with Majorana fermions and underscore the
potential of photonic platforms for realising high-fidelity quantum gates.

</details>


### [264] [Deutsch-Jozsa and Bernstein-Vazirani algorithm using single-particle discrete-time quantum walk](https://arxiv.org/abs/2508.14659)
*Ravi Sangwan,Vikas Ramaswamy,Henry Sukumar,Gudapati Naresh Raghava*

Main category: quant-ph

TL;DR: Efficient quantum computing algorithms demonstrated with quantum walk and optical framework.


<details>
  <summary>Details</summary>
Motivation: To introduce an efficient implementation of the Deutsch-Jozsa and Bernstein-Vazirani algorithm using single-particle discrete-time quantum walk and provide a detailed optical framework for photonic implementation.

Method: The Deutsch-Jozsa and Bernstein-Vazirani algorithms are implemented using single-particle discrete-time quantum walk. A detailed optical framework with specific optical components is provided for photonic quantum walk, utilizing both polarization and path degrees of freedom.

Result: Improved resource efficiency while maintaining the exponential speedup characteristic of both algorithms.

Conclusion: The paper demonstrates efficient implementations of the Deutsch-Jozsa and Bernstein-Vazirani algorithms using single-particle discrete-time quantum walk, with improved resource efficiency and maintained exponential speedup.

Abstract: The paper introduces an efficient implementation of the Deutsch-Jozsa and
Bernstein-Vazirani algorithm using the single-particle discrete-time quantum
walk. We also provide a detailed optical framework with specific optical
components to achieve these implementations in the photonic quantum walk scheme
by simultaneously exploiting both polarization and path degrees of freedom.
These implementations demonstrate improved resource efficiency while
maintaining the exponential speedup characteristic of both algorithms. This
work contributes to the growing field of universal quantum computing using
single particle discrete-time quantum walk.

</details>


### [265] [Analytical bounds for decoy-state quantum key distribution with discrete phase randomization](https://arxiv.org/abs/2508.14664)
*Zhaohui Liu,Ahmed Lawey,Mohsen Razavi*

Main category: quant-ph

TL;DR: 本研究为更易于实现的离散相位随机化QKD协议（BB84和MDI-QKD）提供了安全的密钥生成速率的解析边界，该方法比现有数值方法更简洁且结果准确。


<details>
  <summary>Details</summary>
Motivation: 为了解决在量子密钥分发（QKD）协议安全证明中，实际操作中难以实现连续相位随机化的问题，以及现有基于DPR的证明方法依赖于计算密集型数值优化的问题，本研究旨在提供一种更简洁的解析方法。

Method: 本研究推导了在离散相位随机化（DPR）条件下，BB84和测量设备无关QKD协议的密钥生成速率的解析边界。

Result: 本研究得到的解析边界在相关区域内与繁琐的数值优化方法结果高度吻合，证明了该方法的有效性。

Conclusion: 本研究为基于离散相位随机化（DPR）的量子密钥分发（QKD）协议（特别是BB84和测量设备无关QKD）在实际应用中更易于实现的情况下，提供了安全密钥生成速率的解析边界，并验证了该解析方法在相关区域内与繁琐的数值优化方法结果高度吻合。

Abstract: We analyze the performance of quantum key distribution (QKD) protocols that
rely on discrete phase randomization (DPR). For many QKD protocols that rely on
weak coherent pulses (WCPs), continuous phase randomization is assumed, which
simplifies the security proofs for such protocols. However, it is challenging
to achieve such a perfect phase randomization in practice. As an alternative,
we can select a discrete set of global phase values for WCPs, but we need to
redo the security analysis for such a source. While security proofs
incorporating DPR have been established for several QKD protocols, they often
rely on computationally intensive numerical optimizations. To address this
issue, in this study, we derive analytical bounds on the secret key generation
rate of BB84 and measurement-device-independent QKD protocols in the DPR
setting. Our analytical bounds closely match the results obtained from more
cumbersome numerical methods in the regions of interest.

</details>


### [266] [String Diagrams for Defect-Based Surface Code Computing](https://arxiv.org/abs/2508.14672)
*Mateusz Kupper,Dominic Horsman,Chris Heunen,Niel de Beaudrap*

Main category: quant-ph

TL;DR: Surface codes use lattice surgery or defect braiding for gates. This paper uses ZX-calculus to describe defect braiding, creating a new calculus KNOT and related subtheories. These tools formally analyze braiding operations.


<details>
  <summary>Details</summary>
Motivation: To provide a formal description and analysis of defect braiding operations in surface codes, similar to how ZX-calculus describes lattice surgery, for program design and optimization.

Method: Introduces a graphical calculus KNOT for defect braiding in surface codes, formalizing its description via a fragment of ZX-calculus called the (0, pi)-fragment. It also defines a subtheory of KNOT using a doubling construction to encompass standard braiding techniques, with distinct semantics based on the (0, pi)-fragment of ZX diagrams.

Result: Defines the KNOT calculus and its subtheories, demonstrating their soundness and completeness for the (0, pi)-fragment of ZX diagrams, offering a formal framework to analyze defect braiding procedures.

Conclusion: The paper formalizes defect braiding in surface codes using ZX-calculus, introducing a graphical calculus called KNOT and its subtheories based on standard encoding techniques. These formalisms are sound and complete for analyzing defect braiding procedures within the (0, pi)-fragment of ZX diagrams.

Abstract: Surface codes are a popular choice for implementing fault-tolerant quantum
computing. Two-qubit gates may be realised in these codes using only
nearest-neighbour interactions, either by lattice surgery or by braiding
defects around each other. The effect of lattice surgery operations may be
simply described using the ZX-calculus: a graphical language that has proven
effective for program design and optimisation. In this work, we formalise a
similar description via the ZX-calculus of defect braiding, as it is
conventionally described. We define a graphical calculus KNOT, denoting the
logical effects (in the absence of byproduct operations) of defect braiding in
surface codes: we show how these effects may be described via a fragment of
ZX-calculus which we call the (0, pi)-fragment. We then use a doubling
construction to define a subtheory of KNOT, more specialised to standard
encoding techniques in the defect braiding literature. Within this subtheory,
we encompass standard braiding techniques by families of ribbon-like and
tangle-like diagrams, each with semantics distinct from KNOT, in terms of the
(0, pi)-fragment of ZX diagrams (again in the absence of byproducts). These
subtheories may be used interoperably, and are each sound and complete for the
(0, pi)-fragment of ZX diagrams. This provides a starting point to use the
formal diagrammatics to analyse the operational effects of defect braiding
procedures.

</details>


### [267] [A Classification Program for Nonlocality Paradoxes of Three Qubits](https://arxiv.org/abs/2508.14673)
*Nadish de Silva,Santanil Jana,Ming Yin*

Main category: quant-ph

TL;DR: 本研究扩展了三比特非局域性悖论的知识，引入了新的悖论族，并为它们的完全分类奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 非局域性是量子行为的标志，也是量子通信和计算的资源。理解三比特系统中的强非局域性对于利用量子优势至关重要。

Method: 引入了新的三比特悖论无限族，并证明了这些悖论可以穷尽满足合理正则性条件的悖论。同时，给出了一个高度奇异的悖论示例，并对寻找新的奇异悖论施加了限制。

Result: 发现了新的三比特非局域性悖论，并为完整分类提供了理论基础。证明了新悖论的穷尽性，并对奇异悖论和状态空间进行了探索。

Conclusion: 该工作为三比特非局域性悖论的完整分类提供了详细的路线图，并提出猜想所有悖论都涉及单参数族状态。

Abstract: Nonlocality is a quintessential signature of nonclassical behaviour and a
resource for quantum advantages in communication and computation. The
paradoxical correlations witnessed by strong nonlocality undergird the standard
probabilistic form of nonlocality and provide optimal advantages in numerous
informational tasks.
  Three-qubit systems are the simplest ones that admit strong nonlocality.
Abramsky et al. (TQC, 2017) established the existence of an infinite family of
three-qubit paradoxes, beyond the well-known GHZ paradox, which exhibited a
novel conditional structure.
  In this work, we introduce several new infinite families of three-qubit
paradoxes and articulate a detailed roadmap towards the complete classification
of all three-qubit nonlocality paradoxes. In particular, we prove that our
paradoxes exhaust all those satisfying reasonable regularity conditions. We
give an example of a highly exotic paradox and place constraints on the search
for new exotic paradoxes. We conjecture that all paradoxes must involve states
from a one-parameter family and provide significant evidence in support of this
conjecture.

</details>


### [268] [Contributions to the Theory of Clifford-Cyclotomic Circuits](https://arxiv.org/abs/2508.14674)
*Linh Dinh,Neil J. Ross*

Main category: quant-ph

TL;DR: 本研究改进了量子电路合成算法，减少了所需辅助位数，并将算法推广到更广泛的门集。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是为了改进现有的 Clifford-环分圆电路合成算法，降低所需的辅助位数量，并将其适用范围扩展到更广泛的 $n$ 值。

Method: 本文首先分析了 Clifford-环分圆门集 $\mathcal{G}_n$ 的性质，特别是当 $n=2^k$ 时，证明了仅需 $k-3$ 个辅助位即可合成表示酉矩阵 $U$ 的电路，并给出了 $k=4$ 时的最小辅助位证明。然后，本文将该算法推广到 $n=3\times 2^k$ ($k\geq 3$) 的情况。

Result: 本文成功地将合成 $U$ 所需的辅助位数量从 $k-2$ 减少到 $k-3$（当 $n=2^k, k
eq 4$ 时），并且证明了 $k=4$ 时的 $k-3$ 是最小的。同时，将合成算法扩展到了 $n=3\times 2^k$ ($k\geq 3$) 的情况。

Conclusion: 本文改进了现有的合成算法，证明了当 $n=2^k$ 且 $k
eq 4$ 时，仅需 $k-3$ 个辅助位即可合成量子电路，并且该数量对于 $k=4$ 是最小的。此外，本文还将现有合成算法扩展到了 $n=3	imes 2^k$ 且 $k
eq 3$ 的情况。

Abstract: Let $n$ be a positive integer divisible by 8. The Clifford-cyclotomic gate
set $\mathcal{G}_n$ consists of the Clifford gates, together with a
$z$-rotation of order $n$. It is easy to show that, if a circuit over
$\mathcal{G}_n$ represents a unitary matrix $U$, then the entries of $U$ must
lie in $\mathcal{R}_n$, the smallest subring of $\mathbb{C}$ containing $1/2$
and $\mathrm{exp}(2\pi i/n)$. The converse implication, that every unitary $U$
with entries in $\mathcal{R}_n$ can be represented by a circuit over
$\mathcal{G}_n$, is harder to show, but it was recently proved to be true when
$n=2^k$. In that case, $k-2$ ancillas suffice to synthesize a circuit for $U$,
which is known to be minimal for $k=3$, but not for larger values of $k$. In
the present paper, we make two contributions to the theory of
Clifford-cyclotomic circuits. Firstly, we improve the existing synthesis
algorithm by showing that, when $n=2^k$ and $k\geq 4$, only $k-3$ ancillas are
needed to synthesize a circuit for $U$, which is minimal for $k=4$. Secondly,
we extend the existing synthesis algorithm to the case of $n=3\cdot 2^k$ with
$k\geq 3$.

</details>


### [269] [Trapping and cooling of nanodiamonds in a Paul trap under ultra-high vacuum: Towards matter-wave interferometry with massive objects](https://arxiv.org/abs/2508.14687)
*Omer Feldman,Ben Baruch Shultz,Maria Muretova,Or Dobkowski,Yonathan Japha,David Grosswasser,Ron Folman*

Main category: quant-ph

TL;DR: 利用纳米金刚石和斯特恩-格拉赫力进行物质波干涉，以检验量子力学和引力理论，并展示了囚禁和冷却纳米金刚石的技术细节。


<details>
  <summary>Details</summary>
Motivation: 为了在新的领域检验量子力学中的空间叠加原理等基本概念，并探索量子力学与广义相对论的接口，例如检验引力的量子化。

Method: 利用包含自旋的纳米金刚石作为测试粒子，结合斯特恩-格拉赫力，在时空中实现闭合的物质波干涉仪。

Result: 成功实现了在10^-8 mbar真空度下对纳米金刚石的囚禁，并实现了亚开尔文温度的冷却，证明了纳米金刚石在强激光照射下的稳定性。

Conclusion: 该工作详细介绍了在10^-8 mbar的真空度下对纳米金刚石进行囚禁的技术，该真空度足以实现短时斯特恩-格拉赫干涉仪。同时，文章也描述了冷却到亚开尔文温度的技术细节，并证明了在强1560 nm激光照射下纳米金刚石仍能被囚禁在陷阱中。

Abstract: Quantum mechanics (QM) and General relativity (GR), also known as the theory
of gravity, are the two pillars of modern physics. A matter-wave interferometer
with a massive particle can test numerous fundamental ideas, including the
spatial superposition principle - a foundational concept in QM - in previously
unexplored regimes. It also opens the possibility of probing the interface
between QM and GR, such as testing the quantization of gravity. Consequently,
there exists an intensive effort to realize such an interferometer. While
several approaches are being explored, we focus on utilizing nanodiamonds with
embedded spins as test particles which, in combination with Stern-Gerlach
forces, enable the realization of a closed-loop matter-wave interferometer in
space-time. There is a growing community of groups pursuing this path [1]. We
are posting this technical note (as part of a series of seven such notes), to
highlight our plans and solutions concerning various challenges in this
ambitious endeavor, hoping this will support this growing community. In this
work we detail the trapping of a nanodiamond at 10^-8 mbar, which is good
enough for the realization of a short-duration Stern-Gerlach interferometer. We
describe in detail the cooling we have performed to sub-Kelvin temperatures,
and demonstrate that the nanodiamond remains confined within the trap even
under high-intensity 1560 nm laser illumination. We would be happy to make
available more details upon request.

</details>


### [270] [Quantum teleportation over thermal microwave network](https://arxiv.org/abs/2508.14691)
*W. K. Yam,S. Gandorfer,F. Fesquet,M. Handschuh,K. E. Honasoge,A. Marx,R. Gross,K. G. Fedorov*

Main category: quant-ph

TL;DR: 在4 K的温度下，通过嘈杂的微波通道，在两个稀释制冷机之间成功实现了微波相干态的量子隐形传态，保真度超过了经典通信阈值，证明了分布式超导架构的可行性。


<details>
  <summary>Details</summary>
Motivation: 为了解决需要千毫开尔文温度才能运行的超导量子电路在大规模微波量子网络中的应用挑战。

Method: 通过在4开尔文（K）的温度下，在两个空间分离的稀释制冷机之间，通过一个热微波通道，成功实现了微波相干态的量子隐形传态，并使用了高斯算符形式主义对该协议进行了建模，其中包含了损耗和噪声。

Result: 在1 K和4 K的温度下，量子隐形传态的保真度分别达到了72.3 ± 0.5%和59.9 ± 2.5%，超过了不可克隆阈值和经典通信阈值。

Conclusion: 我们的结果证明了分布式超导架构的实验可行性，并推动了嘈杂量子网络在各种频率下的实际应用。

Abstract: Quantum communication in the microwave regime is set to play an important
role in distributed quantum computing and hybrid quantum networks. However,
typical superconducting quantum circuits require millikelvin temperatures for
operation, which poses a significant challenge for largescale microwave quantum
networks. Here, we present a solution to this challenge by demonstrating the
successful quantum teleportation of microwave coherent states between two
spatially-separated dilution refrigerators over a thermal microwave channel
with a temperature of $4$ K. We distribute two-mode squeezed states over the
noisy channel and employ the resulting quantum entanglement for quantum
teleportation of coherent states with fidelities of $72.3 \pm 0.5 ~\%$ at $1$ K
and $59.9 \pm 2.5 ~\%$ at $4$ K, exceeding the no-cloning and classical
communication thresholds, respectively. We successfully model the teleportation
protocol using a Gaussian operator formalism that includes losses and noise.
This analysis shows that the teleportation infidelity mainly stems from a
parasitic heating of the cold quantum nodes due to the hot network connection.
Our results prove the experimental feasibility of distributed superconducting
architectures and motivate practical applications of noisy quantum networks in
various frequency regimes.

</details>


### [271] [Design of high-efficiency UHV loading of nanodiamonds into a Paul trap: Towards Matter-Wave Interferometry with Massive Objects](https://arxiv.org/abs/2508.14722)
*Rafael Benjaminov,Sela Liran,Or Dobkowski,Yaniv Bar-Haim,Michael Averbukh,Ron Folman*

Main category: quant-ph

TL;DR: 该研究旨在通过纳米金刚石实现量子力学与广义相对论的接口测试，回顾了现有技术并提出新的加载方法，以提高加载效率并应对实验挑战。


<details>
  <summary>Details</summary>
Motivation: 为了在新的领域测试量子力学的空间叠加原理，以及探索量子力学与广义相对论（引力理论）的接口，特别是引力量子化，研究者们正积极尝试构建包含大质量粒子的物质波干涉仪。该研究旨在为这一领域的研究社区提供技术支持，分享在实现此类干涉仪过程中遇到的挑战及解决方案。

Method: 本研究利用嵌入纳米金刚石（ND）中的自旋，结合斯特恩-格拉赫力，在时空中形成闭环，以实现量子力学与广义相对论的接口测试。具体方法包括回顾了将纳米金刚石加载到保罗阱的现有技术，并介绍了利用压电元件和电场力进行纳米金刚石加载和发射的实验。此外，还提出了用于超高真空实验的新型纳米金刚石加载方法。

Result: 研究回顾了纳米金刚石加载到保罗阱的方法、能力和局限性，并展示了在加载和发射纳米金刚石方面的实验结果。研究强调了提高加载效率的重要性，以应对高质量单NV纳米金刚石生产成本高昂的问题，并介绍了为超高真空实验设计的创新加载方法。

Conclusion: 该技术笔记旨在概述研究团队在利用纳米金刚石实现量子力学与广义相对论接口的宏伟目标中所面临的挑战、提出的解决方案以及实验进展。

Abstract: Quantum mechanics (QM) and General relativity (GR), also known as the theory
of gravity, are the two pillars of modern physics. A matter-wave interferometer
with a massive particle, can test numerous fundamental ideas, including the
spatial superposition principle - a foundational concept in QM - in completely
new regimes, as well as the interface between QM and GR, e.g., testing the
quantization of gravity. Consequently, there exists an intensive effort to
realize such an interferometer. While several paths are being pursued, we focus
on utilizing nanodiamonds as our particle, and a spin embedded in the ND
together with Stern-Gerlach forces, to achieve a closed loop in space-time.
There is a growing community of groups pursuing this path [1]. We are posting
this technical note (as part of a series of seven such notes), to highlight our
plans and solutions concerning various challenges in this ambitious endeavor,
hoping this will support this growing community. In this work, we review
current methods for loading nanodiamonds into a Paul trap, and their
capabilities and limitations regarding our application. We also present our
experiments on loading and launching nanodiamonds using a vibrating
piezoelectric element and by electrical forces. Finally, we present our design
of a novel nanodiamond loading method for ultra-high-vacuum experiments. As the
production of highly accurate, high-purity nanodiamonds with a single NV
required for interferometric measurements is expected to be expensive, we put
emphasis on achieving high loading efficiency, while loading the charged ND
into a Paul trap in ultra-high vacuum.

</details>


### [272] [Piecemaker: a resource-efficient entanglement distribution protocol](https://arxiv.org/abs/2508.14737)
*Luise Prielinger,Kenneth Goodenough,Guus Avis,Stefan Krastanov,Don Towsley,Gayane Vardoyan*

Main category: quant-ph

TL;DR: 一种新的量子协议，使用量子开关和优化的贝尔对处理来提高多方纠缠分发的保真度并减少噪声。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有协议中等待所有贝尔对生成后再分发状态的限制，从而减少累积噪声并提高分布式状态的保真度。

Method: 通过利用量子开关分发稳定器状态，并在生成每个贝尔对后立即处理它们，而不是等待所有贝尔对生成，从而减少了平均贝尔对存储时间。协议设计基于图态（高达局部互补）的顶点覆盖结构。

Result: 与基线方案相比，该协议实现了更高或相等的保真度，并将错误保真度降低了 45%。该协议在更广泛的去偏振率和贝尔对生成成功概率下，实现了 1/2 的多方纠缠的关键保真度阈值。

Conclusion: 该协议始终实现相等或更高的分布式状态保真度，并将错误保真度降低多达 45%。

Abstract: We introduce multipartite entanglement distribution protocols that use a
quantum switch to deliver stabilizer states to a number of remote end users. As
in existing schemes, the first step in our protocols involves Bell pair
generation between the switch and each end user. However, unlike existing
schemes that wait for all Bell pairs to be established before distributing the
desired state -- for example, via a projective measurement -- our approach
stores only a minimal subset of Bell pairs while processing every subsequent
Bell pair immediately. In doing so, our protocols reduce the average Bell pair
storage time compared to existing schemes, resulting in less cumulative noise
as a direct consequence. On the theoretical side, our protocol design is
grounded in the structure of vertex covers in graph states up to local
complementation. Through a comprehensive numerical evaluation, we compare the
fidelities of delivered states with those of a baseline scheme, for state sizes
up to n = 50 qubits. Simulations also show that our protocols can achieve the
critical fidelity threshold of 1/2 for multipartite entanglement in a wider
range of depolarization rates and success probabilities of Bell-pair
generation. Overall, our protocols always achieve an equal or higher fidelity
of the distributed state, and can reduce infidelity by up to 45%.

</details>


### [273] [Efficient nonclassical state preparation via generalized parity measurement](https://arxiv.org/abs/2508.14750)
*Chen-yi Zhang,Jun Jing*

Main category: quant-ph

TL;DR: 通过一种新的基于测量的协议，利用原子与玻色子模式的相互作用，可以高效地制备 Fock 态和 Dicke 态，保真度高，所需测量次数少。


<details>
  <summary>Details</summary>
Motivation: 为了解决在量子信息处理和量子计量学中，由玻色子系统的均匀能谱引起的难以制备大数态（特别是 Fock 态）的问题。

Method: 提出了一种基于测量的协议，利用了玻色子模式与辅助双层原子之间的共振 Jaynes-Cummings 相互作用以及对原子的顺序投影测量。通过结合自由演化和测量，可以构造出广义奇偶测量，从而过滤掉不需要的布居数并将目标模式推向所需的 Fock 态。

Result: 在理想情况下，该协议可以在 8 次测量中以超过 98% 的保真度制备 Fock 态 $|n_tangle$（$n_t 
eq 2000$）。在考虑了实际电路 QED 平台中的耗散和退相干的情况下，可以在 6 次测量中以大约 80% 的保真度制备 Fock 态 $|n_tangle$（$n_t 
eq 100$）。此外，该协议还可以在少于 3 次测量中以足够高的保真度制备自旋集合的 Dicke 态 $|J<1000,0angle$。

Conclusion: 该协议可以为量子信息处理和量子计量学制备 Fock 态和 Dicke 态，并且与量子相位估计算法相比，所需的门操作更少。

Abstract: Nonclassical states of bosonic modes, especially the large number states, are
valuable resources for quantum information processing and quantum metrology.
However, it is intricate to apply unitary protocols to generate a desired Fock
state due to the uniform energy spectrum of bosonic system. We here propose a
measurement-based protocol that leverages the resonant Jaynes-Cummings
interaction of the bosonic mode with an ancillary two-level atom and sequential
projective measurements on the atom. Using the generalized parity measurement
constructed by several rounds of free-evolution and measurement with proper
intervals, we can efficiently filter out the unwanted population and push the
target mode conditionally toward the desired Fock state. In ideal situation, a
Fock state $|n_t\approx2000\rangle$ can be prepared with a fidelity over $98\%$
using only $8$ rounds of measurements. With qubit dissipation and dephasing and
cavity decay in the current circuit QED platforms, a Fock state
$|n_t\approx100\rangle$ can be prepared with a fidelity about $80\%$ by $6$
measurements. It is found that the number of the measurement rounds for
preparing a large Fock state $|n_t\rangle$ scales roughly as
$\log_2\sqrt{n_t}$, similar to the number of ancillary qubits required in the
state preparation via the quantum phase estimation algorithm yet costs much
less gate operations. Our protocol can also be used to prepare a large Dicke
state $|J<1000,0\rangle$ of a spin ensemble with a sufficiently high fidelity
by less than $3$ measurements, which is qualified for the quantum Fisher
information approaching the Heisenberg scaling in sensing the rotation phase
along the $x$ axis.

</details>


### [274] [Reinforcement learning entangling operations on spin qubits](https://arxiv.org/abs/2508.14761)
*Mohammad Abedi,Markus Schmitt*

Main category: quant-ph

TL;DR: Reinforcement learning can find high-fidelity quantum computing protocols for semiconductor-based singlet-triplet qubits, overcoming experimental limitations and outperforming traditional methods.


<details>
  <summary>Details</summary>
Motivation: High-fidelity control of one- and two-qubit gates past the error correction threshold is an essential ingredient for scalable quantum computing.

Method: We present a reinforcement learning (RL) approach to find entangling protocols for semiconductor-based singlet-triplet qubits in a double quantum dot. We optimize our RL approach for different regimes and tasks, including training from simulated process tomography reconstruction of unitary gates, and investigate the nuances of RL agent design.

Result: An RL agent can yield performative protocols, while avoiding the model-biases of traditional gradient-based methods, despite the presence of realistically modelled experimental constraints, such as various noise contributions and finite rise-time effects.

Conclusion: We demonstrate that a reinforcement learning (RL) agent can yield performative protocols for high-fidelity control of one- and two-qubit gates past the error correction threshold in semiconductor-based singlet-triplet qubits, avoiding model-biases of traditional gradient-based methods despite realistically modelled experimental constraints.

Abstract: High-fidelity control of one- and two-qubit gates past the error correction
threshold is an essential ingredient for scalable quantum computing. We present
a reinforcement learning (RL) approach to find entangling protocols for
semiconductor-based singlet-triplet qubits in a double quantum dot. Despite the
presence of realistically modelled experimental constraints, such as various
noise contributions and finite rise-time effects, we demonstrate that an RL
agent can yield performative protocols, while avoiding the model-biases of
traditional gradient-based methods. We optimise our RL approach for different
regimes and tasks, including training from simulated process tomography
reconstruction of unitary gates, and investigate the nuances of RL agent
design.

</details>


### [275] [Exploring the Interplay Between Quantum Entanglement and Decoherence](https://arxiv.org/abs/2508.14790)
*Samuel Marquez Gonzalez*

Main category: quant-ph

TL;DR: 研究了退相干如何影响量子纠缠，并强调了在量子技术中保持纠缠的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究退相干机制如何影响纠缠态的完整性，并强调了在量子技术发展中保持纠缠的重要性。

Method: 通过整合理论见解和实验结果，对退相干机制进行了检验，并分析了包括热、电磁和碰撞退相干在内的各种环境因素以及量子噪声（如幅度阻尼、相位阻尼和去极化）对纠缠态的影响。

Result: 量子噪声，如幅度阻尼、相位阻尼和去极化，会影响纠缠态的完整性，而退相干机制会逐渐破坏量子相干性和纠缠性，即所谓的纠缠猝死（ESD）。

Conclusion: 该研究强调了在量子计算和量子通信等量子技术发展中，保持量子纠缠和缓解退相干之间的微妙平衡具有重要意义，因为这对于实现稳健可靠的性能至关重要。

Abstract: Quantum entanglement manifests as a distinctive correlation between particles
that transcends classical boundaries when their quantum states cannot be
described independently. On the other hand, as quantum systems interact with
their surroundings, decoherence emerges, leading to the gradual decay of
quantum coherence and entanglement. In the case of entanglement, this is known
as entanglement sudden death (ESD). Decoherence mechanisms are examined,
focusing on how various environmental factors, such as thermal,
electromagnetic, and collisional decoherence, influence the integrity of
entangled states. The role of quantum noise, such as amplitude damping, phase
damping, and depolarizing, is also analyzed. By integrating theoretical
insights with experimental findings, this study highlights the delicate balance
between maintaining entanglement and mitigating decoherence. The findings have
significant implications for the development of quantum technologies, including
quantum computing and quantum communication, where preserving entanglement is
crucial for achieving robust and reliable performance.

</details>


### [276] [Operational reconstruction of Feynman rules for quantum amplitudes via composition algebras](https://arxiv.org/abs/2508.14822)
*Jens Köplinger,Michael Habeck,Philip Goyal*

Main category: quant-ph

TL;DR: 本文重新审视并评估了量子重建计划中的一个操作模型，该模型描述了测量之间的过渡振幅。通过将公理与数学、选择与物理分开，作者们在不依赖二维空间的情况下，以与坐标无关的方式进行了分析。研究结果将允许的幅度代数归类为实数关联组合代数，并指出观察到的概率与物理学中的玻恩规则类似。作者们还讨论了模型公理的影响以及未来研究的潜在方向。


<details>
  <summary>Details</summary>
Motivation: 本文重新审视了“复杂量子振幅的起源和费曼规则”中提出的一个操作模型，作为量子重建计划的一部分，该模型描述了测量之间的过渡振幅。

Method: 通过在不先要求二维空间的情况下，以与坐标无关的方式仔细评估原始模型来阐明我们的方法论，将公理与数学、选择与物理以及由此产生的推论分开。

Result: 观察到的概率在幅度上是二次的，类似于物理学中的玻恩规则。我们指出了假设的模型公理的一些影响以及重述观察者问题的方法；并宣传我们工作的广泛效用，以获得后续发现，无论是作为结果、泛化还是替代。

Conclusion: 基于模型公理和观察者选择，将标量场和向量空间公理追溯到模型公理和观察者选择，包括加法和乘法单位元和逆元。已知的数学定理将允许的幅度代数分类为实数关联组合代数，即二维（分裂）复数和四维（分裂）四元数。

Abstract: This paper revisits an operational model presented in "Origin of complex
quantum amplitudes and Feynman's rules", Phys. Rev. A 81 (2010), 022109 (P.
Goyal, K. H. Knuth, J. Skilling) as part of the Quantum Reconstruction Program,
describing transition amplitudes between measurements. Our methodology
establishes clarity by separating axioms from mathematics, choices from
physics, and deductions therefrom. We carefully evaluate the original model in
a coordinate-independent way without requiring a two-dimensional space a
priori. All scalar field and vector space axioms are traced from model axioms
and observer choices, including additive and multiplicative units and inverses.
Known theorems in math classify allowable amplitude algebras as the real
associative composition algebras, namely, the two-dimensional (split-)complex
numbers and the four-dimensional (split-)quaternions. Observed probabilities
are quadratic in amplitudes, akin to the Born rule in physics. We point out
select ramifications of postulated model axioms and ways to rephrase observer
questions; and advertise broad utility of our work towards follow-on discovery,
whether as a consequence, generalization, or alternative. One seemingly minute
generalization is sketched in the outlook, with algebraic consequences at the
heart of current open questions in mathematics and physics.

</details>


### [277] [Quantum mechanics, non-locality, and the space discreteness hypothesis](https://arxiv.org/abs/2508.14836)
*W. A. Zúñiga-Galindo*

Main category: quant-ph

TL;DR: 本文提出了一种新的时空模型，基于空间离散化假说，并发展了具有非局域性的量子力学。该模型解决了量子测量问题，并能解释双缝实验。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索空间离散化假说，并在此基础上构建一个与现有理论有显著差异的时空模型和量子力学形式。研究的动机包括解决量子力学的测量问题，并为双缝实验等现象提供新的解释。

Method: 论文基于Bronstein不等式，利用全空间作为空间模型，并考虑时间为实变量。在此框架下，使用了Dirac-von Neumann形式，并提出了$\\mathbb{R}\\times(\\mathbb{R}\\times\\mathcal{X})^{3}$作为时空模型。论文详细阐述了在$L^{2}(\\mathbb{R}\\times\\mathcal{X})$希尔伯特空间上的量子力学形式，并指出了其非局域性。

Result: 提出的时空模型和量子力学形式具有非局域性，允许“鬼魅般的超距作用”。该模型为波函数坍缩提供了一个新的机制，并且认为薛定谔方程在测量时刻也适用。此外，该模型能够自然地解释双缝实验中的明暗条纹。

Conclusion: 该论文提出了一个基于空间离散化假说的时空模型，并在此框架下发展了量子力学的新形式。该模型允许非局域性，并为测量问题提供了一个新的波函数坍缩机制。它还被应用于解释双缝实验。

Abstract: The space discreteness hypothesis asserts that the nature of space at short
distances is radically different from that at large distances. Based on the
Bronstein inequality, here, we use a totally disconnected topological space
$\mathcal{X}$ as a model for the space. However, we consider the time as a real
variable. In this framework, the formalism of Dirac-von Neumann can be used.
This discreteness hypothesis implies that given two different points in space,
there is no continuous curve (a world line) joining them. Consequently, this
hypothesis is not compatible with the theory of relativity. We propose
$\mathbb{R}\times(\mathbb{R}\times\mathcal{X})^{3}$ as a model of a space-time.
For simplicity, we work out our models using
$\mathbb{R}\times(\mathbb{R}\times\mathcal{X})$ as the configuration space.
Quantum mechanics (QM), in the sense of Dirac-von Neumann, on the Hilbert space
$L^{2}(\mathbb{R}\times\mathcal{X})$ is a non-local theory: the Hamiltonians
are non-local operators, and thus, spooky action at a distance is allowed. The
paradigm asserting that the universe is non-locally real implies that the
proposed version of QM admits realism. This version of QM can be specialized to
standard QM by using Hamiltonians acting on wavefunctions supported on the
region $\mathbb{R}\times\mathbb{R}$. We apply the developed formalism to the
measurement problem. We propose a new mechanism for the collapse of the
wavefunction. The mechanism resembles the one proposed by Ghirardi, Ramini, and
Weber, but there are significant differences. The most important feature is
that the Schr\"{o}dinger equation describes the dynamics at all times, even at
the moment of measurement. We also discuss a model for the two-slit experiment,
where bright and dark states of light (proposed recently) naturally occur.

</details>


### [278] [Quantum Interference of Distinguishable Photons Based on Spatially-Resolved Measurements](https://arxiv.org/abs/2508.14845)
*Miguel Angel Gonzalez,Alejandra Alarcón,Andres Camilo Quintero,Daniel Sabogal,Luca Maggio,Vincenzo Tamma,Daniel F. Urrego,Alejandra Valencia*

Main category: quant-ph

TL;DR: 通过空间分辨测量，即使光子在横向动量上可区分，也能观察到量子干涉。


<details>
  <summary>Details</summary>
Motivation: 演示在横向动量上可区分的两个光子之间的量子干涉效应，并通过空间分辨测量来观察这种反直觉的干涉。

Method: 通过近场中的空间分辨测量，演示了两个在横向动量上可区分的光子在平衡分束器上的量子干涉。

Result: 在空间分辨测量中观察到了量子干涉，而在非空间分辨测量中，由于对光子位置的平均化处理，该干涉效应被削弱了。

Conclusion: 实验结果与理论预测一致，其中可区分光子的量子干涉是通过近场中的空间分辨测量实现的。

Abstract: We present experimental results demonstrating the quantum interference of two
photons distinguishable in their transverse momenta, each entering the input
ports of a balanced beam splitter. This counterintuitive interference effect is
made possible through spatially resolved measurements in the near field, i.e.,
by resolving the conjugate variable in which the photons are distinguishable.
Our experimental findings agree with theoretical predictions. We contrast our
results with a non-spatially resolved measurement where averaging over the
photons' positions washes out the quantum interference observed in spatially
resolved measurements.

</details>


### [279] [Power-Law Interactions Stabilize Time Crystals Realizing Quantum Energy Storage and Sensing](https://arxiv.org/abs/2508.14847)
*Ayan Sahoo,Debraj Rakshit*

Main category: quant-ph

TL;DR: 研究了具有幂律相互作用的自旋链中的离散时间晶体（DTC）相。发现在DTC相中，量子电池的能量存储和量子传感的精度随系统尺寸的增加而提高，并且这些特性对相互作用指数的变化具有稳健性。


<details>
  <summary>Details</summary>
Motivation: 研究一维自旋1/2链中具有幂律相互作用的离散时间晶体（DTC）相在周期性 Floquet 驱动下的行为。

Method: 通过泛化 Stark 定域化到幂律相互作用，识别出在广泛的相互作用指数范围内存在的、由相干驱动和空间变化的耦合相互作用的交替作用所稳定的、稳健的周期加倍动力学。

Result: 在DTC相中，系统存储的能量（可被视为量子电池）随系统尺寸超线性增长，但归一化功率不存在扩展优势。此外，DTC相支持增强的量子传感，与时间偏差估计相关的量子 Fisher 信息随系统尺寸超扩展，超越了海森堡极限。量子优势的程度可以通过改变相互作用指数进行调整，但DTC行为在整个过程中保持稳健。

Conclusion: 研究结果将具有幂律相互作用的 Floquet 系统定位为存储量子能量和实现计量增强的稳健平台。

Abstract: We study discrete time-crystalline (DTC) phases in one-dimensional spin-1/2
chains with power-law interactions under periodic Floquet driving. By
generalizing Stark localization to power-law interaction profiles, we identify
robust period-doubled dynamics across a wide range of interaction exponents,
stabilized by the interplay between coherent driving and spatially varying
coupling. Within the DTC phase, the energy stored in the system, interpreted as
a quantum battery, increases superlinearly with system size, although no
scaling advantage persists in normalized power. Beyond energy storage, we
demonstrate that the DTC phase supports enhanced quantum sensing. The quantum
Fisher information associated with estimating timing deviations in the drive
scales superextensively with system size, surpassing the Heisenberg limit. The
degree of quantum advantage can be tuned by varying the interaction exponent,
though DTC behavior remains robust throughout. Our results position power-law
interacting Floquet systems as robust platforms for storing quantum energy and
achieving metrological enhancement.

</details>


### [280] [Single-click protocols for remote state preparation using weak coherent pulses](https://arxiv.org/abs/2508.14857)
*Janice van Dam,Emil R. Hellebek,Tzula B. Propp,Junior R. Gonzales-Ureta,Anders S. Sørensen,Stephanie D. C. Wehner*

Main category: quant-ph

TL;DR: This paper presents new protocols (SC and DSC) for remote state preparation using weak coherent pulses, improving upon existing methods (DC) in terms of speed and efficiency, and showing their utility in quantum networks like QKD.


<details>
  <summary>Details</summary>
Motivation: Remote state preparation (RSP) is a quantum networking technique with applications in blind quantum computing and long-distance quantum key distribution (QKD). The motivation is to develop RSP protocols that have low hardware requirements, ideally using only photonic qubits, and to improve performance compared to existing methods like the double-click (DC) protocol, especially by utilizing weak coherent pulse sources which are practical alternatives to single-photon sources.

Method: The paper introduces two new protocols for remote state preparation (RSP) with a weak-coherent-pulse-based device: a single-click (SC) protocol and a double-single-click (DSC) protocol. The SC protocol requires only one photon to reach the Bell state measurement, improving performance in certain regimes. The DSC protocol repeats the SC protocol twice and applies a CNOT gate between the resulting qubits, mitigating the need for phase stabilization and lowering technical complexity while improving performance in some regimes compared to the previously known double-click (DC) protocol.

Result: The SC protocol consistently achieves higher rates than the DC protocol and does not suffer from an inherently lower fidelity. The DSC protocol can still show performance improvements over the DC protocol and may have reduced technical complexity compared to the SC protocol. The paper also demonstrates the application of these protocols in long-distance QKD using quantum repeaters.

Conclusion: SC consistently achieves higher rates than DC and, interestingly, does not suffer from an inherently lower fidelity than the DC, as is the case for entanglement generation. Although SC provides stronger performance, DSC can still show performance improvements over DC, and it may have reduced technical complexity compared to SC. Lastly, we show how these protocols can be used in long-distance QKD using quantum repeaters.

Abstract: Remote state preparation (RSP) allows one party to remotely prepare a known
quantum state on another party's qubit using entanglement. This can be used in
quantum networks to perform applications such as blind quantum computing or
long-distance quantum key distribution (QKD) with quantum repeaters. Devices to
perform RSP, referred to as a client, ideally have low hardware requirements,
such as only sending photonic qubits. A weak coherent pulse source offers a
practical alternative to true single-photon sources and is already widely used
in QKD. Here, we introduce two new protocols to the previously known protocol
for RSP with a weak-coherent-pulse-based device. The known technique uses a
double-click (DC) protocol, where a photon from both the server and the client
needs to reach an intermediate Bell state measurement. Here, we add to that a
single-click (SC) RSP protocol, which requires only one photon to reach the
Bell state measurement, allowing for better performance in certain regimes. In
addition, we introduce a double-single-click (DSC) protocol, where the SC
protocol is repeated twice, and a CNOT gate is applied between the resulting
qubits. DSC mitigates the need for phase stabilization in certain regimes,
lowering technical complexity while still improving performance compared to DC
in some regimes. We compare these protocols in terms of fidelity and rate,
finding that SC consistently achieves higher rates than DC and, interestingly,
does not suffer from an inherently lower fidelity than the DC, as is the case
for entanglement generation. Although SC provides stronger performance, DSC can
still show performance improvements over DC, and it may have reduced technical
complexity compared to SC. Lastly, we show how these protocols can be used in
long-distance QKD using quantum repeaters.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [281] [Quantization Meets Spikes: Lossless Conversion in the First Timestep via Polarity Multi-Spike Mapping](https://arxiv.org/abs/2508.14520)
*Hangming Zhang,Zheng Li,Qiang Yu*

Main category: cs.NE

TL;DR: 该研究提出了一种名为PMSM的方法，通过信息熵分析和新的映射策略，实现了近乎无损的ANN到SNN转换，能在单个时间步长内达到最先进的准确率，并显著降低能耗。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有ANN到SNN转换方法在实现超低延迟（一个时间步长）时精度损失的问题，以及现有量化方法丢弃负值信息和对超参数敏感导致性能下降的问题。

Method: 提出了一种名为“极性多脉冲映射”（PMSM）的方法，该方法通过信息熵分析量化层的信息损失，并结合一种专门为量化层设计的超参数调整策略，以实现近乎无损的ANN到SNN转换，并利用SNN的多时间步长动力学来保持稳定性能。

Result: PMSM在ViT-S架构上实现了CIFAR-10（98.5%）、CIFAR-100（89.3%）和ImageNet（81.6%）的准确率，创下了高效转换的新标杆。在VGG-16上，与基线方法相比，能耗降低了5倍以上。

Conclusion: 该研究首次从信息熵的角度分析了量化层引入的信息损失，并提出了一种名为“极性多脉冲映射”（PMSM）的方法和针对量化层的超参数调整策略。该方法实现了在第一个时间步长（理论最小延迟）的近乎无损的ANN到SNN转换，并利用SNN的多时间步长动力学来保持复杂任务上的稳定性能。实验结果表明，PMSM在ViT-S架构上实现了CIFAR-10（98.5%）、CIFAR-100（89.3%）和ImageNet（81.6%）的准确率，创下了高效转换的新标杆。此外，与基线方法相比，PMSM在VGG-16上将CIFAR-10和CIFAR-100的能耗降低了5倍以上。

Abstract: Spiking neural networks (SNNs) offer advantages in computational efficiency
via event-driven computing, compared to traditional artificial neural networks
(ANNs). While direct training methods tackle the challenge of
non-differentiable activation mechanisms in SNNs, they often suffer from high
computational and energy costs during training. As a result, ANN-to-SNN
conversion approach still remains a valuable and practical alternative. These
conversion-based methods aim to leverage the discrete output produced by the
quantization layer to obtain SNNs with low latency. Although the theoretical
minimum latency is one timestep, existing conversion methods have struggled to
realize such ultra-low latency without accuracy loss. Moreover, current
quantization approaches often discard negative-value information following
batch normalization and are highly sensitive to the hyperparameter
configuration, leading to degraded performance. In this work, we, for the first
time, analyze the information loss introduced by quantization layers through
the lens of information entropy. Building on our analysis, we introduce
Polarity Multi-Spike Mapping (PMSM) and a hyperparameter adjustment strategy
tailored for the quantization layer. Our method achieves nearly lossless
ANN-to-SNN conversion at the extremity, i.e., the first timestep, while also
leveraging the temporal dynamics of SNNs across multiple timesteps to maintain
stable performance on complex tasks. Experimental results show that our PMSM
achieves state-of-the-art accuracies of 98.5% on CIFAR-10, 89.3% on CIFAR-100
and 81.6% on ImageNet with only one timestep on ViT-S architecture,
establishing a new benchmark for efficient conversion. In addition, our method
reduces energy consumption by over 5x under VGG-16 on CIFAR-10 and CIFAR-100,
compared to the baseline method.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [282] [Optimal Subspace Embeddings: Resolving Nelson-Nguyen Conjecture Up to Sub-Polylogarithmic Factors](https://arxiv.org/abs/2508.14234)
*Shabarish Chenakkod,Michał Dereziński,Xiaoyu Dong*

Main category: cs.DS

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We give a proof of the conjecture of Nelson and Nguyen [FOCS 2013] on the
optimal dimension and sparsity of oblivious subspace embeddings, up to
sub-polylogarithmic factors: For any $n\geq d$ and $\epsilon\geq d^{-O(1)}$,
there is a random $\tilde O(d/\epsilon^2)\times n$ matrix $\Pi$ with $\tilde
O(\log(d)/\epsilon)$ non-zeros per column such that for any
$A\in\mathbb{R}^{n\times d}$, with high probability,
$(1-\epsilon)\|Ax\|\leq\|\Pi Ax\|\leq(1+\epsilon)\|Ax\|$ for all
$x\in\mathbb{R}^d$, where $\tilde O(\cdot)$ hides only sub-polylogarithmic
factors in $d$. Our result in particular implies a new fastest sub-current
matrix multiplication time reduction of size $\tilde O(d/\epsilon^2)$ for a
broad class of $n\times d$ linear regression tasks.
  A key novelty in our analysis is a matrix concentration technique we call
iterative decoupling, which we use to fine-tune the higher-order trace moment
bounds attainable via existing random matrix universality tools [Brailovskaya
and van Handel, GAFA 2024].

</details>


### [283] [Nearly Tight Bounds for the Online Sorting Problem](https://arxiv.org/abs/2508.14287)
*Yossi Azar,Debmalya Panigrahi,Or Vardi*

Main category: cs.DS

TL;DR: 本文通过改进在线排序算法，在空间复杂度和竞争比之间取得了接近最优的权衡，将之前的指数级差距大幅缩小，并为不同空间限制下的问题提供了高效解决方案。


<details>
  <summary>Details</summary>
Motivation: 在线排序问题旨在最小化插入排序数组的数字对之间连续非空单元格的绝对差之和。先前研究在算法的竞争比和空间复杂度之间存在巨大的指数级差距，促使本研究旨在弥合这一差距，并为问题提供更优的解决方案。

Method: 本文提出了一种新的确定性算法，用于解决在线排序问题。具体而言，对于 $m = (1+\varepsilon) n$ 的情况，算法的竞争比为 $O(\log^2 n / \varepsilon)$；对于 $m = \gamma n$（其中 $\gamma = [O(1), O(\log^2 n)]$）的情况，算法的竞争比为 $O(\log^2 n / \gamma)$。这些算法在理论上接近最优，并且可以扩展到处理未知范围的输入。

Result: 本文提出的算法将 $m = (1+\varepsilon) n$ 的竞争比从指数级降低到 $O(\log^2 n / \varepsilon)$，并将 $m = \gamma n$ 的竞争比从指数级降低到 $O(\log^2 n / \gamma)$。特别地，当空间复杂度为 $O(n \log^2 n)$ 时，可以实现 $O(1)$-competitive，这非常接近下界 $\Omega(n \log n / \log \log n)$。这些结果表明，竞争比与 $\gamma$ 的乘积的上界为 $O(\log^2 n)$，而下界为 $\Omega(\log n / \log\log n)$。

Conclusion: 本文在在线排序问题上取得了重要进展，通过设计新的确定性算法，显著缩小了先前上界和下界之间的指数级差距，并为空间复杂度和竞争比之间的权衡提供了接近最优的解决方案。

Abstract: In the online sorting problem, a sequence of $n$ numbers in $[0, 1]$
(including $\{0,1\}$) have to be inserted in an array of size $m \ge n$ so as
to minimize the sum of absolute differences between pairs of numbers occupying
consecutive non-empty cells. Previously, Aamand {\em et al.} (SODA 2023) gave a
deterministic $2^{\sqrt{\log n} \sqrt{\log \log n + \log
(1/\varepsilon)}}$-competitive algorithm when $m = (1+\varepsilon) n$ for any
$\varepsilon \ge \Omega(\log n/n)$. They also showed a lower bound: with $m =
\gamma n$ space, the competitive ratio of any deterministic algorithm is at
least $\frac{1}{\gamma}\cdot\Omega(\log n / \log \log n)$. This left an
exponential gap between the upper and lower bounds for the problem.
  In this paper, we bridge this exponential gap and almost completely resolve
the online sorting problem. First, we give a deterministic $O(\log^2 n /
\varepsilon)$-competitive algorithm with $m = (1+\varepsilon) n$, for any
$\varepsilon \ge \Omega(\log n / n)$. Next, for $m = \gamma n$ where $\gamma =
[O(1), O(\log^2 n)]$, we give a deterministic $O(\log^2 n /
\gamma)$-competitive algorithm. In particular, this implies an
$O(1)$-competitive algorithm with $O(n \log^2 n)$ space, which is within an
$O(\log n\cdot \log \log n)$ factor of the lower bound of $\Omega(n \log n /
\log \log n)$. Combined, the two results imply a close to optimal tradeoff
between space and competitive ratio for the entire range of interest:
specifically, an upper bound of $O(\log^2 n)$ on the product of the competitive
ratio and $\gamma$ while the lower bound on this product is $\Omega(\log n /
\log\log n)$. We also show that these results can be extended to the case when
the range of the numbers is not known in advance, for an additional $O(\log n)$
factor in the competitive ratio.

</details>


### [284] [Sublinear-Time Approximation for Graph Frequency Vectors in Hyperfinite Graphs](https://arxiv.org/abs/2508.14324)
*Gregory Moroie*

Main category: cs.DS

TL;DR: 本研究提出一种亚线性时间算法，在超有限图上，通过采样顶点和使用划分-Oracle，以ε的ℓ1误差近似计算图的k-盘分布。算法的运行时间和摘要大小依赖于图的度d、参数k和误差ε。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在超有限假设下，于亚线性时间近似计算有界度图的k-盘分布（频率向量）的问题。

Method: 该方法利用了Hassidim等人提出的划分-Oracle框架，通过采样N=⌈Cε⁻²⌉个顶点并查询局部划分Oracle，在poly(d,k,ε⁻¹)时间内构建一个大小为|H|=poly(dᵏ,1/ε)的摘要图H。该摘要图的k-盘频率向量在ℓ1距离上近似于原始图的k-盘频率向量，误差不超过ε。

Result: 该研究阐明了运行时间和摘要大小对参数d、k和ε的依赖关系。具体而言，通过采样N=⌈Cε⁻²⌉个顶点并查询局部划分Oracle，可以在poly(d,k,ε⁻¹)的时间内构建一个摘要图H，其k-盘频率向量在ℓ1距离上近似于原始图，误差为ε。

Conclusion: 该研究提供了一种在超有限假设下，于亚线性时间近似计算有界度图的k-盘分布（频率向量）的方法。研究通过重新审视Hassidim等人提出的划分-Oracle框架，明确区分了两种误差来源：(i) 由超有限参数φ控制的割误差，移除不超过φ|V|的边可将此误差控制在ℓ1距离的ε/2以内；(ii) 由精度参数ε控制的采样误差，通过N=Θ(ε^-2)次随机顶点查询和Chernoff界及联合界论证，此误差也控制在ε/2以内。结合两者，该方法以高概率实现了ℓ1距离为ε的总误差。

Abstract: In this work, we address the problem of approximating the $k$-disc
distribution (``frequency vector") of a bounded-degree graph in sublinear-time
under the assumption of hyperfiniteness. We revisit the partition-oracle
framework of Hassidim, Kelner, Nguyen, and Onak \cite{hassidim2009local}, and
provide a concise, self-contained analysis that explicitly separates the two
sources of error: (i) the cut error, controlled by hyperfiniteness parameter
$\phi$, which incurs at most $\varepsilon/2$ in $\ell_1$-distance by removing
at most $\phi |V|$ edges; and (ii) the sampling error, controlled by the
accuracy parameter $\varepsilon$, bounded by $\varepsilon/2$ via
$N=\Theta(\varepsilon^{-2})$ random vertex queries and a Chernoff and union
bound argument. Combining these yields an overall $\ell_1$-error of
$\varepsilon$ with high probability. Algorithmically, we show that by sampling
$N=\lceil C\varepsilon^{-2} \rceil$ vertices and querying the local partition
oracle, one can in time $poly(d,k,\varepsilon^{-1})$ construct a summary graph
$H$ of size $|H|=poly(d^k,1/\varepsilon)$ whose $k$-disc frequency vector
approximates that of the original graph within $\varepsilon$ in
$\ell_1$-distance. Our approach clarifies the dependence of both runtime and
summary-size on the parameter $d$,$k$, and $\varepsilon$.

</details>


### [285] [Improved Online Sorting](https://arxiv.org/abs/2508.14361)
*Jubayer Nirjhor,Nicole Wein*

Main category: cs.DS

TL;DR: 本文提出了一种新的在线排序算法，该算法具有改进的性能，成本为 $(\varepsilon^{-1}\log n)^{O(\log \log n)}$。


<details>
  <summary>Details</summary>
Motivation: 在线排序问题旨在优化在数字在线到达时将其放置在数组中的过程，以最小化相邻元素的成本。本文旨在改进现有算法的性能。

Method: 本文提出了一种新的确定性算法来解决在线排序问题，该算法将每个到达的数字放置在大小为 $(1+\varepsilon) n$ 的数组中，并最小化相邻元素之间绝对差值之和。

Result: 本文提出的算法实现了 $(\varepsilon^{-1}\log n)^{O(\log \log n)}$ 的准对数成本，这比之前由 Aamand 等人提出的算法 $2^{O(\sqrt{\log n \cdot\log\log n +\log \varepsilon^{-1}})}$ 更好。此外，Azar 等人的独立工作实现了 $O(\varepsilon^{-1}\log^2 n)$ 的对数成本。

Conclusion: 本文提出了一个确定性算法，其在线排序问题的成本为 $(\varepsilon^{-1}\log n)^{O(\log \log n)}$，优于先前算法的成本 $2^{O(\sqrt{\log n \cdot\log\log n +\log \varepsilon^{-1}})}$。

Abstract: We study the online sorting problem, where $n$ real numbers arrive in an
online fashion, and the algorithm must immediately place each number into an
array of size $(1+\varepsilon) n$ before seeing the next number. After all $n$
numbers are placed into the array, the cost is defined as the sum over the
absolute differences of all $n-1$ pairs of adjacent numbers in the array,
ignoring empty array cells. Aamand, Abrahamsen, Beretta, and Kleist introduced
the problem and obtained a deterministic algorithm with cost
$2^{O\left(\sqrt{\log n \cdot\log\log n +\log \varepsilon^{-1}}\right)}$, and a
lower bound of $\Omega(\log n / \log\log n)$ for deterministic algorithms. We
obtain a deterministic algorithm with quasi-polylogarithmic cost
$\left(\varepsilon^{-1}\log n\right)^{O\left(\log \log n\right)}$.
  Concurrent and independent work by Azar, Panigrahi, and Vardi achieves
polylogarithmic cost $O(\varepsilon^{-1}\log^2 n)$.

</details>


### [286] [Compact representation of maximal palindromes](https://arxiv.org/abs/2508.14384)
*Takuya Mieno*

Main category: cs.DS

TL;DR: 该研究提出了一种新的 O(n) 比特表示法，用于表示字符串中的所有最大回文数，从而能够以 O(1) 时间检索以任何给定位置为中心的最大回文数的长度。该方法将加速更多节省空间的方法的发展。最后，该方法被用于提出一种能够以 O(log n) 时间计算字符串中任何给定因子中最长回文数的 O(n) 比特数据结构。


<details>
  <summary>Details</summary>
Motivation: 回文是字符串中向前和向后读都相同的字符串。回文结构在字符串算法中是一个基本问题，其潜在应用领域包括形式语言理论和生物信息学。

Method: 提出了一种新颖的 O(n) 比特表示法，用于表示字符串中的所有最大回文数，该表示法能够以 O(1) 时间检索以任何给定位置为中心的最大回文数的长度。

Result: 作为该紧凑表示法的第一项应用，我们提出了一种大小为 O(n) 比特的数​​据结构，它能够以 O(log n) 时间计算字符串中任何给定因子中出现的最长回文数。

Conclusion: 该 O(n) 比特表示法能够加速需要处理回文结构的更多节省空间的方法。

Abstract: Palindromes are strings that read the same forward and backward. The
computation of palindromic structures within strings is a fundamental problem
in string algorithms, being motivated by potential applications in formal
language theory and bioinformatics. Although the number of palindromic factors
in a string of length $n$ can be quadratic, they can be implicitly represented
in $O(n \log n)$ bits of space by storing the lengths of all maximal
palindromes in an integer array, which can be computed in $O(n)$ time
[Manacher, 1975]. In this paper, we propose a novel $O(n)$-bit representation
of all maximal palindromes in a string, which enables $O(1)$-time retrieval of
the length of the maximal palindrome centered at any given position. Since
Manacher's algorithm and the notion of maximal palindromes are widely utilized
for solving numerous problems involving palindromic structures, our compact
representation will accelerate the development of more space-efficient
solutions. Indeed, as the first application of our compact representation of
maximal palindromes, we present a data structure of size $O(n)$ bits that can
compute the longest palindrome appearing in any given factor of the string in
$O(\log n)$ time.

</details>


### [287] [Incremental-Decremental Maximization](https://arxiv.org/abs/2508.14516)
*Yann Disser,Max Klimm,Annette Lutz,Lea Strubberg*

Main category: cs.DS

TL;DR: 提出一种用于增量-递减最大化的框架，该框架通过一次转换一个元素来捕获基础设施的渐进转换。所提出的算法在转换的各个阶段都能保持相对于当前阶段最优解的大效用，适用于具有有界曲率和/或一般次模比率的效用函数，并且比增量最大化更具挑战性。


<details>
  <summary>Details</summary>
Motivation: 提出一个增量-递减最大化框架，以捕获基础设施的渐进转换或更新。

Method: 提出了一种简单的随机算法和一种确定性算法，它们都能在转换的各个阶段保持相对于当前阶段最优解的大效用，找到转换元素的顺序。

Result: 所提出的算法对于具有有界曲率和/或一般次模比率的效用函数，特别是次模函数和总替代函数，能够产生有竞争力的解决方案。

Conclusion: 增量-递减最大化比增量最大化更具挑战性。

Abstract: We introduce a framework for incremental-decremental maximization that
captures the gradual transformation or renewal of infrastructures. In our
model, an initial solution is transformed one element at a time and the utility
of an intermediate solution is given by the sum of the utilities of the
transformed and untransformed parts. We propose a simple randomized and a
deterministic algorithm that both find an order in which to transform the
elements while maintaining a large utility during all stages of transformation,
relative to an optimum solution for the current stage. More specifically, our
algorithms yield competitive solutions for utility functions of bounded
curvature and/or generic submodularity ratio, and, in particular, for
submodular functions, and gross substitute functions. Our results exhibit that
incremental-decremental maximization is substantially more difficult than
incremental maximization.

</details>


### [288] [A $(4/3+\varepsilon)$-Approximation for Preemptive Scheduling with Batch Setup Times](https://arxiv.org/abs/2508.14528)
*Max A. Deppert,David Fischer,Klaus Jansen*

Main category: cs.DS

TL;DR: This paper offers a better approximation algorithm for a complex job scheduling problem with setup times, achieving a 4/3 + ε approximation, an improvement over previous methods.


<details>
  <summary>Details</summary>
Motivation: To find a more efficient approximation algorithm for the NP-hard scheduling problem with preemption and class-dependent setup times, aiming to minimize the makespan.

Method: The algorithm partitions the problem into an 'easy' and a 'hard' part. It then provides a 4/3 T-approximation for the easy part and shows the existence of a 4/3 T-approximation for the hard part with specific properties. Finally, it combines these to achieve the overall approximation.

Result: A (4/3 + ε)-approximate algorithm with a runtime in O(n^2 log(1/ε)) is presented, which improves the approximation ratio from 3/2 to 4/3 + ε for ε < 1/6.

Conclusion: The paper provides a (4/3 + ε)-approximation algorithm for the NP-hard problem of scheduling n jobs with class-dependent setup times on m identical parallel machines, improving upon the previous best approximation ratio of 3/2.

Abstract: We consider the $\mathcal{NP}$-hard problem $\mathrm{P} \mathbf{\vert}
\mathrm{pmtn, setup=s_i} \mathbf{\vert} \mathrm{C_{\max}}$, the problem of
scheduling $n$ jobs, which are divided into $c$ classes, on $m$ identical
parallel machines while allowing preemption. For each class $i$ of the $c$
classes, we are given a setup time $s_i$ that is required to be scheduled
whenever a machine switches from processing a job of one class to a job from
another class. The goal is to find a schedule that minimizes the makespan.
  We give a $(4/3+\varepsilon)$-approximate algorithm with run time in
$\mathcal{O}(n^2 \log(1/\varepsilon))$. For any $\varepsilon < 1/6$, this
improves upon the previously best known approximation ratio of $3/2$ for this
problem.
  Our main technical contributions are as follows. We first partition any
instance into an "easy" and a "hard" part, such that a $4/3 T$-approximation
for the former is easy to compute for some given makespan $T$. We then proceed
to show our main structural result, namely that there always exists a $4/3
T$-approximation for any instance that has a solution with makespan $T$, where
the hard part has some easy to compute properties. Finally, we obtain an
algorithm that computes a $(4/3+\varepsilon)$-approximation in time n
$\mathcal{O}(n^2 \log(1/\varepsilon))$ for general instances by computing
solutions with the previously shown structural properties.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [289] [To Zip Through the Cost Analysis of Probabilistic Programs](https://arxiv.org/abs/2508.14249)
*Matthias Hetzenberger,Georg Moser,Florian Zuleger*

Main category: cs.LO

TL;DR: 该研究通过在 Liquid Haskell 中引入细化类型概率模型，实现了对概率算法预期运行时间的自动化推理，并通过案例研究验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 驱动该研究的背景是随机性在提高算法性能方面的广泛应用，以及在概率编程和概率算法形式化分析领域取得的进展。然而，关于预期运行时间的自动化推理仍然有限。

Method: 通过在 Liquid Haskell 中引入细化类型概率模型来解决自动化推理预期运行时间的问题，该模型通过在类型中编码概率行为来实现对预期值和成本的自动化推理。该模型最初是为有限支持上的离散分布定义的，后来通过公理化方法扩展到支持无限分布。

Result: 该框架通过四项案例研究进行了评估：可合并堆、优惠券收集器、随机快速排序和 zip 树。前两项研究证明了自动化和最小的注释开销。后两项研究展示了该模型如何与交互式证明集成，包括 zip 树的预期运行时间的首次形式化验证。

Conclusion: 该框架通过四项案例研究进行了评估：可合并堆、优惠券收集器、随机快速排序和 zip 树。前两项研究证明了自动化和最小的注释开销。后两项研究展示了该模型如何与交互式证明集成，包括 zip 树的预期运行时间的首次形式化验证。

Abstract: Probabilistic programming and the formal analysis of probabilistic algorithms
are active areas of research, driven by the widespread use of randomness to
improve performance. While functional correctness has seen substantial
progress, automated reasoning about expected runtime remains comparatively
limited. In this work, we address this challenge by introducing a
refinement-typed probability monad in Liquid Haskell. Our monad enables
automated reasoning about expected values and costs by encoding probabilistic
behaviour directly in types. Initially defined for discrete distributions over
finite support, it is extended to support infinite distributions via an
axiomatic approach. By leveraging Liquid Haskell's SMT-based refinement type
checking, our framework provides a high degree of automation. We evaluate our
approach through four case studies: meldable heaps, coupon collector,
randomised quicksort, and zip trees. The first two demonstrate automation with
minimal annotation overhead. The latter two showcase how our monad integrates
with interactive proofs, including the first formal verification of the
expected runtime of zip trees.

</details>


### [290] [Quantum Petri Nets with Event Structures semantics](https://arxiv.org/abs/2508.14531)
*Julien Saan Joachim,Marc de Visme,Stefan Haar*

Main category: cs.LO

TL;DR: 提出量子Petri网（QPN）模型，解决了现有模型在量子并发建模方面的不足，提供了严格的语义和分析框架。


<details>
  <summary>Details</summary>
Motivation: 现有量子Petri网模型缺乏严格的并发和量子语义、分析工具以及展开理论，无法充分建模量子并发。

Method: 本研究通过引入局部定义的量子发生网（LQONs）、QPNs的构造以及QPNs的组合框架，建立了与量子事件结构语义兼容的QPN模型。

Result: 成功建立了与量子事件结构语义兼容的量子Petri网（QPN）模型，并提供了局部定义的量子发生网（LQONs）和组合框架，为量子并发建模奠定了坚实的语义基础，连接了Petri网理论和量子编程。

Conclusion: 本研究提出了一种新的量子Petri网（QPN）模型，它与量子事件结构语义兼容，并提供了严格的并发和量子语义、分析工具和展开理论，填补了现有研究的空白。

Abstract: Classical Petri nets provide a canonical model of concurrency, with unfolding
semantics linking nets, occurrence nets, and event structures. No comparable
framework exists for quantum concurrency: existing ''quantum Petri nets'' lack
rigorous concurrent and sound quantum semantics, analysis tools, and unfolding
theory. We introduce Quantum Petri Nets (QPNs), Petri nets equipped with a
quantum valuation compatible with the quantum event structure semantics of
Clairambault, De Visme, and Winskel (2019). Our contributions are: (i) a local
definition of Quantum Occurrence Nets (LQONs) compatible with quantum event
structures, (ii) a construction of QPNs with a well-defined unfolding
semantics, (iii) a compositional framework for QPNs. This establishes a
semantically well grounded model of quantum concurrency, bridging Petri net
theory and quantum programming.

</details>


### [291] [A Complete and Natural Rule Set for Multi-Qutrit Clifford Circuits](https://arxiv.org/abs/2508.14670)
*Sarah Meng Li,Michele Mosca,Neil J. Ross,John van de Wetering,Yuming Zhao*

Main category: cs.LO

TL;DR: We developed rewrite rules for n-qutrit Clifford circuits, generalizing existing methods to odd prime dimensions and simplifying the description of qutrit Clifford unitaries.


<details>
  <summary>Details</summary>
Motivation: To establish a completeness result for n-qutrit Clifford circuits and present a clean description of the group of qutrit Clifford unitaries.

Method: We generalize Selinger's normal form to qutrits and present a rewrite system to reduce Clifford circuits to this normal form, followed by simplification to a smaller set of rules.

Result: A complete set of rewrite rules for n-qutrit Clifford circuits and a simplified set of generators and relations for the group of qutrit Clifford unitaries.

Conclusion: We provide a complete set of rewrite rules for n-qutrit Clifford circuits, a first for odd prime dimensions.

Abstract: We present a complete set of rewrite rules for n-qutrit Clifford circuits
where n is any non-negative integer. This is the first completeness result for
any fragment of quantum circuits in odd prime dimensions. We first generalize
Selinger's normal form for n-qubit Clifford circuits to the qutrit setting.
Then, we present a rewrite system by which any Clifford circuit can be reduced
to this normal form. We then simplify the rewrite rules in this procedure to a
small natural set of rules, giving a clean presentation of the group of qutrit
Clifford unitaries in terms of generators and relations.

</details>


### [292] [Emerson-Lei and Manna-Pnueli Games for LTLf+ and PPLTL+ Synthesis](https://arxiv.org/abs/2508.14725)
*Daniel Hausmann,Shufang Zhu,Gianmarco Parretti,Christoph Weinhuber,Giuseppe De Giacomo,Nir Piterman*

Main category: cs.LO

TL;DR: This paper introduces new solvers for reactive synthesis in LTLfp and PPLTLp using Manna-Pnueli and Emerson-Lei games. Manna-Pnueli games show performance benefits but combining both approaches may be optimal.


<details>
  <summary>Details</summary>
Motivation: To present the first actual solvers for reactive synthesis in the temporal logics LTLfp and PPLTLp, which allow the use of finite-trace techniques in infinite-trace settings while achieving the expressiveness of full LTL.

Method: The paper presents solvers for reactive synthesis based on games on graphs that leverage DFA-based techniques. It introduces a symbolic solver based on Emerson-Lei games and then Manna-Pnueli games, which embed Manna-Pnueli objectives into the arena and are solved by composing solutions to a DAG of simpler Emerson-Lei games.

Result: Implemented solvers were practically evaluated, showing that Manna-Pnueli games often offer significant advantages over Emerson-Lei games for reactive synthesis in LTLfp and PPLTLp.

Conclusion: Manna-Pnueli games offer significant advantages for reactive synthesis in LTLfp and PPLTLp, though not universally. Combining both Manna-Pnueli games and Emerson-Lei games could further enhance practical performance.

Abstract: Recently, the Manna-Pnueli Hierarchy has been used to define the temporal
logics LTLfp and PPLTLp, which allow to use finite-trace LTLf/PPLTL techniques
in infinite-trace settings while achieving the expressiveness of full LTL. In
this paper, we present the first actual solvers for reactive synthesis in these
logics. These are based on games on graphs that leverage DFA-based techniques
from LTLf/PPLTL to construct the game arena. We start with a symbolic solver
based on Emerson-Lei games, which reduces lower-class properties (guarantee,
safety) to higher ones (recurrence, persistence) before solving the game. We
then introduce Manna-Pnueli games, which natively embed Manna-Pnueli objectives
into the arena. These games are solved by composing solutions to a DAG of
simpler Emerson-Lei games, resulting in a provably more efficient approach. We
implemented the solvers and practically evaluated their performance on a range
of representative formulas. The results show that Manna-Pnueli games often
offer significant advantages, though not universally, indicating that combining
both approaches could further enhance practical performance.

</details>


### [293] [Constraint satisfaction problems, compactness and non-measurable sets](https://arxiv.org/abs/2508.14838)
*Claude Tardif*

Main category: cs.LO

TL;DR: 论文研究了有限关系结构的紧致性。证明了宽度为一的结构在 ZFC 中是紧致的，而宽度大于一的结构则与不可测集的存在性相关。


<details>
  <summary>Details</summary>
Motivation: 为了探究有限关系结构的紧致性及其与集合论公理系统（如 ZFC）以及不可测集的联系。

Method: 研究了有限关系结构 A 的紧致性，其中紧致性的定义为：对于任意同类型的无限关系结构 B，存在从 B 到 A 的同态映射等价于存在从 B 的所有有限子结构到 A 的同态映射。

Result: 证明了宽度为一的结构的紧致性可以在 ZFC 中得到证明，而宽度大于一的结构的紧致性则蕴含了非可测集的性质。

Conclusion: 若 A 的宽度为一时，A 的紧致性可以在 ZFC 系统的框架内被证明，但若 A 的宽度大于一时，A 的紧致性则意味着三维空间中非可测集的出现。

Abstract: A finite relational structure A is called compact if for any infinite
relational structure B of the same type, the existence of a homomorphism from B
to A is equivalent to the existence of homomorphisms from all finite
substructures of B to A. We show that if A has width one, then the compactness
of A can be proved in the axiom system of Zermelo and Fraenkel, but otherwise,
the compactness of A implies the existence of non-measurable sets in 3-space.

</details>


### [294] [Correct Black-Box Monitors for Distributed Deadlock Detection: Formalisation and Implementation (Technical Report)](https://arxiv.org/abs/2508.14851)
*Radosław Jan Rowicki,Adrian Francalanza,Alceste Scalas*

Main category: cs.LO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Many software applications rely on concurrent and distributed (micro)services
that interact via message-passing and various forms of remote procedure calls
(RPC). As these systems organically evolve and grow in scale and complexity,
the risk of introducing deadlocks increases and their impact may worsen: even
if only a few services deadlock, many other services may block while awaiting
responses from the deadlocked ones. As a result, the "core" of the deadlock can
be obfuscated by its consequences on the rest of the system, and diagnosing and
fixing the problem can be challenging.
  In this work we tackle the challenge by proposing distributed black-box
monitors that are deployed alongside each service and detect deadlocks by only
observing the incoming and outgoing messages, and exchanging probes with other
monitors. We present a formal model that captures popular RPC-based application
styles (e.g., gen_servers in Erlang/OTP), and a distributed black-box
monitoring algorithm that we prove sound and complete (i.e., identifies
deadlocked services with neither false positives nor false negatives). We
implement our results in a tool called DDMon for the monitoring of Erlang/OTP
applications, and we evaluate its performance.
  This is the first work that formalises, proves the correctness, and
implements distributed black-box monitors for deadlock detection. Our results
are mechanised in Coq. DDMon is the companion artifact of this paper.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [295] [MOHAF: A Multi-Objective Hierarchical Auction Framework for Scalable and Fair Resource Allocation in IoT Ecosystems](https://arxiv.org/abs/2508.14830)
*Kushagra Agrawal,Polat Goktas,Anjan Bandopadhyay,Debolina Ghosh,Junali Jasmine Jena,Mahendra Kumar Gourisaria*

Main category: cs.DC

TL;DR: MOHAF通过分层聚类和子模优化解决了物联网资源分配的挑战，实现了高效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 传统集中式机制和单一目标拍卖模型难以在动态、分布式环境中实现平衡的系统性能，因此需要一种能联合优化成本、服务质量(QoS)、能源效率和公平性的资源分配机制。

Method: MOHAF是一个分布式资源分配机制，它集成了分层聚类以降低计算复杂性，并采用贪婪的、子模的优化策略，保证了(1-1/e)的近似比。它还包含一个动态定价机制，可实时适应资源利用率，以提高市场稳定性和分配质量。

Result: MOHAF在Google Cluster Data trace上的实验表明，其分配效率（0.263）优于贪婪（0.185）、第一价格（0.138）和随机（0.101）拍卖，同时实现了完美的公平性（Jain's index = 1.000）。消融研究表明，成本和QoS在维持平衡的多目标结果中起着关键作用。此外，MOHAF还具有近乎线性的可扩展性和理论保证。

Conclusion: MOHAF是一个实用且适应性强的解决方案，可用于大规模物联网部署，有效协调效率、公平性和可持续性。

Abstract: The rapid growth of Internet of Things (IoT) ecosystems has intensified the
challenge of efficiently allocating heterogeneous resources in highly dynamic,
distributed environments. Conventional centralized mechanisms and
single-objective auction models, focusing solely on metrics such as cost
minimization or revenue maximization, struggle to deliver balanced system
performance. This paper proposes the Multi-Objective Hierarchical Auction
Framework (MOHAF), a distributed resource allocation mechanism that jointly
optimizes cost, Quality of Service (QoS), energy efficiency, and fairness.
MOHAF integrates hierarchical clustering to reduce computational complexity
with a greedy, submodular optimization strategy that guarantees a (1-1/e)
approximation ratio. A dynamic pricing mechanism adapts in real time to
resource utilization, enhancing market stability and allocation quality.
Extensive experiments on the Google Cluster Data trace, comprising 3,553
requests and 888 resources, demonstrate MOHAF's superior allocation efficiency
(0.263) compared to Greedy (0.185), First-Price (0.138), and Random (0.101)
auctions, while achieving perfect fairness (Jain's index = 1.000). Ablation
studies reveal the critical influence of cost and QoS components in sustaining
balanced multi-objective outcomes. With near-linear scalability, theoretical
guarantees, and robust empirical performance, MOHAF offers a practical and
adaptable solution for large-scale IoT deployments, effectively reconciling
efficiency, equity, and sustainability in distributed resource coordination.

</details>


### [296] [Time-optimal Asynchronous Minimal Vertex Covering by Myopic Robots](https://arxiv.org/abs/2508.14247)
*Saswata Jana,Subhajit Pramanick,Adri Bhattacharya,Partha Sarathi Mandal*

Main category: cs.DC

TL;DR: 本研究提出了一种创新的‘ the robots ’算法，使用局部知识解决了图的最小顶点覆盖问题，并在树和一般图上均实现了最优或接近最优的性能。


<details>
  <summary>Details</summary>
Motivation: 在具有有限可见范围的自主机器人群中，使用局部知识将机器人部署到满足特定属性的顶点是自然的研究问题。本研究的动机是探索机器人能否通过局部知识来填充图的最小顶点覆盖集。

Method: 本研究提出了一系列‘ the robots ’算法，通过局部知识来解决图的最小顶点覆盖问题。算法在单门和多门环境下，针对树和一般图进行了设计和分析，并给出了时间和内存复杂度的最优界限。

Result: 对于树，算法在单门情况下实现了时间和内存的最优，在多门情况下实现了内存最优和（当门数是常数时的）时间最优。特别地，该技术在单门树上实现了最小顶点覆盖。对于一般图，算法在单门和多门情况下（具有O(log Δ)的额外内存）均可在O(|E|)的时间内完成。

Conclusion: 本研究提出了一系列具有有限可见范围的‘ the robots ’算法，以解决图的最小顶点覆盖问题，并在树和一般图上都取得了优秀的性能。

Abstract: In a connected graph with an autonomous robot swarm with limited visibility,
it is natural to ask whether the robots can be deployed to certain vertices
satisfying a given property using only local knowledge. This paper
affirmatively answers the question with a set of \emph{myopic} (finite
visibility range) luminous robots with the aim of \emph{filling a minimal
vertex cover} (MVC) of a given graph $G = (V, E)$. The graph has special
vertices, called \emph{doors}, through which robots enter sequentially.
Starting from the doors, the goal of the robots is to settle on a set of
vertices that forms a minimal vertex cover of $G$ under the asynchronous
($\mathcal{ASYNC}$) scheduler. We are also interested in achieving the
\emph{minimum vertex cover} (MinVC, which is NP-hard \cite{Karp1972} for
general graphs) for a specific graph class using the myopic robots. We
establish lower bounds on the visibility range for the robots and on the time
complexity (which is $\Omega(|E|)$). We present two algorithms for trees: one
for single door, which is both time and memory-optimal, and the other for
multiple doors, which is memory-optimal and achieves time-optimality when the
number of doors is a constant. Interestingly, our technique achieves MinVC on
trees with a single door. We then move to the general graph, where we present
two algorithms, one for the single door and the other for the multiple doors
with an extra memory of $O(\log \Delta)$ for the robots, where $\Delta$ is the
maximum degree of $G$. All our algorithms run in $O(|E|)$ epochs.

</details>


### [297] [Pure Data Spaces](https://arxiv.org/abs/2508.14271)
*Saul Youssef*

Main category: cs.DC

TL;DR: 本文提出了一种基于有限序列的“纯数据”数学框架，并展示了如何从该框架中生成各种数学对象。


<details>
  <summary>Details</summary>
Motivation: 为了探索“纯数据”框架的含义，需要展示如何从该框架中生成熟悉的数学对象。

Method: 本研究提出“纯数据”作为数学和计算的公理化基础，并基于“有限序列”而非逻辑或类型。在这一框架下，数学意义上的对象是“数据”，数学对象的集合是“空间”。随后，通过内同态的斜乜来研究空间，并从纯数据中“有机地生长”出熟悉的数学对象。

Result: 从纯数据中生长出自然数、整数、有理数、布尔空间、矩阵代数、高斯整数、四元数和整数八元数等熟悉的数学对象，并讨论了这些例子所带来的对理论新方向和新探索的见解。

Conclusion: 本研究通过“纯数据”框架，从有限序列的概念出发，建立了数学和计算的公理化基础。研究了空间的理论，其中空间是通过其内同态的斜乜来研究的。通过有机生长的方法，从纯数据中生长出熟悉的数学对象，如自然数、整数、有理数、布尔空间、矩阵代数、高斯整数、四元数和非结合代数，如整数八元数。研究结果为理论的新方向和新探索提供了见解。

Abstract: In a previous work, "pure data" is proposed as an axiomatic foundation for
mathematics and computing, based on "finite sequence" as the foundational
concept rather than based on logic or type. Within this framework, objects with
mathematical meaning are "data" and collections of mathematical objects must
then be associative data, called a "space." A space is then the basic
collection in this framework analogous to sets in Set Theory or objects in
Category Theory. A theory of spaces is developed,where spaces are studied via
their semiring of endomorphisms. To illustrate these concepts, and as a way of
exploring the implications of the framework, pure data spaces are "grown
organically" from the substrate of pure data with minimal combinatoric
definitions. Familiar objects from classical mathematics emerge this way,
including natural numbers, integers, rational numbers, boolean spaces, matrix
algebras, Gaussian Integers, Quaternions, and non-associative algebras like the
Integer Octonions. Insights from these examples are discussed with a view
towards new directions in theory and new exploration.

</details>


### [298] [SSSP-Del: Fully Dynamic Distributed Algorithm for Single-Source Shortest Path](https://arxiv.org/abs/2508.14319)
*Parshan Javanrood,Matei Ripeanu*

Main category: cs.DC

TL;DR: SSSP-Del is a new algorithm for dynamic SSSP that handles edge additions/deletions efficiently in distributed systems.


<details>
  <summary>Details</summary>
Motivation: Modern graphs are both large and dynamic, presenting significant challenges for fundamental queries, such as the Single-Source Shortest Path (SSSP) problem. Naively recomputing the SSSP tree after each topology change is prohibitively expensive, causing on-demand computation to suffer from high latency. Existing dynamic SSSP algorithms often cannot simultaneously handle both edge additions and deletions, operate in distributed memory, and provide low-latency query results.

Method: SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm processes streams of both edge insertions and deletions.

Result: The paper provides a comprehensive evaluation on large real-world and synthetic graphs with millions of vertices, and offers a thorough analysis by evaluating result latency, solution stability, and throughput.

Conclusion: SSSP-Del is a new vertex-centric, asynchronous, and fully distributed algorithm for dynamic SSSP that can handle both edge additions and deletions in a shared-nothing architecture.

Abstract: Modern graphs are both large and dynamic, presenting significant challenges
for fundamental queries, such as the Single-Source Shortest Path (SSSP)
problem. Naively recomputing the SSSP tree after each topology change is
prohibitively expensive, causing on-demand computation to suffer from high
latency. Existing dynamic SSSP algorithms often cannot simultaneously handle
both edge additions and deletions, operate in distributed memory, and provide
low-latency query results. To address these challenges, this paper presents
SSSP-Del, a new vertex-centric, asynchronous, and fully distributed algorithm
for dynamic SSSP. Operating in a shared-nothing architecture, our algorithm
processes streams of both edge insertions and deletions. We conduct a
comprehensive evaluation on large real-world and synthetic graphs with millions
of vertices, and provide a thorough analysis by evaluating result latency,
solution stability, and throughput.

</details>


### [299] [A Hierarchical Sharded Blockchain Balancing Performance and Availability](https://arxiv.org/abs/2508.14457)
*Yongrae Jo,Chanik Park*

Main category: cs.DC

TL;DR: PyloChain 是一种分层分片区块链，可通过投机执行和 DAG 内存池来提高性能和可用性。


<details>
  <summary>Details</summary>
Motivation: 现有区块链分片技术通常以牺牲可用性为代价来优先考虑性能。PyloChain 旨在通过一种分层分片方法来解决这个问题，该方法在可用性和性能之间取得平衡。

Method: PyloChain 采用分层分片方法，包括多个低级别本地链和一个高级别主链。本地链进行投机性交易执行以实现高并行性，而主链利用基于 DAG 的内存池来确保本地区块的可用性并实现高效的拜占庭容错共识，以处理全局交易。

Result: 与最先进的分片区块链相比，PyloChain 的吞吐量提高了 1.49 倍，延迟降低了 2.63 倍。

Conclusion: PyloChain 通过其分层分片结构、投机执行和 DAG 内存池，在可用性和性能之间取得了良好的平衡。与最先进的技术相比，它在吞吐量方面提高了 1.49 倍，延迟方面提高了 2.63 倍。

Abstract: Blockchain networks offer decentralization, transparency, and immutability
for managing critical data but encounter scalability problems as the number of
network members and transaction issuers grows. Sharding is considered a
promising solution to enhance blockchain scalability. However, most existing
blockchain sharding techniques prioritize performance at the cost of
availability (e.g., a failure in a few servers holding a shard leads to data
unavailability). In this paper, we propose PyloChain, a hierarchical sharded
blockchain that balances availability and performance. PyloChain consists of
multiple lower-level local chains and one higher-level main chain. Each local
chain speculatively executes local transactions to achieve high parallelism
across multiple local chains. The main chain leverages a directed-acyclic-graph
(DAG)-based mempool to guarantee local block availability and to enable
efficient Byzantine Fault Tolerance (BFT) consensus to execute global (or
cross-shard) transactions within a collocated sharding. PyloChain speculatively
executes local transactions across multiple local chains to achieve high
parallelism. In order to reduce the number of aborted local transactions,
PyloChain applies a simple scheduling technique to handle global transactions
in the main chain. PyloChain provides a fine-grained auditing mechanism to
mitigate faulty higher-level members by externalizing main chain operations to
lower-level local members. We implemented and evaluated PyloChain,
demonstrating its performance scalability with 1.49x higher throughput and
2.63x faster latency compared to the state-of-the-art balanced hierarchical
sharded blockchain.

</details>


### [300] [Auditable Shared Objects: From Registers to Synchronization Primitives](https://arxiv.org/abs/2508.14506)
*Hagit Attiya,Antonio Fernández Anta,Alessia Milani,Alexandre Rapetti,Corentin Travers*

Main category: cs.DC

TL;DR: 本文将可审计性扩展到多写寄存器，并实现了一种高效的可审计读/写寄存器和LL/SC对象，还实现了防闪烁拒绝列表。


<details>
  <summary>Details</summary>
Motivation: 为了扩展可审计性的概念，即跟踪对共享对象的操作并记录谁访问了什么信息，以赋予数据所有者更多的数据控制权。

Method: 本文提出了一种可审计的n写m读读/写寄存器的实现，其步复杂度为O(n+m)，并使用了具有共识编号m+n的(m+n)-滑动寄存器。共识编号的必要性也得到了证明。此外，该实现还可以扩展以支持可审计的加载链接/条件存储（LL/SC）共享对象，并最终通过实现防闪烁拒绝列表将可审计寄存器与访问控制对象联系起来。

Result: 实现了可审计的n写m读读/写寄存器，步复杂度为O(n+m)，并证明了所用(m+n)-滑动寄存器的共识编号m+n的必要性。该实现可扩展支持可审计的LL/SC共享对象，并能实现防闪烁拒绝列表。

Conclusion: 本文将可审计性从单写寄存器扩展到多写寄存器，并提供了可审计的n写m读读/写寄存器的实现，其步复杂度为O(n+m)。该实现使用了具有共识编号m+n的(m+n)-滑动寄存器，并证明了该共识编号的必要性。该实现可自然地扩展以支持可审计的加载链接/条件存储（LL/SC）共享对象，LL/SC是一种支持许多共享对象高效实现的原始语。最后，通过从可审计寄存器实现防闪烁拒绝列表，将可审计寄存器与其他访问控制对象联系起来。

Abstract: Auditability allows to track operations performed on a shared object,
recording who accessed which information. This gives data owners more control
on their data. Initially studied in the context of single-writer registers,
this work extends the notion of auditability to other shared objects, and
studies their properties.
  We start by moving from single-writer to multi-writer registers, and provide
an implementation of an auditable $n$-writer $m$-reader read / write register,
with $O(n+m)$ step complexity. This implementation uses $(m+n)$-sliding
registers, which have consensus number $m+n$. We show that this consensus
number is necessary. The implementation extends naturally to support an
auditable load-linked / store-conditional (LL/SC) shared object. LL/SC is a
primitive that supports efficient implementation of many shared objects.
Finally, we relate auditable registers to other access control objects, by
implementing an anti-flickering deny list from auditable registers.

</details>


### [301] [The Cost Advantage of Virtual Machine Migrations: Empirical Insights into Amazon's EC2 Marketspace](https://arxiv.org/abs/2508.14883)
*Benedikt Pittl,Werner Mach,Erich Schikuta*

Main category: cs.DC

TL;DR: 通过跨不同市场空间购买和迁移虚拟机来优化云成本。


<details>
  <summary>Details</summary>
Motivation: 识别创建最佳云虚拟机投资组合的最佳实践和指南。

Method: 使用来自亚马逊的定价数据和来自 Bitbrains 数据中心的真实虚拟机利用率数据集进行成本分析。

Result: 成本最优需要创建异构投资组合，即从不同市场空间购买虚拟机。对于运行时间在 6 小时到 1 年之间的虚拟机，迁移到不同市场空间具有成本效益。大多数虚拟机资源未被有效利用，存在显著的成本优化潜力。

Conclusion: 通过在不同市场空间购买虚拟机来创建异构投资组合可以实现成本最优。此外，在运行时迁移虚拟机对于运行时间在 6 小时到 1 年之间的虚拟机来说具有成本效益。大部分虚拟机资源未被有效利用，这表明了未来成本优化的巨大潜力。

Abstract: In recent years, cloud providers have introduced novel approaches for trading
virtual machines. For example, Virtustream introduced so-called muVMs to charge
cloud computing resources while other providers such as Google, Microsoft, or
Amazon re-invented their marketspaces. Today, the market leader Amazon runs six
marketspaces for trading virtual machines. Consumers can purchase bundles of
virtual machines, which are called cloud-portfolios, from multiple marketspaces
and providers. An industry-relevant field of research is to identify best
practices and guidelines on how such optimal portfolios are created. In the
paper at hand, a cost analysis of cloud portfolios is presented. Therefore,
pricing data from Amazon was used as well as a real virtual machine utilization
dataset from the Bitbrains datacenter. The results show that a cost optimum can
only be reached if heterogeneous portfolios are created where virtual machines
are purchased from different marketspaces. Additionally, the cost-benefit of
migrating virtual machines to different marketplaces during runtime is
presented. Such migrations are especially cost-effective for virtual machines
of cloud-portfolios which run between 6 hours and 1 year. The paper further
shows that most of the resources of virtual machines are never utilized by
consumers, which represents a significant future potential for cost
optimization. For the validation of the results, a second dataset of the
Bitbrains datacenter was used, which contains utility data of virtual machines
from a different domain of application.

</details>


### [302] [Boosting Payment Channel Network Liquidity with Topology Optimization and Transaction Selection](https://arxiv.org/abs/2508.14524)
*Krishnendu Chatterjee,Jan Matyáš Křišťan,Stefan Schmid,Jakub Svoboda,Michelle Yeo*

Main category: cs.DC

TL;DR: 该研究通过优化PCN拓扑、通道容量和交易处理决策，提供了一种近似算法来降低成本和提高效率。


<details>
  <summary>Details</summary>
Motivation: 为了最大化PCN的交易吞吐量并延长用户通道的生命周期，需要仔细设计网络拓扑并优化交易处理决策。

Method: 提出了一种$\\(p)$近似算法来解决PCN设计问题，并证明在特定交易分布假设下可将近似比降低到$\\(sqrt(p))$。

Result: 提出了一种$\\(p)$近似算法，并将近似比在特定条件下降低到$\\(sqrt(p))$，并通过在闪电网络上的实证研究来验证其理论分析。

Conclusion: 该研究提出了一种近似算法，用于优化支付通道网络（PCN）的拓扑结构、通道容量和交易处理决策，以最小化通道建立/扩容成本和交易拒绝成本。

Abstract: Payment channel networks (PCNs) are a promising technology that alleviates
blockchain scalability by shifting the transaction load from the blockchain to
the PCN. Nevertheless, the network topology has to be carefully designed to
maximise the transaction throughput in PCNs. Additionally, users in PCNs also
have to make optimal decisions on which transactions to forward and which to
reject to prolong the lifetime of their channels. In this work, we consider an
input sequence of transactions over $p$ parties. Each transaction consists of a
transaction size, source, and target, and can be either accepted or rejected
(entailing a cost). The goal is to design a PCN topology among the $p$
cooperating parties, along with the channel capacities, and then output a
decision for each transaction in the sequence to minimise the cost of creating
and augmenting channels, as well as the cost of rejecting transactions. Our
main contribution is an $\mathcal{O}(p)$ approximation algorithm for the
problem with $p$ parties. We further show that with some assumptions on the
distribution of transactions, we can reduce the approximation ratio to
$\mathcal{O}(\sqrt{p})$. We complement our theoretical analysis with an
empirical study of our assumptions and approach in the context of the Lightning
Network.

</details>


### [303] [A Systematic Evaluation of the Potential of Carbon-Aware Execution for Scientific Workflows](https://arxiv.org/abs/2508.14625)
*Kathleen West,Youssef Moawad,Fabian Lehmann,Vasilis Bountris,Ulf Leser,Yehia Elkhatib,Lauritz Thamsen*

Main category: cs.DC

TL;DR: Scientific workflows are energy-intensive. This study shows that by shifting execution times and scaling resources, their carbon emissions can be significantly reduced (up to 80% with temporal shifting, 67% with resource scaling).


<details>
  <summary>Details</summary>
Motivation: Scientific workflows are resource-intensive and long-running, leading to substantial energy consumption and carbon emissions. While carbon-aware computing methods exist, few have focused on scientific workflows, despite their suitability for such techniques due to their delay tolerance, interruptibility, scalability, and heterogeneity.

Method: The study estimates the carbon footprint of seven real-world Nextflow workflows on different cluster infrastructures using average and marginal carbon intensity data. It then evaluates the impact of carbon-aware temporal shifting, pausing/resuming, and resource scaling on workflows and tasks.

Result: Temporal shifting can decrease emissions by over 80%, and resource scaling can decrease emissions by 67%.

Conclusion: The study demonstrates that scientific workflows present a significant opportunity for carbon-aware computing, with temporal shifting potentially reducing emissions by over 80% and resource scaling by 67%.

Abstract: Scientific workflows are widely used to automate scientific data analysis and
often involve computationally intensive processing of large datasets on compute
clusters. As such, their execution tends to be long-running and
resource-intensive, resulting in substantial energy consumption and, depending
on the energy mix, carbon emissions. Meanwhile, a wealth of carbon-aware
computing methods have been proposed, yet little work has focused specifically
on scientific workflows, even though they present a substantial opportunity for
carbon-aware computing because they are often significantly delay tolerant,
efficiently interruptible, highly scalable and widely heterogeneous. In this
study, we first exemplify the problem of carbon emissions associated with
running scientific workflows, and then show the potential for carbon-aware
workflow execution. For this, we estimate the carbon footprint of seven
real-world Nextflow workflows executed on different cluster infrastructures
using both average and marginal carbon intensity data. Furthermore, we
systematically evaluate the impact of carbon-aware temporal shifting, and the
pausing and resuming of the workflow. Moreover, we apply resource scaling to
workflows and workflow tasks. Finally, we report the potential reduction in
overall carbon emissions, with temporal shifting capable of decreasing
emissions by over 80%, and resource scaling capable of decreasing emissions by
67%.

</details>


### [304] [DAG it off: Latency Prefers No Common Coins](https://arxiv.org/abs/2508.14716)
*Amores-Sesar Ignacio,Grøndal Viktor,Holmgård Adam,Ottendal Mads*

Main category: cs.DC

TL;DR: Black Marlin是一种创新的DAG拜占庭原子广播协议，实现了最优延迟和通信复杂度，无需额外原语，并在实践中表现优于现有协议。


<details>
  <summary>Details</summary>
Motivation: 在部分同步环境下，开发一种不需要可靠广播和公共币原语的有向无环图（DAG）拜占庭原子广播协议。

Method: Black Marlin协议是一种基于有向无环图（DAG）的拜占庭原子广播协议，在部分同步模型下设计，并且不需要可靠广播和公共币原语。

Result: Black Marlin协议实现了最优的3轮通信延迟（拜占庭容错下为4.25轮），并具有最优的通信和摊销通信复杂度。其吞吐量和延迟优于现有的基于DAG的协议。

Conclusion: Black Marlin是一种有向无环图（DAG）为基础的拜占庭原子广播协议，在部分同步环境下，无需可靠广播和公共币原语，实现了最优的3轮通信延迟（拜占庭容错下为4.25轮），同时保持了最优的通信和摊销通信复杂度。

Abstract: We introduce Black Marlin, the first Directed Acyclic Graph (DAG)-based
Byzantine atomic broadcast protocol in a partially synchronous setting that
successfully forgoes the reliable broadcast and common coin primitives. Black
Marlin achieves the optimal latency of 3 rounds of communication (4.25 with
Byzantine faults) while maintaining optimal communication and amortized
communication complexities. We present a formal security analysis of the
protocol, accompanied by empirical evidence that Black Marlin outperforms
state-of-the-art DAG-based protocols in both throughput and latency.

</details>


### [305] [Leveraging Hardware-Aware Computation in Mixed-Precision Matrix Multiply: A Tile-Centric Approach](https://arxiv.org/abs/2508.14848)
*Qiao Zhang,Rabab Alomairy,Dali Wang,Zhuowei Gu,Qinglei Cao*

Main category: cs.DC

TL;DR: 该研究提出了一个自适应混合精度GEMM框架，能在多种硬件上提升计算效率和精度。


<details>
  <summary>Details</summary>
Motivation: 随着优化低精度运算的硬件的出现，需要重新评估数值算法以利用混合精度计算，从而提高性能和能效。

Method: 提出一个自适应混合精度GEMM框架，支持在细粒度的块/瓦片级别使用不同的精度格式，并利用PaRSEC运行时系统在不同架构上平衡工作负载。

Result: 在基于ARM CPU的Fugaku超级计算机、基于Nvidia GPU的A100 DGX以及基于AMD GPU的Frontier超级计算机上，性能表现出良好的扩展性。

Conclusion: 该研究通过引入自适应混合精度GEMM框架，并利用PaRSEC运行时系统进行工作负载均衡，成功地在Fugaku（基于ARM CPU）、A100 DGX（基于Nvidia GPU）和Frontier（基于AMD GPU）等多种硬件架构上实现了高性能扩展，旨在通过结合算法和硬件创新来提高计算效率和精度。

Abstract: General Matrix Multiplication (GEMM) is a critical operation underpinning a
wide range of applications in high-performance computing (HPC) and artificial
intelligence (AI). The emergence of hardware optimized for low-precision
arithmetic necessitates a reevaluation of numerical algorithms to leverage
mixed-precision computations, achieving improved performance and energy
efficiency. This research introduces an adaptive mixed-precision GEMM framework
that supports different precision formats at fine-grained tile/block levels. We
utilize the PaRSEC runtime system to balance workloads across various
architectures. The performance scales well on ARM CPU-based Fugaku
supercomputer, Nvidia GPU-based A100 DGX, and AMD GPU-based Frontier
supercomputer. This research aims to enhance computational efficiency and
accuracy by bridging algorithmic advancements and hardware innovations, driving
transformative progress in various applications.

</details>


<div id='cs.GR'></div>

# cs.GR [[Back]](#toc)

### [306] [A Real-world Display Inverse Rendering Dataset](https://arxiv.org/abs/2508.14411)
*Seokjun Choi,Hoon-Gyu Chung,Yujin Jeon,Giljoo Nam,Seung-Hwan Baek*

Main category: cs.GR

TL;DR: 该研究介绍了首个用于显示器-相机逆渲染的真实世界数据集，并构建了一个成像系统来捕获数据。该数据集能够合成任意显示模式下的图像，并用于评估现有方法和提出一种新的基线方法。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏用于显示器-相机系统的公开真实世界数据集，这阻碍了基于显示器的逆渲染方法的开发和评估。

Method: 作者构建并校准了一个包含 LCD 显示器和立体偏振相机的成像系统，并使用一次一灯（OLAT）显示模式捕获了具有不同几何形状和反射特性的各种物体的数据。此外，还提供了高质量的地面真实几何信息。该数据集允许在任意显示模式和不同噪声水平下合成捕获的图像。

Result: 在提出的数据集上，作者评估了现有的光度立体和逆渲染方法，并提供了一个简单但有效的基线，该基线优于最先进的逆渲染方法。

Conclusion: 该数据集通过提供新颖的、可控的照明条件，为基于显示器的逆渲染方法的发展和评估提供了重要资源。作者提出的方法在现有方法的基础上进行了改进，并且能够处理各种物体和光照条件。

Abstract: Inverse rendering aims to reconstruct geometry and reflectance from captured
images. Display-camera imaging systems offer unique advantages for this task:
each pixel can easily function as a programmable point light source, and the
polarized light emitted by LCD displays facilitates diffuse-specular
separation. Despite these benefits, there is currently no public real-world
dataset captured using display-camera systems, unlike other setups such as
light stages. This absence hinders the development and evaluation of
display-based inverse rendering methods. In this paper, we introduce the first
real-world dataset for display-based inverse rendering. To achieve this, we
construct and calibrate an imaging system comprising an LCD display and stereo
polarization cameras. We then capture a diverse set of objects with diverse
geometry and reflectance under one-light-at-a-time (OLAT) display patterns. We
also provide high-quality ground-truth geometry. Our dataset enables the
synthesis of captured images under arbitrary display patterns and different
noise levels. Using this dataset, we evaluate the performance of existing
photometric stereo and inverse rendering methods, and provide a simple, yet
effective baseline for display inverse rendering, outperforming
state-of-the-art inverse rendering methods. Code and dataset are available on
our project page at https://michaelcsj.github.io/DIR/

</details>


### [307] [MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds](https://arxiv.org/abs/2508.14879)
*Bingquan Dai,Li Ray Luo,Qihong Tang,Jie Wang,Xinyu Lian,Hao Xu,Minghan Qin,Xudong Xu,Bo Dai,Haoqian Wang,Zhaoyang Lyu,Jiangmiao Pang*

Main category: cs.GR

TL;DR: MeshCoder是一个新的框架，它将复杂的3D对象（从点云）转换为可编辑的Blender Python脚本。它使用新的API和大型数据集来训练多模态LLM，实现了优于现有方法的性能，并允许轻松编辑和改进LLM的3D推理。


<details>
  <summary>Details</summary>
Motivation: 现有的方法通常依赖于有限的领域特定语言（DSLs）和小规模数据集，这限制了它们对复杂几何形状和结构的建模能力。为了解决这些挑战，需要一个能够重建复杂3D对象到可编辑程序的框架。

Method: MeshCoder框架，使用一套全面的、表达丰富的Blender Python API来合成复杂的几何形状，并构建了一个大规模的配对对象-代码数据集，其中每个对象的代码被分解为不同的语义部分。然后，训练了一个多模态大语言模型（LLM）来将3D点云转换为可执行的Blender Python脚本。

Result: MeshCoder在形状到代码重建任务上实现了卓越的性能，并且能够通过方便的代码修改促进直观的几何和拓扑编辑。该方法还增强了LLM在3D形状理解任务中的推理能力。

Conclusion: MeshCoder是一个强大的、灵活的程序化3D形状重建和理解解决方案，它能够将复杂的3D对象从点云重建为可编辑的Blender Python脚本，不仅在形状到代码重建任务上实现了卓越的性能，还通过方便的代码修改促进了直观的几何和拓扑编辑。此外，MeshCoder的代码表示增强了LLM在3D形状理解任务中的推理能力。

Abstract: Reconstructing 3D objects into editable programs is pivotal for applications
like reverse engineering and shape editing. However, existing methods often
rely on limited domain-specific languages (DSLs) and small-scale datasets,
restricting their ability to model complex geometries and structures. To
address these challenges, we introduce MeshCoder, a novel framework that
reconstructs complex 3D objects from point clouds into editable Blender Python
scripts. We develop a comprehensive set of expressive Blender Python APIs
capable of synthesizing intricate geometries. Leveraging these APIs, we
construct a large-scale paired object-code dataset, where the code for each
object is decomposed into distinct semantic parts. Subsequently, we train a
multimodal large language model (LLM) that translates 3D point cloud into
executable Blender Python scripts. Our approach not only achieves superior
performance in shape-to-code reconstruction tasks but also facilitates
intuitive geometric and topological editing through convenient code
modifications. Furthermore, our code-based representation enhances the
reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these
contributions establish MeshCoder as a powerful and flexible solution for
programmatic 3D shape reconstruction and understanding.

</details>


### [308] [Snap-Snap: Taking Two Images to Reconstruct 3D Human Gaussians in Milliseconds](https://arxiv.org/abs/2508.14892)
*Jia Lu,Taoran Yi,Jiemin Fang,Chen Yang,Chuiyun Wu,Wei Shen,Wenyu Liu,Qi Tian,Xinggang Wang*

Main category: cs.GR

TL;DR: 从正面和背面两张图中重建3D人体，效果好，速度快。


<details>
  <summary>Details</summary>
Motivation: 为了降低用户创建3D数字人体的门槛，提出从仅有的两个图像（正面和背面）重建三维人体这一具有挑战性但有价值的任务。

Method: 提出了一种基于基础重建模型的新几何重建方法，以预测一致的点云，并通过一个增强算法来补全颜色信息，最终将完整的人体点云转换为三维高斯以提升渲染质量。

Result: 实验结果表明，该方法在THuman2.0和跨域数据集上均取得了最先进的性能，能够在单个NVIDIA RTX 4090上以190毫秒的速度处理1024x1024分辨率的图像，并且能够处理由低成本移动设备拍摄的图像。

Conclusion: 该方法能够从仅有的两个视角（正面和背面）图像高效地重建出完整的人体三维模型，并实现了高质量的渲染效果，同时对输入图像的质量和拍摄设备没有严格要求。

Abstract: Reconstructing 3D human bodies from sparse views has been an appealing topic,
which is crucial to broader the related applications. In this paper, we propose
a quite challenging but valuable task to reconstruct the human body from only
two images, i.e., the front and back view, which can largely lower the barrier
for users to create their own 3D digital humans. The main challenges lie in the
difficulty of building 3D consistency and recovering missing information from
the highly sparse input. We redesign a geometry reconstruction model based on
foundation reconstruction models to predict consistent point clouds even input
images have scarce overlaps with extensive human data training. Furthermore, an
enhancement algorithm is applied to supplement the missing color information,
and then the complete human point clouds with colors can be obtained, which are
directly transformed into 3D Gaussians for better rendering quality.
Experiments show that our method can reconstruct the entire human in 190 ms on
a single NVIDIA RTX 4090, with two images at a resolution of 1024x1024,
demonstrating state-of-the-art performance on the THuman2.0 and cross-domain
datasets. Additionally, our method can complete human reconstruction even with
images captured by low-cost mobile devices, reducing the requirements for data
collection. Demos and code are available at
https://hustvl.github.io/Snap-Snap/.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [309] [An Improved Multi-Agent Algorithm for Cooperative and Competitive Environments by Identifying and Encouraging Cooperation among Agents](https://arxiv.org/abs/2508.14131)
*Junjie Qi,Siqi Mao,Tianyi Tan*

Main category: cs.MA

TL;DR: Improved MADDPG algorithm encourages cooperation in multi-agent systems, achieving better rewards.


<details>
  <summary>Details</summary>
Motivation: Identify and encourage cooperative behavior in multi-agent environments, addressing shortcomings of existing algorithms.

Method: Improve the existing MADDPG algorithm by introducing a new parameter to increase the reward for cooperative behavior.

Result: The improved algorithm outperforms MADDPG in PettingZoo environments, leading to higher team and individual rewards.

Conclusion: The new algorithm helps agents achieve both higher team rewards and individual rewards.

Abstract: We propose an improved algorithm by identifying and encouraging cooperative
behavior in multi-agent environments. First, we analyze the shortcomings of
existing algorithms in addressing multi-agent reinforcement learning problems.
Then, based on the existing algorithm MADDPG, we introduce a new parameter to
increase the reward that an agent can obtain when cooperative behavior among
agents is identified. Finally, we compare our improved algorithm with MADDPG in
environments from PettingZoo. The results show that the new algorithm helps
agents achieve both higher team rewards and individual rewards.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [310] [Large Language Models are Highly Aligned with Human Ratings of Emotional Stimuli](https://arxiv.org/abs/2508.14214)
*Mattson Ogg,Chace Ashcraft,Ritwik Bose,Raphael Norman-Tenazas,Michael Wolmetz*

Main category: cs.AI

TL;DR: 本研究评估了大型语言模型（LLM）在理解和评估人类情感刺激方面的表现。研究发现，GPT-4o在很多方面与人类的评估结果高度一致，尤其是在愉悦感评分上，但在唤起感评分上存在差异。LLM在五分类情感模型上比在二维模型上表现更好，并且LLM的评分比人类更一致。


<details>
  <summary>Details</summary>
Motivation: 为了解LLM在日常生活中（例如作为人类代理或与之互动）的有效性，需要了解它们如何评估情感刺激或情境。LLM在这些情况下的行为与人类行为的一致性，可以指导LLM在特定角色或互动中的应用。

Method: 收集了由人类评估过情感内容的数据集（包括词语和图像），并让多个流行的LLM对这些数据集进行评分，以比较LLM与人类在评估情感线索方面的一致性。

Result: GPT-4o在执行评分任务时，其表现与人类参与者在不同模态、刺激和大多数评分量表上都非常相似（在许多情况下相关系数r≥0.9）。然而，唤起感评分的人类与LLM之间的一致性较低，而愉悦感评分则最为一致。总体而言，LLM在五分类（愉悦、愤怒、悲伤、恐惧、厌恶）的情感框架内比在二维（唤起感和愉悦度）组织上更具一致性。此外，LLM的评分比人类的评分更为一致。

Conclusion: LLM在评估情感线索时与人类有相似之处，但也存在差异，例如在唤起和愉悦感评分方面，以及LLM的评分比人类更一致。

Abstract: Emotions exert an immense influence over human behavior and cognition in both
commonplace and high-stress tasks. Discussions of whether or how to integrate
large language models (LLMs) into everyday life (e.g., acting as proxies for,
or interacting with, human agents), should be informed by an understanding of
how these tools evaluate emotionally loaded stimuli or situations. A model's
alignment with human behavior in these cases can inform the effectiveness of
LLMs for certain roles or interactions. To help build this understanding, we
elicited ratings from multiple popular LLMs for datasets of words and images
that were previously rated for their emotional content by humans. We found that
when performing the same rating tasks, GPT-4o responded very similarly to human
participants across modalities, stimuli and most rating scales (r = 0.9 or
higher in many cases). However, arousal ratings were less well aligned between
human and LLM raters, while happiness ratings were most highly aligned. Overall
LLMs aligned better within a five-category (happiness, anger, sadness, fear,
disgust) emotion framework than within a two-dimensional (arousal and valence)
organization. Finally, LLM ratings were substantially more homogenous than
human ratings. Together these results begin to describe how LLM agents
interpret emotional stimuli and highlight similarities and differences among
biological and artificial intelligence in key behavioral domains.

</details>


### [311] [Explaining Hitori Puzzles: Neurosymbolic Proof Staging for Sequential Decisions](https://arxiv.org/abs/2508.14294)
*Maria Leonor Pacheco,Fabio Somenzi,Dananjay Srinivas,Ashutosh Trivedi*

Main category: cs.AI

TL;DR: A new way to explain tricky puzzles using smart computers and language models. It helps solve Hitori puzzles better.


<details>
  <summary>Details</summary>
Motivation: To address the challenge of explaining complex sequences of decisions by leveraging the strengths of both symbolic reasoning (decision procedures) and the expressive power of LLMs. Hitori puzzles were chosen as a test case due to their mix of local and connectivity constraints, which benefit from different explanation methods.

Method: A neurosymbolic approach combining decision procedures (SAT solvers) and Large Language Models (LLMs) was developed. This approach was applied to generate explanations for Hitori puzzle solutions.

Result: Experimental evidence shows the effectiveness of a tool implementing this approach in assisting humans to solve Hitori puzzles.

Conclusion: We developed a neurosymbolic approach combining decision procedures and LLMs to explain complex decision sequences, specifically for Hitori puzzles. Our approach effectively handles both local constraints with resolution proofs and connectivity constraints with visual explanations, demonstrating the flexibility of integrating SAT solvers and LLMs.

Abstract: We propose a neurosymbolic approach to the explanation of complex sequences
of decisions that combines the strengths of decision procedures and Large
Language Models (LLMs). We demonstrate this approach by producing explanations
for the solutions of Hitori puzzles. The rules of Hitori include local
constraints that are effectively explained by short resolution proofs. However,
they also include a connectivity constraint that is more suitable for visual
explanations. Hence, Hitori provides an excellent testing ground for a flexible
combination of SAT solvers and LLMs. We have implemented a tool that assists
humans in solving Hitori puzzles, and we present experimental evidence of its
effectiveness.

</details>


### [312] [Automated Optimization Modeling through Expert-Guided Large Language Model Reasoning](https://arxiv.org/abs/2508.14410)
*Beinuo Yang,Qishen Zhou,Junyi Li,Xingchen Su,Simon Hu*

Main category: cs.AI

TL;DR: 该研究通过改进数据集、引入新基准（LogiOR）和提出新框架（ORThought）来解决优化建模中的挑战，ORThought 利用链式思考提高了自动化水平和性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在解决优化建模（OM）的耗时和易错性问题上展现出潜力，但现有方法存在标签错误率高、评估范围狭窄和计算效率低等局限性。

Method: 本研究提出了一种名为 ORThought 的新框架，利用链式思考推理中的专家级优化建模原理来自动化优化建模过程。此外，研究还通过系统性地纠正错误和更全面的注释来增强现有数据集，并引入了包含更复杂问题和标准化注释的物流领域新优化建模基准LogiOR。

Result: ORThought 在复杂优化问题上表现优于现有方法，包括多代理框架。

Conclusion: 该研究通过系统性地纠正错误和更全面的注释来增强现有数据集，并引入了包含更复杂问题和标准化注释的物流领域新优化建模基准LogiOR。此外，研究提出了 ORThought 框架，利用链式思考推理中的专家级优化建模原理来自动化优化建模过程。实验证明，ORThought 的表现优于包括多代理框架在内的现有方法，在复杂优化问题上优势尤为显著。最后，研究对 ORThought 进行了系统性分析，明确了关键成功因素和失败模式，为未来基于LLM的优化建模研究提供了宝贵的见解。

Abstract: Optimization Modeling (OM) is essential for solving complex decision-making
problems. However, the process remains time-consuming and error-prone, heavily
relying on domain experts. While Large Language Models (LLMs) show promise in
addressing these challenges through their natural language understanding and
reasoning capabilities, current approaches face three critical limitations:
high benchmark labeling error rates reaching up to 42\%, narrow evaluation
scope that only considers optimal values, and computational inefficiency due to
heavy reliance on multi-agent systems or model fine-tuning. In this work, we
first enhance existing datasets through systematic error correction and more
comprehensive annotation. Additionally, we introduce LogiOR, a new optimization
modeling benchmark from the logistics domain, containing more complex problems
with standardized annotations. Furthermore, we present ORThought, a novel
framework that leverages expert-level optimization modeling principles through
chain-of-thought reasoning to automate the OM process. Through extensive
empirical evaluation, we demonstrate that ORThought outperforms existing
approaches, including multi-agent frameworks, with particularly significant
advantages on complex optimization problems. Finally, we provide a systematic
analysis of our method, identifying critical success factors and failure modes,
providing valuable insights for future research on LLM-based optimization
modeling.

</details>


### [313] [The Agent Behavior: Model, Governance and Challenges in the AI Digital Age](https://arxiv.org/abs/2508.14415)
*Qiang Zhang,Pei Yan,Yijia Xu,Chuanpo Fu,Yong Fang,Yang Liu*

Main category: cs.AI

TL;DR: 该研究提出网络行为生命周期模型、A4A范式和HABD模型，以应对AI智能体模仿人类行为带来的信任和安全挑战，并通过案例验证了模型的有效性，为未来人机协作提供理论和技术支持。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的发展，网络环境中的智能体越来越模仿人类行为，模糊了人与智能体之间的界限，带来了信任、责任、伦理、安全等方面的挑战。智能体行为监管的困难可能导致数据污染和责任归属不明确等问题。

Method: 提出“网络行为生命周期”模型，将网络行为划分为6个阶段，并分析了各阶段人与智能体的行为差异。在此基础上，引入“面向智能体的智能体（A4A）”范式和“人-智能体行为差异（HABD）”模型，从决策机制、执行效率、意图-行为一致性、行为惯性、非理性模式5个维度考察人与智能体行为的根本区别。通过红队渗透和蓝队防御等真实案例验证了模型的有效性。

Result: 通过真实案例验证了所提出模型的有效性。

Conclusion: 该研究提出了网络行为生命周期模型、A4A范式和HABD模型，为实现安全可信的人机协作提供了理论基础和技术路线图，并讨论了未来在动态认知治理架构、行为差异量化和元治理协议栈方面的研究方向。

Abstract: Advancements in AI have led to agents in networked environments increasingly
mirroring human behavior, thereby blurring the boundary between artificial and
human actors in specific contexts. This shift brings about significant
challenges in trust, responsibility, ethics, security and etc. The difficulty
in supervising of agent behaviors may lead to issues such as data contamination
and unclear accountability. To address these challenges, this paper proposes
the "Network Behavior Lifecycle" model, which divides network behavior into 6
stages and systematically analyzes the behavioral differences between humans
and agents at each stage. Based on these insights, the paper further introduces
the "Agent for Agent (A4A)" paradigm and the "Human-Agent Behavioral Disparity
(HABD)" model, which examine the fundamental distinctions between human and
agent behaviors across 5 dimensions: decision mechanism, execution efficiency,
intention-behavior consistency, behavioral inertia, and irrational patterns.
The effectiveness of the model is verified through real-world cases such as red
team penetration and blue team defense. Finally, the paper discusses future
research directions in dynamic cognitive governance architecture, behavioral
disparity quantification, and meta-governance protocol stacks, aiming to
provide a theoretical foundation and technical roadmap for secure and
trustworthy human-agent collaboration.

</details>


### [314] [Who Sees What? Structured Thought-Action Sequences for Epistemic Reasoning in LLMs](https://arxiv.org/abs/2508.14564)
*Luca Annese,Sabrina Patania,Silvia Serino,Tom Foulsham,Silvia Rossi,Azzurra Ruggeri,Dimitri Ognibene*

Main category: cs.AI

TL;DR: 通过将规划器生成的解决方案图转换为“思考-行动”示例，来改进基于 LLM 的智能体在视角采择任务中的表现。虽然 L 型示例略有改善，但并未带来显著的性能提升，表明仅靠结构化示例不足以实现复杂的视角采择能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前基于 LLM 的系统在需要主动感知、协作推理和视角采择（理解另一个智能体可以看到或知道什么）的任务中存在的挑战。

Method: 提出了一种结构化的解决方案处理流程，该流程利用 Fast Downward 规划器生成的转换解决方案图来创建三种类型的示例：最优目标路径（G 型）、信息节点路径（E 型）和逐步最优决策序列（L 型）。这些解决方案被转换为“思考-行动”示例，通过提示 LLM 来明确阐述每个决策背后的推理。

Result: L 型示例在减少澄清请求和总体行动步骤方面略有改善，但未能带来持续的性能提升。智能体在需要基本注意力过滤的任务中表现成功，但在需要心理化遮挡空间或权衡认知行动成本的任务中遇到困难。

Conclusion: 结构化示例本身不足以实现稳健的视角采择，需要明确的信念跟踪、成本建模和更丰富的环境来支持基于 LLM 的智能体进行社会化协作。

Abstract: Recent advances in large language models (LLMs) and reasoning frameworks have
opened new possibilities for improving the perspective -taking capabilities of
autonomous agents. However, tasks that involve active perception, collaborative
reasoning, and perspective taking (understanding what another agent can see or
knows) pose persistent challenges for current LLM-based systems. This study
investigates the potential of structured examples derived from transformed
solution graphs generated by the Fast Downward planner to improve the
performance of LLM-based agents within a ReAct framework. We propose a
structured solution-processing pipeline that generates three distinct
categories of examples: optimal goal paths (G-type), informative node paths
(E-type), and step-by-step optimal decision sequences contrasting alternative
actions (L-type). These solutions are further converted into ``thought-action''
examples by prompting an LLM to explicitly articulate the reasoning behind each
decision. While L-type examples slightly reduce clarification requests and
overall action steps, they do not yield consistent improvements. Agents are
successful in tasks requiring basic attentional filtering but struggle in
scenarios that required mentalising about occluded spaces or weighing the costs
of epistemic actions. These findings suggest that structured examples alone are
insufficient for robust perspective-taking, underscoring the need for explicit
belief tracking, cost modelling, and richer environments to enable socially
grounded collaboration in LLM-based agents.

</details>


### [315] [LeanGeo: Formalizing Competitional Geometry problems in Lean](https://arxiv.org/abs/2508.14644)
*Chendong Song,Zihan Wang,Frederick Pu,Haiming Wang,Xiaohan Lin,Junqi Liu,Jia Li,Zhengying Liu*

Main category: cs.AI

TL;DR: LeanGeo通过Lean 4定理证明器提供了一个统一的框架来形式化和求解几何问题，并包含一个针对IMO等竞赛问题的基准测试，用于评估大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有的几何求解系统难以在一个统一的框架内表达问题，这限制了它们与其他数学领域的集成。此外，由于几何证明通常依赖于直观的图表，因此验证几何问题特别具有挑战性。LeanGeo旨在通过提供一个统一的正式系统来解决这些问题，实现严格的证明验证和与其他数学领域的无缝集成。

Method: LeanGeo是一个在Lean 4定理证明器中形式化和求解几何问题的统一框架。它包含一个高层几何定理的综合库，并利用Lean的基础逻辑进行严格的证明验证和与Mathlib的集成。LeanGeo-Bench是为此框架构建的一个基准测试，包含了来自国际数学奥林匹克（IMO）等高级来源的问题，用于评估大型语言模型在该基准上的表现。

Result: 评估结果显示了最先进的大型语言模型在LeanGeo-Bench上的能力和局限性，突显了在自动化几何推理方面进一步发展的必要性。

Conclusion: LeanGeo是一个统一的几何问题形式化和求解框架，用于Lean 4定理证明器，并附带一个名为LeanGeo-Bench的基准测试，用于评估大型语言模型在几何推理方面的能力。该系统通过将几何定理整合到Lean的基础逻辑中，实现了严格的证明验证和与其他数学领域的无缝集成，为解决几何问题提供了一个统一的框架。

Abstract: Geometry problems are a crucial testbed for AI reasoning capabilities. Most
existing geometry solving systems cannot express problems within a unified
framework, thus are difficult to integrate with other mathematical fields.
Besides, since most geometric proofs rely on intuitive diagrams, verifying
geometry problems is particularly challenging. To address these gaps, we
introduce LeanGeo, a unified formal system for formalizing and solving
competition-level geometry problems within the Lean 4 theorem prover. LeanGeo
features a comprehensive library of high-level geometric theorems with Lean's
foundational logic, enabling rigorous proof verification and seamless
integration with Mathlib. We also present LeanGeo-Bench, a formal geometry
benchmark in LeanGeo, comprising problems from the International Mathematical
Olympiad (IMO) and other advanced sources. Our evaluation demonstrates the
capabilities and limitations of state-of-the-art Large Language Models on this
benchmark, highlighting the need for further advancements in automated
geometric reasoning. We open source the theorem library and the benchmark of
LeanGeo at https://github.com/project-numina/LeanGeo/tree/master.

</details>


### [316] [Entropy-Constrained Strategy Optimization in Urban Floods: A Multi-Agent Framework with LLM and Knowledge Graph Integration](https://arxiv.org/abs/2508.14654)
*Peilin Ji,Xiao Xue,Simeng Wang,Wenhao Yan*

Main category: cs.AI

TL;DR: H-J框架通过集成LLM和多主体协调，提升了城市洪水响应的效率和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 城市洪水频发给应急调度系统带来严峻挑战，现有方法在处理多目标权衡、动态环境变化及LLM生成策略的不稳定性方面存在不足。

Method: 本文提出了一种名为H-J的分层多主体框架，该框架集成了知识引导提示、熵约束生成和反馈驱动优化，构建了一个从多源感知到策略执行和持续改进的闭环流程。

Result: 实验结果表明，H-J框架在交通流畅度、任务成功率和系统鲁棒性方面优于基线方法，证明了其在城市洪水响应中的有效性。

Conclusion: H-J框架在解决城市洪水响应中的不确定性感知、知识约束和多主体协调方面表现出色，有望提升城市应对洪水的能力。

Abstract: In recent years, the increasing frequency of extreme urban rainfall events
has posed significant challenges to emergency scheduling systems. Urban
flooding often leads to severe traffic congestion and service disruptions,
threatening public safety and mobility. However, effective decision making
remains hindered by three key challenges: (1) managing trade-offs among
competing goals (e.g., traffic flow, task completion, and risk mitigation)
requires dynamic, context-aware strategies; (2) rapidly evolving environmental
conditions render static rules inadequate; and (3) LLM-generated strategies
frequently suffer from semantic instability and execution inconsistency.
Existing methods fail to align perception, global optimization, and multi-agent
coordination within a unified framework. To tackle these challenges, we
introduce H-J, a hierarchical multi-agent framework that integrates
knowledge-guided prompting, entropy-constrained generation, and feedback-driven
optimization. The framework establishes a closed-loop pipeline spanning from
multi-source perception to strategic execution and continuous refinement. We
evaluate H-J on real-world urban topology and rainfall data under three
representative conditions: extreme rainfall, intermittent bursts, and daily
light rain. Experiments show that H-J outperforms rule-based and
reinforcement-learning baselines in traffic smoothness, task success rate, and
system robustness. These findings highlight the promise of uncertainty-aware,
knowledge-constrained LLM-based approaches for enhancing resilience in urban
flood response.

</details>


### [317] [MCP-Universe: Benchmarking Large Language Models with Real-World Model Context Protocol Servers](https://arxiv.org/abs/2508.14704)
*Ziyang Luo,Zhiqi Shen,Wenzhuo Yang,Zirui Zhao,Prathyusha Jwalapuram,Amrita Saha,Doyen Sahoo,Silvio Savarese,Caiming Xiong,Junnan Li*

Main category: cs.AI

TL;DR: MCP-Universe是第一个全面的基准，用于评估LLM在现实世界任务中的表现，通过与MCP服务器交互。现有模型如GPT-5表现出局限性，并且面临长上下文和未知工具的挑战。该项目还开源了一个可扩展的评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有的基准测试过于简单，无法捕捉到长时推理和大而陌生的工具空间等现实应用挑战。为了解决这个关键的差距，我们引入了MCP-Universe。

Method: 我们引入了MCP-Universe，这是第一个专门为评估LLM在现实和困难任务中的表现而设计的综合基准，通过与真实世界的MCP服务器进行交互。该基准涵盖了6个核心领域，跨越11个不同的MCP服务器：位置导航、存储库管理、金融分析、3D设计、浏览器自动化和网络搜索。为了确保严格的评估，我们实施了基于执行的评估器，包括用于代理格式合规性的格式评估器、用于时间不变内容匹配的静态评估器以及自动检索实时地面真理用于时间敏感任务的动态评估器。

Result: 通过对GPT-5、Grok-4和Claude-4.0-Sonnet等SOTA模型的广泛评估，我们发现即使是这些领先模型也表现出显著的性能限制。此外，该基准测试对LLM代理提出了重大的长上下文挑战，因为随着交互步骤的增加，输入令牌的数量会迅速增加。它还引入了一个未知的工具挑战，因为LLM代理通常不熟悉MCP服务器的精确用法。值得注意的是，像Cursor这样的企业级代理的表现并不优于标准的ReAct框架。

Conclusion: 现有的基准测试过于简单，无法在长时推理和大型、不熟悉的工具空间等现实应用挑战中对LLM进行充分评估。MCP-Universe是第一个旨在通过与真实世界MCP服务器交互来评估LLM在现实和困难任务中的综合基准。通过对领先的LLM进行广泛评估，即使是像GPT-5、Grok-4和Claude-4.0-Sonnet这样的SOTA模型也表现出显著的性能限制。此外，该基准测试对LLM代理提出了重大的长上下文挑战和未知的工具挑战。我们开源了具有UI支持的可扩展评估框架，以促进MCP生态系统的创新。

Abstract: The Model Context Protocol has emerged as a transformative standard for
connecting large language models to external data sources and tools, rapidly
gaining adoption across major AI providers and development platforms. However,
existing benchmarks are overly simplistic and fail to capture real application
challenges such as long-horizon reasoning and large, unfamiliar tool spaces. To
address this critical gap, we introduce MCP-Universe, the first comprehensive
benchmark specifically designed to evaluate LLMs in realistic and hard tasks
through interaction with real-world MCP servers. Our benchmark encompasses 6
core domains spanning 11 different MCP servers: Location Navigation, Repository
Management, Financial Analysis, 3D Design, Browser Automation, and Web
Searching. To ensure rigorous evaluation, we implement execution-based
evaluators, including format evaluators for agent format compliance, static
evaluators for time-invariant content matching, and dynamic evaluators that
automatically retrieve real-time ground truth for temporally sensitive tasks.
Through extensive evaluation of leading LLMs, we find that even SOTA models
such as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit
significant performance limitations. In addition, our benchmark poses a
significant long-context challenge for LLM agents, as the number of input
tokens increases rapidly with the number of interaction steps. Moreover, it
introduces an unknown-tools challenge, as LLM agents often lack familiarity
with the precise usage of the MCP servers. Notably, enterprise-level agents
like Cursor cannot achieve better performance than standard ReAct frameworks.
Beyond evaluation, we open-source our extensible evaluation framework with UI
support, enabling researchers and practitioners to seamlessly integrate new
agents and MCP servers while fostering innovation in the rapidly evolving MCP
ecosystem.

</details>


### [318] [Data-Driven Probabilistic Evaluation of Logic Properties with PAC-Confidence on Mealy Machines](https://arxiv.org/abs/2508.14710)
*Swantje Plambeck,Ali Salamati,Eyke Huellermeier,Goerschwin Fey*

Main category: cs.AI

TL;DR: Data-driven approach using PAC learning to find safety probability for CPS (Mealy machines) on finite horizon, with active learning and validated on lane-keeping system.


<details>
  <summary>Details</summary>
Motivation: Cyber-Physical Systems (CPS) require powerful models for tasks like verification, diagnosis, or debugging. However, suitable models are often unavailable, and manual extraction is difficult. Data-driven approaches offer a solution for these tasks based on collected system data.

Method: The paper proposes a data-driven approach based on the Probably Approximately Correct (PAC) learning paradigm to determine the safety probability of discrete Cyber-Physical Systems (CPS) with a discrete abstraction in the form of a Mealy machine on a finite horizon of n time steps. The learning process follows an active learning paradigm, where new learning data is sampled in a guided way after an initial learning set is collected.

Result: The paper establishes a connection between discrete logic and probabilistic reachability analysis of systems, providing additional confidence in the determined safety probability.

Conclusion: This paper proposes a data-driven approach using PAC learning and active learning to determine the safety probability of discrete Cyber-Physical Systems (CPS) modeled as Mealy machines on a finite horizon. The approach provides confidence in the determined probability and is validated with a case study on an automated lane-keeping system.

Abstract: Cyber-Physical Systems (CPS) are complex systems that require powerful models
for tasks like verification, diagnosis, or debugging. Often, suitable models
are not available and manual extraction is difficult. Data-driven approaches
then provide a solution to, e.g., diagnosis tasks and verification problems
based on data collected from the system. In this paper, we consider CPS with a
discrete abstraction in the form of a Mealy machine. We propose a data-driven
approach to determine the safety probability of the system on a finite horizon
of n time steps. The approach is based on the Probably Approximately Correct
(PAC) learning paradigm. Thus, we elaborate a connection between discrete logic
and probabilistic reachability analysis of systems, especially providing an
additional confidence on the determined probability. The learning process
follows an active learning paradigm, where new learning data is sampled in a
guided way after an initial learning set is collected. We validate the approach
with a case study on an automated lane-keeping system.

</details>


### [319] [Privileged Self-Access Matters for Introspection in AI](https://arxiv.org/abs/2508.14802)
*Siyuan Song,Harvey Lederman,Jennifer Hu,Kyle Mahowald*

Main category: cs.AI

TL;DR: AI的内省能力需要更严格的定义，现有的模型可能只是“看起来”有内省能力，但实际上并未达到。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型能力的增强，理解它们是否能够“内省”变得越来越重要，但目前缺乏统一的定义。

Method: 提出了一种比最近提出的“轻量级”定义更厚的内省定义：AI的内省是任何通过比具有相同或更低计算成本的第三方可用的过程更可靠的过程来产生有关内部状态的信息的过程。

Result: 通过实验，表明大型语言模型（LLMs）在看似具有轻量级内省能力的同时，未能满足我们提出的更严格的内省定义。

Conclusion: AI模型是否具有内省能力，以及如何定义AI的内省能力。

Abstract: Whether AI models can introspect is an increasingly important practical
question. But there is no consensus on how introspection is to be defined.
Beginning from a recently proposed ''lightweight'' definition, we argue instead
for a thicker one. According to our proposal, introspection in AI is any
process which yields information about internal states through a process more
reliable than one with equal or lower computational cost available to a third
party. Using experiments where LLMs reason about their internal temperature
parameters, we show they can appear to have lightweight introspection while
failing to meaningfully introspect per our proposed definition.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [320] [InverTwin: Solving Inverse Problems via Differentiable Radio Frequency Digital Twin](https://arxiv.org/abs/2508.14204)
*Xingyu Chen,Jianrong Ding,Kai Zheng,Xinmin Fang,Xinyu Zhang,Chris Xiaoxuan Lu,Zhengxiong Li*

Main category: eess.SP

TL;DR: InverTwin框架通过路径空间微分和雷达代理模型，实现了射频数字孪生的双向交互，解决了射频优化中的关键技术难题，并成功应用于射频感知系统。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统射频仿真器在射频感知应用中单向交互的局限性，并实现射频数字孪生的双向交互。

Method: InverTwin框架利用路径空间微分来解决复杂仿真函数中的不连续性问题，并采用雷达代理模型来缓解由射频信号周期性引起的局部非凸性问题，从而实现平滑梯度传播和数字孪生模型的鲁棒优化。

Result: 实验证明，InverTwin框架在增强数据驱动和模型驱动的射频感知系统以实现数字孪生重建方面，具有通用性和有效性。

Conclusion: InverTwin框架通过实现虚拟与物理领域的双向交互，成功创建了射频（RF）数字孪生，克服了传统射频仿真器单向交互的局限性，并在射频感知应用中展现了其有效性。

Abstract: Digital twins (DTs), virtual simulated replicas of physical scenes, are
transforming various industries. However, their potential in radio frequency
(RF) sensing applications has been limited by the unidirectional nature of
conventional RF simulators. In this paper, we present InverTwin, an
optimization-driven framework that creates RF digital twins by enabling
bidirectional interaction between virtual and physical realms. InverTwin
overcomes the fundamental differentiability challenges of RF optimization
problems through novel design components, including path-space differentiation
to address discontinuity in complex simulation functions, and a radar surrogate
model to mitigate local non-convexity caused by RF signal periodicity. These
techniques enable smooth gradient propagation and robust optimization of the DT
model. Our implementation and experiments demonstrate InverTwin's versatility
and effectiveness in augmenting both data-driven and model-driven RF sensing
systems for DT reconstruction.

</details>


### [321] [Weakly-Convex Regularization for Magnetic Resonance Image Denoising](https://arxiv.org/abs/2508.14438)
*Akash Prabakar,Abhishek Shreekant Bhandiwad,Abijith Jagannath Kamath,Chandra Sekhar Seelamantula*

Main category: eess.SP

TL;DR: 提出了一种新的弱凸正则化函数设计方法，用于MRI去噪，提高了性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 深度学习技术在MRI去噪方面表现出色，但缺乏可解释性、可解释性和稳定性。这项工作旨在通过设计弱凸正则化函数来解决这些问题。

Method: 提出了一种构建性方法来设计用于MR图像去噪的弱凸正则化函数，并将其应用于设计具有可解释性和可证明收敛性的弱凸卷积神经网络。

Result: 所提出的技术在去噪性能上与最先进的技术相当，并且在减少去噪伪影方面表现更好，这对大脑微观结构建模有积极影响。

Conclusion: 这项工作提出了一种用于MRI图像去噪的弱凸正则化函数设计方法，该方法在去噪性能上可与最先进的技术相媲美，同时提供了可解释性、可解释性和稳定性，并可应用于设计具有可解释性和可证明收敛性的原型激活函数的弱凸卷积神经网络。

Abstract: Regularization for denoising in magnetic resonance imaging (MRI) is typically
achieved using convex regularization functions. Recently, deep learning
techniques have been shown to provide superior denoising performance. However,
this comes at the price of lack of explainability, interpretability and
stability, which are all crucial to MRI. In this work, we present a
constructive approach for designing weakly-convex regularization functions for
MR image denoising. We show that our technique performs on par with
state-of-the-art denoisers for diffusion-weighted MR image denoising. Our
technique can be applied to design weakly-convex convolutional neural networks
with prototype activation functions that impart interpretability and are
provably convergent. We also show that our technique exhibits fewer denoising
artifacts by demonstrating its effect on brain microstructure modelling.

</details>


### [322] [Pinching-Antenna Systems-Enabled Multi-User Communications: Transmission Structures and Beamforming Optimization](https://arxiv.org/abs/2508.14458)
*Jingjing Zhao,Haowen Song,Xidong Mu,Kaiquan Cai,Yanbo Zhu,Yuanwei Liu*

Main category: eess.SP

TL;DR: 本文提出了三种用于多波导捏合天线系统（PASS）的传输结构（WM、WD、WS），并采用惩罚对偶分解（PDD）算法优化了多用户通信中的最大最小公平性（MMF）问题。结果显示PASS性能优于传统系统，WS适合单播，WM适合多播。


<details>
  <summary>Details</summary>
Motivation: 为了在多用户通信场景下应用多波导捏合天线系统（PASS），需要设计有效的传输结构和优化联合信号处理与波束成形，以实现可靠的通信和提升系统性能。

Method: 本文提出了三种实际的传输结构：波导复用（WM）、波导分配（WD）和波导开关（WS）。针对这些结构，研究了联合基带信号处理和捏合波束成形设计，并针对多用户组播通信系统（包括单播通信作为特例）进行了优化。通过引入惩罚对偶分解（PDD）算法，解决了复杂的非凸最大最小公平性（MMF）问题，并提出了一种低复杂度算法用于WS结构下的单播通信。

Result: 数值结果表明，与传统的固定位置天线系统相比，PASS在最大最小公平性（MMF）性能上有了显著提升。WS结构适用于单播通信，WM结构适用于多播通信。此外，当用户地理位置分散时，WD和WM结构的性能差距可以显著减小。

Conclusion: PASS在多用户通信方面表现出色，与传统天线系统相比，在最大最小公平性（MMF）性能上有了显著提升。具体而言，WS结构适用于单播通信，而WM结构适用于多播通信。当用户地理位置分散时，WD和WM结构的性能差距可以显著减小。

Abstract: Pinching-antenna systems (PASS) represent an innovative advancement in
flexible-antenna technologies, aimed at significantly improving wireless
communications by ensuring reliable line-of-sight connections and dynamic
antenna array reconfigurations. To employ multi-waveguide PASS in multi-user
communications, three practical transmission structures are proposed, namely
waveguide multiplexing (WM), waveguide division (WD), and waveguide switching
(WS). Based on the proposed structures, the joint baseband signal processing
and pinching beamforming design is studied for a general multi-group multicast
communication system, with the unicast communication encompassed as a special
case. A max-min fairness problem is formulated for each proposed transmission
structure, subject to the maximum transmit power constraint. For WM, to solve
the highly-coupled and non-convex MMF problem with complex exponential and
fractional expressions, a penalty dual decomposition (PDD)-based algorithm is
invoked for obtaining locally optimal solutions. Specifically, the augmented
Lagrangian relaxation is first applied to alleviate the stringent coupling
constraints, which is followed by the block decomposition over the resulting
augmented Lagrangian function. Then, the proposed PDD-based algorithm is
extended to solve the MMF problem for both WD and WS. Furthermore, a
low-complexity algorithm is proposed for the unicast case employing the WS
structure, by simultaneously aligning the signal phases and minimizing the
large-scale path loss at each user. Finally, numerical results reveal that: 1)
the MMF performance is significantly improved by employing the PASS compared to
conventional fixed-position antenna systems; 2) WS and WM are suitable for
unicast and multicast communications, respectively; 3) the performance gap
between WD and WM can be significantly alleviated when the users are
geographically isolated.

</details>


### [323] [FPGA Design and Implementation of Fixed-Point Fast Divider Using Goldschmidt Division Algorithm and Mitchell Multiplication Algorithm](https://arxiv.org/abs/2508.14611)
*Jinkun Yang*

Main category: eess.SP

TL;DR: 本论文提出了一种基于Goldschmidt和Mitchell算法的可变比特宽度定点快速除法器，在FPGA上实现了高精度、低延迟和低资源占用的优点。


<details>
  <summary>Details</summary>
Motivation: 提出一种适用于资源受限的高性能FPGA系统的可变比特宽度定点快速除法器。

Method: 本论文提出了一种使用Goldschmidt除法算法和Mitchell乘法算法的可变比特宽度定点快速除法器。该除法器使用Verilog HDL描述，并在Xilinx XC7Z020-2CLG400I FPGA上实现。

Result: 所提出的除法器实现了超过99%的计算精度，最小延迟为99.1 ns，比现有的单精度除法器快31.7 ns。与使用Vedic乘法器的Goldschmidt除法器相比，所提出的设计将Slice Registers减少了46.68%，Slice LUTs减少了4.93%，Slices减少了11.85%，精度损失不到1%，延迟仅增加了24.1 ns。

Conclusion: 该设计在计算速度和资源利用率之间取得了更好的平衡，适用于资源受限的高性能FPGA系统。

Abstract: This paper presents a variable bit-width fixed-point fast divider using
Goldschmidt division algorithm and Mitchell multiplication algorithm. Described
using Verilog HDL and implemented on a Xilinx XC7Z020-2CLG400I FPGA, the
proposed divider achieves over 99% computational accuracy with a minimum
latency of 99.1 ns, which is 31.7 ns faster than existing single-precision
dividers. Compared with a Goldschmidt divider using a Vedic multiplier, the
proposed design reduces Slice Registers by 46.68%, Slice LUTs by 4.93%, and
Slices by 11.85%, with less than 1% accuracy loss and only 24.1 ns additional
delay. These results demonstrate an improved balance between computational
speed and resource utilization, making the divider well-suited for
high-performance FPGA-based systems with strict resource constraints.

</details>


### [324] [Design of a Gm-C Dynamic Amplifier with High Linearity and High Temperature and Power Supply Voltage Stability](https://arxiv.org/abs/2508.14637)
*Jinkun Yang,Pengbin Xu*

Main category: eess.SP

TL;DR: 这篇论文介绍了一种克服温度和电源电压变化影响的 Gm-C 动态放大器，实现了高线性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统放大器在温度和电源电压变化时性能不稳定的问题，本研究旨在开发一种高线性度、高稳定性的 Gm-C 动态放大器。

Method: 该放大器采用非对称差分对来提高跨导线性度，并结合恒定跨导偏置电路来维持跨导和增益的稳定。

Result: 该放大器在 -40 mV 至 40 mV 的差分输入范围内保持接近恒定的增益，总谐波失真（THD）达到 70.5 dB。在 1 mV 差分输入、±10% 电源电压波动和 -40°C 至 120°C 温度变化下，增益的标准差为 262m，分布范围为 15.1 至 16.3。

Conclusion: 该研究 presented 了一个 Gm-C 动态放大器，其特点是高线性度以及在宽温度范围（-40°C 至 120°C）和电源电压波动（±10%）下的高稳定性。

Abstract: This paper presents a Gm-C dynamic amplifier with high linearity and high
temperature and power supply voltage stability. The main part of the amplifier
employs two asymmetric differential pairs to enhance transconductance
linearity. The amplifier maintains a nearly constant gain within a differential
input range of -40 mV to 40 mV, and achieves a total harmonic distortion (THD)
of 70.5 dB. The bias part of the amplifier adopts a constant-gm bias circuit,
which improves the temperature and supply voltage stability of the amplifier's
transconductance and gain. When the differential input is 1 mV, the power
supply voltage fluctuates by $\pm$10%, and the temperature varies between
-40$\mathrm{^\circ C}$ and 120$\mathrm{^\circ C}$, the standard deviation of
the gain distribution is 262m, and the distribution range is from 15.1 to 16.3.

</details>


### [325] [Failure Tolerant Phase-Only Indoor Positioning via Deep Learning](https://arxiv.org/abs/2508.14739)
*Fatih Ayten,Mehmet C. Ilter,Akshay Jain,Ossi Kaltiokallio,Jukka Talvitie,Elena Simona Lohan,Henk Wymeersch,Mikko Valkama*

Main category: eess.SP

TL;DR: 本文提出了一种新的深度学习方法，用于在存在天线故障的情况下进行精确的纯相位定位，并取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 在实际场景中，天线故障会严重影响载波相位定位（CPP）的性能。针对这一挑战，需要开发一种能够应对天线故障的纯相位定位方法。

Method: 提出了一种新的基于深度学习的定位方法，该方法利用双曲线交集原理，并且提出了一种能够抵抗天线故障的处理和学习机制。

Result: 与现有方法相比，所提出的基于深度学习的方法在定位精度方面取得了显著的改进，并且在存在天线故障的情况下表现出鲁棒性。

Conclusion: 所提出的基于深度学习的方法在存在天线故障的情况下，能够实现鲁棒且精确的定位，证明了数据驱动、容忍损伤的纯相位定位机制的可行性。大量的数值结果证明，与现有技术相比，所提出的方法在定位精度方面有了显著的提高。

Abstract: High-precision localization turns into a crucial added value and asset for
next-generation wireless systems. Carrier phase positioning (CPP) enables
sub-meter to centimeter-level accuracy and is gaining interest in 5G-Advanced
standardization. While CPP typically complements time-of-arrival (ToA)
measurements, recent literature has introduced a phase-only positioning
approach in a distributed antenna/MIMO system context with minimal bandwidth
requirements, using deep learning (DL) when operating under ideal hardware
assumptions. In more practical scenarios, however, antenna failures can largely
degrade the performance. In this paper, we address the challenging phase-only
positioning task, and propose a new DL-based localization approach harnessing
the so-called hyperbola intersection principle, clearly outperforming the
previous methods. Additionally, we consider and propose a processing and
learning mechanism that is robust to antenna element failures. Our results show
that the proposed DL model achieves robust and accurate positioning despite
antenna impairments, demonstrating the viability of data-driven,
impairment-tolerant phase-only positioning mechanisms. Comprehensive set of
numerical results demonstrates large improvements in localization accuracy
against the prior art methods.

</details>


### [326] [Full-Duplex Beamforming Optimization for Near-Field ISAC](https://arxiv.org/abs/2508.14753)
*Ahsan Nazar,Zhambyl Shaikhanov,Sennur Ulukus*

Main category: eess.SP

TL;DR: "This paper proposes a joint optimization framework for transmit and receive beamforming in near-field ISAC systems to minimize transmit power while satisfying rate constraints for multi-user communication and sensing. The proposed method uses alternating optimization, semidefinite relaxation, and Rayleigh quotient techniques. Simulation results show that FD-enabled near-field ISAC is more power-efficient than half-duplex and far-field systems."


<details>
  <summary>Details</summary>
Motivation: "Integrated Sensing and Communications (ISAC) is a promising technology for future wireless networks, enabling simultaneous communication and sensing using shared resources. This paper investigates the performance of full-duplex (FD) communication in near-field ISAC systems, where spherical-wave propagation introduces unique beam-focusing capabilities."

Method: "The approach employs alternating optimization combined with semidefinite relaxation and Rayleigh quotient techniques to address the non-convexity of the problem."

Result: "Simulation results demonstrate that FD-enabled near-field ISAC achieves superior power efficiency compared to half-duplex and far-field benchmarks, effectively detecting targets at identical angles while meeting communication requirements."

Conclusion: "FD-enabled near-field ISAC achieves superior power efficiency compared to half-duplex and far-field benchmarks, effectively detecting targets at identical angles while meeting communication requirements."

Abstract: Integrated Sensing and Communications (ISAC) is a promising technology for
future wireless networks, enabling simultaneous communication and sensing using
shared resources. This paper investigates the performance of full-duplex (FD)
communication in near-field ISAC systems, where spherical-wave propagation
introduces unique beam-focusing capabilities. We propose a joint optimization
framework for transmit and receive beamforming at the base station to minimize
transmit power while satisfying rate constraints for multi-user downlink
transmission, multi-user uplink reception, and multi-target sensing. Our
approach employs alternating optimization combined with semidefinite relaxation
and Rayleigh quotient techniques to address the non-convexity of the problem.
Simulation results demonstrate that FD-enabled near-field ISAC achieves
superior power efficiency compared to half-duplex and far-field benchmarks,
effectively detecting targets at identical angles while meeting communication
requirements.

</details>


### [327] [Deep Reinforcement Learning Based Routing for Heterogeneous Multi-Hop Wireless Networks](https://arxiv.org/abs/2508.14884)
*Brian Kim,Justin H. Kong,Terrence J. Moore,Fikadu T. Dagefu*

Main category: eess.SP

TL;DR: 提出了一种基于DQN的路由框架，用于异构多跳无线网络，通过改进邻近节点选择策略来提高性能、可扩展性和适应性。


<details>
  <summary>Details</summary>
Motivation: 现有的Q学习方法在处理大规模或动态网络拓扑（尤其是在拥有不同信道特性的异构网络中）时，由于Q表难以管理，存在可扩展性差和泛化能力弱的问题。为解决这些问题，提出DQN框架以提高可扩展性和适应性。

Method: 提出了一种新颖的基于深度Q网络（DQN）的路由框架，其中每个节点使用深度神经网络（DNN）来估计Q值，并联合选择下一跳中继和通信技术。为改进DNN的性能，提出了一种基于信道增益和节点间速率的邻近节点选择策略，而非简单的基于距离的方法。

Result: 仿真结果表明，所提出的邻近节点选择策略优于简单的基于距离的选择策略。此外，DQN方法在性能上优于各种基准方案，并能与最优方法相媲美。

Conclusion: 所提出的基于深度Q网络的路由框架在异构多跳无线网络中具有良好的性能，能够有效提高可扩展性和适应性，并且优于多种基准方案。

Abstract: Routing in multi-hop wireless networks is a complex problem, especially in
heterogeneous networks where multiple wireless communication technologies
coexist. Reinforcement learning (RL) methods, such as Q-learning, have been
introduced for decentralized routing by allowing nodes to make decisions based
on local observations. However, Q-learning suffers from scalability issues and
poor generalization due to the difficulty in managing the Q-table in large or
dynamic network topologies, especially in heterogeneous networks (HetNets) with
diverse channel characteristics. Thus, in this paper, we propose a novel deep
Q-network (DQN)-based routing framework for heterogeneous multi-hop wireless
networks to maximize the end-to-end rate of the route by improving scalability
and adaptability, where each node uses a deep neural network (DNN) to estimate
the Q-values and jointly select the next-hop relay and a communication
technology for transmission. To achieve better performance with the DNN,
selecting which nodes to exchange information is critical, as it not only
defines the state and action spaces but also determines the input to the DNN.
To this end, we propose neighbor node selection strategies based on channel
gain and rate between nodes rather than a simple distance-based approach for an
improved set of states and actions for DQN-based routing. During training, the
model experiences diverse network topologies to ensure generalization and
robustness, and simulation results show that the proposed neighbor node
selection outperforms simple distance-based selection. Further, we observe that
the DQN-based approach outperforms various benchmark schemes and performs
comparably to the optimal approach.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [328] [A Digital Twin-Based Simulation Framework for Safe Curve Speed Estimation Using Unity](https://arxiv.org/abs/2508.14046)
*Araf Rahman,M. Sabbir Salek,Mashrur Chowdhury,Wayne A. Sarasua*

Main category: eess.SY

TL;DR: 本研究利用基于物理的数字孪生，在Unity中创建一个虚拟环境，为道路曲线上的车辆估算更安全、更适应当前环境（如天气、车辆类型）的速度，并验证了该方法与实际数据的吻合度。


<details>
  <summary>Details</summary>
Motivation: 随着自动驾驶和联网汽车的普及，在曲线处准确估计安全速度变得越来越重要。目前广泛使用的AASHTO设计方法可能过于保守。

Method: 本研究提出了一个基于数字孪生的框架，利用Unity引擎开发的物理驱动的虚拟环境来估计曲线上的安全速度。该框架包括构建选定曲线的3D模型，集成参数化车辆模型，并模拟以确定车辆通过给定曲线的最大安全速度。

Result: 研究结果表明，数字孪生模拟产生的安全曲线速度估计与在曲线中观察到的实际世界速度值一致。

Conclusion: 本研究证明了基于物理的双数字孪生可以为车辆通过水平曲线提供更安全、更自适应的操作速度。

Abstract: Horizontal curves are often associated with roadway crashes due to speed
misjudgment and loss of control. With the growing adoption of autonomous and
connected vehicles, the accurate estimation of safe speed at curves is becoming
increasingly important. The widely used AASHTO design method for safe curve
speed estimation relies on an analytical equation based on a simplified point
mass model, which often uses conservative parameters to account for vehicular
and environmental variations. This paper presents a digital twin-based
framework for estimating safe speed at curves using a physics-driven virtual
environment developed in the Unity engine. In this framework, a real-world
horizontal road curve is selected, and vehicle speed data are collected using a
radar gun under various weather conditions. A 3D model of the road curve is
constructed in a Unity environment using roadway geometric and elevation data.
A parameterized vehicle model is integrated, allowing for variations in mass,
acceleration, and center of gravity to reflect different vehicle types and
loading scenarios. This simulation identifies the maximum safe speed at which a
vehicle can traverse the given curve, providing a more vehicle and
environment-specific estimate of the safe operating speed. The study validated
that the safe curve speed estimates generated by the simulation were consistent
with the real-world speed values observed at a curve. This study demonstrates
how a physics-based digital twin can estimate a safer and more adaptive
operating speed for vehicles traversing horizontal curves.

</details>


### [329] [Towards Unified Probabilistic Verification and Validation of Vision-Based Autonomy](https://arxiv.org/abs/2508.14181)
*Jordan Peper,Yan Miao,Sayan Mitra,Ivan Ruchkin*

Main category: eess.SY

TL;DR: 提出了一种新的方法，通过使用区间马尔可夫决策过程（interval MDPs）来统一验证模型和离线验证，为自动驾驶系统提供了更灵活、适应性更强的安全保证，解决了现有方法在处理测试环境不确定性和分布变化时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的基于抽象的验证技术，特别是针对视觉引导的自主系统，其安全保证依赖于对错误有界或具有已知唯一分布等严格的假设。这些假设过于死板，限制了保证的有效性，尤其是在多样化和不确定的测试环境。因此，需要一种更灵活的方法来处理这些挑战。

Method: 提出了一种统一感知验证模型及其离线验证的方法，该方法利用区间马尔可夫决策过程（interval MDPs），能够提供灵活的端到端保证，并直接适应测试时的分布外（out-of-distribution）条件。

Result: 在具有明确状态估计分布的合成感知马尔可夫链（Markov chain）和山地汽车基准（mountain car benchmark）上进行了评估，结果表明该方法能够提供严格但精确的系统安全界限。

Conclusion: 该方法能够为整体系统安全提供严格但精确的界限。

Abstract: Precise and comprehensive situational awareness is a critical capability of
modern autonomous systems. Deep neural networks that perceive task-critical
details from rich sensory signals have become ubiquitous; however, their
black-box behavior and sensitivity to environmental uncertainty and
distribution shifts make them challenging to verify formally. Abstraction-based
verification techniques for vision-based autonomy produce safety guarantees
contingent on rigid assumptions, such as bounded errors or known unique
distributions. Such overly restrictive and inflexible assumptions limit the
validity of the guarantees, especially in diverse and uncertain test-time
environments. We propose a methodology that unifies the verification models of
perception with their offline validation. Our methodology leverages interval
MDPs and provides a flexible end-to-end guarantee that adapts directly to the
out-of-distribution test-time conditions. We evaluate our methodology on a
synthetic perception Markov chain with well-defined state estimation
distributions and a mountain car benchmark. Our findings reveal that we can
guarantee tight yet rigorous bounds on overall system safety.

</details>


### [330] [A Data-Based Review of Battery Electric Vehicle and Traction Inverter Trends](https://arxiv.org/abs/2508.14224)
*Christoph Sachs,Martin Neuburger*

Main category: eess.SY

TL;DR: BEV 的续航里程和能效可以通过部分负载优化的多电平逆变器得到改善，其中 3L-TNPC 逆变器在成本效益方面表现尤为突出。


<details>
  <summary>Details</summary>
Motivation: 尽管电池电动汽车（BEV）在过去十年中取得了显著进展，但动力总成能量损失仍然限制了实际续航里程并增加了成本。

Method: 结合欧洲市场 BEV 数据集（2010-2025 年车型）和详细的逆变器-电机协同仿真，评估了传统两电平（2L）六半桥（B6）逆变器（使用硅（Si）和碳化硅（SiC）器件）以及两种针对部分负载运行定制的三电平（3L）T型中点箝位（TNPC）和有源中点箝位（ANPC）逆变器的未来效率和成本效益。

Result: 与基于 SiC 的 2L-B6 基线相比，采用仅增加 30% 的 SiC 芯片面积的 3L-TNPC 逆变器可将行驶工况下的动力总成损失降低 0.67 kWh/100 km。

Conclusion: 部分负载优化的多电平逆变器（MLI）被确定为进一步降低 BEV 能耗和总系统成本的成本有效途径。

Abstract: Battery electric vehicles (BEVs) have advanced significantly during the past
decade, yet drivetrain energy losses continue to restrict practical range and
elevate cost. A dataset comprising more than 1000 European-market BEVs (model
years 2010-2025) is combined with detailed inverter-motor co-simulation to
chart technology progress for and quantify the efficiency and cost-saving
potential of partial-load optimised multi-level inverter (MLI) for 2030.
Average drive-cycle range has climbed from 135 km to 455 km, while
fleet-average energy consumption has remained virtually constant. Three
inverter topologies are assessed to evaluate future efficiency and cost
enhancements: a conventional two-level (2L) six halfbridge (B6) inverter with
silicon (Si) and silicon carbide (SiC) devices, and two three-level (3L) T-type
neutral point clamped (TNPC) and active neutral point clamped (ANPC) inverters
tailored for partial-load operation. The 3L-TNPC inverter, realised with only
30% additional SiC chip area, lowers drive-cycle drivetrain losses by 0.67
kWh/100 km relative to a SiC 2L-B6 baseline. These results identify
partial-load optimised MLIs as a cost-effective route to further reduce BEV
energy consumption and total system cost.

</details>


### [331] [Design and Optimization of a Hybrid VLC/THz Infrastructure-to-Vehicle Communication System for Intelligent Transportation](https://arxiv.org/abs/2508.14225)
*Yusef Modami,Hamzeh Beiranvand,Mohammad Taghi Dabiri*

Main category: eess.SY

TL;DR: 本研究提出了一种利用LED路灯提供照明和高速通信的混合VLC/THz系统，以支持未来的智能交通系统。该系统通过优化通信和照明覆盖率，提高了在各种环境条件下的性能和可靠性。


<details>
  <summary>Details</summary>
Motivation: 为了支持智慧城市中未来由6G驱动的智能交通系统（ITS），并利用现有的LED路灯基础设施同时提供节能照明和高速无线连接。

Method: 提出了一种混合基础设施到车辆（I2V）通信框架，该框架集成了可见光通信（VLC）和互补的太赫兹（THz）天线阵列。通过网格搜索方法优化了物理布局，以最大限度地提高照明覆盖率、接收功率、信噪比（SNR）、信干噪比（SINR），并最小化中断概率。

Result: 通过优化，照明覆盖率从35%提高到97%，混合通信覆盖率从49%提高到99.9%。在极端环境条件下，混合系统保持高达99%的覆盖率，而单独使用VLC仅为69%。

Conclusion: 该方案展示了所提出系统的可扩展性、成本效益和实用性，可用于下一代智能交通系统的部署。

Abstract: This paper proposes a hybrid infrastructure-to-vehicle (I2V) communication
framework to support future 6G-enabled intelligent transportation systems (ITS)
in smart cities. Leveraging existing LED streetlighting infrastructure, the
system simultaneously delivers energy-efficient illumination and high-speed
wireless connectivity. The proposed scheme integrates visible light
communication (VLC) with a complementary ter-ahertz (THz) antenna array to
overcome VLC limitations under high ambient light and adverse weather
conditions. Key con-tributions include the design of a VLC/THz access network,
seamless integration with lighting infrastructure, a proposed
switching-combination (PSC) mechanism, and a physical layout optimization
strategy. Using a grid search method, thousands of configurations were
evaluated to maximize lighting coverage, re-ceived power, signal-to-noise ratio
(SNR), signal-to-interference-and-noise ratio (SINR), and minimize outage
probability. Results show that optimized lighting coverage improves from 35% to
97%, while hybrid communication coverage increases from 49%to 99.9% at the same
power level. Under extreme environmental conditions, the hybrid system
maintains up to 99% coverage, compared to 69% with VLC alone. These results
demonstrate the scalability, cost-efficiency, and practicality of the proposed
system for next-generation ITS deployment.

</details>


### [332] [Autonomy at Levels for Spacecraft](https://arxiv.org/abs/2508.14226)
*Daniel Baker,Jeremy Wojcik,Sean Phillips*

Main category: eess.SY

TL;DR: 自主性分层：将自主性嵌入航天器所有层级，通过集成、分布式和嵌入式自主单元实现全面自主。


<details>
  <summary>Details</summary>
Motivation: 将自主性嵌入并贯穿于航天器的所有层级，以实现更全面的自主能力。

Method: 使用系统工程方法，将航天器分解为系统、子系统、组件等多个层级，并在所有层级嵌入自主性单元，形成集成、分布式和嵌入式的“自主系统”。

Result: 通过连接控制回路和自主回路，说明了实现“自主性分层”的方法。

Conclusion: 文章提出“自主性分层”的理念，即自主性应嵌入并贯穿于整个航天器。

Abstract: Autonomy at Levels is the idea that autonomy should be embedded within and
throughout a spacecraft. Using Systems Engineering methods a spacecraft is
typically decomposed into systems, subsystems, assemblies, components, and so
on. All these decomposition levels within all the spacecraft's systems, could
and should have autonomy elements built in. As a result, the "autonomy system"
is made of autonomy elements or units that are integrated, distributed and
embedded within the whole spacecraft. This is like how the power system would
be designed and implemented. Linking control loops and autonomy loops
illustrates how to achieve Autonomy at Levels.

</details>


### [333] [Robust tracking MPC for perturbed nonlinear systems -- Extended version](https://arxiv.org/abs/2508.14248)
*Marco Polver,Daniel Limon,Fabio Previdi,Antonio Ferramosca*

Main category: eess.SY

TL;DR: 本论文提出了一种鲁棒预测控制器，用于跟踪受约束非线性系统的分段常数设定点，并能处理不确定性。


<details>
  <summary>Details</summary>
Motivation: 为受约束的非线性系统提出一种能够跟踪分段常数设定点信号的新颖鲁棒预测控制器。

Method: 通过在每一步求解最优控制问题来扩展非线性MPC的跟踪能力，该问题会惩罚预测的名义系统轨迹与作为决策变量引入的人工参考之间的偏差，以及人工参考与设定点之间的距离。通过施加考虑了不确定性影响的保守约束来确保鲁棒可行性，并通过适当的终端成本和扩展的稳定终端约束来保证收敛到任何可行设定点的邻域。对于无法达到的设定点，也证明了收敛到最优可达稳态输出的邻域。

Result: 该控制器能够处理有界但未必可加的扰动，并能保证收敛到可行设定点或最优可达稳态输出的邻域。

Conclusion: 本论文提出了一种新颖的鲁棒预测控制器，用于处理受约束的非线性系统，并且能够跟踪分段常数设定点信号。

Abstract: This paper presents a novel robust predictive controller for constrained
nonlinear systems that is able to track piece-wise constant setpoint signals.
The tracking model predictive controller presented in this paper extends the
nonlinear MPC for tracking to the more complex case of nonlinear systems
subject to bounded and not necessarily additive perturbations. The optimal
control problem that is solved at each step penalizes the deviation of the
predicted nominal system trajectory from an artificial reference, which is
added as a decision variable, as well as the distance between the artificial
reference and the setpoint. Robust feasibility is ensured by imposing
conservative constraints that take into account the effect of uncertainties and
convergence to a neighborhood of any feasible setpoint is guaranteed by means
of an appropriate terminal cost and an extended stabilizing terminal
constraint. In the case of unreachable setpoints, convergence to a neighborhood
of the optimal reachable steady output is also proved.

</details>


### [334] [Grid-Edge Energy-Flexible Technologies: A Comparative Analysis Across Generators, Loads, and Energy Storage Systems](https://arxiv.org/abs/2508.14297)
*Jesus Silva-Rodriguez,Tianxia Zhao,Ran Mo,Xingpeng Li*

Main category: eess.SY

TL;DR: 一篇关于如何在现代电力系统中通过整合发电机、储能系统和负荷来提高能源灵活性的综述。


<details>
  <summary>Details</summary>
Motivation: 为了促进日益增长的可变可再生能源（RES）并网，并维持电网平衡与稳定，需要提高现代电力系统的能源灵活性。

Method: 本文采用综合分析方法，研究了发电机、储能系统和负荷在现代电力系统中的作用和机制。通过案例研究和实例，探讨了间歇性缓解、削峰和能源储备等灵活性服务。

Result: 本文全面探讨了能源灵活性，涵盖了其定义、重要性、关键技术（发电机、储能系统、负荷）、控制策略、优缺点以及在间歇性缓解、削峰和能源储备等服务中的应用，并通过案例研究进行了论证。

Conclusion: 本文提出的能源灵活性优化方法为实现更可持续和有弹性的能源未来提供了路线图，通过整合发电机、储能系统和负荷等多种资源。

Abstract: This review analysis presents a comprehensive exploration of energy
flexibility in modern power systems. It examines the roles and mechanisms of
flexible technologies across three main categories: generators, energy storage
systems (ESS), and loads. Energy flexibility is defined as the ability to
dynamically adjust supply and/or demand in response to grid conditions to
maintain balance and stability. This is of particular importance to facilitate
the integration of the growing variable renewable energy sources (RES) into
modern power grids. Additionally, traditional supply-side mechanisms to
maintain balance and stability are complemented by advancements in demand-side
management and demand response strategies, which enable loads to adjust
consumption patterns and schedules in response to grid requirements. ESS are
also explored to further enhance flexibility by absorbing excess generation
and/or supplying large load increases that are not able to be met by the less
flexible resources. This paper also explores specific flexibility technologies,
examining their characteristics, control strategies, advantages, and
limitations. Energy flexibility services are also categorized into
intermittency mitigation, peak shaving, and energy reserve provisioning. Each
service is supported by case studies and examples demonstrating how different
resources respond to varying conditions. Ultimately, the findings and reviews
of the various flexible resources in this paper provide a roadmap for
optimizing energy flexibility across diverse resource types, paving the way for
a more sustainable and resilient energy future.

</details>


### [335] [Iterative Youla-Kucera Loop Shaping For Precision Motion Control](https://arxiv.org/abs/2508.14309)
*Xiaohai Hu,Jason Laks,Guoxiao Guo,Xu Chen*

Main category: eess.SY

TL;DR: 通过迭代Youla-Kucera参数化技术，实现多频带抗扰控制。


<details>
  <summary>Details</summary>
Motivation: 为了克服在抑制具有多个频带的振动时出现的关键数值问题。

Method: 提出了一种数值上鲁棒的多频带抗扰控制方法，该方法使用迭代的Youla-Kucera参数化技术。

Result: 在硬盘驱动器伺服系统上进行的数值验证显示了显著的性能改进，从而提高了存储密度。

Conclusion: 该设计方法超越了存储系统，可扩展到各种高精度控制应用，在这些应用中，多频带抗扰性至关重要。

Abstract: This paper presents a numerically robust approach to multi-band disturbance
rejection using an iterative Youla-Kucera parameterization technique. The
proposed method offers precise control over shaping the frequency response of a
feedback loop while maintaining numerical stability through a systematic design
process. By implementing an iterative approach, we overcome a critical
numerical issue in rejecting vibrations with multiple frequency bands.
Meanwhile, our proposed modification of the all-stabilizing Youla-Kucera
architecture enables intuitive design while respecting fundamental performance
trade-offs and minimizing undesired waterbed amplifications. Numerical
validation on a hard disk drive servo system demonstrates significant
performance improvements, enabling enhanced positioning precision for increased
storage density. The design methodology extends beyond storage systems to
various high-precision control applications where multi-band disturbance
rejection is critical.

</details>


### [336] [Dimension-Decomposed Learning for Quadrotor Geometric Attitude Control with Almost Global Exponential Convergence on SO(3)](https://arxiv.org/abs/2508.14422)
*Tianhua Gao,Masashi Izumita,Kohji Tomita,Akiya Kamimura*

Main category: eess.SY

TL;DR: DiD-L 是一种轻量级、可解释的在线学习方法，用于四旋翼飞行器的扰动识别。它通过将高维问题分解为低维子问题来解决欠拟合问题，并能在 MCU 上进行实时运行。该方法无需预训练即可实现指数收敛，并已通过真实实验验证。


<details>
  <summary>Details</summary>
Motivation: 为了解决欠拟合问题，并提高四旋翼几何姿态控制中扰动识别的在线学习方法的轻量化和可解释性。

Method: 提出了一种轻量级且可解释的在线学习方法 DiD-L，并提出 Sliced Adaptive-Neuro Mapping (SANM) 作为 DiD-L 的一个模块实例。DiD-L 将高维映射分解为多个低维子映射（切片），并通过基于李亚普诺夫的自适应律进行在线更新，无需持续激励 (PE) 条件。

Result: DiD-L 可以在 MCU 上以 400 Hz 的实时运行，并通过实验验证。证明了其状态解在几乎全局吸引域内指数收敛到一个任意小的球内，即使在存在时变扰动和惯量不确定性的情况下也是如此，且无需预训练或特定的模型知识。

Conclusion: DiD-L 是首个足够轻量化以在 STM32 等微控制器单元 (MCU) 上以 400 Hz 的实时运行，并通过真实世界实验验证的在线学习方法，用于四旋翼飞行器几何姿态控制中的扰动识别。

Abstract: This paper introduces a lightweight and interpretable online learning
approach called Dimension-Decomposed Learning (DiD-L) for disturbance
identification in quadrotor geometric attitude control. As a module instance of
DiD-L, we propose the Sliced Adaptive-Neuro Mapping (SANM). Specifically, to
address underlying underfitting problems, the high-dimensional mapping for
online identification is axially ``sliced" into multiple low-dimensional
submappings (slices). In this way, the complex high-dimensional problem is
decomposed into a set of simple low-dimensional subtasks addressed by shallow
neural networks and adaptive laws. These neural networks and adaptive laws are
updated online via Lyapunov-based adaptation without the persistent excitation
(PE) condition. To enhance the interpretability of the proposed approach, we
prove that the state solution of the rotational error dynamics exponentially
converges into an arbitrarily small ball within an almost global attraction
domain, despite time-varying disturbances and inertia uncertainties. This
result is novel as it demonstrates exponential convergence without requiring
pre-training for unseen disturbances and specific knowledge of the model. To
our knowledge in the quadrotor control field, DiD-L is the first online
learning approach that is lightweight enough to run in real-time at 400 Hz on
microcontroller units (MCUs) such as STM32, and has been validated through
real-world experiments.

</details>


### [337] [Reformulating Parallel-Connected Lithium-Ion Battery Pack Dynamics with Interconnection Resistances as Ordinary Differential Equations](https://arxiv.org/abs/2508.14454)
*Jaffar Ali Lone,Nilsu Atlan,Simone Fasolato,Davide M Raimondo,Ross Drummond*

Main category: eess.SY

TL;DR: 该研究提出了锂离子电池组电流分布的解析解，简化了分析，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了简化锂离子电池组（并联连接）的模拟和分析，并支持改进其设计和控制。

Method: 提出了解析解，用于计算并联锂离子电池组中的电流分布，并考虑了互连电阻。

Result: 推导出了实现均匀均流的条件，并通过实验数据验证了所提出方法的有效性。

Conclusion: 该研究提出了锂离子电池组（并联连接）电流分布的解析解，考虑了互连电阻的影响，并将描述电池组动态的微分代数方程重构为一组常微分方程，从而简化了模拟和分析。推导出了所有电池实现均匀均流的条件。该方法已通过实验数据验证，并证实其能够捕捉互连电阻引起的主要行为。这些结果可用于改进并联电池组的设计和控制。

Abstract: This work presents analytical solutions for the current distribution in
lithium-ion battery packs composed of cells connected in parallel, explicitly
accounting for the presence of interconnection resistances. These solutions
enable the reformulation of the differential-algebraic equations describing the
pack dynamics into a set of ordinary differential equations, thereby
simplifying simulation and analysis. Conditions under which uniform current
sharing across all cells occurs are also derived. The proposed formulation is
validated against experimental data and confirms its ability to capture the key
behaviours induced by interconnection resistances. These results can support
the improved design and control of parallel-connected battery packs.

</details>


### [338] [Markov Chain-based Model of Blockchain Radio Access Networks](https://arxiv.org/abs/2508.14519)
*Vasileios Kouvakis,Stylianos E. Trevlakis,Alexandros-Apostolos A. Boulogeorgos,Hongwu Liu,Theodoros A. Tsiftsis,Octavia A. Dobre*

Main category: eess.SY

TL;DR: 本研究提出了一个集成了区块链技术的无线接入网（B-RAN）框架，使用排队论和马尔可夫链进行建模，证明了其低延迟和高安全性。


<details>
  <summary>Details</summary>
Motivation: 区块链技术因其安全特性在无线接入网（RAN）领域备受关注，本研究旨在解决现有模型局限性并提出一个集成了区块链技术的RAN框架。

Method: 本研究提出的框架整合了区块链技术到无线接入网（RAN），并利用排队论和马尔可夫链理论来对B-RAN的方面进行建模。

Result: 评估结果显示，该框架具有低延迟和可比拟的安全性。

Conclusion: 该框架通过利用排队论和马尔可夫链理论进行建模，实现了低延迟和可比拟的安全性，适用于各种应用场景。

Abstract: Security has always been a priority, for researchers, service providers and
network operators when it comes to radio access networks (RAN). One wireless
access approach that has captured attention is blockchain enabled RAN (B-RAN)
due to its secure nature. This research introduces a framework that integrates
blockchain technology into RAN while also addressing the limitations of
state-of-the-art models. The proposed framework utilizes queuing and Markov
chain theory to model the aspects of B-RAN. An extensive evaluation of the
models performance is provided, including an analysis of timing factors and a
focused assessment of its security aspects. The results demonstrate reduced
latency and comparable security making the presented framework suitable for
diverse application scenarios.

</details>


### [339] [Distributed Multiple Fault Detection and Estimation in DC Microgrids with Unknown Power Loads](https://arxiv.org/abs/2508.14675)
*Jingwei Dong,Mahdieh S. Sadabadi,Per Mattsson,André Teixeira*

Main category: eess.SY

TL;DR: 该研究提出了一种用于直流微电网的分布式故障诊断方法，能够检测和估算执行器和电力线故障。该方法解决了电力线故障估算中的耦合和激励不足问题，并对执行器故障提供了鲁棒的估算。仿真结果证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 直流微电网中的执行器和电力线故障检测与估算，特别是电力线故障的估算因耦合和激励不足而更具挑战性。

Method: 设计了一个故障估算滤波器，并通过优化问题确定参数；引入“先区分后估算”策略，利用残差时间特性区分负载变化和线路故障；激活正则化最小二乘法估算故障电流，并推导误差上限。

Result: 通过仿真结果验证了所提出方法的有效性。

Conclusion: 提出了一种分布式诊断方案，用于检测和估算直流微电网中的执行器和电力线故障，并考虑了未知负载和随机噪声。对于执行器故障，设计了一个故障估算滤波器，并通过一个易于处理的优化问题来确定滤波器参数，以实现故障估算、与电力线故障解耦以及对噪声的鲁棒性。对于电力线故障，由于故障电流与未知功率负载之间固有的耦合关系，当系统激励不足时，问题会变得不适定，因此估算起来更具挑战性。本研究首次解决了这个关键但未被充分研究的问题。所提出的解决方案引入了一种新颖的“先区分后估算”策略。基于所构建残差的时间特性，制定了一系列诊断规则来区分负载变化和线路故障。一旦检测到电力线故障，将激活正则化最小二乘法来估算故障电流，并推导出估算误差的上限。

Abstract: This paper proposes a distributed diagnosis scheme to detect and estimate
actuator and power line faults in DC microgrids subject to unknown power loads
and stochastic noise. To address actuator faults, we design a fault estimation
filter whose parameters are determined through a tractable optimization problem
to achieve fault estimation, decoupling from power line faults, and robustness
against noise. In contrast, the estimation of power line faults poses greater
challenges due to the inherent coupling between fault currents and unknown
power loads, which becomes ill-posed when the underlying system is
insufficiently excited. To the best of our knowledge, this is the first study
to address this critical yet underexplored issue. Our solution introduces a
novel differentiate-before-estimate strategy. A set of diagnostic rules based
on the temporal characteristics of a constructed residual is developed to
distinguish load changes from line faults. Once a power line fault is detected,
a regularized least-squares method is activated to estimate the fault currents,
for which we further derive an upper bound on the estimation error. Finally,
comprehensive simulation results validate the effectiveness of the proposed
methods.

</details>


### [340] [Assessment of Power System Stability Considering Multiple Time-Scale Dynamics: Insights into Hopf Bifurcations in Presence of GFL and GFM IBRs](https://arxiv.org/abs/2508.14677)
*Luis David Pabon Ospina*

Main category: eess.SY

TL;DR: 该论文认为，在评估电力系统稳定性时，同时考虑快速和缓慢的动态过程至关重要，而传统的基于时间尺度分解的方法可能会遗漏关键的不稳定因素。研究通过一个包含并网逆变器和并网逆变器的案例，说明了它们如何能够互补。


<details>
  <summary>Details</summary>
Motivation: 历史上的电力系统模型忽视了多时间尺度动力学，导致在评估稳定性时可能出现问题。

Method: 使用霍普夫分岔的概念来例证。

Result: 证明了同时考虑多时间尺度动力学对于识别仅通过单一时间尺度分析可能遗漏的不稳定问题至关重要。研究还展示了并网逆变器和并网逆变器如何从多时间尺度动力学的角度进行互补。

Conclusion: 该研究挑战了传统的基于时间尺度分解的电力系统建模范式，强调了同时考虑多时间尺度动力学在评估电力系统稳定性方面的重要性。

Abstract: Real power systems exhibit dynamics that evolve across a wide range of time
scales, from very fast to very slow phenomena. Historically, incorporating
these wide-ranging dynamics into a single model has been impractical. As a
result, power engineers rely on time-scale decomposition to simplify models.
When fast phenomena are evaluated, slow dynamics are neglected (assumed
stable), and vice versa. This paper challenges this paradigm by showing the
importance of assessing power system stability while considering multiple time
scales simultaneously. Using the concept of Hopf bifurcations, it exemplifies
instability issues that would be missed if multi-time-scale dynamics are not
considered. Although this work employs both grid-following and grid-forming
inverter-based resource models, it is not a direct comparison. Instead, it
presents a case study demonstrating how one technology can complement the other
from a multi time-scale dynamics perspective.

</details>


### [341] [Optimal Unpredictable Control for Linear Systems](https://arxiv.org/abs/2508.14686)
*Chendi Qu,Jianping He,Jialun Li,Xiaoming Duan*

Main category: eess.SY

TL;DR: 本研究提出了一种通过添加随机控制输入（不可预测控制）来增强线性系统对抗恶意推断的能力。研究中解决了如何优化随机输入分布以及如何在不可预测性和控制性能之间取得平衡的挑战。通过利用预测误差的方差和置信概率，研究提出了两种优化方法，并推导出了控制输入的解析最优分布，同时将不可预测概率问题转化为可解的线性优化问题。研究最后还量化了控制性能，并设计了相应的LQR和协同控制，模拟结果表明该方法具有不可预测性，并且在所提出的度量下优于常用的高斯和拉普拉斯分布。


<details>
  <summary>Details</summary>
Motivation: 本篇论文研究如何实现线性系统对抗恶意推断的不可预测性。关键思想是添加随机控制输入，称为不可预测控制，以使输出不规则。

Method: 首先，我们利用预测误差的方差和置信概率来量化不可预测性，然后分别构建了两个双阶段随机优化问题。在方差度量下，我们提供了控制输入的解析最优分布。在概率度量下，这是一个非凸优化问题，因此我们提出了一种新颖的数值方法，将问题转化为一个可解的线性优化问题。最后，我们量化了不可预测控制下的控制性能，并相应地设计了不可预测的LQR和协同控制。

Result: 未来的输出因此变得不可预测，推断的性能下降。

Conclusion: 模拟证明了我们的控制算法具有不可预测性。所获得的最优分布在所提出的度量下优于差分隐私中常用的高斯和拉普拉斯分布。

Abstract: In this paper, we investigate how to achieve the unpredictability against
malicious inferences for linear systems. The key idea is to add stochastic
control inputs, named as unpredictable control, to make the outputs irregular.
The future outputs thus become unpredictable and the performance of inferences
is degraded. The major challenges lie in: i) how to formulate optimization
problems to obtain an optimal distribution of stochastic input, under unknown
prediction accuracy of the adversary; and ii) how to achieve the trade-off
between the unpredictability and control performance. We first utilize both
variance and confidence probability of prediction error to quantify
unpredictability, then formulate two two-stage stochastic optimization
problems, respectively. Under variance metric, the analytic optimal
distribution of control input is provided. With probability metric, it is a
non-convex optimization problem, thus we present a novel numerical method and
convert the problem into a solvable linear optimization problem. Last, we
quantify the control performance under unpredictable control, and accordingly
design the unpredictable LQR and cooperative control. Simulations demonstrate
the unpredictability of our control algorithm. The obtained optimal
distribution outperforms Gaussian and Laplace distributions commonly used in
differential privacy under proposed metrics.

</details>


### [342] [Recursive Gaussian Process Regression with Integrated Monotonicity Assumptions for Control Applications](https://arxiv.org/abs/2508.14715)
*Ricus Husmann,Sven Weishaupt,Harald Aschemann*

Main category: eess.SY

TL;DR: 提出了一种改进的递归高斯过程（RGP）回归方法，通过集成软不等式约束（使用EKF和伪测量）和考虑函数单调性假设，提高了性能并降低了计算量，已在控制应用中成功验证。


<details>
  <summary>Details</summary>
Motivation: 提出一种递归高斯过程（RGP）回归的扩展，以满足不等式约束，并适用于控制应用中的实时执行。

Method: 通过引入额外的扩展卡尔曼滤波（EKF）更新步骤和伪测量，将软不等式约束集成到递归高斯过程（RGP）回归中。算法的序贯形式和几个人为启发式方法确保了算法的性能和较低的计算量。

Result: 在模拟中对该算法进行了统计验证，与标准的RGP算法相比，该算法的优势显而易见。

Conclusion: 该算法在蒸汽压缩循环蒸发器的传热值单调性保持学习中得到了成功的实验验证，并利用了先前发布的偏输入输出线性化（IOL）。

Abstract: In this paper, we present an extension to the recursive Gaussian Process
(RGP) regression that enables the satisfaction of inequality constraints and is
well suited for a real-time execution in control applications. The soft
inequality constraints are integrated by introducing an additional extended
Kalman Filter (EKF) update step using pseudo-measurements. The sequential
formulation of the algorithm and several developed heuristics ensure both the
performance and a low computational effort of the algorithm. A special focus
lies on an efficient consideration of monotonicity assumptions for GPs in the
form of inequality constraints. The algorithm is statistically validated in
simulations, where the possible advantages in comparison with the standard RGP
algorithm become obvious. The paper is concluded with a successful experimental
validation of the developed algorithm for the monotonicity-preserving learning
of heat transfer values for the control of a vapor compression cycle
evaporator, leveraging a previously published partial input output
linearization (IOL).

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [343] [MAHL: Multi-Agent LLM-Guided Hierarchical Chiplet Design with Adaptive Debugging](https://arxiv.org/abs/2508.14053)
*Jinwei Tang,Jiayin Qin,Nuo Xu,Pragnya Sudershan Nalla,Yu Cao,Yang,Zhao,Caiwen Ding*

Main category: cs.AR

TL;DR: MAHL是一个层次化的LLM框架，用于优化AI工作负载的chiplet设计，通过多智能体协同解决LLM在chiplet设计中的局限性，显著提升了生成准确性和PPA。


<details>
  <summary>Details</summary>
Motivation: 随着AI等程序工作负载的增长，其高维度特性给芯片设计带来挑战。虽然LLM在硬件描述语言（HDL）生成方面展现出潜力，但LLM驱动的chiplet设计在设计展平、高验证成本和参数优化不精确等方面存在局限。因此，需要创新的方法来解决这些问题。

Method: MAHL是一个基于层次化的大型语言模型（LLM）的芯片小互联（chiplet）设计生成框架。

Result: MAHL显著提高了简单寄存器传输级（RTL）设计的生成准确性，并将真实世界chiplet设计的生成准确性（以Pass@5衡量）从0提高到0.72，超越了传统LLM。与最先进的CLARIE（基于专家）相比，MAHL在某些优化目标下取得了相当或更优的PPA结果。

Conclusion: MAHL框架通过六个协同工作的智能体，实现了AI算法到硬件的映射，包括分层描述生成、检索增强代码生成、基于多样化流程的验证以及多粒度设计空间探索，有效提升了芯片小互联（chiplet）设计的生成效率和功耗、性能及面积（PPA）优化。

Abstract: As program workloads (e.g., AI) increase in size and algorithmic complexity,
the primary challenge lies in their high dimensionality, encompassing computing
cores, array sizes, and memory hierarchies. To overcome these obstacles,
innovative approaches are required. Agile chip design has already benefited
from machine learning integration at various stages, including logic synthesis,
placement, and routing. With Large Language Models (LLMs) recently
demonstrating impressive proficiency in Hardware Description Language (HDL)
generation, it is promising to extend their abilities to 2.5D integration, an
advanced technique that saves area overhead and development costs. However,
LLM-driven chiplet design faces challenges such as flatten design, high
validation cost and imprecise parameter optimization, which limit its chiplet
design capability. To address this, we propose MAHL, a hierarchical LLM-based
chiplet design generation framework that features six agents which
collaboratively enable AI algorithm-hardware mapping, including hierarchical
description generation, retrieval-augmented code generation, diverseflow-based
validation, and multi-granularity design space exploration. These components
together enhance the efficient generation of chiplet design with optimized
Power, Performance and Area (PPA). Experiments show that MAHL not only
significantly improves the generation accuracy of simple RTL design, but also
increases the generation accuracy of real-world chiplet design, evaluated by
Pass@5, from 0 to 0.72 compared to conventional LLMs under the best-case
scenario. Compared to state-of-the-art CLARIE (expert-based), MAHL achieves
comparable or even superior PPA results under certain optimization objectives.

</details>


### [344] [Revisit Choice Network for Synthesis and Technology Mapping](https://arxiv.org/abs/2508.14068)
*Chen Chen,Jiaqi Yin,Cunxi Yu*

Main category: cs.AR

TL;DR: Cristal是一种新的方法，通过构建更高质量的选择网络来改进技术映射，相比现有方法在面积、延迟和运行时长方面均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 之前的无损综合方法在生成选择时忽略了选择的质量，未能有效优化技术映射。

Method: Cristal提出了一种新的基于选择网络构建的选择网络合成和映射流程，包括代表性逻辑锥搜索、通过等价饱和生成多样化选择结构的结构突变，以及具有选择网络构建和验证的优先级排序选择。

Result: 与ABC中的现有方法相比，Cristal在后期映射阶段平均减少了3.85%的面积和8.35%的延迟（面向延迟模式），0.11%的面积和2.74%的延迟（面向面积模式），并在大型组合电路上实现了63.77%的运行时长缩减。

Conclusion: Cristal通过生成数量更少但质量更高的选择来改进技术映射，在面积和延迟方面优于现有方法，并显著减少了运行时间。

Abstract: Choice network construction is a critical technique for alleviating
structural bias issues in Boolean optimization, equivalence checking, and
technology mapping. Previous works on lossless synthesis utilize independent
optimization to generate multiple snapshots, and use simulation and SAT solvers
to identify functionally equivalent nodes. These nodes are then merged into a
subject graph with choice nodes. However, such methods often neglect the
quality of these choices, raising the question of whether they truly contribute
to effective technology mapping.
  This paper introduces Cristal, a novel methodology and framework for
constructing Boolean choice networks. Specifically, Cristal introduces a new
flow of choice network-based synthesis and mapping, including representative
logic cone search, structural mutation for generating diverse choice structures
via equality saturation, and priority-ranking choice selection along with
choice network construction and validation. Through these techniques, Cristal
constructs fewer but higher-quality choices.
  Our experimental results demonstrate that Cristal outperforms the
state-of-the-art Boolean choice network construction implemented in ABC in the
post-mapping stage, achieving average reductions of 3.85%/8.35% (area/delay) in
delay-oriented mode, 0.11%/2.74% in area-oriented mode, and a 63.77% runtime
reduction on large-scale cases across a diverse set of combinational circuits
from the IWLS 2005, ISCAS'89, and EPFL benchmark suites.

</details>


### [345] [AI Agents for Photonic Integrated Circuit Design Automation](https://arxiv.org/abs/2508.14123)
*Ankita Sharma,YuQi Fu,Vahid Ansari,Rishabh Iyer,Fiona Kuang,Kashish Mistry,Raisa Islam Aishy,Sara Ahmad,Joaquin Matres,Dirk R. Englund,Joyce K. S. Poon*

Main category: cs.AR

TL;DR: PhIDO框架使用大型语言模型将自然语言的PIC设计请求转换为布局文件，Gemini-2.5-pro在成本和效率方面表现最佳。


<details>
  <summary>Details</summary>
Motivation: 为了将自然语言的光子集成电路设计请求转换为布局掩膜文件，并评估不同大型语言模型在这一过程中的表现。

Method: 提出了一种名为PhIDO的多代理框架，用于将自然语言的设计请求转换为光子集成电路（PIC）的布局掩膜文件，并使用包含102个设计描述的测试平台比较了7种不同的推理大型语言模型。

Result: 在单设备设计中，成功率高达91%。对于包含15个或更少组件的设计查询，o1、Gemini-2.5-pro和Claude Opus 4实现了最高的端到端通过率（约57%），Gemini-2.5-pro的输出令牌数和成本最低。

Conclusion: PhIDO是一个多代理框架，可以将自然语言的光子集成电路（PIC）设计请求转换为布局掩膜文件。通过对102个设计描述的测试，o1、Gemini-2.5-pro和Claude Opus 4在少于或等于15个组件的设计查询中实现了最高的端到端通过率（约57%），其中Gemini-2.5-pro的输出令牌数和成本最低。

Abstract: We present Photonics Intelligent Design and Optimization (PhIDO), a
multi-agent framework that converts natural-language photonic integrated
circuit (PIC) design requests into layout mask files. We compare 7 reasoning
large language models for PhIDO using a testbench of 102 design descriptions
that ranged from single devices to 112-component PICs. The success rate for
single-device designs was up to 91%. For design queries with less than or equal
to 15 components, o1, Gemini-2.5-pro, and Claude Opus 4 achieved the highest
end-to-end pass@5 success rates of approximately 57%, with Gemini-2.5-pro
requiring the fewest output tokens and lowest cost. The next steps toward
autonomous PIC development include standardized knowledge representations,
expanded datasets, extended verification, and robotic automation.

</details>


### [346] [Cross-Layer Design of Vector-Symbolic Computing: Bridging Cognition and Brain-Inspired Hardware Acceleration](https://arxiv.org/abs/2508.14245)
*Shuting Du,Mohamed Ibrahim,Zishen Wan,Luqi Zheng,Boheng Zhao,Zhenkun Fan,Che-Kai Liu,Tushar Krishna,Arijit Raychowdhury,Haitong Li*

Main category: cs.AR

TL;DR: 本文是关于向量符号架构（VSA）的软硬件协同设计的综述，介绍了VSA原理、硬件技术和跨层设计方法，并提出了一个内存计算系统作为示例。


<details>
  <summary>Details</summary>
Motivation: VSAs在认知应用中被广泛部署，但目前缺乏对VSA软硬件协同设计的统一论述，本文旨在弥合理论探索与高效硬件架构开发之间的差距。

Method: 首先介绍了VSA原理，然后深入讨论了VSA的硬件技术，并提出了一种跨层设计方法，最后通过提出一个内存计算的认知硬件系统进行了演示。

Result: 提出了一种跨层设计方法，并展示了一个内存计算的认知硬件系统，证明了协同设计方法的效率、灵活性和可扩展性。

Conclusion: 本文总结了VSAs的硬件/软件协同设计，讨论了开放性研究挑战。

Abstract: Vector Symbolic Architectures (VSAs) have been widely deployed in various
cognitive applications due to their simple and efficient operations. The
widespread adoption of VSAs has, in turn, spurred the development of numerous
hardware solutions aimed at optimizing their performance. Despite these
advancements, a comprehensive and unified discourse on the convergence of
hardware and algorithms in the context of VSAs remains somewhat limited. The
paper aims to bridge the gap between theoretical software-level explorations
and the development of efficient hardware architectures and emerging technology
fabrics for VSAs, providing insights from the co-design aspect for researchers
from either side. First, we introduce the principles of vector-symbolic
computing, including its core mathematical operations and learning paradigms.
Second, we provide an in-depth discussion on hardware technologies for VSAs,
analyzing analog, mixed-signal, and digital circuit design styles. We compare
hardware implementations of VSAs by carrying out detailed analysis of their
performance characteristics and tradeoffs, allowing us to extract design
guidelines for the development of arbitrary VSA formulations. Third, we discuss
a methodology for cross-layer design of VSAs that identifies synergies across
layers and explores key ingredients for hardware/software co-design of VSAs.
Finally, as a concrete demonstration of this methodology, we propose the first
in-memory computing hierarchical cognition hardware system, showcasing the
efficiency, flexibility, and scalability of this co-design approach. The paper
concludes with a discussion of open research challenges for future
explorations.

</details>


### [347] [Power Stabilization for AI Training Datacenters](https://arxiv.org/abs/2508.14318)
*Esha Choukse,Brijesh Warrier,Scot Heath,Luz Belmont,April Zhao,Hassan Ali Khan,Brian Harry,Matthew Kappel,Russell J. Hewett,Kushal Datta,Yu Pei,Caroline Lichtenberger,John Siegler,David Lukofsky,Zaid Kahn,Gurpreet Sahota,Andy Sullivan,Charles Frederick,Hien Thai,Rebecca Naughton,Daniel Jurnove,Justin Harp,Reid Carper,Nithish Mahalingam,Srini Varkala,Alok Gautam Kumbhare,Satyajit Desai,Venkatesh Ramamurthy,Praneeth Gottumukkala,Girish Bhatia,Kelsey Wildstone,Laurentiu Olariu,Mohammed Ayna,Mike Kendrick,Ricardo Bianchini,Aaron Hurst,Reza Zamani,Xin Li,Gene Oden,Rory Carmichael,Tom Li,Apoorv Gupta,Nilesh Dattani,Lawrence Marwong,Rob Nertney,Jeff Liott,Miro Enev,Divya Ramakrishnan,Ian Buck,Jonah Alben*

Main category: cs.AR

TL;DR: Large AI training jobs use lots of GPUs and cause big power swings that can damage the power grid. This paper looks at software, hardware, and datacenter solutions to even out the power usage, tested with real equipment and a simulator.


<details>
  <summary>Details</summary>
Motivation: Large AI training workloads exhibit high power consumption variability and swings, posing risks to the power grid infrastructure due to potential harmonization with critical utility frequencies. Stabilizing power is necessary for safely scaling these workloads.

Method: The paper analyzes the challenge of power management in large AI training workloads, explores solutions across software, GPU hardware, and datacenter infrastructure, and tests these solutions using real hardware and a cloud power simulator.

Result: The paper presents the pros and cons of various solutions and validates a multi-pronged approach through rigorous testing, providing insights into their real-world efficacy.

Conclusion: A multi-pronged approach involving software, GPU hardware, and datacenter infrastructure is proposed to stabilize power consumption in large AI training workloads, mitigating risks to the power grid.

Abstract: Large Artificial Intelligence (AI) training workloads spanning several tens
of thousands of GPUs present unique power management challenges. These arise
due to the high variability in power consumption during the training. Given the
synchronous nature of these jobs, during every iteration there is a
computation-heavy phase, where each GPU works on the local data, and a
communication-heavy phase where all the GPUs synchronize on the data. Because
compute-heavy phases require much more power than communication phases, large
power swings occur. The amplitude of these power swings is ever increasing with
the increase in the size of training jobs. An even bigger challenge arises from
the frequency spectrum of these power swings which, if harmonized with critical
frequencies of utilities, can cause physical damage to the power grid
infrastructure. Therefore, to continue scaling AI training workloads safely, we
need to stabilize the power of such workloads. This paper introduces the
challenge with production data and explores innovative solutions across the
stack: software, GPU hardware, and datacenter infrastructure. We present the
pros and cons of each of these approaches and finally present a multi-pronged
approach to solving the challenge. The proposed solutions are rigorously tested
using a combination of real hardware and Microsoft's in-house cloud power
simulator, providing critical insights into the efficacy of these interventions
under real-world conditions.

</details>


### [348] [Computing-In-Memory Dataflow for Minimal Buffer Traffic](https://arxiv.org/abs/2508.14375)
*Choongseok Song,Doo Seok Jeong*

Main category: cs.AR

TL;DR: 提出了一种新的 CIM 数据流，以提高深度卷积的效率，通过减少内存利用率和缓冲区流量，在 MobileNet 和 EfficientNet 模型上实现了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决 CIM 加速深度卷积时存在的内存利用率低和缓冲区流量大的问题，特别是后者对延迟和能耗的显著影响。

Method: 提出了一种新颖的 CIM 数据流，并证明了其在 MobileNet 和 EfficientNet 模型上的有效性。

Result: 与基线（传统的重量固定数据流）相比，所提出的数据流将缓冲区流量减少了 77.4-87.0%，并将数据流量的能耗和延迟分别降低了 10.1-17.9% 和 15.6-27.8%。

Conclusion: 所提出的 CIM 数据流通过最大化数据重用和改进深度卷积期间的内存利用率，显著减少了缓冲区流量，并降低了数据流量的能耗和延迟。

Abstract: Computing-In-Memory (CIM) offers a potential solution to the memory wall
issue and can achieve high energy efficiency by minimizing data movement,
making it a promising architecture for edge AI devices. Lightweight models like
MobileNet and EfficientNet, which utilize depthwise convolution for feature
extraction, have been developed for these devices. However, CIM macros often
face challenges in accelerating depthwise convolution, including
underutilization of CIM memory and heavy buffer traffic. The latter, in
particular, has been overlooked despite its significant impact on latency and
energy consumption. To address this, we introduce a novel CIM dataflow that
significantly reduces buffer traffic by maximizing data reuse and improving
memory utilization during depthwise convolution. The proposed dataflow is
grounded in solid theoretical principles, fully demonstrated in this paper.
When applied to MobileNet and EfficientNet models, our dataflow reduces buffer
traffic by 77.4-87.0%, leading to a total reduction in data traffic energy and
latency by 10.1-17.9% and 15.6-27.8%, respectively, compared to the baseline
(conventional weight-stationary dataflow).

</details>


### [349] [Wit-HW: Bug Localization in Hardware Design Code via Witness Test Case Generation](https://arxiv.org/abs/2508.14414)
*Ruiyang Ma,Daikang Kuang,Ziqian Liu,Jiaxi Zhang,Ping Fan,Guojie Luo*

Main category: cs.AR

TL;DR: Wit-HW是一种自动化的硬件bug定位框架，通过生成额外的“证据”测试用例来克服现有技术仅使用单个触发bug的测试用例的局限性，从而更有效地定位硬件设计中的bug。


<details>
  <summary>Details</summary>
Motivation: 现有自动化硬件调试技术通常仅利用单个触发bug的测试用例的信息进行bug定位，这限制了它们分析复杂硬件系统和找出bug根本原因的有效性。

Method: Wit-HW框架将硬件bug定位问题转化为测试用例生成问题，通过寻找除初始触发bug的测试用例之外的一组有效的证据测试用例来增强硬件bug定位。通过分析通过和失败的测试用例之间的执行差异，并结合基于频谱的方法，可以消除无辜的设计语句，并精确定位有问题的语句。为了进一步优化可疑区域，定义了有效证据测试用例的标准，并使用基于变异的策略来生成这些测试用例。

Result: Wit-HW在41个硬件设计的41个bug上进行了评估，在Top-1、Top-5、Top-10的排名中分别定位了49%、73%、88%的bug，显著优于最先进的bug定位技术。此外，Wit-HW在13个来自开源硬件项目的真实世界bug上进行了评估，展示了该方法的鲁棒性能。

Conclusion: Wit-HW在41个硬件设计的41个bug上进行了评估，在Top-1、Top-5、Top-10的排名中分别定位了49%、73%、88%的bug，显著优于最先进的bug定位技术。此外，Wit-HW在13个来自开源硬件项目的真实世界bug上进行了评估，展示了该方法的鲁棒性能。

Abstract: Debugging hardware designs requires significant manual effort during hardware
development. After engineers identify a bug-triggering test case in
simulation-based hardware verification, they usually spend considerable time
analyzing the execution trace to localize the bug. Although numerous automated
hardware debugging techniques exist, they are not applicable to large designs
and deep bugs. A primary reason for their limitations is that these techniques
only utilize the information of a single bug-triggering test case for bug
localization, which prevents them from effectively analyzing intricate hardware
systems and figure out the root cause of bugs. To solve this problem, in this
paper, we transform the hardware bug localization problem into a test
generation problem, aiming to find a set of effective witness test cases beyond
the initial bug-triggering test case to enhance hardware bug localization.
Witness test cases refer to the cases that do not trigger the bug in the faulty
design. By analyzing the execution differences between passing and failing test
cases with spectrum-based method, we can eliminate innocent design statements
and localize the buggy ones. To further refine the suspicious area, we define
the criteria for effective witness test cases and use a mutation-based strategy
to generate such test cases. Based on this approach, we propose an automated
hardware bug localization framework named Wit-HW. We evaluate Wit-HW on 41 bugs
from various hardware designs. The experimental results show that Wit-HW
effectively localize 49%, 73%, 88% bugs within Top-1, Top-5, Top-10 ranks,
significantly outperforming state-of-the-art bug localization techniques.
Additionally, we evaluate Wit-HW on 13 real-world bugs collected from
open-source hardware projects, showcasing the robust performance of our method.

</details>


### [350] [An Open-Source HW-SW Co-Development Framework Enabling Efficient Multi-Accelerator Systems](https://arxiv.org/abs/2508.14582)
*Ryan Albert Antonio,Joren Dumoulin,Xiaoling Yi,Josse Van Delm,Yunhao Deng,Guilherme Paim,Marian Verhelst*

Main category: cs.AR

TL;DR: SNAX 是一个开源的软硬件框架，通过混合耦合、可重用硬件和 MLIR 编译器，实现了高效的多加速器计算集群，性能提升 10 倍以上，利用率超过 90%。


<details>
  <summary>Details</summary>
Motivation: 当前异构加速器中心的计算集群在数据移动效率和软硬件兼容性方面存在不足，阻碍了统一的、平衡性能和易用性的方法。SNAX 旨在解决这些问题。

Method: 提出了一种名为 SNAX 的开源集成软硬件框架，采用了新颖的混合耦合方案（包括松耦合异步控制和紧耦合数据访问），并结合了可重用的硬件模块和基于 MLIR 的编译器，以实现高效的多加速器平台。

Result: 在低功耗异构 SoC 上进行了广泛的实验，证明了 SNAX 的效率和灵活性。与其它加速器系统相比，SNAX 在神经网络性能方面实现了超过 10 倍的提升，同时在全系统运行中保持了超过 90% 的加速器利用率。

Conclusion: SNAX 通过其混合耦合方案、可重用的硬件模块和基于 MLIR 的编译器，实现了高效、灵活且易于开发和部署的多加速器计算集群，在神经网络性能方面取得了显著提升，同时保持了高加速器利用率。

Abstract: Heterogeneous accelerator-centric compute clusters are emerging as efficient
solutions for diverse AI workloads. However, current integration strategies
often compromise data movement efficiency and encounter compatibility issues in
hardware and software. This prevents a unified approach that balances
performance and ease of use. To this end, we present SNAX, an open-source
integrated HW-SW framework enabling efficient multi-accelerator platforms
through a novel hybrid-coupling scheme, consisting of loosely coupled
asynchronous control and tightly coupled data access. SNAX brings reusable
hardware modules designed to enhance compute accelerator utilization, and its
customizable MLIR-based compiler to automate key system management tasks,
jointly enabling rapid development and deployment of customized
multi-accelerator compute clusters. Through extensive experimentation, we
demonstrate SNAX's efficiency and flexibility in a low-power heterogeneous SoC.
Accelerators can easily be integrated and programmed to achieve > 10x
improvement in neural network performance compared to other accelerator systems
while maintaining accelerator utilization of > 90% in full system operation.

</details>


### [351] [ListenToJESD204B: A Lightweight Open-Source JESD204B IP Core for FPGA-Based Ultrasound Acquisition systems](https://arxiv.org/abs/2508.14798)
*Soumyo Bhattacharjee,Federico Villani,Christian Vogt,Andrea Cossettini,Luca Benini*

Main category: cs.AR

TL;DR: 开源的JESD204B接收IP核，适用于Zynq UltraScale+，比商业IP核资源占用少79%，性能优越，已通过硬件验证。


<details>
  <summary>Details</summary>
Motivation: 为了满足超声系统中对大量高速、低延迟同步通道的需求，克服现有商业FPGA IP核的局限性（专有、昂贵、资源消耗大）。

Method: 开发了一个开源的、可综合的SystemVerilog接收IP核，名为ListenToJESD204B，支持JESD204B协议，针对AMD Xilinx Zynq UltraScale+设备进行了优化，并采用了模块化设计以支持高通道数。

Result: ListenToJESD204B IP核占用的逻辑资源比商业IP核减少了79%，支持4个12.8 Gb/s通道，实现了确定性的子类别1延迟，并在80 MSPS、16位采样率下通过双12.8 Gb/s链路进行了30分钟无差错的硬件流数据验证。

Conclusion: 所提出的ListenToJESD204B IP核为AMD Xilinx Zynq UltraScale+设备提供了一种低成本、开源的JESD204B接收器解决方案，具有高吞吐量、低延迟和资源占用少的优点，并通过了仿真和硬件验证。

Abstract: The demand for hundreds of tightly synchronized channels operating at tens of
MSPS in ultrasound systems exceeds conventional low-voltage differential
signaling links' bandwidth, pin count, and latency. Although the JESD204B
serial interface mitigates these limitations, commercial FPGA IP cores are
proprietary, costly, and resource-intensive. We present ListenToJESD204B, an
open-source receiver IP core released under a permissive Solderpad 0.51 license
for AMD Xilinx Zynq UltraScale+ devices. Written in synthesizable
SystemVerilog, the core supports four GTH/GTY lanes at 12.8 Gb/s and provides
cycle-accurate AXI-Stream data alongside deterministic Subclass~1 latency. It
occupies only 107 configurable logic blocks (approximately 437 LUTs),
representing a 79\% reduction compared to comparable commercially available IP.
A modular data path featuring per-lane elastic buffers, SYSREF-locked LMFC
generation, and optional LFSR descrambling facilitates scaling to high lane
counts. We verified protocol compliance through simulation against the Xilinx
JESD204C IP in JESD204B mode and on hardware using TI AFE58JD48 ADCs. Block
stability was verified by streaming 80 MSPS, 16-bit samples over two 12.8 Gb/s
links for 30 minutes with no errors.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [352] [Role of electron-electron interactions in $M$-valley twisted transition metal dichalcogenides](https://arxiv.org/abs/2508.14283)
*Christophe De Beule,Liangtao Peng,E. J. Mele,Shaffique Adam*

Main category: cond-mat.mes-hall

TL;DR: 本研究使用Hartree-Fock方法，发现电子相互作用和层间偏压能调控M-valley moiré系统的电子性质，并可能增强超导性和强关联态。


<details>
  <summary>Details</summary>
Motivation: 研究电子-电子相互作用和层间偏压如何影响M-valley moiré系统中电子的行为

Method: 使用自洽Hartree-Fock近似研究了长程库仑相互作用在M-valley moiré系统中的作用

Result: 对于接近对齐（0°）和反齐（60°）的扭转，不同填充和层间偏压下的态密度表现出显著差异。范霍夫奇点在有限掺杂范围内被固定在费米能量上，这种现象可以增强超导性和强关联态。

Conclusion: 通过引入电子-电子相互作用和层间偏压，结果表明M-valley moiré系统中的相关态可以原位精确调控。

Abstract: We investigate the role of long-range Coulomb interactions in $M$-valley
moir\'e systems using the self-consistent Hartree-Fock approximation. This
platform was recently proposed [Nature 643, 376 (2025) and arXiv:2411.18828
(2024)] as a new class of experimentally realizable moir\'e materials using
twisted transition metal dichalcogenides homobilayers with the 1T structure.
While these seminal studies considered the noninteracting theory without an
interlayer bias due to an electric displacement field, this work shows that
both electron-electron interactions at finite doping and an interlayer bias
strongly modify the moir\'e bands. For small twist angles, the density of
states as a function of filling and interlayer bias shows qualitatively
different behavior for twisting near aligned ($0^\circ$) and antialigned
($60^\circ$) stacking. More interestingly, the Van Hove singularity becomes
pinned to the Fermi energy over a finite range of doping, an effect known to
enhance both superconductivity and strongly correlated states. For aligned
stacking this occurs only at zero electric field, while for antialigned
stacking this happens both at zero and finite field. Our work demonstrates that
correlated states in $M$-valley 1TtTMDs can be strongly tuned \textit{in situ}
both by applying an electric displacement field and by electron doping.

</details>


### [353] [Jahn-Teller-like Distortion in a One-dimensional π-Conjugated Polymer](https://arxiv.org/abs/2508.14416)
*Ziyi Wang,Boyu Qie,Weichen Tang,Jingwei Jiang,Fujia Liu,Peter H. Jacobse,Jiaming Lu,Xinheng Li,Steven G. Louie,Felix R. Fischer,Michael F. Crommie*

Main category: cond-mat.mes-hall

TL;DR: PDFHE聚合物自发地发生大范围的骨架畸变，类似于其镜像对称性的自发对称破缺（SSB）。这种畸变由Jahn-Teller效应驱动，有助于稳定聚合物并扩大带隙。


<details>
  <summary>Details</summary>
Motivation: 尽管在扩展宽度的系统中控制低维π共轭体系的结构畸变仍然具有挑战性，但控制这种行为对于影响其电子性质至关重要。

Method: 通过在贵金属表面合成聚-(二芴庚烯-乙炔)(PDFHE)并使用低温扫描隧道显微镜对其结构和电子态进行表征，并辅以密度泛函理论计算。

Result: PDFHE在非平面异构体中放松，并通过类似Jahn-Teller的机制稳定，消除了相对于有带隙平面结构的电子不稳定性。这种畸变降低了聚合物的总能量并增大了带隙，为SSB提供了微观解释。

Conclusion: 本研究表明，即使在机械强度高的扩展π系统中，微弱的电子-晶格耦合也能自发地驱动显著的结构重排。

Abstract: Structurally distorting low-dimensional {\pi}-conjugated systems can
profoundly influence their electronic properties, but controlling such behavior
in extended-width systems remains challenging. Here we demonstrate that a
one-dimensional conjugated polymer, poly-(difluorenoheptalene-ethynylene)
(PDFHE), undergoes a pronounced out-of-plane backbone distortion, equivalent to
a spontaneous symmetry breaking (SSB) of its mirror symmetry. We synthesized
PDFHE on noble metal surfaces and characterized its structure and electronic
states using low-temperature scanning tunneling microscopy. Rather than
adopting a planar, high-symmetry conformation, PDFHE relaxes into non-planar
isomers stabilized by a Jahn-Teller-like mechanism that relieves an electronic
instability relative to the gapped planar structure. Density functional theory
calculations corroborate these findings, revealing that distortion lowers the
total polymer energy and enlarges the bandgap, providing a microscopic
explanation for the SSB. Our results show that even in mechanically robust
extended {\pi}-systems, subtle electron-lattice coupling can spontaneously
drive significant structural rearrangements.

</details>


### [354] [Influence of local strain on the optical probing of a Ni$^{2+}$ spin in a charged self-assembled quantum dot](https://arxiv.org/abs/2508.14521)
*K. E. Polczynska,S. Karouaz,W. Pacuski,L. Besombes*

Main category: cond-mat.mes-hall

TL;DR: 本研究探索了掺杂Ni$^{2+}$离子的量子点的光学性质，发现应变分布会影响Ni$^{2+}$的自旋结构。通过磁光分析，确定了局域应变各向异性，并识别出$m \Lambda$能级系统。研究还发现，在描述磁场下的发射光谱细节时，必须考虑非零的低对称性项。


<details>
  <summary>Details</summary>
Motivation: 本研究探索了掺杂了与带电激子相互作用的Ni$^{2+}$离子的量子点的光学性质。

Method: 通过系统的磁光分析，我们能够确定局域应变各向异性。

Result: 发现应变分布显著影响Ni$^{2+}$的自旋结构，并识别出可以被光学单独寻址的$m \Lambda$能级系统。

Conclusion: 研究表明，需要考虑非零的低对称性项来准确描述磁场下的发射光谱细节。

Abstract: This study explores the optical properties of quantum dots doped with a
Ni$^{2+}$ ion that interacts with a charged exciton. Systematic magneto-optical
analysis reveals that the strain distribution at the Ni$^{2+}$ site
significantly influences its spin structure. In positively charged dots
dominated by in-plane biaxial strain, the three spins states of the Ni$^{2+}$
(S$_z$=0, S$_z$=$\pm$1) can be observed and the magneto-optical spectra enables
a local strain anisotropy to be determined. However, in most of the dots,
lower-symmetry strain mixes all the Ni$^{2+}$ spin states, thereby increasing
the number of observed optical transitions. In charged dots, we identify
optical transitions that share a common excited state. They form a series of
$\Lambda$ levels systems that can be individually addressed optically to
determine the energy level structure. Magneto-optical measurements demonstrate
that the hole-Ni$^{2+}$ exchange interaction is antiferromagnetic and
considerably stronger than the electron-Ni$^{2+}$ interaction. A spin-effective
model that incorporates local strain orientation can successfully reproduce key
experimental results. Furthermore, we demonstrate that low-symmetry terms in
the hole-Ni$^{2+}$ exchange interaction must be considered in order to
accurately describe the emission spectra details in a magnetic field.

</details>


### [355] [Fabrication, characterization and mechanical loading of Si/SiGe membranes for spin qubit devices](https://arxiv.org/abs/2508.14589)
*Lucas Marcogliese,Ouviyan Sabapathy,Rudolf Richter,Jhih-Sian Tu,Dominique Bougeard,Lars R. Schreiber*

Main category: cond-mat.mes-hall

TL;DR: Suspended Si/SiGe membranes fabricated for spin qubits offer new ways to control strain and valley splitting, potentially improving quantum processors.


<details>
  <summary>Details</summary>
Motivation: To improve critical material properties like the valley splitting landscape for scalable quantum processors by leveraging flexible control of strain and electric fields in Si/SiGe membranes, and to investigate elusive intervalley scattering mechanisms.

Method: Fabrication of locally-etched, suspended SiGe/Si/SiGe membranes and probing their elastic properties (linear elastic and buckling modes) by stressing with a profilometer stylus.

Result: Demonstrated fabrication of SiGe/Si/SiGe membranes and their application in a spin qubit shuttling device. Characterized elastic properties revealing mechanisms for strain coupling to spin qubits.

Conclusion: We report the fabrication of locally-etched, suspended SiGe/Si/SiGe membranes and their application to realize a spin qubit shuttling device for future valley mapping experiments. The membranes can be metallized for back-gate contact and probed for their elastic properties to understand strain coupling to spin qubits.

Abstract: Si/SiGe heterostructures on bulk Si substrates have been shown to host high
fidelity electron spin qubits. Building a scalable quantum processor would,
however, benefit from further improvement of critical material properties such
as the valley splitting landscape. Flexible control of the strain field and the
out-of-plane electric field $\mathcal{E}_z$ may be decisive for valley
splitting enhancement in the presence of alloy disorder. We envision the
Si/SiGe membrane as a versatile scientific platform for investigating
intervalley scattering mechanisms which have thus far remained elusive in
conventional Si/SiGe heterostructures and have the potential to yield
favourable valley splitting distributions. Here, we report the fabrication of
locally-etched, suspended SiGe/Si/SiGe membranes from two different
heterostructures and apply the process to realize a spin qubit shuttling device
on a membrane for future valley mapping experiments. The membranes have a
thickness in the micrometer range and can be metallized to form a back-gate
contact for extended control over the electric field. To probe their elastic
properties, the membranes are stressed by loading with a profilometer stylus at
room temperature. We distinguish between linear elastic and buckling modes,
each offering new mechanisms through which strain can be coupled to spin
qubits.

</details>


### [356] [The heating and cooling of 2D electrons at low temperatures](https://arxiv.org/abs/2508.14694)
*A. K. Jain,J. T. Nicholls,S. N. Holmes,G. Jaliel,C. Chen,I. Farrer,D. A. Ritchie*

Main category: cond-mat.mes-hall

TL;DR: 本文测量了GaAs基高迁移率二维电子气中热电子的冷却长度，并研究了其与温度的关系，结果表明冷却长度随温度升高而减小，且与声子散射时间一致。


<details>
  <summary>Details</summary>
Motivation: 研究热电子在GaAs基高迁移率二维电子气中的冷却长度，并探讨其在热导率和能斯特效应测量中的应用

Method: 通过测量长沟道的温度分布来测量热电子的冷却长度

Result: 在1.8至5K的温度范围内，冷却长度从23μm减小到16μm，非弹性散射时间从0.36ns减小到0.18ns

Conclusion: 电子气体的冷却长度会随着晶格温度的升高而减小，并且与声子散射时间一致

Abstract: We present measurements of the cooling length $\ell_E$ for hot electrons in a
GaAs-based high mobility two-dimensional electron gas (2DEG). The thermal
measurements are performed on a long 60 $\mu$m-wide channel, which is
Joule-heated at one end, along which there are three similar hot-electron
thermocouples, spaced 30 $\mu$m apart. The thermocouples measure an
exponentially decaying temperature profile with a characteristic length
$\ell_E$, which decreases from 23 to 16 $\mu$m as the lattice temperature
increases from 1.8 to 5 K. From a simple one-dimensional model of heat
diffusion, we measure an inelastic scattering time which decreases from $\tau_i
\approx$ 0.36 to 0.18 ns. The measured $\tau_i$ has a magnitude and temperature
dependence consistent with acoustic phonon scattering times. We discuss how the
sample design can be varied for further thermal investigations. Knowledge of
the temperature profile and its gradient will prove useful in measurements of
the thermal conductivity and the Nernst effect.

</details>


### [357] [Intrinsic Linear Response from Zeeman Quantum Geometry in 2D Unconventional Magnets](https://arxiv.org/abs/2508.14745)
*Neelanjan Chakraborti,Sudeep Kumar Ghosh,Snehasish Nandy*

Main category: cond-mat.mes-hall

TL;DR: 新理论框架 ZQGT 解释了非常规磁体中的新型线性输运现象（IGMC），与特定材料相关，并为实验诊断提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 为了探索具有零净磁化但动量依赖自旋分裂的非常规磁体中由量子几何驱动的输运现象，本研究旨在揭示新的线性输运响应，并提出一种能够探测隐藏自旋分裂能带结构的工具。

Method: 利用动量平移和自旋旋转的相互作用，推导并分析了由广义量子几何张量 ZQGT 驱动的线性输运响应。通过研究三种典型的二维非常规磁体（d$_{x^2 - y^2}$ 纵 소 磁体、p 波磁体和混合 d 波纵 소 磁体），揭示了 ZQGT 如何在不同的对称性条件下产生纵向、横向或组合的传导和位移 IGMC。

Result: 在三类二维非常规磁体中，ZQGT 驱动了线性本征陀螺磁流（IGMC）。其中，d$_{x^2 - y^2}$ 纵 소 磁体、p 波磁体和混合 d 波纵 소 磁体分别表现出纵向、横向或组合的传导和位移 IGMC。特别是混合 d 波纵 소 磁体中，对称的 Berry 曲率和反对称的量子度量分别产生了常规量子几何中不存在的纵向传导 IGMC 和横向位移 IGMC。

Conclusion: 该研究提出了 Zeeman 量子几何张量（ZQGT）作为一种新的理论框架，用于理解和控制非常规磁体中的输运现象。研究表明 ZQGT 可以驱动本征陀螺磁流（IGMC），并且在特定的非常规磁体中表现出不同的导电和位移 IGMC 响应。ZQGT 的预测与 RuO$_2$、CrSb 和 MnTe 等化合物相关，为区分非常规磁相提供了实验上可行的诊断方法。

Abstract: Unconventional magnets with zero net magnetization yet momentum-dependent
spin splitting constitute a newly identified class of materials that provide a
rich platform for quantum-geometry-driven transport phenomena. Exploiting the
interplay between momentum translation and spin rotation, we uncover a distinct
linear transport response governed by a generalized quantum geometric tensor,
the Zeeman quantum geometric tensor (ZQGT). We show that the ZQGT drives a
linear intrinsic gyrotropic magnetic current (IGMC) in the three prototypical
two-dimensional unconventional magnets: a time-reversal-broken $d_{x^2 - y^2}$
altermagnet, a time-reversal-symmetric $p$-wave magnet, and a mixed $d$-wave
altermagnet. Depending on symmetry, these magnets exhibit longitudinal,
transverse, or combined conduction and displacement IGMCs in the presence of
spin-orbit coupling. Notably, this response persists even when conventional
Berry curvature contributions vanish, offering a unique probe of hidden
spin-split band structures of unconventional magnets. In particular, for mixed
$d$-wave altermagnets, symmetric Berry curvature and antisymmetric quantum
metric respectively generate longitudinal conduction IGMC and transverse
displacement IGMC- responses absent in conventional quantum geometry. The
predicted signatures, relevant to compounds such as RuO$_2$, CrSb, and MnTe,
provide experimentally accessible diagnostics for distinguishing unconventional
magnetic phases. These findings position the ZQGT as a powerful framework for
probing and controlling transport in next-generation quantum materials.

</details>


### [358] [Near-resonant nuclear spin detection with megahertz mechanical resonators](https://arxiv.org/abs/2508.14754)
*Diego A. Visani,Letizia Catalini,Christian L. Degen,Alexander Eichler,Javier del Pino*

Main category: cond-mat.mes-hall

TL;DR: 该研究提出了一种通过磁场梯度将核自旋耦合到兆赫兹谐振器的方法，以探测和控制核自旋。通过测量谐振器频率方差的增加，可以实现单核自旋探测。


<details>
  <summary>Details</summary>
Motivation: 运行在兆赫兹范围内的机械谐振器已成为基础和应用量子研究的通用平台。它们的出色性能，例如低质量和高品质因数，也使其对力传感实验具有吸引力。

Method: 提出了一种通过磁场梯度将核自旋耦合到兆赫兹谐振器来探测和最终控制核自旋的方法。

Result: 研究表明，由于该传感器与 N 个核自旋集合体之间的动态反作用力，导致传感器的共振频率发生偏移。尽管由于玻尔兹曼极化引起的平均频率偏移在纳米级样品体积中难以测量，但研究证明了自旋集合体变化的极化导致了传感器频率方差的可测量的增加。

Conclusion: 预测方差测量将能够在使用现有谐振器器件的情况下实现单核自旋探测。

Abstract: Mechanical resonators operating in the megahertz range have become a
versatile platform for fundamental and applied quantum research. Their
exceptional properties, such as low mass and high quality factor, make them
also appealing for force sensing experiments. In this work, we propose a method
for detecting, and ultimately controlling, nuclear spins by coupling them to
megahertz resonators via a magnetic field gradient. Dynamical backaction
between the sensor and an ensemble of $N$ nuclear spins produces a shift in the
sensor's resonance frequency. The mean frequency shift due to the Boltzmann
polarization is challenging to measure in nanoscale sample volumes. Here, we
show that the fluctuating polarization of the spin ensemble results in a
measurable increase of the resonator's frequency variance. On the basis of
analytical as well as numerical results, we predict that the variance
measurement will allow single nuclear spin detection with existing resonator
devices.

</details>


### [359] [Controlling Skyrmion Lattice Orientation with Local Magnetic Field Gradients](https://arxiv.org/abs/2508.14771)
*Duc Minh Tran,Edoardo Mangini,Elizabeth M. Jefremovas,Fabian Kammerbauer,Dennis Meier,Robert Frömter,Mathias Kläui*

Main category: cond-mat.mes-hall

TL;DR: 利用磁力显微镜（MFM）技术，通过调整扫描线间距，可以精确控制软磁CoFeB材料中磁扭结晶格的形成、排列、取向和重排，为自旋电子器件提供了一种实用的方法。


<details>
  <summary>Details</summary>
Motivation: 为了精确控制磁扭结晶格的形成和排列，以理解其涌现行为，并推动其在自旋电子和磁振子器件中的集成。

Method: 利用单程磁力显微镜（MFM）技术，通过调整扫描线间距匹配条纹畴的固有周期性，利用MFM尖端的杂散场梯度，诱导条纹畴到孤立的扭结和局部有序晶格的可逆转变，并提取扭结位置计算局部取向有序参数。

Result: 通过重复扫描，系统地提高了$\\langle |\\psi_6| \\rangle$值，表明从无序态向有序六方晶格的转变。通过改变扫描方向，可以确定性地旋转晶格取向，并通过快速傅里叶变换证实了这一点。。

Conclusion: 该方法实现了对二维拓扑自旋系统中扭结自旋纹理的精确控制，包括可重构地控制晶格对称性、有序度和取向，为研究其二位相行为提供了实用且易于访问的平台。

Abstract: Precise control over the formation and arrangement of magnetic skyrmion
lattices is essential for understanding their emergent behavior and advancing
their integration into spintronic and magnonic devices. We report on a simple
and minimally invasive technique to nucleate and manipulate skyrmion lattices
in soft magnetic CoFeB using single-pass magnetic force microscopy (MFM). By
tuning the scan-line spacing to match the intrinsic stripe domain periodicity,
the stray field gradient from the MFM tip induces reversible transitions from
stripe domains to isolated skyrmions and locally ordered lattices. The
resulting skyrmion positions are extracted to compute the local orientational
order parameter $\psi_6$, enabling quantitative evaluation of lattice ordering.
A systematic improvement in $\langle |\psi_6| \rangle$ is observed with
repeated scanning, indicating a transition from a disordered state to ordered
hexagonal lattices. Furthermore, we demonstrate that the lattice orientation
can be deterministically rotated by changing the scanning direction, as
confirmed by both real-space analysis and fast Fourier transformations. This
method enables the controlled creation, reordering, and deletion of metastable
skyrmion textures on demand. Our approach establishes a practical and
accessible platform for studying two-dimensional phase behavior in topological
spin systems, offering direct and reconfigurable control over lattice symmetry,
order, and orientation.

</details>


### [360] [Core position-dependent gyrotropic and damping contributions to the Thiele equation approach for accurate spin-torque vortex oscillator dynamics](https://arxiv.org/abs/2508.14829)
*Colin Ducarme,Simon De Wergifosse,Flavio Abreu Araujo*

Main category: cond-mat.mes-hall

TL;DR: 该研究提出了一种改进的Thiele方法，通过考虑涡旋剖面变形来更准确地模拟STVO动力学，并提出了一种直接从微磁模拟中提取参数的方法，旨在实现低成本、高保真的预测建模，适用于大规模神经形态电路。


<details>
  <summary>Details</summary>
Motivation: 为了在神经形态计算中应用STVO，理解STVO中磁涡旋的非线性动力学至关重要。现有模型要么依赖只能提供定性预测的标准Thiele方程方法（TEA），要么依赖计算量大的微磁模拟（MMS）。

Method: 提出了一种改进的Thiele方法，该方法结合了涡旋剖面变形，用于计算回转和阻尼项。还提出了一种直接从MMS提取回转和阻尼项的方法。

Result: 结果揭示了涡旋核的阻尼各向异性。

Conclusion: 该框架以低计算成本高保真度捕获了STVO动力学的关键非线性，为基于STVO的大规模神经形态电路的预测建模铺平了道路。

Abstract: Understanding the nonlinear dynamics of magnetic vortices in spin-torque
vortex oscillators (STVOs) is essential for their application in neuromorphic
computing. Existing models either rely on the standard Thiele equation approach
(TEA), which offer only qualitative predictions, or on micromagnetic
simulations (MMS), which are computationally demanding. We present a refined
Thiele approach that incorporates the deformation of the vortex profile for the
evaluation of the gyrotropic and damping terms. In this manuscript, a more
realistic ansatz of the vortex magnetization profile is introduced to extract
these effective parameters semi-analytically. A method to extract the
gyrotropic and damping terms directly from MMS is also presented. The resulting
expressions are benchmarked against state-of-the-art analytical derivations,
and reveal a damping anisotropy of the vortex core. This framework captures the
essential nonlinearities of STVO dynamics with high fidelity at low
computational cost, paving the way for predictive modeling of large-scale
neuromorphic circuits based on STVOs.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [361] [Algorithms for Stable Roommate with Externalities](https://arxiv.org/abs/2508.14194)
*Jing Leng,Sanjukta Roy*

Main category: cs.GT

TL;DR: We studied stable roommate assignments (4PS and 2PS) with agent preferences over rooms and roommates. Our serial dictatorship algorithm finds a strategy-proof and Pareto optimal 4PS assignment. TTC algorithms were found to be not strategy-proof or PO. We also found polynomial-time solutions for 2PS assignments under specific preference structures.


<details>
  <summary>Details</summary>
Motivation: The study of the roommate matching model with preferences over both rooms and roommates was recently initiated, and we aim to analyze two types of stable roommate assignments: 4-person stable (4PS) and 2-person stable (2PS) in conjunction with efficiency and strategy-proofness.

Method: We designed a serial dictatorship based algorithm for finding a 4PS assignment that is Pareto optimal and strategy-proof. We also studied variations of the top trading cycle (TTC) based algorithms.

Result: A simple serial dictatorship algorithm yields a 4PS assignment that is Pareto optimal and strategy-proof, but it is not 2PS. TTC variations are neither strategy-proof nor Pareto optimal (PO). We identified specific preference structures for polynomial-time 2PS assignment finding.

Conclusion: We identify preference structures where a 2PS assignment can be found in polynomial time, addressing the NP-complete existence problem.

Abstract: In the roommate matching model, given a set of 2n agents and n rooms, we find
an assignment of a pair of agents to a room. Although the roommate matching
problem is well studied, the study of the model when agents have preference
over both rooms and roommates was recently initiated by Chan et al. [11]. We
study two types of stable roommate assignments, namely, 4-person stable (4PS)
and 2-person stable (2PS) in conjunction with efficiency and
strategy-proofness. We design a simple serial dictatorship based algorithm for
finding a 4PS assignment that is Pareto optimal and strategy-proof. However,
the serial dictatorship algorithm is far from being 2PS. Next, we study top
trading cycle (TTC) based algorithms. We show that variations of TTC cannot be
strategy-proof or PO. Finally, as Chan et al. (2016) showed that deciding the
existence of 2PS assignment is NP-complete, we identify preference structures
where a 2PS assignment can be found in polynomial time.

</details>


### [362] [Explainable Information Design](https://arxiv.org/abs/2508.14196)
*Yiling Chen,Tao Lin,Wei Tang,Jamie Tucker-Foltz*

Main category: cs.GT

TL;DR: 本研究提出了一种信息设计方法，限制使用由状态空间划分定义的信号方案，并将可解释性价格（PoE）确定为最坏情况下的 $1/2$。研究了该方法的计算复杂性，并为特定情况提供了多项式时间算法。


<details>
  <summary>Details</summary>
Motivation: 信息设计（贝叶斯说服）问题中的最优信号方案通常涉及难以解释或沟通的随机化或状态空间的离散划分。我们提出了一种可解释的信息设计方法，用于解决具有连续状态空间的信息设计问题。

Method: 研究了具有连续状态空间的信息设计问题，并将信息设计者限制为使用由状态空间的确定性和单调划分定义的 $K$-划分信号方案，其中为每个部分中的所有状态发送唯一信号。证明了可解释性的价格（PoE）在最坏情况下恰好是 $1/2$。研究了计算最优可解释信号方案的复杂性，发现精确优化问题通常是 NP-难的。

Result: 可解释性价格（PoE）在最坏情况下是 $1/2$，这意味着划分信号方案的性能最多是任意信号方案的两倍。我们开发了一个技术工具，可以将任何最优信号方案（满足双池化性质）转换为实现前者 $1/2$ 期望效用比例的划分信号方案。

Conclusion: 对于 Lipschitz 效用函数，可以在多项式时间内计算出 $\varepsilon$-近似最优的可解释信号方案；对于分段常数效用函数，可以设计一个提供给最优无限制信号方案的 $1/2$ 因子近似值，这与最坏情况下的 PoE 界限相匹配。

Abstract: The optimal signaling schemes in information design (Bayesian persuasion)
problems often involve non-explainable randomization or disconnected partitions
of state space, which are too intricate to be audited or communicated. We
propose explainable information design in the context of information design
with a continuous state space, restricting the information designer to use
$K$-partitional signaling schemes defined by deterministic and monotone
partitions of the state space, where a unique signal is sent for all states in
each part. We first prove that the price of explainability (PoE) -- the ratio
between the performances of the optimal explainable signaling scheme and
unrestricted signaling scheme -- is exactly $1/2$ in the worst case, meaning
that partitional signaling schemes are never worse than arbitrary signaling
schemes by a factor of 2.
  We then study the complexity of computing optimal explainable signaling
schemes. We show that the exact optimization problem is NP-hard in general. But
for Lipschitz utility functions, an $\varepsilon$-approximately optimal
explainable signaling scheme can be computed in polynomial time. And for
piecewise constant utility functions, we provide an efficient algorithm to find
an explainable signaling scheme that provides a $1/2$ approximation to the
optimal unrestricted signaling scheme, which matches the worst-case PoE bound.
  A technical tool we develop is a conversion from any optimal signaling scheme
(which satisfies a bi-pooling property) to a partitional signaling scheme that
achieves $1/2$ fraction of the expected utility of the former. We use this tool
in the proofs of both our PoE result and algorithmic result.

</details>


### [363] [Properties of Egalitarian Sequences of Committees: Theory and Experiments](https://arxiv.org/abs/2508.14439)
*Paula Böhm,Robert Bredereck,Till Fluschnik*

Main category: cs.GT

TL;DR: We study the task of electing egalitarian sequences of committees and introduce several rules and properties for such rules. We settle the computational complexity of finding a winning sequence and classify our rules against our properties. We also compare our rules empirically and test them experimentally against our properties using existing election data.


<details>
  <summary>Details</summary>
Motivation: We study the task of electing egalitarian sequences of $	au$ committees given a set of agents with additive utilities for candidates available on each of $	au$ levels.

Method: We introduce several rules for electing an egalitarian committee sequence and properties for such rules. We settle the computational complexity of finding a winning sequence for our rules and classify them against our properties. We transform sequential election data from existing election data from the literature and compare our rules empirically and test them experimentally against our properties.

Result: We compare our rules empirically and test them experimentally against our properties.

Conclusion: We classify several rules for electing an egalitarian committee sequence against their properties and settle the computational complexity of finding a winning sequence.

Abstract: We study the task of electing egalitarian sequences of $\tau$ committees
given a set of agents with additive utilities for candidates available on each
of $\tau$ levels. We introduce several rules for electing an egalitarian
committee sequence as well as properties for such rules. We settle the
computational complexity of finding a winning sequence for our rules and
classify them against our properties. Additionally, we transform sequential
election data from existing election data from the literature. Using this data
set, we compare our rules empirically and test them experimentally against our
properties.

</details>


### [364] [Learning in Repeated Multi-Objective Stackelberg Games with Payoff Manipulation](https://arxiv.org/abs/2508.14705)
*Phurinut Srisawad,Juergen Branke,Long Tran-Thanh*

Main category: cs.GT

TL;DR: 研究在重复多目标Stackelberg博弈中，领导方如何在未知从属方效用函数的情况下，通过支付操纵来最大化自身收益，并提出了基于EU和longEU的策略，其中longEU在无限重复下可达最优。实验证明该方法有效且能促进合作。


<details>
  <summary>Details</summary>
Motivation: 研究支付操纵在重复多目标Stackelberg博弈中的应用，领导方可能通过提供自身部分收益等方式来影响从属方的确定性最佳响应。由于从属方的效用函数（代表多目标偏好）是未知的但线性的，需要通过交互来推断其权重参数，这给领导方带来了在偏好揭示和即时效用最大化之间进行权衡的顺序决策挑战。

Method: 提出基于预期效用（EU）和长期预期效用（longEU）的操纵策略，以指导领导方选择行动和提供激励，在短期收益和长期影响之间进行权衡。

Result: 提出的操纵策略在累积领导方效用方面优于现有方法，并促进了互利共赢的结果，同时无需显式协商或预知从属方的效用函数。

Conclusion: 该研究提出的基于预期效用（EU）和长期预期效用（longEU）的操纵策略，在无限重复交互下，longEU能收敛到最优操纵。实证结果表明，该方法在不要求显式协商或预知从属方效用函数的情况下，能够提升领导方的累积效用并促进互利共赢。

Abstract: We study payoff manipulation in repeated multi-objective Stackelberg games,
where a leader may strategically influence a follower's deterministic best
response, e.g., by offering a share of their own payoff. We assume that the
follower's utility function, representing preferences over multiple objectives,
is unknown but linear, and its weight parameter must be inferred through
interaction. This introduces a sequential decision-making challenge for the
leader, who must balance preference elicitation with immediate utility
maximisation. We formalise this problem and propose manipulation policies based
on expected utility (EU) and long-term expected utility (longEU), which guide
the leader in selecting actions and offering incentives that trade off
short-term gains with long-term impact. We prove that under infinite repeated
interactions, longEU converges to the optimal manipulation. Empirical results
across benchmark environments demonstrate that our approach improves cumulative
leader utility while promoting mutually beneficial outcomes, all without
requiring explicit negotiation or prior knowledge of the follower's utility
function.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [365] [HDBMS: A Context-Aware Hybrid Graph Traversal Algorithm for Efficient Information Discovery in Social Networks](https://arxiv.org/abs/2508.14092)
*Rowanda Ahmed,Belaynesh Chekol,Mahmoud Alsaleh*

Main category: cs.SI

TL;DR: HDBMS是一种新的图搜索算法，通过概率性地选择节点，比传统的DFS和BFS能更有效地找到有意义的路径。


<details>
  <summary>Details</summary>
Motivation: 传统的图搜索算法（如DFS和BFS）遵循固定的遍历模式。HDBMS旨在提供一种更智能、更具适应性的搜索方法。

Method: 提出了一种名为混合深度-广度有意义搜索（HDBMS）的新型图遍历方法，该方法根据概率节点转换动态调整其探索策略。

Result: HDBMS在保持计算效率的同时，在识别有意义路径方面优于传统算法。

Conclusion: HDBMS通过整合概率决策，构建了一种自适应、结构化的遍历顺序，平衡了深度和广度的探索，在信息检索、社交网络分析和推荐系统等应用中特别有效。结果表明，HDBMS在最具价值的连接不可预测地出现的场景中表现出鲁棒性，使其成为传统图搜索技术的有力替代方案。

Abstract: Graph-searching algorithms play a crucial role in various computational
domains, enabling efficient exploration and pathfinding in structured data.
Traditional approaches, such as Depth-First Search (DFS) and Breadth-First
Search (BFS), follow rigid traversal patterns -- DFS explores branches
exhaustively, while BFS expands level by level. In this paper, we propose the
Hybrid Depth-Breadth Meaningful Search (HDBMS) algorithm, a novel graph
traversal method that dynamically adapts its exploration strategy based on
probabilistic node transitions. Unlike conventional methods, HDBMS prioritizes
traversal paths by estimating the likelihood that a node contains the desired
information, ensuring a more contextually relevant search. Through extensive
experimentation on diverse directed graphs with varying structural properties,
we demonstrate that HDBMS not only maintains competitive computational
efficiency but also outperforms traditional algorithms in identifying
meaningful paths. By integrating probabilistic decision-making, HDBMS
constructs an adaptive and structured traversal order that balances exploration
across depth and breadth, making it particularly effective in applications such
as information retrieval, social network analysis, and recommendation systems.
Our results highlight the robustness of HDBMS in scenarios where the most
valuable connections emerge unpredictably, positioning it as a powerful
alternative to traditional graph-searching techniques.

</details>


### [366] [Non-Dissipative Graph Propagation for Non-Local Community Detection](https://arxiv.org/abs/2508.14097)
*William Leeney,Alessio Gravina,Davide Bacciu*

Main category: cs.SI

TL;DR: uAGNN通过利用非耗散动力学和反常权重矩阵，有效传播长距离信息，从而在异质性图中实现优越的社区检测性能。


<details>
  <summary>Details</summary>
Motivation: 在异质性图中进行社区检测具有挑战性，因为节点通常连接疏远。图神经网络（GNN）的局部消息传递机制在这种情况下效果不佳。该研究认为，在消息传递过程中传播长距离信息对于有效进行异质性图社区检测至关重要。

Method: 提出了一种名为uAGNN（无监督反常图神经网络）的新型无监督社区检测方法，该方法利用非耗散动力学系统来确保稳定性和有效传播长距离信息。通过采用反常权重矩阵，uAGNN能够捕捉局部和全局图结构，克服异质性场景的限制。

Result: uAGNN在十个数据集的广泛实验中，在中高异质性设置下取得了卓越的性能，而传统方法在该类设置下未能利用长距离依赖性。

Conclusion: uAGNN在高中低异质性环境中表现优于传统方法，有潜力成为各种图环境的强大无监督社区检测工具。

Abstract: Community detection in graphs aims to cluster nodes into meaningful groups, a
task particularly challenging in heterophilic graphs, where nodes sharing
similarities and membership to the same community are typically distantly
connected. This is particularly evident when this task is tackled by graph
neural networks, since they rely on an inherently local message passing scheme
to learn the node representations that serve to cluster nodes into communities.
In this work, we argue that the ability to propagate long-range information
during message passing is key to effectively perform community detection in
heterophilic graphs. To this end, we introduce the Unsupervised Antisymmetric
Graph Neural Network (uAGNN), a novel unsupervised community detection approach
leveraging non-dissipative dynamical systems to ensure stability and to
propagate long-range information effectively. By employing antisymmetric weight
matrices, uAGNN captures both local and global graph structures, overcoming the
limitations posed by heterophilic scenarios. Extensive experiments across ten
datasets demonstrate uAGNN's superior performance in high and medium
heterophilic settings, where traditional methods fail to exploit long-range
dependencies. These results highlight uAGNN's potential as a powerful tool for
unsupervised community detection in diverse graph environments.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [367] [NV-like Defects More Common Than Four-Leaf Clovers: A Perspective on High-Throughput Point Defect Data](https://arxiv.org/abs/2508.14223)
*Joel Davidsson*

Main category: cond-mat.mtrl-sci

TL;DR: NV-like defects are surprisingly common across various materials, suggesting a broader potential for quantum technologies. Utilizing databases can accelerate the discovery of new defects.


<details>
  <summary>Details</summary>
Motivation: The research aims to determine the prevalence of NV-like defects and identify new defects for quantum technologies by expanding the search criteria and utilizing point defect databases.

Method: The paper analyzes results of NV-like defects across 17 different materials and discusses expanding search criteria to identify other defects relevant to quantum technologies. It also highlights the utility of point defect databases.

Result: NV-like defects are more common than finding four-leaf clovers, having been identified in 17 different materials. The paper discusses the potential of point defect databases in discovering new quantum technology-relevant defects.

Conclusion: NV-like defects are more common than previously thought and are found in at least 17 different materials. Expanding search criteria and utilizing point defect databases can help discover new defects for quantum technologies.

Abstract: Point defects for quantum technologies is an emerging research area, with the
nitrogen-vacancy (NV) center in diamond at the forefront. However, how rare are
defects with NV-like properties? In this perspective, I highlight the results
of NV-like defects across 17 different materials, revealing that they are more
common than finding four-leaf clovers. I also discuss expanding the search
criteria to identify other defects relevant to quantum technologies. Utilizing
point defect databases will be instrumental in assisting researchers in
discovering previously unexplored defects suitable for quantum technologies.

</details>


### [368] [Engineering and exploiting self-driven domain wall motion in ferrimagnets for neuromorphic computing applications](https://arxiv.org/abs/2508.14252)
*Jeffrey A. Brock,Aleksandr Kurenkov,Aleš Hrabec,Laura J. Heyderman*

Main category: cond-mat.mtrl-sci

TL;DR: 研究發現，通過精確控制亞鐵磁體中的磁疇壁運動，可以簡化神經形態計算設備的設計，實現節能高效的計算。


<details>
  <summary>Details</summary>
Motivation: 為了實現節能的、下一代的神經形態計算架構，研究磁疇壁運動，特別是在標準材料系統中實現複雜功能方面的挑戰。

Method: 本研究結合實驗和微磁模擬，利用橫向交換耦合來控制磁疇壁運動，並結合自旋軌道扭矩來實現神經形態計算的行為。

Result: 通過調整材料和結構參數，成功控制了磁疇壁的運動速度，並實現了人工神經元的洩漏積分和被動重置等神經形態計算功能。

Conclusion: 該研究證明了過渡金屬-稀土亞鐵磁體中的自發磁疇壁運動可以實現神經形態計算的多种功能，並展示了通過調整特徵尺寸、材料成分和手性相互作用強度來控制運動速度的方法。

Abstract: Magnetic domain wall motion has recently garnered significant interest as a
physical mechanism to enable energy-efficient, next-generation brain-inspired
computing architectures. However, realizing all behaviors required for
neuromorphic computing within standard material systems remains a significant
challenge, as these functionalities often rely on competing interactions. Here,
we demonstrate how spontaneous domain wall motion in response to locally
engineered lateral exchange coupling in transition metal-rare earth
ferrimagnets can be leveraged to achieve numerous neuromorphic computing
functionalities in devices with minimal complexity. Through experiments and
micromagnetic simulations, we show how tuning the feature size, material
composition, and chiral interaction strength controls the speed of self-driven
domain wall motion. When integrated with spin-orbit torque, this control gives
rise to behaviors essential for neuromorphic computing, including leaky
integration and passive resetting of artificial neuron potential. These results
establish locally engineered ferrimagnets as a tunable, scalable, and
straightforward platform for domain wall-based computing architectures.

</details>


### [369] [Lattice anharmonicity effects in fluorite oxide single crystals and anomalous increase in phonon lifetime in ceria at elevated temperature](https://arxiv.org/abs/2508.14254)
*Amey Khanolkar,Saqeeb Adnan,Md Minaruzzaman,Linu Malakkal,Darren B. Thomson,David B. Turner,J. Matthew Mann,David H. Hurley,Marat Khafizov*

Main category: cond-mat.mtrl-sci

TL;DR: 氧化铈在高温下表现出异常的声子行为，这可能与晶格的非简谐性有关。


<details>
  <summary>Details</summary>
Motivation: 为了深入了解晶格的非简谐性以及声子重整化在高温下对声子寿命的影响。

Method: 利用拉曼光谱技术，研究了从室温到1273 K温度范围内，氧化铈和氧化钍单晶中三重简并的T$_{2g}$带中心光学声子的频率和线宽的温度依赖性。

Result: 氧化铈和氧化钍都表现出随着温度升高，声子线宽增加的预期行为。然而，氧化铈在1023-1123 K的温度范围内出现异常的线宽减少。现有的第一性原理计算和基于温度无关的声子色散的计算无法解释这一异常。基于声子色散的经验模型可以预测线宽的非单调增加，但峰值出现在实验观察值之下。

Conclusion: 研究发现，在1023-1123 K的温度范围内，氧化铈的声子线宽出现异常减少，这表明晶格的非简谐性诱导的声子重整化在高温下对声子寿命起着重要作用，并导致了声子散射相空间的减少。

Abstract: We investigate the temperature dependence of the frequency and linewidth of
the triply-degenerate T$_{2g}$ zone-centered optical phonon in flux-grown ceria
and hydrothermally-synthesized thoria single crystals from room temperature to
1273 K using Raman spectroscopy. Both crystals exhibit an expected increase in
the phonon linewidth with temperature due to enhanced phonon-phonon scattering.
However, ceria displays an anomalous linewidth reduction in the temperature
range of 1023-1123 K. First-principles phonon linewidth calculations
considering cubic and quartic phonon interactions within
temperature-independent phonon dispersion fail to describe this anomaly. A
parameterization of the temperature-dependent second order interatomic force
constants based on previously reported phonon dispersion measured at room and
high temperatures, predicts a deviation from the monotonic linewidth increase,
albeit at temperatures lower than those observed experimentally for ceria. The
qualitative agreement in the trend of temperature-dependent linewidth suggests
that lattice anharmonicity-induced phonon renormalization plays a role in
phonon lifetime. Specifically, a change in the overlap between softened
acoustic and optical branches in the dispersion curve reduces the available
phonon scattering phase space of the Raman active mode at the zone center,
leading to an increased phonon lifetime within a narrow temperature interval.
These findings provide new insights into higher-order anharmonic interactions
in ceria and thoria, motivating further investigations into the role of
anharmonicity-induced phonon renormalization on phonon lifetimes at high
temperatures.

</details>


### [370] [Real-space first-principles approach to orbitronic phenomena in metallic multilayers](https://arxiv.org/abs/2508.14270)
*Ramon Cardias,Hugo U. R. Strand,Anders Bergman,A. B. Klautau,Tatiana G. Rappoport*

Main category: cond-mat.mtrl-sci

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We develop a real-space first-principles method based on density functional
theory to investigate orbitronic phenomena in complex materials. Using the
Real-Space Linear Muffin-Tin Orbital method within the Atomic Sphere
Approximation (RS-LMTO-ASA) combined with a Chebyshev polynomial expansion of
the Green's functions, we compute orbital (spin) Hall transport and orbital
(spin) accumulation directly in real space. The approach scales linearly with
system size and naturally incorporates disorder, finite-size effects, and
interface roughness. We apply the method to transition-metal-based
heterostructures and demonstrate the emergence of substantial orbital (spin)
accumulation, even in centrosymmetric systems. Our methodology provides a
scalable and flexible framework for realistic simulations of orbital transport
phenomena in complex heterostructures.

</details>


### [371] [Machine-learning interatomic potentials achieving CCSD(T) accuracy for van-der-Waals-dominated systems via Δ-learning](https://arxiv.org/abs/2508.14306)
*Yuji Ikeda,Axel Forslund,Pranav Kumar,Yongliang Ou,Jong Hyun Jung,Andreas Köhn,Blazej Grabowski*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种Δ-学习方法，结合紧束缚基线和在CCSD(T)与基线能量差上训练的MLIP，实现了对包含范德华相互作用的周期性系统的高精度（CCSD(T)级别）原子模拟，并将此方法应用于COF材料的分析。


<details>
  <summary>Details</summary>
Motivation: 为了在合理计算成本下实现高精度（如CCSD(T）级别）的原子尺度模拟，特别是处理包含范德华相互作用的周期性系统，这是传统方法难以实现的。

Method: 本研究采用Δ-学习工作流程，结合了经过色散校正的紧束缚基线和在目标CCSD(T)能量与基线能量之差上训练的MLIP。通过在训练集中包含vdW结合的多聚体，并使用vdW感知的紧束缚基线，实现了具有CCSD(T)精度的形式上局域的MLIP，能够处理受长程vdW力支配的系统。

Result: 该方法生成的势能具有CCSD(T)精度，在训练集和测试集上的均方根能量误差低于0.4 meV/atom。该势能能够准确重现基准分子系统的电子总键合能、键长、谐振频率和分子间相互作用能。对于准二维共价有机框架（COF），该方法分析了其结构、层间结合能和氢吸收。

Conclusion: 该研究提出了一种Δ-学习工作流程，能够以CCSD(T)精度处理包含范德华相互作用的周期性系统，并成功应用于COF材料的结构、层间结合能和氢吸收分析，为包含范德华相互作用的大规模原子模拟提供了实用的方法。

Abstract: Machine-learning interatomic potentials (MLIPs) enable large-scale atomistic
simulations at moderate computational cost while retaining ab initio accuracy.
However, most MLIPs are trained on density-functional theory (DFT), which often
falls short of chemical accuracy (1 kcal/mol). Conversely, coupled-cluster
methods, particularly CCSD(T), which includes single, double, and perturbative
triple excitations, are considered the gold standard of computational chemistry
but rarely applied to periodic systems due to their huge computational cost.
Here we present a $\Delta$-learning workflow to produce interatomic potentials
with CCSD(T) accuracy for periodic systems including van der Waals (vdW)
interactions. The procedure combines a dispersion-corrected tight-binding
baseline with an MLIP trained on the differences of the target CCSD(T) energies
from the baseline. This $\Delta$-learning strategy enables training on compact
molecular fragments while preserving transferability. The dispersion
interactions are captured by including vdW-bound multimers in the training set;
together with the vdW-aware tight-binding baseline, the formally local MLIP
attains CCSD(T) accuracy for systems governed by long-range vdW forces. The
resulting potential yields root-mean-square energy errors below 0.4 meV/atom on
both training and test sets and reproduces electronic total atomization
energies, bond lengths, harmonic vibrational frequencies, and inter-molecular
interaction energies for benchmark molecular systems. We apply the method to a
prototypical quasi-two-dimensional covalent organic framework (COF) composed of
carbon and hydrogen. The COF structure, inter-layer binding energies, and
hydrogen absorption are analyzed at CCSD(T) accuracy. Overall, the developed
$\Delta$-learning approach opens a practical route to large-scale atomistic
simulations that include vdW interactions with chemical accuracy.

</details>


### [372] [Modeling of silver transport in cubic SiC: Integrating molecular dynamics, bounds averaging, and uncertainty quantification](https://arxiv.org/abs/2508.14325)
*Mohamed AbdulHameed,Khadija Mahbuba,Mahmoud Yaseen,Amr Ibrahim,Daniel Moneghan,Benjamin Beeler*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究提出了一种新的模型来解释银在核燃料 SiC 涂层中的迁移行为，解决了以往模型高估扩散率的问题，并为改进核燃料设计提供了见解。


<details>
  <summary>Details</summary>
Motivation: 银从 TRISO 燃料颗粒中迁移并通过 SiC 层迁移并在反应堆组件上沉积，会带来辐射危害和操作挑战。然而，银在完整 3C-SiC 中的确切传输途径仍不清楚，因此需要对该过程进行建模和理解。

Method: 该研究采用了一种结合分子动力学 (MD) 模拟、贝叶斯推断和多尺度建模的方法。MD 模拟用于计算银在 {"Σ3"} 和 {"Σ9"} 晶界 (GBs) 的扩散率，并结合文献值处理其他晶界类型和块体扩散。通过贝叶斯推断拟合实验数据，得到有效的阿伦尼乌斯参数及其可信区间。为解决模型高估扩散率的问题，引入了一个基于可逆陷阱效应的多重修正因子，并进行了敏感性分析以确定关键影响因素。

Result: 该研究建立了一个考虑了晶界和可逆陷阱效应的银在多晶 3C-SiC 中传输的模型。研究结果表明，可逆陷阱修正因子能够很好地重现实验观察到的传输行为，并确定了陷阱解吸能和 {"Σ9"} 晶界扩散率是影响银传输的主要因素。该框架为理解和模拟燃料性能提供了基础。

Conclusion: 这项研究通过结合分子动力学模拟、贝叶斯推断和基于第一原理的修正模型，为理解银在多晶 3C-SiC 中的传输行为提供了一个详细的框架，解决了先前模型高估扩散率的问题，并确定了影响传输的关键因素。

Abstract: Silver released from TRISO fuel particles can migrate through the SiC layer
and deposit on reactor components, posing radiation hazards and operational
challenges. Despite numerous proposed mechanisms, the precise pathway of silver
transport through intact 3C-SiC remains unresolved. We present a
physics-informed model for estimating the effective diffusivity of silver in
polycrystalline 3C-SiC. Molecular dynamics (MD) simulations yield diffusivities
for {\Sigma 3} and {\Sigma 9} grain boundaries (GBs), while literature values
are used for other GB types and the bulk. These are combined using a
bounds-averaging approach accounting for distinct GB transport properties.
Bayesian inference of experimental data provides credible intervals for
effective Arrhenius parameters and reveals a correlation between activation
energy and pre-exponential factor. Although the homogenized model captures
GB-mediated transport mechanisms, it overpredicts silver diffusivity relative
to experiments. To resolve this, a multiplicative correction based on
reversible trapping at nano-pores is introduced. It is derived from first
principles and is shown to reproduce observed transport behavior. Sensitivity
analysis identified trap desorption energy and {\Sigma 9} GB diffusivity as
dominant factors influencing Ag transport. The resulting framework provides a
mechanistic description of Ag transport suitable for integration into
higher-scale fuel performance models.

</details>


### [373] [Modeling oxygen-void interactions in uranium nitride](https://arxiv.org/abs/2508.14329)
*Mohamed AbdulHameed,Anton J. Schneider,Benjamin Beeler,Michael W. D. Cooper*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究利用第一性原理模型，揭示了氧杂质通过降低铀氮化物的表面能来促进其在辐照下的溶胀行为，并确定了影响这一过程的关键因素。


<details>
  <summary>Details</summary>
Motivation: 尽管已报道氧杂质会影响铀氮化物（UN）在辐照下的溶胀行为，但其潜在机制尚不清楚。

Method: 开发了一个第一性原理模型，量化了氧与UN中的空隙和裂变气体泡的相互作用，从而降低了表面能，可能促进溶胀。

Result: 研究发现，取代氧在氮表面位点的偏析是表面能降低的主要驱动因素，而表面空位中的氧作用很小。表面能降低在1-10 nm的空腔和中等温度下最为显著，这与UN的实际溶胀开始时间一致。

Conclusion: 氧含量和空腔尺寸对表面能降低有显著影响，但对孔隙率不敏感。结果表明，氧诱导的表面能降低对于协调UN的力学溶胀模型与实验观察结果至关重要。

Abstract: Oxygen impurities in uranium nitride (UN) are reported to influence its
swelling behavior under irradiation, yet the underlying mechanism remains
unknown. In this work, we develop a first-principles model that quantifies the
interaction of oxygen with voids and fission gas bubbles in UN, leading to a
reduction in surface energy that can promote swelling. The analysis reveals
that segregation of substitutional oxygen at surface nitrogen sites is the
primary driver of surface energy reduction, $|\Delta \sigma|$, while oxygen in
surface hollow sites plays a minor and sometimes counteracting role. $|\Delta
\sigma|$ is most pronounced for small cavities ($R_v$ = 1--10 nm) at
intermediate temperatures that coincide with the onset of breakaway swelling in
UN. Larger voids require higher temperatures for oxygen adsorption to
significantly lower their surface energy. The temperature dependence of
$|\Delta \sigma|$ exhibits three regimes: negligible reduction at low
temperatures due to sluggish oxygen diffusion, a maximum at intermediate
temperatures where oxygen incorporation is optimal, and a decline at high
temperatures due to enhanced bulk solubility. A parametric analysis reveals
that $|\Delta \sigma|$ depends strongly on both oxygen concentration and cavity
size, but is largely insensitive to porosity. Our results suggest that
oxygen-induced surface energy reduction is essential for reconciling the
mechanistic swelling model of UN with experimental observations.

</details>


### [374] [Dynamic Vacancy Levels in CsPbCl3 Obey Equilibrium Defect Thermodynamics](https://arxiv.org/abs/2508.14513)
*Irea Mosquera-Lois,Aron Walsh*

Main category: cond-mat.mtrl-sci

TL;DR: 机器学习力场研究了CsPbCl$_3$中的VCl缺陷，发现它对非辐射损失的影响被高估了，其主要影响是限制开路电压和促进离子迁移，但静态缺陷理论形式对于预测热力学行为仍然有效。


<details>
  <summary>Details</summary>
Motivation: 了解工作条件下的缺陷至关重要，因为它们的电子能级会呈现大的热涨落，这使得静态0 K模型在该领域的有效性受到挑战。

Method: 本研究训练了一个多任务机器学习力场来研究正交MAPbCl$_3$中的$V$Cl。

Result: 研究观察到光学跃迁能级由于软势能面而出现强烈的振荡，但非辐射捕获势垒和热力学电荷跃迁能级均不受影响。研究结果表明，$V$Cl并非先前假设的造成非辐射损失的原因，其对器件性能的影响源于限制开路电压和促进离子迁移等其他机制。

Conclusion: 这项工作表明，尽管卤素钙钛矿中存在强烈的动力学效应，但用于预测热力学行为的传统静态缺陷理论形式仍然有效，这为设计高性能能源材料提供了坚实的基础。

Abstract: Halide vacancies are the dominant point defects in perovskites with
$V_\mathrm{Cl}$ identified as a detrimental trap for the optoelectronic
performance of CsPbCl$_3$, with applications ranging from photodetectors to
solar cells. Understanding these defects under operating conditions is key
since their electronic levels exhibit large thermal fluctuations that challenge
the validity of static 0 K models. However, quantitative modelling of defect
processes requires hybrid density functional theory with spin-orbit coupling,
which is too expensive for direct molecular dynamic simulations. To address
this, we train a multi-task machine learning force field to study
$V_\mathrm{Cl}$ in orthorhombic CsPbCl$_3$ at 300 K.While we observe strong
oscillations in the optical transition level arising from the soft potential
energy surface, neither the non-radiative capture barriers nor the
thermodynamic charge transition levels are affected. Our results reveal that
$V_\mathrm{Cl}$ is not responsible for the non-radiative losses previously
assumed. Instead, its impact on performance arises from other mechanisms, such
as limiting the open-circuit voltage and promoting ionic migration. Our
findings demonstrate that, despite strong dynamical effects in halide
perovskites, the conventional static formalism of defect theory remains valid
for predicting thermodynamic behavior, providing a sound basis for the design
of high-performance energy materials.

</details>


### [375] [Electron charge dynamics and charge separation: A response theory approach](https://arxiv.org/abs/2508.14551)
*Lionel Lacombe,Lucia Reining,Vitaly Gorelov*

Main category: cond-mat.mtrl-sci

TL;DR: 本文使用响应理论研究电荷动力学，重点关注电荷分离。二次响应理论比线性响应理论更能描述电荷分离，并能近似精确时间传播结果。


<details>
  <summary>Details</summary>
Motivation: 研究电荷分离的电子电荷动力学，并评估线性和二次响应理论的优缺点。

Method: 应用响应理论，特别是线性和二次响应理论，通过模型模拟电荷转移系统来分析电荷密度和电流。

Result: 二次响应理论在某些情况下与精确时间传播结果高度一致，并提出和测试了二次响应的近似方法，探讨了高阶项和驻波相互作用U的影响。

Conclusion: 二次响应理论是描述电荷动力学和分离的最小理论，在某些情况下与精确时间传播结果高度一致。

Abstract: This study applies response theory to investigate electron charge dynamics,
with a particular focus on charge separation. We analytically assess the
strengths and limitations of linear and quadratic response theories in
describing charge density and current, illustrated by a model that simulates
charge transfer systems. While linear response accurately captures optical
properties, the quadratic response contains the minimal ingredients required to
describe charge dynamics and separation. Notably, it closely matches exact time
propagation results in some regime that we identify. We propose and test
several approximations to the quadratic response and explore the influence of
higher-order terms and the effect of an on-site interaction $U$.

</details>


### [376] [Altermagnetic magnon transport in the \textit{d}-wave altermagnet \ch{LuFeO3}](https://arxiv.org/abs/2508.14569)
*Edgar Galindez-Ruales,Wanting Yang,Tobias Dannegger Moumita Kundu,Jonas Köhler,Christin Schmitt Felix Fuhrmann,Akashdeep Akashdeep,Duc Minh Tran Xiaoxuan Ma,Gerhard Jakob,Shixun Cao,Ulrich Nowak,Mathias Kläui*

Main category: cond-mat.mtrl-sci

TL;DR: 研究人员在LuFeO3材料中实验证明了磁异向异性材料的非局域磁畴输运，并证实了其在无场磁性自旋输运方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 探索具有零净磁化强度但具有自旋分裂能带结构的磁异向异性材料的特殊磁畴性质，特别是无场自旋输运。

Method: 通过自旋塞贝克效应和自旋霍尔效应实现非局域磁畴可变性振荡子输运的实验演示，并辅以原子尺度自旋动力学模拟和线性自旋波理论计算。

Result: 在d波磁异向异性材料 LuFeO3 中观察到非局域自旋信号，该信号仅在沿磁异向异性方向输运时出现，并且在自旋塞贝克响应中观察到不同磁异向异性方向之间的符号反转，这与理论计算一致。

Conclusion: 该研究为磁异向异性材料中的磁畴可变性振荡子提供了直接证据，并强调了其在无场磁性自旋运输方面的潜力，为低功耗自旋电子学应用提供了一条有前景的途径。

Abstract: Altermagnets exhibit a spin-split band structure despite having zero net
magnetization, leading to special magnonic properties such as anisotropic
magnon lifetimes and field-free spin transport. Here, we present a direct
experimental demonstration of non-local magnon transport in the \textit{d}-wave
altermagnet \ch{LuFeO3}, using both spin Seebeck and spin Hall effect-based
injection and detection. We observe a non-local spin signal at zero magnetic
field when the transport is along an altermagnetic direction, but not for
transport along other directions. The observed sign reversal between two
distinct altermagnetic directions in the spin Seebeck response demonstrates the
altermagnetic nature of the magnon transport. In contrast, when transport is
aligned along or perpendicular to the easy axis, both the first-harmonic signal
and the sign-reversal effect vanish, consistent with symmetry-imposed
suppression. These findings are supported by atomistic spin dynamics
simulations, as well as linear spin wave theory calculations, which explain how
our altermagnetic system hosts anisotropic spin Seebeck transport. Our results
provide direct evidence of direction-dependent magnon splitting in altermagnets
and highlight their potential for field-free magnonic spin transport, offering
a promising pathway for low-power spintronic applications.

</details>


### [377] [Severe plastic deformations, mechanochemistry, and microstructure evolution under high pressure: In Situ Experiments, Four-Scale Theory, New Phenomena, and Rules](https://arxiv.org/abs/2508.14721)
*Valery I. Levitas*

Main category: cond-mat.mtrl-sci

TL;DR: 高压下的塑性变形可以改变相变和化学反应，这对于材料科学和地球科学有重要意义。


<details>
  <summary>Details</summary>
Motivation: 这项研究的动机是为了深入理解高压下严重塑性变形（SPD）与相变和化学反应（PTs/CRs）之间的相互作用，以及这种相互作用如何影响微观结构演变。理解这些过程对于获得新的纳米结构相、进行机电化学合成、军事应用以及理解自然现象至关重要。

Method: 本综述采用多尺度理论和模拟（从原子尺度到纳尺度，再到尺度无关的相场方法）与高压金刚细胞（传统和旋转）的原位实验相结合的方法，来理解高压相变/化学反应、严重塑性变形和微观结构演变之间的相互作用。通过这种方法，研究人员开发了耦合分析/计算/实验的方法，以全面表征过程并确定所有异构标量和张量场。

Result: 该研究揭示了高压相变/化学反应与严重塑性变形之间的相互作用，阐明了塑性变形可以显著降低相变压力和滞后，并导致无法通过其他方式获得的隐藏亚稳相。研究还确定了这些领域的第一条普遍规律，并为经济高效地合成高压相和纳米结构提供了新的见解和方法。此外，该研究还解决了许多长期存在的难题，并为未来的研究方向提供了建议。

Conclusion: 这项综述研究了高压下严重塑性变形（SPD）与相变和化学反应（PTs/CRs）的相互作用，重点关注微观结构演变。研究提出了塑性应变诱导高压相变的新概念，并结合了多尺度理论、模拟和实验方法，揭示了隐藏的亚稳相，并为经济高效地合成高压相和纳米结构提供了新途径。研究还探讨了高压扭转、表面处理、高压摩擦学、剪切带中的相变/反应诱导塑性、深源地震机制、地壳中的微金刚石以及地球以外生命的化学起源等应用。

Abstract: Processes involving severe plastic deformations (SPD) and phase
transformations and chemical reactions (PTs/CRs) under high pressures are
widespread for obtaining new nanostructured phases and their processing,
mechanochemical synthesis, military applications, and nature. SPD strongly
reduce the pressure required for PTs/CRs (by one-two orders of magnitude) and
PT hysteresis; lead to hidden metastable phases, which cannot be obtained
otherwise, and substitute reversible PTs/CRs with irreversible ones. This
review is devoted to breakthroughs in understanding multifaceted interactions
between high-pressure PTs/CRs, SPD, and microstructure evolution from the
viewpoint of advanced mechanics and thermodynamics of materials under stress
and plastic strain tensors. A novel concept of plastic strain-induced PTs/CRs
under high pressure is explored using four-scale theory and simulations (from
atomistic to nano- and scale-free phase-field approaches to macroscale) coupled
to in situ experiments in traditional and rotational diamond anvil cells, and
their integration. Its development revealed various phenomena and
misinterpretations, resolved numerous puzzles, found the first general rules in
these fields, and suggested ways for economic defect-induced synthesis of
high-pressure phases and nanostructures. Coupled
analytical/computational/experimental approaches are developed for complete
characterization of occurring processes and finding all heterogeneous scalar
and tensorial fields. Applications include high-pressure torsion, surface
treatment, high-pressure tribology, PTs/CRs in shear bands leading to severe
transformation/reaction-induced plasticity and self-blown-up processes,
mechanisms of deep-focus earthquakes, the appearance of microdiamonds in
low-pressure-temperature Earth crust, and the mechanochemical origin of life
beyond Earth. Unresolved problems and future directions are outlined.

</details>


### [378] [Trion polaron problem in bulk and two-dimensional materials](https://arxiv.org/abs/2508.14756)
*V. Shahnazaryan,A. Kudlis,K. Varga,I. A. Shelykh,I. V. Tokatly*

Main category: cond-mat.mtrl-sci

TL;DR: A theory for trion polarons (two electrons, one hole, plus phonons) was developed using a variational approximation. Binding energies were calculated for perovskites and monolayer materials, offering benchmarks for experiments.


<details>
  <summary>Details</summary>
Motivation: We developed a microscopic theory of the trion polaron, which is a bound state of two electrons and one hole dressed by longitudinal optical phonons.

Method: We adopted the intermediate coupling variational approximation of Lee, Low, and Pines and generalized it for the three-body problem, starting from the Frohlich Hamiltonian. This yielded an effective three-particle Hamiltonian with renormalized interactions.

Result: The theory provides an effective three-particle Hamiltonian with renormalized interactions, and computed binding energies for various materials.

Conclusion: We computed binding energies for bulk perovskite materials and monolayer materials with strong polar effects, providing benchmarks for spectroscopic measurements.

Abstract: We develop a microscopic theoryof the trion polaron: a bound state of two
electrons and one hole, dressed by longitudinal optical (LO) phonons. Starting
from the Frohlich Hamiltonian, which describes the interaction of charged
particles with LO phonons in three-dimensional (bulk) and two-dimensional
(monolayer) polar crystals, we adopt the intermediate coupling variational
approximation of Lee, Low, and Pines, and generalize it for the three-body
problem. This yields an effective three-particle Hamiltonian with renormalized
electron-electron and electron-hole interactions, similar to those obtained for
exciton polaron and bipolaron problems. We compute the binding energies for a
family of bulk perovskite materials and several atomic monolayer materials
characterized by pronounced polar effects, providing quantitative benchmarks
for spectroscopic measurements.

</details>


### [379] [A practical route to donor binding energies: The DFT-1/2 method for shallow defects](https://arxiv.org/abs/2508.14738)
*Joshua Claes,Bart Partoens,Dirk Lamoen,Marcelo Marques,Lara K. Teles*

Main category: cond-mat.mtrl-sci

TL;DR: 本文提出并验证了一种计算浅层缺陷结合能的DFT-1/2方法，该方法在大超胞计算中具有计算效率高、结果准确的优点。


<details>
  <summary>Details</summary>
Motivation: 为了准确计算浅层缺陷的结合能，需要使用大的超胞来捕捉其波函数的扩展性质。然而，许多超越DFT的方法（如混合泛函）对于直接计算来说不切实际，但标准的DFT方法又因低估带隙和离域化错误而无法提供可靠的结果。因此，需要一种能够克服这些缺陷并保持计算效率的方法。

Method: 本文采用DFT-1/2方法，通过外插到无限超胞大小的方案，计算了硅中V族施主（P, As, Sb, Bi）的结合能。

Result: DFT-1/2方法能够有效地计算大至4096个原子的超胞的浅层缺陷结合能，计算结果准确，计算开销小。

Conclusion: DFT-1/2方法提供了一种简单直接的计算施主结合能的方法，可以用于处理大超胞，具有计算效率高、误差小的优点。

Abstract: Accurately calculating the binding energies of shallow defects requires large
supercells to capture the extended nature of their wavefunctions. This makes
many beyond-DFT methods, such as hybrid functionals, impractical for direct
calculations, often requiring indirect or approximate approaches. However,
standard DFT alone fails to provide reliable results due to the well-known band
gap underestimation and delocalization errors. In this work, we employ the
DFT-1/2 method to address these deficiencies while maintaining computational
efficiency allowing us to reach supercells of up to 4096 atoms. We develop a
practical procedure for applying DFT-1/2 to shallow defects and demonstrate its
effectiveness for group V donors in silicon (P, As, Sb, Bi). By using an
extrapolation scheme to infinite supercell size, we obtain accurate binding
energies with minimal computational overhead. This approach offers a simple and
direct method for calculating donor binding energies.

</details>


### [380] [Suppression of the valence transition in solution-grown single crystals of Eu$_2$Pt$_6$Al$_{15}$](https://arxiv.org/abs/2508.14841)
*Juan Schmidt,Dominic H. Ryan,Oliver Janka,Jutta Kösters,Carsyn L. Mueller,Aashish Sapkota,Rafaela F. S. Penacchio,Tyler J. Slade,Sergey L. Bud'ko,Paul C. Canfield*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究发现了 Eu$_2$Pt$_6$Al$_{15}$ 的新晶型，它通过高温溶液生长法制备，与电弧熔炼样品相比，在低温下几乎抑制了铕的价态转变，并在 14 K 时反铁磁有序。结构分析表明层堆积方式不同。


<details>
  <summary>Details</summary>
Motivation: 尽管已报道了 Eu$_2$Pt$_6$Al$_{15}$ 晶体在低温下发生价态转变，但之前合成的样品（通过电弧熔炼）的性质可能并不完全代表该化合物的固有特性。因此，有必要探索新的合成方法以获得具有不同性质的晶型，并深入研究其物理行为。

Method: 本研究通过高温溶液生长法获得了一种新的 Eu$_2$Pt$_6$Al$_{15}$ 晶型，并采用单晶 X 射线衍射分析了其晶体结构。此外，还研究了不同热处理（包括退火和电弧熔炼淬火）对样品性质的影响。

Result: 通过高温溶液生长法合成的 Eu$_2$Pt$_6$Al$_{15}$ 晶体在低温下几乎抑制了铕的价态转变，并在约 14 K 时表现出反铁磁有序。该晶型的层堆积方式与通过电弧熔炼法合成的样品不同。退火处理对样品性质影响不大，只有通过电弧熔炼和淬火才能将其转化为先前报道的晶型。

Conclusion: 该研究发现了一种新的 Eu$_2$Pt$_6$Al$_{15}$ 晶型，其物理性质与之前报道的通过电弧熔炼法合成的样品不同。该新型晶型在低温下近乎抑制了铕的价态转变，并在约 14 K 时以反铁磁有序的方式排列。通过单晶 X 射线衍射分析，揭示了该新型晶型与先前报道的样品具有不同的层堆积方式。

Abstract: The study of Eu intermetallic compounds has allowed the exploration of
valence fluctuations and transitions in 4f electron systems. Recently, a
Eu$_2$Pt$_6$Al$_{15}$ phase synthesized by arc-melting followed by a thermal
treatment was reported [M. Radzieowski \textit{et al.}, J Am Chem Soc 140(28),
8950-8957 (2018)], which undergoes a transition upon cooling below 45~K that
was interpreted as a valence transition from Eu$^{2+}$ to Eu$^{3+}$. In this
paper, we present the discovery of another polymorph of Eu$_2$Pt$_6$Al$_{15}$
obtained by high-temperature solution growth, that presents different physical
properties than the arc-melted polycrystalline sample. Despite the similarities
in crystal structure and chemical composition, the Eu valence transition is
almost fully suppressed in the solution-grown crystals, allowing the moments
associated with the Eu$^{2+}$ state to order antiferromagnetically at around
14~K. A detailed analysis of the crystal structure using single crystal X-ray
diffraction reveals that, although the solution grown crystals are built from
the same constituent layers as the arc-melted samples, these layers present a
different stacking. The effect of different thermal treatments is also studied.
Different anneal procedures did not result in significant changes of the
intrinsic properties, and only by arc-melting and quenching the crystals we
were able to convert them into the previously reported polymorph.

</details>


### [381] [Physics-Informed ML Exploration of Structure-Transport Relationships in Hard Carbon](https://arxiv.org/abs/2508.14849)
*Nikhil Rampal,Stephen E. Weitzner,Fredrick Omenya,Marissa Wood,David M. Reed,Xiaolin Li,Jonathan R. I. Lee,Liwen F. Wan*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究通过结合机器学习势和分子动力学模拟，研究了钠离子在硬碳负极中的扩散行为，发现了影响钠迁移率的关键结构因素，并为优化硬碳负极性能提供了设计原则。


<details>
  <summary>Details</summary>
Motivation: 硬碳（HC）负极具有高容量但其复杂的、未被充分理解的离子传输行为（特别是局部微观结构与钠迁移率之间的关系）阻碍了其性能的理性优化，因此需要对钠离子在硬碳中的传输行为进行研究。

Method: 本研究引入了一个数据驱动框架，结合了机器学习势和分子动力学模拟，系统地研究了在多种碳密度和钠负载量下钠的扩散行为。通过计算每离子的结构描述符，识别了控制离子传输的微观因素，并利用无监督学习发现了不同的扩散模式（跳跃、团簇和空位捕获），而监督分析则强调了曲折度和Na-Na配位是迁移率的主要决定因素。相关性映射进一步将这些输运机制与本体密度和钠含量等加工变量联系起来。

Result: 该研究通过物理信息方法建立了量化的结构-输运关系，能够描述无序碳的异质性，揭示了曲折度和Na-Na配位是钠迁移率的主要决定因素，并将这些输运机制与加工变量（如本体密度和钠含量）联系起来。

Conclusion: 该研究建立了量化的结构-输运关系，以捕捉无序碳的异质性，为下一代电池系统中高性能硬碳负极的工程设计提供了机制见解和可行的设计原则。

Abstract: Sodium-ion batteries are a cost-effective and sustainable alternative to
lithium-ion systems for large-scale energy storage. Hard carbon (HC) anodes,
composed of disordered graphitic and amorphous domains, offer high capacity but
exhibit complex, poorly understood ion transport behavior. In particular, the
relationship between local microstructure and sodium mobility remains
unresolved, hindering rational performance optimization. Here, we introduce a
data-driven framework that combines machine-learned interatomic potentials with
molecular dynamics simulations to systematically investigate sodium diffusion
across a broad range of carbon densities and sodium loadings. By computing
per-ion structural descriptors, we identify the microscopic factors that govern
ion transport. Unsupervised learning uncovers distinct diffusion modes,
including hopping, clustering, and void trapping, while supervised analysis
highlights tortuosity and NaNa coordination as primary determinants of
mobility. Correlation mapping further connects these transport regimes to
processing variables such as bulk density and sodium content. This
physics-informed approach establishes quantitative structure-transport
relationships that capture the heterogeneity of disordered carbon. Our findings
deliver mechanistic insights into sodium-ion dynamics and provide actionable
design principles for engineering high-performance HC anodes in next-generation
battery systems.

</details>


### [382] [Carrier mobilities and electron-phonon interactions beyond DFT](https://arxiv.org/abs/2508.14852)
*Aleksandr Poliukhin,Nicola Colonna,Francesco Libbi,Samuel Poncé,Nicola Marzari*

Main category: cond-mat.mtrl-sci

TL;DR: 提出了一种新的有限差分框架，用于计算电子-声子耦合，该框架易于使用，并且比现有方法更精确。


<details>
  <summary>Details</summary>
Motivation: 从第一性原理计算电子-声子耦合，特别是在密度泛函理论之外的方法，仍然是一个挑战。

Method: 提出了一种有限差分框架，用于计算任何提供特征值和特征向量的电子结构方法的电子-声子耦合。该方法引入了一种基于特征值差异的新型可投影方案，并利用对称性来减少独立的原子位移数量，从而将计算成本保持在可管理的范围内。

Result: 与传统方法相比，该方法计算电子-声子耦合更加精确，并且在硅和砷化镓的应用中，能够更准确地估计本征载流子漂移迁移率和有效质量。

Conclusion: 该方法提供了一个健壮且易于访问的框架，用于利用最先进的电子结构方法探索复杂材料中的电子-声子相互作用。

Abstract: Electron-phonon coupling is a key interaction that governs diverse physical
processes such as carrier transport, superconductivity, and optical absorption.
Calculating such interactions from first-principles with methods beyond
density-functional theory remains a challenge. We introduce here a
finite-difference framework for computing electron-phonon couplings for any
electronic structure method that provides eigenvalues and eigenvectors, and
showcase applications for hybrid and Koopmans functionals, and $GW$ many-body
perturbation theory. Our approach introduces a novel projectability scheme
based on eigenvalue differences and bypasses many of the limitations of the
direct finite difference methods. It also leverages symmetries to reduce the
number of independent atomic displacements, thereby keeping computational costs
manageable. This approach enables seamless integration with established
first-principles codes for generating displaced supercells, performing Wannier
interpolations, and evaluating transport properties. Applications to silicon
and gallium arsenide show that advanced electronic-structure functionals
predict different electron-phonon couplings and modify band curvatures,
resulting in much more accurate estimates of intrinsic carrier drift mobilities
and effective masses. In general, our method provides a robust and accessible
framework for exploring electron-phonon interactions in complex materials with
state-of-the-art electronic structure methods.

</details>
