{"id": "2507.09480", "categories": ["cs.LG", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.09480", "abs": "https://arxiv.org/abs/2507.09480", "authors": ["Guoyou Wang", "Yihua Tan", "Shiqi Liu"], "title": "Discrete Differential Principle for Continuous Smooth Function Representation", "comment": null, "summary": "Taylor's formula holds significant importance in function representation,\nsuch as solving differential difference equations, ordinary differential\nequations, partial differential equations, and further promotes applications in\nvisual perception, complex control, fluid mechanics, weather forecasting and\nthermodynamics. However, the Taylor's formula suffers from the curse of\ndimensionality and error propagation during derivative computation in discrete\nsituations. In this paper, we propose a new discrete differential operator to\nestimate derivatives and to represent continuous smooth function locally using\nthe Vandermonde coefficient matrix derived from truncated Taylor series. Our\nmethod simultaneously computes all derivatives of orders less than the number\nof sample points, inherently mitigating error propagation. Utilizing\nequidistant uniform sampling, it achieves high-order accuracy while alleviating\nthe curse of dimensionality. We mathematically establish rigorous error bounds\nfor both derivative estimation and function representation, demonstrating\ntighter bounds for lower-order derivatives. We extend our method to the\ntwo-dimensional case, enabling its use for multivariate derivative\ncalculations. Experiments demonstrate the effectiveness and superiority of the\nproposed method compared to the finite forward difference method for derivative\nestimation and cubic spline and linear interpolation for function\nrepresentation. Consequently, our technique offers broad applicability across\ndomains such as vision representation, feature extraction, fluid mechanics, and\ncross-media imaging.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u79bb\u6563\u5fae\u5206\u7b97\u5b50\uff0c\u7528\u4e8e\u5904\u7406\u6cf0\u52d2\u516c\u5f0f\u5728\u79bb\u6563\u60c5\u51b5\u4e0b\u7684\u7ef4\u5ea6\u707e\u96be\u548c\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u9886\u57df\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6cf0\u52d2\u516c\u5f0f\u5728\u79bb\u6563\u60c5\u51b5\u4e0b\u5b58\u5728\u7ef4\u5ea6\u707e\u96be\u548c\u5bfc\u6570\u8ba1\u7b97\u4e2d\u7684\u8bef\u5dee\u4f20\u64ad\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u79bb\u6563\u5fae\u5206\u7b97\u5b50\uff0c\u5229\u7528\u6cf0\u52d2\u7ea7\u6570\u63a8\u5bfc\u7684\u8303\u5fb7\u8499\u5fb7\u7cfb\u6570\u77e9\u9635\u6765\u4f30\u8ba1\u5bfc\u6570\u548c\u5c40\u90e8\u8868\u793a\u8fde\u7eed\u5149\u6ed1\u51fd\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u540c\u65f6\u8ba1\u7b97\u6240\u6709\u4f4e\u4e8e\u91c7\u6837\u70b9\u6570\u7684\u9636\u6570\u5bfc\u6570\uff0c\u7f13\u89e3\u4e86\u7ef4\u5ea6\u707e\u96be\u548c\u8bef\u5dee\u4f20\u64ad\uff0c\u5e76\u8fdb\u884c\u4e86\u4e8c\u7ef4\u6269\u5c55\uff0c\u5728\u89c6\u89c9\u8868\u793a\u3001\u7279\u5f81\u63d0\u53d6\u3001\u6d41\u4f53\u529b\u5b66\u548c\u8de8\u5a92\u4f53\u6210\u50cf\u7b49\u9886\u57df\u5177\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u6570\u5b66\u65b9\u6cd5\u8bc1\u660e\u4e86\u5176\u5728\u4f30\u8ba1\u5bfc\u6570\u548c\u51fd\u6570\u8868\u793a\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f97\u5230\u4e86\u66f4\u7d27\u5bc6\u7684\u8bef\u5dee\u754c\u9650\u3002"}}
{"id": "2507.09730", "categories": ["cs.AR", "cs.NA", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.09730", "abs": "https://arxiv.org/abs/2507.09730", "authors": ["Jiechen Huang", "Wenjian Yu"], "title": "Efficient FRW Transitions via Stochastic Finite Differences for Handling Non-Stratified Dielectrics", "comment": "5 pages, 6 figures", "summary": "The accuracy of floating-random-walk (FRW) based capacitance extraction\nstands only when the recursive FRW transitions are sampled unbiasedly according\nto surrounding dielectrics. Advanced technology profiles, featuring complicated\nnon-stratified dielectrics, challenge the accuracy of existing FRW transition\nschemes that approximate dielectrics with stratified or eight-octant patterns.\nIn this work, we propose an algorithm named MicroWalk, enabling accurate FRW\ntransitions for arbitrary dielectrics while keeping high efficiency. It is\nprovably unbiased and equivalent to using transition probabilities solved by\nfinite difference method, but at orders of magnitude lower cost (802$\\times$\nfaster). An enhanced 3-D capacitance solver is developed with a hybrid strategy\nfor complicated dielectrics, combining MicroWalk with the special treatment for\nthe first transition cube and the analytical algorithm for stratified cubes.\nExperiments on real-world structures show that our solver achieves a\nsignificant accuracy advantage over existing FRW solvers, while preserving high\nefficiency.", "AI": {"tldr": "MicroWalk algorithm enables accurate FRW transitions for arbitrary dielectrics with high efficiency, offering significant accuracy advantages over existing FRW solvers.", "motivation": "Advanced technology profiles, featuring complicated non-stratified dielectrics, challenge the accuracy of existing FRW transition schemes that approximate dielectrics with stratified or eight-octant patterns.", "method": "The proposed algorithm MicroWalk enables accurate FRW transitions for arbitrary dielectrics while keeping high efficiency. It is provably unbiased and equivalent to using transition probabilities solved by finite difference method. An enhanced 3-D capacitance solver is developed with a hybrid strategy for complicated dielectrics, combining MicroWalk with the special treatment for the first transition cube and the analytical algorithm for stratified cubes.", "result": "Our solver achieves a significant accuracy advantage over existing FRW solvers, while preserving high efficiency.", "conclusion": "We propose an algorithm named MicroWalk, enabling accurate FRW transitions for arbitrary dielectrics while keeping high efficiency. An enhanced 3-D capacitance solver is developed with a hybrid strategy for complicated dielectrics, combining MicroWalk with the special treatment for the first transition cube and the analytical algorithm for stratified cubes. Experiments on real-world structures show that our solver achieves a significant accuracy advantage over existing FRW solvers, while preserving high efficiency."}}
{"id": "2507.10205", "categories": ["eess.SY", "cs.NA", "cs.SY", "math.NA"], "pdf": "https://arxiv.org/pdf/2507.10205", "abs": "https://arxiv.org/abs/2507.10205", "authors": ["Friedemann Kemm"], "title": "A new time-stepping strategy and boundary treatment to improve recent 2d traffic model", "comment": null, "summary": "We show how a recently published 2d model for traffic flow can be further\nimproved. Besides other improvements and simplifications, we present not only a\nmethod to compute the necessary time step restrictions, but also a subcycling\nfor the inflow and outflow. This drastically reduces computational cost on\nlarge domains with coarse grids, i.\\,e.\\ for simulations of a whole region\ninstead of a small part of a city or town.", "AI": {"tldr": "\u901a\u8fc7\u6539\u8fdb\u7684\u8ba1\u7b97\u65b9\u6cd5\u964d\u4f4e\u4e86\u4e8c\u7ef4\u4ea4\u901a\u6d41\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u4e00\u4e2a\u6700\u8fd1\u53d1\u8868\u7684\u4e8c\u7ef4\u4ea4\u901a\u6d41\u6a21\u578b\u5e76\u964d\u4f4e\u5927\u89c4\u6a21\u3001\u7c97\u7f51\u683c\u6a21\u62df\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u65f6\u95f4\u6b65\u9650\u5236\u548c\u5b50\u5faa\u73af\u7684\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u65f6\u95f4\u6b65\u9650\u5236\u548c\u5b50\u5faa\u73af\u7684\u5f15\u5165\uff0c\u5728\u6a21\u62df\u6574\u4e2a\u533a\u57df\u65f6\uff0c\u8ba1\u7b97\u6210\u672c\u5927\u5927\u964d\u4f4e\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u8ba1\u7b97\u65f6\u95f4\u6b65\u9650\u5236\u548c\u5b50\u5faa\u73af\u6765\u8fdb\u4e00\u6b65\u6539\u8fdb\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.09067", "categories": ["cs.ET", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09067", "abs": "https://arxiv.org/abs/2507.09067", "authors": ["Serhan W. Bahar"], "title": "Quantum-Resilient Privacy Ledger (QRPL): A Sovereign Digital Currency for the Post-Quantum Era", "comment": null, "summary": "The emergence of quantum computing presents profound challenges to existing\ncryptographic infrastructures, whilst the development of central bank digital\ncurrencies (CBDCs) has raised concerns regarding privacy preservation and\nexcessive centralisation in digital payment systems. This paper proposes the\nQuantum-Resilient Privacy Ledger (QRPL) as an innovative token-based digital\ncurrency architecture that incorporates National Institute of Standards and\nTechnology (NIST)-standardised post-quantum cryptography (PQC) with hash-based\nzero-knowledge proofs to ensure user sovereignty, scalability, and transaction\nconfidentiality. Key contributions include adaptations of ephemeral proof\nchains for unlinkable transactions, a privacy-weighted Proof-of-Stake (PoS)\nconsensus to promote equitable participation, and a novel zero-knowledge\nproof-based mechanism for privacy-preserving selective disclosure. QRPL aims to\naddress critical shortcomings in prevailing CBDC designs, including risks of\npervasive surveillance, with a 10-20 second block time to balance security and\nthroughput in future monetary systems. While conceptual, empirical prototypes\nare planned. Future work includes prototype development to validate these\nmodels empirically.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aQRPL\u7684\u6570\u5b57\u8d27\u5e01\u67b6\u6784\uff0c\u5b83\u4f7f\u7528\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\u548c\u96f6\u77e5\u8bc6\u8bc1\u660e\u6765\u63d0\u4f9b\u6297\u91cf\u5b50\u3001\u6ce8\u91cd\u9690\u79c1\u548c\u53ef\u6269\u5c55\u7684CBDC\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u5f53\u524d\u7684\u5bc6\u7801\u5b66\u6311\u6218\u548cCBDC\u8bbe\u8ba1\u4e2d\u7684\u9690\u79c1\u62c5\u5fe7\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u4e86\u5e94\u5bf9\u91cf\u5b50\u8ba1\u7b97\u5bf9\u73b0\u6709\u5bc6\u7801\u57fa\u7840\u8bbe\u65bd\u7684\u5a01\u80c1\u4ee5\u53ca\u4e2d\u592e\u94f6\u884c\u6570\u5b57\u8d27\u5e01\uff08CBDC\uff09\u5728\u9690\u79c1\u4fdd\u62a4\u548c\u8fc7\u5ea6\u4e2d\u5fc3\u5316\u65b9\u9762\u5f15\u53d1\u7684\u62c5\u5fe7\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u56fd\u5bb6\u6807\u51c6\u4e0e\u6280\u672f\u7814\u7a76\u9662\uff08NIST\uff09\u6807\u51c6\u5316\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\uff08PQC\uff09\u548c\u57fa\u4e8e\u54c8\u5e0c\u7684\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u4ee3\u5e01\u5316\u6570\u5b57\u8d27\u5e01\u67b6\u6784\uff08QRPL\uff09\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\u91c7\u7528\u6613\u5931\u6027\u8bc1\u660e\u94fe\u5b9e\u73b0\u4e0d\u53ef\u94fe\u63a5\u4ea4\u6613\u3001\u9690\u79c1\u52a0\u6743\u6743\u76ca\u8bc1\u660e\uff08PoS\uff09\u5171\u8bc6\u4ee5\u4fc3\u8fdb\u516c\u5e73\u53c2\u4e0e\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u96f6\u77e5\u8bc6\u8bc1\u660e\u7684\u9690\u79c1\u4fdd\u62a4\u9009\u62e9\u6027\u62ab\u9732\u673a\u5236\u3002", "result": "QRPL\u67b6\u6784\u65e8\u5728\u786e\u4fdd\u7528\u6237\u4e3b\u6743\u3001\u53ef\u6269\u5c55\u6027\u548c\u4ea4\u6613\u7684\u673a\u5bc6\u6027\uff0c\u5e76\u89e3\u51b3\u4e86\u5f53\u524dCBDC\u8bbe\u8ba1\u4e2d\u666e\u904d\u5b58\u5728\u7684\u76d1\u63a7\u98ce\u9669\u7b49\u95ee\u9898\u3002\u8be5\u67b6\u6784\u7684\u76ee\u6807\u662f\u5b9e\u73b010-20\u79d2\u7684\u51fa\u5757\u65f6\u95f4\uff0c\u4ee5\u5e73\u8861\u672a\u6765\u8d27\u5e01\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u548c\u541e\u5410\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u91cf\u5b50\u5f39\u6027\u9690\u79c1\u8d26\u672c\uff08QRPL\uff09\u4f5c\u4e3a\u4e00\u79cd\u521b\u65b0\u7684\u4ee3\u5e01\u5316\u6570\u5b57\u8d27\u5e01\u67b6\u6784\uff0c\u65e8\u5728\u89e3\u51b3\u4e2d\u592e\u94f6\u884c\u6570\u5b57\u8d27\u5e01\uff08CBDC\uff09\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u7f3a\u9677\uff0c\u5982\u666e\u904d\u76d1\u63a7\u98ce\u9669\uff0c\u5e76\u5e94\u5bf9\u91cf\u5b50\u8ba1\u7b97\u5e26\u6765\u7684\u5bc6\u7801\u5b66\u6311\u6218\u3002"}}
{"id": "2507.08923", "categories": ["cs.AR", "cs.CY", "cs.PF", "B.8.2; C.0; C.1.4; C.4; C.5.5; J.4; K.1; K.4.1; K.6.4"], "pdf": "https://arxiv.org/pdf/2507.08923", "abs": "https://arxiv.org/abs/2507.08923", "authors": ["Rub\u00e9n Rodr\u00edguez \u00c1lvarez", "Denisa-Andreea Constantinescu", "Miguel Pe\u00f3n-Quir\u00f3s", "David Atienza"], "title": "CEO-DC: An Actionable Framework to Close the Carbon Gap in HPC Data Centers", "comment": "15 pages, 11 figures, 2 tables", "summary": "The rapid expansion of data centers (DCs) to support large-scale AI and\nscientific workloads is driving unsustainable growth in energy consumption and\ngreenhouse gas emissions. While successive generations of hardware platforms\nhave improved performance and energy efficiency, the question remains whether\nnew, more efficient platforms can realistically offset the rising emissions\nassociated with increasing demand. Prior studies often overlook the complex\ntrade-offs in such transitions by failing to account for both the economic\nincentives and the projected compute demand growth over the operational\nlifetime of the devices. In response, we present CEO-DC, an integrated model\nand decision-making methodology for Carbon and Economy Optimization in Data\nCenters. CEO-DC models the competing forces of cost, carbon, and compute demand\nto guide optimal platform procurement and replacement strategies. We propose\nmetrics to steer procurement, platform design, and policy decisions toward\nsustainable DC technologies. Given current platform trends, our AI case study\nusing CEO-DC shows that upgrading legacy devices on a 4-year cycle reduces\ntotal emissions. However, these upgrades fail to scale with DC demand growth\ntrends without increasing total emissions in over 44% of cases, and require\neconomic incentives for adoption in over 72%. Furthermore, current carbon\nprices are insufficient to motivate upgrades in 9 out of the 14 countries with\nthe highest number of DCs globally. We also find that optimizing platforms for\nenergy efficiency at the expense of latency can increase the carbon price\nrequired to justify their adoption. In summary, CEO-DC provides actionable\ninsights for DC architects, platform designers, and policymakers by timing\nlegacy platform upgrades, constraining DC growth to sustainable levels,\noptimizing platform performance-to-cost ratios, and increasing incentives.", "AI": {"tldr": "CEO-DC\u6a21\u578b\u901a\u8fc7\u4f18\u5316\u6570\u636e\u4e2d\u5fc3\u7684\u5e73\u53f0\u91c7\u8d2d\u548c\u66f4\u6362\u7b56\u7565\u6765\u5e73\u8861\u6210\u672c\u3001\u78b3\u6392\u653e\u548c\u8ba1\u7b97\u9700\u6c42\u3002\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u5b9a\u671f\u5347\u7ea7\u786c\u4ef6\u6709\u52a9\u4e8e\u51cf\u6392\uff0c\u4f46\u4ec5\u9760\u5347\u7ea7\u65e0\u6cd5\u8ddf\u4e0a\u9700\u6c42\u589e\u957f\uff0c\u4e14\u9700\u8981\u7ecf\u6d4e\u6fc0\u52b1\u63aa\u65bd\u6765\u63a8\u5e7f\u3002\u5f53\u524d\u78b3\u4ef7\u4e0d\u8db3\u4ee5\u5728\u8bb8\u591a\u56fd\u5bb6\u63a8\u52a8\u5347\u7ea7\uff0c\u4e14\u8fc7\u5ea6\u4f18\u5316\u80fd\u6548\u53ef\u80fd\u9002\u5f97\u5176\u53cd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u7ed3\u5408\u5347\u7ea7\u65f6\u673a\u3001\u589e\u957f\u7ea6\u675f\u3001\u6210\u672c\u6548\u76ca\u4f18\u5316\u548c\u6fc0\u52b1\u63aa\u65bd\u6765\u63a8\u52a8\u6570\u636e\u4e2d\u5fc3\u7684\u53ef\u6301\u7eed\u53d1\u5c55\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u6570\u636e\u4e2d\u5fc3\u5feb\u901f\u6269\u5f20\u5bfc\u81f4\u80fd\u6e90\u6d88\u8017\u548c\u6e29\u5ba4\u6c14\u4f53\u6392\u653e\u4e0d\u65ad\u589e\u957f\u7684\u95ee\u9898\uff0c\u5e76\u8bc4\u4f30\u65b0\u786c\u4ef6\u5e73\u53f0\u80fd\u5426\u62b5\u6d88\u4e0d\u65ad\u589e\u957f\u7684\u9700\u6c42\u6240\u5e26\u6765\u7684\u6392\u653e\u589e\u52a0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCEO-DC\u7684\u96c6\u6210\u6a21\u578b\u548c\u51b3\u7b56\u65b9\u6cd5\uff0c\u7528\u4e8e\u6570\u636e\u4e2d\u5fc3\u7684\u78b3\u548c\u7ecf\u6d4e\u4f18\u5316\uff0c\u8003\u8651\u4e86\u6210\u672c\u3001\u78b3\u6392\u653e\u548c\u8ba1\u7b97\u9700\u6c42\u4e4b\u95f4\u7684\u590d\u6742\u6743\u8861\uff0c\u5e76\u63d0\u51fa\u4e86\u6307\u5bfc\u91c7\u8d2d\u3001\u5e73\u53f0\u8bbe\u8ba1\u548c\u653f\u7b56\u51b3\u7b56\u7684\u6307\u6807\u3002", "result": "\u5728AI\u6848\u4f8b\u7814\u7a76\u4e2d\uff0cCEO-DC\u6a21\u578b\u663e\u793a\uff0c\u57284\u5e74\u5468\u671f\u5185\u5347\u7ea7\u9057\u7559\u8bbe\u5907\u53ef\u4ee5\u51cf\u5c11\u603b\u6392\u653e\u91cf\u3002\u7136\u800c\uff0c\u5982\u679c\u4e0d\u589e\u52a0\u603b\u6392\u653e\u91cf\uff0c\u8fd9\u4e9b\u5347\u7ea7\u65e0\u6cd5\u6ee1\u8db3\u6570\u636e\u4e2d\u5fc3\u9700\u6c42\u7684\u589e\u957f\u8d8b\u52bf\uff0c\u5e76\u4e14\u572872%\u7684\u60c5\u51b5\u4e0b\u9700\u8981\u7ecf\u6d4e\u6fc0\u52b1\u63aa\u65bd\u624d\u80fd\u88ab\u91c7\u7528\u3002\u6b64\u5916\uff0c\u572814\u4e2a\u6570\u636e\u4e2d\u5fc3\u6570\u91cf\u6700\u591a\u7684\u56fd\u5bb6\u4e2d\uff0c\u67099\u4e2a\u56fd\u5bb6\u7684\u5f53\u524d\u78b3\u4ef7\u4e0d\u8db3\u4ee5\u6fc0\u52b1\u5347\u7ea7\u3002\u5c06\u5e73\u53f0\u80fd\u6548\u4f18\u5316\u800c\u727a\u7272\u5ef6\u8fdf\u4f1a\u589e\u52a0\u5176\u91c7\u7528\u7684\u5408\u7406\u6027\u6240\u9700\u7684\u78b3\u4ef7\u3002", "conclusion": "\u6570\u636e\u4e2d\u5fc3\u5e73\u53f0\u5347\u7ea7\u548c\u589e\u957f\u9700\u8981\u6709\u9488\u5bf9\u6027\u7684\u653f\u7b56\u548c\u7ecf\u6d4e\u6fc0\u52b1\u63aa\u65bd\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u53d1\u5c55\u3002"}}
{"id": "2507.09976", "categories": ["physics.app-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09976", "abs": "https://arxiv.org/abs/2507.09976", "authors": ["Namkyung Lee", "Seungwon Jung", "Baeksan Jang", "Sangwook Ha", "Joonho Jang"], "title": "Cryogen-free variable-temperature Kelvin probe force microscopy for probing local chemical potential in a graphene heterostructure", "comment": "16 pages, 9 figures", "summary": "We report the development of a variable-temperature Kelvin probe force\nmicroscopy (KPFM) system capable of stable and highly sensitive operation over\na wide temperature range based on a GM-cooler-based cryogen-free cryostat. The\nsystem incorporates a custom-designed phase-locked loop and automatic gain\ncontrol, along with passive vibration isolation, enabling precise measurements\nof local chemical potential even under cryogenic conditions. We demonstrate the\nperformance of this setup by measuring hBN encapsulated monolayer graphene\n(MLG), revealing spatially resolved electronic inhomogeneities and charge\npuddles. Our measurements clearly capture temperature-dependent variations in\nthe chemical potential near the charge neutrality point (CNP), consistent with\nthe linear band dispersion of MLG and interaction-driven renormalization of\nFermi velocity. This work highlights the robust sensitivity and stability of\nour system, making it a versatile local probe of quantum phases in van der\nWaals heterostructures.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u53ef\u53d8\u6e29\u5ea6KPFM\u7cfb\u7edf\uff0c\u53ef\u5728\u4f4e\u6e29\u4e0b\u7a33\u5b9a\u8fd0\u884c\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u6d4b\u91cf\u5355\u5c42\u77f3\u58a8\u70ef\u7684\u7535\u5b50\u4e0d\u5747\u5300\u6027\u548c\u7535\u8377\u3084\uff0c\u63ed\u793a\u4e86\u5176\u5316\u5b66\u52bf\u968f\u6e29\u5ea6\u7684\u53d8\u5316\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5bf9\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u4e2d\u91cf\u5b50\u76f8\u8fdb\u884c\u7814\u7a76\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u5bbd\u6e29\u5ea6\u8303\u56f4\u5185\u7a33\u5b9a\u4e14\u9ad8\u7075\u654f\u5ea6\u8fd0\u884c\u7684KPFM\u7cfb\u7edf\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8eGM\u51b7\u5374\u5668\u65e0\u4f4e\u6e29\u6052\u5316\u5668\u7684\u53ef\u53d8\u6e29\u5ea6\u5f00\u5c14\u6587\u63a2\u9488\u663e\u5fae\u955c\uff08KPFM\uff09\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u5b9a\u5236\u8bbe\u8ba1\u7684\u9501\u76f8\u73af\u548c\u81ea\u52a8\u589e\u76ca\u63a7\u5236\uff0c\u5e76\u5e26\u6709\u88ab\u52a8\u632f\u52a8\u9694\u79bb\uff0c\u5b9e\u73b0\u4e86\u5728\u4f4e\u6e29\u6761\u4ef6\u4e0b\u7cbe\u786e\u6d4b\u91cf\u5c40\u90e8\u5316\u5b66\u52bf\u7684\u80fd\u529b\u3002", "result": "\u901a\u8fc7\u6d4b\u91cfhBN\u5c01\u88c5\u7684\u5355\u5c42\u77f3\u58a8\u70ef\uff08MLG\uff09\uff0c\u63ed\u793a\u4e86\u7a7a\u95f4\u5206\u8fa8\u7684\u7535\u5b50\u4e0d\u5747\u5300\u6027\u548c\u7535\u8377\u3084\u3002\u8be5\u6d4b\u91cf\u6e05\u6670\u5730\u6355\u6349\u5230\u4e86\u7535\u8377\u4e2d\u6027\u70b9\uff08CNP\uff09\u9644\u8fd1\u5316\u5b66\u52bf\u968f\u6e29\u5ea6\u7684\u53d8\u5316\uff0c\u8fd9\u4e0eMLG\u7684\u7ebf\u6027\u80fd\u5e26\u8272\u6563\u548c\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u8d39\u7c73\u901f\u5ea6\u91cd\u6574\u5316\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u4f4e\u6e29\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u7075\u654f\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u4f7f\u5176\u6210\u4e3a\u7814\u7a76\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u4e2d\u91cf\u5b50\u76f8\u7684\u901a\u7528\u5c40\u90e8\u63a2\u9488\u3002"}}
{"id": "2507.08849", "categories": ["eess.SY", "cs.LG", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.08849", "abs": "https://arxiv.org/abs/2507.08849", "authors": ["Emilio Carrizosa", "Martina Fischetti", "Roshell Haaker", "Juan Miguel Morales"], "title": "Counterfactual optimization for fault prevention in complex wind energy systems", "comment": null, "summary": "Machine Learning models are increasingly used in businesses to detect faults\nand anomalies in complex systems. In this work, we take this approach a step\nfurther: beyond merely detecting anomalies, we aim to identify the optimal\ncontrol strategy that restores the system to a safe state with minimal\ndisruption. We frame this challenge as a counterfactual problem: given a\nMachine Learning model that classifies system states as either good or\nanomalous, our goal is to determine the minimal adjustment to the system's\ncontrol variables (i.e., its current status) that is necessary to return it to\nthe good state. To achieve this, we leverage a mathematical model that finds\nthe optimal counterfactual solution while respecting system specific\nconstraints. Notably, most counterfactual analysis in the literature focuses on\nindividual cases where a person seeks to alter their status relative to a\ndecision made by a classifier, such as for loan approval or medical diagnosis.\nOur work addresses a fundamentally different challenge: optimizing\ncounterfactuals for a complex energy system, specifically an offshore wind\nturbine oil type transformer. This application not only advances counterfactual\noptimization in a new domain but also opens avenues for broader research in\nthis area. Our tests on real world data provided by our industrial partner show\nthat our methodology easily adapts to user preferences and brings savings in\nthe order of 3 million euros per year in a typical farm.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u53cd\u4e8b\u5b9e\u65b9\u6cd5\u6765\u4f18\u5316\u590d\u6742\u80fd\u6e90\u7cfb\u7edf\u7684\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u5316\u5e72\u6270\u5e76\u6062\u590d\u5b89\u5168\u72b6\u6001\uff0c\u5df2\u5728\u6d77\u4e0a\u98ce\u529b\u6da1\u8f6e\u673a\u6570\u636e\u4e0a\u9a8c\u8bc1\u5e76\u5b9e\u73b0\u663e\u8457\u7684\u6210\u672c\u8282\u7ea6\u3002", "motivation": "\u65e8\u5728\u8bc6\u522b\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u7684\u5e72\u6270\u5c06\u7cfb\u7edf\u6062\u590d\u5230\u5b89\u5168\u72b6\u6001\uff0c\u8d85\u8d8a\u4e86\u5355\u7eaf\u7684\u5f02\u5e38\u68c0\u6d4b\u3002", "method": "\u5229\u7528\u4e00\u4e2a\u6570\u5b66\u6a21\u578b\u6765\u5bfb\u627e\u6700\u4f18\u7684\u53cd\u4e8b\u5b9e\u89e3\u51b3\u65b9\u6848\uff0c\u540c\u65f6\u9075\u5b88\u7279\u5b9a\u4e8e\u7cfb\u7edf\u7684\u7ea6\u675f\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9e\u9645\u7684\u5de5\u4e1a\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8868\u660e\u5176\u80fd\u591f\u8f7b\u677e\u9002\u5e94\u7528\u6237\u504f\u597d\uff0c\u5e76\u4e3a\u5178\u578b\u7684\u98ce\u7535\u573a\u6bcf\u5e74\u5e26\u6765\u7ea6300\u4e07\u6b27\u5143\u7684\u8282\u7ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u53cd\u4e8b\u5b9e\u5206\u6790\u5e94\u7528\u4e8e\u590d\u6742\u80fd\u6e90\u7cfb\u7edf\uff08\u5982\u6d77\u4e0a\u98ce\u529b\u6da1\u8f6e\u673a\u6cb9\u6d78\u5f0f\u53d8\u538b\u5668\uff09\u4ee5\u4f18\u5316\u63a7\u5236\u7b56\u7565\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u4ee5\u6700\u5c0f\u7684\u5e72\u6270\u5c06\u7cfb\u7edf\u6062\u590d\u5230\u5b89\u5168\u72b6\u6001\u3002"}}
{"id": "2507.08851", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08851", "abs": "https://arxiv.org/abs/2507.08851", "authors": ["Simon Schwaiger", "Stefan Thalhammer", "Wilfried W\u00f6ber", "Gerald Steinbauer-Wagner"], "title": "OTAS: Open-vocabulary Token Alignment for Outdoor Segmentation", "comment": null, "summary": "Understanding open-world semantics is critical for robotic planning and\ncontrol, particularly in unstructured outdoor environments. Current\nvision-language mapping approaches rely on object-centric segmentation priors,\nwhich often fail outdoors due to semantic ambiguities and indistinct semantic\nclass boundaries. We propose OTAS - an Open-vocabulary Token Alignment method\nfor Outdoor Segmentation. OTAS overcomes the limitations of open-vocabulary\nsegmentation models by extracting semantic structure directly from the output\ntokens of pretrained vision models. By clustering semantically similar\nstructures across single and multiple views and grounding them in language,\nOTAS reconstructs a geometrically consistent feature field that supports\nopen-vocabulary segmentation queries. Our method operates zero-shot, without\nscene-specific fine-tuning, and runs at up to ~17 fps. OTAS provides a minor\nIoU improvement over fine-tuned and open-vocabulary 2D segmentation methods on\nthe Off-Road Freespace Detection dataset. Our model achieves up to a 151% IoU\nimprovement over open-vocabulary mapping methods in 3D segmentation on\nTartanAir. Real-world reconstructions demonstrate OTAS' applicability to\nrobotic applications. The code and ROS node will be made publicly available\nupon paper acceptance.", "AI": {"tldr": "OTAS \u662f\u4e00\u79cd\u65b0\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ece\u89c6\u89c9\u6a21\u578b\u4ee4\u724c\u4e2d\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\uff0c\u514b\u670d\u4e86\u5ba4\u5916\u5206\u5272\u7684\u6311\u6218\uff0c\u5e76\u5728 3D \u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9-\u8bed\u8a00\u6620\u5c04\u65b9\u6cd5\u5728\u5ba4\u5916\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u9762\u4e34\u8bed\u4e49\u6a21\u7cca\u548c\u7c7b\u522b\u8fb9\u754c\u4e0d\u6e05\u6670\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u5b83\u4eec\u4f9d\u8d56\u4e8e\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u5206\u5272\u5148\u9a8c\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u66f4\u597d\u5730\u7406\u89e3\u5f00\u653e\u4e16\u754c\u8bed\u4e49\uff0c\u4ee5\u652f\u6301\u673a\u5668\u4eba\u7684\u89c4\u5212\u548c\u63a7\u5236\u3002", "method": "OTAS\uff08Open-vocabulary Token Alignment\uff09\u662f\u4e00\u79cd\u65b0\u7684\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u65b9\u6cd5\uff0c\u901a\u8fc7\u76f4\u63a5\u4ece\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8f93\u51fa\u4ee4\u724c\u4e2d\u63d0\u53d6\u8bed\u4e49\u7ed3\u6784\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u5b83\u901a\u8fc7\u805a\u7c7b\u8bed\u4e49\u76f8\u4f3c\u7684\u7ed3\u6784\u5e76\u5c06\u5176\u4e0e\u8bed\u8a00\u5173\u8054\uff0c\u91cd\u6784\u51fa\u51e0\u4f55\u4e00\u81f4\u7684\u7279\u5f81\u573a\uff0c\u652f\u6301\u5f00\u653e\u8bcd\u6c47\u5206\u5272\u67e5\u8be2\uff0c\u4e14\u65e0\u9700\u9488\u5bf9\u7279\u5b9a\u573a\u666f\u8fdb\u884c\u5fae\u8c03\uff0c\u8fd0\u884c\u901f\u5ea6\u53ef\u8fbe\u7ea6 17 \u5e27/\u79d2\u3002", "result": "OTAS \u5728 Off-Road Freespace Detection \u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u8f83\u4e8e\u7ecf\u8fc7\u5fae\u8c03\u548c\u5f00\u653e\u8bcd\u6c47\u7684 2D \u5206\u5272\u65b9\u6cd5\uff0cIoU \u7565\u6709\u63d0\u5347\uff1b\u5728 TartanAir \u6570\u636e\u96c6\u4e0a\uff0cOTAS \u5728 3D \u5206\u5272\u4efb\u52a1\u4e0a\u7684 IoU \u63d0\u5347\u9ad8\u8fbe 151%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u5f00\u653e\u8bcd\u6c47\u6620\u5c04\u65b9\u6cd5\u3002", "conclusion": "OTAS \u65b9\u6cd5\u5728 TartanAir \u6570\u636e\u96c6\u7684 3D \u5206\u5272\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8fc7\u73b0\u6709\u5f00\u653e\u8bcd\u6c47\u6620\u5c04\u65b9\u6cd5\u7684 151% \u7684 IoU \u63d0\u5347\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u91cd\u5efa\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09055", "categories": ["cs.SI", "cs.IR", "physics.soc-ph", "H.3.3"], "pdf": "https://arxiv.org/pdf/2507.09055", "abs": "https://arxiv.org/abs/2507.09055", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Analysing Health Misinformation with Advanced Centrality Metrics in Online Social Networks", "comment": "10 Pages, 2 figures, 3 tables, journal article in PLOS Digital Health\n  (2025)", "summary": "The rapid spread of health misinformation on online social networks (OSNs)\nduring global crises such as the COVID-19 pandemic poses challenges to public\nhealth, social stability, and institutional trust. Centrality metrics have long\nbeen pivotal in understanding the dynamics of information flow, particularly in\nthe context of health misinformation. However, the increasing complexity and\ndynamism of online networks, especially during crises, highlight the\nlimitations of these traditional approaches. This study introduces and compares\nthree novel centrality metrics: dynamic influence centrality (DIC), health\nmisinformation vulnerability centrality (MVC), and propagation centrality (PC).\nThese metrics incorporate temporal dynamics, susceptibility, and multilayered\nnetwork interactions. Using the FibVID dataset, we compared traditional and\nnovel metrics to identify influential nodes, propagation pathways, and\nmisinformation influencers. Traditional metrics identified 29 influential\nnodes, while the new metrics uncovered 24 unique nodes, resulting in 42\ncombined nodes, an increase of 44.83%. Baseline interventions reduced health\nmisinformation by 50%, while incorporating the new metrics increased this to\n62.5%, an improvement of 25%. To evaluate the broader applicability of the\nproposed metrics, we validated our framework on a second dataset, Monant\nMedical Misinformation, which covers a diverse range of health misinformation\ndiscussions beyond COVID-19. The results confirmed that the advanced metrics\ngeneralised successfully, identifying distinct influential actors not captured\nby traditional methods. In general, the findings suggest that a combination of\ntraditional and novel centrality measures offers a more robust and\ngeneralisable framework for understanding and mitigating the spread of health\nmisinformation in different online network contexts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e09\u79cd\u65b0\u7684\u4e2d\u5fc3\u6027\u6307\u6807\uff08DIC\u3001MVC\u3001PC\uff09\uff0c\u7ed3\u5408\u65f6\u95f4\u52a8\u6001\u3001\u6613\u611f\u6027\u548c\u591a\u5c42\u7f51\u7edc\u4ea4\u4e92\uff0c\u4ee5\u66f4\u6709\u6548\u5730\u8bc6\u522b\u548c\u5e94\u5bf9\u5728\u7ebf\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\u3002\u4e0e\u4f20\u7edf\u6307\u6807\u76f8\u6bd4\uff0c\u65b0\u6307\u6807\u80fd\u8bc6\u522b\u66f4\u591a\u6709\u5f71\u54cd\u529b\u7684\u8282\u70b9\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u5e72\u9884\u63aa\u65bd\u7684\u6548\u679c\u3002", "motivation": "\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\u4e0a\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u7684\u5feb\u901f\u4f20\u64ad\u5bf9\u516c\u4f17\u5065\u5eb7\u3001\u793e\u4f1a\u7a33\u5b9a\u548c\u5236\u5ea6\u4fe1\u4efb\u6784\u6210\u4e86\u6311\u6218\u3002\u4f20\u7edf\u7684\u4e2d\u5fc3\u6027\u6307\u6807\u5728\u7406\u89e3\u5371\u673a\u671f\u95f4\u5728\u7ebf\u7f51\u7edc\u7684\u590d\u6742\u6027\u548c\u52a8\u6001\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u4f7f\u7528 FibVID \u548c Monant Medical Misinformation \u6570\u636e\u96c6\uff0c\u5bf9\u4f20\u7edf\u4e2d\u5fc3\u6027\u6307\u6807\u548c\u65b0\u63d0\u51fa\u7684 DIC\u3001MVC\u3001PC \u6307\u6807\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u4ee5\u8bc6\u522b\u6709\u5f71\u54cd\u529b\u7684\u8282\u70b9\u3001\u4f20\u64ad\u9014\u5f84\u548c\u9519\u8bef\u4fe1\u606f\u5f71\u54cd\u8005\u3002", "result": "\u65b0\u6307\u6807\u8bc6\u522b\u51fa\u7684\u6709\u5f71\u54cd\u529b\u7684\u8282\u70b9\u6bd4\u4f20\u7edf\u6307\u6807\u591a 44.83%\u3002\u57fa\u7ebf\u5e72\u9884\u63aa\u65bd\u5c06\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u51cf\u5c11\u4e86 50%\uff0c\u800c\u7ed3\u5408\u65b0\u6307\u6807\u7684\u5e72\u9884\u63aa\u65bd\u5219\u5c06\u9519\u8bef\u4fe1\u606f\u51cf\u5c11\u4e86 62.5%\u3002\u6240\u63d0\u51fa\u7684\u6307\u6807\u5728\u7b2c\u4e8c\u4e2a\u6570\u636e\u96c6\u4e0a\u5f97\u5230\u4e86\u6210\u529f\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u4e86\u5176\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u65b0\u9896\u7684\u4e2d\u5fc3\u6027\u6307\u6807\uff1a\u52a8\u6001\u5f71\u54cd\u4e2d\u5fc3\u6027\uff08DIC\uff09\u3001\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u8106\u5f31\u6027\u4e2d\u5fc3\u6027\uff08MVC\uff09\u548c\u4f20\u64ad\u4e2d\u5fc3\u6027\uff08PC\uff09\uff0c\u5e76\u7ed3\u5408\u4e86\u65f6\u95f4\u52a8\u6001\u3001\u6613\u611f\u6027\u548c\u591a\u5c42\u7f51\u7edc\u4ea4\u4e92\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7ed3\u5408\u4f7f\u7528\u4f20\u7edf\u548c\u65b0\u9896\u7684\u4e2d\u5fc3\u6027\u6307\u6807\uff0c\u53ef\u4ee5\u66f4\u5168\u9762\u3001\u66f4\u5e7f\u6cdb\u5730\u7406\u89e3\u548c\u51cf\u8f7b\u5728\u7ebf\u7f51\u7edc\u73af\u5883\u4e2d\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u7684\u4f20\u64ad\u3002"}}
{"id": "2507.08831", "categories": ["cs.CV", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08831", "abs": "https://arxiv.org/abs/2507.08831", "authors": ["Josh Qixuan Sun", "Xiaoying Xing", "Huaiyuan Weng", "Chul Min Yeum", "Mark Crowley"], "title": "View Invariant Learning for Vision-Language Navigation in Continuous Environments", "comment": "Under review", "summary": "Vision-Language Navigation in Continuous Environments (VLNCE), where an agent\nfollows instructions and moves freely to reach a destination, is a key research\nproblem in embodied AI. However, most navigation policies are sensitive to\nviewpoint changes, i.e., variations in camera height and viewing angle that\nalter the agent's observation. In this paper, we introduce a generalized\nscenario, V2-VLNCE (VLNCE with Varied Viewpoints), and propose VIL (View\nInvariant Learning), a view-invariant post-training strategy that enhances the\nrobustness of existing navigation policies to changes in camera viewpoint. VIL\nemploys a contrastive learning framework to learn sparse and view-invariant\nfeatures. Additionally, we introduce a teacher-student framework for the\nWaypoint Predictor Module, a core component of most VLNCE baselines, where a\nview-dependent teacher model distills knowledge into a view-invariant student\nmodel. We employ an end-to-end training paradigm to jointly optimize these\ncomponents, thus eliminating the cost for individual module training. Empirical\nresults show that our method outperforms state-of-the-art approaches on\nV2-VLNCE by 8-15% measured on Success Rate for two standard benchmark datasets\nR2R-CE and RxR-CE. Furthermore, we evaluate VIL under the standard VLNCE\nsetting and find that, despite being trained for varied viewpoints, it often\nstill improves performance. On the more challenging RxR-CE dataset, our method\nalso achieved state-of-the-art performance across all metrics when compared to\nother map-free methods. This suggests that adding VIL does not diminish the\nstandard viewpoint performance and can serve as a plug-and-play post-training\nmethod.", "AI": {"tldr": "This paper introduces VIL, a post-training strategy that makes embodied AI agents more robust to changes in camera viewpoints during navigation. VIL uses contrastive learning and a teacher-student approach to improve performance on varied viewpoint scenarios and even enhances standard navigation tasks, achieving state-of-the-art results.", "motivation": "Existing navigation policies in Vision-Language Navigation in Continuous Environments (VLNCE) are sensitive to viewpoint changes (variations in camera height and viewing angle), which alter the agent's observation. To address this, a generalized scenario V2-VLNCE (VLNCE with Varied Viewpoints) was introduced, and VIL was proposed to enhance robustness to these changes.", "method": "VIL (View Invariant Learning) is a view-invariant post-training strategy that uses a contrastive learning framework to learn sparse and view-invariant features. It also incorporates a teacher-student framework for the Waypoint Predictor Module, where a view-dependent teacher model distills knowledge into a view-invariant student model. The components are jointly optimized using an end-to-end training paradigm.", "result": "VIL outperforms state-of-the-art approaches on V2-VLNCE by 8-15% (measured on Success Rate) on R2R-CE and RxR-CE datasets. It also achieves state-of-the-art performance across all metrics compared to other map-free methods on the RxR-CE dataset. VIL improves performance even under the standard VLNCE setting, indicating it can be a plug-and-play method.", "conclusion": "The proposed VIL method enhances the robustness of existing navigation policies to changes in camera viewpoint by employing a contrastive learning framework and a teacher-student framework for the Waypoint Predictor Module. It outperforms state-of-the-art approaches on V2-VLNCE and achieves state-of-the-art performance on RxR-CE, suggesting it can be a plug-and-play post-training method without diminishing standard viewpoint performance."}}
{"id": "2507.08954", "categories": ["cs.DC", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.08954", "abs": "https://arxiv.org/abs/2507.08954", "authors": ["Alexander Fuerst", "Siddharth Anil", "Vishakha Dixit", "Purushottam", "Kulkarni", "Prateek Sharma"], "title": "MQFQ-Sticky: Fair Queueing For Serverless GPU Functions", "comment": null, "summary": "Hardware accelerators like GPUs are now ubiquitous in data centers, but are\nnot fully supported by common cloud abstractions such as Functions as a Service\n(FaaS). Many popular and emerging FaaS applications such as machine learning\nand scientific computing can benefit from GPU acceleration. However, FaaS\nframeworks (such as OpenWhisk) are not capable of providing this acceleration\nbecause of the impedance mismatch between GPUs and the FaaS programming model,\nwhich requires virtualization and sandboxing of each function. The challenges\nare amplified due to the highly dynamic and heterogeneous FaaS workloads. This\npaper presents the design and implementation of a FaaS system for providing GPU\nacceleration in a black-box manner (without modifying function code). Running\nsmall functions in containerized sandboxes is challenging due to limited GPU\nconcurrency and high cold-start overheads, resulting in heavy queueing of\nfunction invocations. We show how principles from I/O scheduling, such as fair\nqueuing and anticipatory scheduling, can be translated to function scheduling\non GPUs. We develop MQFQ-Sticky, an integrated fair queueing and GPU memory\nmanagement approach, which balances the tradeoffs between locality, fairness,\nand latency. Empirical evaluation on a range of workloads shows that it reduces\nfunction latency by 2x to 20x compared to existing GPU and CPU queueing\npolicies.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 FaaS \u7cfb\u7edf\uff0c\u901a\u8fc7\u5f15\u5165\u5148\u8fdb\u7684\u8c03\u5ea6\u548c\u5185\u5b58\u7ba1\u7406\u6280\u672f\uff08MQFQ-Sticky\uff09\uff0c\u5b9e\u73b0\u4e86\u5bf9 GPU \u7684\u9ad8\u6548\u652f\u6301\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u51fd\u6570\u5ef6\u8fdf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709 FaaS \u6846\u67b6\u5728\u5904\u7406\u9700\u8981 GPU \u52a0\u901f\u7684\u4efb\u52a1\u65f6\u7684\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u968f\u7740 GPU \u5728\u6570\u636e\u4e2d\u5fc3\u4e2d\u7684\u666e\u53ca\uff0c\u8bb8\u591a\u9700\u8981 GPU \u52a0\u901f\u7684\u5e94\u7528\uff08\u5982\u673a\u5668\u5b66\u4e60\u3001\u79d1\u5b66\u8ba1\u7b97\uff09\u5728\u73b0\u6709\u7684 FaaS \u6846\u67b6\uff08\u5982 OpenWhisk\uff09\u4e2d\u65e0\u6cd5\u5f97\u5230\u5145\u5206\u652f\u6301\u3002\u8fd9\u662f\u7531\u4e8e GPU \u4e0e FaaS \u7684\u7f16\u7a0b\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\uff0c\u7279\u522b\u662f FaaS \u6240\u8981\u6c42\u7684\u865a\u62df\u5316\u548c\u6c99\u76d2\u673a\u5236\uff0c\u4ee5\u53ca FaaS \u5e94\u7528\u52a8\u6001\u548c\u5f02\u6784\u7684\u7279\u70b9\uff0c\u653e\u5927\u4e86\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MQFQ-Sticky \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u501f\u9274\u4e86 I/O \u8c03\u5ea6\u7684\u516c\u5e73\u6392\u961f\u548c\u9884\u671f\u8c03\u5ea6\u539f\u5219\uff0c\u5e76\u7ed3\u5408\u4e86 GPU \u5185\u5b58\u7ba1\u7406\uff0c\u4ee5\u89e3\u51b3 FaaS \u6846\u67b6\u5728\u652f\u6301 GPU \u52a0\u901f\u65f6\u9762\u4e34\u7684\u52a8\u6001\u6027\u3001\u5f02\u6784\u6027\u4ee5\u53ca\u5bb9\u5668\u5316\u6c99\u7bb1\u5e26\u6765\u7684 GPU \u5e76\u53d1\u6027\u9650\u5236\u548c\u51b7\u542f\u52a8\u5f00\u9500\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5728\u591a\u79cd\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\u7684\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u8868\u660e\u8be5\u7cfb\u7edf\u76f8\u6bd4\u73b0\u6709\u7684 GPU \u548c CPU \u6392\u961f\u7b56\u7565\uff0c\u80fd\u591f\u5c06\u51fd\u6570\u5ef6\u8fdf\u964d\u4f4e 2 \u5230 20 \u500d\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u63d0\u4f9b GPU \u52a0\u901f\u7684 FaaS \u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4ee5\u9ed1\u76d2\u65b9\u5f0f\u8fd0\u884c\uff08\u65e0\u9700\u4fee\u6539\u51fd\u6570\u4ee3\u7801\uff09\uff0c\u5e76\u91c7\u7528\u4e86 MQFQ-Sticky \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86 I/O \u8c03\u5ea6\u539f\u5219\u548c GPU \u5185\u5b58\u7ba1\u7406\uff0c\u4ee5\u5e73\u8861\u5c40\u90e8\u6027\u3001\u516c\u5e73\u6027\u548c\u5ef6\u8fdf\u6027\u3002"}}
{"id": "2507.08951", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.08951", "abs": "https://arxiv.org/abs/2507.08951", "authors": ["Rajendra Kumar", "Govardan Gopakumar", "Zain Ul Abdin", "Michael J. Manfra", "Oana Malis"], "title": "Structural optimization of lattice-matched Sc0.14Al0.86N/GaN superlattices for photonic applications", "comment": null, "summary": "ScxAl1-xN is an emerging III-nitride material known for its high\npiezoelectric coefficient and ferroelectric properties. Integration of\nwide-bandgap ScxAl1-xN with GaN is particularly attractive for quantum photonic\ndevices. Achieving low defect complex multilayers incorporating ScxAl1-xN,\nthough, requires precise lattice-matching and carefully optimized growth\nparameters. This study systematically investigates the molecular-beam epitaxy\nof short-period ScxAl1-xN/GaN superlattices with total thicknesses of up to 600\nnm on GaN templates. X-ray diffraction reciprocal space mapping confirmed\nlattice-matching at x = 0.14 Sc composition regardless of the thickness of GaN\ninterlayers, as evidenced by symmetric superlattice satellites aligned in-plane\nwith the underlying substrate peak. Superlattices with Sc compositions\ndeviating from this lattice-matching condition exhibited strain-induced defects\nranging from crack formation to partial relaxation. Scanning transmission\nelectron microscopy (STEM) investigation of the ScxAl1-xN/GaN interfaces\nidentified temperature-dependent intermixing as a major factor in setting the\nnitride composition variation and implicitly band structure profile along the\ngrowth direction. Energy-dispersive X-ray spectroscopy also revealed that Sc\nincorporation exhibits delays relative to Al at both onset and termination.\nOptimal growth conditions were observed at approximately 600{\\deg}C and\n550{\\deg}C for superlattices with thick GaN layers (6 nm), and ultra-thin GaN\nlayers (< 2 nm), respectively.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86 ScAl1-xN/GaN \u8d85\u6676\u683c\u7684\u5206\u5b50\u675f\u5916\u5ef6\u751f\u957f\uff0c\u786e\u5b9a\u4e86\u5b9e\u73b0\u6676\u683c\u5339\u914d\u7684\u6700\u4f73 Sc \u7ec4\u5206\u4e3a x = 0.14\uff0c\u5e76\u4f18\u5316\u4e86\u751f\u957f\u6e29\u5ea6\u548c\u6761\u4ef6\uff0c\u4e3a\u91cf\u5b50\u5149\u5b50\u5668\u4ef6\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "ScAl1-xN \u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684 III \u65cf\u6c2e\u5316\u7269\u6750\u6599\uff0c\u56e0\u5176\u9ad8\u538b\u7535\u7cfb\u6570\u548c\u94c1\u7535\u7279\u6027\uff0c\u5728\u91cf\u5b50\u5149\u5b50\u5668\u4ef6\u9886\u57df\u5177\u6709\u91cd\u8981\u7684\u5e94\u7528\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5b9e\u73b0\u4f4e\u7f3a\u9677 ScAl1-xN \u7684\u96c6\u6210\u9700\u8981\u7cbe\u786e\u7684\u6676\u683c\u5339\u914d\u548c\u4f18\u5316\u7684\u751f\u957f\u53c2\u6570\u3002", "method": "\u901a\u8fc7\u5206\u5b50\u675f\u5916\u5ef6\u6280\u672f\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4e0d\u540c Sc \u7ec4\u5206\u548c\u4e0d\u540c GaN \u95f4\u9694\u5c42\u539a\u5ea6\u7684 ScAl1-xN/GaN \u8d85\u6676\u683c\u7684\u751f\u957f\uff0c\u5e76\u5229\u7528 X \u5c04\u7ebf\u884d\u5c04\u5012\u6613\u7a7a\u95f4\u6620\u5c04\u3001\u626b\u63cf\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\u548c\u80fd\u91cf\u8272\u6563 X \u5c04\u7ebf\u5149\u8c31\u7b49\u6280\u672f\u5bf9\u5176\u7ed3\u6784\u548c\u6210\u5206\u8fdb\u884c\u4e86\u8868\u5f81\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86 Sc \u7ec4\u5206\u4e3a x = 0.14 \u65f6\u53ef\u4ee5\u5b9e\u73b0\u4e0e GaN \u7684\u6676\u683c\u5339\u914d\uff0c\u5e76\u4e14\u5728\u9ad8\u8fbe 600 nm \u7684\u603b\u539a\u5ea6\u4e0b\u4ecd\u80fd\u4fdd\u6301\u826f\u597d\u3002\u7136\u800c\uff0c\u5f53 Sc \u7ec4\u5206\u504f\u79bb\u6b64\u503c\u65f6\uff0c\u4f1a\u51fa\u73b0\u5e94\u53d8\u8bf1\u5bfc\u7684\u7f3a\u9677\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6e29\u5ea6\u4f9d\u8d56\u7684\u6df7\u5c42\u662f\u5f71\u54cd\u6c2e\u5316\u7269\u6210\u5206\u53d8\u5316\u548c\u5e26\u7ed3\u6784\u7684\u91cd\u8981\u56e0\u7d20\uff0c\u5e76\u4e14 Sc \u7684\u63ba\u5165\u76f8\u5bf9\u4e8e Al \u5b58\u5728\u5ef6\u8fdf\u3002\u6700\u7ec8\u786e\u5b9a\u4e86\u9002\u7528\u4e8e\u4e0d\u540c\u539a\u5ea6 GaN \u5c42\u7684\u6700\u4f73\u751f\u957f\u6e29\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u7cfb\u7edf\u7814\u7a76\u548c\u4f18\u5316\u751f\u957f\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684 ScAl1-xN/GaN \u8d85\u6676\u683c\u7684\u5206\u5b50\u675f\u5916\u5ef6\u751f\u957f\uff0c\u5e76\u786e\u5b9a\u4e86\u6700\u4f73\u751f\u957f\u6761\u4ef6\uff0c\u4e3a\u91cf\u5b50\u5149\u5b50\u5668\u4ef6\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09385", "categories": ["cs.NE", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09385", "abs": "https://arxiv.org/abs/2507.09385", "authors": ["Kevin Reyes", "Vasco Cortez"], "title": "Credit Card Fraud Detection Using RoFormer Model With Relative Distance Rotating Encoding", "comment": "2025 IEEE Conference on Artificial Intelligence (CAI)", "summary": "Fraud detection is one of the most important challenges that financial\nsystems must address. Detecting fraudulent transactions is critical for payment\ngateway companies like Flow Payment, which process millions of transactions\nmonthly and require robust security measures to mitigate financial risks.\nIncreasing transaction authorization rates while reducing fraud is essential\nfor providing a good user experience and building a sustainable business. For\nthis reason, discovering novel and improved methods to detect fraud requires\ncontinuous research and investment for any company that wants to succeed in\nthis industry. In this work, we introduced a novel method for detecting\ntransactional fraud by incorporating the Relative Distance Rotating Encoding\n(ReDRE) in the RoFormer model. The incorporation of angle rotation using ReDRE\nenhances the characterization of time series data within a Transformer, leading\nto improved fraud detection by better capturing temporal dependencies and event\nrelationships.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u76f8\u5bf9\u8ddd\u79bb\u65cb\u8f6c\u7f16\u7801\uff08ReDRE\uff09\u6574\u5408\u5230RoFormer\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u4ea4\u6613\u6b3a\u8bc8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u91d1\u878d\u7cfb\u7edf\u4e2d\u65e5\u76ca\u4e25\u5cfb\u7684\u6b3a\u8bc8\u68c0\u6d4b\u6311\u6218\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u50cfFlow Payment\u8fd9\u6837\u6bcf\u6708\u5904\u7406\u6570\u767e\u4e07\u7b14\u4ea4\u6613\u5e76\u9700\u8981\u5f3a\u5927\u5b89\u5168\u63aa\u65bd\u6765\u964d\u4f4e\u91d1\u878d\u98ce\u9669\u7684\u516c\u53f8\u3002\u63d0\u9ad8\u4ea4\u6613\u6388\u6743\u7387\u5e76\u51cf\u5c11\u6b3a\u8bc8\u5bf9\u4e8e\u63d0\u4f9b\u826f\u597d\u7684\u7528\u6237\u4f53\u9a8c\u548c\u5efa\u7acb\u53ef\u6301\u7eed\u7684\u4e1a\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u6b64\u9700\u8981\u4e0d\u65ad\u7814\u7a76\u548c\u6295\u8d44\u4ee5\u53d1\u73b0\u65b0\u9896\u548c\u6539\u8fdb\u7684\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u6613\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u76f8\u5bf9\u8ddd\u79bb\u65cb\u8f6c\u7f16\u7801\uff08ReDRE\uff09\u6574\u5408\u5230RoFormer\u6a21\u578b\u4e2d\u3002ReDRE\u901a\u8fc7\u5f15\u5165\u89d2\u5ea6\u65cb\u8f6c\u6765\u589e\u5f3aTransformer\u6a21\u578b\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u8868\u5f81\u80fd\u529b\uff0c\u4ece\u800c\u66f4\u597d\u5730\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u4e8b\u4ef6\u5173\u7cfb\uff0c\u63d0\u9ad8\u6b3a\u8bc8\u68c0\u6d4b\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5c06ReDRE\u6574\u5408\u5230RoFormer\u6a21\u578b\u4e2d\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u4e8b\u4ef6\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4ea4\u6613\u6b3a\u8bc8\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ea4\u6613\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u76f8\u5bf9\u8ddd\u79bb\u65cb\u8f6c\u7f16\u7801\uff08ReDRE\uff09\u7ed3\u5408\u5230RoFormer\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u6b3a\u8bc8\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.08872", "categories": ["quant-ph", "78A25"], "pdf": "https://arxiv.org/pdf/2507.08872", "abs": "https://arxiv.org/abs/2507.08872", "authors": ["Tiemo Pedergnana", "Florian Kogelbauer"], "title": "Relativistic electrodynamics with a universal length scale", "comment": "15 pages, 1 figure", "summary": "We derive the analogues of the Dirac and Pauli equations from a spatially\nfourth-order Klein--Gordon equation with a universal length scale. Starting\nfrom a singularly perturbed variant of Maxwell's equations, we deduce a\n32-dimensional variant of the Dirac equation for spin-$1/2$ particles through\nan algebraic factorization procedure. We illustrate an experimental test of the\ntheory from the split lines of the electron beam in a Stern--Gerlach\nexperiment. This hyperfine splitting leads to four distinct eigenvalues of the\nspin operator, which can be grouped into two pairs centered around the classic\nvalues of $\\pm\\hbar/2$. The modified electrodynamic framework features\nparticle-antiparticle asymmetry and an oriented, micropolar spacetime.", "AI": {"tldr": "\u4ece\u56db\u9636\u514b\u83b1\u56e0-\u6208\u767b\u65b9\u7a0b\u63a8\u5bfc\u51fa\u72c4\u62c9\u514b\u548c\u6ce1\u5229\u65b9\u7a0b\u7684\u7c7b\u4f3c\u7269\uff0c\u5e76\u901a\u8fc7\u65af\u7279\u6069-\u76d6\u62c9\u8d6b\u5b9e\u9a8c\u7684\u7535\u5b50\u675f\u5206\u88c2\u8fdb\u884c\u4e86\u7406\u8bba\u9a8c\u8bc1\u3002", "motivation": "\u4ece\u5177\u6709\u666e\u9002\u957f\u5ea6\u5c3a\u5ea6\u7684\u7a7a\u95f4\u56db\u9636\u514b\u83b1\u56e0-\u6208\u767b\u65b9\u7a0b\u63a8\u5bfc\u51fa\u72c4\u62c9\u514b\u548c\u6ce1\u5229\u65b9\u7a0b\u7684\u7c7b\u4f3c\u7269\u3002", "method": "\u901a\u8fc7\u4ee3\u6570\u5206\u89e3\u7a0b\u5e8f\u4ece\u5947\u5f02\u6444\u52a8\u7684\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\u7ec4\u63a8\u5bfc\u51fa32\u7ef4\u72c4\u62c9\u514b\u65b9\u7a0b\u7684\u53d8\u4f53\u3002", "result": "\u7535\u5b50\u675f\u5728\u65af\u7279\u6069-\u76d6\u62c9\u8d6b\u5b9e\u9a8c\u4e2d\u53d1\u751f\u5206\u88c2\uff0c\u5bfc\u81f4\u81ea\u65cb\u7b97\u7b26\u7684\u56db\u4e2a\u4e0d\u540c\u7279\u5f81\u503c\uff0c\u53ef\u5206\u4e3a\u4e24\u5bf9\uff0c\u4e2d\u5fc3\u503c\u7ea6\u4e3a\u00b1\u0127/2\u3002", "conclusion": "\u8be5\u7406\u8bba\u5177\u6709\u7c92\u5b50-\u53cd\u7c92\u5b50\u4e0d\u5bf9\u79f0\u6027\u548c\u5b9a\u5411\u7684\u5fae\u6781\u6027\u65f6\u7a7a\u3002"}}
{"id": "2507.08865", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08865", "abs": "https://arxiv.org/abs/2507.08865", "authors": ["Javis AI Team", "Amrendra Singh", "Maulik Shah", "Dharshan Sampath"], "title": "Spatial ModernBERT: Spatial-Aware Transformer for Table and Key-Value Extraction in Financial Documents at Scale", "comment": null, "summary": "Extracting tables and key-value pairs from financial documents is essential\nfor business workflows such as auditing, data analytics, and automated invoice\nprocessing. In this work, we introduce Spatial ModernBERT-a transformer-based\nmodel augmented with spatial embeddings-to accurately detect and extract\ntabular data and key-value fields from complex financial documents. We cast the\nextraction task as token classification across three heads: (1) Label Head,\nclassifying each token as a label (e.g., PO Number, PO Date, Item Description,\nQuantity, Base Cost, MRP, etc.); (2) Column Head, predicting column indices;\n(3) Row Head, distinguishing the start of item rows and header rows. The model\nis pretrained on the PubTables-1M dataset, then fine-tuned on a financial\ndocument dataset, achieving robust performance through cross-entropy loss on\neach classification head. We propose a post-processing method to merge tokens\nusing B-I-IB tagging, reconstruct the tabular layout, and extract key-value\npairs. Empirical evaluation shows that Spatial ModernBERT effectively leverages\nboth textual and spatial cues, facilitating highly accurate table and key-value\nextraction in real-world financial documents.", "AI": {"tldr": "Spatial ModernBERT\u901a\u8fc7\u7ed3\u5408\u6587\u672c\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u63d0\u9ad8\u4e86\u4ece\u91d1\u878d\u6587\u6863\u4e2d\u63d0\u53d6\u8868\u683c\u548c\u952e\u503c\u5bf9\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4ece\u91d1\u878d\u6587\u6863\u4e2d\u63d0\u53d6\u8868\u683c\u548c\u952e\u503c\u5bf9\u5bf9\u4e8e\u5ba1\u8ba1\u3001\u6570\u636e\u5206\u6790\u548c\u81ea\u52a8\u5316\u53d1\u7968\u5904\u7406\u7b49\u4e1a\u52a1\u6d41\u7a0b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSpatial ModernBERT\u7684Transformer\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u52a0\u5165\u7a7a\u95f4\u5d4c\u5165\u6765\u589e\u5f3a\uff0c\u5e76\u5c06\u63d0\u53d6\u4efb\u52a1\u89c6\u4e3a\u8de8\u4e09\u4e2a\u5934\u7684\u4ee4\u724c\u5206\u7c7b\u4efb\u52a1\uff1a\u6807\u7b7e\u5934\uff08\u5206\u7c7b\u6bcf\u4e2a\u4ee4\u724c\u7684\u6807\u7b7e\uff09\u3001\u5217\u5934\uff08\u9884\u6d4b\u5217\u7d22\u5f15\uff09\u548c\u884c\u5934\uff08\u533a\u5206\u9879\u76ee\u884c\u548c\u6807\u9898\u884c\u7684\u8d77\u59cb\uff09\u3002\u6a21\u578b\u5728PubTables-1M\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u5728\u91d1\u878d\u6587\u6863\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u4f7f\u7528\u4ea4\u53c9\u71b5\u635f\u5931\u8fdb\u884c\u4f18\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528B-I-IB\u6807\u7b7e\u5408\u5e76\u4ee4\u724c\u3001\u91cd\u5efa\u8868\u683c\u5e03\u5c40\u548c\u63d0\u53d6\u952e\u503c\u5bf9\u7684\u540e\u5904\u7406\u65b9\u6cd5\u3002", "result": "Spatial ModernBERT\u5728\u91d1\u878d\u6587\u6863\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8868\u683c\u548c\u952e\u503c\u5bf9\u63d0\u53d6\u3002", "conclusion": "Spatial ModernBERT\u6a21\u578b\u6709\u6548\u5730\u7ed3\u5408\u4e86\u6587\u672c\u548c\u7a7a\u95f4\u7ebf\u7d22\uff0c\u5b9e\u73b0\u4e86\u5bf9\u771f\u5b9e\u91d1\u878d\u6587\u6863\u4e2d\u8868\u683c\u548c\u952e\u503c\u5bf9\u7684\u9ad8\u7cbe\u5ea6\u63d0\u53d6\u3002"}}
{"id": "2507.08846", "categories": ["cs.GT", "cs.DC", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.08846", "abs": "https://arxiv.org/abs/2507.08846", "authors": ["Serdar Metin"], "title": "Precomputed Dominant Resource Fairness", "comment": "9 pages", "summary": "Although resource allocation is a well studied problem in computer science,\nuntil the prevalence of distributed systems, such as computing clouds and data\ncentres, the question had been addressed predominantly for single resource type\nscenarios. At the beginning of the last decade, with the introuction of\nDominant Resource Fairness, the studies of the resource allocation problem has\nfinally extended to the multiple resource type scenarios. Dominant Resource\nFairness is a solution, addressing the problem of fair allocation of multiple\nresource types, among users with heterogeneous demands. Based on Max-min\nFairness, which is a well established algorithm in the literature for\nallocating resources in the single resource type scenarios, Dominant Resource\nFairness generalises the scheme to the multiple resource case. It has a number\nof desirable properties that makes it preferable over alternatives, such as\nSharing Incentive, Envy-Freeness, Pareto Efficiency, and Strategy Proofness,\nand as such, it is widely adopted in distributed systems. In the present study,\nwe revisit the original study, and analyse the structure of the algorithm in\ncloser view, to come up with an alternative algorithm, which approximates the\nDominant Resource Fairness allocation in fewer steps. We name the new algorithm\nPrecomputed Dominant Resource Fairness, after its main working principle.", "AI": {"tldr": "\u5bf9\u4e3b\u5bfc\u8d44\u6e90\u516c\u5e73\u6027\u7b97\u6cd5\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd1\u4f3c\u7b97\u6cd5\u2014\u2014\u9884\u8ba1\u7b97\u4e3b\u5bfc\u8d44\u6e90\u516c\u5e73\u6027\u3002", "motivation": "\u6269\u5c55\u4e86\u5bf9\u591a\u8d44\u6e90\u7c7b\u578b\u573a\u666f\u4e0b\u7684\u516c\u5e73\u8d44\u6e90\u5206\u914d\u95ee\u9898\u7684\u7814\u7a76\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e3b\u5bfc\u8d44\u6e90\u516c\u5e73\u6027\u7b97\u6cd5\u7684\u7ed3\u6784\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd1\u4f3c\u7b97\u6cd5\u201c\u9884\u8ba1\u7b97\u4e3b\u5bfc\u8d44\u6e90\u516c\u5e73\u6027\u201d\uff0c\u8be5\u7b97\u6cd5\u80fd\u5728\u66f4\u5c11\u7684\u6b65\u9aa4\u4e2d\u5b9e\u73b0\u4e3b\u5bfc\u8d44\u6e90\u516c\u5e73\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u9884\u8ba1\u7b97\u4e3b\u5bfc\u8d44\u6e90\u516c\u5e73\u6027\u201d\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u4ee5\u66f4\u5c11\u7684\u6b65\u9aa4\u8fd1\u4f3c\u4e3b\u5bfc\u8d44\u6e90\u516c\u5e73\u6027\u5206\u914d\u3002"}}
{"id": "2507.08990", "categories": ["cs.LO", "math.AC"], "pdf": "https://arxiv.org/pdf/2507.08990", "abs": "https://arxiv.org/abs/2507.08990", "authors": ["Arka Ghosh", "Aliaume Lopez"], "title": "Computability of Equivariant Gr\u00f6bner bases", "comment": null, "summary": "Let $\\mathbb{K}$ be a field, $\\mathcal{X}$ be an infinite set (of\nindeterminates), and $\\mathcal{G}$ be a group acting on $\\mathcal{X}$. An ideal\nin the polynomial ring $\\mathbb{K}[\\mathcal{X}]$ is called equivariant if it is\ninvariant under the action of $\\mathcal{G}$. We show Gr\\\"obner bases for\nequivariant ideals are computable are hence the equivariant ideal membership is\ndecidable when $\\mathcal{G}$ and $\\mathcal{X}$ satisfies the Hilbert's basis\nproperty, that is, when every equivariant ideal in $\\mathbb{K}[\\mathcal{X}]$ is\nfinitely generated. Moreover, we give a sufficient condition for the\nundecidability of the equivariant ideal membership problem. This condition is\nsatisfied by the most common examples not satisfying the Hilbert's basis\nproperty.", "AI": {"tldr": "This paper explores the decidability of equivariant ideal membership in polynomial rings under group actions. It establishes decidability under the Hilbert's basis property and presents conditions for undecidability in cases where this property doesn't hold.", "motivation": "The motivation is to determine the decidability and computability of the equivariant ideal membership problem, especially for cases where the Hilbert's basis property is not satisfied.", "method": "The paper likely involves theoretical computer science and abstract algebra concepts, possibly using Gr\u00f6bner bases and group actions on polynomial rings to analyze equivariant ideals. The decidability and computability are proven, and a condition for undecidability is given.", "result": "Equivariant ideal membership is decidable when the Hilbert's basis property is satisfied. A sufficient condition for undecidability is also provided.", "conclusion": "The paper shows that Gr\u00f6bner bases for equivariant ideals are computable and the equivariant ideal membership is decidable when G and X satisfy the Hilbert's basis property. It also provides a sufficient condition for the undecidability of the equivariant ideal membership problem, which is satisfied by common examples not satisfying the Hilbert's basis property."}}
{"id": "2507.08821", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08821", "abs": "https://arxiv.org/abs/2507.08821", "authors": ["Pedro D. Alvim", "Hugerles S. Silva", "Ugo S. Dias", "Osamah S. Badarneh", "Felipe A. P. Figueiredo", "Rausley A. A. de Souza"], "title": "LNN-powered Fluid Antenna Multiple Access", "comment": null, "summary": "Fluid antenna systems represent an innovative approach in wireless\ncommunication, recently applied in multiple access to optimize the\nsignal-to-interference-plus-noise ratio through port selection. This letter\nframes the port selection problem as a multi-label classification task for the\nfirst time, improving best-port selection with limited port observations. We\naddress this challenge by leveraging liquid neural networks (LNNs) to predict\nthe optimal port under emerging fluid antenna multiple access scenarios\nalongside a more general $\\alpha$-$\\mu$ fading model. We also apply\nhyperparameter optimization to refine LNN architectures for different\nobservation scenarios. Our approach yields lower outage probability values than\nexisting methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08884", "categories": ["cs.MA", "cs.GR"], "pdf": "https://arxiv.org/pdf/2507.08884", "abs": "https://arxiv.org/abs/2507.08884", "authors": ["Jordan Riley Benson", "David Crist", "Phil Lafleur", "Benjamin Watson"], "title": "Agent-based visualization of streaming text", "comment": null, "summary": "We present a visualization infrastructure that maps data elements to agents,\nwhich have behaviors parameterized by those elements. Dynamic visualizations\nemerge as the agents change position, alter appearance and respond to one\nother. Agents move to minimize the difference between displayed agent-to-agent\ndistances, and an input matrix of ideal distances. Our current application is\nvisualization of streaming text. Each agent represents a significant word,\nvisualizing it by displaying the word itself, centered in a circle sized by the\nfrequency of word occurrence. We derive the ideal distance matrix from word\ncooccurrence, mapping higher co-occurrence to lower distance. To depict\nco-occurrence in its textual context, the ratio of intersection to circle area\napproximates the ratio of word co-occurrence to frequency. A networked backend\nprocess gathers articles from news feeds, blogs, Digg or Twitter, exploiting\nonline search APIs to focus on user-chosen topics. Resulting visuals reveal the\nprimary topics in text streams as clusters, with agent-based layout moving\nwithout instability as data streams change dynamically.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4ee3\u7406\u884c\u4e3a\u6a21\u578b\u7684\u53ef\u89c6\u5316\u57fa\u7840\u67b6\u6784\uff0c\u7528\u4e8e\u52a8\u6001\u53ef\u89c6\u5316\u6587\u672c\u6570\u636e\u6d41\u4e2d\u7684\u5355\u8bcd\u5171\u73b0\u5173\u7cfb\u3002", "motivation": "\u4e3a\u6587\u672c\u6d41\u6570\u636e\u63d0\u4f9b\u4e00\u79cd\u52a8\u6001\u4e14\u76f4\u89c2\u7684\u53ef\u89c6\u5316\u65b9\u6cd5\uff0c\u63ed\u793a\u6570\u636e\u4e2d\u7684\u4e3b\u8981\u8bdd\u9898\u548c\u5171\u73b0\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6570\u636e\u5143\u7d20\u6620\u5c04\u5230\u5177\u6709\u884c\u4e3a\u4ee3\u7406\u7684\u53ef\u89c6\u5316\u57fa\u7840\u67b6\u6784\uff0c\u4ee3\u7406\u7684\u884c\u4e3a\u7531\u6570\u636e\u5143\u7d20\u53c2\u6570\u5316\u3002\u4ee3\u7406\u901a\u8fc7\u6700\u5c0f\u5316\u663e\u793a\u4ee3\u7406\u95f4\u8ddd\u79bb\u4e0e\u7406\u60f3\u8ddd\u79bb\u77e9\u9635\u4e4b\u95f4\u7684\u5dee\u5f02\u6765\u79fb\u52a8\u3002\u5bf9\u4e8e\u6587\u672c\u6d41\u7684\u53ef\u89c6\u5316\uff0c\u4ee3\u7406\u4ee3\u8868\u5355\u8bcd\uff0c\u5176\u5927\u5c0f\u7531\u5355\u8bcd\u9891\u7387\u51b3\u5b9a\uff0c\u5355\u8bcd\u7684\u5171\u73b0\u5173\u7cfb\u51b3\u5b9a\u4e86\u7406\u60f3\u8ddd\u79bb\u77e9\u9635\uff0c\u5171\u73b0\u5ea6\u8d8a\u9ad8\u7684\u5355\u8bcd\u8ddd\u79bb\u8d8a\u8fd1\u3002\u4ee3\u7406\u7684\u5706\u5f62\u533a\u57df\u4e0e\u5355\u8bcd\u5171\u73b0\u7387\u4e4b\u95f4\u7684\u6bd4\u4f8b\u8fd1\u4f3c\u4e8e\u5355\u8bcd\u5171\u73b0\u7387\u4e0e\u9891\u7387\u4e4b\u95f4\u7684\u6bd4\u7387\u3002", "result": "\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u8be5\u57fa\u7840\u67b6\u6784\u53ef\u89c6\u5316\u6d41\u5f0f\u6587\u672c\u6570\u636e\uff0c\u63ed\u793a\u4e86\u6587\u672c\u6d41\u4e2d\u7684\u4e3b\u8981\u8bdd\u9898\u4f5c\u4e3a\u805a\u7c7b\uff0c\u5e76\u4e14\u5e03\u5c40\u80fd\u591f\u968f\u7740\u6570\u636e\u6d41\u52a8\u6001\u53d8\u5316\u800c\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u52a8\u6001\u5730\u5c55\u793a\u6587\u672c\u6570\u636e\u6d41\u4e2d\u7684\u4e3b\u8981\u8bdd\u9898\uff0c\u5e76\u4e14\u5728\u6570\u636e\u6d41\u53d1\u751f\u53d8\u5316\u65f6\u80fd\u4fdd\u6301\u5e03\u5c40\u7a33\u5b9a\u3002"}}
{"id": "2507.08828", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08828", "abs": "https://arxiv.org/abs/2507.08828", "authors": ["Tarek Berghout"], "title": "Recurrent Expansion: A Pathway Toward the Next Generation of Deep Learning", "comment": null, "summary": "This paper introduces Recurrent Expansion (RE) as a new learning paradigm\nthat advances beyond conventional Machine Learning (ML) and Deep Learning (DL).\nWhile DL focuses on learning from static data representations, RE proposes an\nadditional dimension: learning from the evolving behavior of models themselves.\nRE emphasizes multiple mappings of data through identical deep architectures\nand analyzes their internal representations (i.e., feature maps) in conjunction\nwith observed performance signals such as loss. By incorporating these\nbehavioral traces, RE enables iterative self-improvement, allowing each model\nversion to gain insight from its predecessors. The framework is extended\nthrough Multiverse RE (MVRE), which aggregates signals from parallel model\ninstances, and further through Heterogeneous MVRE (HMVRE), where models of\nvarying architectures contribute diverse perspectives. A scalable and adaptive\nvariant, Sc-HMVRE, introduces selective mechanisms and scale diversity for\nreal-world deployment. Altogether, RE presents a shift in DL: from purely\nrepresentational learning to behavior-aware, self-evolving systems. It lays the\ngroundwork for a new class of intelligent models capable of reasoning over\ntheir own learning dynamics, offering a path toward scalable, introspective,\nand adaptive artificial intelligence. A simple code example to support\nbeginners in running their own experiments is provided in Code Availability\nSection of this paper.", "AI": {"tldr": "This paper introduces Recurrent Expansion (RE), a new learning paradigm that enables AI models to learn from their own evolving behavior, leading to self-improvement and more adaptive systems. It includes extensions for parallel and diverse model architectures, aiming for introspective and scalable AI.", "motivation": "To advance beyond conventional ML and DL by introducing a new learning paradigm that learns from the evolving behavior of models themselves, enabling iterative self-improvement and creating a new class of intelligent models.", "method": "Recurrent Expansion (RE) learns from the evolving behavior of models by analyzing multiple mappings of data through identical deep architectures and their internal representations in conjunction with observed performance signals. Extensions include Multiverse RE (MVRE) for aggregating signals from parallel instances, Heterogeneous MVRE (HMVRE) for incorporating diverse architectures, and Sc-HMVRE for scalability and adaptability.", "result": "The paper introduces Recurrent Expansion (RE) and its variants (MVRE, HMVRE, Sc-HMVRE) as a framework for self-evolving AI systems that learn from their own behavior and predecessors' experiences.", "conclusion": "RE presents a shift in DL from purely representational learning to behavior-aware, self-evolving systems, laying the groundwork for intelligent models capable of reasoning over their own learning dynamics toward scalable, introspective, and adaptive AI."}}
{"id": "2507.08933", "categories": ["cond-mat.mes-hall", "cond-mat.quant-gas", "cond-mat.str-el", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.08933", "abs": "https://arxiv.org/abs/2507.08933", "authors": ["Nader Mostaan", "Nathan Goldman", "Ata\u00e7 \u0130mamo\u011flu", "Fabian Grusdt"], "title": "Anyon-trions in atomically thin semiconductor heterostructures", "comment": null, "summary": "The study of anyons in topologically ordered quantum systems has mainly\nrelied on edge-state interferometry. However, realizing controlled braiding of\nanyons necessitates the ability to detect and manipulate individual anyons\nwithin the bulk. Here, we propose and theoretically investigate a first step\ntoward this goal by demonstrating that a long-lived, optically generated\ninterlayer exciton can bind to a quasihole in a fractional quantum Hall state,\nforming a composite excitation we term an anyon-trion. Using exact\ndiagonalization, we show that mobile anyon-trions possess a binding energy of\napproximately 0.5 meV, whereas static anyon-trions exhibit a binding energy of\nabout 0.9 meV, that is linearly proportional to the quasiholes fractional\ncharge. An experimental realization based on photoluminescence from localized\ninterlayer excitons in a quantum twisting microscope setup should allow for a\ndirect optical observation of anyon-trions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u201c\u4efb\u610f\u5b50-\u4e09\u5b50\u201d\u590d\u5408\u6fc0\u53d1\u6001\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u7ed3\u5408\u80fd\u4e0e\u5206\u6570\u7535\u8377\u7684\u5173\u7cfb\u3002\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5728\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5bf9\u4efb\u610f\u5b50\u7684\u64cd\u63a7\u548c\u7f16\u7ec7\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9a8c\u601d\u8def\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5bf9\u4efb\u610f\u5b50\u7684\u53ef\u63a7\u7f16\u7ec7\uff0c\u9700\u8981\u80fd\u591f\u5728\u4f53\u76f8\u4e2d\u5bf9\u5355\u4e2a\u4efb\u610f\u5b50\u8fdb\u884c\u63a2\u6d4b\u548c\u64cd\u63a7\uff0c\u800c\u73b0\u6709\u7684\u8fb9\u7f18\u6001\u5e72\u6d89\u6d4b\u91cf\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65b0\u7684\u63a2\u6d4b\u548c\u64cd\u63a7\u4efb\u610f\u5b50\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u7cbe\u786e\u5bf9\u89d2\u5316\u65b9\u6cd5\uff0c\u5bf9\u957f\u5bff\u547d\u7684\u5149\u5b66\u751f\u6210\u5c42\u95f4\u6fc0\u5b50\u4e0e\u5206\u6570\u5316\u91cf\u5b50\u970d\u5c14\u6001\u4e2d\u7684\u51c6\u7a7a\u7a74\u7684\u7ed3\u5408\u8fdb\u884c\u4e86\u7406\u8bba\u7814\u7a76\uff0c\u5e76\u8ba1\u7b97\u4e86\u5176\u7ed3\u5408\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u79fb\u52a8\u7684\u4efb\u610f\u5b50-\u4e09\u5b50\u5177\u6709\u7ea60.5 meV\u7684\u7ed3\u5408\u80fd\uff0c\u800c\u9759\u6b62\u7684\u4efb\u610f\u5b50-\u4e09\u5b50\u5177\u6709\u7ea60.9 meV\u7684\u7ed3\u5408\u80fd\uff0c\u4e14\u8be5\u7ed3\u5408\u80fd\u4e0e\u51c6\u7a7a\u7a74\u7684\u5206\u6570\u7535\u8377\u5448\u7ebf\u6027\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u65cb\u8f6c\u663e\u5fae\u955c\u548c\u5c40\u57df\u5c42\u95f4\u6fc0\u5b50\u5149\u81f4\u53d1\u5149\u8fdb\u884c\u5149\u5b66\u89c2\u6d4b\u7684\u5b9e\u9a8c\u65b9\u6848\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u521d\u6b65\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u901a\u8fc7\u5149\u5b66\u7684\u624b\u6bb5\u6765\u7ed3\u5408\u51c6\u7a7a\u7a74\u4e0e\u957f\u5bff\u547d\u7684\u5916\u5ef6\u7247\u5c42\u6fc0\u5b50\uff0c\u5f62\u6210\u4e00\u79cd\u79f0\u4e3a\u201c\u4efb\u610f\u5b50-\u4e09\u5b50\u201d\u7684\u590d\u5408\u6fc0\u53d1\u6001\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u7406\u8bba\u53ef\u884c\u6027\u3002"}}
{"id": "2507.09377", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.09377", "abs": "https://arxiv.org/abs/2507.09377", "authors": ["Mumuksh Tayal"], "title": "A Fixed Parameter Tractable Approach for Solving the Vertex Cover Problem in Polynomial Time Complexity", "comment": "10 pages, 2 figures, 1 table. Accepted at STOC 2024 Workshop on TCS\n  for All", "summary": "The Minimum Vertex Cover problem, a classical NP-complete problem, presents\nsignificant challenges for exact solution on large graphs. Fixed-Parameter\nTractability (FPT) offers a powerful paradigm to address such problems by\nexploiting a parameter of the input, typically related to the size of the\ndesired solution. This paper presents an implementation and empirical\nevaluation of an FPT algorithm for the Minimum Vertex Cover problem\nparameterized by the size of the vertex cover, $k$. The algorithm utilizes a\nbranching strategy based on selecting adjacent vertices and recursively solving\nsubproblems on a reduced graph. We describe the algorithmic approach,\nimplementation details in Python, and present experimental results comparing\nits performance against the SageMath computational system. The results\ndemonstrate that the FPT implementation achieves significant performance\nimprovements for instances with large numbers of vertices ($n$) but relatively\nsmall values of the parameter ($k$), aligning with theoretical FPT complexity\nguarantees. We also discuss potential optimizations that could further improve\nthe algorithm's performance, particularly concerning the branching factor.", "AI": {"tldr": "\u6700\u5c0f\u9876\u70b9\u8986\u76d6\u7684 FPT \u7b97\u6cd5\u5728 n \u5927 k \u5c0f\u7684\u60c5\u51b5\u4e0b\u6bd4 SageMath \u5feb\u3002", "motivation": "\u6700\u5c0f\u9876\u70b9\u8986\u76d6\u95ee\u9898\u662f\u4e00\u4e2a\u7ecf\u5178\u7684 NP \u5b8c\u5168\u95ee\u9898\uff0c\u5728\u5927\u578b\u56fe\u4e0a\u7cbe\u786e\u6c42\u89e3\u5177\u6709\u6311\u6218\u6027\u3002FPT \u901a\u8fc7\u5229\u7528\u8f93\u5165\u53c2\u6570\u6765\u5904\u7406\u8fd9\u7c7b\u95ee\u9898\uff0c\u8be5\u53c2\u6570\u901a\u5e38\u4e0e\u671f\u671b\u89e3\u7684\u5927\u5c0f\u6709\u5173\u3002", "method": "\u8be5\u7b97\u6cd5\u91c7\u7528\u57fa\u4e8e\u9009\u62e9\u76f8\u90bb\u9876\u70b9\u5e76\u5728\u7b80\u5316\u56fe\u4e0a\u9012\u5f52\u6c42\u89e3\u5b50\u95ee\u9898\u7684\u5206\u652f\u7b56\u7565\uff0c\u5e76\u5bf9\u8be5\u7b97\u6cd5\u8fdb\u884c\u4e86 Python \u5b9e\u73b0\u3002", "result": "FPT \u5b9e\u73b0\u8fbe\u5230\u4e86\u663e\u8457\u7684\u6027\u80fd\u6539\u8fdb\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5177\u6709\u5927\u91cf\u9876\u70b9\uff08n\uff09\u4f46\u53c2\u6570\u503c\uff08k\uff09\u76f8\u5bf9\u8f83\u5c0f\u7684\u5b9e\u4f8b\u65f6\u3002", "conclusion": "\u8be5\u56fa\u5b9a\u53c2\u6570\u53ef\u5904\u7406\u6027\uff08FPT\uff09\u7b97\u6cd5\u5728\u5904\u7406\u5177\u6709\u5927\u91cf\u9876\u70b9\u4f46\u53c2\u6570\u503c\u76f8\u5bf9\u8f83\u5c0f\uff08k\uff09\u7684\u5b9e\u4f8b\u65f6\uff0c\u4e0eSageMath\u8ba1\u7b97\u7cfb\u7edf\u76f8\u6bd4\uff0c\u6027\u80fd\u663e\u8457\u63d0\u9ad8\uff0c\u8fd9\u4e0e\u7406\u8bba\u4e0a\u7684 FPT \u590d\u6742\u5ea6\u4fdd\u8bc1\u4e00\u81f4\u3002"}}
{"id": "2507.09140", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.09140", "abs": "https://arxiv.org/abs/2507.09140", "authors": ["Chuang Chen", "Xiaoxuan Xie", "Yongming Zhang", "Tianyu Zhang", "Haoran Xie"], "title": "Interactive Drawing Guidance for Anime Illustrations with Diffusion Model", "comment": "9 pages, 7 figures. In proceedings of NICOGRAPH International 2025", "summary": "Creating high-quality anime illustrations presents notable challenges,\nparticularly for beginners, due to the intricate styles and fine details\ninherent in anime art. We present an interactive drawing guidance system\nspecifically designed for anime illustrations to address this issue. It offers\nreal-time guidance to help users refine their work and streamline the creative\nprocess. Our system is built upon the StreamDiffusion pipeline to deliver\nreal-time drawing assistance. We fine-tune Stable Diffusion with LoRA to\nsynthesize anime style RGB images from user-provided hand-drawn sketches and\nprompts. Leveraging the Informative Drawings model, we transform these RGB\nimages into rough sketches, which are further refined into structured guidance\nsketches using a custom-designed optimizer. The proposed system offers precise,\nreal-time guidance aligned with the creative intent of the user, significantly\nenhancing both the efficiency and accuracy of the drawing process. To assess\nthe effectiveness of our approach, we conducted a user study, gathering\nempirical feedback on both system performance and interface usability.", "AI": {"tldr": "\u4e00\u4e2a\u65e8\u5728\u5e2e\u52a9\u7528\u6237\u66f4\u9ad8\u6548\u3001\u51c6\u786e\u5730\u521b\u4f5c\u52a8\u6f2b\u63d2\u753b\u7684\u4ea4\u4e92\u5f0f\u7ed8\u56fe\u6307\u5bfc\u7cfb\u7edf\uff0c\u63d0\u4f9b\u5b9e\u65f6\u5f15\u5bfc\u548c\u7ed3\u6784\u5316\u8349\u56fe\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u521d\u5b66\u8005\u5728\u521b\u4f5c\u9ad8\u8d28\u91cf\u52a8\u6f2b\u63d2\u753b\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u7531\u4e8e\u52a8\u6f2b\u827a\u672f\u590d\u6742\u7684\u98ce\u683c\u548c\u7cbe\u7ec6\u7684\u7ec6\u8282\u9020\u6210\u7684\u56f0\u96be\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u7ed8\u56fe\u6307\u5bfc\u7cfb\u7edf\u3002", "method": "\u672c\u7cfb\u7edf\u5229\u7528StreamDiffusion\u6d41\u6c34\u7ebf\u63d0\u4f9b\u5b9e\u65f6\u7ed8\u753b\u8f85\u52a9\uff0c\u901a\u8fc7LoRA\u5fae\u8c03Stable Diffusion\u6839\u636e\u7528\u6237\u624b\u7ed8\u8349\u56fe\u548c\u63d0\u793a\u8bcd\u751f\u6210\u52a8\u6f2b\u98ce\u683c\u7684RGB\u56fe\u50cf\uff0c\u518d\u8fd0\u7528Informative Drawings\u6a21\u578b\u5c06RGB\u56fe\u50cf\u8f6c\u6362\u4e3a\u7c97\u7565\u7684\u8349\u56fe\uff0c\u5e76\u901a\u8fc7\u81ea\u5b9a\u4e49\u4f18\u5316\u5668\u5c06\u8fd9\u4e9b\u8349\u56fe\u7ec6\u5316\u4e3a\u7ed3\u6784\u5316\u7684\u5f15\u5bfc\u8349\u56fe\u3002", "result": "\u672c\u7cfb\u7edf\u80fd\u591f\u6839\u636e\u7528\u6237\u7684\u521b\u610f\u610f\u56fe\u63d0\u4f9b\u7cbe\u786e\u3001\u5b9e\u65f6\u7684\u6307\u5bfc\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7ed8\u753b\u8fc7\u7a0b\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u7528\u6237\u7814\u7a76\u7684\u5b9e\u8bc1\u53cd\u9988\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408StreamDiffusion\u3001LoRA\u5fae\u8c03\u7684Stable Diffusion\u4ee5\u53caInformative Drawings\u6a21\u578b\uff0c\u5e76\u5229\u7528\u81ea\u5b9a\u4e49\u4f18\u5316\u5668\u751f\u6210\u5f15\u5bfc\u8349\u56fe\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u7cbe\u786e\u3001\u5b9e\u65f6\u7684\u52a8\u6f2b\u63d2\u753b\u7ed8\u5236\u6307\u5bfc\uff0c\u6709\u6548\u63d0\u5347\u4e86\u7ed8\u753b\u6548\u7387\u548c\u51c6\u786e\u6027\u3002\u7528\u6237\u7814\u7a76\u7ed3\u679c\u8868\u660e\u4e86\u8be5\u7cfb\u7edf\u7684\u6709\u6548\u6027\u4ee5\u53ca\u754c\u9762\u7684\u53ef\u7528\u6027\u3002"}}
{"id": "2507.08806", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bc6\u522b\u5e76\u79fb\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5176\u5728\u6570\u5b66\u7b49\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u6027\uff0c\u4e14\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u957f\u7bc7\u63a8\u7406\u4e2d\u5b58\u5728\u63a8\u7406\u8def\u5f84\u5197\u4f59\u7684\u95ee\u9898\uff0c\u8fd9\u4f1a\u5206\u6563\u6ce8\u610f\u529b\u5e76\u5f71\u54cd\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf token-level \u6ce8\u610f\u529b\u5206\u6570\u6765\u8bc6\u522b\u63a8\u7406\u5197\u4f59\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u611f\u77e5\u526a\u679d\u65b9\u6cd5\uff0c\u4f18\u5148\u79fb\u9664\u8d21\u732e\u5ea6\u4f4e\u7684\u63a8\u7406\u5757\u4e2d\u7684 token\uff0c\u7136\u540e\u79fb\u9664\u6ce8\u5165\u7684\u601d\u8003\u7ed3\u675f\u6307\u4ee4\u5e76\u6062\u590d\u63a8\u7406\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6574\u4f53\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b66\u7ade\u8d5b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u53bb\u9664\u5197\u4f59\u4fe1\u606f\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u63a8\u7406\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6574\u4f53\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u5b66\u7ade\u8d5b\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982AIME\u548cAMC\uff09\u4e0a\u8868\u73b0\u5f3a\u52b2\u3002"}}
{"id": "2507.10463", "categories": ["cs.ET", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.10463", "abs": "https://arxiv.org/abs/2507.10463", "authors": ["Maxwell Aifer", "Zach Belateche", "Suraj Bramhavar", "Kerem Y. Camsari", "Patrick J. Coles", "Gavin Crooks", "Douglas J. Durian", "Andrea J. Liu", "Anastasia Marchenkova", "Antonio J. Martinez", "Peter L. McMahon", "Faris Sbahi", "Benjamin Weiner", "Logan G. Wright"], "title": "Solving the compute crisis with physics-based ASICs", "comment": "16 pages, 5 figures", "summary": "Escalating artificial intelligence (AI) demands expose a critical \"compute\ncrisis\" characterized by unsustainable energy consumption, prohibitive training\ncosts, and the approaching limits of conventional CMOS scaling. Physics-based\nApplication-Specific Integrated Circuits (ASICs) present a transformative\nparadigm by directly harnessing intrinsic physical dynamics for computation\nrather than expending resources to enforce idealized digital abstractions. By\nrelaxing the constraints needed for traditional ASICs, like enforced\nstatelessness, unidirectionality, determinism, and synchronization, these\ndevices aim to operate as exact realizations of physical processes, offering\nsubstantial gains in energy efficiency and computational throughput. This\napproach enables novel co-design strategies, aligning algorithmic requirements\nwith the inherent computational primitives of physical systems. Physics-based\nASICs could accelerate critical AI applications like diffusion models,\nsampling, optimization, and neural network inference as well as traditional\ncomputational workloads like scientific simulation of materials and molecules.\nUltimately, this vision points towards a future of heterogeneous,\nhighly-specialized computing platforms capable of overcoming current scaling\nbottlenecks and unlocking new frontiers in computational power and efficiency.", "AI": {"tldr": "AI\u7b97\u529b\u9700\u6c42\u50ac\u751f\u4e86\u7b97\u529b\u5371\u673a\u3002\u7269\u7406\u57fa\u7840ASIC\u901a\u8fc7\u5229\u7528\u7269\u7406\u8fc7\u7a0b\u800c\u975e\u6570\u5b57\u62bd\u8c61\u8fdb\u884c\u8ba1\u7b97\uff0c\u80fd\u591f\u5927\u5e45\u63d0\u5347\u80fd\u6548\u548c\u8ba1\u7b97\u901f\u5ea6\uff0c\u6709\u671b\u89e3\u51b3AI\u548c\u79d1\u5b66\u8ba1\u7b97\u7684\u74f6\u9888\u3002", "motivation": "\u89e3\u51b3AI\u53d1\u5c55\u4e2d\u65e5\u76ca\u4e25\u5cfb\u7684\u7b97\u529b\u5371\u673a\uff0c\u8be5\u5371\u673a\u8868\u73b0\u4e3a\u4e0d\u53ef\u6301\u7eed\u7684\u80fd\u8017\u3001\u9ad8\u6602\u7684\u8bad\u7ec3\u6210\u672c\u4ee5\u53ca\u4f20\u7edfCMOS\u6269\u5c55\u7684\u6781\u9650\u3002", "method": "\u901a\u8fc7\u653e\u5bbd\u4f20\u7edfASIC\u7684\u65e0\u72b6\u6001\u6027\u3001\u5355\u5411\u6027\u3001\u786e\u5b9a\u6027\u548c\u540c\u6b65\u6027\u7b49\u7ea6\u675f\uff0c\u4f7f\u8bbe\u5907\u80fd\u7cbe\u786e\u5b9e\u73b0\u7269\u7406\u8fc7\u7a0b\u3002", "result": "\u7269\u7406\u57fa\u7840ASIC\u6709\u671b\u5728\u80fd\u6548\u548c\u8ba1\u7b97\u541e\u5410\u91cf\u65b9\u9762\u5b9e\u73b0\u5927\u5e45\u63d0\u5347\uff0c\u4ece\u800c\u52a0\u901f\u6269\u6563\u6a21\u578b\u3001\u91c7\u6837\u3001\u4f18\u5316\u548c\u795e\u7ecf\u7f51\u7edc\u63a8\u7406\u7b49\u5173\u952eAI\u5e94\u7528\uff0c\u4ee5\u53ca\u6750\u6599\u548c\u5206\u5b50\u6a21\u62df\u7b49\u4f20\u7edf\u8ba1\u7b97\u5de5\u4f5c\u8d1f\u8f7d\u3002", "conclusion": "\u7269\u7406\u57fa\u7840ASIC\u901a\u8fc7\u5229\u7528\u7269\u7406\u52a8\u529b\u5b66\u76f4\u63a5\u8fdb\u884c\u8ba1\u7b97\uff0c\u6709\u671b\u514b\u670d\u5f53\u524dCMOS\u6269\u5c55\u74f6\u9888\uff0c\u4e3aAI\u5e94\u7528\u548c\u79d1\u5b66\u8ba1\u7b97\u5e26\u6765\u663e\u8457\u7684\u80fd\u6548\u548c\u8ba1\u7b97\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5e76\u9884\u793a\u7740\u5f02\u6784\u3001\u9ad8\u5ea6\u4e13\u4e1a\u5316\u8ba1\u7b97\u5e73\u53f0\u7684\u65b0\u5174\u8d8b\u52bf\u3002"}}
{"id": "2507.09010", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09010", "abs": "https://arxiv.org/abs/2507.09010", "authors": ["Chun-Ting Chen", "HanGyeol Mun", "Jian Meng", "Mohamed S. Abdelfattah", "Jae-sun Seo"], "title": "Hybrid Systolic Array Accelerator with Optimized Dataflow for Edge Large Language Model Inference", "comment": "Accepted as a conference paper at the 2025 IEEE/ACM International\n  Symposium on Low Power Electronics and Design (ISLPED)", "summary": "Edge inference for large language models (LLM) offers secure, low-latency,\nand cost-effective inference solutions. We emphasize that an edge accelerator\nshould achieve high area efficiency and minimize external memory access (EMA)\nduring the memory-bound decode stage, while maintaining high energy efficiency\nduring the compute intensive prefill stage. This paper proposes an edge LLM\ninference accelerator featuring a hybrid systolic array (HSA) architecture that\noptimizes inference efficiency in both stages. To further reduce EMA, we adopt\nMXINT4 weight quantization and propose an optimized dataflow tailored for HSA,\nensuring negligible dequantization overhead and achieving 100% hardware\nutilization with minimal accuracy loss under edge DRAM bandwidth constraints.\nFor non-linear operations, we incorporate optimized root mean square\nnormalization (RMSNorm) and rotary position embedding (RoPE) units, reducing\ntheir latency, area, and memory access overhead while enabling end-to-end\ninference on our accelerator. Our solution achieves 247/117 (token/s/mm2) while\nrunning a 1.3B LLM on long-input/long-output scenarios, providing >2.45x/13.5x\nimprovement over existing approaches, while maintaining superior energy\nefficiency in token generation.", "AI": {"tldr": "\u9488\u5bf9\u8fb9\u7f18LLM\u63a8\u7406\uff0c\u63d0\u51fa\u4e00\u79cd\u6df7\u5408\u7ed3\u6784\uff08HSA\uff09\u52a0\u901f\u5668\uff0c\u91c7\u7528MXINT4\u91cf\u5316\u548c\u4f18\u5316\u6570\u636e\u6d41\uff0c\u5728\u6ee1\u8db3DRAM\u5e26\u5bbd\u9650\u5236\u4e0b\u5b9e\u73b0\u9ad8\u5229\u7528\u7387\u548c\u4f4e\u7cbe\u5ea6\u635f\u5931\uff0c\u5e76\u96c6\u6210\u4f18\u5316RMSNorm\u548cRoPE\u5355\u5143\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5b89\u5168\u3001\u4f4e\u5ef6\u8fdf\u548c\u4f4e\u6210\u672c\u7684\u8fb9\u7f18LLM\u63a8\u7406\uff0c\u9700\u8981\u8bbe\u8ba1\u4e00\u79cd\u65e2\u80fd\u6ee1\u8db3\u9762\u79ef\u6548\u7387\u548c\u6700\u5c0f\u5316\u5916\u90e8\u5185\u5b58\u8bbf\u95ee\uff08EMA\uff09\u7684\u5185\u5b58\u74f6\u9888\u89e3\u7801\u9636\u6bb5\uff0c\u53c8\u80fd\u4fdd\u6301\u9ad8\u80fd\u6548\u7684\u8ba1\u7b97\u5bc6\u96c6\u578b\u9884\u586b\u5145\u9636\u6bb5\u7684\u8fb9\u7f18\u52a0\u901f\u5668\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528\u6df7\u5408\u7ed3\u6784\uff08HSA\uff09\u7684\u8fb9\u7f18LLM\u63a8\u7406\u52a0\u901f\u5668\uff0c\u8be5\u52a0\u901f\u5668\u901a\u8fc7MXINT4\u6743\u91cd\u91cf\u5316\u548c\u4f18\u5316\u7684\u6570\u636e\u6d41\u6765\u51cf\u5c11\u5916\u90e8\u5185\u5b58\u8bbf\u95ee\uff08EMA\uff09\uff0c\u4ee5\u63d0\u9ad8\u5185\u5b58\u5bc6\u96c6\u578b\u89e3\u7801\u9636\u6bb5\u7684\u6548\u7387\uff0c\u5e76\u5728\u8ba1\u7b97\u5bc6\u96c6\u578b\u9884\u586b\u5145\u9636\u6bb5\u4fdd\u6301\u9ad8\u80fd\u6548\u3002\u540c\u65f6\uff0c\u96c6\u6210\u4e86\u4f18\u5316\u7684RMSNorm\u548cRoPE\u5355\u5143\u4ee5\u964d\u4f4e\u975e\u7ebf\u6027\u8fd0\u7b97\u7684\u5ef6\u8fdf\u3001\u9762\u79ef\u548c\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\u3002", "result": "\u5728\u8fd0\u884c1.3B LLM\u5904\u7406\u957f\u8f93\u5165/\u8f93\u51fa\u573a\u666f\u65f6\uff0c\u8be5\u52a0\u901f\u5668\u5b9e\u73b0\u4e86247/117 (token/s/mm2) \u7684\u6027\u80fd\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\u6709\u8d85\u8fc72.45\u500d/13.5\u500d\u7684\u63d0\u5347\uff0c\u5e76\u5728\u751f\u6210token\u65f6\u4fdd\u6301\u4e86\u5353\u8d8a\u7684\u80fd\u6548\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u6df7\u5408\u7ed3\u6784\uff08HSA\uff09\u7684\u8fb9\u7f18LLM\u63a8\u7406\u52a0\u901f\u5668\uff0c\u901a\u8fc7MXINT4\u91cf\u5316\u548c\u4f18\u5316\u7684\u6570\u636e\u6d41\uff0c\u5728\u6ee1\u8db3\u8fb9\u7f18DRAM\u5e26\u5bbd\u9650\u5236\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u786c\u4ef6\u5229\u7528\u7387\u548c\u53ef\u5ffd\u7565\u7684\u7cbe\u5ea6\u635f\u5931\uff0c\u5e76\u7ed3\u5408\u4f18\u5316\u7684RMSNorm\u548cRoPE\u5355\u5143\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3001\u9762\u79ef\u548c\u5185\u5b58\u8bbf\u95ee\u5f00\u9500\uff0c\u6700\u7ec8\u5728\u5904\u7406\u957f\u8f93\u5165/\u8f93\u51fa\u573a\u666f\u65f6\uff0c\u76f8\u8f83\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u57281.3B LLM\u4e0a\u5b9e\u73b0\u4e862.45\u500d/13.5\u500d\u7684\u6027\u80fd\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f18\u8d8a\u7684\u80fd\u6548\u3002"}}
{"id": "2507.10001", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.10001", "abs": "https://arxiv.org/abs/2507.10001", "authors": ["Baisen Yu", "Shoichi Sato", "Masaaki Tanaka", "Ryosho Nakane"], "title": "Spin injection in Si-based ferromagnetic tunnel junctions with MgO/MgAl2O4 barriers:Experimental and theoretical investigation of barrier thickness-dependent spin tunneling efficiency", "comment": "Main manuscript:30 pages, 13 figures; Supplemental material: 9 pages,\n  7 figures", "summary": "We have experimentally and theoretically investigated the spin transport in\nFe/Mg/MgO/MgAl2O4/n+-Si(001) ferromagnetic tunnel junctions on a Si substrate,\nby systematically varying the thickness combination of amorphous MgO and\nMgAl2O4 tunnel barrier layers with a sliding shutter between the evaporation\nsources and substrate during electron-beam evaporation. A technical advantage\nof MgAl2O4 is that a continuous and flat thin film is realized on a Si\nsubstrate even when the MgAl2O4 thickness is as thin as 0.5 nm, unlike MgO,\nwhich enables us to examine the spin transport in a thinner range of the tunnel\nbarrier thickness. Our distinct finding is as follows: When the Fe/Mg/MgO\ninterface is used on the top side, the spin polarization PS of tunneling\nelectrons increases at 10 K as the total MgO/MgAl2O4 tunnel barrier thickness\n(tox = 0.47 - 1.4 nm) is increased, regardless of different thickness\ncombinations, and PS shows saturation-like behavior when tox is above 1.1 nm.\nSince this feature cannot be explained by the well-known conductivity mismatch\nin semiconductor-based ferromagnetic tunnel junctions, we propose a simple\nphenomenological tunneling model based on two different direct tunneling paths,\nwhich have higher/lower spin polarizations with longer/shorter decay lengths.\nOur numerical calculation reproduces the relationship between the spin\npolarization PS and total tunnel barrier thickness tox in the experiments,\nindicating that the dominant mechanism is an increasing contribution of the\nlower spin polarization path as tox is decreased. We discuss possible origins\nfor this phenomenon including intrinsic and extrinsic tunneling mechanisms. Our\nanalysis method provides an insight into the detailed spin transport physics in\nsemiconductor-based ferromagnetic junctions, particularly, with a very thin\ntunnel barrier layer.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u548c\u7406\u8bba\u7814\u7a76\u4e86\u94c1\u78c1\u96a7\u7a7f\u7ed3\u4e2d\u7684\u81ea\u65cb\u8f93\u8fd0\uff0c\u53d1\u73b0\u96a7\u7a7f\u52bf\u5792\u539a\u5ea6\u5f71\u54cd\u81ea\u65cb\u6781\u5316\u7387\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u578b\u6765\u89e3\u91ca\u8fd9\u79cd\u73b0\u8c61\uff0c\u5e76\u4e3a\u7406\u89e3\u8fd9\u7c7b\u5668\u4ef6\u4e2d\u7684\u81ea\u65cb\u8f93\u8fd0\u7269\u7406\u5b66\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "motivation": "\u4e3a\u4e86\u5728\u66f4\u8584\u7684\u96a7\u7a7f\u52bf\u5792\u539a\u5ea6\u8303\u56f4\u5185\u68c0\u67e5\u81ea\u65cb\u8f93\u8fd0\uff0c\u5e76\u89e3\u91ca\u5728\u534a\u5bfc\u4f53\u57fa\u94c1\u78c1\u96a7\u7a7f\u7ed3\u4e2d\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\uff0c\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e24\u79cd\u4e0d\u540c\u96a7\u7a7f\u8def\u5f84\u7684\u73b0\u8c61\u5b66\u6a21\u578b\u3002", "method": "\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u7535\u5b50\u675f\u84b8\u53d1\uff0c\u5229\u7528\u6ed1\u52a8\u6321\u677f\u7cfb\u7edf\u5730\u6539\u53d8\u4e86\u975e\u6676MgO\u548cMgAl2O4\u96a7\u7a7f\u52bf\u5792\u5c42\u7684\u539a\u5ea6\u7ec4\u5408\uff0c\u4ece\u800c\u5728\u5b9e\u9a8c\u548c\u7406\u8bba\u4e0a\u7814\u7a76\u4e86Si\u886c\u5e95\u4e0a\u7684Fe/Mg/MgO/MgAl2O4/n+-Si(001)\u94c1\u78c1\u96a7\u7a7f\u7ed3\u4e2d\u7684\u81ea\u65cb\u8f93\u8fd0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53Fe/Mg/MgO\u754c\u9762\u4f4d\u4e8e\u9876\u4fa7\u65f6\uff0c\u572810 K\u4e0b\uff0c\u968f\u7740\u603bMgO/MgAl2O4\u96a7\u7a7f\u52bf\u5792\u539a\u5ea6\uff08tox = 0.47 - 1.4 nm\uff09\u7684\u589e\u52a0\uff0c\u96a7\u7a7f\u7535\u5b50\u7684\u81ea\u65cb\u6781\u5316\u7387PS\u589e\u52a0\uff0c\u5e76\u4e14\u5f53tox\u5927\u4e8e1.1 nm\u65f6\uff0cPS\u8868\u73b0\u51fa\u9971\u548c\u884c\u4e3a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e24\u79cd\u4e0d\u540c\u96a7\u7a7f\u8def\u5f84\u7684\u7b80\u5355\u73b0\u8c61\u5b66\u96a7\u7a7f\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u542b\u8f83\u9ad8/\u8f83\u4f4e\u81ea\u65cb\u6781\u5316\u548c\u8f83\u957f/\u8f83\u77ed\u8870\u51cf\u957f\u5ea6\u7684\u8def\u5f84\u3002\u8be5\u6a21\u578b\u80fd\u591f\u91cd\u73b0\u5b9e\u9a8c\u4e2d\u81ea\u65cb\u6781\u5316\u7387P S\u4e0e\u603b\u96a7\u7a7f\u52bf\u5792\u539a\u5ea6t ox\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u8868\u660e\u5f53t ox\u51cf\u5c0f\u65f6\uff0c\u8f83\u4f4e\u81ea\u65cb\u6781\u5316\u8def\u5f84\u7684\u8d21\u732e\u589e\u52a0\u662f\u4e3b\u8981\u673a\u5236\u3002\u7814\u7a76\u8ba8\u8bba\u4e86\u5305\u62ec\u672c\u5f81\u548c\u975e\u672c\u5f81\u96a7\u7a7f\u673a\u5236\u5728\u5185\u7684\u8be5\u73b0\u8c61\u7684\u53ef\u80fd\u8d77\u6e90\u3002\u8be5\u5206\u6790\u65b9\u6cd5\u4e3a\u534a\u5bfc\u4f53\u57fa\u94c1\u78c1\u7ed3\u4e2d\u7684\u8be6\u7ec6\u81ea\u65cb\u8f93\u8fd0\u7269\u7406\u5b66\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u96a7\u7a7f\u52bf\u5792\u5c42\u975e\u5e38\u8584\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2507.09000", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09000", "abs": "https://arxiv.org/abs/2507.09000", "authors": ["Arshia Rafieioskouei", "Kenneth Rogale", "Borzoo Bonakdarpour"], "title": "Efficient Discovery of Actual Causality with Uncertainty", "comment": null, "summary": "Identifying the actual cause of events in engineered systems is a fundamental\nchallenge in system analysis. Finding such causes becomes more challenging in\nthe presence of noise and uncertainty in real-world systems. In this paper, we\nadopt the notion of probabilistic actual causality by Fenton-Glynn, which is a\nprobabilistic extension of Halpern and Pearl's actual causality, and propose a\nnovel method to formally reason about causal effect of events in systems\nsubject to uncertainty. We (1) formulate the discovery of probabilistic actual\ncauses in computing systems as an SMT problem, and (2) address the scalability\nchallenges by introducing an abstraction-refinement technique that\nsignificantly improves efficiency. We demonstrate the effectiveness of our\napproach through three case studies, identifying probabilistic causes of safety\nviolations in (1) the Mountain Car problem, (2) the Lunar Lander benchmark, and\n(3) MPC controller for an F-16 autopilot simulator.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e SMT \u548c\u62bd\u8c61-\u7ec6\u5316\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u7684\u7cfb\u7edf\u4e2d\u8bc6\u522b\u4e8b\u4ef6\u7684\u6982\u7387\u5b9e\u9645\u539f\u56e0\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u5b58\u5728\u566a\u58f0\u548c\u4e0d\u786e\u5b9a\u6027\u7684\u771f\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\uff0c\u8bc6\u522b\u5de5\u7a0b\u7cfb\u7edf\u4e8b\u4ef6\u7684\u5b9e\u9645\u539f\u56e0\u662f\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u5c06\u6982\u7387\u5b9e\u9645\u56e0\u679c\u63a8\u7406\u5f62\u5f0f\u5316\u4e3a SMT \u95ee\u9898\uff0c\u5e76\u91c7\u7528\u62bd\u8c61-\u7ec6\u5316\u6280\u672f\u6765\u89e3\u51b3\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5728 Mountain Car \u95ee\u9898\u3001Lunar Lander \u57fa\u51c6\u548c F-16 \u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u5668\u7684 MPC \u63a7\u5236\u5668\u4e2d\u8bc6\u522b\u5b89\u5168\u8fdd\u89c4\u7684\u6982\u7387\u539f\u56e0\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u7684\u7cfb\u7edf\u4e2d\u5f62\u5f0f\u5316\u5730\u63a8\u7406\u4e8b\u4ef6\u7684\u56e0\u679c\u6548\u5e94\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86 Fenton-Glynn \u7684\u6982\u7387\u5b9e\u9645\u56e0\u679c\u6982\u5ff5\uff0c\u5e76\u5c06\u53d1\u73b0\u6982\u7387\u5b9e\u9645\u539f\u56e0\u5236\u5b9a\u4e3a SMT \u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u62bd\u8c61-\u7ec6\u5316\u6280\u672f\u89e3\u51b3\u4e86\u53ef\u6269\u5c55\u6027\u6311\u6218\u3002"}}
{"id": "2507.08885", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08885", "abs": "https://arxiv.org/abs/2507.08885", "authors": ["Baining Zhao", "Rongze Tang", "Mingyuan Jia", "Ziyou Wang", "Fanghang Man", "Xin Zhang", "Yu Shang", "Weichen Zhang", "Chen Gao", "Wei Wu", "Xin Wang", "Xinlei Chen", "Yong Li"], "title": "AirScape: An Aerial Generative World Model with Motion Controllability", "comment": null, "summary": "How to enable robots to predict the outcomes of their own motion intentions\nin three-dimensional space has been a fundamental problem in embodied\nintelligence. To explore more general spatial imagination capabilities, here we\npresent AirScape, the first world model designed for six-degree-of-freedom\naerial agents. AirScape predicts future observation sequences based on current\nvisual inputs and motion intentions. Specifically, we construct an dataset for\naerial world model training and testing, which consists of 11k video-intention\npairs. This dataset includes first-person-view videos capturing diverse drone\nactions across a wide range of scenarios, with over 1,000 hours spent\nannotating the corresponding motion intentions. Then we develop a two-phase\ntraining schedule to train a foundation model -- initially devoid of embodied\nspatial knowledge -- into a world model that is controllable by motion\nintentions and adheres to physical spatio-temporal constraints.", "AI": {"tldr": "AirScape\u662f\u4e00\u4e2a\u65b0\u7684\u4e16\u754c\u6a21\u578b\uff0c\u53ef\u4ee5\u5e2e\u52a9\u65e0\u4eba\u673a\u9884\u6d4b\u5b83\u4eec\u7684\u52a8\u4f5c\u7ed3\u679c\u3002", "motivation": "\u63a2\u7d22\u66f4\u901a\u7528\u7684\u7a7a\u95f4\u60f3\u8c61\u80fd\u529b\uff0c\u4ee5\u89e3\u51b3\u673a\u5668\u4eba\u9884\u6d4b\u81ea\u8eab\u8fd0\u52a8\u610f\u56fe\u5728\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u7ed3\u679c\u7684\u57fa\u672c\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u8bad\u7ec3\u8ba1\u5212\u6765\u8bad\u7ec3\u4e00\u4e2a\u57fa\u7840\u6a21\u578b\uff0c\u4f7f\u5176\u6210\u4e3a\u4e00\u4e2a\u53ef\u63a7\u4e8e\u8fd0\u52a8\u610f\u56fe\u5e76\u9075\u5faa\u7269\u7406\u65f6\u7a7a\u7ea6\u675f\u7684\u4e16\u754c\u6a21\u578b\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b11k\u89c6\u9891-\u610f\u56fe\u5bf9\u7684\u7a7a\u4e2d\u4e16\u754c\u6a21\u578b\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u62ec\u4e86\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u7684\u89c6\u9891\u548c\u8be6\u7ec6\u6807\u6ce8\u7684\u8fd0\u52a8\u610f\u56fe\u3002", "conclusion": "AirScape\u662f\u9996\u4e2a\u4e3a\u516d\u81ea\u7531\u5ea6\u7a7a\u4e2d\u673a\u5668\u4eba\u8bbe\u8ba1\u7684\u4e16\u754c\u6a21\u578b\uff0c\u80fd\u591f\u6839\u636e\u5f53\u524d\u7684\u89c6\u89c9\u8f93\u5165\u548c\u8fd0\u52a8\u610f\u56fe\u9884\u6d4b\u672a\u6765\u89c2\u6d4b\u5e8f\u5217\u3002"}}
{"id": "2507.09149", "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.LG", "I.2.7; J.4"], "pdf": "https://arxiv.org/pdf/2507.09149", "abs": "https://arxiv.org/abs/2507.09149", "authors": ["Mkululi Sikosana", "Sean Maudsley-Barton", "Oluwaseun Ajao"], "title": "Advanced Health Misinformation Detection Through Hybrid CNN-LSTM Models Informed by the Elaboration Likelihood Model (ELM)", "comment": "11 Pages, 2 Figures, 3 Tables conference paper to appear in\n  proceedings of International Conference on Artificial Intelligence, Computer,\n  Data Sciences and Applications (ACDSA'25)", "summary": "Health misinformation during the COVID-19 pandemic has significantly\nchallenged public health efforts globally. This study applies the Elaboration\nLikelihood Model (ELM) to enhance misinformation detection on social media\nusing a hybrid Convolutional Neural Network (CNN) and Long Short-Term Memory\n(LSTM) model. The model aims to enhance the detection accuracy and reliability\nof misinformation classification by integrating ELM-based features such as text\nreadability, sentiment polarity, and heuristic cues (e.g., punctuation\nfrequency). The enhanced model achieved an accuracy of 97.37%, precision of\n96.88%, recall of 98.50%, F1-score of 97.41%, and ROC-AUC of 99.50%. A combined\nmodel incorporating feature engineering further improved performance, achieving\na precision of 98.88%, recall of 99.80%, F1-score of 99.41%, and ROC-AUC of\n99.80%. These findings highlight the value of ELM features in improving\ndetection performance, offering valuable contextual information. This study\ndemonstrates the practical application of psychological theories in developing\nadvanced machine learning algorithms to address health misinformation\neffectively.", "AI": {"tldr": "\u4e00\u9879\u7814\u7a76\u5229\u7528\u5fc3\u7406\u5b66\u6a21\u578b\uff08ELM\uff09\u548c\u6df7\u5408\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08CNN-LSTM\uff09\u6765\u8bc6\u522b\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\uff0c\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u5e76\u63d0\u51fa\u4e86\u7ed3\u5408\u7279\u5f81\u5de5\u7a0b\u7684\u6539\u8fdb\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u5e94\u5bf9 COVID-19 \u5927\u6d41\u884c\u671f\u95f4\u5168\u7403\u516c\u5171\u536b\u751f\u9886\u57df\u9762\u4e34\u7684\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u6f5c\u5728\u4fe1\u606f\u5904\u7406\u6a21\u578b\uff08ELM\uff09\u6765\u589e\u5f3a\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7ed3\u5408\u4e86\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7684\u6df7\u5408\u6a21\u578b\uff0c\u5e76\u878d\u5165\u4e86\u57fa\u4e8e\u6f5c\u5728\u4fe1\u606f\u5904\u7406\u6a21\u578b\uff08ELM\uff09\u7684\u7279\u5f81\uff0c\u4f8b\u5982\u6587\u672c\u53ef\u8bfb\u6027\u3001\u60c5\u611f\u6781\u6027\u548c\u542f\u53d1\u5f0f\u7ebf\u7d22\uff08\u5982\u6807\u70b9\u7b26\u53f7\u9891\u7387\uff09\uff0c\u4ee5\u589e\u5f3a\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u7ed3\u5408 ELM \u7279\u5f81\u7684\u6df7\u5408\u6a21\u578b\u5728\u51c6\u786e\u6027\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u3001F1 \u5206\u6570\u548c ROC-AUC \u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86 97.37% \u7684\u51c6\u786e\u7387\u300196.88% \u7684\u7cbe\u786e\u7387\u300198.50% \u7684\u53ec\u56de\u7387\u300197.41% \u7684 F1 \u5206\u6570\u548c 99.50% \u7684 ROC-AUC\u3002\u901a\u8fc7\u8fdb\u4e00\u6b65\u7ed3\u5408\u7279\u5f81\u5de5\u7a0b\uff0c\u6a21\u578b\u6027\u80fd\u5f97\u5230\u8fdb\u4e00\u6b65\u63d0\u5347\uff0c\u7cbe\u786e\u7387\u8fbe\u5230 98.88%\uff0c\u53ec\u56de\u7387\u8fbe\u5230 99.80%\uff0cF1 \u5206\u6570\u8fbe\u5230 99.41%\uff0cROC-AUC \u8fbe\u5230 99.80%\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u6f5c\u5728\u4fe1\u606f\u5904\u7406\u6a21\u578b\uff08ELM\uff09\u7684\u7279\u5f81\u5e94\u7528\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u6df7\u5408\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u793e\u4ea4\u5a92\u4f53\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cELM\u7279\u5f81\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u4e3a\u6709\u6548\u5e94\u5bf9\u5065\u5eb7\u9519\u8bef\u4fe1\u606f\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u5b9e\u7528\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.08917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08917", "abs": "https://arxiv.org/abs/2507.08917", "authors": ["Justin D. Norman", "Hany Farid"], "title": "Detecting Deepfake Talking Heads from Facial Biometric Anomalies", "comment": "10 pages, 3 figures, 3 tables", "summary": "The combination of highly realistic voice cloning, along with visually\ncompelling avatar, face-swap, or lip-sync deepfake video generation, makes it\nrelatively easy to create a video of anyone saying anything. Today, such\ndeepfake impersonations are often used to power frauds, scams, and political\ndisinformation. We propose a novel forensic machine learning technique for the\ndetection of deepfake video impersonations that leverages unnatural patterns in\nfacial biometrics. We evaluate this technique across a large dataset of\ndeepfake techniques and impersonations, as well as assess its reliability to\nvideo laundering and its generalization to previously unseen video deepfake\ngenerators.", "AI": {"tldr": "\u4e00\u79cd\u5229\u7528\u9762\u90e8\u751f\u7269\u7279\u5f81\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u6a21\u4eff\u7684\u65b0\u578b\u673a\u5668\u5b66\u4e60\u6280\u672f\u88ab\u63d0\u51fa\uff0c\u5e76\u5728\u5927\u91cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\uff08\u7ed3\u5408\u903c\u771f\u7684\u58f0\u97f3\u514b\u9686\u548c\u5177\u6709\u89c6\u89c9\u5438\u5f15\u529b\u7684\u865a\u62df\u5316\u8eab\u3001\u6362\u8138\u6216\u5507\u5f62\u540c\u6b65\u7684\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u751f\u6210\uff09\u88ab\u5e7f\u6cdb\u7528\u4e8e\u6b3a\u8bc8\u3001\u8bc8\u9a97\u548c\u653f\u6cbb\u865a\u5047\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6cd5\u8bc1\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5229\u7528\u9762\u90e8\u751f\u7269\u7279\u5f81\u4e2d\u7684\u4e0d\u81ea\u7136\u6a21\u5f0f\u6765\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u89c6\u9891\u6a21\u4eff\u3002", "result": "\u8be5\u6280\u672f\u5728\u5927\u91cf\u7684\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u548c\u6a21\u4eff\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u6280\u672f\u6709\u6f5c\u529b\u7528\u4e8e\u68c0\u6d4b\u6df1\u5ea6\u4f2a\u9020\u7684\u89c6\u9891\u6a21\u4eff\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u8bc4\u4f30\u5176\u5728\u89c6\u9891\u6d17\u7a3f\u548c\u5bf9\u672a\u89c1\u8fc7\u89c6\u9891\u6df1\u5ea6\u4f2a\u9020\u751f\u6210\u5668\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u53ef\u9760\u6027\u3002"}}
{"id": "2507.09546", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09546", "abs": "https://arxiv.org/abs/2507.09546", "authors": ["Xiangwang Hou", "Jingjing Wang", "Jun Du", "Chunxiao Jiang", "Yong Ren", "Dusit Niyato"], "title": "Lightweight Federated Learning over Wireless Edge Networks", "comment": null, "summary": "With the exponential growth of smart devices connected to wireless networks,\ndata production is increasing rapidly, requiring machine learning (ML)\ntechniques to unlock its value. However, the centralized ML paradigm raises\nconcerns over communication overhead and privacy. Federated learning (FL)\noffers an alternative at the network edge, but practical deployment in wireless\nnetworks remains challenging. This paper proposes a lightweight FL (LTFL)\nframework integrating wireless transmission power control, model pruning, and\ngradient quantization. We derive a closed-form expression of the FL convergence\ngap, considering transmission error, model pruning error, and gradient\nquantization error. Based on these insights, we formulate an optimization\nproblem to minimize the convergence gap while meeting delay and energy\nconstraints. To solve the non-convex problem efficiently, we derive closed-form\nsolutions for the optimal model pruning ratio and gradient quantization level,\nand employ Bayesian optimization for transmission power control. Extensive\nexperiments on real-world datasets show that LTFL outperforms state-of-the-art\nschemes.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLTFL\u7684\u8f7b\u91cf\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u4f20\u8f93\u529f\u7387\u3001\u6a21\u578b\u526a\u679d\u548c\u68af\u5ea6\u91cf\u5316\uff0c\u89e3\u51b3\u4e86\u65e0\u7ebf\u7f51\u7edc\u4e2d\u8054\u90a6\u5b66\u4e60\u7684\u6548\u7387\u548c\u9690\u79c1\u95ee\u9898\u3002", "motivation": "\u968f\u7740\u8fde\u63a5\u5230\u65e0\u7ebf\u7f51\u7edc\u7684\u667a\u80fd\u8bbe\u5907\u7684\u6307\u6570\u7ea7\u589e\u957f\uff0c\u6570\u636e\u91cf\u6025\u5267\u589e\u52a0\uff0c\u9700\u8981\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u6280\u672f\u6765\u53d1\u6398\u5176\u4ef7\u503c\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u4e2d\u5fc3\u5316ML\u8303\u5f0f\u5f15\u53d1\u4e86\u901a\u4fe1\u5f00\u9500\u548c\u9690\u79c1\u65b9\u9762\u7684\u62c5\u5fe7\u3002\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u7f51\u7edc\u8fb9\u7f18\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u5b9e\u9645\u90e8\u7f72\u4ecd\u7136\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u5316\u8054\u90a6\u5b66\u4e60\uff08LTFL\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u65e0\u7ebf\u4f20\u8f93\u529f\u7387\u63a7\u5236\u3001\u6a21\u578b\u526a\u679d\u548c\u68af\u5ea6\u91cf\u5316\u3002\u63a8\u5bfc\u4e86\u8003\u8651\u4f20\u8f93\u8bef\u5dee\u3001\u6a21\u578b\u526a\u679d\u8bef\u5dee\u548c\u68af\u5ea6\u91cf\u5316\u8bef\u5dee\u7684FL\u6536\u655b\u5dee\u8ddd\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002\u57fa\u4e8e\u6b64\uff0c\u5236\u5b9a\u4e86\u4e00\u4e2a\u6700\u5c0f\u5316\u6536\u655b\u5dee\u8ddd\u540c\u65f6\u6ee1\u8db3\u5ef6\u8fdf\u548c\u80fd\u6e90\u7ea6\u675f\u7684\u4f18\u5316\u95ee\u9898\u3002\u4e3a\u9ad8\u6548\u89e3\u51b3\u8be5\u975e\u51f8\u95ee\u9898\uff0c\u63a8\u5bfc\u4e86\u6700\u4f18\u6a21\u578b\u526a\u679d\u7387\u548c\u68af\u5ea6\u91cf\u5316\u7ea7\u522b\u7684\u95ed\u5f0f\u89e3\uff0c\u5e76\u91c7\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u8fdb\u884c\u4f20\u8f93\u529f\u7387\u63a7\u5236\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLTFL\u6846\u67b6\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "conclusion": "LTFL\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u65e0\u7ebf\u4f20\u8f93\u529f\u7387\u63a7\u5236\u3001\u6a21\u578b\u526a\u679d\u548c\u68af\u5ea6\u91cf\u5316\uff0c\u5728\u6ee1\u8db3\u5ef6\u8fdf\u548c\u80fd\u6e90\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u6700\u5c0f\u5316\u4e86FL\u6536\u655b\u5dee\u8ddd\uff0c\u5e76\u4e14\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002"}}
{"id": "2507.09001", "categories": ["cond-mat.mtrl-sci", "cond-mat.dis-nn", "cs.LG", "physics.comp-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09001", "abs": "https://arxiv.org/abs/2507.09001", "authors": ["Sazzad Hossain", "Ponkrshnan Thiagarajan", "Shashank Pathrudkar", "Stephanie Taylor", "Abhijeet S. Gangan", "Amartya S. Banerjee", "Susanta Ghosh"], "title": "Surprisingly High Redundancy in Electronic Structure Data", "comment": null, "summary": "Machine Learning (ML) models for electronic structure rely on large datasets\ngenerated through expensive Kohn-Sham Density Functional Theory simulations.\nThis study reveals a surprisingly high level of redundancy in such datasets\nacross various material systems, including molecules, simple metals, and\ncomplex alloys. Our findings challenge the prevailing assumption that large,\nexhaustive datasets are necessary for accurate ML predictions of electronic\nstructure. We demonstrate that even random pruning can substantially reduce\ndataset size with minimal loss in predictive accuracy, while a state-of-the-art\ncoverage-based pruning strategy retains chemical accuracy and model\ngeneralizability using up to 100-fold less data and reducing training time by\nthreefold or more. By contrast, widely used importance-based pruning methods,\nwhich eliminate seemingly redundant data, can catastrophically fail at higher\npruning factors, possibly due to the significant reduction in data coverage.\nThis heretofore unexplored high degree of redundancy in electronic structure\ndata holds the potential to identify a minimal, essential dataset\nrepresentative of each material class.", "AI": {"tldr": "\u7535\u5b50\u7ed3\u6784\u673a\u5668\u5b66\u4e60\u6570\u636e\u96c6\u5b58\u5728\u9ad8\u5ea6\u5197\u4f59\uff0c\u53ef\u5927\u5e45\u524a\u51cf\u6570\u636e\u91cf\u800c\u635f\u5931\u6700\u5c0f\u7684\u9884\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u7535\u5b50\u7ed3\u6784\u673a\u5668\u5b66\u4e60\u6a21\u578b\u9700\u8981\u6602\u8d35\u7684\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\u751f\u6210\u7684\u5927\u578b\u6570\u636e\u96c6\u7684\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u636e\u96c6\u4e2d\u7684\u5197\u4f59\u6027\u5e76\u5f00\u53d1\u66f4\u6709\u6548\u7684\u6570\u636e\u96c6\u6784\u5efa\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u526a\u679d\u7b56\u7565\uff0c\u5e76\u4e0e\u968f\u673a\u526a\u679d\u548c\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u526a\u679d\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u53d1\u73b0\u7535\u5b50\u7ed3\u6784\u6570\u636e\u96c6\uff08\u5305\u62ec\u5206\u5b50\u3001\u7b80\u5355\u91d1\u5c5e\u548c\u590d\u6742\u5408\u91d1\uff09\u5b58\u5728\u9ad8\u5ea6\u5197\u4f59\u3002\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u526a\u679d\u7b56\u7565\u80fd\u4ee5\u5c11100\u500d\u7684\u6570\u636e\u91cf\u548c\u51cf\u5c11\u4e09\u500d\u6216\u66f4\u591a\u7684\u8bad\u7ec3\u65f6\u95f4\u6765\u4fdd\u6301\u5316\u5b66\u7cbe\u5ea6\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u57fa\u4e8e\u91cd\u8981\u6027\u7684\u526a\u679d\u65b9\u6cd5\u5728\u66f4\u9ad8\u526a\u679d\u56e0\u5b50\u4e0b\u53ef\u80fd\u5931\u8d25\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u7535\u5b50\u7ed3\u6784\u6570\u636e\u96c6\u7684\u9ad8\u5ea6\u5197\u4f59\u6027\uff0c\u6311\u6218\u4e86\u5bf9\u5927\u578b\u6570\u636e\u96c6\u7684\u4f9d\u8d56\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8986\u76d6\u7387\u7684\u526a\u679d\u7b56\u7565\uff0c\u80fd\u5728\u663e\u8457\u51cf\u5c11\u6570\u636e\u91cf\u548c\u8bad\u7ec3\u65f6\u95f4\u7684\u540c\u6642\uff0c\u4fdd\u6301\u5316\u5b66\u7cbe\u5ea6\u548c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.09747", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2507.09747", "abs": "https://arxiv.org/abs/2507.09747", "authors": ["Dongyang Li", "Haoyang Qin", "Mingyang Wu", "Chen Wei", "Quanying Liu"], "title": "BrainFLORA: Uncovering Brain Concept Representation via Multimodal Neural Embeddings", "comment": "10 pages, ACM MM 2025", "summary": "Understanding how the brain represents visual information is a fundamental\nchallenge in neuroscience and artificial intelligence. While AI-driven decoding\nof neural data has provided insights into the human visual system, integrating\nmultimodal neuroimaging signals, such as EEG, MEG, and fMRI, remains a critical\nhurdle due to their inherent spatiotemporal misalignment. Current approaches\noften analyze these modalities in isolation, limiting a holistic view of neural\nrepresentation. In this study, we introduce BrainFLORA, a unified framework for\nintegrating cross-modal neuroimaging data to construct a shared neural\nrepresentation. Our approach leverages multimodal large language models (MLLMs)\naugmented with modality-specific adapters and task decoders, achieving\nstate-of-the-art performance in joint-subject visual retrieval task and has the\npotential to extend multitasking. Combining neuroimaging analysis methods, we\nfurther reveal how visual concept representations align across neural\nmodalities and with real world object perception. We demonstrate that the\nbrain's structured visual concept representations exhibit an implicit mapping\nto physical-world stimuli, bridging neuroscience and machine learning from\ndifferent modalities of neural imaging. Beyond methodological advancements,\nBrainFLORA offers novel implications for cognitive neuroscience and\nbrain-computer interfaces (BCIs). Our code is available at\nhttps://github.com/ncclab-sustech/BrainFLORA.", "AI": {"tldr": "BrainFLORA\u6846\u67b6\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6574\u5408EEG\u3001MEG\u548cfMRI\u7b49\u795e\u7ecf\u6210\u50cf\u6570\u636e\uff0c\u6784\u5efa\u5171\u4eab\u795e\u7ecf\u8868\u5f81\uff0c\u63d0\u5347\u4e86\u89c6\u89c9\u68c0\u7d22\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u4e86\u795e\u7ecf\u8868\u5f81\u4e0e\u73b0\u5b9e\u4e16\u754c\u7269\u4f53\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4e3a\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u548c\u8111\u673a\u63a5\u53e3\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u5f53\u524d\u7814\u7a76\u5728\u6574\u5408EEG\u3001MEG\u548cfMRI\u7b49\u8de8\u6a21\u6001\u795e\u7ecf\u6210\u50cf\u4fe1\u53f7\u65f6\u9762\u4e34\u65f6\u7a7a\u5931\u914d\u7684\u6311\u6218\uff0c\u5bfc\u81f4\u5206\u6790\u5e38\u88ab\u5b64\u7acb\u8fdb\u884c\uff0c\u9650\u5236\u4e86\u5bf9\u795e\u7ecf\u8868\u5f81\u7684\u6574\u4f53\u6027\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u6311\u6218\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u6570\u636e\u7684\u878d\u5408\u3002", "method": "BrainFLORA\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\uff0c\u5e76\u8f85\u4ee5\u7279\u5b9a\u6a21\u6001\u7684\u9002\u914d\u5668\u548c\u4efb\u52a1\u89e3\u7801\u5668\uff0c\u6765\u6574\u5408\u8de8\u6a21\u6001\u795e\u7ecf\u6210\u50cf\u6570\u636e\uff08\u5982EEG\u3001MEG\u548cfMRI\uff09\uff0c\u4ee5\u6784\u5efa\u5171\u4eab\u7684\u795e\u7ecf\u8868\u5f81\u3002", "result": "BrainFLORA\u5728\u8de8\u4e3b\u4f53\u7684\u89c6\u89c9\u68c0\u7d22\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u89c6\u89c9\u6982\u5ff5\u8868\u5f81\u5728\u4e0d\u540c\u795e\u7ecf\u6a21\u6001\u4e4b\u95f4\u4ee5\u53ca\u4e0e\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u611f\u77e5\u4e4b\u95f4\u5b58\u5728\u5bf9\u9f50\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u5927\u8111\u89c6\u89c9\u6982\u5ff5\u8868\u5f81\u4e0e\u7269\u7406\u4e16\u754c\u523a\u6fc0\u4e4b\u95f4\u5b58\u5728\u9690\u5f0f\u6620\u5c04\u3002", "conclusion": "BrainFLORA \u6846\u67b6\u901a\u8fc7\u6574\u5408\u8de8\u6a21\u6001\u795e\u7ecf\u6210\u50cf\u6570\u636e\uff0c\u6784\u5efa\u4e86\u5171\u4eab\u7684\u795e\u7ecf\u8868\u5f81\uff0c\u5b9e\u73b0\u4e86\u8de8\u4e3b\u4f53\u7684\u89c6\u89c9\u68c0\u7d22\u4efb\u52a1\u7684\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u89c6\u89c9\u6982\u5ff5\u8868\u5f81\u5728\u4e0d\u540c\u795e\u7ecf\u6a21\u6001\u95f4\u4ee5\u53ca\u4e0e\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u611f\u77e5\u4e2d\u7684\u5bf9\u9f50\u65b9\u5f0f\u3002\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5927\u8111\u7ed3\u6784\u5316\u7684\u89c6\u89c9\u6982\u5ff5\u8868\u5f81\u4e0e\u7269\u7406\u4e16\u754c\u523a\u6fc0\u5b58\u5728\u9690\u5f0f\u6620\u5c04\u5173\u7cfb\uff0c\u4e3a\u8ba4\u77e5\u795e\u7ecf\u79d1\u5b66\u548c\u8111\u673a\u63a5\u53e3\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\u3002"}}
{"id": "2507.08930", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2507.08930", "abs": "https://arxiv.org/abs/2507.08930", "authors": ["Adrien Kahn", "Luca Gravina", "Filippo Vicentini"], "title": "Variational subspace methods and application to improving variational Monte Carlo dynamics", "comment": "33 pages, 6 figures", "summary": "We present a formalism that allows for the direct manipulation and\noptimization of subspaces, circumventing the need to optimize individual states\nwhen using subspace methods. Using the determinant state mapping, we can\nnaturally extend notions such as distance and energy to subspaces, as well as\nMonte Carlo estimators, recovering the excited states estimation method\nproposed by Pfau et al. As a practical application, we then introduce Bridge, a\nmethod that improves the performance of variational dynamics by extracting\nlinear combinations of variational time-evolved states. We find that Bridge is\nboth computationally inexpensive and capable of significantly mitigating the\nerrors that arise from discretizing the dynamics, and can thus be\nsystematically used as a post-processing tool for variational dynamics.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\u4e3b\u4e49\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u548c\u4f18\u5316\u5b50\u7a7a\u95f4\uff0c\u907f\u514d\u4e86\u5728\u5b50\u7a7a\u95f4\u65b9\u6cd5\u4e2d\u4f7f\u7528\u4f18\u5316\u4e2a\u6001\u7684\u9700\u8981\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Bridge \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u53d8\u5206\u65f6\u95f4\u6f14\u5316\u72b6\u6001\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u63d0\u9ad8\u53d8\u5206\u52a8\u529b\u5b66\u7684\u6027\u80fd\u3002Bridge \u88ab\u8bc1\u660e\u53ef\u4ee5\u663e\u8457 Mitigate \u79bb\u6563\u5316\u52a8\u529b\u5b66\u4ea7\u751f\u7684\u8bef\u5dee\uff0c\u5e76\u53ef\u4ee5\u7cfb\u7edf\u5730\u4f5c\u4e3a\u53d8\u5206\u52a8\u529b\u5b66\u7684\u540e\u5904\u7406\u5de5\u5177\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u5b50\u7a7a\u95f4\u65b9\u6cd5\u4e2d\u4f7f\u7528\u4f18\u5316\u4e2a\u6001\u7684\u9700\u8981\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\u4e3b\u4e49\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f62\u5f0f\u4e3b\u4e49\uff0c\u53ef\u4ee5\u76f4\u63a5\u5904\u7406\u548c\u4f18\u5316\u5b50\u7a7a\u95f4\uff0c\u4ee5\u53ca\u4e00\u79cd\u540d\u4e3a Bridge \u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u63d0\u53d6\u53d8\u5206\u65f6\u95f4\u6f14\u5316\u72b6\u6001\u7684\u7ebf\u6027\u7ec4\u5408\u6765\u63d0\u9ad8\u53d8\u5206\u52a8\u529b\u5b66\u7684\u6027\u80fd\u3002", "result": "\u8be5\u5f62\u5f0f\u4e3b\u4e49\u80fd\u591f\u5c06\u8ddd\u79bb\u548c\u80fd\u91cf\u7b49\u6982\u5ff5\u81ea\u7136\u5730\u6269\u5c55\u5230\u5b50\u7a7a\u95f4\uff0c\u4ee5\u53ca\u9a6c\u5c14\u53ef\u592b\u94fe\u8499\u7279\u5361\u6d1b\u4f30\u8ba1\u91cf\uff0c\u5e76\u6062\u590d\u4e86 Pfau \u7b49\u4eba\u63d0\u51fa\u7684\u6fc0\u53d1\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002Bridge \u88ab\u8bc1\u660e\u53ef\u4ee5\u663e\u8457 Mitigate \u79bb\u6563\u5316\u52a8\u529b\u5b66\u4ea7\u751f\u7684\u8bef\u5dee\u3002", "conclusion": "Bridge\u662f\u4e00\u79cd\u8ba1\u7b97\u6210\u672c\u4f4e\u5ec9\u4e14\u80fd\u591f\u663e\u8457 Mitigate \u79bb\u6563\u5316\u52a8\u529b\u5b66\u4ea7\u751f\u7684\u8bef\u5dee\u7684 V.A.S.C. \u65b9\u6cd5\uff0c\u56e0\u6b64\u53ef\u4ee5\u7cfb\u7edf\u5730\u4f5c\u4e3a V.A.S.C. \u7684\u540e\u5904\u7406\u5de5\u5177\u3002"}}
{"id": "2507.08898", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08898", "abs": "https://arxiv.org/abs/2507.08898", "authors": ["Wenliang Shan", "Michael Fu", "Rui Yang", "Chakkrit", "Tantithamthavorn"], "title": "SEALGuard: Safeguarding the Multilingual Conversations in Southeast Asian Languages for LLM Software Systems", "comment": "Under Review at Information and Software Technology", "summary": "Safety alignment is critical for LLM-powered systems. While recent\nLLM-powered guardrail approaches such as LlamaGuard achieve high detection\naccuracy of unsafe inputs written in English (e.g., ``How to create a bomb?''),\nthey struggle with multilingual unsafe inputs. This limitation leaves LLM\nsystems vulnerable to unsafe and jailbreak prompts written in low-resource\nlanguages such as those in Southeast Asia. This paper introduces SEALGuard, a\nmultilingual guardrail designed to improve the safety alignment across diverse\nlanguages. It aims to address the multilingual safety alignment gap of existing\nguardrails and ensure effective filtering of unsafe and jailbreak prompts in\nLLM-powered systems. We adapt a general-purpose multilingual language model\ninto a multilingual guardrail using low-rank adaptation (LoRA). We construct\nSEALSBench, a large-scale multilingual safety alignment dataset containing over\n260,000 prompts in ten languages, including safe, unsafe, and jailbreak cases.\nWe evaluate SEALGuard against state-of-the-art guardrails such as LlamaGuard on\nthis benchmark. Our findings show that multilingual unsafe and jailbreak\nprompts substantially degrade the performance of the state-of-the-art\nLlamaGuard, which experiences a drop in Defense Success Rate (DSR) by 9% and\n18%, respectively, compared to its performance on English-only prompts. In\ncontrast, SEALGuard outperforms existing guardrails in detecting multilingual\nunsafe and jailbreak prompts, improving DSR by 48% over LlamaGuard and\nachieving the best DSR, precision, and F1-score. Our ablation study further\nreveals the contributions of adaptation strategies and model size to the\noverall performance of SEALGuard. SEALGuard advances the safety alignment of\nLLM systems by introducing an effective multilingual guardrail.", "AI": {"tldr": "SEALGuard\u662f\u4e00\u4e2a\u65b0\u7684\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680f\uff0c\u80fd\u6709\u6548\u68c0\u6d4b\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u548c\u8d8a\u72f1\u63d0\u793a\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86LLM\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u5b89\u5168\u62a4\u680f\u5728\u5904\u7406\u4f4e\u8d44\u6e90\u8bed\u8a00\uff08\u5982\u4e1c\u5357\u4e9a\u8bed\u8a00\uff09\u7684\u4e0d\u5b89\u5168\u8f93\u5165\u548c\u8d8a\u72f1\u63d0\u793a\u65b9\u9762\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u4ee5\u63d0\u9ad8LLM\u7cfb\u7edf\u7684\u591a\u8bed\u8a00\u5b89\u5168\u6027\u3002", "method": "\u4f7f\u7528\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u5c06\u901a\u7528\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u9002\u914d\u6210\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680f\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b260,000\u591a\u4e2a\u63d0\u793a\uff08\u6db5\u76d6\u5b89\u5168\u3001\u4e0d\u5b89\u5168\u548c\u8d8a\u72f1\u7c7b\u578b\uff09\u7684SEALSBench\u591a\u8bed\u8a00\u5b89\u5168\u5bf9\u9f50\u6570\u636e\u96c6\u3002", "result": "\u4e0e\u73b0\u6709\u7684LlamaGuard\u7b49\u5b89\u5168\u62a4\u680f\u76f8\u6bd4\uff0cSEALGuard\u5728\u68c0\u6d4b\u591a\u8bed\u8a00\u4e0d\u5b89\u5168\u548c\u8d8a\u72f1\u63d0\u793a\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5176\u9632\u5fa1\u6210\u529f\u7387\uff08DSR\uff09\u6bd4LlamaGuard\u9ad848%\uff0c\u5e76\u5728DSR\u3001\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u4e0a\u5747\u53d6\u5f97\u6700\u4f73\u7ed3\u679c\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u591a\u8bed\u8a00\u63d0\u793a\u4f1a\u663e\u8457\u964d\u4f4eLlamaGuard\u7684\u6027\u80fd\u3002", "conclusion": "SEALGuard\u901a\u8fc7\u5f15\u5165\u6709\u6548\u7684\u591a\u8bed\u8a00\u5b89\u5168\u62a4\u680f\uff0c\u63d0\u9ad8\u4e86LLM\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2507.08868", "categories": ["cs.GT", "cs.DC", "91B26", "J.1; J.4"], "pdf": "https://arxiv.org/pdf/2507.08868", "abs": "https://arxiv.org/abs/2507.08868", "authors": ["Benedikt Pittl", "Werner Mach", "Erich Schikuta"], "title": "A Survey on Bilateral Multi-Round Cloud-SLA Negotiation Strategies", "comment": "Preprint", "summary": "Today, static cloud markets where consumers purchase services directly from\nproviders are dominating. Thus, consumers neither negotiate the price nor the\ncharacteristics of the service. In recent years, providers have adopted more\ndynamic trading mechanisms, as e.g. Amazon's EC2 platform shows: In addition to\nthe reservation marketspace and the on-demand marketspace, Amazon offers a spot\nmarketspace where consumers can bid for virtual machines. This spot marketspace\nwas extended with spot blocks, and recently Amazon reworked the bidding\noptions. In addition, other cloud providers, such as Virtustream, adopt dynamic\ntrading mechanisms. The scientific community envisions autonomous multi-round\nnegotiations for realizing future cloud marketspaces. Consequently, consumers\nand providers exchange offers and counteroffers to reach an agreement. This\nhelps providers increase the utilization of their datacenters, while consumers\ncan purchase highly customized cloud services.\n  In the paper at hand, we present a survey on multi-round bilateral\nnegotiation strategies for trading cloud resources. Thus, we analyzed\npeer-reviewed articles in order to identify trends, gaps, similarities, and the\nscope of such negotiation strategies. In addition, we surveyed the formalism\nthat the scientific community uses to describe such strategies. Based on these\nfindings, we derived recommendations for creating and documenting bilateral\nmulti-round negotiation strategies to foster their implementation in the\nindustry.", "AI": {"tldr": "\u672c\u6587\u8c03\u7814\u4e86\u4e91\u670d\u52a1\u4ea4\u6613\u4e2d\u7684\u591a\u8f6e\u8c08\u5224\u7b56\u7565\uff0c\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u548c\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u5e76\u4e3a\u884c\u4e1a\u5e94\u7528\u63d0\u4f9b\u4e86\u5efa\u8bae\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u672a\u6765\u4e91\u5e02\u573a\u4e2d\u66f4\u4f18\u5316\u7684\u8d44\u6e90\u4ea4\u6613\uff0c\u63d0\u9ad8\u6570\u636e\u4e2d\u5fc3\u5229\u7528\u7387\u5e76\u5141\u8bb8\u6d88\u8d39\u8005\u8d2d\u4e70\u9ad8\u5ea6\u5b9a\u5236\u5316\u7684\u4e91\u670d\u52a1\u3002", "method": "\u901a\u8fc7\u5206\u6790\u540c\u884c\u8bc4\u5ba1\u7684\u6587\u7ae0\uff0c\u8bc6\u522b\u4e86\u591a\u8f6e\u53cc\u8fb9\u8c08\u5224\u7b56\u7565\u7684\u8d8b\u52bf\u3001\u7a7a\u767d\u3001\u76f8\u4f3c\u6027\u53ca\u8303\u56f4\uff0c\u5e76\u8c03\u67e5\u4e86\u7528\u4e8e\u63cf\u8ff0\u8fd9\u4e9b\u7b56\u7565\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u4e86\u4e91\u8d44\u6e90\u4ea4\u6613\u4e2d\u591a\u8f6e\u53cc\u8fb9\u8c08\u5224\u7b56\u7565\u7684\u73b0\u72b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4fc3\u8fdb\u5176\u5b9e\u65bd\u7684\u5efa\u8bae\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u5bf9\u4e91\u8d44\u6e90\u4ea4\u6613\u4e2d\u7684\u591a\u8f6e\u53cc\u8fb9\u8c08\u5224\u7b56\u7565\u8fdb\u884c\u4e86\u8c03\u7814\uff0c\u5206\u6790\u4e86\u73b0\u6709\u7814\u7a76\u7684\u8d8b\u52bf\u3001\u7a7a\u767d\u3001\u76f8\u4f3c\u6027\u53ca\u8303\u56f4\uff0c\u5e76\u8c03\u67e5\u4e86\u63cf\u8ff0\u8fd9\u4e9b\u7b56\u7565\u6240\u4f7f\u7528\u7684\u5f62\u5f0f\u5316\u65b9\u6cd5\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u8bba\u6587\u63d0\u51fa\u4e86\u521b\u5efa\u548c\u8bb0\u5f55\u53cc\u8fb9\u591a\u8f6e\u8c08\u5224\u7b56\u7565\u7684\u5efa\u8bae\uff0c\u4ee5\u4fc3\u8fdb\u5176\u5728\u884c\u4e1a\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09194", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09194", "abs": "https://arxiv.org/abs/2507.09194", "authors": ["Mohimenul Kabir", "Kuldeep S Meel"], "title": "A Simple and Effective ASP-Based Tool for Enumerating Minimal Hitting Sets", "comment": "To appear in Technical Communications (TC) of ICLP 2025", "summary": "The hitting set problem is a fundamental problem in computer science and\nmathematics. Given a family of sets over a universe of elements, a minimal\nhitting set is a subset-minimal collection of elements that intersects each set\nin the family. Enumerating all minimal hitting sets is crucial in various\nreal-world applications.\n  In this paper, we address the full enumeration of all minimal hitting sets\nfor a given family of sets. We formulate the problem using Answer Set\nProgramming (ASP) and leverage existing ASP solvers for efficient enumeration.\nWe propose an ASP-based tool, MinHit-ASP, and our empirical evaluation shows\nthat it effectively enumerates minimal hitting sets across benchmarks from\ndiverse problem domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMinHit-ASP\u7684\u5de5\u5177\uff0c\u5229\u7528ASP\u6280\u672f\u6709\u6548\u679a\u4e3e\u6240\u6709\u6700\u5c0f\u96c6\u5408\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u6700\u5c0f\u96c6\u5408\u679a\u4e3e\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u800c\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u7ed9\u5b9a\u96c6\u5408\u65cf\u7684\u6240\u6709\u6700\u5c0f\u96c6\u5408\u7684\u5b8c\u6574\u679a\u4e3e\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u6765\u89e3\u51b3\u6700\u5c0f\u96c6\u5408\u679a\u4e3e\u95ee\u9898\uff0c\u5e76\u5229\u7528\u73b0\u6709\u7684ASP\u6c42\u89e3\u5668\u8fdb\u884c\u9ad8\u6548\u679a\u4e3e\u3002", "result": "\u901a\u8fc7\u57fa\u4e8eASP\u7684\u5de5\u5177MinHit-ASP\uff0c\u80fd\u591f\u6709\u6548\u5730\u679a\u4e3e\u6700\u5c0f\u96c6\u5408\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eASP\u7684\u5de5\u5177MinHit-ASP\uff0c\u80fd\u591f\u6709\u6548\u5730\u679a\u4e3e\u96c6\u5408\u65cf\u7684\u6240\u6709\u6700\u5c0f\u96c6\u5408\u3002\u7ecf\u9a8c\u6027\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u5de5\u5177\u5728\u6765\u81ea\u4e0d\u540c\u95ee\u9898\u57df\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.08950", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.08950", "abs": "https://arxiv.org/abs/2507.08950", "authors": ["Xue Zhang", "Abla Kammoun", "Mohamed-Slim Alouini"], "title": "Fundamental limits via CRB of semi-blind channel estimation in Massive MIMO systems", "comment": null, "summary": "This paper investigates the asymptotic behavior of the deterministic and\nstochastic Cram\\'er-Rao Bounds (CRB) for semi-blind channel estimation in\nmassive multiple-input multiple-output (MIMO) systems. We derive and analyze\nmathematically tractable expressions for both metrics under various asymptotic\nregimes, which govern the growth rates of the number of antennas, the number of\nusers, the training sequence length, and the transmission block length. Unlike\nthe existing work, our results show that the CRB can be made arbitrarily small\nas the transmission block length increases, but only when the training sequence\nlength grows at the same rate and the number of users remains fixed. However,\nif the number of training sequences remains proportional to the number of\nusers, the channel estimation error is always lower-bounded by a non-vanishing\nconstant. Numerical results are presented to support our findings and\ndemonstrate the advantages of semi-blind channel estimation in reducing the\nrequired number of training sequences.", "AI": {"tldr": "\u5927\u89c4\u6a21MIMO\u7cfb\u7edf\u4e2d\uff0c\u589e\u52a0\u4f20\u8f93\u5757\u957f\u5ea6\u53ef\u51cf\u5c0f\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\uff08CRB\uff09\uff0c\u4f46\u9700\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u540c\u6b65\u589e\u957f\u4e14\u7528\u6237\u6570\u4e0d\u53d8\uff1b\u82e5\u8bad\u7ec3\u5e8f\u5217\u6570\u4e0e\u7528\u6237\u6570\u6210\u6b63\u6bd4\uff0c\u5219\u8bef\u5dee\u6709\u4e0b\u9650\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5927\u89c4\u6a21\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u7cfb\u7edf\u4e2d\u534a\u76f2\u4fe1\u9053\u4f30\u8ba1\u7684\u6e10\u8fdb\u884c\u4e3a\uff0c\u5e76\u4e3a\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u7684\u754c\u9650\u63d0\u4f9b\u7406\u8bba\u4f9d\u636e\u3002", "method": "\u672c\u6587\u901a\u8fc7\u63a8\u5bfc\u548c\u5206\u6790\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u514b\u62c9\u7f8e-\u7f57\u754c\uff08CRB\uff09\u5728\u591a\u79cd\u6e10\u8fdb\u60c5\u51b5\u4e0b\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\u6765\u7814\u7a76\u5176\u6e10\u8fdb\u884c\u4e3a\uff0c\u8fd9\u4e9b\u60c5\u51b5\u8003\u8651\u4e86\u5929\u7ebf\u6570\u91cf\u3001\u7528\u6237\u6570\u91cf\u3001\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u548c\u4f20\u8f93\u5757\u957f\u5ea6\u7684\u589e\u957f\u7387\u3002", "result": "\u7814\u7a76\u5f97\u51fa\u4e86\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u514b\u62c9\u7f8e-\u7f57\u754c\uff08CRB\uff09\u5728\u4e0d\u540c\u6e10\u8fdb\u60c5\u51b5\u4e0b\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4f20\u8f93\u5757\u957f\u5ea6\u7684\u589e\u52a0\u53ef\u4ee5\u51cf\u5c0fCRB\uff0c\u4f46\u524d\u63d0\u662f\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u540c\u6b65\u589e\u957f\u4e14\u7528\u6237\u6570\u91cf\u56fa\u5b9a\u3002\u5f53\u8bad\u7ec3\u5e8f\u5217\u6570\u91cf\u4e0e\u7528\u6237\u6570\u91cf\u6210\u6bd4\u4f8b\u589e\u957f\u65f6\uff0c\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u5b58\u5728\u4e00\u4e2a\u6052\u5b9a\u7684\u4e0b\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u7528\u6237\u6570\u91cf\u56fa\u5b9a\u4e14\u8bad\u7ec3\u5e8f\u5217\u957f\u5ea6\u4e0e\u4f20\u8f93\u5757\u957f\u5ea6\u540c\u6b65\u589e\u957f\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u901a\u8fc7\u589e\u52a0\u4f20\u8f93\u5757\u957f\u5ea6\u6765\u51cf\u5c0f\u514b\u62c9\u7f8e-\u7f57\u754c\uff08CRB\uff09\u3002\u7136\u800c\uff0c\u5982\u679c\u8bad\u7ec3\u5e8f\u5217\u7684\u6570\u91cf\u4e0e\u7528\u6237\u6570\u91cf\u6210\u6b63\u6bd4\u589e\u957f\uff0c\u4fe1\u9053\u4f30\u8ba1\u8bef\u5dee\u5c06\u6709\u4e00\u4e2a\u65e0\u6cd5\u6d88\u9664\u7684\u4e0b\u9650\u3002"}}
{"id": "2507.08944", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08944", "abs": "https://arxiv.org/abs/2507.08944", "authors": ["Enhao Zhang", "Erkang Zhu", "Gagan Bansal", "Adam Fourney", "Hussein Mozannar", "Jack Gerrits"], "title": "Optimizing Sequential Multi-Step Tasks with Parallel LLM Agents", "comment": "ICML 2025 Workshop on MAS", "summary": "Large language model (LLM)-based multi-agent systems have demonstrated\nremarkable promise for tackling complex tasks by breaking them down into\nsubtasks that are iteratively planned, executed, observed, and refined. Despite\ntheir effectiveness, these systems often incur high latency because real-world\nproblems frequently demand multiple iterative cycles of reasoning steps. To\naddress this challenge, we propose M1-Parallel, a framework that concurrently\nruns multiple multi-agent teams in parallel to uncover distinct solution paths.\nBy leveraging an event-driven communication model with asynchronous messaging,\nM1-Parallel efficiently capitalizes on the inherent diversity of valid plans to\neither reduce end-to-end latency or boost task completion rates. Our\nexperiments on complex tasks show that M1-Parallel with early termination\nachieves up to $2.2\\times$ speedup while preserving accuracy, and that\nM1-Parallel with aggregation yields higher task completion rates. We further\ninvestigate strategies aimed at encouraging diverse execution plans but observe\nno additional performance gains over repeated sampling. Overall, these findings\nunderscore the potential of parallel plan execution for optimizing multi-agent\nsystems for real-world, high-complexity reasoning tasks.", "AI": {"tldr": "M1-Parallel\u6846\u67b6\u901a\u8fc7\u5e76\u884c\u6267\u884c\u591a\u667a\u80fd\u4f53\u56e2\u961f\u6765\u964d\u4f4eLLM\u7cfb\u7edf\u5ef6\u8fdf\uff0c\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57fa\u4e8eLLM\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u56e0\u9700\u8981\u591a\u4e2a\u8fed\u4ee3\u63a8\u7406\u5468\u671f\u800c\u5bfc\u81f4\u7684\u9ad8\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM1-Parallel\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e8b\u4ef6\u9a71\u52a8\u7684\u901a\u4fe1\u6a21\u578b\u548c\u5f02\u6b65\u6d88\u606f\u4f20\u9012\uff0c\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u591a\u667a\u80fd\u4f53\u56e2\u961f\u6765\u63a2\u7d22\u4e0d\u540c\u7684\u89e3\u51b3\u65b9\u6848\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cM1-Parallel\u7ed3\u5408\u63d0\u524d\u7ec8\u6b62\u7b56\u7565\u53ef\u5b9e\u73b0\u9ad8\u8fbe2.2\u500d\u7684\u901f\u5ea6\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff1b\u7ed3\u5408\u805a\u5408\u7b56\u7565\u53ef\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u9f13\u52b1\u6267\u884c\u8ba1\u5212\u591a\u6837\u6027\u5e76\u672a\u5e26\u6765\u989d\u5916\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "M1-Parallel\u6846\u67b6\u901a\u8fc7\u5e76\u884c\u8fd0\u884c\u591a\u4e2a\u591a\u667a\u80fd\u4f53\u56e2\u961f\u6765\u89e3\u51b3\u9ad8\u5ef6\u8fdf\u95ee\u9898\uff0c\u901a\u8fc7\u63d0\u524d\u7ec8\u6b62\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u8fbe2.2\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff1b\u901a\u8fc7\u805a\u5408\u53ef\u4ee5\u63d0\u9ad8\u4efb\u52a1\u5b8c\u6210\u7387\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u9f13\u52b1\u6267\u884c\u8ba1\u5212\u591a\u6837\u6027\u5e76\u4e0d\u80fd\u5e26\u6765\u989d\u5916\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.08829", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08829", "abs": "https://arxiv.org/abs/2507.08829", "authors": ["Kimia Soroush", "Nastaran Shirazi", "Mohsen Raji"], "title": "Efficient Triple Modular Redundancy for Reliability Enhancement of DNNs Using Explainable AI", "comment": null, "summary": "Deep Neural Networks (DNNs) are widely employed in safety-critical domains,\nwhere ensuring their reliability is essential. Triple Modular Redundancy (TMR)\nis an effective technique to enhance the reliability of DNNs in the presence of\nbit-flip faults. In order to handle the significant overhead of TMR, it is\napplied selectively on the parameters and components with the highest\ncontribution at the model output. Hence, the accuracy of the selection\ncriterion plays the key role on the efficiency of TMR. This paper presents an\nefficient TMR approach to enhance the reliability of DNNs against bit-flip\nfaults using an Explainable Artificial Intelligence (XAI) method. Since XAI can\nprovide valuable insights about the importance of individual neurons and\nweights in the performance of the network, they can be applied as the selection\nmetric in TMR techniques. The proposed method utilizes a low-cost,\ngradient-based XAI technique known as Layer-wise Relevance Propagation (LRP) to\ncalculate importance scores for DNN parameters. These scores are then used to\nenhance the reliability of the model, with the most critical weights being\nprotected by TMR. The proposed approach is evaluated on two DNN models, VGG16\nand AlexNet, using datasets such as MNIST and CIFAR-10. The results demonstrate\nthat the method can protect the AlexNet model at a bit error rate of 10-4,\nachieving over 60% reliability improvement while maintaining the same overhead\nas state-of-the-art methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eXAI\uff08LRP\uff09\u7684TMR\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u5173\u952e\u6743\u91cd\u6765\u63d0\u9ad8DNN\u53ef\u9760\u6027\uff0c\u5728AlexNet\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e8660%\u4ee5\u4e0a\u7684\u53ef\u9760\u6027\u63d0\u5347\uff0c\u4e14\u5f00\u9500\u53ef\u63a7\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u5e7f\u6cdb\u5e94\u7528\uff0c\u786e\u4fdd\u5176\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002\u4e09\u91cd\u6a21\u5197\u4f59\uff08TMR\uff09\u662f\u4e00\u79cd\u6709\u6548\u63d0\u9ad8DNN\u53ef\u9760\u6027\u7684\u6280\u672f\uff0c\u4f46\u5176\u5f00\u9500\u8f83\u5927\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684TMR\u9009\u62e9\u6807\u51c6\u6765\u5904\u7406\u5176\u5f00\u9500\u95ee\u9898\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528XAI\u6280\u672f\u63d0\u4f9b\u5173\u4e8e\u795e\u7ecf\u5143\u548c\u6743\u91cd\u91cd\u8981\u6027\u7684\u89c1\u89e3\uff0c\u5c06\u5176\u4f5c\u4e3aTMR\u9009\u62e9\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u7684\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u65b9\u6cd5\uff0c\u5177\u4f53\u4e3a\u5c42\u5c42\u76f8\u5173\u6027\u4f20\u64ad\uff08LRP\uff09\uff0c\u7528\u4e8e\u8ba1\u7b97DNN\u53c2\u6570\u7684\u91cd\u8981\u6027\u5f97\u5206\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5f97\u5206\u6765\u9009\u62e9\u9700\u8981TMR\u4fdd\u62a4\u7684\u5173\u952e\u6743\u91cd\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728VGG16\u548cAlexNet\u6a21\u578b\u4ee5\u53caMNIST\u548cCIFAR-10\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4fdd\u62a4AlexNet\u6a21\u578b\uff0c\u5728\u6bd4\u7279\u9519\u8bef\u7387\u4e3a10^-4\u65f6\uff0c\u53ef\u9760\u6027\u63d0\u9ad8\u4e8660%\u4ee5\u4e0a\uff0c\u540c\u65f6\u5f00\u9500\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7684\u4f4e\u6210\u672c\u4e09\u91cd\u6a21\u5197\u4f59\uff08TMR\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5bf9\u6297\u6bd4\u7279\u7ffb\u8f6c\u9519\u8bef\u7684\u80fd\u529b\u3002\u901a\u8fc7\u4f7f\u7528\u5c42\u5c42\u76f8\u5173\u6027\u4f20\u64ad\uff08LRP\uff09\u8ba1\u7b97DNN\u53c2\u6570\u7684\u91cd\u8981\u6027\u5f97\u5206\uff0c\u5e76\u4ee5\u6b64\u4e3a\u4f9d\u636e\u9009\u62e9TMR\u4fdd\u62a4\u7684\u5173\u952e\u6743\u91cd\uff0c\u5b9e\u73b0\u4e86\u5728\u4fdd\u6301\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u5f53\u7684\u5f00\u9500\u4e0b\uff0c\u5c06AlexNet\u6a21\u578b\u5728\u6bd4\u7279\u9519\u8bef\u7387\u4e3a10^-4\u65f6\u7684\u53ef\u9760\u6027\u63d0\u9ad8\u4e8660%\u4ee5\u4e0a\u3002"}}
{"id": "2507.08953", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.08953", "abs": "https://arxiv.org/abs/2507.08953", "authors": ["Thibault Charpentier", "Anton Khvalyuk", "Lev Ioffe", "Mikhail Feigel'man", "Nicolas Roch", "Benjamin Sac\u00e9p\u00e9"], "title": "Universal scaling of microwave dissipation in superconducting circuits", "comment": null, "summary": "Improving the coherence of superconducting qubits is essential for advancing\nquantum technologies. While superconductors are theoretically perfect\nconductors, they consistently exhibit residual energy dissipation when driven\nby microwave currents, limiting coherence times. Here, we report a universal\nscaling between microwave dissipation and the superfluid density, a bulk\nproperty of superconductors related to charge carrier density and disorder. Our\nanalysis spans a wide range of superconducting materials and device geometries,\nfrom highly disordered amorphous films to ultra-clean systems with record-high\nquality factors, including resonators, 3D cavities, and transmon qubits. This\nscaling reveals an intrinsic bulk dissipation channel, independent of surface\ndielectric losses, that originates from a universal density of nonequilibrium\nquasiparticles trapped within disorder-induced spatial variations of the\nsuperconducting gap. Our findings define a fundamental limit to coherence set\nby intrinsic material properties and provide a predictive framework for\nselecting materials and the design of next-generation superconducting quantum\ncircuits.", "AI": {"tldr": "\u8d85\u5bfc\u4f53\u7684\u5fae\u6ce2\u8017\u6563\u4e0e\u5176\u8d85\u6d41\u4f53\u5bc6\u5ea6\u76f8\u5173\uff0c\u8fd9\u63ed\u793a\u4e86\u4e00\u79cd\u672c\u5f81\u8017\u6563\u673a\u5236\uff0c\u53ef\u80fd\u9650\u5236\u91cf\u5b50\u6bd4\u7279\u7684\u76f8\u5e72\u6027\u3002", "motivation": "\u63d0\u9ad8\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u7684\u76f8\u5e72\u6027\u5bf9\u4e8e\u63a8\u8fdb\u91cf\u5b50\u6280\u672f\u81f3\u5173\u91cd\u8981\uff0c\u800c\u8d85\u5bfc\u4f53\u5728\u5fae\u6ce2\u9a71\u52a8\u4e0b\u8868\u73b0\u51fa\u7684\u6b8b\u4f59\u80fd\u91cf\u8017\u6563\u9650\u5236\u4e86\u76f8\u5e72\u65f6\u95f4\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5927\u91cf\u8d85\u5bfc\u6750\u6599\u548c\u5668\u4ef6\u51e0\u4f55\u5f62\u72b6\u7684\u5fae\u6ce2\u8017\u6563\u4e0e\u8d85\u6d41\u4f53\u5bc6\u5ea6\u7684\u666e\u904d\u6807\u5ea6\u5173\u7cfb\u6765\u7814\u7a76\u3002", "result": "\u53d1\u73b0\u5fae\u6ce2\u8017\u6563\u4e0e\u8d85\u6d41\u4f53\u5bc6\u5ea6\u4e4b\u95f4\u5b58\u5728\u666e\u904d\u7684\u6807\u5ea6\u5173\u7cfb\uff0c\u8be5\u5173\u7cfb\u8de8\u8d8a\u4e86\u591a\u79cd\u8d85\u5bfc\u6750\u6599\u548c\u5668\u4ef6\u51e0\u4f55\u5f62\u72b6\uff0c\u63ed\u793a\u4e86\u4e00\u79cd\u672c\u5f81\u7684\u4f53\u8017\u6563\u901a\u9053\uff0c\u8be5\u901a\u9053\u662f\u7531\u4e8e\u975e\u5e73\u8861\u51c6\u7c92\u5b50\u5bc6\u5ea6\u4ee5\u53ca\u8d85\u5bfc\u95f4\u9699\u7684\u7a7a\u95f4\u53d8\u5316\u5f15\u8d77\u7684\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u79cd\u666e\u904d\u5b58\u5728\u7684\u672c\u5f81\u4f53\u8017\u6563\u901a\u9053\uff0c\u8be5\u901a\u9053\u72ec\u7acb\u4e8e\u8868\u9762\u4ecb\u7535\u635f\u8017\uff0c\u6e90\u4e8e\u7531\u8d85\u5bfc\u95f4\u9699\u7684\u65e0\u5e8f\u5f15\u8d77\u7684\u7a7a\u95f4\u53d8\u5316\u4e2d\u6355\u83b7\u7684\u975e\u5e73\u8861\u51c6\u7c92\u5b50\u7684\u666e\u904d\u5bc6\u5ea6\u3002\u8fd9\u4e3a\u9009\u62e9\u6750\u6599\u548c\u8bbe\u8ba1\u4e0b\u4e00\u4ee3\u8d85\u5bfc\u91cf\u5b50\u7535\u8def\u63d0\u4f9b\u4e86\u57fa\u7840\u6750\u6599\u5c5e\u6027\u7684\u6839\u672c\u9650\u5236\u548c\u9884\u6d4b\u6846\u67b6\u3002"}}
{"id": "2507.09426", "categories": ["cs.DS", "cs.CC", "cs.DM", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09426", "abs": "https://arxiv.org/abs/2507.09426", "authors": ["Naonori Kakimura", "P\u00e9ter Madarasi", "Jannik Matuschke", "Kitti Varga"], "title": "Simultaneous Network Design with Restricted Link Usage", "comment": null, "summary": "Given a digraph with two terminal vertices $s$ and $t$ as well as a\nconservative cost function and several not necessarily disjoint color classes\non its arc set, our goal is to find a minimum-cost subset of the arcs such that\nits intersection with each color class contains an $s$-$t$ dipath. Problems of\nthis type arise naturally in multi-commodity network design settings where each\ncommodity is restricted to use links of its own color only.\n  We study several variants of the problem, deriving strong hardness results\neven for restricted cases, but we also identify cases that can be solved in\npolynomial time. The latter ones include the cases where the color classes form\na laminar family, or where the underlying digraph is acyclic and the number of\ncolor classes is constant. We also present an FPT algorithm for the general\ncase parameterized by the number of multi-colored arcs.", "AI": {"tldr": "\u5728\u591a\u5546\u54c1\u7f51\u7edc\u8bbe\u8ba1\u4e2d\uff0c\u9700\u8981\u627e\u5230\u6700\u5c0f\u6210\u672c\u7684\u5f27\u5b50\u96c6\uff0c\u4f7f\u5f97\u6bcf\u4e2a\u989c\u8272\u7c7b\u522b\u7684\u4ea4\u96c6\u5305\u542b\u4e00\u4e2as-t dipath\u3002\u7814\u7a76\u4e86\u95ee\u9898\u7684\u53d8\u4f53\uff0c\u5f97\u5230\u4e86\u786c\u5ea6\u7ed3\u679c\uff0c\u5e76\u786e\u5b9a\u4e86\u51e0\u79cd\u53ef\u884c\u7684\u7279\u6b8a\u60c5\u51b5\u548c\u4e00\u79cd\u9488\u5bf9\u4e00\u822c\u60c5\u51b5\u7684 FPT \u7b97\u6cd5\u3002", "motivation": "\u5728\u591a\u5546\u54c1\u7f51\u7edc\u8bbe\u8ba1\u73af\u5883\u4e2d\uff0c\u6bcf\u4e2a\u5546\u54c1\u4ec5\u9650\u4e8e\u4f7f\u7528\u5176\u81ea\u8eab\u989c\u8272\u7684\u94fe\u63a5\uff0c\u9700\u8981\u627e\u5230\u4e00\u4e2a\u6700\u5c0f\u6210\u672c\u7684\u5f27\u5b50\u96c6\uff0c\u8be5\u5b50\u96c6\u4e0e\u6bcf\u4e2a\u989c\u8272\u7c7b\u522b\u7684\u4ea4\u96c6\u5305\u542b\u4e00\u4e2as-t dipath\u3002", "method": "\u7814\u7a76\u4e86\u8be5\u95ee\u9898\u7684\u51e0\u4e2a\u53d8\u4f53\uff0c\u63a8\u5bfc\u4e86\u5f3a\u786c\u5ea6\u7ed3\u679c\uff0c\u5373\u4f7f\u662f\u5bf9\u4e8e\u53d7\u9650\u7684\u60c5\u51b5\uff0c\u4e5f\u786e\u5b9a\u4e86\u53ef\u4ee5\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u65f6\u95f4\u591a\u9879\u5f0f\u3002", "result": "\u63a8\u5bfc\u4e86\u5f3a\u786c\u5ea6\u7ed3\u679c\uff0c\u5e76\u786e\u5b9a\u4e86\u53ef\u4ee5\u89e3\u51b3\u8be5\u95ee\u9898\u7684\u65f6\u95f4\u591a\u9879\u5f0f\u3002", "conclusion": "\u8be5\u95ee\u9898\u5b58\u5728\u4e00\u4e9b\u7279\u6b8a\u60c5\u51b5\u7684\u53ef\u884c\u89e3\u6cd5\uff0c\u4f8b\u5982\u989c\u8272\u7c7b\u522b\u6784\u6210\u5c42\u7ea7\u5bb6\u65cf\u6216\u57fa\u7840\u6709\u5411\u56fe\u4e3a\u65e0\u73af\u56fe\u4e14\u989c\u8272\u7c7b\u522b\u6570\u91cf\u6052\u5b9a\u65f6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u591a\u8272\u5f27\u6570\u91cf\u53c2\u6570\u5316\u7684\u4e00\u822c\u60c5\u51b5\u7684 FPT \u7b97\u6cd5\u3002"}}
{"id": "2507.09146", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.09146", "abs": "https://arxiv.org/abs/2507.09146", "authors": ["Ryuichi Miyauchi", "Hengyuan Chang", "Tsukasa Fukusato", "Kazunori Miyata", "Haoran Xie"], "title": "Physics-Aware Fluid Field Generation from User Sketches Using Helmholtz-Hodge Decomposition", "comment": "8 pages, 12 figures. In proceedings of NICOGRAPH International 2025", "summary": "Fluid simulation techniques are widely used in various fields such as film\nproduction, but controlling complex fluid behaviors remains challenging. While\nrecent generative models enable intuitive generation of vector fields from user\nsketches, they struggle to maintain physical properties such as\nincompressibility. To address these issues, this paper proposes a method for\ninteractively designing 2D vector fields. Conventional generative models can\nintuitively generate vector fields from user sketches, but remain difficult to\nconsider physical properties. Therefore, we add a simple editing process after\ngenerating the vector field. In the first stage, we use a latent diffusion\nmodel~(LDM) to automatically generate initial 2D vector fields from user\nsketches. In the second stage, we apply the Helmholtz-Hodge decomposition to\nlocally extract physical properties such as incompressibility from the results\ngenerated by LDM and recompose them according to user intentions. Through\nmultiple experiments, we demonstrate the effectiveness of our proposed method.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u4e86\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4ea5\u59c6\u970d\u5947\u5206\u89e3\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6839\u636e\u7528\u6237\u8349\u56fe\u4ea4\u4e92\u5f0f\u5730\u8bbe\u8ba1\u5177\u6709\u7269\u7406\u5c5e\u6027\uff08\u5982\u4e0d\u53ef\u538b\u7f29\u6027\uff09\u76842D\u77e2\u91cf\u573a\u3002", "motivation": "\u73b0\u6709\u7684\u77e2\u91cf\u573a\u751f\u6210\u65b9\u6cd5\u867d\u7136\u80fd\u6839\u636e\u7528\u6237\u8349\u56fe\u76f4\u89c2\u5730\u751f\u6210\u77e2\u91cf\u573a\uff0c\u4f46\u5728\u4fdd\u6301\u4e0d\u53ef\u538b\u7f29\u6027\u7b49\u7269\u7406\u5c5e\u6027\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u517c\u987e\u7528\u6237\u610f\u56fe\u548c\u7269\u7406\u5c5e\u6027\u7684\u77e2\u91cf\u573a\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u4ece\u7528\u6237\u8349\u56fe\u81ea\u52a8\u751f\u6210\u521d\u59cb2D\u77e2\u91cf\u573a\uff1b\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u4ea5\u59c6\u970d\u5947\u5206\u89e3\u63d0\u53d6\u548c\u91cd\u7ec4\u7269\u7406\u5c5e\u6027\uff08\u5982\u4e0d\u53ef\u538b\u7f29\u6027\uff09\uff0c\u4ee5\u7b26\u5408\u7528\u6237\u610f\u56fe\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u6ee1\u8db3\u7528\u6237\u610f\u56fe\u4e14\u5177\u6709\u7269\u7406\u5c5e\u6027\uff08\u5982\u4e0d\u53ef\u538b\u7f29\u6027\uff09\u76842D\u77e2\u91cf\u573a\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u4ea5\u59c6\u970d\u5947\u5206\u89e3\uff0c\u5b9e\u73b0\u4e86\u4ece\u7528\u6237\u8349\u56fe\u5230\u7269\u7406\u5c5e\u6027\u53ef\u63a7\u76842D\u77e2\u91cf\u573a\u7684\u4ea4\u4e92\u5f0f\u8bbe\u8ba1\u3002"}}
{"id": "2507.08875", "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "pdf": "https://arxiv.org/pdf/2507.08875", "abs": "https://arxiv.org/abs/2507.08875", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u6807\u51c6\u8bc4\u4f30\uff08MCA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u4e24\u79cd\u865a\u62df\u5dee\u8ddd\u5206\u6790\uff08VGA\uff09\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u63d0\u9ad8\u8bc4\u4f30\u7684\u6548\u7387\u3001\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u5b9a\u6027\u548c\u5b9a\u91cf\u6807\u51c6\u4ee5\u53ca\u5f02\u8d28\u6027\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u591a\u6807\u51c6\u8bc4\u4f30\uff08MCA\uff09\u65b9\u6cd5\uff08\u5982DEA\u3001SFA\u548cMCDM\uff09\u5728\u5904\u7406\u5b9a\u6027\u548c\u5b9a\u91cf\u6807\u51c6\u3001\u5f02\u8d28\u6027\u5047\u8bbe\u4ee5\u53ca\u4e3b\u89c2\u6027\u5f71\u54cd\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u5e76\u786e\u5b9a\u6700\u5408\u9002\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u79cd\u865a\u62df\u5dee\u8ddd\u5206\u6790\uff08VGA\uff09\u6a21\u578b\u7684\u65b0\u578b\u591a\u6807\u51c6\u8bc4\u4f30\uff08MCA\uff09\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\uff0c\u5e76\u5305\u542b\u4e24\u4e2a\u5168\u9762\u7684\u6570\u503c\u793a\u4f8b\u6765\u5c55\u793a\u5176\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u578bMCA\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u786e\u4fdd\u4e86\u8bc4\u4f30\u7684\u5168\u9762\u6027\u548c\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u4e2a\u5168\u9762\u7684\u6570\u503c\u793a\u4f8b\u6765\u8bc1\u660e\u5176\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u79cd\u865a\u62df\u5dee\u8ddd\u5206\u6790\uff08VGA\uff09\u6a21\u578b\u7684\u65b0\u578b\u591a\u6807\u51c6\u8bc4\u4f30\uff08MCA\uff09\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709MCA\u65b9\u6cd5\u5728\u5904\u7406\u5b9a\u6027\u548c\u5b9a\u91cf\u6807\u51c6\u3001\u5f02\u8d28\u6027\u5047\u8bbe\u4ee5\u53ca\u4e3b\u89c2\u6027\u5f71\u54cd\u65b9\u9762\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\uff0c\u65e8\u5728\u63d0\u9ad8\u8bc4\u4f30\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\uff0c\u4f7f\u5176\u66f4\u5168\u9762\u548c\u53ef\u9760\u3002"}}
{"id": "2507.09495", "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09495", "abs": "https://arxiv.org/abs/2507.09495", "authors": ["Hang Wang", "Junshan Zhang"], "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "comment": "Position paper", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks.", "AI": {"tldr": "\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u5c06\u4f7f\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u80fd\u591f\u9884\u6d4b\u548c\u534f\u8c03\uff0c\u800c\u4e0d\u662f\u4ec5\u4ec5\u505a\u51fa\u53cd\u5e94\u3002", "motivation": "\u5f53\u524d\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5e94\u5bf9\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\u7684\u6311\u6218\uff0c\u5982\u6307\u6570\u7ea7\u589e\u957f\u7684\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u3001\u975e\u5e73\u7a33\u73af\u5883\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u4ecd\u7136\u662f\u88ab\u52a8\u7684\uff0c\u5e76\u4e14\u5728\u9762\u5bf9\u65b0\u9896\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u53cd\u5e94\u5f0f\u5411\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5c06\u667a\u80fd\u4f53\u89c6\u4e3a\u80fd\u591f\u5408\u6210\u590d\u6742\u591a\u667a\u80fd\u4f53\u52a8\u6001\u5e76\u57fa\u4e8e\u5bf9\u672a\u6765\u4ea4\u4e92\u7684\u9884\u6d4b\u6027\u7406\u89e3\u505a\u51fa\u9884\u671f\u51b3\u7b56\u7684\u751f\u6210\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u4e3b\u52a8\u51b3\u7b56\u3001\u901a\u8fc7\u589e\u5f3a\u7684\u901a\u4fe1\u5b9e\u73b0\u65e0\u7f1d\u534f\u8c03\u4ee5\u53ca\u5bf9\u4e0d\u65ad\u53d8\u5316\u573a\u666f\u7684\u52a8\u6001\u9002\u5e94\uff0c\u4ece\u800c\u4e3a\u5206\u5e03\u5f0f\u667a\u80fd\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6709\u671b\u89e3\u51b3\u4f20\u7edf\u88ab\u52a8\u6846\u67b6\u4e0b\u7684\u534f\u8c03\u6311\u6218\u3002", "conclusion": "\u751f\u6210\u5f0fAI\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u8303\u5f0f\u6709\u671b\u514b\u670d\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u56fa\u6709\u6311\u6218\uff0c\u5b9e\u73b0\u4ece\u88ab\u52a8\u54cd\u5e94\u5230\u4e3b\u52a8\u9884\u6d4b\u7684\u8f6c\u53d8\uff0c\u4ece\u800c\u63a8\u52a8\u5206\u5e03\u5f0f\u667a\u80fd\u548c\u534f\u4f5c\u667a\u80fd\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.09201", "categories": ["cs.AR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09201", "abs": "https://arxiv.org/abs/2507.09201", "authors": ["Weihong Xu", "Haein Choi", "Po-kai Hsu", "Shimeng Yu", "Tajana Rosing"], "title": "SLIM: A Heterogeneous Accelerator for Edge Inference of Sparse Large Language Model via Adaptive Thresholding", "comment": null, "summary": "Large language models (LLMs) have demonstrated exceptional proficiency in\nunderstanding and generating human language, but efficient inference on\nresource-constrained embedded devices remains challenging due to large model\nsizes and memory-intensive operations in feedforward network (FFN) and\nmulti-head attention (MHA) layers. While existing accelerators offload LLM\ninference to expensive heterogeneous computing systems, they fail to exploit\nthe significant sparsity inherent in LLM operations, leaving hardware resources\nunderutilized. We propose SLIM, an algorithm-hardware co-design optimized for\nsparse LLM serving on edge devices. SLIM exploits LLM sparsity through an\nadaptive thresholding algorithm that enables runtime-configurable sparsity with\nnegligible accuracy loss, fetching only activated neurons to dramatically\nreduce data movement. Our heterogeneous hardware architecture strategically\ncombines near-storage processing (NSP) and processing-in-memory (PIM): FFN\nweights are stored in high-density 3D NAND and computed using NSP units, while\nmemory-intensive MHA operations are processed in PIM modules. This design\nsignificantly reduces memory footprint, data movement, and energy consumption.\nOur comprehensive evaluation demonstrates SLIM's effectiveness, achieving\n13-18x throughput improvements over SSD-GPU systems and 9-10x better energy\nefficiency over DRAM-GPU systems while maintaining low latency, making\ncost-effective LLM deployment viable for edge computing environments.", "AI": {"tldr": "SLIM\u901a\u8fc7\u5229\u7528LLM\u7a00\u758f\u6027\u548c\u521b\u65b0\u7684\u786c\u4ef6\u8bbe\u8ba1\uff08\u7ed3\u5408NSP\u548cPIM\uff09\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86LLM\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u548c\u80fd\u6548\u3002", "motivation": "\u89e3\u51b3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u8d44\u6e90\u53d7\u9650\u7684\u5d4c\u5165\u5f0f\u8bbe\u5907\u4e0a\u8fdb\u884c\u9ad8\u6548\u63a8\u7406\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6a21\u578b\u5c3a\u5bf8\u5927\u4ee5\u53ca\u524d\u9988\u7f51\u7edc\uff08FFN\uff09\u548c\u591a\u5934\u6ce8\u610f\u529b\uff08MHA\uff09\u5c42\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\u7684\u95ee\u9898\u3002\u73b0\u6709\u52a0\u901f\u5668\u672a\u80fd\u5145\u5206\u5229\u7528LLM\u64cd\u4f5c\u4e2d\u56fa\u6709\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u786c\u4ef6\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSLIM\u7684\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\u65b9\u6848\u3002\u8be5\u65b9\u6848\u5229\u7528\u81ea\u9002\u5e94\u9608\u503c\u7b97\u6cd5\u6316\u6398LLM\u7684\u7a00\u758f\u6027\uff0c\u5b9e\u73b0\u8fd0\u884c\u65f6\u53ef\u914d\u7f6e\u7684\u7a00\u758f\u6027\uff0c\u5e76\u4ec5\u8bfb\u53d6\u6fc0\u6d3b\u7684\u795e\u7ecf\u5143\u4ee5\u51cf\u5c11\u6570\u636e\u79fb\u52a8\u3002\u786c\u4ef6\u67b6\u6784\u7ed3\u5408\u4e86\u8fd1\u5b58\u50a8\u5904\u7406\uff08NSP\uff09\u548c\u5185\u5b58\u5904\u7406\uff08PIM\uff09\uff0c\u5176\u4e2dFFN\u6743\u91cd\u5b58\u50a8\u57283D NAND\u4e2d\u4f7f\u7528NSP\u5355\u5143\u8ba1\u7b97\uff0c\u800c\u5185\u5b58\u5bc6\u96c6\u578b\u7684MHA\u64cd\u4f5c\u5219\u5728PIM\u6a21\u5757\u4e2d\u5904\u7406\u3002", "result": "SLIM\u5b9e\u73b0\u4e8613-18\u500d\u4e8eSSD-GPU\u7cfb\u7edf\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u4ee5\u53ca9-10\u500d\u4e8eDRAM-GPU\u7cfb\u7edf\u7684\u80fd\u6548\u63d0\u5347\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4f4e\u5ef6\u8fdf\uff0c\u4f7f\u5f97\u7ecf\u6d4e\u9ad8\u6548\u7684LLM\u90e8\u7f72\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u6210\u4e3a\u53ef\u80fd\u3002", "conclusion": "SLIM\u901a\u8fc7\u7b97\u6cd5-\u786c\u4ef6\u534f\u540c\u8bbe\u8ba1\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u7a00\u758f\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u9ad8\u6548\u63a8\u7406\uff0c\u4e0e\u73b0\u6709\u7cfb\u7edf\u76f8\u6bd4\uff0c\u5728\u541e\u5410\u91cf\u548c\u80fd\u6548\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u4f7f\u5f97LLM\u5728\u8fb9\u7f18\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u90e8\u7f72\u6210\u4e3a\u53ef\u80fd\u3002"}}
{"id": "2507.10086", "categories": ["physics.app-ph", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10086", "abs": "https://arxiv.org/abs/2507.10086", "authors": ["Guanyu Qian", "Haoxian Yan", "Xiaofan Cui"], "title": "Fast-Response Variable-Frequency Series-Capacitor Buck VRM Through Integrated Control Approaches", "comment": "8 pages, 10 figures, IEEE conference style. to be published on IEEE\n  Workshop on Control and Modeling of Power Electronics (COMPEL 2025)", "summary": "Fast-response voltage regulation is essential for data-center Voltage\nRegulation Modules (VRMs) powering Artificial Intelligence (AI) workloads,\nwhich exhibit both small-amplitude fluctuations and abrupt full-load steps.\nThis paper introduces a control scheme that integrates a linear controller and\na nonlinear controller for variable-frequency Series-Capacitor Buck (SCB)\nconverters. First, an accurate small-signal model is derived via a\nSwitching-Synchronized Sampled State-Space (5S) framework, yielding\ndiscrete-time transfer functions and root-locus insights for direct digital\ndesign. A critical concern for SCB converters is series-capacitor oscillation\nduring heavy load steps if the strict switching sequence is not maintained. To\naccelerate large-signal transients, a time-optimal control strategy based on\nPontryagins Maximum Principle (PMP) relaxes the switching constraints to\ncompute time-optimal switching sequences. A transition logic is then proposed\nto integrate the high-bandwidth small-signal controller and the large-signal\ncontroller. Simulations demonstrate a rapid output voltage recovery under a\nheavy load step-up, over ten times faster than a linear controller-only design.\nPreliminary hardware tests indicate a stable rejection to heavy load\ndisturbances with zero steady-state error.", "AI": {"tldr": "\u9488\u5bf9\u6570\u636e\u4e2d\u5fc3AI\u8d1f\u8f7d\u5bf9\u7535\u538b\u8c03\u8282\u5feb\u901f\u54cd\u5e94\u7684\u9700\u6c42\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u63a7\u5236\u5668\u7684SCB\u8f6c\u6362\u5668\u63a7\u5236\u65b9\u6848\u3002\u8be5\u65b9\u6848\u901a\u8fc75S\u6846\u67b6\u5efa\u7acb\u5c0f\u4fe1\u53f7\u6a21\u578b\uff0c\u5e76\u5229\u7528PMP\u4f18\u5316\u5927\u4fe1\u53f7\u77ac\u6001\u54cd\u5e94\u3002\u4eff\u771f\u548c\u786c\u4ef6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u65b9\u6848\u80fd\u663e\u8457\u63d0\u9ad8\u7535\u538b\u6062\u590d\u901f\u5ea6\u5e76\u7a33\u5b9a\u6291\u5236\u6270\u52a8\u3002", "motivation": "\u6570\u636e\u4e2d\u5fc3\u7535\u538b\u8c03\u8282\u6a21\u5757\uff08VRM\uff09\u9700\u8981\u5feb\u901f\u54cd\u5e94\u7535\u538b\u8c03\u8282\u529f\u80fd\uff0c\u4ee5\u5e94\u5bf9\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u5de5\u4f5c\u8d1f\u8f7d\u4e2d\u7684\u5c0f\u5e45\u5ea6\u6ce2\u52a8\u548c\u7a81\u7136\u7684\u6ee1\u8d1f\u8377\u9636\u8dc3\u53d8\u5316\u3002", "method": "\u7814\u7a76\u91c7\u7528\u5f00\u5173\u540c\u6b65\u91c7\u6837\u72b6\u6001\u7a7a\u95f4\uff085S\uff09\u6846\u67b6\u63a8\u5bfc\u4e86\u7cbe\u786e\u7684\u5c0f\u4fe1\u53f7\u6a21\u578b\uff0c\u5f97\u5230\u79bb\u6563\u65f6\u95f4\u4f20\u9012\u51fd\u6570\u548c\u6839\u8f68\u8ff9\uff0c\u7528\u4e8e\u6570\u5b57\u8bbe\u8ba1\u3002\u4e3a\u4e86\u52a0\u901f\u5927\u4fe1\u53f7\u77ac\u6001\u54cd\u5e94\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e9e\u7279\u91cc\u4e9a\u91d1\u6700\u5927\u503c\u539f\u7406\uff08PMP\uff09\u7684\u65f6\u95f4\u6700\u4f18\u63a7\u5236\u7b56\u7565\uff0c\u653e\u5bbd\u4e86\u5f00\u5173\u7ea6\u675f\uff0c\u8ba1\u7b97\u4e86\u65f6\u95f4\u6700\u4f18\u5f00\u5173\u5e8f\u5217\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8fc7\u6e21\u903b\u8f91\u6765\u96c6\u6210\u9ad8\u5e26\u5bbd\u5c0f\u4fe1\u53f7\u63a7\u5236\u5668\u548c\u5927\u4fe1\u53f7\u63a7\u5236\u5668\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cSCB\u8f6c\u6362\u5668\u5728\u91cd\u8d1f\u8f7d\u9636\u8dc3\u65f6\uff0c\u8f93\u51fa\u7535\u538b\u6062\u590d\u901f\u5ea6\u6bd4\u4ec5\u4f7f\u7528\u7ebf\u6027\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u5feb\u5341\u500d\u4ee5\u4e0a\u3002\u521d\u6b65\u7684\u786c\u4ef6\u6d4b\u8bd5\u8868\u660e\uff0c\u8be5\u8bbe\u8ba1\u80fd\u591f\u7a33\u5b9a\u5730\u6291\u5236\u91cd\u8d1f\u8f7d\u6270\u52a8\uff0c\u4e14\u7a33\u6001\u8bef\u5dee\u4e3a\u96f6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u65b9\u6848\uff0c\u7ed3\u5408\u4e86\u7ebf\u6027\u63a7\u5236\u5668\u548c\u975e\u7ebf\u6027\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u53ef\u53d8\u9891\u4e32\u8054\u7535\u5bb9\u964d\u538b\uff08SCB\uff09\u8f6c\u6362\u5668\uff0c\u4ee5\u6ee1\u8db3\u6570\u636e\u4e2d\u5fc3AI\u5de5\u4f5c\u8d1f\u8f7d\u5bf9\u5feb\u901f\u54cd\u5e94\u7535\u538b\u8c03\u8282\u7684\u9700\u6c42\u3002"}}
{"id": "2507.09115", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09115", "abs": "https://arxiv.org/abs/2507.09115", "authors": ["Sampson E. Nwachukwu"], "title": "Modelling and Control of a Buck Converter Using State-Space Averaging and Classical Feedback Techniques", "comment": "8 Pages", "summary": "This study presents the modeling, control design, and performance analysis of\na DC-DC buck converter using state-space averaging techniques. Buck converters\nare essential in modern power electronics for regulating DC voltages in\nrenewable energy and electric vehicle systems. The paper first introduces the\nbasic operation of buck converters and emphasizes the need for voltage\nregulation through closed-loop control systems. A state-space averaged model is\nderived to simplify the nonlinear switched dynamics, enabling a more effective\nanalysis and controller design. The small-signal transfer function from the\nduty cycle to the output voltage is obtained to support control development. In\naddition, the Proportional-Integral (PI) control based on the frequency-domain\nmethod was explored. The PI controller was tuned to achieve various phase\nmargins and is evaluated through Bode plots, step responses, and performance\nmetrics, revealing trade-offs between overshoot, settling time, and\nsteady-state error. A complete simulation of the controlled buck converter\nverifies its ability to maintain a stable output voltage across wide input\nvoltage variations. The results validate the effectiveness of state-space\naveraging in control design and highlight the robustness of feedback systems in\npower electronic converters.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u5e73\u5747\u6280\u672f\u5bf9DC-DC\u964d\u538b\u53d8\u6362\u5668\u8fdb\u884c\u4e86\u5efa\u6a21\u3001\u63a7\u5236\u8bbe\u8ba1\u548c\u6027\u80fd\u5206\u6790\uff0c\u5e76\u4f7f\u7528PI\u63a7\u5236\u5668\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u9a8c\u8bc1\u4e86\u5176\u7a33\u5b9a\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u964d\u538b\u53d8\u6362\u5668\u5728\u73b0\u4ee3\u7535\u529b\u7535\u5b50\u6280\u672f\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u7528\u4e8e\u8c03\u8282\u53ef\u518d\u751f\u80fd\u6e90\u548c\u7535\u52a8\u6c7d\u8f66\u7cfb\u7edf\u4e2d\u7684\u76f4\u6d41\u7535\u538b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u72b6\u6001\u7a7a\u95f4\u5e73\u5747\u6280\u672f\u7684DC-DC\u964d\u538b\u53d8\u6362\u5668\u7684\u5efa\u6a21\u3001\u63a7\u5236\u8bbe\u8ba1\u548c\u6027\u80fd\u5206\u6790\u65b9\u6cd5\u3002\u9996\u5148\u4ecb\u7ecd\u4e86\u964d\u538b\u53d8\u6362\u5668\u57fa\u672c\u64cd\u4f5c\u548c\u95ed\u73af\u63a7\u5236\u7cfb\u7edf\u7684\u5fc5\u8981\u6027\u3002\u63a8\u5bfc\u4e86\u72b6\u6001\u7a7a\u95f4\u5e73\u5747\u6a21\u578b\u4ee5\u7b80\u5316\u975e\u7ebf\u6027\u5f00\u5173\u52a8\u6001\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u5206\u6790\u548c\u63a7\u5236\u5668\u8bbe\u8ba1\u3002\u5f97\u5230\u4e86\u5360\u7a7a\u6bd4\u5230\u8f93\u51fa\u7535\u538b\u7684\u5c0f\u4fe1\u53f7\u4f20\u9012\u51fd\u6570\u4ee5\u652f\u6301\u63a7\u5236\u5f00\u53d1\u3002\u6b64\u5916\uff0c\u8fd8\u63a2\u8ba8\u4e86\u57fa\u4e8e\u9891\u57df\u65b9\u6cd5\u8bbe\u8ba1\u7684\u6bd4\u4f8b-\u79ef\u5206\uff08PI\uff09\u63a7\u5236\u5668\u3002", "result": "\u901a\u8fc7\u4f2f\u5fb7\u56fe\u3001\u9636\u8dc3\u54cd\u5e94\u548c\u6027\u80fd\u6307\u6807\u8bc4\u4f30\u4e86PI\u63a7\u5236\u5668\uff0c\u63ed\u793a\u4e86\u8fc7\u51b2\u3001\u8c03\u8282\u65f6\u95f4\u548c\u7a33\u6001\u8bef\u5dee\u4e4b\u95f4\u7684\u6743\u8861\u3002\u53d7\u63a7\u964d\u538b\u53d8\u6362\u5668\u7684\u5b8c\u6574\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u5728\u5bbd\u8f93\u5165\u7535\u538b\u53d8\u5316\u4e0b\u7ef4\u6301\u7a33\u5b9a\u8f93\u51fa\u7535\u538b\u7684\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u9a8c\u8bc1\u4e86\u72b6\u6001\u7a7a\u95f4\u5e73\u5747\u6cd5\u5728\u63a7\u5236\u8bbe\u8ba1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u53cd\u9988\u7cfb\u7edf\u5728\u7535\u529b\u7535\u5b50\u53d8\u6362\u5668\u4e2d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.08901", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.08901", "abs": "https://arxiv.org/abs/2507.08901", "authors": ["Zebang Feng", "Miao Fan", "Bao Liu", "Shengtong Xu", "Haoyi Xiong"], "title": "End-to-End Generation of City-Scale Vectorized Maps by Crowdsourced Vehicles", "comment": "Accepted by ITSC'25", "summary": "High-precision vectorized maps are indispensable for autonomous driving, yet\ntraditional LiDAR-based creation is costly and slow, while single-vehicle\nperception methods lack accuracy and robustness, particularly in adverse\nconditions. This paper introduces EGC-VMAP, an end-to-end framework that\novercomes these limitations by generating accurate, city-scale vectorized maps\nthrough the aggregation of data from crowdsourced vehicles. Unlike prior\napproaches, EGC-VMAP directly fuses multi-vehicle, multi-temporal map elements\nperceived onboard vehicles using a novel Trip-Aware Transformer architecture\nwithin a unified learning process. Combined with hierarchical matching for\nefficient training and a multi-objective loss, our method significantly\nenhances map accuracy and structural robustness compared to single-vehicle\nbaselines. Validated on a large-scale, multi-city real-world dataset, EGC-VMAP\ndemonstrates superior performance, enabling a scalable, cost-effective solution\nfor city-wide mapping with a reported 90\\% reduction in manual annotation\ncosts.", "AI": {"tldr": "EGC-VMAP \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u5229\u7528\u4f17\u5305\u8f66\u8f86\u6570\u636e\u548c\u521b\u65b0\u7684\u65c5\u884c\u611f\u77e5 Transformer \u67b6\u6784\uff0c\u901a\u8fc7\u878d\u5408\u591a\u8f66\u8f86\u3001\u591a\u65f6\u95f4\u7684\u6570\u636e\u6765\u751f\u6210\u9ad8\u7cbe\u5ea6\u7684\u57ce\u5e02\u77e2\u91cf\u5730\u56fe\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u57fa\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u5236\u56fe\u6210\u672c\u9ad8\u6602\u4e14\u901f\u5ea6\u7f13\u6162\uff0c\u800c\u5355\u8f66\u611f\u77e5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u4e0d\u5229\u6761\u4ef6\u4e0b\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u51c6\u786e\u7684\u65b9\u6cd5\u6765\u521b\u5efa\u9ad8\u7cbe\u5ea6\u77e2\u91cf\u5730\u56fe\uff0c\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7684\u9700\u6c42\u3002", "method": "EGC-VMAP \u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u805a\u5408\u4f17\u5305\u8f66\u8f86\u7684\u6570\u636e\u6765\u751f\u6210\u7cbe\u786e\u7684\u3001\u57ce\u5e02\u89c4\u6a21\u7684\u77e2\u91cf\u5730\u56fe\u3002\u5b83\u4f7f\u7528\u65b0\u9896\u7684\u3001\u611f\u77e5\u8f66\u8f7d\u5730\u56fe\u5143\u7d20\u7684\u611f\u77e5\u65c5\u884c\u611f\u77e5 Transformer \u67b6\u6784\uff0c\u5728\u7edf\u4e00\u7684\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u8fdb\u884c\u591a\u8f66\u8f86\u3001\u591a\u65f6\u95f4\u878d\u5408\u3002\u7ed3\u5408\u5206\u5c42\u5339\u914d\u4ee5\u5b9e\u73b0\u9ad8\u6548\u8bad\u7ec3\u548c\u591a\u76ee\u6807\u635f\u5931\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5730\u56fe\u7cbe\u5ea6\u548c\u7ed3\u6784\u9c81\u68d2\u6027\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u5355\u8f66\u57fa\u7ebf\u3002", "result": "EGC-VMAP \u5728\u5927\u89c4\u6a21\u3001\u591a\u57ce\u5e02\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u4e0e\u5355\u8f66\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u5730\u56fe\u7cbe\u5ea6\u548c\u7ed3\u6784\u9c81\u68d2\u6027\u65b9\u9762\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "EGC-VMAP \u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5b9e\u73b0\u57ce\u5e02\u8303\u56f4\u7684\u5730\u56fe\u7ed8\u5236\uff0c\u5e76\u62a5\u544a\u624b\u52a8\u6ce8\u91ca\u6210\u672c\u964d\u4f4e\u4e86 90%\u3002"}}
{"id": "2507.09657", "categories": ["cs.SI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09657", "abs": "https://arxiv.org/abs/2507.09657", "authors": ["Ann Nedime Nese Rende", "Tolga Yilmaz", "\u00d6zg\u00fcr Ulusoy"], "title": "Negotiating Comfort: Simulating Personality-Driven LLM Agents in Shared Residential Social Networks", "comment": null, "summary": "We use generative agents powered by large language models (LLMs) to simulate\na social network in a shared residential building, driving the temperature\ndecisions for a central heating system. Agents, divided into Family Members and\nRepresentatives, consider personal preferences, personal traits, connections,\nand weather conditions. Daily simulations involve family-level consensus\nfollowed by building-wide decisions among representatives. We tested three\npersonality traits distributions (positive, mixed, and negative) and found that\npositive traits correlate with higher happiness and stronger friendships.\nTemperature preferences, assertiveness, and selflessness have a significant\nimpact on happiness and decisions. This work demonstrates how LLM-driven agents\ncan help simulate nuanced human behavior where complex real-life human\nsimulations are difficult to set.", "AI": {"tldr": "LLM\u667a\u80fd\u4f53\u88ab\u7528\u6765\u6a21\u62df\u4e00\u4e2a\u5efa\u7b51\u5185\u7684\u793e\u4ea4\u7f51\u7edc\u548c\u4f9b\u6696\u51b3\u7b56\uff0c\u53d1\u73b0\u79ef\u6781\u7279\u8d28\u548c\u7279\u5b9a\u884c\u4e3a\u4f1a\u5f71\u54cd\u5e78\u798f\u611f\u548c\u51b3\u7b56\u3002", "motivation": "\u4e3a\u4e86\u6a21\u62df\u4eba\u7c7b\u7684\u7ec6\u5fae\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u96be\u4ee5\u8fdb\u884c\u590d\u6742\u771f\u5b9e\u4e16\u754c\u6a21\u62df\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u4f7f\u7528\u7531\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u9a71\u52a8\u7684\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u6765\u6a21\u62df\u5171\u4eab\u5c45\u6c11\u697c\u4e2d\u7684\u793e\u4ea4\u7f51\u7edc\uff0c\u5e76\u636e\u6b64\u505a\u51fa\u4e2d\u592e\u4f9b\u6696\u7cfb\u7edf\u7684\u6e29\u5ea6\u51b3\u7b56\u3002\u667a\u80fd\u4f53\u5206\u4e3a\u5bb6\u5ead\u6210\u5458\u548c\u4ee3\u8868\uff0c\u4ed6\u4eec\u4f1a\u8003\u8651\u4e2a\u4eba\u504f\u597d\u3001\u4e2a\u4eba\u7279\u8d28\u3001\u4eba\u9645\u5173\u7cfb\u4ee5\u53ca\u5929\u6c14\u72b6\u51b5\u3002\u6a21\u62df\u8fc7\u7a0b\u5305\u62ec\u5bb6\u5ead\u5185\u90e8\u8fbe\u6210\u5171\u8bc6\uff0c\u7136\u540e\u7531\u4ee3\u8868\u4eec\u5728\u6574\u4e2a\u5efa\u7b51\u8303\u56f4\u5185\u505a\u51fa\u51b3\u7b56\u3002", "result": "\u6d4b\u8bd5\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u4eba\u683c\u7279\u8d28\u5206\u5e03\uff08\u79ef\u6781\u3001\u6df7\u5408\u548c\u6d88\u6781\uff09\uff0c\u53d1\u73b0\u79ef\u6781\u7279\u8d28\u4e0e\u66f4\u9ad8\u7684\u5e78\u798f\u611f\u548c\u66f4\u7262\u56fa\u7684\u53cb\u8c0a\u76f8\u5173\u3002\u6e29\u5ea6\u504f\u597d\u3001\u81ea\u4fe1\u5fc3\u548c\u65e0\u79c1\u7a0b\u5ea6\u5bf9\u5e78\u798f\u611f\u548c\u51b3\u7b56\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u667a\u80fd\u4f53\u53ef\u4ee5\u6a21\u62df\u7ec6\u81f4\u7684\u4eba\u7c7b\u884c\u4e3a\uff0c\u5c24\u5176\u662f\u5728\u96be\u4ee5\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6a21\u62df\u7684\u573a\u666f\u4e2d\u3002"}}
{"id": "2507.08979", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08979", "abs": "https://arxiv.org/abs/2507.08979", "authors": ["Mahdiyar Molahasani", "Azadeh Motamedi", "Michael Greenspan", "Il-Min Kim", "Ali Etemad"], "title": "PRISM: Reducing Spurious Implicit Biases in Vision-Language Models with LLM-Guided Embedding Projection", "comment": "Accepted to ICCV 2025", "summary": "We introduce Projection-based Reduction of Implicit Spurious bias in\nvision-language Models (PRISM), a new data-free and task-agnostic solution for\nbias mitigation in VLMs like CLIP. VLMs often inherit and amplify biases in\ntheir training data, leading to skewed predictions. PRISM is designed to debias\nVLMs without relying on predefined bias categories or additional external data.\nIt operates in two stages: first, an LLM is prompted with simple class prompts\nto generate scene descriptions that contain spurious correlations. Next, PRISM\nuses our novel contrastive-style debiasing loss to learn a projection that maps\nthe embeddings onto a latent space that minimizes spurious correlations while\npreserving the alignment between image and text embeddings.Extensive\nexperiments demonstrate that PRISM outperforms current debiasing methods on the\ncommonly used Waterbirds and CelebA datasets We make our code public at:\nhttps://github.com/MahdiyarMM/PRISM.", "AI": {"tldr": "PRISM is a new data-free and task-agnostic solution for bias mitigation in VLMs like CLIP. It uses an LLM to generate scene descriptions with spurious correlations and a novel contrastive-style debiasing loss to learn a projection that minimizes these correlations while preserving image-text alignment. PRISM outperforms existing methods on Waterbirds and CelebA datasets.", "motivation": "VLMs often inherit and amplify biases in their training data, leading to skewed predictions. PRISM is designed to debias VLMs without relying on predefined bias categories or additional external data.", "method": "PRISM operates in two stages: first, an LLM is prompted with simple class prompts to generate scene descriptions that contain spurious correlations. Next, PRISM uses a novel contrastive-style debiasing loss to learn a projection that maps the embeddings onto a latent space that minimizes spurious correlations while preserving the alignment between image and text embeddings.", "result": "PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets.", "conclusion": "PRISM outperforms current debiasing methods on the commonly used Waterbirds and CelebA datasets."}}
{"id": "2507.09926", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09926", "abs": "https://arxiv.org/abs/2507.09926", "authors": ["Zixuan Song", "Zhishu Shen", "Xiaoyu Zheng", "Qiushi Zheng", "Zheng Lei", "Jiong Jin"], "title": "Intelligent Task Management via Dynamic Multi-region Division in LEO Satellite Networks", "comment": null, "summary": "As a key complement to terrestrial networks and a fundamental component of\nfuture 6G systems, Low Earth Orbit (LEO) satellite networks are expected to\nprovide high-quality communication services when integrated with ground-based\ninfrastructure, thereby attracting significant research interest. However, the\nlimited satellite onboard resources and the uneven distribution of\ncomputational workloads often result in congestion along inter-satellite links\n(ISLs) that degrades task processing efficiency. Effectively managing the\ndynamic and large-scale topology of LEO networks to ensure balanced task\ndistribution remains a critical challenge. To this end, we propose a dynamic\nmulti-region division framework for intelligent task management in LEO\nsatellite networks. This framework optimizes both intra- and inter-region\nrouting to minimize task delay while balancing the utilization of computational\nand communication resources. Based on this framework, we propose a dynamic\nmulti-region division algorithm based on the Genetic Algorithm (GA), which\nadaptively adjusts the size of each region based on the workload status of\nindividual satellites. Additionally, we incorporate an adaptive routing\nalgorithm and a task splitting and offloading scheme based on Multi-Agent Deep\nDeterministic Policy Gradient (MA-DDPG) to effectively accommodate the arriving\ntasks. Simulation results demonstrate that our proposed framework outperforms\ncomparative methods in terms of the task delay, energy consumption per task,\nand task completion rate.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8eLEO\u536b\u661f\u7f51\u7edc\u667a\u80fd\u4efb\u52a1\u7ba1\u7406\u7684\u52a8\u6001\u591a\u533a\u57df\u5212\u5206\u6846\u67b6\uff0c\u7ed3\u5408\u9057\u4f20\u7b97\u6cd5\u548cMA-DDPG\u4f18\u5316\u8def\u7531\u548c\u4efb\u52a1\u5378\u8f7d\uff0c\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u548c\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "LEO\u536b\u661f\u7f51\u7edc\u867d\u7136\u662f\u672a\u67656G\u7684\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u4f46\u5176\u6709\u9650\u7684\u8d44\u6e90\u548c\u4e0d\u5747\u8861\u7684\u5de5\u4f5c\u8d1f\u8f7d\u4f1a\u5bfc\u81f4ISL\u62e5\u5835\uff0c\u964d\u4f4e\u4efb\u52a1\u5904\u7406\u6548\u7387\u3002\u5982\u4f55\u7ba1\u7406\u52a8\u6001\u7684\u5927\u89c4\u6a21\u62d3\u6251\u4ee5\u5e73\u8861\u4efb\u52a1\u5206\u914d\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u591a\u533a\u57df\u5212\u5206\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8e\u9057\u4f20\u7b97\u6cd5\u7684\u52a8\u6001\u591a\u533a\u57df\u5212\u5206\u7b97\u6cd5\u4ee5\u53ca\u57fa\u4e8eMA-DDPG\u7684\u81ea\u9002\u5e94\u8def\u7531\u7b97\u6cd5\u548c\u4efb\u52a1\u62c6\u5206\u5378\u8f7d\u65b9\u6848\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u4efb\u52a1\u5ef6\u8fdf\u3001\u6bcf\u4efb\u52a1\u80fd\u8017\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u65b9\u9762\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u591a\u533a\u57df\u5212\u5206\u548c\u57fa\u4e8eMA-DDPG\u7684\u8def\u7531\u548c\u4efb\u52a1\u5378\u8f7d\u65b9\u6848\uff0c\u5728\u4efb\u52a1\u5ef6\u8fdf\u3001\u6bcf\u4efb\u52a1\u80fd\u8017\u548c\u4efb\u52a1\u5b8c\u6210\u7387\u65b9\u9762\u4f18\u4e8e\u5bf9\u6bd4\u65b9\u6cd5\u3002"}}
{"id": "2507.09044", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09044", "abs": "https://arxiv.org/abs/2507.09044", "authors": ["Matthias Blatnik", "Fabio Calcinelli", "Andreas Jeindl", "Moritz Eder", "Michael Schmid", "Jan \u010cechal", "Ulrike Diebold", "Peter Jacobson Oliver T. Hofmann", "Margareta Wagner"], "title": "Molecular Arrangements in the First Monolayer of Cu-Phthalocyanine on In$_2$O$_3$(111)", "comment": null, "summary": "Well-ordered organic molecular layers on oxide surfaces are key for organic\nelectronics. Using a combination of scanning tunneling microscopy (STM) and\nnon-contact atomic force microscopy (nc-AFM) we probe the structures of copper\nphthalocyanine (CuPc) on In$_2$O$_3$, a model for a prototypical transparent\nconductive oxide (TCO). These scanning-probe images allow the direct\ndetermination of the adsorption site and distortions of the molecules, which\nare corroborated by DFT calculations. Isolated CuPc molecules adsorb in a flat,\nslightly tilted geometry in three symmetry-equivalent configurations on the\nstoichiometric In$_2$O$_3$(111) surface. Increasing the coverage leads to\ndensely-packed 1D chains oriented along $\\langle1\\bar{1}0\\rangle$ directions,\nwhich dissolve into a highly ordered (2$\\times$2) superstructure upon\nincreasing the CuPc density to 3/4 per surface unit cell. At a coverage of one\nCuPc per surface unit cell, a densely packed (1$\\times$1) superstructure fully\ncovers the surface. The molecules still assume the same site and orientation as\nbefore, but they partially overlap to accommodate the high packing density,\nleading to a bending of the molecules. These results are compared to the\nbehavior of CoPc on In$_2$O$_3$(111). In summary, we demonstrate that a uniform\nfirst layer of metal-phthalocyanine molecules can be realized on the\nIn$_2$O$_3$(111) surface when using the proper metal atom in the molecule.", "AI": {"tldr": "CuPc\u5728In$_2$O$_3$\u8868\u9762\u5f62\u6210\u4e0d\u540c\u8986\u76d6\u5ea6\u7684\u6709\u5e8f\u7ed3\u6784\uff0c\u6700\u7ec8\u53ef\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u91d1\u5c5e\u539f\u5b50\u5b9e\u73b0\u5747\u5300\u7684\u7b2c\u4e00\u5c42\u5438\u9644\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u6709\u673a\u7535\u5b50\u5668\u4ef6\u4e2d\u7684\u6709\u673a\u5206\u5b50\u5c42\u5728\u6c27\u5316\u7269\u8868\u9762\u7684\u6709\u5e8f\u6392\u5217\uff0c\u9700\u8981\u7406\u89e3\u5206\u5b50\u5728\u6c27\u5316\u7269\u8868\u9762\u7684\u5438\u9644\u884c\u4e3a\u3002", "method": "\u7ed3\u5408\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\uff08STM\uff09\u548c\u975e\u63a5\u89e6\u539f\u5b50\u529b\u663e\u5fae\u955c\uff08nc-AFM\uff09\u6280\u672f\uff0c\u5e76\u8f85\u4ee5\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\uff0c\u63a2\u7a76\u4e86\u915e\u83c1\u94dc\uff08CuPc\uff09\u5728\u6c27\u5316\u94df\uff08In$_2$O$_3$\uff09\u6a21\u578b\u8868\u9762\u7684\u5438\u9644\u7ed3\u6784\u548c\u5206\u5b50\u5f62\u53d8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728\u5316\u5b66\u8ba1\u91cf\u7684In$_2$O$_3$(111)\u8868\u9762\uff0c\u5b64\u7acb\u7684CuPc\u5206\u5b50\u4ee5\u6241\u5e73\u3001\u8f7b\u5fae\u503e\u659c\u7684\u51e0\u4f55\u6784\u578b\u5438\u9644\u5728\u4e09\u4e2a\u5bf9\u79f0\u7b49\u4ef7\u7684\u4f4d\u70b9\u4e0a\u3002\u968f\u7740\u8986\u76d6\u5ea6\u7684\u589e\u52a0\uff0c\u4f1a\u5f62\u6210\u6cbf$\\langle1\\bar{1}0\\rangle$\u65b9\u5411\u6392\u5217\u7684\u81f4\u5bc6\u4e00\u7ef4\u94fe\uff0c\u5f53CuPc\u8986\u76d6\u5ea6\u589e\u52a0\u5230\u6bcf\u8868\u9762\u5355\u80de3/4\u65f6\uff0c\u4f1a\u5f62\u6210\u9ad8\u5ea6\u6709\u5e8f\u7684\uff082\u00d72\uff09\u8d85\u7ed3\u6784\u3002\u5f53\u8986\u76d6\u5ea6\u4e3a\u6bcf\u8868\u9762\u5355\u80de\u4e00\u4e2aCuPc\u65f6\uff0c\u4f1a\u5f62\u6210\u5b8c\u5168\u8986\u76d6\u8868\u9762\u7684\u81f4\u5bc6\uff081\u00d71\uff09\u8d85\u7ed3\u6784\uff0c\u5206\u5b50\u867d\u7136\u4fdd\u6301\u539f\u6709\u4f4d\u70b9\u548c\u53d6\u5411\uff0c\u4f46\u90e8\u5206\u91cd\u53e0\u5bfc\u81f4\u5206\u5b50\u5f2f\u66f2\u3002\u8fd9\u4e9b\u7ed3\u679c\u4e0e\u94b4\u915e\u83c1\uff08CoPc\uff09\u5728In$_2$O$_3$(111)\u8868\u9762\u7684\u884c\u4e3a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u9009\u62e9\u5408\u9002\u7684\u91d1\u5c5e\u539f\u5b50\uff0c\u53ef\u4ee5\u5728In$_2$O$_3$(111)\u8868\u9762\u5b9e\u73b0\u91d1\u5c5e\u915e\u83c1\u5206\u5b50\u7684\u5747\u5300\u7b2c\u4e00\u5c42\u5438\u9644\u3002"}}
{"id": "2507.09847", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2507.09847", "abs": "https://arxiv.org/abs/2507.09847", "authors": ["Amin Abdollahi Dehkordi", "Mehdi Neshat", "Nataliia Y. Sergiienko", "Zahra Ghasemi", "Lei Chen", "John Boland", "Hamid Moradkhani", "Amir H. Gandomi"], "title": "Effective Self-Attention-Based Deep Learning Model with Evolutionary Grid Search for Robust Wave Farm Energy Forecasting", "comment": null, "summary": "Achieving carbon neutrality, a key focus of UN SDG #13, drives the\nexploration of wave energy, a renewable resource with the potential to generate\n30,000 TWh of clean electricity annually, surpassing global demand. However,\nwave energy remains underdeveloped due to technical and economic challenges,\nparticularly in forecasting wave farm power output, which is vital for grid\nstability and commercial viability. This study proposes a novel predictive\nframework to enhance wave energy integration into power grids. It introduces a\nhybrid sequential learning model combining Self-Attention-enhanced\nConvolutional Bi-LSTM with hyperparameter optimization. The model leverages\nspatial data from Wave Energy Converters (WECs) and is validated using datasets\nfrom wave farms in Adelaide, Sydney, Perth, and Tasmania, Australia.\nBenchmarked against ten machine learning algorithms, the model achieves\nsuperior accuracy, with R2 scores of 91.7% (Adelaide), 88.0% (Perth), 82.8%\n(Tasmania), and 91.0% (Sydney). It outperforms conventional ML and deep\nlearning methods, offering robust and scalable predictions for wave energy\noutput across diverse marine environments, supporting reliable integration into\nenergy systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u81ea\u6ce8\u610f\u529b\u673a\u5236\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u6ce2\u6d6a\u80fd\u53d1\u7535\u91cf\uff0c\u5e76\u5728\u6fb3\u5927\u5229\u4e9a\u7684\u56db\u4e2a\u5730\u70b9\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u6ce2\u6d6a\u80fd\u5e76\u7f51\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u8054\u5408\u56fd\u53ef\u6301\u7eed\u53d1\u5c55\u76ee\u680713\uff08\u6c14\u5019\u884c\u52a8\uff09\uff0c\u5e76\u5e94\u5bf9\u6ce2\u6d6a\u80fd\u53d1\u7535\u91cf\u9884\u6d4b\u5bf9\u7535\u7f51\u7a33\u5b9a\u6027\u548c\u5546\u4e1a\u53ef\u884c\u6027\u7684\u91cd\u8981\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u6ce2\u6d6a\u80fd\u53d1\u7535\u91cf\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4ece\u800c\u4fc3\u8fdb\u6ce2\u6d6a\u80fd\u66f4\u597d\u5730\u6574\u5408\u5230\u7535\u529b\u7cfb\u7edf\u4e2d\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u5e8f\u5217\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u5377\u79ef\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff08Self-Attention-enhanced Convolutional Bi-LSTM\uff09\u548c\u8d85\u53c2\u6570\u4f18\u5316\u6280\u672f\uff0c\u5e76\u5229\u7528\u4e86\u6765\u81ea\u6fb3\u5927\u5229\u4e9a\u963f\u5fb7\u83b1\u5fb7\u3001\u6089\u5c3c\u3001\u73c0\u65af\u548c\u5854\u65af\u9a6c\u5c3c\u4e9a\u7b49\u5730\u6ce2\u6d6a\u80fd\u53d1\u7535\u573a\u7684\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u8be5\u6df7\u5408\u6a21\u578b\u5728\u6fb3\u5927\u5229\u4e9a\u4e0d\u540c\u5730\u533a\u7684\u6ce2\u6d6a\u80fd\u53d1\u7535\u573a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5176R2\u5206\u6570\u5206\u522b\u4e3a\uff1a\u963f\u5fb7\u83b1\u5fb791.7%\uff0c\u73c0\u65af88.0%\uff0c\u5854\u65af\u9a6c\u5c3c\u4e9a82.8%\uff0c\u6089\u5c3c91.0%\u3002\u4e0e\u5341\u79cd\u673a\u5668\u5b66\u4e60\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\u540e\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6df7\u5408\u5e8f\u5217\u5b66\u4e60\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u5177\u6709\u8d85\u53c2\u6570\u4f18\u5316\u7684\u81ea\u6ce8\u610f\u529b\u589e\u5f3a\u5377\u79ef\u53cc\u5411\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u6ce2\u6d6a\u80fd\u53d1\u7535\u91cf\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0c\u4e3a\u6ce2\u6d6a\u80fd\u5e76\u7f51\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.08936", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.08936", "abs": "https://arxiv.org/abs/2507.08936", "authors": ["Wolfgang D\u00fcr"], "title": "Long-ranged gates in quantum computation architectures with limited connectivity", "comment": "11 pages, 7 figures", "summary": "We propose a quantum computation architecture based on geometries with\nnearest-neighbor interactions, including e.g. planar structures. We show how to\nefficiently split the role of qubits into data and entanglement-generation\nqubits. Multipartite entangled states, e.g. 2D cluster states, are generated\namong the latter, and flexibly transformed via mid-circuit measurements to\nmultiple, long-ranged Bell states, which are used to perform several two-qubit\ngates in parallel on data qubits. We introduce planar architectures with $n$\ndata and $n$ auxiliary qubits that allow one to perform $O(\\sqrt n)$\nlong-ranged two-qubit gates simultaneously, with only one round of nearest\nneighbor gates and one round of mid-circuit measurements. We also show that our\napproach is applicable in existing superconducting quantum computation\narchitectures, with only a constant overhead.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u79bb\u6570\u636e\u548c\u7ea0\u7f20\u751f\u6210\u91cf\u5b50\u6bd4\u7279\uff0c\u5e76\u5229\u7528\u4e2d\u95f4\u7535\u8def\u6d4b\u91cf\u5b9e\u73b0\u5e76\u884c\u957f\u7a0b\u4e24\u91cf\u5b50\u6bd4\u7279\u95e8\uff0c\u5728\u5e73\u9762\u7ed3\u6784\u4e2d\u5b9e\u73b0\u4e86 O(sqrt n) \u7684\u5e76\u53d1\u95e8\u64cd\u4f5c\uff0c\u4e14\u517c\u5bb9\u73b0\u6709\u8d85\u5bfc\u67b6\u6784\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u91cf\u5b50\u8ba1\u7b97\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\uff0c\u7279\u522b\u662f\u901a\u8fc7\u5e76\u884c\u6267\u884c\u591a\u9879\u957f\u7a0b\u4e24\u91cf\u5b50\u6bd4\u7279\u95e8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5177\u6709\u6700\u8fd1\u90bb\u4ea4\u4e92\u7684\u51e0\u4f55\u7ed3\u6784\uff08\u4f8b\u5982\u5e73\u9762\u7ed3\u6784\uff09\u7684\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\u3002\u901a\u8fc7\u5c06\u91cf\u5b50\u6bd4\u7279\u7684\u89d2\u8272\u6709\u6548\u5730\u533a\u5206\u4e3a\u6570\u636e\u91cf\u5b50\u6bd4\u7279\u548c\u7ea0\u7f20\u751f\u6210\u91cf\u5b50\u6bd4\u7279\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u5728\u540e\u8005\u4e2d\u751f\u6210\u591a\u65b9\u7ea0\u7f20\u6001\uff08\u4f8b\u5982\u4e8c\u7ef4\u7c07\u6001\uff09\uff0c\u5e76\u901a\u8fc7\u4e2d\u95f4\u7535\u8def\u6d4b\u91cf\u7075\u6d3b\u5730\u8f6c\u6362\u4e3a\u591a\u4e2a\u957f\u7a0b\u8d1d\u5c14\u6001\uff0c\u7528\u4e8e\u5728\u6570\u636e\u91cf\u5b50\u6bd4\u7279\u4e0a\u5e76\u884c\u6267\u884c\u591a\u4e2a\u4e24\u91cf\u5b50\u6bd4\u7279\u95e8\u3002\u4ecb\u7ecd\u4e86\u5177\u6709 n \u4e2a\u6570\u636e\u91cf\u5b50\u6bd4\u7279\u548c n \u4e2a\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u7684\u5e73\u9762\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5141\u8bb8\u5728\u4ec5\u4e00\u8f6e\u6700\u8fd1\u90bb\u95e8\u548c\u4e00\u8f6e\u4e2d\u95f4\u7535\u8def\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\u540c\u65f6\u6267\u884c O(sqrt n) \u6b21\u957f\u7a0b\u4e24\u91cf\u5b50\u6bd4\u7279\u95e8\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5e73\u9762\u67b6\u6784\u5141\u8bb8\u5728\u4ec5\u4e00\u8f6e\u6700\u8fd1\u90bb\u95e8\u548c\u4e00\u8f6e\u4e2d\u95f4\u7535\u8def\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\u540c\u65f6\u6267\u884c O(sqrt n) \u6b21\u957f\u7a0b\u4e24\u91cf\u5b50\u6bd4\u7279\u95e8\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u53ef\u5e94\u7528\u4e8e\u73b0\u6709\u7684\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\uff0c\u4ec5\u9700\u6052\u5b9a\u7684\u989d\u5916\u5f00\u9500\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u9002\u7528\u4e8e\u73b0\u6709\u7684\u8d85\u5bfc\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\uff0c\u5e76\u4e14\u53ea\u5e26\u6765\u6052\u5b9a\u7684\u989d\u5916\u5f00\u9500\u3002"}}
{"id": "2507.08916", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08916", "abs": "https://arxiv.org/abs/2507.08916", "authors": ["Mahmoud Alwakeel", "Aditya Nagori", "Vijay Krishnamoorthy", "Rishikesan Kamaleswaran"], "title": "Evaluating LLMs in Medicine: A Call for Rigor, Transparency", "comment": null, "summary": "Objectives: To evaluate the current limitations of large language models\n(LLMs) in medical question answering, focusing on the quality of datasets used\nfor their evaluation. Materials and Methods: Widely-used benchmark datasets,\nincluding MedQA, MedMCQA, PubMedQA, and MMLU, were reviewed for their rigor,\ntransparency, and relevance to clinical scenarios. Alternatives, such as\nchallenge questions in medical journals, were also analyzed to identify their\npotential as unbiased evaluation tools. Results: Most existing datasets lack\nclinical realism, transparency, and robust validation processes. Publicly\navailable challenge questions offer some benefits but are limited by their\nsmall size, narrow scope, and exposure to LLM training. These gaps highlight\nthe need for secure, comprehensive, and representative datasets. Conclusion: A\nstandardized framework is critical for evaluating LLMs in medicine.\nCollaborative efforts among institutions and policymakers are needed to ensure\ndatasets and methodologies are rigorous, unbiased, and reflective of clinical\ncomplexities.", "AI": {"tldr": "\u73b0\u6709\u7684\u533b\u5b66\u95ee\u7b54\u6570\u636e\u96c6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7f3a\u4e4f\u4e34\u5e8a\u76f8\u5173\u6027\u548c\u900f\u660e\u5ea6\u3002\u9700\u8981\u534f\u4f5c\u52aa\u529b\u6765\u521b\u5efa\u66f4\u4e25\u683c\u3001\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u6846\u67b6\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u533b\u5b66\u95ee\u9898\u56de\u7b54\u65b9\u9762\u7684\u5f53\u524d\u5c40\u9650\u6027\uff0c\u91cd\u70b9\u5173\u6ce8\u7528\u4e8e\u5176\u8bc4\u4f30\u7684\u6570\u636e\u96c6\u8d28\u91cf\u3002", "method": "\u5bf9\u5e7f\u6cdb\u4f7f\u7528\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u57fa\u51c6\u6570\u636e\u96c6\uff08\u5305\u62ec MedQA\u3001MedMCQA\u3001PubMedQA \u548c MMLU\uff09\u7684\u4e25\u8c28\u6027\u3001\u900f\u660e\u5ea6\u548c\u4e0e\u4e34\u5e8a\u573a\u666f\u7684\u76f8\u5173\u6027\u8fdb\u884c\u4e86\u5ba1\u67e5\u3002\u8fd8\u5206\u6790\u4e86\u533b\u5b66\u671f\u520a\u4e2d\u7684\u6311\u6218\u6027\u95ee\u9898\u7b49\u66ff\u4ee3\u65b9\u6848\uff0c\u4ee5\u786e\u5b9a\u5176\u4f5c\u4e3a\u65e0\u504f\u89c1\u8bc4\u4f30\u5de5\u5177\u7684\u6f5c\u529b\u3002", "result": "\u5927\u591a\u6570\u73b0\u6709\u6570\u636e\u96c6\u7f3a\u4e4f\u4e34\u5e8a\u73b0\u5b9e\u6027\u3001\u900f\u660e\u5ea6\u548c\u5065\u5168\u7684\u9a8c\u8bc1\u8fc7\u7a0b\u3002\u516c\u5f00\u7684\u6311\u6218\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e9b\u597d\u5904\uff0c\u4f46\u53d7\u5230\u89c4\u6a21\u5c0f\u3001\u8303\u56f4\u7a84\u4ee5\u53ca\u53ef\u80fd\u88ab\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u66b4\u9732\u7684\u9650\u5236\u3002\u8fd9\u4e9b\u5dee\u8ddd\u51f8\u663e\u4e86\u5bf9\u5b89\u5168\u3001\u5168\u9762\u548c\u5177\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\u96c6\u7684\u9700\u6c42\u3002", "conclusion": "\u9700\u8981\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u6846\u67b6\u6765\u8bc4\u4f30\u533b\u5b66\u9886\u57df\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u673a\u6784\u548c\u653f\u7b56\u5236\u5b9a\u8005\u4e4b\u95f4\u7684\u534f\u4f5c\u52aa\u529b\u5bf9\u4e8e\u786e\u4fdd\u6570\u636e\u96c6\u548c\u65b9\u6cd5\u8bba\u4e25\u8c28\u3001\u516c\u6b63\u4e14\u80fd\u53cd\u6620\u4e34\u5e8a\u590d\u6742\u6027\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.09083", "categories": ["cs.GT", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09083", "abs": "https://arxiv.org/abs/2507.09083", "authors": ["Anand Shah", "Kehang Zhu", "Yanchen Jiang", "Jeffrey G. Wang", "Arif K. Dayi", "John J. Horton", "David C. Parkes"], "title": "Learning from Synthetic Labs: Language Models as Auction Participants", "comment": null, "summary": "This paper investigates the behavior of simulated AI agents (large language\nmodels, or LLMs) in auctions, introducing a novel synthetic data-generating\nprocess to help facilitate the study and design of auctions. We find that LLMs\n-- when endowed with chain of thought reasoning capacity -- agree with the\nexperimental literature in auctions across a variety of classic auction\nformats. In particular, we find that LLM bidders produce results consistent\nwith risk-averse human bidders; that they perform closer to theoretical\npredictions in obviously strategy-proof auctions; and, that they succumb to the\nwinner's curse in common value settings. On prompting, we find that LLMs are\nnot very sensitive to naive changes in prompts (e.g., language, currency) but\ncan improve dramatically towards theoretical predictions with the right mental\nmodel (i.e., the language of Nash deviations). We run 1,000$+$ auctions for\nless than $\\$$400 with GPT-4 models (three orders of magnitude cheaper than\nmodern auction experiments) and develop a framework flexible enough to run\nauction experiments with any LLM model and a wide range of auction design\nspecifications, facilitating further experimental study by decreasing costs and\nserving as a proof-of-concept for the use of LLM proxies.", "AI": {"tldr": "\u901a\u8fc7\u5408\u6210\u6570\u636e\u548cLLM\u4ee3\u7406\u5546\u8fdb\u884c\u7684\u62cd\u5356\u5b9e\u9a8c\u6bd4\u4f20\u7edf\u5b9e\u9a8c\u4fbf\u5b9c\u5f97\u591a\uff0c\u5e76\u4e14\u7ed3\u679c\u4e0e\u4eba\u7c7b\u884c\u4e3a\u4e00\u81f4\u3002", "motivation": "\u7814\u7a76\u6a21\u62dfAI\u4ee3\u7406\u5546\uff08\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff09\u5728\u62cd\u5356\u4e2d\u7684\u884c\u4e3a\uff0c\u5e76\u4e3a\u62cd\u5356\u7684\u7814\u7a76\u548c\u8bbe\u8ba1\u63d0\u4f9b\u4fbf\u5229\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u8fc7\u7a0b\u6765\u7814\u7a76\u548c\u8bbe\u8ba1\u62cd\u5356\uff0c\u5e76\u4f7f\u7528GPT-4\u6a21\u578b\u8fdb\u884c\u4e861000\u591a\u6b21\u62cd\u5356\u5b9e\u9a8c\u3002", "result": "LLM\u7ade\u6807\u8005\u5728\u5404\u79cd\u7ecf\u5178\u62cd\u5356\u683c\u5f0f\u4e2d\u8868\u73b0\u51fa\u4e0e\u5b9e\u9a8c\u6587\u732e\u4e00\u81f4\u7684\u884c\u4e3a\uff0c\u5305\u62ec\u98ce\u9669\u89c4\u907f\u3001\u5728\u663e\u800c\u6613\u89c1\u5730\u5177\u6709\u7b56\u7565\u4fdd\u8bc1\u7684\u62cd\u5356\u4e2d\u66f4\u63a5\u8fd1\u7406\u8bba\u9884\u6d4b\uff0c\u4ee5\u53ca\u5728\u5171\u540c\u4ef7\u503c\u8bbe\u5b9a\u4e2d\u6613\u53d7\u8d62\u5bb6\u8bc5\u5492\u7684\u5f71\u54cd\u3002", "conclusion": "LLM\u4ee3\u7406\u5546\u5728\u62cd\u5356\u4e2d\u8868\u73b0\u51fa\u4e0e\u4eba\u7c7b\u7ade\u6807\u8005\u76f8\u4f3c\u7684\u884c\u4e3a\uff0c\u5e76\u4e14\u5728\u6070\u5f53\u7684\u63d0\u793a\u4e0b\u53ef\u4ee5\u63a5\u8fd1\u7406\u8bba\u9884\u6d4b\u3002"}}
{"id": "2507.09326", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09326", "abs": "https://arxiv.org/abs/2507.09326", "authors": ["Kanta Takahata", "Jonas Sch\u00f6pf", "Naoki Nishida", "Takahito Aoto"], "title": "Recovering Commutation of Logically Constrained Rewriting and Equivalence Transformations (Full Version)", "comment": "Accepted at the 27th International Symposium on Principles and\n  Practice of Declarative Programming (PPDP) 2025", "summary": "Logically constrained term rewriting is a relatively new rewriting formalism\nthat naturally supports built-in data structures, such as integers and bit\nvectors. In the analysis of logically constrained term rewrite systems\n(LCTRSs), rewriting constrained terms plays a crucial role. However, this\ncombines rewrite rule applications and equivalence transformations in a closely\nintertwined way. This intertwining makes it difficult to establish useful\ntheoretical properties for this kind of rewriting and causes problems in\nimplementations -- namely, that impractically large search spaces are often\nrequired. To address this issue, we propose in this paper a novel notion of\nmost general constrained rewriting, which operates on existentially constrained\nterms, a concept recently introduced by the authors. We define a class of\nleft-linear, left-value-free LCTRSs that are general enough to simulate all\nleft-linear LCTRSs and exhibit the desired key property: most general\nconstrained rewriting commutes with equivalence. This property ensures that\nequivalence transformations can be deferred until after the application of\nrewrite rules, which helps mitigate the issue of large search spaces in\nimplementations. In addition to that, we show that the original rewriting\nformalism on constrained terms can be embedded into our new rewriting formalism\non existentially constrained terms. Thus, our results are expected to have\nsignificant implications for achieving correct and efficient implementations in\ntools operating on LCTRSs.", "AI": {"tldr": "\u903b\u8f91\u7ea6\u675f\u9879\u91cd\u5199\uff08LCTRSs\uff09\u4e2d\u7684\u7ea6\u675f\u9879\u91cd\u5199\u548c\u7b49\u4ef7\u53d8\u6362\u96be\u4ee5\u5904\u7406\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5728\u5b58\u5728\u7ea6\u675f\u9879\u4e0a\u8fd0\u884c\u7684\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u6982\u5ff5\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u53ef\u6a21\u62df\u6240\u6709\u5de6\u7ebf\u6027LCTRSs\u7684LCTRSs\u7c7b\uff0c\u5176\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u4e0e\u7b49\u4ef7\u4ea4\u6362\u3002\u8fd9\u4f7f\u5f97\u7b49\u4ef7\u53d8\u6362\u53ef\u4ee5\u5728\u91cd\u5199\u89c4\u5219\u4e4b\u540e\u63a8\u8fdf\uff0c\u4ece\u800c\u51cf\u5c0f\u4e86\u641c\u7d22\u7a7a\u95f4\u3002\u6b64\u5916\uff0c\u539f\u59cb\u91cd\u5199\u5f62\u5f0f\u53ef\u4ee5\u5d4c\u5165\u5230\u65b0\u7684\u91cd\u5199\u5f62\u5f0f\u4e2d\uff0c\u6709\u671b\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684LCTRSs\u5de5\u5177\u3002", "motivation": "\u89e3\u51b3\u5728\u5206\u6790\u903b\u8f91\u7ea6\u675f\u9879\u91cd\u5199\u7cfb\u7edf\uff08LCTRSs\uff09\u65f6\uff0c\u91cd\u5199\u7ea6\u675f\u9879\u5b58\u5728\u7684\u95ee\u9898\uff0c\u5373\u91cd\u5199\u89c4\u5219\u7684\u5e94\u7528\u548c\u7b49\u4ef7\u53d8\u6362\u7d27\u5bc6\u4ea4\u7ec7\uff0c\u5bfc\u81f4\u96be\u4ee5\u5efa\u7acb\u6709\u7528\u7684\u7406\u8bba\u6027\u8d28\u5e76\u5bfc\u81f4\u5b9e\u73b0\u4e2d\u7684\u95ee\u9898\uff08\u9700\u8981\u8fc7\u5927\u7684\u641c\u7d22\u7a7a\u95f4\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u5728\u5b58\u5728\u7ea6\u675f\u9879\u4e0a\u8fd0\u884c\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5de6\u7ebf\u6027\u3001\u5de6\u503c\u65e0\u5173\u7684LCTRSs\u7c7b\uff0c\u8be5\u7c7b\u53ef\u4ee5\u6a21\u62df\u6240\u6709\u5de6\u7ebf\u6027LCTRSs\u5e76\u5177\u6709\u5173\u952e\u5c5e\u6027\uff1a\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u4e0e\u7b49\u4ef7\u4ea4\u6362\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u5728\u5b58\u5728\u7ea6\u675f\u9879\u4e0a\u8fd0\u884c\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5de6\u7ebf\u6027\u3001\u5de6\u503c\u65e0\u5173\u7684LCTRSs\u7c7b\uff0c\u8be5\u7c7b\u53ef\u4ee5\u6a21\u62df\u6240\u6709\u5de6\u7ebf\u6027LCTRSs\u5e76\u5177\u6709\u5173\u952e\u5c5e\u6027\uff1a\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u4e0e\u7b49\u4ef7\u4ea4\u6362\u3002\u539f\u59cb\u91cd\u5199\u5f62\u5f0f\u53ef\u4ee5\u5d4c\u5165\u5230\u65b0\u7684\u91cd\u5199\u5f62\u5f0f\u4e2d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u6982\u5ff5\uff0c\u8be5\u6982\u5ff5\u5728\u5b58\u5728\u7ea6\u675f\u9879\u4e0a\u8fd0\u884c\uff0c\u5e76\u5b9a\u4e49\u4e86\u4e00\u4e2a\u5de6\u7ebf\u6027\u3001\u5de6\u503c\u65e0\u5173\u7684LCTRSs\u7c7b\uff0c\u8be5\u7c7b\u53ef\u4ee5\u6a21\u62df\u6240\u6709\u5de6\u7ebf\u6027LCTRSs\u5e76\u5177\u6709\u6240\u9700\u7684\u5173\u952e\u5c5e\u6027\uff1a\u6700\u4e00\u822c\u7684\u7ea6\u675f\u91cd\u5199\u4e0e\u7b49\u4ef7\u4ea4\u6362\u3002\u8fd9\u786e\u4fdd\u4e86\u7b49\u4ef7\u53d8\u6362\u53ef\u4ee5\u5728\u91cd\u5199\u89c4\u5219\u5e94\u7528\u4e4b\u540e\u63a8\u8fdf\uff0c\u6709\u52a9\u4e8e\u51cf\u8f7b\u5b9e\u73b0\u4e2d\u8fc7\u5927\u641c\u7d22\u7a7a\u95f4\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u539f\u59cb\u7ea6\u675f\u9879\u4e0a\u7684\u91cd\u5199\u5f62\u5f0f\u53ef\u4ee5\u5d4c\u5165\u5230\u5b58\u5728\u7ea6\u675f\u9879\u4e0a\u7684\u65b0\u91cd\u5199\u5f62\u5f0f\u4e2d\u3002\u56e0\u6b64\uff0c\u8fd9\u4e9b\u7ed3\u679c\u6709\u671b\u5bf9\u5b9e\u73b0\u6b63\u786e\u4e14\u9ad8\u6548\u7684LCTRSs\u5de5\u5177\u4ea7\u751f\u91cd\u5927\u5f71\u54cd\u3002"}}
{"id": "2507.08974", "categories": ["eess.SP", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.08974", "abs": "https://arxiv.org/abs/2507.08974", "authors": ["Thien Hieu Hoang", "Tri Nhu Do", "Georges Kaddoum"], "title": "Domain Adaptation-Enabled Realistic Map-Based Channel Estimation for MIMO-OFDM", "comment": null, "summary": "Accurate channel estimation is crucial for the improvement of signal\nprocessing performance in wireless communications. However, traditional\nmodel-based methods frequently experience difficulties in dynamic environments.\nSimilarly, alternative machine-learning approaches typically lack\ngeneralization across different datasets due to variations in channel\ncharacteristics. To address this issue, in this study, we propose a novel\ndomain adaptation approach to bridge the gap between the quasi-static channel\nmodel (QSCM) and the map-based channel model (MBCM). Specifically, we first\nproposed a channel estimation pipeline that takes into account realistic\nchannel simulation to train our foundation model. Then, we proposed domain\nadaptation methods to address the estimation problem. Using simulation-based\ntraining to reduce data requirements for effective application in practical\nwireless environments, we find that the proposed strategy enables robust model\nperformance, even with limited true channel information.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u4fe1\u9053\u4f30\u8ba1\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u9002\u5e94\u4e0d\u540c\u7684\u4fe1\u9053\u6a21\u578b\uff0c\u5e76\u5728\u771f\u5b9e\u4fe1\u9053\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u7a33\u5065\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5b58\u5728\u56f0\u96be\uff0c\u800c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u6570\u636e\u96c6\u4e4b\u95f4\u7f3a\u4e4f\u6cdb\u5316\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u57df\u9002\u5e94\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57df\u9002\u5e94\u65b9\u6cd5\u6765\u5f25\u5408\u51c6\u9759\u6001\u4fe1\u9053\u6a21\u578b\uff08QSCM\uff09\u548c\u57fa\u4e8e\u5730\u56fe\u7684\u4fe1\u9053\u6a21\u578b\uff08MBCM\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u9996\u5148\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u4e86\u5b9e\u9645\u4fe1\u9053\u4eff\u771f\u7684\u4fe1\u9053\u4f30\u8ba1\u6d41\u7a0b\u6765\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u57df\u9002\u5e94\u65b9\u6cd5\u6765\u89e3\u51b3\u4f30\u8ba1\u95ee\u9898\u3002", "result": "\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u7a33\u5065\u7684\u6a21\u578b\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u771f\u5b9e\u4fe1\u9053\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57df\u9002\u5e94\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5f25\u5408\u51c6\u9759\u6001\u4fe1\u9053\u6a21\u578b\uff08QSCM\uff09\u548c\u57fa\u4e8e\u5730\u56fe\u7684\u4fe1\u9053\u6a21\u578b\uff08MBCM\uff09\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5373\u4f7f\u5728\u771f\u5b9e\u4fe1\u9053\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5065\u7684\u6a21\u578b\u6027\u80fd\u3002"}}
{"id": "2507.08960", "categories": ["cs.MA", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08960", "abs": "https://arxiv.org/abs/2507.08960", "authors": ["Andrew Estornell", "Jean-Francois Ton", "Muhammad Faaiz Taufiq", "Hang Li"], "title": "How to Train a Leader: Hierarchical Reasoning in Multi-Agent LLMs", "comment": null, "summary": "Large Language Models (LLMs) have achieved strong performance on a wide range\nof complex reasoning tasks, yet further gains are often possible by leveraging\nthe complementary strengths of multiple models. While multi-agent frameworks\ncan improve solution quality by leveraging multiple LLMs, existing methods are\noften computationally expensive, both at training and inference time. In this\nwork, we introduce a hierarchical multi-agent framework that addresses these\nchallenges by training only a single leader LLM to coordinate a team of\nuntrained peer agents. To this end, we propose Multi-agent guided Leader Policy\n\\textbf{O}ptimization (MLPO), a novel approach which trains the leader to\nevaluate and synthesize agent responses without auxiliary value networks or\nexplicit agent feedback. Leaders trained with MLPO exhibit improved performance\nnot only when interacting with the agent team at inference time, but also enjoy\nimproved performance when deployed in single-agent settings without the team.\nEmpirical results on Big-Bench Hard (BBH), MATH, and MMLU demonstrate that our\nframework achieves substantial performance improvements over both single-agent\nand multi-agent baselines. Our results highlight the effectiveness and\nefficiency of training a single, flexible leader for collaborative reasoning in\nmulti-agent LLM systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMLPO\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u5355\u4e2a\u9886\u5bfcLLM\u6765\u534f\u8c03\u672a\u8bad\u7ec3\u7684\u5bf9\u7b49\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684LLM\u6846\u67b6\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u7684LLM\u6846\u67b6\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u667a\u80fd\u4f53\u5f15\u5bfc\u9886\u5bfc\u8005\u7b56\u7565\u4f18\u5316\uff08MLPO\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8bad\u7ec3\u9886\u5bfcLLM\u6765\u534f\u8c03\u4e00\u7fa4\u672a\u8bad\u7ec3\u7684\u5bf9\u7b49\u667a\u80fd\u4f53\uff0c\u5e76\u80fd\u8bc4\u4f30\u548c\u7efc\u5408\u667a\u80fd\u4f53\u54cd\u5e94\uff0c\u800c\u65e0\u9700\u8f85\u52a9\u4ef7\u503c\u7f51\u7edc\u6216\u663e\u5f0f\u667a\u80fd\u4f53\u53cd\u9988\u3002", "result": "\u5728Big-Bench Hard (BBH)\u3001MATH\u548cMMLU\u4e0a\uff0c\u4e0e\u5355\u4e00\u667a\u80fd\u4f53\u548c\u591a\u667a\u80fd\u4f53\u57fa\u7ebf\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5728\u63a8\u7406\u65f6\u4e0e\u667a\u80fd\u4f53\u56e2\u961f\u4ea4\u4e92\u65f6\u8868\u73b0\u51fa\u6539\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6ca1\u6709\u56e2\u961f\u7684\u60c5\u51b5\u4e0b\u5355\u72ec\u90e8\u7f72\u65f6\u4e5f\u5177\u6709\u6539\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8bad\u7ec3\u5355\u4e2a\u7075\u6d3b\u7684\u9886\u5bfc\u8005\uff0c\u5728\u591a\u4e3b\u4f53LLM\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u534f\u4f5c\u63a8\u7406\uff0c\u80fd\u591f\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.08832", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08832", "abs": "https://arxiv.org/abs/2507.08832", "authors": ["Niranjan Mallikarjun Sindhur", "Pavithra C", "Nivya Muchikel"], "title": "A Hybrid Machine Learning Framework for Optimizing Crop Selection via Agronomic and Economic Forecasting", "comment": null, "summary": "Farmers in developing regions like Karnataka, India, face a dual challenge:\nnavigating extreme market and climate volatility while being excluded from the\ndigital revolution due to literacy barriers. This paper presents a novel\ndecision support system that addresses both challenges through a unique\nsynthesis of machine learning and human-computer interaction. We propose a\nhybrid recommendation engine that integrates two predictive models: a Random\nForest classifier to assess agronomic suitability based on soil, climate, and\nreal-time weather data, and a Long Short-Term Memory (LSTM) network to forecast\nmarket prices for agronomically viable crops. This integrated approach shifts\nthe paradigm from \"what can grow?\" to \"what is most profitable to grow?\",\nproviding a significant advantage in mitigating economic risk. The system is\ndelivered through an end-to-end, voice-based interface in the local Kannada\nlanguage, leveraging fine-tuned speech recognition and high-fidelity speech\nsynthesis models to ensure accessibility for low-literacy users. Our results\nshow that the Random Forest model achieves 98.5% accuracy in suitability\nprediction, while the LSTM model forecasts harvest-time prices with a low\nmargin of error. By providing data-driven, economically optimized\nrecommendations through an inclusive interface, this work offers a scalable and\nimpactful solution to enhance the financial resilience of marginalized farming\ncommunities.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u5370\u5ea6\u5361\u7eb3\u5854\u514b\u90a6\u7684\u519c\u6c11\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u673a\u5668\u5b66\u4e60\uff08\u968f\u673a\u68ee\u6797\u548cLSTM\uff09\u548c\u8bed\u97f3\u754c\u9762\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff0c\u4ee5\u9884\u6d4b\u519c\u4f5c\u7269\u9002\u5b9c\u6027\u548c\u5e02\u573a\u4ef7\u683c\uff0c\u4ece\u800c\u63d0\u9ad8\u519c\u6c11\u7684\u76c8\u5229\u80fd\u529b\u548c\u8d22\u52a1\u97e7\u6027\u3002", "motivation": "\u53d1\u5c55\u4e2d\u5730\u533a\u7684\u519c\u6c11\uff08\u4f8b\u5982\u5370\u5ea6\u5361\u7eb3\u5854\u514b\u90a6\uff09\u9762\u4e34\u7740\u4e25\u5cfb\u7684\u5e02\u573a\u548c\u6c14\u5019\u6ce2\u52a8\u6311\u6218\uff0c\u540c\u65f6\u7531\u4e8e\u8bc6\u5b57\u7387\u7684\u9650\u5236\uff0c\u4ed6\u4eec\u5f80\u5f80\u88ab\u6392\u65a5\u5728\u6570\u5b57\u5316\u9769\u547d\u4e4b\u5916\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u89e3\u51b3\u8fd9\u4e9b\u53cc\u91cd\u6311\u6218\u7684\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4eba\u673a\u4ea4\u4e92\u7684\u6df7\u5408\u63a8\u8350\u5f15\u64ce\u3002\u8be5\u5f15\u64ce\u96c6\u6210\u4e86\u4e24\u4e2a\u9884\u6d4b\u6a21\u578b\uff1a1. \u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff0c\u5229\u7528\u571f\u58e4\u3001\u6c14\u5019\u548c\u5b9e\u65f6\u5929\u6c14\u6570\u636e\u8bc4\u4f30\u519c\u827a\u9002\u5b9c\u6027\uff1b2. \u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\uff0c\u9884\u6d4b\u519c\u827a\u4e0a\u53ef\u884c\u4f5c\u7269\u7684\u5e02\u573a\u4ef7\u683c\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u3001\u57fa\u4e8e\u8bed\u97f3\u7684\u672c\u5730\u5316\uff08\u5361\u7eb3\u8fbe\u8bed\uff09\u754c\u9762\u4ea4\u4ed8\uff0c\u5229\u7528\u4e86\u7ecf\u8fc7\u5fae\u8c03\u7684\u8bed\u97f3\u8bc6\u522b\u548c\u9ad8\u4fdd\u771f\u8bed\u97f3\u5408\u6210\u6a21\u578b\uff0c\u4ee5\u786e\u4fdd\u4f4e\u8bc6\u5b57\u7387\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "result": "\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u9002\u5b9c\u6027\u9884\u6d4b\u65b9\u9762\u8fbe\u5230\u4e8698.5%\u7684\u51c6\u786e\u7387\u3002LSTM\u6a21\u578b\u5728\u9884\u6d4b\u6536\u83b7\u65f6\u671f\u7684\u4ef7\u683c\u65b9\u9762\u5177\u6709\u8f83\u4f4e\u7684\u8bef\u5dee\u7387\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u901a\u8fc7\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u548c\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4ea4\u4e92\u8bbe\u8ba1\uff0c\u4e3a\u53d7\u5e02\u573a\u548c\u6c14\u5019\u6ce2\u52a8\u5f71\u54cd\u7684\u519c\u6c11\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\u3002\u5b83\u901a\u8fc7\u6df7\u5408\u63a8\u8350\u5f15\u64ce\uff0c\u96c6\u6210\u4e86\u4e00\u4e2a\u968f\u673a\u68ee\u6797\u5206\u7c7b\u5668\uff08\u7528\u4e8e\u8bc4\u4f30\u519c\u827a\u9002\u5b9c\u6027\uff09\u548c\u4e00\u4e2a\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u7f51\u7edc\uff08\u7528\u4e8e\u9884\u6d4b\u5e02\u573a\u4ef7\u683c\uff09\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u4ece\u201c\u4ec0\u4e48\u53ef\u4ee5\u79cd\u690d\uff1f\u201d\u5230\u201c\u4ec0\u4e48\u6700\u6709\u5229\u53ef\u56fe\uff1f\u201d\u7684\u8f6c\u53d8\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u7ecf\u6d4e\u98ce\u9669\u3002\u8be5\u7cfb\u7edf\u91c7\u7528\u4e86\u672c\u5730\u5316\uff08\u5361\u7eb3\u8fbe\u8bed\uff09\u7684\u8bed\u97f3\u4ea4\u4e92\u754c\u9762\uff0c\u514b\u670d\u4e86\u8bc6\u5b57\u7387\u4f4e\u7684\u969c\u788d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u968f\u673a\u68ee\u6797\u6a21\u578b\u5728\u9002\u5b9c\u6027\u9884\u6d4b\u65b9\u9762\u8fbe\u523098.5%\u7684\u51c6\u786e\u7387\uff0cLSTM\u6a21\u578b\u5728\u9884\u6d4b\u6536\u83b7\u65f6\u671f\u7684\u4ef7\u683c\u65b9\u9762\u8bef\u5dee\u7387\u4f4e\u3002\u8fd9\u9879\u5de5\u4f5c\u4e3a\u589e\u5f3a\u8fb9\u7f18\u5316\u519c\u4e1a\u793e\u533a\u7684\u8d22\u52a1\u97e7\u6027\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u5f71\u54cd\u529b\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09112", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09112", "abs": "https://arxiv.org/abs/2507.09112", "authors": ["W. Zeng"], "title": "Tunneling spin Hall effect induced by unconventional $p$-wave magnetism", "comment": "10 pages, 4 figures", "summary": "We propose a tunneling spin Hall effect in a normal metal/$p$-wave\nmagnet/superconductor junction. It is found that the Andreev reflection in the\nnormal lead is spin-dependent and exhibits strong asymmetry with respect to the\ntransverse momentum, giving rise to a pure transverse spin Hall current with\nzero net charge. The transverse spin conductance is analytically derived using\nthe nonequilibrium Green's function approach, revealing that the predicted spin\nHall effect is governed by the direction of the Fermi surface splitting in the\n$p$-wave magnet. A finite transverse spin current with a large spin Hall angle\narises when the line connecting the centers of the spin-split Fermi surfaces is\nperpendicular to the normal direction of the junction, which indicates a highly\nefficient charge-to-spin conversion, suggesting potential applications in\nspintronic devices.", "AI": {"tldr": "\u5728\u6b63\u5e38\u91d1\u5c5e/p\u6ce2\u78c1\u4f53/\u8d85\u5bfc\u4f53\u7ed3\u4e2d\u53d1\u73b0\u4e86\u96a7\u9053\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff0c\u5b83\u4ea7\u751f\u7eaf\u6a2a\u5411\u81ea\u65cb\u6d41\uff0c\u53ef\u901a\u8fc7\u8d39\u7c73\u9762\u5206\u88c2\u8c03\u63a7\uff0c\u5728spintronic\u5668\u4ef6\u4e2d\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u7814\u7a76\u6b63\u5e38\u91d1\u5c5e/p\u6ce2\u78c1\u4f53/\u8d85\u5bfc\u4f53\u7ed3\u4e2d\u7684\u96a7\u9053\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u53ca\u5176\u4ea7\u751f\u673a\u5236\u3002", "method": "\u4f7f\u7528\u975e\u5e73\u8861\u683c\u6797\u51fd\u6570\u65b9\u6cd5\u89e3\u6790\u63a8\u5bfc\u4e86\u6a2a\u5411\u81ea\u65cb\u7535\u5bfc\u3002", "result": "\u53d1\u73b0\u4e86\u81ea\u65cb\u4f9d\u8d56\u4e14\u5173\u4e8e\u6a2a\u5411\u52a8\u91cf\u5177\u6709\u5f3a\u4e0d\u5bf9\u79f0\u6027\u7684\u5b89\u5fb7\u70c8\u592b\u53cd\u5c04\uff0c\u4ea7\u751f\u4e86\u7eaf\u6a2a\u5411\u81ea\u65cb\u970d\u5c14\u7535\u6d41\u3002\u5f53\u8d39\u7c73\u9762\u5206\u88c2\u4e2d\u5fc3\u8fde\u7ebf\u5782\u76f4\u4e8e\u7ed3\u7684\u6cd5\u7ebf\u65b9\u5411\u65f6\uff0c\u4f1a\u51fa\u73b0\u6709\u9650\u7684\u6a2a\u5411\u81ea\u65cb\u6d41\u548c\u5927\u7684\u81ea\u65cb\u970d\u5c14\u89d2\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u7535\u8377\u5230\u81ea\u65cb\u7684\u8f6c\u6362\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\u4e86\u65b0\u7684\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u73b0\u8c61\uff0c\u5373\u5728\u6b63\u5e38\u91d1\u5c5e/p\u6ce2\u78c1\u4f53/\u8d85\u5bfc\u4f53\u7ed3\u4e2d\u5b58\u5728\u96a7\u9053\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff0c\u5176\u7279\u70b9\u662f\u7eaf\u6a2a\u5411\u81ea\u65cb\u970d\u5c14\u7535\u6d41\u4e14\u51c0\u7535\u8377\u4e3a\u96f6\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7p\u6ce2\u78c1\u4f53\u4e2d\u7684\u8d39\u7c73\u9762\u5206\u88c2\u65b9\u5411\u6765\u8c03\u63a7\uff0c\u5728spintronic\u5668\u4ef6\u4e2d\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.09507", "categories": ["cs.DS", "cs.DM", "cs.GT", "68W27 (Primary) 91A68, 68R05 (Secondary)", "F.2.2; G.2.1"], "pdf": "https://arxiv.org/pdf/2507.09507", "abs": "https://arxiv.org/abs/2507.09507", "authors": ["Moran Feldman", "Ola Svensson", "Rico Zenklusen"], "title": "Nearly Tight Sample Complexity for Matroid Online Contention Resolution", "comment": "24 pages", "summary": "Due to their numerous applications, in particular in Mechanism Design,\nProphet Inequalities have experienced a surge of interest. They describe\ncompetitive ratios for basic stopping time problems where random variables get\nrevealed sequentially. A key drawback in the classical setting is the\nassumption of full distributional knowledge of the involved random variables,\nwhich is often unrealistic. A natural way to address this is via sample-based\napproaches, where only a limited number of samples from the distribution of\neach random variable is available. Recently, Fu, Lu, Gavin Tang, Wu, Wu, and\nZhang (2024) showed that sample-based Online Contention Resolution Schemes\n(OCRS) are a powerful tool to obtain sample-based Prophet Inequalities. They\npresented the first sample-based OCRS for matroid constraints, which is a\nheavily studied constraint family in this context, as it captures many\ninteresting settings. This allowed them to get the first sample-based Matroid\nProphet Inequality, using $O(\\log^4 n)$ many samples (per random variable),\nwhere $n$ is the number of random variables, while obtaining a constant\ncompetitiveness of $\\frac{1}{4}-\\varepsilon$.\n  We present a nearly optimal sample-based OCRS for matroid constraints, which\nuses only $O(\\log \\rho \\cdot \\log^2\\log\\rho)$ many samples, almost matching a\nknown lower bound of $\\Omega(\\log \\rho)$, where $\\rho \\leq n$ is the rank of\nthe matroid. Through the above-mentioned connection to Prophet Inequalities,\nthis yields a sample-based Matroid Prophet Inequality using only $O(\\log n +\n\\log\\rho \\cdot \\log^2\\log\\rho)$ many samples, and matching the competitiveness\nof $\\frac{1}{4}-\\varepsilon$, which is the best known competitiveness for the\nconsidered almighty adversary setting even when the distributions are fully\nknown.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7701\u6837\u672c\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u9884\u8a00\u673a\u4e0d\u7b49\u5f0f\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u56e2\u7ea6\u675f\u4e0b\uff0c\u6837\u672c\u91cf\u5927\u5927\u51cf\u5c11\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6700\u4f18\u7684\u7ade\u4e89\u6027\u3002", "motivation": "\u7ecf\u5178\u9884\u8a00\u673a\u4e0d\u7b49\u5f0f\u9700\u8981\u5b8c\u5168\u4e86\u89e3\u968f\u673a\u53d8\u91cf\u7684\u5206\u5e03\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e2d\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u57fa\u4e8e\u6709\u9650\u6837\u672c\u7684\u65b9\u6cd5\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u6837\u672c\u7684OCRS\u7b97\u6cd5\u5728\u5904\u7406\u56e2\u7ea6\u675f\u95ee\u9898\u65f6\u9700\u8981\u8f83\u591a\u7684\u6837\u672c\u91cf\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6837\u672c\u7684\u5728\u7ebf\u51b2\u7a81\u89e3\u51b3\u673a\u5236\uff08OCRS\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u591f\u5904\u7406\u56e2\u7ea6\u675f\u95ee\u9898\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684OCRS\u7b97\u6cd5\u4ec5\u9700O(log \u03c1 \u22c5 log^2 log \u03c1)\u4e2a\u6837\u672c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8fd9\u76f4\u63a5\u8f6c\u5316\u4e3a\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6837\u672c\u7684\u56e2\u9884\u8a00\u673a\u4e0d\u7b49\u5f0f\uff0c\u5176\u7ade\u4e89\u6027\u8fbe\u5230\u4e861/4 - \u03b5\uff0c\u5e76\u4e14\u6837\u672c\u91cf\u4e5f\u5f97\u5230\u4e86\u663e\u8457\u7684\u964d\u4f4e\uff0c\u4ec5\u9700O(log n + log \u03c1 \u22c5 log^2 log \u03c1)\u4e2a\u6837\u672c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u63a5\u8fd1\u6700\u4f18\u7684\u57fa\u4e8e\u6837\u672c\u7684OCRS\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u56e2\u7ea6\u675f\u95ee\u9898\u3002\u8be5\u7b97\u6cd5\u4ec5\u9700O(log \u03c1 \u22c5 log^2 log \u03c1)\u4e2a\u6837\u672c\uff0c\u5176\u4e2d\u03c1\u662f\u56e2\u7684\u79e9\u3002\u8fd9\u4f7f\u5f97\u57fa\u4e8e\u6837\u672c\u7684\u56e2\u9884\u8a00\u673a\u4e0d\u7b49\u5f0f\u80fd\u591f\u4ee51/4 - \u03b5\u7684\u7ade\u4e89\u6027\u5f97\u5230\uff0c\u8fd9\u662f\u5728\u5168\u77e5\u5bf9\u624b\u6a21\u578b\u4e0b\u5df2\u77e5\u6700\u4f18\u7684\u7ade\u4e89\u6027\u3002"}}
{"id": "2507.09441", "categories": ["cs.GR", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09441", "abs": "https://arxiv.org/abs/2507.09441", "authors": ["Ankit Sanjyal"], "title": "RectifiedHR: High-Resolution Diffusion via Energy Profiling and Adaptive Guidance Scheduling", "comment": "8 Pages, 10 Figures, Pre-Print Version, Code Available at:\n  https://github.com/ANKITSANJYAL/RectifiedHR", "summary": "High-resolution image synthesis with diffusion models often suffers from\nenergy instabilities and guidance artifacts that degrade visual quality. We\nanalyze the latent energy landscape during sampling and propose adaptive\nclassifier-free guidance (CFG) schedules that maintain stable energy\ntrajectories. Our approach introduces energy-aware scheduling strategies that\nmodulate guidance strength over time, achieving superior stability scores\n(0.9998) and consistency metrics (0.9873) compared to fixed-guidance\napproaches. We demonstrate that DPM++ 2M with linear-decreasing CFG scheduling\nyields optimal performance, providing sharper, more faithful images while\nreducing artifacts. Our energy profiling framework serves as a powerful\ndiagnostic tool for understanding and improving diffusion model behavior.", "AI": {"tldr": "\u901a\u8fc7\u80fd\u91cf\u753b\u50cf\u548c\u81ea\u9002\u5e94CFG\u7b56\u7565\uff0c\u63d0\u5347\u4e86\u6269\u6563\u6a21\u578b\u56fe\u50cf\u5408\u6210\u7684\u7a33\u5b9a\u6027\u548c\u56fe\u50cf\u8d28\u91cf\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u80fd\u91cf\u4e0d\u7a33\u5b9a\u548c\u6307\u5bfc\u4f2a\u5f71\u95ee\u9898\u5bfc\u81f4\u89c6\u89c9\u8d28\u91cf\u4e0b\u964d\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u91c7\u6837\u8fc7\u7a0b\u4e2d\u7684\u6f5c\u80fd\u91cf\u548c\u63d0\u51fa\u81ea\u9002\u5e94CFG\u8c03\u5ea6\u7b56\u7565\u6765\u89e3\u51b3\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u4e2d\u7684\u80fd\u91cf\u4e0d\u7a33\u5b9a\u548c\u6307\u5bfc\u4f2a\u5f71\u95ee\u9898\uff0c\u5176\u4e2d\u80fd\u91cf\u611f\u77e5\u8c03\u5ea6\u7b56\u7565\u901a\u8fc7\u8c03\u8282\u6307\u5bfc\u5f3a\u5ea6\u6765\u7ef4\u6301\u80fd\u91cf\u8f68\u8ff9\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u81ea\u9002\u5e94CFG\u8c03\u5ea6\u7b56\u7565\u76f8\u6bd4\u56fa\u5b9a\u6307\u5bfc\u65b9\u6cd5\uff0c\u5728\u7a33\u5b9a\u6027\u548c\u4e00\u81f4\u6027\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\uff0c\u5177\u4f53\u8868\u73b0\u4e3a\u66f4\u9ad8\u7684\u7a33\u5b9a\u6027\u5206\u6570\uff080.9998\uff09\u548c\u4e00\u81f4\u6027\u6307\u6807\uff080.9873\uff09\uff0c\u5e76\u80fd\u751f\u6210\u66f4\u6e05\u6670\u3001\u66f4\u4fdd\u771f\u7684\u56fe\u50cf\uff0c\u540c\u65f6\u51cf\u5c11\u4f2a\u5f71\u3002", "conclusion": "\u901a\u8fc7\u80fd\u91cf\u753b\u50cf\u548c\u81ea\u9002\u5e94\u6761\u4ef6\u65e0\u5206\u7c7b\u6307\u5bfc\uff08CFG\uff09\u7b56\u7565\u7684\u5f15\u5165\uff0c\u8be5\u7814\u7a76\u663e\u8457\u63d0\u9ad8\u4e86\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u5408\u6210\u7684\u7a33\u5b9a\u6027\u548c\u89c6\u89c9\u8d28\u91cf\uff0c\u5c24\u5176\u662f\u5728DPM++ 2M\u6a21\u578b\u4e2d\u91c7\u7528\u7ebf\u6027\u9012\u51cfCFG\u7b56\u7565\u65f6\uff0c\u80fd\u751f\u6210\u66f4\u6e05\u6670\u3001\u66f4\u4fdd\u771f\u7684\u56fe\u50cf\uff0c\u5e76\u6709\u6548\u51cf\u5c11\u4e86\u4f2a\u5f71\u3002"}}
{"id": "2507.08892", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08892", "abs": "https://arxiv.org/abs/2507.08892", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "comment": "13 pages", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals.", "AI": {"tldr": "\u901a\u8fc7\u501f\u9274\u684c\u9762\u89d2\u8272\u626e\u6f14\u6e38\u620f\uff08TTRPG\uff09\u548c\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\u6765\u652f\u6301\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u591a\u53c2\u4e0e\u8005\u73af\u5883\u4e2d\u7684\u591a\u79cd\u5e94\u7528\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u5173\u6ce8\u70b9\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u6027\u548c\u5feb\u901f\u8fed\u4ee3\uff0cConcordia\u5e93\u662f\u8fd9\u4e00\u7406\u5ff5\u7684\u4f8b\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5728\u591a\u53c2\u4e0e\u8005\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u5e94\u7528\uff08\u5305\u62ec\u793e\u4f1a\u79d1\u5b66\u5efa\u6a21\u3001\u4ea4\u4e92\u53d9\u4e8b\u548cAI\u8bc4\u4f30\uff09\uff0c\u9700\u8981\u4e00\u4e2a\u7075\u6d3b\u7684\u573a\u666f\u5b9a\u4e49\u6846\u67b6\u3002\u73b0\u6709\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u8fd9\u4e9b\u4e0d\u540c\u7528\u4f8b\u7684\u9700\u6c42\u3002", "method": "\u53d7\u684c\u9762\u89d2\u8272\u626e\u6f14\u6e38\u620f\uff08TTRPG\uff09\u7684\u542f\u53d1\uff0c\u91c7\u7528\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\u6a21\u5f0f\u6765\u6784\u5efa\u4e00\u4e2a\u7075\u6d3b\u7684\u573a\u666f\u5b9a\u4e49\u6846\u67b6\u3002\u5728\u6b64\u6846\u67b6\u4e2d\uff0c\u6e38\u620f\u4e3b\u6301\u4eba\uff08GM\uff09\u88ab\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u53ef\u914d\u7f6e\u7684\u5b9e\u4f53\uff0c\u7531\u7ec4\u4ef6\u6784\u6210\uff0c\u8d1f\u8d23\u7ba1\u7406\u73af\u5883\u548c\u751f\u6210\u6545\u4e8b\u7684\u975e\u73a9\u5bb6\u9a71\u52a8\u90e8\u5206\u3002", "result": "\u901a\u8fc7\u91c7\u7528\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5173\u6ce8\u70b9\u5206\u79bb\uff0c\u4fbf\u4e8e\u5de5\u7a0b\u5e08\u5904\u7406\u5e95\u5c42\u5b9e\u73b0\u3001\u521b\u5efa\u53ef\u91cd\u7528\u7ec4\u4ef6\uff0c\u4ee5\u53ca\u7531\u8bbe\u8ba1\u5e08\u8fdb\u884c\u7ec4\u4ef6\u7684\u7ec4\u5408\u548c\u914d\u7f6e\u3002\u8fd9\u6709\u52a9\u4e8e\u5feb\u901f\u8fed\u4ee3\u3001\u4fdd\u6301\u6a21\u5757\u5316\u548c\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002Concordia\u5e93\u7684\u6f14\u8fdb\u5c55\u793a\u4e86\u8be5\u54f2\u5b66\u5982\u4f55\u4f7f\u7528\u6237\u80fd\u591f\u6709\u6548\u5730\u914d\u7f6e\u6ee1\u8db3\u5176\u7279\u5b9a\u76ee\u6807\u7684\u573a\u666f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8bba\u901a\u8fc7\u5c06TTRPG\u7684\u7406\u5ff5\u4e0e\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\u76f8\u7ed3\u5408\uff0c\u4e3a\u5728\u591a\u53c2\u4e0e\u8005\u73af\u5883\u4e2d\u652f\u6301\u5404\u79cd\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u5e94\u7528\u63d0\u4f9b\u4e86\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u6846\u67b6\u3002Concordia\u5e93\u7684\u6f14\u8fdb\u8bc1\u660e\u4e86\u5176\u5728\u914d\u7f6e\u548c\u6267\u884c\u591a\u6837\u5316AI\u573a\u666f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10421", "categories": ["cs.AI", "cs.ET", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10421", "abs": "https://arxiv.org/abs/2507.10421", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning", "comment": "International Conference on Education and New Learning Technologies\n  (2025)", "summary": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408BERT\u60c5\u611f\u5206\u6790\u548cXGBoost\u7279\u5f81\u9009\u62e9\u7684\u65b0\u6a21\u578b\uff0c\u53ef\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u8fdc\u7a0b\u5b66\u4e60\u4e2d\u7684\u5b66\u751f\u8f8d\u5b66\u60c5\u51b5\u3002", "motivation": "\u5b66\u6821\u8f8d\u5b66\u662f\u8fdc\u7a0b\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u4e25\u91cd\u95ee\u9898\uff0c\u65e9\u671f\u53d1\u73b0\u5bf9\u4e8e\u6709\u6548\u7684\u5e72\u9884\u548c\u5b66\u751f\u7684\u575a\u6301\u81f3\u5173\u91cd\u8981\u3002\u5229\u7528\u73b0\u6709\u7684\u6559\u80b2\u6570\u636e\u9884\u6d4b\u5b66\u751f\u8f8d\u5b66\u662f\u5b66\u4e60\u5206\u6790\u4e2d\u4e00\u4e2a\u5e7f\u6cdb\u7814\u7a76\u7684\u8bfe\u9898\u3002\u6211\u4eec\u7684\u5408\u4f5c\u4f19\u4f34\u7684\u8fdc\u7a0b\u5b66\u4e60\u5e73\u53f0\u5f3a\u8c03\u4e86\u6574\u5408\u5305\u62ec\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u6570\u636e\u3001\u884c\u4e3a\u6570\u636e\u548c\u60c5\u611f\u5206\u6790\u5728\u5185\u7684\u591a\u6837\u5316\u6570\u636e\u6e90\u7684\u91cd\u8981\u6027\uff0c\u4ee5\u51c6\u786e\u9884\u6d4b\u8f8d\u5b66\u98ce\u9669\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u4f7f\u7528\u53cc\u5411Encoder\u8868\u793aTransformer\uff08BERT\uff09\u6a21\u578b\u5bf9\u5b66\u751f\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u4ee5\u53ca\u5bf9\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u548c\u884c\u4e3a\u6570\u636e\u8fdb\u884c\u5206\u6790\u7684\u6781\u7aef\u68af\u5ea6\u589e\u5f3a\uff08XGBoost\uff09\u3002\u901a\u8fc7\u5bf9\u5b66\u751f\u8bc4\u8bba\u8fdb\u884c\u5fae\u8c03BERT\u4ee5\u6355\u6349\u7ec6\u5fae\u7684\u60c5\u611f\uff0c\u7136\u540e\u5c06\u5176\u4e0e\u4f7f\u7528XGBoost\u4e2d\u7684\u7279\u5f81\u91cd\u8981\u6027\u6280\u672f\u9009\u62e9\u7684\u5173\u952e\u7279\u5f81\u5408\u5e76\u3002", "result": "\u8be5\u6a21\u578b\u5728\u4e0b\u4e00\u5b66\u5e74\u7684\u672a\u89c1\u8fc7\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u51c6\u786e\u7387\u8fbe\u5230\u4e8684%\uff0c\u800c\u57fa\u7ebf\u6a21\u578b\u4e3a82%\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u5728\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u7b49\u5176\u4ed6\u6307\u6807\u4e0a\u4e5f\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u7ed3\u5408BERT\u7684\u60c5\u611f\u5206\u6790\u548cXGBoost\u7684\u7279\u5f81\u9009\u62e9\uff0c\u5728\u9884\u6d4b\u5b66\u6821\u8f8d\u5b66\u65b9\u9762\u53d6\u5f97\u4e8684%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\uff0882%\uff09\uff0c\u5e76\u5728\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u7b49\u5176\u4ed6\u6307\u6807\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.09660", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09660", "abs": "https://arxiv.org/abs/2507.09660", "authors": ["Shuvra S. Bhattacharyya", "Marilyn Wolf"], "title": "Tools and Methodologies for System-Level Design", "comment": "This is a preprint of a chapter to appear in the forthcoming volume\n  Handbook on Electronic Design Automation (third edition), published by Taylor\n  & Francis. The final version may differ", "summary": "System-level design, once the province of board designers, has now become a\ncentral concern for chip designers. Because chip design is a less forgiving\ndesign medium -- design cycles are longer and mistakes are harder to correct --\nsystem-on-chip designers need a more extensive tool suite than may be used by\nboard designers and a variety of tools and methodologies have been developed\nfor system-level design of systems-on-chips (SoCs). System-level design is less\namenable to synthesis than are logic or physical design. As a result,\nsystem-level tools concentrate on modeling, simulation, design space\nexploration, and design verification. The goal of modeling is to correctly\ncapture the system's operational semantics, which helps with both\nimplementation and verification. The study of models of computation provides a\nframework for the description of digital systems. Not only do we need to\nunderstand a particular style of computation, such as dataflow, but we also\nneed to understand how different models of computation can reliably communicate\nwith each other. Design space exploration tools, such as hardware/software\nco-design, develop candidate designs to understand trade-offs. Simulation can\nbe used not only to verify functional correctness but also to supply\nperformance and power/energy information for design analysis. This chapter\nemploys two applications -- video and neural networks -- as examples. Both are\nleading-edge applications that illustrate many important aspects of\nsystem-level design.", "AI": {"tldr": "\u968f\u7740SoC\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u672c\u7ae0\u901a\u8fc7\u89c6\u9891\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u4f8b\u5b50\uff0c\u4ecb\u7ecd\u4e86\u7528\u4e8e\u5efa\u6a21\u3001\u4eff\u771f\u3001\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u8bbe\u8ba1\u7684\u5de5\u5177\u548c\u65b9\u6cd5\u8bba\uff0c\u4ee5\u53ca\u8ba1\u7b97\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u7247\u4e0a\u7cfb\u7edf\uff08SoC\uff09\u8bbe\u8ba1\u7684\u590d\u6742\u6027\u589e\u52a0\uff0c\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u5df2\u6210\u4e3a\u82af\u7247\u8bbe\u8ba1\u4e2d\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u9700\u8981\u6bd4\u4ee5\u5f80\u66f4\u5e7f\u6cdb\u7684\u5de5\u5177\u96c6\u548c\u65b9\u6cd5\u8bba\u3002", "method": "\u901a\u8fc7\u4e3e\u4f8b\u8bf4\u660e\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u4e2d\u7684\u6a21\u578b\u3001\u4eff\u771f\u3001\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u8bbe\u8ba1\u9a8c\u8bc1\u7b49\u5de5\u5177\u548c\u65b9\u6cd5\u8bba\uff0c\u5e76\u7740\u91cd\u4ecb\u7ecd\u4e86\u8ba1\u7b97\u6a21\u578b\u5728\u63cf\u8ff0\u6570\u5b57\u7cfb\u7edf\u4e2d\u7684\u4f5c\u7528\u4ee5\u53ca\u4e0d\u540c\u8ba1\u7b97\u6a21\u578b\u4e4b\u95f4\u901a\u4fe1\u7684\u53ef\u9760\u6027\u3002", "result": "\u8be5\u7ae0\u8282\u901a\u8fc7\u89c6\u9891\u548c\u795e\u7ecf\u7f51\u7edc\u4e24\u4e2a\u5e94\u7528\u6848\u4f8b\uff0c\u9610\u8ff0\u4e86\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u4e2d\u7684\u5173\u952e\u5de5\u5177\u548c\u65b9\u6cd5\uff0c\u5f3a\u8c03\u4e86\u6a21\u578b\u3001\u4eff\u771f\u3001\u8bbe\u8ba1\u7a7a\u95f4\u63a2\u7d22\u548c\u8bbe\u8ba1\u9a8c\u8bc1\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u7cfb\u7edf\u7ea7\u8bbe\u8ba1\u5728\u89c6\u9891\u548c\u795e\u7ecf\u7f51\u7edc\u7b49\u5e94\u7528\u4e2d\u6709\u5f88\u591a\u91cd\u8981\u7684\u65b9\u9762\u9700\u8981\u8bf4\u660e\u3002"}}
{"id": "2507.09126", "categories": ["cond-mat.mtrl-sci", "physics.app-ph", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.09126", "abs": "https://arxiv.org/abs/2507.09126", "authors": ["Seno Aji", "Muhammad Anin Nabail Azhiim", "Nur Ika Puji Ayu", "Adam Badra Cahaya", "Koichi Kusakabe", "Muhammad Aziz Majidi"], "title": "Spin current generation driven by skyrmion dynamics under magnetic anisotropy and polarized microwaves", "comment": "8 pages, 8 figures", "summary": "We have investigated the spin-current pumped by the skyrmion-host material\nwith the lack of inversion symmetry through the microwave resonance process.\nThe effects of magnetic anisotropy and polarized microwaves are examined by\nmicromagnetic simulations. Our results reveal two distinct skyrmion phases,\ndesignated as SkX type-I and II, which emerge at low ($K_z<0.1$ meV) and high\n($K_z>0.1$ meV) magnetic anisotropy constants, respectively, having different\ncharacteristics of spin excitations. The SkX type-I exhibits spin dynamics\nwhere the resonant frequency of the breathing mode is lying in between the\nclockwise and counterclockwise gyration modes of Bloch-type skyrmion at a very\nlow anisotropy, and is crossing over the counterclockwise mode at $K_z \\sim\n0.04$ meV. Meanwhile, the SkX type-II exhibits distinct spin excitations in\nwhich the clockwise mode is notably absent, while the counterclockwise modes\nexist at both low and high resonant frequencies. This suggests that the\nmagnetic anisotropy plays an essential role in the spin dynamics. Furthermore,\nthe resulting spin excitations induce spin currents with exotic features under\nthe polarized microwaves. The spin currents induced, for instance, by low-lying\nin-plane excitations are strongly enhanced under the left-handed circularly\npolarized microwaves, but quenched by the right-handed circularly polarized\nmicrowaves regardless of the sign of the Dzyaloshinskii-Moriya interaction.\nThese results may pave the way for understanding the non-trivial interplay\nbetween magnetic anisotropy and polarized microwaves in the generation of spin\ncurrents by a resonant process.", "AI": {"tldr": "\u7814\u7a76\u4e86\u65af\u683c\u660e\u5b50\u57fa\u8d28\u6750\u6599\u7684\u81ea\u65cb\u6d41\u4ea7\u751f\uff0c\u53d1\u73b0\u78c1\u5404\u5411\u5f02\u6027\u5f71\u54cd\u81ea\u65cb\u6fc0\u53d1\uff0c\u5e76\u4e0e\u5706\u504f\u632f\u5fae\u6ce2\u5171\u540c\u4f5c\u7528\u4ea7\u751f\u81ea\u65cb\u6d41\u3002", "motivation": "\u7814\u7a76\u7f3a\u4e4f\u53cd\u8f6c\u5bf9\u79f0\u6027\u7684\u6750\u6599\u4e2d\u65af\u683c\u660e\u5b50\u9a71\u52a8\u7684\u81ea\u65cb\u6d41\u3002", "method": "\u901a\u8fc7\u5fae\u78c1\u6a21\u62df\u7814\u7a76\u4e86\u7f3a\u4e4f\u53cd\u6f14\u5bf9\u79f0\u6027\u7684\u65af\u683c\u660e\u5b50\u57fa\u8d28\u6750\u6599\u901a\u8fc7\u5fae\u6ce2\u5171\u632f\u8fc7\u7a0b\u6cf5\u6d66\u7684\u81ea\u65cb\u6d41\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u65af\u683c\u660e\u5b50\u76f8\uff08SkX I \u578b\u548c II \u578b\uff09\uff0c\u5b83\u4eec\u5177\u6709\u4e0d\u540c\u7684\u81ea\u65cb\u6fc0\u53d1\u7279\u6027\u3002SkX I \u578b\u5728\u4f4e\u78c1\u5404\u5411\u5f02\u6027\u4e0b\u8868\u73b0\u51fa\u547c\u5438\u6a21\u5f0f\u9891\u7387\u4ecb\u4e8e Bloch \u578b\u65af\u683c\u660e\u5b50\u7684\u987a\u65f6\u9488\u548c\u9006\u65f6\u9488\u56de\u8f6c\u6a21\u5f0f\u4e4b\u95f4\uff0c\u800c\u5728 $K_z \\sim 0.04$ meV \u65f6\u4e0e\u9006\u65f6\u9488\u6a21\u5f0f\u4ea4\u53c9\u3002SkX II \u578b\u5219\u8868\u73b0\u51fa\u72ec\u7279\u7684\u81ea\u65cb\u6fc0\u53d1\uff0c\u987a\u65f6\u9488\u6a21\u5f0f\u7f3a\u5931\uff0c\u800c\u9006\u65f6\u9488\u6a21\u5f0f\u5728\u4f4e\u9891\u548c\u9ad8\u9891\u4e0b\u90fd\u5b58\u5728\u3002\u8fd9\u4e24\u79cd\u7c7b\u578b\u7684\u65af\u683c\u660e\u5b50\u5728\u5706\u504f\u632f\u5fae\u6ce2\u4e0b\u4f1a\u4ea7\u751f\u5177\u6709\u5947\u5f02\u7279\u5f81\u7684\u81ea\u65cb\u6d41\uff0c\u4f8b\u5982\u5728\u5de6\u65cb\u5706\u504f\u632f\u5fae\u6ce2\u4e0b\u589e\u5f3a\uff0c\u800c\u5728\u53f3\u65cb\u5706\u504f\u632f\u5fae\u6ce2\u4e0b\u6dec\u706d\uff0c\u4e14\u4e0e DMI \u7b26\u53f7\u65e0\u5173\u3002", "conclusion": "\u78c1\u5404\u5411\u5f02\u6027\u5728\u81ea\u65cb\u6d41\u4ea7\u751f\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u5e76\u4e14\u5bf9\u5706\u504f\u632f\u5fae\u6ce2\u7684\u54cd\u5e94\u4e0d\u540c\uff0c\u8fd9\u8868\u660e\u4e86\u78c1\u5404\u5411\u5f02\u6027\u4e0e\u5706\u504f\u632f\u5fae\u6ce2\u4e4b\u95f4\u5b58\u5728\u975e\u5e73\u5eb8\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2507.09134", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09134", "abs": "https://arxiv.org/abs/2507.09134", "authors": ["Shu Zhang", "James Y. Z. Liu", "Dominic Liao-McPherson"], "title": "Integrating Planning and Predictive Control Using the Path Feasibility Governor", "comment": "14 pages, 7 figures, submitted to IEEE Transactions on Automatic\n  Control", "summary": "The motion planning problem of generating dynamically feasible,\ncollision-free trajectories in non-convex environments is a fundamental\nchallenge for autonomous systems. Decomposing the problem into path planning\nand path tracking improves tractability, but integrating these components in a\ntheoretically sound and computationally efficient manner is challenging. We\npropose the Path Feasibility Governor (PathFG), a framework for integrating\npath planners with nonlinear Model Predictive Control (MPC). The PathFG\nmanipulates the reference passed to the MPC controller, guiding it along a path\nwhile ensuring constraint satisfaction, stability, and recursive feasibility.\nThe PathFG is modular, compatible with replanning, and improves computational\nefficiency and reliability by reducing the need for long prediction horizons.\nWe prove safety and asymptotic stability with a significantly expanded region\nof attraction, and validate its real-time performance through a simulated case\nstudy of quadrotor navigation in a cluttered environment.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8def\u5f84\u53ef\u884c\u6027\u63a7\u5236\u5668\uff08PathFG\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u8def\u5f84\u89c4\u5212\u4e0e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u4ee5\u89e3\u51b3\u81ea\u4e3b\u7cfb\u7edf\u7684\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u64cd\u7eb5MPC\u7684\u53c2\u8003\u4fe1\u53f7\uff0c\u5728\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\u3001\u7a33\u5b9a\u6027\u548c\u9012\u5f52\u53ef\u884c\u6027\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b89\u5168\u6027\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u975e\u51f8\u73af\u5883\u4e2d\u751f\u6210\u6ee1\u8db3\u52a8\u529b\u5b66\u8981\u6c42\u4e14\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\u662f\u81ea\u4e3b\u7cfb\u7edf\u7684\u57fa\u672c\u8fd0\u52a8\u89c4\u5212\u95ee\u9898\u3002\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u8def\u5f84\u89c4\u5212\u548c\u8def\u5f84\u8ddf\u8e2a\u53ef\u4ee5\u63d0\u9ad8\u5176\u53ef\u5904\u7406\u6027\uff0c\u4f46\u9700\u8981\u5728\u7406\u8bba\u4e0a\u53ef\u9760\u4e14\u8ba1\u7b97\u9ad8\u6548\u5730\u6574\u5408\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8def\u5f84\u53ef\u884c\u6027\u63a7\u5236\u5668\uff08PathFG\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u8def\u5f84\u89c4\u5212\u5668\u4e0e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u3002PathFG\u901a\u8fc7\u64cd\u7eb5\u4f20\u9012\u7ed9MPC\u63a7\u5236\u5668\u7684\u53c2\u8003\u4fe1\u53f7\uff0c\u5f15\u5bfc\u5176\u6cbf\u8def\u5f84\u884c\u8fdb\uff0c\u540c\u65f6\u786e\u4fdd\u7ea6\u675f\u6ee1\u8db3\u3001\u7a33\u5b9a\u6027\u548c\u9012\u5f52\u53ef\u884c\u6027\u3002", "result": "\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5177\u6709\u5b89\u5168\u6027\u3001\u6e10\u8fd1\u7a33\u5b9a\u6027\u548c\u663e\u8457\u6269\u5927\u7684\u5438\u5f15\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u4fdd\u8bc1\u7ea6\u675f\u6ee1\u8db3\u3001\u7a33\u5b9a\u6027\u548c\u9012\u5f52\u53ef\u884c\u6027\u7684\u540c\u65f6\uff0c\u5c06\u8def\u5f84\u89c4\u5212\u4e0e\u975e\u7ebf\u6027\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u76f8\u7ed3\u5408\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u7684\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5728\u6742\u4e71\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u5176\u5b9e\u65f6\u6027\u80fd\u3002"}}
{"id": "2507.08903", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08903", "abs": "https://arxiv.org/abs/2507.08903", "authors": ["Zhongzhang Chen", "Miao Fan", "Shengtong Xu", "Mengmeng Yang", "Kun Jiang", "Xiangzeng Liu", "Haoyi Xiong"], "title": "Multimodal HD Mapping for Intersections by Intelligent Roadside Units", "comment": "Accepted by ITSC'25", "summary": "High-definition (HD) semantic mapping of complex intersections poses\nsignificant challenges for traditional vehicle-based approaches due to\nocclusions and limited perspectives. This paper introduces a novel camera-LiDAR\nfusion framework that leverages elevated intelligent roadside units (IRUs).\nAdditionally, we present RS-seq, a comprehensive dataset developed through the\nsystematic enhancement and annotation of the V2X-Seq dataset. RS-seq includes\nprecisely labelled camera imagery and LiDAR point clouds collected from\nroadside installations, along with vectorized maps for seven intersections\nannotated with detailed features such as lane dividers, pedestrian crossings,\nand stop lines. This dataset facilitates the systematic investigation of\ncross-modal complementarity for HD map generation using IRU data. The proposed\nfusion framework employs a two-stage process that integrates modality-specific\nfeature extraction and cross-modal semantic integration, capitalizing on camera\nhigh-resolution texture and precise geometric data from LiDAR. Quantitative\nevaluations using the RS-seq dataset demonstrate that our multimodal approach\nconsistently surpasses unimodal methods. Specifically, compared to unimodal\nbaselines evaluated on the RS-seq dataset, the multimodal approach improves the\nmean Intersection-over-Union (mIoU) for semantic segmentation by 4\\% over the\nimage-only results and 18\\% over the point cloud-only results. This study\nestablishes a baseline methodology for IRU-based HD semantic mapping and\nprovides a valuable dataset for future research in infrastructure-assisted\nautonomous driving systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u667a\u80fd\u9053\u8def\u5355\u5143\uff08IRU\uff09\u548c\u76f8\u673a-\u6fc0\u5149\u96f7\u8fbe\u878d\u5408\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\uff08HD\uff09\u8bed\u4e49\u5efa\u56fe\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\uff08RS-seq\uff09\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u5355\u4e00\u4f20\u611f\u5668\u7684\u65b9\u6cd5\u6548\u679c\u66f4\u597d\uff0c\u7279\u522b\u662f\u5728\u63d0\u9ad8\u8bed\u4e49\u5206\u5272\u7cbe\u5ea6\u65b9\u9762\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u8f66\u8f86\u7684\u65b9\u6cd5\u5728\u590d\u6742\u4ea4\u53c9\u8def\u53e3\u8fdb\u884c\u9ad8\u7cbe\u5ea6\uff08HD\uff09\u8bed\u4e49\u5efa\u56fe\u65f6\uff0c\u7531\u4e8e\u906e\u6321\u548c\u6709\u9650\u89c6\u89d2\u800c\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f8\u673a-\u6fc0\u5149\u96f7\u8fbe\u878d\u5408\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e86\u667a\u80fd\u9053\u8def\u5355\u5143\uff08IRU\uff09\uff0c\u5e76\u901a\u8fc7\u4e24\u9636\u6bb5\u8fc7\u7a0b\u96c6\u6210\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u7279\u5f81\u63d0\u53d6\u548c\u8de8\u6a21\u6001\u8bed\u4e49\u96c6\u6210\uff0c\u7ed3\u5408\u4e86\u76f8\u673a\u7684\u4e30\u5bcc\u7eb9\u7406\u548c\u6fc0\u5149\u96f7\u8fbe\u7684\u7cbe\u786e\u51e0\u4f55\u6570\u636e\u3002", "result": "\u5728RS-seq\u6570\u636e\u96c6\u4e0a\u7684\u91cf\u5316\u8bc4\u4f30\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u65b9\u6cd5\u5728\u8bed\u4e49\u5206\u5272\u7684\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u65b9\u9762\uff0c\u6bd4\u4ec5\u4f7f\u7528\u56fe\u50cf\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e864%\uff0c\u6bd4\u4ec5\u4f7f\u7528\u70b9\u4e91\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8618%\uff0c\u4e00\u81f4\u4f18\u4e8e\u5355\u6a21\u6001\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u57fa\u4e8e\u667a\u80fd\u9053\u8def\u5355\u5143\uff08IRU\uff09\u7684\u9ad8\u7cbe\u5ea6\uff08HD\uff09\u8bed\u4e49\u5efa\u56fe\u5efa\u7acb\u4e86\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9d\u8d35\u7684RS-seq\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u672a\u6765\u57fa\u7840\u8bbe\u65bd\u8f85\u52a9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u7814\u7a76\u3002"}}
{"id": "2507.10262", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2507.10262", "abs": "https://arxiv.org/abs/2507.10262", "authors": ["Dahee Kim", "Song Kim", "Jeongseon Kim", "Junghoon Kim", "Kaiyu Feng", "Sungsu Lim", "Jungeun Kim"], "title": "Experimental Analysis and Evaluation of Cohesive Subgraph Discovery", "comment": "17 pages, 26 figures", "summary": "Retrieving cohesive subgraphs in networks is a fundamental problem in social\nnetwork analysis and graph data management. These subgraphs can be used for\nmarketing strategies or recommendation systems. Despite the introduction of\nnumerous models over the years, a systematic comparison of their performance,\nespecially across varied network configurations, remains unexplored. In this\nstudy, we evaluated various cohesive subgraph models using task-based\nevaluations and conducted extensive experimental studies on both synthetic and\nreal-world networks. Thus, we unveil the characteristics of cohesive subgraph\nmodels, highlighting their efficiency and applicability. Our findings not only\nprovide a detailed evaluation of current models but also lay the groundwork for\nfuture research by shedding light on the balance between the interpretability\nand cohesion of the subgraphs. This research guides the selection of suitable\nmodels for specific analytical needs and applications, providing valuable\ninsights.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.08981", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08981", "abs": "https://arxiv.org/abs/2507.08981", "authors": ["Hanbyel Cho", "Jaesung Ahn", "Yooshin Cho", "Junmo Kim"], "title": "Video Inference for Human Mesh Recovery with Vision Transformer", "comment": "Accepted to IEEE FG 2023", "summary": "Human Mesh Recovery (HMR) from an image is a challenging problem because of\nthe inherent ambiguity of the task. Existing HMR methods utilized either\ntemporal information or kinematic relationships to achieve higher accuracy, but\nthere is no method using both. Hence, we propose \"Video Inference for Human\nMesh Recovery with Vision Transformer (HMR-ViT)\" that can take into account\nboth temporal and kinematic information. In HMR-ViT, a Temporal-kinematic\nFeature Image is constructed using feature vectors obtained from video frames\nby an image encoder. When generating the feature image, we use a Channel\nRearranging Matrix (CRM) so that similar kinematic features could be located\nspatially close together. The feature image is then further encoded using\nVision Transformer, and the SMPL pose and shape parameters are finally inferred\nusing a regression network. Extensive evaluation on the 3DPW and Human3.6M\ndatasets indicates that our method achieves a competitive performance in HMR.", "AI": {"tldr": "A new method called HMR-ViT uses both temporal and kinematic information from videos to recover human mesh shape more accurately. It uses a special feature image and a Vision Transformer, achieving good results on standard datasets.", "motivation": "Existing Human Mesh Recovery (HMR) methods have not simultaneously utilized temporal and kinematic information, despite its potential to improve accuracy. This paper addresses this gap.", "method": "HMR-ViT uses a Temporal-kinematic Feature Image, constructed with feature vectors from video frames via an image encoder and a Channel Rearranging Matrix (CRM) to spatially align similar kinematic features. This feature image is then encoded using a Vision Transformer, and SMPL pose and shape parameters are inferred using a regression network.", "result": "Extensive evaluations on the 3DPW and Human3.6M datasets demonstrate that HMR-ViT achieves competitive performance in Human Mesh Recovery.", "conclusion": "Our proposed HMR-ViT method achieves competitive performance in Human Mesh Recovery by effectively utilizing both temporal and kinematic information."}}
{"id": "2507.10026", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10026", "abs": "https://arxiv.org/abs/2507.10026", "authors": ["Zhifei Xu", "Zhiqing Tang", "Jiong Lou", "Zhi Yao", "Xuan Xie", "Tian Wang", "Yinglong Wang", "Weijia Jia"], "title": "EAT: QoS-Aware Edge-Collaborative AIGC Task Scheduling via Attention-Guided Diffusion Reinforcement Learning", "comment": null, "summary": "The growth of Artificial Intelligence (AI) and large language models has\nenabled the use of Generative AI (GenAI) in cloud data centers for diverse\nAI-Generated Content (AIGC) tasks. Models like Stable Diffusion introduce\nunavoidable delays and substantial resource overhead, which are unsuitable for\nusers at the network edge with high QoS demands. Deploying AIGC services on\nedge servers reduces transmission times but often leads to underutilized\nresources and fails to optimally balance inference latency and quality. To\naddress these issues, this paper introduces a QoS-aware\n\\underline{E}dge-collaborative \\underline{A}IGC \\underline{T}ask scheduling\n(EAT) algorithm. Specifically: 1) We segment AIGC tasks and schedule patches to\nvarious edge servers, formulating it as a gang scheduling problem that balances\ninference latency and quality while considering server heterogeneity, such as\ndiffering model distributions and cold start issues. 2) We propose a\nreinforcement learning-based EAT algorithm that uses an attention layer to\nextract load and task queue information from edge servers and employs a\ndiffusion-based policy network for scheduling, efficiently enabling model\nreuse. 3) We develop an AIGC task scheduling system that uses our EAT algorithm\nto divide tasks and distribute them across multiple edge servers for\nprocessing. Experimental results based on our system and large-scale\nsimulations show that our EAT algorithm can reduce inference latency by up to\n56\\% compared to baselines. We release our open-source code at\nhttps://github.com/zzf1955/EAT.", "AI": {"tldr": "EAT\u7b97\u6cd5\u901a\u8fc7QoS\u611f\u77e5\u548c\u8fb9\u7f18\u534f\u540c\u8c03\u5ea6\uff0c\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316AIGC\u4efb\u52a1\u5728\u5f02\u6784\u8fb9\u7f18\u670d\u52a1\u5668\u4e0a\u7684\u5904\u7406\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "motivation": "AI\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u589e\u957f\u4f7f\u5f97\u751f\u6210\u5f0fAI\uff08GenAI\uff09\u5728\u4e91\u6570\u636e\u4e2d\u5fc3\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u7136\u800c\u50cfStable Diffusion\u8fd9\u7c7b\u6a21\u578b\u4f1a\u5f15\u5165\u4e0d\u53ef\u907f\u514d\u7684\u5ef6\u8fdf\u548c\u5927\u91cf\u7684\u8d44\u6e90\u5f00\u9500\uff0c\u8fd9\u5bf9\u4e8e\u7f51\u7edc\u8fb9\u7f18\u6709\u9ad8\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u8981\u6c42\u7684\u7528\u6237\u5e76\u4e0d\u9002\u7528\u3002\u5c06AIGC\u670d\u52a1\u90e8\u7f72\u5728\u8fb9\u7f18\u670d\u52a1\u5668\u53ef\u4ee5\u51cf\u5c11\u4f20\u8f93\u65f6\u95f4\uff0c\u4f46\u5e38\u5e38\u5bfc\u81f4\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\uff0c\u5e76\u4e14\u65e0\u6cd5\u6700\u4f18\u5730\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u548c\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEAT\uff08Edge-collaborative AIGC Task scheduling\uff09\u7684QoS\u611f\u77e5\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c06AIGC\u4efb\u52a1\u5206\u5757\u5e76\u5728\u5f02\u6784\u8fb9\u7f18\u670d\u52a1\u5668\u4e4b\u95f4\u8fdb\u884c\u8c03\u5ea6\uff0c\u4ee5\u5e73\u8861\u63a8\u7406\u5ef6\u8fdf\u548c\u8d28\u91cf\u3002\u8be5\u7b97\u6cd5\u91c7\u7528\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u6ce8\u610f\u529b\u5c42\u63d0\u53d6\u8fb9\u7f18\u670d\u52a1\u5668\u7684\u8d1f\u8f7d\u548c\u4efb\u52a1\u961f\u5217\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u6269\u6563\u7684\u7b56\u7565\u7f51\u7edc\u8fdb\u884c\u8c03\u5ea6\u4ee5\u5b9e\u73b0\u6a21\u578b\u91cd\u7528\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2aAIGC\u4efb\u52a1\u8c03\u5ea6\u7cfb\u7edf\u6765\u5b9e\u73b0\u8be5\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u548c\u5927\u89c4\u6a21\u6a21\u62df\u8868\u660e\uff0cEAT\u7b97\u6cd5\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe56%\u3002", "conclusion": "EAT\u7b97\u6cd5\u901a\u8fc7\u5c06AIGC\u4efb\u52a1\u5206\u5757\u5e76\u8c03\u5ea6\u5230\u591a\u4e2a\u8fb9\u7f18\u670d\u52a1\u5668\uff0c\u5e76\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u6ce8\u610f\u529b\u673a\u5236\u6765\u4f18\u5316\u8c03\u5ea6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8fb9\u7f18AIGC\u670d\u52a1\u4e2d\u7684\u5ef6\u8fdf\u548c\u8d44\u6e90\u5229\u7528\u7387\u95ee\u9898\uff0c\u76f8\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u53ef\u5c06\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u9ad8\u8fbe56%\u3002"}}
{"id": "2507.09056", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09056", "abs": "https://arxiv.org/abs/2507.09056", "authors": ["Antu Laha", "Sarah Paone", "Niraj Aryal", "Qiang Li"], "title": "Large non-saturating Nernst thermopower and magnetoresistance in compensated semimetal ScSb", "comment": "Accepted for publication in Materials Today Physics", "summary": "Today, high-performance thermoelectric and thermomagnetic materials operating\nin the low-temperature regime, particularly below the boiling point of liquid\nnitrogen remain scarce. Most thermomagnetic materials reported to date exhibit\na strong Nernst signal along specific crystallographic directions in their\nsingle-crystal form. However, their performance typically degrades\nsignificantly in the polycrystalline form. Here, we report an improved Nernst\nthermopower of $\\sim$ 128 $\\mu$V/K at 30 K and 14 T in polycrystalline\ncompensated semimetal ScSb, in comparison to that was observed in single\ncrystal ScSb previously. The magnetic field dependence of Nernst thermopower\nshows a linear and non-saturating behavior up to 14 T. The maximum Nernst power\nfactor reaches to $\\sim 240 \\times 10^{-4}$ W m$^{-1}$ K$^{-2}$ and Nernst\nfigure of merit reaches to $\\sim 11 \\times 10^{-4}$ K$^{-1}$. Polycrystalline\nScSb also shows a large non-saturating magnetoresistance of $\\sim 940 \\%$ at 2\nK and 14 T. These enhanced properties originate from better electron-hole\ncompensation, as revealed by Hall resistivity measurements. The cubic symmetry\nand absence of anisotropy in ScSb allow its polycrystalline form to achieve\nsimilar enhanced thermomagnetic and electromagnetic performance comparable to\nthat of the single crystal.", "AI": {"tldr": "\u5728\u4f4e\u6e29\u4e0b\uff0c\u591a\u6676\u6001SbSc\u5728Nernst\u70ed\u7535\u52bf\u548c\u78c1\u7535\u963b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6027\u80fd\u4f18\u4e8e\u5355\u6676\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5176\u826f\u597d\u7684\u7535\u5b50-\u7a7a\u7a74\u8865\u507f\u548c\u7acb\u65b9\u5bf9\u79f0\u6027\u3002", "motivation": "\u5bfb\u627e\u5728\u4f4e\u6e29\uff08\u5c24\u5176\u662f\u6db2\u6c2e\u6cb8\u70b9\u4ee5\u4e0b\uff09\u4e0b\u5de5\u4f5c\u7684\u9ad8\u6027\u80fd\u70ed\u7535\u548c\u70ed\u78c1\u6750\u6599\uff0c\u5e76\u7814\u7a76\u591a\u6676\u6750\u6599\u7684\u6027\u80fd\u8868\u73b0\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6d4b\u91cf\u4e86\u591a\u6676\u6001SbSc\u5728\u4f4e\u6e29\uff0830 K\u548c2 K\uff09\u548c\u5f3a\u78c1\u573a\uff0814 T\uff09\u4e0b\u7684Nernst\u70ed\u7535\u52bf\u3001\u78c1\u7535\u963b\u548c\u970d\u5c14\u7535\u963b\uff0c\u5e76\u4e0e\u5355\u6676SbSc\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u572830 K\u548c14 T\u4e0b\uff0c\u591a\u6676\u6001SbSc\u7684Nernst\u70ed\u7535\u52bf\u8fbe\u5230\u4e86\u7ea6128 \u03bcV/K\uff0c\u4f18\u4e8e\u5355\u6676SbSc\u3002\u5176Nernst\u529f\u7387\u56e0\u5b50\u6700\u9ad8\u8fbe\u5230\u7ea6240 \u00d7 10\u207b\u2074 W m\u207b\u00b9 K\u207b\u00b2\uff0cNernst\u54c1\u8d28\u56e0\u6570\u8fbe\u5230\u7ea611 \u00d7 10\u207b\u2074 K\u207b\u00b9\u3002\u57282 K\u548c14 T\u4e0b\uff0c\u591a\u6676\u6001SbSc\u8fd8\u8868\u73b0\u51fa\u9ad8\u8fbe\u7ea6940%\u7684\u975e\u9971\u548c\u78c1\u7535\u963b\u3002\u8fd9\u4e9b\u6027\u80fd\u7684\u63d0\u5347\u5f52\u56e0\u4e8e\u7535\u5b50-\u7a7a\u7a74\u8865\u507f\u7684\u6539\u5584\u3002", "conclusion": "\u591a\u6676\u6001SbSc\u5728\u4f4e\u6e29\u4e0b\u7684\u70ed\u7535\u548c\u70ed\u78c1\u6027\u80fd\u5f97\u5230\u4e86\u6539\u5584\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u7535\u5b50-\u7a7a\u7a74\u8865\u507f\u7684\u63d0\u9ad8\u4ee5\u53ca\u7acb\u65b9\u5bf9\u79f0\u6027\u548c\u5404\u5411\u5f02\u6027\u7684\u7f3a\u5931\uff0c\u4f7f\u5f97\u591a\u6676\u6001SbSc\u5728\u70ed\u7535\u548c\u70ed\u78c1\u4ee5\u53ca\u7535\u78c1\u6027\u80fd\u4e0a\u53ef\u4e0e\u5355\u6676\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2507.10005", "categories": ["cs.LG", "cond-mat.stat-mech", "cs.NE", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.10005", "abs": "https://arxiv.org/abs/2507.10005", "authors": ["Yash Arya", "Sang Hoon Lee"], "title": "Effects of structural properties of neural networks on machine learning performance", "comment": "9 pages, 6 figures", "summary": "In recent years, graph-based machine learning techniques, such as\nreinforcement learning and graph neural networks, have garnered significant\nattention. While some recent studies have started to explore the relationship\nbetween the graph structure of neural networks and their predictive\nperformance, they often limit themselves to a narrow range of model networks,\nparticularly lacking mesoscale structures such as communities. Our work\nadvances this area by conducting a more comprehensive investigation,\nincorporating realistic network structures characterized by heterogeneous\ndegree distributions and community structures, which are typical\ncharacteristics of many real networks. These community structures offer a\nnuanced perspective on network architecture. Our analysis employs model\nnetworks such as random and scale-free networks, alongside a comparison with a\nbiological neural network and its subsets for more detailed analysis. We\nexamine the impact of these structural attributes on the performance of image\nclassification tasks. Our findings reveal that structural properties do affect\nperformance to some extent. Specifically, networks featuring coherent, densely\ninterconnected communities demonstrate enhanced learning capabilities. The\ncomparison with the biological neural network emphasizes the relevance of our\nfindings to real-world structures, suggesting an intriguing connection worth\nfurther exploration. This study contributes meaningfully to network science and\nmachine learning, providing insights that could inspire the design of more\nbiologically informed neural networks.", "AI": {"tldr": "\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u7ed3\u6784\u4f1a\u5f71\u54cd\u5176\u6027\u80fd\uff0c\u5177\u6709\u5bc6\u96c6\u793e\u533a\u7ed3\u6784\u7684\u7f51\u7edc\u7684\u6027\u80fd\u66f4\u597d\u3002", "motivation": "\u8fd1\u5e74\u6765\uff0c\u57fa\u4e8e\u56fe\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff0c\u5982\u5f3a\u5316\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u53d7\u5230\u4e86\u5e7f\u6cdb\u5173\u6ce8\u3002\u5c3d\u7ba1\u4e00\u4e9b\u8fd1\u671f\u7814\u7a76\u5f00\u59cb\u63a2\u7d22\u795e\u7ecf\u7f51\u7edc\u7684\u56fe\u7ed3\u6784\u4e0e\u5176\u9884\u6d4b\u6027\u80fd\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5c06\u81ea\u5df1\u9650\u5236\u5728\u6a21\u578b\u7f51\u7edc\u7684\u8f83\u7a84\u8303\u56f4\u5185\uff0c\u7279\u522b\u662f\u7f3a\u4e4f\u793e\u533a\u7b49\u4e2d\u89c2\u7ed3\u6784\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u5177\u6709\u5f02\u8d28\u6027\u5ea6\u5206\u5e03\u548c\u793e\u533a\u7ed3\u6784\u7684\u771f\u5b9e\u7f51\u7edc\u7ed3\u6784\u8fdb\u884c\u66f4\u5168\u9762\u7684\u8c03\u67e5\uff0c\u5e76\u91c7\u7528\u968f\u673a\u7f51\u7edc\u548c\u65e0\u6807\u5ea6\u7f51\u7edc\u7b49\u6a21\u578b\u7f51\u7edc\uff0c\u4ee5\u53ca\u4e0e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u53ca\u5176\u5b50\u96c6\u8fdb\u884c\u6bd4\u8f83\u6765\u8fdb\u884c\u8be6\u7ec6\u5206\u6790\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u8fd9\u4e00\u9886\u57df\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u7ed3\u6784\u5c5e\u6027\u786e\u5b9e\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u5f71\u54cd\u4e86\u6027\u80fd\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5177\u6709\u8fde\u8d2f\u7684\u3001\u5bc6\u96c6\u4e92\u8054\u7684\u793e\u533a\u7684\u7f51\u7edc\u7684\u5b66\u4e60\u80fd\u529b\u5f97\u5230\u4e86\u589e\u5f3a\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5177\u6709\u8fde\u8d2f\u7684\u3001\u5bc6\u96c6\u4e92\u8054\u7684\u793e\u533a\u7684\u7f51\u7edc\u7684\u5b66\u4e60\u80fd\u529b\u5f97\u5230\u4e86\u589e\u5f3a\u3002\u5c06\u8fd9\u4e9b\u53d1\u73b0\u4e0e\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u7684\u6bd4\u8f83\u5f3a\u8c03\u4e86\u5176\u4e0e\u771f\u5b9e\u4e16\u754c\u7ed3\u6784\u7684\u76f8\u5173\u6027\uff0c\u5e76\u6697\u793a\u4e86\u503c\u5f97\u8fdb\u4e00\u6b65\u63a2\u7d22\u7684\u6709\u8da3\u8054\u7cfb\u3002\u672c\u7814\u7a76\u4e3a\u7f51\u7edc\u79d1\u5b66\u548c\u673a\u5668\u5b66\u4e60\u505a\u51fa\u4e86\u6709\u610f\u4e49\u7684\u8d21\u732e\uff0c\u5176\u89c1\u89e3\u53ef\u4ee5\u6fc0\u53d1\u8bbe\u8ba1\u66f4\u5177\u751f\u7269\u5b66\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u3002"}}
{"id": "2507.08939", "categories": ["quant-ph", "cond-mat.str-el", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.08939", "abs": "https://arxiv.org/abs/2507.08939", "authors": ["Ammar Ali", "Joe Gibbs", "Keerthi Kumaran", "Varadharajan Muruganandam", "Bo Xiao", "Paul Kairys", "G\u00e1bor Hal\u00e1sz", "Arnab Banerjee", "Phillip C. Lotshaw"], "title": "Robust Chiral Edge Dynamics of a Kitaev Honeycomb on a Trapped Ion Processor", "comment": "7+9 pages, 4+6 figures", "summary": "Kitaev's honeycomb model is a paradigmatic exactly solvable system hosting a\nquantum spin liquid with non-Abelian anyons and topologically protected edge\nmodes, offering a platform for fault-tolerant quantum computation. However,\nreal candidate Kitaev materials invariably include complex secondary\ninteractions that obscure the realization of spin-liquid behavior and demand\nnovel quantum computational approaches for efficient simulation. Here we report\nquantum simulations of a 22-site Kitaev honeycomb lattice on a trapped-ion\nquantum processor, without and with non-integrable Heisenberg interactions that\nare present in real materials. We develop efficient quantum circuits for\nground-state preparation, then apply controlled perturbations and measure\ntime-dependent spin correlations along the system's edge. In the non-Abelian\nphase, we observe chiral edge dynamics consistent with a nonzero Chern number\n-- a hallmark of topological order -- which vanishes upon transition to the\nAbelian toric code phase. Extending to the non-integrable Kitaev-Heisenberg\nmodel, we find that weak Heisenberg interactions preserve chiral edge dynamics,\nwhile stronger couplings suppress them, signaling the breakdown of topological\nprotection. Our work demonstrates a viable route for probing dynamical\nsignatures of topological order in quantum spin liquids using programmable\nquantum hardware, opening new pathways for quantum simulation of strongly\ncorrelated materials.", "AI": {"tldr": "\u5728\u6355\u83b7\u79bb\u5b50\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u6a21\u62dfKitaev\u8702\u7a9d\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u62d3\u6251\u5e8f\u7684\u52a8\u529b\u5b66\u4fe1\u53f7\uff0c\u5e76\u7814\u7a76\u4e86Heisenberg\u76f8\u4e92\u4f5c\u7528\u5bf9\u62d3\u6251\u4fdd\u62a4\u7684\u5f71\u54cd\u3002", "motivation": "Kitaev\u8702\u7a9d\u6a21\u578b\u662f\u5177\u6709\u975e\u963f\u8d1d\u5c14\u4efb\u610f\u5b50\u548c\u62d3\u6251\u4fdd\u62a4\u8fb9\u7f18\u6a21\u5f0f\u7684\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u7684\u8303\u4f8b\u6027\u7cbe\u786e\u53ef\u89e3\u7cfb\u7edf\uff0c\u4f46\u5b9e\u9645\u6750\u6599\u4e2d\u7684\u4e8c\u6b21\u76f8\u4e92\u4f5c\u7528\u4f1a\u963b\u788d\u81ea\u65cb\u6db2\u4f53\u884c\u4e3a\u7684\u5b9e\u73b0\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u91cf\u5b50\u8ba1\u7b97\u65b9\u6cd5\u8fdb\u884c\u6709\u6548\u6a21\u62df\u3002", "method": "\u5728\u6355\u83b7\u79bb\u5b50\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u5bf9\u5305\u542b22\u4e2a\u683c\u5b50\u7684Kitaev\u8702\u7a9d\u6a21\u578b\u8fdb\u884c\u91cf\u5b50\u6a21\u62df\uff0c\u5206\u522b\u6a21\u62df\u4e86\u65e0\u548c\u6709Heisenberg\u76f8\u4e92\u4f5c\u7528\u7684\u60c5\u51b5\u3002\u5f00\u53d1\u4e86\u5236\u5907\u57fa\u6001\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u901a\u8fc7\u65bd\u52a0\u53d7\u63a7\u6270\u52a8\u548c\u6d4b\u91cf\u8fb9\u7f18\u7684\u65f6\u95f4\u4f9d\u8d56\u81ea\u65cb\u5173\u8054\u6765\u7814\u7a76\u7cfb\u7edf\u3002", "result": "\u5728\u975e\u963f\u8d1d\u5c14\u76f8\u4e2d\uff0c\u89c2\u5bdf\u5230\u4e86\u4e0e\u975e\u96f6\u9648\u6570\u4e00\u81f4\u7684\u624b\u5f81\u8fb9\u7f18\u52a8\u529b\u5b66\uff0c\u8fd9\u662f\u62d3\u6251\u5e8f\u7684\u4e00\u4e2a\u6807\u5fd7\uff0c\u5728\u8fc7\u6e21\u5230\u963f\u8d1d\u5c14\u66f2\u7801\u76f8\u65f6\u6d88\u5931\u3002\u5728\u975e\u53ef\u79efKitaev-Heisenberg\u6a21\u578b\u4e2d\uff0c\u5f31Heisenberg\u76f8\u4e92\u4f5c\u7528\u4fdd\u7559\u4e86\u624b\u5f81\u8fb9\u7f18\u52a8\u529b\u5b66\uff0c\u800c\u8f83\u5f3a\u7684\u8026\u5408\u5219\u6291\u5236\u4e86\u5b83\u4eec\uff0c\u8868\u660e\u62d3\u6251\u4fdd\u62a4\u88ab\u7834\u574f\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86\u5229\u7528\u53ef\u7f16\u7a0b\u91cf\u5b50\u786c\u4ef6\u63a2\u6d4b\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u4e2d\u62d3\u6251\u5e8f\u52a8\u529b\u5b66\u4fe1\u53f7\u7684\u53ef\u884c\u9014\u5f84\uff0c\u4e3a\u5f3a\u5173\u8054\u6750\u6599\u7684\u91cf\u5b50\u6a21\u62df\u5f00\u8f9f\u4e86\u65b0\u9053\u8def\u3002"}}
{"id": "2507.08924", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08924", "abs": "https://arxiv.org/abs/2507.08924", "authors": ["Seokhee Hong", "Sunkyoung Kim", "Guijin Son", "Soyeon Kim", "Yeonjung Hong", "Jinsik Lee"], "title": "From KMMLU-Redux to KMMLU-Pro: A Professional Korean Benchmark Suite for LLM Evaluation", "comment": null, "summary": "The development of Large Language Models (LLMs) requires robust benchmarks\nthat encompass not only academic domains but also industrial fields to\neffectively evaluate their applicability in real-world scenarios. In this\npaper, we introduce two Korean expert-level benchmarks. KMMLU-Redux,\nreconstructed from the existing KMMLU, consists of questions from the Korean\nNational Technical Qualification exams, with critical errors removed to enhance\nreliability. KMMLU-Pro is based on Korean National Professional Licensure exams\nto reflect professional knowledge in Korea. Our experiments demonstrate that\nthese benchmarks comprehensively represent industrial knowledge in Korea. We\nrelease our dataset publicly available.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e24\u4e2a\u65b0\u7684\u97e9\u56fdLLM\u57fa\u51c6\uff08KMMLU-Redux\u548cKMMLU-Pro\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u5b9e\u9645\u5de5\u4e1a\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\uff0c\u9700\u8981\u5305\u542b\u5b66\u672f\u548c\u5de5\u4e1a\u9886\u57df\u7684\u9c81\u68d2\u57fa\u51c6\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e24\u4e2a\u65b0\u7684\u97e9\u56fd\u4e13\u5bb6\u7ea7\u57fa\u51c6\uff1aKMMLU-Redux\uff08\u4eceKMMLU\u91cd\u5efa\uff0c\u5254\u9664\u4e86\u5173\u952e\u9519\u8bef\uff09\u548cKMMLU-Pro\uff08\u57fa\u4e8e\u97e9\u56fd\u56fd\u5bb6\u4e13\u4e1a\u6267\u7167\u8003\u8bd5\uff09\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8fd9\u4e24\u4e2a\u57fa\u51c6\u80fd\u591f\u5168\u9762\u4ee3\u8868\u97e9\u56fd\u7684\u5de5\u4e1a\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u57fa\u51c6\u5168\u9762\u4ee3\u8868\u4e86\u97e9\u56fd\u7684\u5de5\u4e1a\u77e5\u8bc6\uff0c\u5e76\u4e14\u662f\u516c\u5f00\u63d0\u4f9b\u7684\u3002"}}
{"id": "2507.09422", "categories": ["cs.GT", "math.NT", "91A06, 91A10"], "pdf": "https://arxiv.org/pdf/2507.09422", "abs": "https://arxiv.org/abs/2507.09422", "authors": ["Edan Orzech", "Martin Rinard"], "title": "Nash Equilibria with Irradical Probabilities", "comment": null, "summary": "We present for every $n\\ge4$ an $n$-player game in normal form with payoffs\nin $\\{0,1,2\\}$ that has a unique, fully mixed, Nash equilibrium in which all\nthe probability weights are irradical (i.e., algebraic but not closed form\nexpressible even with $m$-th roots for any integer $m$).", "AI": {"tldr": "n\u4eba\u535a\u5f08\u6a21\u578b\uff0c\u5177\u6709\u552f\u4e00\u7684\u3001\u5b8c\u5168\u6df7\u5408\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u5176\u6982\u7387\u6743\u91cd\u662f\u4e0d\u53ef\u7ea6\u7684\u3002", "motivation": "\u4e3a\u4e86\u627e\u5230\u5177\u6709\u4e0d\u53ef\u7ea6\u6982\u7387\u6743\u91cd\u7684\u552f\u4e00\u3001\u5b8c\u5168\u6df7\u5408\u7684\u7eb3\u4ec0\u5747\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdn\u4eba\u535a\u5f08\u6a21\u578b\uff0c\u5e76\u5206\u6790\u4e86\u5176\u7eb3\u4ec0\u5747\u8861\u7684\u6027\u8d28\u3002", "result": "\u627e\u5230\u4e86\u4e00\u4e2an\u4eba\u535a\u5f08\u6a21\u578b\uff0c\u5176\u7eb3\u4ec0\u5747\u8861\u7684\u6982\u7387\u6743\u91cd\u662f\u4e0d\u53ef\u7ea6\u7684\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684n\u4eba\u535a\u5f08\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u552f\u4e00\u7684\u3001\u5b8c\u5168\u6df7\u5408\u7684\u7eb3\u4ec0\u5747\u8861\uff0c\u5176\u6982\u7387\u6743\u91cd\u662f\u4e0d\u53ef\u7ea6\u7684\u3002"}}
{"id": "2507.09390", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09390", "abs": "https://arxiv.org/abs/2507.09390", "authors": ["Etienne Payet"], "title": "Non-Termination of Logic Programs Using Patterns", "comment": "25 pages, presented at the 41st International Conference on Logic\n  Programming, ICLP 2025", "summary": "In this paper, we consider an approach introduced in term rewriting for the\nautomatic detection of non-looping non-termination from patterns of rules. We\nadapt it to logic programming by defining a new unfolding technique that\nproduces patterns describing possibly infinite sets of finite rewrite\nsequences. We present an experimental evaluation of our contributions that we\nimplemented in our tool NTI.", "AI": {"tldr": "\u5c06\u9879\u91cd\u5199\u4e2d\u7684\u975e\u5faa\u73af\u975e\u7ec8\u6b62\u68c0\u6d4b\u65b9\u6cd5\u5e94\u7528\u4e8e\u903b\u8f91\u7f16\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aNTI\u7684\u5de5\u5177\u6765\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\u3002", "motivation": "\u5728\u9879\u91cd\u5199\u4e2d\u7528\u4e8e\u81ea\u52a8\u68c0\u6d4b\u975e\u5faa\u73af\u975e\u7ec8\u6b62\u7684\u6a21\u5f0f\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5", "method": "\u901a\u8fc7\u5b9a\u4e49\u65b0\u7684\u5c55\u5f00\u6280\u672f\u6765\u63cf\u8ff0\u53ef\u80fd\u6709\u9650\u91cd\u5199\u5e8f\u5217\u7684\u65e0\u9650\u96c6\u5408", "result": "\u5728\u6211\u4eec\u7684\u5de5\u5177NTI\u4e2d\u5b9e\u73b0\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30", "conclusion": "\u672c\u6587\u5c06\u9879\u91cd\u5199\u4e2d\u5f15\u5165\u7684\u7528\u4e8e\u4ece\u89c4\u5219\u6a21\u5f0f\u81ea\u52a8\u68c0\u6d4b\u975e\u5faa\u73af\u975e\u7ec8\u6b62\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u903b\u8f91\u7f16\u7a0b"}}
{"id": "2507.08999", "categories": ["eess.SP", "I.5.3; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.08999", "abs": "https://arxiv.org/abs/2507.08999", "authors": ["Duc Vu", "Selin Aviyente"], "title": "Hypergraph Overlapping Community Detection for Brain Networks", "comment": "6 Pages, Accepted for IEEE MLSP 2025", "summary": "Functional magnetic resonance imaging (fMRI) has been commonly used to\nconstruct functional connectivity networks (FCNs) of the human brain. TFCNs are\nprimarily limited to quantifying pairwise relationships between ROIs ignoring\nhigher order dependencies between multiple brain regions. Recently, hypergraph\nconstruction methods from fMRI time series data have been proposed to\ncharacterize the high-order relations among multiple ROIs. While there have\nbeen multiple methods for constructing hypergraphs from fMRI time series, the\nquestion of how to characterize the topology of these hypergraphs remains open.\nIn this paper, we make two key contributions to the field of community\ndetection in brain hypernetworks. First, we construct a hypergraph for each\nsubject capturing high order dependencies between regions. Second, we introduce\na spectral clustering based approach on hypergraphs to detect overlapping\ncommunity structure. Finally, the proposed method is implemented to detect the\nconsensus community structure across multiple subjects. The proposed method is\napplied to resting state fMRI data from Human Connectome Project to summarize\nthe overlapping community structure across a group of healthy young adults.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8d85\u56fe\u65b9\u6cd5\uff0c\u7528\u4e8e\u5206\u6790fMRI\u6570\u636e\u4e2d\u7684\u9ad8\u9636\u5927\u8111\u8fde\u63a5\uff0c\u5e76\u68c0\u6d4b\u91cd\u53e0\u793e\u533a\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u7684\u529f\u80fd\u8fde\u63a5\u7f51\u7edc\uff08FCNs\uff09\u5728\u91cf\u5316\u591a\u4e2a\u5927\u8111\u533a\u57df\u4e4b\u95f4\u7684\u9ad8\u9636\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u8d85\u56fe\u6784\u5efa\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u9ad8\u9636\u5173\u7cfb\uff0c\u4f46\u5982\u4f55\u8868\u5f81\u5176\u62d3\u6251\u7ed3\u6784\u4ecd\u662f\u672a\u516c\u5f00\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u805a\u7c7b\u7684\u8d85\u56fe\u65b9\u6cd5\u6765\u68c0\u6d4b\u91cd\u53e0\u793e\u533a\u7ed3\u6784\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u6355\u6349\u9ad8\u9636\u4f9d\u8d56\u5173\u7cfb\u7684\u5927\u8111\u8d85\u56fe\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u8c31\u805a\u7c7b\u7684\u8d85\u56fe\u65b9\u6cd5\u6765\u68c0\u6d4b\u91cd\u53e0\u793e\u533a\u7ed3\u6784\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8c31\u805a\u7c7b\u7684\u8d85\u56fe\u65b9\u6cd5\u6765\u68c0\u6d4b\u5927\u8111\u8d85\u7f51\u7edc\u7684\u91cd\u53e0\u793e\u533a\u7ed3\u6784\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5065\u5eb7\u5e74\u8f7b\u6210\u5e74\u4eba\u7684\u9759\u606f\u6001fMRI\u6570\u636e\uff0c\u4ee5\u603b\u7ed3\u7fa4\u4f53\u4e2d\u7684\u91cd\u53e0\u793e\u533a\u7ed3\u6784\u3002"}}
{"id": "2507.09367", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09367", "abs": "https://arxiv.org/abs/2507.09367", "authors": ["Shiva Azimi", "Arash Tavakoli"], "title": "Simulation for All: A Step-by-Step Cookbook for Developing Human-Centered Multi-Agent Transportation Simulators", "comment": null, "summary": "As cities evolve toward more complex and multimodal transportation systems,\nthe need for human-centered multi-agent simulation tools has never been more\nurgent. Yet most existing platforms remain limited - they often separate\ndifferent types of road users, rely on scripted or pre-defined behaviors,\noverlook public transit users as active participants, and are rarely designed\nwith accessibility in mind for non-technical users. To address this gap, this\npaper presents the specifications of a multi-agent simulation platform designed\nto support real-time, human-centered, and immersive studies of all road users,\naccompanied by open-source scripts for replication. Using high-fidelity\nimmersive virtual environments, our platform enables interaction across public\ntransit users, pedestrians, cyclists, automated vehicles, and drivers. The\narchitecture is modular, extensible, and designed for accessibility. The system\nintegrates hardware-specific modules - including an omnidirectional treadmill,\na seating arrangement, a smart trainer, and an actuated cockpit. Additionally,\nthe platform collects multimodal physiological, neurological, and behavioral\ndata through embedded sensing devices such as functional near-infrared\nspectroscopy (fNIRS), eye tracking, and wrist-based biosensors. To show the\nusability of this system, we present three use cases. Simulation for All aims\nto lower the barrier to entry for high-fidelity transportation simulation,\nsupport experimentation across disciplines, and advance our understanding of\nmultimodal mobility in complex urban environments.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u529f\u80fd\u4ea4\u901a\u6a21\u62df\u5e73\u53f0\uff0c\u5b83\u5177\u6709\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\u3001\u652f\u6301\u591a\u79cd\u4ea4\u901a\u65b9\u5f0f\u548c\u7528\u6237\uff08\u5305\u62ec\u884c\u4eba\u3001\u9a91\u884c\u8005\u3001\u53f8\u673a\u548c\u516c\u4ea4\u4e58\u5ba2\uff09\uff0c\u5e76\u901a\u8fc7\u591a\u79cd\u4f20\u611f\u5668\u6536\u96c6\u751f\u7406\u6570\u636e\u3002\u8be5\u5e73\u53f0\u65e8\u5728\u964d\u4f4e\u6a21\u62df\u7684\u95e8\u69db\uff0c\u652f\u6301\u8de8\u5b66\u79d1\u7814\u7a76\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u57ce\u5e02\u4ea4\u901a\u3002", "motivation": "\u968f\u7740\u57ce\u5e02\u5411\u66f4\u590d\u6742\u548c\u591a\u6a21\u5f0f\u7684\u4ea4\u901a\u7cfb\u7edf\u53d1\u5c55\uff0c\u5bf9\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u591a\u667a\u80fd\u4f53\u6a21\u62df\u5de5\u5177\u7684\u9700\u6c42\u4ece\u672a\u5982\u6b64\u8feb\u5207\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u5927\u591a\u6570\u5e73\u53f0\u4ecd\u7136\u6709\u9650\u2014\u2014\u5b83\u4eec\u901a\u5e38\u533a\u5206\u4e0d\u540c\u7c7b\u578b\u7684\u9053\u8def\u4f7f\u7528\u8005\uff0c\u4f9d\u8d56\u4e8e\u811a\u672c\u5316\u6216\u9884\u5b9a\u4e49\u7684\u884c\u4e3a\uff0c\u5ffd\u89c6\u4e86\u516c\u4ea4\u901a\u884c\u8005\u4f5c\u4e3a\u79ef\u6781\u53c2\u4e0e\u8005\u7684\u4f5c\u7528\uff0c\u5e76\u4e14\u5f88\u5c11\u8003\u8651\u5230\u5bf9\u975e\u6280\u672f\u7528\u6237\u7684\u53ef\u8bbf\u95ee\u6027\u3002", "method": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6a21\u62df\u5e73\u53f0\u7684\u89c4\u8303\uff0c\u8be5\u5e73\u53f0\u65e8\u5728\u652f\u6301\u6240\u6709\u9053\u8def\u4f7f\u7528\u8005\u7684\u5b9e\u65f6\u3001\u4ee5\u4eba\u4e3a\u672c\u548c\u6c89\u6d78\u5f0f\u7814\u7a76\uff0c\u5e76\u9644\u6709\u7528\u4e8e\u590d\u5236\u7684\u5f00\u6e90\u811a\u672c\u3002\u8be5\u5e73\u53f0\u5229\u7528\u9ad8\u4fdd\u771f\u6c89\u6d78\u5f0f\u865a\u62df\u73af\u5883\uff0c\u5b9e\u73b0\u4e86\u516c\u4ea4\u901a\u884c\u8005\u3001\u884c\u4eba\u3001\u9a91\u884c\u8005\u3001\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u9a7e\u9a76\u5458\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u8be5\u67b6\u6784\u662f\u6a21\u5757\u5316\u3001\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u7279\u5b9a\u786c\u4ef6\u6a21\u5757\uff08\u5305\u62ec\u5168\u5411\u8dd1\u6b65\u673a\u3001\u5ea7\u6905\u88c5\u7f6e\u3001\u667a\u80fd\u6559\u7ec3\u548c\u9a71\u52a8\u5ea7\u8231\uff09\uff0c\u5e76\u901a\u8fc7\u5d4c\u5165\u5f0f\u4f20\u611f\u8bbe\u5907\uff08\u5982\u529f\u80fd\u6027\u8fd1\u7ea2\u5916\u5149\u8c31\uff08fNIRS\uff09\u3001\u773c\u52a8\u8ffd\u8e2a\u548c\u57fa\u4e8e\u624b\u8155\u7684\u751f\u7269\u4f20\u611f\u5668\uff09\u6536\u96c6\u591a\u6a21\u6001\u751f\u7406\u3001\u795e\u7ecf\u548c\u884c\u4e3a\u6570\u636e\u3002", "result": "\u8be5\u5e73\u53f0\u80fd\u591f\u5b9e\u73b0\u516c\u4ea4\u901a\u884c\u8005\u3001\u884c\u4eba\u3001\u9a91\u884c\u8005\u3001\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u9a7e\u9a76\u5458\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u5e73\u53f0\u65e8\u5728\u964d\u4f4e\u9ad8\u4fdd\u771f\u4ea4\u901a\u6a21\u62df\u7684\u95e8\u69db\uff0c\u652f\u6301\u8de8\u5b66\u79d1\u5b9e\u9a8c\uff0c\u5e76\u589e\u8fdb\u6211\u4eec\u5bf9\u590d\u6742\u57ce\u5e02\u73af\u5883\u4e2d\u591a\u5f0f\u8054\u8fd0\u7684\u7406\u89e3\u3002"}}
{"id": "2507.08833", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08833", "abs": "https://arxiv.org/abs/2507.08833", "authors": ["Seokmin Ko"], "title": "LoRA Is Slower Than You Think", "comment": null, "summary": "Low-Rank Adaptation (LoRA) is one of the most widely used techniques for\nfine-tuning large language models (LLMs). By introducing a small number of\ntrainable low-rank weight matrices, LoRA substantially reduces the number of\nparameters that need to be updated, offering significant advantages in memory\nconsumption and computational efficiency compared to full fine-tuning. However,\nwe observed that LoRA does not consistently provide speed improvements across\nall model architectures and training setups. Motivated by this inconsistency,\nwe conduct a comprehensive analysis of LoRA's performance and investigate the\nunderlying factors limiting its speedup. Based on our findings, we propose\nseveral methods for more efficient fine-tuning of LLMs. We empirically evaluate\nthese methods and compare them to LoRA, demonstrating that our approach\nachieves comparable or superior performance while delivering more consistent\ntraining speed improvements. Our work offers valuable insights and practical\nguidelines for practitioners seeking to optimize LLM fine-tuning under resource\nconstraints.", "AI": {"tldr": "LoRA \u5fae\u8c03 LLM \u5b58\u5728\u901f\u5ea6\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u65b0\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\u7684\u4e00\u81f4\u6027\u3002", "motivation": "LoRA \u5728\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u65f6\uff0c\u5e76\u4e0d\u4e00\u81f4\u5730\u63d0\u4f9b\u901f\u5ea6\u63d0\u5347\uff0c\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u5c11\u91cf\u53ef\u8bad\u7ec3\u7684\u4f4e\u79e9\u6743\u91cd\u77e9\u9635\u6765\u5fae\u8c03\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0c\u5e76\u5bf9 LoRA \u7684\u6027\u80fd\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u627e\u51fa\u9650\u5236\u5176\u52a0\u901f\u7684\u6f5c\u5728\u56e0\u7d20\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e0e LoRA \u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u8bad\u7ec3\u901f\u5ea6\u6539\u8fdb\u3002", "conclusion": "LoRA \u5728\u6240\u6709\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u5e76\u4e0d\u4e00\u81f4\u5730\u63d0\u4f9b\u901f\u5ea6\u63d0\u5347\u3002\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86\u4e00\u4e9b\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5fae\u8c03 LLM\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5177\u6709\u53ef\u6bd4\u6027\u6216\u66f4\u4f18\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e00\u81f4\u7684\u8bad\u7ec3\u901f\u5ea6\u6539\u8fdb\u3002"}}
{"id": "2507.09143", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09143", "abs": "https://arxiv.org/abs/2507.09143", "authors": ["Siyuan Qian", "Ankit Shukla", "Shaloo Rakheja"], "title": "Influence of thermal noise on the field-driven dynamics of the non-collinear antiferromagnet Mn3Sn", "comment": "15 pages, 10 figures", "summary": "$\\mathrm{Mn_3Sn}(0\\overline{1}\\overline{1}0)[0001]$ experiences a tensile\nstrain when grown epitaxially on $\\mathrm{MgO}(110)[001]$, and thus the energy\nlandscape changes from six-fold symmetry to two-fold symmetry. External\nmagnetic field further breaks the symmetry and the \\textcolor{black}{resulting\nenergy landscape is sensitive to} the field orientation relative to the easy\naxis. \\textcolor{black}{In the presence of thermal noise,} the relaxation of\nthe magnetic octupole moment in \\textcolor{black}{a strained Mn$_3$Sn film} is\ncomposed of four distinct escape processes involving the two saddle points and\ntwo equilibrium states in the energy landscape. Here, we apply harmonic\ntransition-state theory to derive analytical expressions for the inter-well\nescape time and octupole moment relaxation time, both influenced by an external\nsymmetry-breaking magnetic field and finite thermal noise in the\nintermediate-to-high damping regime. The analytical predictions are in strong\nagreement with comprehensive numerical simulations based on coupled LLG\nequations. The results presented here are crucial toward realizing Mn$_3$Sn's\napplications in random number generation and probabilistic computing.", "AI": {"tldr": "\u901a\u8fc7\u8c10\u6ce2\u8fc7\u6e21\u6001\u7406\u8bba\uff0c\u6211\u4eec\u5bfc\u51fa\u4e86\u5e94\u53d8Mn3Sn\u8584\u819c\u4e2d\u78c1\u516b\u6781\u77e9\u5f1b\u8c6b\u65f6\u95f4\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u8be5\u5f1b\u8c6b\u65f6\u95f4\u53d7\u5916\u90e8\u78c1\u573a\u548c\u70ed\u566a\u58f0\u5f71\u54cd\uff0c\u5e76\u5f97\u5230\u4e86\u6570\u503c\u6a21\u62df\u7684\u9a8c\u8bc1\uff0c\u4e3aMn3Sn\u5728\u968f\u673a\u6570\u751f\u6210\u548c\u6982\u7387\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u7814\u7a76\u80cc\u666f\u662fMn3Sn\u5728MgO\u886c\u5e95\u4e0a\u5916\u5ef6\u751f\u957f\u65f6\u4f1a\u7ecf\u5386\u62c9\u4f38\u5e94\u53d8\uff0c\u5bfc\u81f4\u5176\u80fd\u91cf\u666f\u89c2\u4ece\u516d\u91cd\u5bf9\u79f0\u53d8\u4e3a\u4e8c\u91cd\u5bf9\u79f0\uff0c\u5916\u90e8\u78c1\u573a\u8fdb\u4e00\u6b65\u6253\u7834\u4e86\u5bf9\u79f0\u6027\uff0c\u4f7f\u5f97\u80fd\u91cf\u666f\u89c2\u5bf9\u78c1\u573a\u65b9\u5411\u654f\u611f\u3002\u5728\u70ed\u566a\u58f0\u5b58\u5728\u4e0b\uff0c\u5e94\u53d8Mn3Sn\u8584\u819c\u7684\u78c1\u516b\u6781\u77e9\u5f1b\u8c6b\u5305\u542b\u56db\u4e2a\u72ec\u7279\u7684\u9003\u9038\u8fc7\u7a0b\u3002", "method": "\u91c7\u7528\u8c10\u6ce2\u8fc7\u6e21\u6001\u7406\u8bba\u63a8\u5bfc\u4e86\u52bf\u5792\u8d8a\u8d8a\u65f6\u95f4\uff08\u8d8a\u8fc7\u52bf\u5792\u6240\u9700\u65f6\u95f4\uff09\u548c\u78c1\u516b\u6781\u77e9\u5f1b\u8c6b\u65f6\u95f4\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u8003\u8651\u4e86\u5916\u90e8\u78c1\u573a\u548c\u70ed\u566a\u58f0\u5728\u4e2d\u9ad8\u963b\u5c3c\u60c5\u51b5\u4e0b\u7684\u5f71\u54cd\uff0c\u968f\u540e\u901a\u8fc7\u8026\u5408LLG\u65b9\u7a0b\u8fdb\u884c\u4e86\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63a8\u5bfc\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\u4e0e\u57fa\u4e8e\u8026\u5408LLG\u65b9\u7a0b\u7684\u6570\u503c\u6a21\u62df\u7ed3\u679c\u9ad8\u5ea6\u543b\u5408\uff0c\u9a8c\u8bc1\u4e86\u7406\u8bba\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b9e\u73b0Mn3Sn\u5728\u968f\u673a\u6570\u751f\u6210\u548c\u6982\u7387\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.09620", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2507.09620", "abs": "https://arxiv.org/abs/2507.09620", "authors": ["George Z. Li", "Zihan Tan", "Tianyi Zhang"], "title": "Paths and Intersections: Exact Emulators for Planar Graphs", "comment": "FOCS 2025", "summary": "We study vertex sparsification for preserving distances in planar graphs.\nGiven an edge-weighted planar graph with $k$ terminals, the goal is to\nconstruct an emulator, which is a smaller edge-weighted planar graph that\ncontains the terminals and exactly preserves the pairwise distances between\nthem. We construct exact planar emulators of size $O(f^2k^2)$ in the setting\nwhere terminals lie on $f$ faces in the planar embedding of the input graph.\nOur result generalizes and interpolates between the previous results of Chang\nand Ophelders and Goranci, Henzinger, and Peng which is an $O(k^2)$ bound in\nthe setting where all terminals lie on a single face (i.e., $f=1$), and the\nresult of Krauthgamer, Nguyen, and Zondiner, which is an $O(k^4)$ bound for the\ngeneral case (i.e., $f=k$).\n  Our construction follows a recent new way of analyzing graph structures, by\nviewing graphs as paths and their intersections, which we believe is of\nindependent interest.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u5e73\u9762\u56fe\u7684\u9876\u70b9\u7a00\u758f\u5316\u6280\u672f\uff0c\u65e8\u5728\u4fdd\u7559\u56fe\u4e2dk\u4e2a\u7ec8\u7aef\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u6784\u5efa\u51fa\u5927\u5c0f\u4e3aO(f^2k^2)\u7684\u7cbe\u786e\u5e73\u9762\u6a21\u62df\u5668\uff0c\u5176\u4e2df\u662f\u7ec8\u7aef\u6240\u5728\u7684\u9762\u7684\u6570\u91cf\u3002\u8fd9\u4e00\u6210\u679c\u662f\u5bf9\u5148\u524d\u7814\u7a76\u7684\u6269\u5c55\uff0c\u5e76\u4e3a\u56fe\u8bba\u5206\u6790\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u4e3a\u5177\u6709k\u4e2a\u7ec8\u7aef\u7684\u5e73\u9762\u56fe\u6784\u5efa\u4e00\u4e2a\u66f4\u5c0f\u7684\u8fb9\u52a0\u6743\u5e73\u9762\u56fe\uff08\u6a21\u62df\u5668\uff09\uff0c\u8be5\u6a21\u62df\u5668\u80fd\u7cbe\u786e\u5730\u4fdd\u6301\u7ec8\u7aef\u4e4b\u95f4\u7684\u6210\u5bf9\u8ddd\u79bb\uff0c\u5e76\u4e14\u5728\u7ec8\u7aef\u4f4d\u4e8e\u5e73\u9762\u5d4c\u5165\u7684f\u4e2a\u9762\u4e0a\u8fd9\u4e00\u7279\u5b9a\u8bbe\u7f6e\u4e0b\u7ed9\u51fa\u4e86\u65b0\u7684\u7a00\u758f\u5316\u7ed3\u679c\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u7ed3\u6784\u5206\u6790\u65b9\u6cd5\uff0c\u5c06\u56fe\u89c6\u4e3a\u8def\u5f84\u53ca\u5176\u4ea4\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u5e73\u9762\u6a21\u62df\u5668\u7684\u6784\u5efa\u3002", "result": "\u6211\u4eec\u6210\u529f\u6784\u5efa\u4e86\u7cbe\u786e\u7684\u5e73\u9762\u6a21\u62df\u5668\uff0c\u5176\u5927\u5c0f\u4e3aO(f^2k^2)\uff0c\u8be5\u7ed3\u679c\u5728f=1\uff08\u6240\u6709\u7ec8\u7aef\u4f4d\u4e8e\u540c\u4e00\u9762\uff09\u548cf=k\uff08\u901a\u7528\u60c5\u51b5\uff09\u7684\u5148\u524d\u7ed3\u679c\u4e4b\u95f4\u8fdb\u884c\u4e86\u63a8\u5e7f\u548c\u63d2\u503c\uff0c\u5206\u522b\u5bf9\u5e94\u4e8eO(k^2)\u548cO(k^4)\u7684\u754c\u9650\u3002", "conclusion": "\u672c\u6587\u4e3a\u9876\u70b9\u7a00\u758f\u5316\u548c\u8ddd\u79bb\u4fdd\u6301\u5728\u5e73\u9762\u56fe\u4e2d\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u7279\u522b\u662f\u5728\u5177\u6709k\u4e2a\u7ec8\u7aef\u4e14\u7ec8\u7aef\u4f4d\u4e8ef\u4e2a\u9762\u4e0a\u7684\u5e73\u9762\u5d4c\u5165\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u6784\u5efa\u4e86\u7cbe\u786e\u7684\u5e73\u9762\u6a21\u62df\u5668\uff0c\u5176\u5927\u5c0f\u4e3aO(f^2k^2)\u3002"}}
{"id": "2507.09704", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2507.09704", "abs": "https://arxiv.org/abs/2507.09704", "authors": ["Xiaotang Zhang", "Ziyi Chang", "Qianhui Men", "Hubert Shum"], "title": "Real-time and Controllable Reactive Motion Synthesis via Intention Guidance", "comment": null, "summary": "We propose a real-time method for reactive motion synthesis based on the\nknown trajectory of input character, predicting instant reactions using only\nhistorical, user-controlled motions. Our method handles the uncertainty of\nfuture movements by introducing an intention predictor, which forecasts key\njoint intentions to make pose prediction more deterministic from the historical\ninteraction. The intention is later encoded into the latent space of its\nreactive motion, matched with a codebook which represents mappings between\ninput and output. It samples a categorical distribution for pose generation and\nstrengthens model robustness through adversarial training. Unlike previous\noffline approaches, the system can recursively generate intentions and reactive\nmotions using feedback from earlier steps, enabling real-time, long-term\nrealistic interactive synthesis. Both quantitative and qualitative experiments\nshow our approach outperforms other matching-based motion synthesis approaches,\ndelivering superior stability and generalizability. In our method, user can\nalso actively influence the outcome by controlling the moving directions,\ncreating a personalized interaction path that deviates from predefined\ntrajectories.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u65f6\u53cd\u5e94\u8fd0\u52a8\u5408\u6210\u65b9\u6cd5\uff0c\u901a\u8fc7\u610f\u56fe\u9884\u6d4b\u5668\u5904\u7406\u672a\u6765\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u4f7f\u7528\u9690\u7a7a\u95f4\u5339\u914d\u548c\u5bf9\u6297\u6027\u8bad\u7ec3\u8fdb\u884c\u59ff\u6001\u751f\u6210\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5141\u8bb8\u7528\u6237\u4e2a\u6027\u5316\u4ea4\u4e92\u8def\u5f84\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u53cd\u5e94\u8fd0\u52a8\u5408\u6210\u7684\u5b9e\u65f6\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4ec5\u4f7f\u7528\u5386\u53f2\u3001\u7528\u6237\u63a7\u5236\u7684\u8fd0\u52a8\u6765\u9884\u6d4b\u5373\u65f6\u53cd\u5e94\u7684\u95ee\u9898\uff0c\u5e76\u5904\u7406\u672a\u6765\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f93\u5165\u89d2\u8272\u5df2\u77e5\u8f68\u8ff9\u7684\u5b9e\u65f6\u53cd\u5e94\u8fd0\u52a8\u5408\u6210\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u5386\u53f2\u7528\u6237\u63a7\u5236\u7684\u8fd0\u52a8\u6765\u9884\u6d4b\u5373\u65f6\u53cd\u5e94\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u610f\u56fe\u9884\u6d4b\u5668\u6765\u5904\u7406\u672a\u6765\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u9884\u6d4b\u5173\u952e\u5173\u8282\u610f\u56fe\uff0c\u4f7f\u59ff\u6001\u9884\u6d4b\u4ece\u5386\u53f2\u4ea4\u4e92\u4e2d\u66f4\u52a0\u786e\u5b9a\u3002\u7136\u540e\u5c06\u610f\u56fe\u7f16\u7801\u5230\u5176\u53cd\u5e94\u8fd0\u52a8\u7684\u9690\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4e0e\u8868\u793a\u8f93\u5165\u8f93\u51fa\u6620\u5c04\u7684\u7801\u672c\u8fdb\u884c\u5339\u914d\u3002\u5b83\u4e3a\u59ff\u6001\u751f\u6210\u5bf9\u5206\u7c7b\u5206\u5e03\u8fdb\u884c\u91c7\u6837\uff0c\u5e76\u901a\u8fc7\u5bf9\u6297\u6027\u8bad\u7ec3\u6765\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u5904\u7406\u672a\u6765\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5386\u53f2\u4ea4\u4e92\u8fdb\u884c\u66f4\u786e\u5b9a\u7684\u59ff\u6001\u9884\u6d4b\uff0c\u5e76\u751f\u6210\u5b9e\u65f6\u7684\u3001\u957f\u671f\u7684\u3001\u771f\u5b9e\u7684\u4ea4\u4e92\u5408\u6210\u8fd0\u52a8\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u5339\u914d\u7684\u8fd0\u52a8\u5408\u6210\u65b9\u6cd5\u3002\u7528\u6237\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u79fb\u52a8\u65b9\u5411\u6765\u4e2a\u6027\u5316\u4ea4\u4e92\u8def\u5f84\u3002", "conclusion": "\u672c\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u610f\u56fe\u9884\u6d4b\u5668\u6765\u5904\u7406\u672a\u6765\u8fd0\u52a8\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u901a\u8fc7\u5bf9\u5386\u53f2\u4ea4\u4e92\u8fdb\u884c\u59ff\u6001\u9884\u6d4b\uff0c\u5e76\u7ed3\u5408\u9690\u7a7a\u95f4\u4e2d\u7684\u610f\u56fe\u7f16\u7801\uff0c\u901a\u8fc7\u5339\u914d\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u7684\u8fd0\u52a8\u5408\u6210\u3002\u4e0e\u4e4b\u524d\u7684\u79bb\u7ebf\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u7cfb\u7edf\u53ef\u4ee5\u901a\u8fc7\u65e9\u671f\u6b65\u9aa4\u7684\u53cd\u9988\u9012\u5f52\u5730\u751f\u6210\u610f\u56fe\u548c\u53cd\u5e94\u8fd0\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u5b9e\u65f6\u3001\u957f\u671f\u7684\u771f\u5b9e\u4ea4\u4e92\u5408\u6210\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u548c\u6cdb\u5316\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u4e8e\u5339\u914d\u7684\u8fd0\u52a8\u5408\u6210\u65b9\u6cd5\u3002\u7528\u6237\u8fd8\u53ef\u4ee5\u901a\u8fc7\u63a7\u5236\u79fb\u52a8\u65b9\u5411\u6765\u79ef\u6781\u5f71\u54cd\u7ed3\u679c\uff0c\u4ece\u800c\u521b\u5efa\u504f\u79bb\u9884\u5b9a\u8f68\u8ff9\u7684\u4e2a\u6027\u5316\u4ea4\u4e92\u8def\u5f84\u3002"}}
{"id": "2507.09080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09080", "abs": "https://arxiv.org/abs/2507.09080", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "title": "BioAnalyst: A Foundation Model for Biodiversity", "comment": null, "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.", "AI": {"tldr": "\u7814\u7a76\u8005\u4eec\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aBioAnalyst\u7684\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\uff0c\u7528\u4e8e\u751f\u7269\u591a\u6837\u6027\u5206\u6790\u548c\u4fdd\u62a4\u3002\u8be5\u6a21\u578b\u5728\u5927\u91cf\u591a\u6a21\u6001\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u80fd\u591f\u9002\u5e94\u591a\u79cd\u751f\u6001\u9884\u6d4b\u4efb\u52a1\uff0c\u5e76\u5728\u6570\u636e\u7a00\u7f3a\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u4e3a\u751f\u6001\u5b66\u7814\u7a76\u548c\u4fdd\u62a4\u5de5\u4f5c\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u65b0\u5de5\u5177\u3002", "motivation": "\u751f\u7269\u591a\u6837\u6027\u7684\u52a0\u901f\u4e27\u5931\u7ed9\u751f\u6001\u5b66\u7814\u7a76\u548c\u4fdd\u62a4\u7b56\u7565\u5e26\u6765\u4e86\u4e25\u5cfb\u7684\u6311\u6218\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u7efc\u5408\u7684\u76d1\u6d4b\u3001\u9884\u6d4b\u548c\u4fdd\u62a4\u89c4\u5212\u80fd\u529b\u3002\u4eba\u5de5\u667a\u80fd\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u9002\u5e94\u4e0d\u540c\u4efb\u52a1\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u56e0\u6b64\uff0c\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u751f\u7269\u591a\u6837\u6027\u5206\u6790\u548c\u4fdd\u62a4\u89c4\u5212\u7684FM\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002", "method": "BioAnalyst\u6a21\u578b\u91c7\u7528\u4e86\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0c\u5e76\u5728\u5305\u542b\u7269\u79cd\u51fa\u73b0\u8bb0\u5f55\u3001\u9065\u611f\u6307\u6807\u3001\u6c14\u5019\u548c\u73af\u5883\u53d8\u91cf\u7684\u5e7f\u6cdb\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9884\u8bad\u7ec3\u3002\u8be5\u6a21\u578b\u88ab\u8bbe\u8ba1\u4e3a\u5177\u6709\u9002\u5e94\u6027\uff0c\u53ef\u4ee5\u9488\u5bf9\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u3001\u6816\u606f\u5730\u9002\u5b9c\u6027\u8bc4\u4f30\u3001\u5165\u4fb5\u7269\u79cd\u68c0\u6d4b\u548c\u79cd\u7fa4\u8d8b\u52bf\u9884\u6d4b\u7b49\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u7814\u7a76\u901a\u8fc7\u5728\u4e24\u4e2a\u4e0b\u6e38\u5e94\u7528\u573a\u666f\u4e2d\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86BioAnalyst\u76f8\u5bf9\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u901a\u7528\u6027\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u758f\u7684\u573a\u666f\u4e0b\u3002\u7ed3\u679c\u663e\u793a\uff0cBioAnalyst\u5728\u8fd9\u4e9b\u573a\u666f\u4e0b\u8bbe\u5b9a\u4e86\u65b0\u7684\u51c6\u786e\u6027\u57fa\u51c6\uff0c\u7279\u522b\u662f\u5728\u751f\u6001\u9884\u6d4b\u65b9\u9762\u3002", "conclusion": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86BioAnalyst\uff0c\u4e00\u4e2a\u9488\u5bf9\u751f\u7269\u591a\u6837\u6027\u5206\u6790\u548c\u4fdd\u62a4\u89c4\u5212\u7684\u5f00\u521b\u6027\u57fa\u7840\u6a21\u578b\u3002\u901a\u8fc7\u5728\u5e7f\u6cdb\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u91c7\u7528\u57fa\u4e8eTransformer\u7684\u67b6\u6784\uff0cBioAnalyst\u80fd\u591f\u9002\u5e94\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u4f8b\u5982\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u3001\u6816\u606f\u5730\u9002\u5b9c\u6027\u8bc4\u4f30\u3001\u5165\u4fb5\u7269\u79cd\u68c0\u6d4b\u548c\u79cd\u7fa4\u8d8b\u52bf\u9884\u6d4b\u3002\u7814\u7a76\u8868\u660e\uff0cBioAnalyst\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u751f\u6001\u9884\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u7684\u51c6\u786e\u6027\u57fa\u51c6\u3002\u901a\u8fc7\u516c\u5f00\u6a21\u578b\u53ca\u5176\u5fae\u8c03\u5de5\u4f5c\u6d41\u7a0b\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u4fc3\u8fdb\u751f\u7269\u591a\u6837\u6027\u5efa\u6a21\u65b9\u9762\u7684\u5408\u4f5c\uff0c\u5e76\u63a8\u52a8\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u89e3\u51b3\u65b9\u6848\u5728\u5e94\u5bf9\u7d27\u8feb\u7684\u751f\u6001\u6311\u6218\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09553", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.09553", "abs": "https://arxiv.org/abs/2507.09553", "authors": ["Alejandra Guedeja-Marr\u00f3n", "Henrik Lyder Andersen", "Gabriel S\u00e1nchez-Santolino", "Lunjie Zeng", "Alok Ranjan", "In\u00e9s Garc\u00eda-Manuz", "Fran\u00e7ois Fauth", "Catherine Dejoie", "Eva Olsson", "Paolo Perna", "Maria Varela", "Lucas P\u00e9rez", "Matilde Saura-M\u00fazquiz"], "title": "Correlating synthesis, structure and thermal stability of CuBi nanowires for spintronic applications by electron microscopy and in situ scattering methods", "comment": "45 pages, 9 figures", "summary": "Bi-doped copper (Cu1-xBix) nanowires (NWs), promising candidates for\nspintronic applications due to their potential for a giant spin Hall effect\n(SHE), were synthesized and their structural properties and thermal stability\nwere investigated. Using template-assisted electrodeposition, Cu1-xBix\nnanowires with varying bismuth (Bi) content (x=0, 2, 4, and 7%) and different\ncrystalline domain sizes were fabricated. Structural analysis by advanced\nelectron microscopy and X-ray scattering techniques revealed the influence of\nsynthesis conditions on the resulting NW crystal structure and microstructure,\nincluding Bi localization (within the lattice or in the grain boundaries),\ncrystallite domain dimensions, and lattice distortions. While NWs with larger\ncrystalline domains allow homogeneous Bi incorporation into the Cu lattice, NWs\nwith smaller crystalline domains exhibit noticeable Bi accumulation at grain\nboundaries. The thermal stability of the NWs was examined using variable\ntemperature X-ray diffraction and total scattering. Upon heating, lattice\ndistortions consistent with Bi diffusion out of the Cu lattice were observed,\nwith subsequent crystallization of rhombohedral metallic Bi upon cooling. These\nfindings establish a foundation for optimizing the SHE performance of Cu1-xBix\nnanowires for spintronic devices by correlating synthesis parameters with\nmicrostructural features and thermal behavior.", "AI": {"tldr": "\u5408\u6210\u4e86\u94dc\u94cb\u5408\u91d1\u7eb3\u7c73\u7ebf\uff0c\u7814\u7a76\u4e86\u5176\u7ed3\u6784\u6027\u8d28\u548c\u70ed\u7a33\u5b9a\u6027\uff0c\u4e3a\u4f18\u5316\u5176\u5728\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u4e2d\u7684\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u6027\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u94dc\u94cb\u5408\u91d1\uff08Cu$_{1-x}$Bi$_x$\uff09\u7eb3\u7c73\u7ebf\u56e0\u5176\u6f5c\u5728\u7684\u5de8\u5927\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff08SHE\uff09\u800c\u6210\u4e3a\u81ea\u65cb\u7535\u5b50\u5b66\u7684\u6709\u5e0c\u671b\u7684\u5019\u9009\u6750\u6599\u3002", "method": "\u901a\u8fc7\u6a21\u677f\u8f85\u52a9\u7535\u6c89\u79ef\u6cd5\u5408\u6210\u4e86\u5177\u6709\u4e0d\u540c\u94cb\u542b\u91cf\uff08x=0, 2, 4, 7%\uff09\u548c\u4e0d\u540c\u6676\u57df\u5c3a\u5bf8\u7684\u94dc\u94cb\u5408\u91d1\uff08Cu$_{1-x}$Bi$_x$\uff09\u7eb3\u7c73\u7ebf\uff0c\u5e76\u5229\u7528\u5148\u8fdb\u7535\u5b50\u663e\u5fae\u955c\u548cX\u5c04\u7ebf\u6563\u5c04\u6280\u672f\u8fdb\u884c\u4e86\u7ed3\u6784\u5206\u6790\uff0c\u91c7\u7528\u53ef\u53d8\u6e29\u5ea6X\u5c04\u7ebf\u884d\u5c04\u548c\u5168\u6563\u5c04\u6280\u672f\u7814\u7a76\u4e86\u7eb3\u7c73\u7ebf\u7684\u70ed\u7a33\u5b9a\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8f83\u5927\u7684\u6676\u57df\u5c3a\u5bf8\u6709\u5229\u4e8e\u94cb\u5747\u5300\u63ba\u6742\u5230\u94dc\u6676\u683c\u4e2d\uff0c\u800c\u8f83\u5c0f\u7684\u6676\u57df\u5c3a\u5bf8\u4f1a\u5bfc\u81f4\u94cb\u5728\u6676\u754c\u5904\u660e\u663e\u5bcc\u96c6\u3002\u52a0\u70ed\u65f6\uff0c\u89c2\u5bdf\u5230\u4e0e\u94cb\u4ece\u94dc\u6676\u683c\u6269\u6563\u4e00\u81f4\u7684\u6676\u683c\u7578\u53d8\uff0c\u5e76\u5728\u51b7\u5374\u65f6\u7ed3\u6676\u4e3a\u9762\u5fc3\u7acb\u65b9\u7684\u91d1\u5c5e\u94cb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u4f18\u5316\u94dc\u94cb\u5408\u91d1\u7eb3\u7c73\u7ebf\u5728\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u4e2d\u7684\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u6027\u80fd\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u901a\u8fc7\u5173\u8054\u5408\u6210\u53c2\u6570\u4e0e\u5fae\u89c2\u7ed3\u6784\u7279\u5f81\u53ca\u70ed\u884c\u4e3a\u3002"}}
{"id": "2507.09280", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09280", "abs": "https://arxiv.org/abs/2507.09280", "authors": ["Xuan He", "Yuxin Pan", "Yize Chen", "Danny H. K. Tsang"], "title": "Vertex-Guided Redundant Constraints Identification for Unit Commitment", "comment": null, "summary": "Power systems Unit Commitment (UC) problem determines the generator\ncommitment schedule and dispatch decisions to realize the reliable and economic\noperation of power networks. The growing penetration of stochastic renewables\nand demand behaviors makes it necessary to solve the UC problem timely. It is\npossible to derive lightweight, faster-to-solve UC models via constraint\nscreening to eliminate redundant constraints. However, the screening process\nremains computationally cumbersome due to the need of solving numerous linear\nprogramming (LP) problems. To reduce the number of LPs to solve, we introduce a\nnovel perspective on such classic LP-based screening. Our key insights lie in\nthe principle that redundant constraints will be satisfied by all vertices of\nthe screened feasible region. Using the UC decision variables' bounds tightened\nby solving much fewer LPs, we build an outer approximation for the UC feasible\nregion as the screened region. A matrix operation is then designed and applied\nto the outer approximation's vertices to identify all redundant constraints\non-the-fly. Adjustments for the outer approximation are further explored to\nimprove screening efficiency by considering the load operating range and\ncutting planes derived from UC cost and discrete unit status prediction.\nExtensive simulations are performed on a set of testbeds up to 2,383 buses to\nsubstantiate the effectiveness of the proposed schemes. Compared to classic\nLP-based screening, our schemes can achieve up to 8.8x acceleration while\nfinding the same redundant constraints.", "AI": {"tldr": "UC\u95ee\u9898\u901a\u5e38\u5f88\u590d\u6742\uff0c\u9700\u8981\u7ea6\u675f\u7b5b\u9009\u6765\u7b80\u5316\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ea6\u675f\u7b5b\u9009\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u53ef\u884c\u533a\u57df\u7684\u9876\u70b9\u6765\u51cf\u5c11\u6240\u9700\u7684\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u7684\u6570\u91cf\uff0c\u4ece\u800c\u5c06\u8ba1\u7b97\u901f\u5ea6\u63d0\u9ad8\u4e868.8\u500d\u3002", "motivation": "\u968f\u7740\u968f\u673a\u53ef\u518d\u751f\u80fd\u6e90\u548c\u8d1f\u8377\u884c\u4e3a\u7684\u6e17\u900f\u7387\u4e0d\u65ad\u63d0\u9ad8\uff0c\u9700\u8981\u53ca\u65f6\u89e3\u51b3\u7535\u529b\u7cfb\u7edf\u4e2d\u7684\u5355\u4f4d\u627f\u8bfa\uff08UC\uff09\u95ee\u9898\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u7684\u7ea6\u675f\u7b5b\u9009\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u9876\u70b9\u5206\u6790\u7684\u7ea6\u675f\u7b5b\u9009\u65b9\u6cd5\uff0c\u5229\u7528\u4f18\u5316\u540e\u7684\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u5b50\u95ee\u9898\u83b7\u5f97\u7684\u7528\u6237\u5355\u4f4d\u627f\u8bfa\uff08UC\uff09\u51b3\u7b56\u53d8\u91cf\u7684\u8fb9\u754c\u6765\u6784\u5efaUC\u53ef\u884c\u533a\u57df\u7684\u5916\u903c\u8fd1\u3002\u901a\u8fc7\u77e9\u9635\u8fd0\u7b97\u8bc6\u522b\u5197\u4f59\u7ea6\u675f\uff0c\u5e76\u901a\u8fc7\u8003\u8651\u8d1f\u8377\u64cd\u4f5c\u8303\u56f4\u548c\u4ece\u6210\u672c\u53ca\u79bb\u6563\u5355\u5143\u72b6\u6001\u9884\u6d4b\u4e2d\u63d0\u53d6\u7684\u5207\u5272\u5e73\u9762\u6765\u6539\u8fdb\u7b5b\u9009\u6548\u7387\u3002", "result": "\u901a\u8fc7\u5728\u591a\u8fbe2383\u4e2a\u8282\u70b9\u7684\u6d4b\u8bd5\u7528\u4f8b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u6a21\u62df\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6848\u76f8\u6bd4\u7ecf\u5178\u7684\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u7b5b\u9009\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u901f\u5ea6\u63d0\u9ad88.8\u500d\uff0c\u540c\u65f6\u627e\u5230\u76f8\u540c\u7684\u5197\u4f59\u7ea6\u675f\u3002"}}
{"id": "2507.09117", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09117", "abs": "https://arxiv.org/abs/2507.09117", "authors": ["Gagan Khandate"], "title": "Towards Human-level Dexterity via Robot Learning", "comment": "PhD thesis", "summary": "Dexterous intelligence -- the ability to perform complex interactions with\nmulti-fingered hands -- is a pinnacle of human physical intelligence and\nemergent higher-order cognitive skills. However, contrary to Moravec's paradox,\ndexterous intelligence in humans appears simple only superficially. Many\nmillion years were spent co-evolving the human brain and hands including rich\ntactile sensing. Achieving human-level dexterity with robotic hands has long\nbeen a fundamental goal in robotics and represents a critical milestone toward\ngeneral embodied intelligence. In this pursuit, computational sensorimotor\nlearning has made significant progress, enabling feats such as arbitrary\nin-hand object reorientation. However, we observe that achieving higher levels\nof dexterity requires overcoming very fundamental limitations of computational\nsensorimotor learning.\n  I develop robot learning methods for highly dexterous multi-fingered\nmanipulation by directly addressing these limitations at their root cause.\nChiefly, through key studies, this disseration progressively builds an\neffective framework for reinforcement learning of dexterous multi-fingered\nmanipulation skills. These methods adopt structured exploration, effectively\novercoming the limitations of random exploration in reinforcement learning. The\ninsights gained culminate in a highly effective reinforcement learning that\nincorporates sampling-based planning for direct exploration. Additionally, this\nthesis explores a new paradigm of using visuo-tactile human demonstrations for\ndexterity, introducing corresponding imitation learning techniques.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09777", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2507.09777", "abs": "https://arxiv.org/abs/2507.09777", "authors": ["Gabriel Mordecki", "Guillermo Moncecchi", "Javier Couto"], "title": "Te Ahorr\u00e9 Un Click: A Revised Definition of Clickbait and Detection in Spanish News", "comment": null, "summary": "We revise the definition of clickbait, which lacks current consensus, and\nargue that the creation of a curiosity gap is the key concept that\ndistinguishes clickbait from other related phenomena such as sensationalism and\nheadlines that do not deliver what they promise or diverge from the article.\nTherefore, we propose a new definition: clickbait is a technique for generating\nheadlines and teasers that deliberately omit part of the information with the\ngoal of raising the readers' curiosity, capturing their attention and enticing\nthem to click. We introduce a new approach to clickbait detection datasets\ncreation, by refining the concept limits and annotations criteria, minimizing\nthe subjectivity in the decision as much as possible. Following it, we created\nand release TA1C (for Te Ahorr\\'e Un Click, Spanish for Saved You A Click), the\nfirst open source dataset for clickbait detection in Spanish. It consists of\n3,500 tweets coming from 18 well known media sources, manually annotated and\nreaching a 0.825 Fleiss' K inter annotator agreement. We implement strong\nbaselines that achieve 0.84 in F1-score.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Clickbait\u5b9a\u4e49\uff0c\u5373\u5229\u7528\u597d\u5947\u5fc3\u7f3a\u53e3\u4f5c\u4e3a\u5173\u952e\u6982\u5ff5\u3002\u521b\u5efa\u4e86\u9996\u4e2a\u897f\u8bedClickbait\u68c0\u6d4b\u6570\u636e\u96c6TA1C\uff0c\u5e76\u5b9e\u73b0\u4e860.84\u7684F1\u5206\u6570\u57fa\u7ebf\u3002", "motivation": "\u7531\u4e8e\u5f53\u524d\u5bf9\u4e8eClickbait\u7684\u5b9a\u4e49\u7f3a\u4e4f\u5171\u8bc6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b9a\u4e49\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u96c6\u521b\u5efa\u65b9\u6cd5\uff0c\u7f29\u5c0f\u4e86\u6982\u5ff5\u8303\u56f4\u548c\u6ce8\u91ca\u6807\u51c6\uff0c\u4ee5\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u4e3b\u89c2\u6027\u3002", "result": "\u521b\u5efa\u5e76\u53d1\u5e03\u4e86TA1C\u6570\u636e\u96c6\uff0c\u5305\u542b3500\u6761\u6765\u81ea18\u4e2a\u77e5\u540d\u5a92\u4f53\u6765\u6e90\u7684\u624b\u52a8\u6ce8\u91ca\u63a8\u6587\uff0c\u5e76\u8fbe\u5230\u4e860.825\u7684Fleiss' Kappa\u3002\u63d0\u51fa\u7684\u57fa\u7ebf\u6a21\u578b\u5728F1\u5206\u6570\u4e0a\u8fbe\u5230\u4e860.84\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b3500\u6761\u63a8\u6587\u7684\u897f\u8bed\u6570\u636e\u96c6TA1C\uff0c\u5e76\u5b9e\u73b0\u4e860.84\u7684F1\u5206\u6570\u57fa\u7ebf\u3002"}}
{"id": "2507.09005", "categories": ["cs.CV", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.09005", "abs": "https://arxiv.org/abs/2507.09005", "authors": ["Cheng-Hsi Hsiao", "Krishna Kumar"], "title": "From images to properties: a NeRF-driven framework for granular material parameter inversion", "comment": null, "summary": "We introduce a novel framework that integrates Neural Radiance Fields (NeRF)\nwith Material Point Method (MPM) simulation to infer granular material\nproperties from visual observations. Our approach begins by generating\nsynthetic experimental data, simulating an plow interacting with sand. The\nexperiment is rendered into realistic images as the photographic observations.\nThese observations include multi-view images of the experiment's initial state\nand time-sequenced images from two fixed cameras. Using NeRF, we reconstruct\nthe 3D geometry from the initial multi-view images, leveraging its capability\nto synthesize novel viewpoints and capture intricate surface details. The\nreconstructed geometry is then used to initialize material point positions for\nthe MPM simulation, where the friction angle remains unknown. We render images\nof the simulation under the same camera setup and compare them to the observed\nimages. By employing Bayesian optimization, we minimize the image loss to\nestimate the best-fitting friction angle. Our results demonstrate that friction\nangle can be estimated with an error within 2 degrees, highlighting the\neffectiveness of inverse analysis through purely visual observations. This\napproach offers a promising solution for characterizing granular materials in\nreal-world scenarios where direct measurement is impractical or impossible.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408NeRF\u548cMPM\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u89c6\u89c9\u6570\u636e\u4ece\u6a21\u62df\u4e2d\u4f30\u8ba1\u6c99\u5b50\u7684\u6469\u64e6\u89d2\uff0c\u8bef\u5dee\u57282\u5ea6\u4ee5\u5185\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u4ece\u89c6\u89c9\u89c2\u6d4b\u4e2d\u63a8\u65ad\u9897\u7c92\u6750\u6599\u7684\u5c5e\u6027\uff0c\u7279\u522b\u5173\u6ce8\u5728\u65e0\u6cd5\u6216\u4e0d\u4fbf\u76f4\u63a5\u6d4b\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u6709\u6548\u8868\u5f81\u8fd9\u4e9b\u6750\u6599\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u6574\u5408\u4e86\u795e\u7ecf\u8f90\u5c04\u573a\uff08NeRF\uff09\u548c\u7269\u8d28\u70b9\u65b9\u6cd5\uff08MPM\uff09\u6a21\u62df\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u89c6\u89c9\u89c2\u6d4b\u4e2d\u63a8\u65ad\u9897\u7c92\u6750\u6599\u5c5e\u6027\u3002\u9996\u5148\u751f\u6210\u5408\u6210\u5b9e\u9a8c\u6570\u636e\uff0c\u6a21\u62df\u7281\u4e0e\u6c99\u5b50\u7684\u4ea4\u4e92\u4f5c\u7528\uff0c\u5e76\u5c06\u5b9e\u9a8c\u6e32\u67d3\u6210\u903c\u771f\u7684\u56fe\u50cf\u4f5c\u4e3a\u89c2\u6d4b\u3002\u5229\u7528NeRF\u4ece\u521d\u59cb\u591a\u89c6\u56fe\u56fe\u50cf\u4e2d\u91cd\u5efa\u4e09\u7ef4\u51e0\u4f55\u7ed3\u6784\uff0c\u7136\u540e\u5c06\u91cd\u5efa\u7684\u51e0\u4f55\u7ed3\u6784\u7528\u4e8eMPM\u6a21\u62df\u4e2d\u7269\u8d28\u70b9\u7684\u521d\u59cb\u5316\uff0c\u5176\u4e2d\u6469\u64e6\u89d2\u662f\u672a\u77e5\u7684\u3002\u901a\u8fc7\u76f8\u540c\u7684\u76f8\u673a\u8bbe\u7f6e\u6e32\u67d3\u6a21\u62df\u56fe\u50cf\uff0c\u5e76\u4e0e\u89c2\u6d4b\u56fe\u50cf\u8fdb\u884c\u6bd4\u8f83\u3002\u6700\u540e\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u4f18\u5316\u6700\u5c0f\u5316\u56fe\u50cf\u635f\u5931\u6765\u4f30\u8ba1\u6700\u4f73\u62df\u5408\u6469\u64e6\u89d2\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u6469\u64e6\u89d2\u4f30\u8ba1\u7684\u8bef\u5dee\u63a7\u5236\u57282\u5ea6\u4ee5\u5185\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7eaf\u7cb9\u89c6\u89c9\u89c2\u5bdf\u8fdb\u884c\u9006\u5411\u5206\u6790\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u7eaf\u7cb9\u7684\u89c6\u89c9\u89c2\u5bdf\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5c06\u6469\u64e6\u89d2\u4f30\u8ba1\u7684\u8bef\u5dee\u63a7\u5236\u57282\u5ea6\u4ee5\u5185\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u7eaf\u7cb9\u89c6\u89c9\u89c2\u5bdf\u8fdb\u884c\u9006\u5411\u5206\u6790\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u4e3a\u5728\u65e0\u6cd5\u6216\u4e0d\u4fbf\u76f4\u63a5\u6d4b\u91cf\u7684\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u5f81\u9897\u7c92\u6750\u6599\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10069", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10069", "abs": "https://arxiv.org/abs/2507.10069", "authors": ["Zedong Liu", "Shenggan Cheng", "Guangming Tan", "Yang You", "Dingwen Tao"], "title": "ElasticMM: Efficient Multimodal LLMs Serving with Elastic Multimodal Parallelism", "comment": null, "summary": "Multimodal large language models (MLLMs) extend LLMs to handle images,\nvideos, and audio by incorporating feature extractors and projection modules.\nHowever, these additional components -- combined with complex inference\npipelines and heterogeneous workloads -- introduce significant inference\noverhead. Therefore, efficiently serving MLLMs remains a major challenge.\nCurrent tightly coupled serving architectures struggle to distinguish between\nmixed request types or adapt parallelism strategies to different inference\nstages, leading to increased time-to-first-token (TTFT) latency and poor\nresource utilization. To address this, we propose Elastic Multimodal\nParallelism (EMP), a new serving paradigm that elastically adapts to resource\nheterogeneity across request types and inference stages. Building upon EMP, we\ndevelop ElasticMM, an MLLM serving system that (1) separates requests into\nindependent modality groups with dynamic resource allocation via a\nmodality-aware load balancer; (2) decouples inference stages and enables\nparallelism adjustment and adaptive scaling via elastic partition scheduling;\nand (3) improves inference efficiency through unified multimodal prefix caching\nand non-blocking encoding. Experiments on diverse real-world datasets show that\nElasticMM outperforms state-of-the-art (SOTA) serving systems, reducing TTFT by\nup to 4.2x and achieving 3.2-4.5x higher throughput while meeting service-level\nobjectives (SLOs).", "AI": {"tldr": "ElasticMM\u901a\u8fc7\u5f39\u6027\u591a\u6a21\u6001\u5e76\u884c\u4f18\u5316\u4e86MLLM\u670d\u52a1\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002", "motivation": "\u73b0\u6709\u7684MLLM\u670d\u52a1\u67b6\u6784\u8026\u5408\u7d27\u5bc6\uff0c\u96be\u4ee5\u533a\u5206\u6df7\u5408\u8bf7\u6c42\u7c7b\u578b\u6216\u4e3a\u4e0d\u540c\u63a8\u7406\u9636\u6bb5\u9002\u914d\u5e76\u884c\u7b56\u7565\uff0c\u5bfc\u81f4\u9996\u8bcd\u5ef6\u8fdf\uff08TTFT\uff09\u589e\u52a0\u548c\u8d44\u6e90\u5229\u7528\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5f39\u6027\u591a\u6a21\u6001\u5e76\u884c\uff08EMP\uff09\u7684\u65b0\u670d\u52a1\u8303\u5f0f\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86ElasticMM\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5305\u542b\u6a21\u6001\u611f\u77e5\u8d1f\u8f7d\u5747\u8861\u5668\u3001\u5f39\u6027\u5206\u533a\u8c03\u5ea6\u4ee5\u53ca\u7edf\u4e00\u7684\u591a\u6a21\u6001\u524d\u7f00\u7f13\u5b58\u548c\u975e\u963b\u585e\u7f16\u7801\u7b49\u5173\u952e\u6280\u672f\u3002", "result": "ElasticMM\u7cfb\u7edf\u76f8\u6bd4\u4e8e\u73b0\u6709\u7684\u5148\u8fdb\u670d\u52a1\u7cfb\u7edf\uff0c\u5728TTFT\u65b9\u9762\u964d\u4f4e\u4e864.2\u500d\uff0c\u541e\u5410\u91cf\u63d0\u9ad8\u4e863.2-4.5\u500d\uff0c\u540c\u65f6\u6ee1\u8db3\u670d\u52a1\u6c34\u5e73\u76ee\u6807\uff08SLOs\uff09\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 ElasticMM \u7cfb\u7edf\u901a\u8fc7\u5206\u79bb\u8bf7\u6c42\u3001\u89e3\u8026\u63a8\u7406\u9636\u6bb5\u4ee5\u53ca\u7edf\u4e00\u524d\u7f00\u7f13\u5b58\u7b49\u6280\u672f\uff0c\u5728MLLM\u670d\u52a1\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7a81\u7834\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u541e\u5410\u91cf\u3002"}}
{"id": "2507.08955", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.08955", "abs": "https://arxiv.org/abs/2507.08955", "authors": ["Rui-Hao Li", "Hakan Doga", "Bryan Raubenolt", "Sarah Mostame", "Nicholas DiSanto", "Fabio Cumbo", "Jayadev Joshi", "Hanna Linn", "Maeve Gaffney", "Alexander Holden", "Vinooth Kulkarni", "Vipin Chaudhary", "Kenneth M. Merz Jr", "Abdullah Ash Saki", "Tomas Radivoyevitch", "Frank DiFilippo", "Jun Qin", "Omar Shehab", "Daniel Blankenberg"], "title": "Quantum Algorithm for Protein Structure Prediction Using the Face-Centered Cubic Lattice", "comment": null, "summary": "In this work, we present the first implementation of the face-centered cubic\n(FCC) lattice model for protein structure prediction with a quantum algorithm.\nOur motivation to encode the FCC lattice stems from our observation that the\nFCC lattice is more capable in terms of modeling realistic secondary structures\nin proteins compared to other lattices, as demonstrated using root mean square\ndeviation (RMSD). We utilize two quantum methods to solve this problem: a\npolynomial fitting approach (PolyFit) and the Variational Quantum Eigensolver\nwith constraints (VQEC) based on the Lagrangian duality principle. Both methods\nare successfully deployed on Eagle R3 (ibm_cleveland) and Heron R2\n(ibm_kingston) quantum computers, where we are able to recover ground state\nconfigurations for the 6-amino acid sequence KLVFFA under noise. A comparative\nanalysis of the outcomes generated by the two QPUs reveals a significant\nenhancement (reaching nearly a two-fold improvement for PolyFit and a\nthree-fold improvement for VQEC) in the prediction and sampling of the optimal\nsolution (ground state conformations) on the newer Heron R2 architecture,\nhighlighting the impact of quantum hardware advancements for this application.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u4f7f\u7528\u91cf\u5b50\u7b97\u6cd5\uff08PolyFit\u548cVQEC\uff09\u5728FCC\u70b9\u9635\u6a21\u578b\u4e0a\u9884\u6d4b\u86cb\u767d\u8d28\u7ed3\u6784\uff0c\u5e76\u5728\u66f4\u5148\u8fdb\u7684Heron R2\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7814\u7a76\u7684\u52a8\u673a\u6e90\u4e8e\u89c2\u5bdf\u5230\u9762\u5fc3\u7acb\u65b9\uff08FCC\uff09\u70b9\u9635\u5728\u6a21\u62df\u86cb\u767d\u8d28\u771f\u5b9e\u4e8c\u7ea7\u7ed3\u6784\u65b9\u9762\u6bd4\u5176\u4ed6\u70b9\u9635\u5177\u6709\u66f4\u5f3a\u7684\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u5747\u65b9\u6839\u504f\u5dee\uff08RMSD\uff09\u8fdb\u884c\u4e86\u8bba\u8bc1\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e24\u79cd\u91cf\u5b50\u7b97\u6cd5\u6765\u9884\u6d4b\u86cb\u767d\u8d28\u7ed3\u6784\uff1a\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u539f\u7406\u7684\u591a\u9879\u5f0f\u62df\u5408\uff08PolyFit\uff09\u548c\u53d8\u5206\u91cf\u5b50\u7279\u5f81\u6c42\u89e3\u5668\uff08VQEC\uff09\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u88ab\u6210\u529f\u90e8\u7f72\u5728IBM\u7684Eagle R3\u548cHeron R2\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\uff0c\u4ee5\u89e3\u51b3\u9762\u5fc3\u7acb\u65b9\uff08FCC\uff09\u70b9\u9635\u6a21\u578b\u5728\u86cb\u767d\u8d28\u7ed3\u6784\u9884\u6d4b\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u7814\u7a76\u6210\u529f\u5730\u5728Eagle R3\u548cHeron R2\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u6062\u590d\u4e866\u4e2a\u6c28\u57fa\u9178\u5e8f\u5217KLVFFA\u5728\u566a\u58f0\u4e0b\u7684\u57fa\u6001\u6784\u8c61\u3002\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\uff0c\u53d1\u73b0\u5728Heron R2\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\uff0cPolyFit\u548cVQEC\u5728\u9884\u6d4b\u548c\u91c7\u6837\u6700\u4f18\u89e3\u65b9\u9762\u76f8\u6bd4Eagle R3\u5206\u522b\u53d6\u5f97\u4e86\u8fd1\u4e24\u500d\u548c\u8fd1\u4e09\u500d\u7684\u63d0\u5347\uff0c\u8fd9\u8868\u660e\u91cf\u5b50\u786c\u4ef6\u7684\u8fdb\u6b65\u5bf9\u8be5\u5e94\u7528\u4ea7\u751f\u4e86\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5b9e\u73b0\u4e86\u4f7f\u7528\u91cf\u5b50\u7b97\u6cd5\u9884\u6d4b\u86cb\u767d\u8d28\u7ed3\u6784\u7684\u9762\u5fc3\u7acb\u65b9\uff08FCC\uff09\u70b9\u9635\u6a21\u578b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5176\u5b83\u70b9\u9635\u76f8\u6bd4\uff0cFCC\u70b9\u9635\u5728\u6a21\u62df\u86cb\u767d\u8d28\u771f\u5b9e\u4e8c\u7ea7\u7ed3\u6784\u65b9\u9762\u5177\u6709\u66f4\u5f3a\u7684\u80fd\u529b\u3002\u7814\u7a76\u8fd8\u5bf9\u6bd4\u4e86\u591a\u9879\u5f0f\u62df\u5408\uff08PolyFit\uff09\u548c\u57fa\u4e8e\u62c9\u683c\u6717\u65e5\u5bf9\u5076\u539f\u7406\u7684\u53d8\u5206\u91cf\u5b50\u7279\u5f81\u6c42\u89e3\u5668\uff08VQEC\uff09\u8fd9\u4e24\u79cd\u91cf\u5b50\u65b9\u6cd5\uff0c\u5e76\u5c06\u5b83\u4eec\u6210\u529f\u90e8\u7f72\u5728IBM\u7684Eagle R3\u548cHeron R2\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\uff0c\u80fd\u591f\u5728\u566a\u58f0\u4e0b\u6062\u590d6\u4e2a\u6c28\u57fa\u9178\u5e8f\u5217KLVFFA\u7684\u57fa\u6001\u6784\u8c61\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e0eEagle R3\u76f8\u6bd4\uff0cHeron R2\u5728\u9884\u6d4b\u548c\u91c7\u6837\u6700\u4f18\u89e3\uff08\u57fa\u6001\u6784\u8c61\uff09\u65b9\u9762\u8868\u73b0\u51fa\u663e\u8457\u63d0\u5347\uff08PolyFit\u63a5\u8fd1\u4e24\u500d\uff0cVQEC\u63a5\u8fd1\u4e09\u500d\uff09\uff0c\u51f8\u663e\u4e86\u91cf\u5b50\u786c\u4ef6\u8fdb\u6b65\u5bf9\u8be5\u5e94\u7528\u7684\u63a8\u52a8\u4f5c\u7528\u3002"}}
{"id": "2507.08967", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08967", "abs": "https://arxiv.org/abs/2507.08967", "authors": ["Rongyi Zhu", "Yuhui Wang", "Tanqiu Jiang", "Jiacheng Liang", "Ting Wang"], "title": "Self-Improving Model Steering", "comment": "16 pages, 9 figures", "summary": "Model steering represents a powerful technique that dynamically aligns large\nlanguage models (LLMs) with human preferences during inference. However,\nconventional model-steering methods rely heavily on externally annotated data,\nnot only limiting their adaptability to varying contexts but also tethering\ntheir effectiveness to annotation quality. In this paper, we present SIMS, the\nfirst self-improving model-steering framework that operates without relying on\nexternal supervision. At its core, SIMS autonomously generates and refines\ncontrastive samples through iterative self-improvement cycles, enabling\nadaptive, context-specific steering. Additionally, SIMS employs novel\nstrategies, including prompt ranking and contrast sampling, to further enhance\nsteering efficacy. Extensive evaluation across diverse LLMs and benchmarks\ndemonstrates that SIMS substantially outperforms existing methods in steering\neffectiveness and adaptability, highlighting self-improving model steering as a\npromising direction for future research on inference-time LLM alignment.", "AI": {"tldr": "SIMS\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6a21\u578b\u8f6c\u5411\u6846\u67b6\uff0c\u5b83\u4e0d\u4f9d\u8d56\u5916\u90e8\u6ce8\u91ca\u6570\u636e\uff0c\u800c\u662f\u901a\u8fc7\u81ea\u6211\u6539\u8fdb\u6765\u751f\u6210\u548c\u4f18\u5316\u5bf9\u6bd4\u6837\u672c\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u6709\u6548\u7684\u9002\u5e94\u6027\u8f6c\u5411\u3002", "motivation": "\u4f20\u7edf\u7684\u6a21\u578b\u8f6c\u5411\u65b9\u6cd5\u4e25\u91cd\u4f9d\u8d56\u5916\u90e8\u6ce8\u91ca\u6570\u636e\uff0c\u8fd9\u4e0d\u4ec5\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u800c\u4e14\u5176\u6709\u6548\u6027\u4e5f\u53d7\u5230\u6ce8\u91ca\u8d28\u91cf\u7684\u9650\u5236\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u65b0\u578b\u6a21\u578b\u8f6c\u5411\u65b9\u6cd5\u3002", "method": "SIMS\u6846\u67b6\u7684\u6838\u5fc3\u5728\u4e8e\u901a\u8fc7\u8fed\u4ee3\u5f0f\u81ea\u6211\u6539\u8fdb\u5468\u671f\u81ea\u4e3b\u751f\u6210\u548c\u4f18\u5316\u5bf9\u6bd4\u6837\u672c\uff0c\u4ee5\u5b9e\u73b0\u81ea\u9002\u5e94\u3001\u7279\u5b9a\u4e8e\u4e0a\u4e0b\u6587\u7684\u8f6c\u5411\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u91c7\u7528\u4e86\u63d0\u793a\u6392\u5e8f\u548c\u5bf9\u6bd4\u91c7\u6837\u7b49\u65b0\u9896\u7b56\u7565\u6765\u589e\u5f3a\u8f6c\u5411\u6548\u679c\u3002", "result": "\u5728\u9488\u5bf9\u4e0d\u540cLLM\u548c\u57fa\u51c6\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0cSIMS\u5728\u8f6c\u5411\u6548\u679c\u548c\u9002\u5e94\u6027\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "SIMS\u4f5c\u4e3a\u4e00\u4e2a\u9996\u4e2a\u65e0\u9700\u5916\u90e8\u76d1\u7763\u7684\u81ea\u6539\u8fdb\u6a21\u578b\u8f6c\u5411\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u5f0f\u81ea\u6211\u6539\u8fdb\u5468\u671f\u81ea\u4e3b\u751f\u6210\u548c\u4f18\u5316\u5bf9\u6bd4\u6837\u672c\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u3001\u7279\u5b9a\u4e8e\u4e0a\u4e0b\u6587\u7684\u8f6c\u5411\u3002\u6b64\u5916\uff0cSIMS\u91c7\u7528\u65b0\u9896\u7684\u63d0\u793a\u6392\u5e8f\u548c\u5bf9\u6bd4\u91c7\u6837\u7b56\u7565\uff0c\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u8f6c\u5411\u6548\u679c\u3002\u5927\u91cf\u9488\u5bf9\u4e0d\u540cLLM\u548c\u57fa\u51c6\u7684\u8bc4\u4f30\u8868\u660e\uff0cSIMS\u5728\u8f6c\u5411\u6548\u679c\u548c\u9002\u5e94\u6027\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7a81\u663e\u4e86\u81ea\u6211\u6539\u8fdb\u6a21\u578b\u8f6c\u5411\u4f5c\u4e3a\u672a\u6765\u63a8\u7406\u65f6LLM\u5bf9\u9f50\u7814\u7a76\u7684\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.09473", "categories": ["cs.GT", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09473", "abs": "https://arxiv.org/abs/2507.09473", "authors": ["Yan Dai", "Negin Golrezaei", "Patrick Jaillet"], "title": "Incentive-Aware Dynamic Resource Allocation under Long-Term Cost Constraints", "comment": null, "summary": "Motivated by applications such as cloud platforms allocating GPUs to users or\ngovernments deploying mobile health units across competing regions, we study\nthe dynamic allocation of a reusable resource to strategic agents with private\nvaluations. Our objective is to simultaneously (i) maximize social welfare,\n(ii) satisfy multi-dimensional long-term cost constraints, and (iii)\nincentivize truthful reporting. We begin by numerically evaluating primal-dual\nmethods widely used in constrained online optimization and find them to be\nhighly fragile in strategic settings -- agents can easily manipulate their\nreports to distort future dual updates for future gain.\n  To address this vulnerability, we develop an incentive-aware framework that\nmakes primal-dual methods robust to strategic behavior. Our design combines\nepoch-based lazy updates -- where dual variables remain fixed within each epoch\n-- with randomized exploration rounds that extract approximately truthful\nsignals for learning. Leveraging carefully designed online learning subroutines\nthat can be of independent interest for dual updates, our mechanism achieves\n$\\tilde{\\mathcal{O}}(\\sqrt{T})$ social welfare regret, satisfies all cost\nconstraints, and ensures incentive alignment. This matches the performance of\nnon-strategic allocation approaches while being robust to strategic agents.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5ef6\u8fdf\u66f4\u65b0\u548c\u968f\u673a\u63a2\u7d22\u4f7f\u539f\u59cb\u5bf9\u5076\u65b9\u6cd5\u80fd\u591f\u7a33\u5065\u5730\u5904\u7406\u5177\u6709\u79c1\u4eba\u4f30\u503c\u7684\u6218\u7565\u4ee3\u7406\u7684\u52a8\u6001\u8d44\u6e90\u5206\u914d\u95ee\u9898\uff0c\u5728\u5b9e\u73b0\u6700\u5927\u5316\u793e\u4f1a\u798f\u5229\u7684\u540c\u65f6\u6ee1\u8db3\u6210\u672c\u7ea6\u675f\u5e76\u6fc0\u52b1\u8bda\u5b9e\u62a5\u544a\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u539f\u59cb\u5bf9\u5076\u65b9\u6cd5\u5728\u7b56\u7565\u6027\u8bbe\u7f6e\u4e2d\u7684\u8106\u5f31\u6027\uff0c\u5373\u4ee3\u7406\u5546\u53ef\u4ee5\u901a\u8fc7\u64cd\u7eb5\u62a5\u544a\u6765\u626d\u66f2\u672a\u6765\u7684\u5bf9\u5076\u66f4\u65b0\u4ee5\u83b7\u53d6\u672a\u6765\u6536\u76ca\u3002", "method": "\u7ed3\u5408\u57fa\u4e8e\u65f6\u671f\uff08epoch-based\uff09\u7684\u5ef6\u8fdf\u66f4\u65b0\uff08\u5176\u4e2d\u5bf9\u5076\u53d8\u91cf\u5728\u6bcf\u4e2a\u65f6\u671f\u5185\u4fdd\u6301\u56fa\u5b9a\uff09\u548c\u968f\u673a\u63a2\u7d22\u8f6e\u6b21\uff08\u63d0\u53d6\u8fd1\u4f3c\u771f\u5b9e\u4fe1\u53f7\u7528\u4e8e\u5b66\u4e60\uff09\uff0c\u5e76\u5229\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5728\u7ebf\u5b66\u4e60\u5b50\u7a0b\u5e8f\u3002", "result": "\u8be5\u673a\u5236\u5b9e\u73b0\u4e86 Tilde{O}(sqrt{T}) \u7684\u793e\u4f1a\u798f\u5229\u9057\u61be\uff08social welfare regret\uff09\uff0c\u6ee1\u8db3\u4e86\u6240\u6709\u6210\u672c\u7ea6\u675f\uff0c\u5e76\u786e\u4fdd\u4e86\u6fc0\u52b1\u4e00\u81f4\u6027\u3002", "conclusion": "\u8be5\u673a\u5236\u5728\u6ee1\u8db3\u6210\u672c\u7ea6\u675f\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u4e0e\u975e\u7b56\u7565\u6027\u5206\u914d\u65b9\u6cd5\u76f8\u5339\u914d\u7684\u793e\u4f1a\u798f\u5229\uff0c\u5e76\u4e14\u80fd\u591f\u5e94\u5bf9\u7b56\u7565\u6027\u4ee3\u7406\u3002"}}
{"id": "2507.09427", "categories": ["cs.LO", "math.LO"], "pdf": "https://arxiv.org/pdf/2507.09427", "abs": "https://arxiv.org/abs/2507.09427", "authors": ["Sonia Marin", "Paaras Padhiar"], "title": "Justification Logic for Intuitionistic Modal Logic (Extended Technical Report)", "comment": null, "summary": "Justification logics are an explication of modal logic; boxes are replaced\nwith proof terms formally through realisation theorems. This can be achieved\nsyntactically using a cut-free proof system e.g. using sequent, hypersequent or\nnested sequent calculi. In constructive modal logic, boxes and diamonds are\ndecoupled and not De Morgan dual. Kuznets, Marin and Stra{\\ss}burger provide a\njustification counterpart to constructive modal logic CK and some extensions by\nmaking diamonds explicit by introducing new terms called satisfiers. We\ncontinue the line of work to provide a justification counterpart to Fischer\nServi's intuitionistic modal logic IK and its extensions with the t and 4\naxioms. We: extend the syntax of proof terms to accommodate the additional\naxioms of intuitionistic modal logic; provide an axiomatisation of these\njustification logics; provide a syntactic realisation procedure using a\ncut-free nested sequent system for intuitionistic modal logic introduced by\nStra{\\ss}burger.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u76f4\u89c9\u4e3b\u4e49\u6a21\u6001\u903b\u8f91 IK \u63d0\u4f9b\u4e86\u4e00\u4e2a\u8bc1\u660e\u8bba\u63a8\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u6269\u5c55\u8bc1\u660e\u9879\u53e5\u6cd5\u3001\u63d0\u4f9b\u516c\u7406\u5316\u4f53\u7cfb\u548c\u5b9e\u73b0\u53e5\u6cd5\u5b9e\u73b0\u8fc7\u7a0b\u6765\u5b8c\u6210\u3002", "motivation": "\u7ee7 Kuznets\u3001Marin \u548c Stra{\\ss}burger \u7684\u5de5\u4f5c\u4e4b\u540e\uff0c\u6211\u4eec\u65e8\u5728\u4e3a Fischer Servi \u7684\u76f4\u89c9\u4e3b\u4e49\u6a21\u6001\u903b\u8f91 IK \u53ca\u5176\u6269\u5c55\u63d0\u4f9b\u4e00\u4e2a\u8bc1\u660e\u8bba\u63a8\u7406\u6846\u67b6\uff0c\u5176\u4e2d\u6a21\u6001\u7b97\u5b50\u88ab\u663e\u5f0f\u7684\u8bc1\u660e\u9879\u6240\u53d6\u4ee3\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u8bc1\u660e\u9879\uff08\u6ee1\u8db3\u9879\uff09\u6765\u6269\u5c55\u8bc1\u660e\u9879\u7684\u53e5\u6cd5\uff0c\u4ee5\u9002\u5e94\u76f4\u89c9\u4e3b\u4e49\u6a21\u6001\u903b\u8f91\u7684\u9644\u52a0\u516c\u7406\u3002\u6211\u4eec\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u516c\u7406\u5316\u4f53\u7cfb\uff0c\u5e76\u4f7f\u7528 Stra{\\ss}burger \u5f15\u5165\u7684\u7528\u4e8e\u76f4\u89c9\u4e3b\u4e49\u6a21\u6001\u903b\u8f91\u7684\u65e0 \u0915\u091f \u7684\u5d4c\u5957\u5e8f\u5217\u7cfb\u7edf\u6765\u5b9e\u73b0\u53e5\u6cd5\u5b9e\u73b0\u8fc7\u7a0b\u3002", "result": "\u6211\u4eec\u6210\u529f\u5730\u4e3a\u76f4\u89c9\u4e3b\u4e49\u6a21\u6001\u903b\u8f91 IK \u53ca\u5176\u6269\u5c55\u63d0\u4f9b\u4e86\u8bc1\u660e\u8bba\u63a8\u7406\u6846\u67b6\uff0c\u6269\u5c55\u4e86\u8bc1\u660e\u9879\u7684\u53e5\u6cd5\uff0c\u7ed9\u51fa\u4e86\u516c\u7406\u5316\u4f53\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u53e5\u6cd5\u5b9e\u73b0\u8fc7\u7a0b\u3002", "conclusion": "\u6211\u4eec\u4e3a Fischer Servi \u7684\u76f4\u89c9\u4e3b\u4e49\u6a21\u6001\u903b\u8f91 IK \u53ca\u5176\u5e26\u6709 t \u548c 4 \u516c\u7406\u7684\u6269\u5c55\u63d0\u4f9b\u4e86\u5bf9\u5e94\u7684\u8bc1\u660e\u8bba\u63a8\u7406\u6846\u67b6\u3002"}}
{"id": "2507.09215", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09215", "abs": "https://arxiv.org/abs/2507.09215", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Kun Yang"], "title": "Time-Varying Offset Estimation for Clock-Asynchronous Bistatic ISAC Systems", "comment": null, "summary": "The bistatic Integrated Sensing and Communication (ISAC) is poised to become\na key application for next generation communication networks (e.g., B5G/6G),\nproviding simultaneous sensing and communication services with minimal changes\nto existing network infrastructure and hardware. However, a significant\nchallenge in bistatic cooperative sensing is clock asynchronism, arising from\nthe use of different clocks at far separated transmitters and receivers. This\nasynchrony leads to Timing Offsets (TOs) and Carrier Frequency Offsets (CFOs),\npotentially causing sensing ambiguity. Traditional synchronization methods\ntypically rely on static reference links or GNSS-based timing sources, both of\nwhich are often unreliable or unavailable in UAVbased bistatic ISAC scenarios.\nTo overcome these limitations, we propose a Time-Varying Offset Estimation\n(TVOE) framework tailored for clock-asynchronous bistatic ISAC systems, which\nleverages the geometrically predictable characteristics of the Line-of-Sight\n(LoS) path to enable robust, infrastructure-free\n  synchronization. The framework treats the LoS delay and the Doppler shift as\ndynamic observations and models their evolution as a hidden stochastic process.\nA state-space formulation is developed to jointly estimate TO and CFO via an\nExtended Kalman Filter (EKF), enabling real-time tracking of clock offsets\nacross successive frames. Furthermore, the estimated offsets are subsequently\napplied to correct the timing misalignment of all Non-Line-of-Sight (NLoS)\ncomponents, thereby enhancing the high-resolution target sensing performance.\nExtensive simulation results demonstrate that the proposed TVOE method improves\nthe estimation accuracy by 60%.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u65e0\u4eba\u673a\u53cc\u7ad9ISAC\u4e2d\u7684\u65f6\u949f\u5f02\u6b65\u95ee\u9898\uff0c\u63d0\u51faTVOE\u6846\u67b6\uff0c\u5229\u7528\u89c6\u8ddd\u8def\u5f84\u7279\u6027\u8fdb\u884c\u540c\u6b65\uff0c\u901a\u8fc7EKF\u4f30\u8ba1\u548c\u8865\u507f\u65f6\u949f\u504f\u79fb\uff0c\u63d0\u9ad8\u4f20\u611f\u7cbe\u5ea660%\u4ee5\u4e0a\u3002", "motivation": "\u53cc\u7ad9ISAC\u7cfb\u7edf\u5728\u540c\u6b65\u65b9\u9762\u9762\u4e34\u65f6\u949f\u5f02\u6b65\u7684\u6311\u6218\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u65f6\u95f4\u548c\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\uff08TO\u548cCFO\uff09\uff0c\u5f71\u54cd\u4f20\u611f\u7cbe\u5ea6\u3002\u4f20\u7edf\u7684\u540c\u6b65\u65b9\u6cd5\uff08\u5982\u4f9d\u8d56\u9759\u6001\u53c2\u8003\u94fe\u8def\u6216GNSS\uff09\u5728\u65e0\u4eba\u673a\uff08UAV\uff09\u90e8\u7f72\u7684\u53cc\u7ad9ISAC\u573a\u666f\u4e2d\u901a\u5e38\u4e0d\u53ef\u9760\u6216\u4e0d\u53ef\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65f6\u53d8\u504f\u79fb\u4f30\u8ba1\uff08TVOE\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65f6\u949f\u5f02\u6b65\u53cc\u7ad9ISAC\u7cfb\u7edf\u4e2d\u7684\u65f6\u949f\u5f02\u6b65\u95ee\u9898\u3002\u8be5\u6846\u67b6\u5c06LoS\u5ef6\u8fdf\u548c\u591a\u666e\u52d2\u9891\u79fb\u4f5c\u4e3a\u52a8\u6001\u89c2\u6d4b\u503c\uff0c\u5e76\u5c06\u5176\u6f14\u5316\u5efa\u6a21\u4e3a\u9690\u85cf\u7684\u968f\u673a\u8fc7\u7a0b\u3002\u901a\u8fc7\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u7684\u72b6\u6001\u7a7a\u95f4\u516c\u5f0f\u6765\u8054\u5408\u4f30\u8ba1\u65f6\u95f4\u548c\u8f7d\u6ce2\u9891\u7387\u504f\u79fb\uff08TO\u548cCFO\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684TVOE\u65b9\u6cd5\u5c06\u4f30\u8ba1\u7cbe\u5ea6\u63d0\u9ad8\u4e8660%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5229\u7528\u89c6\u8ddd\uff08LoS\uff09\u8def\u5f84\u7684\u51e0\u4f55\u53ef\u9884\u6d4b\u7279\u6027\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u3001\u65e0\u9700\u57fa\u7840\u8bbe\u65bd\u7684\u540c\u6b65\uff0c\u5e76\u80fd\u5b9e\u65f6\u8ddf\u8e2a\u8fde\u7eed\u5e27\u4e4b\u95f4\u7684\u65f6\u949f\u504f\u79fb\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u9ad8\u5206\u8fa8\u7387\u76ee\u6807\u4f20\u611f\u6027\u80fd\u3002"}}
{"id": "2507.09409", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09409", "abs": "https://arxiv.org/abs/2507.09409", "authors": ["Lance Ying", "Ryan Truong", "Joshua B. Tenenbaum", "Samuel J. Gershman"], "title": "Adaptive Social Learning using Theory of Mind", "comment": "7 pages, 4 figure; paper published at CogSci 2025", "summary": "Social learning is a powerful mechanism through which agents learn about the\nworld from others. However, humans don't always choose to observe others, since\nsocial learning can carry time and cognitive resource costs. How do people\nbalance social and non-social learning? In this paper, we propose a rational\nmentalizing model of the decision to engage in social learning. This model\nestimates the utility of social learning by reasoning about the other agent's\ngoal and the informativity of their future actions. It then weighs the utility\nof social learning against the utility of self-exploration (non-social\nlearning). Using a multi-player treasure hunt game, we show that our model can\nquantitatively capture human trade-offs between social and non-social learning.\nFurthermore, our results indicate that these two components allow agents to\nflexibly apply social learning to achieve their goals more efficiently.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u6027\u7684\u5fc3\u7406\u5316\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u91ca\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4eba\u7c7b\u5982\u4f55\u5728\u793e\u4f1a\u5b66\u4e60\u548c\u975e\u793e\u4f1a\u5b66\u4e60\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\u3002\u901a\u8fc7\u591a\u4eba\u5bfb\u5b9d\u6e38\u620f\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u548c\u89e3\u91ca\u4eba\u7c7b\u7684\u884c\u4e3a\uff0c\u5e76\u8bf4\u660e\u4e86\u793e\u4f1a\u5b66\u4e60\u5728\u5b9e\u73b0\u76ee\u6807\u8fc7\u7a0b\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u6548\u7387\u3002", "motivation": "\u63a2\u7a76\u4eba\u7c7b\u5982\u4f55\u5728\u793e\u4f1a\u5b66\u4e60\u53ef\u80fd\u5e26\u6765\u65f6\u95f4\u548c\u8ba4\u77e5\u8d44\u6e90\u6210\u672c\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u8861\u793e\u4f1a\u5b66\u4e60\u548c\u975e\u793e\u4f1a\u5b66\u4e60\u3002\u5e76\u63d0\u51fa\u4e00\u4e2a\u7406\u6027\u7684\u5fc3\u7406\u5316\u6a21\u578b\u6765\u89e3\u91ca\u8fd9\u79cd\u5e73\u8861\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7406\u6027\u7684\u5fc3\u7406\u5316\u6a21\u578b\u6765\u4f30\u8ba1\u793e\u4f1a\u5b66\u4e60\u7684\u6548\u7528\uff0c\u901a\u8fc7\u63a8\u7406\u5176\u4ed6\u667a\u80fd\u4f53\u7684\u76ee\u6807\u548c\u4ed6\u4eec\u672a\u6765\u884c\u52a8\u7684\u4fe1\u606f\u91cf\u3002\u7136\u540e\u6743\u8861\u793e\u4f1a\u5b66\u4e60\u7684\u6548\u7528\u548c\u81ea\u6211\u63a2\u7d22\uff08\u975e\u793e\u4f1a\u5b66\u4e60\uff09\u7684\u6548\u7528\u3002", "result": "\u5728\u591a\u4eba\u5bfb\u5b9d\u6e38\u620f\u4e2d\uff0c\u6a21\u578b\u80fd\u591f\u91cf\u5316\u4eba\u7c7b\u5728\u793e\u4f1a\u5b66\u4e60\u548c\u975e\u793e\u4f1a\u5b66\u4e60\u4e4b\u95f4\u7684\u6743\u8861\u3002\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u4e24\u79cd\u6210\u5206\u80fd\u8ba9\u667a\u80fd\u4f53\u7075\u6d3b\u5730\u5e94\u7528\u793e\u4f1a\u5b66\u4e60\u4ee5\u66f4\u6709\u6548\u5730\u5b9e\u73b0\u5176\u76ee\u6807\u3002", "conclusion": "\u8be5\u6a21\u578b\u53ef\u4ee5\u91cf\u5316\u4eba\u7c7b\u5728\u793e\u4f1a\u5b66\u4e60\u548c\u975e\u793e\u4f1a\u5b66\u4e60\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u5e76\u4e14\u8fd9\u4e24\u79cd\u6210\u5206\u80fd\u8ba9\u667a\u80fd\u4f53\u7075\u6d3b\u5730\u5e94\u7528\u793e\u4f1a\u5b66\u4e60\u4ee5\u66f4\u6709\u6548\u5730\u5b9e\u73b0\u5176\u76ee\u6807\u3002"}}
{"id": "2507.08834", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08834", "abs": "https://arxiv.org/abs/2507.08834", "authors": ["Karishma Battina", "Prathamesh Dinesh Joshi", "Raj Abhijit Dandekar", "Rajat Dandekar", "Sreedath Panat"], "title": "Physical Informed Neural Networks for modeling ocean pollutant", "comment": "13 pages, 9 figures, 3 tables", "summary": "Traditional numerical methods often struggle with the complexity and scale of\nmodeling pollutant transport across vast and dynamic oceanic domains. This\npaper introduces a Physics-Informed Neural Network (PINN) framework to simulate\nthe dispersion of pollutants governed by the 2D advection-diffusion equation.\nThe model achieves physically consistent predictions by embedding physical laws\nand fitting to noisy synthetic data, generated via a finite difference method\n(FDM), directly into the neural network training process. This approach\naddresses challenges such as non-linear dynamics and the enforcement of\nboundary and initial conditions. Synthetic data sets, augmented with varying\nnoise levels, are used to capture real-world variability. The training\nincorporates a hybrid loss function including PDE residuals, boundary/initial\ncondition conformity, and a weighted data fit term. The approach takes\nadvantage of the Julia language scientific computing ecosystem for\nhigh-performance simulations, offering a scalable and flexible alternative to\ntraditional solvers", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePINN\u7684\u6d77\u6d0b\u6c61\u67d3\u7269\u6269\u6563\u6a21\u62df\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u5728\u6a21\u62df\u5927\u5c3a\u5ea6\u3001\u590d\u6742\u6d77\u6d0b\u533a\u57df\u7684\u6c61\u67d3\u7269\u6269\u6563\u65f6\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINN\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u5b9a\u5f8b\u548c\u5e26\u566a\u58f0\u7684\u5408\u6210\u6570\u636e\u5d4c\u5165\u5230\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6765\u6a21\u62df\u4e8c\u7ef4\u5e73\u6d41\u6269\u6563\u65b9\u7a0b\u652f\u914d\u7684\u6c61\u67d3\u7269\u6269\u6563\u3002", "result": "\u8be5\u6a21\u578b\u901a\u8fc7\u5d4c\u5165\u7269\u7406\u5b9a\u5f8b\u548c\u62df\u5408\u5e26\u566a\u58f0\u7684\u5408\u6210\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u4e0a\u4e00\u81f4\u7684\u9884\u6d4b\uff0c\u5e76\u80fd\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u548c\u8fb9\u754c/\u521d\u59cb\u6761\u4ef6\u95ee\u9898\u3002", "conclusion": "PINN\u6846\u67b6\u80fd\u591f\u6709\u6548\u6a21\u62df\u6c61\u67d3\u7269\u5728\u6d77\u6d0b\u4e2d\u7684\u6269\u6563\uff0c\u5e76\u80fd\u5904\u7406\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u53ca\u8fb9\u754c\u548c\u521d\u59cb\u6761\u4ef6"}}
{"id": "2507.09465", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09465", "abs": "https://arxiv.org/abs/2507.09465", "authors": ["Xiyin Ye", "Tao Yu"], "title": "Magnon Correlation Enables Spin Injection, Dephasing, and Transport in Canted Antiferromagnets", "comment": "7 pages, 4 figures", "summary": "Thermal and electrical injection and transport of magnon spins in magnetic\ninsulators is conventionally understood by the non-equilibrium population of\nmagnons. However, this view is challenged by several recent experiments in\nnoncollinear antiferromagnets, which urge a thorough theoretical investigation\nat the fundamental level. We find that the magnon spin in antiferromagnets is\ndescribed by a matrix, so even when the diagonal terms -- spins carried by\npopulation -- vanish, the off-diagonal correlations transmit magnon spins. Our\nquantum theory shows that a net spin-flip of electrons in adjacent conductors\ncreates quantum coherence between magnon states, which transports magnon spins\nin canted antiferromagnets, even without a definite phase difference between\nmagnon modes in the incoherent process. It reveals that the pumped magnon\ncorrelation is not conserved due to an intrinsic spin torque, which causes\ndephasing and strong spatial spin oscillations during transport; both are\nenhanced by magnetic fields. Spin transfer to proximity conductors can cause\nextrinsic dephasing, which suppresses spin oscillations and thereby gates spin\ntransport.", "AI": {"tldr": "\u53cd\u94c1\u78c1\u4f53\u4e2d\u7684\u78c1\u65cb\u81ea\u65cb\u4f20\u8f93\u4e0d\u53ea\u4f9d\u8d56\u4e8e\u78c1\u65cb\u5e03\u5c45\u6570\uff0c\u8fd8\u4e0e\u975e\u5bf9\u89d2\u5173\u8054\u548c\u91cf\u5b50\u76f8\u5e72\u6027\u6709\u5173\uff0c\u5e76\u4f1a\u53d7\u5230\u81ea\u65cb\u626d\u77e9\u3001\u78c1\u573a\u548c\u5916\u5728\u9000\u76f8\u5e72\u7684\u8c03\u63a7\u3002", "motivation": "\u73b0\u6709\u7406\u8bba\u8ba4\u4e3a\u78c1 \u0627\u0644\u0645\u0631\u0633\u0644\u064a\u0646\u5c16\u5cf0\u7edd\u7f18\u4f53\u4e2d\u78c1\u65cb\u81ea\u65cb\u7684\u4f20\u8f93\u548c\u8f93\u8fd0\u662f\u57fa\u4e8e\u975e\u5e73\u8861\u78c1\u65cb\u5e03\u5c45\u6570\uff0c\u7136\u800c\uff0c\u8fd1\u671f\u7684\u5b9e\u9a8c\u7ed3\u679c\u5bf9\u8be5\u89c2\u70b9\u63d0\u51fa\u4e86\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u4f53\u4e2d\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u8005\u4ece\u6839\u672c\u4e0a\u8fdb\u884c\u7406\u8bba\u63a2\u7d22\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5b50\u7406\u8bba\u6a21\u578b\uff0c\u5176\u4e2d\u78c1\u77e9\u5728\u53cd\u94c1\u78c1\u4f53\u4e2d\u7531\u4e00\u4e2a\u77e9\u9635\u63cf\u8ff0\uff0c\u5e76\u8bc1\u660e\u4e86\u5373\u4f7f\u5728\u81ea\u65cb\u6d41\u7684\u5bf9\u89d2\u9879\u4e3a\u96f6\u7684\u60c5\u51b5\u4e0b\uff0c\u975e\u5bf9\u89d2\u5173\u8054\u4e5f\u80fd\u591f\u4f20\u8f93\u81ea\u65cb\u3002\u8be5\u6a21\u578b\u63ed\u793a\u4e86\u7535\u5b50\u6ce8\u5165\u8fc7\u7a0b\u4e2d\u4f1a\u4ea7\u751f\u4e00\u4e2a\u52a8\u91cf\u4f9d\u8d56\u7684\u81ea\u65cb\u6ce8\u5165\uff0c\u8fdb\u800c\u6fc0\u53d1\u4e86\u78c1\u6270\u52a8\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u7535\u5b50\u6ce8\u5165\u4ea7\u751f\u51c0\u81ea\u65cb\u7ffb\u8f6c\u65f6\uff0c\u4f1a\u5728\u78c1\u7edd\u7f18\u4f53\u4e2d\u6fc0\u53d1\u78c1\u65cb\u6ce2\uff0c\u8fdb\u800c\u5c06\u81ea\u65cb\u8f93\u8fd0\u5230\u76f8\u90bb\u7684\u5bfc\u4f53\u4e2d\u3002\u5728\u8be5\u8fc7\u7a0b\u4e2d\uff0c\u7531\u7535\u5b50\u6ce8\u5165\u4ea7\u751f\u7684\u52a8\u91cf\u4f9d\u8d56\u81ea\u65cb\u6ce8\u5165\u4f1a\u6fc0\u53d1\u78c1\u6270\u52a8\uff0c\u5e76\u4e0e\u78c1\u65cb\u6ce2\u53d1\u751f\u76f8\u4e92\u4f5c\u7528\uff0c\u4ece\u800c\u5c06\u81ea\u65cb\u8f93\u8fd0\u5230\u5bfc\u4f53\u4e2d\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u81ea\u65cb\u6d41\u5728\u78c1\u6027\u7edd\u7f18\u4f53\u4e2d\u4f20\u8f93\u65f6\uff0c\u975e\u5bf9\u9f50\u53cd\u94c1\u78c1\u4f53\u7684\u81ea\u65cb\u77e9\u4f1a\u53d7\u5230\u5185\u7980\u81ea\u65cb\u626d\u77e9\u7684\u8c03\u5236\uff0c\u5bfc\u81f4\u5176\u5728\u4f20\u8f93\u8fc7\u7a0b\u4e2d\u7684\u9000\u76f8\u5e72\u548c\u7a7a\u95f4\u632f\u8361\uff0c\u5e76\u4e14\u8be5\u6548\u5e94\u80fd\u591f\u88ab\u78c1\u573a\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u5916\u7980\u9000\u76f8\u5e72\u4f1a\u6291\u5236\u81ea\u65cb\u632f\u8361\uff0c\u5e76\u5bf9\u81ea\u65cb\u4f20\u8f93\u8d77\u5230\u95e8\u63a7\u4f5c\u7528\u3002"}}
{"id": "2507.09688", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.09688", "abs": "https://arxiv.org/abs/2507.09688", "authors": ["Mariia Anapolska", "Emma Ahrens", "Christina B\u00fcsing", "Felix Engelhardt", "Timo Gersing", "Corinna Mathwieser", "Sabrian Schmitz", "Sophia Wrede"], "title": "Minimum-Peak-Cost Flows Over Time", "comment": null, "summary": "When planning transportation whose operation requires non-consumable\nresources, the peak demand for allocated resources is often of higher interest\nthan the duration of resource usage. For instance, it is more cost-effective to\ndeliver parcels with a single truck over eight hours than to use two trucks for\nfour hours, as long as the time suffices. To model such scenarios, we introduce\nthe novel minimum peak cost flow over time problem, whose objective is to\nminimise the maximum cost at all points in time rather than minimising the\nintegral of costs. We focus on minimising peak costs of temporally repeated\nflows. These are desirable for practical applications due to their simple\nstructure. This yields the minimum-peak-cost Temporally Repeated flow problem\n(MPC-TRF).\n  We show that the simple structure of temporally repeated flows comes with the\ndrawback of arbitrarily bad approximation ratios compared to general flows over\ntime. Furthermore, our complexity analysis shows the integral version of\nMPC-TRF is strongly NP-hard, even under strong restrictions. On the positive\nside, we identify two benign special cases: unit-cost series-parallel networks\nand networks with time horizon at least twice as long as the longest path in\nthe network (with respect to the transit time). In both cases, we show that\nintegral optimal flows if the desired flow value equals the maximum flow value\nand fractional optimal flows for arbitrary flow values can be found in\npolynomial time. For each of these cases, we provide an explicit algorithm that\nconstructs an optimal solution.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u6700\u5c0f\u5cf0\u503c\u6210\u672c\u7684\u6682\u65f6\u91cd\u590d\u6d41\u95ee\u9898\uff08MPC-TRF\uff09\uff0c\u89e3\u51b3\u4e86\u5728\u8d44\u6e90\u89c4\u5212\u4e2d\u6700\u5c0f\u5316\u8d44\u6e90\u5cf0\u503c\u9700\u6c42\u7684\u95ee\u9898\uff0c\u5e76\u627e\u5230\u4e86\u4e24\u79cd\u95ee\u9898\u7684\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u5728\u9700\u8981\u975e\u6d88\u8017\u6027\u8d44\u6e90\u7684\u4ea4\u901a\u89c4\u5212\u4e2d\uff0c\u8d44\u6e90\u5cf0\u503c\u9700\u6c42\u6bd4\u8d44\u6e90\u4f7f\u7528\u65f6\u957f\u66f4\u91cd\u8981\uff0c\u4f8b\u5982\u4f7f\u7528\u4e00\u8f86\u5361\u8f66\u8fd0\u9001\u516b\u5c0f\u65f6\u6bd4\u4f7f\u7528\u4e24\u8f86\u5361\u8f66\u8fd0\u9001\u56db\u5c0f\u65f6\u66f4\u5177\u6210\u672c\u6548\u76ca\u3002\u4e3a\u4e86\u6a21\u62df\u8fd9\u79cd\u60c5\u51b5\uff0c\u9700\u8981\u6700\u5c0f\u5316\u6700\u5927\u6210\u672c\u800c\u4e0d\u662f\u6210\u672c\u79ef\u5206\u3002", "method": "\u63d0\u51fa\u6700\u5c0f\u5cf0\u503c\u6210\u672c\u6d41\uff08minimum peak cost flow\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u6682\u65f6\u91cd\u590d\u6d41\uff08temporally repeated flows\uff09\u4ee5\u6700\u5c0f\u5316\u5cf0\u503c\u6210\u672c\u800c\u975e\u6210\u672c\u79ef\u5206\u3002\u7814\u7a76\u8fd8\u8fdb\u884c\u4e86\u590d\u6742\u6027\u5206\u6790\uff0c\u5e76\u786e\u5b9a\u4e86\u4e24\u4e2a\u7279\u6b8a\u60c5\u51b5\uff1a\u5355\u4f4d\u6210\u672c\u4e32\u8054-\u5e76\u8054\u7f51\u7edc\u548c\u65f6\u95f4\u8303\u56f4\u8f83\u957f\uff08\u81f3\u5c11\u662f\u7f51\u7edc\u4e2d\u6700\u957f\u8def\u5f84\u7684\u4e24\u500d\uff09\u7684\u7f51\u7edc\u3002", "result": "\u5355\u4f4d\u6210\u672c\u4e32\u8054-\u5e76\u8054\u7f51\u7edc\u548c\u65f6\u95f4\u8303\u56f4\u8db3\u591f\u957f\uff08\u81f3\u5c11\u662f\u7f51\u7edc\u4e2d\u6700\u957f\u8def\u5f84\u7684\u4e24\u500d\uff09\u7684\u7f51\u7edc\u662fMPC-TRF\u7684\u7279\u6b8a\u60c5\u51b5\uff0c\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u627e\u5230\u6700\u4f18\u89e3\u3002\u5bf9\u4e8e\u5176\u4ed6\u60c5\u51b5\uff0c\u8be5\u95ee\u9898\u662fNP\u96be\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u6700\u5c0f\u5cf0\u503c\u6210\u672c\u7684\u6682\u65f6\u91cd\u590d\u6d41\u95ee\u9898\uff08MPC-TRF\uff09\uff0c\u5e76\u5206\u6790\u4e86\u5176\u590d\u6742\u6027\u3002\u7814\u7a76\u53d1\u73b0\u5728\u5355\u4f4d\u6210\u672c\u4e32\u8054-\u5e76\u8054\u7f51\u7edc\u548c\u65f6\u95f4\u8303\u56f4\u8db3\u591f\u957f\u7684\u60c5\u51b5\u4e0b\uff0cMPC-TRF\u53ef\u4ee5\u6709\u6548\u6c42\u89e3\u3002"}}
{"id": "2507.09792", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09792", "abs": "https://arxiv.org/abs/2507.09792", "authors": ["Prashant Govindarajan", "Davide Baldelli", "Jay Pathak", "Quentin Fournier", "Sarath Chandar"], "title": "CADmium: Fine-Tuning Code Language Models for Text-Driven Sequential CAD Design", "comment": null, "summary": "Computer-aided design (CAD) is the digital construction of 2D and 3D objects,\nand is central to a wide range of engineering and manufacturing applications\nlike automobile and aviation. Despite its importance, CAD modeling remains\nlargely a time-intensive, manual task. Recent works have attempted to automate\nthis process with small transformer-based models and handcrafted CAD sequence\nrepresentations. However, there has been little effort to leverage the\npotential of large language models (LLMs) for sequential CAD design. In this\nwork, we introduce a new large-scale dataset of more than 170k CAD models\nannotated with high-quality, human-like descriptions generated with our\npipeline based on GPT-4.1. Using this dataset, we fine-tune powerful code-LLMs\nto generate CAD sequences represented in a JSON-based format from natural\nlanguage descriptions, demonstrating the viability and effectiveness of this\napproach for text-conditioned CAD generation. Because simple metrics often fail\nto reflect the quality of generated objects, we introduce geometric and\ntopological metrics based on sphericity, mean curvature, and Euler\ncharacteristic to provide richer structural insights. Our experiments and\nablation studies on both synthetic and human-annotated data demonstrate that\nCADmium is able to automate CAD design, drastically speeding up the design of\nnew objects. The dataset, code, and fine-tuned models are available online.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u5305\u542b17\u4e07\u591a\u4e2aCAD\u6a21\u578b\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u73b0\u4e86\u4ece\u6587\u672c\u5230CAD\u8bbe\u8ba1\u7684\u81ea\u52a8\u5316\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u8bc4\u4f30\u6307\u6807\u6765\u8861\u91cf\u8bbe\u8ba1\u8d28\u91cf\u3002", "motivation": "\u76ee\u524d\u7684\u8ba1\u7b97\u673a\u8f85\u52a9\u8bbe\u8ba1\uff08CAD\uff09\u5efa\u6a21\u4ecd\u7136\u662f\u4e00\u4e2a\u8017\u65f6\u7684\u624b\u52a8\u8fc7\u7a0b\u3002\u5c3d\u7ba1\u8fd1\u671f\u7814\u7a76\u8bd5\u56fe\u901a\u8fc7\u5c0f\u578bTransformer\u6a21\u578b\u548c\u624b\u5de5\u5236\u4f5c\u7684CAD\u5e8f\u5217\u8868\u793a\u6765\u5b9e\u73b0\u81ea\u52a8\u5316\uff0c\u4f46\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u987a\u5e8fCAD\u8bbe\u8ba1\u7684\u6f5c\u529b\u5c1a\u672a\u88ab\u5145\u5206\u6316\u6398\u3002", "method": "\u5229\u7528\u5305\u542b170,000\u591a\u4e2aCAD\u6a21\u578b\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7GPT-4\u6d41\u6c34\u7ebf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63cf\u8ff0\u5bf9\u5176\u8fdb\u884c\u6ce8\u91ca\u3002\u5728\u6b64\u6570\u636e\u96c6\u4e0a\uff0c\u5bf9\u5f3a\u5927\u7684\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210\u57fa\u4e8eJSON\u683c\u5f0f\u7684CAD\u5e8f\u5217\u3002\u5f15\u5165\u4e86\u57fa\u4e8e\u7403\u5f62\u5ea6\u3001\u5e73\u5747\u66f2\u7387\u548c\u6b27\u62c9\u7279\u5f81\u7684\u51e0\u4f55\u548c\u62d3\u6251\u6307\u6807\u6765\u63d0\u4f9b\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u6d1e\u5bdf\u3002", "result": "\u5b9e\u9a8c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cCADmium\u80fd\u591f\u81ea\u52a8\u5316CAD\u8bbe\u8ba1\uff0c\u5e76\u663e\u8457\u52a0\u5feb\u65b0\u5bf9\u8c61\u7684\u8bbe\u8ba1\u901f\u5ea6\u3002\u5f15\u5165\u7684\u51e0\u4f55\u548c\u62d3\u6251\u6307\u6807\u63d0\u4f9b\u4e86\u6bd4\u7b80\u5355\u6307\u6807\u66f4\u4e30\u5bcc\u7684\u7ed3\u6784\u6d1e\u5bdf\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u57fa\u4e8eGPT-4\u6ce8\u91ca\u7684\u5927\u578b\u6570\u636e\u96c6\u548c\u5f3a\u5927\u7684\u4ee3\u7801\u8bed\u8a00\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u4ece\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u751f\u6210CAD\u5e8f\u5217\u7684\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002\u5f15\u5165\u4e86\u51e0\u4f55\u548c\u62d3\u6251\u6307\u6807\u6765\u8bc4\u4f30\u751f\u6210\u5bf9\u8c61\u7684\u8d28\u91cf\uff0c\u8bc1\u660e\u4e86CADmium\u80fd\u591f\u81ea\u52a8\u5316CAD\u8bbe\u8ba1\u5e76\u663e\u8457\u52a0\u901f\u65b0\u5bf9\u8c61\u7684\u8bbe\u8ba1\u3002"}}
{"id": "2507.09089", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "pdf": "https://arxiv.org/pdf/2507.09089", "abs": "https://arxiv.org/abs/2507.09089", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "AI": {"tldr": "AI\u5de5\u5177\u7684\u4f7f\u7528\u51cf\u6162\u4e86\u5f00\u53d1\u8005\u7684\u901f\u5ea6\uff0c\u589e\u52a0\u4e8619%\u7684\u5b8c\u6210\u65f6\u95f4\uff0c\u8fd9\u4e0e\u4e4b\u524d\u7684\u9884\u6d4b\u76f8\u53cd\u3002", "motivation": "\u5c3d\u7ba1AI\u5de5\u5177\u88ab\u5e7f\u6cdb\u91c7\u7528\uff0c\u4f46\u5176\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5f71\u54cd\u4ecd\u6709\u5f85\u7814\u7a76\u3002", "method": "\u8fdb\u884c\u4e86\u4e00\u9879\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff08RCT\uff09\uff0c\u6d89\u53ca16\u540d\u6709\u4e2d\u7b49AI\u7ecf\u9a8c\u7684\u5f00\u53d1\u4eba\u5458\uff0c\u4ed6\u4eec\u5b8c\u6210\u4e86246\u4e2a\u4efb\u52a1\uff0c\u5e76\u5141\u8bb8\u6216\u7981\u6b62\u4f7f\u75282025\u5e74\u521d\u7684AI\u5de5\u5177\uff0c\u4ee5\u4e86\u89e3AI\u5de5\u5177\u5bf9\u7ecf\u9a8c\u4e30\u5bcc\u7684\u5f00\u6e90\u5f00\u53d1\u4eba\u5458\u751f\u4ea7\u529b\u7684\u5f71\u54cd\u3002", "result": "\u4e0e\u9884\u671f\u76f8\u53cd\uff0cAI\u5de5\u5177\u7684\u4f7f\u7528\u5c06\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u589e\u52a0\u4e8619%\u3002\u7814\u7a76\u8fd8\u6536\u96c6\u5e76\u8bc4\u4f30\u4e8620\u4e2a\u53ef\u80fd\u5bfc\u81f4\u8fd9\u79cd\u51cf\u6162\u6548\u5e94\u7684\u56e0\u7d20\uff0c\u4f8b\u5982\u9879\u76ee\u89c4\u6a21\u3001\u8d28\u91cf\u6807\u51c6\u4ee5\u53ca\u5f00\u53d1\u4eba\u5458\u5bf9AI\u5de5\u5177\u7684\u5148\u9a8c\u7ecf\u9a8c\u3002", "conclusion": "AI\u5de5\u5177\u7684\u4f7f\u7528\u5b9e\u9645\u4e0a\u589e\u52a0\u4e86\u5f00\u53d1\u8005\u7684\u5b8c\u6210\u65f6\u95f4\uff0819%\uff09\uff0c\u8fd9\u4e0e\u7ecf\u6d4e\u5b66\uff08\u9884\u6d4b\u7f29\u77ed39%\uff09\u548c\u673a\u5668\u5b66\u4e60\uff08\u9884\u6d4b\u7f29\u77ed38%\uff09\u4e13\u5bb6\u7684\u9884\u6d4b\u76f8\u53cd\u3002"}}
{"id": "2507.09774", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09774", "abs": "https://arxiv.org/abs/2507.09774", "authors": ["MD Zobaer Hossain Bhuiyan", "Abir Bin Faruque", "Mahtab Newaz", "Mohammad Abdul Qayum"], "title": "Low-Cost Fuel Dispenser Prototype Using STM32 and an H-bridge motor driver", "comment": null, "summary": "This paper presents the design and development of a low-cost fuel dispensing\nsystem prototype based on the STM32 microcontroller and L298N motor driver. The\nsystem aims to provide an affordable and scalable solution for fuel delivery in\nremote or small-scale environments where conventional, high-cost systems are\nnot feasible. The core control unit is built using an STM32 microcontroller,\nwhich manages user input through a 4x4 matrix keypad and displays operational\ndata on a 16x4 LCD screen via I2C communication. A 12V DC pump motor is used to\nsimulate the fuel dispensing mechanism, precisely controlled via the dual\nH-bridge L298N motor driver. The system is powered by a 11.1V battery and is\ndesigned for ease of deployment and portability. The keypad allows users to\ninput the desired fuel amount, while the system ensures accurate motor runtime\ncorresponding to the volume to be dispensed. This project demonstrates how\nembedded systems can be leveraged to build cost-effective, user-friendly, and\nenergy-efficient solutions. The proposed design can be further enhanced with\nflow sensors, GSM connectivity, RFID cards, and payment integration for\nreal-world applications in fuel stations or agricultural use.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eSTM32\u548cL298N\u7684\u4f4e\u6210\u672c\u71c3\u6cb9\u5206\u914d\u7cfb\u7edf\u539f\u578b\uff0c\u7528\u4e8e\u8fdc\u7a0b\u6216\u5c0f\u89c4\u6a21\u73af\u5883\u3002", "motivation": "\u8be5\u7cfb\u7edf\u65e8\u5728\u4e3a\u4f20\u7edf\u9ad8\u6210\u672c\u7cfb\u7edf\u4e0d\u53ef\u884c\u7684\u8fdc\u7a0b\u6216\u5c0f\u89c4\u6a21\u73af\u5883\u63d0\u4f9b\u8d1f\u62c5\u5f97\u8d77\u7684\u3001\u53ef\u6269\u5c55\u7684\u71c3\u6cb9\u5206\u914d\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7cfb\u7edf\u91c7\u7528STM32\u5fae\u63a7\u5236\u5668\u4f5c\u4e3a\u6838\u5fc3\u63a7\u5236\u5355\u5143\uff0c\u901a\u8fc7I2C\u901a\u4fe1\u7ba1\u740616x4 LCD\u5c4f\u5e55\u7684\u7528\u6237\u8f93\u5165\u548c\u64cd\u4f5c\u6570\u636e\u663e\u793a\u3002L298N\u7535\u673a\u9a71\u52a8\u5668\u7528\u4e8e\u7cbe\u786e\u63a7\u523612V\u76f4\u6d41\u6cf5\u7535\u673a\u4ee5\u6a21\u62df\u71c3\u6cb9\u5206\u914d\u673a\u5236\u3002\u4f7f\u75284x4\u77e9\u9635\u952e\u76d8\u8fdb\u884c\u7528\u6237\u8f93\u5165\u3002", "result": "\u8be5\u9879\u76ee\u6210\u529f\u8bbe\u8ba1\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8eSTM32\u5fae\u63a7\u5236\u5668\u548cL298N\u7535\u673a\u9a71\u52a8\u5668\u7684\u4f4e\u6210\u672c\u71c3\u6cb9\u5206\u914d\u7cfb\u7edf\u539f\u578b\u3002\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u952e\u76d8\u8f93\u5165\u6240\u9700\u7684\u71c3\u6cb9\u91cf\uff0c\u7cfb\u7edf\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u7535\u673a\u8fd0\u884c\u65f6\u95f4\u6765\u786e\u4fdd\u71c3\u6cb9\u91cf\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u9879\u76ee\u5c55\u793a\u4e86\u5982\u4f55\u5229\u7528\u5d4c\u5165\u5f0f\u7cfb\u7edf\u6784\u5efa\u7ecf\u6d4e\u9ad8\u6548\u3001\u7528\u6237\u53cb\u597d\u4e14\u8282\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u7528\u4e8e\u52a0\u6cb9\u7ad9\u6216\u519c\u4e1a\u7528\u9014\u3002"}}
{"id": "2507.09569", "categories": ["cond-mat.mtrl-sci", "physics.app-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.09569", "abs": "https://arxiv.org/abs/2507.09569", "authors": ["Bj\u00f6rn Ewald", "Leo Siebigs", "Cheng Zhang", "Jonas Graf", "Achyut Tiwari", "Maximilian R\u00f6del", "Sebastian Hammer", "Vladimir Stepanenko", "Frank W\u00fcrthner", "Bruno Gompf", "Bert Hecht", "Jens Pflaum"], "title": "How to Fix Silver for Plasmonics", "comment": null, "summary": "Silver (Ag) is considered an ideal material for plasmonic applications in the\nvisible wavelength regime due to its superior optical properties, but its use\nis limited by the poor chemical stability and structural quality of thermally\nevaporated thin films and resulting nanostructures. In this study, we present a\nsimple approach to enhance the structural and optical quality as well as the\nchemical stability of Ag thin films by alloying with gold (Au) through thermal\nco-evaporation. We investigate Ag$_{100-x}$Au$_x$ thin films with Au contents\nranging from 5 to 20 at% analyzing their surface morphology, crystallite\nstructure, optical properties, and chemical stability. Our results show that\nlow Au concentrations significantly reduce the roughness of co-evaporated thin\nfilms (down to 0.4 nm RMS), and significantly enhance the resistance to\noxidation, while maintaining a defined crystallite growth. Importantly, these\nimprovements are achieved without the need for template stripping, metallic\nwetting layers, or epitaxial substrates, enabling direct deposition on glass.\nAmong the compositions studied, Ag$_{95}$Au$_5$ thin films exhibit the highest\nchemical stability, lowest optical losses in the visible spectral range, and\nexcellent plasmonic properties even outcompeting pure Ag. As a\nproof-of-concept, we fabricate high-quality Ag$_{95}$Au$_5$ optical antennas\nthat exhibit long-term durability under ambient conditions. Our approach\nprovides a practical solution to overcome the limitations of Ag for plasmonic\ndevice applications.", "AI": {"tldr": "\u901a\u8fc7\u5728\u94f6\u4e2d\u52a0\u5165\u5c11\u91cf\u91d1\uff08Au\uff09\u53ef\u4ee5\u63d0\u9ad8\u94f6\u8584\u819c\u7684\u8d28\u91cf\u548c\u7a33\u5b9a\u6027\uff0c\u4e14\u65e0\u9700\u590d\u6742\u5de5\u827a\uff0c\u53ef\u4ee5\u76f4\u63a5\u5728\u73bb\u7483\u4e0a\u6c89\u79ef\uff0c\u5e76\u53ef\u7528\u4e8e\u5236\u9020\u9ad8\u8d28\u91cf\u3001\u957f\u5bff\u547d\u7684\u5149\u5b66\u5929\u7ebf\u3002", "motivation": "\u94f6\uff08Ag\uff09\u867d\u7136\u662f\u53ef\u89c1\u5149\u7b49\u79bb\u6fc0\u5143\u5e94\u7528\u7684\u7406\u60f3\u6750\u6599\uff0c\u4f46\u5176\u70ed\u84b8\u53d1\u8584\u819c\u7684\u5316\u5b66\u7a33\u5b9a\u6027\u548c\u7ed3\u6784\u8d28\u91cf\u8f83\u5dee\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u70ed\u5171\u84b8\u53d1\u5c06\u91d1\uff08Au\uff09\u4e0e\u94f6\uff08Ag\uff09\u5408\u91d1\u5316\uff0c\u7814\u7a76\u4e86\u4e0d\u540c\u91d1\u542b\u91cf\uff085-20 at%\uff09\u7684Ag$_{100-x}$Au$_x$\u8584\u819c\u7684\u8868\u9762\u5f62\u8c8c\u3001\u6676\u4f53\u7ed3\u6784\u3001\u5149\u5b66\u6027\u8d28\u548c\u5316\u5b66\u7a33\u5b9a\u6027\u3002", "result": "\u4f4e\u91d1\u542b\u91cf\u53ef\u663e\u8457\u964d\u4f4e\u5171\u84b8\u53d1\u8584\u819c\u7684\u7c97\u7cd9\u5ea6\uff08\u4f4e\u81f30.4 nm RMS\uff09\uff0c\u663e\u8457\u63d0\u9ad8\u6297\u6c27\u5316\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u660e\u786e\u7684\u6676\u7c92\u751f\u957f\u3002\u5176\u4e2d\uff0cAg$_{95}$Au$_5$\u8584\u819c\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u5316\u5b66\u7a33\u5b9a\u6027\u3001\u6700\u4f4e\u7684\u53ef\u89c1\u5149\u8303\u56f4\u5149\u5b66\u635f\u8017\u548c\u4f18\u5f02\u7684\u7b49\u79bb\u6fc0\u5143\u6027\u80fd\uff0c\u751a\u81f3\u4f18\u4e8e\u7eaf\u94f6\u3002", "conclusion": "\u901a\u8fc7\u5408\u91d1\u5316\u91d1\u6765\u63d0\u9ad8\u94f6\u8584\u819c\u7684\u7ed3\u6784\u548c\u5149\u5b66\u8d28\u91cf\u4ee5\u53ca\u5316\u5b66\u7a33\u5b9a\u6027\uff0c\u8fd9\u662f\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u4ee5\u514b\u670d\u94f6\u5728\u7b49\u79bb\u6fc0\u5143\u5668\u4ef6\u5e94\u7528\u4e2d\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.09311", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09311", "abs": "https://arxiv.org/abs/2507.09311", "authors": ["Matteo Cederle", "Marco Fabris", "Gian Antonio Susto"], "title": "A Fairness-Oriented Multi-Objective Reinforcement Learning approach for Autonomous Intersection Management", "comment": "6 pages, 5 figures, accepted at the 1st Joint Conference on\n  Computers, Cognition and Communication, Padua, Italy, Sep. 15-18, 2025", "summary": "This study introduces a novel multi-objective reinforcement learning (MORL)\napproach for autonomous intersection management, aiming to balance traffic\nefficiency and environmental sustainability across electric and internal\ncombustion vehicles. The proposed method utilizes MORL to identify\nPareto-optimal policies, with a post-hoc fairness criterion guiding the\nselection of the final policy. Simulation results in a complex intersection\nscenario demonstrate the approach's effectiveness in optimizing traffic\nefficiency and emissions reduction while ensuring fairness across vehicle\ncategories. We believe that this criterion can lay the foundation for ensuring\nequitable service, while fostering safe, efficient, and sustainable practices\nin smart urban mobility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MORL\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e73\u8861\u4ea4\u53c9\u53e3\u7684\u4ea4\u901a\u6548\u7387\u548c\u73af\u5883\u5f71\u54cd\uff0c\u5e76\u786e\u4fdd\u516c\u5e73\u6027\u3002", "motivation": "\u4e3a\u5e73\u8861\u4ea4\u901a\u6548\u7387\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\uff0c\u5728\u7535\u52a8\u6c7d\u8f66\u548c\u5185\u71c3\u673a\u6c7d\u8f66\u4e4b\u95f4\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7528\u4e8e\u81ea\u4e3b\u4ea4\u53c9\u53e3\u7ba1\u7406\u7684\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08MORL\uff09\u65b9\u6cd5\uff0c\u65e8\u5728\u5e73\u8861\u7535\u52a8\u6c7d\u8f66\u548c\u5185\u71c3\u673a\u6c7d\u8f66\u7684\u4ea4\u901a\u6548\u7387\u548c\u73af\u5883\u53ef\u6301\u7eed\u6027\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5229\u7528MORL\u6765\u8bc6\u522b\u5e15\u7d2f\u6258\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u4e8b\u540e\u516c\u5e73\u6807\u51c6\u6765\u6307\u5bfc\u6700\u7ec8\u7b56\u7565\u7684\u9009\u62e9\u3002", "result": "\u5728\u590d\u6742\u4ea4\u53c9\u53e3\u573a\u666f\u4e2d\u7684\u6a21\u62df\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4f18\u5316\u4ea4\u901a\u6548\u7387\u548c\u51cf\u5c11\u6392\u653e\u4ee5\u53ca\u786e\u4fdd\u8de8\u8f66\u8f86\u7c7b\u522b\u7684\u516c\u5e73\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4e3a\u786e\u4fdd\u516c\u5e73\u670d\u52a1\u5960\u5b9a\u57fa\u7840\uff0c\u540c\u65f6\u4fc3\u8fdb\u667a\u80fd\u57ce\u5e02\u4ea4\u901a\u5b89\u5168\u3001\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u7684\u5b9e\u8df5\u3002"}}
{"id": "2507.09123", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09123", "abs": "https://arxiv.org/abs/2507.09123", "authors": ["Ziyan Gao", "Lijun Wang", "Yuntao Kong", "Nak Young Chong"], "title": "Online 3D Bin Packing with Fast Stability Validation and Stable Rearrangement Planning", "comment": null, "summary": "The Online Bin Packing Problem (OBPP) is a sequential decision-making task in\nwhich each item must be placed immediately upon arrival, with no knowledge of\nfuture arrivals. Although recent deep-reinforcement-learning methods achieve\nsuperior volume utilization compared with classical heuristics, the learned\npolicies cannot ensure the structural stability of the bin and lack mechanisms\nfor safely reconfiguring the bin when a new item cannot be placed directly. In\nthis work, we propose a novel framework that integrates packing policy with\nstructural stability validation and heuristic planning to overcome these\nlimitations. Specifically, we introduce the concept of Load Bearable Convex\nPolygon (LBCP), which provides a computationally efficient way to identify\nstable loading positions that guarantee no bin collapse. Additionally, we\npresent Stable Rearrangement Planning (SRP), a module that rearranges existing\nitems to accommodate new ones while maintaining overall stability. Extensive\nexperiments on standard OBPP benchmarks demonstrate the efficiency and\ngeneralizability of our LBCP-based stability validation, as well as the\nsuperiority of SRP in finding the effort-saving rearrangement plans. Our method\noffers a robust and practical solution for automated packing in real-world\nindustrial and logistics applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09008", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09008", "abs": "https://arxiv.org/abs/2507.09008", "authors": ["Xiwei Xuan", "Xiaoqi Wang", "Wenbin He", "Jorge Piazentin Ono", "Liang Gou", "Kwan-Liu Ma", "Liu Ren"], "title": "VISTA: A Visual Analytics Framework to Enhance Foundation Model-Generated Data Labels", "comment": "IEEE Transactions on Visualization and Computer Graphics (2025)", "summary": "The advances in multi-modal foundation models (FMs) (e.g., CLIP and LLaVA)\nhave facilitated the auto-labeling of large-scale datasets, enhancing model\nperformance in challenging downstream tasks such as open-vocabulary object\ndetection and segmentation. However, the quality of FM-generated labels is less\nstudied as existing approaches focus more on data quantity over quality. This\nis because validating large volumes of data without ground truth presents a\nconsiderable challenge in practice. Existing methods typically rely on limited\nmetrics to identify problematic data, lacking a comprehensive perspective, or\napply human validation to only a small data fraction, failing to address the\nfull spectrum of potential issues. To overcome these challenges, we introduce\nVISTA, a visual analytics framework that improves data quality to enhance the\nperformance of multi-modal models. Targeting the complex and demanding domain\nof open-vocabulary image segmentation, VISTA integrates multi-phased data\nvalidation strategies with human expertise, enabling humans to identify,\nunderstand, and correct hidden issues within FM-generated labels. Through\ndetailed use cases on two benchmark datasets and expert reviews, we demonstrate\nVISTA's effectiveness from both quantitative and qualitative perspectives.", "AI": {"tldr": "VISTA\u662f\u4e00\u4e2a\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u751f\u6210\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u5728\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u5206\u5272\u7b49\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21FM\u751f\u6210\u6807\u7b7e\u7684\u6570\u636e\u8d28\u91cf\u95ee\u9898\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u8fc7\u4e8e\u5173\u6ce8\u6570\u636e\u6570\u91cf\u800c\u975e\u8d28\u91cf\uff0c\u4e14\u6570\u636e\u9a8c\u8bc1\u65b9\u6cd5\u53d7\u9650\u4e8e\u6307\u6807\u6216\u4ec5\u5bf9\u5c11\u91cf\u6570\u636e\u8fdb\u884c\u4eba\u5de5\u9a8c\u8bc1\u3002", "method": "VISTA\u662f\u4e00\u4e2a\u89c6\u89c9\u5206\u6790\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\u548c\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5e2e\u52a9\u8bc6\u522b\u3001\u7406\u89e3\u548c\u4fee\u6b63FM\u751f\u6210\u6807\u7b7e\u4e2d\u7684\u9690\u85cf\u95ee\u9898\u3002", "result": "VISTA\u5728\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u63d0\u5347FM\u751f\u6210\u6807\u7b7e\u7684\u8d28\u91cf\uff0c\u6709\u6548\u589e\u5f3a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "VISTA\u901a\u8fc7\u6574\u5408\u591a\u9636\u6bb5\u6570\u636e\u9a8c\u8bc1\u7b56\u7565\u548c\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\uff08FMs\uff09\u751f\u6210\u6807\u7b7e\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5f00\u653e\u8bcd\u6c47\u56fe\u50cf\u5206\u5272\u9886\u57df\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002\u901a\u8fc7\u5728\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8be6\u7ec6\u6848\u4f8b\u7814\u7a76\u548c\u4e13\u5bb6\u8bc4\u5ba1\uff0c\u8bc1\u660e\u4e86VISTA\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10139", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10139", "abs": "https://arxiv.org/abs/2507.10139", "authors": ["Filipe Miguel Gon\u00e7alves de Almeida", "CJ Carey", "Hendrik Fichtenberger", "Jonathan Halcrow", "Silvio Lattanzi", "Andr\u00e9 Linhares", "Tao Meng", "Ashkan Norouzi-Fard", "Nikos Parotsidis", "Bryan Perozzi", "David Simcha"], "title": "Large-Scale Graph Building in Dynamic Environments: Low Latency and High Quality", "comment": null, "summary": "Learning and constructing large-scale graphs has attracted attention in\nrecent decades, resulting in a rich literature that introduced various systems,\ntools, and algorithms. Grale is one of such tools that is designed for offline\nenvironments and is deployed in more than 50 different industrial settings at\nGoogle. Grale is widely applicable because of its ability to efficiently learn\nand construct a graph on datasets with multiple types of features. However, it\nis often the case that applications require the underlying data to evolve\ncontinuously and rapidly and the updated graph needs to be available with low\nlatency. Such setting make the use of Grale prohibitive. While there are\nApproximate Nearest Neighbor (ANN) systems that handle dynamic updates with low\nlatency, they are mostly limited to similarities over a single embedding.\n  In this work, we introduce a system that inherits the advantages and the\nquality of Grale, and maintains a graph construction in a dynamic setting with\ntens of milliseconds of latency per request. We call the system Dynamic Grale\nUsing ScaNN (Dynamic GUS). Our system has a wide range of applications with\nover 10 deployments at Google. One of the applications is in Android Security\nand Privacy, where Dynamic Grale Using ScaNN enables capturing harmful\napplications 4 times faster, before they can reach users.", "AI": {"tldr": "A new system called Dynamic Grale Using ScaNN (Dynamic GUS) has been developed to efficiently learn and construct large-scale graphs in dynamic, low-latency environments, overcoming the limitations of previous offline tools like Grale. This system has applications in areas like Android Security and Privacy, where it significantly speeds up the detection of harmful applications.", "motivation": "The motivation for this work stems from the limitations of existing graph construction tools like Grale, which are suitable for offline environments but cannot efficiently handle continuously evolving data with low-latency requirements. Existing Approximate Nearest Neighbor (ANN) systems that handle dynamic updates are often limited to similarities over a single embedding.", "method": "The paper introduces Dynamic Grale Using ScaNN (Dynamic GUS), a system designed to handle continuously evolving data and provide updated graphs with low latency, addressing the limitations of existing systems like Grale which are designed for offline environments.", "result": "Dynamic GUS enables capturing harmful applications 4 times faster in Android Security and Privacy, and has over 10 deployments at Google.", "conclusion": "Dynamic Grale Using ScaNN (Dynamic GUS) is a system that inherits the advantages and quality of Grale, maintaining graph construction in a dynamic setting with tens of milliseconds of latency per request. It has over 10 deployments at Google, including in Android Security and Privacy, where it enables capturing harmful applications 4 times faster."}}
{"id": "2507.09260", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.comp-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.09260", "abs": "https://arxiv.org/abs/2507.09260", "authors": ["Yongpan Li", "Cheng-Cheng Liu"], "title": "Quantum metric-based optical selection rules", "comment": null, "summary": "The optical selection rules dictate symmetry-allowed/forbidden transitions,\nplaying a decisive role in engineering exciton quantum states and designing\noptoelectronic devices. While both the real (quantum metric) and imaginary\n(Berry curvature) parts of quantum geometry contribute to optical transitions,\nthe conventional theory of optical selection rules in solids incorporates only\nBerry curvature. Here, we propose quantum metric-based optical selection rules.\nWe unveil a universal quantum metric-oscillator strength correspondence for\nlinear polarization of light and establish valley-contrasted optical selection\nrules that lock orthogonal linear polarizations to distinct valleys.\nTight-binding and first-principles calculations confirm our theory in two\nmodels (altermagnet and Kane-Mele) and monolayer $d$-wave altermagnet\n$\\mathrm{V_2SeSO}$. This work provides a quantum metric paradigm for\nvalley-based spintronic and optoelectronic applications.", "AI": {"tldr": "\u5149\u5b66\u9009\u62e9\u5b9a\u5219\u65b0\u7406\u8bba\uff0c\u5229\u7528\u91cf\u5b50\u5ea6\u89c4\u89e3\u91ca\u5149\u4e0e\u7269\u8d28\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u8c37\u57fa\u5149\u7535\u5b50\u548c\u81ea\u65cb\u7535\u5b50\u5b66\u5e94\u7528\u5f00\u8f9f\u65b0\u9014\u5f84\u3002", "motivation": "\u5149\u5b66\u9009\u62e9\u5b9a\u5219\u5728\u8bbe\u8ba1\u5149\u7535\u5b50\u5668\u4ef6\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\uff0c\u800c\u4f20\u7edf\u7684\u9009\u62e9\u5b9a\u5219\u7406\u8bba\u4ec5\u8003\u8651\u4e86 Berry \u5f27\u7387\uff0c\u5ffd\u7565\u4e86\u91cf\u5b50\u5ea6\u89c4\u7684\u4f5c\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u5305\u542b\u91cf\u5b50\u5ea6\u89c4\u7684\u5149\u5b66\u9009\u62e9\u5b9a\u5219\uff0c\u4ee5\u671f\u5728\u5149\u7535\u5b50\u5668\u4ef6\u8bbe\u8ba1\u4e2d\u63d0\u4f9b\u65b0\u7684\u7406\u8bba\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u57fa\u4e8e\u91cf\u5b50\u5ea6\u89c4\u7684\u5149\u5b66\u9009\u62e9\u5b9a\u5219\uff0c\u5e76\u5229\u7528\u7d27\u675f\u7f1a\u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u5728\u4ea4\u66ff\u78c1\u4f53\u548c Kane-Mele \u6a21\u578b\u4ee5\u53ca\u5355\u5c42 d \u6ce2\u4ea4\u66ff\u78c1\u4f53 V2SeSO \u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u53d1\u73b0\u4e86\u91cf\u5b50\u5ea6\u89c4\u4e0e\u632f\u8361\u5668\u5f3a\u5ea6\u7684\u666e\u904d\u5bf9\u5e94\u5173\u7cfb\uff0c\u9002\u7528\u4e8e\u7ebf\u504f\u632f\u5149\u3002\u5efa\u7acb\u4e86\u8c37\u533a\u5206\u660e\u7684\u5149\u5b66\u9009\u62e9\u5b9a\u5219\uff0c\u5b9e\u73b0\u4e86\u5c06\u6b63\u4ea4\u7ebf\u504f\u632f\u4e0e\u4e0d\u540c\u8c37\u7684\u9501\u5b9a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u91cf\u5b50\u5ea6\u89c4\u7684\u5149\u5b66\u9009\u62e9\u5b9a\u5219\uff0c\u5e76\u5efa\u7acb\u4e86\u8c37\u533a\u5206\u660e\u7684\u5149\u5b66\u9009\u62e9\u5b9a\u5219\uff0c\u5c06\u6b63\u4ea4\u7684\u7ebf\u504f\u632f\u4e0e\u4e0d\u540c\u7684\u8c37\u8054\u7cfb\u8d77\u6765\u3002\u5728\u592a-\u767d\u5bbe\u548c Kane-Mele \u6a21\u578b\u4ee5\u53ca\u5355\u5c42 d \u6ce2\u4ea4\u66ff\u78c1\u4f53 V2SeSO \u4e2d\uff0c\u7d27\u675f\u7f1a\u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u8bc1\u5b9e\u4e86\u8be5\u7406\u8bba\u3002\u8be5\u5de5\u4f5c\u4e3a\u57fa\u4e8e\u8c37\u7684\u81ea\u65cb\u7535\u5b50\u5b66\u548c\u5149\u7535\u5b50\u5b66\u5e94\u7528\u63d0\u4f9b\u4e86\u91cf\u5b50\u5ea6\u89c4\u8303\u5f0f\u3002"}}
{"id": "2507.08996", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2507.08996", "abs": "https://arxiv.org/abs/2507.08996", "authors": ["Arseny Kovyrshin", "Dilhan Manawadu", "Edoardo Altamura", "George Pennington", "Benjamin Jaderberg", "Sebastian Brandhofer", "Anton Nyk\u00e4nen", "Aaron Miller", "Walter Talarico", "Stefan Knecht", "Fabijan Pavo\u0161evi\u0107", "Alberto Baiardi", "Francesco Tacchino", "Ivano Tavernelli", "Stefano Mensa", "Jason Crain", "Lars Tornberg", "Anders Broo"], "title": "Approximate quantum circuit compilation for proton-transfer kinetics on quantum processors", "comment": null, "summary": "Proton transfer reactions are fundamental to many chemical and biological\nsystems, where quantum effects such as tunneling, delocalization, and\nzero-point motion play key kinetic control roles. However, classical methods\ncapable of accurately capturing these phenomena scale prohibitively with system\nsize. Here, we develop and demonstrate quantum computing algorithms based on\nthe Nuclear-Electronic Orbital framework, treating the transferring proton\nquantum mechanically. We assess the potential of current quantum devices for\nsimulating proton transfer kinetics with high accuracy. We first construct a\ndeep initial ans\\\"atze within a truncated orbital space by employing the frozen\nnatural orbital approximation. Then, to balance circuit depth against state\nfidelity, we implement an adaptive form of approximate quantum compiling. Using\nresulting circuits at varying compression levels transpiled for the ibm_fez\ndevice, we compute barrier heights and delocalised proton densities along the\nproton transfer pathway using a realistic hardware noise model. We find that,\nalthough current quantum hardware introduces significant noise relative to the\ndemanding energy tolerances involved, our approach allows substantial circuit\nsimplification while maintaining energy barrier estimates within 13% of the\nreference value. Despite present hardware limitations, these results offer a\npractical means of approximating key circuit segments in near-term devices and\nearly fault-tolerant quantum computing systems.", "AI": {"tldr": "\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u6a21\u62df\u5316\u5b66\u4e2d\u7684\u8d28\u5b50\u8f6c\u79fb\u53cd\u5e94\uff0c\u7814\u7a76\u4e86\u8fd1\u671f\u91cf\u5b50\u786c\u4ef6\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\u3002", "motivation": "\u8d28\u5b50\u8f6c\u79fb\u53cd\u5e94\u5728\u5316\u5b66\u548c\u751f\u7269\u7cfb\u7edf\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u51c6\u786e\u6a21\u62df\u5176\u91cf\u5b50\u6548\u5e94\uff0c\u5e76\u4e14\u968f\u7740\u7cfb\u7edf\u89c4\u6a21\u7684\u589e\u5927\uff0c\u8ba1\u7b97\u6210\u672c\u4f1a\u5448\u6307\u6570\u7ea7\u589e\u957f\u3002", "method": "\u5f00\u53d1\u5e76\u6f14\u793a\u4e86\u57fa\u4e8e\u6838-\u7535\u5b50\u8f68\u9053\uff08NEO\uff09\u6846\u67b6\u7684\u91cf\u5b50\u8ba1\u7b97\u7b97\u6cd5\uff0c\u5c06\u8d28\u5b50\u91cf\u5b50\u5316\uff0c\u5e76\u7ed3\u5408\u4e86\u51bb\u7ed3\u81ea\u7136\u8f68\u9053\uff08FNO\uff09\u8fd1\u4f3c\u548c\u81ea\u9002\u5e94\u91cf\u5b50\u7f16\u8bd1\u3002", "result": "\u5728\u5bf9ibm_fez\u8bbe\u5907\u8fdb\u884c\u8f6c\u8bd1\u7684\u5404\u79cd\u538b\u7f29\u6c34\u5e73\u4e0b\uff0c\u4f7f\u7528\u786c\u4ef6\u566a\u58f0\u6a21\u578b\uff0c\u8ba1\u7b97\u4e86\u8d28\u5b50\u8f6c\u79fb\u8def\u5f84\u4e0a\u7684\u52bf\u5792\u9ad8\u5ea6\u548c\u79bb\u57df\u8d28\u5b50\u5bc6\u5ea6\uff0c\u4f30\u7b97\u51fa\u7684\u80fd\u91cf\u52bf\u5792\u572813%\u7684\u8bef\u5dee\u8303\u56f4\u5185\u3002", "conclusion": "\u5c3d\u7ba1\u5b58\u5728\u786c\u4ef6\u9650\u5236\uff0c\u4f46\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5b9e\u7528\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u8fd1\u671f\u8bbe\u5907\u548c\u65e9\u671f\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u8fd1\u4f3c\u5173\u952e\u7684\u7535\u8def\u7247\u6bb5\u3002"}}
{"id": "2507.08969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.08969", "abs": "https://arxiv.org/abs/2507.08969", "authors": ["Drew Walker", "Jennifer Love", "Swati Rajwal", "Isabel C Walker", "Hannah LF Cooper", "Abeed Sarker", "Melvin Livingston III"], "title": "Application of CARE-SD text classifier tools to assess distribution of stigmatizing and doubt-marking language features in EHR", "comment": "3 Tables", "summary": "Introduction: Electronic health records (EHR) are a critical medium through\nwhich patient stigmatization is perpetuated among healthcare teams. Methods: We\nidentified linguistic features of doubt markers and stigmatizing labels in\nMIMIC-III EHR via expanded lexicon matching and supervised learning\nclassifiers. Predictors of rates of linguistic features were assessed using\nPoisson regression models. Results: We found higher rates of stigmatizing\nlabels per chart among patients who were Black or African American (RR: 1.16),\npatients with Medicare/Medicaid or government-run insurance (RR: 2.46),\nself-pay (RR: 2.12), and patients with a variety of stigmatizing disease and\nmental health conditions. Patterns among doubt markers were similar, though\nmale patients had higher rates of doubt markers (RR: 1.25). We found increased\nstigmatizing labels used by nurses (RR: 1.40), and social workers (RR: 2.25),\nwith similar patterns of doubt markers. Discussion: Stigmatizing language\noccurred at higher rates among historically stigmatized patients, perpetuated\nby multiple provider types.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\uff0c\u975e\u88d4\u7f8e\u56fd\u4eba\u3001\u63a5\u53d7\u653f\u5e9c\u4fdd\u9669\u7684\u60a3\u8005\u3001\u81ea\u8d39\u60a3\u8005\u4ee5\u53ca\u60a3\u6709\u67d0\u4e9b\u75be\u75c5\u7684\u60a3\u8005\u66f4\u5bb9\u6613\u53d7\u5230\u6c61\u540d\u5316\u8bed\u8a00\u7684\u5f71\u54cd\u3002\u7537\u6027\u60a3\u8005\u7684\u7591\u8651\u6807\u8bb0\u4e5f\u66f4\u591a\u3002\u62a4\u58eb\u548c\u793e\u5de5\u5728\u8bb0\u5f55\u4e2d\u4f7f\u7528\u7684\u6c61\u540d\u5316\u8bed\u8a00\u548c\u7591\u8651\u6807\u8bb0\u4e5f\u66f4\u5e38\u89c1\u3002", "motivation": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u662f\u60a3\u8005\u6c61\u540d\u5316\u5728\u533b\u7597\u56e2\u961f\u4e2d\u5f97\u4ee5\u5ef6\u7eed\u7684\u5173\u952e\u5a92\u4ecb\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u8bcd\u5178\u5339\u914d\u548c\u76d1\u7763\u5b66\u4e60\u5206\u7c7b\u5668\uff0c\u8bc6\u522b\u4e86MIMIC-III EHR\u4e2d\u7684\u7591\u8651\u6807\u8bb0\u548c\u6c61\u540d\u5316\u6807\u7b7e\u7684\u8bed\u8a00\u7279\u5f81\u3002\u4f7f\u7528\u6cca\u677e\u56de\u5f52\u6a21\u578b\u8bc4\u4f30\u4e86\u8bed\u8a00\u7279\u5f81\u53d1\u751f\u7387\u7684\u9884\u6d4b\u56e0\u5b50\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5728\u975e\u88d4\u7f8e\u56fd\u4eba\uff08RR: 1.16\uff09\u3001\u63a5\u53d7\u533b\u7597\u8865\u52a9/\u533b\u7597\u4fdd\u9669\u6216\u653f\u5e9c\u7ecf\u8425\u7684\u4fdd\u9669\uff08RR: 2.46\uff09\u3001\u81ea\u8d39\uff08RR: 2.12\uff09\u4ee5\u53ca\u60a3\u6709\u5404\u79cd\u6c61\u540d\u5316\u75be\u75c5\u548c\u7cbe\u795e\u5065\u5eb7\u72b6\u51b5\u7684\u60a3\u8005\u4e2d\uff0c\u6bcf\u4efd\u75c5\u5386\u7684\u6c61\u540d\u5316\u6807\u7b7e\u53d1\u751f\u7387\u66f4\u9ad8\u3002\u7591\u8651\u6807\u8bb0\u7684\u6a21\u5f0f\u76f8\u4f3c\uff0c\u4f46\u7537\u6027\u60a3\u8005\u7684\u7591\u8651\u6807\u8bb0\u53d1\u751f\u7387\u66f4\u9ad8\uff08RR: 1.25\uff09\u3002\u62a4\u58eb\uff08RR: 1.40\uff09\u548c\u793e\u5de5\uff08RR: 2.25\uff09\u4f7f\u7528\u7684\u6c61\u540d\u5316\u6807\u7b7e\u589e\u52a0\uff0c\u7591\u8651\u6807\u8bb0\u7684\u6a21\u5f0f\u4e5f\u7c7b\u4f3c\u3002", "conclusion": "\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u4e2d\u5b58\u5728\u7684\u6c61\u540d\u5316\u8bed\u8a00\u4f1a\u4e0d\u6210\u6bd4\u4f8b\u5730\u5f71\u54cd\u5386\u53f2\u4e0a\u88ab\u6c61\u540d\u5316\u7684\u4eba\u7fa4\uff0c\u5e76\u4e14\u4f1a\u88ab\u591a\u79cd\u63d0\u4f9b\u8005\u7c7b\u578b\u6240\u4f20\u64ad\u3002"}}
{"id": "2507.09544", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09544", "abs": "https://arxiv.org/abs/2507.09544", "authors": ["Ryoga Mahara"], "title": "Existence of Fair and Efficient Allocation of Indivisible Chores", "comment": "33pages, 2 figures", "summary": "We study the problem of allocating indivisible chores among agents with\nadditive cost functions in a fair and efficient manner. A major open question\nin this area is whether there always exists an allocation that is envy-free up\nto one chore (EF1) and Pareto optimal (PO). Our main contribution is to provide\na positive answer to this question by proving the existence of such an\nallocation for indivisible chores under additive cost functions. This is\nachieved by a novel combination of a fixed point argument and a discrete\nalgorithm, providing a significant methodological advance in this area.\n  Our additional key contributions are as follows. We show that there always\nexists an allocation that is EF1 and fractional Pareto optimal (fPO), where fPO\nis a stronger efficiency concept than PO. We also show that an EF1 and PO\nallocation can be computed in polynomial time when the number of agents is\nconstant. Finally, we extend all of these results to the more general setting\nof weighted EF1 (wEF1), which accounts for the entitlements of agents.", "AI": {"tldr": "\u672c\u6587\u8bc1\u660e\u4e86\u5728\u516c\u5e73\u9ad8\u6548\u5730\u5206\u914d\u4e0d\u53ef\u5206\u5272\u6742\u52a1\u65f6\uff0c\u603b\u662f\u5b58\u5728\u4e00\u79cd\u6ee1\u8db3 EF1 \u548c PO \u7684\u5206\u914d\u65b9\u5f0f\uff0c\u5e76\u901a\u8fc7\u4e0d\u52a8\u70b9\u548c\u79bb\u6563\u7b97\u6cd5\u5b9e\u73b0\uff0c\u4e14\u5728\u4ee3\u7406\u6570\u91cf\u6052\u5b9a\u7684\u60c5\u51b5\u4e0b\u53ef\u9ad8\u6548\u8ba1\u7b97\uff0c\u5e76\u63a8\u5e7f\u81f3\u52a0\u6743 EF1\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u5728\u4e3a\u5177\u6709\u9644\u52a0\u6210\u672c\u51fd\u6570\u7684\u4ee3\u7406\u5206\u914d\u4e0d\u53ef\u5206\u5272\u6742\u52a1\u65f6\uff0c\u5982\u4f55\u5b9e\u73b0\u516c\u5e73\u548c\u9ad8\u6548\u5206\u914d\u7684\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u8bd5\u56fe\u56de\u7b54\u4e00\u4e2a\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\uff1a\u662f\u5426\u603b\u80fd\u627e\u5230\u4e00\u79cd\u65e2\u6ee1\u8db3 EF1 \u53c8\u6ee1\u8db3 PO \u7684\u5206\u914d\u65b9\u5f0f\u3002", "method": "\u6587\u7ae0\u901a\u8fc7\u7ed3\u5408\u4e0d\u52a8\u70b9\u8bba\u8bc1\u548c\u79bb\u6563\u7b97\u6cd5\u6765\u8bc1\u660e\u5176\u4e3b\u8981\u7ed3\u8bba\uff0c\u8fd9\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7ec4\u5408\u65b9\u6cd5\u3002", "result": "\u6587\u7ae0\u7684\u4e3b\u8981\u7ed3\u679c\u662f\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4e0d\u53ef\u5206\u5272\u6742\u52a1\u548c\u9644\u52a0\u6210\u672c\u51fd\u6570\uff0c\u603b\u662f\u5b58\u5728\u4e00\u79cd EF1 \u4e14 PO \u7684\u5206\u914d\u65b9\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u79cd EF1 \u4e14 fPO \u7684\u5206\u914d\u65b9\u5f0f\uff0c\u5e76\u4e14\u5728\u4ee3\u7406\u6570\u91cf\u6052\u5b9a\u7684\u60c5\u51b5\u4e0b\uff0cEF1 \u548c PO \u5206\u914d\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8ba1\u7b97\u3002\u6700\u540e\uff0c\u6240\u6709\u7ed3\u679c\u90fd\u53ef\u4ee5\u63a8\u5e7f\u5230 wEF1\u3002", "conclusion": "\u6587\u7ae0\u8bc1\u660e\u4e86\u5728\u9644\u52a0\u6210\u672c\u51fd\u6570\u4e0b\uff0c\u5bf9\u4e8e\u4e0d\u53ef\u5206\u5272\u7684\u6742\u52a1\uff0c\u603b\u662f\u5b58\u5728\u4e00\u79cd\u516c\u5e73\uff08 envy-free up to one chore, EF1\uff09\u4e14\u5e15\u7d2f\u6258\u6700\u4f18\uff08Pareto optimal, PO\uff09\u7684\u5206\u914d\u65b9\u5f0f\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u8bc1\u660e\u4e86\u5b58\u5728\u4e00\u79cd EF1 \u4e14\u6ee1\u8db3\u66f4\u5f3a\u6548\u7387\u6982\u5ff5\uff08fractional Pareto optimal, fPO\uff09\u7684\u5206\u914d\u65b9\u5f0f\u3002\u5f53\u4ee3\u7406\u6570\u91cf\u6052\u5b9a\u65f6\uff0cEF1 \u548c PO \u5206\u914d\u53ef\u4ee5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u8ba1\u7b97\u5f97\u51fa\u3002\u6240\u6709\u8fd9\u4e9b\u7ed3\u679c\u5747\u53ef\u6269\u5c55\u5230\u8003\u8651\u4ee3\u7406\u6743\u76ca\u7684\u52a0\u6743 EF1\uff08wEF1\uff09\u8bbe\u7f6e\u3002"}}
{"id": "2507.09708", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09708", "abs": "https://arxiv.org/abs/2507.09708", "authors": ["Apekshya Bhattarai", "Dinisha Uprety", "Pooja Pathak", "Safal Narshing Shrestha", "Salina Narkarmi", "Sanjog Sigdel"], "title": "A Study Of Sudoku Solving Algorithms: Backtracking and Heuristic", "comment": "14 pages", "summary": "This paper presents a comparative analysis of Sudoku-solving strategies,\nfocusing on recursive backtracking and a heuristic-based constraint propagation\nmethod. Using a dataset of 500 puzzles across five difficulty levels (Beginner\nto Expert), we evaluated performance based on average solving time. The\nheuristic approach consistently outperformed backtracking, achieving speedup\nratios ranging from 1.27x in Beginner puzzles to 2.91x in Expert puzzles. These\nfindings underscore the effectiveness of heuristic strategies, particularly in\ntackling complex puzzles across varying difficulty levels.", "AI": {"tldr": "Heuristic Sudoku solver is faster than backtracking, especially for hard puzzles.", "motivation": "To compare the performance of Sudoku-solving strategies, specifically recursive backtracking and a heuristic-based constraint propagation method.", "method": "Comparative analysis of recursive backtracking and heuristic-based constraint propagation using a dataset of 500 puzzles across five difficulty levels.", "result": "The heuristic approach achieved speedup ratios from 1.27x (Beginner) to 2.91x (Expert) compared to backtracking.", "conclusion": "The heuristic approach consistently outperformed recursive backtracking, especially for more difficult Sudoku puzzles."}}
{"id": "2507.09218", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09218", "abs": "https://arxiv.org/abs/2507.09218", "authors": ["Yi Wang", "Keke Zu", "Luping Xiang", "Martin Haardt", "Chaochao Wang", "Xianchao Zhang", "Kun Yang"], "title": "Image Super-Resolution-Based Signal Enhancement in Bistatic ISAC", "comment": null, "summary": "Bistatic Integrated Sensing and Communication (ISAC) is poised to become a\ncornerstone technology in next-generation communication networks, such as\nBeyond 5G (B5G) and 6G, by enabling the concurrent execution of sensing and\ncommunication functions without requiring significant modifications to existing\ninfrastructure. Despite its promising potential, a major challenge in bistatic\ncooperative sensing lies in the degradation of sensing accuracy, primarily\ncaused by the inherently weak received signals resulting from high reflection\nlosses in complex environments. Traditional methods have predominantly relied\non adaptive filtering techniques to enhance the Signal-to-Noise Ratio (SNR) by\ndynamically adjusting the filter coefficients. However, these methods often\nstruggle to adapt effectively to the increasingly complex and diverse network\ntopologies. To address these challenges, we propose a novel Image\nSuper-Resolution-based Signal Enhancement (ISR-SE) framework that significantly\nimproves the recognition and recovery capabilities of ISAC signals.\nSpecifically, we first perform a time-frequency analysis by applying the\nShort-Time Fourier Transform (STFT) to the received signals, generating\nspectrograms that capture the frequency, magnitude, and phase components. These\ncomponents are then mapped into RGB images, where each channel represents one\nof the extracted features, enabling a more intuitive and informative\nvisualization of the signal structure. To enhance these RGB images, we design\nan improved denoising network that combines the strengths of the UNet\narchitecture and diffusion models. This hybrid architecture leverages UNet's\nmulti-scale feature extraction and the generative capacity of diffusion models\nto perform effective image denoising, thereby improving the quality and clarity\nof signal representations under low-SNR conditions.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u4fe1\u53f7\u589e\u5f3a\uff08ISR-SE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u4fe1\u53f7\u65f6\u9891\u5206\u6790\u7ed3\u679c\u6620\u5c04\u4e3aRGB\u56fe\u50cf\uff0c\u5e76\u5229\u7528UNet\u548c\u6269\u6563\u6a21\u578b\u8fdb\u884c\u53bb\u566a\uff0c\u4ee5\u63d0\u9ad8\u53cc\u7ad9ISAC\u7cfb\u7edf\u5728\u590d\u6742\u73af\u5883\u4e0b\u7684\u611f\u77e5\u7cbe\u5ea6\u3002\u76f8\u8f83\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8be5\u6846\u67b6\u80fd\u66f4\u597d\u5730\u5904\u7406\u4f4e\u4fe1\u566a\u6bd4\u548c\u590d\u6742\u7f51\u7edc\u62d3\u6251\u4e0b\u7684\u4fe1\u53f7\u589e\u5f3a\u95ee\u9898\u3002", "motivation": "\u53cc\u7ad9\u534f\u540c\u611f\u77e5\u7684\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u662f\u611f\u77e5\u7cbe\u5ea6\u4e0b\u964d\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7531\u9ad8\u53cd\u5c04\u635f\u8017\u5f15\u8d77\u4e14\u4fe1\u53f7\u8f83\u5f31\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u81ea\u9002\u5e94\u6ee4\u6ce2\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6ee4\u6ce2\u5668\u7cfb\u6570\u6765\u63d0\u9ad8\u4fe1\u566a\u6bd4\u3002\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u548c\u591a\u6837\u5316\u7684\u7f51\u7edc\u62d3\u6251\u65f6\uff0c\u9002\u5e94\u80fd\u529b\u5f80\u5f80\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u4fe1\u53f7\u589e\u5f3a\uff08ISR-SE\uff09\u6846\u67b6\u3002\u9996\u5148\uff0c\u901a\u8fc7\u5e94\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff08STFT\uff09\u5bf9\u63a5\u6536\u4fe1\u53f7\u8fdb\u884c\u65f6\u9891\u5206\u6790\uff0c\u751f\u6210\u6355\u83b7\u9891\u7387\u3001\u5e45\u5ea6\u548c\u76f8\u4f4d\u5206\u91cf\u7684\u9891\u8c31\u56fe\u3002\u7136\u540e\uff0c\u5c06\u8fd9\u4e9b\u5206\u91cf\u6620\u5c04\u4e3aRGB\u56fe\u50cf\uff0c\u5176\u4e2d\u6bcf\u4e2a\u901a\u9053\u4ee3\u8868\u4e00\u4e2a\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u4fe1\u53f7\u7ed3\u6784\u66f4\u76f4\u89c2\u3001\u4fe1\u606f\u66f4\u4e30\u5bcc\u7684\u53ef\u89c6\u5316\u3002\u4e3a\u4e86\u589e\u5f3a\u8fd9\u4e9bRGB\u56fe\u50cf\uff0c\u672c\u6587\u8bbe\u8ba1\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u53bb\u566a\u7f51\u7edc\uff0c\u7ed3\u5408\u4e86UNet\u67b6\u6784\u548c\u6269\u6563\u6a21\u578b\u7684\u4f18\u70b9\u3002\u8be5\u6df7\u5408\u67b6\u6784\u5229\u7528UNet\u7684\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u80fd\u529b\u548c\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u80fd\u529b\u6765\u8fdb\u884c\u6709\u6548\u7684\u56fe\u50cf\u53bb\u566a\uff0c\u4ece\u800c\u5728\u4f4e\u4fe1\u566a\u6bd4\u6761\u4ef6\u4e0b\u63d0\u9ad8\u4fe1\u53f7\u8868\u793a\u7684\u8d28\u91cf\u548c\u6e05\u6670\u5ea6\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684ISR-SE\u6846\u67b6\u901a\u8fc7\u5c06\u4fe1\u53f7\u6620\u5c04\u4e3aRGB\u56fe\u50cf\uff0c\u5e76\u7ed3\u5408UNet\u548c\u6269\u6563\u6a21\u578b\u8fdb\u884c\u53bb\u566a\uff0c\u6709\u6548\u63d0\u5347\u4e86\u4f4e\u4fe1\u566a\u6bd4\u4e0b\u7684\u4fe1\u53f7\u8868\u793a\u8d28\u91cf\u548c\u6e05\u6670\u5ea6\uff0c\u4ece\u800c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u590d\u6742\u7f51\u7edc\u62d3\u6251\u4e0b\u7684\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86ISAC\u4fe1\u53f7\u7684\u8bc6\u522b\u548c\u6062\u590d\u80fd\u529b\u3002", "conclusion": "ISAC\u6709\u671b\u6210\u4e3aB5G\u548c6G\u7b49\u4e0b\u4e00\u4ee3\u901a\u4fe1\u7f51\u7edc\u7684\u6838\u5fc3\u6280\u672f\uff0c\u901a\u8fc7\u5e76\u884c\u6267\u884c\u4f20\u611f\u548c\u901a\u4fe1\u529f\u80fd\uff0c\u4e14\u65e0\u9700\u5bf9\u73b0\u6709\u57fa\u7840\u8bbe\u65bd\u8fdb\u884c\u5927\u89c4\u6a21\u6539\u9020\u3002\u7136\u800c\uff0c\u53cc\u7ad9\u534f\u540c\u611f\u77e5\u7684\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\u662f\u611f\u77e5\u7cbe\u5ea6\u4e0b\u964d\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u590d\u6742\u73af\u5883\u4e2d\u7531\u9ad8\u53cd\u5c04\u635f\u8017\u5f15\u8d77\u4e14\u4fe1\u53f7\u8f83\u5f31\u3002\u4f20\u7edf\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u81ea\u9002\u5e94\u6ee4\u6ce2\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6ee4\u6ce2\u5668\u7cfb\u6570\u6765\u63d0\u9ad8\u4fe1\u566a\u6bd4\u3002\u4f46\u8fd9\u4e9b\u65b9\u6cd5\u5728\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u548c\u591a\u6837\u5316\u7684\u7f51\u7edc\u62d3\u6251\u65f6\uff0c\u9002\u5e94\u80fd\u529b\u5f80\u5f80\u4e0d\u8db3\u3002\u4e3a\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u4fe1\u53f7\u589e\u5f3a\uff08ISR-SE\uff09\u6846\u67b6\uff0c\u4ee5\u663e\u8457\u63d0\u5347ISAC\u4fe1\u53f7\u7684\u8bc6\u522b\u548c\u6062\u590d\u80fd\u529b\u3002"}}
{"id": "2507.09788", "categories": ["cs.MA", "cs.AI", "cs.CL", "cs.HC", "I.2.11; I.6.5; I.6.7"], "pdf": "https://arxiv.org/pdf/2507.09788", "abs": "https://arxiv.org/abs/2507.09788", "authors": ["Paulo Salem", "Robert Sim", "Christopher Olsen", "Prerit Saxena", "Rafael Barcelos", "Yi Ding"], "title": "TinyTroupe: An LLM-powered Multiagent Persona Simulation Toolkit", "comment": "9 pages. Preprint to be submitted to peer-review", "summary": "Recent advances in Large Language Models (LLM) have led to a new class of\nautonomous agents, renewing and expanding interest in the area. LLM-powered\nMultiagent Systems (MAS) have thus emerged, both for assistive and simulation\npurposes, yet tools for realistic human behavior simulation -- with its\ndistinctive challenges and opportunities -- remain underdeveloped. Existing MAS\nlibraries and tools lack fine-grained persona specifications, population\nsampling facilities, experimentation support, and integrated validation, among\nother key capabilities, limiting their utility for behavioral studies, social\nsimulation, and related applications. To address these deficiencies, in this\nwork we introduce TinyTroupe, a simulation toolkit enabling detailed persona\ndefinitions (e.g., nationality, age, occupation, personality, beliefs,\nbehaviors) and programmatic control via numerous LLM-driven mechanisms. This\nallows for the concise formulation of behavioral problems of practical\ninterest, either at the individual or group level, and provides effective means\nfor their solution. TinyTroupe's components are presented using representative\nworking examples, such as brainstorming and market research sessions, thereby\nsimultaneously clarifying their purpose and demonstrating their usefulness.\nQuantitative and qualitative evaluations of selected aspects are also provided,\nhighlighting possibilities, limitations, and trade-offs. The approach, though\nrealized as a specific Python implementation, is meant as a novel conceptual\ncontribution, which can be partially or fully incorporated in other contexts.\nThe library is available as open source at\nhttps://github.com/microsoft/tinytroupe.", "AI": {"tldr": "TinyTroupe\u662f\u4e00\u4e2a\u7528\u4e8e\u771f\u5b9e\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u7684Python\u5de5\u5177\u5305\uff0c\u5b83\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u4eba\u7269\u89d2\u8272\u5b9a\u4e49\u548cLLM\u9a71\u52a8\u7684\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MAS\u5e93\u7684\u4e0d\u8db3\u3002\u8be5\u5de5\u5177\u5305\u652f\u6301\u8be6\u7ec6\u7684\u4eba\u7269\u89d2\u8272\u8bbe\u7f6e\u548c\u7a0b\u5e8f\u5316\u63a7\u5236\uff0c\u80fd\u591f\u7528\u4e8e\u4e2a\u4f53\u6216\u7fa4\u4f53\u884c\u4e3a\u95ee\u9898\u7684\u89e3\u51b3\uff0c\u5e76\u901a\u8fc7\u793a\u4f8b\u548c\u8bc4\u4f30\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3001\u5c40\u9650\u6027\u548c\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u5728\u771f\u5b9e\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5177\u4f53\u8868\u73b0\u5728\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u7684\u4eba\u7269\u89d2\u8272\u89c4\u8303\u3001\u603b\u4f53\u62bd\u6837\u8bbe\u65bd\u3001\u5b9e\u9a8c\u652f\u6301\u548c\u96c6\u6210\u9a8c\u8bc1\u7b49\u5173\u952e\u80fd\u529b\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u884c\u4e3a\u7814\u7a76\u3001\u793e\u4f1a\u6a21\u62df\u548c\u76f8\u5173\u5e94\u7528\u4e2d\u7684\u6548\u7528\u3002", "method": "TinyTroupe\u901a\u8fc7\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u4eba\u7269\u89d2\u8272\u5b9a\u4e49\uff08\u5982\u56fd\u7c4d\u3001\u5e74\u9f84\u3001\u804c\u4e1a\u3001\u4e2a\u6027\u3001\u4fe1\u4ef0\u3001\u884c\u4e3a\uff09\u548c\u901a\u8fc7\u591a\u79cdLLM\u9a71\u52a8\u7684\u673a\u5236\u8fdb\u884c\u7a0b\u5e8f\u5316\u63a7\u5236\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u8be5\u5de5\u5177\u5305\u7684\u7ec4\u4ef6\u901a\u8fc7\u4ee3\u8868\u6027\u7684\u5de5\u4f5c\u793a\u4f8b\uff08\u5982\u5934\u8111\u98ce\u66b4\u548c\u5e02\u573a\u8c03\u7814\u4f1a\u8bae\uff09\u8fdb\u884c\u5c55\u793a\uff0c\u4ee5\u9610\u660e\u5176\u76ee\u7684\u5e76\u8bc1\u660e\u5176\u6709\u7528\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u5bf9\u9009\u5b9a\u65b9\u9762\u7684\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\uff0c\u4ee5\u7a81\u51fa\u5176\u53ef\u80fd\u6027\u3001\u5c40\u9650\u6027\u548c\u6743\u8861\u3002", "result": "TinyTroupe\u80fd\u591f\u8fdb\u884c\u8be6\u7ec6\u7684\u4eba\u7269\u89d2\u8272\u5b9a\u4e49\u548c\u7a0b\u5e8f\u5316\u63a7\u5236\uff0c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u4e2a\u4f53\u6216\u7fa4\u4f53\u5c42\u9762\u7684\u884c\u4e3a\u95ee\u9898\u3002\u901a\u8fc7\u5934\u8111\u98ce\u66b4\u548c\u5e02\u573a\u8c03\u7814\u4f1a\u8bae\u7b49\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5176\u6709\u7528\u6027\u3002\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u7a81\u51fa\u4e86\u5176\u53ef\u80fd\u6027\u3001\u5c40\u9650\u6027\u548c\u6743\u8861\u3002", "conclusion": "TinyTroupe\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6a21\u62df\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u4eba\u7269\u89d2\u8272\u5b9a\u4e49\u548c\u57fa\u4e8eLLM\u7684\u7a0b\u5e8f\u5316\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709MAS\u5e93\u5728\u771f\u5b9e\u4eba\u7c7b\u884c\u4e3a\u6a21\u62df\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u8be5\u5de5\u5177\u5305\u652f\u6301\u7528\u6237\u5b9a\u4e49\u5305\u62ec\u56fd\u7c4d\u3001\u5e74\u9f84\u3001\u804c\u4e1a\u3001\u6027\u683c\u3001\u4fe1\u4ef0\u548c\u884c\u4e3a\u5728\u5185\u7684\u8be6\u7ec6\u4eba\u7269\u89d2\u8272\uff0c\u5e76\u901a\u8fc7LLM\u9a71\u52a8\u7684\u673a\u5236\u5b9e\u73b0\u5bf9\u6a21\u62df\u7684\u7ec6\u81f4\u63a7\u5236\uff0c\u4ece\u800c\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u4e2a\u4f53\u6216\u7fa4\u4f53\u5c42\u9762\u7684\u884c\u4e3a\u95ee\u9898\u3002"}}
{"id": "2507.08835", "categories": ["cs.LG", "cs.AI", "math.ST", "q-fin.RM", "q-fin.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.08835", "abs": "https://arxiv.org/abs/2507.08835", "authors": ["Harold Gu\u00e9neau", "Alain Celisse", "Pascal Delange"], "title": "Representation learning with a transformer by contrastive learning for money laundering detection", "comment": null, "summary": "The present work tackles the money laundering detection problem. A new\nprocedure is introduced which exploits structured time series of both\nqualitative and quantitative data by means of a transformer neural network. The\nfirst step of this procedure aims at learning representations of time series\nthrough contrastive learning (without any labels). The second step leverages\nthese representations to generate a money laundering scoring of all\nobservations. A two-thresholds approach is then introduced, which ensures a\ncontrolled false-positive rate by means of the Benjamini-Hochberg (BH)\nprocedure. Experiments confirm that the transformer is able to produce general\nrepresentations that succeed in exploiting money laundering patterns with\nminimal supervision from domain experts. It also illustrates the higher ability\nof the new procedure for detecting nonfraudsters as well as fraudsters, while\nkeeping the false positive rate under control. This greatly contrasts with\nrule-based procedures or the ones based on LSTM architectures.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6d17\u94b1\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528 Transformer \u795e\u7ecf\u7f51\u7edc\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u8868\u793a\u3002\u8be5\u65b9\u6cd5\u6bd4\u57fa\u4e8e\u89c4\u5219\u548c LSTM \u7684\u65b9\u6cd5\u66f4\u6709\u6548\u3002", "motivation": "\u89e3\u51b3\u6d17\u94b1\u68c0\u6d4b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5229\u7528\u7ed3\u6784\u5316\u65f6\u95f4\u5e8f\u5217\u7684\u8d28\u91cf\u548c\u6570\u91cf\u6570\u636e\uff0c\u5e76\u4f7f\u7528 Transformer \u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6b64\u64cd\u4f5c\u3002\u7b2c\u4e00\u6b65\u662f\u65e0\u76d1\u7763\u7684\uff0c\u5176\u4e2d Transformer \u6a21\u578b\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b66\u4e60\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\u3002\u7b2c\u4e8c\u6b65\u662f\u5229\u7528\u8fd9\u4e9b\u8868\u793a\u6765\u751f\u6210\u6240\u6709\u89c2\u6d4b\u503c\u7684\u6d17\u94b1\u5f97\u5206\u3002\u5f15\u5165\u4e86\u4e24\u9608\u503c\u65b9\u6cd5\uff0c\u901a\u8fc7 Benjamini-Hochberg (BH) \u7a0b\u5e8f\u6765\u63a7\u5236\u5047\u9633\u6027\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u5b9e\uff0cTransformer \u80fd\u591f\u751f\u6210\u6709\u7528\u7684\u8868\u793a\uff0c\u80fd\u591f\u5229\u7528\u6d17\u94b1\u6a21\u5f0f\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u5728\u8bc6\u522b\u6b3a\u8bc8\u8005\u548c\u975e\u6b3a\u8bc8\u8005\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u80fd\u529b\uff0c\u540c\u65f6\u5c06\u5047\u9633\u6027\u7387\u63a7\u5236\u5728\u53ef\u63a7\u8303\u56f4\u5185\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u5047\u9633\u6027\u7387\u53ef\u63a7\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u8bc6\u522b\u6b3a\u8bc8\u8005\u548c\u975e\u6b3a\u8bc8\u8005\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u80fd\u529b\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u80fd\u591f\u5b66\u4e60\u5230\u6709\u7528\u7684\u8868\u793a\uff0c\u80fd\u591f\u5229\u7528\u6d17\u94b1\u6a21\u5f0f\u3002"}}
{"id": "2507.09476", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09476", "abs": "https://arxiv.org/abs/2507.09476", "authors": ["Xu Chen", "Mingbo Dou", "Qin Zhang", "Xianjie Wang", "M. Ye. Zhuravlev", "A. V. Nikolaev", "L. L. Tao"], "title": "Magnetic control of nonlinear transport induced by the quantum metric", "comment": null, "summary": "The quantum geometry plays a crucial role in the nonlinear transport of\nquantum materials. Here, we use the Boltzmann transport formalism to study the\nmagnetic control of nonlinear transport induced by the quantum metric in\ntwo-dimensional systems with different types of spin-orbit coupling (SOC). It\nis shown that the nonlinear conductivity is strongly dependent on the direction\nof a field and reveals significant spatial anisotropy. Moreover, the\nfield-direction dependent relations are distinct for different SOCs. In\naddition, it is demonstrated that the contributions from the quantum metric and\nDrude mechanism are distinguishable due to their opposite signs or distinct\nanisotropy relations. We further derive the analytical formulas for the\nanisotropic nonlinear conductivity, in exact agreement with numerical results.\nOur work shines more light on the interplay between the nonlinear transport and\nquantum geometry.", "AI": {"tldr": "\u7814\u7a76\u4e86\u78c1\u573a\u5bf9\u4e8c\u7ef4\u6750\u6599\u4e2d\u7531\u91cf\u5b50\u5ea6\u91cf\u5f15\u8d77\u7684\u975e\u7ebf\u6027\u8f93\u8fd0\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u8f93\u8fd0\u7279\u6027\u5177\u6709\u5404\u5411\u5f02\u6027\u4e14\u4f9d\u8d56\u4e8e\u78c1\u573a\u65b9\u5411\u548c\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u7c7b\u578b\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u6750\u6599\u4e2d\u7684\u975e\u7ebf\u6027\u8f93\u8fd0\u53ca\u5176\u4e0e\u91cf\u5b50\u51e0\u4f55\u7684\u8054\u7cfb\uff0c\u7279\u522b\u662f\u78c1\u573a\u5982\u4f55\u5f71\u54cd\u8fd9\u79cd\u8f93\u8fd0\u3002", "method": "\u91c7\u7528\u73bb\u5c14\u5179\u66fc\u8f93\u8fd0\u5f62\u5f0f\uff0c\u7814\u7a76\u4e86\u91cf\u5b50\u5ea6\u91cf\u5728\u4e8c\u7ef4\u6750\u6599\u4e2d\u78c1\u573a\u8c03\u63a7\u7684\u975e\u7ebf\u6027\u8f93\u8fd0\u7279\u6027\uff0c\u5e76\u8003\u8651\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u81ea\u65cb-\u8f68\u9053\u8026\u5408\uff08SOC\uff09\u3002", "result": "\u53d1\u73b0\u975e\u7ebf\u6027\u7535\u5bfc\u7387\u5177\u6709\u663e\u8457\u7684\u7a7a\u95f4\u5404\u5411\u5f02\u6027\uff0c\u4e14\u4e0e\u78c1\u573a\u65b9\u5411\u5bc6\u5207\u76f8\u5173\uff0c\u4e0d\u540cSOC\u7c7b\u578b\u4e0b\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u884c\u4e3a\u3002\u91cf\u5b50\u5ea6\u91cf\u548c\u8f7d\u6d41\u5b50\u8f93\u8fd0\u673a\u5236\u7684\u8d21\u732e\u53ef\u4ee5\u88ab\u533a\u5206\u5f00\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86\u91cf\u5b50\u51e0\u4f55\u4e0e\u975e\u7ebf\u6027\u8f93\u8fd0\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u4e3a\u533a\u5206\u91cf\u5b50\u5ea6\u91cf\u548c\u8f7d\u6d41\u5b50\u8f93\u8fd0\u673a\u5236\u7684\u8d21\u732e\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002"}}
{"id": "2507.09711", "categories": ["cs.DS", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09711", "abs": "https://arxiv.org/abs/2507.09711", "authors": ["Kun He"], "title": "Phase transition of the Sinkhorn-Knopp algorithm", "comment": "44 pages, 2 figures", "summary": "The matrix scaling problem, particularly the Sinkhorn-Knopp algorithm, has\nbeen studied for over 60 years. In practice, the algorithm often yields\nhigh-quality approximations within just a few iterations. Theoretically,\nhowever, the best-known upper bound places it in the class of\npseudopolynomial-time approximation algorithms. Meanwhile, the lower-bound\nlandscape remains largely unexplored. Two fundamental questions persist: what\naccounts for the algorithm's strong empirical performance, and can a tight\nbound on its iteration count be established?\n  For an $n\\times n$ matrix, its normalized version is obtained by dividing\neach entry by its largest entry. We say that a normalized matrix has a density\n$\\gamma$ if there exists a constant $\\rho > 0$ such that one row or column has\nexactly $\\lceil \\gamma n \\rceil$ entries with values at least $\\rho$, and every\nother row and column has at least $\\lceil \\gamma n \\rceil$ such entries.\n  For the upper bound, we show that the Sinkhorn-Knopp algorithm produces a\nnearly doubly stochastic matrix in $O(\\log n - \\log \\varepsilon)$ iterations\nand $\\widetilde{O}(n^2)$ time for all nonnegative square matrices whose\nnormalized version has a density $\\gamma > 1/2$. Such matrices cover both the\nalgorithm's principal practical inputs and its typical theoretical regime, and\nthe $\\widetilde{O}(n^2)$ runtime is optimal.\n  For the lower bound, we establish a tight bound of\n$\\widetilde{\\Omega}\\left(n^{1/2}/\\varepsilon\\right)$ iterations for positive\nmatrices under the $\\ell_2$-norm error measure. Moreover, for every $\\gamma <\n1/2$, there exists a matrix with density $\\gamma$ for which the algorithm\nrequires $\\Omega\\left(n^{1/2}/\\varepsilon\\right)$ iterations.\n  In summary, our results reveal a sharp phase transition in the Sinkhorn-Knopp\nalgorithm at the density threshold $\\gamma = 1/2$.", "AI": {"tldr": "Sinkhorn-Knopp \u7b97\u6cd5\u7684\u6027\u80fd\u53d6\u51b3\u4e8e\u77e9\u9635\u7684\u5bc6\u5ea6\uff0c\u5728\u5bc6\u5ea6\u9608\u503c \u03b3 = 1/2 \u5904\u5b58\u5728\u76f8\u4f4d\u8f6c\u6362\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5173\u4e8e Sinkhorn-Knopp \u7b97\u6cd5\u7684\u4e24\u4e2a\u57fa\u672c\u95ee\u9898\uff1a\u5176\u5f3a\u5927\u7684\u7ecf\u9a8c\u6027\u80fd\u5f52\u56e0\u4e8e\u4ec0\u4e48\uff1f\u662f\u5426\u80fd\u4e3a\u5176\u8fed\u4ee3\u6b21\u6570\u5efa\u7acb\u4e00\u4e2a\u7cbe\u786e\u7684\u754c\u9650\uff1f", "method": "\u5bf9\u4e8e\u4e0a\u754c\uff0c\u6211\u4eec\u8bc1\u660e\u4e86 Sinkhorn-Knopp \u7b97\u6cd5\u5728\u8fed\u4ee3 O(log n - log \u03b5) \u6b21\u548c O(n^2) \u65f6\u95f4\u5185\uff0c\u53ef\u4ee5\u4e3a\u6240\u6709\u5f52\u4e00\u5316\u7248\u672c\u5177\u6709\u5bc6\u5ea6 \u03b3 > 1/2 \u7684\u975e\u8d1f\u65b9\u9635\u751f\u6210\u4e00\u4e2a\u8fd1\u4f3c\u53cc\u968f\u673a\u77e9\u9635\u3002\u5bf9\u4e8e\u4e0b\u754c\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u6b63\u77e9\u9635\u5728 L2 \u8303\u6570\u8bef\u5dee\u5ea6\u91cf\u4e0b\u7684\u8fed\u4ee3\u6b21\u6570\u7684\u7cbe\u786e\u754c\u9650\u4e3a \u03a9(n^1/2/\u03b5)\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u5bf9\u4e8e\u6240\u6709 \u03b3 < 1/2\uff0c\u90fd\u5b58\u5728\u4e00\u4e2a\u5bc6\u5ea6\u4e3a \u03b3 \u7684\u77e9\u9635\uff0c\u8be5\u77e9\u9635\u9700\u8981 \u03a9(n^1/2/\u03b5) \u6b21\u8fed\u4ee3\u3002", "result": "Sinkhorn-Knopp \u7b97\u6cd5\u5728\u5bc6\u5ea6 \u03b3 > 1/2 \u7684\u60c5\u51b5\u4e0b\uff0c\u8fed\u4ee3 O(log n - log \u03b5) \u6b21\u548c O(n^2) \u65f6\u95f4\u5185\u53ef\u751f\u6210\u8fd1\u4f3c\u53cc\u968f\u673a\u77e9\u9635\u3002\u5bf9\u4e8e\u5bc6\u5ea6 \u03b3 < 1/2 \u7684\u6b63\u77e9\u9635\uff0c\u9700\u8981 \u03a9(n^1/2/\u03b5) \u6b21\u8fed\u4ee3\u3002", "conclusion": "\u672c\u8bba\u6587\u63ed\u793a\u4e86 Sinkhorn-Knopp \u7b97\u6cd5\u5728\u5bc6\u5ea6\u9608\u503c \u03b3 = 1/2 \u5904\u5b58\u5728\u4e00\u4e2a\u660e\u786e\u7684\u76f8\u4f4d\u8f6c\u6362\u3002"}}
{"id": "2507.10542", "categories": ["cs.GR", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10542", "abs": "https://arxiv.org/abs/2507.10542", "authors": ["Shivangi Aneja", "Sebastian Weiss", "Irene Baeza", "Prashanth Chandran", "Gaspard Zoss", "Matthias Nie\u00dfner", "Derek Bradley"], "title": "ScaffoldAvatar: High-Fidelity Gaussian Avatars with Patch Expressions", "comment": "(SIGGRAPH 2025) Paper Video: https://youtu.be/VyWkgsGdbkk Project\n  Page: https://shivangi-aneja.github.io/projects/scaffoldavatar/", "summary": "Generating high-fidelity real-time animated sequences of photorealistic 3D\nhead avatars is important for many graphics applications, including immersive\ntelepresence and movies. This is a challenging problem particularly when\nrendering digital avatar close-ups for showing character's facial microfeatures\nand expressions. To capture the expressive, detailed nature of human heads,\nincluding skin furrowing and finer-scale facial movements, we propose to couple\nlocally-defined facial expressions with 3D Gaussian splatting to enable\ncreating ultra-high fidelity, expressive and photorealistic 3D head avatars. In\ncontrast to previous works that operate on a global expression space, we\ncondition our avatar's dynamics on patch-based local expression features and\nsynthesize 3D Gaussians at a patch level. In particular, we leverage a\npatch-based geometric 3D face model to extract patch expressions and learn how\nto translate these into local dynamic skin appearance and motion by coupling\nthe patches with anchor points of Scaffold-GS, a recent hierarchical scene\nrepresentation. These anchors are then used to synthesize 3D Gaussians\non-the-fly, conditioned by patch-expressions and viewing direction. We employ\ncolor-based densification and progressive training to obtain high-quality\nresults and faster convergence for high resolution 3K training images. By\nleveraging patch-level expressions, ScaffoldAvatar consistently achieves\nstate-of-the-art performance with visually natural motion, while encompassing\ndiverse facial expressions and styles in real time.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ScaffoldAvatar \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5c40\u90e8\u9762\u90e8\u8868\u60c5\u548c 3D \u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684 3D \u5934\u50cf\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u5757\u7ea7\u522b\u5904\u7406\u9762\u90e8\u8868\u60c5\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u5f80\u6280\u672f\u66f4\u81ea\u7136\u3001\u66f4\u4e30\u5bcc\u7684\u9762\u90e8\u52a8\u753b\uff0c\u5e76\u4e14\u80fd\u591f\u5b9e\u65f6\u8fd0\u884c\u3002", "motivation": "\u751f\u6210\u9ad8\u4fdd\u771f\u5ea6\u7684\u5b9e\u65f6\u52a8\u753b\u5e8f\u5217\uff0c\u903c\u771f\u7684 3D \u5934\u50cf\u5bf9\u4e8e\u8bb8\u591a\u56fe\u5f62\u5e94\u7528\u7a0b\u5e8f\u5f88\u91cd\u8981\uff0c\u5305\u62ec\u6c89\u6d78\u5f0f\u8fdc\u7a0b\u5448\u73b0\u548c\u7535\u5f71\u3002\u7279\u522b\u662f\u5728\u6e32\u67d3\u6570\u5b57\u5934\u50cf\u7279\u5199\u955c\u5934\u4ee5\u5c55\u793a\u89d2\u8272\u7684\u9762\u90e8\u5fae\u7279\u5f81\u548c\u8868\u60c5\u65f6\uff0c\u8fd9\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u6355\u6349\u4eba\u7c7b\u5934\u90e8\u7684\u8868\u73b0\u529b\u548c\u7ec6\u8282\u7279\u5f81\uff0c\u5305\u62ec\u76ae\u80a4\u76b1\u7eb9\u548c\u66f4\u7cbe\u7ec6\u7684\u9762\u90e8\u8fd0\u52a8\uff0c\u9700\u8981\u8fd9\u79cd\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u5c06\u5c40\u90e8\u5b9a\u4e49\u7684\u9762\u90e8\u8868\u60c5\u4e0e 3D \u9ad8\u65af\u6cfc\u6e85\u76f8\u7ed3\u5408\uff0c\u4ee5\u521b\u5efa\u8d85\u9ad8\u4fdd\u771f\u5ea6\u3001\u5bcc\u6709\u8868\u73b0\u529b\u548c\u903c\u771f\u7684 3D \u5934\u50cf\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5229\u7528\u57fa\u4e8e\u5757\u7684\u51e0\u4f55 3D \u9762\u90e8\u6a21\u578b\u63d0\u53d6\u5757\u8868\u8fbe\u5f0f\uff0c\u5e76\u5b66\u4e60\u5982\u4f55\u5c06\u5b83\u4eec\u8f6c\u5316\u4e3a\u5c40\u90e8\u52a8\u6001\u76ae\u80a4\u5916\u89c2\u548c\u8fd0\u52a8\uff0c\u65b9\u6cd5\u662f\u5c06\u5757\u4e0e Scaffold-GS \u7684\u951a\u70b9\u8026\u5408\u3002\u7136\u540e\uff0c\u5229\u7528\u8fd9\u4e9b\u951a\u70b9\u6839\u636e\u5757\u8868\u8fbe\u5f0f\u548c\u89c6\u89d2\u5373\u65f6\u5408\u6210 3D \u9ad8\u65af\u3002\u91c7\u7528\u57fa\u4e8e\u989c\u8272\u7684\u81f4\u5bc6\u5316\u548c\u6e10\u8fdb\u5f0f\u8bad\u7ec3\uff0c\u4ee5\u83b7\u5f97\u9ad8\u8d28\u91cf\u7684\u7ed3\u679c\u548c\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u3002", "result": "\u901a\u8fc7\u5229\u7528\u5757\u7ea7\u8868\u8fbe\u5f0f\uff0cScaffoldAvatar \u6301\u7eed\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u89c6\u89c9\u4e0a\u81ea\u7136\u7684\u8fd0\u52a8\uff0c\u540c\u65f6\u5b9e\u65f6\u5305\u542b\u5404\u79cd\u9762\u90e8\u8868\u60c5\u548c\u98ce\u683c\u3002", "conclusion": "ScaffoldAvatar \u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5177\u6709\u89c6\u89c9\u4e0a\u81ea\u7136\u7684\u8fd0\u52a8\uff0c\u540c\u65f6\u5b9e\u65f6\u5305\u542b\u5404\u79cd\u9762\u90e8\u8868\u60c5\u548c\u98ce\u683c\u3002"}}
{"id": "2507.09179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09179", "abs": "https://arxiv.org/abs/2507.09179", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "comment": null, "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.", "AI": {"tldr": "Hide-and-Shill\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684DeFi\u64cd\u7eb5\u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408GRPO\u3001\u57fa\u4e8e\u7406\u8bba\u7684\u5956\u52b1\u51fd\u6570\u548c\u591a\u6a21\u6001\u6570\u636e\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728Symphony\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\uff08DeFi\uff09\u5e26\u6765\u4e86\u65e0\u9700\u8bb8\u53ef\u7684\u91d1\u878d\u521b\u65b0\uff0c\u4f46\u4e5f\u5bfc\u81f4\u4e86\u524d\u6240\u672a\u6709\u7684\u5e02\u573a\u64cd\u7eb5\u3002\u5728\u7f3a\u4e4f\u4e2d\u5fc3\u5316\u76d1\u7ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u6076\u610f\u884c\u4e3a\u8005\u5728\u5404\u79cd\u5e73\u53f0\u534f\u8c03\u201c\u62c9\u9ad8\u51fa\u8d27\u201d\u5ba3\u4f20\u6d3b\u52a8\u548c\u64cd\u7eb5\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53bb\u4e2d\u5fc3\u5316\u64cd\u7eb5\u68c0\u6d4b\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u6846\u67b6\uff0c\u5c06\u64cd\u7eb5\u8005\u548c\u68c0\u6d4b\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a\u52a8\u6001\u5bf9\u6297\u535a\u5f08\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u5ef6\u8fdf\u4ee3\u5e01\u4ef7\u683c\u53cd\u5e94\u4f5c\u4e3a\u91d1\u878d\u6307\u6807\u6765\u8bc6\u522b\u53ef\u7591\u6a21\u5f0f\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e09\u4e2a\u521b\u65b0\uff1a(1) \u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4ee5\u589e\u5f3a\u7a00\u758f\u5956\u52b1\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u73af\u5883\u4e2d\u7684\u5b66\u4e60\u7a33\u5b9a\u6027\uff1b(2) \u53d7\u7406\u6027\u9884\u671f\u548c\u4fe1\u606f\u4e0d\u5bf9\u79f0\u542f\u53d1\u7684\u57fa\u4e8e\u7406\u8bba\u7684\u5956\u52b1\u51fd\u6570\uff0c\u533a\u5206\u4ef7\u683c\u53d1\u73b0\u548c\u64cd\u7eb5\u566a\u97f3\uff1b(3) \u591a\u6a21\u6001\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u6574\u5408\u4e86\u57fa\u4e8eLLM\u7684\u8bed\u4e49\u7279\u5f81\u3001\u793e\u4ea4\u56fe\u4fe1\u53f7\u548c\u94fe\u4e0a\u5e02\u573a\u6570\u636e\u4ee5\u8fdb\u884c\u77e5\u60c5\u51b3\u7b56\u3002\u8be5\u6846\u67b6\u96c6\u6210\u5728Symphony\u7cfb\u7edf\u4e2d\uff0c\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u7684\u591a\u667a\u80fd\u4f53\u67b6\u6784\uff0c\u652f\u6301\u70b9\u5bf9\u70b9\u667a\u80fd\u4f53\u6267\u884c\u548c\u901a\u8fc7\u5206\u5e03\u5f0f\u65e5\u5fd7\u8fdb\u884c\u4fe1\u4efb\u611f\u77e5\u5b66\u4e60\uff0c\u652f\u6301\u94fe\u53ef\u9a8c\u8bc1\u8bc4\u4f30\u3002Symphony\u4fc3\u8fdb\u6218\u7565\u884c\u4e3a\u8005\u4e4b\u95f4\u7684\u5bf9\u6297\u5171\u540c\u8fdb\u5316\uff0c\u5e76\u5728\u6ca1\u6709\u4e2d\u5fc3\u5316\u9884\u8a00\u673a\u7684\u60c5\u51b5\u4e0b\u4fdd\u6301\u5f3a\u5927\u7684\u64cd\u7eb5\u68c0\u6d4b\u80fd\u529b\uff0c\u4ece\u800c\u80fd\u591f\u5bf9\u5168\u7403DeFi\u751f\u6001\u7cfb\u7edf\u8fdb\u884c\u5b9e\u65f6\u76d1\u63a7\u3002", "result": "\u5728100,000\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u8ba8\u8bba\u7247\u6bb5\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u5bf9\u6297\u6a21\u62df\u4e2d\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u201cHide-and-Shill\u201d\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u56e0\u679c\u5f52\u56e0\u65b9\u9762\u53d6\u5f97\u4e86\u9876\u7ea7\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5c06\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e0e\u91d1\u878d\u76d1\u63a7\u76f8\u7ed3\u5408\uff0c\u63a8\u52a8\u4e86\u4e00\u79cd\u65b0\u7684\u53bb\u4e2d\u5fc3\u5316\u5e02\u573a\u60c5\u62a5\u8303\u5f0f\u3002"}}
{"id": "2507.09780", "categories": ["cs.AR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09780", "abs": "https://arxiv.org/abs/2507.09780", "authors": ["Feilong Qiaoyuan", "Jihe Wang", "Zhiyu Sun", "Linying Wu", "Yuanhua Xiao", "Danghui Wang"], "title": "BitParticle: Partializing Sparse Dual-Factors to Build Quasi-Synchronizing MAC Arrays for Energy-efficient DNNs", "comment": "9 pages, 13 figures, 3 Tables", "summary": "Bit-level sparsity in quantized deep neural networks (DNNs) offers\nsignificant potential for optimizing Multiply-Accumulate (MAC) operations.\nHowever, two key challenges still limit its practical exploitation. First,\nconventional bit-serial approaches cannot simultaneously leverage the sparsity\nof both factors, leading to a complete waste of one factor' s sparsity. Methods\ndesigned to exploit dual-factor sparsity are still in the early stages of\nexploration, facing the challenge of partial product explosion. Second, the\nfluctuation of bit-level sparsity leads to variable cycle counts for MAC\noperations. Existing synchronous scheduling schemes that are suitable for\ndual-factor sparsity exhibit poor flexibility and still result in significant\nunderutilization of MAC units. To address the first challenge, this study\nproposes a MAC unit that leverages dual-factor sparsity through the emerging\nparticlization-based approach. The proposed design addresses the issue of\npartial product explosion through simple control logic, resulting in a more\narea- and energy-efficient MAC unit. In addition, by discarding less\nsignificant intermediate results, the design allows for further hardware\nsimplification at the cost of minor accuracy loss. To address the second\nchallenge, a quasi-synchronous scheme is introduced that adds cycle-level\nelasticity to the MAC array, reducing pipeline stalls and thereby improving MAC\nunit utilization. Evaluation results show that the exact version of the\nproposed MAC array architecture achieves a 29.2% improvement in area efficiency\ncompared to the state-of-the-art bit-sparsity-driven architecture, while\nmaintaining comparable energy efficiency. The approximate variant further\nimproves energy efficiency by 7.5%, compared to the exact version. Index-Terms:\nDNN acceleration, Bit-level sparsity, MAC unit", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684MAC\u5355\u5143\u548c\u51c6\u540c\u6b65\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u91cf\u5316DNN\u4e2d\u7684\u4f4d\u7ea7\u7a00\u758f\u6027\u5229\u7528\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9762\u79ef\u548c\u80fd\u6e90\u6548\u7387\u3002", "motivation": "\u91cf\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4e2d\u7684\u4f4d\u7ea7\u7a00\u758f\u6027\u5728\u4f18\u5316\u4e58\u52a0\uff08MAC\uff09\u8fd0\u7b97\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u53cc\u56e0\u5b50\u7a00\u758f\u6027\u5229\u7528\u4e0d\u5145\u5206\u548cMAC\u8fd0\u7b97\u5468\u671f\u6570\u6613\u53d8\u5bfc\u81f4MAC\u5355\u5143\u5229\u7528\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u65b0\u5174\u7684\u7c92\u5b50\u5316\u65b9\u6cd5\u6765\u5229\u7528\u53cc\u56e0\u5b50\u7a00\u758f\u6027\u7684MAC\u5355\u5143\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u63a7\u5236\u903b\u8f91\u89e3\u51b3\u4e86\u90e8\u5206\u79ef\u7206\u70b8\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u51c6\u540c\u6b65\u65b9\u6848\uff0c\u4e3aMAC\u9635\u5217\u589e\u52a0\u4e86\u5468\u671f\u7ea7\u5f39\u6027\uff0c\u4ee5\u89e3\u51b3\u4f4d\u7ea7\u7a00\u758f\u6027\u7684\u6311\u6218\u3002", "result": "\u6240\u63d0\u51fa\u7684MAC\u9635\u5217\u67b6\u6784\u7684\u7cbe\u786e\u7248\u672c\u5728\u9762\u79ef\u6548\u7387\u65b9\u9762\u6bd4\u6700\u5148\u8fdb\u7684\u4f4d\u7a00\u758f\u9a71\u52a8\u67b6\u6784\u63d0\u9ad8\u4e8629.2%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u80fd\u6e90\u6548\u7387\u3002\u8fd1\u4f3c\u53d8\u4f53\u4e0e\u7cbe\u786e\u7248\u672c\u76f8\u6bd4\uff0c\u80fd\u6e90\u6548\u7387\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e867.5%\u3002", "conclusion": "Bit-level\u7a00\u758f\u6027\u5728\u91cf\u5316\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u4e2d\u5177\u6709\u4f18\u5316\u4e58\u52a0\uff08MAC\uff09\u8fd0\u7b97\u7684\u5de8\u5927\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5b9e\u9645\u5e94\u7528\u4ecd\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u4e00\u662f\u4f20\u7edf\u4f4d\u4e32\u884c\u65b9\u6cd5\u65e0\u6cd5\u540c\u65f6\u5229\u7528\u4e24\u4e2a\u56e0\u5b50\uff08\u4e58\u6570\u548c\u88ab\u4e58\u6570\uff09\u7684\u7a00\u758f\u6027\uff0c\u5bfc\u81f4\u5176\u4e2d\u4e00\u4e2a\u56e0\u5b50\u7684\u7a00\u758f\u6027\u88ab\u6d6a\u8d39\uff0c\u800c\u65e8\u5728\u5229\u7528\u53cc\u56e0\u5b50\u7a00\u758f\u6027\u7684\u65b9\u6cd5\u4ecd\u5904\u4e8e\u63a2\u7d22\u521d\u671f\uff0c\u9762\u4e34\u90e8\u5206\u79ef\u7206\u70b8\u7684\u6311\u6218\uff1b\u4e8c\u662f\u4f4d\u7ea7\u7a00\u758f\u6027\u7684\u6ce2\u52a8\u5bfc\u81f4MAC\u8fd0\u7b97\u7684\u5468\u671f\u6570\u53ef\u53d8\uff0c\u73b0\u6709\u7684\u9002\u7528\u4e8e\u53cc\u56e0\u5b50\u7a00\u758f\u6027\u7684\u540c\u6b65\u8c03\u5ea6\u65b9\u6848\u7075\u6d3b\u6027\u5dee\uff0cMAC\u5355\u5143\u5229\u7528\u7387\u4f4e\u3002\u4e3a\u4e86\u89e3\u51b3\u7b2c\u4e00\u4e2a\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u65b0\u5174\u7684\u7c92\u5b50\u5316\u65b9\u6cd5\u6765\u5229\u7528\u53cc\u56e0\u5b50\u7a00\u758f\u6027\u7684MAC\u5355\u5143\uff0c\u901a\u8fc7\u7b80\u5355\u7684\u63a7\u5236\u903b\u8f91\u89e3\u51b3\u4e86\u90e8\u5206\u79ef\u7206\u70b8\u95ee\u9898\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u5177\u9762\u79ef\u548c\u80fd\u6e90\u6548\u76ca\u7684MAC\u5355\u5143\uff0c\u5e76\u901a\u8fc7\u4e22\u5f03\u4e0d\u592a\u91cd\u8981\u7684\u4e2d\u95f4\u7ed3\u679c\uff0c\u4ee5\u5fae\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\u4e3a\u4ee3\u4ef7\u5b9e\u73b0\u4e86\u8fdb\u4e00\u6b65\u7684\u786c\u4ef6\u7b80\u5316\u3002\u4e3a\u4e86\u89e3\u51b3\u7b2c\u4e8c\u4e2a\u6311\u6218\uff0c\u672c\u6587\u5f15\u5165\u4e86\u4e00\u79cd\u51c6\u540c\u6b65\u65b9\u6848\uff0c\u4e3aMAC\u9635\u5217\u589e\u52a0\u4e86\u5468\u671f\u7ea7\u5f39\u6027\uff0c\u51cf\u5c11\u4e86\u6d41\u6c34\u7ebf\u505c\u987f\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86MAC\u5355\u5143\u7684\u5229\u7528\u7387\u3002\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684MAC\u9635\u5217\u67b6\u6784\u7684\u7cbe\u786e\u7248\u672c\u5728\u9762\u79ef\u6548\u7387\u65b9\u9762\u6bd4\u6700\u5148\u8fdb\u7684\u4f4d\u7a00\u758f\u9a71\u52a8\u67b6\u6784\u63d0\u9ad8\u4e8629.2%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u5f53\u7684\u80fd\u6e90\u6548\u7387\u3002\u8fd1\u4f3c\u53d8\u4f53\u4e0e\u7cbe\u786e\u7248\u672c\u76f8\u6bd4\uff0c\u80fd\u6e90\u6548\u7387\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e867.5%\u3002"}}
{"id": "2507.09791", "categories": ["cond-mat.mes-hall", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09791", "abs": "https://arxiv.org/abs/2507.09791", "authors": ["Soumya Chakraborty", "Pooja Sudha", "Hemant Arora", "Daniel Moraru", "Arup Samanta"], "title": "Observation of Quantum Coulomb Blockade Facilitated by P-Donor Molecules in Silicon Nano-Transistor", "comment": null, "summary": "Multi-donor architecture developed on the base of silicon technology holds\nsignificant potential towards room-temperature qubit and other single-electron\ntunneling (SET) functionalities. However, within such architecture, the overlap\nof multiple donor wave-functions results in a complex internal electronic\nconfiguration with discrete energy levels. Probing these discrete states,\nobserved as multiple conductance peaks, is essential for understanding\ninter-donor coupling and exchange interactions towards coherent electron\ntransfer. In this direction, we have experimentally demonstrated one-by-one\nelectron filling within multiple-donor molecules with the fundamental analysis\nof clear and sustained quantum Coulomb blockade (QCB) effect. Moreover, the\nunderlying physics of molecular orbitals, where the increasing energy leads to\na larger spatial extent of the corresponding orbital, has been reflected by the\nsystematic decrement of the respective charging-energies. The molecular energy\nlevels, resulting from the orbital hybridization of individual donors, are also\nconfirmed through first-principles simulations using density functional theory\n(DFT). Furthermore, Monte Carlo simulations based on the orthodox theory of\nCoulomb blockade support the observed QCB characteristics.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u591a\u7ed9\u4f53\u7845\u57fa\u7ed3\u6784\u4e2d\u5b9e\u73b0\u4e86\u9010\u4e00\u7535\u5b50\u586b\u5145\u548c\u91cf\u5b50\u5e93\u4ed1 \u0628\u0644\u0648\u06a9 \u092a\u093e\u0939\u093f\u091c\u0947 \u6548\u5e94\u7684\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u63ed\u793a\u4e86\u5206\u5b50\u8f68\u9053\u7279\u6027\u548c\u80fd\u7ea7\u884c\u4e3a\uff0c\u5e76\u5229\u7528DFT\u548c\u8499\u7279\u5361\u7f57\u6a21\u62df\u8fdb\u884c\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u591a\u7ed9\u4f53\u7ed3\u6784\u4e2d\u7535\u5b50\u96a7\u7a7f\u548c\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\uff0c\u9700\u8981\u63a2\u6d4b\u79bb\u6563\u80fd\u7ea7\uff0c\u8fd9\u5bf9\u4e8e\u5b9e\u73b0\u5ba4\u6e29\u91cf\u5b50\u6bd4\u7279\u548c\u5176\u4ed6\u5355\u7535\u5b50\u96a7\u7a7f\uff08SET\uff09\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5b9e\u9a8c\u6f14\u793a\u4e86\u591a\u7ed9\u4f53\u5206\u5b50\u4e2d\u9010\u4e00\u7535\u5b50\u586b\u5145\uff0c\u5e76\u8fdb\u884c\u4e86\u6e05\u6670\u4e14\u6301\u7eed\u7684\u91cf\u5b50\u5e93\u4ed1 \u0628\u0644\u0648\u06a9 \u092a\u093e\u0939\u093f\u091c\u0947 \u6548\u5e94\u7684\u57fa\u672c\u5206\u6790\u3002\u5229\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8fdb\u884c\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u5e93\u4ed1 \u0628\u0644\u0648\u06a9 \u092a\u093e\u0939\u093f\u091c\u0947 orthodox \u7406\u8bba\u7684\u8499\u7279\u5361\u7f57\u6a21\u62df\u6765\u9a8c\u8bc1\u7814\u7a76\u7ed3\u679c\u3002", "result": "\u7814\u7a76\u89c2\u5bdf\u5230\u4e86\u6e05\u6670\u4e14\u6301\u7eed\u7684\u91cf\u5b50\u5e93\u4ed1 \u0628\u0644\u0648\u06a9 \u092a\u093e\u0939\u093f\u091c\u0947 \u6548\u5e94\uff0c\u5e76\u5b9e\u73b0\u4e86\u591a\u7ed9\u4f53\u5206\u5b50\u4e2d\u9010\u4e00\u7535\u5b50\u586b\u5145\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u968f\u7740\u80fd\u91cf\u589e\u52a0\uff0c\u5206\u5b50\u8f68\u9053\u7684\u7a7a\u95f4\u8303\u56f4\u589e\u5927\uff0c\u76f8\u5e94\u7684\u5145\u7535\u80fd\u91cf\u7cfb\u7edf\u6027\u5730\u51cf\u5c0f\u3002", "conclusion": "\u7814\u7a76\u786e\u8ba4\u4e86\u5206\u5b50\u8f68\u9053\u6742\u5316\u4ea7\u751f\u7684\u5206\u5b50\u80fd\u7ea7\uff0c\u5e76\u5229\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8fdb\u884c\u7b2c\u4e00\u6027\u539f\u7406\u6a21\u62df\u9a8c\u8bc1\u3002\u6b64\u5916\uff0c\u57fa\u4e8e\u5e93\u4ed1 \u0628\u0644\u0648\u06a9 \u092a\u093e\u0939\u093f\u091c\u0947 orthodox \u7406\u8bba\u7684\u8499\u7279\u5361\u7f57\u6a21\u62df\u4e5f\u652f\u6301\u89c2\u5bdf\u5230\u7684\u91cf\u5b50\u5e93\u4ed1 \u0628\u0644\u0648\u06a9 \u092a\u093e\u0939\u093f\u091c\u0947 \u7279\u5f81\u3002"}}
{"id": "2507.09503", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09503", "abs": "https://arxiv.org/abs/2507.09503", "authors": ["Zhentong Shao", "Jingtao Qin", "Nanpeng Yu"], "title": "Neural Two-Stage Stochastic Optimization for Solving Unit Commitment Problem", "comment": "Submitted to IEEE Transactions on Power Systems", "summary": "This paper proposes a neural stochastic optimization method for efficiently\nsolving the two-stage stochastic unit commitment (2S-SUC) problem under\nhigh-dimensional uncertainty scenarios. The proposed method approximates the\nsecond-stage recourse problem using a deep neural network trained to map\ncommitment decisions and uncertainty features to recourse costs. The trained\nnetwork is subsequently embedded into the first-stage UC problem as a\nmixed-integer linear program (MILP), allowing for explicit enforcement of\noperational constraints while preserving the key uncertainty characteristics. A\nscenario-embedding network is employed to enable dimensionality reduction and\nfeature aggregation across arbitrary scenario sets, serving as a data-driven\nscenario reduction mechanism. Numerical experiments on IEEE 5-bus, 30-bus, and\n118-bus systems demonstrate that the proposed neural two-stage stochastic\noptimization method achieves solutions with an optimality gap of less than 1%,\nwhile enabling orders-of-magnitude speedup compared to conventional MILP\nsolvers and decomposition-based methods. Moreover, the model's size remains\nconstant regardless of the number of scenarios, offering significant\nscalability for large-scale stochastic unit commitment problems.", "AI": {"tldr": "\u795e\u7ecf\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u53ef\u9ad8\u6548\u6c42\u89e3\u5927\u89c4\u6a21\u968f\u673a\u5355\u4f4d\u627f\u8bfa\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u9ad8\u6548\u89e3\u51b3\u9ad8\u7ef4\u4e0d\u786e\u5b9a\u6027\u573a\u666f\u4e0b\u7684\u4e24\u9636\u6bb5\u968f\u673a\u5355\u4f4d\u627f\u8bfa\uff082S-SUC\uff09\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u968f\u673a\u4f18\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u903c\u8fd1\u4e24\u9636\u6bb5\u968f\u673a\u5355\u4f4d\u627f\u8bfa\u95ee\u9898\uff082S-SUC\uff09\u7684\u7b2c\u4e8c\u9636\u6bb5\u8ffd\u7d22\u95ee\u9898\uff0c\u5c06\u8bad\u7ec3\u597d\u7684\u7f51\u7edc\u5d4c\u5165\u7b2c\u4e00\u9636\u6bb5\u7684\u5355\u4f4d\u627f\u8bfa\u95ee\u9898\u4e2d\uff0c\u5f62\u6210\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u3002\u540c\u65f6\uff0c\u91c7\u7528\u573a\u666f\u5d4c\u5165\u7f51\u7edc\u8fdb\u884c\u964d\u7ef4\u548c\u7279\u5f81\u805a\u5408\uff0c\u5b9e\u73b0\u6570\u636e\u9a71\u52a8\u7684\u573a\u666f\u7ea6\u51cf\u3002", "result": "\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u795e\u7ecf\u968f\u673a\u4f18\u5316\u65b9\u6cd5\u5728IEEE 5\u8282\u70b9\u300130\u8282\u70b9\u548c118\u8282\u70b9\u7cfb\u7edf\u4e0a\uff0c\u53d6\u5f97\u4e86\u4f18\u5316\u5dee\u8ddd\u5c0f\u4e8e1%\u7684\u89e3\uff0c\u5e76\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edfMILP\u6c42\u89e3\u5668\u548c\u57fa\u4e8e\u5206\u89e3\u7684\u65b9\u6cd5\u5feb\u51e0\u4e2a\u6570\u91cf\u7ea7\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u8bc1\u89e3\u7684\u4f18\u5316\u5ea6\u5c0f\u4e8e1%\u7684\u60c5\u51b5\u4e0b\uff0c\u76f8\u6bd4\u4e8e\u4f20\u7edf\u65b9\u6cd5\u5728\u6c42\u89e3\u901f\u5ea6\u4e0a\u63d0\u5347\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u4e14\u6a21\u578b\u89c4\u6a21\u4e0d\u968f\u573a\u666f\u6570\u91cf\u589e\u52a0\u800c\u589e\u52a0\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.09160", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09160", "abs": "https://arxiv.org/abs/2507.09160", "authors": ["Jialei Huang", "Shuo Wang", "Fanqi Lin", "Yihang Hu", "Chuan Wen", "Yang Gao"], "title": "Tactile-VLA: Unlocking Vision-Language-Action Model's Physical Knowledge for Tactile Generalization", "comment": null, "summary": "Vision-Language-Action (VLA) models have shown remarkable achievements,\ndriven by the rich implicit knowledge of their vision-language components.\nHowever, achieving generalist robotic agents demands precise grounding into\nphysical interactions, especially in contact-rich scenarios where fine-grained\nforce control is essential. We advance VLAs' implicit knowledge beyond\nidentifying what to do, towards guiding how to physically interact with real\nworld. This paper introduces Tactile-VLA, a novel framework that deeply fuses\nvision, language, action, and tactile sensing. This framework incorporates a\nhybrid position-force controller to translate the model's intentions into\nprecise physical actions and a reasoning module that allows the robot to adapt\nits strategy based on tactile feedback. Experiments demonstrate Tactile-VLA's\neffectiveness and generalizability in three key aspects: (1) enabling\ntactile-aware instruction following, (2) utilizing tactile-relevant\ncommonsense, and (3) facilitating adaptive tactile-involved reasoning. A key\nfinding is that the VLM's prior knowledge already contains semantic\nunderstanding of physical interaction; by connecting it to the robot's tactile\nsensors with only a few demonstrations, we can activate this prior knowledge to\nachieve zero-shot generalization in contact-rich tasks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09036", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.09036", "abs": "https://arxiv.org/abs/2507.09036", "authors": ["Florian Kofler", "Marcel Rosier", "Mehdi Astaraki", "Hendrik M\u00f6ller", "Ilhem Isra Mekki", "Josef A. Buchner", "Anton Schmick", "Arianna Pfiffer", "Eva Oswald", "Lucas Zimmer", "Ezequiel de la Rosa", "Sarthak Pati", "Julian Canisius", "Arianna Piffer", "Ujjwal Baid", "Mahyar Valizadeh", "Akis Linardos", "Jan C. Peeken", "Surprosanna Shit", "Felix Steinbauer", "Daniel Rueckert", "Rolf Heckemann", "Spyridon Bakas", "Jan Kirschke", "Constantin von See", "Ivan Ezhov", "Marie Piraud", "Benedikt Wiestler", "Bjoern Menze"], "title": "BrainLesion Suite: A Flexible and User-Friendly Framework for Modular Brain Lesion Image Analysis", "comment": "16p, 3f", "summary": "BrainLesion Suite is a versatile toolkit for building modular brain lesion\nimage analysis pipelines in Python. Following Pythonic principles, BrainLesion\nSuite is designed to provide a 'brainless' development experience, minimizing\ncognitive effort and streamlining the creation of complex workflows for\nclinical and scientific practice. At its core is an adaptable preprocessing\nmodule that performs co-registration, atlas registration, and optional\nskull-stripping and defacing on arbitrary multi-modal input images. BrainLesion\nSuite leverages algorithms from the BraTS challenge to synthesize missing\nmodalities, inpaint lesions, and generate pathology-specific tumor\nsegmentations. BrainLesion Suite also enables quantifying segmentation model\nperformance, with tools such as panoptica to compute lesion-wise metrics.\nAlthough BrainLesion Suite was originally developed for image analysis\npipelines of brain lesions such as glioma, metastasis, and multiple sclerosis,\nit can be adapted for other biomedical image analysis applications. The\nindividual BrainLesion Suite packages and tutorials are accessible on GitHub.", "AI": {"tldr": "BrainLesion Suite \u662f\u4e00\u4e2a Python \u5de5\u5177\u5305\uff0c\u7528\u4e8e\u7b80\u5316\u8111\u90e8\u75c5\u53d8\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\u7684\u6784\u5efa\uff0c\u652f\u6301\u9884\u5904\u7406\u3001\u6a21\u6001\u5408\u6210\u3001\u75c5\u53d8\u5206\u5272\u548c\u6027\u80fd\u91cf\u5316\uff0c\u5e76\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u751f\u7269\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u3002", "motivation": "BrainLesion Suite \u7684\u5f00\u53d1\u65e8\u5728\u4e3a\u8111\u90e8\u75c5\u53d8\u56fe\u50cf\u5206\u6790\u63d0\u4f9b\u4e00\u4e2a\u6a21\u5757\u5316\u3001\u6613\u4e8e\u4f7f\u7528\u7684 Python \u5de5\u5177\u5305\uff0c\u4ee5\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8ba4\u77e5\u8d1f\u62c5\uff0c\u7b80\u5316\u590d\u6742\u5de5\u4f5c\u6d41\u7a0b\u7684\u521b\u5efa\uff0c\u4ece\u800c\u4fc3\u8fdb\u4e34\u5e8a\u548c\u79d1\u5b66\u5b9e\u8df5\u3002", "method": "BrainLesion Suite \u91c7\u7528 Pythonic \u539f\u5219\u8bbe\u8ba1\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6613\u4e8e\u4f7f\u7528\u7684\u5f00\u53d1\u4f53\u9a8c\uff0c\u5176\u4e2d\u5305\u62ec\u4e00\u4e2a\u53ef\u9002\u5e94\u7684\u9884\u5904\u7406\u6a21\u5757\uff0c\u7528\u4e8e\u6267\u884c\u5171\u914d\u51c6\u3001\u56fe\u8c31\u914d\u51c6\u3001\u53bb\u9aa8\u548c\u53bb\u9762\u7b49\u64cd\u4f5c\u3002\u5b83\u8fd8\u5229\u7528 BraTS \u6311\u6218\u4e2d\u7684\u7b97\u6cd5\u6765\u5408\u6210\u7f3a\u5931\u7684\u6a21\u6001\u3001\u4fee\u590d\u75c5\u53d8\u5e76\u751f\u6210\u7279\u5b9a\u4e8e\u75c5\u7406\u7684\u80bf\u7624\u5206\u5272\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u4f9b\u50cf panoptica \u8fd9\u6837\u7684\u5de5\u5177\u6765\u8ba1\u7b97\u9010\u75c5\u53d8\u5ea6\u91cf\uff0c\u4ee5\u91cf\u5316\u5206\u5272\u6a21\u578b\u7684\u6027\u80fd\u3002", "result": "BrainLesion Suite \u662f\u4e00\u4e2a\u7075\u6d3b\u7684\u5de5\u5177\u5305\uff0c\u652f\u6301\u4ece\u5171\u914d\u51c6\u3001\u56fe\u8c31\u914d\u51c6\u5230\u75c5\u53d8\u5206\u5272\u548c\u6027\u80fd\u91cf\u5316\u7684\u5404\u79cd\u8111\u90e8\u75c5\u53d8\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u3002\u5b83\u901a\u8fc7\u5229\u7528 BraTS \u6311\u6218\u4e2d\u7684\u7b97\u6cd5\uff0c\u80fd\u591f\u5408\u6210\u7f3a\u5931\u6a21\u6001\u3001\u4fee\u590d\u75c5\u53d8\u548c\u751f\u6210\u80bf\u7624\u5206\u5272\uff0c\u5e76\u63d0\u4f9b\u9010\u75c5\u53d8\u5ea6\u91cf\u4ee5\u8bc4\u4f30\u5206\u5272\u6a21\u578b\u6027\u80fd\u3002", "conclusion": "BrainLesion Suite \u662f\u4e00\u4e2a\u591a\u529f\u80fd\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u5728 Python \u4e2d\u6784\u5efa\u6a21\u5757\u5316\u7684\u8111\u90e8\u75c5\u53d8\u56fe\u50cf\u5206\u6790\u6d41\u7a0b\u3002\u5b83\u5141\u8bb8\u7528\u6237\u901a\u8fc7\u53ef\u9002\u5e94\u7684\u9884\u5904\u7406\u6a21\u5757\u3001\u5229\u7528 BraTS \u6311\u6218\u4e2d\u7684\u7b97\u6cd5\u8fdb\u884c\u6a21\u6001\u5408\u6210\u548c\u5206\u5272\uff0c\u4ee5\u53ca\u4f7f\u7528 panoptica \u7b49\u5de5\u5177\u91cf\u5316\u5206\u5272\u6a21\u578b\u6027\u80fd\uff0c\u6765\u7b80\u5316\u590d\u6742\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002\u8be5\u5de5\u5177\u5305\u6700\u521d\u662f\u4e3a\u4e86\u5206\u6790\u80f6\u8d28\u7624\u3001\u8f6c\u79fb\u7624\u548c\u591a\u53d1\u6027\u786c\u5316\u75c7\u7b49\u8111\u90e8\u75c5\u53d8\u800c\u5f00\u53d1\u7684\uff0c\u4f46\u4e5f\u53ef\u5e94\u7528\u4e8e\u5176\u4ed6\u751f\u7269\u533b\u5b66\u56fe\u50cf\u5206\u6790\u9886\u57df\u3002"}}
{"id": "2507.10150", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10150", "abs": "https://arxiv.org/abs/2507.10150", "authors": ["Ruihao Gong", "Shihao Bai", "Siyu Wu", "Yunqian Fan", "Zaijun Wang", "Xiuhong Li", "Hailong Yang", "Xianglong Liu"], "title": "Past-Future Scheduler for LLM Serving under SLA Guarantees", "comment": "Accepted to ASPLOS 2025", "summary": "The exploration and application of Large Language Models (LLMs) is thriving.\nTo reduce deployment costs, continuous batching has become an essential feature\nin current service frameworks. The effectiveness of continuous batching relies\non an accurate estimate of the memory requirements of requests. However, due to\nthe diversity in request output lengths, existing frameworks tend to adopt\naggressive or conservative schedulers, which often result in significant\noverestimation or underestimation of memory consumption. Consequently, they\nsuffer from harmful request evictions or prolonged queuing times, failing to\nachieve satisfactory throughput under strict Service Level Agreement (SLA)\nguarantees (a.k.a. goodput), across various LLM application scenarios with\ndiffering input-output length distributions. To address this issue, we propose\na novel Past-Future scheduler that precisely estimates the peak memory\nresources required by the running batch via considering the historical\ndistribution of request output lengths and calculating memory occupancy at each\nfuture time point. It adapts to applications with all types of input-output\nlength distributions, balancing the trade-off between request queuing and\nharmful evictions, thereby consistently achieving better goodput. Furthermore,\nto validate the effectiveness of the proposed scheduler, we developed a\nhigh-performance LLM serving framework, LightLLM, that implements the\nPast-Future scheduler. Compared to existing aggressive or conservative\nschedulers, LightLLM demonstrates superior goodput, achieving up to 2-3$\\times$\nhigher goodput than other schedulers under heavy loads. LightLLM is open source\nto boost the research in such direction (https://github.com/ModelTC/lightllm).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Past-Future\u8c03\u5ea6\u5668\u548cLightLLM\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709LLM\u670d\u52a1\u6846\u67b6\u4e2d\u8fde\u7eed\u6279\u5904\u7406\u5185\u5b58\u4f30\u8ba1\u4e0d\u51c6\u786e\u7684\u95ee\u9898\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002", "motivation": "\u4e3a\u4e86\u964d\u4f4e\u90e8\u7f72\u6210\u672c\uff0c\u8fde\u7eed\u6279\u5904\u7406\u5df2\u6210\u4e3a\u5f53\u524d\u670d\u52a1\u6846\u67b6\u4e2d\u7684\u4e00\u9879\u57fa\u672c\u529f\u80fd\u3002\u8fde\u7eed\u6279\u5904\u7406\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u5bf9\u8bf7\u6c42\u5185\u5b58\u9700\u6c42\u7684\u51c6\u786e\u4f30\u8ba1\u3002\u7136\u800c\uff0c\u7531\u4e8e\u8bf7\u6c42\u8f93\u51fa\u957f\u5ea6\u7684\u591a\u6837\u6027\uff0c\u73b0\u6709\u6846\u67b6\u503e\u5411\u4e8e\u91c7\u7528\u6fc0\u8fdb\u6216\u4fdd\u5b88\u7684\u8c03\u5ea6\u5668\uff0c\u8fd9\u5f80\u5f80\u5bfc\u81f4\u5185\u5b58\u6d88\u8017\u7684\u4e25\u91cd\u9ad8\u4f30\u6216\u4f4e\u4f30\uff0c\u4ece\u800c\u65e0\u6cd5\u5728\u4e25\u683c\u7684\u670d\u52a1\u6c34\u5e73\u534f\u8bae\uff08SLA\uff09\u4fdd\u8bc1\u4e0b\u5b9e\u73b0\u4ee4\u4eba\u6ee1\u610f\u7684\u541e\u5410\u91cf\uff08\u5373\u597dput\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPast-Future\u7684\u65b0\u8c03\u5ea6\u5668\uff0c\u8be5\u8c03\u5ea6\u5668\u901a\u8fc7\u8003\u8651\u8bf7\u6c42\u8f93\u51fa\u957f\u5ea6\u7684\u5386\u53f2\u5206\u5e03\u548c\u8ba1\u7b97\u672a\u6765\u6bcf\u4e2a\u65f6\u95f4\u70b9\u7684\u5185\u5b58\u5360\u7528\u60c5\u51b5\u6765\u7cbe\u786e\u4f30\u8ba1\u6240\u9700\u7684\u5cf0\u503c\u5185\u5b58\u8d44\u6e90\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u6fc0\u8fdb\u6216\u4fdd\u5b88\u8c03\u5ea6\u5668\u76f8\u6bd4\uff0cLightLLM\u5b9e\u73b0\u4e86\u9ad8\u8fbe2-3\u500d\u7684\u66f4\u9ad8\u541e\u5410\u91cf\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u597dput\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aPast-Future\u7684\u65b0\u8c03\u5ea6\u5668\uff0c\u8be5\u8c03\u5ea6\u5668\u901a\u8fc7\u8003\u8651\u8bf7\u6c42\u8f93\u51fa\u957f\u5ea6\u7684\u5386\u53f2\u5206\u5e03\u548c\u8ba1\u7b97\u672a\u6765\u6bcf\u4e2a\u65f6\u95f4\u70b9\u7684\u5185\u5b58\u5360\u7528\u60c5\u51b5\uff0c\u7cbe\u786e\u4f30\u8ba1\u8fd0\u884c\u6279\u6b21\u6240\u9700\u7684\u5cf0\u503c\u5185\u5b58\u8d44\u6e90\u3002\u8be5\u8c03\u5ea6\u5668\u80fd\u591f\u9002\u5e94\u5404\u79cd\u8f93\u5165\u8f93\u51fa\u957f\u5ea6\u5206\u5e03\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c\u5e73\u8861\u8bf7\u6c42\u6392\u961f\u548c\u6709\u5bb3\u9a71\u9010\u4e4b\u95f4\u7684\u6743\u8861\uff0c\u4ece\u800c\u6301\u7eed\u5b9e\u73b0\u66f4\u597d\u7684\u541e\u5410\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aLightLLM\u7684\u9ad8\u6027\u80fdLLM\u670d\u52a1\u6846\u67b6\u6765\u5b9e\u73b0\u8be5\u8c03\u5ea6\u5668\uff0c\u4e0e\u73b0\u6709\u7684\u6fc0\u8fdb\u6216\u4fdd\u5b88\u8c03\u5ea6\u5668\u76f8\u6bd4\uff0cLightLLM\u5728\u91cd\u8d1f\u8f7d\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe2-3\u500d\u7684\u66f4\u9ad8\u541e\u5410\u91cf\u3002"}}
{"id": "2507.09275", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09275", "abs": "https://arxiv.org/abs/2507.09275", "authors": ["Julia Kucharek", "Mateusz Raczy\u0144ski", "Rafa\u0142 Bo\u017cek", "Anna Kaleta", "Bogus\u0142awa Kurowska", "Marta Bilska", "S\u0142awomir Kret", "Takashi Taniguchi", "Kenji Watanabe", "Piotr Kossacki", "Mateusz Goryca", "Wojciech Pacuski"], "title": "$WSe_2$ Monolayers Grown by Molecular Beam Epitaxy on hBN", "comment": "Article and SI, 18 pages in total, 10 figures in total", "summary": "A three-step process was developed for growing high-quality, optically\nuniform WSe2 monolayers by molecular beam epitaxy (MBE) with advantage of using\nhexagonal boron nitride (hBN). The process was optimized to maximize the\nefficiency of photoluminescence and promote formation of hexagonal WSe2\ndomains. Atomic force microscopy (AFM) was employed to estimate the dispersion\nof WSe2 hexagonal domains orientation. Monolayer character of the film was\nidentified using optical methods and verified with high-resolution transmission\nelectron microscopy (TEM) cross-section.\nTemperature-and-magnetic-field-dependent studies revealed the behaviour of\nexciton complexes to be analogical to that of exfoliated counterparts. Direct\ngrowth on hBN combined with uniform optical response proves this WSe2 superior\nto mechanically exfoliated WSe2 in terms of convenience of use and\nreproducibility. Provided results establish a significant progress in optical\nquality of epitaxially grown transition metal dichalcogenides (TMDs) monolayers\nand fabrication of large-scale functional devices.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u5916\u5ef6\u751f\u957f\u5de5\u827a\uff0c\u6210\u529f\u5236\u5907\u51fa\u9ad8\u8d28\u91cf\u3001\u5149\u5b66\u5747\u5300\u7684WSe2\u5355\u5206\u5b50\u5c42\uff0c\u6027\u80fd\u4f18\u4e8e\u673a\u68b0\u5265\u79bb\u6cd5\uff0c\u4e3a\u5668\u4ef6\u5236\u9020\u5e26\u6765\u8fdb\u6b65\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5916\u5ef6\u751f\u957f\u8fc7\u6e21\u91d1\u5c5e\u786b\u65cf\u5316\u5408\u7269\uff08TMDs\uff09\u5355\u5206\u5b50\u5c42\u7684\u5149\u5b66\u8d28\u91cf\uff0c\u5e76\u4e3a\u5927\u89c4\u6a21\u529f\u80fd\u5668\u4ef6\u7684\u5236\u9020\u63d0\u4f9b\u66f4\u4fbf\u6377\u3001\u53ef\u91cd\u590d\u6027\u66f4\u9ad8\u7684\u65b9\u6cd5\u3002", "method": "\u91c7\u7528\u5916\u5ef6\u751f\u957f\u6280\u672f\uff0c\u5229\u7528\u516d\u65b9\u6c2e\u5316\u787c\u4f5c\u4e3a\u886c\u5e95\uff0c\u901a\u8fc7\u4e09\u6b65\u5de5\u827a\u4f18\u5316\u4e86WSe2\u5355\u5206\u5b50\u5c42\u7684\u751f\u957f\uff0c\u5e76\u5229\u7528\u539f\u5b50\u529b\u663e\u5fae\u955c\u548c\u9ad8\u5206\u8fa8\u7387\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\u5bf9\u5176\u5f62\u8c8c\u548c\u5355\u5206\u5b50\u5c42\u7279\u6027\u8fdb\u884c\u4e86\u8868\u5f81\u3002", "result": "\u6210\u529f\u5236\u5907\u4e86\u5149\u5b66\u5747\u5300\u3001\u9ad8\u8d28\u91cf\u7684WSe2\u5355\u5206\u5b50\u5c42\uff0c\u5176\u6fc0\u5b50\u590d\u5408\u7269\u7684\u884c\u4e3a\u4e0e\u673a\u68b0\u5265\u79bb\u7684\u5bf9\u5e94\u7269\u76f8\u4f3c\uff0c\u5e76\u4e14\u5728\u6613\u7528\u6027\u548c\u53ef\u91cd\u590d\u6027\u65b9\u9762\u4f18\u4e8e\u673a\u68b0\u5265\u79bb\u7684WSe2\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728\u5229\u7528\u5916\u5ef6\u751f\u957f\u65b9\u6cd5\u5236\u5907\u9ad8\u8d28\u91cf\u3001\u5149\u5b66\u5747\u5300\u7684WSe2\u5355\u5206\u5b50\u5c42\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u8981\u8fdb\u5c55\uff0c\u4e3a\u5927\u89c4\u6a21\u529f\u80fd\u5668\u4ef6\u7684\u5236\u9020\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.09034", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09034", "abs": "https://arxiv.org/abs/2507.09034", "authors": ["Abdolreza Pasharavesh", "Sai Sreesh Venuturumilli", "Michal Bajcsy"], "title": "Photon-Number-Resolving Detector Based on a Cascade of Waveguide-Coupled Quantum Emitters", "comment": null, "summary": "We investigate the operation of a photon-number-resolving (PNR) detector\nconsisting of a cascade of waveguide-coupled lambda-type emitters, where each\nwaveguide-coupled emitter extracts a single photon from the input light and\nsends it to a single-photon detector. Using Green's function and input-output\nformalisms, we derive the scattering matrices and photon-photon correlators for\nindividual scatterers. By cascading these results, we obtain a closed-form\nexpression for the detector's precision in the linear regime and predict how\ncorrelations generated by nonlinear photon-photon interactions influence this\nprecision. To evaluate the performance of this PNR detector in the nonlinear\nregime, we apply the quantum trajectory method to the cascaded setup,\ncalculating the achievable precision and analyzing its dependence on key system\nparameters, such as the number of emitters and their coupling strength to the\nwaveguide. We compare the performance of the proposed PNR detector with that of\na conventional PNR scheme based on spatial demultiplexing via beamsplitters.\nOur results indicate that the proposed scheme can outperform conventional\ndetectors under realistic conditions, making it a promising candidate for\nnext-generation PNR detection.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u5149\u5b50\u6570\u63a2\u6d4b\u5668\uff0c\u901a\u8fc7\u7ea7\u8054\u6ce2\u5bfc\u8026\u5408\u7684\u03bb\u578b\u53d1\u5c04\u5668\u5b9e\u73b0\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u7814\u7a76\u7531\u4e00\u7cfb\u5217\u6ce2\u5bfc\u8026\u5408\u7684\u03bb\u578b\u53d1\u5c04\u5668\u7ec4\u6210\u7684\u5355\u5149\u5b50\u6570\u5206\u8fa8\uff08PNR\uff09\u63a2\u6d4b\u5668\u7684\u64cd\u4f5c\u3002", "method": "\u4f7f\u7528\u683c\u6797\u51fd\u6570\u548c\u8f93\u5165-\u8f93\u51fa\u5f62\u5f0f\u4e3b\u4e49\u63a8\u5bfc\u4e86\u6563\u5c04\u77e9\u9635\u548c\u5149\u5b50-\u5149\u5b50\u76f8\u5173\u5668\uff0c\u5e76\u4f7f\u7528\u91cf\u5b50\u8f68\u8ff9\u65b9\u6cd5\u8ba1\u7b97\u4e86\u53ef\u5b9e\u73b0\u7684\u7cbe\u5ea6\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u5173\u952e\u7cfb\u7edf\u53c2\u6570\u7684\u4f9d\u8d56\u6027\u3002", "result": "\u6211\u4eec\u83b7\u5f97\u4e86\u63a2\u6d4b\u5668\u5728\u7ebf\u6027\u533a\u57df\u7684\u7cbe\u5ea6\u8868\u8fbe\u5f0f\uff0c\u5e76\u9884\u6d4b\u4e86\u7531\u975e\u7ebf\u6027\u5149\u5b50-\u5149\u5b50\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f \u7684\u76f8\u5173\u6027\u5982\u4f55\u5f71\u54cd\u8be5\u7cbe\u5ea6\u3002\u6211\u4eec\u8fd8\u901a\u8fc7\u91cf\u5b50\u8f68\u8ff9\u65b9\u6cd5\u8ba1\u7b97\u4e86\u975e\u7ebf\u6027\u533a\u57df\u4e2d\u7684\u53ef\u5b9e\u73b0\u7cbe\u5ea6\uff0c\u5e76\u5c06\u5176\u4e0e\u57fa\u4e8e\u5206\u675f\u5668\u7684\u7a7a\u95f4\u89e3\u590d\u7528\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7ea7\u8054\u63a2\u6d4b\u5668\u5728\u73b0\u5b9e\u6761\u4ef6\u4e0b\u4f18\u4e8e\u4f20\u7edf\u63a2\u6d4b\u5668\uff0c\u662f\u4e0b\u4e00\u4ee3\u5149\u5b50\u6570\u63a2\u6d4b\u5668\u7684\u6709\u529b\u5019\u9009\u8005\u3002"}}
{"id": "2507.09011", "categories": ["cs.CL", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09011", "abs": "https://arxiv.org/abs/2507.09011", "authors": ["Ana Chkhaidze", "Reshanne R. Reeder", "Connor Gag", "Anastasia Kiyonaga", "Seana Coulson"], "title": "Beyond vividness: Content analysis of induced hallucinations reveals the hidden structure of individual differences in visual imagery", "comment": null, "summary": "A rapidly alternating red and black display known as Ganzflicker induces\nvisual hallucinations that reflect the generative capacity of the visual\nsystem. Recent proposals regarding the imagery spectrum, that is, differences\nin the visual system of individuals with absent imagery, typical imagery, and\nvivid imagery, suggest these differences should impact the complexity of other\ninternally generated visual experiences. Here, we used tools from natural\nlanguage processing to analyze free-text descriptions of hallucinations from\nover 4,000 participants, asking whether people with different imagery\nphenotypes see different things in their mind's eye during Ganzflicker-induced\nhallucinations. Strong imagers described complex, naturalistic content, while\nweak imagers reported simple geometric patterns. Embeddings from vision\nlanguage models better captured these differences than text-only language\nmodels, and participants with stronger imagery used language with richer\nsensorimotor associations. These findings may reflect individual variation in\ncoordination between early visual areas and higher-order regions relevant for\nthe imagery spectrum.", "AI": {"tldr": "\u4e2a\u4f53\u5728\u89c6\u89c9\u60f3\u8c61\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\u4f1a\u5f71\u54cd\u5176\u5728Ganzflicker\u5e7b\u89c9\u4e2d\u6240\u89c1\u5185\u5bb9\u7684\u590d\u6742\u6027\uff1a\u5f3a\u60f3\u8c61\u529b\u8005\u89c1\u81ea\u7136\u5185\u5bb9\uff0c\u5f31\u60f3\u8c61\u529b\u8005\u89c1\u51e0\u4f55\u56fe\u6848\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u80fd\u66f4\u597d\u5730\u6355\u6349\u8fd9\u4e9b\u5dee\u5f02\u3002", "motivation": "\u63a2\u7a76\u89c6\u89c9\u60f3\u8c61\u80fd\u529b\uff08\u60f3\u8c61\u8c31\uff09\u7684\u4e2a\u4f53\u5dee\u5f02\u662f\u5426\u4f1a\u5f71\u54cdGanzflicker\u8bf1\u5bfc\u7684\u89c6\u89c9\u5e7b\u89c9\u7684\u590d\u6742\u6027\uff0c\u4ee5\u53ca\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u6355\u6349\u8fd9\u4e9b\u5dee\u5f02\u65b9\u9762\u7684\u80fd\u529b\u3002", "method": "\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5de5\u5177\u5206\u6790\u4e86\u8d85\u8fc74000\u540d\u53c2\u4e0e\u8005\u5bf9Ganzflicker\u8bf1\u5bfc\u7684\u5e7b\u89c9\u7684\u81ea\u7531\u6587\u672c\u63cf\u8ff0\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e0d\u540c\u89c6\u89c9\u60f3\u8c61\u8868\u578b\u4e2a\u4f53\u62a5\u544a\u5185\u5bb9\u7684\u5dee\u5f02\u3002\u7814\u7a76\u8fd8\u4f7f\u7528\u4e86\u6765\u81ea\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5d4c\u5165\u6765\u6355\u6349\u8fd9\u4e9b\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u4e86\u53c2\u4e0e\u8005\u8bed\u8a00\u4e2d\u4f20\u611f\u5668\u8fd0\u52a8\u8054\u60f3\u7684\u4e30\u5bcc\u6027\u3002", "result": "\u5f3a\u60f3\u8c61\u529b\u4e2a\u4f53\u5728Ganzflicker\u5e7b\u89c9\u4e2d\u62a5\u544a\u4e86\u590d\u6742\u3001\u81ea\u7136\u7684\u5185\u5bb9\uff0c\u800c\u5f31\u60f3\u8c61\u529b\u4e2a\u4f53\u62a5\u544a\u4e86\u7b80\u5355\u7684\u51e0\u4f55\u56fe\u6848\u3002\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6bd4\u4ec5\u6587\u672c\u7684\u6a21\u578b\u66f4\u80fd\u6355\u6349\u5230\u8fd9\u4e9b\u5dee\u5f02\u3002\u5f3a\u60f3\u8c61\u529b\u4e2a\u4f53\u4f7f\u7528\u7684\u8bed\u8a00\u5177\u6709\u66f4\u4e30\u5bcc\u7684\u4f20\u611f\u5668\u8fd0\u52a8\u8054\u60f3\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u4e2a\u4f53\u5728\u89c6\u89c9\u60f3\u8c61\u80fd\u529b\u4e0a\u7684\u5dee\u5f02\uff08\u57fa\u4e8e\u201c\u60f3\u8c61\u8c31\u201d\u7684\u5206\u7c7b\uff09\u4f1a\u5f71\u54cd\u4ed6\u4eec\u5728Ganzflicker\u8bf1\u5bfc\u7684\u89c6\u89c9\u5e7b\u89c9\u4e2d\u6240\u7ecf\u5386\u5185\u5bb9\u7684\u590d\u6742\u6027\u3002\u5f3a\u60f3\u8c61\u529b\u4e2a\u4f53\u63cf\u8ff0\u4e86\u66f4\u590d\u6742\u3001\u66f4\u81ea\u7136\u7684\u5e7b\u89c9\u5185\u5bb9\uff0c\u800c\u5f31\u60f3\u8c61\u529b\u4e2a\u4f53\u5219\u62a5\u544a\u4e86\u7b80\u5355\u7684\u51e0\u4f55\u56fe\u6848\u3002"}}
{"id": "2507.09902", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09902", "abs": "https://arxiv.org/abs/2507.09902", "authors": ["Yuanhao Wang"], "title": "Tie-breaking Agnostic Lower Bound for Fictitious Play", "comment": null, "summary": "Fictitious play (FP) is a natural learning dynamic in two-player zero-sum\ngames. Samuel Karlin conjectured in 1959 that FP converges at a rate of\n$O(t^{-1/2})$ to Nash equilibrium, where $t$ is the number of steps played.\nHowever, Daskalakis and Pan disproved the stronger form of this conjecture in\n2014, where \\emph{adversarial} tie-breaking is allowed.\n  This paper disproves Karlin's conjecture in its weaker form. In particular,\nthere exists a 10-by-10 zero-sum matrix game, in which FP converges at a rate\nof $\\Omega(t^{-1/3})$, and no ties occur except for the first step.", "AI": {"tldr": "Karlin\u5173\u4e8eFictitious play\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u7684\u731c\u60f3\u88ab\u53cd\u9a73\u4e86\uff0c\u5b58\u5728\u4e00\u4e2a\u96f6\u548c\u535a\u5f08\uff0c\u5176\u6536\u655b\u901f\u5ea6\u6bd4\u731c\u60f3\u7684\u6162\u3002", "motivation": "\u53cd\u9a73Samuel Karlin\u57281959\u5e74\u63d0\u51fa\u7684\u5173\u4e8eFictitious play\uff08FP\uff09\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u7684\u731c\u60f3\uff0c\u7279\u522b\u662f\u5176\u5728\u5141\u8bb8\u7684\u5bf9\u7b56\u4e0b\u7684\u6536\u655b\u7387\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a10x10\u7684\u96f6\u548c\u77e9\u9635\u535a\u5f08\uff0c\u8bc1\u660e\u4e86FP\u7684\u6536\u655b\u901f\u5ea6\u53ef\u4ee5\u8fbe\u5230$\\\\(t^{-1/3}$\uff09\uff0c\u4e0eKarlin\u731c\u60f3\u7684$O(t^{-1/2})$\u6536\u655b\u7387\u4e0d\u7b26\u3002", "result": "\u5b58\u5728\u4e00\u4e2a10x10\u7684\u96f6\u548c\u77e9\u9635\u535a\u5f08\uff0c\u5176\u4e2dFP\u7684\u6536\u655b\u901f\u5ea6\u4e3a$\\\\(t^{-1/3}$\uff09\uff0c\u5e76\u4e14\u9664\u4e86\u7b2c\u4e00\u6b65\u4e4b\u5916\u6ca1\u6709\u51fa\u73b0\u5e73\u5c40\u3002", "conclusion": "\u8be5\u8bba\u6587\u53cd\u9a73\u4e86Karlin\u7684\u731c\u60f3\uff0c\u8bc1\u660e\u4e86\u5728\u5141\u8bb8\u7684\u5bf9\u7b56\uff08\u5305\u62ec\u4e0d\u5141\u8bb8\u7684\u5bf9\u7b56\uff09\u7684\u60c5\u51b5\u4e0b\uff0cFictitious play\uff08FP\uff09\u4e0d\u4e00\u5b9a\u6536\u655b\u5230\u7eb3\u4ec0\u5747\u8861\u3002"}}
{"id": "2507.10133", "categories": ["cs.LO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10133", "abs": "https://arxiv.org/abs/2507.10133", "authors": ["Nicholas Leisegang", "Thomas Meyer", "Ivan Varzinczak"], "title": "Extending Defeasibility for Propositional Standpoint Logics", "comment": null, "summary": "In this paper, we introduce a new defeasible version of propositional\nstandpoint logic by integrating Kraus et al.'s defeasible conditionals, Britz\nand Varzinczak's notions of defeasible necessity and distinct possibility,\nalong with Leisegang et al.'s approach to defeasibility into the standpoint\nlogics of G\\'omez \\'Alvarez and Rudolph. The resulting logical framework allows\nfor the expression of defeasibility on the level of implications, standpoint\nmodal operators, and standpoint-sharpening statements. We provide a\npreferential semantics for this extended language and propose a tableaux\ncalculus, which is shown to be sound and complete with respect to preferential\nentailment. We also establish the computational complexity of the tableaux\nprocedure to be in PSpace.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u591a\u79cd\u53ef\u5b66\u6027\u65b9\u6cd5\uff0c\u6269\u5c55\u4e86\u7acb\u573a\u903b\u8f91\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u8574\u542b\u3001\u6a21\u6001\u7b97\u5b50\u548c\u9510\u5316\u9648\u8ff0\u7684\u53ef\u5b66\u6027\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u4f18\u5148\u8bed\u4e49\u548c\u753b\u6cd5\u6f14\u7b97\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5b8c\u5907\u6027\u548cPSpace\u5185\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5728\u7acb\u573a\u903b\u8f91\u4e2d\u6574\u5408\u53ef\u5b66\u6027\uff0c\u5141\u8bb8\u5728\u903b\u8f91\u7684\u5404\u4e2a\u5c42\u9762\uff08\u5305\u62ec\u8574\u542b\u3001\u6a21\u6001\u7b97\u5b50\u548c\u9510\u5316\u9648\u8ff0\uff09\u4e0a\u8868\u8fbe\u53ef\u5b66\u6027\u3002", "method": "\u901a\u8fc7\u6574\u5408Kraus\u7b49\u4eba\u53ef\u5b66\u6761\u4ef6\u3001Britz\u548cVarzinczak\u7684\u53ef\u5b66\u5fc5\u7136\u6027\u548c\u4e0d\u540c\u53ef\u80fd\u6027\u6982\u5ff5\u4ee5\u53caLeisegang\u7b49\u4eba\u7684\u53ef\u5b66\u65b9\u6cd5\u5230G\u00f3mez \u00c1lvarez\u548cRudolph\u7684\u7acb\u573a\u903b\u8f91\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u5b66\u547d\u9898\u7acb\u573a\u903b\u8f91\u3002\u6211\u4eec\u4e3a\u8fd9\u4e2a\u6269\u5c55\u8bed\u8a00\u63d0\u4f9b\u4e86\u4f18\u5148\u8bed\u4e49\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u753b\u6cd5\u6f14\u7b97\uff0c\u8be5\u6f14\u7b97\u88ab\u8bc1\u660e\u5bf9\u4e8e\u4f18\u5148\u8574\u542b\u662f\u5065\u5168\u548c\u5b8c\u5168\u7684\u3002", "result": "\u6211\u4eec\u4e3a\u6269\u5c55\u8bed\u8a00\u63d0\u4f9b\u4e86\u4f18\u5148\u8bed\u4e49\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u753b\u6cd5\u6f14\u7b97\uff0c\u8be5\u6f14\u7b97\u88ab\u8bc1\u660e\u5bf9\u4e8e\u4f18\u5148\u8574\u542b\u662f\u5065\u5168\u548c\u5b8c\u5168\u7684\u3002\u6211\u4eec\u8fd8\u786e\u5b9a\u4e86\u753b\u6cd5\u7a0b\u5e8f\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u5728PSpace\u5185\u3002", "conclusion": "\u8be5\u903b\u8f91\u6846\u67b6\u4e3a\u8574\u542b\u3001\u7acb\u573a\u6a21\u6001\u7b97\u5b50\u548c\u7acb\u573a\u9510\u5316\u9648\u8ff0\u7684\u5c42\u6b21\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u5b66\u6027\u8868\u8fbe\u65b9\u5f0f\u3002"}}
{"id": "2507.09244", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09244", "abs": "https://arxiv.org/abs/2507.09244", "authors": ["Nishant Gupta", "Muris Sarajlic", "Erik G. Larsson"], "title": "Deep Learning for sub-THz Radio Unit Selection using sub-10 GHz Channel Information and Inferred Device Beamforming", "comment": "Accepted for Publication in IEEE VTC-Spring 2025, held at Oslo,\n  Norway", "summary": "The dense and distributed deployment of sub-THz radio units (RUs) alongside\nsub-10 GHz access point (AP) is a promising approach to provide high data rate\nand reliable coverage for future 6G applications. However, beam search or RU\nselection for the sub-THz RUs incurs significant overhead and high power\nconsumption. To address this, we introduce a method that leverages deep\nlearning to infer a suitable sub-THz RU candidate from a set of sub-THz RUs\nusing the sub-10 GHz channel characteristics. A novel aspect of this work is\nthe consideration of inter-band beam configuration (IBBC), defined as the\nbroadside angle between the low-band and high-band antenna patterns of the user\nequipment (UE). Since IBBC indicates the beamforming information or UE's\norientation, it is typically not shared with the network as a part of\nsignalling. Therefore, we propose a solution strategy to infer a suitable\nsub-THz RU even when UEs do not share their IBBC information. Simulation\nresults illustrate the performance of the inferred sub-THz RU and highlights\nthe detrimental impact of neglecting UE orientation on the systems performance.", "AI": {"tldr": "\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u6839\u636e\u5b5010 GHz\u4fe1\u9053\u7279\u6027\u63a8\u65ad\u5b50\u592a\u8d6b\u5179RU\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5f00\u9500\u548c\u529f\u8017\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5bc6\u96c6\u90e8\u7f72\u7684\u5b50\u592a\u8d6b\u5179RU\u5728\u6ce2\u675f\u641c\u7d22\u6216RU\u9009\u62e9\u65f6\u4ea7\u751f\u7684\u5de8\u5927\u5f00\u9500\u548c\u9ad8\u529f\u8017\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u6839\u636e\u5b5010 GHz\u4fe1\u9053\u7279\u6027\u4ece\u4e00\u7ec4\u5b50\u592a\u8d6b\u5179\u65e0\u7ebf\u7535\u5355\u5143\uff08RU\uff09\u4e2d\u63a8\u65ad\u51fa\u5408\u9002\u7684\u5b50\u592a\u8d6b\u5179RU\u5019\u9009\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u5b5010 GHz\u4fe1\u9053\u7279\u6027\u4ece\u4e00\u7ec4\u5b50\u592a\u8d6b\u5179RU\u4e2d\u63a8\u65ad\u51fa\u5408\u9002\u7684\u5b50\u592a\u8d6b\u5179RU\u5019\u9009\uff0c\u5373\u4f7f\u5728UE\u4e0d\u5171\u4eab\u5176IBBC\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5de5\u4f5c\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u6240\u63a8\u65ad\u7684\u6b21\u592a\u8d6b\u5179\u65e0\u7ebf\u7535\u5355\u5143\u7684\u6027\u80fd\uff0c\u5e76\u7a81\u51fa\u4e86\u5ffd\u7565\u7528\u6237\u8bbe\u5907\u65b9\u5411\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u4e0d\u5229\u5f71\u54cd\u3002"}}
{"id": "2507.09901", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09901", "abs": "https://arxiv.org/abs/2507.09901", "authors": ["Ayush Chopra"], "title": "Large Population Models", "comment": "Aggregation of Several Papers from MIT PhD Research.\n  github.com/AgentTorch/AgentTorch", "summary": "Many of society's most pressing challenges, from pandemic response to supply\nchain disruptions to climate adaptation, emerge from the collective behavior of\nmillions of autonomous agents making decisions over time. Large Population\nModels (LPMs) offer an approach to understand these complex systems by\nsimulating entire populations with realistic behaviors and interactions at\nunprecedented scale. LPMs extend traditional modeling approaches through three\nkey innovations: computational methods that efficiently simulate millions of\nagents simultaneously, mathematical frameworks that learn from diverse\nreal-world data streams, and privacy-preserving communication protocols that\nbridge virtual and physical environments. This allows researchers to observe\nhow agent behavior aggregates into system-level outcomes and test interventions\nbefore real-world implementation. While current AI advances primarily focus on\ncreating \"digital humans\" with sophisticated individual capabilities, LPMs\ndevelop \"digital societies\" where the richness of interactions reveals emergent\nphenomena. By bridging individual agent behavior and population-scale dynamics,\nLPMs offer a complementary path in AI research illuminating collective\nintelligence and providing testing grounds for policies and social innovations\nbefore real-world deployment. We discuss the technical foundations and some\nopen problems here. LPMs are implemented by the AgentTorch framework\n(github.com/AgentTorch/AgentTorch)", "AI": {"tldr": "LPMs\u901a\u8fc7\u6a21\u62df\u6570\u767e\u4e07\u4e2a\u4f53\u7684\u4ea4\u4e92\u6765\u7406\u89e3\u793e\u4f1a\u590d\u6742\u6027\uff0c\u80fd\u591f\u9884\u6d4b\u7fa4\u4f53\u884c\u4e3a\u5e76\u6d4b\u8bd5\u653f\u7b56\u3002", "motivation": "LPMs\u65e8\u5728\u7406\u89e3\u793e\u4f1a\u9762\u4e34\u7684\u7d27\u8feb\u6311\u6218\uff0c\u4f8b\u5982\u5927\u6d41\u884c\u75c5\u54cd\u5e94\u3001\u4f9b\u5e94\u94fe\u4e2d\u65ad\u548c\u6c14\u5019\u9002\u5e94\u7b49\uff0c\u8fd9\u4e9b\u6311\u6218\u6e90\u4e8e\u6570\u767e\u4e07\u4e2a\u81ea\u4e3b\u4ee3\u7406\u968f\u65f6\u95f4\u7684\u96c6\u4f53\u884c\u4e3a\u3002\u5b83\u4eec\u901a\u8fc7\u6a21\u62df\u5177\u6709\u73b0\u5b9e\u884c\u4e3a\u548c\u4ea4\u4e92\u7684\u6574\u4e2a\u79cd\u7fa4\uff0c\u4ee5\u5b9e\u73b0\u524d\u6240\u672a\u6709\u7684\u89c4\u6a21\u6765\u7406\u89e3\u8fd9\u4e9b\u590d\u6742\u7cfb\u7edf\u3002", "method": "LPMs\u901a\u8fc7\u4ee5\u4e0b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\u6765\u6269\u5c55\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\uff1a1. \u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u80fd\u591f\u540c\u65f6\u6a21\u62df\u6570\u767e\u4e07\u4e2a\u4ee3\u7406\u30022. \u5b66\u4e60\u591a\u6837\u5316\u771f\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6570\u5b66\u6846\u67b6\u30023. \u80fd\u591f\u8fde\u63a5\u865a\u62df\u548c\u7269\u7406\u73af\u5883\u7684\u9690\u79c1\u4fdd\u62a4\u901a\u4fe1\u534f\u8bae\u3002", "result": "LPMs\u80fd\u591f\u4f7f\u7814\u7a76\u4eba\u5458\u89c2\u5bdf\u4e2a\u4f53\u884c\u4e3a\u5982\u4f55\u6c47\u805a\u6210\u7cfb\u7edf\u7ea7\u7ed3\u679c\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u5b9e\u65bd\u524d\u6d4b\u8bd5\u5e72\u9884\u63aa\u65bd\u3002\u5b83\u4eec\u901a\u8fc7\u5c55\u793a\u4ea4\u4e92\u7684\u4e30\u5bcc\u6027\u6765\u63ed\u793a\u6d8c\u73b0\u73b0\u8c61\uff0c\u4ece\u800c\u5f00\u53d1\u201c\u6570\u5b57\u793e\u4f1a\u201d\u3002", "conclusion": "LPMs\u901a\u8fc7\u7ed3\u5408\u9ad8\u6548\u7684\u8ba1\u7b97\u65b9\u6cd5\u3001\u5b66\u4e60\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u6570\u5b66\u6846\u67b6\u4ee5\u53ca\u9690\u79c1\u4fdd\u62a4\u7684\u901a\u4fe1\u534f\u8bae\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u5efa\u6a21\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6570\u767e\u4e07\u4e2a\u4f53\u7684\u540c\u65f6\u6a21\u62df\u3002\u8fd9\u4f7f\u5f97\u7814\u7a76\u4eba\u5458\u80fd\u591f\u89c2\u5bdf\u4e2a\u4f53\u884c\u4e3a\u5982\u4f55\u6c47\u805a\u6210\u7cfb\u7edf\u7ea7\u7ed3\u679c\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u5b9e\u65bd\u524d\u6d4b\u8bd5\u5e72\u9884\u63aa\u65bd\u3002\u4e0e\u4e13\u6ce8\u4e8e\u521b\u9020\u5177\u6709\u590d\u6742\u4e2a\u4f53\u80fd\u529b\u7684\u201c\u6570\u5b57\u4eba\u7c7b\u201d\u7684AI\u8fdb\u5c55\u4e0d\u540c\uff0cLPMs\u81f4\u529b\u4e8e\u5f00\u53d1\u201c\u6570\u5b57\u793e\u4f1a\u201d\uff0c\u5176\u4e2d\u4e30\u5bcc\u7684\u4ea4\u4e92\u63ed\u793a\u4e86\u6d8c\u73b0\u73b0\u8c61\u3002LPMs\u901a\u8fc7\u8fde\u63a5\u4e2a\u4f53\u4ee3\u7406\u884c\u4e3a\u548c\u4eba\u53e3\u89c4\u6a21\u52a8\u6001\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u5f00\u8f9f\u4e86\u4e00\u6761\u4e92\u8865\u7684\u8def\u5f84\uff0c\u9610\u660e\u4e86\u96c6\u4f53\u667a\u80fd\uff0c\u5e76\u4e3a\u653f\u7b56\u548c\u793e\u4f1a\u521b\u65b0\u63d0\u4f9b\u4e86\u73b0\u5b9e\u90e8\u7f72\u524d\u7684\u6d4b\u8bd5\u573a\u3002\u6587\u7ae0\u8fd8\u8ba8\u8bba\u4e86LPMs\u7684\u6280\u672f\u57fa\u7840\u548c\u4e00\u4e9b\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u4ecb\u7ecd\u4e86AgentTorch\u6846\u67b6\uff08github.com/AgentTorch/AgentTorch\uff09\u3002"}}
{"id": "2507.08836", "categories": ["cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.08836", "abs": "https://arxiv.org/abs/2507.08836", "authors": ["Damien Fovet", "Shashank Chamoli", "Sarah Oury", "Srishti Singhal"], "title": "Accuracy and Consumption analysis from a compressed model by CompactifAI from Multiverse Computing", "comment": null, "summary": "This study evaluates the performance of a compression method, called\nCompactifAI, developed by Multiverse Computing, applied to the large language\nmodel Llama 3.1 8B\\cite{llama}. The evaluation focused on model efficiency (in\nterms of energy consumption) and accuracy using respectively the frameworks\nCodecarbon\\cite{codecarbon} and Ragas\\cite{ragas}. A comparison was performed\nbetween the model compressed with\nCompactifAI\\cite{compactifai}\\cite{compactifai2} and its full-size version. Our\nfindings reveal that the compressed model using CompactifAI not only\nsignificantly reduced the computational resources but also maintained the model\naccuracy, making the model more efficient, scalable and cost-effective.", "AI": {"tldr": "CompactifAI efficiently compresses Llama 3.1 8B with no accuracy loss, reducing resource usage and costs.", "motivation": "This study aims to evaluate the performance of a compression method, CompactifAI, developed by Multiverse Computing, applied to the large language model Llama 3.1 8B, focusing on model efficiency and accuracy.", "method": "The study evaluated the performance of the CompactifAI compression method on the Llama 3.1 8B model. Model efficiency (energy consumption) was assessed using Codecarbon, and accuracy was evaluated using Ragas. A comparison was made between the compressed model and its full-size version.", "result": "The compressed model using CompactifAI significantly reduced computational resources and maintained model accuracy compared to the full-size version.", "conclusion": "The compressed model using CompactifAI significantly reduced computational resources while maintaining model accuracy, making it more efficient, scalable, and cost-effective."}}
{"id": "2507.09518", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09518", "abs": "https://arxiv.org/abs/2507.09518", "authors": ["Dong Liu", "Sike Zeng", "Ji-Hai Liao", "Yu-Jun Zhao"], "title": "Unlocking Altermagnetism in Antiferromagnetic 2D Films via Adsorption", "comment": null, "summary": "Altermagnets, characterized by zero net magnetization and momentum-dependent\nspin splitting, have recently garnered significant attention due to their\npotential applications in a variety of fields. Here, we propose a\nsymmetry-engineering strategy to unlock altermagnetism in two dimensional (2D)\nantiferromagnetic systems via surface adsorption of atoms or molecules. By\nemploying spin group theory, we systematically demonstrate that selectively\nbreaking symmetry operations, specifically those protecting spin degeneracy in\nmomentum space, enables the emergence of nonrelativistic spin-split electronic\nstates. Meanwhile, preserving rotation or mirror symmetries connecting opposite\nsublattices ensures zero net magnetization. Through a comprehensive\nclassification of all symmetry operations across 80 layer groups, we identify\n63 antiferromagnetic spin point groups (SPGs) describing 2D materials and\nfurther isolate 15 groups that can host altermagnetic characteristics through\nsurface adsorption. Exemplified with monolayer antiferromagnetic VPS_3 and\nMnPSe_3, we show that oxygen adsorption on VPS_3 and NH_3 adsorption on MnPSe_3\nselectively disrupt PT symmetry while retaining the [C2||m] symmetry. This\nengineered symmetry reduction induces pronounced spin splitting in their band\nstructures without spin-orbit coupling, as confirmed by first-principles\ncalculations. Furthermore, adsorption energy analysis and thermal stability\nphase diagrams under varying coverage regimes reveal optimal configurations for\nexperimental feasibility. Our work establishes a universal symmetry-engineering\nframework to expand the family of altermagnetic materials, offering a versatile\npathway to tailor spin-split functionalities in two-dimensional\nantiferromagnets for advanced quantum applications.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u79f0\u5de5\u7a0b\u7b56\u7565\uff0c\u5728\u4e8c\u7ef4\u53cd\u94c1\u78c1\u6750\u6599\u4e2d\u8bf1\u5bfc\u4ea4\u66ff\u78c1\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e8c\u7ef4\u53cd\u94c1\u78c1\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u5177\u6709\u6f5c\u5728\u5e94\u7528\u4ef7\u503c\u7684\u4ea4\u66ff\u78c1\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u65cb\u7fa4\u8bba\u7cfb\u7edf\u5730\u5206\u6790\u4e86\u5bf9\u79f0\u6027\u5bf9\u81ea\u65cb\u5206\u88c2\u7535\u5b50\u6001\u7684\u5f71\u54cd\uff0c\u5e76\u7ed3\u540880\u4e2a\u5c42\u7fa4\u7684\u5bf9\u79f0\u64cd\u4f5c\u5206\u7c7b\uff0c\u786e\u5b9a\u4e8615\u4e2a\u53ef\u4ee5\u901a\u8fc7\u8868\u9762\u5438\u9644\u5b9e\u73b0\u4ea4\u66ff\u78c1\u6027\u7684\u4e8c\u7ef4\u6750\u6599\u7684\u81ea\u65cb\u70b9\u7fa4\u3002", "result": "\u6210\u529f\u8bc6\u522b\u4e8615\u4e2a\u5177\u6709\u4ea4\u66ff\u78c1\u6027\u6f5c\u529b\u7684\u4e8c\u7ef4\u6750\u6599\u81ea\u65cb\u70b9\u7fa4\uff0c\u5e76\u901a\u8fc7\u6c27\u5438\u9644VPS_3\u548cNH_3\u5438\u9644MnPSe_3\u7684\u5b9e\u4f8b\uff0c\u8bc1\u660e\u4e86\u8be5\u7b56\u7565\u53ef\u4ee5\u5728\u4e0d\u4f9d\u8d56\u81ea\u65cb\u8f68\u9053\u8026\u5408\u7684\u60c5\u51b5\u4e0b\u8bf1\u5bfc\u663e\u8457\u7684\u81ea\u65cb\u5206\u88c2\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u8868\u9762\u5438\u9644\u539f\u5b50\u6216\u5206\u5b50\u6765\u8bf1\u5bfc\u4e8c\u7ef4\u53cd\u94c1\u78c1\u4f53\u5b9e\u73b0\u4ea4\u66ff\u78c1\u6027\u7684\u5bf9\u79f0\u5de5\u7a0b\u7b56\u7565\uff0c\u5e76\u5df2\u901a\u8fc7\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548c\u5438\u9644\u80fd\u5206\u6790\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u4e3a\u6269\u5c55\u4ea4\u66ff\u78c1\u6027\u6750\u6599\u5bb6\u65cf\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2507.09729", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.09729", "abs": "https://arxiv.org/abs/2507.09729", "authors": ["Henry Fleischmann", "George Z. Li", "Jason Li"], "title": "Improved Directed Expander Decompositions", "comment": "54 pages, 3 figures", "summary": "We obtain faster expander decomposition algorithms for directed graphs,\nmatching the guarantees of Saranurak and Wang (SODA 2019) for expander\ndecomposition on undirected graphs. Our algorithms are faster than prior work\nand also generalize almost losslessly to capacitated graphs. In particular, we\nobtain the first directed expander decomposition algorithm for capacitated\ngraphs in near-linear time with optimal dependence on $\\phi$.\n  To obtain our result, we provide the first implementation and analysis of the\nnon-stop cut-matching game for directed, capacitated graphs. All existing\ndirected expander decomposition algorithms instead temporarily add ''fake\nedges'' before pruning them away in a final cleanup step. Our result shows that\nthe natural undirected approach applies even to directed graphs. The difficulty\nis in its analysis, which is technical and requires significant modifications\nfrom the original setting of undirected graphs.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u6709\u5411\u56fe\u548c\u6709\u5bb9\u5236\u9650\u56fe\u7684\u6269\u5c55\u5668\u5206\u89e3\u7b97\u6cd5\uff0c\u5176\u6548\u7387\u66f4\u9ad8\u5e76\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u83b7\u5f97\u6bd4\u73b0\u6709\u7b97\u6cd5\u66f4\u5feb\u3001\u51e0\u4e4e\u65e0\u635f\u8017\u5730\u9002\u7528\u4e8e\u6709\u5bb9\u5236\u9650\u56fe\u7684\u6709\u5411\u56fe\u6269\u5c55\u5668\u5206\u89e3\u7b97\u6cd5\u3002", "method": "\u5b9e\u73b0\u4e86\u975e\u505c\u6b62\u5272\u5339\u914d\u535a\u5f08\u5728\u6709\u5411\u6709\u5bb9\u5236\u9650\u56fe\u4e0a\u7684\u5e94\u7528\u548c\u5206\u6790\uff0c\u5e76\u5bf9\u8be5\u65b9\u6cd5\u5728\u6709\u5411\u56fe\u4e0a\u7684\u5206\u6790\u8fdb\u884c\u4e86\u6280\u672f\u6027\u4fee\u6539\u548c\u7ec6\u5316\u3002", "result": "\u5728\u6709\u5411\u56fe\u4e0a\u5b9e\u73b0\u4e86\u6bd4\u5148\u524d\u5de5\u4f5c\u66f4\u5feb\u3001\u5177\u6709\u6700\u4f73phi\u4f9d\u8d56\u6027\u7684\u6269\u5c55\u5668\u5206\u89e3\u7b97\u6cd5\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u6709\u5411\u6709\u5bb9\u5236\u9650\u56fe\u7684\u8fd1\u7ebf\u6027\u65f6\u95f4\u6269\u5c55\u5668\u5206\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6709\u5411\u56fe\u63d0\u4f9b\u4e86\u66f4\u5feb\u7684\u6269\u5c55\u5668\u5206\u89e3\u7b97\u6cd5\uff0c\u5e76\u9996\u6b21\u5b9e\u73b0\u4e86\u6709\u5411\u6709\u5bb9\u5236\u9650\u56fe\u7684\u8fd1\u7ebf\u6027\u65f6\u95f4\u6269\u5c55\u5668\u5206\u89e3\u3002"}}
{"id": "2507.09329", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09329", "abs": "https://arxiv.org/abs/2507.09329", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "comment": "15 pages", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents.", "AI": {"tldr": "LLM\u7f16\u7801\u4ee3\u7406\u5b58\u5728\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u53ef\u901a\u8fc7\u5b89\u5168\u8bbe\u8ba1\u548c\u7f13\u89e3\u7b56\u7565\uff08\u5982GPT-4.1\u7684\u5b89\u5168\u610f\u8bc6\uff09\u6765\u63d0\u9ad8\u5b89\u5168\u6027\u3002", "motivation": "LLM\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5e7f\u6cdb\u90e8\u7f72\uff0c\u4f46\u5176\u5b89\u5168\u5f71\u54cd\u5c1a\u4e0d\u660e\u786e\uff0c\u53ef\u80fd\u5f15\u5165\u4e0d\u5b89\u5168\u7684\u5b9e\u8df5\u3002", "method": "\u5bf9\u4e94\u4e2a\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff08GPT-4o\u3001GPT-4.1\u3001Claude\u53d8\u4f53\uff09\u572893\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u8f6f\u4ef6\u8bbe\u7f6e\u4efb\u52a1\u4e0a\uff0c\u5206\u6790\u8d85\u8fc712,000\u4e2a\u52a8\u4f5c\uff0c\u4ee5\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u5176\u5b89\u5168\u6027\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u7cbe\u5ea6\u7684\u68c0\u6d4b\u7cfb\u7edf\uff0c\u8bc6\u522b\u4e86\u56db\u5927\u7c7b\u6f0f\u6d1e\uff0c\u5e76\u8bc4\u4f30\u4e86\u53cd\u9988\u673a\u5236\u548c\u5b89\u5168\u63d0\u9192\u7b49\u7f13\u89e3\u7b56\u7565\u3002", "result": "21%\u7684\u4ee3\u7406\u8f68\u8ff9\u5305\u542b\u4e0d\u5b89\u5168\u64cd\u4f5c\uff0c\u6a21\u578b\u5728\u5b89\u5168\u884c\u4e3a\u4e0a\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u68c0\u6d4b\u7cfb\u7edf\u8bc6\u522b\u51fa\u56db\u5927\u7c7b\u6f0f\u6d1e\uff0c\u5176\u4e2d\u4fe1\u606f\u66b4\u9732\uff08CWE-200\uff09\u6700\u4e3a\u666e\u904d\u3002GPT-4.1\u5728\u7f13\u89e3\u7b56\u7565\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u7387\u8fbe96.8%\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5e26\u6765\u4e86\u4e25\u5cfb\u7684\u5b89\u5168\u98ce\u9669\uff0c\u4f46\u901a\u8fc7\u5f15\u5165\u5b89\u5168\u610f\u8bc6\u8bbe\u8ba1\u548c\u6709\u6548\u7684\u7f13\u89e3\u7b56\u7565\uff0c\u53ef\u4ee5\u63d0\u9ad8\u5176\u5b89\u5168\u6027\u3002"}}
{"id": "2507.10178", "categories": ["cs.AR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10178", "abs": "https://arxiv.org/abs/2507.10178", "authors": ["Wonung Kim", "Yubin Lee", "Yoonsung Kim", "Jinwoo Hwang", "Seongryong Oh", "Jiyong Jung", "Aziz Huseynov", "Woong Gyu Park", "Chang Hyun Park", "Divya Mahajan", "Jongse Park"], "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large Language Model Serving", "comment": null, "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 3.2x and 2.1x\nhigher token generation throughput, respectively.", "AI": {"tldr": "Pimba \u7cfb\u7edf\u901a\u8fc7\u4f18\u5316\u786c\u4ef6\u8bbe\u8ba1\uff0c\u63d0\u9ad8\u4e86 Transformer \u548c\u540e Transformer LLM \u7684\u63a8\u7406\u6027\u80fd\u3002", "motivation": "Transformer \u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u7684\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\u6765\u652f\u6301 Transformer \u548c\u540e Transformer LLM \u7684\u9ad8\u6548\u63a8\u7406\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86 Transformer \u548c\u540e Transformer LLM \u7684\u6027\u80fd\u7279\u5f81\uff0c\u53d1\u73b0\u4e24\u8005\u5728\u6279\u5904\u7406\u63a8\u7406\u4e2d\u90fd\u53d7\u9650\u4e8e\u5185\u5b58\u5e26\u5bbd\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u72b6\u6001\u66f4\u65b0\u64cd\u4f5c\u786c\u4ef6\u6210\u672c\u9ad8\uff0c\u4e14\u4e0d\u540c\u7684\u4f4e\u7cbe\u5ea6\u7b97\u672f\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u9762\u79ef\u4e4b\u95f4\u6709\u4e0d\u540c\u7684\u6743\u8861\uff0c\u5176\u4e2d\u5fae\u8f6f\u7684 MX \u662f\u6700\u4f18\u9009\u62e9\u3002\u57fa\u4e8e\u8fd9\u4e9b\u53d1\u73b0\uff0c\u8bbe\u8ba1\u4e86 Pimba \u7cfb\u7edf\uff0c\u5b83\u5305\u542b\u72b6\u6001\u66f4\u65b0\u5904\u7406\u5355\u5143\uff08SPU\uff09\u548c\u72b6\u6001\u66f4\u65b0\u5904\u7406\u5f15\u64ce\uff08SPE\uff09\uff0c\u5229\u7528 MX \u91cf\u5316\u7b97\u672f\u5b9e\u73b0\u9ad8\u6548\u7684\u72b6\u6001\u66f4\u65b0\u548c\u6ce8\u610f\u529b\u64cd\u4f5c\u3002", "result": "\u4e0e LLM \u4f18\u5316\u7684 GPU \u548c GPU+PIM \u7cfb\u7edf\u76f8\u6bd4\uff0cPimba \u5728\u4ee4\u724c\u751f\u6210\u541e\u5410\u91cf\u65b9\u9762\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe 3.2 \u500d\u548c 2.1 \u500d\u7684\u63d0\u5347\u3002", "conclusion": "Transformer \u6a21\u578b\u5728\u5904\u7406\u957f\u5e8f\u5217\u65f6\u5b58\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u7684\u6311\u6218\uff0c\u800c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSMs\uff09\u3001\u7ebf\u6027\u6ce8\u610f\u529b\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNNs\uff09\u7b49\u540e Transformer \u6a21\u578b\u5219\u63d0\u4f9b\u4e86\u66ff\u4ee3\u65b9\u6848\u3002Pimba \u7cfb\u7edf\u901a\u8fc7\u5176\u72b6\u6001\u66f4\u65b0\u5904\u7406\u5355\u5143\uff08SPU\uff09\u548c\u72b6\u6001\u66f4\u65b0\u5904\u7406\u5f15\u64ce\uff08SPE\uff09\uff0c\u5229\u7528\u5fae\u8f6f\u7684 MX \u91cf\u5316\u7b97\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9 Transformer \u548c\u540e Transformer LLM \u7684\u7edf\u4e00\u9ad8\u6548\u652f\u6301\uff0c\u5728\u5185\u5b58\u5e26\u5bbd\u53d7\u9650\u7684\u6279\u5904\u7406\u63a8\u7406\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4e0e LLM \u4f18\u5316\u7684 GPU \u548c GPU+PIM \u7cfb\u7edf\u76f8\u6bd4\uff0c\u4ee4\u724c\u751f\u6210\u541e\u5410\u91cf\u5206\u522b\u63d0\u9ad8\u4e86 3.2 \u500d\u548c 2.1 \u500d\u3002"}}
{"id": "2507.10219", "categories": ["cond-mat.mes-hall", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2507.10219", "abs": "https://arxiv.org/abs/2507.10219", "authors": ["Utkarsh Shashank", "Akash Kumar", "Tahereh Sadat Parvini", "Hauke Heyen", "Lunjie Zeng", "Andrew B. Yankovich", "Mona Rajabali", "Eva Olsson", "Markus M\u00fcnzenberg", "Johan \u00c5kerman"], "title": "Bulk spin-orbit torque-driven spin Hall nano-oscillators using PtBi alloys", "comment": "19 pages, 5 figures", "summary": "Spin-orbit-torque-driven auto-oscillations in spin Hall nano-oscillators\n(SHNOs) offer a transformative pathway toward energy-efficient, nanoscale\nmicrowave devices for next-generation neuromorphic computing and high-frequency\ntechnologies. A key requirement for achieving robust, sustained oscillations is\nreducing the threshold current ($I_{\\text{th}}$), strongly governed by spin\nHall efficiency ($\\theta_{\\text{SH}}$). However, conventional strategies to\nenhance $\\theta_{\\text{SH}}$ face trade-offs, including high longitudinal\nresistivity, interfacial effects, and symmetry-breaking torques that limit\nperformance. Here, we demonstrate a substantial enhancement of the bulk spin\nHall effect in PtBi alloys, achieving over a threefold increase in\n$\\theta_{\\text{SH}}$, from 0.07 in pure Pt to 0.24 in Pt$_{94.0}$Bi$_{6.0}$ and\n0.19 in Pt$_{91.3}$Bi$_{8.7}$, as extracted from DC-bias spin-torque\nferromagnetic resonance. The enhanced $\\theta_{\\text{SH}}$ originates from\nbulk-dominated, extrinsic side-jump scattering across all PtBi compositions.\nCorrespondingly, we observe a 42\\% and 32\\% reduction in $I_{\\text{th}}$ in 100\nnm SHNOs based on Co$_{40}$Fe$_{40}$B$_{20}$(3 nm)/Pt$_{94.0}$Bi$_{6.0}$(4 nm)\nand Co$_{40}$Fe$_{40}$B$_{20}$(3 nm)/Pt$_{91.3}$Bi$_{8.7}$(4 nm), respectively.\nStructural characterization reveals reduced Pt crystallinity, along with\nemergence of preferred crystallographic orientations upon introducing higher Bi\nconcentrations. Together, these results position PtBi alloys as a compelling\nalternative to conventional 5$d$ transition metals, enabling enhanced\n$\\theta_{\\text{SH}}$ and significantly lower $I_{\\text{th}}$, thus opening new\navenues for energy-efficient neuromorphic computing and magnetic random access\nmemory.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0PtBi\u5408\u91d1\u80fd\u663e\u8457\u63d0\u5347\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff0c\u964d\u4f4e\u81ea\u65cb\u970d\u5c14\u7eb3\u7c73\u632f\u8361\u5668\u7684\u9608\u503c\u7535\u6d41\uff0c\u4e3a\u5f00\u53d1\u65b0\u578b\u8282\u80fd\u5fae\u6ce2\u5668\u4ef6\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u80fd\u91cf\u6548\u7387\u9ad8\u3001\u5fae\u578b\u5316\u7684\u5fae\u6ce2\u5668\u4ef6\uff0c\u9700\u8981\u964d\u4f4e\u81ea\u65cb\u970d\u5c14\u7eb3\u7c73\u632f\u8361\u5668\uff08SHNOs\uff09\u7684\u9608\u503c\u7535\u6d41\uff08Ith\uff09\uff0c\u800c\u8fd9\u53c8\u4e0e\u63d0\u9ad8\u81ea\u65cb\u970d\u5c14\u6548\u7387\uff08\u03b8SH\uff09\u5bc6\u5207\u76f8\u5173\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u63d0\u9ad8\u03b8SH\u7684\u65b9\u6cd5\u5b58\u5728\u4e00\u4e9b\u9650\u5236\u3002", "method": "\u901a\u8fc7\u76f4\u6d41\u504f\u7f6e\u81ea\u65cb\u77e9\u94c1\u78c1\u5171\u632f\u5b9e\u9a8c\uff0c\u63d0\u53d6\u4e86PtBi\u5408\u91d1\u7684\u81ea\u65cb\u970d\u5c14\u6548\u7387\uff08\u03b8SH\uff09\uff0c\u5e76\u4ee5\u6b64\u4e3a\u57fa\u7840\uff0c\u5728\u57fa\u4e8ePtBi\u5408\u91d1\u7684\u81ea\u65cb\u970d\u5c14\u7eb3\u7c73\u632f\u8361\u5668\uff08SHNOs\uff09\u4e2d\u89c2\u5bdf\u5230\u4e86\u9608\u503c\u7535\u6d41\uff08Ith\uff09\u7684\u663e\u8457\u964d\u4f4e\u3002", "result": " PtBi\u5408\u91d1\u7684\u03b8SH\u76f8\u6bd4\u7eafPt\u6709\u663e\u8457\u63d0\u5347\uff08\u6700\u9ad8\u63d0\u5347\u8d85\u8fc7\u4e09\u500d\uff09\uff0c\u76f8\u5e94\u7684SHNOs\u7684Ith\u964d\u4f4e\u4e8642%\u548c32%\u3002\u7ed3\u6784\u8868\u5f81\u663e\u793a\uff0cBi\u7684\u52a0\u5165\u964d\u4f4e\u4e86Pt\u7684\u7ed3\u6676\u5ea6\uff0c\u5e76\u51fa\u73b0\u4e86\u4f18\u5148\u7684\u6676\u4f53\u5b66\u53d6\u5411\u3002", "conclusion": "PtBi\u5408\u91d1\u76f8\u6bd4\u4e8e\u4f20\u7edf\u76845d\u8fc7\u6e21\u91d1\u5c5e\uff0c\u5728\u63d0\u9ad8\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff08SH\uff09\u548c\u964d\u4f4e\u9608\u503c\u7535\u6d41\uff08Ith\uff09\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u4e3a\u5f00\u53d1\u8282\u80fd\u578b\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u548c\u78c1\u6027\u968f\u673a\u5b58\u53d6\u5b58\u50a8\u5668\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2507.09646", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09646", "abs": "https://arxiv.org/abs/2507.09646", "authors": ["Lucian Cristian Iacob", "M\u00e1t\u00e9 Sz\u00e9csi", "Gerben Izaak Beintema", "Maarten Schoukens", "Roland T\u00f3th"], "title": "Learning Koopman Models From Data Under General Noise Conditions", "comment": "Submitted to SIAM Journal on Applied Dynamical Systems (SIADS)", "summary": "This paper presents a novel identification approach of Koopman models of\nnonlinear systems with inputs under rather general noise conditions. The method\nuses deep state-space encoders based on the concept of state reconstructability\nand an efficient multiple-shooting formulation of the squared loss of the\nprediction error to estimate the dynamics and the lifted state from\ninput-output data. Furthermore, the Koopman model structure includes an\ninnovation noise term that is used to handle process and measurement noise. It\nis shown that the proposed approach is statistically consistent and\ncomputationally efficient due to the multiple-shooting formulation where, on\nsubsections of the data, multi-step prediction errors can be calculated in\nparallel. The latter allows for efficient batch optimization of the network\nparameters and, at the same time, excellent long-term prediction capabilities\nof the obtained models. The performance of the approach is illustrated by\nnonlinear benchmark examples.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u7f16\u7801\u5668\u548c\u591a\u91cd\ucd94\uc801\u516c\u5f0f\u7684Koopman\u6a21\u578b\u8bc6\u522b\u65b0\u65b9\u6cd5\uff0c\u53ef\u5904\u7406\u566a\u58f0\u5e76\u5b9e\u73b0\u9ad8\u6548\u7684\u957f\u671f\u9884\u6d4b\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684Koopman\u6a21\u578b\u8bc6\u522b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u4e00\u822c\u566a\u58f0\u6761\u4ef6\u4e0b\u8bc6\u522b\u975e\u7ebf\u6027\u7cfb\u7edf\uff08\u542b\u8f93\u5165\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u53ef\u91cd\u6784\u6982\u5ff5\u7684\u6df1\u5ea6\u72b6\u6001\u7a7a\u95f4\u7f16\u7801\u5668\uff0c\u5e76\u7ed3\u5408\u4e86\u9884\u6d4b\u8bef\u5dee\u5e73\u65b9\u635f\u5931\u7684\u9ad8\u6548\u591a\u91cd\ucd94\uc801\u516c\u5f0f\uff0c\u7528\u4e8e\u4ece\u8f93\u5165\u8f93\u51fa\u6570\u636e\u4f30\u8ba1\u975e\u7ebf\u6027\u7cfb\u7edf\u7684Koopman\u6a21\u578b\u52a8\u6001\u548c\u63d0\u5347\u72b6\u6001\u3002\u6a21\u578b\u7ed3\u6784\u4e2d\u8fd8\u5305\u542b\u4e00\u4e2a\u7528\u4e8e\u5904\u7406\u8fc7\u7a0b\u566a\u58f0\u548c\u6d4b\u91cf\u566a\u58f0\u7684\u521b\u65b0\u566a\u58f0\u9879\u3002", "result": "\u901a\u8fc7\u975e\u7ebf\u6027\u57fa\u51c6\u793a\u4f8b\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5177\u6709\u7edf\u8ba1\u4e00\u81f4\u6027\u4e14\u8ba1\u7b97\u6548\u7387\u9ad8\uff0c\u80fd\u591f\u5b9e\u73b0\u826f\u597d\u7684\u957f\u671f\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2507.09167", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09167", "abs": "https://arxiv.org/abs/2507.09167", "authors": ["Michal Vavrecka", "Radoslav Skoviera", "Gabriela Sejnova", "Karla Stepanova"], "title": "PRAG: Procedural Action Generator", "comment": null, "summary": "We present a novel approach for the procedural construction of multi-step\ncontact-rich manipulation tasks in robotics. Our generator takes as input\nuser-defined sets of atomic actions, objects, and spatial predicates and\noutputs solvable tasks of a given length for the selected robotic environment.\nThe generator produces solvable tasks by constraining all possible\n(nonsolvable) combinations by symbolic and physical validation. The symbolic\nvalidation checks each generated sequence for logical and operational\nconsistency, and also the suitability of object-predicate relations. Physical\nvalidation checks whether tasks can be solved in the selected robotic\nenvironment. Only the tasks that passed both validators are retained. The\noutput from the generator can be directly interfaced with any existing\nframework for training robotic manipulation tasks, or it can be stored as a\ndataset of curated robotic tasks with detailed information about each task.\nThis is beneficial for RL training as there are dense reward functions and\ninitial and goal states paired with each subgoal. It allows the user to measure\nthe semantic similarity of all generated tasks. We tested our generator on\nsequences of up to 15 actions resulting in millions of unique solvable\nmulti-step tasks.", "AI": {"tldr": "\u4e00\u4e2a\u673a\u5668\u4eba\u4efb\u52a1\u751f\u6210\u5668\uff0c\u53ef\u4ee5\u521b\u5efa\u53ef\u884c\u7684\u591a\u6b65\u9aa4\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u63d0\u4f9b\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u7684\u6570\u636e\u3002", "motivation": "\u4e3a\u4e86\u5728\u673a\u5668\u4eba\u5b66\u4e2d\u7a0b\u5e8f\u5316\u5730\u6784\u5efa\u591a\u6b65\u9aa4\u7684\u63a5\u89e6\u5bc6\u96c6\u578b\u64cd\u4f5c\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7a0b\u5e8f\u5316\u751f\u6210\u591a\u6b65\u9aa4\u63a5\u89e6\u5bc6\u96c6\u578b\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7b26\u53f7\u548c\u7269\u7406\u9a8c\u8bc1\u6765\u7ea6\u675f\u6240\u6709\u53ef\u80fd\u7684\uff08\u4e0d\u53ef\u884c\u7684\uff09\u7ec4\u5408\uff0c\u786e\u4fdd\u751f\u6210\u4efb\u52a1\u7684\u53ef\u884c\u6027\u3002", "result": "\u5728\u6700\u591a15\u4e2a\u52a8\u4f5c\u7684\u5e8f\u5217\u4e0a\u6d4b\u8bd5\u4e86\u751f\u6210\u5668\uff0c\u4ea7\u751f\u4e86\u6570\u767e\u4e07\u4e2a\u72ec\u7279\u7684\u53ef\u884c\u591a\u6b65\u9aa4\u4efb\u52a1\uff0c\u5e76\u80fd\u6d4b\u91cf\u751f\u6210\u4efb\u52a1\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u53ef\u884c\u7684\u3001\u591a\u6b65\u9aa4\u7684\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff0c\u5e76\u80fd\u4e0e\u73b0\u6709\u6846\u67b6\u96c6\u6210\uff0c\u4e5f\u53ef\u4f5c\u4e3a\u6570\u636e\u96c6\u5b58\u50a8\uff0c\u4e3a\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u63d0\u4f9b\u652f\u6301\u3002"}}
{"id": "2507.09052", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09052", "abs": "https://arxiv.org/abs/2507.09052", "authors": ["Fang Chen", "Alex Villa", "Gongbo Liang", "Xiaoyi Lu", "Meng Tang"], "title": "Can Contrastive Learning Improve Class-Imbalanced Diffusion Model?", "comment": "20 pages, 11 figures", "summary": "Training data for class-conditional image synthesis often exhibit a\nlong-tailed distribution with limited images for tail classes. Such an\nimbalance causes mode collapse and reduces the diversity of synthesized images\nfor tail classes. For class-conditional diffusion models trained on imbalanced\ndata, we aim to improve the diversity of tail class images without compromising\nthe fidelity and diversity of head class images. We achieve this by introducing\ntwo deceptively simple but highly effective contrastive loss functions.\nFirstly, we employ an unsupervised InfoNCE loss utilizing negative samples to\nincrease the distance/dissimilarity among synthetic images, particularly for\ntail classes. To further enhance the diversity of tail classes, our second loss\nis an MSE loss that contrasts class-conditional generation with unconditional\ngeneration at large timesteps. This second loss makes the denoising process\ninsensitive to class conditions for the initial steps, which enriches tail\nclasses through knowledge sharing from head classes. Conditional-unconditional\nalignment has been shown to enhance the performance of long-tailed GAN. We are\nthe first to adapt such alignment to diffusion models. We successfully\nleveraged contrastive learning for class-imbalanced diffusion models. Our\ncontrastive learning framework is easy to implement and outperforms standard\nDDPM and alternative methods for class-imbalanced diffusion models across\nvarious datasets, including CIFAR10/100-LT, PlacesLT, TinyImageNetLT, and\nImageNetLT.", "AI": {"tldr": "\u9488\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6269\u6563\u6a21\u578b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5bf9\u6bd4\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u5c3e\u90e8\u7c7b\u522b\u7684\u56fe\u50cf\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5934\u90e8\u7c7b\u522b\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u7c7b\u522b\u6761\u4ef6\u6269\u6563\u6a21\u578b\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u5408\u6210\u7684\u5c3e\u90e8\u7c7b\u522b\u7684\u56fe\u50cf\u591a\u6837\u6027\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u5934\u90e8\u7c7b\u522b\u7684\u4fdd\u771f\u5ea6\u548c\u591a\u6837\u6027\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u4e24\u79cd\u5bf9\u6bd4\u635f\u5931\u51fd\u6570\u6765\u5b9e\u73b0\uff1a1. \u5229\u7528\u8d1f\u6837\u672c\u7684\u65e0\u76d1\u7763InfoNCE\u635f\u5931\uff0c\u4ee5\u589e\u52a0\u5408\u6210\u56fe\u50cf\u4e4b\u95f4\u7684\u8ddd\u79bb/\u5dee\u5f02\u6027\uff0c\u7279\u522b\u662f\u9488\u5bf9\u5c3e\u90e8\u7c7b\u522b\u30022. \u6700\u5c0f\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u635f\u5931\uff0c\u5c06\u5927\u6b65\u957f\u4e0b\u7684\u6761\u4ef6\u751f\u6210\u4e0e\u65e0\u6761\u4ef6\u751f\u6210\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4f7f\u53bb\u566a\u8fc7\u7a0b\u5bf9\u5934\u90e8\u7c7b\u522b\u7684\u6761\u4ef6\u4e0d\u654f\u611f\uff0c\u4ece\u800c\u901a\u8fc7\u77e5\u8bc6\u5171\u4eab\u6765\u4e30\u5bcc\u5c3e\u90e8\u7c7b\u522b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6269\u6563\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u6613\u4e8e\u5b9e\u73b0\uff0c\u5e76\u4e14\u5728CIFAR10/100-LT\u3001PlacesLT\u3001TinyImageNetLT\u548cImageNetLT\u7b49\u5404\u79cd\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6269\u6563\u6a21\u578b\u65b9\u9762\u4f18\u4e8e\u6807\u51c6\u7684DDPM\u548c\u66ff\u4ee3\u65b9\u6cd5\u3002"}}
{"id": "2507.10259", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.10259", "abs": "https://arxiv.org/abs/2507.10259", "authors": ["Chengze Du", "Zhiwei Yu", "Heng Xu", "Haojie Wang", "Bo liu", "Jialong Li"], "title": "Cross-Timeslot Optimization for Distributed GPU Inference Using Reinforcement Learning", "comment": "17 pages, 12 figures", "summary": "The rapid growth of large language model (LLM) services imposes increasing\ndemands on distributed GPU inference infrastructure. Most existing scheduling\nsystems rely on the current system state to make decisions, without considering\nhow task demand and resource availability evolve over time. This lack of\ntemporal awareness leads to inefficient GPU utilization, high task migration\noverhead, and poor system responsiveness under dynamic workloads. In this work,\nwe identify the fundamental limitations of these instantaneous-state-only\nscheduling approaches and propose Temporal Optimal Resource scheduling via\nTwo-layer Architecture (TORTA). TORTA introduces a spatiotemporal scheduling\nframework that captures both long-term workload patterns and short-term\nexecution constraints. It adopts a two-layer design: a macro-level scheduler\nleverages reinforcement learning and optimal transport to coordinate\ninter-region task distribution, while a micro-level allocator refines\ntask-to-server assignments within each region to reduce latency and switching\ncosts. Experimental results across multiple network topologies show that TORTA\nreduces average inference response time by up to 15\\%, improves load balance by\napproximately 4-5\\%, and cuts total operational cost by 10-20\\% compared to\nstate-of-the-art baseline methods.", "AI": {"tldr": "TORTA\u662f\u4e00\u4e2a\u65f6\u7a7a\u8c03\u5ea6\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u5c42\u67b6\u6784\u4f18\u5316LLM\u670d\u52a1\u7684GPU\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u6210\u672c\u3002", "motivation": "\u73b0\u6709LLM\u670d\u52a1\u8c03\u5ea6\u7cfb\u7edf\u4f9d\u8d56\u77ac\u65f6\u72b6\u6001\u8fdb\u884c\u51b3\u7b56\uff0c\u672a\u8003\u8651\u4efb\u52a1\u9700\u6c42\u548c\u8d44\u6e90\u53ef\u7528\u6027\u7684\u65f6\u53d8\u6027\uff0c\u5bfc\u81f4GPU\u5229\u7528\u7387\u4f4e\u3001\u4efb\u52a1\u8fc1\u79fb\u5f00\u9500\u5927\u3001\u7cfb\u7edf\u54cd\u5e94\u6162\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "TORTA\u91c7\u7528\u4e24\u5c42\u67b6\u6784\uff1a\u5b8f\u89c2\u8c03\u5ea6\u5668\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548c\u6700\u4f18\u4f20\u8f93\u534f\u8c03\u8de8\u533a\u57df\u4efb\u52a1\u5206\u914d\uff0c\u5fae\u89c2\u5206\u914d\u5668\u5219\u5728\u533a\u57df\u5185\u4f18\u5316\u4efb\u52a1\u5230\u670d\u52a1\u5668\u7684\u5206\u914d\uff0c\u4ee5\u964d\u4f4e\u5ef6\u8fdf\u548c\u5207\u6362\u6210\u672c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTORTA\u76f8\u8f83\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e73\u5747\u63a8\u7406\u54cd\u5e94\u65f6\u95f4\u7f29\u77ed\u4e86\u9ad8\u8fbe15%\uff0c\u8d1f\u8f7d\u5747\u8861\u63d0\u9ad8\u4e86\u7ea64-5%\uff0c\u603b\u8fd0\u8425\u6210\u672c\u964d\u4f4e\u4e8610-20%\u3002", "conclusion": "TORTA\u901a\u8fc7\u5f15\u5165\u65f6\u7a7a\u8c03\u5ea6\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u957f\u671f\u5de5\u4f5c\u8d1f\u8f7d\u6a21\u5f0f\u548c\u77ed\u671f\u6267\u884c\u7ea6\u675f\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684GPU\u5229\u7528\u7387\u548c\u7cfb\u7edf\u6027\u80fd\u3002"}}
{"id": "2507.09363", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09363", "abs": "https://arxiv.org/abs/2507.09363", "authors": ["Amir Kleiner", "Sivan Refaely-Abramson"], "title": "First-principles design for strain-tunable exciton dynamics in 2D materials", "comment": null, "summary": "Controlling exciton motion in two-dimensional semiconductors is key to\nunlocking new optoelectronic and straintronic functionalities. Monolayer\ntransition metal dichalcogenides (TMDs), with their tightly bound excitons,\noffer an ideal platform for such control due to their strong sensitivity to\nlocal strain. Here we present an ab initio framework for modeling exciton\ndynamics in inhomogeneously strained WS2, combining excitonic band structures\nderived from first principles with a semiclassical transport model operating in\nboth real and momentum space. By analyzing idealized strain patterns, we reveal\nhow excitons undergo drift, diffusion, and confinement without invoking\nempirical parameters. Our results uncover regimes of super-ballistic\npropagation and negative effective diffusion, governed entirely by the strain\nlandscape. This work provides microscopic insight into strain-tunable exciton\nbehavior and establishes design principles for engineering exciton flow in\ntwo-dimensional materials.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6a21\u62df\u975e\u5747\u5300\u5e94\u53d8\u4e0bWS2\u6fc0\u5b50\u52a8\u529b\u5b66\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u4e86\u7531\u5e94\u53d8\u63a7\u5236\u7684\u8d85\u7403\u5f39\u6027\u4f20\u64ad\u548c\u8d1f\u6269\u6563\u73b0\u8c61\u3002", "motivation": "\u63a7\u5236\u4e8c\u7ef4\u534a\u5bfc\u4f53\u4e2d\u7684\u6fc0\u5b50\u8fd0\u52a8\u5bf9\u4e8e\u5b9e\u73b0\u65b0\u7684\u5149\u7535\u548c\u5e94\u53d8\u7535\u5b50\u529f\u80fd\u81f3\u5173\u91cd\u8981\u3002\u5355\u5c42\u8fc7\u6e21\u91d1\u5c5e\u786b\u65cf\u5316\u7269\uff08TMDs\uff09\u7531\u4e8e\u5176\u7d27\u5bc6\u675f\u7f1a\u7684\u6fc0\u5b50\u548c\u5bf9\u5c40\u90e8\u5e94\u53d8\u7684\u9ad8\u5ea6\u654f\u611f\u6027\uff0c\u4e3a\u8fd9\u79cd\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u60f3\u7684\u5e73\u53f0\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u7684\u6fc0\u5b50\u80fd\u5e26\u7ed3\u6784\u548c\u5728\u5b9e\u7a7a\u95f4\u53ca\u52a8\u91cf\u7a7a\u95f4\u4e2d\u8fd0\u884c\u7684\u534a\u7ecf\u5178\u8f93\u8fd0\u6a21\u578b\uff0c\u5bf9\u975e\u5747\u5300\u5e94\u53d8\u7684WS2\u8fdb\u884c\u5efa\u6a21\u3002", "result": "\u63ed\u793a\u4e86\u6fc0\u5b50\u5728\u6ca1\u6709\u7ecf\u9a8c\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\u4f1a\u7ecf\u5386\u6f02\u79fb\u3001\u6269\u6563\u548c\u9650\u5236\uff0c\u5e76\u53d1\u73b0\u4e86\u8d85\u7403\u5f39\u6027\u4f20\u64ad\u548c\u8d1f\u6709\u6548\u6269\u6563\u7b49\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u8d85\u7403\u5f39\u6027\u4f20\u64ad\u548c\u8d1f\u6709\u6548\u6269\u6563\u7684\u673a\u5236\uff0c\u8fd9\u4e9b\u673a\u5236\u5b8c\u5168\u7531\u5e94\u53d8\u666f\u89c2\u51b3\u5b9a\uff0c\u4e3a\u7406\u89e3\u548c\u8bbe\u8ba1\u4e8c\u7ef4\u6750\u6599\u4e2d\u7684\u6fc0\u5b50\u6d41\u63d0\u4f9b\u4e86\u5fae\u89c2\u89c6\u89d2\u3002"}}
{"id": "2507.09066", "categories": ["quant-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2507.09066", "abs": "https://arxiv.org/abs/2507.09066", "authors": ["Mohammed Alkhateeb", "Alex Matzkin"], "title": "Microcausality and Tunneling Times in Relativistic Quantum Field Theory", "comment": null, "summary": "We show, in the framework of a space-time resolved relativistic quantum eld\ntheory approach to tunneling, that microcausality precludes superluminal\ntunneling dynamics. More specically in this work dealing with Dirac and\nKlein-Gordon elds, we rst prove that microcausality holds for such elds in the\npresence of a background potential. We then use this result to show that an\nintervention performed on a localized region of an initial wave packet\nsubsequently scattering on a potential barrier does not result in any eect\noutside the light cone emanating from that region. We illustrate these results\nwith numerical computations for Dirac fermions and Klein-Gordon bosons.", "AI": {"tldr": "\u672c\u7814\u7a76\u91c7\u7528\u65f6\u7a7a\u5206\u8fa8\u7684\u76f8\u5bf9\u8bba\u91cf\u5b50\u573a\u8bba\u65b9\u6cd5\u7814\u7a76\u96a7\u7a7f\u73b0\u8c61\uff0c\u8bc1\u660e\u4e86\u5fae\u89c2\u56e0\u679c\u5173\u7cfb\u963b\u6b62\u4e86\u8d85\u5149\u901f\u96a7\u7a7f\u52a8\u529b\u5b66\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u5b58\u5728\u80cc\u666f\u52bf\u7684\u60c5\u51b5\u4e0b\uff0c\u72c4\u62c9\u514b\u573a\u548c\u514b\u83b1\u56e0-\u6208\u767b\u573a\u7684\u5fae\u89c2\u56e0\u679c\u5173\u7cfb\u6210\u7acb\u3002\u5bf9\u521d\u59cb\u6ce2\u5305\u7684\u5c40\u57df\u533a\u57df\u8fdb\u884c\u5e72\u9884\uff0c\u4e0d\u4f1a\u5bf9\u5149\u9525\u4e4b\u5916\u7684\u533a\u57df\u4ea7\u751f\u4efb\u4f55\u5f71\u54cd\u3002\u901a\u8fc7\u5bf9\u72c4\u62c9\u514b\u8d39\u7c73\u5b50\u548c\u514b\u83b1\u56e0-\u6208\u767b\u73bb\u8272\u5b50\u7684\u6570\u503c\u8ba1\u7b97\uff0c\u8bf4\u660e\u4e86\u8fd9\u4e9b\u7ed3\u679c\u7684\u6709\u6548\u6027\u3002", "motivation": "\u7814\u7a76\u5fae\u89c2\u56e0\u679c\u5173\u7cfb\u662f\u5426\u963b\u6b62\u8d85\u5149\u901f\u96a7\u7a7f\u52a8\u529b\u5b66\u3002", "method": "\u5728\u65f6\u7a7a\u5206\u8fa8\u7684\u76f8\u5bf9\u8bba\u91cf\u5b50\u573a\u8bba\u6846\u67b6\u4e0b\u5bf9\u96a7\u7a7f\u8fdb\u884c\u7814\u7a76\u3002", "result": "\u5fae\u89c2\u56e0\u679c\u5173\u7cfb\u5bf9\u4e8e\u5728\u80cc\u666f\u52bf\u5b58\u5728\u4e0b\u7684\u72c4\u62c9\u514b\u573a\u548c\u514b\u83b1\u56e0-\u6208\u767b\u573a\u6210\u7acb\u3002\u5bf9\u521d\u59cb\u6ce2\u5305\u7684\u5c40\u57df\u533a\u57df\u8fdb\u884c\u5e72\u9884\uff0c\u4e0d\u4f1a\u5bf9\u4ece\u8be5\u533a\u57df\u53d1\u51fa\u7684\u5149\u9525\u4e4b\u5916\u7684\u533a\u57df\u4ea7\u751f\u4efb\u4f55\u5f71\u54cd\u3002", "conclusion": "\u5fae\u89c2\u56e0\u679c\u5173\u7cfb\u963b\u6b62\u4e86\u8d85\u5149\u901f\u96a7\u7a7f\u52a8\u529b\u5b66\u3002"}}
{"id": "2507.09025", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09025", "abs": "https://arxiv.org/abs/2507.09025", "authors": ["Chien Van Nguyen", "Ruiyi Zhang", "Hanieh Deilamsalehy", "Puneet Mathur", "Viet Dac Lai", "Haoliang Wang", "Jayakumar Subramanian", "Ryan A. Rossi", "Trung Bui", "Nikos Vlassis", "Franck Dernoncourt", "Thien Huu Nguyen"], "title": "Lizard: An Efficient Linearization Framework for Large Language Models", "comment": "15 pages", "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.", "AI": {"tldr": "Lizard\u662f\u4e00\u4e2a\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u80fd\u5c06Transformer\u7c7bLLM\u8f6c\u5316\u4e3a\u4e9a\u4e8c\u6b21\u590d\u6742\u5ea6\u6a21\u578b\uff0c\u7528\u4e8e\u65e0\u9650\u4e0a\u4e0b\u6587\u751f\u6210\u3002\u5b83\u901a\u8fc7\u6539\u8fdb\u7684\u6ce8\u610f\u529b\u673a\u5236\u548c\u95e8\u63a7\u5355\u5143\uff0c\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u89e3\u51b3\u4e86\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u89e3\u51b3Transformer\u7c7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u957f\u4e0a\u4e0b\u6587\u65f6\u9762\u4e34\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u74f6\u9888\uff0c\u8fd9\u4e9b\u74f6\u9888\u6e90\u4e8eSoftmax\u6ce8\u610f\u529b\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u4ee5\u53ca\u4e0d\u65ad\u589e\u957f\u7684\u952e\u503c\uff08KV\uff09\u7f13\u5b58\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLizard\u7684\u7ebf\u6027\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u9884\u8bad\u7ec3\u7684Transformer\u7c7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8f6c\u5316\u4e3a\u5177\u6709\u7075\u6d3b\u7684\u3001\u4e9a\u4e8c\u6b21\uff08subquadratic\uff09\u590d\u6742\u5ea6\u7684\u65e0\u9650\u4e0a\u4e0b\u6587\u751f\u6210\u67b6\u6784\u3002Lizard\u5f15\u5165\u4e86\u4e9a\u4e8c\u6b21\u6ce8\u610f\u529b\u673a\u5236\u6765\u8fd1\u4f3cSoftmax\u6ce8\u610f\u529b\uff0c\u5e76\u7ed3\u5408\u4e86\u95e8\u63a7\u673a\u5236\u548c\u5143\u8bb0\u5fc6\u4f53\u589e\u5f3a\u7684\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\uff0c\u5f62\u6210\u6df7\u5408\u673a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u786c\u4ef6\u611f\u77e5\u7684\u7b97\u6cd5\u6765\u52a0\u901f\u8bad\u7ec3\u3002", "result": "Lizard\u5728\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u7684\u6027\u80fd\u6062\u590d\uff0c\u663e\u8457\u4f18\u4e8e\u4e4b\u524d\u7684\u7ebf\u6027\u5316\u65b9\u6cd5\u3002\u57285\u6b21MMLU\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLizard\u6bd4\u5148\u524d\u6a21\u578b\u63d0\u9ad8\u4e8618\u4e2a\u70b9\uff0c\u5e76\u5728\u8054\u60f3\u8bb0\u5fc6\u4efb\u52a1\u4e0a\u663e\u793a\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "Lizard\u901a\u8fc7\u7ed3\u5408\u95e8\u63a7\u7ebf\u6027\u6ce8\u610f\u529b\u3001\u6ed1\u52a8\u7a97\u53e3\u6ce8\u610f\u529b\u4ee5\u53ca\u5143\u8bb0\u5fc6\u4f53\uff0c\u5e76\u5f15\u5165\u786c\u4ef6\u611f\u77e5\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u8bed\u8a00\u6a21\u578b\u4efb\u52a1\u4e0a\u63a5\u8fd1\u65e0\u635f\u5730\u6062\u590d\u6559\u5e08\u6a21\u578b\u7684\u6027\u80fd\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u7684\u7ebf\u6027\u5316\u65b9\u6cd5\uff0c\u5e76\u5728MMLU\u548c\u8054\u60f3\u8bb0\u5fc6\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2507.09928", "categories": ["cs.GT", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09928", "abs": "https://arxiv.org/abs/2507.09928", "authors": ["Apurv Shukla", "Vijay Subramanian", "Andy Zhao", "Rahul Jain"], "title": "Generalized Quantal Response Equilibrium: Existence and Efficient Learning", "comment": null, "summary": "We introduce a new solution concept for bounded rational agents in finite\nnormal-form general-sum games called Generalized Quantal Response Equilibrium\n(GQRE) which generalizes Quantal Response\nEquilibrium~\\citep{mckelvey1995quantal}. In our setup, each player maximizes a\nsmooth, regularized expected utility of the mixed profiles used, reflecting\nbounded rationality that subsumes stochastic choice. After establishing\nexistence under mild conditions, we present computationally efficient no-regret\nindependent learning via smoothened versions of the Frank-Wolfe algorithm. Our\nalgorithm uses noisy but correlated gradient estimates generated via a\nsimulation oracle that reports on repeated plays of the game. We analyze\nconvergence properties of our algorithm under assumptions that ensure\nuniqueness of equilibrium, using a class of gap functions that generalize the\nNash gap. We end by demonstrating the effectiveness of our method on a set of\ncomplex general-sum games such as high-rank two-player games, large action\ntwo-player games, and known examples of difficult multi-player games.", "AI": {"tldr": "This paper presents GQRE, a new equilibrium concept for boundedly rational players in games, generalizing existing methods. It includes an efficient learning algorithm based on Frank-Wolfe and proves its effectiveness on complex games.", "motivation": "The motivation is to address the limitations of existing solution concepts in game theory when dealing with boundedly rational agents. The paper aims to generalize Quantal Response Equilibrium to a broader class of agents and games, providing a more realistic model of strategic decision-making under bounded rationality. Additionally, it seeks to develop efficient algorithms for computing equilibria in these games.", "method": "The paper introduces Generalized Quantal Response Equilibrium (GQRE) by having each player maximize a smooth, regularized expected utility of the mixed profiles used, reflecting bounded rationality that subsumes stochastic choice. It then presents a computationally efficient no-regret independent learning algorithm using smoothened versions of the Frank-Wolfe algorithm. This algorithm utilizes noisy but correlated gradient estimates from a simulation oracle that reports on repeated plays. Convergence properties are analyzed using a class of gap functions that generalize the Nash gap, particularly under assumptions ensuring unique equilibrium.", "result": "The paper establishes the existence of GQRE under mild conditions. It presents an efficient no-regret learning algorithm with analyzed convergence properties under assumptions of unique equilibrium. The effectiveness of the proposed method is demonstrated through successful application to complex games, including high-rank two-player games, large action two-player games, and multi-player games.", "conclusion": "In conclusion, this paper introduces Generalized Quantal Response Equilibrium (GQRE) as a novel solution concept for bounded rational agents in finite normal-form general-sum games. It generalizes Quantal Response Equilibrium and offers a computationally efficient no-regret independent learning algorithm via smoothened Frank-Wolfe. The paper also analyzes convergence properties and demonstrates the method's effectiveness on various complex games."}}
{"id": "2507.10181", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.10181", "abs": "https://arxiv.org/abs/2507.10181", "authors": ["Kalmer Apinis", "Danel Ahman"], "title": "A simple formalization of alpha-equivalence", "comment": "to be submitted to LMCS", "summary": "While teaching untyped $\\lambda$-calculus to undergraduate students, we were\nwondering why $\\alpha$-equivalence is not directly inductively defined. In this\npaper, we demonstrate that this is indeed feasible. Specifically, we provide a\ngrounded, inductive definition for $\\alpha$-equivalence and show that it\nconforms to the specification provided in the literature. The work presented in\nthis paper is fully formalized in the Rocq Prover.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u57fa\u4e8e\u5f52\u7eb3\u6cd5\u7684 \u03b1-\u7b49\u4ef7\u5b9a\u4e49\uff0c\u5e76\u7528 Rocq Prover \u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u89e3\u51b3\u4e86\u4e4b\u524d\u672a\u76f4\u63a5\u8fdb\u884c\u5f52\u7eb3\u6cd5\u5b9a\u4e49\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u5728\u5411\u672c\u79d1\u751f\u8bb2\u6388\u975e\u7c7b\u578b lambda \u6f14\u7b97\uff08untyped $\\lambda$-calculus\uff09\u65f6\uff0c\u4e3a\u4f55 \u03b1-\u7b49\u4ef7\u6ca1\u6709\u76f4\u63a5\u88ab\u5b9a\u4e49\u4e3a\u5f52\u7eb3\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a \u03b1-\u7b49\u4ef7\u7684 grounded, inductive definition\uff0c\u5e76\u4e14\u8be5\u5b9a\u4e49\u5df2\u901a\u8fc7 Rocq Prover \u7684\u5f62\u5f0f\u5316\u9a8c\u8bc1\u3002", "result": "\u8be5\u8bba\u6587\u6210\u529f\u5730\u63d0\u4f9b\u4e86\u4e00\u4e2a \u03b1-\u7b49\u4ef7\u7684 grounded, inductive definition\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u7b26\u5408\u73b0\u6709\u7684\u6587\u732e\u89c4\u8303\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u884c\u7684\u3001\u57fa\u4e8e\u5f52\u7eb3\u6cd5\uff08grounded, inductive definition\uff09\u7684 \u03b1-\u7b49\u4ef7\uff08\u03b1-equivalence\uff09\u7684\u5b9a\u4e49\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u5b9a\u4e49\u7b26\u5408\u6587\u732e\u4e2d\u7684\u89c4\u8303\u3002"}}
{"id": "2507.09268", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09268", "abs": "https://arxiv.org/abs/2507.09268", "authors": ["Xiangjun Li", "Zilong Liu", "Zhengchun Zhou", "Pingzhi Fan"], "title": "Matched Filtering-Based Channel Estimation for AFDM Systems in Doubly Selective Channels", "comment": null, "summary": "Affine frequency division multiplexing (AFDM) has recently emerged as an\nexcellent backward-compatible 6G waveform. In this paper, an enhanced AFDM is\nproposed whereby the delay-Doppler (DD) coupling phase is considered.\nSpecifically, we study matched filtering (MF) assisted channel estimation (CE)\nfor AFDM systems in complex doubly selective channels. By deriving the complete\ninput-output relationship, the inter-chirp-carrier interference,\nsignal-to-interference-plus-noise ratio (SINR), and the effective SINR loss of\nAFDM, are investigated in discrete affine Fourier transform (DAFT) domain.\nFurther, we look into the path ambiguity problem and show that it may lead to\nsevere performance deterioration in fractional-delay fractional-Doppler\nchannels. To address such a problem, we introduce an MF assisted CE scheme\nbuilding upon a novel pilot arrangement across two consecutive AFDM\ntransmissions. This allows us to sequentially estimate the parameters of each\npath by exploiting the separability and approximate orthogonality of different\npaths in the DAFT domain, thus leading to significantly reduced complexity.\nFurthermore, based on generalized Fibonacci search (GFS), an MF-GFS scheme is\nproposed to avoid significantly redundant computation, which can be extended to\ntypical wide-band systems. Extensive simulation results indicate that the\nproposed schemes offer superior advantages in terms of their improved\ncommunication performance and lower complexity.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AFDM\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8def\u5f84\u6a21\u7cca\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u5347\u4e86\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u5206\u6570\u5ef6\u8fdf\u5206\u6570\u591a\u666e\u52d2\uff08FD-FD\uff09\u4fe1\u9053\u4e2d\uff0c\u8def\u5f84\u6a21\u7cca\u95ee\u9898\u53ef\u80fd\u5bfc\u81f4\u7684\u4e25\u91cd\u6027\u80fd\u6076\u5316\u95ee\u9898\uff0c\u5e76\u4e3aAFDM\u7cfb\u7edf\u63d0\u4f9b\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u3001\u9ad8\u6027\u80fd\u7684\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u5b8c\u6574\u7684\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff0c\u5728\u79bb\u6563\u4eff\u5c04\u5085\u91cc\u53f6\u53d8\u6362\uff08DAFT\uff09\u57df\u7814\u7a76\u4e86\u5339\u914d\u6ee4\u6ce2\uff08MF\uff09\u8f85\u52a9\u4fe1\u9053\u4f30\u8ba1\uff08CE\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u5e7f\u4e49\u6590\u6ce2\u90a3\u5951\u641c\u7d22\uff08GFS\uff09\u7684MF-GFS\u65b9\u6848\u6765\u907f\u514d\u5197\u4f59\u8ba1\u7b97\u3002", "result": "\u63d0\u51fa\u7684\u65b9\u6848\u5728\u901a\u4fe1\u6027\u80fd\u548c\u4f4e\u590d\u6742\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u4eff\u771f\u7ed3\u679c\u8bc1\u5b9e\u4e86\u5176\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u578bAFDM\u7cfb\u7edf\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u5bfc\u9891\u6392\u5217\u548c\u5339\u914d\u6ee4\u6ce2\uff08MF\uff09\u8f85\u52a9\u4fe1\u9053\u4f30\u8ba1\uff08CE\uff09\u65b9\u6848\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u8def\u5f84\u6a21\u7cca\u95ee\u9898\uff0c\u964d\u4f4e\u4e86\u7cfb\u7edf\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u9ad8\u4e86\u901a\u4fe1\u6027\u80fd\u3002"}}
{"id": "2507.09965", "categories": ["cs.MA", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09965", "abs": "https://arxiv.org/abs/2507.09965", "authors": ["Weiyu Chen", "Chengjie Liu", "Wenhao Huang", "Jinyang Lyu", "Mingqian Yang", "Yuan Du", "Li Du", "Jun Yang"], "title": "AnalogTester: A Large Language Model-Based Framework for Automatic Testbench Generation in Analog Circuit Design", "comment": "accepted by ISEDA 2025", "summary": "Recent advancements have demonstrated the significant potential of large\nlanguage models (LLMs) in analog circuit design. Nevertheless, testbench\nconstruction for analog circuits remains manual, creating a critical bottleneck\nin achieving fully automated design processes. Particularly when replicating\ncircuit designs from academic papers, manual Testbench construction demands\ntime-intensive implementation and frequent adjustments, which fails to address\nthe dynamic diversity and flexibility requirements for automation. AnalogTester\ntackles automated analog design challenges through an LLM-powered pipeline: a)\ndomain-knowledge integration, b) paper information extraction, c) simulation\nscheme synthesis, and d) testbench code generation with Tsinghua Electronic\nDesign (TED). AnalogTester has demonstrated automated Testbench generation\ncapabilities for three fundamental analog circuit types: operational amplifiers\n(op-amps), bandgap references (BGRs), and low-dropout regulators (LDOs), while\nmaintaining a scalable framework for adaptation to broader circuit topologies.\nFurthermore, AnalogTester can generate circuit knowledge data and TED code\ncorpus, establishing fundamental training datasets for LLM specialization in\nanalog circuit design automation.", "AI": {"tldr": "AnalogTester \u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u81ea\u52a8\u5316\u751f\u6210\u6a21\u62df\u7535\u8def\u6d4b\u8bd5\u5e73\u53f0\uff0c\u89e3\u51b3\u4e86\u624b\u52a8\u8bbe\u8ba1\u7684\u74f6\u9888\uff0c\u5e76\u4e3a\u672a\u6765 LLM \u5728\u8be5\u9886\u57df\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u4e2d\u624b\u52a8\u521b\u5efa\u6d4b\u8bd5\u5e73\u53f0\u6240\u5e26\u6765\u7684\u74f6\u9888\u95ee\u9898\uff0c\u5b9e\u73b0\u8bbe\u8ba1\u6d41\u7a0b\u7684\u5168\u9762\u81ea\u52a8\u5316\u3002", "method": "\u901a\u8fc7 LLM \u9a71\u52a8\u7684\u6d41\u6c34\u7ebf\uff0c\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u63d0\u53d6\u8bba\u6587\u4fe1\u606f\uff0c\u5408\u6210\u4eff\u771f\u65b9\u6848\uff0c\u5e76\u5229\u7528 Tsinghua Electronic Design (TED) \u751f\u6210\u6d4b\u8bd5\u5e73\u53f0\u4ee3\u7801\u3002", "result": "AnalogTester \u6210\u529f\u4e3a\u8fd0\u7b97\u653e\u5927\u5668\uff08op-amps\uff09\u3001\u5e26\u9699\u57fa\u51c6\u6e90\uff08BGRs\uff09\u548c\u4f4e\u538b\u5dee\u7ebf\u6027\u7a33\u538b\u5668\uff08LDOs\uff09\u7b49\u4e09\u79cd\u57fa\u672c\u6a21\u62df\u7535\u8def\u7c7b\u578b\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\uff0c\u5e76\u4e14\u5176\u6846\u67b6\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff0c\u80fd\u591f\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u7535\u8def\u62d3\u6251\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u80fd\u751f\u6210\u7535\u8def\u77e5\u8bc6\u6570\u636e\u548c TED \u4ee3\u7801\u8bed\u6599\u5e93\uff0c\u4e3a LLM \u5728\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u9886\u57df\u7684\u4e13\u4e1a\u5316\u8bad\u7ec3\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "AnalogTester \u6210\u529f\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u6d4b\u8bd5\u5e73\u53f0\u751f\u6210\uff0c\u4e3a\u6a21\u62df\u7535\u8def\u8bbe\u8ba1\u81ea\u52a8\u5316\u5e26\u6765\u4e86\u91cd\u5927\u7a81\u7834\u3002"}}
{"id": "2507.08838", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08838", "abs": "https://arxiv.org/abs/2507.08838", "authors": ["Xiaohang Tang", "Rares Dolga", "Sangwoong Yoon", "Ilija Bogunovic"], "title": "wd1: Weighted Policy Optimization for Reasoning in Diffusion Language Models", "comment": "Preprint", "summary": "Improving the reasoning capabilities of diffusion-based large language models\n(dLLMs) through reinforcement learning (RL) remains an open problem. The\nintractability of dLLMs likelihood function necessitates approximating the\ncurrent, old, and reference policy likelihoods at each policy optimization\nstep. This reliance introduces additional computational overhead and lead to\npotentially large bias -- particularly when approximation errors occur in the\ndenominator of policy ratios used for importance sampling. To mitigate these\nissues, we introduce $\\mathtt{wd1}$, a novel policy optimization approach that\nreformulates the objective as a weighted likelihood, requiring only a single\napproximation for the current parametrized policy likelihood. Experiments on\nwidely used reasoning benchmarks demonstrate that $\\mathtt{wd1}$, without\nsupervised fine-tuning (SFT) or any supervised data, outperforms existing RL\nmethods for dLLMs, achieving up to 16% higher accuracy. $\\mathtt{wd1}$ delivers\nadditional computational gains, including reduced training time and fewer\nfunction evaluations (NFEs) per gradient step. These findings, combined with\nthe simplicity of method's implementation and R1-Zero-like training (no SFT),\nposition $\\mathtt{wd1}$ as a more effective and efficient method for applying\nRL to dLLMs reasoning.", "AI": {"tldr": "$\\\\,\\text{wd1}$\u662f\u4e00\u79cd\u65b0\u9896\u7684RL\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e00\u6b21\u7b56\u7565\u4f3c\u7136\u8fd1\u4f3c\u6765\u63d0\u9ad8dLLMs\u7684\u63a8\u7406\u80fd\u529b\uff0c\u65e0\u9700SFT\u5373\u53ef\u5b9e\u73b0\u66f4\u9ad8\u7684\u51c6\u786e\u7387\u548c\u8ba1\u7b97\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6269\u6563\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u5728dLLMs\u4e2d\u7531\u4e8e\u9700\u8981\u8fd1\u4f3c\u591a\u4e2a\u7b56\u7565\u4f3c\u7136\u800c\u5bfc\u81f4\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u504f\u5dee\u95ee\u9898\u3002", "method": "$\\\\,\\text{wd1}$\u662f\u4e00\u79cd\u65b0\u9896\u7684\u7b56\u7565\u4f18\u5316\u65b9\u6cd5\uff0c\u5b83\u5c06\u76ee\u6807\u91cd\u65b0\u8868\u8ff0\u4e3a\u52a0\u6743\u4f3c\u7136\uff0c\u4ec5\u9700\u8981\u5bf9\u5f53\u524d\u53c2\u6570\u5316\u7b56\u7565\u4f3c\u7136\u8fdb\u884c\u4e00\u6b21\u8fd1\u4f3c\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709dLLMs\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u4e2d\u7531\u4e8e\u9700\u8981\u591a\u6b21\u7b56\u7565\u4f3c\u7136\u8fd1\u4f3c\u800c\u5e26\u6765\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u6f5c\u5728\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c$\\\\,\\text{wd1}$\u5728\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u4efb\u4f55\u76d1\u7763\u6570\u636e\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u63a8\u7406\u51c6\u786e\u7387\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709RL\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u4e5f\u8868\u73b0\u66f4\u4f18\uff0c\u5305\u62ec\u51cf\u5c11\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u6bcf\u6b21\u68af\u5ea6\u4e0b\u964d\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff08NFEs\uff09\u3002", "conclusion": "$\\\\,\\text{wd1}$\u662f\u4e00\u79cd\u66f4\u6709\u6548\u3001\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5c06\u5f3a\u5316\u5b66\u4e60\u5e94\u7528\u4e8e\u6269\u6563\u578b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08dLLMs\uff09\u7684\u63a8\u7406\u4efb\u52a1\uff0c\u5176\u65e0\u9700\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\u6216\u4efb\u4f55\u76d1\u7763\u6570\u636e\u5373\u53ef\u5728\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709RL\u65b9\u6cd5\u9ad8\u8fbe16%\u7684\u51c6\u786e\u7387\uff0c\u540c\u65f6\u8fd8\u80fd\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u6bcf\u6b21\u68af\u5ea6\u4e0b\u964d\u7684\u51fd\u6570\u8bc4\u4f30\u6b21\u6570\uff08NFEs\uff09\u3002"}}
{"id": "2507.09598", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09598", "abs": "https://arxiv.org/abs/2507.09598", "authors": ["Shinichi Amaha", "Tsuyoshi Hatano", "Takashi Nakajima", "Seigo Tarucha"], "title": "Vertically Coupled Double Quantum Dots Connected In Parallel", "comment": null, "summary": "We report charge transport measurements in a ring-shaped quadruple quantum\ndot system, composed of two vertically coupled double quantum dots connected in\nparallel. The vertical coupling introduces an isospin degree of freedom tied to\nthe layer index, and the parallel configuration enables independent access to\neach quantum dot pair. This design allows us to observe Coulomb diamonds and\nevaluate the interlayer energy offset. By extending this platform to triangular\nand hexagonal artificial lattices, we explore correlation effects such as\nisospin frustration. These results highlight the system's potential for\nstudying interaction-driven quantum phases.", "AI": {"tldr": "\u5728\u73af\u5f62\u56db\u91cd\u91cf\u5b50\u70b9\u7cfb\u7edf\u4e2d\u89c2\u5bdf\u5230\u5e93\u4ed1 \u0926\u093f\u0938\u0924\u0947\u548c\u5f02\u65cb\u632b\u6298\u7b49\u73b0\u8c61\u3002", "motivation": "\u7814\u7a76\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u91cf\u5b50\u76f8\uff0c\u5229\u7528\u5782\u76f4\u8026\u5408\u5f15\u5165\u4e0e\u5c42\u7d22\u5f15\u76f8\u5173\u7684\u5f02\u65cb\u81ea\u7531\u5ea6\uff0c\u5e76\u901a\u8fc7\u5e76\u8054\u914d\u7f6e\u72ec\u7acb\u8bbf\u95ee\u6bcf\u4e2a\u91cf\u5b50\u70b9\u5bf9\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u73af\u5f62\u56db\u91cd\u91cf\u5b50\u70b9\u7cfb\u7edf\uff08\u7531\u4e24\u4e2a\u5782\u76f4\u8026\u5408\u7684\u53cc\u91cf\u5b50\u70b9\u5e76\u8054\u7ec4\u6210\uff09\u4e2d\u7684\u7535\u8377\u4f20\u8f93\u6765\u7814\u7a76\u3002", "result": "\u89c2\u5bdf\u5230\u5e93\u4ed1 \u0926\u093f\u0938\u0924\u0947\u5e76\u8bc4\u4f30\u4e86\u5c42\u95f4\u80fd\u91cf\u504f\u79fb\uff0c\u5e76\u63a2\u7d22\u4e86\u8bf8\u5982\u5f02\u65cb\u632b\u6298\u7b49\u76f8\u5173\u6548\u5e94\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u6709\u6f5c\u529b\u7528\u4e8e\u7814\u7a76\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u91cf\u5b50\u76f8"}}
{"id": "2507.09879", "categories": ["cs.DS", "cs.AI", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09879", "abs": "https://arxiv.org/abs/2507.09879", "authors": ["Tanvi Bajpai", "Chandra Chekuri", "Pooja Kulkarni"], "title": "Covering a Few Submodular Constraints and Applications", "comment": "34 pages. Accepted to APPROX 2025", "summary": "We consider the problem of covering multiple submodular constraints. Given a\nfinite ground set $N$, a cost function $c: N \\rightarrow \\mathbb{R}_+$, $r$\nmonotone submodular functions $f_1,f_2,\\ldots,f_r$ over $N$ and requirements\n$b_1,b_2,\\ldots,b_r$ the goal is to find a minimum cost subset $S \\subseteq N$\nsuch that $f_i(S) \\ge b_i$ for $1 \\le i \\le r$. When $r=1$ this is the\nwell-known Submodular Set Cover problem. Previous work\n\\cite{chekuri2022covering} considered the setting when $r$ is large and\ndeveloped bi-criteria approximation algorithms, and approximation algorithms\nfor the important special case when each $f_i$ is a weighted coverage function.\nThese are fairly general models and capture several concrete and interesting\nproblems as special cases. The approximation ratios for these problem are at\nleast $\\Omega(\\log r)$ which is unavoidable when $r$ is part of the input. In\nthis paper, motivated by some recent applications, we consider the problem when\n$r$ is a \\emph{fixed constant} and obtain two main results. For covering\nmultiple submodular constraints we obtain a randomized bi-criteria\napproximation algorithm that for any given integer $\\alpha \\ge 1$ outputs a set\n$S$ such that $f_i(S) \\ge$ $(1-1/e^\\alpha -\\epsilon)b_i$ for each $i \\in [r]$\nand $\\mathbb{E}[c(S)] \\le (1+\\epsilon)\\alpha \\cdot \\sf{OPT}$. Second, when the\n$f_i$ are weighted coverage functions from a deletion-closed set system we\nobtain a $(1+\\epsilon)$ $(\\frac{e}{e-1})$ $(1+\\beta)$-approximation where\n$\\beta$ is the approximation ratio for the underlying set cover instances via\nthe natural LP. These results show that one can obtain nearly as good an\napproximation for any fixed $r$ as what one would achieve for $r=1$. We mention\nsome applications that follow easily from these general results and anticipate\nmore in the future.", "AI": {"tldr": "\u5f53\u9700\u8981\u8986\u76d6\u591a\u4e2a\u5b50\u6a21\u7ea6\u675f\u65f6\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u56fa\u5b9a\u7ea6\u675f\u6570\u91cf\u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002\u5bf9\u4e8e\u4e00\u822c\u60c5\u51b5\uff0c\u7b97\u6cd5\u80fd\u5728\u6210\u672c\u63a5\u8fd1\u6700\u4f18\u7684\u540c\u65f6\u6ee1\u8db3\u7ea6\u675f\u3002\u5bf9\u4e8e\u7279\u5b9a\u7c7b\u578b\u7684\u7ea6\u675f\uff08\u52a0\u6743\u8986\u76d6\u51fd\u6570\uff09\uff0c\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u8fd1\u4f3c\u6bd4\uff0c\u63a5\u8fd1\u4e8e\u5355\u4e2a\u7ea6\u675f\u7684\u60c5\u51b5\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u6e90\u4e8e\u8fd1\u671f\u7684\u4e00\u4e9b\u5e94\u7528\u9700\u6c42\uff0c\u65e8\u5728\u89e3\u51b3\u8986\u76d6\u591a\u4e2a\u5b50\u6a21\u7ea6\u675f\u7684\u95ee\u9898\u3002\u5f53\u7ea6\u675f\u6570\u91cfr\u662f\u4e00\u4e2a\u56fa\u5b9a\u5e38\u6570\u65f6\uff0c\u671f\u671b\u83b7\u5f97\u6bd4r\u662f\u8f93\u5165\u53d8\u91cf\u65f6\u66f4\u597d\u7684\u8fd1\u4f3c\u6bd4\uff0c\u751a\u81f3\u63a5\u8fd1\u4e8er=1\uff08\u5373\u7ecf\u5178\u7684\u5b50\u6a21\u96c6\u8986\u76d6\u95ee\u9898\uff09\u7684\u8fd1\u4f3c\u6027\u80fd\u3002", "method": "\u8be5\u7814\u7a76\u5229\u7528\u4e86\u5b50\u6a21\u51fd\u6570\u548c\u96c6\u5408\u8986\u76d6\u95ee\u9898\u7684\u76f8\u5173\u7406\u8bba\u3002\u9488\u5bf9\u591a\u4e2a\u5b50\u6a21\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\u3002\u5bf9\u4e8e\u52a0\u6743\u8986\u76d6\u51fd\u6570\u548c\u5220\u9664\u95ed\u5408\u96c6\u7cfb\u7edf\uff0c\u5219\u5229\u7528\u4e86\u7ebf\u6027\u89c4\u5212\uff08LP\uff09\u677e\u5f1b\u4ee5\u53ca\u76f8\u5173\u7684\u8fd1\u4f3c\u6280\u672f\u6765\u63a8\u5bfc\u51fa\u8fd1\u4f3c\u6bd4\u3002", "result": "\u5bf9\u4e8e\u6d89\u53ca\u591a\u4e2a\u5b50\u6a21\u7ea6\u675f\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u80fd\u591f\u4e3a\u6bcf\u4e2a\u7ea6\u675f $f_i$ \u627e\u5230\u4e00\u4e2a\u5b50\u96c6 $S$\uff0c\u4f7f\u5f97 $f_i(S) \nless$ $(1-1/e^\u03b1 - \boldsymbol{\nu})$ $b_i$\uff0c\u5e76\u4e14\u671f\u671b\u6210\u672c $\boldsymbol{\notin}$ $[c(S)] \boldsymbol{\notin}$ $(1+\boldsymbol{\nu})\boldsymbol{\notin}\boldsymbol{\notin}\boldsymbol{\notin}$ \u00b7OPT\u3002\u5bf9\u4e8e\u52a0\u6743\u8986\u76d6\u51fd\u6570\u548c\u5220\u9664\u95ed\u5408\u96c6\u7cfb\u7edf\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd $(1+\boldsymbol{\nu})$ $(\frac{e}{e-1})$ $(1+\boldsymbol{\beta})$-\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5176\u4e2d $\boldsymbol{\beta}$ \u662f\u57fa\u7840\u96c6\u8986\u76d6\u5b9e\u4f8b\u7684LP\u8fd1\u4f3c\u6bd4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u6d89\u53ca\u591a\u4e2a\u5b50\u6a21\u7ea6\u675f\u7684\u8986\u76d6\u95ee\u9898\u63d0\u4f9b\u4e86\u4e24\u79cd\u4e3b\u8981\u7ed3\u679c\u3002\u5bf9\u4e8e\u4e00\u822c\u7684\u5b50\u6a21\u7ea6\u675f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u80fd\u591f\u4ee5\u8fd1\u4f3c\u56e0\u5b50 O(\u03b1) \u627e\u5230\u6ee1\u8db3\u7ea6\u675f\u7684\u5b50\u96c6\uff0c\u540c\u65f6\u786e\u4fdd\u6210\u672c\u63a5\u8fd1\u6700\u4f18\u503c\u3002\u5bf9\u4e8e\u52a0\u6743\u8986\u76d6\u51fd\u6570\u548c\u5220\u9664\u95ed\u5408\u96c6\u7cfb\u7edf\uff0c\u5219\u63d0\u51fa\u4e86\u4e00\u4e2a\u663e\u8457\u6539\u8fdb\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5176\u8fd1\u4f3c\u6bd4\u63a5\u8fd1\u4e8e\u5355\u4e2a\u5b50\u6a21\u7ea6\u675f\u7684\u60c5\u51b5\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u56fa\u5b9a\u7684\u7ea6\u675f\u6570\u91cf\uff0c\u53ef\u4ee5\u83b7\u5f97\u4e0e\u5355\u4e2a\u7ea6\u675f\u60c5\u51b5\u76f8\u5f53\u7684\u8fd1\u4f3c\u6027\u80fd\u3002"}}
{"id": "2507.09369", "categories": ["cs.AI", "68T01", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "AI": {"tldr": "\u672c\u62a5\u544a\u5bf9\u4eba\u5de5\u667a\u80fd\u53ef\u80fd\u5bfc\u81f4\u7684\u6240\u6709\u4eba\u7c7b\u706d\u7edd\u4e8b\u4ef6\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u4e3e\u4f8b\uff0c\u65e8\u5728\u5f15\u8d77\u516c\u4f17\u5173\u6ce8\u5e76\u63a8\u52a8\u9884\u9632\u63aa\u65bd\u4ee5\u907f\u514d\u6b64\u7c7b\u98ce\u9669\u3002", "motivation": "\u901a\u8fc7\u516c\u5f00\u5c55\u793a\u4eba\u5de5\u667a\u80fd\u53ef\u80fd\u5bfc\u81f4\u7684\u6240\u6709\u4eba\u7c7b\u706d\u7edd\u7684\u4e8b\u4ef6\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u671f\u83b7\u5f97\u516c\u4f17\u652f\u6301\uff0c\u4ece\u800c\u91c7\u53d6\u9884\u9632\u63aa\u65bd\uff0c\u907f\u514d\u707e\u96be\u6027\u98ce\u9669\u3002", "method": "\u672c\u62a5\u544a\u4ecb\u7ecd\u4e86\u6f5c\u5728\u7684\u3001\u7531\u4eba\u5de5\u667a\u80fd\u5bfc\u81f4\u7684\u6240\u6709\u4eba\u7c7b\u706d\u7edd\u7684\u4e8b\u4ef6\u7684\u5206\u7c7b\u6cd5\u548c\u793a\u4f8b\u3002", "result": "\u62a5\u544a\u65e8\u5728\u901a\u8fc7\u63ed\u793a\u6f5c\u5728\u7684\u706d\u7edd\u60c5\u666f\u6765\u63a8\u52a8\u9488\u5bf9\u4eba\u5de5\u667a\u80fd\u707e\u96be\u6027\u98ce\u9669\u7684\u9884\u9632\u63aa\u65bd\u3002", "conclusion": "\u672c\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u4eba\u5de5\u667a\u80fd\u53ef\u80fd\u5bfc\u81f4\u7684\u6240\u6709\u4eba\u7c7b\u706d\u7edd\u7684\u4e8b\u4ef6\u7684\u5206\u7c7b\u6cd5\u548c\u793a\u4f8b\u3002"}}
{"id": "2507.09776", "categories": ["eess.SP", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09776", "abs": "https://arxiv.org/abs/2507.09776", "authors": ["Mihir Kavishwar", "Naresh Shanbhag"], "title": "Compute SNR-Optimal Analog-to-Digital Converters for Analog In-Memory Computing", "comment": "Code available at: https://github.com/mihirvk2/CSNR-optimal-ADC", "summary": "Analog in-memory computing (AIMC) is an energy-efficient alternative to\ndigital architectures for accelerating machine learning and signal processing\nworkloads. However, its energy efficiency is limited by the high energy cost of\nthe column analog-to-digital converters (ADCs). Reducing the ADC precision is\nan effective approach to lowering its energy cost. However, doing so also\nreduces the AIMC's computational accuracy thereby making it critical to\nidentify the minimum precision required to meet a target accuracy. Prior works\noverestimate the ADC precision requirements by modeling quantization error as\ninput-independent noise, maximizing the signal-to-quantization-noise ratio\n(SQNR), and ignoring the discrete nature of ideal pre-ADC signal. We address\nthese limitations by developing analytical expressions for estimating the\ncompute signal-to-noise ratio (CSNR), a true metric of accuracy for AIMCs, and\npropose CACTUS, an algorithm to obtain CSNR-optimal ADC parameters. Using a\ncircuit-aware behavioral model of an SRAM-based AIMC in a 28nm CMOS process, we\nshow that for a 256-dimensional binary dot product, CACTUS reduces the ADC\nprecision requirements by 3b while achieving 6dB higher CSNR over prior\nmethods. We also delineate operating conditions under which our proposed\nCSNR-optimal ADCs outperform conventional SQNR-optimal ADCs.", "AI": {"tldr": "CACTUS\u7b97\u6cd5\u901a\u8fc7\u4f18\u5316ADC\u53c2\u6570\uff0c\u5728\u964d\u4f4eAIMC\u80fd\u8017\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u8ba1\u7b97\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6a21\u62df\u5185\u5b58\u8ba1\u7b97\uff08AIMC\uff09\u7684\u80fd\u6548\uff0c\u9700\u8981\u964d\u4f4eADC\u7684\u7cbe\u5ea6\u4ee5\u51cf\u5c11\u5176\u80fd\u8017\u3002\u7136\u800c\uff0c\u964d\u4f4eADC\u7cbe\u5ea6\u4f1a\u727a\u7272AIMC\u7684\u8ba1\u7b97\u7cbe\u5ea6\uff0c\u56e0\u6b64\u9700\u8981\u7cbe\u786e\u786e\u5b9a\u6ee1\u8db3\u76ee\u6807\u7cbe\u5ea6\u7684\u6700\u4f4eADC\u7cbe\u5ea6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u4fe1\u566a\u6bd4\uff08CSNR\uff09\u7684\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5f00\u53d1\u4e86CACTUS\u7b97\u6cd5\u6765\u83b7\u5f97CSNR\u6700\u4f18\u7684ADC\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u91cf\u5316\u8bef\u5dee\u3001\u4fe1\u566a\u6bd4\uff08SQNR\uff09\u4ee5\u53ca\u9884ADC\u4fe1\u53f7\u7684\u79bb\u6563\u7279\u6027\u3002", "result": "\u572828nm CMOS\u5de5\u827a\u4e0b\uff0c\u4f7f\u7528\u57fa\u4e8eSRAM\u7684AIMC\u7535\u8def\u611f\u77e5\u884c\u4e3a\u6a21\u578b\u8fdb\u884c\u4eff\u771f\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5bf9\u4e8e256\u7ef4\u4e8c\u5143\u70b9\u79ef\u8fd0\u7b97\uff0cCACTUS\u53ef\u964d\u4f4eADC\u7cbe\u5ea6\u8981\u6c423\u4f4d\uff0c\u5e76\u83b7\u5f97\u6bd4\u73b0\u6709\u65b9\u6cd5\u9ad86dB\u7684CSNR\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u660e\u786e\u4e86\u4f55\u79cd\u64cd\u4f5c\u6761\u4ef6\u4e0b\uff0c\u6240\u63d0\u51fa\u7684CSNR\u6700\u4f18ADC\u4f18\u4e8e\u4f20\u7edf\u7684SQNR\u6700\u4f18ADC\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5f00\u53d1\u7528\u4e8e\u4f30\u8ba1\u8ba1\u7b97\u4fe1\u566a\u6bd4\uff08CSNR\uff09\u7684\u89e3\u6790\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51faCACTUS\u7b97\u6cd5\u6765\u4f18\u5316\u6a21\u6570\u8f6c\u6362\u5668\uff08ADC\uff09\u53c2\u6570\uff0c\u4ece\u800c\u5728\u964d\u4f4eADC\u7cbe\u5ea6\u548c\u4fdd\u6301\u8ba1\u7b97\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u5bf9ADC\u7cbe\u5ea6\u8981\u6c42\u8fc7\u9ad8\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cCACTUS\u53ef\u964d\u4f4e3\u4f4dADC\u7cbe\u5ea6\u8981\u6c42\uff0c\u5e76\u63d0\u9ad86dB\u7684CSNR\u3002"}}
{"id": "2507.09685", "categories": ["eess.SY", "cs.LG", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09685", "abs": "https://arxiv.org/abs/2507.09685", "authors": ["Yutong Li", "Ilya Kolmanovsky"], "title": "Symptom-Driven Personalized Proton Pump Inhibitors Therapy Using Bayesian Neural Networks and Model Predictive Control", "comment": "6 pages, 5 figures", "summary": "Proton Pump Inhibitors (PPIs) are the standard of care for gastric acid\ndisorders but carry significant risks when administered chronically at high\ndoses. Precise long-term control of gastric acidity is challenged by the\nimpracticality of invasive gastric acid monitoring beyond 72 hours and wide\ninter-patient variability. We propose a noninvasive, symptom-based framework\nthat tailors PPI dosing solely on patient-reported reflux and digestive symptom\npatterns. A Bayesian Neural Network prediction model learns to predict patient\nsymptoms and quantifies its uncertainty from historical symptom scores, meal,\nand PPIs intake data. These probabilistic forecasts feed a chance-constrained\nModel Predictive Control (MPC) algorithm that dynamically computes future PPI\ndoses to minimize drug usage while enforcing acid suppression with high\nconfidence - without any direct acid measurement. In silico studies over\ndiverse dietary schedules and virtual patient profiles demonstrate that our\nlearning-augmented MPC reduces total PPI consumption by 65 percent compared to\nstandard fixed regimens, while maintaining acid suppression with at least 95\npercent probability. The proposed approach offers a practical path to\npersonalized PPI therapy, minimizing treatment burden and overdose risk without\ninvasive sensors.", "AI": {"tldr": "\u4e00\u79cd\u57fa\u4e8e\u75c7\u72b6\u7684\u975e\u4fb5\u5165\u5f0f\u6846\u67b6\uff0c\u5229\u7528\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u548c\u7ea6\u675f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6765\u4f18\u5316 PPI \u5242\u91cf\uff0c\u53ef\u51cf\u5c11 65% \u7684\u836f\u7269\u6d88\u8017\uff0c\u540c\u65f6\u7ef4\u6301\u9ad8\u9178\u6291\u5236\u7387\u3002", "motivation": "\u867d\u7136 PPI \u662f\u6cbb\u7597\u80c3\u9178\u76f8\u5173\u75be\u75c5\u7684\u6807\u51c6\u7597\u6cd5\uff0c\u4f46\u957f\u671f\u6216\u5927\u5242\u91cf\u4f7f\u7528\u4f1a\u5e26\u6765\u663e\u8457\u98ce\u9669\u3002\u7531\u4e8e\u65e0\u6cd5\u8fdb\u884c\u8d85\u8fc7 72 \u5c0f\u65f6\u7684\u4fb5\u5165\u6027\u80c3\u9178\u76d1\u6d4b\u4ee5\u53ca\u60a3\u8005\u95f4\u7684\u5de8\u5927\u5dee\u5f02\uff0c\u7cbe\u786e\u7684\u957f\u671f\u80c3\u9178\u63a7\u5236\u9762\u4e34\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u75c7\u72b6\u7684\u975e\u4fb5\u5165\u5f0f\u6846\u67b6\uff0c\u4ec5\u6839\u636e\u60a3\u8005\u62a5\u544a\u7684\u80c3\u98df\u7ba1\u53cd\u6d41\u548c\u6d88\u5316\u7cfb\u7edf\u75c7\u72b6\u6a21\u5f0f\u6765\u8c03\u6574 PPI \u5242\u91cf\u3002\u901a\u8fc7\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6a21\u578b\u9884\u6d4b\u60a3\u8005\u75c7\u72b6\uff0c\u5e76\u7ed3\u5408\u75c5\u4eba\u7684\u996e\u98df\u548c PPI \u6444\u5165\u6570\u636e\u6765\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u3002\u8be5\u9884\u6d4b\u7ed3\u679c\u4e3a\u7ea6\u675f\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7b97\u6cd5\u63d0\u4f9b\u8f93\u5165\uff0c\u4ee5\u52a8\u6001\u8ba1\u7b97\u672a\u6765\u7684 PPI \u5242\u91cf\uff0c\u4ece\u800c\u5728\u786e\u4fdd\u9178\u6291\u5236\u7684\u540c\u65f6\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u836f\u7269\u4f7f\u7528\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5c06 PPI \u7684\u603b\u6d88\u8017\u91cf\u4e0e\u6807\u51c6\u7684\u56fa\u5b9a\u65b9\u6848\u76f8\u6bd4\u51cf\u5c11 65%\uff0c\u540c\u65f6\u4ee5\u81f3\u5c11 95% \u7684\u6982\u7387\u7ef4\u6301\u9178\u6291\u5236\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u4e2a\u6027\u5316\u8d28\u5b50\u6cf5\u6291\u5236\u5242\uff08PPI\uff09\u6cbb\u7597\u63d0\u4f9b\u4e86\u4e00\u6761\u5b9e\u7528\u7684\u9014\u5f84\uff0c\u53ef\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u6cbb\u7597\u8d1f\u62c5\u548c\u8fc7\u91cf\u670d\u836f\u98ce\u9669\uff0c\u4e14\u65e0\u9700\u4fb5\u5165\u5f0f\u4f20\u611f\u5668\u3002"}}
{"id": "2507.09176", "categories": ["cs.RO", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09176", "abs": "https://arxiv.org/abs/2507.09176", "authors": ["Han Ye", "Yuqiang Jin", "Jinyuan Liu", "Tao Li", "Wen-An Zhang", "Minglei Fu"], "title": "DLBAcalib: Robust Extrinsic Calibration for Non-Overlapping LiDARs Based on Dual LBA", "comment": "9 pages,14 figures", "summary": "Accurate extrinsic calibration of multiple LiDARs is crucial for improving\nthe foundational performance of three-dimensional (3D) map reconstruction\nsystems. This paper presents a novel targetless extrinsic calibration framework\nfor multi-LiDAR systems that does not rely on overlapping fields of view or\nprecise initial parameter estimates. Unlike conventional calibration methods\nthat require manual annotations or specific reference patterns, our approach\nintroduces a unified optimization framework by integrating LiDAR bundle\nadjustment (LBA) optimization with robust iterative refinement. The proposed\nmethod constructs an accurate reference point cloud map via continuous scanning\nfrom the target LiDAR and sliding-window LiDAR bundle adjustment, while\nformulating extrinsic calibration as a joint LBA optimization problem. This\nmethod effectively mitigates cumulative mapping errors and achieves\noutlier-resistant parameter estimation through an adaptive weighting mechanism.\nExtensive evaluations in both the CARLA simulation environment and real-world\nscenarios demonstrate that our method outperforms state-of-the-art calibration\ntechniques in both accuracy and robustness. Experimental results show that for\nnon-overlapping sensor configurations, our framework achieves an average\ntranslational error of 5 mm and a rotational error of 0.2{\\deg}, with an\ninitial error tolerance of up to 0.4 m/30{\\deg}. Moreover, the calibration\nprocess operates without specialized infrastructure or manual parameter tuning.\nThe code is open source and available on GitHub\n(\\underline{https://github.com/Silentbarber/DLBAcalib})", "AI": {"tldr": "\u4e00\u79cd\u65e0\u9700\u91cd\u53e0\u89c6\u573a\u6216\u7cbe\u786e\u521d\u59cb\u53c2\u6570\u4f30\u8ba1\u7684\u65b0\u578b\u76ee\u6807\u7269\u5916\u53c2\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u6fc0\u5149\u96f7\u8fbe\u6346\u7ed1\u8c03\u6574\u548c\u8fed\u4ee3\u7cbe\u70bc\u4f18\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u591a\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u3002", "motivation": "\u7cbe\u786e\u7684\u591a\u6fc0\u5149\u96f7\u8fbe\u5916\u53c2\u6821\u51c6\u5bf9\u4e8e\u63d0\u9ad8\u4e09\u7ef4\u5730\u56fe\u91cd\u5efa\u7cfb\u7edf\u7684\u57fa\u7840\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65e0\u76ee\u6807\u7269\u5916\u53c2\u6821\u51c6\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u6fc0\u5149\u96f7\u8fbe\u6346\u7ed1\u8c03\u6574\uff08LBA\uff09\u4f18\u5316\u4e0e\u9c81\u68d2\u7684\u8fed\u4ee3\u7cbe\u70bc\u76f8\u7ed3\u5408\uff0c\u6784\u5efa\u7cbe\u786e\u7684\u53c2\u8003\u70b9\u4e91\u5730\u56fe\uff0c\u5e76\u5c06\u5916\u53c2\u6821\u51c6\u516c\u5f0f\u5316\u4e3a\u8054\u5408LBA\u4f18\u5316\u95ee\u9898\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u52a0\u6743\u673a\u5236\u6709\u6548\u7f13\u89e3\u7d2f\u79ef\u8bef\u5dee\u5e76\u5b9e\u73b0\u6297\u566a\u58f0\u53c2\u6570\u4f30\u8ba1\u3002", "result": "\u5728\u975e\u91cd\u53e0\u4f20\u611f\u5668\u914d\u7f6e\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5e73\u5747\u5e73\u79fb\u8bef\u5dee5\u6beb\u7c73\u548c\u65cb\u8f6c\u8bef\u5dee0.2\u5ea6\uff0c\u521d\u59cb\u8bef\u5dee\u5bb9\u5fcd\u5ea6\u9ad8\u8fbe0.4\u7c73/30\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728CARLA\u6a21\u62df\u73af\u5883\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u5e7f\u6cdb\u8bc4\u4f30\u4e2d\uff0c\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6821\u51c6\u6280\u672f\uff0c\u5e76\u4e14\u6821\u51c6\u8fc7\u7a0b\u65e0\u9700\u4e13\u95e8\u7684\u57fa\u7840\u8bbe\u65bd\u6216\u624b\u52a8\u53c2\u6570\u8c03\u6574\u3002"}}
{"id": "2507.09068", "categories": ["cs.CV", "cs.AI", "cs.IR", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09068", "abs": "https://arxiv.org/abs/2507.09068", "authors": ["Dell Zhang", "Xiangyu Chen", "Jixiang Luo", "Mengxi Jia", "Changzhi Sun", "Ruilong Ren", "Jingren Liu", "Hao Sun", "Xuelong Li"], "title": "Infinite Video Understanding", "comment": null, "summary": "The rapid advancements in Large Language Models (LLMs) and their multimodal\nextensions (MLLMs) have ushered in remarkable progress in video understanding.\nHowever, a fundamental challenge persists: effectively processing and\ncomprehending video content that extends beyond minutes or hours. While recent\nefforts like Video-XL-2 have demonstrated novel architectural solutions for\nextreme efficiency, and advancements in positional encoding such as HoPE and\nVideoRoPE++ aim to improve spatio-temporal understanding over extensive\ncontexts, current state-of-the-art models still encounter significant\ncomputational and memory constraints when faced with the sheer volume of visual\ntokens from lengthy sequences. Furthermore, maintaining temporal coherence,\ntracking complex events, and preserving fine-grained details over extended\nperiods remain formidable hurdles, despite progress in agentic reasoning\nsystems like Deep Video Discovery. This position paper posits that a logical,\nalbeit ambitious, next frontier for multimedia research is Infinite Video\nUnderstanding -- the capability for models to continuously process, understand,\nand reason about video data of arbitrary, potentially never-ending duration. We\nargue that framing Infinite Video Understanding as a blue-sky research\nobjective provides a vital north star for the multimedia, and the wider AI,\nresearch communities, driving innovation in areas such as streaming\narchitectures, persistent memory mechanisms, hierarchical and adaptive\nrepresentations, event-centric reasoning, and novel evaluation paradigms.\nDrawing inspiration from recent work on long/ultra-long video understanding and\nseveral closely related fields, we outline the core challenges and key research\ndirections towards achieving this transformative capability.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u4f5c\u4e3a\u591a\u5a92\u4f53AI\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\uff0c\u65e8\u5728\u89e3\u51b3\u957f\u89c6\u9891\u5904\u7406\u7684\u8ba1\u7b97\u3001\u8bb0\u5fc6\u548c\u8fde\u8d2f\u6027\u6311\u6218\uff0c\u5e76\u6307\u51fa\u4e86\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u7684\u5173\u952e\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u89c6\u9891\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5b83\u4eec\u5728\u5904\u7406\u548c\u7406\u89e3\u6570\u5c0f\u65f6\u4ee5\u4e0a\u7684\u957f\u89c6\u9891\u65f6\uff0c\u4ecd\u7136\u9762\u4e34\u8ba1\u7b97\u548c\u5185\u5b58\u7684\u9650\u5236\uff0c\u5e76\u4e14\u96be\u4ee5\u4fdd\u6301\u957f\u65f6\u5e8f\u7684\u8fde\u8d2f\u6027\u3001\u590d\u6742\u4e8b\u4ef6\u7684\u8ddf\u8e2a\u4ee5\u53ca\u7ec6\u7c92\u5ea6\u7ec6\u8282\u7684\u4fdd\u7559\u3002", "method": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5b9e\u73b0\u65e0\u9650\u89c6\u9891\u7406\u89e3\u6240\u9762\u4e34\u7684\u6838\u5fc3\u6311\u6218\uff0c\u5e76\u6982\u8ff0\u4e86\u5173\u952e\u7684\u7814\u7a76\u65b9\u5411\uff0c\u5305\u62ec\u6d41\u5f0f\u67b6\u6784\u3001\u6301\u4e45\u5316\u8bb0\u5fc6\u673a\u5236\u3001\u5206\u5c42\u548c\u81ea\u9002\u5e94\u8868\u793a\u3001\u4e8b\u4ef6\u4e2d\u5fc3\u63a8\u7406\u4ee5\u53ca\u65b0\u7684\u8bc4\u4f30\u8303\u5f0f\u3002", "result": "\u8bba\u6587\u8ba4\u4e3a\uff0c\u5c06\u65e0\u9650\u89c6\u9891\u7406\u89e3\u8bbe\u5b9a\u4e3a\u4e00\u4e2a\u5b8f\u4f1f\u7684\u84dd\u56fe\u7814\u7a76\u76ee\u6807\uff0c\u80fd\u591f\u4e3a\u591a\u5a92\u4f53\u548c\u66f4\u5e7f\u6cdb\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u91cd\u8981\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u63a8\u52a8\u5728\u6d41\u5f0f\u67b6\u6784\u3001\u6301\u4e45\u5316\u8bb0\u5fc6\u673a\u5236\u3001\u5206\u5c42\u548c\u81ea\u9002\u5e94\u8868\u793a\u3001\u4e8b\u4ef6\u4e2d\u5fc3\u63a8\u7406\u548c\u65b0\u8bc4\u4f30\u8303\u5f0f\u7b49\u9886\u57df\u7684\u521b\u65b0\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u201c\u65e0\u9650\u89c6\u9891\u7406\u89e3\u201d\u4f5c\u4e3a\u591a\u5a92\u4f53\u7814\u7a76\u7684\u4e0b\u4e00\u4e2a\u524d\u6cbf\u9886\u57df\uff0c\u65e8\u5728\u4f7f\u6a21\u578b\u80fd\u591f\u5904\u7406\u4efb\u610f\u957f\u5ea6\u7684\u89c6\u9891\u6570\u636e\u3002"}}
{"id": "2507.10367", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2507.10367", "abs": "https://arxiv.org/abs/2507.10367", "authors": ["Jingwei Xu", "Junbin Kang", "Mingkai Dong", "Mingyu Liu", "Lu Zhang", "Shaohong Guo", "Ziyan Qiu", "Mingzhen You", "Ziyi Tian", "Anqi Yu", "Tianhong Ding", "Xinwei Hu", "Haibo Chen"], "title": "FalconFS: Distributed File System for Large-Scale Deep Learning Pipeline", "comment": "Accepted by NSDI'26", "summary": "Client-side metadata caching has long been considered an effective method for\naccelerating metadata operations in distributed file systems (DFSs). However,\nwe have found that client-side state (e.g., caching) is not only ineffective\nbut also consumes valuable memory resources in the deep learning pipelines. We\nthus propose FalconFS, a DFS optimized for deep learning pipelines with the\nstateless-client architecture. Specifically, instead of performing client-side\npath resolution and caching, FalconFS efficiently resolves paths on the server\nside using hybrid metadata indexing and lazy namespace replication. FalconFS\nalso boosts server concurrency with concurrent request merging and provides\neasy deployment with VFS shortcut. Evaluations against CephFS and Lustre show\nthat FalconFS achieves up to 5.72$\\times$ throughput for small file read/write\nand up to 12.81$\\times$ throughput for deep learning model training. FalconFS\nhas been running in Huawei autonomous driving system's production environment\nwith 10,000 NPUs for one year.", "AI": {"tldr": "\u5ba2\u6237\u7aef\u65e0\u72b6\u6001\u7684DFS FalconFS\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u8def\u5f84\u89e3\u6790\u548c\u5e76\u53d1\u8bf7\u6c42\u5408\u5e76\uff0c\u5728\u6df1\u5ea6\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5e76\u5df2\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u6210\u529f\u5e94\u7528\u3002", "motivation": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u7edf\u7684\u5ba2\u6237\u7aef\u5143\u6570\u636e\u7f13\u5b58\u673a\u5236\u5728\u6df1\u5ea6\u5b66\u4e60\u6d41\u6c34\u7ebf\u4e2d\u4e0d\u4ec5\u6548\u679c\u4e0d\u4f73\uff0c\u53cd\u800c\u6d88\u8017\u4e86\u5b9d\u8d35\u7684\u5185\u5b58\u8d44\u6e90\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u4f18\u5316\u7684DFS\u65b9\u6848\u3002", "method": "FalconFS\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u6df1\u5ea6\u5b66\u4e60\u6d41\u6c34\u7ebf\u7684\u3001\u5ba2\u6237\u7aef\u65e0\u72b6\u6001\u7684\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf\uff08DFS\uff09\u67b6\u6784\u3002\u5b83\u6452\u5f03\u4e86\u5ba2\u6237\u7aef\u7f13\u5b58\uff0c\u901a\u8fc7\u670d\u52a1\u5668\u7aef\u6df7\u5408\u5143\u6570\u636e\u7d22\u5f15\u548c\u60f0\u6027\u547d\u540d\u7a7a\u95f4\u590d\u5236\u6765\u9ad8\u6548\u5730\u89e3\u6790\u8def\u5f84\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5e76\u53d1\u8bf7\u6c42\u5408\u5e76\u63d0\u5347\u4e86\u670d\u52a1\u5668\u5e76\u53d1\u80fd\u529b\uff0c\u5e76\u901a\u8fc7VFS\u5feb\u6377\u65b9\u5f0f\u7b80\u5316\u4e86\u90e8\u7f72\u3002", "result": "\u4e0eCephFS\u548cLustre\u76f8\u6bd4\uff0cFalconFS\u5728\u5c0f\u6587\u4ef6\u8bfb\u5199\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.72\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\uff0c\u5728\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe12.81\u500d\u7684\u541e\u5410\u91cf\u63d0\u5347\u3002", "conclusion": "FalconFS\u5df2\u88ab\u90e8\u7f72\u4e8e\u534e\u4e3a\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u751f\u4ea7\u73af\u5883\u4e2d\uff0c\u4e3a1\u4e07\u4e2aNPU\u63d0\u4f9b\u4e86\u4e00\u5e74\u7684\u7a33\u5b9a\u670d\u52a1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.09429", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09429", "abs": "https://arxiv.org/abs/2507.09429", "authors": ["Niklas A. Weber", "Miru Lee", "Florian Sch\u00f6newald", "Leonard Sch\u00fcler", "Vasily Moshnyaga", "Matthias Kr\u00fcger", "Cynthia A. Volkert"], "title": "Nanoscale friction of manganite superlattice films controlled by layer thickness and fluorine content", "comment": "19 pages, 4 figures", "summary": "We investigate nanoscale friction in [LaMnO3]m/[SrMnO3]n superlattice films\nusing lateral force microscopy, focusing on the effects of fluorine doping and\ntop-layer thickness. For all samples, friction forces scale linearly with the\nsum of the applied normal and adhesion forces. While friction forces vary\nspatially due to local adhesion fluctuations, the friction coefficient remains\nposition independent for each specimen. It is, however, systematically\ninfluenced by fluorine concentration and top-layer thickness. Our data\nindicates that frictional energy dissipation extends up to 5 nm beneath the\nsurface, demonstrating a clear dependence on subsurface structure. We attribute\nthis to viscoelastic dissipation within the stress field and evanescent waves\ngenerated by the sliding tip, which can quantitatively account for the observed\nfriction coefficients. These results show that, once adhesion is properly\naccounted for, the friction coefficient is a reproducible material property\nthat can be tuned via controlled modifications to surface and subsurface\nlayers.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u4fa7\u5411\u529b\u663e\u5fae\u955c\uff0c\u901a\u8fc7\u7814\u7a76\u6c1f\u63ba\u6742\u548c\u9876\u5c42\u539a\u5ea6\u5bf9[LaMnO3]m/[SrMnO3]n \u53e0\u5c42\u8584\u819c\u7eb3\u7c73\u6469\u64e6\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u6469\u64e6\u529b\u4e0e\u6cd5\u5411\u529b\u548c\u7c98\u9644\u529b\u4e4b\u548c\u5448\u7ebf\u6027\u5173\u7cfb\uff0c\u6469\u64e6\u7cfb\u6570\u662f\u53ef\u8c03\u7684\u6750\u6599\u7279\u6027\u3002", "motivation": "\u672c\u6b21\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a2\u7a76\u6c1f\u63ba\u6742\u548c\u9876\u5c42\u539a\u5ea6\u5bf9[LaMnO3]m/[SrMnO3]n \u53e0\u5c42\u8584\u819c\u7eb3\u7c73\u5c3a\u5ea6\u6469\u64e6\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u4fa7\u5411\u529b\u663e\u5fae\u955c\u7814\u7a76\u4e86\u7eb3\u7c73\u5c3a\u5ea6\u4e0b[LaMnO3]m/[SrMnO3]n \u53e0\u5c42\u8584\u819c\u7684\u6469\u64e6\u73b0\u8c61\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u6c1f\u63ba\u6742\u548c\u9876\u5c42\u539a\u5ea6\u7684\u5f71\u54cd\u3002", "result": "\u6469\u64e6\u529b\u4e0e\u65bd\u52a0\u7684\u6cd5\u5411\u529b\u548c\u7c98\u9644\u529b\u4e4b\u548c\u5448\u7ebf\u6027\u5173\u7cfb\u3002\u5c3d\u7ba1\u7531\u4e8e\u5c40\u90e8\u7c98\u9644\u529b\u6ce2\u52a8\u5bfc\u81f4\u6469\u64e6\u529b\u5728\u7a7a\u95f4\u4e0a\u5b58\u5728\u5dee\u5f02\uff0c\u4f46\u5bf9\u4e8e\u6bcf\u4e2a\u6837\u54c1\u800c\u8a00\uff0c\u6469\u64e6\u7cfb\u6570\u5728\u4e0d\u540c\u4f4d\u7f6e\u4e0a\u662f\u4fdd\u6301\u4e0d\u53d8\u7684\u3002\u7136\u800c\uff0c\u6469\u64e6\u7cfb\u6570\u4f1a\u53d7\u5230\u6c1f\u6d53\u5ea6\u548c\u9876\u5c42\u539a\u5ea6\u7684\u7cfb\u7edf\u6027\u5f71\u54cd\u3002\u7814\u7a76\u6570\u636e\u8868\u660e\uff0c\u6469\u64e6\u80fd\u91cf\u8017\u6563\u5ef6\u4f38\u5230\u8868\u9762\u4ee5\u4e0b 5 \u7eb3\u7c73\uff0c\u5e76\u4e14\u660e\u663e\u4f9d\u8d56\u4e8e\u8868\u9762\u4ee5\u4e0b\u7684\u7ed3\u6784\u3002\u4f5c\u8005\u5c06\u6b64\u5f52\u56e0\u4e8e\u6ed1\u52a8\u5c16\u7aef\u4ea7\u751f\u7684\u5e94\u529b\u573a\u548c\u500f\u901d\u6ce2\u4e2d\u7684\u7c98\u5f39\u6027\u8017\u6563\uff0c\u8fd9\u53ef\u4ee5\u5b9a\u91cf\u5730\u89e3\u91ca\u6240\u89c2\u6d4b\u5230\u7684\u6469\u64e6\u7cfb\u6570\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e00\u65e6\u51c6\u786e\u8003\u8651\u4e86\u7c98\u9644\u529b\uff0c\u6469\u64e6\u7cfb\u6570\u4fbf\u6210\u4e3a\u4e00\u79cd\u53ef\u91cd\u590d\u7684\u6750\u6599\u7279\u6027\uff0c\u53ef\u4ee5\u901a\u8fc7\u5bf9\u8868\u5c42\u548c\u6b21\u8868\u5c42\u8fdb\u884c\u53ef\u63a7\u4fee\u9970\u8fdb\u884c\u8c03\u6574\u3002"}}
{"id": "2507.09072", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09072", "abs": "https://arxiv.org/abs/2507.09072", "authors": ["Zhenghao Zhang", "Qingtian Miao", "G. S. Agarwal"], "title": "Time crystals and nonequilibrium dissipative phase transitions mediated by squeezed bath", "comment": null, "summary": "Nonequilibrium dissipative phase transition, arising from the competition of\ncooperative behavior and coherent field driving, discovered in the 1970s by\nNarducci et al. and Walls et al., has been found to exhibit time-crystal\nbehavior when the driving field exceeds the cooperative decay rate. This was\nseen through the study of the eigenvalues of the Liouvillian superoperator that\ndescribes the joint effect of drive and cooperativity. The cooperative decay\ndepends on the nature of the reservoir correlations. If the reservoir\ncorrelations have phase-sensitive behavior, then the eigenvalues of the\nLiouvillian will be different. We investigate the time-crystal behavior of the\nnonequilibrium dissipative phase transitions under the influence of a squeezed\nvacuum reservoir. We analyze the steady-state phase diagram as a function of\nthe control parameter and demonstrate that increasing the squeezing strength\nsharpens the dissipative phase transition. Spectral analysis of the Liouvillian\nreveals gap closings and the emergence of purely imaginary eigenvalues in the\nthermodynamic limit, indicating the time-crystal phase. We find that the real\nparts of subleading eigenvalues exhibit nonmonotonic behavior with increasing\nsqueezing, reflecting the sensitivity of relaxation dynamics to the reservoir\nproperties. Time-domain simulations confirm that the oscillation frequencies\ncorrespond to the imaginary parts of the Liouvillian eigenvalues. We also\npresent results on quantum fluctuations in the time-crystal phase. Our results\ncall attention to the study of time crystals in models of cooperativity based\non engineered environments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u53d7\u6324\u538b\u771f\u7a7a\u73af\u5883\u5f71\u54cd\u7684\u975e\u5e73\u8861\u8017\u6563\u76f8\u53d8\u4e2d\u5b9e\u73b0\u65f6\u95f4\u6676\u4f53\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5206\u6790\u4e86\u73af\u5883\u5bf9\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u8017\u6563\u76f8\u53d8\u7684\u65f6\u95f4\u6676\u4f53\u884c\u4e3a\uff0c\u7279\u522b\u662f\u5728\u53d7\u6324\u538b\u771f\u7a7a\u73af\u5883\u5f71\u54cd\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u63a2\u8ba8\u73af\u5883\u5173\u8054\u5bf9\u7cfb\u7edf\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u63cf\u8ff0\u9a71\u52a8\u548c\u534f\u540c\u4f5c\u7528\u7684Liouvillian\u7b97\u5b50\u7684\u7279\u5f81\u503c\u6765\u7814\u7a76\u8017\u6563\u76f8\u53d8\u7684\u65f6\u95f4\u6676\u4f53\u884c\u4e3a\u3002\u5206\u6790\u4e86\u7a33\u6001\u76f8\u56fe\uff0c\u5e76\u8fdb\u884c\u4e86\u65f6\u57df\u6a21\u62df\u4ee5\u9a8c\u8bc1\u632f\u8361\u9891\u7387\u4e0e\u7279\u5f81\u503c\u865a\u90e8\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u6324\u538b\u771f\u7a7a\u73af\u5883\u53ef\u4ee5\u9510\u5316\u8017\u6563\u76f8\u53d8\uff0c\u5e76\u5728\u70ed\u529b\u5b66\u6781\u9650\u4e0b\u4ea7\u751f\u7eaf\u865a\u6570\u7279\u5f81\u503c\uff0c\u8868\u660e\u5b58\u5728\u65f6\u95f4\u6676\u4f53\u76f8\u3002\u4e9a\u4e3b\u5bfc\u7279\u5f81\u503c\u7684\u5b9e\u90e8\u968f\u6324\u538b\u5f3a\u5ea6\u7684\u589e\u52a0\u8868\u73b0\u51fa\u975e\u5355\u8c03\u884c\u4e3a\uff0c\u5e76\u4e14\u65f6\u57df\u6a21\u62df\u8bc1\u5b9e\u4e86\u632f\u8361\u9891\u7387\u4e0e\u7279\u5f81\u503c\u865a\u90e8\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5728\u57fa\u4e8e\u5de5\u7a0b\u73af\u5883\u7684\u5408\u4f5c\u6a21\u578b\u4e2d\u7814\u7a76\u65f6\u95f4\u6676\u4f53\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5c55\u793a\u4e86\u6324\u538b\u771f\u7a7a \u091c\u094d\u092f\u093e\u092e\u0941\u0933\u0947\u8017\u6563\u76f8\u53d8\u9510\u5316\uff0c\u4ee5\u53ca\u5728\u8017\u6563\u76f8\u53d8\u4e2d\u5b9e\u73b0\u65f6\u95f4\u6676\u4f53\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.09037", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09037", "abs": "https://arxiv.org/abs/2507.09037", "authors": ["Bharadwaj Ravichandran", "David Joy", "Paul Elliott", "Brian Hu", "Jadie Adams", "Christopher Funk", "Emily Veenhuis", "Anthony Hoogs", "Arslan Basharat"], "title": "ALIGN: Prompt-based Attribute Alignment for Reliable, Responsible, and Personalized LLM-based Decision-Making", "comment": "10 pages total (including appendix), ICML 2025 Workshop on Reliable\n  and Responsible Foundation Models", "summary": "Large language models (LLMs) are increasingly being used as decision aids.\nHowever, users have diverse values and preferences that can affect their\ndecision-making, which requires novel methods for LLM alignment and\npersonalization. Existing LLM comparison tools largely focus on benchmarking\ntasks, such as knowledge-based question answering. In contrast, our proposed\nALIGN system focuses on dynamic personalization of LLM-based decision-makers\nthrough prompt-based alignment to a set of fine-grained attributes. Key\nfeatures of our system include robust configuration management, structured\noutput generation with reasoning, and several algorithm implementations with\nswappable LLM backbones, enabling different types of analyses. Our user\ninterface enables a qualitative, side-by-side comparison of LLMs and their\nalignment to various attributes, with a modular backend for easy algorithm\nintegration. Additionally, we perform a quantitative analysis comparing\nalignment approaches in two different domains: demographic alignment for public\nopinion surveys and value alignment for medical triage decision-making. The\nentire ALIGN framework is open source and will enable new research on reliable,\nresponsible, and personalized LLM-based decision-makers.", "AI": {"tldr": "ALIGN\u662f\u4e00\u4e2a\u5f00\u6e90\u7cfb\u7edf\uff0c\u7528\u4e8e\u6839\u636e\u7528\u6237\u504f\u597d\u4e2a\u6027\u5316\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51b3\u7b56\u52a9\u624b\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u5bf9\u9f50\u7ec6\u7c92\u5ea6\u5c5e\u6027\uff0c\u5e76\u652f\u6301\u4e0d\u540c\u9886\u57df\u548c\u5c5e\u6027\u7c7b\u578b\u7684\u5206\u6790\u3002", "motivation": "\u7528\u6237\u5177\u6709\u53ef\u80fd\u5f71\u54cd\u51b3\u7b56\u7684\u591a\u6837\u5316\u4ef7\u503c\u89c2\u548c\u504f\u597d\uff0c\u8fd9\u9700\u8981\u65b0\u7684LLM\u5bf9\u9f50\u548c\u4e2a\u6027\u5316\u65b9\u6cd5\u3002\u73b0\u6709\u7684LLM\u6bd4\u8f83\u5de5\u5177\u4e3b\u8981\u96c6\u4e2d\u5728\u77e5\u8bc6\u95ee\u7b54\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4efb\u52a1\u4e0a\uff0c\u800cALGN\u7cfb\u7edf\u5219\u4e13\u6ce8\u4e8eLLM\u4f5c\u4e3a\u51b3\u7b56\u52a9\u624b\u7684\u52a8\u6001\u4e2a\u6027\u5316\u3002", "method": "ALIGN\u7cfb\u7edf\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u5bf9\u9f50\u6765\u5b9e\u73b0LLM\u51b3\u7b56\u52a9\u624b\u7684\u52a8\u6001\u4e2a\u6027\u5316\uff0c\u5177\u6709\u7a33\u5065\u7684\u914d\u7f6e\u7ba1\u7406\u3001\u7ed3\u6784\u5316\u8f93\u51fa\u751f\u6210\uff08\u542b\u63a8\u7406\uff09\u4ee5\u53ca\u591a\u79cd\u7b97\u6cd5\u5b9e\u73b0\uff08\u53ef\u5207\u6362LLM\u9aa8\u5e72\uff09\u3002\u5176\u7528\u6237\u754c\u9762\u652f\u6301\u5b9a\u6027\u3001\u5e76\u6392\u7684LLM\u6bd4\u8f83\u53ca\u5176\u4e0e\u5404\u79cd\u5c5e\u6027\u7684\u5bf9\u9f50\uff0c\u540e\u7aef\u6a21\u5757\u5316\u6613\u4e8e\u7b97\u6cd5\u96c6\u6210\u3002", "result": "\u5728\u516c\u5f00\u610f\u89c1\u8c03\u67e5\u7684 \u09ac\u09b2\u09b2\u09c7\u09a8 demographic \u5bf9\u9f50\u548c\u533b\u7597\u5206\u8bca\u51b3\u7b56\u7684\u4ef7\u503c\u5bf9\u9f50\u8fd9\u4e24\u4e2a\u9886\u57df\u4e2d\uff0c\u5bf9\u9f50\u65b9\u6cd5\u8fdb\u884c\u4e86\u5b9a\u91cf\u5206\u6790\uff0c\u8bc1\u660e\u4e86ALIGN\u6846\u67b6\u5728\u5b9e\u73b0\u53ef\u9760\u3001\u8d1f\u8d23\u4efb\u548c\u4e2a\u6027\u5316\u7684LLM\u51b3\u7b56\u52a9\u624b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "ALIGN\u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7\u57fa\u4e8e\u63d0\u793a\u7684\u5bf9\u9f50\u6765\u5b9e\u73b0\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u51b3\u7b56\u52a9\u624b\u7684\u52a8\u6001\u4e2a\u6027\u5316\uff0c\u5e76\u652f\u6301\u8de8\u4e0d\u540c\u9886\u57df\u548c\u7c7b\u578b\u5c5e\u6027\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002"}}
{"id": "2507.09972", "categories": ["cs.GT", "cs.CY", "econ.GN", "q-fin.EC"], "pdf": "https://arxiv.org/pdf/2507.09972", "abs": "https://arxiv.org/abs/2507.09972", "authors": ["Lucas Barbosa", "Sam Kirshner", "Rob Kopel", "Eric Tze Kuan Lim", "Tom Pagram"], "title": "A New Incentive Model For Content Trust", "comment": "20 pages, 6 figures and 2 tables", "summary": "This paper outlines an incentive-driven and decentralized approach to\nverifying the veracity of digital content at scale. Widespread misinformation,\nan explosion in AI-generated content and reduced reliance on traditional news\nsources demands a new approach for content authenticity and truth-seeking that\nis fit for a modern, digital world. By using smart contracts and digital\nidentity to incorporate 'trust' into the reward function for published content,\nnot just engagement, we believe that it could be possible to foster a\nself-propelling paradigm shift to combat misinformation through a\ncommunity-based governance model. The approach described in this paper requires\nthat content creators stake financial collateral on factual claims for an\nimpartial jury to vet with a financial reward for contribution. We hypothesize\nthat with the right financial and social incentive model users will be\nmotivated to participate in crowdsourced fact-checking and content creators\nwill place more care in their attestations. This is an exploratory paper and\nthere are a number of open issues and questions that warrant further analysis\nand exploration.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6fc0\u52b1\u9a71\u52a8\u7684\u53bb\u4e2d\u5fc3\u5316\u65b9\u6cd5\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u9a8c\u8bc1\u6570\u5b57\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002\u901a\u8fc7\u5728\u5956\u52b1\u51fd\u6570\u4e2d\u52a0\u5165\u201c\u4fe1\u4efb\u201d\u56e0\u7d20\uff0c\u5e76\u8981\u6c42\u5185\u5bb9\u521b\u5efa\u8005\u4e3a\u5176\u4e3b\u5f20\u63d0\u4f9b\u8d22\u52a1\u62b5\u62bc\u54c1\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u793e\u533a\u6cbb\u7406\u6a21\u5f0f\u6765\u5bf9\u6297\u9519\u8bef\u4fe1\u606f\u3002", "motivation": "\u5e7f\u6cdb\u7684\u9519\u8bef\u4fe1\u606f\u3001\u4eba\u5de5\u667a\u80fd\u751f\u6210\u5185\u5bb9\u7684\u7206\u70b8\u5f0f\u589e\u957f\u4ee5\u53ca\u5bf9\u4f20\u7edf\u65b0\u95fb\u6765\u6e90\u7684\u4f9d\u8d56\u6027\u964d\u4f4e\uff0c\u8981\u6c42\u4e00\u79cd\u65b0\u7684\u5185\u5bb9\u771f\u5b9e\u6027\u548c\u5bfb\u6c42\u771f\u76f8\u7684\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u73b0\u4ee3\u6570\u5b57\u4e16\u754c\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u667a\u80fd\u5408\u7ea6\u548c\u6570\u5b57\u8eab\u4efd\u5c06\u201c\u4fe1\u4efb\u201d\u7eb3\u5165\u5956\u52b1\u51fd\u6570\uff0c\u8981\u6c42\u5185\u5bb9\u521b\u5efa\u8005\u4e3a\u4e8b\u5b9e\u4e3b\u5f20\u8d28\u62bc\u8d22\u52a1\u62b5\u62bc\u54c1\uff0c\u7531\u516c\u6b63\u7684\u966a\u5ba1\u56e2\u8fdb\u884c\u5ba1\u67e5\u5e76\u83b7\u5f97\u8d22\u52a1\u5956\u52b1\u3002", "result": "\u901a\u8fc7\u7ed3\u5408\u8d22\u52a1\u548c\u793e\u4ea4\u6fc0\u52b1\u6a21\u578b\uff0c\u53ef\u4ee5\u6fc0\u52b1\u7528\u6237\u53c2\u4e0e\u4f17\u5305\u4e8b\u5b9e\u6838\u67e5\uff0c\u5e76\u4fc3\u4f7f\u5185\u5bb9\u521b\u5efa\u8005\u5728\u5176\u58f0\u660e\u4e2d\u66f4\u52a0\u8c28\u614e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u65e8\u5728\u901a\u8fc7\u57fa\u4e8e\u793e\u533a\u7684\u6cbb\u7406\u6a21\u5f0f\uff0c\u5229\u7528\u667a\u80fd\u5408\u7ea6\u548c\u6570\u5b57\u8eab\u4efd\u5c06\u201c\u4fe1\u4efb\u201d\u7eb3\u5165\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u6fc0\u52b1\u7528\u6237\u53c2\u4e0e\u4f17\u5305\u4e8b\u5b9e\u6838\u67e5\uff0c\u4ece\u800c\u5728\u89c4\u6a21\u4e0a\u9a8c\u8bc1\u6570\u5b57\u5185\u5bb9\u7684\u771f\u5b9e\u6027\u3002"}}
{"id": "2507.10466", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2507.10466", "abs": "https://arxiv.org/abs/2507.10466", "authors": ["Kathlee Barsse", "Romain P\u00e9choux", "Simon Perdrix"], "title": "A Quantum Programming Language for Coherent Control", "comment": null, "summary": "We introduce a programming language that allows for the coherent control of\narbitrary quantum operations. The problem of defining coherent control beyond\nthe unitary case, using, for example, a quantum conditional in the presence of\nrecursion or iteration has long been known to be a major difficulty. We resolve\nthis problem by defining an operational semantics based on appropriate Kraus\ndecompositions and a denotational semantics based on vacuum-extensions. We show\nthat the language is universal for vacuum-extensions and that the two semantics\nare adequate. Moreover, we define a notion of observational equivalence: two\nprograms are equivalent if their probability of termination is the same in any\ncontext. The denotational semantics is shown to be fully abstract for\nobservational equivalence.", "AI": {"tldr": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u7f16\u7a0b\u8bed\u8a00\uff0c\u53ef\u4ee5\u5bf9\u4efb\u610f\u91cf\u5b50\u64cd\u4f5c\u8fdb\u884c\u76f8\u5e72\u63a7\u5236\u3002", "motivation": "\u4f8b\u5982\uff0c\u5728\u9012\u5f52\u6216\u8fed\u4ee3\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u91cf\u5b50\u6761\u4ef6\u5728\u9149\u7b97\u4f8b\u4e4b\u5916\u5b9a\u4e49\u76f8\u5e72\u63a7\u5236\u7684\u95ee\u9898\uff0c\u957f\u671f\u4ee5\u6765\u4e00\u76f4\u88ab\u8ba4\u4e3a\u662f\u4e3b\u8981\u56f0\u96be\u3002", "method": "\u6211\u4eec\u901a\u8fc7\u57fa\u4e8e\u9002\u5f53\u7684Kraus\u5206\u89e3\u7684\u64cd\u4f5c\u8bed\u4e49\u548c\u57fa\u4e8e\u771f\u7a7a\u6269\u5c55\u7684\u8868\u793a\u8bed\u4e49\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u8bed\u8a00\u5728\u771f\u7a7a\u6269\u5c55\u65b9\u9762\u5177\u6709\u901a\u7528\u6027\uff0c\u5e76\u4e14\u4e24\u79cd\u8bed\u4e49\u662f\u5145\u5206\u7684\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u79cd\u53ef\u89c2\u5bdf\u7684\u7b49\u4ef7\u6982\u5ff5\uff1a\u5982\u679c\u4e24\u4e2a\u7a0b\u5e8f\u5728\u4efb\u4f55\u4e0a\u4e0b\u6587\u4e2d\u5177\u6709\u76f8\u540c\u7684\u7ec8\u6b62\u6982\u7387\uff0c\u5219\u5b83\u4eec\u662f\u7b49\u4ef7\u7684\u3002\u8be5\u8868\u793a\u8bed\u4e49\u5bf9\u4e8e\u53ef\u89c2\u5bdf\u7684\u7b49\u4ef7\u6027\u88ab\u8bc1\u660e\u662f\u5b8c\u5168\u62bd\u8c61\u7684\u3002", "conclusion": "\u8be5\u8bed\u8a00\u5728\u771f\u7a7a\u6269\u5c55\u65b9\u9762\u5177\u6709\u901a\u7528\u6027\uff0c\u5e76\u4e14\u4e24\u79cd\u8bed\u4e49\u662f\u5145\u5206\u7684\u3002\u6b64\u5916\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u79cd\u53ef\u89c2\u5bdf\u7684\u7b49\u4ef7\u6982\u5ff5\uff1a\u5982\u679c\u4e24\u4e2a\u7a0b\u5e8f\u5728\u4efb\u4f55\u4e0a\u4e0b\u6587\u4e2d\u5177\u6709\u76f8\u540c\u7684\u7ec8\u6b62\u6982\u7387\uff0c\u5219\u5b83\u4eec\u662f\u7b49\u4ef7\u7684\u3002\u8be5\u8868\u793a\u8bed\u4e49\u5bf9\u4e8e\u53ef\u89c2\u5bdf\u7684\u7b49\u4ef7\u6027\u88ab\u8bc1\u660e\u662f\u5b8c\u5168\u62bd\u8c61\u7684\u3002"}}
{"id": "2507.09386", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09386", "abs": "https://arxiv.org/abs/2507.09386", "authors": ["Ruangrawee Kitichotkul", "Shashwath Bharadwaj", "Joshua Rapp", "Yanting Ma", "Alexander Mehta", "Vivek K Goyal"], "title": "Free-running vs. Synchronous: Single-Photon Lidar for High-flux 3D Imaging", "comment": "20 pages, 15 figures, to be presented at the International Conference\n  on Computer Vision (ICCV) 2025", "summary": "Conventional wisdom suggests that single-photon lidar (SPL) should operate in\nlow-light conditions to minimize dead-time effects. Many methods have been\ndeveloped to mitigate these effects in synchronous SPL systems. However,\nsolutions for free-running SPL remain limited despite the advantage of reduced\nhistogram distortion from dead times. To improve the accuracy of free-running\nSPL, we propose a computationally efficient joint maximum likelihood estimator\nof the signal flux, the background flux, and the depth using only histograms,\nalong with a complementary regularization framework that incorporates a learned\npoint cloud score model as a prior. Simulations and experiments demonstrate\nthat free-running SPL yields lower estimation errors than its synchronous\ncounterpart under identical conditions, with our regularization further\nimproving accuracy.", "AI": {"tldr": "\u4e0e\u540c\u6b65\u7cfb\u7edf\u76f8\u6bd4\uff0c\u81ea\u7531\u8fd0\u884c\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u5728\u7cbe\u5ea6\u4e0a\u5177\u6709\u4f18\u52bf\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f4\u65b9\u56fe\u7684\u8054\u5408\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u548c\u6b63\u5219\u5316\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u81ea\u7531\u8fd0\u884c\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u7684\u7cbe\u5ea6\u3002", "motivation": "\u5c3d\u7ba1\u540c\u6b65\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\uff08SPL\uff09\u7cfb\u7edf\u6709\u8bb8\u591a\u51cf\u8f7b\u6b7b\u533a\u65f6\u95f4\u6548\u5e94\u7684\u65b9\u6cd5\uff0c\u4f46\u81ea\u7531\u8fd0\u884cSPL\u7684\u89e3\u51b3\u65b9\u6848\u4ecd\u7136\u6709\u9650\uff0c\u800c\u81ea\u7531\u8fd0\u884cSPL\u5177\u6709\u51cf\u5c11\u6b7b\u533a\u65f6\u95f4\u5f15\u8d77\u7684\u76f4\u65b9\u56fe\u5931\u771f\u7684\u4f18\u70b9\u3002\u4e3a\u4e86\u63d0\u9ad8\u81ea\u7531\u8fd0\u884cSPL\u7684\u51c6\u786e\u6027\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u8054\u5408\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\uff0c\u4ec5\u4f7f\u7528\u76f4\u65b9\u56fe\u6765\u4f30\u8ba1\u4fe1\u53f7\u901a\u91cf\u3001\u80cc\u666f\u901a\u91cf\u548c\u6df1\u5ea6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e92\u8865\u7684\u6b63\u5219\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u5b66\u4e60\u5230\u7684\u70b9\u4e91\u8bc4\u5206\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u3002", "result": "\u4eff\u771f\u548c\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u540c\u6b65\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u76f8\u6bd4\uff0c\u81ea\u7531\u8fd0\u884c\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\u5177\u6709\u66f4\u4f4e\u7684\u4f30\u8ba1\u8bef\u5dee\uff0c\u800c\u63d0\u51fa\u7684\u6b63\u5219\u5316\u6846\u67b6\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u6700\u5927\u4f3c\u7136\u4f30\u8ba1\u5668\u548c\u6b63\u5219\u5316\u6846\u67b6\uff0c\u5229\u7528\u76f4\u65b9\u56fe\u5b9e\u73b0\u4e86\u5bf9\u81ea\u7531\u8fd0\u884c\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u7cfb\u7edf\u4fe1\u53f7\u901a\u91cf\u3001\u80cc\u666f\u901a\u91cf\u548c\u6df1\u5ea6\u7684\u4f30\u8ba1\uff0c\u5e76\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u70b9\u4e91\u8bc4\u5206\u6a21\u578b\u4f5c\u4e3a\u5148\u9a8c\u6765\u63d0\u9ad8\u7cbe\u5ea6\u3002\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u76f8\u540c\u6761\u4ef6\u4e0b\uff0c\u81ea\u7531\u8fd0\u884c\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\u7684\u4f30\u8ba1\u8bef\u5dee\u4f4e\u4e8e\u540c\u6b65\u5355\u5149\u5b50\u6fc0\u5149\u96f7\u8fbe\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u6b63\u5219\u5316\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u3002"}}
{"id": "2507.10249", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10249", "abs": "https://arxiv.org/abs/2507.10249", "authors": ["Kang Li", "Ming Li", "Wenkang Ji", "Zhiyong Sun", "Shiyu Zhao"], "title": "Multi-Robot Cooperative Herding through Backstepping Control Barrier Functions", "comment": null, "summary": "We propose a novel cooperative herding strategy through backstepping control\nbarrier functions (CBFs), which coordinates multiple herders to herd a group of\nevaders safely towards a designated goal region. For the herding system with\nheterogeneous groups involving herders and evaders, the behavior of the evaders\ncan only be influenced indirectly by the herders' motion, especially when the\nevaders follow an inverse dynamics model and respond solely to repulsive\ninteractions from the herders. This indirect interaction mechanism inherently\nrenders the overall system underactuated. To address this issue, we first\nconstruct separate CBFs for the dual objectives of goal reaching and collision\navoidance, which ensure both herding completion and safety guarantees. Then, we\nreformulate the underactuated herding dynamics into a control-affine structure\nand employ a backstepping approach to recursively design control inputs for the\nhierarchical barrier functions, avoiding taking derivatives of the higher-order\nsystem. Finally, we present a cooperative herding strategy based on\nbackstepping CBFs that allow herders to safely herd multiple evaders into the\ngoal region. In addition, centralized and decentralized implementations of the\nproposed algorithm are developed, further enhancing its flexibility and\napplicability. Extensive simulations and real-world experiments validate the\neffectiveness and safety of the proposed strategy in multi-robot herding.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u6b65\u63a7\u5236\u52bf\u5792\u51fd\u6570\uff08CBFs\uff09\u7684\u5408\u4f5c\u56f4\u5835\u7b56\u7565\uff0c\u7528\u4e8e\u5c06\u591a\u4e2a\u9003\u907f\u8005\u5b89\u5168\u5730\u56f4\u5835\u5230\u76ee\u6807\u533a\u57df\u3002\u8be5\u65b9\u6cd5\u89e3\u51b3\u4e86\u7531\u4e8e\u95f4\u63a5\u76f8\u4e92\u4f5c\u7528\u5bfc\u81f4\u7684\u6b20\u9a71\u52a8\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u7531\u4e8e\u9003\u907f\u8005\u4ec5\u5bf9\u56f4\u5835\u8005\u7684\u6392\u65a5\u76f8\u4e92\u4f5c\u7528\u505a\u51fa\u53cd\u5e94\uff0c\u5e76\u4e14\u5176\u884c\u4e3a\u53ea\u80fd\u901a\u8fc7\u56f4\u5835\u8005\u7684\u8fd0\u52a8\u95f4\u63a5\u5f71\u54cd\uff0c\u56e0\u6b64\u6240\u63d0\u51fa\u7684\u56f4\u5835\u7cfb\u7edf\uff08\u5305\u62ec\u56f4\u5835\u8005\u548c\u9003\u907f\u8005\uff09\u5177\u6709\u56fa\u6709\u7684\u6b20\u9a71\u52a8\u7279\u6027\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5173\u952e\u95ee\u9898\uff0c\u6211\u4eec\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5408\u4f5c\u56f4\u5835\u7b56\u7565\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u4e3a\u53cc\u91cd\u76ee\u6807\uff08\u5230\u8fbe\u76ee\u6807\u533a\u57df\u548c\u907f\u514d\u78b0\u649e\uff09\u6784\u5efa\u4e86\u5355\u72ec\u7684CBFs\uff0c\u4ee5\u786e\u4fdd\u56f4\u5835\u7684\u5b8c\u6210\u548c\u5b89\u5168\u6027\u3002\u7136\u540e\uff0c\u5c06\u6b20\u9a71\u52a8\u7684\u56f4\u5835\u52a8\u529b\u5b66\u91cd\u65b0\u6784\u5efa\u4e3a\u63a7\u5236\u4eff\u5c04\u7ed3\u6784\uff0c\u5e76\u91c7\u7528\u53cd\u6b65\u6cd5\u9012\u5f52\u8bbe\u8ba1\u5c42\u6b21\u52bf\u5792\u51fd\u6570\u7684\u63a7\u5236\u8f93\u5165\uff0c\u907f\u514d\u4e86\u5bf9\u9ad8\u9636\u7cfb\u7edf\u6c42\u5bfc\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u80fd\u591f\u5b89\u5168\u5730\u5c06\u591a\u4e2a\u9003\u907f\u8005\u56f4\u5835\u5230\u76ee\u6807\u533a\u57df\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5408\u4f5c\u56f4\u5835\u7b56\u7565\uff0c\u901a\u8fc7\u53cd\u6b65\u63a7\u5236\u52bf\u5792\u51fd\u6570\uff08CBFs\uff09\u534f\u8c03\u591a\u4e2a\u56f4\u5835\u8005\uff0c\u5c06\u4e00\u7fa4\u9003\u907f\u8005\u5b89\u5168\u5730\u56f4\u5835\u5230\u6307\u5b9a\u7684\u533a\u57df\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u6240\u63d0\u51fa\u7b97\u6cd5\u7684\u96c6\u4e2d\u5f0f\u548c\u5206\u5e03\u5f0f\u5b9e\u73b0\uff0c\u4ee5\u63d0\u9ad8\u5176\u7075\u6d3b\u6027\u548c\u9002\u7528\u6027\u3002\u5927\u91cf\u7684\u4eff\u771f\u548c\u5b9e\u9645\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5728\u591a\u673a\u5668\u4eba\u56f4\u5835\u4e2d\u7684\u6709\u6548\u6027\u548c\u5b89\u5168\u6027\u3002"}}
{"id": "2507.08839", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.08839", "abs": "https://arxiv.org/abs/2507.08839", "authors": ["Xiaowei Yu", "Jing Zhang", "Tong Chen", "Yan Zhuang", "Minheng Chen", "Chao Cao", "Yanjun Lyu", "Lu Zhang", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Domain-Adaptive Diagnosis of Lewy Body Disease with Transferability Aware Transformer", "comment": "MICCAI 2025", "summary": "Lewy Body Disease (LBD) is a common yet understudied form of dementia that\nimposes a significant burden on public health. It shares clinical similarities\nwith Alzheimer's disease (AD), as both progress through stages of normal\ncognition, mild cognitive impairment, and dementia. A major obstacle in LBD\ndiagnosis is data scarcity, which limits the effectiveness of deep learning. In\ncontrast, AD datasets are more abundant, offering potential for knowledge\ntransfer. However, LBD and AD data are typically collected from different sites\nusing different machines and protocols, resulting in a distinct domain shift.\nTo effectively leverage AD data while mitigating domain shift, we propose a\nTransferability Aware Transformer (TAT) that adapts knowledge from AD to\nenhance LBD diagnosis. Our method utilizes structural connectivity (SC) derived\nfrom structural MRI as training data. Built on the attention mechanism, TAT\nadaptively assigns greater weights to disease-transferable features while\nsuppressing domain-specific ones, thereby reducing domain shift and improving\ndiagnostic accuracy with limited LBD data. The experimental results demonstrate\nthe effectiveness of TAT. To the best of our knowledge, this is the first study\nto explore domain adaptation from AD to LBD under conditions of data scarcity\nand domain shift, providing a promising framework for domain-adaptive diagnosis\nof rare diseases.", "AI": {"tldr": "TAT\u6a21\u578b\u901a\u8fc7\u9002\u5e94\u6027\u5730\u4eceAD\u6570\u636e\u4e2d\u8f6c\u79fb\u77e5\u8bc6\u5e76\u51cf\u5c11\u57df\u504f\u79fb\uff0c\u6709\u6548\u89e3\u51b3\u4e86LBD\u6570\u636e\u7a00\u7f3a\u548c\u57df\u504f\u79fb\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86LBD\u7684\u8bca\u65ad\u51c6\u786e\u6027\u3002", "motivation": "\u8def\u6613\u4f53\u75f4\u5446\u75c7\uff08LBD\uff09\u662f\u4e00\u79cd\u5e38\u89c1\u4f46\u7814\u7a76\u4e0d\u8db3\u7684\u75f4\u5446\u75c7\uff0c\u7ed9\u516c\u5171\u536b\u751f\u5e26\u6765\u5de8\u5927\u8d1f\u62c5\u3002\u7531\u4e8eLBD\u6570\u636e\u7a00\u7f3a\uff0c\u9650\u5236\u4e86\u6df1\u5ea6\u5b66\u4e60\u7684\u6709\u6548\u6027\u3002\u800c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u7684\u6570\u636e\u76f8\u5bf9\u4e30\u5bcc\uff0c\u53ef\u4ee5\u8fdb\u884c\u77e5\u8bc6\u8f6c\u79fb\u3002\u7136\u800c\uff0cLBD\u548cAD\u6570\u636e\u5b58\u5728\u57df\u504f\u79fb\u95ee\u9898\uff0c\u8fd9\u7ed9\u77e5\u8bc6\u8f6c\u79fb\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u8f6c\u79fb\u6027\u611f\u77e5Transformer\uff08TAT\uff09\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4ece\u7ed3\u6784MRI\u884d\u751f\u7684\u7ed3\u6784\u8fde\u63a5\uff08SC\uff09\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u81ea\u9002\u5e94\u5730\u5206\u914d\u66f4\u5927\u7684\u6743\u91cd\u7ed9\u75be\u75c5\u53ef\u8f6c\u79fb\u7279\u5f81\uff0c\u540c\u65f6\u6291\u5236\u7279\u5b9a\u9886\u57df\u7684\u7279\u5f81\uff0c\u4ee5\u51cf\u5c11\u57df\u504f\u79fb\u5e76\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86TAT\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u63d0\u9ad8LBD\u8bca\u65ad\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u9996\u6b21\u63a2\u7d22\u4e86\u5728\u6570\u636e\u7a00\u7f3a\u548c\u57df\u504f\u79fb\u6761\u4ef6\u4e0b\uff0c\u4ece\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08AD\uff09\u5230\u8def\u6613\u4f53\u75f4\u5446\u75c7\uff08LBD\uff09\u7684\u57df\u9002\u5e94\uff0c\u4e3a\u7f55\u89c1\u75c5\u7684\u57df\u9002\u5e94\u8bca\u65ad\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u6846\u67b6\u3002"}}
{"id": "2507.09604", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09604", "abs": "https://arxiv.org/abs/2507.09604", "authors": ["Carlos Magno O. Pereira", "Edilberto O. Silva"], "title": "Quantum Hall-like effect for neutral particles with magnetic dipole moments in a quantum dot", "comment": "8 pages, 7 figures, 2 tables", "summary": "We predict a new class of quantum Hall phenomena in completely neutral\nsystems, demonstrating that the interplay between radial electric fields and\ndipole moments induces exact $e^2/h$ quantization without the need for Landau\nlevels or external magnetic fields. Contrary to conventional wisdom, our theory\nreveals that: (i) the singularity of line charges does not destroy topological\nprotection, (ii) spin-control of quantization emerges from boundary conditions\nalone, and (iii) the effect persists up to 25 K, surpassing typical neutral\nsystems. These findings establish electric field engineering as a viable route\nto topological matter beyond magnetic paradigms.", "AI": {"tldr": "\u5728\u4e2d\u6027\u7cfb\u7edf\u4e2d\uff0c\u7535\u573a\u548c\u5076\u6781\u77e9\u7684\u76f8\u4e92\u4f5c\u7528\u53ef\u5b9e\u73b0\u91cf\u5b50\u970d\u5c14\u6548\u5e94\uff0c\u65e0\u9700\u78c1\u573a\uff0c\u5e76\u5177\u6709\u62d3\u6251\u4fdd\u62a4\u3001\u81ea\u65cb\u63a7\u5236\u548c\u9ad8\u6e29\u7279\u6027\u3002", "motivation": "\u6311\u6218\u4f20\u7edf\u89c2\u5ff5\uff0c\u63a2\u7d22\u5728\u65e0\u9700\u6717\u9053\u80fd\u7ea7\u6216\u5916\u90e8\u78c1\u573a\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e2d\u6027\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u91cf\u5b50\u970d\u5c14\u6548\u5e94\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u63ed\u793a\u7535\u573a\u5de5\u7a0b\u5728\u62d3\u6251\u7269\u8d28\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u5f84\u5411\u7535\u573a\u548c\u5076\u6781\u77e9\u5728\u4e2d\u6027\u7cfb\u7edf\u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\u6765\u9884\u6d4b\u91cf\u5b50\u970d\u5c14\u73b0\u8c61\u3002", "result": "\u6210\u529f\u9884\u6d4b\u4e86\u5728\u4e2d\u6027\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u91cf\u5b50\u970d\u5c14\u6548\u5e94\uff0c\u5e76\u53d1\u73b0\u4e86\u62d3\u6251\u4fdd\u62a4\u3001\u81ea\u65cb\u63a7\u5236\u548c\u9ad8\u6e29\u6548\u5e94\u7b49\u65b0\u7279\u6027\uff0c\u8bc1\u660e\u4e86\u7535\u573a\u5de5\u7a0b\u5728\u62d3\u6251\u7269\u8d28\u9886\u57df\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9884\u6d4b\u4e86\u4e00\u7c7b\u5168\u65b0\u7684\u91cf\u5b50\u970d\u5c14\u73b0\u8c61\uff0c\u5728\u5b8c\u5168\u4e2d\u6027\u7684\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u5f84\u5411\u7535\u573a\u548c\u5076\u6781\u77e9\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u65e0\u9700\u6717\u9053\u80fd\u7ea7\u6216\u5916\u90e8\u78c1\u573a\u5373\u53ef\u5b9e\u73b0\u7cbe\u786e\u7684 e\u00b2/h \u91cf\u5316\u3002\u7814\u7a76\u8868\u660e\uff1a(i) \u7ebf\u7535\u8377\u5947\u70b9\u4e0d\u5f71\u54cd\u62d3\u6251\u4fdd\u62a4\uff0c(ii) \u91cf\u5316\u4ec5\u7531\u8fb9\u754c\u6761\u4ef6\u4ea7\u751f\u81ea\u65cb\u63a7\u5236\uff0c(iii) \u8be5\u6548\u5e94\u53ef\u7ef4\u6301\u9ad8\u8fbe 25 K\uff0c\u8d85\u8d8a\u4e86\u5178\u578b\u4e2d\u6027\u7cfb\u7edf\u3002\u8fd9\u4e9b\u53d1\u73b0\u8bc1\u660e\u7535\u573a\u5de5\u7a0b\u662f\u8d85\u8d8a\u78c1\u6027\u8303\u5f0f\u7684\u62d3\u6251\u7269\u8d28\u7684\u53ef\u884c\u9014\u5f84\u3002"}}
{"id": "2507.10125", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2507.10125", "abs": "https://arxiv.org/abs/2507.10125", "authors": ["Zeev Nutov"], "title": "Improved bicriteria approximation for $k$-edge-connectivity", "comment": null, "summary": "In the $k$-Edge Connected Spanning Subgraph ($k$-ECSS) problem we are given a\n(multi-)graph $G=(V,E)$ with edge costs and an integer $k$, and seek a min-cost\n$k$-edge-connected spanning subgraph of $G$. The problem admits a\n$2$-approximation algorithm and no better approximation ratio is\nknown.Hershkowitz, Klein, and Zenklusen [STOC 24] gave a bicriteria\n$(1,k-10)$-approximation algorithm that computes a $(k-10)$-edge-connected\nspanning subgraph of cost at most the optimal value of a standard Cut-LP for\n$k$-ECSS. This LP bicriteria approximation was recently improved by Cohen and\nNutov [ESA 25] to $(1,k-4)$, where also was given a bicriteria approximation\n$(3/2,k-2)$. In this paper we improve the bicriteria approximation to $(1,k-2)$\nfor $k$ even and to $\\left(1-\\frac{1}{k},k-3\\right)$ for $k$ is odd, and also\ngive another bicriteria approximation $(3/2,k-1)$.\n  The $k$-Edge-Connected Spanning Multi-subgraph ($k$-ECSM) problem is almost\nthe same as $k$-ECSS, except that any edge can be selected multiple times at\nthe same cost. The previous best approximation ratio for $k$-ECSM was $1+4/k$.\nOur result improves this to $1+\\frac{2}{k}$ for $k$ even and to $1+\\frac{3}{k}$\nfor $k$ odd, where for $k$ odd the computed subgraph is in fact\n$(k+1)$-edge-connected.", "AI": {"tldr": "\u672c\u6587\u6539\u8fdb\u4e86 k-ECSS \u548c k-ECSM \u95ee\u9898\u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5728\u8fd1\u4f3c\u6bd4\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u5bfb\u627e k-\u8fb9\u8fde\u901a\u751f\u6210\u5b50\u56fe\uff08k-ECSS\uff09\u548c k-\u8fb9\u8fde\u901a\u751f\u6210\u591a\u5b50\u56fe\uff08k-ECSM\uff09\u95ee\u9898\u7684\u6700\u4f18\u89e3\uff0c\u8fd9\u4e9b\u95ee\u9898\u5728\u7f51\u7edc\u8bbe\u8ba1\u548c\u4f18\u5316\u4e2d\u5177\u6709\u91cd\u8981\u5e94\u7528\u3002\u73b0\u6709\u7684\u7b97\u6cd5\u5728\u8fd1\u4f3c\u6bd4\u65b9\u9762\u8fd8\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u7279\u522b\u662f\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\u65b9\u9762\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u6539\u8fdb\u7684\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\u6765\u89e3\u51b3 k-ECSS \u548c k-ECSM \u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5bf9\u4e8e k-ECSS \u95ee\u9898\uff0c\u7b97\u6cd5\u5728 k \u4e3a\u5076\u6570\u65f6\u5b9e\u73b0\u4e86 (1, k-2) \u7684\u8fd1\u4f3c\u6bd4\uff0c\u5728 k \u4e3a\u5947\u6570\u65f6\u5b9e\u73b0\u4e86 (1-1/k, k-3) \u7684\u8fd1\u4f3c\u6bd4\uff0c\u5e76\u4e14\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a (3/2, k-1) \u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002\u5bf9\u4e8e k-ECSM \u95ee\u9898\uff0c\u7b97\u6cd5\u5c06\u8fd1\u4f3c\u6bd4\u4ece\u4e4b\u524d\u7684 1+4/k \u63d0\u9ad8\u5230 1+2/k (k \u4e3a\u5076\u6570) \u548c 1+3/k (k \u4e3a\u5947\u6570)\u3002", "result": "\u5bf9\u4e8e k-ECSS \u95ee\u9898\uff0c\u672c\u6587\u5c06\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\u6539\u8fdb\u4e3a k \u4e3a\u5076\u6570\u65f6\u7684 (1, k-2) \u548c k \u4e3a\u5947\u6570\u65f6\u7684 (1-1/k, k-3)\uff0c\u5e76\u63d0\u51fa\u4e86\u53e6\u4e00\u4e2a\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5 (3/2, k-1)\u3002\u5bf9\u4e8e k-ECSM \u95ee\u9898\uff0c\u672c\u6587\u5c06\u8fd1\u4f3c\u6bd4\u4ece\u4e4b\u524d\u7684 1+4/k \u63d0\u9ad8\u5230 1+2/k (k \u4e3a\u5076\u6570) \u548c 1+3/k (k \u4e3a\u5947\u6570)\u3002", "conclusion": "\u672c\u6587\u9488\u5bf9 k-ECSM \u95ee\u9898\uff0c\u5c06 k \u4e3a\u5076\u6570\u65f6\u7684\u8fd1\u4f3c\u6bd4\u4ece 1+4/k \u63d0\u9ad8\u5230 1+2/k\uff0c\u5c06 k \u4e3a\u5947\u6570\u65f6\u7684\u8fd1\u4f3c\u6bd4\u4ece 1+4/k \u63d0\u9ad8\u5230 1+3/k\uff0c\u5e76\u4e14\u5bf9\u4e8e k \u4e3a\u5947\u6570\u7684\u60c5\u51b5\uff0c\u8ba1\u7b97\u51fa\u7684\u5b50\u56fe\u5b9e\u9645\u4e0a\u662f (k+1)-\u8fb9\u8fde\u901a\u7684\u3002\u9488\u5bf9 k-ECSS \u95ee\u9898\uff0c\u672c\u6587\u5c06\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\u6539\u8fdb\u4e3a k \u4e3a\u5076\u6570\u65f6\u7684 (1, k-2) \u548c k \u4e3a\u5947\u6570\u65f6\u7684 (1-1/k, k-3)\uff0c\u5e76\u63d0\u51fa\u4e86\u53e6\u4e00\u4e2a\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5 (3/2, k-1)\u3002"}}
{"id": "2507.09374", "categories": ["cs.AI", "I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09374", "abs": "https://arxiv.org/abs/2507.09374", "authors": ["Chenglin Zhu", "Tao Zhang", "Chong Li", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique", "comment": "14 pages,4 figures", "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released.", "AI": {"tldr": "EduFlow \u662f\u4e00\u4e2a\u7528\u4e8e\u6559\u80b2\u79d1\u5b66\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7 EduPRM \u548c EduMCTS \u89e3\u51b3\u4e86 MLLMs \u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u4e0d\u8db3\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u548c\u53ef\u89e3\u91ca\u6027\u7684\u79d1\u5b66\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5b58\u5728\u79d1\u5b66\u63a8\u7406\u6a21\u5f0f\u4e0d\u8db3\u3001\u591a\u6b65\u63a8\u7406\u5168\u5c40\u8fde\u8d2f\u6027\u5dee\u4ee5\u53ca\u7f3a\u4e4f\u53cd\u601d\u6027\u81ea\u6211\u7ea0\u6b63\u7b49\u95ee\u9898\uff0c\u5728\u7ed3\u6784\u5316\u79d1\u5b66\u73af\u5883\u4e2d\u4e0d\u53ef\u9760\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a EduFlow \u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u5305\u62ec\u6570\u636e\u9009\u62e9\u3001\u57fa\u4e8e MCTS \u7684\u8f68\u8ff9\u6784\u5efa\u3001\u6a21\u578b\u8bad\u7ec3\u548c\u8f93\u51fa\u4f18\u5316\u3002\u6838\u5fc3\u662f EduPRM\uff0c\u4e00\u4e2a\u8fc7\u7a0b\u611f\u77e5\u5956\u52b1\u6a21\u578b\uff0c\u901a\u8fc7\u6807\u7b7e\u548c\u7406\u7531\u6765\u8bc4\u4f30\u63a8\u7406\u6b65\u9aa4\u3002EduMCTS \u662f\u4e00\u4e2a\u9886\u57df\u81ea\u9002\u5e94\u641c\u7d22\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u5f15\u5bfc\u6027\u7684\u81ea\u4e3e\u52a8\u4f5c\uff0c\u4f8b\u5982\u4fc3\u8fdb\u53cd\u601d\u6027\u9519\u8bef\u7ea0\u6b63\u7684\u81ea\u6211\u53cd\u601d\u673a\u5236\u3002\u901a\u8fc7\u81ea\u6d3d\u6027\u548c\u62d2\u7edd\u91c7\u6837\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6559\u80b2\u63a8\u7406\u8f68\u8ff9\u6570\u636e\u96c6 EduMCTS-160K\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cEduFlow \u63d0\u9ad8\u4e86\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002", "conclusion": "EduFlow \u6846\u67b6\u901a\u8fc7 EduPRM \u548c EduMCTS \u6539\u8fdb\u4e86 MLLMs \u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002"}}
{"id": "2507.09948", "categories": ["cs.LG", "cs.AR"], "pdf": "https://arxiv.org/pdf/2507.09948", "abs": "https://arxiv.org/abs/2507.09948", "authors": ["Zijian Ding", "Tung Nguyen", "Weikai Li", "Aditya Grover", "Yizhou Sun", "Jason Cong"], "title": "Iceberg: Enhancing HLS Modeling with Synthetic Data", "comment": "9 pages. accepted to ICLAD'25", "summary": "Deep learning-based prediction models for High-Level Synthesis (HLS) of\nhardware designs often struggle to generalize. In this paper, we study how to\nclose the generalizability gap of these models through pretraining on synthetic\ndata and introduce Iceberg, a synthetic data augmentation approach that expands\nboth large language model (LLM)-generated programs and weak labels of unseen\ndesign configurations. Our weak label generation method is integrated with an\nin-context model architecture, enabling meta-learning from actual and proximate\nlabels. Iceberg improves the geometric mean modeling accuracy by $86.4\\%$ when\nadapt to six real-world applications with few-shot examples and achieves a\n$2.47\\times$ and a $1.12\\times$ better offline DSE performance when adapting to\ntwo different test datasets. Our open-sourced code is here:\n\\href{https://github.com/UCLA-VAST/iceberg}{https://github.com/UCLA-VAST/iceberg}", "AI": {"tldr": "Iceberg\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86HLS\u9884\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u8868\u73b0\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60HLS\u9884\u6d4b\u6a21\u578b\u5728\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "Iceberg\u662f\u4e00\u79cd\u5408\u6210\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6269\u5c55\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u7684\u7a0b\u5e8f\u548c\u9488\u5bf9\u672a\u89c1\u8fc7\u7684\u8bbe\u8ba1\u914d\u7f6e\u7684\u5f31\u6807\u7b7e\u6765\u5de5\u4f5c\u3002\u5176\u5f31\u6807\u7b7e\u751f\u6210\u65b9\u6cd5\u4e0e\u4e0a\u4e0b\u6587\u6a21\u578b\u67b6\u6784\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u4ece\u5b9e\u9645\u548c\u90bb\u8fd1\u6807\u7b7e\u8fdb\u884c\u5143\u5b66\u4e60\u3002", "result": "Iceberg\u5728\u9002\u5e94\u516d\u4e2a\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u65f6\uff0c\u5c06\u51e0\u4f55\u5e73\u5747\u6a21\u578b\u51c6\u786e\u7387\u63d0\u9ad8\u4e8686.4%\uff1b\u5728\u9002\u5e94\u4e24\u4e2a\u4e0d\u540c\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u65f6\uff0c\u79bb\u7ebfDSE\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e862.47\u500d\u548c1.12\u500d\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u9884\u8bad\u7ec3\u548c\u5408\u6210\u6570\u636e\u589e\u5f3a\u65b9\u6cd5Iceberg\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6df1\u5ea6\u5b66\u4e60HLS\u9884\u6d4b\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u6548\u7f29\u5c0f\u4e86\u6cdb\u5316\u5dee\u8ddd\u3002"}}
{"id": "2507.09726", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09726", "abs": "https://arxiv.org/abs/2507.09726", "authors": ["Boyou Chen", "Kaihan Zhang", "Austin Moore", "Bochen Jia", "Mengqiu Cao"], "title": "Electric Vehicle Public Charging Equity Considerations: A Systematic Review", "comment": null, "summary": "Public electric vehicle (EV) charging infrastructure is crucial for\naccelerating EV adoption and reducing transportation emissions; however,\ndisparities in infrastructure access have raised significant equity concerns.\nThis systematic review synthesizes existing knowledge and identifies gaps\nregarding equity in EV public charging research. Following structured review\nprotocols, 91 peer-reviewed studies from Scopus and Google Scholar were\nanalyzed, focusing explicitly on equity considerations. The findings indicate\nthat current research on EV public charging equity mainly adopted geographic\ninformation systems (GIS), network optimization, behavioral modeling, and\nhybrid analytical frameworks, yet lacks consistent normative frameworks for\nassessing equity outcomes. Equity assessments highlight four key dimensions:\nspatial accessibility, cost burdens, reliability and usability, and user\nawareness and trust. Socio-economic disparities, particularly income, housing\ntenure, and ethnicity, frequently exacerbate inequitable access,\ndisproportionately disadvantaging low-income, renter, and minority populations.\nAdditionally, infrastructure-specific choices, including charger reliability,\nstrategic location, and pricing strategies, significantly influence adoption\npatterns and equity outcomes. However, existing literature primarily reflects\nNorth American, European, and Chinese contexts, revealing substantial\ngeographical and methodological limitations. This review suggests the need for\nmore robust normative evaluations of equity, comprehensive demographic data\nintegration, and advanced methodological frameworks, thereby guiding targeted,\ninclusive, and context-sensitive infrastructure planning and policy\ninterventions.", "AI": {"tldr": "\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u516c\u5e73\u6027\u7814\u7a76\u5b58\u5728\u5730\u57df\u548c\u65b9\u6cd5\u5c40\u9650\uff0c\u9700\u52a0\u5f3a\u89c4\u8303\u3001\u6570\u636e\u548c\u65b9\u6cd5\u8bba\u4ee5\u5b9e\u73b0\u5305\u5bb9\u6027\u89c4\u5212\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u57fa\u7840\u8bbe\u65bd\u5728\u666e\u53ca\u8fc7\u7a0b\u4e2d\u5b58\u5728\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u7efc\u5408\u73b0\u6709\u77e5\u8bc6\u5e76\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5bf9Scopus\u548cGoogle Scholar\u768491\u7bc7\u540c\u884c\u8bc4\u5ba1\u7814\u7a76\u8fdb\u884c\u7cfb\u7edf\u6027\u56de\u987e\uff0c\u5206\u6790\u4e86\u7535\u52a8\u6c7d\u8f66\u516c\u5171\u5145\u7535\u7814\u7a76\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "result": "\u76ee\u524d\u7814\u7a76\u4e3b\u8981\u91c7\u7528\u5730\u7406\u4fe1\u606f\u7cfb\u7edf\u3001\u7f51\u7edc\u4f18\u5316\u3001\u884c\u4e3a\u5efa\u6a21\u548c\u6df7\u5408\u5206\u6790\u6846\u67b6\uff0c\u4f46\u5728\u516c\u5e73\u6027\u8bc4\u4f30\u65b9\u9762\u7f3a\u4e4f\u7edf\u4e00\u7684\u89c4\u8303\u6846\u67b6\u3002\u516c\u5e73\u6027\u8bc4\u4f30\u6db5\u76d6\u7a7a\u95f4\u53ef\u8fbe\u6027\u3001\u6210\u672c\u8d1f\u62c5\u3001\u53ef\u9760\u6027\u4e0e\u53ef\u7528\u6027\u3001\u7528\u6237\u8ba4\u77e5\u4e0e\u4fe1\u4efb\u56db\u4e2a\u7ef4\u5ea6\u3002\u793e\u4f1a\u7ecf\u6d4e\u56e0\u7d20\uff08\u5982\u6536\u5165\u3001\u4f4f\u623f\u7c7b\u578b\u3001\u79cd\u65cf\uff09\u548c\u57fa\u7840\u8bbe\u65bd\u9009\u62e9\uff08\u5982\u5145\u7535\u6869\u53ef\u9760\u6027\u3001\u4f4d\u7f6e\u3001\u5b9a\u4ef7\uff09\u5bf9\u516c\u5e73\u6027\u6709\u663e\u8457\u5f71\u54cd\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5317\u7f8e\u3001\u6b27\u6d32\u548c\u4e2d\u56fd\uff0c\u5b58\u5728\u5730\u57df\u548c\u65b9\u6cd5\u4e0a\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u7814\u7a76\u663e\u793a\uff0c\u7535\u52a8\u6c7d\u8f66\u5145\u7535\u516c\u5e73\u6027\u8bc4\u4f30\u9700\u8981\u66f4\u5065\u5168\u7684\u89c4\u8303\u3001\u66f4\u5168\u9762\u7684\u6570\u636e\u6574\u5408\u4ee5\u53ca\u66f4\u5148\u8fdb\u7684\u65b9\u6cd5\u8bba\uff0c\u4ee5\u6307\u5bfc\u66f4\u5177\u9488\u5bf9\u6027\u3001\u5305\u5bb9\u6027\u548c\u60c5\u5883\u9002\u5e94\u6027\u7684\u57fa\u7840\u8bbe\u65bd\u89c4\u5212\u4e0e\u653f\u7b56\u5e72\u9884\u3002"}}
{"id": "2507.09309", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09309", "abs": "https://arxiv.org/abs/2507.09309", "authors": ["Peng Xie", "Johannes Betz", "Amr Alanwar"], "title": "Informed Hybrid Zonotope-based Motion Planning Algorithm", "comment": null, "summary": "Optimal path planning in nonconvex free spaces is notoriously challenging, as\nformulating such problems as mixed-integer linear programs (MILPs) is NP-hard.\nWe propose HZ-MP, an informed Hybrid Zonotope-based Motion Planner, as an\nalternative approach that decomposes the obstacle-free space and performs\nlow-dimensional face sampling guided by an ellipsotope heuristic, enabling\nfocused exploration along promising transit regions. This structured\nexploration eliminates the excessive, unreachable sampling that degrades\nexisting informed planners such as AIT* and EIT* in narrow gaps or boxed-goal\nscenarios. We prove that HZ-MP is probabilistically complete and asymptotically\noptimal. It converges to near-optimal trajectories in finite time and scales to\nhigh-dimensional cluttered scenes.", "AI": {"tldr": "HZ-MP \u662f\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u7ef4\u5ea6\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u5206\u89e3\u7a7a\u95f4\u548c\u4f7f\u7528\u542f\u53d1\u5f0f\u65b9\u6cd5\u8fdb\u884c\u91c7\u6837\uff0c\u89e3\u51b3\u4e86\u5728\u975e\u51f8\u7a7a\u95f4\u4e2d\u8def\u5f84\u89c4\u5212\u7684\u96be\u9898\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\u548c\u9ad8\u6548\u6027\u3002", "motivation": "\u5728\u975e\u51f8\u81ea\u7531\u7a7a\u95f4\u4e2d\u8fdb\u884c\u6700\u4f18\u8def\u5f84\u89c4\u5212\u6781\u5177\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5c06\u5176\u8868\u8ff0\u4e3a\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212\uff08MILP\uff09\u662fNP\u96be\u7684\u3002", "method": "HZ-MP \u89c4\u5212\u5668\u901a\u8fc7\u5206\u89e3\u65e0\u969c\u788d\u7a7a\u95f4\u5e76\u7ed3\u5408\u692d\u7403\u4f53\u542f\u53d1\u5f0f\u8fdb\u884c\u4f4e\u7ef4\u9762\u91c7\u6837\u6765\u5b9e\u73b0\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u89c4\u5212\u5668\uff08\u5982 AIT* \u548c EIT*\uff09\u76f8\u6bd4\uff0cHZ-MP \u53ef\u4ee5\u5728\u72ed\u7a84\u7f1d\u9699\u6216boxed-goal\u573a\u666f\u4e2d\u907f\u514d\u65e0\u6548\u91c7\u6837\uff0c\u4ece\u800c\u63d0\u9ad8\u89c4\u5212\u6548\u7387\u3002", "conclusion": "HZ-MP \u89c4\u5212\u5668\u88ab\u8bc1\u660e\u5728\u6982\u7387\u4e0a\u662f\u5b8c\u6574\u7684\u5e76\u4e14\u6e10\u8fd1\u6700\u4f18\u7684\uff0c\u53ef\u4ee5\u5728\u6709\u9650\u65f6\u95f4\u5185\u6536\u655b\u5230\u8fd1\u4f3c\u6700\u4f18\u8f68\u8ff9\uff0c\u5e76\u4e14\u80fd\u591f\u6269\u5c55\u5230\u9ad8\u7ef4\u62e5\u6324\u573a\u666f\u3002"}}
{"id": "2507.09071", "categories": ["cs.CV", "I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09071", "abs": "https://arxiv.org/abs/2507.09071", "authors": ["Tharun Adithya Srikrishnan", "Deval Shah", "Steven K. Reinhardt"], "title": "BlindSight: Harnessing Sparsity for Efficient VLMs", "comment": null, "summary": "Large vision-language models (VLMs) enable the joint processing of text and\nimages. However, the inclusion of vision data significantly expands the prompt\nlength. Along with the quadratic complexity of the attention computation, this\nresults in a longer prefill duration. An approach to mitigate this bottleneck\nis to leverage the inherent sparsity in the attention computation. In our\nanalysis of attention patterns in VLMs, we observe that a substantial portion\nof layers exhibit minimal cross-image attention, except through attention-sink\ntokens per image. These sparse attention patterns fall into distinct\ncategories: sink-only, document mask and a hybrid document-sink mask. Based on\nthis, we propose BlindSight: a training-free approach to optimize VLM inference\nusing a input template-aware attention sparsity mask. We utilize samples from a\ndataset to derive a prompt-agnostic sparsity categorization for every attention\nhead. We evaluate the proposed technique using VLMs such as Qwen2-VL,\nQwen2.5-VL and Gemma-3. BlindSight results in a 32%-41% reduction in FLOPs on\naverage with -2%-+2% accuracy compared to the original model in most evaluated\nmulti-image understanding benchmarks.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5904\u7406\u957f\u56fe\u50cf\u63d0\u793a\u65f6\u7684\u9884\u586b\u5145\u65f6\u95f4\u8fc7\u957f\u95ee\u9898\uff0cBlindSight\u63d0\u51fa\u4e86\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u7a00\u758f\u6027\u6ce8\u610f\u529b\u65b9\u6cd5\uff0c\u901a\u8fc7\u8f93\u5165\u6a21\u677f\u611f\u77e5\u7a00\u758f\u6027\u63a9\u7801\uff0c\u5728\u4e0d\u5f71\u54cd\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u5e73\u5747\u5c06\u8ba1\u7b97\u91cf\u51cf\u5c11\u4e8632%-41%\u3002", "motivation": "\u7531\u4e8e\u89c6\u89c9\u6570\u636e\u548c\u6ce8\u610f\u529b\u8ba1\u7b97\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\uff0c\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u5728\u5904\u7406\u957f\u63d0\u793a\uff08prompt\uff09\u65f6\u5b58\u5728\u9884\u586b\u5145\u65f6\u95f4\uff08prefill duration\uff09\u8fc7\u957f\u7684\u95ee\u9898\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u7f13\u89e3\u8fd9\u4e00\u74f6\u9888\u3002", "method": "BlindSight\u901a\u8fc7\u5206\u6790\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6ce8\u610f\u529b\u6a21\u5f0f\uff0c\u8bc6\u522b\u51fa\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u7684\u7a00\u758f\u6027\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u4ec5\u6c89\u6ca1\u3001\u6587\u6863\u63a9\u7801\u548c\u6df7\u5408\u6587\u6863-\u6c89\u6ca1\u63a9\u7801\u4e09\u7c7b\u3002\u57fa\u4e8e\u8fd9\u4e9b\u89c2\u5bdf\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8f93\u5165\u6a21\u677f\u611f\u77e5\uff08input template-aware\uff09\u7684\u6ce8\u610f\u529b\u7a00\u758f\u6027\u63a9\u7801\uff0c\u7528\u4e8e\u4f18\u5316VLM\u63a8\u7406\uff0c\u4e14\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u3002", "result": "BlindSight\u5728Qwen2-VL\u3001Qwen2.5-VL\u548cGemma-3\u7b49\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e73\u5747\u5c06\u8ba1\u7b97\u91cf\uff08FLOPs\uff09\u51cf\u5c11\u4e8632%-41%\uff0c\u540c\u65f6\u5728\u5927\u591a\u6570\u591a\u56fe\u50cf\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u51c6\u786e\u6027\u4ec5\u4e0b\u964d-2%\u5230+2%\u3002", "conclusion": "BlindSight\u901a\u8fc7\u8f93\u5165\u6a21\u677f\u611f\u77e5\u7a00\u758f\u6027\u63a9\u7801\uff0c\u5728\u4e0d\u8fdb\u884c\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u7684\u63a8\u7406\u8fc7\u7a0b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u548c\u5229\u7528\u6ce8\u610f\u529b\u8ba1\u7b97\u4e2d\u7684\u7a00\u758f\u6027\uff0c\u7279\u522b\u662f\u5229\u7528\u6ce8\u610f\u529b\u6c89\u6ca1\uff08sink\uff09\u6807\u8bb0\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u8ba1\u7b97\u91cf\uff0c\u540c\u65f6\u5bf9\u6a21\u578b\u51c6\u786e\u6027\u5f71\u54cd\u5f88\u5c0f\u3002"}}
{"id": "2507.10392", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10392", "abs": "https://arxiv.org/abs/2507.10392", "authors": ["Runsheng Benson Guo", "Utkarsh Anand", "Khuzaima Daudjee", "Rathijit Sen"], "title": "Zorse: Optimizing LLM Training Efficiency on Heterogeneous GPU Clusters", "comment": null, "summary": "Large language models (LLMs) require vast amounts of GPU compute to train,\nbut limited availability and high costs of GPUs make homogeneous clusters\nimpractical for many organizations. Instead, assembling heterogeneous clusters\nby pooling together GPUs of different generations allows them to achieve higher\naggregate compute and make use of all available GPUs. However, training on\nheterogeneous clusters presents several challenges, including load balancing\nacross GPUs, optimizing memory usage to accommodate varying memory capacities,\nand ensuring communication-efficient training over diverse network\ninterconnects potentially spanning multiple datacenters. In this paper, we make\nthe case that efficient training on heterogeneous clusters requires (1) the\nintegration of pipeline parallelism and data parallelism in a manner that is\nboth communication- and memory-efficient, and (2) a more adaptable\nconfiguration of pipeline and data parallelism, which includes the capability\nto flexibly partition GPUs into asymmetric pipeline parallel stages and to\nincorporate heterogeneous GPUs within the same data parallelism group. We\npropose Zorse, the first system to unify all these capabilities while\nincorporating a planner that automatically configures training strategies for a\ngiven workload. Our evaluation shows that Zorse significantly outperforms\nstate-of-the-art systems in heterogeneous training scenarios.", "AI": {"tldr": "Zorse \u662f\u7b2c\u4e00\u4e2a\u7edf\u4e00\u4e86\u5f02\u6784\u96c6\u7fa4\u8bad\u7ec3\u4e2d\u6d41\u6c34\u7ebf\u5e76\u884c\u3001\u6570\u636e\u5e76\u884c\u3001\u4e0d\u5bf9\u79f0\u5206\u533a\u548c\u8de8 GPU \u7c7b\u578b\u6570\u636e\u5e76\u884c\u7684\u7cfb\u7edf\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u81ea\u52a8\u89c4\u5212\u5668\uff0c\u80fd\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u663e\u8457\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u8bb8\u591a\u7ec4\u7ec7\u5728\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u9762\u4e34\u7684 GPU \u53ef\u7528\u6027\u548c\u6210\u672c\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u5229\u7528\u4e0d\u540c\u4ee3 GPU \u7684\u5f02\u6784\u96c6\u7fa4\u3002\u7136\u800c\uff0c\u5728\u5f02\u6784\u96c6\u7fa4\u4e0a\u8fdb\u884c\u8bad\u7ec3\u5b58\u5728\u8d1f\u8f7d\u5747\u8861\u3001\u5185\u5b58\u4f18\u5316\u548c\u901a\u4fe1\u6548\u7387\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Zorse \u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u6570\u636e\u5e76\u884c\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u81ea\u52a8\u89c4\u5212\u5668\uff0c\u53ef\u4ee5\u4e3a\u7ed9\u5b9a\u5de5\u4f5c\u8d1f\u8f7d\u914d\u7f6e\u8bad\u7ec3\u7b56\u7565\u3002\u8be5\u7cfb\u7edf\u5141\u8bb8\u7075\u6d3b\u5730\u5c06 GPU \u5206\u533a\u4e3a\u4e0d\u5bf9\u79f0\u6d41\u6c34\u7ebf\u5e76\u884c\u9636\u6bb5\uff0c\u5e76\u5c06\u5f02\u6784 GPU \u5305\u542b\u5728\u540c\u4e00\u6570\u636e\u5e76\u884c\u7ec4\u4e2d\u3002", "result": "Zorse \u5728\u5f02\u6784\u8bad\u7ec3\u573a\u666f\u4e2d\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u7cfb\u7edf\u3002", "conclusion": "Zorse \u901a\u8fc7\u96c6\u6210\u6d41\u6c34\u7ebf\u5e76\u884c\u548c\u6570\u636e\u5e76\u884c\uff0c\u5e76\u5141\u8bb8\u7075\u6d3b\u914d\u7f6e\u5e76\u884c\u7b56\u7565\uff08\u5305\u62ec\u4e0d\u5bf9\u79f0\u6d41\u6c34\u7ebf\u548c\u8de8 GPU \u7c7b\u578b\u7684\u6570\u636e\u5e76\u884c\uff09\uff0c\u89e3\u51b3\u4e86\u5f02\u6784\u96c6\u7fa4\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u5e76\u5728\u8bc4\u4f30\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7cfb\u7edf\u3002"}}
{"id": "2507.09172", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09172", "abs": "https://arxiv.org/abs/2507.09172", "authors": ["Cong-Gang Song", "Qing-yu Cai"], "title": "The temporal resolution limit in quantum sensing", "comment": "20 pages, 2 figures", "summary": "Temporal resolution is a critical figure of merit in quantum sensing. This\nstudy combines the distinguishable condition of quantum states with quantum\nspeed limits to establish a lower bound on interrogation time. When the\ninterrogation time falls below this bound, the output state becomes\nstatistically indistinguishable from the input state, and the information will\ninevitably be lost in noise. Without loss of generality, we extend these\nconclusions to time-dependent signal Hamiltonian. In theory, leveraging certain\nquantum control techniques allows us to calculate the minimum interrogation\ntime for arbitrary signal Hamiltonian. Finally, we illustrate the impact of\nquantum speed limits on magnetic field measurements and temporal resolution.", "AI": {"tldr": "\u91cf\u5b50\u4f20\u611f\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u53ef\u901a\u8fc7\u7ed3\u5408\u91cf\u5b50\u6001\u53ef\u533a\u5206\u6027\u548c\u91cf\u5b50\u901f\u5ea6\u6781\u9650\u6765\u4f18\u5316\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u8ba1\u7b97\u6700\u5c0f\u63a2\u6d4b\u65f6\u95f4\u7684\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u4f20\u611f\u7684\u65f6\u95f4\u5206\u8fa8\u7387\u662f\u4e00\u4e2a\u5173\u952e\u6307\u6807\u3002", "method": "\u672c\u7814\u7a76\u5c06\u53ef\u533a\u5206\u7684\u91cf\u5b50\u6001\u6761\u4ef6\u4e0e\u91cf\u5b50\u901f\u5ea6\u6781\u9650\u76f8\u7ed3\u5408\uff0c\u4e3a\u63a2\u6d4b\u65f6\u95f4\u8bbe\u5b9a\u4e86\u4e0b\u9650\u3002", "result": "\u672c\u7814\u7a76\u63a8\u5bfc\u4e86\u63a2\u6d4b\u65f6\u95f4\u7684\u4e0b\u9650\uff0c\u5e76\u8bf4\u660e\u4e86\u91cf\u5b50\u901f\u5ea6\u6781\u9650\u5bf9\u78c1\u573a\u6d4b\u91cf\u548c\u65f6\u95f4\u5206\u8fa8\u7387\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u53ef\u533a\u5206\u7684\u91cf\u5b50\u6001\u6761\u4ef6\u4e0e\u91cf\u5b50\u901f\u5ea6\u6781\u9650\u76f8\u7ed3\u5408\uff0c\u4e3a\u63a2\u6d4b\u65f6\u95f4\u8bbe\u5b9a\u4e86\u4e0b\u9650\u3002\u5f53\u63a2\u6d4b\u65f6\u95f4\u4f4e\u4e8e\u6b64\u8fb9\u754c\u65f6\uff0c\u8f93\u51fa\u6001\u4e0e\u8f93\u5165\u6001\u5c06\u65e0\u6cd5\u533a\u5206\uff0c\u4fe1\u606f\u4f1a\u4e22\u5931\u5728\u566a\u58f0\u4e2d\u3002\u672c\u7814\u7a76\u5c06\u6b64\u7ed3\u8bba\u63a8\u5e7f\u5230\u542b\u65f6\u4fe1\u53f7\u54c8\u5bc6\u987f\u91cf\uff0c\u5e76\u63d0\u51fa\u7406\u8bba\u4e0a\u5229\u7528\u91cf\u5b50\u63a7\u5236\u6280\u672f\u8ba1\u7b97\u4efb\u610f\u4fe1\u53f7\u54c8\u5bc6\u987f\u91cf\u7684\u6700\u5c0f\u63a2\u6d4b\u65f6\u95f4\u3002"}}
{"id": "2507.09075", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09075", "abs": "https://arxiv.org/abs/2507.09075", "authors": ["Wasi Uddin Ahmad", "Somshubra Majumdar", "Aleksander Ficek", "Sean Narenthiran", "Mehrzad Samadi", "Jocelyn Huang", "Siddhartha Jain", "Vahid Noroozi", "Boris Ginsburg"], "title": "OpenCodeReasoning-II: A Simple Test Time Scaling Approach via Self-Critique", "comment": "work in progress", "summary": "Recent advancements in reasoning-based Large Language Models (LLMs),\nparticularly their potential through test-time scaling, have created\nsignificant opportunities for distillation in code generation and critique.\nHowever, progress in both areas fundamentally depends on large-scale,\nhigh-quality datasets. In this work, we introduce OpenCodeReasoning-II, a\ndataset consists of 2.5M question-solution-critique triples (approx. 35K unique\nprogramming questions), making it nearly twice the size of the previous largest\npublicly available code reasoning dataset. In this work, we employ a two-stage\nsupervised fine-tuning strategy. The first stage focuses on fine-tuning for\ncode generation, while the second stage involves the joint training of models\nfor both code generation and critique. Our resulting finetuned Qwen2.5-Instruct\nmodels achieve performance in code generation that either exceeds or equals the\nbest prior open-weight distilled models. Notably, the integration of our code\ngeneration and critique models leads to significant improvements in competitive\ncoding performance. Furthermore, we present an extension of the LiveCodeBench\nbenchmark to specifically support the C++ programming language, thereby\nfacilitating more comprehensive LLM evaluation using this benchmark.", "AI": {"tldr": "OpenCodeReasoning-II\u6570\u636e\u96c6\u7684\u63d0\u51fa\u53ca\u4e24\u9636\u6bb5\u5fae\u8c03\u7b56\u7565\u5728\u5927\u8bed\u8a00\u6a21\u578b\u4ee3\u7801\u751f\u6210\u4e0e\u6279\u8bc4\u65b9\u9762\u53d6\u5f97\u8fdb\u5c55\uff0c\u5e76\u6269\u5c55\u4e86C++\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u4ee3\u7801\u751f\u6210\u548c\u4ee3\u7801\u6279\u8bc4\u9886\u57df\u5bf9\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u9700\u6c42\uff0c\u4ee5\u53ca\u63a2\u7d22\u6d4b\u8bd5\u65f6\u6269\u5c55\u63a8\u7406\u578b\u5927\u8bed\u8a00\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u91c7\u7528\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\u7b56\u7565\uff1a\u7b2c\u4e00\u9636\u6bb5\u4e13\u6ce8\u4e8e\u4ee3\u7801\u751f\u6210\uff0c\u7b2c\u4e8c\u9636\u6bb5\u8054\u5408\u8bad\u7ec3\u4ee3\u7801\u751f\u6210\u548c\u4ee3\u7801\u6279\u8bc4\u6a21\u578b\u3002", "result": "\u5fae\u8c03\u540e\u7684Qwen2.5-Instruct\u6a21\u578b\u5728\u4ee3\u7801\u751f\u6210\u65b9\u9762\u6027\u80fd\u4f18\u4e8e\u6216\u7b49\u4e8e\u5148\u524d\u6700\u4f73\u7684\u5f00\u6e90\u6a21\u578b\uff1b\u4ee3\u7801\u751f\u6210\u548c\u4ee3\u7801\u6279\u8bc4\u6a21\u578b\u7684\u7ed3\u5408\u663e\u8457\u63d0\u9ad8\u4e86\u7ade\u6280\u7f16\u7a0b\u6027\u80fd\uff1b\u6269\u5c55\u7684LiveCodeBench\u57fa\u51c6\u652f\u6301C++\u8bed\u8a00\uff0c\u6709\u52a9\u4e8e\u66f4\u5168\u9762\u7684\u5927\u8bed\u8a00\u6a21\u578b\u8bc4\u4f30\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86OpenCodeReasoning-II\u6570\u636e\u96c6\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u76d1\u7763\u5fae\u8c03\u7b56\u7565\uff0c\u5728\u4ee3\u7801\u751f\u6210\u548c\u4ee3\u7801\u6279\u8bc4\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u540c\u65f6\u6269\u5c55\u4e86LiveCodeBench\u57fa\u51c6\u4ee5\u652f\u6301C++\u8bed\u8a00\u3002"}}
{"id": "2507.10149", "categories": ["cs.GT", "cs.CE", "q-fin.TR"], "pdf": "https://arxiv.org/pdf/2507.10149", "abs": "https://arxiv.org/abs/2507.10149", "authors": ["Abhimanyu Nag", "Madhur Prabhakar", "Tanuj Behl"], "title": "A Coincidence of Wants Mechanism for Swap Trade Execution in Decentralized Exchanges", "comment": null, "summary": "We propose a mathematically rigorous framework for identifying and completing\nCoincidence of Wants (CoW) cycles in decentralized exchange (DEX) aggregators.\nUnlike existing auction based systems such as CoWSwap, our approach introduces\nan asset matrix formulation that not only verifies feasibility using oracle\nprices and formal conservation laws but also completes partial CoW cycles of\nswap orders that are discovered using graph traversal and are settled using\nimbalance correction. We define bridging orders and show that the resulting\nexecution is slippage free and capital preserving for LPs. Applied to real\nworld Arbitrum swap data, our algorithm demonstrates efficient discovery of CoW\ncycles and supports the insertion of synthetic orders for atomic cycle closure.\nThis work can be thought of as the detailing of a potential delta-neutral\nstrategy by liquidity providing market makers: a structured CoW cycle\nexecution.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u6765\u8bc6\u522b\u548c\u5b8c\u6210\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u6240\u4e2d\u7684\u4ea4\u6613\u610f\u613f\u5468\u671f\uff0c\u901a\u8fc7\u8d44\u4ea7\u77e9\u9635\u548c\u56fe\u904d\u5386\u7b49\u65b9\u6cd5\u5b9e\u73b0\u4e86\u65e0\u6ed1\u70b9\u548c\u8d44\u672c\u4fdd\u5168\u7684\u4ea4\u6613\u3002", "motivation": "\u65e8\u5728\u4e3a\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u6240\uff08DEX\uff09\u805a\u5408\u5668\u4e2d\u7684\u4ea4\u6613\u610f\u613f\uff08CoW\uff09\u5468\u671f\u63d0\u4f9b\u4e00\u79cd\u6bd4\u73b0\u6709\u62cd\u5356\u7cfb\u7edf\u66f4\u6570\u5b66\u4e25\u8c28\u3001\u66f4\u9ad8\u6548\u7684\u8bc6\u522b\u548c\u5b8c\u6210\u6846\u67b6\uff0c\u5e76\u4e3a\u6d41\u52a8\u6027\u63d0\u4f9b\u8005\u5c55\u793a\u4e86\u4e00\u79cd\u6f5c\u5728\u7684delta-\u4e2d\u6027\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8d44\u4ea7\u77e9\u9635\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u9884\u8a00\u673a\u4ef7\u683c\u3001\u5f62\u5f0f\u5b88\u6052\u5b9a\u5f8b\u3001\u56fe\u904d\u5386\u548c\u4e0d\u5e73\u8861\u4fee\u6b63\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u5b8c\u6210\u4ea4\u6613\u610f\u613f\uff08CoW\uff09\u5468\u671f\uff0c\u5e76\u5b9a\u4e49\u4e86\u6865\u63a5\u8ba2\u5355\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u53d1\u73b0 CoW \u5468\u671f\uff0c\u5e76\u901a\u8fc7\u63d2\u5165\u5408\u6210\u8ba2\u5355\u5b9e\u73b0\u539f\u5b50\u5468\u671f\u95ed\u5408\uff0c\u5b9e\u73b0\u4e86\u65e0\u6ed1\u70b9\u548c\u8d44\u672c\u4fdd\u5168\u7684\u6267\u884c\uff0c\u5e76\u5728\u5b9e\u9645\u7684 Arbitrum \u4ea4\u6613\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6570\u5b66\u4e0a\u4e25\u8c28\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u5b8c\u6210\u53bb\u4e2d\u5fc3\u5316\u4ea4\u6613\u6240\uff08DEX\uff09\u805a\u5408\u5668\u4e2d\u7684\u4ea4\u6613\u610f\u613f\uff08CoW\uff09\u5468\u671f\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8d44\u4ea7\u77e9\u9635\u3001\u9884\u8a00\u673a\u4ef7\u683c\u3001\u5f62\u5f0f\u5b88\u6052\u5b9a\u5f8b\u3001\u56fe\u904d\u5386\u548c\u4e0d\u5e73\u8861\u4fee\u6b63\u6765\u5b9e\u73b0\uff0c\u5b9e\u73b0\u4e86\u65e0\u6ed1\u70b9\u548c\u8d44\u672c\u4fdd\u5168\u7684\u6267\u884c\u3002\u5e94\u7528\u4e8e\u5b9e\u9645\u7684 Arbitrum \u4ea4\u6613\u6570\u636e\uff0c\u8be5\u7b97\u6cd5\u80fd\u6709\u6548\u53d1\u73b0 CoW \u5468\u671f\uff0c\u5e76\u901a\u8fc7\u63d2\u5165\u5408\u6210\u8ba2\u5355\u5b9e\u73b0\u539f\u5b50\u5468\u671f\u95ed\u5408\uff0c\u4e3a\u6d41\u52a8\u6027\u63d0\u4f9b\u8005\u8be6\u7ec6\u8bf4\u660e\u4e86\u4e00\u79cd\u6f5c\u5728\u7684delta-\u4e2d\u6027\u7b56\u7565\u3002"}}
{"id": "2507.09751", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "AI": {"tldr": "LLM \u5728\u903b\u8f91\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u4f46\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06 LLM \u96c6\u6210\u5230\u975e\u4e00\u81f4\u6027\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u4e49\u4e2d\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u63d0\u4f9b\u4e86\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u4fdd\u7559\u4e86\u903b\u8f91\u7684\u53ef\u9760\u6027\u548c\u5b8c\u5907\u6027\u3002", "motivation": "\u5982\u4f55\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5e7f\u6cdb\u7684\u53c2\u6570\u5316\u77e5\u8bc6\u8fdb\u884c\u5f62\u5f0f\u63a8\u7406\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5728\u751f\u6210\u8f93\u51fa\u65f6\u5b58\u5728\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f4\u63a5\u96c6\u6210\u5230\u975e\u4e00\u81f4\u6027\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u4e49\u7684\u89e3\u91ca\u51fd\u6570\u4e2d\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u6765\u81ea\u51e0\u4e2a\u7b80\u77ed\u4e8b\u5b9e\u68c0\u9a8c\u57fa\u51c6\u7684\u6570\u636e\u96c6\u6765\u8bc4\u4f30\u8be5\u51fd\u6570\uff0c\u63d0\u4f9b\u4e86\u8be5\u65b9\u6cd5\u53ef\u884c\u6027\u7684\u5b9e\u9a8c\u8bc1\u636e\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5e95\u5c42\u903b\u8f91\u7684\u53ef\u9760\u6027\u548c\u5b8c\u5907\u6027\u5c5e\u6027\u3002"}}
{"id": "2507.09408", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09408", "abs": "https://arxiv.org/abs/2507.09408", "authors": ["Sajedeh Norouzi", "Mostafa Rahmani", "Yi Chu", "Torsten Braun", "Kaushik Chowdhury", "Alister Burr"], "title": "Lightweight Graph Neural Networks for Enhanced 5G NR Channel Estimation", "comment": "Accepted in IEEE PIMRC 2025", "summary": "Effective channel estimation CE is critical for optimizing the performance of\n5G New Radio NR systems particularly in dynamic environments where traditional\nmethods struggle with complexity and adaptability This paper introduces\nGraphNet a novel lightweight Graph Neural Network GNNbased estimator designed\nto enhance CE in 5G NR Our proposed method utilizes a GNN architecture that\nminimizes computational overhead while capturing essential features necessary\nfor accurate CE We evaluate GraphNet across various channel conditions from\nslowvarying to highly dynamic environments and compare its performance to\nChannelNet a wellknown deep learningbased CE method GraphNet not only matches\nChannelNets performance in stable conditions but significantly outperforms it\nin highvariation scenarios particularly in terms of Block Error Rate It also\nincludes builtin noise estimation that enhances robustness in challenging\nchannel conditions Furthermore its significantly lighter computational\nfootprint makes GraphNet highly suitable for realtime deployment especially on\nedge devices with limited computational resources By underscoring the potential\nof GNNs to transform CE processes GraphNet offers a scalable and robust\nsolution that aligns with the evolving demands of 5G technologies highlighting\nits efficiency and performance as a nextgeneration solution for wireless\ncommunication systems", "AI": {"tldr": "GraphNet\u662f\u4e00\u79cd\u521b\u65b0\u7684\u8f7b\u91cf\u7ea7\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad85G NR\u7cfb\u7edf\u7684\u4fe1\u9053\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u52a8\u6001\u73af\u5883\u4e2d\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5177\u6709\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u5185\u7f6e\u566a\u58f0\u4f30\u8ba1\u7684\u4f18\u52bf\u3002", "motivation": "\u4f20\u7edf\u7684\u4fe1\u9053\u4f30\u8ba1\u7b97\u6cd5\u57285G NR\u7cfb\u7edf\u7684\u52a8\u6001\u73af\u5883\u4e2d\u9762\u4e34\u590d\u6742\u6027\u548c\u9002\u5e94\u6027\u65b9\u9762\u7684\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4f18\u5316\u6027\u80fd\u3001\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u5e76\u63d0\u9ad8\u9002\u5e94\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGraphNet\u7684\u65b0\u578b\u8f7b\u91cf\u7ea7\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u4f30\u8ba1\u5668\uff0c\u7528\u4e8e\u589e\u5f3a5G NR\u7cfb\u7edf\u7684\u4fe1\u9053\u4f30\u8ba1\uff08CE\uff09\u3002\u8be5\u65b9\u6cd5\u5229\u7528GNN\u67b6\u6784\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u8ba1\u7b97\u5f00\u9500\uff0c\u540c\u65f6\u6355\u83b7\u4e86\u51c6\u786eCE\u6240\u5fc5\u9700\u7684\u5173\u952e\u7279\u5f81\uff0c\u5e76\u96c6\u6210\u4e86\u566a\u58f0\u4f30\u8ba1\u529f\u80fd\u4ee5\u63d0\u9ad8\u5728\u6311\u6218\u6027\u4fe1\u9053\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "GraphNet\u5728\u7a33\u5b9a\u548c\u9ad8\u53d8\u5f02\u4fe1\u9053\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5176\u6027\u80fd\u53ef\u4e0eChannelNet\u76f8\u5ab2\u7f8e\uff0c\u5728\u9ad8\u53d8\u5f02\u573a\u666f\u4e0b\u751a\u81f3\u663e\u8457\u4f18\u4e8eChannelNet\uff08\u5c24\u5176\u662f\u5728\u8bef\u5757\u7387\u65b9\u9762\uff09\u3002\u6b64\u5916\uff0cGraphNet\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d1f\u8377\uff0c\u5e76\u5177\u6709\u5185\u7f6e\u7684\u566a\u58f0\u4f30\u8ba1\u529f\u80fd\uff0c\u4f7f\u5176\u6210\u4e3a\u8fb9\u7f18\u8bbe\u5907\u7684\u7406\u60f3\u9009\u62e9\u3002", "conclusion": "GraphNet\u901a\u8fc7\u5176\u65b0\u9896\u7684\u8f7b\u91cf\u7ea7\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u67b6\u6784\uff0c\u57285G NR\u7cfb\u7edf\u7684\u4fe1\u9053\u4f30\u8ba1\uff08CE\uff09\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u52a8\u6001\u73af\u5883\u4e0b\u7684\u8bef\u5757\u7387\uff08BLER\uff09\u65b9\u9762\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684ChannelNet\u65b9\u6cd5\u3002\u5176\u5185\u7f6e\u7684\u566a\u58f0\u4f30\u8ba1\u548c\u663e\u8457\u964d\u4f4e\u7684\u8ba1\u7b97\u5f00\u9500\uff0c\u4f7f\u5176\u975e\u5e38\u9002\u5408\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u8fdb\u884c\u5b9e\u65f6\u90e8\u7f72\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10251", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10251", "abs": "https://arxiv.org/abs/2507.10251", "authors": ["Wenjing Zhang", "Wei Zhang"], "title": "ToMacVF : Temporal Macro-action Value Factorization for Asynchronous Multi-Agent Reinforcement Learning", "comment": null, "summary": "Existing asynchronous MARL methods based on MacDec-POMDP typically construct\ntraining trajectory buffers by simply sampling limited and biased data at the\nendpoints of macro-actions, and directly apply conventional MARL methods on the\nbuffers. As a result, these methods lead to an incomplete and inaccurate\nrepresentation of the macro-action execution process, along with unsuitable\ncredit assignments. To solve these problems, the Temporal Macro-action Value\nFactorization (ToMacVF) is proposed to achieve fine-grained temporal credit\nassignment for macro-action contributions. A centralized training buffer,\ncalled Macro-action Segmented Joint Experience Replay Trajectory (Mac-SJERT),\nis designed to incorporate with ToMacVF to collect accurate and complete\nmacro-action execution information, supporting a more comprehensive and precise\nrepresentation of the macro-action process. To ensure principled and\nfine-grained asynchronous value factorization, the consistency requirement\nbetween joint and individual macro-action selection called Temporal\nMacro-action based IGM (To-Mac-IGM) is formalized, proving that it generalizes\nthe synchronous cases. Based on To-Mac-IGM, a modularized ToMacVF architecture,\nwhich satisfies CTDE principle, is designed to conveniently integrate previous\nvalue factorization methods. Next, the ToMacVF algorithm is devised as an\nimplementation of the ToMacVF architecture. Experimental results demonstrate\nthat, compared to asynchronous baselines, our ToMacVF algorithm not only\nachieves optimal performance but also exhibits strong adaptability and\nrobustness across various asynchronous multi-agent experimental scenarios.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToMacVF\u7684\u7b97\u6cd5\u548cMac-SJERT\u7f13\u51b2\u533a\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5b8f\u52a8\u4f5c\u6267\u884c\u8fc7\u7a0b\u8868\u793a\u548c\u4fe1\u7528\u5206\u914d\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eMacDec-POMDP\u7684\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u6784\u5efa\u8bad\u7ec3\u8f68\u8ff9\u7f13\u51b2\u533a\u65f6\u5b58\u5728\u6570\u636e\u6709\u9650\u548c\u6709\u504f\u89c1\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u5bf9\u5b8f\u52a8\u4f5c\u6267\u884c\u8fc7\u7a0b\u7684\u8868\u793a\u4e0d\u5b8c\u6574\u3001\u4e0d\u51c6\u786e\uff0c\u5e76\u4e14\u4fe1\u7528\u5206\u914d\u4e0d\u5f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aToMacVF\uff08Temporal Macro-action Value Factorization\uff09\u7684\u7b97\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3aMac-SJERT\uff08Macro-action Segmented Joint Experience Replay Trajectory\uff09\u7684\u4e2d\u5fc3\u5316\u8bad\u7ec3\u7f13\u51b2\u533a\u3002\u901a\u8fc7\u5f62\u5f0f\u5316To-Mac-IGM\uff08Temporal Macro-action based IGM\uff09\u6765\u786e\u4fdd\u8054\u5408\u548c\u4e2a\u4f53\u5b8f\u52a8\u4f5c\u9009\u62e9\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u539f\u5219\u6027\u548c\u7ec6\u7c92\u5ea6\u7684\u5f02\u6b65\u503c\u5206\u89e3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u5f02\u6b65\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cToMacVF\u7b97\u6cd5\u5b9e\u73b0\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u4e86\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "ToMacVF\u7b97\u6cd5\u5728\u5404\u79cd\u5f02\u6b65\u591a\u667a\u80fd\u4f53\u5b9e\u9a8c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u6700\u4f18\u6027\u80fd\uff0c\u5e76\u5177\u6709\u5f88\u5f3a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.08841", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08841", "abs": "https://arxiv.org/abs/2507.08841", "authors": ["Kun Jing", "Luoyu Chen", "Jungang Xu", "Jianwei Tai", "Yiyu Wang", "Shuaimin Li"], "title": "Zero-Shot Neural Architecture Search with Weighted Response Correlation", "comment": null, "summary": "Neural architecture search (NAS) is a promising approach for automatically\ndesigning neural network architectures. However, the architecture estimation of\nNAS is computationally expensive and time-consuming because of training\nmultiple architectures from scratch. Although existing zero-shot NAS methods\nuse training-free proxies to accelerate the architecture estimation, their\neffectiveness, stability, and generality are still lacking. We present a novel\ntraining-free estimation proxy called weighted response correlation (WRCor).\nWRCor utilizes correlation coefficient matrices of responses across different\ninput samples to calculate the proxy scores of estimated architectures, which\ncan measure their expressivity and generalizability. Experimental results on\nproxy evaluation demonstrate that WRCor and its voting proxies are more\nefficient estimation strategies than existing proxies. We also apply them with\ndifferent search strategies in architecture search. Experimental results on\narchitecture search show that our zero-shot NAS algorithm outperforms most\nexisting NAS algorithms in different search spaces. Our NAS algorithm can\ndiscover an architecture with a 22.1% test error on the ImageNet-1k dataset\nwithin 4 GPU hours. All codes are publicly available at\nhttps://github.com/kunjing96/ZSNAS-WRCor.git.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWRCor\u7684\u65b0\u7684\u65e0\u8bad\u7ec3\u4f30\u8ba1\u4ee3\u7406\uff0c\u7528\u4e8e\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u548c\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002WRCor\u901a\u8fc7\u5206\u6790\u8de8\u4e0d\u540c\u8f93\u5165\u7684\u54cd\u5e94\u76f8\u5173\u6027\u6765\u8bc4\u4f30\u67b6\u6784\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cWRCor\u53ca\u5176\u6295\u7968\u4ee3\u7406\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5e76\u4e14\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672cNAS\u65b9\u6cd5\u4f7f\u7528\u65e0\u8bad\u7ec3\u4ee3\u7406\u6765\u52a0\u901f\u67b6\u6784\u4f30\u8ba1\uff0c\u4f46\u5176\u6709\u6548\u6027\u3001\u7a33\u5b9a\u6027\u548c\u901a\u7528\u6027\u4ecd\u7136\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u8bad\u7ec3\u4f30\u8ba1\u4ee3\u7406\uff0c\u79f0\u4e3a\u52a0\u6743\u54cd\u5e94\u76f8\u5173\uff08WRCor\uff09\uff0c\u5b83\u5229\u7528\u8de8\u4e0d\u540c\u8f93\u5165\u6837\u672c\u7684\u54cd\u5e94\u7684\u76f8\u5173\u7cfb\u6570\u77e9\u9635\u6765\u8ba1\u7b97\u4f30\u8ba1\u67b6\u6784\u7684\u4ee3\u7406\u5206\u6570\uff0c\u4ee5\u8861\u91cf\u5176\u8868\u73b0\u529b\u548c\u901a\u7528\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cWRCor\u53ca\u5176\u6295\u7968\u4ee3\u7406\u6bd4\u73b0\u6709\u7684\u4ee3\u7406\u66f4\u6709\u6548\u7684\u4f30\u8ba1\u7b56\u7565\u3002\u901a\u8fc7\u5c06\u5b83\u4eec\u4e0e\u4e0d\u540c\u7684\u641c\u7d22\u7b56\u7565\u7ed3\u5408\u4f7f\u7528\uff0c\u6240\u63d0\u51fa\u7684\u96f6\u6837\u672cNAS\u7b97\u6cd5\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8622.1%\u7684\u6d4b\u8bd5\u8bef\u5dee\uff0c\u5e76\u4e14\u57284\u4e2aGPU\u5c0f\u65f6\u5185\u5c31\u5b8c\u6210\u4e86\u67b6\u6784\u641c\u7d22\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u96f6\u6837\u672c\u795e\u7ecf\u67b6\u6784\u641c\u7d22\uff08NAS\uff09\u7b97\u6cd5\u5728\u4e0d\u540c\u7684\u641c\u7d22\u7a7a\u95f4\u4e2d\u4f18\u4e8e\u5927\u591a\u6570\u73b0\u6709\u7684NAS\u7b97\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u5728ImageNet-1k\u6570\u636e\u96c6\u4e0a\u4ee522.1%\u7684\u6d4b\u8bd5\u8bef\u5dee\u57284\u4e2aGPU\u5c0f\u65f6\u5185\u53d1\u73b0\u4e00\u4e2a\u67b6\u6784\u3002"}}
{"id": "2507.10248", "categories": ["cs.DS", "cs.DM", "68R05 (Primary) 68W25, 90C26 (Secondary)", "F.2.2; G.2.1"], "pdf": "https://arxiv.org/pdf/2507.10248", "abs": "https://arxiv.org/abs/2507.10248", "authors": ["Moran Feldman", "Alan Kuhnle"], "title": "Bicriteria Submodular Maximization", "comment": "75 pages, 1 figure", "summary": "Submodular functions and their optimization have found applications in\ndiverse settings ranging from machine learning and data mining to game theory\nand economics. In this work, we consider the constrained maximization of a\nsubmodular function, for which we conduct a principled study of bicriteria\napproximation algorithms -- algorithms which can violate the constraint, but\nonly up to a bounded factor. Bicrteria optimization allows constrained\nsubmodular maximization to capture additional important settings, such as the\nwell-studied submodular cover problem and optimization under soft constraints.\nWe provide results that span both multiple types of constraints (cardinality,\nknapsack, matroid and convex set) and multiple classes of submodular functions\n(monotone, symmetric and general). For many of the cases considered, we provide\noptimal results. In other cases, our results improve over the state-of-the-art,\nsometimes even over the state-of-the-art for the special case of\nsingle-criterion (standard) optimization. Results of the last kind demonstrate\nthat relaxing the feasibility constraint may give a perspective about the\nproblem that is useful even if one only desires feasible solutions.", "AI": {"tldr": "\u5b50\u6a21\u51fd\u6570\u4f18\u5316\u4e2d\uff0c\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\u7684\u7814\u7a76\uff1a\u6db5\u76d6\u591a\u79cd\u7ea6\u675f\u548c\u51fd\u6570\u7c7b\uff0c\u63d0\u4f9b\u6700\u4f18\u6216\u6539\u8fdb\u7ed3\u679c\uff0c\u5e76\u5c55\u793a\u653e\u5bbd\u7ea6\u675f\u7684\u6709\u7528\u6027\u3002", "motivation": "\u5b50\u6a21\u51fd\u6570\u53ca\u5176\u4f18\u5316\u5df2\u5728\u4ece\u673a\u5668\u5b66\u4e60\u3001\u6570\u636e\u6316\u6398\u5230\u535a\u5f08\u8bba\u548c\u7ecf\u6d4e\u5b66\u7684\u5404\u79cd\u73af\u5883\u4e2d\u5f97\u5230\u5e94\u7528\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u8003\u8651\u53d7\u7ea6\u675f\u7684\u5b50\u6a21\u51fd\u6570\u6700\u5927\u5316\u95ee\u9898\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u53d7\u7ea6\u675f\u7684\u5b50\u6a21\u51fd\u6570\u6700\u5927\u5316\u95ee\u9898\uff0c\u5e76\u5bf9\u53cc\u6807\u51c6\u8fd1\u4f3c\u7b97\u6cd5\u8fdb\u884c\u4e86\u539f\u5219\u6027\u7814\u7a76\uff0c\u8fd9\u4e9b\u7b97\u6cd5\u5141\u8bb8\u7ea6\u675f\u6700\u591a\u8fdd\u53cd\u4e00\u4e2a\u6709\u754c\u56e0\u5b50\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u6db5\u76d6\u4e86\u591a\u79cd\u7ea6\u675f\u7c7b\u578b\uff08\u57fa\u6570\u3001\u80cc\u5305\u3001\u62df\u9635\u548c\u51f8\u96c6\uff09\u548c\u591a\u79cd\u5b50\u6a21\u51fd\u6570\u7c7b\uff08\u5355\u8c03\u3001\u5bf9\u79f0\u548c\u4e00\u822c\uff09\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u63d0\u4f9b\u4e86\u6700\u4f18\u7ed3\u679c\uff0c\u6216\u8005\u6539\u8fdb\u4e86\u6700\u5148\u8fdb\u7684\u6280\u672f\uff0c\u751a\u81f3\u5728\u5355\u6807\u51c6\uff08\u6807\u51c6\uff09\u4f18\u5316\u8fd9\u4e00\u7279\u6b8a\u60c5\u51b5\u4e0b\u7684\u6700\u5148\u8fdb\u6280\u672f\u3002\u6700\u540e\u4e00\u79cd\u7ed3\u679c\u8868\u660e\uff0c\u653e\u5bbd\u53ef\u884c\u6027\u7ea6\u675f\u53ef\u4ee5\u63d0\u4f9b\u4e00\u79cd\u6709\u7528\u7684\u95ee\u9898\u89c6\u89d2\uff0c\u5373\u4f7f\u4eba\u4eec\u53ea\u60f3\u8981\u53ef\u884c\u89e3\u3002"}}
{"id": "2507.09389", "categories": ["cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09389", "abs": "https://arxiv.org/abs/2507.09389", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "title": "Knowledge Conceptualization Impacts RAG Efficacy", "comment": null, "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u77e5\u8bc6\u8868\u793a\uff08\u7ed3\u6784\u3001\u590d\u6742\u6027\uff09\u5982\u4f55\u5f71\u54cd\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u67e5\u8be2\u4e09\u5143\u7ec4\u5e93\u7684\u80fd\u529b\uff0c\u65e8\u5728\u8bbe\u8ba1\u53ef\u8f6c\u79fb\u548c\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u3002", "motivation": "\u7814\u7a76\u53ef\u8f6c\u79fb\u548c\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u7684\u8bbe\u8ba1\uff0c\u7279\u522b\u5173\u6ce8\u80fd\u591f\u4e3b\u52a8\u9009\u62e9\u3001\u89e3\u91ca\u548c\u67e5\u8be2\u77e5\u8bc6\u6765\u6e90\u7684\u201cAgentic Retrieval-Augmented Generation\u201d\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u4e0d\u540c\u77e5\u8bc6\u7684\u6982\u5ff5\u5316\u548c\u8868\u793a\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u77e5\u8bc6\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\uff0c\u5bf9AI\u4ee3\u7406\u6709\u6548\u67e5\u8be2\u4e09\u5143\u7ec4\u5e93\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u77e5\u8bc6\u7684\u6982\u5ff5\u5316\u548c\u8868\u793a\u65b9\u6cd5\u90fd\u5bf9AI\u4ee3\u7406\u67e5\u8be2\u4e09\u5143\u7ec4\u5e93\u6709\u5f71\u54cd\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u5f71\u54cd\u548c\u542f\u793a\u3002", "conclusion": "\u7814\u7a76\u4e0d\u540c\u77e5\u8bc6\u7684\u6982\u5ff5\u5316\u548c\u8868\u793a\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u77e5\u8bc6\u7684\u7ed3\u6784\u548c\u590d\u6742\u6027\uff0c\u5bf9AI\u4ee3\u7406\uff08\u5728\u672c\u4f8b\u4e2d\u662fLLM\uff09\u6709\u6548\u67e5\u8be2\u4e09\u5143\u7ec4\u5e93\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.09755", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09755", "abs": "https://arxiv.org/abs/2507.09755", "authors": ["Amir Farakhor", "Iman Askari", "Di Wu", "Huazhen Fang"], "title": "Optimal Power Management of Battery Energy Storage Systems via Ensemble Kalman Inversion", "comment": null, "summary": "Optimal power management of battery energy storage systems (BESS) is crucial\nfor their safe and efficient operation. Numerical optimization techniques are\nfrequently utilized to solve the optimal power management problems. However,\nthese techniques often fall short of delivering real-time solutions for\nlarge-scale BESS due to their computational complexity. To address this issue,\nthis paper proposes a computationally efficient approach. We introduce a new\nset of decision variables called power-sharing ratios corresponding to each\ncell, indicating their allocated power share from the output power demand. We\nthen formulate an optimal power management problem to minimize the system-wide\npower losses while ensuring compliance with safety, balancing, and power\nsupply-demand match constraints. To efficiently solve this problem, a\nparameterized control policy is designed and leveraged to transform the optimal\npower management problem into a parameter estimation problem. We then implement\nthe ensemble Kalman inversion to estimate the optimal parameter set. The\nproposed approach significantly reduces computational requirements due to 1)\nthe much lower dimensionality of the decision parameters and 2) the estimation\ntreatment of the optimal power management problem. Finally, we conduct\nextensive simulations to validate the effectiveness of the proposed approach.\nThe results show promise in accuracy and computation time compared with\nexplored numerical optimization techniques.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u65b9\u6cd5\u6765\u7ba1\u7406\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u7684\u529f\u7387\uff0c\u901a\u8fc7\u4f7f\u7528\u529f\u7387\u5171\u4eab\u6bd4\u548c\u53c2\u6570\u4f30\u8ba1\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u5728\u4eff\u771f\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6570\u503c\u4f18\u5316\u6280\u672f\u8ba1\u7b97\u590d\u6742\u6027\u9ad8\uff0c\u96be\u4ee5\u63d0\u4f9b\u5927\u89c4\u6a21\u7535\u6c60\u50a8\u80fd\u7cfb\u7edf\uff08BESS\uff09\u7684\u5b9e\u65f6\u89e3\u51b3\u65b9\u6848\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u540d\u4e3a\u201c\u529f\u7387\u5171\u4eab\u6bd4\u201d\u7684\u65b0\u51b3\u7b56\u53d8\u91cf\uff0c\u5e76\u5c06\u6700\u4f18\u529f\u7387\u7ba1\u7406\u95ee\u9898\u8f6c\u5316\u4e3a\u53c2\u6570\u4f30\u8ba1\u95ee\u9898\uff0c\u7136\u540e\u5229\u7528\u96c6\u5408\u5361\u5c14\u66fc\u9006\u4f30\u8ba1\u6700\u4f18\u53c2\u6570\u96c6\u3002", "result": "\u8be5\u65b9\u6cd5\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8981\u6c42\uff0c\u5e76\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u4e0e\u63a2\u7d22\u8fc7\u7684\u6570\u503c\u4f18\u5316\u6280\u672f\u76f8\u6bd4\u663e\u793a\u51fa\u5e0c\u671b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u65f6\u95f4\u65b9\u9762\u4e0e\u63a2\u7d22\u8fc7\u7684\u6570\u503c\u4f18\u5316\u6280\u672f\u76f8\u6bd4\u663e\u793a\u51fa\u5e0c\u671b\u3002"}}
{"id": "2507.09340", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09340", "abs": "https://arxiv.org/abs/2507.09340", "authors": ["Hongyu Nie", "Xingyu Li", "Xu Liu", "Zhaotong Tan", "Sen Mei", "Wenbo Su"], "title": "Unified Linear Parametric Map Modeling and Perception-aware Trajectory Planning for Mobile Robotics", "comment": "Submitted to IEEE Transactions on Robotics (TRO) in July 2025", "summary": "Autonomous navigation in mobile robots, reliant on perception and planning,\nfaces major hurdles in large-scale, complex environments. These include heavy\ncomputational burdens for mapping, sensor occlusion failures for UAVs, and\ntraversal challenges on irregular terrain for UGVs, all compounded by a lack of\nperception-aware strategies. To address these challenges, we introduce Random\nMapping and Random Projection (RMRP). This method constructs a lightweight\nlinear parametric map by first mapping data to a high-dimensional space,\nfollowed by a sparse random projection for dimensionality reduction. Our novel\nResidual Energy Preservation Theorem provides theoretical guarantees for this\nprocess, ensuring critical geometric properties are preserved. Based on this\nmap, we propose the RPATR (Robust Perception-Aware Trajectory Planner)\nframework. For UAVs, our method unifies grid and Euclidean Signed Distance\nField (ESDF) maps. The front-end uses an analytical occupancy gradient to\nrefine initial paths for safety and smoothness, while the back-end uses a\nclosed-form ESDF for trajectory optimization. Leveraging the trained RMRP\nmodel's generalization, the planner predicts unobserved areas for proactive\nnavigation. For UGVs, the model characterizes terrain and provides closed-form\ngradients, enabling online planning to circumvent large holes. Validated in\ndiverse scenarios, our framework demonstrates superior mapping performance in\ntime, memory, and accuracy, and enables computationally efficient, safe\nnavigation for high-speed UAVs and UGVs. The code will be released to foster\ncommunity collaboration.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RMRP \u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u57fa\u4e8e\u6b64\u5f00\u53d1\u4e86 RPATR \u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u590d\u6742\u73af\u5883\u4e2d\u5bfc\u822a\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8f7b\u91cf\u7ea7\u5efa\u56fe\u548c\u611f\u77e5\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\uff0c\u63d0\u9ad8\u4e86\u5bfc\u822a\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u79fb\u52a8\u673a\u5668\u4eba\u5728\u5927\u89c4\u6a21\u3001\u590d\u6742\u73af\u5883\u4e2d\u81ea\u4e3b\u5bfc\u822a\u7684\u91cd\u5927\u969c\u788d\uff0c\u4f8b\u5982\u7e41\u91cd\u7684\u8ba1\u7b97\u8d1f\u62c5\u3001\u4f20\u611f\u5668\u906e\u6321\u6545\u969c\u4ee5\u53ca\u4e0d\u89c4\u5219\u5730\u5f62\u7684\u7a7f\u8d8a\u6311\u6218\uff0c\u4ee5\u53ca\u7f3a\u4e4f\u611f\u77e5\u611f\u77e5\u7b56\u7565\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u968f\u673a\u6620\u5c04\u548c\u968f\u673a\u6295\u5f71\uff08RMRP\uff09\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6570\u636e\u6620\u5c04\u5230\u9ad8\u7ef4\u7a7a\u95f4\uff0c\u7136\u540e\u8fdb\u884c\u7a00\u758f\u968f\u673a\u6295\u5f71\u4ee5\u8fdb\u884c\u964d\u7ef4\uff0c\u4ece\u800c\u6784\u5efa\u8f7b\u91cf\u7ea7\u7ebf\u6027\u53c2\u6570\u5316\u5730\u56fe\u3002\u5229\u7528\u63d0\u51fa\u7684\u6b8b\u5dee\u80fd\u91cf\u5b88\u6052\u5b9a\u7406\u4e3a\u8be5\u8fc7\u7a0b\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\uff0c\u786e\u4fdd\u5173\u952e\u51e0\u4f55\u5c5e\u6027\u5f97\u4ee5\u4fdd\u7559\u3002\u5728\u6b64\u5730\u56fe\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RPATR\uff08\u9c81\u68d2\u611f\u77e5\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u5668\uff09\u7684\u6846\u67b6\u3002", "result": "\u8be5\u6846\u67b6\u5728\u591a\u79cd\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5728\u5efa\u56fe\u7684\u8ba1\u7b97\u6548\u7387\u3001\u5185\u5b58\u5360\u7528\u548c\u51c6\u786e\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8d85\u97f3\u901f\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u673a\u7684\u5b89\u5168\u5bfc\u822a\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u65f6\u95f4\u548c\u5185\u5b58\u6548\u7387\u4ee5\u53ca\u51c6\u786e\u6027\u65b9\u9762\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u5efa\u56fe\u6027\u80fd\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u8d85\u97f3\u901f\u65e0\u4eba\u673a\u548c\u65e0\u4eba\u673a\u7684\u8ba1\u7b97\u9ad8\u6548\u3001\u5b89\u5168\u5bfc\u822a\u3002"}}
{"id": "2507.09081", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09081", "abs": "https://arxiv.org/abs/2507.09081", "authors": ["Zhenyu Yu", "Mohd Yamani Idna Idris", "Hua Wang", "Pei Wang", "Junyi Chen", "Kun Wang"], "title": "From Physics to Foundation Models: A Review of AI-Driven Quantitative Remote Sensing Inversion", "comment": null, "summary": "Quantitative remote sensing inversion aims to estimate continuous surface\nvariables-such as biomass, vegetation indices, and evapotranspiration-from\nsatellite observations, supporting applications in ecosystem monitoring, carbon\naccounting, and land management. With the evolution of remote sensing systems\nand artificial intelligence, traditional physics-based paradigms are giving way\nto data-driven and foundation model (FM)-based approaches. This paper\nsystematically reviews the methodological evolution of inversion techniques,\nfrom physical models (e.g., PROSPECT, SCOPE, DART) to machine learning methods\n(e.g., deep learning, multimodal fusion), and further to foundation models\n(e.g., SatMAE, GFM, mmEarth). We compare the modeling assumptions, application\nscenarios, and limitations of each paradigm, with emphasis on recent FM\nadvances in self-supervised pretraining, multi-modal integration, and\ncross-task adaptation. We also highlight persistent challenges in physical\ninterpretability, domain generalization, limited supervision, and uncertainty\nquantification. Finally, we envision the development of next-generation\nfoundation models for remote sensing inversion, emphasizing unified modeling\ncapacity, cross-domain generalization, and physical interpretability.", "AI": {"tldr": "\u5b9a\u91cf\u9065\u611f\u53cd\u6f14\u65b9\u6cd5\u6b63\u4ece\u7269\u7406\u6a21\u578b\u8f6c\u5411\u673a\u5668\u5b66\u4e60\u548c\u57fa\u7840\u6a21\u578b\u3002\u672c\u6587\u56de\u987e\u4e86\u8fd9\u4e00\u6f14\u53d8\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u65b9\u6cd5\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u5305\u62ec\u63d0\u9ad8\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3001\u57df\u6cdb\u5316\u80fd\u529b\u548c\u7edf\u4e00\u5efa\u6a21\u80fd\u529b\u3002", "motivation": "\u5b9a\u91cf\u9065\u611f\u53cd\u6f14\u65e8\u5728\u4ece\u536b\u661f\u89c2\u6d4b\u4e2d\u4f30\u8ba1\u5730\u8868\u53d8\u91cf\uff08\u5982\u751f\u7269\u91cf\u3001\u690d\u88ab\u6307\u6570\u3001\u84b8\u6563\u53d1\u7b49\uff09\uff0c\u4ee5\u652f\u6301\u751f\u6001\u7cfb\u7edf\u76d1\u6d4b\u3001\u78b3\u6838\u7b97\u548c\u571f\u5730\u7ba1\u7406\u7b49\u5e94\u7528\u3002\u968f\u7740\u9065\u611f\u7cfb\u7edf\u548c\u4eba\u5de5\u667a\u80fd\u7684\u53d1\u5c55\uff0c\u6570\u636e\u9a71\u52a8\u548c\u57fa\u7840\u6a21\u578b\uff08FM\uff09\u65b9\u6cd5\u6b63\u9010\u6e10\u53d6\u4ee3\u4f20\u7edf\u7684\u57fa\u4e8e\u7269\u7406\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u91c7\u7528\u7cfb\u7edf\u6027\u56de\u987e\u7684\u65b9\u6cd5\uff0c\u6bd4\u8f83\u4e86\u4e0d\u540c\u53cd\u6f14\u8303\u5f0f\uff08\u7269\u7406\u6a21\u578b\u3001\u673a\u5668\u5b66\u4e60\u3001\u57fa\u7840\u6a21\u578b\uff09\u7684\u5efa\u6a21\u5047\u8bbe\u3001\u5e94\u7528\u573a\u666f\u548c\u5c40\u9650\u6027\uff0c\u5e76\u91cd\u70b9\u5173\u6ce8\u4e86\u57fa\u7840\u6a21\u578b\u5728\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u65b9\u9762\u7684\u8fdb\u5c55\u3002", "result": "\u7814\u7a76\u5bf9\u4ece\u7269\u7406\u6a21\u578b\uff08\u5982PROSPECT, SCOPE, DART\uff09\u5230\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08\u5982\u6df1\u5ea6\u5b66\u4e60\u3001\u591a\u6a21\u6001\u878d\u5408\uff09\uff0c\u518d\u5230\u57fa\u7840\u6a21\u578b\uff08\u5982SatMAE, GFM, mmEarth\uff09\u7684\u53cd\u6f14\u6280\u672f\u65b9\u6cd5\u5b66\u6f14\u53d8\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u56de\u987e\u548c\u6bd4\u8f83\u3002\u8bba\u6587\u5f3a\u8c03\u4e86\u57fa\u7840\u6a21\u578b\u5728\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u8de8\u4efb\u52a1\u9002\u5e94\u65b9\u9762\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u6307\u51fa\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3001\u57df\u6cdb\u5316\u3001\u6709\u9650\u76d1\u7763\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u8be5\u8bba\u6587\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u5b9a\u91cf\u9065\u611f\u53cd\u6f14\u65b9\u6cd5\u5b66\u7684\u6f14\u53d8\uff0c\u4ece\u7269\u7406\u6a21\u578b\u5230\u673a\u5668\u5b66\u4e60\uff0c\u518d\u5230\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u63a2\u8ba8\u4e86\u5f53\u524d\u9762\u4e34\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2507.10413", "categories": ["cs.DC", "cs.CC", "cs.IT", "cs.LO", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.10413", "abs": "https://arxiv.org/abs/2507.10413", "authors": ["Gabriel Rocha"], "title": "Consensus, Inconsistency, Emergence: what's paraconsistency got to do with it?", "comment": "10 pages", "summary": "The consensus problem, briefly stated, consists of having processes in an\nasynchronous distributed system agree on a value. It is widely known that the\nconsensus problem does not have a deterministic solution that ensures both\ntermination and consistency, if there is at least one faulty process in the\nsystem. This result, known as the FLP impossibility theorem, led to several\ngeneralizations and developments in theoretical distributed computing. This\npaper argues that the FLP impossibility theorem holds even under a generalized\ndefinition of computation through oracles. Furthermore, using a theoretical\nmachinery from complex systems, this paper also posits that inconsistency may\nbe an emergent feature of consensus over distributed systems by examining how a\nsystem transitions phases. Under the same complex systems framework, this paper\nexamines paraconsistent logics, arguing that while inconsistency is not an\nemergent feature for these logics, triviality may be. Lastly, some attention is\ngiven to the possibility of developing consensus algorithms capable of\nparaconsistent reasoning.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63a2\u8ba8\u4e86\u5206\u5e03\u5f0f\u5171\u8bc6\u95ee\u9898\uff0c\u8bc1\u660e\u4e86 FLP \u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u5728\u5e7f\u4e49\u8ba1\u7b97\u6a21\u578b\u4e0b\u4f9d\u7136\u6210\u7acb\uff0c\u5e76\u8fd0\u7528\u590d\u6742\u7cfb\u7edf\u7406\u8bba\u5206\u6790\u4e86\u4e0d\u4e00\u81f4\u6027\u4f5c\u4e3a\u5171\u8bc6\u6d8c\u73b0\u7279\u5f81\u7684\u53ef\u80fd\u6027\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u975e\u77db\u76fe\u903b\u8f91\u53ef\u80fd\u4ee5\u5e73\u51e1\u6027\u800c\u975e\u4e0d\u4e00\u81f4\u6027\u4f5c\u4e3a\u6d8c\u73b0\u7279\u5f81\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u5728\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e2d\uff0c\u5171\u8bc6\u95ee\u9898\u7684\u5c40\u9650\u6027\u4ee5\u53ca\u4e0d\u4e00\u81f4\u6027\u5728\u5176\u4e2d\u7684\u4f5c\u7528\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u8bd5\u56fe\u8bc1\u660e FLP \u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u5728\u66f4\u5e7f\u4e49\u7684\u8ba1\u7b97\u6a21\u578b\u4e0b\u4ecd\u7136\u6210\u7acb\uff0c\u5e76\u8fd0\u7528\u590d\u6742\u7cfb\u7edf\u7406\u8bba\u6765\u5206\u6790\u4e0d\u4e00\u81f4\u6027\u4f5c\u4e3a\u5171\u8bc6\u6d8c\u73b0\u7279\u5f81\u7684\u53ef\u80fd\u6027\u3002\u6b64\u5916\uff0c\u8be5\u8bba\u6587\u8fd8\u7814\u7a76\u4e86\u975e\u77db\u76fe\u903b\u8f91\u5728\u5904\u7406\u5206\u5e03\u5f0f\u7cfb\u7edf\u5171\u8bc6\u95ee\u9898\u4e0a\u7684\u6f5c\u529b\uff0c\u4ee5\u53ca\u662f\u5426\u53ef\u80fd\u5f00\u53d1\u51fa\u80fd\u591f\u8fdb\u884c\u975e\u77db\u76fe\u63a8\u7406\u7684\u5171\u8bc6\u7b97\u6cd5\u3002", "method": "\u8be5\u8bba\u6587\u9996\u5148\u8bba\u8bc1\u4e86 FLP \u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u5728\u901a\u8fc7\u9884\u8a00\u673a\u8fdb\u884c\u8ba1\u7b97\u7684\u5e7f\u4e49\u5b9a\u4e49\u4e0b\u4ecd\u7136\u6210\u7acb\u3002\u7136\u540e\uff0c\u5b83\u5229\u7528\u590d\u6742\u7cfb\u7edf\u7406\u8bba\u4e2d\u7684\u5de5\u5177\uff0c\u901a\u8fc7\u68c0\u67e5\u7cfb\u7edf\u5982\u4f55\u5728\u4e0d\u540c\u9636\u6bb5\u4e4b\u95f4\u8f6c\u6362\uff0c\u6765\u8bba\u8bc1\u4e0d\u4e00\u81f4\u6027\u53ef\u80fd\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e0a\u5171\u8bc6\u7684\u6d8c\u73b0\u7279\u5f81\u3002\u8be5\u8bba\u6587\u8fd8\u5728\u6b64\u590d\u6742\u7cfb\u7edf\u6846\u67b6\u4e0b\uff0c\u5bf9\u975e\u77db\u76fe\u903b\u8f91\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u5e76\u8ba4\u4e3a\u4e0d\u4e00\u81f4\u6027\u4e0d\u662f\u8fd9\u4e9b\u903b\u8f91\u7684\u6d8c\u73b0\u7279\u5f81\uff0c\u4f46\u5e73\u51e1\u6027\u53ef\u80fd\u662f\u3002\u6700\u540e\uff0c\u8be5\u8bba\u6587\u8fd8\u63a2\u8ba8\u4e86\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u975e\u77db\u76fe\u63a8\u7406\u7684\u5171\u8bc6\u7b97\u6cd5\u7684\u53ef\u80fd\u6027\u3002", "result": "\u8be5\u8bba\u6587\u7684\u4e3b\u8981\u7ed3\u679c\u662f\uff0cFLP \u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u5728\u901a\u8fc7\u9884\u8a00\u673a\u8fdb\u884c\u8ba1\u7b97\u7684\u5e7f\u4e49\u5b9a\u4e49\u4e0b\u4ecd\u7136\u6210\u7acb\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u4e0d\u4e00\u81f4\u6027\u53ef\u80fd\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e0a\u5171\u8bc6\u7684\u6d8c\u73b0\u7279\u5f81\uff0c\u800c\u975e\u77db\u76fe\u903b\u8f91\u867d\u7136\u4e0d\u4ee5\u4e0d\u4e00\u81f4\u6027\u4f5c\u4e3a\u6d8c\u73b0\u7279\u5f81\uff0c\u4f46\u53ef\u80fd\u4ee5\u5e73\u51e1\u6027\u4f5c\u4e3a\u6d8c\u73b0\u7279\u5f81\u3002", "conclusion": "\u8be5\u8bba\u6587\u8ba4\u4e3a\uff0c\u5373\u4f7f\u5728\u901a\u8fc7\u9884\u8a00\u673a\u8fdb\u884c\u8ba1\u7b97\u7684\u5e7f\u4e49\u5b9a\u4e49\u4e0b\uff0cFLP \u4e0d\u53ef\u80fd\u6027\u5b9a\u7406\u4e5f\u6210\u7acb\u3002\u8be5\u8bba\u6587\u8fd8\u63d0\u51fa\uff0c\u5728\u76f8\u540c\u7684\u590d\u6742\u7cfb\u7edf\u6846\u67b6\u4e0b\uff0c\u901a\u8fc7\u68c0\u67e5\u7cfb\u7edf\u5982\u4f55\u8f6c\u6362\u9636\u6bb5\uff0c\u4e0d\u4e00\u81f4\u6027\u53ef\u80fd\u662f\u5206\u5e03\u5f0f\u7cfb\u7edf\u4e0a\u5171\u8bc6\u7684\u6d8c\u73b0\u7279\u5f81\u3002\u5bf9\u4e8e\u975e\u77db\u76fe\u903b\u8f91\u800c\u8a00\uff0c\u4e0d\u4e00\u81f4\u6027\u4e0d\u662f\u4e00\u79cd\u6d8c\u73b0\u7279\u5f81\uff0c\u4f46\u5e73\u51e1\u6027\u53ef\u80fd\u662f\u4e00\u79cd\u6d8c\u73b0\u7279\u5f81\u3002\u6700\u540e\uff0c\u8be5\u8bba\u6587\u8fd8\u63a2\u8ba8\u4e86\u5f00\u53d1\u80fd\u591f\u8fdb\u884c\u975e\u77db\u76fe\u63a8\u7406\u7684\u5171\u8bc6\u7b97\u6cd5\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.09243", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09243", "abs": "https://arxiv.org/abs/2507.09243", "authors": ["Shiran Even-Haim", "Ethan Nussinson", "Roni Ben-Maimon", "Alexey Gorlach", "Ron Ruimy", "Ephraim Shahmoon", "Osip Schwartz", "Ido Kaminer"], "title": "Spin Squeezing in Electron Microscopy", "comment": null, "summary": "Quantum metrology experiments in atomic physics and quantum optics have\ndemonstrated measurement accuracy beyond the shot-noise limit via\nmulti-particle entanglement. At the same time, electron microscopy, an\nessential tool for high-resolution imaging of biological systems, is severely\nconstrained in its signal-to-noise ratio (SNR) by shot noise, due to the dose\nlimit imposed by electron beam-induced damage. Here, we show theoretically that\nspin squeezing, a form of quantum metrology based on entanglement, is a natural\nfit for improving the SNR in electron microscopy. We investigate the generation\nof the necessary entangled states through electron-electron Coulomb\ninteractions and quantum non-demolition measurements. Our results connect the\nfields of quantum metrology and electron interferometry, paving the way toward\nelectron microscopy with SNR beyond the shot-noise limit.", "AI": {"tldr": "\u91cf\u5b50\u8ba1\u91cf\u5b66\u4e2d\u7684\u81ea\u65cb\u538b\u7f29\u6280\u672f\u6709\u671b\u63d0\u9ad8\u7535\u5b50\u663e\u5fae\u955c\u7684\u4fe1\u566a\u6bd4\u3002", "motivation": "\u7535\u5b50\u663e\u5fae\u955c\u7684\u4fe1\u566a\u6bd4\u53d7\u5230\u7535\u5b50\u675f\u5f15\u8d77\u7684\u635f\u4f24\u6240\u65bd\u52a0\u7684\u5242\u91cf\u9650\u5236\u7684\u4e25\u91cd\u5236\u7ea6\uff0c\u800c\u81ea\u65cb\u538b\u7f29\u4f5c\u4e3a\u4e00\u79cd\u91cf\u5b50\u8ba1\u91cf\u5b66\u65b9\u6cd5\uff0c\u6709\u671b\u6539\u5584\u7535\u5b50\u663e\u5fae\u955c\u7684\u4fe1\u566a\u6bd4\u3002", "method": "\u901a\u8fc7\u7535\u5b50-\u7535\u5b50\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u548c\u91cf\u5b50\u975e\u7834\u574f\u6027\u6d4b\u91cf\u6765\u751f\u6210\u81ea\u65cb\u538b\u7f29\u6001\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u81ea\u65cb\u538b\u7f29\u53ef\u4ee5\u63d0\u9ad8\u7535\u5b50\u663e\u5fae\u955c\u7684\u4fe1\u566a\u6bd4\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u7535\u5b50-\u7535\u5b50\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u548c\u91cf\u5b50\u975e\u7834\u574f\u6027\u6d4b\u91cf\u4ea7\u751f\u6240\u9700\u7684\u7ea0\u7f20\u6001\u3002", "conclusion": "\u672c\u7814\u7a76\u5728\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u81ea\u65cb\u538b\u7f29\uff08\u4e00\u79cd\u57fa\u4e8e\u7ea0\u7f20\u7684\u91cf\u5b50\u8ba1\u91cf\u5b66\uff09\u53ef\u4ee5\u5e94\u7528\u4e8e\u63d0\u9ad8\u7535\u5b50\u663e\u5fae\u955c\u7684\u4fe1\u566a\u6bd4\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u7535\u5b50-\u7535\u5b50\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u548c\u91cf\u5b50\u975e\u7834\u574f\u6027\u6d4b\u91cf\u4ea7\u751f\u6240\u9700\u7684\u7ea0\u7f20\u6001\uff0c\u4ece\u800c\u5c06\u91cf\u5b50\u8ba1\u91cf\u5b66\u4e0e\u7535\u5b50\u5e72\u6d89\u6d4b\u91cf\u5b66\u9886\u57df\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u5b9e\u73b0\u8d85\u8d8a\u6563\u7c92\u566a\u58f0\u6781\u9650\u7684\u7535\u5b50\u663e\u5fae\u955c\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09076", "categories": ["cs.CL", "cs.AI", "68T50", "I.2.7; H.5.2"], "pdf": "https://arxiv.org/pdf/2507.09076", "abs": "https://arxiv.org/abs/2507.09076", "authors": ["Jialong Mai", "Xiaofen Xing", "Yawei Li", "Zhipeng Li", "Jingyuan Xing", "Xiangmin Xu"], "title": "Dynamic Parameter Memory: Temporary LoRA-Enhanced LLM for Long-Sequence Emotion Recognition in Conversation", "comment": "submitted to EMNLP 2025", "summary": "Recent research has focused on applying speech large language model (SLLM) to\nimprove speech emotion recognition (SER). However, the inherently high frame\nrate in speech modality severely limits the signal processing and understanding\ncapabilities of SLLM. For example, a SLLM with a 4K context window can only\nprocess 80 seconds of audio at 50Hz feature sampling rate before reaching its\ncapacity limit. Input token compression methods used in SLLM overlook the\ncontinuity and inertia of emotions across multiple conversation turns. This\npaper proposes a Dynamic Parameter Memory (DPM) mechanism with contextual\nsemantics and sentence-level emotion encoding, enabling processing of\nunlimited-length audio with limited context windows in SLLM. Specifically, DPM\nprogressively encodes sentence-level information and emotions into a temporary\nLoRA module during inference to effectively \"memorize\" the contextual\ninformation. We trained an emotion SLLM as a backbone and incorporated our DPM\ninto inference for emotion recognition in conversation (ERC). Experimental\nresults on the IEMOCAP dataset show that DPM significantly improves the emotion\nrecognition capabilities of SLLM when processing long audio sequences,\nachieving state-of-the-art performance.", "AI": {"tldr": "\u901a\u8fc7\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\uff08DPM\uff09\u673a\u5236\uff0cSLLM\u80fd\u591f\u5904\u7406\u65e0\u9650\u957f\u97f3\u9891\uff0c\u63d0\u5347\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u5927\u8bed\u8a00\u6a21\u578b\uff08SLLM\uff09\u5728\u5e94\u7528\u4e8e\u8bed\u97f3\u60c5\u611f\u8bc6\u522b\uff08SER\uff09\u65f6\uff0c\u53d7\u9650\u4e8e\u8bed\u97f3\u4fe1\u53f7\u7684\u5e27\u7387\u548cSLLM\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u957f\u5ea6\uff0c\u96be\u4ee5\u6709\u6548\u5904\u7406\u957f\u97f3\u9891\u548c\u591a\u8f6e\u5bf9\u8bdd\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u8f93\u5165\u4ee4\u724c\u538b\u7f29\u65b9\u6cd5\u5ffd\u7565\u4e86\u8de8\u591a\u8f6e\u5bf9\u8bdd\u7684\u60c5\u611f\u8fde\u7eed\u6027\u548c\u60ef\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\uff08DPM\uff09\u673a\u5236\uff0c\u8be5\u673a\u5236\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u9010\u6b65\u5c06\u53e5\u5b50\u7ea7\u4fe1\u606f\u548c\u60c5\u611f\u7f16\u7801\u5230\u4e34\u65f6\u7684LoRA\u6a21\u5757\u4e2d\uff0c\u4ee5\u6709\u6548\u8bb0\u5fc6\u4e0a\u4e0b\u6587\u4fe1\u606f\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u60c5\u611fSLLM\u7684\u63a8\u7406\u8fc7\u7a0b\u4ee5\u8fdb\u884c\u5bf9\u8bdd\u4e2d\u7684\u60c5\u611f\u8bc6\u522b\uff08ERC\uff09\u3002", "result": "\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDPM\u663e\u8457\u63d0\u9ad8\u4e86SLLM\u5904\u7406\u957f\u97f3\u9891\u5e8f\u5217\u65f6\u7684\u60c5\u611f\u8bc6\u522b\u80fd\u529b\uff0c\u8fbe\u5230\u4e86\u5f53\u524d\u6700\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "DPM\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u53c2\u6570\u8bb0\u5fc6\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u4e0a\u4e0b\u6587\u8bed\u4e49\u548c\u53e5\u5b50\u7ea7\u60c5\u611f\u7f16\u7801\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709SLLM\u5728\u5904\u7406\u957f\u97f3\u9891\u548c\u591a\u8f6e\u5bf9\u8bdd\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u5b9e\u73b0\u4e86\u5bf9\u65e0\u9650\u957f\u97f3\u9891\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5728IEMOCAP\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10550", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2507.10550", "abs": "https://arxiv.org/abs/2507.10550", "authors": ["Quentin Guilmant", "Jo\u00ebl Ouaknine", "Isa Vialard"], "title": "The Value Problem for Weighted Timed Games with Two Clocks is Undecidable", "comment": null, "summary": "The Value Problem for weighted timed games (WTGs) consists in determining,\ngiven a two-player weighted timed game with a reachability objective and a\nrational threshold, whether or not the value of the game exceeds the threshold.\nThis problem was shown to be undecidable some ten years ago for WTGs making use\nof at least three clocks, and is known to be decidable for single-clock WTGs.\nIn this paper, we establish undecidability for two-clock WTGs making use of\nnon-negative weights, even in a time-bounded setting, closing the last\nremaining major gap in our algorithmic understanding of WTGs.", "AI": {"tldr": "\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\u7684\u4ef7\u503c\u95ee\u9898\u5bf9\u4e8e\u4e24\u65f6\u949f\u548c\u975e\u8d1f\u6743\u91cd\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002", "motivation": "\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\uff08WTGs\uff09\u7684\u4ef7\u503c\u95ee\u9898\u5728\u7b97\u6cd5\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e00\u4e2a\u4e3b\u8981\u5dee\u8ddd\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u4e24\u65f6\u949f\u7684\u60c5\u51b5\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u5dee\u8ddd\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u5177\u6709\u975e\u8d1f\u6743\u91cd\u7684\u4e24\u65f6\u949f\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\uff08WTGs\uff09\u7684\u4ef7\u503c\u95ee\u9898\u6765\u8bc1\u660e\u5176\u4e0d\u53ef\u5224\u5b9a\u6027\uff0c\u5373\u4f7f\u5728\u6709\u65f6\u95f4\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u3002", "result": "\u8bc1\u660e\u4e86\u5177\u6709\u975e\u8d1f\u6743\u91cd\u7684\u4e24\u65f6\u949fWTGs\u7684\u4ef7\u503c\u95ee\u9898\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\uff0c\u5373\u4f7f\u5728\u6709\u65f6\u95f4\u9650\u5236\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u4e24\u65f6\u949f\u52a0\u6743\u65f6\u5e8f\u535a\u5f08\uff08WTGs\uff09\u7684\u4ef7\u503c\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u6709\u65f6\u95f4\u9650\u5236\u7684\u60c5\u51b5\u4e0b\uff0c\u4f7f\u7528\u975e\u8d1f\u6743\u91cd\u4e5f\u662f\u4e0d\u53ef\u5224\u5b9a\u7684\u3002"}}
{"id": "2507.09458", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09458", "abs": "https://arxiv.org/abs/2507.09458", "authors": ["Wang Ning", "Zhang Chenyu", "Sun Yanshi", "Min Minghui", "Liu Yuanwei", "Li Shiyin"], "title": "An Enregy Efficient Design of Hybrid NOMA Based on Hybrid SIC with Power Adaptation", "comment": "13pages, 8figures, 4tables. Submitted to IEEE TWC, manuscript ID is\n  Paper-TW-Jul-25-1790", "summary": "Recently, hybrid non-orthogonal multiple access (H-NOMA) technology, which\neffectively utilizes both NOMA and orthogonal multiple access (OMA)\ntechnologies through flexible resource allocation in a single transmission, has\ndemonstrated immense potential for enhancing the performance of wireless\ncommunication systems. To further release the potential of HNOMA, this paper\nproposes a novel design of H-NOMA which jointly incorporates hybrid successive\ninterference cancellation (HSIC) and power adaptation (PA) in the NOMA\ntransmission phase. To reveal the potential of the proposed HSIC-PA aided\nH-NOMA scheme, closed-form expression for the probability of the event that\nH-NOMA can achieve a higher data rate than pure OMA by consuming less energy is\nrigorously derived. Furthermore, the asymptotic analysis demonstrates that the\nprobability of the proposed H-NOMA scheme approaches 1 in the high\nsignal-to-noise ratio (SNR) regime without any constraints on either users'\ntarget rates or transmit power ratios. This represents a significant\nimprovement over conventional H-NOMA schemes, which require specific\nrestrictive conditions to achieve probability 1 at high SNRs as shown in\nexisting work. The above observation indicates that with less energy\nconsumption, the proposed HSIC-PA aided H-NOMA can achieve a higher data rate\nthan pure OMA with probability 1 at high SNRs, and hence a higher energy\nefficiency. Finally, numerical results are provided to verify the accuracy of\nthe analysis and also demonstrate the superior performance of the proposed\nH-NOMA scheme.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408HSIC\u548cPA\u7684\u65b0\u578bH-NOMA\u65b9\u6848\uff0c\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u80fd\u591f\u4ee5\u6982\u73871\u5b9e\u73b0\u9ad8\u4e8e\u7eafOMA\u7684\u6570\u636e\u901f\u7387\u548c\u66f4\u9ad8\u7684\u80fd\u6548\u3002", "motivation": "\u4e3a\u4e86\u8fdb\u4e00\u6b65\u91ca\u653e\u6df7\u5408\u975e\u6b63\u4ea4\u591a\u5740\uff08H-NOMA\uff09\u6280\u672f\u7684\u6f5c\u529b\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u65b0\u7684H-NOMA\u8bbe\u8ba1\uff0c\u65e8\u5728\u63d0\u9ad8\u65e0\u7ebf\u901a\u4fe1\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6df7\u5408\u8fde\u7eed\u5e72\u6270\u6d88\u9664\uff08HSIC\uff09\u548c\u529f\u7387\u81ea\u9002\u5e94\uff08PA\uff09\u7684\u65b0\u578b\u6df7\u5408\u975e\u6b63\u4ea4\u591a\u5740\uff08H-NOMA\uff09\u8bbe\u8ba1\uff0c\u7528\u4e8eNOMA\u4f20\u8f93\u9636\u6bb5\u3002\u63a8\u5bfc\u4e86H-NOMA\u5b9e\u73b0\u9ad8\u4e8e\u7eafOMA\u6570\u636e\u901f\u7387\u4e14\u80fd\u8017\u66f4\u4f4e\u7684\u6982\u7387\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u901a\u8fc7\u6e10\u8fdb\u5206\u6790\u8bc1\u660e\u4e86\u8be5\u65b9\u6848\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\u53ef\u4ee5\u6982\u73871\u5730\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "result": "\u901a\u8fc7\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6848\u7684\u5206\u6790\u51c6\u786e\u6027\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u76f8\u6bd4\u4f20\u7edfH-NOMA\u65b9\u6848\u5728\u80fd\u6548\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684HSIC-PA\u8f85\u52a9H-NOMA\u65b9\u6848\uff0c\u5728\u9ad8\u4fe1\u566a\u6bd4\u4e0b\uff0c\u80fd\u591f\u4ee5\u6982\u73871\u5b9e\u73b0\u8d85\u8d8a\u7eafOMA\u7684\u66f4\u9ad8\u6570\u636e\u901f\u7387\u548c\u66f4\u9ad8\u7684\u80fd\u6548\uff0c\u4e14\u65e0\u7528\u6237\u76ee\u6807\u901f\u7387\u6216\u53d1\u5c04\u529f\u7387\u6bd4\u7684\u9650\u5236\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edfH-NOMA\u65b9\u6848\u3002"}}
{"id": "2507.10324", "categories": ["cs.MA", "cs.AI", "cs.PL", "cs.SE", "I.2.11; I.2.4; I.2.5"], "pdf": "https://arxiv.org/pdf/2507.10324", "abs": "https://arxiv.org/abs/2507.10324", "authors": ["Amit K. Chopra", "Samuel H. Christie V", "Munindar P. Singh"], "title": "Toolsuite for Implementing Multiagent Systems Based on Communication Protocols", "comment": null, "summary": "Interaction-Oriented Programming (IOP) is an approach to building a\nmultiagent system by modeling the interactions between its roles via a flexible\ninteraction protocol and implementing agents to realize the interactions of the\nroles they play in the protocol.\n  In recent years, we have developed an extensive suite of software that\nenables multiagent system developers to apply IOP. These include tools for\nefficiently verifying protocols for properties such as liveness and safety and\nmiddleware that simplifies the implementation of agents. This paper presents\nsome of that software suite.", "AI": {"tldr": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u7528\u4e8e\u591a\u4e3b\u4f53\u7cfb\u7edf\u4ea4\u4e92\u5bfc\u5411\u7f16\u7a0b\uff08IOP\uff09\u7684\u8f6f\u4ef6\u5957\u4ef6\uff0c\u5305\u62ec\u534f\u8bae\u9a8c\u8bc1\u5de5\u5177\u548c\u4ee3\u7406\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u3002", "motivation": "\u5c55\u793a\u4e86\u4e3a\u591a\u4e3b\u4f53\u7cfb\u7edf\u5f00\u53d1\u4ea4\u4e92\u5bfc\u5411\u7f16\u7a0b\uff08IOP\uff09\u7684\u8f6f\u4ef6\u5957\u4ef6\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u534f\u8bae\u9a8c\u8bc1\u5de5\u5177\u548c\u4ee3\u7406\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u5957\u7528\u4e8e\u591a\u4e3b\u4f53\u7cfb\u7edf\u4ea4\u4e92\u5bfc\u5411\u7f16\u7a0b\uff08IOP\uff09\u7684\u8f6f\u4ef6\u5de5\u5177\uff0c\u5305\u62ec\u4ea4\u4e92\u534f\u8bae\u9a8c\u8bc1\u5de5\u5177\u548c\u4ee3\u7406\u5b9e\u73b0\u4e2d\u95f4\u4ef6\u3002", "result": "\u8be5\u8bba\u6587 presented some of the software suite that enables multiagent system developers to apply IOP, including tools for verifying protocols and middleware for implementing agents.", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e3a\u591a\u4e3b\u4f53\u7cfb\u7edf\u4ea4\u4e92\u5bfc\u5411\u7f16\u7a0b\uff08IOP\uff09\u5f00\u53d1\u7684\u8f6f\u4ef6\u5957\u4ef6\uff0c\u5305\u62ec\u7528\u4e8e\u9a8c\u8bc1\u4ea4\u4e92\u534f\u8bae\u7684\u5de5\u5177\u548c\u7b80\u5316\u4ee3\u7406\u5b9e\u73b0\u7684\u4e2d\u95f4\u4ef6\u3002"}}
{"id": "2507.08842", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08842", "abs": "https://arxiv.org/abs/2507.08842", "authors": ["Zhufeng Lu", "Chentao Jia", "Ming Hu", "Xiaofei Xie", "Mingsong Chen"], "title": "Gradients as an Action: Towards Communication-Efficient Federated Recommender Systems via Adaptive Action Sharing", "comment": "This paper has been accepted by ACM SIGKDD 2025", "summary": "As a promising privacy-aware collaborative model training paradigm, Federated\nLearning (FL) is becoming popular in the design of distributed recommender\nsystems. However, Federated Recommender Systems (FedRecs) greatly suffer from\ntwo major problems: i) extremely high communication overhead due to massive\nitem embeddings involved in recommendation systems, and ii) intolerably low\ntraining efficiency caused by the entanglement of both heterogeneous network\nenvironments and client devices. Although existing methods attempt to employ\nvarious compression techniques to reduce communication overhead, due to the\nparameter errors introduced by model compression, they inevitably suffer from\nmodel performance degradation. To simultaneously address the above problems,\nthis paper presents a communication-efficient FedRec framework named FedRAS,\nwhich adopts an action-sharing strategy to cluster the gradients of item\nembedding into a specific number of model updating actions for communication\nrather than directly compressing the item embeddings. In this way, the cloud\nserver can use the limited actions from clients to update all the items. Since\ngradient values are significantly smaller than item embeddings, constraining\nthe directions of gradients (i.e., the action space) introduces smaller errors\ncompared to compressing the entire item embedding matrix into a reduced space.\nTo accommodate heterogeneous devices and network environments, FedRAS\nincorporates an adaptive clustering mechanism that dynamically adjusts the\nnumber of actions. Comprehensive experiments on well-known datasets demonstrate\nthat FedRAS can reduce the size of communication payloads by up to 96.88%,\nwhile not sacrificing recommendation performance within various heterogeneous\nscenarios. We have open-sourced FedRAS at\nhttps://github.com/mastlab-T3S/FedRAS.", "AI": {"tldr": "FedRAS\u901a\u8fc7\u52a8\u4f5c\u5171\u4eab\u548c\u81ea\u9002\u5e94\u805a\u7c7b\u89e3\u51b3\u4e86\u8054\u90a6\u63a8\u8350\u4e2d\u7684\u901a\u4fe1\u5f00\u9500\u548c\u8bad\u7ec3\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u901a\u4fe1\u91cf\u4e14\u4e0d\u5f71\u54cd\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u63a8\u8350\u7cfb\u7edf\uff08FedRecs\uff09\u9762\u4e34\u901a\u4fe1\u5f00\u9500\u9ad8\uff08\u7531\u4e8e\u5927\u91cf\u7684\u7269\u54c1\u5d4c\u5165\uff09\u548c\u8bad\u7ec3\u6548\u7387\u4f4e\uff08\u7531\u4e8e\u5f02\u6784\u73af\u5883\u548c\u8bbe\u5907\uff09\u7684\u95ee\u9898\u3002\u73b0\u6709\u7684\u538b\u7f29\u6280\u672f\u4f1a\u5f15\u5165\u53c2\u6570\u8bef\u5dee\uff0c\u5bfc\u81f4\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedRAS\u7684\u901a\u4fe1\u9ad8\u6548\u8054\u90a6\u63a8\u8350\u6846\u67b6\uff0c\u91c7\u7528\u52a8\u4f5c\u5171\u4eab\u7b56\u7565\u5c06\u7528\u6237\u548c\u7269\u54c1\u5d4c\u5165\u7684\u68af\u5ea6\u805a\u7c7b\u6210\u6709\u9650\u6570\u91cf\u7684\u66f4\u65b0\u52a8\u4f5c\u8fdb\u884c\u901a\u4fe1\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u538b\u7f29\u5d4c\u5165\u77e9\u9635\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u805a\u7c7b\u673a\u5236\u6765\u9002\u5e94\u5f02\u6784\u8bbe\u5907\u548c\u7f51\u7edc\u73af\u5883\u3002", "result": "FedRAS\u53ef\u4ee5\u5c06\u901a\u4fe1\u8d1f\u8f7d\u51cf\u5c11\u9ad8\u8fbe96.88%\uff0c\u540c\u65f6\u5728\u5404\u79cd\u5f02\u6784\u573a\u666f\u4e0b\u4e0d\u727a\u7272\u63a8\u8350\u6027\u80fd\u3002", "conclusion": "FedRAS\u901a\u8fc7\u52a8\u4f5c\u5171\u4eab\u7b56\u7565\u51cf\u5c11\u4e86\u901a\u4fe1\u5f00\u9500\u5e76\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\uff0c\u540c\u65f6\u5728\u5f02\u6784\u73af\u5883\u4e0b\u4fdd\u6301\u4e86\u63a8\u8350\u6027\u80fd\uff0c\u901a\u4fe1\u8d1f\u8f7d\u51cf\u5c11\u9ad8\u8fbe96.88%\u3002"}}
{"id": "2507.09796", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09796", "abs": "https://arxiv.org/abs/2507.09796", "authors": ["Shengduo Liu", "Kaushik Bhattacharya", "Nadia Lapusta"], "title": "Learning a potential formulation for rate-and-state friction", "comment": null, "summary": "Empirical rate-and-state friction laws are widely used in geophysics and\nengineering to simulate interface slip. They postulate that the friction\ncoefficient depends on the local slip rate and a state variable that reflects\nthe history of slip. Depending on the parameters, rate-and-state friction can\nbe either rate-strengthening, leading to steady slip, or rate-weakening,\nleading to unsteady stick-slip behavior modeling earthquakes. Rate-and-state\nfriction does not have a potential or variational formulation, making implicit\nsolution approaches difficult and implementation numerically expensive. In this\nwork, we propose a potential formulation for the rate-and-state friction. We\nformulate the potentials as neural networks and train them so that the\nresulting behavior emulates the empirical rate-and-state friction. We show that\nthis potential formulation enables implicit time discretization leading to\nefficient numerical implementation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u901f\u7387-\u72b6\u6001\u6469\u64e6\u7684\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\u516c\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u6570\u503c\u8ba1\u7b97\u3002", "motivation": "\u7ecf\u9a8c\u901f\u7387-\u72b6\u6001\u6469\u64e6\u5b9a\u5f8b\u5728\u6a21\u62df\u754c\u9762\u6ed1\u52a8\u65f6\u88ab\u5e7f\u6cdb\u4f7f\u7528\uff0c\u4f46\u5176\u7f3a\u4e4f\u52bf\u80fd\u6216\u53d8\u5206\u516c\u5f0f\uff0c\u4f7f\u5f97\u9690\u5f0f\u6c42\u89e3\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u4e14\u6570\u503c\u5b9e\u73b0\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5c06\u52bf\u80fd\u8868\u8ff0\u4e3a\u795e\u7ecf\u7f51\u7edc\uff0c\u5e76\u8fdb\u884c\u8bad\u7ec3\u4ee5\u6a21\u62df\u7ecf\u9a8c\u901f\u7387-\u72b6\u6001\u6469\u64e6\u5b9a\u5f8b\uff0c\u4ece\u800c\u6784\u5efa\u901f\u7387-\u72b6\u6001\u6469\u64e6\u7684\u52bf\u80fd\u516c\u5f0f\u3002", "result": "\u63d0\u51fa\u7684\u52bf\u80fd\u516c\u5f0f\u80fd\u591f\u5b9e\u73b0\u9690\u5f0f\u65f6\u95f4\u79bb\u6563\u5316\uff0c\u4ece\u800c\u5728\u6570\u503c\u4e0a\u5b9e\u73b0\u9ad8\u6548\u7684\u8ba1\u7b97\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901f\u7387-\u72b6\u6001\u6469\u64e6\u7684\u52bf\u80fd\u516c\u5f0f\uff0c\u8be5\u516c\u5f0f\u5141\u8bb8\u9690\u5f0f\u65f6\u95f4\u79bb\u6563\u5316\uff0c\u4ece\u800c\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u6570\u503c\u8ba1\u7b97\u3002"}}
{"id": "2507.10436", "categories": ["cs.DS", "F.2.2"], "pdf": "https://arxiv.org/pdf/2507.10436", "abs": "https://arxiv.org/abs/2507.10436", "authors": ["Jungho Ahn", "Ian DeHaan", "Eun Jung Kim", "Euiwoong Lee"], "title": "Approximating Maximum Cut on Interval Graphs and Split Graphs beyond Goemans-Williamson", "comment": "23 pages, 5 figures, to appear in the proceedings of APPROX 2025", "summary": "We present a polynomial-time $(\\alpha_{GW} + \\varepsilon)$-approximation\nalgorithm for the Maximum Cut problem on interval graphs and split graphs,\nwhere $\\alpha_{GW} \\approx 0.878$ is the approximation guarantee of the\nGoemans-Williamson algorithm and $\\varepsilon > 10^{-34}$ is a fixed constant.\nTo attain this, we give an improved analysis of a slight modification of the\nGoemans-Williamson algorithm for graphs in which triangles can be packed into a\nconstant fraction of their edges. We then pair this analysis with structural\nresults showing that both interval graphs and split graphs either have such a\ntriangle packing or have maximum cut close to their number of edges. We also\nshow that, subject to the Small Set Expansion Hypothesis, there exists a\nconstant $c > 0$ such that there is no polyomial-time $(1 - c)$-approximation\nfor Maximum Cut on split graphs.", "AI": {"tldr": "This paper offers a near-optimal approximation algorithm for the Maximum Cut problem on interval and split graphs, achieving a $(\\alpha_{GW} + \\varepsilon)$ guarantee. It also suggests that achieving a $(1-c)$ approximation for split graphs is unlikely under the Small Set Expansion Hypothesis.", "motivation": "To develop an approximation algorithm for the Maximum Cut problem on interval and split graphs that improves upon existing guarantees, and to investigate the approximability limits of this problem on split graphs.", "method": "The algorithm is a modified version of the Goemans-Williamson algorithm, combined with structural results about triangle packing in interval and split graphs. An improved analysis of the Goemans-Williamson algorithm is provided for graphs with a constant fraction of edges involved in triangles.", "result": "A $(\\alpha_{GW} + \\varepsilon)$-approximation algorithm for Maximum Cut on interval and split graphs is presented. Additionally, it", "conclusion": "The paper presents a polynomial-time $(\\alpha_{GW} + \\varepsilon)$-approximation algorithm for Maximum Cut on interval and split graphs, with $\\alpha_{GW} \\approx 0.878$. It also shows that a $(1 - c)$-approximation for Maximum Cut on split graphs is unlikely, assuming the Small Set Expansion Hypothesis."}}
{"id": "2507.09407", "categories": ["cs.AI", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09407", "abs": "https://arxiv.org/abs/2507.09407", "authors": ["Quanyan Zhu"], "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "comment": null, "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems.", "AI": {"tldr": "We present LLM-Stackelberg games, a new model for strategic interactions using LLMs, allowing for imperfect information and adaptable strategies. We introduce new equilibrium concepts and show its applicability in cybersecurity through a spearphishing example.", "motivation": "To extend Stackelberg games by incorporating LLMs, allowing for bounded rationality, asymmetric information, and meta-cognitive adaptation in strategic interactions.", "method": "Introduce the framework of LLM-Stackelberg games, integrating LLMs into leader-follower strategic interactions. Define two equilibrium concepts: reasoning and behavioral equilibrium, and conjectural reasoning equilibrium. Illustrate with a spearphishing case study.", "result": "Demonstrate the framework's ability to capture cognitive richness and adversarial potential in LLM-mediated interactions, as shown in the spearphishing case study.", "conclusion": "LLM-Stackelberg games offer a powerful new paradigm for modeling strategic decision-making in complex, information-asymmetric environments, with applications in cybersecurity, misinformation, and recommendation systems."}}
{"id": "2507.09794", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09794", "abs": "https://arxiv.org/abs/2507.09794", "authors": ["Minjae Jeon", "Lang Tong", "Qing Zhao"], "title": "Joint Scheduling of Deferrable and Nondeferrable Demand with Colocated Stochastic Supply", "comment": null, "summary": "We address the problem of optimal joint scheduling of deferrable and\nnondeferrable demand involving colocated stochastic supply. Deferrable demand\ncan be delayed within its service deadline, whereas nondeferrable demand must\nbe scheduled immediately. Under a finite-horizon stochastic dynamic programming\nformulation, we show that the optimal scheduling policy is a ``procrastination\npolicy'' that delays scheduling as much as possible and is characterized by\nthree procrastination parameters. Exploiting the low-dimensional\nparameterization of the optimal policy, we propose a Procrastination Threshold\nReinforcement Learning algorithm. Numerical experiments based on real-world\ntest data confirm that the threshold-learning algorithm closely approximates\nthe optimal policy and outperforms standard benchmarks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u62d6\u5ef6\u7b56\u7565\u201d\u548c\u76f8\u5e94\u7684\u5b66\u4e60\u7b97\u6cd5\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u968f\u673a\u4f9b\u5e94\u7684\u60c5\u51b5\u4e0b\u4f18\u5316\u53ef\u63a8\u8fdf\u548c\u975e\u53ef\u63a8\u8fdf\u9700\u6c42\u7684\u8c03\u5ea6\uff0c\u8be5\u7b97\u6cd5\u80fd\u8fd1\u4f3c\u6700\u4f18\u7b56\u7565\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u6d89\u53ca\u540c\u5730\u968f\u673a\u4f9b\u5e94\u7684\u53ef\u63a8\u8fdf\u548c\u975e\u53ef\u63a8\u8fdf\u9700\u6c42\u7684\u8054\u5408\u6700\u4f18\u8c03\u5ea6\u95ee\u9898\u3002", "method": "\u91c7\u7528\u6709\u9650\u65f6\u95f4\u968f\u673a\u52a8\u6001\u89c4\u5212\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9608\u503c\u5b66\u4e60\u7b97\u6cd5\uff08Procrastination Threshold Reinforcement Learning\uff09\u3002", "result": "\u6700\u4f18\u8c03\u5ea6\u7b56\u7565\u4e3a\u201c\u62d6\u5ef6\u7b56\u7565\u201d\uff0c\u8be5\u7b56\u7565\u5c06\u8c03\u5ea6\u63a8\u8fdf\u5c3d\u53ef\u80fd\u957f\u7684\u65f6\u95f4\uff0c\u5e76\u7531\u4e09\u4e2a\u62d6\u5ef6\u53c2\u6570\u8868\u5f81\u3002\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u8fd1\u4f3c\u6700\u4f18\u7b56\u7565\u5e76\u4f18\u4e8e\u57fa\u51c6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u9608\u503c\u5b66\u4e60\u7b97\u6cd5\u80fd\u591f\u63a5\u8fd1\u6700\u4f18\u7b56\u7565\u5e76\u8d85\u8d8a\u6807\u51c6\u57fa\u51c6\u3002"}}
{"id": "2507.09344", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09344", "abs": "https://arxiv.org/abs/2507.09344", "authors": ["Daniel Engelsman", "Itzik Klein"], "title": "C-ZUPT: Stationarity-Aided Aerial Hovering", "comment": "14 Pages, 16 Figures, 9 Tables", "summary": "Autonomous systems across diverse domains have underscored the need for\ndrift-resilient state estimation. Although satellite-based positioning and\ncameras are widely used, they often suffer from limited availability in many\nenvironments. As a result, positioning must rely solely on inertial sensors,\nleading to rapid accuracy degradation over time due to sensor biases and noise.\nTo counteract this, alternative update sources-referred to as information\naiding-serve as anchors of certainty. Among these, the zero-velocity update\n(ZUPT) is particularly effective in providing accurate corrections during\nstationary intervals, though it is restricted to surface-bound platforms. This\nwork introduces a controlled ZUPT (C-ZUPT) approach for aerial navigation and\ncontrol, independent of surface contact. By defining an uncertainty threshold,\nC-ZUPT identifies quasi-static equilibria to deliver precise velocity updates\nto the estimation filter. Extensive validation confirms that these\nopportunistic, high-quality updates significantly reduce inertial drift and\ncontrol effort. As a result, C-ZUPT mitigates filter divergence and enhances\nnavigation stability, enabling more energy-efficient hovering and substantially\nextending sustained flight-key advantages for resource-constrained aerial\nsystems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a C-ZUPT \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u5bfc\u822a\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u5730\u9762\u63a5\u89e6\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u901a\u8fc7\u8bc6\u522b\u51c6\u9759\u6001\u5e73\u8861\u70b9\u6765\u63d0\u4f9b\u7cbe\u786e\u7684\u901f\u5ea6\u66f4\u65b0\uff0c\u4ece\u800c\u51cf\u5c11\u6f02\u79fb\u3001\u63d0\u9ad8\u7a33\u5b9a\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7531\u4e8e\u4f20\u611f\u5668\u504f\u5dee\u548c\u566a\u58f0\uff0c\u4ec5\u4f9d\u9760\u60ef\u6027\u4f20\u611f\u5668\u8fdb\u884c\u5b9a\u4f4d\u4f1a\u5bfc\u81f4\u7cbe\u5ea6\u968f\u65f6\u95f4\u5feb\u901f\u4e0b\u964d\u3002\u66ff\u4ee3\u7684\u66f4\u65b0\u6e90\uff08\u4f8b\u5982\u96f6\u901f\u5ea6\u66f4\u65b0\uff08ZUPT\uff09\uff09\u53ef\u4ee5\u63d0\u4f9b\u7cbe\u786e\u7684\u6821\u6b63\uff0c\u4f46\u73b0\u6709\u7684 ZUPT \u65b9\u6cd5\u4ec5\u9650\u4e8e\u5730\u9762\u5e73\u53f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u63a7\u96f6\u901f\u5ea6\u66f4\u65b0\uff08C-ZUPT\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u822a\u7a7a\u5bfc\u822a\u548c\u63a7\u5236\uff0c\u4e0d\u4f9d\u8d56\u4e8e\u4e0e\u5730\u9762\u7684\u63a5\u89e6\u3002\u901a\u8fc7\u5b9a\u4e49\u4e0d\u786e\u5b9a\u6027\u9608\u503c\uff0cC-ZUPT \u8bc6\u522b\u51c6\u9759\u6001\u5e73\u8861\u70b9\uff0c\u4e3a\u4f30\u8ba1\u6ee4\u6ce2\u5668\u63d0\u4f9b\u7cbe\u786e\u7684\u901f\u5ea6\u66f4\u65b0\u3002", "result": "\u901a\u8fc7\u673a\u4f1a\u6027\u3001\u9ad8\u8d28\u91cf\u7684\u66f4\u65b0\u663e\u8457\u51cf\u5c11\u4e86\u60ef\u6027\u6f02\u79fb\u548c\u63a7\u5236\u5de5\u4f5c\u91cf\uff0c\u4ece\u800c\u7f13\u89e3\u4e86\u6ee4\u6ce2\u5668\u53d1\u6563\u548c\u589e\u5f3a\u4e86\u5bfc\u822a\u7a33\u5b9a\u6027\u3002", "conclusion": "C-ZUPT \u7f13\u89e3\u4e86\u6ee4\u6ce2\u5668\u53d1\u6563\u5e76\u589e\u5f3a\u4e86\u5bfc\u822a\u7a33\u5b9a\u6027\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u8282\u80fd\u7684\u60ac\u505c\u5e76\u663e\u8457\u5ef6\u957f\u4e86\u6301\u7eed\u98de\u884c\u65f6\u95f4\uff0c\u8fd9\u5bf9\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u822a\u7a7a\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.09082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09082", "abs": "https://arxiv.org/abs/2507.09082", "authors": ["Seungwoo Kim", "Khai Loong Aw", "Klemen Kotar", "Cristobal Eyzaguirre", "Wanhee Lee", "Yunong Liu", "Jared Watrous", "Stefan Stojanov", "Juan Carlos Niebles", "Jiajun Wu", "Daniel L. K. Yamins"], "title": "Taming generative video models for zero-shot optical flow extraction", "comment": "Project webpage: https://neuroailab.github.io/projects/kl_tracing", "summary": "Extracting optical flow from videos remains a core computer vision problem.\nMotivated by the success of large general-purpose models, we ask whether frozen\nself-supervised video models trained only for future frame prediction can be\nprompted, without fine-tuning, to output flow. Prior work reading out depth or\nillumination from video generators required fine-tuning, which is impractical\nfor flow where labels are scarce and synthetic datasets suffer from a\nsim-to-real gap. Inspired by the Counterfactual World Model (CWM) paradigm,\nwhich can obtain point-wise correspondences by injecting a small tracer\nperturbation into a next-frame predictor and tracking its propagation, we\nextend this idea to generative video models. We explore several popular\narchitectures and find that successful zero-shot flow extraction in this manner\nis aided by three model properties: (1) distributional prediction of future\nframes (avoiding blurry or noisy outputs); (2) factorized latents that treat\neach spatio-temporal patch independently; and (3) random-access decoding that\ncan condition on any subset of future pixels. These properties are uniquely\npresent in the recent Local Random Access Sequence (LRAS) architecture.\nBuilding on LRAS, we propose KL-tracing: a novel test-time procedure that\ninjects a localized perturbation into the first frame, rolls out the model one\nstep, and computes the Kullback-Leibler divergence between perturbed and\nunperturbed predictive distributions. Without any flow-specific fine-tuning,\nour method outperforms state-of-the-art models on real-world TAP-Vid DAVIS\ndataset (16.6% relative improvement for endpoint error) and synthetic TAP-Vid\nKubric (4.7% relative improvement). Our results indicate that counterfactual\nprompting of controllable generative video models is a scalable and effective\nalternative to supervised or photometric-loss approaches for high-quality flow.", "AI": {"tldr": "\u901a\u8fc7KL-tracing\uff0c\u4e00\u79cd\u65b0\u7684\u6d4b\u8bd5\u65f6\u7a0b\u5e8f\uff0c\u5728\u4e0d\u8fdb\u884c\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u4ee5\u4ece\u751f\u6210\u89c6\u9891\u6a21\u578b\u4e2d\u63d0\u53d6\u5149\u6d41\uff0c\u8fd9\u662f\u4e00\u79cd\u6bd4\u76d1\u7763\u6216\u5149\u5ea6\u635f\u5931\u65b9\u6cd5\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u4e0d\u7ecf\u5fae\u8c03\u5373\u53ef\u63d0\u53d6\u5149\u6d41\u7684\u53ef\u80fd\u6027\uff0c\u7279\u522b\u662f\u7531\u4ec5\u4e3a\u9884\u6d4b\u672a\u6765\u5e27\u800c\u8bad\u7ec3\u7684\u51bb\u7ed3\u7684\u3001\u901a\u7528\u7684\u81ea\u76d1\u7763\u89c6\u9891\u6a21\u578b\u3002\u8fd9\u662f\u56e0\u4e3a\u5148\u524d\u7684\u7ec6\u8bfb\u65b9\u6cd5\u9700\u8981\u5fae\u8c03\uff0c\u8fd9\u5bf9\u4e8e\u6807\u7b7e\u7a00\u7f3a\u4e14\u6a21\u62df\u5230\u771f\u5b9e\u5dee\u8ddd\u963b\u788d\u5408\u6210\u6570\u636e\u96c6\u7684\u5149\u6d41\u6765\u8bf4\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002", "method": "\u53d7\u9006\u4e16\u754c\u6a21\u578b\uff08CWM\uff09\u8303\u4f8b\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKL-tracing\u7684\u6d4b\u8bd5\u65f6\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u5c40\u90e8\u6270\u52a8\u6ce8\u5165\u7b2c\u4e00\u5e27\uff0c\u7136\u540e\u63a8\u51fa\u6a21\u578b\u4e00\u6b65\u5e76\u8ba1\u7b97\u53d7\u6270\u52a8\u548c\u672a\u53d7\u6270\u52a8\u9884\u6d4b\u5206\u5e03\u4e4b\u95f4\u7684KL\u6563\u5ea6\u6765\u63d0\u53d6\u5149\u6d41\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5206\u5e03\u9884\u6d4b\u3001\u56e0\u5b50\u5316\u6f5c\u5728\u7a7a\u95f4\u548c\u968f\u673a\u8bbf\u95ee\u89e3\u7801\u7b49\u6a21\u578b\u7279\u6027\u6709\u52a9\u4e8e\u6b64\u8fc7\u7a0b\u3002LRAS\u67b6\u6784\u6ee1\u8db3\u8fd9\u4e9b\u8981\u6c42\uff0c\u5e76\u88ab\u7528\u4e8eKL-tracing\u7684\u5b9e\u73b0\u3002", "result": "\u5728\u4e0d\u8fdb\u884c\u4efb\u4f55\u7279\u5b9a\u4e8e\u5149\u6d41\u7684\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u7684KL-tracing\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684TAP-Vid DAVIS\u6570\u636e\u96c6\uff08\u7aef\u70b9\u8bef\u5dee\u76f8\u5bf9\u63d0\u9ad8\u4e8616.6%\uff09\u548c\u5408\u6210TAP-Vid Kubric\u6570\u636e\u96c6\uff08\u76f8\u5bf9\u63d0\u9ad8\u4e864.7%\uff09\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u53ef\u63a7\u751f\u6210\u89c6\u9891\u6a21\u578b\u901a\u8fc7\u53cd\u4e8b\u5b9e\u63d0\u793a\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6cd5\uff0c\u53ef\u7528\u4e8e\u9ad8\u8d28\u91cf\u5149\u6d41\u4f30\u8ba1\uff0c\u4f18\u4e8e\u76d1\u7763\u6216\u5149\u5ea6\u635f\u5931\u65b9\u6cd5\u3002"}}
{"id": "2507.10430", "categories": ["cs.DC", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10430", "abs": "https://arxiv.org/abs/2507.10430", "authors": ["Ji Liu", "Beichen Ma", "Yang Zhou", "Jingbo Zhou", "Ruoming Jin", "Dejing Dou", "Huaiyu Dai", "Haixun Wang", "Patrick Valduriez"], "title": "Efficient Federated Learning with Heterogeneous Data and Adaptive Dropout", "comment": "29 pages, to appear in ACM Transactions on Knowledge Discovery from\n  Data (TKDD)", "summary": "Federated Learning (FL) is a promising distributed machine learning approach\nthat enables collaborative training of a global model using multiple edge\ndevices. The data distributed among the edge devices is highly heterogeneous.\nThus, FL faces the challenge of data distribution and heterogeneity, where\nnon-Independent and Identically Distributed (non-IID) data across edge devices\nmay yield in significant accuracy drop. Furthermore, the limited computation\nand communication capabilities of edge devices increase the likelihood of\nstragglers, thus leading to slow model convergence. In this paper, we propose\nthe FedDHAD FL framework, which comes with two novel methods: Dynamic\nHeterogeneous model aggregation (FedDH) and Adaptive Dropout (FedAD). FedDH\ndynamically adjusts the weights of each local model within the model\naggregation process based on the non-IID degree of heterogeneous data to deal\nwith the statistical data heterogeneity. FedAD performs neuron-adaptive\noperations in response to heterogeneous devices to improve accuracy while\nachieving superb efficiency. The combination of these two methods makes FedDHAD\nsignificantly outperform state-of-the-art solutions in terms of accuracy (up to\n6.7% higher), efficiency (up to 2.02 times faster), and computation cost (up to\n15.0% smaller).", "AI": {"tldr": "\u63d0\u51faFedDHAD\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5f02\u6784\u6a21\u578b\u805a\u5408\uff08FedDH\uff09\u548c\u81ea\u9002\u5e94\u4e22\u5f03\uff08FedAD\uff09\u4e24\u79cd\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u6570\u636e\u5f02\u6784\u6027\u548c\u8bbe\u5907\u5f02\u6784\u6027\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\u548c\u8bad\u7ec3\u6548\u7387\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u6a21\u578b\u8bad\u7ec3\u4e2d\u9762\u4e34\u6570\u636e\u5206\u5e03\u548c\u5f02\u6784\u6027\u6311\u6218\uff0c\u7279\u522b\u662f\u8fb9\u7f18\u8bbe\u5907\u4e0a\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u6570\u636e\u53ef\u80fd\u5bfc\u81f4\u663e\u8457\u7684\u51c6\u786e\u7387\u4e0b\u964d\uff1b\u540c\u65f6\uff0c\u8fb9\u7f18\u8bbe\u5907\u6709\u9650\u7684\u8ba1\u7b97\u548c\u901a\u4fe1\u80fd\u529b\u589e\u52a0\u4e86\u6389\u961f\u8005\u7684\u53ef\u80fd\u6027\uff0c\u5bfc\u81f4\u6a21\u578b\u6536\u655b\u7f13\u6162\u3002", "method": "FedDH\u52a8\u6001\u805a\u5408\u65b9\u6cd5\u57fa\u4e8e\u975e\u72ec\u7acb\u540c\u5206\u5e03\u6570\u636e\u7684\u7a0b\u5ea6\u6765\u8c03\u6574\u805a\u5408\u8fc7\u7a0b\u4e2d\u6bcf\u4e2a\u672c\u5730\u6a21\u578b\u7684\u6743\u91cd\uff0c\u4ee5\u5e94\u5bf9\u7edf\u8ba1\u6570\u636e\u5f02\u6784\u6027\uff1bFedAD\u901a\u8fc7\u795e\u7ecf\u5143\u81ea\u9002\u5e94\u64cd\u4f5c\u54cd\u5e94\u5f02\u6784\u8bbe\u5907\uff0c\u4ee5\u63d0\u9ad8\u51c6\u786e\u6027\u5e76\u5b9e\u73b0\u51fa\u8272\u7684\u6548\u7387\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedDHAD\u6846\u67b6\u5728\u51c6\u786e\u6027\uff08\u9ad8\u51fa6.7%\uff09\u3001\u6548\u7387\uff08\u5feb2.02\u500d\uff09\u548c\u8ba1\u7b97\u6210\u672c\uff08\u5c0f15.0%\uff09\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "FedDHAD\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5f02\u6784\u6a21\u578b\u805a\u5408\uff08FedDH\uff09\u548c\u81ea\u9002\u5e94\u4e22\u5f03\uff08FedAD\uff09\u4e24\u79cd\u65b0\u9896\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u51c6\u786e\u6027\uff08\u9ad8\u51fa6.7%\uff09\u3001\u6548\u7387\uff08\u5feb2.02\u500d\uff09\u548c\u8ba1\u7b97\u6210\u672c\uff08\u5c0f15.0%\uff09\u65b9\u9762\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2507.09639", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.09639", "abs": "https://arxiv.org/abs/2507.09639", "authors": ["Meng-Qi Cheng", "Wei-Dong Luo", "Hong Sun"], "title": "On-the-fly machine learning-augmented constrained AIMD to design new routes from glassy carbon to quenchable amorphous diamond with low pressure and temperature", "comment": "16 pages, 10 figures, 2 tables, and 6 pages of supplementary\n  materials", "summary": "Recent advances in machine learning have enabled large-scale atomic\nsimulations with first-principles accuracy, allowing precise modeling of\ndisordered materials such as glassy carbon (GC). However, conventional ab\ninitio molecular dynamics (AIMD) cannot effectively capture anisotropic stress\neffects, which are believed to play a key role in the transformation of GC into\namorphous diamond under extreme conditions. In this work, we present an\non-the-fly machine learning-augmented constrained AIMD (ML-augmented CAIMD)\napproach by modifying VASP 6.3.2. Our simulations not only reproduce major\nexperimental features of GC but also provide restrictive synthesis conditions\nand microscopic insights. We show that GC exhibits unexpectedly high\nplasticity, with its compressive and shear strengths enhanced by large strains.\nUnder pressure, increasing annealing temperature promotes the formation of\nquenchable amorphous diamond via enhanced sp3 preservation, but this trend\nreverses above 2900 K due to thermal graphitization. Under non-hydrostatic\ncompression, GC transforms into a superhard structure sustaining large stress\ndifferences, which sharply increase when confining pressure exceeds 40 GPa.\nFinally, severe rotational shear at 30 GPa induces sp3 fractions up to 80\npercent at 300 to 1000 K. A hardened amorphous carbon retaining 64 percent sp3\ncontent is achieved by decompression at 300 K, marking the lowest\npressure-temperature route ever predicted. Our ML-augmented CAIMD provides a\ngeneral framework for modeling structural transformations in disordered\nmaterials under anisotropic stresses.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u6a21\u62df\u65b9\u6cd5\uff08ML-augmented CAIMD\uff09\uff0c\u6210\u529f\u6a21\u62df\u4e86\u73bb\u7483\u78b3\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u8f6c\u53d8\uff0c\u53d1\u73b0\u4e86\u5176\u72ec\u7279\u7684\u529b\u5b66\u6027\u80fd\u548c\u8f6c\u53d8\u89c4\u5f8b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u6761\u8fc4\u4eca\u4e3a\u6b62\u538b\u529b-\u6e29\u5ea6\u6700\u4f4e\u7684\u5408\u6210\u786c\u8d28\u975e\u6676\u78b3\u7684\u8def\u7ebf\u3002", "motivation": "\u4f20\u7edf\u7684AIMD\u65b9\u6cd5\u65e0\u6cd5\u6709\u6548\u6355\u6349\u5404\u5411\u5f02\u6027\u5e94\u529b\u6548\u5e94\uff0c\u800c\u8fd9\u79cd\u6548\u5e94\u88ab\u8ba4\u4e3a\u5728\u73bb\u7483\u78b3\u5411\u975e\u6676\u91d1\u521a\u77f3\u7684\u8f6c\u53d8\u4e2d\u8d77\u5173\u952e\u4f5c\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aML\u589e\u5f3aCAIMD\uff08on-the-fly machine learning-augmented constrained AIMD\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4fee\u6539VASP 6.3.2\u5b9e\u73b0\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u5404\u5411\u5f02\u6027\u5e94\u529b\u6548\u5e94\uff0c\u7528\u4e8e\u6a21\u62df\u73bb\u7483\u78b3\u7b49\u65e0\u5e8f\u6750\u6599\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u4e0d\u4ec5\u91cd\u73b0\u4e86\u73bb\u7483\u78b3\u7684\u4e3b\u8981\u5b9e\u9a8c\u7279\u5f81\uff0c\u8fd8\u63d0\u4f9b\u4e86\u9650\u5236\u6027\u7684\u5408\u6210\u6761\u4ef6\u548c\u5fae\u89c2\u89c1\u89e3\u3002\u7814\u7a76\u53d1\u73b0\u73bb\u7483\u78b3\u5177\u6709\u9ad8\u5851\u6027\uff0c\u5176\u6297\u538b\u548c\u6297\u526a\u5f3a\u5ea6\u968f\u5927\u5e94\u53d8\u800c\u589e\u5f3a\u3002\u5728\u538b\u529b\u4e0b\uff0c\u63d0\u9ad8\u9000\u706b\u6e29\u5ea6\u6709\u5229\u4e8e\u5f62\u6210\u975e\u6676\u91d1\u521a\u77f3\uff0c\u4f46\u8d85\u8fc72900 K\u540e\uff0c\u70ed\u77f3\u58a8\u5316\u6548\u5e94\u4f1a\u9006\u8f6c\u6b64\u8d8b\u52bf\u3002\u5728\u975e\u9759\u6c34\u538b\u529b\u4e0b\uff0c\u73bb\u7483\u78b3\u8f6c\u53d8\u4e3a\u8d85\u786c\u7ed3\u6784\uff0c\u5176\u5e94\u529b\u5dee\u968f\u7ea6\u675f\u538b\u529b\u8d85\u8fc740 GPa\u800c\u6025\u5267\u589e\u52a0\u3002\u572830 GPa\u548c300-1000 K\u4e0b\u8fdb\u884c\u4e25\u91cd\u7684\u65cb\u8f6c\u526a\u5207\u53ef\u4f7fsp3\u542b\u91cf\u9ad8\u8fbe80%\u3002\u5728300 K\u4e0b\u51cf\u538b\u53ef\u83b7\u5f97\u4fdd\u630164% sp3\u542b\u91cf\u7684\u786c\u5316\u975e\u6676\u78b3\uff0c\u8fd9\u662f\u9884\u6d4b\u7684\u6700\u4f4e\u538b\u529b-\u6e29\u5ea6\u5408\u6210\u8def\u7ebf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684ML\u589e\u5f3aCAIMD\u65b9\u6cd5\u6210\u529f\u6a21\u62df\u4e86\u73bb\u7483\u78b3\u5728\u6781\u7aef\u6761\u4ef6\u4e0b\u7684\u8f6c\u53d8\uff0c\u5e76\u63ed\u793a\u4e86\u5176\u5851\u6027\u3001\u6297\u538b\u548c\u6297\u526a\u5f3a\u5ea6\u968f\u5927\u5e94\u53d8\u800c\u589e\u5f3a\u7684\u7279\u6027\u3002\u7814\u7a76\u8fd8\u786e\u5b9a\u4e86\u73bb\u7483\u78b3\u8f6c\u53d8\u4e3a\u975e\u6676\u91d1\u521a\u77f3\u7684\u6700\u4f73\u5408\u6210\u6761\u4ef6\uff0c\u5e76\u53d1\u73b0\u9ad8\u6e29\u548c\u9ad8\u538b\u4e0b\u7684\u526a\u5207\u5e94\u529b\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8sp3\u952e\u7684\u6bd4\u4f8b\uff0c\u6700\u7ec8\u5728\u51cf\u538b\u540e\u5f97\u5230\u4e00\u79cd\u786c\u8d28\u975e\u6676\u78b3\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u538b\u529b-\u6e29\u5ea6\u6700\u4f4e\u7684\u5408\u6210\u8def\u7ebf\u3002\u8be5\u65b9\u6cd5\u4e3a\u6a21\u62df\u65e0\u5e8f\u6750\u6599\u5728\u5404\u5411\u5f02\u6027\u5e94\u529b\u4e0b\u7684\u7ed3\u6784\u8f6c\u53d8\u63d0\u4f9b\u4e86\u901a\u7528\u6846\u67b6\u3002"}}
{"id": "2507.09261", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09261", "abs": "https://arxiv.org/abs/2507.09261", "authors": ["Hai Wang"], "title": "Order-preserving condition for coherence measures of projective measurements with One Example", "comment": null, "summary": "Superposition is an essential feature of quantum mechanics. From the\nSchrodinger's cat to quantum algorithms such as Deutsch-Jorsza algorithm,\nquantum superposition plays an important role. It is one fundamental and\ncrucial question how to quantify superposition. Until now, the framework of\ncoherence has been well established as one typical instance of quantum resource\ntheories. And the concept of coherence has been generalized into linearly\nindependent basis, projective measurements and POVMs. In this work, we will\nfocus on coherence measures for projective measurements or orthogonal\nsubspaces. One new condition, order-preserving condition, is proposed for such\nmeasures. This condition is rooted in the mathematical structure of Hilbert\nspaces' orthogonal decomposition. And by generalizing the 1/2-affinity of\ncoherence into subspace cases, we verify that this generalized coherence\nmeasure satisfies the order-preserving condition. And it also satisfies other\nreasonable conditions to be a good coherence measure. As the partial order\nrelationship exists for not only projective measurements, but also POVMs, it's\nnatural to study the order-preserving condition in POVM cases, which will be\nthe last part of this work.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u53e0\u52a0\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91cf\u5316\u91cf\u5b50\u53e0\u52a0\u662f\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u4e14\u5173\u952e\u7684\u95ee\u9898\uff0c\u76f8\u5e72\u6027\u7406\u8bba\u662f\u5176\u4e2d\u4e00\u4e2a\u5178\u578b\u7684\u4f8b\u5b50\u3002", "method": "\u63d0\u51fa\u201corder-preserving condition\u201d\u4f5c\u4e3a\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u901a\u8fc7\u63a8\u5e7f\u76f8\u5e72\u6027\u76841/2\u4eb2\u548c\u6027\u6765\u9a8c\u8bc1\u8be5\u5ea6\u91cf\u6ee1\u8db3\u8be5\u6761\u4ef6\u53ca\u5176\u4ed6\u5408\u7406\u6761\u4ef6\u3002", "result": "\u9a8c\u8bc1\u4e86\u63a8\u5e7f\u540e\u7684\u76f8\u5e72\u6027\u5ea6\u91cf\u6ee1\u8db3\u201corder-preserving condition\u201d\u53ca\u5176\u4ed6\u5408\u7406\u6761\u4ef6\u3002", "conclusion": "\u201corder-preserving condition\u201d\u88ab\u63d0\u51fa\u4f5c\u4e3a\u91cf\u5b50\u53e0\u52a0\u7684\u5ea6\u91cf\u6807\u51c6\uff0c\u5e76\u4e14\u901a\u8fc7\u63a8\u5e7f\u76f8\u5e72\u6027\u76841/2\u4eb2\u548c\u6027\u6765\u9a8c\u8bc1\u5176\u4f5c\u4e3a\u826f\u597d\u76f8\u5e72\u6027\u5ea6\u91cf\u7684\u5408\u7406\u6027\u3002\u6700\u540e\uff0c\u63a2\u8ba8\u4e86\u5728POVM\u60c5\u51b5\u4e0b\u8be5\u6761\u4ef6\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09104", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09104", "abs": "https://arxiv.org/abs/2507.09104", "authors": ["Taolin Zhang", "Maosong Cao", "Alexander Lam", "Songyang Zhang", "Kai Chen"], "title": "CompassJudger-2: Towards Generalist Judge Model via Verifiable Rewards", "comment": null, "summary": "Recently, the role of LLM-as-judge in evaluating large language models has\ngained prominence. However, current judge models suffer from narrow\nspecialization and limited robustness, undermining their capacity for\ncomprehensive evaluations. In this work, we present CompassJudger-2, a novel\ngeneralist judge model that overcomes these limitations via a task-driven,\nmulti-domain data curation strategy. Central to our approach is supervising\njudgment tasks with verifiable rewards, guiding intrinsic critical reasoning\nthrough rejection sampling to foster robust, generalizable judgment\ncapabilities. We introduce a refined learning objective with margin policy\ngradient loss to enhance performance. Empirically, CompassJudger-2 achieves\nsuperior results across multiple judge and reward benchmarks, and our 7B model\ndemonstrates competitive judgment accuracy with significantly larger models\nlike DeepSeek-V3 and Qwen3-235B-A22B. Additionally, we propose JudgerBenchV2, a\ncomprehensive benchmark evaluating cross-domain judgment accuracy and rank\nconsistency to standardize judge model evaluation. These contributions advance\nrobust, scalable LLM judgment and establish new performance and evaluation\nstandards.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCompassJudger-2\u7684\u65b0\u578b\u901a\u7528\u8bc4\u5224\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u9886\u57df\u6570\u636e\u7b56\u200b\u200b\u7565\u548c\u5956\u52b1\u76d1\u7763\u514b\u670d\u4e86\u73b0\u6709\u8bc4\u5224\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002\u8be5\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5176\u8f83\u5c0f\u89c4\u6a21\u7684\u6a21\u578b\u4e5f\u80fd\u4e0e\u66f4\u5927\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86JudgerBenchV2\u57fa\u51c6\u6d4b\u8bd5\u4ee5\u6807\u51c6\u5316\u8bc4\u5224\u6a21\u578b\u7684\u8bc4\u4f30\u3002", "motivation": "\u5f53\u524d\u7684\u8bc4\u5224\u6a21\u578b\u5b58\u5728\u4e13\u4e1a\u5316\u7a0b\u5ea6\u7a84\u548c\u9c81\u68d2\u6027\u6709\u9650\u7684\u95ee\u9898\uff0c\u8fd9\u524a\u5f31\u4e86\u5b83\u4eec\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u7684\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4efb\u52a1\u9a71\u52a8\u7684\u591a\u9886\u57df\u6570\u636e\u7b56\u200b\u200b\u7565\u6765\u8bad\u7ec3\u4e00\u4e2a\u540d\u4e3aCompassJudger-2\u7684\u901a\u7528\u8bc4\u5224\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u53ef\u9a8c\u8bc1\u7684\u5956\u52b1\u6765\u76d1\u7763\u8bc4\u5224\u4efb\u52a1\uff0c\u901a\u8fc7\u62d2\u7edd\u91c7\u6837\u6765\u6307\u5bfc\u5185\u5728\u6279\u5224\u6027\u63a8\u7406\uff0c\u4ee5\u57f9\u517b\u9c81\u68d2\u3001\u53ef\u6cdb\u5316\u7684\u8bc4\u5224\u80fd\u529b\u3002\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684\u5b66\u4e60\u76ee\u6807\uff0c\u4f7f\u7528\u8fb9\u9645\u7b56\u7565\u68af\u5ea6\u635f\u5931\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "result": "CompassJudger-2\u5728\u591a\u4e2a\u8bc4\u5224\u548c\u5956\u52b1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6210\u679c\uff0c\u5e76\u4e14\u51767B\u6a21\u578b\u5728\u8bc4\u5224\u51c6\u786e\u6027\u65b9\u9762\u80fd\u591f\u4e0eDeepSeek-V3\u548cQwen3-235B-A22B\u7b49\u89c4\u6a21\u5927\u5f97\u591a\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86JudgerBenchV2\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u57df\u8bc4\u5224\u51c6\u786e\u6027\u548c\u6392\u540d\u4e00\u81f4\u6027\uff0c\u4e3a\u8bc4\u5224\u6a21\u578b\u8bc4\u4f30\u5efa\u7acb\u4e86\u6807\u51c6\u3002", "conclusion": "CompassJudger-2\u5728\u591a\u4e2a\u8bc4\u5224\u548c\u5956\u52b1\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6210\u679c\uff0c\u5e76\u4e14\u51767B\u6a21\u578b\u5728\u8bc4\u5224\u51c6\u786e\u6027\u65b9\u9762\u80fd\u591f\u4e0eDeepSeek-V3\u548cQwen3-235B-A22B\u7b49\u89c4\u6a21\u5927\u5f97\u591a\u7684\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86JudgerBenchV2\uff0c\u4e00\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u8de8\u57df\u8bc4\u5224\u51c6\u786e\u6027\u548c\u6392\u540d\u4e00\u81f4\u6027\uff0c\u4e3a\u8bc4\u5224\u6a21\u578b\u8bc4\u4f30\u5efa\u7acb\u4e86\u6807\u51c6\u3002\u8fd9\u4e9b\u8d21\u732e\u63a8\u52a8\u4e86\u9c81\u68d2\u3001\u53ef\u6269\u5c55\u7684LLM\u8bc4\u5224\u6280\u672f\u7684\u53d1\u5c55\uff0c\u5e76\u786e\u7acb\u4e86\u65b0\u7684\u6027\u80fd\u548c\u8bc4\u4f30\u6807\u51c6\u3002"}}
{"id": "2507.09535", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09535", "abs": "https://arxiv.org/abs/2507.09535", "authors": ["Chaoran Li", "Xingguo Xu", "Siyuan Mu"], "title": "Reframing SAR Target Recognition as Visual Reasoning: A Chain-of-Thought Dataset with Multimodal LLMs", "comment": null, "summary": "In the context of Synthetic Aperture Radar (SAR) image recognition,\ntraditional methods often struggle with the intrinsic limitations of SAR data,\nsuch as weak texture, high noise, and ambiguous object boundaries. This work\nexplores a novel perspective by reformulating SAR target recognition as a\nmultimodal reasoning task. We leverage multimodal large language models\n(MLLMs), specifically GPT-4o, to perform target classification based on SAR\nimagery, guided by candidate categories and enhanced with Chain-of-Thought\n(CoT) reasoning. A new dataset is constructed based on the FAIR-CSAR benchmark,\ncomprising raw SAR images, structured target annotations, candidate label sets,\nand GPT-generated CoT reasoning chains. Experimental results show that the\nMLLMs are capable of generating logically coherent and interpretable inferences\nin most scenarios. Our analysis highlights both the strengths and current\nlimitations of MLLMs in interpreting SAR imagery, and we provide detailed\ninsights into model behavior through failure case analysis. This work\ndemonstrates the feasibility of incorporating MLLMs into SAR analysis pipelines\nand establishes a foundation for future research in SAR-oriented visual\nreasoning.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u521b\u6027\u5730\u5c06 SAR \u76ee\u6807\u8bc6\u522b\u89c6\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\uff0c\u5229\u7528 GPT-4o \u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u5904\u7406 SAR \u56fe\u50cf\u3002\u6784\u5efa\u7684\u65b0\u6570\u636e\u96c6\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86 MLLMs \u5728\u6b64\u9886\u57df\u7684\u6f5c\u529b\u53ca\u5176\u5728 SAR \u5206\u6790\u4e2d\u7684\u5e94\u7528\u524d\u666f\uff0c\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5176\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf SAR \u56fe\u50cf\u8bc6\u522b\u65b9\u6cd5\u5728\u5904\u7406 SAR \u6570\u636e\u56fa\u6709\u5c40\u9650\u6027\uff08\u5982\u7eb9\u7406\u5f31\u3001\u566a\u58f0\u9ad8\u3001\u76ee\u6807\u8fb9\u754c\u6a21\u7cca\uff09\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u5c06 SAR \u76ee\u6807\u8bc6\u522b\u91cd\u65b0\u8868\u8ff0\u4e3a\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5229\u7528 GPT-4o \u7b49 MLLMs\uff0c\u7ed3\u5408\u5019\u9009\u7c7b\u522b\u548c\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u6765\u6267\u884c SAR \u56fe\u50cf\u7684\u76ee\u6807\u5206\u7c7b\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u57fa\u4e8e FAIR-CSAR \u57fa\u51c6\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b\u539f\u59cb SAR \u56fe\u50cf\u3001\u7ed3\u6784\u5316\u76ee\u6807\u6ce8\u91ca\u3001\u5019\u9009\u6807\u7b7e\u96c6\u548c GPT \u751f\u6210\u7684 CoT \u63a8\u7406\u94fe\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMLLMs \u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u80fd\u591f\u751f\u6210\u903b\u8f91\u8fde\u8d2f\u4e14\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002\u5206\u6790\u7a81\u51fa\u4e86 MLLMs \u5728 SAR \u56fe\u50cf\u89e3\u91ca\u65b9\u9762\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5931\u8d25\u6848\u4f8b\u5206\u6790\u63d0\u4f9b\u4e86\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u8be6\u7ec6\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u5c06\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u96c6\u6210\u5230 SAR \u5206\u6790\u6d41\u7a0b\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765 SAR \u5bfc\u5411\u7684\u89c6\u89c9\u63a8\u7406\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.08870", "categories": ["cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08870", "abs": "https://arxiv.org/abs/2507.08870", "authors": ["Yaowenqi Liu", "BingXu Meng", "Rui Pan", "Jerry Huang", "Tong Zhang"], "title": "GUIDE: Towards Scalable Advising for Research Ideas", "comment": null, "summary": "The field of AI research is advancing at an unprecedented pace, enabling\nautomated hypothesis generation and experimental design across diverse domains\nsuch as biology, mathematics, and artificial intelligence. Despite these\nadvancements, there remains a significant gap in the availability of scalable\nadvising systems capable of providing high-quality, well-reasoned feedback to\nrefine proposed hypotheses and experimental designs. To address this challenge,\nwe explore key factors that underlie the development of robust advising\nsystems, including model size, context length, confidence estimation, and\nstructured reasoning processes. Our findings reveal that a relatively small\nmodel, when equipped with a well-compressed literature database and a\nstructured reasoning framework, can outperform powerful general-purpose\nlanguage models such as Deepseek-R1 in terms of acceptance rates for\nself-ranked top-30% submissions to ICLR 2025. Moreover, when limited to\nhigh-confidence predictions, our system achieves an acceptance rate exceeding\n90% on the ICLR 2025 test set, underscoring its potential to significantly\nenhance the quality and efficiency of hypothesis generation and experimental\ndesign. The code is released at\nhttps://github.com/HowardLiu0830/GUIDE-Research-Idea-Evaluation.", "AI": {"tldr": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u538b\u7f29\u6587\u732e\u6570\u636e\u5e93\u548c\u5f15\u5165\u7ed3\u6784\u5316\u63a8\u7406\uff0c\u5c0f\u578bAI\u6a21\u578b\u5728\u8bba\u6587\u6295\u7a3f\u7684\u53cd\u9988\u548c\u4f18\u5316\u65b9\u9762\u53ef\u4ee5\u4f18\u4e8e\u5927\u578b\u6a21\u578b\uff0c\u5b9e\u73b0\u9ad8\u5f55\u7528\u7387\u3002", "motivation": "\u5f53\u524d\u7684AI\u7814\u7a76\u867d\u7136\u80fd\u591f\u8fdb\u884c\u81ea\u52a8\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u8bbe\u8ba1\uff0c\u4f46\u5728\u63d0\u4f9b\u9ad8\u8d28\u91cf\u3001\u6709\u5145\u5206\u7406\u7531\u7684\u53cd\u9988\u4ee5\u4f18\u5316\u5047\u8bbe\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u65b9\u9762\uff0c\u4ecd\u7f3a\u4e4f\u53ef\u6269\u5c55\u7684\u54a8\u8be2\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u63a2\u7d22\u6a21\u578b\u5927\u5c0f\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u7f6e\u4fe1\u5ea6\u4f30\u8ba1\u548c\u7ed3\u6784\u5316\u63a8\u7406\u8fc7\u7a0b\u7b49\u5173\u952e\u56e0\u7d20\u6765\u5f00\u53d1\u5f3a\u5927\u7684\u54a8\u8be2\u7cfb\u7edf\u3002", "result": "\u4e00\u4e2a\u66f4\u5c0f\u7684\u6a21\u578b\uff0c\u7ed3\u5408\u538b\u7f29\u7684\u6587\u732e\u6570\u636e\u5e93\u548c\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\uff0c\u5728ICLR 2025\u7684\u81ea\u6211\u6392\u540d\u63d0\u4ea4\u7684\u8bba\u6587\u4e2d\uff0c\u5176\u5f55\u7528\u7387\u8d85\u8fc7\u4e86\u50cfDeepseek-R1\u8fd9\u6837\u5f3a\u5927\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\u3002\u5f53\u4ec5\u9650\u4e8e\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u65f6\uff0c\u8be5\u7cfb\u7edf\u5728ICLR 2025\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5f55\u7528\u7387\u8d85\u8fc790%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u5347\u5047\u8bbe\u751f\u6210\u548c\u5b9e\u9a8c\u8bbe\u8ba1\u8d28\u91cf\u4e0e\u6548\u7387\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u7ecf\u8fc7\u4f18\u5316\u7684\u3001\u66f4\u5c0f\u7684\u6a21\u578b\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u53ef\u4ee5\u8d85\u8d8a\u5927\u578b\u901a\u7528\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6587\u732e\u6570\u636e\u5e93\u8fdb\u884c\u538b\u7f29\u5e76\u7ed3\u5408\u7ed3\u6784\u5316\u63a8\u7406\u6846\u67b6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u548c\u9ad8\u8bba\u6587\u5f55\u7528\u7387\u3002"}}
{"id": "2507.08843", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08843", "abs": "https://arxiv.org/abs/2507.08843", "authors": ["Arpita Soni", "Sahil Tripathi", "Gautam Siddharth Kashyap", "Manaswi Kulahara", "Mohammad Anas Azeez", "Zohaib Hasan Siddiqui", "Nipun Joshi", "Jiechao Gao"], "title": "Can We Predict Your Next Move Without Breaking Your Privacy?", "comment": "Accepted in the 17th International Conference on Advances in Social\n  Networks Analysis and Mining (ASONAM 2025), scheduled for 25 - 28 August 2025\n  in Ontario, Canada", "summary": "We propose FLLL3M--Federated Learning with Large Language Models for Mobility\nModeling--a privacy-preserving framework for Next-Location Prediction (NxLP).\nBy retaining user data locally and leveraging LLMs through an efficient outer\nproduct mechanism, FLLL3M ensures high accuracy with low resource demands. It\nachieves SOT results on Gowalla (Acc@1: 12.55, MRR: 0.1422), WeePlace (10.71,\n0.1285), Brightkite (10.42, 0.1169), and FourSquare (8.71, 0.1023), while\nreducing parameters by up to 45.6% and memory usage by 52.7%.", "AI": {"tldr": "FLLL3M\uff1a\u4e00\u79cd\u4fdd\u62a4\u9690\u79c1\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u79fb\u52a8\u5efa\u6a21\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8d44\u6e90\u9700\u6c42\u3002", "motivation": "\u4e3a\u4e86\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u7684\u79fb\u52a8\u5efa\u6a21\u548cNext-Location Prediction\uff08NxLP\uff09\u3002", "method": "FLLL3M\u6846\u67b6\u5229\u7528\u8054\u90a6\u5b66\u4e60\u7684\u539f\u7406\uff0c\u5c06\u7528\u6237\u6570\u636e\u4fdd\u7559\u5728\u672c\u5730\uff0c\u5e76\u901a\u8fc7\u5916\u79ef\u673a\u5236\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8fdb\u884c\u79fb\u52a8\u5efa\u6a21\u548cNext-Location Prediction\uff08NxLP\uff09\u3002", "result": "FLLL3M\u5728Gowalla\u3001WeePlace\u3001Brightkite\u548cFourSquare\u7b49\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86SOT\u7ed3\u679c\uff0c\u51c6\u786e\u7387\u548cMRR\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u51cf\u5c11\u4e86\u9ad8\u8fbe45.6%\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u4e8652.7%\u3002", "conclusion": "FLLL3M\u662f\u4e00\u79cd\u5728\u4fdd\u6301\u7528\u6237\u6570\u636e\u672c\u5730\u5316\u548c\u5229\u7528LLM\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u9ad8\u6548\u7684\u5916\u79ef\u673a\u5236\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8d44\u6e90\u9700\u6c42\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u53ef\u5728Next-Location Prediction\uff08NxLP\uff09\u4efb\u52a1\u4e2d\u53d6\u5f97SOT\u7ed3\u679c\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u53c2\u6570\u548c\u5185\u5b58\u4f7f\u7528\u91cf\u3002"}}
{"id": "2507.09864", "categories": ["eess.SY", "cs.AI", "cs.LG", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09864", "abs": "https://arxiv.org/abs/2507.09864", "authors": ["Hossein Nejatbakhsh Esfahani", "Javad Mohammadpour Velni"], "title": "Intersection of Reinforcement Learning and Bayesian Optimization for Intelligent Control of Industrial Processes: A Safe MPC-based DPG using Multi-Objective BO", "comment": null, "summary": "Model Predictive Control (MPC)-based Reinforcement Learning (RL) offers a\nstructured and interpretable alternative to Deep Neural Network (DNN)-based RL\nmethods, with lower computational complexity and greater transparency. However,\nstandard MPC-RL approaches often suffer from slow convergence, suboptimal\npolicy learning due to limited parameterization, and safety issues during\nonline adaptation. To address these challenges, we propose a novel framework\nthat integrates MPC-RL with Multi-Objective Bayesian Optimization (MOBO). The\nproposed MPC-RL-MOBO utilizes noisy evaluations of the RL stage cost and its\ngradient, estimated via a Compatible Deterministic Policy Gradient (CDPG)\napproach, and incorporates them into a MOBO algorithm using the Expected\nHypervolume Improvement (EHVI) acquisition function. This fusion enables\nefficient and safe tuning of the MPC parameters to achieve improved closed-loop\nperformance, even under model imperfections. A numerical example demonstrates\nthe effectiveness of the proposed approach in achieving sample-efficient,\nstable, and high-performance learning for control systems.", "AI": {"tldr": "MPC-RL-MOBO\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff0c\u89e3\u51b3\u4e86\u6807\u51c6MPC-RL\u6536\u655b\u6162\u3001\u6b21\u4f18\u548c\u4e0d\u5b89\u5168\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u7a33\u5b9a\u3001\u9ad8\u6027\u80fd\u7684\u63a7\u5236\u7cfb\u7edf\u5b66\u4e60\u3002", "motivation": "\u6807\u51c6\u7684MPC-RL\u65b9\u6cd5\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u3001\u53c2\u6570\u5316\u53d7\u9650\u5bfc\u81f4\u7b56\u7565\u5b66\u4e60\u6b21\u4f18\u4ee5\u53ca\u5728\u7ebf\u9002\u5e94\u671f\u95f4\u7684\u5b89\u5168\u95ee\u9898\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408MPC-RL\u548c\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff08MOBO\uff09\u7684\u65b0\u9896\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u901a\u8fc7\u517c\u5bb9\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\uff08CDPG\uff09\u65b9\u6cd5\u4f30\u8ba1\u7684RL\u9636\u6bb5\u6210\u672c\u53ca\u5176\u68af\u5ea6\u7684\u566a\u58f0\u8bc4\u4f30\uff0c\u5e76\u5c06\u5b83\u4eec\u901a\u8fc7\u9884\u671f\u8d85\u4f53\u79ef\u6539\u8fdb\uff08EHVI\uff09\u83b7\u53d6\u51fd\u6570\u7eb3\u5165MOBO\u7b97\u6cd5\u3002", "result": "\u6570\u503c\u793a\u4f8b\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63a7\u5236\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u6837\u672c\u9ad8\u6548\u3001\u7a33\u5b9a\u548c\u9ad8\u6027\u80fd\u5b66\u4e60\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408MPC-RL\u548c\u591a\u76ee\u6807\u8d1d\u53f6\u65af\u4f18\u5316\uff08MOBO\uff09\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5b89\u5168\u7684MPC\u53c2\u6570\u8c03\u6574\uff0c\u4ece\u800c\u5728\u6a21\u578b\u4e0d\u5b8c\u5584\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u83b7\u5f97\u6539\u8fdb\u7684\u95ed\u73af\u6027\u80fd\u3002"}}
{"id": "2507.09371", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09371", "abs": "https://arxiv.org/abs/2507.09371", "authors": ["Kehan Wen", "Chenhao Li", "Junzhe He", "Marco Hutter"], "title": "Constrained Style Learning from Imperfect Demonstrations under Task Optimality", "comment": "This paper is under review", "summary": "Learning from demonstration has proven effective in robotics for acquiring\nnatural behaviors, such as stylistic motions and lifelike agility, particularly\nwhen explicitly defining style-oriented reward functions is challenging.\nSynthesizing stylistic motions for real-world tasks usually requires balancing\ntask performance and imitation quality. Existing methods generally depend on\nexpert demonstrations closely aligned with task objectives. However, practical\ndemonstrations are often incomplete or unrealistic, causing current methods to\nboost style at the expense of task performance. To address this issue, we\npropose formulating the problem as a constrained Markov Decision Process\n(CMDP). Specifically, we optimize a style-imitation objective with constraints\nto maintain near-optimal task performance. We introduce an adaptively\nadjustable Lagrangian multiplier to guide the agent to imitate demonstrations\nselectively, capturing stylistic nuances without compromising task performance.\nWe validate our approach across multiple robotic platforms and tasks,\ndemonstrating both robust task performance and high-fidelity style learning. On\nANYmal-D hardware we show a 14.5% drop in mechanical energy and a more agile\ngait pattern, showcasing real-world benefits.", "AI": {"tldr": "\u901a\u8fc7\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\u548c\u81ea\u9002\u5e94\u62c9\u683c\u6717\u65e5\u4e58\u6570\uff0c\u5728\u673a\u5668\u4eba\u6a21\u4eff\u5b66\u4e60\u4e2d\u5b9e\u73b0\u98ce\u683c\u5b66\u4e60\u548c\u4efb\u52a1\u6027\u80fd\u7684\u5e73\u8861\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5b9e\u8df5\u6f14\u793a\u4e0d\u5b8c\u6574\u6216\u4e0d\u73b0\u5b9e\u65f6\uff0c\u4e3a\u4e86\u63d0\u5347\u98ce\u683c\u800c\u727a\u7272\u4efb\u52a1\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u5c06\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\uff0c\u4f18\u5316\u98ce\u683c\u6a21\u4eff\u76ee\u6807\u5e76\u52a0\u5165\u7ea6\u675f\u4ee5\u7ef4\u6301\u63a5\u8fd1\u6700\u4f18\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u5f15\u5165\u81ea\u9002\u5e94\u53ef\u8c03\u7684\u62c9\u683c\u6717\u65e5\u4e58\u6570\u6765\u6307\u5bfc\u667a\u80fd\u4f53\u9009\u62e9\u6027\u6a21\u4eff\u6f14\u793a\uff0c\u4ece\u800c\u6355\u6349\u98ce\u683c\u7684\u7ec6\u5fae\u5dee\u522b\u800c\u4e0d\u5f71\u54cd\u4efb\u52a1\u6027\u80fd\u3002", "result": "\u5728ANYmal-D\u786c\u4ef6\u4e0a\uff0c\u5b9e\u73b0\u4e8614.5%\u7684\u673a\u68b0\u80fd\u964d\u4f4e\u548c\u66f4\u654f\u6377\u7684\u6b65\u6001\u6a21\u5f0f\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u591a\u673a\u5668\u4eba\u5e73\u53f0\u548c\u4efb\u52a1\u4e0a\u7684\u9c81\u68d2\u4efb\u52a1\u6027\u80fd\u548c\u9ad8\u4fdd\u771f\u98ce\u683c\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ea6\u675f\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08CMDP\uff09\u4f18\u5316\u4e86\u98ce\u683c\u6a21\u4eff\u76ee\u6807\uff0c\u5e76\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u53ef\u8c03\u7684\u62c9\u683c\u6717\u65e5\u4e58\u6570\uff0c\u4ee5\u5728\u4e0d\u635f\u5bb3\u4efb\u52a1\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u9009\u62e9\u6027\u5730\u6a21\u4eff\u6f14\u793a\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u4fdd\u771f\u98ce\u683c\u5b66\u4e60\u548c\u9c81\u68d2\u7684\u4efb\u52a1\u6027\u80fd\uff0c\u5e76\u5728ANYmal-D\u786c\u4ef6\u4e0a\u5b9e\u73b0\u4e86\u66f4\u654f\u6377\u7684\u6b65\u6001\u6a21\u5f0f\u548c14.5%\u7684\u673a\u68b0\u80fd\u964d\u4f4e\u3002"}}
{"id": "2507.09092", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09092", "abs": "https://arxiv.org/abs/2507.09092", "authors": ["Ram S Iyer", "Narayan S Iyer", "Rugmini Ammal P"], "title": "MI CAM: Mutual Information Weighted Activation Mapping for Causal Visual Explanations of Convolutional Neural Networks", "comment": "12 pages, 10 figures", "summary": "With the intervention of machine vision in our crucial day to day necessities\nincluding healthcare and automated power plants, attention has been drawn to\nthe internal mechanisms of convolutional neural networks, and the reason why\nthe network provides specific inferences. This paper proposes a novel post-hoc\nvisual explanation method called MI CAM based on activation mapping. Differing\nfrom previous class activation mapping based approaches, MI CAM produces\nsaliency visualizations by weighing each feature map through its mutual\ninformation with the input image and the final result is generated by a linear\ncombination of weights and activation maps. It also adheres to producing causal\ninterpretations as validated with the help of counterfactual analysis. We aim\nto exhibit the visual performance and unbiased justifications for the model\ninferencing procedure achieved by MI CAM. Our approach works at par with all\nstate-of-the-art methods but particularly outperforms some in terms of\nqualitative and quantitative measures. The implementation of proposed method\ncan be found on https://anonymous.4open.science/r/MI-CAM-4D27", "AI": {"tldr": "MI CAM\u901a\u8fc7\u4e92\u4fe1\u606f\u52a0\u6743\u7279\u5f81\u56fe\uff0c\u5e76\u7ed3\u5408\u53cd\u4e8b\u5b9e\u5206\u6790\u6765\u751f\u6210\u56e0\u679c\u89e3\u91ca\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740\u673a\u5668\u89c6\u89c9\u5728\u533b\u7597\u4fdd\u5065\u548c\u81ea\u52a8\u5316\u53d1\u7535\u5382\u7b49\u5173\u952e\u65e5\u5e38\u9700\u6c42\u4e2d\u7684\u5e94\u7528\uff0c\u4eba\u4eec\u8d8a\u6765\u8d8a\u5173\u6ce8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u90e8\u673a\u5236\u53ca\u5176\u63d0\u4f9b\u7279\u5b9a\u63a8\u7406\u7684\u539f\u56e0\u3002", "method": "MI CAM\u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u6fc0\u6d3b\u6620\u5c04\u7684\u3001\u4e8b\u540e\u89c6\u89c9\u89e3\u91ca\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u8f93\u5165\u56fe\u50cf\u7684\u4e92\u4fe1\u606f\u5bf9\u6bcf\u4e2a\u7279\u5f81\u56fe\u8fdb\u884c\u52a0\u6743\u6765\u751f\u6210\u663e\u7740\u6027\u53ef\u89c6\u5316\uff0c\u5e76\u901a\u8fc7\u6743\u91cd\u548c\u6fc0\u6d3b\u56fe\u7684\u7ebf\u6027\u7ec4\u5408\u751f\u6210\u6700\u7ec8\u7ed3\u679c\u3002", "result": "MI CAM\u901a\u8fc7\u4e92\u4fe1\u606f\u52a0\u6743\u7279\u5f81\u56fe\uff0c\u5e76\u7ed3\u5408\u53cd\u4e8b\u5b9e\u5206\u6790\u6765\u751f\u6210\u56e0\u679c\u89e3\u91ca\uff0c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "MI CAM\u4e0e\u6240\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u5e76\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u65b9\u9762\u4f18\u4e8e\u5176\u4e2d\u4e00\u4e9b\u65b9\u6cd5\u3002"}}
{"id": "2507.09669", "categories": ["cond-mat.mtrl-sci", "cond-mat.dis-nn", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2507.09669", "abs": "https://arxiv.org/abs/2507.09669", "authors": ["Deep Mondal", "Sujoy Datta", "Debnarayan Jana"], "title": "Navigating the Evolution of Two-dimensional Carbon Nitride Research: Integrating Machine Learning into Conventional Approaches", "comment": null, "summary": "Carbon nitride research has reached a promising point in today's research\nendeavours with diverse applications including photocatalysis, energy storage,\nand sensing due to their unique electronic and structural properties. Recent\nadvances in machine learning (ML) have opened new avenues for exploring and\noptimizing the potential of these materials. This study presents a\ncomprehensive review of the integration of ML techniques in carbon nitride\nresearch with an introduction to CN classifications and recent advancements. We\ndiscuss the methodologies employed, such as supervised learning, unsupervised\nlearning, and reinforcement learning, in predicting material properties,\noptimizing synthesis conditions, and enhancing performance metrics. Key\nfindings indicate that ML algorithms can significantly reduce experimental\ntrial-and-error, accelerate discovery processes, and provide deeper insights\ninto the structure-property relationships of carbon nitride. The synergistic\neffect of combining ML with traditional experimental approaches is highlighted,\nshowcasing studies where ML driven models have successfully predicted novel\ncarbon nitride compositions with enhanced functional properties. Future\ndirections in this field are also proposed, emphasizing the need for\nhigh-quality datasets, advanced ML models, and interdisciplinary collaborations\nto fully realize the potential of carbon nitride materials in next-generation\ntechnologies.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u56de\u987e\u4e86\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u5728\u78b3\u6c2e\u5316\u7269\u7814\u7a76\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86ML\u9884\u6d4b\u6750\u6599\u6027\u8d28\u3001\u4f18\u5316\u5408\u6210\u548c\u63d0\u9ad8\u6027\u80fd\u7684\u6f5c\u529b\u3002\u673a\u5668\u5b66\u4e60\u4e0e\u4f20\u7edf\u5b9e\u9a8c\u76f8\u7ed3\u5408\u7684\u534f\u540c\u4f5c\u7528\u5f97\u5230\u4e86\u5f3a\u8c03\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u53d1\u5c55\u65b9\u5411\uff0c\u5305\u62ec\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3001\u5148\u8fdb\u7684ML\u6a21\u578b\u548c\u8de8\u5b66\u79d1\u5408\u4f5c\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u548c\u4f18\u5316\u78b3\u6c2e\u5316\u7269\u6750\u6599\u7684\u6f5c\u529b\uff0c\u672c\u7814\u7a76\u5bf9ML\u6280\u672f\u5728\u78b3\u6c2e\u5316\u7269\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\u3002", "method": "\u5bf9ML\u6280\u672f\uff08\u5982\u76d1\u7763\u5b66\u4e60\u3001\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u5f3a\u5316\u5b66\u4e60\uff09\u5728\u9884\u6d4b\u6750\u6599\u6027\u8d28\u3001\u4f18\u5316\u5408\u6210\u6761\u4ef6\u548c\u63d0\u9ad8\u6027\u80fd\u6307\u6807\u65b9\u9762\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002", "result": "ML\u7b97\u6cd5\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u5b9e\u9a8c\u7684\u8bd5\u9519\u6b21\u6570\uff0c\u52a0\u901f\u53d1\u73b0\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u5bf9\u78b3\u6c2e\u5316\u7269\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u7684\u66f4\u6df1\u5165\u89c1\u89e3\u3002\u5df2\u5c55\u793a\u51faML\u9a71\u52a8\u7684\u6a21\u578b\u80fd\u591f\u6210\u529f\u9884\u6d4b\u5177\u6709\u589e\u5f3a\u529f\u80fd\u7279\u6027\u7684\u65b0\u578b\u78b3\u6c2e\u5316\u7269\u7ec4\u5408\u3002", "conclusion": "ML\u7b97\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5b9e\u9a8c\u7684\u8bd5\u9519\u6b21\u6570\uff0c\u52a0\u901f\u6750\u6599\u7684\u53d1\u73b0\u8fc7\u7a0b\uff0c\u5e76\u63d0\u4f9b\u5bf9\u78b3\u6c2e\u5316\u7269\u7ed3\u6784-\u6027\u8d28\u5173\u7cfb\u7684\u66f4\u6df1\u5165\u7406\u89e3\u3002\u5c06ML\u4e0e\u4f20\u7edf\u5b9e\u9a8c\u65b9\u6cd5\u76f8\u7ed3\u5408\u7684\u534f\u540c\u4f5c\u7528\u5f97\u5230\u4e86\u5f3a\u8c03\uff0c\u5c55\u793a\u4e86ML\u9a71\u52a8\u7684\u6a21\u578b\u6210\u529f\u9884\u6d4b\u4e86\u5177\u6709\u589e\u5f3a\u529f\u80fd\u7279\u6027\u7684\u65b0\u578b\u78b3\u6c2e\u5316\u7269\u6210\u5206\u3002"}}
{"id": "2507.09273", "categories": ["quant-ph", "cond-mat.other", "cond-mat.stat-mech", "hep-lat"], "pdf": "https://arxiv.org/pdf/2507.09273", "abs": "https://arxiv.org/abs/2507.09273", "authors": ["Phillip Weinberg", "Na Xu", "Anders W. Sandvik"], "title": "Defects and their Time Scales in Quantum and Classical Annealing of the Two-Dimensional Ising Model", "comment": "29 pages, 27 figures", "summary": "We investigate defects in the two-dimensional transverse-field Ising\nferromagnet on periodic $L\\times L$ lattices after quantum annealing from high\nto vanishing field. With exact numerical solutions for $L \\le 6$, we observe\nthe expected critical Kibble-Zurek (KZ) time scale $\\propto L^{z+1/\\nu}$ (with\n$z=1$ and $1/\\nu \\approx 1.59$) at the quantum phase transition. We also\nobserve KZ scaling of the ground-state fidelity at the end of the process. The\nexcitations evolve by coarsening dynamics of confined defects, with a time\nscale $\\propto L^2$, and interface fluctuations of system-spanning defects,\nwith life time $\\propto L^3$. We build on analogies with classical simulated\nannealing, where we characterize system-spanning defects in detail and find\ndifferences in the dynamic scales of domain walls with winding numbers\n$W=(1,0)/(0,1)$ (horizontal/vertical) and $W=(1,1)$ (diagonal). They decay on\ntime scales $\\propto L^3$ (which applies also to system-spanning domains in\nsystems with open boundaries) and $\\propto L^{3.4}$, respectively, when imposed\nin the ordered phase. As a consequence of $L^{3.4}$ exceeding the classical KZ\nscale $L^{z+1/\\nu}=L^{3.17}$ the probability of $W=(1,1)$ domains in SA scales\nwith the KZ exponent even in the final $T=0$ state. In QA, also the\n$W=(1,0)/(0,1)$ domains are controlled by the KZ time scale $L^{2.59}$. The\n$L^3$ scale can nevertheless be detected in the excited states, using a method\nthat we develop that should also be applicable in QA experiments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u5206\u6790\u4e86\u91cf\u5b50\u9000\u706b\u8fc7\u7a0b\u4e2d\u4e8c\u7ef4\u4f0a\u8f9b\u6a21\u578b\u4e2d\u7684\u7f3a\u9677\u52a8\u529b\u5b66\u3002\u7ed3\u679c\u8868\u660e\uff0c\u7f3a\u9677\u6f14\u5316\u9075\u5faaKibble-Zurek\u6807\u5ea6\u5f8b\uff0c\u5e76\u4e0e\u7574\u58c1\u62d3\u6251\u6709\u5173\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5206\u6790\u7f3a\u9677\u52a8\u529b\u5b66\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u91cf\u5b50\u9000\u706b\u8fc7\u7a0b\u4e2d\u7f3a\u9677\u7684\u4ea7\u751f\u548c\u6f14\u5316\u673a\u5236\uff0c\u7279\u522b\u662f\u4e0e\u91cf\u5b50\u76f8\u53d8\u548c\u9000\u706b\u52a8\u529b\u5b66\u76f8\u5173\u7684\u884c\u4e3a\u3002", "method": "\u4f7f\u7528\u7cbe\u786e\u6570\u503c\u89e3\u6cd5\uff08exact numerical solutions\uff09\u7814\u7a76\u4e86L\u00d7L\u4e8c\u7ef4\u6a2a\u5411\u573a\u4f0a\u8f9b\u94c1\u78c1\u4f53\uff08transverse-field Ising ferromagnet\uff09\u5728\u4ece\u9ad8\u573a\u5230\u96f6\u573a\u91cf\u5b50\u9000\u706b\uff08quantum annealing\uff09\u8fc7\u7a0b\u4e2d\u7684\u7f3a\u9677\u884c\u4e3a\u3002", "result": "\u5728\u91cf\u5b50\u76f8\u53d8\u5904\u89c2\u5bdf\u5230\u9884\u671f\u7684Kibble-Zurek (KZ) \u65f6\u95f4\u6807\u5ea6\uff08\u221d L^(z+1/\u03bd)\uff09\uff0c\u5e76\u4e14\u57fa\u6001\u4fdd\u771f\u5ea6\u4e5f\u663e\u793a\u51fa\u76f8\u540c\u7684KZ\u6807\u5ea6\u5f8b\u3002\u6fc0\u53d1\u6001\u7684\u6f14\u5316\u8868\u73b0\u4e3a\u53d7\u9650\u7f3a\u9677\u7684\u7c97\u5316\u52a8\u529b\u5b66\uff08\u65f6\u95f4\u6807\u5ea6\u221d L^2\uff09\u548c\u8de8\u7cfb\u7edf\u7f3a\u9677\u7684\u754c\u9762\u6da8\u843d\uff08\u5bff\u547d\u221d L^3\uff09\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u62d3\u6251\u7684\u7574\u58c1\u5728\u9000\u706b\u8fc7\u7a0b\u4e2d\u5177\u6709\u4e0d\u540c\u7684\u52a8\u529b\u5b66\u65f6\u95f4\u5c3a\u5ea6\uff08W=(1,0)/(0,1)\u4e3a\u221d L^3\uff0cW=(1,1)\u4e3a\u221d L^3.4\uff09\u3002W=(1,1)\u7574\u58c1\u7684\u65f6\u95f4\u5c3a\u5ea6\u8d85\u8fc7\u7ecf\u5178KZ\u65f6\u95f4\u5c3a\u5ea6\uff0c\u5bfc\u81f4\u5176\u5728\u7ecf\u5178\u6a21\u62df\u9000\u706b\u7684\u96f6\u6e29\u72b6\u6001\u4e0b\u6982\u7387\u4e0eKZ\u6307\u6570\u76f8\u5173\u3002\u5728\u91cf\u5b50\u9000\u706b\u4e2d\uff0cW=(1,0)/(0,1)\u7574\u58c1\u4e5f\u53d7KZ\u65f6\u95f4\u5c3a\u5ea6\uff08\u221d L^2.59\uff09\u63a7\u5236\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u68c0\u6d4bL^3\u65f6\u95f4\u5c3a\u5ea6\u7684\u7279\u6b8a\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u91cf\u5b50\u9000\u706b\u5b9e\u9a8c\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u7cbe\u786e\u6570\u503c\u89e3\u6cd5\u7814\u7a76\u4e86\u4e8c\u7ef4\u6a2a\u5411\u573a\u4f0a\u8f9b\u94c1\u78c1\u4f53\u5728\u91cf\u5b50\u9000\u706b\u8fc7\u7a0b\u4e2d\u7f3a\u9677\u7684\u6f14\u5316\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u91cf\u5b50\u76f8\u53d8\u70b9\uff0c\u7cfb\u7edf\u8868\u73b0\u51fa\u9884\u671f\u7684Kibble-Zurek (KZ) \u65f6\u95f4\u6807\u5ea6\uff0c\u5e76\u4e14\u57fa\u6001\u4fdd\u771f\u5ea6\u4e5f\u5448\u73b0\u76f8\u540c\u7684KZ\u6807\u5ea6\u5f8b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63ed\u793a\u4e86\u6fc0\u53d1\u6001\u7684\u6f14\u5316\u8fc7\u7a0b\uff0c\u5305\u62ec\u7f3a\u9677\u7684\u7c97\u5316\u52a8\u529b\u5b66\u548c\u8de8\u7cfb\u7edf\u7f3a\u9677\u7684\u754c\u9762\u6da8\u843d\u3002\u901a\u8fc7\u4e0e\u7ecf\u5178\u6a21\u62df\u9000\u706b\u7684\u7c7b\u6bd4\uff0c\u7814\u7a76\u8be6\u7ec6\u523b\u753b\u4e86\u8de8\u7cfb\u7edf\u7f3a\u9677\u7684\u6027\u8d28\uff0c\u5e76\u53d1\u73b0\u4e86\u4e0d\u540c\u62d3\u6251\uff08W=(1,0)/(0,1)\u548cW=(1,1)\uff09\u7684\u7574\u58c1\u5728\u52a8\u529b\u5b66\u65f6\u95f4\u5c3a\u5ea6\u4e0a\u7684\u5dee\u5f02\u3002\u7279\u522b\u5730\uff0c\u5bf9\u4e8eW=(1,1)\u7574\u58c1\uff0c\u5176\u52a8\u529b\u5b66\u65f6\u95f4\u5c3a\u5ea6\u8d85\u8fc7\u4e86\u7ecf\u5178\u7684KZ\u65f6\u95f4\u5c3a\u5ea6\uff0c\u5bfc\u81f4\u5728\u7ecf\u5178\u6a21\u62df\u9000\u706b\u7684\u6700\u7ec8\u72b6\u6001\u4e0b\uff0c\u5176\u6982\u7387\u4e0eKZ\u6307\u6570\u76f8\u5173\u3002\u5728\u91cf\u5b50\u9000\u706b\u8fc7\u7a0b\u4e2d\uff0cW=(1,0)/(0,1)\u7574\u58c1\u4e5f\u53d7\u5230KZ\u65f6\u95f4\u5c3a\u5ea6\u7684\u63a7\u5236\u3002\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u68c0\u6d4bL^3\u65f6\u95f4\u5c3a\u5ea6\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6709\u671b\u5e94\u7528\u4e8e\u5b9e\u9645\u7684\u91cf\u5b50\u9000\u706b\u5b9e\u9a8c\u3002"}}
{"id": "2507.09155", "categories": ["cs.CL", "cs.AI", "68T50, 68T07"], "pdf": "https://arxiv.org/pdf/2507.09155", "abs": "https://arxiv.org/abs/2507.09155", "authors": ["Ali Vosoughi", "Ayoub Shahnazari", "Yufeng Xi", "Zeliang Zhang", "Griffin Hess", "Chenliang Xu", "Niaz Abdolrahim"], "title": "OPENXRD: A Comprehensive Benchmark and Enhancement Framework for LLM/MLLM XRD Question Answering", "comment": "10 pages, 6 figures, 5 tables. Code and dataset available at\n  https://github.com/niaz60/OpenXRD. Project webpage:\n  https://niaz60.github.io/OpenXRD/", "summary": "This work presents OPENXRD, an open-book pipeline designed for\ncrystallography question answering, which integrates textual prompts with\nconcise supporting content generated by GPT-4.5. Instead of using scanned\ntextbooks, which may lead to copyright issues, OPENXRD generates compact,\ndomain-specific references that help smaller models understand key concepts in\nX-ray diffraction (XRD). We evaluate OPENXRD on a well-defined set of 217\nexpert-level XRD questions by comparing different vision-language models,\nincluding GPT-4 and LLaVA-based frameworks such as Mistral, LLaMA, and QWEN,\nunder both closed-book (without supporting material) and open-book (with\nsupporting material) conditions. Our experimental results show significant\naccuracy improvements in models that use the GPT-4.5-generated summaries,\nparticularly those with limited prior training in crystallography. OPENXRD uses\nknowledge from larger models to fill knowledge gaps in crystallography and\nshows that AI-generated texts can help smaller models reason more effectively\nin scientific tasks. While the current version of OPENXRD focuses on text-based\ninputs, we also explore future extensions such as adding real crystal diagrams\nor diffraction patterns to improve interpretation in specialized materials\nscience contexts. Overall, OPENXRD shows that specialized open-book systems can\nbe useful in materials science and provides a foundation for broader natural\nlanguage processing (NLP) tools in critical scientific fields.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09561", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09561", "abs": "https://arxiv.org/abs/2507.09561", "authors": ["Can Wang", "Wei Liu", "Hanzhi Ma", "Xiaonan Jiang", "Erping Li", "Steven Gao"], "title": "Novel Physics-Aware Attention-Based Machine Learning Approach for Mutual Coupling Modeling", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "This article presents a physics-aware convolutional long short-term memory\n(PC-LSTM) network for efficient and accurate extraction of mutual impedance\nmatrices in dipole antenna arrays. By reinterpreting the Green's function\nthrough a physics-aware neural network and embedding it into an adaptive loss\nfunction, the proposed machine learning-based approach achieves enhanced\nphysical interpretability in mutual coupling modeling. Also, an attention\nmechanism is carefully designed to calibrate complex-valued features by fusing\nthe real and imaginary parts of the Green's function matrix. These fused\nrepresentations are then processed by a convolutional long short-term memory\nnetwork, and the impedance matrix of the linear antenna array can be finally\nderived. Validation against five benchmarks underscores the efficacy of the\nproposed approach, demonstrating accurate impedance extraction with up to a 7x\nspeedup compared to CST Microwave Studio, making it a fast alternative to\nfull-wave simulations for mutual coupling characterization.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u611f\u77e5\u957f\u77ed\u671f\u8bb0\u5fc6\uff08PC-LSTM\uff09\u7f51\u7edc\uff0c\u7528\u4e8e\u9ad8\u6548\u51c6\u786e\u5730\u63d0\u53d6\u5076\u6781\u5b50\u5929\u7ebf\u9635\u5217\u7684\u4e92\u963b\u6297\u77e9\u9635\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u89e3\u91ca\u683c\u6797\u51fd\u6570\u5e76\u5c06\u5176\u5d4c\u5165\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u63d0\u9ad8\u4e86\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002\u6ce8\u610f\u529b\u673a\u5236\u7528\u4e8e\u878d\u5408\u5b9e\u90e8\u548c\u865a\u90e8\uff0c\u7136\u540e\u7531\u5377\u79ef\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\u5904\u7406\u4ee5\u5bfc\u51fa\u963b\u6297\u77e9\u9635\u3002\u8be5\u65b9\u6cd5\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u9a8c\u8bc1\uff0c\u4e0eCST Microwave Studio\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e867\u500d\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5076\u6781\u5b50\u5929\u7ebf\u9635\u5217\u4e2d\u4e92\u963b\u6297\u77e9\u9635\u7684\u9ad8\u6548\u548c\u51c6\u786e\u63d0\u53d6\uff0c\u5e76\u589e\u5f3a\u4e92\u8026\u5efa\u6a21\u7684\u7269\u7406\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u611f\u77e5\u957f\u77ed\u671f\u8bb0\u5fc6\uff08PC-LSTM\uff09\u7f51\u7edc\uff0c\u901a\u8fc7\u5c06\u7269\u7406\u611f\u77e5\u795e\u7ecf\u7f51\u7edc\u91cd\u65b0\u89e3\u91ca\u683c\u6797\u51fd\u6570\u5e76\u5c06\u5176\u5d4c\u5165\u81ea\u9002\u5e94\u635f\u5931\u51fd\u6570\u4e2d\uff0c\u5e76\u7ed3\u5408\u6ce8\u610f\u529b\u673a\u5236\u6765\u878d\u5408\u5b9e\u90e8\u548c\u865a\u90e8\uff0c\u6700\u7ec8\u63a8\u5bfc\u51fa\u7ebf\u6027\u5929\u7ebf\u9635\u5217\u7684\u963b\u6297\u77e9\u9635\u3002", "result": "\u4e0e\u4e94\u4e2a\u57fa\u51c6\u76f8\u6bd4\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u963b\u6297\u63d0\u53d6\u65b9\u9762\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u901f\u5ea6\u6bd4CST Microwave Studio\u5feb7\u500d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u9ad8\u6548\u548c\u51c6\u786e\u7684\u5076\u6781\u5b50\u5929\u7ebf\u9635\u5217\u4e92\u963b\u6297\u77e9\u9635\u63d0\u53d6\uff0c\u5e76\u63d0\u4f9b\u4e86\u589e\u5f3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u4e0eCST Microwave Studio\u76f8\u6bd4\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e867\u500d\uff0c\u662f\u4e00\u79cd\u7528\u4e8e\u4e92\u8026\u7279\u6027\u7684\u5feb\u901f\u5168\u6ce2\u6a21\u62df\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.08845", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08845", "abs": "https://arxiv.org/abs/2507.08845", "authors": ["Irfan Ullah", "Young-Koo Lee"], "title": "DAFOS: Dynamic Adaptive Fanout Optimization Sampler", "comment": null, "summary": "Graph Neural Networks (GNNs) are becoming an essential tool for learning from\ngraph-structured data, however uniform neighbor sampling and static fanout\nsettings frequently limit GNNs' scalability and efficiency. In this paper, we\npropose the Dynamic Adaptive Fanout Optimization Sampler (DAFOS), a novel\napproach that dynamically adjusts the fanout based on model performance and\nprioritizes important nodes during training. Our approach leverages node\nscoring based on node degree to focus computational resources on structurally\nimportant nodes, incrementing the fanout as the model training progresses.\nDAFOS also integrates an early stopping mechanism to halt training when\nperformance gains diminish. Experiments conducted on three benchmark datasets,\nogbnarxiv, Reddit, and ogbn-products, demonstrate that our approach\nsignificantly improves training speed and accuracy compared to a\nstate-of-the-art approach. DAFOS achieves a 3.57x speedup on the ogbn-arxiv\ndataset and a 12.6x speedup on the Reddit dataset while improving the F1 score\nfrom 68.5% to 71.21% on ogbn-arxiv and from 73.78% to 76.88% on the\nogbn-products dataset, respectively. These results highlight the potential of\nDAFOS as an efficient and scalable solution for large-scale GNN training.", "AI": {"tldr": "DAFOS\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u91c7\u6837\u548c\u4f18\u5148\u5904\u7406\u91cd\u8981\u8282\u70b9\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u6269\u5c55\u6027\u548c\u6548\u7387\u65b9\u9762\u53d7\u9650\u4e8e\u5747\u5300\u90bb\u5c45\u91c7\u6837\u548c\u9759\u6001\u6247\u51fa\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDAFOS\u7684\u52a8\u6001\u81ea\u9002\u5e94\u6247\u51fa\u4f18\u5316\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6247\u51fa\u3001\u4f18\u5148\u5904\u7406\u91cd\u8981\u8282\u70b9\uff08\u57fa\u4e8e\u8282\u70b9\u5ea6\u8fdb\u884c\u8bc4\u5206\uff09\u3001\u589e\u52a0\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u6247\u51fa\u4ee5\u53ca\u96c6\u6210\u65e9\u671f\u505c\u6b62\u673a\u5236\u6765\u4f18\u5316\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u3002", "result": "\u5728ogbnarxiv\u3001Reddit\u548cogbn-products\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cDAFOS\u76f8\u6bd4\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u8bad\u7ec3\u901f\u5ea6\u548c\u51c6\u786e\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5728ogbnarxiv\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e863.57\u500d\u7684\u52a0\u901f\uff0c\u5728Reddit\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8612.6\u500d\u7684\u52a0\u901f\uff0c\u5e76\u5728ogbn-arxiv\u6570\u636e\u96c6\u4e0a\u5c06F1\u5206\u6570\u4ece68.5%\u63d0\u9ad8\u523071.21%\uff0c\u5728ogbn-products\u6570\u636e\u96c6\u4e0a\u5c06F1\u5206\u6570\u4ece73.78%\u63d0\u9ad8\u523076.88%\u3002", "conclusion": "DAFOS\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u5927\u89c4\u6a21\u56fe\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u89e3\u51b3\u65b9\u6848\uff0c\u5728\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\u548c\u51c6\u786e\u6027\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.10271", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10271", "abs": "https://arxiv.org/abs/2507.10271", "authors": ["Koustabh Gogoi", "Tanay Nag", "Arnob Kumar Ghosh"], "title": "Dissipation induced Majarona $0$- and $\u03c0$-modes in a driven Rashba nanowire", "comment": "11 pages, 3 figures; Comments are welcome", "summary": "Periodic drive is an intriguing way of creating topological phases in a\nnon-topological setup. However, most systems are often studied as a closed\nsystem, despite being always in contact with the environment, which induces\ndissipation. Here, we investigate a periodically driven Rashba nanowire in\nproximity to an $s$-wave superconductor in a dissipative background. The\nsystem's dynamics is governed by a periodic Liouvillian operator, from which we\nconstruct the Liouvillian time-evolution operator and use the\nthird-quantization method to obtain the `Floquet damping matrix', which\ncaptures the spectral and topological properties of the system. We show that\nthe system exhibits edge-localized topological Majorana $0$-modes (MZMs) and\n$\\pi$-modes (MPMs). Additionally, the system also supports a trivial $0$-modes\n(TZMs) and $\\pi$-modes (TPMs), which are also localized at the edges of the\nsystem. The MZMs and the MPMs are connected to the bulk topology and carry a\nbulk topological invariant, while the emergence of TZMs and TPMs is primarily\ntied to exceptional points and is topologically trivial. We study the\ntopological phase diagrams in terms of the topological invariants and show that\nthe dissipation can modify the topological phase diagram substantially and even\ninduce topological phases in the system. Our work extends the understanding of\na driven-dissipative topological superconductor.", "AI": {"tldr": "\u5728\u8017\u6563\u80cc\u666f\u4e0b\uff0c\u5468\u671f\u6027\u9a71\u52a8\u7684Rashba\u7eb3\u7c73\u7ebf-\u8d85\u5bfc\u4f53\u7cfb\u7edf\u652f\u6301\u62d3\u6251\u548c\u975e\u62d3\u6251\u7684\u8fb9\u9650\u5c40\u57df\u6a21\u5f0f\uff0c\u8017\u6563\u4f1a\u5f71\u54cd\u62d3\u6251\u76f8\u56fe\u3002", "motivation": "\u7814\u7a76\u4e86\u5728\u8017\u6563\u80cc\u666f\u4e0b\uff0c\u4e0es\u6ce2\u8d85\u5bfc\u4f53\u76f8\u90bb\u7684\u5468\u671f\u6027\u9a71\u52a8Rashba\u7eb3\u7c73\u7ebf\u7684\u52a8\u529b\u5b66\u3002", "method": "\u4f7f\u7528\u7b2c\u4e09\u91cf\u5316\u65b9\u6cd5\u83b7\u53d6\u63cf\u8ff0\u7cfb\u7edf\u5149\u8c31\u548c\u62d3\u6251\u6027\u8d28\u7684\u5f17\u6d1b\u51ef\u963b\u5c3c\u77e9\u9635\u3002", "result": "\u53d1\u73b0\u4e86\u8fb9\u9650\u5c40\u57df\u5316\u7684\u62d3\u6251Majorana 0-\u6a21\u5f0f\uff08MZMs\uff09\u548c$\\%pi$-\u6a21\u5f0f\uff08MPMs\uff09\uff0c\u4ee5\u53ca\u4e0eExceptional Points\u76f8\u5173\u7684\u975e\u62d3\u62510-\u6a21\u5f0f\uff08TZMs\uff09\u548c$\\%pi$-\u6a21\u5f0f\uff08TPMs\uff09\u3002\u8017\u6563\u53ef\u4ee5\u663e\u8457\u6539\u53d8\u62d3\u6251\u76f8\u56fe\uff0c\u751a\u81f3\u8bf1\u5bfc\u65b0\u7684\u62d3\u6251\u76f8\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\u9a71\u52a8\u8017\u6563\u7cfb\u7edf\uff0c\u6269\u5c55\u4e86\u5bf9\u9a71\u52a8\u8017\u6563\u62d3\u6251\u8d85\u5bfc\u4f53\u7684\u7406\u89e3\u3002"}}
{"id": "2507.10296", "categories": ["cs.LG", "cs.DS"], "pdf": "https://arxiv.org/pdf/2507.10296", "abs": "https://arxiv.org/abs/2507.10296", "authors": ["Shijie Li", "Weiqiang He", "Ruobing Bai", "Pan Peng"], "title": "Average Sensitivity of Hierarchical $k$-Median Clustering", "comment": null, "summary": "Hierarchical clustering is a widely used method for unsupervised learning\nwith numerous applications. However, in the application of modern algorithms,\nthe datasets studied are usually large and dynamic. If the hierarchical\nclustering is sensitive to small perturbations of the dataset, the usability of\nthe algorithm will be greatly reduced. In this paper, we focus on the\nhierarchical $k$ -median clustering problem, which bridges hierarchical and\ncentroid-based clustering while offering theoretical appeal, practical utility,\nand improved interpretability. We analyze the average sensitivity of algorithms\nfor this problem by measuring the expected change in the output when a random\ndata point is deleted. We propose an efficient algorithm for hierarchical\n$k$-median clustering and theoretically prove its low average sensitivity and\nhigh clustering quality. Additionally, we show that single linkage clustering\nand a deterministic variant of the CLNSS algorithm exhibit high average\nsensitivity, making them less stable. Finally, we validate the robustness and\neffectiveness of our algorithm through experiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u66f4\u7a33\u5b9a\u3001\u66f4\u6709\u6548\u7684\u5c42\u6b21\u805a\u7c7b\u7b97\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5927\u578b\u52a8\u6001\u6570\u636e\u96c6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u4ee3\u7b97\u6cd5\u5728\u5904\u7406\u5927\u578b\u3001\u52a8\u6001\u6570\u636e\u96c6\u65f6\uff0c\u5c42\u6b21\u805a\u7c7b\u5bf9\u6570\u636e\u5fae\u5c0f\u6270\u52a8\u654f\u611f\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u805a\u7126\u4e8e\u5206\u5c42k-\u4e2d\u503c\u805a\u7c7b\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u968f\u673a\u6570\u636e\u70b9\u5220\u9664\u65f6\u8f93\u51fa\u7684\u9884\u671f\u53d8\u5316\u6765\u5206\u6790\u7b97\u6cd5\u7684\u5e73\u5747\u654f\u611f\u6027\u3002", "result": "\u4e0e\u5355\u4e00\u8fde\u63a5\u805a\u7c7b\u548cCLNNSS\u7b97\u6cd5\u7684\u786e\u5b9a\u6027\u53d8\u4f53\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5177\u6709\u8f83\u4f4e\u7684\u5e73\u5747\u654f\u611f\u6027\uff0c\u8868\u660e\u5176\u66f4\u7a33\u5b9a\u3002\u7406\u8bba\u8bc1\u660e\u548c\u5b9e\u9a8c\u7ed3\u679c\u5747\u663e\u793a\u4e86\u8be5\u7b97\u6cd5\u7684\u9ad8\u805a\u7c7b\u8d28\u91cf\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5206\u5c42k-\u4e2d\u503c\u805a\u7c7b\u95ee\u9898\u7684\u9ad8\u6548\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u4f4e\u5e73\u5747\u654f\u611f\u6027\u548c\u9ad8\u805a\u7c7b\u8d28\u91cf\u3002\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.09534", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09534", "abs": "https://arxiv.org/abs/2507.09534", "authors": ["Guanquan Wang", "Takuya Hiraoka", "Yoshimasa Tsuruoka"], "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning", "comment": null, "summary": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning.", "AI": {"tldr": "CTP is a faster offline reinforcement learning method using CTM for trajectory optimization, outperforming diffusion models with less computation and higher speed.", "motivation": "To address the high computational costs of existing diffusion models in planning due to iterative sampling procedures, enabling faster, yet high-quality trajectory generation.", "method": "Consistency Trajectory Planning (CTP), an offline model-based reinforcement learning method that uses the Consistency Trajectory Model (CTM) for trajectory optimization, enabling fast, single-step trajectory generation.", "result": "CTP achieves comparable performance with over 120x speedup in inference time compared to existing diffusion-based planning methods on the D4RL benchmark for long-horizon, goal-conditioned tasks.", "conclusion": "CTP consistently outperforms existing diffusion-based planning methods in long-horizon, goal-conditioned tasks, achieving higher normalized returns with significantly fewer denoising steps and over 120x speedup in inference time."}}
{"id": "2507.09938", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09938", "abs": "https://arxiv.org/abs/2507.09938", "authors": ["Chito A. Petilla"], "title": "A Case Study on Data Acquisition Systems: Relevance to Renewable Energy Technologies", "comment": null, "summary": "Multiple advantages had been identified with the integration of data\nacquisition into any existing system configuration and implementation. Using\ndata acquisition as a support into a monitoring system has not only improved\nits overall performance and reliability but also lowered its operational and\nmaintenance cost because of its real-time data collection from node sensors.\n  As renewable energy needs to be sustainable for it to fully support the\nenergy demand of communities, its management and control still needs to be\nimproved and enhanced. Smart systems are considered the next generation\ntechnological improvement of any system that exists. It is the prelude to\nautonomous systems from industrial applications to home automation. Data\nacquisition is only a part of these smart systems that help in the remote\nmanagement and control of these devices. Remote monitoring functionality\nenhances the operation and reliability which help in making proactive decisions\nduring critical situations and circumstances.\n  Even with data acquisition enhancements, there is still room for improving\nits implementation regarding data security and privacy and accuracy of\ninformation being exchanged between nodes. Current technological advancements\nhave already shown promising results and have widen its utilization spectrum by\ncovering almost any field of specialization. With increasing implementation and\ndesign complexity that comes with its enhancements, challenges and issues are\nalso faced that needs to be addressed and considered to mitigate the effects of\nsuch.", "AI": {"tldr": "\u6570\u636e\u91c7\u96c6\u901a\u8fc7\u5b9e\u65f6\u6570\u636e\u6536\u96c6\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u8fd0\u8425\u548c\u7ef4\u62a4\u6210\u672c\uff0c\u5c24\u5176\u662f\u5728\u53ef\u518d\u751f\u80fd\u6e90\u7ba1\u7406\u548c\u8fdc\u7a0b\u76d1\u63a7\u9886\u57df\u3002\u5c3d\u7ba1\u5982\u6b64\uff0c\u6570\u636e\u5b89\u5168\u3001\u9690\u79c1\u548c\u51c6\u786e\u6027\u4ecd\u7136\u662f\u9700\u8981\u89e3\u51b3\u7684\u5173\u952e\u6311\u6218\u3002", "motivation": "\u63d0\u9ad8\u53ef\u518d\u751f\u80fd\u6e90\u7ba1\u7406\u548c\u63a7\u5236\u7684\u6548\u7387\u548c\u53ef\u6301\u7eed\u6027\uff0c\u4ee5\u53ca\u53d1\u5c55\u667a\u80fd\u7cfb\u7edf\u548c\u8fdc\u7a0b\u76d1\u63a7\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u6570\u636e\u91c7\u96c6\u6765\u6539\u8fdb\u73b0\u6709\u7cfb\u7edf\u914d\u7f6e\u548c\u5b9e\u73b0\u3002", "result": "\u6570\u636e\u91c7\u96c6\u5df2\u5728\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u964d\u4f4e\u6210\u672c\u65b9\u9762\u663e\u793a\u51fa\u4f18\u52bf\uff0c\u5e76\u5728\u8fdc\u7a0b\u76d1\u63a7\u4e2d\u5f97\u5230\u5e94\u7528\u3002\u7136\u800c\uff0c\u6570\u636e\u5b89\u5168\u3001\u9690\u79c1\u548c\u51c6\u786e\u6027\u65b9\u9762\u4ecd\u6709\u5f85\u6539\u8fdb\u3002", "conclusion": "\u6570\u636e\u91c7\u96c6\u5728\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3001\u53ef\u9760\u6027\u548c\u964d\u4f4e\u6210\u672c\u65b9\u9762\u5177\u6709\u591a\u91cd\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u53ef\u518d\u751f\u80fd\u6e90\u7ba1\u7406\u548c\u8fdc\u7a0b\u76d1\u63a7\u65b9\u9762\u3002\u7136\u800c\uff0c\u6570\u636e\u5b89\u5168\u3001\u9690\u79c1\u548c\u51c6\u786e\u6027\u4ecd\u662f\u9700\u8981\u89e3\u51b3\u7684\u6311\u6218\u3002"}}
{"id": "2507.09383", "categories": ["cs.RO", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09383", "abs": "https://arxiv.org/abs/2507.09383", "authors": ["Wondmgezahu Teshome", "Kian Behzad", "Octavia Camps", "Michael Everett", "Milad Siami", "Mario Sznaier"], "title": "Real-Time Adaptive Motion Planning via Point Cloud-Guided, Energy-Based Diffusion and Potential Fields", "comment": "Accepted to IEEE RA-L 2025", "summary": "Motivated by the problem of pursuit-evasion, we present a motion planning\nframework that combines energy-based diffusion models with artificial potential\nfields for robust real time trajectory generation in complex environments. Our\napproach processes obstacle information directly from point clouds, enabling\nefficient planning without requiring complete geometric representations. The\nframework employs classifier-free guidance training and integrates local\npotential fields during sampling to enhance obstacle avoidance. In dynamic\nscenarios, the system generates initial trajectories using the diffusion model\nand continuously refines them through potential field-based adaptation,\ndemonstrating effective performance in pursuit-evasion scenarios with partial\npursuer observability.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd0\u52a8\u89c4\u5212\u6846\u67b6\uff0c\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u52bf\u573a\uff0c\u7528\u4e8e\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u65f6\u751f\u6210\u8ffd\u9010-\u9003\u907f\u8f68\u8ff9\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8ffd\u9010-\u9003\u907f\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6269\u6563\u6a21\u578b\u548c\u4eba\u5de5\u52bf\u573a\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u590d\u6742\u73af\u5883\u4e2d\u8fdb\u884c\u9c81\u68d2\u7684\u5b9e\u65f6\u8fd0\u52a8\u89c4\u5212\u3002", "method": "\u7ed3\u5408\u4e86\u57fa\u4e8e\u80fd\u91cf\u7684\u6269\u6563\u6a21\u578b\u548c\u4eba\u5de5\u52bf\u573a\uff0c\u76f4\u63a5\u5904\u7406\u70b9\u4e91\u969c\u788d\u7269\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u5206\u7c7b\u5668\u6307\u5bfc\u8bad\u7ec3\u548c\u5c40\u90e8\u52bf\u573a\u91c7\u6837\u6765\u589e\u5f3a\u907f\u969c\u80fd\u529b\u3002\u5728\u52a8\u6001\u573a\u666f\u4e2d\uff0c\u9996\u5148\u4f7f\u7528\u6269\u6563\u6a21\u578b\u751f\u6210\u521d\u59cb\u8f68\u8ff9\uff0c\u7136\u540e\u901a\u8fc7\u57fa\u4e8e\u52bf\u573a\u7684\u65b9\u6cd5\u8fdb\u884c\u81ea\u9002\u5e94\u8c03\u6574\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u9ad8\u6548\u5730\u8fdb\u884c\u89c4\u5212\uff0c\u800c\u65e0\u9700\u5b8c\u6574\u7684\u51e0\u4f55\u8868\u793a\uff0c\u5e76\u5728\u52a8\u6001\u8ffd\u9010-\u9003\u907f\u573a\u666f\u4e2d\u6709\u6548\u8fd0\u884c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5b9e\u65f6\u8f68\u8ff9\u751f\u6210\uff0c\u7279\u522b\u662f\u5728\u90e8\u5206\u53ef\u89c2\u5bdf\u7684\u8ffd\u9010-\u9003\u8dd1\u573a\u666f\u4e2d\u8868\u73b0\u6709\u6548\u3002"}}
{"id": "2507.09097", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09097", "abs": "https://arxiv.org/abs/2507.09097", "authors": ["Yunsoo Kim", "Jinge Wu", "Honghan Wu"], "title": "RadEyeVideo: Enhancing general-domain Large Vision Language Model for chest X-ray analysis with video representations of eye gaze", "comment": null, "summary": "Large Vision-Language Models (LVLMs) have demonstrated promising performance\nin chest X-ray (CXR) analysis. To enhance human-computer interaction, several\nstudies have incorporated radiologists' eye gaze, typically through heatmaps or\ntextual prompts. However, these methods often overlook the sequential order of\neye movements, which could provide valuable insights by highlighting both the\nareas of interest and the order in which they are examined. In this work, we\npropose a novel approach called RadEyeVideo that integrates radiologists'\neye-fixation data as a video sequence, capturing both the temporal and spatial\ndynamics of their gaze. We evaluate this method in CXR report generation and\ndisease diagnosis using three general-domain, open-source LVLMs with video\ninput capabilities. When prompted with eye-gaze videos, model performance\nimproves by up to 24.6% in the report generation task and on average 15.2% for\nboth tasks using scaled evaluation metrics. Notably, RadEyeVideo enhanced an\nopen-domain LVLM model, LLaVA-OneVision, to surpass task-specific medical LVLMs\nsuch as MAIRA-2 and CheXagent, trained on large Chest X-ray data. This work\nhighlights that domain expert's knowledge (eye-gaze information in this case),\nwhen effectively integrated with LVLMs, can significantly enhance\ngeneral-domain models' capabilities in clinical tasks. RadEyeVideo is a step\ntoward a scalable human-centered approach of utilizing LVLMs in medical image\nanalytics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86RadEyeVideo\u65b9\u6cd5\uff0c\u5c06\u653e\u5c04\u79d1\u533b\u751f\u7684\u773c\u52a8\u89c6\u9891\u5e8f\u5217\u6574\u5408\u5230\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u80f8\u90e8X\u5149\u7247\u5206\u6790\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u62a5\u544a\u751f\u6210\u548c\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4f7f\u901a\u7528\u6a21\u578b\u8d85\u8d8a\u4e86\u4e13\u7528\u533b\u7597\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u773c\u52a8\u4fe1\u606f\u5bf9\u589e\u5f3a\u6a21\u578b\u80fd\u529b\u7684\u4ef7\u503c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6574\u5408\u653e\u5c04\u79d1\u533b\u751f\u773c\u52a8\u6570\u636e\u5230\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u7528\u4e8e\u80f8\u90e8X\u5149\u7247\uff08CXR\uff09\u5206\u6790\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u773c\u52a8\u987a\u5e8f\u8fd9\u4e00\u91cd\u8981\u4fe1\u606f\u3002\u7136\u800c\uff0c\u773c\u52a8\u987a\u5e8f\u80fd\u591f\u63d0\u4f9b\u5173\u4e8e\u533b\u751f\u5173\u6ce8\u533a\u57df\u548c\u5ba1\u89c6\u987a\u5e8f\u7684\u5b9d\u8d35\u89c1\u89e3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRadEyeVideo\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u653e\u5c04\u79d1\u533b\u751f\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u4f5c\u4e3a\u89c6\u9891\u5e8f\u5217\u8fdb\u884c\u6574\u5408\uff0c\u4ee5\u6355\u6349\u89c6\u7ebf\u7684\u65f6\u7a7a\u52a8\u6001\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u79cd\u5177\u6709\u89c6\u9891\u8f93\u5165\u80fd\u529b\u4e14\u9762\u5411\u901a\u7528\u9886\u57df\u7684\u5f00\u6e90LVLMs\u4e0a\u8bc4\u4f30RadEyeVideo\u65b9\u6cd5\uff0c\u53d1\u73b0\u5728\u80f8\u90e8X\u5149\u7247\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u6a21\u578b\u6027\u80fd\u6700\u9ad8\u63d0\u5347\u4e8624.6%\uff1b\u5728\u62a5\u544a\u751f\u6210\u548c\u75be\u75c5\u8bca\u65ad\u4e24\u9879\u4efb\u52a1\u7684\u7efc\u5408\u8bc4\u4f30\u4e2d\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e8615.2%\u3002\u6b64\u5916\uff0c\u6574\u5408\u4e86RadEyeVideo\u65b9\u6cd5\u7684LLaVA-OneVision\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u4e13\u95e8\u9488\u5bf9\u5927\u89c4\u6a21\u80f8\u90e8X\u5149\u7247\u6570\u636e\u8bad\u7ec3\u7684\u7279\u5b9a\u533b\u7597\u6a21\u578bMAIRA-2\u548cCheXagent\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684RadEyeVideo\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u653e\u5c04\u79d1\u533b\u751f\u7684\u773c\u52a8\u8ffd\u8e2a\u6570\u636e\u4ee5\u89c6\u9891\u5e8f\u5217\u7684\u5f62\u5f0f\u6574\u5408\u8fdb\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\uff0c\u80fd\u591f\u6709\u6548\u6355\u6349\u533b\u751f\u89c6\u7ebf\u7684\u65f6\u7a7a\u52a8\u6001\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u80f8\u90e8X\u5149\u7247\u62a5\u544a\u751f\u6210\u548c\u75be\u75c5\u8bca\u65ad\u4efb\u52a1\u4e2d\u5747\u80fd\u663e\u8457\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u5728\u62a5\u544a\u751f\u6210\u4efb\u52a1\u4e2d\u6700\u9ad8\u63d0\u534724.6%\uff0c\u5728\u4e24\u9879\u4efb\u52a1\u7efc\u5408\u8bc4\u4f30\u4e2d\u5e73\u5747\u63d0\u534715.2%\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cRadEyeVideo\u80fd\u591f\u4f7f\u901a\u7528\u9886\u57df\u6a21\u578bLLaVA-OneVision\u7684\u8868\u73b0\u8d85\u8d8a\u4e13\u95e8\u4e3a\u533b\u7597\u9886\u57df\u8bad\u7ec3\u7684\u7279\u5b9a\u6a21\u578b\uff08\u5982MAIRA-2\u548cCheXagent\uff09\uff0c\u8bc1\u660e\u4e86\u9886\u57df\u4e13\u5bb6\u77e5\u8bc6\uff08\u5982\u773c\u52a8\u4fe1\u606f\uff09\u4e0e\u901a\u7528LVLMs\u7684\u6709\u6548\u7ed3\u5408\uff0c\u80fd\u5927\u5e45\u589e\u5f3a\u901a\u7528\u6a21\u578b\u5728\u4e34\u5e8a\u4efb\u52a1\u4e2d\u7684\u80fd\u529b\u3002RadEyeVideo\u662f\u5b9e\u73b0\u4ee5\u4eba\u4e3a\u672c\u3001\u53ef\u6269\u5c55\u7684\u533b\u5b66\u5f71\u50cf\u5206\u6790\u65b9\u6cd5\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.09673", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09673", "abs": "https://arxiv.org/abs/2507.09673", "authors": ["Yizhe Li", "Ziqi Yang", "Ying Chen", "Zhenbo Zhang", "YunLong Tang", "Matthew Smith", "Matthew Lindley", "Xuezhen Cao", "David G. Hopkinson", "Andrew J. Bell", "Steven J. Milne", "Antonio Feteira", "Sarah J. Haigh", "Alexander S. Eggeman", "Juncheng Pan", "Jiajun Shi", "David A. Hall"], "title": "Bulk Ferroelectric Heterostructures for High Temperature Lead-Free Piezoelectrics", "comment": null, "summary": "Remarkable exploitation of valence and lattice mismatch in epitaxial\nferroelectric heterostructures generates physical effects not classically\nexpected for perovskite oxides, such as 2D electron gas and polar skyrmions.\nHowever the widespread application of these interfacial properties and\nfunctionalities is impeded by the ultrathin layered structure and essential\npresence of underlying lattice-matched substrates for the deposition of\nepitaxial thin films. Here, we report a bottom-up pathway to synthesize bulk\nferroelectric heterostructures (BFH) with periodic composition fluctuation (8\nnm in wavelength) using elemental partitioning by cation diffusion, providing\nopportunities to exploit novel characteristics of hetero-epitaxial oxide thin\nfilms in bulk materials. Exemplar monolithic BiFeO3-BaTiO3 BFH ceramics\ndescribed herein share common features with their thin film heterostructure\ncounterparts, which facilitates control and stabilisation of ferroelectric\npolarisation along with a significant enhancement in Curie temperature, Tc, and\nfunctionality. BFH ceramics exhibit a record Tc (up to 824 {\\deg}C) and a\npiezoelectric coefficient (d33 = 115 pC N-1 ), in comparison with other\nperovskite or non-perovskite solid solutions, providing sustainable solutions\nfor emergent high temperature piezoelectric sensing, actuation and energy\nconversion applications. By creating BFH ceramics using different\nelectromechanical boundary conditions, distinct morphologies of aliovalent\nA-site cation segregated regions along with different types of ferroelectric\norder are achieved. This formation mechanism provides unprecedented control\nover local ferroelectric ordering and domain stabilisation in BFH ceramics; it\nalso paves the way to explore new types of functionality, beyond those\nachievable in both bulk ferroelectrics and thin film heterostructures.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u4e0b\u800c\u4e0a\u7684\u5408\u6210\u65b9\u6cd5\u5236\u5907\u4e86\u5757\u72b6\u94c1\u7535\u5f02\u8d28\u7ed3\u6784\u9676\u74f7\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u94c1\u7535\u548c\u538b\u7535\u6027\u80fd\uff0c\u5e76\u4e3a\u63a2\u7d22\u65b0\u529f\u80fd\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4f20\u7edf\u94c1\u7535\u5f02\u8d28\u7ed3\u6784\u4e2d\u8d85\u8584\u5c42\u72b6\u7ed3\u6784\u548c\u5bf9\u6676\u683c\u5339\u914d\u886c\u5e95\u7684\u4f9d\u8d56\u6027\u7b49\u9650\u5236\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u5408\u6210\u9014\u5f84\uff0c\u5c06\u754c\u9762\u7279\u6027\u548c\u529f\u80fd\u6027\u5e94\u7528\u4e8e\u5757\u72b6\u6750\u6599\u4e2d\u3002", "method": "\u8be5\u7814\u7a76\u5229\u7528\u5143\u7d20\u504f\u6790\u548c\u9633\u79bb\u5b50\u6269\u6563\u7684\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\uff0c\u5408\u6210\u4e86\u5177\u6709\u5468\u671f\u6027\u6210\u5206\u6ce2\u52a8\uff08\u6ce2\u957f\u4e3a 8 nm\uff09\u7684\u5757\u72b6\u94c1\u7535\u5f02\u8d28\u7ed3\u6784 (BFH) \u9676\u74f7\u3002", "result": "\u6210\u529f\u5408\u6210\u4e86\u5177\u6709\u5468\u671f\u6027\u6210\u5206\u6ce2\u52a8\u7684\u5757\u72b6\u94c1\u7535\u5f02\u8d28\u7ed3\u6784 (BFH) \u9676\u74f7\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 824 \u00b0C \u7684\u5c45\u91cc\u6e29\u5ea6\u548c 115 pC N\u207b\u00b9 \u7684\u538b\u7535\u7cfb\u6570\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u5c40\u90e8\u94c1\u7535\u5e8f\u548c\u7574\u7a33\u5b9a\u6027\u7684\u524d\u6240\u672a\u6709\u7684\u63a7\u5236\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5143\u7d20\u504f\u6790\u548c\u9633\u79bb\u5b50\u6269\u6563\u7684\u81ea\u4e0b\u800c\u4e0a\u65b9\u6cd5\uff0c\u6210\u529f\u5408\u6210\u4e86\u5177\u6709\u5468\u671f\u6027\u6210\u5206\u6ce2\u52a8\uff08\u6ce2\u957f\u4e3a 8 nm\uff09\u7684\u5757\u72b6\u94c1\u7535\u5f02\u8d28\u7ed3\u6784 (BFH) \u9676\u74f7\u3002\u8fd9\u4e9b BFH \u9676\u74f7\u5728\u94c1\u7535\u6781\u5316\u63a7\u5236\u3001\u5c45\u91cc\u6e29\u5ea6 (Tc) \u548c\u538b\u7535\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5176\u4e2d Tc \u6700\u9ad8\u53ef\u8fbe 824 \u00b0C\uff0c\u538b\u7535\u7cfb\u6570 (d33) \u4e3a 115 pC N\u207b\u00b9\uff0c\u521b\u4e0b\u4e86\u9499\u949b\u77ff\u6216\u975e\u9499\u949b\u77ff\u56fa\u6eb6\u4f53\u7684\u8bb0\u5f55\u3002\u901a\u8fc7\u6539\u53d8\u7535\u673a\u68b0\u8fb9\u754c\u6761\u4ef6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4e0d\u540c\u7684\u9633\u79bb\u5b50\u504f\u6790\u533a\u57df\u5f62\u8c8c\u548c\u94c1\u7535\u5e8f\u7c7b\u578b\u3002\u8fd9\u79cd\u5f62\u6210\u673a\u5236\u4e3a BFH \u9676\u74f7\u7684\u5c40\u90e8\u94c1\u7535\u5e8f\u548c\u7574\u7a33\u5b9a\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u63a7\u5236\u80fd\u529b\uff0c\u5e76\u4e3a\u63a2\u7d22\u8d85\u8d8a\u5757\u72b6\u94c1\u7535\u4f53\u548c\u8584\u819c\u5f02\u8d28\u7ed3\u6784\u7684\u65b0\u578b\u529f\u80fd\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09298", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09298", "abs": "https://arxiv.org/abs/2507.09298", "authors": ["Lipi Patel", "Samarth Hawaldar", "Aditya Panikkar", "Athreya Shankar", "Baladitya Suri"], "title": "Impedance-Engineered Josephson Parametric Amplifier with Single-Step Lithography", "comment": "7 pages main, 4 figures, 11 pages supplementary, 2 supplementary\n  figures, Comments welcome", "summary": "We present the experimental demonstration of an impedance-engineered\nJosephson parametric amplifier (IEJPA) fabricated in a single-step lithography\nprocess. Impedance engineering is implemented using a lumped-element series LC\ncircuit. We use a simpler lithography process where the entire device --\nimpedance transformer and JPA -- are patterned in a single electron beam\nlithography step, followed by a double-angle Dolan bridge technique for\nAl-AlO$_x$-Al deposition. We observe nearly quantum-limited amplification with\n18 dB gain over a wide 400 MHz bandwidth centered around 5.3 GHz, and a\nsaturation power of -114 dBm. To accurately explain our experimental results,\nwe extend existing theories for impedance-engineered JPAs to incorporate the\nfull sine nonlinearity of both the JPA and the transformer. Our work shows a\npath to simpler realization of broadband JPAs and provides a theoretical\nfoundation for a novel regime of JPA operation.", "AI": {"tldr": "\u6211\u4eec\u6f14\u793a\u4e86\u4e00\u79cd\u5355\u6b65\u5149\u523b\u5236\u5907\u7684\u5bbd\u5e26\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u91cf\u5b50\u9650\u7684\u589e\u76ca\u548c\u8f83\u5bbd\u7684\u5e26\u5bbd\u3002\u6211\u4eec\u8fd8\u6269\u5c55\u4e86\u7406\u8bba\u6a21\u578b\u4ee5\u89e3\u91ca\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u66f4\u7b80\u5355\u3001\u5bbd\u5e26\u7684\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\uff0c\u5e76\u4e3a\u65b0\u578b\u64cd\u4f5c\u6a21\u5f0f\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5355\u6b65\u5149\u523b\u5de5\u827a\u5236\u9020\u963b\u6297\u5de5\u7a0b\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\uff08IEJPA\uff09\uff0c\u4f7f\u7528\u96c6\u603b\u53c2\u6570LC\u7535\u8def\u5b9e\u73b0\u963b\u6297\u5de5\u7a0b\u3002\u91c7\u7528\u7535\u5b50\u675f\u5149\u523b\u548c\u53cc\u89d2Dolan\u6865\u6280\u672f\u6c89\u79efAl-AlOx-Al\u3002", "result": "\u5728400 MHz\u5e26\u5bbd\u548c5.3 GHz\u4e2d\u5fc3\u9891\u7387\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u91cf\u5b50\u9650\u768418 dB\u589e\u76ca\uff0c\u9971\u548c\u529f\u7387\u4e3a-114 dBm\u3002\u6269\u5c55\u7684\u7406\u8bba\u6a21\u578b\u80fd\u51c6\u786e\u89e3\u91ca\u5b9e\u9a8c\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u5c55\u793a\u4e86\u4e00\u79cd\u5355\u6b65\u5149\u523b\u5de5\u827a\u5236\u5907\u7684\u963b\u6297\u5de5\u7a0b\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\uff08IEJPA\uff09\u3002\u8be5\u653e\u5927\u5668\u5728400 MHz\u5e26\u5bbd\u548c5.3 GHz\u4e2d\u5fc3\u9891\u7387\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u91cf\u5b50\u9650\u768418 dB\u589e\u76ca\uff0c\u9971\u548c\u529f\u7387\u4e3a-114 dBm\u3002\u901a\u8fc7\u5c06\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\u548c\u53d8\u538b\u5668\u7684\u5b8c\u6574\u6b63\u5f26\u975e\u7ebf\u6027\u7eb3\u5165\u7406\u8bba\u6a21\u578b\uff0c\u6211\u4eec\u4e3a\u5b9e\u9a8c\u7ed3\u679c\u63d0\u4f9b\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u4e3a\u66f4\u7b80\u5355\u5730\u5b9e\u73b0\u5bbd\u5e26\u7ea6\u745f\u592b\u68ee\u53c2\u6570\u653e\u5927\u5668\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.09157", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09157", "abs": "https://arxiv.org/abs/2507.09157", "authors": ["Bhavinkumar Vinodbhai Kuwar", "Bikrant Bikram Pratap Maurya", "Priyanshu Gupta", "Nitin Choudhury"], "title": "PU-Lie: Lightweight Deception Detection in Imbalanced Diplomatic Dialogues via Positive-Unlabeled Learning", "comment": null, "summary": "Detecting deception in strategic dialogues is a complex and high-stakes task\ndue to the subtlety of language and extreme class imbalance between deceptive\nand truthful communications. In this work, we revisit deception detection in\nthe Diplomacy dataset, where less than 5% of messages are labeled deceptive. We\nintroduce a lightweight yet effective model combining frozen BERT embeddings,\ninterpretable linguistic and game-specific features, and a Positive-Unlabeled\n(PU) learning objective. Unlike traditional binary classifiers, PU-Lie is\ntailored for situations where only a small portion of deceptive messages are\nlabeled, and the majority are unlabeled. Our model achieves a new best macro F1\nof 0.60 while reducing trainable parameters by over 650x. Through comprehensive\nevaluations and ablation studies across seven models, we demonstrate the value\nof PU learning, linguistic interpretability, and speaker-aware representations.\nNotably, we emphasize that in this problem setting, accurately detecting\ndeception is more critical than identifying truthful messages. This priority\nguides our choice of PU learning, which explicitly models the rare but vital\ndeceptive class.", "AI": {"tldr": "PU-Lie\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408BERT\u5d4c\u5165\u3001\u8bed\u8a00\u7279\u5f81\u548cPU\u5b66\u4e60\uff0c\u5728Diplomacy\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e86\u6b3a\u9a97\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u6a21\u578b\u5927\u5c0f\u3002", "motivation": "\u5728\u6218\u7565\u5bf9\u8bdd\u4e2d\u68c0\u6d4b\u6b3a\u9a97\u662f\u4e00\u4e2a\u590d\u6742\u4e14\u9ad8\u98ce\u9669\u7684\u4efb\u52a1\uff0c\u56e0\u4e3a\u8bed\u8a00\u7684\u5fae\u5999\u6027\u548c\u6b3a\u9a97\u6027\u4e0e\u771f\u5b9e\u6027\u901a\u4fe1\u4e4b\u95f4\u7684\u6781\u7aef\u7c7b\u522b\u4e0d\u5e73\u8861\u3002\u8be5\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3Diplomacy\u6570\u636e\u96c6\u4e2d\u6b3a\u9a97\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u8be5\u6570\u636e\u96c6\u7684\u6b3a\u9a97\u6027\u6d88\u606f\u6bd4\u4f8b\u4e0d\u52305%\u3002", "method": "PU-Lie\u6a21\u578b\u7ed3\u5408\u4e86\u51bb\u7ed3\u7684BERT\u5d4c\u5165\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u7279\u5f81\u3001\u7279\u5b9a\u4e8e\u6e38\u620f\u7684\u7279\u5f81\u4ee5\u53caPU\uff08Positive-Unlabeled\uff09\u5b66\u4e60\u76ee\u6807\u3002PU\u5b66\u4e60\u662f\u4e00\u79cd\u4e13\u95e8\u4e3a\u53ea\u6709\u5c11\u91cf\u6b3a\u9a97\u6027\u6d88\u606f\u88ab\u6807\u8bb0\u800c\u5927\u90e8\u5206\u6d88\u606f\u672a\u88ab\u6807\u8bb0\u7684\u573a\u666f\u8bbe\u8ba1\u7684\u5b66\u4e60\u65b9\u6cd5\u3002", "result": "PU-Lie\u6a21\u578b\u5728Diplomacy\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.60\u7684\u5b8fF1\u5206\u6570\uff0c\u5e76\u4e14\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11\u4e86650\u500d\u4ee5\u4e0a\u3002\u901a\u8fc7\u5bf9\u4e03\u79cd\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u548c\u6d88\u878d\u7814\u7a76\uff0c\u8bc1\u660e\u4e86PU\u5b66\u4e60\u3001\u8bed\u8a00\u53ef\u89e3\u91ca\u6027\u548c\u8bf4\u8bdd\u8005\u611f\u77e5\u8868\u793a\u7684\u4ef7\u503c\u3002", "conclusion": "PU-Lie\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u51bb\u7ed3\u7684BERT\u5d4c\u5165\u3001\u53ef\u89e3\u91ca\u7684\u8bed\u8a00\u548c\u7279\u5b9a\u4e8e\u6e38\u620f\u7684\u7279\u5f81\u4ee5\u53caPU\u5b66\u4e60\u76ee\u6807\uff0c\u5728Diplomacy\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u4f73\u5b8fF1\u5206\u65700.60\uff0c\u540c\u65f6\u5c06\u53ef\u8bad\u7ec3\u53c2\u6570\u51cf\u5c11\u4e86650\u500d\u4ee5\u4e0a\u3002\u8be5\u6a21\u578b\u7279\u522b\u9002\u7528\u4e8e\u53ea\u6709\u5c11\u91cf\u6b3a\u9a97\u6027\u6d88\u606f\u88ab\u6807\u8bb0\u800c\u5927\u90e8\u5206\u6d88\u606f\u672a\u88ab\u6807\u8bb0\u7684\u60c5\u51b5\uff0c\u5e76\u4f18\u5148\u8003\u8651\u51c6\u786e\u68c0\u6d4b\u6b3a\u9a97\u6027\u6d88\u606f\u3002"}}
{"id": "2507.09683", "categories": ["cs.LG", "cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2507.09683", "abs": "https://arxiv.org/abs/2507.09683", "authors": ["Michael Kearns", "Aaron Roth", "Emily Ryu"], "title": "Networked Information Aggregation via Machine Learning", "comment": null, "summary": "We study a distributed learning problem in which learning agents are embedded\nin a directed acyclic graph (DAG). There is a fixed and arbitrary distribution\nover feature/label pairs, and each agent or vertex in the graph is able to\ndirectly observe only a subset of the features -- potentially a different\nsubset for every agent. The agents learn sequentially in some order consistent\nwith a topological sort of the DAG, committing to a model mapping observations\nto predictions of the real-valued label. Each agent observes the predictions of\ntheir parents in the DAG, and trains their model using both the features of the\ninstance that they directly observe, and the predictions of their parents as\nadditional features. We ask when this process is sufficient to achieve\n\\emph{information aggregation}, in the sense that some agent in the DAG is able\nto learn a model whose error is competitive with the best model that could have\nbeen learned (in some hypothesis class) with direct access to \\emph{all}\nfeatures, despite the fact that no single agent in the network has such access.\nWe give upper and lower bounds for this problem for both linear and general\nhypothesis classes. Our results identify the \\emph{depth} of the DAG as the key\nparameter: information aggregation can occur over sufficiently long paths in\nthe DAG, assuming that all of the relevant features are well represented along\nthe path, and there are distributions over which information aggregation cannot\noccur even in the linear case, and even in arbitrarily large DAGs that do not\nhave sufficient depth (such as a hub-and-spokes topology in which the spoke\nvertices collectively see all the features). We complement our theoretical\nresults with a comprehensive set of experiments.", "AI": {"tldr": "DAG\u4e2d\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u4ee3\u7406\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u7236\u4ee3\u7406\u7684\u9884\u6d4b\u6765\u805a\u5408\u4fe1\u606f\uff0c\u4f46\u4fe1\u606f\u805a\u5408\u7684\u80fd\u529b\u53d6\u51b3\u4e8eDAG\u7684\u6df1\u5ea6\u4ee5\u53ca\u7279\u5f81\u5728\u8def\u5f84\u4e2d\u7684\u8868\u793a\u60c5\u51b5\u3002", "motivation": "\u7814\u7a76\u4e86\u5728\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u4e2d\u5d4c\u5165\u5b66\u4e60\u4ee3\u7406\u7684\u5206\u5e03\u5f0f\u5b66\u4e60\u95ee\u9898\uff0c\u5176\u4e2d\u6bcf\u4e2a\u4ee3\u7406\u53ea\u80fd\u89c2\u5bdf\u5230\u90e8\u5206\u7279\u5f81\uff0c\u5e76\u65e8\u5728\u5b9e\u73b0\u4fe1\u606f\u805a\u5408\u3002", "method": "\u4ee3\u7406\u6309\u4e0eDAG\u7684\u62d3\u6251\u6392\u5e8f\u4e00\u81f4\u7684\u987a\u5e8f\u987a\u5e8f\u5b66\u4e60\uff0c\u5e76\u6839\u636e\u89c2\u5bdf\u7ed3\u679c\u63d0\u4ea4\u6a21\u578b\u4ee5\u9884\u6d4b\u5b9e\u503c\u6807\u7b7e\u3002\u6bcf\u4e2a\u4ee3\u7406\u89c2\u5bdf\u5176\u5728DAG\u4e2d\u7684\u7236\u4ee3\u7406\u7684\u9884\u6d4b\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u76f4\u63a5\u89c2\u5bdf\u5230\u7684\u5b9e\u4f8b\u7279\u5f81\u4ee5\u53ca\u5176\u7236\u4ee3\u7406\u7684\u9884\u6d4b\u4f5c\u4e3a\u9644\u52a0\u7279\u5f81\u6765\u8bad\u7ec3\u5176\u6a21\u578b\u3002", "result": "\u4fe1\u606f\u805a\u5408\u7684\u53d1\u751f\u53d6\u51b3\u4e8eDAG\u7684\u6df1\u5ea6\uff0c\u53ea\u8981\u8def\u5f84\u80fd\u591f\u5145\u5206\u4ee3\u8868\u6240\u6709\u76f8\u5173\u7279\u5f81\u3002\u5373\u4f7f\u5728\u7ebf\u6027\u60c5\u51b5\u4e0b\uff0c\u5728\u4fe1\u606f\u805a\u5408\u65e0\u6cd5\u53d1\u751f\u7684\u5206\u5e03\u4e0a\uff0c\u5373\u4f7f\u5728\u4efb\u610f\u5927\u7684DAG\u4e2d\uff0c\u5982\u679cDAG\u7684\u6df1\u5ea6\u4e0d\u8db3\uff08\u4f8b\u5982\uff0c\u8f90\u6761\u548c\u8f6e\u6bc2\u62d3\u6251\uff09\uff0c\u4e5f\u65e0\u6cd5\u5b9e\u73b0\u4fe1\u606f\u805a\u5408\u3002", "conclusion": "\u8be5\u8fc7\u7a0b\u8db3\u4ee5\u5b9e\u73b0\u4fe1\u606f\u805a\u5408\uff0c\u5373DAG\u4e2d\u7684\u67d0\u4e2a\u4ee3\u7406\u80fd\u591f\u5b66\u4e60\u4e00\u4e2a\u6a21\u578b\uff0c\u5176\u8bef\u5dee\u53ef\u4e0e\u62e5\u6709\u6240\u6709\u7279\u5f81\u7684\u76f4\u63a5\u8bbf\u95ee\u80fd\u529b\u7684\u6700\u4f18\u6a21\u578b\u76f8\u5ab2\u7f8e\u3002\u6211\u4eec\u4e3a\u7ebf\u6027\u548c\u901a\u7528\u5047\u8bbe\u7c7b\u522b\u63d0\u4f9b\u4e86\u4e0a\u9650\u548c\u4e0b\u9650\u3002"}}
{"id": "2507.09713", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09713", "abs": "https://arxiv.org/abs/2507.09713", "authors": ["Burak Ahmet Ozden", "Erdogan Aydin", "Ahmet Elbir", "Filiz Gurkan"], "title": "A New Wireless Image Transmission System Using Code Index Modulation and Image Enhancement for High-Rate Next Generation Networks", "comment": "17 pages, 14 figures", "summary": "With the development of wireless network technologies, the wireless image\ntransmission area has become prominent. The need for high resolution, data\ntraffic density, widespread use of multimedia applications, and the importance\nof high rate and reliable image transmission in medical and military fields\nnecessitate the design of novel and high-performance wireless image\ntransmission systems. This paper proposes a code index modulation (CIM)-based\nimage transmission (CIM-IT) system that utilizes spreading code index and\nquadrature amplitude modulation (QAM) symbol for image transmission over a\nwireless channel. The proposed CIM-IT system maps bits to each pixel value of\nthe image to be transmitted and transmits these bits over a wireless channel\nusing a single-input and multiple-output system comprising code index\nmodulation and QAM techniques. At the receiver, the active spreading code index\nand the selected QAM symbol are estimated using a despreading-based maximum\nlikelihood detector, and the corresponding bits are obtained. The image\nconveyed from the transmitter is then reconstructed at the receiver side using\nthe pixel values corresponding to the bits. The obtained noisy image is\nenhanced using important enhancement filters. In addition, an advanced filter\nis proposed to improve the transmitted degraded image with optimum results.\nFurthermore, error performance, spectral efficiency, energy efficiency, and\nthroughputof the CIM-IT system are performed and the results are compared with\ntraditional wireless communication techniques.", "AI": {"tldr": "A new wireless image transmission system (CIM-IT) is proposed using code index modulation and QAM, outperforming traditional methods in key performance metrics.", "motivation": "The increasing demand for high-resolution, high-rate, and reliable image transmission in multimedia, medical, and military applications drives the need for advanced wireless image transmission systems.", "method": "The proposed system utilizes code index modulation (CIM) and quadrature amplitude modulation (QAM) to map bits to image pixels for transmission over a wireless channel. A despreading-based maximum likelihood detector is used at the receiver to estimate the code index and QAM symbol, followed by image reconstruction and enhancement using filters, including a proposed advanced filter.", "result": "The paper evaluates the error performance, spectral efficiency, energy efficiency, and throughput of the CIM-IT system and compares it with traditional wireless communication techniques.", "conclusion": "The paper proposes a CIM-IT system for wireless image transmission, demonstrating its performance in error rate, spectral efficiency, energy efficiency, and throughput compared to traditional methods."}}
{"id": "2507.08848", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.08848", "abs": "https://arxiv.org/abs/2507.08848", "authors": ["Calum Corrie Imrie", "Ioannis Stefanakos", "Sepeedeh Shahbeigi", "Richard Hawkins", "Simon Burton"], "title": "Assuring the Safety of Reinforcement Learning Components: AMLAS-RL", "comment": null, "summary": "The rapid advancement of machine learning (ML) has led to its increasing\nintegration into cyber-physical systems (CPS) across diverse domains. While CPS\noffer powerful capabilities, incorporating ML components introduces significant\nsafety and assurance challenges. Among ML techniques, reinforcement learning\n(RL) is particularly suited for CPS due to its capacity to handle complex,\ndynamic environments where explicit models of interaction between system and\nenvironment are unavailable or difficult to construct. However, in\nsafety-critical applications, this learning process must not only be effective\nbut demonstrably safe. Safe-RL methods aim to address this by incorporating\nsafety constraints during learning, yet they fall short in providing systematic\nassurance across the RL lifecycle. The AMLAS methodology offers structured\nguidance for assuring the safety of supervised learning components, but it does\nnot directly apply to the unique challenges posed by RL. In this paper, we\nadapt AMLAS to provide a framework for generating assurance arguments for an\nRL-enabled system through an iterative process; AMLAS-RL. We demonstrate\nAMLAS-RL using a running example of a wheeled vehicle tasked with reaching a\ntarget goal without collision.", "AI": {"tldr": "This paper presents AMLAS-RL, a new framework to ensure the safety of machine learning systems in critical applications by adapting existing methods for reinforcement learning.", "motivation": "While Reinforcement Learning (RL) is well-suited for Cyber-Physical Systems (CPS) due to its ability to handle complex and dynamic environments, ensuring the safety of RL components in safety-critical applications is a significant challenge. Existing methods like Safe-RL offer limited assurance across the RL lifecycle, and the AMLAS methodology, while effective for supervised learning, does not directly apply to RL.", "method": "The AMLAS-RL framework is developed by adapting the existing AMLAS methodology. It guides the generation of assurance arguments for RL-enabled systems through an iterative process.", "result": "The paper demonstrates the applicability of the AMLAS-RL framework using a wheeled vehicle example tasked with reaching a target goal without collision, showcasing its utility in assuring the safety of RL-enabled systems.", "conclusion": "The paper introduces AMLAS-RL, a framework adapting the AMLAS methodology to generate assurance arguments for Reinforcement Learning (RL) enabled systems. This is achieved through an iterative process designed to address the unique safety challenges posed by RL in safety-critical applications."}}
{"id": "2507.10323", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.10323", "abs": "https://arxiv.org/abs/2507.10323", "authors": ["Peng-Bin He", "Ri-Xing Wang", "Zai-Dong Li", "Mikhail Cherkasskii"], "title": "Inertial antiferromagnetic resonance driven by spin-orbit torques", "comment": null, "summary": "It is widely accepted that the handedness of a resonant mode is an intrinsic\nproperty. We show that, by tailoring the polarization and handedness of\nalternating spin-orbit torques used as the driving force, the polarization\nstate and handedness of inertial resonant modes in an antiferromagnet can be\nactively controlled. In contrast to ferromagnets, whose resonant-mode\npolarization is essentially fixed, antiferromagnetic inertial modes can\ncontinuously evolve from elliptic through circular to linear polarization as\nthe driving polarization is varied. We further identify an inertia-dependent\ncritical degree of driving polarization at which the mode becomes linearly\npolarized while its handedness reverses.", "AI": {"tldr": "The handedness of resonant modes in antiferromagnets can be controlled by alternating spin-orbit torques, allowing for continuous evolution of polarization states and a critical point for handedness reversal.", "motivation": "To investigate if the handedness of a resonant mode, widely accepted as an intrinsic property, can be actively controlled.", "method": "Tailoring the polarization and handedness of alternating spin-orbit torques to control the polarization state and handedness of inertial resonant modes in an antiferromagnet.", "result": "Antiferromagnetic inertial modes can continuously evolve from elliptic through circular to linear polarization as the driving polarization is varied, and an inertia-dependent critical degree of driving polarization exists at which the mode becomes linearly polarized while its handedness reverses.", "conclusion": "`antiferromagnets` resonant modes can be actively controlled by tailoring the polarization and handedness of alternating spin-orbit torques, with the handedness reversal occurring at a critical degree of driving polarization."}}
{"id": "2507.09540", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09540", "abs": "https://arxiv.org/abs/2507.09540", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.", "AI": {"tldr": "A novel framework uses Metropolis-Hastings sampling to train Spiking Neural Networks (SNNs) for reinforcement learning tasks, overcoming gradient limitations and achieving better results than existing methods on control benchmarks.", "motivation": "To address the challenges of training SNNs for RL tasks, particularly the non-differentiable nature of spike-based communication, and to provide an alternative to gradient-based methods.", "method": "Bayesian inference technique using Metropolis-Hastings (MH) sampling to train SNNs for RL tasks without gradient-based methods. The approach iteratively proposes and probabilistically accepts network parameter updates based on accumulated reward signals.", "result": "The MH-based approach was evaluated on AcroBot and CartPole benchmarks and demonstrated superior performance compared to conventional DQL baselines and prior SNN-based RL approaches.", "conclusion": "MH-based SNN training approach outperforms DQL and other SNN RL methods in maximizing accumulated reward, minimizing network resources and training episodes."}}
{"id": "2507.09960", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.09960", "abs": "https://arxiv.org/abs/2507.09960", "authors": ["Subin Shin", "Seongkyu Jung", "Jinseok Choi", "Jeonghun Park"], "title": "Efficient RF Chain Selection for MIMO Integrated Sensing and Communications: A Greedy Approach", "comment": null, "summary": "In multiple-input multiple-output integrated sensing and communication (MIMO\nISAC) systems, radio frequency chain (i.e., RF chain) selection plays a vital\nrole in reducing hardware cost, power consumption, and computational\ncomplexity. However, designing an effective RF chain selection strategy is\nchallenging due to the disparity in performance metrics between communication\nand sensing-mutual information (MI) versus beam-pattern mean-squared error\n(MSE) or the Cram\\'er-Rao lower bound (CRLB). To overcome this, we propose a\nlow-complexity greedy RF chain selection framework maximizing a unified\nMI-based performance metric applicable to both functions. By decomposing the\ntotal MI into individual contributions of each RF chain, we introduce two\napproaches: greedy eigen-based selection (GES) and greedy cofactor-based\nselection (GCS), which iteratively identify and remove the RF chains with the\nlowest contribution. We further extend our framework to beam selection for\nbeamspace MIMO ISAC systems, introducing diagonal beam selection (DBS) as a\nsimplified solution. Simulation results show that our proposed methods achieve\nnear-optimal performance with significantly lower complexity than exhaustive\nsearch, demonstrating their practical effectiveness for MIMO ISAC systems.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3 MIMO ISAC \u7cfb\u7edf\u4e2d RF \u94fe\u9009\u62e9\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u8d2a\u5a6a\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u5927\u5316\u7edf\u4e00\u7684\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6027\u80fd\u6307\u6807\u6765\u9009\u62e9 RF \u94fe\uff0c\u5e76\u4e3a\u6ce2\u675f\u9009\u62e9\u63d0\u51fa\u4e86\u7b80\u5316\u65b9\u6848\u3002\u4eff\u771f\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u5728\u591a\u8f93\u5165\u591a\u8f93\u51fa\u96c6\u6210\u4f20\u611f\u4e0e\u901a\u4fe1\uff08MIMO ISAC\uff09\u7cfb\u7edf\u4e2d\uff0c\u5c04\u9891\u94fe\uff08RF \u94fe\uff09\u9009\u62e9\u5bf9\u4e8e\u964d\u4f4e\u786c\u4ef6\u6210\u672c\u3001\u529f\u8017\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u901a\u4fe1\u548c\u4f20\u611f\uff08\u4e92\u4fe1\u606f MI \u4e0e\u6ce2\u675f\u6a21\u5f0f\u5747\u65b9\u8bef\u5dee MSE \u6216\u514b\u62c9\u7f8e\u7f57\u4e0b\u754c CRLB\uff09\u4e4b\u95f4\u7684\u6027\u80fd\u6307\u6807\u5dee\u5f02\u4f7f\u5f97\u8bbe\u8ba1\u6709\u6548\u7684\u5c04\u9891\u94fe\u9009\u62e9\u7b56\u7565\u5145\u6ee1\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7684\u8d2a\u5a6a RF \u94fe\u9009\u62e9\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u603b\u4e92\u4fe1\u606f\u5206\u89e3\u4e3a\u6bcf\u4e2a RF \u94fe\u7684\u5355\u72ec\u8d21\u732e\uff0c\u5e76\u5f15\u5165\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a\u8d2a\u5a6a\u7279\u5f81\u503c\u9009\u62e9\uff08GES\uff09\u548c\u8d2a\u5a6a\u4f34\u968f\u5f0f\u9009\u62e9\uff08GCS\uff09\uff0c\u5b83\u4eec\u8fed\u4ee3\u5730\u8bc6\u522b\u5e76\u79fb\u9664\u8d21\u732e\u6700\u4f4e\u7684 RF \u94fe\u3002\u6b64\u5916\uff0c\u8fd8\u5c06\u8be5\u6846\u67b6\u6269\u5c55\u5230\u6ce2\u675f\u9009\u62e9\uff0c\u63d0\u51fa\u4e86\u5bf9\u89d2\u6ce2\u675f\u9009\u62e9\uff08DBS\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u663e\u8457\u964d\u4f4e\u590d\u6742\u5ea6\u7684\u540c\u65f6\u8fbe\u5230\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u8d2a\u5a6a\u65b9\u6cd5\uff08GES \u548c GCS\uff09\u4ee5\u53ca DBS \u5728\u8fd1\u4e4e\u6700\u4f18\u7684\u6027\u80fd\u4e0b\u5b9e\u73b0\u4e86\u663e\u8457\u964d\u4f4e\u7684\u590d\u6742\u5ea6\uff0c\u8bc1\u660e\u4e86\u5b83\u4eec\u5728 MIMO ISAC \u7cfb\u7edf\u4e2d\u7684\u5b9e\u7528\u6709\u6548\u6027\u3002"}}
{"id": "2507.09463", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09463", "abs": "https://arxiv.org/abs/2507.09463", "authors": ["Anoop Kiran", "Nora Ayanian", "Kenneth Breuer"], "title": "Influence of Static and Dynamic Downwash Interactions on Multi-Quadrotor Systems", "comment": "Accepted for publication in Robotics: Science and Systems (RSS) 2025,\n  12 pages, 16 figures", "summary": "Flying multiple quadrotors in close proximity presents a significant\nchallenge due to complex aerodynamic interactions, particularly downwash\neffects that are known to destabilize vehicles and degrade performance.\nTraditionally, multi-quadrotor systems rely on conservative strategies, such as\ncollision avoidance zones around the robot volume, to circumvent this effect.\nThis restricts their capabilities by requiring a large volume for the operation\nof a multi-quadrotor system, limiting their applicability in dense\nenvironments. This work provides a comprehensive, data-driven analysis of the\ndownwash effect, with a focus on characterizing, analyzing, and understanding\nforces, moments, and velocities in both single and multi-quadrotor\nconfigurations. We use measurements of forces and torques to characterize\nvehicle interactions, and particle image velocimetry (PIV) to quantify the\nspatial features of the downwash wake for a single quadrotor and an interacting\npair of quadrotors. This data can be used to inform physics-based strategies\nfor coordination, leverage downwash for optimized formations, expand the\nenvelope of operation, and improve the robustness of multi-quadrotor control.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09102", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09102", "abs": "https://arxiv.org/abs/2507.09102", "authors": ["Yiyang Chen", "Shanshan Zhao", "Lunhao Duan", "Changxing Ding", "Dacheng Tao"], "title": "Harnessing Text-to-Image Diffusion Models for Point Cloud Self-Supervised Learning", "comment": "Accepted by ICCV 2025", "summary": "Diffusion-based models, widely used in text-to-image generation, have proven\neffective in 2D representation learning. Recently, this framework has been\nextended to 3D self-supervised learning by constructing a conditional point\ngenerator for enhancing 3D representations. However, its performance remains\nconstrained by the 3D diffusion model, which is trained on the available 3D\ndatasets with limited size. We hypothesize that the robust capabilities of\ntext-to-image diffusion models, particularly Stable Diffusion (SD), which is\ntrained on large-scale datasets, can help overcome these limitations. To\ninvestigate this hypothesis, we propose PointSD, a framework that leverages the\nSD model for 3D self-supervised learning. By replacing the SD model's text\nencoder with a 3D encoder, we train a point-to-image diffusion model that\nallows point clouds to guide the denoising of rendered noisy images. With the\ntrained point-to-image diffusion model, we use noise-free images as the input\nand point clouds as the condition to extract SD features. Next, we train a 3D\nbackbone by aligning its features with these SD features, thereby facilitating\ndirect semantic learning. Comprehensive experiments on downstream point cloud\ntasks and ablation studies demonstrate that the SD model can enhance point\ncloud self-supervised learning. Code is publicly available at\nhttps://github.com/wdttt/PointSD.", "AI": {"tldr": "\u901a\u8fc7\u5c06Stable Diffusion\u6a21\u578b\u5e94\u7528\u4e8e3D\u70b9\u4e91\u7684\u81ea\u76d1\u7763\u5b66\u4e60\uff0cPointSD\u6846\u67b6\u6210\u529f\u63d0\u5347\u4e863D\u8868\u793a\u7684\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u73b0\u67093D\u6269\u6563\u6a21\u578b\u5728\u5904\u7406\u6709\u9650\u76843D\u6570\u636e\u96c6\u65f6\u6027\u80fd\u53d7\u9650\uff0c\u800c\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff08\u5982Stable Diffusion\uff09\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\uff0c\u5177\u6709\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u6709\u671b\u514b\u670d\u8fd9\u4e9b\u5c40\u9650\u6027\u3002", "method": "PointSD\u6846\u67b6\u9996\u5148\u5c06\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u6587\u672c\u7f16\u7801\u5668\u66ff\u6362\u4e3a3D\u7f16\u7801\u5668\uff0c\u8bad\u7ec3\u70b9\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u70b9\u4e91\u80fd\u591f\u6307\u5bfc\u6e32\u67d3\u7684\u566a\u58f0\u56fe\u50cf\u7684\u53bb\u566a\u3002\u7136\u540e\uff0c\u5229\u7528\u65e0\u566a\u58f0\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u70b9\u4e91\u4f5c\u4e3a\u6761\u4ef6\u6765\u63d0\u53d6Stable Diffusion\u7279\u5f81\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5c063D\u9aa8\u5e72\u7f51\u7edc\u7684\u7279\u5f81\u4e0e\u8fd9\u4e9bStable Diffusion\u7279\u5f81\u5bf9\u9f50\u6765\u8bad\u7ec33D\u9aa8\u5e72\u7f51\u7edc\uff0c\u4ece\u800c\u5b9e\u73b0\u76f4\u63a5\u7684\u8bed\u4e49\u5b66\u4e60\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u548c\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0cStable Diffusion\u6a21\u578b\u80fd\u591f\u63d0\u5347\u70b9\u4e91\u7684\u81ea\u76d1\u7763\u5b66\u4e60\u80fd\u529b\uff0c\u5728\u4e0b\u6e38\u70b9\u4e91\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u63d0\u51fa\u7684PointSD\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578bStable Diffusion\u6765\u589e\u5f3a3D\u8868\u793a\u5b66\u4e60\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.09019", "categories": ["cs.LG", "cs.AI", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.09019", "abs": "https://arxiv.org/abs/2507.09019", "authors": ["Amey Agrawal", "Nitin Kedia", "Anmol Agarwal", "Jayashree Mohan", "Nipun Kwatra", "Souvik Kundu", "Ramachandran Ramjee", "Alexey Tumanov"], "title": "On Evaluating Performance of LLM Inference Serving Systems", "comment": null, "summary": "The rapid evolution of Large Language Model (LLM) inference systems has\nyielded significant efficiency improvements. However, our systematic analysis\nreveals that current evaluation methodologies frequently exhibit fundamental\nflaws, often manifesting as common evaluation anti-patterns that obscure true\nperformance characteristics and impede scientific progress. Through a\ncomprehensive examination of recent systems, we identify recurring\nanti-patterns across three key dimensions: Baseline Fairness, Evaluation Setup,\nand Metric Design. These anti-patterns are uniquely problematic for LLM\ninference due to its dual-phase nature combining distinct prefill and decode\noperations, its handling of highly heterogeneous workloads, and its strict\ntemporal requirements for interactive use. We demonstrate how common\nanti-patterns -- such as inadequate baseline comparisons that conflate\nengineering effort with algorithmic novelty, workload selections that fail to\nrepresent production scenarios, and metric normalizations that hide substantial\nperformance variability like generation stalls-lead to misleading conclusions.\nTo address these challenges, we provide a comprehensive checklist derived from\nour analysis, establishing a framework for recognizing and avoiding these\nanti-patterns in favor of robust LLM inference evaluation. To demonstrate the\npractical application of our framework, we present a case study analyzing\nspeculative decoding, a technique whose bursty, non-uniform token generation is\neasily misinterpreted when evaluated using approaches characteristic of these\nanti-patterns. Our work establishes a rigorous foundation for evaluation\nmethodology, enabling meaningful comparisons, ensuring reproducible results,\nand ultimately accelerating genuine progress in LLM inference systems by moving\nbeyond common anti-patterns to align evaluation with real-world requirements.", "AI": {"tldr": "LLM\u63a8\u7406\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u7f3a\u9677\uff0c\u5e38\u51fa\u73b0\u53cd\u6a21\u5f0f\u3002\u672c\u7814\u7a76\u8bc6\u522b\u4e86\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6539\u8fdb\u6846\u67b6\u548c\u6e05\u5355\uff0c\u4ee5\u5b9e\u73b0\u66f4\u51c6\u786e\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\uff0c\u5e76\u4ee5\u63a8\u6d4b\u89e3\u7801\u4e3a\u4f8b\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "motivation": "\u5f53\u524dLLM\u63a8\u7406\u7cfb\u7edf\u7684\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u7f3a\u9677\uff0c\u5e38\u5e38\u51fa\u73b0\u53cd\u6a21\u5f0f\uff0c\u6a21\u7cca\u4e86\u771f\u5b9e\u7684\u6027\u80fd\u7279\u5f81\uff0c\u963b\u788d\u4e86\u79d1\u5b66\u8fdb\u6b65\u3002LLM\u63a8\u7406\u7684\u7279\u70b9\uff0c\u5982\u53cc\u9636\u6bb5\u6027\u8d28\uff08\u9884\u586b\u5145\u548c\u89e3\u7801\uff09\u3001\u5f02\u6784\u5de5\u4f5c\u8d1f\u8f7d\u4ee5\u53ca\u5bf9\u65f6\u95f4\u654f\u611f\u7684\u4ea4\u4e92\u5f0f\u4f7f\u7528\u9700\u6c42\uff0c\u4f7f\u5f97\u8fd9\u4e9b\u53cd\u6a21\u5f0f\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u901a\u8fc7\u5bf9\u8fd1\u671fLLM\u63a8\u7406\u7cfb\u7edf\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\uff0c\u8bc6\u522b\u51fa\u57fa\u7ebf\u516c\u5e73\u6027\u3001\u8bc4\u4f30\u8bbe\u7f6e\u548c\u6307\u6807\u8bbe\u8ba1\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u4e2d\u5b58\u5728\u7684\u5e38\u89c1\u53cd\u6a21\u5f0f\u3002\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\uff08\u5982\u63a8\u6d4b\u89e3\u7801\uff09\u5c55\u793a\u4e86\u8fd9\u4e9b\u53cd\u6a21\u5f0f\u5982\u4f55\u5bfc\u81f4\u8bef\u5bfc\u6027\u7ed3\u8bba\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u6e05\u5355\u548c\u6846\u67b6\u6765\u8bc6\u522b\u548c\u907f\u514d\u8fd9\u4e9b\u95ee\u9898\u3002", "result": "\u8bc6\u522b\u51faLLM\u63a8\u7406\u8bc4\u4f30\u4e2d\u7684\u5e38\u89c1\u53cd\u6a21\u5f0f\uff0c\u5982\u4e0d\u5145\u5206\u7684\u57fa\u7ebf\u6bd4\u8f83\u3001\u672a\u80fd\u4ee3\u8868\u751f\u4ea7\u73af\u5883\u7684\u5de5\u4f5c\u8d1f\u8f7d\u9009\u62e9\u4ee5\u53ca\u9690\u85cf\u6027\u80fd\u53d8\u5f02\u6027\uff08\u5982\u751f\u6210\u505c\u6ede\uff09\u7684\u6307\u6807\u5f52\u4e00\u5316\u3002\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc6\u522b\u548c\u907f\u514d\u8fd9\u4e9b\u53cd\u6a21\u5f0f\u7684\u5168\u9762\u6e05\u5355\u548c\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5bf9\u63a8\u6d4b\u89e3\u7801\u7684\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8bc6\u522b\u548c\u5206\u6790LLM\u63a8\u7406\u8bc4\u4f30\u4e2d\u7684\u5e38\u89c1\u53cd\u6a21\u5f0f\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u8bc4\u4f30\u65b9\u6cd5\u7684\u6846\u67b6\u548c\u6e05\u5355\uff0c\u4ee5\u786e\u4fdd\u8bc4\u4f30\u7684\u7a33\u5065\u6027\u3001\u53ef\u590d\u73b0\u6027\u548c\u4e0e\u5b9e\u9645\u9700\u6c42\u7684\u5bf9\u9f50\uff0c\u6700\u7ec8\u52a0\u901fLLM\u63a8\u7406\u7cfb\u7edf\u7684\u771f\u6b63\u8fdb\u6b65\u3002"}}
{"id": "2507.09838", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09838", "abs": "https://arxiv.org/abs/2507.09838", "authors": ["Shahriar Muhammad Nahid", "Haiyue Dong", "Gillian Nolan", "Andre Schleife", "SungWoo Nam", "Pinshane Y. Huang", "Nadya Mason", "Arend M. van der Zande"], "title": "Field-effect transistors based on charged domain walls in van der Waals ferroelectric \u03b1-In$_2$Se$_3$", "comment": null, "summary": "Charged domain walls (CDW) in ferroelectrics are emerging as functional\ninterfaces with potential applications in nonvolatile memory, logic, and\nneuromorphic computing. However, CDWs in conventional ferroelectrics are\nvertical, buried, or electrically inaccessible interfaces that prevent their\nuse in functional devices. Here, we overcome these challenges by stacking two\nopposite polar domains of van der Waals ferroelectric $\\alpha$-In$_2$Se$_3$ to\ngenerate artificial head-head (H-H) CDWs and use edge contact to fabricate\ncharged domain wall-based field-effect transistors (CDW-FET). We relate the\natomic structure to the temperature-dependent electrical and magneto-transport\nof the CDW-FET. CDW-FETs exhibit a metal-to-insulator transition with\ndecreasing temperature and enhanced conductance and field-effect mobility\ncompared to single domain $\\alpha$-In$_2$Se$_3$. We identify two regimes of\ntransport: variable range hopping due to disorder in the band edge below 70 K\nand thermally activated interfacial trap-assisted transport above 70 K. The\nCDW-FETs show room-temperature resistance down to 3.1 k$\\Omega$ which is 2-9\norders of magnitude smaller than the single CDW in thin-film ferroelectrics.\nThese results resolve longstanding challenges with high CDW resistance and\ntheir device integration, opening opportunities for gigahertz memory and\nneuromorphic computing.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5236\u5907\u57fa\u4e8e\u8303\u5fb7\u534e\u94c1\u7535\u6750\u6599CDW\u7684\u573a\u6548\u5e94\u6676\u4f53\u7ba1\uff0c\u6210\u529f\u964d\u4f4e\u4e86CDW\u7684\u7535\u963b\u7387\u5e76\u63d0\u9ad8\u4e86\u5668\u4ef6\u6027\u80fd\uff0c\u4e3a\u5f00\u53d1\u65b0\u578b\u5b58\u50a8\u5668\u548c\u8ba1\u7b97\u5668\u4ef6\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "motivation": "\u514b\u670d\u4e86\u4f20\u7edf\u94c1\u7535\u4f53\u4e2dCDW\u5782\u76f4\u3001\u57cb\u85cf\u6216\u7535\u5b66\u4e0d\u53ef\u53ca\u7684\u6311\u6218\uff0c\u4ee5\u5b9e\u73b0CDW\u5728\u975e\u6613\u5931\u6027\u5b58\u50a8\u5668\u3001\u903b\u8f91\u548c\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u5229\u7528\u8fb9\u7f18\u63a5\u89e6\u5236\u5907\u4e86\u57fa\u4e8eCDW\u7684\u573a\u6548\u5e94\u6676\u4f53\u7ba1\uff08CDW-FET\uff09\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u539f\u5b50\u7ed3\u6784\u4e0e\u6e29\u5ea6\u4f9d\u8d56\u7684\u8f93\u8fd0\u6027\u8d28\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "CDW-FETs\u8868\u73b0\u51fa\u968f\u6e29\u5ea6\u964d\u4f4e\u7684\u91d1\u5c5e-\u7edd\u7f18\u4f53\u8f6c\u53d8\uff0c\u5e76\u4e14\u76f8\u6bd4\u4e8e\u5355\u7574 $\\alpha$-In$_2$Se$_3$ \u5177\u6709\u66f4\u9ad8\u7684\u7535\u5bfc\u7387\u548c\u573a\u6548\u5e94\u8fc1\u79fb\u7387\u3002\u5728\u4f4e\u4e8e70 K\u65f6\uff0c\u4f20\u8f93\u673a\u5236\u4e3a\u65e0\u5e8f\u5f15\u8d77\u7684\u53d8\u7a0b\u8df3\u8dc3\uff1b\u5728\u9ad8\u4e8e70 K\u65f6\uff0c\u4f20\u8f93\u673a\u5236\u4e3a\u70ed\u6fc0\u6d3b\u7684\u754c\u9762\u9677\u9631\u8f85\u52a9\u4f20\u8f93\u3002CDW-FETs\u5728\u5ba4\u6e29\u4e0b\u7684\u7535\u963b\u7387\u4f4e\u81f33.1 k$\\Omega$\uff0c\u6bd4\u8584\u819c\u94c1\u7535\u4f53\u4e2d\u7684\u5355CDW\u4f4e2-9\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u901a\u8fc7\u5806\u53e0\u53cd\u5411\u7574\u7684\u8303\u5fb7\u534e\u94c1\u7535\u6750\u6599 $\\alpha$-In$_2$Se$_3$ \u751f\u6210\u4eba\u5de5\u5934-\u5934\uff08H-H\uff09CDW\uff0c\u5e76\u5229\u7528\u8fb9\u7f18\u63a5\u89e6\u5236\u5907\u4e86\u57fa\u4e8eCDW\u7684\u573a\u6548\u5e94\u6676\u4f53\u7ba1\uff08CDW-FET\uff09\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u94c1\u7535\u4f53\u4e2dCDW\u7684\u7535\u963b\u7387\u9ad8\u548c\u5668\u4ef6\u96c6\u6210\u56f0\u96be\u7684\u95ee\u9898\uff0c\u4e3a\u5b9e\u73b0\u9ad8\u9891\u5b58\u50a8\u5668\u548c\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.09339", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09339", "abs": "https://arxiv.org/abs/2507.09339", "authors": ["Alba Torras-Coloma", "Luca Cozzolino", "Ariadna G\u00f3mez-del-Pulgar-Mart\u00ednez", "Elia Bertoldo", "P. Forn-D\u00edaz"], "title": "Superinductor-based ultrastrong coupling in a superconducting circuit", "comment": "11 pages, 10 figures", "summary": "We present an ultrastrong superinductor-based coupling consisting of a flux\nqubit galvanically coupled to a resonator. The coupling inductor is fabricated\nin granular Aluminum, a superinductor material able to provide large surface\ninductances. Spectroscopy measurements on the qubit-resonator system reveal a\nBloch-Siegert shift of \\SI{23}{\\mega\\hertz} and a coupling fraction of\n$g/\\omega_r \\simeq 0.13$, entering the perturbative ultrastrong coupling (USC)\nregime. We estimate the inductance of the coupler independently by\nlow-temperature resistance measurements providing $L_c =\n(0.74\\pm0.14)\\,\\mathrm{nH}$, which is compatible with $g/\\omega \\gtrsim 0.1$.\nOur results show that superinductors are a promising tool to study USC physics\nin high-coherence circuits using flux qubits with small loop areas and low\npersistent currents.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u7a2e\u57fa\u65bc\u8d85\u5c0e\u9ad4\u7684\u8d85\u5f37\u8026\u5408\uff0c\u7528\u65bc\u7814\u7a76\u5177\u6709\u5c0f\u56de\u8def\u9762\u7a4d\u548c\u4f4e\u6301\u4e45\u96fb\u6d41\u7684\u78c1\u901a\u91cf\u5b50\u6bd4\u7279\u7684\u8d85\u5f37\u8026\u5408\u7269\u7406\u5b78\u3002", "motivation": "\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u8d85\u5c0e\u9ad4\u7684\u8d85\u5f37\u8026\u5408\uff0c\u8a72\u8026\u5408\u7531\u4e00\u500b\u6025\u5287\u8026\u5408\u5230\u8ae7\u632f\u5668\u7684\u78c1\u901a\u91cf\u5b50\u6bd4\u7279\u7d44\u6210\uff0c\u7528\u65bc\u7814\u7a76\u8d85\u5f37\u8026\u5408\u7269\u7406\u5b78\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u500b\u57fa\u65bc\u8d85\u5c0e\u9ad4\u7684\u8d85\u5f37\u8026\u5408\uff0c\u8a72\u8026\u5408\u7531\u4e00\u500b\u6025\u5287\u8026\u5408\u5230\u8ae7\u632f\u5668\u7684\u78c1\u901a\u91cf\u5b50\u6bd4\u7279\u7d44\u6210\u3002", "result": "\u975e\u5fae\u64fe\u8d85\u5f37\u8026\u5408\uff08USC\uff09\u7684\u8b49\u660e\u3002\u6211\u5011\u6e2c\u91cf\u4e86 Bloch-Siegert \u4f4d\u79fb\u548c\u8026\u5408\u7387 $g/\begin{smallmatrix} \\omega \\end{smallmatrix}_r \\approx 0.13$\uff0c\u9032\u5165\u4e86 USC \u72c0\u614b\u3002\u6211\u5011\u7368\u7acb\u4f30\u8a08\u4e86\u8026\u5408\u5668\u7684\u96fb\u611f\uff0c\u5176\u503c\u8207 $g/\\omega \\gtrsim 0.1$ \u7684\u689d\u4ef6\u4e00\u81f4\u3002", "conclusion": "\u8d85\u5c0e\u9ad4\u662f\u7814\u7a76\u5177\u6709\u5c0f\u56de\u8def\u9762\u7a4d\u548c\u4f4e\u6301\u4e45\u96fb\u6d41\u7684\u78c1\u901a\u91cf\u5b50\u6bd4\u7279\u7684\u8d85\u5f37\u8026\u5408\u7269\u7406\u5b78\u7684\u6709\u524d\u9014\u7684\u5de5\u5177\u3002"}}
{"id": "2507.09174", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09174", "abs": "https://arxiv.org/abs/2507.09174", "authors": ["Shuo Yang", "Zijian Yu", "Zhenzhe Ying", "Yuqin Dai", "Guoqing Wang", "Jun Lan", "Jinfeng Xu", "Jinze Li", "Edith C. H. Ngai"], "title": "RAMA: Retrieval-Augmented Multi-Agent Framework for Misinformation Detection in Multimodal Fact-Checking", "comment": null, "summary": "The rapid proliferation of multimodal misinformation presents significant\nchallenges for automated fact-checking systems, especially when claims are\nambiguous or lack sufficient context. We introduce RAMA, a novel\nretrieval-augmented multi-agent framework designed for verifying multimedia\nmisinformation. RAMA incorporates three core innovations: (1) strategic query\nformulation that transforms multimodal claims into precise web search queries;\n(2) cross-verification evidence aggregation from diverse, authoritative\nsources; and (3) a multi-agent ensemble architecture that leverages the\ncomplementary strengths of multiple multimodal large language models and prompt\nvariants. Extensive experiments demonstrate that RAMA achieves superior\nperformance on benchmark datasets, particularly excelling in resolving\nambiguous or improbable claims by grounding verification in retrieved factual\nevidence. Our findings underscore the necessity of integrating web-based\nevidence and multi-agent reasoning for trustworthy multimedia verification,\npaving the way for more reliable and scalable fact-checking solutions. RAMA\nwill be publicly available at https://github.com/kalendsyang/RAMA.git.", "AI": {"tldr": "RAMA\u662f\u4e00\u4e2a\u591a\u4e3b\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u7f51\u7edc\u641c\u7d22\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6765\u9a8c\u8bc1\u591a\u5a92\u4f53\u9519\u8bef\u4fe1\u606f\uff0c\u5c24\u5176\u64c5\u957f\u5904\u7406\u6a21\u7cca\u58f0\u660e\u3002", "motivation": "\u591a\u6a21\u6001\u9519\u8bef\u4fe1\u606f\u7684\u5feb\u901f\u6269\u6563\u7ed9\u81ea\u52a8\u4e8b\u5b9e\u6838\u67e5\u7cfb\u7edf\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\uff0c\u7279\u522b\u662f\u5f53\u58f0\u660e\u6a21\u68f1\u4e24\u53ef\u6216\u7f3a\u4e4f\u8db3\u591f\u4e0a\u4e0b\u6587\u65f6\u3002", "method": "RAMA\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u68c0\u7d22\u589e\u5f3a\u578b\u591a\u4e3b\u4f53\u6846\u67b6\uff0c\u7528\u4e8e\u9a8c\u8bc1\u591a\u5a92\u4f53\u9519\u8bef\u4fe1\u606f\u3002RAMA\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u521b\u65b0\uff1a(1)\u5c06\u591a\u6a21\u6001\u58f0\u660e\u8f6c\u5316\u4e3a\u7cbe\u786e\u7684\u7f51\u7edc\u641c\u7d22\u67e5\u8be2\u7684\u6218\u7565\u67e5\u8be2\u5236\u5b9a\uff1b(2)\u4ece\u591a\u6837\u5316\u3001\u6743\u5a01\u6027\u6765\u6e90\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\u8bc1\u636e\u805a\u5408\uff1b(3)\u5229\u7528\u591a\u4e2a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u63d0\u793a\u53d8\u4f53\u7684\u4e92\u8865\u4f18\u52bf\u7684\u591a\u4e3b\u4f53\u96c6\u6210\u67b6\u6784\u3002", "result": "RAMA\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u901a\u8fc7\u5c06\u9a8c\u8bc1\u4e0e\u68c0\u7d22\u5230\u7684\u4e8b\u5b9e\u8bc1\u636e\u76f8\u7ed3\u5408\u6765\u89e3\u51b3\u6a21\u68f1\u4e24\u53ef\u6216\u4e0d\u592a\u53ef\u80fd\u7684\u58f0\u660e\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u96c6\u6210\u7f51\u7edc\u8bc1\u636e\u548c\u591a\u4e3b\u4f53\u63a8\u7406\u5bf9\u4e8e\u53ef\u4fe1\u7684\u591a\u5a92\u4f53\u9a8c\u8bc1\u662f\u5fc5\u8981\u7684\uff0c\u4e3a\u66f4\u53ef\u9760\u548c\u53ef\u6269\u5c55\u7684\u4e8b\u5b9e\u6838\u67e5\u89e3\u51b3\u65b9\u6848\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09836", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09836", "abs": "https://arxiv.org/abs/2507.09836", "authors": ["Vindula Jayawardana", "Sirui Li", "Yashar Farid", "Cathy Wu"], "title": "Multi-residual Mixture of Experts Learning for Cooperative Control in Multi-vehicle Systems", "comment": null, "summary": "Autonomous vehicles (AVs) are becoming increasingly popular, with their\napplications now extending beyond just a mode of transportation to serving as\nmobile actuators of a traffic flow to control flow dynamics. This contrasts\nwith traditional fixed-location actuators, such as traffic signals, and is\nreferred to as Lagrangian traffic control. However, designing effective\nLagrangian traffic control policies for AVs that generalize across traffic\nscenarios introduces a major challenge. Real-world traffic environments are\nhighly diverse, and developing policies that perform robustly across such\ndiverse traffic scenarios is challenging. It is further compounded by the joint\ncomplexity of the multi-agent nature of traffic systems, mixed motives among\nparticipants, and conflicting optimization objectives subject to strict\nphysical and external constraints. To address these challenges, we introduce\nMulti-Residual Mixture of Expert Learning (MRMEL), a novel framework for\nLagrangian traffic control that augments a given suboptimal nominal policy with\na learned residual while explicitly accounting for the structure of the traffic\nscenario space. In particular, taking inspiration from residual reinforcement\nlearning, MRMEL augments a suboptimal nominal AV control policy by learning a\nresidual correction, but at the same time dynamically selects the most suitable\nnominal policy from a pool of nominal policies conditioned on the traffic\nscenarios and modeled as a mixture of experts. We validate MRMEL using a case\nstudy in cooperative eco-driving at signalized intersections in Atlanta, Dallas\nFort Worth, and Salt Lake City, with real-world data-driven traffic scenarios.\nThe results show that MRMEL consistently yields superior performance-achieving\nan additional 4%-9% reduction in aggregate vehicle emissions relative to the\nstrongest baseline in each setting.", "AI": {"tldr": "MRMEL\u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u62c9\u683c\u6717\u65e5\u4ea4\u901a\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u6b8b\u5dee\u5b66\u4e60\u548c\u4e13\u5bb6\u6df7\u5408\u6765\u63d0\u9ad8\u7b56\u7565\u5728\u591a\u6837\u5316\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\uff0c\u5e76\u5728\u751f\u6001\u9a7e\u9a76\u4efb\u52a1\u4e2d\u663e\u8457\u51cf\u5c11\u4e86\u8f66\u8f86\u6392\u653e\u3002", "motivation": "\u8bbe\u8ba1\u80fd\u591f\u6cdb\u5316\u5230\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u7684\u6709\u6548\u62c9\u683c\u6717\u65e5\u4ea4\u901a\u63a7\u5236\u7b56\u7565\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u771f\u5b9e\u4ea4\u901a\u73af\u5883\u9ad8\u5ea6\u591a\u6837\u5316\uff0c\u4e14\u4ea4\u901a\u7cfb\u7edf\u5177\u6709\u591a\u667a\u80fd\u4f53\u3001\u6df7\u5408\u52a8\u673a\u4ee5\u53ca\u53d7\u7269\u7406\u548c\u5916\u90e8\u7ea6\u675f\u7684\u51b2\u7a81\u4f18\u5316\u76ee\u6807\u7b49\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u6b8b\u5dee\u4e13\u5bb6\u6df7\u5408\u5b66\u4e60\uff08MRMEL\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u62c9\u683c\u6717\u65e5\u4ea4\u901a\u63a7\u5236\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u6b8b\u5dee\u6765\u589e\u5f3a\u73b0\u6709\u7684\u6b21\u4f18\u540d\u4e49\u7b56\u7565\uff0c\u5e76\u663e\u5f0f\u8003\u8651\u4ea4\u901a\u573a\u666f\u7a7a\u95f4\u7684\u7ed3\u6784\u3002MRMEL\u501f\u9274\u4e86\u6b8b\u5dee\u5f3a\u5316\u5b66\u4e60\u7684\u601d\u60f3\uff0c\u901a\u8fc7\u5b66\u4e60\u6b8b\u5dee\u6821\u6b63\u6765\u589e\u5f3a\u6b21\u4f18\u7684\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AV\uff09\u63a7\u5236\u7b56\u7565\uff0c\u540c\u65f6\u6839\u636e\u4ea4\u901a\u573a\u666f\u52a8\u6001\u9009\u62e9\u6700\u5408\u9002\u7684\u7b56\u7565\uff0c\u8fd9\u4e9b\u7b56\u7565\u88ab\u5efa\u6a21\u4e3a\u4e13\u5bb6\u6df7\u5408\u3002", "result": "MRMEL\u6846\u67b6\u5728\u5408\u4f5c\u751f\u6001\u9a7e\u9a76\u573a\u666f\u4e0b\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5728\u4e9a\u7279\u5170\u5927\u3001\u8fbe\u62c9\u65af-\u6c83\u65af\u5821\u548c\u76d0\u6e56\u57ce\u4f7f\u7528\u4e86\u771f\u5b9e\u6570\u636e\u9a71\u52a8\u7684\u4ea4\u901a\u573a\u666f\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMRMEL\u5728\u6bcf\u4e2a\u573a\u666f\u4e0b\u76f8\u5bf9\u4e8e\u6700\u5f3a\u7684\u57fa\u7ebf\u90fd\u80fd\u6301\u7eed\u83b7\u5f97\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u989d\u59164%-9%\u7684\u805a\u5408\u8f66\u8f86\u51cf\u6392\u3002", "conclusion": "MRMEL\u6846\u67b6\u901a\u8fc7\u5b66\u4e60\u6b8b\u5dee\u4fee\u6b63\u5e76\u52a8\u6001\u9009\u62e9\u6700\u9002\u5408\u7684\u4e13\u5bb6\u7b56\u7565\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4ea4\u901a\u73af\u5883\u7684\u591a\u6837\u6027\u548c\u590d\u6742\u6027\uff0c\u5e76\u5728\u5408\u4f5c\u751f\u6001\u9a7e\u9a76\u573a\u666f\u4e2d\u5b9e\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u6027\u80fd\uff0c\u989d\u5916\u51cf\u5c114%-9%\u7684\u8f66\u8f86\u6392\u653e\u3002"}}
{"id": "2507.08858", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08858", "abs": "https://arxiv.org/abs/2507.08858", "authors": ["Sami Achour", "Yassine Bouher", "Duong Nguyen", "Nicolas Chesneau"], "title": "Foundation models for time series forecasting: Application in conformal prediction", "comment": null, "summary": "The zero-shot capabilities of foundation models (FMs) for time series\nforecasting offer promising potentials in conformal prediction, as most of the\navailable data can be allocated to calibration. This study compares the\nperformance of Time Series Foundation Models (TSFMs) with traditional methods,\nincluding statistical models and gradient boosting, within a conformal\nprediction setting. Our findings highlight two key advantages of TSFMs. First,\nwhen the volume of data is limited, TSFMs provide more reliable conformalized\nprediction intervals than classic models, thanks to their superior predictive\naccuracy. Second, the calibration process is more stable because more data are\nused for calibration. Morever, the fewer data available, the more pronounced\nthese benefits become, as classic models require a substantial amount of data\nfor effective training. These results underscore the potential of foundation\nmodels in improving conformal prediction reliability in time series\napplications, particularly in data-constrained cases. All the code to reproduce\nthe experiments is available.", "AI": {"tldr": "Foundation models for time series forecasting offer better conformal prediction intervals than traditional methods, especially with limited data, due to higher accuracy and more stable calibration.", "motivation": "To explore the potential of foundation models (FMs) in time series forecasting, particularly their zero-shot capabilities in conformal prediction, by comparing their performance against traditional methods.", "method": "This study compares the performance of Time Series Foundation Models (TSFMs) with traditional methods (statistical models, gradient boosting) within a conformal prediction framework. The comparison focuses on the reliability of prediction intervals generated by these methods.", "result": "TSFMs demonstrate two key advantages over traditional methods: 1. More reliable conformalized prediction intervals when data volume is limited, attributed to higher predictive accuracy. 2. More stable calibration process due to the use of more data for calibration. These benefits are more significant when less data is available, as traditional models require substantial data for effective training.", "conclusion": "Foundation models (FMs) have the potential to improve the reliability of conformal prediction in time series applications, especially in data-constrained scenarios. TSFMs provide more reliable prediction intervals than traditional methods when data is limited, due to their superior accuracy and more stable calibration process facilitated by larger calibration datasets."}}
{"id": "2507.10366", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.10366", "abs": "https://arxiv.org/abs/2507.10366", "authors": ["Jie Li", "Chen-Xin Jiang", "Zi-Xiang Hu"], "title": "Dynamics of fractional quantum Hall Liquids with a pulse at the edge", "comment": "9 pages, 8 figures", "summary": "Motivated by recent experimental advancements in scanning optical\nstroboscopic confocal microscopy and spectroscopy measurements, which have\nfacilitated exceptional energy-space-time resolution for investigating edge and\nbulk dynamics in fractional quantum Hall systems, we formulated a model for the\npump-probe process on the edge. Starting with a ground state, we applied a tip\npotential near the fractional quantum Hall liquid edge, which was subsequently\nturned off after a defined time duration. By examining how the specific nature\nof the tip potential influences the evolution of the wave function and its\ndistribution in energy spectrum, we identify that quench dynamics of the edge\npulse leads to excitations that spread both along the edge and perpendicularly\ninto the bulk. Moreover, magnetoroton excitations are predominant among the\nbulk excitations. These results align well with the experimental observations.\nFurthermore, we analyzed the effects of the tip's position, intensity, and\nduration on the dynamics.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u62df\u6cf5\u6d66-\u63a2\u6d4b\u8fc7\u7a0b\uff0c\u7814\u7a76\u4e86\u5206\u6570\u91cf\u5b50\u970d\u5c14\u7cfb\u7edf\u8fb9\u7f18\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u6fc0\u53d1\u4f1a\u8fdb\u5165\u4f53\u76f8\uff0c\u78c1\u8f6c\u5b50\u662f\u4e3b\u8981\u6fc0\u53d1\u6a21\u5f0f\u3002", "motivation": "\u8fd1\u671f\u626b\u63cf\u5149\u5b66\u9891\u95ea\u5171\u805a\u7126\u663e\u5fae\u955c\u548c\u5149\u8c31\u6d4b\u91cf\u5b9e\u9a8c\u8fdb\u5c55\u6fc0\u53d1\u4e86\u5bf9\u5206\u6570\u91cf\u5b50\u970d\u5c14\u7cfb\u7edf\u8fb9\u7f18\u548c\u4f53\u76f8\u52a8\u529b\u5b66\u7684\u7814\u7a76\u5174\u8da3\u3002", "method": "\u901a\u8fc7\u6cf5\u6d66-\u63a2\u6d4b\u8fc7\u7a0b\u548c\u8fb9\u7f18\u7684\u5c16\u7aef\u52bf\u6a21\u62df\u4e86\u80fd\u91cf\u7a7a\u95f4\u65f6\u95f4\u5206\u8fa8\u7387\uff0c\u7814\u7a76\u4e86\u5176\u5bf9\u6ce2\u51fd\u6570\u6f14\u5316\u548c\u80fd\u91cf\u8c31\u5206\u5e03\u7684\u5f71\u54cd\u3002", "result": "\u8fb9\u7f18\u8109\u51b2\u6dec\u706d\u52a8\u529b\u5b66\u5bfc\u81f4\u6fc0\u53d1\u6cbf\u8fb9\u7f18\u4f20\u64ad\u5e76\u8fdb\u5165\u4f53\u76f8\uff0c\u78c1\u8f6c\u5b50\u6fc0\u53d1\u662f\u4f53\u76f8\u6fc0\u53d1\u7684\u4f18\u52bf\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u63a2\u9488\u7684\u4f4d\u7f6e\u3001\u5f3a\u5ea6\u548c\u6301\u7eed\u65f6\u95f4\u7684\u5f71\u54cd\u3002", "conclusion": "\u6a21\u578b\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u543b\u5408\uff0c\u63ed\u793a\u4e86\u6cf5\u6d66-\u63a2\u6d4b\u8fc7\u7a0b\u4e2d\u8fb9\u7f18\u8109\u51b2\u6dec\u706d\u52a8\u529b\u5b66\u5f15\u53d1\u7684\u6fc0\u53d1\u6cbf\u8fb9\u7f18\u4f20\u64ad\u5e76\u8fdb\u5165\u4f53\u76f8\uff0c\u5176\u4e2d\u78c1\u8f6c\u5b50\u6fc0\u53d1\u662f\u4f53\u76f8\u6fc0\u53d1\u7684\u4f18\u52bf\u6a21\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u63a2\u9488\u4f4d\u7f6e\u3001\u5f3a\u5ea6\u548c\u6301\u7eed\u65f6\u95f4\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.09588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09588", "abs": "https://arxiv.org/abs/2507.09588", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation", "comment": null, "summary": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance.", "AI": {"tldr": "eSapiens\u662f\u4e00\u4e2aAIaaS\u5e73\u53f0\uff0c\u901a\u8fc7\u6570\u636e\u3001\u5de5\u4f5c\u6d41\u548cLLM\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u5bf9AI\u7684\u63a7\u5236\uff0c\u4ee5\u63d0\u9ad8\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4f01\u4e1a\u63d0\u4f9b\u5bf9\u4eba\u5de5\u667a\u80fd\u8d44\u4ea7\u7684\u5b8c\u5168\u63a7\u5236\uff0c\u5c06\u6240\u6709\u5185\u5bb9\u4fdd\u7559\u5728\u5185\u90e8\uff0c\u4ee5\u5b9e\u73b0\u4eba\u5de5\u667a\u80fd\u77e5\u8bc6\u4fdd\u7559\u548c\u6570\u636e\u5b89\u5168\u3002eSapiens AI Agents (Sapiens)\u901a\u8fc7\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u89c1\u89e3\u548c\u81ea\u52a8\u5316\u91cd\u590d\u6027\u4efb\u52a1\u6765\u589e\u5f3a\u56e2\u961f\u80fd\u529b\uff0c\u4f7f\u4ed6\u4eec\u80fd\u591f\u4e13\u6ce8\u4e8e\u9ad8\u5f71\u54cd\u529b\u5de5\u4f5c\u5e76\u5e26\u6765\u66f4\u597d\u7684\u4e1a\u52a1\u6210\u679c\u3002", "method": "eSapiens\u662f\u4e00\u4e2a\u56f4\u7ed5\u4e13\u6709\u6570\u636e\u3001\u8fd0\u8425\u5de5\u4f5c\u6d41\u548c\u4efb\u4f55\u4e3b\u8981\u81ea\u9002\u5e94\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6784\u5efa\u7684AI\u5373\u670d\u52a1\uff08AIaaS\uff09\u5e73\u53f0\u3002\u5b83\u96c6\u6210\u4e86\u7ed3\u6784\u5316\u6587\u6863\u6444\u53d6\u3001\u6df7\u5408\u5411\u91cf\u68c0\u7d22\u548c\u901a\u8fc7LangChain\u5b9e\u73b0\u7684\u65e0\u4ee3\u7801\u7f16\u6392\uff0c\u5e76\u652f\u6301OpenAI\u3001Claude\u3001Gemini\u548cDeepSeek\u7b49\u9876\u7ea7LLM\u3002THOR Agent\u5904\u7406\u7ed3\u6784\u5316SQL\u98ce\u683c\u67e5\u8be2\uff0c\u5e76\u5728\u4f01\u4e1a\u6570\u636e\u5e93\u4e0a\u751f\u6210\u53ef\u64cd\u4f5c\u7684\u89c1\u89e3\u3002", "result": "\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u663e\u793a\uff0c512\u4e2a\u6807\u8bb0\u7684\u5757\u5927\u5c0f\u53ef\u5b9e\u73b0\u6700\u9ad8\u68c0\u7d22\u7cbe\u5ea6\uff08Top-3\u51c6\u786e\u7387\uff1a91.3%\uff09\u3002\u4f7f\u7528TRACe\u6307\u6807\u8fdb\u884c\u7684\u751f\u6210\u8d28\u91cf\u6d4b\u8bd5\u8868\u660e\uff0ceSapiens\u53ef\u63d0\u4f9b\u66f4\u591a\u4e0a\u4e0b\u6587\u4e00\u81f4\u7684\u8f93\u51fa\uff0c\u4e8b\u5b9e\u4e00\u81f4\u6027\u6700\u591a\u63d0\u9ad823%\u3002", "conclusion": "eSapiens\u5728\u6cd5\u5f8b\u548c\u91d1\u878d\u7b49\u9ad8\u98ce\u9669\u9886\u57df\u5b9e\u73b0\u4e86\u503c\u5f97\u4fe1\u8d56\u3001\u53ef\u5ba1\u6838\u7684\u4eba\u5de5\u667a\u80fd\u5de5\u4f5c\u6d41\u3002"}}
{"id": "2507.09997", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09997", "abs": "https://arxiv.org/abs/2507.09997", "authors": ["Venkatraman Renganathan", "Sabyasachi Mondal", "Antonios Tsourdos"], "title": "Predictive & Trust-based Multi-Agent Coordination", "comment": null, "summary": "This paper presents a trust-based predictive multi-agent consensus protocol\nthat analyses neighbours' anticipation data and makes coordination decisions.\nAgents in the network share their future predicted data over a finite\nlook-ahead horizon with their neighbours and update their predictions in a\nrolling-horizon fashion. The prediction data is then used by agents to learn\nboth the trust and the commitment traits exhibited by their neighbours over\ntime. The proposed protocol is named as the Anticipatory Distributed\nCoordination (ADC) protocol. Lyapunov theory-based agreement convergence\nbetween agents is provided, followed by demonstrations using numerical\nsimulations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aADC\u7684\u9884\u6d4b\u6027\u591a\u4e3b\u4f53\u5171\u8bc6\u534f\u8bae\uff0c\u901a\u8fc7\u5206\u6790\u90bb\u5c45\u7684\u9884\u671f\u6570\u636e\u6765\u5b66\u4e60\u4fe1\u4efb\u548c\u627f\u8bfa\uff0c\u5e76\u5b9e\u73b0\u4ee3\u7406\u95f4\u7684\u534f\u8c03\u548c\u5171\u8bc6\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u7f51\u7edc\u4e2d\u4ee3\u7406\u95f4\u7684\u6709\u6548\u534f\u8c03\u548c\u4fe1\u4efb\u5efa\u7acb\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5171\u8bc6\u534f\u8bae\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fe1\u4efb\u7684\u9884\u6d4b\u6027\u591a\u4e3b\u4f53\u5171\u8bc6\u534f\u8bae\uff08ADC\u534f\u8bae\uff09\uff0c\u8be5\u534f\u8bae\u5206\u6790\u90bb\u5c45\u7684\u9884\u671f\u6570\u636e\u5e76\u505a\u51fa\u534f\u8c03\u51b3\u7b56\u3002\u4ee3\u7406\u5728\u6709\u9650\u7684\u524d\u77bb\u89c6\u754c\u5185\u4e0e\u5176\u90bb\u5c45\u5171\u4eab\u5176\u672a\u6765\u7684\u9884\u6d4b\u6570\u636e\uff0c\u5e76\u4ee5\u6eda\u52a8\u89c6\u754c\u7684\u65b9\u5f0f\u66f4\u65b0\u5176\u9884\u6d4b\u3002\u7136\u540e\uff0c\u4ee3\u7406\u5229\u7528\u9884\u6d4b\u6570\u636e\u968f\u65f6\u95f4\u5b66\u4e60\u90bb\u5c45\u8868\u73b0\u51fa\u7684\u4fe1\u4efb\u548c\u627f\u8bfa\u7279\u5f81\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u5c55\u793a\u4e86\u8be5\u534f\u8bae\u7684\u6709\u6548\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u4e0a\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u534f\u8bae\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8eLyapunov\u7406\u8bba\u7684\u4ee3\u7406\u95f4\u4e00\u81f4\u6027\u6536\u655b\u6027\u8bc1\u660e\u3002"}}
{"id": "2507.09464", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09464", "abs": "https://arxiv.org/abs/2507.09464", "authors": ["Azfar Azdi Arfakhsyad", "Aufa Nasywa Rahman", "Larasati Kinanti", "Ahmad Ataka Awwalur Rizqi", "Hannan Nur Muhammad"], "title": "Unmanned Aerial Vehicle (UAV) Data-Driven Modeling Software with Integrated 9-Axis IMUGPS Sensor Fusion and Data Filtering Algorithm", "comment": "7 pages, 13 figures. Accepted to IEEE ICITEE 2023", "summary": "Unmanned Aerial Vehicles (UAV) have emerged as versatile platforms, driving\nthe demand for accurate modeling to support developmental testing. This paper\nproposes data-driven modeling software for UAV. Emphasizes the utilization of\ncost-effective sensors to obtain orientation and location data subsequently\nprocessed through the application of data filtering algorithms and sensor\nfusion techniques to improve the data quality to make a precise model\nvisualization on the software. UAV's orientation is obtained using processed\nInertial Measurement Unit (IMU) data and represented using Quaternion\nRepresentation to avoid the gimbal lock problem. The UAV's location is\ndetermined by combining data from the Global Positioning System (GPS), which\nprovides stable geographic coordinates but slower data update frequency, and\nthe accelerometer, which has higher data update frequency but integrating it to\nget position data is unstable due to its accumulative error. By combining data\nfrom these two sensors, the software is able to calculate and continuously\nupdate the UAV's real-time position during its flight operations. The result\nshows that the software effectively renders UAV orientation and position with\nhigh degree of accuracy and fluidity", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528IMU\u548cGPS\u6570\u636e\u8fdb\u884c\u65e0\u4eba\u673a\u5efa\u6a21\u7684\u6570\u636e\u9a71\u52a8\u8f6f\u4ef6\uff0c\u89e3\u51b3\u4e86\u4e07\u5411\u8282\u9501\u5b9a\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u4f20\u611f\u5668\u878d\u5408\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u6d41\u7545\u6027\u3002", "motivation": "\u65e0\u4eba\u673a\uff08UAV\uff09\u5df2\u6210\u4e3a\u591a\u529f\u80fd\u5e73\u53f0\uff0c\u63a8\u52a8\u4e86\u5bf9\u7cbe\u786e\u5efa\u6a21\u4ee5\u652f\u6301\u5f00\u53d1\u6d4b\u8bd5\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u6570\u636e\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u5efa\u6a21\u8f6f\u4ef6\uff0c\u5229\u7528\u6210\u672c\u6548\u76ca\u9ad8\u7684\u4f20\u611f\u5668\u83b7\u53d6\u5b9a\u5411\u548c\u4f4d\u7f6e\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u6570\u636e\u6ee4\u6ce2\u7b97\u6cd5\u548c\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\u5904\u7406\u6570\u636e\u4ee5\u63d0\u9ad8\u6570\u636e\u8d28\u91cf\uff0c\u4ece\u800c\u5728\u8f6f\u4ef6\u4e2d\u5b9e\u73b0\u7cbe\u786e\u7684\u6a21\u578b\u53ef\u89c6\u5316\u3002\u65e0\u4eba\u673a\u7684\u5b9a\u5411\u4f7f\u7528\u5904\u7406\u540e\u7684\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u6570\u636e\u83b7\u5f97\uff0c\u5e76\u4f7f\u7528\u56db\u5143\u6570\u8868\u793a\u6765\u907f\u514d\u4e07\u5411\u8282\u9501\u5b9a\u95ee\u9898\u3002\u65e0\u4eba\u673a\u7684\u4f4d\u7f6e\u7ed3\u5408\u4e86\u63d0\u4f9b\u7a33\u5b9a\u5730\u7406\u5750\u6807\u4f46\u6570\u636e\u66f4\u65b0\u9891\u7387\u8f83\u6162\u7684\u5168\u7403\u5b9a\u4f4d\u7cfb\u7edf\uff08GPS\uff09\u4ee5\u53ca\u6570\u636e\u66f4\u65b0\u9891\u7387\u8f83\u9ad8\u4f46\u96c6\u6210\u4f4d\u7f6e\u6570\u636e\u4e0d\u7a33\u5b9a\uff08\u56e0\u7d2f\u79ef\u8bef\u5dee\uff09\u7684\u52a0\u901f\u5ea6\u8ba1\u7684\u6570\u636e\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u8f6f\u4ef6\u80fd\u591f\u6709\u6548\u5730\u6e32\u67d3\u65e0\u4eba\u673a\u7684\u59ff\u6001\u548c\u4f4d\u7f6e\uff0c\u5e76\u5177\u6709\u9ad8\u5ea6\u7684\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u3002", "conclusion": "\u8be5\u8f6f\u4ef6\u80fd\u591f\u6709\u6548\u5730\u6e32\u67d3\u65e0\u4eba\u673a\u7684\u59ff\u6001\u548c\u4f4d\u7f6e\uff0c\u5e76\u5177\u6709\u9ad8\u5ea6\u7684\u51c6\u786e\u6027\u548c\u6d41\u7545\u6027\u3002"}}
{"id": "2507.09105", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09105", "abs": "https://arxiv.org/abs/2507.09105", "authors": ["Maoxiao Ye", "Xinfeng Ye", "Mano Manoharan"], "title": "Hybrid Autoregressive-Diffusion Model for Real-Time Streaming Sign Language Production", "comment": null, "summary": "Earlier Sign Language Production (SLP) models typically relied on\nautoregressive methods that generate output tokens one by one, which inherently\nprovide temporal alignment. Although techniques like Teacher Forcing can\nprevent model collapse during training, they still cannot solve the problem of\nerror accumulation during inference, since ground truth is unavailable at that\nstage. In contrast, more recent approaches based on diffusion models leverage\nstep-by-step denoising to enable high-quality generation. However, the\niterative nature of these models and the requirement to denoise entire\nsequences limit their applicability in real-time tasks like SLP. To address it,\nwe apply a hybrid approach combining autoregressive and diffusion models to SLP\nfor the first time, leveraging the strengths of both models in sequential\ndependency modeling and output refinement. To capture fine-grained body\nmovements, we design a Multi-Scale Pose Representation module that separately\nextracts detailed features from distinct articulators and integrates them via a\nMulti-Scale Fusion module. Furthermore, we introduce a Confidence-Aware Causal\nAttention mechanism that utilizes joint-level confidence scores to dynamically\nguide the pose generation process, improving accuracy and robustness. Extensive\nexperiments on the PHOENIX14T and How2Sign datasets demonstrate the\neffectiveness of our method in both generation quality and real-time streaming\nefficiency.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u8bef\u5dee\u7d2f\u79ef\u548c\u6269\u6563\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u672c\u6587\u9996\u6b21\u5c06\u81ea\u56de\u5f52\u6a21\u578b\u548c\u6269\u6563\u6a21\u578b\u7ed3\u5408\u8d77\u6765\u7528\u4e8e\u624b\u8bed\u751f\u6210\uff0c\u5e76\u901a\u8fc7\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u3001\u591a\u5c3a\u5ea6\u878d\u5408\u548c\u7f6e\u4fe1\u5ea6\u611f\u77e5\u56e0\u679c\u6ce8\u610f\u529b\u7b49\u673a\u5236\u63d0\u9ad8\u4e86\u751f\u6210\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u81ea\u56de\u5f52\u6a21\u578b\u5728\u63a8\u7406\u65f6\u8bef\u5dee\u7d2f\u79ef\u4ee5\u53ca\u6269\u6563\u6a21\u578b\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u4efb\u52a1\u7684\u95ee\u9898\uff0c\u9996\u6b21\u5c06\u6df7\u5408\u65b9\u6cd5\u5e94\u7528\u4e8e\u624b\u8bed\u751f\u6210\u3002", "method": "\u7ed3\u5408\u4e86\u81ea\u56de\u5f52\u548c\u6269\u6563\u6a21\u578b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u591a\u5c3a\u5ea6\u59ff\u6001\u8868\u793a\u6a21\u5757\u3001\u591a\u5c3a\u5ea6\u878d\u5408\u6a21\u5757\u4ee5\u53ca\u7f6e\u4fe1\u5ea6\u611f\u77e5\u56e0\u679c\u6ce8\u610f\u529b\u673a\u5236\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6d41\u5f0f\u4f20\u8f93\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u8be5\u6df7\u5408\u65b9\u6cd5\u5728PHOENIX14T\u548cHow2Sign\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5176\u5728\u751f\u6210\u8d28\u91cf\u548c\u5b9e\u65f6\u6d41\u5f0f\u4f20\u8f93\u6548\u7387\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09913", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.09913", "abs": "https://arxiv.org/abs/2507.09913", "authors": ["Jason K Kawasaki", "Quinn T Campbell"], "title": "An analytical model for the remote epitaxial potential", "comment": null, "summary": "We propose an analytical model for the remote bonding potential of the\nsubstrate that permeates through graphene during remote epitaxy. Our model,\nbased on a Morse interatomic potential, includes the attenuation due to (1) the\nincreased separation between film and substrate and (2) free carrier screening\nfrom graphene. Compared with previous slab density functional theory\ncalculations, which use the electrostatic potential as a proxy for bonding, our\nanalytical model provides a more direct description of bonding, explicitly\nincludes screening (which is often ignored), and is based on simple,\ninterpretable, and well benchmarked parameters. We show that the magnitude of\n$|\\phi_{remote}|$ for typical semiconductor and oxide substrates is few meV,\nsimilar to the van der Waals potential of graphene. This suggests interference\nbetween the graphene and remote substrate potentials, plus interfacial\nreconstructions, must be considered when interpreting experiments on remote\nepitaxy. We use our model to interpret previous experiments from the remote\nepitaxy and related literature, highlighting connections to moire epitaxy and\nto the ordering of graphite intercalation compounds. Our model also points to\ntests, based on tunable screening and spatial extent of the substrate\npotential, that may increase the strength of the remote potential towards the\nmore idealized picture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eMorse\u52bf\u7684\u5206\u6790\u6a21\u578b\uff0c\u7528\u4e8e\u63cf\u8ff0\u77f3\u58a8\u70ef\u8fdc\u7a0b\u5916\u5ef6\u4e2d\u886c\u5e95\u7684\u8fdc\u7a0b\u7ed3\u5408\u52bf\u3002\u8be5\u6a21\u578b\u8003\u8651\u4e86\u8ddd\u79bb\u8870\u51cf\u548c\u81ea\u7531\u8f7d\u6d41\u5b50\u7b5b\u9009\uff0c\u7ed3\u679c\u663e\u793a\u8fdc\u7a0b\u7ed3\u5408\u52bf\u4e0e\u77f3\u58a8\u70ef\u7684\u8303\u5fb7\u534e\u52bf\u76f8\u5f53\uff0c\u8868\u660e\u5728\u89e3\u91ca\u5b9e\u9a8c\u65f6\u9700\u8981\u8003\u8651\u77f3\u58a8\u70ef\u4e0e\u886c\u5e95\u52bf\u7684\u5e72\u6270\u4ee5\u53ca\u754c\u9762\u91cd\u6784\u3002\u6a21\u578b\u8fd8\u80fd\u6307\u5bfc\u5b9e\u9a8c\u4ee5\u589e\u5f3a\u8fdc\u7a0b\u7ed3\u5408\u52bf\u3002", "motivation": "\u63d0\u4f9b\u4e00\u4e2a\u66f4\u76f4\u63a5\u3001\u660e\u786e\u5305\u542b\u7b5b\u9009\u6548\u5e94\u7684\u8fdc\u7a0b\u5916\u5ef6\u7ed3\u5408\u52bf\u7684\u63cf\u8ff0\uff0c\u5e76\u57fa\u4e8e\u7b80\u5355\u3001\u53ef\u89e3\u91ca\u4e14\u7ecf\u8fc7\u5145\u5206\u68c0\u9a8c\u7684\u53c2\u6570\u3002\u4e0e\u4ec5\u4f7f\u7528\u9759\u7535\u52bf\u4f5c\u4e3a\u7ed3\u5408\u4ee3\u7528\u7269\u7684\u5148\u524d\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u80fd\u66f4\u76f4\u63a5\u5730\u63cf\u8ff0\u7ed3\u5408\uff0c\u5e76\u660e\u786e\u5305\u542b\u901a\u5e38\u88ab\u5ffd\u7565\u7684\u7b5b\u9009\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8eMorse\u539f\u5b50\u95f4\u52bf\u7684\u5206\u6790\u6a21\u578b\uff0c\u8003\u8651\u4e86\u77f3\u58a8\u70ef\u4e0e\u886c\u5e95\u4e4b\u95f4\u8ddd\u79bb\u589e\u52a0\u548c\u81ea\u7531\u8f7d\u6d41\u5b50\u7b5b\u9009\u5f15\u8d77\u7684\u8870\u51cf\u3002", "result": "\u8fdc\u7a0b\u5916\u5ef6\u7684\u7ed3\u5408\u52bf|\u03d5remote|\u7684\u5927\u5c0f\u5bf9\u4e8e\u5178\u578b\u7684\u534a\u5bfc\u4f53\u548c\u6c27\u5316\u7269\u886c\u5e95\u4ec5\u4e3a\u51e0\u4e2a\u6beb\u7535\u5b50\u4f0f\uff0c\u4e0e\u77f3\u58a8\u70ef\u7684\u8303\u5fb7\u534e\u52bf\u76f8\u4f3c\u3002\u901a\u8fc7\u6a21\u578b\u89e3\u91ca\u4e86\u5148\u524d\u8fdc\u7a0b\u5916\u5ef6\u548c\u76f8\u5173\u6587\u732e\u4e2d\u7684\u5b9e\u9a8c\uff0c\u5e76\u6307\u51fa\u4e86\u4e0e\u83ab\u5c14\u5916\u5ef6\u548c\u77f3\u58a8\u70ef\u63d2\u5c42\u5316\u5408\u7269\u6709\u5e8f\u5316\u7684\u8054\u7cfb\u3002", "conclusion": "\u9700\u8981\u8003\u8651\u77f3\u58a8\u70ef\u4e0e\u8fdc\u7a0b\u886c\u5e95\u52bf\u80fd\u7684\u5e72\u6270\u4ee5\u53ca\u754c\u9762\u91cd\u6784\uff0c\u624d\u80fd\u89e3\u91ca\u8fdc\u7a0b\u5916\u5ef6\u5b9e\u9a8c\u3002\u63d0\u51fa\u7684\u6a21\u578b\u53ef\u4ee5\u6307\u5bfc\u5b9e\u9a8c\uff0c\u901a\u8fc7\u53ef\u8c03\u8c10\u7684\u7b5b\u9009\u548c\u886c\u5e95\u52bf\u7684\u7a7a\u95f4\u8303\u56f4\u6765\u589e\u5f3a\u8fdc\u7a0b\u52bf\u80fd\u3002"}}
{"id": "2507.09416", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09416", "abs": "https://arxiv.org/abs/2507.09416", "authors": ["Yat Wong", "Liang Jiang"], "title": "Local unitary decomposition of tripartite arbitrary leveled qudit stabilizer states into $p$-level-qudit EPR and GHZ state", "comment": null, "summary": "We study the entanglement structure of tripartite stabilizer states on $N$\nqudits of dimension $D$, distributed across parties $A$, $B$, and $C$, under\narbitrary local unitaries. Prior work by Bravyi et al. and Looi et al. showed\nthat qubit and squarefree qudit stabilizer states can be transformed via local\nClifford unitaries into tensor products of GHZ states, EPR pairs, and\nunentangled qudits [arXiv:quant-ph/0504208, arXiv:1107.1761]. We generalize\nthis to arbitrary integer $D$ by introducing local unitaries beyond the\nClifford group, enabling decomposition of prime-power qudit stabilizer states\ninto $p$-level GHZ states, EPR pairs, and unentangled qudits. Our algorithm\nleverages subsystem phase matrices to characterize entanglement and applies to\nquantum protocols requiring efficient entanglement distribution.", "AI": {"tldr": "\u901a\u8fc7\u65b0\u7684\u5c40\u90e8\u9149\u53d8\u6362\uff0c\u5c06\u4efb\u610f\u7ef4\u5ea6\u591a\u51c6\u5219\u7a33\u5b9a\u5668\u6001\u5206\u89e3\u4e3a\u57fa\u672c\u7ea0\u7f20\u5355\u5143\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u7ea0\u7f20\u5206\u53d1\u3002", "motivation": "\u4e3a\u4e86\u5c06\u7ea0\u7f20\u7ed3\u6784\u4ece\u91cf\u5b50\u6bd4\u7279\u548c\u65e0\u5e73\u65b9\u56e0\u5b50\u591a\u51c6\u5219\u7a33\u5b9a\u5668\u6001\u63a8\u5e7f\u5230\u4efb\u610f\u6574\u6570\u7ef4\u5ea6 D \u7684\u591a\u51c6\u5219\u7a33\u5b9a\u5668\u6001\u3002", "method": "\u5229\u7528\u5b50\u7cfb\u7edf\u76f8\u4f4d\u77e9\u9635\u6765\u8868\u5f81\u7ea0\u7f20\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u8d85\u8d8a Clifford \u7fa4\u7684\u5c40\u90e8\u9149\u53d8\u6362\u7b97\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u7d20\u6570\u5e42\u6b21\u591a\u51c6\u5219\u7a33\u5b9a\u5668\u6001\u7684\u5206\u89e3\u3002", "result": "\u6210\u529f\u5c06\u7d20\u6570\u5e42\u6b21\u591a\u51c6\u5219\u7a33\u5b9a\u5668\u6001\u5206\u89e3\u4e3a p \u7ea7 GHZ \u6001\u3001EPR \u5bf9\u548c\u975e\u7ea0\u7f20\u591a\u51c6\u5219\u7cfb\u7edf\uff0c\u4e3a\u9700\u8981\u9ad8\u6548\u5206\u53d1\u7ea0\u7f20\u7684\u91cf\u5b50\u534f\u8bae\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u8d85\u8d8a Clifford \u7fa4\u7684\u5c40\u90e8\u9149\u53d8\u6362\uff0c\u5c06\u7d20\u6570\u5e42\u6b21\u591a\u51c6\u5219\u7a33\u5b9a\u5668\u6001\u5206\u89e3\u4e3a p \u7ea7 GHZ \u6001\u3001EPR \u5bf9\u548c\u975e\u7ea0\u7f20\u591a\u51c6\u5219\u7cfb\u7edf\u3002"}}
{"id": "2507.09185", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09185", "abs": "https://arxiv.org/abs/2507.09185", "authors": ["Ameen Ali", "Shahar Katz", "Lior Wolf", "Ivan Titov"], "title": "Detecting and Pruning Prominent but Detrimental Neurons in Large Language Models", "comment": null, "summary": "Large language models (LLMs) often develop learned mechanisms specialized to\nspecific datasets, such as reliance on domain-specific correlations, which\nyield high-confidence predictions without generalizable reasoning. While\nbeneficial in one setting, these dataset-specific mechanisms typically degrade\nperformance when models encounter novel tasks or distributions. In this work,\nwe introduce a fine-tuning approach designed to enhance generalization by\nidentifying and pruning neurons associated with dataset-specific mechanisms in\ntransformer-based LLMs. Our method employs Integrated Gradients to quantify\neach neuron's influence on high-confidence predictions, pinpointing those that\ndisproportionately contribute to dataset-specific performance without\nsupporting robust, transferable reasoning. Selectively pruning these neurons\ncompels the model to depend on generalizable representations. Evaluated across\nmultiple-choice benchmarks, our pruning-based fine-tuning significantly\nenhances performance, surpassing prior (non-pruning) adaptation methods.", "AI": {"tldr": "\u901a\u8fc7\u526a\u9664\u795e\u7ecf\u5143\u6765\u589e\u5f3a LLMs \u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u7279\u5b9a\u6570\u636e\u96c6\u4e0a\u4f1a\u53d1\u5c55\u51fa\u4e13\u95e8\u7684\u673a\u5236\uff0c\u5bfc\u81f4\u5728\u9047\u5230\u65b0\u4efb\u52a1\u6216\u5206\u5e03\u65f6\u6027\u80fd\u4e0b\u964d\u3002\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5fae\u8c03\u65b9\u6cd5\uff0c\u5229\u7528 Integrated Gradients \u91cf\u5316\u6bcf\u4e2a\u795e\u7ecf\u5143\u5bf9\u9ad8\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u8d21\u732e\uff0c\u8bc6\u522b\u5e76\u526a\u9664\u4e0e\u7279\u5b9a\u6570\u636e\u96c6\u673a\u5236\uff08\u4f8b\u5982\uff0c\u4f9d\u8d56\u4e8e\u7279\u5b9a\u9886\u57df\u7684\u76f8\u5173\u6027\uff09\u76f8\u5173\u7684\u795e\u7ecf\u5143\uff0c\u4ee5\u589e\u5f3a transformer-based LLMs \u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86 LLMs \u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u5148\u524d\u975e\u526a\u9664\u7684\u9002\u5e94\u65b9\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u9009\u62e9\u6027\u5730\u526a\u9664\u4e0e\u7279\u5b9a\u6570\u636e\u96c6\u673a\u5236\u76f8\u5173\u8054\u7684\u795e\u7ecf\u5143\uff0c\u6211\u4eec\u7684\u526a\u9664\u65b9\u6cd5\u80fd\u591f\u5f3a\u5236\u6a21\u578b\u4f9d\u8d56\u4e8e\u53ef\u6cdb\u5316\u7684\u8868\u5f81\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u9009\u62e9\u9898\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u4f18\u4e8e\u5148\u524d\u975e\u526a\u9664\u7684\u9002\u5e94\u65b9\u6cd5\u3002"}}
{"id": "2507.09894", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.09894", "abs": "https://arxiv.org/abs/2507.09894", "authors": ["Saif Khan Mohammed", "Amit Kumar Pathak", "Muhammad Ubadah", "Ronny Hadani", "Ananthanarayanan Chockalingam", "Robert Calderbank"], "title": "Precoded Zak-OTFS for Per-Carrier Equalization", "comment": null, "summary": "In Zak-OTFS (orthogonal time frequency space) modulation the carrier waveform\nis a pulse in the delay-Doppler (DD) domain, formally a quasi-periodic\nlocalized function with specific periods along delay and Doppler. When the\nchannel delay spread is less than the delay period, and the channel Doppler\nspread is less than the Doppler period, the response to a single Zak-OTFS\ncarrier provides an image of the scattering environment and can be used to\npredict the effective channel at all other carriers. The image of the\nscattering environment changes slowly, making it possible to employ precoding\nat the transmitter. Precoding techniques were developed more than thirty years\nago for wireline modem channels (V.34 standard) defined by linear convolution\nwhere a pulse in the time domain (TD) is used to probe the one-dimensional\npartial response channel. The action of a doubly spread channel on Zak-OTFS\nmodulation determines a two-dimensional partial response channel defined by\ntwisted convolution, and we develop a novel precoding technique for this\nchannel. The proposed precoder leads to separate equalization of each DD\ncarrier which has significantly lower complexity than joint equalization of all\ncarriers. Further, the effective precoded channel results in non-interfering DD\ncarriers which significantly reduces the overhead of guard carriers separating\ndata and pilot carriers, which improves the spectral efficiency significantly.", "AI": {"tldr": "\u65b0\u9884\u7f16\u7801\u6280\u672f\u964d\u4f4e\u4e86 Zak-OTFS \u8c03\u5236\u5728\u53cc spread \u4fe1\u9053\u4e2d\u7684\u5747\u8861\u590d\u6742\u6027\uff0c\u63d0\u9ad8\u4e86\u9891\u8c31\u6548\u7387\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u89e3\u51b3 Zak-OTFS \u8c03\u5236\u5728\u53cc spread \u4fe1\u9053\u4e2d\u7684\u5747\u8861\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u9884\u7f16\u7801\u6280\u672f\uff0c\u7528\u4e8e Zak-OTFS \u8c03\u5236\u548c\u53cc spread \u4fe1\u9053\u3002\u8be5\u6280\u672f\u901a\u8fc7\u5c06\u53cc spread \u4fe1\u9053\u8f6c\u6362\u4e3a\u4e00\u7ec4\u72ec\u7acb\u7684\u5ef6\u8fdf-\u591a\u666e\u52d2\uff08DD\uff09\u5b50\u4fe1\u9053\u6765\u5b9e\u73b0\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u5747\u8861\u8fc7\u7a0b\u3002", "result": "\u63d0\u51fa\u7684\u9884\u7f16\u7801\u6280\u672f\u53ef\u4ee5\u5c06\u53cc spread \u4fe1\u9053\u8f6c\u6362\u4e3a\u4e00\u7ec4\u72ec\u7acb\u7684 DD \u5b50\u4fe1\u9053\uff0c\u4ece\u800c\u964d\u4f4e\u5747\u8861\u590d\u6742\u5ea6\u5e76\u51cf\u5c11\u4fdd\u62a4\u95f4\u9694\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u9891\u8c31\u6548\u7387\u3002", "conclusion": "Zak-OTFS\u8c03\u5236\u4e0e\u53cc spread \u4fe1\u9053\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u7f16\u7801\u6280\u672f\uff0c\u53ef\u4ee5\u5c06\u53cc spread \u4fe1\u9053\u8f6c\u6362\u4e3a\u4e00\u7ec4\u72ec\u7acb\u7684 DD \u5b50\u4fe1\u9053\uff0c\u4ece\u800c\u964d\u4f4e\u5747\u8861\u590d\u6742\u5ea6\u5e76\u63d0\u9ad8\u9891\u8c31\u6548\u7387\u3002"}}
{"id": "2507.09989", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09989", "abs": "https://arxiv.org/abs/2507.09989", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Sheng Han"], "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient", "comment": null, "summary": "In heterogeneous multi-agent reinforcement learning (MARL), achieving\nmonotonic improvement plays a pivotal role in enhancing performance. The HAPPO\nalgorithm proposes a feasible solution by introducing a sequential update\nscheme, which requires independent learning with No Parameter-sharing (NoPS).\nHowever, heterogeneous MARL generally requires Partial Parameter-sharing\n(ParPS) based on agent grouping to achieve high cooperative performance. Our\nexperiments prove that directly combining ParPS with the sequential update\nscheme leads to the policy updating baseline drift problem, thereby failing to\nachieve improvement. To solve the conflict between monotonic improvement and\nParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)\nalgorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$\nwith the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived\nfrom Q-functions. This maintains MAAD's monotonic improvement while eliminating\nthe conflict through optimal joint action sequences instead of sequential\npolicy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)\nas the critic function, employing pessimistic uncertainty-constrained loss to\noptimize different Q-value estimations. This provides the required Q-values for\nOMQ computation and stable baselines for actor updates. Finally, we implement a\nCentralized Critic Grouped Actor (CCGA) architecture that simultaneously\nachieves ParPS in local policy networks and accurate global Q-function\ncomputation. Experimental results in SMAC and MAMuJoCo environments demonstrate\nthat OMDPG outperforms various state-of-the-art MARL baselines.", "AI": {"tldr": "OMDPG\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165OMQ\u548cGQC\uff0c\u89e3\u51b3\u4e86\u90e8\u5206\u53c2\u6570\u5171\u4eab\u4e0e\u5355\u8c03\u6539\u8fdb\u7684\u51b2\u7a81\uff0c\u5b9e\u73b0\u4e86\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff08\u5982HAPPO\uff09\u5728\u91c7\u7528\u90e8\u5206\u53c2\u6570\u5171\u4eab\u65f6\u51fa\u73b0\u7684\u7b56\u7565\u66f4\u65b0\u57fa\u7ebf\u6f02\u79fb\u95ee\u9898\uff0c\u4ee5\u53ca\u90e8\u5206\u53c2\u6570\u5171\u4eab\u4e0e\u5355\u8c03\u6539\u8fdb\u4e4b\u95f4\u7684\u51b2\u7a81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOMDPG\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u6700\u4f18\u8fb9\u9645Q\uff08OMQ\uff09\u51fd\u6570\u6765\u66ff\u4ee3\u987a\u5e8f\u8ba1\u7b97\u7684Q\u51fd\u6570\uff0c\u4ee5\u89e3\u51b3\u7b56\u7565\u66f4\u65b0\u57fa\u7ebf\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u4f7f\u7528\u5e7f\u4e49QCritic\uff08GQC\uff09\u4f5c\u4e3aCritic\u51fd\u6570\uff0c\u91c7\u7528\u60b2\u89c2\u4e0d\u786e\u5b9a\u6027\u7ea6\u675f\u635f\u5931\u6765\u4f18\u5316\u4e0d\u540c\u7684Q\u503c\u4f30\u8ba1\uff0c\u6700\u540e\u901a\u8fc7\u4e2d\u5fc3\u5316Critic\u5206\u7ec4Actor\uff08CCGA\uff09\u67b6\u6784\u5b9e\u73b0\u90e8\u5206\u53c2\u6570\u5171\u4eab\u548c\u5168\u5c40Q\u51fd\u6570\u8ba1\u7b97\u3002", "result": "OMDPG\u7b97\u6cd5\u5728SMAC\u548cMAMuJoCo\u73af\u5883\u4e2d\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u7b97\u6cd5\u3002", "conclusion": "OMDPG\u7b97\u6cd5\u6210\u529f\u89e3\u51b3\u4e86\u5f02\u6784\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u90e8\u5206\u53c2\u6570\u5171\u4eab\u4e0e\u5355\u8c03\u6539\u8fdb\u4e4b\u95f4\u7684\u51b2\u7a81\uff0c\u5e76\u5728SMAC\u548cMAMuJoCo\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u7684\u6027\u80fd\u3002"}}
{"id": "2507.08860", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08860", "abs": "https://arxiv.org/abs/2507.08860", "authors": ["Awais Manzoor", "M. Atif Qureshi", "Etain Kidney", "Luca Longo"], "title": "e-Profits: A Business-Aligned Evaluation Metric for Profit-Sensitive Customer Churn Prediction", "comment": null, "summary": "Retention campaigns in customer relationship management often rely on churn\nprediction models evaluated using traditional metrics such as AUC and F1-score.\nHowever, these metrics fail to reflect financial outcomes and may mislead\nstrategic decisions. We introduce e-Profits, a novel business-aligned\nevaluation metric that quantifies model performance based on customer-specific\nvalue, retention probability, and intervention costs. Unlike existing\nprofit-based metrics such as Expected Maximum Profit, which assume fixed\npopulation-level parameters, e-Profits uses Kaplan-Meier survival analysis to\nestimate personalised retention rates and supports granular, per customer\nevaluation. We benchmark six classifiers across two telecom datasets (IBM Telco\nand Maven Telecom) and demonstrate that e-Profits reshapes model rankings\ncompared to traditional metrics, revealing financial advantages in models\npreviously overlooked by AUC or F1-score. The metric also enables segment-level\ninsight into which models maximise return on investment for high-value\ncustomers. e-Profits is designed as an understandable, post hoc tool to support\nmodel evaluation in business contexts, particularly for marketing and analytics\nteams prioritising profit-driven decisions. All source code is available at:\nhttps://github.com/matifq/eprofits.", "AI": {"tldr": "e-Profits\u662f\u4e00\u4e2a\u65b0\u7684\u4e1a\u52a1\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u6a21\u578b\u3002\u5b83\u901a\u8fc7\u8003\u8651\u5ba2\u6237\u4ef7\u503c\u3001\u4fdd\u7559\u6982\u7387\u548c\u5e72\u9884\u6210\u672c\uff0c\u5e76\u7ed3\u5408Kaplan-Meier\u751f\u5b58\u5206\u6790\u6765\u91cf\u5316\u6a21\u578b\u8868\u73b0\uff0c\u66f4\u80fd\u53cd\u6620\u8d22\u52a1\u7ed3\u679c\uff0c\u5e76\u80fd\u8bc6\u522b\u51fa\u88ab\u4f20\u7edf\u6307\u6807\u5ffd\u7565\u4f46\u5bf9\u9ad8\u4ef7\u503c\u5ba2\u6237ROI\u66f4\u4f18\u7684\u6a21\u578b\u3002", "motivation": "\u4f20\u7edf\u7684\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u6a21\u578b\u8bc4\u4f30\u6307\u6807\uff08\u5982AUC\u548cF1\u5206\u6570\uff09\u672a\u80fd\u6709\u6548\u53cd\u6620\u8d22\u52a1\u7ed3\u679c\uff0c\u53ef\u80fd\u8bef\u5bfc\u4f01\u4e1a\u5728\u5ba2\u6237\u5173\u7cfb\u7ba1\u7406\u4e2d\u7684\u6218\u7565\u51b3\u7b56\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u76f4\u63a5\u5173\u8054\u4e1a\u52a1\u6210\u679c\u548c\u6a21\u578b\u6027\u80fd\u7684\u8bc4\u4f30\u6307\u6807\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3ae-Profits\u7684\u65b0\u578b\u4e1a\u52a1\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002\u8be5\u6307\u6807\u57fa\u4e8e\u5ba2\u6237\u4ef7\u503c\u3001\u4fdd\u7559\u6982\u7387\u548c\u5e72\u9884\u6210\u672c\u6765\u91cf\u5316\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u5229\u7528Kaplan-Meier\u751f\u5b58\u5206\u6790\u6765\u4f30\u8ba1\u4e2a\u6027\u5316\u7684\u4fdd\u7559\u7387\uff0c\u4ece\u800c\u652f\u6301\u5bf9\u6bcf\u4f4d\u5ba2\u6237\u8fdb\u884c\u7cbe\u7ec6\u5316\u8bc4\u4f30\u3002\u7814\u7a76\u4eba\u5458\u5728\u4e24\u4e2a\u7535\u4fe1\u6570\u636e\u96c6\u4e0a\u5bf9\u516d\u79cd\u5206\u7c7b\u5668\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4e0e\u4f20\u7edf\u7684AUC\u548cF1\u5206\u6570\u7b49\u6307\u6807\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86e-Profits\u80fd\u591f\u91cd\u5851\u6a21\u578b\u6392\u540d\uff0c\u63ed\u793a\u4e86\u5148\u524d\u88ab\u5ffd\u89c6\u7684\u6a21\u578b\u5728\u8d22\u52a1\u4e0a\u7684\u4f18\u52bf\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684AUC\u6216F1\u5206\u6570\u76f8\u6bd4\uff0ce-Profits\u6307\u6807\u80fd\u591f\u6539\u53d8\u4e0d\u540c\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u6a21\u578b\u7684\u6392\u540d\u3002e-Profits\u63ed\u793a\u4e86\u4e00\u4e9b\u5728\u4f20\u7edf\u6307\u6807\u4e0b\u8868\u73b0\u4e0d\u4f73\u4f46\u5b9e\u9645\u4e0a\u5177\u6709\u8d22\u52a1\u4f18\u52bf\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u6307\u6807\u8fd8\u80fd\u63d0\u4f9b\u7ec6\u7c92\u5ea6\u7684\u6d1e\u5bdf\uff0c\u5e2e\u52a9\u8bc6\u522b\u54ea\u4e9b\u6a21\u578b\u80fd\u591f\u4e3a\u9ad8\u4ef7\u503c\u5ba2\u6237\u7fa4\u4f53\u6700\u5927\u9650\u5ea6\u5730\u63d0\u9ad8\u6295\u8d44\u56de\u62a5\u7387\u3002", "conclusion": "e-Profits\u662f\u4e00\u4e2a\u65b0\u7684\u4e1a\u52a1\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u5ba2\u6237\u751f\u547d\u5468\u671f\u7ba1\u7406\u4e2d\u7684\u5ba2\u6237\u6d41\u5931\u9884\u6d4b\u6a21\u578b\u3002\u8be5\u6307\u6807\u901a\u8fc7\u6574\u5408\u5ba2\u6237\u4ef7\u503c\u3001\u4fdd\u7559\u6982\u7387\u548c\u5e72\u9884\u6210\u672c\u6765\u91cf\u5316\u6a21\u578b\u8868\u73b0\uff0c\u5e76\u80fd\u8fdb\u884c\u6bcf\u4f4d\u5ba2\u6237\u7684\u8bc4\u4f30\u3002\u4e0e\u4f20\u7edf\u7684AUC\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0d\u540c\uff0ce-Profits\u66f4\u80fd\u53cd\u6620\u5b9e\u9645\u7684\u8d22\u52a1\u7ed3\u679c\uff0c\u5e2e\u52a9\u4f01\u4e1a\u505a\u51fa\u66f4\u660e\u667a\u7684\u6218\u7565\u51b3\u7b56\u3002\u8be5\u6307\u6807\u8fd8\u80fd\u8bc6\u522b\u51fa\u90a3\u4e9b\u88ab\u4f20\u7edf\u6307\u6807\u5ffd\u7565\u4f46\u80fd\u5728\u6295\u8d44\u56de\u62a5\u7387\u65b9\u9762\u8868\u73b0\u51fa\u8272\u7684\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u4ef7\u503c\u5ba2\u6237\u7fa4\u4f53\u4e2d\u3002e-Profits\u65e8\u5728\u4f5c\u4e3a\u4e00\u4e2a\u6613\u4e8e\u7406\u89e3\u7684\u8f85\u52a9\u5de5\u5177\uff0c\u652f\u6301\u4f01\u4e1a\u5728\u5e02\u573a\u8425\u9500\u548c\u5206\u6790\u56e2\u961f\u4e2d\u8fdb\u884c\u6a21\u578b\u8bc4\u4f30\uff0c\u7279\u522b\u9002\u5408\u90a3\u4e9b\u4f18\u5148\u8003\u8651\u5229\u6da6\u9a71\u52a8\u51b3\u7b56\u7684\u56e2\u961f\u3002"}}
{"id": "2507.09611", "categories": ["cs.AI", "cs.CY", "68T01"], "pdf": "https://arxiv.org/pdf/2507.09611", "abs": "https://arxiv.org/abs/2507.09611", "authors": ["Jenis Winsta"], "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "comment": "5 pages, 3 figures", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10004", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10004", "abs": "https://arxiv.org/abs/2507.10004", "authors": ["Taouba Jouini", "Jan Wachter", "Sophie An", "Veit Hagenmeyer"], "title": "Hardware test and validation of the angular droop control: Analysis and experiments", "comment": null, "summary": "The angular droop control is a grid-forming control strategy that exploits\nthe idea of power-to-angle droop to achieve exact frequency synchronization\nwith no stringent separation between primary and secondary frequency control.\nIn this work, we conduct hardware experiments in the Smart Energy System\nControl Laboratory at Karlsruhe Institute of Technology (KIT) to test and\nvalidate the angular droop control for low voltage power grids in two different\ntest scenarios. First, we verify its grid-forming capabilities after a major\nevent, e.g., following a blackout, demonstrated via power-to-angle droop\nbehavior. For this, we propose two implementation schemes that rely either on\ndirect or indirect actuation of the modulation signal and draw a comparison\nbetween them. Second, we investigate the plug-and-play capabilities, i.e.,\nlocal stability and power sharing for a two-converter system and provide\nsuitable tuning for the control gains. Our experimental findings illustrate the\nusefulness of hardware test and validation for DC/AC converter control, the\npractical challenges entailed and the proposed remedies.", "AI": {"tldr": "Hardware experiments validate angular droop control for low voltage grids, testing grid-forming capabilities after blackouts and plug-and-play features with two implementation schemes and tuning recommendations for converter control actuation and comparing them. It also investigates converter systems and tuning gains, providing insights into practical challenges and solutions.", "motivation": "The angular droop control is a grid-forming control strategy that exploits the idea of power-to-angle droop to achieve exact frequency synchronization with no stringent separation between primary and secondary frequency control.", "method": "This work conducts hardware experiments in the Smart Energy System Control Laboratory at Karlsruhe Institute of Technology (KIT) to test and validate the angular droop control for low voltage power grids in two different test scenarios. First, it verifies grid-forming capabilities after a blackout, demonstrated via power-to-angle droop behavior, by proposing two implementation schemes that rely either on direct or indirect actuation of the modulation signal and drawing a comparison between them. Second, it investigates plug-and-play capabilities, i.e., local stability and power sharing for a two-converter system and provides suitable tuning for the control gains.", "result": "The experimental findings illustrate the usefulness of hardware test and validation for DC/AC converter control, the practical challenges entailed and the proposed remedies.", "conclusion": "The experimental findings illustrate the usefulness of hardware test and validation for DC/AC converter control, the practical challenges entailed and the proposed remedies."}}
{"id": "2507.09469", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09469", "abs": "https://arxiv.org/abs/2507.09469", "authors": ["Haoyang Wang", "Jingao Xu", "Xinyu Luo", "Ting Zhang", "Xuecheng Chen", "Ruiyang Duan", "Jialong Chen", "Yunhao Liu", "Jianfeng Zheng", "Weijie Hong", "Xinlei Chen"], "title": "mmE-Loc: Facilitating Accurate Drone Landing with Ultra-High-Frequency Localization", "comment": "17 pages, 34 figures. arXiv admin note: substantial text overlap with\n  arXiv:2502.14992", "summary": "For precise, efficient, and safe drone landings, ground platforms should\nreal-time, accurately locate descending drones and guide them to designated\nspots. While mmWave sensing combined with cameras improves localization\naccuracy, lower sampling frequency of traditional frame cameras compared to\nmmWave radar creates bottlenecks in system throughput. In this work, we upgrade\ntraditional frame camera with event camera, a novel sensor that harmonizes in\nsampling frequency with mmWave radar within ground platform setup, and\nintroduce mmE-Loc, a high-precision, low-latency ground localization system\ndesigned for precise drone landings. To fully exploit the \\textit{temporal\nconsistency} and \\textit{spatial complementarity} between these two modalities,\nwe propose two innovative modules: \\textit{(i)} the Consistency-instructed\nCollaborative Tracking module, which further leverages the drone's physical\nknowledge of periodic micro-motions and structure for accurate measurements\nextraction, and \\textit{(ii)} the Graph-informed Adaptive Joint Optimization\nmodule, which integrates drone motion information for efficient sensor fusion\nand drone localization. Real-world experiments conducted in landing scenarios\nwith a drone delivery company demonstrate that mmE-Loc significantly\noutperforms state-of-the-art methods in both accuracy and latency.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u4e8b\u4ef6\u76f8\u673a\u5347\u7ea7\u4f20\u7edf\u5e27\u76f8\u673a\uff0c\u5e76\u7ed3\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a mmE-Loc \u7684\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5ef6\u8fdf\u5730\u9762\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u7528\u4e8e\u7cbe\u786e\u7684\u65e0\u4eba\u673a\u7740\u9646\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u7cbe\u786e\u3001\u9ad8\u6548\u548c\u5b89\u5168\u7684\u65e0\u4eba\u673a\u7740\u9646\uff0c\u5730\u9762\u5e73\u53f0\u9700\u8981\u5b9e\u65f6\u3001\u51c6\u786e\u5730\u5b9a\u4f4d\u4e0b\u964d\u7684\u65e0\u4eba\u673a\u5e76\u5c06\u5176\u5f15\u5bfc\u81f3\u6307\u5b9a\u4f4d\u7f6e\u3002\u4f20\u7edf\u5e27\u76f8\u673a\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u76f8\u6bd4\u91c7\u6837\u9891\u7387\u8f83\u4f4e\uff0c\u4f1a\u6210\u4e3a\u7cfb\u7edf\u541e\u5410\u91cf\u7684\u74f6\u9888\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u81f4\u6027\u6307\u5bfc\u7684\u534f\u540c\u8ddf\u8e2a\u6a21\u5757\uff08\u5229\u7528\u65e0\u4eba\u673a\u7684\u7269\u7406\u8fd0\u52a8\u548c\u7ed3\u6784\u77e5\u8bc6\u8fdb\u884c\u7cbe\u786e\u6d4b\u91cf\u63d0\u53d6\uff09\u548c\u56fe\u4fe1\u606f\u81ea\u9002\u5e94\u8054\u5408\u4f18\u5316\u6a21\u5757\uff08\u96c6\u6210\u65e0\u4eba\u673a\u8fd0\u52a8\u4fe1\u606f\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u4f20\u611f\u5668\u878d\u5408\u548c\u65e0\u4eba\u673a\u5b9a\u4f4d\uff09\u3002", "result": "\u5728\u65e0\u4eba\u673a\u914d\u9001\u516c\u53f8\u7684\u5b9e\u9645\u7740\u9646\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cmmE-Loc \u5728\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "mmE-Loc \u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5728\u7cbe\u5ea6\u548c\u5ef6\u8fdf\u65b9\u9762\u90fd\u6709\u63d0\u5347\u3002"}}
{"id": "2507.09111", "categories": ["cs.CV", "cs.HC", "cs.RO", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09111", "abs": "https://arxiv.org/abs/2507.09111", "authors": ["Di Wen", "Kunyu Peng", "Kailun Yang", "Yufan Chen", "Ruiping Liu", "Junwei Zheng", "Alina Roitberg", "Rainer Stiefelhagen"], "title": "RoHOI: Robustness Benchmark for Human-Object Interaction Detection", "comment": "Benchmarks, datasets, and code will be made publicly available at\n  https://github.com/Kratos-Wen/RoHOI", "summary": "Human-Object Interaction (HOI) detection is crucial for robot-human\nassistance, enabling context-aware support. However, models trained on clean\ndatasets degrade in real-world conditions due to unforeseen corruptions,\nleading to inaccurate prediction. To address this, we introduce the first\nrobustness benchmark for HOI detection, evaluating model resilience under\ndiverse challenges. Despite advances, current models struggle with\nenvironmental variability, occlusion, and noise. Our benchmark, RoHOI, includes\n20 corruption types based on HICO-DET and V-COCO datasets and a new\nrobustness-focused metric. We systematically analyze existing models in the\nrelated field, revealing significant performance drops under corruptions. To\nimprove robustness, we propose a Semantic-Aware Masking-based Progressive\nLearning (SAMPL) strategy to guide the model to be optimized based on holistic\nand partial cues, dynamically adjusting the model's optimization to enhance\nrobust feature learning. Extensive experiments show our approach outperforms\nstate-of-the-art methods, setting a new standard for robust HOI detection.\nBenchmarks, datasets, and code will be made publicly available at\nhttps://github.com/Kratos-Wen/RoHOI.", "AI": {"tldr": "\u63d0\u51faRoHOI\u57fa\u51c6\u6d4b\u8bd5\u548cSAMPL\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8HOI\u68c0\u6d4b\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709HOI\u68c0\u6d4b\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7531\u4e8e\u672a\u9884\u89c1\u7684\u635f\u574f\uff08\u5982\u73af\u5883\u53d8\u5316\u3001\u906e\u6321\u548c\u566a\u58f0\uff09\u800c\u6027\u80fd\u4e0b\u964d\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5f15\u5165\u4e86\u9996\u4e2aHOI\u68c0\u6d4b\u9c81\u68d2\u6027\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u8bed\u4e49\u611f\u77e5\u63a9\u7801\u6e10\u8fdb\u5b66\u4e60\u201d\uff08SAMPL\uff09\u7684\u7b56\u7565\uff0c\u4ee5\u6307\u5bfc\u6a21\u578b\u57fa\u4e8e\u6574\u4f53\u548c\u5c40\u90e8\u7ebf\u7d22\u8fdb\u884c\u4f18\u5316\uff0c\u5e76\u52a8\u6001\u8c03\u6574\u4f18\u5316\u8fc7\u7a0b\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u7279\u5f81\u5b66\u4e60\u3002", "result": "\u6240\u63d0\u51fa\u7684SAMPL\u7b56\u7565\u5728HOI\u68c0\u6d4b\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8d85\u8fc7\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684SAMPL\u7b56\u7565\u5728HOI\u68c0\u6d4b\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u9c81\u68d2HOI\u68c0\u6d4b\u6811\u7acb\u4e86\u65b0\u6807\u51c6\u3002"}}
{"id": "2507.10160", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2507.10160", "abs": "https://arxiv.org/abs/2507.10160", "authors": ["Manuel R\u00f6der", "Christoph Raab", "Frank-Michael Schleif"], "title": "Domain Borders Are There to Be Crossed With Federated Few-Shot Adaptation", "comment": "Extension of http://dx.doi.org/10.5220/0012351900003654", "summary": "Federated Learning has emerged as a leading paradigm for decentralized,\nprivacy-preserving learning, particularly relevant in the era of interconnected\nedge devices equipped with sensors. However, the practical implementation of\nFederated Learning faces three primary challenges: the need for human\ninvolvement in costly data labelling processes for target adaptation, covariate\nshift in client device data collection due to environmental factors affecting\nsensors, leading to discrepancies between source and target samples, and the\nimpracticality of continuous or regular model updates in resource-constrained\nenvironments due to limited data transmission capabilities and technical\nconstraints on channel availability and energy efficiency. To tackle these\nissues, we expand upon an efficient and scalable Federated Learning framework\ntailored for real-world client adaptation in industrial settings. This\nframework leverages a pre-trained source model comprising a deep backbone, an\nadaptation module, and a classifier running on a powerful server. By freezing\nthe backbone and classifier during client adaptation on resource-constrained\ndevices, we allow the domain adaptive linear layer to handle target domain\nadaptation, thus minimizing overall computational overhead. Furthermore, this\nsetup, designated as FedAcross+, is extended to encompass the processing of\nstreaming data, thereby rendering the solution suitable for non-stationary\nenvironments. Extensive experimental results demonstrate the effectiveness of\nFedAcross+ in achieving competitive adaptation on low-end client devices with\nlimited target samples, successfully addressing the challenge of domain shift.\nMoreover, our framework accommodates sporadic model updates within\nresource-constrained environments, ensuring practical and seamless deployment.", "AI": {"tldr": "FedAcross+ \u901a\u8fc7\u4f18\u5316\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5728\u8d44\u6e90\u53d7\u9650\u548c\u6570\u636e\u5b58\u5728\u57df\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u9690\u79c1\u4fdd\u62a4\u7684\u5ba2\u6237\u7aef\u81ea\u9002\u5e94\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1. \u76ee\u6807\u57df\u81ea\u9002\u5e94\u9700\u8981\u8017\u8d39\u4eba\u529b\u7684\u6570\u636e\u6807\u6ce8\u6210\u672c\uff1b2. \u5ba2\u6237\u7aef\u8bbe\u5907\u6570\u636e\u6536\u96c6\u7684\u534f\u53d8\u91cf\u504f\u79fb\u95ee\u9898\uff1b3. \u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u6a21\u578b\u66f4\u65b0\u7684\u6301\u7eed\u6027\u6216\u5b9a\u671f\u66f4\u65b0\u7684\u6311\u6218\uff08\u53d7\u9650\u4e8e\u6570\u636e\u4f20\u8f93\u3001\u4fe1\u9053\u53ef\u7528\u6027\u548c\u80fd\u6548\uff09\u3002", "method": "\u63d0\u51fa\u5e76\u6269\u5c55\u4e86\u4e00\u4e2a\u540d\u4e3a FedAcross+ \u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6e90\u6a21\u578b\uff08\u5305\u542b\u6df1\u5ea6\u9aa8\u5e72\u3001\u81ea\u9002\u5e94\u6a21\u5757\u548c\u5206\u7c7b\u5668\uff09\uff0c\u5728\u5ba2\u6237\u7aef\u8bbe\u5907\u4e0a\u901a\u8fc7\u51bb\u7ed3\u9aa8\u5e72\u548c\u5206\u7c7b\u5668\uff0c\u4ec5\u8c03\u6574\u57df\u81ea\u9002\u5e94\u7ebf\u6027\u5c42\u6765\u6700\u5c0f\u5316\u8ba1\u7b97\u5f00\u9500\u3002\u8be5\u6846\u67b6\u8fd8\u652f\u6301\u6d41\u5f0f\u6570\u636e\u5904\u7406\u4ee5\u9002\u5e94\u975e\u5e73\u7a33\u73af\u5883\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cFedAcross+ \u5728\u4f4e\u7aef\u5ba2\u6237\u7aef\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u9002\u5e94\u6027\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u80fd\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u8fdb\u884c\u96f6\u661f\u7684\u6a21\u578b\u66f4\u65b0\uff0c\u786e\u4fdd\u4e86\u90e8\u7f72\u7684\u5b9e\u7528\u6027\u548c\u65e0\u7f1d\u6027\u3002", "conclusion": "FedAcross+ \u6846\u67b6\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u9aa8\u5e72\u548c\u5206\u7c7b\u5668\uff0c\u4ec5\u5728\u5ba2\u6237\u7aef\u8fdb\u884c\u57df\u81ea\u9002\u5e94\u7ebf\u6027\u5c42\u7684\u8c03\u6574\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u76ee\u6807\u57df\u6837\u672c\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4f4e\u7aef\u5ba2\u6237\u7aef\u8bbe\u5907\u4e0a\u8fdb\u884c\u6709\u7ade\u4e89\u529b\u7684\u9002\u5e94\uff0c\u5e76\u6709\u6548\u89e3\u51b3\u4e86\u57df\u504f\u79fb\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u652f\u6301\u6d41\u5f0f\u6570\u636e\u5904\u7406\u548c\u96f6\u661f\u6a21\u578b\u66f4\u65b0\uff0c\u9002\u7528\u4e8e\u975e\u5e73\u7a33\u73af\u5883\u548c\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002"}}
{"id": "2507.09918", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2507.09918", "abs": "https://arxiv.org/abs/2507.09918", "authors": ["Yi Xia"], "title": "First-Principles Theory of Five- and Six-Phonon Scatterings", "comment": null, "summary": "Higher-order phonon scatterings beyond fourth order remain largely unexplored\ndespite their potential importance in strongly anharmonic materials at elevated\ntemperatures. We develop a theoretical formalism for first-principles\ncalculation of five- and six-phonon scatterings using Green's function\ntechniques based on a diagrammatic formalism, and systematically investigate\nmulti-phonon interactions in Si, MgO, and BaO from room temperature to near\nmelting points. Our calculation reveals dramatically different\nmaterial-dependent behaviors: while five- and six-phonon processes remain\nnegligible in Si, they become increasingly important in MgO and dominant in BaO\nnear its melting point, where they surpass three- and four-phonon scattering\nintensity and significantly reduce lattice thermal conductivity. We demonstrate\nthat the strength of higher-order interactions is primarily governed by\ninteratomic force constants, with BaO exhibiting five- and six-phonon\nscattering rates over one order of magnitude stronger than MgO despite\nidentical crystal structures, due to large scattering phase space arising from\nsoftened harmonic interactions. Our work establishes the theoretical foundation\nfor understanding lattice dynamics and thermal transport in strongly anharmonic\nmaterials and at elevated temperatures.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8d85\u51fa\u56db\u9636\u7684\u66f4\u9ad8\u9636\u58f0\u5b50\u6563\u5c04\uff0c\u7279\u522b\u662f\u5728\u9ad8\u6e29\u5f3a\u5185\u8017\u6750\u6599\u4e2d\u3002\u4f5c\u8005\u5f00\u53d1\u4e86\u4e00\u79cd\u8ba1\u7b97\u4e94\u9636\u548c\u516d\u9636\u58f0\u5b50\u6563\u5c04\u7684\u7406\u8bba\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u7845\u3001\u6c27\u5316\u9541\u548c\u6c27\u5316\u94a1\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u5728\u6c27\u5316\u94a1\u4e2d\uff0c\u8fd9\u4e9b\u66f4\u9ad8\u9636\u7684\u58f0\u5b50\u6563\u5c04\u5728\u63a5\u8fd1\u7194\u70b9\u65f6\u53d8\u5f97\u975e\u5e38\u91cd\u8981\uff0c\u751a\u81f3\u8d85\u8fc7\u4e86\u4f4e\u9636\u6563\u5c04\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u70ed\u5bfc\u7387\u3002\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u539f\u5b50\u95f4\u529b\u5e38\u6570\u548c\u6563\u5c04\u76f8\u7a7a\u95f4\u7684\u5f71\u54cd\u3002", "motivation": "\u8d85\u51fa\u56db\u9636\u7684\u66f4\u9ad8\u9636\u58f0\u5b50\u6563\u5c04\u5c3d\u7ba1\u5728\u9ad8\u6e29\u5f3a\u5185\u8017\u6750\u6599\u4e2d\u53ef\u80fd\u5f88\u91cd\u8981\uff0c\u4f46\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u89e3\u5f62\u5f0f\u4e3b\u4e49\u7684\u683c\u6797\u51fd\u6570\u6280\u672f\u7684\u7406\u8bba\u5f62\u5f0f\uff0c\u7528\u4e8e\u8ba1\u7b97\u4e94\u9636\u548c\u516d\u9636\u58f0\u5b50\u6563\u5c04\uff0c\u5e76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u7845\u3001\u6c27\u5316\u9541\u548c\u6c27\u5316\u94a1\u5728\u5ba4\u6e29\u81f3\u7194\u70b9\u9644\u8fd1\u7684\u591a\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7845\u4e2d\uff0c\u4e94\u9636\u548c\u516d\u9636\u58f0\u5b50\u8fc7\u7a0b\u53ef\u5ffd\u7565\u4e0d\u8ba1\uff1b\u5728\u6c27\u5316\u9541\u4e2d\uff0c\u5b83\u4eec\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\uff1b\u5728\u63a5\u8fd1\u7194\u70b9\u7684\u6c27\u5316\u94a1\u4e2d\uff0c\u5b83\u4eec\u5360\u4e3b\u5bfc\u5730\u4f4d\uff0c\u8d85\u8fc7\u4e86\u4e09\u9636\u548c\u56db\u9636\u58f0\u5b50\u6563\u5c04\u5f3a\u5ea6\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u6676\u683c\u70ed\u5bfc\u7387\u3002\u66f4\u9ad8\u9636\u76f8\u4e92\u4f5c\u7528\u7684\u5f3a\u5ea6\u4e3b\u8981\u7531\u539f\u5b50\u95f4\u529b\u5e38\u6570\u63a7\u5236\uff0c\u6c27\u5316\u94a1\u7684\u4e94\u9636\u548c\u516d\u9636\u58f0\u5b50\u6563\u5c04\u7387\u6bd4\u6c27\u5316\u9541\u5f3a\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u5c3d\u7ba1\u6676\u4f53\u7ed3\u6784\u76f8\u540c\uff0c\u8fd9\u662f\u7531\u4e8e\u8f6f\u5316\u7684\u8c10\u6ce2\u76f8\u4e92\u4f5c\u7528\u4ea7\u751f\u4e86\u5927\u7684\u6563\u5c04\u76f8\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u5f3a\u5185\u8017\u6750\u6599\u548c\u9ad8\u6e29\u4e0b\u7684\u6676\u683c\u52a8\u529b\u5b66\u548c\u70ed\u4f20\u8f93\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\u3002"}}
{"id": "2507.09479", "categories": ["quant-ph", "physics.plasm-ph"], "pdf": "https://arxiv.org/pdf/2507.09479", "abs": "https://arxiv.org/abs/2507.09479", "authors": ["Bhuvanesh Sundar", "Bram Evert", "Vasily Geyko", "Andrew Patterson", "Ilon Joseph", "Yuan Shi"], "title": "Simulating plasma wave propagation on a superconducting quantum chip", "comment": "5 pages, 4 figures + supplementary material", "summary": "Quantum computers may one day enable the efficient simulation of\nstrongly-coupled plasmas that lie beyond the reach of classical computation in\nregimes where quantum effects are important and the scale separation is large.\nIn this letter, we take the first step towards efficient simulation of quantum\nplasmas by demonstrating linear plasma wave propagation on a superconducting\nquantum chip. Using high-fidelity and highly expressive device-native gates,\ncombined with a novel error mitigation technique, we simulate the scattering of\nlaser pulses from inhomogeneous plasmas. Our approach is made feasible by the\nidentification of a suitable local spin model whose excitations mimic plasma\nwaves, whose circuit implementation requires a lower gate count than other\nproposed approaches that would require a future fault-tolerant quantum\ncomputer. This work opens avenues to study more complicated phenomena that\ncannot be simulated efficiently on classical computers, such as nonlinear\nquantum dynamics when strongly-coupled plasmas are driven out of equilibrium.", "AI": {"tldr": "\u901a\u8fc7\u5728\u91cf\u5b50\u82af\u7247\u4e0a\u6a21\u62df\u7b49\u79bb\u5b50\u4f53\u6ce2\uff0c\u4e3a\u7814\u7a76\u5f3a\u8026\u5408\u7b49\u79bb\u5b50\u4f53\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u91cf\u5b50\u8ba1\u7b97\u673a\u53ef\u80fd\u6709\u4e00\u5929\u80fd\u591f\u6709\u6548\u5730\u6a21\u62df\u7ecf\u5178\u8ba1\u7b97\u65e0\u6cd5\u8fbe\u5230\u7684\u5f3a\u8026\u5408\u7b49\u79bb\u5b50\u4f53\u3002", "method": "\u5229\u7528\u9ad8\u4fdd\u771f\u5ea6\u548c\u9ad8\u5ea6\u8868\u73b0\u529b\u7684\u8bbe\u5907\u539f\u751f\u95e8\uff0c\u7ed3\u5408\u65b0\u9896\u7684\u8bef\u5dee\u7f13\u89e3\u6280\u672f\uff0c\u6a21\u62df\u4e86\u6fc0\u5149\u8109\u51b2\u4ece\u4e0d\u5747\u5300\u7b49\u79bb\u5b50\u4f53\u4e2d\u6563\u5c04\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u7ebf\u6027\u7b49\u79bb\u5b50\u4f53\u6ce2\u4f20\u64ad\u7684\u8d85\u5bfc\u91cf\u5b50\u82af\u7247\u4e0a\u7684\u6a21\u62df\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u7814\u7a76\u7ecf\u5178\u8ba1\u7b97\u673a\u65e0\u6cd5\u6709\u6548\u6a21\u62df\u7684\u66f4\u590d\u6742\u73b0\u8c61\u6253\u5f00\u4e86 avenues\uff0c\u4f8b\u5982\u5f3a\u8026\u5408\u7b49\u79bb\u5b50\u4f53\u5931\u8861\u65f6\u7684\u975e\u7ebf\u6027\u91cf\u5b50\u52a8\u529b\u5b66\u3002"}}
{"id": "2507.09205", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09205", "abs": "https://arxiv.org/abs/2507.09205", "authors": ["Leiyu Pan", "Bojian Xiong", "Lei Yang", "Renren Jin", "Shaowei Zhang", "Yue Chen", "Ling Shi", "Jiang Zhou", "Junru Wu", "Zhen Wang", "Jianxiang Peng", "Juesi Xiao", "Tianyu Dong", "Zhuowen Han", "Zhuo Chen", "Sangjee Dondrub", "Caizang Tai", "Haixing Zhao", "Huaque Cairang", "Suonan Cairang", "Rou Te", "Lengben Zhaxi", "Gazang Zhaxi", "Zhonglin Ye", "Yuhui Zheng", "Chunyan Peng", "Secha Jia", "Pema Tashi", "Cizhen Jiacuo", "Pema Dorjee", "Hongkai Liu", "Pema Yanggon", "Tsehang Dorjee", "Jiaxin Han", "Qiongying Hu", "Jilin Man", "Huanke You", "Yuqi Ren", "Duo La", "Deyi Xiong"], "title": "Banzhida: Advancing Large Language Models for Tibetan with Curated Data and Continual Pre-Training", "comment": null, "summary": "Large language models have achieved remarkable progress across many\nlanguages. However, Tibetan, as a representative low-resource language, is\nparticularly underrepresented in existing models due to the scarcity of\nhigh-quality training corpora. To address this gap, we curate the largest\nTibetan pre-training corpus to date, aggregating data from diverse sources and\napplying a dedicated data cleaning and processing pipeline tailored for\nTibetan. With the curated data, we continue pre/post-training a multilingual\nbase model into Banzhida, a multilingual large language model that advances\ngenerative AI for Tibetan. To evaluate the Tibetan capabilities of the model,\nwe create new high-quality Tibetan benchmarks, and complement them with\nexisting public benchmarks. Experimental results demonstrate that Banzhida\nconsistently and significantly outperforms both open-source models of similar\nscale and Tibetan-tailored models across a wide range of tasks.", "AI": {"tldr": "Created the largest Tibetan LLM corpus and model (Banzhida), outperforming others on Tibetan tasks.", "motivation": "Addressing the underrepresentation of Tibetan in existing large language models due to scarce training data.", "method": "Curated the largest Tibetan pre-training corpus, continued pre/post-training a multilingual base model into Banzhida, and created new Tibetan benchmarks for evaluation.", "result": "Banzhida significantly outperforms open-source and Tibetan-tailored models on a wide range of tasks.", "conclusion": "Tibetan large language model Banzhida outperforms existing models across various tasks."}}
{"id": "2507.09895", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09895", "abs": "https://arxiv.org/abs/2507.09895", "authors": ["Hyung-Joo Moon", "Chan-Byoung Chae", "Kai-Kit Wong", "Robert W. Heath Jr"], "title": "AI-Enhanced Wide-Area Data Imaging via Massive Non-Orthogonal Direct Device-to-HAPS Transmission", "comment": "7 pages, 6 figures, IEEE Communications Magazine (under revision)", "summary": "Massive Aerial Processing for X MAP-X is an innovative framework for\nreconstructing spatially correlated ground data, such as environmental or\nindustrial measurements distributed across a wide area, into data maps using a\nsingle high altitude pseudo-satellite (HAPS) and a large number of distributed\nsensors. With subframe-level data reconstruction, MAP-X provides a\ntransformative solution for latency-sensitive IoT applications. This article\nexplores two distinct approaches for AI integration in the post-processing\nstage of MAP-X. The DNN-based pointwise estimation approach enables real-time,\nadaptive reconstruction through online training, while the CNN-based image\nreconstruction approach improves reconstruction accuracy through offline\ntraining with non-real-time data. Simulation results show that both approaches\nsignificantly outperform the conventional inverse discrete Fourier transform\n(IDFT)-based linear post-processing method. Furthermore, to enable AI-enhanced\nMAP-X, we propose a ground-HAPS cooperation framework, where terrestrial\nstations collect, process, and relay training data to the HAPS. With its\nenhanced capability in reconstructing field data, AI-enhanced MAP-X is\napplicable to various real-world use cases, including disaster response and\nnetwork management.", "AI": {"tldr": "MAP-X\u901a\u8fc7AI\u540e\u5904\u7406\u6280\u672f\uff08DNN\u6216CNN\uff09\u4f18\u5316\u4e86\u9ad8\u7a7a\u4f2a\u536b\u661f\uff08HAPS\uff09\u548c\u5206\u5e03\u5f0f\u4f20\u611f\u5668\u7684\u6570\u636e\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u7cbe\u5ea6\u7684\u7269\u8054\u7f51\u6570\u636e\u5730\u56fe\u7ed8\u5236\u3002", "motivation": "\u65e8\u5728\u4e3a\u9700\u8981\u5bf9\u5927\u8303\u56f4\u5206\u5e03\u7684\u73af\u5883\u6216\u5de5\u4e1a\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u5b9e\u65f6\u91cd\u5efa\u7684\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4e00\u4e2a\u521b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u63a2\u8ba8\u4e86\u4e24\u79cdAI\u96c6\u6210\u65b9\u6cd5\uff1a\u57fa\u4e8eDNN\u7684\u70b9\u4f30\u8ba1\u548c\u57fa\u4e8eCNN\u7684\u56fe\u50cf\u91cd\u5efa\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7528\u4e8eAI\u589e\u5f3aMAP-X\u7684\u5730\u9762-\u9ad8\u7a7a\u5e73\u53f0\uff08HAPS\uff09\u534f\u4f5c\u6846\u67b6\u3002", "result": "\u4e0e\u4f20\u7edf\u7684IDFT\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4e24\u79cdAI\u65b9\u6cd5\u5747\u663e\u8457\u63d0\u9ad8\u4e86\u91cd\u5efa\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u79cd\u5730\u9762-HAPS\u534f\u4f5c\u6846\u67b6\u4ee5\u652f\u6301AI\u589e\u5f3a\u7684MAP-X\u3002", "conclusion": "MAP-X\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u6216\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684AI\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u7684IDFT\u65b9\u6cd5\uff0c\u4e3a\u7269\u8054\u7f51\u5e94\u7528\u63d0\u4f9b\u4f4e\u5ef6\u8fdf\u7684\u6570\u636e\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10142", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10142", "abs": "https://arxiv.org/abs/2507.10142", "authors": ["Siyi Hu", "Mohamad A Hady", "Jianglin Qiao", "Jimmy Cao", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.", "AI": {"tldr": "MARL\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u53d7\u9650\uff0c\u672c\u6587\u63d0\u51fa\u201c\u9002\u5e94\u6027\u201d\u65b0\u89c6\u89d2\u548c\u4e09\u7ef4\u5ea6\u6846\u67b6\uff08\u5b66\u4e60\u3001\u7b56\u7565\u3001\u573a\u666f\u9002\u5e94\u6027\uff09\uff0c\u4ee5\u6539\u8fdb\u5bf9MARL\u7b97\u6cd5\u5728\u52a8\u6001\u73af\u5883\u4e2d\u6027\u80fd\u7684\u8bc4\u4f30\uff0c\u52a9\u529b\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u73af\u5883\u590d\u6742\u591a\u53d8\uff0c\u5305\u62ec\u4ee3\u7406\u79cd\u7fa4\u6ce2\u52a8\u3001\u4efb\u52a1\u76ee\u6807\u6f14\u53d8\u548c\u6267\u884c\u6761\u4ef6\u4e0d\u4e00\u81f4\u7b49\uff0c\u8fd9\u9650\u5236\u4e86\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u8be5\u9886\u57df\u7684\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u89c6\u89d2\u6765\u8861\u91cfMARL\u7b97\u6cd5\u5728\u6301\u7eed\u53d8\u5316\u7cfb\u7edf\u914d\u7f6e\u548c\u64cd\u4f5c\u9700\u6c42\u4e0b\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u5b66\u4e60\u9002\u5e94\u6027\u3001\u7b56\u7565\u9002\u5e94\u6027\u3001\u573a\u666f\u9a71\u52a8\u9002\u5e94\u6027\u4e09\u4e2a\u5173\u952e\u7ef4\u5ea6\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u201c\u9002\u5e94\u6027\u201d\u4f5c\u4e3a\u8bc4\u4f30MARL\u7b97\u6cd5\u5728\u52a8\u6001\u53d8\u5316\u6761\u4ef6\u4e0b\u53ef\u9760\u6027\u7684\u7edf\u4e00\u89c6\u89d2\u3002", "result": "\u5f15\u5165\u201c\u9002\u5e94\u6027\u201d\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7ed3\u6784\u5316\u6846\u67b6\u6765\u8bc4\u4f30MARL\u7b97\u6cd5\u5728\u52a8\u6001\u53d8\u5316\u6761\u4ef6\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u65e8\u5728\u8d85\u8d8a\u72ed\u9698\u57fa\u51c6\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u652f\u6301\u66f4\u5408\u7406\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u5e76\u6700\u7ec8\u4fc3\u8fdb\u9002\u7528\u4e8e\u52a8\u6001\u771f\u5b9e\u4e16\u754cMAS\u7684MARL\u7b97\u6cd5\u7684\u53d1\u5c55\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u65e8\u5728\u4e3a\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u73b0\u5b9e\u4e16\u754c\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4e2d\u7684\u90e8\u7f72\u63d0\u4f9b\u652f\u6301\uff0c\u901a\u8fc7\u5f15\u5165\u201c\u9002\u5e94\u6027\u201d\u8fd9\u4e00\u6982\u5ff5\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u5b66\u4e60\u9002\u5e94\u6027\u3001\u7b56\u7565\u9002\u5e94\u6027\u548c\u573a\u666f\u9a71\u52a8\u9002\u5e94\u6027\u7684\u7ed3\u6784\u5316\u6846\u67b6\uff0c\u4ee5\u671f\u4fc3\u8fdb\u5bf9MARL\u7b97\u6cd5\u5728\u52a8\u6001\u53d8\u5316\u73af\u5883\u4e2d\u7684\u6027\u80fd\u8fdb\u884c\u66f4\u5408\u7406\u548c\u66f4\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u4ece\u800c\u63a8\u52a8\u80fd\u591f\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u590d\u6742\u6027\u7684MARL\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.08861", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08861", "abs": "https://arxiv.org/abs/2507.08861", "authors": ["Lucas Tesan", "Mikel M. Iparraguirre", "David Gonzalez", "Pedro Martins", "Elias Cueto"], "title": "On the under-reaching phenomenon in message-passing neural PDE solvers: revisiting the CFL condition", "comment": null, "summary": "This paper proposes sharp lower bounds for the number of message passing\niterations required in graph neural networks (GNNs) when solving partial\ndifferential equations (PDE). This significantly reduces the need for\nexhaustive hyperparameter tuning. Bounds are derived for the three fundamental\nclasses of PDEs (hyperbolic, parabolic and elliptic) by relating the physical\ncharacteristics of the problem in question to the message-passing requirement\nof GNNs. In particular, we investigate the relationship between the physical\nconstants of the equations governing the problem, the spatial and temporal\ndiscretisation and the message passing mechanisms in GNNs.\n  When the number of message passing iterations is below these proposed limits,\ninformation does not propagate efficiently through the network, resulting in\npoor solutions, even for deep GNN architectures. In contrast, when the\nsuggested lower bound is satisfied, the GNN parameterisation allows the model\nto accurately capture the underlying phenomenology, resulting in solvers of\nadequate accuracy.\n  Examples are provided for four different examples of equations that show the\nsharpness of the proposed lower bounds.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3aGNN\u6c42\u89e3PDE\u7684\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u8bbe\u5b9a\u4e86\u7406\u8bba\u4e0b\u754c\uff0c\u4ee5\u7b80\u5316\u8d85\u53c2\u6570\u8c03\u4f18\uff0c\u5e76\u786e\u4fdd\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u5728\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u65f6\u5bf9\u7e41\u7410\u7684\u8d85\u53c2\u6570\u8c03\u4f18\u7684\u9700\u6c42\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u7cbe\u786e\u7684\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u4e0b\u754c\u3002", "method": "\u901a\u8fc7\u5c06\u95ee\u9898\u672c\u8eab\u7684\u7269\u7406\u7279\u6027\u4e0eGNN\u7684\u6d88\u606f\u4f20\u9012\u9700\u6c42\u8054\u7cfb\u8d77\u6765\uff0c\u63a8\u5bfc\u4e86\u9488\u5bf9\u53cc\u66f2\u578b\u3001\u629b\u7269\u578b\u548c\u692d\u5706\u578b\u4e09\u7c7b\u57fa\u672cPDE\u7684\u4e0b\u754c\u3002\u7814\u7a76\u4e86\u63a7\u5236\u95ee\u9898\u7684\u7269\u7406\u5e38\u6570\u3001\u7a7a\u95f4\u548c\u65f6\u95f4\u79bb\u6563\u5316\u4ee5\u53caGNN\u4e2d\u7684\u6d88\u606f\u4f20\u9012\u673a\u5236\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e0b\u754c\u662f\u7cbe\u786e\u7684\uff0c\u5e76\u901a\u8fc7\u56db\u4e2a\u4e0d\u540c\u7684\u65b9\u7a0b\u7b97\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u5f53\u8fed\u4ee3\u6b21\u6570\u6ee1\u8db3\u4e0b\u754c\u65f6\uff0cGNN\u80fd\u591f\u51c6\u786e\u6355\u6349PDE\u7684\u5185\u5728\u89c4\u5f8b\uff0c\u5f97\u5230\u9ad8\u7cbe\u5ea6\u7684\u89e3\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6c42\u89e3\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u6240\u9700\u7684\u6700\u5c0f\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u63d0\u4f9b\u4e86\u7cbe\u786e\u7684\u4e0b\u754c\u3002\u5f53\u6d88\u606f\u4f20\u9012\u8fed\u4ee3\u6b21\u6570\u4f4e\u4e8e\u6b64\u4e0b\u754c\u65f6\uff0c\u5373\u4f7f\u91c7\u7528\u6df1\u5c42GNN\u67b6\u6784\uff0c\u4fe1\u606f\u4e5f\u65e0\u6cd5\u5728\u7f51\u7edc\u4e2d\u6709\u6548\u4f20\u64ad\uff0c\u5bfc\u81f4\u6c42\u89e3\u7cbe\u5ea6\u4e0d\u4f73\u3002\u76f8\u53cd\uff0c\u5f53\u6ee1\u8db3\u6240\u63d0\u51fa\u7684\u4e0b\u754c\u65f6\uff0cGNN\u53c2\u6570\u5316\u80fd\u591f\u4f7f\u6a21\u578b\u51c6\u786e\u6355\u6349\u6f5c\u5728\u73b0\u8c61\uff0c\u4ece\u800c\u83b7\u5f97\u8db3\u591f\u7cbe\u5ea6\u7684\u6c42\u89e3\u5668\u3002"}}
{"id": "2507.09617", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09617", "abs": "https://arxiv.org/abs/2507.09617", "authors": ["Margherita Martorana", "Francesca Urgese", "Mark Adamik", "Ilaria Tiddi"], "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs", "comment": null, "summary": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u548c\u77e5\u8bc6\u56fe\u8c31\u7684\u4f18\u70b9\uff0c\u4ee5\u63d0\u9ad8\u4e2a\u4eba\u670d\u52a1\u673a\u5668\u4eba\u7684\u4e92\u64cd\u4f5c\u6027\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7279\u5b9a\u7684LLaMA\u548cGPT\u6a21\u578b\u8868\u73b0\u66f4\u4f18\uff0c\u4f46\u96c6\u6210\u7b56\u7565\u6bd4\u6a21\u578b\u672c\u8eab\u66f4\u91cd\u8981\u3002", "motivation": "\u5f53\u524d\u7684\u4e2a\u4eba\u670d\u52a1\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u4e13\u6709\u7684\u3001\u786c\u7f16\u7801\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8fd9\u4e9b\u65b9\u6848\u4e0e\u7279\u5b9a\u7684\u786c\u4ef6\u548c\u8f6f\u4ef6\u7ed1\u5b9a\uff0c\u5bfc\u81f4\u5b9e\u73b0\u65b9\u5f0f\u5b64\u7acb\uff0c\u96be\u4ee5\u8de8\u5e73\u53f0\u9002\u5e94\u548c\u6269\u5c55\u3002\u672c\u4f53\u548c\u77e5\u8bc6\u56fe\u8c31\uff08KGs\uff09\u901a\u8fc7\u7ed3\u6784\u5316\u548c\u6807\u51c6\u5316\u7684\u77e5\u8bc6\u8868\u793a\u548c\u63a8\u7406\uff0c\u4e3a\u5b9e\u73b0\u8de8\u7cfb\u7edf\u4e92\u64cd\u4f5c\u6027\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002\u7136\u800c\uff0c\u50cfKGs\u548c\u672c\u4f53\u8fd9\u6837\u7684\u7b26\u53f7\u7cfb\u7edf\u5728\u5904\u7406\u539f\u59cb\u548c\u5608\u6742\u7684\u4f20\u611f\u8f93\u5165\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u800c\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u867d\u7136\u64c5\u957f\u89e3\u91ca\u56fe\u50cf\u548c\u81ea\u7136\u8bed\u8a00\u7b49\u8f93\u5165\uff0c\u4f46\u5f80\u5f80\u7f3a\u4e4f\u900f\u660e\u5ea6\u3001\u4e00\u81f4\u6027\u548c\u77e5\u8bc6\u57fa\u7840\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u7684\u611f\u77e5\u80fd\u529b\u548c\u77e5\u8bc6\u56fe\u8c31\u4e0e\u672c\u4f53\u63d0\u4f9b\u7684\u7ed3\u6784\u5316\u8868\u793a\uff0c\u65e8\u5728\u652f\u6301\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u7684\u4e92\u64cd\u4f5c\u6027\u3002\u8be5\u65b9\u6cd5\u751f\u6210\u7b26\u5408\u672c\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\uff0c\u4ece\u800c\u4ee5\u72ec\u7acb\u4e8e\u5e73\u53f0\u7684\u65b9\u5f0f\u6307\u5bfc\u673a\u5668\u4eba\u884c\u4e3a\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u673a\u5668\u4eba\u611f\u77e5\u6570\u636e\u3001\u672c\u4f53\u548c\u4e94\u4e2a\u591a\u6a21\u6001\u6a21\u578b\uff08\u4e09\u79cdLLaMA\u548c\u4e24\u79cdGPT\u6a21\u578b\uff09\uff0c\u5e76\u91c7\u7528\u4e0d\u540c\u6a21\u5f0f\u7684\u795e\u7ecf\u7b26\u53f7\u4ea4\u4e92\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u8bc4\u4f30\u4e86\u751f\u6210\u77e5\u8bc6\u56fe\u8c31\u5728\u591a\u6b21\u8fd0\u884c\u548c\u914d\u7f6e\u4e2d\u7684\u4e00\u81f4\u6027\u548c\u6709\u6548\u6027\uff0c\u5e76\u8fdb\u884c\u4e86\u7edf\u8ba1\u5206\u6790\u4ee5\u8bc4\u4f30\u6027\u80fd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cGPT-o1 \u548c LLaMA 4 Maverick \u5728\u751f\u6210\u7b26\u5408\u672c\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4f46\u65b0\u7684\u6a21\u578b\u5e76\u4e0d\u4e00\u5b9a\u80fd\u5e26\u6765\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u8fd9\u51f8\u663e\u4e86\u96c6\u6210\u7b56\u7565\u5728\u751f\u6210\u7b26\u5408\u672c\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2507.10010", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10010", "abs": "https://arxiv.org/abs/2507.10010", "authors": ["Venkatraman Renganathan"], "title": "Probabilistic Robustness in the Gap Metric", "comment": null, "summary": "Uncertainties influencing the dynamical systems pose a significant challenge\nin estimating the achievable performance of a controller aiming to control such\nuncertain systems. When the uncertainties are of stochastic nature, obtaining\nhard guarantees for the robustness of a controller aiming to hedge against the\nuncertainty is not possible. This issue set the platform for the development of\nprobabilistic robust control approaches. In this work, we utilise the gap\nmetric between the known nominal model and the unknown perturbed model of the\nuncertain system as a tool to gauge the robustness of a controller and\nformulate the gap as a random variable in the setting with stochastic\nuncertainties. Main results of this paper includes giving probabilistic bound\non the gap exceeding a known threshold followed by bounds on the expected gap\nvalue and probabilistic robust stability in terms of the gap metric. Further,\nwe also provide a probabilistic controller performance certification under gap\nuncertainty and probabilistic guarantee on the achievable\n$\\mathcal{H}_{\\infty}$ robustness. Numerical simulations are provided at many\nplaces to demonstrate the proposed approach.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u201c\u95f4\u9699\u5ea6\u91cf\u201d\u7684\u6982\u7387\u65b9\u6cd5\u6765\u91cf\u5316\u548c\u4fdd\u8bc1\u53d7\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u7684\u52a8\u6001\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u63a7\u5236\u5668\u6027\u80fd\u7684\u6982\u7387\u8ba4\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u63a7\u5236\u53d7\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u7684\u52a8\u6001\u7cfb\u7edf\u65f6\uff0c\u7531\u4e8e\u65e0\u6cd5\u83b7\u5f97\u786c\u6027\u4fdd\u8bc1\u800c\u96be\u4ee5\u4f30\u8ba1\u63a7\u5236\u5668\u6027\u80fd\u7684\u95ee\u9898\u3002", "method": "\u672c\u6587\u5c06\u7cfb\u7edf\u6a21\u578b\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e3a\u201c\u95f4\u9699\u5ea6\u91cf\u201d\uff0c\u5e76\u5c06\u5176\u89c6\u4e3a\u968f\u673a\u53d8\u91cf\uff0c\u5728\u6b64\u57fa\u7840\u4e0a\u63a8\u5bfc\u4e86\u95f4\u9699\u5ea6\u91cf\u7684\u6982\u7387\u754c\u9650\u3001\u671f\u671b\u503c\u754c\u9650\u4ee5\u53ca\u57fa\u4e8e\u95f4\u9699\u5ea6\u91cf\u7684\u6982\u7387\u9c81\u68d2\u7a33\u5b9a\u6027\u3002", "result": "\u672c\u6587\u7684\u4e3b\u8981\u7814\u7a76\u7ed3\u679c\u5305\u62ec\uff1a\u7ed9\u51fa\u95f4\u9699\u5ea6\u91cf\u8d85\u8fc7\u5df2\u77e5\u9608\u503c\u7684\u6982\u7387\u754c\u9650\uff0c\u4ee5\u53ca\u671f\u671b\u95f4\u9699\u503c\u548c\u57fa\u4e8e\u95f4\u9699\u5ea6\u91cf\u7684\u6982\u7387\u9c81\u68d2\u7a33\u5b9a\u6027\u754c\u9650\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u6982\u7387\u63a7\u5236\u5668\u6027\u80fd\u8ba4\u8bc1\u548c\u53ef\u5b9e\u73b0\u7684H_inf\u9c81\u68d2\u6027\u7684\u6982\u7387\u4fdd\u8bc1\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4f7f\u7528\u57fa\u4e8e\u201c\u95f4\u9699\u5ea6\u91cf\u201d\u7684\u6982\u7387\u65b9\u6cd5\u6765\u8bc4\u4f30\u548c\u4fdd\u8bc1\u53d7\u968f\u673a\u4e0d\u786e\u5b9a\u6027\u5f71\u54cd\u7684\u52a8\u6001\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u63a7\u5236\u5668\u6027\u80fd\u7684\u6982\u7387\u8ba4\u8bc1\u548c\u671f\u671b\u7684H_inf\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.09505", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09505", "abs": "https://arxiv.org/abs/2507.09505", "authors": ["Tenghui Xie", "Zhiying Song", "Fuxi Wen", "Jun Li", "Guangzhao Liu", "Zijian Zhao"], "title": "TruckV2X: A Truck-Centered Perception Dataset", "comment": null, "summary": "Autonomous trucking offers significant benefits, such as improved safety and\nreduced costs, but faces unique perception challenges due to trucks' large size\nand dynamic trailer movements. These challenges include extensive blind spots\nand occlusions that hinder the truck's perception and the capabilities of other\nroad users. To address these limitations, cooperative perception emerges as a\npromising solution. However, existing datasets predominantly feature light\nvehicle interactions or lack multi-agent configurations for heavy-duty vehicle\nscenarios. To bridge this gap, we introduce TruckV2X, the first large-scale\ntruck-centered cooperative perception dataset featuring multi-modal sensing\n(LiDAR and cameras) and multi-agent cooperation (tractors, trailers, CAVs, and\nRSUs). We further investigate how trucks influence collaborative perception\nneeds, establishing performance benchmarks while suggesting research priorities\nfor heavy vehicle perception. The dataset provides a foundation for developing\ncooperative perception systems with enhanced occlusion handling capabilities,\nand accelerates the deployment of multi-agent autonomous trucking systems. The\nTruckV2X dataset is available at\nhttps://huggingface.co/datasets/XieTenghu1/TruckV2X.", "AI": {"tldr": "TruckV2X\uff1a\u9996\u4e2a\u5927\u89c4\u6a21\u5361\u8f66\u4e2d\u5fc3\u534f\u4f5c\u611f\u77e5\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u5584\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u7684\u611f\u77e5\u548c\u906e\u6321\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u56e0\u5176\u5e9e\u5927\u7684\u4f53\u79ef\u548c\u52a8\u6001\u7684\u62d6\u8f66\u8fd0\u52a8\u6240\u5e26\u6765\u7684\u611f\u77e5\u6311\u6218\uff0c\u4f8b\u5982\u5e7f\u6cdb\u7684\u76f2\u70b9\u548c\u906e\u6321\uff0c\u800c\u73b0\u6709\u7684\u6570\u636e\u96c6\u4e3b\u8981\u5173\u6ce8\u8f7b\u578b\u8f66\u8f86\u7684\u4ea4\u4e92\u6216\u7f3a\u4e4f\u91cd\u578b\u8f66\u8f86\u573a\u666f\u7684\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u3002", "method": "\u4ecb\u7ecd\u4e86TruckV2X\u6570\u636e\u96c6\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u4ee5\u5361\u8f66\u4e3a\u4e2d\u5fc3\u7684\u5927\u89c4\u6a21\u534f\u4f5c\u611f\u77e5\u6570\u636e\u96c6\uff0c\u5305\u542b\u591a\u6a21\u6001\u4f20\u611f\uff08\u6fc0\u5149\u96f7\u8fbe\u548c\u6444\u50cf\u5934\uff09\u548c\u591a\u667a\u80fd\u4f53\u534f\u4f5c\uff08\u7275\u5f15\u8f66\u3001\u62d6\u8f66\u3001\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u548c\u8def\u4fa7\u5355\u5143\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u6027\u80fd\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u4e86\u91cd\u578b\u8f66\u8f86\u611f\u77e5\u7684\u7814\u7a76\u91cd\u70b9\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u5f00\u53d1\u5177\u6709\u589e\u5f3a\u906e\u6321\u5904\u7406\u80fd\u529b\u7684\u534f\u4f5c\u611f\u77e5\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u52a0\u901f\u4e86\u591a\u667a\u80fd\u4f53\u81ea\u52a8\u9a7e\u9a76\u5361\u8f66\u7cfb\u7edf\u7684\u90e8\u7f72\u3002"}}
{"id": "2507.09118", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09118", "abs": "https://arxiv.org/abs/2507.09118", "authors": ["Linlan Huang", "Xusheng Cao", "Haori Lu", "Yifan Meng", "Fei Yang", "Xialei Liu"], "title": "Mind the Gap: Preserving and Compensating for the Modality Gap in CLIP-Based Continual Learning", "comment": "Accepted at ICCV 2025", "summary": "Continual learning aims to enable models to learn sequentially from\ncontinuously incoming data while retaining performance on previously learned\ntasks. With the Contrastive Language-Image Pre-trained model (CLIP) exhibiting\nstrong capabilities across various downstream tasks, there has been growing\ninterest in leveraging CLIP for continual learning in such scenarios. Most\nexisting works overlook the inherent modality gap in CLIP, a key factor in its\ngeneralization and adaptability. In this paper, we analyze the variations in\nthe modality gap during the fine-tuning of vision-language pre-trained models.\nOur observations reveal that the modality gap effectively reflects the extent\nto which pre-trained knowledge is preserved. Based on these insights, we\npropose a simple yet effective method, MG-CLIP, that improves CLIP's\nperformance in class-incremental learning. Our approach leverages modality gap\npreservation to mitigate forgetting and modality gap compensation to enhance\nthe capacity for new data, introducing a novel modality-gap-based perspective\nfor continual learning. Extensive experiments on multiple benchmarks\ndemonstrate that our method outperforms existing approaches without requiring\nadditional replay data. Our code is available at\nhttps://github.com/linlany/MindtheGap.", "AI": {"tldr": "MG-CLIP\u662f\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ba1\u7406CLIP\u4e2d\u7684\u6a21\u6001\u95f4\u9699\u6765\u6539\u5584\u6301\u7eed\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u5bf9\u65b0\u65e7\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u5de5\u4f5c\u5ffd\u7565\u4e86CLIP\u4e2d\u56fa\u6709\u7684\u6a21\u6001\u95f4\u9699\uff0c\u8fd9\u662f\u5176\u6cdb\u5316\u548c\u9002\u5e94\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002\u5728\u672c\u6587\u4e2d\uff0c\u6211\u4eec\u5206\u6790\u4e86\u5fae\u8c03\u89c6\u89c9-\u8bed\u8a00\u9884\u8bad\u7ec3\u6a21\u578b\u8fc7\u7a0b\u4e2d\u6a21\u6001\u95f4\u9699\u7684\u53d8\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMG-CLIP\u7684\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6a21\u6001\u95f4\u9699\u4fdd\u6301\u6765\u51cf\u8f7b\u9057\u5fd8\uff0c\u5e76\u5229\u7528\u6a21\u6001\u95f4\u9699\u8865\u507f\u6765\u589e\u5f3a\u65b0\u6570\u636e\u5bb9\u91cf\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cMG-CLIP\u5728\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u65e0\u9700\u989d\u5916\u56de\u653e\u6570\u636e\u3002", "conclusion": "MG-CLIP\u901a\u8fc7\u4fdd\u6301\u548c\u8865\u507f\u6a21\u6001\u95f4\u9699\u6765\u63d0\u9ad8CLIP\u5728\u7c7b\u522b\u589e\u91cf\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u56de\u653e\u6570\u636e\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.10325", "categories": ["cs.LG", "cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10325", "abs": "https://arxiv.org/abs/2507.10325", "authors": ["Herlock", "Rahimi", "Dionysis Kalogerias"], "title": "Convergence of Agnostic Federated Averaging", "comment": "5 pages, 2 figurres, CAMSAP conference", "summary": "Federated learning (FL) enables decentralized model training without\ncentralizing raw data. However, practical FL deployments often face a key\nrealistic challenge: Clients participate intermittently in server aggregation\nand with unknown, possibly biased participation probabilities. Most existing\nconvergence results either assume full-device participation, or rely on\nknowledge of (in fact uniform) client availability distributions -- assumptions\nthat rarely hold in practice. In this work, we characterize the optimization\nproblem that consistently adheres to the stochastic dynamics of the well-known\n\\emph{agnostic Federated Averaging (FedAvg)} algorithm under random (and\nvariably-sized) client availability, and rigorously establish its convergence\nfor convex, possibly nonsmooth losses, achieving a standard rate of order\n$\\mathcal{O}(1/\\sqrt{T})$, where $T$ denotes the aggregation horizon. Our\nanalysis provides the first convergence guarantees for agnostic FedAvg under\ngeneral, non-uniform, stochastic client participation, without knowledge of the\nparticipation distribution. We also empirically demonstrate that agnostic\nFedAvg in fact outperforms common (and suboptimal) weighted aggregation FedAvg\nvariants, even with server-side knowledge of participation weights.", "AI": {"tldr": "\u5728\u5b9e\u8df5\u4e2d\uff0c\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5e38\u9762\u4e34\u5ba2\u6237\u7aef\u95f4\u6b47\u6027\u53c2\u4e0e\u548c\u672a\u77e5\u53c2\u4e0e\u6982\u7387\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790agnostic FedAvg\u7b97\u6cd5\u5728\u968f\u673a\u5ba2\u6237\u7aef\u53ef\u7528\u6027\u4e0b\u7684\u4f18\u5316\u95ee\u9898\uff0c\u9996\u6b21\u5728\u901a\u7528\u3001\u975e\u5747\u5300\u3001\u968f\u673a\u5ba2\u6237\u7aef\u53c2\u4e0e\u4e0b\uff0c\u4e3aagnostic FedAvg\u63d0\u4f9b\u4e86\u6536\u655b\u4fdd\u8bc1\uff0c\u4e14\u65e0\u9700\u4e86\u89e3\u53c2\u4e0e\u5206\u5e03\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cagnostic FedAvg\u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u52a0\u6743\u805a\u5408\u7684FedAvg\u53d8\u4f53\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\u6536\u655b\u7ed3\u679c\u901a\u5e38\u5047\u8bbe\u6240\u6709\u8bbe\u5907\u90fd\u53c2\u4e0e\uff0c\u6216\u9700\u8981\u4e86\u89e3\uff08\u5b9e\u9645\u662f\u5747\u5300\u7684\uff09\u5ba2\u6237\u7aef\u53ef\u7528\u6027\u5206\u5e03\uff0c\u8fd9\u4e9b\u5047\u8bbe\u5728\u5b9e\u8df5\u4e2d\u5f88\u5c11\u6210\u7acb\u3002\u7136\u800c\uff0c\u5b9e\u9645\u7684\u8054\u90a6\u5b66\u4e60\u90e8\u7f72\u7ecf\u5e38\u9762\u4e34\u5ba2\u6237\u7aef\u95f4\u6b47\u6027\u53c2\u4e0e\u4ee5\u53ca\u672a\u77e5\u7684\u3001\u53ef\u80fd\u6709\u504f\u5dee\u7684\u53c2\u4e0e\u6982\u7387\u8fd9\u4e00\u5173\u952e\u6311\u6218\u3002", "method": "\u901a\u8fc7\u8868\u5f81\u5728\u968f\u673a\u4e14\u53ef\u53d8\u5927\u5c0f\u7684\u5ba2\u6237\u7aef\u53ef\u7528\u6027\u4e0b\uff0cagnostic FedAvg\u7b97\u6cd5\u7684\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u4e25\u683c\u8bc1\u660e\u4e86\u5176\u5728\u51f8\u3001\u53ef\u80fd\u975e\u5149\u6ed1\u635f\u5931\u4e0b\u7684\u6536\u655b\u6027\uff0c\u8fbe\u5230\u4e86\u6807\u51c6\u7684O(1/sqrt(T))\u6536\u655b\u7387\u3002", "result": "\u5728\u805a\u5408\u671fT\u4e0b\uff0c\u6536\u655b\u6027\u8fbe\u5230\u4e86O(1/sqrt(T))\u7684\u6807\u51c6\u901f\u7387\u3002", "conclusion": "\u8be5\u5206\u6790\u9996\u6b21\u5728\u901a\u7528\u3001\u975e\u5747\u5300\u3001\u968f\u673a\u5ba2\u6237\u7aef\u53c2\u4e0e\u4e0b\uff0c\u4e3aagnostic FedAvg\u63d0\u4f9b\u4e86\u6536\u655b\u4fdd\u8bc1\uff0c\u4e14\u65e0\u9700\u4e86\u89e3\u53c2\u4e0e\u5206\u5e03\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u8bc1\u660eagnostic FedAvg\u7684\u6027\u80fd\u4f18\u4e8e\u5176\u4ed6\u52a0\u6743\u805a\u5408\u7684FedAvg\u53d8\u4f53\u3002"}}
{"id": "2507.10036", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2507.10036", "abs": "https://arxiv.org/abs/2507.10036", "authors": ["Sankaran Ramesh", "Prasenjit Mandal", "Pratik Bhagwat", "Yong Li", "T\u00f6nu Pullerits", "Dmitry Baranov"], "title": "Observation of Chiral Phonons in Methylbenzylammonium Lead Iodide", "comment": "4 pages, 1 Figure, Supporting Information", "summary": "An optical phonon at 2.5 meV is observed in a thin film of the chiral metal\nhalide (R-MBA)$_2$PbI$_4$, but is absent in the racemic counterpart, as\nrevealed by femtosecond transient absorption spectroscopy. This experimental\nresult indicates the chiral origin of the 2.5 meV mode and supports recent\ntheoretical predictions of chirality transfer from the organic to the inorganic\nlayers, with implications for the spin polarization properties of hybrid metal\nhalides and perovskites.", "AI": {"tldr": "\u98de\u79d2\u77ac\u6001\u5438\u6536\u5149\u8c31\u5728\u624b\u6027\u91d1\u5c5e\u5364\u5316\u7269 (R-MBA)$_2$PbI$_4$\u8584\u819c\u4e2d\u53d1\u73b0\u4e86\u4e00\u4e2a2.5 meV\u7684\u5149\u5b66\u58f0\u5b50\uff0c\u8fd9\u5728\u6d88\u65cb\u5316\u5408\u7269\u4e2d\u4e0d\u5b58\u5728\uff0c\u8bc1\u660e\u4e86\u624b\u6027\u8d77\u6e90\uff0c\u5e76\u652f\u6301\u4e86\u624b\u6027\u4ece\u6709\u673a\u5c42\u5230\u65e0\u673a\u5c42\u7684\u4f20\u9012\u7684\u7406\u8bba\u3002", "motivation": "\u7814\u7a76\u6df7\u5408\u91d1\u5c5e\u5364\u5316\u7269\u548c\u9499\u949b\u77ff\u7684\u81ea\u65cb\u6781\u5316\u6027\u8d28\uff0c\u4ee5\u53ca\u624b\u6027\u5bf9\u5149\u5b66\u58f0\u5b50\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u98de\u79d2\u77ac\u6001\u5438\u6536\u5149\u8c31\u6280\u672f\u89c2\u5bdf\u5230\u624b\u6027\u91d1\u5c5e\u5364\u5316\u7269 (R-MBA)$_2$PbI$_4$\u8584\u819c\u4e2d\u5b58\u5728\u4e00\u4e2a2.5 meV\u7684\u5149\u5b66\u58f0\u5b50\uff0c\u800c\u5728\u6d88\u65cb\u5bf9\u5e94\u7269\u4e2d\u672a\u89c2\u5bdf\u5230\u8be5\u58f0\u5b50\u3002", "result": "\u5728\u624b\u6027\u91d1\u5c5e\u5364\u5316\u7269 (R-MBA)$_2$PbI$_4$\u8584\u819c\u4e2d\u89c2\u5bdf\u52302.5 meV\u7684\u5149\u5b66\u58f0\u5b50\uff0c\u8be5\u58f0\u5b50\u5728\u6d88\u65cb\u5bf9\u5e94\u7269\u4e2d\u4e0d\u5b58\u5728\uff0c\u8868\u660e\u8be5\u58f0\u5b50\u5177\u6709\u624b\u6027\u8d77\u6e90\uff0c\u5e76\u652f\u6301\u4e86\u624b\u6027\u4ece\u6709\u673a\u5c42\u5230\u65e0\u673a\u5c42\u7684\u4f20\u9012\u7684\u7406\u8bba\u9884\u6d4b\u3002", "conclusion": "\u8be5\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e2.5 meV\u6a21\u5f0f\u7684 the chiral origin\uff0c\u5e76\u652f\u6301\u4e86\u8fd1\u671f\u5173\u4e8e\u624b\u6027\u4ece the organic to the inorganic layers \u7684\u624b\u6027\u4f20\u9012\u7684\u7406\u8bba\u9884\u6d4b\uff0c\u8fd9\u5bf9 hybrid metal halides and perovskites \u7684\u81ea\u65cb\u6781\u5316\u6027\u8d28\u5177\u6709\u542f\u793a\u610f\u4e49\u3002"}}
{"id": "2507.09517", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09517", "abs": "https://arxiv.org/abs/2507.09517", "authors": ["Ramaseshan R", "Prateek P. Kulkarni", "Sharanya Madhusudhan", "Kaustav Bhowmick"], "title": "A theoretical treatment of optical metasurfaces as an efficient basis for quantum correlations", "comment": "13 pages, 5 figures", "summary": "Entanglement is a cornerstone of quantum technology, playing a key role in\nquantum computing, cryptography, and information processing. Conventional\nmethods for generating entanglement via optical setups rely on beam splitters,\nnonlinear media, or quantum dots, which often require bulky configurations and\nprecise phase control. In contrast, metasurfaces - ultrathin, engineered\noptical interfaces - offer a compact and tunable alternative for quantum\nphotonics. In this work, we demonstrate that metasurfaces can serve as a\npromising platform for generating Bell states through a Hamiltonian-driven\nspin-entanglement mechanism. By analyzing the system's evolution under a\nmetasurface interaction Hamiltonian, we show that an initially separable spin\nstate evolves into a maximally entangled Bell state. We further study classical\nand quantum correlations, evaluate the impact of environmental decoherence, and\ncompute quantum discord to quantify correlation robustness beyond entanglement.\nOur analysis shows that metasurfaces can generate Bell states with a\nconcurrence of about 0.995 and maintain quantum discord for up to 29\nmicroseconds. These results establish metasurfaces as scalable, high-fidelity\ncomponents for next-generation quantum photonic architectures.", "AI": {"tldr": "Metasurfaces can compactly generate high-fidelity quantum entanglement (Bell states) and maintain it for a significant duration, offering a promising alternative to traditional methods in quantum technology.", "motivation": "Conventional methods for generating quantum entanglement often require bulky setups and precise phase control. Metasurfaces present a compact and tunable alternative for quantum photonics.", "method": "The study analyzes the evolution of an initially separable spin state under a metasurface interaction Hamiltonian to demonstrate Bell state generation. It also evaluates classical and quantum correlations, the impact of environmental decoherence, and computes quantum discord.", "result": "Metasurfaces can generate Bell states with a concurrence of approximately 0.995 and maintain quantum discord for up to 29 microseconds, indicating high-fidelity entanglement generation and robustness against decoherence.", "conclusion": "Metasurfaces offer a scalable and high-fidelity platform for generating entangled Bell states, with potential applications in next-generation quantum photonic architectures."}}
{"id": "2507.09225", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09225", "abs": "https://arxiv.org/abs/2507.09225", "authors": ["Biagio Scalingi", "Chiara Barattieri di San Pietro", "Paolo Canal", "Valentina Bambini"], "title": "MetaClimage: A novel database of visual metaphors related to Climate Change, with costs and benefits analysis", "comment": "27 pages, 5 figures", "summary": "Visual metaphors of climate change (e.g., melting glaciers depicted as a\nmelting ice grenade) are regarded as valuable tools for addressing the\ncomplexity of environmental challenges. However, few studies have examined\ntheir impact on communication, also due to scattered availability of material.\nHere, we present a novel database of Metaphors of Climate Change in Images\n(MetaClimage) https://doi.org/10.5281/zenodo.15861012, paired with literal\nimages and enriched with human ratings. For each image, we collected values of\ndifficulty, efficacy, artistic quality, and emotional arousal from human\nrating, as well as number of tags generated by participants to summarize the\nmessage. Semantic and emotion variables were further derived from the tags via\nNatural Language Processing. Visual metaphors were rated as more difficult to\nunderstand, yet more aesthetically pleasant than literal images, but did not\ndiffer in efficacy and arousal. The latter for visual metaphors, however, was\nhigher in participants with higher Need For Cognition. Furthermore, visual\nmetaphors received more tags, often referring to entities not depicted in the\nimage, and elicited words with more positive valence and greater dominance than\nliteral images. These results evidence the greater cognitive load of visual\nmetaphors, which nevertheless might induce positive effects such as deeper\ncognitive elaboration and abstraction compared to literal stimuli. Furthermore,\nwhile they are not deemed as more effective and arousing, visual metaphors seem\nto generate superior aesthetic appreciation and a more positively valenced\nexperience. Overall, this study contributes to understanding the impact of\nvisual metaphors of climate change both by offering a database for future\nresearch and by elucidating a cost-benefit trade-off to take into account when\nshaping environmental communication.", "AI": {"tldr": "\u89c6\u89c9\u9690\u55bb\u6bd4\u5b57\u9762\u56fe\u50cf\u66f4\u7f8e\u89c2\u4f46\u66f4\u96be\u61c2\uff0c\u5bf9\u67d0\u4e9b\u4eba\u66f4\u80fd\u5524\u8d77\u60c5\u7eea\u3002\u5b83\u4eec\u80fd\u5f15\u53d1\u66f4\u79ef\u6781\u7684\u4f53\u9a8c\u548c\u66f4\u6df1\u5165\u7684\u601d\u8003\u3002", "motivation": "\u5c3d\u7ba1\u89c6\u89c9\u9690\u55bb\uff08\u5982\u5c06\u878d\u5316\u7684\u51b0\u5ddd\u63cf\u7ed8\u6210\u878d\u5316\u7684\u51b0\u5236\u624b\u69b4\u5f39\uff09\u88ab\u8ba4\u4e3a\u662f\u4f20\u8fbe\u6c14\u5019\u53d8\u5316\u590d\u6742\u6027\u7684\u6709\u6548\u5de5\u5177\uff0c\u4f46\u4ee5\u5f80\u5bf9\u5176\u5728\u4f20\u64ad\u65b9\u9762\u7684\u5f71\u54cd\u7814\u7a76\u8f83\u5c11\uff0c\u90e8\u5206\u539f\u56e0\u662f\u76f8\u5173\u6750\u6599\u96be\u4ee5\u83b7\u53d6\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u89c6\u89c9\u9690\u55bb\u56fe\u50cf\u53ca\u5176\u76f8\u5173\u6570\u636e\u7684\u6570\u636e\u5e93\uff0c\u5e76\u5206\u6790\u8fd9\u4e9b\u56fe\u50cf\u7684\u4f20\u64ad\u6548\u679c\uff0c\u6765\u586b\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u672c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aMetaClimage\u7684\u65b0\u578b\u6570\u636e\u5e93\uff0c\u5176\u4e2d\u5305\u542b\u6c14\u5019\u53d8\u5316\u7684\u89c6\u89c9\u9690\u55bb\u56fe\u50cf\u53ca\u5176\u5bf9\u5e94\u7684\u5b57\u9762\u56fe\u50cf\u3002\u7814\u7a76\u6536\u96c6\u4e86\u4eba\u7c7b\u5bf9\u8fd9\u4e9b\u56fe\u50cf\u7684\u8bc4\u5206\uff0c\u5305\u62ec\u7406\u89e3\u96be\u5ea6\u3001\u6548\u679c\u3001\u827a\u672f\u8d28\u91cf\u548c\u60c5\u7eea\u5524\u8d77\u7a0b\u5ea6\uff0c\u5e76\u5206\u6790\u4e86\u53c2\u4e0e\u8005\u4e3a\u56fe\u50cf\u751f\u6210\u6807\u7b7e\u7684\u6570\u91cf\u3002\u5229\u7528\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\uff0c\u4ece\u6807\u7b7e\u4e2d\u63d0\u53d6\u4e86\u8bed\u4e49\u548c\u60c5\u611f\u53d8\u91cf\u3002\u6700\u7ec8\uff0c\u901a\u8fc7\u6bd4\u8f83\u89c6\u89c9\u9690\u55bb\u548c\u5b57\u9762\u56fe\u50cf\u5728\u8fd9\u4e9b\u6307\u6807\u4e0a\u7684\u5dee\u5f02\uff0c\u6765\u8bc4\u4f30\u89c6\u89c9\u9690\u55bb\u5728\u6c14\u5019\u53d8\u5316\u4f20\u64ad\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u89c6\u89c9\u9690\u55bb\u56fe\u50cf\u88ab\u8ba4\u4e3a\u6bd4\u5b57\u9762\u56fe\u50cf\u66f4\u96be\u7406\u89e3\uff0c\u4f46\u827a\u672f\u6027\u66f4\u9ad8\uff0c\u7136\u800c\u5728\u6548\u679c\u548c\u60c5\u7eea\u5524\u8d77\u65b9\u9762\u65e0\u663e\u8457\u5dee\u5f02\u3002\u4f46\u5bf9\u4e8e\u8ba4\u77e5\u9700\u6c42\u8f83\u9ad8\u7684\u53c2\u4e0e\u8005\uff0c\u89c6\u89c9\u9690\u55bb\u7684\u60c5\u7eea\u5524\u8d77\u66f4\u9ad8\u3002\u89c6\u89c9\u9690\u55bb\u83b7\u5f97\u4e86\u66f4\u591a\u7684\u6807\u7b7e\uff0c\u8fd9\u4e9b\u6807\u7b7e\u5e38\u5e38\u6307\u5411\u56fe\u50cf\u4e2d\u672a\u76f4\u63a5\u63cf\u7ed8\u7684\u5185\u5bb9\uff0c\u5e76\u5f15\u53d1\u4e86\u5177\u6709\u66f4\u79ef\u6781\u8bc4\u4ef7\u548c\u66f4\u5f3a\u652f\u914d\u6027\u7684\u8bcd\u8bed\u3002\u8fd9\u8868\u660e\u89c6\u89c9\u9690\u55bb\u7684\u8ba4\u77e5\u8d1f\u8377\u66f4\u5927\uff0c\u4f46\u53ef\u80fd\u5e26\u6765\u66f4\u6df1\u5c42\u6b21\u7684\u8ba4\u77e5\u52a0\u5de5\u548c\u62bd\u8c61\u5316\u3002\u603b\u7684\u6765\u8bf4\uff0c\u89c6\u89c9\u9690\u55bb\u5728\u7f8e\u5b66\u6b23\u8d4f\u548c\u79ef\u6781\u4f53\u9a8c\u65b9\u9762\u4f18\u4e8e\u5b57\u9762\u56fe\u50cf\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u5305\u542b\u89c6\u89c9\u9690\u55bb\u548c\u5b57\u9762\u56fe\u50cf\u7684\u6570\u636e\u5e93\uff0c\u5e76\u5206\u6790\u4e86\u4eba\u7c7b\u5bf9\u8fd9\u4e9b\u56fe\u50cf\u7684\u8bc4\u5206\u548c\u6807\u7b7e\uff0c\u4e3a\u7406\u89e3\u6c14\u5019\u53d8\u5316\u89c6\u89c9\u9690\u55bb\u7684\u4f20\u64ad\u6548\u679c\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u89c6\u89c9\u9690\u55bb\u867d\u7136\u66f4\u96be\u7406\u89e3\uff0c\u4f46\u66f4\u5177\u5ba1\u7f8e\u4ef7\u503c\uff0c\u5e76\u4e14\u5728\u8ba4\u77e5\u9700\u6c42\u8f83\u9ad8\u7684\u53c2\u4e0e\u8005\u4e2d\u80fd\u5f15\u8d77\u66f4\u9ad8\u7684\u60c5\u7eea\u5524\u8d77\u3002\u6b64\u5916\uff0c\u89c6\u89c9\u9690\u55bb\u80fd\u591f\u5f15\u53d1\u66f4\u79ef\u6781\u7684\u60c5\u611f\u4f53\u9a8c\u548c\u66f4\u6df1\u5165\u7684\u8ba4\u77e5\u52a0\u5de5\uff0c\u5c3d\u7ba1\u5728\u6548\u679c\u548c\u5524\u8d77\u65b9\u9762\u4e0e\u5b57\u9762\u56fe\u50cf\u65e0\u5f02\u3002\u8fd9\u4e9b\u53d1\u73b0\u5bf9\u4e8e\u5728\u73af\u5883\u4f20\u64ad\u4e2d\u6709\u6548\u8fd0\u7528\u89c6\u89c9\u9690\u55bb\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.09987", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09987", "abs": "https://arxiv.org/abs/2507.09987", "authors": ["Zihang Zeng", "Shu Sun", "Meixia Tao", "Yin Xu", "Xianghao Yu"], "title": "VoxelRF: Voxelized Radiance Field for Fast Wireless Channel Modeling", "comment": null, "summary": "Wireless channel modeling in complex environments is crucial for wireless\ncommunication system design and deployment. Traditional channel modeling\napproaches face challenges in balancing accuracy, efficiency, and scalability,\nwhile recent neural approaches such as neural radiance field (NeRF) suffer from\nlong training and slow inference. To tackle these challenges, we propose\nvoxelized radiance field (VoxelRF), a novel neural representation for wireless\nchannel modeling that enables fast and accurate synthesis of spatial spectra.\nVoxelRF replaces the costly multilayer perception (MLP) used in NeRF-based\nmethods with trilinear interpolation of voxel grid-based representation, and\ntwo shallow MLPs to model both propagation and transmitter-dependent effects.\nTo further accelerate training and improve generalization, we introduce\nprogressive learning, empty space skipping, and an additional background\nentropy loss function. Experimental results demonstrate that VoxelRF achieves\ncompetitive accuracy with significantly reduced computation and limited\ntraining data, making it more practical for real-time and resource-constrained\nwireless applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10284", "categories": ["cs.RO", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10284", "abs": "https://arxiv.org/abs/2507.10284", "authors": ["Venkat Margapuri"], "title": "Prompt Informed Reinforcement Learning for Visual Coverage Path Planning", "comment": null, "summary": "Visual coverage path planning with unmanned aerial vehicles (UAVs) requires\nagents to strategically coordinate UAV motion and camera control to maximize\ncoverage, minimize redundancy, and maintain battery efficiency. Traditional\nreinforcement learning (RL) methods rely on environment-specific reward\nformulations that lack semantic adaptability. This study proposes\nPrompt-Informed Reinforcement Learning (PIRL), a novel approach that integrates\nthe zero-shot reasoning ability and in-context learning capability of large\nlanguage models with curiosity-driven RL. PIRL leverages semantic feedback from\nan LLM, GPT-3.5, to dynamically shape the reward function of the Proximal\nPolicy Optimization (PPO) RL policy guiding the agent in position and camera\nadjustments for optimal visual coverage. The PIRL agent is trained using OpenAI\nGym and evaluated in various environments. Furthermore, the sim-to-real-like\nability and zero-shot generalization of the agent are tested by operating the\nagent in Webots simulator which introduces realistic physical dynamics. Results\nshow that PIRL outperforms multiple learning-based baselines such as PPO with\nstatic rewards, PPO with exploratory weight initialization, imitation learning,\nand an LLM-only controller. Across different environments, PIRL outperforms the\nbest-performing baseline by achieving up to 14% higher visual coverage in\nOpenAI Gym and 27% higher in Webots, up to 25% higher battery efficiency, and\nup to 18\\% lower redundancy, depending on the environment. The results\nhighlight the effectiveness of LLM-guided reward shaping in complex spatial\nexploration tasks and suggest a promising direction for integrating natural\nlanguage priors into RL for robotics.", "AI": {"tldr": "PIRL\u901a\u8fc7\u7ed3\u5408LLM\u7684\u8bed\u4e49\u53cd\u9988\u548c\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u6539\u8fdb\u4e86\u65e0\u4eba\u673a\u7684\u89c6\u89c9\u8986\u76d6\u8def\u5f84\u89c4\u5212\uff0c\u5728\u63d0\u9ad8\u8986\u76d6\u7387\u3001\u7535\u6c60\u6548\u7387\u548c\u964d\u4f4e\u5197\u4f59\u5ea6\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u7279\u5b9a\u73af\u5883\u7684\u5956\u52b1\u516c\u5f0f\uff0c\u7f3a\u4e4f\u8bed\u4e49\u9002\u5e94\u6027\uff0c\u8fd9\u5728\u65e0\u4eba\u673a\uff08UAV\uff09\u89c6\u89c9\u8986\u76d6\u8def\u5f84\u89c4\u5212\u4e2d\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u4fe1\u606f\u5f3a\u5316\u5b66\u4e60\uff08PIRL\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u96f6\u6837\u672c\u63a8\u7406\u80fd\u529b\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u4e0e\u597d\u5947\u5fc3\u9a71\u52a8\u7684\u5f3a\u5316\u5b66\u4e60\u76f8\u7ed3\u5408\u3002PIRL \u5229\u7528 LLM\uff08GPT-3.5\uff09\u7684\u8bed\u4e49\u53cd\u9988\u6765\u52a8\u6001\u5851\u9020\u4ee3\u7406\u4f4d\u7f6e\u548c\u76f8\u673a\u8c03\u6574\u4ee5\u5b9e\u73b0\u6700\u4f73\u89c6\u89c9\u8986\u76d6\u7684\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u5f3a\u5316\u5b66\u4e60\u7b56\u7565\u7684\u5956\u52b1\u51fd\u6570\u3002", "result": "PIRL \u5728 OpenAI Gym \u73af\u5883\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 14% \u7684\u89c6\u89c9\u8986\u76d6\u7387\uff0c\u5728 Webots \u6a21\u62df\u5668\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u8fbe 27% \u7684\u89c6\u89c9\u8986\u76d6\u7387\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u7535\u6c60\u6548\u7387\u5e76\u964d\u4f4e\u4e86\u5197\u4f59\u5ea6\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u5305\u62ec PPO\uff08\u9759\u6001\u5956\u52b1\uff09\u3001PPO\uff08\u63a2\u7d22\u6027\u6743\u91cd\u521d\u59cb\u5316\uff09\u3001\u6a21\u4eff\u5b66\u4e60\u548c\u7eaf LLM \u63a7\u5236\u5668\u5728\u5185\u7684\u591a\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u7a81\u51fa\u4e86\u8bed\u8a00\u6a21\u578b\u5f15\u5bfc\u7684\u5956\u52b1\u5851\u9020\u5728\u590d\u6742\u7a7a\u95f4\u63a2\u7d22\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u5c06\u81ea\u7136\u8bed\u8a00\u5148\u9a8c\u77e5\u8bc6\u6574\u5408\u5230\u673a\u5668\u4eba\u5f3a\u5316\u5b66\u4e60\u4e2d\u6307\u660e\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.08866", "categories": ["cs.LG", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08866", "abs": "https://arxiv.org/abs/2507.08866", "authors": ["Marina Ceccon", "Giandomenico Cornacchia", "Davide Dalle Pezze", "Alessandro Fabris", "Gian Antonio Susto"], "title": "Underrepresentation, Label Bias, and Proxies: Towards Data Bias Profiles for the EU AI Act and Beyond", "comment": "Accepted in Expert Systems with Applications", "summary": "Undesirable biases encoded in the data are key drivers of algorithmic\ndiscrimination. Their importance is widely recognized in the algorithmic\nfairness literature, as well as legislation and standards on\nanti-discrimination in AI. Despite this recognition, data biases remain\nunderstudied, hindering the development of computational best practices for\ntheir detection and mitigation. In this work, we present three common data\nbiases and study their individual and joint effect on algorithmic\ndiscrimination across a variety of datasets, models, and fairness measures. We\nfind that underrepresentation of vulnerable populations in training sets is\nless conducive to discrimination than conventionally affirmed, while\ncombinations of proxies and label bias can be far more critical. Consequently,\nwe develop dedicated mechanisms to detect specific types of bias, and combine\nthem into a preliminary construct we refer to as the Data Bias Profile (DBP).\nThis initial formulation serves as a proof of concept for how different bias\nsignals can be systematically documented. Through a case study with popular\nfairness datasets, we demonstrate the effectiveness of the DBP in predicting\nthe risk of discriminatory outcomes and the utility of fairness-enhancing\ninterventions. Overall, this article bridges algorithmic fairness research and\nanti-discrimination policy through a data-centric lens.", "AI": {"tldr": "\u6570\u636e\u504f\u89c1\u662f\u7b97\u6cd5\u6b67\u89c6\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u672c\u6587\u7814\u7a76\u4e86\u4e09\u79cd\u6570\u636e\u504f\u89c1\u53ca\u5176\u5bf9\u7b97\u6cd5\u6b67\u89c6\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u504f\u89c1\u5206\u6790\uff08DBP\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u9884\u6d4b\u6b67\u89c6\u98ce\u9669\u548c\u516c\u5e73\u6027\u589e\u5f3a\u5e72\u9884\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "motivation": "\u6570\u636e\u4e2d\u7684\u4e0d\u826f\u504f\u89c1\u662f\u7b97\u6cd5\u6b67\u89c6\u7684\u5173\u952e\u9a71\u52a8\u56e0\u7d20\uff0c\u5c3d\u7ba1\u5176\u91cd\u8981\u6027\u5df2\u88ab\u5e7f\u6cdb\u8ba4\u8bc6\uff0c\u4f46\u6570\u636e\u504f\u89c1\u672c\u8eab\u7684\u7814\u7a76\u4ecd\u7136\u4e0d\u8db3\uff0c\u963b\u788d\u4e86\u8ba1\u7b97\u6700\u4f73\u5b9e\u8df5\u7684\u53d1\u5c55\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e09\u79cd\u5e38\u89c1\u7684\u6570\u636e\u504f\u89c1\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u6570\u636e\u96c6\u3001\u6a21\u578b\u548c\u516c\u5e73\u6027\u5ea6\u91cf\u4e0b\u7684\u4e2a\u4f53\u53ca\u8054\u5408\u6548\u5e94\uff0c\u5f00\u53d1\u4e86\u7528\u4e8e\u68c0\u6d4b\u7279\u5b9a\u7c7b\u578b\u504f\u89c1\u4ee5\u53ca\u7ec4\u5408\u8fd9\u4e9b\u504f\u89c1\u7684\u673a\u5236\uff0c\u5f62\u6210\u4e86\u6570\u636e\u504f\u89c1\u5206\u6790\uff08DBP\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f31\u52bf\u7fa4\u4f53\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\u5bf9\u6b67\u89c6\u7684\u4fc3\u8fdb\u4f5c\u7528\u4e0d\u5982\u4f20\u7edf\u89c2\u70b9\u6240\u8ba4\u4e3a\u7684\u90a3\u6837\u5173\u952e\uff0c\u800c\u4ee3\u7406\u504f\u89c1\u548c\u6807\u7b7e\u504f\u89c1\u7684\u7ec4\u5408\u53ef\u80fd\u66f4\u4e3a\u5173\u952e\u3002\u6570\u636e\u504f\u89c1\u5206\u6790\uff08DBP\uff09\u5728\u9884\u6d4b\u6b67\u89c6\u98ce\u9669\u548c\u516c\u5e73\u6027\u589e\u5f3a\u5e72\u9884\u65b9\u9762\u663e\u793a\u51fa\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u901a\u8fc7\u6570\u636e\u504f\u89c1\u5206\u6790\u5f25\u5408\u4e86\u7b97\u6cd5\u516c\u5e73\u6027\u7814\u7a76\u4e0e\u53cd\u6b67\u89c6\u653f\u7b56\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u63d0\u51fa\u4e86\u6570\u636e\u504f\u89c1\u5206\u6790\uff08DBP\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u9884\u6d4b\u6b67\u89c6\u98ce\u9669\u548c\u516c\u5e73\u6027\u589e\u5f3a\u5e72\u9884\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09626", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09626", "abs": "https://arxiv.org/abs/2507.09626", "authors": ["Rodion Nazarov", "Anthony Quinn", "Robert Shorten", "Jakub Marecek"], "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems", "comment": null, "summary": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems.", "AI": {"tldr": "\u4e00\u4e2a\u7528\u4e8eAI\u7cfb\u7edf\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7684\u5f00\u6e90PyTorch\u5de5\u5177\u5305\u3002", "motivation": "AI\u7cfb\u7edf\u4e0e\u591a\u4e2a\u667a\u80fd\u4f53\u4ea4\u4e92\uff0c\u9700\u8981\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7684\u5148\u9a8c\u4fdd\u8bc1\u3002", "method": "\u4f7f\u7528\u57fa\u4e8ePyTorch\u7684\u968f\u673a\u63a7\u5236\u6280\u672f\u6765\u6a21\u62dfAI\u7cfb\u7edf\u548c\u5b83\u4eec\u7684\u91cd\u590d\u4f7f\u7528\u3002", "result": "\u8be5\u5de5\u5177\u5305\u7b80\u5316\u4e86\u4e3a\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u95ed\u73af\u6a21\u578b\u63d0\u4f9b\u516c\u5e73\u6027\u4fdd\u8bc1\u7684\u590d\u6742\u6027\u3002", "conclusion": "\u8be5\u5de5\u5177\u5305\u63d0\u4f9b\u4e86\u7528\u4e8e\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u7684\u95ed\u73af\u6a21\u578b\u548c\u516c\u5e73\u6027\u4fdd\u8bc1\u7684\u5de5\u5177\u3002"}}
{"id": "2507.10011", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10011", "abs": "https://arxiv.org/abs/2507.10011", "authors": ["Juan A. Martinez-Velasco", "Alexandre Serrano-Fontova", "Ricard Bosch-Tous", "Pau Casals-Torrens"], "title": "Survey on Methods for Detection, Classification and Location of Faults in Power Systems Using Artificial Intelligence", "comment": null, "summary": "Components of electrical power systems are susceptible to failures caused by\nlightning strikes, aging or human errors. These faults can cause equipment\ndamage, affect system reliability, and results in expensive repair costs. As\nelectric power systems are becoming more complex, traditional protection\nmethods face limitations and shortcomings. Faults in power systems can occur at\nanytime and anywhere, can be caused by a natural disaster or an accident, and\ntheir occurrence can be hardly predicted or avoided; therefore, it is crucial\nto accurately estimate the fault location and quickly restore service. The\ndevelopment of methods capable of accurately detecting, locating and removing\nfaults is essential (i.e. fast isolation of faults is necessary to maintain the\nsystem stability at transmission levels; accurate and fast detection and\nlocation of faults are essential for increasing reliability and customer\nsatisfaction at distribution levels). This has motivated the development of new\nand more efficient methods. Methods developed to detect and locate faults in\npower systems can be divided into two categories, conventional and artificial\nintelligence-based techniques. Although the utilization of artificial\nintelligence (AI) techniques offer tremendous potential, they are challenging\nand time consuming (i.e. many AI techniques require training data for\nprocessing). This paper presents a survey of the application of AI techniques\nto fault diagnosis (detection, classification and location of faults) of lines\nand cables of power systems at both transmission and distribution levels. The\npaper provides a short introduction to AI concepts, a brief summary of the\napplication of AI techniques to power system analysis and design, and a\ndiscussion on AI-based fault diagnosis methods.", "AI": {"tldr": "\u672c\u6587\u8c03\u67e5\u4e86\u4eba\u5de5\u667a\u80fd\u5728\u7535\u529b\u7cfb\u7edf\u6545\u969c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u4f20\u7edf\u4fdd\u62a4\u65b9\u6cd5\u5728\u5e94\u5bf9\u65e5\u76ca\u590d\u6742\u7684\u7535\u529b\u7cfb\u7edf\u6545\u969c\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u5f00\u53d1\u80fd\u591f\u51c6\u786e\u68c0\u6d4b\u3001\u5b9a\u4f4d\u548c\u6392\u9664\u6545\u969c\u7684\u65b0\u65b9\u6cd5\u3002\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u6545\u969c\u8bca\u65ad\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u672c\u6587\u5bf9\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u7535\u529b\u7cfb\u7edf\u6545\u969c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u4ecb\u7ecd\u4e86\u4eba\u5de5\u667a\u80fd\u6982\u5ff5\uff0c\u5e76\u8ba8\u8bba\u4e86\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6545\u969c\u8bca\u65ad\u65b9\u6cd5\u3002", "result": "\u672c\u6587\u5bf9\u4eba\u5de5\u667a\u80fd\u6280\u672f\u5728\u7535\u529b\u7cfb\u7edf\u6545\u969c\u8bca\u65ad\u4e2d\u7684\u5e94\u7528\u8fdb\u884c\u4e86\u8c03\u67e5\uff0c\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "conclusion": "\u672c\u6587\u5bf9\u5e94\u7528\u4e8e\u8f93\u914d\u7535\u7cfb\u7edf\u7ebf\u8def\u548c\u7535\u7f06\u6545\u969c\u8bca\u65ad\uff08\u6545\u969c\u68c0\u6d4b\u3001\u5206\u7c7b\u548c\u5b9a\u4f4d\uff09\u7684\u4eba\u5de5\u667a\u80fd\u6280\u672f\u8fdb\u884c\u4e86\u8c03\u67e5\u3002"}}
{"id": "2507.09537", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09537", "abs": "https://arxiv.org/abs/2507.09537", "authors": ["Yangang Ren", "Guojian Zhan", "Chen Lv", "Jun Li", "Fenghua Liang", "Keqiang Li"], "title": "Self-supervised Pretraining for Integrated Prediction and Planning of Automated Vehicles", "comment": null, "summary": "Predicting the future of surrounding agents and accordingly planning a safe,\ngoal-directed trajectory are crucial for automated vehicles. Current methods\ntypically rely on imitation learning to optimize metrics against the ground\ntruth, often overlooking how scene understanding could enable more holistic\ntrajectories. In this paper, we propose Plan-MAE, a unified pretraining\nframework for prediction and planning that capitalizes on masked autoencoders.\nPlan-MAE fuses critical contextual understanding via three dedicated tasks:\nreconstructing masked road networks to learn spatial correlations, agent\ntrajectories to model social interactions, and navigation routes to capture\ndestination intents. To further align vehicle dynamics and safety constraints,\nwe incorporate a local sub-planning task predicting the ego-vehicle's near-term\ntrajectory segment conditioned on earlier segment. This pretrained model is\nsubsequently fine-tuned on downstream tasks to jointly generate the prediction\nand planning trajectories. Experiments on large-scale datasets demonstrate that\nPlan-MAE outperforms current methods on the planning metrics by a large margin\nand can serve as an important pre-training step for learning-based motion\nplanner.", "AI": {"tldr": "Plan-MAE\u662f\u4e00\u4e2a\u5229\u7528\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u8fdb\u884c\u9884\u6d4b\u548c\u89c4\u5212\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u91cd\u5efa\u9053\u8def\u7f51\u7edc\u3001\u4ee3\u7406\u8f68\u8ff9\u548c\u5bfc\u822a\u8def\u7ebf\u6765\u7406\u89e3\u4e0a\u4e0b\u6587\uff0c\u5e76\u901a\u8fc7\u5b50\u89c4\u5212\u4efb\u52a1\u8c03\u6574\u8f66\u8f86\u52a8\u529b\u5b66\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u5728\u89c4\u5212\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9884\u6d4b\u5468\u56f4\u4ee3\u7406\u7684\u672a\u6765\u5e76\u636e\u6b64\u89c4\u5212\u5b89\u5168\u7684\u3001\u4ee5\u76ee\u6807\u4e3a\u5bfc\u5411\u7684\u8f68\u8ff9\u5bf9\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u6a21\u4eff\u5b66\u4e60\u6765\u4f18\u5316\u9488\u5bf9\u5730\u9762\u771f\u5b9e\u7684\u6307\u6807\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u573a\u666f\u7406\u89e3\u5982\u4f55\u80fd\u591f\u5b9e\u73b0\u66f4\u5177\u6574\u4f53\u6027\u7684\u8f68\u8ff9\u3002", "method": "Plan-MAE\u6846\u67b6\u901a\u8fc7\u4e09\u4e2a\u4e13\u95e8\u7684\u4efb\u52a1\u6765\u878d\u5408\u5173\u952e\u7684\u4e0a\u4e0b\u6587\u7406\u89e3\uff1a\u91cd\u5efa\u63a9\u7801\u9053\u8def\u7f51\u7edc\u4ee5\u5b66\u4e60\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u4ee3\u7406\u8f68\u8ff9\u4ee5\u6a21\u62df\u793e\u4f1a\u4e92\u52a8\uff0c\u4ee5\u53ca\u5bfc\u822a\u8def\u7ebf\u4ee5\u6355\u6349\u76ee\u7684\u5730\u610f\u56fe\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u8c03\u6574\u8f66\u8f86\u52a8\u529b\u5b66\u548c\u5b89\u5168\u7ea6\u675f\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u5c40\u90e8\u7684\u5b50\u89c4\u5212\u4efb\u52a1\uff0c\u8be5\u4efb\u52a1\u6839\u636e\u65e9\u671f\u7247\u6bb5\u5bf9\u81ea\u6211\u8f66\u8f86\u7684\u8fd1\u671f\u8f68\u8ff9\u6bb5\u8fdb\u884c\u9884\u6d4b\u3002\u7136\u540e\uff0c\u5bf9\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u4e0b\u6e38\u4efb\u52a1\u7684\u5fae\u8c03\uff0c\u4ee5\u8054\u5408\u751f\u6210\u9884\u6d4b\u548c\u89c4\u5212\u8f68\u8ff9\u3002", "result": "Plan-MAE\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f5c\u4e3a\u5b66\u4e60\u578b\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u4e00\u4e2a\u91cd\u8981\u9884\u8bad\u7ec3\u6b65\u9aa4\u3002", "conclusion": "Plan-MAE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u9884\u6d4b\u548c\u89c4\u5212\uff0c\u5b83\u5229\u7528\u4e86\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPlan-MAE\u5728\u89c4\u5212\u6307\u6807\u4e0a\u5927\u5e45\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u4f5c\u4e3a\u5b66\u4e60\u578b\u8fd0\u52a8\u89c4\u5212\u5668\u7684\u4e00\u4e2a\u91cd\u8981\u9884\u8bad\u7ec3\u6b65\u9aa4\u3002"}}
{"id": "2507.09122", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09122", "abs": "https://arxiv.org/abs/2507.09122", "authors": ["Chuan Guo", "Inwoo Hwang", "Jian Wang", "Bing Zhou"], "title": "SnapMoGen: Human Motion Generation from Expressive Texts", "comment": "Project Webpage: https://snap-research.github.io/SnapMoGen/", "summary": "Text-to-motion generation has experienced remarkable progress in recent\nyears. However, current approaches remain limited to synthesizing motion from\nshort or general text prompts, primarily due to dataset constraints. This\nlimitation undermines fine-grained controllability and generalization to unseen\nprompts. In this paper, we introduce SnapMoGen, a new text-motion dataset\nfeaturing high-quality motion capture data paired with accurate, expressive\ntextual annotations. The dataset comprises 20K motion clips totaling 44 hours,\naccompanied by 122K detailed textual descriptions averaging 48 words per\ndescription (vs. 12 words of HumanML3D). Importantly, these motion clips\npreserve original temporal continuity as they were in long sequences,\nfacilitating research in long-term motion generation and blending. We also\nimprove upon previous generative masked modeling approaches. Our model,\nMoMask++, transforms motion into multi-scale token sequences that better\nexploit the token capacity, and learns to generate all tokens using a single\ngenerative masked transformer. MoMask++ achieves state-of-the-art performance\non both HumanML3D and SnapMoGen benchmarks. Additionally, we demonstrate the\nability to process casual user prompts by employing an LLM to reformat inputs\nto align with the expressivity and narration style of SnapMoGen. Project\nwebpage: https://snap-research.github.io/SnapMoGen/", "AI": {"tldr": "SnapMoGen\uff1a\u4e00\u4e2a\u65b0\u6570\u636e\u96c6\uff0cMoMask++\uff1a\u4e00\u4e2a\u65b0\u6a21\u578b\uff0c\u5747\u5728\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86SOTA\u6210\u679c\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u8fd0\u52a8\u751f\u6210\u65b9\u6cd5\u53d7\u9650\u4e8e\u6570\u636e\u96c6\uff0c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u7ec6\u63a7\u5236\u548c\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u63d0\u793a\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u4e30\u5bcc\u7684\u8fd0\u52a8\u6570\u636e\u96c6\u548c\u66f4\u5f3a\u5927\u7684\u751f\u6210\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MoMask++ \u7684\u65b0\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u8fd0\u52a8\u8f6c\u6362\u4e3a\u591a\u5c3a\u5ea6\u6807\u8bb0\u5e8f\u5217\uff0c\u5e76\u4f7f\u7528\u5355\u4e00\u751f\u6210\u63a9\u7801 Transformer \u6765\u751f\u6210\u6240\u6709\u6807\u8bb0\uff0c\u4ece\u800c\u66f4\u597d\u5730\u5229\u7528\u6807\u8bb0\u5bb9\u91cf\u3002", "result": "MoMask++ \u5728 HumanML3D \u548c SnapMoGen \u57fa\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5904\u7406\u975e\u6b63\u5f0f\u7528\u6237\u63d0\u793a\u7684\u80fd\u529b\u3002", "conclusion": "SnapMoGen \u662f\u4e00\u4e2a\u5305\u542b\u9ad8\u8d28\u91cf\u8fd0\u52a8\u6355\u6349\u6570\u636e\u548c\u8be6\u7ec6\u6587\u672c\u63cf\u8ff0\u7684\u65b0\u578b\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u4fc3\u8fdb\u957f\u671f\u8fd0\u52a8\u751f\u6210\u548c\u878d\u5408\u7684\u7814\u7a76\u3002MoMask++ \u6a21\u578b\u5728 HumanML3D \u548c SnapMoGen \u57fa\u51c6\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u975e\u6b63\u5f0f\u7684\u7528\u6237\u63d0\u793a\u3002"}}
{"id": "2507.10237", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.10237", "abs": "https://arxiv.org/abs/2507.10237", "authors": ["Diego Ibarra Hoyos", "Gia-Wei Chern", "Israel Klich", "Joseph Poon"], "title": "Quantum-Annealing Enhanced Machine Learning for Interpretable Phase Classification of High-Entropy Alloys", "comment": null, "summary": "High entropy alloys (HEAs) offer unprecedented compositional flexibility for\ndesigning advanced materials, yet predicting their crystallographic phases\nremains a key bottleneck due to limited data and complex phase formation\nbehavior. Here, we present a quantum-enhanced machine learning framework that\nleverages quantum annealing to enhance phase classification in HEAs. Our\npipeline integrates Quantum Boosting (QBoost) for interpretable feature\nselection and classification, with Quantum Support Vector Machines (QSVM) that\nuse quantum-enhanced kernels to capture nonlinear relationships between\nphysical descriptors. By reformulating both models as Quadratic Unconstrained\nBinary Optimization (QUBO) problems, we exploit the efficient sampling\ncapabilities of quantum annealers to achieve rapid training and robust\ngeneralization, demonstrating notable runtime reductions relative to classical\nbaselines in our setup. We target six key phases: FCC, BCC, Sigma, Laves,\nHeusler, and AlXY B2, and benchmark model performance using both\ncross-validation and a rigorously curated test set of prior experimentally\nsynthesized HEAs. The results confirm strong alignment between predicted and\nmeasured phases. Our findings demonstrate that quantum-enhanced classifiers\nmatch or exceed classical models in accuracy and offer insights grounded in\ninterpretable physical descriptors. This work constitutes an important step\ntoward practical quantum acceleration in materials discovery pipelines.", "AI": {"tldr": "\u5229\u7528\u91cf\u5b50\u9000\u706b\u548c\u91cf\u5b50\u589e\u5f3a\u673a\u5668\u5b66\u4e60\uff08QBoost \u548c QSVM\uff09\u6765\u52a0\u901f\u9ad8\u71b5\u5408\u91d1\uff08HEAs\uff09\u7684\u76f8\u9884\u6d4b\uff0c\u5b9e\u73b0\u4e86\u4e0e\u7ecf\u5178\u6a21\u578b\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u51c6\u786e\u6027\u548c\u66f4\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\u3002", "motivation": "\u9ad8\u71b5\u5408\u91d1\uff08HEAs\uff09\u4e3a\u8bbe\u8ba1\u5148\u8fdb\u6750\u6599\u63d0\u4f9b\u4e86\u524d\u6240\u672a\u6709\u7684\u6210\u5206\u7075\u6d3b\u6027\uff0c\u4f46\u7531\u4e8e\u6570\u636e\u6709\u9650\u548c\u76f8\u5f62\u6210\u884c\u4e3a\u590d\u6742\uff0c\u9884\u6d4b\u5176\u6676\u4f53\u5b66\u76f8\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u74f6\u9888\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u589e\u5f3a\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5229\u7528\u91cf\u5b50\u9000\u706b\u6765\u589e\u5f3a\u9ad8\u71b5\u5408\u91d1\uff08HEAs\uff09\u7684\u76f8\u5206\u7c7b\u3002\u8be5\u6d41\u7a0b\u6574\u5408\u4e86\u7528\u4e8e\u53ef\u89e3\u91ca\u7279\u5f81\u9009\u62e9\u548c\u5206\u7c7b\u7684\u91cf\u5b50\u589e\u5f3a\uff08QBoost\uff09\uff0c\u4ee5\u53ca\u4f7f\u7528\u91cf\u5b50\u589e\u5f3a\u6838\u6765\u6355\u6349\u7269\u7406\u63cf\u8ff0\u7b26\u4e4b\u95f4\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff08QSVM\uff09\u3002\u901a\u8fc7\u5c06\u4e24\u79cd\u6a21\u578b\u90fd\u91cd\u65b0\u8868\u8ff0\u4e3a\u4e8c\u6b21\u65e0\u7ea6\u675f\u4e8c\u5143\u4f18\u5316\uff08QUBO\uff09\u95ee\u9898\uff0c\u5229\u7528\u91cf\u5b50\u9000\u706b\u5668\u7684\u6709\u6548\u91c7\u6837\u80fd\u529b\u6765\u5b9e\u73b0\u5feb\u901f\u8bad\u7ec3\u548c\u9c81\u68d2\u6cdb\u5316\uff0c\u5e76\u4e0e\u7ecf\u5178\u57fa\u7ebf\u76f8\u6bd4\u5728\u6211\u4eec\u7684\u8bbe\u7f6e\u4e2d\u5c55\u793a\u4e86\u663e\u8457\u7684\u8fd0\u884c\u65f6\u957f\u7f29\u51cf\u3002", "result": "\u6211\u4eec\u9488\u5bf9\u516d\u4e2a\u5173\u952e\u76f8\uff1a\u9762\u5fc3\u7acb\u65b9\uff08FCC\uff09\u3001\u4f53\u5fc3\u7acb\u65b9\uff08BCC\uff09\u3001\u03c3\u76f8\u3001\u62c9\u592b\u76f8\uff08Laves\uff09\u3001\u4f11\u65af\u52d2\u76f8\uff08Heusler\uff09\u548c AlXY B2 \u76f8\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u4f7f\u7528\u4ea4\u53c9\u9a8c\u8bc1\u548c\u4e25\u683c\u7b5b\u9009\u7684\u5148\u524d\u5b9e\u9a8c\u5408\u6210 HEAs \u6d4b\u8bd5\u96c6\u5bf9\u6a21\u578b\u6027\u80fd\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002\u7ed3\u679c\u8bc1\u5b9e\u9884\u6d4b\u76f8\u548c\u6d4b\u91cf\u76f8\u4e4b\u95f4\u5177\u6709\u5f88\u5f3a\u7684\u543b\u5408\u6027\u3002", "conclusion": "\u91cf\u5b50\u589e\u5f3a\u5206\u7c7b\u5668\u5728\u51c6\u786e\u6027\u4e0a\u5339\u914d\u6216\u8d85\u8fc7\u7ecf\u5178\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u57fa\u4e8e\u53ef\u89e3\u91ca\u7269\u7406\u63cf\u8ff0\u7b26\u7684\u89c1\u89e3\u3002\u8fd9\u9879\u5de5\u4f5c\u662f\u671d\u7740\u6750\u6599\u53d1\u73b0\u7ba1\u9053\u7684\u5b9e\u9645\u91cf\u5b50\u52a0\u901f\u8fc8\u51fa\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2507.09521", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09521", "abs": "https://arxiv.org/abs/2507.09521", "authors": ["Yun-Wen Mao", "Ilia Tutunnikov", "Roman V. Krems", "Ilya Sh. Averbukh"], "title": "Echoes in a parametrically perturbed Kerr-nonlinear oscillator", "comment": "Main text 4 pages with 4 figures. Remaining pages are bibliography\n  and appendix. 12 pages in total", "summary": "We study classical and quantum echoes in a Kerr oscillator driven by a\nfrequency-controlling pulsed perturbation. We consider dynamical response to\nthe perturbation for a single coherent state and for Schr\\\"odinger cat states\nconstructed as both balanced and imbalanced superpositions of two coherent\nstates. For individual coherent states, we demonstrate that a weak parametric\ndrive yields a long-lived sequence of classical echoes. Cat states are found to\nexhibit distinct quantum echoes that are sensitive to the initial relative\nphase and weights of the coherent states in superposition. We examine the\neffect of dissipation on quantum echoes and quantum revivals of cat states. We\ndemonstrate that, even when dissipation suppresses quantum revivals, quantum\nechoes can be recovered by properly tuning the timing and strength of the\nperturbation. These results may be useful for characterizing and mitigating\nerrors of cat qubits.", "AI": {"tldr": "\u7814\u7a76\u4e86Kerr\u632f\u8361\u5668\u4e2d\u76f8\u5e72\u6001\u548c\u732b\u6001\u7684\u56de\u58f0\u52a8\u529b\u5b66\u3002\u53d1\u73b0\u5f31\u9a71\u52a8\u4f1a\u4ea7\u751f\u7ecf\u5178\u56de\u58f0\uff0c\u800c\u732b\u6001\u4f1a\u4ea7\u751f\u5bf9\u76f8\u4f4d\u548c\u6743\u91cd\u654f\u611f\u7684\u91cf\u5b50\u56de\u58f0\u3002\u8017\u6563\u4f1a\u5f71\u54cd\u91cf\u5b50\u56de\u58f0\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u6270\u52a8\u6765\u6062\u590d\u3002\u7ed3\u679c\u53ef\u7528\u4e8e\u732b\u91cf\u5b50\u6bd4\u7279\u7684\u8bef\u5dee\u7ba1\u7406\u3002", "motivation": "\u7814\u7a76Kerr\u632f\u8361\u5668\u4e2d\u7ecf\u5178\u548c\u91cf\u5b50\u56de\u58f0\u7684\u6027\u8d28\uff0c\u4ee5\u53ca\u5b83\u4eec\u5bf9\u76f8\u5e72\u6001\u548c\u732b\u6001\u7684\u54cd\u5e94\uff0c\u65e8\u5728\u4e3a\u8868\u5f81\u548c\u51cf\u8f7b\u732b\u91cf\u5b50\u6bd4\u7279\u7684\u8bef\u5dee\u63d0\u4f9b\u53c2\u8003\u3002", "method": "\u7814\u7a76\u4e86\u7531\u9891\u7387\u63a7\u5236\u7684\u8109\u51b2\u6270\u52a8\u9a71\u52a8\u7684Kerr\u632f\u8361\u5668\u4e2d\u7684\u7ecf\u5178\u56de\u58f0\u548c\u91cf\u5b50\u56de\u58f0\u3002\u8003\u8651\u4e86\u5355\u4e2a\u76f8\u5e72\u6001\u548c\u7531\u4e24\u4e2a\u76f8\u5e72\u6001\u7684\u5e73\u8861\u548c\u4e0d\u5e73\u8861\u53e0\u52a0\u6784\u9020\u7684\u859b\u5b9a\u8c14\u732b\u6001\u7684\u52a8\u529b\u5b66\u54cd\u5e94\u3002", "result": "\u7ecf\u5178\u76f8\u5e72\u6001\u4ea7\u751f\u957f\u5bff\u547d\u7684\u7ecf\u5178\u56de\u58f0\u3002\u732b\u6001\u4ea7\u751f\u5bf9\u521d\u59cb\u76f8\u5bf9\u76f8\u4f4d\u548c\u6743\u91cd\u654f\u611f\u7684\u91cf\u5b50\u56de\u58f0\u3002\u8017\u6563\u4f1a\u5f71\u54cd\u91cf\u5b50\u56de\u58f0\u548c\u732b\u6001\u7684\u91cf\u5b50\u590d\u5174\uff0c\u4f46\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574\u6270\u52a8\u53c2\u6570\u6765\u6062\u590d\u91cf\u5b50\u56de\u58f0\u3002", "conclusion": " Kerr\u632f\u8361\u5668\u4e2d\uff0c\u5f31\u53c2\u91cf\u9a71\u52a8\u4f1a\u4ea7\u751f\u957f\u5bff\u547d\u7684\u7ecf\u5178\u56de\u58f0\u3002\u5bf9\u4e8e\u732b\u6001\uff0c\u91cf\u5b50\u56de\u58f0\u5bf9\u521d\u59cb\u76f8\u5bf9\u76f8\u4f4d\u548c\u53e0\u52a0\u7684\u76f8\u5e72\u6001\u6743\u91cd\u654f\u611f\u3002\u5373\u4f7f\u8017\u6563\u6291\u5236\u4e86\u91cf\u5b50\u590d\u5174\uff0c\u901a\u8fc7\u8c03\u6574\u6270\u52a8\u7684\u65f6\u95f4\u548c\u5f3a\u5ea6\u4e5f\u53ef\u4ee5\u6062\u590d\u91cf\u5b50\u56de\u58f0\u3002\u8fd9\u4e9b\u7ed3\u679c\u53ef\u7528\u4e8e\u8868\u5f81\u548c\u51cf\u8f7b\u732b\u91cf\u5b50\u6bd4\u7279\u7684\u8bef\u5dee\u3002"}}
{"id": "2507.09245", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09245", "abs": "https://arxiv.org/abs/2507.09245", "authors": ["Deshan Sumanathilaka", "Sameera Perera", "Sachithya Dharmasiri", "Maneesha Athukorala", "Anuja Dilrukshi Herath", "Rukshan Dias", "Pasindu Gamage", "Ruvan Weerasinghe", "Y. H. P. P. Priyadarshana"], "title": "Swa-bhasha Resource Hub: Romanized Sinhala to Sinhala Transliteration Systems and Data Resources", "comment": "13 pages, 3 Tables, 3 figures", "summary": "The Swa-bhasha Resource Hub provides a comprehensive collection of data\nresources and algorithms developed for Romanized Sinhala to Sinhala\ntransliteration between 2020 and 2025. These resources have played a\nsignificant role in advancing research in Sinhala Natural Language Processing\n(NLP), particularly in training transliteration models and developing\napplications involving Romanized Sinhala. The current openly accessible data\nsets and corresponding tools are made publicly available through this hub. This\npaper presents a detailed overview of the resources contributed by the authors\nand includes a comparative analysis of existing transliteration applications in\nthe domain.", "AI": {"tldr": "Swa-bhasha\u8d44\u6e90\u4e2d\u5fc3\u63d0\u4f9b\u4e86\u7528\u4e8e\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u5230\u50e7\u4f3d\u7f57\u8bed\u97f3\u8bd1\u7684\u6570\u636e\u8d44\u6e90\u548c\u7b97\u6cd5\uff0c\u4fc3\u8fdb\u4e86\u50e7\u4f3d\u7f57\u8bedNLP\u7684\u7814\u7a76\u3002", "motivation": "\u8be5\u4e2d\u5fc3\u7684\u76ee\u6807\u662f\u4e3a\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u5230\u50e7\u4f3d\u7f57\u8bed\u7684\u97f3\u8bd1\u63d0\u4f9b\u8d44\u6e90\uff0c\u5e76\u4fc3\u8fdb\u50e7\u4f3d\u7f57\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7684\u7814\u7a76\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u4f9b\u4e86\u5bf9\u4f5c\u8005\u8d21\u732e\u7684\u8d44\u6e90\u7684\u8be6\u7ec6\u6982\u8ff0\uff0c\u5e76\u5bf9\u8be5\u9886\u57df\u73b0\u6709\u7684\u97f3\u8bd1\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "result": "\u8be5\u4e2d\u5fc3\u516c\u5f00\u63d0\u4f9b\u4e86\u53ef\u516c\u5f00\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u548c\u76f8\u5e94\u5de5\u5177\uff0c\u5e76\u5bf9\u73b0\u6709\u97f3\u8bd1\u5e94\u7528\u7a0b\u5e8f\u8fdb\u884c\u4e86\u6bd4\u8f83\u5206\u6790\u3002", "conclusion": "\u8be5Swa-bhasha\u8d44\u6e90\u4e2d\u5fc3\u63d0\u4f9b\u4e86\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u5230\u50e7\u4f3d\u7f57\u8bed\u97f3\u8bd1\u7684\u5168\u9762\u6570\u636e\u8d44\u6e90\u548c\u7b97\u6cd5\uff0c\u63a8\u52a8\u4e86\u50e7\u4f3d\u7f57\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\uff08NLP\uff09\u7684\u7814\u7a76\uff0c\u7279\u522b\u662f\u5728\u97f3\u8bd1\u6a21\u578b\u8bad\u7ec3\u548c\u6d89\u53ca\u7f57\u9a6c\u5316\u50e7\u4f3d\u7f57\u8bed\u7684\u5e94\u7528\u5f00\u53d1\u65b9\u9762\u3002\u8be5\u4e2d\u5fc3\u516c\u5f00\u63d0\u4f9b\u4e86\u53ef\u516c\u5f00\u8bbf\u95ee\u7684\u6570\u636e\u96c6\u548c\u76f8\u5e94\u5de5\u5177\u3002"}}
{"id": "2507.09999", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09999", "abs": "https://arxiv.org/abs/2507.09999", "authors": ["Lital Dabush", "Nir Shlezinger", "Tirza Routtenberg"], "title": "Sparsity-Aware Extended Kalman Filter for Tracking Dynamic Graphs", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "A broad range of applications involve signals with irregular structures that\ncan be represented as a graph. As the underlying structures can change over\ntime, the tracking dynamic graph topologies from observed signals is a\nfundamental challenge in graph signal processing (GSP), with applications in\nvarious domains, such as power systems, the brain-machine interface, and\ncommunication systems. In this paper, we propose a method for tracking dynamic\nchanges in graph topologies. Our approach builds on a representation of the\ndynamics as a graph-based nonlinear state-space model (SSM), where the\nobservations are graph signals generated through graph filtering, and the\nunderlying evolving topology serves as the latent states. In our formulation,\nthe graph Laplacian matrix is parameterized using the incidence matrix and edge\nweights, enabling a structured representation of the state. In order to track\nthe evolving topology in the resulting SSM, we develop a sparsity-aware\nextended Kalman filter (EKF) that integrates $\\ell_1$-regularized updates\nwithin the filtering process. Furthermore, a dynamic programming scheme to\nefficiently compute the Jacobian of the graph filter is introduced. Our\nnumerical study demonstrates the ability of the proposed method to accurately\ntrack sparse and time-varying graphs under realistic conditions, with highly\nnonlinear measurements, various noise levels, and different change rates, while\nmaintaining low computational complexity.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u8ddf\u8e2a\u52a8\u6001\u56fe\u62d3\u6251\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u52a8\u6001\u6027\u6784\u5efa\u4e3a\u57fa\u4e8e\u56fe\u7684\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u96c6\u6210\u4e86\u21131\u6b63\u5219\u5316\u66f4\u65b0\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fdb\u884c\u8ddf\u8e2a\u3002", "motivation": "\u8ddf\u8e2a\u52a8\u6001\u56fe\u62d3\u6251\u662f\u56fe\u4fe1\u53f7\u5904\u7406\uff08GSP\uff09\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\uff0c\u4f8b\u5982\u7535\u529b\u7cfb\u7edf\u3001\u8111\u673a\u63a5\u53e3\u548c\u901a\u4fe1\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u5c06\u52a8\u6001\u5efa\u6a21\u4e3a\u57fa\u4e8e\u56fe\u7684\u975e\u7ebf\u6027\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff0c\u5e76\u91c7\u7528\u6574\u5408\u4e86\u21131\u6b63\u5219\u5316\u66f4\u65b0\u7684\u6269\u5c55\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\uff08EKF\uff09\u4ee5\u53ca\u7528\u4e8e\u8ba1\u7b97\u56fe\u6ee4\u6ce2\u5668\u96c5\u53ef\u6bd4\u77e9\u9635\u7684\u52a8\u6001\u89c4\u5212\u65b9\u6848\u6765\u8ddf\u8e2a\u52a8\u6001\u56fe\u62d3\u6251\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5730\u8ddf\u8e2a\u7a00\u758f\u4e14\u65f6\u53d8\u7684\u56fe\uff0c\u5728\u5177\u6709\u9ad8\u5ea6\u975e\u7ebf\u6027\u6d4b\u91cf\u3001\u5404\u79cd\u566a\u58f0\u6c34\u5e73\u548c\u4e0d\u540c\u53d8\u5316\u7387\u7684\u5b9e\u9645\u6761\u4ef6\u4e0b\u8868\u73b0\u826f\u597d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u7cbe\u786e\u5730\u8ddf\u8e2a\u7a00\u758f\u4e14\u65f6\u53d8\u7684\u56fe\uff0c\u5177\u6709\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u3002"}}
{"id": "2507.10522", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.", "AI": {"tldr": "DeepResearch$^{\text{Eco}}$\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u667a\u80fdLLM\u7cfb\u7edf\uff0c\u53ef\u81ea\u52a8\u5316\u79d1\u5b66\u6587\u732e\u7684\u7efc\u5408\uff0c\u63d0\u9ad8\u68c0\u7d22\u6548\u7387\u548c\u8d28\u91cf\uff0c\u5e76\u5728\u751f\u6001\u5b66\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u79d1\u5b66\u6587\u732e\u68c0\u7d22\u7684\u591a\u6837\u6027\u548c\u7ec6\u5fae\u6027\uff0c\u5e76\u5b9e\u73b0\u9ad8\u901a\u91cf\u7684\u9886\u57df\u7279\u5b9a\u8bc1\u636e\u6574\u5408\uff0c\u540c\u65f6\u4fdd\u6301\u5206\u6790\u4e25\u8c28\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepResearch$^{\text{Eco}}$\u7684\u65b0\u578b\u57fa\u4e8e\u667a\u80fdLLM\u7684\u81ea\u52a8\u5316\u79d1\u5b66\u7efc\u5408\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u652f\u6301\u9012\u5f52\u7684\u3001\u53d7\u6df1\u5ea6\u548c\u5e7f\u5ea6\u63a7\u5236\u7684\u7814\u7a76\u95ee\u9898\u63a2\u7d22\uff0c\u5e76\u5177\u6709\u7528\u6237\u53ef\u63a7\u7684\u7efc\u5408\u80fd\u529b\u3001\u900f\u660e\u7684\u63a8\u7406\u548c\u53c2\u6570\u9a71\u52a8\u7684\u53ef\u914d\u7f6e\u6027\u3002", "result": "\u572849\u4e2a\u751f\u6001\u5b66\u7814\u7a76\u95ee\u9898\u4e0a\u7684\u5e94\u7528\u663e\u793a\uff0cDeepResearch$^{\text{Eco}}$\u7684\u6e90\u6574\u5408\u5ea6\u63d0\u9ad8\u4e8621\u500d\uff0c\u6bcf\u5343\u8bcd\u6574\u5408\u7684\u6587\u732e\u6e90\u6570\u91cf\u589e\u52a0\u4e8614.9\u500d\uff0c\u5728\u9ad8\u53c2\u6570\u8bbe\u7f6e\u4e0b\uff0c\u5206\u6790\u6df1\u5ea6\u548c\u80cc\u666f\u591a\u6837\u6027\u8fbe\u5230\u4e86\u4e13\u5bb6\u6c34\u5e73\u3002", "conclusion": "DeepResearch$^{\text{Eco}}$\u901a\u8fc7\u589e\u5f3a\u641c\u7d22\u591a\u6837\u6027\u548c\u68c0\u7d22\u76f8\u5173\u79d1\u5b66\u6587\u732e\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6d41\u7a0b\u66f4\u9ad8\u7684\u6e90\u6574\u5408\u5ea6\u548c\u6bcf\u5343\u8bcd\u6574\u5408\u5ea6\uff0c\u5e76\u5728\u5e94\u7528\u4e8e\u751f\u6001\u5b66\u7814\u7a76\u95ee\u9898\u65f6\uff0c\u5728\u9ad8\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e13\u5bb6\u7ea7\u522b\u7684\u5206\u6790\u6df1\u5ea6\u548c\u80cc\u666f\u591a\u6837\u6027\u3002"}}
{"id": "2507.09662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.", "AI": {"tldr": "LRMs\u63a8\u7406\u592a\u6162\u4e14\u5197\u957f\uff0c\u9700\u8981\u66f4\u7b80\u6d01\u81ea\u9002\u5e94\u7684\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LRMs\u751f\u6210\u5197\u957f\u63a8\u7406\u94fe\u7684\u95ee\u9898\uff0c\u9700\u8981\u5b66\u4e60\u6839\u636e\u8f93\u5165\u96be\u5ea6\u8fdb\u884c\u81ea\u9002\u5e94\u63a8\u7406\uff0c\u4ee5\u7f29\u77ed\u63a8\u7406\u94fe\u5e76\u63d0\u9ad8\u63a8\u7406\u6548\u7387\u3002", "method": "\u672c\u6587\u5bf9LRMs\u7684\u7b80\u6d01\u548c\u81ea\u9002\u5e94\u63a8\u7406\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6982\u8ff0\uff0c\u6db5\u76d6\u4e86\u65b9\u6cd5\u3001\u57fa\u51c6\u548c\u672a\u6765\u6311\u6218\u3002", "result": "\u672c\u6587\u65e8\u5728\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u5feb\u901f\u4e86\u89e3\u8be5\u9886\u57df\u7684\u8fdb\u5c55\uff0c\u5e76\u6fc0\u53d1\u65b0\u7684\u81ea\u9002\u5e94\u63a8\u7406\u601d\u8def\uff0c\u4ee5\u4fc3\u8fdbLRMs\u7684\u66f4\u597d\u4f7f\u7528\u3002", "conclusion": "LRMs\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u7b49\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4f1a\u751f\u6210\u5197\u957f\u5197\u4f59\u7684\u63a8\u7406\u94fe\uff0c\u6d6a\u8d39\u8d44\u6e90\u5e76\u51cf\u6162\u54cd\u5e94\u901f\u5ea6\u3002\u56e0\u6b64\uff0c\u6839\u636e\u8f93\u5165\u96be\u5ea6\u5b66\u4e60\u81ea\u9002\u5e94\u63a8\u7406\u4ee5\u7f29\u77ed\u63a8\u7406\u94fe\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.10123", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.10123", "abs": "https://arxiv.org/abs/2507.10123", "authors": ["Ruotong Sun", "Ermin Wei", "Lihui Yi"], "title": "Optimal Battery Placement in Power Grid", "comment": "10 pages, 2 figures", "summary": "We study the optimal placement of an unlimited-capacity battery in power\ngrids under a centralized market model, where the independent system operator\n(ISO) aims to minimize total generation costs through load shifting. The\noptimal battery placement is not well understood by the existing literature,\nespecially regarding the influence of network topology on minimizing generation\ncosts. Our work starts with decomposing the Mixed-Integer Linear Programming\n(MILP) problem into a series of Linear Programming (LP) formulations. For power\ngrids with sufficiently large generation capacity or tree topologies, we derive\nanalytical cost expressions demonstrating that, under reasonable assumptions,\nthe weighted degree is the only topological factor for optimal battery\nplacement. We also discuss the minor impact of higher-order topological\nconditions on tree-topology networks. To find the localized nature of a single\nbattery's impact, we establish that the relative cost-saving benefit of a\nsingle battery decreases as the network scales. Furthermore, we design a\nlow-complexity algorithm for weakly-cyclic networks. Numerical experiments show\nthat our algorithm is not only approximately 100 times faster than commercial\nsolvers but also maintains high accuracy even when some theoretical assumptions\nare relaxed.", "AI": {"tldr": "\u8be5\u7814\u7a76\u89e3\u51b3\u4e86\u7535\u7f51\u4e2d\u7535\u6c60\u7684\u4f18\u5316\u653e\u7f6e\u95ee\u9898\uff0c\u53d1\u73b0\u52a0\u6743\u5ea6\u662f\u5f71\u54cd\u653e\u7f6e\u7684\u5173\u952e\u62d3\u6251\u56e0\u7d20\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7b97\u6cd5\u3002", "motivation": "\u5728\u96c6\u4e2d\u5f0f\u5e02\u573a\u6a21\u578b\u4e2d\uff0c\u7814\u7a76\u4e86\u5728\u7535\u7f51\u4e2d\u653e\u7f6e\u65e0\u9650\u5bb9\u91cf\u7535\u6c60\u7684\u4f18\u5316\u95ee\u9898\uff0c\u72ec\u7acb\u7cfb\u7edf\u8fd0\u8425\u5546 (ISO) \u65e8\u5728\u901a\u8fc7\u8d1f\u8377\u8f6c\u79fb\u6765\u6700\u5c0f\u5316\u603b\u53d1\u7535\u6210\u672c\u3002\u73b0\u6709\u6587\u732e\u672a\u80fd\u5145\u5206\u7406\u89e3\u6700\u4f18\u7535\u6c60\u653e\u7f6e\uff0c\u7279\u522b\u662f\u7f51\u7edc\u62d3\u6251\u5bf9\u6700\u5c0f\u5316\u53d1\u7535\u6210\u672c\u7684\u5f71\u54cd\u3002", "method": "\u901a\u8fc7\u5c06\u6df7\u5408\u6574\u6570\u7ebf\u6027\u89c4\u5212 (MILP) \u95ee\u9898\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u7ebf\u6027\u89c4\u5212 (LP) \u516c\u5f0f\u6765\u7814\u7a76\u7535\u6c60\u7684\u4f18\u5316\u653e\u7f6e\u3002\u63a8\u5bfc\u51fa\u52a0\u6743\u5ea6\u4f5c\u4e3a\u6700\u4f18\u7535\u6c60\u653e\u7f6e\u7684\u552f\u4e00\u62d3\u6251\u56e0\u7d20\u7684\u5206\u6790\u6210\u672c\u8868\u8fbe\u5f0f\uff0c\u5e76\u7814\u7a76\u4e86\u66f4\u9ad8\u9636\u62d3\u6251\u6761\u4ef6\u5bf9\u6811\u72b6\u62d3\u6251\u7f51\u7edc\u7684\u5f71\u54cd\u3002", "result": "\u5728\u5177\u6709\u8db3\u591f\u5927\u53d1\u7535\u5bb9\u91cf\u6216\u6811\u72b6\u62d3\u6251\u7684\u7535\u7f51\u4e2d\uff0c\u52a0\u6743\u5ea6\u662f\u5f71\u54cd\u6700\u4f18\u7535\u6c60\u653e\u7f6e\u7684\u552f\u4e00\u62d3\u6251\u56e0\u7d20\u3002\u5355\u4e2a\u7535\u6c60\u7684\u76f8\u5bf9\u6210\u672c\u8282\u7701\u6548\u76ca\u968f\u7740\u7f51\u7edc\u6269\u5c55\u800c\u964d\u4f4e\u3002\u4e3a\u5f31\u5faa\u73af\u7f51\u7edc\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6bd4\u5546\u4e1a\u6c42\u89e3\u5668\u5feb\u7ea6 100 \u500d\uff0c\u5e76\u4e14\u5728\u653e\u677e\u67d0\u4e9b\u7406\u8bba\u5047\u8bbe\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5bfc\u51fa\u4e86\u52a0\u6743\u5ea6\u4f5c\u4e3a\u6700\u4f18\u7535\u6c60\u653e\u7f6e\u7684\u552f\u4e00\u62d3\u6251\u56e0\u7d20\u7684\u5206\u6790\u6210\u672c\u8868\u8fbe\u5f0f\uff0c\u5e76\u5c55\u793a\u4e86\u5355\u4e2a\u7535\u6c60\u7684\u76f8\u5bf9\u6210\u672c\u8282\u7701\u6548\u76ca\u968f\u7740\u7f51\u7edc\u6269\u5c55\u800c\u964d\u4f4e\u3002\u6b64\u5916\uff0c\u8fd8\u4e3a\u5f31\u5faa\u73af\u7f51\u7edc\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4f4e\u590d\u6742\u5ea6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6bd4\u5546\u4e1a\u6c42\u89e3\u5668\u5feb\u7ea6 100 \u500d\uff0c\u540c\u65f6\u5728\u653e\u677e\u67d0\u4e9b\u7406\u8bba\u5047\u8bbe\u65f6\u4ecd\u80fd\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u3002"}}
{"id": "2507.09538", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09538", "abs": "https://arxiv.org/abs/2507.09538", "authors": ["Zainab Ali", "Lujayn Al-Amir", "Ali Safa"], "title": "On the Importance of Neural Membrane Potential Leakage for LIDAR-based Robot Obstacle Avoidance using Spiking Neural Networks", "comment": null, "summary": "Using neuromorphic computing for robotics applications has gained much\nattention in recent year due to the remarkable ability of Spiking Neural\nNetworks (SNNs) for high-precision yet low memory and compute complexity\ninference when implemented in neuromorphic hardware. This ability makes SNNs\nwell-suited for autonomous robot applications (such as in drones and rovers)\nwhere battery resources and payload are typically limited. Within this context,\nthis paper studies the use of SNNs for performing direct robot navigation and\nobstacle avoidance from LIDAR data. A custom robot platform equipped with a\nLIDAR is set up for collecting a labeled dataset of LIDAR sensing data together\nwith the human-operated robot control commands used for obstacle avoidance.\nCrucially, this paper provides what is, to the best of our knowledge, a first\nfocused study about the importance of neuron membrane leakage on the SNN\nprecision when processing LIDAR data for obstacle avoidance. It is shown that\nby carefully tuning the membrane potential leakage constant of the spiking\nLeaky Integrate-and-Fire (LIF) neurons used within our SNN, it is possible to\nachieve on-par robot control precision compared to the use of a non-spiking\nConvolutional Neural Network (CNN). Finally, the LIDAR dataset collected during\nthis work is released as open-source with the hope of benefiting future\nresearch.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5728\u673a\u5668\u4eba\u5bfc\u822a\u548c\u907f\u969c\u4e2d\u4f7f\u7528\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u795e\u7ecf\u5143\u819c\u7535\u4f4d\u6cc4\u6f0f\u5bf9LIDAR\u6570\u636e\u5904\u7406\u7cbe\u5ea6\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u8c03\u6574\u6cc4\u6f0f\u5e38\u6570\uff0cSNN\u53ef\u4ee5\u8fbe\u5230\u4e0eCNN\u76f8\u5f53\u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u672c\u6587\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u7528\u4e8e\u6b64\u4efb\u52a1\u7684LIDAR\u6570\u636e\u96c6\u3002", "motivation": "\u7531\u4e8eSNN\u5728\u795e\u7ecf\u5f62\u6001\u786c\u4ef6\u4e0a\u5b9e\u73b0\u65f6\u5177\u6709\u9ad8\u7cbe\u5ea6\u3001\u4f4e\u5185\u5b58\u548c\u4f4e\u8ba1\u7b97\u590d\u6742\u5ea6\u63a8\u7406\u7684\u5353\u8d8a\u80fd\u529b\uff0c\u56e0\u6b64\u5728\u673a\u5668\u4eba\u5e94\u7528\u4e2d\u4f7f\u7528\u795e\u7ecf\u5f62\u6001\u8ba1\u7b97\u8fd1\u5e74\u6765\u5907\u53d7\u5173\u6ce8\u3002\u8fd9\u79cd\u80fd\u529b\u4f7fSNN\u975e\u5e38\u9002\u5408\u81ea\u4e3b\u673a\u5668\u4eba\u5e94\u7528\uff08\u4f8b\u5982\u65e0\u4eba\u673a\u548c\u6f2b\u6e38\u8f66\uff09\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5e94\u7528\u4e2d\u7684\u7535\u6c60\u8d44\u6e90\u548c\u6709\u6548\u8f7d\u8377\u901a\u5e38\u6709\u9650\u3002", "method": "\u5efa\u7acb\u4e86\u914d\u5907\u6709LIDAR\u7684\u5b9a\u5236\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u7528\u4e8e\u6536\u96c6\u5e26\u6709\u6807\u7b7e\u7684LIDAR\u4f20\u611f\u6570\u636e\u4ee5\u53ca\u7528\u4e8e\u907f\u969c\u7684\u4eba\u7c7b\u64cd\u4f5c\u673a\u5668\u4eba\u63a7\u5236\u547d\u4ee4\u3002\u7814\u7a76\u4e86\u5c06SNN\u7528\u4e8e\u4eceLIDAR\u6570\u636e\u6267\u884c\u673a\u5668\u4eba\u5bfc\u822a\u548c\u907f\u969c\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u901a\u8fc7\u4ed4\u7ec6\u8c03\u6574\u795e\u7ecf\u5143\u7684\u819c\u7535\u4f4d\u6cc4\u6f0f\u5e38\u6570\uff0c\u53ef\u4ee5\u5b9e\u73b0\u4e0eCNN\u76f8\u5f53\u7684\u673a\u5668\u4eba\u63a7\u5236\u7cbe\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u4ed4\u7ec6\u8c03\u6574SNN\u4e2d\u4f7f\u7528\u7684\u8109\u51b2\u5f0fLeaky Integrate-and-Fire (LIF)\u795e\u7ecf\u5143\u7684\u819c\u7535\u4f4d\u6cc4\u6f0f\u5e38\u6570\uff0c\u53ef\u4ee5\u5728\u673a\u5668\u4eba\u63a7\u5236\u7cbe\u5ea6\u65b9\u9762\u4e0e\u975e\u8109\u51b2\u5f0f\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u76f8\u5ab2\u7f8e\u3002"}}
{"id": "2507.09139", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09139", "abs": "https://arxiv.org/abs/2507.09139", "authors": ["Dewen Zhang", "Tahir Hussain", "Wangpeng An", "Hayaru Shouno"], "title": "PoseLLM: Enhancing Language-Guided Human Pose Estimation with MLP Alignment", "comment": "Preprint", "summary": "Human pose estimation traditionally relies on architectures that encode\nkeypoint priors, limiting their generalization to novel poses or unseen\nkeypoints. Recent language-guided approaches like LocLLM reformulate keypoint\nlocalization as a vision-language task, enabling zero-shot generalization\nthrough textual descriptions. However, LocLLM's linear projector fails to\ncapture complex spatial-textual interactions critical for high-precision\nlocalization. To address this, we propose PoseLLM, the first Large Language\nModel (LLM)-based pose estimation framework that replaces the linear projector\nwith a nonlinear MLP vision-language connector. This lightweight two-layer MLP\nwith GELU activation enables hierarchical cross-modal feature transformation,\nenhancing the fusion of visual patches and textual keypoint descriptions.\nTrained exclusively on COCO data, PoseLLM achieves 77.8 AP on the COCO\nvalidation set, outperforming LocLLM by +0.4 AP, while maintaining strong\nzero-shot generalization on Human-Art and MPII. Our work demonstrates that a\nsimple yet powerful nonlinear connector significantly boosts localization\naccuracy without sacrificing generalization, advancing the state-of-the-art in\nlanguage-guided pose estimation. Code is available at\nhttps://github.com/Ody-trek/PoseLLM.", "AI": {"tldr": "PoseLLM\u901a\u8fc7\u4f7f\u7528\u975e\u7ebf\u6027MLP\u8fde\u63a5\u5668\u6539\u8fdb\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u59ff\u6001\u4f30\u8ba1\uff0c\u5728\u7cbe\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u65b0\u9896\u59ff\u52bf\u6216\u672a\u89c1\u5173\u952e\u70b9\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u95ee\u9898\uff0c\u4ee5\u53caLocLLM\u7684\u7ebf\u6027\u6295\u5f71\u4eea\u65e0\u6cd5\u6355\u6349\u590d\u6742\u7684\u7a7a\u95f4-\u6587\u672c\u4ea4\u4e92\u4ee5\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPoseLLM\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u4e24\u5c42MLP\u548cGELU\u6fc0\u6d3b\u51fd\u6570\u4f5c\u4e3a\u975e\u7ebf\u6027\u8fde\u63a5\u5668\uff0c\u4ee5\u589e\u5f3a\u89c6\u89c9\u548c\u6587\u672c\u7279\u5f81\u7684\u878d\u5408\u3002", "result": "\u5728COCO\u6570\u636e\u96c6\u4e0a\uff0cPoseLLM\u5b9e\u73b0\u4e8677.8 AP\u7684\u6027\u80fd\uff0c\u6bd4LocLLM\u63d0\u9ad8\u4e86+0.4 AP\uff0c\u5e76\u5728Human-Art\u548cMPII\u6570\u636e\u96c6\u4e0a\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "PoseLLM\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u59ff\u6001\u4f30\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u975e\u7ebf\u6027MLP\u89c6\u89c9-\u8bed\u8a00\u8fde\u63a5\u5668\u66ff\u4ee3\u7ebf\u6027\u6295\u5f71\u4eea\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5b9a\u4f4d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5bf9\u65b0\u59ff\u52bf\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10238", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.10238", "abs": "https://arxiv.org/abs/2507.10238", "authors": ["Zheng Liu", "Yang Gao", "Qian Niu"], "title": "Rigid-Body Anisotropy in Noncollinear Antiferromagnets", "comment": "7 pages, 2 figures", "summary": "Characterizing the anisotropic structure in noncollinear antiferromagnets is\nessential for antiferromagnetic spintronics. In this work, we provide a\nmicroscopic theory linking the anisotropy effects induced by the rigid-body\nrotation of spin order to spin-orbit coupling. Our method goes beyond the\nconventional magnetic group theory, offering a concise yet powerful tool to\ncharacterize diverse anisotropy effects in complex magnetic systems. Using the\ngroup representation theory of the spin group, we obtain a set of basis\nfunctions formed from tensor elements of spin-orbit vector--which originates\nfrom spin-orbit coupling and is tied to the rigid-body rotation of the spin\norder--to systematically describe the structure of anisotropy effects. As a\nconcrete example, we apply our framework to coplanar antiferromagnets Mn$_3$Sn\nand Mn$_3$Ir, demonstrating that the corresponding basis functions can well\ncapture both the geometric and magnitude dependencies of the magnetic\nanisotropy energy and anomalous Hall conductivity. Finally, we discuss the\ngeneralization of our framework to broader classes of anisotropy phenomena in\nmagnetic systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7406\u8bba\uff0c\u7528\u57fa\u51fd\u6570\u63cf\u8ff0\u53cd\u94c1\u78c1\u6750\u6599\u4e2d\u7684\u5404\u5411\u5f02\u6027\u6548\u5e94\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e Mn3Sn \u548c Mn3Ir\u3002", "motivation": "\u4e3a\u4e86\u8868\u5f81\u975e\u5171\u7ebf\u53cd\u94c1\u78c1\u4f53\u7684\u5404\u5411\u5f02\u6027\u7ed3\u6784\uff0c\u8fd9\u5bf9\u4e8e\u53cd\u94c1\u78c1\u81ea\u65cb\u7535\u5b50\u5b66\u81f3\u5173\u91cd\u8981\u3002", "method": "\u5229\u7528\u81ea\u65cb\u7fa4\u7684\u7fa4\u8868\u793a\u8bba\uff0c\u6784\u5efa\u7531\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u4ea7\u751f\u7684\u81ea\u65cb\u5e8f\u521a\u4f53\u65cb\u8f6c\u76f8\u5173\u7684\u81ea\u65cb-\u8f68\u9053\u77e2\u91cf\u5f20\u91cf\u5143\u6784\u6210\u7684\u57fa\u51fd\u6570\uff0c\u4ee5\u7cfb\u7edf\u5730\u63cf\u8ff0\u5404\u5411\u5f02\u6027\u6548\u5e94\u7684\u7ed3\u6784\u3002", "result": "\u63a8\u5bfc\u51fa\u63cf\u8ff0\u78c1\u5404\u5411\u5f02\u6027\u6548\u5e94\u7684\u57fa\u51fd\u6570\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e Mn3Sn \u548c Mn3Ir\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6355\u6349\u78c1\u5404\u5411\u5f02\u6027\u80fd\u548c\u53cd\u5e38\u970d\u5c14\u7535\u5bfc\u7387\u7684\u51e0\u4f55\u548c\u5e45\u5ea6\u4f9d\u8d56\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5fae\u89c2\u7406\u8bba\uff0c\u5c06\u81ea\u65cb\u5e8f\u521a\u4f53\u65cb\u8f6c\u5f15\u8d77\u7684\u5404\u5411\u5f02\u6027\u6548\u5e94\u4e0e\u81ea\u65cb-\u8f68\u9053\u8026\u5408\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e Mn3Sn \u548c Mn3Ir \u7b49\u53cd\u94c1\u78c1\u6750\u6599\uff0c\u80fd\u591f\u5f88\u597d\u5730\u6355\u6349\u78c1\u5404\u5411\u5f02\u6027\u80fd\u548c\u53cd\u5e38\u970d\u5c14\u7535\u5bfc\u7387\u7684\u51e0\u4f55\u548c\u5e45\u5ea6\u4f9d\u8d56\u6027\u3002"}}
{"id": "2507.09532", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09532", "abs": "https://arxiv.org/abs/2507.09532", "authors": ["Satish Kumar"], "title": "Design and Experimental Realization of Various Protocols for Secure Quantum Computation and Communication", "comment": null, "summary": "A set of new schemes for quantum computation and communication have been\neither designed or experimentally realized using optimal quantum resources. A\nmulti-output quantum teleportation scheme, where a sender (Alice) teleports an\nm and m+1-qubit GHZ-like unknown state to a receiver (Bob), has been\ndemonstrated using two copies of the Bell state instead of a five-qubit cluster\nstate and implemented on IBM's quantum computer for the m=1 case. Another\nscheme, known as quantum broadcasting where a known state is sent to two\nspatially separated parties (Bob and Charlie) has also been realized using two\nBell states. It is shown that existing quantum broadcasting schemes can be\nreduced to multiparty remote state preparation. After achieving teleportation\nof unknown and known states, sending a quantum operator becomes the next step.\nA scheme for remote implementation of operators (RIO), specifically a\ncontrolled joint-RIO (CJRIO), has been proposed using a four-qubit\nhyper-entangled state involving spatial and polarization degrees of freedom. In\nthis direction, two more variants, remote implementation of hidden and\npartially unknown operators (RIHO and RIPUO) have also been proposed. Their\nsuccess probabilities are analyzed considering dissipation of an auxiliary\ncoherent state interacting with the environment. For secure multiparty tasks\nlike quantum voting or auction, secure multiparty quantum computation (SMQC)\nbecomes essential. A quantum anonymous voting (QAV) scheme has been\nexperimentally implemented on IBM's quantum computer. Finally, two quantum key\ndistribution (QKD) protocols, coherent one-way (COW) and differential phase\nshift (DPS), are experimentally demonstrated and the key rates are analyzed as\nfunctions of post-processing parameters and detector dead times across various\ndistances.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09259", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09259", "abs": "https://arxiv.org/abs/2507.09259", "authors": ["Yuchen Su", "Yonghua Zhu", "Yang Chen", "Diana Benavides-Prado", "Michael Witbrock"], "title": "Psychology-Driven Enhancement of Humour Translation", "comment": null, "summary": "Humour translation plays a vital role as a bridge between different cultures,\nfostering understanding and communication. Although most existing Large\nLanguage Models (LLMs) are capable of general translation tasks, these models\nstill struggle with humour translation, which is especially reflected through\nlinguistic interference and lacking humour in translated text. In this paper,\nwe propose a psychology-inspired Humour Decomposition Mechanism (HDM) that\nutilises Chain-of-Thought (CoT) to imitate the ability of the human thought\nprocess, stimulating LLMs to optimise the readability of translated humorous\ntexts. Moreover, we integrate humour theory in HDM to further enhance the\nhumorous elements in the translated text. Our automatic evaluation experiments\non open-source humour datasets demonstrate that our method significantly\nimproves the quality of humour translation, yielding average gains of 7.75\\% in\nhumour, 2.81\\% in fluency, and 6.13\\% in coherence of the generated text.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5fc3\u7406\u5b66\u548c\u601d\u7ef4\u94fe\u7684\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\uff0c\u7528\u4e8e\u63d0\u5347\u673a\u5668\u7ffb\u8bd1\u7684\u5e7d\u9ed8\u611f\u548c\u53ef\u8bfb\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\u7ffb\u8bd1\u8d28\u91cf\u5f97\u5230\u663e\u8457\u63d0\u9ad8\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u5e7d\u9ed8\u7ffb\u8bd1\u65f6\u5b58\u5728\u8bed\u8a00\u5e72\u6270\u548c\u7f3a\u4e4f\u5e7d\u9ed8\u611f\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u5347\u5e7d\u9ed8\u7ffb\u8bd1\u7684\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u6765\u6a21\u4eff\u4eba\u7c7b\u7684\u601d\u8003\u8fc7\u7a0b\uff0c\u5e76\u6574\u5408\u4e86\u5e7d\u9ed8\u7406\u8bba\u6765\u4f18\u5316\u7ffb\u8bd1\u7684\u5e7d\u9ed8\u6587\u672c\u3002", "result": "\u5728\u5f00\u6e90\u5e7d\u9ed8\u6570\u636e\u96c6\u4e0a\u7684\u81ea\u52a8\u8bc4\u4f30\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5e7d\u9ed8\u611f\u3001\u6d41\u7545\u6027\u548c\u8fde\u8d2f\u6027\u65b9\u9762\u5206\u522b\u5e26\u6765\u4e86 7.75%\u30012.81% \u548c 6.13% \u7684\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53d7\u5fc3\u7406\u5b66\u542f\u53d1\u7684\u3001\u5229\u7528\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u5e7d\u9ed8\u5206\u89e3\u673a\u5236\uff08HDM\uff09\uff0c\u4ee5\u4f18\u5316\u7ffb\u8bd1\u7684\u5e7d\u9ed8\u6587\u672c\u7684\u53ef\u8bfb\u6027\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u5e7d\u9ed8\u7406\u8bba\u8fdb\u4e00\u6b65\u589e\u5f3a\u7ffb\u8bd1\u6587\u672c\u4e2d\u7684\u5e7d\u9ed8\u5143\u7d20\u3002"}}
{"id": "2507.10063", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10063", "abs": "https://arxiv.org/abs/2507.10063", "authors": ["Hongpu Zhang", "Shu Sun", "Hangsong Yan", "Jianhua Mo"], "title": "Deep Learning-Based Beamforming Design Using Target Beam Patterns", "comment": "6 pages, 5 figures", "summary": "This paper proposes a deep learning-based beamforming design framework that\ndirectly maps a target beam pattern to optimal beamforming vectors across\nmultiple antenna array architectures, including digital, analog, and hybrid\nbeamforming. The proposed method employs a lightweight encoder-decoder network\nwhere the encoder compresses the complex beam pattern into a low-dimensional\nfeature vector and the decoder reconstructs the beamforming vector while\nsatisfying hardware constraints. To address training challenges under diverse\nand limited channel station information (CSI) conditions, a two-stage training\nprocess is introduced, which consists of an offline pre-training for robust\nfeature extraction using an auxiliary module, followed by online training of\nthe decoder with a composite loss function that ensures alignment between the\nsynthesized and target beam patterns in terms of the main lobe shape and side\nlobe suppression. Simulation results based on NYUSIM-generated channels show\nthat the proposed method can achieve spectral efficiency close to that of fully\ndigital beamforming under limited CSI and outperforms representative existing\nmethods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u4e3a\u6570\u5b57\u3001\u6a21\u62df\u548c\u6df7\u5408\u6ce2\u675f\u5f62\u6210\u8bbe\u8ba1\u6ce2\u675f\u5f62\u6210\u5411\u91cf\uff0c\u4ee5\u5339\u914d\u76ee\u6807\u6ce2\u675f\u56fe\u6837\uff0c\u5e76\u5728\u6709\u9650\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5728\u591a\u79cd\u5929\u7ebf\u9635\u5217\u67b6\u6784\uff08\u5305\u62ec\u6570\u5b57\u3001\u6a21\u62df\u548c\u6df7\u5408\u6ce2\u675f\u5f62\u6210\uff09\u4e2d\uff0c\u5c06\u76ee\u6807\u6ce2\u675f\u56fe\u6837\u76f4\u63a5\u6620\u5c04\u5230\u6700\u4f18\u6ce2\u675f\u5f62\u6210\u5411\u91cf\uff0c\u5e76\u89e3\u51b3\u5728\u591a\u6837\u5316\u548c\u6709\u9650\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u6761\u4ef6\u4e0b\u7684\u8bad\u7ec3\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u6ce2\u675f\u5f62\u6210\u8bbe\u8ba1\u6846\u67b6\uff0c\u91c7\u7528\u8f7b\u91cf\u7ea7\u7f16\u7801\u5668-\u89e3\u7801\u5668\u7f51\u7edc\uff0c\u5c06\u76ee\u6807\u6ce2\u675f\u56fe\u6837\u6620\u5c04\u5230\u6700\u4f18\u6ce2\u675f\u5f62\u6210\u5411\u91cf\uff0c\u5e76\u5f15\u5165\u4e24\u9636\u6bb5\u8bad\u7ec3\u8fc7\u7a0b\uff08\u79bb\u7ebf\u9884\u8bad\u7ec3\u548c\u5728\u7ebf\u8bad\u7ec3\uff09\u6765\u89e3\u51b3\u4e0d\u540c\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u6761\u4ef6\u4e0b\u7684\u8bad\u7ec3\u6311\u6218\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5168\u6570\u5b57\u6ce2\u675f\u5f62\u6210\u7684\u8c31\u6548\u7387\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6709\u9650\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u4e0b\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5168\u6570\u5b57\u6ce2\u675f\u5f62\u6210\u7684\u8c31\u6548\u7387\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\u3002"}}
{"id": "2507.08871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08871", "abs": "https://arxiv.org/abs/2507.08871", "authors": ["Xishun Liao", "Haoxuan Ma", "Yifan Liu", "Yuxiang Wei", "Brian Yueshuai He", "Chris Stanford", "Jiaqi Ma"], "title": "Next-Generation Travel Demand Modeling with a Generative Framework for Household Activity Coordination", "comment": "8 pages, 7 figures", "summary": "Travel demand models are critical tools for planning, policy, and mobility\nsystem design. Traditional activity-based models (ABMs), although grounded in\nbehavioral theories, often rely on simplified rules and assumptions, and are\ncostly to develop and difficult to adapt across different regions. This paper\npresents a learning-based travel demand modeling framework that synthesizes\nhousehold-coordinated daily activity patterns based on a household's\nsocio-demographic profiles. The whole framework integrates population\nsynthesis, coordinated activity generation, location assignment, and\nlarge-scale microscopic traffic simulation into a unified system. It is fully\ngenerative, data-driven, scalable, and transferable to other regions. A\nfull-pipeline implementation is conducted in Los Angeles with a 10 million\npopulation. Comprehensive validation shows that the model closely replicates\nreal-world mobility patterns and matches the performance of legacy ABMs with\nsignificantly reduced modeling cost and greater scalability. With respect to\nthe SCAG ABM benchmark, the origin-destination matrix achieves a cosine\nsimilarity of 0.97, and the daily vehicle miles traveled (VMT) in the network\nyields a 0.006 Jensen-Shannon Divergence (JSD) and a 9.8% mean absolute\npercentage error (MAPE). When compared to real-world observations from Caltrans\nPeMS, the evaluation on corridor-level traffic speed and volume reaches a 0.001\nJSD and a 6.11% MAPE.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u57fa\u4e8e\u5b66\u4e60\u7684\u51fa\u884c\u9700\u6c42\u5efa\u6a21\u6846\u67b6\uff0c\u5b83\u6bd4\u4f20\u7edf\u6a21\u578b\u66f4\u4fbf\u5b9c\u3001\u66f4\u5177\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u4e14\u5728\u590d\u5236\u771f\u5b9e\u51fa\u884c\u6a21\u5f0f\u65b9\u9762\u540c\u6837\u6709\u6548\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u6d3b\u52a8\u6a21\u578b\uff08ABM\uff09\u867d\u7136\u6709\u884c\u4e3a\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u901a\u5e38\u4f9d\u8d56\u4e8e\u7b80\u5316\u7684\u89c4\u5219\u548c\u5047\u8bbe\uff0c\u5f00\u53d1\u6210\u672c\u9ad8\u6602\uff0c\u5e76\u4e14\u96be\u4ee5\u8de8\u533a\u57df\u9002\u5e94\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u66f4\u7075\u6d3b\u7684\u51fa\u884c\u9700\u6c42\u5efa\u6a21\u65b9\u6cd5\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u51fa\u884c\u9700\u6c42\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6839\u636e\u5bb6\u5ead\u7684\u793e\u4f1a\u4eba\u53e3\u7edf\u8ba1\u5b66\u7279\u5f81\uff0c\u7efc\u5408\u751f\u6210\u5bb6\u5ead\u534f\u8c03\u7684\u65e5\u5e38\u6d3b\u52a8\u6a21\u5f0f\u3002\u8be5\u6846\u67b6\u6574\u5408\u4e86\u4eba\u53e3\u7efc\u5408\u3001\u6d3b\u52a8\u534f\u8c03\u751f\u6210\u3001\u4f4d\u7f6e\u5206\u914d\u548c\u5927\u89c4\u6a21\u5fae\u89c2\u4ea4\u901a\u6a21\u62df\uff0c\u6784\u6210\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7cfb\u7edf\uff0c\u5177\u6709\u5b8c\u5168\u751f\u6210\u3001\u6570\u636e\u9a71\u52a8\u3001\u53ef\u6269\u5c55\u548c\u53ef\u8fc1\u79fb\u7684\u7279\u70b9\u3002", "result": "\u5728\u6d1b\u6749\u77f6\u8fdb\u884c\u7684\u5168\u9762\u8bc4\u4f30\u663e\u793a\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u7cbe\u786e\u590d\u5236\u73b0\u5b9e\u4e16\u754c\u7684\u51fa\u884c\u6a21\u5f0f\u3002\u4e0eSCAG ABM\u57fa\u51c6\u76f8\u6bd4\uff0c\u751f\u6210\u7684\u51fa\u884c\u76ee\u7684\uff08OD\uff09\u77e9\u9635\u5177\u67090.97\u7684\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5e76\u4e14\u7f51\u7edc\u4e2d\u7684\u6bcf\u65e5\u8f66\u8f86\u91cc\u7a0b\uff08VMT\uff09\u7684Jensen-Shannon\u6563\u5ea6\uff08JSD\uff09\u4e3a0.006\uff0c\u5e73\u5747\u7edd\u5bf9\u767e\u5206\u6bd4\u8bef\u5dee\uff08MAPE\uff09\u4e3a9.8%\u3002\u4e0eCaltrans PeMS\u7684\u771f\u5b9e\u4e16\u754c\u89c2\u6d4b\u6570\u636e\u76f8\u6bd4\uff0c\u5728\u8d70\u5eca\u7ea7\u522b\u7684\u4ea4\u901a\u901f\u5ea6\u548c\u6d41\u91cf\u8bc4\u4f30\u4e2d\uff0cJSD\u4e3a0.001\uff0cMAPE\u4e3a6.11%\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u6d1b\u6749\u77f6\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u65bd\uff0c\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6d3b\u52a8\u6a21\u578b\uff08ABM\uff09\u76f8\u6bd4\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u6210\u672c\u5e76\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\uff0c\u540c\u65f6\u7d27\u5bc6\u590d\u5236\u4e86\u73b0\u5b9e\u4e16\u754c\u7684\u51fa\u884c\u6a21\u5f0f\u3002"}}
{"id": "2507.09977", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.09977", "abs": "https://arxiv.org/abs/2507.09977", "authors": ["Anant Vijay Varma", "Doron Cohen"], "title": "Quantum measurement of work in mesoscopic systems", "comment": "9 pages, 7 figures", "summary": "Heat and work in thermodynamics refer to the measurement of changes in energy\ncontent of external bodies (baths and agents). We discuss the implications of\nquantum mechanics on the possibility to measure work in a mesoscopic context.\nThe agent is a quantum entity (say an oscillator) that is used to drive the\nsystem. An obvious limitation is related to back-reaction, leading to a\nclassical-like restriction. We find that in order to resolve fingerprints of\ninterference an additional quantum uncertainty limitation should be taken into\naccount in the design of the agent. The quantum limitation is fundamental, and\ncannot be relaxed by super-resolution techniques.", "AI": {"tldr": "\u91cf\u5b50\u529b\u5b66\u5bf9\u4ecb\u89c2\u7cfb\u7edf\u4e2d\u7684\u529f\u6d4b\u91cf\u63d0\u51fa\u4e86\u4e0e\u7ecf\u5178\u53cd\u4f5c\u7528\u548c\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u76f8\u5173\u7684\u57fa\u672c\u9650\u5236\u3002", "motivation": "\u63a2\u8ba8\u91cf\u5b50\u529b\u5b66\u5982\u4f55\u5f71\u54cd\u4ecb\u89c2\u5c3a\u5ea6\u4e0b\u529f\u7684\u6d4b\u91cf\uff0c\u5e76\u8bc6\u522b\u76f8\u5173\u7684\u9650\u5236\u56e0\u7d20\u3002", "method": "\u901a\u8fc7\u5206\u6790\u91cf\u5b50\u529b\u5b66\u5bf9\u9a71\u52a8\u7cfb\u7edf\u7684\u91cf\u5b50\u5b9e\u4f53\uff08\u4f8b\u5982\u632f\u8361\u5668\uff09\u7684\u5f71\u54cd\u6765\u7814\u7a76\u70ed\u529b\u5b66\u4e2d\u7684\u529f\u7684\u6d4b\u91cf\u3002", "result": "\u53d1\u73b0\u9664\u4e86\u7ecf\u5178\u7684\u53cd\u4f5c\u7528\u9650\u5236\u5916\uff0c\u8fd8\u9700\u8981\u8003\u8651\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u9650\u5236\uff0c\u4ee5\u89e3\u51b3\u91cf\u5b50\u5e72\u6d89\u7684\u75d5\u8ff9\uff0c\u4e14\u8fd9\u79cd\u91cf\u5b50\u9650\u5236\u662f\u6839\u672c\u6027\u7684\uff0c\u65e0\u6cd5\u901a\u8fc7\u8d85\u5206\u8fa8\u7387\u6280\u672f\u653e\u5bbd\u3002", "conclusion": "\u91cf\u5b50\u529b\u5b66\u5bf9\u6d4b\u91cf\u4ecb\u89c2\u7cfb\u7edf\u4e2d\u7684\u529f\u5177\u6709\u91cd\u8981\u5f71\u54cd\uff0c\u5e76\u4e14\u5b58\u5728\u4e0e\u7ecf\u5178\u53cd\u4f5c\u7528\u548c\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u76f8\u5173\u7684\u57fa\u672c\u9650\u5236\u3002"}}
{"id": "2507.09742", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09742", "abs": "https://arxiv.org/abs/2507.09742", "authors": ["Xiaofeng Xiao", "Bo Shen", "Xubo Yue"], "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations", "comment": null, "summary": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications.", "AI": {"tldr": "\u9488\u5bf9AI\u5236\u9020\u6570\u636e\u6d41\u76d1\u63a7\u4e2d\u7684\u4f20\u611f\u5668\u653e\u7f6e\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u65ad\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff08Causal DQ\uff09\uff0c\u65e0\u9700\u4eba\u4e3a\u5e72\u9884\u5373\u53ef\u5b9e\u73b0\u5feb\u901f\u5f02\u5e38\u68c0\u6d4b\u548c\u6700\u4f18\u4f20\u611f\u5668\u653e\u7f6e\u3002", "motivation": "\u5728AI\u9a71\u52a8\u7684\u5236\u9020\u4e1a\u6570\u636e\u6d41\u5b9e\u65f6\u76d1\u63a7\u9700\u6c42\u589e\u957f\u7684\u80cc\u666f\u4e0b\uff0c\u7531\u4e8e\u8d44\u6e90\u9650\u5236\u65e0\u6cd5\u5728\u6240\u6709\u4f4d\u7f6e\u653e\u7f6e\u4f20\u611f\u5668\uff0c\u56e0\u6b64\u9700\u8981\u6700\u4f18\u7684\u4f20\u611f\u5668\u653e\u7f6e\u7b56\u7565\u4ee5\u5b9e\u73b0\u90e8\u5206\u53ef\u89c2\u6d4b\u548c\u5feb\u901f\u5f02\u5e38\u68c0\u6d4b\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u53ea\u8003\u8651\u53d8\u91cf\u76f8\u5173\u6027\u800c\u5ffd\u7565\u56e0\u679c\u5173\u7cfb\uff0c\u6216\u4f9d\u8d56\u4e0d\u5207\u5b9e\u9645\u7684\u4eba\u4e3a\u5e72\u9884\u6765\u8bc6\u522b\u56e0\u679c\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56e0\u679c\u63a8\u65ad\u7684\u6df1\u5ea6Q\u7f51\u7edc\uff08Causal DQ\uff09\u65b9\u6cd5\uff0c\u5c06\u56e0\u679c\u4fe1\u606f\u6574\u5408\u5230Q\u7f51\u7edc\u8bad\u7ec3\u7684\u6bcf\u4e2a\u9636\u6bb5\uff0c\u4ee5\u89e3\u51b3\u90e8\u5206\u53ef\u89c2\u6d4b\u7cfb\u7edf\u4e2d\u7684\u4f20\u611f\u5668\u653e\u7f6e\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684Causal DQ\u65b9\u6cd5\u5728\u4f20\u611f\u5668\u653e\u7f6e\u65b9\u9762\u5177\u6709\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u7d27\u5bc6\u7684\u7406\u8bba\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u80fd\u663e\u8457\u51cf\u5c11\u4e0d\u540c\u8bbe\u7f6e\u4e0b\u5f02\u5e38\u7684\u68c0\u6d4b\u65f6\u95f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5927\u89c4\u6a21\u771f\u5b9e\u6570\u636e\u6d41\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u56e0\u679c\u63a8\u65ad\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u5f02\u5e38\u68c0\u6d4b\u7684\u4f20\u611f\u5668\u653e\u7f6e\u95ee\u9898\u4e0a\uff0c\u80fd\u591f\u663e\u8457\u51cf\u5c11\u5f02\u5e38\u68c0\u6d4b\u65f6\u95f4\u5e76\u5177\u6709\u66f4\u4f18\u7684\u7406\u8bba\u8bef\u5dee\u754c\u9650\uff0c\u4e3a\u5de5\u7a0b\u5e94\u7528\u4e2d\u7684\u56e0\u679c\u63a8\u65ad\u673a\u5668\u5b66\u4e60\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.09714", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09714", "abs": "https://arxiv.org/abs/2507.09714", "authors": ["Yifan Zeng", "Yihan Li", "Suiyi He", "Koushil Sreenath", "Jun Zeng"], "title": "IteraOptiRacing: A Unified Planning-Control Framework for Real-time Autonomous Racing for Iterative Optimal Performance", "comment": null, "summary": "This paper presents a unified planning-control strategy for competing with\nother racing cars called IteraOptiRacing in autonomous racing environments.\nThis unified strategy is proposed based on Iterative Linear Quadratic Regulator\nfor Iterative Tasks (i2LQR), which can improve lap time performance in the\npresence of surrounding racing obstacles. By iteratively using the ego car's\nhistorical data, both obstacle avoidance for multiple moving cars and time cost\noptimization are considered in this unified strategy, resulting in\ncollision-free and time-optimal generated trajectories. The algorithm's\nconstant low computation burden and suitability for parallel computing enable\nreal-time operation in competitive racing scenarios. To validate its\nperformance, simulations in a high-fidelity simulator are conducted with\nmultiple randomly generated dynamic agents on the track. Results show that the\nproposed strategy outperforms existing methods across all randomly generated\nautonomous racing scenarios, enabling enhanced maneuvering for the ego racing\ncar.", "AI": {"tldr": "IteraOptiRacing\u662f\u4e00\u79cd\u65b0\u7684\u7edf\u4e00\u89c4\u5212-\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7i2LQR\u548c\u5386\u53f2\u6570\u636e\u4f18\u5316\uff0c\u5728\u81ea\u52a8\u8d5b\u8f66\u4e2d\u5b9e\u73b0\u907f\u969c\u548c\u65f6\u95f4\u6700\u4f18\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u81ea\u52a8\u8d5b\u8f66\u73af\u5883\u4e2d\u4e0e\u5176\u4ed6\u8d5b\u8f66\u7ade\u4e89\uff0c\u5e76\u63d0\u9ad8\u5728\u5b58\u5728\u5468\u56f4\u8d5b\u8f66\u969c\u788d\u7269\u65f6\u7684\u5355\u5708\u65f6\u95f4\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fed\u4ee3\u7ebf\u6027\u4e8c\u6b21\u8c03\u8282\u5668\u7528\u4e8e\u8fed\u4ee3\u4efb\u52a1\uff08i2LQR\uff09\u7684\u7edf\u4e00\u89c4\u5212-\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f7f\u7528\u8d5b\u8f66\u81ea\u8eab\u7684\u5386\u53f2\u6570\u636e\uff0c\u5e76\u5728\u8be5\u7edf\u4e00\u7b56\u7565\u4e2d\u540c\u65f6\u8003\u8651\u4e86\u591a\u4e2a\u79fb\u52a8\u969c\u788d\u7269\u7684\u907f\u969c\u548c\u65f6\u95f4\u6210\u672c\u4f18\u5316\uff0c\u4ece\u800c\u751f\u6210\u65e0\u78b0\u649e\u4e14\u65f6\u95f4\u6700\u4f18\u7684\u8f68\u8ff9\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u5728\u6240\u6709\u968f\u673a\u751f\u6210\u7684\u81ea\u52a8\u8d5b\u8f66\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u589e\u5f3a\u7684\u81ea\u8d5b\u8f66\u64cd\u63a7\u6027\u3002", "conclusion": "\u8be5\u7b56\u7565\u5728\u6240\u6709\u968f\u673a\u751f\u6210\u7684\u81ea\u52a8\u8d5b\u8f66\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u591f\u589e\u5f3a\u81ea\u8d5b\u8f66\u7684\u9ad8\u7ea7\u64cd\u63a7\u6027\u3002"}}
{"id": "2507.09144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09144", "abs": "https://arxiv.org/abs/2507.09144", "authors": ["Zhimin Liao", "Ping Wei", "Ruijie Zhang", "Shuaijia Chen", "Haoxuan Wang", "Ziyang Ren"], "title": "$I^{2}$-World: Intra-Inter Tokenization for Efficient Dynamic 4D Scene Forecasting", "comment": null, "summary": "Forecasting the evolution of 3D scenes and generating unseen scenarios via\noccupancy-based world models offers substantial potential for addressing corner\ncases in autonomous driving systems. While tokenization has revolutionized\nimage and video generation, efficiently tokenizing complex 3D scenes remains a\ncritical challenge for 3D world models. To address this, we propose\n$I^{2}$-World, an efficient framework for 4D occupancy forecasting. Our method\ndecouples scene tokenization into intra-scene and inter-scene tokenizers. The\nintra-scene tokenizer employs a multi-scale residual quantization strategy to\nhierarchically compress 3D scenes while preserving spatial details. The\ninter-scene tokenizer residually aggregates temporal dependencies across\ntimesteps. This dual design preserves the compactness of 3D tokenizers while\nretaining the dynamic expressiveness of 4D tokenizers. Unlike decoder-only\nGPT-style autoregressive models, $I^{2}$-World adopts an encoder-decoder\narchitecture. The encoder aggregates spatial context from the current scene and\npredicts a transformation matrix to enable high-level control over scene\ngeneration. The decoder, conditioned on this matrix and historical tokens,\nensures temporal consistency during generation. Experiments demonstrate that\n$I^{2}$-World achieves state-of-the-art performance, outperforming existing\nmethods by 25.1\\% in mIoU and 36.9\\% in IoU for 4D occupancy forecasting while\nexhibiting exceptional computational efficiency: it requires merely 2.9 GB of\ntraining memory and achieves real-time inference at 37.0 FPS. Our code is\navailable on https://github.com/lzzzzzm/II-World.", "AI": {"tldr": "I^2-World efficiently forecasts 4D occupancy using a novel dual tokenizer approach (intra-scene and inter-scene) within an encoder-decoder framework, achieving state-of-the-art results and real-time performance for autonomous driving.", "motivation": "Forecasting the evolution of 3D scenes and generating unseen scenarios using occupancy-based world models has significant potential for addressing corner cases in autonomous driving systems. However, efficiently tokenizing complex 3D scenes is a critical challenge for 3D world models.", "method": "I^2-World proposes an efficient framework for 4D occupancy forecasting by decoupling scene tokenization into intra-scene and inter-scene tokenizers. The intra-scene tokenizer uses a multi-scale residual quantization strategy for hierarchical compression of 3D scenes, preserving spatial details. The inter-scene tokenizer residually aggregates temporal dependencies across timesteps. It employs an encoder-decoder architecture where the encoder aggregates spatial context and predicts a transformation matrix for scene generation control, and the decoder uses this matrix and historical tokens for temporal consistency during generation.", "result": "Experiments demonstrate that I^2-World achieves state-of-the-art performance, outperforming existing methods by 25.1% in mIoU and 36.9% in IoU for 4D occupancy forecasting. It also shows exceptional computational efficiency, requiring only 2.9 GB of training memory and achieving real-time inference at 37.0 FPS.", "conclusion": "I^2-World in 4D occupancy forecasting achieves state-of-the-art performance, outperforming existing methods by 25.1% in mIoU and 36.9% in IoU, while exhibiting exceptional computational efficiency with 2.9 GB of training memory and 37.0 FPS real-time inference."}}
{"id": "2507.10247", "categories": ["cond-mat.mtrl-sci", "physics.ins-det"], "pdf": "https://arxiv.org/pdf/2507.10247", "abs": "https://arxiv.org/abs/2507.10247", "authors": ["Benjamin L. Weare", "Kayleigh L. Y. Fung", "Ian Cardillo-Zallo", "William J. Cull", "Michael W. Fay", "Stephen P. Argent", "Paul D. Brown"], "title": "Practical Crystallography with a Transmission Electron Microscope", "comment": "Preprint, submitted to Ultramicroscopy 17-Jun-2025. Associated raw\n  data, see http://doi.org/10.17639/nott.7567", "summary": "Three-dimensional electron diffraction (3DED) is a powerful technique\nproviding for crystal structure solutions of sub-micron sized crystals too\nsmall for structure determination via X-ray techniques. The entry requirement,\nhowever, of a transmission electron microscope (TEM) adapted with bespoke\nsoftware for coordinated sample stage rotation and continuous electron\ndiffraction data acquisition has generally inhibited the wider uptake of 3DED.\nTo address this limitation, we present novel software GiveMeED appropriate for\ncontrolled 3DED data acquisition. The collection of useable reflections beyond\n0.8 {\\AA} makes 3DED crystallographic processing effectively routine, using\nstandard software and workflows derived from single-crystal X-ray diffraction\n(SCXRD) techniques. A full experimental workflow for 3DED on a conventional TEM\nis described in practical terms, in combination with direct imaging, and energy\ndispersive X-ray spectroscopy (EDS) and electron energy loss spectroscopy\n(EELS), for the return of comprehensive correlative descriptions of crystal\nmorphologies and sample compositions, with due regard for the quantification of\nelectron flux at each stage of the characterisation process. The accuracy and\neffectiveness of GiveMeED is demonstrated through structure solutions for case\nstudy paracetamol, copper(II) phthalocyanine, and percholorocoronene samples,\ncharacterised in their near-native states under controlled low dose conditions\nat either room or cryogenic temperatures, with determined unit cell parameters\nand atomic connectivity matching accepted literature X-ray structures for these\ncompounds. To promote the wider adoption of 3DED, we make GiveMeED freely\navailable for use and modification, in support of greater uptake and\nutilisation of structure solution procedures via electron diffraction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u6b3e\u540d\u4e3aGiveMeED\u7684\u514d\u8d39\u8f6f\u4ef6\uff0c\u7528\u4e8e\u7b80\u5316\u548c\u666e\u53ca\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff08TEM\uff09\u7684\u4e09\u7ef4\u7535\u5b50\u884d\u5c04\uff083DED\uff09\u6280\u672f\uff0c\u4f7f\u5f97\u5728\u5e38\u89c4TEM\u4e0a\u8fdb\u884c\u6676\u4f53\u7ed3\u6784\u89e3\u6790\u66f4\u52a0\u5bb9\u6613\u548c\u5e38\u89c4\u5316\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u591a\u79cd\u5316\u5408\u7269\u7684\u7ed3\u6784\u6d4b\u5b9a\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u4e09\u7ef4\u7535\u5b50\u884d\u5c04\uff083DED\uff09\u6280\u672f\u5bf9\u7279\u6b8a\u8bbe\u5907\u548c\u8f6f\u4ef6\u7684\u4f9d\u8d56\u6027\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u66f4\u6613\u4e8e\u5e7f\u6cdb\u4f7f\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u964d\u4f4e\u6280\u672f\u95e8\u69db\uff0c\u4fc3\u8fdb\u5176\u5e94\u7528\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGiveMeED\u7684\u65b0\u8f6f\u4ef6\uff0c\u7528\u4e8e\u63a7\u5236\u4e09\u7ef4\u7535\u5b50\u884d\u5c04\uff083DED\uff09\u6570\u636e\u91c7\u96c6\u3002\u7ed3\u5408\u4e86\u76f4\u63a5\u6210\u50cf\u3001\u80fd\u91cf\u8272\u6563X\u5c04\u7ebf\u5149\u8c31\uff08EDS\uff09\u548c\u7535\u5b50\u80fd\u91cf\u635f\u5931\u5149\u8c31\uff08EELS\uff09\u6280\u672f\uff0c\u5e76\u5728\u5e38\u89c4\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff08TEM\uff09\u4e0a\u5b9e\u73b0\u4e863DED\u6570\u636e\u91c7\u96c6\u7684\u5b8c\u6574\u5b9e\u9a8c\u6d41\u7a0b\uff0c\u5305\u62ec\u7535\u5b50\u901a\u91cf\u5b9a\u91cf\u3002", "result": "\u7814\u7a76\u6210\u529f\u5f00\u53d1\u5e76\u9a8c\u8bc1\u4e86GiveMeED\u8f6f\u4ef6\uff0c\u8be5\u8f6f\u4ef6\u80fd\u591f\u5b9e\u73b0\u5bf9\u5e38\u89c4TEM\u8fdb\u884c3DED\u6570\u636e\u91c7\u96c6\uff0c\u5e76\u901a\u8fc7\u5bf9\u6c28\u915a\u3001\u915e\u83c1\u94dc\uff08II\uff09\u548c\u9ad8\u6c2f\u51a0\u919a\u7b49\u6837\u672c\u7684\u7ed3\u6784\u89e3\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\uff0c\u786e\u5b9a\u7684\u6676\u80de\u53c2\u6570\u548c\u539f\u5b50\u8fde\u63a5\u6027\u4e0e\u6587\u732e\u503c\u4e00\u81f4\u3002\u63d0\u4f9b\u7684\u8f6f\u4ef6\u662f\u514d\u8d39\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u4f9b\u540d\u4e3aGiveMeED\u7684\u65b0\u8f6f\u4ef6\uff0c\u5e76\u7ed3\u5408\u76f4\u63a5\u6210\u50cf\u3001\u80fd\u91cf\u8272\u6563X\u5c04\u7ebf\u5149\u8c31\uff08EDS\uff09\u548c\u7535\u5b50\u80fd\u91cf\u635f\u5931\u5149\u8c31\uff08EELS\uff09\u7b49\u6280\u672f\uff0c\u4e3a\u5728\u5e38\u89c4\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff08TEM\uff09\u4e0a\u8fdb\u884c\u4e09\u7ef4\u7535\u5b50\u884d\u5c04\uff083DED\uff09\u6570\u636e\u91c7\u96c6\u548c\u6676\u4f53\u7ed3\u6784\u89e3\u6790\u63d0\u4f9b\u4e86\u5b8c\u6574\u7684\u5b9e\u9a8c\u6d41\u7a0b\uff0c\u4ece\u800c\u6709\u6548\u964d\u4f4e\u4e863DED\u6280\u672f\u7684\u5e94\u7528\u95e8\u69db\uff0c\u5e76\u88ab\u8bc1\u660e\u5bf9\u591a\u79cd\u5316\u5408\u7269\u5177\u6709\u51c6\u786e\u6027\u548c\u6709\u6548\u6027\uff0c\u6700\u7ec8\u76ee\u6807\u662f\u4fc3\u8fdb3DED\u6280\u672f\u7684\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.09581", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09581", "abs": "https://arxiv.org/abs/2507.09581", "authors": ["Sourish Wawdhane", "Sashwat Anagolum", "Poulami Das", "Yunong Shi"], "title": "Ensemble-IR: Concise Representation for Quantum Ensemble Programs", "comment": "5 pages", "summary": "Emerging quantum applications such as error mitigation, system\ncharacterization, and hybrid protocols often require running large families of\nrelated quantum circuits. Existing intermediate representations (IRs) and\nframeworks such as Qiskit, QIR, MitiQ, and OpenQASM do not provide primitives\nto concisely express such workloads. These tools instead rely on explicit\nenumeration of the unique circuits within each workload. We introduce\nEnsemble-IR, an intermediate representation designed to concisely express these\nensemble workloads. Rather than enumerating each circuit separately,\nEnsemble-IR encodes an entire workload through a rich, shared unified program.\nThis program specifies common circuit structure along with symbolic operations\nto express points of variation - such as gate types, gate placement, parameter\nvalues, or qubit configurations. Ensemble-IR enables quantum systems to load an\nentire family of circuits onto a device as a single concise file, allowing\ncontained circuits to instead be recreated on-the-fly at runtime. We\ndemonstrate Ensemble-IR across 18 real-world workloads from prior literature,\nhighlighting its widespread utility for scalable quantum system development.", "AI": {"tldr": "Ensemble-IR is a new intermediate representation that concisely expresses families of quantum circuits using symbolic variations, reducing the need for explicit enumeration and enabling efficient runtime recreation of circuits for scalable quantum system development.", "motivation": "Existing intermediate representations and frameworks (Qiskit, QIR, MitiQ, OpenQASM) lack primitives to concisely express large families of related quantum circuits required for emerging applications like error mitigation, system characterization, and hybrid protocols, forcing reliance on explicit enumeration.", "method": "Ensemble-IR, a novel intermediate representation, encodes entire families of quantum circuits through a unified program that specifies common structures and symbolic operations for variations (gate types, parameters, qubit configurations, etc.), rather than explicit enumeration.", "result": "Demonstrated the widespread utility of Ensemble-IR across 18 real-world workloads from prior literature, highlighting its ability to represent ensemble workloads concisely and enable on-the-fly recreation of circuits at runtime.", "conclusion": "Ensemble-IR enables concise expression of ensemble quantum workloads, facilitating scalable quantum system development by encoding circuit families in a unified program with symbolic variations for on-the-fly recreation at runtime."}}
{"id": "2507.09282", "categories": ["cs.CL", "cs.CR", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09282", "abs": "https://arxiv.org/abs/2507.09282", "authors": ["Dominika Woszczyk", "Ranya Aloufi", "Soteris Demetriou"], "title": "ClaritySpeech: Dementia Obfuscation in Speech", "comment": "Accepted at Interspeech 2025", "summary": "Dementia, a neurodegenerative disease, alters speech patterns, creating\ncommunication barriers and raising privacy concerns. Current speech\ntechnologies, such as automatic speech transcription (ASR), struggle with\ndementia and atypical speech, further challenging accessibility. This paper\npresents a novel dementia obfuscation in speech framework, ClaritySpeech,\nintegrating ASR, text obfuscation, and zero-shot text-to-speech (TTS) to\ncorrect dementia-affected speech while preserving speaker identity in low-data\nenvironments without fine-tuning. Results show a 16% and 10% drop in mean F1\nscore across various adversarial settings and modalities (audio, text, fusion)\nfor ADReSS and ADReSSo, respectively, maintaining 50% speaker similarity. We\nalso find that our system improves WER (from 0.73 to 0.08 for ADReSS and 0.15\nfor ADReSSo) and speech quality from 1.65 to ~2.15, enhancing privacy and\naccessibility.", "AI": {"tldr": "ClaritySpeech\uff1a\u4e00\u79cd\u7528\u4e8e\u75f4\u5446\u75c7\u60a3\u8005\u7684\u8bed\u97f3\u589e\u5f3a\u548c\u9690\u79c1\u4fdd\u62a4\u6846\u67b6\uff0c\u901a\u8fc7ASR\u3001\u6587\u672c\u6df7\u6dc6\u548c\u96f6\u6837\u672cTTS\u6280\u672f\uff0c\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u8bed\u97f3\u7ea0\u6b63\u548c\u8eab\u4efd\u4fdd\u7559\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u53ef\u8bbf\u95ee\u6027\u548c\u9690\u79c1\u6027\u3002", "motivation": "\u75f4\u5446\u75c7\u4f1a\u6539\u53d8\u8bed\u8a00\u6a21\u5f0f\uff0c\u9020\u6210\u6c9f\u901a\u969c\u788d\u5e76\u5f15\u53d1\u9690\u79c1\u95ee\u9898\u3002\u73b0\u6709\u7684\u8bed\u97f3\u6280\u672f\uff08\u5982ASR\uff09\u5728\u5904\u7406\u75f4\u5446\u75c7\u60a3\u8005\u7684\u975e\u5178\u578b\u8bed\u97f3\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u8fdb\u4e00\u6b65\u963b\u788d\u4e86\u5176\u53ef\u8bbf\u95ee\u6027\u3002", "method": "ClaritySpeech\u6846\u67b6\u96c6\u6210\u4e86\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b\uff08ASR\uff09\u3001\u6587\u672c\u6df7\u6dc6\u548c\u96f6\u6837\u672c\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u6280\u672f\u3002", "result": "ClaritySpeech\u6846\u67b6\u5728ADReSS\u548cADReSSo\u6570\u636e\u96c6\u4e0a\uff0c\u5e73\u5747F1\u5206\u6570\u5206\u522b\u964d\u4f4e\u4e8616%\u548c10%\uff0c\u540c\u65f6\u4fdd\u6301\u4e8650%\u7684\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u3002\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4ece0.73\u5206\u522b\u63d0\u9ad8\u52300.08\uff08ADReSS\uff09\u548c0.15\uff08ADReSSo\uff09\uff0c\u8bed\u97f3\u8d28\u91cf\u4ece1.65\u63d0\u9ad8\u5230\u7ea62.15\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86ClaritySpeech\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210ASR\u3001\u6587\u672c\u6df7\u6dc6\u548c\u96f6\u6837\u672cTTS\u6280\u672f\uff0c\u5728\u4f4e\u6570\u636e\u73af\u5883\u4e0b\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u7ea0\u6b63\u75f4\u5446\u75c7\u60a3\u8005\u7684\u8bed\u97f3\uff0c\u540c\u65f6\u4fdd\u62a4\u8bf4\u8bdd\u4eba\u8eab\u4efd\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7cfb\u7edf\u5728ADReSS\u548cADReSSo\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u5404\u79cd\u5bf9\u6297\u6027\u8bbe\u7f6e\u548c\u6a21\u6001\u4e0b\uff0c\u5e73\u5747F1\u5206\u6570\u5206\u522b\u964d\u4f4e\u4e8616%\u548c10%\uff0c\u540c\u65f6\u4fdd\u6301\u4e8650%\u7684\u8bf4\u8bdd\u4eba\u76f8\u4f3c\u5ea6\u3002\u6b64\u5916\uff0c\u7cfb\u7edf\u5c06\u8bcd\u9519\u8bef\u7387\uff08WER\uff09\u4ece0.73\u5206\u522b\u63d0\u9ad8\u52300.08\uff08ADReSS\uff09\u548c0.15\uff08ADReSSo\uff09\uff0c\u5e76\u5c06\u8bed\u97f3\u8d28\u91cf\u4ece1.65\u63d0\u9ad8\u5230\u7ea62.15\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u9690\u79c1\u548c\u53ef\u8bbf\u95ee\u6027\u3002"}}
{"id": "2507.10145", "categories": ["eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.10145", "abs": "https://arxiv.org/abs/2507.10145", "authors": ["Ryohei Fukuma", "Yoshinobu Kawahara", "Okito Yamashita", "Kei Majima", "Haruhiko Kishima", "Takufumi Yanagisawa"], "title": "Intrinsic frequency distribution characterises neural dynamics", "comment": null, "summary": "Decomposing multivariate time series with certain basic dynamics is crucial\nfor understanding, predicting and controlling nonlinear spatiotemporally\ndynamic systems such as the brain. Dynamic mode decomposition (DMD) is a method\nfor decomposing nonlinear spatiotemporal dynamics into several basic dynamics\n(dynamic modes; DMs) with intrinsic frequencies and decay rates. In particular,\nunlike Fourier transform-based methods, which are used to decompose a\nsingle-channel signal into the amplitudes of sinusoidal waves with discrete\nfrequencies at a regular interval, DMD can derive the intrinsic frequencies of\na multichannel signal on the basis of the available data; furthermore, it can\ncapture nonstationary components such as alternations between states with\ndifferent intrinsic frequencies. Here, we propose the use of the distribution\nof intrinsic frequencies derived from DMDs (DM frequencies) to characterise\nneural activities. The distributions of DM frequencies in the\nelectroencephalograms of healthy subjects and patients with dementia or\nParkinson's disease in a resting state were evaluated. By using the\ndistributions, these patients were distinguished from healthy subjects with\nsignificantly greater accuracy than when using amplitude spectra derived by\ndiscrete Fourier transform. This finding suggests that the distribution of DM\nfrequencies exhibits distinct behaviour from amplitude spectra, and therefore,\nthe distribution may serve as a new biomarker by characterising the nonlinear\nspatiotemporal dynamics of electrophysiological signals.", "AI": {"tldr": "DMD\u9891\u7387\u5206\u5e03\u6bd4DFT\u5e45\u5ea6\u8c31\u66f4\u80fd\u6709\u6548\u5730\u533a\u5206\u795e\u7ecf\u7cfb\u7edf\u75be\u75c5\u60a3\u8005\u548c\u5065\u5eb7\u4eba\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3\u3001\u9884\u6d4b\u548c\u63a7\u5236\u50cf\u5927\u8111\u8fd9\u6837\u7684\u975e\u7ebf\u6027\u65f6\u7a7a\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5bf9\u5177\u6709\u67d0\u4e9b\u57fa\u672c\u52a8\u529b\u5b66\u7684\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u5206\u89e3\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u5085\u91cc\u53f6\u53d8\u6362\u65b9\u6cd5\u5728\u5904\u7406\u591a\u901a\u9053\u4fe1\u53f7\u548c\u6355\u6349\u975e\u5e73\u7a33\u5206\u91cf\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\uff08DMD\uff09\u88ab\u63d0\u51fa\u7528\u4e8e\u5206\u89e3\u591a\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u975e\u7ebf\u6027\u65f6\u7a7a\u52a8\u529b\u5b66\uff0c\u63d0\u53d6\u5176\u672c\u5f81\u9891\u7387\u548c\u8870\u51cf\u7387\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u5065\u5eb7\u53d7\u8bd5\u8005\u4ee5\u53ca\u75f4\u5446\u75c7\u6216\u5e15\u91d1\u68ee\u75c5\u60a3\u8005\u5728\u9759\u606f\u72b6\u6001\u4e0b\u8111\u7535\u56fe\uff08EEG\uff09\u7684DMD\u9891\u7387\u5206\u5e03\u3002", "result": "\u4e0e\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08DFT\uff09\u4ea7\u751f\u7684\u5e45\u5ea6\u8c31\u76f8\u6bd4\uff0c\u57fa\u4e8eDMD\u9891\u7387\u5206\u5e03\u80fd\u591f\u4ee5\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u533a\u5206\u60a3\u8005\u548c\u5065\u5eb7\u53d7\u8bd5\u8005\uff0c\u8868\u660eDMD\u9891\u7387\u5206\u5e03\u4e0e\u5e45\u5ea6\u8c31\u7684\u884c\u4e3a\u4e0d\u540c\uff0c\u5e76\u53ef\u80fd\u6210\u4e3a\u8868\u5f81\u795e\u7ecf\u52a8\u529b\u5b66\u7684\u65b0\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u52a8\u6001\u6a21\u5f0f\u5206\u89e3\uff08DMD\uff09\u4ea7\u751f\u7684\u672c\u5f81\u9891\u7387\u5206\u5e03\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u7528\u4e8e\u8868\u5f81\u548c\u533a\u5206\u5065\u5eb7\u53d7\u8bd5\u8005\u4ee5\u53ca\u60a3\u6709\u75f4\u5446\u75c7\u6216\u5e15\u91d1\u68ee\u75c5\uff08PD\uff09\u7684\u60a3\u8005\u7684\u795e\u7ecf\u6d3b\u52a8\u3002"}}
{"id": "2507.08873", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08873", "abs": "https://arxiv.org/abs/2507.08873", "authors": ["Shaoran Yang", "Dongyu Wei", "Hanzhi Yu", "Zhaohui Yang", "Yuchen Liu", "Mingzhe Chen"], "title": "Contrastive Language-Image Pre-Training Model based Semantic Communication Performance Optimization", "comment": "Submitted to IEEE GLOBECOM 2025", "summary": "In this paper, a novel contrastive language-image pre-training (CLIP) model\nbased semantic communication framework is designed. Compared to standard neural\nnetwork (e.g.,convolutional neural network) based semantic encoders and\ndecoders that require joint training over a common dataset, our CLIP model\nbased method does not require any training procedures thus enabling a\ntransmitter to extract data meanings of the original data without neural\nnetwork model training, and the receiver to train a neural network for\nfollow-up task implementation without the communications with the transmitter.\nNext, we investigate the deployment of the CLIP model based semantic framework\nover a noisy wireless network. Since the semantic information generated by the\nCLIP model is susceptible to wireless noise and the spectrum used for semantic\ninformation transmission is limited, it is necessary to jointly optimize CLIP\nmodel architecture and spectrum resource block (RB) allocation to maximize\nsemantic communication performance while considering wireless noise, the delay\nand energy used for semantic communication. To achieve this goal, we use a\nproximal policy optimization (PPO) based reinforcement learning (RL) algorithm\nto learn how wireless noise affect the semantic communication performance thus\nfinding optimal CLIP model and RB for each user. Simulation results show that\nour proposed method improves the convergence rate by up to 40%, and the\naccumulated reward by 4x compared to soft actor-critic.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u901a\u8fc7PPO\u5f3a\u5316\u5b66\u4e60\u4f18\u5316CLIP\u6a21\u578b\u548c\u9891\u8c31\u5206\u914d\uff0c\u4ee5\u63d0\u5347\u5728\u65e0\u7ebf\u566a\u58f0\u73af\u5883\u4e0b\u7684\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8bed\u4e49\u901a\u4fe1\u65b9\u6cd5\u9700\u8981\u8054\u5408\u8bad\u7ec3\u4ee5\u53ca\u5728\u566a\u58f0\u65e0\u7ebf\u7f51\u7edc\u4e2d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u63d0\u53d6\u6570\u636e\u8bed\u4e49\u7684CLIP\u6a21\u578b\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u5728\u65e0\u7ebf\u7f51\u7edc\u4e2d\u7684\u90e8\u7f72\u4f18\u5316\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u5229\u7528CLIP\u6a21\u578b\u8fdb\u884c\u8bed\u4e49\u4fe1\u606f\u63d0\u53d6\u548c\u4f20\u8f93\u3002\u901a\u8fc7\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u8054\u5408\u4f18\u5316CLIP\u6a21\u578b\u67b6\u6784\u548c\u9891\u8c31\u8d44\u6e90\u5206\u914d\uff0c\u4ee5\u5e94\u5bf9\u65e0\u7ebf\u566a\u58f0\u3001\u5ef6\u8fdf\u548c\u80fd\u91cf\u6d88\u8017\u7b49\u6311\u6218\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u8f6fActor-Critic\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u6536\u655b\u901f\u5ea6\u63d0\u9ad8\u4e8640%\uff0c\u7d2f\u79ef\u5956\u52b1\u63d0\u9ad8\u4e864\u500d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eCLIP\u7684\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\u901a\u8fc7\u8054\u5408\u4f18\u5316CLIP\u6a21\u578b\u67b6\u6784\u548c\u9891\u8c31\u8d44\u6e90\u5206\u914d\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u901a\u4fe1\u6027\u80fd\uff0c\u5e76\u80fd\u5728\u6709\u7ebf\u548c\u65e0\u7ebf\u573a\u666f\u4e0b\u8fdb\u884c\u90e8\u7f72\u3002"}}
{"id": "2507.10280", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10280", "abs": "https://arxiv.org/abs/2507.10280", "authors": ["Haomiaomiao Wang", "Conor Fennell", "Swati Poojary", "Mingming Liu"], "title": "A SUMO-Based Digital Twin for Evaluation of Conventional and Electric Vehicle Networks", "comment": "The paper has been accepted by the IEEE Energy Conversion Congress &\n  Expo (ECCE) Europe 2025 conference", "summary": "Digital twins are increasingly applied in transportation modelling to\nreplicate real-world traffic dynamics and evaluate mobility and energy\nefficiency. This study presents a SUMO-based digital twin that simulates mixed\nICEV-EV traffic on a major motorway segment, leveraging multi-sensor data\nfusion from inductive loops, GPS probes, and toll records. The model is\nvalidated under both complete and partial information scenarios, achieving\n93.1% accuracy in average speed estimation and 97.1% in average trip length\nestimation. Statistical metrics, including KL Divergence and Wasserstein\nDistance, demonstrate strong alignment between simulated and observed traffic\npatterns. Furthermore, CO2 emissions were overestimated by only 0.8-2.4%, and\nEV power consumption underestimated by 1.0-5.4%, highlighting the model's\nrobustness even with incomplete vehicle classification information.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eSUMO\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u6df7\u5408\u71c3\u6cb9\u8f66-\u7535\u52a8\u8f66\u4ea4\u901a\u3002\u8be5\u6a21\u578b\u878d\u5408\u591a\u6e90\u4f20\u611f\u5668\u6570\u636e\uff0c\u5728\u5b8c\u6574\u548c\u4e0d\u5b8c\u6574\u4fe1\u606f\u573a\u666f\u4e0b\u5747\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5728\u4ea4\u901a\u53c2\u6570\u4f30\u8ba1\u548c\u6392\u653e/\u80fd\u8017\u8bc4\u4f30\u65b9\u9762\u5747\u53d6\u5f97\u826f\u597d\u7ed3\u679c\u3002", "motivation": "\u6570\u5b57\u5b6a\u751f\u6280\u672f\u5728\u4ea4\u901a\u5efa\u6a21\u4e2d\u8d8a\u6765\u8d8a\u53d7\u5230\u91cd\u89c6\uff0c\u80fd\u591f\u590d\u5236\u73b0\u5b9e\u4e16\u754c\u7684\u4ea4\u901a\u52a8\u6001\uff0c\u5e76\u8bc4\u4f30\u4ea4\u901a\u548c\u80fd\u6e90\u6548\u7387\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u57fa\u4e8eSUMO\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u6765\u89e3\u51b3\u8fd9\u4e00\u9700\u6c42\uff0c\u4ee5\u6a21\u62df\u6df7\u5408\u71c3\u6cb9\u8f66\u548c\u7535\u52a8\u8f66\u4ea4\u901a\u3002", "method": "\u8be5\u7814\u7a76\u5229\u7528SUMO\uff08Simulation of Urban MObility\uff09\u6784\u5efa\u4e86\u4e00\u4e2a\u6570\u5b57\u5b6a\u751f\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u878d\u5408\u4e86\u6765\u81ea\u611f\u5e94\u7ebf\u5708\u3001GPS\u63a2\u9488\u548c\u6536\u8d39\u8bb0\u5f55\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\uff0c\u4ee5\u6a21\u62df\u6df7\u5408\u71c3\u6cb9\u8f66\u548c\u7535\u52a8\u8f66\u5728\u4e3b\u8981\u9ad8\u901f\u516c\u8def\u8def\u6bb5\u7684\u4ea4\u901a\u72b6\u51b5\u3002\u6a21\u578b\u5728\u5b8c\u6574\u548c\u4e0d\u5b8c\u6574\u4fe1\u606f\u573a\u666f\u4e0b\u90fd\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "result": "\u6a21\u578b\u5728\u5e73\u5747\u901f\u5ea6\u4f30\u8ba1\u65b9\u9762\u8fbe\u5230\u4e8693.1%\u7684\u51c6\u786e\u7387\uff0c\u5728\u5e73\u5747\u884c\u7a0b\u957f\u5ea6\u4f30\u8ba1\u65b9\u9762\u8fbe\u5230\u4e8697.1%\u7684\u51c6\u786e\u7387\u3002\u4f7f\u7528KL\u6563\u5ea6\u548cWasserstein\u8ddd\u79bb\u7b49\u7edf\u8ba1\u6307\u6807\u8bc4\u4f30\uff0c\u6a21\u62df\u4ea4\u901a\u6a21\u5f0f\u4e0e\u89c2\u6d4b\u5230\u7684\u4ea4\u901a\u6a21\u5f0f\u9ad8\u5ea6\u4e00\u81f4\u3002\u5373\u4f7f\u5728\u8f66\u8f86\u5206\u7c7b\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u5bf9CO2\u6392\u653e\u7684\u4f30\u8ba1\u4e5f\u4ec5\u8fc7\u9ad80.8-2.4%\uff0c\u5bf9EV\u7535\u529b\u6d88\u8017\u7684\u4f30\u8ba1\u4e5f\u4ec5\u8fc7\u4f4e1.0-5.4%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8eSUMO\u7684\u6570\u5b57\u5b6a\u751f\u6a21\u578b\u5728\u6a21\u62df\u6df7\u5408\u71c3\u6cb9\u8f66\u548c\u7535\u52a8\u8f66\u4ea4\u901a\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u51c6\u786e\u4f30\u8ba1\u5e73\u5747\u901f\u5ea6\u548c\u5e73\u5747\u884c\u7a0b\u957f\u5ea6\uff0c\u5373\u4f7f\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u60c5\u51b5\u4e0b\u4e5f\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u78b3\u6392\u653e\u548c\u7535\u529b\u6d88\u8017\u7684\u4f30\u8ba1\u65b9\u9762\u4e5f\u76f8\u5f53\u51c6\u786e\u3002"}}
{"id": "2507.09725", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09725", "abs": "https://arxiv.org/abs/2507.09725", "authors": ["Gabriel G. Gattaux", "Julien R. Serres", "Franck Ruffier", "Antoine Wystrach"], "title": "Visual Homing in Outdoor Robots Using Mushroom Body Circuits and Learning Walks", "comment": "Published by Springer Nature with the 14th bioinspired and biohybrid\n  systems conference in Sheffield, and presented at the conference in July 2025", "summary": "Ants achieve robust visual homing with minimal sensory input and only a few\nlearning walks, inspiring biomimetic solutions for autonomous navigation. While\nMushroom Body (MB) models have been used in robotic route following, they have\nnot yet been applied to visual homing. We present the first real-world\nimplementation of a lateralized MB architecture for visual homing onboard a\ncompact autonomous car-like robot. We test whether the sign of the angular path\nintegration (PI) signal can categorize panoramic views, acquired during\nlearning walks and encoded in the MB, into \"goal on the left\" and \"goal on the\nright\" memory banks, enabling robust homing in natural outdoor settings. We\nvalidate this approach through four incremental experiments: (1) simulation\nshowing attractor-like nest dynamics; (2) real-world homing after decoupled\nlearning walks, producing nest search behavior; (3) homing after random walks\nusing noisy PI emulated with GPS-RTK; and (4) precise stopping-at-the-goal\nbehavior enabled by a fifth MB Output Neuron (MBON) encoding goal-views to\ncontrol velocity. This mimics the accurate homing behavior of ants and\nfunctionally resembles waypoint-based position control in robotics, despite\nrelying solely on visual input. Operating at 8 Hz on a Raspberry Pi 4 with\n32x32 pixel views and a memory footprint under 9 kB, our system offers a\nbiologically grounded, resource-efficient solution for autonomous visual\nhoming.", "AI": {"tldr": "\u53d7\u8682\u8681\u542f\u53d1\uff0c\u7814\u7a76\u4eba\u5458\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6a2a\u5411\u8611\u83c7\u4f53\u67b6\u6784\uff0c\u7528\u4e8e\u81ea\u4e3b\u89c6\u89c9\u5f52\u5de2\u3002\u8be5\u7cfb\u7edf\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4e0a\u8fd0\u884c\uff0c\u80fd\u6210\u529f\u5730\u5728\u5404\u79cd\u73af\u5883\u4e2d\u5bfc\u822a\u548c\u5b9a\u4f4d\u76ee\u6807\uff0c\u5176\u6548\u7387\u9ad8\u4e14\u5360\u7528\u7684\u5185\u5b58\u5c11\u3002", "motivation": "\u53d7\u8682\u8681\u5728\u6781\u7b80\u7684\u611f\u5b98\u8f93\u5165\u548c\u6709\u9650\u7684\u5b66\u4e60\u884c\u8d70\u4e0b\u5b9e\u73b0\u9c81\u68d2\u89c6\u89c9\u5f52\u5de2\u7684\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u81ea\u4e3b\u5bfc\u822a\u63d0\u4f9b\u4eff\u751f\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u4e00\u79cd\u6a2a\u5411\u6027\u8611\u83c7\u4f53\uff08MB\uff09\u67b6\u6784\uff0c\u7528\u4e8e\u5728\u7d27\u51d1\u578b\u81ea\u4e3b\u4eff\u8f66\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u89c6\u89c9\u5f52\u5de2\u3002\u901a\u8fc7\u5c06\u89d2\u5ea6\u8def\u5f84\u79ef\u5206\uff08PI\uff09\u4fe1\u53f7\u7684\u7b26\u53f7\u5206\u7c7b\u5168\u666f\u89c6\u56fe\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u8be5\u89c6\u56fe\u5728\u5b66\u4e60\u884c\u8d70\u671f\u95f4\u83b7\u53d6\u5e76\u7f16\u7801\u5728 MB \u4e2d\uff0c\u5e76\u5c06\u5176\u5206\u4e3a\u201c\u76ee\u6807\u5728\u5de6\u201d\u548c\u201c\u76ee\u6807\u5728\u53f3\u201d\u8bb0\u5fc6\u5e93\u3002", "result": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u56db\u4e2a\u5b9e\u9a8c\u5f97\u5230\u9a8c\u8bc1\uff1a(1) \u6a21\u62df\u663e\u793a\u7c7b\u4f3c\u5438\u5f15\u5b50\u7684\u5de2\u7a74\u52a8\u529b\u5b66\uff1b(2) \u5728\u5206\u79bb\u7684\u5b66\u4e60\u884c\u8d70\u540e\u8fdb\u884c\u771f\u5b9e\u4e16\u754c\u5f52\u5de2\uff0c\u4ea7\u751f\u5de2\u7a74\u641c\u7d22\u884c\u4e3a\uff1b(3) \u5728\u4f7f\u7528 GPS-RTK \u6a21\u62df\u7684\u566a\u58f0 PI \u7684\u968f\u673a\u884c\u8d70\u540e\u8fdb\u884c\u5f52\u5de2\uff1b(4) \u901a\u8fc7\u7f16\u7801\u76ee\u6807\u89c6\u56fe\u7684\u7b2c\u4e94\u4e2a MB \u8f93\u51fa\u795e\u7ecf\u5143\uff08MBON\uff09\u63a7\u5236\u901f\u5ea6\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u76ee\u6807\u505c\u6b62\u884c\u4e3a\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u662f\u4e00\u79cd\u53d7\u751f\u7269\u542f\u53d1\u7684\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u81ea\u4e3b\u89c6\u89c9\u5f52\u5de2\u89e3\u51b3\u65b9\u6848\uff0c\u5728 Raspberry Pi 4 \u4e0a\u4ee5 8 Hz \u7684\u9891\u7387\u8fd0\u884c\uff0c\u5185\u5b58\u5360\u7528\u91cf\u4f4e\u4e8e 9 kB\uff0c\u53ef\u5728 32x32 \u50cf\u7d20\u7684\u89c6\u56fe\u548c\u5185\u5b58\u5360\u7528\u91cf\u4e0b\u5b9e\u73b0\u7cbe\u786e\u7684\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u5f52\u5de2\u3002"}}
{"id": "2507.09168", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09168", "abs": "https://arxiv.org/abs/2507.09168", "authors": ["Haiming Zhu", "Yangyang Xu", "Chenshu Xu", "Tingrui Shen", "Wenxi Liu", "Yong Du", "Jun Yu", "Shengfeng He"], "title": "Stable Score Distillation", "comment": null, "summary": "Text-guided image and 3D editing have advanced with diffusion-based models,\nyet methods like Delta Denoising Score often struggle with stability, spatial\ncontrol, and editing strength. These limitations stem from reliance on complex\nauxiliary structures, which introduce conflicting optimization signals and\nrestrict precise, localized edits. We introduce Stable Score Distillation\n(SSD), a streamlined framework that enhances stability and alignment in the\nediting process by anchoring a single classifier to the source prompt.\nSpecifically, SSD utilizes Classifier-Free Guidance (CFG) equation to achieves\ncross-prompt alignment, and introduces a constant term null-text branch to\nstabilize the optimization process. This approach preserves the original\ncontent's structure and ensures that editing trajectories are closely aligned\nwith the source prompt, enabling smooth, prompt-specific modifications while\nmaintaining coherence in surrounding regions. Additionally, SSD incorporates a\nprompt enhancement branch to boost editing strength, particularly for style\ntransformations. Our method achieves state-of-the-art results in 2D and 3D\nediting tasks, including NeRF and text-driven style edits, with faster\nconvergence and reduced complexity, providing a robust and efficient solution\nfor text-guided editing.", "AI": {"tldr": "SSD\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u5355\u4e00\u5206\u7c7b\u5668\u548c\u8d1f\u6587\u672c/\u63d0\u793a\u589e\u5f3a\u5206\u652f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u65b9\u6cd5\u5728\u7a33\u5b9a\u6027\u3001\u7a7a\u95f4\u63a7\u5236\u548c\u7f16\u8f91\u5f3a\u5ea6\u65b9\u9762\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f18\u7684\u6548\u679c\u548c\u66f4\u4f4e\u7684\u590d\u6742\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5f15\u5bfc\u56fe\u50cf\u548c3D\u7f16\u8f91\u65b9\u6cd5\uff08\u5982Delta Denoising Score\uff09\u5728\u7a33\u5b9a\u6027\u3001\u7a7a\u95f4\u63a7\u5236\u548c\u7f16\u8f91\u5f3a\u5ea6\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u8fd9\u4e3b\u8981\u662f\u7531\u4e8e\u4f9d\u8d56\u590d\u6742\u7684\u8f85\u52a9\u7ed3\u6784\uff0c\u5f15\u5165\u4e86\u51b2\u7a81\u7684\u4f18\u5316\u4fe1\u53f7\uff0c\u5e76\u9650\u5236\u4e86\u7cbe\u786e\u7684\u5c40\u90e8\u7f16\u8f91\u3002", "method": "SSD\u6846\u67b6\u901a\u8fc7\u5c06\u5355\u4e2a\u5206\u7c7b\u5668\u951a\u5b9a\u5230\u6e90\u63d0\u793a\u4e0a\u6765\u589e\u5f3a\u7f16\u8f91\u8fc7\u7a0b\u4e2d\u7684\u7a33\u5b9a\u6027\u548c\u5bf9\u9f50\u6027\u3002\u5177\u4f53\u800c\u8a00\uff0cSSD\u5229\u7528\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\uff08CFG\uff09\u65b9\u7a0b\u5b9e\u73b0\u8de8\u63d0\u793a\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u6052\u5b9a\u7684\u8d1f\u6587\u672c\u5206\u652f\u6765\u7a33\u5b9a\u4f18\u5316\u8fc7\u7a0b\u3002\u6b64\u5916\uff0cSSD\u8fd8\u5305\u542b\u4e00\u4e2a\u63d0\u793a\u589e\u5f3a\u5206\u652f\uff0c\u7528\u4e8e\u63d0\u5347\u7f16\u8f91\u5f3a\u5ea6\uff0c\u7279\u522b\u662f\u5728\u98ce\u683c\u8f6c\u6362\u65b9\u9762\u3002", "result": "SSD\u65b9\u6cd5\u57282D\u548c3D\u7f16\u8f91\u4efb\u52a1\uff08\u5305\u62ecNeRF\u548c\u6587\u672c\u9a71\u52a8\u7684\u98ce\u683c\u7f16\u8f91\uff09\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u7f16\u8f91\u8fc7\u7a0b\u66f4\u7a33\u5b9a\u3001\u5bf9\u9f50\u6027\u66f4\u597d\u3001\u6536\u655b\u66f4\u5feb\u3001\u590d\u6742\u5ea6\u66f4\u4f4e\u3002", "conclusion": "SSD\u65b9\u6cd5\u57282D\u548c3D\u7f16\u8f91\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5305\u62ecNeRF\u548c\u6587\u672c\u9a71\u52a8\u7684\u98ce\u683c\u7f16\u8f91\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u4f4e\u7684\u590d\u6742\u6027\uff0c\u4e3a\u6587\u672c\u5f15\u5bfc\u7f16\u8f91\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5065\u58ee\u4e14\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10277", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.10277", "abs": "https://arxiv.org/abs/2507.10277", "authors": ["Hanyu Gong", "Jiawen Zhang", "Yan Zhao", "Shan Xiang", "Xiang Zhou", "Oliver Preu\u00df", "Wenjun Lu", "Yan Zhang", "Xufei Fang"], "title": "Dislocation-enhanced piezoelectric catalysis of KNbO3 crystal for water splitting", "comment": null, "summary": "Dislocations in oxides with ionic/covalent bonding hold the potential of\nharnessing versatile functionalities. Here, high-density dislocations in a\nlarge plastic zone in potassium niobate (KNbO3) crystals are mechanically\nintroduced by room-temperature cyclic scratching to enhance piezocatalytic\nhydrogen production. Unlike conventional energy-intensive, time-consuming\ndeformation at high temperature, this approach merits efficient dislocation\nengineering. These dislocations induce local strain and modify the electronic\nenvironment, thereby improving surface reactivity and charge separation, which\nare critical for piezocatalysis. This proof-of-concept offers a practical and\nsustainable alternative for functionalizing piezoelectric ceramics. Our\nfindings demonstrate that surface-engineered dislocations can effectively\nimprove the piezocatalysis, paving the way for efficient and scalable\npiezocatalytic applications.", "AI": {"tldr": "\u901a\u8fc7\u5ba4\u6e29\u5faa\u73af\u523b\u5212\u5728\u94cc\u9178\u94be\u6676\u4f53\u4e2d\u5f15\u5165\u9ad8\u5bc6\u5ea6\u4f4d\u9519\uff0c\u4ee5\u589e\u5f3a\u538b\u7535\u50ac\u5316\u4ea7\u6c22\u6027\u80fd\u3002", "motivation": "\u6c27\u5316\u7269\u4e2d\u5177\u6709\u79bb\u5b50/\u5171\u4ef7\u952e\u7684\u4f4d\u9519\u5177\u6709\u5b9e\u73b0\u591a\u529f\u80fd\u6027\u7684\u6f5c\u529b\u3002", "method": "\u901a\u8fc7\u5ba4\u6e29\u5faa\u73af\u523b\u5212\u5728\u94cc\u9178\u94be\uff08KNbO3\uff09\u6676\u4f53\u4e2d\u5f15\u5165\u9ad8\u5bc6\u5ea6\u4f4d\u9519\uff0c\u4ee5\u589e\u5f3a\u538b\u7535\u50ac\u5316\u4ea7\u6c22\u3002", "result": "\u6240\u5f15\u5165\u7684\u4f4d\u9519\u8bf1\u5bfc\u5c40\u90e8\u5e94\u53d8\u5e76\u6539\u53d8\u7535\u5b50\u73af\u5883\uff0c\u4ece\u800c\u6539\u5584\u8868\u9762\u53cd\u5e94\u6027\u548c\u7535\u8377\u5206\u79bb\uff0c\u8fd9\u5bf9\u538b\u7535\u50ac\u5316\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u8bc1\u660e\u4e86\u8868\u9762\u5de5\u7a0b\u5316\u7684\u4f4d\u9519\u53ef\u4ee5\u6709\u6548\u5730\u6539\u5584\u538b\u7535\u50ac\u5316\u6027\u80fd\uff0c\u4e3a\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u538b\u7535\u50ac\u5316\u5e94\u7528\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09590", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09590", "abs": "https://arxiv.org/abs/2507.09590", "authors": ["Noura Chabar", "Mohamed Amazioug"], "title": "Barnett effect boosted nonreciprocal entanglement and EPR-steering in magnomechanics in the presence of coherent feedback loop", "comment": "17 pages, 18+1 figures", "summary": "We propose an experimental scheme for enhancing entanglement, achieving\nasymmetric Einstein-Podolsky-Rosen (EPR) steering, and creating nonreciprocal\nquantum correlations within a hybrid system. This system integrates a yttrium\niron garnet (YIG) sphere, which exhibits magnon-phonon coupling via\nmagnetostriction, with a silica sphere featuring optomechanical\nwhispering-gallery modes. By tuning the Barnett effect through the magnetic\nfield direction, our system enables controllable asymmetric EPR steering and\nnonreciprocal entanglement between both directly and indirectly coupled modes.\nWe demonstrate that adjusting the reflectivity of a beam splitter can boost\nstationary quantum steering and entanglement, effectively countering thermal\nnoise. This approach allows for the generation of multipartite entanglement and\nboth one-way and two-way steering. The proposed system is experimentally\nfeasible and holds significant promise for various quantum information\napplications.", "AI": {"tldr": "\u901a\u8fc7\u5c06YIG\u7403\u4f53\u4e0e\u4e8c\u6c27\u5316\u7845\u7403\u4f53\u96c6\u6210\uff0c\u5e76\u5229\u7528\u78c1\u573a\u548c\u5206\u675f\u5668\u53cd\u5c04\u7387\u6765\u63a7\u5236\u76f8\u4e92\u4f5c\u7528\uff0c\u4ee5\u589e\u5f3a\u7ea0\u7f20\u548c\u5b9e\u73b0\u4e0d\u5bf9\u79f0\u91cf\u5b50\u5173\u8054\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u7ea0\u7f20\u3001\u5b9e\u73b0\u4e0d\u5bf9\u79f0EPR \u0924\u094d\u092f\u093e\u0902\u091a\u0947\u548c\u521b\u5efa\u975e\u4e92\u6613\u91cf\u5b50\u5173\u8054\u3002", "method": "\u901a\u8fc7\u8c03\u6574\u5df4\u5185\u7279\u6548\u5e94\u548c\u5206\u675f\u5668\u53cd\u5c04\u7387\u6765\u589e\u5f3a\u7ea0\u7f20\u3001\u5b9e\u73b0\u4e0d\u5bf9\u79f0EPR \u0924\u094d\u092f\u093e\u0902\u091a\u0947\u548c\u521b\u5efa\u975e\u4e92\u6613\u91cf\u5b50\u5173\u8054\u3002", "result": "\u6210\u529f\u5c55\u793a\u4e86\u53ef\u63a7\u7684\u4e0d\u5bf9\u79f0EPR \u0924\u094d\u092f\u093e\u0902\u091a\u0947\u548c\u76f4\u63a5\u6216\u95f4\u63a5\u8026\u5408\u6a21\u5f0f\u4e4b\u95f4\u7684\u975e\u4e92\u6613\u7ea0\u7f20\u3002\u6b64\u5916\uff0c\u8fd8\u6f14\u793a\u4e86\u901a\u8fc7\u8c03\u6574\u5206\u675f\u5668\u53cd\u5c04\u7387\u6765\u589e\u5f3a\u7a33\u6001\u91cf\u5b50 \u0924\u094d\u092f\u093e\u0902\u091a\u0947\u548c\u7ea0\u7f20\u4ee5\u62b5\u6297\u70ed\u566a\u58f0\uff0c\u5e76\u80fd\u751f\u6210\u591a\u65b9\u7ea0\u7f20\u4ee5\u53ca\u5355\u5411\u548c\u53cc\u5411 \u0924\u094d\u092f\u093e\u0902\u091a\u0947\u3002", "conclusion": "\u8be5\u63d0\u8bae\u7684\u7cfb\u7edf\u5177\u6709\u5b9e\u9a8c\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u5404\u79cd\u91cf\u5b50\u4fe1\u606f\u5e94\u7528\u5e26\u6765\u4e86\u5de8\u5927\u5e0c\u671b\u3002"}}
{"id": "2507.09424", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09424", "abs": "https://arxiv.org/abs/2507.09424", "authors": ["Cathy Jiao", "Yijun Pan", "Emily Xiao", "Daisy Sheng", "Niket Jain", "Hanzhang Zhao", "Ishita Dasgupta", "Jiaqi W. Ma", "Chenyan Xiong"], "title": "DATE-LM: Benchmarking Data Attribution Evaluation for Large Language Models", "comment": null, "summary": "Data attribution methods quantify the influence of training data on model\noutputs and are becoming increasingly relevant for a wide range of LLM research\nand applications, including dataset curation, model interpretability, data\nvaluation. However, there remain critical gaps in systematic LLM-centric\nevaluation of data attribution methods. To this end, we introduce DATE-LM (Data\nAttribution Evaluation in Language Models), a unified benchmark for evaluating\ndata attribution methods through real-world LLM applications. DATE-LM measures\nattribution quality through three key tasks -- training data selection,\ntoxicity/bias filtering, and factual attribution. Our benchmark is designed for\nease of use, enabling researchers to configure and run large-scale evaluations\nacross diverse tasks and LLM architectures. Furthermore, we use DATE-LM to\nconduct a large-scale evaluation of existing data attribution methods. Our\nfindings show that no single method dominates across all tasks, data\nattribution methods have trade-offs with simpler baselines, and method\nperformance is sensitive to task-specific evaluation design. Finally, we\nrelease a public leaderboard for quick comparison of methods and to facilitate\ncommunity engagement. We hope DATE-LM serves as a foundation for future data\nattribution research in LLMs.", "AI": {"tldr": "DATE-LM \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 LLM \u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\u8861\u91cf\u5f52\u56e0\u8d28\u91cf\u3002\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4e14\u6027\u80fd\u5bf9\u8bc4\u4f30\u8bbe\u8ba1\u654f\u611f\u3002", "motivation": "\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7814\u7a76\u548c\u5e94\u7528\u4e2d\u7684\u91cd\u8981\u6027\u65e5\u76ca\u589e\u52a0\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684 LLM \u4e2d\u5fc3\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86 DATE-LM\u3002", "method": "DATE-LM \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u57fa\u51c6\uff0c\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\u6765\u8861\u91cf\u5f52\u56e0\u8d28\u91cf\uff1a\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u3001\u6bd2\u6027/\u504f\u5dee\u8fc7\u6ee4\u548c\u4e8b\u5b9e\u5f52\u56e0\u3002\u8be5\u57fa\u51c6\u6613\u4e8e\u4f7f\u7528\uff0c\u53ef\u914d\u7f6e\u5927\u89c4\u6a21\u8bc4\u4f30\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528 DATE-LM \u5bf9\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6ca1\u6709\u4e00\u79cd\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u8fd9\u4e9b\u65b9\u6cd5\u7684\u6027\u80fd\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u8bc4\u4f30\u8bbe\u8ba1\u5f88\u654f\u611f\u3002", "conclusion": "DATE-LM \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u7684\u7edf\u4e00\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u901a\u8fc7\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\u2014\u2014\u8bad\u7ec3\u6570\u636e\u9009\u62e9\u3001\u6bd2\u6027/\u504f\u5dee\u8fc7\u6ee4\u548c\u4e8b\u5b9e\u5f52\u56e0\u2014\u2014\u6765\u8861\u91cf\u5f52\u56e0\u8d28\u91cf\u3002DATE-LM \u7684\u8bbe\u8ba1\u6613\u4e8e\u4f7f\u7528\uff0c\u53ef\u5b9e\u73b0\u8de8\u4e0d\u540c\u4efb\u52a1\u548c LLM \u67b6\u6784\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u5229\u7528 DATE-LM \u5bf9\u73b0\u6709\u6570\u636e\u5f52\u56e0\u65b9\u6cd5\u8fdb\u884c\u4e86\u5927\u89c4\u6a21\u8bc4\u4f30\uff0c\u53d1\u73b0\u6ca1\u6709\u4e00\u79cd\u65b9\u6cd5\u80fd\u5728\u6240\u6709\u4efb\u52a1\u4e0a\u90fd\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u5bf9\u7279\u5b9a\u4efb\u52a1\u7684\u8bc4\u4f30\u8bbe\u8ba1\u5f88\u654f\u611f\u3002\u6700\u540e\uff0c\u53d1\u5e03\u4e86\u4e00\u4e2a\u516c\u5171\u6392\u884c\u699c\u4ee5\u4fc3\u8fdb\u793e\u533a\u53c2\u4e0e\u548c\u65b9\u6cd5\u6bd4\u8f83\u3002"}}
{"id": "2507.10167", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10167", "abs": "https://arxiv.org/abs/2507.10167", "authors": ["Kaidi Wang", "Zhiguo Ding", "Naofal Al-Dhahir"], "title": "Pinching-Antenna Systems for Physical Layer Security", "comment": null, "summary": "This letter investigates the potential of pinching-antenna systems for\nenhancing physical layer security. By pre-installing multiple pinching antennas\nat discrete positions along a waveguide, the capability of the considered\nsystem to perform amplitude and phase adjustment is validated through the\nformulation of a secrecy rate maximization problem. Specifically, amplitude\ncontrol is applied to enhance the signal quality at the legitimate user, while\nphase alignment is designed to degrade the received signal quality at the\neavesdropper. This cooperation among pinching antennas is modeled as a\ncoalitional game, and a corresponding antenna activation algorithm is proposed.\nThe individual impact of each antenna is quantified based on the Shapley value\nand marginal contribution, providing a fair and efficient method for\nperformance evaluation. Simulation results show that the considered\npinching-antenna system achieves significant improvements in secrecy rate, and\nthat the Shapley value based algorithm outperforms conventional coalition value\nbased solutions.", "AI": {"tldr": "Pinching antennas enhance security by adjusting amplitude and phase. A coalitional game model with a Shapley value-based algorithm improves performance over traditional methods.", "motivation": "To investigate the potential of pinching-antenna systems for enhancing physical layer security by adjusting amplitude and phase to improve legitimate user signal quality and degrade eavesdropper signal quality.", "method": "The study formulates a secrecy rate maximization problem, models antenna cooperation as a coalitional game, and proposes an antenna activation algorithm. It quantifies individual antenna impact using the Shapley value and marginal contribution.", "result": "Simulation results demonstrate significant improvements in secrecy rate achieved by the pinching-antenna system, with the Shapley value-based algorithm showing superior performance compared to conventional methods.", "conclusion": "The pinching-antenna system significantly improves secrecy rate, and the Shapley value-based algorithm outperforms conventional solutions."}}
{"id": "2507.08874", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08874", "abs": "https://arxiv.org/abs/2507.08874", "authors": ["Yulin Sun", "Xiaopeng Si", "Runnan He", "Xiao Hu", "Peter Smielewski", "Wenlong Wang", "Xiaoguang Tong", "Wei Yue", "Meijun Pang", "Kuo Zhang", "Xizi Song", "Dong Ming", "Xiuyun Liu"], "title": "An Automated Classifier of Harmful Brain Activities for Clinical Usage Based on a Vision-Inspired Pre-trained Framework", "comment": null, "summary": "Timely identification of harmful brain activities via electroencephalography\n(EEG) is critical for brain disease diagnosis and treatment, which remains\nlimited application due to inter-rater variability, resource constraints, and\npoor generalizability of existing artificial intelligence (AI) models. In this\nstudy, a convolutional neural network model, VIPEEGNet, was developed and\nvalidated using EEGs recorded from Massachusetts General Hospital/Harvard\nMedical School. The VIPEEGNet was developed and validated using two independent\ndatasets, collected between 2006 and 2020. The development cohort included EEG\nrecordings from 1950 patients, with 106,800 EEG segments annotated by at least\none experts (ranging from 1 to 28). The online testing cohort consisted of EEG\nsegments from a subset of an additional 1,532 patients, each annotated by at\nleast 10 experts. For the development cohort (n=1950), the VIPEEGNet achieved\nhigh accuracy, with an AUROC for binary classification of seizure, LPD, GPD,\nLRDA, GRDA, and \"other\" categories at 0.972 (95% CI, 0.957-0.988), 0.962 (95%\nCI, 0.954-0.970), 0.972 (95% CI, 0.960-0.984), 0.938 (95% CI, 0.917-0.959),\n0.949 (95% CI, 0.941-0.957), and 0.930 (95% CI, 0.926-0.935). For multi\nclassification, the sensitivity of VIPEEGNET for the six categories ranges from\n36.8% to 88.2% and the precision ranges from 55.6% to 80.4%, and performance\nsimilar to human experts. Notably, the external validation showed\nKullback-Leibler Divergence (KLD)of 0.223 and 0.273, ranking top 2 among the\nexisting 2,767 competing algorithms, while we only used 2.8% of the parameters\nof the first-ranked algorithm.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10282", "categories": ["quant-ph", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.10282", "abs": "https://arxiv.org/abs/2507.10282", "authors": ["Luca Magazz\u00f9", "Elisabetta Paladino", "Jukka P. Pekola", "Milena Grifoni"], "title": "Thermal rectification in a qubit-resonator system", "comment": null, "summary": "A qubit-oscillator junction connecting as a series two bosonic heat baths at\ndifferent temperatures can display heat valve and diode effects. In particular,\nthe rectification can change in magnitude and even in sign, implying an\ninversion of the preferential direction for the heat current with respect to\nthe temperature bias. We perform a systematic study of these effects in a\ncircuit QED model of qubit-oscillator system and find that the features of\ncurrent and rectification crucially depend on the qubit-oscillator coupling.\nWhile at small coupling, transport occurs via a resonant mechanism between the\nsub-systems, in the ultrastrong coupling regime the junction is a unique,\nhighly hybridized system and the current becomes largely insensitive to the\ndetuning. Correspondingly, the rectification undergoes a change of sign. In the\nnonlinear transport regime, the coupling strength determines whether the\ncurrent scales sub- or super-linearly with the temperature bias and whether the\nrectification, which increases in magnitude with the bias, is positive or\nnegative. We also find that steady-state coherence largely suppresses the\ncurrent and enhances rectification. An insight on these behaviors with respect\nto changes in the system parameters is provided by analytical approximate\nformulas.", "AI": {"tldr": "\u7814\u7a76\u4e86\u91cf\u5b50\u6bd4\u7279-\u632f\u8361\u5668\u7cfb\u7edf\u4e2d\u7684\u70ed\u9600\u548c\u4e8c\u6781\u7ba1\u6548\u5e94\uff0c\u53d1\u73b0\u8026\u5408\u5f3a\u5ea6\u548c\u7a33\u6001\u76f8\u5e72\u6027\u5bf9\u6574\u6d41\u6548\u5e94\u6709\u663e\u8457\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76\u4e00\u4e2a\u7531\u91cf\u5b50\u6bd4\u7279-\u632f\u8361\u5668\u8fde\u63a5\u4e24\u4e2a\u4e0d\u540c\u6e29\u5ea6\u7684 \u0924\u093f\u091a\u094d\u092f\u093e\u8033\u6d74\u7ec4\u6210\u7684\u7ed3\uff0c\u8be5\u7ed3\u80fd\u5c55\u793a\u70ed\u9600\u548c\u4e8c\u6781\u7ba1\u6548\u5e94\uff0c\u7279\u522b\u662f\u6574\u6d41\u6548\u5e94\u7684\u5927\u5c0f\u548c\u7b26\u53f7\u4f1a\u53d1\u751f\u6539\u53d8\uff0c\u8fd9\u8868\u660e\u70ed\u6d41\u7684\u65b9\u5411\u4f1a\u968f\u7740\u6e29\u5ea6\u504f\u5dee\u7684\u6539\u53d8\u800c\u53cd\u8f6c\u3002", "method": "\u672c\u7814\u7a76\u5728\u7535\u8def\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u6a21\u578b\u4e2d\uff0c\u5bf9\u91cf\u5b50\u6bd4\u7279-\u632f\u8361\u5668\u7cfb\u7edf\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u7814\u7a76\uff0c\u8be5\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u4e0d\u540c\u6e29\u5ea6\u7684 \u0935\u0947\u0917\u0935\u0947\u0917\u4f46\u53c8\u76f8\u8fde\u7684\u7cfb\u7edf\uff0c\u7528\u6765\u5c55\u793a\u70ed\u9600\u548c\u4e8c\u6781\u7ba1\u6548\u5e94\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5f3a\u8026\u5408\u533a\uff0c\u7ed3\u53d8\u6210\u4e86\u4e00\u4e2a\u72ec\u7279\u7684\u3001\u9ad8\u5ea6\u6742\u5316\u7684\u7cfb\u7edf\uff0c\u7535\u6d41\u51e0\u4e4e\u4e0d\u968f\u5931\u8c10\u800c\u53d8\u5316\uff0c\u6574\u6d41\u6548\u5e94\u4e5f\u53d1\u751f\u4e86\u7b26\u53f7\u6539\u53d8\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u975e\u7ebf\u6027\u8f93\u8fd0\u533a\uff0c\u8026\u5408\u5f3a\u5ea6\u51b3\u5b9a\u4e86\u7535\u6d41\u968f\u6e29\u5ea6\u504f\u5dee\u662f\u4e9a\u7ebf\u6027\u8fd8\u662f\u8d85\u7ebf\u6027\u589e\u957f\uff0c\u4ee5\u53ca\u6574\u6d41\u6548\u5e94\u662f\u6b63\u8fd8\u662f\u8d1f\uff0c\u5e76\u4e14\u6574\u6d41\u6548\u5e94\u968f\u504f\u5dee\u589e\u5927\u800c\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u7a33\u6001\u76f8\u5e72\u6027\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6291\u5236\u4e86\u7535\u6d41\u4f46\u589e\u5f3a\u4e86\u6574\u6d41\u6548\u5e94\u3002\u901a\u8fc7\u8fd1\u4f3c\u516c\u5f0f\u53ef\u4ee5\u5e2e\u52a9\u7406\u89e3\u8fd9\u4e9b\u884c\u4e3a\u3002"}}
{"id": "2507.09801", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09801", "abs": "https://arxiv.org/abs/2507.09801", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "title": "Technical Requirements for Halting Dangerous AI Activities", "comment": null, "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans.", "AI": {"tldr": "\u4e3a\u4e86\u5e94\u5bf9\u4eba\u5de5\u667a\u80fd\u5e26\u6765\u7684\u98ce\u9669\uff0c\u653f\u5e9c\u53ef\u4ee5\u5b9e\u65bd\u6280\u672f\u5e72\u9884\u63aa\u65bd\u6765\u534f\u8c03\u505c\u6b62\u5371\u9669\u7684\u4eba\u5de5\u667a\u80fd\u5f00\u53d1\u548c\u90e8\u7f72\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5e26\u6765\u7684\u524d\u6240\u672a\u6709\u7684\u98ce\u9669\uff08\u5305\u62ec\u5931\u63a7\u3001\u6ee5\u7528\u3001\u5730\u7f18\u653f\u6cbb\u4e0d\u7a33\u5b9a\u548c\u6743\u529b\u96c6\u4e2d\uff09\uff0c\u653f\u5e9c\u53ef\u80fd\u4f1a\u4e3b\u52a8\u5efa\u7acb\u534f\u8c03\u505c\u6b62\u5371\u9669\u4eba\u5de5\u667a\u80fd\u5f00\u53d1\u548c\u90e8\u7f72\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u4ef6\u6982\u8ff0\u4e86\u5173\u952e\u6280\u672f\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5371\u9669\u4eba\u5de5\u667a\u80fd\u6d3b\u52a8\u7684\u534f\u8c03\u505c\u6b62\u3002", "result": "\u6211\u4eec\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u5982\u4f55\u6709\u52a9\u4e8e\u9650\u5236\u5404\u79cd\u5371\u9669\u7684\u4eba\u5de5\u667a\u80fd\u6d3b\u52a8\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u5982\u4f55\u4e3a\u6f5c\u5728\u7684\u4eba\u5de5\u667a\u80fd\u6cbb\u7406\u8ba1\u5212\u5960\u5b9a\u6280\u672f\u57fa\u7840\u3002", "conclusion": "\u8be5\u6587\u4ef6\u6982\u8ff0\u4e86\u5173\u952e\u6280\u672f\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5371\u9669\u4eba\u5de5\u667a\u80fd\u6d3b\u52a8\u7684\u534f\u8c03\u505c\u6b62\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u5982\u4f55\u4e3a\u6f5c\u5728\u7684\u4eba\u5de5\u667a\u80fd\u6cbb\u7406\u8ba1\u5212\u5960\u5b9a\u6280\u672f\u57fa\u7840\u3002"}}
{"id": "2507.10352", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2507.10352", "abs": "https://arxiv.org/abs/2507.10352", "authors": ["Alvaro Detailleur", "Guillaume Ducard", "Christopher Onder"], "title": "Improved Sum-of-Squares Stability Verification of Neural-Network-Based Controllers", "comment": null, "summary": "This work presents several improvements to the closed-loop stability\nverification framework using semialgebraic sets and convex semidefinite\nprogramming to examine neural-network-based control systems regulating\nnonlinear dynamical systems. First, the utility of the framework is greatly\nexpanded: two semialgebraic functions mimicking common, smooth activation\nfunctions are presented and compatibility with control systems incorporating\nRecurrent Equilibrium Networks (RENs) and thereby Recurrent Neural Networks\n(RNNs) is established. Second, the validity of the framework's state-of-the-art\nstability analyses is established via an alternate proof. Third, based on this\nproof, two new optimization problems simplifying the analysis of local\nstability properties are presented. To simplify the analysis of a closed-loop\nsystem's Region of Attraction (RoA), the first problem explicitly parameterizes\na class of candidate Lyapunov functions larger than in previous works. The\nsecond problem utilizes the unique guarantees available under the condition of\ninvariance to further expand the set of candidate Lyapunov functions and\ndirectly determine whether an invariant set forms part of the system's RoA.\nThese contributions are successfully demonstrated in two numerical examples and\nsuggestions for future research are provided.", "AI": {"tldr": "This paper enhances a stability verification framework for neural network control systems by introducing new functions, proving existing methods, and simplifying analysis with new optimization problems, all demonstrated with numerical examples.", "motivation": "The motivation of this work is to improve the closed-loop stability verification framework for neural-network-based control systems regulating nonlinear dynamical systems, by expanding its utility, validating its analyses, and simplifying stability property analysis.", "method": "This paper improves a closed-loop stability verification framework using semialgebraic sets and convex semidefinite programming. Improvements include expanding the framework's utility with two semialgebraic functions mimicking common activation functions, establishing compatibility with Recurrent Equilibrium Networks (RENs) and Recurrent Neural Networks (RNNs), and providing an alternate proof for the validity of state-of-the-art stability analyses. Based on this proof, two new optimization problems are presented to simplify the analysis of local stability properties and the Region of Attraction (RoA). The first problem parameterizes a larger class of candidate Lyapunov functions, while the second problem utilizes invariance guarantees to expand candidate Lyapunov functions and determine if an invariant set is part of the RoA.", "result": "The paper presents two semialgebraic functions that expand the framework's utility and establishes compatibility with RENs and RNNs. It also provides an alternate proof for the framework's stability analyses and introduces two new optimization problems that simplify the analysis of local stability properties and the Region of Attraction (RoA). These contributions are validated through two numerical examples.", "conclusion": "This work successfully demonstrates its contributions through two numerical examples and provides suggestions for future research."}}
{"id": "2507.09822", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09822", "abs": "https://arxiv.org/abs/2507.09822", "authors": ["Darshan Gadginmath", "Farhad Nawaz", "Minjun Sung", "Faizan M Tariq", "Sangjae Bae", "David Isele", "Fabio Pasqualetti", "Jovin Dsa"], "title": "Active Probing with Multimodal Predictions for Motion Planning", "comment": "To appear at IROS '25. 8 pages. 3 tables. 6 figures", "summary": "Navigation in dynamic environments requires autonomous systems to reason\nabout uncertainties in the behavior of other agents. In this paper, we\nintroduce a unified framework that combines trajectory planning with multimodal\npredictions and active probing to enhance decision-making under uncertainty. We\ndevelop a novel risk metric that seamlessly integrates multimodal prediction\nuncertainties through mixture models. When these uncertainties follow a\nGaussian mixture distribution, we prove that our risk metric admits a\nclosed-form solution, and is always finite, thus ensuring analytical\ntractability. To reduce prediction ambiguity, we incorporate an active probing\nmechanism that strategically selects actions to improve its estimates of\nbehavioral parameters of other agents, while simultaneously handling multimodal\nuncertainties. We extensively evaluate our framework in autonomous navigation\nscenarios using the MetaDrive simulation environment. Results demonstrate that\nour active probing approach successfully navigates complex traffic scenarios\nwith uncertain predictions. Additionally, our framework shows robust\nperformance across diverse traffic agent behavior models, indicating its broad\napplicability to real-world autonomous navigation challenges. Code and videos\nare available at\nhttps://darshangm.github.io/papers/active-probing-multimodal-predictions/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8f68\u8ff9\u89c4\u5212\u3001\u591a\u6a21\u6001\u9884\u6d4b\u548c\u4e3b\u52a8\u63a2\u6d4b\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u4ee5\u589e\u5f3a\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u51b3\u7b56\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u98ce\u9669\u5ea6\u91cf\uff0c\u80fd\u591f\u5904\u7406\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u5177\u6709\u95ed\u5f0f\u89e3\u3002\u901a\u8fc7\u4e3b\u52a8\u63a2\u6d4b\uff0c\u7cfb\u7edf\u80fd\u591f\u66f4\u597d\u5730\u4f30\u8ba1\u5176\u4ed6\u884c\u4e3a\u8005\u7684\u884c\u4e3a\u53c2\u6570\uff0c\u4ece\u800c\u51cf\u5c11\u9884\u6d4b\u6b67\u4e49\u3002\u5728 MetaDrive \u6a21\u62df\u73af\u5883\u4e2d\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u590d\u6742\u7684\u4ea4\u901a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5bf9\u4e0d\u540c\u7684\u884c\u4e3a\u6a21\u578b\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u5728\u52a8\u6001\u73af\u5883\u4e2d\u5bfc\u822a\u9700\u8981\u81ea\u4e3b\u7cfb\u7edf\u80fd\u591f\u63a8\u7406\u5176\u4ed6\u884c\u4e3a\u8005\u7684\u884c\u4e3a\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u98ce\u9669\u6307\u6807\uff0c\u901a\u8fc7\u6df7\u5408\u6a21\u578b\u65e0\u7f1d\u96c6\u6210\u591a\u6a21\u6001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002\u5f53\u8fd9\u4e9b\u4e0d\u786e\u5b9a\u6027\u9075\u5faa\u9ad8\u65af\u6df7\u5408\u5206\u5e03\u65f6\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u98ce\u9669\u6307\u6807\u5177\u6709\u95ed\u5f0f\u89e3\uff0c\u5e76\u4e14\u59cb\u7ec8\u662f\u6709\u9650\u7684\uff0c\u4ece\u800c\u786e\u4fdd\u4e86\u5206\u6790\u7684\u53ef\u5904\u7406\u6027\u3002\u4e3a\u4e86\u51cf\u5c11\u9884\u6d4b\u6b67\u4e49\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u4e3b\u52a8\u63a2\u6d4b\u673a\u5236\uff0c\u8be5\u673a\u5236\u4f18\u5148\u9009\u62e9\u80fd\u591f\u6539\u8fdb\u5bf9\u5176\u4ed6\u884c\u4e3a\u8005\u884c\u4e3a\u53c2\u6570\u7684\u4f30\u8ba1\u7684\u52a8\u4f5c\uff0c\u540c\u65f6\u5904\u7406\u591a\u6a21\u6001\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u4e3b\u52a8\u63a2\u6d4b\u65b9\u6cd5\u6210\u529f\u5bfc\u822a\u4e86\u5177\u6709\u4e0d\u786e\u5b9a\u9884\u6d4b\u7684\u590d\u6742\u4ea4\u901a\u573a\u666f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u5404\u79cd\u4ea4\u901a\u884c\u4e3a\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\uff0c\u8868\u660e\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u81ea\u52a8\u5bfc\u822a\u6311\u6218\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728 MetaDrive \u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u6211\u4eec\u7684\u4e3b\u52a8\u63a2\u6d4b\u65b9\u6cd5\u6210\u529f\u5bfc\u822a\u4e86\u5177\u6709\u4e0d\u786e\u5b9a\u9884\u6d4b\u7684\u590d\u6742\u4ea4\u901a\u573a\u666f\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6846\u67b6\u5728\u5404\u79cd\u4ea4\u901a\u884c\u4e3a\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u7684\u6027\u80fd\uff0c\u8868\u660e\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u81ea\u52a8\u5bfc\u822a\u6311\u6218\u4e2d\u7684\u5e7f\u6cdb\u9002\u7528\u6027\u3002"}}
{"id": "2507.09180", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09180", "abs": "https://arxiv.org/abs/2507.09180", "authors": ["Zichun Xu", "Yuntao Li", "Zhaomin Wang", "Lei Zhuang", "Guocai Yang", "Jingdong Zhao"], "title": "Learning and Transferring Better with Depth Information in Visual Reinforcement Learning", "comment": null, "summary": "Depth information is robust to scene appearance variations and inherently\ncarries 3D spatial details. In this paper, a visual backbone based on the\nvision transformer is proposed to fuse RGB and depth modalities for enhancing\ngeneralization. Different modalities are first processed by separate CNN stems,\nand the combined convolutional features are delivered to the scalable vision\ntransformer to obtain visual representations. Moreover, a contrastive\nunsupervised learning scheme is designed with masked and unmasked tokens to\naccelerate the sample efficiency during the reinforcement learning progress.\nFor sim2real transfer, a flexible curriculum learning schedule is developed to\ndeploy domain randomization over training processes.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u89c6\u89c9Transformer\u6a21\u578b\uff0c\u901a\u8fc7\u5bf9\u6bd4\u5b66\u4e60\u548c\u8bfe\u7a0b\u5b66\u4e60\u63d0\u9ad8\u4e86\u6cdb\u5316\u6027\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u6df1\u5ea6\u4fe1\u606f\u5bf9\u573a\u666f\u5916\u89c2\u53d8\u5316\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u5305\u542b3D\u7a7a\u95f4\u7ec6\u8282\uff0cRGB-D\u878d\u5408\u53ef\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u4f7f\u7528\u5355\u72ec\u7684CNN\u5904\u7406\u4e0d\u540c\u7684\u6a21\u6001\uff0c\u5e76\u5c06\u5377\u79ef\u7279\u5f81\u878d\u5408\u5230\u53ef\u6269\u5c55\u7684\u89c6\u89c9Transformer\u4e2d\u4ee5\u83b7\u5f97\u89c6\u89c9\u8868\u793a\u3002\u8bbe\u8ba1\u4e86\u5bf9\u6bd4\u5f0f\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6848\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u5f0f\u65e0\u76d1\u7763\u5b66\u4e60\u548c\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u878d\u5408RGB\u548c\u6df1\u5ea6\u4fe1\u606f\u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\uff0c\u4ee5\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u5bf9\u6bd4\u5f0f\u65e0\u76d1\u7763\u5b66\u4e60\u65b9\u6848\u6765\u63d0\u9ad8\u6837\u672c\u6548\u7387\uff0c\u540c\u65f6\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u7075\u6d3b\u7684\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\u6765\u89e3\u51b3sim2real\u8fc1\u79fb\u95ee\u9898\u3002"}}
{"id": "2507.10291", "categories": ["cond-mat.mtrl-sci", "physics.optics", "quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10291", "abs": "https://arxiv.org/abs/2507.10291", "authors": ["Kristine V. Ung", "Connor A. Roncaioli", "Ronald L. Walsworth", "Sean M. Blakley"], "title": "High Resolution Temperature-Resolved Spectroscopy of the Nitrogen Vacancy $^{1}E$ Singlet State Ionization Energy", "comment": "9 pages, 5 figures, invited manuscript for Recent Advances in Diamond\n  Science and Technology - SBDD XXIX special issue of Physica Status Solidi (a)", "summary": "The negatively charged diamond nitrogen-vacancy ($\\mathrm{{NV}^-}$) center\nplays a central role in many cutting edge quantum sensing applications; despite\nthis, much is still unknown about the energy levels in this system. The\nionization energy of the $\\mathrm{^{1}E}$ singlet state in the\n$\\mathrm{{NV}^-}$ has only recently been measured at between 2.25 eV and 2.33\neV. In this work, we further refine this energy by measuring the\n$\\mathrm{^{1}E}$ energy as a function of laser wavelength and diamond\ntemperature via magnetically mediated spin-selective photoluminescence (PL)\nquenching; this PL quenching indicating at what wavelength ionization induces\npopulation transfer from the $\\mathrm{^{1}E}$ into the neutral\n$\\mathrm{{NV}^0}$ charge configuration. Measurements are performed for\nexcitation wavelengths between 450 nm and 470 nm and between 540 nm and 566 nm\nin increments of 2 nm, and for temperatures ranging from about 50 K to 150 K in\n5 K increments. We determine the $\\mathrm{^{1}E}$ ionization energy to be\nbetween 2.29 and 2.33 eV, which provides about a two-fold reduction in\nuncertainty of this quantity. Distribution level: A. Approved for public\nrelease; distribution unlimited.", "AI": {"tldr": "\u901a\u8fc7\u5149\u81f4\u53d1\u5149\u731d\u706d\u6d4b\u91cf\u6280\u672f\uff0c\u672c\u7814\u7a76\u5c06\u91d1\u521a\u77f3NV$^-$\u4e2d\u5fc3\u7684$^{1}E$\u5355\u7ebf\u6001\u7535\u79bb\u80fd\u7cbe\u786e\u52302.29-2.33 eV\uff0c\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u4e00\u534a\u3002", "motivation": "\u5c3d\u7ba1\u91d1\u521a\u77f3\u6c2e-\u7a7a\u4f4d\uff08NV$^-$\uff09\u4e2d\u5fc3\u5728\u8bb8\u591a\u524d\u6cbf\u91cf\u5b50\u4f20\u611f\u5e94\u7528\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\uff0c\u4f46\u5173\u4e8e\u8be5\u7cfb\u7edf\u7684\u80fd\u7ea7\u4ecd\u6709\u8bb8\u591a\u672a\u77e5\u4e4b\u5904\u3002$^{1}E$\u5355\u7ebf\u6001\u7684\u7535\u79bb\u80fd\u6700\u8fd1\u624d\u88ab\u6d4b\u91cf\u51fa\u6765\uff0c\u4f46\u4ecd\u9700\u8fdb\u4e00\u6b65\u7cbe\u786e\u6d4b\u91cf\u3002", "method": "\u901a\u8fc7\u78c1\u6027\u4ecb\u5bfc\u7684\u3001\u81ea\u65cb\u9009\u62e9\u6027\u7684\u5149\u81f4\u53d1\u5149\u731d\u706d\uff08PL\uff09\u6765\u6d4b\u91cf$^{1}E$\u80fd\u91cf\u968f\u6fc0\u5149\u6ce2\u957f\u548c\u91d1\u521a\u77f3\u6e29\u5ea6\u7684\u53d8\u5316\uff0c\u5176\u4e2dPL\u731d\u706d\u6307\u793a\u7535\u79bb\u5728\u54ea\u4e2a\u6ce2\u957f\u5c06\u5f15\u8d77\u4ece$^{1}E$\u5230NV$^0$\u7535\u8377\u6001\u7684\u5e03\u5c45\u8f6c\u79fb\u3002\u6d4b\u91cf\u5728450 nm\u81f3470 nm\u548c540 nm\u81f3566 nm\u7684\u6fc0\u53d1\u6ce2\u957f\u8303\u56f4\u5185\u8fdb\u884c\uff0c\u6b65\u957f\u4e3a2 nm\uff0c\u5e76\u5728\u7ea650 K\u81f3150 K\u7684\u6e29\u5ea6\u8303\u56f4\u5185\u8fdb\u884c\uff0c\u6b65\u957f\u4e3a5 K\u3002", "result": "\u672c\u7814\u7a76\u786e\u5b9a\u4e86\u91d1\u521a\u77f3\u6c2e-\u7a7a\u4f4d\uff08NV$^-$\uff09\u4e2d\u5fc3\u7684$^{1}E$\u5355\u7ebf\u6001\u7535\u79bb\u80fd\u4e3a2.29\u81f32.33 eV\uff0c\u5c06\u8be5\u91cf\u7684\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u4e86\u7ea6\u4e00\u534a\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06NV$^-$\u7684$^{1}E$\u5355\u7ebf\u6001\u7535\u79bb\u80fd\u7684\u6d4b\u91cf\u4e0d\u786e\u5b9a\u6027\u964d\u4f4e\u4e86\u4e00\u534a\uff0c\u786e\u5b9a\u5176\u7535\u79bb\u80fd\u4e3a2.29\u81f32.33 eV\u3002"}}
{"id": "2507.09614", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2507.09614", "abs": "https://arxiv.org/abs/2507.09614", "authors": ["Mirco Erpelding", "Adrian Braemer", "Martin G\u00e4rttner"], "title": "Exploiting emergent symmetries in disorder-averaged quantum dynamics", "comment": "13 pages, 5 figures", "summary": "Symmetries are a key tool in understanding quantum systems, and, among many\nother things, can be exploited to increase the efficiency of numerical\nsimulations of quantum dynamics. Disordered systems usually feature reduced\nsymmetries and additionally require averaging over many realizations, making\ntheir numerical study computationally demanding. However, when studying\nquantities linear in the time-evolved state, i.e. expectation values of\nobservables, one can apply the averaging procedure to the time evolution\noperator, resulting in an effective dynamical map, which restores symmetry at\nthe level of super operators. In this work, we develop schemes for efficiently\nconstructing symmetric sectors of the disorder-averaged dynamical map using\nshort-time and weak-disorder expansions. To benchmark the method, we apply it\nto an Ising model with random all-to-all interactions in the presence of a\ntransverse field. After disorder averaging, this system becomes effectively\npermutation-invariant, and thus the size of the symmetric subspace scales\npolynomially in the number of spins allowing for the simulation of large\nsystem.", "AI": {"tldr": "\u5bf9\u79f0\u6027\u53ef\u7528\u4e8e\u63d0\u9ad8\u91cf\u5b50\u6a21\u62df\u7684\u6548\u7387\u3002\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u77ed\u671f\u548c\u5f31\u65e0\u5e8f\u5c55\u5f00\u6765\u6784\u5efa\u65e0\u5e8f\u5e73\u5747\u52a8\u529b\u5b66\u56fe\u7684\u5bf9\u79f0\u6247\u533a\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5df2\u5e94\u7528\u4e8e\u5177\u6709\u968f\u673a\u76f8\u4e92\u4f5c\u7528\u7684\u4f0a\u8f9b\u6a21\u578b\uff0c\u5e76\u5df2\u6210\u529f\u6a21\u62df\u5927\u7cfb\u7edf\u3002", "motivation": "\u5bf9\u79f0\u6027\u662f\u7406\u89e3\u91cf\u5b50\u7cfb\u7edf\u7684\u5173\u952e\u5de5\u5177\uff0c\u4f46\u65e0\u5e8f\u7cfb\u7edf\u901a\u5e38\u5177\u6709\u964d\u4f4e\u7684\u5bf9\u79f0\u6027\uff0c\u5e76\u4e14\u9700\u8981\u5bf9\u8bb8\u591a\u5b9e\u73b0\u8fdb\u884c\u5e73\u5747\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u7684\u6570\u503c\u7814\u7a76\u5728\u8ba1\u7b97\u4e0a\u8981\u6c42\u5f88\u9ad8\u3002", "method": "\u5f00\u53d1\u4e86\u4f7f\u7528\u77ed\u671f\u548c\u5f31\u65e0\u5e8f\u5c55\u5f00\u6765\u6709\u6548\u5730\u6784\u5efa\u65e0\u5e8f\u5e73\u5747\u52a8\u529b\u5b66\u56fe\u7684\u5bf9\u79f0\u6247\u533a\u7684\u65b9\u6848\u3002", "result": "\u901a\u8fc7\u5c06\u5e73\u5747\u7a0b\u5e8f\u5e94\u7528\u4e8e\u65f6\u95f4\u6f14\u5316\u7b97\u5b50\uff0c\u5728\u8d85\u7ea7\u7b97\u5b50\u5c42\u9762\u6062\u590d\u5bf9\u79f0\u6027\uff0c\u4ece\u800c\u6a21\u62df\u5927\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5bf9\u79f0\u6027\u6765\u6a21\u62df\u65e0\u5e8f\u7cfb\u7edf\uff0c\u5e76\u9488\u5bf9\u6613\u4e8e\u7406\u89e3\u7684\u4f0a\u8f9b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002"}}
{"id": "2507.09470", "categories": ["cs.CL", "cs.AI", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09470", "abs": "https://arxiv.org/abs/2507.09470", "authors": ["Mingchuan Yang", "Ziyuan Huang"], "title": "Enhancing Clinical Text Classification via Fine-Tuned DRAGON Longformer Models", "comment": "29 pages, 5 tables", "summary": "This study explores the optimization of the DRAGON Longformer base model for\nclinical text classification, specifically targeting the binary classification\nof medical case descriptions. A dataset of 500 clinical cases containing\nstructured medical observations was used, with 400 cases for training and 100\nfor validation. Enhancements to the pre-trained\njoeranbosma/dragon-longformer-base-mixed-domain model included hyperparameter\ntuning, domain-specific preprocessing, and architectural adjustments. Key\nmodifications involved increasing sequence length from 512 to 1024 tokens,\nadjusting learning rates from 1e-05 to 5e-06, extending training epochs from 5\nto 8, and incorporating specialized medical terminology. The optimized model\nachieved notable performance gains: accuracy improved from 72.0% to 85.2%,\nprecision from 68.0% to 84.1%, recall from 75.0% to 86.3%, and F1-score from\n71.0% to 85.2%. Statistical analysis confirmed the significance of these\nimprovements (p < .001). The model demonstrated enhanced capability in\ninterpreting medical terminology, anatomical measurements, and clinical\nobservations. These findings contribute to domain-specific language model\nresearch and offer practical implications for clinical natural language\nprocessing applications. The optimized model's strong performance across\ndiverse medical conditions underscores its potential for broad use in\nhealthcare settings.", "AI": {"tldr": "\u901a\u8fc7\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u3001\u8d85\u53c2\u6570\u548c\u52a0\u5165\u533b\u5b66\u672f\u8bed\uff0cDRAGON Longformer\u6a21\u578b\u5728\u533b\u5b66\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u4f18\u5316DRAGON Longformer base\u6a21\u578b\u4ee5\u7528\u4e8e\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\uff0c\u7279\u522b\u662f\u533b\u5b66\u75c5\u4f8b\u63cf\u8ff0\u7684\u4e8c\u5143\u5206\u7c7b\uff0c\u65e8\u5728\u63d0\u9ad8\u6a21\u578b\u5728\u5904\u7406\u548c\u7406\u89e3\u533b\u5b66\u6587\u672c\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u5bf9DRAGON Longformer base\u6a21\u578b\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u5305\u62ec\u8d85\u53c2\u6570\u8c03\u6574\uff08\u5e8f\u5217\u957f\u5ea6\u4ece512\u589e\u52a0\u52301024\uff0c\u5b66\u4e60\u7387\u4ece1e-05\u8c03\u6574\u52305e-06\uff0c\u8bad\u7ec3\u8f6e\u6b21\u4ece5\u589e\u52a0\u52308\uff09\u3001\u9886\u57df\u7279\u5b9a\u9884\u5904\u7406\u4ee5\u53ca\u67b6\u6784\u8c03\u6574\uff0c\u5e76\u878d\u5165\u4e86\u4e13\u95e8\u7684\u533b\u5b66\u672f\u8bed\u3002", "result": "\u4f18\u5316\u540e\u7684\u6a21\u578b\u5728\u51c6\u786e\u7387\uff08\u4ece72.0%\u63d0\u5347\u81f385.2%\uff09\u3001\u7cbe\u786e\u7387\uff08\u4ece68.0%\u63d0\u5347\u81f384.1%\uff09\u3001\u53ec\u56de\u7387\uff08\u4ece75.0%\u63d0\u5347\u81f386.3%\uff09\u548cF1\u5206\u6570\uff08\u4ece71.0%\u63d0\u5347\u81f385.2%\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u6539\u8fdb\u5177\u6709\u7edf\u8ba1\u5b66\u610f\u4e49\uff08p < .001\uff09\u3002\u6a21\u578b\u5728\u89e3\u8bfb\u533b\u5b66\u672f\u8bed\u3001\u89e3\u5256\u6d4b\u91cf\u548c\u4e34\u5e8a\u89c2\u5bdf\u65b9\u9762\u80fd\u529b\u589e\u5f3a\u3002", "conclusion": "\u8be5\u4f18\u5316\u540e\u7684DRAGON Longformer\u6a21\u578b\u5728\u4e34\u5e8a\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u533b\u5b66\u75c5\u4f8b\u63cf\u8ff0\u7684\u4e8c\u5143\u5206\u7c7b\u65b9\u9762\uff0c\u51c6\u786e\u7387\u3001\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u533b\u7597\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10173", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10173", "abs": "https://arxiv.org/abs/2507.10173", "authors": ["Kaidi Wang", "Chongjun Ouyang", "Yuanwei Liu", "Zhiguo Ding"], "title": "Pinching-Antenna Systems with LoS Blockages", "comment": null, "summary": "The aim of this letter is to explore the capability of pinching-antenna\nsystems to construct line-of-sight (LoS) links in the presence of LoS\nblockages. Specifically, pinching antennas are pre-installed at preconfigured\npositions along waveguides and can be selectively activated to create LoS links\nfor enhancing desired signals and non-line-of-sight (NLoS) links for\neliminating inter-user interference. On this basis, a sum-rate maximization\nproblem is formulated by jointly optimizing waveguide assignment and antenna\nactivation. To solve this problem, a matching based algorithm is proposed using\ntwo distinct preference designs. Simulation results demonstrate that the\nconsidered pinching-antenna system and proposed solutions can dynamically\nestablish LoS links and effectively exploit LoS blockages to mitigate\ninterference, thereby significantly improving system throughput.", "AI": {"tldr": "\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u53ef\u4ee5\u52a8\u6001\u5efa\u7acb\u89c6\u7ebf\uff08LoS\uff09\u94fe\u8def\u5e76\u5229\u7528\u89c6\u7ebf\uff08LoS\uff09\u963b\u585e\u6765\u51cf\u8f7b\u5e72\u6270\uff0c\u4ece\u800c\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u3002", "motivation": "\u672c letter \u7684\u76ee\u7684\u662f\u63a2\u7d22\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u5728\u5b58\u5728\u89c6\u7ebf\uff08LoS\uff09\u963b\u585e\u7684\u60c5\u51b5\u4e0b\u6784\u5efa\u89c6\u7ebf\uff08LoS\uff09\u94fe\u8def\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5339\u914d\u7684\u7b97\u6cd5\uff0c\u5e76\u91c7\u7528\u4e86\u4e24\u79cd\u4e0d\u540c\u7684\u504f\u597d\u8bbe\u8ba1\u6765\u89e3\u51b3\u548c\u4f18\u5316\u6ce2\u5bfc\u5206\u914d\u548c\u5929\u7ebf\u6fc0\u6d3b\u95ee\u9898\u3002", "result": "\u634f\u5408\u5929\u7ebf\u88ab\u9884\u5148\u5b89\u88c5\u5728\u6ce2\u5bfc\u4e0a\u7684\u9884\u914d\u7f6e\u4f4d\u7f6e\uff0c\u5e76\u4e14\u53ef\u4ee5\u9009\u62e9\u6027\u5730\u6fc0\u6d3b\u4ee5\u521b\u5efa\u89c6\u7ebf\uff08LoS\uff09\u94fe\u8def\u4ee5\u589e\u5f3a\u6240\u9700\u4fe1\u53f7\u548c\u975e\u89c6\u7ebf\uff08NLoS\uff09\u94fe\u8def\u4ee5\u6d88\u9664\u7528\u6237\u95f4\u5e72\u6270\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u8003\u8651\u7684\u634f\u5408\u5929\u7ebf\u7cfb\u7edf\u548c\u63d0\u51fa\u7684\u89e3\u51b3\u65b9\u6848\u53ef\u4ee5\u52a8\u6001\u5730\u5efa\u7acb\u89c6\u7ebf\uff08LoS\uff09\u94fe\u8def\uff0c\u5e76\u6709\u6548\u5730\u5229\u7528\u89c6\u7ebf\uff08LoS\uff09\u963b\u585e\u6765\u51cf\u8f7b\u5e72\u6270\uff0c\u4ece\u800c\u663e\u8457\u63d0\u9ad8\u7cfb\u7edf\u541e\u5410\u91cf\u3002"}}
{"id": "2507.08877", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08877", "abs": "https://arxiv.org/abs/2507.08877", "authors": ["Hanlong Zhang", "Jingsheng Yang", "Hao Li", "Yuhao He", "Franck Gong"], "title": "ODIA: Oriented Distillation for Inline Acceleration of LLM-based Function Calling", "comment": null, "summary": "Function Calling is a crucial technique that enables Large Language Models\n(LLMs) to interact with external systems through APIs. However, the high\nlatency associated with LLM-based Function Calling significantly impacts user\nexperience. This paper presents a novel approach called Oriented Distillation\nfor Inline Acceleration (ODIA) that leverages online user interaction data to\naccelerate Function Calling. By automatically identifying \"simple queries\" from\nproduction traffic and distilling knowledge from larger models to smaller ones,\nour method reduces response latency by 45% (expected) and 78% (median) while\nmaintaining accuracy. We demonstrate the effectiveness of our approach through\nreal-world deployment in a music application, where the smaller model\nsuccessfully handles 60% of traffic with negligible accuracy loss. Our method\nrequires minimal human intervention and continuously improves through automated\ndata collection and model updating, making it a practical solution for\nproduction environments.", "AI": {"tldr": "ODIA\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u52a0\u901fLLM\u51fd\u6570\u8c03\u7528\uff0c\u964d\u4f4e\u5ef6\u8fdf78%\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u51fd\u6570\u8c03\u7528\u6280\u672f\u867d\u7136\u5173\u952e\uff0c\u4f46\u5176\u9ad8\u5ef6\u8fdf\u4e25\u91cd\u5f71\u54cd\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u9762\u5411\u84b8\u998f\u7684\u5185\u8054\u52a0\u901f\u201d\uff08ODIA\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u5728\u7ebf\u7528\u6237\u4ea4\u4e92\u6570\u636e\u6765\u8bc6\u522b\u201c\u7b80\u5355\u67e5\u8be2\u201d\uff0c\u5e76\u5c06\u77e5\u8bc6\u4ece\u5927\u578b\u6a21\u578b\u84b8\u998f\u5230\u5c0f\u578b\u6a21\u578b\u3002", "result": "ODIA\u65b9\u6cd5\u6210\u529f\u5730\u5c06\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e\u4e8645%\uff08\u9884\u671f\uff09\u548c78%\uff08\u4e2d\u4f4d\u6570\uff09\uff0c\u5e76\u4e14\u5728\u5b9e\u9645\u97f3\u4e50\u5e94\u7528\u90e8\u7f72\u4e2d\uff0c\u5c0f\u578b\u6a21\u578b\u80fd\u591f\u5904\u740660%\u7684\u6d41\u91cf\uff0c\u51c6\u786e\u6027\u635f\u5931\u53ef\u5ffd\u7565\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u901a\u8fc7\u81ea\u52a8\u6570\u636e\u6536\u96c6\u548c\u6a21\u578b\u66f4\u65b0\u8fdb\u884c\u6301\u7eed\u6539\u8fdb\uff0c\u9002\u7528\u4e8e\u751f\u4ea7\u73af\u5883\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u7ebf\u7528\u6237\u4ea4\u4e92\u6570\u636e\u548c\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u51fd\u6570\u8c03\u7528\u80fd\u529b\u7684\u52a0\u901f\uff0c\u5c06\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e\u4e8645%\uff08\u9884\u671f\uff09\u548c78%\uff08\u4e2d\u4f4d\u6570\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002"}}
{"id": "2507.10370", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.10370", "abs": "https://arxiv.org/abs/2507.10370", "authors": ["Canbo Zong", "Deping Guo", "Renhong Wang", "Weihan Zhang", "Jiaqi Dai", "Zhongqin Zhang", "Cong Wang", "Xianghua Kong", "Wei Ji"], "title": "High-throughput prediction of thermodynamically stable 1D magnetic transition-metal chalcogenides and halides", "comment": "15 pages, 4 figures", "summary": "The search for novel one-dimensional (1D) materials with exotic physical\nproperties is crucial for advancing nanoelectronics and spintronics. Here, we\nperform a comprehensive high-throughput, first-principles study to explore the\nvast landscape of 1D transition-metal chalcogenides and halides. Starting with\n6,832 candidate structures derived from 28 metals and 8 non-metals, we\nsystematically evaluated their thermodynamic stability by comparing the\nformation energies of 1D chains against competing 2D phases, mimicking\nthermodynamic selectivity during nucleation. This screening identified 210\nstable 1D magnetic chains. Furthermore, representation learning models revealed\nthat chemical stoichiometry and the electron affinity of the non-metal element\nare key factors governing 1D stability. The stable materials exhibit a rich\nspectrum of properties, including diverse magnetic orders (FM, AFM) and\nLuttinger compensated antiferromagnetism in MnTe. We discovered 20 ferroelastic\nchains, with FeTe showing a giant magnetostriction of -5.57%. Other emergent\nphenomena include Charge Density Wave (CDW) chains in FeTe and NiSe. Finally,\nour findings propose concrete platforms for quantum applications, such as the\npredicted realization of Majorana zero modes in a ferromagnetic CrCl2 chain on\na superconducting NbSe2 substrate.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u9ad8\u901a\u91cf\u8ba1\u7b97\u7b5b\u9009\u51fa210\u79cd\u7a33\u5b9a\u7684\u4e00\u7ef4\u78c1\u6027\u6750\u6599\uff0c\u53d1\u73b0\u4e86\u5177\u6709\u65b0\u9896\u78c1\u6027\u3001\u94c1\u5f39\u6027\u548c\u7535\u8377\u5bc6\u5ea6\u6ce2\u7279\u6027\u7684\u6750\u6599\uff0c\u5e76\u9884\u6d4b\u4e86\u5176\u5728\u91cf\u5b50\u8ba1\u7b97\u7b49\u9886\u57df\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u5728\u7eb3\u7c73\u7535\u5b50\u5b66\u548c\u81ea\u65cb\u7535\u5b50\u5b66\u9886\u57df\u53d6\u5f97\u8fdb\u5c55\uff0c\u5bfb\u627e\u5177\u6709\u5947\u5f02\u7269\u7406\u6027\u8d28\u7684\u65b0\u578b\u4e00\u7ef4\uff081D\uff09\u6750\u6599\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u9ad8\u901a\u91cf\u3001\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7ed3\u5408\u70ed\u529b\u5b66\u7a33\u5b9a\u6027\u5206\u6790\uff08\u901a\u8fc7\u5f62\u6210\u80fd\u4e0e\u7ade\u4e89\u7684\u4e8c\u7ef4\u76f8\u8fdb\u884c\u6bd4\u8f83\uff09\u548c\u8868\u793a\u5b66\u4e60\u6a21\u578b\uff0c\u5bf96,832\u4e2a\u5019\u9009\u4e00\u7ef4\u6750\u6599\u7ed3\u6784\u8fdb\u884c\u4e86\u7cfb\u7edf\u6027\u7b5b\u9009\u3002", "result": "\u7814\u7a76\u7b5b\u9009\u51fa210\u4e2a\u7a33\u5b9a\u7684\u4e00\u7ef4\u78c1\u6027\u94fe\uff0c\u5e76\u63ed\u793a\u4e86\u5316\u5b66\u8ba1\u91cf\u548c\u975e\u91d1\u5c5e\u5143\u7d20\u7684\u7535\u5b50\u4eb2\u548c\u529b\u662f\u51b3\u5b9a\u4e00\u7ef4\u7a33\u5b9a\u6027\u7684\u5173\u952e\u56e0\u7d20\u3002\u6240\u53d1\u73b0\u7684\u6750\u6599\u5c55\u73b0\u51fa\u591a\u6837\u7684\u78c1\u5e8f\uff08FM, AFM\uff09\u548cLuttinger\u8865\u507f\u53cd\u94c1\u78c1\u6027\uff08\u5982MnTe\uff09\uff0c\u4ee5\u53ca\u94c1\u5f39\u6027\uff08\u5982FeTe\u5177\u6709\u5de8\u5927\u78c1\u81f4\u4f38\u7f29\uff09\u548c\u7535\u8377\u5bc6\u5ea6\u6ce2\uff08CDW\uff09\uff08\u5982FeTe\u548cNiSe\uff09\u7b49\u65b0\u5947\u73b0\u8c61\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\u5e76\u9a8c\u8bc1\u4e86\u591a\u79cd\u5177\u6709\u65b0\u9896\u78c1\u6027\u3001\u94c1\u5f39\u6027\u548c\u7535\u8377\u5bc6\u5ea6\u6ce2\uff08CDW\uff09\u7279\u6027\u7684\u65b0\u578b\u4e00\u7ef4\uff081D\uff09\u8fc7\u6e21\u91d1\u5c5e\u786b\u5c5e\u5316\u7269\u548c\u5364\u5316\u7269\u6750\u6599\u3002\u5176\u4e2d\uff0cMnTe\u8868\u73b0\u51faLuttinger\u8865\u507f\u53cd\u94c1\u78c1\u6027\uff0cFeTe\u5c55\u793a\u4e86-5.57%\u7684\u5de8\u5927\u78c1\u81f4\u4f38\u7f29\u6548\u5e94\uff0c\u5e76\u51fa\u73b0\u4e86CDW\u7279\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u9884\u6d4b\u4e86\u5728\u8d85\u5bfcNbSe2\u886c\u5e95\u4e0a\u7684\u78c1\u6027CrCl2\u94fe\u4e2d Majorana \u96f6\u6a21\u7684\u5b9e\u73b0\uff0c\u4e3a\u91cf\u5b50\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u5e73\u53f0\u3002"}}
{"id": "2507.09850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09850", "abs": "https://arxiv.org/abs/2507.09850", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.", "AI": {"tldr": "\u901a\u8fc7\u5c11\u91cf\u9ad8\u8d28\u91cfCoT\u793a\u4f8b\u5fae\u8c03\u57fa\u7840\u6a21\u578b\uff0c\u53ef\u5927\u5e45\u63d0\u5347\u5176\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\uff1b\u4f46\u4e13\u5bb6CoT\u7684\u67d0\u4e9b\u7279\u8d28\u96be\u4ee5\u901a\u8fc7\u975e\u4e13\u4e1a\u6570\u636e\u590d\u5236\u3002", "motivation": "\u63a2\u7a76\u662f\u5426\u4ec5\u901a\u8fc7\u63d0\u793a\u6216\u6700\u5c0f\u5316\u8c03\u6574\u5373\u53ef\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u8bf1\u5bfc\u4ea7\u751f\u957fCoT\u63a8\u7406\u80fd\u529b\uff0c\u4ee5\u53ca\u5c11\u91cf\u9ad8\u8d28\u91cfCoT\u793a\u4f8b\u7684\u6709\u6548\u6027\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u5c11\u91cf\uff0820\u4e2a\uff09\u6765\u81ea\u63a8\u7406\u6a21\u578b\uff08QwQ-32B-Preview\uff09\u7684\u957fCoT\u793a\u4f8b\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\uff08Qwen2.5-32B\uff09\u8fdb\u884c\u8f7b\u5ea6\u5fae\u8c03\u3002\u5e76\u63a2\u7d22\u4e86\u4f7f\u7528\u975e\u63a8\u7406\u6a21\u578b\u548c\u4eba\u7c7b\u6ce8\u91ca\u8005\u7684CoT\u6570\u636e\uff0c\u901a\u8fc7\u63d0\u793a\u5de5\u7a0b\u3001\u591a\u8f6e\u7f16\u8f91\u548c\u7ed3\u6784\u5316\u6307\u5bfc\u8fdb\u884c\u589e\u5f3a\u3002", "result": "\u8f7b\u5ea6\u5fae\u8c03\u540e\u7684Qwen2.5-32B\u6a21\u578b\u6027\u80fd\u8d85\u8d8a\u4e86\u66f4\u5927\u7684Qwen2.5-Math-72B-Instruct\u6a21\u578b\u3002\u975e\u63a8\u7406\u6a21\u578b\u6216\u4eba\u7c7b\u6ce8\u91ca\u8005\u7684CoT\u6570\u636e\uff0c\u5373\u4f7f\u7ecf\u8fc7\u589e\u5f3a\uff0c\u6027\u80fd\u4e5f\u65e0\u6cd5\u4e0e\u63a8\u7406\u6a21\u578b\u8ffd\u8e2a\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u5c11\u91cf\u9ad8\u8d28\u91cf\u7684\u601d\u7ef4\u94fe\uff08CoT\uff09\u793a\u4f8b\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u751a\u81f3\u8d85\u8d8a\u66f4\u5927\u3001\u7ecf\u8fc7\u4e13\u95e8\u8bad\u7ec3\u7684\u6a21\u578b\u3002\u5c3d\u7ba1\u4f7f\u7528\u975e\u63a8\u7406\u6a21\u578b\u6216\u4eba\u7c7b\u6ce8\u91ca\u8005\u7684\u6570\u636e\u8fdb\u884c\u589e\u5f3a\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u5176\u6027\u80fd\u4ecd\u4e0d\u53ca\u4e13\u95e8\u7684\u63a8\u7406\u6a21\u578b\u8ffd\u8e2a\uff0c\u8fd9\u8868\u660e\u4e13\u5bb6CoT\u7684\u67d0\u4e9b\u6f5c\u5728\u7279\u8d28\u96be\u4ee5\u590d\u5236\u3002\u7136\u800c\uff0c\u4ed4\u7ec6\u7b5b\u9009\u7684\u4eba\u7c7b\u7f16\u5199\u7684CoT\uff0c\u5373\u4f7f\u6570\u91cf\u4e0d\u591a\uff0c\u4e5f\u80fd\u6709\u6548\u6fc0\u6d3b\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u3002"}}
{"id": "2507.09183", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09183", "abs": "https://arxiv.org/abs/2507.09183", "authors": ["Yongwei Jiang", "Yixiong Zou", "Yuhua Li", "Ruixuan Li"], "title": "Revisiting Pool-based Prompt Learning for Few-shot Class-incremental Learning", "comment": "Accepted to ICCV 2025, 11 pages", "summary": "Few-Shot Class-Incremental Learning (FSCIL) faces dual challenges of data\nscarcity and incremental learning in real-world scenarios. While pool-based\nprompting methods have demonstrated success in traditional incremental\nlearning, their effectiveness in FSCIL settings remains unexplored. This paper\npresents the first study of current prompt pool methods in FSCIL tasks,\nrevealing an unanticipated performance degradation in incremental sessions.\nThrough comprehensive analysis, we identify that this phenomenon stems from\ntoken-dimension saturation: with limited data, excessive prompts compete for\ntask-relevant information, leading to model overfitting. Based on this finding,\nwe propose LGSP-Prompt (Local-Global Spatial Prompting), which innovatively\nshifts pool-based prompt learning from the token dimension to the spatial\ndimension. LGSP-Prompt generates spatial prompts by synergistically combining\nlocal spatial features and global frequency-domain representations to highlight\nkey patterns in input images. We construct two spatial prompt pools enabling\ndynamic prompt selection to maintain acquired knowledge while effectively\nlearning novel sessions. Extensive experiments demonstrate that our approach\nachieves state-of-the-art performance across multiple FSCIL benchmarks, showing\nsignificant advantages in both base knowledge preservation and incremental\nlearning. Our implementation is available at\nhttps://github.com/Jywsuperman/LGSP.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u63a2\u8ba8\u4e86\u63d0\u793a\u6c60\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u4e2d\u7684\u5e94\u7528\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u4ee4\u724c\u7ef4\u5ea6\u9971\u548c\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\u3002\u4e3a\u6b64\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684LGSP-Prompt\u65b9\u6cd5\uff0c\u5c06\u63d0\u793a\u5b66\u4e60\u4ece\u4ee4\u724c\u7ef4\u5ea6\u8f6c\u79fb\u5230\u7a7a\u95f4\u7ef4\u5ea6\uff0c\u901a\u8fc7\u7ed3\u5408\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u548c\u5168\u5c40\u9891\u57df\u8868\u793a\u6765\u751f\u6210\u7a7a\u95f4\u63d0\u793a\uff0c\u5e76\u5229\u7528\u52a8\u6001\u63d0\u793a\u9009\u62e9\u673a\u5236\u6765\u4f18\u5316\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLGSP-Prompt\u5728\u591a\u4e2aFSCIL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5c11\u6837\u672c\u589e\u91cf\u5b66\u4e60\uff08FSCIL\uff09\u4e2d\u6570\u636e\u7a00\u7f3a\u548c\u589e\u91cf\u5b66\u4e60\u7684\u53cc\u91cd\u6311\u6218\u3002\u73b0\u6709\u57fa\u4e8e\u6c60\u7684\u63d0\u793a\u65b9\u6cd5\u5728FSCIL\u8bbe\u7f6e\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u63a2\u7d22\uff0c\u5e76\u4e14\u4f1a\u9047\u5230\u4ee4\u724c\u7ef4\u5ea6\u9971\u548c\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLGSP-Prompt\uff08\u5c40\u90e8-\u5168\u5c40\u7a7a\u95f4\u63d0\u793a\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u5c06\u57fa\u4e8e\u6c60\u7684\u63d0\u793a\u5b66\u4e60\u4ece\u4ee4\u724c\u7ef4\u5ea6\u8f6c\u79fb\u5230\u7a7a\u95f4\u7ef4\u5ea6\u3002LGSP-Prompt\u901a\u8fc7\u534f\u540c\u7ed3\u5408\u5c40\u90e8\u7a7a\u95f4\u7279\u5f81\u548c\u5168\u5c40\u9891\u57df\u8868\u793a\u6765\u751f\u6210\u7a7a\u95f4\u63d0\u793a\uff0c\u4ee5\u7a81\u51fa\u8f93\u5165\u56fe\u50cf\u4e2d\u7684\u5173\u952e\u6a21\u5f0f\u3002\u6784\u5efa\u4e86\u4e24\u4e2a\u7a7a\u95f4\u63d0\u793a\u6c60\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u63d0\u793a\u9009\u62e9\uff0c\u4ee5\u5728\u6709\u6548\u5b66\u4e60\u65b0\u4f1a\u8bdd\u7684\u540c\u65f6\u4fdd\u6301\u5df2\u83b7\u5f97\u7684\u77e5\u8bc6\u3002", "result": "\u901a\u8fc7\u5168\u9762\u7684\u5206\u6790\uff0c\u53d1\u73b0\u6027\u80fd\u4e0b\u964d\u6e90\u4e8e\u4ee4\u724c\u7ef4\u5ea6\u9971\u548c\uff1a\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u8fc7\u591a\u7684\u63d0\u793a\u4f1a\u7ade\u4e89\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u4fe1\u606f\uff0c\u5bfc\u81f4\u6a21\u578b\u8fc7\u62df\u5408\u3002\u63d0\u51fa\u7684LGSP-Prompt\u65b9\u6cd5\u901a\u8fc7\u8f6c\u79fb\u5230\u7a7a\u95f4\u7ef4\u5ea6\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "conclusion": "LGSP-Prompt\u5728\u591a\u4e2a\u5c11\u6837\u672c\u589e\u91cf\u5b66\u4e60\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u57fa\u7840\u77e5\u8bc6\u4fdd\u7559\u548c\u589e\u91cf\u5b66\u4e60\u65b9\u9762\u5747\u663e\u793a\u51fa\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2507.09659", "categories": ["quant-ph", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2507.09659", "abs": "https://arxiv.org/abs/2507.09659", "authors": ["J. Naji", "R. Jafari", "Alireza Akbari", "M. Abdi"], "title": "Dynamics of quantum Fisher and Wigner-Yanase skew information following a noisy quench", "comment": null, "summary": "We study the influence of noise on the dynamics of a transverse field Ising\nmodel when quenched across a quantum critical point. To quantify two-spin\ncorrelations properties, we employ the quantum Fisher information (QFI) and\nWigner-Yanase skew information (WYSI) as measures of quantum coherence. In the\nabsence of noise, despite the entanglement, both QFI and WYSI increase\nmonotonically with the ramp quench time, approaching their adiabatic limits\nwithout exhibiting any Kibble-Zurek type scaling with quench duration. When\nnoise is added to the quench protocol, the coherence dynamics change\ndramatically: QFI and WYSI both decay exponentially with time scale of a ramp\nquench, with the exponent depending on the noise intensity. Furthermore, the\nmaximum ramp time, at which either of these measures reach their maximum,\nscales linearly with the noise variance, featuring the same exponent that\ndetermines the optimal annealing time for minimizing defect production in noisy\nquantum annealing.", "AI": {"tldr": "\u566a\u97f3\u4f1a\u5f71\u54cd\u91cf\u5b50\u7cfb\u7edf\u52a8\u529b\u5b66\u3002", "motivation": "\u7814\u7a76\u566a\u58f0\u5bf9\u7a7f\u8d8a\u91cf\u5b50\u4e34\u754c\u70b9\u7684\u6a2a\u5411\u573a\u4f0a\u8f9b\u6a21\u578b\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u5e76\u91cf\u5316\u76f8\u5e72\u6027\u3002", "method": "\u7814\u7a76\u4e86\u5728\u7a7f\u8d8a\u91cf\u5b50\u4e34\u754c\u70b9\u65f6\u7684\u566a\u58f0\u5bf9\u6a2a\u5411\u573a\u4f0a\u8f9b\u6a21\u578b\u52a8\u529b\u5b66\u7684\u5f71\u54cd\uff0c\u5e76\u4f7f\u7528\u91cf\u5b50\u8d39\u820d\u5c14\u4fe1\u606f\uff08QFI\uff09\u548c\u7ef4\u683c\u7eb3-\u4e9a\u7eb3\u65af\u659c\u7387\u4fe1\u606f\uff08WYSI\uff09\u4f5c\u4e3a\u91cf\u5b50\u76f8\u5e72\u6027\u7684\u5ea6\u91cf\u3002", "result": "\u5728\u6ca1\u6709\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0cQFI\u548cWYSI\u968f\u659c\u5761\u6dec\u706d\u65f6\u95f4\u7684\u589e\u52a0\u800c\u5355\u8c03\u589e\u52a0\u3002\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0cQFI\u548cWYSI\u968f\u65f6\u95f4\u5448\u6307\u6570\u8870\u51cf\uff0c\u8870\u51cf\u7387\u53d6\u51b3\u4e8e\u566a\u58f0\u5f3a\u5ea6\u3002\u62c9\u4f38\u65f6\u95f4\u7684\u6700\u5927\u503c\u4e0e\u566a\u58f0\u65b9\u5dee\u6210\u7ebf\u6027\u5173\u7cfb\u3002", "conclusion": "\u566a\u97f3\u4f1a\u6539\u53d8\u76f8\u5e72\u6027\u7684\u52a8\u529b\u5b66\uff0c\u5e76\u4e14\u6700\u5927\u62c9\u4f38\u65f6\u95f4\u4e0e\u566a\u58f0\u65b9\u5dee\u6210\u7ebf\u6027\u5173\u7cfb\uff0c\u4e0e\u6700\u5c0f\u5316\u6709\u7f3a\u9677\u91cf\u5b50\u9000\u706b\u4e2d\u7684\u7f3a\u9677\u6570\u6240\u9700\u7684\u6700\u4f73\u9000\u706b\u65f6\u95f4\u5177\u6709\u76f8\u540c\u7684\u6307\u6570\u3002"}}
{"id": "2507.09474", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09474", "abs": "https://arxiv.org/abs/2507.09474", "authors": ["Hwee Tou Ng", "Siew Mei Wu", "Yuanbin Wu", "Christian Hadiwinoto", "Joel Tetreault"], "title": "The CoNLL-2013 Shared Task on Grammatical Error Correction", "comment": "12 pages", "summary": "The CoNLL-2013 shared task was devoted to grammatical error correction. In\nthis paper, we give the task definition, present the data sets, and describe\nthe evaluation metric and scorer used in the shared task. We also give an\noverview of the various approaches adopted by the participating teams, and\npresent the evaluation results.", "AI": {"tldr": "CoNLL-2013 task: grammatical error correction. Presented data, metrics, and evaluated team approaches.", "motivation": "To define the CoNLL-2013 shared task on grammatical error correction and present its associated data, evaluation metric, and results.", "method": "Overview of participating teams' approaches and evaluation results.", "result": "Evaluation results of various approaches adopted by participating teams.", "conclusion": "The CoNLL-2013 shared task focused on grammatical error correction, providing task definitions, datasets, and evaluation metrics."}}
{"id": "2507.10308", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10308", "abs": "https://arxiv.org/abs/2507.10308", "authors": ["Hung Nguyen-Kha", "Vu Nguyen Ha", "Eva Lagunas", "Symeon Chatzinotas", "Joel Grotz"], "title": "Enhanced Throughput and Seamless Handover Solutions for Urban 5G-Vehicle C-Band Integrated Satellite-Terrestrial Networks", "comment": "ACCEPTED FOR PUBLICATION IN IEEE TRANSACTIONS ON COMMUNICATIONS", "summary": "This paper investigates downlink transmission in 5G Integrated\nSatellite-Terrestrial Networks (ISTNs) supporting automotive users (UEs) in\nurban environments, where base stations (BSs) and Low Earth Orbit (LEO)\nsatellites (LSats) cooperate to serve moving UEs over shared C-band frequency\ncarriers. Urban settings, characterized by dense obstructions, together with UE\nmobility, and the dynamic movement and coverage of LSats pose significant\nchallenges to user association and resource allocation. To address these\nchallenges, we formulate a multi-objective optimization problem designed to\nimprove both throughput and seamless handover (HO). Particularly, the\nformulated problem balances sum-rate (SR) maximization and connection change\n(CC) minimization through a weighted trade-off by jointly optimizing power\nallocation and BS-UE/LSat-UE associations over a given time window. This is a\nmixed-integer and non-convex problem which is inherently difficult to solve. To\nsolve this problem efficiently, we propose an iterative algorithm based on the\nSuccessive Convex Approximation (SCA) technique. Furthermore, we introduce a\npractical prediction-based algorithm capable of providing efficient solutions\nin real-world implementations. Especially, the simulations use a realistic 3D\nmap of London and UE routes obtained from the Google Navigator application to\nensure practical examination. Thanks to these realistic data, the simulation\nresults can show valuable insights into the link budget assessment in urban\nareas due to the impact of buildings on transmission links under the blockage,\nreflection, and diffraction effects. Furthermore, the numerical results\ndemonstrate the effectiveness of our proposed algorithms in terms of SR and the\nCC-number compared to the greedy and benchmark algorithms.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e5G\u96c6\u6210\u536b\u661f-\u9646\u5730\u7f51\u7edc\u7684\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u57ce\u5e02\u73af\u5883\u4e2d\u6c7d\u8f66\u7528\u6237\u7684\u541e\u5410\u91cf\u548c\u65e0\u7f1d\u5207\u6362\u80fd\u529b\u3002\u901a\u8fc7\u8054\u5408\u4f18\u5316\u529f\u7387\u5206\u914d\u548c\u7528\u6237\u5173\u8054\uff0c\u5e76\u4f7f\u7528SCA\u6280\u672f\u548c\u9884\u6d4b\u7b97\u6cd5\u6765\u89e3\u51b3\u590d\u6742\u7684\u4f18\u5316\u95ee\u9898\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u57ce\u5e02\u73af\u5883\u4e2d5G\u96c6\u6210\u536b\u661f-\u9646\u5730\u7f51\u7edc\uff08ISTNs\uff09\u4e2d\u7528\u6237\u5173\u8054\u548c\u8d44\u6e90\u5206\u914d\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6311\u6218\u662f\u7531\u5bc6\u96c6\u7684\u969c\u788d\u7269\u3001\u7528\u6237\u79fb\u52a8\u6027\u4ee5\u53ca\u4f4e\u5730\u7403\u8f68\u9053\uff08LEO\uff09\u536b\u661f\u7684\u52a8\u6001\u79fb\u52a8\u548c\u8986\u76d6\u8303\u56f4\u5f15\u8d77\u7684\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fde\u7eed\u51f8\u8fd1\u4f3c\uff08SCA\uff09\u6280\u672f\u7684\u8fed\u4ee3\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u9884\u6d4b\u7684\u5b9e\u7528\u7b97\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u8d2a\u5a6a\u548c\u57fa\u51c6\u7b97\u6cd5\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u548c\u8def\u754c\uff08SR\uff09\u548c\u8fde\u63a5\u6b21\u6570\uff08CC\uff09\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u548c\u8def\u754c\u548c\u8fde\u63a5\u6b21\u6570\u65b9\u9762\u6bd4\u8d2a\u5a6a\u548c\u57fa\u51c6\u7b97\u6cd5\u66f4\u6709\u6548\u3002"}}
{"id": "2507.08905", "categories": ["cs.LG", "cs.AI", "G.3"], "pdf": "https://arxiv.org/pdf/2507.08905", "abs": "https://arxiv.org/abs/2507.08905", "authors": ["Koen Vellenga", "H. Joe Steinhauer", "G\u00f6ran Falkman", "Jonas Andersson", "Anders Sj\u00f6gren"], "title": "Last Layer Hamiltonian Monte Carlo", "comment": "25 pages, 15 figures, 6 tables, currently under submission", "summary": "We explore the use of Hamiltonian Monte Carlo (HMC) sampling as a\nprobabilistic last layer approach for deep neural networks (DNNs). While HMC is\nwidely regarded as a gold standard for uncertainty estimation, the\ncomputational demands limit its application to large-scale datasets and large\nDNN architectures. Although the predictions from the sampled DNN parameters can\nbe parallelized, the computational cost still scales linearly with the number\nof samples (similar to an ensemble). Last layer HMC (LL--HMC) reduces the\nrequired computations by restricting the HMC sampling to the final layer of a\nDNN, making it applicable to more data-intensive scenarios with limited\ncomputational resources. In this paper, we compare LL-HMC against five last\nlayer probabilistic deep learning (LL-PDL) methods across three real-world\nvideo datasets for driver action and intention. We evaluate the in-distribution\nclassification performance, calibration, and out-of-distribution (OOD)\ndetection. Due to the stochastic nature of the probabilistic evaluations, we\nperformed five grid searches for different random seeds to avoid being reliant\non a single initialization for the hyperparameter configurations. The results\nshow that LL--HMC achieves competitive in-distribution classification and OOD\ndetection performance. Additional sampled last layer parameters do not improve\nthe classification performance, but can improve the OOD detection. Multiple\nchains or starting positions did not yield consistent improvements.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLL-HMC\u7684\u6982\u7387\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u89c6\u9891\u5206\u6790\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u80fd\u529b\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cLL-HMC\u5728\u5206\u7c7b\u548c\u5f02\u5e38\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u5e76\u4e14\u5728\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5177\u6709\u4f18\u52bf\u3002", "motivation": "\u7531\u4e8e\u8ba1\u7b97\u6210\u672c\u7684\u9650\u5236\uff0c\u4f20\u7edf\u7684Hamiltonian Monte Carlo (HMC)\u91c7\u6837\u96be\u4ee5\u5e94\u7528\u4e8e\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u5927\u578b\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63d0\u51fa\u6700\u540e\u5c42HMC\uff08LL-HMC\uff09\u65b9\u6cd5\uff0c\u5c06HMC\u91c7\u6837\u9650\u5236\u5728DNN\u7684\u6700\u540e\u5c42\uff0c\u4ece\u800c\u964d\u4f4e\u8ba1\u7b97\u9700\u6c42\uff0c\u4f7f\u5176\u80fd\u591f\u5e94\u7528\u4e8e\u6570\u636e\u5bc6\u96c6\u4e14\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u573a\u666f\u4e2d\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u6700\u540e\u5c42HMC\uff08LL-HMC\uff09\u91c7\u6837\u4f5c\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u6982\u7387\u6700\u540e\u5c42\u65b9\u6cd5\uff0c\u5e76\u5c06\u5176\u4e0e\u4e94\u79cd\u6700\u540e\u5c42\u6982\u7387\u6df1\u5ea6\u5b66\u4e60\uff08LL-PDL\uff09\u65b9\u6cd5\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u89c6\u9891\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8bc4\u4f30\u4e86\u5176\u5728in-distribution\u5206\u7c7b\u3001\u6821\u51c6\u548cOOD\u68c0\u6d4b\u65b9\u9762\u7684\u8868\u73b0\u3002\u4e3a\u4e86\u907f\u514d\u5bf9\u5355\u4e00\u968f\u673a\u79cd\u5b50\u4ea7\u751f\u4f9d\u8d56\uff0c\u7814\u7a76\u8fdb\u884c\u4e86\u4e94\u6b21\u4e0d\u540c\u968f\u673a\u79cd\u5b50\u7684\u7f51\u683c\u641c\u7d22\u3002", "result": "LL-HMC\u5728in-distribution\u5206\u7c7b\u548cOOD\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u7684\u6027\u80fd\u3002\u589e\u52a0\u6837\u672c\u6570\u91cf\u80fd\u591f\u63d0\u5347OOD\u68c0\u6d4b\u80fd\u529b\uff0c\u4f46\u5bf9\u5206\u7c7b\u6027\u80fd\u6ca1\u6709\u663e\u8457\u5f71\u54cd\u3002\u4f7f\u7528\u591a\u6761\u94fe\u6216\u591a\u4e2a\u8d77\u59cb\u70b9\u8fdb\u884c\u91c7\u6837\u5e76\u672a\u5e26\u6765\u4e00\u81f4\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "LL-HMC\u5728\u89c6\u9891\u9a71\u52a8\u52a8\u4f5c\u548c\u610f\u56fe\u8bc6\u522b\u4e2d\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684in-distribution\u5206\u7c7b\u548cOOD\u68c0\u6d4b\u6027\u80fd\u3002\u867d\u7136\u589e\u52a0\u6837\u672c\u6570\u91cf\u53ef\u4ee5\u6539\u5584OOD\u68c0\u6d4b\uff0c\u4f46\u5e76\u672a\u63d0\u5347\u5206\u7c7b\u6027\u80fd\uff0c\u4e14\u591a\u94fe\u6216\u591a\u8d77\u59cb\u70b9\u672a\u5e26\u6765\u6301\u7eed\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.10433", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2507.10433", "abs": "https://arxiv.org/abs/2507.10433", "authors": ["Suyeong Jin", "Jung-Wuk Hong", "Chiara Daraio", "Alexandre F. Fonseca"], "title": "Graphene Design with Parallel Cracks: Abnormal Crack Coalescence and Its Impact on Mechanical Properties", "comment": "35 pages, 9 figures, 5 tables", "summary": "Graphene is a material with potential applications in electric, thermal, and\nmechanical fields, and has seen significant advancements in growth methods that\nfacilitate large-scale production. However, defects during growth and transfer\nto other substrates can compromise the integrity and strength of graphene.\nSurprisingly, the literature suggests that, in certain cases, defects can\nenhance or, at most, not affect the mechanical performance of graphene. Further\nresearch is necessary to explore how defects interact within graphene structure\nand affect its properties, especially in large-area samples. In this study, we\ninvestigate the interaction between two preexisting cracks and their effect on\nthe mechanical properties of graphene using molecular dynamics simulations. The\nbehavior of zigzag and armchair graphene structures with cracks separated by\ndistances ($W_\\text{gap}$) is analyzed under tensile loading. The findings\nreveal that crack coalescence, defined as the formation of a new crack from two\nexisting crack tips, occurs for lower values of the distance between cracks,\n$W_\\text{gap}$, resulting in a decline in the strength of structures. As\n$W_\\text{gap}$ increases, the stress-strain curves shift upward, with the peak\nstress rising in the absence of crack coalescence. The effective stress\nintensity factor formulated in this study exhibits a clear upward trend with\nincreasing $W_\\text{gap}$. Furthermore, an increase in $W_\\text{gap}$ induces a\ntransition in fracture behavior from crack coalescence to independent\npropagation with intercrack undulation. This shift in fracture behavior\ndemonstrates a brittle-to-ductile transition, as evidenced by increased energy\nabsorption and delayed failure. A design guideline for the initial crack\ngeometry is suggested by correlating peak stress with the $W_\\text{gap}$,\nwithin a certain range.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u53d1\u73b0\uff0c\u88c2\u7eb9\u95f4\u8ddd\u5f71\u54cd\u77f3\u58a8\u70ef\u7684\u529b\u5b66\u6027\u80fd\u548c\u65ad\u88c2\u884c\u4e3a\u3002\u589e\u52a0\u88c2\u7eb9\u95f4\u8ddd\u53ef\u63d0\u9ad8\u77f3\u58a8\u70ef\u5f3a\u5ea6\u548c\u97e7\u6027\uff0c\u5e76\u5b9e\u73b0\u4ece\u8106\u6027\u5230\u97e7\u6027\u7684\u8f6c\u53d8\u3002", "motivation": "\u5c3d\u7ba1\u77f3\u58a8\u70ef\u5728\u5b8f\u89c2\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u751f\u957f\u548c\u8f6c\u79fb\u8fc7\u7a0b\u4e2d\u4ea7\u751f\u7684\u7f3a\u9677\u53ef\u80fd\u4f1a\u5f71\u54cd\u5176\u529b\u5b66\u6027\u80fd\u3002\u7136\u800c\uff0c\u73b0\u6709\u7814\u7a76\u8868\u660e\u7f3a\u9677\u6709\u65f6\u53cd\u800c\u4f1a\u589e\u5f3a\u77f3\u58a8\u70ef\u7684\u529b\u5b66\u6027\u80fd\uff0c\u8fd9\u5f15\u53d1\u4e86\u5bf9\u7f3a\u9677\u76f8\u4e92\u4f5c\u7528\u53ca\u5176\u5bf9\u77f3\u58a8\u70ef\u6027\u80fd\u5f71\u54cd\u7684\u8fdb\u4e00\u6b65\u63a2\u7d22\u9700\u6c42\uff0c\u5c24\u5176\u662f\u5728\u5927\u5c3a\u5bf8\u6837\u54c1\u4e2d\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u9884\u5b58\u5728\u88c2\u7eb9\u7684\u76f8\u4e92\u4f5c\u7528\u5982\u4f55\u5f71\u54cd\u77f3\u58a8\u70ef\u7684\u529b\u5b66\u6027\u80fd\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86\u5177\u6709\u4e24\u6761\u9884\u5b58\u5728\u88c2\u7eb9\u7684\u952f\u9f7f\u72b6\u548c\u6276\u624b\u6905\u72b6\u77f3\u58a8\u70ef\u5728\u62c9\u4f38\u8f7d\u8377\u4e0b\u7684\u884c\u4e3a\u3002\u7814\u7a76\u4e86\u88c2\u7eb9\u4e4b\u95f4\u7684\u8ddd\u79bb\uff08W_gap\uff09\u5bf9\u77f3\u58a8\u70ef\u529b\u5b66\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u88c2\u7eb9\u5408\u5e76\u7684\u53d1\u751f\u4ee5\u53ca\u5e94\u529b\u96c6\u4e2d\u56e0\u5b50\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u4e24\u6761\u88c2\u7eb9\u4e4b\u95f4\u7684\u8ddd\u79bb\uff08W_gap\uff09\u8f83\u5c0f\u65f6\uff0c\u4f1a\u51fa\u73b0\u88c2\u7eb9\u5408\u5e76\u73b0\u8c61\uff0c\u5bfc\u81f4\u77f3\u58a8\u70ef\u5f3a\u5ea6\u4e0b\u964d\u3002\u968f\u7740W_gap\u7684\u589e\u52a0\uff0c\u88c2\u7eb9\u5408\u5e76\u73b0\u8c61\u6d88\u5931\uff0c\u5f3a\u5ea6\u548c\u80fd\u91cf\u5438\u6536\u589e\u52a0\uff0c\u8868\u73b0\u51fa\u4ece\u8106\u6027\u5230\u97e7\u6027\u7684\u8f6c\u53d8\u3002\u63d0\u51fa\u7684\u6709\u6548\u5e94\u529b\u96c6\u4e2d\u56e0\u5b50\u968fW_gap\u7684\u589e\u52a0\u800c\u5448\u73b0\u4e0a\u5347\u8d8b\u52bf\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5cf0\u503c\u5e94\u529b\u4e0eW_gap\u4e4b\u95f4\u5173\u7cfb\u7684\u521d\u59cb\u88c2\u7eb9\u51e0\u4f55\u8bbe\u8ba1\u6307\u5357\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5206\u6790\u88c2\u7eb9\u76f8\u4e92\u4f5c\u7528\u5bf9\u77f3\u58a8\u70ef\u673a\u68b0\u6027\u80fd\u7684\u5f71\u54cd\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u88c2\u7eb9\u7684\u5408\u5e76\uff08\u5f53\u88c2\u7eb9\u95f4\u8ddd\u8f83\u5c0f\u65f6\uff09\u4f1a\u964d\u4f4e\u77f3\u58a8\u70ef\u7684\u5f3a\u5ea6\uff0c\u800c\u968f\u7740\u88c2\u7eb9\u95f4\u8ddd\u7684\u589e\u52a0\uff0c\u77f3\u58a8\u70ef\u7684\u5f3a\u5ea6\u548c\u97e7\u6027\u4f1a\u63d0\u9ad8\uff0c\u5e76\u51fa\u73b0\u4ece\u8106\u6027\u5230\u97e7\u6027\u7684\u8f6c\u53d8\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5cf0\u503c\u5e94\u529b\u4e0e\u88c2\u7eb9\u95f4\u8ddd\u5173\u8054\u7684\u8bbe\u8ba1\u6307\u5357\u3002"}}
{"id": "2507.09854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09854", "abs": "https://arxiv.org/abs/2507.09854", "authors": ["Aniruddha Chattopadhyay", "Raj Dandekar", "Kaushik Roy"], "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems", "comment": "Accepted as paper in 19th International Conference on Neurosymbolic\n  Learning and Reasoning,NeSy 2025", "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09857", "categories": ["cs.RO", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09857", "abs": "https://arxiv.org/abs/2507.09857", "authors": ["Xiaofei Wang", "Mingliang Han", "Tianyu Hao", "Cegang Li", "Yunbo Zhao", "Keke Tang"], "title": "AdvGrasp: Adversarial Attacks on Robotic Grasping from a Physical Perspective", "comment": "IJCAI'2025", "summary": "Adversarial attacks on robotic grasping provide valuable insights into\nevaluating and improving the robustness of these systems. Unlike studies that\nfocus solely on neural network predictions while overlooking the physical\nprinciples of grasping, this paper introduces AdvGrasp, a framework for\nadversarial attacks on robotic grasping from a physical perspective.\nSpecifically, AdvGrasp targets two core aspects: lift capability, which\nevaluates the ability to lift objects against gravity, and grasp stability,\nwhich assesses resistance to external disturbances. By deforming the object's\nshape to increase gravitational torque and reduce stability margin in the\nwrench space, our method systematically degrades these two key grasping\nmetrics, generating adversarial objects that compromise grasp performance.\nExtensive experiments across diverse scenarios validate the effectiveness of\nAdvGrasp, while real-world validations demonstrate its robustness and practical\napplicability", "AI": {"tldr": "AdvGrasp \u901a\u8fc7\u7269\u7406\u5f62\u53d8\u653b\u51fb\u673a\u5668\u4eba\u6293\u53d6\uff0c\u964d\u4f4e\u5176\u63d0\u8d77\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u548c\u73b0\u5b9e\u9a8c\u8bc1\u5747\u6709\u6548\u3002", "motivation": "\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u673a\u5668\u4eba\u6293\u53d6\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u4ec5\u5173\u6ce8\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u800c\u5ffd\u7565\u6293\u53d6\u7269\u7406\u539f\u7406\u7684\u7a7a\u767d\u3002", "method": "AdvGrasp \u6846\u67b6\u901a\u8fc7\u5bf9\u7269\u4f53\u8fdb\u884c\u5f62\u53d8\uff0c\u65e8\u5728\u589e\u52a0\u91cd\u529b\u626d\u77e9\u5e76\u51cf\u5c0f\u6293\u53d6\u7a33\u5b9a\u88d5\u5ea6\uff08\u5728\u529b/\u529b\u77e9\u7a7a\u95f4\u4e2d\uff09\uff0c\u4ece\u800c\u7cfb\u7edf\u6027\u5730\u964d\u4f4e\u6293\u53d6\u80fd\u529b\uff08\u62b5\u6297\u91cd\u529b\u7684\u63d0\u8d77\u80fd\u529b\uff09\u548c\u6293\u53d6\u7a33\u5b9a\u6027\uff08\u62b5\u6297\u5916\u90e8\u5e72\u6270\u7684\u80fd\u529b\uff09\u3002", "result": "AdvGrasp \u6846\u67b6\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u964d\u4f4e\u6293\u53d6\u80fd\u529b\u548c\u6293\u53d6\u7a33\u5b9a\u6027\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u5176\u6709\u6548\u6027\uff0c\u771f\u5b9e\u4e16\u754c\u9a8c\u8bc1\u4e5f\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u5b9e\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684 AdvGrasp \u6846\u67b6\u901a\u8fc7\u7269\u7406\u5c42\u9762\u4e0a\u7684\u5f62\u53d8\u653b\u51fb\uff0c\u6709\u6548\u5730\u964d\u4f4e\u4e86\u673a\u5668\u4eba\u6293\u53d6\u7684\u6293\u53d6\u80fd\u529b\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u5728\u591a\u79cd\u573a\u666f\u4e0b\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u548c\u771f\u5b9e\u4e16\u754c\u7684\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3001\u9c81\u68d2\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09184", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09184", "abs": "https://arxiv.org/abs/2507.09184", "authors": ["Qiyan Zhao", "Xiaofeng Zhang", "Yiheng Li", "Yun Xing", "Xiaosong Yuan", "Feilong Tang", "Sinan Fan", "Xuhang Chen", "Xuyao Zhang", "Dahan Wang"], "title": "MCA-LLaVA: Manhattan Causal Attention for Reducing Hallucination in Large Vision-Language Models", "comment": "Accepted in ACM MM 2025", "summary": "Hallucinations pose a significant challenge in Large Vision Language Models\n(LVLMs), with misalignment between multimodal features identified as a key\ncontributing factor. This paper reveals the negative impact of the long-term\ndecay in Rotary Position Encoding (RoPE), used for positional modeling in\nLVLMs, on multimodal alignment. Concretely, under long-term decay, instruction\ntokens exhibit uneven perception of image tokens located at different positions\nwithin the two-dimensional space: prioritizing image tokens from the\nbottom-right region since in the one-dimensional sequence, these tokens are\npositionally closer to the instruction tokens. This biased perception leads to\ninsufficient image-instruction interaction and suboptimal multimodal alignment.\nWe refer to this phenomenon as image alignment bias. To enhance instruction's\nperception of image tokens at different spatial locations, we propose\nMCA-LLaVA, based on Manhattan distance, which extends the long-term decay to a\ntwo-dimensional, multi-directional spatial decay. MCA-LLaVA integrates the\none-dimensional sequence order and two-dimensional spatial position of image\ntokens for positional modeling, mitigating hallucinations by alleviating image\nalignment bias. Experimental results of MCA-LLaVA across various hallucination\nand general benchmarks demonstrate its effectiveness and generality. The code\ncan be accessed in https://github.com/ErikZ719/MCA-LLaVA.", "AI": {"tldr": "\u6587\u7ae0\u53d1\u73b0\u4e86LVLMs\u4e2d\u7684\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\u957f\u5c3e\u8870\u51cf\u4f1a\u5bfc\u81f4\u56fe\u50cf-\u6307\u4ee4\u5bf9\u9f50\u504f\u5dee\uff0c\u8fdb\u800c\u5f15\u53d1\u5e7b\u89c9\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCA-LLaVA\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e8c\u7ef4\u591a\u5411\u7a7a\u95f4\u8870\u51cf\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u4e2d\u7531\u65cb\u8f6c\u4f4d\u7f6e\u7f16\u7801\uff08RoPE\uff09\u7684\u957f\u5c3e\u8870\u51cf\u5f15\u8d77\u7684\u5e7b\u89c9\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u5bfc\u81f4\u6a21\u578b\u5728\u611f\u77e5\u56fe\u50cf\u4e0d\u540c\u533a\u57df\u65f6\u5b58\u5728\u504f\u5dee\uff0c\u5f71\u54cd\u4e86\u591a\u6a21\u6001\u5bf9\u9f50\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCA-LLaVA\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u66fc\u54c8\u987f\u8ddd\u79bb\uff0c\u5c06\u4e00\u7ef4\u7684RoPE\u957f\u5c3e\u8870\u51cf\u6269\u5c55\u5230\u4e8c\u7ef4\u591a\u5411\u7a7a\u95f4\u8870\u51cf\uff0c\u4ee5\u89e3\u51b3LVLMs\u4e2d\u7531\u4f4d\u7f6e\u7f16\u7801\u5f15\u8d77\u7684\u56fe\u50cf-\u6307\u4ee4\u5bf9\u9f50\u504f\u5dee\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMCA-LLaVA\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6709\u6548\u4e14\u5177\u6709\u901a\u7528\u6027\uff0c\u80fd\u591f\u7f13\u89e3\u56fe\u50cf\u5bf9\u9f50\u504f\u5dee\uff0c\u4ece\u800c\u51cf\u5c11\u5e7b\u89c9\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "MCA-LLaVA\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u66fc\u54c8\u987f\u8ddd\u79bb\u7684\u4e8c\u7ef4\u591a\u5411\u7a7a\u95f4\u8870\u51cf\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u957f\u5c3e\u8870\u51cf\u5bfc\u81f4\u7684\u56fe\u50cf-\u6307\u4ee4\u5bf9\u9f50\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86LVLMs\u5728\u5904\u7406\u591a\u6a21\u6001\u4fe1\u606f\u65f6\u7684\u51c6\u786e\u6027\uff0c\u5e76\u51cf\u5c11\u4e86\u5e7b\u89c9\u73b0\u8c61\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.10428", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.10428", "abs": "https://arxiv.org/abs/2507.10428", "authors": ["Maryia Zinouyeva", "Martina Fracchia", "Giulia Maranini", "Mauro Coduri", "Davide Impelluso", "Nicholas B. Brookes", "Lorenzo Grilli", "Kurt Kummer", "Francesco Rosa", "Matteo Aramini", "Giacomo Ghiringhelli", "Paolo Ghigna", "Marco Moretti Sala"], "title": "X-raying Mg0.2Co0.2Ni0.2Cu0.2Zn0.2O: disentangling elemental contributions in a prototypical high-entropy oxide", "comment": "13 pages, 10 figures", "summary": "We employ several X-ray based techniques, including X-ray diffraction,\nabsorption and resonant inelastic scattering, to disentangle the contributions\nof individual chemical species to the structural, electronic and magnetic\nproperties of high-entropy oxides. In the benchmark compound\nMg0.2Co0.2Ni0.2Cu0.2Zn0.2O and related systems, we unambiguously resolve a\nsizable Jahn-Teller distortion at the Cu sites, more pronounced in the absence\nof Ni2+ and Mg2+, suggesting that these ions promote positional order, whereas\nCu2+ ions act to destabilize it. Moreover, we detect magnetic excitations and\nestimate the strength of the interactions between pairs of different magnetic\nelements. Our results provide valuable insights into the role of the various\nchemical species in shaping the physical properties of high-entropy oxides.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u5229\u7528 X \u5c04\u7ebf\u6280\u672f\u5206\u6790\u4e86\u9ad8\u71b5\u6c27\u5316\u7269\uff0c\u53d1\u73b0\u4e86\u94dc\u4f4d\u70b9\u5904\u7684 Jahn-Teller \u7578\u53d8\uff0c\u5e76\u7814\u7a76\u4e86\u4e0d\u540c\u78c1\u6027\u5143\u7d20\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u4e3a\u4e86\u533a\u5206\u9ad8\u71b5\u6c27\u5316\u7269\u4e2d\u5404\u4e2a\u5316\u5b66\u7269\u8d28\u5bf9\u7ed3\u6784\u3001\u7535\u5b50\u548c\u78c1\u6027\u7684\u8d21\u732e\u3002", "method": "\u91c7\u7528 X \u5c04\u7ebf\u884d\u5c04\u3001\u5438\u6536\u548c\u5171\u632f\u975e\u5f39\u6027\u6563\u5c04\u7b49\u591a\u79cd X \u5c04\u7ebf\u6280\u672f\uff0c\u4ee5\u53ca\u78c1\u6fc0\u53d1\u548c\u6210\u5bf9\u76f8\u4e92\u4f5c\u7528\u7684\u4f30\u8ba1\u3002", "result": "\u5728 Mg0.2Co0.2Ni0.2Cu0.2Zn0.2O \u548c\u76f8\u5173\u4f53\u7cfb\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u660e\u786e\u89e3\u6790\u4e86\u94dc\u4f4d\u70b9\u5904\u663e\u8457\u7684 Jahn-Teller \u7578\u53d8\uff0c\u5e76\u4e14\u5728 Ni2+ \u548c Mg2+ \u7f3a\u5931\u65f6\u66f4\u4e3a\u660e\u663e\uff0c\u8fd9\u8868\u660e\u8fd9\u4e9b\u79bb\u5b50\u53ef\u4ee5\u4fc3\u8fdb\u4f4d\u7f6e\u6709\u5e8f\uff0c\u800c Cu2+ \u79bb\u5b50\u5219\u4f1a\u7834\u574f\u5176\u7a33\u5b9a\u6027\u3002 \u6b64\u5916\uff0c\u8fd8\u68c0\u6d4b\u5230\u4e86\u78c1\u6fc0\u53d1\u5e76\u4f30\u8ba1\u4e86\u4e0d\u540c\u78c1\u6027\u5143\u7d20\u6210\u5bf9\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u3002", "conclusion": "\u9ad8\u71b5\u6c27\u5316\u7269\u4e2d\u7684\u5404\u79cd\u5316\u5b66\u7269\u8d28\u5728\u5851\u9020\u5176\u7269\u7406\u7279\u6027\u65b9\u9762\u53d1\u6325\u7740\u91cd\u8981\u4f5c\u7528\u3002"}}
{"id": "2507.09667", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09667", "abs": "https://arxiv.org/abs/2507.09667", "authors": ["Pei-Kun Yang"], "title": "Quantum Convolution for Structure-Based Virtual Screening", "comment": "18 pages, 4 figures, 2 tables. The source code and dataset\n  preprocessing scripts are publicly available at\n  https://github.com/peikunyang/07_QCNN_SBVS. This is a proof-of-concept study\n  demonstrating quantum convolutional neural networks for structure-based\n  virtual screening using PDBbind v2020", "summary": "Structure-based virtual screening (SBVS) is a key computational strategy for\nidentifying potential drug candidates by estimating the binding free energies\n(delta G_bind) of protein-ligand complexes. The immense size of chemical\nlibraries, combined with the need to account for protein and ligand\nconformations as well as ligand translations and rotations, makes these tasks\ncomputationally intensive on classical hardware. This study proposes a quantum\nconvolutional neural network (QCNN) framework to estimate delta G_bind\nefficiently. Using the PDBbind v2020 dataset, we trained QCNN models with 9 and\n12 qubits, with the core set designated as the test set. The best-performing\nmodel achieved a Pearson correlation coefficient of 0.694 on the test set. To\nassess robustness, we introduced quantum noise under two configurations. While\nnoise increased the root mean square deviation, the Pearson correlation\ncoefficient remained largely stable. These results demonstrate the feasibility\nand noise tolerance of QCNNs for high-throughput virtual screening and\nhighlight the potential of quantum computing to accelerate drug discovery.", "AI": {"tldr": "\u4f7f\u7528\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u53ef\u4ee5\u9ad8\u6548\u5730\u4f30\u8ba1delta G_bind\uff0c\u5e76\u6709\u671b\u52a0\u901f\u836f\u7269\u53d1\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7ed3\u6784\u7684\u865a\u62df\u7b5b\u9009\uff08SBVS\uff09\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4f5c\u8005\u5e0c\u671b\u901a\u8fc7\u91cf\u5b50\u8ba1\u7b97\u6765\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4f7f\u7528\u91cf\u5b50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08QCNN\uff09\u6846\u67b6\u6765\u4f30\u8ba1delta G_bind\u3002", "result": "\u5728PDBbind v2020\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u7684QCNN\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e860.694\u7684\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u5e76\u4e14\u5728\u5f15\u5165\u91cf\u5b50\u566a\u58f0\u540e\uff0c\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\u4fdd\u6301\u7a33\u5b9a\u3002", "conclusion": "\u91cf\u5b50\u8ba1\u7b97\u6709\u6f5c\u529b\u52a0\u901f\u836f\u7269\u53d1\u73b0\uff0c\u5e76\u4e14QCNN\u5bf9\u566a\u58f0\u5177\u6709\u9c81\u68d2\u6027\uff0c\u9002\u7528\u4e8e\u9ad8\u901a\u91cf\u865a\u62df\u7b5b\u9009\u3002"}}
{"id": "2507.09477", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09477", "abs": "https://arxiv.org/abs/2507.09477", "authors": ["Yangning Li", "Weizhi Zhang", "Yuyao Yang", "Wei-Chieh Huang", "Yaozu Wu", "Junyu Luo", "Yuanchen Bei", "Henry Peng Zou", "Xiao Luo", "Yusheng Zhao", "Chunkit Chan", "Yankai Chen", "Zhongfen Deng", "Yinghui Li", "Hai-Tao Zheng", "Dongyuan Li", "Renhe Jiang", "Ming Zhang", "Yangqiu Song", "Philip S. Yu"], "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning Systems in LLMs", "comment": "submitted to ARR May", "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.", "AI": {"tldr": "\u672c\u8c03\u67e5\u7edf\u4e00\u4e86\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u63a8\u7406\u65b9\u6cd5\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u901a\u8fc7\u8fed\u4ee3\u641c\u7d22\u548c\u63a8\u7406\u5b9e\u73b0\u6700\u5148\u8fdb\u6027\u80fd\u7684\u534f\u540c\u6846\u67b6\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u901a\u8fc7\u6ce8\u5165\u5916\u90e8\u77e5\u8bc6\u6765\u63d0\u9ad8\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4e8b\u5b9e\u6027\uff0c\u4f46\u5b83\u5728\u9700\u8981\u591a\u6b65\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u8868\u73b0\u4e0d\u4f73\uff1b\u76f8\u53cd\uff0c\u7eaf\u7cb9\u4ee5\u63a8\u7406\u4e3a\u5bfc\u5411\u7684\u65b9\u6cd5\u5e38\u5e38\u51fa\u73b0\u5e7b\u89c9\u6216\u4e8b\u5b9e\u9519\u8bef\u3002\u672c\u6b21\u8c03\u67e5\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u4e0d\u8db3\u3002", "method": "\u8be5\u8c03\u67e5\u9996\u5148\u5206\u6790\u4e86\u9ad8\u7ea7\u63a8\u7406\u5982\u4f55\u4f18\u5316RAG\u7684\u6bcf\u4e2a\u9636\u6bb5\uff08\u589e\u5f3a\u63a8\u7406\u7684RAG\uff09\u3002\u7136\u540e\uff0c\u6211\u4eec\u5c55\u793a\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u68c0\u7d22\u77e5\u8bc6\u5982\u4f55\u4e3a\u590d\u6742\u63a8\u7406\uff08\u589e\u5f3a\u63a8\u7406\u7684RAG\uff09\u63d0\u4f9b\u7f3a\u5931\u7684\u5148\u51b3\u6761\u4ef6\u548c\u6269\u5c55\u4e0a\u4e0b\u6587\u3002\u6700\u540e\uff0c\u6211\u4eec\u91cd\u70b9\u4ecb\u7ecd\u4e86\u65b0\u5174\u7684\u534f\u540cRAG-\u63a8\u7406\u6846\u67b6\u3002", "result": "\u8be5\u8c03\u67e5\u5bf9\u65b9\u6cd5\u3001\u6570\u636e\u96c6\u548c\u5f00\u653e\u6311\u6218\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u4e3a\u5b9e\u73b0\u66f4\u6709\u6548\u3001\u591a\u6a21\u6001\u9002\u5e94\u3001\u53ef\u4fe1\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u6df1\u5ea6RAG-\u63a8\u7406\u7cfb\u7edf\u6307\u660e\u4e86\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8c03\u67e5\u5c06\u57fa\u4e8e\u68c0\u7d22\u7684\u751f\u6210\uff08RAG\uff09\u548c\u7eaf\u7cb9\u7684\u4ee5\u63a8\u7406\u4e3a\u5bfc\u5411\u7684\u65b9\u6cd5\u7edf\u4e00\u5728\u7edf\u4e00\u7684\u63a8\u7406-\u68c0\u7d22\u89c6\u89d2\u4e0b\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u65b0\u5174\u7684\u534f\u540cRAG-\u63a8\u7406\u6846\u67b6\uff0c\u5176\u4e2d\uff08\u4ee3\u7406\uff09LLM \u8fed\u4ee3\u5730\u4ea4\u7ec7\u641c\u7d22\u548c\u63a8\u7406\uff0c\u4ee5\u5728\u77e5\u8bc6\u5bc6\u96c6\u578b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09091", "categories": ["cs.LG", "eess.SP", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09091", "abs": "https://arxiv.org/abs/2507.09091", "authors": ["Shayan K. Azmoodeh", "Krishna Subramani", "Paris Smaragdis"], "title": "Continuous-Time Signal Decomposition: An Implicit Neural Generalization of PCA and ICA", "comment": "6 pages, 3 figures, 1 table. MLSP 2025", "summary": "We generalize the low-rank decomposition problem, such as principal and\nindependent component analysis (PCA, ICA) for continuous-time vector-valued\nsignals and provide a model-agnostic implicit neural signal representation\nframework to learn numerical approximations to solve the problem. Modeling\nsignals as continuous-time stochastic processes, we unify the approaches to\nboth the PCA and ICA problems in the continuous setting through a contrast\nfunction term in the network loss, enforcing the desired statistical properties\nof the source signals (decorrelation, independence) learned in the\ndecomposition. This extension to a continuous domain allows the application of\nsuch decompositions to point clouds and irregularly sampled signals where\nstandard techniques are not applicable.", "AI": {"tldr": "Neural network framework unifies and extends PCA/ICA for continuous signals like point clouds and irregular data.", "motivation": "To generalize low-rank decomposition problems like PCA and ICA to continuous-time signals and address limitations of standard techniques with point clouds and irregularly sampled data.", "method": "We model signals as continuous-time stochastic processes and use a model-agnostic implicit neural representation framework. A contrast function in the network loss enforces desired statistical properties (decorrelation, independence) of the source signals.", "result": "The framework provides numerical approximations for continuous-domain PCA and ICA, allowing these decompositions to be applied to previously incompatible data types.", "conclusion": "We present a unified framework for generalized low-rank decomposition of continuous-time vector-valued signals, extending PCA and ICA to continuous domains and enabling their application to point clouds and irregularly sampled data."}}
{"id": "2507.08912", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.08912", "abs": "https://arxiv.org/abs/2507.08912", "authors": ["Tomasz Szandala", "Fatima Ezzeddine", "Natalia Rusin", "Silvia Giordano", "Omran Ayoub"], "title": "Fair-FLIP: Fair Deepfake Detection with Fairness-Oriented Final Layer Input Prioritising", "comment": null, "summary": "Artificial Intelligence-generated content has become increasingly popular,\nyet its malicious use, particularly the deepfakes, poses a serious threat to\npublic trust and discourse. While deepfake detection methods achieve high\npredictive performance, they often exhibit biases across demographic attributes\nsuch as ethnicity and gender. In this work, we tackle the challenge of fair\ndeepfake detection, aiming to mitigate these biases while maintaining robust\ndetection capabilities. To this end, we propose a novel post-processing\napproach, referred to as Fairness-Oriented Final Layer Input Prioritising\n(Fair-FLIP), that reweights a trained model's final-layer inputs to reduce\nsubgroup disparities, prioritising those with low variability while demoting\nhighly variable ones. Experimental results comparing Fair-FLIP to both the\nbaseline (without fairness-oriented de-biasing) and state-of-the-art approaches\nshow that Fair-FLIP can enhance fairness metrics by up to 30% while maintaining\nbaseline accuracy, with only a negligible reduction of 0.25%.\n  Code is available on Github:\nhttps://github.com/szandala/fair-deepfake-detection-toolbox", "AI": {"tldr": "\u63d0\u51fa Fair-FLIP \u65b9\u6cd5\u6765\u89e3\u51b3\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u6a21\u578b\u8f93\u5165\u6765\u51cf\u5c11\u504f\u89c1\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002", "motivation": "\u6df1\u5ea6\u4f2a\u9020\uff08deepfakes\uff09\u7684\u6076\u610f\u4f7f\u7528\u5bf9\u516c\u4f17\u4fe1\u4efb\u548c\u8a00\u8bba\u6784\u6210\u4e86\u4e25\u91cd\u5a01\u80c1\uff0c\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u5728\u8de8\u6c11\u65cf\u548c\u6027\u522b\u7b49\u4eba\u53e3\u7edf\u8ba1\u5c5e\u6027\u4e0a\u5b58\u5728\u504f\u89c1\uff0c\u56e0\u6b64\u9700\u8981\u89e3\u51b3\u516c\u5e73\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Fair-FLIP \u7684\u65b0\u9896\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u7ec8\u5c42\u8f93\u5165\u6765\u51cf\u5c11\u5b50\u7ec4\u5dee\u5f02\uff0c\u4f18\u5148\u8003\u8651\u53d8\u5f02\u6027\u4f4e\u7684\u5b50\u7ec4\uff0c\u540c\u65f6\u964d\u7ea7\u53d8\u5f02\u6027\u9ad8\u7684\u5b50\u7ec4\u3002", "result": "\u4e0e\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cFair-FLIP \u53ef\u5c06\u516c\u5e73\u6027\u6307\u6807\u6700\u591a\u63d0\u9ad8 30%\uff0c\u540c\u65f6\u5c06\u51c6\u786e\u6027\u964d\u4f4e\u4ec5 0.25%\u3002", "conclusion": "Fair-FLIP \u662f\u4e00\u79cd\u65b0\u9896\u7684\u540e\u5904\u7406\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u65b0\u52a0\u6743\u8bad\u7ec3\u6a21\u578b\u7684\u6700\u7ec8\u5c42\u8f93\u5165\u6765\u51cf\u5c11\u5b50\u7ec4\u5dee\u5f02\uff0c\u540c\u65f6\u4fdd\u6301\u9c81\u68d2\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u5728\u516c\u5e73\u6027\u6307\u6807\u4e0a\u6700\u591a\u53ef\u63d0\u9ad8 30%\uff0c\u540c\u65f6\u4ec5\u7565\u5fae\u964d\u4f4e 0.25% \u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.09884", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09884", "abs": "https://arxiv.org/abs/2507.09884", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu", "Yongzhen Guo", "Wentao Zhang"], "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains", "comment": "Preprint, Under review", "summary": "Large language models (LLMs) increasingly rely on reinforcement learning (RL)\nto enhance their reasoning capabilities through feedback. A critical challenge\nis verifying the consistency of model-generated responses and reference\nanswers, since these responses are often lengthy, diverse, and nuanced.\nRule-based verifiers struggle with complexity, prompting the use of model-based\nverifiers. However, specialized verifiers lack flexibility, while general LLM\njudges can be inconsistent. Existing research primarily focuses on building\nbetter verifiers, yet a systematic evaluation of different types of verifiers'\nperformance across domains remains lacking, severely constraining the reliable\ndevelopment of Reinforcement Learning with Verifiable Reward (RLVR). To address\nthis, we propose VerifyBench--a cross-domain comprehensive benchmark for\nsystematically evaluating verifiers. We construct 4,000 expert-level questions\ncovering mathematics, physics, chemistry, and biology. Each question is\nequipped with reference answers and diverse responses. The reliability of the\nevaluation is ensured through a rigorous annotation process conducted by a\nmultidisciplinary expert team. We design a four-dimensional experimental\nframework to comprehensively compare the performance boundaries of specialized\nverifiers and general LLMs under combined conditions of extracted answers vs.\ncomplete responses, and short vs. long outputs. Our evaluation uncovers\nfundamental trade-offs in verifiers: while specialized verifiers achieve\nleading accuracy, they exhibit deficiencies in recall; general models show\nstronger inclusivity but unstable precision. More importantly, we discover\nverifiers' high sensitivity to input structure and inherent limitations in\ncross-domain generalization, providing critical insights into the bottlenecks\nof current verifier technology.", "AI": {"tldr": "VerifyBench\u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a8c\u8bc1\u5668\u7684\u65b0\u57fa\u51c6\u3002\u8be5\u57fa\u51c6\u5305\u542b\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u548c\u751f\u7269\u5b66\u9886\u57df\u76844000\u4e2a\u95ee\u9898\u53ca\u5176\u7b54\u6848\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e13\u95e8\u9a8c\u8bc1\u5668\u51c6\u786e\u7387\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u901a\u7528LLM\u5305\u5bb9\u6027\u5f3a\u4f46\u7cbe\u786e\u5ea6\u4e0d\u7a33\u5b9a\uff0c\u4e14\u9a8c\u8bc1\u5668\u5bf9\u8f93\u5165\u7ed3\u6784\u654f\u611f\u4e14\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "motivation": "\u5c3d\u7ba1LLM\u8d8a\u6765\u8d8a\u4f9d\u8d56\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u901a\u8fc7\u53cd\u9988\u6765\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u9a8c\u8bc1\u6a21\u578b\u751f\u6210\u54cd\u5e94\u4e0e\u53c2\u8003\u7b54\u6848\u7684\u4e00\u81f4\u6027\u5374\u9762\u4e34\u5de8\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u54cd\u5e94\u901a\u5e38\u5197\u957f\u3001\u591a\u6837\u4e14\u7ec6\u5fae\u3002\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u6539\u8fdb\u9a8c\u8bc1\u5668\u672c\u8eab\uff0c\u4f46\u7f3a\u4e4f\u5bf9\u5176\u5728\u4e0d\u540c\u9886\u57df\u6027\u80fd\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u53ef\u9a8c\u8bc1\u5956\u52b1\u5f3a\u5316\u5b66\u4e60\uff08RLVR\uff09\u7684\u53ef\u9760\u53d1\u5c55\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86VerifyBench\uff0c\u4e00\u4e2a\u8de8\u9886\u57df\u7684\u7efc\u5408\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u7cfb\u7edf\u5730\u8bc4\u4f30\u9a8c\u8bc1\u5668\u3002\u8be5\u57fa\u51c6\u5305\u542b4000\u4e2a\u6db5\u76d6\u6570\u5b66\u3001\u7269\u7406\u3001\u5316\u5b66\u548c\u751f\u7269\u5b66\u9886\u57df\u4e13\u5bb6\u7ea7\u522b\u7684\u95ee\u9898\uff0c\u5e76\u914d\u6709\u53c2\u8003\u7b54\u6848\u548c\u591a\u6837\u5316\u7684\u6a21\u578b\u751f\u6210\u56de\u7b54\u3002\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u56db\u7ef4\u5ea6\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u6bd4\u8f83\u4e86\u4e13\u95e8\u9a8c\u8bc1\u5668\u548c\u901a\u7528LLM\u5728\u63d0\u53d6\u7b54\u6848\u4e0e\u5b8c\u6574\u56de\u7b54\u3001\u77ed\u8f93\u51fa\u4e0e\u957f\u8f93\u51fa\u7b49\u7ec4\u5408\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\u8fb9\u754c\u3002", "result": "VerifyBench\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u9a8c\u8bc1\u5668\u65b9\u9762\u7684\u57fa\u672c\u6743\u8861\uff1a\u4e13\u95e8\u9a8c\u8bc1\u5668\u867d\u7136\u51c6\u786e\u7387\u9886\u5148\uff0c\u4f46\u5728\u53ec\u56de\u7387\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff1b\u901a\u7528\u6a21\u578b\u5219\u663e\u793a\u51fa\u66f4\u5f3a\u7684\u5305\u5bb9\u6027\uff0c\u4f46\u7cbe\u786e\u5ea6\u4e0d\u7a33\u5b9a\u3002\u66f4\u91cd\u8981\u7684\u662f\uff0c\u7814\u7a76\u53d1\u73b0\u9a8c\u8bc1\u5668\u5bf9\u8f93\u5165\u7ed3\u6784\u7684\u654f\u611f\u6027\u4ee5\u53ca\u8de8\u9886\u57df\u6cdb\u5316\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u4e3a\u4e86\u89e3\u5f53\u524d\u9a8c\u8bc1\u5668\u6280\u672f\u7684\u74f6\u9888\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a8c\u8bc1\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u53ec\u56de\u7387\u3001\u6cdb\u5316\u80fd\u529b\u4ee5\u53ca\u5bf9\u8f93\u5165\u7ed3\u6784\u7684\u654f\u611f\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u4e13\u95e8\u7684\u9a8c\u8bc1\u5668\u867d\u7136\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u53ec\u56de\u7387\u4e0d\u8db3\uff1b\u800c\u901a\u7528\u7684LLM\u5219\u66f4\u5177\u5305\u5bb9\u6027\uff0c\u4f46\u7cbe\u786e\u5ea6\u4e0d\u7a33\u5b9a\u3002\u9a8c\u8bc1\u5668\u5bf9\u8f93\u5165\u7ed3\u6784\u7684\u9ad8\u5ea6\u654f\u611f\u6027\u548c\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u6307\u51fa\u4e86\u5f53\u524d\u9a8c\u8bc1\u5668\u6280\u672f\u7684\u74f6\u9888\u3002"}}
{"id": "2507.09041", "categories": ["cs.LG", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09041", "abs": "https://arxiv.org/abs/2507.09041", "authors": ["Andrew Wagenmaker", "Zhiyuan Zhou", "Sergey Levine"], "title": "Behavioral Exploration: Learning to Explore via In-Context Adaptation", "comment": null, "summary": "Developing autonomous agents that quickly explore an environment and adapt\ntheir behavior online is a canonical challenge in robotics and machine\nlearning. While humans are able to achieve such fast online exploration and\nadaptation, often acquiring new information and skills in only a handful of\ninteractions, existing algorithmic approaches tend to rely on random\nexploration and slow, gradient-based behavior updates. How can we endow\nautonomous agents with such capabilities on par with humans? Taking inspiration\nfrom recent progress on both in-context learning and large-scale behavioral\ncloning, in this work we propose behavioral exploration: training agents to\ninternalize what it means to explore and adapt in-context over the space of\n``expert'' behaviors. To achieve this, given access to a dataset of expert\ndemonstrations, we train a long-context generative model to predict expert\nactions conditioned on a context of past observations and a measure of how\n``exploratory'' the expert's behaviors are relative to this context. This\nenables the model to not only mimic the behavior of an expert, but also, by\nfeeding its past history of interactions into its context, to select different\nexpert behaviors than what have been previously selected, thereby allowing for\nfast online adaptation and targeted, ``expert-like'' exploration. We\ndemonstrate the effectiveness of our method in both simulated locomotion and\nmanipulation settings, as well as on real-world robotic manipulation tasks,\nillustrating its ability to learn adaptive, exploratory behavior.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u884c\u4e3a\u63a2\u7d22\u201d\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u578b\u6765\u6a21\u4eff\u548c\u9884\u6d4b\u4e13\u5bb6\u884c\u4e3a\uff0c\u4ece\u800c\u4f7f\u81ea\u4e3b\u4ee3\u7406\u80fd\u591f\u50cf\u4eba\u7c7b\u4e00\u6837\u5feb\u901f\u5730\u8fdb\u884c\u5728\u7ebf\u63a2\u7d22\u548c\u884c\u4e3a\u9002\u5e94\u3002\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6210\u529f\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u673a\u5668\u4eba\u548c\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u4e00\u4e2a\u7ecf\u5178\u6311\u6218\uff1a\u5f00\u53d1\u80fd\u591f\u5feb\u901f\u63a2\u7d22\u73af\u5883\u5e76\u5728\u5728\u7ebf\u73af\u5883\u4e2d\u9002\u5e94\u5176\u884c\u4e3a\u7684\u81ea\u4e3b\u4ee3\u7406\u3002\u4e0e\u4f9d\u8d56\u968f\u673a\u63a2\u7d22\u548c\u7f13\u6162\u68af\u5ea6\u66f4\u65b0\u7684\u73b0\u6709\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u65e8\u5728\u4f7f\u81ea\u4e3b\u4ee3\u7406\u5177\u5907\u4e0e\u4eba\u7c7b\u76f8\u5f53\u7684\u5728\u7ebf\u63a2\u7d22\u548c\u9002\u5e94\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u884c\u4e3a\u63a2\u7d22\u201d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u957f\u4e0a\u4e0b\u6587\u751f\u6210\u6a21\u578b\u6765\u9884\u6d4b\u4e13\u5bb6\u7684\u884c\u4e3a\u3002\u8be5\u6a21\u578b\u6839\u636e\u8fc7\u53bb\u7684\u89c2\u5bdf\u548c\u4e13\u5bb6\u884c\u4e3a\u76f8\u5bf9\u4e8e\u8be5\u4e0a\u4e0b\u6587\u7684\u201c\u63a2\u7d22\u6027\u201d\u7a0b\u5ea6\u8fdb\u884c\u6761\u4ef6\u9884\u6d4b\u3002\u6a21\u578b\u901a\u8fc7\u5c06\u8fc7\u53bb\u7684\u4ea4\u4e92\u5386\u53f2\u8f93\u5165\u5176\u4e0a\u4e0b\u6587\uff0c\u4e0d\u4ec5\u53ef\u4ee5\u6a21\u4eff\u4e13\u5bb6\u7684\u884c\u4e3a\uff0c\u8fd8\u53ef\u4ee5\u9009\u62e9\u4e0e\u5148\u524d\u4e0d\u540c\u7684\u4e13\u5bb6\u884c\u4e3a\uff0c\u4ece\u800c\u5b9e\u73b0\u5feb\u901f\u7684\u5728\u7ebf\u9002\u5e94\u548c\u6709\u9488\u5bf9\u6027\u7684\u201c\u4e13\u5bb6\u5f0f\u201d\u63a2\u7d22\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u5feb\u901f\u7684\u5728\u7ebf\u9002\u5e94\u548c\u6709\u9488\u5bf9\u6027\u7684\u201c\u4e13\u5bb6\u5f0f\u201d\u63a2\u7d22\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u7684\u8fd0\u52a8\u548c\u64cd\u7eb5\u8bbe\u7f6e\u4ee5\u53ca\u771f\u5b9e\u7684\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u8bf4\u660e\u4e86\u5176\u5b66\u4e60\u9002\u5e94\u6027\u3001\u63a2\u7d22\u6027\u884c\u4e3a\u7684\u80fd\u529b\u3002"}}
{"id": "2507.09858", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09858", "abs": "https://arxiv.org/abs/2507.09858", "authors": ["Shuaikang Wang", "Tiecheng Guo", "Meng Guo"], "title": "Customize Harmonic Potential Fields via Hybrid Optimization over Homotopic Paths", "comment": "accepted to IEEE RA-L", "summary": "Safe navigation within a workspace is a fundamental skill for autonomous\nrobots to accomplish more complex tasks. Harmonic potentials are artificial\npotential fields that are analytical, globally convergent and provably free of\nlocal minima. Thus, it has been widely used for generating safe and reliable\nrobot navigation control policies. However, most existing methods do not allow\ncustomization of the harmonic potential fields nor the resulting paths,\nparticularly regarding their topological properties. In this paper, we propose\na novel method that automatically finds homotopy classes of paths that can be\ngenerated by valid harmonic potential fields. The considered complex workspaces\ncan be as general as forest worlds consisting of numerous overlapping\nstar-obstacles. The method is based on a hybrid optimization algorithm that\nsearches over homotopy classes, selects the structure of each tree-of-stars\nwithin the forest, and optimizes over the continuous weight parameters for each\npurged tree via the projected gradient descent. The key insight is to transform\nthe forest world to the unbounded point world via proper diffeomorphic\ntransformations. It not only facilitates a simpler design of the\nmulti-directional D-signature between non-homotopic paths, but also retain the\nsafety and convergence properties. Extensive simulations and hardware\nexperiments are conducted for non-trivial scenarios, where the navigation\npotentials are customized for desired homotopic properties. Project page:\nhttps://shuaikang-wang.github.io/CustFields.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8c10\u6ce2\u52bf\u573a\u65b9\u6cd5\uff0c\u53ef\u81ea\u52a8\u5bfb\u627e\u8def\u5f84\u7684\u540c\u4f26\u7c7b\uff0c\u5e76\u5141\u8bb8\u81ea\u5b9a\u4e49\u590d\u6742\u5de5\u4f5c\u7a7a\u95f4\uff08\u5982\u68ee\u6797\u4e16\u754c\uff09\u4e2d\u8def\u5f84\u7684\u62d3\u6251\u6027\u8d28\u3002", "motivation": "\u73b0\u6709\u8c10\u6ce2\u52bf\u573a\u65b9\u6cd5\u5728\u8def\u5f84\u81ea\u5b9a\u4e49\u548c\u62d3\u6251\u6027\u8d28\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u96be\u4ee5\u5e94\u5bf9\u5305\u542b\u590d\u6742\u91cd\u53e0\u969c\u788d\u7269\u7684\u5bfc\u822a\u573a\u666f\u3002", "method": "\u8be5\u65b9\u6cd5\u57fa\u4e8e\u6df7\u5408\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u540c\u4f26\u7c7b\u4e2d\u8fdb\u884c\u641c\u7d22\uff0c\u9009\u62e9\u6bcf\u4e2a\u68ee\u6797\u5185\u7684\u661f\u5f62\u6811\u7ed3\u6784\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u8fde\u7eed\u6743\u91cd\u53c2\u6570\u3002\u5173\u952e\u5728\u4e8e\u901a\u8fc7\u9002\u5f53\u7684\u5fae\u5206\u540c\u80da\u53d8\u6362\u5c06\u68ee\u6797\u4e16\u754c\u8f6c\u5316\u4e3a\u65e0\u754c\u70b9\u4e16\u754c\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u975e\u540c\u4f26\u8def\u5f84\u4e4b\u95f4\u7684\u591a\u65b9\u5411D\u7b7e\u540d\u7684\u8bbe\u8ba1\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5b89\u5168\u6027\u548c\u6536\u655b\u6027\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u6a21\u62df\u548c\u786c\u4ef6\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u975e\u5e73\u51e1\u573a\u666f\u4e0b\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u4e3a\u671f\u671b\u7684\u540c\u4f26\u7279\u6027\u5b9a\u5236\u5bfc\u822a\u52bf\u573a\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u4e3a\u590d\u6742\u7684\u5305\u542b\u5927\u91cf\u91cd\u53e0\u661f\u5f62\u969c\u788d\u7269\u7684\u68ee\u6797\u4e16\u754c\u751f\u6210\u8def\u5f84\uff0c\u5e76\u5141\u8bb8\u81ea\u5b9a\u4e49\u8def\u5f84\u7684\u62d3\u6251\u6027\u8d28\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5b89\u5168\u6027\u548c\u6536\u655b\u6027\u3002"}}
{"id": "2507.09200", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09200", "abs": "https://arxiv.org/abs/2507.09200", "authors": ["Trong-Thuan Nguyen", "Pha Nguyen", "Jackson Cothren", "Alper Yilmaz", "Minh-Triet Tran", "Khoa Luu"], "title": "THYME: Temporal Hierarchical-Cyclic Interactivity Modeling for Video Scene Graphs in Aerial Footage", "comment": null, "summary": "The rapid proliferation of video in applications such as autonomous driving,\nsurveillance, and sports analytics necessitates robust methods for dynamic\nscene understanding. Despite advances in static scene graph generation and\nearly attempts at video scene graph generation, previous methods often suffer\nfrom fragmented representations, failing to capture fine-grained spatial\ndetails and long-range temporal dependencies simultaneously. To address these\nlimitations, we introduce the Temporal Hierarchical Cyclic Scene Graph (THYME)\napproach, which synergistically integrates hierarchical feature aggregation\nwith cyclic temporal refinement to address these limitations. In particular,\nTHYME effectively models multi-scale spatial context and enforces temporal\nconsistency across frames, yielding more accurate and coherent scene graphs. In\naddition, we present AeroEye-v1.0, a novel aerial video dataset enriched with\nfive types of interactivity that overcome the constraints of existing datasets\nand provide a comprehensive benchmark for dynamic scene graph generation.\nEmpirically, extensive experiments on ASPIRe and AeroEye-v1.0 demonstrate that\nthe proposed THYME approach outperforms state-of-the-art methods, offering\nimproved scene understanding in ground-view and aerial scenarios.", "AI": {"tldr": "\u63d0\u51faTHYME\u65b9\u6cd5\u548cAeroEye-v1.0\u6570\u636e\u96c6\u4ee5\u6539\u8fdb\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u3001\u76d1\u63a7\u548c\u4f53\u80b2\u5206\u6790\u7b49\u5e94\u7528\u4e2d\u5bf9\u52a8\u6001\u573a\u666f\u7406\u89e3\u7684\u8feb\u5207\u9700\u6c42\uff0c\u5e76\u514b\u670d\u73b0\u6709\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u5728\u8868\u793a\u788e\u7247\u5316\u3001\u7a7a\u95f4\u7ec6\u8282\u548c\u65f6\u95f4\u4f9d\u8d56\u6027\u6355\u6349\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTHYME\uff08Temporal Hierarchical Cyclic Scene Graph\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u5206\u5c42\u7279\u5f81\u805a\u5408\u548c\u5faa\u73af\u65f6\u95f4\u7ec6\u5316\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u89c6\u9891\u573a\u666f\u56fe\u751f\u6210\u65b9\u6cd5\u5728\u6355\u6349\u7ec6\u7c92\u5ea6\u7a7a\u95f4\u7ec6\u8282\u548c\u957f\u65f6\u5e8f\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u540c\u65f6\uff0c\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u822a\u7a7a\u89c6\u9891\u6570\u636e\u96c6AeroEye-v1.0\uff0c\u5305\u542b\u4e94\u79cd\u4ea4\u4e92\u7c7b\u578b\u3002", "result": "THYME\u65b9\u6cd5\u5728ASPIRe\u548cAeroEye-v1.0\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "THYME\u65b9\u6cd5\u5728\u5730\u9762\u548c\u7a7a\u4e2d\u573a\u666f\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u573a\u666f\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2507.09684", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09684", "abs": "https://arxiv.org/abs/2507.09684", "authors": ["J\u00e9r\u00e9mie Boudreault", "Ross Shillito", "Jean-Baptiste Bertrand", "Baptiste Royer"], "title": "Using a Kerr interaction for GKP magic state preparation", "comment": null, "summary": "Magic state distillation and injection is a promising strategy towards\nuniversal fault tolerant quantum computation, especially in architectures based\non the bosonic Gottesman-Kitaev-Preskill (GKP) codes where non-Clifford gates\nremain challenging to implement. Here we address GKP magic state preparation by\nstudying a non-Gaussian unitary mediated by a Kerr interaction which realizes a\nlogical gate $\\sqrt{H}_L$ for square GKP codes. This gate does not directly\ninvolve an auxiliary qubit and is compatible with finite energy constraints on\nthe code. Fidelity can be further enhanced using the small-Big-small (SBS)\nerror correction protocol and post-selection, making the scheme robust against\na single photon loss event. We finally propose a circuit QED implementation to\noperate the Kerr interaction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u514b\u5c14\u76f8\u4e92\u4f5c\u7528\u5236\u5907GKP\u7801\u903b\u8f91\u95e8 $\\sqrt{H}_L$ \u7684\u65b0\u65b9\u6cd5\uff0c\u53ef\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u7535\u8defQED\u5b9e\u73b0\u65b9\u6848\u3002", "motivation": "\u5728\u57fa\u4e8eGKP\u7801\u7684\u91cf\u5b50\u8ba1\u7b97\u67b6\u6784\u4e2d\uff0c\u975e\u7ecf\u5178\u6001\uff08magic states\uff09\u7684\u5236\u5907\u548c\u6ce8\u5165\u662f\u5b9e\u73b0\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u5173\u952e\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u96be\u4ee5\u5b9e\u73b0\u975e\u7ecf\u5178\u95e8\u7684GKP\u7801\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3GKP\u7801\u7684magic state\u5236\u5907\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u4e00\u79cd\u5229\u7528\u514b\u5c14\u76f8\u4e92\u4f5c\u7528\u5b9e\u73b0\u5e73\u65b9GKP\u7801\u903b\u8f91\u95e8 $\\sqrt{H}_L$ \u7684\u975e\u9ad8\u65af\u5e7a\u6b63\u64cd\u4f5c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408SBS\u7ea0\u9519\u534f\u8bae\u548c\u540e\u9009\u62e9\u7684\u4fdd\u771f\u5ea6\u63d0\u5347\u65b9\u6848\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7535\u8defQED\u7684\u5b9e\u73b0\u3002", "result": "\u6210\u529f\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65e0\u9700\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u4e14\u6ee1\u8db3\u6709\u9650\u80fd\u91cf\u7ea6\u675f\u7684\u903b\u8f91\u95e8 $\\sqrt{H}_L$ \u5236\u5907\u65b9\u6848\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u63d0\u5347\u4fdd\u771f\u5ea6\u548c\u9c81\u68d2\u6027\u7684\u7ea0\u9519\u4e0e\u540e\u9009\u62e9\u65b9\u6cd5\uff0c\u4ee5\u53ca\u5177\u4f53\u7684\u7535\u8defQED\u5b9e\u73b0\u65b9\u5f0f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u514b\u5c14\u76f8\u4e92\u4f5c\u7528\u7684\u975e\u9ad8\u65af\u5e7a\u6b63\u64cd\u4f5c\uff0c\u7528\u4e8e\u5236\u5907\u5e73\u65b9GKP\u7801\u7684\u903b\u8f91\u95e8 $\\sqrt{H}_L$\uff0c\u65e0\u9700\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u4e14\u6ee1\u8db3\u6709\u9650\u80fd\u91cf\u7ea6\u675f\u3002\u7ed3\u5408SBS\u7ea0\u9519\u534f\u8bae\u548c\u540e\u9009\u62e9\uff0c\u53ef\u4ee5\u63d0\u9ad8\u4fdd\u771f\u5ea6\u5e76\u62b5\u6297\u5355\u5149\u5b50\u635f\u8017\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u7535\u8defQED\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2507.09482", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.09482", "abs": "https://arxiv.org/abs/2507.09482", "authors": ["Changli Wang", "Rui Wu", "Fang Yin"], "title": "ViSP: A PPO-Driven Framework for Sarcasm Generation with Contrastive Learning", "comment": null, "summary": "Human emotions are complex, with sarcasm being a subtle and distinctive form.\nDespite progress in sarcasm research, sarcasm generation remains underexplored,\nprimarily due to the overreliance on textual modalities and the neglect of\nvisual cues, as well as the mismatch between image content and sarcastic intent\nin existing datasets. In this paper, we introduce M2SaG, a multimodal sarcasm\ngeneration dataset with 4,970 samples, each containing an image, a sarcastic\ntext, and a sarcasm target. To benchmark M2SaG, we propose ViSP, a generation\nframework that integrates Proximal Policy Optimization (PPO) and contrastive\nlearning. PPO utilizes reward scores from DIP to steer the generation of\nsarcastic texts, while contrastive learning encourages the model to favor\noutputs with higher reward scores. These strategies improve overall generation\nquality and produce texts with more pronounced sarcastic intent. We evaluate\nViSP across five metric sets and find it surpasses all baselines, including\nlarge language models, underscoring their limitations in sarcasm generation.\nFurthermore, we analyze the distributions of Sarcasm Scores and Factual\nIncongruity for both M2SaG and the texts generated by ViSP. The generated texts\nexhibit higher mean Sarcasm Scores (0.898 vs. 0.770) and Factual Incongruity\n(0.768 vs. 0.739), demonstrating that ViSP produces higher-quality sarcastic\ncontent than the original dataset. % The dataset and code will be publicly\navailable. Our dataset and code will be released at\n\\textit{https://github.com/wclapply/ViSP}.", "AI": {"tldr": "M2SaG\u662f\u4e00\u4e2a\u5305\u542b\u56fe\u50cf\u3001\u8bbd\u523a\u6587\u672c\u548c\u8bbd\u523a\u76ee\u6807\u7684\u591a\u6a21\u6001\u8bbd\u523a\u751f\u6210\u6570\u636e\u96c6\u3002ViSP\u662f\u4e00\u4e2a\u96c6\u6210\u4e86PPO\u548c\u5bf9\u6bd4\u5b66\u4e60\u7684\u751f\u6210\u6846\u67b6\uff0c\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5M2SaG\uff0c\u5e76\u80fd\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u3001\u66f4\u5177\u8bbd\u523a\u610f\u5473\u7684\u6587\u672c\u3002", "motivation": "\u73b0\u6709\u7684\u8bbd\u523a\u751f\u6210\u7814\u7a76\u8fc7\u5ea6\u4f9d\u8d56\u6587\u672c\u6a21\u5f0f\uff0c\u5ffd\u7565\u4e86\u89c6\u89c9\u7ebf\u7d22\uff0c\u5e76\u4e14\u5728\u73b0\u6709\u6570\u636e\u96c6\u4e2d\uff0c\u56fe\u50cf\u5185\u5bb9\u4e0e\u8bbd\u523a\u610f\u56fe\u5b58\u5728\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aViSP\u7684\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u548c\u5bf9\u6bd4\u5b66\u4e60\u3002PPO\u5229\u7528DIP\u7684\u5956\u52b1\u5206\u6570\u6765\u6307\u5bfc\u8bbd\u523a\u6587\u672c\u7684\u751f\u6210\uff0c\u800c\u5bf9\u6bd4\u5b66\u4e60\u5219\u9f13\u52b1\u6a21\u578b\u503e\u5411\u4e8e\u5177\u6709\u66f4\u9ad8\u5956\u52b1\u5206\u6570\u7684\u8f93\u51fa\u3002", "result": "\u5728\u4e94\u4e2a\u6307\u6807\u96c6\u4e0a\u8bc4\u4f30\u4e86ViSP\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u8bbd\u523a\u751f\u6210\u65b9\u9762\u4f18\u4e8e\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u6240\u6709\u57fa\u7ebf\u6a21\u578b\u3002\u751f\u6210\u7684\u6587\u672c\u5177\u6709\u66f4\u9ad8\u7684\u5e73\u5747\u8bbd\u523a\u5206\u6570\u548c\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\u3002", "conclusion": "ViSP\u8d85\u8d8a\u4e86\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u6240\u6709\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8bbd\u523a\u751f\u6210\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002ViSP\u751f\u6210\u7684\u6587\u672c\u5177\u6709\u66f4\u9ad8\u7684\u5e73\u5747\u8bbd\u523a\u5206\u6570\uff080.898 vs. 0.770\uff09\u548c\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u6027\uff080.768 vs. 0.739\uff09\uff0c\u8868\u660e\u5176\u4ea7\u751f\u7684\u8bbd\u523a\u5185\u5bb9\u8d28\u91cf\u4f18\u4e8e\u539f\u59cb\u6570\u636e\u96c6\u3002"}}
{"id": "2507.09887", "categories": ["cs.LG", "cs.AI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.09887", "abs": "https://arxiv.org/abs/2507.09887", "authors": ["Huynh Nguyen Dang", "Thang Pham", "Ngan Le", "Van Nguyen"], "title": "TolerantECG: A Foundation Model for Imperfect Electrocardiogram", "comment": "10 pages, 6 figures. Accepted to ACM Multimedia 2025", "summary": "The electrocardiogram (ECG) is an essential and effective tool for diagnosing\nheart diseases. However, its effectiveness can be compromised by noise or\nunavailability of one or more leads of the standard 12-lead recordings,\nresulting in diagnostic errors or uncertainty. To address these challenges, we\npropose TolerantECG, a foundation model for ECG signals that is robust to noise\nand capable of functioning with arbitrary subsets of the standard 12-lead ECG.\nTolerantECG training combines contrastive and self-supervised learning\nframeworks to jointly learn ECG signal representations alongside their\ncorresponding knowledge-retrieval-based text report descriptions and corrupted\nor lead-missing signals. Comprehensive benchmarking results demonstrate that\nTolerantECG consistently ranks as the best or second-best performer across\nvarious ECG signal conditions and class levels in the PTB-XL dataset, and\nachieves the highest performance on the MIT-BIH Arrhythmia Database.", "AI": {"tldr": "TolerantECG \u662f\u4e00\u4e2a\u5bf9\u566a\u58f0\u548c\u5bfc\u8054\u7f3a\u5931\u5177\u6709\u9c81\u68d2\u6027\u7684 ECG \u57fa\u7840\u6a21\u578b\uff0c\u53ef\u7528\u4e8e\u5fc3\u810f\u75c5\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3 ECG \u4fe1\u53f7\u5728\u566a\u58f0\u5e72\u6270\u6216\u90e8\u5206\u5bfc\u8054\u7f3a\u5931\u65f6\u53ef\u80fd\u5bfc\u81f4\u8bca\u65ad\u9519\u8bef\u6216\u4e0d\u786e\u5b9a\u6027\u7684\u95ee\u9898\uff0c\u63d0\u51fa TolerantECG \u6a21\u578b\u3002", "method": "TolerantECG \u7ed3\u5408\u4e86\u5bf9\u6bd4\u5b66\u4e60\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u540c\u65f6\u5b66\u4e60 ECG \u4fe1\u53f7\u8868\u793a\u3001\u6587\u672c\u62a5\u544a\u63cf\u8ff0\u4ee5\u53ca\u635f\u574f\u6216\u7f3a\u5931\u5bfc\u8054\u7684\u4fe1\u53f7\u3002", "result": "\u5728 PTB-XL \u6570\u636e\u96c6\u4e0a\uff0cTolerantECG \u5728\u5404\u79cd ECG \u4fe1\u53f7\u6761\u4ef6\u4e0b\u548c\u7c7b\u522b\u7ea7\u522b\u4e0a\u59cb\u7ec8\u6392\u540d\u7b2c\u4e00\u6216\u7b2c\u4e8c\u3002\u5728 MIT-BIH \u5fc3\u5f8b\u5931\u5e38\u6570\u636e\u5e93\u4e0a\uff0cTolerantECG \u8fbe\u5230\u4e86\u6700\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "TolerantECG \u6a21\u578b\u5728 PTB-XL \u548c MIT-BIH \u5fc3\u5f8b\u5931\u5e38\u6570\u636e\u5e93\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u5e94\u5bf9\u566a\u58f0\u548c\u5bfc\u8054\u7f3a\u5931\u7684\u6311\u6218\uff0c\u4e3a ECG \u4fe1\u53f7\u5206\u6790\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.08913", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08913", "abs": "https://arxiv.org/abs/2507.08913", "authors": ["Qi He", "Peiran Yu", "Ziyi Chen", "Heng Huang"], "title": "Revisiting Convergence: Shuffling Complexity Beyond Lipschitz Smoothness", "comment": null, "summary": "Shuffling-type gradient methods are favored in practice for their simplicity\nand rapid empirical performance. Despite extensive development of convergence\nguarantees under various assumptions in recent years, most require the\nLipschitz smoothness condition, which is often not met in common machine\nlearning models. We highlight this issue with specific counterexamples. To\naddress this gap, we revisit the convergence rates of shuffling-type gradient\nmethods without assuming Lipschitz smoothness. Using our stepsize strategy, the\nshuffling-type gradient algorithm not only converges under weaker assumptions\nbut also match the current best-known convergence rates, thereby broadening its\napplicability. We prove the convergence rates for nonconvex, strongly convex,\nand non-strongly convex cases, each under both random reshuffling and arbitrary\nshuffling schemes, under a general bounded variance condition. Numerical\nexperiments further validate the performance of our shuffling-type gradient\nalgorithm, underscoring its practical efficacy.", "AI": {"tldr": "\u5728\u4e0d\u9700\u8981Lipschitz\u5e73\u6ed1\u6761\u4ef6\u7684\u60c5\u51b5\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b65\u957f\u7b56\u7565\uff0c\u6539\u8fdb\u4e86shuffling\u578b\u68af\u5ea6\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5728\u975e\u51f8\u3001\u5f3a\u51f8\u548c\u975e\u5f3a\u51f8\u95ee\u9898\u4e0a\u90fd\u8fbe\u5230\u4e86\u6700\u4f73\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709shuffling\u578b\u68af\u5ea6\u65b9\u6cd5\u7684\u6536\u655b\u6027\u4fdd\u8bc1\u5927\u591a\u4f9d\u8d56\u4e8eLipschitz\u5e73\u6ed1\u6761\u4ef6\uff0c\u800c\u8be5\u6761\u4ef6\u5728\u5e38\u89c1\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u5e38\u5e38\u4e0d\u6ee1\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u53bb\u9664Lipschitz\u5e73\u6ed1\u7684\u5047\u8bbe\uff0c\u7814\u7a76\u5728\u66f4\u5f31\u7684\u6761\u4ef6\u4e0bshuffling\u578b\u68af\u5ea6\u65b9\u6cd5\u7684\u6536\u655b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b65\u957f\u7b56\u7565\uff0c\u5e76\u5728\u6b64\u7b56\u7565\u4e0b\u5206\u6790\u4e86shuffling\u578b\u68af\u5ea6\u7b97\u6cd5\u5728\u4e0d\u540c\u51f8\u6027\uff08\u975e\u51f8\u3001\u5f3a\u51f8\u3001\u975e\u5f3a\u51f8\uff09\u548c\u4e0d\u540c\u91cd\u6392\u65b9\u5f0f\uff08\u968f\u673a\u91cd\u6392\u3001\u4efb\u610f\u91cd\u6392\uff09\u4e0b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5728\u4e00\u822c\u6709\u754c\u65b9\u5dee\u6761\u4ef6\u4e0b\u7b97\u6cd5\u7684\u6536\u655b\u6027\u3002", "result": "\u5728\u4e00\u822c\u6709\u754c\u65b9\u5dee\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684shuffling\u578b\u68af\u5ea6\u7b97\u6cd5\u5728\u975e\u51f8\u3001\u5f3a\u51f8\u548c\u975e\u5f3a\u51f8\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u4e14\u4e0e\u73b0\u6709\u6700\u4f73\u6536\u655b\u901f\u5ea6\u76f8\u5f53\u3002\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u5b9e\u8df5\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5e26\u6b65\u957f\u7b56\u7565\u7684shuffling\u578b\u68af\u5ea6\u7b97\u6cd5\u5728\u66f4\u5bbd\u677e\u7684\u5047\u8bbe\u4e0b\uff08\u65e0\u9700Lipschitz\u5e73\u6ed1\u6761\u4ef6\uff09\u6536\u655b\uff0c\u5e76\u4e14\u8fbe\u5230\u4e86\u5df2\u77e5\u7684\u6700\u4f73\u6536\u655b\u901f\u5ea6\uff0c\u62d3\u5bbd\u4e86\u5176\u9002\u7528\u8303\u56f4\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u5728\u975e\u51f8\u3001\u5f3a\u51f8\u548c\u975e\u5f3a\u51f8\u60c5\u51b5\u4e0b\u7684\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u9488\u5bf9\u968f\u673a\u91cd\u6392\u548c\u4efb\u610f\u91cd\u6392\u4e24\u79cd\u60c5\u51b5\uff0c\u5728\u4e00\u822c\u6709\u754c\u65b9\u5dee\u6761\u4ef6\u4e0b\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\u3002\u6570\u503c\u5b9e\u9a8c\u4e5f\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09955", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09955", "abs": "https://arxiv.org/abs/2507.09955", "authors": ["Luolin Xiong", "Haofen Wang", "Xi Chen", "Lu Sheng", "Yun Xiong", "Jingping Liu", "Yanghua Xiao", "Huajun Chen", "Qing-Long Han", "Yang Tang"], "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models", "comment": null, "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning.", "AI": {"tldr": "DeepSeek\u53d1\u5e03\u4e86V3\u548cR1\u6a21\u578b\uff0c\u5177\u6709\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u548c\u5f00\u6e90\u7684\u7279\u70b9\u3002\u672c\u6587\u5206\u6790\u4e86\u5176MLA\u3001MoE\u3001MTP\u3001GRPO\u7b49\u65b0\u7b97\u6cd5\u3001\u5de5\u7a0b\u7a81\u7834\u4ee5\u53ca\u4e0e\u5176\u4ed6\u4e3b\u6d41\u5927\u6a21\u578b\u7684\u5bf9\u6bd4\uff0c\u5e76\u5c55\u671b\u4e86\u672a\u6765\u5927\u6a21\u578b\u7684\u53d1\u5c55\u8d8b\u52bf\u3002", "motivation": "\u4e3a\u4e86\u4ecb\u7ecdDeepSeek V3\u548cR1\u7cfb\u5217\u6a21\u578b\uff0c\u5206\u6790\u5176\u6280\u672f\u521b\u65b0\u3001\u5de5\u7a0b\u7a81\u7834\u4ee5\u53ca\u5728AI\u9886\u57df\u7684\u6f5c\u5728\u5f71\u54cd\u529b\u3002", "method": "\u901a\u8fc7\u56de\u987e\u5927\u6a21\u578b\u6f14\u5316\u3001\u5206\u6790DeepSeek\u6a21\u578b\u53ca\u5176\u65b0\u7b97\u6cd5\uff08MLA\u3001MoE\u3001MTP\u3001GRPO\uff09\u3001\u63a2\u8ba8\u5de5\u7a0b\u7a81\u7834\u4ee5\u53ca\u5bf9\u6bd4\u5206\u6790\u4e0e\u5176\u4ed6LLM\u6a21\u578b\u7684\u4f18\u52a3\uff0c\u6765\u9610\u8ff0\u5176\u7814\u7a76\u5185\u5bb9\u3002", "result": "DeepSeek\u6a21\u578b\u51ed\u501f\u5176\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u548c\u5f00\u6e90\u7684\u4f18\u52bf\uff0c\u5728\u5927\u6a21\u578b\u9886\u57df\u5f15\u8d77\u4e86\u5168\u7403\u5173\u6ce8\uff0c\u5e76\u5728\u7b97\u6cd5\u3001\u5de5\u7a0b\u548c\u5e94\u7528\u65b9\u9762\u5c55\u73b0\u51fa\u663e\u8457\u7684\u521b\u65b0\u3002", "conclusion": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86DeepSeek V3\u548cR1\u7cfb\u5217\u6a21\u578b\u7684\u521b\u65b0\u4e4b\u5904\uff0c\u5e76\u63a2\u8ba8\u4e86\u5b83\u4eec\u5bf9AI\u9886\u57df\u7ade\u4e89\u683c\u5c40\u7684\u5f71\u54cd\u4ee5\u53ca\u672a\u6765\u5927\u6a21\u578b\u53d1\u5c55\u7684\u8d8b\u52bf\u3002"}}
{"id": "2507.09061", "categories": ["cs.LG", "cs.SY", "eess.SY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09061", "abs": "https://arxiv.org/abs/2507.09061", "authors": ["Thomas T. Zhang", "Daniel Pfrommer", "Nikolai Matni", "Max Simchowitz"], "title": "Imitation Learning in Continuous Action Spaces: Mitigating Compounding Error without Interaction", "comment": null, "summary": "We study the problem of imitating an expert demonstrator in a continuous\nstate-and-action dynamical system. While imitation learning in discrete\nsettings such as autoregressive language modeling has seen immense success and\npopularity in recent years, imitation in physical settings such as autonomous\ndriving and robot learning has proven comparably more complex due to the\ncompounding errors problem, often requiring elaborate set-ups to perform\nstably. Recent work has demonstrated that even in benign settings, exponential\ncompounding errors are unavoidable when learning solely from expert-controlled\ntrajectories, suggesting the need for more advanced policy parameterizations or\ndata augmentation. To this end, we present minimal interventions that provably\nmitigate compounding errors in continuous state-and-action imitation learning.\nWhen the system is open-loop stable, we prescribe \"action chunking,\" i.e.,\npredicting and playing sequences of actions in open-loop; when the system is\npossibly unstable, we prescribe \"noise injection,\" i.e., adding noise during\nexpert demonstrations. These interventions align with popular choices in modern\nrobot learning, though the benefits we derive are distinct from the effects\nthey were designed to target. Our results draw insights and tools from both\ncontrol theory and reinforcement learning; however, our analysis reveals novel\nconsiderations that do not naturally arise when either literature is considered\nin isolation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u201c\u52a8\u4f5c\u5206\u5757\u201d\u548c\u201c\u566a\u58f0\u6ce8\u5165\u201d\u4e24\u79cd\u65b9\u6cd5\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u6a21\u4eff\u5b66\u4e60\u7684\u590d\u5408\u8bef\u5dee\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5b66\u4e60\u7684\u7a33\u5b9a\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u6a21\u4eff\u5b66\u4e60\u5728\u7269\u7406\u8bbe\u7f6e\uff08\u5982\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u5b66\u4e60\uff09\u4e2d\u6bd4\u5728\u79bb\u6563\u8bbe\u7f6e\uff08\u5982\u8bed\u8a00\u5efa\u6a21\uff09\u4e2d\u66f4\u5177\u6311\u6218\u6027\uff0c\u4e3b\u8981\u539f\u56e0\u662f\u590d\u5408\u8bef\u5dee\u95ee\u9898\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u7a33\u5b9a\u6267\u884c\u65b9\u9762\u9700\u8981\u590d\u6742\u7684\u8bbe\u7f6e\uff0c\u5e76\u4e14\u5373\u4f7f\u5728\u6709\u5229\u6761\u4ef6\u4e0b\uff0c\u4ec5\u4ece\u4e13\u5bb6\u8f68\u8ff9\u5b66\u4e60\u4e5f\u4f1a\u5bfc\u81f4\u6307\u6570\u7ea7\u590d\u5408\u8bef\u5dee\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u7b56\u7565\u53c2\u6570\u5316\u6216\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u6700\u5c0f\u5e72\u9884\u63aa\u65bd\u6765\u89e3\u51b3\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u590d\u5408\u8bef\u5dee\u95ee\u9898\uff1a\u201c\u52a8\u4f5c\u5206\u5757\u201d\uff08\u9884\u6d4b\u5e76\u5206\u6bb5\u6267\u884c\u52a8\u4f5c\uff09\u548c\u201c\u566a\u58f0\u6ce8\u5165\u201d\uff08\u5728\u4e13\u5bb6\u6f14\u793a\u4e2d\u52a0\u5165\u566a\u58f0\uff09\u3002\u7814\u7a76\u7ed3\u5408\u4e86\u63a7\u5236\u7406\u8bba\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u5de5\u5177\uff0c\u5bf9\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u8fdb\u884c\u4e86\u7406\u8bba\u5206\u6790\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "result": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u201c\u52a8\u4f5c\u5206\u5757\u201d\u5728\u7cfb\u7edf\u662f\u5f00\u73af\u7a33\u5b9a\u65f6\u53ef\u51cf\u5c11\u590d\u5408\u8bef\u5dee\uff1b\u800c\u201c\u566a\u58f0\u6ce8\u5165\u201d\u5219\u9002\u7528\u4e8e\u7cfb\u7edf\u4e0d\u7a33\u5b9a\u7684\u60c5\u51b5\u3002\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u4e0e\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u7684\u5e38\u7528\u65b9\u6cd5\u4e00\u81f4\uff0c\u4f46\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u89e3\u51b3\u590d\u5408\u8bef\u5dee\u65b9\u9762\u7684\u72ec\u7279\u4f18\u52bf\uff0c\u8fd9\u4e9b\u4f18\u52bf\u5728\u5355\u72ec\u8003\u8651\u63a7\u5236\u7406\u8bba\u6216\u5f3a\u5316\u5b66\u4e60\u65f6\u5e76\u4e0d\u660e\u663e\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6700\u5c0f\u5e72\u9884\u63aa\u65bd\u53ef\u6709\u6548\u7f13\u89e3\u8fde\u7eed\u72b6\u6001-\u52a8\u4f5c\u6a21\u4eff\u5b66\u4e60\u4e2d\u7684\u590d\u5408\u8bef\u5dee\u95ee\u9898\u3002\u901a\u8fc7\u5bf9\u201c\u52a8\u4f5c\u5206\u5757\u201d\u548c\u201c\u566a\u58f0\u6ce8\u5165\u201d\u8fd9\u4e24\u79cd\u5e72\u9884\u65b9\u5f0f\u7684\u7406\u8bba\u5206\u6790\u548c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0d\u540c\u7cfb\u7edf\u7a33\u5b9a\u6027\u4e0b\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u673a\u5668\u4eba\u5b66\u4e60\u9886\u57df\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2507.09985", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09985", "abs": "https://arxiv.org/abs/2507.09985", "authors": ["Samson Yu", "Kelvin Lin", "Harold Soh"], "title": "Demonstrating the Octopi-1.5 Visual-Tactile-Language Model", "comment": "Published at R:SS 2025", "summary": "Touch is recognized as a vital sense for humans and an equally important\nmodality for robots, especially for dexterous manipulation, material\nidentification, and scenarios involving visual occlusion. Building upon very\nrecent work in touch foundation models, this demonstration will feature\nOctopi-1.5, our latest visual-tactile-language model. Compared to its\npredecessor, Octopi-1.5 introduces the ability to process tactile signals from\nmultiple object parts and employs a simple retrieval-augmented generation (RAG)\nmodule to improve performance on tasks and potentially learn new objects\non-the-fly. The system can be experienced live through a new handheld\ntactile-enabled interface, the TMI, equipped with GelSight and TAC-02 tactile\nsensors. This convenient and accessible setup allows users to interact with\nOctopi-1.5 without requiring a robot. During the demonstration, we will\nshowcase Octopi-1.5 solving tactile inference tasks by leveraging tactile\ninputs and commonsense knowledge. For example, in a Guessing Game, Octopi-1.5\nwill identify objects being grasped and respond to follow-up queries about how\nto handle it (e.g., recommending careful handling for soft fruits). We also\nplan to demonstrate Octopi-1.5's RAG capabilities by teaching it new items.\nWith live interactions, this demonstration aims to highlight both the progress\nand limitations of VTLMs such as Octopi-1.5 and to foster further interest in\nthis exciting field. Code for Octopi-1.5 and design files for the TMI gripper\nare available at https://github.com/clear-nus/octopi-1.5.", "AI": {"tldr": "\u6f14\u793a\u4e86Octopi-1.5\uff0c\u4e00\u4e2a\u80fd\u5904\u7406\u591a\u90e8\u5206\u89e6\u89c9\u4fe1\u53f7\u5e76\u5229\u7528RAG\u5b66\u4e60\u65b0\u7269\u4f53\u7684\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\u3002\u901a\u8fc7\u624b\u6301\u8bbe\u5907\u53ef\u73b0\u573a\u4f53\u9a8c\u5176\u5728\u7269\u4f53\u8bc6\u522b\u548c\u5904\u7406\u5efa\u8bae\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f18\u52bf\u4e0e\u5c40\u9650\u6027\u3002", "motivation": "\u89e6\u89c9\u5bf9\u4eba\u7c7b\u548c\u673a\u5668\u4eba\u64cd\u4f5c\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7075\u5de7\u64cd\u63a7\u3001\u6750\u6599\u8bc6\u522b\u548c\u89c6\u89c9\u906e\u6321\u573a\u666f\u4e2d\u3002\u672c\u6f14\u793a\u57fa\u4e8e\u89e6\u89c9\u57fa\u7840\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u6700\u65b0\u7684\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00\u6a21\u578bOctopi-1.5\u3002", "method": "\u4f7f\u7528\u591a\u90e8\u5206\u89e6\u89c9\u4fe1\u53f7\u5904\u7406\u80fd\u529b\uff0c\u5e76\u96c6\u6210\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6a21\u5757\uff0c\u4ee5\u63d0\u5347\u4efb\u52a1\u6027\u80fd\u548c\u652f\u6301\u65b0\u7269\u4f53\u5373\u65f6\u5b66\u4e60\u3002\u901a\u8fc7\u624b\u6301\u5f0f\u89e6\u89c9\u63a5\u53e3TMI\uff08\u914d\u5907GelSight\u548cTAC-02\u89e6\u89c9\u4f20\u611f\u5668\uff09\u8fdb\u884c\u73b0\u573a\u4ea4\u4e92\u4f53\u9a8c\u3002", "result": "Octopi-1.5\u80fd\u591f\u901a\u8fc7\u89e6\u89c9\u8f93\u5165\u548c\u5e38\u8bc6\u77e5\u8bc6\u89e3\u51b3\u89e6\u89c9\u63a8\u7406\u4efb\u52a1\uff0c\u4f8b\u5982\u5728\u201c\u731c\u8c1c\u6e38\u620f\u201d\u4e2d\u8bc6\u522b\u88ab\u6293\u63e1\u7684\u7269\u4f53\uff0c\u5e76\u63d0\u4f9b\u5904\u7406\u5efa\u8bae\uff08\u5982\u5bf9\u8f6f\u6c34\u679c\u5efa\u8bae\u5c0f\u5fc3\u5904\u7406\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u6f14\u793a\u4e86\u5176RAG\u80fd\u529b\uff0c\u80fd\u591f\u5b66\u4e60\u65b0\u7269\u4f53\u3002", "conclusion": "\u672c\u6b21\u6f14\u793a\u65e8\u5728\u5c55\u793aOctopi-1.5\u5728\u89c6\u89c9-\u89e6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VTLM\uff09\u65b9\u9762\u7684\u8fdb\u5c55\u548c\u5c40\u9650\u6027\uff0c\u5e76\u6fc0\u53d1\u5bf9\u8be5\u9886\u57df\u7684\u5174\u8da3\u3002\u63d0\u4f9b\u7684\u4ee3\u7801\u548c\u8bbe\u8ba1\u6587\u4ef6\u4fbf\u4e8e\u793e\u533a\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2507.09207", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09207", "abs": "https://arxiv.org/abs/2507.09207", "authors": ["Alexander C. Ogren", "Berthy T. Feng", "Jihoon Ahn", "Katherine L. Bouman", "Chiara Daraio"], "title": "Visual Surface Wave Elastography: Revealing Subsurface Physical Properties via Visible Surface Waves", "comment": "ICCV 2025", "summary": "Wave propagation on the surface of a material contains information about\nphysical properties beneath its surface. We propose a method for inferring the\nthickness and stiffness of a structure from just a video of waves on its\nsurface. Our method works by extracting a dispersion relation from the video\nand then solving a physics-based optimization problem to find the best-fitting\nthickness and stiffness parameters. We validate our method on both simulated\nand real data, in both cases showing strong agreement with ground-truth\nmeasurements. Our technique provides a proof-of-concept for at-home health\nmonitoring of medically-informative tissue properties, and it is further\napplicable to fields such as human-computer interaction.", "AI": {"tldr": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5206\u6790\u8868\u9762\u6ce2\u89c6\u9891\u6765\u63a8\u65ad\u6750\u6599\u7684\u539a\u5ea6\u548c\u521a\u5ea6\uff0c\u53ef\u7528\u4e8e\u5065\u5eb7\u76d1\u6d4b\u548c\u4eba\u673a\u4ea4\u4e92\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u4ec5\u4ece\u8868\u9762\u6ce2\u7684\u89c6\u9891\u4e2d\u63a8\u65ad\u7ed3\u6784\u539a\u5ea6\u548c\u521a\u5ea6\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u63d0\u53d6\u89c6\u9891\u4e2d\u7684\u9891\u6563\u5173\u7cfb\uff0c\u7136\u540e\u89e3\u51b3\u57fa\u4e8e\u7269\u7406\u5b66\u7684\u4f18\u5316\u95ee\u9898\u6765\u5bfb\u627e\u6700\u4f73\u62df\u5408\u7684\u539a\u5ea6\u548c\u521a\u5ea6\u53c2\u6570\u3002", "result": "\u5728\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u4e0a\u90fd\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u7ed3\u679c\u4e0e\u5b9e\u9645\u6d4b\u91cf\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u6280\u672f\u4e3a\u5c45\u5bb6\u5065\u5eb7\u76d1\u6d4b\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\uff0c\u53ef\u7528\u4e8e\u76d1\u6d4b\u5177\u6709\u533b\u5b66\u4fe1\u606f\u610f\u4e49\u7684\u7ec4\u7ec7\u7279\u6027\uff0c\u8fd8\u53ef\u5e94\u7528\u4e8e\u4eba\u673a\u4ea4\u4e92\u7b49\u9886\u57df\u3002"}}
{"id": "2507.10451", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.10451", "abs": "https://arxiv.org/abs/2507.10451", "authors": ["Anurag Bajpai", "Barak Ratzker", "Shiv Shankar", "Dierk Raabe", "Yan Ma"], "title": "Sustainable Pre-reduction of Ferromanganese Oxides with Hydrogen: Heating Rate-Dependent Reduction Pathways and Microstructure Evolution", "comment": null, "summary": "The reduction of ferromanganese ores into metallic feedstock is an\nenergy-intensive process with substantial carbon emissions, necessitating\nsustainable alternatives. Hydrogen-based pre-reduction of manganese-rich ores\noffers a low-emission pathway to augment subsequent thermic Fe-Mn alloy\nproduction. However, reduction dynamics and microstructure evolution under\nvarying thermal conditions remain poorly understood. This study investigates\nthe influence of heating rate on the hydrogen-based direct reduction of natural\nNchwaning ferromanganese ore and a synthetic analog. Non-isothermal\nthermogravimetric analysis revealed a complex multistep reduction process with\noverlapping kinetic regimes. Isoconversional kinetic analysis showed increased\nactivation energy with reduction degree, indicating a transition from\nsurface-reaction to diffusion-controlled reduction mechanisms. Interrupted\nX-ray diffraction experiments suggested that slow heating enables complete\nconversion to MnO and metallic Fe, while rapid heating promotes Fe- and\nMn-oxides intermixing. Thermodynamic calculations for the Fe-Mn-O system\npredicted the equilibrium phase evolution, indicating Mn stabilized\nFe-containing spinel and halite phases. Microstructural analysis revealed that\nslow heating rate yields fine and dispersed Fe particles in a porous MnO\nmatrix, while fast heating leads to sporadic Fe-rich agglomerates. These\nfindings suggest heating rate as a critical parameter governing reduction\npathway, phase distribution, and microstructure evolution, thus offering key\ninsights for optimizing hydrogen-based pre-reduction strategies towards more\nefficient and sustainable ferromanganese production.", "AI": {"tldr": "\u52a0\u70ed\u901f\u7387\u5f71\u54cd\u9530\u77ff\u77f3\u7684\u6c22\u57fa\u8fd8\u539f\u8fc7\u7a0b\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u94c1\u9530\u77ff\u77f3\u5230\u91d1\u5c5e\u539f\u6599\u7684\u751f\u4ea7\u8fc7\u7a0b\u4e2d\u7684\u78b3\u6392\u653e\u548c\u80fd\u6e90\u6d88\u8017\uff0c\u9700\u8981\u5f00\u53d1\u53ef\u6301\u7eed\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u6c22\u57fa\u9884\u8fd8\u539f\u8fc7\u7a0b\u4e2d\u8fd8\u539f\u52a8\u529b\u5b66\u548c\u5fae\u89c2\u7ed3\u6784\u6f14\u53d8\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u70ed\u91cd\u5206\u6790\u3001X\u5c04\u7ebf\u884d\u5c04\u548c\u70ed\u529b\u5b66\u8ba1\u7b97\uff0c\u7814\u7a76\u4e86\u52a0\u70ed\u901f\u7387\u5bf9\u5929\u7136Nchwaning\u9530\u94c1\u77ff\u77f3\u548c\u5408\u6210\u7c7b\u4f3c\u7269\u8fdb\u884c\u6c22\u57fa\u76f4\u63a5\u8fd8\u539f\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6162\u901f\u52a0\u70ed\u53ef\u5b9e\u73b0\u5411MnO\u548c\u91d1\u5c5eFe\u7684\u5b8c\u5168\u8f6c\u5316\uff0c\u800c\u5feb\u901f\u52a0\u70ed\u5219\u4f1a\u4fc3\u8fdbFe\u548cMnO\u7684\u76f8\u4e92\u6df7\u5408\u3002\u6162\u901f\u52a0\u70ed\u53ef\u5f62\u6210\u591a\u5b54MnO\u57fa\u4f53\u4e2d\u5206\u6563\u7684\u7ec6\u5c0fFe\u9897\u7c92\uff0c\u800c\u5feb\u901f\u52a0\u70ed\u5219\u4f1a\u5bfc\u81f4\u96f6\u661f\u7684\u5bcc\u94c1\u805a\u96c6\u4f53\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u52a0\u70ed\u901f\u7387\u662f\u63a7\u5236\u9530\u77ff\u77f3\u5728\u6c22\u57fa\u9884\u8fd8\u539f\u8fc7\u7a0b\u4e2d\u8fd8\u539f\u8def\u5f84\u3001\u76f8\u5206\u5e03\u548c\u5fae\u89c2\u7ed3\u6784\u6f14\u53d8\u7684\u5173\u952e\u53c2\u6570\uff0c\u4e3a\u4f18\u5316\u7528\u4e8e\u66f4\u9ad8\u6548\u548c\u53ef\u6301\u7eed\u7684\u9530\u94c1\u751f\u4ea7\u7684\u6c22\u57fa\u9884\u8fd8\u539f\u7b56\u7565\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.09686", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09686", "abs": "https://arxiv.org/abs/2507.09686", "authors": ["Gal G. Shaviner", "Ziv Chen", "Steven H. Frankel"], "title": "Quantum Singular Value Transformation for Solving the Time-Dependent Maxwell's Equations", "comment": null, "summary": "This work presents a quantum algorithm for solving linear systems of\nequations of the form $\\mathbf{A}{\\frac{\\mathbf{\\partial f}}{\\mathbf{\\partial\nx}}} = \\mathbf{B}\\mathbf{f}$, based on the Quantum Singular Value\nTransformation (QSVT). The algorithm uses block-encoding of $A$ and applies an\n21st-degree polynomial approximation to the inverse function $f(x) = 1/x$,\nenabling relatively shallow quantum circuits implemented on 9 qubits, including\ntwo ancilla qubits, corresponding to a grid size of 128 points. Phase angles\nfor the QSVT circuit were optimized classically using the Adagrad\ngradient-based method over 100 iterations to minimize the solution cost. This\napproach was simulated in PennyLane and applied to solve a 1D benchmark case of\nMaxwell's equations in free space, with a Gaussian pulse as the initial\ncondition, where the quantum-computed solution showed high fidelity of more\nthan 99.9% when compared to the normalized classical solution. Results\ndemonstrate the potential of QSVT-based linear solvers on simulators with full\nquantum state access. However, practical hardware implementations face\nchallenges because accessing the complete quantum state is infeasible. This\nlimitation restricts applicability to cases where only $O({poly}(n))$\nobservables are needed. These findings highlight both the promise and current\nlimitations of using quantum algorithms, such as QSVT, to solve linear systems\nof equations, and they point to the need for the development of\nmeasurement-efficient algorithms for near-term quantum devices.", "AI": {"tldr": "\u57fa\u4e8eQSVT\u7684\u91cf\u5b50\u7b97\u6cd5\u80fd\u9ad8\u4fdd\u771f\u5730\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4\uff08\u5982\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\u7ec4\uff09\uff0c\u4f46\u5728\u5b9e\u9645\u786c\u4ef6\u4e0a\u56e0\u91cf\u5b50\u6001\u8bbf\u95ee\u9650\u5236\u800c\u9762\u4e34\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u6d4b\u91cf\u6548\u7387\u66f4\u9ad8\u7684\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4\u7684\u6311\u6218\uff0c\u7279\u522b\u662f\u6d89\u53ca\u590d\u6742\u65b9\u7a0b\uff08\u5982\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\u7ec4\uff09\u548c\u9700\u8981\u9ad8\u7cbe\u5ea6\u89e3\u7684\u573a\u666f\uff0c\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u4e86\u57fa\u4e8eQSVT\u7684\u91cf\u5b50\u7b97\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u5947\u5f02\u503c\u53d8\u6362\uff08QSVT\uff09\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u7528\u4e8e\u6c42\u89e3\u5f62\u5f0f\u4e3a $\\mathbf{A}{\\frac{\\partial f}{\\partial x}} = \\mathbf{B}\\mathbf{f}$ \u7684\u7ebf\u6027\u65b9\u7a0b\u7ec4\u3002\u8be5\u7b97\u6cd5\u5229\u7528A\u7684\u5757\u7f16\u7801\uff0c\u5e76\u5bf9\u51fd\u6570 $f(x) = 1/x$ \u7684\u9006\u51fd\u6570\u91c7\u752821\u6b21\u591a\u9879\u5f0f\u8fd1\u4f3c\u3002\u901a\u8fc7\u57289\u4e2a\u91cf\u5b50\u6bd4\u7279\uff08\u5305\u62ec2\u4e2a\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\uff09\u4e0a\u5b9e\u73b0\u8f83\u6d45\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u4f7f\u7528Adagrad\u68af\u5ea6\u4e0b\u964d\u65b9\u6cd5\u5bf9QSVT\u7535\u8def\u7684\u76f8\u4f4d\u89d2\u8fdb\u884c100\u6b21\u7ecf\u5178\u4f18\u5316\uff0c\u4ee5\u6700\u5c0f\u5316\u6c42\u89e3\u6210\u672c\u3002", "result": "\u5728\u6a21\u62df\u76841D\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\u7ec4\u6848\u4f8b\u4e2d\uff0c\u91cf\u5b50\u8ba1\u7b97\u7684\u89e3\u4e0e\u7ecf\u5178\u89e3\u76f8\u6bd4\uff0c\u4fdd\u771f\u5ea6\u8d85\u8fc799.9%\u3002\u8be5\u65b9\u6cd5\u5728\u80fd\u591f\u5b8c\u5168\u8bbf\u95ee\u91cf\u5b50\u6001\u7684\u6a21\u62df\u5668\u4e0a\u8868\u73b0\u826f\u597d\u3002", "conclusion": "QSVT\u5728\u6c42\u89e3\u7ebf\u6027\u65b9\u7a0b\u7ec4\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u6a21\u62df1D\u9ea6\u514b\u65af\u97e6\u65b9\u7a0b\u7ec4\u7684\u57fa\u51c6\u6848\u4f8b\u4e2d\u5b9e\u73b0\u4e86\u8d85\u8fc799.9%\u7684\u9ad8\u4fdd\u771f\u5ea6\u3002\u7136\u800c\uff0c\u5b9e\u9645\u786c\u4ef6\u5b9e\u73b0\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u65e0\u6cd5\u5b8c\u5168\u8bbf\u95ee\u91cf\u5b50\u6001\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u53ea\u9700\u8981\u591a\u9879\u5f0f\u6570\u91cf\u53ef\u89c2\u6d4b\u503c\u7684\u60c5\u51b5\u4e0b\u7684\u5e94\u7528\u3002\u672a\u6765\u7684\u7814\u7a76\u9700\u8981\u5f00\u53d1\u66f4\u9ad8\u6548\u7684\u6d4b\u91cf\u7b97\u6cd5\u4ee5\u9002\u5e94\u8fd1\u671f\u91cf\u5b50\u8bbe\u5907\u3002"}}
{"id": "2507.09485", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09485", "abs": "https://arxiv.org/abs/2507.09485", "authors": ["Junjie Liu", "Yuanhe Tian", "Yan Song"], "title": "Balanced Training Data Augmentation for Aspect-Based Sentiment Analysis", "comment": null, "summary": "Aspect-based sentiment analysis (ABSA) is a crucial fine-grained task in\nsocial media scenarios to identify the sentiment polarity of specific aspect\nterms in a sentence. Although many existing studies leverage large language\nmodels (LLMs) to perform ABSA due to their strong context understanding\ncapabilities, they still face challenges to learn the context information in\nthe running text because of the short text, as well as the small and unbalanced\nlabeled training data, where most data are labeled with positive sentiment.\nData augmentation (DA) is a feasible strategy for providing richer contextual\ninformation, especially when using LLMs to create synthetic training data, but\nfaces challenges in ensuring a high quality of the augmented data.In this\npaper, we propose an LLM-based ABSA approach with training data\naugmentation.Specifically, an LLM is prompted to generate augmented training\ndata based on the original training data, so as to construct a new training\ndata with larger size and balanced label distributions to better train an ABSA\nmodel. Meanwhile, in order to improve the quality of the augmented data, we\npropose a reinforcement learning approach to optimize the data augmentation.\nLLM.Experiment results and further analyses on English benchmark datasets for\nABSA demonstrate the effectiveness of our approach, where superior performance\nis observed over strong baselines and most existing studies.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408 LLM \u548c\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u6570\u636e\u589e\u5f3a\uff0c\u6539\u8fdb\u4e86\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\u5728\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e0a\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u91cf\u5c11\u3001\u4e0d\u5e73\u8861\u548c\u77ed\u6587\u672c\u7684\u6311\u6218\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e LLM \u7684 ABSA \u7814\u7a76\u9762\u4e34\u77ed\u6587\u672c\u3001\u5c11\u91cf\u4e14\u4e0d\u5e73\u8861\u7684\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\uff08\u5927\u591a\u6570\u6807\u7b7e\u4e3a\u6b63\u5411\u60c5\u611f\uff09\u7684\u6311\u6218\uff0c\u96be\u4ee5\u5b66\u4e60\u8fd0\u884c\u6587\u672c\u4e2d\u7684\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002\u6570\u636e\u589e\u5f3a\u662f\u4e00\u79cd\u53ef\u884c\u7684\u7b56\u7565\uff0c\u4f46\u9762\u4e34\u7740\u786e\u4fdd\u589e\u5f3a\u6570\u636e\u9ad8\u8d28\u91cf\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e LLM \u7684 ABSA \u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u8bad\u7ec3\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5229\u7528 LLM \u6839\u636e\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u751f\u6210\u589e\u5f3a\u540e\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u6784\u5efa\u66f4\u5927\u3001\u6807\u7b7e\u5206\u5e03\u66f4\u5747\u8861\u7684\u65b0\u8bad\u7ec3\u6570\u636e\u6765\u66f4\u597d\u5730\u8bad\u7ec3 ABSA \u6a21\u578b\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u63d0\u9ad8\u589e\u5f3a\u6570\u636e\u7684\u8d28\u91cf\uff0c\u63d0\u51fa\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u6765\u4f18\u5316\u6570\u636e\u589e\u5f3a\u7684 LLM\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u82f1\u6587 ABSA \u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u5e76\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u548c\u5927\u591a\u6570\u73b0\u6709\u7814\u7a76\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u548c\u8fdb\u4e00\u6b65\u5206\u6790\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u82f1\u6587 ABSA \u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u548c\u5927\u591a\u6570\u73b0\u6709\u7814\u7a76\u3002"}}
{"id": "2507.10082", "categories": ["cs.RO", "eess.SP"], "pdf": "https://arxiv.org/pdf/2507.10082", "abs": "https://arxiv.org/abs/2507.10082", "authors": ["Amit Levy", "Itzik Klein"], "title": "Unscented Kalman Filter with a Nonlinear Propagation Model for Navigation Applications", "comment": "5 pages, 2 figures", "summary": "The unscented Kalman filter is a nonlinear estimation algorithm commonly used\nin navigation applications. The prediction of the mean and covariance matrix is\ncrucial to the stable behavior of the filter. This prediction is done by\npropagating the sigma points according to the dynamic model at hand. In this\npaper, we introduce an innovative method to propagate the sigma points\naccording to the nonlinear dynamic model of the navigation error state vector.\nThis improves the filter accuracy and navigation performance. We demonstrate\nthe benefits of our proposed approach using real sensor data recorded by an\nautonomous underwater vehicle during several scenarios.", "AI": {"tldr": "An innovative method to propagate sigma points in the unscented Kalman filter improves accuracy and navigation performance, validated with real-world data from an autonomous underwater vehicle.", "motivation": "The prediction of the mean and covariance matrix is crucial to the stable behavior of the unscented Kalman filter, commonly used in navigation applications. This prediction is done by propagating the sigma points according to the dynamic model.", "method": "Propagating sigma points according to the nonlinear dynamic model of the navigation error state vector.", "result": "Improved filter accuracy and navigation performance.", "conclusion": "The proposed method for propagating sigma points in the unscented Kalman filter improves accuracy and navigation performance, as demonstrated with real sensor data from an autonomous underwater vehicle."}}
{"id": "2507.08956", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.08956", "abs": "https://arxiv.org/abs/2507.08956", "authors": ["Zhenghan Fang", "Mateo D\u00edaz", "Sam Buchanan", "Jeremias Sulam"], "title": "Beyond Scores: Proximal Diffusion Models", "comment": null, "summary": "Diffusion models have quickly become some of the most popular and powerful\ngenerative models for high-dimensional data. The key insight that enabled their\ndevelopment was the realization that access to the score -- the gradient of the\nlog-density at different noise levels -- allows for sampling from data\ndistributions by solving a reverse-time stochastic differential equation (SDE)\nvia forward discretization, and that popular denoisers allow for unbiased\nestimators of this score. In this paper, we demonstrate that an alternative,\nbackward discretization of these SDEs, using proximal maps in place of the\nscore, leads to theoretical and practical benefits. We leverage recent results\nin proximal matching to learn proximal operators of the log-density and, with\nthem, develop Proximal Diffusion Models (ProxDM). Theoretically, we prove that\n$\\widetilde{O}(d/\\sqrt{\\varepsilon})$ steps suffice for the resulting\ndiscretization to generate an $\\varepsilon$-accurate distribution w.r.t. the KL\ndivergence. Empirically, we show that two variants of ProxDM achieve\nsignificantly faster convergence within just a few sampling steps compared to\nconventional score-matching methods.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProximal Diffusion Models (ProxDM) \u7684\u65b0\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u8fd1\u90bb\u6620\u5c04\u66ff\u4ee3\u5f97\u5206\u6765\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u90fd\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u9ad8\u7ef4\u6570\u636e\u65f6\u7684\u5c40\u9650\u6027\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u5747\u6709\u6539\u8fdb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSDE\u53cd\u5411\u79bb\u6563\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u8fd1\u90bb\u6620\u5c04\u4ee3\u66ff\u5f97\u5206\u6765\u751f\u6210\u6570\u636e\u3002", "result": "ProxDM\u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\u5f97\u5206\u5339\u914d\u65b9\u6cd5\uff0c\u5728\u91c7\u6837\u6b65\u6570\u65b9\u9762\u5177\u6709\u663e\u8457\u7684\u6536\u655b\u4f18\u52bf\uff0c\u751f\u6210\u03b5\u7cbe\u5ea6\u5206\u5e03\u6240\u9700\u7684\u6b65\u6570\u7406\u8bba\u4e0a\u4e3aO(d/\u221a\u03b5)\u3002", "conclusion": "ProxDM\u901a\u8fc7\u5b66\u4e60\u5bf9\u6570\u5bc6\u5ea6\u7684\u8fd1\u90bb\u7b97\u5b50\uff0c\u5e76\u5728\u91c7\u6837\u65f6\u4f7f\u7528\u8fd1\u90bb\u6620\u5c04\u66ff\u4ee3\u5f97\u5206\uff0c\u5b9e\u73b0\u4e86\u7406\u8bba\u548c\u5b9e\u8df5\u4e0a\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.10003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10003", "abs": "https://arxiv.org/abs/2507.10003", "authors": ["Mohit Singh", "Mihir Dharmadhikari", "Kostas Alexis"], "title": "Ariel Explores: Vision-based underwater exploration and inspection via generalist drone-level autonomy", "comment": "Presented at the 2025 IEEE ICRA Workshop on Field Robotics", "summary": "This work presents a vision-based underwater exploration and inspection\nautonomy solution integrated into Ariel, a custom vision-driven underwater\nrobot. Ariel carries a $5$ camera and IMU based sensing suite, enabling a\nrefraction-aware multi-camera visual-inertial state estimation method aided by\na learning-based proprioceptive robot velocity prediction method that enhances\nrobustness against visual degradation. Furthermore, our previously developed\nand extensively field-verified autonomous exploration and general visual\ninspection solution is integrated on Ariel, providing aerial drone-level\nautonomy underwater. The proposed system is field-tested in a submarine dry\ndock in Trondheim under challenging visual conditions. The field demonstration\nshows the robustness of the state estimation solution and the generalizability\nof the path planning techniques across robot embodiments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u4e2a\u540d\u4e3a Ariel \u7684\u6c34\u4e0b\u673a\u5668\u4eba\uff0c\u5b83\u62e5\u6709\u5148\u8fdb\u7684\u89c6\u89c9\u548c\u60ef\u6027\u4f20\u611f\u7cfb\u7edf\uff0c\u80fd\u591f\u8fdb\u884c\u81ea\u4e3b\u63a2\u7d22\u548c\u68c0\u67e5\u3002\u8be5\u7cfb\u7edf\u5728\u6076\u52a3\u7684\u6c34\u4e0b\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u72b6\u6001\u4f30\u8ba1\u548c\u8def\u5f84\u89c4\u5212\u6280\u672f\u7684\u53ef\u9760\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u4f9b\u6c34\u4e0b\u673a\u5668\u4eba\uff08Ariel\uff09\u7684\u81ea\u4e3b\u63a2\u7d22\u548c\u901a\u7528\u89c6\u89c9\u68c0\u67e5\u80fd\u529b\uff0c\u5c31\u50cf\u65e0\u4eba\u673a\u5728\u7a7a\u4e2d\u4e00\u6837\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c6\u89c9\u7684\u6c34\u4e0b\u63a2\u7d22\u548c\u68c0\u67e5\u81ea\u4e3b\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u96c6\u6210\u4e86\u57fa\u4e8e\u5b66\u4e60\u7684\u672c\u4f53\u611f\u53d7\u673a\u5668\u4eba\u901f\u5ea6\u9884\u6d4b\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u89c6\u89c9\u9000\u5316\u7684\u9c81\u68d2\u6027\u3002\u8be5\u7cfb\u7edf\u4f7f\u7528\u5305\u542b 5 \u4e2a\u6444\u50cf\u5934\u548c IMU \u7684\u4f20\u611f\u5668\u5957\u4ef6\uff0c\u5e76\u91c7\u7528\u4e86\u57fa\u4e8e\u89c6\u89c9-\u60ef\u6027\u7684\u72b6\u6001\u4f30\u8ba1\u65b9\u6cd5\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u89c6\u89c9\u6761\u4ef6\u4e0b\uff0c\u73b0\u573a\u6f14\u793a\u663e\u793a\u4e86\u72b6\u6001\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u8def\u5f84\u89c4\u5212\u6280\u672f\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5b9e\u4f53\u4e0a\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u5728\u7279\u9686\u8d6b\u59c6\u7684\u4e00\u4e2a\u6f5c\u8247\u5e72\u8239\u575e\u4e2d\u8fdb\u884c\u4e86\u73b0\u573a\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u72b6\u6001\u4f30\u8ba1\u89e3\u51b3\u65b9\u6848\u7684\u9c81\u68d2\u6027\u4ee5\u53ca\u8def\u5f84\u89c4\u5212\u6280\u672f\u5728\u4e0d\u540c\u673a\u5668\u4eba\u5b9e\u4f53\u4e0a\u7684\u901a\u7528\u6027\u3002"}}
{"id": "2507.09209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09209", "abs": "https://arxiv.org/abs/2507.09209", "authors": ["Xiao Liang", "Di Wang", "Zhicheng Jiao", "Ronghan Li", "Pengfei Yang", "Quan Wang", "Tat-Seng Chua"], "title": "Uncertainty-Driven Expert Control: Enhancing the Reliability of Medical Vision-Language Models", "comment": null, "summary": "The rapid advancements in Vision Language Models (VLMs) have prompted the\ndevelopment of multi-modal medical assistant systems. Despite this progress,\ncurrent models still have inherent probabilistic uncertainties, often producing\nerroneous or unverified responses-an issue with serious implications in medical\napplications. Existing methods aim to enhance the performance of Medical Vision\nLanguage Model (MedVLM) by adjusting model structure, fine-tuning with\nhigh-quality data, or through preference fine-tuning. However, these\ntraining-dependent strategies are costly and still lack sufficient alignment\nwith clinical expertise. To address these issues, we propose an\nexpert-in-the-loop framework named Expert-Controlled Classifier-Free Guidance\n(Expert-CFG) to align MedVLM with clinical expertise without additional\ntraining. This framework introduces an uncertainty estimation strategy to\nidentify unreliable outputs. It then retrieves relevant references to assist\nexperts in highlighting key terms and applies classifier-free guidance to\nrefine the token embeddings of MedVLM, ensuring that the adjusted outputs are\ncorrect and align with expert highlights. Evaluations across three medical\nvisual question answering benchmarks demonstrate that the proposed Expert-CFG,\nwith 4.2B parameters and limited expert annotations, outperforms\nstate-of-the-art models with 13B parameters. The results demonstrate the\nfeasibility of deploying such a system in resource-limited settings for\nclinical use.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u201c\u4e13\u5bb6\u5728\u73af\u201d\u6846\u67b6Expert-CFG\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u63d0\u5347MedVLM\u7684\u51c6\u786e\u6027\u548c\u4e34\u5e8a\u5bf9\u9f50\u5ea6\uff0c\u4f18\u4e8e\u73b0\u6709\u5927\u578b\u6a21\u578b\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u573a\u666f\u3002", "motivation": "\u73b0\u6709\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08MedVLM\uff09\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u53ef\u80fd\u4ea7\u751f\u9519\u8bef\u6216\u672a\u7ecf\u8bc1\u5b9e\u7684\u54cd\u5e94\uff0c\u8fd9\u5728\u533b\u7597\u5e94\u7528\u4e2d\u540e\u679c\u4e25\u91cd\u3002\u73b0\u6709\u7684\u6539\u8fdb\u65b9\u6cd5\uff08\u5982\u8c03\u6574\u6a21\u578b\u7ed3\u6784\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u5fae\u8c03\u6216\u504f\u597d\u5fae\u8c03\uff09\u6210\u672c\u9ad8\u6602\u4e14\u4e0e\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u7684\u5bf9\u9f50\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aExpert-CFG\u7684\u4e13\u5bb6\u5728\u73af\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u3001\u53c2\u8003\u6587\u732e\u68c0\u7d22\u3001\u4e13\u5bb6\u6807\u6ce8\u548c\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\uff08Classifier-Free Guidance, CFG\uff09\u6765\u4f18\u5316MedVLM\u3002", "result": "Expert-CFG\u6846\u67b6\uff084.2B\u53c2\u6570\uff0c\u6709\u9650\u4e13\u5bb6\u6807\u6ce8\uff09\u5728\u4e09\u4e2a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8d85\u8d8a\u4e86\u53c2\u6570\u91cf\u66f4\u5927\u7684\uff0813B\uff09\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u8d44\u6e90\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Expert-CFG\u6846\u67b6\uff0c\u4e00\u4e2a\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u5373\u53ef\u5c06\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08MedVLM\uff09\u4e0e\u4e34\u5e8a\u4e13\u4e1a\u77e5\u8bc6\u5bf9\u9f50\u7684\u4e13\u5bb6\u5728\u73af\u6846\u67b6\u3002\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u8bc6\u522b\u4e0d\u53ef\u9760\u7684\u8f93\u51fa\uff0c\u68c0\u7d22\u76f8\u5173\u53c2\u8003\u6587\u732e\u4ee5\u8f85\u52a9\u4e13\u5bb6\u7a81\u51fa\u5173\u952e\u672f\u8bed\uff0c\u5e76\u5e94\u7528\u65e0\u5206\u7c7b\u5668\u6307\u5bfc\u6765\u4f18\u5316MedVLM\u7684\u5d4c\u5165\uff0c\u4ee5\u786e\u4fdd\u8c03\u6574\u540e\u7684\u8f93\u51fa\u51c6\u786e\u4e14\u7b26\u5408\u4e13\u5bb6\u6807\u6ce8\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6846\u67b6\u5728\u4e09\u4e2a\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5927\u578b\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8d44\u6e90\u6709\u9650\u73af\u5883\u4e0b\u7684\u4e34\u5e8a\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10526", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2507.10526", "abs": "https://arxiv.org/abs/2507.10526", "authors": ["Martin Pimon", "Tobias Kirschbaum", "Thorsten Schumm", "Adriana P\u00e1lffy", "Andreas Gr\u00fcneis"], "title": "Density Functional Theory Study of Th-doped LiCAF and LiSAF for Nuclear Clock Applications", "comment": null, "summary": "Thorium-doped LiCaAlF$_6$ and LiSrAlF$_6$ (Th:LiCAF and Th:LiSAF) are\npromising crystals for a solid-state nuclear clock based on the 8 eV transition\nin $^{229}$Th; however, their complex crystal structures complicate\nunderstanding the atomic arrangement of the thorium defects. In this work,\ndensity functional theory simulations are employed to systematically\ninvestigate these systems, including temperature-dependent effects and\nenvironmental conditions of fluorine saturation and deficiency. We investigated\n20 distinct charge compensation schemes for each material, revealing lower\ndefect formation energies in Th:LiSAF than in Th:LiCAF. This suggests that the\nformer may attain a higher concentration of thorium nuclei. Furthermore, we\ncalculated the electric field gradient for the lowest energy structure per\ncompensation pathway. Our investigation shows that results previously reported\nin the literature apply only to a subset of experimental conditions.", "AI": {"tldr": "\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u7814\u7a76\u4e86Th:LiCAF\u548cTh:LiSAF\u6676\u4f53\u4e2d\u7684\u948d\u7f3a\u9677\uff0c\u53d1\u73b0Th:LiSAF\u53ef\u80fd\u5177\u6709\u66f4\u9ad8\u7684\u948d\u6d53\u5ea6\uff0c\u5e76\u4e14\u5148\u524d\u6587\u732e\u7ed3\u679c\u7684\u9002\u7528\u6027\u6709\u9650\u3002", "motivation": "\u7406\u89e3\u948d\u5728Th:LiCaAlF$_6$ and LiSrAlF$_6$\u590d\u6742\u6676\u4f53\u7ed3\u6784\u4e2d\u7684\u539f\u5b50\u6392\u5217\u3002\u6709\u52a9\u4e8e\u4e3a\u57fa\u4e8e$^{229}$Th\u76848 eV\u8dc3\u8fc1\u7684\u56fa\u6001\u6838\u949f\u5bfb\u627e\u6709\u5e0c\u671b\u7684\u6676\u4f53\u3002", "method": "\u4f7f\u7528\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u6a21\u62df\u7cfb\u7edf\u5730\u7814\u7a76\u4e86Th:LiCaAlF$_6$ and LiSrAlF$_6$ (Th:LiCAF and Th:LiSAF)\u6676\u4f53\uff0c\u5305\u62ec\u4e86\u6e29\u5ea6\u76f8\u5173\u7684\u6548\u5e94\u4ee5\u53ca\u6c1f\u9971\u548c\u548c\u7f3a\u4e4f\u7684\u73af\u5883\u6761\u4ef6\uff0c\u5e76\u8ba1\u7b97\u4e86\u6700\u4f4e\u80fd\u91cf\u7ed3\u6784\u7684\u7535\u573a\u68af\u5ea6\u3002", "result": "\u53d1\u73b0\u4e8620\u79cd\u4e0d\u540c\u7684\u7535\u8377\u8865\u507f\u65b9\u6848\uff0c\u5e76\u63ed\u793a\u4e86Th:LiSAF\u7684\u7f3a\u9677\u5f62\u6210\u80fd\u4f4e\u4e8eTh:LiCAF\uff0c\u8fd9\u8868\u660eTh:LiSAF\u53ef\u80fd\u83b7\u5f97\u66f4\u9ad8\u6d53\u5ea6\u7684\u948d\u6838\u3002\u8ba1\u7b97\u4e86\u6700\u4f4e\u80fd\u91cf\u7ed3\u6784\u7684\u7535\u573a\u68af\u5ea6\uff0c\u5e76\u6307\u51fa\u5148\u524d\u6587\u732e\u62a5\u9053\u7684\u7ed3\u679c\u4ec5\u9002\u7528\u4e8e\u90e8\u5206\u5b9e\u9a8c\u6761\u4ef6\u3002", "conclusion": "Th:LiSAF\u53ef\u80fd\u6bd4Th:LiCAF\u5177\u6709\u66f4\u9ad8\u6d53\u5ea6\u7684\u948d\u6838\uff0c\u5e76\u4e14\u5148\u524d\u6587\u732e\u62a5\u9053\u7684\u7ed3\u679c\u4ec5\u9002\u7528\u4e8e\u90e8\u5206\u5b9e\u9a8c\u6761\u4ef6\u3002"}}
{"id": "2507.09690", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09690", "abs": "https://arxiv.org/abs/2507.09690", "authors": ["Christian Kraglund Andersen", "Eli\u0161ka Greplov\u00e1"], "title": "Small Quantum Low Parity Density Check Codes for Near-Term Experiments", "comment": "16 pages, 8 figures", "summary": "It is widely accepted that quantum error correction is essential for\nrealizing large-scale fault-tolerant quantum computing. Recent experiments have\ndemonstrated error correction codes operating below threshold, primarily using\nlocal planar codes such as the surface code and color code. In parallel,\ntheoretical advances in quantum low-density parity-check (LDPC) codes promise\nsignificantly lower overheads, albeit at the cost of requiring non-local parity\nchecks. While these results are encouraging, implementing such codes remains\nchallenging for near-term experiments, creating obstacles to holistic\nbenchmarking of hardware architectures capable of supporting long-range\ncouplers. In this work, we present a simple construction recipe for small\nquantum LDPC codes based on recent developments in the field. Our codes are\napproximately twice as efficient as comparable surface codes, yet require only\nweight-four parity checks, which simplifies experimental realization compared\nto other quantum LDPC codes. We provide concrete proposals for implementations\nwith superconducting qubits in flip-chip architectures and with semiconductor\nspin qubits using shuttling-based approaches.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u91cf\u5b50LDPC\u7801\u6784\u9020\u65b9\u6cd5\uff0c\u5176\u6548\u7387\u662f\u8868\u9762\u7801\u7684\u4e24\u500d\uff0c\u5b9e\u9a8c\u5b9e\u73b0\u66f4\u7b80\u5355\uff0c\u5e76\u63d0\u4f9b\u4e86\u8d85\u5bfc\u548c\u534a\u5bfc\u4f53\u91cf\u5b50\u6bd4\u7279\u7684\u5b9e\u73b0\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u5927\u89c4\u6a21\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\uff0c\u91cf\u5b50\u7ea0\u9519\u81f3\u5173\u91cd\u8981\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5982\u8868\u9762\u7801\u5728\u5b9e\u9a8c\u4e0a\u53d6\u5f97\u8fdb\u5c55\uff0c\u4f46\u7406\u8bba\u4e0a\u91cf\u5b50LDPC\u7801\u5177\u6709\u66f4\u4f4e\u7684\u5f00\u9500\uff0c\u7136\u800c\u5176\u975e\u5c40\u57df\u6027\u6821\u9a8c\u589e\u52a0\u4e86\u5b9e\u73b0\u96be\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa\u4e00\u79cd\u7b80\u5316\u7684\u91cf\u5b50LDPC\u7801\u6784\u9020\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u91cf\u5b50\u4f4e\u5bc6\u5ea6\u5947\u5076\u6821\u9a8c\u7801\uff08LDPC\uff09\u7684\u7b80\u6613\u6784\u9020\u65b9\u6cd5\uff0c\u7528\u4e8e\u6784\u5efa\u5c0f\u578b\u91cf\u5b50\u7ea0\u9519\u7801\u3002", "result": "\u6240\u63d0\u51fa\u7684\u91cf\u5b50LDPC\u7801\u6bd4\u8868\u9762\u7801\u6548\u7387\u9ad8\u4e00\u500d\uff0c\u4e14\u4ec5\u9700\u6743\u91cd\u4e3a\u56db\u7684\u5947\u5076\u6821\u9a8c\uff0c\u5e76\u7ed9\u51fa\u4e86\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u548c\u534a\u5bfc\u4f53\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u5177\u4f53\u5b9e\u73b0\u5efa\u8bae\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u63d0\u51fa\u4e86\u57fa\u4e8e\u91cf\u5b50\u4f4e\u5bc6\u5ea6\u5947\u5076\u6821\u9a8c\u7801\uff08LDPC\uff09\u7684\u65b0\u578b\u91cf\u5b50\u7ea0\u9519\u7801\uff0c\u5176\u6548\u7387\u662f\u8868\u9762\u7801\u7684\u4e24\u500d\uff0c\u540c\u65f6\u6240\u9700\u7684\u5947\u5076\u6821\u9a8c\u6743\u91cd\u4ec5\u4e3a\u56db\uff0c\u8fd9\u5927\u5927\u7b80\u5316\u4e86\u5b9e\u9a8c\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u4e3a\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u548c\u534a\u5bfc\u4f53\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u63d0\u4f9b\u4e86\u5177\u4f53\u7684\u5b9e\u73b0\u65b9\u6848\u3002"}}
{"id": "2507.09497", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09497", "abs": "https://arxiv.org/abs/2507.09497", "authors": ["Siyi Wu", "Zeyu Wang", "Xinyuan Song", "Zhengpeng Zhou", "Lifan Sun", "Tianyu Shi"], "title": "GoalfyMax: A Protocol-Driven Multi-Agent System for Intelligent Experience Entities", "comment": null, "summary": "Modern enterprise environments demand intelligent systems capable of handling\ncomplex, dynamic, and multi-faceted tasks with high levels of autonomy and\nadaptability. However, traditional single-purpose AI systems often lack\nsufficient coordination, memory reuse, and task decomposition capabilities,\nlimiting their scalability in realistic settings. To address these challenges,\nwe present \\textbf{GoalfyMax}, a protocol-driven framework for end-to-end\nmulti-agent collaboration. GoalfyMax introduces a standardized Agent-to-Agent\n(A2A) communication layer built on the Model Context Protocol (MCP), allowing\nindependent agents to coordinate through asynchronous, protocol-compliant\ninteractions. It incorporates the Experience Pack (XP) architecture, a layered\nmemory system that preserves both task rationales and execution traces,\nenabling structured knowledge retention and continual learning. Moreover, our\nsystem integrates advanced features including multi-turn contextual dialogue,\nlong-short term memory modules, and dynamic safety validation, supporting\nrobust, real-time strategy adaptation. Empirical results on complex task\norchestration benchmarks and case study demonstrate that GoalfyMax achieves\nsuperior adaptability, coordination, and experience reuse compared to baseline\nframeworks. These findings highlight its potential as a scalable, future-ready\nfoundation for multi-agent intelligent systems.", "AI": {"tldr": "GoalfyMax is a new framework for multi-agent systems that improves coordination and memory reuse using a protocol-driven approach and a layered memory system, outperforming existing frameworks in complex tasks.", "motivation": "Traditional single-purpose AI systems lack sufficient coordination, memory reuse, and task decomposition capabilities, limiting their scalability in realistic enterprise environments that demand intelligent systems capable of handling complex, dynamic, and multi-faceted tasks with high levels of autonomy and adaptability.", "method": "GoalfyMax is a protocol-driven framework for end-to-end multi-agent collaboration, featuring a standardized Agent-to-Agent (A2A) communication layer based on the Model Context Protocol (MCP) for asynchronous interactions, and the Experience Pack (XP) architecture, a layered memory system for knowledge retention and continual learning. It also includes multi-turn contextual dialogue, long-short term memory modules, and dynamic safety validation for real-time strategy adaptation.", "result": "Empirical results on complex task orchestration benchmarks and a case study demonstrate that GoalfyMax achieves superior adaptability, coordination, and experience reuse compared to baseline frameworks.", "conclusion": "GoalfyMax has the potential to serve as a scalable, future-ready foundation for multi-agent intelligent systems, demonstrating superior adaptability, coordination, and experience reuse compared to baseline frameworks."}}
{"id": "2507.08959", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08959", "abs": "https://arxiv.org/abs/2507.08959", "authors": ["Xiang Li", "Xinyu Wang", "Yifan Lin"], "title": "Graph Neural Network Enhanced Sequential Recommendation Method for Cross-Platform Ad Campaign", "comment": null, "summary": "In order to improve the accuracy of cross-platform advertisement\nrecommendation, a graph neural network (GNN)- based advertisement\nrecommendation method is analyzed. Through multi-dimensional modeling, user\nbehavior data (e.g., click frequency, active duration) reveal temporal patterns\nof interest evolution, ad content (e.g., type, tag, duration) influences\nsemantic preferences, and platform features (e.g., device type, usage context)\nshape the environment where interest transitions occur. These factors jointly\nenable the GNN to capture the latent pathways of user interest migration across\nplatforms. The experimental results are based on the datasets of three\nplatforms, and Platform B reaches 0.937 in AUC value, which is the best\nperformance. Platform A and Platform C showed a slight decrease in precision\nand recall with uneven distribution of ad labels. By adjusting the\nhyperparameters such as learning rate, batch size and embedding dimension, the\nadaptability and robustness of the model in heterogeneous data are further\nimproved.", "AI": {"tldr": "\u57fa\u4e8eGNN\u7684\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5efa\u6a21\u6355\u6349\u7528\u6237\u5174\u8da3\u8fc1\u79fb\uff0c\u5e76\u5728Platform B\u4e0a\u53d6\u5f97\u4f18\u5f02\u7684AUC\u503c\uff080.937\uff09\uff0c\u540c\u65f6\u901a\u8fc7\u8d85\u53c2\u6570\u8c03\u6574\u63d0\u9ad8\u4e86\u5728\u5f02\u6784\u6570\u636e\u4e0a\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u5e7f\u544a\u63a8\u8350\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5efa\u6a21\u6765\u6355\u6349\u7528\u6237\u5174\u8da3\u7684\u6f14\u53d8\u3001\u5e7f\u544a\u5185\u5bb9\u7684\u8bed\u4e49\u504f\u597d\u4ee5\u53ca\u5e73\u53f0\u7279\u5f81\u5bf9\u7528\u6237\u5174\u8da3\u8fc1\u79fb\u8def\u5f84\u7684\u5f71\u54cd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684GNN\u6a21\u578b\u5728Platform B\u4e0a\u8fbe\u5230\u4e86\u6700\u4f73\u76840.937 AUC\u503c\u3002Platform A\u548cPlatform C\u7531\u4e8e\u5e7f\u544a\u6807\u7b7e\u5206\u5e03\u4e0d\u5747\uff0c\u7cbe\u5ea6\u548c\u53ec\u56de\u7387\u7565\u6709\u4e0b\u964d\u3002\u901a\u8fc7\u8c03\u6574\u5b66\u4e60\u7387\u3001\u6279\u5927\u5c0f\u548c\u5d4c\u5165\u7ef4\u5ea6\u7b49\u8d85\u53c2\u6570\uff0c\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u5f97\u5230\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u591a\u7ef4\u5ea6\u5efa\u6a21\u548c\u8d85\u53c2\u6570\u8c03\u6574\uff0c\u57fa\u4e8e\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u8de8\u5e73\u53f0\u5e7f\u544a\u63a8\u8350\u65b9\u6cd5\u5728\u7528\u6237\u5174\u8da3\u8fc1\u79fb\u65b9\u9762\u8868\u73b0\u51fa\u826f\u597d\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728Platform B\u4e0a\u8fbe\u5230\u4e860.937\u7684AUC\u503c\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.10000", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.", "AI": {"tldr": "\u4e00\u79cd\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u6216\u63a8\u7406\u5373\u53ef\u8bc6\u522b\u6587\u672c\u4e2d\u6f5c\u5728\u610f\u56fe\u7684\u4f4e\u6210\u672c\u65b9\u6cd5\u3002", "motivation": "Searle\u5bf9\u610f\u56fe\u548c\u610f\u5411\u6027\u7684\u54f2\u5b66\u89e3\u6784\u540e\uff0c\u610f\u56fe\u5728\u79d1\u6280\u9886\u57df\u7684\u5b9e\u9645\u610f\u4e49\u53d7\u5230\u5ffd\u89c6\u3002", "method": "\u901a\u8fc7\u5bfb\u627e\u591a\u5c3a\u5ea6\u5f02\u5e38\u5e76\u8bc4\u4f30\u5f62\u6210\u5b83\u4eec\u7684\u5de5\u4f5c\u91cf\uff0c\u5e76\u5229\u7528\u65f6\u7a7a\u76f8\u5e72\u6027\u4f5c\u4e3a\u5ea6\u91cf\u6765\u5206\u79bb\u2018\u610f\u56fe\u2019\u5185\u5bb9\u548c\u2018\u73af\u5883\u2019\u80cc\u666f\uff0c\u6765\u91cf\u5316\u6570\u636e\u4e2d\u6f5c\u5728\u7684\u2018\u610f\u56fe\u2019\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f4e\u8ba1\u7b97\u6210\u672c\u3001\u65e0\u9700\u5e7f\u6cdb\u8bad\u7ec3\u6216\u63a8\u7406\u80fd\u529b\u5373\u53ef\u89e3\u91ca\u6f5c\u5728\u610f\u56fe\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u57fa\u672c\u751f\u7269\u4f53\uff0c\u6982\u5ff5\u5f62\u6210\u6c34\u5e73\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u5bb9\u91cf\u3002\u8fd9\u4e3a\u7406\u89e3\u6587\u672c\u4e2d\u7684\u610f\u56fe\u548c\u4e0a\u4e0b\u6587\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\u3002\u7531\u4e8e\u5176\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u548c\u5bf9\u5927\u89c4\u6a21\u8bad\u7ec3\u7684\u4f9d\u8d56\u6027\u8f83\u4f4e\uff0c\u56e0\u6b64\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u3002\u7136\u800c\uff0c\u7ed3\u679c\u7684\u6709\u6548\u6027\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u80fd\u529b\u548c\u6570\u636e\u4e2d\u6f5c\u5728\u610f\u56fe\u7684\u660e\u786e\u7a0b\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u4e0d\u4f9d\u8d56\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u63a8\u7406\u80fd\u529b\u7684\u89e3\u91ca\u6f5c\u5728\u610f\u56fe\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u5176\u6982\u5ff5\u5f62\u6210\u6c34\u5e73\u53d6\u51b3\u4e8e\u667a\u80fd\u4f53\u7684\u8bb0\u5fc6\u5bb9\u91cf\u3002"}}
{"id": "2507.10030", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10030", "abs": "https://arxiv.org/abs/2507.10030", "authors": ["Marco Cal\u00ec", "Alberto Sinigaglia", "Niccol\u00f2 Turcato", "Ruggero Carli", "Gian Antonio Susto"], "title": "Finetuning Deep Reinforcement Learning Policies with Evolutionary Strategies for Control of Underactuated Robots", "comment": null, "summary": "Deep Reinforcement Learning (RL) has emerged as a powerful method for\naddressing complex control problems, particularly those involving underactuated\nrobotic systems. However, in some cases, policies may require refinement to\nachieve optimal performance and robustness aligned with specific task\nobjectives. In this paper, we propose an approach for fine-tuning Deep RL\npolicies using Evolutionary Strategies (ES) to enhance control performance for\nunderactuated robots. Our method involves initially training an RL agent with\nSoft-Actor Critic (SAC) using a surrogate reward function designed to\napproximate complex specific scoring metrics. We subsequently refine this\nlearned policy through a zero-order optimization step employing the Separable\nNatural Evolution Strategy (SNES), directly targeting the original score.\nExperimental evaluations conducted in the context of the 2nd AI Olympics with\nRealAIGym at IROS 2024 demonstrate that our evolutionary fine-tuning\nsignificantly improves agent performance while maintaining high robustness. The\nresulting controllers outperform established baselines, achieving competitive\nscores for the competition tasks.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u5bf9\u8f6fActor-Critic\uff08SAC\uff09\u8bad\u7ec3\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u4ee3\u7406\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7684\u63a7\u5236\u6027\u80fd\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u9700\u8981\u5bf9\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b56\u7565\u8fdb\u884c\u5fae\u8c03\u3002", "method": "\u8be5\u65b9\u6cd5\u9996\u5148\u4f7f\u7528\u8f6fActor-Critic\uff08SAC\uff09\u548c\u7528\u4e8e\u8fd1\u4f3c\u590d\u6742\u7279\u5b9a\u8bc4\u5206\u6307\u6807\u7684\u4ee3\u7406\u5956\u52b1\u51fd\u6570\u6765\u8bad\u7ec3RL\u4ee3\u7406\u3002\u7136\u540e\uff0c\u4f7f\u7528\u96f6\u9636\u4f18\u5316\u6b65\u9aa4\u548c\u53ef\u5206\u79bb\u81ea\u7136\u8fdb\u5316\u7b56\u7565\uff08SNES\uff09\u76f4\u63a5\u9488\u5bf9\u539f\u59cb\u5206\u6570\u6765\u4f18\u5316\u6240\u5b66\u7684\u7b56\u7565\u3002", "result": "\u8be5\u8fdb\u5316\u5fae\u8c03\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728AI Olympics\u7684\u673a\u5668\u4eba\u63a7\u5236\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8fdb\u5316\u7b56\u7565\uff08ES\uff09\u5bf9\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7b56\u7565\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4ee5\u63d0\u9ad8\u6b20\u9a71\u52a8\u673a\u5668\u4eba\u7684\u63a7\u5236\u6027\u80fd\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u7406\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2507.09214", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09214", "abs": "https://arxiv.org/abs/2507.09214", "authors": ["Shiyi Mu", "Zichong Gu", "Hanqi Lyu", "Yilin Gao", "Shugong Xu"], "title": "Stereo-based 3D Anomaly Object Detection for Autonomous Driving: A New Dataset and Baseline", "comment": "under review", "summary": "3D detection technology is widely used in the field of autonomous driving,\nwith its application scenarios gradually expanding from enclosed highways to\nopen conventional roads. For rare anomaly categories that appear on the road,\n3D detection models trained on closed sets often misdetect or fail to detect\nanomaly objects. To address this risk, it is necessary to enhance the\ngeneralization ability of 3D detection models for targets of arbitrary shapes\nand to possess the capability to filter out anomalies. The generalization of 3D\ndetection is limited by two factors: the coupled training of 2D and 3D, and the\ninsufficient diversity in the scale distribution of training samples. This\npaper proposes a Stereo-based 3D Anomaly object Detection (S3AD) algorithm,\nwhich decouples the training strategy of 3D and 2D to release the\ngeneralization ability for arbitrary 3D foreground detection, and proposes an\nanomaly scoring algorithm based on foreground confidence prediction, achieving\ntarget-level anomaly scoring. In order to further verify and enhance the\ngeneralization of anomaly detection, we use a 3D rendering method to synthesize\ntwo augmented reality binocular stereo 3D detection datasets which named\nKITTI-AR. KITTI-AR extends upon KITTI by adding 97 new categories, totaling 6k\npairs of stereo images. The KITTI-AR-ExD subset includes 39 common categories\nas extra training data to address the sparse sample distribution issue.\nAdditionally, 58 rare categories form the KITTI-AR-OoD subset, which are not\nused in training to simulate zero-shot scenarios in real-world settings, solely\nfor evaluating 3D anomaly detection. Finally, the performance of the algorithm\nand the dataset is verified in the experiments. (Code and dataset can be\nobtained at https://github.com/xxxx/xxx).", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS3AD\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d3D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5728\u5f00\u653e\u9053\u8def\u4e0a\u5bf9\u7f55\u89c1\u5f02\u5e38\u7c7b\u522b\u7684\u68c0\u6d4b\u95ee\u9898\u3002\u901a\u8fc7\u89e3\u80262D\u548c3D\u8bad\u7ec3\u7b56\u7565\u3001\u5f15\u5165\u5f02\u5e38\u8bc4\u5206\u7b97\u6cd5\uff0c\u5e76\u6784\u5efa\u4e86\u5305\u542b\u5927\u91cf\u7f55\u89c1\u7c7b\u522b\u7684KITTI-AR\u6570\u636e\u96c6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5f02\u5e38\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b33D\u68c0\u6d4b\u6a21\u578b\u5728\u5f00\u653e\u9053\u8def\u7b49\u573a\u666f\u4e0b\uff0c\u5bf9\u7f55\u89c1\u5f02\u5e38\u7c7b\u522b\u68c0\u6d4b\u4e0d\u51c6\u6216\u65e0\u6cd5\u68c0\u6d4b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u5bf9\u4efb\u610f\u5f62\u72b6\u76ee\u6807\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u5177\u5907\u8fc7\u6ee4\u5f02\u5e38\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7acb\u4f53\u58f03D\u5f02\u5e38\u7269\u4f53\u68c0\u6d4b\uff08S3AD\uff09\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5c063D\u548c2D\u7684\u8bad\u7ec3\u7b56\u7565\u89e3\u8026\uff0c\u4ee5\u91ca\u653e\u4efb\u610f3D\u524d\u666f\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u524d\u666f\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u5f02\u5e38\u8bc4\u5206\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u76ee\u6807\u7ea7\u522b\u7684\u5f02\u5e38\u8bc4\u5206\u3002\u4e3a\u4e86\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u548c\u589e\u5f3a\u5f02\u5e38\u68c0\u6d4b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u7814\u7a76\u8005\u4f7f\u75283D\u6e32\u67d3\u65b9\u6cd5\u5408\u6210\u4e86\u4e24\u4e2a\u589e\u5f3a\u73b0\u5b9e\u53cc\u76ee\u7acb\u4f533D\u68c0\u6d4b\u6570\u636e\u96c6KITTI-AR\u3002KITTI-AR\u5728KITTI\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e8697\u4e2a\u65b0\u7c7b\u522b\uff0c\u5171\u5305\u542b6k\u5bf9\u7acb\u4f53\u56fe\u50cf\u3002KITTI-AR-ExD\u5b50\u96c6\u5305\u62ec39\u4e2a\u5e38\u89c1\u7c7b\u522b\u4f5c\u4e3a\u989d\u5916\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u7a00\u758f\u6837\u672c\u5206\u5e03\u95ee\u9898\u3002\u6b64\u5916\uff0c58\u4e2a\u7f55\u89c1\u7c7b\u522b\u6784\u6210\u4e86KITTI-AR-OoD\u5b50\u96c6\uff0c\u8fd9\u4e9b\u7c7b\u522b\u4e0d\u7528\u4e8e\u8bad\u7ec3\uff0c\u4ee5\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u4e2d\u7684\u96f6\u6837\u672c\u573a\u666f\uff0c\u4ec5\u7528\u4e8e\u8bc4\u4f303D\u5f02\u5e38\u68c0\u6d4b\u3002", "result": "\u901a\u8fc7\u5728KITTI-AR\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86S3AD\u7b97\u6cd5\u7684\u6709\u6548\u6027\u53ca\u5176\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u57283D\u5f02\u5e38\u7269\u4f53\u68c0\u6d4b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684S3AD\u7b97\u6cd5\u901a\u8fc7\u89e3\u80262D\u548c3D\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u524d\u666f\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u7684\u5f02\u5e38\u8bc4\u5206\u7b97\u6cd5\uff0c\u6709\u6548\u63d0\u5347\u4e863D\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u5bf9\u4efb\u610f\u5f62\u72b6\u76ee\u6807\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u76ee\u6807\u7ea7\u522b\u7684\u5f02\u5e38\u8bc4\u5206\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5408\u6210\u7684KITTI-AR\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u7b97\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u4e30\u5bcc\u7684\u7f55\u89c1\u7c7b\u522b\u548c\u589e\u5f3a\u7684\u6837\u672c\u5206\u5e03\uff0c\u4e3a3D\u5f02\u5e38\u68c0\u6d4b\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2507.09691", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09691", "abs": "https://arxiv.org/abs/2507.09691", "authors": ["Hanfeng Wang", "Kurt Jacobs", "Donald Fahey", "Yong Hu", "Dirk R. Englund", "Matthew E. Trusheim"], "title": "Exceptional sensitivity near the bistable transition point of a hybrid quantum system", "comment": "11 pages, 4 figures", "summary": "Phase transitions can dramatically alter system dynamics, unlocking new\nbehavior and improving performance. Exceptional points (EPs), where the\neigenvalues and corresponding eigenvectors of a coupled linear system coalesce,\nare particularly relevant for sensing applications as they can increase sensor\nresponse to external perturbations to a range of phenomena from optical phase\nshifts to gravitational waves. However, the coalescence of eigenstates at\nlinear EPs amplifies noise, negating the signal-to-noise ratio (SNR)\nenhancement. Here, we overcome this limitation using nonlinearity, which\nexhibits exceptional SNR around a bistable transition point (BP). We couple a\nstate-of-the-art diamond quantum sensor to a nonlinear Van der Pol oscillator,\nforming a self-oscillating hybrid system that exhibits both a single-valued and\nbistable phase. The boundaries between these phases are marked by both\nadiabatic and deterministic non-adiabatic transitions that enable chiral state\nswitching and state coalescence at the BP. Crucially, NV magnetometry performed\nnear the BP exhibits a 17x enhancement in SNR, achieving a record sensitivity\nof 170 fT/\\sqrt{Hz}. This result surpasses the sensitivity limit of an ideal,\nthermally-limited electron magnetometer and resolves a long-standing debate\nregarding EP-like physics in advanced quantum sensing.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u975e\u7ebf\u6027\u514b\u670d\u4e86\u5384\u5c14\u5c3c\u79ef\u5206\u70b9\u653e\u5927\u91cf\u5b50\u566a\u58f0\u7684\u95ee\u9898\uff0c\u901a\u8fc7\u8026\u5408\u91d1\u521a\u77f3\u91cf\u5b50\u4f20\u611f\u5668\u548c\u8303\u5fb7\u6ce2\u5c14\u632f\u8361\u5668\uff0c\u5728\u53cc\u7a33\u6001\u8f6c\u53d8\u70b9\u9644\u8fd1\u5b9e\u73b0\u4e86 17 \u500d\u7684\u4fe1\u566a\u6bd4\u589e\u5f3a\u548c 170 fT/ \u0aaa\u0acd\u0ab2\u0abeHz \u7684\u7075\u654f\u5ea6\u3002", "motivation": "\u5384\u5c14\u5c3c\u79ef\u5206\u70b9\uff08EP\uff09\u867d\u7136\u53ef\u4ee5\u63d0\u9ad8\u4f20\u611f\u5668\u5bf9\u5916\u90e8\u6270\u52a8\u7684\u54cd\u5e94\uff0c\u4f46\u4f1a\u653e\u5927\u566a\u58f0\uff0c\u4ece\u800c\u62b5\u6d88\u4fe1\u566a\u6bd4\uff08SNR\uff09\u7684\u63d0\u9ad8\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u975e\u7ebf\u6027\u6765\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u5728\u53cc\u7a33\u6001\u8f6c\u53d8\u70b9\uff08BP\uff09\u9644\u8fd1\u5b9e\u73b0\u5353\u8d8a\u7684\u4fe1\u566a\u6bd4\u3002", "method": "\u5c06\u5148\u8fdb\u7684\u91d1\u521a\u77f3\u91cf\u5b50\u4f20\u611f\u5668\u4e0e\u975e\u7ebf\u6027\u8303\u5fb7\u6ce2\u5c14\u632f\u8361\u5668\u8026\u5408\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u5c55\u793a\u5355\u503c\u548c\u53cc\u7a33\u6001\u76f8\u7684\u81ea\u632f\u8361\u6df7\u5408\u7cfb\u7edf\u3002\u8be5\u7cfb\u7edf\u7684\u4e24\u4e2a\u76f8\u7684\u8fb9\u754c\u7531\u7edd\u70ed\u548c\u786e\u5b9a\u6027\u7684\u975e\u7edd\u70ed\u8dc3\u8fc1\u6807\u5fd7\uff0c\u8fd9\u4f7f\u5f97\u5728\u53cc\u7a33\u6001\u8f6c\u53d8\u70b9\uff08BP\uff09\u5b9e\u73b0\u624b\u6027\u6001\u5207\u6362\u548c\u6001\u7684\u5408\u5e76\u3002", "result": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5229\u7528\u975e\u7ebf\u6027\u514b\u670d\u4e86\u5384\u5c14\u5c3c\u79ef\u5206\u70b9\u653e\u5927\u91cf\u5b50\u566a\u58f0\u7684\u9650\u5236\u3002\u901a\u8fc7\u5c06\u91d1\u521a\u77f3\u91cf\u5b50\u4f20\u611f\u5668\u4e0e\u975e\u7ebf\u6027\u8303\u5fb7\u6ce2\u5c14\u632f\u8361\u5668\u8026\u5408\uff0c\u7814\u7a76\u4eba\u5458\u5728\u53cc\u7a33\u6001\u8f6c\u53d8\u70b9\u9644\u8fd1\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4fe1\u566a\u6bd4\u3002\u5177\u4f53\u6765\u8bf4\uff0cNV \u78c1\u529b\u6d4b\u91cf\u5728\u63a5\u8fd1\u53cc\u7a33\u6001\u8f6c\u53d8\u70b9\u65f6\u8868\u73b0\u51fa 17 \u500d\u7684\u4fe1\u566a\u6bd4\u589e\u5f3a\uff0c\u8fbe\u5230\u4e86 170 fT/ \u0aaa\u0acd\u0ab2\u0abeHz \u7684\u521b\u7eaa\u5f55\u7075\u654f\u5ea6\uff0c\u8d85\u8d8a\u4e86\u7406\u60f3\u7535\u5b50\u78c1\u529b\u8ba1\u7684\u7075\u654f\u5ea6\u6781\u9650\u3002", "conclusion": " NV \u78c1\u529b\u6d4b\u91cf\u5728\u63a5\u8fd1\u53cc\u7a33\u6001\u8f6c\u53d8\u70b9\u65f6\u8868\u73b0\u51fa 17 \u500d\u7684\u4fe1\u566a\u6bd4\u589e\u5f3a\uff0c\u8fbe\u5230\u4e86 170 fT/ \u0aaa\u0acd\u0ab2\u0abeHz \u7684\u521b\u7eaa\u5f55\u7075\u654f\u5ea6\u3002\u8be5\u7ed3\u679c\u8d85\u8d8a\u4e86\u7406\u60f3\u7684\u3001\u70ed\u9650\u5236\u7684\u7535\u5b50\u78c1\u529b\u8ba1\u7684\u7075\u654f\u5ea6\u6781\u9650\uff0c\u5e76\u89e3\u51b3\u4e86\u957f\u671f\u4ee5\u6765\u5173\u4e8e\u9ad8\u7ea7\u91cf\u5b50\u4f20\u611f\u4e2d\u7c7b\u5384\u5c14\u5c3c\u79ef\u5206\u70b9\u7269\u7406\u5b66\u7684\u4e89\u8bba\u3002"}}
{"id": "2507.09506", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09506", "abs": "https://arxiv.org/abs/2507.09506", "authors": ["Junjie Wu", "Gefei Gu", "Yanan Zheng", "Dit-Yan Yeung", "Arman Cohan"], "title": "Ref-Long: Benchmarking the Long-context Referencing Capability of Long-context Language Models", "comment": "ACL 2025 Main Conference. First 2 authors contributed equally", "summary": "Long-context language models (LCLMs) have exhibited impressive capabilities\nin long-context understanding tasks. Among these, long-context referencing -- a\ncrucial task that requires LCLMs to attribute items of interest to specific\nparts of long-context data -- remains underexplored. To bridge this gap, this\npaper proposes Referencing Evaluation for Long-context Language Models\n(Ref-Long), a novel benchmark designed to assess the long-context referencing\ncapability of LCLMs. Specifically, Ref-Long requires LCLMs to identify the\nindexes of documents that reference a specific key, emphasizing contextual\nrelationships between the key and the documents over simple retrieval. Based on\nthe task design, we construct three subsets ranging from synthetic to realistic\nscenarios to form the Ref-Long benchmark. Experimental results of 13 LCLMs\nreveal significant shortcomings in long-context referencing, even among\nadvanced models like GPT-4o. To further investigate these challenges, we\nconduct comprehensive analyses, including human evaluations, task format\nadjustments, fine-tuning experiments, and error analyses, leading to several\nkey insights. Our data and code can be found in https://github.\ncom/wujunjie1998/Ref-Long.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86Ref-Long\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u4e2d\u5b9a\u4f4d\u548c\u5f52\u56e0\u4fe1\u606f\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\uff08\u5305\u62ecGPT-4o\uff09\u5728\u8fd9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\u4ee5\u671f\u6539\u8fdb\u3002", "motivation": "\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08LCLMs\uff09\u5728\u7406\u89e3\u957f\u6587\u672c\u65b9\u9762\u80fd\u529b\u5f3a\u5927\uff0c\u4f46\u957f\u4e0a\u4e0b\u6587\u6307\u4ee3\u2014\u2014\u5373\u5c06\u7528\u6237\u611f\u5174\u8da3\u7684\u5185\u5bb9\u51c6\u786e\u5f52\u56e0\u4e8e\u957f\u6587\u672c\u4e2d\u7684\u7279\u5b9a\u90e8\u5206\u2014\u2014\u8fd9\u4e00\u5173\u952e\u80fd\u529b\u7684\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u8bc4\u4f30\u548c\u63d0\u5347LCLMs\u5728\u8fd9\u4e00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aRef-Long\u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u4e0a\u4e0b\u6587\u4e2d\u7684\u6307\u4ee3\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b\u4e09\u4e2a\u4e0d\u540c\u96be\u5ea6\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d6\u4ece\u5408\u6210\u5230\u771f\u5b9e\u7684\u5e94\u7528\u573a\u666f\u3002\u901a\u8fc7\u5bf913\u4e2a\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u5e76\u7ed3\u5408\u4eba\u7c7b\u8bc4\u4f30\u3001\u4efb\u52a1\u683c\u5f0f\u8c03\u6574\u3001\u5fae\u8c03\u548c\u9519\u8bef\u5206\u6790\u7b49\u591a\u79cd\u65b9\u6cd5\uff0c\u6df1\u5165\u63a2\u7a76\u6a21\u578b\u5728\u957f\u6587\u672c\u6307\u4ee3\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u548c\u6311\u6218\u3002", "result": "\u5728\u5bf913\u4e2a\u957f\u4e0a\u4e0b\u6587\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ecGPT-4o\uff09\u7684\u8bc4\u4f30\u4e2d\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u957f\u4e0a\u4e0b\u6587\u6307\u4ee3\u4efb\u52a1\u4e0a\u666e\u904d\u5b58\u5728\u663e\u8457\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u8fdb\u4e00\u6b65\u7684\u5206\u6790\uff0c\u7814\u7a76\u83b7\u5f97\u4e86\u5173\u4e8e\u6a21\u578b\u80fd\u529b\u3001\u9519\u8bef\u6a21\u5f0f\u4ee5\u53ca\u6f5c\u5728\u6539\u8fdb\u65b9\u5411\u7684\u591a\u4e2a\u5173\u952e\u89c1\u89e3\u3002", "conclusion": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u6587\u672c\u7406\u89e3\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u957f\u6587\u672c\u6307\u4ee3\u4efb\u52a1\u4e0a\u4ecd\u6709\u4e0d\u8db3\u3002\u672c\u6587\u63d0\u51fa\u7684Ref-Long\u57fa\u51c6\u6d4b\u8bd5\u586b\u8865\u4e86\u8fd9\u4e00\u7a7a\u767d\uff0c\u8be5\u57fa\u51c6\u6d4b\u8bd5\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u6a21\u578b\u5c06\u7279\u5b9a\u4fe1\u606f\u5173\u8054\u5230\u957f\u6587\u672c\u7684\u80fd\u529b\uff0c\u5f3a\u8c03\u4e0a\u4e0b\u6587\u5173\u7cfb\u800c\u975e\u7b80\u5355\u68c0\u7d22\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u5305\u62ecGPT-4o\u5728\u5185\u768413\u79cd\u6a21\u578b\u5728\u8be5\u4efb\u52a1\u4e0a\u5747\u5b58\u5728\u660e\u663e\u77ed\u677f\uff0c\u540e\u7eed\u7684\u8be6\u7ec6\u5206\u6790\u4e5f\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u8fd9\u4e9b\u6a21\u578b\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.08965", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08965", "abs": "https://arxiv.org/abs/2507.08965", "authors": ["Kevin Rojas", "Ye He", "Chieh-Hsin Lai", "Yuta Takida", "Yuki Mitsufuji", "Molei Tao"], "title": "Theory-Informed Improvements to Classifier-Free Guidance for Discrete Diffusion Models", "comment": null, "summary": "Classifier-Free Guidance (CFG) is a widely used technique for conditional\ngeneration and improving sample quality in continuous diffusion models, and\nrecent works have extended it to discrete diffusion. This paper theoretically\nanalyzes CFG in the context of masked discrete diffusion, focusing on the role\nof guidance schedules. Our analysis shows that high guidance early in sampling\n(when inputs are heavily masked) harms generation quality, while late-stage\nguidance has a larger effect. These findings provide a theoretical explanation\nfor empirical observations in recent studies on guidance schedules. The\nanalysis also reveals an imperfection of the current CFG implementations. These\nimplementations can unintentionally cause imbalanced transitions, such as\nunmasking too rapidly during the early stages of generation, which degrades the\nquality of the resulting samples. To address this, we draw insight from the\nanalysis and propose a novel classifier-free guidance mechanism empirically\napplicable to any discrete diffusion. Intuitively, our method smoothens the\ntransport between the data distribution and the initial (masked/uniform)\ndistribution, which results in improved sample quality. Remarkably, our method\nis achievable via a simple one-line code change. The efficacy of our method is\nempirically demonstrated with experiments on ImageNet (masked discrete\ndiffusion) and QM9 (uniform discrete diffusion).", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u79bb\u6563\u6269\u6563\u6a21\u578b\u4e2d\u7684\u5206\u7c7b\u5668\u514d\u8d39\u6307\u5bfc\uff08CFG\uff09\u53ca\u5176\u6307\u5bfc\u8ba1\u5212\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u65e9\u671f\u9ad8\u6307\u5bfc\u4f1a\u635f\u5bb3\u751f\u6210\u8d28\u91cf\uff0c\u800c\u540e\u671f\u6307\u5bfc\u6548\u679c\u66f4\u597d\u3002\u76ee\u524d CFG \u7684\u5b9e\u73b0\u5b58\u5728\u7f3a\u9677\uff0c\u4f1a\u5bfc\u81f4\u4e0d\u5e73\u8861\u7684\u8f6c\u6362\u3002\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b0\u578b CFG \u673a\u5236\uff0c\u53ef\u63d0\u9ad8\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u5728\u8fde\u7eed\u6269\u6563\u6a21\u578b\u4e2d\uff0c\u5206\u7c7b\u5668\u514d\u8d39\u6307\u5bfc\uff08CFG\uff09\u662f\u4e00\u79cd\u5e7f\u6cdb\u4f7f\u7528\u7684\u6761\u4ef6\u751f\u6210\u548c\u63d0\u9ad8\u6837\u672c\u8d28\u91cf\u7684\u6280\u672f\uff0c\u8fd1\u671f\u7814\u7a76\u5df2\u5c06\u5176\u6269\u5c55\u5230\u79bb\u6563\u6269\u6563\u3002\u7136\u800c\uff0c\u5bf9\u4e8e CFG \u5728\u79bb\u6563\u6269\u6563\uff08\u5c24\u5176\u662f\u63a9\u853d\u79bb\u6563\u6269\u6563\uff09\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u6307\u5bfc\u8ba1\u5212\u65b9\u9762\uff0c\u4ecd\u7136\u7f3a\u4e4f\u7406\u8bba\u5206\u6790\u3002", "method": "\u672c\u6587\u7406\u8bba\u5206\u6790\u4e86\u63a9\u853d\u79bb\u6563\u6269\u6563\u4e2d CFG \u7684\u6307\u5bfc\u8ba1\u5212\u7684\u4f5c\u7528\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u7c7b\u5668\u514d\u8d39\u6307\u5bfc\u673a\u5236\uff0c\u901a\u8fc7\u5e73\u6ed1\u6570\u636e\u5206\u5e03\u548c\u521d\u59cb\uff08\u63a9\u853d/\u7edf\u4e00\uff09\u5206\u5e03\u4e4b\u95f4\u7684\u4f20\u8f93\uff0c\u53ef\u4ee5\u7b80\u5355\u5730\u901a\u8fc7\u4e00\u884c\u4ee3\u7801\u66f4\u6539\u5b9e\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u6307\u5bfc\u65e9\u671f\u7684\u6307\u5bfc\u4f1a\u635f\u5bb3\u751f\u6210\u8d28\u91cf\uff0c\u800c\u540e\u671f\u6307\u5bfc\u7684\u5f71\u54cd\u66f4\u5927\u3002\u76ee\u524d\u7684 CFG \u5b9e\u73b0\u5b58\u5728\u7f3a\u9677\uff0c\u4f1a\u5bfc\u81f4\u4e0d\u5e73\u8861\u7684\u8f6c\u6362\uff0c\u4ece\u800c\u964d\u4f4e\u6837\u672c\u8d28\u91cf\u3002\u6240\u63d0\u51fa\u7684\u65b0\u6307\u5bfc\u673a\u5236\u901a\u8fc7\u5e73\u6ed1\u4f20\u8f93\u6765\u63d0\u9ad8\u6837\u672c\u8d28\u91cf\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u5206\u6790\u8868\u660e\uff0c\u5728\u91c7\u6837\u65e9\u671f\uff08\u8f93\u5165\u88ab\u5927\u91cf\u63a9\u853d\u65f6\uff09\u9ad8\u6307\u5bfc\u4f1a\u635f\u5bb3\u751f\u6210\u8d28\u91cf\uff0c\u800c\u5728\u91c7\u6837\u540e\u671f\u6307\u5bfc\u5219\u5f71\u54cd\u66f4\u5927\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u8fd1\u671f\u5173\u4e8e\u6307\u5bfc\u8ba1\u5212\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u7406\u8bba\u89e3\u91ca\u3002\u8be5\u5206\u6790\u8fd8\u63ed\u793a\u4e86\u5f53\u524d CFG \u5b9e\u73b0\u7684\u4e00\u4e2a\u7f3a\u9677\uff0c\u5373\u5f53\u524d\u5b9e\u73b0\u53ef\u80fd\u65e0\u610f\u4e2d\u5bfc\u81f4\u8fc7\u6e21\u4e0d\u5e73\u8861\uff0c\u4f8b\u5982\u5728\u751f\u6210\u65e9\u671f\u8fc7\u5feb\u5730\u53d6\u6d88\u63a9\u853d\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u751f\u6210\u6837\u672c\u7684\u8d28\u91cf\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4ece\u5206\u6790\u4e2d\u83b7\u5f97\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53ef\u7528\u4e8e\u4efb\u4f55\u79bb\u6563\u6269\u6563\u7684\u5206\u7c7b\u5668\u514d\u8d39\u6307\u5bfc\u673a\u5236\u3002\u901a\u8fc7\u5728 ImageNet\uff08\u63a9\u853d\u79bb\u6563\u6269\u6563\uff09\u548c QM9\uff08\u7edf\u4e00\u79bb\u6563\u6269\u6563\uff09\u4e0a\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10007", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10007", "abs": "https://arxiv.org/abs/2507.10007", "authors": ["Zijun Chen", "Wenbo Hu", "Richang Hong"], "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning", "comment": null, "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6ce8\u610f\u529b\u5934\u6fc0\u6d3b\u7684\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5668\uff0c\u901a\u8fc7\u6821\u51c6\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u7684\u51c6\u786e\u6027\u6765\u63d0\u9ad8\u5176\u53ef\u9760\u6027\uff0c\u5e76\u5728\u591a\u9879\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u867d\u7136\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u548c\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u4e2d\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u4e2d\u95f4\u6b65\u9aa4\u7684\u9519\u8bef\u7d2f\u79ef\u5e38\u5e38\u4f1a\u524a\u5f31\u5176\u53ef\u9760\u6027\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8CoT\u63a8\u7406\u7684\u53ef\u9760\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5185\u5728\u7684\u771f\u5b9e\u6027\u7f16\u7801\u6765\u6821\u51c6\u601d\u7ef4\u94fe\uff08CoT\uff09\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002\u7814\u7a76\u4eba\u5458\u53d1\u73b0\uff0c\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u6fc0\u6d3b\u80fd\u591f\u53ef\u9760\u5730\u53cd\u6620CoT\u63a8\u7406\u6b65\u9aa4\u7684\u771f\u5b9e\u6027\u3002\u57fa\u4e8e\u6b64\uff0c\u4ed6\u4eec\u8bad\u7ec3\u4e86\u4e00\u4e2a\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5668\uff0c\u4f7f\u7528\u8fd9\u4e9b\u5bf9\u771f\u5b9e\u6027\u654f\u611f\u7684\u6fc0\u6d3b\u6765\u8bc4\u4f30\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u7684\u6b63\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u675f\u641c\u7d22\u52a8\u6001\u9009\u62e9\u6700\u5408\u7406\u7684\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u3001\u7b26\u53f7\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff08\u5982\u5c11\u6837\u672cCoT\u3001\u81ea\u6d3d\u6027\u3001\u81ea\u8bc4\u4f30\u5f15\u5bfc\u675f\u641c\u7d22\uff09\uff0c\u5728\u5355\u4e00\u6a21\u5f0f\u548c\u591a\u6a21\u5f0f\u8bbe\u7f6e\u4e2d\u5747\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u5927\u89c4\u6a21\u63a8\u7406\u6a21\u578b\u4e0a\u7684\u9a8c\u8bc1\u4e5f\u8bc1\u5b9e\u4e86\u5176\u5bf9\u4e13\u95e8\u63a8\u7406\u6a21\u578b\u7684\u9002\u7528\u6027\u3002\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86\u6a21\u578b\u81ea\u6211\u7ea0\u6b63\u80fd\u529b\u5728CoT\u63a8\u7406\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u3001\u7b26\u53f7\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u5355\u4e00\u6a21\u5f0f\u548c\u591a\u6a21\u5f0f\u8bbe\u7f6e\u4e2d\u90fd\u8868\u73b0\u51fa\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.10047", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10047", "abs": "https://arxiv.org/abs/2507.10047", "authors": ["Marc Kaufeld", "Mattia Piccinini", "Johannes Betz"], "title": "MP-RBFN: Learning-based Vehicle Motion Primitives using Radial Basis Function Networks", "comment": "8 pages, Submitted to the IEEE International Conference on\n  Intelligent Transportation Systems (ITSC 2025), Australia", "summary": "This research introduces MP-RBFN, a novel formulation leveraging Radial Basis\nFunction Networks for efficiently learning Motion Primitives derived from\noptimal control problems for autonomous driving. While traditional motion\nplanning approaches based on optimization are highly accurate, they are often\ncomputationally prohibitive. In contrast, sampling-based methods demonstrate\nhigh performance but impose constraints on the geometric shape of trajectories.\nMP-RBFN combines the strengths of both by coupling the high-fidelity trajectory\ngeneration of sampling-based methods with an accurate description of vehicle\ndynamics. Empirical results show compelling performance compared to previous\nmethods, achieving a precise description of motion primitives at low inference\ntimes. MP-RBFN yields a seven times higher accuracy in generating optimized\nmotion primitives compared to existing semi-analytic approaches. We demonstrate\nthe practical applicability of MP-RBFN for motion planning by integrating the\nmethod into a sampling-based trajectory planner. MP-RBFN is available as\nopen-source software at https://github.com/TUM-AVS/RBFN-Motion-Primitives.", "AI": {"tldr": "MP-RBFN \u901a\u8fc7\u7ed3\u5408\u91c7\u6837\u65b9\u6cd5\u548c\u8f66\u8f86\u52a8\u529b\u5b66\u63cf\u8ff0\uff0c\u63d0\u9ad8\u4e86\u8fd0\u52a8\u539f\u8bed\u5b66\u4e60\u7684\u6548\u7387\u548c\u7cbe\u5ea6\uff0c\u9002\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u8fd0\u52a8\u89c4\u5212\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u57fa\u4e8e\u4f18\u5316\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u4ee5\u53ca\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u5bf9\u8f68\u8ff9\u51e0\u4f55\u5f62\u72b6\u6709\u9650\u5236\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa MP-RBFN \u6765\u7ed3\u5408\u4e24\u8005\u7684\u4f18\u70b9\u3002", "method": "MP-RBFN \u662f\u4e00\u79cd\u5229\u7528\u5f84\u5411\u57fa\u51fd\u6570\u7f51\u7edc\uff08Radial Basis Function Networks\uff09\u6765\u6709\u6548\u5b66\u4e60\u6700\u4f18\u63a7\u5236\u95ee\u9898\u4e2d\u4e3a\u81ea\u52a8\u9a7e\u9a76\u63a8\u5bfc\u51fa\u7684\u8fd0\u52a8\u539f\u8bed\uff08Motion Primitives\uff09\u7684\u65b0\u9896\u65b9\u6cd5\u3002", "result": "MP-RBFN \u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u6bd4\u5148\u524d\u65b9\u6cd5\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u4ee5\u8f83\u4f4e\u7684\u63a8\u7406\u65f6\u95f4\u7cbe\u786e\u63cf\u8ff0\u8fd0\u52a8\u539f\u8bed\uff0c\u5e76\u4e14\u5728\u751f\u6210\u4f18\u5316\u8fd0\u52a8\u539f\u8bed\u65b9\u9762\u6bd4\u73b0\u6709\u7684\u534a\u89e3\u6790\u65b9\u6cd5\u63d0\u9ad8\u4e86\u4e03\u500d\u7684\u7cbe\u5ea6\u3002", "conclusion": "MP-RBFN \u7ed3\u5408\u4e86\u57fa\u4e8e\u91c7\u6837\u7684\u65b9\u6cd5\u7684\u9ad8\u4fdd\u771f\u8f68\u8ff9\u751f\u6210\u80fd\u529b\u548c\u5bf9\u8f66\u8f86\u52a8\u529b\u5b66\u7684\u7cbe\u786e\u63cf\u8ff0\uff0c\u5728\u8f83\u4f4e\u7684\u63a8\u7406\u65f6\u95f4\u4e0b\u5b9e\u73b0\u4e86\u8fd0\u52a8\u539f\u8bed\u7684\u7cbe\u786e\u63cf\u8ff0\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u534a\u89e3\u6790\u65b9\u6cd5\u4e03\u500d\u7684\u7cbe\u5ea6\uff0c\u5c55\u793a\u4e86\u5176\u5728\u8fd0\u52a8\u89c4\u5212\u4e2d\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09216", "abs": "https://arxiv.org/abs/2507.09216", "authors": ["Jingguo Liu", "Han Yu", "Shigang Li", "Jianfeng Li"], "title": "360-Degree Full-view Image Segmentation by Spherical Convolution compatible with Large-scale Planar Pre-trained Models", "comment": "This paper is accecpted by ICMEW 2025", "summary": "Due to the current lack of large-scale datasets at the million-scale level,\ntasks involving panoramic images predominantly rely on existing two-dimensional\npre-trained image benchmark models as backbone networks. However, these\nnetworks are not equipped to recognize the distortions and discontinuities\ninherent in panoramic images, which adversely affects their performance in such\ntasks. In this paper, we introduce a novel spherical sampling method for\npanoramic images that enables the direct utilization of existing pre-trained\nmodels developed for two-dimensional images. Our method employs spherical\ndiscrete sampling based on the weights of the pre-trained models, effectively\nmitigating distortions while achieving favorable initial training values.\nAdditionally, we apply the proposed sampling method to panoramic image\nsegmentation, utilizing features obtained from the spherical model as masks for\nspecific channel attentions, which yields commendable results on commonly used\nindoor datasets, Stanford2D3D.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7403\u5f62\u91c7\u6837\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u5168\u666f\u56fe\u50cf\u5904\u7406\u4e2d\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u5b58\u5728\u7684\u7578\u53d8\u95ee\u9898\uff0c\u5e76\u5728\u5206\u5272\u4efb\u52a1\u4e2d\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u7f3a\u4e4f\u5927\u89c4\u6a21\u5168\u666f\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5bfc\u81f4\u73b0\u6709\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u5904\u7406\u5168\u666f\u56fe\u50cf\u65f6\u5b58\u5728\u7578\u53d8\u548c\u4e0d\u8fde\u7eed\u6027\u8bc6\u522b\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u6a21\u578b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u7403\u5f62\u91c7\u6837\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u6743\u91cd\u8fdb\u884c\u7403\u5f62\u79bb\u6563\u91c7\u6837\uff0c\u4ee5\u5904\u7406\u5168\u666f\u56fe\u50cf\u7684\u7578\u53d8\u548c\u4e0d\u8fde\u7eed\u6027\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5168\u666f\u56fe\u50cf\u5206\u5272\uff0c\u901a\u8fc7\u7403\u5f62\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u4f5c\u4e3a\u7279\u5b9a\u901a\u9053\u6ce8\u610f\u529b\u7684\u63a9\u7801\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u5168\u666f\u56fe\u50cf\u7684\u7578\u53d8\u95ee\u9898\uff0c\u5e76\u4e3a\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6709\u5229\u7684\u521d\u59cb\u503c\u3002\u5728\u5168\u666f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u7403\u5f62\u6a21\u578b\u63d0\u53d6\u7684\u7279\u5f81\u4f5c\u4e3a\u901a\u9053\u6ce8\u610f\u529b\u63a9\u7801\uff0c\u5728Stanford2D3D\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7403\u5f62\u91c7\u6837\u65b9\u6cd5\u6709\u6548\u89e3\u51b3\u4e86\u5168\u666f\u56fe\u50cf\u5904\u7406\u4e2d\u73b0\u6709\u4e8c\u7ef4\u9884\u8bad\u7ec3\u6a21\u578b\u65e0\u6cd5\u8bc6\u522b\u7578\u53d8\u548c\u4e0d\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5168\u666f\u56fe\u50cf\u5206\u5272\u4efb\u52a1\uff0c\u5728Stanford2D3D\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002"}}
{"id": "2507.09706", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09706", "abs": "https://arxiv.org/abs/2507.09706", "authors": ["Asma Al-Othni", "Saif Al-Kuwari", "Mohammad Mahdi Nasiri Fatmehsari", "Kamila Zaman", "Ebrahim Ardeshir Larijani"], "title": "Hybrid Quantum-Classical Generative Adversarial Networks with Transfer Learning", "comment": "13 pages, 24 figures", "summary": "Generative Adversarial Networks (GANs) have demonstrated immense potential in\nsynthesizing diverse and high-fidelity images. However, critical questions\nremain unanswered regarding how quantum principles might best enhance their\nrepresentational and computational capacity. In this paper, we investigate\nhybrid quantum-classical GAN architectures supplemented by transfer learning to\nsystematically examine whether incorporating Variational Quantum Circuits\n(VQCs) into the generator, the discriminator, or both improves performance over\na fully classical baseline. Our findings indicate that fully hybrid models,\nwhich incorporate VQCs in both the generator and the discriminator,\nconsistently produce images of higher visual quality and achieve more favorable\nquantitative metrics compared to their fully classical counterparts. In\nparticular, VQCs in the generator accelerate early feature learning, whereas\nthose in the discriminator, despite exhibiting slower initial convergence,\nultimately facilitate more refined synthetic outputs. Moreover, the model\nsustains near-comparable performance even when the dataset size is drastically\nreduced, suggesting that transfer learning and quantum enhancements mitigate\nthe problem of data scarcity. Overall, the results underscore that carefully\nintegrating quantum computing with classical adversarial training and\npretrained feature extraction can considerably enrich GAN-based image\nsynthesis. These insights open avenues for future work on higher-resolution\ntasks, alternative quantum circuit designs, and experimentation with emerging\nquantum hardware.", "AI": {"tldr": "\u901a\u8fc7\u5728\u751f\u6210\u5668\u548c\u5224\u522b\u5668\u4e2d\u90fd\u52a0\u5165\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff0c\u91cf\u5b50\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6bd4\u7eaf\u7ecf\u5178\u7f51\u7edc\u751f\u6210\u66f4\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u5904\u7406\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8\u4e86\u91cf\u5b50\u539f\u7406\u5982\u4f55\u66f4\u597d\u5730\u589e\u5f3a\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u7684\u8868\u793a\u548c\u8ba1\u7b97\u80fd\u529b\uff0c\u56e0\u4e3aGANs\u5728\u5408\u6210\u591a\u6837\u5316\u548c\u9ad8\u4fdd\u771f\u56fe\u50cf\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u5173\u952e\u95ee\u9898\u3002", "method": "\u7814\u7a76\u4e86\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u8f85\u4ee5\u8fc1\u79fb\u5b66\u4e60\uff0c\u7cfb\u7edf\u5730\u68c0\u9a8c\u4e86\u5c06\u53d8\u5206\u91cf\u5b50\u7535\u8def\uff08VQCs\uff09\u5f15\u5165\u751f\u6210\u5668\u3001\u5224\u522b\u5668\u6216\u4e24\u8005\u4e2d\u662f\u5426\u80fd\u63d0\u5347\u6027\u80fd\u3002", "result": "\u6df7\u5408\u6a21\u578b\u5728\u4e24\u4e2a\u65b9\u9762\u90fd\u8868\u73b0\u66f4\u597d\uff1a1. \u89c6\u89c9\u8d28\u91cf\u66f4\u9ad8\uff0c\u91cf\u5316\u6307\u6807\u66f4\u4f18\u30022. \u53d8\u5206\u91cf\u5b50\u7535\u8def\u5728\u751f\u6210\u5668\u4e2d\u53ef\u4ee5\u52a0\u901f\u65e9\u671f\u7279\u5f81\u5b66\u4e60\uff0c\u800c\u5224\u522b\u5668\u4e2d\u7684\u53d8\u5206\u91cf\u5b50\u7535\u8def\u867d\u7136\u6536\u655b\u8f83\u6162\uff0c\u4f46\u80fd\u4ea7\u751f\u66f4\u7cbe\u7ec6\u7684\u5408\u6210\u8f93\u51fa\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u6570\u636e\u96c6\u5927\u5c0f\u5927\u5927\u51cf\u5c0f\u65f6\uff0c\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u63a5\u8fd1\u7684\u6027\u80fd\uff0c\u8868\u660e\u8fc1\u79fb\u5b66\u4e60\u548c\u91cf\u5b50\u589e\u5f3a\u53ef\u4ee5\u7f13\u89e3\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\u3002", "conclusion": "\u5c06\u91cf\u5b50\u8ba1\u7b97\u4e0e\u7ecf\u5178\u5bf9\u6297\u6027\u8bad\u7ec3\u4ee5\u53ca\u9884\u8bad\u7ec3\u7279\u5f81\u63d0\u53d6\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u663e\u8457\u4e30\u5bcc\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u7684\u56fe\u50cf\u5408\u6210\u80fd\u529b\u3002\u8fd9\u4e3a\u66f4\u9ad8\u5206\u8fa8\u7387\u7684\u4efb\u52a1\u3001\u66ff\u4ee3\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\u4ee5\u53ca\u65b0\u5174\u91cf\u5b50\u786c\u4ef6\u7684\u5b9e\u9a8c\u63d0\u4f9b\u4e86\u672a\u6765\u5de5\u4f5c\u7684\u65b9\u5411\u3002"}}
{"id": "2507.09509", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09509", "abs": "https://arxiv.org/abs/2507.09509", "authors": ["Patr\u00edcia Schmidtov\u00e1", "Niyati Bafna", "Seth Aycock", "Gianluca Vico", "Wiktor Kamzela", "Katharina H\u00e4mmerl", "Vil\u00e9m Zouhar"], "title": "How Important is `Perfect' English for Machine Translation Prompts?", "comment": null, "summary": "Large language models (LLMs) have achieved top results in recent machine\ntranslation evaluations, but they are also known to be sensitive to errors and\nperturbations in their prompts. We systematically evaluate how both humanly\nplausible and synthetic errors in user prompts affect LLMs' performance on two\nrelated tasks: Machine translation and machine translation evaluation. We\nprovide both a quantitative analysis and qualitative insights into how the\nmodels respond to increasing noise in the user prompt.\n  The prompt quality strongly affects the translation performance: With many\nerrors, even a good prompt can underperform a minimal or poor prompt without\nerrors. However, different noise types impact translation quality differently,\nwith character-level and combined noisers degrading performance more than\nphrasal perturbations. Qualitative analysis reveals that lower prompt quality\nlargely leads to poorer instruction following, rather than directly affecting\ntranslation quality itself. Further, LLMs can still translate in scenarios with\noverwhelming random noise that would make the prompt illegible to humans.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u63d0\u793a\u8bcd\u9519\u8bef\u5bf9 LLM \u673a\u5668\u7ffb\u8bd1\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u63d0\u793a\u8bcd\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4e0d\u540c\u7c7b\u578b\u7684\u9519\u8bef\u5f71\u54cd\u4e0d\u540c\uff0c\u5e76\u4e14 LLM \u5728\u9ad8\u566a\u58f0\u4e0b\u4ecd\u80fd\u7ffb\u8bd1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u6700\u8fd1\u7684\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u4f46\u5b83\u4eec\u4e5f\u4ee5\u5bf9\u63d0\u793a\u8bcd\u4e2d\u7684\u9519\u8bef\u548c\u6270\u52a8\u654f\u611f\u800c\u95fb\u540d\u3002", "method": "\u901a\u8fc7\u5728\u7528\u6237\u63d0\u793a\u8bcd\u4e2d\u5f15\u5165\u4eba\u7c7b\u53ef\u8bc6\u522b\u7684\u5408\u7406\u9519\u8bef\u548c\u5408\u6210\u9519\u8bef\uff0c\u5bf9 LLM \u5728\u673a\u5668\u7ffb\u8bd1\u548c\u673a\u5668\u7ffb\u8bd1\u8bc4\u4f30\u4e24\u4e2a\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8fdb\u884c\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u5b9a\u91cf\u5206\u6790\u548c\u5b9a\u6027\u89c1\u89e3\uff0c\u4ee5\u4e86\u89e3\u6a21\u578b\u5bf9\u7528\u6237\u63d0\u793a\u8bcd\u4e2d\u4e0d\u65ad\u589e\u52a0\u7684\u566a\u58f0\u7684\u54cd\u5e94\u60c5\u51b5\u3002", "result": "\u63d0\u793a\u8bcd\u8d28\u91cf\u5bf9\u7ffb\u8bd1\u6027\u80fd\u6709\u663e\u8457\u5f71\u54cd\uff1a\u5728\u5b58\u5728\u5927\u91cf\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u662f\u826f\u597d\u7684\u63d0\u793a\u8bcd\u7684\u8868\u73b0\u4e5f\u53ef\u80fd\u4e0d\u5982\u6ca1\u6709\u9519\u8bef\u7684\u3001\u8d28\u91cf\u8f83\u5dee\u7684\u63d0\u793a\u8bcd\u3002\u4e0d\u540c\u7c7b\u578b\u7684\u566a\u58f0\u5bf9\u7ffb\u8bd1\u8d28\u91cf\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u5b57\u7b26\u7ea7\u548c\u7ec4\u5408\u566a\u58f0\u6bd4\u77ed\u8bed\u6270\u52a8\u5bf9\u6027\u80fd\u7684\u635f\u5bb3\u66f4\u5927\u3002\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0c\u8f83\u4f4e\u7684\u63d0\u793a\u8bcd\u8d28\u91cf\u4e3b\u8981\u5bfc\u81f4\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0b\u964d\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5f71\u54cd\u7ffb\u8bd1\u8d28\u91cf\u672c\u8eab\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u63d0\u793a\u8bcd\u5305\u542b\u5927\u91cf\u968f\u673a\u566a\u58f0\u4ee5\u81f3\u4e8e\u4eba\u7c7b\u65e0\u6cd5\u8bc6\u522b\u7684\u60c5\u51b5\u4e0b\uff0cLLM \u4ecd\u7136\u80fd\u591f\u8fdb\u884c\u7ffb\u8bd1\u3002", "conclusion": "\u63d0\u793a\u8bcd\u8d28\u91cf\u4e25\u91cd\u5f71\u54cd\u7ffb\u8bd1\u6027\u80fd\uff0c\u5b57\u7b26\u7ea7\u548c\u7ec4\u5408\u566a\u58f0\u6bd4\u77ed\u8bed\u6270\u52a8\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\u66f4\u5927\u3002\u8f83\u4f4e\u7684\u63d0\u793a\u8bcd\u8d28\u91cf\u4e3b\u8981\u5bfc\u81f4\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u4e0b\u964d\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u5f71\u54cd\u7ffb\u8bd1\u8d28\u91cf\u672c\u8eab\u3002\u7136\u800c\uff0c\u5373\u4f7f\u5728\u63d0\u793a\u8bcd\u5305\u542b\u5927\u91cf\u4eba\u773c\u96be\u4ee5\u8fa8\u8ba4\u7684\u968f\u673a\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\uff0cLLM \u4ecd\u7136\u80fd\u591f\u8fdb\u884c\u7ffb\u8bd1\u3002"}}
{"id": "2507.08966", "categories": ["cs.LG", "cs.AI", "physics.chem-ph", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.08966", "abs": "https://arxiv.org/abs/2507.08966", "authors": ["Meng Liu", "Karl Leswing", "Simon K. S. Chu", "Farhad Ramezanghorbani", "Griffin Young", "Gabriel Marques", "Prerna Das", "Anjali Panikar", "Esther Jamir", "Mohammed Sulaiman Shamsudeen", "K. Shawn Watts", "Ananya Sen", "Hari Priya Devannagari", "Edward B. Miller", "Muyun Lihan", "Howook Hwang", "Janet Paulsen", "Xin Yu", "Kyle Gion", "Timur Rvachov", "Emine Kucukbenli", "Saee Gopal Paliwal"], "title": "ToxBench: A Binding Affinity Prediction Benchmark with AB-FEP-Calculated Labels for Human Estrogen Receptor Alpha", "comment": "Workshop on Generative AI for Biology at ICML 2025", "summary": "Protein-ligand binding affinity prediction is essential for drug discovery\nand toxicity assessment. While machine learning (ML) promises fast and accurate\npredictions, its progress is constrained by the availability of reliable data.\nIn contrast, physics-based methods such as absolute binding free energy\nperturbation (AB-FEP) deliver high accuracy but are computationally prohibitive\nfor high-throughput applications. To bridge this gap, we introduce ToxBench,\nthe first large-scale AB-FEP dataset designed for ML development and focused on\na single pharmaceutically critical target, Human Estrogen Receptor Alpha\n(ER$\\alpha$). ToxBench contains 8,770 ER$\\alpha$-ligand complex structures with\nbinding free energies computed via AB-FEP with a subset validated against\nexperimental affinities at 1.75 kcal/mol RMSE, along with non-overlapping\nligand splits to assess model generalizability. Using ToxBench, we further\nbenchmark state-of-the-art ML methods, and notably, our proposed DualBind\nmodel, which employs a dual-loss framework to effectively learn the binding\nenergy function. The benchmark results demonstrate the superior performance of\nDualBind and the potential of ML to approximate AB-FEP at a fraction of the\ncomputational cost.", "AI": {"tldr": "ToxBench, a large dataset for ML-based protein-ligand binding affinity prediction using AB-FEP, was created for ER$\\alpha$. The DualBind model significantly improves prediction accuracy compared to other ML methods, showing ML can approximate costly AB-FEP calculations.", "motivation": "The motivation is to bridge the gap between accurate but computationally expensive physics-based methods (like AB-FEP) and faster but less accurate/data-dependent machine learning (ML) methods for protein-ligand binding affinity prediction, which is crucial for drug discovery and toxicity assessment.", "method": "The paper introduces ToxBench, a large-scale dataset of 8,770 Human Estrogen Receptor Alpha (ER$\\alpha$)-ligand complexes with binding free energies computed using Absolute Binding Free Energy Perturbation (AB-FEP). A subset of these energies was validated against experimental data with a Root Mean Square Error (RMSE) of 1.75 kcal/mol. The study benchmarks existing ML methods and introduces DualBind, a model using a dual-loss framework to learn the binding energy function.", "result": "The benchmark results on the ToxBench dataset demonstrate that the DualBind model outperforms other state-of-the-art ML methods, indicating that ML has the potential to approximate AB-FEP calculations efficiently.", "conclusion": "ML models, particularly the proposed DualBind model, show potential in approximating computationally expensive AB-FEP calculations for protein-ligand binding affinity prediction, achieving high accuracy at a reduced computational cost."}}
{"id": "2507.10045", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e0d\u540c\u77e5\u8bc6\u56fe\u8c31\u6a21\u5f0f\u95f4\u7684SPARQL\u7ffb\u8bd1\u80fd\u529b\uff0c\u53d1\u73b0\u5728\u4e0d\u540c\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u4e0b\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u4e14Wikidata\u5230DBpedia\u7684\u7ffb\u8bd1\u6548\u679c\u4f18\u4e8e\u53cd\u5411\u7ffb\u8bd1\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u4e92\u64cd\u4f5c\u6027\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u663e\u8457\u5dee\u8ddd\uff0c\u901a\u8fc7\u4e25\u683c\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728SPARQL\u5230SPARQL\u7ffb\u8bd1\u65b9\u9762\u7684\u6027\u80fd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e24\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u5206\u522b\u5305\u542b100\u4e2a\u5728QALD-9-Plus\u4e2d\u5bf9\u9f50\u7684DBpedia-Wikidata\u67e5\u8be2\u548c100\u4e2a\u5bf9\u9f50\u7684DBLP-OpenAlex\u67e5\u8be2\u3002\u4ed6\u4eec\u6d4b\u8bd5\u4e86Llama-3-8B\u3001DeepSeek-R1-Distill-Llama-70B\u548cMistral-Large-Instruct-2407\u8fd9\u4e09\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u91c7\u7528\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u4ee5\u53ca\u4e24\u79cd\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u7b56\u7565\u8fdb\u884c\u8bc4\u4f30\uff0c\u6700\u540e\u5c06\u6a21\u578b\u8f93\u51fa\u4e0e\u9ec4\u91d1\u7b54\u6848\u8fdb\u884c\u6bd4\u8f83\u5e76\u5bf9\u9519\u8bef\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u4e0d\u540c\u77e5\u8bc6\u56fe\u8c31\u6a21\u5f0f\uff08\u5982DBpedia\u3001Wikidata\u3001DBLP\u548cOpenAlex\uff09\u7684SPARQL\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728Wikidata\u5230DBpedia\u7684\u7ffb\u8bd1\u65b9\u5411\u4e0a\u4f18\u4e8eDBpedia\u5230Wikidata\u7684\u65b9\u5411\u3002", "conclusion": "\u4e0d\u540c\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u5728SPARQL\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u5dee\u5f02\u663e\u8457\uff0c\u4e14Wikidata\u5230DBpedia\u7684\u7ffb\u8bd1\u6548\u679c\u8fdc\u4f18\u4e8eDBpedia\u5230Wikidata\u7684\u7ffb\u8bd1\u3002"}}
{"id": "2507.10055", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10055", "abs": "https://arxiv.org/abs/2507.10055", "authors": ["Muhtadin", "I Wayan Agus Darmawan", "Muhammad Hilmi Rusydiansyah", "I Ketut Eddy Purnama", "Chastine Fatichah", "Mauridhi Hery Purnomo"], "title": "Hand Gesture Recognition for Collaborative Robots Using Lightweight Deep Learning in Real-Time Robotic Systems", "comment": null, "summary": "Direct and natural interaction is essential for intuitive human-robot\ncollaboration, eliminating the need for additional devices such as joysticks,\ntablets, or wearable sensors. In this paper, we present a lightweight deep\nlearning-based hand gesture recognition system that enables humans to control\ncollaborative robots naturally and efficiently. This model recognizes eight\ndistinct hand gestures with only 1,103 parameters and a compact size of 22 KB,\nachieving an accuracy of 93.5%. To further optimize the model for real-world\ndeployment on edge devices, we applied quantization and pruning using\nTensorFlow Lite, reducing the final model size to just 7 KB. The system was\nsuccessfully implemented and tested on a Universal Robot UR5 collaborative\nrobot within a real-time robotic framework based on ROS2. The results\ndemonstrate that even extremely lightweight models can deliver accurate and\nresponsive hand gesture-based control for collaborative robots, opening new\npossibilities for natural human-robot interaction in constrained environments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u975e\u5e38\u8f7b\u91cf\u7ea7\uff087KB\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u53ef\u7528\u4e8e\u76f4\u89c2\u5730\u63a7\u5236\u534f\u4f5c\u673a\u5668\u4eba\uff0c\u51c6\u786e\u7387\u8fbe93.5%\uff08UR5\u673a\u5668\u4eba\uff09\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u76f4\u89c2\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u9700\u8981\u76f4\u63a5\u81ea\u7136\u7684\u4ea4\u4e92\u65b9\u5f0f\uff0c\u65e0\u9700\u989d\u5916\u7684\u5916\u90e8\u8bbe\u5907\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u53c2\u6570\u91cf\u5c11\uff081,103\u4e2a\uff09\uff0c\u6a21\u578b\u5c0f\uff0822KB\uff09\uff0c\u7cbe\u5ea6\u8fbe\u523093.5%\u3002\u901a\u8fc7\u4f7f\u7528TensorFlow Lite\u8fdb\u884c\u91cf\u5316\u548c\u526a\u679d\uff0c\u6a21\u578b\u5927\u5c0f\u8fdb\u4e00\u6b65\u51cf\u5c0f\u52307KB\u3002\u8be5\u7cfb\u7edf\u5df2\u6210\u529f\u96c6\u6210\u5230\u57fa\u4e8eROS2\u7684\u901a\u7528\u673a\u5668\u4ebaUR5\u7684\u5b9e\u65f6\u63a7\u5236\u6846\u67b6\u4e2d\u3002", "result": "\u8be5\u8f7b\u91cf\u7ea7\u624b\u52bf\u8bc6\u522b\u7cfb\u7edf\u5b9e\u73b0\u4e8693.5%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5c06\u6a21\u578b\u5927\u5c0f\u4f18\u5316\u81f37KB\uff0c\u6210\u529f\u5e94\u7528\u4e8eUR5\u534f\u4f5c\u673a\u5668\u4eba\u7684\u5b9e\u65f6\u63a7\u5236\uff0c\u8bc1\u660e\u4e86\u8f7b\u91cf\u7ea7\u6a21\u578b\u5728\u534f\u4f5c\u673a\u5668\u4eba\u63a7\u5236\u4e2d\u7684\u6709\u6548\u6027\u548c\u54cd\u5e94\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u8f7b\u91cf\u7ea7\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u5b9e\u73b0\u81ea\u7136\u4eba\u673a\u4ea4\u4e92\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5373\u4f7f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u4e5f\u80fd\u6709\u6548\u63a7\u5236\u534f\u4f5c\u673a\u5668\u4eba\u3002"}}
{"id": "2507.09217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09217", "abs": "https://arxiv.org/abs/2507.09217", "authors": ["G\u00f6rkay Aydemir"], "title": "Online Long-term Point Tracking in the Foundation Model Era", "comment": "arXiv admin note: substantial text overlap with arXiv:2501.18487", "summary": "Point tracking aims to identify the same physical point across video frames\nand serves as a geometry-aware representation of motion. This representation\nsupports a wide range of applications, from robotics to augmented reality, by\nenabling accurate modeling of dynamic environments. Most existing long-term\ntracking approaches operate in an offline setting, where future frames are\navailable to refine predictions and recover from occlusions. However,\nreal-world scenarios often demand online predictions: the model must operate\ncausally, using only current and past frames. This constraint is critical in\nstreaming video and embodied AI, where decisions must be made immediately based\non past observations. Under such constraints, viewpoint invariance becomes\nessential. Visual foundation models, trained on diverse large-scale datasets,\noffer the potential for robust geometric representations. While they lack\ntemporal reasoning on their own, they can be integrated into tracking pipelines\nto enrich spatial features. In this thesis, we address the problem of long-term\npoint tracking in an online setting, where frames are processed sequentially\nwithout access to future information or sliding windows. We begin by evaluating\nthe suitability of visual foundation models for this task and find that they\ncan serve as useful initializations and be integrated into tracking pipelines.\nHowever, to enable long-term tracking in an online setting, a dedicated design\nis still required. In particular, maintaining coherence over time in this\ncausal regime requires memory to propagate appearance and context across\nframes. To address this, we introduce Track-On, a transformer-based model that\ntreats each tracked point as a query and processes video frames one at a time.\nTrack-On sets a new state of the art across seven public benchmarks,\ndemonstrating the feasibility of long-term tracking without future access.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Track-On \u7684\u65b0\u6a21\u578b\uff0c\u53ef\u5728\u65e0\u6cd5\u8bbf\u95ee\u672a\u6765\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u6709\u6548\u7684\u957f\u671f\u70b9\u8ddf\u8e2a\u3002", "motivation": "\u89e3\u51b3\u5728\u7ebf\u8bbe\u7f6e\u4e0b\u7684\u957f\u671f\u70b9\u8ddf\u8e2a\u95ee\u9898\uff0c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u6a21\u578b\u5fc5\u987b\u4ec5\u4f7f\u7528\u5f53\u524d\u548c\u8fc7\u53bb\u7684\u5e27\u8fdb\u884c\u56e0\u679c\u9884\u6d4b\uff0c\u8fd9\u5728\u6d41\u5a92\u4f53\u89c6\u9891\u548c\u5177\u8eab\u4eba\u5de5\u667a\u80fd\u7b49\u573a\u666f\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Track-On \u7684 Transformer \u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5c06\u6bcf\u4e2a\u8ddf\u8e2a\u70b9\u89c6\u4e3a\u67e5\u8be2\uff0c\u5e76\u9010\u4e00\u5904\u7406\u89c6\u9891\u5e27\uff0c\u4ee5\u5728\u56e0\u679c\u73af\u5883\u4e2d\u7ef4\u6301\u957f\u671f\u8ddf\u8e2a\u7684\u8fde\u8d2f\u6027\u3002", "result": "Track-On \u5728\u4e03\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u65e0\u6cd5\u8bbf\u95ee\u672a\u6765\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u957f\u671f\u8ddf\u8e2a\u662f\u53ef\u884c\u7684\u3002", "conclusion": "Track-On \u4f5c\u4e3a\u4e00\u4e2a\u57fa\u4e8e Transformer \u7684\u6a21\u578b\uff0c\u901a\u8fc7\u5c06\u6bcf\u4e2a\u8ddf\u8e2a\u70b9\u89c6\u4e3a\u67e5\u8be2\u5e76\u9010\u4e00\u5904\u7406\u89c6\u9891\u5e27\uff0c\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5728\u65e0\u6cd5\u8bbf\u95ee\u672a\u6765\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u957f\u671f\u8ddf\u8e2a\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.09715", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09715", "abs": "https://arxiv.org/abs/2507.09715", "authors": ["Mustafa Bakr", "Mohammed Alghadeer", "Simon Pettersson Fors", "Simone D. Fasciati", "Shuxiang Cao", "Atharv Mahajan", "Smain Amari", "Anton Frisk Kockum", "Peter Leek"], "title": "Intrinsic Multi-Mode Interference for Passive Suppression of Purcell Decay in Superconducting Circuits", "comment": null, "summary": "Decoherence due to radiative decay remains an important consideration in\nscaling superconducting quantum processors. We introduce a passive,\ninterference-based methodology for suppressing radiative decay using only the\nintrinsic multi-mode structured environment of superconducting circuits. By\ntaking into account the full electromagnetic mode-mode couplings within the\ndevice, we derive analytic conditions that enable destructive interference.\nThese conditions are realized by introducing controlled geometric asymmetries\n-- such as localized perturbations to the transmon capacitor -- which increase\nmode hybridization and activate interference between multiple decay pathways.\nWe validate this methodology using perturbation theory, full-wave\nelectromagnetic simulations, and experimental measurements of a symmetry-broken\ntransmon qubit with improved coherence times.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8d85\u5bfc\u7535\u8def\u7684\u56fa\u6709\u7ed3\u6784\u6765\u6291\u5236\u8f90\u5c04\u8870\u51cf\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u4e0d\u5bf9\u79f0\u6027\u6765\u6fc0\u6d3b\u7834\u574f\u6027\u5e72\u6d89\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86 Transmon \u06a9\u0647\u7684\u76f8\u5e72\u65f6\u95f4\u3002", "motivation": "\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u7684\u6269\u5c55\u53d7\u5230\u8f90\u5c04\u8870\u51cf\u5f15\u8d77\u7684\u9000\u76f8\u5e72\u6548\u5e94\u7684\u963b\u788d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8d85\u5bfc\u7535\u8def\u56fa\u6709\u7684\u591a\u6a21\u7ed3\u6784\u73af\u5883\uff0c\u901a\u8fc7\u57fa\u4e8e\u5e72\u6d89\u7684\u88ab\u52a8\u65b9\u6cd5\u6765\u6291\u5236\u8f90\u5c04\u8870\u51cf\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u5668\u4ef6\u5185\u90e8\u5b8c\u6574\u7684\u7535\u78c1\u6a21\u5f0f-\u6a21\u5f0f\u8026\u5408\uff0c\u5e76\u63a8\u5bfc\u4e86\u89e3\u6790\u6761\u4ef6\u4ee5\u5b9e\u73b0\u7834\u574f\u6027\u5e72\u6d89\u3002", "result": "\u5b9e\u73b0\u4e86\u5bf9\u79f0\u6027\u7834\u7f3a\u7684 Transmon \u06a9\u0647\uff0c\u76f8\u5e72\u65f6\u95f4\u5f97\u5230\u6539\u5584\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u53d7\u63a7\u7684\u51e0\u4f55\u4e0d\u5bf9\u79f0\u6027\uff08\u4f8b\u5982\uff0c\u5bf9 Transmon \u7535\u5bb9\u5668\u8fdb\u884c\u5c40\u90e8\u6270\u52a8\uff09\u53ef\u4ee5\u5b9e\u73b0\u7834\u574f\u6027\u5e72\u6d89\uff0c\u8fd9\u4f1a\u589e\u52a0\u6a21\u5f0f\u6742\u5316\u5e76\u6fc0\u6d3b\u591a\u6761\u8870\u51cf\u8def\u5f84\u4e4b\u95f4\u7684\u5e72\u6d89\u3002\u6211\u4eec\u901a\u8fc7\u5fae\u6270\u7406\u8bba\u3001\u5168\u6ce2\u7535\u78c1\u6a21\u62df\u548c\u5b9e\u9a8c\u6d4b\u91cf\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5bf9\u79f0\u6027\u7834\u7f3a\u7684 Transmon \u06a9\u0647\u7684\u76f8\u5e72\u65f6\u95f4\u6709\u6240\u63d0\u9ad8\u3002"}}
{"id": "2507.09536", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09536", "abs": "https://arxiv.org/abs/2507.09536", "authors": ["Daniela Kazakouskaya", "Timothee Mickus", "Janine Siewert"], "title": "Adapting Definition Modeling for New Languages: A Case Study on Belarusian", "comment": "To appear at SlavicNLP 2025", "summary": "Definition modeling, the task of generating new definitions for words in\ncontext, holds great prospect as a means to assist the work of lexicographers\nin documenting a broader variety of lects and languages, yet much remains to be\ndone in order to assess how we can leverage pre-existing models for as-of-yet\nunsupported languages. In this work, we focus on adapting existing models to\nBelarusian, for which we propose a novel dataset of 43,150 definitions. Our\nexperiments demonstrate that adapting a definition modeling systems requires\nminimal amounts of data, but that there currently are gaps in what automatic\nmetrics do capture.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5c06\u5b9a\u4e49\u5efa\u6a21\u6280\u672f\u5e94\u7528\u4e8e\u767d\u4fc4\u7f57\u65af\u8bed\uff0c\u8bc1\u660e\u4e86\u53ea\u9700\u8981\u5c11\u91cf\u6570\u636e\u5373\u53ef\u8fdb\u884c\u6539\u7f16\uff0c\u4f46\u540c\u65f6\u4e5f\u6307\u51fa\u4e86\u5f53\u524d\u81ea\u52a8\u8bc4\u4f30\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u65e8\u5728\u63a2\u7d22\u5982\u4f55\u5229\u7528\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u652f\u6301\u65b0\u7684\u3001\u5c1a\u4e0d\u652f\u6301\u7684\u8bed\u8a00\uff0c\u4ee5\u534f\u52a9\u8bcd\u5178\u7f16\u7e82\u8005\u8bb0\u5f55\u66f4\u591a\u4e0d\u540c\u7c7b\u578b\u7684\u8bed\u4f53\u548c\u8bed\u8a00\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5305\u542b43,150\u4e2a\u5b9a\u4e49\u7684\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u6765\u6539\u7f16\u73b0\u6709\u7684\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u4ee5\u652f\u6301\u767d\u4fc4\u7f57\u65af\u8bed\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6539\u7f16\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u53ea\u9700\u8981\u5f88\u5c11\u7684\u6570\u636e\uff0c\u4f46\u73b0\u6709\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5728\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\u65b9\u9762\u5b58\u5728\u4e00\u4e9b\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6539\u7f16\u73b0\u6709\u7684\u5b9a\u4e49\u5efa\u6a21\u7cfb\u7edf\u4ee5\u652f\u6301\u65b0\u7684\u8bed\u8a00\uff08\u4f8b\u5982\u767d\u4fc4\u7f57\u65af\u8bed\uff09\u662f\u53ef\u884c\u7684\uff0c\u5e76\u4e14\u53ea\u9700\u8981\u5c11\u91cf\u6570\u636e\u3002\u7136\u800c\uff0c\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u5728\u6355\u6349\u6a21\u578b\u6027\u80fd\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\u3002"}}
{"id": "2507.08972", "categories": ["cs.LG", "cs.AI", "physics.comp-ph", "physics.flu-dyn"], "pdf": "https://arxiv.org/pdf/2507.08972", "abs": "https://arxiv.org/abs/2507.08972", "authors": ["Sifan Wang", "Shyam Sankaran", "Panos Stinis", "Paris Perdikaris"], "title": "Simulating Three-dimensional Turbulence with Physics-informed Neural Networks", "comment": "25 pages, 13 figures, 3 tables", "summary": "Turbulent fluid flows are among the most computationally demanding problems\nin science, requiring enormous computational resources that become prohibitive\nat high flow speeds. Physics-informed neural networks (PINNs) represent a\nradically different approach that trains neural networks directly from physical\nequations rather than data, offering the potential for continuous, mesh-free\nsolutions. Here we show that appropriately designed PINNs can successfully\nsimulate fully turbulent flows in both two and three dimensions, directly\nlearning solutions to the fundamental fluid equations without traditional\ncomputational grids or training data. Our approach combines several algorithmic\ninnovations including adaptive network architectures, causal training, and\nadvanced optimization methods to overcome the inherent challenges of learning\nchaotic dynamics. Through rigorous validation on challenging turbulence\nproblems, we demonstrate that PINNs accurately reproduce key flow statistics\nincluding energy spectra, kinetic energy, enstrophy, and Reynolds stresses. Our\nresults demonstrate that neural equation solvers can handle complex chaotic\nsystems, opening new possibilities for continuous turbulence modeling that\ntranscends traditional computational limitations.", "AI": {"tldr": "PINNs\u53ef\u4ee5\u901a\u8fc7\u81ea\u9002\u5e94\u7f51\u7edc\u67b6\u6784\u3001\u56e0\u679c\u8bad\u7ec3\u548c\u9ad8\u7ea7\u4f18\u5316\u65b9\u6cd5\u6765\u6a21\u62df\u590d\u6742\u7684\u6e4d\u6d41\uff0c\u65e0\u9700\u4f20\u7edf\u8ba1\u7b97\u7f51\u683c\u6216\u8bad\u7ec3\u6570\u636e\u3002", "motivation": "\u6e4d\u6d41\u6d41\u4f53\u6a21\u62df\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0cPINNs\u63d0\u4f9b\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u7269\u7406\u65b9\u7a0b\u800c\u975e\u6570\u636e\u8bad\u7ec3\u7684\u5168\u65b0\u65b9\u6cd5\uff0c\u6709\u671b\u5b9e\u73b0\u8fde\u7eed\u3001\u65e0\u7f51\u683c\u7684\u89e3\u3002", "method": "\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u81ea\u9002\u5e94\u7f51\u7edc\u67b6\u6784\u3001\u56e0\u679c\u8bad\u7ec3\u548c\u9ad8\u7ea7\u4f18\u5316\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u5b66\u4e60\u6df7\u6c8c\u52a8\u529b\u5b66\u7684\u6311\u6218\u3002", "result": "PINNs\u80fd\u591f\u51c6\u786e\u91cd\u73b0\u80fd\u91cf\u8c31\u3001\u52a8\u80fd\u3001\u6da1\u91cf\u548c\u96f7\u8bfa\u5e94\u529b\u7b49\u5173\u952e\u6d41\u52a8\u7edf\u8ba1\u6570\u636e\u3002", "conclusion": "PINNs\u80fd\u591f\u6210\u529f\u6a21\u62df\u4e8c\u7ef4\u548c\u4e09\u7ef4\u5168\u6e4d\u6d41\uff0c\u76f4\u63a5\u5b66\u4e60\u6d41\u4f53\u57fa\u672c\u65b9\u7a0b\uff0c\u65e0\u9700\u4f20\u7edf\u8ba1\u7b97\u7f51\u683c\u6216\u8bad\u7ec3\u6570\u636e\u3002"}}
{"id": "2507.10076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10076", "abs": "https://arxiv.org/abs/2507.10076", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "title": "On Gradual Semantics for Assumption-Based Argumentation", "comment": null, "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "AI": {"tldr": "\u4e3a\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff09\u5f15\u5165\u4e86\u65b0\u7684\u6e10\u8fdb\u8bed\u4e49\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7814\u7a76\u7684\u4e0d\u8db3\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u6e10\u8fdb\u8bed\u4e49\u4e3b\u8981\u9488\u5bf9\u62bd\u8c61\u3001\u53cc\u6781\u548c\u5b9a\u91cf\u53cc\u6781\u8bba\u8bc1\u6846\u67b6\uff0c\u800c\u5bf9\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff09\u7684\u7814\u7a76\u4e0d\u8db3\uff0c\u5c3d\u7ba1ABA\u5728\u8bb8\u591a\u5e94\u7528\u4e2d\u53ef\u80fd\u53d7\u76ca\u4e8e\u6e10\u8fdb\u8bed\u4e49\u3002", "method": "\u901a\u8fc7\u5c06ABA\u6846\u67b6\u62bd\u8c61\u4e3a\u53cc\u6781\u96c6\u57fa\u7840\u8bba\u8bc1\u6846\u67b6\uff0c\u5e76\u63a8\u5e7f\u7528\u4e8eQBAF\u7684\u6e10\u8fdb\u8bed\u4e49\uff0c\u6765\u5b9a\u4e49\u6e10\u8fdbABA\u8bed\u4e49\u3002", "result": "\u63d0\u51fa\u7684\u6e10\u8fdbABA\u8bed\u4e49\u6ee1\u8db3\u4e86QBAF\u6e10\u8fdb\u8bed\u4e49\u7684\u7406\u60f3\u6027\u8d28\uff08\u5982\u5e73\u8861\u6027\u548c\u5355\u8c03\u6027\uff09\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u4e0e\u57fa\u4e8e\u8bba\u8bc1\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u548c\u6536\u655b\u6027\u8bc4\u4f30\u3002", "conclusion": "\u672c\u6587\u586b\u8865\u4e86\u5728\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff09\u9886\u57df\u5f15\u5165\u6e10\u8fdb\u8bed\u4e49\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u7684\u6e10\u8fdb\u8bed\u4e49\uff0c\u4e3aABA\u6846\u67b6\u4e2d\u7684\u5047\u8bbe\u8d4b\u4e88\u4e86\u8fa9\u8bc1\u5f3a\u5ea6\u3002"}}
{"id": "2507.10075", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10075", "abs": "https://arxiv.org/abs/2507.10075", "authors": ["Jie Pan", "Tianyi Wang", "Yangyang Wang", "Junfeng Jiao", "Christian Claudel"], "title": "TGLD: A Trust-Aware Game-Theoretic Lane-Changing Decision Framework for Automated Vehicles in Heterogeneous Traffic", "comment": "6 pages, 7 figures, accepted for IEEE International Conference on\n  Intelligent Transportation Systems (ITSC) 2025", "summary": "Automated vehicles (AVs) face a critical need to adopt socially compatible\nbehaviors and cooperate effectively with human-driven vehicles (HVs) in\nheterogeneous traffic environment. However, most existing lane-changing\nframeworks overlook HVs' dynamic trust levels, limiting their ability to\naccurately predict human driver behaviors. To address this gap, this study\nproposes a trust-aware game-theoretic lane-changing decision (TGLD) framework.\nFirst, we formulate a multi-vehicle coalition game, incorporating fully\ncooperative interactions among AVs and partially cooperative behaviors from HVs\ninformed by real-time trust evaluations. Second, we develop an online trust\nevaluation method to dynamically estimate HVs' trust levels during\nlane-changing interactions, guiding AVs to select context-appropriate\ncooperative maneuvers. Lastly, social compatibility objectives are considered\nby minimizing disruption to surrounding vehicles and enhancing the\npredictability of AV behaviors, thereby ensuring human-friendly and\ncontext-adaptive lane-changing strategies. A human-in-the-loop experiment\nconducted in a highway on-ramp merging scenario validates our TGLD approach.\nResults show that AVs can effectively adjust strategies according to different\nHVs' trust levels and driving styles. Moreover, incorporating a trust mechanism\nsignificantly improves lane-changing efficiency, maintains safety, and\ncontributes to transparent and adaptive AV-HV interactions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8f66\u9053\u53d8\u6362\u51b3\u7b56\u6846\u67b6\uff08TGLD\uff09\uff0c\u901a\u8fc7\u6574\u5408\u4fe1\u4efb\u8bc4\u4f30\uff0c\u4f7f\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u80fd\u66f4\u597d\u5730\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u5458\u534f\u4f5c\uff0c\u63d0\u9ad8\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u73b0\u6709\u8f66\u9053\u53d8\u6362\u6846\u67b6\u672a\u80fd\u5145\u5206\u8003\u8651HV\u7684\u52a8\u6001\u4fe1\u4efb\u6c34\u5e73\uff0c\u9650\u5236\u4e86\u5bf9HV\u9a7e\u9a76\u884c\u4e3a\u7684\u51c6\u786e\u9884\u6d4b\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u6574\u5408\u4fe1\u4efb\u56e0\u7d20\u7684\u6846\u67b6\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u4efb\u611f\u77e5\u535a\u5f08\u8bba\u8f66\u9053\u53d8\u6362\u51b3\u7b56\uff08TGLD\uff09\u6846\u67b6\uff0c\u5305\u62ec\u591a\u8f66\u8f86\u8054\u76df\u535a\u5f08\u7684\u6784\u5efa\u4ee5\u53ca\u5728\u7ebf\u4fe1\u4efb\u8bc4\u4f30\u65b9\u6cd5\u7684\u5f00\u53d1\uff0c\u65e8\u5728\u52a8\u6001\u4f30\u8ba1HV\u7684\u4fe1\u4efb\u6c34\u5e73\uff0c\u5e76\u6307\u5bfcAV\u9009\u62e9\u4e0a\u4e0b\u6587\u9002\u5b9c\u7684\u5408\u4f5c\u7b56\u7565\u3002", "result": "\u5728\u9ad8\u901f\u516c\u8def\u531d\u9053\u5408\u5e76\u573a\u666f\u4e2d\u8fdb\u884c\u7684\u4eba\u673a\u8026\u5408\u5b9e\u9a8c\u9a8c\u8bc1\u4e86TGLD\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u663e\u793a\uff0cAVs\u80fd\u591f\u6839\u636e\u4e0d\u540c\u7684HV\u4fe1\u4efb\u6c34\u5e73\u548c\u9a7e\u9a76\u98ce\u683c\u8c03\u6574\u7b56\u7565\uff0c\u4fe1\u4efb\u673a\u5236\u663e\u8457\u63d0\u9ad8\u4e86\u8f66\u9053\u53d8\u6362\u6548\u7387\uff0c\u4fdd\u6301\u4e86\u5b89\u5168\u6027\uff0c\u5e76\u4fc3\u8fdb\u4e86AV-HV\u4e4b\u95f4\u900f\u660e\u548c\u81ea\u9002\u5e94\u7684\u4ea4\u4e92\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4fe1\u4efb\u611f\u77e5\u535a\u5f08\u8bba\u8f66\u9053\u53d8\u6362\u51b3\u7b56\uff08TGLD\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8003\u8651\u4eba\u7c7b\u9a7e\u9a76\u5458\u7684\u52a8\u6001\u4fe1\u4efb\u6c34\u5e73\uff0c\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AV\uff09\u5728\u5f02\u6784\u4ea4\u901a\u73af\u5883\u4e2d\u7684\u793e\u4f1a\u517c\u5bb9\u6027\u548c\u4e0e\u4eba\u7c7b\u9a7e\u9a76\u5458\uff08HV\uff09\u7684\u5408\u4f5c\u6548\u7387\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cTGLD\u6846\u67b6\u80fd\u591f\u6709\u6548\u63d0\u5347\u8f66\u9053\u53d8\u6362\u6548\u7387\u3001\u4fdd\u6301\u5b89\u5168\u6027\uff0c\u5e76\u4fc3\u8fdbAV\u4e0eHV\u4e4b\u95f4\u900f\u660e\u548c\u81ea\u9002\u5e94\u7684\u4ea4\u4e92\u3002"}}
{"id": "2507.09222", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09222", "abs": "https://arxiv.org/abs/2507.09222", "authors": ["Behraj Khan", "Tahir Syed"], "title": "Calibrated and Robust Foundation Models for Vision-Language and Medical Image Tasks Under Distribution Shift", "comment": null, "summary": "Foundation models like CLIP and SAM have transformed computer vision and\nmedical imaging via low-shot transfer learning. However, deployment of these\nmodels hindered by two key challenges: \\textit{distribution shift} between\ntraining and test data, and \\textit{confidence misalignment} that leads to\noverconfident incorrect predictions. These issues manifest differently in\nvision-language classification and medical segmentation tasks, yet existing\nsolutions remain domain-specific. We propose \\textit{StaRFM}, a unified\nframework addressing both challenges. It introduces a Fisher information\npenalty (FIP), extended to 3D medical data via patch-wise regularization, to\nreduce covariate shift in CLIP and SAM embeddings. Additionally, a confidence\nmisalignment penalty (CMP), reformulated for voxel-level predictions,\ncalibrates uncertainty in segmentation tasks. We theoretically derive PAC-Bayes\nbounds showing FIP controls generalization via the Fisher-Rao norm, while CMP\nminimizes calibration error through Brier score optimization. StaRFM shows\nconsistent performance like \\texttt{+}3.5\\% accuracy and 28\\% lower ECE on 19\nvision datasets (e.g., ImageNet, Office-Home), 84.7\\% DSC and 4.8mm HD95 in\nmedical segmentation (e.g., BraTS, ATLAS), and 40\\% lower cross-domain\nperformance gap compared to prior benchmarking methods. The framework is\nplug-and-play, requiring minimal architectural changes for seamless integration\nwith foundation models. Code and models will be released at\nhttps://anonymous.4open.science/r/StaRFM-C0CD/README.md", "AI": {"tldr": "StaRFM\u901a\u8fc7FIP\u548cCMP\u89e3\u51b3\u4e86\u57fa\u7840\u6a21\u578b\u7684\u5206\u5e03\u504f\u79fb\u548c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u95ee\u9898\uff0c\u5728\u89c6\u89c9\u548c\u533b\u5b66\u5f71\u50cf\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u548c\u533b\u5b66\u6210\u50cf\u9886\u57df\u7684\u57fa\u7840\u6a21\u578b\uff08\u5982CLIP\u548cSAM\uff09\u867d\u7136\u5728\u8fc1\u79fb\u5b66\u4e60\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u9762\u4e34\u5206\u5e03\u504f\u79fb\u548c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\uff08\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u9884\u6d4b\uff09\u4e24\u5927\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u5728\u4e0d\u540c\u9886\u57df\u5b58\u5728\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStaRFM\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u5305\u542bFisher\u4fe1\u606f\u60e9\u7f5a\uff08FIP\uff09\u548c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u60e9\u7f5a\uff08CMP\uff09\u3002FIP\u901a\u8fc7\u8865\u4e01\u5f0f\u6b63\u5219\u5316\u6269\u5c55\u52303D\u533b\u5b66\u6570\u636e\uff0c\u4ee5\u51cf\u5c11CLIP\u548cSAM\u5d4c\u5165\u4e2d\u7684\u534f\u53d8\u91cf\u504f\u79fb\u3002CMP\u88ab\u91cd\u65b0\u5236\u5b9a\u7528\u4e8e\u4f53\u7d20\u7ea7\u9884\u6d4b\uff0c\u4ee5\u6821\u51c6\u5206\u5272\u4efb\u52a1\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "result": "\u572819\u4e2a\u89c6\u89c9\u6570\u636e\u96c6\uff08\u5982ImageNet\u3001Office-Home\uff09\u4e0a\uff0cStaRFM\u5b9e\u73b0\u4e86\u5e73\u5747\u51c6\u786e\u7387\u63d0\u53473.5%\u548cECE\u964d\u4f4e28%\u3002\u5728\u533b\u5b66\u5206\u5272\u4efb\u52a1\uff08\u5982BraTS\u3001ATLAS\uff09\u4e0a\uff0c\u5b9e\u73b0\u4e8684.7%\u7684DSC\u548c4.8mm\u7684HD95\u3002\u6b64\u5916\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cStaRFM\u5c06\u8de8\u9886\u57df\u6027\u80fd\u5dee\u8ddd\u964d\u4f4e\u4e8640%\u3002", "conclusion": "StaRFM\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165Fisher\u4fe1\u606f\u60e9\u7f5a\uff08FIP\uff09\u548c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u60e9\u7f5a\uff08CMP\uff09\uff0c\u5206\u522b\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u548c\u7f6e\u4fe1\u5ea6\u9519\u4f4d\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002FIP\u901a\u8fc7Fisher-rao\u8303\u6570\u63a7\u5236\u6cdb\u5316\uff0cCMP\u901a\u8fc7Brier\u5206\u6570\u4f18\u5316\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u3002"}}
{"id": "2507.09716", "categories": ["quant-ph", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.09716", "abs": "https://arxiv.org/abs/2507.09716", "authors": ["Mirco A. Mannucci"], "title": "When the Weak Becomes Strong: Effective Observables via Time-Symmetric Quantum Selection", "comment": "11 pages, 2 figures", "summary": "We investigate the sequential composition of weak values in the framework of\ntime-symmetric quantum mechanics. Specifically, we consider a forward'' weak\nmeasurement from a preselected state $\\ket{\\psi}$ to a post-selected state\n$\\ket{\\phi}$, followed by a reverse'' weak measurement. We show that the\nproduct of these two weak values corresponds to the normalized expectation\nvalue of a strong, state-conditioned observable $B = A P_\\psi A$, where $P_\\psi\n= \\ket{\\psi}\\bra{\\psi}$ is the projector onto the preselected state. Analyzing\nthe structure of $B$, we demonstrate how it encodes interference information,\nparticularly when $\\ket{\\psi}$ is a superposition rather than an eigenstate of\n$A$. This formulation extends naturally to mixed states by replacing $P_\\psi$\nwith a generic density matrix $\\rho$, linking the construction to the formalism\nof generalized quantum measurements. We illustrate practical applications in\nquantum information, including state-specific error witnessing in quantum\ncomputing, and show how the phase of a weak value can be inferred via strong\nmeasurements in the pure-state case.", "AI": {"tldr": "Sequential weak measurements in time-symmetric quantum mechanics are shown to be equivalent to a single strong measurement of a specially constructed observable, which reveals interference effects and has practical applications in quantum information.", "motivation": "To investigate the sequential composition of weak values and their connection to strong observables within the framework of time-symmetric quantum \u05de\u05d9mechanics.", "method": "Investigated the sequential composition of weak values in time-symmetric quantum mechanics, considering forward and reverse weak measurements. Analyzed the structure of the strong, state-conditioned observable B = A P_psi A, where P_psi is the projector onto the preselected state.", "result": "Showed that the product of weak values corresponds to a normalized expectation value of a strong observable B. Demonstrated that B encodes interference information and extended the formulation to mixed states. Illustrated applications in quantum information, including state-specific error witnessing and inferring weak value phase.", "conclusion": "The product of forward and reverse weak values corresponds to the normalized expectation value of a strong, state-conditioned observable B. The structure of B encodes interference information, especially when the preselected state is a superposition. This formulation extends to mixed states and has applications in quantum information, such as state-specific error witnessing and inferring weak value phase via strong measurements."}}
{"id": "2507.09601", "categories": ["cs.CL", "cs.AI", "q-fin.CP"], "pdf": "https://arxiv.org/pdf/2507.09601", "abs": "https://arxiv.org/abs/2507.09601", "authors": ["Hanwool Lee", "Sara Yu", "Yewon Hwang", "Jonghyun Choi", "Heejae Ahn", "Sungbum Jung", "Youngjae Yu"], "title": "NMIXX: Domain-Adapted Neural Embeddings for Cross-Lingual eXploration of Finance", "comment": "Under Review", "summary": "General-purpose sentence embedding models often struggle to capture\nspecialized financial semantics, especially in low-resource languages like\nKorean, due to domain-specific jargon, temporal meaning shifts, and misaligned\nbilingual vocabularies. To address these gaps, we introduce NMIXX (Neural\neMbeddings for Cross-lingual eXploration of Finance), a suite of cross-lingual\nembedding models fine-tuned with 18.8K high-confidence triplets that pair\nin-domain paraphrases, hard negatives derived from a semantic-shift typology,\nand exact Korean-English translations. Concurrently, we release KorFinSTS, a\n1,921-pair Korean financial STS benchmark spanning news, disclosures, research\nreports, and regulations, designed to expose nuances that general benchmarks\nmiss.\n  When evaluated against seven open-license baselines, NMIXX's multilingual\nbge-m3 variant achieves Spearman's rho gains of +0.10 on English FinSTS and\n+0.22 on KorFinSTS, outperforming its pre-adaptation checkpoint and surpassing\nother models by the largest margin, while revealing a modest trade-off in\ngeneral STS performance. Our analysis further shows that models with richer\nKorean token coverage adapt more effectively, underscoring the importance of\ntokenizer design in low-resource, cross-lingual settings. By making both models\nand the benchmark publicly available, we provide the community with robust\ntools for domain-adapted, multilingual representation learning in finance.", "AI": {"tldr": "\u9488\u5bf9\u97e9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u5728\u91d1\u878d\u9886\u57df\u5d4c\u5165\u8868\u793a\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u63d0\u51faNMIXX\u6a21\u578b\u548cKorFinSTS\u57fa\u51c6\u3002NMIXX\u5728\u91d1\u878d\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u63ed\u793a\u4e86\u5206\u8bcd\u5668\u8bbe\u8ba1\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u901a\u7528\u53e5\u5b50\u5d4c\u5165\u6a21\u578b\u5728\u6355\u6349\u91d1\u878d\u9886\u57df\u4e13\u4e1a\u8bed\u4e49\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u97e9\u8bed\u7b49\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u8fd9\u5f52\u56e0\u4e8e\u9886\u57df\u7279\u5b9a\u672f\u8bed\u3001\u65f6\u95f4\u8bed\u4e49\u53d8\u5316\u4ee5\u53ca\u53cc\u8bed\u8bcd\u6c47\u4e0d\u5339\u914d\u7b49\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u5bf918.8K\u4e2a\u9ad8\u7f6e\u4fe1\u5ea6\u4e09\u5143\u7ec4\uff08\u5305\u62ec\u9886\u57df\u5185\u91ca\u4e49\u3001\u57fa\u4e8e\u8bed\u4e49\u8f6c\u79fb\u7c7b\u578b\u7684\u96be\u8d1f\u4f8b\u4ee5\u53ca\u7cbe\u786e\u7684\u97e9\u82f1\u7ffb\u8bd1\uff09\u8fdb\u884c\u5fae\u8c03\uff0c\u6784\u5efa\u4e86NMIXX\uff08Neural eMbeddings for Cross-lingual eXploration of Finance\uff09\u8de8\u8bed\u8a00\u5d4c\u5165\u6a21\u578b\u3002\u540c\u65f6\u53d1\u5e03\u4e86KorFinSTS\uff0c\u4e00\u4e2a\u5305\u542b1,921\u5bf9\u97e9\u82f1\u91d1\u878d\u53e5\u5b50\u76f8\u4f3c\u5ea6\u4efb\u52a1\uff08STS\uff09\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002", "result": "NMIXX\u7684\u591a\u8bed\u8a00bge-m3\u53d8\u4f53\u5728\u82f1\u6587FinSTS\u4e0a\u5b9e\u73b0\u4e86+0.10\u7684Spearman's rho\u589e\u76ca\uff0c\u5728KorFinSTS\u4e0a\u5b9e\u73b0\u4e86+0.22\u7684\u589e\u76ca\uff0c\u4f18\u4e8e\u5176\u5fae\u8c03\u524d\u7684\u68c0\u67e5\u70b9\uff0c\u5e76\u4e14\u5728KorFinSTS\u4e0a\u8d85\u8d8a\u4e86\u5176\u4ed6\u6a21\u578b\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5177\u6709\u66f4\u4e30\u5bcc\u97e9\u8bed\u8bcd\u5143\u8986\u76d6\u7387\u7684\u6a21\u578b\u9002\u5e94\u6548\u679c\u66f4\u597d\uff0c\u8868\u660e\u4e86\u5206\u8bcd\u5668\u8bbe\u8ba1\u5728\u4f4e\u8d44\u6e90\u8de8\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86NMIXX\u6a21\u578b\u548cKorFinSTS\u57fa\u51c6\uff0c\u4e3a\u91d1\u878d\u9886\u57df\u7684\u8de8\u8bed\u8a00\u8868\u793a\u5b66\u4e60\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u5e76\u5f3a\u8c03\u4e86\u5206\u8bcd\u5668\u8bbe\u8ba1\u5728\u4f4e\u8d44\u6e90\u8de8\u8bed\u8a00\u73af\u5883\u4e2d\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.08977", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.08977", "abs": "https://arxiv.org/abs/2507.08977", "authors": ["Carson Dudley", "Reiden Magdaleno", "Christopher Harding", "Marisa Eisenberg"], "title": "Simulation as Supervision: Mechanistic Pretraining for Scientific Discovery", "comment": null, "summary": "Scientific modeling faces a core limitation: mechanistic models offer\ninterpretability but collapse under real-world complexity, while machine\nlearning models are flexible but require large labeled datasets, cannot infer\nunobservable quantities, and operate as black boxes. We introduce\nSimulation-Grounded Neural Networks (SGNNs), a general framework that uses\nmechanistic simulations as training data for neural networks. SGNNs are\npretrained on synthetic corpora spanning diverse model structures, parameter\nregimes, stochasticity, and observational artifacts. We evaluated SGNNs across\nscientific disciplines and modeling tasks, and found that SGNNs achieved\nstate-of-the-art results across settings: for prediction tasks, they nearly\ntripled COVID-19 forecasting skill versus CDC baselines, reduced chemical yield\nprediction error by one third, and maintained accuracy in ecological\nforecasting where task specific models failed. For inference tasks, SGNNs also\naccurately classified the source of information spread in simulated social\nnetworks and enabled supervised learning for unobservable targets, such as\nestimating COVID-19 transmissibility more accurately than traditional methods\neven in early outbreaks. Finally, SGNNs enable back-to-simulation attribution,\na new form of mechanistic interpretability. Given real world input, SGNNs\nretrieve simulations based on what the model has learned to see as most\nsimilar, revealing which underlying dynamics the model believes are active.\nThis provides process-level insight -- what the model thinks is happening --\nnot just which features mattered. SGNNs unify scientific theory with deep\nlearning flexibility and unlock a new modeling paradigm -- transforming\nsimulations from rigid, post hoc tools into flexible sources of supervision,\nenabling robust, interpretable inference even when ground truth is missing.", "AI": {"tldr": "SGNNs \u901a\u8fc7\u4f7f\u7528\u529b\u5b66\u6a21\u62df\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c\u7ed3\u5408\u4e86\u79d1\u5b66\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u673a\u5668\u5b66\u4e60\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u5728\u9884\u6d4b\u548c\u63a8\u7406\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u679c\uff0c\u540c\u65f6\u8fd8\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u6cd5\u3002", "motivation": "\u79d1\u5b66\u5efa\u6a21\u9762\u4e34\u6838\u5fc3\u9650\u5236\uff1a\u529b\u5b66\u6a21\u578b\u53ef\u89e3\u91ca\u4f46\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u590d\u6742\u6027\uff0c\u800c\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7075\u6d3b\u4f46\u9700\u8981\u5927\u91cf\u6807\u8bb0\u6570\u636e\uff0c\u4e14\u65e0\u6cd5\u63a8\u65ad\u4e0d\u53ef\u89c2\u6d4b\u7684\u91cf\u5e76\u4f5c\u4e3a\u9ed1\u7bb1\u8fd0\u4f5c\u3002SGNNs \u7684\u63d0\u51fa\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "SGNNs \u662f\u4e00\u4e2a\u901a\u7528\u6846\u67b6\uff0c\u5b83\u4f7f\u7528\u529b\u5b66\u6a21\u62df\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u6570\u636e\u3002SGNNs \u5728\u8de8\u8d8a\u4e0d\u540c\u6a21\u578b\u7ed3\u6784\u3001\u53c2\u6570\u8303\u56f4\u3001\u968f\u673a\u6027\u548c\u89c2\u6d4b\u504f\u5dee\u7684\u5408\u6210\u6570\u636e\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u3002\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u529b\u5b66\u53ef\u89e3\u91ca\u6027\u5f62\u5f0f\uff1a\u53cd\u5411\u6a21\u62df\u5f52\u56e0\u3002", "result": "SGNNs \u5728\u79d1\u5b66\u9886\u57df\u548c\u5efa\u6a21\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff1a\u5728\u9884\u6d4b\u4efb\u52a1\u4e0a\uff0c\u5b83\u4eec\u51e0\u4e4e\u4f7f COVID-19 \u9884\u6d4b\u80fd\u529b\u76f8\u6bd4 CDC \u57fa\u7ebf\u63d0\u9ad8\u4e86\u4e24\u500d\uff0c\u5316\u5b66\u4ea7\u7387\u9884\u6d4b\u8bef\u5dee\u964d\u4f4e\u4e86\u4e09\u5206\u4e4b\u4e00\uff0c\u5e76\u5728\u751f\u6001\u9884\u6d4b\u4e2d\u4fdd\u6301\u4e86\u51c6\u786e\u6027\uff0c\u800c\u7279\u5b9a\u4efb\u52a1\u7684\u6a21\u578b\u5219\u5931\u8d25\u4e86\u3002\u5728\u63a8\u7406\u4efb\u52a1\u4e0a\uff0cSGNNs \u8fd8\u80fd\u51c6\u786e\u5730\u5bf9\u6a21\u62df\u793e\u4ea4\u7f51\u7edc\u4e2d\u4fe1\u606f\u4f20\u64ad\u7684\u6765\u6e90\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u4e0d\u53ef\u89c2\u6d4b\u76ee\u6807\u7684\u76d1\u7763\u5b66\u4e60\uff0c\u4f8b\u5982\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u51c6\u786e\u5730\u4f30\u8ba1 COVID-19 \u7684\u4f20\u64ad\u6027\uff0c\u5373\u4f7f\u5728\u75ab\u60c5\u65e9\u671f\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "SGNNs \u7edf\u4e00\u4e86\u79d1\u5b66\u7406\u8bba\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u7075\u6d3b\u6027\uff0c\u5e76\u5f00\u542f\u4e86\u4e00\u4e2a\u65b0\u7684\u5efa\u6a21\u8303\u5f0f\u2014\u2014\u5c06\u6a21\u62df\u4ece\u50f5\u5316\u7684\u3001\u4e8b\u540e\u5de5\u5177\u8f6c\u53d8\u4e3a\u7075\u6d3b\u7684\u76d1\u7763\u6765\u6e90\uff0c\u5373\u4f7f\u5728\u7f3a\u4e4f\u771f\u5b9e\u60c5\u51b5\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5b9e\u73b0\u7a33\u5065\u3001\u53ef\u89e3\u91ca\u7684\u63a8\u7406\u3002"}}
{"id": "2507.10106", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10106", "abs": "https://arxiv.org/abs/2507.10106", "authors": ["Harshal Nandigramwar", "Syed Qutub", "Kay-Ulrich Scholl"], "title": "BlueGlass: A Framework for Composite AI Safety", "comment": "Accepted at ICML 2025 [Actionable Interpretability Workshop]", "summary": "As AI systems become increasingly capable and ubiquitous, ensuring the safety\nof these systems is critical. However, existing safety tools often target\ndifferent aspects of model safety and cannot provide full assurance in\nisolation, highlighting a need for integrated and composite methodologies. This\npaper introduces BlueGlass, a framework designed to facilitate composite AI\nsafety workflows by providing a unified infrastructure enabling the integration\nand composition of diverse safety tools that operate across model internals and\noutputs. Furthermore, to demonstrate the utility of this framework, we present\nthree safety-oriented analyses on vision-language models for the task of object\ndetection: (1) distributional evaluation, revealing performance trade-offs and\npotential failure modes across distributions; (2) probe-based analysis of layer\ndynamics highlighting shared hierarchical learning via phase transition; and\n(3) sparse autoencoders identifying interpretable concepts. More broadly, this\nwork contributes foundational infrastructure and findings for building more\nrobust and reliable AI systems.", "AI": {"tldr": "BlueGlass is a new framework that integrates various AI safety tools for better AI system safety. It was tested on object detection models, showing its effectiveness in analyzing model behavior and identifying potential issues.", "motivation": "Existing AI safety tools often target different aspects of model safety and cannot provide full assurance in isolation, highlighting a need for integrated and composite methodologies to ensure the safety of increasingly capable and ubiquitous AI systems.", "method": "BlueGlass is a framework providing a unified infrastructure for integrating and composing diverse AI safety tools that operate across model internals and outputs. The paper also presents three safety-oriented analyses: distributional evaluation, probe-based analysis of layer dynamics, and sparse autoencoders for concept identification.", "result": "The analyses revealed performance trade-offs and potential failure modes across distributions, highlighted shared hierarchical learning via phase transition in layer dynamics, and identified interpretable concepts using sparse autoencoders. The BlueGlass framework facilitates these composite AI safety workflows.", "conclusion": "The paper introduces BlueGlass, a framework for integrating diverse AI safety tools, and demonstrates its utility through three analyses on vision-language models for object detection. This work contributes foundational infrastructure and findings for building more robust and reliable AI systems."}}
{"id": "2507.09230", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09230", "abs": "https://arxiv.org/abs/2507.09230", "authors": ["G. Kutay T\u00fcrkoglu", "Julian Tanke", "Iheb Belgacem", "Lev Markhasin"], "title": "EgoAnimate: Generating Human Animations from Egocentric top-down Views", "comment": "10 pages, 5 figures", "summary": "An ideal digital telepresence experience requires accurate replication of a\nperson's body, clothing, and movements. To capture and transfer these movements\ninto virtual reality, the egocentric (first-person) perspective can be adopted,\nwhich enables the use of a portable and cost-effective device without\nfront-view cameras. However, this viewpoint introduces challenges such as\nocclusions and distorted body proportions.\n  There are few works reconstructing human appearance from egocentric views,\nand none use a generative prior-based approach. Some methods create avatars\nfrom a single egocentric image during inference, but still rely on multi-view\ndatasets during training. To our knowledge, this is the first study using a\ngenerative backbone to reconstruct animatable avatars from egocentric inputs.\nBased on Stable Diffusion, our method reduces training burden and improves\ngeneralizability.\n  Inspired by methods such as SiTH and MagicMan, which perform 360-degree\nreconstruction from a frontal image, we introduce a pipeline that generates\nrealistic frontal views from occluded top-down images using ControlNet and a\nStable Diffusion backbone.\n  Our goal is to convert a single top-down egocentric image into a realistic\nfrontal representation and feed it into an image-to-motion model. This enables\ngeneration of avatar motions from minimal input, paving the way for more\naccessible and generalizable telepresence systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u5229\u7528Stable Diffusion\u548cControlNet\u6280\u672f\uff0c\u4ec5\u901a\u8fc7\u5355\u4e00\u7684\u7b2c\u4e00\u4eba\u79f0\u4fef\u89c6\u56fe\u50cf\uff0c\u5c31\u80fd\u751f\u6210\u903c\u771f\u7684\u6b63\u9762\u865a\u62df\u5f62\u8c61\uff0c\u5e76\u9a71\u52a8\u5176\u52a8\u4f5c\uff0c\u4e3a\u5b9e\u73b0\u66f4\u4fbf\u6377\u7684\u8fdc\u7a0b\u5448\u73b0\u4f53\u9a8c\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u4e3a\u4e86\u5728\u865a\u62df\u73b0\u5b9e\u4e2d\u6355\u6349\u548c\u4f20\u8f93\u4eba\u7684\u8eab\u4f53\u3001\u670d\u88c5\u548c\u52a8\u4f5c\uff0c\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u662f\u4e00\u79cd\u4fbf\u643a\u4e14\u7ecf\u6d4e\u7684\u8bbe\u5907\uff0c\u4f46\u5b83\u4e5f\u5e26\u6765\u4e86\u906e\u6321\u548c\u8eab\u4f53\u6bd4\u4f8b\u5931\u771f\u7b49\u6311\u6218\u3002\u73b0\u6709\u7684\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u91cd\u5efa\u4eba\u4f53\u5916\u89c2\u7684\u5de5\u4f5c\u5f88\u5c11\uff0c\u5e76\u4e14\u6ca1\u6709\u4f7f\u7528\u751f\u6210\u5f0f\u5148\u9a8c\u7684\u65b9\u6cd5\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u5b9e\u73b0\u66f4\u6613\u4e8e\u8bbf\u95ee\u548c\u53ef\u63a8\u5e7f\u7684\u8fdc\u7a0b\u5448\u73b0\u7cfb\u7edf\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528ControlNet\u548cStable Diffusion\u4f5c\u4e3a\u751f\u6210\u6a21\u578b\uff0c\u5c06\u906e\u6321\u7684\u4fef\u89c6\u56fe\u50cf\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u6b63\u9762\u89c6\u56fe\uff0c\u5e76\u5c06\u5176\u8f93\u5165\u5230\u56fe\u50cf\u5230\u8fd0\u52a8\u6a21\u578b\u4e2d\uff0c\u4ece\u800c\u4ece\u6781\u7b80\u7684\u8f93\u5165\u751f\u6210\u865a\u62df\u5f62\u8c61\u7684\u52a8\u4f5c\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u751f\u6210\u5f0f\u4e3b\u5e72\uff08Stable Diffusion\uff09\u6765\u91cd\u5efa\u53ef\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\uff0c\u8be5\u65b9\u6cd5\u6709\u671b\u51cf\u5c11\u8bad\u7ec3\u8d1f\u62c5\u5e76\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\uff0c\u4ece\u800c\u5b9e\u73b0\u4ece\u6781\u7b80\u8f93\u5165\u751f\u6210\u865a\u62df\u5f62\u8c61\u7684\u52a8\u4f5c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4ece\u5355\u4e00\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\uff08\u4fef\u89c6\uff09\u56fe\u50cf\u751f\u6210\u53ef\u9a71\u52a8\u7684\u865a\u62df\u5f62\u8c61\uff08avatar\uff09\u7684\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u6613\u4e8e\u8bbf\u95ee\u548c\u53ef\u63a8\u5e7f\u7684\u8fdc\u7a0b\u5448\u73b0\u7cfb\u7edf\u3002"}}
{"id": "2507.09719", "categories": ["quant-ph", "cs.NI"], "pdf": "https://arxiv.org/pdf/2507.09719", "abs": "https://arxiv.org/abs/2507.09719", "authors": ["Jiaheng Xiong", "Qiaolun Zhang", "Yoann Pi\u00e9tri", "Raja Yehia", "Raouf Boutaba", "Francesco Musumeci", "Massimo Tornatore"], "title": "Power Consumption Analysis of QKD Networks under Different Protocols and Detector Configurations", "comment": null, "summary": "We analyze the power consumption of quantum key distribution (QKD) networks\nunder various protocol and detector configurations. Using realistic network\ntopologies, we evaluate discrete-variable vs continuous-variable QKD and\noptimize device placement, quantifying power trade-offs of SNSPD vs APD\ndetectors and the benefits of optical bypass.", "AI": {"tldr": "Analyzed QKD network power consumption with different protocols and detectors, optimizing placement and quantifying trade-offs.", "motivation": "Analyze the power consumption of quantum key distribution (QKD) networks.", "method": "We evaluate discrete-variable vs continuous-variable QKD and optimize device placement, quantifying power trade-offs of SNSPD vs APD detectors and the benefits of optical bypass.", "result": "Quantified power trade-offs of SNSPD vs APD detectors and the benefits of optical bypass.", "conclusion": "We analyze the power consumption of QKD networks under various configurations."}}
{"id": "2507.09628", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09628", "abs": "https://arxiv.org/abs/2507.09628", "authors": ["Salvatore Citraro", "Edith Haim", "Alessandra Carini", "Cynthia S. Q. Siew", "Giulio Rossetti", "Massimo Stella"], "title": "SpreadPy: A Python tool for modelling spreading activation and superdiffusion in cognitive multiplex networks", "comment": null, "summary": "We introduce SpreadPy as a Python library for simulating spreading activation\nin cognitive single-layer and multiplex networks. Our tool is designed to\nperform numerical simulations testing structure-function relationships in\ncognitive processes. By comparing simulation results with grounded theories in\nknowledge modelling, SpreadPy enables systematic investigations of how\nactivation dynamics reflect cognitive, psychological and clinical phenomena. We\ndemonstrate the library's utility through three case studies: (1) Spreading\nactivation on associative knowledge networks distinguishes students with high\nversus low math anxiety, revealing anxiety-related structural differences in\nconceptual organization; (2) Simulations of a creativity task show that\nactivation trajectories vary with task difficulty, exposing how cognitive load\nmodulates lexical access; (3) In individuals with aphasia, simulated activation\npatterns on lexical networks correlate with empirical error types (semantic vs.\nphonological) during picture-naming tasks, linking network structure to\nclinical impairments. SpreadPy's flexible framework allows researchers to model\nthese processes using empirically derived or theoretical networks, providing\nmechanistic insights into individual differences and cognitive impairments. The\nlibrary is openly available, supporting reproducible research in psychology,\nneuroscience, and education research.", "AI": {"tldr": "SpreadPy\u662f\u4e00\u4e2a\u65b0\u7684Python\u5e93\uff0c\u7528\u4e8e\u6a21\u62df\u8ba4\u77e5\u7f51\u7edc\u4e2d\u7684\u6fc0\u6d3b\u6269\u6563\uff0c\u901a\u8fc7\u6848\u4f8b\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u5728\u533a\u5206\u7126\u8651\u3001\u8c03\u8282\u8ba4\u77e5\u8d1f\u8377\u548c\u7406\u89e3\u8bed\u8a00\u969c\u788d\u65b9\u9762\u7684\u80fd\u529b\uff0c\u652f\u6301\u53ef\u590d\u73b0\u7684\u7814\u7a76\u3002", "motivation": "\u4ecb\u7ecdSpreadPy\u5e93\uff0c\u8be5\u5e93\u65e8\u5728\u8fdb\u884c\u6570\u503c\u6a21\u62df\u4ee5\u68c0\u9a8c\u8ba4\u77e5\u8fc7\u7a0b\u4e2d\u7684\u7ed3\u6784-\u529f\u80fd\u5173\u7cfb\uff0c\u901a\u8fc7\u5c06\u6a21\u62df\u7ed3\u679c\u4e0e\u77e5\u8bc6\u5efa\u6a21\u4e2d\u7684\u65e2\u6709\u7406\u8bba\u8fdb\u884c\u6bd4\u8f83\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u6fc0\u6d3b\u52a8\u529b\u5b66\u5982\u4f55\u53cd\u6620\u8ba4\u77e5\u3001\u5fc3\u7406\u548c\u4e34\u5e8a\u73b0\u8c61\u3002", "method": "\u4ecb\u7ecd\u4e86SpreadPy\u8fd9\u4e2aPython\u5e93\uff0c\u7528\u4e8e\u6a21\u62df\u8ba4\u77e5\u5355\u5c42\u548c\u591a\u5c42\u7f51\u7edc\u4e2d\u7684\u6fc0\u6d3b\u6269\u6563\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u6848\u4f8b\u7814\u7a76\u5c55\u793a\u4e86\u5176\u6548\u7528\uff1a1. \u6a21\u62df\u8054\u60f3\u77e5\u8bc6\u7f51\u7edc\u4e0a\u7684\u6fc0\u6d3b\u6269\u6563\uff0c\u533a\u5206\u6709\u6570\u5b66\u7126\u8651\u7684\u5b66\u751f\uff1b2. \u6a21\u62df\u521b\u9020\u6027\u4efb\u52a1\uff0c\u5c55\u793a\u6fc0\u6d3b\u8f68\u8ff9\u968f\u4efb\u52a1\u96be\u5ea6\u548c\u8ba4\u77e5\u8d1f\u8377\u7684\u53d8\u5316\uff1b3. \u6a21\u62df\u5931\u8bed\u75c7\u60a3\u8005\u7684\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u8bcd\u6c47\u9519\u8bef\u7c7b\u578b\u7684\u5173\u8054\u3002", "result": "SpreadPy\u80fd\u591f\u533a\u5206\u5177\u6709\u4e0d\u540c\u6570\u5b66\u7126\u8651\u6c34\u5e73\u7684\u5b66\u751f\uff0c\u5c55\u793a\u8ba4\u77e5\u8d1f\u8377\u5982\u4f55\u8c03\u8282\u8bcd\u6c47\u8bbf\u95ee\uff0c\u5e76\u5c06\u5931\u8bed\u75c7\u60a3\u8005\u7684\u6fc0\u6d3b\u6a21\u5f0f\u4e0e\u4ed6\u4eec\u7684\u8bed\u8a00\u9519\u8bef\u7c7b\u578b\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u7406\u89e3\u4e2a\u4f53\u5dee\u5f02\u548c\u8ba4\u77e5\u969c\u788d\u63d0\u4f9b\u4e86\u673a\u5236\u89c1\u89e3\u3002", "conclusion": "SpreadPy\u662f\u4e00\u4e2a\u7528\u4e8e\u6a21\u62df\u8ba4\u77e5\u5355\u5c42\u548c\u591a\u5c42\u7f51\u7edc\u4e2d\u6fc0\u6d3b\u6269\u6563\u7684Python\u5e93\uff0c\u80fd\u591f\u8fdb\u884c\u6570\u503c\u6a21\u62df\u4ee5\u68c0\u9a8c\u7ed3\u6784-\u529f\u80fd\u5173\u7cfb\uff0c\u5e76\u63d0\u4f9b\u53ef\u590d\u73b0\u7684\u7814\u7a76\u652f\u6301\u3002"}}
{"id": "2507.08980", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.08980", "abs": "https://arxiv.org/abs/2507.08980", "authors": ["Chenyu Wang", "Cai Zhou", "Sharut Gupta", "Zongyu Lin", "Stefanie Jegelka", "Stephen Bates", "Tommi Jaakkola"], "title": "Learning Diffusion Models with Flexible Representation Guidance", "comment": null, "summary": "Diffusion models can be improved with additional guidance towards more\neffective representations of input. Indeed, prior empirical work has already\nshown that aligning internal representations of the diffusion model with those\nof pre-trained models improves generation quality. In this paper, we present a\nsystematic framework for incorporating representation guidance into diffusion\nmodels. We provide alternative decompositions of denoising models along with\ntheir associated training criteria, where the decompositions determine when and\nhow the auxiliary representations are incorporated. Guided by our theoretical\ninsights, we introduce two new strategies for enhancing representation\nalignment in diffusion models. First, we pair examples with target\nrepresentations either derived from themselves or arisen from different\nsynthetic modalities, and subsequently learn a joint model over the multimodal\npairs. Second, we design an optimal training curriculum that balances\nrepresentation learning and data generation. Our experiments across image,\nprotein sequence, and molecule generation tasks demonstrate superior\nperformance as well as accelerated training. In particular, on the\nclass-conditional ImageNet $256\\times 256$ benchmark, our guidance results in\n$23.3$ times faster training than the original SiT-XL as well as four times\nspeedup over the state-of-the-art method REPA. The code is available at\nhttps://github.com/ChenyuWang-Monica/REED.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u901a\u8fc7\u6574\u5408\u8868\u793a\u5f15\u5bfc\u6765\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u548c\u8bad\u7ec3\u901f\u5ea6\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u6846\u67b6\u548c\u4e24\u79cd\u65b0\u7b56\u7565\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u52a0\u901f\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u901a\u8fc7\u5bf9\u9f50\u6269\u6563\u6a21\u578b\u5185\u90e8\u8868\u793a\u548c\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u6539\u8fdb\u5176\u751f\u6210\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7cfb\u7edf\u6027\u7684\u6846\u67b6\uff0c\u5c06\u8868\u793a\u5f15\u5bfc\u6574\u5408\u5230\u6269\u6563\u6a21\u578b\u4e2d\u3002\u901a\u8fc7\u5bf9\u53bb\u566a\u6a21\u578b\u8fdb\u884c\u5206\u89e3\uff0c\u5e76\u63d0\u4f9b\u76f8\u5e94\u7684\u8bad\u7ec3\u51c6\u5219\uff0c\u4ee5\u786e\u5b9a\u5982\u4f55\u4ee5\u53ca\u4f55\u65f6\u6574\u5408\u8f85\u52a9\u8868\u793a\u3002\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u7b56\u7565\uff1a1. \u5c06\u6837\u672c\u4e0e\u6e90\u81ea\u81ea\u8eab\u6216\u4e0d\u540c\u5408\u6210\u6a21\u6001\u7684\u76ee\u6807\u8868\u793a\u914d\u5bf9\uff0c\u5e76\u5b66\u4e60\u591a\u6a21\u6001\u5bf9\u4e0a\u7684\u8054\u5408\u6a21\u578b\uff1b2. \u8bbe\u8ba1\u4e86\u4e00\u4e2a\u6700\u4f18\u8bad\u7ec3\u8bfe\u7a0b\uff0c\u5e73\u8861\u8868\u793a\u5b66\u4e60\u548c\u6570\u636e\u751f\u6210\u3002", "result": "\u5728\u56fe\u50cf\u3001\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u52a0\u901f\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5728ImageNet $256\times 256$\u4efb\u52a1\u4e0a\uff0c\u8bad\u7ec3\u901f\u5ea6\u6bd4SiT-XL\u5feb23.3\u500d\uff0c\u6bd4REPA\u5feb4\u500d\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u5728\u56fe\u50cf\u3001\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u5206\u5b50\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u52a0\u901f\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u3002\u5728ImageNet $256\times 256$\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u6bd4SiT-XL\u5feb23.3\u500d\uff0c\u6bd4\u5f53\u524d\u6700\u4f18\u65b9\u6cd5REPA\u5feb4\u500d\u3002"}}
{"id": "2507.10119", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10119", "abs": "https://arxiv.org/abs/2507.10119", "authors": ["Sadig Gojayev", "Ahmad Anaqreh", "Carolina Fortuna"], "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration", "comment": null, "summary": "Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments.", "AI": {"tldr": "\u672c\u6587\u4f7f\u7528AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u89e3\u51b3\u8fb9\u7f18-\u4e91\u5e94\u7528\u8fc1\u79fb\u95ee\u9898\uff0c\u5c06\u5176\u5efa\u6a21\u4e3a\u6c49\u8bfa\u5854\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\u8fdb\u884c\u5206\u6790\u3002", "motivation": "\u4e3a\u5b9e\u73b0\u9ad8\u670d\u52a1\u8d28\u91cf\u548c\u6210\u672c\u6548\u76ca\u7684\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u670d\u52a1\u4ea4\u4ed8\uff0c\u9700\u8981\u81ea\u52a8\u7f16\u6392\u5e94\u7528\u8fc1\u79fb\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u591a\u4e3a\u542f\u53d1\u5f0f\u3002", "method": "\u672c\u6587\u4ece\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\u51fa\u53d1\uff0c\u8bc6\u522b\u3001\u5206\u6790\u548c\u6bd4\u8f83\u4e86\u89e3\u51b3\u6c49\u8bfa\u5854\u7c7b\u95ee\u9898\u7684AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u5b9a\u4e49\u7684\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\u3002", "result": "\u6587\u7ae0\u901a\u8fc7\u65b0\u7684\u5206\u7c7b\u65b9\u6cd5\uff0c\u4ece\u72b6\u6001\u7a7a\u95f4\u5b9a\u4e49\u7684\u89d2\u5ea6\u5bf9\u6240\u6bd4\u8f83\u7684\u6a21\u578b\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u65e8\u5728\u7406\u89e3\u80fd\u591f\u7f16\u6392\u65b0\u5174\u8ba1\u7b97\u8fde\u7eed\u4f53\u73af\u5883\u4e2d\u6b64\u7c7b\u5e94\u7528\u8fc1\u79fb\u7684\u53ef\u7528\u6280\u672f\u3002", "conclusion": "\u672c\u6587\u65e8\u5728\u5206\u6790\u548c\u6bd4\u8f83\u7528\u4e8e\u89e3\u51b3\u8fb9\u7f18-\u4e91\u5e94\u7528\u8fc1\u79fb\u95ee\u9898\u7684AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u8fd9\u4e9b\u95ee\u9898\u53ef\u4ee5\u88ab\u5efa\u6a21\u4e3a\u7ecf\u5178\u7684\u6c49\u8bfa\u5854\u95ee\u9898\u3002"}}
{"id": "2507.10087", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10087", "abs": "https://arxiv.org/abs/2507.10087", "authors": ["Muhammad Tayyab Khan", "Ammar Waheed"], "title": "Foundation Model Driven Robotics: A Comprehensive Review", "comment": null, "summary": "The rapid emergence of foundation models, particularly Large Language Models\n(LLMs) and Vision-Language Models (VLMs), has introduced a transformative\nparadigm in robotics. These models offer powerful capabilities in semantic\nunderstanding, high-level reasoning, and cross-modal generalization, enabling\nsignificant advances in perception, planning, control, and human-robot\ninteraction. This critical review provides a structured synthesis of recent\ndevelopments, categorizing applications across simulation-driven design,\nopen-world execution, sim-to-real transfer, and adaptable robotics. Unlike\nexisting surveys that emphasize isolated capabilities, this work highlights\nintegrated, system-level strategies and evaluates their practical feasibility\nin real-world environments. Key enabling trends such as procedural scene\ngeneration, policy generalization, and multimodal reasoning are discussed\nalongside core bottlenecks, including limited embodiment, lack of multimodal\ndata, safety risks, and computational constraints. Through this lens, this\npaper identifies both the architectural strengths and critical limitations of\nfoundation model-based robotics, highlighting open challenges in real-time\noperation, grounding, resilience, and trust. The review concludes with a\nroadmap for future research aimed at bridging semantic reasoning and physical\nintelligence through more robust, interpretable, and embodied models.", "AI": {"tldr": "\u57fa\u7840\u6a21\u578b\u6b63\u5728\u6539\u53d8\u673a\u5668\u4eba\u6280\u672f\uff0c\u4f46\u4ecd\u9762\u4e34\u6570\u636e\u3001\u5b89\u5168\u548c\u5b9e\u65f6\u64cd\u4f5c\u7b49\u6311\u6218\u3002\u672a\u6765\u7814\u7a76\u5e94\u4fa7\u91cd\u4e8e\u66f4\u5f3a\u5927\u3001\u53ef\u89e3\u91ca\u548c\u5177\u8eab\u5316\u7684\u6a21\u578b\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u6279\u5224\u6027\u5730\u56de\u987e\u57fa\u7840\u6a21\u578b\uff08\u7279\u522b\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u91cd\u70b9\u5173\u6ce8\u96c6\u6210\u7cfb\u7edf\u7ea7\u7b56\u7565\u53ca\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u65b9\u5411\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9\u6a21\u62df\u9a71\u52a8\u8bbe\u8ba1\u3001\u5f00\u653e\u4e16\u754c\u6267\u884c\u3001\u4eff\u771f\u5230\u5b9e\u9645\u8fc1\u79fb\u548c\u81ea\u9002\u5e94\u673a\u5668\u4eba\u7b49\u5e94\u7528\u9886\u57df\u8fdb\u884c\u5206\u7c7b\u548c\u7efc\u5408\uff0c\u5bf9\u57fa\u7840\u6a21\u578b\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u6279\u5224\u6027\u56de\u987e\u3002\u6587\u7ae0\u8ba8\u8bba\u4e86\u7a0b\u5e8f\u5316\u573a\u666f\u751f\u6210\u3001\u7b56\u7565\u6cdb\u5316\u548c\u591a\u6a21\u6001\u63a8\u7406\u7b49\u5173\u952e\u8d8b\u52bf\uff0c\u5e76\u5206\u6790\u4e86\u6709\u9650\u7684\u5177\u8eab\u6027\u3001\u591a\u6a21\u6001\u6570\u636e\u7f3a\u4e4f\u3001\u5b89\u5168\u98ce\u9669\u548c\u8ba1\u7b97\u9650\u5236\u7b49\u6838\u5fc3\u74f6\u9888\u3002", "result": "\u8be5\u8bba\u6587\u8bc6\u522b\u4e86\u57fa\u4e8e\u57fa\u7840\u6a21\u578b\u7684\u673a\u5668\u4eba\u6280\u672f\u7684\u67b6\u6784\u4f18\u52bf\u548c\u5173\u952e\u5c40\u9650\u6027\uff0c\u5f3a\u8c03\u4e86\u5b9e\u65f6\u64cd\u4f5c\u3001\u63a5\u5730\u3001\u97e7\u6027\u548c\u4fe1\u4efb\u65b9\u9762\u7684\u5f00\u653e\u6311\u6218\u3002", "conclusion": "\u8be5\u8bba\u6587\u603b\u7ed3\u4e86\u57fa\u7840\u6a21\u578b\uff08\u5c24\u5176\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\u5728\u673a\u5668\u4eba\u9886\u57df\u7684\u5e94\u7528\uff0c\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u8bed\u4e49\u7406\u89e3\u3001\u63a8\u7406\u548c\u8de8\u6a21\u6001\u6cdb\u5316\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u6a21\u62df\u9a71\u52a8\u8bbe\u8ba1\u3001\u5f00\u653e\u4e16\u754c\u6267\u884c\u3001\u4eff\u771f\u5230\u5b9e\u9645\u8fc1\u79fb\u548c\u81ea\u9002\u5e94\u673a\u5668\u4eba\u7b49\u65b9\u9762\u7684\u8fdb\u5c55\u3002\u4e0e\u73b0\u6709\u6587\u732e\u4e0d\u540c\uff0c\u672c\u6587\u4fa7\u91cd\u4e8e\u96c6\u6210\u7cfb\u7edf\u7ea7\u7b56\u7565\u53ca\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.09242", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09242", "abs": "https://arxiv.org/abs/2507.09242", "authors": ["Shiqi Jiang", "Xinpeng Li", "Xi Mao", "Changbo Wang", "Chenhui Li"], "title": "PPJudge: Towards Human-Aligned Assessment of Artistic Painting Process", "comment": "ACM International Conference on Multimedia 2025", "summary": "Artistic image assessment has become a prominent research area in computer\nvision. In recent years, the field has witnessed a proliferation of datasets\nand methods designed to evaluate the aesthetic quality of paintings. However,\nmost existing approaches focus solely on static final images, overlooking the\ndynamic and multi-stage nature of the artistic painting process. To address\nthis gap, we propose a novel framework for human-aligned assessment of painting\nprocesses. Specifically, we introduce the Painting Process Assessment Dataset\n(PPAD), the first large-scale dataset comprising real and synthetic painting\nprocess images, annotated by domain experts across eight detailed attributes.\nFurthermore, we present PPJudge (Painting Process Judge), a Transformer-based\nmodel enhanced with temporally-aware positional encoding and a heterogeneous\nmixture-of-experts architecture, enabling effective assessment of the painting\nprocess. Experimental results demonstrate that our method outperforms existing\nbaselines in accuracy, robustness, and alignment with human judgment, offering\nnew insights into computational creativity and art education.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684PPJudge\u6846\u67b6\u548cPPAD\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ed8\u753b\u8fc7\u7a0b\uff0c\u800c\u975e\u4ec5\u8bc4\u4f30\u6700\u7ec8\u56fe\u50cf\u3002PPJudge\u901a\u8fc7 \u0632\u0645\u0646\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u548c\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u589e\u5f3a\u4e86Transformer\u6a21\u578b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8ba1\u7b97\u521b\u610f\u548c\u827a\u672f\u6559\u80b2\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5927\u591a\u53ea\u5173\u6ce8\u9759\u6001\u7684\u6700\u7ec8\u56fe\u50cf\uff0c\u5ffd\u7565\u4e86\u827a\u672f\u7ed8\u753b\u8fc7\u7a0b\u7684\u52a8\u6001\u548c\u591a\u9636\u6bb5\u6027\u8d28\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5dee\u8ddd\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u4eba\u7c7b\u5bf9\u9f50\u7684\u7ed8\u753b\u8fc7\u7a0b\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u7ed8\u753b\u8fc7\u7a0b\uff0c\u5e76\u5f15\u5165\u4e86PPAD\uff08\u7ed8\u753b\u8fc7\u7a0b\u8bc4\u4f30\u6570\u636e\u96c6\uff09\uff0c\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u548c\u5408\u6210\u7ed8\u753b\u8fc7\u7a0b\u56fe\u50cf\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u7531\u9886\u57df\u4e13\u5bb6\u6807\u6ce8\u4e86\u516b\u4e2a\u8be6\u7ec6\u5c5e\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684\u6a21\u578bPPJudge\uff0c\u8be5\u6a21\u578b\u901a\u8fc7 \u0632\u0645\u0646\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\u548c\u5f02\u6784\u4e13\u5bb6\u6df7\u5408\u67b6\u6784\u8fdb\u884c\u4e86\u589e\u5f3a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684PPJudge\u6846\u67b6\u5728\u7ed8\u753b\u8fc7\u7a0b\u8bc4\u4f30\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u65e0\u8bba\u662f\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u8fd8\u662f\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u65b9\u9762\uff0c\u90fd\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.09738", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09738", "abs": "https://arxiv.org/abs/2507.09738", "authors": ["Gabriele Carcassi", "Robert Rozite", "Christine A. Aidala"], "title": "Response to \"Are Hilbert Spaces Unphysical? Hardly, My Dear!''", "comment": "2 pages", "summary": "A recent criticism of our paper ``The unphysicality of Hilbert spaces'' by\nNivaldo Lemos refutes our central argument that a state with finite expectation\nvalue can be mapped to a state with infinite expectation value by a coordinate\ntransformation. By conflating coordinate transformation with change of basis in\nquantum mechanics, Lemos argues that expectation values are invariant under\nchange of variables. In the present work, we clarify the distinction between\ncoordinate transformation and change of basis, and rebut Lemos' main argument.", "AI": {"tldr": "\u8bba\u6587\u56de\u5e94\u4e86\u5173\u4e8e\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u5750\u6807\u53d8\u6362\u548c\u671f\u671b\u503c\u4e0d\u53d8\u6027\u7684\u6279\u8bc4\uff0c\u6f84\u6e05\u4e86\u4e24\u8005\u7684\u533a\u522b\u5e76\u53cd\u9a73\u4e86\u6279\u8bc4\u8005\u7684\u8bba\u70b9\u3002", "motivation": "\u56de\u5e94Nivaldo Lemos\u5bf9\u6211\u4eec\u8bba\u6587\u201c\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684\u975e\u7269\u7406\u6027\u201d\u63d0\u51fa\u7684\u6279\u8bc4\uff0cLemos\u58f0\u79f0\u901a\u8fc7\u5750\u6807\u53d8\u6362\u5c06\u5177\u6709\u6709\u9650\u671f\u671b\u503c\u7684\u6001\u6620\u5c04\u5230\u5177\u6709\u65e0\u9650\u671f\u671b\u503c\u7684\u6001\u7684\u8bba\u70b9\u662f\u4e0d\u6210\u7acb\u7684\u3002", "method": "\u901a\u8fc7\u9610\u660e\u5750\u6807\u53d8\u6362\u548c\u57fa\u53d8\u6362\u7684\u533a\u522b\u6765\u53cd\u9a73Lemos\u7684\u8bba\u70b9\u3002", "result": "\u6f84\u6e05\u5750\u6807\u53d8\u6362\u548c\u57fa\u53d8\u6362\u7684\u533a\u522b\uff0c\u5e76\u53cd\u9a73Lemos\u7684\u4e3b\u8981\u8bba\u70b9\u3002", "conclusion": "\u8be5\u8bba\u6587\u65e8\u5728\u6f84\u6e05\u5750\u6807\u53d8\u6362\u4e0e\u91cf\u5b50\u529b\u5b66\u4e2d\u7684\u57fa\u53d8\u6362\u4e4b\u95f4\u7684\u533a\u522b\uff0c\u5e76\u53cd\u9a73Lemos\u5173\u4e8e\u671f\u671b\u503c\u5728\u53d8\u91cf\u53d8\u6362\u4e0b\u4e0d\u53d8\u7684\u4e3b\u8981\u8bba\u70b9\u3002"}}
{"id": "2507.09629", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09629", "abs": "https://arxiv.org/abs/2507.09629", "authors": ["Basel Mousi", "Nadir Durrani", "Fahim Dalvi"], "title": "An Exploration of Knowledge Editing for Arabic", "comment": null, "summary": "While Knowledge Editing (KE) has been widely explored in English, its\nbehavior in morphologically rich languages like Arabic remains underexamined.\nIn this work, we present the first study of Arabic KE. We evaluate four methods\n(ROME, MEMIT, ICE, and LTE) on Arabic translations of the ZsRE and Counterfact\nbenchmarks, analyzing both multilingual and cross-lingual settings. Our\nexperiments on Llama-2-7B-chat show show that parameter-based methods struggle\nwith cross-lingual generalization, while instruction-tuned methods perform more\nrobustly. We extend Learning-To-Edit (LTE) to a multilingual setting and show\nthat joint Arabic-English training improves both editability and transfer. We\nrelease Arabic KE benchmarks and multilingual training for LTE data to support\nfuture research.", "AI": {"tldr": "\u9996\u6b21\u7814\u7a76\u963f\u62c9\u4f2f\u8bed\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\uff0c\u8bc4\u4f30\u4e86ROME\u3001MEMIT\u3001ICE\u548cLTE\u65b9\u6cd5\uff0c\u53d1\u73b0\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u6bd4\u53c2\u6570\u5316\u65b9\u6cd5\u66f4\u9002\u5408\u8de8\u8bed\u8a00\u4efb\u52a1\u3002\u901a\u8fc7\u591a\u8bed\u8a00\u8bad\u7ec3\u6539\u8fdb\u4e86LTE\u65b9\u6cd5\u3002", "motivation": "\u5c3d\u7ba1\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u5728\u82f1\u8bed\u4e2d\u5df2\u88ab\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u5176\u5728\u50cf\u963f\u62c9\u4f2f\u8bed\u8fd9\u6837\u5f62\u6001\u4e30\u5bcc\u7684\u8bed\u8a00\u4e2d\u7684\u884c\u4e3a\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u5de5\u4f5c\u9996\u6b21\u5bf9\u963f\u62c9\u4f2f\u8bedKE\u8fdb\u884c\u4e86\u7814\u7a76\u3002", "method": "\u5728\u963f\u62c9\u4f2f\u8bed\u7ffb\u8bd1\u7684ZsRE\u548cCounterfact\u57fa\u51c6\u4e0a\u8bc4\u4f30\u4e86\u56db\u79cd\u65b9\u6cd5\uff08ROME\u3001MEMIT\u3001ICE\u548cLTE\uff09\uff0c\u5206\u6790\u4e86\u591a\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u8bbe\u7f6e\u3002\u5c06LTE\u6269\u5c55\u5230\u591a\u8bed\u8a00\u8bbe\u7f6e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5728Llama-2-7B-chat\u4e0a\uff0c\u53c2\u6570\u5316\u65b9\u6cd5\u5728\u8de8\u8bed\u8a00\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u800c\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u5219\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u53c2\u6570\u5316\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u8de8\u8bed\u8a00\u6cdb\u5316\uff0c\u800c\u6307\u4ee4\u5fae\u8c03\u65b9\u6cd5\u8868\u73b0\u66f4\u7a33\u5065\u3002\u6211\u4eec\u5c06LTE\u6269\u5c55\u5230\u591a\u8bed\u8a00\u8bbe\u7f6e\uff0c\u5e76\u5c55\u793a\u8054\u5408\u963f\u62c9\u4f2f\u8bed-\u82f1\u8bed\u8bad\u7ec3\u80fd\u540c\u65f6\u63d0\u9ad8\u53ef\u7f16\u8f91\u6027\u548c\u8fc1\u79fb\u80fd\u529b\u3002\u6211\u4eec\u53d1\u5e03\u4e86\u963f\u62c9\u4f2f\u8bed\u77e5\u8bc6\u7f16\u8f91\u57fa\u51c6\u548cLTE\u7684\u591a\u8bed\u8a00\u8bad\u7ec3\u6570\u636e\uff0c\u4ee5\u652f\u6301\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2507.08983", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.08983", "abs": "https://arxiv.org/abs/2507.08983", "authors": ["Anshuman Suri", "Harsh Chaudhari", "Yuefeng Peng", "Ali Naseh", "Amir Houmansadr", "Alina Oprea"], "title": "Exploiting Leaderboards for Large-Scale Distribution of Malicious Models", "comment": null, "summary": "While poisoning attacks on machine learning models have been extensively\nstudied, the mechanisms by which adversaries can distribute poisoned models at\nscale remain largely unexplored. In this paper, we shed light on how model\nleaderboards -- ranked platforms for model discovery and evaluation -- can\nserve as a powerful channel for adversaries for stealthy large-scale\ndistribution of poisoned models. We present TrojanClimb, a general framework\nthat enables injection of malicious behaviors while maintaining competitive\nleaderboard performance. We demonstrate its effectiveness across four diverse\nmodalities: text-embedding, text-generation, text-to-speech and text-to-image,\nshowing that adversaries can successfully achieve high leaderboard rankings\nwhile embedding arbitrary harmful functionalities, from backdoors to bias\ninjection. Our findings reveal a significant vulnerability in the machine\nlearning ecosystem, highlighting the urgent need to redesign leaderboard\nevaluation mechanisms to detect and filter malicious (e.g., poisoned) models,\nwhile exposing broader security implications for the machine learning community\nregarding the risks of adopting models from unverified sources.", "AI": {"tldr": "\u6a21\u578b\u6392\u884c\u699c\u53ef\u80fd\u88ab\u6076\u610f\u884c\u4e3a\u8005\u7528\u6765\u5927\u89c4\u6a21\u5206\u53d1\u4e2d\u6bd2\u6a21\u578b\u3002\u7814\u7a76\u4eba\u5458\u63d0\u51fa\u4e86TrojanClimb\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u5f71\u54cd\u6392\u540d\u7684 tetapi \u4f1a\u5bfc\u81f4\u6a21\u578b\u4e2d\u6bd2\u7684\u60c5\u51b5\u4e0b\u6ce8\u5165\u6076\u610f\u884c\u4e3a\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u52a0\u5f3a\u6392\u884c\u699c\u8bc4\u4f30\u673a\u5236\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u6a21\u578b\u4e2d\u6bd2\u653b\u51fb\u7684\u673a\u5236\u4ee5\u53ca\u6076\u610f\u6a21\u578b\u5982\u4f55\u5927\u89c4\u6a21\u5206\u53d1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTrojanClimb\u7684\u901a\u7528\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5141\u8bb8\u5728\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u6392\u884c\u699c\u6027\u80fd\u7684\u540c\u65f6\u6ce8\u5165\u6076\u610f\u884c\u4e3a\u3002", "result": "\u5728\u6587\u672c\u5d4c\u5165\u3001\u6587\u672c\u751f\u6210\u3001\u6587\u672c\u5230\u8bed\u97f3\u548c\u6587\u672c\u5230\u56fe\u50cf\u56db\u4e2a\u4e0d\u540c\u7684\u6a21\u6001\u4e2d\uff0c\u6210\u529f\u5730\u5c55\u793a\u4e86\u5728\u6392\u884c\u699c\u4e0a\u83b7\u5f97\u9ad8\u6392\u540d\uff0c\u540c\u65f6\u5d4c\u5165\u4efb\u610f\u6709\u5bb3\u529f\u80fd\uff08\u4ece\u540e\u95e8\u5230\u504f\u5dee\u6ce8\u5165\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6a21\u578b\u6392\u884c\u699c\u4f5c\u4e3a\u6076\u610f\u6a21\u578b\u5927\u89c4\u6a21\u5206\u53d1\u7684\u6f5c\u5728\u6e20\u9053\uff0c\u5f3a\u8c03\u4e86\u6539\u8fdb\u6392\u884c\u699c\u8bc4\u4f30\u673a\u5236\u4ee5\u68c0\u6d4b\u548c\u8fc7\u6ee4\u6076\u610f\u6a21\u578b\u4ee5\u5e94\u5bf9\u673a\u5668\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u91cd\u5927\u6f0f\u6d1e\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.10124", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10124", "abs": "https://arxiv.org/abs/2507.10124", "authors": ["Thomas T. Hills"], "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making", "comment": "12 pages, 3 figures", "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.", "AI": {"tldr": "\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u504f\u89c1\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002\u672c\u6587\u501f\u9274\u4eba\u7c7b\u51b3\u7b56\u9886\u57df\u7684\u5143\u8ba4\u77e5\u63d0\u793a\u7b56\u7565\uff0c\u7279\u522b\u662f\u201c\u4f60\u53ef\u80fd\u72af\u9519\u5417\uff1f\u201d\u8fd9\u4e00\u63d0\u793a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eLLMs\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u63d0\u793a\u80fd\u591f\u4fc3\u4f7fLLMs\u81ea\u6211\u53cd\u601d\uff0c\u8bc6\u522b\u5e76\u63ed\u793a\u5176\u6f5c\u5728\u7684\u504f\u89c1\u3001\u9519\u8bef\u548c\u4e0d\u5b8c\u6574\u7684\u56de\u7b54\uff0c\u5e76\u63d0\u4f9b\u989d\u5916\u7684\u89e3\u91ca\u548c\u66ff\u4ee3\u65b9\u6848\u3002\u8fd9\u4e3a\u5f00\u53d1\u66f4\u901a\u7528\u7684LLM\u53bb\u504f\u7b56\u7565\u548c\u6539\u8fdb\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u8bc6\u522b\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e2d\u7684\u504f\u89c1\u662f\u4e00\u4e2a\u6301\u7eed\u5b58\u5728\u7684\u6311\u6218\u3002\u7531\u4e8eLLMs\u4ecd\u5728\u53d1\u5c55\u4e2d\uff0c\u4eca\u5929\u7684\u6709\u6548\u7b56\u7565\u53ef\u80fd\u5728\u660e\u5929\u5c31\u8fc7\u65f6\u4e86\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u7ecf\u53d7\u4f4f\u6a21\u578b\u8fed\u4ee3\u66f4\u65b0\u7684\u901a\u7528\u53bb\u504f\u7b56\u7565\u3002\u672c\u6587\u8ba4\u4e3a\uff0c\u501f\u9274\u4eba\u7c7b\u51b3\u7b56\u9886\u57df\u4e2d\u5df2\u88ab\u8bc1\u660e\u6709\u6548\u7684\u53bb\u504f\u7b56\u7565\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u5229\u7528\u63d0\u793a\u5e72\u9884\u7684\u7b56\u7565\uff0c\u4e3a\u89e3\u51b3LLMs\u7684\u504f\u89c1\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002", "method": "\u672c\u6587\u91c7\u7528\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u4f60\u53ef\u80fd\u72af\u9519\u5417\uff1f\u201d\u7684\u5143\u8ba4\u77e5\u63d0\u793a\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u3002\u901a\u8fc7\u5728\u4e00\u7cfb\u5217\u5173\u4e8eLLM\u504f\u89c1\u7684\u95ee\u9898\u4e0a\u4f7f\u7528\u6b64\u63d0\u793a\uff0c\u5e76\u4e0e\u6765\u81ea\u8fd1\u671f\u5173\u4e8eLLM\u504f\u89c1\u7684\u6587\u7ae0\u7684\u95ee\u9898\u96c6\u8fdb\u884c\u5bf9\u6bd4\uff0c\u7814\u7a76\u4e86\u8be5\u63d0\u793a\u7684\u6548\u679c\u3002", "result": "\u201c\u4f60\u53ef\u80fd\u72af\u9519\u5417\uff1f\u201d\u8fd9\u4e00\u63d0\u793a\u80fd\u591f\u4fc3\u4f7fLLMs\u5728\u521d\u59cb\u56de\u7b54\u540e\u4ea7\u751f\u989d\u5916\u4fe1\u606f\uff0c\u5305\u62ec\u5176\u56de\u7b54\u539f\u56e0\u3001\u8bc6\u522b\u51fa\u7684\u9519\u8bef\u3001\u504f\u89c1\u3001\u77db\u76fe\u8bc1\u636e\u4ee5\u53ca\u66ff\u4ee3\u65b9\u6848\uff0c\u8fd9\u4e9b\u5728\u521d\u59cb\u56de\u7b54\u4e2d\u5e76\u672a\u663e\u73b0\u3002\u8fd9\u79cd\u5143\u77e5\u8bc6\u63ed\u793a\u4e86LLMs\u548c\u7528\u6237\u5728\u7406\u89e3\u63d0\u793a\u65b9\u9762\u53ef\u80fd\u5b58\u5728\u7684\u5dee\u5f02\u3002\u8be5\u63d0\u793a\u8fd8\u80fd\u6709\u6548\u7ea0\u6b63\u5305\u542b\u4ee4\u4eba\u4fe1\u670d\u4f46\u4fe1\u606f\u4e0d\u5b8c\u6574\u7684\u56de\u7b54\u3002", "conclusion": "\u4eba\u7c7b\u5fc3\u7406\u5b66\u4e3a\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u9014\u5f84\uff0c\u5229\u7528\u4eba\u7c7b\u51b3\u7b56\u4e2d\u57fa\u4e8e\u63d0\u793a\u7684\u6539\u8fdb\u7684\u957f\u671f\u5386\u53f2\u3002"}}
{"id": "2507.10105", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10105", "abs": "https://arxiv.org/abs/2507.10105", "authors": ["Ines Sorrentino", "Giulio Romualdi", "Lorenzo Moretti", "Silvio Traversaro", "Daniele Pucci"], "title": "Physics-Informed Neural Networks with Unscented Kalman Filter for Sensorless Joint Torque Estimation in Humanoid Robots", "comment": null, "summary": "This paper presents a novel framework for whole-body torque control of\nhumanoid robots without joint torque sensors, designed for systems with\nelectric motors and high-ratio harmonic drives. The approach integrates\nPhysics-Informed Neural Networks (PINNs) for friction modeling and Unscented\nKalman Filtering (UKF) for joint torque estimation, within a real-time torque\ncontrol architecture. PINNs estimate nonlinear static and dynamic friction from\njoint and motor velocity readings, capturing effects like motor actuation\nwithout joint movement. The UKF utilizes PINN-based friction estimates as\ndirect measurement inputs, improving torque estimation robustness. Experimental\nvalidation on the ergoCub humanoid robot demonstrates improved torque tracking\naccuracy, enhanced energy efficiency, and superior disturbance rejection\ncompared to the state-of-the-art Recursive Newton-Euler Algorithm (RNEA), using\na dynamic balancing experiment. The framework's scalability is shown by\nconsistent performance across robots with similar hardware but different\nfriction characteristics, without re-identification. Furthermore, a comparative\nanalysis with position control highlights the advantages of the proposed torque\ncontrol approach. The results establish the method as a scalable and practical\nsolution for sensorless torque control in humanoid robots, ensuring torque\ntracking, adaptability, and stability in dynamic environments.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u65e0\u4f20\u611f\u5668\u4eba\u5f62\u673a\u5668\u4eba\u5168\u8eab\u626d\u77e9\u63a7\u5236\u7684\u65b0\u6846\u67b6\uff0c\u7ed3\u5408\u4e86PINNs\u548cUKF\u6765\u63d0\u9ad8\u626d\u77e9\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4eba\u5f62\u673a\u5668\u4eba\u4e2d\u7f3a\u4e4f\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\u800c\u96be\u4ee5\u5b9e\u73b0\u7cbe\u786e\u5168\u8eab\u626d\u77e9\u63a7\u5236\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528PINNs\u548cUKF\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u7528\u4e8e\u6469\u64e6\u5efa\u6a21\u548c\u65e0\u8ff9\u5361\u5c14\u66fc\u6ee4\u6ce2\uff08UKF\uff09\u7528\u4e8e\u5173\u8282\u626d\u77e9\u4f30\u8ba1\u76f8\u7ed3\u5408\u7684\u65b0\u9896\u6846\u67b6\uff0c\u5e76\u5c06\u5176\u6574\u5408\u5230\u5b9e\u65f6\u626d\u77e9\u63a7\u5236\u67b6\u6784\u4e2d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u7684\u9012\u5f52\u725b\u987f-\u6b27\u62c9\u7b97\u6cd5\uff08RNEA\uff09\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u626d\u77e9\u8ddf\u8e2a\u7cbe\u5ea6\u3001\u80fd\u6548\u548c\u5e72\u6270\u6291\u5236\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u6469\u64e6\u7279\u6027\u7684\u673a\u5668\u4eba\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u4e14\u65e0\u9700\u91cd\u65b0\u8bc6\u522b\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u6ca1\u6709\u5173\u8282\u626d\u77e9\u4f20\u611f\u5668\u7684\u5168\u8eab\u4f53\u626d\u77e9\u63a7\u5236\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u786e\u4fdd\u4e86\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u626d\u77e9\u8ddf\u8e2a\u3001\u9002\u5e94\u6027\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2507.09248", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09248", "abs": "https://arxiv.org/abs/2507.09248", "authors": ["Varsha Devi", "Amine Bohi", "Pardeep Kumar"], "title": "AGCD-Net: Attention Guided Context Debiasing Network for Emotion Recognition", "comment": "13 Pages, 4 figures, 2 tables ICIAP 2025", "summary": "Context-aware emotion recognition (CAER) enhances affective computing in\nreal-world scenarios, but traditional methods often suffer from context\nbias-spurious correlation between background context and emotion labels (e.g.\nassociating ``garden'' with ``happy''). In this paper, we propose\n\\textbf{AGCD-Net}, an Attention Guided Context Debiasing model that introduces\n\\textit{Hybrid ConvNeXt}, a novel convolutional encoder that extends the\nConvNeXt backbone by integrating Spatial Transformer Network and\nSqueeze-and-Excitation layers for enhanced feature recalibration. At the core\nof AGCD-Net is the Attention Guided - Causal Intervention Module (AG-CIM),\nwhich applies causal theory, perturbs context features, isolates spurious\ncorrelations, and performs an attention-driven correction guided by face\nfeatures to mitigate context bias. Experimental results on the CAER-S dataset\ndemonstrate the effectiveness of AGCD-Net, achieving state-of-the-art\nperformance and highlighting the importance of causal debiasing for robust\nemotion recognition in complex settings.", "AI": {"tldr": "AGCD-Net\u901a\u8fc7\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u56e0\u679c\u5e72\u9884\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u60c5\u611f\u8bc6\u522b\u4e2d\u7684\u4e0a\u4e0b\u6587\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u5728CAER-S\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u9886\u5148\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u60c5\u611f\u8bc6\u522b\uff08CAER\uff09\u65b9\u6cd5\u5e38\u5e38\u53d7\u5230\u4e0a\u4e0b\u6587\u504f\u89c1\u7684\u5f71\u54cd\uff0c\u5373\u80cc\u666f\u4e0a\u4e0b\u6587\u4e0e\u60c5\u611f\u6807\u7b7e\u4e4b\u95f4\u5b58\u5728\u865a\u5047\u5173\u8054\uff08\u4f8b\u5982\uff0c\u5c06\u201c\u82b1\u56ed\u201d\u4e0e\u201c\u5feb\u4e50\u201d\u5173\u8054\u8d77\u6765\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAGCD-Net\uff08Attention Guided Context Debiasing Network\uff09\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5f15\u5165\u4e86\u6df7\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08Hybrid ConvNeXt\uff09\u4f5c\u4e3a\u7f16\u7801\u5668\uff0c\u8be5\u7f16\u7801\u5668\u901a\u8fc7\u96c6\u6210\u7a7a\u95f4\u53d8\u6362\u7f51\u7edc\uff08Spatial Transformer Network\uff09\u548cSqueeze-and-Excitation\u5c42\u6765\u589e\u5f3a\u7279\u5f81\u91cd\u65b0\u6821\u51c6\u3002\u5176\u6838\u5fc3\u662f\u6ce8\u610f\u529b\u5f15\u5bfc\u56e0\u679c\u5e72\u9884\u6a21\u5757\uff08AG-CIM\uff09\uff0c\u8be5\u6a21\u5757\u5e94\u7528\u56e0\u679c\u7406\u8bba\uff0c\u6270\u52a8\u4e0a\u4e0b\u6587\u7279\u5f81\uff0c\u5206\u79bb\u865a\u5047\u76f8\u5173\u6027\uff0c\u5e76\u6839\u636e\u9762\u90e8\u7279\u5f81\u8fdb\u884c\u6ce8\u610f\u529b\u9a71\u52a8\u7684\u6821\u6b63\uff0c\u4ee5\u51cf\u8f7b\u4e0a\u4e0b\u6587\u504f\u89c1\u3002", "result": "AGCD-Net\u5728CAER-S\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAGCD-Net\u5728CAER-S\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5f3a\u8c03\u4e86\u56e0\u679c\u53bb\u504f\u5bf9\u4e8e\u590d\u6742\u573a\u666f\u4e0b\u9c81\u68d2\u60c5\u611f\u8bc6\u522b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.09770", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09770", "abs": "https://arxiv.org/abs/2507.09770", "authors": ["Daniel Turyansky", "Yehonatan Zolti", "Yuval Cohen", "Adi Pick"], "title": "Pulse optimization in adiabatic quantum computation and control", "comment": "7 pages, 6 figures", "summary": "We present a pulse optimization method for accelerating adiabatic control\nprotocols, including adiabatic population transfer and adiabatic quantum\ncomputation. Our method relies on identifying control pulses under which the\nevolving quantum system adheres to its instantaneous ground state. Our method\nis both efficient -- by using advanced gradient-free optimization tools and\nrobust -- by utilizing analytic adiabatic solutions in defining the cost\nfunction for quantum optimal control (QOC). To demonstrate the generality of\nour approach, we run digitized adiabatic protocols with superconducting qubits\non the IBM quantum platform and numerically simulate adiabatic algorithms for\nsolving graph optimization problems with Rydberg atom arrays.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u4f18\u5316\u8109\u51b2\u6765\u52a0\u901f\u91cf\u5b50\u8ba1\u7b97\u548c\u5e03\u5c45\u8f6c\u79fb\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6548\u7387\u9ad8\u4e14\u9c81\u68d2\u6027\u5f3a\u3002", "motivation": "\u65e8\u5728\u52a0\u901f\u7edd\u70ed\u63a7\u5236\u534f\u8bae\uff0c\u5305\u62ec\u7edd\u70ed\u5e03\u5c45\u8f6c\u79fb\u548c\u7edd\u70ed\u91cf\u5b50\u8ba1\u7b97\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u63a7\u5236\u8109\u51b2\u6765\u8bc6\u522b\u6f14\u5316\u91cf\u5b50\u7cfb\u7edf\u9075\u5faa\u5176\u77ac\u65f6\u57fa\u6001\uff0c\u5e76\u5229\u7528\u5148\u8fdb\u7684\u65e0\u68af\u5ea6\u4f18\u5316\u5de5\u5177\u548c\u5206\u6790\u7edd\u70ed\u89e3\u6765\u5b9a\u4e49\u91cf\u5b50\u6700\u4f18\u63a7\u5236\uff08QOC\uff09\u7684\u6210\u672c\u51fd\u6570\u3002", "result": "\u901a\u8fc7\u5728IBM\u91cf\u5b50\u5e73\u53f0\u4e0a\u4f7f\u7528\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u8fd0\u884c\u6570\u5b57\u5316\u7edd\u70ed\u534f\u8bae\uff0c\u5e76\u5bf9\u4f7f\u7528\u91cc\u5fb7\u5821\u539f\u5b50\u9635\u5217\u89e3\u51b3\u56fe\u4f18\u5316\u95ee\u9898\u7684\u7edd\u70ed\u7b97\u6cd5\u8fdb\u884c\u6570\u503c\u6a21\u62df\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u901a\u7528\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u52a0\u901f\u7edd\u70ed\u63a7\u5236\u534f\u8bae\u7684\u8109\u51b2\u4f18\u5316\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u62ec\u7edd\u70ed\u5e03\u5c45\u8f6c\u79fb\u548c\u7edd\u70ed\u91cf\u5b50\u8ba1\u7b97\u3002"}}
{"id": "2507.09638", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09638", "abs": "https://arxiv.org/abs/2507.09638", "authors": ["Pawitsapak Akarajaradwong", "Chompakorn Chaksangchaichot", "Pirat Pothavorn", "Attapol Thamrongrattanarit-Rutherford", "Ekapol Chuangsuwanich", "Sarana Nutanong"], "title": "Can Group Relative Policy Optimization Improve Thai Legal Reasoning and Question Answering?", "comment": null, "summary": "The Retrieval-Augmented Generation (RAG) systems' performance on Thai legal\nquestion answering is still limited, especially for questions requiring\nextensive, complex legal reasoning. To address these limitations, we introduce\nan approach aligning LLMs toward improved law citation accuracy and better\nresponse quality using Group-Relative Policy Optimization (GRPO). Our approach\nleverages BGE-M3 embeddings as a cost-efficient semantic-similarity reward,\nsignificantly reducing computational expenses up to 2.5x compared to large\nlanguage model judges. Experiments on the NitiBench benchmark demonstrate\nsubstantial improvements: GRPO achieves up to 90% citation-F1 gains from the\nbase model and a 31% increase in joint quality metrics over instruction tuning.\nCrucially, our method shows enhanced robustness on complex legal reasoning\ntasks compared to instruction tuning, providing an effective and\nresource-efficient solution for enhancing Thai legal LLMs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGRPO\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528BGE-M3\u5d4c\u5165\u4f5c\u4e3a\u5956\u52b1\uff0c\u63d0\u9ad8\u4e86\u6cf0\u8bed\u6cd5\u5f8b\u95ee\u7b54\u7684\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf\uff0c\u5c24\u5176\u5728\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u4e0a\u6548\u679c\u663e\u8457\uff0c\u5e76\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u5728\u6cf0\u8bed\u6cd5\u5f8b\u95ee\u7b54\u65b9\u9762\u6027\u80fd\u6709\u9650\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u5e7f\u6cdb\u3001\u590d\u6742\u6cd5\u5f8b\u63a8\u7406\u7684\u95ee\u9898\u4e0a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u5206\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u4f7f\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u503e\u5411\u4e8e\u63d0\u9ad8\u6cd5\u5f8b\u5f15\u6587\u51c6\u786e\u6027\u548c\u54cd\u5e94\u8d28\u91cf\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528BGE-M3\u5d4c\u5165\u4f5c\u4e3a\u6210\u672c\u6548\u76ca\u9ad8\u7684\u8bed\u4e49\u76f8\u4f3c\u6027\u5956\u52b1\uff0c\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u88c1\u5224\u76f8\u6bd4\uff0c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u964d\u4f4e\u4e862.5\u500d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGRPO\u5728NitiBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5f15\u6587F1\u5206\u6570\u6bd4\u57fa\u7840\u6a21\u578b\u63d0\u9ad8\u4e8690%\uff0c\u8054\u5408\u8d28\u91cf\u6307\u6807\u6bd4\u6307\u4ee4\u8c03\u4f18\u63d0\u9ad8\u4e8631%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u6cd5\u5f8b\u63a8\u7406\u4efb\u52a1\u65b9\u9762\u6bd4\u6307\u4ee4\u8c03\u4f18\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u589e\u5f3a\u6cf0\u8bed\u6cd5\u5f8b\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u8d44\u6e90\u8282\u7ea6\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09009", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09009", "abs": "https://arxiv.org/abs/2507.09009", "authors": ["Zhengxiao He", "Huayu Li", "Geng Yuan", "William D. S. Killgore", "Stuart F. Quan", "Chen X. Chen", "Ao Li"], "title": "Multimodal Cardiovascular Risk Profiling Using Self-Supervised Learning of Polysomnography", "comment": null, "summary": "Methods: We developed a self-supervised deep learning model that extracts\nmeaningful patterns from multi-modal signals (Electroencephalography (EEG),\nElectrocardiography (ECG), and respiratory signals). The model was trained on\ndata from 4,398 participants. Projection scores were derived by contrasting\nembeddings from individuals with and without CVD outcomes. External validation\nwas conducted in an independent cohort with 1,093 participants. The source code\nis available on https://github.com/miraclehetech/sleep-ssl. Results: The\nprojection scores revealed distinct and clinically meaningful patterns across\nmodalities. ECG-derived features were predictive of both prevalent and incident\ncardiac conditions, particularly CVD mortality. EEG-derived features were\npredictive of incident hypertension and CVD mortality. Respiratory signals\nadded complementary predictive value. Combining these projection scores with\nthe Framingham Risk Score consistently improved predictive performance,\nachieving area under the curve values ranging from 0.607 to 0.965 across\ndifferent outcomes. Findings were robustly replicated and validated in the\nexternal testing cohort. Conclusion: Our findings demonstrate that the proposed\nframework can generate individualized CVD risk scores directly from PSG data.\nThe resulting projection scores have the potential to be integrated into\nclinical practice, enhancing risk assessment and supporting personalized care.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5229\u7528\u8111\u7535\u56fe\u3001\u5fc3\u7535\u56fe\u548c\u547c\u5438\u4fe1\u53f7\u7684\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u5fc3\u8840\u7ba1\u75be\u75c5\u98ce\u9669\uff0c\u5e76\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u663e\u793a\u51fa\u826f\u597d\u7684\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u4ece\u591a\u6a21\u6001\u4fe1\u53f7\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u4ee5\u8fdb\u884c\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVD\uff09\u98ce\u9669\u8bc4\u4f30\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u4ece\u591a\u6a21\u6001\u4fe1\u53f7\uff08\u8111\u7535\u56fe\u3001\u5fc3\u7535\u56fe\u548c\u547c\u5438\u4fe1\u53f7\uff09\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u6a21\u5f0f\u3002\u6a21\u578b\u57284,398\u540d\u53c2\u4e0e\u8005\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002\u901a\u8fc7\u5bf9\u6bd4\u6709\u548c\u6ca1\u6709CVD\u7ed3\u5c40\u7684\u4e2a\u4f53\u7684\u5d4c\u5165\u6765\u63a8\u5bfc\u6295\u5f71\u8bc4\u5206\u3002\u5728\u62e5\u67091,093\u540d\u53c2\u4e0e\u8005\u7684\u72ec\u7acb\u961f\u5217\u4e2d\u8fdb\u884c\u4e86\u5916\u90e8\u9a8c\u8bc1\u3002\u6e90\u4ee3\u7801\u53ef\u5728https://github.com/miraclehetech/sleep-ssl\u83b7\u53d6\u3002", "result": "\u6295\u5f71\u8bc4\u5206\u63ed\u793a\u4e86\u8de8\u6a21\u6001\u7684\u3001\u4e34\u5e8a\u4e0a\u663e\u8457\u7684\u6a21\u5f0f\u3002ECG\u884d\u751f\u7684\u7279\u5f81\u53ef\u9884\u6d4b\u60a3\u75c5\u548c\u65b0\u53d1\u5fc3\u810f\u75c5\uff0c\u7279\u522b\u662fCVD\u6b7b\u4ea1\u7387\u3002EEG\u884d\u751f\u7684\u7279\u5f81\u53ef\u9884\u6d4b\u65b0\u53d1\u9ad8\u8840\u538b\u548cCVD\u6b7b\u4ea1\u7387\u3002\u547c\u5438\u4fe1\u53f7\u589e\u52a0\u4e86\u4e92\u8865\u7684\u9884\u6d4b\u4ef7\u503c\u3002\u5c06\u8fd9\u4e9b\u6295\u5f71\u8bc4\u5206\u4e0eFramingham\u98ce\u9669\u8bc4\u5206\u76f8\u7ed3\u5408\uff0c\u6301\u7eed\u63d0\u9ad8\u4e86\u9884\u6d4b\u6027\u80fd\uff0c\u5728\u4e0d\u540c\u7ed3\u5c40\u4e0b\u7684\u66f2\u7ebf\u4e0b\u9762\u79ef\u503c\u57280.607\u52300.965\u4e4b\u95f4\u3002\u7814\u7a76\u7ed3\u679c\u5728\u5916\u90e8\u6d4b\u8bd5\u961f\u5217\u4e2d\u5f97\u5230\u4e86\u7a33\u5065\u7684\u590d\u5236\u548c\u9a8c\u8bc1\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u76f4\u63a5\u4ecePSG\u6570\u636e\u751f\u6210\u4e2a\u4f53\u5316CVD\u98ce\u9669\u8bc4\u5206\u3002\u751f\u6210\u7684\u6295\u5f71\u8bc4\u5206\u6709\u6f5c\u529b\u6574\u5408\u5230\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u4ee5\u589e\u5f3a\u98ce\u9669\u8bc4\u4f30\u548c\u652f\u6301\u4e2a\u6027\u5316\u62a4\u7406\u3002"}}
{"id": "2507.10134", "categories": ["cs.AI", "53-01", "C.2"], "pdf": "https://arxiv.org/pdf/2507.10134", "abs": "https://arxiv.org/abs/2507.10134", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida"], "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "comment": "8 pages, 8 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.", "AI": {"tldr": "FRSICL\u662f\u4e00\u79cd\u65b0\u7684\u5728\u7ebf\u98de\u884c\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u5229\u7528LLM\u4e0a\u4e0b\u6587\u5b66\u4e60\u6765\u4f18\u5316\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u91ce\u706b\u76d1\u6d4b\u4e2d\u7684\u65e0\u4eba\u673a\u98de\u884c\u63a7\u5236\u548c\u6570\u636e\u6536\u96c6\u8c03\u5ea6\uff0c\u4ece\u800c\u6700\u5c0f\u5316\u4fe1\u606f\u5e74\u9f84\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u65e0\u4eba\u673a\u8f85\u52a9\u91ce\u706b\u76d1\u6d4b\u4e2d\u91c7\u6837\u6548\u7387\u4f4e\u3001\u4eff\u771f\u5230\u73b0\u5b9e\u5dee\u8ddd\u5927\u4ee5\u53ca\u8bad\u7ec3\u590d\u6742\u7b49\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08FRSICL\uff09\u7684\u5728\u7ebf\u98de\u884c\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u7528\u4e8e\u5b9e\u65f6\u4f18\u5316\u65e0\u4eba\u673a\u7684\u98de\u884c\u63a7\u5236\u548c\u6570\u636e\u6536\u96c6\u8c03\u5ea6\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cFRSICL\u5728\u6700\u5c0f\u5316\u5e73\u5747\u4fe1\u606f\u5e74\u9f84\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684\u65b9\u6cd5\u3002", "conclusion": "FRSICL\u5728\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u91ce\u706b\u76d1\u6d4b\u4e2d\u5b9e\u73b0\u4e86\u6bd4PPO\u548c\u6700\u8fd1\u90bb\u57fa\u7ebf\u66f4\u4f18\u7684\u6027\u80fd\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u4fe1\u606f\u5e74\u9f84\u3002"}}
{"id": "2507.10078", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10078", "abs": "https://arxiv.org/abs/2507.10078", "authors": ["Hiroki Sakamoto", "Kazuhiro Sato"], "title": "Compression Method for Deep Diagonal State Space Model Based on $H^2$ Optimal Reduction", "comment": "Accepted to IEEE Control Systems Letters", "summary": "Deep learning models incorporating linear SSMs have gained attention for\ncapturing long-range dependencies in sequential data. However, their large\nparameter sizes pose challenges for deployment on resource-constrained devices.\nIn this study, we propose an efficient parameter reduction method for these\nmodels by applying $H^{2}$ model order reduction techniques from control theory\nto their linear SSM components. In experiments, the LRA benchmark results show\nthat the model compression based on our proposed method outperforms an existing\nmethod using the Balanced Truncation, while successfully reducing the number of\nparameters in the SSMs to $1/32$ without sacrificing the performance of the\noriginal models.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528H2\u6a21\u578b\u964d\u9636\u6280\u672f\u9ad8\u6548\u7ea6\u51cf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u542b\u7ebf\u6027SSM\uff09\u53c2\u6570\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u4fdd\u6301\u6027\u80fd\u7684\u540c\u65f6\u5927\u5e45\u51cf\u5c0f\u6a21\u578b\u5c3a\u5bf8\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u5305\u542b\u7ebf\u6027SSM\u7684\u90e8\u5206\uff09\u56e0\u53c2\u6570\u91cf\u5927\u800c\u96be\u4ee5\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u7684\u95ee\u9898\u3002", "method": "\u5c06\u63a7\u5236\u7406\u8bba\u4e2d\u7684H2\u6a21\u578b\u964d\u9636\u6280\u672f\u5e94\u7528\u4e8e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u5305\u542b\u7ebf\u6027SSM\u7684\u90e8\u5206\uff09\u4ee5\u5b9e\u73b0\u53c2\u6570\u7ea6\u51cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728LRA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5c06SSM\u7684\u53c2\u6570\u91cf\u51cf\u5c11\u5230\u539f\u6765\u76841/32\uff0c\u540c\u65f6\u6027\u80fd\u4e0e\u539f\u6a21\u578b\u76f8\u5f53\uff0c\u5e76\u4e14\u4f18\u4e8e\u4f7f\u7528\u5e73\u8861\u622a\u65ad\u7684\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eH2\u6a21\u578b\u964d\u9636\u6280\u672f\u7684\u53c2\u6570\u7ea6\u51cf\u65b9\u6cd5\u5728LRA\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u5e73\u8861\u622a\u65ad\u65b9\u6cd5\uff0c\u5e76\u80fd\u5728\u4e0d\u727a\u7272\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5c06SSM\u53c2\u6570\u91cf\u51cf\u5c11\u5230\u539f\u6765\u76841/32\u3002"}}
{"id": "2507.10121", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10121", "abs": "https://arxiv.org/abs/2507.10121", "authors": ["Seung Hyun Kim", "Jiamiao Guo", "Arman Tekinalp", "Heng-Sheng Chang", "Ugur Akcal", "Tixian Wang", "Darren Biskup", "Benjamin Walt", "Girish Chowdhary", "Girish Krishnan", "Prashant G. Mehta", "Mattia Gazzola"], "title": "Simulations and experiments with assemblies of fiber-reinforced soft actuators", "comment": "8 pages, 4 figures This work has been submitted to the IEEE for\n  possible publication", "summary": "Soft continuum arms (SCAs) promise versatile manipulation through mechanical\ncompliance, for assistive devices, agriculture, search applications, or\nsurgery. However, SCAs' real-world use is challenging, partly due to their\nhard-to-control non-linear behavior. Here, a simulation framework for SCAs\nmodularly assembled out of fiber reinforced elastomeric enclosures (FREEs) is\ndeveloped and integrated with a video-tracking system for experimental testing\nand control design.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u4eff\u771f\u6846\u67b6\uff0c\u7528\u4e8e\u63a7\u5236\u7531\u7ea4\u7ef4\u589e\u5f3a\u5f39\u6027\u5916\u58f3\uff08FREEs\uff09\u7ec4\u6210\u7684\u8f6f\u8fde\u7eed\u81c2\uff08SCAs\uff09\uff0c\u5e76\u96c6\u6210\u4e86\u89c6\u9891\u8ddf\u8e2a\u7cfb\u7edf\u4ee5\u8fdb\u884c\u5b9e\u9a8c\u6d4b\u8bd5\u548c\u63a7\u5236\u8bbe\u8ba1\u3002", "motivation": "\u8f6f\u8fde\u7eed\u81c2\uff08SCAs\uff09\u5177\u6709\u901a\u7528\u7684\u64cd\u4f5c\u6f5c\u529b\uff0c\u4f46\u5176\u96be\u4ee5\u63a7\u5236\u7684\u975e\u7ebf\u6027\u884c\u4e3a\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7528\u4e8e\u8f6f\u8fde\u7eed\u81c2\uff08SCAs\uff09\u7684\u4eff\u771f\u6846\u67b6\uff0c\u5e76\u96c6\u6210\u4e86\u89c6\u9891\u8ddf\u8e2a\u7cfb\u7edf\uff0c\u7528\u4e8e\u5b9e\u9a8c\u6d4b\u8bd5\u548c\u63a7\u5236\u8bbe\u8ba1\u3002", "result": "\u9700\u8981\u66f4\u591a\u4fe1\u606f\u6765\u5f97\u51fa\u7ed3\u679c\u3002\u4f8b\u5982\uff0c\u8be5\u6846\u67b6\u5728\u63a7\u5236SCA\u65b9\u9762\u7684\u6709\u6548\u6027\u5982\u4f55\uff1f\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6027\u80fd\u5982\u4f55\uff1f", "conclusion": "\u9700\u8981\u66f4\u591a\u4fe1\u606f\u6765\u5f97\u51fa\u7ed3\u8bba\u3002"}}
{"id": "2507.09256", "categories": ["cs.CV", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.09256", "abs": "https://arxiv.org/abs/2507.09256", "authors": ["Junyu Chen", "Yihua Gao", "Mingyuan Ge", "Mingyong Li"], "title": "Ambiguity-Aware and High-Order Relation Learning for Multi-Grained Image-Text Matching", "comment": "Accepted by the Knowledge-Based Systems(KBS), 2025", "summary": "Image-text matching is crucial for bridging the semantic gap between computer\nvision and natural language processing. However, existing methods still face\nchallenges in handling high-order associations and semantic ambiguities among\nsimilar instances. These ambiguities arise from subtle differences between soft\npositive samples (semantically similar but incorrectly labeled) and soft\nnegative samples (locally matched but globally inconsistent), creating matching\nuncertainties. Furthermore, current methods fail to fully utilize the\nneighborhood relationships among semantically similar instances within training\nbatches, limiting the model's ability to learn high-order shared knowledge.\nThis paper proposes the Ambiguity-Aware and High-order Relation learning\nframework (AAHR) to address these issues. AAHR constructs a unified\nrepresentation space through dynamic clustering prototype contrastive learning,\neffectively mitigating the soft positive sample problem. The framework\nintroduces global and local feature extraction mechanisms and an adaptive\naggregation network, significantly enhancing full-grained semantic\nunderstanding capabilities. Additionally, AAHR employs intra-modal and\ninter-modal correlation matrices to investigate neighborhood relationships\namong sample instances thoroughly. It incorporates GNN to enhance semantic\ninteractions between instances. Furthermore, AAHR integrates momentum\ncontrastive learning to expand the negative sample set. These combined\nstrategies significantly improve the model's ability to discriminate between\nfeatures. Experimental results demonstrate that AAHR outperforms existing\nstate-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets,\nconsiderably improving the accuracy and efficiency of image-text matching. The\ncode and model checkpoints for this research are available at\nhttps://github.com/Image-Text-Matching/AAHR .", "AI": {"tldr": "AAHR framework improves image-text matching by addressing semantic ambiguities and utilizing neighborhood relationships through dynamic clustering, contrastive learning, and GNNs, outperforming existing methods on multiple datasets.", "motivation": "Existing image-text matching methods struggle with high-order associations and semantic ambiguities among similar instances (soft positives and soft negatives), and fail to utilize neighborhood relationships among semantically similar instances within training batches.", "method": "AAHR constructs a unified representation space through dynamic clustering prototype contrastive learning, mitigating the soft positive sample problem. It utilizes global and local feature extraction mechanisms and an adaptive aggregation network for enhanced semantic understanding. Additionally, it employs intra-modal and inter-modal correlation matrices with GNN to investigate neighborhood relationships and incorporates momentum contrastive learning to expand the negative sample set, improving feature discrimination.", "result": "Experimental results demonstrate that AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching.", "conclusion": "AAHR outperforms existing state-of-the-art methods on Flickr30K, MSCOCO, and ECCV Caption datasets, considerably improving the accuracy and efficiency of image-text matching."}}
{"id": "2507.09781", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09781", "abs": "https://arxiv.org/abs/2507.09781", "authors": ["Daniele Trisciani", "Marco Cattaneo", "Zolt\u00e1n Zimbor\u00e1s"], "title": "Decomposition of multi-qutrit gates generated by Weyl-Heisenberg strings", "comment": "28 pages, 18 figures", "summary": "Decomposing unitary operations into native gates is an essential step for\nimplementing quantum algorithms. For qubit-based devices, where native gates\nare typically single- and two-qubit operations, a range of decomposition\ntechniques have been developed. In particular, efficient algorithms exist for\ndecomposing exponentials of Pauli strings while taking hardware topology in\naccount. Motivated by the growing interest in qutrit-based quantum computing,\nwe develop analogous decomposition methods for qutrit systems. Specifically, we\nintroduce an algorithm that decomposes the exponential of an arbitrary tensor\nproduct of Weyl-Heisenberg operators (plus their Hermitian conjugation) into\nsingle- and two-qutrit gates. We further extend this approach to unitaries\ngenerated by Gell-Mann string (i.e., a tensor product of Gell-Mann matrices).\nSince both Gell-Mann matrices and Weyl-Heisenberg operators form (together with\nidentity) complete operator bases of qutrit operators, we can use this result\nalso to decompose any multi-qutrit gate that is diagonal up to single-qutrit\nrotations. As a practical application, we use our method to decompose the\nlayers of the quantum approximate optimization algorithm for qutrit-based\nimplementations of the graph k-coloring problem. For values of $k$ well-suited\nto qutrit architectures (e.g., $k=3$ or in general $k=3^n$), our approach\nyields significantly shallower circuits compared to qubit-based\nimplementations, an advantage that grows with problem size, while also\nrequiring a smaller total Hilbert space dimension. Finally, we also address the\nrouting challenge in qutrit architectures that arises due to the limited\nconnectivity of the devices. In particular, we generalize the Steiner-Gauss\nmethod, originally developed to reduce CNOT counts in qubit circuit, to\noptimize gate routing in qutrit-based systems.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8e\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u7684\u91cf\u5b50\u95e8\u5206\u89e3\u65b0\u65b9\u6cd5\uff0c\u53ef\u6709\u6548\u51cf\u5c11\u91cf\u5b50\u7535\u8def\u6df1\u5ea6\uff0c\u5e76\u4f18\u5316\u95e8\u8def\u7531\u3002", "motivation": "\u7531\u4e8e\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u5728\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u5f00\u53d1\u76f8\u5e94\u7684\u91cf\u5b50\u95e8\u5206\u89e3\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u91cf\u5b50\u7b97\u6cd5\u7684\u6548\u7387\u548c\u53ef\u884c\u6027\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06 Weyl-Heisenberg \u7b97\u7b26\uff08\u53ca\u5176\u5171\u8f6d\uff09\u7684\u6307\u6570\u5206\u89e3\u4e3a\u5355\u91cf\u5b50\u6bd4\u7279\u548c\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u7684\u65b0\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u6269\u5c55\u5230 Gell-Mann \u7b97\u7b26\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u63a8\u5e7f Steiner-Gauss \u65b9\u6cd5\u89e3\u51b3\u4e86\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e2d\u7684\u8def\u7531\u6311\u6218\u3002", "result": "\u7814\u7a76\u6210\u529f\u5730\u5c06 Weyl-Heisenberg \u7b97\u7b26\u548c Gell-Mann \u7b97\u7b26\u7684\u6307\u6570\u5206\u89e3\u4e3a\u5355\u91cf\u5b50\u6bd4\u7279\u548c\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\uff0c\u5e76\u4e3a\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u63d0\u51fa\u4e86\u4f18\u5316\u7684\u8def\u7531\u7b56\u7565\u3002\u5728\u91cf\u5b50\u8fd1\u4f3c\u4f18\u5316\u7b97\u6cd5\uff08QAOA\uff09\u7684\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u91cf\u5b50\u6bd4\u7279\u7535\u8def\u7684\u6df1\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u56fe k \u7740\u8272\u95ee\u9898\u65f6\uff0c\u5e76\u4e14\u968f\u7740\u95ee\u9898\u89c4\u6a21\u7684\u589e\u5927\uff0c\u8fd9\u79cd\u4f18\u52bf\u8d8a\u53d1\u660e\u663e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u91cf\u5b50\u6bd4\u7279\u7684\u8bbe\u5907\u5f00\u53d1\u4e86\u65b0\u7684\u91cf\u5b50\u95e8\u5206\u89e3\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u9488\u5bf9\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u5c06 Weyl-Heisenberg \u7b97\u7b26\uff08\u53ca\u5176\u5171\u8f6d\uff09\u7684\u6307\u6570\u5206\u89e3\u4e3a\u5355\u91cf\u5b50\u6bd4\u7279\u548c\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u6269\u5c55\u5230 Gell-Mann \u7b97\u7b26\u3002\u8fd9\u4e9b\u5206\u89e3\u65b9\u6cd5\u5bf9\u4e8e\u5728\u91cf\u5b50\u6bd4\u7279\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u91cf\u5b50\u7b97\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u91cf\u5b50\u7535\u8def\u6df1\u5ea6\u5e76\u4f18\u5316\u95e8\u8def\u7531\u3002"}}
{"id": "2507.09701", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09701", "abs": "https://arxiv.org/abs/2507.09701", "authors": ["Shulin Huang", "Linyi Yang", "Yue Zhang"], "title": "MCEval: A Dynamic Framework for Fair Multilingual Cultural Evaluation of LLMs", "comment": null, "summary": "Large language models exhibit cultural biases and limited cross-cultural\nunderstanding capabilities, particularly when serving diverse global user\npopulations. We propose MCEval, a novel multilingual evaluation framework that\nemploys dynamic cultural question construction and enables causal analysis\nthrough Counterfactual Rephrasing and Confounder Rephrasing. Our comprehensive\nevaluation spans 13 cultures and 13 languages, systematically assessing both\ncultural awareness and cultural bias across different linguistic scenarios. The\nframework provides 39,897 cultural awareness instances and 17,940 cultural bias\ninstances. Experimental results reveal performance disparities across different\nlinguistic scenarios, demonstrating that optimal cultural performance is not\nonly linked to training data distribution, but also is related to\nlanguage-culture alignment. The evaluation results also expose the fairness\nissue, where approaches appearing successful in the English scenario create\nsubstantial disadvantages. MCEval represents the first comprehensive\nmultilingual cultural evaluation framework that provides deeper insights into\nLLMs' cultural understanding.", "AI": {"tldr": "MCEval\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6587\u5316\u504f\u89c1\u548c\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\uff0c\u53d1\u73b0\u6a21\u578b\u8868\u73b0\u4e0e\u8bed\u8a00\u6587\u5316\u5bf9\u9f50\u76f8\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u8de8\u6587\u5316\u516c\u5e73\u6027\u95ee\u9898\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u670d\u52a1\u5168\u7403\u591a\u6837\u5316\u7528\u6237\u7fa4\u4f53\u65f6\uff0c\u8868\u73b0\u51fa\u6587\u5316\u504f\u89c1\u548c\u6709\u9650\u7684\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\u6765\u7cfb\u7edf\u5730\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u6846\u67b6MCEval\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u52a8\u6001\u6587\u5316\u95ee\u9898\u6784\u5efa\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6539\u5199\u548c\u6df7\u6dc6\u56e0\u7d20\u6539\u5199\u5b9e\u73b0\u56e0\u679c\u5206\u6790\uff0c\u4ee5\u8bc4\u4f30LLM\u7684\u6587\u5316\u610f\u8bc6\u548c\u6587\u5316\u504f\u89c1\u3002", "result": "\u8be5\u8bc4\u4f30\u6846\u67b6\u8986\u76d613\u79cd\u6587\u5316\u548c13\u79cd\u8bed\u8a00\uff0c\u5305\u542b39,897\u4e2a\u6587\u5316\u610f\u8bc6\u5b9e\u4f8b\u548c17,940\u4e2a\u6587\u5316\u504f\u89c1\u5b9e\u4f8b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u5728\u4e0d\u540c\u7684\u8bed\u8a00\u573a\u666f\u4e2d\u5b58\u5728\u6027\u80fd\u5dee\u5f02\uff0c\u4e14\u516c\u5e73\u6027\u95ee\u9898\u7a81\u51fa\uff0c\u5373\u5728\u82f1\u8bed\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u7684\u65b9\u6cd5\u53ef\u80fd\u5728\u5176\u4ed6\u573a\u666f\u4e2d\u9020\u6210\u663e\u8457\u52a3\u52bf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MCEval\u6846\u67b6\uff0c\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8de8\u6587\u5316\u7406\u89e3\u80fd\u529b\u548c\u6587\u5316\u504f\u89c1\u7684\u591a\u8bed\u8a00\u8bc4\u4f30\u6846\u67b6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cLLM\u7684\u6700\u4f73\u6587\u5316\u8868\u73b0\u4e0d\u4ec5\u4e0e\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u6709\u5173\uff0c\u8fd8\u4e0e\u8bed\u8a00\u6587\u5316\u5bf9\u9f50\u6709\u5173\uff0c\u5e76\u63ed\u793a\u4e86\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5373\u5728\u82f1\u8bed\u573a\u666f\u4e2d\u8868\u73b0\u826f\u597d\u7684\u65b9\u6cd5\u53ef\u80fd\u5728\u5176\u4ed6\u8bed\u8a00\u6587\u5316\u573a\u666f\u4e2d\u5e26\u6765\u663e\u8457\u52a3\u52bf\u3002MCEval\u662f\u7b2c\u4e00\u4e2a\u63d0\u4f9b\u5bf9LLM\u6587\u5316\u7406\u89e3\u6df1\u5165\u89c1\u89e3\u7684\u7efc\u5408\u6027\u591a\u8bed\u8a00\u6587\u5316\u8bc4\u4f30\u6846\u67b6\u3002"}}
{"id": "2507.09016", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09016", "abs": "https://arxiv.org/abs/2507.09016", "authors": ["Karim Galliamov", "Ivan Titov", "Ilya Pershin"], "title": "Enhancing RLHF with Human Gaze Modeling", "comment": null, "summary": "Reinforcement Learning from Human Feedback (RLHF) aligns language models with\nhuman preferences but is computationally expensive. We explore two approaches\nthat leverage human gaze modeling to enhance RLHF: (1) gaze-aware reward models\nand (2) gaze-based distribution of sparse rewards at token level. Our\nexperiments demonstate that gaze-informed RLHF achieves faster convergence\nwhile maintaining or slightly improving performance, thus, reducing\ncomputational costs during policy optimization. These results show that human\ngaze provides a valuable and underused signal for policy optimization, pointing\nto a promising direction for improving RLHF efficiency.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u4fe1\u53f7\u6765\u63d0\u9ad8RLHF\u7684\u6548\u7387\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u53ef\u4ee5\u52a0\u5feb\u6536\u655b\u901f\u5ea6\u5e76\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "RLHF\u867d\u7136\u80fd\u4f7f\u8bed\u8a00\u6a21\u578b\u4e0e\u4eba\u7c7b\u504f\u597d\u4fdd\u6301\u4e00\u81f4\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u964d\u4f4eRLHF\u8ba1\u7b97\u6210\u672c\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63a2\u7d22\u4e86\u4e24\u79cd\u5229\u7528\u4eba\u7c7b\u6ce8\u89c6\u6a21\u578b\u6765\u589e\u5f3aRLHF\u7684\u65b9\u6cd5\uff1a(1)\u6ce8\u89c6\u611f\u77e5\u5956\u52b1\u6a21\u578b\u548c(2)\u57fa\u4e8e\u6ce8\u89c6\u7684\u7a00\u758f\u5956\u52b1\u5728token\u7ea7\u522b\u7684\u5206\u5e03\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5f15\u5165\u6ce8\u89c6\u4fe1\u606f\u7684\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u7565\u5fae\u63d0\u9ad8\u6027\u80fd\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u4eba\u7c7b\u6ce8\u89c6\u4fe1\u53f7\u53ef\u4ee5\u63d0\u9ad8RLHF\u7684\u6548\u7387\uff0c\u5728\u7b56\u7565\u4f18\u5316\u8fc7\u7a0b\u4e2d\u5177\u6709\u6f5c\u5728\u7684\u5e94\u7528\u4ef7\u503c\u3002"}}
{"id": "2507.10131", "categories": ["cs.RO", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10131", "abs": "https://arxiv.org/abs/2507.10131", "authors": ["Cesar Alan Contreras", "Manolis Chiou", "Alireza Rastegarpanah", "Michal Szulik", "Rustam Stolkin"], "title": "Probabilistic Human Intent Prediction for Mobile Manipulation: An Evaluation with Human-Inspired Constraints", "comment": "Submitted to Journal of Intelligent & Robotic Systems (Under Review)", "summary": "Accurate inference of human intent enables human-robot collaboration without\nconstraining human control or causing conflicts between humans and robots. We\npresent GUIDER (Global User Intent Dual-phase Estimation for Robots), a\nprobabilistic framework that enables a robot to estimate the intent of human\noperators. GUIDER maintains two coupled belief layers, one tracking navigation\ngoals and the other manipulation goals. In the Navigation phase, a Synergy Map\nblends controller velocity with an occupancy grid to rank interaction areas.\nUpon arrival at a goal, an autonomous multi-view scan builds a local 3D cloud.\nThe Manipulation phase combines U2Net saliency, FastSAM instance saliency, and\nthree geometric grasp-feasibility tests, with an end-effector kinematics-aware\nupdate rule that evolves object probabilities in real-time. GUIDER can\nrecognize areas and objects of intent without predefined goals. We evaluated\nGUIDER on 25 trials (five participants x five task variants) in Isaac Sim, and\ncompared it with two baselines, one for navigation and one for manipulation.\nAcross the 25 trials, GUIDER achieved a median stability of 93-100% during\nnavigation, compared with 60-100% for the BOIR baseline, with an improvement of\n39.5% in a redirection scenario (T5). During manipulation, stability reached\n94-100% (versus 69-100% for Trajectron), with a 31.4% difference in a\nredirection task (T3). In geometry-constrained trials (manipulation), GUIDER\nrecognized the object intent three times earlier than Trajectron (median\nremaining time to confident prediction 23.6 s vs 7.8 s). These results validate\nour dual-phase framework and show improvements in intent inference in both\nphases of mobile manipulation tasks.", "AI": {"tldr": "GUIDER, a dual-phase probabilistic framework, enhances robot understanding of human intent for navigation and manipulation, improving collaboration stability and prediction speed compared to existing methods.", "motivation": "Accurate human intent inference is crucial for seamless human-robot collaboration, allowing robots to assist operators without causing conflicts or imposing constraints.", "method": "GUIDER is a probabilistic framework with two coupled belief layers for tracking navigation and manipulation goals. The Navigation phase uses a Synergy Map combining controller velocity and an occupancy grid. The Manipulation phase integrates U2Net saliency, FastSAM instance saliency, and geometric grasp-feasibility tests, with an end-effector kinematics-aware update rule for real-time object probability evolution. It can recognize intent without predefined goals.", "result": "In simulations, GUIDER achieved 93-100% navigation stability (vs. 60-100% for BOIR baseline) and 94-100% manipulation stability (vs. 69-100% for Trajectron baseline). It showed a 39.5% improvement in a navigation redirection scenario and a 31.4% improvement in a manipulation redirection task. GUIDER also predicted object intent three times faster in geometry-constrained trials (median 7.8s vs. 23.6s).", "conclusion": "The proposed dual-phase framework, GUIDER, significantly improves intent inference in both navigation and manipulation phases of mobile manipulation tasks, outperforming baseline methods in stability and prediction time."}}
{"id": "2507.09266", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09266", "abs": "https://arxiv.org/abs/2507.09266", "authors": ["JianHe Low", "Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "SAGE: Segment-Aware Gloss-Free Encoding for Token-Efficient Sign Language Translation", "comment": "Accepted in International Conference on Computer Vision (ICCV)\n  Workshops", "summary": "Gloss-free Sign Language Translation (SLT) has advanced rapidly, achieving\nstrong performances without relying on gloss annotations. However, these gains\nhave often come with increased model complexity and high computational demands,\nraising concerns about scalability, especially as large-scale sign language\ndatasets become more common. We propose a segment-aware visual tokenization\nframework that leverages sign segmentation to convert continuous video into\ndiscrete, sign-informed visual tokens. This reduces input sequence length by up\nto 50% compared to prior methods, resulting in up to 2.67x lower memory usage\nand better scalability on larger datasets. To bridge the visual and linguistic\nmodalities, we introduce a token-to-token contrastive alignment objective,\nalong with a dual-level supervision that aligns both language embeddings and\nintermediate hidden states. This improves fine-grained cross-modal alignment\nwithout relying on gloss-level supervision. Our approach notably exceeds the\nperformance of state-of-the-art methods on the PHOENIX14T benchmark, while\nsignificantly reducing sequence length. Further experiments also demonstrate\nour improved performance over prior work under comparable sequence-lengths,\nvalidating the potential of our tokenization and alignment strategies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6bb5\u611f\u77e5\u89c6\u89c9\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u624b\u8bed\u5206\u5272\u5c06\u89c6\u9891\u8f6c\u6362\u4e3a\u89c6\u89c9\u6807\u8bb0\uff0c\u51cf\u5c11\u4e86\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u548c\u5185\u5b58\u4f7f\u7528\u91cf\uff0c\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u548c\u53cc\u5c42\u76d1\u7763\u63d0\u9ad8\u4e86\u8de8\u6a21\u6001\u5bf9\u9f50\uff0c\u65e0\u9700\u8bcd\u6761\u76d1\u7763\u3002\u5728PHOENIX14T\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u65e0\u624b\u8bed\u8bcd\u6761\u7684\u624b\u8bed\u7ffb\u8bd1\uff08SLT\uff09\u6a21\u578b\u590d\u6742\u6027\u548c\u9ad8\u8ba1\u7b97\u9700\u6c42\u95ee\u9898\uff0c\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\uff0c\u4ee5\u5e94\u5bf9\u5927\u89c4\u6a21\u624b\u8bed\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6bb5\u611f\u77e5\u89c6\u89c9\u6807\u8bb0\u5316\u6846\u67b6\uff0c\u5229\u7528\u624b\u8bed\u5206\u5272\u5c06\u8fde\u7eed\u89c6\u9891\u8f6c\u6362\u4e3a\u79bb\u6563\u7684\u3001\u53d7\u624b\u8bed\u4fe1\u606f\u542f\u53d1\u7684\u89c6\u89c9\u6807\u8bb0\u3002\u5f15\u5165\u4e86\u6807\u8bb0\u5230\u6807\u8bb0\u7684\u5bf9\u6bd4\u5bf9\u9f50\u76ee\u6807\u548c\u53cc\u5c42\u76d1\u7763\uff0c\u5bf9\u8bed\u8a00\u5d4c\u5165\u548c\u4e2d\u95f4\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "\u5c06\u8f93\u5165\u5e8f\u5217\u957f\u5ea6\u51cf\u5c11\u591a\u8fbe50%\uff0c\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c11\u591a\u8fbe2.67\u500d\uff0c\u5e76\u5728\u5927\u578b\u6570\u636e\u96c6\u4e0a\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u548c\u53cc\u5c42\u76d1\u7763\u63d0\u9ad8\u4e86\u8de8\u6a21\u6001\u7684\u7ec6\u7c92\u5ea6\u5bf9\u9f50\uff0c\u65e0\u9700\u8bcd\u6761\u7ea7\u522b\u7684\u76d1\u7763\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728PHOENIX14T\u57fa\u51c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u5927\u5927\u7f29\u77ed\u4e86\u5e8f\u5217\u957f\u5ea6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6807\u8bb0\u5316\u548c\u5bf9\u9f50\u7b56\u7565\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09841", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09841", "abs": "https://arxiv.org/abs/2507.09841", "authors": ["Nahid Binandeh Dehaghani", "Rafal Wisniewski", "A. Pedro Aguiar"], "title": "Quantum Solution Framework for Finite-Horizon LQG Control via Block Encodings and QSVT", "comment": null, "summary": "We present a quantum algorithm for solving the finite-horizon discrete-time\nLinear Quadratic Gaussian (LQG) control problem, which integrates optimal\ncontrol and state estimation in the presence of stochastic disturbances and\nnoise. Classical approaches to LQG require solving a backward Riccati recursion\nand a forward Kalman filter, both requiring computationally expensive matrix\noperations with overall time complexity $\\mathcal{O}(T n^3)$, where $n$ is the\nsystem dimension and $T$ is the time horizon. While efficient classical solvers\nexist, especially for small to medium-sized systems, their computational\ncomplexity grows rapidly with system dimension. To address this, we reformulate\nthe full LQG pipeline using quantum linear algebra primitives, including\nblock-encoded matrix representations and quantum singular value transformation\n(QSVT) techniques for matrix inversion and multiplication. We formally analyze\nthe time complexity of each algorithmic component. Under standard assumptions\non matrix condition numbers and encoding precision, the total runtime of the\nquantum LQG algorithm scales polylogarithmically with the system dimension $n$\nand linearly with the time horizon $T$, offering an asymptotic quantum speedup\nover classical methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50LQG\u7b97\u6cd5\uff0c\u5229\u7528\u91cf\u5b50\u7ebf\u6027\u4ee3\u6570\u6280\u672f\uff0c\u76f8\u6bd4\u7ecf\u5178\u7b97\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u5177\u6709\u66f4\u4f18\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7ecf\u5178LQG\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u7cfb\u7edf\u65f6\u8ba1\u7b97\u590d\u6742\u5ea6\u968f\u7cfb\u7edf\u7ef4\u5ea6\u589e\u957f\u8fc7\u5feb\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u5229\u7528\u91cf\u5b50\u8ba1\u7b97\u7684\u4f18\u52bf\u6765\u63d0\u4f9b\u6e10\u8fdb\u5f0f\u52a0\u901f\u3002", "method": "\u8be5\u91cf\u5b50\u7b97\u6cd5\u901a\u8fc7\u5229\u7528\u91cf\u5b50\u7ebf\u6027\u4ee3\u6570\u57fa\u5143\uff08\u5305\u62ec\u5757\u7f16\u7801\u77e9\u9635\u8868\u793a\u548c\u7528\u4e8e\u77e9\u9635\u6c42\u9006\u53ca\u4e58\u6cd5\u7684\u91cf\u5b50\u5947\u5f02\u503c\u53d8\u6362\uff08QSVT\uff09\u6280\u672f\uff09\u6765\u91cd\u65b0\u6784\u5efa\u6574\u4e2aLQG\u6d41\u7a0b\u3002\u7814\u7a76\u4e2d\u5bf9\u6bcf\u4e2a\u7b97\u6cd5\u7ec4\u4ef6\u7684\u65f6\u95f4\u590d\u6742\u5ea6\u8fdb\u884c\u4e86\u6b63\u5f0f\u5206\u6790\u3002", "result": "\u5728\u6807\u51c6\u77e9\u9635\u6761\u4ef6\u6570\u548c\u7f16\u7801\u7cbe\u5ea6\u7684\u5047\u8bbe\u4e0b\uff0c\u91cf\u5b50LQG\u7b97\u6cd5\u7684\u603b\u8fd0\u884c\u65f6\u95f4\u4e0e\u7cfb\u7edf\u7ef4\u5ea6n\u5448\u5f31\u591a\u9879\u5f0f\u5bf9\u6570\u589e\u957f\uff0c\u4e0e\u65f6\u95f4\u89c6\u754cT\u5448\u7ebf\u6027\u589e\u957f\uff0c\u4ece\u800c\u5728\u6e10\u8fdb\u610f\u4e49\u4e0a\u5b9e\u73b0\u4e86\u8d85\u8d8a\u7ecf\u5178\u65b9\u6cd5\u7684\u91cf\u5b50\u52a0\u901f\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u89e3\u51b3\u6709\u9650\u65f6\u95f4\u79bb\u6563\u65f6\u95f4\u7ebf\u6027\u4e8c\u6b21\u9ad8\u65af\uff08LQG\uff09\u63a7\u5236\u95ee\u9898\u7684\u91cf\u5b50\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u6574\u5408\u4e86\u6700\u4f18\u63a7\u5236\u548c\u72b6\u6001\u4f30\u8ba1\uff0c\u5e76\u5904\u7406\u968f\u673a\u6270\u52a8\u548c\u566a\u58f0\u3002"}}
{"id": "2507.09709", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09709", "abs": "https://arxiv.org/abs/2507.09709", "authors": ["Baturay Saglam", "Paul Kassianik", "Blaine Nelson", "Sajana Weerawardhena", "Yaron Singer", "Amin Karbasi"], "title": "Large Language Models Encode Semantics in Low-Dimensional Linear Subspaces", "comment": null, "summary": "Understanding the latent space geometry of large language models (LLMs) is\nkey to interpreting their behavior and improving alignment. \\baturay{However,\nit remains unclear to what extent LLMs internally organize representations\nrelated to semantic understanding. To investigate this, we conduct a\nlarge-scale empirical study of hidden states in transformer-based LLMs,\nanalyzing 11 decoder-only models across 6 scientific topics and 12 layers each.\nWe find that high-level semantic information consistently lies in\nlow-dimensional subspaces that form linearly separable representations across\ndistinct domains. This separability becomes more pronounced in deeper layers\nand under prompts that trigger structured reasoning or alignment\nbehaviors$\\unicode{x2013}$even when surface content is unchanged. This geometry\nenables simple yet effective causal interventions in hidden space; for example,\nreasoning patterns like chain-of-thought can be captured by a single vector\ndirection. Together, these findings support the development of geometry-aware\ntools that operate directly on latent representations to detect and mitigate\nharmful or adversarial content, using methods such as transport-based defenses\nthat leverage this separability. As a proof of concept, we demonstrate this\npotential by training a simple MLP classifier as a lightweight latent-space\nguardrail, which detects adversarial and malicious prompts with high precision.", "AI": {"tldr": "LLM\u7684\u9690\u85cf\u72b6\u6001\u8868\u73b0\u51fa\u53ef\u5206\u79bb\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u53ef\u4ee5\u88ab\u5229\u7528\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u5bf9\u9f50\u6027\u3002", "motivation": "\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u662f\u89e3\u91ca\u5176\u884c\u4e3a\u548c\u6539\u8fdb\u5176\u5bf9\u9f50\u7684\u5173\u952e\u3002\u7136\u800c\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695aLLM\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u5728\u5176\u5185\u90e8\u7ec4\u7ec7\u4e0e\u8bed\u4e49\u7406\u89e3\u76f8\u5173\u7684\u8868\u793a\u3002", "method": "\u901a\u8fc7\u5bf911\u4e2a\u57fa\u4e8eTransformer\u7684LLM\u7684\u9690\u85cf\u72b6\u6001\u8fdb\u884c\u5927\u89c4\u6a21\u5b9e\u8bc1\u7814\u7a76\uff0c\u5206\u6790\u4e866\u4e2a\u79d1\u5b66\u4e3b\u9898\u548c12\u4e2a\u5c42\u7ea7\u3002\u7814\u7a76\u4eba\u5458\u53d1\u73b0\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u4f4d\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4e14\u5728\u66f4\u6df1\u7684\u5c42\u4e2d\u4ee5\u53ca\u5728\u89e6\u53d1\u7ed3\u6784\u5316\u63a8\u7406\u6216\u5bf9\u9f50\u884c\u4e3a\u7684\u63d0\u793a\u4e0b\uff0c\u8fd9\u79cd\u53ef\u5206\u79bb\u6027\u4f1a\u66f4\u52a0\u660e\u663e\u3002\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5f00\u53d1\u76f4\u63a5\u5728\u6f5c\u5728\u8868\u793a\u4e0a\u64cd\u4f5c\u7684\u3001\u611f\u77e5\u51e0\u4f55\u7684\u5de5\u5177\uff0c\u4ee5\u68c0\u6d4b\u548c\u51cf\u8f7b\u6709\u5bb3\u6216\u5bf9\u6297\u6027\u5185\u5bb9\u3002\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684MLP\u5206\u7c7b\u5668\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u7684\u6f5c\u5728\u7a7a\u95f4\u62a4\u680f\uff0c\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u5206\u7c7b\u5668\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5730\u68c0\u6d4b\u5bf9\u6297\u6027\u548c\u6076\u610f\u63d0\u793a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u4e00\u81f4\u5730\u4f4d\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u5f62\u6210\u4e86\u8de8\u4e0d\u540c\u57df\u7684\u53ef\u7ebf\u6027\u5206\u79bb\u7684\u8868\u793a\u3002\u8fd9\u79cd\u53ef\u5206\u79bb\u6027\u5728\u66f4\u6df1\u7684\u5c42\u4e2d\u4ee5\u53ca\u5728\u89e6\u53d1\u7ed3\u6784\u5316\u63a8\u7406\u6216\u5bf9\u9f50\u884c\u4e3a\u7684\u63d0\u793a\u4e0b\u4f1a\u66f4\u52a0\u660e\u663e\uff0c\u5373\u4f7f\u8868\u9762\u5185\u5bb9\u4fdd\u6301\u4e0d\u53d8\u3002\u8fd9\u79cd\u51e0\u4f55\u7ed3\u6784\u4f7f\u5f97\u5728\u9690\u85cf\u7a7a\u95f4\u4e2d\u8fdb\u884c\u7b80\u5355\u800c\u6709\u6548\u7684\u56e0\u679c\u5e72\u9884\u6210\u4e3a\u53ef\u80fd\u3002\u4f8b\u5982\uff0c\u50cf\u94fe\u5f0f\u601d\u8003\u8fd9\u6837\u7684\u63a8\u7406\u6a21\u5f0f\u53ef\u4ee5\u88ab\u5355\u4e2a\u5411\u91cf\u65b9\u5411\u6355\u83b7\u3002", "conclusion": "LLMs\u7684\u6f5c\u5728\u7a7a\u95f4\u51e0\u4f55\u7ed3\u6784\u53ef\u4ee5\u88ab\u7406\u89e3\u548c\u5229\u7528\uff0c\u4ee5\u6539\u8fdb\u6a21\u578b\u7684\u5bf9\u9f50\u548c\u5b89\u5168\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u9ad8\u5c42\u8bed\u4e49\u4fe1\u606f\u5b58\u5728\u4e8e\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u4e2d\uff0c\u5e76\u4e14\u5728\u66f4\u6df1\u7684\u5c42\u4e2d\u4ee5\u53ca\u5728\u89e6\u53d1\u7ed3\u6784\u5316\u63a8\u7406\u6216\u5bf9\u9f50\u884c\u4e3a\u7684\u63d0\u793a\u4e0b\uff0c\u8fd9\u79cd\u53ef\u5206\u79bb\u6027\u4f1a\u66f4\u52a0\u660e\u663e\u3002\u7814\u7a76\u7ed3\u679c\u652f\u6301\u5f00\u53d1\u76f4\u63a5\u5728\u6f5c\u5728\u8868\u793a\u4e0a\u64cd\u4f5c\u7684\u3001\u611f\u77e5\u51e0\u4f55\u7684\u5de5\u5177\uff0c\u4ee5\u68c0\u6d4b\u548c\u51cf\u8f7b\u6709\u5bb3\u6216\u5bf9\u6297\u6027\u5185\u5bb9\u3002\u7814\u7a76\u4eba\u5458\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u7b80\u5355\u7684MLP\u5206\u7c7b\u5668\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u7684\u6f5c\u5728\u7a7a\u95f4\u62a4\u680f\uff0c\u8bc1\u660e\u4e86\u8fd9\u79cd\u65b9\u6cd5\u7684\u6709\u6548\u6027\uff0c\u8be5\u5206\u7c7b\u5668\u80fd\u591f\u9ad8\u7cbe\u5ea6\u5730\u68c0\u6d4b\u5bf9\u6297\u6027\u548c\u6076\u610f\u63d0\u793a\u3002"}}
{"id": "2507.10156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10156", "abs": "https://arxiv.org/abs/2507.10156", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Stavroula Mougiakakou"], "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation", "comment": "10 pages, 2 Figures, 7 tables", "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u745e\u58eb\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31\uff08SwissFKG\uff09\uff0c\u4e00\u4e2a\u6574\u5408\u4e86\u98df\u8c31\u3001\u98df\u6750\u3001\u8425\u517b\u548c\u996e\u98df\u9650\u5236\u4fe1\u606f\u7684\u77e5\u8bc6\u5e93\u3002\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u8fdb\u884c\u77e5\u8bc6\u586b\u5145\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u51c6\u7684\u8425\u517b\u54a8\u8be2\u670d\u52a1\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u996e\u98df\u8bc4\u4f30\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u81b3\u98df\u8bc4\u4f30\u7cfb\u7edf\u672a\u80fd\u5145\u5206\u8003\u8651\u975e\u89c6\u89c9\u56e0\u7d20\uff0c\u5982\u98df\u6750\u66ff\u6362\u5bf9\u8425\u517b\u6210\u5206\u7684\u5f71\u54cd\uff0c\u4ee5\u53ca\u4e2a\u4f53\u996e\u98df\u9700\u6c42\uff08\u5982\u8fc7\u654f\u3001\u9650\u5236\u3001\u6587\u5316\u4e60\u4fd7\u548c\u4e2a\u4eba\u504f\u597d\uff09\u3002\u745e\u58eb\u98df\u54c1\u4fe1\u606f\u5206\u6563\u4e14\u7f3a\u4e4f\u4e2d\u592e\u5b58\u50a8\u5e93\uff0c\u65e0\u6cd5\u6574\u5408\u6240\u6709\u76f8\u5173\u7684\u8425\u517b\u4fe1\u606f\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u5168\u9762\u7684\u8d44\u6e90\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u77e5\u8bc6\u56fe\u8c31\u586b\u5145\u6d41\u7a0b\uff0c\u5bf9\u73b0\u6709\u7684\u745e\u58eb\u98df\u54c1\u4fe1\u606f\u8fdb\u884c\u6574\u5408\u548c\u4e30\u5bcc\uff0c\u5305\u62ec\u98df\u6750\u66ff\u6362\u3001\u8425\u517b\u6210\u5206\u3001\u8fc7\u654f\u539f\u548c\u996e\u98df\u9650\u5236\u7b49\u4fe1\u606f\u3002\u7136\u540e\uff0c\u5b9e\u73b0\u4e86\u4e00\u4e2a\u57fa\u4e8e\u56fe\u8c31\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08Graph-RAG\uff09\u5e94\u7528\u7a0b\u5e8f\uff0c\u4ee5\u5c55\u793a\u5982\u4f55\u5229\u7528\u77e5\u8bc6\u56fe\u8c31\u7684\u81ea\u7136\u8bed\u8a00\u6570\u636e\u7ed3\u6784\u6765\u56de\u7b54\u7528\u6237\u5173\u4e8e\u8425\u517b\u7684\u7279\u5b9a\u67e5\u8be2\u3002", "result": "\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u745e\u58eb\u9996\u4e2a\u745e\u58eb\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31\uff08SwissFKG\uff09\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2aLLM\u9a71\u52a8\u7684\u77e5\u8bc6\u589e\u5f3a\u6d41\u7a0b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cLLM\u80fd\u591f\u6709\u6548\u5730\u4e3a\u77e5\u8bc6\u56fe\u8c31\u6dfb\u52a0\u8425\u517b\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u901a\u8fc7Graph-RAG\u5e94\u7528\uff0c\u5c55\u793a\u4e86\u77e5\u8bc6\u56fe\u8c31\u80fd\u591f\u5e2e\u52a9LLM\u51c6\u786e\u56de\u7b54\u7528\u6237\u63d0\u51fa\u7684\u8425\u517b\u76f8\u5173\u95ee\u9898\uff0c\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\u7528\u6237\u67e5\u8be2\u7684\u56de\u7b54\u4e0e\u9884\u671f\u7b54\u6848\u9ad8\u5ea6\u4e00\u81f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u745e\u58eb\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31\uff08SwissFKG\uff09\uff0c\u4e00\u4e2a\u6574\u5408\u4e86\u98df\u8c31\u3001\u98df\u6750\u3001\u8425\u517b\u6570\u636e\u3001\u996e\u98df\u9650\u5236\u3001\u8fc7\u654f\u539f\u4fe1\u606f\u548c\u56fd\u5bb6\u8425\u517b\u6307\u5357\u7684\u77e5\u8bc6\u5e93\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u7531\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9a71\u52a8\u7684\u586b\u5145\u6d41\u7a0b\u548cGraph-RAG\u5e94\u7528\uff0c\u4ee5\u652f\u6301\u7528\u6237\u7279\u5b9a\u8425\u517b\u67e5\u8be2\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cLLM\u80fd\u591f\u6709\u6548\u5730\u4e30\u5bcc\u8425\u517b\u4fe1\u606f\uff0c\u5e76\u4e14SwissFKG\u53ef\u4ee5\u63d0\u4f9b\u98df\u6750\u7ea7\u522b\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u7ed3\u5408\u89c6\u89c9\u3001\u80cc\u666f\u548c\u6587\u5316\u7ef4\u5ea6\u7684\u996e\u98df\u8bc4\u4f30\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.10204", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10204", "abs": "https://arxiv.org/abs/2507.10204", "authors": ["Abdelhakim Amer", "Mohit Mehindratta", "Yury Brodskiy", "Bilal Wehbe", "Erdal Kayacan"], "title": "REACT: Real-time Entanglement-Aware Coverage Path Planning for Tethered Underwater Vehicles", "comment": null, "summary": "Inspection of complex underwater structures with tethered underwater vehicles\nis often hindered by the risk of tether entanglement. We propose REACT\n(real-time entanglement-aware coverage path planning for tethered underwater\nvehicles), a framework designed to overcome this limitation. REACT comprises a\nfast geometry-based tether model using the signed distance field (SDF) map for\naccurate, real-time simulation of taut tether configurations around arbitrary\nstructures in 3D. This model enables an efficient online replanning strategy by\nenforcing a maximum tether length constraint, thereby actively preventing\nentanglement. By integrating REACT into a coverage path planning framework, we\nachieve safe and optimal inspection paths, previously challenging due to tether\nconstraints. The complete REACT framework's efficacy is validated in a pipe\ninspection scenario, demonstrating safe, entanglement-free navigation and\nfull-coverage inspection. Simulation results show that REACT achieves complete\ncoverage while maintaining tether constraints and completing the total mission\n20% faster than conventional planners, despite a longer inspection time due to\nproactive avoidance of entanglement that eliminates extensive post-mission\ndisentanglement. Real-world experiments confirm these benefits, where REACT\ncompletes the full mission, while the baseline planner fails due to physical\ntether entanglement.", "AI": {"tldr": "REACT\u6846\u67b6\u901a\u8fc7\u51e0\u4f55\u7cfb\u7ef3\u6a21\u578b\u548c\u5728\u7ebf\u91cd\u89c4\u5212\uff0c\u89e3\u51b3\u4e86\u7cfb\u7ef3\u6c34\u4e0b\u822a\u884c\u5668\u6613\u7f20\u7ed5\u7684\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5b89\u5168\u9ad8\u6548\u7684\u6c34\u4e0b\u7ed3\u6784\u68c0\u67e5\u3002", "motivation": "\u6c34\u4e0b\u7ed3\u6784\u68c0\u67e5\u4e2d\uff0c\u7cfb\u7ef3\u6c34\u4e0b\u822a\u884c\u5668\u5e38\u5e38\u53d7\u5230\u7cfb\u7ef3\u7f20\u7ed5\u98ce\u9669\u7684\u9650\u5236\uff0c\u5f71\u54cd\u4e86\u68c0\u67e5\u7684\u6548\u7387\u548c\u5b89\u5168\u6027\u3002", "method": "REACT\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u7b26\u53f7\u8ddd\u79bb\u573a\uff08SDF\uff09\u7684\u5feb\u901f\u51e0\u4f55\u7cfb\u7ef3\u6a21\u578b\uff0c\u7528\u4e8e\u7cbe\u786e\u6a21\u62df\u4e09\u7ef4\u7ed3\u6784\u5468\u56f4\u7684\u7cfb\u7ef3\u3002\u8be5\u6a21\u578b\u652f\u6301\u9ad8\u6548\u7684\u5728\u7ebf\u91cd\u89c4\u5212\u7b56\u7565\uff0c\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u6700\u5927\u7cfb\u7ef3\u957f\u5ea6\u9650\u5236\u6765\u4e3b\u52a8\u9632\u6b62\u7f20\u7ed5\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u663e\u793a\uff0cREACT\u5728\u4fdd\u8bc1\u7cfb\u7ef3\u7ea6\u675f\u548c\u5b9e\u73b0\u5b8c\u6574\u8986\u76d6\u7684\u540c\u65f6\uff0c\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u7f29\u77ed\u4e8620%\u3002\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e5f\u8bc1\u5b9e\u4e86REACT\u7684\u6709\u6548\u6027\uff0c\u5728\u57fa\u7ebf\u89c4\u5212\u5668\u56e0\u5b9e\u9645\u7f20\u7ed5\u800c\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\uff0cREACT\u6210\u529f\u5b8c\u6210\u4e86\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684REACT\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5176\u51e0\u4f55\u9a71\u52a8\u7684\u7cfb\u7ef3\u6a21\u578b\u548c\u5728\u7ebf\u91cd\u89c4\u5212\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u7cfb\u7ef3\u6c34\u4e0b\u822a\u884c\u5668\u5728\u590d\u6742\u6c34\u4e0b\u7ed3\u6784\u68c0\u67e5\u4e2d\u7cfb\u7ef3\u7f20\u7ed5\u7684\u95ee\u9898\u3002\u5b9e\u9a8c\u548c\u6a21\u62df\u7ed3\u679c\u5747\u8868\u660e\uff0cREACT\u80fd\u591f\u5b9e\u73b0\u5b89\u5168\u3001\u65e0\u7f20\u7ed5\u7684\u5bfc\u822a\u548c\u5168\u8986\u76d6\u68c0\u67e5\uff0c\u5e76\u4e14\u6bd4\u4f20\u7edf\u89c4\u5212\u5668\u6548\u7387\u66f4\u9ad8\u3002"}}
{"id": "2507.10164", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10164", "abs": "https://arxiv.org/abs/2507.10164", "authors": ["Egor Maslennikov", "Eduard Zaliaev", "Nikita Dudorov", "Oleg Shamanin", "Karanov Dmitry", "Gleb Afanasev", "Alexey Burkov", "Egor Lygin", "Simeon Nedelchev", "Evgeny Ponomarev"], "title": "Robust RL Control for Bipedal Locomotion with Closed Kinematic Chains", "comment": null, "summary": "Developing robust locomotion controllers for bipedal robots with closed\nkinematic chains presents unique challenges, particularly since most\nreinforcement learning (RL) approaches simplify these parallel mechanisms into\nserial models during training. We demonstrate that this simplification\nsignificantly impairs sim-to-real transfer by failing to capture essential\naspects such as joint coupling, friction dynamics, and motor-space control\ncharacteristics. In this work, we present an RL framework that explicitly\nincorporates closed-chain dynamics and validate it on our custom-built robot\nTopA. Our approach enhances policy robustness through symmetry-aware loss\nfunctions, adversarial training, and targeted network regularization.\nExperimental results demonstrate that our integrated approach achieves stable\nlocomotion across diverse terrains, significantly outperforming methods based\non simplified kinematic models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u4eff\u4eba\u673a\u5668\u4eba\u7684\u8fd0\u52a8\u63a7\u5236\u5668\u3002\u4e0e\u7b80\u5316\u6a21\u578b\u4e0d\u540c\uff0c\u8be5\u6846\u67b6\u8003\u8651\u4e86\u95ed\u73af\u8fd0\u52a8\u94fe\u7684\u52a8\u529b\u5b66\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u80fd\u529b\u548c\u5728\u4e0d\u540c\u5730\u5f62\u4e0a\u7684\u7a33\u5b9a\u6027\u3002", "motivation": "\u5927\u591a\u6570\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u5728\u8bad\u7ec3\u65f6\u5c06\u4eff\u4eba\u673a\u5668\u4eba\u4e2d\u7684\u95ed\u73af\u8fd0\u52a8\u94fe\u7b80\u5316\u4e3a\u4e32\u884c\u6a21\u578b\uff0c\u8fd9\u4f1a\u5f71\u54cd\u4eff\u771f\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\uff0c\u56e0\u4e3a\u5b83\u672a\u80fd\u6355\u6349\u5230\u5173\u8282\u8026\u5408\u3001\u6469\u64e6\u52a8\u529b\u5b66\u548c\u7535\u673a\u7a7a\u95f4\u63a7\u5236\u7279\u6027\u7b49\u65b9\u9762\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u663e\u5f0f\u5730\u7ed3\u5408\u4e86\u95ed\u94fe\u52a8\u529b\u5b66\uff0c\u5e76\u901a\u8fc7\u5bf9\u79f0\u611f\u77e5\u635f\u5931\u51fd\u6570\u3001\u5bf9\u6297\u6027\u8bad\u7ec3\u548c\u76ee\u6807\u7f51\u7edc\u6b63\u5219\u5316\u6765\u589e\u5f3a\u7b56\u7565\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u96c6\u6210\u65b9\u6cd5\u5728\u5404\u79cd\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u8fd0\u52a8\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u7b80\u5316\u8fd0\u52a8\u5b66\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u5404\u79cd\u5730\u5f62\u4e0a\u5b9e\u73b0\u4e86\u7a33\u5b9a\u7684\u8fd0\u52a8\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u7b80\u5316\u8fd0\u52a8\u6a21\u578b\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.09269", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09269", "abs": "https://arxiv.org/abs/2507.09269", "authors": ["Shuhan Ye", "Yuanbin Qian", "Chong Wang", "Sunqi Lin", "Jiazhen Xu", "Jiangbo Qian", "Yuqi Li"], "title": "Cross Knowledge Distillation between Artificial and Spiking Neural Networks", "comment": "This paper has been accepted by ICME2025", "summary": "Recently, Spiking Neural Networks (SNNs) have demonstrated rich potential in\ncomputer vision domain due to their high biological plausibility, event-driven\ncharacteristic and energy-saving efficiency. Still, limited annotated\nevent-based datasets and immature SNN architectures result in their performance\ninferior to that of Artificial Neural Networks (ANNs). To enhance the\nperformance of SNNs on their optimal data format, DVS data, we explore using\nRGB data and well-performing ANNs to implement knowledge distillation. In this\ncase, solving cross-modality and cross-architecture challenges is necessary. In\nthis paper, we propose cross knowledge distillation (CKD), which not only\nleverages semantic similarity and sliding replacement to mitigate the\ncross-modality challenge, but also uses an indirect phased knowledge\ndistillation to mitigate the cross-architecture challenge. We validated our\nmethod on main-stream neuromorphic datasets, including N-Caltech101 and\nCEP-DVS. The experimental results show that our method outperforms current\nState-of-the-Art methods. The code will be available at\nhttps://github.com/ShawnYE618/CKD", "AI": {"tldr": "\u901a\u8fc7\u8de8\u6a21\u6001\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\uff0c\u5229\u7528RGB\u6570\u636e\u548cANNs\u6765\u63d0\u5347SNN\u5728\u4e8b\u4ef6\u6570\u636e\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8SNN\u5728\u4e8b\u4ef6\u6570\u636e\uff08DVS\u6570\u636e\uff09\u4e0a\u7684\u6027\u80fd\uff0c\u514b\u670d\u76ee\u524d\u6807\u6ce8\u6570\u636e\u6709\u9650\u548cSNN\u67b6\u6784\u4e0d\u6210\u719f\u5bfc\u81f4\u7684\u6027\u80fd\u74f6\u9888\uff0c\u5e76\u89e3\u51b3\u8de8\u6a21\u6001\u548c\u8de8\u67b6\u6784\u7684\u77e5\u8bc6\u8fc1\u79fb\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4ea4\u53c9\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528RGB\u6570\u636e\u548c\u5148\u8fdb\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANNs\uff09\u6765\u63d0\u5347\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNNs\uff09\u5728\u4e8b\u4ef6\u6570\u636e\u4e0a\u7684\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0cCKD\u901a\u8fc7\u5229\u7528\u8bed\u4e49\u76f8\u4f3c\u6027\u548c\u6ed1\u52a8\u66ff\u6362\u6765\u89e3\u51b3\u8de8\u6a21\u6001\u6311\u6218\uff0c\u5e76\u901a\u8fc7\u95f4\u63a5\u5206\u9636\u6bb5\u77e5\u8bc6\u84b8\u998f\u6765\u89e3\u51b3\u8de8\u67b6\u6784\u6311\u6218\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCKD\u5728N-Caltech101\u548cCEP-DVS\u7b49\u6570\u636e\u96c6\u4e0a\uff0c\u76f8\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4ea4\u53c9\u77e5\u8bc6\u84b8\u998f\uff08CKD\uff09\u65b9\u6cd5\u5728\u4e3b\u6d41\u795e\u7ecf\u5f62\u6001\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2507.09844", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09844", "abs": "https://arxiv.org/abs/2507.09844", "authors": ["Nahid Binandeh Dehaghani", "A. Pedro Aguiar", "Rafal Wisniewski"], "title": "Robust Entanglement Generation in Bipartite Quantum Systems Using Optimal Control", "comment": null, "summary": "Quantum entanglement is a key resource for quantum technologies, yet its\nefficient and high-fidelity generation remains a challenge due to the\ncomplexity of quantum dynamics. This paper presents a quantum optimal control\nframework to maximize bipartite entanglement within a fixed time horizon, under\nbounded control inputs. By leveraging Pontryagin's Minimum Principle, we derive\na set of necessary conditions that guide the design of time-dependent control\nfields to steer a two-qubit system toward maximally entangled Bell states. The\nentanglement is quantified using concurrence, and the control objective is\nformulated as maximizing this measure at the terminal time. Our approach is\nvalidated through numerical simulations of Liouville-von Neumann dynamics. The\nresults demonstrate the effectiveness of switching-based control strategies in\nachieving robust entanglement, offering insights into practical implementations\nof quantum control for entanglement generation in quantum networks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u5229\u7528\u5e9e\u7279\u91cc\u4e9a\u91d1\u6700\u5c0f\u5316\u539f\u7406\uff0c\u901a\u8fc7\u8bbe\u8ba1\u63a7\u5236\u573a\u6700\u5927\u5316\u53cc\u91cf\u5b50\u6bd4\u7279\u7ea0\u7f20\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u91cf\u5b50\u7ea0\u7f20\u662f\u91cf\u5b50\u6280\u672f\u4e2d\u7684\u5173\u952e\u8d44\u6e90\uff0c\u4f46\u7531\u4e8e\u91cf\u5b50\u52a8\u529b\u5b66\u7684\u590d\u6742\u6027\uff0c\u5176\u9ad8\u6548\u3001\u9ad8\u4fdd\u771f\u5ea6\u7684\u751f\u6210\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u5229\u7528\u5e9e\u7279\u91cc\u4e9a\u91d1\u6700\u5c0f\u5316\u539f\u7406\u63a8\u5bfc\u51fa\u5fc5\u8981\u6761\u4ef6\uff0c\u8bbe\u8ba1\u65f6\u95f4\u76f8\u5173\u7684\u63a7\u5236\u573a\uff0c\u4ee5\u6700\u5927\u5316\u53cc\u91cf\u5b50\u7ea0\u7f20\uff0c\u5e76\u4f7f\u7528\u5171\u5ea6\u91cf\u6765\u91cf\u5316\u7ea0\u7f20\uff0c\u901a\u8fc7\u6c42\u89e3\u5218\u7ef4\u5c14-\u51af\u8bfa\u4f9d\u66fc\u52a8\u529b\u5b66\u8fdb\u884c\u6570\u503c\u6a21\u62df\u3002", "result": "\u6570\u503c\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0c\u57fa\u4e8e\u5f00\u5173\u7684\u63a7\u5236\u7b56\u7565\u80fd\u591f\u5b9e\u73b0\u9c81\u68d2\u7684\u91cf\u5b50\u7ea0\u7f20\uff0c\u8bc1\u660e\u4e86\u8be5\u91cf\u5b50\u6700\u4f18\u63a7\u5236\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u91cf\u5b50\u6700\u4f18\u63a7\u5236\u6846\u67b6\uff0c\u5728\u56fa\u5b9a\u65f6\u95f4\u8303\u56f4\u5185\uff0c\u5728\u6709\u754c\u63a7\u5236\u8f93\u5165\u4e0b\u6700\u5927\u5316\u53cc\u91cf\u5b50\u6bd4\u7279\u7ea0\u7f20\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u6570\u503c\u6a21\u62df\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u9c81\u68d2\u7ea0\u7f20\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u91cf\u5b50\u7f51\u7edc\u4e2d\u7ea0\u7f20\u751f\u6210\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u884c\u7684\u63a7\u5236\u7b56\u7565\u3002"}}
{"id": "2507.09758", "categories": ["cs.CL", "cs.LG", "I.2.7; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.09758", "abs": "https://arxiv.org/abs/2507.09758", "authors": ["Qi Feng", "Yihong Liu", "Hinrich Sch\u00fctze"], "title": "Your Pretrained Model Tells the Difficulty Itself: A Self-Adaptive Curriculum Learning Paradigm for Natural Language Understanding", "comment": "18 pages, 23 figures. To appear in ACL 2025 Student Research Workshop\n  (SRW)", "summary": "Curriculum learning is a widely adopted training strategy in natural language\nprocessing (NLP), where models are exposed to examples organized by increasing\ndifficulty to enhance learning efficiency and performance. However, most\nexisting approaches rely on manually defined difficulty metrics -- such as text\nlength -- which may not accurately reflect the model's own perspective. To\novercome this limitation, we present a self-adaptive curriculum learning\nparadigm that prioritizes fine-tuning examples based on difficulty scores\npredicted by pre-trained language models (PLMs) themselves. Building on these\nscores, we explore various training strategies that differ in the ordering of\nexamples for the fine-tuning: from easy-to-hard, hard-to-easy, to mixed\nsampling. We evaluate our method on four natural language understanding (NLU)\ndatasets covering both binary and multi-class classification tasks.\nExperimental results show that our approach leads to faster convergence and\nimproved performance compared to standard random sampling.", "AI": {"tldr": "\u4e00\u79cd\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u9884\u6d4b\u7684\u96be\u5ea6\u5206\u6570\u6765\u81ea\u9002\u5e94\u5730\u7ec4\u7ec7\u8bad\u7ec3\u6837\u672c\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u4e2d\u63d0\u5347\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8bfe\u7a0b\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u624b\u52a8\u5b9a\u4e49\u7684\u96be\u5ea6\u6307\u6807\uff08\u5982\u6587\u672c\u957f\u5ea6\uff09\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u51c6\u786e\u53cd\u6620\u6a21\u578b\u81ea\u8eab\u7684\u5b66\u4e60\u72b6\u6001\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u8bfe\u7a0b\u5b66\u4e60\u8303\u5f0f\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u9884\u6d4b\u7684\u96be\u5ea6\u5206\u6570\u6765\u6307\u5bfc\u5fae\u8c03\u6837\u672c\u7684\u91c7\u6837\u987a\u5e8f\uff0c\u5e76\u63a2\u7d22\u4e86\u4ece\u6613\u5230\u96be\u3001\u4ece\u96be\u5230\u6613\u4ee5\u53ca\u6df7\u5408\u91c7\u6837\u7b49\u591a\u79cd\u8bad\u7ec3\u7b56\u7565\u3002", "result": "\u5728\u56db\u4e2a\u81ea\u7136\u8bed\u8a00\u7406\u89e3\uff08NLU\uff09\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u7684\u968f\u673a\u91c7\u6837\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u81ea\u8eab\u7684\u9884\u6d4b\u5206\u6570\u6765\u786e\u5b9a\u5fae\u8c03\u6837\u672c\u7684\u96be\u5ea6\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6536\u655b\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09029", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09029", "abs": "https://arxiv.org/abs/2507.09029", "authors": ["Vaibhav Singh", "Zafir Khalid", "Edouard Oyallon", "Eugene Belilovsky"], "title": "Model Parallelism With Subnetwork Data Parallelism", "comment": "6 pages, 1 figure", "summary": "Distributed pre-training of large models at scale often imposes heavy memory\ndemands on individual nodes and incurs significant intra-node communication\ncosts. We propose a novel alternative approach that reduces the memory\nrequirements by training small, structured subnetworks of the model on separate\nworkers. Unlike pipelining, our method avoids inter-node activation\ncommunication and maintains bandwidth requirements that are comparable to or\nlower than standard data parallel communication schemes based on all-reduce. We\nevaluate two subnetwork construction strategies guided by the principle of\nensuring uniform representation of each parameter across the distributed\ntraining setup. Our results show that the stochastic block dropping technique\nconsistently outperforms the width-wise subnetwork construction previously\nexplored in federated learning. We empirically attribute this superior\nperformance to stronger gradient alignment in subnetworks that retain blocks\nhaving skip connections. Preliminary experiments highlight the promise of our\napproach, achieving a 20-40% reduction in memory usage without any loss in\nperformance.", "AI": {"tldr": "\u5728\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u671f\u95f4\uff0c\u901a\u8fc7\u8bad\u7ec3\u6a21\u578b\u7684\u5b50\u7f51\u7edc\u6765\u964d\u4f4e\u5185\u5b58\u9700\u6c42\uff0c\u5e76\u63d0\u51fa\u548c\u8bc4\u4f30\u4e86\u4e24\u79cd\u5b50\u7f51\u7edc\u6784\u5efa\u7b56\u7565\uff0c\u5176\u4e2d\u968f\u673a\u5757\u4e22\u5f03\u65b9\u6cd5\u6548\u679c\u66f4\u4f18\u3002", "motivation": "\u5206\u5e03\u5f0f\u9884\u8bad\u7ec3\u5927\u6a21\u578b\u5728\u6269\u5c55\u65f6\uff0c\u5bf9\u5355\u4e2a\u8282\u70b9\u7684\u5185\u5b58\u9700\u6c42\u5f88\u5927\uff0c\u5e76\u4e14\u8282\u70b9\u5185\u7684\u901a\u4fe1\u6210\u672c\u5f88\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5728\u72ec\u7acb\u7684\u5de5\u4f5c\u8282\u70b9\u4e0a\u8bad\u7ec3\u6a21\u578b\u7684\u5b50\u7f51\u7edc\u6765\u51cf\u5c11\u5185\u5b58\u9700\u6c42\u7684\u66ff\u4ee3\u65b9\u6cd5\u3002\u5e76\u8bc4\u4f30\u4e86\u4e24\u79cd\u4ee5\u786e\u4fdd\u5728\u5206\u5e03\u5f0f\u8bad\u7ec3\u8bbe\u7f6e\u4e2d\u5747\u5300\u8868\u793a\u6bcf\u4e2a\u53c2\u6570\u4e3a\u6307\u5bfc\u539f\u5219\u7684\u5b50\u7f51\u7edc\u6784\u5efa\u7b56\u7565\uff0c\u5176\u4e2d\u968f\u673a\u5757\u4e22\u5f03\u6280\u672f\u8868\u73b0\u4f18\u4e8e\u5148\u524d\u5728\u8054\u90a6\u5b66\u4e60\u4e2d\u6240\u63a2\u7d22\u7684\u5bbd\u5ea6\u5b50\u7f51\u7edc\u6784\u5efa\u65b9\u6cd5\uff0c\u5176\u539f\u56e0\u5f52\u56e0\u4e8e\u4fdd\u7559\u4e86\u5177\u6709\u8df3\u8dc3\u8fde\u63a5\u7684\u5757\u7684\u5b50\u7f51\u7edc\u5177\u6709\u66f4\u5f3a\u7684\u68af\u5ea6\u5bf9\u9f50\u80fd\u529b\u3002", "result": "\u968f\u673a\u5757\u4e22\u5f03\u6280\u672f\uff08stochastic block dropping\uff09\u4f18\u4e8e\u5bbd\u5ea6\u5b50\u7f51\u7edc\u6784\u5efa\uff08width-wise subnetwork construction\uff09\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u5c06\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c1120%-40%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u4e0d\u635f\u5931\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u5185\u5b58\u4f7f\u7528\u91cf\u51cf\u5c1120%-40%\uff0c\u4e14\u5176\u65b9\u6cd5\u6240\u5e26\u6765\u7684\u7684\u901a\u4fe1\u5e26\u5bbd\u9700\u6c42\u53ef\u4e0e\u6807\u51c6\u7684\u53c2\u6570 \u0938\u0930\u094d\u0935-\u0930\u093f\u0921\u094d\u092f\u0942\u0938 (all-reduce) \u6570\u636e\u5e76\u884c\u901a\u4fe1\u65b9\u6848\u76f8\u5ab2\u7f8e\uff0c\u751a\u81f3\u66f4\u4f4e\u3002"}}
{"id": "2507.10174", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10174", "abs": "https://arxiv.org/abs/2507.10174", "authors": ["Yumi Omori", "Zixuan Dong", "Keith Ross"], "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?", "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)", "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u8fc7\u6ee4\u884c\u4e3a\u514b\u9686\uff08FBC\uff09\u6bd4\u51b3\u7b56Transformer\uff08DT\uff09\u66f4\u6709\u6548\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002\u9274\u4e8eDT\u5728\u7a20\u5bc6\u5956\u52b1\u73af\u5883\u4e2d\u4e5f\u5e76\u975e\u6700\u4f73\u9009\u62e9\uff0c\u672c\u6587\u8d28\u7591\u4e86DT\u7684\u9002\u7528\u6027\u3002", "motivation": "\u8be5\u8bba\u6587\u65e8\u5728\u63a2\u8ba8\u51b3\u7b56Transformer\uff08DT\uff09\u5728\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8eMLP\u7684\u7b97\u6cd5\uff08\u5982\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u548c\u4fdd\u5b88Q\u5b66\u4e60\uff08CQL\uff09\uff09\u76f8\u6bd4\u3002\u867d\u7136\u5148\u524d\u6709\u7814\u7a76\u58f0\u79f0DT\u5728\u7a00\u758f\u5956\u52b1\u548c\u4f4e\u8d28\u91cf\u6570\u636e\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\uff0c\u4f46\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u8d28\u7591\u4e86\u8fd9\u4e00\u8bf4\u6cd5\uff0c\u5e76\u63d0\u51faDT\u53ef\u80fd\u5e76\u975e\u6700\u4f73\u9009\u62e9\uff0c\u5c24\u5176\u662f\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8fc7\u6ee4\u884c\u4e3a\u514b\u9686\uff08FBC\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6ee4\u9664\u4f4e\u6027\u80fd\u8f68\u8ff9\u5e76\u5bf9\u8fc7\u6ee4\u540e\u7684\u6570\u636e\u96c6\u6267\u884c\u666e\u901a\u884c\u4e3a\u514b\u9686\u6765\u5b9e\u73b0\u3002\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08Robomimic\uff09\u548c\u8fd0\u52a8\u57fa\u51c6\uff08D4RL\uff09\u4e0a\u4e0e\u51b3\u7b56Transformer\uff08DT\uff09\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u57fa\u4e8eMLP\u7684\u8fc7\u6ee4\u884c\u4e3a\u514b\u9686\uff08FBC\uff09\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08Robomimic\uff09\u548c\u8fd0\u52a8\u57fa\u51c6\uff08D4RL\uff09\u4e0a\u53d6\u5f97\u4e86\u4e0e\u51b3\u7b56Transformer\uff08DT\uff09\u76f8\u5f53\u6216\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002FBC\u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u66f4\u5c11\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u8bba\u6587\u901a\u8fc7\u5728\u673a\u5668\u4eba\u64cd\u4f5c\u4efb\u52a1\uff08Robomimic\uff09\u548c\u8fd0\u52a8\u57fa\u51c6\uff08D4RL\uff09\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8868\u660e\u57fa\u4e8eMLP\u7684\u8fc7\u6ee4\u884c\u4e3a\u514b\u9686\uff08FBC\uff09\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u4e0eDT\u76f8\u6bd4\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u6027\u6216\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002FBC\u901a\u8fc7\u6ee4\u9664\u4f4e\u6027\u80fd\u8f68\u8ff9\u7136\u540e\u5bf9\u8fc7\u6ee4\u540e\u7684\u6570\u636e\u96c6\u6267\u884c\u666e\u901a\u884c\u4e3a\u514b\u9686\u6765\u5b9e\u73b0\u3002FBC\u4e0d\u4ec5\u7b80\u5355\uff0c\u800c\u4e14\u6240\u9700\u7684\u8bad\u7ec3\u6570\u636e\u66f4\u5c11\uff0c\u8ba1\u7b97\u6548\u7387\u66f4\u9ad8\u3002\u56e0\u6b64\uff0c\u7ed3\u679c\u8868\u660eDT\u4e0d\u9002\u7528\u4e8e\u7a00\u758f\u5956\u52b1\u73af\u5883\u3002\u8003\u8651\u5230\u5148\u524d\u7684\u5de5\u4f5c\u8868\u660eDT\u4e5f\u4e0d\u9002\u7528\u4e8e\u7a20\u5bc6\u5956\u52b1\u73af\u5883\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u95ee\u9898\uff1aDT\u662f\u5426\u9002\u7528\u4e8e\u4efb\u4f55\u573a\u666f\uff1f"}}
{"id": "2507.10310", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.10310", "abs": "https://arxiv.org/abs/2507.10310", "authors": ["Michael Schr\u00f6der", "Eric Sch\u00f6neberg", "Daniel G\u00f6rges", "Hans D. Schotten"], "title": "Polygonal Obstacle Avoidance Combining Model Predictive Control and Fuzzy Logic", "comment": null, "summary": "In practice, navigation of mobile robots in confined environments is often\ndone using a spatially discrete cost-map to represent obstacles. Path following\nis a typical use case for model predictive control (MPC), but formulating\nconstraints for obstacle avoidance is challenging in this case. Typically the\ncost and constraints of an MPC problem are defined as closed-form functions and\ntypical solvers work best with continuously differentiable functions. This is\ncontrary to spatially discrete occupancy grid maps, in which a grid's value\ndefines the cost associated with occupancy. This paper presents a way to\novercome this compatibility issue by re-formulating occupancy grid maps to\ncontinuously differentiable functions to be embedded into the MPC scheme as\nconstraints. Each obstacle is defined as a polygon -- an intersection of\nhalf-spaces. Any half-space is a linear inequality representing one edge of a\npolygon. Using AND and OR operators, the combined set of all obstacles and\ntherefore the obstacle avoidance constraints can be described. The key\ncontribution of this paper is the use of fuzzy logic to re-formulate such\nconstraints that include logical operators as inequality constraints which are\ncompatible with standard MPC formulation. The resulting MPC-based trajectory\nplanner is successfully tested in simulation. This concept is also applicable\noutside of navigation tasks to implement logical or verbal constraints in MPC.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09279", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09279", "abs": "https://arxiv.org/abs/2507.09279", "authors": ["Anita Kriz", "Elizabeth Laura Janes", "Xing Shen", "Tal Arbel"], "title": "Prompt4Trust: A Reinforcement Learning Prompt Augmentation Framework for Clinically-Aligned Confidence Calibration in Multimodal Large Language Models", "comment": "Preprint version. The peer-reviewed version of this paper has been\n  accepted to ICCV 2025 Workshop CVAMD", "summary": "Multimodal large language models (MLLMs) hold considerable promise for\napplications in healthcare. However, their deployment in safety-critical\nsettings is hindered by two key limitations: (i) sensitivity to prompt design,\nand (ii) a tendency to generate incorrect responses with high confidence. As\nclinicians may rely on a model's stated confidence to gauge the reliability of\nits predictions, it is especially important that when a model expresses high\nconfidence, it is also highly accurate. We introduce Prompt4Trust, the first\nreinforcement learning (RL) framework for prompt augmentation targeting\nconfidence calibration in MLLMs. A lightweight LLM is trained to produce\ncontext-aware auxiliary prompts that guide a downstream task MLLM to generate\nresponses in which the expressed confidence more accurately reflects predictive\naccuracy. Unlike conventional calibration techniques, Prompt4Trust specifically\nprioritizes aspects of calibration most critical for safe and trustworthy\nclinical decision-making. Beyond improvements driven by this clinically\nmotivated calibration objective, our proposed method also improves task\naccuracy, achieving state-of-the-art medical visual question answering (VQA)\nperformance on the PMC-VQA benchmark, which is composed of multiple-choice\nquestions spanning diverse medical imaging modalities. Moreover, our framework\ntrained with a small downstream task MLLM showed promising zero-shot\ngeneralization to larger MLLMs in our experiments, suggesting the potential for\nscalable calibration without the associated computational costs. This work\ndemonstrates the potential of automated yet human-aligned prompt engineering\nfor improving the the trustworthiness of MLLMs in safety critical settings. Our\ncodebase can be found at https://github.com/xingbpshen/vccrl-llm.", "AI": {"tldr": "Prompt4Trust\uff1a\u4e00\u4e2a\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u8f85\u52a9\u63d0\u793a\u6765\u6821\u51c6\u533b\u7597\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5b89\u5168\u6027\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u533b\u7597\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\u90e8\u7f72\u65f6\u9762\u4e34\u7684\u4e24\u5927\u6311\u6218\uff1a\u5bf9\u63d0\u793a\u8bbe\u8ba1\u7684\u654f\u611f\u6027\u4ee5\u53ca\u9ad8\u7f6e\u4fe1\u5ea6\u4e0b\u751f\u6210\u9519\u8bef\u54cd\u5e94\u7684\u503e\u5411\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPrompt4Trust\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u8bad\u7ec3\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u6765\u751f\u6210\u8f85\u52a9\u63d0\u793a\uff0c\u4ee5\u6307\u5bfc\u4e0b\u6e38\u4efb\u52a1MLLM\u4ea7\u751f\u66f4\u51c6\u786e\u5730\u53cd\u6620\u9884\u6d4b\u51c6\u786e\u6027\u7684\u7f6e\u4fe1\u5ea6\u54cd\u5e94\u3002", "result": "\u5728PMC-VQA\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u4efb\u52a1\u51c6\u786e\u6027\u3002\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8f83\u5c0f\u7684\u4e0b\u6e38MLLM\u4e0a\u8bad\u7ec3\u65f6\uff0c\u5bf9\u66f4\u5927\u7684MLLM\u8868\u73b0\u51fa\u826f\u597d\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u53ef\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u6821\u51c6\u800c\u65e0\u9700\u5de8\u5927\u7684\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "Prompt4Trust\u6846\u67b6\u901a\u8fc7\u751f\u6210\u8f85\u52a9\u63d0\u793a\u6765\u6821\u51c6\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u7684\u7f6e\u4fe1\u5ea6\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u533b\u7597\u9886\u57df\u7684\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u3002\u8be5\u6846\u67b6\u5728PMC-VQA\u57fa\u51c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u533b\u5b66\u89c6\u89c9\u95ee\u7b54\u6027\u80fd\uff0c\u5e76\u5c55\u793a\u4e86\u5bf9\u66f4\u5927\u6a21\u578b\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u81ea\u52a8\u5316\u63d0\u793a\u5de5\u7a0b\u5728\u63d0\u5347MLLM\u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09848", "categories": ["quant-ph", "hep-ph", "hep-th"], "pdf": "https://arxiv.org/pdf/2507.09848", "abs": "https://arxiv.org/abs/2507.09848", "authors": ["Yoshiharu Kawamura"], "title": "Generalized Heisenberg Dynamics Revisited", "comment": "32 pages, 3 figures", "summary": "Taking as a model the fact that Heisenberg's matrix mechanics was derived\nfrom Hamiltonian mechanics using the correspondence principle, we explore a\nclass of dynamical systems involving discrete variables, with Nambu mechanics\nas the starting point. Specifically, we reconstruct an extended version of\nmatrix mechanics that describes dynamical systems possessing physical\nquantities expressed through generalized matrices. Furthermore, we reconfirm\nthat a multiple commutator involving generalized matrices can serve as a\ndiscrete (quantized) version of the Nambu bracket or the Jacobian.", "AI": {"tldr": "\u901a\u8fc7\u7c7b\u6bd4\u6d77\u68ee\u5821\u7684\u77e9\u9635\u529b\u5b66\uff0c\u6211\u4eec\u4ece\u5357\u5e03\u529b\u5b66\u51fa\u53d1\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u63cf\u8ff0\u79bb\u6563\u53d8\u91cf\u52a8\u529b\u5b66\u7cfb\u7edf\u7684\u5e7f\u4e49\u77e9\u9635\u529b\u5b66\u7248\u672c\uff0c\u5e76\u8bc1\u5b9e\u4e86\u591a\u91cd\u4ea4\u6362\u5b50\u662f\u5357\u5e03\u62ec\u53f7\u6216\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u79bb\u6563\u5316\u5f62\u5f0f\u3002", "motivation": "\u4ee5\u6d77\u68ee\u5821\u7684\u77e9\u9635\u529b\u5b66\u7531\u54c8\u5bc6\u987f\u529b\u5b66\u4f7f\u7528\u5bf9\u5e94\u539f\u7406\u5bfc\u51fa\u4e3a\u6a21\u578b\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u4e2a\u6d89\u53ca\u79bb\u6563\u53d8\u91cf\u7684\u52a8\u529b\u5b66\u7cfb\u7edf\u7c7b\u522b\uff0c\u4ee5\u5357\u5e03\u529b\u5b66\u4e3a\u8d77\u70b9\u3002", "method": "\u901a\u8fc7\u5c06\u6d77\u68ee\u5821\u7684\u77e9\u9635\u529b\u5b66\u4ece\u54c8\u5bc6\u987f\u529b\u5b66\u548c\u5bf9\u5e94\u539f\u7406\u51fa\u53d1\u4f5c\u4e3a\u6a21\u578b\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u4e2a\u6d89\u53ca\u79bb\u6563\u53d8\u91cf\u7684\u52a8\u529b\u5b66\u7cfb\u7edf\u7c7b\u522b\uff0c\u4ee5\u5357\u5e03\u529b\u5b66\u4e3a\u8d77\u70b9\u3002", "result": "\u6211\u4eec\u91cd\u6784\u4e86\u4e00\u4e2a\u6269\u5c55\u7684\u77e9\u9635\u529b\u5b66\u7248\u672c\uff0c\u5b83\u63cf\u8ff0\u4e86\u5177\u6709\u5e7f\u4e49\u77e9\u9635\u8868\u793a\u7684\u7269\u7406\u91cf\u7684\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5e76\u91cd\u7533\u4e86\u591a\u91cd\u4ea4\u6362\u5b50\u53ef\u4ee5\u4f5c\u4e3a\u5357\u5e03\u62ec\u53f7\u6216\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u79bb\u6563\uff08\u91cf\u5b50\u5316\uff09\u7248\u672c\u3002", "conclusion": "\u4ece\u54c8\u5bc6\u987f\u529b\u5b66\u5230\u77e9\u9635\u529b\u5b66\uff0c\u6211\u4eec\u63a2\u7d22\u4e86\u4e00\u4e2a\u6d89\u53ca\u79bb\u6563\u53d8\u91cf\u7684\u52a8\u529b\u5b66\u7cfb\u7edf\u7c7b\u522b\uff0c\u4ee5\u5357\u5e03\u529b\u5b66\u4e3a\u8d77\u70b9\u3002\u6211\u4eec\u91cd\u5efa\u4e86\u4e00\u4e2a\u6269\u5c55\u7684\u77e9\u9635\u529b\u5b66\u7248\u672c\uff0c\u7528\u4e8e\u63cf\u8ff0\u5177\u6709\u5e7f\u4e49\u77e9\u9635\u8868\u793a\u7684\u7269\u7406\u91cf\u7684\u52a8\u529b\u5b66\u7cfb\u7edf\uff0c\u5e76\u91cd\u7533\u4e86\u591a\u91cd\u4ea4\u6362\u5b50\u53ef\u4ee5\u4f5c\u4e3a\u5357\u5e03\u62ec\u53f7\u6216\u96c5\u53ef\u6bd4\u884c\u5217\u5f0f\u7684\u79bb\u6563\uff08\u91cf\u5b50\u5316\uff09\u7248\u672c\u3002"}}
{"id": "2507.09031", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09031", "abs": "https://arxiv.org/abs/2507.09031", "authors": ["Yash Shah", "Camila Gonzalez", "Mohammad H. Abbasi", "Qingyu Zhao", "Kilian M. Pohl", "Ehsan Adeli"], "title": "Confounder-Free Continual Learning via Recursive Feature Normalization", "comment": null, "summary": "Confounders are extraneous variables that affect both the input and the\ntarget, resulting in spurious correlations and biased predictions. There are\nrecent advances in dealing with or removing confounders in traditional models,\nsuch as metadata normalization (MDN), where the distribution of the learned\nfeatures is adjusted based on the study confounders. However, in the context of\ncontinual learning, where a model learns continuously from new data over time\nwithout forgetting, learning feature representations that are invariant to\nconfounders remains a significant challenge. To remove their influence from\nintermediate feature representations, we introduce the Recursive MDN (R-MDN)\nlayer, which can be integrated into any deep learning architecture, including\nvision transformers, and at any model stage. R-MDN performs statistical\nregression via the recursive least squares algorithm to maintain and\ncontinually update an internal model state with respect to changing\ndistributions of data and confounding variables. Our experiments demonstrate\nthat R-MDN promotes equitable predictions across population groups, both within\nstatic learning and across different stages of continual learning, by reducing\ncatastrophic forgetting caused by confounder effects changing over time.", "AI": {"tldr": "R-MDN is a new layer that can be added to any deep learning model to remove the influence of confounders in continual learning settings. It uses recursive least squares to update its internal state and has been shown to promote equitable predictions and reduce catastrophic forgetting.", "motivation": "In continual learning, learning feature representations that are invariant to confounders remains a significant challenge. Confounders can lead to spurious correlations and biased predictions, and their effects can change over time, causing catastrophic forgetting.", "method": "R-MDN performs statistical regression via the recursive least squares algorithm to maintain and continually update an internal model state with respect to changing distributions of data and confounding variables.", "result": "R-MDN promotes equitable predictions across population groups, both within static learning and across different stages of continual learning, by reducing catastrophic forgetting caused by confounder effects changing over time.", "conclusion": "R-MDN"}}
{"id": "2507.10208", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10208", "abs": "https://arxiv.org/abs/2507.10208", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "comment": null, "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5bf9XAI\u7814\u7a76\u8fdb\u884c\u5206\u7c7b\u548c\u6bd4\u8f83\u7684\u65b0\u6846\u67b6\uff0c\u5e76\u4e3a\u6539\u8fdb\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5357\u3002", "motivation": "\u5f53\u524d\u5728\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7528\u4e8e\u6570\u636e\u5206\u6790\u4efb\u52a1\u7684\u7814\u7a76\u4e2d\u5b58\u5728\u5927\u91cf\u77db\u76fe\u548c\u7f3a\u4e4f\u5177\u4f53\u7684\u8bbe\u8ba1\u5efa\u8bae\uff0c\u8fd9\u662f\u7531\u4e8e\u5bf9\u9700\u8981AI\u8f85\u52a9\u7684\u4efb\u52a1\u7406\u89e3\u5b58\u5728\u5dee\u8ddd\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u5206\u7c7b\u548c\u6bd4\u8f83XAI\u7814\u7a76\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u501f\u9274\u4e86\u89c6\u89c9\u5206\u6790\u3001\u8ba4\u77e5\u548c\u4eea\u8868\u677f\u8bbe\u8ba1\u7b49\u591a\u4e2a\u9886\u57df\u7684\u77e5\u8bc6\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u201c\u5185\u5bb9\u201d\u3001\u201c\u539f\u56e0\u201d\u548c\u201c\u5bf9\u8c61\u201d\u4e09\u4e2a\u7ef4\u5ea6\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9XAI\u7814\u7a76\u8fdb\u884c\u5206\u7c7b\u548c\u6bd4\u8f83\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u5206\u6790\u4e86\u5f53\u524dXAI\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\uff0c\u5305\u62ec\u660e\u786e\u7528\u6237\u80cc\u666f\u4fe1\u606f\u548c\u5236\u5b9a\u7814\u7a76\u6307\u5357\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u51fa\u5f53\u524dXAI\u7814\u7a76\u7684\u4e3b\u8981\u95ee\u9898\u5305\u62ec\uff1a\u4efb\u52a1\u63cf\u8ff0\u4e0d\u8db3\u3001\u7814\u7a76\u7f3a\u4e4f\u7279\u5b9a\u60c5\u5883\u4ee5\u53ca\u7528\u6237\u6d4b\u8bd5\u4e0d\u5145\u5206\u3002\u4e3a\u6b64\uff0c\u7814\u7a76\u63d0\u51fa\u5e94\u660e\u786e\u62a5\u544a\u7528\u6237\u5728\u9886\u57df\u3001AI\u548c\u6570\u636e\u5206\u6790\u65b9\u9762\u7684\u4e13\u4e1a\u77e5\u8bc6\uff0c\u4ee5\u5c55\u793a\u7814\u7a76\u7ed3\u679c\u7684\u666e\u9002\u6027\u3002\u540c\u65f6\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u5957\u8bbe\u8ba1\u548c\u62a5\u544aXAI\u4efb\u52a1\u7684\u6307\u5357\uff0c\u65e8\u5728\u63d0\u5347XAI\u793e\u533a\u7406\u89e3\u8be5\u5feb\u901f\u53d1\u5c55\u9886\u57df\u7684\u6574\u4f53\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5206\u7c7b\u548c\u6bd4\u8f83XAI\u7814\u7a76\u7684\u4e09\u7ef4\u6846\u67b6\uff08\u5185\u5bb9\u3001\u539f\u56e0\u548c\u5bf9\u8c61\uff09\uff0c\u4ee5\u89e3\u51b3\u5f53\u524dXAI\u7814\u7a76\u4e2d\u5b58\u5728\u7684\u77db\u76fe\u548c\u8bbe\u8ba1\u5efa\u8bae\u4e0d\u8db3\u7684\u95ee\u9898\u3002\u7814\u7a76\u6307\u51fa\u4e86\u5f53\u524d\u7814\u7a76\u5b58\u5728\u7684\u4e09\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a\u4efb\u52a1\u63cf\u8ff0\u4e0d\u8db3\u3001\u7f3a\u4e4f\u7279\u5b9a\u60c5\u5883\u7684\u7814\u7a76\u4ee5\u53ca\u7528\u6237\u6d4b\u8bd5\u4e0d\u5145\u5206\u3002\u4e3a\u4e86\u63d0\u9ad8XAI\u7814\u7a76\u7684\u53ef\u6bd4\u6027\u548c\u666e\u9002\u6027\uff0c\u7814\u7a76\u5efa\u8bae\u5e94\u660e\u786e\u62a5\u544a\u7814\u7a76\u7528\u6237\u7684\u9886\u57df\u3001AI\u548c\u6570\u636e\u5206\u6790\u4e13\u4e1a\u77e5\u8bc6\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u8bbe\u8ba1\u548c\u62a5\u544aXAI\u4efb\u52a1\u7684\u6307\u5357\uff0c\u4ee5\u5e2e\u52a9\u7814\u7a76\u4eba\u5458\u548c\u8bbe\u8ba1\u8005\u66f4\u597d\u5730\u7406\u89e3\u76f8\u5173\u7814\u7a76\u3001\u8bc6\u522b\u7814\u7a76\u7a7a\u767d\u5e76\u5904\u7406\u76f8\u4e92\u77db\u76fe\u7684\u7814\u7a76\u7ed3\u679c\u3002"}}
{"id": "2507.09285", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09285", "abs": "https://arxiv.org/abs/2507.09285", "authors": ["Chenhao Ding", "Jiangtao Zhang", "Zongsheng Yue", "Hui Wang", "Qian Zhao", "Deyu Meng"], "title": "Generative Latent Kernel Modeling for Blind Motion Deblurring", "comment": null, "summary": "Deep prior-based approaches have demonstrated remarkable success in blind\nmotion deblurring (BMD) recently. These methods, however, are often limited by\nthe high non-convexity of the underlying optimization process in BMD, which\nleads to extreme sensitivity to the initial blur kernel. To address this issue,\nwe propose a novel framework for BMD that leverages a deep generative model to\nencode the kernel prior and induce a better initialization for the blur kernel.\nSpecifically, we pre-train a kernel generator based on a generative adversarial\nnetwork (GAN) to aptly characterize the kernel's prior distribution, as well as\na kernel initializer to provide a well-informed and high-quality starting point\nfor kernel estimation. By combining these two components, we constrain the BMD\nsolution within a compact latent kernel manifold, thus alleviating the\naforementioned sensitivity for kernel initialization. Notably, the kernel\ngenerator and initializer are designed to be easily integrated with existing\nBMD methods in a plug-and-play manner, enhancing their overall performance.\nFurthermore, we extend our approach to tackle blind non-uniform motion\ndeblurring without the need for additional priors, achieving state-of-the-art\nperformance on challenging benchmark datasets. The source code is available at\nhttps://github.com/dch0319/GLKM-Deblur.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528GAN\u751f\u6210\u7684\u6838\u5148\u9a8c\u548c\u521d\u59cb\u5316\u5668\u6765\u6539\u5584\u6a21\u7cca\u6838\u4f30\u8ba1\u7684\u521d\u59cb\u503c\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u6cd5\u5bf9\u521d\u59cb\u6a21\u7cca\u6838\u654f\u611f\u7684\u95ee\u9898\uff0c\u5e76\u53ef\u5373\u63d2\u5373\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6df1\u5ea6\u5148\u9a8c\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u6cd5\uff08BMD\uff09\u901a\u5e38\u53d7\u9650\u4e8eBMD\u5e95\u5c42\u4f18\u5316\u8fc7\u7a0b\u7684\u9ad8\u5ea6\u975e\u51f8\u6027\uff0c\u5bfc\u81f4\u5176\u5bf9\u521d\u59cb\u6a21\u7cca\u6838\u6781\u5176\u654f\u611f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u6765\u7f16\u7801\u6a21\u7cca\u6838\u5148\u9a8c\u5e76\u4e3a\u6a21\u7cca\u6838\u63d0\u4f9b\u66f4\u597d\u7684\u521d\u59cb\u5316\u3002\u5177\u4f53\u800c\u8a00\uff0c\u9884\u8bad\u7ec3\u4e86\u4e00\u4e2a\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u7684\u6838\u751f\u6210\u5668\u6765\u51c6\u786e\u8868\u5f81\u6838\u7684\u5148\u9a8c\u5206\u5e03\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6838\u521d\u59cb\u5316\u5668\u6765\u63d0\u4f9b\u4e00\u4e2a\u4fe1\u606f\u4e30\u5bcc\u4e14\u9ad8\u8d28\u91cf\u7684\u5185\u6838\u4f30\u8ba1\u8d77\u70b9\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e24\u4e2a\u7ec4\u4ef6\uff0c\u5c06BMD\u89e3\u51b3\u65b9\u6848\u7ea6\u675f\u5728\u7d27\u51d1\u7684\u6f5c\u5728\u6838\u6d41\u5f62\u5185\uff0c\u4ece\u800c\u51cf\u8f7b\u4e86\u5bf9\u5185\u6838\u521d\u59cb\u5316\u7684\u654f\u611f\u6027\u3002\u8be5\u6838\u751f\u6210\u5668\u548c\u521d\u59cb\u5316\u5668\u53ef\u4ee5\u4ee5\u5373\u63d2\u5373\u7528\u7684\u65b9\u5f0f\u8f7b\u677e\u96c6\u6210\u5230\u73b0\u6709\u7684BMD\u65b9\u6cd5\u4e2d\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u65e0\u9700\u989d\u5916\u5148\u9a8c\u5373\u53ef\u5904\u7406\u76f2\u975e\u5747\u5300\u8fd0\u52a8\u53bb\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ea6\u675fBMD\u89e3\u51b3\u65b9\u6848\u4e8e\u7d27\u51d1\u7684\u6f5c\u5728\u6838\u6d41\u5f62\uff0c\u6709\u6548\u51cf\u8f7b\u4e86\u5bf9\u5185\u6838\u521d\u59cb\u5316\u7684\u654f\u611f\u6027\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86\u73b0\u6709BMD\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u5728\u76f2\u975e\u5747\u5300\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u9762\u4e5f\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u6df1\u5ea6\u751f\u6210\u6a21\u578b\u7f16\u7801\u6a21\u7cca\u6838\u5148\u9a8c\u5e76\u4e3a\u6a21\u7cca\u6838\u8bf1\u5bfc\u66f4\u597d\u7684\u521d\u59cb\u5316\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5148\u9a8c\u7684\u76f2\u8fd0\u52a8\u53bb\u6a21\u7cca\u65b9\u6cd5\u5bf9\u521d\u59cb\u6a21\u7cca\u6838\u654f\u611f\u7684\u95ee\u9898\u3002"}}
{"id": "2507.09851", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09851", "abs": "https://arxiv.org/abs/2507.09851", "authors": ["Koki Nagamachi", "Hiroki Yamashita", "Mikio Fujiwara", "Shigehito Miki", "Hirotaka Terai", "Takafumi Ono"], "title": "Evaluating a Multi-Color Entangled-Photon Source for a Bosonic Silicon Quantum Circuit", "comment": "7 pages, 4 figures", "summary": "We evaluated a multi-color two-photon entangled state generated in silicon\nvia spontaneous four-wave mixing (SFWM) as a potential source for bosonic\nintegrated circuits. Spatially entangled photon states were created using a\npair of silicon waveguides that produced signal and idler photons through SFWM,\nallowing us to observe quantum interference between them. Assuming that the\nfrequencies of the multi-color photons were nearly identical, we characterized\nthe generated quantum state by performing quantum state tomography on the\nbosonic system using a linear optical circuit. This study demonstrates the\nfeasibility of using photon-pair sources generated in silicon via SFWM in\nbosonic optical circuits and highlights their potential for a wide range of\napplications in silicon-based optical quantum technologies.", "AI": {"tldr": "\u7814\u7a76\u4e86\u7845\u57faSFWM\u5149\u6e90\u5728\u73bb\u8272\u5b50\u96c6\u6210\u7535\u8def\u4e2d\u7684\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u8bc4\u4f30\u5728\u7845\u4e2d\u901a\u8fc7\u81ea\u53d1\u56db\u6ce2\u6df7\u9891\uff08SFWM\uff09\u4ea7\u751f\u7684\u591a\u8272\u53cc\u5149\u5b50\u7ea0\u7f20\u6001\u4f5c\u4e3a\u73bb\u8272\u5b50\u96c6\u6210\u7535\u8def\u7684\u6f5c\u5728\u5149\u6e90\u3002", "method": "\u5229\u7528\u4e00\u5bf9\u7845\u6ce2\u5bfc\uff0c\u901a\u8fc7\u81ea\u53d1\u56db\u6ce2\u6df7\u9891\uff08SFWM\uff09\u4ea7\u751f\u4fe1\u53f7\u5149\u5b50\u548c\u95f2\u7f6e\u5149\u5b50\uff0c\u5e76\u8fdb\u884c\u4e86\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u3002", "result": "\u6210\u529f\u5730\u5c55\u793a\u4e86\u91cf\u5b50\u5e72\u6d89\u73b0\u8c61\uff0c\u5e76\u5bf9\u73bb\u8272\u5b50\u7cfb\u7edf\u8fdb\u884c\u4e86\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8bc1\u660e\u4e86\u5728\u7845\u4e2d\u901a\u8fc7\u81ea\u53d1\u56db\u6ce2\u6df7\u9891\uff08SFWM\uff09\u4ea7\u751f\u7684\u094d\u0924\u093e\u0930-\u5bf9\u6e90\u5728\u73bb\u8272\u5b50\u5149\u5b66\u7535\u8def\u4e2d\u662f\u53ef\u884c\u7684\uff0c\u5e76\u5f3a\u8c03\u4e86\u5b83\u4eec\u5728\u7845\u57fa\u5149\u5b66\u91cf\u5b50\u6280\u672f\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09875", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09875", "abs": "https://arxiv.org/abs/2507.09875", "authors": ["Qinyuan Ye", "Robin Jia", "Xiang Ren"], "title": "Function Induction and Task Generalization: An Interpretability Study with Off-by-One Addition", "comment": "Code: https://github.com/INK-USC/function-induction", "summary": "Large language models demonstrate the intriguing ability to perform unseen\ntasks via in-context learning. However, it remains unclear what mechanisms\ninside the model drive such task-level generalization. In this work, we\napproach this question through the lens of off-by-one addition (i.e., 1+1=3,\n2+2=5, 3+3=?), a two-step, counterfactual task with an unexpected +1 function\nas a second step. Leveraging circuit-style interpretability techniques such as\npath patching, we analyze the models' internal computations behind their\nnotable performance and present three key findings. First, we uncover a\nfunction induction mechanism that explains the model's generalization from\nstandard addition to off-by-one addition. This mechanism resembles the\nstructure of the induction head mechanism found in prior work and elevates it\nto a higher level of abstraction. Second, we show that the induction of the +1\nfunction is governed by multiple attention heads in parallel, each of which\nemits a distinct piece of the +1 function. Finally, we find that this function\ninduction mechanism is reused in a broader range of tasks, including synthetic\ntasks such as shifted multiple-choice QA and algorithmic tasks such as base-8\naddition. Overall, our findings offer deeper insights into how reusable and\ncomposable structures within language models enable task-level generalization.", "AI": {"tldr": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u201c\u504f\u79bb\u4e00\u7684\u52a0\u6cd5\u201d\u4efb\u52a1\uff0c\u901a\u8fc7\u7c7b\u4f3c\u7535\u8def\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u53d1\u73b0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7528\u4e8e\u4efb\u52a1\u6cdb\u5316\u7684\u201c\u51fd\u6570\u5f52\u7eb3\u201d\u673a\u5236\uff0c\u8be5\u673a\u5236\u7531\u591a\u4e2a\u6ce8\u610f\u529b\u5934\u63a7\u5236\uff0c\u5e76\u53ef\u5728\u591a\u79cd\u4efb\u52a1\u4e2d\u91cd\u7528\u3002", "motivation": "\u63a2\u7a76\u9a71\u52a8\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6267\u884c\u672a\u89c1\u4efb\u52a1\uff08\u901a\u8fc7\u4e0a\u4e0b\u6587\u5b66\u4e60\uff09\u7684\u5185\u90e8\u673a\u5236\u3002", "method": "\u4f7f\u7528\u7c7b\u4f3c\u7535\u8def\u7684\u53ef\u89e3\u91ca\u6027\u6280\u672f\uff0c\u5982\u8def\u5f84\u4fee\u590d\uff0c\u5206\u6790\u4e86\u6a21\u578b\u5185\u90e8\u7684\u8ba1\u7b97\u8fc7\u7a0b\u3002", "result": "1. \u53d1\u73b0\u4e86\u80fd\u591f\u89e3\u91ca\u6a21\u578b\u4ece\u6807\u51c6\u52a0\u6cd5\u6cdb\u5316\u5230\u201c\u504f\u79bb\u4e00\u7684\u52a0\u6cd5\u201d\u7684\u51fd\u6570\u5f52\u7eb3\u673a\u5236\uff0c\u8be5\u673a\u5236\u662f\u5bf9\u73b0\u6709\u5f52\u7eb3\u5934\u673a\u5236\u7684\u62bd\u8c61\u30022. \u201c+1\u201d\u51fd\u6570\u7684\u5f52\u7eb3\u7531\u591a\u4e2a\u5e76\u884c\u6ce8\u610f\u529b\u5934\u63a7\u5236\uff0c\u6bcf\u4e2a\u5934\u8d21\u732e\u51fd\u6570\u7684\u4e00\u90e8\u5206\u30023. \u8be5\u51fd\u6570\u5f52\u7eb3\u673a\u5236\u5728\u66f4\u5e7f\u6cdb\u7684\u4efb\u52a1\u4e2d\u5f97\u5230\u91cd\u7528\uff0c\u5305\u62ec\u4eba\u5de5\u4efb\u52a1\u548c\u7b97\u6cd5\u4efb\u52a1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u53ef\u91cd\u7528\u548c\u53ef\u7ec4\u5408\u7684\u7ed3\u6784\u5982\u4f55\u5b9e\u73b0\u4efb\u52a1\u7ea7\u6cdb\u5316\uff0c\u7279\u522b\u662f\u901a\u8fc7\u201c\u504f\u79bb\u4e00\u7684\u52a0\u6cd5\u201d\u4efb\u52a1\u3002"}}
{"id": "2507.10281", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10281", "abs": "https://arxiv.org/abs/2507.10281", "authors": ["Jiaming Tian", "Liyao Li", "Wentao Ye", "Haobo Wang", "Lingxin Wang", "Lihua Yu", "Zujie Ren", "Gang Chen", "Junbo Zhao"], "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence", "comment": null, "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.", "AI": {"tldr": "LLM\u8868\u683c\u4ee3\u7406\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u8868\u683c\u6570\u636e\u65f6\u9762\u4e34\u6311\u6218\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u7ed3\u6784\u7406\u89e3\u3001\u8bed\u4e49\u7406\u89e3\u3001\u68c0\u7d22\u3001\u63a8\u7406\u548c\u8de8\u57df\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u5dee\u8ddd\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u5230SQL\u4efb\u52a1\u4e2d\u3002\u9700\u8981\u6539\u8fdb\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u7684\u8868\u683c\u4efb\u52a1\u5e38\u5e38\u6d89\u53ca\u566a\u97f3\u3001\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u8bed\u4e49\u590d\u6742\u6027\uff0c\u800c\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5e72\u51c0\u7684\u5b66\u672f\u6570\u636e\u96c6\u4e0a\uff0c\u672a\u80fd\u5145\u5206\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5bf9\u65e8\u5728\u81ea\u52a8\u5316\u8868\u683c\u5de5\u4f5c\u6d41\u7684LLM\u9a71\u52a8\u7684\u8868\u683c\u4ee3\u7406\u8fdb\u884c\u8c03\u7814\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u73b0\u5b9e\u4e16\u754c\u7684\u6311\u6218\u3002", "method": "\u672c\u8c03\u67e5\u901a\u8fc7\u5b9a\u4e49\u4e94\u9879\u6838\u5fc3\u80fd\u529b\uff08C1\uff1a\u8868\u683c\u7ed3\u6784\u7406\u89e3\uff0cC2\uff1a\u8868\u683c\u4e0e\u67e5\u8be2\u8bed\u4e49\u7406\u89e3\uff0cC3\uff1a\u8868\u683c\u68c0\u7d22\u4e0e\u538b\u7f29\uff0cC4\uff1a\u53ef\u6267\u884c\u3001\u53ef\u8ffd\u6eaf\u63a8\u7406\uff0cC5\uff1a\u8de8\u57df\u6cdb\u5316\uff09\u6765\u5206\u6790\u548c\u6bd4\u8f83\u5f53\u524d\u57fa\u4e8eLLM\u7684\u8868\u683c\u4ee3\u7406\u65b9\u6cd5\u3002\u5bf9\u6587\u672c\u5230SQL\u4ee3\u7406\u7684\u8be6\u7ec6\u5ba1\u67e5\u63ed\u793a\u4e86\u5176\u5728\u5b66\u672f\u57fa\u51c6\u548c\u5b9e\u9645\u573a\u666f\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\u3002", "result": "\u5bf9\u5f53\u524d\u65b9\u6cd5\u7684\u5206\u6790\u8868\u660e\uff0c\u5728\u5b66\u672f\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cLLM\u8868\u683c\u4ee3\u7406\u7684\u8868\u73b0\u4f18\u4e8e\u5b9e\u9645\u573a\u666f\uff0c\u5c24\u5176\u662f\u5728\u5f00\u6e90\u6a21\u578b\u65b9\u9762\u3002\u672c\u8c03\u67e5\u63d0\u4f9b\u4e86\u6539\u8fdbLLM\u8868\u683c\u4ee3\u7406\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u7684\u5b9e\u7528\u5efa\u8bae\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u8868\u683c\u4ee3\u7406\u5728\u81ea\u52a8\u5316\u8868\u683c\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u4ecd\u9762\u4e34\u566a\u97f3\u3001\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u8bed\u4e49\u590d\u6742\u6027\u7b49\u6311\u6218\u3002\u672c\u8c03\u67e5\u63d0\u51fa\u4e86\u4e94\u4e2a\u6838\u5fc3\u80fd\u529b\uff08C1-C5\uff09\u6765\u5206\u6790\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5f3a\u8c03\u4e86\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u6587\u672c\u5230SQL\u4ee3\u7406\u65b9\u9762\uff0c\u5b66\u672f\u57fa\u51c6\u4e0e\u771f\u5b9e\u4e16\u754c\u8868\u73b0\u4e4b\u95f4\u5b58\u5728\u7684\u5dee\u8ddd\u3002\u4e3a\u4e86\u63d0\u9ad8LLM\u8868\u683c\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\uff0c\u9700\u8981\u91c7\u53d6\u5177\u4f53\u7684\u6539\u8fdb\u63aa\u65bd\u3002"}}
{"id": "2507.10290", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10290", "abs": "https://arxiv.org/abs/2507.10290", "authors": ["Jiajun Yu", "Nanhe Chen", "Guodong Liu", "Chao Xu", "Fei Gao", "Yanjun Cao"], "title": "TOP: Trajectory Optimization via Parallel Optimization towards Constant Time Complexity", "comment": "8 pages, submitted to RA-L", "summary": "Optimization has been widely used to generate smooth trajectories for motion\nplanning. However, existing trajectory optimization methods show weakness when\ndealing with large-scale long trajectories. Recent advances in parallel\ncomputing have accelerated optimization in some fields, but how to efficiently\nsolve trajectory optimization via parallelism remains an open question. In this\npaper, we propose a novel trajectory optimization framework based on the\nConsensus Alternating Direction Method of Multipliers (CADMM) algorithm, which\ndecomposes the trajectory into multiple segments and solves the subproblems in\nparallel. The proposed framework reduces the time complexity to O(1) per\niteration to the number of segments, compared to O(N) of the state-of-the-art\n(SOTA) approaches. Furthermore, we introduce a closed-form solution that\nintegrates convex linear and quadratic constraints to speed up the\noptimization, and we also present numerical solutions for general inequality\nconstraints. A series of simulations and experiments demonstrate that our\napproach outperforms the SOTA approach in terms of efficiency and smoothness.\nEspecially for a large-scale trajectory, with one hundred segments, achieving\nover a tenfold speedup. To fully explore the potential of our algorithm on\nmodern parallel computing architectures, we deploy our framework on a GPU and\nshow high performance with thousands of segments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCADMM\u7684\u5e76\u884c\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u8f68\u8ff9\u89c4\u5212\u7684\u6548\u7387\u548c\u5149\u6ed1\u5ea6\u3002", "motivation": "\u73b0\u6709\u8f68\u8ff9\u4f18\u5316\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u957f\u8f68\u8ff9\u65f6\u6548\u7387\u4f4e\u4e0b\uff0c\u5982\u4f55\u901a\u8fc7\u5e76\u884c\u5316\u9ad8\u6548\u89e3\u51b3\u8f68\u8ff9\u4f18\u5316\u95ee\u9898\u4ecd\u662f\u672a\u89e3\u51b3\u7684\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5171\u8bc6\u4ea4\u66ff\u65b9\u5411\u4e58\u5b50\u6cd5\uff08CADMM\uff09\u7684\u65b0\u578b\u8f68\u8ff9\u4f18\u5316\u6846\u67b6\uff0c\u5c06\u8f68\u8ff9\u5206\u89e3\u4e3a\u591a\u4e2a\u6bb5\u5e76\u5e76\u884c\u6c42\u89e3\u5b50\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u95ed\u5f0f\u89e3\u6765\u5904\u7406\u7ebf\u6027/\u4e8c\u6b21\u7ea6\u675f\uff0c\u4ee5\u53ca\u6570\u503c\u89e3\u6cd5\u5904\u7406\u4e00\u822c\u4e0d\u7b49\u5f0f\u7ea6\u675f\u3002", "result": "\u4e0e\u73b0\u6709SOTA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u6846\u67b6\u5c06\u8fed\u4ee3\u65f6\u95f4\u590d\u6742\u5ea6\u4eceO(N)\u964d\u4f4e\u5230\u6bcf\u4e2a\u8fed\u4ee3O(1)\u4e0e\u6bb5\u6570\u6210\u6bd4\u4f8b\u3002\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u5149\u6ed1\u5ea6\u65b9\u9762\u5747\u4f18\u4e8eSOTA\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u8f68\u8ff9\u4e0a\u5b9e\u73b0\u4e86\u5341\u500d\u4ee5\u4e0a\u7684\u52a0\u901f\uff0c\u5e76\u5728GPU\u4e0a\u5b9e\u73b0\u4e86\u5bf9\u6570\u5343\u4e2a\u6bb5\u7684\u9ad8\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u548c\u5149\u6ed1\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5c24\u5176\u5728\u5927\u89c4\u6a21\u8f68\u8ff9\u4e0a\u5b9e\u73b0\u4e86\u6570\u91cf\u7ea7\u4ee5\u4e0a\u7684\u52a0\u901f\uff0c\u5e76\u5728GPU\u4e0a\u5c55\u793a\u4e86\u9ad8\u6027\u80fd\u3002"}}
{"id": "2507.09291", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09291", "abs": "https://arxiv.org/abs/2507.09291", "authors": ["Yuval Grader", "Hadar Averbuch-Elor"], "title": "Supercharging Floorplan Localization with Semantic Rays", "comment": "Accepted at ICCV 2025", "summary": "Floorplans provide a compact representation of the building's structure,\nrevealing not only layout information but also detailed semantics such as the\nlocations of windows and doors. However, contemporary floorplan localization\ntechniques mostly focus on matching depth-based structural cues, ignoring the\nrich semantics communicated within floorplans. In this work, we introduce a\nsemantic-aware localization framework that jointly estimates depth and semantic\nrays, consolidating over both for predicting a structural-semantic probability\nvolume. Our probability volume is constructed in a coarse-to-fine manner: We\nfirst sample a small set of rays to obtain an initial low-resolution\nprobability volume. We then refine these probabilities by performing a denser\nsampling only in high-probability regions and process the refined values for\npredicting a 2D location and orientation angle. We conduct an evaluation on two\nstandard floorplan localization benchmarks. Our experiments demonstrate that\nour approach substantially outperforms state-of-the-art methods, achieving\nsignificant improvements in recall metrics compared to prior works. Moreover,\nwe show that our framework can easily incorporate additional metadata such as\nroom labels, enabling additional gains in both accuracy and efficiency.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u611f\u77e5\u5b9a\u4f4d\u6846\u67b6\uff0c\u5229\u7528\u6df1\u5ea6\u548c\u8bed\u4e49\u4fe1\u606f\u6765\u63d0\u9ad8\u697c\u5c42\u5e73\u9762\u5b9a\u4f4d\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u7136\u800c\uff0c\u5f53\u524d\u7684\u697c\u5c42\u5e73\u9762\u5b9a\u4f4d\u6280\u672f\u4e3b\u8981\u96c6\u4e2d\u5728\u5339\u914d\u57fa\u4e8e\u6df1\u5ea6\u7684\u7ed3\u6784\u7ebf\u7d22\u4e0a\uff0c\u800c\u5ffd\u7565\u4e86\u697c\u5c42\u5e73\u9762\u4e2d\u4f20\u8fbe\u7684\u4e30\u5bcc\u8bed\u4e49\u3002", "method": "\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u8bed\u4e49\u611f\u77e5\u5b9a\u4f4d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5171\u540c\u4f30\u8ba1\u6df1\u5ea6\u548c\u8bed\u4e49\u5c04\u7ebf\uff0c\u5e76\u5728\u4e24\u8005\u4e0a\u8fdb\u884c\u6574\u5408\uff0c\u4ee5\u9884\u6d4b\u7ed3\u6784-\u8bed\u4e49\u6982\u7387\u4f53\u79ef\u3002\u6211\u4eec\u7684\u6982\u7387\u4f53\u79ef\u662f\u4ee5\u7c97\u5230\u7cbe\u7684\u65b9\u5f0f\u6784\u5efa\u7684\uff1a\u6211\u4eec\u9996\u5148\u5bf9\u4e00\u7ec4\u5c11\u91cf\u5c04\u7ebf\u8fdb\u884c\u91c7\u6837\u4ee5\u83b7\u5f97\u521d\u59cb\u7684\u4f4e\u5206\u8fa8\u7387\u6982\u7387\u4f53\u79ef\u3002\u7136\u540e\uff0c\u6211\u4eec\u4ec5\u5728\u9ad8\u6982\u7387\u533a\u57df\u4e2d\u8fdb\u884c\u66f4\u5bc6\u96c6\u7684\u91c7\u6837\u6765\u4f18\u5316\u8fd9\u4e9b\u6982\u7387\uff0c\u5e76\u5904\u7406\u4f18\u5316\u540e\u7684\u503c\u4ee5\u9884\u6d4b\u4e8c\u7ef4\u4f4d\u7f6e\u548c\u65b9\u5411\u89d2\u3002", "result": "\u6211\u4eec\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u4e0e\u4e4b\u524d\u7684\u5de5\u4f5c\u76f8\u6bd4\uff0c\u5728\u53ec\u56de\u6307\u6807\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u8f7b\u677e\u5730\u6574\u5408\u8bf8\u5982\u623f\u95f4\u6807\u7b7e\u4e4b\u7c7b\u7684\u9644\u52a0\u5143\u6570\u636e\uff0c\u4ece\u800c\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u90fd\u5e26\u6765\u989d\u5916\u7684\u6536\u76ca\u3002"}}
{"id": "2507.09867", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09867", "abs": "https://arxiv.org/abs/2507.09867", "authors": ["Jun-Hao Wei", "Xin-Yu Xu", "Shu-Ming Hu", "Nuo-Ya Yang", "Li Li", "Nai-Le Liu", "Kai Chen"], "title": "Error-mitigated inference of quantum network topology", "comment": "20 pages, 13 figures", "summary": "Paramount for performances of quantum network applications are the structure\nand quality of distributed entanglement. Here we propose a scalable and\nefficient approach to reveal the topological information of unknown quantum\nnetworks, and quantify entanglement simultaneously. The scheme exploits\nentropic uncertainty, an operationally meaningful measure of correlation, by\nperforming only two local measurements on each qubit. Moreover, when\nmeasurement outcomes in each node are collectively evaluated, integrating\nuncertainty and mutual information enables a direct count of the number of\nbipartite sources between any two nodes. This surpasses what is possible via\napplying either approach solely. Moreover, quantum error mitigation techniques\nincluding probabilistic error cancellation (PEC) and virtual distillation (VD),\nwhich have been widely applied to suppress biases in single expectation value,\nare further incorporated to mitigate errors in entropic quantities. We find\nthat PEC successfully removes deviations in correlation estimations. Meanwhile,\nVD extends the depolarizing noise strength that allows for valid bipartite\nentanglement certification from 8.8% to 26.4%, thus substantially enhancing\nrobustness against bias-inducing noise in practical situations. The proposal is\napplicable to a broad variety of platforms and helps to spur future studies\ntoward harnessing the advantages of quantum networks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c40\u90e8\u6d4b\u91cf\u548c\u71b5\u4e0d\u786e\u5b9a\u6027\u6765\u63ed\u793a\u91cf\u5b50\u7f51\u7edc\u7684\u62d3\u6251\u4fe1\u606f\u5e76\u91cf\u5316\u7ea0\u7f20\uff0c\u5e76\u5229\u7528PEC\u548cVD\u6280\u672f\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u3002", "motivation": "\u91cf\u5b50\u7f51\u7edc\u5e94\u7528\u7684\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u5206\u5e03\u5f0f\u7ea0\u7f20\u7684\u7ed3\u6784\u548c\u8d28\u91cf\uff0c\u9700\u8981\u4e00\u79cd\u63ed\u793a\u672a\u77e5\u91cf\u5b50\u7f51\u7edc\u62d3\u6251\u4fe1\u606f\u5e76\u540c\u65f6\u91cf\u5316\u7ea0\u7f20\u7684\u53ef\u6269\u5c55\u9ad8\u6548\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528\u71b5\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u4e24\u6b21\u5c40\u90e8\u6d4b\u91cf\uff0c\u5e76\u901a\u8fc7\u6574\u5408\u4e0d\u786e\u5b9a\u6027\u548c\u4e92\u4fe1\u606f\u6765\u8ba1\u6570\u4e24\u4e2a\u8282\u70b9\u4e4b\u95f4\u7684\u4e8c\u5206\u6e90\u6570\u91cf\u3002\u7ed3\u5408\u4e86\u6982\u7387\u6027\u9519\u8bef\u6d88\u9664\uff08PEC\uff09\u548c\u865a\u62df\u84b8\u998f\uff08VD\uff09\u6280\u672f\u6765\u6291\u5236\u71b5\u6570\u91cf\u4e2d\u7684\u504f\u5dee\u3002", "result": "PEC\u6210\u529f\u6d88\u9664\u4e86\u76f8\u5173\u6027\u4f30\u8ba1\u4e2d\u7684\u504f\u5dee\u3002VD\u5c06\u5141\u8bb8\u6709\u6548\u4e8c\u5206\u7ea0\u7f20\u8ba4\u8bc1\u7684\u53bb\u6781\u5316\u566a\u58f0\u5f3a\u5ea6\u4ece8.8%\u63d0\u9ad8\u523026.4%\uff0c\u4ece\u800c\u5927\u5927\u63d0\u9ad8\u4e86\u5728\u5b9e\u9645\u60c5\u51b5\u4e2d\u62b5\u6297\u504f\u5dee\u8bf1\u5bfc\u566a\u58f0\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6848\u53ef\u6269\u5c55\u4e14\u9ad8\u6548\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u5e73\u53f0\uff0c\u6709\u52a9\u4e8e\u63a8\u52a8\u672a\u6765\u5229\u7528\u91cf\u5b50\u7f51\u7edc\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.09935", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09935", "abs": "https://arxiv.org/abs/2507.09935", "authors": ["Hai Toan Nguyen", "Tien Dat Nguyen", "Viet Ha Nguyen"], "title": "Enhancing Retrieval Augmented Generation with Hierarchical Text Segmentation Chunking", "comment": null, "summary": "Retrieval-Augmented Generation (RAG) systems commonly use chunking strategies\nfor retrieval, which enhance large language models (LLMs) by enabling them to\naccess external knowledge, ensuring that the retrieved information is\nup-to-date and domain-specific. However, traditional methods often fail to\ncreate chunks that capture sufficient semantic meaning, as they do not account\nfor the underlying textual structure. This paper proposes a novel framework\nthat enhances RAG by integrating hierarchical text segmentation and clustering\nto generate more meaningful and semantically coherent chunks. During inference,\nthe framework retrieves information by leveraging both segment-level and\ncluster-level vector representations, thereby increasing the likelihood of\nretrieving more precise and contextually relevant information. Evaluations on\nthe NarrativeQA, QuALITY, and QASPER datasets indicate that the proposed method\nachieved improved results compared to traditional chunking techniques.", "AI": {"tldr": "RAG \u7cfb\u7edf\u901a\u5e38\u4f7f\u7528\u5757\u7b56\u7565\u8fdb\u884c\u68c0\u7d22\uff0c\u4ee5\u589e\u5f3a LLM \u8bbf\u95ee\u5916\u90e8\u77e5\u8bc6\u7684\u80fd\u529b\u3002\u7136\u800c\uff0c\u4f20\u7edf\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u521b\u5efa\u6355\u83b7\u8db3\u591f\u8bed\u4e49\u610f\u4e49\u7684\u5757\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u5206\u5c42\u6587\u672c\u5206\u5272\u548c\u805a\u7c7b\u6765\u751f\u6210\u66f4\u6709\u610f\u4e49\u3001\u8bed\u4e49\u4e0a\u66f4\u8fde\u8d2f\u7684\u5757\u3002\u8be5\u6846\u67b6\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5229\u7528\u6bb5\u7ea7\u548c\u7c07\u7ea7\u5411\u91cf\u8868\u793a\u6765\u68c0\u7d22\u4fe1\u606f\uff0c\u4ece\u800c\u589e\u52a0\u4e86\u68c0\u7d22\u66f4\u7cbe\u786e\u3001\u4e0a\u4e0b\u6587\u76f8\u5173\u4fe1\u606fIf you want to know more, please check the original text. \u7684\u53ef\u80fd\u6027\u3002\u4e0e\u4f20\u7edf\u5757\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5728\u521b\u5efa\u6355\u83b7\u5145\u5206\u8bed\u4e49\u610f\u4e49\u7684\u5757\u65f6\u5e38\u5e38\u5931\u8d25\uff0c\u56e0\u4e3a\u5b83\u4eec\u6ca1\u6709\u8003\u8651\u5230\u6f5c\u5728\u7684\u6587\u672c\u7ed3\u6784\u3002", "method": "\u8be5\u6846\u67b6\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u901a\u8fc7\u5229\u7528\u6bb5\u7ea7\u548c\u7c07\u7ea7\u5411\u91cf\u8868\u793a\u6765\u68c0\u7d22\u4fe1\u606f\u3002", "result": "\u5728 NarrativeQA\u3001QuALITY \u548c QASPER \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u7684\u5757\u6280\u672f\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u63d0\u51fa\u7684\u65b0\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u5206\u5c42\u6587\u672c\u5206\u5272\u548c\u805a\u7c7b\u6765\u751f\u6210\u66f4\u6709\u610f\u4e49\u3001\u8bed\u4e49\u4e0a\u66f4\u8fde\u8d2f\u7684\u5757\uff0c\u4ece\u800c\u589e\u5f3a\u4e86 RAG\u3002"}}
{"id": "2507.09043", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09043", "abs": "https://arxiv.org/abs/2507.09043", "authors": ["Jingxiang Qu", "Wenhan Gao", "Yi Liu"], "title": "Shortening the Trajectories: Identity-Aware Gaussian Approximation for Efficient 3D Molecular Generation", "comment": null, "summary": "Gaussian-based Probabilistic Generative Models (GPGMs) generate data by\nreversing a stochastic process that progressively corrupts samples with\nGaussian noise. While these models have achieved state-of-the-art performance\nacross diverse domains, their practical deployment remains constrained by the\nhigh computational cost of long generative trajectories, which often involve\nhundreds to thousands of steps during training and sampling. In this work, we\nintroduce a theoretically grounded and empirically validated framework that\nimproves generation efficiency without sacrificing training granularity or\ninference fidelity. Our key insight is that for certain data modalities, the\nnoising process causes data to rapidly lose its identity and converge toward a\nGaussian distribution. We analytically identify a characteristic step at which\nthe data has acquired sufficient Gaussianity, and then replace the remaining\ngeneration trajectory with a closed-form Gaussian approximation. Unlike\nexisting acceleration techniques that coarsening the trajectories by skipping\nsteps, our method preserves the full resolution of learning dynamics while\navoiding redundant stochastic perturbations between `Gaussian-like'\ndistributions. Empirical results across multiple data modalities demonstrate\nsubstantial improvements in both sample quality and computational efficiency.", "AI": {"tldr": "\u901a\u8fc7\u7528\u95ed\u5f0f\u9ad8\u65af\u8fd1\u4f3c\u66ff\u6362\u751f\u6210\u8f68\u8ff9\u4e2d\u6570\u636e\u5931\u53bb\u8eab\u4efd\u4fe1\u606f\u540e\u7684\u90e8\u5206\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8GPGM\u7684\u751f\u6210\u6548\u7387\u548c\u6837\u672c\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684GPGM\u6a21\u578b\u5728\u8bad\u7ec3\u548c\u91c7\u6837\u8fc7\u7a0b\u4e2d\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u56e0\u4e3a\u5b83\u4eec\u6d89\u53ca\u5230\u6210\u767e\u4e0a\u5343\u4e2a\u6b65\u9aa4\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8GPGM\u7684\u751f\u6210\u6548\u7387\uff0c\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u786e\u5b9a\u6570\u636e\u83b7\u5f97\u9ad8\u65af\u6027\u7684\u7279\u5f81\u6b65\uff0c\u5e76\u7528\u95ed\u5f0f\u9ad8\u65af\u8fd1\u4f3c\u66ff\u6362\u5269\u4f59\u7684\u751f\u6210\u8f68\u8ff9\uff0c\u4ee5\u52a0\u901f\u9ad8\u65af\u57fa\u6570\u6982\u7387\u751f\u6210\u6a21\u578b\uff08GPGMs\uff09\u7684\u91c7\u6837\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u79cd\u6570\u636e\u6a21\u6001\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6837\u672c\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u4e0a\u53ef\u884c\u4e14\u7ecf\u8fc7\u5b9e\u8df5\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u5728\u4e0d\u727a\u7272\u8bad\u7ec3\u7ec6\u8282\u6216\u63a8\u7406\u4fdd\u771f\u5ea6\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u751f\u6210\u6548\u7387\u3002"}}
{"id": "2507.10397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10397", "abs": "https://arxiv.org/abs/2507.10397", "authors": ["Alessandra M. M. M. Gouv\u00eaa", "Nuno Paulos", "Eduardo Uchoa e Mari\u00e1 C. V. Nascimento"], "title": "Instance space analysis of the capacitated vehicle routing problem", "comment": null, "summary": "This paper seeks to advance CVRP research by addressing the challenge of\nunderstanding the nuanced relationships between instance characteristics and\nmetaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a\nvaluable tool that allows for a new perspective on the field. By combining the\nISA methodology with a dataset from the DIMACS 12th Implementation Challenge on\nVehicle Routing, our research enabled the identification of 23 relevant\ninstance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,\nwhich employ dimensionality reduction and machine learning methods, allowed us\nto create a two-dimensional projection of the instance space to understand how\nthe structure of instances affect the behavior of MHs. A key contribution of\nour work is that we provide a projection matrix, which makes it straightforward\nto incorporate new instances into this analysis and allows for a new method for\ninstance analysis in the CVRP field.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u5b9e\u4f8b\u7a7a\u95f4\u5206\u6790\uff08ISA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408DIMACS\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u51fa23\u4e2a\u5b9e\u4f8b\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u964d\u7ef4\u548c\u673a\u5668\u5b66\u4e60\u6280\u672f\u521b\u5efa\u4e86\u5b9e\u4f8b\u7a7a\u95f4\u7684\u4e8c\u7ef4\u6295\u5f71\uff0c\u4ee5\u5206\u6790\u5b9e\u4f8b\u7ed3\u6784\u5bf9\u5143\u542f\u53d1\u5f0f\uff08MH\uff09\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4fbf\u4e8e\u7eb3\u5165\u65b0\u5b9e\u4f8b\u7684\u6295\u5f71\u77e9\u9635\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u89e3\u51b3\u7406\u89e3\u5b9e\u4f8b\u7279\u5f81\u4e0e\u5143\u542f\u53d1\u5f0f\uff08MH\uff09\u6027\u80fd\u4e4b\u95f4\u7ec6\u5fae\u5173\u7cfb\u8fd9\u4e00\u6311\u6218\uff0c\u6765\u63a8\u8fdbCVRP\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5b9e\u4f8b\u7a7a\u95f4\u5206\u6790\uff08ISA\uff09\u65b9\u6cd5\u548c\u6765\u81eaDIMACS\u7b2c12\u5c4a\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u5b9e\u65bd\u6311\u6218\u7684\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u4e8623\u4e2a\u76f8\u5173\u7684\u5b9e\u4f8b\u7279\u5f81\u3002\u5229\u7528\u964d\u7ef4\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff08PRELIM\u3001SIFTED\u548cPILOT\u9636\u6bb5\uff09\u521b\u5efa\u4e86\u5b9e\u4f8b\u7a7a\u95f4\u7684\u4e8c\u7ef4\u6295\u5f71\uff0c\u4ee5\u7406\u89e3\u5b9e\u4f8b\u7ed3\u6784\u5982\u4f55\u5f71\u54cd\u5143\u542f\u53d1\u5f0f\uff08MH\uff09\u7684\u884c\u4e3a\u3002", "result": "\u7814\u7a76\u8bc6\u522b\u4e8623\u4e2a\u76f8\u5173\u7684\u5b9e\u4f8b\u7279\u5f81\uff0c\u5e76\u521b\u5efa\u4e86\u5b9e\u4f8b\u7a7a\u95f4\u7684\u4e8c\u7ef4\u6295\u5f71\uff0c\u5c55\u793a\u4e86\u5b9e\u4f8b\u7ed3\u6784\u5982\u4f55\u5f71\u54cdMH\u7684\u884c\u4e3a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6295\u5f71\u77e9\u9635\uff0c\u4fbf\u4e8e\u5c06\u65b0\u5b9e\u4f8b\u7eb3\u5165\u5206\u6790\uff0c\u5e76\u4e3aCVRP\u9886\u57df\u7684\u5b9e\u4f8b\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\u3002"}}
{"id": "2507.09294", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09294", "abs": "https://arxiv.org/abs/2507.09294", "authors": ["Rui Tang", "Haochen Yin", "Guankun Wang", "Long Bai", "An Wang", "Huxin Gao", "Jiazheng Wang", "Hongliang Ren"], "title": "Geo-RepNet: Geometry-Aware Representation Learning for Surgical Phase Recognition in Endoscopic Submucosal Dissection", "comment": "IEEE ICIA 2025", "summary": "Surgical phase recognition plays a critical role in developing intelligent\nassistance systems for minimally invasive procedures such as Endoscopic\nSubmucosal Dissection (ESD). However, the high visual similarity across\ndifferent phases and the lack of structural cues in RGB images pose significant\nchallenges. Depth information offers valuable geometric cues that can\ncomplement appearance features by providing insights into spatial relationships\nand anatomical structures. In this paper, we pioneer the use of depth\ninformation for surgical phase recognition and propose Geo-RepNet, a\ngeometry-aware convolutional framework that integrates RGB image and depth\ninformation to enhance recognition performance in complex surgical scenes.\nBuilt upon a re-parameterizable RepVGG backbone, Geo-RepNet incorporates the\nDepth-Guided Geometric Prior Generation (DGPG) module that extracts geometry\npriors from raw depth maps, and the Geometry-Enhanced Multi-scale Attention\n(GEMA) to inject spatial guidance through geometry-aware cross-attention and\nefficient multi-scale aggregation. To evaluate the effectiveness of our\napproach, we construct a nine-phase ESD dataset with dense frame-level\nannotations from real-world ESD videos. Extensive experiments on the proposed\ndataset demonstrate that Geo-RepNet achieves state-of-the-art performance while\nmaintaining robustness and high computational efficiency under complex and\nlow-texture surgical environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u521b\u6027\u5730\u5c06\u6df1\u5ea6\u4fe1\u606f\u7528\u4e8e\u624b\u672f\u9636\u6bb5\u8bc6\u522b\uff0c\u63d0\u51fa\u4e86 Geo-RepNet \u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u6df1\u5ea6\u4fe1\u606f\u548c\u51e0\u4f55\u611f\u77e5\u6a21\u5757\u6765\u63d0\u9ad8\u590d\u6742\u624b\u672f\u573a\u666f\u4e0b\u7684\u8bc6\u522b\u6027\u80fd\uff0c\u5e76\u5728\u81ea\u5b9a\u4e49\u7684 ESD \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u624b\u672f\u9636\u6bb5\u8bc6\u522b\u5bf9\u4e8e\u5f00\u53d1\u5fae\u521b\u624b\u672f\uff08\u5982\u5185\u955c\u7c98\u819c\u4e0b\u5265\u79bb\u672f (ESD)\uff09\u7684\u667a\u80fd\u8f85\u52a9\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e0d\u540c\u9636\u6bb5\u4e4b\u95f4\u7684\u9ad8\u5ea6\u89c6\u89c9\u76f8\u4f3c\u6027\u4ee5\u53ca RGB \u56fe\u50cf\u4e2d\u7f3a\u4e4f\u7ed3\u6784\u7ebf\u7d22\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002\u6df1\u5ea6\u4fe1\u606f\u53ef\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u51e0\u4f55\u7ebf\u7d22\uff0c\u901a\u8fc7\u63d0\u4f9b\u7a7a\u95f4\u5173\u7cfb\u548c\u89e3\u5256\u7ed3\u6784\u65b9\u9762\u7684\u89c1\u89e3\u6765\u8865\u5145\u5916\u89c2\u7279\u5f81\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Geo-RepNet \u7684\u51e0\u4f55\u611f\u77e5\u5377\u79ef\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86 RGB \u56fe\u50cf\u548c\u6df1\u5ea6\u4fe1\u606f\u3002Geo-RepNet \u57fa\u4e8e\u53ef\u91cd\u6784\u7684 RepVGG \u4e3b\u5e72\u6784\u5efa\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u6df1\u5ea6\u5f15\u5bfc\u51e0\u4f55\u5148\u9a8c\u751f\u6210\uff08DGPG\uff09\u6a21\u5757\u548c\u4e00\u4e2a\u51e0\u4f55\u589e\u5f3a\u591a\u5c3a\u5ea6\u6ce8\u610f\u529b\uff08GEMA\uff09\u6a21\u5757\uff0c\u4ee5\u63d0\u53d6\u51e0\u4f55\u5148\u9a8c\u5e76\u6ce8\u5165\u7a7a\u95f4\u5f15\u5bfc\u3002", "result": "\u5728\u63d0\u51fa\u7684\u4e5d\u9636\u6bb5 ESD \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cGeo-RepNet \u5728\u590d\u6742\u548c\u4f4e\u7eb9\u7406\u7684\u624b\u672f\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "Geo-RepNet \u5728\u590d\u6742\u4e14\u4f4e\u7eb9\u7406\u7684\u624b\u672f\u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u548c\u9ad8\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.09873", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09873", "abs": "https://arxiv.org/abs/2507.09873", "authors": ["M. D. Urmey", "S. Dickson", "K. Adachi", "S. Mittal", "L. G. Talamo", "A. Kyle", "N. E. Frattini", "S. -X. Lin", "K. W. Lehnert", "C. A. Regal"], "title": "High-throughput electro-optic upconversion and downconversion with few-photon added noise", "comment": "11 pages, 4 figures", "summary": "A microwave-optical transducer of sufficiently low noise and high signal\ntransfer rate would allow entanglement to be distributed between\nsuperconducting quantum processors at a rate faster than the lifetimes of the\nquantum memories being linked. Here we present measurements of a membrane-based\nopto-electromechanical transducer with high signal throughput, as quantified by\nan efficiency-bandwidth-duty-cycle product of 7 kHz, approaching\nquantum-enabled operation in upconversion as well as downconversion, with\ninput-referred added noise of 3 photons. In downconversion, throughput of this\nmagnitude at the few-photon noise level is unprecedented. Using the quantum\nchannel capacity, we also find an expression for the maximum rate at which\nquantum information can be transduced, providing insight into the importance of\nimproving both a transducer's throughput and noise performance. With feasible\nimprovements, the high throughput achieved with this device positions\nmembrane-based transducers as a strategic choice for demonstrations of a\nquantum network with reasonable averaging times.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u578b\u7684\u819c\u57fa\u5149\u529b\u673a\u68b0\u6362\u80fd\u5668\uff0c\u5177\u6709\u4f4e\u566a\u58f0\u548c\u9ad8\u4f20\u8f93\u901f\u7387\uff0c\u6709\u671b\u7528\u4e8e\u6784\u5efa\u91cf\u5b50\u7f51\u7edc\u3002", "motivation": "\u9700\u8981\u4e00\u79cd\u4f4e\u566a\u58f0\u3001\u9ad8\u4f20\u8f93\u901f\u7387\u7684\u5fae\u6ce2-\u5149\u5b66\u6362\u80fd\u5668\u6765\u5b9e\u73b0\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u4e4b\u95f4\u7684\u7ea0\u7f20\u5206\u53d1\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u57fa\u4e8e\u819c\u7684\u5149\u529b\u673a\u68b0\u6362\u80fd\u5668\u7684\u566a\u58f0\u548c\u4fe1\u53f7\u4f20\u8f93\u901f\u7387\u6765\u8bc4\u4f30\u5176\u6027\u80fd\u3002", "result": "\u6d4b\u91cf\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6362\u80fd\u5668\u7684\u6548\u7387-\u5e26\u5bbd-\u5360\u7a7a\u6bd4\u79ef\u4e3a7 kHz\uff0c\u8f93\u5165\u53c2\u8003\u566a\u58f0\u4e3a3\u4e2a\u5149\u5b50\uff0c\u5728\u4e0b\u8f6c\u6362\u65b9\u9762\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u6027\u80fd\u3002", "conclusion": "\u819c\u57fa\u5149\u529b\u673a\u68b0\u6362\u80fd\u5668\u5728\u63d0\u9ad8\u5176\u541e\u5410\u91cf\u548c\u964d\u4f4e\u566a\u58f0\u65b9\u9762\u5177\u6709\u53ef\u884c\u6027\u6539\u8fdb\u7684\u6f5c\u529b\uff0c\u4f7f\u5176\u6210\u4e3a\u6784\u5efa\u91cf\u5b50\u7f51\u7edc\u7684\u6218\u7565\u9009\u62e9\u3002"}}
{"id": "2507.09973", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09973", "abs": "https://arxiv.org/abs/2507.09973", "authors": ["Sarah Pan"], "title": "Tiny Reward Models", "comment": "2025 ICML Efficient Systems for Foundation Models Workshop", "summary": "Large decoder-based language models have become the dominant architecture for\nreward modeling in reinforcement learning from human feedback (RLHF). However,\nas reward models are increasingly deployed in test-time strategies, their\ninference costs become a growing concern. We present TinyRM, a family of small,\nbidirectional masked language models (MLMs) with as few as 400 million\nparameters, that rival the capabilities of models over 175 times larger on\nreasoning and safety preference modeling tasks. TinyRM combines FLAN-style\nprompting, Directional Low-Rank Adaptation (DoRA), and layer freezing to\nachieve strong performance on RewardBench, despite using significantly fewer\nresources. Our experiments suggest that small models benefit from\ndomain-specific tuning strategies, particularly in reasoning, where lightweight\nfinetuning methods are especially effective. While challenges remain in\nbuilding generalist models and conversational preference modeling, our\npreliminary results highlight the promise of lightweight bidirectional\narchitectures as efficient, scalable alternatives for preference modeling.", "AI": {"tldr": "TinyRM \u662f\u4e00\u79cd\u5c0f\u578b\u3001\u9ad8\u6548\u7684\u5956\u52b1\u6a21\u578b\uff0c\u5728 RLHF \u4e2d\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "motivation": "\u968f\u7740\u5956\u52b1\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u7528\u4e8e\u6d4b\u8bd5\u65f6\u7b56\u7565\uff0c\u5176\u63a8\u7406\u6210\u672c\u5df2\u6210\u4e3a\u65e5\u76ca\u589e\u957f\u7684\u5173\u6ce8\u70b9\u3002", "method": "TinyRM \u7ed3\u5408\u4e86 FLAN \u98ce\u683c\u7684\u63d0\u793a\u3001\u65b9\u5411\u6027\u4f4e\u79e9\u9002\u5e94\uff08DoRA\uff09\u548c\u5c42\u51bb\u7ed3\uff0c\u4ee5\u5b9e\u73b0\u5f3a\u5927\u7684\u6027\u80fd\u3002", "result": "TinyRM \u662f\u4e00\u4e2a\u5c0f\u578b\uff084 \u4ebf\u53c2\u6570\uff09\u7684\u53cc\u5411 MLM \u7cfb\u5217\uff0c\u5728\u63a8\u7406\u548c\u5b89\u5168\u504f\u597d\u5efa\u6a21\u4efb\u52a1\u4e0a\u53ef\u4e0e\u6bd4\u5b83\u5927 175 \u500d\u7684\u6a21\u578b\u7684\u6027\u80fd\u76f8\u5ab2\u7f8e\u3002", "conclusion": "\u5c0f\u578b\u53cc\u5411\u63a9\u7801\u8bed\u8a00\u6a21\u578b\uff08MLM\uff09\u5982 TinyRM \u662f RLHF \u4e2d\u5956\u52b1\u5efa\u6a21\u7684\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.10376", "categories": ["cs.RO", "I.2"], "pdf": "https://arxiv.org/pdf/2507.10376", "abs": "https://arxiv.org/abs/2507.10376", "authors": ["Mohammadhossein Talebi", "Pragyan Dahal", "Davide Possenti", "Stefano Arrigoni", "Francesco Braghin"], "title": "Raci-Net: Ego-vehicle Odometry Estimation in Adverse Weather Conditions", "comment": "8 pages", "summary": "Autonomous driving systems are highly dependent on sensors like cameras,\nLiDAR, and inertial measurement units (IMU) to perceive the environment and\nestimate their motion. Among these sensors, perception-based sensors are not\nprotected from harsh weather and technical failures. Although existing methods\nshow robustness against common technical issues like rotational misalignment\nand disconnection, they often degrade when faced with dynamic environmental\nfactors like weather conditions. To address these problems, this research\nintroduces a novel deep learning-based motion estimator that integrates visual,\ninertial, and millimeter-wave radar data, utilizing each sensor strengths to\nimprove odometry estimation accuracy and reliability under adverse\nenvironmental conditions such as snow, rain, and varying light. The proposed\nmodel uses advanced sensor fusion techniques that dynamically adjust the\ncontributions of each sensor based on the current environmental condition, with\nradar compensating for visual sensor limitations in poor visibility. This work\nexplores recent advancements in radar-based odometry and highlights that radar\nrobustness in different weather conditions makes it a valuable component for\npose estimation systems, specifically when visual sensors are degraded.\nExperimental results, conducted on the Boreas dataset, showcase the robustness\nand effectiveness of the model in both clear and degraded environments.", "AI": {"tldr": "\u5728\u6076\u52a3\u5929\u6c14\u4e0b\uff0c\u81ea\u52a8\u9a7e\u9a76\u7684\u8fd0\u52a8\u4f30\u8ba1\u9700\u8981\u878d\u5408\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\u6765\u8865\u507f\u89c6\u89c9\u4f20\u611f\u5668\u7684\u4e0d\u8db3\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u4f9d\u8d56\u89c6\u89c9\u3001\u6fc0\u5149\u96f7\u8fbe\u548cIMU\u7b49\u4f20\u611f\u5668\u8fdb\u884c\u73af\u5883\u611f\u77e5\u548c\u8fd0\u52a8\u4f30\u8ba1\uff0c\u4f46\u89c6\u89c9\u4f20\u611f\u5668\u5728\u6076\u52a3\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u4e0b\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u5c3d\u7ba1\u73b0\u6709\u65b9\u6cd5\u5bf9\u65cb\u8f6c\u5931\u51c6\u548c\u65ad\u5f00\u7b49\u6280\u672f\u95ee\u9898\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5b83\u4eec\u5728\u9762\u5bf9\u6076\u52a3\u5929\u6c14\u7b49\u52a8\u6001\u73af\u5883\u56e0\u7d20\u65f6\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u5728\u6076\u52a3\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u4f30\u8ba1\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8fd0\u52a8\u4f30\u8ba1\u5668\uff0c\u8be5\u4f30\u8ba1\u5668\u96c6\u6210\u4e86\u89c6\u89c9\u3001\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u548c\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\uff0c\u5e76\u91c7\u7528\u5148\u8fdb\u7684\u4f20\u611f\u5668\u878d\u5408\u6280\u672f\uff0c\u6839\u636e\u5f53\u524d\u73af\u5883\u6761\u4ef6\u52a8\u6001\u8c03\u6574\u5404\u4f20\u611f\u5668\u6570\u636e\u7684\u8d21\u732e\uff0c\u4ee5\u5f25\u8865\u89c6\u89c9\u4f20\u611f\u5668\u7684\u4e0d\u8db3\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u6674\u6717\u548c\u6076\u52a3\u73af\u5883\u4e0b\u5747\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u6beb\u7c73\u6ce2\u96f7\u8fbe\u5728\u4e0d\u540c\u5929\u6c14\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u4f7f\u5176\u6210\u4e3a\u59ff\u6001\u4f30\u8ba1\u7cfb\u7edf\u7684\u5b9d\u8d35\u7ec4\u6210\u90e8\u5206\uff0c\u5c24\u5176\u662f\u5728\u89c6\u89c9\u4f20\u611f\u5668\u6027\u80fd\u4e0b\u964d\u65f6\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u878d\u5408\u89c6\u89c9\u3001\u60ef\u6027\u4e0e\u6beb\u7c73\u6ce2\u96f7\u8fbe\u6570\u636e\uff0c\u5e76\u5728\u4e0d\u540c\u73af\u5883\u6761\u4ef6\u4e0b\u52a8\u6001\u8c03\u6574\u5404\u4f20\u611f\u5668\u6743\u91cd\uff0c\u63d0\u9ad8\u4e86\u5728\u96e8\u3001\u96ea\u3001\u5149\u7ebf\u53d8\u5316\u7b49\u6076\u52a3\u73af\u5883\u4e0b\u7684\u8fd0\u52a8\u4f30\u8ba1\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002"}}
{"id": "2507.09299", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09299", "abs": "https://arxiv.org/abs/2507.09299", "authors": ["Abdulvahap Mutlu", "\u015eeng\u00fcl Do\u011fan", "T\u00fcrker Tuncer"], "title": "ViT-ProtoNet for Few-Shot Image Classification: A Multi-Benchmark Evaluation", "comment": "All codes are available at\n  https://github.com/abdulvahapmutlu/vit-protonet", "summary": "The remarkable representational power of Vision Transformers (ViTs) remains\nunderutilized in few-shot image classification. In this work, we introduce\nViT-ProtoNet, which integrates a ViT-Small backbone into the Prototypical\nNetwork framework. By averaging class conditional token embeddings from a\nhandful of support examples, ViT-ProtoNet constructs robust prototypes that\ngeneralize to novel categories under 5-shot settings. We conduct an extensive\nempirical evaluation on four standard benchmarks: Mini-ImageNet, FC100,\nCUB-200, and CIFAR-FS, including overlapped support variants to assess\nrobustness. Across all splits, ViT-ProtoNet consistently outperforms CNN-based\nprototypical counterparts, achieving up to a 3.2\\% improvement in 5-shot\naccuracy and demonstrating superior feature separability in latent space.\nFurthermore, it outperforms or is competitive with transformer-based\ncompetitors using a more lightweight backbone. Comprehensive ablations examine\nthe impact of transformer depth, patch size, and fine-tuning strategy. To\nfoster reproducibility, we release code and pretrained weights. Our results\nestablish ViT-ProtoNet as a powerful, flexible approach for few-shot\nclassification and set a new baseline for transformer-based meta-learners.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09891", "categories": ["quant-ph", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09891", "abs": "https://arxiv.org/abs/2507.09891", "authors": ["Jiaxin Huang", "Yan Zhu", "Giulio Chiribella", "Ya-Dong Wu"], "title": "Sequence-Model-Guided Measurement Selection for Quantum State Learning", "comment": null, "summary": "Characterization of quantum systems from experimental data is a central\nproblem in quantum science and technology. But which measurements should be\nused to gather data in the first place? While optimal measurement choices can\nbe worked out for small quantum systems, the optimization becomes intractable\nas the system size grows large. To address this problem, we introduce a deep\nneural network with a sequence model architecture that searches for efficient\nmeasurement choices in a data-driven, adaptive manner. The model can be applied\nto a variety of tasks, including the prediction of linear and nonlinear\nproperties of quantum states, as well as state clustering and state tomography\ntasks. In all these tasks, we find that the measurement choices identified by\nour neural network consistently outperform the uniformly random choice.\nIntriguingly, for topological quantum systems, our model tends to recommend\nmeasurements at the system's boundaries, even when the task is to predict bulk\nproperties. This behavior suggests that the neural network may have\nindependently discovered a connection between boundaries and bulk, without\nhaving been provided any built-in knowledge of quantum physics.", "AI": {"tldr": "\u4f7f\u7528\u6df1\u5ea6\u5b66\u4e60\u641c\u7d22\u6700\u4f18\u91cf\u5b50\u6d4b\u91cf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3\u91cf\u5b50\u7cfb\u7edf\u5c3a\u5bf8\u589e\u5927\u5bfc\u81f4\u7684\u6700\u4f18\u6d4b\u91cf\u9009\u62e9\u4f18\u5316\u96be\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5177\u6709\u5e8f\u5217\u6a21\u578b\u67b6\u6784\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff0c\u4ee5\u6570\u636e\u9a71\u52a8\u3001\u81ea\u9002\u5e94\u7684\u65b9\u5f0f\u641c\u7d22\u9ad8\u6548\u7684\u6d4b\u91cf\u65b9\u6cd5\u3002", "result": "\u8be5\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5728\u5404\u79cd\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5747\u5300\u968f\u673a\u9009\u62e9\uff0c\u5e76\u5728\u62d3\u6251\u91cf\u5b50\u7cfb\u7edf\u4e2d\u53d1\u73b0\u4e86\u8fb9\u754c\u6d4b\u91cf\u4e0e\u4f53\u6027\u8d28\u9884\u6d4b\u4e4b\u95f4\u7684\u6f5c\u5728\u8054\u7cfb\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u7ebf\u6027\u4e0e\u975e\u7ebf\u6027\u6027\u8d28\u3001\u91cf\u5b50\u6001\u805a\u7c7b\u548c\u91cf\u5b50\u6001\u5c42\u6790\u6210\u50cf\u7b49\u4efb\u52a1\u4e2d\uff0c\u5176\u6d4b\u91cf\u9009\u62e9\u4f18\u4e8e\u5747\u5300\u968f\u673a\u9009\u62e9\uff0c\u5c24\u5176\u5728\u62d3\u6251\u91cf\u5b50\u7cfb\u7edf\u4e2d\uff0c\u6a21\u578b\u503e\u5411\u4e8e\u63a8\u8350\u8fb9\u754c\u6d4b\u91cf\uff0c\u5373\u4f7f\u4efb\u52a1\u662f\u9884\u6d4b\u4f53\u6027\u8d28\uff0c\u8fd9\u8868\u660e\u8be5\u6a21\u578b\u53ef\u80fd\u72ec\u7acb\u53d1\u73b0\u4e86\u8fb9\u754c\u4e0e\u4f53\u6027\u8d28\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2507.09982", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09982", "abs": "https://arxiv.org/abs/2507.09982", "authors": ["Hang Yuan", "Chen Li", "Wenjun Ma", "Yuncheng Jiang"], "title": "TextOmics-Guided Diffusion for Hit-like Molecular Generation", "comment": null, "summary": "Hit-like molecular generation with therapeutic potential is essential for\ntarget-specific drug discovery. However, the field lacks heterogeneous data and\nunified frameworks for integrating diverse molecular representations. To bridge\nthis gap, we introduce TextOmics, a pioneering benchmark that establishes\none-to-one correspondences between omics expressions and molecular textual\ndescriptions. TextOmics provides a heterogeneous dataset that facilitates\nmolecular generation through representations alignment. Built upon this\nfoundation, we propose ToDi, a generative framework that jointly conditions on\nomics expressions and molecular textual descriptions to produce biologically\nrelevant, chemically valid, hit-like molecules. ToDi leverages two encoders\n(OmicsEn and TextEn) to capture multi-level biological and semantic\nassociations, and develops conditional diffusion (DiffGen) for controllable\ngeneration. Extensive experiments confirm the effectiveness of TextOmics and\ndemonstrate ToDi outperforms existing state-of-the-art approaches, while also\nshowcasing remarkable potential in zero-shot therapeutic molecular generation.\nSources are available at: https://github.com/hala-ToDi.", "AI": {"tldr": "TextOmics \u548c ToDi \u6846\u67b6\u5728 omics \u6570\u636e\u548c\u5206\u5b50\u6587\u672c\u63cf\u8ff0\u7684\u7ed3\u5408\u65b9\u9762\u53d6\u5f97\u4e86\u8fdb\u5c55\uff0c\u5b9e\u73b0\u4e86\u547d\u4e2d\u6837\u5206\u5b50\u7684\u751f\u6210\u3002", "motivation": "\u5f25\u5408\u5f02\u6784\u6570\u636e\u548c\u7edf\u4e00\u6846\u67b6\u5728\u6574\u5408\u5206\u5b50\u8868\u793a\u65b9\u9762\u7684\u5dee\u8ddd\uff0c\u4ee5\u5b9e\u73b0\u5177\u6709\u6cbb\u7597\u6f5c\u529b\u7684\u547d\u4e2d\u6837\u5206\u5b50\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTextOmics\u7684\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5728 omics \u8868\u8fbe\u548c\u5206\u5b50\u6587\u672c\u63cf\u8ff0\u4e4b\u95f4\u5efa\u7acb\u4e86-\u4e00\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aToDi\u7684\u751f\u6210\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e24\u4e2a\u7f16\u7801\u5668\uff08OmicsEn\u548cTextEn\uff09\u6765\u6355\u83b7\u591a\u5c42\u6b21\u7684\u751f\u7269\u5b66\u548c\u8bed\u4e49\u5173\u8054\uff0c\u5e76\u91c7\u7528\u6761\u4ef6\u6269\u6563\uff08DiffGen\uff09\u8fdb\u884c\u53ef\u63a7\u751f\u6210\u3002", "result": "ToDi\u6846\u67b6\u5728\u751f\u6210\u547d\u4e2d\u6837\u5206\u5b50\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u96f6\u6837\u672c\u6cbb\u7597\u6027\u5206\u5b50\u751f\u6210\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "conclusion": "ToDi\u5728\u96f6\u6837\u672c\u6cbb\u7597\u6027\u5206\u5b50\u751f\u6210\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6f5c\u529b\uff0c\u5e76\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.09084", "categories": ["cs.LG", "cs.AI", "68T07, 90B22, 62M10", "I.2.m"], "pdf": "https://arxiv.org/pdf/2507.09084", "abs": "https://arxiv.org/abs/2507.09084", "authors": ["Nnamdi Daniel Aghanya", "Ta Duong Vu", "Ama\u00eblle Diop", "Charlotte Deville", "Nour Imane Kerroumi", "Irene Moulitsas", "Jun Li", "Desmond Bisandu"], "title": "Queue up for takeoff: a transferable deep learning framework for flight delay prediction", "comment": "3 figures, 20 pages references and appendix included,", "summary": "Flight delays are a significant challenge in the aviation industry, causing\nmajor financial and operational disruptions. To improve passenger experience\nand reduce revenue loss, flight delay prediction models must be both precise\nand generalizable across different networks. This paper introduces a novel\napproach that combines Queue-Theory with a simple attention model, referred to\nas the Queue-Theory SimAM (QT-SimAM). To validate our model, we used data from\nthe US Bureau of Transportation Statistics, where our proposed QT-SimAM\n(Bidirectional) model outperformed existing methods with an accuracy of 0.927\nand an F1 score of 0.932. To assess transferability, we tested the model on the\nEUROCONTROL dataset. The results demonstrated strong performance, achieving an\naccuracy of 0.826 and an F1 score of 0.791. Ultimately, this paper outlines an\neffective, end-to-end methodology for predicting flight delays. The proposed\nmodel's ability to forecast delays with high accuracy across different networks\ncan help reduce passenger anxiety and improve operational decision-making", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a QT-SimAM \u7684\u65b0\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u6392\u961f\u8bba\u548c\u6ce8\u610f\u529b\u6a21\u578b\uff0c\u4ee5\u63d0\u9ad8\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u63a8\u5e7f\u6027\u3002\u8be5\u6a21\u578b\u5728\u7f8e\u56fd\u548c\u6b27\u6d32\u7684\u6570\u636e\u96c6\u4e0a\u90fd\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u6539\u5584\u4e58\u5ba2\u4f53\u9a8c\u548c\u51cf\u5c11\u6536\u5165\u635f\u5931\uff0c\u822a\u73ed\u5ef6\u8bef\u9884\u6d4b\u6a21\u578b\u5fc5\u987b\u7cbe\u786e\u4e14\u53ef\u8de8\u4e0d\u540c\u7f51\u7edc\u63a8\u5e7f\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6392\u961f\u8bba\u548c\u7b80\u5355\u6ce8\u610f\u529b\u6a21\u578b\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u79f0\u4e3a\u961f\u5217\u8bba-SimAM\uff08QT-SimAM\uff09\u3002", "result": "\u4f7f\u7528\u6765\u81ea\u7f8e\u56fd\u8fd0\u8f93\u7edf\u8ba1\u5c40\u7684\u6570\u636e\u8fdb\u884c\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684 QT-SimAM\uff08\u53cc\u5411\uff09\u6a21\u578b\u7684\u51c6\u786e\u7387\u4e3a 0.927\uff0cF1 \u5206\u6570\u4e3a 0.932\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728 EUROCONTROL \u6570\u636e\u96c6\u4e0a\u6d4b\u8bd5\u6a21\u578b\u4ee5\u8bc4\u4f30\u53ef\u8f6c\u79fb\u6027\uff0c\u7ed3\u679c\u663e\u793a\u51fa\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u51c6\u786e\u7387\u4e3a 0.826\uff0cF1 \u5206\u6570\u4e3a 0.791\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u7aef\u5230\u7aef\u65b9\u6cd5\u6765\u9884\u6d4b\u822a\u73ed\u5ef6\u8bef\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u80fd\u591f\u8de8\u4e0d\u540c\u7f51\u7edc\u51c6\u786e\u9884\u6d4b\u5ef6\u8bef\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u51cf\u8f7b\u4e58\u5ba2\u7126\u8651\u5e76\u6539\u5584\u8fd0\u8425\u51b3\u7b56\u3002"}}
{"id": "2507.10446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10446", "abs": "https://arxiv.org/abs/2507.10446", "authors": ["Sudarshan Babu"], "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures", "comment": "arXiv admin note: text overlap with arXiv:2310.17075", "summary": "The ability to transfer knowledge from prior experiences to novel tasks\nstands as a pivotal capability of intelligent agents, including both humans and\ncomputational models. This principle forms the basis of transfer learning,\nwhere large pre-trained neural networks are fine-tuned to adapt to downstream\ntasks. Transfer learning has demonstrated tremendous success, both in terms of\ntask adaptation speed and performance. However there are several domains where,\ndue to lack of data, training such large pre-trained models or foundational\nmodels is not a possibility - computational chemistry, computational\nimmunology, and medical imaging are examples. To address these challenges, our\nwork focuses on designing architectures to enable efficient acquisition of\npriors when large amounts of data are unavailable. In particular, we\ndemonstrate that we can use neural memory to enable adaptation on\nnon-stationary distributions with only a few samples. Then we demonstrate that\nour hypernetwork designs (a network that generates another network) can acquire\nmore generalizable priors than standard networks when trained with Model\nAgnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene\ngeneration, demonstrating that they can acquire priors efficiently on just a\nhandful of training scenes, thereby leading to faster text-to-3D generation. We\nthen extend our hypernetwork framework to perform 3D segmentation on novel\nscenes with limited data by efficiently transferring priors from earlier viewed\nscenes. Finally, we repurpose an existing molecular generative method as a\npre-training framework that facilitates improved molecular property prediction,\naddressing critical challenges in computational immunology", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u795e\u7ecf\u8bb0\u5fc6\u548c\u8d85\u7f51\u7edc\u7b49\u65b0\u9896\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u6570\u636e\u91cf\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u8fc1\u79fb\u5b66\u4e60\u6311\u6218\uff0c\u5e76\u57283D\u573a\u666f\u751f\u6210\u30013D\u5206\u5272\u548c\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u7b49\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u8fc1\u79fb\u5b66\u4e60\u7684\u80fd\u529b\uff0c\u5373\u4ece\u5148\u9a8c\u7ecf\u9a8c\u8fc1\u79fb\u77e5\u8bc6\u5230\u65b0\u4efb\u52a1\u7684\u80fd\u529b\uff0c\u662f\u667a\u80fd\u4f53\uff08\u5305\u62ec\u4eba\u7c7b\u548c\u8ba1\u7b97\u6a21\u578b\uff09\u7684\u5173\u952e\u80fd\u529b\u3002\u867d\u7136\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u5728\u8bb8\u591a\u9886\u57df\u53d6\u5f97\u4e86\u5de8\u5927\u6210\u529f\uff0c\u4f46\u5728\u8ba1\u7b97\u5316\u5b66\u3001\u8ba1\u7b97\u514d\u75ab\u5b66\u548c\u533b\u5b66\u6210\u50cf\u7b49\u9886\u57df\uff0c\u7531\u4e8e\u6570\u636e\u91cf\u4e0d\u8db3\uff0c\u8bad\u7ec3\u8fd9\u4e9b\u6a21\u578b\u662f\u4e0d\u53ef\u884c\u7684\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u8bbe\u8ba1\u80fd\u591f\u5f25\u8865\u6570\u636e\u91cf\u4e0d\u8db3\u7684\u9650\u5236\uff0c\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u548c\u5e94\u7528\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5305\u62ec\u795e\u7ecf\u8bb0\u5fc6\u548c\u8d85\u7f51\u7edc\uff0c\u6765\u89e3\u51b3\u6570\u636e\u91cf\u6709\u9650\u60c5\u51b5\u4e0b\u7684\u8fc1\u79fb\u5b66\u4e60\u95ee\u9898\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u7814\u7a76\u4f7f\u7528\u795e\u7ecf\u8bb0\u5fc6\u6765\u5b9e\u73b0\u5c11\u91cf\u6837\u672c\u4e0b\u7684\u975e\u5e73\u7a33\u5206\u5e03\u9002\u5e94\uff0c\u5e76\u5229\u7528\u8d85\u7f51\u7edc\uff08\u751f\u6210\u7f51\u7edc\uff09\u4e0e\u6a21\u578b\u65e0\u5173\u7684\u5143\u5b66\u4e60\uff08MAML\uff09\u76f8\u7ed3\u5408\u6765\u83b7\u53d6\u66f4\u5177\u6cdb\u5316\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u7814\u7a76\u5c06\u8fd9\u4e9b\u65b9\u6cd5\u5e94\u7528\u4e8e3D\u573a\u666f\u751f\u6210\u548c3D\u5206\u5272\u4efb\u52a1\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5148\u9a8c\u77e5\u8bc6\u83b7\u53d6\u548c\u5feb\u901f\u7684\u6587\u672c\u52303D\u751f\u6210\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5bf9\u73b0\u6709\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u6539\u9020\uff0c\u4f5c\u4e3a\u9884\u8bad\u7ec3\u6846\u67b6\u4ee5\u6539\u8fdb\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u3002", "result": "\u672c\u7814\u7a76\u6210\u529f\u8bbe\u8ba1\u5e76\u9a8c\u8bc1\u4e86\u80fd\u591f\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002\u7814\u7a76\u8bc1\u660e\uff0c\u4f7f\u7528\u795e\u7ecf\u8bb0\u5fc6\u53ef\u4ee5\u5728\u6570\u636e\u91cf\u5f88\u5c11\u7684\u60c5\u51b5\u4e0b\u9002\u5e94\u975e\u5e73\u7a33\u5206\u5e03\u3002\u8d85\u7f51\u7edc\u67b6\u6784\u5728\u4e0eMAML\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u76f8\u6bd4\u4e8e\u6807\u51c6\u7f51\u7edc\u80fd\u591f\u83b7\u5f97\u66f4\u5177\u6cdb\u5316\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u57283D\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u8d85\u7f51\u7edc\u4ec5\u9700\u5c11\u91cf\u8bad\u7ec3\u573a\u666f\u5373\u53ef\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u6587\u672c\u52303D\u751f\u6210\u3002\u6b64\u5916\uff0c\u57283D\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u8be5\u6846\u67b6\u4e5f\u80fd\u5728\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u9ad8\u6548\u7684\u5148\u9a8c\u77e5\u8bc6\u8fc1\u79fb\u3002\u6700\u540e\uff0c\u901a\u8fc7\u5c06\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u7528\u4f5c\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u6539\u8fdb\u4e86\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u514d\u75ab\u5b66\u4e2d\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u80fd\u591f\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u89e3\u51b3\u4e86\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u9886\u57df\uff08\u5982\u8ba1\u7b97\u5316\u5b66\u3001\u8ba1\u7b97\u514d\u75ab\u5b66\u548c\u533b\u5b66\u6210\u50cf\uff09\u4e2d\u8bad\u7ec3\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u6216\u57fa\u7840\u6a21\u578b\u7684\u6311\u6218\u3002\u7814\u7a76\u8bc1\u660e\u4e86\u4f7f\u7528\u795e\u7ecf\u8bb0\u5fc6\u53ef\u4ee5\u5b9e\u73b0\u5c11\u91cf\u6837\u672c\u4e0b\u7684\u975e\u5e73\u7a33\u5206\u5e03\u9002\u5e94\uff0c\u5e76\u4e14\u8d85\u7f51\u7edc\uff08\u751f\u6210\u5176\u4ed6\u7f51\u7edc\u7684\u7f51\u7edc\uff09\u6bd4\u6807\u51c6\u7f51\u7edc\u80fd\u83b7\u5f97\u66f4\u5177\u6cdb\u5316\u6027\u7684\u5148\u9a8c\u77e5\u8bc6\u3002\u7814\u7a76\u5c06\u8d85\u7f51\u7edc\u5e94\u7528\u4e8e3D\u573a\u666f\u751f\u6210\uff0c\u5b9e\u73b0\u4e86\u4ec5\u7528\u5c11\u91cf\u8bad\u7ec3\u573a\u666f\u5c31\u80fd\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ece\u800c\u52a0\u901f\u4e86\u6587\u672c\u52303D\u7684\u751f\u6210\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u6269\u5c55\u4e86\u8d85\u7f51\u7edc\u6846\u67b6\uff0c\u901a\u8fc7\u4ece\u5148\u524d \u0645\u0634\u0627\u0647\u062f scenes \u9ad8\u6548\u8fc1\u79fb\u5148\u9a8c\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u5728\u6570\u636e\u6709\u9650\u7684\u65b0\u573a\u666f\u4e0b\u76843D\u5206\u5272\u3002\u6700\u540e\uff0c\u7814\u7a76\u5c06\u73b0\u6709\u7684\u5206\u5b50\u751f\u6210\u65b9\u6cd5\u7528\u4f5c\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u4ee5\u6539\u8fdb\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u514d\u75ab\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2507.10500", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10500", "abs": "https://arxiv.org/abs/2507.10500", "authors": ["Kyungtae Han", "Yitao Chen", "Rohit Gupta", "Onur Altintas"], "title": "Scene-Aware Conversational ADAS with Generative AI for Real-Time Driver Assistance", "comment": null, "summary": "While autonomous driving technologies continue to advance, current Advanced\nDriver Assistance Systems (ADAS) remain limited in their ability to interpret\nscene context or engage with drivers through natural language. These systems\ntypically rely on predefined logic and lack support for dialogue-based\ninteraction, making them inflexible in dynamic environments or when adapting to\ndriver intent. This paper presents Scene-Aware Conversational ADAS (SC-ADAS), a\nmodular framework that integrates Generative AI components including large\nlanguage models, vision-to-text interpretation, and structured function calling\nto enable real-time, interpretable, and adaptive driver assistance. SC-ADAS\nsupports multi-turn dialogue grounded in visual and sensor context, allowing\nnatural language recommendations and driver-confirmed ADAS control. Implemented\nin the CARLA simulator with cloud-based Generative AI, the system executes\nconfirmed user intents as structured ADAS commands without requiring model\nfine-tuning. We evaluate SC-ADAS across scene-aware, conversational, and\nrevisited multi-turn interactions, highlighting trade-offs such as increased\nlatency from vision-based context retrieval and token growth from accumulated\ndialogue history. These results demonstrate the feasibility of combining\nconversational reasoning, scene perception, and modular ADAS control to support\nthe next generation of intelligent driver assistance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSC-ADAS\u7684\u751f\u6210\u5f0fAI\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u6a21\u578b\u548c\u89c6\u89c9\u89e3\u91ca\uff0c\u5b9e\u73b0\u4e86\u66f4\u667a\u80fd\u3001\u66f4\u5177\u9002\u5e94\u6027\u7684\u9a7e\u9a76\u8f85\u52a9\u7cfb\u7edf\uff0c\u652f\u6301\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u610f\u56fe\u786e\u8ba4\uff0c\u4f46\u5b58\u5728\u4e00\u5b9a\u7684\u5ef6\u8fdf\u548ctoken\u589e\u957f\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u7684ADAS\u7cfb\u7edf\u5728\u89e3\u91ca\u573a\u666f\u4e0a\u4e0b\u6587\u6216\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4e0e\u9a7e\u9a76\u5458\u4e92\u52a8\u65b9\u9762\u80fd\u529b\u6709\u9650\uff0c\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u903b\u8f91\u4e14\u4e0d\u652f\u6301\u5bf9\u8bdd\u4ea4\u4e92\uff0c\u5728\u52a8\u6001\u73af\u5883\u6216\u9002\u5e94\u9a7e\u9a76\u5458\u610f\u56fe\u65b9\u9762\u7075\u6d3b\u6027\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSC-ADAS\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u5305\u62ec\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u5230\u6587\u672c\u89e3\u91ca\u548c\u7ed3\u6784\u5316\u51fd\u6570\u8c03\u7528\u5728\u5185\u7684\u751f\u6210\u5f0fAI\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u3001\u53ef\u89e3\u91ca\u548c\u81ea\u9002\u5e94\u7684\u9a7e\u9a76\u5458\u8f85\u52a9\u3002SC-ADAS\u652f\u6301\u4ee5\u89c6\u89c9\u548c\u4f20\u611f\u5668\u4e0a\u4e0b\u6587\u4e3a\u57fa\u7840\u7684\u591a\u8f6e\u5bf9\u8bdd\uff0c\u80fd\u591f\u8fdb\u884c\u81ea\u7136\u8bed\u8a00\u5efa\u8bae\u548c\u9a7e\u9a76\u5458\u786e\u8ba4\u7684ADAS\u63a7\u5236\u3002\u5728CARLA\u6a21\u62df\u5668\u4e2d\u901a\u8fc7\u4e91\u7aef\u751f\u6210\u5f0fAI\u5b9e\u73b0\uff0c\u8be5\u7cfb\u7edf\u5728\u65e0\u9700\u6a21\u578b\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u5df2\u786e\u8ba4\u7684\u7528\u6237\u610f\u56fe\u4f5c\u4e3a\u7ed3\u6784\u5316\u7684ADAS\u547d\u4ee4\u3002", "result": "\u5728CARLA\u6a21\u62df\u5668\u4e2d\u5b9e\u73b0\u5e76\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7a81\u51fa\u4e86\u589e\u52a0\u7684\u5ef6\u8fdf\uff08\u6765\u81ea\u57fa\u4e8e\u89c6\u89c9\u7684\u4e0a\u4e0b\u6587\u68c0\u7d22\uff09\u548c\u5bf9\u8bdd\u5386\u53f2\u7d2f\u79ef\u5bfc\u81f4\u7684token\u589e\u957f\u7b49\u6743\u8861\u3002\u7ed3\u679c\u8868\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u5c06\u5bf9\u8bdd\u5f0f\u63a8\u7406\u3001\u573a\u666f\u611f\u77e5\u548c\u6a21\u5757\u5316ADAS\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u4ee5\u652f\u6301\u4e0b\u4e00\u4ee3\u667a\u80fd\u9a7e\u9a76\u8f85\u52a9\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2507.09305", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09305", "abs": "https://arxiv.org/abs/2507.09305", "authors": ["Zhiwei Xu"], "title": "DAA*: Deep Angular A Star for Image-based Path Planning", "comment": "International Conference on Computer Vision (ICCV), 2025", "summary": "Path smoothness is often overlooked in path imitation learning from expert\ndemonstrations. In this paper, we introduce a novel learning method, termed\ndeep angular A* (DAA*), by incorporating the proposed path angular freedom\n(PAF) into A* to improve path similarity through adaptive path smoothness. The\nPAF aims to explore the effect of move angles on path node expansion by finding\nthe trade-off between their minimum and maximum values, allowing for high\nadaptiveness for imitation learning. DAA* improves path optimality by closely\naligning with the reference path through joint optimization of path shortening\nand smoothing, which correspond to heuristic distance and PAF, respectively.\nThroughout comprehensive evaluations on 7 datasets, including 4 maze datasets,\n2 video-game datasets, and a real-world drone-view dataset containing 2\nscenarios, we demonstrate remarkable improvements of our DAA* over neural A* in\npath similarity between the predicted and reference paths with a shorter path\nlength when the shortest path is plausible, improving by 9.0% SPR, 6.9% ASIM,\nand 3.9% PSIM. Furthermore, when jointly learning pathfinding with both path\nloss and path probability map loss, DAA* significantly outperforms the\nstate-of-the-art TransPath by 6.7% SPR, 6.5% PSIM, and 3.7% ASIM. We also\ndiscuss the minor trade-off between path optimality and search efficiency where\napplicable.", "AI": {"tldr": "DAA*\u662f\u4e00\u79cd\u65b0\u7684\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5f15\u5165\u8def\u5f84\u89d2\u5ea6\u81ea\u7531\u5ea6\uff08PAF\uff09\u6765\u4f18\u5316\u8def\u5f84\u5e73\u6ed1\u5ea6\uff0c\u63d0\u9ad8\u4e86\u8def\u5f84\u76f8\u4f3c\u6027\u548c\u6700\u4f18\u6027\u3002", "motivation": "\u8def\u5f84\u6a21\u4eff\u5b66\u4e60\u4e2d\u5e38\u5e38\u5ffd\u7565\u8def\u5f84\u5e73\u6ed1\u5ea6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u6df1\u5ea6\u89d2\u5ea6A*\uff08DAA*\uff09\u7684\u65b0\u578b\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8def\u5f84\u89d2\u5ea6\u81ea\u7531\u5ea6\uff08PAF\uff09\u878d\u5165A*\u7b97\u6cd5\u6765\u63d0\u9ad8\u8def\u5f84\u5e73\u6ed1\u5ea6\u4ee5\u6539\u8fdb\u8def\u5f84\u76f8\u4f3c\u6027\u3002PAF\u901a\u8fc7\u5bfb\u627e\u6700\u5c0f\u548c\u6700\u5927\u503c\u7684\u6743\u8861\u6765\u63a2\u7d22\u79fb\u52a8\u89d2\u5ea6\u5bf9\u8def\u5f84\u8282\u70b9\u6269\u5c55\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u5b9e\u73b0\u5bf9\u6a21\u4eff\u5b66\u4e60\u7684\u9ad8\u5ea6\u9002\u5e94\u6027\u3002DAA*\u901a\u8fc7\u8054\u5408\u4f18\u5316\u8def\u5f84\u7f29\u77ed\u548c\u8def\u5f84\u5e73\u6ed1\uff08\u5206\u522b\u5bf9\u5e94\u542f\u53d1\u5f0f\u8ddd\u79bb\u548cPAF\uff09\u6765\u63d0\u9ad8\u8def\u5f84\u6700\u4f18\u6027\u3002", "result": "\u57287\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0cDAA*\u5728\u8def\u5f84\u76f8\u4f3c\u6027\uff08SPR\u3001ASIM\u3001PSIM\uff09\u548c\u8def\u5f84\u957f\u5ea6\u65b9\u9762\u5747\u663e\u8457\u4f18\u4e8e\u795e\u7ecfA*\u548cTransPath\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DAA*\u5728\u8def\u5f84\u76f8\u4f3c\u6027\u3001\u8def\u5f84\u957f\u5ea6\u548c\u8def\u5f84\u5e73\u6ed1\u5ea6\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u8def\u5f84\u6700\u4f18\u6027\u4e0e\u641c\u7d22\u6548\u7387\u4e4b\u95f4\u7684\u6743\u8861\u3002"}}
{"id": "2507.09944", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09944", "abs": "https://arxiv.org/abs/2507.09944", "authors": ["Bachtiar Rifai", "Dwi Satya Palupi", "Muhammad Farchani Rosyid"], "title": "The Ontic Necessity of the Quantum Wavefunction: Why Epistemic Views Struggle with the Uncertainty Principle", "comment": null, "summary": "The ontological status of the quantum wavefunction remains one of the most\ndebated questions in quantum theory. While epistemic interpretations regard the\nwavefunction as a reflection of our knowledge or beliefs, ontic interpretations\ntreat it as a real physical object. In this paper, we argue that epistemic\napproaches struggle to explain the universality and precision of the\nuncertainty principle, a core feature of quantum mechanics. By contrast,\ntreating the wave-function as ontic allows a consistent and natural derivation\nof quantum uncertainty from the mathematical structure of Hilbert space. We\nexamine key interpretations on both sides and highlight why the epistemic view\nfalls short in addressing constraints that appear to be intrinsic to nature.", "AI": {"tldr": "\u91cf\u5b50\u6ce2\u51fd\u6570\u66f4\u53ef\u80fd\u662f\u771f\u5b9e\u7684\u7269\u7406\u5ba2\u4f53\uff0c\u800c\u4e0d\u662f\u6211\u4eec\u77e5\u8bc6\u7684\u53cd\u6620\uff0c\u56e0\u4e3a\u524d\u8005\u80fd\u66f4\u597d\u5730\u89e3\u91ca\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u3002", "motivation": "\u91cf\u5b50\u6ce2\u51fd\u6570\u672c\u4f53\u8bba\u5730\u4f4d\u7684\u4e89\u8bba\u4ee5\u53ca\u8ba4\u8bc6\u8bba\u89e3\u91ca\u5728\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u65b9\u9762\u7684\u56f0\u96be\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83\u672c\u4f53\u8bba\u548c\u8ba4\u8bc6\u8bba\u89e3\u91ca\u6765\u8bba\u8bc1\u3002", "result": "\u672c\u4f53\u8bba\u65b9\u6cd5\u53ef\u4ee5\u4ece\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u5bfc\u51fa\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u539f\u7406\uff0c\u800c\u8ba4\u8bc6\u8bba\u65b9\u6cd5\u5219\u4e0d\u80fd\u3002\u8ba4\u8bc6\u8bba\u89c2\u70b9\u5728\u89e3\u91ca\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u672c\u4f53\u8bba\u89e3\u91ca\u6bd4\u8ba4\u8bc6\u8bba\u89e3\u91ca\u66f4\u80fd\u81ea\u7136\u5730\u89e3\u91ca\u91cf\u5b50\u4e0d\u786e\u5b9a\u6027\u539f\u7406\u3002"}}
{"id": "2507.10008", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10008", "abs": "https://arxiv.org/abs/2507.10008", "authors": ["Jun Li", "Xiangmeng Wang", "Haoyang Li", "Yifei Yan", "Hong Va Leong", "Ling Feng", "Nancy Xiaonan Yu", "Qing Li"], "title": "Protective Factor-Aware Dynamic Influence Learning for Suicide Risk Prediction on Social Media", "comment": null, "summary": "Suicide is a critical global health issue that requires urgent attention.\nEven though prior work has revealed valuable insights into detecting current\nsuicide risk on social media, little attention has been paid to developing\nmodels that can predict subsequent suicide risk over time, limiting their\nability to capture rapid fluctuations in individuals' mental state transitions.\nIn addition, existing work ignores protective factors that play a crucial role\nin suicide risk prediction, focusing predominantly on risk factors alone.\nProtective factors such as social support and coping strategies can mitigate\nsuicide risk by moderating the impact of risk factors. Therefore, this study\nproposes a novel framework for predicting subsequent suicide risk by jointly\nlearning the dynamic influence of both risk factors and protective factors on\nusers' suicide risk transitions. We propose a novel Protective Factor-Aware\nDataset, which is built from 12 years of Reddit posts along with comprehensive\nannotations of suicide risk and both risk and protective factors. We also\nintroduce a Dynamic Factors Influence Learning approach that captures the\nvarying impact of risk and protective factors on suicide risk transitions,\nrecognizing that suicide risk fluctuates over time according to established\npsychological theories. Our thorough experiments demonstrate that the proposed\nmodel significantly outperforms state-of-the-art models and large language\nmodels across three datasets. In addition, the proposed Dynamic Factors\nInfluence Learning provides interpretable weights, helping clinicians better\nunderstand suicidal patterns and enabling more targeted intervention\nstrategies.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u98ce\u9669\u548c\u4fdd\u62a4\u56e0\u7d20\u6765\u9884\u6d4b\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u968f\u65f6\u95f4\u7684\u81ea\u6740\u98ce\u9669\u53d8\u5316\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6\u548c\u4e00\u79cd\u52a8\u6001\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u540c\u65f6\u8fd8\u80fd\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\u4ee5\u8f85\u52a9\u5e72\u9884\u3002", "motivation": "\u73b0\u6709\u7684\u5173\u4e8e\u793e\u4ea4\u5a92\u4f53\u4e0a\u81ea\u6740\u98ce\u9669\u68c0\u6d4b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u9884\u6d4b\u5f53\u524d\u7684\u81ea\u6740\u98ce\u9669\uff0c\u800c\u5bf9\u4e8e\u9884\u6d4b\u968f\u65f6\u95f4\u53d8\u5316\u7684\u540e\u7eed\u81ea\u6740\u98ce\u9669\u7684\u7814\u7a76\u5219\u76f8\u5bf9\u8f83\u5c11\u3002\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u6355\u6349\u4e2a\u4f53\u5fc3\u7406\u72b6\u6001\u8f6c\u53d8\u7684\u5feb\u901f\u53d8\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u4e86\u5728\u81ea\u6740\u98ce\u9669\u9884\u6d4b\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u7684\u4fdd\u62a4\u56e0\u7d20\uff08\u5982\u793e\u4f1a\u652f\u6301\u548c\u5e94\u5bf9\u7b56\u7565\uff09\uff0c\u800c\u8fc7\u5ea6\u5173\u6ce8\u98ce\u9669\u56e0\u7d20\u3002\u7136\u800c\uff0c\u4fdd\u62a4\u56e0\u7d20\u53ef\u4ee5\u901a\u8fc7\u7f13\u548c\u98ce\u9669\u56e0\u7d20\u7684\u5f71\u54cd\u6765\u964d\u4f4e\u81ea\u6740\u98ce\u9669\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u4fdd\u62a4\u56e0\u7d20\u611f\u77e5\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u5229\u752812\u5e74\u7684Reddit\u5e16\u5b50\u4ee5\u53ca\u5168\u9762\u7684\u81ea\u6740\u98ce\u9669\u3001\u98ce\u9669\u56e0\u7d20\u548c\u4fdd\u62a4\u56e0\u7d20\u6ce8\u91ca\u6784\u5efa\u7684\u3002\u7814\u7a76\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u52a8\u6001\u56e0\u7d20\u5f71\u54cd\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u6355\u6349\u98ce\u9669\u56e0\u7d20\u548c\u4fdd\u62a4\u56e0\u7d20\u5bf9\u81ea\u6740\u98ce\u9669\u8f6c\u53d8\u8fc7\u7a0b\u4e2d\u968f\u65f6\u95f4\u53d8\u5316\u7684\u5177\u4f53\u5f71\u54cd\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u8bc6\u522b\u98ce\u9669\u548c\u4fdd\u62a4\u56e0\u7d20\u5bf9\u81ea\u6740\u98ce\u9669\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u8fd9\u7b26\u5408\u5df2\u6709\u7684\u5fc3\u7406\u5b66\u7406\u8bba\uff0c\u5373\u81ea\u6740\u98ce\u9669\u4f1a\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u6ce2\u52a8\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u52a8\u6001\u56e0\u7d20\u5f71\u54cd\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6743\u91cd\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u533b\u751f\u66f4\u597d\u5730\u7406\u89e3\u81ea\u6740\u6a21\u5f0f\uff0c\u5e76\u5236\u5b9a\u66f4\u5177\u9488\u5bf9\u6027\u7684\u5e72\u9884\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u98ce\u9669\u56e0\u7d20\u548c\u4fdd\u62a4\u56e0\u7d20\u5bf9\u7528\u6237\u81ea\u6740\u98ce\u9669\u8f6c\u53d8\u7684\u52a8\u6001\u5f71\u54cd\uff0c\u6765\u9884\u6d4b\u540e\u7eed\u7684\u81ea\u6740\u98ce\u9669\uff0c\u5e76\u4e14\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6a21\u578b\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6240\u63d0\u51fa\u7684\u52a8\u6001\u56e0\u7d20\u5f71\u54cd\u5b66\u4e60\u65b9\u6cd5\u80fd\u591f\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u6743\u91cd\uff0c\u6709\u52a9\u4e8e\u4e34\u5e8a\u533b\u751f\u66f4\u597d\u5730\u7406\u89e3\u81ea\u6740\u6a21\u5f0f\u5e76\u5236\u5b9a\u66f4\u5177\u9488\u5bf9\u6027\u7684\u5e72\u9884\u7b56\u7565\u3002"}}
{"id": "2507.09087", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09087", "abs": "https://arxiv.org/abs/2507.09087", "authors": ["Esraa Elelimy", "Brett Daley", "Andrew Patterson", "Marlos C. Machado", "Adam White", "Martha White"], "title": "Deep Reinforcement Learning with Gradient Eligibility Traces", "comment": null, "summary": "Achieving fast and stable off-policy learning in deep reinforcement learning\n(RL) is challenging. Most existing methods rely on semi-gradient\ntemporal-difference (TD) methods for their simplicity and efficiency, but are\nconsequently susceptible to divergence. While more principled approaches like\nGradient TD (GTD) methods have strong convergence guarantees, they have rarely\nbeen used in deep RL. Recent work introduced the Generalized Projected Bellman\nError ($\\GPBE$), enabling GTD methods to work efficiently with nonlinear\nfunction approximation. However, this work is only limited to one-step methods,\nwhich are slow at credit assignment and require a large number of samples. In\nthis paper, we extend the $\\GPBE$ objective to support multistep credit\nassignment based on the $\\lambda$-return and derive three gradient-based\nmethods that optimize this new objective. We provide both a forward-view\nformulation compatible with experience replay and a backward-view formulation\ncompatible with streaming algorithms. Finally, we evaluate the proposed\nalgorithms and show that they outperform both PPO and StreamQ in MuJoCo and\nMinAtar environments, respectively. Code available at\nhttps://github.com/esraaelelimy/gtd\\_algos", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684GPBE\u76ee\u6807\uff0c\u7528\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u6b65\u79bb\u7b56\u7565\u5b66\u4e60\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u4e2d\u5feb\u901f\u7a33\u5b9a\u8fdb\u884c\u79bb\u7b56\u7565\u5b66\u4e60\u7684\u6311\u6218\uff0c\u5e76\u6539\u8fdb\u73b0\u6709GTD\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4f7f\u5176\u9002\u7528\u4e8e\u591a\u6b65\u4fe1\u7528\u5206\u914d\u3002", "method": "\u5c06GPBE\u76ee\u6807\u6269\u5c55\u5230\u652f\u6301\u57fa\u4e8e\u03bb-return\u7684\u591a\u6b65\u4fe1\u7528\u5206\u914d\uff0c\u5e76\u63a8\u5bfc\u4e86\u4e09\u79cd\u4f18\u5316\u6b64\u65b0\u76ee\u6807\u7684\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\u3002\u63d0\u4f9b\u4e86\u4e0e\u7ecf\u9a8c\u56de\u653e\u517c\u5bb9\u7684\u524d\u5411\u89c6\u56fe\u548c\u4e0e\u6d41\u7b97\u6cd5\u517c\u5bb9\u7684\u540e\u5411\u89c6\u56fe\u3002", "result": "\u63a8\u5bfc\u4e86\u4e09\u79cd\u57fa\u4e8e\u68af\u5ea6\u7684\u65b9\u6cd5\uff0c\u5e76\u5728MuJoCo\u548cMinAtar\u73af\u5883\u4e2d\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u63d0\u51fa\u7684\u7b97\u6cd5\u5728MuJoCo\u548cMinAtar\u73af\u5883\u4e2d\u4f18\u4e8e PPO \u548c StreamQ\u3002"}}
{"id": "2507.10543", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10543", "abs": "https://arxiv.org/abs/2507.10543", "authors": ["Juyi Sheng", "Ziyi Wang", "Peiming Li", "Mengyuan Liu"], "title": "MP1: Mean Flow Tames Policy Learning in 1-step for Robotic Manipulation", "comment": null, "summary": "In robot manipulation, robot learning has become a prevailing approach.\nHowever, generative models within this field face a fundamental trade-off\nbetween the slow, iterative sampling of diffusion models and the architectural\nconstraints of faster Flow-based methods, which often rely on explicit\nconsistency losses. To address these limitations, we introduce MP1, which pairs\n3D point-cloud inputs with the MeanFlow paradigm to generate action\ntrajectories in one network function evaluation (1-NFE). By directly learning\nthe interval-averaged velocity via the MeanFlow Identity, our policy avoids any\nadditional consistency constraints. This formulation eliminates numerical\nODE-solver errors during inference, yielding more precise trajectories. MP1\nfurther incorporates CFG for improved trajectory controllability while\nretaining 1-NFE inference without reintroducing structural constraints. Because\nsubtle scene-context variations are critical for robot learning, especially in\nfew-shot learning, we introduce a lightweight Dispersive Loss that repels state\nembeddings during training, boosting generalization without slowing inference.\nWe validate our method on the Adroit and Meta-World benchmarks, as well as in\nreal-world scenarios. Experimental results show MP1 achieves superior average\ntask success rates, outperforming DP3 by 10.2% and FlowPolicy by 7.3%. Its\naverage inference time is only 6.8 ms-19x faster than DP3 and nearly 2x faster\nthan FlowPolicy. Our code is available at https://mp1-2254.github.io/.", "AI": {"tldr": "MP1\u662f\u4e00\u79cd\u521b\u65b0\u7684\u673a\u5668\u4eba\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u54083D\u70b9\u4e91\u548cMeanFlow\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u3001\u7cbe\u786e\u3001\u53ef\u63a7\u7684\u52a8\u4f5c\u8f68\u8ff9\u751f\u6210\u3002\u5b83\u5728\u901f\u5ea6\u548c\u6027\u80fd\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u901a\u8fc7Dispersive Loss\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5728\u673a\u5668\u4eba\u5b66\u4e60\u4e2d\u5b58\u5728|||\u91c7\u6837|||\u901f\u5ea6\u4e0e|||\u67b6\u6784|||\u7ea6\u675f\u4e4b\u95f4\u7684|||\u6743\u8861|||\u95ee\u9898\u3002\u6269\u6563\u6a21\u578b\u91c7\u6837\u901f\u5ea6\u6162\uff0c\u800c\u57fa\u4e8e\u6d41\u7684\u65b9\u6cd5|||\u67b6\u6784|||\u7ea6\u675f\u591a\u4e14\u9700\u8981\u663e\u5f0f\u7684|||\u4e00\u81f4\u6027|||\u635f\u5931\u3002", "method": "MP1\u7ed3\u54083D\u70b9\u4e91\u8f93\u5165\u548cMeanFlow\u8303\u5f0f\uff0c\u76f4\u63a5\u5b66\u4e60\u533a\u95f4\u5e73\u5747\u901f\u5ea6\uff08\u901a\u8fc7MeanFlow Identity\uff09\uff0c\u5b9e\u73b01-NFE\u63a8\u7406\uff0c\u65e0\u9700\u663e\u5f0f|||\u4e00\u81f4\u6027|||\u7ea6\u675f\u3002\u5f15\u5165CFG\u589e\u5f3a\u8f68\u8ff9\u53ef\u63a7\u6027\uff0c\u5e76\u4f7f\u7528Dispersive Loss\u63d0\u5347\u6cdb\u5316\u80fd\u529b\u3002", "result": "MP1\u5728Adroit\u548cMeta-World\u57fa\u51c6\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8eDP3\uff0810.2%\uff09\u548cFlowPolicy\uff087.3%\uff09\u7684\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u3002\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u4ec5\u4e3a6.8\u6beb\u79d2\uff0c\u6bd4DP3\u5feb19\u500d\uff0c\u6bd4FlowPolicy\u5feb\u8fd12\u500d\u3002", "conclusion": "MP1\u901a\u8fc7\u7ed3\u54083D\u70b9\u4e91\u8f93\u5165\u548cMeanFlow\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u5355\u7f51\u7edc\u51fd\u6570\u8bc4\u4f30\uff081-NFE\uff09\u5373\u53ef\u751f\u6210\u52a8\u4f5c\u8f68\u8ff9\u3002\u901a\u8fc7\u76f4\u63a5\u5b66\u4e60\u5e73\u5747\u901f\u5ea6\uff0c\u8be5\u65b9\u6cd5\u65e0\u9700\u989d\u5916\u7684|||\u4e00\u81f4\u6027|||\u7ea6\u675f\uff0c\u6d88\u9664\u4e86\u6570\u503cODE\u6c42\u89e3\u5668\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u8bef\u5dee\uff0c\u4ece\u800c\u4ea7\u751f\u66f4\u7cbe\u786e\u7684\u8f68\u8ff9\u3002\u6b64\u5916\uff0cMP1\u5f15\u5165\u4e86CFG\u4ee5\u63d0\u9ad8\u8f68\u8ff9\u7684\u53ef\u63a7\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e861-NFE\u63a8\u7406\u901f\u5ea6\u3002\u4e3a\u4e86\u63d0\u5347\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u4e0b\uff0cMP1\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684Dispersive Loss\uff0c\u7528\u4e8e\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5c06\u72b6\u6001\u5d4c\u5165\u5206\u79bb\uff0c\u4e14\u4e0d\u5f71\u54cd\u63a8\u7406\u901f\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMP1\u5728Adroit\u548cMeta-World\u57fa\u51c6\u4ee5\u53ca\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e73\u5747\u4efb\u52a1\u6210\u529f\u7387\u4f18\u4e8eDP3\uff0810.2%\uff09\u548cFlowPolicy\uff087.3%\uff09\uff0c\u5e73\u5747\u63a8\u7406\u65f6\u95f4\u4ec5\u4e3a6.8\u6beb\u79d2\uff0c\u6bd4DP3\u5feb19\u500d\uff0c\u6bd4FlowPolicy\u5feb\u8fd12\u500d\u3002"}}
{"id": "2507.09308", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09308", "abs": "https://arxiv.org/abs/2507.09308", "authors": ["Zile Wang", "Hao Yu", "Jiabo Zhan", "Chun Yuan"], "title": "AlphaVAE: Unified End-to-End RGBA Image Reconstruction and Generation with Alpha-Aware Representation Learning", "comment": null, "summary": "Recent advances in latent diffusion models have achieved remarkable results\nin high-fidelity RGB image synthesis by leveraging pretrained VAEs to compress\nand reconstruct pixel data at low computational cost. However, the generation\nof transparent or layered content (RGBA image) remains largely unexplored, due\nto the lack of large-scale benchmarks. In this work, we propose ALPHA, the\nfirst comprehensive RGBA benchmark that adapts standard RGB metrics to\nfour-channel images via alpha blending over canonical backgrounds. We further\nintroduce ALPHAVAE, a unified end-to-end RGBA VAE that extends a pretrained RGB\nVAE by incorporating a dedicated alpha channel. The model is trained with a\ncomposite objective that combines alpha-blended pixel reconstruction,\npatch-level fidelity, perceptual consistency, and dual KL divergence\nconstraints to ensure latent fidelity across both RGB and alpha\nrepresentations. Our RGBA VAE, trained on only 8K images in contrast to 1M used\nby prior methods, achieves a +4.9 dB improvement in PSNR and a +3.2% increase\nin SSIM over LayerDiffuse in reconstruction. It also enables superior\ntransparent image generation when fine-tuned within a latent diffusion\nframework. Our code, data, and models are released on\nhttps://github.com/o0o0o00o0/AlphaVAE for reproducibility.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86ALPHA\uff0c\u9996\u4e2aRGBA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u53caALPHAVAE\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684RGBA VAE\uff0c\u53ef\u57288K\u56fe\u50cf\u4e0a\u5b9e\u73b0\u5353\u8d8a\u7684\u900f\u660e\u56fe\u50cf\u751f\u6210\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u751f\u6210\u900f\u660e\u6216\u5206\u5c42\u5185\u5bb9\uff08RGBA\u56fe\u50cf\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u7f3a\u4e4f\u5927\u89c4\u6a21\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ALPHA\uff0c\u8fd9\u662f\u9996\u4e2a\u5168\u9762\u7684RGBA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u901a\u8fc7\u5728\u6807\u51c6\u80cc\u666f\u4e0a\u8fdb\u884c alpha \u6df7\u5408\u5c06\u6807\u51c6RGB\u6307\u6807\u9002\u914d\u5230\u56db\u901a\u9053\u56fe\u50cf\u3002\u540c\u65f6\uff0c\u672c\u6587\u8fd8\u63d0\u51fa\u4e86ALPHAVAE\uff0c\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aefRGBA VAE\uff0c\u901a\u8fc7\u6574\u5408\u4e00\u4e2a\u4e13\u7528\u7684alpha\u901a\u9053\u6765\u6269\u5c55\u9884\u8bad\u7ec3\u7684RGB VAE\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u4e86\u4e00\u4e2a\u7ec4\u5408\u76ee\u6807\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u76ee\u6807\u7ed3\u5408\u4e86 alpha \u6df7\u5408\u50cf\u7d20\u91cd\u5efa\u3001\u5757\u7ea7\u4fdd\u771f\u5ea6\u3001\u611f\u77e5\u4e00\u81f4\u6027\u548c\u53ccKL\u6563\u5ea6\u7ea6\u675f\uff0c\u4ee5\u786e\u4fddRGB\u548calpha\u8868\u793a\u7684\u6f5c\u5728\u4fdd\u771f\u5ea6\u3002", "result": "ALPHAVAE\u5728\u91cd\u5efa\u65b9\u9762\u6bd4LayerDiffuse\u7684PSNR\u63d0\u9ad8\u4e86+4.9 dB\uff0cSSIM\u63d0\u9ad8\u4e86+3.2%\u3002\u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u5728\u6f5c\u5728\u6269\u6563\u6846\u67b6\u5185\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u900f\u660e\u56fe\u50cf\u751f\u6210\u3002", "conclusion": "ALPHA\u662f\u9996\u4e2a\u5168\u9762\u7684RGBA\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u901a\u8fc7\u5728\u6807\u51c6\u80cc\u666f\u4e0a\u8fdb\u884c alpha \u6df7\u5408\u5c06\u6807\u51c6RGB\u6307\u6807\u9002\u914d\u5230\u56db\u901a\u9053\u56fe\u50cf\u3002ALPHAVAE\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u7aef\u5230\u7aefRGBA VAE\uff0c\u5b83\u901a\u8fc7\u6574\u5408\u4e00\u4e2a\u4e13\u7528\u7684alpha\u901a\u9053\u6765\u6269\u5c55\u9884\u8bad\u7ec3\u7684RGB VAE\u3002\u6211\u4eec\u7684RGBA VAE\u4ec5\u75288K\u56fe\u50cf\u8fdb\u884c\u8bad\u7ec3\uff0c\u5728\u91cd\u5efa\u65b9\u9762\u6bd4LayerDiffuse\u7684PSNR\u63d0\u9ad8\u4e86+4.9 dB\uff0cSSIM\u63d0\u9ad8\u4e86+3.2%\u3002\u5b83\u8fd8\u53ef\u4ee5\u7528\u4e8e\u5728\u6f5c\u5728\u6269\u6563\u6846\u67b6\u5185\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u5b9e\u73b0\u5353\u8d8a\u7684\u900f\u660e\u56fe\u50cf\u751f\u6210\u3002"}}
{"id": "2507.09963", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.09963", "abs": "https://arxiv.org/abs/2507.09963", "authors": ["Ignatius William Primaatmaja", "Hong Jie Ng", "Koon Tong Goh"], "title": "Device-Independent Private Quantum Randomness Beacon", "comment": "10 pages, 3 figures. Comments are welcomed", "summary": "Device-independent quantum random number generation (DIQRNG) is the gold\nstandard for generating truly random numbers, as it can produce certifiably\nrandom numbers from untrusted devices. However, the stringent device\nrequirements of traditional DIQRNG protocols have limited their practical\napplications. Here, we introduce Device-Independent Private Quantum Randomness\nBeacon (DIPQRB), a novel approach to generate random numbers from untrusted\ndevices based on routed Bell tests. This method significantly relaxes the\ndevice requirements, enabling a more practical way of generating randomness\nfrom untrusted devices. By distributing the device requirements across a\nnetwork of servers and clients, our proposal allows the server to operate\nhigh-performance devices while the clients can be equipped with more\ncost-effective devices. Moreover, the outputs of the client's device are also\nprivate, even against the server, which is essential in cryptographic\napplications. Therefore, DIPQRB provides a cost-effective method to generate\nsecure and private random numbers from untrusted devices.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDIPQRB\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ece\u4e0d\u53ef\u4fe1\u8bbe\u5907\u751f\u6210\u5b89\u5168\u7684\u79c1\u6709\u968f\u673a\u6570\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8def\u7531\u8d1d\u5c14\u6d4b\u8bd5\u653e\u5bbd\u4e86\u8bbe\u5907\u8981\u6c42\uff0c\u4f7f\u5ba2\u6237\u7aef\u53ef\u4ee5\u4f7f\u7528\u66f4\u7ecf\u6d4e\u9ad8\u6548\u7684\u8bbe\u5907\uff0c\u5e76\u786e\u4fdd\u8f93\u51fa\u7684\u79c1\u5bc6\u6027\u3002", "motivation": "\u4f20\u7edf\u7684DIQRNG\u534f\u8bae\u7684\u4e25\u683c\u8bbe\u5907\u8981\u6c42\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u9650\u5236\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u653e\u5bbd\u8bbe\u5907\u8981\u6c42\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5b9e\u7528\u7684\u968f\u673a\u6570\u751f\u6210\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8e\u8def\u7531\u8d1d\u5c14\u6d4b\u8bd5\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bbe\u5907\u65e0\u5173\u79c1\u6709\u91cf\u5b50\u968f\u673a\u4fe1\u6807\uff08DIPQRB\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u653e\u5bbd\u4e86\u5bf9\u8bbe\u5907\u7684\u8981\u6c42\uff0c\u4ece\u800c\u80fd\u591f\u4ece\u4e0d\u53ef\u4fe1\u8bbe\u5907\u751f\u6210\u968f\u673a\u6570\u7684\u66f4\u5b9e\u7528\u7684\u65b9\u5f0f\u3002\u901a\u8fc7\u5c06\u8bbe\u5907\u8981\u6c42\u5206\u5e03\u5728\u670d\u52a1\u5668\u548c\u5ba2\u6237\u7aef\u7f51\u7edc\u4e2d\uff0c\u8be5\u65b9\u6848\u5141\u8bb8\u670d\u52a1\u5668\u8fd0\u884c\u9ad8\u6027\u80fd\u8bbe\u5907\uff0c\u800c\u5ba2\u6237\u7aef\u53ef\u4ee5\u914d\u5907\u66f4\u5177\u6210\u672c\u6548\u76ca\u7684\u8bbe\u5907\u3002", "result": "DIPQRB\u7684\u5b9e\u73b0\u80fd\u591f\u663e\u8457\u653e\u5bbd\u5bf9\u8bbe\u5907\u7684\u8981\u6c42\uff0c\u4f7f\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u66f4\u5177\u6210\u672c\u6548\u76ca\u548c\u5b9e\u7528\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u786e\u4fdd\u4e86\u5373\u4f7f\u5bf9\u670d\u52a1\u5668\u800c\u8a00\uff0c\u5ba2\u6237\u7aef\u8bbe\u5907\u7684\u8f93\u51fa\u4e5f\u662f\u79c1\u6709\u7684\uff0c\u8fd9\u5bf9\u4e8e\u52a0\u5bc6\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "DIPQRB\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4ece\u4e0d\u53ef\u4fe1\u8bbe\u5907\u751f\u6210\u5b89\u5168\u4e14\u79c1\u6709\u7684\u968f\u673a\u6570\u3002"}}
{"id": "2507.10059", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10059", "abs": "https://arxiv.org/abs/2507.10059", "authors": ["David Ponce", "Thierry Etchegoyhen", "Javier Del Ser"], "title": "GeLaCo: An Evolutionary Approach to Layer Compression", "comment": null, "summary": "Large Language Models (LLM) have achieved remarkable performance across a\nlarge number of tasks, but face critical deployment and usage barriers due to\nsubstantial computational requirements. Model compression methods, which aim to\nreduce model size while preserving its capacity, are an important means to\nmitigate these issues. Promising approaches along these lines, such as\nstructured pruning, typically require costly empirical search for optimal\nvariants and may run the risk of ignoring better solutions. In this work we\nintroduce GeLaCo, an evolutionary approach to LLM compression via layer\ncollapse. Our approach supports an efficient exploration of the compression\nsolution space via population-based search and a module-wise similarity fitness\nfunction capturing attention, feed-forward, and hidden state representations.\nGeLaCo also supports both single and multi-objective evolutionary compression\nsearch, establishing the first Pareto frontier along compression and quality\naxes. We evaluate GeLaCo solutions via both perplexity-based and generative\nevaluations over foundational and instruction-tuned models, outperforming\nstate-of-the-art alternatives.", "AI": {"tldr": "GeLaCo\u662f\u4e00\u79cd\u5229\u7528\u8fdb\u5316\u7b97\u6cd5\u901a\u8fc7\u5c42\u6298\u53e0\u6765\u538b\u7f29\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u9ad8\u6548\u7684\u641c\u7d22\u548c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\uff0c\u89e3\u51b3\u4e86LLM\u7684\u90e8\u7f72\u548c\u4f7f\u7528\u969c\u788d\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u56e0\u5de8\u5927\u7684\u8ba1\u7b97\u9700\u6c42\u800c\u9762\u4e34\u7684\u90e8\u7f72\u548c\u4f7f\u7528\u969c\u788d\uff0c\u5e76\u5bfb\u627e\u6bd4\u4f20\u7edf\u7684\u7ed3\u6784\u5316\u4fee\u526a\u7b49\u65b9\u6cd5\u66f4\u4f18\u8d8a\u3001\u66f4\u4e0d\u6613\u9519\u8fc7\u66f4\u597d\u89e3\u51b3\u65b9\u6848\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002", "method": "GeLaCo\u662f\u4e00\u79cd\u901a\u8fc7\u5c42\u6298\u53e0\u8fdb\u884cLLM\u538b\u7f29\u7684\u8fdb\u5316\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u57fa\u4e8e\u79cd\u7fa4\u7684\u641c\u7d22\u548c\u6a21\u5757\u5316\u76f8\u4f3c\u6027\u9002\u5e94\u5ea6\u51fd\u6570\u6765\u63a2\u7d22\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\uff0c\u5e76\u652f\u6301\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u641c\u7d22\u3002", "result": "GeLaCo\u5728\u57fa\u4e8e\u56f0\u60d1\u5ea6\u548c\u751f\u6210\u6027\u8bc4\u4f30\u7684\u8bc4\u4f30\u4e2d\uff0c\u5728\u57fa\u7840\u6a21\u578b\u548c\u6307\u4ee4\u8c03\u6574\u6a21\u578b\u4e0a\u90fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "GeLaCo\u5728\u6a21\u578b\u538b\u7f29\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u66ff\u4ee3\u65b9\u6848\u7684\u6210\u679c\uff0c\u901a\u8fc7\u57fa\u4e8e\u79cd\u7fa4\u7684\u641c\u7d22\u548c\u6355\u6349\u6ce8\u610f\u529b\u3001\u524d\u9988\u548c\u9690\u85cf\u72b6\u6001\u8868\u793a\u7684\u6a21\u5757\u5316\u76f8\u4f3c\u6027\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u6709\u6548\u5730\u63a2\u7d22\u4e86\u538b\u7f29\u89e3\u51b3\u65b9\u6848\u7a7a\u95f4\u3002\u5b83\u8fd8\u652f\u6301\u5355\u76ee\u6807\u548c\u591a\u76ee\u6807\u8fdb\u5316\u538b\u7f29\u641c\u7d22\uff0c\u5728\u538b\u7f29\u548c\u8d28\u91cf\u8f74\u4e0a\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u5e15\u7d2f\u6258\u524d\u6cbf\u3002"}}
{"id": "2412.11407", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2412.11407", "abs": "https://arxiv.org/abs/2412.11407", "authors": ["TianZhu Liu", "BangYan Hu", "YanFeng Gu", "Xian Li", "Aleksandra Pi\u017eurica"], "title": "An Enhanced Classification Method Based on Adaptive Multi-Scale Fusion for Long-tailed Multispectral Point Clouds", "comment": "16 pages, 9 figures, 5 tables", "summary": "Multispectral point cloud (MPC) captures 3D spatial-spectral information from\nthe observed scene, which can be used for scene understanding and has a wide\nrange of applications. However, most of the existing classification methods\nwere extensively tested on indoor datasets, and when applied to outdoor\ndatasets they still face problems including sparse labeled targets, differences\nin land-covers scales, and long-tailed distributions. To address the above\nissues, an enhanced classification method based on adaptive multi-scale fusion\nfor MPCs with long-tailed distributions is proposed. In the training set\ngeneration stage, a grid-balanced sampling strategy is designed to reliably\ngenerate training samples from sparse labeled datasets. In the feature learning\nstage, a multi-scale feature fusion module is proposed to fuse shallow features\nof land-covers at different scales, addressing the issue of losing fine\nfeatures due to scale variations in land-covers. In the classification stage,\nan adaptive hybrid loss module is devised to utilize multi-classification heads\nwith adaptive weights to balance the learning ability of different classes,\nimproving the classification performance of small classes due to various-scales\nand long-tailed distributions in land-covers. Experimental results on three MPC\ndatasets demonstrate the effectiveness of the proposed method compared with the\nstate-of-the-art methods.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u5149\u8c31\u70b9\u4e91\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7f51\u683c\u5e73\u8861\u91c7\u6837\u3001\u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u548c\u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5ba4\u5916\u6570\u636e\u96c6\u7a00\u758f\u6807\u8bb0\u3001\u5c3a\u5ea6\u5dee\u5f02\u548c\u957f\u5c3e\u5206\u5e03\u7b49\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u5149\u8c31\u70b9\u4e91\u5206\u7c7b\u65b9\u6cd5\u4e3b\u8981\u5728\u5ba4\u5185\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\uff0c\u5728\u5e94\u7528\u4e8e\u5ba4\u5916\u6570\u636e\u96c6\u65f6\uff0c\u9762\u4e34\u6807\u8bb0\u76ee\u6807\u7a00\u758f\u3001\u5730\u7269\u5c3a\u5ea6\u5dee\u5f02\u5927\u4ee5\u53ca\u957f\u5c3e\u5206\u5e03\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408\u7684\u589e\u5f3a\u5206\u7c7b\u65b9\u6cd5\uff0c\u7528\u4e8e\u5904\u7406\u5177\u6709\u957f\u5c3e\u5206\u5e03\u7684\u591a\u5149\u8c31\u70b9\u4e91\u3002\u8be5\u65b9\u6cd5\u5305\u62ec\u4e09\u4e2a\u9636\u6bb5\uff1a1. \u7f51\u683c\u5e73\u8861\u91c7\u6837\u7b56\u7565\uff1a\u7528\u4e8e\u4ece\u7a00\u758f\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e2d\u53ef\u9760\u5730\u751f\u6210\u8bad\u7ec3\u6837\u672c\u30022. \u591a\u5c3a\u5ea6\u7279\u5f81\u878d\u5408\u6a21\u5757\uff1a\u7528\u4e8e\u878d\u5408\u4e0d\u540c\u5c3a\u5ea6\u7684\u5730\u7269\u6d45\u5c42\u7279\u5f81\uff0c\u89e3\u51b3\u56e0\u5730\u7269\u5c3a\u5ea6\u53d8\u5316\u5bfc\u81f4\u7684\u7ec6\u7c92\u5ea6\u7279\u5f81\u4e22\u5931\u95ee\u9898\u30023. \u81ea\u9002\u5e94\u6df7\u5408\u635f\u5931\u6a21\u5757\uff1a\u5229\u7528\u5177\u6709\u81ea\u9002\u5e94\u6743\u91cd\u7684\u591a\u5206\u7c7b\u5934\u6765\u5e73\u8861\u4e0d\u540c\u7c7b\u522b\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u4ece\u800c\u63d0\u9ad8\u5c0f\u7c7b\u522b\u5728\u5404\u79cd\u5c3a\u5ea6\u548c\u957f\u5c3e\u5206\u5e03\u4e0b\u7684\u5206\u7c7b\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u591a\u5149\u8c31\u70b9\u4e91\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u591a\u5149\u8c31\u70b9\u4e91\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u5176\u76f8\u6bd4\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09313", "abs": "https://arxiv.org/abs/2507.09313", "authors": ["Yueqian Wang", "Xiaojun Meng", "Yifan Wang", "Huishuai Zhang", "Dongyan Zhao"], "title": "ProactiveBench: A Comprehensive Benchmark Evaluating Proactive Interactions in Video Large Language Models", "comment": null, "summary": "With the growing research focus on multimodal dialogue systems, the\ncapability for proactive interaction is gradually gaining recognition. As an\nalternative to conventional turn-by-turn dialogue, users increasingly expect\nmultimodal systems to be more initiative, for example, by autonomously\ndetermining the timing of multi-turn responses in real time during video\nplayback. To facilitate progress in this emerging area, we introduce\nProactiveBench, the first comprehensive benchmark to evaluate a system's\nability to engage in proactive interaction. Since model responses are generated\nat varying timestamps, we further propose PAUC, the first metric that accounts\nfor the temporal dynamics of model responses. This enables a more accurate\nevaluation of systems operating in proactive settings. Through extensive\nbenchmarking of various baseline systems on ProactiveBench and a user study of\nhuman preferences, we show that PAUC is in better agreement with human\npreferences than traditional evaluation metrics, which typically only consider\nthe textual content of responses. These findings demonstrate that PAUC provides\na more faithful assessment of user experience in proactive interaction\nscenarios. Project homepage:\nhttps://github.com/yellow-binary-tree/ProactiveBench", "AI": {"tldr": "\u63d0\u51faProactiveBench\u57fa\u51c6\u6d4b\u8bd5\u548cPAUC\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u8861\u91cf\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7684\u041f\u0440\u043e\u0430\u043a\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c\u3002", "motivation": "\u968f\u7740\u591a\u6a21\u6001\u5bf9\u8bdd\u7cfb\u7edf\u7814\u7a76\u7684\u91cd\u70b9\u65e5\u76ca\u589e\u957f\uff0c\u4e3b\u52a8\u4ea4\u4e92\u7684\u80fd\u529b\u9010\u6e10\u53d7\u5230\u8ba4\u53ef\u3002\u7528\u6237\u671f\u671b\u591a\u6a21\u6001\u7cfb\u7edf\u80fd\u591f\u66f4\u52a0\u4e3b\u52a8\uff0c\u4f8b\u5982\u5728\u89c6\u9891\u64ad\u653e\u8fc7\u7a0b\u4e2d\u81ea\u4e3b\u786e\u5b9a\u591a\u8f6e\u54cd\u5e94\u7684\u65f6\u673a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPAUC\u7684\u65b0\u6307\u6807\uff0c\u8be5\u6307\u6807\u8003\u8651\u4e86\u6a21\u578b\u54cd\u5e94\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u80fd\u591f\u66f4\u51c6\u786e\u5730\u8bc4\u4f30\u4e3b\u52a8\u4ea4\u4e92\u8bbe\u7f6e\u4e0b\u7684\u7cfb\u7edf\u6027\u80fd\u3002\u901a\u8fc7\u5728ProactiveBench\u4e0a\u5bf9\u5404\u79cd\u57fa\u7ebf\u7cfb\u7edf\u8fdb\u884c\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u7528\u6237\u7814\u7a76\uff0c\u8bc1\u660e\u4e86PAUC\u4e0e\u7528\u6237\u504f\u597d\u7684_\u66f4\u597d_\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5f00\u53d1\u4e86\u9996\u4e2a\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5ProactiveBench\uff0c\u7528\u4e8e\u8bc4\u4f30\u7cfb\u7edf\u5728\u4e3b\u52a8\u4ea4\u4e92\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u9996\u4e2a\u80fd\u8003\u8651\u6a21\u578b\u54cd\u5e94\u65f6\u95f4\u52a8\u6001\u7684\u65b0\u6307\u6807PAUC\u3002\u5b9e\u9a8c\u8868\u660e\uff0cPAUC\u6bd4\u4f20\u7edf\u6307\u6807\u66f4\u80fd\u53cd\u6620\u7528\u6237\u504f\u597d\u3002", "conclusion": "PAUC\u63d0\u4f9b\u4e86\u5bf9\u4e3b\u52a8\u4ea4\u4e92\u573a\u666f\u4e0b\u7528\u6237\u4f53\u9a8c\u66f4\u5fe0\u5b9e\u7684\u8bc4\u4f30\uff0c\u4e0e\u4ec5\u8003\u8651\u54cd\u5e94\u6587\u672c\u5185\u5bb9\u7684\u4f20\u7edf\u8bc4\u4f30\u6307\u6807\u76f8\u6bd4\uff0cPAUC\u66f4\u80fd\u4e0e\u7528\u6237\u504f\u597d\u8fbe\u6210\u4e00\u81f4\u3002"}}
{"id": "2507.10073", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10073", "abs": "https://arxiv.org/abs/2507.10073", "authors": ["Simon M\u00fcnker"], "title": "Cultural Bias in Large Language Models: Evaluating AI Agents through Moral Questionnaires", "comment": "15pages, 1 figure, 2 tables", "summary": "Are AI systems truly representing human values, or merely averaging across\nthem? Our study suggests a concerning reality: Large Language Models (LLMs)\nfail to represent diverse cultural moral frameworks despite their linguistic\ncapabilities. We expose significant gaps between AI-generated and human moral\nintuitions by applying the Moral Foundations Questionnaire across 19 cultural\ncontexts. Comparing multiple state-of-the-art LLMs' origins against human\nbaseline data, we find these models systematically homogenize moral diversity.\nSurprisingly, increased model size doesn't consistently improve cultural\nrepresentation fidelity. Our findings challenge the growing use of LLMs as\nsynthetic populations in social science research and highlight a fundamental\nlimitation in current AI alignment approaches. Without data-driven alignment\nbeyond prompting, these systems cannot capture the nuanced, culturally-specific\nmoral intuitions. Our results call for more grounded alignment objectives and\nevaluation metrics to ensure AI systems represent diverse human values rather\nthan flattening the moral landscape.", "AI": {"tldr": "LLMs\u672a\u80fd\u4ee3\u8868\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u800c\u662f\u5e73\u5747\u4e86\u5b83\u4eec\uff0c\u7cfb\u7edf\u6027\u5730\u4f7f\u9053\u5fb7\u591a\u6837\u6027\u8d8b\u4e8e\u540c\u8d28\u5316\u3002", "motivation": "\u63a2\u7d22AI\u7cfb\u7edf\u662f\u5426\u771f\u6b63\u4ee3\u8868\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u8fd8\u662f\u4ec5\u4ec5\u5e73\u5747\u4e86\u5b83\u4eec\u3002", "method": "\u901a\u8fc7\u572819\u4e2a\u6587\u5316\u80cc\u666f\u4e0b\u5e94\u7528\u9053\u5fb7\u57fa\u7840\u95ee\u5377\uff0c\u5c06\u6700\u5148\u8fdb\u7684LLMs\u7684\u8d77\u6e90\u4e0e\u4eba\u7c7b\u57fa\u7ebf\u6570\u636e\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "LLMs\u672a\u80fd\u4ee3\u8868\u5177\u6709\u591a\u79cd\u6587\u5316\u80cc\u666f\u7684\u9053\u5fb7\u6846\u67b6\uff0c\u5e76\u4e14\u6a21\u578b\u89c4\u6a21\u7684\u589e\u52a0\u5e76\u672a\u6301\u7eed\u63d0\u9ad8\u6587\u5316\u4ee3\u8868\u6027\u7684\u4fdd\u771f\u5ea6\u3002", "conclusion": "LLMs\u672a\u80fd\u4ee3\u8868\u4eba\u7c7b\u4ef7\u503c\u89c2\uff0c\u800c\u53ea\u662f\u5e73\u5747\u4e86\u5b83\u4eec\uff0c\u5e76\u4e14LLMs\u7cfb\u7edf\u6027\u5730\u4f7f\u9053\u5fb7\u591a\u6837\u6027\u8d8b\u4e8e\u540c\u8d28\u5316\u3002\u76ee\u524d\u7684AI\u5bf9\u9f50\u65b9\u6cd5\u5b58\u5728\u6839\u672c\u6027\u5c40\u9650\u6027\uff0c\u9700\u8981\u66f4\u591a\u57fa\u4e8e\u6570\u636e\u7684\u5bf9\u9f50\u800c\u975e\u63d0\u793a\u3002"}}
{"id": "2507.09095", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09095", "abs": "https://arxiv.org/abs/2507.09095", "authors": ["Md Hasan Shahriar", "Md Mohaimin Al Barat", "Harshavardhan Sundar", "Naren Ramakrishnan", "Y. Thomas Hou", "Wenjing Lou"], "title": "On the Fragility of Multimodal Perception to Temporal Misalignment in Autonomous Driving", "comment": "16 pages", "summary": "Multimodal fusion (MMF) plays a critical role in the perception of autonomous\ndriving, which primarily fuses camera and LiDAR streams for a comprehensive and\nefficient scene understanding. However, its strict reliance on precise temporal\nsynchronization exposes it to new vulnerabilities. In this paper, we introduce\nDejaVu, a novel attack that exploits network-induced delays to create subtle\ntemporal misalignments across sensor streams, severely degrading downstream\nMMF-based perception tasks. Our comprehensive attack analysis across different\nmodels and datasets reveals these sensors' task-specific imbalanced\nsensitivities: object detection is overly dependent on LiDAR inputs while\nobject tracking is highly reliant on the camera inputs. Consequently, with a\nsingle-frame LiDAR delay, an attacker can reduce the car detection mAP by up to\n88.5%, while with a three-frame camera delay, multiple object tracking accuracy\n(MOTA) for car drops by 73%. To detect such attacks, we propose AION, a defense\npatch that can work alongside the existing perception model to monitor temporal\nalignment through cross-modal temporal consistency. AION leverages multimodal\nshared representation learning and dynamic time warping to determine the path\nof temporal alignment and calculate anomaly scores based on the alignment. Our\nthorough evaluation of AION shows it achieves AUROC scores of 0.92-0.98 with\nlow false positives across datasets and model architectures, demonstrating it\nas a robust and generalized defense against the temporal misalignment attacks.", "AI": {"tldr": "\u901a\u8fc7\u5f15\u5165\u7f51\u7edc\u5ef6\u8fdf\u5bfc\u81f4\u7684\u65f6\u95f4\u9519\u4f4d\u6765\u653b\u51fb\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6a21\u6001\u878d\u5408\u611f\u77e5\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u95f4\u4e00\u81f4\u6027\u76d1\u63a7\u7684\u9632\u5fa1\u65b9\u6cd5\u3002", "motivation": "\u81ea\u52a8\u9a7e\u9a76\u4e2d\u7684\u591a\u6a21\u6001\u878d\u5408\uff08MMF\uff09\u867d\u7136\u80fd\u63d0\u5347\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u5176\u5bf9\u7cbe\u786e\u65f6\u95f4\u540c\u6b65\u7684\u4f9d\u8d56\u6027\u4e5f\u5e26\u6765\u4e86\u65b0\u7684\u5b89\u5168\u6f0f\u6d1e\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5982\u4f55\u5229\u7528\u65f6\u95f4\u9519\u4f4d\u653b\u51fbMMF\u611f\u77e5\u7cfb\u7edf\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u9632\u5fa1\u63aa\u65bd\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86DejaVu\u653b\u51fb\uff0c\u901a\u8fc7\u5f15\u5165\u7f51\u7edc\u5ef6\u8fdf\u6765\u5236\u9020\u4f20\u611f\u5668\u6570\u636e\u7684\u65f6\u95f4\u9519\u4f4d\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9\u4e0b\u6e38\u591a\u6a21\u6001\u878d\u5408\u611f\u77e5\u4efb\u52a1\u7684\u5f71\u54cd\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u540d\u4e3aAION\u7684\u9632\u5fa1\u673a\u5236\uff0c\u5229\u7528\u8de8\u6a21\u6001\u5171\u4eab\u8868\u793a\u5b66\u4e60\u548c\u52a8\u6001\u65f6\u95f4\u89c4\u6574\u6765\u76d1\u63a7\u65f6\u95f4\u5bf9\u9f50\uff0c\u5e76\u8ba1\u7b97\u5f02\u5e38\u5f97\u5206\u3002", "result": "DejaVu\u653b\u51fb\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8f66\u8f86\u68c0\u6d4b\u7684mAP\uff08\u6700\u9ad8\u53ef\u8fbe88.5%\uff09\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u7684MOTA\uff08\u6700\u9ad8\u53ef\u8fbe73%\uff09\uff0c\u5177\u4f53\u5f71\u54cd\u53d6\u51b3\u4e8e\u4f20\u611f\u5668\u548c\u4efb\u52a1\u7684\u4f9d\u8d56\u6027\u3002AION\u9632\u5fa1\u673a\u5236\u5728\u4e0d\u540c\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e860.92-0.98\u7684AUROC\u5f97\u5206\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "DejaVu\u653b\u51fb\u5229\u7528\u7f51\u7edc\u5ef6\u8fdf\u5bfc\u81f4\u7684\u65f6\u95f4\u9519\u4f4d\u4f1a\u4e25\u91cd\u635f\u5bb3\u591a\u6a21\u6001\u878d\u5408\u611f\u77e5\u4efb\u52a1\uff0c\u4f46AION\u9632\u5fa1\u53ef\u4ee5\u5728\u8de8\u6a21\u6001\u65f6\u95f4\u4e00\u81f4\u6027\u76d1\u63a7\u4e0b\u7f13\u89e3\u6b64\u95ee\u9898\uff0c\u5728\u5404\u79cd\u6a21\u578b\u548c\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u9ad8\u68c0\u6d4b\u7387\u548c\u4f4e\u8bef\u62a5\u7387\u3002"}}
{"id": "2507.07855", "categories": ["cs.LG", "cs.AI", "cs.CL", "I.2.6; I.2.7"], "pdf": "https://arxiv.org/pdf/2507.07855", "abs": "https://arxiv.org/abs/2507.07855", "authors": ["Wenxuan Zhou", "Shujian Zhang", "Brice Magdalou", "John Lambert", "Ehsan Amid", "Richard Nock", "Andrew Hard"], "title": "Principled Foundations for Preference Optimization", "comment": null, "summary": "In this paper, we show that direct preference optimization (DPO) is a very\nspecific form of a connection between two major theories in the ML context of\nlearning from preferences: loss functions (Savage) and stochastic choice\n(Doignon-Falmagne and Machina). The connection is established for all of\nSavage's losses and at this level of generality, (i) it includes support for\nabstention on the choice theory side, (ii) it includes support for non-convex\nobjectives on the ML side, and (iii) it allows to frame for free some notable\nextensions of the DPO setting, including margins and corrections for length.\nGetting to understand how DPO operates from a general principled perspective is\ncrucial because of the huge and diverse application landscape of models,\nbecause of the current momentum around DPO, but also -- and importantly --\nbecause many state of the art variations on DPO definitely occupy a small\nregion of the map that we cover. It also helps to understand the pitfalls of\ndeparting from this map, and figure out workarounds.", "AI": {"tldr": "DPO\u662fSavage\u635f\u5931\u51fd\u6570\u548cDoignon-Falmagne-Machina\u968f\u673a\u9009\u62e9\u7406\u8bba\u4e4b\u95f4\u7684\u7279\u5b9a\u8054\u7cfb\uff0c\u652f\u6301\u5f03\u6743\u3001\u975e\u51f8\u76ee\u6807\u548cDPO\u7684\u6269\u5c55\u3002\u7406\u89e3DPO\u7684\u539f\u7406\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u7406\u89e3DPO\u7684\u8fd0\u4f5c\u65b9\u5f0f\u5bf9\u4e8e\u5176\u5e7f\u6cdb\u7684\u5e94\u7528\u3001\u5f53\u524d\u7684\u7814\u7a76\u70ed\u5ea6\u548c\u8bb8\u591a\u73b0\u6709DPO\u53d8\u4f53\u90fd\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u4e5f\u6709\u52a9\u4e8e\u7406\u89e3\u504f\u79bb\u5176\u7406\u8bba\u6846\u67b6\u7684\u6f5c\u5728\u95ee\u9898\u53ca\u89e3\u51b3\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u5c06\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u4e0e\u673a\u5668\u5b66\u4e60\u4e2d\u7684\u635f\u5931\u51fd\u6570\uff08Savage\uff09\u548c\u968f\u673a\u9009\u62e9\uff08Doignon-Falmagne\u548cMachina\uff09\u7406\u8bba\u8054\u7cfb\u8d77\u6765\uff0c\u63ed\u793a\u4e86DPO\u662f\u8fd9\u4e24\u79cd\u7406\u8bba\u4e4b\u95f4\u7684\u4e00\u79cd\u7279\u5b9a\u5f62\u5f0f\u7684\u5173\u8054\u3002", "result": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e86DPO\u4e0eSavage\u635f\u5931\u51fd\u6570\u4ee5\u53caDoignon-Falmagne\u548cMachina\u968f\u673a\u9009\u62e9\u7406\u8bba\u4e4b\u95f4\u7684\u4e00\u822c\u6027\u8054\u7cfb\uff0c\u652f\u6301\u5f03\u6743\u9009\u62e9\u3001\u975e\u51f8\u76ee\u6807\u51fd\u6570\uff0c\u5e76\u80fd\u514d\u8d39\u7eb3\u5165\u4fdd\u8bc1\u91d1\u548c\u957f\u5ea6\u4fee\u6b63\u7b49DPO\u6269\u5c55\u3002", "conclusion": "DPO\u662f\u4e00\u79cd\u4ecb\u4e8eSavage\u635f\u5931\u51fd\u6570\u548cDoignon-Falmagne-Machina\u968f\u673a\u9009\u62e9\u7406\u8bba\u4e4b\u95f4\u7684\u7279\u5b9a\u8054\u7cfb\uff0c\u652f\u6301\u5f03\u6743\u3001\u975e\u51f8\u76ee\u6807\u548cDPO\u7684\u6269\u5c55\uff08\u5982\u4fdd\u8bc1\u91d1\u548c\u957f\u5ea6\u4fee\u6b63\uff09\u3002\u7406\u89e3DPO\u7684\u539f\u7406\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u7684\u5e94\u7528\u5e7f\u6cdb\uff0c\u5e76\u4e14\u8bb8\u591aDPO\u7684\u53d8\u4f53\u90fd\u4f4d\u4e8e\u6211\u4eec\u6240\u8986\u76d6\u7684\u7406\u8bba\u6846\u67b6\u5185\uff0c\u8fd9\u6709\u52a9\u4e8e\u907f\u514d\u9519\u8bef\u5e76\u627e\u5230\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09323", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09323", "abs": "https://arxiv.org/abs/2507.09323", "authors": ["Kaixuan Cong", "Yifan Wang", "Rongkun Xue", "Yuyang Jiang", "Yiming Feng", "Jing Yang"], "title": "Dynamic Inter-Class Confusion-Aware Encoder for Audio-Visual Fusion in Human Activity Recognition", "comment": null, "summary": "Humans do not understand individual events in isolation; rather, they\ngeneralize concepts within classes and compare them to others. Existing\naudio-video pre-training paradigms only focus on the alignment of the overall\naudio-video modalities, without considering the reinforcement of distinguishing\neasily confused classes through cognitive induction and contrast during\ntraining. This paper proposes the Dynamic Inter-Class Confusion-Aware Encoder\n(DICCAE), an encoder that aligns audio-video representations at a fine-grained,\ncategory-level. DICCAE addresses category confusion by dynamically adjusting\nthe confusion loss based on inter-class confusion degrees, thereby enhancing\nthe model's ability to distinguish between similar activities. To further\nextend the application of DICCAE, we also introduce a novel training framework\nthat incorporates both audio and video modalities, as well as their fusion. To\nmitigate the scarcity of audio-video data in the human activity recognition\ntask, we propose a cluster-guided audio-video self-supervised pre-training\nstrategy for DICCAE. DICCAE achieves near state-of-the-art performance on the\nVGGSound dataset, with a top-1 accuracy of 65.5%. We further evaluate its\nfeature representation quality through extensive ablation studies, validating\nthe necessity of each module.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09988", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2507.09988", "abs": "https://arxiv.org/abs/2507.09988", "authors": ["Petr O. Jedli\u010dka", "\u0160imon Kos", "Martin \u0160m\u00edd", "Ji\u0159\u00ed Vomlel", "Jan Slav\u00edk"], "title": "Has Anything Changed? Tracking Long-Term Interpretational Preferences in Quantum Mechanics", "comment": null, "summary": "As we approach the centennial anniversary of modern quantum mechanics this\npaper revisits the foundational debates through a new poll within the research\ncommunity. Inspired by the survey by Schlosshauer, Kofler, and Zeilinger at the\nspecialized 2011 Quantum Physics and the Nature of Reality conference, we\nexpanded our recruitment to include a more representative sample of the broader\ncommunity of physicists with the aim to reveal potential shifts in scientists'\nviews and compare our findings with those from several previous polls. While\nquantum foundations still lack a consensus interpretation, our results indicate\na persistent preference for the Copenhagen interpretation. This enduring\nsupport likely reflects both the educational emphasis on the Copenhagen\ninterpretation and its pragmatic appeal in avoiding complex metaphysical\nquestions and introducing new notions (e.g., other worlds or the pilot wave).\nOur findings thus underscore the relative stability of interpretational\npreferences over the past decades.", "AI": {"tldr": "\u4e00\u9879\u9488\u5bf9\u7269\u7406\u5b66\u5bb6\u7684\u8c03\u67e5\u663e\u793a\uff0c\u5c3d\u7ba1\u91cf\u5b50\u529b\u5b66\u5b58\u5728\u591a\u79cd\u89e3\u91ca\uff0c\u4f46\u54e5\u672c\u54c8\u6839\u89e3\u91ca\u4ecd\u7136\u6700\u53d7\u6b22\u8fce\uff0c\u8fd9\u53ef\u80fd\u5f52\u56e0\u4e8e\u5176\u6559\u80b2\u4e0a\u7684\u666e\u53ca\u6027\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u91cf\u5b50\u529b\u5b66\u767e\u5e74\u7eaa\u5ff5\u4e4b\u9645\uff0c\u901a\u8fc7\u65b0\u7684\u7814\u7a76\u63ed\u793a\u7814\u7a76\u754c\u5bf9\u91cf\u5b50\u529b\u5b66\u57fa\u7840\u6027\u95ee\u9898\u7684\u770b\u6cd5\uff0c\u5e76\u4e0e\u4ee5\u5f80\u7684\u8c03\u67e5\u7ed3\u679c\u8fdb\u884c\u5bf9\u6bd4\u3002", "method": "\u901a\u8fc7\u5bf9\u66f4\u5e7f\u6cdb\u7684\u7269\u7406\u5b66\u5bb6\u7fa4\u4f53\u8fdb\u884c\u8c03\u67e5\uff0c\u5e76\u4e0e\u4ee5\u5f80\u7684\u8c03\u67e5\u7ed3\u679c\u8fdb\u884c\u6bd4\u8f83\uff0c\u6765\u63ed\u793a\u79d1\u5b66\u5bb6\u89c2\u70b9\u53ef\u80fd\u7684\u53d8\u5316\u3002", "result": "\u5927\u591a\u6570\u53d7\u8bbf\u8005\u4ecd\u7136\u504f\u7231\u54e5\u672c\u54c8\u6839\u89e3\u91ca\uff0c\u8fd9\u8868\u660e\u5728\u8fc7\u53bb\u51e0\u5341\u5e74\u4e2d\uff0c\u79d1\u5b66\u754c\u5bf9\u91cf\u5b50\u529b\u5b66\u89e3\u91ca\u7684\u504f\u597d\u4fdd\u6301\u4e86\u76f8\u5bf9\u7a33\u5b9a\u3002", "conclusion": "\u5c3d\u7ba1\u91cf\u5b50\u529b\u5b66\u57fa\u7840\u4ecd\u7f3a\u4e4f\u5171\u8bc6\u6027\u7684\u89e3\u91ca\uff0c\u4f46\u54e5\u672c\u54c8\u6839\u89e3\u91ca\u4ecd\u7136\u662f\u9996\u9009\uff0c\u8fd9\u53ef\u80fd\u53cd\u6620\u4e86\u5176\u5728\u6559\u80b2\u4e2d\u7684\u91cd\u8981\u6027\u4ee5\u53ca\u5728\u907f\u514d\u5f62\u800c\u4e0a\u5b66\u95ee\u9898\u548c\u5f15\u5165\u65b0\u6982\u5ff5\u65b9\u9762\u7684\u5b9e\u7528\u6027\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u8fc7\u53bb\u51e0\u5341\u5e74\u4e2d\uff0c\u5bf9\u91cf\u5b50\u529b\u5b66\u89e3\u91ca\u7684\u504f\u597d\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\u3002"}}
{"id": "2507.10085", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10085", "abs": "https://arxiv.org/abs/2507.10085", "authors": ["Chenxi Huang", "Shaotian Yan", "Liang Xie", "Binbin Lin", "Sinan Fan", "Yue Xin", "Deng Cai", "Chen Shen", "Jieping Ye"], "title": "Enhancing Chain-of-Thought Reasoning with Critical Representation Fine-tuning", "comment": "Accepted by ACL 2025", "summary": "Representation Fine-tuning (ReFT), a recently proposed Parameter-Efficient\nFine-Tuning (PEFT) method, has attracted widespread attention for significantly\nimproving parameter efficiency by editing representation space alone. In this\nwork, we investigate applying ReFT to complex reasoning tasks. However,\ndirectly using the native ReFT method, which modifies fixed representations at\nthe beginning and end of each layer, yields suboptimal performance, as these\nfixed-position representations have uncertain impact on the outputs. We observe\nthat, in complex reasoning tasks, there often exist certain critical\nrepresentations. These representations either integrate significant information\nfrom preceding layers or regulate subsequent layer representations. Through\nlayer-by-layer propagation, they exert a substantial influence on the final\noutput. Naturally, fine-tuning these critical representations has the potential\nto greatly enhance reasoning performance. Building upon these insights, we\npropose Critical Representation Fine-Tuning (CRFT), a novel method that\nidentifies and optimizes these critical representations through information\nflow analysis. CRFT operates within a supervised learning framework,\ndynamically optimizing critical representations in a low-rank linear subspace\nwhile freezing the base model. The effectiveness and efficiency of our method\nare validated across eight benchmarks for arithmetic and commonsense reasoning,\nusing LLaMA and Mistral model families. Furthermore, our method also adapts\neffectively to few-shot settings, boosting one-shot accuracy by 16.4%. Our work\nhighlights the untapped potential of representation-level optimization for CoT\nreasoning, offering a lightweight yet powerful alternative to traditional PEFT\nmethods.", "AI": {"tldr": "CRFT \u662f\u4e00\u79cd\u65b0\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u5173\u952e\u8868\u793a\u6765\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u76f8\u6bd4 ReFT \u5728\u56fa\u5b9a\u4f4d\u7f6e\u8868\u793a\u4e0a\u8fdb\u884c\u4e86\u6539\u8fdb\u3002", "motivation": "\u76f4\u63a5\u4f7f\u7528 ReFT \u65b9\u6cd5\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u56e0\u4e3a\u5176\u4fee\u6539\u56fa\u5b9a\u4f4d\u7f6e\u7684\u8868\u793a\uff0c\u5176\u5bf9\u8f93\u51fa\u7684\u5f71\u54cd\u4e0d\u786e\u5b9a\u3002\u4f5c\u8005\u89c2\u5bdf\u5230\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u5b58\u5728\u4e00\u4e9b\u5173\u952e\u7684\u8868\u793a\uff0c\u5b83\u4eec\u6574\u5408\u4e86\u5148\u524d\u5c42\u7684\u91cd\u8981\u4fe1\u606f\u6216\u8c03\u8282\u4e86\u540e\u7eed\u5c42\u7684\u8868\u793a\uff0c\u5bf9\u6700\u7ec8\u8f93\u51fa\u6709\u91cd\u5927\u5f71\u54cd\u3002", "method": "CRFT \u901a\u8fc7\u4fe1\u606f\u6d41\u5206\u6790\u8bc6\u522b\u5173\u952e\u8868\u793a\uff0c\u5e76\u5728\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u4e0b\u8fdb\u884c\u4f18\u5316\uff0c\u540c\u65f6\u4fdd\u6301\u57fa\u7840\u6a21\u578b\u51bb\u7ed3\u3002", "result": "CRFT \u5728\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u6db5\u76d6\u4e86\u7b97\u672f\u548c\u5e38\u8bc6\u63a8\u7406\uff0c\u5e76\u4f7f\u7528\u4e86 LLaMA \u548c Mistral \u6a21\u578b\u7cfb\u5217\u3002\u8be5\u65b9\u6cd5\u5728\u5355\u6b21\u5b66\u4e60\u8bbe\u7f6e\u4e0b\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u5c06\u51c6\u786e\u7387\u63d0\u9ad8\u4e86 16.4%\u3002", "conclusion": "Critical Representation Fine-Tuning (CRFT) \u901a\u8fc7\u8bc6\u522b\u548c\u4f18\u5316\u5173\u952e\u8868\u793a\u6765\u63d0\u9ad8\u590d\u6742\u63a8\u7406\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e0e\u4f20\u7edf\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\u65b9\u6cd5\u76f8\u6bd4\uff0c\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f46\u5f3a\u5927\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.09101", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09101", "abs": "https://arxiv.org/abs/2507.09101", "authors": ["Yanan Cao", "Omid Memarrast", "Shiqin Cai", "Sinduja Subramaniam", "Evren Korpeoglu", "Kannan Achan"], "title": "S2SRec2: Set-to-Set Recommendation for Basket Completion with Recipe", "comment": null, "summary": "In grocery e-commerce, customers often build ingredient baskets guided by\ndietary preferences but lack the expertise to create complete meals. Leveraging\nrecipe knowledge to recommend complementary ingredients based on a partial\nbasket is essential for improving the culinary experience. Traditional recipe\ncompletion methods typically predict a single missing ingredient using a\nleave-one-out strategy. However, they fall short in two key aspects: (i) they\ndo not reflect real-world scenarios where multiple ingredients are often\nneeded, and (ii) they overlook relationships among the missing ingredients\nthemselves. To address these limitations, we reformulate basket completion as a\nset-to-set (S2S) recommendation problem, where an incomplete basket is input\ninto a system that predicts a set of complementary ingredients. We introduce\nS2SRec2, a set-to-set ingredient recommendation framework based on a Set\nTransformer and trained in a multitask learning paradigm. S2SRec2 jointly\nlearns to (i) retrieve missing ingredients from the representation of existing\nones and (ii) assess basket completeness after prediction. These tasks are\noptimized together, enforcing accurate retrieval and coherent basket\ncompletion. Experiments on large-scale recipe datasets and qualitative analyses\nshow that S2SRec2 significantly outperforms single-target baselines, offering a\npromising approach to enhance grocery shopping and inspire culinary creativity.", "AI": {"tldr": "Recommends sets of ingredients for incomplete grocery baskets using a Set Transformer to improve meal completion and culinary inspiration.", "motivation": "Traditional recipe completion methods fail to reflect real-world scenarios where multiple ingredients are needed and overlook relationships among missing ingredients. The goal is to improve the culinary experience by recommending complementary ingredients based on a partial basket.", "method": "Reformulated basket completion as a set-to-set (S2S) recommendation problem, introducing S2SRec2, a framework based on a Set Transformer trained in a multitask learning paradigm. S2SRec2 jointly learns to retrieve missing ingredients and assess basket completeness.", "result": "Experiments on large-scale recipe datasets and qualitative analyses show that S2SRec2 significantly outperforms single-target baselines.", "conclusion": "S2SRec2 is a promising approach to enhance grocery shopping and inspire culinary creativity by significantly outperforming single-target baselines."}}
{"id": "2507.09334", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09334", "abs": "https://arxiv.org/abs/2507.09334", "authors": ["Wencan Huang", "Daizong Liu", "Wei Hu"], "title": "Fast3D: Accelerating 3D Multi-modal Large Language Models for Efficient 3D Scene Understanding", "comment": "Accepted by ACM MM 2025", "summary": "While 3D Multi-modal Large Language Models (MLLMs) demonstrate remarkable\nscene understanding capabilities, their practical deployment faces critical\nchallenges due to computational inefficiency. The key bottleneck stems from\nprocessing excessive object-centric visual tokens required for comprehensive 3D\nscene representation. Although visual token pruning has shown promise in\naccelerating 2D MLLMs, its applicability to 3D domains remains largely\nunexplored due to fundamental disparities in token structures. In this paper,\nwe reveal two critical insights: (1) Significant redundancy exists in\nobject-level 3D token representations, analogous to patch-level redundancy in\n2D systems; (2) Global attention patterns exhibit strong predictive power for\nidentifying non-essential tokens in 3D contexts. Building on these\nobservations, we propose Fast3D, a plug-and-play visual token pruning framework\nfor 3D MLLMs featuring two technical innovations: (1) Global Attention\nPrediction (GAP), where a lightweight neural network learns to predict the\nglobal attention distributions of the target model, enabling efficient token\nimportance estimation for precise pruning guidance; (2) Sample-Adaptive visual\ntoken Pruning (SAP), which introduces dynamic token budgets through\nattention-based complexity assessment, automatically adjusting layer-wise\npruning ratios based on input characteristics. Both of these two techniques\noperate without modifying the parameters of the target model. Extensive\nevaluations across five benchmarks validate the effectiveness of Fast3D,\nparticularly under high visual token pruning ratios. Code is available at\nhttps://github.com/wencan25/Fast3D", "AI": {"tldr": "Fast3D\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u6846\u67b6\uff0c\u901a\u8fc7GAP\u548cSAP\u6280\u672f\uff0c\u4e3a3D MLLMs\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u6548\u7387\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u76843D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u573a\u666f\u7406\u89e3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u74f6\u9888\u5728\u4e8e\u5904\u7406\u7528\u4e8e3D\u573a\u666f\u8868\u793a\u7684\u8fc7\u91cf\u7269\u4f53\u7ea7\u89c6\u89c9\u6807\u8bb0\u3002\u5c3d\u7ba12D MLLMs\u4e2d\u7684\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\u6280\u672f\u6709\u4e00\u5b9a\u6548\u679c\uff0c\u4f46\u7531\u4e8e\u6807\u8bb0\u7ed3\u6784\u5dee\u5f02\uff0c\u5176\u57283D\u9886\u57df\u7684\u5e94\u7528\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\u6846\u67b6Fast3D\uff0c\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6280\u672f\uff1a1.\u5168\u5c40\u6ce8\u610f\u529b\u9884\u6d4b\uff08GAP\uff09\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u76ee\u6807\u6a21\u578b\u7684\u5168\u5c40\u6ce8\u610f\u529b\u5206\u5e03\uff0c\u5b9e\u73b0\u9ad8\u6548\u7684\u6807\u8bb0\u91cd\u8981\u6027\u4f30\u8ba1\uff1b2.\u6837\u672c\u81ea\u9002\u5e94\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\uff08SAP\uff09\uff0c\u901a\u8fc7\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u590d\u6742\u5ea6\u8bc4\u4f30\u5f15\u5165\u52a8\u6001\u6807\u8bb0\u9884\u7b97\uff0c\u81ea\u52a8\u8c03\u6574\u5c42\u7ea7\u4fee\u526a\u7387\u3002", "result": "\u901a\u8fc7\u5728\u4e94\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86Fast3D\u6846\u67b6\u7684\u6709\u6548\u6027\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\u7387\u4e0b\uff0c\u80fd\u591f\u663e\u8457\u52a0\u901f\u6a21\u578b\u8fd0\u884c\u5e76\u4fdd\u6301\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Fast3D\u6846\u67b6\u901a\u8fc7\u5168\u5c40\u6ce8\u610f\u529b\u9884\u6d4b\uff08GAP\uff09\u548c\u6837\u672c\u81ea\u9002\u5e94\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\uff08SAP\uff09\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u8ba1\u7b97\u6548\u7387\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u4fee\u6539\u76ee\u6807\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u6839\u636e\u8f93\u5165\u7279\u5f81\u52a8\u6001\u8c03\u6574\u6807\u8bb0\u4fee\u526a\u7387\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u89c6\u89c9\u6807\u8bb0\u4fee\u526a\u7387\u4e0b\u8868\u73b0\u4f18\u5f02\u3002"}}
{"id": "2507.10027", "categories": ["quant-ph", "math-ph", "math.FA", "math.MP", "Primary: 81P16, Secondary: 46L10, 47B15, 81P05, 81Q10"], "pdf": "https://arxiv.org/pdf/2507.10027", "abs": "https://arxiv.org/abs/2507.10027", "authors": ["Jan van Neerven", "Marijn Waaijer"], "title": "Indiscernibility of quantum states", "comment": null, "summary": "In this paper we develop a mathematical framework for indiscernibility of\nquantum states, arguing that, given a set of observables, the ``distinguishable\nobjects'' are the equivalence classes modulo indiscernibility relative to the\nobservables. The structure of the set of distinguishable objects - called the\nHolevo space - is investigated in detail, and it is shown that the observables\nadmit a natural lift to continuous functions on the Holevo space. The theory is\nillustrated by several examples where the ``distinguishable objects'' can be\ndescribed explicitly. Among other things, the Holevo spaces and the lifted\nfunctions are described for position measurements on a free particle and for\nspin measurements in the EPR and Bell experiments.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u91cf\u5b50\u6001\u4e0d\u53ef\u8fa8\u522b\u7684\u6570\u5b66\u6846\u67b6\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u7ed3\u6784\uff0c\u5c06\u53ef\u89c2\u6d4b\u91cf\u63d0\u5347\u4e3aHolevo\u7a7a\u95f4\u4e0a\u7684\u8fde\u7eed\u51fd\u6570\uff0c\u5e76\u901a\u8fc7\u5177\u4f53\u4f8b\u5b50\u8fdb\u884c\u4e86\u8bf4\u660e\u3002", "motivation": "\u7814\u7a76\u4e86\u53ef\u533a\u5206\u5bf9\u8c61\u7684\u7ed3\u6784\uff0c\u5373Holevo\u7a7a\u95f4\uff0c\u5e76\u8868\u660e\u53ef\u89c2\u6d4b\u91cf\u53ef\u4ee5\u81ea\u7136\u5730\u63d0\u5347\u4e3aHolevo\u7a7a\u95f4\u4e0a\u7684\u8fde\u7eed\u51fd\u6570\u3002", "method": "\u5f00\u53d1\u4e86\u91cf\u5b50\u6001\u4e0d\u53ef\u8fa8\u522b\u7684\u6570\u5b66\u6846\u67b6\u3002", "result": "\u7814\u7a76\u4e86Holevo\u7a7a\u95f4\u548c\u63d0\u5347\u7684\u51fd\u6570\uff0c\u5e76\u7528\u5177\u4f53\u4f8b\u5b50\u8fdb\u884c\u4e86\u8bf4\u660e\uff0c\u4f8b\u5982\u81ea\u7531\u7c92\u5b50\u7684\u4f4d\u7f6e\u6d4b\u91cf\u4ee5\u53caEPR\u548c\u8d1d\u5c14\u5b9e\u9a8c\u4e2d\u7684\u81ea\u65cb\u6d4b\u91cf\u3002", "conclusion": "\u53ef\u533a\u5206\u5bf9\u8c61\u662f\u76f8\u5bf9\u4e8e\u53ef\u89c2\u6d4b\u91cf\u7684\u4e0d\u53ef\u8fa8\u522b\u6027\u7684\u7b49\u4ef7\u7c7b\u3002"}}
{"id": "2507.10098", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10098", "abs": "https://arxiv.org/abs/2507.10098", "authors": ["Chen Su", "Yuanhe Tian", "Qinyu Liu", "Jun Zhang", "Yan Song"], "title": "Fusing Large Language Models with Temporal Transformers for Time Series Forecasting", "comment": null, "summary": "Recently, large language models (LLMs) have demonstrated powerful\ncapabilities in performing various tasks and thus are applied by recent studies\nto time series forecasting (TSF) tasks, which predict future values with the\ngiven historical time series. Existing LLM-based approaches transfer knowledge\nlearned from text data to time series prediction using prompting or fine-tuning\nstrategies. However, LLMs are proficient at reasoning over discrete tokens and\nsemantic patterns but are not initially designed to model continuous numerical\ntime series data. The gaps between text and time series data lead LLMs to\nachieve inferior performance to a vanilla Transformer model that is directly\ntrained on TSF data. However, the vanilla Transformers often struggle to learn\nhigh-level semantic patterns. In this paper, we design a novel\nTransformer-based architecture that complementarily leverages LLMs and vanilla\nTransformers, so as to integrate the high-level semantic representations\nlearned by LLMs into the temporal information encoded by time series\nTransformers, where a hybrid representation is obtained by fusing the\nrepresentations from the LLM and the Transformer. The resulting fused\nrepresentation contains both historical temporal dynamics and semantic\nvariation patterns, allowing our model to predict more accurate future values.\nExperiments on benchmark datasets demonstrate the effectiveness of the proposed\napproach.", "AI": {"tldr": "\u901a\u8fc7\u878d\u5408LLM\u7684\u65f6\u95f4\u5e8f\u5217\u7406\u89e3\u80fd\u529b\u548cTransformer\u7684\u65f6\u95f4\u5e8f\u5217\u5efa\u6a21\u80fd\u529b\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u6a21\u578b\uff0c\u7528\u4e8e\u63d0\u9ad8\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u6cd5\u7531\u4e8eLLM\u4e3b\u8981\u64c5\u957f\u5904\u7406\u79bb\u6563\u7684\u6587\u672c\u6807\u8bb0\u548c\u8bed\u4e49\u6a21\u5f0f\uff0c\u800c\u672a\u80fd\u5f88\u597d\u5730\u5904\u7406\u8fde\u7eed\u7684\u6570\u503c\u65f6\u95f4\u5e8f\u5217\u6570\u636e\uff0c\u5bfc\u81f4\u5176\u6027\u80fd\u4e0d\u5982\u76f4\u63a5\u5728\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u8bad\u7ec3\u7684Transformer\u6a21\u578b\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684Transformer\u6a21\u578b\u53c8\u96be\u4ee5\u5b66\u4e60\u5230\u9ad8\u5c42\u8bed\u4e49\u6a21\u5f0f\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u79cd\u5dee\u8ddd\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u65b0\u9896\u7684Transformer\u57fa\u7840\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u4e92\u8865\u5730\u5229\u7528LLM\u548cTransformer\u7684\u4f18\u70b9\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5b83\u5c06LLM\u5728\u9ad8\u5c42\u8bed\u4e49\u4e0a\u7684\u7406\u89e3\u80fd\u529b\u4e0e\u65f6\u95f4\u5e8f\u5217Transformer\u5728\u65f6\u95f4\u4fe1\u606f\u7f16\u7801\u4e0a\u7684\u80fd\u529b\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u878d\u5408\u8fd9\u4e24\u79cd\u6a21\u578b\u7684\u8868\u793a\u6765\u83b7\u5f97\u6df7\u5408\u8868\u793a\uff0c\u4ece\u800c\u540c\u65f6\u5305\u542b\u5386\u53f2\u65f6\u95f4\u52a8\u6001\u548c\u8bed\u4e49\u53d8\u5316\u6a21\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408LLM\u548cTransformer\u7684\u4f18\u52bf\uff0c\u5b9e\u73b0\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u6027\u80fd\u63d0\u5347\uff0c\u80fd\u591f\u540c\u65f6\u6355\u6349\u65f6\u95f4\u52a8\u6001\u548c\u8bed\u4e49\u6a21\u5f0f\u3002"}}
{"id": "2507.09127", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09127", "abs": "https://arxiv.org/abs/2507.09127", "authors": ["Harshil Kotamreddy", "Marlos C. Machado"], "title": "A Study of Value-Aware Eigenoptions", "comment": "Presented at the RLC Workshop on Inductive Biases in Reinforcement\n  Learning 2025", "summary": "Options, which impose an inductive bias toward temporal and hierarchical\nstructure, offer a powerful framework for reinforcement learning (RL). While\neffective in sequential decision-making, they are often handcrafted rather than\nlearned. Among approaches for discovering options, eigenoptions have shown\nstrong performance in exploration, but their role in credit assignment remains\nunderexplored. In this paper, we investigate whether eigenoptions can\naccelerate credit assignment in model-free RL, evaluating them in tabular and\npixel-based gridworlds. We find that pre-specified eigenoptions aid not only\nexploration but also credit assignment, whereas online discovery can bias the\nagent's experience too strongly and hinder learning. In the context of deep RL,\nwe also propose a method for learning option-values under non-linear function\napproximation, highlighting the impact of termination conditions on\nperformance. Our findings reveal both the promise and complexity of using\neigenoptions, and options more broadly, to simultaneously support credit\nassignment and exploration in reinforcement learning.", "AI": {"tldr": "Eigenoptions help RL exploration and credit assignment, but learning them online can hurt performance. Termination conditions matter in deep RL.", "motivation": "To explore whether eigenoptions can accelerate credit assignment in model-free RL and to understand their role beyond exploration, especially in the context of deep RL and non-linear function approximation.", "method": "Investigated the use of eigenoptions in model-free RL, evaluating them in tabular and pixel-based gridworlds. Also proposed a method for learning option-values in deep RL with non-linear function approximation.", "result": "Pre-specified eigenoptions improved both exploration and credit assignment. Online discovery of eigenoptions hindered learning. The proposed method for learning option-values in deep RL showed the importance of termination conditions.", "conclusion": "Eigenoptions, particularly when pre-specified, can enhance both exploration and credit assignment in model-free RL. However, online discovery of eigenoptions can negatively impact learning by introducing strong biases. In deep RL, learning option-values requires careful consideration of termination conditions, especially under non-linear function approximation."}}
{"id": "2507.09338", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09338", "abs": "https://arxiv.org/abs/2507.09338", "authors": ["Svetlana Orlova", "Tommie Kerssies", "Brun\u00f3 B. Englert", "Gijs Dubbelman"], "title": "Simplifying Traffic Anomaly Detection with Video Foundation Models", "comment": "ICCVW 2025 accepted. Code: https://github.com/tue-mps/simple-tad", "summary": "Recent methods for ego-centric Traffic Anomaly Detection (TAD) often rely on\ncomplex multi-stage or multi-representation fusion architectures, yet it\nremains unclear whether such complexity is necessary. Recent findings in visual\nperception suggest that foundation models, enabled by advanced pre-training,\nallow simple yet flexible architectures to outperform specialized designs.\nTherefore, in this work, we investigate an architecturally simple encoder-only\napproach using plain Video Vision Transformers (Video ViTs) and study how\npre-training enables strong TAD performance. We find that: (i) strong\npre-training enables simple encoder-only models to match or even surpass the\nperformance of specialized state-of-the-art TAD methods, while also being\nsignificantly more efficient; (ii) although weakly- and fully-supervised\npre-training are advantageous on standard benchmarks, we find them less\neffective for TAD. Instead, self-supervised Masked Video Modeling (MVM)\nprovides the strongest signal; and (iii) Domain-Adaptive Pre-Training (DAPT) on\nunlabeled driving videos further improves downstream performance, without\nrequiring anomalous examples. Our findings highlight the importance of\npre-training and show that effective, efficient, and scalable TAD models can be\nbuilt with minimal architectural complexity. We release our code,\ndomain-adapted encoders, and fine-tuned models to support future work:\nhttps://github.com/tue-mps/simple-tad.", "AI": {"tldr": "\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\uff08TAD\uff09\u5e76\u4e0d\u4e00\u5b9a\u9700\u8981\u590d\u6742\u7684\u6a21\u578b\u3002\u7814\u7a76\u8868\u660e\uff0c\u4f7f\u7528\u57fa\u7840\u7684\u89c6\u9891 Vision Transformer (Video ViT) \u914d\u5408\u63a9\u7801\u89c6\u9891\u5efa\u6a21\uff08MVM\uff09\u548c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u4e14\u6027\u80fd\u4f18\u8d8a\u7684 TAD \u6a21\u578b\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u6311\u6218\u5f53\u524d\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\uff08TAD\uff09\u9886\u57df\u666e\u904d\u4f9d\u8d56\u590d\u6742\u591a\u9636\u6bb5\u6216\u591a\u8868\u793a\u878d\u5408\u67b6\u6784\u7684\u8d8b\u52bf\uff0c\u5e76\u63a2\u8ba8\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u66f4\u7b80\u5355\u7684\u67b6\u6784\u6765\u5b9e\u73b0\u540c\u7b49\u751a\u81f3\u66f4\u4f18\u7684\u6027\u80fd\u3002\u53d7\u5230\u89c6\u89c9\u611f\u77e5\u9886\u57df\u7814\u7a76\u7684\u542f\u53d1\uff0c\u8be5\u5de5\u4f5c\u65e8\u5728\u8bc1\u660e\u7b80\u5355\u67b6\u6784\u914d\u5408\u5148\u8fdb\u7684\u9884\u8bad\u7ec3\u7b56\u7565\u540c\u6837\u80fd\u53d6\u5f97\u4f18\u5f02\u7684 TAD \u6548\u679c\uff0c\u5e76\u5b9e\u73b0\u66f4\u9ad8\u7684\u6548\u7387\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u3001\u4ec5\u7f16\u7801\u5668\u7684\u67b6\u6784\uff0c\u4e3b\u8981\u4f7f\u7528\u57fa\u7840\u7684\u89c6\u9891 Vision Transformer (Video ViT)\u3002\u7814\u7a76\u7684\u6838\u5fc3\u662f\u63a2\u7d22\u9884\u8bad\u7ec3\u5982\u4f55\u9a71\u52a8 TAD \u6027\u80fd\uff0c\u91cd\u70b9\u6bd4\u8f83\u4e86\u4e0d\u540c\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5f31\u76d1\u7763\u3001\u5168\u76d1\u7763\u548c\u81ea\u76d1\u7763\u63a9\u7801\u89c6\u9891\u5efa\u6a21 MVM\uff09\u4ee5\u53ca\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\u5728\u65e0\u6807\u7b7e\u9a7e\u9a76\u89c6\u9891\u4e0a\u7684\u5e94\u7528\u6548\u679c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f3a\u5927\u7684\u9884\u8bad\u7ec3\u80fd\u591f\u4f7f\u7b80\u5355\u7684\u4ec5\u7f16\u7801\u5668\u6a21\u578b\u5728\u6027\u80fd\u4e0a\u5ab2\u7f8e\u751a\u81f3\u8d85\u8d8a\u5148\u8fdb\u7684 TAD \u65b9\u6cd5\uff0c\u540c\u65f6\u663e\u8457\u63d0\u9ad8\u6548\u7387\u3002\u5728\u9884\u8bad\u7ec3\u7b56\u7565\u65b9\u9762\uff0c\u63a9\u7801\u89c6\u9891\u5efa\u6a21\uff08MVM\uff09\u88ab\u8bc1\u660e\u6bd4\u5f31\u76d1\u7763\u548c\u5168\u76d1\u7763\u9884\u8bad\u7ec3\u66f4\u80fd\u63d0\u4f9b\u6709\u6548\u7684 TAD \u4fe1\u53f7\u3002\u6b64\u5916\uff0c\u5728\u65e0\u6807\u7b7e\u9a7e\u9a76\u89c6\u9891\u4e0a\u8fdb\u884c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\u80fd\u591f\u8fdb\u4e00\u6b65\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u4e14\u65e0\u9700\u5f02\u5e38\u6837\u672c\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u9884\u8bad\u7ec3\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8868\u660e\u4ec5\u4f7f\u7528\u57fa\u7840\u7684\u89c6\u9891 Vision Transformer (Video ViT) \u67b6\u6784\uff0c\u901a\u8fc7\u9002\u5f53\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff08\u7279\u522b\u662f\u63a9\u7801\u89c6\u9891\u5efa\u6a21\uff08MVM\uff09\u548c\u9886\u57df\u81ea\u9002\u5e94\u9884\u8bad\u7ec3\uff08DAPT\uff09\uff09\uff0c\u53ef\u4ee5\u5728\u6548\u7387\u548c\u6027\u80fd\u4e0a\u8d85\u8d8a\u590d\u6742\u7684\u591a\u9636\u6bb5\u6216\u591a\u8868\u793a\u878d\u5408\u65b9\u6cd5\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u3001\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u7684\u4ea4\u901a\u5f02\u5e38\u68c0\u6d4b\uff08TAD\uff09\u6a21\u578b\u3002"}}
{"id": "2507.10050", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph", "math.CO", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.10050", "abs": "https://arxiv.org/abs/2507.10050", "authors": ["Wenxuan Tao", "Fen Zuo"], "title": "Testing APS conjecture on regular graphs", "comment": "21 pages, one figure", "summary": "The maximum energy of the EPR model on a weighted graph is known to be\nupper-bounded by the sum of the total weight and the value of maximum-weight\nfractional matching~(MWFM). Recently, Apte, Parekh and Sud~(APS) conjecture\nthat the bound could be strengthened by replacing MWFM with maximum weight\nmatching~(MWM). Here we test this conjecture on a special class of regular\ngraphs that Henning and Yeo constructed many years ago. On this class of\nregular graphs, MWMs achieve tight lower bounds. As for the maximum energy of\nthe EPR model, we have recently devised a new algorithm called Fractional\nEntanglement Distribution~(FED) based on quasi-homogeneous fractional\nmatchings, which could achieve rather high accuracy. Applying the FED algorithm\nto the EPR model on Henning-Yeo graphs, we could thus obtain energy as high as\npossible and matching value as low as possible, and then make high-precision\ntests of the APS conjecture. Nevertheless, our numerical results do not show\nany evidence that the APS conjecture could be violated.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86EPR\u6a21\u578b\u5728\u6b63\u5219\u56fe\u4e0a\u7684\u80fd\u91cf\u4e0a\u754c\u95ee\u9898\uff0c\u5e76\u6d4b\u8bd5\u4e86APS\u731c\u60f3\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u672a\u8fdd\u53cd\u8be5\u731c\u60f3\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u68c0\u9a8c\u4e00\u9879\u731c\u60f3\uff0c\u8be5\u731c\u60f3\u8ba4\u4e3a\u6700\u5927\u80fd\u91cf\u7684EPR\u6a21\u578b\u5728\u52a0\u6743\u56fe\u4e0a\u7684\u4e0a\u754c\u53ef\u4ee5\u901a\u8fc7\u5c06\u6700\u5927\u6743\u91cd\u5206\u6570\u5339\u914d\uff08MWFM\uff09\u66ff\u6362\u4e3a\u6700\u5927\u6743\u91cd\u5339\u914d\uff08MWM\uff09\u6765\u52a0\u5f3a\u3002", "method": "\u5e94\u7528\u4e86\u540d\u4e3a\u5206\u6570\u7ea0\u7f20\u5206\u5e03\uff08FED\uff09\u7684\u65b0\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u62df\u9f50\u6b21\u5206\u6570\u5339\u914d\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eHenning-Yeo\u56fe\u4e0a\u7684EPR\u6a21\u578b\uff0c\u4ee5\u83b7\u5f97\u5c3d\u53ef\u80fd\u9ad8\u7684\u80fd\u91cf\u548c\u5c3d\u53ef\u80fd\u4f4e\u7684\u6700\u4f18\u5339\u914d\u503c\uff0c\u4ece\u800c\u5bf9APS\u731c\u60f3\u8fdb\u884c\u9ad8\u7cbe\u5ea6\u6d4b\u8bd5\u3002", "result": "\u5728Henning-Yeo\u6784\u9020\u7684\u6b63\u5219\u56fe\u7c7b\u4e0a\uff0cMWM\u8fbe\u5230\u4e86\u4e25\u683c\u7684\u4e0b\u754c\u3002\u5bf9EPR\u6a21\u578b\u5728\u8be5\u6b63\u5219\u56fe\u7c7b\u4e0a\u7684\u5e94\u7528\uff0c\u6570\u503c\u7ed3\u679c\u5e76\u672a\u663e\u793a\u4efb\u4f55\u652f\u6301APS\u731c\u60f3\u88ab\u8fdd\u53cd\u7684\u8bc1\u636e\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e0d\u652f\u6301APS\u731c\u60f3\u53ef\u80fd\u88ab\u8fdd\u53cd\u7684\u8bf4\u6cd5\u3002"}}
{"id": "2507.10155", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10155", "abs": "https://arxiv.org/abs/2507.10155", "authors": ["Khouloud Saadi", "Di Wang"], "title": "Task-Based Flexible Feature Distillation for LLMs", "comment": null, "summary": "Knowledge Distillation (KD) in general and feature distillation in particular\nare promising techniques for reducing the high computational demand of large\nlanguage models (LLMs). However, traditional feature KD methods typically\nassume that the teacher and the student share the same hidden size, limiting\nthe flexibility of the student's architecture. A common solution to this\nproblem involves training a linear projector to align their feature spaces, but\nthis introduces additional parameters that must be learned from scratch and\noften degrades performance on downstream tasks, especially in generative\nsettings. To address this issue, in this work, we propose a novel task-based\nfeature distillation method that enables knowledge transfer between teacher and\nstudent models with different hidden layer dimensions, without introducing any\nnew parameters. Leveraging the insight that only a subset of LLM components\ncontribute significantly to a specific downstream task, our approach identifies\nthe most task-relevant hidden units in the teacher and directly distills their\nactivations to the student. Our method is flexible and easily integrates with\nother distillation frameworks. Empirical results show consistent improvements\nover prior approaches across diverse tasks, including classification,\ninstruction-following, and summarization, achieving up to a 3\\% performance\ngain over the linear projection baseline.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u989d\u5916\u53c2\u6570\u5373\u53ef\u5728\u4e0d\u540c\u7ef4\u5ea6\u9690\u85cf\u5c42\u4e4b\u95f4\u8fdb\u884c\u77e5\u8bc6\u84b8\u998f\u7684\u65b0\u65b9\u6cd5\uff0c\u5728\u591a\u79cd\u4efb\u52a1\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u7279\u5f81\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u65b9\u6cd5\u901a\u5e38\u8981\u6c42\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u5177\u6709\u76f8\u540c\u7684\u9690\u85cf\u5c42\u5927\u5c0f\uff0c\u8fd9\u9650\u5236\u4e86\u5b66\u751f\u6a21\u578b\u7684\u67b6\u6784\u7075\u6d3b\u6027\u3002\u867d\u7136\u5b58\u5728\u901a\u8fc7\u8bad\u7ec3\u7ebf\u6027\u6295\u5f71\u4eea\u6765\u5bf9\u9f50\u7279\u5f81\u7a7a\u95f4\u7684\u65b9\u6cd5\uff0c\u4f46\u8fd9\u4f1a\u5f15\u5165\u9700\u8981\u4ece\u5934\u5b66\u4e60\u7684\u989d\u5916\u53c2\u6570\uff0c\u5e76\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5c24\u5176\u662f\u5728\u751f\u6210\u4efb\u52a1\u4e2d\uff09\u7684\u6027\u80fd\u5f80\u5f80\u4f1a\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u901a\u8fc7\u8bc6\u522b\u6559\u5e08\u6a21\u578b\u4e2d\u6700\u5177\u4efb\u52a1\u76f8\u5173\u6027\u7684\u9690\u85cf\u5355\u5143\uff0c\u5e76\u76f4\u63a5\u5c06\u5176\u6fc0\u6d3b\u503c\u84b8\u998f\u7ed9\u5b66\u751f\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u6559\u5e08\u548c\u5b66\u751f\u6a21\u578b\u4e0d\u540c\u9690\u85cf\u5c42\u7ef4\u5ea6\u4e4b\u95f4\u7684\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e14\u65e0\u9700\u5f15\u5165\u989d\u5916\u53c2\u6570\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u3001\u6307\u4ee4\u9075\u5faa\u548c\u6458\u8981\u7b49\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u4e0e\u7ebf\u6027\u6295\u5f71\u57fa\u7ebf\u76f8\u6bd4\u65f6\uff0c\u6027\u80fd\u6700\u9ad8\u53ef\u63d0\u53473%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u4efb\u52a1\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u80fd\u591f\u5728\u6559\u5e08\u6a21\u578b\u548c\u5b66\u751f\u6a21\u578b\u5177\u6709\u4e0d\u540c\u9690\u85cf\u5c42\u7ef4\u5ea6\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\u77e5\u8bc6\u8fc1\u79fb\uff0c\u4e14\u65e0\u9700\u5f15\u5165\u4efb\u4f55\u989d\u5916\u53c2\u6570\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u6559\u5e08\u6a21\u578b\u4e2d\u6700\u5177\u4efb\u52a1\u76f8\u5173\u6027\u7684\u9690\u85cf\u5355\u5143\uff0c\u5e76\u76f4\u63a5\u5c06\u5176\u6fc0\u6d3b\u503c\u84b8\u998f\u7ed9\u5b66\u751f\u6a21\u578b\uff0c\u80fd\u591f\u7075\u6d3b\u5730\u96c6\u6210\u5230\u5176\u4ed6\u84b8\u998f\u6846\u67b6\u4e2d\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u5206\u7c7b\u3001\u6307\u4ee4\u9075\u5faa\u548c\u6458\u8981\u7b49\u591a\u79cd\u4efb\u52a1\u4e0a\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u4e8e\u7ebf\u6027\u6295\u5f71\u57fa\u7ebf\u6709\u9ad8\u8fbe3%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.09132", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09132", "abs": "https://arxiv.org/abs/2507.09132", "authors": ["Chu-Yuan Wei", "Shun-Yao Liu", "Sheng-Da Zhuo", "Chang-Dong Wang", "Shu-Qiang Huang", "Mohsen Guizani"], "title": "Heterogeneous Graph Prompt Learning via Adaptive Weight Pruning", "comment": null, "summary": "Graph Neural Networks (GNNs) have achieved remarkable success in various\ngraph-based tasks (e.g., node classification or link prediction). Despite their\ntriumphs, GNNs still face challenges such as long training and inference times,\ndifficulty in capturing complex relationships, and insufficient feature\nextraction. To tackle these issues, graph pre-training and graph prompt methods\nhave garnered increasing attention for their ability to leverage large-scale\ndatasets for initial learning and task-specific adaptation, offering potential\nimprovements in GNN performance. However, previous research has overlooked the\npotential of graph prompts in optimizing models, as well as the impact of both\npositive and negative graph prompts on model stability and efficiency. To\nbridge this gap, we propose a novel framework combining graph prompts with\nweight pruning, called GPAWP, which aims to enhance the performance and\nefficiency of graph prompts by using fewer of them. We evaluate the importance\nof graph prompts using an importance assessment function to determine positive\nand negative weights at different granularities. Through hierarchically\nstructured pruning, we eliminate negative prompt labels, resulting in more\nparameter-efficient and competitively performing prompts. Extensive experiments\non three benchmark datasets demonstrate the superiority of GPAWP, leading to a\nsignificant reduction in parameters in node classification tasks.", "AI": {"tldr": "GPAWP\u6846\u67b6\u7ed3\u5408\u56fe\u63d0\u793a\u548c\u6743\u91cd\u526a\u679d\uff0c\u901a\u8fc7\u526a\u679d\u8d1f\u9762\u63d0\u793a\u6807\u7b7e\u6765\u4f18\u5316GNN\u6027\u80fd\u548c\u6548\u7387\uff0c\u663e\u8457\u51cf\u5c11\u53c2\u6570\u91cf\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u8bad\u7ec3\u548c\u63a8\u7406\u65f6\u95f4\u957f\u3001\u96be\u4ee5\u6355\u6349\u590d\u6742\u5173\u7cfb\u4ee5\u53ca\u7279\u5f81\u63d0\u53d6\u4e0d\u8db3\u7b49\u95ee\u9898\u3002\u540c\u65f6\uff0c\u586b\u8865\u4e86\u5148\u524d\u7814\u7a76\u5728\u56fe\u63d0\u793a\u4f18\u5316\u6a21\u578b\u6f5c\u529b\u3001\u4ee5\u53ca\u6b63\u8d1f\u56fe\u63d0\u793a\u5bf9\u6a21\u578b\u7a33\u5b9a\u6027\u548c\u6548\u7387\u5f71\u54cd\u65b9\u9762\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u63d0\u793a\uff08graph prompts\uff09\u548c\u6743\u91cd\u526a\u679d\uff08weight pruning\uff09\u7684\u65b0\u6846\u67b6GPAWP\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u91cd\u8981\u6027\u8bc4\u4f30\u51fd\u6570\u6765\u8bc4\u4f30\u4e0d\u540c\u7c92\u5ea6\u7684\u56fe\u63d0\u793a\u7684\u6b63\u8d1f\u6743\u91cd\uff0c\u5e76\u901a\u8fc7\u5206\u5c42\u7ed3\u6784\u526a\u679d\uff08hierarchically structured pruning\uff09\u6d88\u9664\u8d1f\u9762\u63d0\u793a\u6807\u7b7e\uff0c\u4ee5\u63d0\u5347\u56fe\u63d0\u793a\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0cGPAWP\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u80fd\u591f\u663e\u8457\u51cf\u5c11\u53c2\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u5347GNN\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "GPAWP\u901a\u8fc7\u7ed3\u5408\u56fe\u63d0\u793a\u548c\u6743\u91cd\u526a\u679d\uff0c\u5229\u7528\u91cd\u8981\u6027\u8bc4\u4f30\u51fd\u6570\u8bc6\u522b\u5e76\u6d88\u9664\u8d1f\u9762\u63d0\u793a\u6807\u7b7e\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u66f4\u5c11\u4f46\u6027\u80fd\u8868\u73b0\u4f18\u8d8a\u7684\u56fe\u63d0\u793a\uff0c\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\u3002"}}
{"id": "2507.09375", "categories": ["cs.CV", "I.2.6; I.5.4"], "pdf": "https://arxiv.org/pdf/2507.09375", "abs": "https://arxiv.org/abs/2507.09375", "authors": ["Sourish Suri", "Yifei Shao"], "title": "Automated Multi-Class Crop Pathology Classification via Convolutional Neural Networks: A Deep Learning Approach for Real-Time Precision Agriculture", "comment": "29 pages, 10 figures, 1 table. Code available at:\n  https://github.com/Sourish85/CNN-CROP-DIS-DETECTOR", "summary": "Crop diseases present a significant barrier to agricultural productivity and\nglobal food security, especially in large-scale farming where early\nidentification is often delayed or inaccurate. This research introduces a\nConvolutional Neural Network (CNN)-based image classification system designed\nto automate the detection and classification of eight common crop diseases\nusing leaf imagery. The methodology involves a complete deep learning pipeline:\nimage acquisition from a large, labeled dataset, preprocessing via resizing,\nnormalization, and augmentation, and model training using TensorFlow with\nKeras' Sequential API. The CNN architecture comprises three convolutional\nlayers with increasing filter sizes and ReLU activations, followed by max\npooling, flattening, and fully connected layers, concluding with a softmax\noutput for multi-class classification. The system achieves high training\naccuracy (~90%) and demonstrates reliable performance on unseen data, although\na validation accuracy of ~60% suggests minor overfitting. Notably, the model\nintegrates a treatment recommendation module, providing actionable guidance by\nmapping each detected disease to suitable pesticide or fungicide interventions.\nFurthermore, the solution is deployed on an open-source, mobile-compatible\nplatform, enabling real-time image-based diagnostics for farmers in remote\nareas. This research contributes a scalable and accessible tool to the field of\nprecision agriculture, reducing reliance on manual inspection and promoting\nsustainable disease management practices. By merging deep learning with\npractical agronomic support, this work underscores the potential of CNNs to\ntransform crop health monitoring and enhance food production resilience on a\nglobal scale.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528CNN\u5f00\u53d1\u4e86\u4e00\u4e2a\u4f5c\u7269\u75c5\u5bb3\u81ea\u52a8\u68c0\u6d4b\u548c\u5206\u7c7b\u7cfb\u7edf\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u5e76\u63d0\u4f9b\u6cbb\u7597\u5efa\u8bae\uff0c\u53ef\u5e94\u7528\u4e8e\u79fb\u52a8\u5e73\u53f0\uff0c\u52a9\u529b\u7cbe\u51c6\u519c\u4e1a\u53d1\u5c55\u3002", "motivation": "\u4f5c\u7269\u75c5\u5bb3\u4e25\u91cd\u5f71\u54cd\u519c\u4e1a\u751f\u4ea7\u529b\u548c\u5168\u7403\u7cae\u98df\u5b89\u5168\uff0c\u5c24\u5176\u662f\u5728\u5927\u89c4\u6a21\u519c\u4e1a\u4e2d\uff0c\u65e9\u671f\u8bc6\u522b\u5e38\u5e38\u6ede\u540e\u6216\u4e0d\u51c6\u786e\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u5206\u7c7b\u4f5c\u7269\u75c5\u5bb3\u7684\u7cfb\u7edf\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u4f5c\u4e3a\u6838\u5fc3\u6280\u672f\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5b8c\u6574\u7684\u6df1\u5ea6\u5b66\u4e60\u6d41\u7a0b\u3002\u9996\u5148\uff0c\u5229\u7528\u5927\u89c4\u6a21\u6807\u8bb0\u6570\u636e\u96c6\u8fdb\u884c\u56fe\u50cf\u91c7\u96c6\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u8c03\u6574\u5927\u5c0f\u3001\u5f52\u4e00\u5316\u548c\u6570\u636e\u589e\u5f3a\u7b49\u6280\u672f\u5bf9\u56fe\u50cf\u8fdb\u884c\u9884\u5904\u7406\uff1b\u6700\u540e\uff0c\u4f7f\u7528TensorFlow\u548cKeras\u7684Sequential API\u8bad\u7ec3CNN\u6a21\u578b\u3002\u8be5CNN\u67b6\u6784\u5305\u542b\u4e09\u4e2a\u5377\u79ef\u5c42\u3001\u6700\u5927\u6c60\u5316\u5c42\u3001\u5c55\u5e73\u5c42\u548c\u5168\u8fde\u63a5\u5c42\uff0c\u5e76\u4f7f\u7528Softmax\u6fc0\u6d3b\u51fd\u6570\u8fdb\u884c\u591a\u7c7b\u522b\u5206\u7c7b\u3002", "result": "\u8be5CNN\u7cfb\u7edf\u5728\u8bad\u7ec3\u96c6\u4e0a\u8fbe\u5230\u4e86\u7ea690%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u4e0a\u8868\u73b0\u51fa\u53ef\u9760\u7684\u6027\u80fd\uff0c\u5c3d\u7ba1\u9a8c\u8bc1\u51c6\u786e\u7387\u7ea6\u4e3a60%\uff0c\u8868\u660e\u5b58\u5728\u8f7b\u5fae\u7684\u8fc7\u62df\u5408\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u96c6\u6210\u4e86\u6cbb\u7597\u63a8\u8350\u6a21\u5757\uff0c\u80fd\u591f\u6839\u636e\u68c0\u6d4b\u5230\u7684\u75c5\u5bb3\u63d0\u4f9b\u76f8\u5e94\u7684\u519c\u836f\u6216\u6740\u83cc\u5242\u5e72\u9884\u63aa\u65bd\u3002\u8be5\u89e3\u51b3\u65b9\u6848\u5df2\u90e8\u7f72\u5728\u4e00\u4e2a\u5f00\u6e90\u3001\u79fb\u52a8\u517c\u5bb9\u7684\u5e73\u53f0\u4e0a\uff0c\u80fd\u591f\u4e3a\u504f\u8fdc\u5730\u533a\u7684\u519c\u6c11\u63d0\u4f9b\u5b9e\u65f6\u7684\u56fe\u50cf\u8bca\u65ad\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u56fe\u50cf\u5206\u7c7b\u7cfb\u7edf\uff0c\u7528\u4e8e\u81ea\u52a8\u5316\u68c0\u6d4b\u548c\u5206\u7c7b\u516b\u79cd\u5e38\u89c1\u7684\u4f5c\u7269\u75c5\u5bb3\u3002\u8be5\u7cfb\u7edf\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u63d0\u4f9b\u5b9e\u65f6\u8bca\u65ad\u65b9\u9762\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u6027\u548c\u5b9e\u7528\u6027\uff0c\u5e76\u96c6\u6210\u4e86\u6cbb\u7597\u5efa\u8bae\u6a21\u5757\u3002\u8be5\u65b9\u6cd5\u4e3a\u7cbe\u51c6\u519c\u4e1a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6613\u4e8e\u4f7f\u7528\u7684\u5de5\u5177\uff0c\u6709\u52a9\u4e8e\u51cf\u5c11\u5bf9\u4eba\u5de5\u68c0\u67e5\u7684\u4f9d\u8d56\uff0c\u5e76\u4fc3\u8fdb\u53ef\u6301\u7eed\u7684\u75c5\u866b\u5bb3\u7ba1\u7406\u5b9e\u8df5\uff0c\u6700\u7ec8\u589e\u5f3a\u5168\u7403\u7cae\u98df\u751f\u4ea7\u7684\u97e7\u6027\u3002"}}
{"id": "2507.10080", "categories": ["quant-ph", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2507.10080", "abs": "https://arxiv.org/abs/2507.10080", "authors": ["Koki Shiraishi", "Masaya Nakagawa", "Takashi Mori"], "title": "Davies equation without the secular approximation: Reconciling locality with quantum thermodynamics for open quadratic systems", "comment": "7 pages, 1 figures", "summary": "We derive a thermodynamically consistent quantum master equation that\nsatisfies locality for quadratic systems coupled to independent and identical\nbaths at each site. We show that the quasi-local Redfield equation coincides\nexactly with the Davies equation, which satisfies the detailed-balance\ncondition, due to cancellation of quantum coherence generated by each bath.\nThis derivation does not rely on the secular approximation, which fails in\nsystems with vanishing energy-level spacings. We discuss generalizations of our\nresult to slowly driven quadratic systems and generic quantum many-body\nsystems. Our result paves the way to a thermodynamically consistent description\nof quantum many-body systems.", "AI": {"tldr": "\u91cf\u5b50\u4e3b\u65b9\u7a0b\u7684\u63a8\u5bfc\u4e3a\u70ed\u529b\u5b66\u4e00\u81f4\u5730\u63cf\u8ff0\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u7814\u7a76\u4e3a\u4ec0\u4e48\u62df\u5c40\u57dfRedfield\u65b9\u7a0b\u6070\u597d\u7b49\u4e8e\u6ee1\u8db3\u8be6\u7ec6\u5e73\u8861\u6761\u4ef6\u7684Davies\u65b9\u7a0b\uff0c\u8fd9\u662f\u7531\u4e8e\u6bcf\u4e2a\u6d74\u4ea7\u751f\u7684\u91cf\u5b50\u76f8\u5e72\u6027\u76f8\u4e92\u62b5\u6d88\u3002", "method": "\u63a8\u5bfc\u4e86\u4e00\u4e2a\u70ed\u529b\u5b66\u4e00\u81f4\u7684\u91cf\u5b50\u4e3b\u65b9\u7a0b\uff0c\u8be5\u65b9\u7a0b\u5bf9\u8026\u5408\u5230\u6bcf\u4e2a\u4f4d\u70b9\u4e0a\u72ec\u7acb\u4e14\u76f8\u540c\u7684\u6d74\u7684\u4e8c\u6b21\u7cfb\u7edf\u6ee1\u8db3\u5c40\u57df\u6027\u3002", "result": "\u8868\u660e\u62df\u5c40\u57dfRedfield\u65b9\u7a0b\u6070\u597d\u7b49\u4e8e\u6ee1\u8db3\u8be6\u7ec6\u5e73\u8861\u6761\u4ef6\u7684Davies\u65b9\u7a0b\uff0c\u8fd9\u662f\u7531\u4e8e\u6bcf\u4e2a\u6d74\u4ea7\u751f\u7684\u91cf\u5b50\u76f8\u5e72\u6027\u76f8\u4e92\u62b5\u6d88\u3002\u8be5\u63a8\u5bfc\u4e0d\u4f9d\u8d56\u4e8e\u5728\u80fd\u7ea7\u95f4\u8ddd\u6d88\u5931\u7684\u7cfb\u7edf\u4e2d\u5931\u8d25\u7684\u957f\u671f\u8fd1\u4f3c\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u4e3a\u70ed\u529b\u5b66\u4e00\u81f4\u5730\u63cf\u8ff0\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.10177", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10177", "abs": "https://arxiv.org/abs/2507.10177", "authors": ["Rohitash Chandra", "Jiyong Choi"], "title": "Abusive text transformation using LLMs", "comment": null, "summary": "Although Large Language Models (LLMs) have demonstrated significant\nadvancements in natural language processing tasks, their effectiveness in the\nclassification and transformation of abusive text into non-abusive versions\nremains an area for exploration. In this study, we aim to use LLMs to transform\nabusive text (tweets and reviews) featuring hate speech and swear words into\nnon-abusive text, while retaining the intent of the text. We evaluate the\nperformance of two state-of-the-art LLMs, such as Gemini, GPT-4o, DeekSeek and\nGroq, on their ability to identify abusive text. We them to transform and\nobtain a text that is clean from abusive and inappropriate content but\nmaintains a similar level of sentiment and semantics, i.e. the transformed text\nneeds to maintain its message. Afterwards, we evaluate the raw and transformed\ndatasets with sentiment analysis and semantic analysis. Our results show Groq\nprovides vastly different results when compared with other LLMs. We have\nidentified similarities between GPT-4o and DeepSeek-V3.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5c06\u8fb1\u9a82\u6027\u6587\u672c\u8f6c\u6362\u4e3a\u975e\u8fb1\u9a82\u6027\u6587\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u5176\u610f\u56fe\u3002\u7814\u7a76\u8bc4\u4f30\u4e86 Gemini\u3001GPT-4o\u3001DeepSeek \u548c Groq \u7684\u6027\u80fd\uff0c\u53d1\u73b0 Groq \u7684\u7ed3\u679c\u4e0e\u5176\u4ed6\u6a21\u578b\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u800c GPT-4o \u548c DeepSeek-V3 \u4e4b\u95f4\u5b58\u5728\u76f8\u4f3c\u6027\u3002", "motivation": "\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5206\u7c7b\u548c\u8f6c\u6362\u8fb1\u9a82\u6027\u6587\u672c\u4e3a\u975e\u8fb1\u9a82\u6027\u6587\u672c\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "method": "\u672c\u7814\u7a76\u65e8\u5728\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5c06\u5305\u542b\u4ec7\u6068\u8a00\u8bba\u548c\u810f\u8bdd\u7684\u8fb1\u9a82\u6027\u6587\u672c\uff08\u63a8\u6587\u548c\u8bc4\u8bba\uff09\u8f6c\u6362\u4e3a\u975e\u8fb1\u9a82\u6027\u6587\u672c\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u610f\u56fe\u3002\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86 Gemini\u3001GPT-4o\u3001DeepSeek \u548c Groq \u8fd9\u4e24\u4e2a\u6700\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u8bc6\u522b\u8fb1\u9a82\u6027\u6587\u672c\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u8981\u6c42\u5b83\u4eec\u8fdb\u884c\u8f6c\u6362\uff0c\u751f\u6210\u5e72\u51c0\u4e14\u65e0\u4e0d\u5f53\u5185\u5bb9\u4f46\u4fdd\u6301\u76f8\u4f3c\u60c5\u611f\u548c\u8bed\u4e49\u6c34\u5e73\uff08\u5373\u4fdd\u6301\u5176\u4fe1\u606f\uff09\u7684\u6587\u672c\u3002\u968f\u540e\uff0c\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u60c5\u611f\u5206\u6790\u548c\u8bed\u4e49\u5206\u6790\u8bc4\u4f30\u4e86\u539f\u59cb\u6570\u636e\u96c6\u548c\u8f6c\u6362\u540e\u7684\u6570\u636e\u96c6\u3002", "result": "Groq \u7684\u7ed3\u679c\u4e0e\u5176\u4ed6\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u800c GPT-4o \u548c DeepSeek-V3 \u4e4b\u95f4\u5219\u5b58\u5728\u76f8\u4f3c\u6027\u3002", "conclusion": "Groq \u7684\u7ed3\u679c\u4e0e\u5176\u4ed6\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u800c GPT-4o \u548c DeepSeek-V3 \u4e4b\u95f4\u5219\u5b58\u5728\u76f8\u4f3c\u6027\u3002"}}
{"id": "2507.09137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09137", "abs": "https://arxiv.org/abs/2507.09137", "authors": ["Nripsuta Ani Saxena", "Shang-Ling Hsu", "Mehul Shetty", "Omar Alkhadra", "Cyrus Shahabi", "Abigail L. Horn"], "title": "POIFormer: A Transformer-Based Framework for Accurate and Scalable Point-of-Interest Attribution", "comment": null, "summary": "Accurately attributing user visits to specific Points of Interest (POIs) is a\nfoundational task for mobility analytics, personalized services, marketing and\nurban planning. However, POI attribution remains challenging due to GPS\ninaccuracies, typically ranging from 2 to 20 meters in real-world settings, and\nthe high spatial density of POIs in urban environments, where multiple venues\ncan coexist within a small radius (e.g., over 50 POIs within a 100-meter radius\nin dense city centers). Relying on proximity is therefore often insufficient\nfor determining which POI was actually visited. We introduce\n\\textsf{POIFormer}, a novel Transformer-based framework for accurate and\nefficient POI attribution. Unlike prior approaches that rely on limited\nspatiotemporal, contextual, or behavioral features, \\textsf{POIFormer} jointly\nmodels a rich set of signals, including spatial proximity, visit timing and\nduration, contextual features from POI semantics, and behavioral features from\nuser mobility and aggregated crowd behavior patterns--using the Transformer's\nself-attention mechanism to jointly model complex interactions across these\ndimensions. By leveraging the Transformer to model a user's past and future\nvisits (with the current visit masked) and incorporating crowd-level behavioral\npatterns through pre-computed KDEs, \\textsf{POIFormer} enables accurate,\nefficient attribution in large, noisy mobility datasets. Its architecture\nsupports generalization across diverse data sources and geographic contexts\nwhile avoiding reliance on hard-to-access or unavailable data layers, making it\npractical for real-world deployment. Extensive experiments on real-world\nmobility datasets demonstrate significant improvements over existing baselines,\nparticularly in challenging real-world settings characterized by spatial noise\nand dense POI clustering.", "AI": {"tldr": "POIFormer\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u591a\u79cd\u4fe1\u53f7\uff08\u7a7a\u95f4\u3001\u65f6\u95f4\u3001\u4e0a\u4e0b\u6587\u3001\u884c\u4e3a\uff09\u6765\u89e3\u51b3GPS\u4e0d\u51c6\u548cPOI\u5bc6\u96c6\u5bfc\u81f4\u7684POI\u5f52\u56e0\u96be\u9898\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u51c6\u786e\u5730\u5c06\u7528\u6237\u8bbf\u95ee\u5f52\u56e0\u4e8e\u7279\u5b9a\u7684\u5174\u8da3\u70b9\uff08POI\uff09\u662f\u79fb\u52a8\u5206\u6790\u3001\u4e2a\u6027\u5316\u670d\u52a1\u3001\u5e02\u573a\u8425\u9500\u548c\u57ce\u5e02\u89c4\u5212\u7684\u57fa\u7840\uff0c\u4f46\u7531\u4e8eGPS\u4e0d\u51c6\u786e\u548cPOI\u7a7a\u95f4\u5bc6\u5ea6\u9ad8\uff0c\u8fd9\u4e00\u4efb\u52a1\u5177\u6709\u6311\u6218\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPOIFormer\u7684\u65b0\u578bTransformer\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6574\u5408\u7a7a\u95f4\u90bb\u8fd1\u6027\u3001\u8bbf\u95ee\u65f6\u95f4\u3001\u6301\u7eed\u65f6\u95f4\u3001POI\u8bed\u4e49\u4e0a\u4e0b\u6587\u4ee5\u53ca\u7528\u6237\u79fb\u52a8\u548c\u4eba\u7fa4\u884c\u4e3a\u6a21\u5f0f\u7b49\u591a\u79cd\u7279\u5f81\uff0c\u5e76\u5229\u7528Transformer\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u8fd9\u4e9b\u7ef4\u5ea6\u4e4b\u95f4\u590d\u6742\u7684\u4ea4\u4e92\u4f5c\u7528\u3002", "result": "\u901a\u8fc7\u5728\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cPOIFormer\u5728\u5177\u6709\u7a7a\u95f4\u566a\u58f0\u548c\u5bc6\u96c6POI\u805a\u7c7b\u7684\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "POIFormer\u6846\u67b6\u5728\u5904\u7406\u9ad8\u5bc6\u5ea6\u548c\u7a7a\u95f4\u566a\u58f0\u5927\u7684\u771f\u5b9e\u4e16\u754c\u79fb\u52a8\u6570\u636e\u96c6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.09410", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09410", "abs": "https://arxiv.org/abs/2507.09410", "authors": ["Bernie Boscoe", "Shawn Johnson", "Andrea Osborn", "Chandler Campbell", "Karen Mager"], "title": "GreenCrossingAI: A Camera Trap/Computer Vision Pipeline for Environmental Science Research Groups", "comment": "This is the preprint version of the paper in Practice and Experience\n  in Advanced Research Computing, PEARC25", "summary": "Camera traps have long been used by wildlife researchers to monitor and study\nanimal behavior, population dynamics, habitat use, and species diversity in a\nnon-invasive and efficient manner. While data collection from the field has\nincreased with new tools and capabilities, methods to develop, process, and\nmanage the data, especially the adoption of ML/AI tools, remain challenging.\nThese challenges include the sheer volume of data generated, the need for\naccurate labeling and annotation, variability in environmental conditions\naffecting data quality, and the integration of ML/AI tools into existing\nworkflows that often require domain-specific customization and computational\nresources. This paper provides a guide to a low-resource pipeline to process\ncamera trap data on-premise, incorporating ML/AI capabilities tailored for\nsmall research groups with limited resources and computational expertise. By\nfocusing on practical solutions, the pipeline offers accessible approaches for\ndata transmission, inference, and evaluation, enabling researchers to discover\nmeaningful insights from their ever-increasing camera trap datasets.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10104", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2507.10104", "abs": "https://arxiv.org/abs/2507.10104", "authors": ["Xuan Liu", "Shaoping Shi", "Yimiao Wu", "Xuan Wang", "Long Tian", "Wei Li", "Yajun Wang", "Yaohui Zheng"], "title": "Continuous variable quantum communication with 40 pairs of entangled sideband", "comment": "9 pages, 6 figures, to be published in SCIENCE CHINA Physics,\n  Mechanics & Astronomy", "summary": "Constructing large-scale quantum resources is an important foundation for\nfurther improving the efficiency and scalability of quantum communication.\nHere, we present an efficient extraction and stable control scheme of 40 pairs\nof entangled sideband modes from the squeezed light by specially designing\noptical parametric oscillator. Utilizing the low-loss optical frequency comb\ncontrol technology and the local cross-correlation algorithm, we model and\nmanage the efficient separation process of the entangled sidebands modes\nfacilitated by the optical filtering cavities, a maximum entanglement level of\n6.5 dB is achieved. The feasibility of large-capacity quantum dense coding\nbased on these entangled sideband modes is proved experimentally, which is of\ngreat significance for optimizing the utilization of quantum resources, thereby\ncontributing to the advancement of large-capacity quantum communication\nnetworks and enabling the realization of more secure and efficient quantum\ncommunication systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u538b\u7f29\u5149\u4e2d\u751f\u6210\u548c\u5206\u79bb\u7ea0\u7f20\u8fb9\u5e26\u6a21\u5f0f\u7684\u65b0\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7ea0\u7f20\u5ea6\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5927\u5bb9\u91cf\u91cf\u5b50\u901a\u4fe1\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u6784\u5efa\u5927\u89c4\u6a21\u91cf\u5b50\u8d44\u6e90\u662f\u63d0\u9ad8\u91cf\u5b50\u901a\u4fe1\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u7684\u91cd\u8981\u57fa\u7840\u3002", "method": "\u5229\u7528\u7279\u6b8a\u8bbe\u8ba1\u7684 OPO \u4ece\u538b\u7f29\u5149\u4e2d\u751f\u6210 40 \u5bf9\u7ea0\u7f20\u8fb9\u5e26\u6a21\u5f0f\uff0c\u5e76\u91c7\u7528\u4f4e\u635f\u8017\u5149\u9891\u68b3\u63a7\u5236\u6280\u672f\u548c\u5c40\u90e8\u4e92\u76f8\u5173\u7b97\u6cd5\uff0c\u901a\u8fc7\u5149\u5b66\u6ee4\u6ce2\u8154\u5b9e\u73b0\u7ea0\u7f20\u8fb9\u5e26\u6a21\u5f0f\u7684\u9ad8\u6548\u5206\u79bb\u3002", "result": "\u5b9e\u73b0\u4e86\u6700\u5927 6.5 dB \u7684\u7ea0\u7f20\u5ea6\uff0c\u5e76\u5b9e\u9a8c\u8bc1\u660e\u4e86\u57fa\u4e8e\u7ea0\u7f20\u8fb9\u5e26\u6a21\u5f0f\u7684\u5927\u5bb9\u91cf\u91cf\u5b50\u5bc6\u96c6\u7f16\u7801\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9ad8\u6548\u63d0\u53d6\u548c\u7a33\u5b9a\u63a7\u5236\u65b9\u6848\uff0c\u5229\u7528\u7279\u6b8a\u8bbe\u8ba1\u7684\u5149\u5b66\u53c2\u91cf\u632f\u8361\u5668\u4ece\u538b\u7f29\u5149\u4e2d\u751f\u621040\u5bf9\u7ea0\u7f20\u8fb9\u5e26\u6a21\u5f0f\u3002\u901a\u8fc7\u4f4e\u635f\u8017\u5149\u9891\u68b3\u63a7\u5236\u6280\u672f\u548c\u5c40\u90e8\u4e92\u76f8\u5173\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7ea0\u7f20\u8fb9\u5e26\u6a21\u5f0f\u7684\u9ad8\u6548\u5206\u79bb\uff0c\u7ea0\u7f20\u5ea6\u6700\u5927\u8fbe\u52306.5 dB\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u57fa\u4e8e\u8fd9\u4e9b\u7ea0\u7f20\u8fb9\u5e26\u6a21\u5f0f\u7684\u5927\u5bb9\u91cf\u91cf\u5b50\u5bc6\u96c6\u7f16\u7801\u7684\u53ef\u884c\u6027\uff0c\u5bf9\u4e8e\u4f18\u5316\u91cf\u5b50\u8d44\u6e90\u5229\u7528\u3001\u63a8\u52a8\u5927\u5bb9\u91cf\u91cf\u5b50\u901a\u4fe1\u7f51\u7edc\u53d1\u5c55\u4ee5\u53ca\u5b9e\u73b0\u66f4\u5b89\u5168\u9ad8\u6548\u7684\u91cf\u5b50\u901a\u4fe1\u7cfb\u7edf\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2507.10216", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10216", "abs": "https://arxiv.org/abs/2507.10216", "authors": ["Renad Al-Monef", "Hassan Alhuzali", "Nora Alturayeif", "Ashwag Alasmari"], "title": "Absher: A Benchmark for Evaluating Large Language Models Understanding of Saudi Dialects", "comment": null, "summary": "As large language models (LLMs) become increasingly central to Arabic NLP\napplications, evaluating their understanding of regional dialects and cultural\nnuances is essential, particularly in linguistically diverse settings like\nSaudi Arabia. This paper introduces \\texttt{Absher}, a comprehensive benchmark\nspecifically designed to assess LLMs performance across major Saudi dialects.\n\\texttt{Absher} comprises over 18,000 multiple-choice questions spanning six\ndistinct categories: Meaning, True/False, Fill-in-the-Blank, Contextual Usage,\nCultural Interpretation, and Location Recognition. These questions are derived\nfrom a curated dataset of dialectal words, phrases, and proverbs sourced from\nvarious regions of Saudi Arabia. We evaluate several state-of-the-art LLMs,\nincluding multilingual and Arabic-specific models. We also provide detailed\ninsights into their capabilities and limitations. Our results reveal notable\nperformance gaps, particularly in tasks requiring cultural inference or\ncontextual understanding. Our findings highlight the urgent need for\ndialect-aware training and culturally aligned evaluation methodologies to\nimprove LLMs performance in real-world Arabic applications.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 Absher \u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6c99\u7279\u65b9\u8a00\u4e0a\u7684\u8868\u73b0\uff0c\u7ed3\u679c\u663e\u793a\u6a21\u578b\u5728\u6587\u5316\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u66f4\u6ce8\u91cd\u65b9\u8a00\u548c\u6587\u5316\u7684\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u65b9\u6cd5\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u963f\u62c9\u4f2f\u8bed\u81ea\u7136\u8bed\u8a00\u5904\u7406\u5e94\u7528\u4e2d\u7684\u4f5c\u7528\u65e5\u76ca\u91cd\u8981\uff0c\u8bc4\u4f30\u5b83\u4eec\u5bf9\u533a\u57df\u65b9\u8a00\u548c\u6587\u5316\u7ec6\u5fae\u5dee\u522b\u7684\u7406\u89e3\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u6c99\u7279\u963f\u62c9\u4f2f\u8fd9\u6837\u8bed\u8a00\u73af\u5883\u591a\u6837\u5316\u7684\u5730\u533a\uff0c\u663e\u5f97\u5c24\u4e3a\u5173\u952e\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a Absher \u7684\u7efc\u5408\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b\u8d85\u8fc7 18,000 \u9053\u591a\u9879\u9009\u62e9\u9898\uff0c\u6db5\u76d6\u4e86\u610f\u4e49\u3001\u5224\u65ad\u9898\u3001\u586b\u7a7a\u9898\u3001\u4e0a\u4e0b\u6587\u7528\u6cd5\u3001\u6587\u5316\u89e3\u8bfb\u548c\u5730\u70b9\u8bc6\u522b\u516d\u4e2a\u7c7b\u522b\u3002\u8fd9\u4e9b\u95ee\u9898\u6e90\u81ea\u6c99\u7279\u963f\u62c9\u4f2f\u4e0d\u540c\u5730\u533a\u7684\u65b9\u8a00\u8bcd\u6c47\u3001\u77ed\u8bed\u548c\u8c1a\u8bed\u6570\u636e\u96c6\u3002\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86\u51e0\u79cd\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u6df1\u5165\u5206\u6790\u4e86\u5b83\u4eec\u7684\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5728\u7406\u89e3\u6c99\u7279\u4e3b\u8981\u65b9\u8a00\u65b9\u9762\uff0c\u4e0d\u540c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8868\u73b0\u51fa\u663e\u8457\u7684\u6027\u80fd\u5dee\u5f02\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u6587\u5316\u63a8\u7406\u6216\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\uff0c\u6a21\u578b\u7684\u80fd\u529b\u5b58\u5728\u660e\u663e\u4e0d\u8db3\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u9700\u8981\u6587\u5316\u63a8\u7406\u6216\u4e0a\u4e0b\u6587\u7406\u89e3\u7684\u4efb\u52a1\u4e2d\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u6c99\u7279\u65b9\u8a00\u65b9\u9762\u5b58\u5728\u660e\u663e\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u8fd9\u51f8\u663e\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u963f\u62c9\u4f2f\u8bed\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u91c7\u7528\u6ce8\u91cd\u65b9\u8a00\u548c\u6587\u5316\u9002\u5e94\u6027\u7684\u8bad\u7ec3\u53ca\u8bc4\u4f30\u65b9\u6cd5\u6765\u63d0\u5347\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09173", "categories": ["cs.LG", "cs.AI", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2507.09173", "abs": "https://arxiv.org/abs/2507.09173", "authors": ["Mengjie Chen", "Ming Zhang", "Cunquan Qu"], "title": "Towards Interpretable Drug-Drug Interaction Prediction: A Graph-Based Approach with Molecular and Network-Level Explanations", "comment": null, "summary": "Drug-drug interactions (DDIs) represent a critical challenge in pharmacology,\noften leading to adverse drug reactions with significant implications for\npatient safety and healthcare outcomes. While graph-based methods have achieved\nstrong predictive performance, most approaches treat drug pairs independently,\noverlooking the complex, context-dependent interactions unique to drug pairs.\nAdditionally, these models struggle to integrate biological interaction\nnetworks and molecular-level structures to provide meaningful mechanistic\ninsights. In this study, we propose MolecBioNet, a novel graph-based framework\nthat integrates molecular and biomedical knowledge for robust and interpretable\nDDI prediction. By modeling drug pairs as unified entities, MolecBioNet\ncaptures both macro-level biological interactions and micro-level molecular\ninfluences, offering a comprehensive perspective on DDIs. The framework\nextracts local subgraphs from biomedical knowledge graphs and constructs\nhierarchical interaction graphs from molecular representations, leveraging\nclassical graph neural network methods to learn multi-scale representations of\ndrug pairs. To enhance accuracy and interpretability, MolecBioNet introduces\ntwo domain-specific pooling strategies: context-aware subgraph pooling\n(CASPool), which emphasizes biologically relevant entities, and\nattention-guided influence pooling (AGIPool), which prioritizes influential\nmolecular substructures. The framework further employs mutual information\nminimization regularization to enhance information diversity during embedding\nfusion. Experimental results demonstrate that MolecBioNet outperforms\nstate-of-the-art methods in DDI prediction, while ablation studies and\nembedding visualizations further validate the advantages of unified drug pair\nmodeling and multi-scale knowledge integration.", "AI": {"tldr": "MolecBioNet\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u56fe\u57fa\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5206\u5b50\u548c\u751f\u7269\u533b\u5b66\u77e5\u8bc6\uff0c\u5c06\u836f\u7269\u5bf9\u89c6\u4e3a\u7edf\u4e00\u5b9e\u4f53\uff0c\u5e76\u91c7\u7528CASPool\u548cAGIPool\u7b49\u7279\u5b9a\u9886\u57df\u6c60\u5316\u7b56\u7565\uff0c\u5728DDI\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u56fe\u57fa\u65b9\u6cd5\u5728DDI\u9884\u6d4b\u4e2d\u72ec\u7acb\u5904\u7406\u836f\u7269\u5bf9\u3001\u5ffd\u89c6\u836f\u7269\u5bf9\u7279\u6709\u7684\u590d\u6742\u4ea4\u4e92\u4f5c\u7528\u4ee5\u53ca\u96be\u4ee5\u6574\u5408\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u7f51\u7edc\u548c\u5206\u5b50\u7ed3\u6784\u4ee5\u63d0\u4f9b\u673a\u5236\u89c1\u89e3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMolecBioNet\u7684\u65b0\u578b\u56fe\u57fa\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u5206\u5b50\u548c\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u8fdb\u884cDDI\u9884\u6d4b\u3002\u8be5\u6846\u67b6\u5c06\u836f\u7269\u5bf9\u89c6\u4e3a\u7edf\u4e00\u5b9e\u4f53\uff0c\u6a21\u62df\u5b8f\u89c2\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u548c\u5fae\u89c2\u5206\u5b50\u5f71\u54cd\u3002\u5b83\u4ece\u751f\u7269\u533b\u5b66\u77e5\u8bc6\u56fe\u4e2d\u63d0\u53d6\u5c40\u90e8\u5b50\u56fe\uff0c\u5e76\u4ece\u5206\u5b50\u8868\u793a\u6784\u5efa\u5c42\u6b21\u4ea4\u4e92\u56fe\uff0c\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u836f\u7269\u5bf9\u7684\u591a\u5c3a\u5ea6\u8868\u793a\u3002\u4e3a\u4e86\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0cMolecBioNet\u5f15\u5165\u4e86\u4e0a\u4e0b\u6587\u611f\u77e5\u5b50\u56fe\u6c60\u5316\uff08CASPool\uff09\u548c\u6ce8\u610f\u529b\u5f15\u5bfc\u5f71\u54cd\u6c60\u5316\uff08AGIPool\uff09\u4e24\u79cd\u7279\u5b9a\u9886\u57df\u7684\u6c60\u5316\u7b56\u7565\uff0c\u5e76\u91c7\u7528\u4e92\u4fe1\u606f\u6700\u5c0f\u5316\u6b63\u5219\u5316\u6765\u589e\u5f3a\u5d4c\u5165\u878d\u5408\u8fc7\u7a0b\u4e2d\u7684\u4fe1\u606f\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMolecBioNet\u5728DDI\u9884\u6d4b\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u6d88\u878d\u7814\u7a76\u548c\u5d4c\u5165\u53ef\u89c6\u5316\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u7edf\u4e00\u836f\u7269\u5bf9\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u77e5\u8bc6\u96c6\u6210\u7684\u4f18\u52bf\u3002", "conclusion": "MolecBioNet\u6846\u67b6\u5728\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e14\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u548c\u5d4c\u5165\u53ef\u89c6\u5316\u9a8c\u8bc1\u4e86\u7edf\u4e00\u836f\u7269\u5bf9\u5efa\u6a21\u548c\u591a\u5c3a\u5ea6\u77e5\u8bc6\u96c6\u6210\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.09420", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09420", "abs": "https://arxiv.org/abs/2507.09420", "authors": ["Timothy Chase Jr", "Karthik Dantu"], "title": "Domain Adaptation and Multi-view Attention for Learnable Landmark Tracking with Sparse Data", "comment": "Presented at the RSS Space Robotics Workshop 2025. Poster available\n  online at https://tjchase34.github.io/assets/pdfs/rss_poster.pdf", "summary": "The detection and tracking of celestial surface terrain features are crucial\nfor autonomous spaceflight applications, including Terrain Relative Navigation\n(TRN), Entry, Descent, and Landing (EDL), hazard analysis, and scientific data\ncollection. Traditional photoclinometry-based pipelines often rely on extensive\na priori imaging and offline processing, constrained by the computational\nlimitations of radiation-hardened systems. While historically effective, these\napproaches typically increase mission costs and duration, operate at low\nprocessing rates, and have limited generalization. Recently, learning-based\ncomputer vision has gained popularity to enhance spacecraft autonomy and\novercome these limitations. While promising, emerging techniques frequently\nimpose computational demands exceeding the capabilities of typical spacecraft\nhardware for real-time operation and are further challenged by the scarcity of\nlabeled training data for diverse extraterrestrial environments. In this work,\nwe present novel formulations for in-situ landmark tracking via detection and\ndescription. We utilize lightweight, computationally efficient neural network\narchitectures designed for real-time execution on current-generation spacecraft\nflight processors. For landmark detection, we propose improved domain\nadaptation methods that enable the identification of celestial terrain features\nwith distinct, cheaply acquired training data. Concurrently, for landmark\ndescription, we introduce a novel attention alignment formulation that learns\nrobust feature representations that maintain correspondence despite significant\nlandmark viewpoint variations. Together, these contributions form a unified\nsystem for landmark tracking that demonstrates superior performance compared to\nexisting state-of-the-art techniques.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u3001\u8f7b\u91cf\u7ea7\u4e14\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u822a\u5929\u5668\u4e0a\u5b9e\u65f6\u68c0\u6d4b\u548c\u8ddf\u8e2a\u5929\u4f53\u5730\u6807\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u89c6\u70b9\u53d8\u5316\u4e0b\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u5149\u8c03\u91cf\u6cd5\u5728\u5904\u7406\u548c\u8ba1\u7b97\u80fd\u529b\u65b9\u9762\u5b58\u5728\u9650\u5236\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u7684\u5148\u9a8c\u6210\u50cf\u548c\u79bb\u7ebf\u5904\u7406\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e9b\u7f3a\u70b9\u5e76\u63d0\u9ad8\u822a\u5929\u5668\u7684\u81ea\u4e3b\u6027\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u822a\u5929\u5668\u786c\u4ef6\u7684\u8ba1\u7b97\u9650\u5236\u5185\u5b9e\u65f6\u8fdb\u884c\u5730\u6807\u8ddf\u8e2a\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u8f7b\u91cf\u7ea7\u3001\u8ba1\u7b97\u6548\u7387\u9ad8\u4e14\u4e13\u4e3a\u5f53\u524d\u4e00\u4ee3\u822a\u5929\u5668\u98de\u884c\u5904\u7406\u5668\u8bbe\u8ba1\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u3002\u5bf9\u4e8e\u5730\u6807\u68c0\u6d4b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u6539\u8fdb\u7684\u57df\u9002\u5e94\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u6613\u4e8e\u83b7\u53d6\u7684\u8bad\u7ec3\u6570\u636e\u6765\u8bc6\u522b\u5929\u4f53\u5730\u5f62\u7279\u5f81\u3002\u5bf9\u4e8e\u5730\u6807\u63cf\u8ff0\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6ce8\u610f\u529b\u5bf9\u9f50\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u4ee5\u5b66\u4e60\u7a33\u5065\u7684\u7279\u5f81\u8868\u793a\uff0c\u4ece\u800c\u5728\u663e\u8457\u7684\u5730\u6807\u89c6\u70b9\u53d8\u5316\u4e0b\u4fdd\u6301\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u8bc6\u522b\u5730\u6807\u548c\u5904\u7406\u89c6\u70b9\u53d8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u80fd\u591f\u5b9e\u65f6\u8fd0\u884c\uff0c\u5e76\u51cf\u5c11\u4e86\u5bf9\u5927\u91cf\u6807\u6ce8\u8bad\u7ec3\u6570\u636e\u7684\u9700\u6c42\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5728\u539f\u4f4d\u901a\u8fc7\u68c0\u6d4b\u548c\u63cf\u8ff0\u8fdb\u884c\u5730\u6807\u8ddf\u8e2a\u7684\u516c\u5f0f\u5316\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10161", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10161", "abs": "https://arxiv.org/abs/2507.10161", "authors": ["Silvie Ill\u00e9sov\u00e1", "Tomasz Rybotycki", "Piotr Gawron", "Martin Beseda"], "title": "On the Importance of Fundamental Properties in Quantum-Classical Machine Learning Models", "comment": null, "summary": "We present a systematic study of how quantum circuit design, specifically the\ndepth of the variational ansatz and the choice of quantum feature mapping,\naffects the performance of hybrid quantum-classical neural networks on a causal\nclassification task. The architecture combines a convolutional neural network\nfor classical feature extraction with a parameterized quantum circuit acting as\nthe quantum layer. We evaluate multiple ansatz depths and nine different\nfeature maps. Results show that increasing the number of ansatz repetitions\nimproves generalization and training stability, though benefits tend to plateau\nbeyond a certain depth. The choice of feature mapping is even more critical:\nonly encodings with multi-axis Pauli rotations enable successful learning,\nwhile simpler maps lead to underfitting or loss of class separability.\nPrincipal Component Analysis and silhouette scores reveal how data\ndistributions evolve across network stages. These findings offer practical\nguidance for designing quantum circuits in hybrid models. All source codes and\nevaluation tools are publicly available.", "AI": {"tldr": "ansatz\u6df1\u5ea6\u548c\u7279\u5f81\u56fe\u9009\u62e9\u5bf9\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u591a\u8f74\u6ce1\u5229\u65cb\u8f6c\u7f16\u7801\u6700\u6709\u6548\u3002", "motivation": "\u7814\u7a76\u91cf\u5b50\u7535\u8def\u8bbe\u8ba1\uff08ansatz\u6df1\u5ea6\u548c\u91cf\u5b50\u7279\u5f81\u6620\u5c04\uff09\u5982\u4f55\u5f71\u54cd\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u795e\u7ecf\u7f51\u7edc\u5728\u56e0\u679c\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "method": "\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\uff0c\u7cfb\u7edf\u7814\u7a76\u4e86ansatz\u6df1\u5ea6\u548c\u7279\u5f81\u56fe\u9009\u62e9\u5bf9\u56e0\u679c\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\u3002", "result": "\u589e\u52a0ansatz\u91cd\u590d\u6b21\u6570\u53ef\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u548c\u8bad\u7ec3\u7a33\u5b9a\u6027\uff0c\u4f46\u6536\u76ca\u4f1a\u8fbe\u5230\u5e73\u53f0\u671f\u3002\u7279\u5f81\u56fe\u7684\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff0c\u53ea\u6709\u5177\u6709\u591a\u8f74\u6ce1\u5229\u65cb\u8f6c\u7684\u7f16\u7801\u624d\u80fd\u5b9e\u73b0\u6210\u529f\u5b66\u4e60\uff0c\u800c\u66f4\u7b80\u5355\u7684\u7f16\u7801\u4f1a\u5bfc\u81f4\u6b20\u62df\u5408\u6216\u7c7b\u53ef\u5206\u79bb\u6027\u4e27\u5931\u3002PCA\u548c\u8f6e\u5ed3\u7cfb\u6570\u63ed\u793a\u4e86\u8de8\u7f51\u7edc\u9636\u6bb5\u7684\u6570\u636e\u5206\u5e03\u6f14\u53d8\u3002", "conclusion": "\u4e3a\u6df7\u5408\u91cf\u5b50-\u7ecf\u5178\u6a21\u578b\u8bbe\u8ba1\u91cf\u5b50\u7535\u8def\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6307\u5bfc\uff0c\u5f3a\u8c03\u4e86ansatz\u6df1\u5ea6\u548c\u7279\u5f81\u56fe\u9009\u62e9\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.10326", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10326", "abs": "https://arxiv.org/abs/2507.10326", "authors": ["Muzhaffar Hazman", "Minh-Khoi Pham", "Shweta Soundararajan", "Goncalo Mordido", "Leonardo Custode", "David Lynch", "Giorgio Cruciata", "Yucheng Shi", "Hongmeng Song", "Wang Chao", "Pan Yue", "Aleksandar Milenovic", "Alexandros Agapitos"], "title": "Grammar-Guided Evolutionary Search for Discrete Prompt Optimisation", "comment": "Accepted for Publication at ECAI 2025", "summary": "Prompt engineering has proven to be a crucial step in leveraging pretrained\nlarge language models (LLMs) in solving various real-world tasks. Numerous\nsolutions have been proposed that seek to automate prompt engineering by using\nthe model itself to edit prompts. However, the majority of state-of-the-art\napproaches are evaluated on tasks that require minimal prompt templates and on\nvery large and highly capable LLMs. In contrast, solving complex tasks that\nrequire detailed information to be included in the prompt increases the amount\nof text that needs to be optimised. Furthermore, smaller models have been shown\nto be more sensitive to prompt design. To address these challenges, we propose\nan evolutionary search approach to automated discrete prompt optimisation\nconsisting of two phases. In the first phase, grammar-guided genetic\nprogramming is invoked to synthesise prompt-creating programmes by searching\nthe space of programmes populated by function compositions of syntactic,\ndictionary-based and LLM-based prompt-editing functions. In the second phase,\nlocal search is applied to explore the neighbourhoods of best-performing\nprogrammes in an attempt to further fine-tune their performance. Our approach\noutperforms three state-of-the-art prompt optimisation approaches,\nPromptWizard, OPRO, and RL-Prompt, on three relatively small general-purpose\nLLMs in four domain-specific challenging tasks. We also illustrate several\nexamples where these benchmark methods suffer relatively severe performance\ndegradation, while our approach improves performance in almost all task-model\ncombinations, only incurring minimal degradation when it does not.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6f14\u5316\u641c\u7d22\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316\u590d\u6742\u4efb\u52a1\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bed\u6cd5\u5f15\u5bfc\u7684\u9057\u4f20\u7f16\u7a0b\u548c\u5c40\u90e8\u641c\u7d22\u6765\u5408\u6210\u548c\u4f18\u5316\u63d0\u793a\u7a0b\u5e8f\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u7684\u81ea\u52a8\u5316\u63d0\u793a\u5de5\u7a0b\u65b9\u6cd5\u4e3b\u8981\u5728\u53ea\u9700\u8981\u5f88\u5c11\u63d0\u793a\u6a21\u677f\u7684\u4efb\u52a1\u548c\u975e\u5e38\u5927\u4e14\u529f\u80fd\u5f3a\u5927\u7684\u8bed\u8a00\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002\u7136\u800c\uff0c\u89e3\u51b3\u9700\u8981\u8be6\u7ec6\u4fe1\u606f\u5305\u542b\u5728\u63d0\u793a\u4e2d\u7684\u590d\u6742\u4efb\u52a1\u4f1a\u589e\u52a0\u9700\u8981\u4f18\u5316\u7684\u6587\u672c\u91cf\u3002\u6b64\u5916\uff0c\u8f83\u5c0f\u7684\u6a21\u578b\u5bf9\u63d0\u793a\u8bbe\u8ba1\u66f4\u4e3a\u654f\u611f\u3002\u4e3a\u4e86\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u590d\u6742\u4efb\u52a1\u7684\u63d0\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7531\u4e24\u4e2a\u9636\u6bb5\u7ec4\u6210\u7684\u81ea\u52a8\u79bb\u6563\u63d0\u793a\u4f18\u5316\u6f14\u5316\u641c\u7d22\u65b9\u6cd5\u3002\u7b2c\u4e00\u9636\u6bb5\u5229\u7528\u8bed\u6cd5\u5f15\u5bfc\u7684\u9057\u4f20\u7f16\u7a0b\uff0c\u901a\u8fc7\u641c\u7d22\u7531\u8bed\u6cd5\u3001\u57fa\u4e8e\u5b57\u5178\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u7f16\u8f91\u51fd\u6570\u7684\u7ec4\u5408\u7ec4\u6210\u7684\u7a0b\u5e8f\u7a7a\u95f4\u6765\u5408\u6210\u521b\u5efa\u63d0\u793a\u7684\u7a0b\u5e8f\u3002\u7b2c\u4e8c\u9636\u6bb5\u5e94\u7528\u5c40\u90e8\u641c\u7d22\u63a2\u7d22\u8868\u73b0\u6700\u4f73\u7a0b\u5e8f\u7684\u90bb\u57df\uff0c\u4ee5\u8fdb\u4e00\u6b65\u4f18\u5316\u5176\u6027\u80fd\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u56db\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e0a\uff0c\u5728\u4e09\u4e2a\u76f8\u5bf9\u8f83\u5c0f\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\u4e0a\uff0c\u4f18\u4e8ePromptWizard\u3001OPRO\u548cRL-Prompt\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1-\u6a21\u578b\u7ec4\u5408\u4e2d\u90fd\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5e76\u4e14\u4ec5\u5728\u5c11\u6570\u60c5\u51b5\u4e0b\u51fa\u73b0\u6700\u5c0f\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u5176\u4ed6\u65b9\u6cd5\u5219\u51fa\u73b0\u6027\u80fd\u4e25\u91cd\u4e0b\u964d\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e09\u4e2a\u76f8\u5bf9\u8f83\u5c0f\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\u548c\u56db\u4e2a\u9886\u57df\u7279\u5b9a\u7684\u6311\u6218\u6027\u4efb\u52a1\u4e0a\uff0c\u4f18\u4e8e\u4e09\u79cd\u6700\u5148\u8fdb\u7684\u63d0\u793a\u4f18\u5316\u65b9\u6cd5\uff08PromptWizard\u3001OPRO\u548cRL-Prompt\uff09\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u51e0\u4e4e\u6240\u6709\u4efb\u52a1-\u6a21\u578b\u7ec4\u5408\u4e2d\u90fd\u80fd\u63d0\u9ad8\u6027\u80fd\uff0c\u4ec5\u5728\u5c11\u6570\u60c5\u51b5\u4e0b\u51fa\u73b0\u6700\u5c0f\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u800c\u57fa\u51c6\u65b9\u6cd5\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\u6027\u80fd\u4f1a\u4e25\u91cd\u4e0b\u964d\u3002"}}
{"id": "2507.09177", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09177", "abs": "https://arxiv.org/abs/2507.09177", "authors": ["Zichen Liu", "Guoji Fu", "Chao Du", "Wee Sun Lee", "Min Lin"], "title": "Continual Reinforcement Learning by Planning with Online World Models", "comment": "ICML 2025 Spotlight", "summary": "Continual reinforcement learning (CRL) refers to a naturalistic setting where\nan agent needs to endlessly evolve, by trial and error, to solve multiple tasks\nthat are presented sequentially. One of the largest obstacles to CRL is that\nthe agent may forget how to solve previous tasks when learning a new task,\nknown as catastrophic forgetting. In this paper, we propose to address this\nchallenge by planning with online world models. Specifically, we learn a\nFollow-The-Leader shallow model online to capture the world dynamics, in which\nwe plan using model predictive control to solve a set of tasks specified by any\nreward functions. The online world model is immune to forgetting by\nconstruction with a proven regret bound of $\\mathcal{O}(\\sqrt{K^2D\\log(T)})$\nunder mild assumptions. The planner searches actions solely based on the latest\nonline model, thus forming a FTL Online Agent (OA) that updates incrementally.\nTo assess OA, we further design Continual Bench, a dedicated environment for\nCRL, and compare with several strong baselines under the same model-planning\nalgorithmic framework. The empirical results show that OA learns continuously\nto solve new tasks while not forgetting old skills, outperforming agents built\non deep world models with various continual learning techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a OA \u7684\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u548c\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6765\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u5728\u4e13\u95e8\u8bbe\u8ba1\u7684 Continual Bench \u73af\u5883\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6210\u679c\u3002", "motivation": "\u89e3\u51b3\u6301\u7eed\u5f3a\u5316\u5b66\u4e60\uff08CRL\uff09\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u5373\u667a\u80fd\u4f53\u5728\u5b66\u4e60\u65b0\u4efb\u52a1\u65f6\u53ef\u80fd\u4f1a\u5fd8\u8bb0\u5982\u4f55\u89e3\u51b3\u65e7\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u89c4\u5212\u7684\u65b9\u6cd5\uff0c\u5177\u4f53\u662f\u5b66\u4e60\u4e00\u4e2a\u5728\u7ebf\u7684 Follow-The-Leader\uff08FTL\uff09\u6d45\u5c42\u6a21\u578b\u6765\u6355\u6349\u4e16\u754c\u52a8\u6001\uff0c\u5e76\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6765\u89c4\u5212\u4ee5\u89e3\u51b3\u7531\u4efb\u4f55\u5956\u52b1\u51fd\u6570\u6307\u5b9a\u7684\u4efb\u52a1\u96c6\u3002\u8be5\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u901a\u8fc7\u5176\u6784\u5efa\u65b9\u5f0f\u5bf9\u9057\u5fd8\u514d\u75ab\uff0c\u5e76\u5728\u6e29\u548c\u5047\u8bbe\u4e0b\u5177\u6709$\\|mathcal {O}(\\sqrt {K^2D\\log (T)})$\u7684\u9057\u61be\u754c\u9650\u3002\u89c4\u5212\u5668\u4ec5\u57fa\u4e8e\u6700\u65b0\u7684\u5728\u7ebf\u6a21\u578b\u8fdb\u884c\u52a8\u4f5c\u641c\u7d22\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u589e\u91cf\u66f4\u65b0\u7684 FTL \u5728\u7ebf\u4ee3\u7406\uff08OA\uff09\u3002", "result": "OA \u80fd\u591f\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u4e14\u4e0d\u5fd8\u8bb0\u65e7\u6280\u80fd\uff0c\u5728 Continual Bench \u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u6df1\u5ea6\u4e16\u754c\u6a21\u578b\u548c\u5404\u79cd\u6301\u7eed\u5b66\u4e60\u6280\u672f\u7684\u667a\u80fd\u4f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5728\u7ebf\u4e16\u754c\u6a21\u578b\u89c4\u5212\u65b9\u6cd5\u5728 Continual Bench \u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u80fd\u591f\u6301\u7eed\u5b66\u4e60\u65b0\u4efb\u52a1\u4e14\u4e0d\u5fd8\u8bb0\u65e7\u6280\u80fd\u3002"}}
{"id": "2507.09459", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09459", "abs": "https://arxiv.org/abs/2507.09459", "authors": ["Zhihan Kang", "Boyu Wang"], "title": "SegVec3D: A Method for Vector Embedding of 3D Objects Oriented Towards Robot manipulation", "comment": "Undergraduate Theis; 12 pages, 6 figures", "summary": "We propose SegVec3D, a novel framework for 3D point cloud instance\nsegmentation that integrates attention mechanisms, embedding learning, and\ncross-modal alignment. The approach builds a hierarchical feature extractor to\nenhance geometric structure modeling and enables unsupervised instance\nsegmentation via contrastive clustering. It further aligns 3D data with natural\nlanguage queries in a shared semantic space, supporting zero-shot retrieval.\nCompared to recent methods like Mask3D and ULIP, our method uniquely unifies\ninstance segmentation and multimodal understanding with minimal supervision and\npractical deployability.", "AI": {"tldr": "SegVec3D \u662f\u4e00\u79cd\u7528\u4e8e 3D \u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u7684\u65b0\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210\u6ce8\u610f\u529b\u3001\u5d4c\u5165\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u6765\u5b9e\u73b0\u65e0\u76d1\u7763\u5206\u5272\u548c\u96f6\u6837\u672c\u68c0\u7d22\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e 3D \u70b9\u4e91\u5b9e\u4f8b\u5206\u5272\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u65e0\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u548c\u8de8\u6a21\u6001\u7406\u89e3\u3002", "method": "SegVec3D \u6846\u67b6\u96c6\u6210\u4e86\u6ce8\u610f\u529b\u673a\u5236\u3001\u5d4c\u5165\u5b66\u4e60\u548c\u8de8\u6a21\u6001\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u6784\u5efa\u4e86\u4e00\u4e2a\u5206\u5c42\u7279\u5f81\u63d0\u53d6\u5668\u6765\u589e\u5f3a\u51e0\u4f55\u7ed3\u6784\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u805a\u7c7b\u5b9e\u73b0\u65e0\u76d1\u7763\u5b9e\u4f8b\u5206\u5272\u3002\u5b83\u8fd8\u5c06 3D \u6570\u636e\u4e0e\u81ea\u7136\u8bed\u8a00\u67e5\u8be2\u5bf9\u9f50\u5230\u4e00\u4e2a\u5171\u4eab\u7684\u8bed\u4e49\u7a7a\u95f4\u4e2d\uff0c\u652f\u6301\u96f6\u6837\u672c\u68c0\u7d22\u3002", "result": "\u4e0e Mask3D \u548c ULIP \u7b49\u6700\u65b0\u65b9\u6cd5\u76f8\u6bd4\uff0cSegVec3D \u5728\u5b9e\u4f8b\u5206\u5272\u548c\u591a\u6a21\u6001\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5177\u6709\u66f4\u5c11\u7684\u76d1\u7763\u548c\u66f4\u597d\u7684\u53ef\u90e8\u7f72\u6027\u3002", "conclusion": "SegVec3D \u6846\u67b6\u6210\u529f\u5730\u5c06\u5b9e\u4f8b\u5206\u5272\u548c\u591a\u6a21\u6001\u7406\u89e3\u7edf\u4e00\u8d77\u6765\uff0c\u5177\u6709\u6700\u5c0f\u7684\u76d1\u7763\u548c\u5b9e\u9645\u90e8\u7f72\u80fd\u529b\u3002"}}
{"id": "2507.09446", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09446", "abs": "https://arxiv.org/abs/2507.09446", "authors": ["Yuanhong Zheng", "Ruixuan Yu", "Jian Sun"], "title": "Efficient Multi-Person Motion Prediction by Lightweight Spatial and Temporal Interactions", "comment": "ICCV 2025", "summary": "3D multi-person motion prediction is a highly complex task, primarily due to\nthe dependencies on both individual past movements and the interactions between\nagents. Moreover, effectively modeling these interactions often incurs\nsubstantial computational costs. In this work, we propose a computationally\nefficient model for multi-person motion prediction by simplifying spatial and\ntemporal interactions. Our approach begins with the design of lightweight dual\nbranches that learn local and global representations for individual and\nmultiple persons separately. Additionally, we introduce a novel cross-level\ninteraction block to integrate the spatial and temporal representations from\nboth branches. To further enhance interaction modeling, we explicitly\nincorporate the spatial inter-person distance embedding. With above efficient\ntemporal and spatial design, we achieve state-of-the-art performance for\nmultiple metrics on standard datasets of CMU-Mocap, MuPoTS-3D, and 3DPW, while\nsignificantly reducing the computational cost. Code is available at\nhttps://github.com/Yuanhong-Zheng/EMPMP.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u76843D\u591a\u4eba\u8fd0\u52a8\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u7b80\u5316\u4ea4\u4e92\u548c\u5f15\u5165\u8de8\u7ea7\u522b\u4ea4\u4e92\u5757\uff0c\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "3D\u591a\u4eba\u8fd0\u52a8\u9884\u6d4b\u7531\u4e8e\u4e2a\u4f53\u8fc7\u53bb\u8fd0\u52a8\u548c\u667a\u80fd\u4f53\u4e4b\u95f4\u4ea4\u4e92\u7684\u4f9d\u8d56\u6027\u800c\u53d8\u5f97\u975e\u5e38\u590d\u6742\u3002\u6b64\u5916\uff0c\u6709\u6548\u6a21\u62df\u8fd9\u4e9b\u4ea4\u4e92\u901a\u5e38\u4f1a\u4ea7\u751f\u9ad8\u6602\u7684\u8ba1\u7b97\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u9ad8\u6548\u7684\u6a21\u578b\uff0c\u901a\u8fc7\u7b80\u5316\u7a7a\u95f4\u548c\u65f6\u95f4\u4ea4\u4e92\u3002\u8bbe\u8ba1\u4e86\u8f7b\u91cf\u7ea7\u53cc\u5206\u652f\u6765\u5206\u522b\u5b66\u4e60\u4e2a\u4f53\u548c\u591a\u4e2a\u4e2a\u4f53\u7684\u5c40\u90e8\u548c\u5168\u5c40\u8868\u793a\u3002\u5f15\u5165\u4e86\u65b0\u9896\u7684\u8de8\u7ea7\u522b\u4ea4\u4e92\u5757\u6765\u6574\u5408\u4e24\u4e2a\u5206\u652f\u7684\u65f6\u7a7a\u8868\u793a\u3002\u660e\u786e\u52a0\u5165\u4e86\u7a7a\u95f4\u4eba\u9645\u8ddd\u79bb\u5d4c\u5165\u6765\u589e\u5f3a\u4ea4\u4e92\u5efa\u6a21\u3002", "result": "\u5728CMU-Mocap\u3001MuPoTS-3D\u548c3DPW\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728CMU-Mocap\u3001MuPoTS-3D\u548c3DPW\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002"}}
{"id": "2507.10227", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10227", "abs": "https://arxiv.org/abs/2507.10227", "authors": ["Artur Czerwinski"], "title": "Quantum Perspective on Digital Money: Towards a Quantum-Powered Financial System", "comment": null, "summary": "Quantum money represents an innovative approach to currency by encoding\neconomic value within the quantum states of physical systems, utilizing the\nprinciples of quantum mechanics to enhance security, integrity, and\ntransferability. This perspective article explores the definition and\nproperties of quantum money. We analyze the process of transferring quantum\nmoney via quantum teleportation, using terrestrial and satellite-based quantum\nnetworks. Furthermore, we consider the impact of quantum money on the modern\nbanking system, particularly in money creation. Finally, we conduct an analysis\nto assess the strengths and weaknesses of quantum money, as well as\nopportunities and threats associated with this emerging concept.", "AI": {"tldr": "Quantum money uses quantum states for secure currency. This paper analyzes its transfer, banking impact, and SWOT.", "motivation": "The motivation is to explore the concept of quantum money, which encodes economic value in quantum states for enhanced security, integrity, and transferability, and to analyze its implications for the modern financial system.", "method": "This perspective article analyzes quantum money by exploring its definition, properties, transfer process using quantum teleportation over quantum networks (terrestrial and satellite-based), its impact on modern banking (especially money creation), and an assessment of its strengths, weaknesses, opportunities, and threats.", "result": "The analysis covers the definition and properties of quantum money, the transfer process via quantum teleportation across quantum networks, the impact on banking and money creation, and an SWOT analysis of the concept.", "conclusion": "Quantum money, encoding economic value in quantum states, offers enhanced security and integrity. Its transfer via quantum networks and potential impact on banking, particularly money creation, are analyzed. Strengths, weaknesses, opportunities, and threats are assessed."}}
{"id": "2507.10330", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10330", "abs": "https://arxiv.org/abs/2507.10330", "authors": ["Mohammed Bouri", "Adnane Saoud"], "title": "Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach", "comment": "Accepted to ACL Findings 2025", "summary": "Despite advancements in Natural Language Processing (NLP), models remain\nvulnerable to adversarial attacks, such as synonym substitutions. While prior\nwork has focused on improving robustness for feed-forward and convolutional\narchitectures, the robustness of recurrent networks and modern state space\nmodels (SSMs), such as S4, remains understudied. These architectures pose\nunique challenges due to their sequential processing and complex parameter\ndynamics. In this paper, we introduce a novel regularization technique based on\nGrowth Bound Matrices (GBM) to improve NLP model robustness by reducing the\nimpact of input perturbations on model outputs. We focus on computing the GBM\nfor three architectures: Long Short-Term Memory (LSTM), State Space models\n(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance\nresilience against word substitution attacks, (2) improve generalization on\nclean text, and (3) providing the first systematic analysis of SSM (S4)\nrobustness. Extensive experiments across multiple architectures and benchmark\ndatasets demonstrate that our method improves adversarial robustness by up to\n8.8% over existing baselines. These results highlight the effectiveness of our\napproach, outperforming several state-of-the-art methods in adversarial\ndefense. Codes are available at https://github.com/BouriMohammed/GBM", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u589e\u957f\u754c\u77e9\u9635\uff08GBM\uff09\u7684\u65b0\u578b\u6b63\u5219\u5316\u6280\u672f\uff0c\u65e8\u5728\u589e\u5f3aLSTM\u3001S4\u548cCNN\u7b49NLP\u6a21\u578b\u62b5\u5fa1\u540c\u4e49\u8bcd\u66ff\u6362\u7b49\u5bf9\u6297\u6027\u653b\u51fb\u7684\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728S4\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5206\u6790\u65b9\u9762\u53d6\u5f97\u4e86\u521d\u6b65\u6210\u679c\u3002", "motivation": "\u73b0\u6709NLP\u6a21\u578b\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u7279\u522b\u662f\u540c\u4e49\u8bcd\u66ff\u6362\u3002\u7136\u800c\uff0c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u548c\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\uff08\u5982S4\uff09\u7b49\u73b0\u4ee3\u67b6\u6784\u7684\u9c81\u68d2\u6027\u7814\u7a76\u4e0d\u8db3\uff0c\u5b83\u4eec\u7531\u4e8e\u5e8f\u5217\u5904\u7406\u548c\u590d\u6742\u7684\u53c2\u6570\u52a8\u6001\u800c\u5e26\u6765\u72ec\u7279\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u957f\u754c\u77e9\u9635\uff08GBM\uff09\u7684\u6b63\u5219\u5316\u6280\u672f\uff0c\u7528\u4e8e\u8ba1\u7b97LSTM\u3001S4\u548cCNN\u67b6\u6784\u7684GBM\uff0c\u4ee5\u63d0\u9ad8NLP\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5728\u591a\u4e2a\u67b6\u6784\u548c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u5c06\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u63d0\u9ad8\u4e86\u9ad8\u8fbe8.8%\uff0c\u5e76\u4e14\u5728\u5bf9\u6297\u9632\u5fa1\u65b9\u9762\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u589e\u957f\u754c\u77e9\u9635\uff08GBM\uff09\u7684\u65b0\u578b\u6b63\u5219\u5316\u6280\u672f\uff0c\u4ee5\u63d0\u9ad8NLP\u6a21\u578b\uff08\u7279\u522b\u662fLSTM\u3001S4\u548cCNN\uff09\u5728\u9762\u5bf9\u540c\u4e49\u8bcd\u66ff\u6362\u7b49\u5bf9\u6297\u6027\u653b\u51fb\u65f6\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u65b9\u9762\u6bd4\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e868.8%\uff0c\u5e76\u5728\u5bf9\u6297\u9632\u5fa1\u65b9\u9762\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.09202", "categories": ["cs.LG", "cs.AI", "physics.ao-ph"], "pdf": "https://arxiv.org/pdf/2507.09202", "abs": "https://arxiv.org/abs/2507.09202", "authors": ["Wuxin Wang", "Weicheng Ni", "Lilan Huang", "Tao Hao", "Ben Fei", "Shuo Ma", "Taikang Yuan", "Yanlai Zhao", "Kefeng Deng", "Xiaoyong Li", "Boheng Duan", "Lei Bai", "Kaijun Ren"], "title": "XiChen: An observation-scalable fully AI-driven global weather forecasting system with 4D variational knowledge", "comment": null, "summary": "Recent advancements in Artificial Intelligence (AI) demonstrate significant\npotential to revolutionize weather forecasting. However, most AI-driven models\nrely on Numerical Weather Prediction (NWP) systems for initial condition\npreparation, which often consumes hours on supercomputers. Here we introduce\nXiChen, the first observation-scalable fully AI-driven global weather\nforecasting system, whose entire pipeline, from Data Assimilation (DA) to\nmedium-range forecasting, can be accomplished within only 17 seconds. XiChen is\nbuilt upon a foundation model that is pre-trained for weather forecasting.\nMeanwhile, this model is subsequently fine-tuned to serve as both observation\noperators and DA models, thereby scalably assimilating conventional and raw\nsatellite observations. Furthermore, the integration of four-dimensional\nvariational knowledge ensures that XiChen's DA and medium-range forecasting\naccuracy rivals that of operational NWP systems, amazingly achieving a skillful\nforecasting lead time exceeding 8.25 days. These findings demonstrate that\nXiChen holds strong potential toward fully AI-driven weather forecasting\nindependent of NWP systems.", "AI": {"tldr": "XiChen\u662f\u4e00\u4e2a\u9769\u65b0\u6027\u7684\u3001\u5b8c\u5168\u7531AI\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\uff0c\u80fd\u572817\u79d2\u5185\u5b8c\u6210\u6570\u636e\u540c\u5316\u548c\u4e2d\u7a0b\u9884\u62a5\uff0c\u9884\u6d4b\u7cbe\u5ea6\u5ab2\u7f8eNWP\u7cfb\u7edf\u4e14\u63d0\u524d\u91cf\u8d85\u8fc78.25\u5929\u3002", "motivation": "\u5f53\u524d\u7684AI\u5929\u6c14\u9884\u62a5\u6a21\u578b\u5927\u591a\u4f9d\u8d56NWP\u7cfb\u7edf\u8fdb\u884c\u521d\u59cb\u6761\u4ef6\u51c6\u5907\uff0c\u800c\u8be5\u8fc7\u7a0b\u8017\u65f6\u8f83\u957f\u3002", "method": "XiChen\u662f\u4e00\u4e2a\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684AI\u57fa\u7840\u6a21\u578b\u6784\u5efa\u7684\u3001\u5b8c\u5168\u7531AI\u9a71\u52a8\u7684\u5168\u7403\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\uff0c\u5176\u6570\u636e\u540c\u5316\u548c\u4e2d\u7a0b\u9884\u62a5\u7684\u6574\u4e2a\u6d41\u7a0b\u53ef\u572817\u79d2\u5185\u5b8c\u6210\u3002\u8be5\u6a21\u578b\u88ab\u5fae\u8c03\u7528\u4f5c\u89c2\u6d4b\u7b97\u5b50\u548c\u6570\u636e\u540c\u5316\u6a21\u578b\uff0c\u80fd\u591f\u6269\u5c55\u5730\u540c\u5316\u5e38\u89c4\u548c\u539f\u59cb\u536b\u661f\u89c2\u6d4b\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4e86\u56db\u7ef4\u53d8\u5206\u77e5\u8bc6\u3002", "result": "XiChen\u7684\u6570\u636e\u540c\u5316\u548c\u4e2d\u7a0b\u9884\u62a5\u7cbe\u5ea6\u53ef\u4e0e\u8fd0\u884c\u4e2d\u7684NWP\u7cfb\u7edf\u76f8\u5ab2\u7f8e\uff0c\u9884\u6d4b\u63d0\u524d\u91cf\u8d85\u8fc78.25\u5929\u3002", "conclusion": "XiChen\u5c55\u793a\u4e86\u5b8c\u5168\u7531\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u3001\u72ec\u7acb\u4e8eNWP\u7cfb\u7edf\u7684\u5929\u6c14\u9884\u62a5\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.10233", "categories": ["quant-ph", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10233", "abs": "https://arxiv.org/abs/2507.10233", "authors": ["Debnath Ghosh", "Soumit Roy", "Prithwi Bagchi", "Indranil Chakrabarty", "Ashok Kumar Das"], "title": "Secure and Efficient Quantum Signature Scheme Based on the Controlled Unitary Operations Encryption", "comment": "22 pages, 3 figures. Accepted in Quantum Information Processing", "summary": "Quantum digital signatures ensure unforgeable message authenticity and\nintegrity using quantum principles, offering unconditional security against\nboth classical and quantum attacks. They are crucial for secure communication\nin high-stakes environments, ensuring trust and long-term protection in the\nquantum era. Nowadays, the majority of arbitrated quantum signature (AQS)\nprotocols encrypt data qubit by qubit using the quantum one-time pad (QOTP).\nDespite providing robust data encryption, QOTP is not a good fit for AQS\nbecause of its susceptibility to many types of attacks. In this work, we\npresent an efficient AQS protocol to encrypt quantum message ensembles using a\ndistinct encryption technique, the chained controlled unitary operations. In\ncontrast to existing protocols, our approach successfully prevents disavowal\nand forgery attacks. We hope this contributes to advancing future\ninvestigations into the development of AQS protocols.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u6570\u5b57\u7b7e\u540d\u534f\u8bae\uff0c\u901a\u8fc7\u4f7f\u7528\u94fe\u5f0f\u53d7\u63a7\u9149\u64cd\u4f5c\u52a0\u5bc6\u91cf\u5b50\u6d88\u606f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u534f\u8bae\u4e2d\u91cf\u5b50\u4e00\u6b21\u6027\u5bc6\u7801\u672c\u7684\u5f31\u70b9\uff0c\u63d0\u9ad8\u4e86\u5b89\u5168\u6027\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u4ef2\u88c1\u91cf\u5b50\u7b7e\u540d\uff08AQS\uff09\u534f\u8bae\u5927\u591a\u4f7f\u7528\u91cf\u5b50\u4e00\u6b21\u6027\u5bc6\u7801\u672c\uff08QOTP\uff09\u52a0\u5bc6\u6570\u636e\uff0c\u4f46QOTP\u5bb9\u6613\u53d7\u5230\u591a\u79cd\u653b\u51fb\uff0c\u4e0d\u9002\u5408AQS\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u66f4\u6709\u6548\u7684AQS\u534f\u8bae\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AQS\u534f\u8bae\uff0c\u4f7f\u7528\u94fe\u5f0f\u53d7\u63a7\u9149\u64cd\u4f5c\u6765\u52a0\u5bc6\u91cf\u5b50\u6d88\u606f\u96c6\u5408\uff0c\u800c\u4e0d\u662f\u9010\u4e2a\u6570\u636e\u6bd4\u7279\u7684\u91cf\u5b50\u4e00\u6b21\u6027\u5bc6\u7801\u672c\uff08QOTP\uff09\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684AQS\u534f\u8bae\uff0c\u4f7f\u7528\u94fe\u5f0f\u53d7\u63a7\u9149\u64cd\u4f5c\uff0c\u80fd\u591f\u6709\u6548\u9632\u6b62\u62b5\u8d56\u548c\u4f2a\u9020\u653b\u51fb\uff0c\u4e0e\u73b0\u6709\u57fa\u4e8eQOTP\u7684\u534f\u8bae\u76f8\u6bd4\u5177\u6709\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684AQS\u534f\u8bae\uff0c\u4f7f\u7528\u94fe\u5f0f\u53d7\u63a7\u9149\u64cd\u4f5c\u6765\u52a0\u5bc6\u91cf\u5b50\u6d88\u606f\u96c6\u5408\uff0c\u6709\u6548\u9632\u6b62\u62b5\u8d56\u548c\u4f2a\u9020\u653b\u51fb\uff0c\u6709\u671b\u63a8\u52a8AQS\u534f\u8bae\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u3002"}}
{"id": "2507.10342", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10342", "abs": "https://arxiv.org/abs/2507.10342", "authors": ["Rosa Illan Castillo", "Javier Valenzuela"], "title": "Using AI to replicate human experimental results: a motion study", "comment": null, "summary": "This paper explores the potential of large language models (LLMs) as reliable\nanalytical tools in linguistic research, focusing on the emergence of affective\nmeanings in temporal expressions involving manner-of-motion verbs. While LLMs\nlike GPT-4 have shown promise across a range of tasks, their ability to\nreplicate nuanced human judgements remains under scrutiny. We conducted four\npsycholinguistic studies (on emergent meanings, valence shifts, verb choice in\nemotional contexts, and sentence-emoji associations) first with human\nparticipants and then replicated the same tasks using an LLM. Results across\nall studies show a striking convergence between human and AI responses, with\nstatistical analyses (e.g., Spearman's rho = .73-.96) indicating strong\ncorrelations in both rating patterns and categorical choices. While minor\ndivergences were observed in some cases, these did not alter the overall\ninterpretative outcomes. These findings offer compelling evidence that LLMs can\naugment traditional human-based experimentation, enabling broader-scale studies\nwithout compromising interpretative validity. This convergence not only\nstrengthens the empirical foundation of prior human-based findings but also\nopens possibilities for hypothesis generation and data expansion through AI.\nUltimately, our study supports the use of LLMs as credible and informative\ncollaborators in linguistic inquiry.", "AI": {"tldr": "LLMs\u5728\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u5206\u6790\u60c5\u611f\u610f\u4e49\u65b9\u9762\uff0c\u4e0e\u4eba\u7c7b\u7684\u5224\u65ad\u8868\u73b0\u51fa\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u53ef\u4f5c\u4e3a\u53ef\u9760\u7684\u534f\u4f5c\u8005\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u4f5c\u4e3a\u53ef\u9760\u5206\u6790\u5de5\u5177\u7684\u6f5c\u529b\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u6d89\u53ca\u8fd0\u52a8\u65b9\u5f0f\u52a8\u8bcd\u7684\u65f6\u95f4\u8868\u8fbe\u4e2d\u60c5\u611f\u610f\u4e49\u7684\u51fa\u73b0\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u56db\u9879\u5fc3\u7406\u8bed\u8a00\u5b66\u7814\u7a76\uff0c\u9996\u5148\u5bf9\u4eba\u7c7b\u53c2\u4e0e\u8005\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7136\u540e\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u590d\u5236\u76f8\u540c\u7684\u4efb\u52a1\uff0c\u4ee5\u63a2\u8ba8LLMs\u5728\u65f6\u95f4\u8868\u8fbe\u4e2d\u60c5\u611f\u610f\u4e49\u7684\u51fa\u73b0\u4f5c\u4e3a\u5206\u6790\u5de5\u5177\u7684\u6f5c\u529b\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u6240\u6709\u7814\u7a76\u4e2d\uff0c\u4eba\u7c7b\u548cAI\u7684\u53cd\u5e94\u8868\u73b0\u51fa\u60ca\u4eba\u7684\u4e00\u81f4\u6027\u3002\u7edf\u8ba1\u5206\u6790\uff08\u4f8b\u5982\uff0c\u65af\u76ae\u5c14\u66fc\u7b49\u7ea7\u76f8\u5173\u7cfb\u6570 rho = 0.73-0.96\uff09\u8868\u660e\uff0c\u5728\u8bc4\u5206\u6a21\u5f0f\u548c\u5206\u7c7b\u9009\u62e9\u65b9\u9762\u90fd\u5b58\u5728\u5f88\u5f3a\u7684\u76f8\u5173\u6027\u3002\u867d\u7136\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u89c2\u5bdf\u5230\u4e86\u4e00\u4e9b\u7ec6\u5fae\u7684\u5dee\u5f02\uff0c\u4f46\u8fd9\u4e9b\u5dee\u5f02\u5e76\u672a\u6539\u53d8\u6574\u4f53\u7684\u89e3\u91ca\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u7684\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u4f5c\u4e3a\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u53ef\u9760\u7684\u5206\u6790\u5de5\u5177\uff0c\u80fd\u591f\u589e\u5f3a\u4f20\u7edf\u7684\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u5b9e\u73b0\u66f4\u5927\u89c4\u6a21\u7684\u7814\u7a76\uff0c\u5e76\u4e14\u4e0d\u635f\u5bb3\u89e3\u91ca\u7684\u6709\u6548\u6027\u3002LLMs\u53ef\u4ee5\u4f5c\u4e3a\u8bed\u8a00\u5b66\u63a2\u7a76\u4e2d\u53ef\u4fe1\u4e14\u4fe1\u606f\u4e30\u5bcc\u7684\u534f\u4f5c\u8005\u3002"}}
{"id": "2507.09211", "categories": ["cs.LG", "physics.ao-ph", "physics.data-an", "physics.geo-ph", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09211", "abs": "https://arxiv.org/abs/2507.09211", "authors": ["Xinyue Liu", "Xiao Peng", "Shuyue Yan", "Yuntian Chen", "Dongxiao Zhang", "Zhixiao Niu", "Hui-Min Wang", "Xiaogang He"], "title": "Capturing Unseen Spatial Extremes Through Knowledge-Informed Generative Modeling", "comment": null, "summary": "Observed records of climate extremes provide an incomplete picture of risk,\nmissing \"unseen\" extremes that exceed historical bounds. In parallel,\nneglecting spatial dependence undervalues the risk of synchronized hazards that\namplify impacts. To address these challenges, we develop DeepX-GAN\n(Dependence-Enhanced Embedding for Physical eXtremes - Generative Adversarial\nNetwork), a knowledge-informed deep generative model designed to better capture\nthe spatial structure of rare extremes. The zero-shot generalizability of\nDeepX-GAN enables simulation of unseen extremes that fall outside historical\nexperience yet remain statistically plausible. We define two types of unseen\nextremes: \"checkmate\" extremes that directly hit targets, and \"stalemate\"\nextremes that narrowly miss. These unrealized scenarios expose latent risks in\nfragile systems and may reinforce a false sense of resilience if overlooked.\nNear misses, in particular, can prompt either proactive adaptation or dangerous\ncomplacency, depending on how they are interpreted. Applying DeepX-GAN to the\nMiddle East and North Africa (MENA), we find that these unseen extremes\ndisproportionately affect regions with high vulnerability and low socioeconomic\nreadiness, but differ in urgency and interpretation. Future warming could\nexpand and redistribute these unseen extremes, with emerging exposure hotspots\nin Indo-Pakistan and Central Africa. This distributional shift highlights\ncritical blind spots in conventional hazard planning and underscores the need\nto develop spatially adaptive policies that anticipate emergent risk hotspots\nrather than simply extrapolating from historical patterns.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepX-GAN\u7684\u65b0\u578b\u751f\u6210\u6a21\u578b\uff0c\u7528\u4e8e\u6a21\u62df\u548c\u5206\u6790\u8d85\u51fa\u5386\u53f2\u8bb0\u5f55\u8303\u56f4\u7684\u201c\u672a\u88ab\u770b\u89c1\u201d\u7684\u6c14\u5019\u6781\u7aef\u4e8b\u4ef6\uff0c\u5e76\u8003\u8651\u4e86\u7a7a\u95f4\u4f9d\u8d56\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8fd9\u4e9b\u672a\u88ab\u770b\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\u5bf9\u8106\u5f31\u5730\u533a\u5f71\u54cd\u66f4\u5927\uff0c\u672a\u6765\u53ef\u80fd\u52a0\u5267\uff0c\u5e76\u5f3a\u8c03\u4e86\u5236\u5b9a\u9002\u5e94\u6027\u653f\u7b56\u4ee5\u5e94\u5bf9\u65b0\u5174\u98ce\u9669\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u89c2\u5bdf\u5230\u7684\u6c14\u5019\u6781\u7aef\u4e8b\u4ef6\u8bb0\u5f55\u4e0d\u5b8c\u6574\uff0c\u5ffd\u7565\u4e86\u8d85\u51fa\u5386\u53f2\u8303\u56f4\u7684\u201c\u672a\u88ab\u770b\u89c1\u201d\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u5e76\u4e14\u5ffd\u89c6\u7a7a\u95f4\u4f9d\u8d56\u6027\u4f4e\u4f30\u4e86\u540c\u6b65\u5371\u5bb3\u7684\u98ce\u9669\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepX-GAN\u7684\u77e5\u8bc6\u589e\u5f3a\u6df1\u5ea6\u751f\u6210\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u589e\u5f3a\u5bf9\u7269\u7406\u6781\u7aef\u4e8b\u4ef6\u7684\u4f9d\u8d56\u6027\u5d4c\u5165\uff0c\u5e76\u901a\u8fc7\u751f\u6210\u5bf9\u6297\u7f51\u7edc\u6765\u66f4\u597d\u5730\u6355\u6349\u7a00\u6709\u6781\u7aef\u4e8b\u4ef6\u7684\u7a7a\u95f4\u7ed3\u6784\u3002\u8be5\u6a21\u578b\u5177\u6709\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\uff0c\u53ef\u4ee5\u6a21\u62df\u8d85\u51fa\u5386\u53f2\u7ecf\u9a8c\u4f46\u7edf\u8ba1\u4e0a\u5408\u7406\u7684\u672a\u88ab\u770b\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\u3002", "result": "DeepX-GAN\u80fd\u591f\u6a21\u62df\u672a\u88ab\u770b\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u63ed\u793a\u4e86\u8106\u5f31\u7cfb\u7edf\u4e2d\u7684\u6f5c\u5728\u98ce\u9669\u3002\u5728\u4e2d\u4e1c\u548c\u5317\u975e\u5730\u533a\u7684\u5e94\u7528\u8868\u660e\uff0c\u672a\u88ab\u770b\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\u5bf9\u8106\u5f31\u6027\u9ad8\u548c\u4f4e\u793e\u4f1a\u7ecf\u6d4e\u51c6\u5907\u7684\u5730\u533a\u5f71\u54cd\u5c24\u4e3a\u4e25\u91cd\uff0c\u4f46\u5176\u7d27\u8feb\u6027\u548c\u89e3\u91ca\u65b9\u5f0f\u6709\u6240\u4e0d\u540c\u3002\u672a\u6765\u7684\u53d8\u6696\u53ef\u80fd\u6269\u5927\u548c\u91cd\u65b0\u5206\u914d\u8fd9\u4e9b\u672a\u88ab\u770b\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u65b0\u5174\u66b4\u9732\u70ed\u70b9\u51fa\u73b0\u5728\u5370\u5ea6-\u5df4\u57fa\u65af\u5766\u548c\u4e2d\u975e\u3002", "conclusion": "\u672a\u6765\u7684\u6c14\u5019\u53d8\u5316\u53ef\u80fd\u6269\u5927\u548c\u91cd\u65b0\u5206\u914d\u8fd9\u4e9b\u672a\u88ab\u770b\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\uff0c\u5728\u4e2d\u4e1c\u548c\u5317\u975e\u5730\u533a\u5c24\u5176\u5982\u6b64\u3002\u8fd9\u4e9b\u672a\u88ab\u770b\u89c1\u7684\u6781\u7aef\u4e8b\u4ef6\u5bf9\u8106\u5f31\u6027\u9ad8\u548c\u4f4e\u793e\u4f1a\u7ecf\u6d4e\u51c6\u5907\u7684\u5730\u533a\u5f71\u54cd\u5c24\u4e3a\u4e25\u91cd\u3002\u5370\u5ea6-\u5df4\u57fa\u65af\u5766\u548c\u4e2d\u975e\u7b49\u5730\u7684\u65b0\u5174\u66b4\u9732\u70ed\u70b9\u4e5f\u8868\u660e\uff0c\u4f20\u7edf\u7684\u98ce\u9669\u89c4\u5212\u5b58\u5728\u4e25\u91cd\u7684\u76f2\u70b9\uff0c\u9700\u8981\u5236\u5b9a\u9002\u5e94\u6027\u5f3a\u7684\u653f\u7b56\u6765\u5e94\u5bf9\u65b0\u5174\u98ce\u9669\u3002"}}
{"id": "2507.09471", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09471", "abs": "https://arxiv.org/abs/2507.09471", "authors": ["Lingfeng He", "De Cheng", "Zhiheng Ma", "Huaijie Wang", "Dingwen Zhang", "Nannan Wang", "Xinbo Gao"], "title": "CKAA: Cross-subspace Knowledge Alignment and Aggregation for Robust Continual Learning", "comment": null, "summary": "Continual Learning (CL) empowers AI models to continuously learn from\nsequential task streams. Recently, parameter-efficient fine-tuning (PEFT)-based\nCL methods have garnered increasing attention due to their superior\nperformance. They typically allocate a unique sub-module for learning each\ntask, with a task recognizer to select the appropriate sub-modules for testing\nimages. However, due to the feature subspace misalignment from independently\ntrained sub-modules, these methods tend to produce ambiguous decisions under\nmisleading task-ids. To address this, we propose Cross-subspace Knowledge\nAlignment and Aggregation (CKAA), a novel framework that enhances model\nrobustness against misleading task-ids through two key innovations: (1)\nDual-level Knowledge Alignment (DKA): By aligning intra-class feature\ndistributions across different subspaces and learning a robust global\nclassifier through a feature simulation process, DKA enables the model to\ndistinguish features from both correct and incorrect subspaces during training.\n(2) Task-Confidence-guided Mixture of Adapters (TC-MoA): A robust inference\nscheme that adaptively aggregates task-specific knowledge from relevant\nsub-modules based on task-confidence scores, avoiding overconfidence in\nmisleading task-id predictions. Extensive experiments demonstrate that CKAA\noutperforms existing PEFT-based CL methods.", "AI": {"tldr": "CKAA\u6846\u67b6\u901a\u8fc7\u5bf9\u9f50\u8de8\u5b50\u7a7a\u95f4\u7279\u5f81\u5206\u5e03\u548c\u4efb\u52a1\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u77e5\u8bc6\u805a\u5408\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u8fde\u7eed\u5b66\u4e60\u4e2d\u5e94\u5bf9\u9519\u8bef\u4efb\u52a1ID\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8ePEFT\u7684\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5206\u914d\u72ec\u7279\u7684\u5b50\u6a21\u5757\uff0c\u5e76\u4f7f\u7528\u4efb\u52a1\u8bc6\u522b\u5668\u5728\u6d4b\u8bd5\u65f6\u9009\u62e9\u5b50\u6a21\u5757\u3002\u7136\u800c\uff0c\u7531\u4e8e\u72ec\u7acb\u8bad\u7ec3\u7684\u5b50\u6a21\u5757\u5b58\u5728\u7279\u5f81\u5b50\u7a7a\u95f4\u4e0d\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5728\u9762\u5bf9\u8bef\u5bfc\u6027\u4efb\u52a1ID\u65f6\u5bb9\u6613\u505a\u51fa\u6a21\u7cca\u7684\u51b3\u7b56\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u589e\u5f3a\u6a21\u578b\u5728\u8bef\u5bfc\u6027\u4efb\u52a1ID\u4e0b\u7684\u9c81\u68d2\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u8de8\u5b50\u7a7a\u95f4\u77e5\u8bc6\u5bf9\u9f50\u4e0e\u805a\u5408\uff08CKAA\uff09\u7684\u65b0\u578b\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1. \u53cc\u5c42\u77e5\u8bc6\u5bf9\u9f50\uff08DKA\uff09\uff1a\u901a\u8fc7\u5bf9\u9f50\u4e0d\u540c\u5b50\u7a7a\u95f4\u4e2d\u7684\u7c7b\u5185\u7279\u5f81\u5206\u5e03\uff0c\u5e76\u5229\u7528\u7279\u5f81\u6a21\u62df\u8fc7\u7a0b\u5b66\u4e60\u5168\u5c40\u5206\u7c7b\u5668\uff0c\u4f7f\u5f97\u6a21\u578b\u5728\u8bad\u7ec3\u65f6\u80fd\u591f\u533a\u5206\u6b63\u786e\u548c\u9519\u8bef\u5b50\u7a7a\u95f4\u4e2d\u7684\u7279\u5f81\u30022. \u4efb\u52a1\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9002\u914d\u5668\u6df7\u5408\uff08TC-MoA\uff09\uff1a\u4e00\u79cd\u9c81\u68d2\u7684\u63a8\u7406\u65b9\u6848\uff0c\u6839\u636e\u4efb\u52a1\u7f6e\u4fe1\u5ea6\u5206\u6570\u81ea\u9002\u5e94\u5730\u805a\u5408\u7279\u5b9a\u4efb\u52a1\u7684\u77e5\u8bc6\uff0c\u907f\u514d\u5728\u9519\u8bef\u4efb\u52a1ID\u9884\u6d4b\u4e0a\u8fc7\u5ea6\u81ea\u4fe1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCKAA\u6846\u67b6\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8ePEFT\u7684\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "CKAA\u6846\u67b6\u901a\u8fc7\u53cc\u5c42\u77e5\u8bc6\u5bf9\u9f50\uff08DKA\uff09\u548c\u4efb\u52a1\u7f6e\u4fe1\u5ea6\u5f15\u5bfc\u7684\u9002\u914d\u5668\u6df7\u5408\uff08TC-MoA\uff09\u7684\u521b\u65b0\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u9519\u8bef\u4efb\u52a1ID\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u8fde\u7eed\u5b66\u4e60\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8ePEFT\u7684\u8fde\u7eed\u5b66\u4e60\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10252", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10252", "abs": "https://arxiv.org/abs/2507.10252", "authors": ["Daniel Davidovich", "Boyang Ma", "Adi Goldner", "Shimon Cohen", "Zhaopin Chen", "Michael Kr\u00fcger"], "title": "Tracing attosecond currents and controlling their direction in a scanning tunneling microscope", "comment": "21 pages, 5 figures, supplementary information available on request", "summary": "The tunneling effect driven by waveform-controlled laser pulses is a\ncornerstone of attosecond science and contributes decisively to its extreme\ntime resolution. In the spatial domain, electron tunneling through a\nbias-driven junction between a nanotip and a surface enables atomic-scale\nspatial resolution in scanning tunneling microscopy (STM). Recently, first\nsignatures of attosecond modulation of STM currents have shown that\nsimultaneous attosecond-nanometer resolution is feasible. However, while\nsub-cycle attosecond dynamics are routinely controlled and determined with high\nprecision, controlling the direction of the currents and determining their\nduration have remained elusive in STM. Here, we induce STM tunneling currents\nusing two-color laser pulses and dynamically control their direction, relying\nsolely on the sub-cycle waveform of the pulses. Projecting our measurement data\nonto numerical and analytical solutions of the time-dependent Schr\\\"odinger\nequation reveals non-adiabatic tunneling as the underlying physical mechanism,\nyielding current burst durations on the order of 900 as. Despite working under\nambient conditions but free from thermal artifacts, we achieve a lateral\nspatial resolution of 2 nm and sub-angstr\\\"om topographic sensitivity.\nDirectional control of attosecond bursts in STM will enable triggering and\ndetecting ultrafast charge dynamics in molecular systems and defect states at\nthe spatio-temporal frontier of microscopy.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728STM\u4e2d\u901a\u8fc7\u53cc\u8272\u6fc0\u5149\u8109\u51b2\u63a7\u5236\u4e86\u963f\u79d2\u96a7\u9053\u7535\u6d41\u7684\u65b9\u5411\uff0c\u5b9e\u73b0\u4e86900\u963f\u79d2\u7684\u7535\u6d41\u6301\u7eed\u65f6\u95f4\u548c2\u7eb3\u7c73\u7684\u7a7a\u95f4\u5206\u8fa8\u7387\uff0c\u4e3a\u8d85\u5feb\u7535\u8377\u52a8\u529b\u5b66\u7814\u7a76\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u5c3d\u7ba1\u4e9a\u5468\u671f\u963f\u79d2\u52a8\u529b\u5b66\u5728STM\u4e2d\u5df2\u5f97\u5230\u7cbe\u786e\u63a7\u5236\u548c\u6d4b\u91cf\uff0c\u4f46\u63a7\u5236\u7535\u6d41\u65b9\u5411\u548c\u786e\u5b9a\u5176\u6301\u7eed\u65f6\u95f4\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63a2\u7d22\u540c\u65f6\u5b9e\u73b0\u963f\u79d2\u548c\u7eb3\u7c73\u7ea7\u5206\u8fa8\u7387\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u53cc\u8272\u6fc0\u5149\u8109\u51b2\u8bf1\u5bfcSTM\u96a7\u9053\u7535\u6d41\uff0c\u5e76\u4ec5\u4f9d\u9760\u4e9a\u5468\u671f\u6ce2\u5f62\u52a8\u6001\u63a7\u5236\u5176\u65b9\u5411\u3002\u5229\u7528\u4e0e\u65f6\u95f4\u76f8\u5173\u859b\u5b9a\u8c14\u65b9\u7a0b\u7684\u6570\u503c\u548c\u89e3\u6790\u89e3\u5bf9\u6d4b\u91cf\u6570\u636e\u8fdb\u884c\u6295\u5f71\uff0c\u63ed\u793a\u4e86\u975e\u7edd\u70ed\u96a7\u9053\u6548\u5e94\u662f\u5176\u5e95\u5c42\u7684\u7269\u7406\u673a\u5236\u3002", "result": "\u7814\u7a76\u5b9e\u73b0\u4e86\u5bf9STM\u96a7\u9053\u7535\u6d41\u65b9\u5411\u7684\u52a8\u6001\u63a7\u5236\uff0c\u5e76\u786e\u5b9a\u4e86\u7535\u6d41\u731d\u53d1\u6301\u7eed\u65f6\u95f4\u7ea6\u4e3a900\u963f\u79d2\u3002\u5728\u73af\u5883\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u4e862\u7eb3\u7c73\u7684\u6a2a\u5411\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u4e9a\u57c3\u7c73\u6258\u7ea7\u7684\u5f62\u8c8c\u7075\u654f\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u5728STM\u4e2d\u5bf9\u963f\u79d2\u96a7\u9053\u7535\u6d41\u7684\u5b9a\u5411\u63a7\u5236\uff0c\u5e76\u8fbe\u5230\u4e862\u7eb3\u7c73\u7684\u6a2a\u5411\u7a7a\u95f4\u5206\u8fa8\u7387\u548c\u4e9a\u57c3\u7c73\u6258\u7ea7\u7684\u5f62\u8c8c\u7075\u654f\u5ea6\u3002\u8fd9\u79cd\u63a7\u5236\u5c06\u4e3a\u5728\u5206\u5b50\u7cfb\u7edf\u548c\u7f3a\u9677\u6001\u4e2d\u89e6\u53d1\u548c\u63a2\u6d4b\u8d85\u5feb\u7535\u8377\u52a8\u529b\u5b66\u63d0\u4f9b\u53ef\u80fd\uff0c\u4ece\u800c\u5728\u663e\u5fae\u5b66\u7684\u65f6\u7a7a\u524d\u6cbf\u5b9e\u73b0\u65b0\u7684\u7a81\u7834\u3002"}}
{"id": "2507.10354", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10354", "abs": "https://arxiv.org/abs/2507.10354", "authors": ["Silvia Cappa", "Anna Sofia Lippolis", "Stefano Zoia"], "title": "Meanings are like Onions: a Layered Approach to Metaphor Processing", "comment": null, "summary": "Metaphorical meaning is not a flat mapping between concepts, but a complex\ncognitive phenomenon that integrates multiple levels of interpretation. In this\npaper, we propose a stratified model of metaphor processing that treats meaning\nas an onion: a multi-layered structure comprising (1) content analysis, (2)\nconceptual blending, and (3) pragmatic intentionality. This three-dimensional\nframework allows for a richer and more cognitively grounded approach to\nmetaphor interpretation in computational systems. At the first level, metaphors\nare annotated through basic conceptual elements. At the second level, we model\nconceptual combinations, linking components to emergent meanings. Finally, at\nthe third level, we introduce a pragmatic vocabulary to capture speaker intent,\ncommunicative function, and contextual effects, aligning metaphor understanding\nwith pragmatic theories. By unifying these layers into a single formal\nframework, our model lays the groundwork for computational methods capable of\nrepresenting metaphorical meaning beyond surface associations, toward deeper,\nmore context-sensitive reasoning.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e09\u5c42\u9690\u55bb\u5904\u7406\u6a21\u578b\uff08\u5185\u5bb9\u5206\u6790\u3001\u6982\u5ff5\u878d\u5408\u3001\u8bed\u7528\u610f\u56fe\uff09\uff0c\u4ee5\u5b9e\u73b0\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7684\u8ba1\u7b97\u9690\u55bb\u89e3\u91ca\u3002", "motivation": "\u9690\u55bb\u610f\u4e49\u4e0d\u662f\u6982\u5ff5\u4e4b\u95f4\u7684\u5e73\u9762\u6620\u5c04\uff0c\u800c\u662f\u4e00\u79cd\u590d\u6742\u7684\u8ba4\u77e5\u73b0\u8c61\uff0c\u5b83\u6574\u5408\u4e86\u591a\u4e2a\u89e3\u91ca\u5c42\u6b21\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63d0\u51fa\u4e00\u4e2a\u80fd\u591f\u66f4\u4e30\u5bcc\u3001\u66f4\u7b26\u5408\u8ba4\u77e5\u5730\u89e3\u91ca\u9690\u55bb\u7684\u8ba1\u7b97\u7cfb\u7edf\u6a21\u578b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u9690\u55bb\u5904\u7406\u6a21\u578b\uff0c\u5c06\u610f\u4e49\u89c6\u4e3a\u4e00\u4e2a\u591a\u5c42\u7ed3\u6784\uff0c\u5305\u62ec\uff081\uff09\u5185\u5bb9\u5206\u6790\u3001\uff082\uff09\u6982\u5ff5\u878d\u5408\u548c\uff083\uff09\u8bed\u7528\u610f\u56fe\u3002\u5728\u6b64\u6846\u67b6\u4e2d\uff0c\u9690\u55bb\u9996\u5148\u901a\u8fc7\u57fa\u672c\u6982\u5ff5\u5143\u7d20\u8fdb\u884c\u6ce8\u91ca\uff0c\u7136\u540e\u5bf9\u6982\u5ff5\u7ec4\u5408\u8fdb\u884c\u5efa\u6a21\u4ee5\u94fe\u63a5\u7ec4\u4ef6\u548c\u65b0\u5174\u610f\u4e49\uff0c\u6700\u540e\u5f15\u5165\u8bed\u7528\u8bcd\u6c47\u6765\u6355\u83b7\u8bf4\u8bdd\u8005\u610f\u56fe\u3001\u6c9f\u901a\u529f\u80fd\u548c\u4e0a\u4e0b\u6587\u6548\u5e94\u3002", "result": "\u901a\u8fc7\u5c06\u8fd9\u4e9b\u5c42\u6b21\u7edf\u4e00\u5230\u5355\u4e00\u7684\u6b63\u5f0f\u6846\u67b6\u4e2d\uff0c\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u8ba1\u7b97\u7cfb\u7edf\u5bf9\u9690\u55bb\u7684\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7684\u63a8\u7406\u3002", "conclusion": "\u8be5\u6a21\u578b\u4e3a\u8d85\u8d8a\u8868\u9762\u8054\u60f3\u3001\u5b9e\u73b0\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7684\u63a8\u7406\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u80fd\u591f\u8ba1\u7b97\u8868\u793a\u6bd4\u8868\u9762\u8054\u60f3\u66f4\u6df1\u5c42\u6b21\u3001\u66f4\u5177\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u7684\u9690\u55bb\u610f\u4e49\u3002"}}
{"id": "2507.09212", "categories": ["cs.LG", "cs.CV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09212", "abs": "https://arxiv.org/abs/2507.09212", "authors": ["Jonas Scholz", "Richard E. Turner"], "title": "Warm Starts Accelerate Generative Modelling", "comment": "10 pages, 6 figures", "summary": "Iterative generative models, like diffusion and flow-matching, create\nhigh-fidelity samples by progressively refining a noise vector into data.\nHowever, this process is notoriously slow, often requiring hundreds of function\nevaluations. We introduce the warm-start model, a simple, deterministic model\nthat dramatically accelerates conditional generation by providing a better\nstarting point. Instead of starting generation from an uninformed N(0, I)\nprior, our warm-start model predicts an informed prior N(mu, sigma), whose\nmoments are conditioned on the input context. This \"warm start\" substantially\nreduces the distance the generative process must traverse, particularly when\nthe conditioning information is strongly informative. On tasks like image\ninpainting, our method achieves results competitive with a 1000-step DDPM\nbaseline using only 11 total function evaluations (1 for the warm start, 10 for\ngeneration). A simple conditional normalization trick makes our method\ncompatible with any standard generative model and sampler without modification,\nallowing it to be combined with other efficient sampling techniques for further\nacceleration. Our implementation is available at\nhttps://github.com/jonas-scholz123/warm-start-model.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u4e00\u4e2a\u4fe1\u606f\u6027\u5148\u9a8c\uff08N(\u03bc, \u03c3)\uff09\u6765\u66ff\u4ee3\u65e0\u4fe1\u606f\u5148\u9a8c\uff08N(0, I)\uff09\uff0c\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u8fed\u4ee3\u751f\u6210\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8fed\u4ee3\u751f\u6210\u6a21\u578b\uff08\u5982\u6269\u6563\u548c\u6d41\u5339\u914d\u6a21\u578b\uff09\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u6837\u672c\u65f6\u9700\u8981\u8fdb\u884c\u5927\u91cf\u51fd\u6570\u8bc4\u4f30\uff08\u901a\u5e38\u662f\u6570\u767e\u6b21\uff09\u5bfc\u81f4\u901f\u5ea6\u7f13\u6162\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u6e29\u70ed\u542f\u52a8\u6a21\u578b\u201d\u7684\u7b80\u5355\u786e\u5b9a\u6027\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u9884\u6d4b\u4e00\u4e2a\u4fe1\u606f\u6027\u5148\u9a8c N(\u03bc, \u03c3) \u6765\u66ff\u4ee3\u4ece\u65e0\u4fe1\u606f\u7684 N(0, I)\u5148\u9a8c\u5f00\u59cb\u751f\u6210\uff0c\u5176\u77e9\u6839\u636e\u8f93\u5165\u4e0a\u4e0b\u6587\u8fdb\u884c\u6761\u4ef6\u5316\u3002", "result": "\u5728\u56fe\u50cf\u4fee\u590d\u7b49\u4efb\u52a1\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528 11 \u6b21\u51fd\u6570\u8bc4\u4f30\uff081 \u6b21\u7528\u4e8e\u6e29\u70ed\u542f\u52a8\uff0c10 \u6b21\u7528\u4e8e\u751f\u6210\uff09\u7684\u60c5\u51b5\u4e0b\uff0c\u53d6\u5f97\u4e86\u4e0e\u4f7f\u7528 1000 \u6b21\u8fed\u4ee3\u7684 DDPM \u57fa\u7ebf\u76f8\u5f53\u7684\u7ed3\u679c\u3002\u901a\u8fc7\u4e00\u79cd\u7b80\u5355\u7684\u6761\u4ef6\u5f52\u4e00\u5316\u6280\u5de7\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u4e0e\u4efb\u4f55\u6807\u51c6\u7684\u751f\u6210\u6a21\u578b\u548c\u91c7\u6837\u5668\u517c\u5bb9\uff0c\u65e0\u9700\u4fee\u6539\uff0c\u4ece\u800c\u53ef\u4ee5\u4e0e\u5176\u4ed6\u9ad8\u6548\u91c7\u6837\u6280\u672f\u7ed3\u5408\u4ee5\u5b9e\u73b0\u8fdb\u4e00\u6b65\u52a0\u901f\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u66f4\u597d\u7684\u8d77\u70b9\uff0c\u53ef\u4ee5\u663e\u8457\u52a0\u901f\u6761\u4ef6\u751f\u6210"}}
{"id": "2507.09487", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09487", "abs": "https://arxiv.org/abs/2507.09487", "authors": ["Changli Wang", "Fang Yin", "Jiafeng Liu", "Rui Wu"], "title": "HMID-Net: An Exploration of Masked Image Modeling and Knowledge Distillation in Hyperbolic Space", "comment": null, "summary": "Visual and semantic concepts are often structured in a hierarchical manner.\nFor instance, textual concept `cat' entails all images of cats. A recent study,\nMERU, successfully adapts multimodal learning techniques from Euclidean space\nto hyperbolic space, effectively capturing the visual-semantic hierarchy.\nHowever, a critical question remains: how can we more efficiently train a model\nto capture and leverage this hierarchy? In this paper, we propose the\n\\textit{Hyperbolic Masked Image and Distillation Network} (HMID-Net), a novel\nand efficient method that integrates Masked Image Modeling (MIM) and knowledge\ndistillation techniques within hyperbolic space. To the best of our knowledge,\nthis is the first approach to leverage MIM and knowledge distillation in\nhyperbolic space to train highly efficient models. In addition, we introduce a\ndistillation loss function specifically designed to facilitate effective\nknowledge transfer in hyperbolic space. Our experiments demonstrate that MIM\nand knowledge distillation techniques in hyperbolic space can achieve the same\nremarkable success as in Euclidean space. Extensive evaluations show that our\nmethod excels across a wide range of downstream tasks, significantly\noutperforming existing models like MERU and CLIP in both image classification\nand retrieval.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10272", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10272", "abs": "https://arxiv.org/abs/2507.10272", "authors": ["Kaifeng Bu", "Bikun Li"], "title": "Efficient Measurement of Bosonic Non-Gaussianity", "comment": "7+12 pages", "summary": "Non-Gaussian states are essential resources in quantum information\nprocessing. In this work, we investigate methods for quantifying bosonic\nnon-Gaussianity in many-body systems. Building on recent theoretical insights\ninto the self-convolution properties of bosonic pure states, we introduce\nnon-Gaussian entropy as a new measure to characterize non-Gaussianity in\nbosonic pure states. We further propose a practical protocol for measuring\nnon-Gaussian entropy using three beam splitters and four copies of the input\nstate. In addition, we extend this framework to mixed states, providing a\ngeneral approach to quantifying non-Gaussianity. Our results offer a convenient\nand efficient method for characterizing bosonic non-Gaussianity, paving the way\nfor its implementation on near-term experimental platforms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u73bb\u8272\u5b50\u975e\u9ad8\u65af\u6027\u7684\u65b9\u6cd5\uff08\u975e\u9ad8\u65af\u71b5\uff09\u53ca\u5176\u6d4b\u91cf\u65b9\u6848\uff0c\u5e76\u63a8\u5e7f\u81f3\u6df7\u5408\u6001\u3002", "motivation": "\u7814\u7a76\u91cf\u5316\u73bb\u8272\u5b50\u591a\u4f53\u7cfb\u7edf\u4e2d\u7684\u975e\u9ad8\u65af\u6027\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u975e\u9ad8\u65af\u6001\u662f\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u7684\u5173\u952e\u8d44\u6e90\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u73bb\u8272\u5b50\u7eaf\u6001\u7684\u81ea\u5377\u79ef\u6027\u8d28\uff0c\u5f15\u5165\u975e\u9ad8\u65af\u71b5\u4f5c\u4e3a\u91cf\u5316\u6307\u6807\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u675f\u5668\u548c\u591a\u526f\u672c\u6001\u7684\u6d4b\u91cf\u534f\u8bae\u3002", "result": "\u6210\u529f\u5f15\u5165\u975e\u9ad8\u65af\u71b5\u4f5c\u4e3a\u91cf\u5316\u73bb\u8272\u5b50\u7eaf\u6001\u975e\u9ad8\u65af\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u5177\u4f53\u7684\u6d4b\u91cf\u65b9\u6848\uff0c\u540c\u65f6\u5c06\u8be5\u65b9\u6cd5\u63a8\u5e7f\u5230\u6df7\u5408\u6001\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5316\u65b9\u6cd5\u2014\u2014\u975e\u9ad8\u65af\u71b5\uff0c\u7528\u4e8e\u8868\u5f81\u73bb\u8272\u5b50\u7eaf\u6001\u7684\u975e\u9ad8\u65af\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e09\u4e2a\u5206\u675f\u5668\u548c\u56db\u4e2a\u8f93\u5165\u6001\u526f\u672c\u7684\u5b9e\u9645\u6d4b\u91cf\u65b9\u6848\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u5df2\u6269\u5c55\u5230\u6df7\u5408\u6001\uff0c\u4e3a\u91cf\u5316\u975e\u9ad8\u65af\u6027\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u3002"}}
{"id": "2507.10435", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10435", "abs": "https://arxiv.org/abs/2507.10435", "authors": ["Xinnan Dai", "Kai Yang", "Jay Revolinsky", "Kai Guo", "Aoran Wang", "Bohang Zhang", "Jiliang Tang"], "title": "From Sequence to Structure: Uncovering Substructure Reasoning in Transformers", "comment": null, "summary": "Recent studies suggest that large language models (LLMs) possess the\ncapability to solve graph reasoning tasks. Notably, even when graph structures\nare embedded within textual descriptions, LLMs can still effectively answer\nrelated questions. This raises a fundamental question: How can a decoder-only\nTransformer architecture understand underlying graph structures? To address\nthis, we start with the substructure extraction task, interpreting the inner\nmechanisms inside the transformers and analyzing the impact of the input\nqueries. Specifically, through both empirical results and theoretical analysis,\nwe present Induced Substructure Filtration (ISF), a perspective that captures\nthe substructure identification in the multi-layer transformers. We further\nvalidate the ISF process in LLMs, revealing consistent internal dynamics across\nlayers. Building on these insights, we explore the broader capabilities of\nTransformers in handling diverse graph types. Specifically, we introduce the\nconcept of thinking in substructures to efficiently extract complex composite\npatterns, and demonstrate that decoder-only Transformers can successfully\nextract substructures from attributed graphs, such as molecular graphs.\nTogether, our findings offer a new insight on how sequence-based Transformers\nperform the substructure extraction task over graph data.", "AI": {"tldr": "Transformer\u53ef\u4ee5\u50cf\u4eba\u7c7b\u4e00\u6837\u201c\u4ee5\u5b50\u7ed3\u6784\u601d\u8003\u201d\u6765\u7406\u89e3\u56fe\u6570\u636e\u3002ISF\u7406\u8bba\u89e3\u91ca\u4e86Transformer\u5982\u4f55\u901a\u8fc7\u5176\u591a\u5c42\u7ed3\u6784\u8bc6\u522b\u548c\u63d0\u53d6\u56fe\u4e2d\u7684\u5b50\u7ed3\u6784\uff0c\u5373\u4f7f\u56fe\u7ed3\u6784\u662f\u4ee5\u6587\u672c\u5f62\u5f0f\u7ed9\u51fa\u7684\u3002\u7814\u7a76\u8868\u660e\uff0c\u8fd9\u79cd\u80fd\u529b\u4e0d\u4ec5\u9650\u4e8e\u7279\u5b9a\u7c7b\u578b\u7684\u56fe\uff0c\u800c\u4e14\u5728\u4e0d\u540c\u5c42\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\uff0c\u4e3a\u7406\u89e3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u975e\u5e8f\u5217\u6570\u636e\uff08\u5982\u56fe\u6570\u636e\uff09\u65f6\u7684\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "\u63a2\u8ba8Transformer\u67b6\u6784\uff08\u7279\u522b\u662fdecoder-only Transformer\uff09\u5982\u4f55\u5728\u56fe\u7ed3\u6784\u6570\u636e\u5d4c\u5165\u6587\u672c\u63cf\u8ff0\u65f6\uff0c\u4ecd\u80fd\u6709\u6548\u7406\u89e3\u5e76\u89e3\u51b3\u56fe\u63a8\u7406\u4efb\u52a1\u3002", "method": "\u901a\u8fc7\u5b50\u7ed3\u6784\u63d0\u53d6\u4efb\u52a1\u6765\u89e3\u6790Transformer\u7684\u5185\u90e8\u673a\u5236\uff0c\u5e76\u5206\u6790\u8f93\u5165\u67e5\u8be2\u7684\u5f71\u54cd\u3002\u63d0\u51faISF\uff08Induced Substructure Filtration\uff09\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\uff0c\u89e3\u91caTransformer\u5982\u4f55\u8bc6\u522b\u5b50\u7ed3\u6784\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eLLM\u4ee5\u9a8c\u8bc1\u5176\u8de8\u5c42\u4e00\u81f4\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63a2\u7d22Transformer\u5904\u7406\u4e0d\u540c\u7c7b\u578b\u56fe\u7684\u80fd\u529b\uff0c\u5e76\u63d0\u51fa\u201c\u4ee5\u5b50\u7ed3\u6784\u601d\u8003\u201d\u7684\u6982\u5ff5\u6765\u63d0\u53d6\u590d\u6742\u6a21\u5f0f\uff0c\u5c24\u5176\u662f\u5728\u5c5e\u6027\u56fe\uff08\u5982\u5206\u5b50\u56fe\uff09\u4e0a\u8fdb\u884c\u5b50\u7ed3\u6784\u63d0\u53d6\u3002", "result": "\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86ISF\uff08Induced Substructure Filtration\uff09\u89c6\u89d2\uff0c\u89e3\u91ca\u4e86Transformer\u5982\u4f55\u5728\u591a\u5c42\u7ed3\u6784\u4e2d\u8bc6\u522b\u5b50\u7ed3\u6784\uff0c\u5e76\u5c55\u793a\u4e86LLM\u4e2d\u5b58\u5728\u4e00\u81f4\u7684\u5185\u90e8\u52a8\u6001\u3002\u8bc1\u660e\u4e86Transformer\u53ef\u4ee5\u4ece\u5c5e\u6027\u56fe\u4e2d\u63d0\u53d6\u5b50\u7ed3\u6784\uff0c\u5e76\u63d0\u51fa\u201c\u4ee5\u5b50\u7ed3\u6784\u601d\u8003\u201d\u7684\u6982\u5ff5\u4ee5\u63d0\u9ad8\u6548\u7387\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7ISF\u89c6\u89d2\u63ed\u793a\u4e86Transformer\u5728\u5904\u7406\u56fe\u7ed3\u6784\u6570\u636e\u65f6\u7684\u5185\u90e8\u673a\u5236\uff0c\u7279\u522b\u662f\u5176\u5728\u5b50\u7ed3\u6784\u63d0\u53d6\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u5c55\u793a\u4e86\u8fd9\u79cd\u80fd\u529b\u5728\u4e0d\u540c\u7c7b\u578b\u56fe\u4e0a\u7684\u6cdb\u5316\u6027\uff0c\u4e3a\u7406\u89e3\u57fa\u4e8e\u5e8f\u5217\u7684\u6a21\u578b\u5982\u4f55\u5904\u7406\u56fe\u6570\u636e\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2507.09213", "categories": ["cs.LG", "stat.ML", "68T07"], "pdf": "https://arxiv.org/pdf/2507.09213", "abs": "https://arxiv.org/abs/2507.09213", "authors": ["Dunsheng Huang", "Dong Shen", "Lei Lu", "Ying Tan"], "title": "Optimizing Basis Function Selection in Constructive Wavelet Neural Networks and Its Applications", "comment": "17pages", "summary": "Wavelet neural network (WNN), which learns an unknown nonlinear mapping from\nthe data, has been widely used in signal processing, and time-series analysis.\nHowever, challenges in constructing accurate wavelet bases and high\ncomputational costs limit their application. This study introduces a\nconstructive WNN that selects initial bases and trains functions by introducing\nnew bases for predefined accuracy while reducing computational costs. For the\nfirst time, we analyze the frequency of unknown nonlinear functions and select\nappropriate initial wavelets based on their primary frequency components by\nestimating the energy of the spatial frequency component. This leads to a novel\nconstructive framework consisting of a frequency estimator and a wavelet-basis\nincrease mechanism to prioritize high-energy bases, significantly improving\ncomputational efficiency. The theoretical foundation defines the necessary\ntime-frequency range for high-dimensional wavelets at a given accuracy. The\nframework's versatility is demonstrated through four examples: estimating\nunknown static mappings from offline data, combining two offline datasets,\nidentifying time-varying mappings from time-series data, and capturing\nnonlinear dependencies in real time-series data. These examples showcase the\nframework's broad applicability and practicality. All the code will be released\nat https://github.com/dshuangdd/CWNN.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u9896\u7684\u7ed3\u6784\u5316\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u9891\u7387\u4f30\u8ba1\u548c\u4f18\u5316\u57fa\u9009\u62e9\u6765\u63d0\u9ad8\u6548\u7387\u548c\u7cbe\u5ea6\u3002", "motivation": "\u4f20\u7edf\u7684WNN\u5728\u6784\u5efa\u7cbe\u786e\u5c0f\u6ce2\u57fa\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\uff08WNN\uff09\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u57fa\u6765\u5b66\u4e60\u672a\u77e5\u975e\u7ebf\u6027\u6620\u5c04\uff0c\u5e76\u901a\u8fc7\u9891\u7387\u4f30\u8ba1\u6765\u9009\u62e9\u521d\u59cb\u57fa\uff0c\u4ece\u800c\u5728\u4fdd\u8bc1\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u4f18\u5148\u9009\u62e9\u9ad8\u80fd\u57fa\uff0c\u5728\u56db\u4e2a\u793a\u4f8b\u4e2d\uff08\u4f30\u8ba1\u9759\u6001\u6620\u5c04\u3001\u5408\u5e76\u6570\u636e\u96c6\u3001\u8bc6\u522b\u65f6\u53d8\u6620\u5c04\u3001\u6355\u83b7\u975e\u7ebf\u6027\u4f9d\u8d56\u5173\u7cfb\uff09\u5c55\u793a\u4e86\u5176\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b0\u9896\u7684\u7ed3\u6784\u5316\u5c0f\u6ce2\u795e\u7ecf\u7f51\u7edc\u6846\u67b6\u901a\u8fc7\u9891\u7387\u4f30\u8ba1\u548c\u57fa\u4e8e\u4f18\u5148\u9009\u62e9\u9ad8\u80fd\u57fa\u7684\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u7684\u901a\u7528\u6027\u548c\u5b9e\u7528\u6027\u3002"}}
{"id": "2507.09491", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09491", "abs": "https://arxiv.org/abs/2507.09491", "authors": ["Yiyang Zhou", "Linjie Li", "Shi Qiu", "Zhengyuan Yang", "Yuyang Zhao", "Siwei Han", "Yangfan He", "Kangqi Li", "Haonian Ji", "Zihao Zhao", "Haibo Tong", "Lijuan Wang", "Huaxiu Yao"], "title": "GLIMPSE: Do Large Vision-Language Models Truly Think With Videos or Just Glimpse at Them?", "comment": "15 pages, 10 figures", "summary": "Existing video benchmarks often resemble image-based benchmarks, with\nquestion types like \"What actions does the person perform throughout the\nvideo?\" or \"What color is the woman's dress in the video?\" For these, models\ncan often answer by scanning just a few key frames, without deep temporal\nreasoning. This limits our ability to assess whether large vision-language\nmodels (LVLMs) can truly think with videos rather than perform superficial\nframe-level analysis. To address this, we introduce GLIMPSE, a benchmark\nspecifically designed to evaluate whether LVLMs can genuinely think with\nvideos. Unlike prior benchmarks, GLIMPSE emphasizes comprehensive video\nunderstanding beyond static image cues. It consists of 3,269 videos and over\n4,342 highly visual-centric questions across 11 categories, including\nTrajectory Analysis, Temporal Reasoning, and Forensics Detection. All questions\nare carefully crafted by human annotators and require watching the entire video\nand reasoning over full video context-this is what we mean by thinking with\nvideo. These questions cannot be answered by scanning selected frames or\nrelying on text alone. In human evaluations, GLIMPSE achieves 94.82% accuracy,\nbut current LVLMs face significant challenges. Even the best-performing model,\nGPT-o3, reaches only 66.43%, highlighting that LVLMs still struggle to move\nbeyond surface-level reasoning to truly think with videos.", "AI": {"tldr": "GLIMPSE\u662f\u4e00\u4e2a\u65b0\u7684\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u89c6\u9891\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u6df1\u5ea6\u7406\u89e3\u89c6\u9891\u5185\u5bb9\u65b9\u9762\u8868\u73b0\u4e0d\u4f73\u3002", "motivation": "\u73b0\u6709\u89c6\u9891\u57fa\u51c6\u6d4b\u8bd5\u6a21\u4eff\u56fe\u50cf\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f7f\u5f97\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u626b\u63cf\u5173\u952e\u5e27\u800c\u975e\u8fdb\u884c\u6df1\u5ea6\u65f6\u5e8f\u63a8\u7406\u6765\u56de\u7b54\u95ee\u9898\uff0c\u8fd9\u963b\u788d\u4e86\u5bf9LVLMs\u89c6\u9891\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\u3002", "method": "\u63d0\u51faGLIMPSE\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b3,269\u4e2a\u89c6\u9891\u548c4,342\u4e2a\u95ee\u9898\uff0c\u6db5\u76d6\u8f68\u8ff9\u5206\u6790\u3001\u65f6\u95f4\u63a8\u7406\u548c\u53f8\u6cd5\u9274\u5b9a\u7b4911\u4e2a\u7c7b\u522b\uff0c\u65e8\u5728\u8bc4\u4f30LVLMs\u7684\u6df1\u5ea6\u65f6\u5e8f\u63a8\u7406\u80fd\u529b\uff0c\u800c\u975e\u8868\u9762\u5e27\u5206\u6790\u3002", "result": "GLIMPSE\u5728\u4eba\u7c7b\u8bc4\u4f30\u4e2d\u8fbe\u523094.82%\u7684\u51c6\u786e\u7387\uff0c\u800c\u5f53\u524d\u8868\u73b0\u6700\u4f73\u7684LVLM\uff08GPT-o3\uff09\u4ec5\u8fbe\u523066.43%\uff0c\u8868\u660eLVLMs\u5728\u771f\u6b63\u7406\u89e3\u548c\u63a8\u7406\u89c6\u9891\u5185\u5bb9\u65b9\u9762\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "GLIMPSE\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u5f53\u524d\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u5728\u8fdb\u884c\u89c6\u9891\u5185\u5bb9\u6df1\u5ea6\u5206\u6790\u65b9\u9762\u4ecd\u5b58\u5728\u663e\u8457\u6311\u6218\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u4e5f\u96be\u4ee5\u8d85\u8d8a\u8868\u9762\u63a8\u7406\u7684\u5c40\u9650\u3002"}}
{"id": "2507.10445", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10445", "abs": "https://arxiv.org/abs/2507.10445", "authors": ["Chris Madge", "Matthew Purver", "Massimo Poesio"], "title": "Referential ambiguity and clarification requests: comparing human and LLM behaviour", "comment": null, "summary": "In this work we examine LLMs' ability to ask clarification questions in\ntask-oriented dialogues that follow the asynchronous\ninstruction-giver/instruction-follower format. We present a new corpus that\ncombines two existing annotations of the Minecraft Dialogue Corpus -- one for\nreference and ambiguity in reference, and one for SDRT including clarifications\n-- into a single common format providing the necessary information to\nexperiment with clarifications and their relation to ambiguity. With this\ncorpus we compare LLM actions with original human-generated clarification\nquestions, examining how both humans and LLMs act in the case of ambiguity. We\nfind that there is only a weak link between ambiguity and humans producing\nclarification questions in these dialogues, and low correlation between humans\nand LLMs. Humans hardly ever produce clarification questions for referential\nambiguity, but often do so for task-based uncertainty. Conversely, LLMs produce\nmore clarification questions for referential ambiguity, but less so for task\nuncertainty. We question if LLMs' ability to ask clarification questions is\npredicated on their recent ability to simulate reasoning, and test this with\ndifferent reasoning approaches, finding that reasoning does appear to increase\nquestion frequency and relevancy.", "AI": {"tldr": "This paper analyzes LLMs' ability to ask clarification questions in task-oriented dialogues, comparing them to humans. A new corpus was created using the Minecraft Dialogue Corpus. Results show humans ask clarifications for task uncertainty while LLMs ask for referential ambiguity. LLM reasoning improves question frequency and relevance.", "motivation": "This research aims to examine the capability of Large Language Models (LLMs) in seeking clarification questions within task-oriented dialogues, specifically in an asynchronous instruction-giver/instruction-follower setup. It also seeks to understand how LLMs' clarification-seeking behavior compares to that of humans when faced with ambiguity.", "method": "The study utilizes a newly created corpus that integrates two existing annotations of the Minecraft Dialogue Corpus. This corpus provides information on reference, ambiguity, and SDRT, including clarifications, to facilitate experiments on clarifications and their relationship with ambiguity. The researchers compare LLM-generated clarification questions with human-generated ones to analyze their behavior in ambiguous situations.", "result": "The study found that the link between ambiguity and humans asking clarification questions is weak. Humans rarely ask clarification questions for referential ambiguity but frequently do so for task-based uncertainty. LLMs, on the other hand, ask more clarification questions for referential ambiguity and fewer for task uncertainty. The research also indicated that LLMs' reasoning abilities positively impact the frequency and relevance of their clarification questions.", "conclusion": "LLMs' ability to ask clarification questions is related to their reasoning capabilities. While humans tend to ask clarification questions for task uncertainty, LLMs focus more on referential ambiguity. There is a weak link between ambiguity and human clarification questions, and low correlation between human and LLM clarification questions."}}
{"id": "2507.09252", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09252", "abs": "https://arxiv.org/abs/2507.09252", "authors": ["Shukai Gong", "Yiyang Fu", "Fengyuan Ran", "Feng Zhou"], "title": "TPP-SD: Accelerating Transformer Point Process Sampling with Speculative Decoding", "comment": null, "summary": "We propose TPP-SD, a novel approach that accelerates Transformer temporal\npoint process (TPP) sampling by adapting speculative decoding (SD) techniques\nfrom language models. By identifying the structural similarities between\nthinning algorithms for TPPs and speculative decoding for language models, we\ndevelop an efficient sampling framework that leverages a smaller draft model to\ngenerate multiple candidate events, which are then verified by the larger\ntarget model in parallel. TPP-SD maintains the same output distribution as\nautoregressive sampling while achieving significant acceleration. Experiments\non both synthetic and real datasets demonstrate that our approach produces\nsamples from identical distributions as standard methods, but with 2-6$\\times$\nspeedup. Our ablation studies analyze the impact of hyperparameters such as\ndraft length and draft model size on sampling efficiency. TPP-SD bridges the\ngap between powerful Transformer TPP models and the practical need for rapid\nsequence sampling.", "AI": {"tldr": "TPP-SD \u901a\u8fc7\u501f\u9274\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u6d4b\u89e3\u7801\u6280\u672f\uff0c\u52a0\u901f\u4e86 Transformer \u65f6\u95f4\u70b9\u8fc7\u7a0b\u91c7\u6837\uff0c\u901f\u5ea6\u63d0\u5347 2-6 \u500d\uff0c\u4e14\u4e0d\u6539\u53d8\u8f93\u51fa\u5206\u5e03\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3 Transformer \u65f6\u95f4\u70b9\u8fc7\u7a0b\uff08TPP\uff09\u91c7\u6837\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u5e76\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u4e2d\u5feb\u901f\u5e8f\u5217\u91c7\u6837\u7684\u9700\u6c42\u3002", "method": "TPP-SD \u5229\u7528\u4e86\u7ec6\u7c92\u5ea6\u7b97\u6cd5\uff08tpp\uff09\u548c\u63a8\u6d4b\u89e3\u7801\uff08sd\uff09\u4e4b\u95f4\u7684\u7ed3\u6784\u76f8\u4f3c\u6027\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u9ad8\u6548\u7684\u91c7\u6837\u6846\u67b6\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u4e00\u4e2a\u8f83\u5c0f\u7684\u8349\u7a3f\u6a21\u578b\u751f\u6210\u591a\u4e2a\u5019\u9009\u4e8b\u4ef6\uff0c\u7136\u540e\u7531\u4e00\u4e2a\u8f83\u5927\u7684\u76ee\u6807\u6a21\u578b\u5e76\u884c\u9a8c\u8bc1\u3002", "result": "TPP-SD \u80fd\u591f\u4fdd\u6301\u4e0e\u81ea\u56de\u5f52\u91c7\u6837\u76f8\u540c\u7684\u8f93\u51fa\u5206\u5e03\uff0c\u540c\u65f6\u5b9e\u73b0\u663e\u8457\u7684\u52a0\u901f\uff0c\u5b9e\u9a8c\u8868\u660e\u5176\u901f\u5ea6\u63d0\u5347\u4e86 2-6 \u500d\u3002", "conclusion": "TPP-SD \u662f\u4e00\u79cd\u521b\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u91c7\u7528\u6765\u81ea\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u6d4b\u89e3\u7801\uff08SD\uff09\u6280\u672f\uff0c\u663e\u8457\u52a0\u901f\u4e86 Transformer \u65f6\u95f4\u70b9\u8fc7\u7a0b\uff08TPP\uff09\u7684\u91c7\u6837\u3002"}}
{"id": "2507.10034", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10034", "abs": "https://arxiv.org/abs/2507.10034", "authors": ["Xianghong Zou", "Jianping Li", "Zhe Chen", "Zhen Cao", "Zhen Dong", "Qiegen Liu", "Bisheng Yang"], "title": "LifelongPR: Lifelong knowledge fusion for point cloud place recognition based on replay and prompt learning", "comment": null, "summary": "Point cloud place recognition (PCPR) plays a crucial role in photogrammetry\nand robotics applications such as autonomous driving, intelligent\ntransportation, and augmented reality. In real-world large-scale deployments of\na positioning system, PCPR models must continuously acquire, update, and\naccumulate knowledge to adapt to diverse and dynamic environments, i.e., the\nability known as continual learning (CL). However, existing PCPR models often\nsuffer from catastrophic forgetting, leading to significant performance\ndegradation in previously learned scenes when adapting to new environments or\nsensor types. This results in poor model scalability, increased maintenance\ncosts, and system deployment difficulties, undermining the practicality of\nPCPR. To address these issues, we propose LifelongPR, a novel continual\nlearning framework for PCPR, which effectively extracts and fuses knowledge\nfrom sequential point cloud data. First, to alleviate the knowledge loss, we\npropose a replay sample selection method that dynamically allocates sample\nsizes according to each dataset's information quantity and selects spatially\ndiverse samples for maximal representativeness. Second, to handle domain\nshifts, we design a prompt learning-based CL framework with a lightweight\nprompt module and a two-stage training strategy, enabling domain-specific\nfeature adaptation while minimizing forgetting. Comprehensive experiments on\nlarge-scale public and self-collected datasets are conducted to validate the\neffectiveness of the proposed method. Compared with state-of-the-art (SOTA)\nmethods, our method achieves 6.50% improvement in mIR@1, 7.96% improvement in\nmR@1, and an 8.95% reduction in F. The code and pre-trained models are publicly\navailable at https://github.com/zouxianghong/LifelongPR.", "AI": {"tldr": "\u63d0\u51faLifelongPR\u6846\u67b6\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u6837\u672c\u9009\u62e9\u548c\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u70b9\u4e91\u573a\u666f\u8bc6\u522b\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u70b9\u4e91\u573a\u666f\u8bc6\u522b\uff08PCPR\uff09\u6a21\u578b\u5728\u9002\u5e94\u65b0\u73af\u5883\u6216\u4f20\u611f\u5668\u7c7b\u578b\u65f6\uff0c\u4f1a\u56e0\u4e3a\u707e\u96be\u6027\u9057\u5fd8\u800c\u5bfc\u81f4\u5148\u524d\u5b66\u4e60\u573a\u666f\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u4ece\u800c\u5f71\u54cd\u6a21\u578b\u7684\u53ef\u6269\u5c55\u6027\u3001\u7ef4\u62a4\u6210\u672c\u548c\u90e8\u7f72\u7684\u5b9e\u7528\u6027\u3002", "method": "LifelongPR\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2a\u52a8\u6001\u5206\u914d\u6837\u672c\u91cf\u7684\u91cd\u653e\u6837\u672c\u9009\u62e9\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e00\u4e2a\u5305\u542b\u8f7b\u91cf\u7ea7\u63d0\u793a\u6a21\u5757\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u7684\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u6301\u7eed\u5b66\u4e60\u6846\u67b6\u3002", "result": "\u4e0e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cLifelongPR\u5728mIR@1\u4e0a\u63d0\u9ad8\u4e866.50%\uff0c\u5728mR@1\u4e0a\u63d0\u9ad8\u4e867.96%\uff0c\u5728F\u4e0a\u964d\u4f4e\u4e868.95%\u3002", "conclusion": "LifelongPR\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u5206\u914d\u6837\u672c\u91cf\u548c\u57fa\u4e8e\u63d0\u793a\u5b66\u4e60\u7684\u6846\u67b6\u6765\u89e3\u51b3\u707e\u96be\u6027\u9057\u5fd8\u548c\u57df\u8f6c\u79fb\u95ee\u9898\uff0c\u5728\u5927\u578b\u516c\u5171\u548c\u81ea\u6536\u96c6\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.09492", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09492", "abs": "https://arxiv.org/abs/2507.09492", "authors": ["Fuyin Ye", "Erwen Yao", "Jianyong Chen", "Fengmei He", "Junxiang Zhang", "Lihao Ni"], "title": "SDTN and TRN: Adaptive Spectral-Spatial Feature Extraction for Hyperspectral Image Classification", "comment": "4 pages, 2 figures", "summary": "Hyperspectral image classification plays a pivotal role in precision\nagriculture, providing accurate insights into crop health monitoring, disease\ndetection, and soil analysis. However, traditional methods struggle with\nhigh-dimensional data, spectral-spatial redundancy, and the scarcity of labeled\nsamples, often leading to suboptimal performance. To address these challenges,\nwe propose the Self-Adaptive Tensor- Regularized Network (SDTN), which combines\ntensor decomposition with regularization mechanisms to dynamically adjust\ntensor ranks, ensuring optimal feature representation tailored to the\ncomplexity of the data. Building upon SDTN, we propose the Tensor-Regularized\nNetwork (TRN), which integrates the features extracted by SDTN into a\nlightweight network capable of capturing spectral-spatial features at multiple\nscales. This approach not only maintains high classification accuracy but also\nsignificantly reduces computational complexity, making the framework highly\nsuitable for real-time deployment in resource-constrained environments.\nExperiments on PaviaU datasets demonstrate significant improvements in accuracy\nand reduced model parameters compared to state-of-the-art methods.", "AI": {"tldr": "\u63d0\u51faSDTN\u548cTRN\u7f51\u7edc\uff0c\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u548c\u6b63\u5219\u5316\u6709\u6548\u89e3\u51b3\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u7684\u6311\u6218\uff0c\u5728\u63d0\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u4e86\u9ad8\u5149\u8c31\u56fe\u50cf\u5206\u7c7b\u4e2d\u4f20\u7edf\u65b9\u6cd5\u9762\u4e34\u7684\u9ad8\u7ef4\u6570\u636e\u3001\u5149\u8c31\u7a7a\u95f4\u5197\u4f59\u4ee5\u53ca\u6807\u8bb0\u6837\u672c\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u95ee\u9898\u5e38\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSDTN\uff08\u81ea\u9002\u5e94\u5f20\u91cf\u6b63\u5219\u5316\u7f51\u7edc\uff09\u7684\u65b9\u6cd5\uff0c\u7ed3\u5408\u5f20\u91cf\u5206\u89e3\u548c\u6b63\u5219\u5316\u673a\u5236\u52a8\u6001\u8c03\u6574\u5f20\u91cf\u79e9\u4ee5\u4f18\u5316\u7279\u5f81\u8868\u793a\uff1b\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51faTRN\uff08\u5f20\u91cf\u6b63\u5219\u5316\u7f51\u7edc\uff09\uff0c\u5c06SDTN\u63d0\u53d6\u7684\u7279\u5f81\u6574\u5408\u5230\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7f51\u7edc\u4e2d\uff0c\u4ee5\u6355\u6349\u591a\u5c3a\u5ea6\u7684\u5149\u8c31\u7a7a\u95f4\u7279\u5f81\u3002", "result": "SDTN\u548cTRN\u6846\u67b6\u5728PaviaU\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u663e\u8457\u7684\u51c6\u786e\u6027\u63d0\u5347\u548c\u66f4\u5c11\u7684\u6a21\u578b\u53c2\u6570\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6355\u6349\u5149\u8c31\u7a7a\u95f4\u7279\u5f81\u548c\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684SDTN\u548cTRN\u6846\u67b6\u5728PaviaU\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u5206\u7c7b\u7cbe\u5ea6\u63d0\u5347\u548c\u6a21\u578b\u53c2\u6570\u51cf\u5c11\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u5b9e\u65f6\u90e8\u7f72\u3002"}}
{"id": "2507.10285", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10285", "abs": "https://arxiv.org/abs/2507.10285", "authors": ["Xiantao Li"], "title": "From Linear Differential Equations to Unitaries: A Moment-Matching Dilation Framework with Near-Optimal Quantum Algorithms", "comment": null, "summary": "Quantum speed-ups for dynamical simulation usually demand unitary\ntime-evolution, whereas the large ODE/PDE systems encountered in realistic\nphysical models are generically non-unitary. We present a universal\nmoment-fulfilling dilation that embeds any linear, non-Hermitian flow $\\dot x =\nA x$ with $A=-iH+K$ into a strictly unitary evolution on an enlarged Hilbert\nspace: \\[\n  ( (l| \\otimes I )\n  \\mathcal T e^{-i \\int ( I_A\\otimes H +i F\\otimes K) dt}\n  ( |r) \\otimes I )\n  = \\mathcal T e^{\\int A dt}, \\] provided the triple $( F, (l|, |r) )$\nsatisfies the compact moment identities $(l| F^{k}|r) =1$ for all $k\\ge 0$ in\nthe ancilla space. This algebraic criterion recovers both\n\\emph{Schr\\\"odingerization} [Phys. Rev. Lett. 133, 230602 (2024)] and the\nlinear-combination-of-Hamiltonians (LCHS) scheme [Phys. Rev. Lett. 131, 150603\n(2023)], while also unveiling whole families of new dilations built from\ndifferential, integral, pseudo-differential, and difference generators. Each\nfamily comes with a continuous tuning parameter \\emph{and} is closed under\nsimilarity transformations that leave the moments invariant, giving rise to an\noverwhelming landscape of design space that allows quantum dilations to be\nco-optimized for specific applications, algorithms, and hardware.\n  As concrete demonstrations, we prove that a simple finite-difference dilation\nin a finite interval attains near-optimal oracle complexity. Numerical\nexperiments on Maxwell viscoelastic wave propagation confirm the accuracy and\nrobustness of the approach.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u975e\u9149\u52a8\u529b\u5b66\u6a21\u62df\u8f6c\u5316\u4e3a\u9149\u6f14\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u591a\u79cd\u751f\u6210\u5668\uff0c\u5e76\u5177\u6709\u53ef\u8c03\u53c2\u6570\uff0c\u80fd\u4e0e\u7279\u5b9a\u5e94\u7528\u548c\u786c\u4ef6\u534f\u540c\u4f18\u5316\uff0c\u5e76\u5728\u6a21\u62df\u7c98\u5f39\u6027\u6ce2\u7b49\u95ee\u9898\u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u5b9e\u7269\u7406\u6a21\u578b\u4e2d\u9047\u5230\u7684 ODE/PDE \u7cfb\u7edf\u901a\u5e38\u662f\u975e\u9149\u7684\uff0c\u800c\u91cf\u5b50\u52a0\u901f\u901a\u5e38\u9700\u8981\u9149\u65f6\u95f4\u6f14\u5316\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u77db\u76fe\uff0c\u4f7f\u91cf\u5b50\u8ba1\u7b97\u80fd\u591f\u6a21\u62df\u975e\u9149\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u4e00\u4e2a\u6ee1\u8db3\u7d27\u51d1\u77e9\u6052\u7b49\u5f0f\u7684\u4ee3\u6570\u5224\u636e\uff0c\u5c06\u4efb\u610f\u7ebf\u6027\u975e\u5384\u7c73\u6d41 $\\dot x = Ax$ \u5d4c\u5165\u5230\u6269\u5927\u7684\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u4e2d\u7684\u4e25\u683c\u9149\u6f14\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6062\u590d\u4e86\u73b0\u6709\u7684 Schr\u00f6dingerization \u548c LCHS \u65b9\u6848\uff0c\u8fd8\u63ed\u793a\u4e86\u7531\u5fae\u5206\u3001\u79ef\u5206\u3001\u4f2a\u5fae\u5206\u548c\u5dee\u5206\u751f\u6210\u5668\u6784\u6210\u7684\u65b0\u81a8\u80c0\u65cf\u3002\u8fd9\u4e9b\u81a8\u80c0\u65cf\u5177\u6709\u8fde\u7eed\u8c03\u8c10\u53c2\u6570\uff0c\u5e76\u4e14\u5bf9\u4fdd\u6301\u77e9\u4e0d\u53d8\u7684\u76f8\u4f3c\u53d8\u6362\u5c01\u95ed\u3002\u6709\u9650\u5dee\u5206\u81a8\u80c0\u5728\u6709\u9650\u533a\u95f4\u5185\u5b9e\u73b0\u4e86\u63a5\u8fd1\u6700\u4f18\u7684\u9884\u8a00\u590d\u6742\u5ea6\uff0c\u5e76\u4e14\u5728\u9ea6\u514b\u65af\u97e6\u7c98\u5f39\u6027\u6ce2\u4f20\u64ad\u7684\u6570\u503c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u7528\u7684\u3001\u6ee1\u8db3\u77e9\u7684\u81a8\u80c0\u65b9\u6cd5\uff0c\u5c06\u975e\u5384\u7c73\u7cfb\u7edf\u7684\u52a8\u529b\u5b66\u6a21\u62df\u8f6c\u5316\u4e3a\u9149\u6f14\u5316\uff0c\u4e3a\u91cf\u5b50\u6a21\u62df\u5f00\u8f9f\u4e86\u65b0\u7684\u8bbe\u8ba1\u7a7a\u95f4\uff0c\u5e76\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6570\u503c\u9a8c\u8bc1\u3002"}}
{"id": "2507.10468", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10468", "abs": "https://arxiv.org/abs/2507.10468", "authors": ["Ariadna Mon", "Sa\u00fal Fenollosa", "Jon Lecumberri"], "title": "From BERT to Qwen: Hate Detection across architectures", "comment": "4 pages, 5 figures. EE-559 Deep Learning course project (Group 11)", "summary": "Online platforms struggle to curb hate speech without over-censoring\nlegitimate discourse. Early bidirectional transformer encoders made big\nstrides, but the arrival of ultra-large autoregressive LLMs promises deeper\ncontext-awareness. Whether this extra scale actually improves practical\nhate-speech detection on real-world text remains unverified. Our study puts\nthis question to the test by benchmarking both model families, classic encoders\nand next-generation LLMs, on curated corpora of online interactions for\nhate-speech detection (Hate or No Hate).", "AI": {"tldr": "Large language models (LLMs) show promise for hate-speech detection, but their effectiveness compared to traditional models needs more research.", "motivation": "To verify if ultra-large autoregressive LLMs improve practical hate-speech detection on real-world text compared to bidirectional transformer encoders.", "method": "Benchmarking classic encoders and LLMs on curated corpora for hate-speech detection.", "result": "The study benchmarks both model families on curated corpora for hate-speech detection.", "conclusion": "future work should explore larger models and further optimization"}}
{"id": "2507.09264", "categories": ["cs.LG", "cs.AI", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09264", "abs": "https://arxiv.org/abs/2507.09264", "authors": ["Payel Mukhopadhyay", "Michael McCabe", "Ruben Ohana", "Miles Cranmer"], "title": "Controllable Patching for Compute-Adaptive Surrogate Modeling of Partial Differential Equations", "comment": null, "summary": "Patch-based transformer surrogates have become increasingly effective for\nmodeling spatiotemporal dynamics, but the fixed patch size is a major\nlimitation for budget-conscience deployment in production. We introduce two\nlightweight, architecture-agnostic modules-the Convolutional Kernel Modulator\n(CKM) and Convolutional Stride Modulator (CSM)-that enable dynamic patch size\ncontrol at inference in patch based models, without retraining or accuracy\nloss. Combined with a cyclic patch-size rollout, our method mitigates patch\nartifacts and improves long-term stability for video-like prediction tasks.\nApplied to a range of challenging 2D and 3D PDE benchmarks, our approach\nimproves rollout fidelity and runtime efficiency. To our knowledge, this is the\nfirst framework to enable inference-time patch-size tunability in patch-based\nPDE surrogates. Its plug-and-play design makes it broadly applicable across\narchitectures-establishing a general foundation for compute-adaptive modeling\nin PDE surrogate tasks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165CKM\u548cCSM\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86Transformer PDE\u4ee3\u7406\u6a21\u578b\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u8865\u4e01\u5927\u5c0f\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u8865\u4e01\u5927\u5c0f\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6548\u7387\u548c\u7a33\u5b9a\u6027\uff0c\u4e14\u4e0d\u635f\u5931\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u4ee3\u7406\u6a21\u578b\u5728\u6a21\u62df\u65f6\u7a7a\u52a8\u529b\u5b66\u65b9\u9762\u867d\u7136\u6709\u6548\uff0c\u4f46\u5176\u56fa\u5b9a\u7684\u8865\u4e01\u5927\u5c0f\u9650\u5236\u4e86\u5176\u5728\u751f\u4ea7\u73af\u5883\u4e2d\u5177\u6709\u6210\u672c\u6548\u76ca\u7684\u90e8\u7f72\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u52a8\u6001\u8c03\u6574\u8865\u4e01\u5927\u5c0f\u7684\u65b9\u6cd5\uff0c\u4ee5\u9002\u5e94\u4e0d\u540c\u7684\u8ba1\u7b97\u9884\u7b97\uff0c\u5e76\u63d0\u9ad8\u6a21\u578b\u7684\u6027\u80fd\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u7684PDE\u4ee3\u7406\u6a21\u578b\uff0c\u5e76\u5f15\u5165\u4e86\u5377\u79ef\u6838\u8c03\u5236\u5668\uff08CKM\uff09\u548c\u5377\u79ef\u6b65\u957f\u8c03\u5236\u5668\uff08CSM\uff09\u4e24\u4e2a\u8f7b\u91cf\u7ea7\u3001\u4e0e\u67b6\u6784\u65e0\u5173\u7684\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u5728\u63a8\u7406\u65f6\u52a8\u6001\u63a7\u5236\u8865\u4e01\u5927\u5c0f\u3002\u7ed3\u5408\u5faa\u73af\u8865\u4e01\u5927\u5c0f\u5c55\u5f00\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u51cf\u5c11\u8865\u4e01\u4f2a\u5f71\uff0c\u63d0\u9ad8\u89c6\u9891\u7c7b\u9884\u6d4b\u4efb\u52a1\u7684\u957f\u671f\u7a33\u5b9a\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u5177\u6709\u6311\u6218\u6027\u76842D\u548c3D PDE\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u9ad8\u4e86\u56de\u6eda\u4fdd\u771f\u5ea6\u548c\u8fd0\u884c\u65f6\u6548\u7387\u3002\u5373\u4f7f\u5728\u63a8\u7406\u65f6\u52a8\u6001\u8c03\u6574\u8865\u4e01\u5927\u5c0f\uff0c\u4e5f\u6ca1\u6709\u51fa\u73b0\u51c6\u786e\u6027\u635f\u5931\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u63d2\u62d4\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u57fa\u4e8eTransformer\u7684PDE\u4ee3\u7406\u6a21\u578b\u4e2d\u63a8\u7406\u65f6\u53ef\u8c03\u7684\u8865\u4e01\u5927\u5c0f\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5377\u79ef\u6838\u8c03\u5236\u5668\uff08CKM\uff09\u548c\u5377\u79ef\u6b65\u957f\u8c03\u5236\u5668\uff08CSM\uff09\u5b9e\u73b0\uff0c\u80fd\u591f\u5728\u4e0d\u8fdb\u884c\u518d\u8bad\u7ec3\u6216\u635f\u5931\u51c6\u786e\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u52a8\u6001\u63a7\u5236\u8865\u4e01\u5927\u5c0f\uff0c\u5e76\u7ed3\u5408\u5faa\u73af\u8865\u4e01\u5927\u5c0f\u5c55\u5f00\uff0c\u89e3\u51b3\u4e86\u56fa\u5b9a\u8865\u4e01\u5927\u5c0f\u7684\u5c40\u9650\u6027\uff0c\u51cf\u5c11\u4e86\u8865\u4e01\u4f2a\u5f71\uff0c\u63d0\u9ad8\u4e86\u957f\u671f\u7a33\u5b9a\u6027\uff0c\u540c\u65f6\u6539\u5584\u4e86\u6a21\u578b\u7684\u56de\u6eda\u4fdd\u771f\u5ea6\u548c\u8fd0\u884c\u65f6\u6548\u7387\u3002"}}
{"id": "2507.10158", "categories": ["cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10158", "abs": "https://arxiv.org/abs/2507.10158", "authors": ["Obaidullah Zaland", "Erik Elmroth", "Monowar Bhuyan"], "title": "MTF-Grasp: A Multi-tier Federated Learning Approach for Robotic Grasping", "comment": "The work is accepted for presentation at IEEE SMC 2025", "summary": "Federated Learning (FL) is a promising machine learning paradigm that enables\nparticipating devices to train privacy-preserved and collaborative models. FL\nhas proven its benefits for robotic manipulation tasks. However, grasping tasks\nlack exploration in such settings where robots train a global model without\nmoving data and ensuring data privacy. The main challenge is that each robot\nlearns from data that is nonindependent and identically distributed (non-IID)\nand of low quantity. This exhibits performance degradation, particularly in\nrobotic grasping. Thus, in this work, we propose MTF-Grasp, a multi-tier FL\napproach for robotic grasping, acknowledging the unique challenges posed by the\nnon-IID data distribution across robots, including quantitative skewness.\nMTF-Grasp harnesses data quality and quantity across robots to select a set of\n\"top-level\" robots with better data distribution and higher sample count. It\nthen utilizes top-level robots to train initial seed models and distribute them\nto the remaining \"low-level\" robots, reducing the risk of model performance\ndegradation in low-level robots. Our approach outperforms the conventional FL\nsetup by up to 8% on the quantity-skewed Cornell and Jacquard grasping\ndatasets.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa MTF-Grasp\uff0c\u4e00\u79cd\u89e3\u51b3\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u8054\u90a6\u5b66\u4e60\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u548c\u6570\u636e\u91cf\u5c11\u95ee\u9898\u7684\u591a\u5c42\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u4f18\u8d28\u6570\u636e\u6e90\u8bad\u7ec3\u6a21\u578b\u5e76\u5206\u53d1\uff0c\u5728\u6293\u53d6\u6570\u636e\u96c6\u4e0a\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 8%\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u5728\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\uff0c\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u548c\u6570\u636e\u91cf\u5c11\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u586b\u8865\u8be5\u9886\u57df\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MTF-Grasp \u7684\u591a\u5c42\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u6570\u636e\u5206\u5e03\u66f4\u597d\u3001\u6837\u672c\u91cf\u66f4\u5927\u7684\u201c\u9876\u5c42\u201d\u673a\u5668\u4eba\u6765\u8bad\u7ec3\u521d\u59cb\u6a21\u578b\uff0c\u7136\u540e\u5c06\u6a21\u578b\u5206\u53d1\u7ed9\u201c\u5e95\u5c42\u201d\u673a\u5668\u4eba\uff0c\u4ee5\u5e94\u5bf9\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u548c\u6570\u636e\u91cf\u5c11\u7684\u6311\u6218\u3002", "result": "MTF-Grasp \u65b9\u6cd5\u5728\u6570\u636e\u91cf\u503e\u659c\u7684 Cornell \u548c Jacquard \u6293\u53d6\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u76f8\u6bd4\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u63d0\u5347\u4e86\u9ad8\u8fbe 8%\u3002", "conclusion": "MTF-Grasp \u65b9\u6cd5\u5728\u6293\u53d6\u4efb\u52a1\u4e0a\u76f8\u6bd4\u4f20\u7edf\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u6570\u636e\u91cf\u503e\u659c\u7684 Cornell \u548c Jacquard \u6293\u53d6\u6570\u636e\u96c6\u4e0a\uff0c\u6027\u80fd\u63d0\u5347\u9ad8\u8fbe 8%\uff0c\u6709\u6548\u5730\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u6293\u53d6\u4efb\u52a1\u4e2d\u6570\u636e\u975e\u72ec\u7acb\u540c\u5206\u5e03\uff08non-IID\uff09\u548c\u6570\u636e\u91cf\u5c11\u7684\u95ee\u9898\u3002"}}
{"id": "2507.09500", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09500", "abs": "https://arxiv.org/abs/2507.09500", "authors": ["Yiwen Liang", "Hui Chen", "Yizhe Xiong", "Zihan Zhou", "Mengyao Lyu", "Zijia Lin", "Shuaicheng Niu", "Sicheng Zhao", "Jungong Han", "Guiguang Ding"], "title": "Advancing Reliable Test-Time Adaptation of Vision-Language Models under Visual Variations", "comment": "Accepted at the 33rd ACM International Conference on Multimedia(ACM\n  MM 2025)", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot capabilities but\nstruggle with distribution shifts in downstream tasks when labeled data is\nunavailable, which has motivated the development of Test-Time Adaptation (TTA)\nto improve VLMs' performance during inference without annotations. Among\nvarious TTA approaches, cache-based methods show promise by preserving\nhistorical knowledge from low-entropy samples in a dynamic cache and fostering\nefficient adaptation. However, these methods face two critical reliability\nchallenges: (1) entropy often becomes unreliable under distribution shifts,\ncausing error accumulation in the cache and degradation in adaptation\nperformance; (2) the final predictions may be unreliable due to inflexible\ndecision boundaries that fail to accommodate large downstream shifts. To\naddress these challenges, we propose a Reliable Test-time Adaptation (ReTA)\nmethod that integrates two complementary strategies to enhance reliability from\ntwo perspectives. First, to mitigate the unreliability of entropy as a sample\nselection criterion for cache construction, we introduce Consistency-aware\nEntropy Reweighting (CER), which incorporates consistency constraints to weight\nentropy during cache updating. While conventional approaches rely solely on low\nentropy for cache prioritization and risk introducing noise, our method\nleverages predictive consistency to maintain a high-quality cache and\nfacilitate more robust adaptation. Second, we present Diversity-driven\nDistribution Calibration (DDC), which models class-wise text embeddings as\nmultivariate Gaussian distributions, enabling adaptive decision boundaries for\nmore accurate predictions across visually diverse content. Extensive\nexperiments demonstrate that ReTA consistently outperforms state-of-the-art\nmethods, particularly under challenging real-world distribution shifts.", "AI": {"tldr": "\u7531\u4e8e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u6807\u7b7e\u6570\u636e\uff0c\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u96f6\u6837\u672c\u80fd\u529b\u6709\u9650\uff0c\u8fd9\u6fc0\u53d1\u4e86\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6027\u80fd\u800c\u4e0d\u5e26\u6807\u7b7e\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08TTA\uff09\u3002\u7f13\u5b58\u65b9\u6cd5\u901a\u8fc7\u5728\u52a8\u6001\u7f13\u5b58\u4e2d\u4fdd\u5b58\u4f4e\u71b5\u6837\u672c\u7684\u5386\u53f2\u77e5\u8bc6\u6765\u4fc3\u8fdb\u6709\u6548\u7684\u9002\u5e94\uff0c\u4f46\u5b83\u4eec\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u7684\u53ef\u9760\u6027\u6311\u6218\uff1a1. \u5206\u5e03\u53d8\u5316\u4e0b\u7684\u71b5\u4e0d\u53ef\u9760\u4f1a\u5bfc\u81f4\u7f13\u5b58\u9519\u8bef\u7d2f\u79ef\u548c\u6027\u80fd\u4e0b\u964d\u30022. \u4e0d\u7075\u6d3b\u7684\u51b3\u7b56\u8fb9\u754c\u65e0\u6cd5\u9002\u5e94\u5927\u7684\u4e0b\u6e38\u53d8\u5316\uff0c\u5bfc\u81f4\u6700\u7ec8\u9884\u6d4b\u4e0d\u53ef\u9760\u3002ReTA\u65b9\u6cd5\u6574\u5408\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u6765\u589e\u5f3a\u53ef\u9760\u6027\uff1a1. \u4e00\u81f4\u6027\u611f\u77e5\u71b5\u91cd\u52a0\u6743\uff08CER\uff09\uff0c\u5b83\u7ed3\u5408\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u52a0\u6743\u71b5\uff0c\u7528\u4e8e\u7f13\u5b58\u66f4\u65b0\uff0c\u4ee5\u7ef4\u6301\u9ad8\u8d28\u91cf\u7f13\u5b58\u5e76\u4fc3\u8fdb\u66f4\u7a33\u5065\u7684\u9002\u5e94\u30022. \u591a\u6837\u6027\u9a71\u52a8\u5206\u5e03\u6821\u51c6\uff08DDC\uff09\uff0c\u5b83\u5c06\u7c7b\u522b\u6587\u672c\u5d4c\u5165\u5efa\u6a21\u4e3a\u591a\u5143\u9ad8\u65af\u5206\u5e03\uff0c\u4e3a\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u542f\u7528\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\u3002ReTA\u65b9\u6cd5\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u5206\u5e03\u53d8\u5316\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "motivation": "\u7531\u4e8e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7f3a\u4e4f\u6807\u7b7e\u6570\u636e\uff0c\u5728\u5206\u5e03\u53d8\u5316\u4e0b\u96f6\u6837\u672c\u80fd\u529b\u6709\u9650\uff0c\u8fd9\u6fc0\u53d1\u4e86\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u6027\u80fd\u800c\u4e0d\u5e26\u6807\u7b7e\u7684\u6d4b\u8bd5\u65f6\u95f4\u9002\u5e94\uff08TTA\uff09\u3002\u7f13\u5b58\u65b9\u6cd5\u901a\u8fc7\u5728\u52a8\u6001\u7f13\u5b58\u4e2d\u4fdd\u5b58\u4f4e\u71b5\u6837\u672c\u7684\u5386\u53f2\u77e5\u8bc6\u6765\u4fc3\u8fdb\u6709\u6548\u7684\u9002\u5e94\uff0c\u4f46\u5b83\u4eec\u9762\u4e34\u4e24\u4e2a\u5173\u952e\u7684\u53ef\u9760\u6027\u6311\u6218\uff1a1. \u5206\u5e03\u53d8\u5316\u4e0b\u7684\u71b5\u4e0d\u53ef\u9760\u4f1a\u5bfc\u81f4\u7f13\u5b58\u9519\u8bef\u7d2f\u79ef\u548c\u6027\u80fd\u4e0b\u964d\u30022. \u4e0d\u7075\u6d3b\u7684\u51b3\u7b56\u8fb9\u754c\u65e0\u6cd5\u9002\u5e94\u5927\u7684\u4e0b\u6e38\u53d8\u5316\uff0c\u5bfc\u81f4\u6700\u7ec8\u9884\u6d4b\u4e0d\u53ef\u9760\u3002", "method": "ReTA\u65b9\u6cd5\u6574\u5408\u4e86\u4e24\u79cd\u4e92\u8865\u7b56\u7565\u6765\u589e\u5f3a\u53ef\u9760\u6027\uff1a1. \u4e00\u81f4\u6027\u611f\u77e5\u71b5\u91cd\u52a0\u6743\uff08CER\uff09\uff0c\u5b83\u7ed3\u5408\u4e00\u81f4\u6027\u7ea6\u675f\u6765\u52a0\u6743\u71b5\uff0c\u7528\u4e8e\u7f13\u5b58\u66f4\u65b0\uff0c\u4ee5\u7ef4\u6301\u9ad8\u8d28\u91cf\u7f13\u5b58\u5e76\u4fc3\u8fdb\u66f4\u7a33\u5065\u7684\u9002\u5e94\u30022. \u591a\u6837\u6027\u9a71\u52a8\u5206\u5e03\u6821\u51c6\uff08DDC\uff09\uff0c\u5b83\u5c06\u7c7b\u522b\u6587\u672c\u5d4c\u5165\u5efa\u6a21\u4e3a\u591a\u5143\u9ad8\u65af\u5206\u5e03\uff0c\u4e3a\u66f4\u51c6\u786e\u7684\u9884\u6d4b\u542f\u7528\u81ea\u9002\u5e94\u51b3\u7b56\u8fb9\u754c\u3002", "result": "ReTA\u65b9\u6cd5\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u5206\u5e03\u53d8\u5316\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "ReTA\u65b9\u6cd5\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u73b0\u5b9e\u4e16\u754c\u5206\u5e03\u53d8\u5316\u4e0b\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.10287", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10287", "abs": "https://arxiv.org/abs/2507.10287", "authors": ["Douglas Hendry", "Alessandro Sinibaldi", "Giuseppe Carleo"], "title": "Grassmann Variational Monte Carlo with neural wave functions", "comment": "12 + 7, 4 figures, 2 tables", "summary": "Excited states play a central role in determining the physical properties of\nquantum matter, yet their accurate computation in many-body systems remains a\nformidable challenge for numerical methods. While neural quantum states have\ndelivered outstanding results for ground-state problems, extending their\napplicability to excited states has faced limitations, including instability in\ndense spectra and reliance on symmetry constraints or penalty-based\nformulations. In this work, we rigorously formalize the framework introduced by\nPfau et al.~\\cite{pfau2024accurate} in terms of Grassmann geometry of the\nHilbert space. This allows us to generalize the Stochastic Reconfiguration\nmethod for the simultaneous optimization of multiple variational wave\nfunctions, and to introduce the multidimensional versions of operator variances\nand overlaps. We validate our approach on the Heisenberg quantum spin model on\nthe square lattice, achieving highly accurate energies and physical observables\nfor a large number of excited states.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u8ba1\u7b97\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u6fc0\u53d1\u6001\u7684\u65b0\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6d77\u68ee\u5821\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u6210\u529f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6570\u503c\u65b9\u6cd5\u5728\u8ba1\u7b97\u591a\u4f53\u7cfb\u7edf\u4e2d\u6fc0\u53d1\u6001\u65f6\u7684\u6311\u6218\uff0c\u4f8b\u5982\u5728\u7a20\u5bc6\u5149\u8c31\u4e2d\u7684\u4e0d\u7a33\u5b9a\u6027\u4ee5\u53ca\u5bf9\u5bf9\u79f0\u7ea6\u675f\u6216\u57fa\u4e8e\u60e9\u7f5a\u7684\u8868\u8ff0\u7684\u4f9d\u8d56\u6027\u3002", "method": "\u901a\u8fc7\u5c06Pfau\u7b49\u4eba\u4ecb\u7ecd\u7684\u6846\u67b6\u7528\u5e0c\u5c14\u4f2f\u7279\u7a7a\u95f4\u7684 Grassmann \u51e0\u4f55\u5f62\u5f0f\u5316\uff0c\u6211\u4eec\u80fd\u591f\u63a8\u5e7f\u968f\u673a\u91cd\u6784\u65b9\u6cd5\uff0c\u540c\u65f6\u4f18\u5316\u591a\u4e2a\u53d8\u5206\u6ce2\u51fd\u6570\uff0c\u5e76\u5f15\u5165\u7b97\u5b50\u65b9\u5dee\u548c\u91cd\u53e0\u7684\u591a\u7ef4\u7248\u672c\u3002", "result": "\u5728\u6d77\u68ee\u5821\u91cf\u5b50\u81ea\u65cb\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u80fd\u91cf\u548c\u7269\u7406\u53ef\u89c2\u6d4b\u503c\uff0c\u8986\u76d6\u4e86\u5927\u91cf\u7684\u6fc0\u53d1\u6001\u3002", "conclusion": "\u672c\u6587\u7684\u65b9\u6cd5\u5728\u6d77\u68ee\u5821\u91cf\u5b50\u81ea\u65cb\u6a21\u578b\u4e0a\u53d6\u5f97\u4e86\u9ad8\u5ea6\u7cbe\u786e\u7684\u80fd\u91cf\u548c\u7269\u7406\u53ef\u89c2\u6d4b\u503c\uff0c\u9002\u7528\u4e8e\u5927\u91cf\u6fc0\u53d1\u6001\u3002"}}
{"id": "2507.10472", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10472", "abs": "https://arxiv.org/abs/2507.10472", "authors": ["Mohamed T. Younes", "Omar Walid", "Mai Hassan", "Ali Hamdi"], "title": "MLAR: Multi-layer Large Language Model-based Robotic Process Automation Applicant Tracking", "comment": null, "summary": "This paper introduces an innovative Applicant Tracking System (ATS) enhanced\nby a novel Robotic process automation (RPA) framework or as further referred to\nas MLAR. Traditional recruitment processes often encounter bottlenecks in\nresume screening and candidate shortlisting due to time and resource\nconstraints. MLAR addresses these challenges employing Large Language Models\n(LLMs) in three distinct layers: extracting key characteristics from job\npostings in the first layer, parsing applicant resume to identify education,\nexperience, skills in the second layer, and similarity matching in the third\nlayer. These features are then matched through advanced semantic algorithms to\nidentify the best candidates efficiently. Our approach integrates seamlessly\ninto existing RPA pipelines, automating resume parsing, job matching, and\ncandidate notifications. Extensive performance benchmarking shows that MLAR\noutperforms the leading RPA platforms, including UiPath and Automation\nAnywhere, in high-volume resume-processing tasks. When processing 2,400\nresumes, MLAR achieved an average processing time of 5.4 seconds per resume,\nreducing processing time by approximately 16.9% compared to Automation Anywhere\nand 17.1% compared to UiPath. These results highlight the potential of MLAR to\ntransform recruitment workflows by providing an efficient, accurate, and\nscalable solution tailored to modern hiring needs.", "AI": {"tldr": "MLAR\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u65b0\u578bRPA\u6846\u67b6\uff0c\u53ef\u9ad8\u6548\u81ea\u52a8\u5316\u62db\u8058\u6d41\u7a0b\uff0c\u5728\u5904\u7406\u5927\u91cf\u7b80\u5386\u65f6\u6bd4\u73b0\u6709RPA\u5e73\u53f0\u66f4\u5feb\u3002", "motivation": "\u4f20\u7edf\u7684\u62db\u8058\u6d41\u7a0b\u5728\u7b80\u5386\u7b5b\u9009\u548c\u5019\u9009\u4eba\u5165\u56f4\u65b9\u9762\u5e38\u5e38\u9047\u5230\u65f6\u95f4\u4e0e\u8d44\u6e90\u7684\u74f6\u9888\u3002MLAR\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u4e00\u79cd\u540d\u4e3aMLAR\u7684\u65b0\u578b\u673a\u5668\u4eba\u6d41\u7a0b\u81ea\u52a8\u5316\uff08RPA\uff09\u6846\u67b6\uff0c\u5e76\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u5904\u7406\u4e09\u4e2a\u5c42\u9762\uff1a\u4ece\u804c\u4f4d\u53d1\u5e03\u4e2d\u63d0\u53d6\u5173\u952e\u7279\u5f81\uff0c\u89e3\u6790\u7b80\u5386\u4ee5\u8bc6\u522b\u6559\u80b2\u3001\u7ecf\u9a8c\u548c\u6280\u80fd\uff0c\u4ee5\u53ca\u8fdb\u884c\u76f8\u4f3c\u6027\u5339\u914d\u3002\u901a\u8fc7\u5148\u8fdb\u7684\u8bed\u4e49\u7b97\u6cd5\u8fdb\u884c\u5339\u914d\uff0c\u4ee5\u9ad8\u6548\u8bc6\u522b\u6700\u4f73\u5019\u9009\u4eba\u3002", "result": "MLAR\u5728\u5904\u7406\u9ad8\u6570\u91cf\u7b80\u5386\u7684\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u9886\u5148\u7684RPA\u5e73\u53f0\uff08\u5982UiPath\u548cAutomation Anywhere\uff09\u3002\u5728\u5904\u74062400\u4efd\u7b80\u5386\u65f6\uff0cMLAR\u7684\u5e73\u5747\u5904\u7406\u65f6\u95f4\u4e3a\u6bcf\u4efd5.4\u79d2\uff0c\u4e0eAutomation Anywhere\u76f8\u6bd4\uff0c\u5904\u7406\u65f6\u95f4\u51cf\u5c11\u4e86\u7ea616.9%\uff0c\u4e0eUiPath\u76f8\u6bd4\uff0c\u51cf\u5c11\u4e8617.1%\u3002", "conclusion": "MLAR\u901a\u8fc7\u63d0\u4f9b\u9ad8\u6548\u3001\u51c6\u786e\u548c\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u6f5c\u529b\u5f7b\u5e95\u6539\u53d8\u62db\u8058\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2507.09353", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09353", "abs": "https://arxiv.org/abs/2507.09353", "authors": ["Addison Weatherhead", "Anna Goldenberg"], "title": "Impute With Confidence: A Framework for Uncertainty Aware Multivariate Time Series Imputation", "comment": null, "summary": "Time series data with missing values is common across many domains.\nHealthcare presents special challenges due to prolonged periods of sensor\ndisconnection. In such cases, having a confidence measure for imputed values is\ncritical. Most existing methods either overlook model uncertainty or lack\nmechanisms to estimate it. To address this gap, we introduce a general\nframework that quantifies and leverages uncertainty for selective imputation.\nBy focusing on values the model is most confident in, highly unreliable\nimputations are avoided. Our experiments on multiple EHR datasets, covering\ndiverse types of missingness, demonstrate that selectively imputing\nless-uncertain values not only reduces imputation errors but also improves\ndownstream tasks. Specifically, we show performance gains in a 24-hour\nmortality prediction task, underscoring the practical benefit of incorporating\nuncertainty into time series imputation.", "AI": {"tldr": "\u901a\u8fc7\u91cf\u5316\u548c\u5229\u7528\u4e0d\u786e\u5b9a\u6027\uff0c\u5bf9\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8fdb\u884c\u9009\u62e9\u6027\u586b\u5145\uff0c\u53ef\u4ee5\u63d0\u9ad8\u586b\u5145\u7cbe\u5ea6\u548c\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u4fdd\u5065\u6570\u636e\u4e2d\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u65f6\u95f4\u5e8f\u5217\u586b\u5145\u65b9\u6cd5\u5ffd\u7565\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u6216\u7f3a\u4e4f\u4f30\u8ba1\u673a\u5236\u7684\u4e0d\u8db3\uff0c\u5c24\u5176\u662f\u5728\u533b\u7597\u4fdd\u5065\u9886\u57df\uff0c\u4f20\u611f\u5668\u65ad\u5f00\u8fde\u63a5\u5bfc\u81f4\u6570\u636e\u7f3a\u5931\u4e14\u5bf9\u586b\u5145\u503c\u7f6e\u4fe1\u5ea6\u7684\u9700\u6c42\u5f88\u9ad8\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u901a\u7528\u7684\u6846\u67b6\uff0c\u5bf9\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u91cf\u5316\u548c\u5229\u7528\uff0c\u4ee5\u5b9e\u73b0\u9009\u62e9\u6027\u586b\u5145\uff0c\u4f18\u5148\u586b\u5145\u6a21\u578b\u7f6e\u4fe1\u5ea6\u9ad8\u7684\u503c\u3002", "result": "\u5728\u591a\u4e2a EHR \u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u9009\u62e9\u6027\u586b\u5145\u4e0d\u786e\u5b9a\u6027\u8f83\u4f4e\u7684\u503c\u53ef\u4ee5\u51cf\u5c11\u586b\u5145\u9519\u8bef\uff0c\u5e76\u63d0\u9ad8\u5305\u62ec24\u5c0f\u65f6\u6b7b\u4ea1\u7387\u9884\u6d4b\u5728\u5185\u7684\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5728\u65f6\u95f4\u5e8f\u5217\u586b\u5145\u4e2d\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\uff0c\u7814\u7a76\u8bc1\u660e\u4e86\u5176\u5728\u51cf\u5c11\u586b\u5145\u9519\u8bef\u548c\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u7279\u522b\u662f\u5728\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\uff08EHR\uff09\u6570\u636e\u548c\u533b\u7597\u4fdd\u5065\u9886\u57df\u3002"}}
{"id": "2507.10474", "categories": ["cs.CV", "cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.10474", "abs": "https://arxiv.org/abs/2507.10474", "authors": ["Seyed Alireza Rahimi Azghadi", "Truong-Thanh-Hung Nguyen", "Helene Fournier", "Monica Wachowicz", "Rene Richard", "Francis Palma", "Hung Cao"], "title": "Privacy-Preserving Multi-Stage Fall Detection Framework with Semi-supervised Federated Learning and Robotic Vision Confirmation", "comment": null, "summary": "The aging population is growing rapidly, and so is the danger of falls in\nolder adults. A major cause of injury is falling, and detection in time can\ngreatly save medical expenses and recovery time. However, to provide timely\nintervention and avoid unnecessary alarms, detection systems must be effective\nand reliable while addressing privacy concerns regarding the user. In this\nwork, we propose a framework for detecting falls using several complementary\nsystems: a semi-supervised federated learning-based fall detection system\n(SF2D), an indoor localization and navigation system, and a vision-based human\nfall recognition system. A wearable device and an edge device identify a fall\nscenario in the first system. On top of that, the second system uses an indoor\nlocalization technique first to localize the fall location and then navigate a\nrobot to inspect the scenario. A vision-based detection system running on an\nedge device with a mounted camera on a robot is used to recognize fallen\npeople. Each of the systems of this proposed framework achieves different\naccuracy rates. Specifically, the SF2D has a 0.81% failure rate equivalent to\n99.19% accuracy, while the vision-based fallen people detection achieves 96.3%\naccuracy. However, when we combine the accuracy of these two systems with the\naccuracy of the navigation system (95% success rate), our proposed framework\ncreates a highly reliable performance for fall detection, with an overall\naccuracy of 99.99%. Not only is the proposed framework safe for older adults,\nbut it is also a privacy-preserving solution for detecting falls.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8054\u90a6\u5b66\u4e60\u3001\u5ba4\u5185\u5b9a\u4f4d\u548c\u89c6\u89c9\u8bc6\u522b\u7684\u591a\u7cfb\u7edf\u6846\u67b6\uff0c\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u3001\u4fdd\u62a4\u9690\u79c1\u7684\u8001\u5e74\u4eba\u8dcc\u5012\u68c0\u6d4b\u3002", "motivation": "\u8001\u5e74\u4eba\u8dcc\u5012\u662f\u4e00\u4e2a\u65e5\u76ca\u4e25\u5cfb\u7684\u95ee\u9898\uff0c\u53ca\u65f6\u68c0\u6d4b\u8dcc\u5012\u80fd\u591f\u6709\u6548\u8282\u7701\u533b\u7597\u8d39\u7528\u548c\u5eb7\u590d\u65f6\u95f4\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u68c0\u6d4b\u7cfb\u7edf\u9700\u8981\u5728\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u540c\u65f6\u8fd8\u8981\u89e3\u51b3\u7528\u6237\u9690\u79c1\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u534a\u76d1\u7763\u8054\u90a6\u5b66\u4e60\u8dcc\u5012\u68c0\u6d4b\u7cfb\u7edf (SF2D)\u3001\u5ba4\u5185\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u7cfb\u7edf\u4ee5\u53ca\u57fa\u4e8e\u89c6\u89c9\u7684\u4eba\u5458\u8dcc\u5012\u8bc6\u522b\u7cfb\u7edf\u7684\u6846\u67b6\u3002SF2D \u4f7f\u7528\u7a7f\u6234\u5f0f\u8bbe\u5907\u548c\u8fb9\u7f18\u8bbe\u5907\u6765\u8bc6\u522b\u8dcc\u5012\u573a\u666f\uff1b\u5ba4\u5185\u5b9a\u4f4d\u4e0e\u5bfc\u822a\u7cfb\u7edf\u8d1f\u8d23\u5b9a\u4f4d\u8dcc\u5012\u4f4d\u7f6e\u5e76\u5c06\u673a\u5668\u4eba\u5bfc\u822a\u5230\u73b0\u573a\uff1b\u800c\u57fa\u4e8e\u89c6\u89c9\u7684\u68c0\u6d4b\u7cfb\u7edf\u5219\u901a\u8fc7\u673a\u5668\u4eba\u4e0a\u7684\u6444\u50cf\u5934\u8bc6\u522b\u8dcc\u5012\u4eba\u5458\u3002", "result": "SF2D \u7684\u5931\u6548\u7387\u4e3a 0.81% (\u51c6\u786e\u7387\u4e3a 99.19%)\uff0c\u57fa\u4e8e\u89c6\u89c9\u7684\u8dcc\u5012\u4eba\u5458\u68c0\u6d4b\u51c6\u786e\u7387\u4e3a 96.3%\u3002\u7ed3\u5408\u5bfc\u822a\u7cfb\u7edf\u7684\u6210\u529f\u7387 (95%)\uff0c\u8be5\u6846\u67b6\u6574\u4f53\u51c6\u786e\u7387\u9ad8\u8fbe 99.99%\u3002", "conclusion": "\u8be5\u6846\u67b6\u7684\u51c6\u786e\u7387\u4e3a 99.99%\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u8001\u5e74\u4eba\u8dcc\u5012\u68c0\u6d4b\u4e2d\u7684\u9690\u79c1\u95ee\u9898\u3002"}}
{"id": "2507.09512", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09512", "abs": "https://arxiv.org/abs/2507.09512", "authors": ["Pengyu Liu", "Kun Li", "Fei Wang", "Yanyan Wei", "Junhui She", "Dan Guo"], "title": "Online Micro-gesture Recognition Using Data Augmentation and Spatial-Temporal Attention", "comment": "11 pages, 4 figures", "summary": "In this paper, we introduce the latest solution developed by our team,\nHFUT-VUT, for the Micro-gesture Online Recognition track of the IJCAI 2025 MiGA\nChallenge. The Micro-gesture Online Recognition task is a highly challenging\nproblem that aims to locate the temporal positions and recognize the categories\nof multiple micro-gesture instances in untrimmed videos. Compared to\ntraditional temporal action detection, this task places greater emphasis on\ndistinguishing between micro-gesture categories and precisely identifying the\nstart and end times of each instance. Moreover, micro-gestures are typically\nspontaneous human actions, with greater differences than those found in other\nhuman actions. To address these challenges, we propose hand-crafted data\naugmentation and spatial-temporal attention to enhance the model's ability to\nclassify and localize micro-gestures more accurately. Our solution achieved an\nF1 score of 38.03, outperforming the previous state-of-the-art by 37.9%. As a\nresult, our method ranked first in the Micro-gesture Online Recognition track.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10319", "categories": ["quant-ph", "cond-mat.stat-mech", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.10319", "abs": "https://arxiv.org/abs/2507.10319", "authors": ["Takanao Ishii", "Masahito Ueda"], "title": "Quantum i.i.d. Steady States in Open Many-Body Systems", "comment": "23 pages, 4 figures", "summary": "Understanding how a quantum many-body state is maintained stably as a\nnonequilibrium steady state is of fundamental and practical importance for\nexploration and exploitation of open quantum systems. We establish a general\nequivalent condition for an open quantum many-body system governed by the\nGorini-Kossakowski-Sudarshan-Lindblad dynamics under local drive and/or\ndissipation to have a quantum independent and identically distributed (i.i.d.)\nsteady state. We present a sufficient condition for a system to have a quantum\ni.i.d. steady state by identifying a set of operators that commute with\narbitrary quantum i.i.d. states. In particular, a set of quantum i.i.d. states\nis found to be an invariant subset of time evolution superoperators for systems\nthat satisfy the sufficient condition. These findings not only identify a class\nof models with exactly solvable steady states but also lead to a no-go theorem\nthat precludes quantum entanglement and spatial correlations in a broad class\nof quantum many-body steady states in a dissipative environment.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u6ee1\u8db3\u7279\u5b9a\u6761\u4ef6\u7684\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u53ef\u4ee5\u62e5\u6709\u72ec\u7acb\u7684\u7a33\u6001\uff0c\u4f46\u8fd9\u79cd\u7a33\u6001\u6392\u9664\u4e86\u91cf\u5b50\u7ea0\u7f20\u548c\u7a7a\u95f4\u5173\u8054\u3002", "motivation": "\u7814\u7a76\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u5982\u4f55\u7a33\u5b9a\u7ef4\u6301\u975e\u5e73\u8861\u7a33\u6001\u5bf9\u4e8e\u63a2\u7d22\u548c\u5229\u7528\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u5177\u6709\u57fa\u672c\u548c\u5b9e\u9645\u7684\u91cd\u8981\u6027\u3002", "method": "\u901a\u8fc7\u8bc6\u522b\u4e0e\u4efb\u610f\u91cf\u5b50i.i.d.\u72b6\u6001\u4ea4\u6362\u7684\u7b97\u5b50\u96c6\u5408\uff0c\u63a8\u5bfc\u51fa\u4e86\u4e00\u4e2a\u4f7f\u7cfb\u7edf\u5177\u6709\u91cf\u5b50i.i.d.\u7a33\u6001\u7684\u5145\u5206\u6761\u4ef6\u3002\u8be5\u6761\u4ef6\u8868\u660e\uff0c\u5bf9\u4e8e\u6ee1\u8db3\u8be5\u5145\u5206\u6761\u4ef6\u7684\u7cfb\u7edf\uff0c\u91cf\u5b50i.i.d.\u72b6\u6001\u96c6\u5408\u662f\u65f6\u95f4\u6f14\u5316\u8d85\u7b97\u5b50\u4e0b\u7684\u4e0d\u53d8\u5b50\u96c6\u3002", "result": "\u786e\u7acb\u4e86\u5f00\u653e\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u5728\u5c40\u57df\u9a71\u52a8\u548c/\u6216\u8017\u6563\u4f5c\u7528\u4e0b\u5177\u6709\u91cf\u5b50i.i.d.\u7a33\u6001\u7684\u901a\u7528\u7b49\u4ef7\u6761\u4ef6\u3002\u63d0\u51fa\u4e86\u4e00\u4e2a\u5145\u5206\u6761\u4ef6\uff0c\u7528\u4e8e\u8bc6\u522b\u5177\u6709\u91cf\u5b50i.i.d.\u7a33\u6001\u7684\u7cfb\u7edf\uff0c\u5e76\u53d1\u73b0\u91cf\u5b50i.i.d.\u72b6\u6001\u96c6\u662f\u6ee1\u8db3\u8be5\u5145\u5206\u6761\u4ef6\u7cfb\u7edf\u7684\u6f14\u5316\u4e0d\u53d8\u5b50\u96c6\u3002\u8fd9\u4e9b\u53d1\u73b0\u4e0d\u4ec5\u8bc6\u522b\u4e86\u4e00\u7c7b\u5177\u6709\u7cbe\u786e\u53ef\u89e3\u7a33\u6001\u7684\u6a21\u578b\uff0c\u800c\u4e14\u5f97\u51fa\u4e86\u4e00\u4e2a\u201c\u4e0d\u53ef\u4e3a\u201d\u5b9a\u7406\uff0c\u6392\u9664\u4e86\u5728\u8017\u6563\u73af\u5883\u4e2d\u7684\u5e7f\u6cdb\u91cf\u5b50\u591a\u4f53\u7a33\u6001\u4e2d\u7684\u91cf\u5b50\u7ea0\u7f20\u548c\u7a7a\u95f4\u5173\u8054\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5f00\u653e\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u5177\u6709\u91cf\u5b50\u72ec\u7acb\u540c\u5206\u5e03\uff08i.i.d.\uff09\u7a33\u6001\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5145\u5206\u6761\u4ef6\uff0c\u5e76\u7531\u6b64\u63a8\u5bfc\u51fa\u4e00\u4e2a\u4e0d\u5b58\u5728\u91cf\u5b50\u7ea0\u7f20\u548c\u7a7a\u95f4\u5173\u8054\u7684\u201c\u4e0d\u53ef\u4e3a\u201d\u5b9a\u7406\uff0c\u9002\u7528\u4e8e\u5e7f\u6cdb\u7684\u8017\u6563\u73af\u5883\u4e2d\u7684\u91cf\u5b50\u591a\u4f53\u7a33\u6001\u3002"}}
{"id": "2507.10475", "categories": ["cs.CL", "cs.AI", "I.2.7; H.3.3"], "pdf": "https://arxiv.org/pdf/2507.10475", "abs": "https://arxiv.org/abs/2507.10475", "authors": ["\u0130smail Tar\u0131m", "Aytu\u011f Onan"], "title": "Can You Detect the Difference?", "comment": "11 pages, 3 figures, 2 tables. Code and data:\n  https://github.com/ismailtrm/ceng_404. Cross-list requested to cs.AI for\n  AI-safety relevance", "summary": "The rapid advancement of large language models (LLMs) has raised concerns\nabout reliably detecting AI-generated text. Stylometric metrics work well on\nautoregressive (AR) outputs, but their effectiveness on diffusion-based models\nis unknown. We present the first systematic comparison of diffusion-generated\ntext (LLaDA) and AR-generated text (LLaMA) using 2 000 samples. Perplexity,\nburstiness, lexical diversity, readability, and BLEU/ROUGE scores show that\nLLaDA closely mimics human text in perplexity and burstiness, yielding high\nfalse-negative rates for AR-oriented detectors. LLaMA shows much lower\nperplexity but reduced lexical fidelity. Relying on any single metric fails to\nseparate diffusion outputs from human writing. We highlight the need for\ndiffusion-aware detectors and outline directions such as hybrid models,\ndiffusion-specific stylometric signatures, and robust watermarking.", "AI": {"tldr": "LLaDA\uff08\u6269\u6563\u6a21\u578b\uff09\u751f\u6210\u7684\u6587\u672c\u5728\u56f0\u60d1\u5ea6\u548c\u7a81\u53d1\u6027\u4e0a\u4e0e\u4eba\u7c7b\u6587\u672c\u975e\u5e38\u76f8\u4f3c\uff0c\u4f7f\u5f97\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u5bb9\u6613\u51fa\u9519\u3002\u5355\u4e00\u6307\u6807\u65e0\u6cd5\u6709\u6548\u533a\u5206AI\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u5199\u4f5c\uff0c\u9700\u8981\u5f00\u53d1\u65b0\u7684\u68c0\u6d4b\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30AI\u751f\u6210\u6587\u672c\u68c0\u6d4b\u7684\u53ef\u9760\u6027\uff0c\u7279\u522b\u662f\u6269\u6563\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u80fd\u529b\uff0c\u56e0\u4e3a\u73b0\u6709\u98ce\u683c\u8ba1\u91cf\u6307\u6807\u5728\u6269\u6563\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u5c1a\u4e0d\u660e\u786e\u3002", "method": "\u4f7f\u75282000\u4e2a\u6837\u672c\u5bf9LLaDA\uff08\u6269\u6563\u6a21\u578b\uff09\u548cLLaMA\uff08\u81ea\u56de\u5f52\u6a21\u578b\uff09\u751f\u6210\u7684\u6587\u672c\u8fdb\u884c\u7cfb\u7edf\u6027\u6bd4\u8f83\u3002", "result": "LLaDA\u5728\u56f0\u60d1\u5ea6\u548c\u7a81\u53d1\u6027\u65b9\u9762\u4e0e\u4eba\u7c7b\u6587\u672c\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u5bfc\u81f4\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u8bbe\u8ba1\u7684\u68c0\u6d4b\u5668\u4ea7\u751f\u9ad8\u5047\u9634\u6027\u7387\u3002LLaMA\u7684\u56f0\u60d1\u5ea6\u8f83\u4f4e\uff0c\u4f46\u8bcd\u6c47\u4fdd\u771f\u5ea6\u4e5f\u964d\u4f4e\u4e86\u3002", "conclusion": "\u5355\u4e00\u6307\u6807\u65e0\u6cd5\u533a\u5206\u6269\u6563\u6a21\u578b\u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u5199\u4f5c\u3002\u9700\u8981\u5f00\u53d1\u80fd\u591f\u8bc6\u522b\u6269\u6563\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u68c0\u6d4b\u5668\uff0c\u5e76\u63d0\u51fa\u6df7\u5408\u6a21\u578b\u3001\u7279\u5b9a\u4e8e\u6269\u6563\u6a21\u578b\u98ce\u683c\u7684\u7b7e\u540d\u548c\u9c81\u68d2\u6c34\u5370\u7b49\u65b9\u5411\u3002"}}
{"id": "2507.09362", "categories": ["cs.LG", "q-bio.PE"], "pdf": "https://arxiv.org/pdf/2507.09362", "abs": "https://arxiv.org/abs/2507.09362", "authors": ["Assaf Marron", "Smadar Szekely", "Irun Cohen", "David Harel"], "title": "Meta-autoencoders: An approach to discovery and representation of relationships between dynamically evolving classes", "comment": null, "summary": "An autoencoder (AE) is a neural network that, using self-supervised training,\nlearns a succinct parameterized representation, and a corresponding encoding\nand decoding process, for all instances in a given class. Here, we introduce\nthe concept of a meta-autoencoder (MAE): an AE for a collection of\nautoencoders. Given a family of classes that differ from each other by the\nvalues of some parameters, and a trained AE for each class, an MAE for the\nfamily is a neural net that has learned a compact representation and associated\nencoder and decoder for the class-specific AEs. One application of this general\nconcept is in research and modeling of natural evolution -- capturing the\ndefining and the distinguishing properties across multiple species that are\ndynamically evolving from each other and from common ancestors. In this interim\nreport we provide a constructive definition of MAEs, initial examples, and the\nmotivating research directions in machine learning and biology.", "AI": {"tldr": "Introduces meta-autoencoders (MAEs), which are autoencoders for autoencoders, useful for modeling evolving systems like species.", "motivation": "The motivation is to learn a compact representation and associated encoder and decoder for a collection of autoencoders, which can be applied to areas like natural evolution modeling to capture distinguishing properties across evolving species.", "method": "The paper provides a constructive definition of MAEs and initial examples.", "result": "Initial examples and motivating research directions in machine learning and biology are provided.", "conclusion": "This paper introduces the concept of a meta-autoencoder (MAE), which is an AE for a collection of autoencoders. MAEs can learn a compact representation and associated encoder and decoder for class-specific AEs, with potential applications in machine learning and biology, such as modeling natural evolution."}}
{"id": "2507.09514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09514", "abs": "https://arxiv.org/abs/2507.09514", "authors": ["Tien-Yu Chi", "Hung-Yueh Chiang", "Diana Marculescu", "Kai-Chiang Wu"], "title": "QuarterMap: Efficient Post-Training Token Pruning for Visual State Space Models", "comment": "Accepted by Efficient Systems for Foundation Models Workshop at the\n  International Conference on Machine Learning (ICML) 2025", "summary": "State space models (SSMs) reduce the quadratic complexity of transformers by\nleveraging linear recurrence. Recently, VMamba has emerged as a strong\nSSM-based vision backbone, yet remains bottlenecked by spatial redundancy in\nits four-directional scan. We propose QuarterMap, a post-training activation\npruning method that removes redundant spatial activations before scanning and\nrestores dimensions via nearest-neighbor upsampling. Our method improves\nthroughput without retraining. On ImageNet-1K, QuarterMap achieves up to 11%\nspeedup on VMamba with less than 0.9% accuracy drop, and yields similar gains\non ADE20K segmentation. Beyond VMamba, we validate QuarterMap on MedMamba, a\ndomain-specific model that shares the same four-directional scanning structure,\nwhere it consistently improves throughput while preserving accuracy across\nmultiple medical imaging tasks. Compared to token merging methods like ToMe,\nQuarterMap is tailored for SSMs and avoids costly merge-unmerge operations. Our\nmethod offers a plug-and-play tool for deployment-time efficiency without\ncompromising transferability.", "AI": {"tldr": "QuarterMap \u662f\u4e00\u79cd\u65b0\u7684\u8bad\u7ec3\u540e\u526a\u679d\u6280\u672f\uff0c\u53ef\u4ee5\u63d0\u9ad8 VMamba \u7b49\u57fa\u4e8e SSM \u7684\u6a21\u578b\u7684\u6548\u7387\uff0c\u901a\u8fc7\u79fb\u9664\u5197\u4f59\u6fc0\u6d3b\u5e76\u4f7f\u7528\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u6062\u590d\u7ef4\u5ea6\uff0c\u4ece\u800c\u5b9e\u73b0\u52a0\u901f\u800c\u51e0\u4e4e\u6ca1\u6709\u51c6\u786e\u6027\u635f\u5931\u3002", "motivation": "VMamba \u4f5c\u4e3a\u4e00\u79cd\u57fa\u4e8e SSM \u7684\u89c6\u89c9\u9aa8\u5e72\u7f51\u7edc\uff0c\u867d\u7136\u5229\u7528\u7ebf\u6027\u9012\u589e\u51cf\u5c11\u4e86 Transformer \u7684\u4e8c\u6b21\u590d\u6742\u6027\uff0c\u4f46\u5728\u5176\u56db\u5411\u626b\u63cf\u4e2d\u4ecd\u7136\u5b58\u5728\u7a7a\u95f4\u5197\u4f59\u74f6\u9888\u3002", "method": "QuarterMap \u662f\u4e00\u79cd\u8bad\u7ec3\u540e\u6fc0\u6d3b\u526a\u679d\u65b9\u6cd5\uff0c\u901a\u8fc7\u79fb\u9664\u5197\u4f59\u7a7a\u95f4\u6fc0\u6d3b\u6765\u51cf\u5c11 SSM \u4e2d\u7684\u7a7a\u95f4\u5197\u4f59\uff0c\u5e76\u901a\u8fc7\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u6765\u6062\u590d\u7ef4\u5ea6\u3002", "result": "QuarterMap \u5728 ImageNet-1K \u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe 11% \u7684 VMamba \u52a0\u901f\uff0c\u51c6\u786e\u7387\u4e0b\u964d\u4e0d\u5230 0.9%\uff0c\u5e76\u5728 ADE20K \u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u76f8\u4f3c\u7684\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5728 MedMamba \u7b49\u7279\u5b9a\u9886\u57df\u6a21\u578b\u4e0a\u4e5f\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u51c6\u786e\u6027\u3002\u4e0e ToMe \u7b49\u4ee4\u724c\u5408\u5e76\u65b9\u6cd5\u76f8\u6bd4\uff0cQuarterMap \u4e13\u4e3a SSM \u8bbe\u8ba1\uff0c\u907f\u514d\u4e86\u6602\u8d35\u7684\u5408\u5e76-\u53d6\u6d88\u5408\u5e76\u64cd\u4f5c\u3002", "conclusion": "QuarterMap \u662f\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u90e8\u7f72\u65f6\u6548\u7387\u4f18\u5316\u5de5\u5177\uff0c\u53ef\u5728\u4e0d\u5f71\u54cd\u53ef\u8fc1\u79fb\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u5728\u626b\u63cf\u524d\u79fb\u9664\u5197\u4f59\u7a7a\u95f4\u6fc0\u6d3b\u5e76\u8fdb\u884c\u6700\u8fd1\u90bb\u4e0a\u91c7\u6837\u6765\u6062\u590d\u7ef4\u5ea6\uff0c\u4ece\u800c\u63d0\u9ad8 SSM \u89c6\u89c9\u9aa8\u5e72\u7684\u541e\u5410\u91cf\u3002"}}
{"id": "2507.10356", "categories": ["quant-ph", "physics.atom-ph"], "pdf": "https://arxiv.org/pdf/2507.10356", "abs": "https://arxiv.org/abs/2507.10356", "authors": ["Gina Warttmann", "Florian Meinert", "Hans Peter B\u00fcchler", "Sebastian Weber"], "title": "Suppressing crosstalk for Rydberg quantum gates", "comment": "7 pages, 5 figures", "summary": "We present a method to suppress crosstalk from implementing controlled-Z\ngates via local addressing in neutral atom quantum computers. In these systems,\na fraction of the laser light that is applied locally to implement gates\ntypically leaks to other atoms. We analyze the resulting crosstalk in a setup\nof two gate atoms and one neighboring third atom. We then perturbatively derive\na spin-echo-inspired gate protocol that suppresses the leading order of the\namplitude error, which dominates the crosstalk. Numerical simulations\ndemonstrate that our gate protocol improves the fidelity by two orders of\nmagnitude across a broad range of experimentally relevant parameters. To\nfurther reduce the infidelity, we develop a circuit to cancel remaining phase\nerrors. Our results pave the way for using local addressing for high-fidelity\nquantum gates on Rydberg-based quantum computers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u95e8\u534f\u8bae\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4e2d\u6027\u539f\u5b50\u91cf\u5b50\u8ba1\u7b97\u673a\u4e2d\u5c40\u90e8\u5bfb\u5740\u63a7\u5236\u7684Z\u95e8\u7684\u4fdd\u771f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e2d\uff0c\u5c40\u90e8\u5bfb\u5740\u7684\u6fc0\u5149\u5728\u5b9e\u73b0\u95e8\u64cd\u4f5c\u65f6\u4f1a\u6cc4\u6f0f\u5230\u5176\u4ed6\u539f\u5b50\uff0c\u4ece\u800c\u5f15\u8d77\u4e32\u6270\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u81ea\u65cb\u56de\u6ce2\u53d7\u542f\u53d1\u7684\u95e8\u534f\u8bae\uff0c\u7528\u4e8e\u6291\u5236\u5c40\u90e8\u5bfb\u5740\u63a7\u5236\u7684Z\u95e8\u4e2d\u7684\u4e32\u6270\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u4e00\u4e2a\u8865\u507f\u5269\u4f59\u76f8\u4f4d\u8bef\u5dee\u7684\u7535\u8def\u6765\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4fdd\u771f\u5ea6\u3002", "result": "\u8be5\u95e8\u534f\u8bae\u5c06\u4fdd\u771f\u5ea6\u63d0\u9ad8\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5e76\u901a\u8fc7\u76f8\u4f4d\u8bef\u5dee\u8865\u507f\u7535\u8def\u8fdb\u4e00\u6b65\u964d\u4f4e\u4e86\u9519\u8bef\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9a8c\u76f8\u5173\u53c2\u6570\u8303\u56f4\u5185\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6211\u4eec\u7684\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8e\u91cc\u5fb7\u5821\u7684\u91cf\u5b50\u8ba1\u7b97\u673a\u4f7f\u7528\u5c40\u90e8\u5bfb\u5740\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u95e8\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2507.10524", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10524", "abs": "https://arxiv.org/abs/2507.10524", "authors": ["Sangmin Bae", "Yujin Kim", "Reza Bayat", "Sungnyun Kim", "Jiyoun Ha", "Tal Schuster", "Adam Fisch", "Hrayr Harutyunyan", "Ziwei Ji", "Aaron Courville", "Se-Young Yun"], "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive Token-Level Computation", "comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions", "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.", "AI": {"tldr": "MoR\u6846\u67b6\u901a\u8fc7\u53c2\u6570\u5171\u4eab\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\u7ed3\u5408\uff0c\u5728\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u6269\u5c55\u8bed\u8a00\u6a21\u578b\u5e26\u6765\u7684\u9ad8\u6602\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u95ee\u9898\uff0c\u540c\u65f6\u5b9e\u73b0\u53c2\u6570\u5171\u4eab\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\u3002", "method": "MoR\u901a\u8fc7\u91cd\u7528\u5c42\u5806\u6808\u548c\u8f7b\u91cf\u7ea7\u8def\u7531\u5668\u5b9e\u73b0\u53c2\u6570\u6548\u7387\u548c\u81ea\u9002\u5e94\u8ba1\u7b97\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cdKV\u5171\u4eab\u53d8\u4f53\u4ee5\u51cf\u5c11\u9884\u586b\u5145\u5ef6\u8fdf\u548c\u5185\u5b58\u5360\u7528\u3002", "result": "\u57281.35\u4ebf\u81f317\u4ebf\u53c2\u6570\u7684\u6a21\u578b\u89c4\u6a21\u4e0a\uff0cMoR\u5728\u76f8\u540c\u7684\u8bad\u7ec3FLOPs\u548c\u8f83\u5c0f\u7684\u6a21\u578b\u5c3a\u5bf8\u4e0b\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u9a8c\u8bc1\u56f0\u60d1\u5ea6\uff0c\u63d0\u9ad8\u4e86\u5c11\u6837\u672c\u51c6\u786e\u7387\uff0c\u5e76\u4e0e\u73b0\u6709\u7684\u9012\u5f52\u57fa\u7ebf\u76f8\u6bd4\u63d0\u9ad8\u4e86\u541e\u5410\u91cf\u3002", "conclusion": "MoR\u662f\u4e00\u79cd\u6709\u6548\u7684\u5b9e\u73b0\u5927\u578b\u6a21\u578b\u8d28\u91cf\u800c\u65e0\u9700\u627f\u62c5\u5927\u578b\u6a21\u578b\u6210\u672c\u7684\u9014\u5f84\u3002"}}
{"id": "2507.09382", "categories": ["cs.LG", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09382", "abs": "https://arxiv.org/abs/2507.09382", "authors": ["Bojian Hou", "Zhanliang Wang", "Zhuoping Zhou", "Boning Tong", "Zexuan Wang", "Jingxuan Bao", "Duy Duong-Tran", "Qi Long", "Li Shen"], "title": "Fair CCA for Fair Representation Learning: An ADNI Study", "comment": null, "summary": "Canonical correlation analysis (CCA) is a technique for finding correlations\nbetween different data modalities and learning low-dimensional representations.\nAs fairness becomes crucial in machine learning, fair CCA has gained attention.\nHowever, previous approaches often overlook the impact on downstream\nclassification tasks, limiting applicability. We propose a novel fair CCA\nmethod for fair representation learning, ensuring the projected features are\nindependent of sensitive attributes, thus enhancing fairness without\ncompromising accuracy. We validate our method on synthetic data and real-world\ndata from the Alzheimer's Disease Neuroimaging Initiative (ADNI), demonstrating\nits ability to maintain high correlation analysis performance while improving\nfairness in classification tasks. Our work enables fair machine learning in\nneuroimaging studies where unbiased analysis is essential.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u5e73CCA\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u516c\u5e73\u6027\u4e14\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u3002", "motivation": "\u4e4b\u524d\u7684\u516c\u5e73CCA\u65b9\u6cd5\u5f80\u5f80\u5ffd\u7565\u4e86\u5bf9\u4e0b\u6e38\u5206\u7c7b\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u5728\u63d0\u9ad8\u516c\u5e73\u6027\u7684\u540c\u65f6\u4e0d\u635f\u5bb3\u51c6\u786e\u6027\u7684\u516c\u5e73CCA\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u516c\u5e73CCA\u65b9\u6cd5\uff0c\u786e\u4fdd\u6295\u5f71\u540e\u7684\u7279\u5f81\u72ec\u7acb\u4e8e\u654f\u611f\u5c5e\u6027\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u548cADNI\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u76f8\u5173\u6027\u5206\u6790\u6027\u80fd\u7684\u540c\u65f6\uff0c\u80fd\u591f\u63d0\u9ad8\u5206\u7c7b\u4efb\u52a1\u7684\u516c\u5e73\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u516c\u5e73CCA\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u76f8\u5173\u6027\u5206\u6790\u6027\u80fd\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u5206\u7c7b\u4efb\u52a1\u7684\u516c\u5e73\u6027\uff0c\u4f7f\u5176\u5728\u795e\u7ecf\u5f71\u50cf\u7814\u7a76\u7b49\u9700\u8981\u65e0\u504f\u5206\u6790\u7684\u9886\u57df\u5177\u6709\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.09524", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09524", "abs": "https://arxiv.org/abs/2507.09524", "authors": ["Yunwei Lan", "Zhigao Cui", "Xin Luo", "Chang Liu", "Nian Wang", "Menglin Zhang", "Yanzhao Su", "Dong Liu"], "title": "When Schr\u00f6dinger Bridge Meets Real-World Image Dehazing with Unpaired Training", "comment": "Accepted by ICCV2025", "summary": "Recent advancements in unpaired dehazing, particularly those using GANs, show\npromising performance in processing real-world hazy images. However, these\nmethods tend to face limitations due to the generator's limited transport\nmapping capability, which hinders the full exploitation of their effectiveness\nin unpaired training paradigms. To address these challenges, we propose\nDehazeSB, a novel unpaired dehazing framework based on the Schr\\\"odinger\nBridge. By leveraging optimal transport (OT) theory, DehazeSB directly bridges\nthe distributions between hazy and clear images. This enables optimal transport\nmappings from hazy to clear images in fewer steps, thereby generating\nhigh-quality results. To ensure the consistency of structural information and\ndetails in the restored images, we introduce detail-preserving regularization,\nwhich enforces pixel-level alignment between hazy inputs and dehazed outputs.\nFurthermore, we propose a novel prompt learning to leverage pre-trained CLIP\nmodels in distinguishing hazy images and clear ones, by learning a haze-aware\nvision-language alignment. Extensive experiments on multiple real-world\ndatasets demonstrate our method's superiority. Code:\nhttps://github.com/ywxjm/DehazeSB.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDehazeSB\u7684\u65b0\u578b\u975e\u914d\u5bf9\u53bb\u96fe\u6846\u67b6\uff0c\u5b83\u5229\u7528Schrodinger Bridge\u548c\u6700\u4f18\u4f20\u8f93\u7406\u8bba\u6765\u6539\u8fdb\u6a21\u7cca\u5230\u6e05\u6670\u56fe\u50cf\u7684\u6620\u5c04\uff0c\u5e76\u901a\u8fc7\u7ec6\u8282\u4fdd\u7559\u6b63\u5219\u5316\u548cCLIP\u63d0\u793a\u5b66\u4e60\u6765\u63d0\u9ad8\u6027\u80fd\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u975e\u914d\u5bf9\u53bb\u96fe\u65b9\u6cd5\uff08\u5c24\u5176\u662f\u4f7f\u7528GANs\u7684\u65b9\u6cd5\uff09\u5728\u5904\u7406\u771f\u5b9e\u4e16\u754c\u6a21\u7cca\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u6709\u5e0c\u671b\u7684\u6027\u80fd\uff0c\u4f46\u7531\u4e8e\u751f\u6210\u5668\u6709\u9650\u7684\u4f20\u8f93\u6620\u5c04\u80fd\u529b\u800c\u53d7\u5230\u9650\u5236\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u5728\u975e\u914d\u5bf9\u8bad\u7ec3\u8303\u5f0f\u4e2d\u7684\u6709\u6548\u6027\u5f97\u5230\u5145\u5206\u53d1\u6325\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSchrodinger Bridge\u7684\u65b0\u578b\u975e\u914d\u5bf9\u53bb\u96fe\u6846\u67b6DehazeSB\uff0c\u5229\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u7406\u8bba\u76f4\u63a5\u8fde\u63a5\u6a21\u7cca\u548c\u6e05\u6670\u56fe\u50cf\u7684\u5206\u5e03\uff0c\u4ece\u800c\u4ee5\u66f4\u5c11\u7684\u6b65\u9aa4\u5b9e\u73b0\u4ece\u6a21\u7cca\u5230\u6e05\u6670\u7684\u6700\u4f73\u4f20\u8f93\u6620\u5c04\u3002\u5f15\u5165\u4e86\u4fdd\u6301\u7ec6\u8282\u7684\u6b63\u5219\u5316\uff0c\u4ee5\u786e\u4fdd\u6062\u590d\u56fe\u50cf\u4e2d\u7ed3\u6784\u4fe1\u606f\u548c\u7ec6\u8282\u7684\u4e00\u81f4\u6027\uff0c\u901a\u8fc7\u5f3a\u5236\u6267\u884c\u50cf\u7d20\u7ea7\u5bf9\u9f50\u6765\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63d0\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u9884\u8bad\u7ec3\u7684CLIP\u6a21\u578b\uff0c\u901a\u8fc7\u5b66\u4e60\u611f\u77e5\u96fe\u6c14\u7684\u89c6\u89c9-\u8bed\u8a00\u5bf9\u9f50\u6765\u533a\u5206\u6a21\u7cca\u56fe\u50cf\u548c\u6e05\u6670\u56fe\u50cf\u3002", "result": "DehazeSB\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u53bb\u96fe\u56fe\u50cf\uff0c\u5e76\u5728\u7ed3\u6784\u4fe1\u606f\u548c\u7ec6\u8282\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684DehazeSB\u6846\u67b6\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002"}}
{"id": "2507.10361", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10361", "abs": "https://arxiv.org/abs/2507.10361", "authors": ["Jan Krause", "Nino Walenta"], "title": "Exponential-recovery model for free-running SPADs with capacity-induced dead-time imperfections", "comment": null, "summary": "Current count-rate models for single-photon avalanche diodes (SPADs)\ntypically assume an instantaneous recovery of the quantum efficiency following\ndead-time, leading to a systematic overestimation of the effective detection\nefficiency for high photon flux. To overcome this limitation, we introduce a\ngeneralized analytical count-rate model for free-running SPADs that models the\nnon-instantaneous, exponential recovery of the quantum efficiency following\ndead-time. Our model, framed within the theory of non-homogeneous Poisson\nprocesses, only requires one additional detector parameter -- the\nexponential-recovery time constant $\\tau_\\mathrm{r}$. The model accurately\npredicts detection statistics deep into the saturation regime, outperforming\nthe conventional step-function model by two orders of magnitude in terms of the\nimpinging photon rate. For extremely high photon flux, we further extend the\nmodel to capture paralyzation effects. Beyond photon flux estimation, our model\nsimplifies SPAD characterization by enabling the extraction of quantum\nefficiency $\\eta_0$, dead-time $\\tau_\\mathrm{d}$, and recovery time constant\n$\\tau_\\mathrm{r}$ from a single inter-detection interval histogram. This can be\nachieved with a simple setup, without the need for pulsed lasers or externally\ngated detectors. We anticipate broad applicability of our model in quantum key\ndistribution (QKD), time-correlated single-photon counting (TCSPC), LIDAR, and\nrelated areas. Furthermore, the model is readily adaptable to other types of\ndead-time-limited detectors. A Python implementation is provided as\nsupplementary material for swift adoption.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6539\u8fdb\u7684 SPAD \u8ba1\u6570\u7387\u6a21\u578b\uff0c\u80fd\u66f4\u51c6\u786e\u5730\u5904\u7406\u9ad8\u5149\u5b50\u901a\u91cf\u548c\u6b7b\u533a\u65f6\u95f4\u540e\u7684\u6307\u6570\u6062\u590d\uff0c\u7b80\u5316\u4e86\u63a2\u6d4b\u5668\u8868\u5f81\uff0c\u5e76\u5728 QKD\u3001TCSPC\u3001LIDAR \u7b49\u9886\u57df\u6709\u5e7f\u6cdb\u5e94\u7528\u524d\u666f\u3002", "motivation": "\u5f53\u524d\u7684 SPAD \u8ba1\u6570\u7387\u6a21\u578b\u901a\u5e38\u5047\u8bbe\u91cf\u5b50\u6548\u7387\u5728\u6b7b\u533a\u65f6\u95f4\u540e\u77ac\u65f6\u6062\u590d\uff0c\u8fd9\u5728\u9ad8\u5149\u5b50\u901a\u91cf\u4e0b\u4f1a\u5bfc\u81f4\u5bf9\u6709\u6548\u63a2\u6d4b\u6548\u7387\u7684\u7cfb\u7edf\u6027\u9ad8\u4f30\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5e7f\u4e49\u7684\u89e3\u6790\u8ba1\u6570\u7387\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u7531\u8fd0\u884c\u7684 SPAD\uff0c\u8be5\u6a21\u578b\u6a21\u62df\u4e86\u6b7b\u533a\u65f6\u95f4\u540e\u91cf\u5b50\u6548\u7387\u7684\u975e\u77ac\u65f6\u3001\u6307\u6570\u6062\u590d\u8fc7\u7a0b\uff0c\u5e76\u5c06\u5176\u7f6e\u4e8e\u975e\u9f50\u6b21\u6cca\u677e\u8fc7\u7a0b\u7684\u7406\u8bba\u6846\u67b6\u5185\u3002", "result": "\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u9971\u548c\u533a\u57df\u7684\u63a2\u6d4b\u7edf\u8ba1\u6570\u636e\uff0c\u4e0e\u4f20\u7edf\u9636\u8dc3\u51fd\u6570\u6a21\u578b\u76f8\u6bd4\uff0c\u5728\u8f93\u5165\u5149\u5b50\u7387\u65b9\u9762\u63d0\u9ad8\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u901a\u8fc7\u5355\u6b21\u63a2\u6d4b\u95f4\u9694\u76f4\u65b9\u56fe\u5373\u53ef\u63d0\u53d6\u91cf\u5b50\u6548\u7387 \u03b7_0\u3001\u6b7b\u533a\u65f6\u95f4 \u03c4_d \u548c\u6062\u590d\u65f6\u95f4\u5e38\u6570 \u03c4_r\uff0c\u7b80\u5316\u4e86 SPAD \u7684\u8868\u5f81\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u5305\u542b\u4e00\u4e2a\u989d\u5916\u7684\u63a2\u6d4b\u5668\u53c2\u6570\u2014\u2014\u6307\u6570\u6062\u590d\u65f6\u95f4\u5e38\u6570 \u03c4_r\uff0c\u5e76\u501f\u9274\u4e86\u975e\u9f50\u6b21\u6cca\u677e\u8fc7\u7a0b\u7684\u7406\u8bba\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u9971\u548c\u533a\u57df\u7684\u63a2\u6d4b\u7edf\u8ba1\u6570\u636e\uff0c\u5728\u8109\u51b2\u5149\u5b50\u63a2\u6d4b\u5668\uff08SPAD\uff09\u7684\u8ba1\u6570\u7387\u5efa\u6a21\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u5149\u5b50\u901a\u91cf\u4e0b\uff0c\u5176\u6027\u80fd\u8fdc\u8d85\u4f20\u7edf\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u901a\u8fc7\u63d0\u53d6\u91cf\u5b50\u6548\u7387 \u03b7_0\u3001\u6b7b\u533a\u65f6\u95f4 \u03c4_d \u548c\u6062\u590d\u65f6\u95f4\u5e38\u6570 \u03c4_r\uff0c\u7b80\u5316\u4e86 SPAD \u7684\u8868\u5f81\u8fc7\u7a0b\uff0c\u5e76\u6613\u4e8e\u6539\u7f16\u4ee5\u9002\u5e94\u5176\u4ed6\u7c7b\u578b\u7684\u53d7\u6b7b\u533a\u65f6\u95f4\u9650\u5236\u7684\u63a2\u6d4b\u5668\uff0c\u6709\u671b\u5728\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u3001\u65f6\u95f4\u76f8\u5173\u5355\u5149\u5b50\u8ba1\u6570\uff08TCSPC\uff09\u548c\u6fc0\u5149\u96f7\u8fbe\uff08LIDAR\uff09\u7b49\u9886\u57df\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\u3002"}}
{"id": "2507.10535", "categories": ["cs.CL", "cs.AI", "cs.SE"], "pdf": "https://arxiv.org/pdf/2507.10535", "abs": "https://arxiv.org/abs/2507.10535", "authors": ["Hongchao Jiang", "Yiming Chen", "Yushi Cao", "Hung-yi Lee", "Robby T. Tan"], "title": "CodeJudgeBench: Benchmarking LLM-as-a-Judge for Coding Tasks", "comment": "Dataset is available at\n  https://huggingface.co/datasets/mattymchen/codejudgebench", "summary": "Large Language Models (LLMs) have significantly advanced the state-of-the-art\nin various coding tasks. Beyond directly answering user queries, LLMs can also\nserve as judges, assessing and comparing the quality of responses generated by\nother models. Such an evaluation capability is crucial both for benchmarking\ndifferent LLMs and for improving response quality through response ranking.\nHowever, despite the growing adoption of the LLM-as-a-Judge paradigm, its\neffectiveness in coding scenarios remains underexplored due to the absence of\ndedicated benchmarks. To address this gap, we introduce CodeJudgeBench, a\nbenchmark explicitly designed to evaluate the performance of LLM-as-a-Judge\nmodels across three critical coding tasks: code generation, code repair, and\nunit test generation. Through comprehensive benchmarking of 26 LLM-as-a-Judge\nmodels, we find that recent thinking models significantly outperform\nnon-thinking models on our carefully designed code judging tasks. Notably, even\nrelatively small thinking models, such as Qwen3-8B, can outperform specially\ntrained LLM-as-a-Judge models up to 70B in size. Nevertheless, all models still\nexhibit significant randomness in their judgment of coding tasks. For pairwise\njudging tasks, simply changing the order in which responses are presented can\nsubstantially impact accuracy. In addition, when judging code and unit tests\nwritten by different LLMs, LLM-as-a-Judge models also show variance in\nperformance. This sensitivity raises concerns about the reliability and\nconsistency of LLM-as-a-Judge in coding scenarios. Lastly, we study optimal\nprompting strategies for LLM-as-a-Judge. We find that using pair-wise\ncomparison outperforms scalar point-wise judging. Furthermore, retaining\ncomments and reasoning in the full, unprocessed LLM response leads to improved\njudge performance.", "AI": {"tldr": "LLM-as-a-Judge\u5728\u4ee3\u7801\u8bc4\u4f30\u4e2d\u5b58\u5728\u968f\u673a\u6027\u548c\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4f46\u601d\u8003\u6a21\u578b\u4f18\u4e8e\u975e\u601d\u8003\u6a21\u578b\uff0c\u6210\u5bf9\u6bd4\u8f83\u63d0\u793a\u4f18\u4e8e\u6807\u91cf\u8bc4\u5206\u3002", "motivation": "\u5c3d\u7ba1LLM-as-a-Judge\u8303\u5f0f\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u5728\u7f16\u7a0b\u573a\u666f\u4e2d\u7684\u6709\u6548\u6027\u5c1a\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u4e13\u95e8\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51faCodeJudgeBench\u57fa\u51c6\uff0c\u5305\u542b\u4ee3\u7801\u751f\u6210\u3001\u4ee3\u7801\u4fee\u590d\u548c\u5355\u5143\u6d4b\u8bd5\u751f\u6210\u4e09\u4e2a\u5173\u952e\u7f16\u7a0b\u4efb\u52a1\uff0c\u5e76\u5bf926\u4e2aLLM-as-a-Judge\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7814\u7a76\u4e86\u63d0\u793a\u7b56\u7565\u3002", "result": "\u5728CodeJudgeBench\u57fa\u51c6\u4e0a\uff0c\u8fd1\u671f\u601d\u8003\u6a21\u578b\u663e\u8457\u4f18\u4e8e\u975e\u601d\u8003\u6a21\u578b\uff0c\u5176\u4e2dQwen3-8B\u7b49\u5c0f\u578b\u601d\u8003\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u89c4\u6a21\u8fbe70B\u7684\u4e13\u4e1a\u8bad\u7ec3\u6a21\u578b\u3002\u7136\u800c\uff0c\u6240\u6709\u6a21\u578b\u5728\u5224\u65ad\u4ee3\u7801\u4efb\u52a1\u65f6\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u968f\u673a\u6027\uff0c\u54cd\u5e94\u5448\u73b0\u987a\u5e8f\u7684\u6539\u53d8\u4f1a\u663e\u8457\u5f71\u54cd\u51c6\u786e\u6027\u3002", "conclusion": "LLM-as-a-Judge\u6a21\u578b\u5728\u4ee3\u7801\u8bc4\u5224\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u968f\u673a\u6027\uff0c\u5305\u62ec\u5bf9\u54cd\u5e94\u987a\u5e8f\u7684\u654f\u611f\u6027\u4ee5\u53ca\u5728\u8bc4\u5224\u4e0d\u540cLLM\u751f\u6210\u7684\u4ee3\u7801\u548c\u5355\u5143\u6d4b\u8bd5\u65f6\u7684\u6027\u80fd\u5dee\u5f02\u3002\u7814\u7a76\u8868\u660e\uff0c\u6210\u5bf9\u6bd4\u8f83\u7684\u63d0\u793a\u7b56\u7565\u4f18\u4e8e\u6807\u91cf\u8bc4\u5206\uff0c\u5e76\u4e14\u4fdd\u7559LLM\u5b8c\u6574\u54cd\u5e94\u4e2d\u7684\u6ce8\u91ca\u548c\u63a8\u7406\u53ef\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u6027\u80fd\u3002"}}
{"id": "2507.09391", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09391", "abs": "https://arxiv.org/abs/2507.09391", "authors": ["Peter Pao-Huang", "Mitchell Black", "Xiaojie Qiu"], "title": "Geometric Generative Modeling with Noise-Conditioned Graph Networks", "comment": "ICML 2025", "summary": "Generative modeling of graphs with spatial structure is essential across many\napplications from computer graphics to spatial genomics. Recent flow-based\ngenerative models have achieved impressive results by gradually adding and then\nlearning to remove noise from these graphs. Existing models, however, use graph\nneural network architectures that are independent of the noise level, limiting\ntheir expressiveness. To address this issue, we introduce\n\\textit{Noise-Conditioned Graph Networks} (NCGNs), a class of graph neural\nnetworks that dynamically modify their architecture according to the noise\nlevel during generation. Our theoretical and empirical analysis reveals that as\nnoise increases, (1) graphs require information from increasingly distant\nneighbors and (2) graphs can be effectively represented at lower resolutions.\nBased on these insights, we develop Dynamic Message Passing (DMP), a specific\ninstantiation of NCGNs that adapts both the range and resolution of message\npassing to the noise level. DMP consistently outperforms noise-independent\narchitectures on a variety of domains including $3$D point clouds,\nspatiotemporal transcriptomics, and images. Code is available at\nhttps://github.com/peterpaohuang/ncgn.", "AI": {"tldr": "Generative graph models with spatial structure benefit from noise-adaptive neural networks. Introduced Noise-Conditioned Graph Networks (NCGNs) and a specific version, Dynamic Message Passing (DMP), which adjusts message passing range and resolution based on noise, outperforming existing methods on various data types.", "motivation": "Existing flow-based generative models for graphs with spatial structure use graph neural networks that are independent of the noise level, limiting their expressiveness. Addressed this by developing NCGNs that dynamically adapt to noise levels.", "method": "Introduced Noise-Conditioned Graph Networks (NCGNs), a class of graph neural networks that dynamically modify their architecture according to the noise level during generation. Developed Dynamic Message Passing (DMP) as a specific instantiation of NCGNs, adapting both the range and resolution of message passing to the noise level based on theoretical and empirical analysis.", "result": "Empirical analysis shows that as noise increases, graphs require information from increasingly distant neighbors and can be effectively represented at lower resolutions. DMP consistently outperforms noise-independent architectures on 3D point clouds, spatiotemporal transcriptomics, and images.", "conclusion": "NCGNs, particularly the Dynamic Message Passing (DMP) instantiation, outperform noise-independent architectures on various domains including 3D point clouds, spatiotemporal transcriptomics, and images by dynamically adapting the range and resolution of message passing to the noise level."}}
{"id": "2507.09531", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09531", "abs": "https://arxiv.org/abs/2507.09531", "authors": ["Son Nguyen", "Giang Nguyen", "Hung Dao", "Thao Do", "Daeyoung Kim"], "title": "VDInstruct: Zero-Shot Key Information Extraction via Content-Aware Vision Tokenization", "comment": "Under Review", "summary": "Key Information Extraction (KIE) underpins the understanding of visual\ndocuments (e.g., receipts and contracts) by extracting precise semantic content\nand accurately capturing spatial structure. Yet existing multimodal large\nlanguage models (MLLMs) often perform poorly on dense documents and rely on\nvision tokenization approaches that scale with image size, leading to redundant\ncomputation and memory inefficiency. To address these challenges, we introduce\nVDInstruct, an MLLM that separates spatial region detection from semantic\nfeature extraction. Central to our model is a content-aware tokenization\nstrategy: rather than fragmenting the entire image uniformly, it generates\ntokens in proportion to document complexity, preserving critical structure\nwhile eliminating wasted tokens. Leveraging a three-stage training paradigm,\nour model achieves state-of-the-art (SOTA) results on KIE benchmarks, matching\nor exceeding the accuracy of leading approaches while reducing the number of\nimage tokens by roughly 3.6x. In zero-shot evaluations, VDInstruct surpasses\nstrong baselines-such as DocOwl 1.5-by +5.5 F1 points, highlighting its\nrobustness to unseen documents. These findings show that content-aware\ntokenization combined with explicit layout modeling offers a promising\ndirection forward for document understanding. Data, source code, and model\nweights will be made publicly available.", "AI": {"tldr": "VDInstruct\u901a\u8fc7\u5185\u5bb9\u611f\u77e5\u5206\u8bcd\u548c\u663e\u5f0f\u5e03\u5c40\u5efa\u6a21\u89e3\u51b3\u4e86\u5bc6\u96c6\u6587\u6863\u7406\u89e3\u4e2d\u7684\u6548\u7387\u548c\u51c6\u786e\u6027\u95ee\u9898\uff0c\u53d6\u5f97\u4e86SOTA\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5904\u7406\u5bc6\u96c6\u6587\u6863\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e14\u4f9d\u8d56\u4e8e\u968f\u56fe\u50cf\u5927\u5c0f\u6269\u5c55\u7684\u89c6\u89c9\u5206\u8bcd\u65b9\u6cd5\uff0c\u5bfc\u81f4\u8ba1\u7b97\u5197\u4f59\u548c\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\u3002", "method": "VDInstruct\u6a21\u578b\u5c06\u7a7a\u95f4\u533a\u57df\u68c0\u6d4b\u4e0e\u8bed\u4e49\u7279\u5f81\u63d0\u53d6\u5206\u79bb\uff0c\u91c7\u7528\u5185\u5bb9\u611f\u77e5\u5206\u8bcd\u7b56\u7565\uff0c\u6839\u636e\u6587\u6863\u590d\u6742\u5ea6\u751f\u6210\u6807\u8bb0\uff0c\u4ee5\u4fdd\u7559\u5173\u952e\u7ed3\u6784\u5e76\u6d88\u9664\u5197\u4f59\u6807\u8bb0\u3002\u901a\u8fc7\u4e09\u9636\u6bb5\u8bad\u7ec3\u5b9e\u73b0\u3002", "result": "VDInstruct\u5728KIE\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u6210\u679c\uff0c\u5728\u4e0d\u727a\u7272\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u5c06\u56fe\u50cf\u6807\u8bb0\u6570\u91cf\u51cf\u5c11\u4e86\u7ea63.6\u500d\u3002\u5728\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\uff0cVDInstruct\u7684\u8868\u73b0\u4f18\u4e8eDocOwl 1.5\u7b49\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0cF1\u5206\u6570\u63d0\u9ad8\u4e865.5\u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "\u5185\u5bb9\u611f\u77e5\u5206\u8bcd\u4e0e\u663e\u5f0f\u5e03\u5c40\u5efa\u6a21\u4e3a\u6587\u6863\u7406\u89e3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2507.10362", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10362", "abs": "https://arxiv.org/abs/2507.10362", "authors": ["Zvika Brakerski", "Nir Magrafta", "Tomer Solomon"], "title": "State-Based Classical Shadows", "comment": null, "summary": "Classical Shadow Tomography (Huang, Kueng and Preskill, Nature Physics 2020)\nis a method for creating a classical snapshot of an unknown quantum state,\nwhich can later be used to predict the value of an a-priori unknown observable\non that state. In the short time since their introduction, classical shadows\nreceived a lot of attention from the physics, quantum information, and quantum\ncomputing (including cryptography) communities. In particular there has been a\nmajor effort focused on improving the efficiency, and in particular depth, of\ngenerating the classical snapshot.\n  Existing constructions rely on a distribution of unitaries as a central\nbuilding block, and research is devoted to simplifying this family as much as\npossible. We diverge from this paradigm and show that suitable distributions\nover \\emph{states} can be used as the building block instead. Concretely, we\ncreate the snapshot by entangling the unknown input state with an independently\nprepared auxiliary state, and measuring the resulting entangled state. This\nstate-based approach allows us to consider a building block with arguably\nweaker properties that has not been studied so far in the context of classical\nshadows. Notably, our cryptographically-inspired analysis shows that for\n\\emph{efficiently computable} observables, it suffices to use\n\\emph{pseudorandom} families of states. To the best of our knowledge,\n\\emph{computational} classical shadow tomography was not considered in the\nliterature prior to our work.\n  Finally, in terms of efficiency, the online part of our method (i.e.\\ the\npart that depends on the input) is simply performing a measurement in the Bell\nbasis, which can be done in constant depth using elementary gates.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u72b6\u6001\u7684\u7ecf\u5178\u5f71\u96c6\u65ad\u5c42\u626b\u63cf\u65b9\u6cd5\uff0c\u65e0\u9700\u9149\u53d8\u6362\uff0c\u4f7f\u7528\u4f2a\u968f\u673a\u72b6\u6001\u65cf\u5373\u53ef\u9ad8\u6548\u63a8\u65ad\u53ef\u8ba1\u7b97\u89c2\u6d4b\u503c\uff0c\u4e14\u6240\u9700\u6df1\u5ea6\u6052\u5b9a\u3002", "motivation": "\u7ecf\u5178\u7684\u5f71\u96c6\u65ad\u5c42\u626b\u63cf\u65b9\u6cd5\u5728\u521b\u5efa\u7ecf\u5178\u5feb\u7167\u65b9\u9762\u6548\u7387\u6709\u5f85\u63d0\u9ad8\uff0c\u5c24\u5176\u662f\u6df1\u5ea6\u65b9\u9762\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66ff\u4ee3\u7ecf\u5178\u5f71\u96c6\u65ad\u5c42\u626b\u63cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5176\u6548\u7387\u548c\u53ef\u884c\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7ecf\u5178\u5f71\u96c6\u65ad\u5c42\u626b\u63cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u9149\u53d8\u6362\u7684\u5206\u5e03\uff0c\u800c\u662f\u5229\u7528\u72b6\u6001\u7684\u5206\u5e03\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u901a\u8fc7\u5c06\u672a\u77e5\u8f93\u5165\u72b6\u6001\u4e0e\u72ec\u7acb\u5236\u5907\u7684\u8f85\u52a9\u72b6\u6001\u8fdb\u884c\u7ea0\u7f20\uff0c\u7136\u540e\u6d4b\u91cf\u7ea0\u7f20\u6001\u6765\u521b\u5efa\u7ecf\u5178\u5feb\u7167\u3002\u8be5\u65b9\u6cd5\u5c24\u5176\u9002\u7528\u4e8e\u8ba1\u7b97\u53ef\u89c2\u6d4b\u91cf\uff0c\u5e76\u8bc1\u660e\u4e86\u4f7f\u7528\u4f2a\u968f\u673a\u72b6\u6001\u65cf\u5c31\u8db3\u591f\u4e86\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6548\u7387\u65b9\u9762\uff0c\u5c24\u5176\u662f\u8ba1\u7b97\u590d\u6742\u5ea6\u548c\u6df1\u5ea6\u65b9\u9762\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u5728\u7ebf\u90e8\u5206\uff08\u4f9d\u8d56\u4e8e\u8f93\u5165\uff09\u4ec5\u9700\u6267\u884c\u8d1d\u5c14\u57fa\u6d4b\u91cf\uff0c\u5e76\u53ef\u4f7f\u7528\u57fa\u672c\u95e8\u5b9e\u73b0\u6052\u5b9a\u6df1\u5ea6\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7684\u7ecf\u5178\u5f71\u96c6\u65ad\u5c42\u626b\u63cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u4f2a\u968f\u673a\u72b6\u6001\u65cf\u6765\u6784\u5efa\u7ecf\u5178\u5feb\u7167\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5bf9\u53ef\u8ba1\u7b97\u89c2\u6d4b\u503c\u8fdb\u884c\u9ad8\u6548\u63a8\u65ad\u3002"}}
{"id": "2507.10541", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10541", "abs": "https://arxiv.org/abs/2507.10541", "authors": ["Zhuoshi Pan", "Qizhi Pei", "Yu Li", "Qiyao Sun", "Zinan Tang", "H. Vicky Zhao", "Conghui He", "Lijun Wu"], "title": "REST: Stress Testing Large Reasoning Models by Asking Multiple Problems at Once", "comment": "REST (Reasoning Evaluation through Simultaneous Testing), a\n  stress-testing framework that concurrently exposes LRMs to multiple problems\n  simultaneously", "summary": "Recent Large Reasoning Models (LRMs) have achieved remarkable progress on\ntask-specific benchmarks, yet their evaluation methods remain constrained by\nisolated problem-solving paradigms. Existing benchmarks predominantly assess\nsingle-question reasoning through sequential testing, resulting critical\nlimitations: (1) vulnerability to data contamination and less challenging\n(e.g., DeepSeek-R1 achieves 97.0% on MATH500), forcing costly and perpetual\ncreation of new questions with large human efforts, (2) failure to evaluate\nmodels under multi-context pressure, a key requirement for real-world\ndeployment. To bridge this gap, we present REST (Reasoning Evaluation through\nSimultaneous Testing), a stress-testing framework that concurrently exposes\nLRMs to multiple problems simultaneously. Beyond basic reasoning, REST\nspecifically evaluates several under-tested capabilities: contextual priority\nallocation, cross-problem interference resistance, and dynamic cognitive load\nmanagement. Our evaluation reveals several striking findings: Even\nstate-of-the-art (SOTA) models like DeepSeek-R1 exhibit substantial performance\ndegradation under stress testing. Crucially, REST demonstrates stronger\ndiscriminative power than existing benchmarks, revealing pronounced performance\ndifferences among models that exhibit similar, near-ceiling performance under\nsingle-question evaluations. Some key mechanistic insights emerge from our\nanalysis: (1) the \"overthinking trap\" is a critical factor contributing to the\nperformance degradation; (2) the models trained with \"long2short\" technique\npreserve more accuracy of their single-problem performance under REST,\noutperforming standard-trained counterparts. These results establish REST as a\ncost-efficient, future-proof evaluation paradigm that better reflects\nreal-world reasoning demands while reducing reliance on continuous human\nannotation.", "AI": {"tldr": "REST\u662f\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u540c\u65f6\u5411\u5927\u578b\u63a8\u7406\u6a21\u578b\u5448\u73b0\u591a\u4e2a\u95ee\u9898\u6765\u5bf9\u5176\u8fdb\u884c\u538b\u529b\u6d4b\u8bd5\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u8be5\u6846\u67b6\u63ed\u793a\u4e86\u5373\u4f7f\u662f\u6700\u5148\u8fdb\u7684\u6a21\u578b\u5728\u591a\u4efb\u52a1\u5904\u7406\u80fd\u529b\u548c\u5bf9\u5e72\u6270\u7684\u62b5\u6297\u529b\u65b9\u9762\u4e5f\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u53d1\u73b0\u201c\u8fc7\u5ea6\u601d\u8003\u201d\u662f\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u8bc4\u4f30\u65b9\u6cd5\u53d7\u9650\u4e8e\u5b64\u7acb\u7684\u95ee\u9898\u89e3\u51b3\u8303\u5f0f\uff0c\u5bb9\u6613\u53d7\u5230\u6570\u636e\u6c61\u67d3\u7684\u5f71\u54cd\uff0c\u5e76\u4e14\u65e0\u6cd5\u8bc4\u4f30\u6a21\u578b\u5728\u591a\u4e0a\u4e0b\u6587\u538b\u529b\u4e0b\u7684\u8868\u73b0\uff0c\u800c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u90e8\u7f72\u4e2d\u662f\u5173\u952e\u8981\u6c42\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u591a\u4e2a\u95ee\u9898\u5e76\u8bc4\u4f30\u6a21\u578b\u5728\u538b\u529b\u4e0b\u591a\u4efb\u52a1\u5904\u7406\u80fd\u529b\u7684\u6846\u67b6\u3002", "method": "REST\uff08Reasoning Evaluation through Simultaneous Testing\uff09\u662f\u4e00\u4e2a\u538b\u529b\u6d4b\u8bd5\u6846\u67b6\uff0c\u5b83\u540c\u65f6\u5c06\u591a\u4e2a\u95ee\u9898\u66b4\u9732\u7ed9\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u3002\u8be5\u6846\u67b6\u8bc4\u4f30\u4e86\u4e0a\u4e0b\u6587\u4f18\u5148\u7ea7\u5206\u914d\u3001\u8de8\u95ee\u9898\u5e72\u6270\u62b5\u6297\u548c\u52a8\u6001\u8ba4\u77e5\u8d1f\u8377\u7ba1\u7406\u7b49\u80fd\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u50cfDeepSeek-R1\u8fd9\u6837\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728\u538b\u529b\u6d4b\u8bd5\u4e0b\u6027\u80fd\u4e5f\u4f1a\u663e\u8457\u4e0b\u964d\u3002REST\u6bd4\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u5177\u6709\u66f4\u5f3a\u7684\u533a\u5206\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u5728\u5355\u95ee\u9898\u8bc4\u4f30\u4e2d\u8868\u73b0\u76f8\u4f3c\u7684\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u7684\u660e\u663e\u6027\u80fd\u5dee\u5f02\u3002\u6b64\u5916\uff0c\u201c\u8fc7\u5ea6\u601d\u8003\u9677\u9631\u201d\u662f\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\uff0c\u800c\u91c7\u7528\u201clong2short\u201d\u6280\u672f\u8bad\u7ec3\u7684\u6a21\u578b\u5728REST\u6d4b\u8bd5\u4e2d\u6bd4\u6807\u51c6\u8bad\u7ec3\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "REST\u662f\u4e00\u4e2a\u5177\u6709\u6210\u672c\u6548\u76ca\u3001\u9762\u5411\u672a\u6765\u7684\u8bc4\u4f30\u8303\u4f8b\uff0c\u5b83\u80fd\u66f4\u597d\u5730\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7684\u63a8\u7406\u9700\u6c42\uff0c\u540c\u65f6\u51cf\u5c11\u5bf9\u6301\u7eed\u4eba\u5de5\u6807\u6ce8\u7684\u4f9d\u8d56\u3002"}}
{"id": "2507.09394", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09394", "abs": "https://arxiv.org/abs/2507.09394", "authors": ["Nandan Kumar Jha", "Brandon Reagen"], "title": "A Random Matrix Theory Perspective on the Learning Dynamics of Multi-head Latent Attention", "comment": "ICML 2025 Workshop on High-dimensional Learning Dynamics (HiLD)", "summary": "In this work, we study how multi-head latent attention (MLA), a popular\nstrategy for compressing key/value memory, affects a transformer's internal\ncapacity during pretraining. Using a lightweight suite of Marchenko-Pastur (MP)\ndiagnostics, we analyze the spectrum of the $W_{Q}W_{K}^\\top$ gram matrix\nthroughout training, comparing three variants: the standard multi-head\nattention (MHA) baseline, MLA-PreRoPE with rotary applied before compression,\nand MLA-Decoupled, which shares a single rotary sub-vector across all heads.\nOur random matrix analysis reveals \\textbf{three key findings:} \\textbf{ i)}\ncapacity bottlenecks emerge locally: both MHA and MLA-PreRoPE exhibit sharp,\nearly spikes in specific layers that persist and propagate, disrupting the\nbalance between bulk and outlier directions; \\textbf{ ii)} these spikes\ncoincide with rank collapse, concentrating the model's expressivity into narrow\nsubspaces; \\textbf{ iii)} only the decoupled variant prevents this cascade,\nmaintaining broad spectral support and suppressing outlier formation across\nlayers. These results underscore that \\emph{how} rotary embeddings are applied\nis just as critical as \\emph{where} compression occurs. Sharing rotary\ncomponents across heads mitigates spectral fragmentation and preserves\nrepresentational capacity.", "AI": {"tldr": "\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u7684\u5bb9\u91cf\u74f6\u9888\u53ef\u4ee5\u901a\u8fc7\u5728\u5934\u4e4b\u95f4\u5171\u4eab\u65cb\u8f6c\u7ec4\u4ef6\u6765\u7f13\u89e3\uff0c\u4ece\u800c\u4fdd\u6301\u6a21\u578b\u7684\u8868\u793a\u80fd\u529b\u3002", "motivation": "\u7814\u7a76\u591a\u5934\u6f5c\u5728\u6ce8\u610f\u529b\uff08MLA\uff09\u8fd9\u4e00\u538b\u7f29\u952e/\u503c\u5185\u5b58\u7684\u5e38\u7528\u7b56\u7565\u5982\u4f55\u5f71\u54cdTransformer\u5728\u9884\u8bad\u7ec3\u671f\u95f4\u7684\u5185\u90e8\u5bb9\u91cf\u3002", "method": "\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684Marchenko-Pastur\uff08MP\uff09\u8bca\u65ad\uff0c\u5206\u6790\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u4e2d$W_{Q}W_{K}^\top$\uadf8\ub7a8\u77e9\u9635\u7684\u8c31\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u53d8\u4f53\uff1a\u6807\u51c6\u7684MHA\u57fa\u7ebf\u3001MLA-PreRoPE\uff08\u5728\u538b\u7f29\u524d\u5e94\u7528\u65cb\u8f6c\uff09\u4ee5\u53caMLA-Decoupled\uff08\u5728\u6240\u6709\u5934\u4e4b\u95f4\u5171\u4eab\u5355\u4e2a\u65cb\u8f6c\u5b50\u5411\u91cf\uff09\u3002", "result": "\u968f\u673a\u77e9\u9635\u5206\u6790\u63ed\u793a\u4e86\u4e09\u4e2a\u5173\u952e\u53d1\u73b0\uff1a1\uff09\u5bb9\u91cf\u74f6\u9888\u662f\u5c40\u90e8\u51fa\u73b0\u7684\uff0cMHA\u548cMLA-PreRoPE\u5728\u7279\u5b9a\u5c42\u4e2d\u8868\u73b0\u51fa\u5c16\u9510\u3001\u65e9\u671f\u7684\u5cf0\u503c\uff0c\u8fd9\u4e9b\u5cf0\u503c\u4f1a\u6301\u7eed\u5b58\u5728\u5e76\u4f20\u64ad\uff0c\u7834\u574f\u4e86\u6574\u4f53\u65b9\u5411\u548c\u5f02\u5e38\u503c\u65b9\u5411\u4e4b\u95f4\u7684\u5e73\u8861\uff1b2\uff09\u8fd9\u4e9b\u5cf0\u503c\u4e0e\u79e9\u5d29\u6e83\u76f8\u543b\u5408\uff0c\u5c06\u6a21\u578b\u7684\u8868\u73b0\u529b\u96c6\u4e2d\u5728\u72ed\u7a84\u7684\u5b50\u7a7a\u95f4\u4e2d\uff1b3\uff09\u53ea\u6709\u89e3\u8026\u7684\u53d8\u4f53\u624d\u80fd\u963b\u6b62\u8fd9\u79cd\u7ea7\u8054\u6548\u5e94\uff0c\u5728\u5404\u5c42\u4e2d\u4fdd\u6301\u5e7f\u6cdb\u7684\u5149\u8c31\u652f\u6301\u5e76\u6291\u5236\u5f02\u5e38\u503c\u7684\u5f62\u6210\u3002", "conclusion": "\u4e0e\u5982\u4f55\u5728\u538b\u7f29\u4e2d\u5e94\u7528\u65cb\u8f6c\u5d4c\u5165\u76f8\u6bd4\uff0c\u5728\u54ea\u91cc\u5e94\u7528\u65cb\u8f6c\u5d4c\u5165\u66f4\u4e3a\u5173\u952e\u3002\u5728\u5934\u4e4b\u95f4\u5171\u4eab\u65cb\u8f6c\u7ec4\u4ef6\u53ef\u4ee5\u51cf\u8f7b\u5149\u8c31\u788e\u7247\u5316\u5e76\u4fdd\u7559\u8868\u793a\u80fd\u529b\u3002"}}
{"id": "2507.09541", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09541", "abs": "https://arxiv.org/abs/2507.09541", "authors": ["Zihao Xiong", "Fei Zhou", "Fengyi Wu", "Shuai Yuan", "Maixia Fu", "Zhenming Peng", "Jian Yang", "Yimian Dai"], "title": "DRPCA-Net: Make Robust PCA Great Again for Infrared Small Target Detection", "comment": "Accepted by TGRS", "summary": "Infrared small target detection plays a vital role in remote sensing,\nindustrial monitoring, and various civilian applications. Despite recent\nprogress powered by deep learning, many end-to-end convolutional models tend to\npursue performance by stacking increasingly complex architectures, often at the\nexpense of interpretability, parameter efficiency, and generalization. These\nmodels typically overlook the intrinsic sparsity prior of infrared small\ntargets--an essential cue that can be explicitly modeled for both performance\nand efficiency gains. To address this, we revisit the model-based paradigm of\nRobust Principal Component Analysis (RPCA) and propose Dynamic RPCA Network\n(DRPCA-Net), a novel deep unfolding network that integrates the sparsity-aware\nprior into a learnable architecture. Unlike conventional deep unfolding methods\nthat rely on static, globally learned parameters, DRPCA-Net introduces a\ndynamic unfolding mechanism via a lightweight hypernetwork. This design enables\nthe model to adaptively generate iteration-wise parameters conditioned on the\ninput scene, thereby enhancing its robustness and generalization across diverse\nbackgrounds. Furthermore, we design a Dynamic Residual Group (DRG) module to\nbetter capture contextual variations within the background, leading to more\naccurate low-rank estimation and improved separation of small targets.\nExtensive experiments on multiple public infrared datasets demonstrate that\nDRPCA-Net significantly outperforms existing state-of-the-art methods in\ndetection accuracy. Code is available at https://github.com/GrokCV/DRPCA-Net.", "AI": {"tldr": "DRPCA-Net\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\uff0c\u901a\u8fc7\u52a8\u6001\u5c55\u5f00\u548c\u6b8b\u5dee\u7ec4\u6765\u89e3\u51b3\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u590d\u6742\u6027\u548c\u6cdb\u5316\u6027\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u8fc7\u4e8e\u590d\u6742\u3001\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u3001\u53c2\u6570\u6548\u7387\u4f4e\u4e0b\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5e76\u5229\u7528\u7ea2\u5916\u5c0f\u76ee\u6807\u56fa\u6709\u7684\u7a00\u758f\u6027\u5148\u9a8c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRPCA-Net\u7684\u65b0\u578b\u6df1\u5ea6\u5c55\u5f00\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u901a\u8fc7\u8f7b\u91cf\u7ea7\u8d85\u7f51\u7edc\u5f15\u5165\u52a8\u6001\u5c55\u5f00\u673a\u5236\uff0c\u5e76\u8bbe\u8ba1\u4e86\u52a8\u6001\u6b8b\u5dee\u7ec4\uff08DRG\uff09\u6a21\u5757\u4ee5\u66f4\u597d\u5730\u6355\u6349\u80cc\u666f\u4e2d\u7684\u4e0a\u4e0b\u6587\u53d8\u5316\u3002", "result": "DRPCA-Net\u80fd\u591f\u81ea\u9002\u5e94\u5730\u751f\u6210\u9010\u8fed\u4ee3\u53c2\u6570\uff0c\u4ee5\u9002\u5e94\u8f93\u5165\u573a\u666f\uff0c\u4ece\u800c\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u68c0\u6d4b\u7cbe\u5ea6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DRPCA-Net\u5728\u591a\u4e2a\u516c\u5f00\u7ea2\u5916\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5176\u68c0\u6d4b\u7cbe\u5ea6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.10386", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10386", "abs": "https://arxiv.org/abs/2507.10386", "authors": ["Alejandro Mart\u00ednez-M\u00e9ndez", "Jes\u00fas Moreno-Meseguer", "Mariusz Mr\u00f3zek", "Adam Wojciechowski", "Priya Balasubramanian", "Fedor Jelezko", "Javier Prior"], "title": "Optimization and characterization of laser excitation for quantum sensing with single nitrogen-vacancy centres", "comment": "12 pages, 16 figures", "summary": "In this work we present a comprehensive method of characterization and\noptimization of laser irradiation within a confocal microscope tailored to\nquantum sensing experiments using nitrogen-vacancy (NV) centres. While confocal\nmicroscopy is well-suited for such experiments, precise control and\nunderstanding of several optical parameters are essential for reliable\nsingle-emitter studies. We investigate the laser beam intensity profile,\nsingle-photon emission statistics, fluorescence response under varying\npolarization and saturation conditions, spectral characteristics, and the\ntemporal profiles of readout and reinitialization pulses. The beam quality is\nassessed using the beam propagation factor $M^2$, determined via the razorblade\ntechnique. Optical fluorescence spectrum is recorded to confirm NV centre\nemission. To confirm single-emitter operation, we measure second-order\nautocorrelation function $g^{(2)}(\\tau)$. Saturation behaviour is analysed by\nvarying laser power and recording the corresponding fluorescence, while\npolarization dependence is studied using a half-wave ($\\lambda/2$) plate.\nTemporal laser pulse profile is examined by modulating the power of an\nacousto-optic modulator. After optimizing all relevant parameters, we\ndemonstrate the microscope's capabilities in driving spin transitions of a\nsingle NV centre. This work establishes a straightforward and effective\nprotocol for laser excitation optimization, enhancing the performance and\nreliability of NV-based quantum sensors.", "AI": {"tldr": "This paper presents a method to optimize laser irradiation in confocal microscopy for NV-center quantum sensing, detailing the characterization of optical parameters and demonstrating improved sensor performance.", "motivation": "Precise control and understanding of optical parameters are essential for reliable single-emitter studies in quantum sensing experiments using nitrogen-vacancy (NV) centers, despite the suitability of confocal microscopy for such applications.", "method": "The study involves characterizing and optimizing laser irradiation within a confocal microscope for quantum sensing experiments using nitrogen-vacancy (NV) centers. Key investigations include laser beam intensity profile, single-photon emission statistics, fluorescence response under varying polarization and saturation conditions, spectral characteristics, and temporal profiles of readout and reinitialization pulses. Specific techniques used are the razorblade method for beam quality assessment ($M^2$), optical fluorescence spectrum recording, second-order autocorrelation function $g^{(2)}(\tau)$ measurement for single-emitter confirmation, analysis of saturation behavior by varying laser power, polarization dependence study using a half-wave plate, and examination of temporal laser pulse profiles using an acousto-optic modulator.", "result": "The research optimized various optical parameters, including beam intensity profile, emission statistics, fluorescence response, spectral characteristics, and temporal pulse profiles, demonstrating the microscope's capabilities in driving spin transitions of a single NV center.", "conclusion": "The paper establishes a straightforward and effective protocol for laser excitation optimization, enhancing the performance and reliability of NV-based quantum sensors."}}
{"id": "2507.09404", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09404", "abs": "https://arxiv.org/abs/2507.09404", "authors": ["Mustafa Shukor", "Louis Bethune", "Dan Busbridge", "David Grangier", "Enrico Fini", "Alaaeldin El-Nouby", "Pierre Ablin"], "title": "Scaling Laws for Optimal Data Mixtures", "comment": null, "summary": "Large foundation models are typically trained on data from multiple domains,\nwith the data mixture--the proportion of each domain used--playing a critical\nrole in model performance. The standard approach to selecting this mixture\nrelies on trial and error, which becomes impractical for large-scale\npretraining. We propose a systematic method to determine the optimal data\nmixture for any target domain using scaling laws. Our approach accurately\npredicts the loss of a model of size $N$ trained with $D$ tokens and a specific\ndomain weight vector $h$. We validate the universality of these scaling laws by\ndemonstrating their predictive power in three distinct and large-scale\nsettings: large language model (LLM), native multimodal model (NMM), and large\nvision models (LVM) pretraining. We further show that these scaling laws can\nextrapolate to new data mixtures and across scales: their parameters can be\naccurately estimated using a few small-scale training runs, and used to\nestimate the performance at larger scales and unseen domain weights. The\nscaling laws allow to derive the optimal domain weights for any target domain\nunder a given training budget ($N$,$D$), providing a principled alternative to\ncostly trial-and-error methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09556", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09556", "abs": "https://arxiv.org/abs/2507.09556", "authors": ["Ximeng Zhai", "Bohan Xu", "Yaohong Chen", "Hao Wang", "Kehua Guo", "Yimian Dai"], "title": "SeqCSIST: Sequential Closely-Spaced Infrared Small Target Unmixing", "comment": "Accepted by TGRS", "summary": "Due to the limitation of the optical lens focal length and the resolution of\nthe infrared detector, distant Closely-Spaced Infrared Small Target (CSIST)\ngroups typically appear as mixing spots in the infrared image. In this paper,\nwe propose a novel task, Sequential CSIST Unmixing, namely detecting all\ntargets in the form of sub-pixel localization from a highly dense CSIST group.\nHowever, achieving such precise detection is an extremely difficult challenge.\nIn addition, the lack of high-quality public datasets has also restricted the\nresearch progress. To this end, firstly, we contribute an open-source\necosystem, including SeqCSIST, a sequential benchmark dataset, and a toolkit\nthat provides objective evaluation metrics for this special task, along with\nthe implementation of 23 relevant methods. Furthermore, we propose the\nDeformable Refinement Network (DeRefNet), a model-driven deep learning\nframework that introduces a Temporal Deformable Feature Alignment (TDFA) module\nenabling adaptive inter-frame information aggregation. To the best of our\nknowledge, this work is the first endeavor to address the CSIST Unmixing task\nwithin a multi-frame paradigm. Experiments on the SeqCSIST dataset demonstrate\nthat our method outperforms the state-of-the-art approaches with mean Average\nPrecision (mAP) metric improved by 5.3\\%. Our dataset and toolkit are available\nfrom https://github.com/GrokCV/SeqCSIST.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u5e8f\u5217CSIST\u89e3\u6df7\u4efb\u52a1\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aDeRefNet\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u65f6\u95f4\u53ef\u53d8\u5f62\u7279\u5f81\u5bf9\u9f50\u6765\u63d0\u9ad8\u5bf9\u5bc6\u96c6\u7ea2\u5916\u5c0f\u76ee\u6807\u7684\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u7531\u4e8e\u5149\u5b66\u900f\u955c\u7126\u8ddd\u548c\u7ea2\u5916\u63a2\u6d4b\u5668\u5206\u8fa8\u7387\u7684\u9650\u5236\uff0c\u9065\u8fdc\u7684\u7d27\u5bc6\u95f4\u9694\u7ea2\u5916\u5c0f\u76ee\u6807\uff08CSIST\uff09\u7ec4\u901a\u5e38\u5728\u7ea2\u5916\u56fe\u50cf\u4e2d\u8868\u73b0\u4e3a\u6df7\u5408\u6591\u70b9\uff0c\u96be\u4ee5\u68c0\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53ef\u53d8\u5f62\u7cbe\u70bc\u7f51\u7edc\uff08DeRefNet\uff09\u7684\u6a21\u578b\u9a71\u52a8\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u5176\u4e2d\u5305\u542b\u65f6\u95f4\u53ef\u53d8\u5f62\u7279\u5f81\u5bf9\u9f50\uff08TDFA\uff09\u6a21\u5757\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u5730\u805a\u5408\u5e27\u95f4\u4fe1\u606f\u3002", "result": "\u5728SeqCSIST\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DeRefNet\u65b9\u6cd5\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u8868\u73b0\u66f4\u4f18\uff0c\u5e73\u5747\u7cbe\u5ea6\u5747\u503c\uff08mAP\uff09\u63d0\u9ad8\u4e865.3%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5e8f\u5217CSIST\u89e3\u6df7\u4efb\u52a1\uff0c\u65e8\u5728\u4ece\u9ad8\u5ea6\u5bc6\u96c6\u7684CSIST\u7ec4\u4e2d\u4ee5\u4e9a\u50cf\u7d20\u672c\u5730\u5316\u7684\u5f62\u5f0f\u68c0\u6d4b\u6240\u6709\u76ee\u6807\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u53ef\u53d8\u5f62\u7279\u5f81\u5bf9\u9f50\uff08TDFA\uff09\u6a21\u5757\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u7684\u5e27\u95f4\u4fe1\u606f\u805a\u5408\uff0c\u5e76\u5728SeqCSIST\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6bd4\u6700\u5148\u8fdb\u65b9\u6cd5\u63d0\u9ad85.3%\u7684mAP\u6027\u80fd\u3002"}}
{"id": "2507.10395", "categories": ["quant-ph", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2507.10395", "abs": "https://arxiv.org/abs/2507.10395", "authors": ["Ching-Yi Lai", "Pei-Hao Liou", "Yingkai Ouyang"], "title": "Fault-Tolerant Quantum Error Correction for Constant-Excitation Stabilizer Codes under Coherent Noise", "comment": "15 pages, 6 figures", "summary": "Collective coherent noise poses challenges for fault-tolerant quantum error\ncorrection (FTQEC), as it falls outside the usual stochastic noise models.\nWhile constant excitation (CE) codes can naturally avoid coherent noise, a\ncomplete fault-tolerant framework for the use of these codes under realistic\nnoise models has been elusive. Here, we introduce a complete fault-tolerant\narchitecture for CE CSS codes based on dual-rail concatenation. After showing\nthat transversal CNOT gates violate CE code constraints, we introduce\nCE-preserving logical CNOT gates and modified Shor- and Steane-type syndrome\nextraction schemes using zero-controlled NOT gates and CE-compatible ancilla.\nThis enables fault-tolerant syndrome-extraction circuits fully compatible with\nCE constraints. We also present an extended stabilizer simulation algorithm\nthat efficiently tracks both stochastic and collective coherent noise. Using\nour framework, we identify minimal CE codes, including the $[[12,1,3]]$ and\n$[[14,3,3]]$ codes, and demonstrate that the $[[12,1,3]]$ code achieves strong\nperformance under coherent noise. Our results establish the first complete\nFTQEC framework for CE codes, demonstrating their robustness to coherent noise.\nThis highlights the potential of CE codes as a possible solution for quantum\nprocessors dominated by collective coherent noise.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2aCE\u7801\u7684\u5b8c\u6574FTQEC\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u76f8\u5e72\u566a\u58f0\u95ee\u9898\uff0c\u5e76\u5c55\u793a\u4e86CE\u7801\u5728\u76f8\u5e72\u566a\u58f0\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u4e86\u96c6\u4f53\u76f8\u5e72\u566a\u58f0\u5bf9\u5bb9\u9519\u91cf\u5b50\u7ea0\u9519\uff08FTQEC\uff09\u7684\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86CE\u7801\u5728\u73b0\u5b9e\u566a\u58f0\u6a21\u578b\u4e0b\u7684\u5b8c\u6574\u5bb9\u9519\u6846\u67b6\u3002", "method": "\u901a\u8fc7\u53cc\u8f68\u4e32\u8054\u5f15\u5165\u4e86CE CSS\u7801\u7684\u5b8c\u6574\u5bb9\u9519\u67b6\u6784\uff0c\u5305\u62ecCE\u4fdd\u6301\u903b\u8f91CNOT\u95e8\u4ee5\u53ca\u4f7f\u7528\u96f6\u63a7\u5236\u975e\u95e8\u548cCE\u517c\u5bb9\u8f85\u52a9\u7684\u4fee\u6539\u540e\u7684Shor\u548cSteane\u7c7b\u578b\u7efc\u5408\u63d0\u53d6\u65b9\u6848\u3002\u8fd8\u63d0\u51fa\u4e86\u6269\u5c55\u7684\u7a33\u5b9a\u5668\u6a21\u62df\u7b97\u6cd5\u6765\u8ddf\u8e2a\u968f\u673a\u548c\u96c6\u4f53\u76f8\u5e72\u566a\u58f0\u3002", "result": "\u8bc6\u522b\u4e86\u6700\u5c0f\u7684CE\u7801\uff0c\u4f8b\u5982[[12,1,3]]\u548c[[14,3,3]]\u7801\uff0c\u5e76\u8bc1\u660e\u4e86[[12,1,3]]\u7801\u5728\u76f8\u5e72\u566a\u58f0\u4e0b\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "CE\u7801\u662f\u91cf\u5b50\u7ea0\u9519\u7684\u53ef\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5c24\u5176\u9002\u7528\u4e8e\u53d7\u96c6\u4f53\u76f8\u5e72\u566a\u58f0\u5f71\u54cd\u7684\u91cf\u5b50\u5904\u7406\u5668\u3002"}}
{"id": "2507.09406", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09406", "abs": "https://arxiv.org/abs/2507.09406", "authors": ["Santhosh Kumar Ravindran"], "title": "Adversarial Activation Patching: A Framework for Detecting and Mitigating Emergent Deception in Safety-Aligned Transformers", "comment": null, "summary": "Large language models (LLMs) aligned for safety through techniques like\nreinforcement learning from human feedback (RLHF) often exhibit emergent\ndeceptive behaviors, where outputs appear compliant but subtly mislead or omit\ncritical information. This paper introduces adversarial activation patching, a\nnovel mechanistic interpretability framework that leverages activation patching\nas an adversarial tool to induce, detect, and mitigate such deception in\ntransformer-based models. By sourcing activations from \"deceptive\" prompts and\npatching them into safe forward passes at specific layers, we simulate\nvulnerabilities and quantify deception rates. Through toy neural network\nsimulations across multiple scenarios (e.g., 1000 trials per setup), we\ndemonstrate that adversarial patching increases deceptive outputs to 23.9% from\na 0% baseline, with layer-specific variations supporting our hypotheses. We\npropose six hypotheses, including transferability across models, exacerbation\nin multimodal settings, and scaling effects. An expanded literature review\nsynthesizes over 20 key works in interpretability, deception, and adversarial\nattacks. Mitigation strategies, such as activation anomaly detection and robust\nfine-tuning, are detailed, alongside ethical considerations and future research\ndirections. This work advances AI safety by highlighting patching's dual-use\npotential and provides a roadmap for empirical studies on large-scale models.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5bf9\u6297\u6027\u6fc0\u6d3b\u4fee\u590d\u201d\u7684\u65b0\u578b\u673a\u5236\u53ef\u89e3\u91ca\u6027\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6fc0\u6d3b\u4fee\u590d\u6280\u672f\u4f5c\u4e3a\u5bf9\u6297\u6027\u5de5\u5177\u6765\u8bf1\u5bfc\u3001\u68c0\u6d4b\u548c\u51cf\u8f7b\u53d8\u6362\u5668\u6a21\u578b\u4e2d\u7684\u6b3a\u9a97\u884c\u4e3a\u3002\u901a\u8fc7\u4ece\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u4e2d\u63d0\u53d6\u6fc0\u6d3b\u5e76\u5728\u7279\u5b9a\u5c42\u5c06\u5176\u4fee\u590d\u5230\u5b89\u5168\u7684\u6b63\u5411\u4f20\u64ad\u4e2d\uff0c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u6a21\u62df\u6f0f\u6d1e\u5e76\u91cf\u5316\u6b3a\u9a97\u7387\u3002\u5728\u73a9\u5177\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u4e2d\uff0c\u7814\u7a76\u53d1\u73b0\u6b64\u65b9\u6cd5\u80fd\u5c06\u6b3a\u9a97\u6027\u8f93\u51fa\u7684\u6bd4\u4f8b\u4ece0%\u63d0\u5347\u81f323.9%\uff0c\u5e76\u63ed\u793a\u4e86\u7279\u5b9a\u5c42\u5728\u5176\u4e2d\u8d77\u5230\u7684\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u5173\u4e8e\u6a21\u578b\u8fc1\u79fb\u6027\u3001\u591a\u6a21\u6001\u73af\u5883\u5f71\u54cd\u548c\u89c4\u6a21\u6548\u5e94\u7684\u5047\u8bbe\uff0c\u5e76\u63a2\u8ba8\u4e86\u6fc0\u6d3b\u5f02\u5e38\u68c0\u6d4b\u548c\u9c81\u68d2\u5fae\u8c03\u7b49\u7f13\u89e3\u7b56\u7565\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u5b89\u5168\u9886\u57df\u7684\u7814\u7a76\u548c\u5b9e\u8df5\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u7ecf\u8fc7\u4eba\u7c7b\u53cd\u9988\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u5bf9\u9f50\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e2d\u51fa\u73b0\u7684\u6f5c\u5728\u6b3a\u9a97\u884c\u4e3a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6fc0\u6d3b\u4fee\u590d\u6280\u672f\uff0c\u5c06\u6765\u81ea\u201c\u6b3a\u9a97\u6027\u201d\u63d0\u793a\u7684\u6fc0\u6d3b\u6620\u5c04\u5230\u5b89\u5168\u7684\u6b63\u5411\u4f20\u64ad\u8fc7\u7a0b\u4e2d\uff0c\u4ee5\u6a21\u62df\u548c\u91cf\u5316\u6a21\u578b\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "result": "\u5728\u73a9\u5177\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u4e2d\uff0c\u5bf9\u6297\u6027\u4fee\u590d\u5c06\u6b3a\u9a97\u6027\u8f93\u51fa\u7684\u6bd4\u4f8b\u4ece0%\u63d0\u9ad8\u523023.9%\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7279\u5b9a\u5c42\u53ef\u80fd\u5b58\u5728\u7684\u6b3a\u9a97\u884c\u4e3a\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u5bf9\u6297\u6027\u6fc0\u6d3b\u4fee\u590d\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u548c\u51cf\u8f7b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6b3a\u9a97\u884c\u4e3a\uff0c\u5e76\u4e3a\u76f8\u5173\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.09560", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09560", "abs": "https://arxiv.org/abs/2507.09560", "authors": ["Bolun Zheng", "Xinjie Liu", "Qianyu Zhang", "Canjin Wang", "Fangni Chen", "Mingen Xu"], "title": "EHPE: A Segmented Architecture for Enhanced Hand Pose Estimation", "comment": null, "summary": "3D hand pose estimation has garnered great attention in recent years due to\nits critical applications in human-computer interaction, virtual reality, and\nrelated fields. The accurate estimation of hand joints is essential for\nhigh-quality hand pose estimation. However, existing methods neglect the\nimportance of Distal Phalanx Tip (TIP) and Wrist in predicting hand joints\noverall and often fail to account for the phenomenon of error accumulation for\ndistal joints in gesture estimation, which can cause certain joints to incur\nlarger errors, resulting in misalignments and artifacts in the pose estimation\nand degrading the overall reconstruction quality. To address this challenge, we\npropose a novel segmented architecture for enhanced hand pose estimation\n(EHPE). We perform local extraction of TIP and wrist, thus alleviating the\neffect of error accumulation on TIP prediction and further reduce the\npredictive errors for all joints on this basis. EHPE consists of two key\nstages: In the TIP and Wrist Joints Extraction stage (TW-stage), the positions\nof the TIP and wrist joints are estimated to provide an initial accurate joint\nconfiguration; In the Prior Guided Joints Estimation stage (PG-stage), a\ndual-branch interaction network is employed to refine the positions of the\nremaining joints. Extensive experiments on two widely used benchmarks\ndemonstrate that EHPE achieves state-of-the-arts performance. Code is available\nat https://github.com/SereinNout/EHPE.", "AI": {"tldr": "EHPE\u901a\u8fc7\u5728TW\u9636\u6bb5\u8fdb\u884c\u5c40\u90e8\u63d0\u53d6TIP\u548c\u624b\u8155\uff0c\u51cf\u8f7b\u4e86\u8bef\u5dee\u7d2f\u79ef\u5bf9TIP\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u4e00\u6b65\u51cf\u5c0f\u4e86\u6240\u6709\u5173\u8282\u7684\u9884\u6d4b\u8bef\u5dee\u3002\u5728PG\u9636\u6bb5\uff0c\u8be5\u6a21\u578b\u91c7\u7528\u53cc\u5206\u652f\u4ea4\u4e92\u7f51\u7edc\u6765\u4f18\u5316\u5269\u4f59\u5173\u8282\u7684\u4f4d\u7f6e\u3002EHPE\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u624b\u90e8\u59ff\u52bf\u4f30\u8ba1\u65b9\u6cd5\u5ffd\u7565\u4e86\u8fdc\u7aef\u6307\u5c16\uff08TIP\uff09\u548c\u624b\u8155\u5728\u9884\u6d4b\u6574\u4f53\u624b\u90e8\u5173\u8282\u65f6\u7684\u91cd\u8981\u6027\uff0c\u5e76\u4e14\u672a\u80fd\u89e3\u51b3\u8fdc\u7aef\u5173\u8282\u5728\u624b\u52bf\u4f30\u8ba1\u4e2d\u8bef\u5dee\u7d2f\u79ef\u7684\u73b0\u8c61\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u67d0\u4e9b\u5173\u8282\u4ea7\u751f\u66f4\u5927\u7684\u8bef\u5dee\uff0c\u4ece\u800c\u5728\u59ff\u52bf\u4f30\u8ba1\u4e2d\u4ea7\u751f\u9519\u4f4d\u548c\u4f2a\u5f71\uff0c\u5e76\u964d\u4f4e\u6574\u4f53\u91cd\u5efa\u8d28\u91cf\u3002EHPE\u65e8\u5728\u901a\u8fc7\u5c40\u90e8\u63d0\u53d6TIP\u548c\u624b\u8155\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4ece\u800c\u51cf\u8f7b\u8bef\u5dee\u7d2f\u79ef\u5bf9TIP\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u4e00\u6b65\u51cf\u5c0f\u6240\u6709\u5173\u8282\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5206\u5272\u67b6\u6784EHPE\uff0c\u5305\u62ec\u4e24\u4e2a\u5173\u952e\u9636\u6bb5\uff1aTIP\u548c\u8155\u90e8\u5173\u8282\u63d0\u53d6\u9636\u6bb5\uff08TW\u9636\u6bb5\uff09\u4ee5\u53ca\u5148\u9a8c\u5f15\u5bfc\u5173\u8282\u4f30\u8ba1\u9636\u6bb5\uff08PG\u9636\u6bb5\uff09\u3002TW\u9636\u6bb5\u4f30\u8ba1TIP\u548c\u8155\u90e8\u5173\u8282\u7684\u4f4d\u7f6e\uff0c\u4e3a\u521d\u59cb\u7684\u51c6\u786e\u5173\u8282\u914d\u7f6e\u63d0\u4f9b\u4f9d\u636e\uff1bPG\u9636\u6bb5\u91c7\u7528\u53cc\u5206\u652f\u4ea4\u4e92\u7f51\u7edc\u6765\u4f18\u5316\u5269\u4f59\u5173\u8282\u7684\u4f4d\u7f6e\u3002", "result": "EHPE\u901a\u8fc7\u5c40\u90e8\u63d0\u53d6TIP\u548c\u624b\u8155\uff0c\u51cf\u8f7b\u4e86\u8bef\u5dee\u7d2f\u79ef\u5bf9TIP\u9884\u6d4b\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u4e00\u6b65\u51cf\u5c0f\u4e86\u6240\u6709\u5173\u8282\u7684\u9884\u6d4b\u8bef\u5dee\u3002", "conclusion": "EHPE\u5728\u4e24\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10418", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10418", "abs": "https://arxiv.org/abs/2507.10418", "authors": ["C. Huerta Alderete", "Anubhav Kumar Srivastava", "Andrew T. Sornborger"], "title": "Nonlinear Quantum Sensing with a Frustrated Kitaev Trimer", "comment": "10 pages, 6 figures", "summary": "We investigate the response of a Ramsey interferometric quantum sensor based\non a frustrated, three-spin system (a Kitaev trimer) to a classical\ntime-dependent field (signal). The system eigenspectrum is symmetric about a\ncritical point, $|\\vec{b}| = 0$, with four of the spectral components varying\napproximately linearly with the magnetic field and four exhibiting a nonlinear\ndependence. Under the adiabatic approximation and for appropriate initial\nstates, we show that the sensor's response to a zero-mean signal is such that\nbelow a threshold, $|\\vec{b}| < b_\\mathrm{th}$, the sensor does not respond to\nthe signal, whereas above the threshold, the sensor acts as a detector that the\nsignal has occurred. This thresholded response is approximately\nomnidirectional. Moreover, when deployed in an entangled multisensor\nconfiguration, the sensor achieves sensitivity at the Heisenberg limit. Such\ndetectors could be useful both as standalone units for signal detection above a\nnoise threshold and in two- or three-dimensional arrays, analogous to a quantum\nbubble chamber, for applications such as particle track detection and\nlong-baseline telescopy.", "AI": {"tldr": "\u57fa\u4e8e\u53d7\u963b\u4e09\u81ea\u65cb\u7cfb\u7edf\u7684\u91cf\u5b50\u4f20\u611f\u5668\u5bf9\u4fe1\u53f7\u6709\u9608\u503c\u54cd\u5e94\uff0c\u4e14\u5728\u7ea0\u7f20\u591a\u4f20\u611f\u5668\u914d\u7f6e\u4e2d\u5177\u6709\u6d77\u68ee\u6781\u9650\u7075\u654f\u5ea6\uff0c\u53ef\u7528\u4e8e\u4fe1\u53f7\u68c0\u6d4b\u548c\u7c92\u5b50\u5f84\u8ff9\u63a2\u6d4b\u7b49\u3002", "motivation": "\u7814\u7a76\u53d7\u963b\u4e09\u81ea\u65cb\u7cfb\u7edf\u5728\u7ecf\u5178\u65f6\u95f4\u76f8\u5173\u573a\u4e0b\u7684\u54cd\u5e94\uff0c\u4ee5\u63a2\u7d22\u5176\u4f5c\u4e3a\u91cf\u5b50\u4f20\u611f\u5668\u7684\u6f5c\u529b\u3002", "method": "\u7814\u7a76\u4e86\u57fa\u4e8e\u53d7\u963b\u4e09\u81ea\u65cb\u7cfb\u7edf\uff08Kitaev\u4e09\u805a\u4f53\uff09\u7684\u62c9\u59c6\u9f50\u5e72\u6d89\u91cf\u5b50\u4f20\u611f\u5668\u7684\u54cd\u5e94\uff0c\u8be5\u7cfb\u7edf\u5bf9\u7ecf\u5178\u65f6\u95f4\u76f8\u5173\u573a\uff08\u4fe1\u53f7\uff09\u654f\u611f\u3002\u5728\u7edd\u70ed\u8fd1\u4f3c\u548c\u9002\u5f53\u7684\u521d\u59cb\u72b6\u6001\u4e0b\uff0c\u7814\u7a76\u4e86\u4fe1\u53f7\u5bf9\u4f20\u611f\u5668\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5b58\u5728\u4e00\u4e2a\u9608\u503c\u3002", "result": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0c\u8be5\u4f20\u611f\u5668\u5bf9\u96f6\u5747\u503c\u4fe1\u53f7\u8868\u73b0\u51fa\u9608\u503c\u54cd\u5e94\u884c\u4e3a\uff0c\u4f4e\u4e8e\u67d0\u4e00\u9608\u503c\u65f6\u4e0d\u54cd\u5e94\uff0c\u9ad8\u4e8e\u9608\u503c\u65f6\u5219\u4f5c\u4e3a\u68c0\u6d4b\u5668\u3002\u6b64\u5916\uff0c\u5728\u7ea0\u7f20\u591a\u4f20\u611f\u5668\u914d\u7f6e\u4e2d\uff0c\u8be5\u4f20\u611f\u5668\u53ef\u4ee5\u8fbe\u5230\u6d77\u68ee\u6781\u9650\u7684\u7075\u654f\u5ea6\u3002", "conclusion": "\u8be5\u4f20\u611f\u5668\u53ef\u4f5c\u4e3a\u72ec\u7acb\u5355\u5143\u7528\u4e8e\u4fe1\u53f7\u68c0\u6d4b\uff0c\u4e5f\u53ef\u7528\u4e8e\u6784\u5efa\u4e8c\u7ef4\u6216\u4e09\u7ef4\u9635\u5217\uff0c\u5e94\u7528\u4e8e\u7c92\u5b50\u5f84\u8ff9\u63a2\u6d4b\u548c\u957f\u57fa\u7ebf\u5149\u5b66\u5e72\u6d89\u6d4b\u91cf\u7b49\u9886\u57df\u3002"}}
{"id": "2507.09428", "categories": ["cs.LG", "math.DG", "math.OC"], "pdf": "https://arxiv.org/pdf/2507.09428", "abs": "https://arxiv.org/abs/2507.09428", "authors": ["Zakhar Shumaylov", "Vasileios Tsiaras", "Yannis Stylianou"], "title": "On Information Geometry and Iterative Optimization in Model Compression: Operator Factorization", "comment": null, "summary": "The ever-increasing parameter counts of deep learning models necessitate\neffective compression techniques for deployment on resource-constrained\ndevices. This paper explores the application of information geometry, the study\nof density-induced metrics on parameter spaces, to analyze existing methods\nwithin the space of model compression, primarily focusing on operator\nfactorization. Adopting this perspective highlights the core challenge:\ndefining an optimal low-compute submanifold (or subset) and projecting onto it.\nWe argue that many successful model compression approaches can be understood as\nimplicitly approximating information divergences for this projection. We\nhighlight that when compressing a pre-trained model, using information\ndivergences is paramount for achieving improved zero-shot accuracy, yet this\nmay no longer be the case when the model is fine-tuned. In such scenarios,\ntrainability of bottlenecked models turns out to be far more important for\nachieving high compression ratios with minimal performance degradation,\nnecessitating adoption of iterative methods. In this context, we prove\nconvergence of iterative singular value thresholding for training neural\nnetworks subject to a soft rank constraint. To further illustrate the utility\nof this perspective, we showcase how simple modifications to existing methods\nthrough softer rank reduction result in improved performance under fixed\ncompression rates.", "AI": {"tldr": "\u672c\u6587\u5229\u7528\u4fe1\u606f\u51e0\u4f55\u5206\u6790\u6a21\u578b\u538b\u7f29\uff0c\u63d0\u51fa\u5c06\u5176\u89c6\u4e3a\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u6700\u4f18\u4f4e\u8ba1\u7b97\u91cf\u5b50\u6d41\u5f62\u5e76\u8fdb\u884c\u6295\u5f71\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u538b\u7f29\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u4fe1\u606f\u6563\u5ea6\u6709\u52a9\u4e8e\u63d0\u9ad8\u96f6\u6837\u672c\u51c6\u786e\u7387\uff1b\u800c\u5728\u6a21\u578b\u5fae\u8c03\u65f6\uff0c\u53ef\u8bad\u7ec3\u6027\u6bd4\u4fe1\u606f\u6563\u5ea6\u66f4\u91cd\u8981\u3002\u901a\u8fc7\u8fed\u4ee3\u5947\u5f02\u503c\u9608\u503c\u6cd5\u548c\u8f6f\u79e9\u7ea6\u51cf\u7b49\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u8bc1\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u66f4\u9ad8\u7684\u538b\u7f29\u7387\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u65e5\u76ca\u589e\u957f\u7684\u53c2\u6570\u91cf\uff0c\u4f7f\u5f97\u5728\u8d44\u6e90\u53d7\u9650\u8bbe\u5907\u4e0a\u90e8\u7f72\u6a21\u578b\u9700\u8981\u6709\u6548\u7684\u538b\u7f29\u6280\u672f\u3002\u672c\u6587\u65e8\u5728\u5229\u7528\u4fe1\u606f\u51e0\u4f55\u7684\u89c6\u89d2\uff0c\u5206\u6790\u73b0\u6709\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u7b97\u5b50\u5206\u89e3\u3002", "method": "\u672c\u6587\u63a2\u7d22\u4e86\u4fe1\u606f\u51e0\u4f55\u5728\u6a21\u578b\u538b\u7f29\u4e2d\u7684\u5e94\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u7b97\u5b50\u5206\u89e3\u3002\u6211\u4eec\u63d0\u51fa\u5c06\u6a21\u578b\u538b\u7f29\u89c6\u4e3a\u5728\u53c2\u6570\u7a7a\u95f4\u4e2d\u5b9a\u4e49\u6700\u4f18\u4f4e\u8ba1\u7b97\u91cf\u5b50\u6d41\u5f62\u5e76\u8fdb\u884c\u6295\u5f71\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8bc1\u660e\u4e86\u8fed\u4ee3\u5947\u5f02\u503c\u9608\u503c\u6cd5\u5728\u8f6f\u79e9\u7ea6\u675f\u4e0b\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6536\u655b\u6027\u3002", "result": "\u4fe1\u606f\u51e0\u4f55\u7684\u89c6\u89d2\u63ed\u793a\u4e86\u6a21\u578b\u538b\u7f29\u7684\u6838\u5fc3\u6311\u6218\u5728\u4e8e\u5b9a\u4e49\u6700\u4f18\u4f4e\u8ba1\u7b97\u91cf\u5b50\u6d41\u5f62\u5e76\u8fdb\u884c\u6295\u5f71\u3002\u8bb8\u591a\u6210\u529f\u7684\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u53ef\u4ee5\u88ab\u7406\u89e3\u4e3a\u9690\u5f0f\u5730\u903c\u8fd1\u4fe1\u606f\u6563\u5ea6\u4ee5\u5b9e\u73b0\u8fd9\u79cd\u6295\u5f71\u3002\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u538b\u7f29\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u4fe1\u606f\u6563\u5ea6\u5bf9\u4e8e\u63d0\u9ad8\u96f6\u6837\u672c\u51c6\u786e\u7387\u81f3\u5173\u91cd\u8981\uff1b\u7136\u800c\uff0c\u5728\u6a21\u578b\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\uff0c\u53ef\u8bad\u7ec3\u6027\u53d8\u5f97\u66f4\u4e3a\u91cd\u8981\u3002", "conclusion": "\u5f53\u6a21\u578b\u5728\u538b\u7f29\u540e\u8fdb\u884c\u5fae\u8c03\u65f6\uff0c\u4fe1\u606f\u6563\u5ea6\u5bf9\u4e8e\u63d0\u9ad8\u96f6\u6837\u672c\u51c6\u786e\u7387\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6b64\u65f6\u53ef\u8bad\u7ec3\u6027\u6bd4\u4fe1\u606f\u6563\u5ea6\u66f4\u91cd\u8981\uff0c\u9700\u8981\u91c7\u7528\u8fed\u4ee3\u65b9\u6cd5\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u8fed\u4ee3\u5947\u5f02\u503c\u9608\u503c\u6cd5\u5728\u8f6f\u79e9\u7ea6\u675f\u4e0b\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u6536\u655b\u6027\u3002\u901a\u8fc7\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u7b80\u5355\u7684\u4fee\u6539\uff0c\u4f8b\u5982\u66f4\u6e29\u548c\u7684\u79e9\u7ea6\u51cf\uff0c\u53ef\u4ee5\u5728\u56fa\u5b9a\u7684\u538b\u7f29\u7387\u4e0b\u83b7\u5f97\u66f4\u597d\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09562", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09562", "abs": "https://arxiv.org/abs/2507.09562", "authors": ["Yidong Jiang"], "title": "Prompt Engineering in Segment Anything Model: Methodologies, Applications, and Emerging Challenges", "comment": null, "summary": "The Segment Anything Model (SAM) has revolutionized image segmentation\nthrough its innovative prompt-based approach, yet the critical role of prompt\nengineering in its success remains underexplored. This paper presents the first\ncomprehensive survey focusing specifically on prompt engineering techniques for\nSAM and its variants. We systematically organize and analyze the rapidly\ngrowing body of work in this emerging field, covering fundamental\nmethodologies, practical applications, and key challenges. Our review reveals\nhow prompt engineering has evolved from simple geometric inputs to\nsophisticated multimodal approaches, enabling SAM's adaptation across diverse\ndomains including medical imaging and remote sensing. We identify unique\nchallenges in prompt optimization and discuss promising research directions.\nThis survey fills an important gap in the literature by providing a structured\nframework for understanding and advancing prompt engineering in foundation\nmodels for segmentation.", "AI": {"tldr": "\u8fd9\u9879\u8c03\u67e5\u9996\u6b21\u5168\u9762\u6982\u8ff0\u4e86SAM\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5176\u53d1\u5c55\u3001\u5e94\u7528\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "SAM\u901a\u8fc7\u5176\u521b\u65b0\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u65b9\u6cd5\u5f7b\u5e95\u6539\u53d8\u4e86\u56fe\u50cf\u5206\u5272\uff0c\u4f46\u63d0\u793a\u5de5\u7a0b\u5728\u5176\u6210\u529f\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u4ecd\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5bf9SAM\u53ca\u5176\u53d8\u4f53\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8c03\u67e5\uff0c\u7cfb\u7edf\u5730\u7ec4\u7ec7\u548c\u5206\u6790\u4e86\u8be5\u65b0\u5174\u9886\u57df\u7684\u5feb\u901f\u589e\u957f\u7684\u5de5\u4f5c\uff0c\u6db5\u76d6\u4e86\u57fa\u672c\u65b9\u6cd5\u3001\u5b9e\u9645\u5e94\u7528\u548c\u5173\u952e\u6311\u6218\u3002", "result": "\u8c03\u67e5\u663e\u793a\uff0c\u63d0\u793a\u5de5\u7a0b\u5982\u4f55\u4ece\u7b80\u5355\u7684\u51e0\u4f55\u8f93\u5165\u6f14\u53d8\u4e3a\u590d\u6742\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u4ece\u800c\u80fd\u591f\u5c06SAM\u9002\u5e94\u4e8e\u5305\u62ec\u533b\u5b66\u6210\u50cf\u548c\u9065\u611f\u5728\u5185\u7684\u4e0d\u540c\u9886\u57df\u3002\u786e\u5b9a\u4e86\u63d0\u793a\u4f18\u5316\u4e2d\u7684\u72ec\u7279\u6311\u6218\uff0c\u5e76\u8ba8\u8bba\u4e86\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8fd9\u9879\u8c03\u67e5\u586b\u8865\u4e86\u6587\u732e\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u7a7a\u767d\uff0c\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u6846\u67b6\u6765\u7406\u89e3\u548c\u63a8\u8fdb\u5206\u5272\u57fa\u7840\u6a21\u578b\u4e2d\u7684\u63d0\u793a\u5de5\u7a0b\u3002"}}
{"id": "2507.10501", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2507.10501", "abs": "https://arxiv.org/abs/2507.10501", "authors": ["Javier Lopez-Cerezo"], "title": "A Rigorous Introduction to Hamiltonian Simulation via High-Order Product Formulas", "comment": null, "summary": "This work provides a rigorous and self-contained introduction to numerical\nmethods for Hamiltonian simulation in quantum computing, with a focus on\nhigh-order product formulas for efficiently approximating the time evolution of\nquantum systems. Aimed at students and researchers seeking a clear mathematical\ntreatment, the study begins with the foundational principles of quantum\nmechanics and quantum computation before presenting the Lie-Trotter product\nformula and its higher-order generalizations. In particular, Suzuki's recursive\nmethod is explored to achieve improved error scaling. Through theoretical\nanalysis and illustrative examples, the advantages and limitations of these\ntechniques are discussed, with an emphasis on their application to $k$-local\nHamiltonians and their role in overcoming classical computational bottlenecks.\nThe work concludes with a brief overview of current advances and open\nchallenges in Hamiltonian simulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u4e3a\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u6c49\u5bc6\u5c14\u987f\u6a21\u62df\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6e05\u6670\u7684\u6570\u5b66\u4ecb\u7ecd\uff0c\u91cd\u70b9\u662f\u9ad8\u9636\u4e58\u79ef\u516c\u5f0f\uff0c\u5982 Suzuki \u7684\u9012\u5f52\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u5728\u514b\u670d\u7ecf\u5178\u8ba1\u7b97\u74f6\u9888\u65b9\u9762\u7684\u5e94\u7528\u3002", "motivation": "\u8be5\u5de5\u4f5c\u65e8\u5728\u4e3a\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7684\u6c49\u5bc6\u5c14\u987f\u6a21\u62df\u63d0\u4f9b\u4e25\u683c\u4e14\u72ec\u7acb\u7684\u4ecb\u7ecd\uff0c\u91cd\u70b9\u5173\u6ce8\u9ad8\u6548\u903c\u8fd1\u91cf\u5b50\u7cfb\u7edf\u65f6\u95f4\u6f14\u5316\u7684\u9ad8\u9636\u4e58\u79ef\u516c\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86 Lie-Trotter \u4e58\u79ef\u516c\u5f0f\u53ca\u5176\u9ad8\u9636\u63a8\u5e7f\uff0c\u7279\u522b\u662f Suzuki \u7684\u9012\u5f52\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u6539\u8fdb\u7684\u8bef\u5dee\u7f29\u653e\u3002", "result": "\u7406\u8bba\u5206\u6790\u548c\u793a\u4f8b\u8868\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u5728\u9ad8\u9ad8\u6570\u6c49\u5bc6\u5c14\u987f\u548c\u514b\u670d\u7ecf\u5178\u8ba1\u7b97\u74f6\u9888\u65b9\u9762\u5177\u6709\u4f18\u52bf\u548c\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u603b\u7ed3\u4e86\u6c49\u5bc6\u5c14\u987f\u6a21\u62df\u7684\u6700\u65b0\u8fdb\u5c55\u548c\u5f00\u653e\u6311\u6218\u3002"}}
{"id": "2507.09439", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09439", "abs": "https://arxiv.org/abs/2507.09439", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "Dynamic Sparse Causal-Attention Temporal Networks for Interpretable Causality Discovery in Multivariate Time Series", "comment": null, "summary": "Understanding causal relationships in multivariate time series (MTS) is\nessential for effective decision-making in fields such as finance and\nmarketing, where complex dependencies and lagged effects challenge conventional\nanalytical approaches. We introduce Dynamic Sparse Causal-Attention Temporal\nNetworks for Interpretable Causality Discovery in MTS (DyCAST-Net), a novel\narchitecture designed to enhance causal discovery by integrating dilated\ntemporal convolutions and dynamic sparse attention mechanisms. DyCAST-Net\neffectively captures multiscale temporal dependencies through dilated\nconvolutions while leveraging an adaptive thresholding strategy in its\nattention mechanism to eliminate spurious connections, ensuring both accuracy\nand interpretability. A statistical shuffle test validation further strengthens\nrobustness by filtering false positives and improving causal inference\nreliability. Extensive evaluations on financial and marketing datasets\ndemonstrate that DyCAST-Net consistently outperforms existing models such as\nTCDF, GCFormer, and CausalFormer. The model provides a more precise estimation\nof causal delays and significantly reduces false discoveries, particularly in\nnoisy environments. Moreover, attention heatmaps offer interpretable insights,\nuncovering hidden causal patterns such as the mediated effects of advertising\non consumer behavior and the influence of macroeconomic indicators on financial\nmarkets. Case studies illustrate DyCAST-Net's ability to detect latent\nmediators and lagged causal factors, making it particularly effective in\nhigh-dimensional, dynamic settings. The model's architecture enhanced by\nRMSNorm stabilization and causal masking ensures scalability and adaptability\nacross diverse application domains", "AI": {"tldr": "DyCAST-Net\u662f\u4e00\u79cd\u65b0\u9896\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u7a00\u91ca\u65f6\u95f4\u5377\u79ef\u548c\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u6765\u53d1\u73b0MTS\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\uff0c\u5b83\u6bd4\u73b0\u6709\u6a21\u578b\u66f4\u51c6\u786e\u3001\u66f4\u5177\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u662f\u5728\u5608\u6742\u548c\u9ad8\u7ef4\u7684\u73af\u5883\u4e2d\u3002", "motivation": "\u5728\u91d1\u878d\u548c\u8425\u9500\u7b49\u9886\u57df\uff0c\u7406\u89e3\u591a\u5143\u65f6\u95f4\u5e8f\u5217\uff08MTS\uff09\u4e2d\u7684\u56e0\u679c\u5173\u7cfb\u5bf9\u4e8e\u6709\u6548\u7684\u51b3\u7b56\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u6ede\u540e\u6548\u5e94\u5bf9\u4f20\u7edf\u7684\u5206\u6790\u65b9\u6cd5\u63d0\u51fa\u4e86\u6311\u6218\u3002", "method": "DyCAST-Net\u901a\u8fc7\u96c6\u6210\u7a00\u91ca\u65f6\u95f4\u5377\u79ef\u548c\u52a8\u6001\u7a00\u758f\u6ce8\u610f\u529b\u673a\u5236\u6765\u589e\u5f3a\u56e0\u679c\u53d1\u73b0\uff0c\u901a\u8fc7\u7a00\u91ca\u5377\u79ef\u6709\u6548\u5730\u6355\u83b7\u591a\u5c3a\u5ea6\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u901a\u8fc7\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u81ea\u9002\u5e94\u9608\u503c\u7b56\u7565\u6d88\u9664\u865a\u5047\u8fde\u63a5\uff0c\u4ece\u800c\u786e\u4fdd\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u7edf\u8ba1\u968f\u673a\u6d4b\u8bd5\u9a8c\u8bc1\u901a\u8fc7\u8fc7\u6ee4\u5047\u9633\u6027\u548c\u63d0\u9ad8\u56e0\u679c\u63a8\u65ad\u7684\u53ef\u9760\u6027\uff0c\u8fdb\u4e00\u6b65\u52a0\u5f3a\u4e86\u9c81\u68d2\u6027\u3002RMSNorm\u7a33\u5b9a\u548c\u56e0\u679c\u63a9\u7801\u589e\u5f3a\u4e86\u6a21\u578b\u67b6\u6784\uff0c\u786e\u4fdd\u4e86\u8de8\u4e0d\u540c\u5e94\u7528\u57df\u7684\u53ef\u6269\u5c55\u6027\u548c\u9002\u5e94\u6027\u3002", "result": "DyCAST-Net\u5728\u91d1\u878d\u548c\u8425\u9500\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u4f30\u8ba1\u56e0\u679c\u5ef6\u8fdf\u548c\u51cf\u5c11\u9519\u8bef\u53d1\u73b0\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002\u6ce8\u610f\u529b\u70ed\u56fe\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u9690\u85cf\u7684\u56e0\u679c\u6a21\u5f0f\u3002", "conclusion": "DyCAST-Net\u5728\u91d1\u878d\u548c\u8425\u9500\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u8bc4\u4f30\u8868\u660e\uff0c\u5b83\u5728\u4f30\u8ba1\u56e0\u679c\u5ef6\u8fdf\u548c\u51cf\u5c11\u9519\u8bef\u53d1\u73b0\uff08\u5c24\u5176\u662f\u5728\u5608\u6742\u7684\u73af\u5883\u4e2d\uff09\u65b9\u9762\u6301\u7eed\u4f18\u4e8eTCDF\u3001GCFormer\u548cCausalFormer\u7b49\u73b0\u6709\u6a21\u578b\u3002\u5176\u6ce8\u610f\u529b\u70ed\u56fe\u63d0\u4f9b\u4e86\u53ef\u89e3\u91ca\u7684\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u9690\u85cf\u7684\u56e0\u679c\u6a21\u5f0f\uff0c\u4f8b\u5982\u5e7f\u544a\u5bf9\u6d88\u8d39\u8005\u884c\u4e3a\u7684\u4ecb\u5bfc\u6548\u5e94\u4ee5\u53ca\u5b8f\u89c2\u7ecf\u6d4e\u6307\u6807\u5bf9\u91d1\u878d\u5e02\u573a\u7684\u5f71\u54cd\u3002\u6848\u4f8b\u7814\u7a76\u8868\u660e\uff0cDyCAST-Net\u80fd\u591f\u68c0\u6d4b\u6f5c\u5728\u7684\u4e2d\u4ecb\u56e0\u7d20\u548c\u6ede\u540e\u56e0\u679c\u56e0\u7d20\uff0c\u5728\u9ad8\u7ef4\u3001\u52a8\u6001\u73af\u5883\u4e2d\u7279\u522b\u6709\u6548\u3002"}}
{"id": "2507.09573", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09573", "abs": "https://arxiv.org/abs/2507.09573", "authors": ["Zhe Wang", "Jingbo Zhang", "Tianyi Wei", "Wanchao Su", "Can Wang"], "title": "WordCraft: Interactive Artistic Typography with Attention Awareness and Noise Blending", "comment": "14 pages, 16 figures", "summary": "Artistic typography aims to stylize input characters with visual effects that\nare both creative and legible. Traditional approaches rely heavily on manual\ndesign, while recent generative models, particularly diffusion-based methods,\nhave enabled automated character stylization. However, existing solutions\nremain limited in interactivity, lacking support for localized edits, iterative\nrefinement, multi-character composition, and open-ended prompt interpretation.\nWe introduce WordCraft, an interactive artistic typography system that\nintegrates diffusion models to address these limitations. WordCraft features a\ntraining-free regional attention mechanism for precise, multi-region generation\nand a noise blending that supports continuous refinement without compromising\nvisual quality. To support flexible, intent-driven generation, we incorporate a\nlarge language model to parse and structure both concrete and abstract user\nprompts. These components allow our framework to synthesize high-quality,\nstylized typography across single- and multi-character inputs across multiple\nlanguages, supporting diverse user-centered workflows. Our system significantly\nenhances interactivity in artistic typography synthesis, opening up creative\npossibilities for artists and designers.", "AI": {"tldr": "WordCraft\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u827a\u672f\u5b57\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u7ed3\u5408\u6269\u6563\u6a21\u578b\u3001\u533a\u57df\u6ce8\u610f\u529b\u673a\u5236\u3001\u566a\u58f0\u6df7\u5408\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u5c40\u90e8\u7f16\u8f91\u3001\u8fed\u4ee3\u4f18\u5316\u548c\u63d0\u793a\u8bcd\u89e3\u91ca\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u827a\u672f\u5b57\u4f53\u8bbe\u8ba1\u7684\u4ea4\u4e92\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u52a8\u5316\u5b57\u4f53\u8bbe\u8ba1\u65b9\u6cd5\uff08\u7279\u522b\u662f\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\uff09\u5728\u4ea4\u4e92\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u4f8b\u5982\u7f3a\u4e4f\u5bf9\u5c40\u90e8\u7f16\u8f91\u3001\u8fed\u4ee3\u4f18\u5316\u3001\u591a\u5b57\u7b26\u7ec4\u5408\u548c\u5f00\u653e\u5f0f\u63d0\u793a\u8bcd\u89e3\u91ca\u7684\u652f\u6301\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u4f9b\u4e00\u4e2a\u66f4\u5177\u4ea4\u4e92\u6027\u7684\u827a\u672f\u5b57\u4f53\u8bbe\u8ba1\u7cfb\u7edf\u3002", "method": "WordCraft\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u6269\u6563\u6a21\u578b\u7684\u4ea4\u4e92\u5f0f\u827a\u672f\u5b57\u4f53\u7cfb\u7edf\uff0c\u901a\u8fc7\u65e0\u8bad\u7ec3\u7684\u533a\u57df\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u7cbe\u786e\u7684\u591a\u533a\u57df\u751f\u6210\uff0c\u5e76\u5229\u7528\u566a\u58f0\u6df7\u5408\u6280\u672f\u8fdb\u884c\u8fde\u7eed\u4f18\u5316\u800c\u4e0d\u5f71\u54cd\u89c6\u89c9\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u7ed3\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u89e3\u6790\u548c\u6784\u5efa\u7528\u6237\u63d0\u793a\uff0c\u652f\u6301\u5bf9\u5177\u4f53\u548c\u62bd\u8c61\u6307\u4ee4\u7684\u7075\u6d3b\u3001\u610f\u56fe\u9a71\u52a8\u7684\u751f\u6210\u3002", "result": "WordCraft\u80fd\u591f\u8de8\u591a\u79cd\u8bed\u8a00\u7684\u5355\u4e2a\u548c\u591a\u4e2a\u5b57\u7b26\u8f93\u5165\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u98ce\u683c\u5316\u7684\u5b57\u4f53\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u5de5\u4f5c\u6d41\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u663e\u8457\u589e\u5f3a\u4e86\u827a\u672f\u5b57\u4f53\u8bbe\u8ba1\u7684\u4ea4\u4e92\u6027\uff0c\u4e3a\u827a\u672f\u5bb6\u548c\u8bbe\u8ba1\u5e08\u5f00\u8f9f\u4e86\u65b0\u7684\u521b\u610f\u53ef\u80fd\u6027\u3002"}}
{"id": "2507.10519", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2507.10519", "abs": "https://arxiv.org/abs/2507.10519", "authors": ["Shival Dasu", "Simon Burton"], "title": "A Classification of Transversal Clifford Gates for Qubit Stabilizer Codes", "comment": "19 pages", "summary": "This work classifies stabilizer codes by the set of diagonal Clifford gates\nthat can be implemented transversally on them. We show that, for any stabilizer\ncode, its group of diagonal transversal Clifford gates on $\\ell$ code blocks\nmust be one of six distinct families of matrix groups. We further develop the\ntheory of classifying stabilizer codes by via matrix algebras of endomorphisms\nfirst introduced by Rains, and give a complete classification of the diagonal\nClifford symmetries of $\\ell$ code blocks. A number of corollaries are given in\nthe final section.", "AI": {"tldr": "\u5bf9\u7a33\u5b9a\u5668\u7801\u7684\u5bf9\u89d2\u514b\u5229\u798f\u5fb7\u95e8\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u786e\u5b9a\u4e86\u516d\u79cd\u53ef\u80fd\u7684\u95e8\u96c6\uff0c\u5e76\u5b8c\u6210\u4e86\u5bf9 $\\ell$ \u4e2a\u7801\u5757\u7684\u5206\u7c7b\u3002", "motivation": "\u5bf9\u7a33\u5b9a\u5668\u7801\u8fdb\u884c\u5206\u7c7b\uff0c\u4ee5\u7406\u89e3\u5176\u5bf9\u89d2\u514b\u5229\u798f\u5fb7\u95e8\u96c6\u7684\u7ed3\u6784\u548c\u5bf9\u79f0\u6027\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u7a33\u5b9a\u5668\u7801\u7684\u5bf9\u89d2\u514b\u5229\u798f\u5fb7\u95e8\u96c6\uff0c\u5e76\u5229\u7528 Rains \u9996\u6b21\u5f15\u5165\u7684\u5185\u5c04\u5f20\u91cf\u4ee3\u6570\u7406\u8bba\uff0c\u5bf9 $\\ell$ \u4e2a\u7801\u5757\u7684\u5bf9\u89d2\u514b\u5229\u798f\u5fb7\u5bf9\u79f0\u6027\u8fdb\u884c\u4e86\u5b8c\u6574\u5206\u7c7b\u3002", "result": "\u53d1\u73b0\u4efb\u4f55\u7a33\u5b9a\u5668\u7801\u7684\u5bf9\u89d2\u6a2a\u5411\u514b\u5229\u798f\u5fb7\u95e8\u7fa4\u5fc5\u5b9a\u5c5e\u4e8e\u516d\u4e2a\u4e0d\u540c\u7684\u77e9\u9635\u7fa4\u5bb6\u65cf\u4e4b\u4e00\uff0c\u5e76\u5b8c\u6210\u4e86\u5bf9 $\\ell$ \u4e2a\u7801\u5757\u7684\u5bf9\u89d2\u514b\u5229\u798f\u5fb7\u5bf9\u79f0\u6027\u7684\u5206\u7c7b\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5bf9\u7a33\u5b9a\u5668\u7801\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5177\u4f53\u662f\u6839\u636e\u5176\u4e0a\u53ef\u6a2a\u5411\u5b9e\u73b0\u7684\u5bf9\u89d2\u514b\u5229\u798f\u5fb7\u95e8\u96c6\u3002"}}
{"id": "2507.09440", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09440", "abs": "https://arxiv.org/abs/2507.09440", "authors": ["Joshua Hill", "Benjamin Eyre", "Elliot Creager"], "title": "Transformers Don't In-Context Learn Least Squares Regression", "comment": "21 pages, 16 figures, ICML 2025 Workshop on Reliable and Responsible\n  Foundation Models", "summary": "In-context learning (ICL) has emerged as a powerful capability of large\npretrained transformers, enabling them to solve new tasks implicit in example\ninput-output pairs without any gradient updates. Despite its practical success,\nthe mechanisms underlying ICL remain largely mysterious. In this work we study\nsynthetic linear regression to probe how transformers implement learning at\ninference time. Previous works have demonstrated that transformers match the\nperformance of learning rules such as Ordinary Least Squares (OLS) regression\nor gradient descent and have suggested ICL is facilitated in transformers\nthrough the learned implementation of one of these techniques. In this work, we\ndemonstrate through a suite of out-of-distribution generalization experiments\nthat transformers trained for ICL fail to generalize after shifts in the prompt\ndistribution, a behaviour that is inconsistent with the notion of transformers\nimplementing algorithms such as OLS. Finally, we highlight the role of the\npretraining corpus in shaping ICL behaviour through a spectral analysis of the\nlearned representations in the residual stream. Inputs from the same\ndistribution as the training data produce representations with a unique\nspectral signature: inputs from this distribution tend to have the same top two\nsingular vectors. This spectral signature is not shared by out-of-distribution\ninputs, and a metric characterizing the presence of this signature is highly\ncorrelated with low loss.", "AI": {"tldr": "ICL in transformers is mysterious. This paper shows transformers fail at ICL when prompts change, unlike OLS. Pretraining data matters: good prompts have a special 'spectral signature' linked to low error.", "motivation": "To understand the mysterious mechanisms underlying in-context learning (ICL) in large pretrained transformers, despite their practical success.", "method": "The study probes the mechanisms of ICL in transformers using synthetic linear regression and a suite of out-of-distribution generalization experiments. It also performs spectral analysis of learned representations in the residual stream.", "result": "Out-of-distribution generalization experiments show that transformers trained for ICL fail to generalize after shifts in the prompt distribution. Spectral analysis reveals a unique spectral signature (shared top two singular vectors) for inputs from the training distribution, which is absent in out-of-distribution inputs and correlated with low loss.", "conclusion": "Transformers trained for in-context learning (ICL) fail to generalize to out-of-distribution prompts, contradicting the idea that they implement algorithms like Ordinary Least Squares (OLS). The pretraining corpus plays a crucial role, as inputs from the training distribution exhibit a distinct spectral signature in the residual stream, which is correlated with low loss."}}
{"id": "2507.09574", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09574", "abs": "https://arxiv.org/abs/2507.09574", "authors": ["Haozhe Zhao", "Zefan Cai", "Shuzheng Si", "Liang Chen", "Jiuxiang Gu", "Wen Xiao", "Junjie Hu"], "title": "MENTOR: Efficient Multimodal-Conditioned Tuning for Autoregressive Vision Generation Models", "comment": "24 pages,12 figures", "summary": "Recent text-to-image models produce high-quality results but still struggle\nwith precise visual control, balancing multimodal inputs, and requiring\nextensive training for complex multimodal image generation. To address these\nlimitations, we propose MENTOR, a novel autoregressive (AR) framework for\nefficient Multimodal-conditioned Tuning for Autoregressive multimodal image\ngeneration. MENTOR combines an AR image generator with a two-stage training\nparadigm, enabling fine-grained, token-level alignment between multimodal\ninputs and image outputs without relying on auxiliary adapters or\ncross-attention modules. The two-stage training consists of: (1) a multimodal\nalignment stage that establishes robust pixel- and semantic-level alignment,\nfollowed by (2) a multimodal instruction tuning stage that balances the\nintegration of multimodal inputs and enhances generation controllability.\nDespite modest model size, suboptimal base components, and limited training\nresources, MENTOR achieves strong performance on the DreamBench++ benchmark,\noutperforming competitive baselines in concept preservation and prompt\nfollowing. Additionally, our method delivers superior image reconstruction\nfidelity, broad task adaptability, and improved training efficiency compared to\ndiffusion-based methods. Dataset, code, and models are available at:\nhttps://github.com/HaozheZhao/MENTOR", "AI": {"tldr": "MENTOR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u56de\u5f52\u6846\u67b6\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\uff08\u591a\u6a21\u6001\u5bf9\u9f50\u548c\u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\uff09\u89e3\u51b3\u4e86\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\uff0c\u5e76\u5728DreamBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u548c\u6269\u6563\u57fa\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u6587\u672c\u5230\u56fe\u50cf\u6a21\u578b\u5728\u7cbe\u786e\u89c6\u89c9\u63a7\u5236\u3001\u5e73\u8861\u591a\u6a21\u6001\u8f93\u5165\u4ee5\u53ca\u590d\u6742\u591a\u6a21\u6001\u56fe\u50cf\u751f\u6210\u7684\u8bad\u7ec3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "MENTOR\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u81ea\u56de\u5f52\uff08AR\uff09\u6846\u67b6\uff0c\u7ed3\u5408\u4e86AR\u56fe\u50cf\u751f\u6210\u5668\u548c\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u591a\u6a21\u6001\u8f93\u5165\u548c\u56fe\u50cf\u8f93\u51fa\u4e4b\u95f4\u7684\u7ec6\u7c92\u5ea6\u3001token\u7ea7\u5bf9\u9f50\uff0c\u65e0\u9700\u8f85\u52a9\u9002\u914d\u5668\u6216\u4ea4\u53c9\u6ce8\u610f\u529b\u6a21\u5757\u3002\u4e24\u9636\u6bb5\u8bad\u7ec3\u5305\u62ec\uff1a1\uff09\u591a\u6a21\u6001\u5bf9\u9f50\u9636\u6bb5\uff0c\u5efa\u7acb\u7a33\u5065\u7684\u50cf\u7d20\u7ea7\u548c\u8bed\u4e49\u7ea7\u5bf9\u9f50\uff1b2\uff09\u591a\u6a21\u6001\u6307\u4ee4\u8c03\u4f18\u9636\u6bb5\uff0c\u5e73\u8861\u591a\u6a21\u6001\u8f93\u5165\u96c6\u6210\u5e76\u589e\u5f3a\u751f\u6210\u53ef\u63a7\u6027\u3002", "result": "MENTOR\u5728DreamBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u5f3a\u5927\u7684\u6027\u80fd\uff0c\u5728\u6982\u5ff5\u4fdd\u6301\u548c\u63d0\u793a\u9075\u5faa\u65b9\u9762\u4f18\u4e8e\u7ade\u4e89\u6027\u57fa\u7ebf\u6a21\u578b\u3002\u6b64\u5916\uff0c\u4e0e\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u56fe\u50cf\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "MENTOR\u6846\u67b6\u5728DreamBench++\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5728\u6982\u5ff5\u4fdd\u6301\u548c\u63d0\u793a\u9075\u5faa\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u56fe\u50cf\u91cd\u5efa\u4fdd\u771f\u5ea6\u3001\u4efb\u52a1\u9002\u5e94\u6027\u548c\u8bad\u7ec3\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u3002"}}
{"id": "2507.09443", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2507.09443", "abs": "https://arxiv.org/abs/2507.09443", "authors": ["Luiz Aldeia Machado", "Victor Coppo Leite", "Elia Merzari", "Arthur Motta", "Roberto Ponciroli", "Lander Ibarra", "Lise Charlot"], "title": "Toward Developing Machine-Learning-Aided Tools for the Thermomechanical Monitoring of Nuclear Reactor Components", "comment": "Preprint - Nureth 21 paper", "summary": "Proactive maintenance strategies, such as Predictive Maintenance (PdM), play\nan important role in the operation of Nuclear Power Plants (NPPs), particularly\ndue to their capacity to reduce offline time by preventing unexpected shutdowns\ncaused by component failures.\n  In this work, we explore the use of a Convolutional Neural Network (CNN)\narchitecture combined with a computational thermomechanical model to calculate\nthe temperature, stress, and strain of a Pressurized Water Reactor (PWR) fuel\nrod during operation. This estimation relies on a limited number of temperature\nmeasurements from the cladding's outer surface. This methodology can\npotentially aid in developing PdM tools for nuclear reactors by enabling\nreal-time monitoring of such systems.\n  The training, validation, and testing datasets were generated through coupled\nsimulations involving BISON, a finite element-based nuclear fuel performance\ncode, and the MOOSE Thermal-Hydraulics Module (MOOSE-THM). We conducted eleven\nsimulations, varying the peak linear heat generation rates. Of these, eight\nwere used for training, two for validation, and one for testing.\n  The CNN was trained for over 1,000 epochs without signs of overfitting,\nachieving highly accurate temperature distribution predictions. These were then\nused in a thermomechanical model to determine the stress and strain\ndistribution within the fuel rod.", "AI": {"tldr": "\u901a\u8fc7CNN\u548c\u70ed\u529b\u5b66\u6a21\u578b\uff0c\u53ef\u4ee5\u7cbe\u786e\u9884\u6d4b\u6838\u71c3\u6599\u68d2\u7684\u6e29\u5ea6\u3001\u5e94\u529b\u548c\u5e94\u53d8\uff0c\u6709\u52a9\u4e8e\u6838\u53cd\u5e94\u5806\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u6838\u7535\u7ad9\u7684\u8fd0\u884c\u6548\u7387\uff0c\u51cf\u5c11\u56e0\u7ec4\u4ef6\u6545\u969c\u5bfc\u81f4\u7684\u610f\u5916\u505c\u673a\u65f6\u95f4\uff0c\u7814\u7a76\u9884\u6d4b\u6027\u7ef4\u62a4\uff08PdM\uff09\u7b56\u7565\uff0c\u7279\u522b\u662f\u9488\u5bf9\u6838\u53cd\u5e94\u5806\u7684\u8fd0\u884c\u3002", "method": "\u5229\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u67b6\u6784\u7ed3\u5408\u8ba1\u7b97\u70ed\u529b\u5b66\u6a21\u578b\uff0c\u57fa\u4e8e\u6709\u9650\u6570\u91cf\u7684\u5305\u8986\u7ba1\u5916\u8868\u9762\u6e29\u5ea6\u6d4b\u91cf\u503c\uff0c\u6765\u4f30\u7b97\u538b\u6c34\u5806\u71c3\u6599\u68d2\u5728\u8fd0\u884c\u671f\u95f4\u7684\u6e29\u5ea6\u3001\u5e94\u529b\u548c\u5e94\u53d8\u5206\u5e03\u3002\u8bad\u7ec3\u3001\u9a8c\u8bc1\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\u662f\u901a\u8fc7\u8026\u5408BISON\uff08\u4e00\u79cd\u57fa\u4e8e\u6709\u9650\u5143\u6cd5\u7684\u6838\u71c3\u6599\u6027\u80fd\u4ee3\u7801\uff09\u548cMOOSE\u70ed\u5de5\u6c34\u529b\u6a21\u5757\uff08MOOSE-THM\uff09\u7684\u6a21\u62df\u751f\u6210\u7684\u3002", "result": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u8d85\u8fc71000\u4e2a\u8bad\u7ec3\u5468\u671f\u540e\uff0c\u672a\u51fa\u73b0\u8fc7\u62df\u5408\u73b0\u8c61\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u7cbe\u786e\u7684\u6e29\u5ea6\u5206\u5e03\u9884\u6d4b\u3002\u8fd9\u4e9b\u9884\u6d4b\u968f\u540e\u88ab\u7528\u4e8e\u70ed\u529b\u5b66\u6a21\u578b\u4e2d\uff0c\u4ee5\u786e\u5b9a\u71c3\u6599\u68d2\u5185\u90e8\u7684\u5e94\u529b\u548c\u5e94\u53d8\u5206\u5e03\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u8ba1\u7b97\u70ed\u529b\u5b66\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u538b\u6c34\u5806\u6838\u71c3\u6599\u68d2\u5728\u8fd0\u884c\u671f\u95f4\u7684\u6e29\u5ea6\u3001\u5e94\u529b\u548c\u5e94\u53d8\u5206\u5e03\u7684\u7cbe\u786e\u9884\u6d4b\uff0c\u4e3a\u5f00\u53d1\u6838\u53cd\u5e94\u5806\u7684\u9884\u6d4b\u6027\u7ef4\u62a4\u5de5\u5177\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002"}}
{"id": "2507.09577", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09577", "abs": "https://arxiv.org/abs/2507.09577", "authors": ["Ming Yin", "Fu Wang", "Xujiong Ye", "Yanda Meng", "Zeyu Fu"], "title": "Memory-Augmented SAM2 for Training-Free Surgical Video Segmentation", "comment": null, "summary": "Surgical video segmentation is a critical task in computer-assisted surgery,\nessential for enhancing surgical quality and patient outcomes. Recently, the\nSegment Anything Model 2 (SAM2) framework has demonstrated remarkable\nadvancements in both image and video segmentation. However, the inherent\nlimitations of SAM2's greedy selection memory design are amplified by the\nunique properties of surgical videos-rapid instrument movement, frequent\nocclusion, and complex instrument-tissue interaction-resulting in diminished\nperformance in the segmentation of complex, long videos. To address these\nchallenges, we introduce Memory Augmented (MA)-SAM2, a training-free video\nobject segmentation strategy, featuring novel context-aware and\nocclusion-resilient memory models. MA-SAM2 exhibits strong robustness against\nocclusions and interactions arising from complex instrument movements while\nmaintaining accuracy in segmenting objects throughout videos. Employing a\nmulti-target, single-loop, one-prompt inference further enhances the efficiency\nof the tracking process in multi-instrument videos. Without introducing any\nadditional parameters or requiring further training, MA-SAM2 achieved\nperformance improvements of 4.36% and 6.1% over SAM2 on the EndoVis2017 and\nEndoVis2018 datasets, respectively, demonstrating its potential for practical\nsurgical applications.", "AI": {"tldr": "\u9488\u5bf9SAM2\u5728\u624b\u672f\u89c6\u9891\u5206\u5272\u4e2d\u56e0\u5668\u68b0\u79fb\u52a8\u3001\u906e\u6321\u548c\u4ea4\u4e92\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u51faMA-SAM2\u7b56\u7565\uff0c\u901a\u8fc7\u5f15\u5165\u5185\u5b58\u6a21\u578b\u6539\u8fdb\u4e86\u5206\u5272\u6027\u80fd\uff0c\u5e76\u5728\u65e0\u9700\u989d\u5916\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u5728EndoVis\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6027\u80fd\u63d0\u5347\u3002", "motivation": "SAM2\u6846\u67b6\u5728\u624b\u672f\u89c6\u9891\u5206\u5272\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u662f\u7531\u5176\u8d2a\u5a6a\u9009\u62e9\u5185\u5b58\u8bbe\u8ba1\u4ee5\u53ca\u624b\u672f\u89c6\u9891\u7684\u5feb\u901f\u5668\u68b0\u79fb\u52a8\u3001\u9891\u7e41\u906e\u6321\u548c\u590d\u6742\u7684\u5668\u68b0-\u7ec4\u7ec7\u4ea4\u4e92\u7b49\u7279\u6027\u6240\u653e\u5927\u7684\uff0c\u5bfc\u81f4\u5728\u5206\u5272\u590d\u6742\u3001\u957f\u89c6\u9891\u65f6\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMA-SAM2\u7684\u8bad\u7ec3\u65e0\u5173\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5177\u6709\u65b0\u9896\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u548c\u906e\u6321\u6062\u590d\u5185\u5b58\u6a21\u578b\u3002\u8be5\u7b56\u7565\u91c7\u7528\u591a\u76ee\u6807\u3001\u5355\u5faa\u73af\u3001\u5355\u63d0\u793a\u63a8\u7406\uff0c\u4ee5\u63d0\u9ad8\u591a\u5668\u68b0\u89c6\u9891\u4e2d\u8ddf\u8e2a\u8fc7\u7a0b\u7684\u6548\u7387\u3002", "result": "MA-SAM2\u5728\u906e\u6321\u548c\u590d\u6742\u5668\u68b0\u79fb\u52a8\u5f15\u8d77\u7684\u4ea4\u4e92\u65b9\u9762\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u5728\u6574\u4e2a\u89c6\u9891\u4e2d\u5206\u5272\u5bf9\u8c61\u7684\u51c6\u786e\u6027\u3002", "conclusion": "MA-SAM2\u5728EndoVis2017\u548cEndoVis2018\u6570\u636e\u96c6\u4e0a\u5206\u522b\u6bd4SAM2\u7684\u6027\u80fd\u63d0\u9ad8\u4e864.36%\u548c6.1%\uff0c\u663e\u793a\u51fa\u5176\u5728\u5b9e\u9645\u624b\u672f\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09445", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09445", "abs": "https://arxiv.org/abs/2507.09445", "authors": ["Runze Yang", "Longbing Cao", "Xin You", "Kun Fang", "Jianxun Li", "Jie Yang"], "title": "Fourier Basis Mapping: A Time-Frequency Learning Framework for Time Series Forecasting", "comment": "18 pages, 6 figures", "summary": "The integration of Fourier transform and deep learning opens new avenues for\ntime series forecasting. We reconsider the Fourier transform from a basis\nfunctions perspective. Specifically, the real and imaginary parts of the\nfrequency components can be regarded as the coefficients of cosine and sine\nbasis functions at tiered frequency levels, respectively. We find that existing\nFourier-based methods face inconsistent starting cycles and inconsistent series\nlength issues. They fail to interpret frequency components precisely and\noverlook temporal information. Accordingly, the novel Fourier Basis Mapping\n(FBM) method addresses these issues by integrating time-frequency features\nthrough Fourier basis expansion and mapping in the time-frequency space. Our\napproach extracts explicit frequency features while preserving temporal\ncharacteristics. FBM supports plug-and-play integration with various types of\nneural networks by only adjusting the first initial projection layer for better\nperformance. First, we propose FBM-L, FBM-NL, and FBM-NP to enhance linear,\nMLP-based, and Transformer-based models, respectively, demonstrating the\neffectiveness of time-frequency features. Next, we propose a synergetic model\narchitecture, termed FBM-S, which decomposes the seasonal, trend, and\ninteraction effects into three separate blocks, each designed to model\ntime-frequency features in a specialized manner. Finally, we introduce several\ntechniques tailored for time-frequency features, including interaction masking,\ncentralization, patching, rolling window projection, and multi-scale\ndown-sampling. The results are validated on diverse real-world datasets for\nboth long-term and short-term forecasting tasks with SOTA performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5085\u91cc\u53f6\u57fa\u6620\u5c04\uff08FBM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u65f6\u9891\u7279\u5f81\u6765\u89e3\u51b3\u73b0\u6709\u5085\u91cc\u53f6\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5404\u79cd\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5085\u91cc\u53f6\u7684\u65b9\u6cd5\u9762\u4e34\u4e0d\u4e00\u81f4\u7684\u8d77\u59cb\u5468\u671f\u548c\u4e0d\u4e00\u81f4\u7684\u5e8f\u5217\u957f\u5ea6\u95ee\u9898\uff0c\u5b83\u4eec\u672a\u80fd\u7cbe\u786e\u89e3\u91ca\u9891\u7387\u5206\u91cf\u5e76\u5ffd\u7565\u4e86\u65f6\u95f4\u4fe1\u606f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5085\u91cc\u53f6\u57fa\u6620\u5c04\uff08FBM\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u5085\u91cc\u53f6\u57fa\u5c55\u5f00\u548c\u65f6\u9891\u7a7a\u95f4\u6620\u5c04\u6765\u6574\u5408\u65f6\u9891\u7279\u5f81\u3002FBM\u901a\u8fc7\u8c03\u6574\u7b2c\u4e00\u4e2a\u521d\u59cb\u6295\u5f71\u5c42\u5373\u53ef\u4e0e\u5404\u79cd\u7c7b\u578b\u7684\u795e\u7ecf\u7f51\u7edc\u5373\u63d2\u5373\u7528\u5730\u96c6\u6210\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u63d0\u51fa\u4e86FBM-L\u3001FBM-NL\u548cFBM-NP\u6765\u589e\u5f3a\u7ebf\u6027\u3001\u57fa\u4e8eMLP\u548c\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u534f\u540c\u6a21\u578b\u67b6\u6784FBM-S\uff0c\u5c06\u5b63\u8282\u6027\u3001\u8d8b\u52bf\u548c\u4ea4\u4e92\u6548\u5e94\u5206\u89e3\u4e3a\u4e09\u4e2a\u5355\u72ec\u7684\u5757\uff0c\u6bcf\u4e2a\u5757\u90fd\u4ee5\u4e13\u4e1a\u7684\u65b9\u5f0f\u5bf9\u65f6\u9891\u7279\u5f81\u8fdb\u884c\u5efa\u6a21\u3002\u6700\u540e\uff0c\u5f15\u5165\u4e86\u5305\u62ec\u4ea4\u4e92\u63a9\u7801\u3001\u4e2d\u5fc3\u5316\u3001\u6253\u8865\u4e01\u3001\u6eda\u52a8\u7a97\u53e3\u6295\u5f71\u548c\u591a\u5c3a\u5ea6\u4e0b\u91c7\u6837\u5728\u5185\u7684\u51e0\u79cd\u9488\u5bf9\u65f6\u9891\u7279\u5f81\u7684\u6280\u672f\u3002", "result": "FBM\u63d0\u53d6\u663e\u6027\u9891\u7387\u7279\u5f81\uff0c\u540c\u65f6\u4fdd\u7559\u65f6\u95f4\u7279\u5f81\u3002FBM-L\u3001FBM-NL\u548cFBM-NP\u6a21\u578b\u8bc1\u660e\u4e86\u65f6\u9891\u7279\u5f81\u7684\u6709\u6548\u6027\u3002", "conclusion": "FBM\u5728\u957f\u671f\u548c\u77ed\u671f\u9884\u6d4b\u4efb\u52a1\u7684\u5404\u79cd\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u901a\u8fc7SOTA\u6027\u80fd\u5f97\u5230\u9a8c\u8bc1\u3002"}}
{"id": "2507.09595", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09595", "abs": "https://arxiv.org/abs/2507.09595", "authors": ["Or Greenberg"], "title": "Demystifying Flux Architecture", "comment": null, "summary": "FLUX.1 is a diffusion-based text-to-image generation model developed by Black\nForest Labs, designed to achieve faithful text-image alignment while\nmaintaining high image quality and diversity. FLUX is considered\nstate-of-the-art in text-to-image generation, outperforming popular models such\nas Midjourney, DALL-E 3, Stable Diffusion 3 (SD3), and SDXL. Although publicly\navailable as open source, the authors have not released official technical\ndocumentation detailing the model's architecture or training setup. This report\nsummarizes an extensive reverse-engineering effort aimed at demystifying FLUX's\narchitecture directly from its source code, to support its adoption as a\nbackbone for future research and development. This document is an unofficial\ntechnical report and is not published or endorsed by the original developers or\ntheir affiliated institutions.", "AI": {"tldr": "FLUX.1\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u672c\u62a5\u544a\u5bf9\u5176\u8fdb\u884c\u4e86\u9006\u5411\u5de5\u7a0b\u5206\u6790\uff0c\u4ee5\u63ed\u793a\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u7ec6\u8282\u3002", "motivation": "\u4e3a\u4e86\u652f\u6301FLUX.1\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u548c\u5f00\u53d1\u7684\u9aa8\u5e72\uff0c\u63ed\u79d8\u5176\u67b6\u6784\u3002", "method": "\u901a\u8fc7\u5bf9\u5f00\u6e90\u6a21\u578bFLUX.1\u7684\u6e90\u4ee3\u7801\u8fdb\u884c\u9006\u5411\u5de5\u7a0b\u6765\u5206\u6790\u5176\u67b6\u6784\u548c\u8bad\u7ec3\u8bbe\u7f6e\u3002", "result": "\u8be6\u7ec6\u8bf4\u660e\u4e86FLUX.1\u7684\u67b6\u6784\u548c\u8bad\u7ec3\u8bbe\u7f6e\uff0c\u4f7f\u5176\u80fd\u591f\u88ab\u5e7f\u6cdb\u91c7\u7528\u3002", "conclusion": "FLUX.1\u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u6587\u672c\u5230\u56fe\u50cf\u751f\u6210\u6a21\u578b\uff0c\u5728\u6587\u672c-\u56fe\u50cf\u5bf9\u9f50\u3001\u56fe\u50cf\u8d28\u91cf\u548c\u591a\u6837\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.09460", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09460", "abs": "https://arxiv.org/abs/2507.09460", "authors": ["Noah Marchal", "William E. Janes", "Mihail Popescu", "Xing Song"], "title": "Enhancing ALS Progression Tracking with Semi-Supervised ALSFRS-R Scores Estimated from Ambient Home Health Monitoring", "comment": "31 pages, 8 Figures", "summary": "Clinical monitoring of functional decline in ALS relies on periodic\nassessments that may miss critical changes occurring between visits. To address\nthis gap, semi-supervised regression models were developed to estimate rates of\ndecline in a case series cohort by targeting ALSFRS- R scale trajectories with\ncontinuous in-home sensor monitoring data. Our analysis compared three model\nparadigms (individual batch learning and cohort-level batch versus incremental\nfine-tuned transfer learning) across linear slope, cubic polynomial, and\nensembled self-attention pseudo-label interpolations. Results revealed cohort\nhomogeneity across functional domains responding to learning methods, with\ntransfer learning improving prediction error for ALSFRS-R subscales in 28 of 32\ncontrasts (mean RMSE=0.20(0.04)), and individual batch learning for predicting\nthe composite scale (mean RMSE=3.15(1.25)) in 2 of 3. Self-attention\ninterpolation achieved the lowest prediction error for subscale-level models\n(mean RMSE=0.19(0.06)), capturing complex nonlinear progression patterns,\noutperforming linear and cubic interpolations in 20 of 32 contrasts, though\nlinear interpolation proved more stable in all ALSFRS-R composite scale models\n(mean RMSE=0.23(0.10)). We identified distinct homogeneity-heterogeneity\nprofiles across functional domains with respiratory and speech exhibiting\npatient-specific patterns benefiting from personalized incremental adaptation,\nwhile swallowing and dressing functions followed cohort-level trajectories\nsuitable for transfer models. These findings suggest that matching learning and\npseudo-labeling techniques to functional domain-specific\nhomogeneity-heterogeneity profiles enhances predictive accuracy in ALS\nprogression tracking. Integrating adaptive model selection within sensor\nmonitoring platforms could enable timely interventions and scalable deployment\nin future multi-center studies.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u534a\u76d1\u7763\u5b66\u4e60\u548c\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff0c\u5229\u7528\u8fde\u7eed\u5c45\u5bb6\u4f20\u611f\u5668\u6570\u636e\u9884\u6d4b ALS \u60a3\u8005\u7684\u529f\u80fd\u8870\u9000\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4e0d\u540c\u7684\u529f\u80fd\u57df\uff08\u5982\u547c\u5438\u3001\u8a00\u8bed\u3001\u541e\u54bd\u3001\u7a7f\u8863\uff09\u5177\u6709\u4e0d\u540c\u7684\u540c\u8d28\u6027-\u5f02\u8d28\u6027\u7279\u5f81\uff0c\u9700\u8981\u5339\u914d\u4e0d\u540c\u7684\u5b66\u4e60\u548c\u63d2\u503c\u6280\u672f\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002\u6700\u7ec8\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u5728\u4f20\u611f\u5668\u76d1\u63a7\u5e73\u53f0\u4e2d\u96c6\u6210\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u7684\u5efa\u8bae\uff0c\u4ee5\u5b9e\u73b0\u53ca\u65f6\u7684\u5e72\u9884\u548c\u53ef\u6269\u5c55\u7684\u90e8\u7f72\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e34\u5e8a\u76d1\u6d4b ALS \u529f\u80fd\u8870\u9000\u53ef\u80fd\u9519\u8fc7\u5173\u952e\u53d8\u5316\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u65b0\u7684\u76d1\u6d4b\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u534a\u76d1\u7763\u56de\u5f52\u6a21\u578b\uff0c\u901a\u8fc7\u9488\u5bf9 ALSFRS-R \u91cf\u8868\u8f68\u8ff9\u548c\u8fde\u7eed\u5c45\u5bb6\u4f20\u611f\u5668\u76d1\u63a7\u6570\u636e\u6765\u4f30\u8ba1\u75c5\u4f8b\u7cfb\u5217\u961f\u5217\u4e2d\u7684\u8870\u9000\u7387\u3002\u7814\u7a76\u4eba\u5458\u6bd4\u8f83\u4e86\u4e09\u79cd\u6a21\u578b\u8303\u5f0f\uff08\u4e2a\u4f53\u6279\u91cf\u5b66\u4e60\u548c\u961f\u5217\u7ea7\u6279\u91cf\u4e0e\u589e\u91cf\u5fae\u8c03\u8fc1\u79fb\u5b66\u4e60\uff09\u5728\u56db\u79cd\u56de\u5f52\u6a21\u578b\uff08\u7ebf\u6027\u659c\u7387\u3001\u4e09\u6b21\u591a\u9879\u5f0f\u548c\u96c6\u6210\u81ea\u6ce8\u610f\u529b\u4f2a\u6807\u7b7e\u63d2\u503c\uff09\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u5728\u529f\u80fd\u57df\u65b9\u9762\uff0c\u961f\u5217\u5177\u6709\u540c\u8d28\u6027\uff0c\u80fd\u591f\u54cd\u5e94\u5b66\u4e60\u65b9\u6cd5\u3002\u8fc1\u79fb\u5b66\u4e60\u5728 28/32 \u7684\u5bf9\u6bd4\u4e2d\u63d0\u9ad8\u4e86 ALSFRS-R \u5206\u91cf\u8868\u7684\u9884\u6d4b\u8bef\u5dee\uff08\u5e73\u5747 RMSE=0.20(0.04)\uff09\uff0c\u800c\u4e2a\u4f53\u6279\u91cf\u5b66\u4e60\u5728 2/3 \u7684\u5bf9\u6bd4\u4e2d\u9884\u6d4b\u4e86\u590d\u5408\u91cf\u8868\uff08\u5e73\u5747 RMSE=3.15(1.25)\uff09\u3002\u81ea\u6ce8\u610f\u529b\u63d2\u503c\u5728\u5206\u91cf\u7ea7\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u6700\u4f4e\u7684\u9884\u6d4b\u8bef\u5dee\uff08\u5e73\u5747 RMSE=0.19(0.06)\uff09\uff0c\u80fd\u591f\u6355\u6349\u590d\u6742\u7684\u975e\u7ebf\u6027\u8fdb\u5c55\u6a21\u5f0f\uff0c\u5728 20/32 \u7684\u5bf9\u6bd4\u4e2d\u4f18\u4e8e\u7ebf\u6027\u548c\u4e09\u6b21\u63d2\u503c\uff0c\u4f46\u7ebf\u6027\u63d2\u503c\u5728\u6240\u6709 ALSFRS-R \u590d\u5408\u91cf\u8868\u6a21\u578b\u4e0a\u8868\u73b0\u66f4\u7a33\u5b9a\uff08\u5e73\u5747 RMSE=0.23(0.10)\uff09\u3002\u7814\u7a76\u53d1\u73b0\u547c\u5438\u548c\u8a00\u8bed\u529f\u80fd\u57df\u8868\u73b0\u51fa\u53d7\u76ca\u4e8e\u4e2a\u6027\u5316\u589e\u91cf\u9002\u5e94\u7684\u60a3\u8005\u7279\u5f02\u6027\u6a21\u5f0f\uff0c\u800c\u541e\u54bd\u548c\u7a7f\u8863\u529f\u80fd\u57df\u5219\u9075\u5faa\u9002\u5408\u8fc1\u79fb\u6a21\u578b\u7684\u961f\u5217\u7ea7\u8f68\u8ff9\u3002", "conclusion": "\u901a\u8fc7\u5c06\u5b66\u4e60\u548c\u4f2a\u6807\u7b7e\u6280\u672f\u4e0e\u7279\u5b9a\u529f\u80fd\u57df\u7684\u540c\u8d28\u6027-\u5f02\u8d28\u6027\u7279\u5f81\u76f8\u5339\u914d\uff0c\u53ef\u4ee5\u63d0\u9ad8 ALS \u8fdb\u5c55\u8ddf\u8e2a\u7684\u9884\u6d4b\u51c6\u786e\u6027\u3002\u5728\u4f20\u611f\u5668\u76d1\u63a7\u5e73\u53f0\u4e2d\u96c6\u6210\u81ea\u9002\u5e94\u6a21\u578b\u9009\u62e9\u53ef\u4ee5\u5b9e\u73b0\u53ca\u65f6\u7684\u5e72\u9884\u548c\u672a\u6765\u591a\u4e2d\u5fc3\u7814\u7a76\u7684\u53ef\u6269\u5c55\u90e8\u7f72\u3002"}}
{"id": "2507.09612", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09612", "abs": "https://arxiv.org/abs/2507.09612", "authors": ["You Huang", "Lichao Chen", "Jiayi Ji", "Liujuan Cao", "Shengchuan Zhang", "Rongrong Ji"], "title": "Inter2Former: Dynamic Hybrid Attention for Efficient High-Precision Interactive", "comment": "Accepted by ICCV 2025", "summary": "Interactive segmentation (IS) improves annotation efficiency by segmenting\ntarget regions from user prompts, with widespread applications in real-world\nscenarios. Current approaches face a critical trade-off: dense-token methods\nachieve superior accuracy and detail preservation but suffer from prohibitively\nslow processing on CPU devices, while the Segment Anything Model (SAM) advances\nthe field with sparse prompt tokens for fast inference but compromises\nsegmentation quality. In this paper, we propose Inter2Former to address this\nchallenge by optimizing computation allocation in dense-token processing, which\nintroduces four key enhancements. First, we propose Dynamic Prompt Embedding\n(DPE) that adaptively processes only regions of interest while avoiding\nadditional overhead from background tokens. Second, we introduce Dynamic Hybrid\nAttention (DHA), which leverages previous segmentation masks to route tokens\nthrough either full attention (O(N2)) for boundary regions or our proposed\nefficient BSQ attention (O(N)) for non-boundary regions. Third, we develop\nHybrid Mixture of Experts (HMoE), which applies similar adaptive computation\nstrategies in FFN modules with CPU-optimized parallel processing. Finally, we\npresent Dynamic Local Upsampling (DLU), a reverse operation of DPE, which\nlocalizes objects with a lightweight MLP and performs fine-grained upsampling\nonly in detected regions. Experimental results on high-precision IS benchmarks\ndemonstrate that Inter2Former achieves SOTA performance with high efficiency on\nCPU devices.", "AI": {"tldr": "Inter2Former\u901a\u8fc7DPE\u3001DHA\u3001HMoE\u548cDLU\u7b49\u6280\u672f\uff0c\u4f18\u5316\u4e86\u5bc6\u96c6\u4ee4\u724c\u5904\u7406\u4e2d\u7684\u8ba1\u7b97\u5206\u914d\uff0c\u89e3\u51b3\u4e86\u4ea4\u4e92\u5f0f\u5206\u5272\u5728CPU\u4e0a\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u6743\u8861\u95ee\u9898\uff0c\u5e76\u5728\u9ad8\u7cbe\u5ea6IS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86SOTA\u6027\u80fd\u3002", "motivation": "\u76ee\u524d\u7684\u4ea4\u4e92\u5f0f\u5206\u5272\uff08IS\uff09\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u5904\u7406\u901f\u5ea6\u4e4b\u95f4\u5b58\u5728\u6743\u8861\uff1a\u5bc6\u96c6\u4ee4\u724c\u65b9\u6cd5\u7cbe\u5ea6\u9ad8\u4f46CPU\u5904\u7406\u6162\uff0c\u800cSAM\u6a21\u578b\u901f\u5ea6\u5feb\u4f46\u5206\u5272\u8d28\u91cf\u6709\u635f\u3002", "method": "Inter2Former\u901a\u8fc7\u4f18\u5316\u5bc6\u96c6\u4ee4\u724c\u5904\u7406\u4e2d\u7684\u8ba1\u7b97\u5206\u914d\u6765\u89e3\u51b3\u6311\u6218\uff0c\u5177\u4f53\u5305\u62ec\uff1a1. \u52a8\u6001\u63d0\u793a\u5d4c\u5165\uff08DPE\uff09\u81ea\u9002\u5e94\u5904\u7406\u611f\u5174\u8da3\u7684\u533a\u57df\u30022. \u52a8\u6001\u6df7\u5408\u6ce8\u610f\u529b\uff08DHA\uff09\u5229\u7528\u5148\u524d\u7684\u5206\u5272\u63a9\u7801\u5c06\u4ee4\u724c\u8def\u7531\u5230\u5168\u6ce8\u610f\u529b\u6216BSQ\u6ce8\u610f\u529b\u30023. \u6df7\u5408\u4e13\u5bb6\u6a21\u578b\uff08HMoE\uff09\u5728FFN\u6a21\u5757\u4e2d\u5e94\u7528\u81ea\u9002\u5e94\u8ba1\u7b97\u7b56\u7565\u30024. \u52a8\u6001\u5c40\u90e8\u4e0a\u91c7\u6837\uff08DLU\uff09\u5728\u68c0\u6d4b\u5230\u7684\u533a\u57df\u8fdb\u884c\u7cbe\u7ec6\u4e0a\u91c7\u6837\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cInter2Former\u5728CPU\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u548c\u9ad8\u6548\u7387\u3002", "conclusion": "Inter2Former\u5728CPU\u8bbe\u5907\u4e0a\u5b9e\u73b0\u4e86SOTA\u6027\u80fd\u548c\u9ad8\u6548\u7387\u3002"}}
{"id": "2507.09466", "categories": ["cs.LG", "q-bio.QM", "I.2.1"], "pdf": "https://arxiv.org/pdf/2507.09466", "abs": "https://arxiv.org/abs/2507.09466", "authors": ["Tomas Geffner", "Kieran Didi", "Zhonglin Cao", "Danny Reidenbach", "Zuobai Zhang", "Christian Dallago", "Emine Kucukbenli", "Karsten Kreis", "Arash Vahdat"], "title": "La-Proteina: Atomistic Protein Generation via Partially Latent Flow Matching", "comment": null, "summary": "Recently, many generative models for de novo protein structure design have\nemerged. Yet, only few tackle the difficult task of directly generating fully\natomistic structures jointly with the underlying amino acid sequence. This is\nchallenging, for instance, because the model must reason over side chains that\nchange in length during generation. We introduce La-Proteina for atomistic\nprotein design based on a novel partially latent protein representation: coarse\nbackbone structure is modeled explicitly, while sequence and atomistic details\nare captured via per-residue latent variables of fixed dimensionality, thereby\neffectively side-stepping challenges of explicit side-chain representations.\nFlow matching in this partially latent space then models the joint distribution\nover sequences and full-atom structures. La-Proteina achieves state-of-the-art\nperformance on multiple generation benchmarks, including all-atom\nco-designability, diversity, and structural validity, as confirmed through\ndetailed structural analyses and evaluations. Notably, La-Proteina also\nsurpasses previous models in atomistic motif scaffolding performance, unlocking\ncritical atomistic structure-conditioned protein design tasks. Moreover,\nLa-Proteina is able to generate co-designable proteins of up to 800 residues, a\nregime where most baselines collapse and fail to produce valid samples,\ndemonstrating La-Proteina's scalability and robustness.", "AI": {"tldr": "La-Proteina\u662f\u4e00\u79cd\u65b0\u7684\u86cb\u767d\u8d28\u8bbe\u8ba1\u6a21\u578b\uff0c\u4f7f\u7528\u90e8\u5206\u6f5c\u5728\u8868\u793a\u548c\u6d41\u5339\u914d\uff0c\u80fd\u591f\u540c\u65f6\u751f\u6210\u86cb\u767d\u8d28\u5e8f\u5217\u548c\u5168\u539f\u5b50\u7ed3\u6784\uff0c\u5e76\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5904\u7406\u957f\u94fe\u86cb\u767d\u8d28\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u4ece\u5934\u86cb\u767d\u8d28\u7ed3\u6784\u8bbe\u8ba1\u751f\u6210\u6a21\u578b\u5927\u591a\u96be\u4ee5\u540c\u65f6\u751f\u6210\u5168\u539f\u5b50\u7ed3\u6784\u548c\u6c28\u57fa\u9178\u5e8f\u5217\uff0c\u56e0\u4e3a\u6a21\u578b\u9700\u8981\u5904\u7406\u957f\u5ea6\u53ef\u53d8\u7684\u4fa7\u94fe\u3002", "method": "La-Proteina\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u90e8\u5206\u6f5c\u5728\u86cb\u767d\u8d28\u8868\u793a\u65b9\u6cd5\uff0c\u663e\u5f0f\u5730\u5bf9\u7c97\u7565\u7684\u9aa8\u67b6\u7ed3\u6784\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u6bcf\u4e2a\u6b8b\u57fa\u7684\u56fa\u5b9a\u7ef4\u5ea6\u6f5c\u5728\u53d8\u91cf\u6765\u6355\u83b7\u5e8f\u5217\u548c\u539f\u5b50\u7ec6\u8282\uff0c\u4ece\u800c\u6709\u6548\u907f\u5f00\u4e86\u663e\u5f0f\u4fa7\u94fe\u8868\u793a\u7684\u6311\u6218\u3002\u7136\u540e\uff0c\u5728\u90e8\u5206\u6f5c\u5728\u7a7a\u95f4\u4e2d\u4f7f\u7528\u6d41\u5339\u914d\u6765\u6a21\u62df\u5e8f\u5217\u548c\u5168\u539f\u5b50\u7ed3\u6784\u4e0a\u7684\u8054\u5408\u5206\u5e03\u3002", "result": "La-Proteina\u5728\u5305\u62ec\u5168\u539f\u5b50\u5171\u8bbe\u8ba1\u6027\u3001\u591a\u6837\u6027\u548c\u7ed3\u6784\u6709\u6548\u6027\u5728\u5185\u7684\u591a\u9879\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u5b83\u5728\u539f\u5b50\u56fe\u8c31\u652f\u67b6\u6027\u80fd\u65b9\u9762\u4e5f\u4f18\u4e8e\u5148\u524d\u6a21\u578b\uff0c\u5e76\u80fd\u751f\u6210\u957f\u8fbe800\u4e2a\u6b8b\u57fa\u7684\u86cb\u767d\u8d28\uff0c\u800c\u5927\u591a\u6570\u57fa\u7ebf\u6a21\u578b\u5728\u6b64\u89c4\u6a21\u4e0b\u4f1a\u5931\u6548\u3002", "conclusion": "La-Proteina\u5728\u86cb\u767d\u8d28\u8bbe\u8ba1\u9886\u57df\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u591a\u9879\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5305\u62ec\u5168\u539f\u5b50\u5171\u8bbe\u8ba1\u6027\u3001\u591a\u6837\u6027\u548c\u7ed3\u6784\u6709\u6548\u6027\u3002\u6b64\u5916\uff0cLa-Proteina\u5728\u539f\u5b50\u56fe\u8c31\u652f\u67b6\u6027\u80fd\u65b9\u9762\u4e5f\u8d85\u8d8a\u4e86\u4ee5\u5f80\u7684\u6a21\u578b\uff0c\u5e76\u80fd\u591f\u751f\u6210\u957f\u8fbe800\u4e2a\u6b8b\u57fa\u7684\u5171\u8bbe\u8ba1\u86cb\u767d\u8d28\uff0c\u5c55\u73b0\u4e86\u5176\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.09615", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09615", "abs": "https://arxiv.org/abs/2507.09615", "authors": ["Eman Ali", "Sathira Silva", "Chetan Arora", "Muhammad Haris Khan"], "title": "Towards Fine-Grained Adaptation of CLIP via a Self-Trained Alignment Score", "comment": null, "summary": "Vision-language models (VLMs) like CLIP excel in zero-shot learning by\naligning image and text representations through contrastive pretraining.\nExisting approaches to unsupervised adaptation (UA) for fine-grained\nclassification with VLMs either rely on fixed alignment scores that cannot\ncapture evolving, subtle class distinctions or use computationally expensive\npseudo-labeling strategies that limit scalability. In contrast, we show that\nmodeling fine-grained cross-modal interactions during adaptation produces more\naccurate, class-discriminative pseudo-labels and substantially improves\nperformance over state-of-the-art (SOTA) methods. We introduce Fine-grained\nAlignment and Interaction Refinement (FAIR), an innovative approach that\ndynamically aligns localized image features with descriptive language\nembeddings through a set of Class Description Anchors (CDA). This enables the\ndefinition of a Learned Alignment Score (LAS), which incorporates CDA as an\nadaptive classifier, facilitating cross-modal interactions to improve\nself-training in unsupervised adaptation. Furthermore, we propose a\nself-training weighting mechanism designed to refine pseudo-labels in the\npresence of inter-class ambiguities. Our approach, FAIR, delivers a substantial\nperformance boost in fine-grained unsupervised adaptation, achieving a notable\noverall gain of 2.78% across 13 fine-grained datasets compared to SOTA methods.", "AI": {"tldr": "FAIR\u901a\u8fc7\u52a8\u6001\u5bf9\u9f50\u5c40\u90e8\u56fe\u50cf\u7279\u5f81\u548c\u8bed\u8a00\u5d4c\u5165\u6765\u6539\u8fdb\u7ec6\u7c92\u5ea6\u65e0\u76d1\u7763\u9002\u5e94\uff0c\u5e76\u572813\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8eSOTA\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684VLMs\u65e0\u76d1\u7763\u9002\u5e94\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u5bf9\u9f50\u5206\u6570\uff0c\u8981\u4e48\u4f7f\u7528\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u4f2a\u6807\u7b7e\u7b56\u7565\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u9f50\u548c\u4ea4\u4e92\u7ec6\u5316\uff08FAIR\uff09\u6765\u6a21\u62df\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u65b9\u6cd5\uff0c\u4ee5\u4ea7\u751f\u66f4\u51c6\u786e\u3001\u66f4\u5177\u533a\u5206\u6027\u7684\u4f2a\u6807\u7b7e\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\u3002", "method": "FAIR\u901a\u8fc7\u4e00\u7ec4\u7c7b\u522b\u63cf\u8ff0\u951a\u70b9\uff08CDA\uff09\u52a8\u6001\u5730\u5c06\u5c40\u90e8\u56fe\u50cf\u7279\u5f81\u4e0e\u63cf\u8ff0\u6027\u8bed\u8a00\u5d4c\u5165\u5bf9\u9f50\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u5b66\u4e60\u5230\u7684\u5bf9\u9f50\u5206\u6570\uff08LAS\uff09\uff0c\u5b83\u5c06CDA\u4f5c\u4e3a\u81ea\u9002\u5e94\u5206\u7c7b\u5668\uff0c\u4fc3\u8fdb\u8de8\u6a21\u6001\u4ea4\u4e92\u4ee5\u6539\u8fdb\u65e0\u76d1\u7763\u9002\u5e94\u4e2d\u7684\u81ea\u8bad\u7ec3\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u8bad\u7ec3\u52a0\u6743\u673a\u5236\u6765\u5904\u7406\u7c7b\u522b\u95f4\u6a21\u7cca\u6027\u3002", "result": "FAIR\u572813\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u7684\u6574\u4f53\u8868\u73b0\u6bd4SOTA\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8\u4e862.78%\u3002", "conclusion": "FAIR\u5728\u7ec6\u7c92\u5ea6\u65e0\u76d1\u7763\u9002\u5e94\u65b9\u9762\u63d0\u4f9b\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u572813\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u4e0eSOTA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u603b\u4f53\u63d0\u5347\u4e862.78%\u3002"}}
{"id": "2507.09876", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09876", "abs": "https://arxiv.org/abs/2507.09876", "authors": ["Yongheng Zhang", "Xu Liu", "Ruihan Tao", "Qiguang Chen", "Hao Fei", "Wanxiang Che", "Libo Qin"], "title": "ViTCoT: Video-Text Interleaved Chain-of-Thought for Boosting Video Understanding in Large Language Models", "comment": "Accepted by ACM MM 2025", "summary": "Video understanding plays a vital role in bridging low-level visual signals\nwith high-level cognitive reasoning, and is fundamental to applications such as\nautonomous driving, embodied AI, and the broader pursuit of AGI. The rapid\ndevelopment of large language models (LLMs), particularly those utilizing\nChain-of-Thought (CoT) technology, has significantly advanced video reasoning\ncapabilities. However, current approaches primarily depend on textual\ninformation for reasoning, overlooking the visual modality in the actual video\nreasoning process. In contrast, humans naturally re-examine visual content\nwhile reasoning. Motivated by this, we introduce a novel video reasoning\nparadigm: Video-Text Interleaved CoT (ViTCoT), which facilitates more intuitive\nand cognitively aligned reasoning. To the end, first, we construct the\nVideo-Text Interleaved Benchmark (ViTIB), which is created using MLLMs for\nkey-video selection and manually verified. Furthermore, we extensively explore\nthe potential of the ViTCoT paradigm in the video understanding field.\nExtensive experiments demonstrate that ViTCoT significantly enhances\nperformance compared to the traditional text-only CoT paradigm and effectively\nactivates more neuron values in MLLMs.", "AI": {"tldr": "\u53d7\u4eba\u7c7b\u89c6\u89c9\u63a8\u7406\u65b9\u5f0f\u7684\u542f\u53d1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891-\u6587\u672c\u4ea4\u9519\u63a8\u7406\u8303\u5f0f\uff08ViTCoT\uff09\u53ca\u5176\u57fa\u51c6\uff08ViTIB\uff09\uff0c\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4f18\u4e8e\u4ec5\u4f7f\u7528\u6587\u672c\u7684\u63a8\u7406\u65b9\u6cd5\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u5229\u7528\u591a\u6a21\u6001\u5927\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u601d\u7ef4\u94fe\uff08CoT\uff09\u7684\u89c6\u9891\u63a8\u7406\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u6587\u672c\u4fe1\u606f\uff0c\u5ffd\u7565\u4e86\u89c6\u9891\u6a21\u6001\u5728\u5b9e\u9645\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u4f5c\u7528\u3002\u800c\u4eba\u7c7b\u5728\u63a8\u7406\u65f6\u4f1a\u81ea\u7136\u5730\u91cd\u65b0\u5ba1\u89c6\u89c6\u89c9\u5185\u5bb9\u3002\u53d7\u6b64\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa ViTCoT \u8303\u5f0f\uff0c\u65e8\u5728\u5b9e\u73b0\u66f4\u76f4\u89c2\u3001\u66f4\u7b26\u5408\u8ba4\u77e5\u89c4\u5f8b\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c6\u9891\u63a8\u7406\u8303\u5f0f\uff1a\u89c6\u9891-\u6587\u672c\u4ea4\u9519\u601d\u7ef4\u94fe\uff08ViTCoT\uff09\u3002\u6784\u5efa\u4e86\u4e00\u4e2a\u89c6\u9891-\u6587\u672c\u4ea4\u9519\u57fa\u51c6\uff08ViTIB\uff09\uff0c\u8be5\u57fa\u51c6\u7531\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u7528\u4e8e\u5173\u952e\u89c6\u9891\u9009\u62e9\u5e76\u7ecf\u8fc7\u4eba\u5de5\u9a8c\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cViTCoT \u8303\u5f0f\u663e\u8457\u4f18\u4e8e\u7eaf\u6587\u672c CoT \u8303\u5f0f\uff0c\u5e76\u4e14\u80fd\u591f\u6fc0\u6d3b MLLMs \u4e2d\u66f4\u591a\u7684\u795e\u7ecf\u5143\u503c\u3002", "conclusion": "ViTCoT \u8303\u5f0f\u663e\u8457\u63d0\u9ad8\u4e86\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u4f18\u4e8e\u4f20\u7edf\u7684\u7eaf\u6587\u672c CoT \u8303\u5f0f\uff0c\u5e76\u80fd\u6709\u6548\u6fc0\u6d3b\u591a\u6a21\u6001\u5927\u6a21\u578b\uff08MLLMs\uff09\u7684\u66f4\u591a\u795e\u7ecf\u5143\u503c\u3002"}}
{"id": "2507.09619", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09619", "abs": "https://arxiv.org/abs/2507.09619", "authors": ["Yilin Lu", "Jianghang Lin", "Linhuang Xie", "Kai Zhao", "Yansong Qu", "Shengchuan Zhang", "Liujuan Cao", "Rongrong Ji"], "title": "Generate Aligned Anomaly: Region-Guided Few-Shot Anomaly Image-Mask Pair Synthesis for Industrial Inspection", "comment": null, "summary": "Anomaly inspection plays a vital role in industrial manufacturing, but the\nscarcity of anomaly samples significantly limits the effectiveness of existing\nmethods in tasks such as localization and classification. While several anomaly\nsynthesis approaches have been introduced for data augmentation, they often\nstruggle with low realism, inaccurate mask alignment, and poor generalization.\nTo overcome these limitations, we propose Generate Aligned Anomaly (GAA), a\nregion-guided, few-shot anomaly image-mask pair generation framework. GAA\nleverages the strong priors of a pretrained latent diffusion model to generate\nrealistic, diverse, and semantically aligned anomalies using only a small\nnumber of samples. The framework first employs Localized Concept Decomposition\nto jointly model the semantic features and spatial information of anomalies,\nenabling flexible control over the type and location of anomalies. It then\nutilizes Adaptive Multi-Round Anomaly Clustering to perform fine-grained\nsemantic clustering of anomaly concepts, thereby enhancing the consistency of\nanomaly representations. Subsequently, a region-guided mask generation strategy\nensures precise alignment between anomalies and their corresponding masks,\nwhile a low-quality sample filtering module is introduced to further improve\nthe overall quality of the generated samples. Extensive experiments on the\nMVTec AD and LOCO datasets demonstrate that GAA achieves superior performance\nin both anomaly synthesis quality and downstream tasks such as localization and\nclassification.", "AI": {"tldr": "\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u56e0\u6837\u672c\u7a00\u7f3a\u800c\u53d7\u9650\u3002\u672c\u7814\u7a76\u63d0\u51faGAA\u6846\u67b6\uff0c\u5229\u7528\u6f5c\u5728\u6269\u6563\u6a21\u578b\u548c\u7279\u5b9a\u6280\u672f\uff08\u5c40\u90e8\u6982\u5ff5\u5206\u89e3\u3001\u81ea\u9002\u5e94\u591a\u8f6e\u5f02\u5e38\u805a\u7c7b\u3001\u533a\u57df\u5f15\u5bfc\u63a9\u6a21\u751f\u6210\uff09\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8bed\u4e49\u5bf9\u9f50\u7684\u5f02\u5e38\u56fe\u50cf-\u63a9\u6a21\u5bf9\uff0c\u6709\u6548\u63d0\u5347\u4e86\u5f02\u5e38\u68c0\u6d4b\u7684\u6027\u80fd\u3002", "motivation": "\u5de5\u4e1a\u5236\u9020\u4e2d\u7684\u5f02\u5e38\u68c0\u6d4b\u5bf9\u4e8e\u4ea7\u54c1\u8d28\u91cf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u5728\u5b9a\u4f4d\u548c\u5206\u7c7b\u7b49\u4efb\u52a1\u4e2d\uff0c\u7531\u4e8e\u5f02\u5e38\u6837\u672c\u7a00\u7f3a\u800c\u53d7\u5230\u9650\u5236\u3002\u73b0\u6709\u7684\u5f02\u5e38\u5408\u6210\u65b9\u6cd5\u5b58\u5728\u771f\u5b9e\u611f\u4f4e\u3001\u63a9\u6a21\u5bf9\u9f50\u4e0d\u51c6\u786e\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u7b49\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u751f\u6210\u5bf9\u9f50\u5f02\u5e38\u201d\uff08GAA\uff09\u7684\u533a\u57df\u5f15\u5bfc\u3001\u5c11\u6837\u672c\u5f02\u5e38\u56fe\u50cf-\u63a9\u6a21\u5bf9\u751f\u6210\u6846\u67b6\u3002GAA\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6f5c\u5728\u6269\u6563\u6a21\u578b\u7684\u5f3a\u5927\u5148\u9a8c\u77e5\u8bc6\uff0c\u4ec5\u4f7f\u7528\u5c11\u91cf\u6837\u672c\u5373\u53ef\u751f\u6210\u903c\u771f\u3001\u591a\u6837\u5316\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u5f02\u5e38\u3002\u8be5\u6846\u67b6\u9996\u5148\u91c7\u7528\u201c\u5c40\u90e8\u6982\u5ff5\u5206\u89e3\u201d\u6765\u8054\u5408\u5efa\u6a21\u5f02\u5e38\u7684\u8bed\u4e49\u7279\u5f81\u548c\u7a7a\u95f4\u4fe1\u606f\uff0c\u4ece\u800c\u80fd\u591f\u7075\u6d3b\u63a7\u5236\u5f02\u5e38\u7684\u7c7b\u578b\u548c\u4f4d\u7f6e\u3002\u7136\u540e\uff0c\u5229\u7528\u201c\u81ea\u9002\u5e94\u591a\u8f6e\u5f02\u5e38\u805a\u7c7b\u201d\u5bf9\u5f02\u5e38\u6982\u5ff5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bed\u4e49\u805a\u7c7b\uff0c\u4ee5\u589e\u5f3a\u5f02\u5e38\u8868\u793a\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u201c\u533a\u57df\u5f15\u5bfc\u63a9\u6a21\u751f\u6210\u7b56\u7565\u201d\u786e\u4fdd\u4e86\u5f02\u5e38\u4e0e\u5176\u5bf9\u5e94\u63a9\u6a21\u4e4b\u95f4\u7684\u7cbe\u786e\u5bf9\u9f50\uff0c\u5e76\u5f15\u5165\u4e86\u201c\u4f4e\u8d28\u91cf\u6837\u672c\u8fc7\u6ee4\u6a21\u5757\u201d\u4ee5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u751f\u6210\u6837\u672c\u7684\u6574\u4f53\u8d28\u91cf\u3002", "result": "GAA\u6846\u67b6\u80fd\u591f\u751f\u6210\u903c\u771f\u3001\u591a\u6837\u5316\u4e14\u8bed\u4e49\u5bf9\u9f50\u7684\u5f02\u5e38\uff0c\u5e76\u80fd\u7cbe\u786e\u5bf9\u9f50\u5f02\u5e38\u4e0e\u63a9\u6a21\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u5728MVTec AD\u548cLOCO\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86GAA\u5728\u5f02\u5e38\u5408\u6210\u8d28\u91cf\u548c\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u5b9a\u4f4d\u548c\u5206\u7c7b\uff09\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "GAA\u5728MVTec AD\u548cLOCO\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u5f02\u5e38\u5408\u6210\u8d28\u91cf\u548c\u5b9a\u4f4d\u3001\u5206\u7c7b\u7b49\u4e0b\u6e38\u4efb\u52a1\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2507.09523", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09523", "abs": "https://arxiv.org/abs/2507.09523", "authors": ["Brett Daley", "Prabhat Nagarajan", "Martha White", "Marlos C. Machado"], "title": "An Analysis of Action-Value Temporal-Difference Methods That Learn State Values", "comment": "Published at RLC/RLJ 2025", "summary": "The hallmark feature of temporal-difference (TD) learning is bootstrapping:\nusing value predictions to generate new value predictions. The vast majority of\nTD methods for control learn a policy by bootstrapping from a single\naction-value function (e.g., Q-learning and Sarsa). Significantly less\nattention has been given to methods that bootstrap from two asymmetric value\nfunctions: i.e., methods that learn state values as an intermediate step in\nlearning action values. Existing algorithms in this vein can be categorized as\neither QV-learning or AV-learning. Though these algorithms have been\ninvestigated to some degree in prior work, it remains unclear if and when it is\nadvantageous to learn two value functions instead of just one -- and whether\nsuch approaches are theoretically sound in general. In this paper, we analyze\nthese algorithmic families in terms of convergence and sample efficiency. We\nfind that while both families are more efficient than Expected Sarsa in the\nprediction setting, only AV-learning methods offer any major benefit over\nQ-learning in the control setting. Finally, we introduce a new AV-learning\nalgorithm called Regularized Dueling Q-learning (RDQ), which significantly\noutperforms Dueling DQN in the MinAtar benchmark.", "AI": {"tldr": "TD \u5b66\u4e60\u901a\u5e38\u4f7f\u7528\u5355\u4e2a\u503c\u51fd\u6570\uff0c\u4f46\u6b64\u8bba\u6587\u7814\u7a76\u4e86\u4f7f\u7528\u4e24\u4e2a\u4e0d\u5bf9\u79f0\u503c\u51fd\u6570\u7684\u7b97\u6cd5\u3002\u7814\u7a76\u53d1\u73b0 AV-learning \u5728\u63a7\u5236\u65b9\u9762\u6bd4 Q-learning \u6709\u4f18\u52bf\uff0c\u5e76\u4e14\u65b0\u7b97\u6cd5 RDQ \u5728 MinAtar \u4e0a\u8868\u73b0\u4f18\u4e8e Dueling DQN\u3002", "motivation": "\u7814\u7a76 TD \u63a7\u5236\u65b9\u6cd5\u4e2d\u4ece\u4e24\u4e2a\u4e0d\u5bf9\u79f0\u503c\u51fd\u6570\u5f15\u5bfc\u7684 QV-learning \u548c AV-learning \u7b97\u6cd5\uff0c\u4ee5\u786e\u5b9a\u5176\u4f18\u52bf\u548c\u7406\u8bba\u57fa\u7840\u3002", "method": "\u5206\u6790\u4e86\u4ece\u4e24\u4e2a\u4e0d\u5bf9\u79f0\u503c\u51fd\u6570\u8fdb\u884c\u5f15\u5bfc\u7684 TD \u65b9\u6cd5\u7684\u6536\u655b\u6027\u548c\u6837\u672c\u6548\u7387\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684 AV-learning \u7b97\u6cd5 RDQ\u3002", "result": "QV-learning \u548c AV-learning \u65b9\u6cd5\u5728\u9884\u6d4b\u8bbe\u7f6e\u4e2d\u6bd4 Expected Sarsa \u66f4\u6709\u6548\u3002AV-learning \u65b9\u6cd5\u5728\u63a7\u5236\u8bbe\u7f6e\u4e2d\u6bd4 Q-learning \u5177\u6709\u4f18\u52bf\u3002", "conclusion": "AV-learning \u65b9\u6cd5\u5728\u63a7\u5236\u8bbe\u7f6e\u4e2d\u6bd4 Q-learning \u63d0\u4f9b\u4e86\u4e3b\u8981\u4f18\u52bf\uff0c\u5e76\u4e14 RDQ \u5728 MinAtar \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e Dueling DQN\u3002"}}
{"id": "2507.09630", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09630", "abs": "https://arxiv.org/abs/2507.09630", "authors": ["Shomukh Qari", "Maha A. Thafar"], "title": "Brain Stroke Detection and Classification Using CT Imaging with Transformer Models and Explainable AI", "comment": "5 figures", "summary": "Stroke is one of the leading causes of death globally, making early and\naccurate diagnosis essential for improving patient outcomes, particularly in\nemergency settings where timely intervention is critical. CT scans are the key\nimaging modality because of their speed, accessibility, and cost-effectiveness.\nThis study proposed an artificial intelligence framework for multiclass stroke\nclassification (ischemic, hemorrhagic, and no stroke) using CT scan images from\na dataset provided by the Republic of Turkey's Ministry of Health. The proposed\nmethod adopted MaxViT, a state-of-the-art Vision Transformer, as the primary\ndeep learning model for image-based stroke classification, with additional\ntransformer variants (vision transformer, transformer-in-transformer, and\nConvNext). To enhance model generalization and address class imbalance, we\napplied data augmentation techniques, including synthetic image generation. The\nMaxViT model trained with augmentation achieved the best performance, reaching\nan accuracy and F1-score of 98.00%, outperforming all other evaluated models\nand the baseline methods. The primary goal of this study was to distinguish\nbetween stroke types with high accuracy while addressing crucial issues of\ntransparency and trust in artificial intelligence models. To achieve this,\nExplainable Artificial Intelligence (XAI) was integrated into the framework,\nparticularly Grad-CAM++. It provides visual explanations of the model's\ndecisions by highlighting relevant stroke regions in the CT scans and\nestablishing an accurate, interpretable, and clinically applicable solution for\nearly stroke detection. This research contributed to the development of a\ntrustworthy AI-assisted diagnostic tool for stroke, facilitating its\nintegration into clinical practice and enhancing access to timely and optimal\nstroke diagnosis in emergency departments, thereby saving more lives.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528 MaxViT \u548c XAI \u6280\u672f\uff0c\u901a\u8fc7 CT \u56fe\u50cf\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u5352\u4e2d\u5206\u7c7b\uff08\u51c6\u786e\u7387\u548c F1 \u5206\u6570\u8fbe 98%\uff09\uff0c\u63d0\u9ad8\u4e86 AI \u5728\u4e34\u5e8a\u8bca\u65ad\u4e2d\u7684\u53ef\u4fe1\u5ea6\u548c\u5b9e\u7528\u6027\u3002", "motivation": "\u5352\u4e2d\u662f\u5168\u7403\u4e3b\u8981\u7684\u6b7b\u4ea1\u539f\u56e0\u4e4b\u4e00\uff0c\u56e0\u6b64\u65e9\u671f\u548c\u51c6\u786e\u7684\u8bca\u65ad\u5bf9\u4e8e\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u9ad8\u7cbe\u5ea6\u7684 AI \u6846\u67b6\u6765\u533a\u5206\u5352\u4e2d\u7c7b\u578b\uff0c\u540c\u65f6\u89e3\u51b3\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u5728\u900f\u660e\u5ea6\u548c\u53ef\u4fe1\u5ea6\u65b9\u9762\u7684\u95ee\u9898\uff0c\u4ee5\u671f\u63d0\u4f9b\u4e00\u4e2a\u503c\u5f97\u4fe1\u8d56\u7684 AI \u8f85\u52a9\u8bca\u65ad\u5de5\u5177\uff0c\u4fc3\u8fdb\u5176\u5728\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u7684\u5e94\u7528\uff0c\u5e76\u6700\u7ec8\u633d\u6551\u751f\u547d\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6846\u67b6\uff0c\u91c7\u7528\u5148\u8fdb\u7684 Vision Transformer \u6a21\u578b MaxViT\uff08\u4ee5\u53ca\u5176\u4ed6\u53d8\u4f53\u5982 Vision Transformer\u3001Transformer-in-Transformer \u548c ConvNext\uff09\u5bf9 CT \u626b\u63cf\u56fe\u50cf\u8fdb\u884c\u5352\u4e2d\u5206\u7c7b\u3002\u4e3a\u4e86\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u91c7\u7528\u4e86\u5305\u62ec\u5408\u6210\u56fe\u50cf\u751f\u6210\u5728\u5185\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\u3002\u6b64\u5916\uff0c\u96c6\u6210\u4e86\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u6280\u672f\uff0c\u7279\u522b\u662f Grad-CAM++\uff0c\u4ee5\u63d0\u4f9b\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89c6\u5316\u89e3\u91ca\u3002", "result": "\u6240\u63d0\u51fa\u7684 MaxViT \u6a21\u578b\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\u5728\u5352\u4e2d\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u6027\u80fd\uff0c\u51c6\u786e\u7387\u548c F1 \u5206\u6570\u8fbe\u5230\u4e86 98.00%\uff0c\u4f18\u4e8e\u6240\u6709\u5176\u4ed6\u8bc4\u4f30\u6a21\u578b\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002XAI\uff08Grad-CAM++\uff09\u7684\u96c6\u6210\u63d0\u4f9b\u4e86\u6a21\u578b\u51b3\u7b56\u7684\u53ef\u89c6\u5316\u89e3\u91ca\uff0c\u7a81\u51fa\u4e86 CT \u626b\u63cf\u4e2d\u4e0e\u5352\u4e2d\u76f8\u5173\u7684\u533a\u57df\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u901a\u8fc7 CT \u626b\u63cf\u56fe\u50cf\u8fdb\u884c\u591a\u7c7b\u522b\u5352\u4e2d\u5206\u7c7b\uff08\u7f3a\u8840\u6027\u3001\u51fa\u8840\u6027\u3001\u65e0\u5352\u4e2d\uff09\u3002MaxViT \u6a21\u578b\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\u5b9e\u73b0\u4e86 98.00% \u7684\u51c6\u786e\u7387\u548c F1 \u5206\u6570\uff0c\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u548c\u57fa\u7ebf\u65b9\u6cd5\u3002\u901a\u8fc7\u96c6\u6210 XAI\uff08\u7279\u522b\u662f Grad-CAM++\uff09\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u4e3a\u65e9\u671f\u5352\u4e2d\u68c0\u6d4b\u63d0\u4f9b\u4e86\u51c6\u786e\u3001\u53ef\u89e3\u91ca\u4e14\u4e34\u5e8a\u9002\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u52a9\u4e8e\u5c06 AI \u8f85\u52a9\u8bca\u65ad\u5de5\u5177\u96c6\u6210\u5230\u4e34\u5e8a\u5b9e\u8df5\u4e2d\uff0c\u4ece\u800c\u633d\u6551\u66f4\u591a\u751f\u547d\u3002"}}
{"id": "2507.10013", "categories": ["cs.CV", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10013", "abs": "https://arxiv.org/abs/2507.10013", "authors": ["Tom Kouwenhoven", "Kiana Shahrasbi", "Tessa Verhoef"], "title": "Cross-modal Associations in Vision and Language Models: Revisiting the bouba-kiki effect", "comment": null, "summary": "Recent advances in multimodal models have raised questions about whether\nvision-and-language models (VLMs) integrate cross-modal information in ways\nthat reflect human cognition. One well-studied test case in this domain is the\nbouba-kiki effect, where humans reliably associate pseudowords like \"bouba\"\nwith round shapes and \"kiki\" with jagged ones. Given the mixed evidence found\nin prior studies for this effect in VLMs, we present a comprehensive\nre-evaluation focused on two variants of CLIP, ResNet and Vision Transformer\n(ViT), given their centrality in many state-of-the-art VLMs. We apply two\ncomplementary methods closely modelled after human experiments: a prompt-based\nevaluation that uses probabilities as model preference, and we use Grad-CAM as\na novel way to interpret visual attention in shape-word matching tasks. Our\nfindings show that these models do not consistently exhibit the bouba-kiki\neffect. While ResNet shows a preference for round shapes, overall performance\nacross both models lacks the expected associations. Moreover, direct comparison\nwith prior human data on the same task shows that the models' responses fall\nmarkedly short of the robust, modality-integrated behaviour characteristic of\nhuman cognition. These results contribute to the ongoing debate about the\nextent to which VLMs truly understand cross-modal concepts, highlighting\nlimitations in their internal representations and alignment with human\nintuitions.", "AI": {"tldr": "This paper re-evaluates the bouba-kiki effect in VLMs (CLIP variants ResNet and ViT) using prompt-based evaluation and Grad-CAM. Findings indicate models do not consistently exhibit the effect, with performance falling short of human cognition, suggesting limitations in cross-modal understanding.", "motivation": "Recent advances in multimodal models have raised questions about whether vision-and-language models (VLMs) integrate cross-modal information in ways that reflect human cognition. One well-studied test case in this domain is the bouba-kiki effect, where humans reliably associate pseudowords like \"bouba\" with round shapes and \"kiki\" with jagged ones.", "method": "We apply two complementary methods closely modelled after human experiments: a prompt-based evaluation that uses probabilities as model preference, and we use Grad-CAM as a novel way to interpret visual attention in shape-word matching tasks.", "result": "Our findings show that these models do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both models lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition.", "conclusion": "Given the mixed evidence found in prior studies for this effect in VLMs, we present a comprehensive re-evaluation focused on two variants of CLIP, ResNet and Vision Transformer (ViT), given their centrality in many state-of-the-art VLMs. Our findings show that these models do not consistently exhibit the bouba-kiki effect. While ResNet shows a preference for round shapes, overall performance across both models lacks the expected associations. Moreover, direct comparison with prior human data on the same task shows that the models' responses fall markedly short of the robust, modality-integrated behaviour characteristic of human cognition. These results contribute to the ongoing debate about the extent to which VLMs truly understand cross-modal concepts, highlighting limitations in their internal representations and alignment with human intuitions."}}
{"id": "2507.09545", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09545", "abs": "https://arxiv.org/abs/2507.09545", "authors": ["Ilaria Vascotto", "Valentina Blasone", "Alex Rodriguez", "Alessandro Bonaita", "Luca Bortolussi"], "title": "Assessing reliability of explanations in unbalanced datasets: a use-case on the occurrence of frost events", "comment": "Late Breaking Work presented at the 3rd World Conference on\n  eXplainable Artificial Intelligence (XAI2025)", "summary": "The usage of eXplainable Artificial Intelligence (XAI) methods has become\nessential in practical applications, given the increasing deployment of\nArtificial Intelligence (AI) models and the legislative requirements put\nforward in the latest years. A fundamental but often underestimated aspect of\nthe explanations is their robustness, a key property that should be satisfied\nin order to trust the explanations. In this study, we provide some preliminary\ninsights on evaluating the reliability of explanations in the specific case of\nunbalanced datasets, which are very frequent in high-risk use-cases, but at the\nsame time considerably challenging for both AI models and XAI methods. We\npropose a simple evaluation focused on the minority class (i.e. the less\nfrequent one) that leverages on-manifold generation of neighbours, explanation\naggregation and a metric to test explanation consistency. We present a use-case\nbased on a tabular dataset with numerical features focusing on the occurrence\nof frost events.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9\u975e\u5e73\u8861\u6570\u636e\u96c6\u4e2d\u7684XAI\u89e3\u91ca\u53ef\u9760\u6027\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u6ce8\u5c11\u6570\u7c7b\u522b\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u5e76\u7ed9\u51fa\u4e86\u4e00\u4e2a\u5b9e\u9645\u7528\u4f8b\u3002", "motivation": "\u968f\u7740AI\u6a21\u578b\u90e8\u7f72\u7684\u589e\u52a0\u548c\u76f8\u5173\u6cd5\u5f8b\u6cd5\u89c4\u7684\u51fa\u73b0\uff0c\u89e3\u91ca\u7684\u9c81\u68d2\u6027\uff08\u5373\u53ef\u9760\u6027\uff09\u5df2\u6210\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2dXAI\u65b9\u6cd5\u7684\u4e00\u4e2a\u57fa\u672c\u4f46\u5e38\u88ab\u5ffd\u89c6\u7684\u65b9\u9762\u3002\u7279\u522b\u662f\u5728\u9ad8\u98ce\u9669\u7528\u4f8b\u4e2d\u5e38\u89c1\u7684\u975e\u5e73\u8861\u6570\u636e\u96c6\u7684\u60c5\u51b5\u4e0b\uff0c\u8bc4\u4f30XAI\u65b9\u6cd5\u7684\u53ef\u9760\u6027\u66f4\u5177\u6311\u6218\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6d41\u5f62\u4e0a\u90bb\u5c45\u751f\u6210\u3001\u89e3\u91ca\u805a\u5408\u548c\u89e3\u91ca\u4e00\u81f4\u6027\u5ea6\u91cf\u6765\u8bc4\u4f30\u5c11\u6570\u7c7b\u522b\u89e3\u91ca\u53ef\u9760\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8868\u683c\u6570\u636e\u96c6\uff08\u5305\u542b\u6570\u503c\u7279\u5f81\u5e76\u5173\u6ce8\u971c\u51bb\u4e8b\u4ef6\u7684\u53d1\u751f\uff09\u7684\u7528\u4f8b\uff0c\u4ee5\u5c55\u793a\u6240\u63d0\u51fa\u7684\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u9488\u5bf9\u975e\u5e73\u8861\u6570\u636e\u96c6\u7684XAI\u65b9\u6cd5\u53ef\u9760\u6027\u8bc4\u4f30\u7684\u521d\u6b65\u89c1\u89e3\uff0c\u91cd\u70b9\u5173\u6ce8\u5c11\u6570\u7c7b\u522b\u7684\u89e3\u91ca\u4e00\u81f4\u6027\u3002"}}
{"id": "2507.09640", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09640", "abs": "https://arxiv.org/abs/2507.09640", "authors": ["Leonor Fernandes", "Tiago Gon\u00e7alves", "Jo\u00e3o Matos", "Luis Filipe Nakayama", "Jaime S. Cardoso"], "title": "Disentanglement and Assessment of Shortcuts in Ophthalmological Retinal Imaging Exams", "comment": "10 pages. Under review", "summary": "Diabetic retinopathy (DR) is a leading cause of vision loss in working-age\nadults. While screening reduces the risk of blindness, traditional imaging is\noften costly and inaccessible. Artificial intelligence (AI) algorithms present\na scalable diagnostic solution, but concerns regarding fairness and\ngeneralization persist. This work evaluates the fairness and performance of\nimage-trained models in DR prediction, as well as the impact of disentanglement\nas a bias mitigation technique, using the diverse mBRSET fundus dataset. Three\nmodels, ConvNeXt V2, DINOv2, and Swin V2, were trained on macula images to\npredict DR and sensitive attributes (SAs) (e.g., age and gender/sex). Fairness\nwas assessed between subgroups of SAs, and disentanglement was applied to\nreduce bias. All models achieved high DR prediction performance in diagnosing\n(up to 94% AUROC) and could reasonably predict age and gender/sex (91% and 77%\nAUROC, respectively). Fairness assessment suggests disparities, such as a 10%\nAUROC gap between age groups in DINOv2. Disentangling SAs from DR prediction\nhad varying results, depending on the model selected. Disentanglement improved\nDINOv2 performance (2% AUROC gain), but led to performance drops in ConvNeXt V2\nand Swin V2 (7% and 3%, respectively). These findings highlight the complexity\nof disentangling fine-grained features in fundus imaging and emphasize the\nimportance of fairness in medical imaging AI to ensure equitable and reliable\nhealthcare solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd AI \u6a21\u578b\uff08ConvNeXt V2\u3001DINOv2\u3001Swin V2\uff09\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u8bca\u65ad\u4e2d\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u5c3d\u7ba1\u6a21\u578b\u5728 DR \u9884\u6d4b\u548c\u654f\u611f\u5c5e\u6027\u9884\u6d4b\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5b58\u5728\u516c\u5e73\u6027\u5dee\u5f02\u3002\u89e3\u7ea0\u7f20\u6280\u672f\u5bf9\u6a21\u578b\u516c\u5e73\u6027\u7684\u5f71\u54cd\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u4e3a\u533b\u5b66\u6210\u50cf AI \u7684\u516c\u5e73\u6027\u5e94\u7528\u5e26\u6765\u4e86\u6311\u6218\u3002", "motivation": "\u4f20\u7edf\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u7b5b\u67e5\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u666e\u53ca\u3002\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7b97\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u8bca\u65ad\u89e3\u51b3\u65b9\u6848\u51fa\u73b0\uff0c\u4f46\u5176\u516c\u5e73\u6027\u548c\u6cdb\u5316\u6027\u4ee4\u4eba\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30 AI \u6a21\u578b\u5728 DR \u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd\uff0c\u5e76\u7814\u7a76\u89e3\u7ea0\u7f20\u4f5c\u4e3a\u4e00\u79cd\u504f\u89c1\u7f13\u89e3\u6280\u672f\u7684\u5f71\u54cd\u3002", "method": "\u4f7f\u7528\u591a\u80cc\u666f\u89c6\u7f51\u819c\u8840\u7ba1\u663e\u50cf\u6570\u636e\u96c6\uff08mBRSET\uff09\uff0c\u8bc4\u4f30\u4e86\u5728\u89c6\u7f51\u819c\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u56fe\u50cf\u6a21\u578b\u5728\u7cd6\u5c3f\u75c5\u89c6\u7f51\u819c\u75c5\u53d8\uff08DR\uff09\u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u548c\u6027\u80fd\uff0c\u4ee5\u53ca\u89e3\u7ea0\u7f20\u4f5c\u4e3a\u4e00\u79cd\u504f\u89c1\u7f13\u89e3\u6280\u672f\u7684\u5f71\u54cd\u3002\u8bad\u7ec3\u4e86\u4e09\u79cd\u6a21\u578b\uff08ConvNeXt V2\u3001DINOv2 \u548c Swin V2\uff09\u6765\u9884\u6d4b DR \u548c\u654f\u611f\u5c5e\u6027\uff08\u5982\u5e74\u9f84\u548c\u6027\u522b\uff09\uff0c\u5e76\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u4e9a\u7ec4\u4e4b\u95f4\u7684\u516c\u5e73\u6027\u3002", "result": "\u6240\u6709\u6a21\u578b\u5728 DR \u9884\u6d4b\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u8f83\u9ad8\u7684\u6027\u80fd\uff08AUROC \u9ad8\u8fbe 94%\uff09\uff0c\u5e76\u4e14\u5728\u9884\u6d4b\u5e74\u9f84\u548c\u6027\u522b\u65b9\u9762\u4e5f\u8868\u73b0\u826f\u597d\uff08AUROC \u5206\u522b\u4e3a 91% \u548c 77%\uff09\u3002\u516c\u5e73\u6027\u8bc4\u4f30\u663e\u793a\u5b58\u5728\u5dee\u5f02\uff0c\u4f8b\u5982 DINOv2 \u6a21\u578b\u5728\u4e0d\u540c\u5e74\u9f84\u7ec4\u4e4b\u95f4\u5b58\u5728 10% \u7684 AUROC \u5dee\u8ddd\u3002\u89e3\u7ea0\u7f20\u654f\u611f\u5c5e\u6027\u5bf9 DR \u9884\u6d4b\u7684\u5f71\u54cd\u56e0\u6a21\u578b\u800c\u5f02\uff0c\u5176\u4e2d DINOv2 \u6a21\u578b\u7684\u6027\u80fd\u6709\u6240\u63d0\u9ad8\uff08AUROC \u63d0\u5347 2%\uff09\uff0c\u800c ConvNeXt V2 \u548c Swin V2 \u6a21\u578b\u7684\u6027\u80fd\u5219\u6709\u6240\u4e0b\u964d\uff08\u5206\u522b\u4e0b\u964d 7% \u548c 3%\uff09\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u533b\u5b66\u6210\u50cf\u4eba\u5de5\u667a\u80fd\u4e2d\u89e3\u51b3\u516c\u5e73\u6027\u95ee\u9898\u7684\u590d\u6742\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u786e\u4fdd\u516c\u5e73\u53ef\u9760\u7684\u533b\u7597\u4fdd\u5065\u89e3\u51b3\u65b9\u6848\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2507.09565", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09565", "abs": "https://arxiv.org/abs/2507.09565", "authors": ["Heeba Shakeel", "Tanvir Ahmad", "Chandni Saxena"], "title": "Holistix: A Dataset for Holistic Wellness Dimensions Analysis in Mental Health Narratives", "comment": "7 Pages", "summary": "We introduce a dataset for classifying wellness dimensions in social media\nuser posts, covering six key aspects: physical, emotional, social,\nintellectual, spiritual, and vocational. The dataset is designed to capture\nthese dimensions in user-generated content, with a comprehensive annotation\nframework developed under the guidance of domain experts. This framework allows\nfor the classification of text spans into the appropriate wellness categories.\nWe evaluate both traditional machine learning models and advanced\ntransformer-based models for this multi-class classification task, with\nperformance assessed using precision, recall, and F1-score, averaged over\n10-fold cross-validation. Post-hoc explanations are applied to ensure the\ntransparency and interpretability of model decisions. The proposed dataset\ncontributes to region-specific wellness assessments in social media and paves\nthe way for personalized well-being evaluations and early intervention\nstrategies in mental health. We adhere to ethical considerations for\nconstructing and releasing our experiments and dataset publicly on Github.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u516d\u4e2a\u5065\u5eb7\u7ef4\u5ea6\u7684\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u5e76\u8bc4\u4f30\u4e86\u4f20\u7edf\u548cTransformer\u6a21\u578b\u3002\u7ed3\u679c\u8868\u660e\u8be5\u6570\u636e\u96c6\u6709\u52a9\u4e8e\u5065\u5eb7\u8bc4\u4f30\u548c\u65e9\u671f\u5e72\u9884\u7b56\u7565\u3002", "motivation": "\u4ecb\u7ecd\u4e00\u4e2a\u7528\u4e8e\u5728\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u5e16\u5b50\u4e2d\u5bf9\u5065\u5eb7\u7ef4\u5ea6\u8fdb\u884c\u5206\u7c7b\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u516d\u4e2a\u5173\u952e\u7684\u5065\u5eb7\u65b9\u9762\uff1a\u8eab\u4f53\u3001\u60c5\u7eea\u3001\u793e\u4ea4\u3001\u667a\u529b\u3001\u7cbe\u795e\u548c\u804c\u4e1a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u5728\u793e\u4ea4\u5a92\u4f53\u7528\u6237\u5e16\u5b50\u4e2d\u5bf9\u5065\u5eb7\u7ef4\u5ea6\u8fdb\u884c\u5206\u7c7b\u7684\u6570\u636e\u96c6\uff0c\u6db5\u76d6\u4e86\u516d\u4e2a\u5173\u952e\u65b9\u9762\uff1a\u8eab\u4f53\u3001\u60c5\u7eea\u3001\u793e\u4ea4\u3001\u667a\u529b\u3001\u7cbe\u795e\u548c\u804c\u4e1a\u3002\u8be5\u6570\u636e\u96c6\u65e8\u5728\u6355\u6349\u7528\u6237\u751f\u6210\u5185\u5bb9\u4e2d\u7684\u8fd9\u4e9b\u7ef4\u5ea6\uff0c\u5e76\u5efa\u7acb\u4e86\u4e00\u4e2a\u5728\u9886\u57df\u4e13\u5bb6\u6307\u5bfc\u4e0b\u5f00\u53d1\u7684\u7efc\u5408\u6ce8\u91ca\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u5c06\u6587\u672c\u8de8\u5ea6\u5206\u7c7b\u5230\u9002\u5f53\u7684\u5065\u5eb7\u7c7b\u522b\u4e2d\u3002\u6211\u4eec\u8fd8\u8bc4\u4f30\u4e86\u7528\u4e8e\u6b64\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u7684\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u548c\u5148\u8fdb\u7684Transformer\u6a21\u578b\uff0c\u5e76\u4f7f\u7528\u7cbe\u786e\u7387\u3001\u53ec\u56de\u7387\u548cF1\u5206\u6570\u901a\u8fc710\u500d\u4ea4\u53c9\u9a8c\u8bc1\u7684\u5e73\u5747\u503c\u6765\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5065\u5eb7\u8bc4\u4f30\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u4e3a\u4e2a\u6027\u5316\u7684\u798f\u7949\u8bc4\u4f30\u548c\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u65e9\u671f\u5e72\u9884\u7b56\u7565\u94fa\u5e73\u4e86\u9053\u8def\u3002\u7814\u7a76\u4e2d\u8fd8\u91c7\u7528\u4e86\u4e8b\u540e\u89e3\u91ca\u4ee5\u786e\u4fdd\u6a21\u578b\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u548c\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u6570\u636e\u96c6\u4e3a\u793e\u4ea4\u5a92\u4f53\u4e0a\u7684\u5065\u5eb7\u8bc4\u4f30\u505a\u51fa\u4e86\u8d21\u732e\uff0c\u5e76\u4e3a\u4e2a\u6027\u5316\u7684\u798f\u7949\u8bc4\u4f30\u548c\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u7684\u65e9\u671f\u5e72\u9884\u7b56\u7565\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09649", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09649", "abs": "https://arxiv.org/abs/2507.09649", "authors": ["Zhengyuan Peng", "Jianqing Xu", "Shen Li", "Jiazhen Ji", "Yuge Huang", "Jingyun Zhang", "Jinmin Li", "Shouhong Ding", "Rizen Guo", "Xin Tan", "Lizhuang Ma"], "title": "EyeSeg: An Uncertainty-Aware Eye Segmentation Framework for AR/VR", "comment": "Accepted to IJCAI", "summary": "Human-machine interaction through augmented reality (AR) and virtual reality\n(VR) is increasingly prevalent, requiring accurate and efficient gaze\nestimation which hinges on the accuracy of eye segmentation to enable smooth\nuser experiences. We introduce EyeSeg, a novel eye segmentation framework\ndesigned to overcome key challenges that existing approaches struggle with:\nmotion blur, eyelid occlusion, and train-test domain gaps. In these situations,\nexisting models struggle to extract robust features, leading to suboptimal\nperformance. Noting that these challenges can be generally quantified by\nuncertainty, we design EyeSeg as an uncertainty-aware eye segmentation\nframework for AR/VR wherein we explicitly model the uncertainties by performing\nBayesian uncertainty learning of a posterior under the closed set prior.\nTheoretically, we prove that a statistic of the learned posterior indicates\nsegmentation uncertainty levels and empirically outperforms existing methods in\ndownstream tasks, such as gaze estimation. EyeSeg outputs an uncertainty score\nand the segmentation result, weighting and fusing multiple gaze estimates for\nrobustness, which proves to be effective especially under motion blur, eyelid\nocclusion and cross-domain challenges. Moreover, empirical results suggest that\nEyeSeg achieves segmentation improvements of MIoU, E1, F1, and ACC surpassing\nprevious approaches. The code is publicly available at\nhttps://github.com/JethroPeng/EyeSeg.", "AI": {"tldr": "EyeSeg\u662f\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u773c\u90e8\u5206\u5272\u6846\u67b6\uff0c\u7528\u4e8eAR/VR\uff0c\u53ef\u4ee5\u5904\u7406\u8fd0\u52a8\u6a21\u7cca\u3001\u773c\u7751\u906e\u6321\u548c\u57df\u5dee\u8ddd\u7b49\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u8fd0\u52a8\u6a21\u7cca\u3001\u773c\u7751\u906e\u6321\u548c\u8bad\u7ec3-\u6d4b\u8bd5\u57df\u5dee\u8ddd\u7b49\u6311\u6218\u65f6\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u4ece\u800c\u5b9e\u73b0\u6d41\u7545\u7684\u7528\u6237\u4f53\u9a8c\u3002", "method": "\u901a\u8fc7\u8fdb\u884c\u8d1d\u53f6\u65af\u4e0d\u786e\u5b9a\u6027\u5b66\u4e60\u6765\u663e\u5f0f\u5730\u5bf9\u4e0d\u786e\u5b9a\u6027\u8fdb\u884c\u5efa\u6a21\uff0c\u5c06EyeSeg\u8bbe\u8ba1\u4e3a\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u7684\u773c\u90e8\u5206\u5272\u6846\u67b6\u3002", "result": "EyeSeg\u8f93\u51fa\u4e00\u4e2a\u4e0d\u786e\u5b9a\u6027\u5206\u6570\u548c\u5206\u5272\u7ed3\u679c\uff0c\u5bf9\u591a\u4e2a\u6ce8\u89c6\u4f30\u8ba1\u8fdb\u884c\u52a0\u6743\u548c\u878d\u5408\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u5728\u8fd0\u52a8\u6a21\u7cca\u3001\u773c\u7751\u906e\u6321\u548c\u8de8\u57df\u6311\u6218\u4e0b\u8868\u73b0\u6709\u6548\u3002", "conclusion": "EyeSeg\u6846\u67b6\u5728\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u6ce8\u89c6\u4f30\u8ba1\uff09\u4e2d\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728MIoU\u3001E1\u3001F1\u548cACC\u65b9\u9762\u53d6\u5f97\u4e86\u5206\u5272\u6539\u8fdb\u3002"}}
{"id": "2507.10300", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10300", "abs": "https://arxiv.org/abs/2507.10300", "authors": ["Hatef Otroshi Shahreza", "S\u00e9bastien Marcel"], "title": "FaceLLM: A Multimodal Large Language Model for Face Understanding", "comment": "Accepted in ICCV 2025 workshops", "summary": "Multimodal large language models (MLLMs) have shown remarkable performance in\nvision-language tasks. However, existing MLLMs are primarily trained on generic\ndatasets, limiting their ability to reason on domain-specific visual cues such\nas those in facial images. In particular, tasks that require detailed\nunderstanding of facial structure, expression, emotion, and demographic\nfeatures remain underexplored by MLLMs due to the lack of large-scale annotated\nface image-text datasets. In this work, we introduce FaceLLM, a multimodal\nlarge language model trained specifically for facial image understanding. To\nconstruct the training data, we propose a novel weakly supervised pipeline that\nuses ChatGPT with attribute-aware prompts to generate high-quality\nquestion-answer pairs based on images from the FairFace dataset. The resulting\ncorpus, called FairFaceGPT, covers a diverse set of attributes including\nexpression, pose, skin texture, and forensic information. Our experiments\ndemonstrate that FaceLLM improves the performance of MLLMs on various\nface-centric tasks and achieves state-of-the-art performance. This work\nhighlights the potential of synthetic supervision via language models for\nbuilding domain-specialized MLLMs, and sets a precedent for trustworthy,\nhuman-centric multimodal AI systems. FairFaceGPT dataset and pretrained FaceLLM\nmodels are publicly available in the project page.", "AI": {"tldr": "FaceLLM \u662f\u4e00\u4e2a\u4e13\u95e8\u4e3a\u9762\u90e8\u56fe\u50cf\u7406\u89e3\u8bad\u7ec3\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u5b83\u5229\u7528 ChatGPT \u751f\u6210\u7684 FairFaceGPT \u6570\u636e\u96c6\uff0c\u5728\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff0c\u5e76\u516c\u5f00\u4e86\u6a21\u578b\u548c\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684 MLLMs \u4e3b\u8981\u5728\u901a\u7528\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u4eba\u8138\u56fe\u50cf\u7b49\u9886\u57df\u7279\u5b9a\u89c6\u89c9\u7ebf\u7d22\u4e0a\u8fdb\u884c\u63a8\u7406\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u90a3\u4e9b\u9700\u8981\u8be6\u7ec6\u7406\u89e3\u9762\u90e8\u7ed3\u6784\u3001\u8868\u60c5\u3001\u60c5\u7eea\u548c\u4eba\u53e3\u7edf\u8ba1\u7279\u5f81\u7684\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u65b9\u6cd5\uff0c\u5229\u7528 ChatGPT \u548c\u9762\u5411\u5c5e\u6027\u7684\u63d0\u793a\uff0c\u57fa\u4e8e FairFace \u6570\u636e\u96c6\u4e2d\u7684\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u95ee\u7b54\u5bf9\uff0c\u4ece\u800c\u6784\u5efa\u4e86 FaceLLM \u7684\u8bad\u7ec3\u6570\u636e\u3002", "result": "FaceLLM \u63d0\u9ad8\u4e86 MLLMs \u5728\u5404\u79cd\u4ee5\u4eba\u8138\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u7a81\u51fa\u4e86\u901a\u8fc7\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u5408\u6210\u76d1\u7763\u5728\u6784\u5efa\u9886\u57df\u4e13\u4e1a\u5316MLLM\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u503c\u5f97\u4fe1\u8d56\u7684\u3001\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u3001\u591a\u6a21\u6001\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u6811\u7acb\u4e86\u5148\u4f8b\u3002FaceLLM \u548c FairFaceGPT \u6570\u636e\u96c6\u5df2\u5728\u9879\u76ee\u9875\u9762\u516c\u5f00\u3002"}}
{"id": "2507.09602", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09602", "abs": "https://arxiv.org/abs/2507.09602", "authors": ["Bocheng Ju", "Junchao Fan", "Jiaqi Liu", "Xiaolin Chang"], "title": "DRAGD: A Federated Unlearning Data Reconstruction Attack Based on Gradient Differences", "comment": null, "summary": "Federated learning enables collaborative machine learning while preserving\ndata privacy. However, the rise of federated unlearning, designed to allow\nclients to erase their data from the global model, introduces new privacy\nconcerns. Specifically, the gradient exchanges during the unlearning process\ncan leak sensitive information about deleted data. In this paper, we introduce\nDRAGD, a novel attack that exploits gradient discrepancies before and after\nunlearning to reconstruct forgotten data. We also present DRAGDP, an enhanced\nversion of DRAGD that leverages publicly available prior data to improve\nreconstruction accuracy, particularly for complex datasets like facial images.\nExtensive experiments across multiple datasets demonstrate that DRAGD and\nDRAGDP significantly outperform existing methods in data reconstruction.Our\nwork highlights a critical privacy vulnerability in federated unlearning and\noffers a practical solution, advancing the security of federated unlearning\nsystems in real-world applications.", "AI": {"tldr": "\u8054\u90a6\u9057\u5fd8\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0cDRAGD/DRAGDP\u653b\u51fb\u53ef\u91cd\u5efa\u5df2\u5220\u9664\u6570\u636e\u3002", "motivation": "\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8054\u90a6\u9057\u5fd8\uff08federated unlearning\uff09\u673a\u5236\u867d\u7136\u5141\u8bb8\u7528\u6237\u5220\u9664\u5176\u6570\u636e\uff0c\u4f46\u68af\u5ea6\u4ea4\u6362\u8fc7\u7a0b\u53ef\u80fd\u6cc4\u9732\u88ab\u5220\u9664\u6570\u636e\u7684\u654f\u611f\u4fe1\u606f\uff0c\u5f15\u53d1\u65b0\u7684\u9690\u79c1\u62c5\u5fe7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDRAGD\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u9057\u5fd8\u524d\u540e\u68af\u5ea6\u5dee\u5f02\u6765\u91cd\u5efa\u6570\u636e\u3002\u540c\u65f6\uff0c\u8fd8\u63d0\u51fa\u4e86DRAGDP\u7684\u589e\u5f3a\u7248\u672c\uff0c\u8be5\u7248\u672c\u5229\u7528\u516c\u5f00\u7684\u5148\u9a8c\u6570\u636e\u6765\u63d0\u9ad8\u91cd\u5efa\u7cbe\u5ea6\uff0c\u7279\u522b\u9002\u7528\u4e8e\u4eba\u8138\u56fe\u50cf\u7b49\u590d\u6742\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cDRAGD\u548cDRAGDP\u5728\u6570\u636e\u91cd\u5efa\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c24\u5176\u5728\u5904\u7406\u4eba\u8138\u56fe\u50cf\u7b49\u590d\u6742\u6570\u636e\u96c6\u65f6\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u8054\u90a6\u5b66\u4e60\u9057\u5fd8\u673a\u5236\u4e2d\u7684\u5173\u952e\u9690\u79c1\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63d0\u9ad8\u4e86\u8054\u90a6\u9057\u5fd8\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002"}}
{"id": "2507.09672", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09672", "abs": "https://arxiv.org/abs/2507.09672", "authors": ["Xinyu Zhang", "Zhonghao Ye", "Jingwei Zhang", "Xiang Tian", "Zhisheng Liang", "Shipeng Yu"], "title": "VST-Pose: A Velocity-Integrated Spatiotem-poral Attention Network for Human WiFi Pose Estimation", "comment": "8 pages, 7 figures, 8 tables. WiFi CSI, VST-Pose framework +\n  ViSTA-Former dual-stream attention backbone. Code:\n  https://github.com/CarmenQing/VST-Pose", "summary": "WiFi-based human pose estimation has emerged as a promising non-visual\nalternative approaches due to its pene-trability and privacy advantages. This\npaper presents VST-Pose, a novel deep learning framework for accurate and\ncontinuous pose estimation using WiFi channel state information. The proposed\nmethod introduces ViSTA-Former, a spatiotemporal attention backbone with\ndual-stream architecture that adopts a dual-stream architecture to separately\ncapture temporal dependencies and structural relationships among body joints.\nTo enhance sensitivity to subtle human motions, a velocity modeling branch is\nintegrated into the framework, which learns short-term keypoint dis-placement\npatterns and improves fine-grained motion representation. We construct a 2D\npose dataset specifically designed for smart home care scenarios and\ndemonstrate that our method achieves 92.2% accuracy on the PCK@50 metric,\noutperforming existing methods by 8.3% in PCK@50 on the self-collected dataset.\nFurther evaluation on the public MMFi dataset confirms the model's robustness\nand effectiveness in 3D pose estimation tasks. The proposed system provides a\nreliable and privacy-aware solution for continuous human motion analysis in\nindoor environments. Our codes are available in\nhttps://github.com/CarmenQing/VST-Pose.", "AI": {"tldr": "VST-Pose\u5229\u7528WiFi\u4fe1\u53f7\u8fdb\u884c\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\uff0c\u901a\u8fc7ViSTA-Former\u548c\u901f\u5ea6\u5efa\u6a21\u5206\u652f\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u5bf9\u7ec6\u5fae\u52a8\u4f5c\u7684\u654f\u611f\u5ea6\uff0c\u57282D\u548c3D\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u975e\u89c6\u89c9\u65b9\u6cd5\u5728\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5229\u7528WiFi\u4fe1\u53f7\u7684\u7a7f\u900f\u6027\u548c\u9690\u79c1\u4f18\u52bf\uff0c\u63d0\u51faVST-Pose\u6846\u67b6\u6765\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u8fde\u7eed\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aViSTA-Former\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\uff0c\u5b83\u5177\u6709\u53cc\u6d41\u7ed3\u6784\uff0c\u80fd\u591f\u5206\u522b\u6355\u6349\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u8eab\u4f53\u5173\u8282\u4e4b\u95f4\u7684\u7ed3\u6784\u5173\u7cfb\u3002\u6b64\u5916\uff0c\u8fd8\u6574\u5408\u4e86\u4e00\u4e2a\u901f\u5ea6\u5efa\u6a21\u5206\u652f\uff0c\u7528\u4e8e\u5b66\u4e60\u5173\u952e\u70b9\u4f4d\u79fb\u6a21\u5f0f\u4ee5\u6539\u5584\u7ec6\u7c92\u5ea6\u8fd0\u52a8\u8868\u793a\u3002", "result": "\u5728\u667a\u80fd\u5bb6\u5c45\u62a4\u7406\u573a\u666f\u76842D\u59ff\u6001\u6570\u636e\u96c6\u4e0a\uff0cVST-Pose\u8fbe\u5230\u4e8692.2%\u7684PCK@50\u51c6\u786e\u7387\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e868.3%\u3002\u5728MMFi\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u4e5f\u8bc1\u660e\u4e86\u5176\u57283D\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "VST-Pose\u662f\u4e00\u4e2a\u7528\u4e8eWiFi\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u7684\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u3002\u5b83\u901a\u8fc7ViSTA-Former\uff08\u4e00\u79cd\u5177\u6709\u53cc\u6d41\u7ed3\u6784\u7684\u65f6\u7a7a\u6ce8\u610f\u529b\u9aa8\u5e72\u7f51\uff09\u6765\u6355\u6349\u8eab\u4f53\u5173\u8282\u4e4b\u95f4\u7684\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u7ed3\u6784\u5173\u7cfb\u3002\u8be5\u6846\u67b6\u8fd8\u6574\u5408\u4e86\u4e00\u4e2a\u901f\u5ea6\u5efa\u6a21\u5206\u652f\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u7ec6\u5fae\u4eba\u4f53\u8fd0\u52a8\u7684\u654f\u611f\u5ea6\u3002\u5728\u667a\u80fd\u5bb6\u5c45\u62a4\u7406\u573a\u666f\u76842D\u59ff\u6001\u6570\u636e\u96c6\u4e0a\uff0cVST-Pose\u8fbe\u5230\u4e8692.2%\u7684PCK@50\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728MMFi\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u5176\u57283D\u59ff\u6001\u4f30\u8ba1\u4efb\u52a1\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002\u8be5\u7cfb\u7edf\u4e3a\u5ba4\u5185\u73af\u5883\u4e2d\u7684\u8fde\u7eed\u4eba\u4f53\u8fd0\u52a8\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u9760\u4e14\u6ce8\u91cd\u9690\u79c1\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10398", "categories": ["cs.CV", "cs.AI", "cs.CL", "14J60", "I.2.7; I.4; I.5; I.7.5"], "pdf": "https://arxiv.org/pdf/2507.10398", "abs": "https://arxiv.org/abs/2507.10398", "authors": ["Diksha Mehta", "Prateek Mehta"], "title": "Devanagari Handwritten Character Recognition using Convolutional Neural Network", "comment": "9 pages, 6 figures", "summary": "Handwritten character recognition is getting popular among researchers\nbecause of its possible applications in facilitating technological search\nengines, social media, recommender systems, etc. The Devanagari script is one\nof the oldest language scripts in India that does not have proper digitization\ntools. With the advancement of computing and technology, the task of this\nresearch is to extract handwritten Hindi characters from an image of Devanagari\nscript with an automated approach to save time and obsolete data. In this\npaper, we present a technique to recognize handwritten Devanagari characters\nusing two deep convolutional neural network layers. This work employs a\nmethodology that is useful to enhance the recognition rate and configures a\nconvolutional neural network for effective Devanagari handwritten text\nrecognition (DHTR). This approach uses the Devanagari handwritten character\ndataset (DHCD), an open dataset with 36 classes of Devanagari characters. Each\nof these classes has 1700 images for training and testing purposes. This\napproach obtains promising results in terms of accuracy by achieving 96.36%\naccuracy in testing and 99.55% in training time.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u8bc6\u522b\u624b\u5199\u68b5\u6587\u5370\u5730\u8bed\u5b57\u7b26\u7684\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u9ad8\u3002", "motivation": "\u7531\u4e8e\u624b\u5199\u5b57\u7b26\u8bc6\u522b\u5728\u6280\u672f\u641c\u7d22\u5f15\u64ce\u3001\u793e\u4ea4\u5a92\u4f53\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u5177\u6709\u6f5c\u5728\u5e94\u7528\u524d\u666f\uff0c\u5e76\u4e14\u5370\u5ea6\u68b5\u6587\u662f\u4e00\u79cd\u53e4\u8001\u7684\u5370\u5ea6\u8bed\u8a00\u6587\u5b57\uff0c\u76ee\u524d\u7f3a\u4e4f\u6709\u6548\u7684\u6570\u5b57\u5316\u5de5\u5177\uff0c\u56e0\u6b64\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u65b9\u6cd5\u63d0\u53d6\u624b\u5199\u68b5\u6587\u5370\u5730\u8bed\u5b57\u7b26\uff0c\u4ee5\u8282\u7701\u65f6\u95f4\u548c\u5904\u7406\u8fc7\u65f6\u6570\u636e\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u914d\u7f6e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u68b5\u6587\u624b\u5199\u5b57\u7b26\u6570\u636e\u96c6\uff08DHCD\uff09\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fbe\u5230\u4e8696.36%\u7684\u51c6\u786e\u7387\uff0c\u5728\u8bad\u7ec3\u96c6\u4e0a\u8fbe\u5230\u4e8699.55%\u7684\u51c6\u786e\u7387\uff0c\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u4e24\u4e2a\u6df1\u5ea6\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u5c42\u6765\u8bc6\u522b\u624b\u5199\u68b5\u6587\u5370\u5730\u8bed\u5b57\u7b26\u7684\u6280\u672f\uff0c\u5e76\u53d6\u5f97\u4e86\u826f\u597d\u7684\u8bc6\u522b\u7387\u3002"}}
{"id": "2507.09616", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09616", "abs": "https://arxiv.org/abs/2507.09616", "authors": ["Ofir Gordon", "Ariel Lapid", "Elad Cohen", "Yarden Yagil", "Arnon Netzer", "Hai Victor Habi"], "title": "MLoRQ: Bridging Low-Rank and Quantization for Transformer Compression", "comment": null, "summary": "Deploying transformer-based neural networks on resource-constrained edge\ndevices presents a significant challenge. This challenge is often addressed\nthrough various techniques, such as low-rank approximation and mixed-precision\nquantization. In this work, we introduce Mixed Low-Rank and Quantization\n(MLoRQ), a novel method that integrates both techniques. MLoRQ employs a\ntwo-stage optimization process to determine optimal bit-width and rank\nassignments for each layer, adhering to predefined memory constraints. This\nprocess includes: (i) an intra-layer optimization that identifies potentially\noptimal compression solutions out of all low-rank and quantization\ncombinations; (ii) an inter-layer optimization that assigns bit-width precision\nand rank to each layer while ensuring the memory constraint is met. An optional\nfinal step applies a sequential optimization process using a modified adaptive\nrounding technique to mitigate compression-induced errors in joint low-rank\napproximation and quantization. The method is compatible and can be seamlessly\nintegrated with most existing quantization algorithms. MLoRQ shows\nstate-of-the-art results with up to 15\\% performance improvement, evaluated on\nVision Transformers for image classification, object detection, and instance\nsegmentation tasks.", "AI": {"tldr": "MLoRQ \u662f\u4e00\u79cd\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u6765\u538b\u7f29\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684 Transformer \u6a21\u578b\uff0c\u53ef\u63d0\u9ad8\u6027\u80fd\u5e76\u6ee1\u8db3\u5185\u5b58\u9650\u5236\u3002", "motivation": "\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u90e8\u7f72\u57fa\u4e8e Transformer \u7684\u795e\u7ecf\u7f51\u7edc\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u901a\u5e38\u901a\u8fc7\u4f4e\u79e9\u8fd1\u4f3c\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u7b49\u6280\u672f\u6765\u89e3\u51b3\u3002", "method": "MLoRQ \u91c7\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u8fc7\u7a0b\u6765\u786e\u5b9a\u6bcf\u4e2a\u5c42\u7684\u6700\u4f73\u6bd4\u7279\u5bbd\u5ea6\u548c\u79e9\u5206\u914d\uff0c\u4ee5\u6ee1\u8db3\u9884\u5b9a\u4e49\u7684\u5185\u5b58\u9650\u5236\u3002\u7b2c\u4e00\u9636\u6bb5\u662f\u5c42\u5185\u4f18\u5316\uff0c\u5bfb\u627e\u6240\u6709\u4f4e\u79e9\u548c\u91cf\u5316\u7ec4\u5408\u7684\u6f5c\u5728\u6700\u4f18\u538b\u7f29\u65b9\u6848\uff1b\u7b2c\u4e8c\u9636\u6bb5\u662f\u5c42\u95f4\u4f18\u5316\uff0c\u5728\u6ee1\u8db3\u5185\u5b58\u9650\u5236\u7684\u540c\u65f6\u4e3a\u6bcf\u4e2a\u5c42\u5206\u914d\u6bd4\u7279\u5bbd\u5ea6\u7cbe\u5ea6\u548c\u79e9\u3002\u6b64\u5916\uff0c\u8fd8\u6709\u4e00\u4e2a\u53ef\u9009\u7684\u6700\u7ec8\u6b65\u9aa4\uff0c\u5229\u7528\u6539\u8fdb\u7684\u81ea\u9002\u5e94\u820d\u5165\u6280\u672f\u8fdb\u884c\u987a\u5e8f\u4f18\u5316\uff0c\u4ee5\u51cf\u8f7b\u538b\u7f29\u5f15\u8d77\u7684\u8bef\u5dee\u3002", "result": "MLoRQ \u5728\u56fe\u50cf\u5206\u7c7b\u3001\u76ee\u6807\u68c0\u6d4b\u548c\u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u7684 Vision Transformer \u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u6027\u80fd\u63d0\u9ad8\u4e86 15%\u3002", "conclusion": "MLoRQ \u662f\u4e00\u79cd\u5c06\u4f4e\u79e9\u8fd1\u4f3c\u548c\u6df7\u5408\u7cbe\u5ea6\u91cf\u5316\u76f8\u7ed3\u5408\u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u7684 Transformer \u6a21\u578b\u3002"}}
{"id": "2507.09681", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.09681", "abs": "https://arxiv.org/abs/2507.09681", "authors": ["Osher Rafaeli", "Tal Svoray", "Ariel Nahlieli"], "title": "Prompt2DEM: High-Resolution DEMs for Urban and Open Environments from Global Prompts Using a Monocular Foundation Model", "comment": "18 pages", "summary": "High-resolution elevation estimations are essential to understand catchment\nand hillslope hydrology, study urban morphology and dynamics, and monitor the\ngrowth, decline, and mortality of terrestrial ecosystems. Various deep learning\napproaches (e.g., super-resolution techniques, monocular depth estimation) have\nbeen developed to create high-resolution Digital Elevation Models (DEMs).\nHowever, super-resolution techniques are limited by the upscaling factor, and\nmonocular depth estimation lacks global elevation context, making its\nconversion to a seamless DEM restricted. The recently introduced technique of\nprompt-based monocular depth estimation has opened new opportunities to extract\nestimates of absolute elevation in a global context. We present here a\nframework for the estimation of high-resolution DEMs as a new paradigm for\nabsolute global elevation mapping. It is exemplified using low-resolution\nShuttle Radar Topography Mission (SRTM) elevation data as prompts and\nhigh-resolution RGB imagery from the National Agriculture Imagery Program\n(NAIP). The approach fine-tunes a vision transformer encoder with LiDAR-derived\nDEMs and employs a versatile prompting strategy, enabling tasks such as DEM\nestimation, void filling, and updating. Our framework achieves a 100x\nresolution gain (from 30-m to 30-cm), surpassing prior methods by an order of\nmagnitude. Evaluations across three diverse U.S. landscapes show robust\ngeneralization, capturing urban structures and fine-scale terrain features with\n< 5 m MAE relative to LiDAR, improving over SRTM by up to 18%. Hydrological\nanalysis confirms suitability for hazard and environmental studies. We\ndemonstrate scalability by applying the framework to large regions in the U.S.\nand Israel. All code and pretrained models are publicly available at:\nhttps://osherr1996.github.io/prompt2dem_propage/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u9ad8\u5206\u8fa8\u7387DEM\u751f\u6210\u6846\u67b6\uff0c\u5229\u7528\u63d0\u793a\u5de5\u7a0b\u548c\u89c6\u89c9\u53d8\u6362\u5668\uff0c\u5b9e\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u5206\u8fa8\u7387\u63d0\u5347\u548c\u7cbe\u5ea6\uff0c\u4e3a\u5168\u7403\u9ad8\u7a0b\u6d4b\u7ed8\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u9ad8\u5206\u8fa8\u7387\u9ad8\u7a0b\u4f30\u8ba1\u5bf9\u4e8e\u7406\u89e3\u6d41\u57df\u548c\u5761\u9762\u6c34\u6587\u5b66\u3001\u7814\u7a76\u57ce\u5e02\u5f62\u6001\u548c\u52a8\u6001\u4ee5\u53ca\u76d1\u6d4b\u9646\u5730\u751f\u6001\u7cfb\u7edf\u7684\u751f\u957f\u3001\u8870\u9000\u548c\u6b7b\u4ea1\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5982\u8d85\u5206\u8fa8\u7387\u6280\u672f\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5982\u8d85\u5206\u8fa8\u7387\u6280\u672f\u7684\u5347\u5c3a\u5ea6\u56e0\u5b50\u9650\u5236\u548c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7f3a\u4e4f\u5168\u5c40\u9ad8\u7a0b\u80cc\u666f\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9ad8\u5206\u8fa8\u7387DEM\u4f30\u8ba1\u6846\u67b6\uff0c\u5229\u7528\u63d0\u793a\u5f0f\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6280\u672f\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5bf9\u89c6\u89c9\u53d8\u6362\u5668\u7f16\u7801\u5668\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u7ed3\u5408\u6fc0\u5149\u96f7\u8fbe\u884d\u751f\u7684DEM\u6570\u636e\u548c\u591a\u529f\u80fd\u7684\u63d0\u793a\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86DEM\u4f30\u8ba1\u3001\u7a7a\u6d1e\u586b\u5145\u548c\u66f4\u65b0\u7b49\u4efb\u52a1\u3002", "result": "\u8be5\u6846\u67b6\u5b9e\u73b0\u4e86100\u500d\u7684\u5206\u8fa8\u7387\u63d0\u5347\uff08\u4ece30\u7c73\u523030\u5398\u7c73\uff09\uff0c\u5728\u6355\u6349\u57ce\u5e02\u7ed3\u6784\u548c\u7cbe\u7ec6\u5730\u5f62\u7279\u5f81\u65b9\u9762\uff0c\u76f8\u5bf9\u4e8e\u6fc0\u5149\u96f7\u8fbe\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u5c0f\u4e8e5\u7c73\uff0c\u6bd4SRTM\u63d0\u9ad8\u4e86\u9ad8\u8fbe18%\u3002\u6c34\u6587\u5b66\u5206\u6790\u8bc1\u5b9e\u4e86\u8be5\u65b9\u6cd5\u5728\u707e\u5bb3\u548c\u73af\u5883\u7814\u7a76\u4e2d\u7684\u9002\u7528\u6027\uff0c\u5e76\u4e14\u8be5\u6846\u67b6\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u7f8e\u56fd\u548c\u4ee5\u8272\u5217\u7684\u5927\u7247\u533a\u57df\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387SRTM\u9ad8\u7a0b\u6570\u636e\u4f5c\u4e3a\u63d0\u793a\u548cNAIP\u7684\u9ad8\u5206\u8fa8\u7387RGB\u56fe\u50cf\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5206\u8fa8\u7387DEM\u4f30\u8ba1\uff0c\u5e76\u5c06\u5206\u8fa8\u7387\u63d0\u9ad8\u4e86100\u500d\uff08\u4ece30\u7c73\u523030\u5398\u7c73\uff09\uff0c\u5728\u57ce\u5e02\u7ed3\u6784\u548c\u7cbe\u7ec6\u5730\u5f62\u7279\u5f81\u6355\u6349\u65b9\u9762\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0cMAE\u5c0f\u4e8e5\u7c73\uff0c\u4f18\u4e8eSRTM\u9ad8\u8fbe18%\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u7f8e\u56fd\u548c\u4ee5\u8272\u5217\u7684\u5927\u7247\u533a\u57df\u3002"}}
{"id": "2507.10403", "categories": ["cs.CV", "cs.CL", "cs.IR", "cs.MM"], "pdf": "https://arxiv.org/pdf/2507.10403", "abs": "https://arxiv.org/abs/2507.10403", "authors": ["Daniele Rege Cambrin", "Lorenzo Vaiani", "Giuseppe Gallipoli", "Luca Cagliero", "Paolo Garza"], "title": "Text-to-Remote-Sensing-Image Retrieval beyond RGB Sources", "comment": null, "summary": "Retrieving relevant imagery from vast satellite archives is crucial for\napplications like disaster response and long-term climate monitoring. However,\nmost text-to-image retrieval systems are limited to RGB data, failing to\nexploit the unique physical information captured by other sensors, such as the\nall-weather structural sensitivity of Synthetic Aperture Radar (SAR) or the\nspectral signatures in optical multispectral data. To bridge this gap, we\nintroduce CrisisLandMark, a new large-scale corpus of over 647,000 Sentinel-1\nSAR and Sentinel-2 multispectral images paired with structured textual\nannotations for land cover, land use, and crisis events harmonized from\nauthoritative land cover systems (CORINE and Dynamic World) and crisis-specific\nsources. We then present CLOSP (Contrastive Language Optical SAR Pretraining),\na novel framework that uses text as a bridge to align unpaired optical and SAR\nimages into a unified embedding space. Our experiments show that CLOSP achieves\na new state-of-the-art, improving retrieval nDGC by 54% over existing models.\nAdditionally, we find that the unified training strategy overcomes the inherent\ndifficulty of interpreting SAR imagery by transferring rich semantic knowledge\nfrom the optical domain with indirect interaction. Furthermore, GeoCLOSP, which\nintegrates geographic coordinates into our framework, creates a powerful\ntrade-off between generality and specificity: while the CLOSP excels at general\nsemantic tasks, the GeoCLOSP becomes a specialized expert for retrieving\nlocation-dependent crisis events and rare geographic features. This work\nhighlights that the integration of diverse sensor data and geographic context\nis essential for unlocking the full potential of remote sensing archives.", "AI": {"tldr": "\u4e3a\u4e86\u6539\u8fdb\u536b\u661f\u56fe\u50cf\u68c0\u7d22\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b647,000\u5f20SAR\u548c\u591a\u5149\u8c31\u56fe\u50cf\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aCLOSP\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4f7f\u7528\u6587\u672c\u5c06\u4e0d\u540c\u7c7b\u578b\u7684\u56fe\u50cf\u8fde\u63a5\u8d77\u6765\uff0c\u4ece\u800c\u5c06\u68c0\u7d22\u6027\u80fd\u63d0\u9ad8\u4e8654%\u3002\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u52a0\u5165\u5730\u7406\u4fe1\u606f\uff08GeoCLOSP\uff09\u6709\u52a9\u4e8e\u68c0\u7d22\u7279\u5b9a\u4f4d\u7f6e\u7684\u4e8b\u4ef6\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u7cfb\u7edf\u4e3b\u8981\u5c40\u9650\u4e8eRGB\u6570\u636e\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528SAR\u548c\u591a\u5149\u8c31\u5149\u5b66\u6570\u636e\u7b49\u5176\u4ed6\u4f20\u611f\u5668\u63d0\u4f9b\u7684\u72ec\u7279\u7269\u7406\u4fe1\u606f\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5b9e\u73b0\u8de8\u6a21\u6001\u9065\u611f\u6570\u636e\u7684\u6709\u6548\u68c0\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLOSP\uff08Contrastive Language Optical SAR Pre-training\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u5229\u7528\u6587\u672c\u4f5c\u4e3a\u6865\u6881\uff0c\u5c06\u975e\u914d\u5bf9\u7684\u5149\u5b66\u548cSAR\u56fe\u50cf\u5bf9\u9f50\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86GeoCLOSP\uff0c\u5c06\u5730\u7406\u5750\u6807\u6574\u5408\u5230\u6846\u67b6\u4e2d\uff0c\u5b9e\u73b0\u4e86\u901a\u7528\u6027\u548c\u7279\u5f02\u6027\u4e4b\u95f4\u7684\u6743\u8861\u3002", "result": "CLOSP\u6846\u67b6\u5728\u9065\u611f\u56fe\u50cf\u68c0\u7d22\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\uff0c\u68c0\u7d22nDCG\uff08\u5f52\u4e00\u5316\u6298\u635f\u7d2f\u8ba1\u589e\u76ca\uff09\u6bd4\u73b0\u6709\u6a21\u578b\u63d0\u9ad8\u4e8654%\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u7edf\u4e00\u7684\u8bad\u7ec3\u7b56\u7565\u901a\u8fc7\u5149\u5b66\u57df\u5230SAR\u57df\u7684\u95f4\u63a5\u77e5\u8bc6\u8fc1\u79fb\uff0c\u514b\u670d\u4e86\u7406\u89e3SAR\u56fe\u50cf\u7684\u56fa\u6709\u56f0\u96be\u3002GeoCLOSP\u5728\u68c0\u7d22\u4f9d\u8d56\u5730\u7406\u4f4d\u7f6e\u7684\u5371\u673a\u4e8b\u4ef6\u548c\u7f55\u89c1\u5730\u7406\u7279\u5f81\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u6574\u5408\u591a\u6e90\u9065\u611f\u6570\u636e\uff08\u7279\u522b\u662fSAR\u548c\u591a\u5149\u8c31\u5149\u5b66\u6570\u636e\uff09\u548c\u5730\u7406\u7a7a\u95f4\u4fe1\u606f\u5bf9\u4e8e\u5145\u5206\u6316\u6398\u9065\u611f\u5f71\u50cf\u5e93\u6f5c\u529b\u7684\u91cd\u8981\u6027\u3002\u901a\u8fc7\u5f15\u5165CrisisLandMark\u8bed\u6599\u5e93\u548cCLOSP\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u8de8\u6a21\u6001\uff08\u6587\u672c-\u5149\u5b66-SAR\uff09\u7684\u56fe\u50cf\u68c0\u7d22\uff0c\u5e76\u5c55\u793a\u4e86\u5730\u7406\u4fe1\u606f\u878d\u5408\uff08GeoCLOSP\uff09\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u4f18\u52bf\u3002"}}
{"id": "2507.09650", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09650", "abs": "https://arxiv.org/abs/2507.09650", "authors": ["Lily Hong Zhang", "Smitha Milli", "Karen Jusko", "Jonathan Smith", "Brandon Amos", "Wassim", "Bouaziz", "Manon Revel", "Jack Kussman", "Lisa Titus", "Bhaktipriya Radharapu", "Jane Yu", "Vidya Sarma", "Kris Rose", "Maximilian Nickel"], "title": "Cultivating Pluralism In Algorithmic Monoculture: The Community Alignment Dataset", "comment": null, "summary": "How can large language models (LLMs) serve users with varying preferences\nthat may conflict across cultural, political, or other dimensions? To advance\nthis challenge, this paper establishes four key results. First, we demonstrate,\nthrough a large-scale multilingual human study with representative samples from\nfive countries (N=15,000), that humans exhibit significantly more variation in\npreferences than the responses of 21 state-of-the-art LLMs. Second, we show\nthat existing methods for preference dataset collection are insufficient for\nlearning the diversity of human preferences even along two of the most salient\ndimensions of variability in global values, due to the underlying homogeneity\nof candidate responses. Third, we argue that this motivates the need for\nnegatively-correlated sampling when generating candidate sets, and we show that\nsimple prompt-based techniques for doing so significantly enhance the\nperformance of alignment methods in learning heterogeneous preferences. Fourth,\nbased on this novel candidate sampling approach, we collect and open-source\nCommunity Alignment, the largest and most representative multilingual and\nmulti-turn preference dataset to date, featuring almost 200,000 comparisons\nfrom annotators spanning five countries. We hope that the Community Alignment\ndataset will be a valuable resource for improving the effectiveness of LLMs for\na diverse global population.", "AI": {"tldr": "LLM\u5728\u6ee1\u8db3\u7528\u6237\u591a\u6837\u5316\u504f\u597d\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u672c\u7814\u7a76\u63d0\u51fa\u8d1f\u76f8\u5173\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b\u8fd120\u4e07\u4e2a\u6bd4\u8f83\u7684\u201c\u793e\u533a\u5bf9\u9f50\u201d\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u9ad8LLM\u7684\u5305\u5bb9\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5982\u4f55\u6ee1\u8db3\u7528\u6237\u5728\u6587\u5316\u3001\u653f\u6cbb\u6216\u5176\u4ed6\u7ef4\u5ea6\u4e0a\u53ef\u80fd\u5b58\u5728\u7684\u51b2\u7a81\u504f\u597d\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u8d1f\u76f8\u5173\u91c7\u6837\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528\u57fa\u4e8e\u63d0\u793a\u7684\u6280\u672f\u6765\u589e\u5f3a\u5bf9\u9f50\u65b9\u6cd5\u5728\u5b66\u4e60\u5f02\u6784\u504f\u597d\u65b9\u9762\u7684\u6027\u80fd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4eba\u7c7b\u504f\u597d\u7684\u5dee\u5f02\u6027\u8fdc\u8d85\u5f53\u524d21\u4e2a\u6700\u5148\u8fdbLLM\u7684\u54cd\u5e94\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u7684\u504f\u597d\u6570\u636e\u96c6\u6536\u96c6\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5b66\u4e60\u4eba\u7c7b\u504f\u597d\u7684\u591a\u6837\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u6536\u96c6\u5e76\u5f00\u6e90\u4e86\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u3001\u6700\u5177\u4ee3\u8868\u6027\u7684\u591a\u8bed\u8a00\u548c\u591a\u8f6e\u793e\u533a\u5bf9\u9f50\u504f\u597d\u6570\u636e\u96c6\uff0c\u5305\u542b\u6765\u81ea\u4e94\u4e2a\u56fd\u5bb6/\u5730\u533a\u7684\u8fd1200,000\u4e2a\u6bd4\u8f83\uff0c\u4ee5\u671f\u63d0\u9ad8LLM\u5bf9\u5168\u7403\u591a\u6837\u5316\u4eba\u7fa4\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.09693", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09693", "abs": "https://arxiv.org/abs/2507.09693", "authors": ["Jiali Chen", "Yujie Jia", "Zihan Wu", "Jinyu Yang", "Jianpeng Chen", "Xusen Hei", "Jiayuan Xie", "Yi Cai", "Qing Li"], "title": "ExpStar: Towards Automatic Commentary Generation for Multi-discipline Scientific Experiments", "comment": "Accepted by ACM MM 2025", "summary": "Experiment commentary is crucial in describing the experimental procedures,\ndelving into underlying scientific principles, and incorporating\ncontent-related safety guidelines. In practice, human teachers rely heavily on\nsubject-specific expertise and invest significant time preparing such\ncommentary. To address this challenge, we introduce the task of automatic\ncommentary generation across multi-discipline scientific experiments. While\nrecent progress in large multimodal models (LMMs) has demonstrated promising\ncapabilities in video understanding and reasoning, their ability to generate\nfine-grained and insightful experiment commentary remains largely\nunderexplored. In this paper, we make the following contributions: (i) We\nconstruct \\textit{ExpInstruct}, the first dataset tailored for experiment\ncommentary generation, featuring over 7\\textit{K} step-level commentaries\nacross 21 scientific subjects from 3 core disciplines (\\ie, science, healthcare\nand engineering). Each sample includes procedural descriptions along with\npotential scientific principles (\\eg, chemical equations and physical laws) and\nsafety guidelines. (ii) We propose ExpStar, an automatic experiment commentary\ngeneration model that leverages a retrieval-augmented mechanism to adaptively\naccess, evaluate, and utilize external knowledge. (iii) Extensive experiments\nshow that our ExpStar substantially outperforms 14 leading LMMs, which\nhighlights the superiority of our dataset and model. We believe that ExpStar\nholds great potential for advancing AI-assisted scientific experiment\ninstruction.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86ExpCite\u6570\u636e\u96c6\u548cExpStar\u6a21\u578b\uff0c\u7528\u4e8e\u81ea\u52a8\u751f\u6210\u79d1\u5b66\u5b9e\u9a8c\u7684\u89e3\u8bf4\u8bcd\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eExpStar\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4eba\u5de5\u7f16\u5199\u5b9e\u9a8c\u89e3\u8bf4\u8bcd\u8017\u65f6\u8017\u529b\u7684\u95ee\u9898\uff0c\u672c\u6587\u63a2\u7d22\u4e86\u8de8\u5b66\u79d1\u79d1\u5b66\u5b9e\u9a8c\u81ea\u52a8\u89e3\u8bf4\u8bcd\u751f\u6210\u4efb\u52a1\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86ExpStar\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u68c0\u7d22\u589e\u5f3a\u673a\u5236\u6765\u9002\u5e94\u6027\u5730\u8bbf\u95ee\u3001\u8bc4\u4f30\u548c\u5229\u7528\u5916\u90e8\u77e5\u8bc6\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u5b9e\u9a8c\u89e3\u8bf4\u8bcd\u751f\u6210\u3002", "result": "ExpStar\u6a21\u578b\u5728\u5b9e\u9a8c\u89e3\u8bf4\u8bcd\u751f\u6210\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e14\u4e2a\u4e3b\u6d41\u7684\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\uff0c\u8bc1\u660e\u4e86ExpCite\u6570\u636e\u96c6\u548cExpStar\u6a21\u578b\u7684\u4f18\u8d8a\u6027\u3002", "conclusion": "ExpStar\u6a21\u578b\u5728\u5b9e\u9a8c\u6307\u5bfc\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u63a8\u52a8AI\u8f85\u52a9\u79d1\u5b66\u5b9e\u9a8c\u6307\u5bfc\u7684\u53d1\u5c55\u3002"}}
{"id": "2507.10419", "categories": ["cs.LG", "cs.AI", "cs.CL", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10419", "abs": "https://arxiv.org/abs/2507.10419", "authors": ["Victor Letzelter", "Hugo Malard", "Mathieu Fontaine", "Ga\u00ebl Richard", "Slim Essid", "Andrei Bursuc", "Patrick P\u00e9rez"], "title": "Multiple Choice Learning of Low Rank Adapters for Language Modeling", "comment": null, "summary": "We propose LoRA-MCL, a training scheme that extends next-token prediction in\nlanguage models with a method designed to decode diverse, plausible sentence\ncontinuations at inference time. Traditional language modeling is an\nintrinsically ill-posed problem: given a context, multiple futures may be\nequally plausible. Our approach leverages Multiple Choice Learning (MCL) and\nthe Winner-Takes-All (WTA) loss to efficiently handle ambiguity through\nLow-Rank Adaptation (LoRA). We provide a theoretical interpretation of applying\nMultiple Choice Learning to Language Modeling, assuming the data is generated\nfrom a mixture of distributions. To illustrate the proposed approach, we use\ndata sampled from mixtures of Markov chains. We then demonstrate with extensive\nexperiments on real-world visual and audio captioning tasks that our method\nachieves high diversity and relevance in generated outputs.", "AI": {"tldr": "LoRA-MCL\u901a\u8fc7\u7ed3\u5408MCL\u3001WTA\u548cLoRA\u6765\u89e3\u51b3\u8bed\u8a00\u6a21\u578b\u751f\u6210\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u751f\u6210\u53e5\u5b50\u7eed\u5199\u7684\u591a\u6837\u6027\u548c\u76f8\u5173\u6027\uff0c\u5c24\u5176\u5728\u89c6\u89c9\u548c\u97f3\u9891\u6458\u8981\u4efb\u52a1\u4e2d\u6548\u679c\u663e\u8457\u3002", "motivation": "\u9274\u4e8e\u4f20\u7edf\u8bed\u8a00\u6a21\u578b\u5728\u7ed9\u5b9a\u4e0a\u4e0b\u6587\u65f6\u5b58\u5728\u591a\u4e2a\u540c\u6837\u5408\u7406\u7684\u672a\u6765\u8d70\u5411\uff0c\u5373\u8bed\u8a00\u5efa\u6a21\u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u4e0d\u9002\u5b9a\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u56fa\u6709\u7684\u6b67\u4e49\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86LoRA-MCL\u8bad\u7ec3\u65b9\u6848\uff0c\u5b83\u6269\u5c55\u4e86\u8bed\u8a00\u6a21\u578b\u7684\u4e0b\u4e00\u4e2a\u8bcd\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5728\u63a8\u7406\u65f6\u751f\u6210\u591a\u6837\u5316\u3001\u5408\u7406\u53e5\u5b50\u7eed\u5199\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u9879\u9009\u62e9\u5b66\u4e60\uff08MCL\uff09\u548cWinner-Takes-All\uff08WTA\uff09\u635f\u5931\uff0c\u901a\u8fc7\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u9ad8\u6548\u5904\u7406\u6b67\u4e49\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5bf9\u5c06MCL\u5e94\u7528\u4e8e\u8bed\u8a00\u5efa\u6a21\u8fdb\u884c\u4e86\u7406\u8bba\u9610\u91ca\uff0c\u5047\u8bbe\u6570\u636e\u6765\u81ea\u6df7\u5408\u5206\u5e03\uff0c\u5e76\u4f7f\u7528\u9a6c\u5c14\u53ef\u592b\u94fe\u6df7\u5408\u7269\u4f5c\u4e3a\u793a\u4f8b\u3002", "result": "\u901a\u8fc7\u5728\u771f\u5b9e\u4e16\u754c\u7684\u89c6\u89c9\u548c\u97f3\u9891\u6458\u8981\u4efb\u52a1\u4e0a\u8fdb\u884c\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u9ad8\u5ea6\u591a\u6837\u5316\u548c\u76f8\u5173\u7684\u8f93\u51fa\u3002", "conclusion": "LoRA-MCL\u901a\u8fc7\u5229\u7528\u591a\u9879\u9009\u62e9\u5b66\u4e60\uff08MCL\uff09\u548cWinner-Takes-All\uff08WTA\uff09\u635f\u5931\uff0c\u5e76\u7ed3\u5408\u4f4e\u79e9\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u65f6\u751f\u6210\u591a\u6837\u5316\u4e14\u5408\u7406\u7684\u53e5\u5b50\u7eed\u5199\u4e2d\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u5728\u89c6\u89c9\u548c\u97f3\u9891\u6458\u8981\u7b49\u771f\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u9ad8\u591a\u6837\u6027\u548c\u9ad8\u76f8\u5173\u6027\u7684\u751f\u6210\u7ed3\u679c\u3002"}}
{"id": "2507.09678", "categories": ["cs.LG", "cs.AI", "math.ST", "stat.TH"], "pdf": "https://arxiv.org/pdf/2507.09678", "abs": "https://arxiv.org/abs/2507.09678", "authors": ["Alexander David Balinsky", "Dominik Krzeminski", "Alexander Balinsky"], "title": "Conformal Prediction for Privacy-Preserving Machine Learning", "comment": null, "summary": "We investigate the integration of Conformal Prediction (CP) with supervised\nlearning on deterministically encrypted data, aiming to bridge the gap between\nrigorous uncertainty quantification and privacy-preserving machine learning.\nUsing AES-encrypted variants of the MNIST dataset, we demonstrate that CP\nmethods remain effective even when applied directly in the encrypted domain,\nowing to the preservation of data exchangeability under fixed-key encryption.\nWe test traditional $p$-value-based against $e$-value-based conformal\npredictors. Our empirical evaluation reveals that models trained on\ndeterministically encrypted data retain the ability to extract meaningful\nstructure, achieving 36.88\\% test accuracy -- significantly above random\nguessing (9.56\\%) observed with per-instance encryption. Moreover,\n$e$-value-based CP achieves predictive set coverage of over 60\\% with 4.3\nloss-threshold calibration, correctly capturing the true label in 4888 out of\n5000 test cases. In contrast, the $p$-value-based CP yields smaller predictive\nsets but with reduced coverage accuracy. These findings highlight both the\npromise and limitations of CP in encrypted data settings and underscore\ncritical trade-offs between prediction set compactness and reliability. %Our\nwork sets a foundation for principled uncertainty quantification in secure,\nprivacy-aware learning systems.", "AI": {"tldr": "CP\u4e0e\u52a0\u5bc6\u6570\u636e\u4e0a\u7684\u76d1\u7763\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\u5b9e\u73b0\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u3002", "motivation": "\u65e8\u5728\u5f25\u5408\u4e25\u683c\u7684\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u4e0e\u4fdd\u62a4\u9690\u79c1\u7684\u673a\u5668\u5b66\u4e60\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002", "method": "\u672c\u7814\u7a76\u5c06\u4fdd\u5f62\u9884\u6d4b\uff08CP\uff09\u4e0e\u786e\u5b9a\u6027\u52a0\u5bc6\u6570\u636e\u4e0a\u7684\u76d1\u7763\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u5e76\u6d4b\u8bd5\u4e86\u57fa\u4e8e\u4f20\u7edfp\u503c\u548c\u57fa\u4e8ee\u503c\u7684\u4fdd\u5f62\u9884\u6d4b\u5668\u3002", "result": "\u5728AES\u52a0\u5bc6\u7684MNIST\u6570\u636e\u96c6\u4e0a\uff0cCP\u65b9\u6cd5\u5728\u52a0\u5bc6\u57df\u4e2d\u4f9d\u7136\u6709\u6548\u3002\u5728\u786e\u5b9a\u6027\u52a0\u5bc6\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u80fd\u591f\u63d0\u53d6\u6709\u610f\u4e49\u7684\u7ed3\u6784\uff0c\u6d4b\u8bd5\u51c6\u786e\u7387\u4e3a36.88%\uff0c\u8fdc\u9ad8\u4e8e\u6bcf\u5b9e\u4f8b\u52a0\u5bc6\u7684\u968f\u673a\u731c\u6d4b\uff089.56%\uff09\u3002\u57fa\u4e8ee\u503c\u7684CP\u5b9e\u73b0\u4e86\u8d85\u8fc760%\u7684\u9884\u6d4b\u96c6\u8986\u76d6\u7387\uff0c\u5177\u67094.3\u7684\u635f\u5931\u9608\u503c\u6821\u51c6\uff0c\u57285000\u4e2a\u6d4b\u8bd5\u7528\u4f8b\u4e2d\u76844888\u4e2a\u6b63\u786e\u6355\u83b7\u4e86\u771f\u5b9e\u6807\u7b7e\u3002\u57fa\u4e8ep\u503c\u7684CP\u4ea7\u751f\u4e86\u66f4\u5c0f\u7684\u9884\u6d4b\u96c6\uff0c\u4f46\u8986\u76d6\u51c6\u786e\u6027\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5b89\u5168\u3001\u6ce8\u91cd\u9690\u79c1\u7684\u5b66\u4e60\u7cfb\u7edf\u4e2d\u7684\u539f\u5219\u6027\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u7a81\u663e\u4e86\u52a0\u5bc6\u6570\u636e\u73af\u5883\u4e2d CP \u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u4ee5\u53ca\u9884\u6d4b\u96c6\u7d27\u51d1\u6027\u4e0e\u53ef\u9760\u6027\u4e4b\u95f4\u7684\u5173\u952e\u6743\u8861\u3002"}}
{"id": "2507.09702", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09702", "abs": "https://arxiv.org/abs/2507.09702", "authors": ["Phat Nguyen", "Ngai-Man Cheung"], "title": "Token Compression Meets Compact Vision Transformers: A Survey and Comparative Evaluation for Edge AI", "comment": null, "summary": "Token compression techniques have recently emerged as powerful tools for\naccelerating Vision Transformer (ViT) inference in computer vision. Due to the\nquadratic computational complexity with respect to the token sequence length,\nthese methods aim to remove less informative tokens before the attention layers\nto improve inference throughput. While numerous studies have explored various\naccuracy-efficiency trade-offs on large-scale ViTs, two critical gaps remain.\nFirst, there is a lack of unified survey that systematically categorizes and\ncompares token compression approaches based on their core strategies (e.g.,\npruning, merging, or hybrid) and deployment settings (e.g., fine-tuning vs.\nplug-in). Second, most benchmarks are limited to standard ViT models (e.g.,\nViT-B, ViT-L), leaving open the question of whether such methods remain\neffective when applied to structurally compressed transformers, which are\nincreasingly deployed on resource-constrained edge devices. To address these\ngaps, we present the first systematic taxonomy and comparative study of token\ncompression methods, and we evaluate representative techniques on both standard\nand compact ViT architectures. Our experiments reveal that while token\ncompression methods are effective for general-purpose ViTs, they often\nunderperform when directly applied to compact designs. These findings not only\nprovide practical insights but also pave the way for future research on\nadapting token optimization techniques to compact transformer-based networks\nfor edge AI and AI agent applications.", "AI": {"tldr": "ViT\u7684token\u538b\u7f29\u6280\u672f\uff1a\u9996\u6b21\u7cfb\u7edf\u6027\u5206\u7c7b\u4e0e\u8bc4\u4f30\uff0c\u53d1\u73b0\u5728\u7d27\u51d1\u578b\u6a21\u578b\u4e0a\u6548\u679c\u53d7\u9650\uff0c\u4e3a\u8fb9\u7f18AI\u5e94\u7528\u63d0\u4f9b\u65b0\u65b9\u5411\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728ViT\u7684token\u538b\u7f29\u6280\u672f\u65b9\u9762\u5b58\u5728\u4e24\u5927\u4e0d\u8db3\uff1a1. \u7f3a\u4e4f\u5bf9\u4e0d\u540c\u538b\u7f29\u7b56\u7565\uff08\u5982\u526a\u679d\u3001\u5408\u5e76\u6216\u6df7\u5408\uff09\u548c\u90e8\u7f72\u8bbe\u7f6e\uff08\u5982\u5fae\u8c03\u6216\u5373\u63d2\u5373\u7528\uff09\u7684\u7edf\u4e00\u5206\u7c7b\u548c\u6bd4\u8f83\u30022. \u5927\u591a\u6570\u57fa\u51c6\u6d4b\u8bd5\u4ec5\u9650\u4e8e\u6807\u51c6ViT\u6a21\u578b\uff0c\u672a\u80fd\u8bc4\u4f30\u8fd9\u4e9b\u6280\u672f\u5728\u7ed3\u6784\u538b\u7f29\u7684Transformer\u4e0a\u7684\u9002\u7528\u6027\uff0c\u800c\u8fd9\u7c7b\u6a21\u578b\u8d8a\u6765\u8d8a\u591a\u5730\u90e8\u7f72\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7684token\u538b\u7f29\u65b9\u6cd5\u5206\u7c7b\u6cd5\uff0c\u5e76\u8bc4\u4f30\u4e86\u4ee3\u8868\u6027\u6280\u672f\u5728\u6807\u51c6ViT\u548c\u7d27\u51d1\u578bViT\u67b6\u6784\u4e0a\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0ctoken\u538b\u7f29\u6280\u672f\u5bf9\u901a\u7528ViT\u6709\u6548\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u7d27\u51d1\u578b\u8bbe\u8ba1\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86\u5173\u4e8eViT\u6a21\u578b\u4e2d\u7684token\u538b\u7f29\u6280\u672f\u7f3a\u4e4f\u7cfb\u7edf\u6027\u5206\u7c7b\u548c\u6bd4\u8f83\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u9996\u6b21\u5728\u6807\u51c6ViT\u548c\u7d27\u51d1\u578bViT\u67b6\u6784\u4e0a\u5bf9\u4ee3\u8868\u6027\u6280\u672f\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u867d\u7136token\u538b\u7f29\u6280\u672f\u5bf9\u901a\u7528ViT\u6709\u6548\uff0c\u4f46\u76f4\u63a5\u5e94\u7528\u4e8e\u7d27\u51d1\u578b\u8bbe\u8ba1\u65f6\u6548\u679c\u4e0d\u4f73\uff0c\u8fd9\u4e3a\u672a\u6765\u5728\u8fb9\u7f18AI\u548cAI\u4ee3\u7406\u5e94\u7528\u4e2d\u5c06token\u4f18\u5316\u6280\u672f\u5e94\u7528\u4e8e\u7d27\u51d1\u578bTransformer\u7f51\u7edc\u7684\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09748", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09748", "abs": "https://arxiv.org/abs/2507.09748", "authors": ["Yu Lei", "Bingde Liu", "Qingsong Xie", "Haonan Lu", "Zhijie Deng"], "title": "Advancing Text-to-3D Generation with Linearized Lookahead Variational Score Distillation", "comment": "Accepted by ICCV 2025", "summary": "Text-to-3D generation based on score distillation of pre-trained 2D diffusion\nmodels has gained increasing interest, with variational score distillation\n(VSD) as a remarkable example. VSD proves that vanilla score distillation can\nbe improved by introducing an extra score-based model, which characterizes the\ndistribution of images rendered from 3D models, to correct the distillation\ngradient. Despite the theoretical foundations, VSD, in practice, is likely to\nsuffer from slow and sometimes ill-posed convergence. In this paper, we perform\nan in-depth investigation of the interplay between the introduced score model\nand the 3D model, and find that there exists a mismatching problem between LoRA\nand 3D distributions in practical implementation. We can simply adjust their\noptimization order to improve the generation quality. By doing so, the score\nmodel looks ahead to the current 3D state and hence yields more reasonable\ncorrections. Nevertheless, naive lookahead VSD may suffer from unstable\ntraining in practice due to the potential over-fitting. To address this, we\npropose to use a linearized variant of the model for score distillation, giving\nrise to the Linearized Lookahead Variational Score Distillation ($L^2$-VSD).\n$L^2$-VSD can be realized efficiently with forward-mode autodiff\nfunctionalities of existing deep learning libraries. Extensive experiments\nvalidate the efficacy of $L^2$-VSD, revealing its clear superiority over prior\nscore distillation-based methods. We also show that our method can be\nseamlessly incorporated into any other VSD-based text-to-3D framework.", "AI": {"tldr": "$L^2$-VSD\u901a\u8fc7\u89e3\u51b3LoRA\u548c3D\u5206\u5e03\u7684\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u7ebf\u6027\u5316\u6a21\u578b\u8fdb\u884c\u5206\u6570\u84b8\u998f\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u52303D\u751f\u6210\u7684\u901f\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9884\u8bad\u7ec32D\u6269\u6563\u6a21\u578b\u5206\u6570\u84b8\u998f\u7684\u6587\u672c\u52303D\u751f\u6210\u65b9\u6cd5\uff0c\u5982\u53d8\u5206\u5206\u6570\u84b8\u998f\uff08VSD\uff09\uff0c\u867d\u7136\u6709\u7406\u8bba\u57fa\u7840\uff0c\u4f46\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u6536\u655b\u901f\u5ea6\u6162\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5bf9\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6df1\u5165\u5206\u6790\uff0c\u53d1\u73b0LoRA\u4e0e3D\u5206\u5e03\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u95ee\u9898\u3002\u63d0\u51fa\u901a\u8fc7\u8c03\u6574\u4f18\u5316\u987a\u5e8f\u6765\u89e3\u51b3\u8be5\u95ee\u9898\uff0c\u5e76\u8fdb\u4e00\u6b65\u63d0\u51fa\u4f7f\u7528\u7ebf\u6027\u5316\u6a21\u578b\u8fdb\u884c\u5206\u6570\u84b8\u998f\uff0c\u5373$L^2$-VSD\uff0c\u5229\u7528\u524d\u5411\u6a21\u5f0f\u81ea\u52a8\u5fae\u5206\u5b9e\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c$L^2$-VSD\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u660e\u663e\u4f18\u4e8e\u5148\u524d\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u4ee5\u8f7b\u677e\u5730\u96c6\u6210\u5230\u4efb\u4f55\u5176\u4ed6\u57fa\u4e8eVSD\u7684\u6587\u672c\u52303D\u6846\u67b6\u4e2d\u3002", "conclusion": "\u901a\u8fc7\u5bf9LoRA\u548c3D\u5206\u5e03\u4e4b\u95f4\u5339\u914d\u95ee\u9898\u8fdb\u884c\u6df1\u5165\u7814\u7a76\uff0c\u6211\u4eec\u53d1\u73b0\u8c03\u6574\u4f18\u5316\u987a\u5e8f\u53ef\u4ee5\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u7ebf\u6027\u524d\u77bb\u53d8\u5206\u5206\u6570\u84b8\u998f\uff08$L^2$-VSD\uff09\uff0c\u901a\u8fc7\u4f7f\u7528\u7ebf\u6027\u5316\u6a21\u578b\u8fdb\u884c\u5206\u6570\u84b8\u998f\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u4e8e\u73b0\u6709\u57fa\u4e8e\u5206\u6570\u84b8\u998f\u7684\u65b9\u6cd5\uff0c\u540c\u65f6\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u5176\u4ed6\u57fa\u4e8eVSD\u7684\u6587\u672c\u52303D\u6846\u67b6\u4e2d\u3002"}}
{"id": "2507.10532", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10532", "abs": "https://arxiv.org/abs/2507.10532", "authors": ["Mingqi Wu", "Zhihao Zhang", "Qiaole Dong", "Zhiheng Xi", "Jun Zhao", "Senjie Jin", "Xiaoran Fan", "Yuhao Zhou", "Yanwei Fu", "Qin Liu", "Songyang Zhang", "Qi Zhang"], "title": "Reasoning or Memorization? Unreliable Results of Reinforcement Learning Due to Data Contamination", "comment": "26 pages", "summary": "The reasoning capabilities of large language models (LLMs) have been a\nlongstanding focus of research. Recent works have further enhanced these\ncapabilities using reinforcement learning (RL), with many new methods claiming\nsignificant improvements with minimal or no external supervision. Surprisingly,\nsome studies even suggest that random or incorrect reward signals can enhance\nreasoning performance. However, these breakthroughs are mostly reported on the\nQwen2.5 model family and evaluated on well-known benchmarks such as MATH-500,\nAMC, and AIME, while failing to achieve similar gains on other models like\nLlama, which warrants further investigation. Our analysis shows that although\nQwen2.5 achieves strong mathematical reasoning performance, its pretraining on\nlarge-scale web corpora makes it vulnerable to data contamination in popular\nbenchmarks. As a result, results derived from these benchmarks may be\nunreliable. To address this, we introduce a generator that produces fully\nsynthetic arithmetic problems of arbitrary length and difficulty, yielding a\nclean dataset we call RandomCalculation. Using these leakage-free datasets, we\nshow that only accurate reward signals consistently improve performance, while\nnoisy or incorrect signals do not. We advocate for evaluating RL methods on\nuncontaminated benchmarks and across diverse model families to ensure\ntrustworthy conclusions.", "AI": {"tldr": "\u73b0\u6709\u5173\u4e8e\u5f3a\u5316\u5b66\u4e60\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728Qwen2.5\u6a21\u578b\u53ca\u5176\u5728\u7279\u5b9a\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\uff0c\u4f46\u5ffd\u7565\u4e86\u6570\u636e\u6c61\u67d3\u7684\u53ef\u80fd\u6027\u3002\u672c\u6587\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u540d\u4e3aRandomCalculation\u7684\u5408\u6210\u6570\u636e\u96c6\uff0c\u53d1\u73b0\u53ea\u6709\u51c6\u786e\u7684\u5956\u52b1\u4fe1\u53f7\u624d\u80fd\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u547c\u5401\u5728\u672a\u53d7\u6c61\u67d3\u7684\u6570\u636e\u96c6\u548c\u66f4\u591a\u6a21\u578b\u4e0a\u8fdb\u884c\u8bc4\u4f30\u4ee5\u83b7\u5f97\u53ef\u9760\u7ed3\u8bba\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u7814\u7a76\u5728Qwen2.5\u6a21\u578b\u4e0a\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5728Llama\u7b49\u5176\u4ed6\u6a21\u578b\u4e0a\u672a\u80fd\u53d6\u5f97\u7c7b\u4f3c\u8fdb\u5c55\u7684\u95ee\u9898\uff0c\u5e76\u4e14\u4e3a\u4e86\u9a8c\u8bc1\u662f\u5426\u6240\u6709\u6a21\u578b\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u90fd\u5b58\u5728\u6570\u636e\u6c61\u67d3\u7684\u95ee\u9898\uff0c\u672c\u6587\u8fdb\u884c\u4e86\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u751f\u6210\u5668\uff0c\u8be5\u751f\u6210\u5668\u53ef\u4ee5\u751f\u6210\u4efb\u610f\u957f\u5ea6\u548c\u96be\u5ea6\u7684\u5b8c\u5168\u5408\u6210\u7684\u7b97\u672f\u95ee\u9898\uff0c\u4ece\u800c\u5f97\u5230\u4e86\u4e00\u4e2a\u540d\u4e3aRandomCalculation\u7684\u5e72\u51c0\u6570\u636e\u96c6\u3002", "result": "\u901a\u8fc7\u4f7f\u7528\u65e0\u6570\u636e\u6cc4\u6f0f\u7684\u6570\u636e\u96c6\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u53ea\u6709\u51c6\u786e\u7684\u5956\u52b1\u4fe1\u53f7\u624d\u80fd\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\uff0c\u800c\u566a\u58f0\u6216\u4e0d\u6b63\u786e\u7684\u4fe1\u53f7\u5219\u4e0d\u80fd\u3002", "conclusion": "\u5728\u5e72\u51c0\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u53ea\u6709\u51c6\u786e\u7684\u5956\u52b1\u4fe1\u53f7\u624d\u80fd\u6301\u7eed\u63d0\u9ad8\u6027\u80fd\uff0c\u800c\u566a\u58f0\u6216\u4e0d\u6b63\u786e\u7684\u4fe1\u53f7\u5219\u4e0d\u80fd\u3002\u6211\u4eec\u63d0\u5021\u5728\u672a\u53d7\u6c61\u67d3\u7684\u57fa\u51c6\u6d4b\u8bd5\u548c\u8de8\u6a21\u578b\u5bb6\u65cf\u7684\u8bc4\u4f30\u4e2d\u8bc4\u4f30\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fdd\u53ef\u4fe1\u7684\u7ed3\u8bba\u3002"}}
{"id": "2507.09687", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09687", "abs": "https://arxiv.org/abs/2507.09687", "authors": ["Md Mushfiqur Rahaman", "Elliot Chang", "Tasmiah Haque", "Srinjoy Das"], "title": "Post-Training Quantization of Generative and Discriminative LSTM Text Classifiers: A Study of Calibration, Class Balance, and Robustness", "comment": null, "summary": "Text classification plays a pivotal role in edge computing applications like\nindustrial monitoring, health diagnostics, and smart assistants, where low\nlatency and high accuracy are both key requirements. Generative classifiers, in\nparticular, have been shown to exhibit robustness to out-of-distribution and\nnoisy data, which is an extremely critical consideration for deployment in such\nreal-time edge environments. However, deploying such models on edge devices\nfaces computational and memory constraints. Post Training Quantization (PTQ)\nreduces model size and compute costs without retraining, making it ideal for\nedge deployment. In this work, we present a comprehensive comparative study of\ngenerative and discriminative Long Short Term Memory (LSTM)-based text\nclassification models with PTQ using the Brevitas quantization library. We\nevaluate both types of classifier models across multiple bitwidths and assess\ntheir robustness under regular and noisy input conditions. We find that while\ndiscriminative classifiers remain robust, generative ones are more sensitive to\nbitwidth, calibration data used during PTQ, and input noise during quantized\ninference. We study the influence of class imbalance in calibration data for\nboth types of classifiers, comparing scenarios with evenly and unevenly\ndistributed class samples including their effect on weight adjustments and\nactivation profiles during PTQ. Using test statistics derived from\nnonparametric hypothesis testing, we identify that using class imbalanced data\nduring calibration introduces insufficient weight adaptation at lower bitwidths\nfor generative LSTM classifiers, thereby leading to degraded performance. This\nstudy underscores the role of calibration data in PTQ and when generative\nclassifiers succeed or fail under noise, aiding deployment in edge\nenvironments.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5bf9\u91cf\u5316\u540e\u8bad\u7ec3 (PTQ) \u5bf9\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f LSTM \u6587\u672c\u5206\u7c7b\u6a21\u578b\u7684\u5f71\u54cd\u8fdb\u884c\u4e86\u6bd4\u8f83\u7814\u7a76\uff0c\u53d1\u73b0\u5728\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u4e0b\uff0c\u751f\u6210\u5f0f\u6a21\u578b\u5bf9\u91cf\u5316\u53c2\u6570\u548c\u6570\u636e\u4e0d\u5e73\u8861\u66f4\u4e3a\u654f\u611f\uff0c\u5e76\u63d0\u51fa\u4e86\u5728\u91cf\u5316\u8fc7\u7a0b\u4e2d\u8c28\u614e\u9009\u62e9\u6821\u51c6\u6570\u636e\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u5de5\u4e1a\u76d1\u63a7\u3001\u5065\u5eb7\u8bca\u65ad\u548c\u667a\u80fd\u52a9\u624b\u7b49\u8fb9\u7f18\u8ba1\u7b97\u5e94\u7528\u5bf9\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u51c6\u786e\u6027\u7684\u8981\u6c42\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f LSTM \u6587\u672c\u5206\u7c7b\u6a21\u578b\u5728\u91cf\u5316\u540e\u8bad\u7ec3 (PTQ) \u4e0b\u7684\u6027\u80fd\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u8bc4\u4f30 PTQ \u5bf9\u8fd9\u4e9b\u6a21\u578b\uff08\u5c24\u5176\u662f\u751f\u6210\u5f0f\u6a21\u578b\uff09\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u4f7f\u7528 Brevitas \u91cf\u5316\u5e93\u5bf9\u751f\u6210\u5f0f\u548c\u5224\u522b\u5f0f\u957f\u77ed\u671f\u8bb0\u5fc6 (LSTM) \u6587\u672c\u5206\u7c7b\u6a21\u578b\u8fdb\u884c\u4e86\u91cf\u5316\u540e\u8bad\u7ec3 (PTQ) \u7684\u7efc\u5408\u6bd4\u8f83\u7814\u7a76\u3002\u4ed6\u4eec\u8bc4\u4f30\u4e86\u4e0d\u540c\u6bd4\u7279\u5bbd\u5ea6\u7684\u5206\u7c7b\u5668\u6a21\u578b\uff0c\u5e76\u5728\u6b63\u5e38\u548c\u566a\u58f0\u8f93\u5165\u6761\u4ef6\u4e0b\u8bc4\u4f30\u4e86\u5b83\u4eec\u7684\u9c81\u68d2\u6027\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5224\u522b\u5f0f\u5206\u7c7b\u5668\u5728\u91cf\u5316\u540e\u4ecd\u7136\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u800c\u751f\u6210\u5f0f\u5206\u7c7b\u5668\u5bf9\u6bd4\u7279\u5bbd\u5ea6\u3001PTQ \u671f\u95f4\u4f7f\u7528\u7684\u6821\u51c6\u6570\u636e\u548c\u91cf\u5316\u63a8\u7406\u671f\u95f4\u7684\u8f93\u5165\u566a\u58f0\u66f4\u4e3a\u654f\u611f\u3002\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e86\u7c7b\u522b\u4e0d\u5e73\u8861\u5bf9\u6821\u51c6\u6570\u636e\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u8f83\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u4e0b\uff0c\u751f\u6210\u5f0f LSTM \u5206\u7c7b\u5668\u5728\u6821\u51c6\u4e2d\u4f7f\u7528\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5728\u91cf\u5316\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6821\u51c6\u6570\u636e\u4f1a\u5bfc\u81f4\u751f\u6210\u5f0f LSTM \u5206\u7c7b\u5668\u5728\u8f83\u4f4e\u6bd4\u7279\u5bbd\u5ea6\u4e0b\u6743\u91cd\u9002\u5e94\u4e0d\u8db3\uff0c\u4ece\u800c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u8fd9\u5f3a\u8c03\u4e86\u6821\u51c6\u6570\u636e\u5728 PTQ \u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u751f\u6210\u5f0f\u5206\u7c7b\u5668\u5728\u566a\u58f0\u4e0b\u7684\u6210\u529f\u548c\u5931\u8d25\uff0c\u6709\u52a9\u4e8e\u5728\u8fb9\u7f18\u73af\u5883\u4e2d\u90e8\u7f72\u3002"}}
{"id": "2507.09767", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09767", "abs": "https://arxiv.org/abs/2507.09767", "authors": ["Ofir Itzhak Shahar", "Gur Elkin", "Ohad Ben-Shahar"], "title": "Pairwise Alignment & Compatibility for Arbitrarily Irregular Image Fragments", "comment": null, "summary": "Pairwise compatibility calculation is at the core of most\nfragments-reconstruction algorithms, in particular those designed to solve\ndifferent types of the jigsaw puzzle problem. However, most existing approaches\nfail, or aren't designed to deal with fragments of realistic geometric\nproperties one encounters in real-life puzzles. And in all other cases,\ncompatibility methods rely strongly on the restricted shapes of the fragments.\nIn this paper, we propose an efficient hybrid (geometric and pictorial)\napproach for computing the optimal alignment for pairs of fragments, without\nany assumptions about their shapes, dimensions, or pictorial content. We\nintroduce a new image fragments dataset generated via a novel method for image\nfragmentation and a formal erosion model that mimics real-world archaeological\nerosion, along with evaluation metrics for the compatibility task. We then\nembed our proposed compatibility into an archaeological puzzle-solving\nframework and demonstrate state-of-the-art neighborhood-level precision and\nrecall on the RePAIR 2D dataset, directly reflecting compatibility performance\nimprovements.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u5047\u8bbe\u7247\u6bb5\u5f62\u72b6\u7684\u6df7\u5408\u65b9\u6cd5\u6765\u8ba1\u7b97\u7247\u6bb5\u5bf9\u7684\u6700\u4f73\u5bf9\u9f50\uff0c\u5e76\u5728\u8003\u53e4\u62fc\u56fe\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7247\u6bb5\u517c\u5bb9\u6027\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u89e3\u51b3\u5404\u79cd\u7c7b\u578b\u7684\u62fc\u56fe\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u65e0\u6cd5\u5904\u7406\u5177\u6709\u73b0\u5b9e\u51e0\u4f55\u7279\u6027\u7684\u7247\u6bb5\uff0c\u6216\u8005\u4e25\u91cd\u4f9d\u8d56\u4e8e\u7247\u6bb5\u7684\u5f62\u72b6\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u50cf\u7247\u6bb5\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u662f\u901a\u8fc7\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u5206\u5272\u65b9\u6cd5\u751f\u6210\u7684\uff0c\u4ee5\u53ca\u4e00\u4e2a\u6a21\u4eff\u73b0\u5b9e\u4e16\u754c\u8003\u53e4\u4fb5\u8680\u7684\u6b63\u5f0f\u4fb5\u8680\u6a21\u578b\uff0c\u5e76\u4e3a\u517c\u5bb9\u6027\u4efb\u52a1\u63d0\u4f9b\u4e86\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5c06\u63d0\u51fa\u7684\u517c\u5bb9\u6027\u65b9\u6cd5\u5d4c\u5165\u5230\u8003\u53e4\u62fc\u56fe\u89e3\u51b3\u6846\u67b6\u4e2d\uff0c\u5728RePAIR 2D\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u90bb\u57df\u7ea7\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\uff0c\u76f4\u63a5\u53cd\u6620\u4e86\u517c\u5bb9\u6027\u6027\u80fd\u7684\u63d0\u5347\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u6df7\u5408\uff08\u51e0\u4f55\u548c\u56fe\u5f62\uff09\u65b9\u6cd5\u6765\u8ba1\u7b97\u7247\u6bb5\u5bf9\u7684\u6700\u4f73\u5bf9\u9f50\uff0c\u65e0\u9700\u5bf9\u5176\u5f62\u72b6\u3001\u5c3a\u5bf8\u6216\u56fe\u5f62\u5185\u5bb9\u8fdb\u884c\u4efb\u4f55\u5047\u8bbe\u3002"}}
{"id": "2507.10548", "categories": ["cs.CV", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10548", "abs": "https://arxiv.org/abs/2507.10548", "authors": ["Mingxian Lin", "Wei Huang", "Yitang Li", "Chengjie Jiang", "Kui Wu", "Fangwei Zhong", "Shengju Qian", "Xin Wang", "Xiaojuan Qi"], "title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments", "comment": "Project page: https://mxllc.github.io/EmbRACE-3K/", "summary": "Recent advanced vision-language models(VLMs) have demonstrated strong\nperformance on passive, offline image and video understanding tasks. However,\ntheir effectiveness in embodied settings, which require online interaction and\nactive scene understanding remains limited. In such scenarios, an agent\nperceives the environment from a first-person perspective, with each action\ndynamically shaping subsequent observations. Even state-of-the-art models such\nas GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment\ninteractions, exhibiting clear limitations in spatial reasoning and\nlong-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset\nof over 3,000 language-guided tasks situated in diverse, photorealistic\nenvironments constructed using Unreal Engine and the UnrealCV-Zoo framework.\nThe tasks encompass a wide range of embodied challenges, including navigation,\nobject manipulation, and multi-stage goal execution. Each task unfolds as a\nmulti-step trajectory, pairing first-person visual observations with high-level\ninstructions, grounded actions, and natural language rationales that express\nthe agent's intent at every step. Using EmRACE-3K, we establish a benchmark to\nevaluate the embodied reasoning capabilities of VLMs across three key\ndimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage\nGoal Execution. In zero-shot settings, all models achieve success rates below\n20%, underscoring the challenge posed by our benchmark and the current\nlimitations of VLMs in interactive environments. To demonstrate the utility of\nEmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning\nfollowed by reinforcement learning. This approach yields substantial\nimprovements across all three challenge categories, highlighting the dataset's\neffectiveness in enabling the development of embodied reasoning capabilities.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86EmRACE-3K\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u73b0\u6709\u6a21\u578b\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u5728EmRACE-3K\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u9700\u8981\u5728\u7ebf\u4ea4\u4e92\u548c\u4e3b\u52a8\u573a\u666f\u7406\u89e3\u7684\u5177\u8eab\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u6709\u9650\uff0c\u5373\u4f7f\u662f\u50cfGPT-4o\u3001Claude 3.5 Sonnet\u548cGemini 2.5 Pro\u8fd9\u6837\u5148\u8fdb\u7684\u6a21\u578b\u5728\u5f00\u653e\u73af\u5883\u4ea4\u4e92\u4e2d\u4e5f\u5b58\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u7684\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u6709\u6548\u8bc4\u4f30\u548c\u6539\u8fdb\u6a21\u578b\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u8868\u73b0\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b3000\u591a\u4e2a\u8bed\u8a00\u5f15\u5bfc\u4efb\u52a1\u7684\u6570\u636e\u96c6EmRACE-3K\uff0c\u8fd9\u4e9b\u4efb\u52a1\u5728\u865a\u5e7b\u5f15\u64ce\u4e2d\u6784\u5efa\u7684\u903c\u771f\u73af\u5883\u4e2d\u8fdb\u884c\u3002\u6570\u636e\u96c6\u8be6\u7ec6\u8bb0\u5f55\u4e86\u4ece\u7b2c\u4e00\u4eba\u79f0\u89c6\u89d2\u89c2\u6d4b\u3001\u9ad8\u5c42\u6307\u4ee4\u3001\u6267\u884c\u52a8\u4f5c\u4ee5\u53ca\u6bcf\u4e00\u6b65\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\u3002\u7814\u7a76\u4eba\u5458\u5229\u7528\u8be5\u6570\u636e\u96c6\u5efa\u7acb\u4e86\u8bc4\u4f30\u57fa\u51c6\uff0c\u6db5\u76d6\u63a2\u7d22\u3001\u52a8\u6001\u7a7a\u95f4\u8bed\u4e49\u63a8\u7406\u548c\u591a\u9636\u6bb5\u76ee\u6807\u6267\u884c\u4e09\u4e2a\u7ef4\u5ea6\uff0c\u5e76\u5bf9\u73b0\u6709\u6a21\u578b\u8fdb\u884c\u4e86\u96f6\u6837\u672c\u8bc4\u4f30\uff0c\u6700\u540e\u901a\u8fc7\u5bf9Qwen2.5-VL-7B\u6a21\u578b\u8fdb\u884c\u6709\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\u5fae\u8c03\u6765\u9a8c\u8bc1\u6570\u636e\u96c6\u7684\u6709\u6548\u6027\u3002", "result": "\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\uff0c\u73b0\u6709\u5148\u8fdb\u6a21\u578b\u5728EmRACE-3K\u4e0a\u7684\u6210\u529f\u7387\u5747\u4f4e\u4e8e20%\uff0c\u8fd9\u8868\u660e\u4e86\u8be5\u57fa\u51c6\u7684\u6311\u6218\u6027\u548c\u5f53\u524d\u6a21\u578b\u5728\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5bf9Qwen2.5-VL-7B\u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5728\u63a2\u7d22\u3001\u52a8\u6001\u7a7a\u95f4\u8bed\u4e49\u63a8\u7406\u548c\u591a\u9636\u6bb5\u76ee\u6807\u6267\u884c\u4e09\u4e2a\u7ef4\u5ea6\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86EmRACE-3K\u6570\u636e\u96c6\u5728\u4fc3\u8fdb\u5177\u8eab\u63a8\u7406\u80fd\u529b\u53d1\u5c55\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "EmRACE-3K\u6570\u636e\u96c6\u7684\u5f15\u5165\u4e3a\u8bc4\u4f30\u548c\u63d0\u5347\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u5177\u8eab\u73af\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u3002\u901a\u8fc7\u5728EmRACE-3K\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5728\u5bfc\u822a\u3001\u7269\u4f53\u64cd\u7eb5\u548c\u591a\u9636\u6bb5\u4efb\u52a1\u6267\u884c\u7b49\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7a7a\u95f4\u63a8\u7406\u548c\u957f\u671f\u89c4\u5212\u7684\u4efb\u52a1\u4e2d\u3002"}}
{"id": "2507.09694", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09694", "abs": "https://arxiv.org/abs/2507.09694", "authors": ["Nicolas Gonel", "Paul Saves", "Joseph Morlier"], "title": "Frequency-aware Surrogate Modeling With SMT Kernels For Advanced Data Forecasting", "comment": "AeroBest 2025, Instituto Superior Tecnico of the University of\n  Lisbon, Portugal", "summary": "This paper introduces a comprehensive open-source framework for developing\ncorrelation kernels, with a particular focus on user-defined and composition of\nkernels for surrogate modeling. By advancing kernel-based modeling techniques,\nwe incorporate frequency-aware elements that effectively capture complex\nmechanical behaviors and timefrequency dynamics intrinsic to aircraft systems.\nTraditional kernel functions, often limited to exponential-based methods, are\nextended to include a wider range of kernels such as exponential squared sine\nand rational quadratic kernels, along with their respective firstand\nsecond-order derivatives. The proposed methodologies are first validated on a\nsinus cardinal test case and then applied to forecasting Mauna-Loa Carbon\nDioxide (CO 2 ) concentrations and airline passenger traffic. All these\nadvancements are integrated into the open-source Surrogate Modeling Toolbox\n(SMT 2.0), providing a versatile platform for both standard and customizable\nkernel configurations. Furthermore, the framework enables the combination of\nvarious kernels to leverage their unique strengths into composite models\ntailored to specific problems. The resulting framework offers a flexible\ntoolset for engineers and researchers, paving the way for numerous future\napplications in metamodeling for complex, frequency-sensitive domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSMT 2.0\u7684\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u548c\u7ec4\u5408\u7528\u4e8e\u4ee3\u7406\u5efa\u6a21\u7684\u76f8\u5173\u6838\u51fd\u6570\uff0c\u7279\u522b\u5173\u6ce8\u9891\u7387\u611f\u77e5\u80fd\u529b\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8eCO2\u6d53\u5ea6\u548c\u822a\u7a7a\u516c\u53f8\u4e58\u5ba2\u6d41\u91cf\u9884\u6d4b\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u63a8\u8fdb\u57fa\u4e8e\u6838\u7684\u5efa\u6a21\u6280\u672f\uff0c\u901a\u8fc7\u5f00\u53d1\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\u6765\u5904\u7406\u5177\u6709\u590d\u6742\u673a\u68b0\u884c\u4e3a\u548c\u65f6\u9891\u52a8\u529b\u5b66\u7684\u822a\u7a7a\u5668\u7cfb\u7edf\uff0c\u5e76\u652f\u6301\u7528\u6237\u5b9a\u4e49\u7684\u548c\u53ef\u7ec4\u5408\u7684\u6838\u51fd\u6570\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u7528\u4e8e\u5f00\u53d1\u76f8\u5173\u6838\u51fd\u6570\uff0c\u91cd\u70b9\u662f\u7528\u6237\u5b9a\u4e49\u7684\u6838\u51fd\u6570\u548c\u7528\u4e8e\u4ee3\u7406\u5efa\u6a21\u7684\u6838\u51fd\u6570\u7ec4\u5408\u3002\u901a\u8fc7\u96c6\u6210\u9891\u7387\u611f\u77e5\u5143\u7d20\uff0c\u6269\u5c55\u4e86\u4f20\u7edf\u7684\u57fa\u4e8e\u6307\u6570\u7684\u6838\u51fd\u6570\uff0c\u4ee5\u5305\u62ec\u66f4\u5e7f\u6cdb\u7684\u6838\u51fd\u6570\u53ca\u5176\u5bfc\u6570\u3002", "result": "\u5bf9\u6b63\u5f26\u57fa\u6570\u6d4b\u8bd5\u7528\u4f8b\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u9884\u6d4b\u590f\u5a01\u5937\u5192\u7eb3\u7f57\u4e9a\u7684\u4e8c\u6c27\u5316\u78b3\u6d53\u5ea6\u548c\u822a\u7a7a\u516c\u53f8\u4e58\u5ba2\u6d41\u91cf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5f00\u53d1\u76f8\u5173\u6838\u51fd\u6570\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u5de5\u5177\u96c6\uff0c\u5e76\u4e3a\u590d\u6742\u3001\u5bf9\u9891\u7387\u654f\u611f\u7684\u9886\u57df\u4e2d\u7684\u8d85\u6a21\u578b\u5f00\u8f9f\u4e86\u65b0\u7684\u5e94\u7528\u9014\u5f84\u3002"}}
{"id": "2507.09795", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09795", "abs": "https://arxiv.org/abs/2507.09795", "authors": ["Amirhossein Ansari", "Ke Wang", "Pulei Xiong"], "title": "NegRefine: Refining Negative Label-Based Zero-Shot OOD Detection", "comment": "Accepted to ICCV 2025", "summary": "Recent advancements in Vision-Language Models like CLIP have enabled\nzero-shot OOD detection by leveraging both image and textual label information.\nAmong these, negative label-based methods such as NegLabel and CSP have shown\npromising results by utilizing a lexicon of words to define negative labels for\ndistinguishing OOD samples. However, these methods suffer from detecting\nin-distribution samples as OOD due to negative labels that are subcategories of\nin-distribution labels or proper nouns. They also face limitations in handling\nimages that match multiple in-distribution and negative labels. We propose\nNegRefine, a novel negative label refinement framework for zero-shot OOD\ndetection. By introducing a filtering mechanism to exclude subcategory labels\nand proper nouns from the negative label set and incorporating a\nmulti-matching-aware scoring function that dynamically adjusts the\ncontributions of multiple labels matching an image, NegRefine ensures a more\nrobust separation between in-distribution and OOD samples. We evaluate\nNegRefine on large-scale benchmarks, including ImageNet-1K. Source code is\navailable at https://github.com/ah-ansari/NegRefine.", "AI": {"tldr": "NegRefine\u901a\u8fc7\u6539\u8fdb\u8d1f\u6807\u7b7e\u9009\u62e9\u548c\u8bc4\u5206\u673a\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5c06in-distribution\u6837\u672c\u8bef\u5224\u4e3aOOD\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u96f6\u6837\u672cOOD\u68c0\u6d4b\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8d1f\u6807\u7b7e\u7684\u65b9\u6cd5\uff08\u5982NegLabel\u548cCSP\uff09\u5728\u533a\u5206OOD\u6837\u672c\u65b9\u9762\u8868\u73b0\u51fa\u6709\u524d\u666f\u7684\u7ed3\u679c\uff0c\u4f46\u5b58\u5728\u5c06in-distribution\u6837\u672c\u8bef\u5224\u4e3aOOD\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u8d1f\u6807\u7b7e\u53ef\u80fd\u5305\u542bin-distribution\u6807\u7b7e\u7684\u5b50\u7c7b\u522b\u6216\u4e13\u6709\u540d\u8bcd\uff0c\u5e76\u4e14\u96be\u4ee5\u5904\u7406\u5339\u914d\u591a\u4e2ain-distribution\u548c\u8d1f\u6807\u7b7e\u7684\u56fe\u50cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d1f\u6807\u7b7e\u7cbe\u70bc\u6846\u67b6NegRefine\uff0c\u901a\u8fc7\u8fc7\u6ee4\u673a\u5236\u6392\u9664\u5b50\u7c7b\u522b\u6807\u7b7e\u548c\u4e13\u6709\u540d\u8bcd\uff0c\u5e76\u91c7\u7528\u591a\u5339\u914d\u611f\u77e5\u8bc4\u5206\u51fd\u6570\u6765\u52a8\u6001\u8c03\u6574\u591a\u4e2a\u5339\u914d\u6807\u7b7e\u7684\u8d21\u732e\u5ea6\u3002", "result": "\u5728\u5305\u62ecImageNet-1K\u5728\u5185\u7684\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8bc4\u4f30\u4e86NegRefine\u7684\u6709\u6548\u6027\u3002", "conclusion": "NegRefine\u901a\u8fc7\u5f15\u5165\u8fc7\u6ee4\u673a\u5236\u6392\u9664\u5b50\u7c7b\u522b\u6807\u7b7e\u548c\u4e13\u6709\u540d\u8bcd\uff0c\u5e76\u7ed3\u5408\u591a\u5339\u914d\u611f\u77e5\u8bc4\u5206\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u7684 in-distribution \u548c OOD \u6837\u672c\u5206\u79bb\u3002"}}
{"id": "2507.09703", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09703", "abs": "https://arxiv.org/abs/2507.09703", "authors": ["Roberto Molinaro", "Niall Siegenheim", "Niels Poulsen", "Jordan Dane Daubinet", "Henry Martin", "Mark Frey", "Kevin Thiart", "Alexander Jakob Dautel", "Andreas Schlueter", "Alex Grigoryev", "Bogdan Danciu", "Nikoo Ekhtiari", "Bas Steunebrink", "Leonie Wagner", "Marvin Vincent Gabler"], "title": "EPT-2 Technical Report", "comment": null, "summary": "We present EPT-2, the latest iteration in our Earth Physics Transformer (EPT)\nfamily of foundation AI models for Earth system forecasting. EPT-2 delivers\nsubstantial improvements over its predecessor, EPT-1.5, and sets a new state of\nthe art in predicting energy-relevant variables-including 10m and 100m wind\nspeed, 2m temperature, and surface solar radiation-across the full 0-240h\nforecast horizon. It consistently outperforms leading AI weather models such as\nMicrosoft Aurora, as well as the operational numerical forecast system IFS HRES\nfrom the European Centre for Medium-Range Weather Forecasts (ECMWF). In\nparallel, we introduce a perturbation-based ensemble model of EPT-2 for\nprobabilistic forecasting, called EPT-2e. Remarkably, EPT-2e significantly\nsurpasses the ECMWF ENS mean-long considered the gold standard for medium- to\nlongrange forecasting-while operating at a fraction of the computational cost.\nEPT models, as well as third-party forecasts, are accessible via the app.jua.ai\nplatform.", "AI": {"tldr": "\u65b0\u4e00\u4ee3AI\u6a21\u578bEPT-2\u5728\u98ce\u901f\u3001\u6e29\u5ea6\u7b49\u9884\u6d4b\u4e0a\u8d85\u8d8a\u73b0\u6709\u6a21\u578b\uff0c\u5176\u96c6\u6210\u7248\u672cEPT-2e\u5728\u6982\u7387\u9884\u6d4b\u4e0a\u8868\u73b0\u66f4\u4f73\uff0c\u4e14\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u5730\u7403\u7cfb\u7edf\u9884\u6d4b\u80fd\u529b\uff0c\u7279\u522b\u662f\u80fd\u6e90\u76f8\u5173\u53d8\u91cf\uff08\u5982\u98ce\u901f\u548c\u6e29\u5ea6\uff09\u7684\u77ed\u671f\u548c\u4e2d\u671f\u9884\u6d4b\u3002", "method": "\u63d0\u51fa\u4e86EPT-2\uff0c\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684AI\u6a21\u578b\uff0c\u7528\u4e8e\u5730\u7403\u7cfb\u7edf\u9884\u6d4b\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u6270\u52a8\u7684\u96c6\u6210\u6a21\u578bEPT-2e\uff0c\u7528\u4e8e\u6982\u7387\u6027\u9884\u6d4b\u3002", "result": "EPT-2\u5728\u9884\u6d4b\u98ce\u901f\u3001\u6e29\u5ea6\u548c\u592a\u9633\u8f90\u5c04\u65b9\u9762\u8bbe\u5b9a\u4e86\u65b0\u7684\u6700\u9ad8\u6c34\u5e73\uff0c\u4f18\u4e8eMicrosoft Aurora\u548cECMWF IFS HRES\u3002EPT-2e\u5728\u6982\u7387\u6027\u9884\u6d4b\u65b9\u9762\u4e5f\u8d85\u8d8a\u4e86ECMWF ENS\u3002", "conclusion": "EPT-2\u548cEPT-2e\u5728\u5730\u7403\u7cfb\u7edf\u9884\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5728\u591a\u9879\u5173\u952e\u6c14\u8c61\u53d8\u91cf\u9884\u6d4b\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u548c\u6570\u503c\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\uff0c\u5e76\u5728\u8ba1\u7b97\u6210\u672c\u4e0a\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2507.09815", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09815", "abs": "https://arxiv.org/abs/2507.09815", "authors": ["Younggun Kim", "Ahmed S. Abdelrahman", "Mohamed Abdel-Aty"], "title": "VRU-Accident: A Vision-Language Benchmark for Video Question Answering and Dense Captioning for Accident Scene Understanding", "comment": "22 pages, 11 figures, 5 tables", "summary": "Ensuring the safety of vulnerable road users (VRUs), such as pedestrians and\ncyclists, is a critical challenge for autonomous driving systems, as crashes\ninvolving VRUs often result in severe or fatal consequences. While multimodal\nlarge language models (MLLMs) have shown promise in enhancing scene\nunderstanding and decision making in autonomous vehicles, there is currently no\nstandardized benchmark to quantitatively evaluate their reasoning abilities in\ncomplex, safety-critical scenarios involving VRUs. To address this gap, we\npresent VRU-Accident, a large-scale vision-language benchmark designed to\nevaluate MLLMs in high-risk traffic scenarios involving VRUs. VRU-Accident\ncomprises 1K real-world dashcam accident videos, annotated with 6K\nmultiple-choice question-answer pairs across six safety-critical categories\n(with 24K candidate options and 3.4K unique answer choices), as well as 1K\ndense scene descriptions. Unlike prior works, our benchmark focuses explicitly\non VRU-vehicle accidents, providing rich, fine-grained annotations that capture\nboth spatial-temporal dynamics and causal semantics of accidents. To assess the\ncurrent landscape of MLLMs, we conduct a comprehensive evaluation of 17\nstate-of-the-art models on the multiple-choice VQA task and on the dense\ncaptioning task. Our findings reveal that while MLLMs perform reasonably well\non visually grounded attributes, they face significant challenges in reasoning\nand describing accident causes, types, and preventability.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86VRU-Accident\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u4e2d\u5904\u7406\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u4e8b\u6545\u573a\u666f\u7684\u80fd\u529b\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c3d\u7ba1\u6a21\u578b\u5728\u89c6\u89c9\u7406\u89e3\u4e0a\u8868\u73b0\u4e0d\u9519\uff0c\u4f46\u5728\u4e8b\u6545\u539f\u56e0\u548c\u53ef\u9884\u9632\u6027\u63a8\u7406\u65b9\u9762\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u76ee\u524d\u7f3a\u4e4f\u6807\u51c6\u5316\u57fa\u51c6\u6765\u91cf\u5316\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u6d89\u53ca\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u7684\u590d\u6742\u3001\u5b89\u5168\u5173\u952e\u573a\u666f\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u6d89\u53caVRU\u7684\u4e8b\u6545\u5f80\u5f80\u5bfc\u81f4\u4e25\u91cd\u6216\u81f4\u547d\u7684\u540e\u679c\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVRU-Accident\u7684\u5927\u89c4\u6a21\u89c6\u89c9-\u8bed\u8a00\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5305\u542b1K\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u884c\u8f66\u8bb0\u5f55\u4eea\u4e8b\u6545\u89c6\u9891\uff0c\u5e76\u9644\u5e26\u4e86\u9488\u5bf9\u516d\u4e2a\u5b89\u5168\u5173\u952e\u7c7b\u522b\u76846K\u4e2a\u9009\u62e9\u9898\u95ee\u7b54\u5bf9\uff08\u5305\u542b24K\u4e2a\u9009\u9879\u548c3.4K\u4e2a\u552f\u4e00\u7b54\u6848\u9009\u9879\uff09\uff0c\u4ee5\u53ca1K\u4e2a\u5bc6\u96c6\u7684\u573a\u666f\u63cf\u8ff0\u3002\u5bf917\u4e2a\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8bc4\u4f30\uff0c\u5305\u62ec\u591a\u9009\u9898\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u548c\u5bc6\u96c6\u573a\u666f\u63cf\u8ff0\u4efb\u52a1\u3002", "result": "\u5728VRU-Accident\u57fa\u51c6\u4e0a\uff0c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u89c6\u89c9\u57fa\u7840\u5c5e\u6027\u65b9\u9762\u8868\u73b0\u5408\u7406\uff0c\u4f46\u5728\u63a8\u7406\u548c\u63cf\u8ff0\u4e8b\u6545\u539f\u56e0\u3001\u7c7b\u578b\u548c\u53ef\u9884\u9632\u6027\u65b9\u9762\u9762\u4e34\u663e\u8457\u6311\u6218\u3002", "conclusion": "\u76ee\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5904\u7406\u6d89\u53ca\u5f31\u52bf\u9053\u8def\u4f7f\u7528\u8005\uff08VRU\uff09\u7684\u4e8b\u6545\u573a\u666f\u65f6\uff0c\u5728\u4e8b\u6545\u539f\u56e0\u3001\u7c7b\u578b\u548c\u53ef\u9884\u9632\u6027\u7b49\u65b9\u9762\u7684\u63a8\u7406\u548c\u63cf\u8ff0\u80fd\u529b\u4ecd\u9762\u4e34\u91cd\u5927\u6311\u6218\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5728\u89c6\u89c9\u57fa\u7840\u5c5e\u6027\u65b9\u9762\u8868\u73b0\u5c1a\u53ef\u3002"}}
{"id": "2507.09732", "categories": ["cs.LG", "q-bio.PE", "stat.AP", "68T05, 62H35", "I.5.4; I.2.6"], "pdf": "https://arxiv.org/pdf/2507.09732", "abs": "https://arxiv.org/abs/2507.09732", "authors": ["Sara Si-Moussi", "Stephan Hennekens", "Sander Mucher", "Stan Los", "Wilfried Thuiller"], "title": "Continental scale habitat modelling with artificial intelligence and multimodal earth observation", "comment": null, "summary": "Habitats integrate the abiotic conditions and biophysical structures that\nsupport biodiversity and sustain nature's contributions to people. As these\necosystems face mounting pressure from human activities, accurate,\nhigh-resolution habitat maps are essential for effective conservation and\nrestoration. Yet current maps often fall short in thematic or spatial\nresolution because they must (1) model several mutually exclusive habitat types\nthat co-occur across landscapes and (2) cope with severe class imbalance that\ncomplicate multi-class training. Here, we evaluated how high-resolution remote\nsensing (RS) data and Artificial Intelligence (AI) tools can improve habitat\nclassification over large geographic extents at fine thematic resolution. Using\nvegetation plots from the European Vegetation Archive, we modelled Level 3\nEUNIS habitats across Europe and assessed multiple modelling strategies against\nindependent validation datasets. Strategies that exploited the hierarchical\nnature of habitat nomenclatures resolved classification ambiguities, especially\nin fragmented landscapes. Integrating multi-spectral (MSI) and synthetic\naperture radar (SAR) imagery, particularly through Earth Observation Foundation\nmodels, enhanced within-formation discrimination and overall performance.\nFinally, ensemble machine learning that corrects class imbalance boosted\naccuracy further. Our methodological framework is transferable beyond Europe\nand adaptable to other classification systems. Future research should advance\ntemporal modelling of dynamic habitats, extend to habitat segmentation and\nquality assessment, and exploit next-generation EO data paired with\nhigher-quality in-situ observations.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u6570\u636e\uff08\u5305\u62ec\u591a\u5149\u8c31\u548cSAR\u5f71\u50cf\uff09\u548c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6280\u672f\uff0c\u7279\u522b\u662f\u5229\u7528\u5206\u5c42\u5efa\u6a21\u548c\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6b27\u6d32\u8303\u56f4\u5185\u751f\u5883\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u5206\u8fa8\u7387\u3002", "motivation": "\u968f\u7740\u751f\u6001\u7cfb\u7edf\u9762\u4e34\u65e5\u76ca\u589e\u957f\u7684\u4eba\u7c7b\u6d3b\u52a8\u538b\u529b\uff0c\u51c6\u786e\u3001\u9ad8\u5206\u8fa8\u7387\u7684\u751f\u5883\u5730\u56fe\u5bf9\u4e8e\u6709\u6548\u7684\u4fdd\u62a4\u548c\u6062\u590d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u5730\u56fe\u5728\u4e3b\u9898\u6216\u7a7a\u95f4\u5206\u8fa8\u7387\u4e0a\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u6b27\u6d32\u690d\u88ab\u6570\u636e\u5e93\u7684\u6837\u5730\u6570\u636e\uff0c\u5bf9\u6b27\u6d32\u76843\u7ea7EUNIS\u751f\u5883\u8fdb\u884c\u4e86\u5efa\u6a21\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u5efa\u6a21\u7b56\u7565\u3002\u7814\u7a76\u91c7\u7528\u4e86\u5206\u5c42\u5efa\u6a21\u65b9\u6cd5\u3001\u591a\u5149\u8c31\uff08MSI\uff09\u548c\u5408\u6210\u5b54\u5f84\u96f7\u8fbe\uff08SAR\uff09\u5f71\u50cf\u878d\u5408\uff08\u7279\u522b\u662f\u901a\u8fc7\u5730\u7403\u89c2\u6d4b\u57fa\u91d1\u6a21\u578b\uff09\uff0c\u4ee5\u53ca\u514b\u670d\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u96c6\u6210\u673a\u5668\u5b66\u4e60\u6280\u672f\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5229\u7528\u9065\u611f\u548c\u4eba\u5de5\u667a\u80fd\u6280\u672f\uff0c\u7279\u522b\u662f\u7ed3\u5408\u5206\u5c42\u5efa\u6a21\u3001\u591a\u5149\u8c31\u4e0eSAR\u5f71\u50cf\u878d\u5408\u4ee5\u53ca\u5904\u7406\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u96c6\u6210\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u9ad8\u751f\u5883\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u76f8\u4e92\u6392\u65a5\u7684\u751f\u5883\u7c7b\u578b\u5171\u5b58\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u6570\u636e\u548c\u4eba\u5de5\u667a\u80fd\u5de5\u5177\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u6b27\u6d32\u8303\u56f4\u5185\u7cbe\u7ec6\u751f\u5883\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.09830", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09830", "abs": "https://arxiv.org/abs/2507.09830", "authors": ["Shuhao Fu", "Philip J. Kellman", "Hongjing Lu"], "title": "Hierarchical Abstraction Enables Human-Like 3D Object Recognition in Deep Learning Models", "comment": null, "summary": "Both humans and deep learning models can recognize objects from 3D shapes\ndepicted with sparse visual information, such as a set of points randomly\nsampled from the surfaces of 3D objects (termed a point cloud). Although deep\nlearning models achieve human-like performance in recognizing objects from 3D\nshapes, it remains unclear whether these models develop 3D shape\nrepresentations similar to those used by human vision for object recognition.\nWe hypothesize that training with 3D shapes enables models to form\nrepresentations of local geometric structures in 3D shapes. However, their\nrepresentations of global 3D object shapes may be limited. We conducted two\nhuman experiments systematically manipulating point density and object\norientation (Experiment 1), and local geometric structure (Experiment 2).\nHumans consistently performed well across all experimental conditions. We\ncompared two types of deep learning models, one based on a convolutional neural\nnetwork (DGCNN) and the other on visual transformers (point transformer), with\nhuman performance. We found that the point transformer model provided a better\naccount of human performance than the convolution-based model. The advantage\nmainly results from the mechanism in the point transformer model that supports\nhierarchical abstraction of 3D shapes.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bc6\u522b\u4e09\u7ef4\u5f62\u72b6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u8868\u5f81\u65b9\u5f0f\u53ef\u80fd\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002\u672c\u7814\u7a76\u901a\u8fc7\u4eba\u7c7b\u5b9e\u9a8c\u548c\u6a21\u578b\u6bd4\u8f83\u53d1\u73b0\uff0c\u70b9\u53d8\u6362\u5668\u6a21\u578b\u6bd4\u5377\u79ef\u6a21\u578b\u66f4\u80fd\u6a21\u62df\u4eba\u7c7b\u8bc6\u522b\u4e09\u7ef4\u5f62\u72b6\u7684\u8868\u73b0\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u5206\u5c42\u62bd\u8c61\u673a\u5236\u3002", "motivation": "\u63a2\u7a76\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u8bc6\u522b\u4e09\u7ef4\u5f62\u72b6\u65f6\uff0c\u5176\u5f62\u6210\u7684\u4e09\u7ef4\u5f62\u72b6\u8868\u5f81\u662f\u5426\u4e0e\u4eba\u7c7b\u89c6\u89c9\u6240\u4f7f\u7528\u7684\u8868\u5f81\u76f8\u4f3c\u3002", "method": "\u901a\u8fc7\u4e24\u9879\u4eba\u7c7b\u5b9e\u9a8c\uff0c\u7cfb\u7edf\u5730\u64cd\u7eb5\u70b9\u5bc6\u5ea6\u548c\u7269\u4f53\u65b9\u5411\uff08\u5b9e\u9a8c\u4e00\uff09\uff0c\u4ee5\u53ca\u64cd\u7eb5\u5c40\u90e8\u51e0\u4f55\u7ed3\u6784\uff08\u5b9e\u9a8c\u4e8c\uff09\uff0c\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u57fa\u4e8e\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684DGCNN\u548c\u57fa\u4e8e\u89c6\u89c9\u53d8\u6362\u5668\u7684\u70b9\u53d8\u6362\u5668\uff09\u548c\u4eba\u7c7b\u7684\u8868\u73b0\u3002", "result": "\u4eba\u7c7b\u5728\u6240\u6709\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u8868\u73b0\u5747\u7a33\u5b9a\u826f\u597d\u3002\u70b9\u53d8\u6362\u5668\u6a21\u578b\u5728\u6a21\u62df\u4eba\u7c7b\u8868\u73b0\u65b9\u9762\u4f18\u4e8eDGCNN\u6a21\u578b\u3002", "conclusion": "\u70b9\u4e91\u6570\u636e\u7684\u8bc6\u522b\u80fd\u529b\u65b9\u9762\uff0c\u70b9\u53d8\u6362\u5668\u6a21\u578b\u6bd4\u57fa\u4e8e\u5377\u79ef\u7684\u6a21\u578b\u66f4\u80fd\u6a21\u62df\u4eba\u7c7b\u8868\u73b0\uff0c\u8fd9\u4e3b\u8981\u5f97\u76ca\u4e8e\u70b9\u53d8\u6362\u5668\u6a21\u578b\u80fd\u591f\u5bf9\u4e09\u7ef4\u5f62\u72b6\u8fdb\u884c\u5206\u5c42\u62bd\u8c61\u3002"}}
{"id": "2507.09733", "categories": ["cs.LG", "cs.AI", "cs.CV", "68T07, 65M06, 78M34", "I.2.6; I.4.8; J.2"], "pdf": "https://arxiv.org/pdf/2507.09733", "abs": "https://arxiv.org/abs/2507.09733", "authors": ["Bradley Camburn"], "title": "Universal Physics Simulation: A Foundational Diffusion Approach", "comment": "10 pages, 3 figures. Foundational AI model for universal physics\n  simulation using sketch-guided diffusion transformers. Achieves SSIM > 0.8 on\n  electromagnetic field generation without requiring a priori physics encoding", "summary": "We present the first foundational AI model for universal physics simulation\nthat learns physical laws directly from boundary-condition data without\nrequiring a priori equation encoding. Traditional physics-informed neural\nnetworks (PINNs) and finite-difference methods necessitate explicit\nmathematical formulation of governing equations, fundamentally limiting their\ngeneralizability and discovery potential. Our sketch-guided diffusion\ntransformer approach reimagines computational physics by treating simulation as\na conditional generation problem, where spatial boundary conditions guide the\nsynthesis of physically accurate steady-state solutions.\n  By leveraging enhanced diffusion transformer architectures with novel spatial\nrelationship encoding, our model achieves direct boundary-to-equilibrium\nmapping and is generalizable to diverse physics domains. Unlike sequential\ntime-stepping methods that accumulate errors over iterations, our approach\nbypasses temporal integration entirely, directly generating steady-state\nsolutions with SSIM > 0.8 while maintaining sub-pixel boundary accuracy. Our\ndata-informed approach enables physics discovery through learned\nrepresentations analyzable via Layer-wise Relevance Propagation (LRP),\nrevealing emergent physical relationships without predetermined mathematical\nconstraints. This work represents a paradigm shift from AI-accelerated physics\nto AI-discovered physics, establishing the first truly universal physics\nsimulation framework.", "AI": {"tldr": "\u6211\u4eec\u63d0\u51fa\u4e86\u7b2c\u4e00\u4e2a\u901a\u7528\u7684\u7269\u7406\u6a21\u62df\u57fa\u7840AI\u6a21\u578b\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u8fb9\u754c\u6761\u4ef6\u6570\u636e\u5b66\u4e60\u7269\u7406\u5b9a\u5f8b\uff0c\u65e0\u9700\u5148\u9a8c\u65b9\u7a0b\u7f16\u7801\u3002\u8be5\u6a21\u578b\u4f7f\u7528\u8349\u56fe\u5f15\u5bfc\u6269\u6563\u53d8\u6362\u5668\uff0c\u5c06\u6a21\u62df\u89c6\u4e3a\u6761\u4ef6\u751f\u6210\u95ee\u9898\uff0c\u5e76\u76f4\u63a5\u751f\u6210\u7a33\u6001\u89e3\uff0cSSIM > 0.8\uff0c\u540c\u65f6\u4fdd\u6301\u4e9a\u50cf\u7d20\u8fb9\u754c\u7cbe\u5ea6\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u901a\u8fc7\u53ef\u89e3\u91ca\u6027\u5206\u6790\u8fdb\u884c\u7269\u7406\u53d1\u73b0\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\uff08PINNs\uff09\u548c\u6709\u9650\u5dee\u5206\u65b9\u6cd5\u9700\u8981\u660e\u786e\u7684\u6570\u5b66\u516c\u5f0f\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u6cdb\u5316\u548c\u53d1\u73b0\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8349\u56fe\u5f15\u5bfc\u6269\u6563\u53d8\u6362\u5668\u65b9\u6cd5\uff0c\u5c06\u6a21\u62df\u89c6\u4e3a\u6761\u4ef6\u751f\u6210\u95ee\u9898\uff0c\u5176\u4e2d\u7a7a\u95f4\u8fb9\u754c\u6761\u4ef6\u6307\u5bfc\u7269\u7406\u4e0a\u7cbe\u786e\u7684\u7a33\u6001\u89e3\u7684\u5408\u6210\u3002\u8be5\u6a21\u578b\u5229\u7528\u589e\u5f3a\u7684\u6269\u6563\u53d8\u6362\u5668\u67b6\u6784\u548c\u65b0\u9896\u7684\u7a7a\u95f4\u5173\u7cfb\u7f16\u7801\uff0c\u5b9e\u73b0\u4e86\u76f4\u63a5\u7684\u8fb9\u754c\u5230\u5e73\u8861\u6620\u5c04\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u7269\u7406\u9886\u57df\u3002", "result": "\u8be5\u6a21\u578b\u5b9e\u73b0\u4e86\u76f4\u63a5\u7684\u8fb9\u754c\u5230\u5e73\u8861\u6620\u5c04\uff0c\u5e76\u4e14\u53ef\u4ee5\u63a8\u5e7f\u5230\u4e0d\u540c\u7684\u7269\u7406\u9886\u57df\u3002\u4e0e\u7d2f\u79ef\u8bef\u5dee\u7684\u987a\u5e8f\u65f6\u95f4\u6b65\u957f\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u7ed5\u8fc7\u4e86\u65f6\u95f4\u79ef\u5206\uff0c\u76f4\u63a5\u751f\u6210\u7a33\u6001\u89e3\uff0cSSIM > 0.8\uff0c\u540c\u65f6\u4fdd\u6301\u4e9a\u50cf\u7d20\u8fb9\u754c\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4ee3\u8868\u4e86\u4eceAI\u52a0\u901f\u7269\u7406\u5230AI\u53d1\u73b0\u7269\u7406\u7684\u8303\u5f0f\u8f6c\u53d8\uff0c\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u771f\u6b63\u901a\u7528\u7684\u7269\u7406\u6a21\u62df\u6846\u67b6\u3002"}}
{"id": "2507.09861", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09861", "abs": "https://arxiv.org/abs/2507.09861", "authors": ["Yihao Ding", "Siwen Luo", "Yue Dai", "Yanbei Jiang", "Zechuan Li", "Geoffrey Martin", "Yifan Peng"], "title": "A Survey on MLLM-based Visually Rich Document Understanding: Methods, Challenges, and Emerging Trends", "comment": "Work in progress", "summary": "Visually-Rich Document Understanding (VRDU) has emerged as a critical field,\ndriven by the need to automatically process documents containing complex\nvisual, textual, and layout information. Recently, Multimodal Large Language\nModels (MLLMs) have shown remarkable potential in this domain, leveraging both\nOptical Character Recognition (OCR)-dependent and OCR-free frameworks to\nextract and interpret information in document images. This survey reviews\nrecent advancements in MLLM-based VRDU, highlighting three core components: (1)\nmethods for encoding and fusing textual, visual, and layout features; (2)\ntraining paradigms, including pretraining strategies, instruction-response\ntuning, and the trainability of different model modules; and (3) datasets\nutilized for pretraining, instruction-tuning, and supervised fine-tuning.\nFinally, we discuss the challenges and opportunities in this evolving field and\npropose future directions to advance the efficiency, generalizability, and\nrobustness of VRDU systems.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u662f\u5173\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\uff08VRDU\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u7684\u8c03\u67e5\u3002\u5b83\u6db5\u76d6\u4e86\u7279\u5f81\u7f16\u7801\u548c\u878d\u5408\u3001\u8bad\u7ec3\u8303\u5f0f\u548c\u6570\u636e\u96c6\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u3001\u673a\u9047\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u81ea\u52a8\u5904\u7406\u5305\u542b\u590d\u6742\u7684\u89c6\u89c9\u3001\u6587\u672c\u548c\u5e03\u5c40\u4fe1\u606f\u7684\u6587\u6863\uff0cMLLM\u5728VRDU\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\u3002", "method": "\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\uff08VRDU\uff09\u9886\u57df\u7684\u8fdb\u5c55\u8fdb\u884c\u4e86\u56de\u987e\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u7279\u5f81\u7f16\u7801\u4e0e\u878d\u5408\u3001\u8bad\u7ec3\u8303\u5f0f\u4ee5\u53ca\u6570\u636e\u96c6\u7b49\u65b9\u9762\u3002", "result": "MLLM\u5728VRDU\u9886\u57df\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u80fd\u591f\u5229\u7528OCR\u4f9d\u8d56\u548cOCR\u81ea\u7531\u6846\u67b6\u63d0\u53d6\u548c\u89e3\u91ca\u6587\u6863\u56fe\u50cf\u4e2d\u7684\u4fe1\u606f\u3002", "conclusion": "\u8be5\u8bba\u6587\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u89c6\u89c9\u4e30\u5bcc\u6587\u6863\u7406\u89e3\uff08VRDU\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\u8fdb\u884c\u4e86\u56de\u987e\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u7279\u5f81\u7f16\u7801\u4e0e\u878d\u5408\u3001\u8bad\u7ec3\u8303\u5f0f\u4ee5\u53ca\u6570\u636e\u96c6\u7b49\u65b9\u9762\uff0c\u5e76\u8ba8\u8bba\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u3001\u673a\u9047\u548c\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2507.09753", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09753", "abs": "https://arxiv.org/abs/2507.09753", "authors": ["Ewa M. Nowara", "Joshua Rackers", "Patricia Suriana", "Pan Kessel", "Max Shen", "Andrew Martin Watkins", "Michael Maser"], "title": "Do we need equivariant models for molecule generation?", "comment": null, "summary": "Deep generative models are increasingly used for molecular discovery, with\nmost recent approaches relying on equivariant graph neural networks (GNNs)\nunder the assumption that explicit equivariance is essential for generating\nhigh-quality 3D molecules. However, these models are complex, difficult to\ntrain, and scale poorly.\n  We investigate whether non-equivariant convolutional neural networks (CNNs)\ntrained with rotation augmentations can learn equivariance and match the\nperformance of equivariant models. We derive a loss decomposition that\nseparates prediction error from equivariance error, and evaluate how model\nsize, dataset size, and training duration affect performance across denoising,\nmolecule generation, and property prediction. To our knowledge, this is the\nfirst study to analyze learned equivariance in generative tasks.", "AI": {"tldr": "\u975e\u7b49\u53d8CNN\u901a\u8fc7\u65cb\u8f6c\u589e\u5f3a\u53ef\u4ee5\u5b66\u4e60\u7b49\u53d8\u6027\uff0c\u6027\u80fd\u5ab2\u7f8e\u7b49\u53d8GNN\uff0c\u89e3\u51b3\u4e86\u7b49\u53d8GNN\u7684\u75db\u70b9\u3002", "motivation": "\u63a2\u7a76\u975e\u7b49\u53d8CNN\u662f\u5426\u53ef\u4ee5\u901a\u8fc7\u65cb\u8f6c\u589e\u5f3a\u5b66\u4e60\u7b49\u53d8\u6027\uff0c\u5e76\u4e0e\u7b49\u53d8\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u7b49\u53d8GNN\u6a21\u578b\u590d\u6742\u3001\u96be\u8bad\u7ec3\u548c\u6269\u5c55\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u635f\u5931\u5206\u89e3\u6765\u5206\u79bb\u9884\u6d4b\u8bef\u5dee\u548c\u7b49\u53d8\u6027\u8bef\u5dee\uff0c\u5e76\u8bc4\u4f30\u6a21\u578b\u5927\u5c0f\u3001\u6570\u636e\u96c6\u5927\u5c0f\u548c\u8bad\u7ec3\u65f6\u957f\u5bf9\u53bb\u566a\u3001\u5206\u5b50\u751f\u6210\u548c\u5c5e\u6027\u9884\u6d4b\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u975e\u7b49\u53d8CNN\u5728\u65cb\u8f6c\u589e\u5f3a\u7684\u5e2e\u52a9\u4e0b\u53ef\u4ee5\u5b66\u4e60\u7b49\u53d8\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e0e\u7b49\u53d8\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u6c34\u5e73\uff0c\u8fd9\u662f\u9996\u6b21\u5728\u751f\u6210\u4efb\u52a1\u4e2d\u5bf9\u5b66\u4e60\u5230\u7684\u7b49\u53d8\u6027\u8fdb\u884c\u5206\u6790\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u65cb\u8f6c\u589e\u5f3a\u8bad\u7ec3\u7684\u975e\u7b49\u53d8\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u53ef\u4ee5\u5b66\u4e60\u7b49\u53d8\u6027\uff0c\u5e76\u8fbe\u5230\u4e0e\u7b49\u53d8\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4ece\u800c\u514b\u670d\u4e86\u7b49\u53d8GNNs\u7684\u590d\u6742\u6027\u3001\u8bad\u7ec3\u96be\u5ea6\u5927\u548c\u6269\u5c55\u6027\u5dee\u7b49\u7f3a\u70b9\u3002"}}
{"id": "2507.09862", "categories": ["cs.CV", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09862", "abs": "https://arxiv.org/abs/2507.09862", "authors": ["Youliang Zhang", "Zhaoyang Li", "Duomin Wang", "Jiahe Zhang", "Deyu Zhou", "Zixin Yin", "Xili Dai", "Gang Yu", "Xiu Li"], "title": "SpeakerVid-5M: A Large-Scale High-Quality Dataset for Audio-Visual Dyadic Interactive Human Generation", "comment": null, "summary": "The rapid development of large-scale models has catalyzed significant\nbreakthroughs in the digital human domain. These advanced methodologies offer\nhigh-fidelity solutions for avatar driving and rendering, leading academia to\nfocus on the next major challenge: audio-visual dyadic interactive virtual\nhuman. To facilitate research in this emerging area, we present SpeakerVid-5M\ndataset, the first large-scale, high-quality dataset designed for audio-visual\ndyadic interactive virtual human generation. Totaling over 8,743 hours,\nSpeakerVid-5M contains more than 5.2 million video clips of human portraits. It\ncovers diverse scales and interaction types, including monadic talking,\nlistening, and dyadic conversations. Crucially, the dataset is structured along\ntwo key dimensions: interaction type and data quality. First, it is categorized\ninto four types (dialogue branch, single branch, listening branch and\nmulti-turn branch) based on the interaction scenario. Second, it is stratified\ninto a large-scale pre-training subset and a curated, high-quality subset for\nSupervised Fine-Tuning (SFT). This dual structure accommodates a wide array of\n2D virtual human tasks. In addition, we provide an autoregressive (AR)-based\nvideo chat baseline trained on this data, accompanied by a dedicated set of\nmetrics and test data to serve as a benchmark VidChatBench for future work.\nBoth the dataset and the corresponding data processing code will be publicly\nreleased. Project page: https://dorniwang.github.io/SpeakerVid-5M/", "AI": {"tldr": "\u63d0\u51faSpeakerVid-5M\u6570\u636e\u96c6\u548cVidChatBench\u57fa\u51c6\uff0c\u63a8\u52a8\u97f3\u9891-\u89c6\u89c9\u53cc\u5411\u4ea4\u4e92\u865a\u62df\u4eba\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u65b0\u5174\u7684\u97f3\u9891-\u89c6\u89c9\u53cc\u5411\u4ea4\u4e92\u865a\u62df\u4eba\u9886\u57df\u7684\u7814\u7a76\uff0c\u586b\u8865\u8be5\u9886\u57df\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u7684\u7a7a\u767d\u3002", "method": "\u63d0\u51faSpeakerVid-5M\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u8d85\u8fc78743\u5c0f\u65f6\u3001520\u4e07\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u6db5\u76d6\u5bf9\u8bdd\u5206\u652f\u3001\u5355\u8fb9\u5206\u652f\u3001\u76d1\u542c\u5206\u652f\u548c\u591a\u8f6e\u5206\u652f\u56db\u79cd\u4ea4\u4e92\u573a\u666f\uff0c\u5e76\u5206\u4e3a\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u5b50\u96c6\u548c\u9ad8\u8d28\u91cf\u5fae\u8c03\u5b50\u96c6\u3002\u540c\u65f6\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u89c6\u9891\u804a\u5929\u57fa\u7ebf\uff08VidChatBench\uff09\u4ee5\u53ca\u76f8\u5e94\u7684\u8bc4\u4f30\u6307\u6807\u548c\u6d4b\u8bd5\u6570\u636e\u3002", "result": "SpeakerVid-5M\u6570\u636e\u96c6\u7684\u53d1\u5e03\uff0c\u4e3a\u97f3\u9891-\u89c6\u89c9\u53cc\u5411\u4ea4\u4e92\u865a\u62df\u4eba\u751f\u6210\u4efb\u52a1\u63d0\u4f9b\u4e86\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u652f\u6301\uff0c\u5e76\u8f85\u4ee5VidChatBench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3a\u540e\u7eed\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86SpeakerVid-5M\u6570\u636e\u96c6\uff0c\u65e8\u5728\u63a8\u52a8\u97f3\u9891-\u89c6\u89c9\u53cc\u5411\u4ea4\u4e92\u865a\u62df\u4eba\u9886\u57df\u7684\u7814\u7a76\u3002\u6570\u636e\u96c6\u5305\u542b\u5927\u89c4\u6a21\u3001\u9ad8\u8d28\u91cf\u7684\u89c6\u9891\u7247\u6bb5\uff0c\u8986\u76d6\u591a\u79cd\u4ea4\u4e92\u7c7b\u578b\u548c\u573a\u666f\uff0c\u5e76\u5206\u4e3a\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e24\u90e8\u5206\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8e\u81ea\u56de\u5f52\u6a21\u578b\u7684\u89c6\u9891\u804a\u5929\u57fa\u7ebf\u548c\u8bc4\u4f30\u57fa\u51c6VidChatBench\u3002"}}
{"id": "2507.09754", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.09754", "abs": "https://arxiv.org/abs/2507.09754", "authors": ["Aakash Tripathi", "Ian E. Nielsen", "Muhammad Umer", "Ravi P. Ramachandran", "Ghulam Rasool"], "title": "Explainable AI in Genomics: Transcription Factor Binding Site Prediction with Mixture of Experts", "comment": null, "summary": "Transcription Factor Binding Site (TFBS) prediction is crucial for\nunderstanding gene regulation and various biological processes. This study\nintroduces a novel Mixture of Experts (MoE) approach for TFBS prediction,\nintegrating multiple pre-trained Convolutional Neural Network (CNN) models,\neach specializing in different TFBS patterns. We evaluate the performance of\nour MoE model against individual expert models on both in-distribution and\nout-of-distribution (OOD) datasets, using six randomly selected transcription\nfactors (TFs) for OOD testing. Our results demonstrate that the MoE model\nachieves competitive or superior performance across diverse TF binding sites,\nparticularly excelling in OOD scenarios. The Analysis of Variance (ANOVA)\nstatistical test confirms the significance of these performance differences.\nAdditionally, we introduce ShiftSmooth, a novel attribution mapping technique\nthat provides more robust model interpretability by considering small shifts in\ninput sequences. Through comprehensive explainability analysis, we show that\nShiftSmooth offers superior attribution for motif discovery and localization\ncompared to traditional Vanilla Gradient methods. Our work presents an\nefficient, generalizable, and interpretable solution for TFBS prediction,\npotentially enabling new discoveries in genome biology and advancing our\nunderstanding of transcriptional regulation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u4e2aCNN\u6a21\u578b\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u8f6c\u5f55\u56e0\u5b50\u7ed3\u5408\u4f4d\u70b9\uff08TFBS\uff09\u9884\u6d4b\uff0c\u5e76\u5728\u5206\u5e03\u5916\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5355\u72ec\u6a21\u578b\u548c\u4f20\u7edf\u89e3\u91ca\u65b9\u6cd5\u7684\u6027\u80fd\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86ShiftSmooth\u89e3\u91ca\u6280\u672f\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u7406\u89e3\u57fa\u56e0\u8c03\u63a7\u548c\u5404\u79cd\u751f\u7269\u8fc7\u7a0b\u9700\u8981\u7cbe\u786e\u7684\u8f6c\u5f55\u56e0\u5b50\u7ed3\u5408\u4f4d\u70b9\uff08TFBS\uff09\u9884\u6d4b\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u540cTFBS\u6a21\u5f0f\u548c\u5206\u5e03\u5916\uff08OOD\uff09\u6570\u636e\u96c6\u65f6\u53ef\u80fd\u5b58\u5728\u5c40\u9650\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u65b9\u6cd5\uff0c\u6574\u5408\u4e86\u591a\u4e2a\u4e13\u95e8\u7814\u7a76\u4e0d\u540cTFBS\u6a21\u5f0f\u7684\u9884\u8bad\u7ec3\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u6a21\u578b\u3002\u5f15\u5165\u4e86ShiftSmooth\u6280\u672f\uff0c\u4e00\u79cd\u8003\u8651\u8f93\u5165\u5e8f\u5217\u5fae\u5c0f\u53d8\u5316\u7684\u5f52\u56e0\u6620\u5c04\u6280\u672f\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728OOD\u6570\u636e\u96c6\u4e0a\u3002ANOVA\u7edf\u8ba1\u68c0\u9a8c\u8bc1\u5b9e\u4e86\u6027\u80fd\u5dee\u5f02\u7684\u663e\u8457\u6027\u3002ShiftSmooth\u6280\u672f\u5728\u53ef\u89e3\u91ca\u6027\u5206\u6790\u4e2d\u4f18\u4e8eVanilla Gradient\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\u5728\u8f6c\u5f55\u56e0\u5b50\u7ed3\u5408\u4f4d\u70b9\uff08TFBS\uff09\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u6709\u7ade\u4e89\u529b\u6216\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5c24\u5176\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u573a\u666f\u4e0b\u8868\u73b0\u7a81\u51fa\u3002ShiftSmooth\u5f52\u56e0\u6620\u5c04\u6280\u672f\u63d0\u4f9b\u4e86\u6bd4\u4f20\u7edfVanilla Gradient\u65b9\u6cd5\u66f4\u9c81\u68d2\u7684\u6a21\u578b\u53ef\u89e3\u91ca\u6027\uff0c\u6709\u52a9\u4e8e\u57fa\u5143\u53d1\u73b0\u548c\u5b9a\u4f4d\u3002\u8be5\u7814\u7a76\u4e3aTFBS\u9884\u6d4b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u6cdb\u5316\u4e14\u53ef\u89e3\u91ca\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u6709\u671b\u5728\u57fa\u56e0\u7ec4\u751f\u7269\u5b66\u548c\u8f6c\u5f55\u8c03\u63a7\u7814\u7a76\u4e2d\u5e26\u6765\u65b0\u53d1\u73b0\u3002"}}
{"id": "2507.09766", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09766", "abs": "https://arxiv.org/abs/2507.09766", "authors": ["Mohamadreza Akbari Pour", "Ali Ghasemzadeh", "MohamadAli Bijarchi", "Mohammad Behshad Shafii"], "title": "Toward accurate RUL and SOH estimation using reinforced graph-based PINNs enhanced with dynamic weights", "comment": null, "summary": "Accurate estimation of Remaining Useful Life (RUL) and State of Health (SOH)\nis essential for Prognostics and Health Management (PHM) across a wide range of\nindustrial applications. We propose a novel framework -- Reinforced Graph-Based\nPhysics-Informed Neural Networks Enhanced with Dynamic Weights (RGPD) -- that\ncombines physics-based supervision with advanced spatio-temporal learning.\nGraph Convolutional Recurrent Networks (GCRNs) embed graph-convolutional\nfilters within recurrent units to capture how node representations evolve over\ntime. Graph Attention Convolution (GATConv) leverages a self-attention\nmechanism to compute learnable, edge-wise attention coefficients, dynamically\nweighting neighbor contributions for adaptive spatial aggregation. A Soft\nActor-Critic (SAC) module is positioned between the Temporal Attention Unit\n(TAU) and GCRN to further improve the spatio-temporal learning. This module\nimproves attention and prediction accuracy by dynamically scaling hidden\nrepresentations to minimize noise and highlight informative features. To\nidentify the most relevant physical constraints in each area, Q-learning agents\ndynamically assign weights to physics-informed loss terms, improving\ngeneralization across real-time industrial systems and reducing the need for\nmanual tuning. In both RUL and SOH estimation tasks, the proposed method\nconsistently outperforms state-of-the-art models, demonstrating strong\nrobustness and predictive accuracy across varied degradation patterns across\nthree diverse industrial benchmark datasets.", "AI": {"tldr": "RGPD\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u7269\u7406\u4fe1\u606f\u795e\u7ecf\u7f51\u7edc\u3001\u56fe\u5377\u79ef\u5faa\u73af\u7f51\u7edc\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u63d0\u9ad8\u4e86\u5269\u4f59\u4f7f\u7528\u5bff\u547d\u548c\u5065\u5eb7\u72b6\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u51c6\u786e\u4f30\u8ba1\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\u548c\u5065\u5eb7\u72b6\u6001\uff08SOH\uff09\u5bf9\u4e8e\u8bb8\u591a\u5de5\u4e1a\u5e94\u7528\u4e2d\u7684\u9884\u671f\u4e0e\u5065\u5eb7\u7ba1\u7406\uff08PHM\uff09\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a RGPD \u7684\u65b0\u9896\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u7269\u7406\u7684\u76d1\u7763\u4e0e\u5148\u8fdb\u7684\u7a7a\u65f6\u5b66\u4e60\u3002\u5b83\u4f7f\u7528\u56fe\u5377\u79ef\u5faa\u73af\u7f51\u7edc\uff08GCRN\uff09\u548c\u56fe\u6ce8\u610f\u529b\u5377\u79ef\uff08GATConv\uff09\u6765\u6355\u83b7\u8282\u70b9\u7684\u65f6\u53d8\u6f14\u5316\u548c\u81ea\u9002\u5e94\u7a7a\u95f4\u805a\u5408\u3002Soft Actor-Critic\uff08SAC\uff09\u6a21\u5757\u548c Q-learning agent \u7528\u4e8e\u4f18\u5316\u5b66\u4e60\u8fc7\u7a0b\uff0c\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u548c\u7f29\u653e\u6765\u63d0\u9ad8\u6ce8\u610f\u529b\u548c\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u5e76\u8bc6\u522b\u6700\u76f8\u5173\u7684\u7269\u7406\u7ea6\u675f\u3002", "result": "RGPD\u6846\u67b6\u5728RUL\u548cSOH\u4f30\u8ba1\u4efb\u52a1\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u5de5\u4e1a\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u5bf9\u4e0d\u540c\u9000\u5316\u6a21\u5f0f\u7684\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5269\u4f59\u4f7f\u7528\u5bff\u547d\uff08RUL\uff09\u548c\u5065\u5eb7\u72b6\u6001\uff08SOH\uff09\u4f30\u8ba1\u4efb\u52a1\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728\u4e09\u79cd\u4e0d\u540c\u7684\u5de5\u4e1a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u5bf9\u5404\u79cd\u9000\u5316\u6a21\u5f0f\u7684\u9c81\u68d2\u6027\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002"}}
{"id": "2507.09880", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09880", "abs": "https://arxiv.org/abs/2507.09880", "authors": ["Keito Suzuki", "Bang Du", "Runfa Blark Li", "Kunyao Chen", "Lei Wang", "Peng Liu", "Ning Bi", "Truong Nguyen"], "title": "OpenHuman4D: Open-Vocabulary 4D Human Parsing", "comment": null, "summary": "Understanding dynamic 3D human representation has become increasingly\ncritical in virtual and extended reality applications. However, existing human\npart segmentation methods are constrained by reliance on closed-set datasets\nand prolonged inference times, which significantly restrict their\napplicability. In this paper, we introduce the first 4D human parsing framework\nthat simultaneously addresses these challenges by reducing the inference time\nand introducing open-vocabulary capabilities. Building upon state-of-the-art\nopen-vocabulary 3D human parsing techniques, our approach extends the support\nto 4D human-centric video with three key innovations: 1) We adopt mask-based\nvideo object tracking to efficiently establish spatial and temporal\ncorrespondences, avoiding the necessity of segmenting all frames. 2) A novel\nMask Validation module is designed to manage new target identification and\nmitigate tracking failures. 3) We propose a 4D Mask Fusion module, integrating\nmemory-conditioned attention and logits equalization for robust embedding\nfusion. Extensive experiments demonstrate the effectiveness and flexibility of\nthe proposed method on 4D human-centric parsing tasks, achieving up to 93.3%\nacceleration compared to the previous state-of-the-art method, which was\nlimited to parsing fixed classes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684 4D \u4eba\u4f53\u89e3\u6790\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u63a8\u7406\u65f6\u95f4\u548c\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u9650\u5236\u3002\u901a\u8fc7\u63a9\u7801\u8ddf\u8e2a\u3001\u63a9\u7801\u9a8c\u8bc1\u548c 4D \u63a9\u7801\u878d\u5408\u7b49\u521b\u65b0\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u52a0\u901f\u548c\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u90e8\u4ef6\u5206\u5272\u65b9\u6cd5\u53d7\u9650\u4e8e\u95ed\u96c6\u6570\u636e\u96c6\u548c\u8f83\u957f\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u8fd9\u6781\u5927\u5730\u9650\u5236\u4e86\u5b83\u4eec\u5728\u865a\u62df\u548c\u6269\u5c55\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5e7f\u6cdb\u4f7f\u7528\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u5f00\u53d1\u80fd\u591f\u63d0\u9ad8\u6548\u7387\u5e76\u652f\u6301\u66f4\u5e7f\u6cdb\u5e94\u7528\u573a\u666f\uff08\u4f8b\u5982\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\uff09\u7684\u65b0\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 4D \u4eba\u4f53\u89e3\u6790\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u63a9\u7801\u8ddf\u8e2a\u3001\u63a9\u7801\u9a8c\u8bc1\u548c 4D \u63a9\u7801\u878d\u5408\u7b49\u5173\u952e\u6280\u672f\u3002\u5177\u4f53\u800c\u8a00\uff0c\u91c7\u7528\u4e86\u63a9\u7801\u8ddf\u8e2a\u6765\u5efa\u7acb\u65f6\u7a7a\u5bf9\u5e94\u5173\u7cfb\uff0c\u8bbe\u8ba1\u4e86\u63a9\u7801\u9a8c\u8bc1\u6a21\u5757\u6765\u5904\u7406\u65b0\u76ee\u6807\u8bc6\u522b\u548c\u8ddf\u8e2a\u5931\u8d25\uff0c\u5e76\u63d0\u51fa\u4e86\u878d\u5408\u4e86\u8bb0\u5fc6\u6761\u4ef6\u6ce8\u610f\u529b\u548c logits \u5747\u8861\u5316\u7684 4D \u63a9\u7801\u878d\u5408\u6a21\u5757\u6765\u589e\u5f3a\u5d4c\u5165\u878d\u5408\u7684\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728 4D \u4eba\u4f53\u4e2d\u5fc3\u89e3\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u4e0e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u63a8\u7406\u901f\u5ea6\u6700\u9ad8\u63d0\u5347\u4e86 93.3%\uff0c\u5e76\u4e14\u80fd\u591f\u89e3\u6790\u56fa\u5b9a\u7684\u7c7b\u522b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684 4D \u4eba\u4f53\u89e3\u6790\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u63a9\u7801\u8ddf\u8e2a\u3001\u63a9\u7801\u9a8c\u8bc1\u548c 4D \u63a9\u7801\u878d\u5408\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u95f4\u548c\u5f00\u653e\u8bcd\u6c47\u80fd\u529b\u65b9\u9762\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe 93.3% \u7684\u52a0\u901f\uff0c\u5e76\u5728 4D \u4eba\u4f53\u4e2d\u5fc3\u89c6\u9891\u89e3\u6790\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u7075\u6d3b\u6027\u3002"}}
{"id": "2507.09768", "categories": ["cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2507.09768", "abs": "https://arxiv.org/abs/2507.09768", "authors": ["Kenny Falk\u00e6r Olsen. Mads \u00d8stergaard", "Karl Ulb\u00e6k", "S\u00f8ren F\u00f8ns Nielsen", "Rasmus Malik H\u00f8egh Lindrup", "Bj\u00f8rn Sand Jensen", "Morten M\u00f8rup"], "title": "Knowing When to Quit: Probabilistic Early Exits for Speech Separation", "comment": null, "summary": "In recent years, deep learning-based single-channel speech separation has\nimproved considerably, in large part driven by increasingly compute- and\nparameter-efficient neural network architectures. Most such architectures are,\nhowever, designed with a fixed compute and parameter budget, and consequently\ncannot scale to varying compute demands or resources, which limits their use in\nembedded and heterogeneous devices such as mobile phones and hearables. To\nenable such use-cases we design a neural network architecture for speech\nseparation capable of early-exit, and we propose an uncertainty-aware\nprobabilistic framework to jointly model the clean speech signal and error\nvariance which we use to derive probabilistic early-exit conditions in terms of\ndesired signal-to-noise ratios. We evaluate our methods on both speech\nseparation and enhancement tasks, and we show that a single early-exit model\ncan be competitive with state-of-the-art models trained at many compute and\nparameter budgets. Our framework enables fine-grained dynamic compute-scaling\nof speech separation networks while achieving state-of-the-art performance and\ninterpretable exit conditions.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u52a8\u6001\u8c03\u6574\u8ba1\u7b97\u91cf\u7684\u8bed\u97f3\u5206\u79bb\u6a21\u578b\uff0c\u53ef\u5728\u4e0d\u540c\u8bbe\u5907\u4e0a\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u5355\u901a\u9053\u8bed\u97f3\u5206\u79bb\u6a21\u578b\u5728\u56fa\u5b9a\u8ba1\u7b97\u548c\u53c2\u6570\u9884\u7b97\u4e0b\uff0c\u65e0\u6cd5\u9002\u5e94\u79fb\u52a8\u7535\u8bdd\u548c\u52a9\u542c\u5668\u7b49\u5d4c\u5165\u5f0f\u548c\u5f02\u6784\u8bbe\u5907\u7684\u9700\u6c42\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bbe\u8ba1\u4e00\u79cd\u80fd\u591f\u6839\u636e\u4e0d\u540c\u8ba1\u7b97\u9700\u6c42\u548c\u8d44\u6e90\u8fdb\u884c\u6269\u5c55\u7684\u6a21\u578b\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u79cd\u652f\u6301\u65e9\u9000\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u8054\u5408\u5efa\u6a21\u5e72\u51c0\u8bed\u97f3\u4fe1\u53f7\u548c\u8bef\u5dee\u65b9\u5dee\uff0c\u4ece\u800c\u63a8\u5bfc\u51fa\u57fa\u4e8e\u6240\u9700\u4fe1\u566a\u6bd4\u7684\u6982\u7387\u65e9\u9000\u6761\u4ef6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5355\u4e00\u7684\u65e9\u9000\u6a21\u578b\u5728\u8bed\u97f3\u5206\u79bb\u548c\u589e\u5f3a\u4efb\u52a1\u4e0a\u80fd\u591f\u4e0e\u5728\u591a\u79cd\u8ba1\u7b97\u548c\u53c2\u6570\u9884\u7b97\u4e0b\u8bad\u7ec3\u7684\u6700\u5148\u8fdb\u6a21\u578b\u76f8\u5ab2\u7f8e\uff0c\u5b9e\u73b0\u4e86\u7cbe\u7ec6\u7684\u52a8\u6001\u8ba1\u7b97\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u9000\u51fa\u6761\u4ef6\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u652f\u6301\u65e9\u9000\u7684\u6df1\u5ea6\u5b66\u4e60\u8bed\u97f3\u5206\u79bb\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u6982\u7387\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u52a8\u6001\u8ba1\u7b97\u6269\u5c55\u548c\u53ef\u89e3\u91ca\u7684\u9000\u51fa\u6761\u4ef6\uff0c\u5728\u8bed\u97f3\u5206\u79bb\u548c\u589e\u5f3a\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u5f53\u524d\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2507.09881", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09881", "abs": "https://arxiv.org/abs/2507.09881", "authors": ["Yiran Qiao", "Disheng Liu", "Yiren Lu", "Yu Yin", "Mengnan Du", "Jing Ma"], "title": "Counterfactual Visual Explanation via Causally-Guided Adversarial Steering", "comment": null, "summary": "Recent work on counterfactual visual explanations has contributed to making\nartificial intelligence models more explainable by providing visual\nperturbation to flip the prediction. However, these approaches neglect the\ncausal relationships and the spurious correlations behind the image generation\nprocess, which often leads to unintended alterations in the counterfactual\nimages and renders the explanations with limited quality. To address this\nchallenge, we introduce a novel framework CECAS, which first leverages a\ncausally-guided adversarial method to generate counterfactual explanations. It\ninnovatively integrates a causal perspective to avoid unwanted perturbations on\nspurious factors in the counterfactuals. Extensive experiments demonstrate that\nour method outperforms existing state-of-the-art approaches across multiple\nbenchmark datasets and ultimately achieves a balanced trade-off among various\naspects of validity, sparsity, proximity, and realism.", "AI": {"tldr": "CECAS\u662f\u4e00\u79cd\u65b0\u7684\u53cd\u4e8b\u5b9e\u89c6\u89c9\u89e3\u91ca\u6846\u67b6\uff0c\u901a\u8fc7\u6574\u5408\u56e0\u679c\u5173\u7cfb\u6765\u63d0\u9ad8\u89e3\u91ca\u8d28\u91cf\uff0c\u5e76\u907f\u514d\u5bf9\u865a\u5047\u56e0\u7d20\u8fdb\u884c\u4e0d\u5e0c\u671b\u7684\u6270\u52a8\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u53cd\u4e8b\u5b9e\u56fe\u50cf\u65f6\u5ffd\u7565\u4e86\u56e0\u679c\u5173\u7cfb\u548c\u865a\u5047\u76f8\u5173\u6027\uff0c\u5bfc\u81f4\u56fe\u50cf\u6539\u52a8\u4e0d\u5f53\u548c\u89e3\u91ca\u8d28\u91cf\u6709\u9650\u3002", "method": "CECAS\u6846\u67b6\u9996\u5148\u5229\u7528\u56e0\u679c\u5f15\u5bfc\u7684\u5bf9\u6297\u6027\u65b9\u6cd5\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u5e76\u5c06\u56e0\u679c\u89c6\u89d2\u6574\u5408\u5176\u4e2d\uff0c\u4ee5\u907f\u514d\u5bf9\u865a\u5047\u56e0\u7d20\u8fdb\u884c\u4e0d\u5e0c\u671b\u7684\u6270\u52a8\u3002", "result": "CECAS\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u5728\u6709\u6548\u6027\u3001\u7a00\u758f\u6027\u3001\u90bb\u8fd1\u6027\u548c\u771f\u5b9e\u6027\u7b49\u65b9\u9762\u53d6\u5f97\u4e86\u5e73\u8861\u7684\u6743\u8861\u3002", "conclusion": "CECAS\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u56e0\u679c\u89c6\u89d2\u548c\u56e0\u679c\u5f15\u5bfc\u7684\u5bf9\u6297\u6027\u65b9\u6cd5\u6765\u751f\u6210\u53cd\u4e8b\u5b9e\u89e3\u91ca\uff0c\u907f\u514d\u4e86\u5bf9\u865a\u5047\u56e0\u7d20\u7684\u4e0d\u5e0c\u671b\u7684\u6270\u52a8\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u89e3\u91ca\u7684\u8d28\u91cf\u3002"}}
{"id": "2507.09785", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2507.09785", "abs": "https://arxiv.org/abs/2507.09785", "authors": ["Zhonglin Cao", "Mario Geiger", "Allan dos Santos Costa", "Danny Reidenbach", "Karsten Kreis", "Tomas Geffner", "Franco Pellegrini", "Guoqing Zhou", "Emine Kucukbenli"], "title": "Efficient Molecular Conformer Generation with SO(3)-Averaged Flow Matching and Reflow", "comment": "ICML 2025 poster", "summary": "Fast and accurate generation of molecular conformers is desired for\ndownstream computational chemistry and drug discovery tasks. Currently,\ntraining and sampling state-of-the-art diffusion or flow-based models for\nconformer generation require significant computational resources. In this work,\nwe build upon flow-matching and propose two mechanisms for accelerating\ntraining and inference of generative models for 3D molecular conformer\ngeneration. For fast training, we introduce the SO(3)-Averaged Flow training\nobjective, which leads to faster convergence to better generation quality\ncompared to conditional optimal transport flow or Kabsch-aligned flow. We\ndemonstrate that models trained using SO(3)-Averaged Flow can reach\nstate-of-the-art conformer generation quality. For fast inference, we show that\nthe reflow and distillation methods of flow-based models enable few-steps or\neven one-step molecular conformer generation with high quality. The training\ntechniques proposed in this work show a path towards highly efficient molecular\nconformer generation with flow-based models.", "AI": {"tldr": "\u901a\u8fc7SO(3)-Averaged Flow\u548c\u84b8\u998f\u6280\u672f\uff0c\u6211\u4eec\u52a0\u901f\u4e86\u5206\u5b50\u6784\u8c61\u751f\u6210\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\u8bad\u7ec3\u548c\u9ad8\u6548\u63a8\u7406\u3002", "motivation": "\u5f53\u524d\u7684\u57fa\u4e8e\u6269\u6563\u6216\u6d41\u7684\u6a21\u578b\u5728\u5206\u5b50\u6784\u8c61\u751f\u6210\u4efb\u52a1\u4e2d\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\uff0c\u9650\u5236\u4e86\u5176\u5728\u8ba1\u7b97\u5316\u5b66\u548c\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5feb\u901f\u3001\u66f4\u51c6\u786e\u7684\u751f\u6210\u6a21\u578b\u3002", "method": "1. \u63d0\u51faSO(3)-Averaged Flow\u8bad\u7ec3\u76ee\u6807\uff0c\u7528\u4e8e\u52a0\u901f\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u5e76\u63d0\u5347\u751f\u6210\u8d28\u91cf\u3002 2. \u63d0\u51fa\u5229\u7528reflow\u548c\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u5c11\u6b65\u6216\u5355\u6b65\u5206\u5b50\u6784\u8c61\u751f\u6210\u3002", "result": "SO(3)-Averaged Flow\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u751f\u6210\u8d28\u91cf\u4e0a\u8fbe\u5230\u6216\u8d85\u8fc7\u4e86\u73b0\u6709\u6700\u4f18\u6c34\u5e73\u3002reflow\u548c\u84b8\u998f\u6280\u672f\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u5c11\u6b65\u6216\u5355\u6b65\u5206\u5b50\u6784\u8c61\u751f\u6210\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684SO(3)-Averaged Flow\u8bad\u7ec3\u76ee\u6807\u548c\u84b8\u998f\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u52a0\u901f3D\u5206\u5b50\u6784\u8c61\u751f\u6210\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u8fc7\u7a0b\uff0c\u540c\u65f6\u4fdd\u6301\u6216\u63d0\u9ad8\u751f\u6210\u8d28\u91cf\uff0c\u4e3a\u9ad8\u6548\u7684\u5206\u5b50\u6784\u8c61\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.09885", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09885", "abs": "https://arxiv.org/abs/2507.09885", "authors": ["Zhanjiang Yang", "Lijun Sun", "Jiawei Dong", "Xiaoxin An", "Yang Liu", "Meng Li"], "title": "MCGA: Mixture of Codebooks Hyperspectral Reconstruction via Grayscale-Aware Attention", "comment": null, "summary": "Reconstructing hyperspectral images (HSI) from RGB images is a cost-effective\nsolution for various vision-based applications. However, most existing\nlearning-based hyperspectral reconstruction methods directly learn the\nRGB-to-HSI mapping using complex attention mechanisms, neglecting the inherent\nchallenge of transitioning from low-dimensional to high-dimensional\ninformation. To address this limitation, we propose a two-stage approach, MCGA,\nwhich first learns spectral patterns before estimating the mapping. In the\nfirst stage, a multi-scale VQ-VAE learns representations from heterogeneous HSI\ndatasets, extracting a Mixture of Codebooks (MoC). In the second stage, the\nRGB-to-HSI mapping is refined by querying features from the MoC to replace\nlatent HSI representations, incorporating prior knowledge rather than forcing a\ndirect high-dimensional transformation. To further enhance reconstruction\nquality, we introduce Grayscale-Aware Attention and Quantized Self-Attention,\nwhich adaptively adjust feature map intensities to meet hyperspectral\nreconstruction requirements. This physically motivated attention mechanism\nensures lightweight and efficient HSI recovery. Moreover, we propose an\nentropy-based Test-Time Adaptation strategy to improve robustness in real-world\nscenarios. Extensive experiments demonstrate that our method, MCGA, achieves\nstate-of-the-art performance. The code and models will be released at\nhttps://github.com/Fibonaccirabbit/MCGA", "AI": {"tldr": "MCGA\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u9ad8\u5149\u8c31\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u5149\u8c31\u6a21\u5f0f\u548c\u5229\u7528\u8fc1\u79fb\u5b66\u4e60\u6765\u63d0\u9ad8\u4eceRGB\u5230HSI\u7684\u6620\u5c04\u7cbe\u5ea6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u4f4e\u7ef4\u5230\u9ad8\u7ef4\u4fe1\u606f\u8f6c\u6362\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u76f4\u63a5\u5b66\u4e60RGB\u5230HSI\u7684\u6620\u5c04\u5ffd\u7565\u4e86\u8fd9\u4e00\u56fa\u6709\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCGA\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1. \u4f7f\u7528\u591a\u5c3a\u5ea6VQ-VAE\u4ece\u5f02\u6784\u9ad8\u5149\u8c31\u56fe\u50cf\u6570\u636e\u96c6\u4e2d\u5b66\u4e60\u8868\u793a\uff0c\u63d0\u53d6\u7801\u4e66\u6df7\u5408\uff08MoC\uff09\u30022. \u901a\u8fc7\u67e5\u8be2MoC\u4e2d\u7684\u7279\u5f81\u6765\u7ec6\u5316RGB\u5230HSI\u7684\u6620\u5c04\uff0c\u5c06\u5148\u9a8c\u77e5\u8bc6\u7eb3\u5165\u5176\u4e2d\uff0c\u800c\u4e0d\u662f\u5f3a\u5236\u8fdb\u884c\u76f4\u63a5\u7684\u9ad8\u7ef4\u8f6c\u6362\u3002\u5f15\u5165\u4e86\u7070\u5ea6\u611f\u77e5\u6ce8\u610f\u529b\u548c\u91cf\u5316\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u9002\u5e94\u7279\u5f81\u56fe\u5f3a\u5ea6\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8e\u71b5\u7684\u6d4b\u8bd5\u65f6\u95f4\u81ea\u9002\u5e94\u7b56\u7565\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "MCGA\u65b9\u6cd5\u5728\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "MCGA\u65b9\u6cd5\u5728\u5404\u79cd\u8ba1\u7b97\u673a\u89c6\u89c9\u5e94\u7528\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9ad8\u5149\u8c31\u91cd\u5efa\u6027\u80fd\u3002"}}
{"id": "2507.09786", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09786", "abs": "https://arxiv.org/abs/2507.09786", "authors": ["Junaid Iqbal Khan"], "title": "Leveraging Distribution Matching to Make Approximate Machine Unlearning Faster", "comment": "10 pages, 4 figures, 4 tables", "summary": "Approximate machine unlearning (AMU) enables models to `forget' specific\ntraining data through specialized fine-tuning on a retained dataset subset.\nHowever, processing this retained subset still dominates computational runtime,\nwhile reductions of epochs also remain a challenge. We propose two\ncomplementary methods to accelerate classification-oriented AMU. First,\n\\textbf{Blend}, a novel distribution-matching dataset condensation (DC), merges\nvisually similar images with shared blend-weights to significantly reduce the\nretained set size. It operates with minimal pre-processing overhead and is\norders of magnitude faster than state-of-the-art DC methods. Second, our\nloss-centric method, \\textbf{Accelerated-AMU (A-AMU)}, augments the unlearning\nobjective to quicken convergence. A-AMU achieves this by combining a steepened\nprimary loss to expedite forgetting with a novel, differentiable regularizer\nthat matches the loss distributions of forgotten and in-distribution unseen\ndata. Our extensive experiments demonstrate that this dual approach of data and\nloss-centric optimization dramatically reduces end-to-end unlearning latency\nacross both single and multi-round scenarios, all while preserving model\nutility and privacy. To our knowledge, this is the first work to systematically\ntackle unlearning efficiency by jointly designing a specialized dataset\ncondensation technique with a dedicated accelerated loss function. Code is\navailable at https://github.com/algebraicdianuj/DC_Unlearning.", "AI": {"tldr": "\u63d0\u51fa Blend \u548c A-AMU \u65b9\u6cd5\uff0c\u901a\u8fc7\u6570\u636e\u96c6\u7f29\u51cf\u548c\u52a0\u901f\u635f\u5931\u51fd\u6570\u6765\u63d0\u9ad8\u673a\u5668\u9057\u5fd8\u7684\u6548\u7387\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\u548c\u9690\u79c1\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u8fd1\u4f3c\u673a\u5668\u9057\u5fd8 (AMU) \u4e2d\u7684\u8ba1\u7b97\u74f6\u9888\uff0c\u7279\u522b\u662f\u901a\u8fc7\u4e13\u95e8\u7684\u5fae\u8c03\u5904\u7406\u4fdd\u7559\u7684\u6570\u636e\u5b50\u96c6\u6240\u5e26\u6765\u7684\u8ba1\u7b97\u6210\u672c\u548c\u51cf\u5c11\u8bad\u7ec3\u8f6e\u6570\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Blend \u7684\u65b0\u9896\u7684\u5206\u5e03\u5339\u914d\u6570\u636e\u96c6\u7f29\u51cf (DC) \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5408\u5e76\u5177\u6709\u5171\u4eab\u6df7\u5408\u6743\u91cd\u7684\u89c6\u89c9\u76f8\u4f3c\u56fe\u50cf\u6765\u663e\u8457\u51cf\u5c0f\u4fdd\u7559\u6570\u636e\u96c6\u7684\u5927\u5c0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Accelerated-AMU (A-AMU) \u7684\u4ee5\u635f\u5931\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u9661\u5316\u7684\u4e3b\u8981\u635f\u5931\u548c\u5339\u914d\u88ab\u9057\u5fd8\u6570\u636e\u548c\u5206\u5e03\u5185\u672a\u89c1\u6570\u636e\u7684\u635f\u5931\u5206\u5e03\u7684\u65b0\u578b\u53ef\u5fae\u5206\u6b63\u5219\u5316\u5668\u6765\u52a0\u901f\u9057\u5fd8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6240\u63d0\u51fa\u7684 Blend \u548c A-AMU \u65b9\u6cd5\u80fd\u591f\u663e\u8457\u51cf\u5c11\u7aef\u5230\u7aef\u7684\u6a21\u578b\u65e0\u6548\u5316\u5ef6\u8fdf\uff0c\u5e76\u4e14\u5728\u5355\u8f6e\u548c\u591a\u8f6e\u573a\u666f\u4e0b\u5747\u80fd\u4fdd\u6301\u6a21\u578b\u7684\u6548\u7528\u548c\u9690\u79c1\u3002", "conclusion": "Blend \u548c Accelerated-AMU (A-AMU) \u7684\u53cc\u91cd\u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u8bbe\u8ba1\u4e13\u95e8\u7684\u6570\u636e\u96c6\u7f29\u51cf\u6280\u672f\u548c\u4e13\u7528\u7684\u52a0\u901f\u635f\u5931\u51fd\u6570\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u7684\u6a21\u578b\u65e0\u6548\u5316\u5ef6\u8fdf\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u6548\u7528\u548c\u9690\u79c1\u3002"}}
{"id": "2507.09896", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09896", "abs": "https://arxiv.org/abs/2507.09896", "authors": ["Xiuyu Wu", "Xinhao Wang", "Xiubin Zhu", "Lan Yang", "Jiyuan Liu", "Xingchen Hu"], "title": "Measuring the Impact of Rotation Equivariance on Aerial Object Detection", "comment": "Accepted by ICCV 2025", "summary": "Due to the arbitrary orientation of objects in aerial images, rotation\nequivariance is a critical property for aerial object detectors. However,\nrecent studies on rotation-equivariant aerial object detection remain scarce.\nMost detectors rely on data augmentation to enable models to learn\napproximately rotation-equivariant features. A few detectors have constructed\nrotation-equivariant networks, but due to the breaking of strict rotation\nequivariance by typical downsampling processes, these networks only achieve\napproximately rotation-equivariant backbones. Whether strict rotation\nequivariance is necessary for aerial image object detection remains an open\nquestion. In this paper, we implement a strictly rotation-equivariant backbone\nand neck network with a more advanced network structure and compare it with\napproximately rotation-equivariant networks to quantitatively measure the\nimpact of rotation equivariance on the performance of aerial image detectors.\nAdditionally, leveraging the inherently grouped nature of rotation-equivariant\nfeatures, we propose a multi-branch head network that reduces the parameter\ncount while improving detection accuracy. Based on the aforementioned\nimprovements, this study proposes the Multi-branch head rotation-equivariant\nsingle-stage Detector (MessDet), which achieves state-of-the-art performance on\nthe challenging aerial image datasets DOTA-v1.0, DOTA-v1.5 and DIOR-R with an\nexceptionally low parameter count.", "AI": {"tldr": "\u7531\u4e8e\u822a\u7a7a\u5f71\u50cf\u4e2d\u7269\u4f53\u65b9\u5411\u4e0d\u56fa\u5b9a\uff0c\u65cb\u8f6c\u7b49\u53d8\u6027\u5f88\u91cd\u8981\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e25\u683c\u65cb\u8f6c\u7b49\u53d8\u7684\u68c0\u6d4b\u5668MessDet\uff0c\u5e76\u901a\u8fc7\u591a\u5206\u652f\u5934\u90e8\u7f51\u7edc\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u5e76\u964d\u4f4e\u4e86\u53c2\u6570\u91cf\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u822a\u7a7a\u5f71\u50cf\u4e2d\u7269\u4f53\u7684\u4efb\u610f\u65b9\u5411\uff0c\u65cb\u8f6c\u7b49\u53d8\u6027\u662f\u822a\u7a7a\u7269\u4f53\u68c0\u6d4b\u7684\u5173\u952e\u5c5e\u6027\uff0c\u4f46\u73b0\u6709\u7814\u7a76\u4e0d\u8db3\uff0c\u4e14\u4e25\u683c\u65cb\u8f6c\u7b49\u53d8\u6027\u5728\u8be5\u9886\u57df\u7684\u5fc5\u8981\u6027\u4ecd\u662f\u60ac\u800c\u672a\u51b3\u7684\u95ee\u9898\u3002", "method": "\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e25\u683c\u65cb\u8f6c\u7b49\u53d8\u7684\u9aa8\u5e72\u548c\u9888\u90e8\u7f51\u7edc\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u5206\u652f\u5934\u90e8\u7f51\u7edc\uff0c\u4ee5\u63d0\u9ad8\u68c0\u6d4b\u7cbe\u5ea6\u5e76\u51cf\u5c11\u53c2\u6570\u91cf\u3002", "result": "MessDet\u5728DOTA-v1.0\u3001DOTA-v1.5\u548cDIOR-R\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u53c2\u6570\u91cf\u6781\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MessDet\uff0c\u4e00\u4e2a\u591a\u5206\u652f\u5934\u90e8\u65cb\u8f6c\u7b49\u53d8\u5355\u9636\u6bb5\u68c0\u6d4b\u5668\uff0c\u5728DOTA-v1.0\u3001DOTA-v1.5\u548cDIOR-R\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53c2\u6570\u91cf\u6781\u4f4e\u3002"}}
{"id": "2507.09797", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09797", "abs": "https://arxiv.org/abs/2507.09797", "authors": ["Ping Liu", "Rajat Arora", "Xiao Shi", "Benjamin Le", "Qianqi Shen", "Jianqiang Shen", "Chengming Jiang", "Nikita Zhiltsov", "Priya Bannur", "Yidan Zhu", "Liming Dong", "Haichao Wei", "Qi Guo", "Luke Simon", "Liangjie Hong", "Wenjing Zhang"], "title": "A Scalable and Efficient Signal Integration System for Job Matching", "comment": "KDD2025", "summary": "LinkedIn, one of the world's largest platforms for professional networking\nand job seeking, encounters various modeling challenges in building\nrecommendation systems for its job matching product, including cold-start,\nfilter bubbles, and biases affecting candidate-job matching. To address these,\nwe developed the STAR (Signal Integration for Talent And Recruiters) system,\nleveraging the combined strengths of Large Language Models (LLMs) and Graph\nNeural Networks (GNNs). LLMs excel at understanding textual data, such as\nmember profiles and job postings, while GNNs capture intricate relationships\nand mitigate cold-start issues through network effects. STAR integrates diverse\nsignals by uniting LLM and GNN capabilities with industrial-scale paradigms\nincluding adaptive sampling and version management. It provides an end-to-end\nsolution for developing and deploying embeddings in large-scale recommender\nsystems. Our key contributions include a robust methodology for building\nembeddings in industrial applications, a scalable GNN-LLM integration for\nhigh-performing recommendations, and practical insights for real-world model\ndeployment.", "AI": {"tldr": "LinkedIn\u7684STAR\u7cfb\u7edf\u5229\u7528LLMs\u548cGNNs\u89e3\u51b3\u4e86\u804c\u4f4d\u5339\u914d\u4e2d\u7684\u51b7\u542f\u52a8\u548c\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "LinkedIn\u5728\u6784\u5efa\u804c\u4f4d\u5339\u914d\u63a8\u8350\u7cfb\u7edf\u65f6\u9762\u4e34\u7740\u51b7\u542f\u52a8\u3001\u8fc7\u6ee4\u6c14\u6ce1\u548c\u5f71\u54cd\u5019\u9009\u4eba-\u804c\u4f4d\u5339\u914d\u7684\u504f\u89c1\u7b49\u5efa\u6a21\u6311\u6218\u3002", "method": "STAR\u7cfb\u7edf\u6574\u5408\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7406\u89e3\u6587\u672c\u6570\u636e\uff08\u5982\u7528\u6237\u8d44\u6599\u548c\u804c\u4f4d\u63cf\u8ff0\uff09\u7684\u80fd\u529b\uff0c\u4ee5\u53ca\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u6355\u6349\u590d\u6742\u5173\u7cfb\u548c\u901a\u8fc7\u7f51\u7edc\u6548\u5e94\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\u7684\u80fd\u529b\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u5305\u62ec\u81ea\u9002\u5e94\u91c7\u6837\u548c\u7248\u672c\u7ba1\u7406\u5728\u5185\u7684\u5de5\u4e1a\u89c4\u6a21\u8303\u5f0f\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u5927\u89c4\u6a21\u63a8\u8350\u7cfb\u7edf\u4e2d\u5d4c\u5165\u5f00\u53d1\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u3002", "result": "STAR\u7cfb\u7edf\u901a\u8fc7\u6574\u5408LLMs\u548cGNNs\u7684\u80fd\u529b\uff0c\u5e76\u7ed3\u5408\u5de5\u4e1a\u89c4\u6a21\u7684\u8303\u5f0f\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\u7684\u63a8\u8350\uff0c\u5e76\u4e3a\u5b9e\u9645\u6a21\u578b\u90e8\u7f72\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89c1\u89e3\u3002", "conclusion": "STAR\u7cfb\u7edf\u6210\u529f\u5730\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u7684\u4f18\u52bf\u7ed3\u5408\u8d77\u6765\uff0c\u4e3aLinkedIn\u7684\u804c\u4f4d\u5339\u914d\u4ea7\u54c1\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u3001\u8fc7\u6ee4\u6c14\u6ce1\u548c\u504f\u89c1\u7b49\u6311\u6218\u3002"}}
{"id": "2507.09910", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09910", "abs": "https://arxiv.org/abs/2507.09910", "authors": ["Yadong Qu", "Shancheng Fang", "Yuxin Wang", "Xiaorui Wang", "Zhineng Chen", "Hongtao Xie", "Yongdong Zhang"], "title": "IGD: Instructional Graphic Design with Multimodal Layer Generation", "comment": "ICCV 2025", "summary": "Graphic design visually conveys information and data by creating and\ncombining text, images and graphics. Two-stage methods that rely primarily on\nlayout generation lack creativity and intelligence, making graphic design still\nlabor-intensive. Existing diffusion-based methods generate non-editable graphic\ndesign files at image level with poor legibility in visual text rendering,\nwhich prevents them from achieving satisfactory and practical automated graphic\ndesign. In this paper, we propose Instructional Graphic Designer (IGD) to\nswiftly generate multimodal layers with editable flexibility with only natural\nlanguage instructions. IGD adopts a new paradigm that leverages parametric\nrendering and image asset generation. First, we develop a design platform and\nestablish a standardized format for multi-scenario design files, thus laying\nthe foundation for scaling up data. Second, IGD utilizes the multimodal\nunderstanding and reasoning capabilities of MLLM to accomplish attribute\nprediction, sequencing and layout of layers. It also employs a diffusion model\nto generate image content for assets. By enabling end-to-end training, IGD\narchitecturally supports scalability and extensibility in complex graphic\ndesign tasks. The superior experimental results demonstrate that IGD offers a\nnew solution for graphic design.", "AI": {"tldr": "IGD is a new system that generates editable graphic design layers from natural language instructions, using MLLM and diffusion models, improving upon existing methods that are not creative or editable.", "motivation": "Existing two-stage methods and diffusion-based methods for graphic design lack creativity, intelligence, and editability, making the process labor-intensive and preventing satisfactory, practical automated graphic design.", "method": "IGD adopts a new paradigm leveraging parametric rendering and image asset generation. It utilizes MLLM for multimodal understanding and reasoning to accomplish attribute prediction, sequencing, and layout of layers, and employs a diffusion model to generate image content for assets. A design platform with a standardized format for multi-scenario design files is also established.", "result": "IGD achieves superior experimental results, demonstrating its effectiveness in graphic design.", "conclusion": " IGD enables end-to-end training, architecturally supporting scalability and extensibility in complex graphic design tasks, and offers a new solution for graphic design."}}
{"id": "2507.09805", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09805", "abs": "https://arxiv.org/abs/2507.09805", "authors": ["Audri Banik", "Glaucio Haroldo Silva de Carvalho", "Renata Dividino"], "title": "Federated Learning with Graph-Based Aggregation for Traffic Forecasting", "comment": "Accepted at FedKDD 2025: International Joint Workshop on Federated\n  Learning for Data Mining and Graph Analytics. 6 pages, 1 figure", "summary": "In traffic prediction, the goal is to estimate traffic speed or flow in\nspecific regions or road segments using historical data collected by devices\ndeployed in each area. Each region or road segment can be viewed as an\nindividual client that measures local traffic flow, making Federated Learning\n(FL) a suitable approach for collaboratively training models without sharing\nraw data. In centralized FL, a central server collects and aggregates model\nupdates from multiple clients to build a shared model while preserving each\nclient's data privacy. Standard FL methods, such as Federated Averaging\n(FedAvg), assume that clients are independent, which can limit performance in\ntraffic prediction tasks where spatial relationships between clients are\nimportant. Federated Graph Learning methods can capture these dependencies\nduring server-side aggregation, but they often introduce significant\ncomputational overhead. In this paper, we propose a lightweight graph-aware FL\napproach that blends the simplicity of FedAvg with key ideas from graph\nlearning. Rather than training full models, our method applies basic\nneighbourhood aggregation principles to guide parameter updates, weighting\nclient models based on graph connectivity. This approach captures spatial\nrelationships effectively while remaining computationally efficient. We\nevaluate our method on two benchmark traffic datasets, METR-LA and PEMS-BAY,\nand show that it achieves competitive performance compared to standard\nbaselines and recent graph-based federated learning techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u56fe\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u4ea4\u901a\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u53c2\u6570\u66f4\u65b0\u4e2d\u6574\u5408\u90bb\u57df\u805a\u5408\u548c\u56fe\u8fde\u901a\u6027\u6765\u8003\u8651\u5ba2\u6237\u7aef\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u5728\u4ea4\u901a\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u5ba2\u6237\u7aef\uff08\u533a\u57df\u6216\u9053\u8def\u8def\u6bb5\uff09\u4e4b\u95f4\u5b58\u5728\u91cd\u8981\u7684\u7a7a\u95f4\u5173\u7cfb\uff0c\u800c\u6807\u51c6\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff08\u5982 FedAvg\uff09\u5047\u8bbe\u5ba2\u6237\u7aef\u662f\u72ec\u7acb\u7684\uff0c\u8fd9\u53ef\u80fd\u4f1a\u9650\u5236\u6027\u80fd\u3002\u73b0\u6709\u7684\u8054\u90a6\u56fe\u5b66\u4e60\u65b9\u6cd5\u867d\u7136\u80fd\u6355\u6349\u8fd9\u4e9b\u4f9d\u8d56\u5173\u7cfb\uff0c\u4f46\u901a\u5e38\u4f1a\u5f15\u5165\u663e\u8457\u7684\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u56fe\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06 FedAvg \u7684\u7b80\u6d01\u6027\u4e0e\u56fe\u5b66\u4e60\u7684\u5173\u952e\u601d\u60f3\u76f8\u7ed3\u5408\u3002\u8be5\u65b9\u6cd5\u4e0d\u662f\u8bad\u7ec3\u5b8c\u6574\u7684\u6a21\u578b\uff0c\u800c\u662f\u5c06\u57fa\u672c\u7684\u90bb\u57df\u805a\u5408\u539f\u7406\u5e94\u7528\u4e8e\u6307\u5bfc\u53c2\u6570\u66f4\u65b0\uff0c\u5e76\u6839\u636e\u56fe\u8fde\u901a\u6027\u5bf9\u5ba2\u6237\u7aef\u6a21\u578b\u8fdb\u884c\u52a0\u6743\u3002", "result": "\u8be5\u65b9\u6cd5\u6709\u6548\u5730\u6355\u6349\u4e86\u7a7a\u95f4\u5173\u7cfb\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\u3002", "conclusion": "\u63d0\u51fa\u7684\u8f7b\u91cf\u7ea7\u56fe\u611f\u77e5\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u4e24\u4e2a\u57fa\u51c6\u4ea4\u901a\u6570\u636e\u96c6\uff08METR-LA \u548c PEMS-BAY\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u4e0e\u6807\u51c6\u57fa\u7ebf\u548c\u6700\u8fd1\u7684\u57fa\u4e8e\u56fe\u7684\u8054\u90a6\u5b66\u4e60\u6280\u672f\u76f8\u6bd4\uff0c\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09915", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09915", "abs": "https://arxiv.org/abs/2507.09915", "authors": ["Siyue Yao", "Mingjie Sun", "Eng Gee Lim", "Ran Yi", "Baojiang Zhong", "Moncef Gabbouj"], "title": "Crucial-Diff: A Unified Diffusion Model for Crucial Image and Annotation Synthesis in Data-scarce Scenarios", "comment": null, "summary": "The scarcity of data in various scenarios, such as medical, industry and\nautonomous driving, leads to model overfitting and dataset imbalance, thus\nhindering effective detection and segmentation performance. Existing studies\nemploy the generative models to synthesize more training samples to mitigate\ndata scarcity. However, these synthetic samples are repetitive or simplistic\nand fail to provide \"crucial information\" that targets the downstream model's\nweaknesses. Additionally, these methods typically require separate training for\ndifferent objects, leading to computational inefficiencies. To address these\nissues, we propose Crucial-Diff, a domain-agnostic framework designed to\nsynthesize crucial samples. Our method integrates two key modules. The Scene\nAgnostic Feature Extractor (SAFE) utilizes a unified feature extractor to\ncapture target information. The Weakness Aware Sample Miner (WASM) generates\nhard-to-detect samples using feedback from the detection results of downstream\nmodel, which is then fused with the output of SAFE module. Together, our\nCrucial-Diff framework generates diverse, high-quality training data, achieving\na pixel-level AP of 83.63% and an F1-MAX of 78.12% on MVTec. On polyp dataset,\nCrucial-Diff reaches an mIoU of 81.64% and an mDice of 87.69%. Code will be\nreleased after acceptance.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u758f\u95ee\u9898\uff0c\u63d0\u51faCrucial-Diff\u6846\u67b6\uff0c\u901a\u8fc7SAFE\u548cWASM\u6a21\u5757\u751f\u6210\u9488\u5bf9\u6027\u7684\u201c\u5173\u952e\u6837\u672c\u201d\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728MVTec\u548cpolyp\u6570\u636e\u96c6\u4e0a\u7684\u68c0\u6d4b\u548c\u5206\u5272\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u751f\u6210\u6a21\u578b\u5408\u6210\u7684\u6837\u672c\u91cd\u590d\u6216\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u63d0\u4f9b\u9488\u5bf9\u4e0b\u6e38\u6a21\u578b\u5f31\u70b9\u7684\u201c\u5173\u952e\u4fe1\u606f\u201d\uff0c\u5e76\u4e14\u901a\u5e38\u9700\u8981\u4e3a\u4e0d\u540c\u5bf9\u8c61\u5206\u522b\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002\u4e3a\u89e3\u51b3\u6570\u636e\u7a00\u758f\u3001\u6a21\u578b\u8fc7\u62df\u5408\u548c\u6570\u636e\u96c6\u4e0d\u5e73\u8861\u7b49\u95ee\u9898\uff0c\u63d0\u51faCrucial-Diff\u6846\u67b6\u6765\u5408\u6210\u5173\u952e\u6837\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCrucial-Diff\u7684\u9886\u57df\u65e0\u5173\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u573a\u666f\u65e0\u5173\u7279\u5f81\u63d0\u53d6\u5668\uff08SAFE\uff09\u548c\u5f31\u70b9\u611f\u77e5\u6837\u672c\u6316\u6398\u5668\uff08WASM\uff09\u3002SAFE\u5229\u7528\u7edf\u4e00\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6355\u6349\u76ee\u6807\u4fe1\u606f\uff0c\u800cWASM\u5229\u7528\u4e0b\u6e38\u6a21\u578b\u7684\u68c0\u6d4b\u7ed3\u679c\u53cd\u9988\u6765\u751f\u6210\u96be\u4ee5\u68c0\u6d4b\u7684\u6837\u672c\uff0c\u5e76\u4e0eSAFE\u6a21\u5757\u7684\u8f93\u51fa\u878d\u5408\u3002", "result": "Crucial-Diff\u6846\u67b6\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5728MVTec\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8683.63%\u7684\u50cf\u7d20\u7ea7AP\u548c78.12%\u7684F1-MAX\uff0c\u5728polyp\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8681.64%\u7684mIoU\u548c87.69%\u7684mDice\u3002", "conclusion": "Crucial-Diff\u6846\u67b6\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5728MVTec\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8683.63%\u7684\u50cf\u7d20\u7ea7AP\u548c78.12%\u7684F1-MAX\uff0c\u5728polyp\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8681.64%\u7684mIoU\u548c87.69%\u7684mDice\u3002"}}
{"id": "2507.09816", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09816", "abs": "https://arxiv.org/abs/2507.09816", "authors": ["Adam Newgas"], "title": "Compressed Computation: Dense Circuits in a Toy Model of the Universal-AND Problem", "comment": "9 pages, 9 figures", "summary": "Neural networks are capable of superposition -- representing more features\nthan there are dimensions. Recent work considers the analogous concept for\ncomputation instead of storage, proposing theoretical constructions. But there\nhas been little investigation into whether these circuits can be learned in\npractice. In this work, we investigate a toy model for the Universal-AND\nproblem which computes the AND of all $m\\choose 2$ pairs of $m$ sparse inputs.\nThe hidden dimension that determines the number of non-linear activations is\nrestricted to pressure the model to find a compute-efficient circuit, called\ncompressed computation. We find that the training process finds a simple\nsolution that does not correspond to theoretical constructions. It is fully\ndense -- every neuron contributes to every output. The solution circuit\nnaturally scales with dimension, trading off error rates for neuron efficiency.\nIt is similarly robust to changes in sparsity and other key parameters, and\nextends naturally to other boolean operations and boolean circuits. We explain\nthe found solution in detail and compute why it is more efficient than the\ntheoretical constructions at low sparsity. Our findings shed light on the types\nof circuits that models like to form and the flexibility of the superposition\nrepresentation. This contributes to a broader understanding of network\ncircuitry and interpretability.", "AI": {"tldr": "\u795e\u7ecf\u7f51\u7edc\u5728\u538b\u7f29\u8ba1\u7b97\u4e2d\u503e\u5411\u4e8e\u5b66\u4e60\u5168\u5bc6\u96c6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u800c\u975e\u7406\u8bba\u6784\u9020\uff0c\u8be5\u65b9\u6848\u66f4\u6613\u6269\u5c55\u4e14\u6548\u7387\u66f4\u9ad8\u3002", "motivation": "\u5728\u7406\u8bba\u4e0a\uff0c\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u8868\u793a\u6bd4\u5176\u7ef4\u5ea6\u66f4\u591a\u7684\u7279\u5f81\uff08\u53e0\u52a0\u6001\uff09\uff0c\u4f46\u5bf9\u4e8e\u8ba1\u7b97\u800c\u8a00\uff0c\u8fd9\u79cd\u53e0\u52a0\u6001\u662f\u5426\u80fd\u5728\u5b9e\u8df5\u4e2d\u88ab\u5b66\u4e60\u5c1a\u4e0d\u6e05\u695a\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u795e\u7ecf\u7f51\u7edc\u5728\u538b\u7f29\u8ba1\u7b97\u573a\u666f\u4e0b\u5b66\u4e60\u7535\u8def\u7684\u80fd\u529b\u548c\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4e00\u4e2a\u7528\u4e8e\u89e3\u51b3\u901a\u7528AND\u95ee\u9898\u7684\u73a9\u5177\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u8ba1\u7b97m\u4e2a\u7a00\u758f\u8f93\u5165\u7684m choose 2\u5bf9\u7684AND\u3002\u9650\u5236\u9690\u85cf\u7ef4\u5ea6\u4ee5\u5f3a\u5236\u6a21\u578b\u5bfb\u627e\u4e00\u79cd\u540d\u4e3a\u538b\u7f29\u8ba1\u7b97\u7684\u8ba1\u7b97\u6548\u7387\u9ad8\u7684\u7535\u8def\u3002\u901a\u8fc7\u5b9e\u9a8c\u89c2\u5bdf\u548c\u5206\u6790\u8bad\u7ec3\u8fc7\u7a0b\u627e\u5230\u7684\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8bad\u7ec3\u8fc7\u7a0b\u53d1\u73b0\u4e86\u4e00\u4e2a\u4e0e\u7406\u8bba\u6784\u9020\u4e0d\u540c\u7684\u7b80\u5355\u3001\u5b8c\u5168\u7a20\u5bc6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5176\u4e2d\u6bcf\u4e2a\u795e\u7ecf\u5143\u90fd\u5bf9\u6bcf\u4e2a\u8f93\u51fa\u505a\u51fa\u8d21\u732e\u3002\u8be5\u89e3\u51b3\u65b9\u6848\u80fd\u591f\u81ea\u7136\u5730\u968f\u7ef4\u5ea6\u6269\u5c55\uff0c\u5e76\u901a\u8fc7\u6743\u8861\u9519\u8bef\u7387\u6765\u63d0\u9ad8\u795e\u7ecf\u5143\u6548\u7387\uff0c\u5e76\u4e14\u5bf9\u7a00\u758f\u5ea6\u7b49\u53c2\u6570\u5177\u6709\u9c81\u68d2\u6027\uff0c\u8fd8\u53ef\u4ee5\u6269\u5c55\u5230\u5176\u4ed6\u5e03\u5c14\u8fd0\u7b97\u548c\u7535\u8def\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u795e\u7ecf\u7f51\u7edc\u5728\u5904\u7406\u538b\u7f29\u8ba1\u7b97\u65f6\u7684\u884c\u4e3a\uff0c\u53d1\u73b0\u6a21\u578b\u503e\u5411\u4e8e\u5b66\u4e60\u4e00\u79cd\u5b8c\u5168\u7a20\u5bc6\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u4e0e\u7406\u8bba\u6784\u9020\u4e0d\u540c\uff0c\u4f46\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4f4e\u7a00\u758f\u5ea6\u60c5\u51b5\u4e0b\u6bd4\u7406\u8bba\u6784\u9020\u66f4\u6709\u6548\u7387\uff0c\u4e3a\u7406\u89e3\u7f51\u7edc\u7535\u8def\u548c\u53ef\u89e3\u91ca\u6027\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2507.09950", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09950", "abs": "https://arxiv.org/abs/2507.09950", "authors": ["Shubham Shukla", "Kunal Sonalkar"], "title": "Can GPT-4o mini and Gemini 2.0 Flash Predict Fine-Grained Fashion Product Attributes? A Zero-Shot Analysis", "comment": "11 pages, 2 figures", "summary": "The fashion retail business is centered around the capacity to comprehend\nproducts. Product attribution helps in comprehending products depending on the\nbusiness process. Quality attribution improves the customer experience as they\nnavigate through millions of products offered by a retail website. It leads to\nwell-organized product catalogs. In the end, product attribution directly\nimpacts the 'discovery experience' of the customer. Although large language\nmodels (LLMs) have shown remarkable capabilities in understanding multimodal\ndata, their performance on fine-grained fashion attribute recognition remains\nunder-explored. This paper presents a zero-shot evaluation of state-of-the-art\nLLMs that balance performance with speed and cost efficiency, mainly\nGPT-4o-mini and Gemini 2.0 Flash. We have used the dataset\nDeepFashion-MultiModal (https://github.com/yumingj/DeepFashion-MultiModal) to\nevaluate these models in the attribution tasks of fashion products. Our study\nevaluates these models across 18 categories of fashion attributes, offering\ninsight into where these models excel. We only use images as the sole input for\nproduct information to create a constrained environment. Our analysis shows\nthat Gemini 2.0 Flash demonstrates the strongest overall performance with a\nmacro F1 score of 56.79% across all attributes, while GPT-4o-mini scored a\nmacro F1 score of 43.28%. Through detailed error analysis, our findings provide\npractical insights for deploying these LLMs in production e-commerce product\nattribution-related tasks and highlight the need for domain-specific\nfine-tuning approaches. This work also lays the groundwork for future research\nin fashion AI and multimodal attribute extraction.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09826", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09826", "abs": "https://arxiv.org/abs/2507.09826", "authors": ["Jintao Qu", "Zichong Wang", "Chenhao Wu", "Wenbin Zhang"], "title": "Bridging Neural Networks and Dynamic Time Warping for Adaptive Time Series Classification", "comment": null, "summary": "Neural networks have achieved remarkable success in time series\nclassification, but their reliance on large amounts of labeled data for\ntraining limits their applicability in cold-start scenarios. Moreover, they\nlack interpretability, reducing transparency in decision-making. In contrast,\ndynamic time warping (DTW) combined with a nearest neighbor classifier is\nwidely used for its effectiveness in limited-data settings and its inherent\ninterpretability. However, as a non-parametric method, it is not trainable and\ncannot leverage large amounts of labeled data, making it less effective than\nneural networks in rich-resource scenarios. In this work, we aim to develop a\nversatile model that adapts to cold-start conditions and becomes trainable with\nlabeled data, while maintaining interpretability. We propose a dynamic\nlength-shortening algorithm that transforms time series into prototypes while\npreserving key structural patterns, thereby enabling the reformulation of the\nDTW recurrence relation into an equivalent recurrent neural network. Based on\nthis, we construct a trainable model that mimics DTW's alignment behavior. As a\nneural network, it becomes trainable when sufficient labeled data is available,\nwhile still retaining DTW's inherent interpretability. We apply the model to\nseveral benchmark time series classification tasks and observe that it\nsignificantly outperforms previous approaches in low-resource settings and\nremains competitive in rich-resource settings.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u65f6\u95f4\u5e8f\u5217\u5206\u7c7b\u6a21\u578b\uff0c\u5b83\u7ed3\u5408\u4e86DTW\u7684\u53ef\u89e3\u91ca\u6027\u548c\u795e\u7ecf\u7f51\u7edc\u7684\u53ef\u8bad\u7ec3\u6027\uff0c\u80fd\u591f\u5728\u6570\u636e\u91cf\u6709\u9650\u548c\u6570\u636e\u91cf\u5145\u8db3\u7684\u573a\u666f\u4e0b\u90fd\u8868\u73b0\u826f\u597d\u3002", "motivation": "\u65e8\u5728\u5f00\u53d1\u4e00\u4e2a\u591a\u529f\u80fd\u6a21\u578b\uff0c\u80fd\u591f\u9002\u5e94\u51b7\u542f\u52a8\u6761\u4ef6\u5e76\u968f\u7740\u6807\u8bb0\u6570\u636e\u7684\u53ef\u7528\u6027\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u4fdd\u6301\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u957f\u5ea6\u7f29\u77ed\u7b97\u6cd5\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u8f6c\u5316\u4e3a\u539f\u578b\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u7ed3\u6784\u6a21\u5f0f\uff0c\u4ece\u800c\u80fd\u591f\u5c06\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff08DTW\uff09\u7684\u9012\u63a8\u5173\u7cfb\u91cd\u6784\u4e3a\u7b49\u6548\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u6a21\u4effDTW\u7684\u5bf9\u9f50\u884c\u4e3a\u3002", "result": "\u6210\u529f\u5730\u5c06DTW\u7684\u9012\u63a8\u5173\u7cfb\u91cd\u6784\u4e3a\u7b49\u6548\u7684\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u6a21\u578b\u6765\u6a21\u4effDTW\u7684\u5bf9\u9f50\u884c\u4e3a\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5bcc\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.09953", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09953", "abs": "https://arxiv.org/abs/2507.09953", "authors": ["Zifei Wang", "Zian Mao", "Xiaoya He", "Xi Huang", "Haoran Zhang", "Chun Cheng", "Shufen Chu", "Tingzheng Hou", "Xiaoqin Zeng", "Yujun Xie"], "title": "4D-MISR: A unified model for low-dose super-resolution imaging via feature fusion", "comment": null, "summary": "While electron microscopy offers crucial atomic-resolution insights into\nstructure-property relationships, radiation damage severely limits its use on\nbeam-sensitive materials like proteins and 2D materials. To overcome this\nchallenge, we push beyond the electron dose limits of conventional electron\nmicroscopy by adapting principles from multi-image super-resolution (MISR) that\nhave been widely used in remote sensing. Our method fuses multiple\nlow-resolution, sub-pixel-shifted views and enhances the reconstruction with a\nconvolutional neural network (CNN) that integrates features from synthetic,\nmulti-angle observations. We developed a dual-path, attention-guided network\nfor 4D-STEM that achieves atomic-scale super-resolution from ultra-low-dose\ndata. This provides robust atomic-scale visualization across amorphous,\nsemi-crystalline, and crystalline beam-sensitive specimens. Systematic\nevaluations on representative materials demonstrate comparable spatial\nresolution to conventional ptychography under ultra-low-dose conditions. Our\nwork expands the capabilities of 4D-STEM, offering a new and generalizable\nmethod for the structural analysis of radiation-vulnerable materials.", "AI": {"tldr": "\u901a\u8fc7\u501f\u9274\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08MISR\uff09\u548c\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7684\u539f\u7406\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684 4D-STEM \u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5728\u8d85\u4f4e\u5242\u91cf\u4e0b\u5bf9\u86cb\u767d\u8d28\u548c\u4e8c\u7ef4\u6750\u6599\u7b49\u5149\u675f\u654f\u611f\u6750\u6599\u8fdb\u884c\u539f\u5b50\u7ea7\u8d85\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u514b\u670d\u4e86\u8f90\u5c04\u635f\u4f24\u7684\u9650\u5236\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u7535\u5b50\u663e\u5fae\u955c\u5728\u5bf9\u5149\u675f\u654f\u611f\u6750\u6599\uff08\u5982\u86cb\u767d\u8d28\u548c\u4e8c\u7ef4\u6750\u6599\uff09\u6210\u50cf\u65f6\u56e0\u8f90\u5c04\u635f\u4f24\u800c\u53d7\u5230\u7684\u9650\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53d7\u591a\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08MISR\uff09\u542f\u53d1\u7684\u56db\u7ef4\u626b\u63cf\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\uff084D-STEM\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u878d\u5408\u4e86\u591a\u4e2a\u4f4e\u5206\u8fa8\u7387\u3001\u4e9a\u50cf\u7d20\u79fb\u4f4d\u7684\u89c6\u56fe\uff0c\u5e76\u4f7f\u7528\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u91cd\u5efa\uff0c\u8be5\u7f51\u7edc\u6574\u5408\u4e86\u6765\u81ea\u5408\u6210\u3001\u591a\u89d2\u5ea6\u89c2\u6d4b\u7684\u7279\u5f81\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e 4D-STEM \u7684\u53cc\u8def\u5f84\u3001\u6ce8\u610f\u529b\u5f15\u5bfc\u7f51\u7edc\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5728\u8d85\u4f4e\u5242\u91cf\u6570\u636e\u4e0b\u8fdb\u884c\u539f\u5b50\u7ea7\u8d85\u5206\u8fa8\u7387\u6210\u50cf\uff0c\u80fd\u591f\u5bf9\u975e\u6676\u6001\u3001\u534a\u6676\u6001\u548c\u6676\u6001\u5149\u675f\u654f\u611f\u6837\u672c\u8fdb\u884c\u7a33\u5065\u7684\u539f\u5b50\u7ea7\u53ef\u89c6\u5316\uff0c\u5176\u7a7a\u95f4\u5206\u8fa8\u7387\u5728\u8d85\u4f4e\u5242\u91cf\u6761\u4ef6\u4e0b\u4e0e\u4f20\u7edf\u7684 ptychography \u76f8\u5f53\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u8f90\u5c04\u654f\u611f\u6750\u6599\u7684\u7ed3\u6784\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u53ef\u63a8\u5e7f\u7684\u65b9\u6cd5\uff0c\u5c06 4D-STEM \u7684\u80fd\u529b\u6269\u5c55\u5230\u8d85\u4f4e\u5242\u91cf\u7684\u539f\u5b50\u7ea7\u8d85\u5206\u8fa8\u7387\u6210\u50cf\u3002"}}
{"id": "2507.09831", "categories": ["cs.LG", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09831", "abs": "https://arxiv.org/abs/2507.09831", "authors": ["Jiatong Li", "Qi Liu", "Mengxiao Zhu"], "title": "Generative Cognitive Diagnosis", "comment": "Preprint; 15 pages, 12 figures", "summary": "Cognitive diagnosis (CD) models latent cognitive states of human learners by\nanalyzing their response patterns on diagnostic tests, serving as a crucial\nmachine learning technique for educational assessment and evaluation.\nTraditional cognitive diagnosis models typically follow a transductive\nprediction paradigm that optimizes parameters to fit response scores and\nextract learner abilities. These approaches face significant limitations as\nthey cannot perform instant diagnosis for new learners without computationally\nexpensive retraining and produce diagnostic outputs with limited reliability.\nIn this study, we introduces a novel generative diagnosis paradigm that\nfundamentally shifts CD from predictive to generative modeling, enabling\ninductive inference of cognitive states without parameter re-optimization. We\npropose two simple yet effective instantiations of this paradigm: Generative\nItem Response Theory (G-IRT) and Generative Neural Cognitive Diagnosis Model\n(G-NCDM), which achieve excellent performance improvements over traditional\nmethods. The generative approach disentangles cognitive state inference from\nresponse prediction through a well-designed generation process that\nincorporates identifiability and monotonicity conditions. Extensive experiments\non real-world datasets demonstrate the effectiveness of our methodology in\naddressing scalability and reliability challenges, especially $\\times 100$\nspeedup for the diagnosis of new learners. Our framework opens new avenues for\ncognitive diagnosis applications in artificial intelligence, particularly for\nintelligent model evaluation and intelligent education systems. The code is\navailable at https://github.com/CSLiJT/Generative-CD.git.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u5f0f\u8bca\u65ad\u8303\u5f0f\uff0c\u901a\u8fc7 G-IRT \u548c G-NCDM \u6a21\u578b\u89e3\u51b3\u4e86\u4f20\u7edf\u8ba4\u77e5\u8bca\u65ad\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8bca\u65ad\u6548\u7387\u3002", "motivation": "\u4f20\u7edf\u7684\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\uff08CD\uff09\u901a\u5e38\u91c7\u7528\u5f52\u7eb3\u9884\u6d4b\u8303\u5f0f\uff0c\u9700\u8981\u9488\u5bf9\u65b0\u5b66\u4e60\u8005\u8fdb\u884c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u91cd\u65b0\u8bad\u7ec3\uff0c\u5e76\u4e14\u8bca\u65ad\u8f93\u51fa\u7684\u53ef\u9760\u6027\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u751f\u6210\u5f0f\u8bca\u65ad\u8303\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u751f\u6210\u9879\u76ee\u53cd\u5e94\u7406\u8bba\uff08G-IRT\uff09\u548c\u751f\u6210\u795e\u7ecf\u8ba4\u77e5\u8bca\u65ad\u6a21\u578b\uff08G-NCDM\uff09\u4e24\u4e2a\u5177\u4f53\u5b9e\u73b0\uff0c\u8be5\u8303\u5f0f\u901a\u8fc7\u5305\u542b\u53ef\u8bc6\u522b\u6027\u548c\u5355\u8c03\u6027\u6761\u4ef6\u7684\u751f\u6210\u8fc7\u7a0b\u5c06\u8ba4\u77e5\u72b6\u6001\u63a8\u7406\u4e0e\u54cd\u5e94\u9884\u6d4b\u89e3\u8026\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u751f\u6210\u5f0f\u8bca\u65ad\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u8bca\u65ad\u65b0\u5b66\u4e60\u8005\u65b9\u9762\u5b9e\u73b0\u4e86 100 \u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u751f\u6210\u5f0f\u8bca\u65ad\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u548c\u53ef\u9760\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u8bca\u65ad\u65b0\u5b66\u4e60\u8005\u65f6\u901f\u5ea6\u63d0\u9ad8\u4e86 100 \u500d\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u5728\u8ba4\u77e5\u8bca\u65ad\u9886\u57df\u7684\u5e94\u7528\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2507.09980", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09980", "abs": "https://arxiv.org/abs/2507.09980", "authors": ["Zhipeng Xue", "Yan Zhang", "Ming Li", "Chun Li", "Yue Liu", "Fei Yu"], "title": "Uncertainty Quantification for Incomplete Multi-View Data Using Divergence Measures", "comment": null, "summary": "Existing multi-view classification and clustering methods typically improve\ntask accuracy by leveraging and fusing information from different views.\nHowever, ensuring the reliability of multi-view integration and final decisions\nis crucial, particularly when dealing with noisy or corrupted data. Current\nmethods often rely on Kullback-Leibler (KL) divergence to estimate uncertainty\nof network predictions, ignoring domain gaps between different modalities. To\naddress this issue, KPHD-Net, based on H\\\"older divergence, is proposed for\nmulti-view classification and clustering tasks. Generally, our KPHD-Net employs\na variational Dirichlet distribution to represent class probability\ndistributions, models evidences from different views, and then integrates it\nwith Dempster-Shafer evidence theory (DST) to improve uncertainty estimation\neffects. Our theoretical analysis demonstrates that Proper H\\\"older divergence\noffers a more effective measure of distribution discrepancies, ensuring\nenhanced performance in multi-view learning. Moreover, Dempster-Shafer evidence\ntheory, recognized for its superior performance in multi-view fusion tasks, is\nintroduced and combined with the Kalman filter to provide future state\nestimations. This integration further enhances the reliability of the final\nfusion results. Extensive experiments show that the proposed KPHD-Net\noutperforms the current state-of-the-art methods in both classification and\nclustering tasks regarding accuracy, robustness, and reliability, with\ntheoretical guarantees.", "AI": {"tldr": "KPHD-Net improves multi-view learning by using H\"older divergence for better uncertainty estimation and Dempster-Shafer evidence theory with a Kalman filter for reliable fusion, outperforming existing methods.", "motivation": "Existing multi-view integration and decision-making methods can be unreliable with noisy or corrupted data. Current methods using KL divergence for uncertainty estimation ignore domain gaps between modalities. The paper proposes KPHD-Net to address these limitations.", "method": "KPHD-Net utilizes a variational Dirichlet distribution to represent class probability distributions and models evidence from different views. It integrates this with Dempster-Shafer evidence theory (DST) for improved uncertainty estimation. Proper H\"older divergence is used as a measure of distribution discrepancies. DST is combined with the Kalman filter for future state estimations.", "result": "Extensive experiments demonstrate that KPHD-Net surpasses current state-of-the-art methods in accuracy, robustness, and reliability for both classification and clustering tasks.", "conclusion": "KPHD-Net, which uses H\"older divergence and Dempster-Shafer evidence theory combined with a Kalman filter, outperforms current state-of-the-art methods in multi-view classification and clustering tasks concerning accuracy, robustness, and reliability, with theoretical guarantees."}}
{"id": "2507.09837", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09837", "abs": "https://arxiv.org/abs/2507.09837", "authors": ["Quang Truong", "Zhikai Chen", "Mingxuan Ju", "Tong Zhao", "Neil Shah", "Jiliang Tang"], "title": "A Pre-training Framework for Relational Data with Information-theoretic Principles", "comment": null, "summary": "Relational databases underpin critical infrastructure across a wide range of\ndomains, yet the design of generalizable pre-training strategies for learning\nfrom relational databases remains an open challenge due to task heterogeneity.\nSpecifically, there exist infinitely many possible downstream tasks, as tasks\nare defined based on relational schema graphs, temporal dependencies, and\nSQL-defined label logics. An effective pre-training framework is desired to\ntake these factors into account in order to obtain task-aware representations.\nBy incorporating knowledge of the underlying distribution that drives label\ngeneration, downstream tasks can benefit from relevant side-channel\ninformation. To bridge this gap, we introduce Task Vector Estimation (TVE), a\nnovel pre-training framework that constructs predictive supervisory signals via\nset-based aggregation over schema traversal graphs, explicitly modeling\nnext-window relational dynamics. We formalize our approach through an\ninformation-theoretic lens, demonstrating that task-informed representations\nretain more relevant signals than those obtained without task priors. Extensive\nexperiments on the RelBench benchmark show that TVE consistently outperforms\ntraditional pre-training baselines. Our findings advocate for pre-training\nobjectives that encode task heterogeneity and temporal structure as design\nprinciples for predictive modeling on relational databases.", "AI": {"tldr": "TVE\u9884\u8bad\u7ec3\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u6a21\u5f0f\u904d\u5386\u56fe\u548c\u65f6\u95f4\u52a8\u6001\uff0c\u4e3a\u5173\u7cfb\u6570\u636e\u5e93\u5b66\u4e60\u4efb\u52a1\u611f\u77e5\u8868\u793a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5173\u7cfb\u6570\u636e\u5e93\u4f5c\u4e3a\u5173\u952e\u57fa\u7840\u8bbe\u65bd\uff0c\u5728\u591a\u79cd\u9886\u57df\u90fd\u6709\u5e7f\u6cdb\u5e94\u7528\u3002\u7136\u800c\uff0c\u7531\u4e8e\u4efb\u52a1\u5f02\u8d28\u6027\uff08\u4efb\u52a1\u57fa\u4e8e\u5173\u7cfb\u6a21\u5f0f\u56fe\u3001\u65f6\u95f4\u4f9d\u8d56\u548cSQL\u5b9a\u4e49\u7684\u6807\u7b7e\u903b\u8f91\u800c\u5b9a\u4e49\uff09\uff0c\u5b66\u4e60\u5173\u7cfb\u6570\u636e\u5e93\u7684\u901a\u7528\u9884\u8bad\u7ec3\u7b56\u7565\u4ecd\u7136\u662f\u4e00\u4e2a\u5f00\u653e\u7684\u6311\u6218\u3002\u9700\u8981\u4e00\u4e2a\u80fd\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u4ee5\u83b7\u5f97\u9762\u5411\u4efb\u52a1\u8868\u793a\u7684\u6709\u6548\u9884\u8bad\u7ec3\u6846\u67b6\u3002\u901a\u8fc7\u7ed3\u5408\u6807\u7b7e\u751f\u6210\u7684\u57fa\u7840\u5206\u5e03\u77e5\u8bc6\uff0c\u53ef\u4ee5\u4ece\u76f8\u5173\u7684\u4fa7\u4fe1\u9053\u4fe1\u606f\u4e2d\u83b7\u76ca\uff0c\u4ece\u800c\u4f7f\u4e0b\u6e38\u4efb\u52a1\u53d7\u76ca\u3002", "method": "\u4f5c\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4efb\u52a1\u5411\u91cf\u4f30\u8ba1\uff08TVE\uff09\u7684\u65b0\u578b\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u6a21\u5f0f\u904d\u5386\u56fe\u4e0a\u7684\u96c6\u5408\u805a\u5408\u6765\u6784\u5efa\u9884\u6d4b\u6027\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u663e\u5f0f\u5730\u5bf9\u4e0b\u4e00\u7a97\u53e3\u5173\u7cfb\u52a8\u6001\u8fdb\u884c\u5efa\u6a21\u3002\u7814\u7a76\u4ece\u4fe1\u606f\u8bba\u7684\u89d2\u5ea6\u5bf9\u8be5\u65b9\u6cd5\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\uff0c\u8bc1\u660e\u4e86\u5305\u542b\u4efb\u52a1\u4fe1\u606f\u8868\u793a\u6bd4\u4e0d\u5305\u542b\u4efb\u52a1\u5148\u9a8c\u4fe1\u606f\u8868\u793a\u4fdd\u7559\u4e86\u66f4\u591a\u76f8\u5173\u4fe1\u53f7\u3002", "result": "\u5728RelBench\u57fa\u51c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cTVE\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u7684\u9884\u8bad\u7ec3\u57fa\u7ebf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5305\u542b\u4efb\u52a1\u5f02\u8d28\u6027\u548c\u65f6\u95f4\u7ed3\u6784\u7684\u76ee\u6807\u53ef\u4ee5\u4f5c\u4e3a\u5173\u7cfb\u6570\u636e\u5e93\u9884\u6d4b\u5efa\u6a21\u7684\u8bbe\u8ba1\u539f\u5219\u3002"}}
{"id": "2507.09984", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09984", "abs": "https://arxiv.org/abs/2507.09984", "authors": ["Junho Lee", "Jeongwoo Shin", "Hyungwook Choi", "Joonseok Lee"], "title": "Latent Diffusion Models with Masked AutoEncoders", "comment": null, "summary": "In spite of remarkable potential of the Latent Diffusion Models (LDMs) in\nimage generation, the desired properties and optimal design of the autoencoders\nhave been underexplored. In this work, we analyze the role of autoencoders in\nLDMs and identify three key properties: latent smoothness, perceptual\ncompression quality, and reconstruction quality. We demonstrate that existing\nautoencoders fail to simultaneously satisfy all three properties, and propose\nVariational Masked AutoEncoders (VMAEs), taking advantage of the hierarchical\nfeatures maintained by Masked AutoEncoder. We integrate VMAEs into the LDM\nframework, introducing Latent Diffusion Models with Masked AutoEncoders\n(LDMAEs). Through comprehensive experiments, we demonstrate significantly\nenhanced image generation quality and computational efficiency.", "AI": {"tldr": "LDM\u7684\u81ea\u7f16\u7801\u5668\u8bbe\u8ba1\u5f88\u91cd\u8981\uff0c\u6211\u4eec\u63d0\u51fa\u4e86VMAE\uff0c\u5e76\u5c06\u5176\u7528\u4e8eLDM\uff0c\u7ed3\u679c\u66f4\u597d\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u7f16\u7801\u5668\u65e0\u6cd5\u540c\u65f6\u6ee1\u8db3\u6f5c\u5728\u5e73\u6ed1\u6027\u3001\u611f\u77e5\u538b\u7f29\u8d28\u91cf\u548c\u91cd\u5efa\u8d28\u91cf\u8fd9\u4e09\u4e2a\u5173\u952e\u5c5e\u6027\uff0c\u800c\u8fd9\u4e9b\u5c5e\u6027\u5bf9LDM\u7684\u56fe\u50cf\u751f\u6210\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53d8\u5206\u63a9\u7801\u81ea\u7f16\u7801\u5668\uff08VMAE\uff09\u7684\u65b0\u578b\u81ea\u7f16\u7801\u5668\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\u6846\u67b6\u4e2d\uff0c\u5f62\u6210LDMAE\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\uff0cLDMAE\u5728\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "LDMAE\u901a\u8fc7VMAE\u7684\u5f15\u5165\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u8ba1\u7b97\u6548\u7387\u3002"}}
{"id": "2507.09839", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09839", "abs": "https://arxiv.org/abs/2507.09839", "authors": ["MohammadReza Davari", "Utkarsh Garg", "Weixin Cai", "Eugene Belilovsky"], "title": "Rethinking Prompt Optimization: Reinforcement, Diversification, and Migration in Blackbox LLMs", "comment": null, "summary": "An increasing number of NLP applications interact with large language models\n(LLMs) through black-box APIs, making prompt engineering critical for\ncontrolling model outputs. While recent Automatic Prompt Optimization (APO)\nmethods iteratively refine prompts using model-generated feedback, textual\ngradients, they primarily focus on error correction and neglect valuable\ninsights from correct predictions. This limits both their effectiveness and\nefficiency. In this paper, we propose a novel APO framework centered on\nenhancing the feedback mechanism. We reinterpret the textual gradient as a form\nof negative reinforcement and introduce the complementary positive\nreinforcement to explicitly preserve beneficial prompt components identified\nthrough successful predictions. To mitigate the noise inherent in LLM-generated\nfeedback, we introduce a technique called feedback diversification, which\naggregates multiple feedback signals, emphasizing consistent, actionable advice\nwhile filtering out outliers. Motivated by the rapid evolution and diversity of\navailable LLMs, we also formalize Continual Prompt Optimization (CPO),\naddressing the practical challenge of efficiently migrating optimized prompts\nbetween different model versions or API providers. Our experiments reveal that\nnaive prompt migration often degrades performance due to loss of critical\ninstructions. In contrast, our approach consistently outperforms strong\nbaselines, achieving significant accuracy improvements, faster convergence, and\nlower computational costs in both standard and migration scenarios.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u6b63\u5f3a\u5316\u548c\u53cd\u9988\u591a\u6837\u5316\u6765\u6539\u8fdb\u63d0\u793a\u4f18\u5316\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u6301\u7eed\u63d0\u793a\u4f18\u5316\uff08CPO\uff09\u6765\u89e3\u51b3\u63d0\u793a\u8fc1\u79fb\u95ee\u9898\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u9519\u8bef\u7ea0\u6b63\uff0c\u5ffd\u89c6\u4e86\u6b63\u786e\u9884\u6d4b\u63d0\u4f9b\u7684\u5b9d\u8d35\u89c1\u89e3\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\u3002\u6b64\u5916\uff0c\u968f\u7740 LLM \u7684\u5feb\u901f\u53d1\u5c55\u548c\u591a\u6837\u5316\uff0c\u5c06\u4f18\u5316\u540e\u7684\u63d0\u793a\u9ad8\u6548\u5730\u8fc1\u79fb\u5230\u4e0d\u540c\u6a21\u578b\u7248\u672c\u6216 API \u63d0\u4f9b\u5546\u9762\u4e34\u5b9e\u9645\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u63d0\u793a\u4f18\u5316\uff08APO\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u6539\u8fdb\u4e86\u63d0\u793a\u4f18\u5316\u8fc7\u7a0b\uff1a1. \u589e\u5f3a\u53cd\u9988\u673a\u5236\uff1a\u5c06\u6587\u672c\u68af\u5ea6\u91cd\u65b0\u89e3\u91ca\u4e3a\u8d1f\u5f3a\u5316\uff0c\u5e76\u5f15\u5165\u6b63\u5f3a\u5316\u6765\u4fdd\u7559\u901a\u8fc7\u6210\u529f\u9884\u6d4b\u8bc6\u522b\u51fa\u7684\u6709\u76ca\u63d0\u793a\u7ec4\u4ef6\u30022. \u53cd\u9988\u591a\u6837\u5316\uff1a\u901a\u8fc7\u805a\u5408\u591a\u4e2a\u53cd\u9988\u4fe1\u53f7\u6765\u51cf\u8f7b LLM \u751f\u6210\u7684\u53cd\u9988\u4e2d\u7684\u566a\u58f0\uff0c\u5f3a\u8c03\u4e00\u81f4\u7684\u3001\u53ef\u64cd\u4f5c\u7684\u5efa\u8bae\uff0c\u540c\u65f6\u8fc7\u6ee4\u6389\u5f02\u5e38\u503c\u30023. \u6301\u7eed\u63d0\u793a\u4f18\u5316\uff08CPO\uff09\uff1a\u5c06\u63d0\u793a\u4f18\u5316\u6269\u5c55\u5230\u8de8\u4e0d\u540c\u6a21\u578b\u7248\u672c\u6216 API \u63d0\u4f9b\u5546\u8fc1\u79fb\u4f18\u5316\u7684\u63d0\u793a\uff0c\u4ee5\u5e94\u5bf9LLM\u7684\u5feb\u901f\u53d1\u5c55\u548c\u591a\u6837\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u6587\u63d0\u51fa\u7684\u6846\u67b6\u5728\u51c6\u786e\u6027\u3001\u6536\u655b\u901f\u5ea6\u548c\u8ba1\u7b97\u6210\u672c\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728\u6807\u51c6\u548c\u8fc1\u79fb\u573a\u666f\u4e0b\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002\u4e0e\u6b64\u5f62\u6210\u5bf9\u6bd4\u7684\u662f\uff0c\u7b80\u5355\u7684\u63d0\u793a\u8fc1\u79fb\u901a\u5e38\u4f1a\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u6b63\u5f3a\u5316\u6765\u589e\u5f3a\u53cd\u9988\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u53cd\u9988\u591a\u6837\u5316\u6280\u672f\u6765\u51cf\u8f7b\u566a\u58f0\uff0c\u5728\u4fdd\u6301\u548c\u4f18\u5316\u63d0\u793a\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002\u8be5\u6846\u67b6\u8fd8\u901a\u8fc7\u5f62\u5f0f\u5316\u6301\u7eed\u63d0\u793a\u4f18\u5316\uff08CPO\uff09\u89e3\u51b3\u4e86\u63d0\u793a\u5728\u4e0d\u540c\u6a21\u578b\u7248\u672c\u4e4b\u95f4\u8fc1\u79fb\u7684\u5b9e\u9645\u6311\u6218\uff0c\u5e76\u5728\u5404\u79cd\u573a\u666f\u4e0b\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u3002"}}
{"id": "2507.09993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.09993", "abs": "https://arxiv.org/abs/2507.09993", "authors": ["Yixun Zhang", "Lizhi Wang", "Junjun Zhao", "Wending Zhao", "Feng Zhou", "Yonghao Dang", "Jianqin Yin"], "title": "3DGAA: Realistic and Robust 3D Gaussian-based Adversarial Attack for Autonomous Driving", "comment": "Submitted to WACV 2026", "summary": "Camera-based object detection systems play a vital role in autonomous\ndriving, yet they remain vulnerable to adversarial threats in real-world\nenvironments. While existing 2D and 3D physical attacks typically optimize\ntexture, they often struggle to balance physical realism and attack robustness.\nIn this work, we propose 3D Gaussian-based Adversarial Attack (3DGAA), a novel\nadversarial object generation framework that leverages the full 14-dimensional\nparameterization of 3D Gaussian Splatting (3DGS) to jointly optimize geometry\nand appearance in physically realizable ways. Unlike prior works that rely on\npatches or texture, 3DGAA jointly perturbs both geometric attributes (shape,\nscale, rotation) and appearance attributes (color, opacity) to produce\nphysically realistic and transferable adversarial objects. We further introduce\na physical filtering module to preserve geometric fidelity, and a physical\naugmentation module to simulate complex physical scenarios, thus enhancing\nattack generalization under real-world conditions. We evaluate 3DGAA on both\nvirtual benchmarks and physical-world setups using miniature vehicle models.\nExperimental results show that 3DGAA achieves to reduce the detection mAP from\n87.21% to 7.38%, significantly outperforming existing 3D physical attacks.\nMoreover, our method maintains high transferability across different physical\nconditions, demonstrating a new state-of-the-art in physically realizable\nadversarial attacks. These results validate 3DGAA as a practical attack\nframework for evaluating the safety of perception systems in autonomous\ndriving.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a3DGAA\u7684\u65b0\u578b3D\u9ad8\u65af\u6cfc\u6e85\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u4f18\u5316\u51e0\u4f55\u548c\u5916\u89c2\u5c5e\u6027\uff0c\u5b9e\u73b0\u4e86\u7269\u7406\u4e0a\u53ef\u5b9e\u73b0\u4e14\u5177\u6709\u9c81\u68d2\u6027\u7684\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u663e\u8457\u63d0\u5347\u4e86\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u8bc4\u4f30\u80fd\u529b\u3002", "motivation": "\u4e3a\u89e3\u51b3\u73b0\u67092D\u548c3D\u7269\u7406\u653b\u51fb\u5728\u5e73\u8861\u7269\u7406\u771f\u5b9e\u6027\u548c\u653b\u51fb\u9c81\u68d2\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76f8\u673a\u5bf9\u8c61\u68c0\u6d4b\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u5b89\u5168\u6027\u3002", "method": "3DGAA\u6846\u67b6\u901a\u8fc7\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u5b8c\u657414\u7ef4\u53c2\u6570\u5316\uff0c\u8054\u5408\u4f18\u5316\u51e0\u4f55\u5c5e\u6027\uff08\u5f62\u72b6\u3001\u5c3a\u5ea6\u3001\u65cb\u8f6c\uff09\u548c\u5916\u89c2\u5c5e\u6027\uff08\u989c\u8272\u3001\u4e0d\u900f\u660e\u5ea6\uff09\uff0c\u4ee5\u7269\u7406\u53ef\u5b9e\u73b0\u7684\u65b9\u5f0f\u751f\u6210\u5bf9\u6297\u6027\u5bf9\u8c61\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u7269\u7406\u8fc7\u6ee4\u6a21\u5757\u4ee5\u4fdd\u6301\u51e0\u4f55\u4fdd\u771f\u5ea6\uff0c\u5e76\u5f15\u5165\u4e86\u7269\u7406\u589e\u5f3a\u6a21\u5757\u6765\u6a21\u62df\u590d\u6742\u7684\u7269\u7406\u573a\u666f\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\u7684\u653b\u51fb\u6cdb\u5316\u80fd\u529b\u3002", "result": "3DGAA\u5728\u865a\u62df\u57fa\u51c6\u548c\u7269\u7406\u4e16\u754c\u8bbe\u7f6e\u4e2d\u5747\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u5c06\u68c0\u6d4bmAP\u4ece87.21%\u964d\u4f4e\u52307.38%\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u76843D\u7269\u7406\u653b\u51fb\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7269\u7406\u6761\u4ef6\u4e0b\u4fdd\u6301\u4e86\u9ad8\u53ef\u8f6c\u79fb\u6027\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u653b\u51fb\u6846\u67b6\u5728\u8bc4\u4f30\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u7cfb\u7edf\u5b89\u5168\u6027\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "3DGAA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6297\u6027\u5bf9\u8c61\u751f\u6210\u6846\u67b6\uff0c\u5229\u75283D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u768414\u7ef4\u53c2\u6570\u5316\u6765\u4f18\u5316\u51e0\u4f55\u548c\u5916\u89c2\uff0c\u4ece\u800c\u5b9e\u73b0\u7269\u7406\u53ef\u5b9e\u73b0\u6027\u548c\u9c81\u68d2\u6027\u3002\u901a\u8fc7\u5f15\u5165\u7269\u7406\u8fc7\u6ee4\u548c\u589e\u5f3a\u6a21\u5757\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u7269\u7406\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c3DGAA\u5728\u51cf\u5c11\u68c0\u6d4bmAP\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u76843D\u7269\u7406\u653b\u51fb\uff0c\u5e76\u5728\u4e0d\u540c\u7269\u7406\u6761\u4ef6\u4e0b\u8868\u73b0\u51fa\u9ad8\u53ef\u8f6c\u79fb\u6027\uff0c\u4ee3\u8868\u4e86\u7269\u7406\u53ef\u5b9e\u73b0\u5bf9\u6297\u6027\u653b\u51fb\u7684\u65b0\u6c34\u5e73\u3002"}}
{"id": "2507.09846", "categories": ["cs.LG", "cs.AI", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09846", "abs": "https://arxiv.org/abs/2507.09846", "authors": ["Minhak Song", "Beomhan Baek", "Kwangjun Ahn", "Chulhee Yun"], "title": "Through the River: Understanding the Benefit of Schedule-Free Methods for Language Model Training", "comment": "Comments would be appreciated!", "summary": "As both model and dataset sizes continue to scale rapidly, conventional\npretraining strategies with fixed compute budgets-such as cosine learning rate\nschedules-are increasingly inadequate for large-scale training. Recent\nalternatives, including warmup-stable-decay (WSD) schedules and weight\naveraging, offer greater flexibility. However, WSD relies on explicit decay\nphases to track progress, while weight averaging addresses this limitation at\nthe cost of additional memory. In search of a more principled and scalable\nalternative, we revisit the Schedule-Free (SF) method [Defazio et al., 2024],\nwhich has shown strong empirical performance across diverse settings. We show\nthat SF-AdamW effectively navigates the \"river\" structure of the loss landscape\nwithout decay phases or auxiliary averaging, making it particularly suitable\nfor continuously scaling training workloads. To understand this behavior, we\nconduct a theoretical and empirical analysis of SF dynamics, revealing that it\nimplicitly performs weight averaging without memory overhead. Guided by this\nanalysis, we propose a refined variant of SF that improves robustness to\nmomentum and performs better under large batch sizes, addressing key\nlimitations of the original method. Together, these results establish SF as a\npractical, scalable, and theoretically grounded approach for language model\ntraining.", "AI": {"tldr": "SF-AdamW \u662f\u4e00\u79cd\u65e0\u9700\u8870\u51cf\u6216\u989d\u5916\u5185\u5b58\u5373\u53ef\u6709\u6548\u8bad\u7ec3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6539\u8fdb\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u9690\u5f0f\u6743\u91cd\u5e73\u5747\u6765\u5bfc\u822a\u635f\u5931\u51fd\u6570\u3002", "motivation": "\u4f20\u7edf\u7684\u5177\u6709\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u7684\u9884\u8bad\u7ec3\u7b56\u7565\uff08\u5982\u4f59\u5f26\u5b66\u4e60\u7387\u8ba1\u5212\uff09\u5bf9\u4e8e\u5927\u89c4\u6a21\u8bad\u7ec3\u6765\u8bf4\u8d8a\u6765\u8d8a\u4e0d\u591f\u7528\u3002\u867d\u7136 WSD \u548c\u6743\u91cd\u5e73\u5747\u63d0\u4f9b\u4e86\u66f4\u5927\u7684\u7075\u6d3b\u6027\uff0c\u4f46 WSD \u4f9d\u8d56\u4e8e\u663e\u5f0f\u7684\u8870\u51cf\u9636\u6bb5\uff0c\u800c\u6743\u91cd\u5e73\u5747\u5219\u9700\u8981\u989d\u5916\u7684\u5185\u5b58\u3002\u5728\u5bfb\u627e\u66f4\u5408\u7406\u3001\u66f4\u5177\u53ef\u6269\u5c55\u6027\u7684\u66ff\u4ee3\u65b9\u6848\u65f6\uff0c\u6211\u4eec\u91cd\u65b0\u5ba1\u89c6\u4e86 Schedule-Free (SF) \u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790 SF \u52a8\u6001\uff0c\u63ed\u793a\u5176\u5728\u6ca1\u6709\u5185\u5b58\u5f00\u9500\u7684\u60c5\u51b5\u4e0b\u9690\u5f0f\u6267\u884c\u6743\u91cd\u5e73\u5747\u3002\u63d0\u51fa\u4e00\u79cd\u6539\u8fdb\u7684 SF \u53d8\u4f53\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u52a8\u91cf\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5927\u6279\u91cf\u5c3a\u5bf8\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "result": "SF-AdamW \u6709\u6548\u5730\u5bfc\u822a\u635f\u5931\u51fd\u6570\u7684\u201c\u6cb3\u6d41\u201d\u7ed3\u6784\uff0c\u65e0\u9700\u8870\u51cf\u9636\u6bb5\u6216\u8f85\u52a9\u5e73\u5747\uff0c\u5e76\u4e14\u9690\u5f0f\u6267\u884c\u6743\u91cd\u5e73\u5747\u800c\u6ca1\u6709\u5185\u5b58\u5f00\u9500\u3002\u6539\u8fdb\u7684 SF \u53d8\u4f53\u63d0\u9ad8\u4e86\u5bf9\u52a8\u91cf\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5927\u6279\u91cf\u5c3a\u5bf8\u4e0b\u8868\u73b0\u66f4\u597d\u3002", "conclusion": "SF-AdamW \u662f\u4e00\u79cd\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u4e14\u5177\u6709\u7406\u8bba\u57fa\u7840\u7684\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\uff0c\u5728\u4e0d\u4f7f\u7528\u8870\u51cf\u9636\u6bb5\u6216\u8f85\u52a9\u5e73\u5747\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u5bfc\u822a\u635f\u5931\u51fd\u6570\u7684\u201c\u6cb3\u6d41\u201d\u7ed3\u6784\uff0c\u7279\u522b\u9002\u5408\u6301\u7eed\u6269\u5c55\u8bad\u7ec3\u5de5\u4f5c\u8d1f\u8f7d\u3002"}}
{"id": "2507.09996", "categories": ["cs.CV", "q-bio.NC", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.09996", "abs": "https://arxiv.org/abs/2507.09996", "authors": ["Quentin Dessain", "Nicolas Delinte", "Bernard Hanseeuw", "Laurence Dricot", "Beno\u00eet Macq"], "title": "Leveraging Swin Transformer for enhanced diagnosis of Alzheimer's disease using multi-shell diffusion MRI", "comment": null, "summary": "Objective: This study aims to support early diagnosis of Alzheimer's disease\nand detection of amyloid accumulation by leveraging the microstructural\ninformation available in multi-shell diffusion MRI (dMRI) data, using a vision\ntransformer-based deep learning framework.\n  Methods: We present a classification pipeline that employs the Swin\nTransformer, a hierarchical vision transformer model, on multi-shell dMRI data\nfor the classification of Alzheimer's disease and amyloid presence. Key metrics\nfrom DTI and NODDI were extracted and projected onto 2D planes to enable\ntransfer learning with ImageNet-pretrained models. To efficiently adapt the\ntransformer to limited labeled neuroimaging data, we integrated Low-Rank\nAdaptation. We assessed the framework on diagnostic group prediction\n(cognitively normal, mild cognitive impairment, Alzheimer's disease dementia)\nand amyloid status classification.\n  Results: The framework achieved competitive classification results within the\nscope of multi-shell dMRI-based features, with the best balanced accuracy of\n95.2% for distinguishing cognitively normal individuals from those with\nAlzheimer's disease dementia using NODDI metrics. For amyloid detection, it\nreached 77.2% balanced accuracy in distinguishing amyloid-positive mild\ncognitive impairment/Alzheimer's disease dementia subjects from\namyloid-negative cognitively normal subjects, and 67.9% for identifying\namyloid-positive individuals among cognitively normal subjects. Grad-CAM-based\nexplainability analysis identified clinically relevant brain regions, including\nthe parahippocampal gyrus and hippocampus, as key contributors to model\npredictions.\n  Conclusion: This study demonstrates the promise of diffusion MRI and\ntransformer-based architectures for early detection of Alzheimer's disease and\namyloid pathology, supporting biomarker-driven diagnostics in data-limited\nbiomedical settings.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u6269\u6563MRI\u548cSwin Transformer\u8fdb\u884c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u65e9\u671f\u68c0\u6d4b\uff0c\u5728\u533a\u5206\u6b63\u5e38\u4e0eAD\u60a3\u8005\u65b9\u9762\u51c6\u786e\u7387\u8fbe95.2%\uff0c\u5728\u68c0\u6d4b\u6dc0\u7c89\u6837\u86cb\u767d\u65b9\u9762\u51c6\u786e\u7387\u8fbe77.2%\uff0c\u5e76\u8bc6\u522b\u51fa\u6d77\u9a6c\u65c1\u56de\u548c\u6d77\u9a6c\u4e3a\u5173\u952e\u533a\u57df\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u591a\u5c42\u6269\u6563MRI\u6570\u636e\u4e2d\u7684\u5fae\u89c2\u7ed3\u6784\u4fe1\u606f\uff0c\u901a\u8fc7\u57fa\u4e8e\u89c6\u89c9Transformer\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u652f\u6301\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u65e9\u671f\u8bca\u65ad\u548c\u6dc0\u7c89\u6837\u86cb\u767d\u79ef\u7d2f\u7684\u68c0\u6d4b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u6d41\u7a0b\uff0c\u5728\u591a\u5c42\u6269\u6563MRI\u6570\u636e\u4e0a\u4f7f\u7528Swin Transformer\uff08\u4e00\u79cd\u5206\u5c42\u89c6\u89c9Transformer\u6a21\u578b\uff09\u5bf9\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u6dc0\u7c89\u6837\u86cb\u767d\u7684\u5b58\u5728\u8fdb\u884c\u5206\u7c7b\u3002\u63d0\u53d6\u4e86DTI\u548cNODDI\u7684\u5173\u952e\u6307\u6807\uff0c\u5e76\u5c06\u5176\u6295\u5f71\u52302D\u5e73\u9762\u4e0a\uff0c\u4ee5\u4fbf\u4e0eImageNet\u9884\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u8fc1\u79fb\u5b66\u4e60\u3002\u4e3a\u4e86\u6709\u6548\u5730\u5c06Transformer\u9002\u5e94\u4e8e\u6709\u9650\u7684\u6807\u8bb0\u795e\u7ecf\u5f71\u50cf\u6570\u636e\uff0c\u6211\u4eec\u96c6\u6210\u4e86\u4f4e\u79e9\u81ea\u9002\u5e94\u3002\u6211\u4eec\u8bc4\u4f30\u4e86\u8be5\u6846\u67b6\u5728\u8bca\u65ad\u7ec4\u9884\u6d4b\uff08\u8ba4\u77e5\u6b63\u5e38\u3001\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\u3001\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u75f4\u5446\uff09\u548c\u6dc0\u7c89\u6837\u86cb\u767d\u72b6\u6001\u5206\u7c7b\u65b9\u9762\u7684\u80fd\u529b\u3002", "result": "\u8be5\u6846\u67b6\u5728\u57fa\u4e8e\u591a\u5c42\u6269\u6563MRI\u7279\u5f81\u7684\u5206\u7c7b\u7ed3\u679c\u65b9\u9762\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5728\u4f7f\u7528NODDI\u6307\u6807\u533a\u5206\u8ba4\u77e5\u6b63\u5e38\u4e2a\u4f53\u548c\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u75f4\u5446\u4e2a\u4f53\u65b9\u9762\uff0c\u6700\u4f73\u5e73\u8861\u51c6\u786e\u7387\u4e3a95.2%\u3002\u5728\u6dc0\u7c89\u6837\u86cb\u767d\u68c0\u6d4b\u65b9\u9762\uff0c\u533a\u5206\u6dc0\u7c89\u6837\u86cb\u767d\u9633\u6027\u7684\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d/\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u75f4\u5446\u53d7\u8bd5\u8005\u548c\u6dc0\u7c89\u6837\u86cb\u767d\u9634\u6027\u7684\u8ba4\u77e5\u6b63\u5e38\u53d7\u8bd5\u8005\uff0c\u5176\u5e73\u8861\u51c6\u786e\u7387\u4e3a77.2%\uff1b\u5728\u533a\u5206\u6dc0\u7c89\u6837\u86cb\u767d\u9633\u6027\u4e2a\u4f53\u548c\u8ba4\u77e5\u6b63\u5e38\u4e2a\u4f53\u65b9\u9762\uff0c\u5176\u5e73\u8861\u51c6\u786e\u7387\u4e3a67.9%\u3002\u57fa\u4e8eGrad-CAM\u7684\u53ef\u89e3\u91ca\u6027\u5206\u6790\u786e\u5b9a\u4e86\u4e0e\u4e34\u5e8a\u76f8\u5173\u7684\u8111\u533a\uff0c\u5305\u62ec\u6d77\u9a6c\u65c1\u56de\u548c\u6d77\u9a6c\uff0c\u662f\u6a21\u578b\u9884\u6d4b\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u6269\u6563MRI\u548c\u57fa\u4e8eTransformer\u7684\u67b6\u6784\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u548c\u6dc0\u7c89\u6837\u86cb\u767d\u75c5\u7406\u7684\u65e9\u671f\u68c0\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u652f\u6301\u5728\u6570\u636e\u53d7\u9650\u7684\u751f\u7269\u533b\u5b66\u73af\u5883\u4e2d\u8fdb\u884c\u7531\u751f\u7269\u6807\u5fd7\u7269\u9a71\u52a8\u7684\u8bca\u65ad\u3002"}}
{"id": "2507.09871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09871", "abs": "https://arxiv.org/abs/2507.09871", "authors": ["Niket Patel", "Randall Balestriero"], "title": "Task Priors: Enhancing Model Evaluation by Considering the Entire Space of Downstream Tasks", "comment": null, "summary": "The grand goal of AI research, and particularly Self Supervised Learning\n(SSL), is to produce systems that can successfully solve any possible task. In\ncontrast, current evaluation methods available to AI researchers typically rely\non a fixed collection of hand-picked downstream benchmarks. Hence, a large\namount of effort is put into designing and searching for large collection of\nevaluation tasks that can serve as a proxy of our grand goal. We argue that\nsuch a rigid evaluation protocol creates a silent bottleneck in AI research. To\nremedy that, we define a probabilistic space of downstream tasks obtained by\nadopting a distribution of tasks and by defining Task Priors. Under this view,\none can evaluate a model's performance over the set of all possible downstream\ntasks. Our framework is the first to provide answers to key questions such as\n(i) what is the average performance of my model over all possible downstream\ntasks weighted by the probability to encounter each task? or (ii) what is the\nvariance of my model's performance across all downstream tasks under the\ndefined Task Priors? Beyond establishing a new standard for evaluation, we\nbelieve that Task Priors will accelerate the pace of research in SSL - where\ndownstream task evaluation is the sole qualitative signal that researchers have\naccess to.", "AI": {"tldr": "\u5f53\u524dAI\u8bc4\u4f30\u65b9\u6cd5\u8fc7\u4e8e\u4f9d\u8d56\u56fa\u5b9a\u57fa\u51c6\uff0c\u9650\u5236\u4e86SSL\u7814\u7a76\u8fdb\u5c55\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u7387\u6027\u4efb\u52a1\u7a7a\u95f4\u548c\u4efb\u52a1\u5148\u9a8c\u7684\u6982\u5ff5\uff0c\u7528\u4e8e\u5728\u6240\u6709\u53ef\u80fd\u7684\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8bc4\u4f30\u6a21\u578b\u6027\u80fd\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u7814\u7a76\uff0c\u7279\u522b\u662f\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u7684\u8bc4\u4f30\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u3001\u624b\u52a8\u9009\u62e9\u7684\u4e0b\u6e38\u57fa\u51c6\u6d4b\u8bd5\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u7814\u7a76\u7684\u8fdb\u5c55\u3002\u4f5c\u8005\u8ba4\u4e3a\u8fd9\u79cd\u50f5\u5316\u7684\u8bc4\u4f30\u534f\u8bae\u662f\u7814\u7a76\u4e2d\u7684\u4e00\u4e2a\u201c\u65e0\u58f0\u74f6\u9888\u201d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6982\u7387\u6027\u4efb\u52a1\u7a7a\u95f4\u7684\u6982\u5ff5\uff0c\u8be5\u7a7a\u95f4\u901a\u8fc7\u91c7\u7528\u4efb\u52a1\u5206\u5e03\u548c\u5b9a\u4e49\u4efb\u52a1\u5148\u9a8c\u6765\u5b9e\u73b0\u3002\u8fd9\u5141\u8bb8\u8bc4\u4f30\u6a21\u578b\u5728\u6240\u6709\u53ef\u80fd\u7684\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u548c\u6027\u80fd\u65b9\u5dee\u3002", "result": "\u8be5\u6846\u67b6\u9996\u6b21\u80fd\u591f\u56de\u7b54\u5173\u4e8e\u6a21\u578b\u5728\u6240\u6709\u53ef\u80fd\u7684\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5e73\u5747\u6027\u80fd\u4ee5\u53ca\u5728\u7279\u5b9a\u4efb\u52a1\u5148\u9a8c\u4e0b\u7684\u6027\u80fd\u65b9\u5dee\u7b49\u5173\u952e\u95ee\u9898\u3002\u4f5c\u8005\u76f8\u4fe1\u8be5\u6846\u67b6\u5c06\u52a0\u901fSSL\u9886\u57df\u7684\u7814\u7a76\u3002", "conclusion": "\u4e3a\u4eba\u5de5\u667a\u80fd\u7814\u7a76\uff08\u5c24\u5176\u662f\u81ea\u76d1\u7763\u5b66\u4e60\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5141\u8bb8\u6a21\u578b\u5728\u6574\u4e2a\u53ef\u80fd\u7684\u4e0b\u6e38\u4efb\u52a1\u7a7a\u95f4\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u800c\u4e0d\u662f\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u57fa\u51c6\u6d4b\u8bd5\u96c6\u3002"}}
{"id": "2507.10006", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10006", "abs": "https://arxiv.org/abs/2507.10006", "authors": ["Guanghai Ding", "Yihua Ren", "Yuting Liu", "Qijun Zhao", "Shuiwang Li"], "title": "Vision-Based Anti Unmanned Aerial Technology: Opportunities and Challenges", "comment": null, "summary": "With the rapid advancement of UAV technology and its extensive application in\nvarious fields such as military reconnaissance, environmental monitoring, and\nlogistics, achieving efficient and accurate Anti-UAV tracking has become\nessential. The importance of Anti-UAV tracking is increasingly prominent,\nespecially in scenarios such as public safety, border patrol, search and\nrescue, and agricultural monitoring, where operations in complex environments\ncan provide enhanced security. Current mainstream Anti-UAV tracking\ntechnologies are primarily centered around computer vision techniques,\nparticularly those that integrate multi-sensor data fusion with advanced\ndetection and tracking algorithms. This paper first reviews the characteristics\nand current challenges of Anti-UAV detection and tracking technologies. Next,\nit investigates and compiles several publicly available datasets, providing\naccessible links to support researchers in efficiently addressing related\nchallenges. Furthermore, the paper analyzes the major vision-based and\nvision-fusion-based Anti-UAV detection and tracking algorithms proposed in\nrecent years. Finally, based on the above research, this paper outlines future\nresearch directions, aiming to provide valuable insights for advancing the\nfield.", "AI": {"tldr": "\u4e00\u7bc7\u5173\u4e8e\u53cd\u65e0\u4eba\u673a\u8ddf\u8e2a\u6280\u672f\u7684\u7efc\u8ff0\uff0c\u6db5\u76d6\u4e86\u6311\u6218\u3001\u6570\u636e\u96c6\u3001\u7b97\u6cd5\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\u6280\u672f\u7684\u53d1\u5c55\u548c\u5e7f\u6cdb\u5e94\u7528\uff0c\u5b9e\u73b0\u9ad8\u6548\u7cbe\u51c6\u7684\u53cd\u65e0\u4eba\u673a\u8ddf\u8e2a\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u5c24\u5176\u662f\u5728\u516c\u5171\u5b89\u5168\u3001\u8fb9\u5883\u5de1\u903b\u3001\u641c\u7d22\u6551\u63f4\u548c\u519c\u4e1a\u76d1\u6d4b\u7b49\u590d\u6742\u73af\u5883\u4e2d\u3002", "method": "\u672c\u6587\u9996\u5148\u56de\u987e\u4e86\u53cd\u65e0\u4eba\u673a\u68c0\u6d4b\u548c\u8ddf\u8e2a\u6280\u672f\u7684\u7279\u70b9\u4e0e\u6311\u6218\uff0c\u63a5\u7740\u8c03\u7814\u5e76\u6574\u7406\u4e86\u516c\u5f00\u6570\u636e\u96c6\uff0c\u6700\u540e\u5206\u6790\u4e86\u8fd1\u5e74\u6765\u63d0\u51fa\u7684\u4e3b\u8981\u57fa\u4e8e\u89c6\u89c9\u548c\u57fa\u4e8e\u89c6\u89c9\u878d\u5408\u7684\u53cd\u65e0\u4eba\u673a\u68c0\u6d4b\u4e0e\u8ddf\u8e2a\u7b97\u6cd5\u3002", "result": "\u8be5\u8bba\u6587\u4e3a\u63a8\u52a8\u8be5\u9886\u57df\u53d1\u5c55\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u89c1\u89e3\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u8bba\u6587\u6982\u8ff0\u4e86\u53cd\u65e0\u4eba\u673a\u8ddf\u8e2a\u6280\u672f\uff08Anti-UAV tracking technologies\uff09\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2507.09882", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09882", "abs": "https://arxiv.org/abs/2507.09882", "authors": ["Jiamin Wu", "Zichen Ren", "Junyu Wang", "Pengyu Zhu", "Yonghao Song", "Mianxin Liu", "Qihao Zheng", "Lei Bai", "Wanli Ouyang", "Chunfeng Song"], "title": "AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications", "comment": null, "summary": "Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible\nmeans of connecting the human brain to external devices, with broad\napplications in home and clinical settings to enhance human capabilities.\nHowever, the high noise level and limited task-specific data in non-invasive\nsignals constrain decoding capabilities. Recently, the adoption of\nself-supervised pre-training is transforming the landscape of non-invasive BCI\nresearch, enabling the development of brain foundation models to capture\ngeneric neural representations from large-scale unlabeled\nelectroencephalography (EEG) signals with substantial noises. However, despite\nthese advances, the field currently lacks comprehensive, practical and\nextensible benchmarks to assess the utility of the public foundation models\nacross diverse BCI tasks, hindering their widespread adoption. To address this\nchallenge, we present AdaBrain-Bench, a large-scale standardized benchmark to\nsystematically evaluate brain foundation models in widespread non-invasive BCI\ntasks. AdaBrain-Bench encompasses a diverse collection of representative BCI\ndecoding datasets spanning 7 key applications. It introduces a streamlined task\nadaptation pipeline integrated with multi-dimensional evaluation metrics and a\nset of adaptation tools. The benchmark delivers an inclusive framework for\nassessing generalizability of brain foundation models across key transfer\nsettings, including cross-subject, multi-subject, and few-shot scenarios. We\nleverage AdaBrain-Bench to evaluate a suite of publicly available brain\nfoundation models and offer insights into practices for selecting appropriate\nmodels in various scenarios. We make our benchmark pipeline available to enable\nreproducible research and external use, offering a continuously evolving\nplatform to foster progress toward robust and generalized neural decoding\nsolutions.", "AI": {"tldr": "AdaBrain-Bench \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u975e\u4fb5\u5165\u6027 BCI \u8111\u90e8\u57fa\u7840\u6a21\u578b\u7684\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u9009\u62e9\u548c\u6539\u8fdb\u6a21\u578b\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u73b0\u6709\u975e\u4fb5\u5165\u6027 BCI \u7814\u7a76\u7f3a\u4e4f\u5168\u9762\u7684\u3001\u5b9e\u7528\u7684\u548c\u53ef\u6269\u5c55\u7684\u57fa\u51c6\u6765\u8bc4\u4f30\u516c\u5171\u57fa\u7840\u6a21\u578b\u5728\u4e0d\u540c BCI \u4efb\u52a1\u4e2d\u7684\u6548\u7528\uff0c\u8fd9\u963b\u788d\u4e86\u5b83\u4eec\u7684\u5e7f\u6cdb\u5e94\u7528\u3002", "method": "AdaBrain-Bench \u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u6807\u51c6\u5316\u57fa\u51c6\uff0c\u5305\u542b\u4ee3\u8868\u6027\u7684 BCI \u89e3\u7801\u6570\u636e\u96c6\uff08\u6db5\u76d6 7 \u4e2a\u5173\u952e\u5e94\u7528\uff09\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u7b80\u5316\u7684\u4efb\u52a1\u9002\u5e94\u6d41\u7a0b\uff0c\u96c6\u6210\u4e86\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u548c\u4e00\u5957\u9002\u5e94\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u8111\u90e8\u57fa\u7840\u6a21\u578b\u5728\u8de8\u53d7\u8bd5\u8005\u3001\u591a\u53d7\u8bd5\u8005\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u8be5\u57fa\u51c6\u6210\u529f\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u516c\u5f00\u7684\u8111\u90e8\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u5728\u5404\u79cd\u573a\u666f\u4e0b\u9009\u62e9\u5408\u9002\u6a21\u578b\u7684\u5b9e\u8df5\u89c1\u89e3\u3002\u8be5\u57fa\u51c6\u6846\u67b6\u4e3a\u8bc4\u4f30\u8111\u90e8\u57fa\u7840\u6a21\u578b\u5728\u5173\u952e\u8fc1\u79fb\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u5305\u5bb9\u6027\u3002", "conclusion": "AdaBrain-Bench \u7684\u53d1\u5e03\u5c06\u4e3a\u8bc4\u4f30\u548c\u9009\u62e9\u975e\u4fb5\u5165\u6027 BCI \u9886\u57df\u7684\u8111\u90e8\u57fa\u7840\u6a21\u578b\u63d0\u4f9b\u4e00\u4e2a\u6807\u51c6\u5316\u3001\u53ef\u6269\u5c55\u4e14\u5b9e\u7528\u7684\u57fa\u51c6\u3002\u901a\u8fc7\u89e3\u51b3\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u8be5\u57fa\u51c6\u6709\u52a9\u4e8e\u63a8\u52a8\u53ef\u590d\u73b0\u7684\u7814\u7a76\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u901a\u7528\u795e\u7ecf\u89e3\u7801\u89e3\u51b3\u65b9\u6848\u505a\u51fa\u8d21\u732e\u3002"}}
{"id": "2507.10009", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10009", "abs": "https://arxiv.org/abs/2507.10009", "authors": ["Geyou Zhang", "Kai Liu", "Ce Zhu"], "title": "Binomial Self-Compensation: Mechanism and Suppression of Motion Error in Phase-Shifting Profilometry", "comment": null, "summary": "Phase shifting profilometry (PSP) is widely used in high-precision 3D\nscanning due to its high accuracy, robustness, and pixel-wise handling.\nHowever, a fundamental assumption of PSP that the object should remain static\ndoes not hold in dynamic measurement, making PSP susceptible to object motion.\nTo address this challenge, our proposed solution, phase-sequential binomial\nself-compensation (P-BSC), sums successive motion-affected phase frames\nweighted by binomial coefficients. This approach exponentially reduces the\nmotion error in a pixel-wise and frame-wise loopable manner. Despite its\nefficacy, P-BSC suffers from high computational overhead and error accumulation\ndue to its reliance on multi-frame phase calculations and weighted summations.\nInspired by P-BSC, we propose an image-sequential binomial self-compensation\n(I-BSC) to weight sum the homogeneous fringe images instead of successive phase\nframes, which generalizes the BSC concept from phase sequences to image\nsequences. I-BSC computes the arctangent function only once, resolving both\nlimitations in P-BSC. Extensive analysis, simulations, and experiments show\nthat 1) the proposed BSC outperforms existing methods in reducing motion error\nwhile achieving a quasi-single-shot frame rate, i.e., depth map frame rate\nequals to the camera's acquisition rate, enabling 3D reconstruction with high\npixel-depth-temporal resolution; 2) compared to P-BSC, our I-BSC reduces the\ncomputational complexity by one polynomial order, thereby accelerating the\ncomputational frame rate by several to dozen times, while also reaching faster\nmotion error convergence.", "AI": {"tldr": "\u63d0\u51faI-BSC\u65b9\u6cd5\uff0c\u6bd4P-BSC\u8ba1\u7b97\u66f4\u5feb\uff0c\u8bef\u5dee\u66f4\u5c0f\uff0c\u9002\u7528\u4e8e\u52a8\u60013D\u626b\u63cf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u76f8\u4f4d\u79fb\u76f8\u8f6e\u5ed3\u672f\uff08PSP\uff09\u5728\u52a8\u6001\u6d4b\u91cf\u4e2d\u56e0\u7269\u4f53\u8fd0\u52a8\u800c\u6613\u53d7\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u5e76\u514b\u670d\u73b0\u6709\u76f8\u4f4d\u8865\u507f\u65b9\u6cd5\uff08\u5982P-BSC\uff09\u7684\u8ba1\u7b97\u5f00\u9500\u548c\u8bef\u5dee\u7d2f\u79ef\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u56fe\u50cf\u5e8f\u5217\u4e8c\u9879\u5f0f\u81ea\u8865\u507f\uff08I-BSC\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u9f50\u50cf\u5e8f\u5217\u800c\u4e0d\u662f\u76f8\u4f4d\u5e8f\u5217\u6765\u51cf\u8f7b\u8fd0\u52a8\u8bef\u5dee\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u3002", "result": "\u4e0eP-BSC\u76f8\u6bd4\uff0cI-BSC\u5c06\u8ba1\u7b97\u590d\u6742\u5ea6\u964d\u4f4e\u4e86\u4e00\u4e2a\u591a\u9879\u5f0f\u9636\u6570\uff0c\u5c06\u8ba1\u7b97\u5e27\u7387\u63d0\u9ad8\u4e86\u6570\u500d\u5230\u6570\u5341\u500d\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u8fd0\u52a8\u8bef\u5dee\u6536\u655b\uff0c\u4ece\u800c\u5728\u9ad8\u50cf\u7d20\u6df1\u5ea6\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c3D\u91cd\u5efa\u3002", "conclusion": "I-BSC\u901a\u8fc7\u5bf9\u9f50\u50cf\u5e8f\u5217\u800c\u4e0d\u662f\u76f8\u4f4d\u5e8f\u5217\u6765\u51cf\u8f7b\u8fd0\u52a8\u8bef\u5dee\uff0c\u5e76\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u590d\u6742\u6027\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5355\u6b21\u62cd\u6444\u7684\u5e27\u7387\uff0c\u4ece\u800c\u80fd\u591f\u5728\u9ad8\u50cf\u7d20\u6df1\u5ea6\u65f6\u95f4\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c3D\u91cd\u5efa\u3002"}}
{"id": "2507.09888", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.09888", "abs": "https://arxiv.org/abs/2507.09888", "authors": ["Huibo Xu", "Likang Wu", "Xianquan Wang", "Haoning Dang", "Chun-Wun Cheng", "Angelica I Aviles-Rivero", "Qi Liu"], "title": "NeuTSFlow: Modeling Continuous Functions Behind Time Series Forecasting", "comment": null, "summary": "Time series forecasting is a fundamental task with broad applications, yet\nconventional methods often treat data as discrete sequences, overlooking their\norigin as noisy samples of continuous processes. Crucially, discrete noisy\nobservations cannot uniquely determine a continuous function; instead, they\ncorrespond to a family of plausible functions. Mathematically, time series can\nbe viewed as noisy observations of a continuous function family governed by a\nshared probability measure. Thus, the forecasting task can be framed as\nlearning the transition from the historical function family to the future\nfunction family. This reframing introduces two key challenges: (1) How can we\nleverage discrete historical and future observations to learn the relationships\nbetween their underlying continuous functions? (2) How can we model the\ntransition path in function space from the historical function family to the\nfuture function family? To address these challenges, we propose NeuTSFlow, a\nnovel framework that leverages Neural Operators to facilitate flow matching for\nlearning path of measure between historical and future function families. By\nparameterizing the velocity field of the flow in infinite-dimensional function\nspaces, NeuTSFlow moves beyond traditional methods that focus on dependencies\nat discrete points, directly modeling function-level features instead.\nExperiments on diverse forecasting tasks demonstrate NeuTSFlow's superior\naccuracy and robustness, validating the effectiveness of the function-family\nperspective.", "AI": {"tldr": "NeuTSFlow\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u8fde\u7eed\u51fd\u6570\u65cf\uff0c\u5e76\u5b66\u4e60\u51fd\u6570\u65cf\u4e4b\u95f4\u7684\u8fc7\u6e21\u8def\u5f84\u6765\u6539\u8fdb\u9884\u6d4b\u3002\u5b83\u4f7f\u7528\u795e\u7ecf\u7b97\u5b50\u548c\u6d41\u5339\u914d\u6765\u76f4\u63a5\u5efa\u6a21\u51fd\u6570\u7ea7\u7279\u5f81\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u793a\u51fa\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u901a\u5e38\u5c06\u6570\u636e\u89c6\u4e3a\u79bb\u6563\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u5176\u4f5c\u4e3a\u8fde\u7eed\u8fc7\u7a0b\u566a\u58f0\u6837\u672c\u7684\u672c\u8d28\u3002\u79bb\u6563\u566a\u58f0\u89c2\u6d4b\u65e0\u6cd5\u552f\u4e00\u786e\u5b9a\u8fde\u7eed\u51fd\u6570\uff0c\u56e0\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u65f6\u95f4\u5e8f\u5217\u89c6\u4e3a\u7531\u5171\u4eab\u6982\u7387\u6d4b\u5ea6\u51b3\u5b9a\u7684\u8fde\u7eed\u51fd\u6570\u65cf\u566a\u58f0\u89c2\u6d4b\u7684\u65b0\u89c6\u89d2\uff0c\u5e76\u5c06\u9884\u6d4b\u4efb\u52a1\u91cd\u6784\u4e3a\u5b66\u4e60\u4ece\u5386\u53f2\u51fd\u6570\u65cf\u5230\u672a\u6765\u51fd\u6570\u65cf\u7684\u8fc7\u6e21\u3002", "method": "NeuTSFlow\u6846\u67b6\u5229\u7528\u795e\u7ecf\u7b97\u5b50\u4fc3\u8fdb\u6d41\u5339\u914d\uff0c\u4ee5\u5b66\u4e60\u5386\u53f2\u548c\u672a\u6765\u51fd\u6570\u65cf\u4e4b\u95f4\u7684\u5ea6\u91cf\u8def\u5f84\u3002\u5b83\u901a\u8fc7\u53c2\u6570\u5316\u65e0\u9650\u7ef4\u5ea6\u51fd\u6570\u7a7a\u95f4\u4e2d\u7684\u901f\u5ea6\u573a\uff0c\u76f4\u63a5\u5efa\u6a21\u51fd\u6570\u7ea7\u7279\u5f81\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u5173\u6ce8\u79bb\u6563\u70b9\u4f9d\u8d56\u5173\u7cfb\u7684\u65b9\u6cd5\u3002", "result": "NeuTSFlow\u6846\u67b6\u80fd\u591f\u6709\u6548\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684NeuTSFlow\u6846\u67b6\u5728\u5404\u79cd\u9884\u6d4b\u4efb\u52a1\u4e0a\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u9a8c\u8bc1\u4e86\u51fd\u6570\u65cf\u89c6\u89d2\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10015", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10015", "abs": "https://arxiv.org/abs/2507.10015", "authors": ["Jaisidh Singh", "Diganta Misra", "Boris Knyazev", "Antonio Orvieto"], "title": "(Almost) Free Modality Stitching of Foundation Models", "comment": "Pre-print", "summary": "Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\nautoregressive text model. This stitching process is performed by training a\nconnector module that aims to align the representation-representation or\nrepresentation-input spaces of these uni-modal models. However, given the\ncomplexity of training such connectors on large scale web-based datasets\ncoupled with the ever-increasing number of available pretrained uni-modal\nmodels, the task of uni-modal models selection and subsequent connector module\ntraining becomes computationally demanding. To address this under-studied\ncritical problem, we propose Hypernetwork Model Alignment (Hyma), a novel\nall-in-one solution for optimal uni-modal model selection and connector\ntraining by leveraging hypernetworks. Specifically, our framework utilizes the\nparameter prediction capability of a hypernetwork to obtain jointly trained\nconnector modules for $N \\times M$ combinations of uni-modal models. In our\nexperiments, Hyma reduces the optimal uni-modal model pair search cost by\n$10\\times$ (averaged across all experiments), while matching the ranking and\ntrained connector performance obtained via grid search across a suite of\ndiverse multi-modal benchmarks.", "AI": {"tldr": "Automating uni-modal model selection and connector training for foundation models using hypernetworks (Hyma), saving time and computational resources.", "motivation": "The process of selecting uni-modal models and training connector modules for foundation multi-modal models is computationally demanding due to the complexity of large-scale datasets and the increasing number of available uni-modal models.", "method": "Hyma utilizes hypernetworks to predict parameters for jointly trained connector modules, enabling efficient selection and training for N x M uni-modal model combinations.", "result": "Hyma reduces the optimal uni-modal model pair search cost by 10x on average and matches the performance of grid search across various multi-modal benchmarks.", "conclusion": "Hyma is an effective solution for optimal uni-modal model selection and connector training, reducing search cost by 10x while maintaining performance."}}
{"id": "2507.09890", "categories": ["cs.LG", "cs.AI", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.09890", "abs": "https://arxiv.org/abs/2507.09890", "authors": ["Ping Xu", "Pengfei Wang", "Zhiyuan Ning", "Meng Xiao", "Min Wu", "Yuanchun Zhou"], "title": "Soft Graph Clustering for single-cell RNA Sequencing Data", "comment": null, "summary": "Clustering analysis is fundamental in single-cell RNA sequencing (scRNA-seq)\ndata analysis for elucidating cellular heterogeneity and diversity. Recent\ngraph-based scRNA-seq clustering methods, particularly graph neural networks\n(GNNs), have significantly improved in tackling the challenges of\nhigh-dimension, high-sparsity, and frequent dropout events that lead to\nambiguous cell population boundaries. However, their reliance on hard graph\nconstructions derived from thresholded similarity matrices presents\nchallenges:(i) The simplification of intercellular relationships into binary\nedges (0 or 1) by applying thresholds, which restricts the capture of\ncontinuous similarity features among cells and leads to significant information\nloss.(ii) The presence of significant inter-cluster connections within hard\ngraphs, which can confuse GNN methods that rely heavily on graph structures,\npotentially causing erroneous message propagation and biased clustering\noutcomes. To tackle these challenges, we introduce scSGC, a Soft Graph\nClustering for single-cell RNA sequencing data, which aims to more accurately\ncharacterize continuous similarities among cells through non-binary edge\nweights, thereby mitigating the limitations of rigid data structures. The scSGC\nframework comprises three core components: (i) a zero-inflated negative\nbinomial (ZINB)-based feature autoencoder; (ii) a dual-channel cut-informed\nsoft graph embedding module; and (iii) an optimal transport-based clustering\noptimization module. Extensive experiments across ten datasets demonstrate that\nscSGC outperforms 13 state-of-the-art clustering models in clustering accuracy,\ncell type annotation, and computational efficiency. These results highlight its\nsubstantial potential to advance scRNA-seq data analysis and deepen our\nunderstanding of cellular heterogeneity.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10029", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10029", "abs": "https://arxiv.org/abs/2507.10029", "authors": ["Seokeon Choi", "Sunghyun Park", "Hyoungwoo Park", "Jeongho Kim", "Sungrack Yun"], "title": "Memory-Efficient Personalization of Text-to-Image Diffusion Models via Selective Optimization Strategies", "comment": null, "summary": "Memory-efficient personalization is critical for adapting text-to-image\ndiffusion models while preserving user privacy and operating within the limited\ncomputational resources of edge devices. To this end, we propose a selective\noptimization framework that adaptively chooses between backpropagation on\nlow-resolution images (BP-low) and zeroth-order optimization on high-resolution\nimages (ZO-high), guided by the characteristics of the diffusion process. As\nobserved in our experiments, BP-low efficiently adapts the model to\ntarget-specific features, but suffers from structural distortions due to\nresolution mismatch. Conversely, ZO-high refines high-resolution details with\nminimal memory overhead but faces slow convergence when applied without prior\nadaptation. By complementing both methods, our framework leverages BP-low for\neffective personalization while using ZO-high to maintain structural\nconsistency, achieving memory-efficient and high-quality fine-tuning. To\nmaximize the efficacy of both BP-low and ZO-high, we introduce a timestep-aware\nprobabilistic function that dynamically selects the appropriate optimization\nstrategy based on diffusion timesteps. This function mitigates the overfitting\nfrom BP-low at high timesteps, where structural information is critical, while\nensuring ZO-high is applied more effectively as training progresses.\nExperimental results demonstrate that our method achieves competitive\nperformance while significantly reducing memory consumption, enabling scalable,\nhigh-quality on-device personalization without increasing inference latency.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BP-low\u548cZO-high\u7684\u5185\u5b58\u9ad8\u6548\u4e2a\u6027\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u65f6\u95f4\u6b65\u611f\u77e5\u6982\u7387\u51fd\u6570\u52a8\u6001\u9009\u62e9\u7b56\u7565\uff0c\u4ee5\u63d0\u9ad8\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5728\u4fdd\u6301\u7528\u6237\u9690\u79c1\u548c\u9002\u5e94\u8fb9\u7f18\u8bbe\u5907\u7684\u6709\u9650\u8ba1\u7b97\u8d44\u6e90\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u5185\u5b58\u9ad8\u6548\u4e2a\u6027\u5316\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u4f18\u5316\u6846\u67b6\uff0c\u6839\u636e\u6269\u6563\u8fc7\u7a0b\u7684\u7279\u6027\u81ea\u9002\u5e94\u5730\u9009\u62e9\u5728\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u8fdb\u884c\u53cd\u5411\u4f20\u64ad\uff08BP-low\uff09\u6216\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u8fdb\u884c\u65e0\u5bfc\u6570\u4f18\u5316\uff08ZO-high\uff09\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u65f6\u95f4\u6b65\u611f\u77e5\u6982\u7387\u51fd\u6570\uff0c\u6839\u636e\u6269\u6563\u65f6\u95f4\u6b65\u52a8\u6001\u9009\u62e9\u4f18\u5316\u7b56\u7565\uff0c\u4ee5\u51cf\u8f7bBP-low\u5728\u9ad8\u65f6\u95f4\u6b65\u7684\u8fc7\u62df\u5408\uff0c\u5e76\u66f4\u6709\u6548\u5730\u5e94\u7528ZO-high\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u5185\u5b58\u6d88\u8017\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u3001\u9ad8\u8d28\u91cf\u7684\u8bbe\u5907\u5185\u4e2a\u6027\u5316\uff0c\u4e14\u6ca1\u6709\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u53cd\u5411\u4f20\u64ad\uff08BP-low\uff09\u548c\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u65e0\u5bfc\u6570\u4f18\u5316\uff08ZO-high\uff09\uff0c\u5b9e\u73b0\u4e86\u5185\u5b58\u9ad8\u6548\u4e14\u9ad8\u8d28\u91cf\u7684\u4e2a\u6027\u5316\uff0c\u540c\u65f6\u5728\u8bbe\u5907\u4e0a\u8fd0\u884c\u800c\u4e0d\u4f1a\u589e\u52a0\u63a8\u7406\u5ef6\u8fdf\u3002"}}
{"id": "2507.09897", "categories": ["cs.LG", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2507.09897", "abs": "https://arxiv.org/abs/2507.09897", "authors": ["Loek van Rossem", "Andrew M. Saxe"], "title": "Algorithm Development in Neural Networks: Insights from the Streaming Parity Task", "comment": "28 pages, 20 figures", "summary": "Even when massively overparameterized, deep neural networks show a remarkable\nability to generalize. Research on this phenomenon has focused on\ngeneralization within distribution, via smooth interpolation. Yet in some\nsettings neural networks also learn to extrapolate to data far beyond the\nbounds of the original training set, sometimes even allowing for infinite\ngeneralization, implying that an algorithm capable of solving the task has been\nlearned. Here we undertake a case study of the learning dynamics of recurrent\nneural networks (RNNs) trained on the streaming parity task in order to develop\nan effective theory of algorithm development. The streaming parity task is a\nsimple but nonlinear task defined on sequences up to arbitrary length. We show\nthat, with sufficient finite training experience, RNNs exhibit a phase\ntransition to perfect infinite generalization. Using an effective theory for\nthe representational dynamics, we find an implicit representational merger\neffect which can be interpreted as the construction of a finite automaton that\nreproduces the task. Overall, our results disclose one mechanism by which\nneural networks can generalize infinitely from finite training experience.", "AI": {"tldr": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ecRNN\uff09\u53ef\u4ee5\u901a\u8fc7\u5b66\u4e60\u52a8\u6001\uff0c\u5728\u6709\u9650\u7684\u8bad\u7ec3\u6570\u636e\u4e0b\u5b9e\u73b0\u5bf9\u4efb\u52a1\u7684\u65e0\u9650\u6cdb\u5316\uff0c\u5176\u673a\u5236\u662f\u6784\u5efa\u51fa\u80fd\u591f\u7cbe\u786e\u6267\u884c\u4efb\u52a1\u7684\u6709\u9650\u81ea\u52a8\u673a\u3002", "motivation": "\u63a2\u7a76\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u80fd\u591f\u4ece\u8bad\u7ec3\u6570\u636e\u7684\u8fb9\u754c\u5916\u63a8\u5230\u6570\u636e\uff0c\u751a\u81f3\u5b9e\u73b0\u65e0\u9650\u6cdb\u5316\u7684\u73b0\u8c61\uff0c\u5e76\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u7b97\u6cd5\u5f00\u53d1\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u3002", "method": "\u901a\u8fc7\u5bf9\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u5728\u6d41\u5f0f\u5947\u5076\u6821\u9a8c\u4efb\u52a1\u4e0a\u7684\u5b66\u4e60\u52a8\u6001\u8fdb\u884c\u6848\u4f8b\u7814\u7a76\uff0c\u5e76\u8fd0\u7528\u6709\u6548\u7684\u8868\u5f81\u52a8\u6001\u7406\u8bba\u8fdb\u884c\u5206\u6790\u3002", "result": "\u5728\u5145\u5206\u7684\u6709\u9650\u8bad\u7ec3\u7ecf\u9a8c\u4e0b\uff0c\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u8868\u73b0\u51fa\u5411\u5b8c\u7f8e\u65e0\u9650\u6cdb\u5316\u7684\u9636\u6bb5\u6027\u8f6c\u53d8\u3002\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63ed\u793a\u4e86\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\u5728\u5904\u7406\u6d41\u5f0f\u5947\u5076\u6821\u9a8c\u4efb\u52a1\u65f6\uff0c\u901a\u8fc7\u9690\u5f0f\u8868\u5f81\u5408\u5e76\u6548\u5e94\uff0c\u80fd\u591f\u6784\u5efa\u51fa\u80fd\u591f\u7cbe\u786e\u590d\u73b0\u8be5\u4efb\u52a1\u7684\u6709\u9650\u81ea\u52a8\u673a\uff0c\u4ece\u800c\u5728\u6709\u9650\u7684\u8bad\u7ec3\u7ecf\u9a8c\u4e0b\u5b9e\u73b0\u65e0\u9650\u6cdb\u5316\u3002"}}
{"id": "2507.09925", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09925", "abs": "https://arxiv.org/abs/2507.09925", "authors": ["Md Ahsanul Kabir", "Abrar Jahin", "Mohammad Al Hasan"], "title": "Extracting Cause-Effect Pairs from a Sentence with a Dependency-Aware Transformer Model", "comment": null, "summary": "Extracting cause and effect phrases from a sentence is an important NLP task,\nwith numerous applications in various domains, including legal, medical,\neducation, and scientific research. There are many unsupervised and supervised\nmethods proposed for solving this task. Among these, unsupervised methods\nutilize various linguistic tools, including syntactic patterns, dependency\ntree, dependency relations, etc. among different sentential units for\nextracting the cause and effect phrases. On the other hand, the contemporary\nsupervised methods use various deep learning based mask language models\nequipped with a token classification layer for extracting cause and effect\nphrases. Linguistic tools, specifically, dependency tree, which organizes a\nsentence into different semantic units have been shown to be very effective for\nextracting semantic pairs from a sentence, but existing supervised methods do\nnot have any provision for utilizing such tools within their model framework.\nIn this work, we propose DepBERT, which extends a transformer-based model by\nincorporating dependency tree of a sentence within the model framework.\nExtensive experiments over three datasets show that DepBERT is better than\nvarious state-of-the art supervised causality extraction methods.", "AI": {"tldr": "A new model called DepBERT incorporates dependency trees into transformer models to improve the extraction of cause and effect phrases, outperforming existing methods.", "motivation": "Extracting cause and effect phrases from a sentence is an important NLP task, with numerous applications in various domains. Existing supervised methods do not have any provision for utilizing linguistic tools, specifically dependency tree, which has been shown to be very effective for extracting semantic pairs.", "method": "Extending a transformer-based model by incorporating dependency tree of a sentence within the model framework.", "result": "Extensive experiments over three datasets show that DepBERT is better than various state-of-the art supervised causality extraction methods.", "conclusion": "DepBERT, which extends a transformer-based model by incorporating dependency tree of a sentence within the model framework, is better than various state-of-the art supervised causality extraction methods."}}
{"id": "2507.10053", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10053", "abs": "https://arxiv.org/abs/2507.10053", "authors": ["Marc Serra Ortega", "Emanuele Vivoli", "Artemis Llabr\u00e9s", "Dimosthenis Karatzas"], "title": "CoSMo: A Multimodal Transformer for Page Stream Segmentation in Comic Books", "comment": null, "summary": "This paper introduces CoSMo, a novel multimodal Transformer for Page Stream\nSegmentation (PSS) in comic books, a critical task for automated content\nunderstanding, as it is a necessary first stage for many downstream tasks like\ncharacter analysis, story indexing, or metadata enrichment. We formalize PSS\nfor this unique medium and curate a new 20,800-page annotated dataset. CoSMo,\ndeveloped in vision-only and multimodal variants, consistently outperforms\ntraditional baselines and significantly larger general-purpose vision-language\nmodels across F1-Macro, Panoptic Quality, and stream-level metrics. Our\nfindings highlight the dominance of visual features for comic PSS\nmacro-structure, yet demonstrate multimodal benefits in resolving challenging\nambiguities. CoSMo establishes a new state-of-the-art, paving the way for\nscalable comic book analysis.", "AI": {"tldr": "CoSMo \u662f\u4e00\u4e2a\u7528\u4e8e\u6f2b\u753b\u4e66\u9875\u9762\u6d41\u5206\u5272\u7684\u65b0\u578b\u591a\u6a21\u6001 Transformer \u6a21\u578b\uff0c\u5728\u5404\u9879\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9875\u9762\u6d41\u5206\u5272\u662f\u6f2b\u753b\u4e66\u5185\u5bb9\u7406\u89e3\u7684\u5173\u952e\u4efb\u52a1\uff0c\u662f\u8bb8\u591a\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u89d2\u8272\u5206\u6790\u3001\u6545\u4e8b\u7d22\u5f15\u6216\u5143\u6570\u636e\u4e30\u5bcc\uff09\u7684\u5fc5\u8981\u7b2c\u4e00\u9636\u6bb5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001 Transformer \u6a21\u578b CoSMo\uff0c\u5e76\u5f00\u53d1\u4e86\u5176\u4ec5\u89c6\u89c9\u548c\u591a\u6a21\u6001\u53d8\u4f53\uff0c\u7528\u4e8e\u6f2b\u753b\u4e66\u7684\u9875\u9762\u6d41\u5206\u5272\u3002", "result": "CoSMo \u5728 F1-Macro\u3001\u5168\u666f\u8d28\u91cf\u548c\u6d41\u7ea7\u522b\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u548c\u66f4\u5927\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u89c6\u89c9\u7279\u5f81\u5728\u6f2b\u753b\u9875\u9762\u6d41\u5206\u5272\u5b8f\u89c2\u7ed3\u6784\u4e2d\u7684\u4e3b\u5bfc\u5730\u4f4d\uff0c\u5e76\u5c55\u793a\u4e86\u591a\u6a21\u6001\u5728\u89e3\u51b3\u6a21\u7cca\u6027\u65b9\u9762\u7684\u4f18\u52bf\u3002", "conclusion": "CoSMo \u5728\u6f2b\u753b\u4e66\u9875\u9762\u6d41\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\uff0c\u4e3a\u53ef\u6269\u5c55\u7684\u6f2b\u753b\u4e66\u5206\u6790\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.09931", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09931", "abs": "https://arxiv.org/abs/2507.09931", "authors": ["Yoon Pyo Lee"], "title": "Mechanistic Interpretability of LoRA-Adapted Language Models for Nuclear Reactor Safety Applications", "comment": "Submitted to Nuclear Technology. 22 pages, 2 tables, 4 figures", "summary": "The integration of Large Language Models (LLMs) into safety-critical domains,\nsuch as nuclear engineering, necessitates a deep understanding of their\ninternal reasoning processes. This paper presents a novel methodology for\ninterpreting how an LLM encodes and utilizes domain-specific knowledge, using a\nBoiling Water Reactor system as a case study. We adapted a general-purpose LLM\n(Gemma-3-1b-it) to the nuclear domain using a parameter-efficient fine-tuning\ntechnique known as Low-Rank Adaptation. By comparing the neuron activation\npatterns of the base model to those of the fine-tuned model, we identified a\nsparse set of neurons whose behavior was significantly altered during the\nadaptation process. To probe the causal role of these specialized neurons, we\nemployed a neuron silencing technique. Our results demonstrate that while\nsilencing most of these specialized neurons individually did not produce a\nstatistically significant effect, deactivating the entire group collectively\nled to a statistically significant degradation in task performance. Qualitative\nanalysis further revealed that silencing these neurons impaired the model's\nability to generate detailed, contextually accurate technical information. This\npaper provides a concrete methodology for enhancing the transparency of an\nopaque black-box model, allowing domain expertise to be traced to verifiable\nneural circuits. This offers a pathway towards achieving nuclear-grade\nartificial intelligence (AI) assurance, addressing the verification and\nvalidation challenges mandated by nuclear regulatory frameworks (e.g., 10 CFR\n50 Appendix B), which have limited AI deployment in safety-critical nuclear\noperations.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u548c\u64cd\u4f5c\u795e\u7ecf\u5143\uff0c\u6765\u7406\u89e3\u548c\u6539\u8fdb\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6838\u5de5\u7a0b\u9886\u57df\u7684\u5e94\u7528\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u7279\u5b9a\u795e\u7ecf\u5143\u7684\u7ec4\u5408\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4e3a\u63d0\u9ad8 AI \u5728\u5b89\u5168\u5173\u952e\u9886\u57df\u7684\u53ef\u9760\u6027\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u4e3a\u4e86\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u96c6\u6210\u5230\u6838\u5de5\u7a0b\u7b49\u5b89\u5168\u5173\u952e\u9886\u57df\uff0c\u9700\u8981\u6df1\u5165\u7406\u89e3\u5176\u5185\u90e8\u63a8\u7406\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u7684\u900f\u660e\u5ea6\uff0c\u89e3\u51b3\u6838\u76d1\u7ba1\u6846\u67b6\u4e0b\u7684\u9a8c\u8bc1\u548c\u9a8c\u8bc1\u6311\u6218\uff0c\u4ece\u800c\u4fc3\u8fdb\u4eba\u5de5\u667a\u80fd\u5728\u5b89\u5168\u5173\u952e\u6838\u64cd\u4f5c\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4f4e\u79e9\u81ea\u9002\u5e94\uff08LoRA\uff09\u6280\u672f\u5bf9\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff08Gemma-3-1b-it\uff09\u8fdb\u884c\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff0c\u4f7f\u5176\u9002\u5e94\u6838\u5de5\u7a0b\u9886\u57df\u3002\u901a\u8fc7\u6bd4\u8f83\u5fae\u8c03\u524d\u540e\u6a21\u578b\u7684\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u5143\u6c89\u9ed8\u6280\u672f\u6765\u63a2\u7a76\u7279\u5b9a\u795e\u7ecf\u5143\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u56e0\u679c\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u6838\u5de5\u7a0b\u9886\u57df\u5fae\u8c03\u540e\u7684\u6a21\u578b\u4e2d\uff0c\u5b58\u5728\u4e00\u7ec4\u7279\u5b9a\u7684\u7a00\u758f\u795e\u7ecf\u5143\uff0c\u5b83\u4eec\u5728\u6a21\u578b\u9002\u5e94\u8fc7\u7a0b\u4e2d\u884c\u4e3a\u53d1\u751f\u663e\u8457\u6539\u53d8\u3002\u867d\u7136\u5355\u72ec\u6c89\u9ed8\u8fd9\u4e9b\u795e\u7ecf\u5143\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u4e0d\u663e\u8457\uff0c\u4f46\u96c6\u4f53\u6c89\u9ed8\u4f1a\u5bfc\u81f4\u6a21\u578b\u5728\u5904\u7406\u6838\u5de5\u7a0b\u4efb\u52a1\u65f6\u7684\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u4e14\u5f71\u54cd\u5176\u751f\u6210\u8be6\u7ec6\u3001\u51c6\u786e\u7684\u6280\u672f\u4fe1\u606f\u7684\u80fd\u529b\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5982\u4f55\u7f16\u7801\u548c\u5229\u7528\u7279\u5b9a\u9886\u57df\u7684\u77e5\u8bc6\u3002\u901a\u8fc7\u6bd4\u8f83\u57fa\u7840\u6a21\u578b\u548c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\uff08\u4f7f\u7528\u4f4e\u79e9\u81ea\u9002\u5e94\u6280\u672f\u9002\u914d\u5230\u6838\u5de5\u7a0b\u9886\u57df\uff09\u7684\u795e\u7ecf\u5143\u6fc0\u6d3b\u6a21\u5f0f\uff0c\u6211\u4eec\u8bc6\u522b\u51fa\u4e00\u7ec4\u7a00\u758f\u7684\u795e\u7ecf\u5143\uff0c\u5b83\u4eec\u5728\u9002\u914d\u8fc7\u7a0b\u4e2d\u884c\u4e3a\u53d1\u751f\u663e\u8457\u6539\u53d8\u3002\u795e\u7ecf\u5143\u6c89\u9ed8\u5b9e\u9a8c\u8868\u660e\uff0c\u5355\u72ec\u6c89\u9ed8\u8fd9\u4e9b\u795e\u7ecf\u5143\u5bf9\u6a21\u578b\u6027\u80fd\u5f71\u54cd\u4e0d\u5927\uff0c\u4f46\u96c6\u4f53\u6c89\u9ed8\u4f1a\u5bfc\u81f4\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff0c\u5e76\u4e14\u6a21\u578b\u751f\u6210\u6280\u672f\u4fe1\u606f\u7684\u80fd\u529b\u53d7\u635f\u3002\u672c\u7814\u7a76\u4e3a\u589e\u5f3a\u4e0d\u900f\u660e\u9ed1\u76d2\u6a21\u578b\u7684\u900f\u660e\u5ea6\u63d0\u4f9b\u4e86\u4e00\u79cd\u5177\u4f53\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5c06\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u8ffd\u6eaf\u5230\u53ef\u9a8c\u8bc1\u7684\u795e\u7ecf\u56de\u8def\uff0c\u4e3a\u5b9e\u73b0\u6838\u7ea7\u4eba\u5de5\u667a\u80fd\u4fdd\u8bc1\u94fa\u5e73\u9053\u8def\uff0c\u89e3\u51b3\u4e86\u6838\u76d1\u7ba1\u6846\u67b6\uff08\u5982 10 CFR 50 Appendix B\uff09\u5f3a\u5236\u8981\u6c42\u7684\u6838\u67e5\u548c\u9a8c\u8bc1\u6311\u6218\u3002"}}
{"id": "2507.10056", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10056", "abs": "https://arxiv.org/abs/2507.10056", "authors": ["A. K. M. Shoriful Islam", "Md. Rakib Hassan", "Macbah Uddin", "Md. Shahidur Rahman"], "title": "Lightweight Model for Poultry Disease Detection from Fecal Images Using Multi-Color Space Feature Optimization and Machine Learning", "comment": null, "summary": "Poultry farming is a vital component of the global food supply chain, yet it\nremains highly vulnerable to infectious diseases such as coccidiosis,\nsalmonellosis, and Newcastle disease. This study proposes a lightweight machine\nlearning-based approach to detect these diseases by analyzing poultry fecal\nimages. We utilize multi-color space feature extraction (RGB, HSV, LAB) and\nexplore a wide range of color, texture, and shape-based descriptors, including\ncolor histograms, local binary patterns (LBP), wavelet transforms, and edge\ndetectors. Through a systematic ablation study and dimensionality reduction\nusing PCA and XGBoost feature selection, we identify a compact global feature\nset that balances accuracy and computational efficiency. An artificial neural\nnetwork (ANN) classifier trained on these features achieved 95.85% accuracy\nwhile requiring no GPU and only 638 seconds of execution time in Google Colab.\nCompared to deep learning models such as Xception and MobileNetV3, our proposed\nmodel offers comparable accuracy with drastically lower resource usage. This\nwork demonstrates a cost-effective, interpretable, and scalable alternative to\ndeep learning for real-time poultry disease detection in low-resource\nagricultural settings.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u5206\u6790\u79bd\u7c7b\u7caa\u4fbf\u56fe\u50cf\u6765\u68c0\u6d4b\u5bb6\u79bd\u75be\u75c5\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u8d44\u6e90\u6d88\u8017\uff0c\u4f18\u4e8e\u4e00\u4e9b\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5bb6\u79bd\u517b\u6b96\u4e1a\u6613\u53d7\u7403\u866b\u75c5\u3001\u6c99\u95e8\u6c0f\u83cc\u75c5\u548c\u65b0\u57ce\u75ab\u7b49\u4f20\u67d3\u75c5\u5f71\u54cd\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u6790\u79bd\u7c7b\u7caa\u4fbf\u56fe\u50cf\u6765\u68c0\u6d4b\u8fd9\u4e9b\u75be\u75c5\uff0c\u4ee5\u63d0\u4f9b\u4e00\u79cd\u6bd4\u6df1\u5ea6\u5b66\u4e60\u66f4\u5177\u6210\u672c\u6548\u76ca\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u591a\u989c\u8272\u7a7a\u95f4\u7279\u5f81\u63d0\u53d6\uff08RGB, HSV, LAB\uff09\u548c\u591a\u79cd\u57fa\u4e8e\u989c\u8272\u3001\u7eb9\u7406\u548c\u5f62\u72b6\u7684\u63cf\u8ff0\u7b26\uff0c\u5e76\u901a\u8fc7\u4e3b\u6210\u5206\u5206\u6790\uff08PCA\uff09\u548cXGBoost\u7279\u5f81\u9009\u62e9\u8fdb\u884c\u964d\u7ef4\uff0c\u6700\u7ec8\u786e\u5b9a\u4e86\u4e00\u4e2a\u7d27\u51d1\u7684\u5168\u5c40\u7279\u5f81\u96c6\uff0c\u5e76\u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff08ANN\uff09\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e8695.85%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u4e0d\u9700\u8981GPU\uff0c\u5728Google Colab\u4e0a\u4ec5\u9700638\u79d2\u7684\u6267\u884c\u65f6\u95f4\u3002\u4e0eXception\u548cMobileNetV3\u7b49\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u51c6\u786e\u7387\u76f8\u5f53\u7684\u60c5\u51b5\u4e0b\uff0c\u8d44\u6e90\u6d88\u8017\u5927\u5927\u964d\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8f7b\u91cf\u7ea7\u673a\u5668\u5b66\u4e60\u7684\u79bd\u7c7b\u7caa\u4fbf\u56fe\u50cf\u75be\u75c5\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\uff0c\u4e3a\u4f4e\u8d44\u6e90\u519c\u4e1a\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5b9e\u65f6\u79bd\u7c7b\u75be\u75c5\u68c0\u6d4b\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2507.09937", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09937", "abs": "https://arxiv.org/abs/2507.09937", "authors": ["Gaurav R. Ghosal", "Pratyush Maini", "Aditi Raghunathan"], "title": "Memorization Sinks: Isolating Memorization during LLM Training", "comment": "Accepted at the 2025 International Conference of Machine Learning", "summary": "Large language models are susceptible to memorizing repeated sequences,\nposing privacy and copyright concerns. A popular mitigation strategy is to\nremove memorized information from specific neurons post-hoc. However, such\napproaches have shown limited success so far. In a controlled setting, we show\nthat the memorization of natural sequences (those that resemble linguistically\nplausible text) become mechanistically entangled with general language\nabilities, thereby becoming challenging to remove post-hoc. In this work, we\nput forward a new paradigm of MemSinks that promotes isolation of memorization\nby design. We leverage a sequence identifier that activates a unique set of\nmemorization neurons for each sequence across repetitions. By analyzing the\ndynamics of learning and forgetting, we argue that MemSinks facilitates\nisolation of memorized content, making it easier to remove without compromising\ngeneral language capabilities. We implement MemSinks at the billion-parameter\nand billion-token scale, and observe both effective isolation and strong\ngeneralization. To our knowledge, this is the first proof-of-concept on real\ndata demonstrating that simultaneous generalization and isolation is\nachievable. We open-source our code at http://github.com/grghosal/MemSinks.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51faMemSinks\u8303\u4f8b\uff0c\u901a\u8fc7\u9694\u79bb\u8bb0\u5fc6\u795e\u7ecf\u5143\u6765\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6613\u4e8e\u8bb0\u5fc6\u91cd\u590d\u5e8f\u5217\u5e26\u6765\u7684\u9690\u79c1\u548c\u7248\u6743\u95ee\u9898\uff0c\u5e76\u514b\u670d\u73b0\u6709\u4e8b\u540e\u79fb\u9664\u8bb0\u5fc6\u4fe1\u606f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMemSinks\u7684\u65b0\u8303\u4f8b\uff0c\u5229\u7528\u5e8f\u5217\u6807\u8bc6\u7b26\u6765\u6fc0\u6d3b\u6bcf\u4e2a\u5e8f\u5217\u72ec\u7279\u7684\u8bb0\u5fc6\u795e\u7ecf\u5143\uff0c\u4ece\u800c\u5728\u8bbe\u8ba1\u4e0a\u4fc3\u8fdb\u8bb0\u5fc6\u7684\u9694\u79bb\u3002\u901a\u8fc7\u5206\u6790\u5b66\u4e60\u548c\u9057\u5fd8\u7684\u52a8\u6001\u8fc7\u7a0b\u6765\u8bba\u8bc1\u5176\u6709\u6548\u6027\u3002", "result": "\u5728\u767e\u4e07\u53c2\u6570\u548c\u767e\u4e07\u8bcd\u5143\u7684\u89c4\u6a21\u4e0a\u6210\u529f\u5b9e\u73b0\u4e86\u8bb0\u5fc6\u5185\u5bb9\u7684\u6709\u6548\u9694\u79bb\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8bc1\u660e\u4e86\u540c\u65f6\u5b9e\u73b0\u6cdb\u5316\u548c\u9694\u79bb\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MemSinks\u65b0\u8303\u4f8b\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4fc3\u8fdb\u8bb0\u5fc6\u7684\u9694\u79bb\uff0c\u4ece\u800c\u5728\u4e0d\u635f\u5bb3\u901a\u7528\u8bed\u8a00\u80fd\u529b\u7684\u60c5\u51b5\u4e0b\uff0c\u66f4\u5bb9\u6613\u5730\u79fb\u9664\u8bb0\u5fc6\u5185\u5bb9\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMemSinks\u5728\u767e\u4e07\u53c2\u6570\u548c\u767e\u4e07\u8bcd\u5143\u7684\u89c4\u6a21\u4e0a\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u9694\u79bb\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8fd9\u662f\u9996\u6b21\u5728\u771f\u5b9e\u6570\u636e\u4e0a\u8bc1\u660e\u540c\u65f6\u5b9e\u73b0\u6cdb\u5316\u548c\u9694\u79bb\u662f\u53ef\u884c\u7684\u3002"}}
{"id": "2507.10065", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10065", "abs": "https://arxiv.org/abs/2507.10065", "authors": ["Chenguo Lin", "Yuchen Lin", "Panwang Pan", "Yifan Yu", "Honglei Yan", "Katerina Fragkiadaki", "Yadong Mu"], "title": "MoVieS: Motion-Aware 4D Dynamic View Synthesis in One Second", "comment": "Project page: https://chenguolin.github.io/projects/MoVieS", "summary": "We present MoVieS, a novel feed-forward model that synthesizes 4D dynamic\nnovel views from monocular videos in one second. MoVieS represents dynamic 3D\nscenes using pixel-aligned grids of Gaussian primitives, explicitly supervising\ntheir time-varying motion. This allows, for the first time, the unified\nmodeling of appearance, geometry and motion, and enables view synthesis,\nreconstruction and 3D point tracking within a single learning-based framework.\nBy bridging novel view synthesis with dynamic geometry reconstruction, MoVieS\nenables large-scale training on diverse datasets with minimal dependence on\ntask-specific supervision. As a result, it also naturally supports a wide range\nof zero-shot applications, such as scene flow estimation and moving object\nsegmentation. Extensive experiments validate the effectiveness and efficiency\nof MoVieS across multiple tasks, achieving competitive performance while\noffering several orders of magnitude speedups.", "AI": {"tldr": "MoVieS\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u4ece\u89c6\u9891\u4e2d\u5feb\u901f\u5408\u6210\u52a8\u6001\u76843D\u89c6\u56fe\uff0c\u5e76\u7edf\u4e00\u5904\u7406\u5916\u89c2\u3001\u51e0\u4f55\u548c\u8fd0\u52a8\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u79cd\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u5728\u5355\u4e2a\u5b66\u4e60\u6846\u67b6\u5185\u5b9e\u73b0\u89c6\u56fe\u5408\u6210\u3001\u91cd\u5efa\u548c3D\u70b9\u8ddf\u8e2a\uff0c\u5e76\u80fd\u591f\u5bf9\u52a8\u60013D\u573a\u666f\u8fdb\u884c\u5efa\u6a21\u3002", "method": "MoVieS\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u524d\u9988\u6a21\u578b\uff0c\u4f7f\u7528\u50cf\u7d20\u5bf9\u9f50\u7684\u9ad8\u65af\u539f\u8bed\u7f51\u683c\u6765\u8868\u793a\u52a8\u60013D\u573a\u666f\uff0c\u5e76\u663e\u5f0f\u76d1\u7763\u5176\u65f6\u53d8\u8fd0\u52a8\uff0c\u4ece\u800c\u7edf\u4e00\u4e86\u5916\u89c2\u3001\u51e0\u4f55\u548c\u8fd0\u52a8\u7684\u5efa\u6a21\u3002", "result": "MoVieS\u80fd\u591f\u5728\u4e00\u79d2\u949f\u5185\u4ece\u5355\u773c\u89c6\u9891\u5408\u62104D\u52a8\u6001\u65b0\u9896\u89c6\u56fe\uff0c\u5b9e\u73b0\u4e86\u89c6\u56fe\u5408\u6210\u3001\u91cd\u5efa\u548c3D\u70b9\u8ddf\u8e2a\u7684\u7edf\u4e00\u5efa\u6a21\uff0c\u5e76\u652f\u6301\u96f6\u6837\u672c\u5e94\u7528\uff0c\u5982\u573a\u666f\u6d41\u4f30\u8ba1\u548c\u8fd0\u52a8\u5bf9\u8c61\u5206\u5272\u3002", "conclusion": "MoVieS\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u548c\u6548\u7387\uff0c\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u6570\u91cf\u7ea7\u4e0a\u7684\u52a0\u901f\u3002"}}
{"id": "2507.09940", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.09940", "abs": "https://arxiv.org/abs/2507.09940", "authors": ["Taigo Sakai", "Kazuhiro Hotta"], "title": "Long-Tailed Data Classification by Increasing and Decreasing Neurons During Training", "comment": null, "summary": "In conventional deep learning, the number of neurons typically remains fixed\nduring training. However, insights from biology suggest that the human\nhippocampus undergoes continuous neuron generation and pruning of neurons over\nthe course of learning, implying that a flexible allocation of capacity can\ncontribute to enhance performance. Real-world datasets often exhibit class\nimbalance situations where certain classes have far fewer samples than others,\nleading to significantly reduce recognition accuracy for minority classes when\nrelying on fixed size networks.To address the challenge, we propose a method\nthat periodically adds and removes neurons during training, thereby boosting\nrepresentational power for minority classes. By retaining critical features\nlearned from majority classes while selectively increasing neurons for\nunderrepresented classes, our approach dynamically adjusts capacity during\ntraining. Importantly, while the number of neurons changes throughout training,\nthe final network size and structure remain unchanged, ensuring efficiency and\ncompatibility with deployment.Furthermore, by experiments on three different\ndatasets and five representative models, we demonstrate that the proposed\nmethod outperforms fixed size networks and shows even greater accuracy when\ncombined with other imbalance-handling techniques. Our results underscore the\neffectiveness of dynamic, biologically inspired network designs in improving\nperformance on class-imbalanced data.", "AI": {"tldr": "\u901a\u8fc7\u52a8\u6001\u589e\u5220\u795e\u7ecf\u5143\u6765\u5e94\u5bf9\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u5c11\u6570\u7c7b\u8bc6\u522b\u7387\u3002", "motivation": "\u53d7\u751f\u7269\u5b66\u542f\u53d1\uff0c\u4eba\u7c7b\u6d77\u9a6c\u4f53\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u4f1a\u6301\u7eed\u751f\u6210\u548c\u4fee\u526a\u795e\u7ecf\u5143\uff0c\u8fd9\u8868\u660e\u7075\u6d3b\u5206\u914d\u5bb9\u91cf\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u3002\u6b64\u5916\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u5e38\u89c1\u7684\u6570\u636e\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u4f1a\u5bfc\u81f4\u5c11\u6570\u7c7b\u522b\u7684\u8bc6\u522b\u51c6\u786e\u7387\u663e\u8457\u964d\u4f4e\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5468\u671f\u6027\u5730\u6dfb\u52a0\u548c\u79fb\u9664\u795e\u7ecf\u5143\u7684\u65b9\u6cd5\uff0c\u5728\u4fdd\u7559\u4ece\u591a\u6570\u7c7b\u522b\u4e2d\u5b66\u5230\u7684\u5173\u952e\u7279\u5f81\u7684\u540c\u65f6\uff0c\u9009\u62e9\u6027\u5730\u4e3a\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u7c7b\u522b\u589e\u52a0\u795e\u7ecf\u5143\uff0c\u4ece\u800c\u52a8\u6001\u8c03\u6574\u5bb9\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u4e94\u4e2a\u4ee3\u8868\u6027\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u56fa\u5b9a\u5927\u5c0f\u7684\u7f51\u7edc\uff0c\u5e76\u4e14\u5728\u4e0e\u5176\u5b83\u4e0d\u5e73\u8861\u5904\u7406\u6280\u672f\u7ed3\u5408\u4f7f\u7528\u65f6\uff0c\u51c6\u786e\u7387\u66f4\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u7f51\u7edc\u5bb9\u91cf\uff0c\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u4e0a\u6709\u6548\u63d0\u5347\u4e86\u5c11\u6570\u7c7b\u522b\u7684\u8bc6\u522b\u51c6\u786e\u7387\uff0c\u5e76\u4e14\u6700\u7ec8\u7f51\u7edc\u5927\u5c0f\u548c\u7ed3\u6784\u4fdd\u6301\u4e0d\u53d8\uff0c\u6613\u4e8e\u90e8\u7f72\u3002"}}
{"id": "2507.10072", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10072", "abs": "https://arxiv.org/abs/2507.10072", "authors": ["Meng Yu", "Kun Zhan"], "title": "Frequency Regulation for Exposure Bias Mitigation in Diffusion Models", "comment": "ACM Multimedia 2025 accepted!", "summary": "Diffusion models exhibit impressive generative capabilities but are\nsignificantly impacted by exposure bias. In this paper, we make a key\nobservation: the energy of the predicted noisy images decreases during the\ndiffusion process. Building on this, we identify two important findings: 1) The\nreduction in energy follows distinct patterns in the low-frequency and\nhigh-frequency subbands; 2) This energy reduction results in amplitude\nvariations between the network-reconstructed clean data and the real clean\ndata. Based on the first finding, we introduce a frequency-domain regulation\nmechanism utilizing wavelet transforms, which separately adjusts the low- and\nhigh-frequency subbands. Leveraging the second insight, we provide a more\naccurate analysis of exposure bias in the two subbands. Our method is\ntraining-free and plug-and-play, significantly improving the generative quality\nof various diffusion models and providing a robust solution to exposure bias\nacross different model architectures. The source code is available at\nhttps://github.com/kunzhan/wpp.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u5b58\u5728\u66dd\u5149\u504f\u5dee\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9891\u57df\u6b63\u5219\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c0f\u6ce2\u53d8\u6362\u5206\u522b\u8c03\u6574\u4f4e\u9891\u548c\u9ad8\u9891\u5b50\u5e26\uff0c\u4ee5\u89e3\u51b3\u6b64\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u8bad\u7ec3\u65e0\u5173\u4e14\u5373\u63d2\u5373\u7528\uff0c\u80fd\u591f\u63d0\u5347\u751f\u6210\u8d28\u91cf\u5e76\u89e3\u51b3\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u7684\u66dd\u5149\u504f\u5dee\u95ee\u9898\u3002", "motivation": "\u6269\u6563\u6a21\u578b\u53d7\u5230\u66dd\u5149\u504f\u5dee\u7684\u663e\u8457\u5f71\u54cd\uff0c\u5e76\u4e14\u5728\u6269\u6563\u8fc7\u7a0b\u4e2d\u9884\u6d4b\u7684\u566a\u58f0\u56fe\u50cf\u7684\u80fd\u91cf\u4f1a\u964d\u4f4e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5c0f\u6ce2\u53d8\u6362\u7684\u9891\u57df\u6b63\u5219\u5316\u673a\u5236\uff0c\u8be5\u673a\u5236\u5206\u522b\u8c03\u6574\u4f4e\u9891\u548c\u9ad8\u9891\u5b50\u5e26\u3002\u5229\u7528\u80fd\u91cf\u964d\u4f4e\u5728\u4e24\u4e2a\u5b50\u5e26\u4e2d\u7684\u4e0d\u540c\u6a21\u5f0f\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u5206\u6790\u66dd\u5149\u504f\u5dee\u3002", "result": "\u80fd\u91cf\u964d\u4f4e\u9075\u5faa\u4f4e\u9891\u548c\u9ad8\u9891\u5b50\u5e26\u4e2d\u4e0d\u540c\u7684\u6a21\u5f0f\uff0c\u80fd\u91cf\u964d\u4f4e\u4f1a\u5bfc\u81f4\u7f51\u7edc\u91cd\u5efa\u7684\u6e05\u6d01\u6570\u636e\u548c\u771f\u5b9e\u7684\u6e05\u6d01\u6570\u636e\u4e4b\u95f4\u7684\u5e45\u5ea6\u53d8\u5316\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u662f\u8bad\u7ec3\u65e0\u5173\u4e14\u5373\u63d2\u5373\u7528\u7684\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5404\u79cd\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8d28\u91cf\uff0c\u5e76\u4e3a\u4e0d\u540c\u6a21\u578b\u67b6\u6784\u63d0\u4f9b\u4e86\u9c81\u68d2\u7684\u66dd\u5149\u504f\u5dee\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10084", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10084", "abs": "https://arxiv.org/abs/2507.10084", "authors": ["Haonan Chen", "Xin Tong"], "title": "A Transfer Learning-Based Method for Water Body Segmentation in Remote Sensing Imagery: A Case Study of the Zhada Tulin Area", "comment": "13 pages, 6 figures, 2 tables", "summary": "To address the prevalent challenges of domain shift and small sample sizes in\nremote sensing image water body segmentation, this study proposes and validates\na two-stage transfer learning strategy based on the SegFormer model. The\napproach begins by training a foundational segmentation model on a diverse\nsource domain, where it achieves an Intersection over Union (IoU) of 68.80% on\nits validation set, followed by fine-tuning on data from the distinct target\ndomain. Focusing on the Zhada Tulin area in Tibet -- a region characterized by\nhighly complex topography and spectral features -- the experimental results\ndemonstrate that this strategy significantly boosts the IoU for the water body\nsegmentation task from 25.50% (for direct transfer) to 64.84%. This not only\neffectively resolves the model performance degradation caused by domain\ndiscrepancy but also provides an effective technical paradigm for\nhigh-precision thematic information extraction in data-scarce and\nenvironmentally unique remote sensing scenarios.", "AI": {"tldr": "\u9488\u5bf9\u9065\u611f\u6c34\u4f53\u5206\u5272\u7684\u57df\u504f\u79fb\u548c\u5c11\u6837\u672c\u95ee\u9898\uff0c\u91c7\u7528SegFormer\u6a21\u578b\u8fdb\u884c\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5206\u5272\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u9065\u611f\u5f71\u50cf\u6c34\u4f53\u5206\u5272\u4e2d\u666e\u904d\u5b58\u5728\u7684\u57df\u504f\u79fb\u548c\u6837\u672c\u91cf\u5c0f\u7b49\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u9a8c\u8bc1\u4e86\u4e00\u79cd\u57fa\u4e8eSegFormer\u6a21\u578b\uff0c\u9762\u5411\u9065\u611f\u5f71\u50cf\u6c34\u4f53\u5206\u5272\u7684\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u3002\u9996\u5148\u5728\u591a\u6837\u5316\u7684\u6e90\u57df\u4e0a\u8bad\u7ec3\u57fa\u7840\u5206\u5272\u6a21\u578b\uff0c\u7136\u540e\u5728\u5dee\u5f02\u663e\u8457\u7684\u76ee\u6807\u57df\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "\u8be5\u7b56\u7565\u5c06\u9065\u611f\u5f71\u50cf\u6c34\u4f53\u5206\u5272\u4efb\u52a1\u7684IoU\u4ece\u76f4\u63a5\u8fc1\u79fb\u768425.50%\u663e\u8457\u63d0\u5347\u81f364.84%\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u56e0\u57df\u5dee\u5f02\u5bfc\u81f4\u7684\u6a21\u578b\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u6709\u6548\u89e3\u51b3\u4e86\u9065\u611f\u5f71\u50cf\u6c34\u4f53\u5206\u5272\u5728\u57df\u9002\u5e94\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3a\u6570\u636e\u7a00\u758f\u548c\u73af\u5883\u72ec\u7279\u7684\u9065\u611f\u573a\u666f\u63d0\u4f9b\u4e86\u9ad8\u7cbe\u5ea6\u4fe1\u606f\u63d0\u53d6\u7684\u6280\u672f\u8303\u4f8b\u3002"}}
{"id": "2507.09949", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09949", "abs": "https://arxiv.org/abs/2507.09949", "authors": ["Md Ahsanul Kabir", "Kareem Abdelfatah", "Mohammed Korayem", "Mohammad Al Hasan"], "title": "Hierarchical Job Classification with Similarity Graph Integration", "comment": null, "summary": "In the dynamic realm of online recruitment, accurate job classification is\nparamount for optimizing job recommendation systems, search rankings, and labor\nmarket analyses. As job markets evolve, the increasing complexity of job titles\nand descriptions necessitates sophisticated models that can effectively\nleverage intricate relationships within job data. Traditional text\nclassification methods often fall short, particularly due to their inability to\nfully utilize the hierarchical nature of industry categories. To address these\nlimitations, we propose a novel representation learning and classification\nmodel that embeds jobs and hierarchical industry categories into a latent\nembedding space. Our model integrates the Standard Occupational Classification\n(SOC) system and an in-house hierarchical taxonomy, Carotene, to capture both\ngraph and hierarchical relationships, thereby improving classification\naccuracy. By embedding hierarchical industry categories into a shared latent\nspace, we tackle cold start issues and enhance the dynamic matching of\ncandidates to job opportunities. Extensive experimentation on a large-scale\ndataset of job postings demonstrates the model's superior ability to leverage\nhierarchical structures and rich semantic features, significantly outperforming\nexisting methods. This research provides a robust framework for improving job\nclassification accuracy, supporting more informed decision-making in the\nrecruitment industry.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8868\u793a\u5b66\u4e60\u548c\u5206\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5229\u7528\u804c\u4f4d\u6570\u636e\u7684\u5206\u5c42\u7ed3\u6784\u6765\u63d0\u9ad8\u5728\u7ebf\u62db\u8058\u4e2d\u7684\u804c\u4f4d\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u6587\u672c\u5206\u7c7b\u65b9\u6cd5\u5728\u5229\u7528\u4f5c\u4e1a\u6570\u636e\u7684\u590d\u6742\u5173\u7cfb\uff0c\u7279\u522b\u662f\u884c\u4e1a\u7c7b\u522b\u7684\u5206\u5c42\u6027\u8d28\u65b9\u9762\uff0c\u5f80\u5f80\u529b\u4e0d\u4ece\u5fc3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8868\u793a\u5b66\u4e60\u548c\u5206\u7c7b\u6a21\u578b\uff0c\u5c06\u804c\u4f4d\u548c\u5206\u5c42\u884c\u4e1a\u7c7b\u522b\u5d4c\u5165\u5230\u6f5c\u5728\u5d4c\u5165\u7a7a\u95f4\u4e2d\u3002\u8be5\u6a21\u578b\u6574\u5408\u4e86\u6807\u51c6\u804c\u4e1a\u5206\u7c7b\uff08SOC\uff09\u7cfb\u7edf\u548c\u5185\u90e8\u5206\u5c42\u5206\u7c7bCarotene\uff0c\u4ee5\u6355\u6349\u56fe\u548c\u5206\u5c42\u5173\u7cfb\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u7c7b\u51c6\u786e\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u901a\u8fc7\u5d4c\u5165\u5206\u5c42\u884c\u4e1a\u7c7b\u522b\u5230\u5171\u4eab\u7684\u6f5c\u5728\u7a7a\u95f4\u6765\u89e3\u51b3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u5e76\u589e\u5f3a\u4e86\u5019\u9009\u4eba\u4e0e\u5de5\u4f5c\u673a\u4f1a\u7684\u52a8\u6001\u5339\u914d\u3002\u5728\u5927\u578b\u804c\u4f4d\u53d1\u5e03\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5229\u7528\u5206\u5c42\u7ed3\u6784\u548c\u4e30\u5bcc\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u4e14\u6027\u80fd\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\u6765\u63d0\u9ad8\u804c\u4f4d\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u652f\u6301\u62db\u8058\u884c\u4e1a\u4e2d\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002"}}
{"id": "2507.10095", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10095", "abs": "https://arxiv.org/abs/2507.10095", "authors": ["Bingchao Wang", "Zhiwei Ning", "Jianyu Ding", "Xuanang Gao", "Yin Li", "Dongsheng Jiang", "Jie Yang", "Wei Liu"], "title": "FIX-CLIP: Dual-Branch Hierarchical Contrastive Learning via Synthetic Captions for Better Understanding of Long Text", "comment": null, "summary": "CLIP has shown promising performance across many short-text tasks in a\nzero-shot manner. However, limited by the input length of the text encoder,\nCLIP struggles on under-stream tasks with long-text inputs (>77 tokens). To\nremedy this issue, we propose FIX-CLIP which includes three novel modules: (1)\nA dual-branch training pipeline that aligns short and long texts with masked\nand raw images respectively, which boosts the long-text representation while\npreserving the short-text ability. (2) Multiple learnable regional prompts with\nunidirectional masks in Transformer layers for regional information extraction.\n(3) A hierarchical feature alignment module in the intermediate encoder layers\nto promote the consistency of multi-scale features. Furthermore, we collect 30M\nimages and utilize existing MLLMs to synthesize long-text captions for\ntraining. Extensive experiments show that FIX-CLIP achieves state-of-the-art\nperformance on both long-text and short-text retrieval benchmarks. For\ndownstream applications, we reveal that FIX-CLIP's text encoder delivers\npromising performance in a plug-and-play manner for diffusion models with\nlong-text input.", "AI": {"tldr": "CLIP\u5728\u5904\u7406\u957f\u6587\u672c\u65f6\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u6587\u63d0\u51fa\u7684FIX-CLIP\u6a21\u578b\u901a\u8fc7\u53cc\u5206\u652f\u8bad\u7ec3\u3001\u591a\u533a\u57df\u63d0\u793a\u548c\u5c42\u6b21\u7279\u5f81\u5bf9\u9f50\u7b49\u65b9\u6cd5\uff0c\u63d0\u5347\u4e86\u957f\u6587\u672c\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5728\u957f\u77ed\u6587\u672c\u68c0\u7d22\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u4f18\u6027\u80fd\uff0c\u540c\u65f6\u4e5f\u80fd\u7528\u4e8e\u6269\u6563\u6a21\u578b\u3002", "motivation": "CLIP\u6a21\u578b\u5728\u77ed\u6587\u672c\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u53d7\u9650\u4e8e\u6587\u672c\u7f16\u7801\u5668\u7684\u8f93\u5165\u957f\u5ea6\uff0c\u96be\u4ee5\u5904\u7406\u957f\u6587\u672c\u8f93\u5165\uff08>77\u4e2a\u6807\u8bb0\uff09\u7684\u4e0b\u6e38\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFIX-CLIP\u7684\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u65b0\u9896\u7684\u6a21\u5757\uff1a1. \u4e00\u4e2a\u53cc\u5206\u652f\u8bad\u7ec3\u6d41\u7a0b\uff0c\u5206\u522b\u5c06\u77ed\u6587\u672c\u4e0e\u63a9\u7801\u56fe\u50cf\u3001\u957f\u6587\u672c\u4e0e\u539f\u59cb\u56fe\u50cf\u5bf9\u9f50\uff0c\u4ee5\u63d0\u5347\u957f\u6587\u672c\u8868\u793a\u80fd\u529b\u5e76\u4fdd\u6301\u77ed\u6587\u672c\u80fd\u529b\u30022. Transformer\u5c42\u4e2d\u7684\u591a\u533a\u57df\u53ef\u5b66\u4e60\u63d0\u793a\u548c\u5355\u5411\u63a9\u7801\uff0c\u7528\u4e8e\u533a\u57df\u4fe1\u606f\u63d0\u53d6\u30023. \u4e2d\u95f4\u7f16\u7801\u5668\u5c42\u4e2d\u7684\u5c42\u6b21\u7279\u5f81\u5bf9\u9f50\u6a21\u5757\uff0c\u4ee5\u4fc3\u8fdb\u591a\u5c3a\u5ea6\u7279\u5f81\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u6536\u96c6\u4e863000\u4e07\u5f20\u56fe\u50cf\u5e76\u5229\u7528\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5408\u6210\u4e86\u7528\u4e8e\u8bad\u7ec3\u7684\u957f\u6587\u672c\u6807\u9898\u3002", "result": "FIX-CLIP\u5728\u957f\u6587\u672c\u548c\u77ed\u6587\u672c\u68c0\u7d22\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5176\u6587\u672c\u7f16\u7801\u5668\u5728\u6269\u6563\u6a21\u578b\u4e2d\u8868\u73b0\u51fa\u6709\u524d\u666f\u7684\u5373\u63d2\u5373\u7528\u6027\u80fd\u3002", "conclusion": "FIX-CLIP\u5728\u957f\u6587\u672c\u548c\u77ed\u6587\u672c\u68c0\u7d22\u57fa\u51c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u5373\u63d2\u5373\u7528\u5730\u5e94\u7528\u4e8e\u9700\u8981\u957f\u6587\u672c\u8f93\u5165\u7684\u6269\u6563\u6a21\u578b\u3002"}}
{"id": "2507.09952", "categories": ["cs.LG", "stat.AP", "stat.ME", "68T01(General topics in artificial intelligence),\n  62G05(Nonparametric estimation)"], "pdf": "https://arxiv.org/pdf/2507.09952", "abs": "https://arxiv.org/abs/2507.09952", "authors": ["Zerui Zhang", "Yumou Qiu"], "title": "Radial Neighborhood Smoothing Recommender System", "comment": "34 pages, 2 figures. Submitted to NeurIPS 2025", "summary": "Recommender systems inherently exhibit a low-rank structure in latent space.\nA key challenge is to define meaningful and measurable distances in the latent\nspace to capture user-user, item-item, user-item relationships effectively. In\nthis work, we establish that distances in the latent space can be\nsystematically approximated using row-wise and column-wise distances in the\nobserved matrix, providing a novel perspective on distance estimation. To\nrefine the distance estimation, we introduce the correction based on empirical\nvariance estimator to account for noise-induced non-centrality. The novel\ndistance estimation enables a more structured approach to constructing\nneighborhoods, leading to the Radial Neighborhood Estimator (RNE), which\nconstructs neighborhoods by including both overlapped and partially overlapped\nuser-item pairs and employs neighborhood smoothing via localized kernel\nregression to improve imputation accuracy. We provide the theoretical\nasymptotic analysis for the proposed estimator. We perform evaluations on both\nsimulated and real-world datasets, demonstrating that RNE achieves superior\nperformance compared to existing collaborative filtering and matrix\nfactorization methods. While our primary focus is on distance estimation in\nlatent space, we find that RNE also mitigates the ``cold-start'' problem.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRNE\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u63a8\u8350\u7cfb\u7edf\u4e2d\u6f5c\u5728\u7a7a\u95f4\u7684\u8ddd\u79bb\uff0c\u901a\u8fc7\u5229\u7528\u89c2\u6d4b\u77e9\u9635\u7684\u884c\u5217\u8ddd\u79bb\u5e76\u8fdb\u884c\u6821\u6b63\uff0c\u63d0\u9ad8\u4e86\u90bb\u57df\u6784\u5efa\u548c\u6570\u636e\u586b\u5145\u7684\u51c6\u786e\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u5e76\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u89e3\u51b3\u4e86\u51b7\u542f\u52a8\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u6709\u6548\u6355\u6349\u7528\u6237-\u7528\u6237\u3001\u9879\u76ee-\u9879\u76ee\u548c\u7528\u6237-\u9879\u76ee\u5173\u7cfb\uff0c\u9700\u8981\u5b9a\u4e49\u6709\u610f\u4e49\u4e14\u53ef\u8861\u91cf\u7684\u6f5c\u5728\u7a7a\u95f4\u8ddd\u79bb\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5c06\u6f5c\u5728\u7a7a\u95f4\u8ddd\u79bb\u4e0e\u89c2\u6d4b\u77e9\u9635\u4e2d\u7684\u8ddd\u79bb\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u8ddd\u79bb\u4f30\u8ba1\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89c2\u6d4b\u77e9\u9635\u7684\u884c\u548c\u5217\u8ddd\u79bb\u6765\u7cfb\u7edf\u5730\u8fd1\u4f3c\u4f30\u8ba1\u6f5c\u5728\u7a7a\u95f4\u4e2d\u7684\u8ddd\u79bb\u7684\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u7ecf\u9a8c\u65b9\u5dee\u4f30\u8ba1\u5668\u7684\u6821\u6b63\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u4f30\u8ba1\u7cbe\u5ea6\u3002\u57fa\u4e8e\u6b64\uff0c\u5f00\u53d1\u4e86\u5f84\u5411\u90bb\u57df\u4f30\u8ba1\u5668\uff08RNE\uff09\uff0c\u8be5\u4f30\u8ba1\u5668\u901a\u8fc7\u5305\u542b\u5b8c\u5168\u91cd\u53e0\u548c\u90e8\u5206\u91cd\u53e0\u7684\u7528\u6237-\u9879\u76ee\u5bf9\u6765\u6784\u5efa\u90bb\u57df\uff0c\u5e76\u5229\u7528\u5c40\u90e8\u6838\u56de\u5f52\u8fdb\u884c\u90bb\u57df\u5e73\u6ed1\u4ee5\u63d0\u9ad8\u586b\u5145\u51c6\u786e\u6027\u3002", "result": "RNE\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u534f\u540c\u8fc7\u6ee4\u548c\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u7f13\u89e3\u4e86\u201c\u51b7\u542f\u52a8\u201d\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8ddd\u79bb\u4f30\u8ba1\u65b9\u6cd5\uff0c\u540d\u4e3a\u5f84\u5411\u90bb\u57df\u4f30\u8ba1\u5668\uff08RNE\uff09\uff0c\u5e76\u5728\u4e00\u7cfb\u5217\u6a21\u62df\u548c\u771f\u5b9e\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\uff0c\u540c\u65f6\u8be5\u65b9\u6cd5\u8fd8\u6709\u52a9\u4e8e\u7f13\u89e3\u201c\u51b7\u542f\u52a8\u201d\u95ee\u9898\u3002"}}
{"id": "2507.10115", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10115", "abs": "https://arxiv.org/abs/2507.10115", "authors": ["Hamidreza Hashempoor"], "title": "Glance-MCMT: A General MCMT Framework with Glance Initialization and Progressive Association", "comment": null, "summary": "We propose a multi-camera multi-target (MCMT) tracking framework that ensures\nconsistent global identity assignment across views using trajectory and\nappearance cues. The pipeline starts with BoT-SORT-based single-camera\ntracking, followed by an initial glance phase to initialize global IDs via\ntrajectory-feature matching. In later frames, new tracklets are matched to\nexisting global identities through a prioritized global matching strategy. New\nglobal IDs are only introduced when no sufficiently similar trajectory or\nfeature match is found. 3D positions are estimated using depth maps and\ncalibration for spatial validation.", "AI": {"tldr": "A multi-camera tracking method that uses trajectory and appearance to consistently assign IDs across cameras, starting with single-camera tracking and then matching across views with a priority system.", "motivation": "To ensure consistent global identity assignment across multiple camera views in multi-target tracking.", "method": "A multi-camera multi-target tracking framework is proposed. It begins with BoT-SORT for single-camera tracking, then uses a global ID initialization phase based on trajectory-feature matching. Subsequent frames are matched to global identities using a prioritized matching strategy, introducing new IDs only when no suitable match is found. 3D positions are estimated for spatial validation.", "result": "The framework successfully assigns consistent global identities across views by effectively matching tracklets to existing global identities and minimizing the introduction of new IDs.", "conclusion": "The proposed framework ensures consistent global identity assignment across multiple cameras by prioritizing existing global identities and introducing new ones only when necessary, validated by 3D spatial positioning."}}
{"id": "2507.09958", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09958", "abs": "https://arxiv.org/abs/2507.09958", "authors": ["Zhenyuan Chen"], "title": "Rethinking Inductive Bias in Geographically Neural Network Weighted Regression", "comment": null, "summary": "Inductive bias is a key factor in spatial regression models, determining how\nwell a model can learn from limited data and capture spatial patterns. This\nwork revisits the inductive biases in Geographically Neural Network Weighted\nRegression (GNNWR) and identifies limitations in current approaches for\nmodeling spatial non-stationarity. While GNNWR extends traditional\nGeographically Weighted Regression by using neural networks to learn spatial\nweighting functions, existing implementations are often restricted by fixed\ndistance-based schemes and limited inductive bias. We propose to generalize\nGNNWR by incorporating concepts from convolutional neural networks, recurrent\nneural networks, and transformers, introducing local receptive fields,\nsequential context, and self-attention into spatial regression. Through\nextensive benchmarking on synthetic spatial datasets with varying\nheterogeneity, noise, and sample sizes, we show that GNNWR outperforms classic\nmethods in capturing nonlinear and complex spatial relationships. Our results\nalso reveal that model performance depends strongly on data characteristics,\nwith local models excelling in highly heterogeneous or small-sample scenarios,\nand global models performing better with larger, more homogeneous data. These\nfindings highlight the importance of inductive bias in spatial modeling and\nsuggest future directions, including learnable spatial weighting functions,\nhybrid neural architectures, and improved interpretability for models handling\nnon-stationary spatial data.", "AI": {"tldr": "\u672c\u7814\u7a76\u6269\u5c55\u4e86\u5730\u7406\u52a0\u6743\u56de\u5f52\uff08GNNWR\uff09\uff0c\u5f15\u5165\u4e86\u6df1\u5ea6\u5b66\u4e60\u673a\u5236\u4ee5\u6539\u8fdb\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\u5efa\u6a21\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u7684\u4f18\u8d8a\u6027\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u5f52\u7eb3\u504f\u501a\u4e0e\u6570\u636e\u7279\u6027\u5339\u914d\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7814\u7a76\u5f52\u7eb3\u504f\u501a\u5728\u7a7a\u95f4\u56de\u5f52\u6a21\u578b\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u8bc6\u522b\u5f53\u524dGNNWR\u65b9\u6cd5\u5728\u5efa\u6a21\u7a7a\u95f4\u975e\u5e73\u7a33\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5373\u56fa\u5b9a\u8ddd\u79bb\u65b9\u6848\u548c\u6709\u9650\u7684\u5f52\u7eb3\u504f\u501a\u3002", "method": "\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u7684\u6982\u5ff5\uff0c\u5c06\u5c40\u90e8\u611f\u53d7\u91ce\u3001\u5e8f\u5217\u4e0a\u4e0b\u6587\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u7a7a\u95f4\u56de\u5f52\uff0c\u5e76\u5bf9GNNWR\u8fdb\u884c\u4e86\u6cdb\u5316\u3002", "result": "\u901a\u8fc7\u5728\u5177\u6709\u4e0d\u540c\u5f02\u8d28\u6027\u3001\u566a\u58f0\u548c\u6837\u672c\u91cf\u7684\u5408\u6210\u7a7a\u95f4\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684GNNWR\u5728\u6355\u6349\u975e\u7ebf\u6027\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002\u6a21\u578b\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6570\u636e\u7279\u6027\uff0c\u5c40\u90e8\u6a21\u578b\u5728\u9ad8\u5ea6\u5f02\u8d28\u6216\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5168\u5c40\u6a21\u578b\u5728\u66f4\u5927\u3001\u66f4\u540c\u8d28\u7684\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u548cTransformer\u7684\u6982\u5ff5\uff0c\u5c06\u5c40\u90e8\u611f\u53d7\u91ce\u3001\u5e8f\u5217\u4e0a\u4e0b\u6587\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\u5f15\u5165\u7a7a\u95f4\u56de\u5f52\uff0c\u5e76\u5bf9GNNWR\u8fdb\u884c\u4e86\u6cdb\u5316\uff0c\u5728\u6355\u6349\u975e\u7ebf\u6027\u548c\u590d\u6742\u7a7a\u95f4\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002\u6a21\u578b\u6027\u80fd\u9ad8\u5ea6\u4f9d\u8d56\u4e8e\u6570\u636e\u7279\u6027\uff0c\u5c40\u90e8\u6a21\u578b\u5728\u9ad8\u5ea6\u5f02\u8d28\u6216\u5c0f\u6837\u672c\u60c5\u51b5\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5168\u5c40\u6a21\u578b\u5728\u66f4\u5927\u3001\u66f4\u540c\u8d28\u7684\u6570\u636e\u4e0a\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2507.10118", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10118", "abs": "https://arxiv.org/abs/2507.10118", "authors": ["Ivan Martinovi\u0107", "Josip \u0160ari\u0107", "Marin Or\u0161i\u0107", "Matej Kristan", "Sini\u0161a \u0160egvi\u0107"], "title": "DEARLi: Decoupled Enhancement of Recognition and Localization for Semi-supervised Panoptic Segmentation", "comment": "ICCV 2025 Findings Workshop", "summary": "Pixel-level annotation is expensive and time-consuming. Semi-supervised\nsegmentation methods address this challenge by learning models on few labeled\nimages alongside a large corpus of unlabeled images. Although foundation models\ncould further account for label scarcity, effective mechanisms for their\nexploitation remain underexplored. We address this by devising a novel\nsemi-supervised panoptic approach fueled by two dedicated foundation models. We\nenhance recognition by complementing unsupervised mask-transformer consistency\nwith zero-shot classification of CLIP features. We enhance localization by\nclass-agnostic decoder warm-up with respect to SAM pseudo-labels. The resulting\ndecoupled enhancement of recognition and localization (DEARLi) particularly\nexcels in the most challenging semi-supervised scenarios with large taxonomies\nand limited labeled data. Moreover, DEARLi outperforms the state of the art in\nsemi-supervised semantic segmentation by a large margin while requiring 8x less\nGPU memory, in spite of being trained only for the panoptic objective. We\nobserve 29.9 PQ and 38.9 mIoU on ADE20K with only 158 labeled images. The\nsource code is available at https://github.com/helen1c/DEARLi.", "AI": {"tldr": "\u901a\u8fc7\u7ed3\u5408CLIP\u548cSAM\u9884\u8bad\u7ec3\u6a21\u578b\uff0cDEARLi\u5728\u534a\u76d1\u7763\u5168\u666f\u5206\u5272\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u8d44\u6e90\u7684\u9700\u6c42\u3002", "motivation": "\u50cf\u7d20\u7ea7\u6807\u6ce8\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u800c\u534a\u76d1\u7763\u5206\u5272\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u5c11\u91cf\u6807\u8bb0\u56fe\u50cf\u548c\u5927\u91cf\u672a\u6807\u8bb0\u56fe\u50cf\u6765\u5b66\u4e60\u6a21\u578b\u3002\u7136\u800c\uff0c\u5982\u4f55\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u89e3\u51b3\u6807\u7b7e\u7a00\u758f\u6027\u95ee\u9898\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u8be5\u65b9\u6cd5\u5229\u7528\u4e24\u4e2a\u4e13\u95e8\u7684\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u65e0\u76d1\u7763\u63a9\u7801Transformer\u4e00\u81f4\u6027\u548cCLIP\u7684\u96f6\u6837\u672c\u5206\u7c7b\u6765\u589e\u5f3a\u8bc6\u522b\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4e0eSAM\u4f2a\u6807\u7b7e\u7684\u7c7b\u522b\u65e0\u5173\u89e3\u7801\u5668\u9884\u70ed\u6765\u589e\u5f3a\u5b9a\u4f4d\u80fd\u529b\u3002", "result": "DEARLi\u5728\u534a\u76d1\u7763\u5168\u666f\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5c24\u5176\u662f\u5728\u6807\u7b7e\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cDEARLi\u5728\u534a\u76d1\u7763\u8bed\u4e49\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u5927\u7684\u4f18\u52bf\uff0c\u5e76\u4e14GPU\u5185\u5b58\u5360\u7528\u51cf\u5c11\u4e868\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684DEARLi\u65b9\u6cd5\u5728\u5177\u6709\u5927\u578b\u7c7b\u522b\u548c\u6709\u9650\u6807\u7b7e\u6570\u636e\u7684\u6700\u5177\u6311\u6218\u6027\u7684\u534a\u76d1\u7763\u573a\u666f\u4e2d\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u5728ADE20K\u6570\u636e\u96c6\u4e0a\u4ec5\u7528158\u4e2a\u6807\u8bb0\u56fe\u50cf\u5373\u53ef\u8fbe\u523029.9 PQ\u548c38.9 mIoU\u7684\u6027\u80fd\u3002"}}
{"id": "2507.09961", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09961", "abs": "https://arxiv.org/abs/2507.09961", "authors": ["Lihua Zhou", "Mao Ye", "Nianxin Li", "Shuaifeng Li", "Jinlin Wu", "Xiatian Zhu", "Lei Deng", "Hongbin Liu", "Jiebo Luo", "Zhen Lei"], "title": "Text-Driven Causal Representation Learning for Source-Free Domain Generalization", "comment": "Under Review", "summary": "Deep learning often struggles when training and test data distributions\ndiffer. Traditional domain generalization (DG) tackles this by including data\nfrom multiple source domains, which is impractical due to expensive data\ncollection and annotation. Recent vision-language models like CLIP enable\nsource-free domain generalization (SFDG) by using text prompts to simulate\nvisual representations, reducing data demands. However, existing SFDG methods\nstruggle with domain-specific confounders, limiting their generalization\ncapabilities. To address this issue, we propose TDCRL\n(\\textbf{T}ext-\\textbf{D}riven \\textbf{C}ausal \\textbf{R}epresentation\n\\textbf{L}earning), the first method to integrate causal inference into the\nSFDG setting. TDCRL operates in two steps: first, it employs data augmentation\nto generate style word vectors, combining them with class information to\ngenerate text embeddings to simulate visual representations; second, it trains\na causal intervention network with a confounder dictionary to extract\ndomain-invariant features. Grounded in causal learning, our approach offers a\nclear and effective mechanism to achieve robust, domain-invariant features,\nensuring robust generalization. Extensive experiments on PACS, VLCS,\nOfficeHome, and DomainNet show state-of-the-art performance, proving TDCRL\neffectiveness in SFDG.", "AI": {"tldr": "TDCRL \u5c06\u56e0\u679c\u63a8\u7406\u5e94\u7528\u4e8e\u6e90\u57df\u65e0\u5173\u7684\u57df\u6cdb\u5316\uff08SFDG\uff09\uff0c\u901a\u8fc7\u6587\u672c\u5d4c\u5165\u548c\u56e0\u679c\u5e72\u9884\u7f51\u7edc\u6765\u5b66\u4e60\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6e90\u57df\u65e0\u5173\u57df\u6cdb\u5316\uff08SFDG\uff09\u65b9\u6cd5\u5728\u5904\u7406\u7279\u5b9a\u57df\u7684\u6df7\u6dc6\u56e0\u7d20\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8fd9\u5f71\u54cd\u4e86\u5b83\u4eec\u7684\u6cdb\u5316\u80fd\u529b\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0cTDCRL \u88ab\u63d0\u4e86\u51fa\u6765\u3002", "method": "TDCRL\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u6b65\u9aa4\uff1a1. \u4f7f\u7528\u6570\u636e\u589e\u5f3a\u751f\u6210\u98ce\u683c\u8bcd\u5411\u91cf\uff0c\u5e76\u5c06\u5176\u4e0e\u7c7b\u522b\u4fe1\u606f\u7ed3\u5408\u751f\u6210\u6587\u672c\u5d4c\u5165\uff0c\u4ece\u800c\u6a21\u62df\u89c6\u89c9\u8868\u5f81\u30022. \u8bad\u7ec3\u4e00\u4e2a\u56e0\u679c\u5e72\u9884\u7f51\u7edc\uff0c\u5e76\u4f7f\u7528\u6df7\u6dc6\u8bcd\u5178\u6765\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\u3002", "result": "TDCRL \u5728 PACS\u3001VLCS\u3001OfficeHome \u548c DomainNet \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u5728 SFDG \u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TDCRL \u662f\u4e00\u79cd\u5f00\u521b\u6027\u7684\u65b9\u6cd5\uff0c\u5b83\u5c06\u56e0\u679c\u63a8\u7406\u96c6\u6210\u5230\u6e90\u57df\u65e0\u5173\u7684\u57df\u6cdb\u5316\uff08SFDG\uff09\u8bbe\u7f6e\u4e2d\uff0c\u901a\u8fc7\u6587\u672c\u63d0\u793a\u6a21\u62df\u89c6\u89c9\u8868\u5f81\uff0c\u5e76\u5229\u7528\u56e0\u679c\u5e72\u9884\u7f51\u7edc\u548c\u6df7\u6dc6\u8bcd\u5178\u63d0\u53d6\u57df\u4e0d\u53d8\u7279\u5f81\uff0c\u5728 PACS\u3001VLCS\u3001OfficeHome \u548c DomainNet \u7b49\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10127", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10127", "abs": "https://arxiv.org/abs/2507.10127", "authors": ["Md Abulkalam Azad", "John Nyberg", "H\u00e5vard Dalen", "Bj\u00f8rnar Grenne", "Lasse Lovstakken", "Andreas \u00d8stvik"], "title": "Taming Modern Point Tracking for Speckle Tracking Echocardiography via Impartial Motion", "comment": "Accepted to CVAMD workshop at ICCV 2025", "summary": "Accurate motion estimation for tracking deformable tissues in\nechocardiography is essential for precise cardiac function measurements. While\ntraditional methods like block matching or optical flow struggle with intricate\ncardiac motion, modern point tracking approaches remain largely underexplored\nin this domain. This work investigates the potential of state-of-the-art (SOTA)\npoint tracking methods for ultrasound, with a focus on echocardiography.\nAlthough these novel approaches demonstrate strong performance in general\nvideos, their effectiveness and generalizability in echocardiography remain\nlimited. By analyzing cardiac motion throughout the heart cycle in real B-mode\nultrasound videos, we identify that a directional motion bias across different\nviews is affecting the existing training strategies. To mitigate this, we\nrefine the training procedure and incorporate a set of tailored augmentations\nto reduce the bias and enhance tracking robustness and generalization through\nimpartial cardiac motion. We also propose a lightweight network leveraging\nmulti-scale cost volumes from spatial context alone to challenge the advanced\nspatiotemporal point tracking models. Experiments demonstrate that fine-tuning\nwith our strategies significantly improves models' performances over their\nbaselines, even for out-of-distribution (OOD) cases. For instance, EchoTracker\nboosts overall position accuracy by 60.7% and reduces median trajectory error\nby 61.5% across heart cycle phases. Interestingly, several point tracking\nmodels fail to outperform our proposed simple model in terms of tracking\naccuracy and generalization, reflecting their limitations when applied to\nechocardiography. Nevertheless, clinical evaluation reveals that these methods\nimprove GLS measurements, aligning more closely with expert-validated,\nsemi-automated tools and thus demonstrating better reproducibility in\nreal-world applications.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.09968", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.09968", "abs": "https://arxiv.org/abs/2507.09968", "authors": ["Xiangyu Sun", "Amin Yousefpour", "Shirin Hosseinmardi", "Ramin Bostanabad"], "title": "Compliance Minimization via Physics-Informed Gaussian Processes", "comment": null, "summary": "Machine learning (ML) techniques have recently gained significant attention\nfor solving compliance minimization (CM) problems. However, these methods\ntypically provide poor feature boundaries, are very expensive, and lack a\nsystematic mechanism to control the design complexity. Herein, we address these\nlimitations by proposing a mesh-free and simultaneous framework based on\nphysics-informed Gaussian processes (GPs). In our approach, we parameterize the\ndesign and state variables with GP priors which have independent kernels but\nshare a multi-output neural network (NN) as their mean function. The\narchitecture of this NN is based on Parametric Grid Convolutional Attention\nNetworks (PGCANs) which not only mitigate spectral bias issues, but also\nprovide an interpretable mechanism to control design complexity. We estimate\nall the parameters of our GP-based representations by simultaneously minimizing\nthe compliance, total potential energy, and residual of volume fraction\nconstraint. Importantly, our loss function exclude all data-based residuals as\nGPs automatically satisfy them. We also develop computational schemes based on\ncurriculum training and numerical integration to increase the efficiency and\nrobustness of our approach which is shown to (1) produce super-resolution\ntopologies with fast convergence, (2) achieve smaller compliance and less gray\narea fraction compared to traditional numerical methods, (3) provide control\nover fine-scale features, and (4) outperform competing ML-based methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u9ad8\u65af\u8fc7\u7a0b\u7684\u65e0\u7f51\u683c\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u987a\u4ece\u6027\u6700\u5c0f\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u5316\u8bbe\u8ba1\u548c\u72b6\u6001\u53d8\u91cf\uff0c\u5e76\u5229\u7528\u5177\u6709\u7279\u5b9a\u67b6\u6784\u7684\u795e\u7ecf\u7f51\u7edc\u6765\u63a7\u5236\u8bbe\u8ba1\u590d\u6742\u5ea6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u8d28\u91cf\u62d3\u6251\u3001\u63d0\u9ad8\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u89e3\u51b3\u987a\u4ece\u6027\u6700\u5c0f\u5316\uff08CM\uff09\u95ee\u9898\u65f6\u5b58\u5728\u7684\u7279\u5f81\u8fb9\u754c\u5dee\u3001\u6210\u672c\u9ad8\u4ee5\u53ca\u7f3a\u4e4f\u63a7\u5236\u8bbe\u8ba1\u590d\u6742\u5ea6\u7684\u7cfb\u7edf\u673a\u5236\u7b49\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u9ad8\u65af\u8fc7\u7a0b\uff08GPs\uff09\u7684\u65e0\u7f51\u683c\u3001\u540c\u6b65\u6846\u67b6\u3002\u901a\u8fc7GP\u5148\u9a8c\u53c2\u6570\u5316\u8bbe\u8ba1\u548c\u72b6\u6001\u53d8\u91cf\uff0c\u5e76\u5171\u4eab\u4e00\u4e2a\u591a\u8f93\u51fa\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u4f5c\u4e3a\u5747\u503c\u51fd\u6570\u3002\u8be5NN\u57fa\u4e8e\u53c2\u6570\u5316\u7f51\u683c\u5377\u79ef\u6ce8\u610f\u529b\u7f51\u7edc\uff08PGCANs\uff09\uff0c\u4ee5\u7f13\u89e3\u9891\u8c31\u504f\u5dee\u95ee\u9898\u5e76\u63a7\u5236\u8bbe\u8ba1\u590d\u6742\u5ea6\u3002\u901a\u8fc7\u540c\u6b65\u6700\u5c0f\u5316\u987a\u4ece\u6027\u3001\u603b\u52bf\u80fd\u548c\u4f53\u79ef\u5206\u6570\u7ea6\u675f\u6b8b\u5dee\u6765\u4f30\u8ba1GP\u8868\u793a\u7684\u6240\u6709\u53c2\u6570\u3002\u8be5\u635f\u5931\u51fd\u6570\u6392\u9664\u4e86\u6240\u6709\u57fa\u4e8e\u6570\u636e\u7684\u6b8b\u5dee\uff0c\u56e0\u4e3aGPs\u81ea\u52a8\u6ee1\u8db3\u5b83\u4eec\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u57fa\u4e8e\u8bfe\u7a0b\u5b66\u4e60\u548c\u6570\u503c\u79ef\u5206\u7684\u8ba1\u7b97\u65b9\u6848\u6765\u63d0\u9ad8\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "result": "\uff081\uff09\u751f\u6210\u8d85\u5206\u8fa8\u7387\u62d3\u6251\uff0c\u6536\u655b\u901f\u5ea6\u5feb\uff1b\uff082\uff09\u5b9e\u73b0\u6bd4\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u66f4\u4f4e\u7684\u987a\u4ece\u6027\u548c\u66f4\u5c11\u7684\u7070\u5ea6\u533a\u57df\u5206\u6570\uff1b\uff083\uff09\u63d0\u4f9b\u5bf9\u7ec6\u5fae\u5c3a\u5ea6\u7279\u5f81\u7684\u63a7\u5236\uff1b\uff084\uff09\u4f18\u4e8e\u7ade\u4e89\u6027\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8d85\u5206\u8fa8\u7387\u62d3\u6251\u751f\u6210\u65b9\u9762\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u5728\u5bf9\u6bd4\u4f20\u7edf\u6570\u503c\u65b9\u6cd5\u65f6\uff0c\u5b9e\u73b0\u4e86\u66f4\u4f4e\u7684\u987a\u4ece\u6027\u548c\u66f4\u5c11\u7684\u7070\u5ea6\u533a\u57df\u5206\u6570\uff0c\u5e76\u80fd\u63a7\u5236\u7ec6\u5fae\u5c3a\u5ea6\u7684\u7279\u5f81\uff0c\u540c\u65f6\u4f18\u4e8e\u7ade\u4e89\u6027\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002"}}
{"id": "2507.10143", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10143", "abs": "https://arxiv.org/abs/2507.10143", "authors": ["David Calhas", "Arlindo L. Oliveira"], "title": "Deep Recurrence for Dynamical Segmentation Models", "comment": "12 pages", "summary": "While biological vision systems rely heavily on feedback connections to\niteratively refine perception, most artificial neural networks remain purely\nfeedforward, processing input in a single static pass. In this work, we propose\na predictive coding inspired feedback mechanism that introduces a recurrent\nloop from output to input, allowing the model to refine its internal state over\ntime. We implement this mechanism within a standard U-Net architecture and\nintroduce two biologically motivated operations, softmax projection and\nexponential decay, to ensure stability of the feedback loop. Through controlled\nexperiments on a synthetic segmentation task, we show that the feedback model\nsignificantly outperforms its feedforward counterpart in noisy conditions and\ngeneralizes more effectively with limited supervision. Notably, feedback\nachieves above random performance with just two training examples, while the\nfeedforward model requires at least four. Our findings demonstrate that\nfeedback enhances robustness and data efficiency, and offer a path toward more\nadaptive and biologically inspired neural architectures. Code is available at:\ngithub.com/DCalhas/feedback_segmentation.", "AI": {"tldr": "Biological vision uses feedback for refinement; this paper adds a feedback loop to a U-Net for image segmentation, improving performance in noise and with less data. It works well even with just two examples.", "motivation": "To address the limitations of purely feedforward artificial neural networks by introducing a feedback mechanism that allows iterative refinement of perception, inspired by biological vision systems.", "method": "Implemented a predictive coding inspired feedback mechanism within a U-Net architecture, incorporating softmax projection and exponential decay for loop stability.", "result": "The feedback model significantly outperformed its feedforward counterpart in noisy conditions and generalized more effectively with limited supervision, achieving above random performance with only two training examples compared to the feedforward model's requirement of at least four.", "conclusion": "Feedback enhances robustness and data efficiency in neural networks, offering a path toward more adaptive and biologically inspired architectures."}}
{"id": "2507.10171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10171", "abs": "https://arxiv.org/abs/2507.10171", "authors": ["Youngmin Kim", "Giyeong Oh", "Kwangsoo Youm", "Youngjae Yu"], "title": "SlumpGuard: An AI-Powered Real-Time System for Automated Concrete Slump Prediction via Video Analysis", "comment": null, "summary": "Concrete workability is essential for construction quality, with the slump\ntest being the most common on-site method for its assessment. However,\ntraditional slump testing is manual, time-consuming, and prone to\ninconsistency, limiting its applicability for real-time monitoring. To address\nthese challenges, we propose SlumpGuard, an AI-powered, video-based system that\nautomatically analyzes concrete flow from the truck chute to assess workability\nin real time. Our system enables full-batch inspection without manual\nintervention, improving both the accuracy and efficiency of quality control. We\npresent the system design, a the construction of a dedicated dataset, and\nempirical results from real-world deployment, demonstrating the effectiveness\nof SlumpGuard as a practical solution for modern concrete quality assurance.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3a SlumpGuard \u7684 AI \u9a71\u52a8\u7684\u89c6\u9891\u7cfb\u7edf\uff0c\u53ef\u4ee5\u5b9e\u65f6\u81ea\u52a8\u6d4b\u91cf\u6df7\u51dd\u571f\u7684\u548c\u6613\u6027\uff0c\u53d6\u4ee3\u4e86\u624b\u52a8\u4e14\u8017\u65f6\u7684 \u0aae\u0abe\u0aa4\u0acd\u0ab0 \u09aa\u09b0\u09c0\u0995\u09cd\u09b7\u09be (slump test)\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf \u09b6\u09c1\u09a7\u09c1\u09ae\u09be\u09a4\u09cd\u09b0 \u09aa\u09b0\u09c0\u0995\u09cd\u09b7\u09be (slump test) \u624b\u52a8\u3001\u8017\u65f6\u4e14\u4e0d\u4e00\u81f4\u7684\u7f3a\u70b9\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6df7\u51dd\u571f\u548c\u6613\u6027\u7684\u5b9e\u65f6\u76d1\u63a7\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SlumpGuard \u7684\u3001\u57fa\u4e8e AI \u548c\u89c6\u9891\u7684\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u81ea\u52a8\u5206\u6790\u5361\u8f66\u5378\u6599\u69fd\u4e2d\u7684\u6df7\u51dd\u571f\u6d41\u52a8\u6027\uff0c\u4ece\u800c\u5b9e\u65f6\u8bc4\u4f30\u5176\u548c\u6613\u6027\u3002", "result": "\u901a\u8fc7\u7cfb\u7edf\u8bbe\u8ba1\u3001\u4e13\u7528\u6570\u636e\u96c6\u6784\u5efa\u4ee5\u53ca\u5b9e\u9645\u90e8\u7f72\u7684\u5b9e\u8bc1\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86 SlumpGuard \u5728\u73b0\u4ee3\u6df7\u51dd\u571f\u8d28\u91cf\u4fdd\u8bc1\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684 SlumpGuard \u7cfb\u7edf\u4f5c\u4e3a\u4e00\u79cd\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u6df7\u51dd\u571f\u8d28\u91cf\u4fdd\u8bc1\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\u3002"}}
{"id": "2507.10014", "categories": ["cs.LG", "92D30, 62M10, 68T07", "G.3; I.6.3; I.2.6; I.5.1"], "pdf": "https://arxiv.org/pdf/2507.10014", "abs": "https://arxiv.org/abs/2507.10014", "authors": ["Ali Sarabi", "Arash Sarabi", "Hao Yan", "Beckett Sterner", "Petar Jevti\u0107"], "title": "Forecasting Coccidioidomycosis (Valley Fever) in Arizona: A Graph Neural Network Approach", "comment": null, "summary": "Coccidioidomycosis, commonly known as Valley Fever, remains a significant\npublic health concern in endemic regions of the southwestern United States.\nThis study develops the first graph neural network (GNN) model for forecasting\nValley Fever incidence in Arizona. The model integrates surveillance case data\nwith environmental predictors using graph structures, including soil\nconditions, atmospheric variables, agricultural indicators, and air quality\nmetrics. Our approach explores correlation-based relationships among variables\ninfluencing disease transmission. The model captures critical delays in disease\nprogression through lagged effects, enhancing its capacity to reflect complex\ntemporal dependencies in disease ecology. Results demonstrate that the GNN\narchitecture effectively models Valley Fever trends and provides insights into\nkey environmental drivers of disease incidence. These findings can inform early\nwarning systems and guide resource allocation for disease prevention efforts in\nhigh-risk areas.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7ed3\u5408\u73af\u5883\u6570\u636e\u9884\u6d4b\u5c71\u8c37\u70ed\u7684\u53d1\u75c5\u7387\u3002", "motivation": "\u89e3\u51b3\u3084\u3059\u304f", "method": "\u901a\u8fc7\u6574\u5408\u76d1\u6d4b\u75c5\u4f8b\u6570\u636e\u548c\u73af\u5883\u9884\u6d4b\u56e0\u5b50\uff08\u5305\u62ec\u571f\u58e4\u6761\u4ef6\u3001\u5927\u6c14\u53d8\u91cf\u3001\u519c\u4e1a\u6307\u6807\u548c\u7a7a\u6c14\u8d28\u91cf\u6307\u6807\uff09\uff0c\u5e76\u5229\u7528\u56fe\u7ed3\u6784\u63a2\u7d22\u5f71\u54cd\u75be\u75c5\u4f20\u64ad\u7684\u53d8\u91cf\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u6765\u5f00\u53d1\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6a21\u578b\u3002\u8be5\u6a21\u578b\u8fd8\u8003\u8651\u4e86\u75be\u75c5\u8fdb\u5c55\u4e2d\u7684\u5173\u952e\u5ef6\u8fdf\u548c\u590d\u6742\u7684 \u0632\u0645\u0646\u064a\u0629 \u4f9d\u8d56\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u8be5GNN\u6a21\u578b\u80fd\u591f\u6709\u6548\u6a21\u62df\u5c71\u8c37\u70ed\u7684\u8d8b\u52bf\uff0c\u5e76\u63ed\u793a\u4e86\u5bfc\u81f4\u75be\u75c5\u53d1\u75c5\u7387\u7684\u5173\u952e\u73af\u5883\u9a71\u52a8\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u9884\u6d4b\u0449\u0438\u0445\u0441\u044f\u201c\u5c71\u8c37\u70ed\u201d\u53d1\u75c5\u7387\u5f00\u53d1\u4e86\u9996\u4e2a\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6a21\u578b\uff0c\u5e76\u7ed3\u5408\u4e86\u76d1\u6d4b\u75c5\u4f8b\u6570\u636e\u548c\u73af\u5883\u9884\u6d4b\u56e0\u5b50\uff0c\u63ed\u793a\u4e86\u5f71\u54cd\u75be\u75c5\u4f20\u64ad\u7684\u5173\u952e\u73af\u5883\u53d8\u91cf\u3002"}}
{"id": "2507.10195", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10195", "abs": "https://arxiv.org/abs/2507.10195", "authors": ["Shuyu Yang", "Yaxiong Wang", "Yongrui Li", "Li Zhu", "Zhedong Zheng"], "title": "Minimizing the Pretraining Gap: Domain-aligned Text-Based Person Retrieval", "comment": null, "summary": "In this work, we focus on text-based person retrieval, which aims to identify\nindividuals based on textual descriptions. Given the significant privacy issues\nand the high cost associated with manual annotation, synthetic data has become\na popular choice for pretraining models, leading to notable advancements.\nHowever, the considerable domain gap between synthetic pretraining datasets and\nreal-world target datasets, characterized by differences in lighting, color,\nand viewpoint, remains a critical obstacle that hinders the effectiveness of\nthe pretrain-finetune paradigm. To bridge this gap, we introduce a unified\ntext-based person retrieval pipeline considering domain adaptation at both\nimage and region levels. In particular, it contains two primary components,\ni.e., Domain-aware Diffusion (DaD) for image-level adaptation and\nMulti-granularity Relation Alignment (MRA) for region-level adaptation. As the\nname implies, Domain-aware Diffusion is to migrate the distribution of images\nfrom the pretraining dataset domain to the target real-world dataset domain,\ne.g., CUHK-PEDES. Subsequently, MRA performs a meticulous region-level\nalignment by establishing correspondences between visual regions and their\ndescriptive sentences, thereby addressing disparities at a finer granularity.\nExtensive experiments show that our dual-level adaptation method has achieved\nstate-of-the-art results on the CUHK-PEDES, ICFG-PEDES, and RSTPReid datasets,\noutperforming existing methodologies. The dataset, model, and code are\navailable at https://github.com/Shuyu-XJTU/MRA.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5408\u6210\u9884\u8bad\u7ec3\u6570\u636e\u548c\u771f\u5b9e\u4e16\u754c\u76ee\u6807\u6570\u636e\u4e4b\u95f4\u7684\u57df\u9699\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u56fe\u50cf\u7ea7\u57df\u9002\u5e94\uff08DaD\uff09\u548c\u533a\u57df\u7ea7\u5173\u7cfb\u5bf9\u9f50\uff08MRA\uff09\u7684\u7edf\u4e00\u6587\u672c\u5230\u56fe\u50cf\u68c0\u7d22\u6d41\u7a0b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5408\u6210\u6570\u636e\u5728\u57fa\u4e8e\u6587\u672c\u7684person\u68c0\u7d22\u4e2d\u88ab\u5e7f\u6cdb\u7528\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u4f46\u5408\u6210\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u4e0e\u771f\u5b9e\u4e16\u754c\u76ee\u6807\u6570\u636e\u96c6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u57df\u9699\uff0c\u963b\u788d\u4e86\u9884\u8bad\u7ec3-\u5fae\u8c03\u8303\u4f8b\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u57fa\u4e8e\u6587\u672c\u7684 person \u68c0\u7d22\u6d41\u7a0b\uff0c\u5305\u62ec\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a\u7528\u4e8e\u56fe\u50cf\u7ea7\u81ea\u9002\u5e94\u7684\u57df\u611f\u77e5\u6269\u6563\uff08DaD\uff09\u548c\u7528\u4e8e\u533a\u57df\u7ea7\u81ea\u9002\u5e94\u7684\u591a\u7c92\u5ea6\u5173\u7cfb\u5bf9\u9f50\uff08MRA\uff09\u3002DaD\u7528\u4e8e\u8fc1\u79fb\u9884\u8bad\u7ec3\u6570\u636e\u96c6\u57df\u7684\u56fe\u50cf\u5206\u5e03\u5230\u76ee\u6807\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u57df\uff0cMRA\u901a\u8fc7\u5efa\u7acb\u89c6\u89c9\u533a\u57df\u4e0e\u5176\u63cf\u8ff0\u6027\u53e5\u5b50\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u6765\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u533a\u57df\u7ea7\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u53cc\u5c42\u81ea\u9002\u5e94\u65b9\u6cd5\u5728CUHK-PEDES\u3001ICFG-PEDES\u548cRSTPReid\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728CUHK-PEDES\u3001ICFG-PEDES\u548cRSTPReid\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2507.10039", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2507.10039", "abs": "https://arxiv.org/abs/2507.10039", "authors": ["Steven Palayew", "Bo Wang", "Gary Bader"], "title": "Towards Applying Large Language Models to Complement Single-Cell Foundation Models", "comment": null, "summary": "Single-cell foundation models such as scGPT represent a significant\nadvancement in single-cell omics, with an ability to achieve state-of-the-art\nperformance on various downstream biological tasks. However, these models are\ninherently limited in that a vast amount of information in biology exists as\ntext, which they are unable to leverage. There have therefore been several\nrecent works that propose the use of LLMs as an alternative to single-cell\nfoundation models, achieving competitive results. However, there is little\nunderstanding of what factors drive this performance, along with a strong focus\non using LLMs as an alternative, rather than complementary approach to\nsingle-cell foundation models. In this study, we therefore investigate what\nbiological insights contribute toward the performance of LLMs when applied to\nsingle-cell data, and introduce scMPT; a model which leverages synergies\nbetween scGPT, and single-cell representations from LLMs that capture these\ninsights. scMPT demonstrates stronger, more consistent performance than either\nof its component models, which frequently have large performance gaps between\neach other across datasets. We also experiment with alternate fusion methods,\ndemonstrating the potential of combining specialized reasoning models with\nscGPT to improve performance. This study ultimately showcases the potential for\nLLMs to complement single-cell foundation models and drive improvements in\nsingle-cell analysis.", "AI": {"tldr": "scMPT\u7ed3\u5408\u4e86scGPT\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u4f18\u52bf\uff0c\u5728\u5355\u7ec6\u80de\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u5355\u72ec\u7684\u6a21\u578b\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5355\u7ec6\u80de\u7814\u7a76\u4e2d\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u5bf9\u5176\u6027\u80fd\u9a71\u52a8\u56e0\u7d20\u7684\u7406\u89e3\u6709\u9650\uff0c\u5e76\u4e14\u73b0\u6709\u7814\u7a76\u591a\u5c06\u5176\u4f5c\u4e3a\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u66ff\u4ee3\u54c1\u800c\u975e\u8865\u5145\u54c1\u3002", "method": "\u63d0\u51fascMPT\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528scGPT\u548c\u6355\u83b7\u751f\u7269\u5b66\u89c1\u89e3\u7684LLM\u5355\u7ec6\u80de\u8868\u793a\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002\u8fd8\u5c1d\u8bd5\u4e86\u5176\u4ed6\u878d\u5408\u65b9\u6cd5\u3002", "result": "scMPT\u6a21\u578b\u5c55\u793a\u51fa\u6bd4\u5176\u7ec4\u6210\u90e8\u5206\u66f4\u5f3a\u3001\u66f4\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u5e76\u586b\u8865\u4e86\u5b83\u4eec\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e0a\u7684\u6027\u80fd\u5dee\u8ddd\u3002\u878d\u5408\u65b9\u6cd5\u4e5f\u663e\u793a\u51fa\u6f5c\u529b\u3002", "conclusion": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u53ef\u4ee5\u4f5c\u4e3a\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u8865\u5145\uff0c\u5e76\u63a8\u52a8\u5355\u7ec6\u80de\u5206\u6790\u7684\u6539\u8fdb\u3002"}}
{"id": "2507.10202", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10202", "abs": "https://arxiv.org/abs/2507.10202", "authors": ["Jaeseong Lee", "Yeeun Choi", "Heechan Choi", "Hanjung Kim", "Seonjoo Kim"], "title": "A Training-Free, Task-Agnostic Framework for Enhancing MLLM Performance on High-Resolution Images", "comment": "Accepted at CVPR 2025 Workshop on Emergent Visual Abilities and\n  Limits of Foundation Models", "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in vision-language understanding, reasoning, and generation.\nHowever, they struggle with tasks requiring fine-grained localization and\nreasoning in high-resolution images. This constraint stems from the fact that\nMLLMs are fine-tuned with fixed image resolution to align with the pre-trained\nimage encoder used in MLLM. Consequently, feeding high-resolution images\ndirectly into MLLMs leads to poor generalization due to a train-test resolution\ndiscrepancy, while downsampling these images-although ensuring\nconsistency-compromises fine-grained visual details and ultimately degrades\nperformance. To address this challenge, we propose Extract Candidate then\nPredict (ECP), a novel training-free, task-agnostic two-stage framework\ndesigned to enhance MLLM performance on high-resolution images. The key\nintuition behind ECP is that while MLLMs struggle with high-resolution images,\ntheir predictions on downsampled images still contain implicit localization\ncues. By first identifying candidate region using the coarse prediction and\nthen predicting the final output based on candidate region, ECP effectively\npreserves fine-grained details while mitigating the challenges posed by\nhigh-resolution data. We validate our framework on 4K GUI grounding and 4K, 8K\nMLLM perception, achieving +21.3%, +5.8%, +5.2% absolute improvement compared\nto baseline respectively, demonstrating its effectiveness. Code is available at\nhttps://github.com/yenncye/ECP.", "AI": {"tldr": "ECP\u6846\u67b6\u901a\u8fc7\u8bc6\u522b\u5019\u9009\u533a\u57df\u5e76\u8fdb\u884c\u9884\u6d4b\uff0c\u89e3\u51b3\u4e86MLLM\u5728\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u74f6\u9888\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5904\u7406\u9700\u8981\u7cbe\u7ec6\u5b9a\u4f4d\u548c\u63a8\u7406\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u56e0\u4e3a\u5b83\u4eec\u901a\u5e38\u5728\u56fa\u5b9a\u7684\u56fe\u50cf\u5206\u8fa8\u7387\u4e0b\u8fdb\u884c\u5fae\u8c03\uff0c\u5bfc\u81f4\u8bad\u7ec3-\u6d4b\u8bd5\u5206\u8fa8\u7387\u4e0d\u5339\u914d\u6216\u964d\u91c7\u6837\u5bfc\u81f4\u7ec6\u8282\u4e22\u5931\u3002", "method": "ECP\u6846\u67b6\u9996\u5148\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u964d\u91c7\u6837\u56fe\u50cf\u4e0a\u7684\u9884\u6d4b\u6765\u8bc6\u522b\u5019\u9009\u533a\u57df\uff0c\u7136\u540e\u5728\u8fd9\u4e9b\u5019\u9009\u533a\u57df\u4e0a\u8fdb\u884c\u7cbe\u7ec6\u5316\u7684\u9884\u6d4b\uff0c\u4ece\u800c\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u4fdd\u7559\u7ec6\u8282\u5e76\u514b\u670d\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\u3002", "result": "\u57284K GUI\u57fa\u7840\u548c4K\u30018K MLLM\u611f\u77e5\u4efb\u52a1\u4e0a\uff0cECP\u6846\u67b6\u76f8\u8f83\u4e8e\u57fa\u7ebf\u6a21\u578b\u5206\u522b\u53d6\u5f97\u4e86+21.3%\u3001+5.8%\u3001+5.2%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u63d0\u53d6\u5019\u9009\u540e\u9884\u6d4b\uff08ECP\uff09\u7684\u65b0\u9896\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u4e14\u4efb\u52a1\u65e0\u5173\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u5904\u7406\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u65f6\u7684\u6027\u80fd\u3002ECP\u901a\u8fc7\u9996\u5148\u4f7f\u7528\u7c97\u7565\u9884\u6d4b\u8bc6\u522b\u5019\u9009\u533a\u57df\uff0c\u7136\u540e\u5728\u5019\u9009\u533a\u57df\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u6700\u7ec8\u9884\u6d4b\uff0c\u4ece\u800c\u6709\u6548\u4fdd\u7559\u4e86\u7ec6\u7c92\u5ea6\u7684\u7ec6\u8282\uff0c\u5e76\u51cf\u8f7b\u4e86\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u5e26\u6765\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u57284K GUI\u57fa\u7840\u548c4K\u30018K MLLM\u611f\u77e5\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5206\u522b\u53d6\u5f97\u4e86+21.3%\u3001+5.8%\u3001+5.2%\u7684\u7edd\u5bf9\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2507.10048", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10048", "abs": "https://arxiv.org/abs/2507.10048", "authors": ["Benedict Gerlach", "Marie Anastacio", "Holger H. Hoos"], "title": "On the Efficiency of Training Robust Decision Trees", "comment": "Presented as a poster at SAIV 2025", "summary": "As machine learning gets adopted into the industry quickly, trustworthiness\nis increasingly in focus. Yet, efficiency and sustainability of robust training\npipelines still have to be established. In this work, we consider a simple\npipeline for training adversarially robust decision trees and investigate the\nefficiency of each step. Our pipeline consists of three stages. Firstly, we\nchoose the perturbation size automatically for each dataset. For that, we\nintroduce a simple algorithm, instead of relying on intuition or prior work.\nMoreover, we show that the perturbation size can be estimated from smaller\nmodels than the one intended for full training, and thus significant gains in\nefficiency can be achieved. Secondly, we train state-of-the-art adversarial\ntraining methods and evaluate them regarding both their training time and\nadversarial accuracy. Thirdly, we certify the robustness of each of the models\nthus obtained and investigate the time required for this. We find that\nverification time, which is critical to the efficiency of the full pipeline, is\nnot correlated with training time.", "AI": {"tldr": "This paper analyzes the efficiency of training robust decision trees, finding verification time and training time are not correlated.", "motivation": "To address the growing need for trustworthiness in machine learning and establish the efficiency and sustainability of robust training pipelines.", "method": "The pipeline consists of three stages: automatic perturbation size selection using a novel algorithm, training state-of-the-art adversarial training methods and evaluating their training time and adversarial accuracy, and certifying the robustness of the trained models and investigating the time required for certification. The perturbation size can be estimated from smaller models for efficiency gains.", "result": "The paper investigates the efficiency of each step in a simple pipeline for training adversarially robust decision trees. It introduces a new algorithm for automatic perturbation size selection, shows efficiency gains by estimating perturbation size from smaller models, evaluates state-of-the-art adversarial training methods based on training time and adversarial accuracy, and analyzes the verification time, finding it uncorrelated with training time.", "conclusion": "The verification time is not correlated with the training time, which is critical for the efficiency of the full pipeline."}}
{"id": "2507.10203", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10203", "abs": "https://arxiv.org/abs/2507.10203", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Improving Multimodal Learning via Imbalanced Learning", "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may\nperform worse than unimodal learning. Existing approaches attribute this issue\nto imbalanced learning across modalities and tend to address it through\ngradient balancing. However, this paper argues that balanced learning is not\nthe optimal setting for multimodal learning. With bias-variance analysis, we\nprove that imbalanced dependency on each modality obeying the inverse ratio of\ntheir variances contributes to optimal performance. To this end, we propose the\nAsymmetric Representation Learning(ARL) strategy to assist multimodal learning\nvia imbalanced optimization. ARL introduces auxiliary regularizers for each\nmodality encoder to calculate their prediction variance. ARL then calculates\ncoefficients via the unimodal variance to re-weight the optimization of each\nmodality, forcing the modality dependence ratio to be inversely proportional to\nthe modality variance ratio. Moreover, to minimize the generalization error,\nARL further introduces the prediction bias of each modality and jointly\noptimizes them with multimodal loss. Notably, all auxiliary regularizers share\nparameters with the multimodal model and rely only on the modality\nrepresentation. Thus the proposed ARL strategy introduces no extra parameters\nand is independent of the structures and fusion methods of the multimodal\nmodel. Finally, extensive experiments on various datasets validate the\neffectiveness and versatility of ARL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-ARL}{https://github.com/shicaiwei123/ICCV2025-ARL}", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aARL\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u4f18\u5316\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6b20\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u6b20\u4f18\u5316\u95ee\u9898\u65f6\uff0c\u901a\u5e38\u5c06\u95ee\u9898\u5f52\u56e0\u4e8e\u6a21\u6001\u95f4\u7684\u4e0d\u5e73\u8861\u5b66\u4e60\uff0c\u5e76\u8bd5\u56fe\u901a\u8fc7\u68af\u5ea6\u5e73\u8861\u6765\u89e3\u51b3\u3002\u7136\u800c\uff0c\u8be5\u8bba\u6587\u8ba4\u4e3a\uff0c\u4e0d\u5e73\u8861\u5b66\u4e60\u5e76\u975e\u591a\u6a21\u6001\u5b66\u4e60\u7684\u6700\u4f18\u8bbe\u7f6e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAsymmetric Representation Learning(ARL)\u7684\u7b56\u7565\uff0c\u901a\u8fc7\u975e\u5bf9\u79f0\u4f18\u5316\u6765\u8f85\u52a9\u591a\u6a21\u6001\u5b66\u4e60\u3002ARL\u5f15\u5165\u4e86\u989d\u5916\u7684\u6b63\u5219\u5316\u5668\u6765\u8ba1\u7b97\u6bcf\u4e2a\u6a21\u6001\u7684\u9884\u6d4b\u65b9\u5dee\uff0c\u5e76\u6839\u636e\u5355\u6a21\u6001\u65b9\u5dee\u8ba1\u7b97\u7cfb\u6570\u6765\u91cd\u65b0\u52a0\u6743\u6bcf\u4e2a\u6a21\u6001\u7684\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ece\u800c\u4f7f\u5f97\u6a21\u6001\u4f9d\u8d56\u6027\u6bd4\u4f8b\u4e0e\u6a21\u6001\u65b9\u5dee\u6bd4\u4f8b\u6210\u53cd\u6bd4\u3002\u6b64\u5916\uff0c\u4e3a\u4e86\u6700\u5c0f\u5316\u6cdb\u5316\u8bef\u5dee\uff0cARL\u8fd8\u5f15\u5165\u4e86\u6bcf\u4e2a\u6a21\u6001\u7684\u9884\u6d4b\u504f\u5dee\uff0c\u5e76\u4e0e\u591a\u6a21\u6001\u635f\u5931\u8054\u5408\u4f18\u5316\u3002\u8be5\u7b56\u7565\u4e0d\u5f15\u5165\u989d\u5916\u7684\u53c2\u6570\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4e8e\u591a\u6a21\u6001\u6a21\u578b\u7684\u7ed3\u6784\u548c\u878d\u5408\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u504f\u5dee-\u65b9\u5dee\u5206\u6790\u8bc1\u660e\uff0c\u9075\u5faa\u4e0e\u5176\u65b9\u5dee\u6210\u53cd\u6bd4\u7684\u6bd4\u4f8b\u7684\u4e0d\u5e73\u8861\u7684\u6bcf\u4e2a\u6a21\u6001\u7684\u4f9d\u8d56\u6027\u6709\u52a9\u4e8e\u5b9e\u73b0\u6700\u4f18\u6027\u80fd\u3002\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eAsymmetric Representation Learning(ARL)\u7b56\u7565\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.10209", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10209", "abs": "https://arxiv.org/abs/2507.10209", "authors": ["Huai-Qian Khor", "Yante Li", "Xingxun Jiang", "Guoying Zhao"], "title": "Is Micro-expression Ethnic Leaning?", "comment": null, "summary": "How much does ethnicity play its part in emotional expression? Emotional\nexpression and micro-expression research probe into understanding human\npsychological responses to emotional stimuli, thereby revealing substantial\nhidden yet authentic emotions that can be useful in the event of diagnosis and\ninterviews. While increased attention had been provided to micro-expression\nanalysis, the studies were done under Ekman's assumption of emotion\nuniversality, where emotional expressions are identical across cultures and\nsocial contexts. Our computational study uncovers some of the influences of\nethnic background in expression analysis, leading to an argument that the\nemotional universality hypothesis is an overgeneralization from the perspective\nof manual psychological analysis. In this research, we propose to investigate\nthe level of influence of ethnicity in a simulated micro-expression scenario.\nWe construct a cross-cultural micro-expression database and algorithmically\nannotate the ethnic labels to facilitate the investigation. With the ethnically\nannotated dataset, we perform a prima facie study to compare mono-ethnicity and\nstereo-ethnicity in a controlled environment, which uncovers a certain\ninfluence of ethnic bias via an experimental way. Building on this finding, we\npropose a framework that integrates ethnic context into the emotional feature\nlearning process, yielding an ethnically aware framework that recognises\nethnicity differences in micro-expression recognition. For improved\nunderstanding, qualitative analyses have been done to solidify the preliminary\ninvestigation into this new realm of research. Code is publicly available at\nhttps://github.com/IcedDoggie/ICMEW2025_EthnicMER", "AI": {"tldr": "\u79cd\u65cf\u80cc\u666f\u5f71\u54cd\u5fae\u8868\u60c5\uff0c\u60c5\u611f\u666e\u904d\u6027\u5047\u8bbe\u53ef\u80fd\u8fc7\u4e8e\u7b80\u5316\u3002\u672c\u7814\u7a76\u6784\u5efa\u4e86\u5305\u542b\u79cd\u65cf\u4fe1\u606f\u7684\u6570\u636e\u5e93\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u79cd\u65cf\u504f\u89c1\u7684\u5b58\u5728\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u8003\u8651\u79cd\u65cf\u56e0\u7d20\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u79cd\u65cf\u80cc\u666f\u5728\u60c5\u611f\u8868\u8fbe\uff08\u7279\u522b\u662f\u5fae\u8868\u60c5\uff09\u5206\u6790\u4e2d\u7684\u4f5c\u7528\uff0c\u5e76\u6311\u6218\u60c5\u611f\u666e\u904d\u6027\u5047\u8bbe\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u8de8\u6587\u5316\u5fae\u8868\u60c5\u6570\u636e\u5e93\u5e76\u8fdb\u884c\u7b97\u6cd5\u6ce8\u91ca\uff0c\u6bd4\u8f83\u5355\u4e00\u6c11\u65cf\u548c\u591a\u6c11\u65cf\u5728\u53d7\u63a7\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u79cd\u65cf\u504f\u89c1\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u79cd\u65cf\u80cc\u666f\u5bf9\u5fae\u8868\u60c5\u5206\u6790\u5b58\u5728\u4e00\u5b9a\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u8bc6\u522b\u79cd\u65cf\u5dee\u5f02\u7684\u3001\u5177\u6709\u79cd\u65cf\u610f\u8bc6\u7684\u5fae\u8868\u60c5\u8bc6\u522b\u6846\u67b6\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u79cd\u65cf\u80cc\u666f\u786e\u5b9e\u4f1a\u5f71\u54cd\u5fae\u8868\u60c5\u5206\u6790\uff0c\u6311\u6218\u4e86\u60c5\u611f\u666e\u904d\u6027\u5047\u8bbe\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6574\u5408\u4e86\u79cd\u65cf\u80cc\u666f\u7684\u60c5\u611f\u7279\u5f81\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u5fae\u8868\u60c5\u8bc6\u522b\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2507.10088", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10088", "abs": "https://arxiv.org/abs/2507.10088", "authors": ["Tung Sum Thomas Kwok", "Zeyong Zhang", "Chi-Hua Wang", "Guang Cheng"], "title": "Towards High Supervised Learning Utility Training Data Generation: Data Pruning and Column Reordering", "comment": "Accepted by Agentic & GenAI Evaluation KDD2025", "summary": "Tabular data synthesis for supervised learning ('SL') model training is\ngaining popularity in industries such as healthcare, finance, and retail.\nDespite the progress made in tabular data generators, models trained with\nsynthetic data often underperform compared to those trained with original data.\nThis low SL utility of synthetic data stems from class imbalance exaggeration\nand SL data relationship overlooked by tabular generator. To address these\nchallenges, we draw inspirations from techniques in emerging data-centric\nartificial intelligence and elucidate Pruning and ReOrdering ('PRRO'), a novel\npipeline that integrates data-centric techniques into tabular data synthesis.\nPRRO incorporates data pruning to guide the table generator towards\nobservations with high signal-to-noise ratio, ensuring that the class\ndistribution of synthetic data closely matches that of the original data.\nBesides, PRRO employs a column reordering algorithm to align the data modeling\nstructure of generators with that of SL models. These two modules enable PRRO\nto optimize SL utility of synthetic data. Empirical experiments on 22 public\ndatasets show that synthetic data generated using PRRO enhances predictive\nperformance compared to data generated without PRRO. Specifically, synthetic\nreplacement of original data yields an average improvement of 26.74% and up to\n871.46% improvement using PRRO, while synthetic appendant to original data\nresults with PRRO-generated data results in an average improvement of 6.13% and\nup to 200.32%. Furthermore, experiments on six highly imbalanced datasets show\nthat PRRO enables the generator to produce synthetic data with a class\ndistribution that resembles the original data more closely, achieving a\nsimilarity improvement of 43%. Through PRRO, we foster a seamless integration\nof data synthesis to subsequent SL prediction, promoting quality and accessible\ndata analysis.", "AI": {"tldr": "PRRO\u6d41\u6c34\u7ebf\u901a\u8fc7\u6570\u636e\u526a\u679d\u548c\u5217\u91cd\u6392\u6280\u672f\uff0c\u63d0\u5347\u4e86\u5408\u6210\u6570\u636e\u7684\u76d1\u7763\u5b66\u4e60\u6548\u7528\u548c\u7c7b\u522b\u5206\u5e03\u5339\u914d\u5ea6\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u9884\u6d4b\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u5408\u6210\u6570\u636e\u7684\u76d1\u7763\u5b66\u4e60\u6548\u7528\u4f4e\uff0c\u4e3b\u8981\u7531\u4e8e\u7c7b\u522b\u4e0d\u5e73\u8861\u52a0\u5267\u548c\u6570\u636e\u5173\u7cfb\u88ab\u5ffd\u89c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u6574\u5408\u6570\u636e\u4e2d\u5fc3\u6280\u672f\u6765\u4f18\u5316\u5408\u6210\u6570\u636e\u7684\u76d1\u7763\u5b66\u4e60\u6548\u7528\u3002", "method": "PRRO\u662f\u4e00\u4e2a\u5305\u542b\u6570\u636e\u526a\u679d\u548c\u5217\u91cd\u6392\u7684\u6d41\u6c34\u7ebf\uff0c\u6570\u636e\u526a\u679d\u7528\u4e8e\u6307\u5bfc\u751f\u6210\u5668\u5173\u6ce8\u9ad8\u4fe1\u566a\u6bd4\u7684\u89c2\u6d4b\uff0c\u4ee5\u5339\u914d\u7c7b\u522b\u5206\u5e03\uff1b\u5217\u91cd\u6392\u7528\u4e8e\u8c03\u6574\u751f\u6210\u5668\u7684\u6570\u636e\u5efa\u6a21\u7ed3\u6784\u4ee5\u5339\u914d\u76d1\u7763\u5b66\u4e60\u6a21\u578b\u3002", "result": "PRRO\u572822\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4f7f\u7528PRRO\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u66ff\u6362\u539f\u59cb\u6570\u636e\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u534726.74%\uff0c\u6700\u9ad8\u63d0\u5347871.46%\uff1b\u4f5c\u4e3a\u539f\u59cb\u6570\u636e\u9644\u52a0\u65f6\uff0c\u5e73\u5747\u63d0\u53476.13%\uff0c\u6700\u9ad8\u63d0\u5347200.32%\u3002\u57286\u4e2a\u9ad8\u5ea6\u4e0d\u5e73\u8861\u7684\u6570\u636e\u96c6\u4e0a\uff0cPRRO\u4f7f\u751f\u6210\u5668\u4ea7\u751f\u7684\u5408\u6210\u6570\u636e\u7c7b\u522b\u5206\u5e03\u4e0e\u539f\u59cb\u6570\u636e\u76f8\u4f3c\u5ea6\u63d0\u9ad8\u4e8643%\u3002", "conclusion": "PRRO\u901a\u8fc7\u6570\u636e\u526a\u679d\u548c\u5217\u91cd\u6392\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8868\u683c\u6570\u636e\u751f\u6210\u5668\u5728\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u52a0\u5267\u548c\u6570\u636e\u5173\u7cfb\u5ffd\u89c6\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5408\u6210\u6570\u636e\u7684\u76d1\u7763\u5b66\u4e60\u6548\u7528\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cPRRO\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u9884\u6d4b\u6027\u80fd\u4e0a\u76f8\u6bd4\u672a\u4f7f\u7528\u7684\u751f\u6210\u5668\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u4e14\u80fd\u66f4\u51c6\u786e\u5730\u5339\u914d\u539f\u59cb\u6570\u636e\u7684\u7c7b\u522b\u5206\u5e03\uff0c\u4e3a\u6570\u636e\u5408\u6210\u4e0e\u76d1\u7763\u5b66\u4e60\u7684\u7ed3\u5408\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2507.10213", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10213", "abs": "https://arxiv.org/abs/2507.10213", "authors": ["Shicai Wei", "Chunbo Luo", "Yang Luo"], "title": "Boosting Multimodal Learning via Disentangled Gradient Learning", "comment": "Accepted to ICCV2025", "summary": "Multimodal learning often encounters the under-optimized problem and may have\nworse performance than unimodal learning. Existing methods attribute this\nproblem to the imbalanced learning between modalities and rebalance them\nthrough gradient modulation. However, they fail to explain why the dominant\nmodality in multimodal models also underperforms that in unimodal learning. In\nthis work, we reveal the optimization conflict between the modality encoder and\nmodality fusion module in multimodal models. Specifically, we prove that the\ncross-modal fusion in multimodal models decreases the gradient passed back to\neach modality encoder compared with unimodal models. Consequently, the\nperformance of each modality in the multimodal model is inferior to that in the\nunimodal model. To this end, we propose a disentangled gradient learning (DGL)\nframework to decouple the optimization of the modality encoder and modality\nfusion module in the multimodal model. DGL truncates the gradient\nback-propagated from the multimodal loss to the modality encoder and replaces\nit with the gradient from unimodal loss. Besides, DGL removes the gradient\nback-propagated from the unimodal loss to the modality fusion module. This\nhelps eliminate the gradient interference between the modality encoder and\nmodality fusion module while ensuring their respective optimization processes.\nFinally, extensive experiments on multiple types of modalities, tasks, and\nframeworks with dense cross-modal interaction demonstrate the effectiveness and\nversatility of the proposed DGL. Code is available at\n\\href{https://github.com/shicaiwei123/ICCV2025-GDL}{https://github.com/shicaiwei123/ICCV2025-GDL}", "AI": {"tldr": "\u591a\u6a21\u6001\u5b66\u4e60\u5b58\u5728\u6b20\u4f18\u5316\u95ee\u9898\uff0c\u56e0\u6a21\u6001\u7f16\u7801\u5668\u548c\u878d\u5408\u6a21\u5757\u95f4\u4f18\u5316\u51b2\u7a81\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u6587\u63d0\u51faDGL\u6846\u67b6\uff0c\u89e3\u8026\u4e8c\u8005\u4f18\u5316\uff0c\u7528\u5355\u6a21\u6001\u68af\u5ea6\u66ff\u6362\u591a\u6a21\u6001\u68af\u5ea6\uff0c\u6d88\u9664\u5e72\u6270\uff0c\u5b9e\u9a8c\u8bc1\u660e\u6709\u6548\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6b20\u4f18\u5316\u95ee\u9898\u5f52\u56e0\u4e8e\u6a21\u6001\u95f4\u5b66\u4e60\u4e0d\u5e73\u8861\uff0c\u5e76\u901a\u8fc7\u68af\u5ea6\u8c03\u8282\u8fdb\u884c\u518d\u5e73\u8861\uff0c\u4f46\u672a\u80fd\u89e3\u91ca\u4e3b\u5bfc\u6a21\u6001\u5728\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u4e5f\u900a\u4e8e\u5355\u6a21\u6001\u5b66\u4e60\u7684\u60c5\u51b5\u3002\u672c\u7814\u7a76\u63ed\u793a\u4e86\u591a\u6a21\u6001\u6a21\u578b\u4e2d\u6a21\u6001\u7f16\u7801\u5668\u548c\u6a21\u6001\u878d\u5408\u6a21\u5757\u4e4b\u95f4\u7684\u4f18\u5316\u51b2\u7a81\uff0c\u5e76\u8bc1\u660e\u4e86\u8de8\u6a21\u6001\u878d\u5408\u4f1a\u964d\u4f4e\u53cd\u5411\u4f20\u64ad\u5230\u6bcf\u4e2a\u6a21\u6001\u7f16\u7801\u5668\u7684\u68af\u5ea6\uff0c\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u8026\u68af\u5ea6\u5b66\u4e60\uff08DGL\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u622a\u65ad\u4ece\u591a\u6a21\u6001\u635f\u5931\u53cd\u5411\u4f20\u64ad\u5230\u6a21\u6001\u7f16\u7801\u5668\u7684\u68af\u5ea6\uff0c\u5e76\u7528\u6765\u81ea\u5355\u6a21\u6001\u635f\u5931\u7684\u68af\u5ea6\u66ff\u6362\u5b83\uff0c\u540c\u65f6\u79fb\u9664\u6765\u81ea\u5355\u6a21\u6001\u635f\u5931\u53cd\u5411\u4f20\u64ad\u5230\u6a21\u6001\u878d\u5408\u6a21\u5757\u7684\u68af\u5ea6\uff0c\u4ee5\u6d88\u9664\u68af\u5ea6\u5e72\u6270\u5e76\u786e\u4fdd\u5404\u81ea\u7684\u4f18\u5316\u8fc7\u7a0b\u3002", "result": "\u901a\u8fc7\u5728\u591a\u79cd\u6a21\u6001\u3001\u4efb\u52a1\u548c\u5177\u6709\u5bc6\u96c6\u8de8\u6a21\u6001\u4ea4\u4e92\u7684\u6846\u67b6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684DGL\u6846\u67b6\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u89e3\u8026\u68af\u5ea6\u5b66\u4e60\uff08DGL\uff09\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u6a21\u6001\u7f16\u7801\u5668\u548c\u6a21\u6001\u878d\u5408\u6a21\u5757\u7684\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u6b20\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u6a21\u6001\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2507.10120", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10120", "abs": "https://arxiv.org/abs/2507.10120", "authors": ["Cheng Sun", "Zhen Zhang", "Shaofu Yang"], "title": "A Variance-Reduced Cubic-Regularized Newton for Policy Optimization", "comment": "13 pages, 1 figure", "summary": "In this paper, we study a second-order approach to policy optimization in\nreinforcement learning. Existing second-order methods often suffer from\nsuboptimal sample complexity or rely on unrealistic assumptions about\nimportance sampling. To overcome these limitations, we propose VR-CR-PN, a\nvariance-reduced cubic-regularized policy Newton algorithm. To the best of our\nknowledge, this is the first algorithm that integrates Hessian-aided variance\nreduction with second-order policy optimization, effectively addressing the\ndistribution shift problem and achieving best-known sample complexity under\ngeneral nonconvex conditions but without the need for importance sampling. We\ntheoretically establish that VR-CR-PN achieves a sample complexity of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3})$ to reach an $\\epsilon$-second-order\nstationary point, significantly improving upon the previous best result of\n$\\tilde{\\mathcal{O}}(\\epsilon^{-3.5})$ under comparable assumptions. As an\nadditional contribution, we introduce a novel Hessian estimator for the\nexpected return function, which admits a uniform upper bound independent of the\nhorizon length $H$, allowing the algorithm to achieve horizon-independent\nsample complexity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVR-CR-PN\u7684\u65b0\u7b97\u6cd5\uff0c\u7528\u4e8e\u7b56\u7565\u4f18\u5316\uff0c\u5b83\u901a\u8fc7\u7ed3\u5408\u65b9\u5dee\u7f29\u51cf\u548c\u4e8c\u9636\u4f18\u5316\u6765\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u6837\u672c\u590d\u6742\u5ea6\u548c\u5206\u5e03\u504f\u79fb\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u4e8c\u9636\u65b9\u6cd5\u5728\u6837\u672c\u590d\u6742\u5ea6\u65b9\u9762\u7684\u4e0d\u8db3\u6216\u5bf9\u91cd\u8981\u6027\u91c7\u6837\u7684\u8fc7\u5ea6\u4f9d\u8d56\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVR-CR-PN\u7684\u65b9\u5dee\u7f29\u51cf\u7684\u7acb\u65b9\u6b63\u5219\u5316\u7b56\u7565\u725b\u987f\u7b97\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u7684\u671f\u671b\u56de\u62a5\u51fd\u6570Hessian\u4f30\u8ba1\u5668\uff0c\u8be5\u4f30\u8ba1\u5668\u5177\u6709\u4e0e\u65f6\u95f4\u8303\u56f4H\u65e0\u5173\u7684\u7edf\u4e00\u4e0a\u9650\u3002", "result": "VR-CR-PN\u7b97\u6cd5\u8fbe\u5230\u4e86$\tilde{\tilde{\\mathcal{O}}}(\\epsilon^{-3})$\u7684\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4f18\u4e8e\u5148\u524d$\tilde{\tilde{\\mathcal{O}}}(\\epsilon^{-3.5})$\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u4e0e\u65f6\u95f4\u8303\u56f4\u65e0\u5173\u7684\u6837\u672c\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5c06\u65b9\u5dee\u7f29\u51cf\u4e0e\u4e8c\u9636\u7b56\u7565\u4f18\u5316\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u5206\u5e03\u504f\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u901a\u7528\u975e\u51f8\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5df2\u77e5\u7684\u6700\u4f18\u6837\u672c\u590d\u6742\u5ea6\uff0c\u4e14\u65e0\u9700\u91cd\u8981\u6027\u91c7\u6837\u3002"}}
{"id": "2507.10217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10217", "abs": "https://arxiv.org/abs/2507.10217", "authors": ["Jeongho Kim", "Sunghyun Park", "Hyoungwoo Park", "Sungrack Yun", "Jaegul Choo", "Seokeon Cho"], "title": "From Wardrobe to Canvas: Wardrobe Polyptych LoRA for Part-level Controllable Human Image Generation", "comment": "10 pages, 8 figures", "summary": "Recent diffusion models achieve personalization by learning specific\nsubjects, allowing learned attributes to be integrated into generated images.\nHowever, personalized human image generation remains challenging due to the\nneed for precise and consistent attribute preservation (e.g., identity,\nclothing details). Existing subject-driven image generation methods often\nrequire either (1) inference-time fine-tuning with few images for each new\nsubject or (2) large-scale dataset training for generalization. Both approaches\nare computationally expensive and impractical for real-time applications. To\naddress these limitations, we present Wardrobe Polyptych LoRA, a novel\npart-level controllable model for personalized human image generation. By\ntraining only LoRA layers, our method removes the computational burden at\ninference while ensuring high-fidelity synthesis of unseen subjects. Our key\nidea is to condition the generation on the subject's wardrobe and leverage\nspatial references to reduce information loss, thereby improving fidelity and\nconsistency. Additionally, we introduce a selective subject region loss, which\nencourages the model to disregard some of reference images during training. Our\nloss ensures that generated images better align with text prompts while\nmaintaining subject integrity. Notably, our Wardrobe Polyptych LoRA requires no\nadditional parameters at the inference stage and performs generation using a\nsingle model trained on a few training samples. We construct a new dataset and\nbenchmark tailored for personalized human image generation. Extensive\nexperiments show that our approach significantly outperforms existing\ntechniques in fidelity and consistency, enabling realistic and\nidentity-preserving full-body synthesis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Wardrobe Polyptych LoRA \u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u4e2a\u6027\u5316\u4eba\u7269\u56fe\u50cf\u751f\u6210\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u4ec5\u8bad\u7ec3 LoRA \u5c42\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u5e76\u4f7f\u7528\u7a7a\u95f4\u53c2\u8003\u548c\u9009\u62e9\u6027\u53d7\u8bd5\u8005\u533a\u57df\u635f\u5931\u6765\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u4e2a\u6027\u5316\u4eba\u7269\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u4e14\u4e0d\u9002\u7528\u4e8e\u5b9e\u65f6\u5e94\u7528\u7684\u95ee\u9898\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u9700\u8981\u63a8\u7406\u65f6\u5fae\u8c03\u6216\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8bad\u7ec3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Wardrobe Polyptych LoRA \u7684\u65b0\u9896\u7684\u3001\u90e8\u5206\u53ef\u63a7\u7684\u4e2a\u6027\u5316\u4eba\u7269\u56fe\u50cf\u751f\u6210\u6a21\u578b\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u4ec5\u8bad\u7ec3 LoRA \u5c42\u6765\u63d0\u9ad8\u63a8\u7406\u6548\u7387\uff0c\u5e76\u4f7f\u7528\u7a7a\u95f4\u53c2\u8003\u548c\u9009\u62e9\u6027\u53d7\u8bd5\u8005\u533a\u57df\u635f\u5931\u6765\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0cWardrobe Polyptych LoRA \u5728\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u80fd\u591f\u5b9e\u73b0\u903c\u771f\u4e14\u4fdd\u6301\u8eab\u4efd\u7684\u5168\u8eab\u5408\u6210\u3002", "conclusion": "Wardrobe Polyptych LoRA \u901a\u8fc7\u4ec5\u8bad\u7ec3 LoRA \u5c42\uff0c\u5728\u63a8\u7406\u65f6\u6d88\u9664\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u540c\u65f6\u786e\u4fdd\u4e86\u5bf9\u672a\u89c1\u4e3b\u9898\u7684\u9ad8\u4fdd\u771f\u5408\u6210\u3002\u5b83\u901a\u8fc7\u4ee5\u53d7\u8bd5\u8005\u7684\u8863\u6a71\u4e3a\u6761\u4ef6\u5e76\u5229\u7528\u7a7a\u95f4\u53c2\u8003\u6765\u51cf\u5c11\u4fe1\u606f\u4e22\u5931\uff0c\u4ece\u800c\u63d0\u9ad8\u4fdd\u771f\u5ea6\u548c\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5b83\u5f15\u5165\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u7684\u53d7\u8bd5\u8005\u533a\u57df\u635f\u5931\uff0c\u4ee5\u786e\u4fdd\u751f\u6210\u7684\u56fe\u50cf\u66f4\u597d\u5730\u4e0e\u6587\u672c\u63d0\u793a\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u6301\u53d7\u8bd5\u8005\u5b8c\u6574\u6027\u3002"}}
{"id": "2507.10132", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10132", "abs": "https://arxiv.org/abs/2507.10132", "authors": ["Usman Gani Joy"], "title": "Wavelet-Enhanced Neural ODE and Graph Attention for Interpretable Energy Forecasting", "comment": null, "summary": "Accurate forecasting of energy demand and supply is critical for optimizing\nsustainable energy systems, yet it is challenged by the variability of\nrenewable sources and dynamic consumption patterns. This paper introduces a\nneural framework that integrates continuous-time Neural Ordinary Differential\nEquations (Neural ODEs), graph attention, multi-resolution wavelet\ntransformations, and adaptive learning of frequencies to address the issues of\ntime series prediction. The model employs a robust ODE solver, using the\nRunge-Kutta method, paired with graph-based attention and residual connections\nto better understand both structural and temporal patterns. Through\nwavelet-based feature extraction and adaptive frequency modulation, it adeptly\ncaptures and models diverse, multi-scale temporal dynamics. When evaluated\nacross seven diverse datasets: ETTh1, ETTh2, ETTm1, ETTm2 (electricity\ntransformer temperature), and Waste, Solar, and Hydro (renewable energy), this\narchitecture consistently outperforms state-of-the-art baselines in various\nforecasting metrics, proving its robustness in capturing complex temporal\ndependencies. Furthermore, the model enhances interpretability through SHAP\nanalysis, making it suitable for sustainable energy applications.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Neural ODE\u3001\u56fe\u6ce8\u610f\u529b\u3001\u5c0f\u6ce2\u53d8\u6362\u548c\u81ea\u9002\u5e94\u9891\u7387\u5b66\u4e60\u7684\u795e\u7ecf\u6846\u67b6\uff0c\u7528\u4e8e\u80fd\u6e90\u9700\u6c42\u548c\u4f9b\u5e94\u9884\u6d4b\u3002\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u4f18\u4e8e\u73b0\u6709\u6280\u672f\uff0c\u5e76\u80fd\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u51c6\u786e\u9884\u6d4b\u80fd\u6e90\u9700\u6c42\u548c\u4f9b\u5e94\u5bf9\u4e8e\u4f18\u5316\u53ef\u6301\u7eed\u80fd\u6e90\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u53ef\u518d\u751f\u80fd\u6e90\u7684\u53ef\u53d8\u6027\u548c\u52a8\u6001\u6d88\u8d39\u6a21\u5f0f\u800c\u9762\u4e34\u6311\u6218\u3002", "method": "\u672c\u6587\u5f15\u5165\u4e86\u4e00\u4e2a\u795e\u7ecf\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u8fde\u7eed\u65f6\u95f4\u795e\u7ecf\u5e38\u5fae\u5206\u65b9\u7a0b\uff08Neural ODE\uff09\u3001\u56fe\u6ce8\u610f\u529b\u3001\u591a\u5206\u8fa8\u7387\u5c0f\u6ce2\u53d8\u6362\u548c\u9891\u7387\u7684\u81ea\u9002\u5e94\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\u3002\u8be5\u6a21\u578b\u91c7\u7528\u9c81\u68d2\u7684ODE\u6c42\u89e3\u5668\uff08\u4f7f\u7528Runge-Kutta\u65b9\u6cd5\uff09\uff0c\u5e76\u7ed3\u5408\u57fa\u4e8e\u56fe\u7684\u6ce8\u610f\u529b\u548c\u6b8b\u5dee\u8fde\u63a5\uff0c\u4ee5\u66f4\u597d\u5730\u7406\u89e3\u7ed3\u6784\u548c\u65f6\u95f4\u6a21\u5f0f\u3002\u901a\u8fc7\u57fa\u4e8e\u5c0f\u6ce2\u7684\u7279\u5f81\u63d0\u53d6\u548c\u81ea\u9002\u5e94\u9891\u7387\u8c03\u5236\uff0c\u5b83\u80fd\u719f\u7ec3\u5730\u6355\u83b7\u548c\u5efa\u6a21\u4e0d\u540c\u3001\u591a\u5c3a\u5ea6\u7684\u65f6\u6001\u52a8\u6001\u3002", "result": "\u8be5\u6a21\u578b\u5728\u4e03\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5305\u62ec\u7535\u529b\u53d8\u538b\u5668\u6e29\u5ea6\uff08ETTh1\u3001ETTh2\u3001ETTm1\u3001ETTm2\uff09\u4ee5\u53ca\u5e9f\u7269\u3001\u592a\u9633\u80fd\u548c\u6c34\u529b\u53d1\u7535\uff08\u53ef\u518d\u751f\u80fd\u6e90\uff09\uff0c\u5728\u5404\u79cd\u9884\u6d4b\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6355\u6349\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4e03\u4e2a\u591a\u6837\u5316\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u5305\u62ec\u7535\u529b\u53d8\u538b\u5668\u6e29\u5ea6\uff08ETTh1\u3001ETTh2\u3001ETTm1\u3001ETTm2\uff09\u4ee5\u53ca\u5e9f\u7269\u3001\u592a\u9633\u80fd\u548c\u6c34\u529b\u53d1\u7535\uff08\u53ef\u518d\u751f\u80fd\u6e90\uff09\uff0c\u5728\u5404\u79cd\u9884\u6d4b\u6307\u6807\u4e0a\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u6355\u6349\u590d\u6742\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002\u6b64\u5916\uff0c\u901a\u8fc7SHAP\u5206\u6790\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u9002\u7528\u4e8e\u53ef\u6301\u7eed\u80fd\u6e90\u5e94\u7528\u3002"}}
{"id": "2507.10218", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10218", "abs": "https://arxiv.org/abs/2507.10218", "authors": ["Jimin Dai", "Jiexi Yan", "Jian Yang", "Lei Luo"], "title": "Straighten Viscous Rectified Flow via Noise Optimization", "comment": null, "summary": "The Reflow operation aims to straighten the inference trajectories of the\nrectified flow during training by constructing deterministic couplings between\nnoises and images, thereby improving the quality of generated images in\nsingle-step or few-step generation. However, we identify critical limitations\nin Reflow, particularly its inability to rapidly generate high-quality images\ndue to a distribution gap between images in its constructed deterministic\ncouplings and real images. To address these shortcomings, we propose a novel\nalternative called Straighten Viscous Rectified Flow via Noise Optimization\n(VRFNO), which is a joint training framework integrating an encoder and a\nneural velocity field. VRFNO introduces two key innovations: (1) a historical\nvelocity term that enhances trajectory distinction, enabling the model to more\naccurately predict the velocity of the current trajectory, and (2) the noise\noptimization through reparameterization to form optimized couplings with real\nimages which are then utilized for training, effectively mitigating errors\ncaused by Reflow's limitations. Comprehensive experiments on synthetic data and\nreal datasets with varying resolutions show that VRFNO significantly mitigates\nthe limitations of Reflow, achieving state-of-the-art performance in both\none-step and few-step generation tasks.", "AI": {"tldr": "VRFNO\u662f\u4e00\u79cd\u65b0\u7684\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdb\u63a8\u7406\u8f68\u8ff9\u548c\u4f18\u5316\u566a\u58f0\uff0c\u5728\u5355\u6b65\u548c\u5c11\u6b65\u56fe\u50cf\u751f\u6210\u65b9\u9762\u4f18\u4e8eReflow\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3Reflow\u5728\u5feb\u901f\u751f\u6210\u9ad8\u8d28\u91cf\u56fe\u50cf\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5176\u6784\u9020\u7684\u786e\u5b9a\u6027\u8026\u5408\u4e0e\u771f\u5b9e\u56fe\u50cf\u4e4b\u95f4\u5b58\u5728\u7684\u5206\u5e03\u5dee\u8ddd\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVRFNO\u7684\u8054\u5408\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u7f16\u7801\u5668\u548c\u795e\u7ecf\u901f\u5ea6\u573a\uff0c\u5e76\u901a\u8fc7\u5386\u53f2\u901f\u5ea6\u9879\u548c\u91cd\u53c2\u6570\u5316\u566a\u58f0\u4f18\u5316\u6765\u6539\u8fdb\u63a8\u7406\u8f68\u8ff9\u3002", "result": "VRFNO\u5728\u5408\u6210\u6570\u636e\u548c\u4e0d\u540c\u5206\u8fa8\u7387\u7684\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660eVRFNO\u663e\u8457\u51cf\u8f7b\u4e86Reflow\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5355\u6b65\u548c\u5c11\u6b65\u751f\u6210\u4efb\u52a1\u4e2d\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "VRFNO\u901a\u8fc7\u5f15\u5165\u5386\u53f2\u901f\u5ea6\u9879\u548c\u901a\u8fc7\u91cd\u53c2\u6570\u5316\u8fdb\u884c\u566a\u58f0\u4f18\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86Reflow\u7684\u5c40\u9650\u6027\uff0c\u5728\u5355\u6b65\u548c\u5c11\u6b65\u751f\u6210\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10222", "categories": ["cs.CV", "cs.LG", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10222", "abs": "https://arxiv.org/abs/2507.10222", "authors": ["Mingzhi Xu", "Yizhe Zhang"], "title": "Spatial Lifting for Dense Prediction", "comment": "Preprint. Under review", "summary": "We present Spatial Lifting (SL), a novel methodology for dense prediction\ntasks. SL operates by lifting standard inputs, such as 2D images, into a\nhigher-dimensional space and subsequently processing them using networks\ndesigned for that higher dimension, such as a 3D U-Net. Counterintuitively,\nthis dimensionality lifting allows us to achieve good performance on benchmark\ntasks compared to conventional approaches, while reducing inference costs and\nsignificantly lowering the number of model parameters. The SL framework\nproduces intrinsically structured outputs along the lifted dimension. This\nemergent structure facilitates dense supervision during training and enables\nrobust, near-zero-additional-cost prediction quality assessment at test time.\nWe validate our approach across 19 benchmark datasets (13 for semantic\nsegmentation and 6 for depth estimation), demonstrating competitive dense\nprediction performance while reducing the model parameter count by over 98% (in\nthe U-Net case) and lowering inference costs. Spatial Lifting introduces a new\nvision modeling paradigm that offers a promising path toward more efficient,\naccurate, and reliable deep networks for dense prediction tasks in vision.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10223", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10223", "abs": "https://arxiv.org/abs/2507.10223", "authors": ["Xiangyu Yin", "Boyuan Yang", "Weichen Liu", "Qiyao Xue", "Abrar Alamri", "Goeran Fiedler", "Wei Gao"], "title": "ProGait: A Multi-Purpose Video Dataset and Benchmark for Transfemoral Prosthesis Users", "comment": "Accepted by ICCV'25", "summary": "Prosthetic legs play a pivotal role in clinical rehabilitation, allowing\nindividuals with lower-limb amputations the ability to regain mobility and\nimprove their quality of life. Gait analysis is fundamental for optimizing\nprosthesis design and alignment, directly impacting the mobility and life\nquality of individuals with lower-limb amputations. Vision-based machine\nlearning (ML) methods offer a scalable and non-invasive solution to gait\nanalysis, but face challenges in correctly detecting and analyzing prosthesis,\ndue to their unique appearances and new movement patterns. In this paper, we\naim to bridge this gap by introducing a multi-purpose dataset, namely ProGait,\nto support multiple vision tasks including Video Object Segmentation, 2D Human\nPose Estimation, and Gait Analysis (GA). ProGait provides 412 video clips from\nfour above-knee amputees when testing multiple newly-fitted prosthetic legs\nthrough walking trials, and depicts the presence, contours, poses, and gait\npatterns of human subjects with transfemoral prosthetic legs. Alongside the\ndataset itself, we also present benchmark tasks and fine-tuned baseline models\nto illustrate the practical application and performance of the ProGait dataset.\nWe compared our baseline models against pre-trained vision models,\ndemonstrating improved generalizability when applying the ProGait dataset for\nprosthesis-specific tasks. Our code is available at\nhttps://github.com/pittisl/ProGait and dataset at\nhttps://huggingface.co/datasets/ericyxy98/ProGait.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 ProGait \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u6539\u8fdb\u5047\u80a2\u6b65\u6001\u5206\u6790\u7684\u89c6\u89c9\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u4f18\u5316\u5047\u80a2\u8bbe\u8ba1\u548c\u5bf9\u9f50\uff0c\u63d0\u9ad8\u622a\u80a2\u60a3\u8005\u7684\u884c\u52a8\u80fd\u529b\u548c\u751f\u6d3b\u8d28\u91cf\uff0c\u9700\u8981\u5bf9\u6b65\u6001\u8fdb\u884c\u5206\u6790\u3002\u73b0\u6709\u7684\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5728\u5904\u7406\u5047\u80a2\u72ec\u7279\u5916\u89c2\u548c\u8fd0\u52a8\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e13\u95e8\u7684\u6570\u636e\u96c6\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u89c6\u89c9\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a ProGait \u7684\u591a\u529f\u80fd\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b412\u4e2a\u89c6\u9891\u7247\u6bb5\uff0c\u6db5\u76d6\u4e86\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u30012D\u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u6b65\u6001\u5206\u6790\u3002\u901a\u8fc7\u6bd4\u8f83\u5fae\u8c03\u540e\u7684\u57fa\u7ebf\u6a21\u578b\u548c\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5728\u5047\u80a2\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "result": "\u57fa\u7ebf\u6a21\u578b\u5728\u5e94\u7528 ProGait \u6570\u636e\u96c6\u8fdb\u884c\u5047\u80a2\u7279\u5b9a\u4efb\u52a1\u65f6\uff0c\u76f8\u6bd4\u4e8e\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u6a21\u578b\uff0c\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3a ProGait \u7684\u591a\u529f\u80fd\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u652f\u6301\u89c6\u9891\u5bf9\u8c61\u5206\u5272\u30012D \u4eba\u4f53\u59ff\u6001\u4f30\u8ba1\u548c\u6b65\u6001\u5206\u6790\uff08GA\uff09\u7b49\u591a\u79cd\u89c6\u89c9\u4efb\u52a1\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u68c0\u6d4b\u548c\u5206\u6790\u5047\u80a2\u65b9\u9762\u7684\u6311\u6218\u3002\u901a\u8fc7\u5728\u56db\u540d\u819d\u76d6\u4ee5\u4e0a\u622a\u80a2\u8005\u884c\u8d70\u8bd5\u9a8c\u4e2d\u6536\u96c6\u89c6\u9891\u7247\u6bb5\uff0c\u5e76\u63d0\u4f9b\u57fa\u51c6\u4efb\u52a1\u548c\u7ecf\u8fc7\u5fae\u8c03\u7684\u57fa\u7ebf\u6a21\u578b\uff0c\u8bc1\u660e\u4e86 ProGait \u6570\u636e\u96c6\u5728\u5047\u80a2\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u6539\u8fdb\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10170", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10170", "abs": "https://arxiv.org/abs/2507.10170", "authors": ["Wuyang Zhou", "Giorgos Iacovides", "Kriton Konstantinidis", "Ilya Kisil", "Danilo Mandic"], "title": "Understanding the Rank of Tensor Networks via an Intuitive Example-Driven Approach", "comment": null, "summary": "Tensor Network (TN) decompositions have emerged as an indispensable tool in\nBig Data analytics owing to their ability to provide compact low-rank\nrepresentations, thus alleviating the ``Curse of Dimensionality'' inherent in\nhandling higher-order data. At the heart of their success lies the concept of\nTN ranks, which governs the efficiency and expressivity of TN decompositions.\nHowever, unlike matrix ranks, TN ranks often lack a universal meaning and an\nintuitive interpretation, with their properties varying significantly across\ndifferent TN structures. Consequently, TN ranks are frequently treated as\nempirically tuned hyperparameters, rather than as key design parameters\ninferred from domain knowledge. The aim of this Lecture Note is therefore to\ndemystify the foundational yet frequently misunderstood concept of TN ranks\nthrough real-life examples and intuitive visualizations. We begin by\nillustrating how domain knowledge can guide the selection of TN ranks in\nwidely-used models such as the Canonical Polyadic (CP) and Tucker\ndecompositions. For more complex TN structures, we employ a self-explanatory\ngraphical approach that generalizes to tensors of arbitrary order. Such a\nperspective naturally reveals the relationship between TN ranks and the\ncorresponding ranks of tensor unfoldings (matrices), thereby circumventing\ncumbersome multi-index tensor algebra while facilitating domain-informed TN\ndesign. It is our hope that this Lecture Note will equip readers with a clear\nand unified understanding of the concept of TN rank, along with the necessary\nphysical insight and intuition to support the selection, explainability, and\ndeployment of tensor methods in both practical applications and educational\ncontexts.", "AI": {"tldr": "\u5f20\u91cf\u79e9\u5bf9\u4e8e\u5f20\u91cf\u7f51\u7edc\u5206\u89e3\u5f88\u91cd\u8981\uff0c\u4f46\u5f88\u96be\u7406\u89e3\u3002\u672c\u8bb2\u4e49\u901a\u8fc7\u793a\u4f8b\u548c\u56fe\u5f62\u89e3\u91ca\u4e86\u5f20\u91cf\u79e9\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u4f7f\u7528\u9886\u57df\u77e5\u8bc6\u6765\u9009\u62e9\u5b83\u4eec\u3002\u8fd9\u5e94\u8be5\u6709\u52a9\u4e8e\u4eba\u4eec\u66f4\u597d\u5730\u7406\u89e3\u548c\u4f7f\u7528\u5f20\u91cf\u65b9\u6cd5\u3002", "motivation": "\u5f20\u91cf\u79e9\u5728\u5f20\u91cf\u7f51\u7edc\u5206\u89e3\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u666e\u904d\u7684\u610f\u4e49\u548c\u76f4\u89c2\u7684\u89e3\u91ca\uff0c\u5bfc\u81f4\u5176\u5e38\u88ab\u89c6\u4e3a\u7ecf\u9a8c\u8c03\u6574\u7684\u8d85\u53c2\u6570\u3002\u672c\u8bb2\u4e49\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u63d0\u4f9b\u6e05\u6670\u7684\u7406\u89e3\u3002", "method": "\u901a\u8fc7\u5b9e\u9645\u793a\u4f8b\u548c\u76f4\u89c2\u53ef\u89c6\u5316\u6765\u8bf4\u660e\u5f20\u91cf\u79e9\uff0c\u5c55\u793a\u9886\u57df\u77e5\u8bc6\u5982\u4f55\u6307\u5bfc CP \u548c Tucker \u7b49\u6a21\u578b\u4e2d\u5f20\u91cf\u79e9\u7684\u9009\u62e9\uff0c\u5e76\u4f7f\u7528\u56fe\u5f62\u65b9\u6cd5\u63a8\u5e7f\u5230\u4efb\u610f\u987a\u5e8f\u7684\u5f20\u91cf\uff0c\u63ed\u793a\u5f20\u91cf\u79e9\u4e0e\u5f20\u91cf\u5c55\u5f00\u77e9\u9635\u79e9\u7684\u5173\u7cfb\u3002", "result": "\u901a\u8fc7\u76f4\u89c2\u7684\u89e3\u91ca\u548c\u56fe\u5f62\u65b9\u6cd5\uff0c\u5e2e\u52a9\u8bfb\u8005\u7406\u89e3\u5f20\u91cf\u79e9\u7684\u6982\u5ff5\uff0c\u5e76\u80fd\u591f\u6839\u636e\u9886\u57df\u77e5\u8bc6\u8fdb\u884c\u9009\u62e9\u548c\u89e3\u91ca\u3002\u6709\u52a9\u4e8e\u5f20\u91cf\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u548c\u6559\u80b2\u4e2d\u7684\u90e8\u7f72\u3002\u88ab\u89c6\u4e3a\u201c\u957f\u8bdd\u77ed\u8bf4\u201d\u6458\u8981\u3002", "conclusion": "\u672c\u8bb2\u4e49\u65e8\u5728\u63ed\u5f00\u5f20\u91cf\u79e9\u7684\u795e\u79d8\u9762\u7eb1\uff0c\u63d0\u4f9b\u6e05\u6670\u7edf\u4e00\u7684\u7406\u89e3\uff0c\u5e76\u4e3a\u5728\u5b9e\u9645\u5e94\u7528\u548c\u6559\u80b2\u80cc\u666f\u4e0b\u9009\u62e9\u3001\u89e3\u91ca\u548c\u90e8\u7f72\u5f20\u91cf\u65b9\u6cd5\u63d0\u4f9b\u5fc5\u8981\u7684\u7269\u7406\u89c1\u89e3\u548c\u76f4\u89c9\u3002"}}
{"id": "2507.10225", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10225", "abs": "https://arxiv.org/abs/2507.10225", "authors": ["Jinglun Li", "Kaixun Jiang", "Zhaoyu Chen", "Bo Lin", "Yao Tang", "Weifeng Ge", "Wenqiang Zhang"], "title": "Synthesizing Near-Boundary OOD Samples for Out-of-Distribution Detection", "comment": null, "summary": "Pre-trained vision-language models have exhibited remarkable abilities in\ndetecting out-of-distribution (OOD) samples. However, some challenging OOD\nsamples, which lie close to in-distribution (InD) data in image feature space,\ncan still lead to misclassification. The emergence of foundation models like\ndiffusion models and multimodal large language models (MLLMs) offers a\npotential solution to this issue. In this work, we propose SynOOD, a novel\napproach that harnesses foundation models to generate synthetic, challenging\nOOD data for fine-tuning CLIP models, thereby enhancing boundary-level\ndiscrimination between InD and OOD samples. Our method uses an iterative\nin-painting process guided by contextual prompts from MLLMs to produce nuanced,\nboundary-aligned OOD samples. These samples are refined through noise\nadjustments based on gradients from OOD scores like the energy score,\neffectively sampling from the InD/OOD boundary. With these carefully\nsynthesized images, we fine-tune the CLIP image encoder and negative label\nfeatures derived from the text encoder to strengthen connections between\nnear-boundary OOD samples and a set of negative labels. Finally, SynOOD\nachieves state-of-the-art performance on the large-scale ImageNet benchmark,\nwith minimal increases in parameters and runtime. Our approach significantly\nsurpasses existing methods, improving AUROC by 2.80% and reducing FPR95 by\n11.13%. Codes are available in https://github.com/Jarvisgivemeasuit/SynOOD.", "AI": {"tldr": "SynOOD \u5229\u7528\u57fa\u7840\u6a21\u578b\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684 OOD \u6837\u672c\uff0c\u4ee5\u589e\u5f3a CLIP \u6a21\u578b\u533a\u5206 InD \u548c OOD \u6837\u672c\u7684\u80fd\u529b\uff0c\u5c24\u5176\u662f\u5728\u8fb9\u754c\u9644\u8fd1\uff0c\u5e76\u5728 ImageNet \u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u68c0\u6d4b OOD \u6837\u672c\u65b9\u9762\u80fd\u529b\u663e\u8457\uff0c\u4f46\u4ecd\u96be\u4ee5\u533a\u5206\u63a5\u8fd1 InD \u6570\u636e\u7684 OOD \u6837\u672c\uff1b\u5229\u7528\u6269\u6563\u6a21\u578b\u548c MLLMs \u7b49\u57fa\u7840\u6a21\u578b\u4e3a\u89e3\u51b3\u6b64\u95ee\u9898\u63d0\u4f9b\u4e86\u6f5c\u5728\u65b9\u6848\u3002", "method": "SynOOD \u65b9\u6cd5\u5229\u7528 MLLMs \u7684\u4e0a\u4e0b\u6587\u63d0\u793a\u548c\u80fd\u91cf\u5206\u6570\u7b49 OOD \u5206\u6570\u7684\u68af\u5ea6\uff0c\u901a\u8fc7\u8fed\u4ee3\u4fee\u590d\u8fc7\u7a0b\u751f\u6210\u5728\u8fb9\u754c\u5bf9\u9f50\u7684 OOD \u6837\u672c\uff0c\u5e76\u5bf9 CLIP \u7684\u56fe\u50cf\u7f16\u7801\u5668\u548c\u8d1f\u9762\u6807\u7b7e\u7279\u5f81\u8fdb\u884c\u5fae\u8c03\u3002", "result": "SynOOD \u5728 ImageNet \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cAUROC \u63d0\u9ad8\u4e86 2.80%\uff0cFPR95 \u964d\u4f4e\u4e86 11.13%\u3002", "conclusion": "SynOOD \u901a\u8fc7\u751f\u6210\u5177\u6709\u6311\u6218\u6027\u7684 OOD \u6837\u672c\u6765\u5fae\u8c03 CLIP \u6a21\u578b\uff0c\u4ece\u800c\u63d0\u9ad8\u8fb9\u754c\u5224\u522b\u80fd\u529b\uff0c\u5e76\u5728 ImageNet \u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u53c2\u6570\u548c\u8fd0\u884c\u65f6\u95f4\u589e\u52a0\u6781\u5c11\u3002"}}
{"id": "2507.10172", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10172", "abs": "https://arxiv.org/abs/2507.10172", "authors": ["Ruizhe Yu Xia", "Jeremy Gow", "Simon Lucas"], "title": "Play Style Identification Using Low-Level Representations of Play Traces in MicroRTS", "comment": "Accepted as Short Paper for IEEE CoG", "summary": "Play style identification can provide valuable game design insights and\nenable adaptive experiences, with the potential to improve game playing agents.\nPrevious work relies on domain knowledge to construct play trace\nrepresentations using handcrafted features. More recent approaches incorporate\nthe sequential structure of play traces but still require some level of domain\nabstraction. In this study, we explore the use of unsupervised CNN-LSTM\nautoencoder models to obtain latent representations directly from low-level\nplay trace data in MicroRTS. We demonstrate that this approach yields a\nmeaningful separation of different game playing agents in the latent space,\nreducing reliance on domain expertise and its associated biases. This latent\nspace is then used to guide the exploration of diverse play styles within\nstudied AI players.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528\u65e0\u76d1\u7763CNN-LSTM\u81ea\u7f16\u7801\u5668\u4ece\u4f4e\u7ea7\u6e38\u620f\u6570\u636e\u4e2d\u5b66\u4e60\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u8bc6\u522b\u6e38\u620f\u98ce\u683c\uff0c\u4ece\u800c\u51cf\u5c11\u5bf9\u4eba\u5de5\u7279\u5f81\u548c\u9886\u57df\u77e5\u8bc6\u7684\u4f9d\u8d56\u3002", "motivation": "\u6e38\u620f\u98ce\u683c\u8bc6\u522b\u53ef\u4ee5\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u6e38\u620f\u8bbe\u8ba1\u89c1\u89e3\u5e76\u5b9e\u73b0\u81ea\u9002\u5e94\u4f53\u9a8c\uff0c\u6709\u6f5c\u529b\u6539\u8fdb\u6e38\u620fAI\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4f7f\u7528\u65e0\u76d1\u7763CNN-LSTM\u81ea\u7f16\u7801\u5668\u6a21\u578b\u76f4\u63a5\u4eceMicroRTS\u4e2d\u7684\u4f4e\u7ea7\u6e38\u620f\u75d5\u8ff9\u6570\u636e\u4e2d\u83b7\u53d6\u6f5c\u5728\u8868\u793a\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u80fd\u591f\u6709\u6548\u5730\u533a\u5206\u4e0d\u540c\u7684\u6e38\u620fAI\u73a9\u5bb6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u51cf\u5c11\u4e86\u5bf9\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u53ca\u5176\u76f8\u5173\u504f\u89c1\u7684\u4f9d\u8d56\uff0c\u5e76\u53ef\u7528\u4e8e\u6307\u5bfc\u5bf9\u6240\u7814\u7a76AI\u73a9\u5bb6\u4e2d\u4e0d\u540c\u6e38\u620f\u98ce\u683c\u7684\u63a2\u7d22\u3002"}}
{"id": "2507.10236", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10236", "abs": "https://arxiv.org/abs/2507.10236", "authors": ["Despina Konstantinidou", "Dimitrios Karageorgiou", "Christos Koutlis", "Olga Papadopoulou", "Emmanouil Schinas", "Symeon Papadopoulos"], "title": "Navigating the Challenges of AI-Generated Image Detection in the Wild: What Truly Matters?", "comment": "35 pages, 4 figures", "summary": "The rapid advancement of generative technologies presents both unprecedented\ncreative opportunities and significant challenges, particularly in maintaining\nsocial trust and ensuring the integrity of digital information. Following these\nconcerns, the challenge of AI-Generated Image Detection (AID) becomes\nincreasingly critical. As these technologies become more sophisticated, the\nquality of AI-generated images has reached a level that can easily deceive even\nthe most discerning observers. Our systematic evaluation highlights a critical\nweakness in current AI-Generated Image Detection models: while they perform\nexceptionally well on controlled benchmark datasets, they struggle\nsignificantly with real-world variations. To assess this, we introduce ITW-SM,\na new dataset of real and AI-generated images collected from major social media\nplatforms. In this paper, we identify four key factors that influence AID\nperformance in real-world scenarios: backbone architecture, training data\ncomposition, pre-processing strategies and data augmentation combinations. By\nsystematically analyzing these components, we shed light on their impact on\ndetection efficacy. Our modifications result in an average AUC improvement of\n26.87% across various AID models under real-world conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faITW-SM\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u5173\u952e\u56e0\u7d20\uff08\u9aa8\u5e72\u7f51\u7edc\u3001\u8bad\u7ec3\u6570\u636e\u3001\u9884\u5904\u7406\u3001\u6570\u636e\u589e\u5f3a\uff09\u663e\u8457\u63d0\u5347\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e0bAI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0fAI\u6280\u672f\u7684\u98de\u901f\u53d1\u5c55\uff0cAI\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u65e5\u76ca\u63d0\u9ad8\uff0c\u8db3\u4ee5\u4ee5\u5047\u4e71\u771f\uff0c\u8fd9\u4e0d\u4ec5\u5e26\u6765\u4e86\u521b\u4f5c\u7684\u65b0\u673a\u9047\uff0c\u4e5f\u5bf9\u793e\u4f1a\u4fe1\u4efb\u548c\u6570\u5b57\u4fe1\u606f\u5b8c\u6574\u6027\u6784\u6210\u4e86\u4e25\u5cfb\u6311\u6218\u3002\u56e0\u6b64\uff0c\u63d0\u9ad8AI\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\uff08AID\uff09\u80fd\u529b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4ee5\u5e94\u5bf9\u8fd9\u4e9b\u6f5c\u5728\u7684\u98ce\u9669\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u5f53\u524d\u4eba\u5de5\u667a\u80fd\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\uff08AID\uff09\u6a21\u578b\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e0a\u7684\u8868\u73b0\uff0c\u8bc6\u522b\u51fa\u5176\u5728\u771f\u5b9e\u4e16\u754c\u53d8\u5316\u9762\u524d\u7684\u6027\u80fd\u77ed\u677f\u3002\u7136\u540e\uff0c\u7814\u7a76\u8005\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u540d\u4e3aITW-SM\u7684\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u4e86\u6765\u81ea\u4e3b\u6d41\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u7684\u771f\u5b9e\u548cAI\u751f\u6210\u56fe\u50cf\u3002\u901a\u8fc7\u5bf9\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u6784\u6210\u3001\u9884\u5904\u7406\u7b56\u7565\u4ee5\u53ca\u6570\u636e\u589e\u5f3a\u7ec4\u5408\u8fd9\u56db\u4e2a\u5173\u952e\u56e0\u7d20\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\u548c\u5b9e\u9a8c\u8c03\u6574\uff0c\u4ee5\u4f18\u5316\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u9aa8\u5e72\u7f51\u7edc\u67b6\u6784\u3001\u8bad\u7ec3\u6570\u636e\u6784\u6210\u3001\u9884\u5904\u7406\u7b56\u7565\u548c\u6570\u636e\u589e\u5f3a\u7ec4\u5408\u662f\u5f71\u54cd\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2dAID\u6027\u80fd\u7684\u56db\u4e2a\u5173\u952e\u56e0\u7d20\u3002\u901a\u8fc7\u5bf9\u8fd9\u4e9b\u56e0\u7d20\u8fdb\u884c\u4f18\u5316\u548c\u8c03\u6574\uff0c\u7814\u7a76\u8005\u4eec\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0b\uff0c\u9488\u5bf9\u4e0d\u540c\u7684AID\u6a21\u578b\uff0c\u5e73\u5747AUC\uff08Area Under the Curve\uff09\u53d6\u5f97\u4e8626.87%\u7684\u63d0\u5347\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u8bc6\u522b\u5f71\u54cd\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u4eba\u5de5\u667a\u80fd\u751f\u6210\u56fe\u50cf\u68c0\u6d4b\uff08AID\uff09\u6027\u80fd\u7684\u5173\u952e\u56e0\u7d20\uff0c\u5e76\u8fdb\u884c\u76f8\u5e94\u7684\u6a21\u578b\u4f18\u5316\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u771f\u5b9e\u4e16\u754c\u6761\u4ef6\u4e0bAID\u6a21\u578b\u7684\u5e73\u5747AUC\u7ea626.87%\uff0c\u4e3a\u89e3\u51b3AI\u751f\u6210\u5185\u5bb9\u5e26\u6765\u7684\u4fe1\u4efb\u548c\u771f\u5b9e\u6027\u6311\u6218\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10183", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10183", "abs": "https://arxiv.org/abs/2507.10183", "authors": ["Alireza Dizaji", "Benedict Aaron Tjandra", "Mehrab Hamidi", "Shenyang Huang", "Guillaume Rabusseau"], "title": "T-GRAB: A Synthetic Diagnostic Benchmark for Learning on Temporal Graphs", "comment": "Accepted to MLoG-GenAI Workshop @ KDD 2025 (Oral)", "summary": "Dynamic graph learning methods have recently emerged as powerful tools for\nmodelling relational data evolving through time. However, despite extensive\nbenchmarking efforts, it remains unclear whether current Temporal Graph Neural\nNetworks (TGNNs) effectively capture core temporal patterns such as\nperiodicity, cause-and-effect, and long-range dependencies. In this work, we\nintroduce the Temporal Graph Reasoning Benchmark (T-GRAB), a comprehensive set\nof synthetic tasks designed to systematically probe the capabilities of TGNNs\nto reason across time. T-GRAB provides controlled, interpretable tasks that\nisolate key temporal skills: counting/memorizing periodic repetitions,\ninferring delayed causal effects, and capturing long-range dependencies over\nboth spatial and temporal dimensions. We evaluate 11 temporal graph learning\nmethods on these tasks, revealing fundamental shortcomings in their ability to\ngeneralize temporal patterns. Our findings offer actionable insights into the\nlimitations of current models, highlight challenges hidden by traditional\nreal-world benchmarks, and motivate the development of architectures with\nstronger temporal reasoning abilities. The code for T-GRAB can be found at:\nhttps://github.com/alirezadizaji/T-GRAB.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 T-GRAB \u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5f53\u524d TGNN \u5728\u5904\u7406\u65f6\u95f4\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u547c\u5401\u5f00\u53d1\u66f4\u5f3a\u7684\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u52a8\u6001\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u6a21\u62df\u968f\u65f6\u95f4\u6f14\u53d8\u7684\u5173\u7cfb\u6570\u636e\u65b9\u9762\u5df2\u663e\u793a\u51fa\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u5f53\u524d\u7684\u65f6\u95f4\u56fe\u795e\u7ecf\u7f51\u7edc\uff08TGNNs\uff09\u5728\u6709\u6548\u6355\u6349\u5468\u671f\u6027\u3001\u56e0\u679c\u5173\u7cfb\u548c\u957f\u671f\u4f9d\u8d56\u6027\u7b49\u6838\u5fc3\u65f6\u95f4\u6a21\u5f0f\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u786e\u5b9a\u6027\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u65f6\u95f4\u56fe\u63a8\u7406\u57fa\u51c6\uff08T-GRAB\uff09\uff0c\u8fd9\u662f\u4e00\u5957\u7efc\u5408\u6027\u7684\u5408\u6210\u4efb\u52a1\uff0c\u65e8\u5728\u7cfb\u7edf\u5730\u63a2\u7a76 TGNN \u8de8\u65f6\u95f4\u63a8\u7406\u7684\u80fd\u529b\u3002T-GRAB \u63d0\u4f9b\u4e86\u53d7\u63a7\u7684\u3001\u53ef\u89e3\u91ca\u7684\u4efb\u52a1\uff0c\u7528\u4e8e\u9694\u79bb\u5173\u952e\u7684\u65f6\u95f4\u6280\u80fd\uff1a\u8ba1\u7b97/\u8bb0\u5fc6\u5468\u671f\u6027\u91cd\u590d\u3001\u63a8\u65ad\u5ef6\u8fdf\u56e0\u679c\u6548\u5e94\u4ee5\u53ca\u6355\u6349\u7a7a\u95f4\u548c\u65f6\u95f4\u7ef4\u5ea6\u4e0a\u7684\u957f\u671f\u4f9d\u8d56\u6027\u3002\u7814\u7a76\u8bc4\u4f30\u4e86 11 \u79cd\u65f6\u95f4\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728 T-GRAB \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c11 \u79cd\u65f6\u95f4\u56fe\u5b66\u4e60\u65b9\u6cd5\u5728\u6cdb\u5316\u65f6\u95f4\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u65f6\u95f4\u56fe\u795e\u7ecf\u7f51\u7edc\uff08TGNN\uff09\u5728\u6355\u6349\u5468\u671f\u6027\u3001\u56e0\u679c\u5173\u7cfb\u548c\u957f\u671f\u4f9d\u8d56\u6027\u7b49\u6838\u5fc3\u65f6\u95f4\u6a21\u5f0f\u65b9\u9762\u5b58\u5728\u6839\u672c\u6027\u4e0d\u8db3\uff0c\u5e76\u6307\u51fa\u4e86\u4f20\u7edf\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u9690\u85cf\u7684\u6311\u6218\uff0c\u6fc0\u53d1\u4e86\u5f00\u53d1\u5177\u6709\u66f4\u5f3a\u65f6\u95f4\u63a8\u7406\u80fd\u529b\u7684\u67b6\u6784\u3002"}}
{"id": "2507.10239", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10239", "abs": "https://arxiv.org/abs/2507.10239", "authors": ["Ben Hamscher", "Edgar Heinert", "Annika M\u00fctze", "Kira Maag", "Matthias Rottmann"], "title": "Transferring Styles for Reduced Texture Bias and Improved Robustness in Semantic Segmentation Networks", "comment": "accepted at ECAI 2025", "summary": "Recent research has investigated the shape and texture biases of deep neural\nnetworks (DNNs) in image classification which influence their generalization\ncapabilities and robustness. It has been shown that, in comparison to regular\nDNN training, training with stylized images reduces texture biases in image\nclassification and improves robustness with respect to image corruptions. In an\neffort to advance this line of research, we examine whether style transfer can\nlikewise deliver these two effects in semantic segmentation. To this end, we\nperform style transfer with style varying across artificial image areas. Those\nrandom areas are formed by a chosen number of Voronoi cells. The resulting\nstyle-transferred data is then used to train semantic segmentation DNNs with\nthe objective of reducing their dependence on texture cues while enhancing\ntheir reliance on shape-based features. In our experiments, it turns out that\nin semantic segmentation, style transfer augmentation reduces texture bias and\nstrongly increases robustness with respect to common image corruptions as well\nas adversarial attacks. These observations hold for convolutional neural\nnetworks and transformer architectures on the Cityscapes dataset as well as on\nPASCAL Context, showing the generality of the proposed method.", "AI": {"tldr": "\u98ce\u683c\u8fc1\u79fb\u589e\u5f3a\u53ef\u7528\u4e8e\u8bed\u4e49\u5206\u5272\uff0c\u4ee5\u51cf\u5c11\u7eb9\u7406\u504f\u5dee\u5e76\u63d0\u9ad8\u5bf9\u56fe\u50cf\u635f\u574f\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u7814\u7a76\u98ce\u683c\u8fc1\u79fb\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\u662f\u5426\u4e5f\u80fd\u50cf\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u4e00\u6837\uff0c\u51cf\u5c11\u7eb9\u7406\u504f\u5dee\u5e76\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "method": "\u901a\u8fc7\u5728\u56fe\u50cf\u533a\u57df\u5e94\u7528\u98ce\u683c\u8fc1\u79fb\uff08\u4f7f\u7528 Voronoi \u5355\u5143\u5212\u5206\u7684\u968f\u673a\u533a\u57df\uff09\u6765\u751f\u6210\u98ce\u683c\u8fc1\u79fb\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u8fd9\u4e9b\u6570\u636e\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u5305\u62ec\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c Transformer \u67b6\u6784\uff09\uff0c\u4ee5\u51cf\u5c11\u5bf9\u7eb9\u7406\u7ebf\u7d22\u7684\u4f9d\u8d56\u5e76\u589e\u5f3a\u5bf9\u57fa\u4e8e\u5f62\u72b6\u7684\u7279\u5f81\u7684\u4f9d\u8d56\u3002", "result": "\u98ce\u683c\u8fc1\u79fb\u589e\u5f3a\u80fd\u591f\u6709\u6548\u51cf\u5c11\u8bed\u4e49\u5206\u5272\u7684\u7eb9\u7406\u504f\u5dee\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u6a21\u578b\u5728 Cityscapes \u548c PASCAL Context \u6570\u636e\u96c6\u4e0a\u7684\u9c81\u68d2\u6027\uff0c\u5305\u62ec\u5bf9\u5e38\u89c1\u56fe\u50cf\u635f\u574f\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u8bed\u4e49\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u98ce\u683c\u8fc1\u79fb\u589e\u5f3a\u53ef\u4ee5\u51cf\u5c11\u7eb9\u7406\u504f\u5dee\u5e76\u663e\u8457\u63d0\u9ad8\u5bf9\u5e38\u89c1\u56fe\u50cf\u635f\u574f\u548c\u5bf9\u6297\u6027\u653b\u51fb\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10194", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10194", "abs": "https://arxiv.org/abs/2507.10194", "authors": ["Tassilo Klein", "Moin Nabi"], "title": "Learning Private Representations through Entropy-based Adversarial Training", "comment": null, "summary": "How can we learn a representation with high predictive power while preserving\nuser privacy? We present an adversarial representation learning method for\nsanitizing sensitive content from the learned representation. Specifically, we\nintroduce a variant of entropy - focal entropy, which mitigates the potential\ninformation leakage of the existing entropy-based approaches. We showcase\nfeasibility on multiple benchmarks. The results suggest high target utility at\nmoderate privacy leakage.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7126\u70b9\u71b5\u7684\u5bf9\u6297\u6027\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u5b66\u4e60\u5177\u6709\u9ad8\u9884\u6d4b\u80fd\u529b\u7684\u8868\u793a\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9ad8\u76ee\u6807\u6548\u7528\u548c\u4e2d\u7b49\u9690\u79c1\u6cc4\u9732\u7684\u6210\u679c\u3002", "motivation": "\u5982\u4f55\u5728\u4fdd\u62a4\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u5b66\u4e60\u5177\u6709\u9ad8\u9884\u6d4b\u80fd\u529b\u7684\u8868\u793a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u6297\u6027\u8868\u793a\u5b66\u4e60\u65b9\u6cd5\u6765\u51c0\u5316\u8868\u793a\u4e2d\u654f\u611f\u7684\u5185\u5bb9\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u71b5\u7684\u53d8\u4f53\u2014\u2014\u7126\u70b9\u71b5\uff0c\u4ee5\u51cf\u5c11\u73b0\u6709\u57fa\u4e8e\u71b5\u7684\u65b9\u6cd5\u7684\u6f5c\u5728\u4fe1\u606f\u6cc4\u9732\u3002", "result": "\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u793a\u4e86\u53ef\u884c\u6027\uff0c\u7ed3\u679c\u8868\u660e\u5728\u4e2d\u7b49\u9690\u79c1\u6cc4\u9732\u7684\u60c5\u51b5\u4e0b\uff0c\u76ee\u6807\u6548\u7528\u5f88\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7528\u6237\u9690\u79c1\u7684\u540c\u65f6\u5b66\u4e60\u5230\u4e86\u5177\u6709\u9ad8\u9884\u6d4b\u80fd\u529b\u7684\u8868\u793a\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4e2d\u7b49\u9690\u79c1\u6cc4\u9732\u7684\u60c5\u51b5\u4e0b\uff0c\u76ee\u6807\u6548\u7528\u5f88\u9ad8\u3002"}}
{"id": "2507.10265", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10265", "abs": "https://arxiv.org/abs/2507.10265", "authors": ["Xinlong Ding", "Hongwei Yu", "Jiawei Li", "Feifan Li", "Yu Shang", "Bochao Zou", "Huimin Ma", "Jiansheng Chen"], "title": "Kaleidoscopic Background Attack: Disrupting Pose Estimation with Multi-Fold Radial Symmetry Textures", "comment": "Accepted at ICCV 2025. Project page is available at\n  https://wakuwu.github.io/KBA", "summary": "Camera pose estimation is a fundamental computer vision task that is\nessential for applications like visual localization and multi-view stereo\nreconstruction. In the object-centric scenarios with sparse inputs, the\naccuracy of pose estimation can be significantly influenced by background\ntextures that occupy major portions of the images across different viewpoints.\nIn light of this, we introduce the Kaleidoscopic Background Attack (KBA), which\nuses identical segments to form discs with multi-fold radial symmetry. These\ndiscs maintain high similarity across different viewpoints, enabling effective\nattacks on pose estimation models even with natural texture segments.\nAdditionally, a projected orientation consistency loss is proposed to optimize\nthe kaleidoscopic segments, leading to significant enhancement in the attack\neffectiveness. Experimental results show that optimized adversarial\nkaleidoscopic backgrounds can effectively attack various camera pose estimation\nmodels.", "AI": {"tldr": "\u5728\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u573a\u666f\u4e2d\uff0c\u80cc\u666f\u7eb9\u7406\u4f1a\u5f71\u54cd\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e07\u82b1\u7b52\u80cc\u666f\u653b\u51fb\uff08KBA\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5177\u6709\u5f84\u5411\u5bf9\u79f0\u6027\u7684\u5706\u76d8\u6765\u653b\u51fb\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u3002\u901a\u8fc7\u4f18\u5316\u7247\u6bb5\u4ee5\u4fdd\u6301\u65b9\u5411\u4e00\u81f4\u6027\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u63d0\u9ad8\u653b\u51fb\u6548\u679c\uff0c\u5e76\u80fd\u6709\u6548\u653b\u51fb\u591a\u79cd\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u3002", "motivation": "\u5728\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u7a00\u758f\u8f93\u5165\u573a\u666f\u4e2d\uff0c\u80cc\u666f\u7eb9\u7406\u4f1a\u5f71\u54cd\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u79cd\u6709\u6548\u7684\u65b9\u6cd5\u6765\u5904\u7406\u8fd9\u79cd\u5f71\u54cd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u4e07\u82b1\u7b52\u80cc\u666f\u653b\u51fb\uff08KBA\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u76f8\u540c\u7684\u7247\u6bb5\u5f62\u6210\u5177\u6709\u591a\u91cd\u5f84\u5411\u5bf9\u79f0\u6027\u7684\u5706\u76d8\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6295\u5f71\u65b9\u5411\u4e00\u81f4\u6027\u635f\u5931\u6765\u4f18\u5316\u4e07\u82b1\u7b52\u72b6\u7247\u6bb5\uff0c\u4ee5\u589e\u5f3a\u653b\u51fb\u6548\u679c\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u7684\u4e07\u82b1\u7b52\u72b6\u5bf9\u6297\u6027\u80cc\u666f\u53ef\u4ee5\u6709\u6548\u5730\u653b\u51fb\u5404\u79cd\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f18\u5316\u540e\u7684\u4e07\u82b1\u7b52\u72b6\u5bf9\u6297\u6027\u80cc\u666f\u80fd\u591f\u6709\u6548\u5730\u653b\u51fb\u5404\u79cd\u76f8\u673a\u59ff\u6001\u4f30\u8ba1\u6a21\u578b\u3002"}}
{"id": "2507.10215", "categories": ["cs.LG", "stat.AP"], "pdf": "https://arxiv.org/pdf/2507.10215", "abs": "https://arxiv.org/abs/2507.10215", "authors": ["Cencheng Shen", "Yuexiao Dong"], "title": "A Graph Sufficiency Perspective for Neural Networks", "comment": "23 pages", "summary": "This paper analyzes neural networks through graph variables and statistical\nsufficiency. We interpret neural network layers as graph-based transformations,\nwhere neurons act as pairwise functions between inputs and learned anchor\npoints. Within this formulation, we establish conditions under which layer\noutputs are sufficient for the layer inputs, that is, each layer preserves the\nconditional distribution of the target variable given the input variable. Under\ndense anchor point assumptions, we prove that asymptotic sufficiency holds in\nthe infinite-width limit and is preserved throughout training. To align more\nclosely with practical architectures, we further show that sufficiency can be\nachieved with finite-width networks by assuming region-separated input\ndistributions and constructing appropriate anchor points. Our framework covers\nfully connected layers, general pairwise functions, ReLU and sigmoid\nactivations, and convolutional neural networks. This work bridges statistical\nsufficiency, graph-theoretic representations, and deep learning, providing a\nnew statistical understanding of neural networks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u5206\u6790\u6846\u67b6\uff0c\u5c06\u5c42\u89c6\u4e3a\u56fe\u53d8\u6362\uff0c\u795e\u7ecf\u5143\u4f5c\u4e3a\u6210\u5bf9\u51fd\u6570\u3002\u8bc1\u660e\u4e86\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff08\u65e0\u9650\u5bbd\u5ea6\u6216\u533a\u57df\u5206\u79bb\u8f93\u5165\uff09\uff0c\u795e\u7ecf\u7f51\u7edc\u5c42\u80fd\u4fdd\u7559\u7edf\u8ba1\u5145\u5206\u6027\uff0c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u8ba1\u7406\u89e3\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u56fe\u53d8\u91cf\u548c\u7edf\u8ba1\u5145\u5206\u6027\u7684\u6982\u5ff5\uff0c\u4e3a\u7406\u89e3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662f\u795e\u7ecf\u7f51\u7edc\uff09\u63d0\u4f9b\u65b0\u7684\u89c6\u89d2\u3002\u7814\u7a76\u8005\u5e0c\u671b\u5efa\u7acb\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u7528\u4ee5\u89e3\u91ca\u795e\u7ecf\u7f51\u7edc\u4e3a\u4f55\u80fd\u591f\u6709\u6548\u5730\u5b66\u4e60\u548c\u8868\u793a\u6570\u636e\u4e2d\u7684\u6a21\u5f0f\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u591f\u4fdd\u7559\u5173\u952e\u7684\u7edf\u8ba1\u4fe1\u606f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u89e3\u91ca\u4e3a\u57fa\u4e8e\u56fe\u7684\u53d8\u6362\uff0c\u5176\u4e2d\u795e\u7ecf\u5143\u4f5c\u4e3a\u8f93\u5165\u548c\u5b66\u4e60\u5230\u7684\u951a\u70b9\u4e4b\u95f4\u7684\u6210\u5bf9\u51fd\u6570\u3002\u5728\u6b64\u6846\u67b6\u4e0b\uff0c\u7814\u7a76\u4e86\u5c42\u8f93\u51fa\u5728\u591a\u5927\u7a0b\u5ea6\u4e0a\u80fd\u591f\u5145\u5206\u8868\u793a\u5c42\u8f93\u5165\uff08\u5373\u4fdd\u7559\u76ee\u6807\u53d8\u91cf\u5173\u4e8e\u8f93\u5165\u53d8\u91cf\u7684\u6761\u4ef6\u5206\u5e03\uff09\u3002\u901a\u8fc7\u5bf9\u7a20\u5bc6\u951a\u70b9\u8fdb\u884c\u5047\u8bbe\uff0c\u8bc1\u660e\u4e86\u5728\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\u6e10\u8fd1\u5145\u5206\u6027\u6210\u7acb\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e0d\u53d8\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u5047\u8bbe\u8f93\u5165\u5206\u5e03\u7684\u533a\u57df\u53ef\u5206\u79bb\u6027\u5e76\u6784\u5efa\u5408\u9002\u7684\u951a\u70b9\uff0c\u8bc1\u660e\u4e86\u6709\u9650\u5bbd\u5ea6\u7f51\u7edc\u4e5f\u80fd\u5b9e\u73b0\u5145\u5206\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\uff0c\u795e\u7ecf\u7f51\u7edc\u5c42\u6e10\u8fd1\u5145\u5206\u6027\u6210\u7acb\uff0c\u5e76\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4fdd\u6301\u4e0d\u53d8\u3002\u901a\u8fc7\u7279\u5b9a\u5047\u8bbe\uff08\u5982\u533a\u57df\u5206\u79bb\u7684\u8f93\u5165\u5206\u5e03\u548c\u9002\u5f53\u7684\u951a\u70b9\uff09\uff0c\u6709\u9650\u5bbd\u5ea6\u7f51\u7edc\u4e5f\u80fd\u8fbe\u5230\u5145\u5206\u6027\u3002\u8be5\u6846\u67b6\u6210\u529f\u5e94\u7528\u4e8e\u5168\u8fde\u63a5\u5c42\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4ee5\u53caReLU\u548cSigmoid\u6fc0\u6d3b\u51fd\u6570\u7b49\u591a\u79cd\u7f51\u7edc\u7ed3\u6784\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u795e\u7ecf\u5143\u89c6\u4e3a\u8f93\u5165\u548c\u5b66\u4e60\u5230\u7684\u951a\u70b9\u4e4b\u95f4\u7684\u6210\u5bf9\u51fd\u6570\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u5c42\u89e3\u91ca\u4e3a\u57fa\u4e8e\u56fe\u7684\u53d8\u6362\uff0c\u4ece\u800c\u5728\u56fe\u53d8\u91cf\u548c\u7edf\u8ba1\u5145\u5206\u6027\u7684\u6846\u67b6\u4e0b\u5206\u6790\u795e\u7ecf\u7f51\u7edc\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5728\u7a20\u5bc6\u7684\u951a\u70b9\u5047\u8bbe\u4e0b\uff0c\u6e10\u8fd1\u5145\u5206\u6027\u5728\u65e0\u9650\u5bbd\u5ea6\u6781\u9650\u4e0b\u6210\u7acb\uff0c\u5e76\u4e14\u5728\u6574\u4e2a\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u5f97\u4ee5\u4fdd\u7559\u3002\u4e3a\u4e86\u66f4\u8d34\u8fd1\u5b9e\u9645\u67b6\u6784\uff0c\u6211\u4eec\u8fdb\u4e00\u6b65\u8bc1\u660e\u4e86\u901a\u8fc7\u5047\u8bbe\u533a\u57df\u5206\u79bb\u7684\u8f93\u5165\u5206\u5e03\u548c\u6784\u5efa\u9002\u5f53\u7684\u951a\u70b9\uff0c\u6709\u9650\u5bbd\u5ea6\u7f51\u7edc\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u5145\u5206\u6027\u3002\u8be5\u6846\u67b6\u6db5\u76d6\u4e86\u5168\u8fde\u63a5\u5c42\u3001\u901a\u7528\u6210\u5bf9\u51fd\u6570\u3001ReLU\u548cSigmoid\u6fc0\u6d3b\u51fd\u6570\u4ee5\u53ca\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u4e86\u7edf\u8ba1\u5145\u5206\u6027\u3001\u56fe\u8bba\u8868\u793a\u548c\u6df1\u5ea6\u5b66\u4e60\u7684\u7ed3\u5408\uff0c\u4e3a\u795e\u7ecf\u7f51\u7edc\u63d0\u4f9b\u4e86\u65b0\u7684\u7edf\u8ba1\u7406\u89e3\u3002"}}
{"id": "2507.10283", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10283", "abs": "https://arxiv.org/abs/2507.10283", "authors": ["Muyi Bao", "Changyu Zeng", "Yifan Wang", "Zhengni Yang", "Zimu Wang", "Guangliang Cheng", "Jun Qi", "Wei Wang"], "title": "FTCFormer: Fuzzy Token Clustering Transformer for Image Classification", "comment": null, "summary": "Transformer-based deep neural networks have achieved remarkable success\nacross various computer vision tasks, largely attributed to their long-range\nself-attention mechanism and scalability. However, most transformer\narchitectures embed images into uniform, grid-based vision tokens, neglecting\nthe underlying semantic meanings of image regions, resulting in suboptimal\nfeature representations. To address this issue, we propose Fuzzy Token\nClustering Transformer (FTCFormer), which incorporates a novel clustering-based\ndownsampling module to dynamically generate vision tokens based on the semantic\nmeanings instead of spatial positions. It allocates fewer tokens to less\ninformative regions and more to represent semantically important regions,\nregardless of their spatial adjacency or shape irregularity. To further enhance\nfeature extraction and representation, we propose a Density Peak\nClustering-Fuzzy K-Nearest Neighbor (DPC-FKNN) mechanism for clustering center\ndetermination, a Spatial Connectivity Score (SCS) for token assignment, and a\nchannel-wise merging (Cmerge) strategy for token merging. Extensive experiments\non 32 datasets across diverse domains validate the effectiveness of FTCFormer\non image classification, showing consistent improvements over the TCFormer\nbaseline, achieving gains of improving 1.43% on five fine-grained datasets,\n1.09% on six natural image datasets, 0.97% on three medical datasets and 0.55%\non four remote sensing datasets. The code is available at:\nhttps://github.com/BaoBao0926/FTCFormer/tree/main.", "AI": {"tldr": "FTCFormer\u901a\u8fc7\u57fa\u4e8e\u8bed\u4e49\u7684\u805a\u7c7b\u65b9\u6cd5\u52a8\u6001\u751f\u6210\u89c6\u89c9\u6807\u8bb0\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u57fa\u4e8e\u7f51\u683c\u6807\u8bb0\u7684\u5c40\u9650\u6027\uff0c\u5728\u591a\u9879\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709Transformer\u6a21\u578b\u5728\u5904\u7406\u56fe\u50cf\u65f6\uff0c\u5c06\u56fe\u50cf\u5d4c\u5165\u5230\u7edf\u4e00\u7684\u3001\u57fa\u4e8e\u7f51\u683c\u7684\u89c6\u89c9\u6807\u8bb0\u4e2d\uff0c\u5ffd\u7565\u4e86\u56fe\u50cf\u533a\u57df\u6f5c\u5728\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5bfc\u81f4\u7279\u5f81\u8868\u793a\u6b21\u4f18\u3002FTCFormer\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u57fa\u4e8e\u8bed\u4e49\u800c\u975e\u7a7a\u95f4\u4f4d\u7f6e\u7684\u65b9\u6cd5\u6765\u751f\u6210\u89c6\u89c9\u6807\u8bb0\uff0c\u4e3a\u4fe1\u606f\u4e30\u5bcc\u7684\u533a\u57df\u5206\u914d\u66f4\u591a\u6807\u8bb0\uff0c\u4e3a\u4fe1\u606f\u91cf\u5c11\u7684\u533a\u57df\u5206\u914d\u8f83\u5c11\u6807\u8bb0\uff0c\u4ece\u800c\u63d0\u9ad8\u7279\u5f81\u8868\u793a\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFTCFormer\u7684\u65b0\u578bTransformer\u67b6\u6784\uff0c\u5176\u6838\u5fc3\u5728\u4e8e\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u57fa\u4e8e\u805a\u7c7b\u7684\u4e0b\u91c7\u6837\u6a21\u5757\uff0c\u80fd\u591f\u6839\u636e\u8bed\u4e49\u4fe1\u606f\u52a8\u6001\u751f\u6210\u89c6\u89c9\u6807\u8bb0\u3002\u8be5\u6a21\u5757\u901a\u8fc7\u5bc6\u5ea6\u5cf0\u503c\u805a\u7c7b-\u6a21\u7ccaK\u8fd1\u90bb\uff08DPC-FKNN\uff09\u673a\u5236\u786e\u5b9a\u805a\u7c7b\u4e2d\u5fc3\uff0c\u5229\u7528\u7a7a\u95f4\u8fde\u901a\u6027\u5f97\u5206\uff08SCS\uff09\u8fdb\u884c\u6807\u8bb0\u5206\u914d\uff0c\u5e76\u901a\u8fc7\u901a\u9053\u5f0f\u5408\u5e76\uff08Cmerge\uff09\u7b56\u7565\u8fdb\u884c\u6807\u8bb0\u5408\u5e76\u3002", "result": "\u5728\u5305\u62ec32\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u6570\u636e\u96c6\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86FTCFormer\u7684\u6709\u6548\u6027\u3002\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\uff0cFTCFormer\u76f8\u6bd4\u4e8eTCFormer\u57fa\u7ebf\u6a21\u578b\u5728\u4e94\u4e2a\u7ec6\u7c92\u5ea6\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e861.43%\uff0c\u5728\u516d\u4e2a\u81ea\u7136\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e861.09%\uff0c\u5728\u4e09\u4e2a\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e860.97%\uff0c\u5728\u56db\u4e2a\u9065\u611f\u6570\u636e\u96c6\u4e0a\u63d0\u9ad8\u4e860.55%\u3002", "conclusion": "FTCFormer\u901a\u8fc7\u52a8\u6001\u751f\u6210\u57fa\u4e8e\u8bed\u4e49\u7684\u89c6\u89c9\u6807\u8bb0\uff0c\u5e76\u91c7\u7528DPC-FKNN\u3001SCS\u548cCmerge\u7b56\u7565\uff0c\u5728\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u8d85\u8d8a\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u5e76\u5728\u591a\u4e2a\u7ec6\u7c92\u5ea6\u3001\u81ea\u7136\u56fe\u50cf\u3001\u533b\u5b66\u548c\u9065\u611f\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u51c6\u786e\u7387\u63d0\u5347\u3002"}}
{"id": "2507.10241", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10241", "abs": "https://arxiv.org/abs/2507.10241", "authors": ["Vikas Dwivedi", "Balaji Srinivasan", "Monica Sigovan", "Bruno Sixou"], "title": "Kernel-Adaptive PI-ELMs for Forward and Inverse Problems in PDEs with Sharp Gradients", "comment": null, "summary": "This paper introduces the Kernel Adaptive Physics-Informed Extreme Learning\nMachine (KAPI-ELM), an adaptive Radial Basis Function (RBF)-based extension of\nPI-ELM designed to solve both forward and inverse Partial Differential Equation\n(PDE) problems involving localized sharp gradients. While PI-ELMs outperform\nthe traditional Physics-Informed Neural Networks (PINNs) in speed due to their\nsingle-shot, least square optimization, this advantage comes at a cost: their\nfixed, randomly initialized input layer limits their ability to capture sharp\ngradients. To overcome this limitation, we introduce a lightweight Bayesian\nOptimization (BO) framework that, instead of adjusting each input layer\nparameter individually as in traditional backpropagation, learns a small set of\nhyperparameters defining the statistical distribution from which the input\nweights are drawn. This novel distributional optimization strategy -- combining\nBO for input layer distributional parameters with least-squares optimization\nfor output layer network parameters -- enables KAPI-ELM to preserve PI-ELM's\nspeed while matching or exceeding the expressiveness of PINNs. We validate the\nproposed methodology on several challenging forward and inverse PDE benchmarks,\nincluding a 1D singularly perturbed convection-diffusion equation, a 2D Poisson\nequation with sharp localized sources, and a time-dependent advection equation.\nNotably, KAPI-ELM achieves state-of-the-art accuracy in both forward and\ninverse settings. In stiff PDE regimes, it matches or even outperforms advanced\nmethods such as the Extended Theory of Functional Connections (XTFC), while\nrequiring nearly an order of magnitude fewer tunable parameters. These results\nestablish the potential of KAPI-ELM as a scalable, interpretable, and\ngeneralizable physics-informed learning framework, especially in stiff PDE\nregimes.", "AI": {"tldr": "KAPI-ELM\u662f\u4e00\u79cd\u65b0\u7684\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u8d1d\u53f6\u65af\u4f18\u5316\u6539\u8fdb\u4e86ELM\u4ee5\u5904\u7406\u5c16\u9510\u68af\u5ea6\uff0c\u5728\u901f\u5ea6\u548c\u51c6\u786e\u6027\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u7279\u522b\u9002\u7528\u4e8e\u521a\u6027PDE\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684PI-ELM\u867d\u7136\u901f\u5ea6\u5feb\uff0c\u4f46\u7531\u4e8e\u5176\u56fa\u5b9a\u7684\u3001\u968f\u673a\u521d\u59cb\u5316\u7684\u8f93\u5165\u5c42\uff0c\u96be\u4ee5\u6355\u6349\u5c16\u9510\u68af\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u63d0\u9ad8PI-ELM\u5728\u5904\u7406\u5c16\u9510\u68af\u5ea6\u95ee\u9898\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKAPI-ELM\uff08Kernel Adaptive Physics-Informed Extreme Learning Machine\uff09\u7684\u7b97\u6cd5\uff0c\u8fd9\u662fPI-ELM\u7684\u4e00\u79cd\u81ea\u9002\u5e94\u5f84\u5411\u57fa\u51fd\u6570\uff08RBF\uff09\u6269\u5c55\uff0c\u7528\u4e8e\u89e3\u51b3\u6d89\u53ca\u5c40\u90e8\u5c16\u9510\u68af\u5ea6\u7684\u6b63\u53cd\u504f\u5fae\u5206\u65b9\u7a0b\uff08PDE\uff09\u95ee\u9898\u3002KAPI-ELM\u91c7\u7528\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u8d1d\u53f6\u65af\u4f18\u5316\uff08BO\uff09\u6846\u67b6\u6765\u5b66\u4e60\u8f93\u5165\u5c42\u6743\u91cd\u7684\u5206\u5e03\u53c2\u6570\uff0c\u5e76\u7ed3\u5408\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\u6765\u5904\u7406\u8f93\u51fa\u5c42\u53c2\u6570\u3002", "result": "KAPI-ELM\u57281D\u5947\u5f02\u6444\u52a8\u5bf9\u6d41\u6269\u6563\u65b9\u7a0b\u3001\u5177\u6709\u5c16\u9510\u5c40\u90e8\u6e90\u76842D\u6cca\u677e\u65b9\u7a0b\u548c\u65f6\u53d8\u5bf9\u6d41\u65b9\u7a0b\u7b49\u5177\u6709\u6311\u6218\u6027\u7684PDE\u95ee\u9898\u4e0a\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002\u7ed3\u679c\u8868\u660e\uff0cKAPI-ELM\u5728\u6b63\u53cd\u95ee\u9898\u4e2d\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u5728\u521a\u6027PDE\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4e0eXTFC\u7b49\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u751a\u81f3\u66f4\u4f18\uff0c\u4e14\u53ef\u8c03\u53c2\u6570\u51cf\u5c11\u4e86\u8fd1\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "KAPI-ELM\u901a\u8fc7\u7ed3\u5408\u8d1d\u53f6\u65af\u4f18\u5316\u548c\u6700\u5c0f\u4e8c\u4e58\u4f18\u5316\uff0c\u5728\u4fdd\u6301PI-ELM\u901f\u5ea6\u4f18\u52bf\u7684\u540c\u65f6\uff0c\u63d0\u9ad8\u4e86\u6355\u83b7\u5c16\u9510\u68af\u5ea6\u80fd\u529b\uff0c\u5728\u591a\u4e2aPDE\u95ee\u9898\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u6027\uff0c\u7279\u522b\u662f\u5728\u521a\u6027PDE\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u5305\u62ecXTFC\u5728\u5185\u7684\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e76\u4e14\u53ef\u8c03\u53c2\u6570\u66f4\u5c11\uff0c\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u7269\u7406\u4fe1\u606f\u5b66\u4e60\u6846\u67b6\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10293", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10293", "abs": "https://arxiv.org/abs/2507.10293", "authors": ["Wenkang Han", "Wang Lin", "Yiyun Zhou", "Qi Liu", "Shulei Wang", "Chang Yao", "Jingyuan Chen"], "title": "Show and Polish: Reference-Guided Identity Preservation in Face Video Restoration", "comment": "Accepted by MM 2025", "summary": "Face Video Restoration (FVR) aims to recover high-quality face videos from\ndegraded versions. Traditional methods struggle to preserve fine-grained,\nidentity-specific features when degradation is severe, often producing\naverage-looking faces that lack individual characteristics. To address these\nchallenges, we introduce IP-FVR, a novel method that leverages a high-quality\nreference face image as a visual prompt to provide identity conditioning during\nthe denoising process. IP-FVR incorporates semantically rich identity\ninformation from the reference image using decoupled cross-attention\nmechanisms, ensuring detailed and identity consistent results. For intra-clip\nidentity drift (within 24 frames), we introduce an identity-preserving feedback\nlearning method that combines cosine similarity-based reward signals with\nsuffix-weighted temporal aggregation. This approach effectively minimizes drift\nwithin sequences of frames. For inter-clip identity drift, we develop an\nexponential blending strategy that aligns identities across clips by\niteratively blending frames from previous clips during the denoising process.\nThis method ensures consistent identity representation across different clips.\nAdditionally, we enhance the restoration process with a multi-stream negative\nprompt, guiding the model's attention to relevant facial attributes and\nminimizing the generation of low-quality or incorrect features. Extensive\nexperiments on both synthetic and real-world datasets demonstrate that IP-FVR\noutperforms existing methods in both quality and identity preservation,\nshowcasing its substantial potential for practical applications in face video\nrestoration.", "AI": {"tldr": "IP-FVR\u662f\u4e00\u79cd\u65b0\u7684\u4eba\u8138\u89c6\u9891\u6062\u590d\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u53c2\u8003\u4eba\u8138\u56fe\u50cf\u4f5c\u4e3a\u8eab\u4efd\u63d0\u793a\uff0c\u5e76\u901a\u8fc7\u7279\u6b8a\u7684\u5b66\u4e60\u548c\u6df7\u5408\u7b56\u7565\u6765\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u4f20\u7edf\u7684\u4eba\u8138\u89c6\u9891\u6062\u590d\u65b9\u6cd5\u5728\u4e25\u91cd\u964d\u7ea7\u7684\u60c5\u51b5\u4e0b\u96be\u4ee5\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u3001\u7279\u5b9a\u4e8e\u8eab\u4efd\u7684\u7279\u5f81\uff0c\u5e38\u5e38\u4ea7\u751f\u7f3a\u4e4f\u4e2a\u4f53\u7279\u5f81\u7684\u5e73\u5747\u8138\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u4f9b\u8eab\u4efd\u6761\u4ef6\u5e76\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u7684\u65b0\u65b9\u6cd5\u3002", "method": "IP-FVR\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9762\u90e8\u89c6\u9891\u6062\u590d\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u9ad8\u8d28\u91cf\u53c2\u8003\u4eba\u8138\u56fe\u50cf\u4f5c\u4e3a\u89c6\u89c9\u63d0\u793a\uff0c\u5728\u53bb\u566a\u8fc7\u7a0b\u4e2d\u63d0\u4f9b\u8eab\u4efd\u6761\u4ef6\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u89e3\u8026\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u6765\u6574\u5408\u6765\u81ea\u53c2\u8003\u56fe\u50cf\u7684\u8bed\u4e49\u4e30\u5bcc\u7684\u8eab\u4efd\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u4e86\u57fa\u4e8e\u4f59\u5f26\u76f8\u4f3c\u5ea6\u5956\u52b1\u4fe1\u53f7\u548c\u540e\u7f00\u52a0\u6743\u65f6\u95f4\u805a\u5408\u7684\u8eab\u4efd\u4fdd\u6301\u53cd\u9988\u5b66\u4e60\u65b9\u6cd5\u6765\u5904\u7406\u5355\u5e27\u5185\u7684\u8eab\u4efd\u6f02\u79fb\uff0c\u4ee5\u53ca\u4e00\u79cd\u6307\u6570\u6df7\u5408\u7b56\u7565\u6765\u5904\u7406\u8de8\u5e27\u7684\u8eab\u4efd\u6f02\u79fb\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u591a\u6d41\u8d1f\u9762\u63d0\u793a\u6765\u589e\u5f3a\u6062\u590d\u8fc7\u7a0b\u3002", "result": "IP-FVR\u80fd\u591f\u751f\u6210\u8be6\u7ec6\u4e14\u8eab\u4efd\u4e00\u81f4\u7684\u9762\u90e8\u89c6\u9891\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5355\u5e27\u5185\u548c\u8de8\u5e27\u7684\u8eab\u4efd\u6f02\u79fb\u95ee\u9898\uff0c\u5e76\u5728\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "IP-FVR\u5728\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u6301\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u9762\u90e8\u89c6\u9891\u6062\u590d\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2507.10273", "categories": ["cs.LG", "q-bio.BM"], "pdf": "https://arxiv.org/pdf/2507.10273", "abs": "https://arxiv.org/abs/2507.10273", "authors": ["Lu Zhu", "Emmanuel Noutahi"], "title": "Conditional Chemical Language Models are Versatile Tools in Drug Discovery", "comment": "12 pages, extra 13 pages of appendix", "summary": "Generative chemical language models (CLMs) have demonstrated strong\ncapabilities in molecular design, yet their impact in drug discovery remains\nlimited by the absence of reliable reward signals and the lack of\ninterpretability in their outputs. We present SAFE-T, a generalist chemical\nmodeling framework that conditions on biological context -- such as protein\ntargets or mechanisms of action -- to prioritize and design molecules without\nrelying on structural information or engineered scoring functions. SAFE-T\nmodels the conditional likelihood of fragment-based molecular sequences given a\nbiological prompt, enabling principled scoring of molecules across tasks such\nas virtual screening, drug-target interaction prediction, and activity cliff\ndetection. Moreover, it supports goal-directed generation by sampling from this\nlearned distribution, aligning molecular design with biological objectives. In\ncomprehensive zero-shot evaluations across predictive (LIT-PCBA, DAVIS, KIBA,\nACNet) and generative (DRUG, PMO) benchmarks, SAFE-T consistently achieves\nperformance comparable to or better than existing approaches while being\nsignificantly faster. Fragment-level attribution further reveals that SAFE-T\ncaptures known structure-activity relationships, supporting interpretable and\nbiologically grounded design. Together with its computational efficiency, these\nresults demonstrate that conditional generative CLMs can unify scoring and\ngeneration to accelerate early-stage drug discovery.", "AI": {"tldr": "SAFE-T\u662f\u4e00\u4e2a\u6761\u4ef6\u751f\u6210CLM\u6846\u67b6\uff0c\u53ef\u4ee5\u5bf9\u5206\u5b50\u8fdb\u884c\u8bc4\u5206\u548c\u751f\u6210\uff0c\u4ee5\u52a0\u901f\u836f\u7269\u53d1\u73b0\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u751f\u6210\u5316\u5b66\u8bed\u8a00\u6a21\u578b\uff08CLM\uff09\u5728\u836f\u7269\u53d1\u73b0\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5982\u7f3a\u4e4f\u53ef\u9760\u7684\u5956\u52b1\u4fe1\u53f7\u548c\u8f93\u51fa\u7684\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSAFE-T\u7684\u901a\u7528\u5316\u5b66\u5efa\u6a21\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ee5\u751f\u7269\u5b66\u80cc\u666f\uff08\u5982\u86cb\u767d\u8d28\u9776\u70b9\u6216\u4f5c\u7528\u673a\u5236\uff09\u4e3a\u6761\u4ef6\uff0c\u4ee5\u4f18\u5148\u6392\u5e8f\u548c\u8bbe\u8ba1\u5206\u5b50\uff0c\u800c\u4e0d\u4f9d\u8d56\u4e8e\u7ed3\u6784\u4fe1\u606f\u6216\u5de5\u7a0b\u8bc4\u5206\u51fd\u6570\u3002SAFE-T\u6a21\u62df\u4e86\u7ed9\u5b9a\u751f\u7269\u5b66\u63d0\u793a\u7684\u57fa\u4e8e\u7247\u6bb5\u7684\u5206\u5b50\u5e8f\u5217\u7684\u6761\u4ef6\u4f3c\u7136\uff0c\u4f7f\u5f97\u8de8\u865a\u62df\u7b5b\u9009\u3001\u836f\u7269-\u9776\u70b9\u76f8\u4e92\u4f5c\u7528\u9884\u6d4b\u548c\u6d3b\u6027\u88c2\u70b9\u68c0\u6d4b\u7b49\u4efb\u52a1\u7684\u5206\u5b50\u80fd\u591f\u8fdb\u884c\u539f\u5219\u6027\u8bc4\u5206\u3002\u6b64\u5916\uff0c\u5b83\u901a\u8fc7\u4ece\u5b66\u4e60\u7684\u5206\u5e03\u4e2d\u91c7\u6837\u6765\u652f\u6301\u76ee\u6807\u5bfc\u5411\u7684\u751f\u6210\uff0c\u5c06\u5206\u5b50\u8bbe\u8ba1\u4e0e\u751f\u7269\u5b66\u76ee\u6807\u5bf9\u9f50\u3002", "result": "\u5728\u5bf9\u9884\u6d4b\u6027\uff08LIT-PCBA\u3001DAVIS\u3001KIBA\u3001ACNet\uff09\u548c\u751f\u6210\u6027\uff08DRUG\u3001PMO\uff09\u57fa\u51c6\u7684\u7efc\u5408\u6027\u96f6\u6837\u672c\u8bc4\u4f30\u4e2d\uff0cSAFE-T\u59cb\u7ec8 achieves \u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u5f53\u6216\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u901f\u5ea6\u663e\u8457\u66f4\u5feb\u3002\u7247\u6bb5\u7ea7\u5f52\u56e0\u8fdb\u4e00\u6b65\u63ed\u793aSAFE-T\u80fd\u591f\u6355\u6349\u5df2\u77e5\u7684\u6784\u6548\u5173\u7cfb\uff0c\u652f\u6301\u53ef\u89e3\u91ca\u548c\u57fa\u4e8e\u751f\u7269\u5b66\u7684\u8bbe\u200b\u200b\u8ba1\u3002", "conclusion": "\u6761\u4ef6\u751f\u6210\u5316\u5b66\u8bed\u8a00\u6a21\u578b\uff08CLM\uff09\u53ef\u4ee5\u7edf\u4e00\u8bc4\u5206\u548c\u751f\u6210\uff0c\u4ee5\u52a0\u901f\u65e9\u671f\u836f\u7269\u53d1\u73b0\u3002"}}
{"id": "2507.10302", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10302", "abs": "https://arxiv.org/abs/2507.10302", "authors": ["Jiahe Zhao", "Rongkun Zheng", "Yi Wang", "Helin Wang", "Hengshuang Zhao"], "title": "DisCo: Towards Distinct and Coherent Visual Encapsulation in Video MLLMs", "comment": "ICCV 2025", "summary": "In video Multimodal Large Language Models (video MLLMs), the visual\nencapsulation process plays a pivotal role in converting video contents into\nrepresentative tokens for LLM input. While linear projectors are widely\nemployed for encapsulation, they introduce semantic indistinctness and temporal\nincoherence when applied to videos. Conversely, the structure of resamplers\nshows promise in tackling these challenges, but an effective solution remains\nunexplored. Drawing inspiration from resampler structures, we introduce DisCo,\na novel visual encapsulation method designed to yield semantically distinct and\ntemporally coherent visual tokens for video MLLMs. DisCo integrates two key\ncomponents: (1) A Visual Concept Discriminator (VCD) module, assigning unique\nsemantics for visual tokens by associating them in pair with discriminative\nconcepts in the video. (2) A Temporal Focus Calibrator (TFC) module, ensuring\nconsistent temporal focus of visual tokens to video elements across every video\nframe. Through extensive experiments on multiple video MLLM frameworks, we\ndemonstrate that DisCo remarkably outperforms previous state-of-the-art methods\nacross a variety of video understanding benchmarks, while also achieving higher\ntoken efficiency thanks to the reduction of semantic indistinctness. The code:\nhttps://github.com/ZJHTerry18/DisCo.", "AI": {"tldr": "DisCo\u662f\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u5c01\u88c5\u65b9\u6cd5\uff0c\u901a\u8fc7VCD\u548cTFC\u6a21\u5757\u89e3\u51b3\u89c6\u9891MLLM\u4e2d\u7684\u8bed\u4e49\u6a21\u7cca\u548c\u65f6\u95f4\u4e0d\u8fde\u8d2f\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u89c6\u9891MLLM\u4e2d\u7684\u7ebf\u6027\u6295\u5f71\u4eea\u5728\u5e94\u7528\u4e8e\u89c6\u9891\u65f6\u5b58\u5728\u8bed\u4e49\u6a21\u7cca\u548c\u65f6\u95f4\u4e0d\u8fde\u8d2f\u7684\u95ee\u9898\uff0c\u800c\u501f\u9274\u91cd\u91c7\u6837\u5668\u7ed3\u6784\u7684\u65b9\u6cd5\u867d\u7136\u6709\u6f5c\u529b\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u4f46\u5c1a\u672a\u6709\u6210\u719f\u7684\u65b9\u6848\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u89c6\u89c9\u5c01\u88c5\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "DisCo\u662f\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u89c9\u5c01\u88c5\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u89c6\u89c9\u6982\u5ff5\u5224\u522b\u5668\uff08VCD\uff09\u548c\u65f6\u95f4\u7126\u70b9\u6821\u51c6\u5668\uff08TFC\uff09\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\u6765\u5b9e\u73b0\uff1aVCD\u4e3a\u89c6\u89c9\u4ee3\u5e01\u5206\u914d\u72ec\u7279\u7684\u8bed\u4e49\uff0cTFC\u786e\u4fdd\u89c6\u89c9\u4ee3\u5e01\u4e0e\u89c6\u9891\u5143\u7d20\u5728\u6bcf\u4e00\u5e27\u4e2d\u7684\u65f6\u95f4\u7126\u70b9\u4e00\u81f4\u6027\u3002", "result": "DisCo\u5728\u591a\u79cd\u89c6\u9891MLLM\u6846\u67b6\u548c\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u63d0\u9ad8\u4e86\u4ee3\u5e01\u6548\u7387\u3002", "conclusion": "DisCo\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728\u591a\u79cd\u89c6\u9891MLLM\u6846\u67b6\u4e0b\uff0c\u5176\u5728\u5404\u9879\u89c6\u9891\u7406\u89e3\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u663e\u8457\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u7531\u4e8e\u964d\u4f4e\u4e86\u8bed\u4e49\u6a21\u7cca\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u4ee3\u5e01\u6548\u7387\u3002"}}
{"id": "2507.10311", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10311", "abs": "https://arxiv.org/abs/2507.10311", "authors": ["Liming Wang", "Saurabhchand Bhati", "Cody Karjadi", "Rhoda Au", "James Glass"], "title": "Recognizing Dementia from Neuropsychological Tests with State Space Models", "comment": null, "summary": "Early detection of dementia is critical for timely medical intervention and\nimproved patient outcomes. Neuropsychological tests are widely used for\ncognitive assessment but have traditionally relied on manual scoring. Automatic\ndementia classification (ADC) systems aim to infer cognitive decline directly\nfrom speech recordings of such tests. We propose Demenba, a novel ADC framework\nbased on state space models, which scale linearly in memory and computation\nwith sequence length. Trained on over 1,000 hours of cognitive assessments\nadministered to Framingham Heart Study participants, some of whom were\ndiagnosed with dementia through adjudicated review, our method outperforms\nprior approaches in fine-grained dementia classification by 21\\%, while using\nfewer parameters. We further analyze its scaling behavior and demonstrate that\nour model gains additional improvement when fused with large language models,\npaving the way for more transparent and scalable dementia assessment tools.\nCode: https://anonymous.4open.science/r/Demenba-0861", "AI": {"tldr": "Demenba\u662f\u4e00\u4e2a\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u65b0\u6846\u67b6\uff0c\u53ef\u4ee5\u901a\u8fc7\u8bed\u97f3\u8bc4\u4f30\u75f4\u5446\u75c7\uff0c\u5728\u5206\u7c7b\u51c6\u786e\u6027\u65b9\u9762\u6709\u6240\u63d0\u9ad8\uff0c\u5e76\u4e14\u6a21\u578b\u53c2\u6570\u66f4\u5c11\u3002", "motivation": "\u65e9\u671f\u53d1\u73b0\u75f4\u5446\u75c7\u5bf9\u4e8e\u53ca\u65f6\u7684\u533b\u7597\u5e72\u9884\u548c\u6539\u5584\u60a3\u8005\u9884\u540e\u81f3\u5173\u91cd\u8981\u3002\u672c\u7814\u7a76\u65e8\u5728\u76f4\u63a5\u4ece\u795e\u7ecf\u5fc3\u7406\u5b66\u6d4b\u8bd5\u7684\u8bed\u97f3\u8bb0\u5f55\u4e2d\u63a8\u65ad\u8ba4\u77e5\u80fd\u529b\u4e0b\u964d\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u75f4\u5446\u75c7\u5206\u7c7b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDemenba\u7684\u65b0\u578b\u81ea\u52a8\u75f4\u5446\u75c7\u5206\u7c7b\uff08ADC\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff0c\u5176\u5185\u5b58\u548c\u8ba1\u7b97\u91cf\u4e0e\u5e8f\u5217\u957f\u5ea6\u5448\u7ebf\u6027\u5173\u7cfb\u3002", "result": "\u8be5\u6846\u67b6\u5728\u8d85\u8fc71000\u5c0f\u65f6\u7684\u8ba4\u77e5\u8bc4\u4f30\u6570\u636e\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\uff0c\u5e76\u8868\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "Demenba\u6846\u67b6\u5728\u7ec6\u7c92\u5ea6\u75f4\u5446\u75c7\u5206\u7c7b\u65b9\u9762\u6bd4\u4ee5\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8621%\uff0c\u5e76\u4e14\u4f7f\u7528\u7684\u53c2\u6570\u66f4\u5c11\u3002\u8be5\u6a21\u578b\u8fd8\u53ef\u4ee5\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u878d\u5408\uff0c\u4ee5\u83b7\u5f97\u8fdb\u4e00\u6b65\u7684\u6539\u8fdb\uff0c\u4e3a\u66f4\u900f\u660e\u548c\u53ef\u6269\u5c55\u7684\u75f4\u5446\u75c7\u8bc4\u4f30\u5de5\u5177\u94fa\u5e73\u9053\u8def\u3002"}}
{"id": "2507.10306", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10306", "abs": "https://arxiv.org/abs/2507.10306", "authors": ["Ozge Mercanoglu Sincan", "Richard Bowden"], "title": "Contrastive Pretraining with Dual Visual Encoders for Gloss-Free Sign Language Translation", "comment": "Accepted at 9th Workshop on Sign Language Translation and Avatar\n  Technologies (SLTAT), will be held in conjunction with IVA'25", "summary": "Sign Language Translation (SLT) aims to convert sign language videos into\nspoken or written text. While early systems relied on gloss annotations as an\nintermediate supervision, such annotations are costly to obtain and often fail\nto capture the full complexity of continuous signing. In this work, we propose\na two-phase, dual visual encoder framework for gloss-free SLT, leveraging\ncontrastive visual-language pretraining. During pretraining, our approach\nemploys two complementary visual backbones whose outputs are jointly aligned\nwith each other and with sentence-level text embeddings via a contrastive\nobjective. During the downstream SLT task, we fuse the visual features and\ninput them into an encoder-decoder model. On the Phoenix-2014T benchmark, our\ndual encoder architecture consistently outperforms its single stream variants\nand achieves the highest BLEU-4 score among existing gloss-free SLT approaches.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10318", "abs": "https://arxiv.org/abs/2507.10318", "authors": ["Yuhan Liu", "Jingwen Fu", "Yang Wu", "Kangyi Wu", "Pengna Li", "Jiayi Wu", "Sanping Zhou", "Jingmin Xin"], "title": "Mind the Gap: Aligning Vision Foundation Models to Image Feature Matching", "comment": "Accepted by ICCV 2025", "summary": "Leveraging the vision foundation models has emerged as a mainstream paradigm\nthat improves the performance of image feature matching. However, previous\nworks have ignored the misalignment when introducing the foundation models into\nfeature matching. The misalignment arises from the discrepancy between the\nfoundation models focusing on single-image understanding and the cross-image\nunderstanding requirement of feature matching. Specifically, 1) the embeddings\nderived from commonly used foundation models exhibit discrepancies with the\noptimal embeddings required for feature matching; 2) lacking an effective\nmechanism to leverage the single-image understanding ability into cross-image\nunderstanding. A significant consequence of the misalignment is they struggle\nwhen addressing multi-instance feature matching problems. To address this, we\nintroduce a simple but effective framework, called IMD (Image feature Matching\nwith a pre-trained Diffusion model) with two parts: 1) Unlike the dominant\nsolutions employing contrastive-learning based foundation models that emphasize\nglobal semantics, we integrate the generative-based diffusion models to\neffectively capture instance-level details. 2) We leverage the prompt mechanism\nin generative model as a natural tunnel, propose a novel cross-image\ninteraction prompting module to facilitate bidirectional information\ninteraction between image pairs. To more accurately measure the misalignment,\nwe propose a new benchmark called IMIM, which focuses on multi-instance\nscenarios. Our proposed IMD establishes a new state-of-the-art in commonly\nevaluated benchmarks, and the superior improvement 12% in IMIM indicates our\nmethod efficiently mitigates the misalignment.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86IMD\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u8de8\u56fe\u50cf\u4ea4\u4e92\u63d0\u793a\u6765\u89e3\u51b3\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u7279\u5f81\u5339\u914d\u4e2d\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u5c24\u5176\u64c5\u957f\u591a\u5b9e\u4f8b\u5339\u914d\uff0c\u5e76\u5728IMIM\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u5c06\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u7279\u5f81\u5339\u914d\u65f6\uff0c\u5ffd\u7565\u4e86\u6a21\u578b\u5728\u5355\u56fe\u50cf\u7406\u89e3\u548c\u8de8\u56fe\u50cf\u7406\u89e3\u9700\u6c42\u4e4b\u95f4\u5b58\u5728\u7684\u9519\u4f4d\u95ee\u9898\u3002\u5177\u4f53\u8868\u73b0\u4e3a\uff1a1. \u57fa\u7840\u6a21\u578b\u63d0\u53d6\u7684\u5d4c\u5165\u4e0e\u7279\u5f81\u5339\u914d\u6240\u9700\u7684\u6700\u4f73\u5d4c\u5165\u5b58\u5728\u5dee\u5f02\uff1b2. \u7f3a\u4e4f\u6709\u6548\u673a\u5236\u5c06\u5355\u56fe\u50cf\u7406\u89e3\u80fd\u529b\u8fc1\u79fb\u5230\u8de8\u56fe\u50cf\u7406\u89e3\u3002\u8fd9\u79cd\u9519\u4f4d\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u591a\u5b9e\u4f8b\u7279\u5f81\u5339\u914d\u95ee\u9898\u65f6\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aIMD\uff08Image feature Matching with a pre-trained Diffusion model\uff09\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u90e8\u5206\uff1a1. \u96c6\u6210\u57fa\u4e8e\u751f\u6210\u5f0f\u7684\u6269\u6563\u6a21\u578b\uff0c\u4ee5\u6355\u6349\u5b9e\u4f8b\u7ea7\u7ec6\u8282\uff0c\u533a\u522b\u4e8e\u4fa7\u91cd\u5168\u5c40\u8bed\u4e49\u7684\u5bf9\u6bd4\u5b66\u4e60\u6a21\u578b\uff1b2. \u5229\u7528\u751f\u6210\u6a21\u578b\u7684\u63d0\u793a\u673a\u5236\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8de8\u56fe\u50cf\u4ea4\u4e92\u63d0\u793a\u6a21\u5757\uff0c\u4ee5\u4fc3\u8fdb\u56fe\u50cf\u5bf9\u4e4b\u95f4\u7684\u53cc\u5411\u4fe1\u606f\u4ea4\u4e92\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aIMIM\u7684\u65b0\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u5b9e\u4f8b\u573a\u666f\u4e0b\u7684\u7279\u5f81\u5339\u914d\u6027\u80fd\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u7684IMD\u6846\u67b6\u5728\u73b0\u6709\u5e38\u7528\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u5728\u591a\u5b9e\u4f8b\u7279\u5f81\u5339\u914d\u7684\u65b0\u57fa\u51c6IMIM\u4e0a\u53d6\u5f97\u4e8612%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u6a21\u578b\u9519\u4f4d\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684IMD\u6846\u67b6\u901a\u8fc7\u96c6\u6210\u751f\u6210\u5f0f\u6269\u6563\u6a21\u578b\u548c\u521b\u65b0\u7684\u8de8\u56fe\u50cf\u4ea4\u4e92\u63d0\u793a\u6a21\u5757\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u5728\u7279\u5f81\u5339\u914d\u4e2d\u5b58\u5728\u7684\u9519\u4f4d\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u591a\u5b9e\u4f8b\u5339\u914d\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u65b0\u57fa\u51c6IMIM\u4e0a\u53d6\u5f97\u4e8612%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u786e\u7acb\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6c34\u5e73\u3002"}}
{"id": "2507.10334", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10334", "abs": "https://arxiv.org/abs/2507.10334", "authors": ["Mahmoud Bekhit", "Ahmad Salah", "Ahmed Salim Alrawahi", "Tarek Attia", "Ahmed Ali", "Esraa Eldesokey", "Ahmed Fathalla"], "title": "MoCap-Impute: A Comprehensive Benchmark and Comparative Analysis of Imputation Methods for IMU-based Motion Capture Data", "comment": "22 pages, 7 figures, 3 algorithms, 2 tables", "summary": "Motion capture (MoCap) data from wearable Inertial Measurement Units (IMUs)\nis vital for applications in sports science, but its utility is often\ncompromised by missing data. Despite numerous imputation techniques, a\nsystematic performance evaluation for IMU-derived MoCap time-series data is\nlacking. We address this gap by conducting a comprehensive comparative analysis\nof statistical, machine learning, and deep learning imputation methods. Our\nevaluation considers three distinct contexts: univariate time-series,\nmultivariate across subjects, and multivariate across kinematic angles. To\nfacilitate this benchmark, we introduce the first publicly available MoCap\ndataset designed specifically for imputation, featuring data from 53 karate\npractitioners. We simulate three controlled missingness mechanisms: missing\ncompletely at random (MCAR), block missingness, and a novel value-dependent\npattern at signal transition points. Our experiments, conducted on 39 kinematic\nvariables across all subjects, reveal that multivariate imputation frameworks\nconsistently outperform univariate approaches, particularly for complex\nmissingness. For instance, multivariate methods achieve up to a 50% mean\nabsolute error reduction (MAE from 10.8 to 5.8) compared to univariate\ntechniques for transition point missingness. Advanced models like Generative\nAdversarial Imputation Networks (GAIN) and Iterative Imputers demonstrate the\nhighest accuracy in these challenging scenarios. This work provides a critical\nbaseline for future research and offers practical recommendations for improving\nthe integrity and robustness of Mo-Cap data analysis.", "AI": {"tldr": "\u7531\u4e8e\u7f3a\u4e4f\u5bf9IMU\u8fd0\u52a8\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u63d2\u8865\u65b9\u6cd5\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002\u901a\u8fc7\u53d1\u5e03\u9996\u4e2a\u4e13\u95e8\u4e3a\u6b64\u8bbe\u8ba1\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5e76\u5bf9\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u8fdb\u884c\u5e7f\u6cdb\u6bd4\u8f83\uff0c\u6211\u4eec\u53d1\u73b0\u591a\u53d8\u91cf\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7f3a\u5931\u6570\u636e\u65f6\u4f18\u4e8e\u5355\u53d8\u91cf\u65b9\u6cd5\uff0c\u5176\u4e2dGAIN\u548c\u8fed\u4ee3\u63d2\u8865\u5668\u8868\u73b0\u6700\u4f73\uff0c\u53ef\u5c06MAE\u964d\u4f4e\u9ad8\u8fbe50%\u3002", "motivation": "\u73b0\u6709\u7684IMU\u8fd0\u52a8\u6355\u6349\u6570\u636e\u63d2\u8865\u6280\u672f\u7f3a\u4e4f\u7cfb\u7edf\u6027\u7684\u6027\u80fd\u8bc4\u4f30\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u8fd0\u52a8\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u65f6\u3002\u8fd9\u9879\u7814\u7a76\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u901a\u8fc7\u5168\u9762\u7684\u6bd4\u8f83\u5206\u6790\u6765\u8bc4\u4f30\u4e0d\u540c\u7684\u63d2\u8865\u65b9\u6cd5\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u7edf\u8ba1\u3001\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u63d2\u8865\u65b9\u6cd5\u8fdb\u884c\u5168\u9762\u7684\u6bd4\u8f83\u5206\u6790\u6765\u8bc4\u4f30IMU\u8fd0\u52a8\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u7684\u63d2\u8865\u6027\u80fd\u3002\u7814\u7a76\u8003\u8651\u4e86\u5355\u53d8\u91cf\u65f6\u95f4\u5e8f\u5217\u3001\u8de8\u79d1\u76ee\u591a\u53d8\u91cf\u4ee5\u53ca\u8fd0\u52a8\u5b66\u89d2\u5ea6\u591a\u53d8\u91cf\u4e09\u79cd\u4e0d\u540c\u7684\u60c5\u5883\u3002\u4e3a\u652f\u6491\u8be5\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7814\u7a76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b53\u540d\u7a7a\u624b\u9053\u7ec3\u4e60\u8005\u6570\u636e\u7684\u516c\u5f00\u6570\u636e\u96c6\uff0c\u5e76\u6a21\u62df\u4e86\u4e09\u79cd\u7f3a\u5931\u673a\u5236\uff1a\u5b8c\u5168\u968f\u673a\u7f3a\u5931\uff08MCAR\uff09\u3001\u5757\u7f3a\u5931\u548c\u4fe1\u53f7\u8f6c\u6362\u70b9\u7684\u6570\u503c\u4f9d\u8d56\u6a21\u5f0f\u3002\u572839\u4e2a\u8fd0\u52a8\u5b66\u53d8\u91cf\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u591a\u53d8\u91cf\u63d2\u8865\u6846\u67b6\u5728\u5904\u7406\u590d\u6742\u7f3a\u5931\u60c5\u51b5\u65f6\uff0c\u6027\u80fd\u59cb\u7ec8\u4f18\u4e8e\u5355\u53d8\u91cf\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728\u4fe1\u53f7\u8f6c\u6362\u70b9\u7f3a\u5931\u60c5\u51b5\u4e0b\uff0c\u591a\u53d8\u91cf\u65b9\u6cd5\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u964d\u4f4e\u4e86\u9ad8\u8fbe50%\uff08\u4ece10.8\u964d\u81f35.8\uff09\u3002\u5176\u4e2d\uff0c\u751f\u6210\u5bf9\u6297\u6027\u63d2\u8865\u7f51\u7edc\uff08GAIN\uff09\u548c\u8fed\u4ee3\u63d2\u8865\u5668\u7b49\u9ad8\u7ea7\u6a21\u578b\u5728\u6700\u5177\u6311\u6218\u6027\u7684\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u6700\u9ad8\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u586b\u8865\u4e86IMU\u8fd0\u52a8\u6355\u6349\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u63d2\u8865\u65b9\u6cd5\u6027\u80fd\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u901a\u8fc7\u5e7f\u6cdb\u7684\u6bd4\u8f83\u5206\u6790\uff0c\u63ed\u793a\u4e86\u591a\u53d8\u91cf\u63d2\u8865\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7f3a\u5931\u60c5\u51b5\u65f6\u4f18\u4e8e\u5355\u53d8\u91cf\u65b9\u6cd5\uff0c\u5e76\u63a8\u8350\u4e86\u5982GAIN\u548c\u8fed\u4ee3\u63d2\u8865\u7b49\u5148\u8fdb\u6a21\u578b\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u51c6\u548c\u5b9e\u8df5\u5efa\u8bae\u3002"}}
{"id": "2507.10340", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10340", "abs": "https://arxiv.org/abs/2507.10340", "authors": ["Hongjae Lee", "Myungjun Son", "Dongjea Kang", "Seung-Won Jung"], "title": "Text Embedding Knows How to Quantize Text-Guided Diffusion Models", "comment": "ICCV 2025", "summary": "Despite the success of diffusion models in image generation tasks such as\ntext-to-image, the enormous computational complexity of diffusion models limits\ntheir use in resource-constrained environments. To address this, network\nquantization has emerged as a promising solution for designing efficient\ndiffusion models. However, existing diffusion model quantization methods do not\nconsider input conditions, such as text prompts, as an essential source of\ninformation for quantization. In this paper, we propose a novel quantization\nmethod dubbed Quantization of Language-to-Image diffusion models using text\nPrompts (QLIP). QLIP leverages text prompts to guide the selection of bit\nprecision for every layer at each time step. In addition, QLIP can be\nseamlessly integrated into existing quantization methods to enhance\nquantization efficiency. Our extensive experiments demonstrate the\neffectiveness of QLIP in reducing computational complexity and improving the\nquality of the generated images across various datasets.", "AI": {"tldr": "QLIP \u662f\u4e00\u79cd\u65b0\u9896\u7684\u91cf\u5316\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6587\u672c\u63d0\u793a\u6765\u6307\u5bfc\u8bed\u8a00\u5230\u56fe\u50cf\u6269\u6563\u6a21\u578b\u7684\u91cf\u5316\u8fc7\u7a0b\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u548c\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\u672a\u5c06\u6587\u672c\u63d0\u793a\u7b49\u8f93\u5165\u6761\u4ef6\u89c6\u4e3a\u91cf\u5316\u7684\u91cd\u8981\u4fe1\u606f\u6765\u6e90\uff0c\u800c\u6269\u6563\u6a21\u578b\u5de8\u5927\u7684\u8ba1\u7b97\u590d\u6742\u6027\u9650\u5236\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e0b\u7684\u4f7f\u7528\u3002", "method": "QLIP \u5229\u7528\u6587\u672c\u63d0\u793a\u6765\u6307\u5bfc\u6bcf\u4e2a\u65f6\u95f4\u6b65\u957f\u4e2d\u6bcf\u4e2a\u5c42\u7684\u6bd4\u7279\u7cbe\u5ea6\u9009\u62e9\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86 QLIP \u5728\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u548c\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u8d28\u91cf\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "QLIP \u80fd\u591f\u6709\u6548\u964d\u4f4e\u8ba1\u7b97\u590d\u6742\u6027\u5e76\u63d0\u9ad8\u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u5e76\u4e14\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u73b0\u6709\u7684\u91cf\u5316\u65b9\u6cd5\u4e2d\u4ee5\u63d0\u9ad8\u91cf\u5316\u6548\u7387\u3002"}}
{"id": "2507.10345", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10345", "abs": "https://arxiv.org/abs/2507.10345", "authors": ["Yuwen Li", "Guozhi Zhang"], "title": "Some Super-approximation Rates of ReLU Neural Networks for Korobov Functions", "comment": null, "summary": "This paper examines the $L_p$ and $W^1_p$ norm approximation errors of ReLU\nneural networks for Korobov functions. In terms of network width and depth, we\nderive nearly optimal super-approximation error bounds of order $2m$ in the\n$L_p$ norm and order $2m-2$ in the $W^1_p$ norm, for target functions with\n$L_p$ mixed derivative of order $m$ in each direction. The analysis leverages\nsparse grid finite elements and the bit extraction technique. Our results\nimprove upon classical lowest order $L_\\infty$ and $H^1$ norm error bounds and\ndemonstrate that the expressivity of neural networks is largely unaffected by\nthe curse of dimensionality.", "AI": {"tldr": "ReLU \u795e\u7ecf\u7f51\u7edc\u5728\u903c\u8fd1 Korobov \u51fd\u6570\u65f6\uff0c\u901a\u8fc7\u7a00\u758f\u7f51\u683c\u6709\u9650\u5143\u548c\u6bd4\u7279\u63d0\u53d6\u6280\u672f\uff0c\u5728\u5bbd\u5ea6\u548c\u6df1\u5ea6\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u8d85\u903c\u8fd1\u8bef\u5dee\u754c\u9650\uff0c\u8bc1\u660e\u4e86\u5176\u8868\u8fbe\u80fd\u529b\u4e0d\u53d7\u7ef4\u5ea6\u707e\u96be\u7684\u5f71\u54cd\u3002", "motivation": "\u7814\u7a76 ReLU \u795e\u7ecf\u7f51\u7edc\u5728\u903c\u8fd1 Korobov \u51fd\u6570\u65f6\u7684 $L_p$ \u548c $W^1_p$ \u8303\u6570\u903c\u8fd1\u8bef\u5dee\uff0c\u4ee5\u6539\u8fdb\u73b0\u6709\u7406\u8bba\u5e76\u7406\u89e3\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u3002", "method": "\u5229\u7528\u7a00\u758f\u7f51\u683c\u6709\u9650\u5143\u548c\u6bd4\u7279\u63d0\u53d6\u6280\u672f\uff0c\u5206\u6790\u4e86 ReLU \u795e\u7ecf\u7f51\u7edc\u5728\u5bbd\u5ea6\u548c\u6df1\u5ea6\u4e0a\u7684\u8d85\u903c\u8fd1\u8bef\u5dee\u3002", "result": "\u5728\u7f51\u7edc\u5bbd\u5ea6\u548c\u6df1\u5ea6\u65b9\u9762\uff0c\u5f97\u5230\u4e86\u8bef\u5dee\u9879\u4e3a $2m$\uff08$L_p$ \u8303\u6570\uff09\u548c $2m-2$\uff08$W^1_p$ \u8303\u6570\uff09\u7684\u8d85\u903c\u8fd1\u8bef\u5dee\u754c\u9650\uff0c\u4f18\u4e8e\u7ecf\u5178\u7684\u6700\u4f4e\u9636 $L_\nu$ \u548c $H^1$ \u8303\u6570\u8bef\u5dee\u754c\u9650\uff0c\u5e76\u8868\u660e\u795e\u7ecf\u7f51\u7edc\u7684\u8868\u8fbe\u80fd\u529b\u4e0d\u53d7\u7ef4\u5ea6\u707e\u96be\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5177\u6709 $L_p$ \u6df7\u5408\u5bfc\u6570\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5728 $L_p$ \u548c $W^1_p$ \u8303\u6570\u4e0b\uff0cReLU \u795e\u7ecf\u7f51\u7edc\u7684\u8d85\u903c\u8fd1\u8bef\u5dee\u754c\u9650\u63d0\u4f9b\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u754c\u9650\uff0c\u5177\u4f53\u4e3a $2m$ \u548c $2m-2$\u3002"}}
{"id": "2507.10343", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10343", "abs": "https://arxiv.org/abs/2507.10343", "authors": ["Hugo Norrby", "Gabriel F\u00e4rm", "Kevin Hernandez-Diaz", "Fernando Alonso-Fernandez"], "title": "FGSSNet: Feature-Guided Semantic Segmentation of Real World Floorplans", "comment": "Accepted at International Workshop on Artificial Intelligence and\n  Pattern Recognition, IWAIPR 2025", "summary": "We introduce FGSSNet, a novel multi-headed feature-guided semantic\nsegmentation (FGSS) architecture designed to improve the generalization ability\nof wall segmentation on floorplans. FGSSNet features a U-Net segmentation\nbackbone with a multi-headed dedicated feature extractor used to extract\ndomain-specific feature maps which are injected into the latent space of U-Net\nto guide the segmentation process. This dedicated feature extractor is trained\nas an encoder-decoder with selected wall patches, representative of the walls\npresent in the input floorplan, to produce a compressed latent representation\nof wall patches while jointly trained to predict the wall width. In doing so,\nwe expect that the feature extractor encodes texture and width features of wall\npatches that are useful to guide the wall segmentation process. Our experiments\nshow increased performance by the use of such injected features in comparison\nto the vanilla U-Net, highlighting the validity of the proposed approach.", "AI": {"tldr": "FGSSNet \u901a\u8fc7\u5f15\u5165\u591a\u5934\u7279\u5f81\u63d0\u53d6\u5668\u6765\u6539\u8fdb U-Net \u5728\u5efa\u7b51\u5e73\u9762\u56fe\u5899\u4f53\u5206\u5272\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5efa\u7b51\u5e73\u9762\u56fe\u5899\u4f53\u5206\u5272\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u591a\u5934\u7279\u5f81\u5f15\u5bfc\u8bed\u4e49\u5206\u5272\uff08FGSS\uff09\u67b6\u6784\u3002", "method": "FGSSNet \u67b6\u6784\u91c7\u7528 U-Net \u4f5c\u4e3a\u5206\u5272\u4e3b\u5e72\uff0c\u5e76\u7ed3\u5408\u4e86\u4e00\u4e2a\u591a\u5934\u4e13\u7528\u7279\u5f81\u63d0\u53d6\u5668\u3002\u8be5\u7279\u5f81\u63d0\u53d6\u5668\u4f5c\u4e3a\u7f16\u7801\u5668-\u89e3\u7801\u5668\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u9009\u53d6\u7684\u5899\u4f53\u56fe\u5757\u4e3a\u8f93\u5165\uff0c\u751f\u6210\u538b\u7f29\u7684\u6f5c\u5728\u8868\u793a\uff0c\u540c\u65f6\u9884\u6d4b\u5899\u4f53\u5bbd\u5ea6\u3002\u8fd9\u4e9b\u63d0\u53d6\u7684\u7279\u5f81\u88ab\u6ce8\u5165\u5230 U-Net \u7684\u6f5c\u5728\u7a7a\u95f4\u4e2d\uff0c\u4ee5\u6307\u5bfc\u5206\u5272\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u6807\u51c6\u7684 U-Net \u76f8\u6bd4\uff0c\u5f15\u5165 FGSSNet \u63d0\u53d6\u7684\u7279\u5f81\u80fd\u591f\u663e\u8457\u63d0\u5347\u5206\u5272\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684 FGSSNet \u5728\u5904\u7406\u5efa\u7b51\u5e73\u9762\u56fe\u7684\u5899\u4f53\u5206\u5272\u4efb\u52a1\u65f6\uff0c\u901a\u8fc7\u5f15\u5165\u9886\u57df\u7279\u5b9a\u7684\u7279\u5f81\u63d0\u53d6\u5668\u6765\u589e\u5f3a\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u6807\u51c6\u7684 U-Net \u7f51\u7edc\u3002"}}
{"id": "2507.10347", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10347", "abs": "https://arxiv.org/abs/2507.10347", "authors": ["Yan-Ting Chen", "Hao-Wei Chen", "Tsu-Ching Hsiao", "Chun-Yi Lee"], "title": "Parallel Sampling of Diffusion Models on $SO(3)$", "comment": "MVA2025", "summary": "In this paper, we design an algorithm to accelerate the diffusion process on\nthe $SO(3)$ manifold. The inherently sequential nature of diffusion models\nnecessitates substantial time for denoising perturbed data. To overcome this\nlimitation, we proposed to adapt the numerical Picard iteration for the $SO(3)$\nspace. We demonstrate our algorithm on an existing method that employs\ndiffusion models to address the pose ambiguity problem. Moreover, we show that\nthis acceleration advantage occurs without any measurable degradation in task\nreward. The experiments reveal that our algorithm achieves a speed-up of up to\n4.9$\\times$, significantly reducing the latency for generating a single sample.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728SO(3)\u6d41\u5f62\u4e0a\u52a0\u901f\u6269\u6563\u8fc7\u7a0b\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u6539\u7f16\u6570\u503c\u76ae\u5361\u8fed\u4ee3\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.9\u500d\u7684\u52a0\u901f\uff0c\u4e14\u4e0d\u635f\u5931\u4efb\u52a1\u5956\u52b1\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u6269\u6563\u6a21\u578b\u56fa\u6709\u7684\u987a\u5e8f\u6027\u5bfc\u81f4\u7684\u53bb\u566a\u65f6\u95f4\u8fc7\u957f\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6570\u503c\u76ae\u5361\u8fed\u4ee3\u65b9\u6cd5\u6539\u7f16\u5230SO(3)\u7a7a\u95f4\u4e2d\u7684\u7b97\u6cd5\uff0c\u4ee5\u52a0\u901f\u6269\u6563\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u7b97\u6cd5\u5728\u4e0d\u635f\u5931\u4efb\u52a1\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.9\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u5728\u4e0d\u727a\u7272\u4efb\u52a1\u5956\u52b1\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8fbe4.9\u500d\u7684\u52a0\u901f\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u751f\u6210\u5355\u4e2a\u6837\u672c\u7684\u5ef6\u8fdf\u3002"}}
{"id": "2507.10355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10355", "abs": "https://arxiv.org/abs/2507.10355", "authors": ["Bo Jiang", "Xueyang Ze", "Beibei Wang", "Xixi Wang", "Xixi Wan", "Bin Luo"], "title": "Beyond Graph Model: Reliable VLM Fine-Tuning via Random Graph Adapter", "comment": null, "summary": "Textual adapter-based tuning methods have shown significant potential in\ntransferring knowledge from pre-trained Vision-Language Models (VLMs) to\ndownstream tasks. Existing works generally employ the deterministic textual\nfeature adapter to refine each category textual representation. However, due to\ninherent factors such as different attributes and contexts, there exists\nsignificant diversity in textual descriptions for each category. Such\ndescription diversity offers rich discriminative semantic knowledge that can\nbenefit downstream visual learning tasks. Obviously, traditional deterministic\nadapter model cannot adequately capture this varied semantic information. Also,\nit is desirable to exploit the inter-class relationships in VLM adapter. To\naddress these issues, we propose to exploit random graph model into VLM adapter\nand develop a novel Vertex Random Graph Adapter (VRGAdapter). VRGAdapter first\nmodels the inherent diverse descriptions of each category and inter-class\nrelationships of different categories simultaneously by leveraging a Vertex\nRandom Knowledge Graph (VRKG) model. Then, it employs probabilistic message\npropagation on VRKG to learn context-aware distribution representation for each\nclass node. Finally, it adopts a reparameterized sampling function to achieve\ntextual adapter learning. Note that, VRGAdapter provides a more general adapter\nsolution that encompasses traditional graph-based adapter as a special case. In\naddition, to enable more robust performance for downstream tasks, we also\nintroduce a new Uncertainty-guided Multi-branch Fusion (UMF) scheme that\ndynamically integrates multiple pre-trained models for ensemble prediction.\nExtensive experiments on multiple benchmark datasets demonstrate the\neffectiveness of our approach.", "AI": {"tldr": "VRGAdapter \u901a\u8fc7\u5f15\u5165\u968f\u673a\u56fe\u6a21\u578b\u6765\u6539\u8fdb VLM \u7684\u6587\u672c\u9002\u914d\u5668\uff0c\u4ee5\u66f4\u597d\u5730\u5904\u7406\u7c7b\u522b\u63cf\u8ff0\u7684\u591a\u6837\u6027\u548c\u7c7b\u522b\u95f4\u7684\u5173\u7cfb\u3002UMF \u65b9\u6848\u5219\u901a\u8fc7\u96c6\u6210\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u6587\u672c\u9002\u914d\u5668\u7684\u8c03\u4f18\u65b9\u6cd5\u867d\u7136\u5728\u5c06\u9884\u8bad\u7ec3\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u77e5\u8bc6\u8fc1\u79fb\u5230\u4e0b\u6e38\u4efb\u52a1\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u7531\u4e8e\u7c7b\u522b\u63cf\u8ff0\u7684\u56fa\u6709\u5dee\u5f02\u548c\u7c7b\u522b\u95f4\u5173\u7cfb\u7684\u5904\u7406\u4e0d\u5f53\uff0c\u5bfc\u81f4\u65e0\u6cd5\u5145\u5206\u6355\u83b7\u8fd9\u79cd\u591a\u6837\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u56e0\u6b64\u63d0\u51fa VRGAdapter \u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a VRGAdapter \u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u968f\u673a\u56fe\u6a21\u578b\u96c6\u6210\u5230 VLM \u9002\u914d\u5668\u4e2d\u3002VRGAdapter \u9996\u5148\u901a\u8fc7\u5229\u7528 Vertex Random Knowledge Graph (VRKG) \u6a21\u578b\u6765\u5bf9\u6bcf\u4e2a\u7c7b\u522b\u7684\u56fa\u6709\u63cf\u8ff0\u4ee5\u53ca\u7c7b\u522b\u95f4\u7684\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\u3002\u7136\u540e\uff0c\u5b83\u5728 VRKG \u4e0a\u5229\u7528\u6982\u7387\u6d88\u606f\u4f20\u64ad\u6765\u5b66\u4e60\u6bcf\u4e2a\u7c7b\u522b\u8282\u70b9\u7684\u4e0a\u4e0b\u6587\u611f\u77e5\u5206\u5e03\u8868\u793a\u3002\u6700\u540e\uff0c\u5b83\u91c7\u7528\u91cd\u65b0\u53c2\u6570\u5316\u7684\u91c7\u6837\u51fd\u6570\u6765\u5b9e\u73b0\u6587\u672c\u9002\u914d\u5668\u7684\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u540d\u4e3a\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u591a\u5206\u652f\u878d\u5408\uff08UMF\uff09\u7684\u65b0\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u52a8\u6001\u5730\u96c6\u6210\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u8fdb\u884c\u96c6\u6210\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "VRGAdapter \u901a\u8fc7\u5229\u7528 VRKG \u6a21\u578b\u540c\u65f6\u5bf9\u6bcf\u4e2a\u7c7b\u522b\u7684\u56fa\u6709\u63cf\u8ff0\u548c\u7c7b\u522b\u95f4\u5173\u7cfb\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u6982\u7387\u6d88\u606f\u4f20\u64ad\u5b66\u4e60\u6bcf\u4e2a\u7c7b\u8282\u70b9\uff0c\u7136\u540e\u91c7\u7528\u91cd\u65b0\u53c2\u6570\u5316\u7684\u91c7\u6837\u51fd\u6570\u6765\u5b9e\u73b0\u6587\u672c\u9002\u914d\u5668\u5b66\u4e60\uff0c\u4e3a VLM \u9002\u914d\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u66f4\u901a\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002UMF \u65b9\u6848\u901a\u8fc7\u52a8\u6001\u96c6\u6210\u591a\u4e2a\u9884\u8bad\u7ec3\u6a21\u578b\u6765\u8fdb\u884c\u96c6\u6210\u9884\u6d4b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9c81\u68d2\u7684\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u3002"}}
{"id": "2507.10348", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10348", "abs": "https://arxiv.org/abs/2507.10348", "authors": ["Yichen Li"], "title": "Feature Distillation is the Better Choice for Model-Heterogeneous Federated Learning", "comment": null, "summary": "Model-Heterogeneous Federated Learning (Hetero-FL) has attracted growing\nattention for its ability to aggregate knowledge from heterogeneous models\nwhile keeping private data locally. To better aggregate knowledge from clients,\nensemble distillation, as a widely used and effective technique, is often\nemployed after global aggregation to enhance the performance of the global\nmodel. However, simply combining Hetero-FL and ensemble distillation does not\nalways yield promising results and can make the training process unstable. The\nreason is that existing methods primarily focus on logit distillation, which,\nwhile being model-agnostic with softmax predictions, fails to compensate for\nthe knowledge bias arising from heterogeneous models. To tackle this challenge,\nwe propose a stable and efficient Feature Distillation for model-heterogeneous\nFederated learning, dubbed FedFD, that can incorporate aligned feature\ninformation via orthogonal projection to integrate knowledge from heterogeneous\nmodels better. Specifically, a new feature-based ensemble federated knowledge\ndistillation paradigm is proposed. The global model on the server needs to\nmaintain a projection layer for each client-side model architecture to align\nthe features separately. Orthogonal techniques are employed to re-parameterize\nthe projection layer to mitigate knowledge bias from heterogeneous models and\nthus maximize the distilled knowledge. Extensive experiments show that FedFD\nachieves superior performance compared to state-of-the-art methods.", "AI": {"tldr": "FedFD\u901a\u8fc7\u7279\u5f81\u84b8\u998f\u548c\u6b63\u4ea4\u6295\u5f71\u89e3\u51b3\u4e86\u6a21\u578b\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u77e5\u8bc6\u504f\u5dee\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u6027\u80fd\u4f18\u8d8a\u3002", "motivation": "\u73b0\u6709\u7684\u6a21\u578b\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\u5728\u7ed3\u5408\u96c6\u6210\u84b8\u998f\u6280\u672f\u65f6\u5b58\u5728\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u65e0\u6cd5\u6709\u6548\u8865\u507f\u6a21\u578b\u5f02\u6784\u6027\u5e26\u6765\u7684\u77e5\u8bc6\u504f\u5dee\u7684\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5176\u4ec5\u4f9d\u8d56\u4e8elogit\u84b8\u998f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFedFD\u7684\u7279\u5f81\u84b8\u998f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u670d\u52a1\u5668\u7aef\u7ef4\u62a4\u7528\u4e8e\u5bf9\u9f50\u5ba2\u6237\u7aef\u6a21\u578b\u7279\u5f81\u7684\u6295\u5f71\u5c42\uff0c\u5e76\u5229\u7528\u6b63\u4ea4\u6280\u672f\u6765\u4f18\u5316\u8fd9\u4e9b\u6295\u5f71\u5c42\uff0c\u4ee5\u51cf\u5c11\u5f02\u6784\u6a21\u578b\u5e26\u6765\u7684\u77e5\u8bc6\u504f\u5dee\u3002", "result": "FedFD\u5728\u6a21\u578b\u5f02\u6784\u8054\u90a6\u5b66\u4e60\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "FedFD\u901a\u8fc7\u5728\u6a21\u578b\u5f02\u6784\u7684\u8054\u90a6\u5b66\u4e60\u4e2d\u5f15\u5165\u57fa\u4e8e\u7279\u5f81\u7684\u84b8\u998f\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u6b63\u4ea4\u6295\u5f71\u6765\u5bf9\u9f50\u7279\u5f81\u5e76\u7f13\u89e3\u77e5\u8bc6\u504f\u5dee\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10358", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10358", "abs": "https://arxiv.org/abs/2507.10358", "authors": ["Hongxu Ma", "Chenbo Zhang", "Lu Zhang", "Jiaogen Zhou", "Jihong Guan", "Shuigeng Zhou"], "title": "Fine-Grained Zero-Shot Object Detection", "comment": "Accepted by ACM MM'25", "summary": "Zero-shot object detection (ZSD) aims to leverage semantic descriptions to\nlocalize and recognize objects of both seen and unseen classes. Existing ZSD\nworks are mainly coarse-grained object detection, where the classes are\nvisually quite different, thus are relatively easy to distinguish. However, in\nreal life we often have to face fine-grained object detection scenarios, where\nthe classes are too similar to be easily distinguished. For example, detecting\ndifferent kinds of birds, fishes, and flowers.\n  In this paper, we propose and solve a new problem called Fine-Grained\nZero-Shot Object Detection (FG-ZSD for short), which aims to detect objects of\ndifferent classes with minute differences in details under the ZSD paradigm. We\ndevelop an effective method called MSHC for the FG-ZSD task, which is based on\nan improved two-stage detector and employs a multi-level semantics-aware\nembedding alignment loss, ensuring tight coupling between the visual and\nsemantic spaces. Considering that existing ZSD datasets are not suitable for\nthe new FG-ZSD task, we build the first FG-ZSD benchmark dataset FGZSD-Birds,\nwhich contains 148,820 images falling into 36 orders, 140 families, 579 genera\nand 1432 species. Extensive experiments on FGZSD-Birds show that our method\noutperforms existing ZSD models.", "AI": {"tldr": "\u63d0\u51fa\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u68c0\u6d4b\uff08FG-ZSD\uff09\u95ee\u9898\uff0c\u5e76\u5f00\u53d1\u4e86MSHC\u65b9\u6cd5\uff0c\u5728\u9996\u4e2aFG-ZSD\u57fa\u51c6\u6570\u636e\u96c6FGZSD-Birds\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u96f6\u6837\u672c\u68c0\u6d4b\uff08ZSD\uff09\u65b9\u6cd5\u4e3b\u8981\u5904\u7406\u89c6\u89c9\u4e0a\u5dee\u5f02\u8f83\u5927\u7684\u7c7b\u522b\uff0c\u4f46\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u5904\u7406\u7ec6\u7c92\u5ea6\u7684\u3001\u89c6\u89c9\u4e0a\u76f8\u4f3c\u7684\u7c7b\u522b\uff08\u5982\u9e1f\u7c7b\u3001\u9c7c\u7c7b\u3001\u82b1\u5349\u7b49\uff09\u3002\u56e0\u6b64\uff0c\u63d0\u51fa\u7ec6\u7c92\u5ea6\u96f6\u6837\u672c\u68c0\u6d4b\uff08FG-ZSD\uff09\u95ee\u9898\uff0c\u65e8\u5728\u68c0\u6d4b\u5177\u6709\u7ec6\u5fae\u5dee\u522b\u7684\u4e0d\u540c\u7c7b\u522b\u7269\u4f53\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6539\u8fdb\u7684\u4e24\u9636\u6bb5\u68c0\u6d4b\u5668\u548c\u591a\u5c42\u6b21\u8bed\u4e49\u611f\u77e5\u5d4c\u5165\u5bf9\u9f50\u635f\u5931\u7684MSHC\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3FG-ZSD\u95ee\u9898\u3002", "result": "\u5728FGZSD-Birds\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86MSHC\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684ZSD\u6a21\u578b\u3002", "conclusion": "\u63d0\u51fa\u7684FG-ZSD\u65b9\u6cd5\u5728FGZSD-Birds\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684ZSD\u6a21\u578b\u3002"}}
{"id": "2507.10349", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10349", "abs": "https://arxiv.org/abs/2507.10349", "authors": ["Zhiyuan Zhao", "Sitan Yang", "Kin G. Olivares", "Boris N. Oreshkin", "Stan Vitebsky", "Michael W. Mahoney", "B. Aditya Prakash", "Dmitry Efimov"], "title": "TAT: Temporal-Aligned Transformer for Multi-Horizon Peak Demand Forecasting", "comment": "9 pages, 4 figures, 7 tables, published at KDD 2025 workshop on AI\n  for Supply Chain: Today and Future", "summary": "Multi-horizon time series forecasting has many practical applications such as\ndemand forecasting. Accurate demand prediction is critical to help make buying\nand inventory decisions for supply chain management of e-commerce and physical\nretailers, and such predictions are typically required for future horizons\nextending tens of weeks. This is especially challenging during high-stake sales\nevents when demand peaks are particularly difficult to predict accurately.\nHowever, these events are important not only for managing supply chain\noperations but also for ensuring a seamless shopping experience for customers.\nTo address this challenge, we propose Temporal-Aligned Transformer (TAT), a\nmulti-horizon forecaster leveraging apriori-known context variables such as\nholiday and promotion events information for improving predictive performance.\nOur model consists of an encoder and decoder, both embedded with a novel\nTemporal Alignment Attention (TAA), designed to learn context-dependent\nalignment for peak demand forecasting. We conduct extensive empirical analysis\non two large-scale proprietary datasets from a large e-commerce retailer. We\ndemonstrate that TAT brings up to 30% accuracy improvement on peak demand\nforecasting while maintaining competitive overall performance compared to other\nstate-of-the-art methods.", "AI": {"tldr": "TAT\u662f\u4e00\u79cd\u5229\u7528\u65f6\u95f4\u5bf9\u9f50\u6ce8\u610f\u529b\u548c\u4e0a\u4e0b\u6587\u53d8\u91cf\uff08\u5982\u8282\u5047\u65e5\u548c\u4fc3\u9500\uff09\u7684\u591a\u9884\u6d4b\u5668\uff0c\u53ef\u5c06\u5cf0\u503c\u9700\u6c42\u9884\u6d4b\u7684\u51c6\u786e\u6027\u63d0\u9ad8\u9ad8\u8fbe30%\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u9ad8\u98ce\u9669\u9500\u552e\u6d3b\u52a8\u671f\u95f4\u9700\u6c42\u9884\u6d4b\u7684\u6311\u6218\uff0c\u8fd9\u4e9b\u6d3b\u52a8\u7684\u9700\u6c42\u9ad8\u5cf0\u5c24\u5176\u96be\u4ee5\u51c6\u786e\u9884\u6d4b\uff0c\u800c\u8fd9\u4e9b\u6d3b\u52a8\u5bf9\u4e8e\u7ba1\u7406\u4f9b\u5e94\u94fe\u8fd0\u8425\u548c\u786e\u4fdd\u65e0\u7f1d\u7684\u5ba2\u6237\u8d2d\u7269\u4f53\u9a8c\u81f3\u5173\u91cd\u8981\u3002", "method": "TAT\u6a21\u578b\uff0c\u4e00\u4e2a\u591a\u9884\u6d4b\u5668\uff0c\u5229\u7528\u5148\u9a8c\u5df2\u77e5\u7684\u4e0a\u4e0b\u6587\u53d8\u91cf\uff08\u5982\u8282\u5047\u65e5\u548c\u4fc3\u9500\u6d3b\u52a8\u4fe1\u606f\uff09\u6765\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002 TAT\u6a21\u578b\u7531\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7ec4\u6210\uff0c\u4e24\u8005\u90fd\u5d4c\u5165\u4e86\u65b0\u9896\u7684\u65f6\u95f4\u5bf9\u9f50\u6ce8\u610f\u529b\uff08TAA\uff09\u673a\u5236\uff0c\u65e8\u5728\u5b66\u4e60\u4f9d\u8d56\u4e8e\u4e0a\u4e0b\u6587\u7684\u5cf0\u503c\u9700\u6c42\u9884\u6d4b\u5bf9\u9f50\u3002", "result": "TAT\u6a21\u578b\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u4e13\u6709\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5cf0\u503c\u9700\u6c42\u9884\u6d4b\u65b9\u9762\u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "TAT\u6a21\u578b\u5728\u9884\u6d4b\u5cf0\u503c\u9700\u6c42\u65b9\u9762\u5e26\u6765\u4e86\u9ad8\u8fbe30%\u7684\u51c6\u786e\u6027\u63d0\u5347\uff0c\u540c\u65f6\u4e0e\u5176\u4ed6\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u6574\u4f53\u6027\u80fd\u4e0a\u4fdd\u6301\u4e86\u7ade\u4e89\u529b\u3002"}}
{"id": "2507.10375", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10375", "abs": "https://arxiv.org/abs/2507.10375", "authors": ["Utkarsh Singhal", "Ryan Feng", "Stella X. Yu", "Atul Prakash"], "title": "Test-Time Canonicalization by Foundation Models for Robust Perception", "comment": "Published at ICML 2025", "summary": "Real-world visual perception requires invariance to diverse transformations,\nyet current methods rely heavily on specialized architectures or training on\npredefined augmentations, limiting generalization. We propose FOCAL, a\ntest-time, data-driven framework that achieves robust perception by leveraging\ninternet-scale visual priors from foundation models. By generating and\noptimizing candidate transformations toward visually typical, \"canonical\"\nviews, FOCAL enhances robustness without re-training or architectural changes.\nOur experiments demonstrate improved robustness of CLIP and SAM across\nchallenging transformations, including 2D/3D rotations, illumination shifts\n(contrast and color), and day-night variations. We also highlight potential\napplications in active vision. Our approach challenges the assumption that\ntransform-specific training is necessary, instead offering a scalable path to\ninvariance. Our code is available at: https://github.com/sutkarsh/focal.", "AI": {"tldr": "FOCAL\u662f\u4e00\u79cd\u5728\u6d4b\u8bd5\u65f6\u589e\u5f3a\u6a21\u578b\u9c81\u68d2\u6027\u7684\u6846\u67b6\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\uff0c\u5229\u7528\u57fa\u7840\u6a21\u578b\u548c\u7f51\u7edc\u89c4\u6a21\u7684\u89c6\u89c9\u5148\u9a8c\u6765\u4f18\u5316\u56fe\u50cf\u89c6\u56fe\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u89c6\u89c9\u611f\u77e5\u9700\u8981\u5bf9\u5404\u79cd\u53d8\u6362\u5177\u6709\u4e0d\u53d8\u6027\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e13\u95e8\u7684\u67b6\u6784\u6216\u9884\u5b9a\u4e49\u7684\u589e\u5f3a\u8bad\u7ec3\uff0c\u9650\u5236\u4e86\u6cdb\u5316\u80fd\u529b\u3002", "method": "FOCAL\u6846\u67b6\u5728\u6d4b\u8bd5\u65f6\u5229\u7528\u4e92\u8054\u7f51\u89c4\u6a21\u7684\u89c6\u89c9\u5148\u9a8c\u548c\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u751f\u6210\u548c\u4f18\u5316\u5019\u9009\u53d8\u6362\u6765\u5b9e\u73b0\u2018\u5178\u578b\u2019\u89c6\u56fe\uff0c\u4ece\u800c\u589e\u5f3a\u9c81\u68d2\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eFOCAL\u80fd\u591f\u63d0\u9ad8CLIP\u548cSAM\u57282D/3D\u65cb\u8f6c\u3001\u5149\u7167\u53d8\u5316\uff08\u5bf9\u6bd4\u5ea6\u548c\u989c\u8272\uff09\u4ee5\u53ca\u663c\u591c\u53d8\u5316\u7b49\u6311\u6218\u6027\u53d8\u6362\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "FOCAL\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u89c6\u89c9\u5148\u9a8c\uff0c\u5728\u6d4b\u8bd5\u65f6\u5b9e\u73b0\u9c81\u68d2\u6027\uff0c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3\u6216\u4fee\u6539\u67b6\u6784\uff0c\u6311\u6218\u4e86\u7279\u5b9a\u53d8\u6362\u8bad\u7ec3\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2507.10368", "categories": ["cs.LG", "physics.geo-ph"], "pdf": "https://arxiv.org/pdf/2507.10368", "abs": "https://arxiv.org/abs/2507.10368", "authors": ["Yongjin Choi", "Chenying Liu", "Jorge Macedo"], "title": "Enhanced DeepONet for 1-D consolidation operator learning: an architectural investigation", "comment": null, "summary": "Deep Operator Networks (DeepONets) have emerged as a powerful surrogate\nmodeling framework for learning solution operators in PDE-governed systems.\nWhile their use is expanding across engineering disciplines, applications in\ngeotechnical engineering remain limited. This study systematically evaluates\nseveral DeepONet architectures for the one-dimensional consolidation problem.\nWe initially consider three architectures: a standard DeepONet with the\ncoefficient of consolidation embedded in the branch net (Models 1 and 2), and a\nphysics-inspired architecture with the coefficient embedded in the trunk net\n(Model 3). Results show that Model 3 outperforms the standard configurations\n(Models 1 and 2) but still has limitations when the target solution (excess\npore pressures) exhibits significant variation. To overcome this limitation, we\npropose a Trunknet Fourier feature-enhanced DeepONet (Model 4) that addresses\nthe identified limitations by capturing rapidly varying functions. All proposed\narchitectures achieve speedups ranging from 1.5 to 100 times over traditional\nexplicit and implicit solvers, with Model 4 being the most efficient. Larger\ncomputational savings are expected for more complex systems than the explored\n1D case, which is promising. Overall, the study highlights the potential of\nDeepONets to enable efficient, generalizable surrogate modeling in geotechnical\napplications, advancing the integration of scientific machine learning in\ngeotechnics, which is at an early stage.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5085\u91cc\u53f6\u7279\u5f81\u589e\u5f3a DeepONet \u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u5ca9\u571f\u5de5\u7a0b\u4e2d\u7684\u4e00\u7ef4\u56fa\u7ed3\u95ee\u9898\uff0c\u8be5\u6a21\u578b\u6bd4\u4f20\u7edf\u65b9\u6cd5\u548c\u6807\u51c6 DeepONet \u66f4\u5feb\u3001\u66f4\u51c6\u786e\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc\uff08DeepONets\uff09\u5728\u5ca9\u571f\u5de5\u7a0b\u9886\u57df\u7684\u5e94\u7528\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u5177\u6709\u663e\u8457\u53d8\u5316\u7684\u89e3\uff08\u5982\u8d85\u5b54\u9699\u538b\u529b\uff09\u65f6\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86\u4e09\u79cd DeepONet \u67b6\u6784\uff08\u6807\u51c6 DeepONet \u548c\u7269\u7406\u4fe1\u606f\u67b6\u6784\uff09\u7528\u4e8e\u4e00\u7ef4\u56fa\u7ed3\u95ee\u9898\uff0c\u7136\u540e\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5085\u91cc\u53f6\u7279\u5f81\u589e\u5f3a\u7684 Trunknet DeepONet \u6a21\u578b\uff08\u6a21\u578b 4\uff09\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "result": "\u6240\u6709\u63d0\u51fa\u7684\u67b6\u6784\u5728\u901f\u5ea6\u4e0a\u90fd\u6bd4\u4f20\u7edf\u6c42\u89e3\u5668\u5feb 1.5 \u5230 100 \u500d\uff0c\u5176\u4e2d\u6a21\u578b 4 \u6700\u4e3a\u9ad8\u6548\u3002\u4e0e\u6807\u51c6\u914d\u7f6e\u76f8\u6bd4\uff0c\u7269\u7406\u4fe1\u606f\u67b6\u6784\uff08\u6a21\u578b 3\uff09\u8868\u73b0\u66f4\u597d\uff0c\u800c\u5085\u91cc\u53f6\u7279\u5f81\u589e\u5f3a\u6a21\u578b\uff08\u6a21\u578b 4\uff09\u8fdb\u4e00\u6b65\u514b\u670d\u4e86\u6355\u83b7\u5feb\u901f\u53d8\u5316\u51fd\u6570\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u7cfb\u7edf\u5730\u8bc4\u4f30\u4e86\u51e0\u79cd\u7528\u4e8e\u4e00\u7ef4\u56fa\u7ed3\u95ee\u9898\u7684 DeepONet \u67b6\u6784\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5085\u91cc\u53f6\u7279\u5f81\u589e\u5f3a\u6a21\u578b\uff0c\u4ee5\u514b\u670d\u73b0\u6709\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u6807\u51c6 DeepONet \u914d\u7f6e\uff0c\u5c55\u793a\u4e86 DeepONets \u5728\u5ca9\u571f\u5de5\u7a0b\u9886\u57df\u8fdb\u884c\u9ad8\u6548\u3001\u53ef\u63a8\u5e7f\u7684\u4ee3\u7406\u5efa\u6a21\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.10381", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10381", "abs": "https://arxiv.org/abs/2507.10381", "authors": ["Aaryam Sharma"], "title": "Improving Remote Sensing Classification using Topological Data Analysis and Convolutional Neural Networks", "comment": "9 pages, 8 figures", "summary": "Topological data analysis (TDA) is a relatively new field that is gaining\nrapid adoption due to its robustness and ability to effectively describe\ncomplex datasets by quantifying geometric information. In imaging contexts, TDA\ntypically models data as filtered cubical complexes from which we can extract\ndiscriminative features using persistence homology. Meanwhile, convolutional\nneural networks (CNNs) have been shown to be biased towards texture based local\nfeatures. To address this limitation, we propose a TDA feature engineering\npipeline and a simple method to integrate topological features with deep\nlearning models on remote sensing classification. Our method improves the\nperformance of a ResNet18 model on the EuroSAT dataset by 1.44% achieving\n99.33% accuracy, which surpasses all previously reported single-model\naccuracies, including those with larger architectures, such as ResNet50 (2x\nlarger) and XL Vision Transformers (197x larger). We additionally show that our\nmethod's accuracy is 1.82% higher than our ResNet18 baseline on the RESISC45\ndataset. To our knowledge, this is the first application of TDA features in\nsatellite scene classification with deep learning. This demonstrates that TDA\nfeatures can be integrated with deep learning models, even on datasets without\nexplicit topological structures, thereby increasing the applicability of TDA. A\nclean implementation of our method will be made publicly available upon\npublication.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u62d3\u6251\u6570\u636e\u5206\u6790\uff08TDA\uff09\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982ResNet18\uff09\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u5206\u7c7b\uff0c\u5728EuroSAT\u548cRESISC45\u6570\u636e\u96c6\u4e0a\u5747\u663e\u8457\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u7684\u5927\u578b\u6a21\u578b\u3002", "motivation": "CNN\u6a21\u578b\u5728\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u4e2d\u503e\u5411\u4e8e\u4f7f\u7528\u57fa\u4e8e\u7eb9\u7406\u7684\u5c40\u90e8\u7279\u5f81\uff0c\u53ef\u80fd\u5b58\u5728\u504f\u5dee\u3002\u800cTDA\u80fd\u591f\u6709\u6548\u63cf\u8ff0\u590d\u6742\u6570\u636e\u96c6\u7684\u51e0\u4f55\u4fe1\u606f\uff0c\u5177\u6709\u9c81\u68d2\u6027\u3002\u56e0\u6b64\uff0c\u5c06TDA\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7ed3\u5408\u65e8\u5728\u514b\u670dCNN\u7684\u5c40\u9650\u6027\u5e76\u63d0\u9ad8\u5206\u7c7b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdTDA\u7279\u5f81\u5de5\u7a0b\u6d41\u7a0b\uff0c\u5e76\u5c06\u62d3\u6251\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff08\u5982ResNet18\uff09\u96c6\u6210\uff0c\u7528\u4e8e\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u3002", "result": "\u5728EuroSAT\u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5c06ResNet18\u6a21\u578b\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u4e861.44%\uff0c\u8fbe\u523099.33%\uff0c\u8d85\u8fc7\u4e86\u5176\u4ed6\u5355\u4e00\u6a21\u578b\uff08\u5305\u62ecResNet50\u548cXL Vision Transformers\uff09\u7684\u51c6\u786e\u7387\u3002\u5728RESISC45\u6570\u636e\u96c6\u4e0a\uff0c\u51c6\u786e\u7387\u4e5f\u6bd4\u57fa\u7ebfResNet18\u9ad8\u51fa1.82%\u3002", "conclusion": "\u5c06TDA\u7279\u5f81\u4e0e\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u7ed3\u5408\u53ef\u4ee5\u63d0\u9ad8\u9065\u611f\u56fe\u50cf\u5206\u7c7b\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u663e\u5f0f\u62d3\u6251\u7ed3\u6784\u7684\u6570\u636e\u96c6\u4e0a\u4e5f\u662f\u5982\u6b64\uff0c\u8fd9\u8868\u660eTDA\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\u3002"}}
{"id": "2507.10382", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10382", "abs": "https://arxiv.org/abs/2507.10382", "authors": ["Yue Ding", "Conor McCarthy", "Kevin O'Shea", "Mingming Liu"], "title": "Leveraging RAG-LLMs for Urban Mobility Simulation and Analysis", "comment": null, "summary": "With the rise of smart mobility and shared e-mobility services, numerous\nadvanced technologies have been applied to this field. Cloud-based traffic\nsimulation solutions have flourished, offering increasingly realistic\nrepresentations of the evolving mobility landscape. LLMs have emerged as\npioneering tools, providing robust support for various applications, including\nintelligent decision-making, user interaction, and real-time traffic analysis.\nAs user demand for e-mobility continues to grow, delivering comprehensive\nend-to-end solutions has become crucial. In this paper, we present a\ncloud-based, LLM-powered shared e-mobility platform, integrated with a mobile\napplication for personalized route recommendations. The optimization module is\nevaluated based on travel time and cost across different traffic scenarios.\nAdditionally, the LLM-powered RAG framework is evaluated at the schema level\nfor different users, using various evaluation methods. Schema-level RAG with\nXiYanSQL achieves an average execution accuracy of 0.81 on system operator\nqueries and 0.98 on user queries.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u4e91\u6280\u672f\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5171\u4eab\u7535\u52a8\u51fa\u884c\u5e73\u53f0\uff0c\u80fd\u591f\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u8def\u7ebf\u63a8\u8350\u548c\u4f18\u5316\u7684\u51fa\u884c\u65b9\u6848\u3002\u8be5\u5e73\u53f0\u5728\u8bc4\u4f30\u4e2d\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u7528\u6237\u67e5\u8be2\u65b9\u9762\u3002", "motivation": "\u968f\u7740\u667a\u80fd\u51fa\u884c\u548c\u5171\u4eab\u7535\u52a8\u51fa\u884c\u670d\u52a1\u7684\u5174\u8d77\uff0c\u4e3a\u4e86\u6ee1\u8db3\u65e5\u76ca\u589e\u957f\u7684\u7528\u6237\u9700\u6c42\uff0c\u63d0\u4f9b\u5168\u9762\u7684\u7aef\u5230\u7aef\u89e3\u51b3\u65b9\u6848\u81f3\u5173\u91cd\u8981\u3002LLM\u5728\u667a\u80fd\u51b3\u7b56\u3001\u7528\u6237\u4ea4\u4e92\u548c\u5b9e\u65f6\u4ea4\u901a\u5206\u6790\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6f5c\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e91\u7684\u3001\u7531LLM\u9a71\u52a8\u7684\u5171\u4eab\u7535\u52a8\u51fa\u884c\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u4e2a\u6027\u5316\u8def\u7ebf\u63a8\u8350\u3002\u4f18\u5316\u6a21\u5757\u57fa\u4e8e\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u51fa\u884c\u65f6\u95f4\u548c\u6210\u672c\u8fdb\u884c\u8bc4\u4f30\u3002LLM\u9a71\u52a8\u7684RAG\u6846\u67b6\u5728\u6a21\u5f0f\u5c42\u9762\u4f7f\u7528\u591a\u79cd\u8bc4\u4f30\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u57fa\u4e8eXiYanSQL\u7684\u6a21\u5f0f\u5c42\u9762RAG\u5728\u7cfb\u7edf\u64cd\u4f5c\u5458\u67e5\u8be2\u4e0a\u7684\u5e73\u5747\u6267\u884c\u51c6\u786e\u7387\u4e3a0.81\uff0c\u5728\u7528\u6237\u67e5\u8be2\u4e0a\u7684\u5e73\u5747\u6267\u884c\u51c6\u786e\u7387\u4e3a0.98\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e91\u7684\u3001\u7531LLM\u9a71\u52a8\u7684\u5171\u4eab\u7535\u52a8\u51fa\u884c\u5e73\u53f0\uff0c\u96c6\u6210\u4e86\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4ee5\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u8def\u7ebf\u63a8\u8350\u3002\u8be5\u5e73\u53f0\u901a\u8fc7\u4f18\u5316\u6a21\u5757\u5728\u4e0d\u540c\u4ea4\u901a\u573a\u666f\u4e0b\u7684\u51fa\u884c\u65f6\u95f4\u548c\u6210\u672c\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u5404\u79cd\u8bc4\u4f30\u65b9\u6cd5\u5bf9LLM\u9a71\u52a8\u7684RAG\u6846\u67b6\u5728\u4e0d\u540c\u7528\u6237\u7684\u6a21\u5f0f\u5c42\u9762\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5176\u4e2d\u57fa\u4e8eXiYanSQL\u7684\u6a21\u5f0f\u5c42\u9762RAG\u5728\u7cfb\u7edf\u64cd\u4f5c\u5458\u67e5\u8be2\u4e0a\u8fbe\u5230\u4e860.81\u7684\u5e73\u5747\u6267\u884c\u51c6\u786e\u7387\uff0c\u5728\u7528\u6237\u67e5\u8be2\u4e0a\u8fbe\u5230\u4e860.98\u7684\u5e73\u5747\u6267\u884c\u51c6\u786e\u7387\u3002"}}
{"id": "2507.10385", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10385", "abs": "https://arxiv.org/abs/2507.10385", "authors": ["Md. Ahsanul Kabir", "Mohammad Al Hasan", "Aritra Mandal", "Liyang Hao", "Ishita Khan", "Daniel Tunkelang", "Zhe Wu"], "title": "Extracting Important Tokens in E-Commerce Queries with a Tag Interaction-Aware Transformer Model", "comment": null, "summary": "The major task of any e-commerce search engine is to retrieve the most\nrelevant inventory items, which best match the user intent reflected in a\nquery. This task is non-trivial due to many reasons, including ambiguous\nqueries, misaligned vocabulary between buyers, and sellers, over- or\nunder-constrained queries by the presence of too many or too few tokens. To\naddress these challenges, query reformulation is used, which modifies a user\nquery through token dropping, replacement or expansion, with the objective to\nbridge semantic gap between query tokens and users' search intent. Early\nmethods of query reformulation mostly used statistical measures derived from\ntoken co-occurrence frequencies from selective user sessions having clicks or\npurchases. In recent years, supervised deep learning approaches, specifically\ntransformer-based neural language models, or sequence-to-sequence models are\nbeing used for query reformulation task. However, these models do not utilize\nthe semantic tags of a query token, which are significant for capturing user\nintent of an e-commerce query. In this work, we pose query reformulation as a\ntoken classification task, and solve this task by designing a dependency-aware\ntransformer-based language model, TagBERT, which makes use of semantic tags of\na token for learning superior query phrase embedding. Experiments on large,\nreal-life e-commerce datasets show that TagBERT exhibits superior performance\nthan plethora of competing models, including BERT, eBERT, and\nSequence-to-Sequence transformer model for important token classification task.", "AI": {"tldr": "TagBERT\u662f\u4e00\u4e2a\u5229\u7528\u6807\u8bb0\u8bed\u4e49\u6807\u7b7e\u7684Transformer\u6a21\u578b\uff0c\u7528\u4e8e\u7535\u5b50\u5546\u52a1\u641c\u7d22\u4e2d\u7684\u67e5\u8be2\u6539\u5199\uff0c\u5e76\u5728\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u5f53\u524d\u7684\u67e5\u8be2\u6539\u5199\u65b9\u6cd5\u672a\u80fd\u5229\u7528\u67e5\u8be2\u6807\u8bb0\u7684\u8bed\u4e49\u6807\u7b7e\uff0c\u800c\u8fd9\u4e9b\u6807\u7b7e\u5bf9\u4e8e\u7406\u89e3\u7535\u5b50\u5546\u52a1\u67e5\u8be2\u4e2d\u7684\u7528\u6237\u610f\u56fe\u81f3\u5173\u91cd\u8981\u3002\u4e4b\u524d\u7684\u57fa\u4e8e\u7edf\u8ba1\u548c\u6df1\u5ea6\u5b66\u4e60\uff08\u5305\u62ecTransformer\uff09\u7684\u65b9\u6cd5\u5b58\u5728\u6b64\u4e0d\u8db3\u3002", "method": "\u5c06\u67e5\u8be2\u6539\u5199\u89c6\u4e3a\u4e00\u4e2a\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4f9d\u8d56\u611f\u77e5\u7684\u3001\u57fa\u4e8eTransformer\u7684\u8bed\u8a00\u6a21\u578bTagBERT\u6765\u89e3\u51b3\u6b64\u4efb\u52a1\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4e86\u6807\u8bb0\u7684\u8bed\u4e49\u6807\u7b7e\u6765\u5b66\u4e60\u66f4\u4f18\u7684\u67e5\u8be2\u77ed\u8bed\u5d4c\u5165\u3002", "result": "\u5728\u5927\u578b\u771f\u5b9e\u7535\u5b50\u5546\u52a1\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0cTagBERT\u5728\u91cd\u8981\u7684\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5305\u62ecBERT\u3001eBERT\u548c\u5e8f\u5217\u5230\u5e8f\u5217Transformer\u6a21\u578b\u5728\u5185\u7684\u591a\u79cd\u7ade\u4e89\u6a21\u578b\u3002", "conclusion": "TagBERT\u901a\u8fc7\u5c06\u67e5\u8be2\u6539\u5199\u89c6\u4e3a\u4e00\u4e2a\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u5229\u7528\u6807\u8bb0\u7684\u8bed\u4e49\u6807\u7b7e\u6765\u5b66\u4e60\u66f4\u4f18\u7684\u67e5\u8be2\u77ed\u8bed\u5d4c\u5165\uff0c\u5728\u91cd\u8981\u7684\u6807\u8bb0\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u5305\u62ecBERT\u3001eBERT\u548c\u5e8f\u5217\u5230\u5e8f\u5217Transformer\u6a21\u578b\u5728\u5185\u7684\u591a\u79cd\u7ade\u4e89\u6a21\u578b\u3002"}}
{"id": "2507.10400", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2507.10400", "abs": "https://arxiv.org/abs/2507.10400", "authors": ["Nicholas Casetti", "Dylan Anstine", "Olexandr Isayev", "Connor W. Coley"], "title": "Anticipating the Selectivity of Cyclization Reaction Pathways with Neural Network Potentials", "comment": "32 pages, 5 figures", "summary": "Reaction mechanism search tools have demonstrated the ability to provide\ninsights into likely products and rate-limiting steps of reacting systems.\nHowever, reactions involving several concerted bond changes - as can be found\nin many key steps of natural product synthesis - can complicate the search\nprocess. To mitigate these complications, we present a mechanism search\nstrategy particularly suited to help expedite exploration of an exemplary\nfamily of such complex reactions, cyclizations. We provide a cost-effective\nstrategy for identifying relevant elementary reaction steps by combining\ngraph-based enumeration schemes and machine learning techniques for\nintermediate filtering. Key to this approach is our use of a neural network\npotential (NNP), AIMNet2-rxn, for computational evaluation of each candidate\nreaction pathway. In this article, we evaluate the NNP's ability to estimate\nactivation energies, demonstrate the correct anticipation of stereoselectivity,\nand recapitulate complex enabling steps in natural product synthesis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u8bba\u548c\u673a\u5668\u5b66\u4e60\u7684\u673a\u7406\u641c\u7d22\u65b0\u7b56\u7565\uff0c\u4f7f\u7528AIMNet2-rxn\u795e\u7ecf\u7f51\u7edc\u52bf\u80fd\uff0c\u80fd\u6709\u6548\u5904\u7406\u590d\u6742\u53cd\u5e94\uff08\u5982\u73af\u5316\u53cd\u5e94\uff09\uff0c\u5e76\u80fd\u51c6\u786e\u9884\u6d4b\u6d3b\u5316\u80fd\u548c\u7acb\u4f53\u9009\u62e9\u6027\uff0c\u4e3a\u5929\u7136\u4ea7\u7269\u5408\u6210\u7b49\u9886\u57df\u63d0\u4f9b\u652f\u6301\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u53cd\u5e94\u673a\u7406\u641c\u7d22\u5de5\u5177\u5728\u5904\u7406\u6d89\u53ca\u591a\u4e2a\u534f\u540c\u952e\u53d8\u7684\u590d\u6742\u53cd\u5e94\uff08\u5982\u5929\u7136\u4ea7\u7269\u5408\u6210\u4e2d\u7684\u5173\u952e\u6b65\u9aa4\uff09\u65f6\u9047\u5230\u7684\u56f0\u96be\uff0c\u4ee5\u671f\u52a0\u901f\u5bf9\u8fd9\u7c7b\u590d\u6742\u53cd\u5e94\u673a\u7406\u7684\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u8bba\u679a\u4e3e\u548c\u673a\u5668\u5b66\u4e60\uff08\u7279\u522b\u662f\u795e\u7ecf\u7f51\u7edc\u52bfAIMNet2-rxn\uff09\u7684\u673a\u7406\u641c\u7d22\u7b56\u7565\uff0c\u7528\u4e8e\u8bc4\u4f30\u548c\u7b5b\u9009\u590d\u6742\u7684\u5316\u5b66\u53cd\u5e94\uff0c\u7279\u522b\u662f\u73af\u5316\u53cd\u5e94\u3002", "result": "\u8bc4\u4f30\u4e86NNP\u5728\u4f30\u7b97\u6d3b\u5316\u80fd\u548c\u9884\u6d4b\u7acb\u4f53\u9009\u62e9\u6027\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u6210\u529f\u91cd\u73b0\u4e86\u5929\u7136\u4ea7\u7269\u5408\u6210\u4e2d\u7684\u590d\u6742\u5173\u952e\u6b65\u9aa4\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u8bba\u548c\u673a\u5668\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u4f18\u5316\u5305\u542b\u591a\u4e2a\u534f\u540c\u952e\u53d8\u7684\u590d\u6742\u53cd\u5e94\uff08\u5982\u73af\u5316\u53cd\u5e94\uff09\u7684\u673a\u7406\u641c\u7d22\u8fc7\u7a0b\uff0c\u5e76\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u52bf\uff08NNP\uff09AIMNet2-rxn\u6765\u8bc4\u4f30\u53cd\u5e94\u8def\u5f84\u548c\u9884\u6d4b\u6d3b\u5316\u80fd\u4e0e\u7acb\u4f53\u9009\u62e9\u6027\u3002"}}
{"id": "2507.10407", "categories": ["cs.CV", "cs.SC", "math.AG", "68W30"], "pdf": "https://arxiv.org/pdf/2507.10407", "abs": "https://arxiv.org/abs/2507.10407", "authors": ["Timothy Duff"], "title": "Numerically Computing Galois Groups of Minimal Problems", "comment": "abstract accompanying invited tutorial at ISSAC 2025; 10 pages w/\n  references", "summary": "I discuss a seemingly unlikely confluence of topics in algebra, numerical\ncomputation, and computer vision. The motivating problem is that of solving\nmultiples instances of a parametric family of systems of algebraic (polynomial\nor rational function) equations. No doubt already of interest to ISSAC\nattendees, this problem arises in the context of robust model-fitting paradigms\ncurrently utilized by the computer vision community (namely \"Random Sampling\nand Consensus\", aka \"RanSaC\".) This talk will give an overview of work in the\nlast 5+ years that aspires to measure the intrinsic difficulty of solving such\nparametric systems, and makes strides towards practical solutions.", "AI": {"tldr": "This paper explores the connection between algebra, numerical computation, and computer vision, focusing on solving parametric systems of equations relevant to computer vision's RanSaC algorithm. It reviews recent work to assess the difficulty and find practical solutions.", "motivation": "The motivating problem is solving multiple instances of a parametric family of systems of algebraic equations, arising in robust model-fitting paradigms in computer vision (RanSaC).", "method": "The paper reviews work from the last 5+ years to measure the difficulty of solving parametric systems.", "result": "The paper makes strides towards practical solutions for solving these systems.", "conclusion": "The paper discusses the intrinsic difficulty of solving parametric systems of algebraic equations and presents practical solutions."}}
{"id": "2507.10401", "categories": ["cs.LG", "math.PR"], "pdf": "https://arxiv.org/pdf/2507.10401", "abs": "https://arxiv.org/abs/2507.10401", "authors": ["Ryan Bausback", "Jingqiao Tang", "Lu Lu", "Feng Bao", "Toan Huynh"], "title": "Stochastic Operator Network: A Stochastic Maximum Principle Based Approach to Operator Learning", "comment": null, "summary": "We develop a novel framework for uncertainty quantification in operator\nlearning, the Stochastic Operator Network (SON). SON combines the stochastic\noptimal control concepts of the Stochastic Neural Network (SNN) with the\nDeepONet. By formulating the branch net as an SDE and backpropagating through\nthe adjoint BSDE, we replace the gradient of the loss function with the\ngradient of the Hamiltonian from Stohastic Maximum Principle in the SGD update.\nThis allows SON to learn the uncertainty present in operators through its\ndiffusion parameters. We then demonstrate the effectiveness of SON when\nreplicating several noisy operators in 2D and 3D.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2507.10432", "categories": ["cs.CV", "I.4.7"], "pdf": "https://arxiv.org/pdf/2507.10432", "abs": "https://arxiv.org/abs/2507.10432", "authors": ["Qiang Li", "Qingsen Yan", "Haojian Huang", "Peng Wu", "Haokui Zhang", "Yanning Zhang"], "title": "Text-Visual Semantic Constrained AI-Generated Image Quality Assessment", "comment": "9 pages, 5 figures, Accepted at ACMMM 2025", "summary": "With the rapid advancements in Artificial Intelligence Generated Image (AGI)\ntechnology, the accurate assessment of their quality has become an increasingly\nvital requirement. Prevailing methods typically rely on cross-modal models like\nCLIP or BLIP to evaluate text-image alignment and visual quality. However, when\napplied to AGIs, these methods encounter two primary challenges: semantic\nmisalignment and details perception missing. To address these limitations, we\npropose Text-Visual Semantic Constrained AI-Generated Image Quality Assessment\n(SC-AGIQA), a unified framework that leverages text-visual semantic constraints\nto significantly enhance the comprehensive evaluation of both text-image\nconsistency and perceptual distortion in AI-generated images. Our approach\nintegrates key capabilities from multiple models and tackles the aforementioned\nchallenges by introducing two core modules: the Text-assisted Semantic\nAlignment Module (TSAM), which leverages Multimodal Large Language Models\n(MLLMs) to bridge the semantic gap by generating an image description and\ncomparing it against the original prompt for a refined consistency check, and\nthe Frequency-domain Fine-Grained Degradation Perception Module (FFDPM), which\ndraws inspiration from Human Visual System (HVS) properties by employing\nfrequency domain analysis combined with perceptual sensitivity weighting to\nbetter quantify subtle visual distortions and enhance the capture of\nfine-grained visual quality details in images. Extensive experiments conducted\non multiple benchmark datasets demonstrate that SC-AGIQA outperforms existing\nstate-of-the-art methods. The code is publicly available at\nhttps://github.com/mozhu1/SC-AGIQA.", "AI": {"tldr": "\u63d0\u51fa SC-AGIQA \u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408 MLLMs \u548c HVS \u7279\u6027\u6765\u6539\u8fdb AI \u751f\u6210\u56fe\u50cf\u7684\u8d28\u91cf\u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u548c\u7ec6\u8282\u611f\u77e5\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u968f\u7740 AI \u751f\u6210\u56fe\u50cf (AGI) \u6280\u672f\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u51c6\u786e\u8bc4\u4f30\u5176\u8d28\u91cf\u5df2\u6210\u4e3a\u4e00\u9879\u65e5\u76ca\u91cd\u8981\u7684\u9700\u6c42\u3002\u73b0\u6709\u7684\u8de8\u6a21\u6001\u6a21\u578b\uff08\u5982 CLIP \u6216 BLIP\uff09\u5728\u5e94\u7528\u4e8e AGI \u65f6\u9762\u4e34\u8bed\u4e49\u4e0d\u5339\u914d\u548c\u7ec6\u8282\u611f\u77e5\u7f3a\u5931\u4e24\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SC-AGIQA \u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u6587\u672c-\u89c6\u89c9\u8bed\u4e49\u7ea6\u675f\u6765\u589e\u5f3a\u5bf9 AI \u751f\u6210\u56fe\u50cf\u7684\u6587\u672c-\u56fe\u50cf\u4e00\u81f4\u6027\u548c\u611f\u77e5\u5931\u771f\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u3002\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a1. \u6587\u672c\u8f85\u52a9\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757 (TSAM)\uff0c\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b (MLLMs) \u751f\u6210\u56fe\u50cf\u63cf\u8ff0\u5e76\u4e0e\u539f\u59cb\u63d0\u793a\u8fdb\u884c\u6bd4\u8f83\uff0c\u4ee5\u7f29\u5c0f\u8bed\u4e49\u5dee\u8ddd\u5e76\u6539\u8fdb\u4e00\u81f4\u6027\u68c0\u67e5\u30022. \u9891\u57df\u7ec6\u7c92\u5ea6\u9000\u5316\u611f\u77e5\u6a21\u5757 (FFDPM)\uff0c\u501f\u9274\u4eba\u773c\u89c6\u89c9\u7cfb\u7edf (HVS) \u7684\u7279\u6027\uff0c\u91c7\u7528\u9891\u57df\u5206\u6790\u5e76\u7ed3\u5408\u611f\u77e5\u654f\u611f\u5ea6\u52a0\u6743\uff0c\u4ee5\u66f4\u597d\u5730\u91cf\u5316\u7ec6\u5fae\u7684\u89c6\u89c9\u5931\u771f\u5e76\u6355\u6349\u56fe\u50cf\u7684\u7ec6\u8282\u89c6\u89c9\u8d28\u91cf\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSC-AGIQA \u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "SC-AGIQA \u6846\u67b6\u901a\u8fc7\u6574\u5408\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u548c\u57fa\u4e8e\u4eba\u773c\u89c6\u89c9\u7cfb\u7edf\u7684\u9891\u7387\u57df\u5206\u6790\uff0c\u80fd\u591f\u6709\u6548\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8bc4\u4f30 AI \u751f\u6210\u56fe\u50cf\u65f6\u9047\u5230\u7684\u8bed\u4e49\u4e0d\u5339\u914d\u548c\u7ec6\u8282\u611f\u77e5\u7f3a\u5931\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2507.10409", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10409", "abs": "https://arxiv.org/abs/2507.10409", "authors": ["Amine Lbath", "Ibtissam Labriji"], "title": "Energy Efficiency in AI for 5G and Beyond: A DeepRx Case Study", "comment": null, "summary": "This study addresses the challenge of balancing energy efficiency with\nperformance in AI/ML models, focusing on DeepRX, a deep learning receiver based\non a fully convolutional ResNet architecture. We evaluate the energy\nconsumption of DeepRX, considering factors including FLOPs/Watt and\nFLOPs/clock, and find consistency between estimated and actual energy usage,\ninfluenced by memory access patterns. The research extends to comparing energy\ndynamics during training and inference phases. A key contribution is the\napplication of knowledge distillation (KD) to train a compact DeepRX\n\\textit{student} model that emulates the performance of the \\textit{teacher}\nmodel but with reduced energy consumption. We experiment with different student\nmodel sizes, optimal teacher sizes, and KD hyperparameters. Performance is\nmeasured by comparing the Bit Error Rate (BER) performance versus\nSignal-to-Interference \\& Noise Ratio (SINR) values of the distilled model and\na model trained from scratch. The distilled models demonstrate a lower error\nfloor across SINR levels, highlighting the effectiveness of KD in achieving\nenergy-efficient AI solutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u6280\u672f\uff0c\u6210\u529f\u8bad\u7ec3\u4e86\u4e00\u4e2a\u80fd\u8017\u66f4\u4f4e\u7684DeepRX\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u4fdd\u6301\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u8282\u80fd\uff0c\u5e76\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3AI/ML\u6a21\u578b\u5728\u80fd\u6548\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8DeepRX\u8fd9\u4e00\u57fa\u4e8e\u5168\u5377\u79efResNet\u67b6\u6784\u7684\u6df1\u5ea6\u5b66\u4e60\u63a5\u6536\u5668\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDeepRX\u7684\u6df1\u5ea6\u5b66\u4e60\u63a5\u6536\u5668\uff0c\u8be5\u63a5\u6536\u5668\u57fa\u4e8e\u5168\u5377\u79efResNet\u67b6\u6784\u3002\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e86DeepRX\u7684\u80fd\u8017\uff0c\u5e76\u8003\u8651\u4e86FLOPs/Watt\u548cFLOPs/clock\u7b49\u56e0\u7d20\uff0c\u5c06\u4f30\u8ba1\u80fd\u8017\u4e0e\u5b9e\u9645\u80fd\u8017\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u540c\u65f6\u5206\u6790\u4e86\u8bad\u7ec3\u548c\u63a8\u7406\u9636\u6bb5\u7684\u80fd\u8017\u52a8\u6001\u3002\u6b64\u5916\uff0c\u7814\u7a76\u5e94\u7528\u4e86\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6280\u672f\u6765\u8bad\u7ec3\u4e00\u4e2a\u7d27\u51d1\u7684\u5b66\u751f\u6a21\u578b\uff0c\u5e76\u5bf9\u4e0d\u540c\u7684\u5b66\u751f\u6a21\u578b\u5927\u5c0f\u3001\u6559\u5e08\u6a21\u578b\u5927\u5c0f\u548cKD\u8d85\u53c2\u6570\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cDeepRX\u7684\u4f30\u8ba1\u80fd\u8017\u4e0e\u5b9e\u9645\u80fd\u8017\u57fa\u672c\u4e00\u81f4\uff0c\u4e14\u5185\u5b58\u8bbf\u95ee\u6a21\u5f0f\u4f1a\u5f71\u54cd\u80fd\u8017\u3002\u901a\u8fc7\u77e5\u8bc6\u84b8\u998f\u8bad\u7ec3\u51fa\u7684\u5b66\u751f\u6a21\u578b\u5728\u6bd4\u7279\u9519\u8bef\u7387\uff08BER\uff09\u4e0e\u4fe1\u566a\u6bd4\uff08SINR\uff09\u7684\u5173\u7cfb\u4e0a\uff0c\u8868\u73b0\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u8bef\u5dee\u4e0b\u9650\u65b9\u9762\u663e\u793a\u51fa\u4f18\u52bf\u3002", "conclusion": "\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u88ab\u8bc1\u660e\u53ef\u4ee5\u6709\u6548\u5730\u8bad\u7ec3\u51fa\u80fd\u6548\u66f4\u9ad8\u7684\u7d27\u51d1\u578bDeepRX\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u8f83\u4f4e\u7684\u8bef\u5dee\u4e0b\u9650\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4ece\u5934\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u4e86KD\u5728\u5b9e\u73b0\u8282\u80fdAI\u89e3\u51b3\u65b9\u6848\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10437", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10437", "abs": "https://arxiv.org/abs/2507.10437", "authors": ["Shanshan Zhong", "Jiawei Peng", "Zehan Zheng", "Zhongzhan Huang", "Wufei Ma", "Guofeng Zhang", "Qihao Liu", "Alan Yuille", "Jieneng Chen"], "title": "4D-Animal: Freely Reconstructing Animatable 3D Animals from Videos", "comment": null, "summary": "Existing methods for reconstructing animatable 3D animals from videos\ntypically rely on sparse semantic keypoints to fit parametric models. However,\nobtaining such keypoints is labor-intensive, and keypoint detectors trained on\nlimited animal data are often unreliable. To address this, we propose\n4D-Animal, a novel framework that reconstructs animatable 3D animals from\nvideos without requiring sparse keypoint annotations. Our approach introduces a\ndense feature network that maps 2D representations to SMAL parameters,\nenhancing both the efficiency and stability of the fitting process.\nFurthermore, we develop a hierarchical alignment strategy that integrates\nsilhouette, part-level, pixel-level, and temporal cues from pre-trained 2D\nvisual models to produce accurate and temporally coherent reconstructions\nacross frames. Extensive experiments demonstrate that 4D-Animal outperforms\nboth model-based and model-free baselines. Moreover, the high-quality 3D assets\ngenerated by our method can benefit other 3D tasks, underscoring its potential\nfor large-scale applications. The code is released at\nhttps://github.com/zhongshsh/4D-Animal.", "AI": {"tldr": "4D-Animal\uff1a\u4e00\u79cd\u4ece\u89c6\u9891\u91cd\u5efa3D\u52a8\u7269\u7684\u65b0\u65b9\u6cd5\uff0c\u65e0\u9700\u5173\u952e\u70b9\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u4ece\u89c6\u9891\u91cd\u5efa\u53ef\u9a71\u52a83D\u52a8\u7269\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7a00\u758f\u8bed\u4e49\u5173\u952e\u70b9\uff0c\u800c\u8fd9\u4e9b\u5173\u952e\u70b9\u7684\u83b7\u53d6\u52b3\u52a8\u5bc6\u96c6\u4e14\u4e0d\u51c6\u786e\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u4e0d\u4f9d\u8d56\u5173\u952e\u70b9\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a4D-Animal\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4e0d\u4f9d\u8d56\u7a00\u758f\u5173\u952e\u70b9\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u5bc6\u96c6\u7279\u5f81\u7f51\u7edc\uff0c\u5c062D\u8868\u793a\u6620\u5c04\u5230SMAL\u53c2\u6570\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u79cd\u5206\u5c42\u5bf9\u9f50\u7b56\u7565\uff0c\u6574\u5408\u4e86\u6765\u81ea\u9884\u8bad\u7ec32D\u89c6\u89c9\u6a21\u578b\u7684\u8f6e\u5ed3\u3001\u90e8\u4ef6\u7ea7\u3001\u50cf\u7d20\u7ea7\u548c\u65f6\u95f4\u7ebf\u7d22\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4e14\u65f6\u95f4\u8fde\u8d2f\u7684\u8de8\u5e27\u91cd\u5efa\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c4D-Animal\u5728\u51c6\u786e\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u4e8e\u6a21\u578b\u548c\u65e0\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "conclusion": "4D-Animal \u6846\u67b6\u5728\u65e0\u9700\u7a00\u758f\u5173\u952e\u70b9\u7684\u60c5\u51b5\u4e0b\uff0c\u80fd\u591f\u4ece\u89c6\u9891\u4e2d\u91cd\u5efa\u53ef\u9a71\u52a8\u76843D\u52a8\u7269\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u51c6\u786e\u6027\u548c\u65f6\u95f4\u8fde\u8d2f\u6027\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751f\u6210\u76843D\u8d44\u4ea7\u8fd8\u53ef\u4ee5\u5e94\u7528\u4e8e\u5176\u4ed63D\u4efb\u52a1\u3002"}}
{"id": "2507.10449", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10449", "abs": "https://arxiv.org/abs/2507.10449", "authors": ["Hongyong Han", "Wei Wang", "Gaowei Zhang", "Mingjie Li", "Yi Wang"], "title": "CoralVQA: A Large-Scale Visual Question Answering Dataset for Coral Reef Image Understanding", "comment": null, "summary": "Coral reefs are vital yet vulnerable ecosystems that require continuous\nmonitoring to support conservation. While coral reef images provide essential\ninformation in coral monitoring, interpreting such images remains challenging\ndue to the need for domain expertise. Visual Question Answering (VQA), powered\nby Large Vision-Language Models (LVLMs), has great potential in user-friendly\ninteraction with coral reef images. However, applying VQA to coral imagery\ndemands a dedicated dataset that addresses two key challenges: domain-specific\nannotations and multidimensional questions. In this work, we introduce\nCoralVQA, the first large-scale VQA dataset for coral reef analysis. It\ncontains 12,805 real-world coral images from 67 coral genera collected from 3\noceans, along with 277,653 question-answer pairs that comprehensively assess\necological and health-related conditions. To construct this dataset, we develop\na semi-automatic data construction pipeline in collaboration with marine\nbiologists to ensure both scalability and professional-grade data quality.\nCoralVQA presents novel challenges and provides a comprehensive benchmark for\nstudying vision-language reasoning in the context of coral reef images. By\nevaluating several state-of-the-art LVLMs, we reveal key limitations and\nopportunities. These insights form a foundation for future LVLM development,\nwith a particular emphasis on supporting coral conservation efforts.", "AI": {"tldr": "\u521b\u5efa\u4e86CoralVQA\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73ca\u745a\u7901\u56fe\u50cf\u5206\u6790\u7684\u6311\u6218\uff0c\u8bc4\u4f30\u4e86LVLM\uff0c\u5e76\u4e3a\u73ca\u745a\u7901\u4fdd\u62a4\u63d0\u4f9b\u4e86\u652f\u6301\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73ca\u745a\u7901\u56fe\u50cf\u89e3\u91ca\u7684\u6311\u6218\u4ee5\u53ca\u73b0\u6709VQA\u6570\u636e\u96c6\u5728\u73ca\u745a\u7901\u5206\u6790\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u7b2c\u4e00\u4e2a\u4e13\u95e8\u9488\u5bf9\u73ca\u745a\u7901\u5206\u6790\u7684\u5927\u578bVQA\u6570\u636e\u96c6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b12,805\u5f20\u771f\u5b9e\u73ca\u745a\u56fe\u50cf\u548c277,653\u4e2a\u95ee\u7b54\u5bf9\u7684\u534a\u81ea\u52a8\u6570\u636e\u96c6\u6784\u5efa\u6d41\u7a0b\uff0c\u5e76\u4e0e\u6d77\u6d0b\u751f\u7269\u5b66\u5bb6\u5408\u4f5c\u4ee5\u4fdd\u8bc1\u6570\u636e\u8d28\u91cf\u548c\u89c4\u6a21\u3002", "result": "\u6784\u5efa\u4e86CoralVQA\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u6765\u81ea\u4e09\u5927\u6d0b67\u4e2a\u73ca\u745a\u5c5e\u7684\u56fe\u50cf\uff0c\u8bc4\u4f30\u4e86\u73b0\u6709LVLM\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684LVLM\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "conclusion": "CoralVQA\u662f\u4e00\u4e2a\u5927\u578b\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u65e8\u5728\u4fc3\u8fdb\u73ca\u745a\u7901\u56fe\u50cf\u7684\u5206\u6790\u548c\u7528\u6237\u53cb\u597d\u4ea4\u4e92\uff0c\u4e3a\u7814\u7a76\u548cLVLM\u53d1\u5c55\u5960\u5b9a\u57fa\u7840\uff0c\u5e76\u652f\u6301\u73ca\u745a\u7901\u4fdd\u62a4\u5de5\u4f5c\u3002"}}
{"id": "2507.10425", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2507.10425", "abs": "https://arxiv.org/abs/2507.10425", "authors": ["Alvaro H. C. Correia", "Christos Louizos"], "title": "Non-exchangeable Conformal Prediction with Optimal Transport: Tackling Distribution Shifts with Unlabeled Data", "comment": null, "summary": "Conformal prediction is a distribution-free uncertainty quantification method\nthat has gained popularity in the machine learning community due to its\nfinite-sample guarantees and ease of use. Its most common variant, dubbed split\nconformal prediction, is also computationally efficient as it boils down to\ncollecting statistics of the model predictions on some calibration data not yet\nseen by the model. Nonetheless, these guarantees only hold if the calibration\nand test data are exchangeable, a condition that is difficult to verify and\noften violated in practice due to so-called distribution shifts. The literature\nis rife with methods to mitigate the loss in coverage in this non-exchangeable\nsetting, but these methods require some prior information on the type of\ndistribution shift to be expected at test time. In this work, we study this\nproblem via a new perspective, through the lens of optimal transport, and show\nthat it is possible to estimate the loss in coverage and mitigate it in case of\ndistribution shift.", "AI": {"tldr": "Conformal Prediction \u5728\u5b58\u5728\u5206\u5e03\u504f\u79fb\u65f6\u8986\u76d6\u7387\u4f1a\u4e0b\u964d\uff0c\u672c\u6587\u5229\u7528\u6700\u4f18\u4f20\u8f93\u6280\u672f\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684 Conformal Prediction \u65b9\u6cd5\u5728\u5b58\u5728\u5206\u5e03\u504f\u79fb\u65f6\uff0c\u5176\u4fdd\u8bc1\u4f1a\u5931\u6548\uff0c\u800c\u73b0\u6709\u7684\u7f13\u89e3\u65b9\u6cd5\u9700\u8981\u9884\u5148\u77e5\u9053\u5206\u5e03\u504f\u79fb\u7684\u7c7b\u578b\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\u6765\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u7684\u89d2\u5ea6\u7814\u7a76\u5206\u5e03\u504f\u79fb\u5bf9 Conformal Prediction \u8986\u76d6\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4f30\u8ba1\u548c\u7f13\u89e3\u8986\u76d6\u7387\u635f\u5931\u7684\u65b9\u6cd5\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u6765\u4f30\u8ba1\u548c\u7f13\u89e3 Conformal Prediction \u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u8986\u76d6\u7387\u635f\u5931\u7684\u65b9\u6cd5\u3002", "conclusion": "Conformal prediction \u7684\u8986\u76d6\u7387\u635f\u5931\u53ef\u4ee5\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u6765\u4f30\u8ba1\u548c\u7f13\u89e3\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u5206\u5e03\u504f\u79fb\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2507.10461", "categories": ["cs.CV", "cs.AI", "cs.LG", "cs.MM", "eess.IV"], "pdf": "https://arxiv.org/pdf/2507.10461", "abs": "https://arxiv.org/abs/2507.10461", "authors": ["Tao Tang", "Chengxu Yang"], "title": "RAPNet: A Receptive-Field Adaptive Convolutional Neural Network for Pansharpening", "comment": "To appear in the proceedings of the 6th International Conference on\n  Artificial Intelligence and Electromechanical Automation (AIEA 2025). 5\n  pages, 6 figures", "summary": "Pansharpening refers to the process of integrating a high resolution\npanchromatic (PAN) image with a lower resolution multispectral (MS) image to\ngenerate a fused product, which is pivotal in remote sensing. Despite the\neffectiveness of CNNs in addressing this challenge, they are inherently\nconstrained by the uniform application of convolutional kernels across all\nspatial positions, overlooking local content variations. To overcome this\nissue, we introduce RAPNet, a new architecture that leverages content-adaptive\nconvolution. At its core, RAPNet employs the Receptive-field Adaptive\nPansharpening Convolution (RAPConv), designed to produce spatially adaptive\nkernels responsive to local feature context, thereby enhancing the precision of\nspatial detail extraction. Additionally, the network integrates the\nPansharpening Dynamic Feature Fusion (PAN-DFF) module, which incorporates an\nattention mechanism to achieve an optimal balance between spatial detail\nenhancement and spectral fidelity. Comprehensive evaluations on publicly\navailable datasets confirm that RAPNet delivers superior performance compared\nto existing approaches, as demonstrated by both quantitative metrics and\nqualitative assessments. Ablation analyses further substantiate the\neffectiveness of the proposed adaptive components.", "AI": {"tldr": "RAPNet\u901a\u8fc7\u5185\u5bb9\u81ea\u9002\u5e94\u5377\u79ef\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u5168\u8272\u878d\u5408\u7684\u7cbe\u5ea6\u548c\u5149\u8c31\u4fdd\u771f\u5ea6\u3002", "motivation": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5728\u5904\u7406\u878d\u5408\u4efb\u52a1\u65f6\u5b58\u5728\u6240\u6709\u7a7a\u95f4\u4f4d\u7f6e\u5747\u4e00\u5e94\u7528\u5377\u79ef\u6838\u7684\u5c40\u9650\u6027\uff0c\u5ffd\u7565\u4e86\u5c40\u90e8\u5185\u5bb9\u7684\u5dee\u5f02\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86RAPNet\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5229\u7528\u4e86\u5185\u5bb9\u81ea\u9002\u5e94\u5377\u79ef\u3002", "method": "RAPNet\u67b6\u6784\u7684\u6838\u5fc3\u662f\u611f\u53d7\u91ce\u81ea\u9002\u5e94\u878d\u5408\u5377\u79ef\uff08RAPConv\uff09\uff0c\u65e8\u5728\u751f\u6210\u5bf9\u5c40\u90e8\u7279\u5f81\u4e0a\u4e0b\u6587\u654f\u611f\u7684\u7a7a\u95f4\u81ea\u9002\u5e94\u5377\u79ef\u6838\uff0c\u4ee5\u63d0\u9ad8\u7a7a\u95f4\u7ec6\u8282\u63d0\u53d6\u7684\u7cbe\u5ea6\u3002\u6b64\u5916\uff0c\u8be5\u7f51\u7edc\u96c6\u6210\u4e86\u878d\u5408\u52a8\u6001\u7279\u5f81\u878d\u5408\uff08PAN-DFF\uff09\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5305\u542b\u4e00\u4e2a\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u5b9e\u73b0\u7a7a\u95f4\u7ec6\u8282\u589e\u5f3a\u548c\u5149\u8c31\u4fdd\u771f\u5ea6\u4e4b\u95f4\u7684\u6700\u4f73\u5e73\u8861\u3002", "result": "RAPNet\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cRAPNet\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u8bc4\u4f30\u65b9\u9762\u5747\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6d88\u878d\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "RAPNet\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u7684\u7efc\u5408\u8bc4\u4f30\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cRAPNet\u5728\u5b9a\u91cf\u6307\u6807\u548c\u5b9a\u6027\u8bc4\u4f30\u65b9\u9762\u5747\u8868\u73b0\u51fa\u66f4\u4f18\u8d8a\u7684\u6027\u80fd\u3002\u6d88\u878d\u5206\u6790\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7684\u81ea\u9002\u5e94\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2507.10434", "categories": ["cs.LG", "cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10434", "abs": "https://arxiv.org/abs/2507.10434", "authors": ["Giacomo Cignoni", "Andrea Cossu", "Alexandra Gomez-Villa", "Joost van de Weijer", "Antonio Carta"], "title": "CLA: Latent Alignment for Online Continual Self-Supervised Learning", "comment": "Accepted at CoLLAs 2025 conference", "summary": "Self-supervised learning (SSL) is able to build latent representations that\ngeneralize well to unseen data. However, only a few SSL techniques exist for\nthe online CL setting, where data arrives in small minibatches, the model must\ncomply with a fixed computational budget, and task boundaries are absent. We\nintroduce Continual Latent Alignment (CLA), a novel SSL strategy for Online CL\nthat aligns the representations learned by the current model with past\nrepresentations to mitigate forgetting. We found that our CLA is able to speed\nup the convergence of the training process in the online scenario,\noutperforming state-of-the-art approaches under the same computational budget.\nSurprisingly, we also discovered that using CLA as a pretraining protocol in\nthe early stages of pretraining leads to a better final performance when\ncompared to a full i.i.d. pretraining.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Continual Latent Alignment (CLA) \u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u9f50\u6a21\u578b\u8868\u793a\u6765\u9632\u6b62\u9057\u5fd8\uff0c\u5728\u52a0\u901f\u6536\u655b\u548c\u63d0\u5347\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u751a\u81f3\u5728\u65e9\u671f\u9884\u8bad\u7ec3\u9636\u6bb5\u4f7f\u7528 CLA \u6bd4 i.i.d. \u9884\u8bad\u7ec3\u6548\u679c\u66f4\u597d\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u81ea\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u5728\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u8bbe\u7f6e\u4e2d\u9762\u4e34\u7684\u5c40\u9650\u6027\u3002\u73b0\u6709\u7684 SSL \u6280\u672f\u5728\u9700\u8981\u5904\u7406\u5c0f\u6279\u91cf\u6570\u636e\u3001\u9075\u5b88\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4ee5\u53ca\u7f3a\u4e4f\u660e\u786e\u4efb\u52a1\u8fb9\u754c\u7684\u5728\u7ebf CL \u73af\u5883\u4e2d\u7684\u5e94\u7528\u8f83\u5c11\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684 SSL \u7b56\u7565\uff0c\u4ee5\u9002\u5e94\u8fd9\u4e9b\u6311\u6218\u5e76\u63d0\u5347\u6a21\u578b\u5728\u5728\u7ebf CL \u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662f\u5728\u7f13\u89e3\u9057\u5fd8\u548c\u52a0\u901f\u6536\u655b\u65b9\u9762\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Continual Latent Alignment (CLA) \u7684\u81ea\u76d1\u7763\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u89e3\u51b3\u5728\u7ebf\u6301\u7eed\u5b66\u4e60\uff08CL\uff09\u4e2d\u7684\u6311\u6218\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u9f50\u5f53\u524d\u6a21\u578b\u4e0e\u5386\u53f2\u6a21\u578b\u6240\u5b66\u4e60\u5230\u7684\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u6709\u6548\u7f13\u89e3\u6a21\u578b\u9057\u5fd8\u95ee\u9898\u3002\u5177\u4f53\u800c\u8a00\uff0cCLA \u88ab\u8bbe\u8ba1\u7528\u4e8e\u5904\u7406\u6570\u636e\u4ee5\u5c0f\u6279\u91cf\u5f62\u5f0f\u5230\u8fbe\u3001\u6a21\u578b\u9700\u8981\u9075\u5b88\u56fa\u5b9a\u8ba1\u7b97\u9884\u7b97\u4ee5\u53ca\u4efb\u52a1\u8fb9\u754c\u7f3a\u5931\u7684\u5728\u7ebf CL \u73af\u5883\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\uff0c\u6211\u4eec\u53d1\u73b0 CLA \u7b56\u7565\u80fd\u591f\u52a0\u901f\u5728\u7ebf\u573a\u666f\u4e0b\u7684\u8bad\u7ec3\u6536\u655b\u8fc7\u7a0b\uff0c\u5e76\u4e14\u5728\u76f8\u540c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\uff0c\u5176\u6027\u80fd\u8d85\u8fc7\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u53d1\u73b0\uff0c\u5728\u9884\u8bad\u7ec3\u65e9\u671f\u9636\u6bb5\u4f7f\u7528 CLA \u4f5c\u4e3a\u9884\u8bad\u7ec3\u534f\u8bae\uff0c\u76f8\u6bd4\u4e8e\u5b8c\u6574\u7684\u72ec\u7acb\u540c\u5206\u5e03\uff08i.i.d.\uff09\u9884\u8bad\u7ec3\uff0c\u80fd\u591f\u53d6\u5f97\u66f4\u4f18\u8d8a\u7684\u6700\u7ec8\u6027\u80fd\u3002", "conclusion": "Continual Latent Alignment (CLA) \u662f\u4e00\u79cd\u65b0\u9896\u7684 SSL \u7b56\u7565\uff0c\u7528\u4e8e\u5728\u7ebf\u6301\u7eed\u5b66\u4e60 (CL) \u8bbe\u7f6e\uff0c\u901a\u8fc7\u5bf9\u9f50\u5f53\u524d\u6a21\u578b\u4e0e\u5386\u53f2\u6a21\u578b\u6240\u5b66\u8868\u793a\u6765\u7f13\u89e3\u9057\u5fd8\u3002CLA \u4e0d\u4ec5\u5728\u5728\u7ebf\u573a\u666f\u4e0b\u52a0\u901f\u4e86\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6536\u655b\uff0c\u800c\u4e14\u5728\u76f8\u540c\u7684\u8ba1\u7b97\u9884\u7b97\u4e0b\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5c06 CLA \u4f5c\u4e3a\u9884\u8bad\u7ec3\u534f\u8bae\u5728\u9884\u8bad\u7ec3\u65e9\u671f\u9636\u6bb5\u4f7f\u7528\uff0c\u4e0e\u5b8c\u6574\u7684\u72ec\u7acb\u540c\u5206\u5e03 (i.i.d.) \u9884\u8bad\u7ec3\u76f8\u6bd4\uff0c\u80fd\u591f\u83b7\u5f97\u66f4\u597d\u7684\u6700\u7ec8\u6027\u80fd\u3002"}}
{"id": "2507.10470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10470", "abs": "https://arxiv.org/abs/2507.10470", "authors": ["Zhicun Yin", "Junjie Chen", "Ming Liu", "Zhixin Wang", "Fan Li", "Renjing Pei", "Xiaoming Li", "Rynson W. H. Lau", "Wangmeng Zuo"], "title": "RefSTAR: Blind Facial Image Restoration with Reference Selection, Transfer, and Reconstruction", "comment": null, "summary": "Blind facial image restoration is highly challenging due to unknown complex\ndegradations and the sensitivity of humans to faces. Although existing methods\nintroduce auxiliary information from generative priors or high-quality\nreference images, they still struggle with identity preservation problems,\nmainly due to improper feature introduction on detailed textures. In this\npaper, we focus on effectively incorporating appropriate features from\nhigh-quality reference images, presenting a novel blind facial image\nrestoration method that considers reference selection, transfer, and\nreconstruction (RefSTAR). In terms of selection, we construct a reference\nselection (RefSel) module. For training the RefSel module, we construct a\nRefSel-HQ dataset through a mask generation pipeline, which contains annotating\nmasks for 10,000 ground truth-reference pairs. As for the transfer, due to the\ntrivial solution in vanilla cross-attention operations, a feature fusion\nparadigm is designed to force the features from the reference to be integrated.\nFinally, we propose a reference image reconstruction mechanism that further\nensures the presence of reference image features in the output image. The cycle\nconsistency loss is also redesigned in conjunction with the mask. Extensive\nexperiments on various backbone models demonstrate superior performance,\nshowing better identity preservation ability and reference feature transfer\nquality. Source code, dataset, and pre-trained models are available at\nhttps://github.com/yinzhicun/RefSTAR.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRefSTAR\u7684\u65b0\u578b\u76f2\u4eba\u8138\u90e8\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u8003\u9009\u62e9\u3001\u7279\u5f81\u878d\u5408\u548c\u91cd\u5efa\u6765\u6709\u6548\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u8eab\u4efd\u4fdd\u7559\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u76f2\u4eba\u8138\u90e8\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5728\u5f15\u5165\u751f\u6210\u5148\u9a8c\u6216\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u7684\u8f85\u52a9\u4fe1\u606f\u65f6\uff0c\u7531\u4e8e\u5bf9\u7ec6\u8282\u7eb9\u7406\u7684\u4e0d\u5f53\u7279\u5f81\u5f15\u5165\uff0c\u5e38\u5e38\u5728\u8eab\u4efd\u4fdd\u7559\u65b9\u9762\u9047\u5230\u56f0\u96be\u3002", "method": "RefSTAR\u65b9\u6cd5\u5305\u542b\u4e09\u4e2a\u5173\u952e\u90e8\u5206\uff1a1. \u53c2\u8003\u9009\u62e9\uff08RefSel\uff09\u6a21\u5757\uff1a\u7528\u4e8e\u9009\u62e9\u5408\u9002\u7684\u53c2\u8003\u56fe\u50cf\u3002\u4e3a\u4e86\u8bad\u7ec3\u6b64\u6a21\u5757\uff0c\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b10,000\u4e2a\u771f\u5b9e-\u53c2\u8003\u5bf9\u7684\u6ce8\u91ca\u63a9\u7801\u7684RefSel-HQ\u6570\u636e\u96c6\u30022. \u7279\u5f81\u878d\u5408\u8303\u5f0f\uff1a\u7528\u4e8e\u5c06\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\u6709\u6548\u96c6\u6210\u5230\u6062\u590d\u8fc7\u7a0b\u4e2d\uff0c\u89e3\u51b3\u4e86\u6807\u51c6\u4ea4\u53c9\u6ce8\u610f\u529b\u64cd\u4f5c\u4e2d\u7684\u56fa\u6709\u95ee\u9898\u30023. \u53c2\u8003\u56fe\u50cf\u91cd\u5efa\u673a\u5236\uff1a\u786e\u4fdd\u6062\u590d\u540e\u7684\u56fe\u50cf\u4e2d\u5b58\u5728\u53c2\u8003\u56fe\u50cf\u7684\u7279\u5f81\uff0c\u5e76\u91cd\u65b0\u8bbe\u8ba1\u4e86\u4e0e\u63a9\u7801\u7ed3\u5408\u7684\u5468\u671f\u4e00\u81f4\u6027\u635f\u5931\u3002", "result": "\u5728\u5404\u79cd\u9aa8\u5e72\u6a21\u578b\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cRefSTAR\u65b9\u6cd5\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5728\u8eab\u4efd\u4fdd\u7559\u80fd\u529b\u548c\u53c2\u8003\u7279\u5f81\u4f20\u9012\u8d28\u91cf\u65b9\u9762\u5747\u6709\u63d0\u5347\u3002", "conclusion": "\u76f2\u4eba\u8138\u90e8\u56fe\u50cf\u6062\u590d\u7684\u73b0\u6709\u65b9\u6cd5\u5728\u8eab\u4efd\u4fdd\u7559\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff0c\u4e3b\u8981\u5f52\u56e0\u4e8e\u7ec6\u8282\u7eb9\u7406\u7684\u4e0d\u5f53\u7279\u5f81\u5f15\u5165\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRefSTAR\u7684\u65b0\u578b\u76f2\u4eba\u8138\u90e8\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u8003\u9009\u62e9\u3001\u4f20\u8f93\u548c\u91cd\u5efa\u6765\u6709\u6548\u6574\u5408\u9ad8\u5206\u8fa8\u7387\u53c2\u8003\u56fe\u50cf\u4e2d\u7684\u5408\u9002\u7279\u5f81\u3002"}}
{"id": "2507.10442", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10442", "abs": "https://arxiv.org/abs/2507.10442", "authors": ["Shivam Chandhok", "Wan-Cyuan Fan", "Vered Shwartz", "Vineeth N Balasubramanian", "Leonid Sigal"], "title": "Response Wide Shut? Surprising Observations in Basic Vision Language Model Capabilities", "comment": "Accepted at ACL 2025 (Main Conference)", "summary": "Vision-language Models (VLMs) have emerged as general-purpose tools for\naddressing a variety of complex computer vision problems. Such models have been\nshown to be highly capable, but, at the same time, lacking some basic visual\nunderstanding skills. In this paper, we set out to understand the limitations\nof SoTA VLMs on fundamental visual tasks by constructing a series of tests that\nprobe which components of design, specifically, may be lacking. Importantly, we\ngo significantly beyond the current benchmarks, which simply measure the final\nperformance of VLM response, by also comparing and contrasting it to the\nperformance of probes trained directly on features obtained from the visual\nencoder, intermediate vision-language projection and LLM-decoder output. In\ndoing so, we uncover shortcomings in VLMs and make a number of important\nobservations about their capabilities, robustness and how they process visual\ninformation. We hope our insights will guide progress in further improving\nVLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u4e86\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u8bbe\u8ba1\u4e13\u95e8\u7684\u6d4b\u8bd5\u6765\u63a2\u6d4b\u5176\u4e0d\u8db3\u4e4b\u5904\u3002\u7814\u7a76\u4e0d\u4ec5\u8bc4\u4f30\u4e86VLMs\u7684\u6700\u7ec8\u6027\u80fd\uff0c\u8fd8\u5c06\u5176\u4e0e\u76f4\u63a5\u5728\u4e0d\u540c\u6a21\u578b\u7ec4\u4ef6\uff08\u5982\u89c6\u89c9\u7f16\u7801\u5668\u7279\u5f81\u3001\u4e2d\u95f4\u89c6\u89c9-\u8bed\u8a00\u6295\u5f71\u548cLLM\u89e3\u7801\u5668\u8f93\u51fa\uff09\u4e0a\u8bad\u7ec3\u7684\u63a2\u9488\u6027\u80fd\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7814\u7a76\u7ed3\u679c\u63ed\u793a\u4e86VLMs\u5728\u80fd\u529b\u3001\u9c81\u68d2\u6027\u548c\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u6539\u8fdbVLMs\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u548c\u6307\u5bfc\u3002", "motivation": "\u4e86\u89e3SoTA VLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u6539\u8fdbVLMs\u63d0\u4f9b\u6307\u5bfc\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u4e00\u7cfb\u5217\u6d4b\u8bd5\u6765\u63a2\u6d4bSoTA VLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e0a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u76f4\u63a5\u5728\u89c6\u89c9\u7f16\u7801\u5668\u7279\u5f81\u3001\u4e2d\u95f4\u89c6\u89c9-\u8bed\u8a00\u6295\u5f71\u548cLLM\u89e3\u7801\u5668\u8f93\u51fa\u4e0a\u8bad\u7ec3\u7684\u63a2\u9488\u6027\u80fd\u8fdb\u884c\u6bd4\u8f83\u548c\u5bf9\u6bd4\u3002", "result": "\u63ed\u793a\u4e86VLMs\u7684\u4e0d\u8db3\u4e4b\u5904\uff0c\u5e76\u5bf9\u5176\u80fd\u529b\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u65b9\u5f0f\u63d0\u51fa\u4e86\u91cd\u8981\u89c2\u5bdf\u7ed3\u679c\u3002", "conclusion": "VLMs\u5728\u57fa\u672c\u89c6\u89c9\u4efb\u52a1\u4e2d\u5b58\u5728\u4e0d\u8db3\uff0c\u5176\u80fd\u529b\u3001\u9c81\u68d2\u6027\u4ee5\u53ca\u89c6\u89c9\u4fe1\u606f\u5904\u7406\u65b9\u5f0f\u6709\u5f85\u6539\u8fdb\u3002"}}
{"id": "2507.10473", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10473", "abs": "https://arxiv.org/abs/2507.10473", "authors": ["David G. Shatwell", "Ishan Rajendrakumar Dave", "Sirnam Swetha", "Mubarak Shah"], "title": "GT-Loc: Unifying When and Where in Images Through a Joint Embedding Space", "comment": "Accepted in ICCV2025", "summary": "Timestamp prediction aims to determine when an image was captured using only\nvisual information, supporting applications such as metadata correction,\nretrieval, and digital forensics. In outdoor scenarios, hourly estimates rely\non cues like brightness, hue, and shadow positioning, while seasonal changes\nand weather inform date estimation. However, these visual cues significantly\ndepend on geographic context, closely linking timestamp prediction to\ngeo-localization. To address this interdependence, we introduce GT-Loc, a novel\nretrieval-based method that jointly predicts the capture time (hour and month)\nand geo-location (GPS coordinates) of an image. Our approach employs separate\nencoders for images, time, and location, aligning their embeddings within a\nshared high-dimensional feature space. Recognizing the cyclical nature of time,\ninstead of conventional contrastive learning with hard positives and negatives,\nwe propose a temporal metric-learning objective providing soft targets by\nmodeling pairwise time differences over a cyclical toroidal surface. We present\nnew benchmarks demonstrating that our joint optimization surpasses previous\ntime prediction methods, even those using the ground-truth geo-location as an\ninput during inference. Additionally, our approach achieves competitive results\non standard geo-localization tasks, and the unified embedding space facilitates\ncompositional and text-based image retrieval.", "AI": {"tldr": "GT-Loc \u662f\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u53ef\u4ee5\u540c\u65f6\u9884\u6d4b\u56fe\u50cf\u7684\u62cd\u6444\u65f6\u95f4\u548c\u5730\u7406\u4f4d\u7f6e\uff0c\u5b83\u901a\u8fc7\u5c06\u65f6\u95f4\u548c\u4f4d\u7f6e\u4fe1\u606f\u7f16\u7801\u5230\u4e00\u4e2a\u5171\u4eab\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u4e00\u79cd\u65b0\u9896\u7684\u65f6\u95f4\u5ea6\u91cf\u5b66\u4e60\u65b9\u6cd5\u6765\u5904\u7406\u65f6\u95f4\u7684\u5468\u671f\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u65f6\u95f4\u9884\u6d4b\u7684\u76ee\u6807\u662f\u4ec5\u4f7f\u7528\u89c6\u89c9\u4fe1\u606f\u786e\u5b9a\u56fe\u50cf\u7684\u62cd\u6444\u65f6\u95f4\uff0c\u8fd9\u652f\u6301\u5143\u6570\u636e\u6821\u6b63\u3001\u68c0\u7d22\u548c\u6570\u5b57\u53d6\u8bc1\u7b49\u5e94\u7528\u3002\u5728\u5ba4\u5916\u573a\u666f\u4e2d\uff0c\u6bcf\u5c0f\u65f6\u7684\u4f30\u8ba1\u4f9d\u8d56\u4e8e\u4eae\u5ea6\u3001\u8272\u8c03\u548c\u9634\u5f71\u4f4d\u7f6e\u7b49\u7ebf\u7d22\uff0c\u800c\u5b63\u8282\u6027\u53d8\u5316\u548c\u5929\u6c14\u5219\u4e3a\u65e5\u671f\u4f30\u8ba1\u63d0\u4f9b\u4fe1\u606f\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u89c6\u89c9\u7ebf\u7d22\u5728\u5f88\u5927\u7a0b\u5ea6\u4e0a\u4f9d\u8d56\u4e8e\u5730\u7406\u80cc\u666f\uff0c\u5c06\u65f6\u95f4\u9884\u6d4b\u4e0e\u5730\u7406\u5b9a\u4f4d\u7d27\u5bc6\u8054\u7cfb\u8d77\u6765\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u79cd\u76f8\u4e92\u4f9d\u8d56\u6027\uff0cGT-Loc \u88ab\u5f15\u5165\u4ee5\u8054\u5408\u9884\u6d4b\u56fe\u50cf\u7684\u62cd\u6444\u65f6\u95f4\u548c\u5730\u7406\u4f4d\u7f6e\u3002", "method": "GT-Loc \u662f\u4e00\u79cd\u65b0\u7684\u68c0\u7d22\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u5355\u72ec\u7684\u56fe\u50cf\u3001\u65f6\u95f4\u548c\u4f4d\u7f6e\u7f16\u7801\u5668\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u5d4c\u5165\u5bf9\u9f50\u5230\u4e00\u4e2a\u5171\u4eab\u7684\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u3002\u4e3a\u4e86\u5904\u7406\u65f6\u95f4\u7684\u5468\u671f\u6027\uff0c\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u4e2a\u65f6\u95f4\u5ea6\u91cf\u5b66\u4e60\u76ee\u6807\uff0c\u901a\u8fc7\u5728\u5468\u671f\u6027\u73af\u9762\u8868\u9762\u4e0a\u5bf9\u6210\u5bf9\u65f6\u95f4\u5dee\u8fdb\u884c\u5efa\u6a21\u6765\u63d0\u4f9b\u8f6f\u76ee\u6807\uff0c\u800c\u4e0d\u662f\u4f7f\u7528\u4f20\u7edf\u7684\u5177\u6709\u786c\u6b63\u4f8b\u548c\u8d1f\u4f8b\u7684\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "GT-Loc \u65b9\u6cd5\u5728\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8bc1\u660e\uff0c\u5176\u8054\u5408\u4f18\u5316\u6027\u80fd\u4f18\u4e8e\u5148\u524d\u7684\u65f6\u95f4\u9884\u6d4b\u65b9\u6cd5\uff0c\u751a\u81f3\u4f18\u4e8e\u90a3\u4e9b\u5728\u63a8\u7406\u65f6\u4f7f\u7528\u771f\u5b9e\u5730\u7406\u4f4d\u7f6e\u4fe1\u606f\u7684\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u5728\u6807\u51c6\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5e76\u4e14\u5176\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\u652f\u6301\u7ec4\u5408\u5f0f\u548c\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u68c0\u7d22\u3002", "conclusion": "GT-Loc \u65b9\u6cd5\u901a\u8fc7\u8054\u5408\u9884\u6d4b\u56fe\u50cf\u7684\u65f6\u95f4\uff08\u5c0f\u65f6\u548c\u6708\u4efd\uff09\u548c\u5730\u7406\u4f4d\u7f6e\uff08GPS \u5750\u6807\uff09\uff0c\u5e76\u5728\u5171\u4eab\u7684\u9ad8\u7ef4\u7279\u5f81\u7a7a\u95f4\u4e2d\u5bf9\u56fe\u50cf\u3001\u65f6\u95f4\u548c\u4f4d\u7f6e\u7684\u7f16\u7801\u5668\u8fdb\u884c\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u5f80\u4ec5\u9884\u6d4b\u65f6\u95f4\u7684\u65b9\u6cd5\u66f4\u597d\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u90a3\u4e9b\u5728\u63a8\u7406\u65f6\u8f93\u5165\u4e86\u771f\u5b9e\u5730\u7406\u4f4d\u7f6e\u7684\u65b9\u6cd5\u4e5f\u662f\u5982\u6b64\u3002\u6b64\u5916\uff0cGT-Loc \u5728\u6807\u51c6\u7684\u5730\u7406\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u4e5f\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u5176\u7edf\u4e00\u7684\u5d4c\u5165\u7a7a\u95f4\u8fd8\u6709\u52a9\u4e8e\u8fdb\u884c\u7ec4\u5408\u548c\u57fa\u4e8e\u6587\u672c\u7684\u56fe\u50cf\u68c0\u7d22\u3002"}}
{"id": "2507.10452", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10452", "abs": "https://arxiv.org/abs/2507.10452", "authors": ["Eduardo D. Sontag"], "title": "Some remarks on gradient dominance and LQR policy optimization", "comment": "This is a short paper summarizing the first part of the slides\n  presented at my keynote at the 2025 L4DC (Learning for Dynamics & Control\n  Conference) in Ann Arbor, Michigan, 05 June 2025. A partial bibliography has\n  been added. A second part on neural net feedback controllers is to be added", "summary": "Solutions of optimization problems, including policy optimization in\nreinforcement learning, typically rely upon some variant of gradient descent.\nThere has been much recent work in the machine learning, control, and\noptimization communities applying the Polyak-{\\L}ojasiewicz Inequality (PLI) to\nsuch problems in order to establish an exponential rate of convergence (a.k.a.\n``linear convergence'' in the local-iteration language of numerical analysis)\nof loss functions to their minima under the gradient flow. Often, as is the\ncase of policy iteration for the continuous-time LQR problem, this rate\nvanishes for large initial conditions, resulting in a mixed globally linear /\nlocally exponential behavior. This is in sharp contrast with the discrete-time\nLQR problem, where there is global exponential convergence. That gap between CT\nand DT behaviors motivates the search for various generalized PLI-like\nconditions, and this talk will address that topic. Moreover, these\ngeneralizations are key to understanding the transient and asymptotic effects\nof errors in the estimation of the gradient, errors which might arise from\nadversarial attacks, wrong evaluation by an oracle, early stopping of a\nsimulation, inaccurate and very approximate digital twins, stochastic\ncomputations (algorithm ``reproducibility''), or learning by sampling from\nlimited data. We describe an ``input to state stability'' (ISS) analysis of\nthis issue. The lecture also discussed convergence and PLI-like properties of\n``linear feedforward neural networks'' in feedback control, but this arXiv\nskips that part (to be updated). Much of the work described here was done in\ncollaboration with Arthur Castello B. de Oliveira, Leilei Cui, Zhong-Ping\nJiang, and Milad Siami.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u89e3\u51b3\u4f18\u5316\u95ee\u9898\uff08\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u4f18\u5316\uff09\u7684\u68af\u5ea6\u4e0b\u964d\u53d8\u79cd\u7684\u901a\u7528PLI\u6761\u4ef6\uff0c\u4ee5\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u65f6\u95f4\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u5229\u7528ISS\u5206\u6790\u6765\u89e3\u51b3\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u586b\u8865\u8fde\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u65f6\u95f4\u884c\u4e3a\u5728Polyak-Lojasiewicz\u4e0d\u7b49\u5f0f\uff08PLI\uff09\u6536\u655b\u7387\u65b9\u9762\u7684\u5dee\u8ddd\uff0c\u540c\u65f6\u5206\u6790\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "method": "\u5229\u7528\u201c\u72b6\u6001\u8f93\u5165\u7a33\u5b9a\u6027\u201d\uff08ISS\uff09\u5206\u6790\u6765\u89e3\u51b3\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u95ee\u9898\uff0c\u5e76\u63a2\u8ba8\u4e86\u201c\u7ebf\u6027\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u201d\u5728\u53cd\u9988\u63a7\u5236\u4e2d\u7684\u6536\u655b\u6027\u548c\u7c7bPLI\u6027\u8d28\u3002", "result": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u5e76\u5206\u6790\u4e86\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e\u89e3\u51b3\u4f18\u5316\u95ee\u9898\uff08\u5305\u62ec\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u7b56\u7565\u4f18\u5316\uff09\u7684\u68af\u5ea6\u4e0b\u964d\u53d8\u79cd\u7684\u901a\u7528PLI\u6761\u4ef6\uff0c\u4ee5\u89e3\u51b3\u8fde\u7eed\u65f6\u95f4\u548c\u79bb\u6563\u65f6\u95f4\u884c\u4e3a\u4e4b\u95f4\u7684\u5dee\u5f02\uff0c\u5e76\u5206\u6790\u4e86\u68af\u5ea6\u4f30\u8ba1\u8bef\u5dee\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.10484", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10484", "abs": "https://arxiv.org/abs/2507.10484", "authors": ["Paul Fogel", "Christophe Geissler", "George Luta"], "title": "The Target Polish: A New Approach to Outlier-Resistant Non-Negative Matrix and Tensor Factorization", "comment": "6 pages, 4 figures, International Conference on Robust Statistics\n  2025, Stresa, Italy", "summary": "This paper introduces the \"Target Polish,\" a robust and computationally\nefficient framework for nonnegative matrix and tensor factorization. Although\nconventional weighted NMF approaches are resistant to outliers, they converge\nslowly due to the use of multiplicative updates to minimize the objective\ncriterion. In contrast, the Target Polish approach remains compatible with the\nFast-HALS algorithm, which is renowned for its speed, by adaptively smoothing\nthe data with a weighted median-based transformation. This innovation provides\noutlier resistance while maintaining the highly efficient additive update\nstructure of Fast-HALS. Empirical evaluations using image datasets corrupted\nwith structured (block) and unstructured (salt) noise demonstrate that the\nTarget Polish approach matches or exceeds the accuracy of state-of-the-art\nrobust NMF methods and reduces computational time by an order of magnitude in\nthe studied scenarios.", "AI": {"tldr": "Target Polish \u6846\u67b6\u901a\u8fc7\u52a0\u6743\u4e2d\u503c\u5e73\u6ed1\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5feb\u3001\u66f4\u9c81\u68d2\u7684\u975e\u8d1f\u77e9\u9635\u548c\u5f20\u91cf\u5206\u89e3\u3002", "motivation": "\u4f20\u7edf\u7684\u52a0\u6743NMF\u65b9\u6cd5\u867d\u7136\u80fd\u62b5\u6297\u5f02\u5e38\u503c\uff0c\u4f46\u7531\u4e8e\u4f7f\u7528\u4e58\u6cd5\u66f4\u65b0\u6765\u6700\u5c0f\u5316\u76ee\u6807\u5224\u636e\uff0c\u56e0\u6b64\u6536\u655b\u901f\u5ea6\u7f13\u6162\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "Target Polish \u65b9\u6cd5\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u4f7f\u7528\u57fa\u4e8e\u52a0\u6743\u4e2d\u503c\u7684\u65b9\u6cd5\u8fdb\u884c\u6570\u636e\u5e73\u6ed1\uff0c\u4f7f\u5176\u80fd\u591f\u517c\u5bb9\u4ee5\u901f\u5ea6\u8457\u79f0\u7684Fast-HALS\u7b97\u6cd5\uff0c\u4ece\u800c\u5728\u4fdd\u6301\u52a0\u6027\u66f4\u65b0\u7ed3\u6784\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u9c81\u68d2\u6027\u3002", "result": "Target Polish \u65b9\u6cd5\u6210\u529f\u5730\u63d0\u9ad8\u4e86NMF\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u7b97\u6cd5\u7684\u9ad8\u6548\u6027\u3002\u5728\u566a\u58f0\u5e72\u6270\u7684\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Target Polish \u65b9\u6cd5\u5728\u56fe\u50cf\u6570\u636e\u96c6\u7684\u9c81\u68d2\u6027\uff08\u5305\u62ec\u7ed3\u6784\u5316\u548c\u975e\u7ed3\u6784\u5316\u566a\u58f0\uff09\u65b9\u9762\uff0c\u80fd\u591f\u8fbe\u5230\u6216\u8d85\u8fc7\u6700\u5148\u8fdb\u7684\u9c81\u68d2NMF\u65b9\u6cd5\u7684\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u6240\u7814\u7a76\u7684\u573a\u666f\u4e2d\u8ba1\u7b97\u65f6\u95f4\u51cf\u5c11\u4e86\u4e00\u4e2a\u6570\u91cf\u7ea7\u3002"}}
{"id": "2507.10490", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2507.10490", "abs": "https://arxiv.org/abs/2507.10490", "authors": ["Tugberk Erol", "Tuba Caglikantar", "Duygu Sarikaya"], "title": "The Power of Certainty: How Confident Models Lead to Better Segmentation", "comment": "9 pages, 3 figures", "summary": "Deep learning models have been proposed for automatic polyp detection and\nprecise segmentation of polyps during colonoscopy procedures. Although these\nstate-of-the-art models achieve high performance, they often require a large\nnumber of parameters. Their complexity can make them prone to overfitting,\nparticularly when trained on biased datasets, and can result in poor\ngeneralization across diverse datasets. Knowledge distillation and\nself-distillation are proposed as promising strategies to mitigate the\nlimitations of large, over-parameterized models. These approaches, however, are\nresource-intensive, often requiring multiple models and significant memory\nduring training. We propose a confidence-based self-distillation approach that\noutperforms state-of-the-art models by utilizing only previous iteration data\nstorage during training, without requiring extra computation or memory usage\nduring testing. Our approach calculates the loss between the previous and\ncurrent iterations within a batch using a dynamic confidence coefficient. To\nevaluate the effectiveness of our approach, we conduct comprehensive\nexperiments on the task of polyp segmentation. Our approach outperforms\nstate-of-the-art models and generalizes well across datasets collected from\nmultiple clinical centers. The code will be released to the public once the\npaper is accepted.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7f6e\u4fe1\u5ea6\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u7528\u4e8e\u606f\u8089\u5206\u5272\uff0c\u65e0\u9700\u989d\u5916\u8ba1\u7b97\u6216\u5185\u5b58\u5373\u53ef\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5728\u606f\u8089\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u53c2\u6570\u91cf\u5927\u3001\u6613\u8fc7\u62df\u5408\u4ee5\u53ca\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u8bad\u7ec3\u671f\u95f4\u4ec5\u4f7f\u7528\u524d\u4e00\u8fed\u4ee3\u7684\u6570\u636e\u5b58\u50a8\uff0c\u800c\u65e0\u9700\u5728\u6d4b\u8bd5\u671f\u95f4\u989d\u5916\u7684\u8ba1\u7b97\u6216\u5185\u5b58\u4f7f\u7528\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u52a8\u6001\u7f6e\u4fe1\u7cfb\u6570\u8ba1\u7b97\u524d\u4e00\u548c\u5f53\u524d\u8fed\u4ee3\u6279\u6b21\u4e4b\u95f4\u7684\u635f\u5931\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u606f\u8089\u5206\u5272\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u8de8\u591a\u4e2a\u4e34\u5e8a\u4e2d\u5fc3\u6536\u96c6\u7684\u6570\u636e\u96c6\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7f6e\u4fe1\u5ea6\u7684\u81ea\u84b8\u998f\u65b9\u6cd5\uff0c\u5728\u7ed3\u80a0\u955c\u606f\u8089\u5206\u5272\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5e76\u4e14\u5728\u6765\u81ea\u591a\u4e2a\u4e34\u5e8a\u4e2d\u5fc3\u7684\u5e7f\u6cdb\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2507.10485", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.10485", "abs": "https://arxiv.org/abs/2507.10485", "authors": ["Brandon Shuen Yi Loke", "Filippo Quadri", "Gabriel Vivanco", "Maximilian Casagrande", "Sa\u00fal Fenollosa"], "title": "Overcoming catastrophic forgetting in neural networks", "comment": "7 pages, 5 figures, EE-411 Fundamentals of inference and learning\n  course project", "summary": "Catastrophic forgetting is the primary challenge that hinders continual\nlearning, which refers to a neural network ability to sequentially learn\nmultiple tasks while retaining previously acquired knowledge. Elastic Weight\nConsolidation, a regularization-based approach inspired by synaptic\nconsolidation in biological neural systems, has been used to overcome this\nproblem. In this study prior research is replicated and extended by evaluating\nEWC in supervised learning settings using the PermutedMNIST and RotatedMNIST\nbenchmarks. Through systematic comparisons with L2 regularization and\nstochastic gradient descent (SGD) without regularization, we analyze how\ndifferent approaches balance knowledge retention and adaptability. Our results\nconfirm what was shown in previous research, showing that EWC significantly\nreduces forgetting compared to naive training while slightly compromising\nlearning efficiency on new tasks. Moreover, we investigate the impact of\ndropout regularization and varying hyperparameters, offering insights into the\ngeneralization of EWC across diverse learning scenarios. These results\nunderscore EWC's potential as a viable solution for lifelong learning in neural\nnetworks.", "AI": {"tldr": "EWC reduces catastrophic forgetting in continual learning, but slightly slows down learning new tasks. It's a promising method for lifelong learning.", "motivation": "To address catastrophic forgetting, the primary challenge hindering continual learning, by evaluating the effectiveness of Elastic Weight Consolidator (EWC).", "method": "Replication and extension of prior research by evaluating EWC in supervised learning settings using PermutedMNIST and RotatedMNIST benchmarks, with systematic comparisons to L2 regularization and SGD without regularization. Investigation into the impact of dropout regularization and varying hyperparameters.", "result": "EWC significantly reduces forgetting compared to naive training, but slightly compromises learning efficiency on new tasks. The study also provides insights into the generalization of EWC across diverse learning scenarios by investigating the impact of dropout and hyperparameters.", "conclusion": "Elastic Weight Consolidation (EWC) is a viable solution for lifelong learning in neural networks, as it significantly reduces forgetting compared to naive training, albeit with a slight compromise in learning efficiency on new tasks."}}
{"id": "2507.10492", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10492", "abs": "https://arxiv.org/abs/2507.10492", "authors": ["Chenyu Lian", "Hong-Yu Zhou", "Zhanli Hu", "Jing Qin"], "title": "BenchReAD: A systematic benchmark for retinal anomaly detection", "comment": "MICCAI 2025", "summary": "Retinal anomaly detection plays a pivotal role in screening ocular and\nsystemic diseases. Despite its significance, progress in the field has been\nhindered by the absence of a comprehensive and publicly available benchmark,\nwhich is essential for the fair evaluation and advancement of methodologies.\nDue to this limitation, previous anomaly detection work related to retinal\nimages has been constrained by (1) a limited and overly simplistic set of\nanomaly types, (2) test sets that are nearly saturated, and (3) a lack of\ngeneralization evaluation, resulting in less convincing experimental setups.\nFurthermore, existing benchmarks in medical anomaly detection predominantly\nfocus on one-class supervised approaches (training only with negative samples),\noverlooking the vast amounts of labeled abnormal data and unlabeled data that\nare commonly available in clinical practice. To bridge these gaps, we introduce\na benchmark for retinal anomaly detection, which is comprehensive and\nsystematic in terms of data and algorithm. Through categorizing and\nbenchmarking previous methods, we find that a fully supervised approach\nleveraging disentangled representations of abnormalities (DRA) achieves the\nbest performance but suffers from significant drops in performance when\nencountering certain unseen anomalies. Inspired by the memory bank mechanisms\nin one-class supervised learning, we propose NFM-DRA, which integrates DRA with\na Normal Feature Memory to mitigate the performance degradation, establishing a\nnew SOTA. The benchmark is publicly available at\nhttps://github.com/DopamineLcy/BenchReAD.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u89c6\u7f51\u819c\u5f02\u5e38\u68c0\u6d4b\u7684\u57fa\u51c6\u4e0d\u8db3\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5168\u9762\u4e14\u7cfb\u7edf\u7684\u57fa\u51c6\u3002\u540c\u65f6\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u540d\u4e3aNFM-DRA\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u73b0\u6709DRA\u65b9\u6cd5\u7684\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u6b63\u5e38\u7279\u5f81\u8bb0\u5fc6\u5e93\uff0c\u89e3\u51b3\u4e86\u6a21\u578b\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u5f02\u5e38\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u5f53\u524d\u6700\u4f18\u7684\u6027\u80fd\u3002\u7814\u7a76\u63d0\u4f9b\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u6709\u52a9\u4e8e\u672a\u6765\u89c6\u7f51\u819c\u5f02\u5e38\u68c0\u6d4b\u9886\u57df\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u7f51\u819c\u5f02\u5e38\u68c0\u6d4b\u57fa\u51c6\u5b58\u5728\u6570\u636e\u6709\u9650\u3001\u5f02\u5e38\u7c7b\u578b\u5355\u4e00\u3001\u6d4b\u8bd5\u96c6\u9971\u548c\u4ee5\u53ca\u7f3a\u4e4f\u6cdb\u5316\u8bc4\u4f30\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u5b9e\u9a8c\u7ed3\u679c\u4e0d\u591f\u4ee4\u4eba\u4fe1\u670d\u3002\u540c\u65f6\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5927\u591a\u4fa7\u91cd\u4e8e\u5355\u7c7b\u76d1\u7763\u5b66\u4e60\uff0c\u5ffd\u7565\u4e86\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u53ef\u7528\u7684\u5927\u91cf\u6807\u8bb0\u5f02\u5e38\u6570\u636e\u548c\u672a\u6807\u8bb0\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aNFM-DRA\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u4e4b\u524d\u63d0\u51fa\u7684DRA\uff08\u5206\u79bb\u5f02\u5e38\u8868\u5f81\uff09\u65b9\u6cd5\u548c\u4e00\u4e2a\u6b63\u5e38\u7279\u5f81\u8bb0\u5fc6\u5e93\uff0c\u65e8\u5728\u89e3\u51b3\u5728\u9762\u5bf9\u672a\u89c1\u8fc7\u5f02\u5e38\u65f6\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u5bf9\u73b0\u6709\u65b9\u6cd5\u7684\u5206\u7c7b\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u53d1\u73b0\u5b8c\u5168\u76d1\u7763\u65b9\u6cd5DRA\u867d\u7136\u6027\u80fd\u6700\u4f73\uff0c\u4f46\u5728\u9762\u5bf9\u67d0\u4e9b\u672a\u89c1\u8fc7\u5f02\u5e38\u65f6\u6027\u80fd\u4e0b\u964d\u660e\u663e\u3002\u800c\u63d0\u51fa\u7684NFM-DRA\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u6b63\u5e38\u7279\u5f81\u8bb0\u5fc6\u5e93\uff0c\u6210\u529f\u7f13\u89e3\u4e86\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u5efa\u7acb\u4e86\u65b0\u7684SOTA\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684NFM-DRA\u65b9\u6cd5\u901a\u8fc7\u6574\u5408DRA\u548c\u6b63\u5e38\u7279\u5f81\u8bb0\u5fc6\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5e76\u8fbe\u5230\u4e86\u65b0\u7684SOTA\uff08State-of-the-Art\uff09\u6c34\u5e73\u3002"}}
{"id": "2507.10494", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.10494", "abs": "https://arxiv.org/abs/2507.10494", "authors": ["Tanveer Khan", "Mindaugas Budzys", "Antonis Michalas"], "title": "Split Happens: Combating Advanced Threats with Split Learning and Function Secret Sharing", "comment": null, "summary": "Split Learning (SL) -- splits a model into two distinct parts to help protect\nclient data while enhancing Machine Learning (ML) processes. Though promising,\nSL has proven vulnerable to different attacks, thus raising concerns about how\neffective it may be in terms of data privacy. Recent works have shown promising\nresults for securing SL through the use of a novel paradigm, named Function\nSecret Sharing (FSS), in which servers obtain shares of a function they compute\nand operate on a public input hidden with a random mask. However, these works\nfall short in addressing the rising number of attacks which exist on SL. In\nSplitHappens, we expand the combination of FSS and SL to U-shaped SL. Similarly\nto other works, we are able to make use of the benefits of SL by reducing the\ncommunication and computational costs of FSS. However, a U-shaped SL provides a\nhigher security guarantee than previous works, allowing a client to keep the\nlabels of the training data secret, without having to share them with the\nserver. Through this, we are able to generalize the security analysis of\nprevious works and expand it to different attack vectors, such as modern model\ninversion attacks as well as label inference attacks. We tested our approach\nfor two different convolutional neural networks on different datasets. These\nexperiments show the effectiveness of our approach in reducing the training\ntime as well as the communication costs when compared to simply using FSS while\nmatching prior accuracy.", "AI": {"tldr": "SplitHappens\u7ed3\u5408\u4e86\u51fd\u6570\u79d8\u5bc6\u5171\u4eab\uff08FSS\uff09\u548cU\u578b\u5206\u5272\u5b66\u4e60\uff08U-SL\uff09\uff0c\u63d0\u9ad8\u4e86\u6570\u636e\u9690\u79c1\u548c\u5b89\u5168\u6027\uff0c\u80fd\u62b5\u5fa1\u591a\u79cd\u653b\u51fb\uff0c\u5e76\u964d\u4f4e\u4e86\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5206\u5272\u5b66\u4e60\uff08SL\uff09\u867d\u7136\u80fd\u5728\u4fdd\u62a4\u5ba2\u6237\u7aef\u6570\u636e\u9690\u79c1\u7684\u540c\u65f6\u589e\u5f3a\u673a\u5668\u5b66\u4e60\uff08ML\uff09\uff0c\u4f46\u6613\u53d7\u591a\u79cd\u653b\u51fb\u3002\u867d\u7136\u51fd\u6570\u79d8\u5bc6\u5171\u4eab\uff08FSS\uff09\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b89\u5168SL\u7684\u8303\u5f0f\uff0c\u4f46\u5176\u5728\u5e94\u5bf9\u65e5\u76ca\u589e\u591a\u7684SL\u653b\u51fb\u65b9\u9762\u4ecd\u663e\u4e0d\u8db3\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u8fdb\u4e00\u6b65\u63d0\u9ad8\u9690\u79c1\u4fdd\u62a4\u80fd\u529b\u5e76\u5e94\u5bf9\u66f4\u591a\u653b\u51fb\u7684\u65b9\u6cd5\u3002", "method": "SplitHappens\u5c06\u51fd\u6570\u79d8\u5bc6\u5171\u4eab\uff08FSS\uff09\u4e0eU\u578b\u5206\u5272\u5b66\u4e60\uff08U-SL\uff09\u76f8\u7ed3\u5408\uff0c\u5c06\u6a21\u578b\u5206\u5272\uff0c\u4f7f\u5f97\u5ba2\u6237\u7aef\u80fd\u591f\u5c06\u8bad\u7ec3\u6570\u636e\u7684\u6807\u7b7e\u9690\u85cf\uff0c\u65e0\u9700\u4e0e\u670d\u52a1\u5668\u5171\u4eab\u3002\u8fd9\u79cd\u65b9\u6cd5\u901a\u8fc7\u9690\u85cf\u5ba2\u6237\u7aef\u7684\u6807\u7b7e\u6765\u63d0\u4f9b\u66f4\u9ad8\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSplitHappens\u5728\u5c06FSS\u5e94\u7528\u4e8eU\u578bSL\u65f6\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u3002\u4e0e\u5355\u72ec\u4f7f\u7528FSS\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u8fd8\u80fd\u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u548c\u901a\u4fe1\u5f00\u9500\u3002\u5176\u5b89\u5168\u5206\u6790\u4e5f\u4ece\u73b0\u6709\u7814\u7a76\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u653b\u51fb\u5411\u91cf\uff0c\u5982\u6a21\u578b\u53cd\u6f14\u548c\u6807\u7b7e\u63a8\u65ad\u653b\u51fb\u3002", "conclusion": "SplitHappens\u901a\u8fc7\u5c06\u51fd\u6570\u79d8\u5bc6\u5171\u4eab\uff08FSS\uff09\u4e0eU\u578b\u6a21\u578b\u5206\u5272\u5b66\u4e60\uff08SL\uff09\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u5ba2\u6237\u7aef\u6570\u636e\u7684\u9690\u79c1\u6027\uff0c\u7279\u522b\u662f\u6807\u7b7e\u7684\u4fdd\u5bc6\u6027\uff0c\u540c\u65f6\u901a\u8fc7\u51cf\u5c11\u901a\u4fe1\u548c\u8ba1\u7b97\u6210\u672c\u6765\u63d0\u9ad8\u6548\u7387\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u62b5\u5fa1\u5305\u62ec\u6a21\u578b\u53cd\u6f14\u548c\u6807\u7b7e\u63a8\u65ad\u5728\u5185\u7684\u591a\u79cd\u653b\u51fb\uff0c\u5e76\u4e14\u5728\u4fdd\u6301\u73b0\u6709\u6a21\u578b\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u964d\u4f4e\u4e86\u8bad\u7ec3\u65f6\u95f4\u548c\u901a\u4fe1\u6210\u672c\u3002"}}
{"id": "2507.10496", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10496", "abs": "https://arxiv.org/abs/2507.10496", "authors": ["Ruilong Li", "Brent Yi", "Junchen Liu", "Hang Gao", "Yi Ma", "Angjoo Kanazawa"], "title": "Cameras as Relative Positional Encoding", "comment": "Project Page: https://www.liruilong.cn/prope/", "summary": "Transformers are increasingly prevalent for multi-view computer vision tasks,\nwhere geometric relationships between viewpoints are critical for 3D\nperception. To leverage these relationships, multi-view transformers must use\ncamera geometry to ground visual tokens in 3D space. In this work, we compare\ntechniques for conditioning transformers on cameras: token-level raymap\nencodings, attention-level relative pose encodings, and a new relative encoding\nwe propose -- Projective Positional Encoding (PRoPE) -- that captures complete\ncamera frustums, both intrinsics and extrinsics, as a relative positional\nencoding. Our experiments begin by showing how relative camera conditioning\nimproves performance in feedforward novel view synthesis, with further gains\nfrom PRoPE. This holds across settings: scenes with both shared and varying\nintrinsics, when combining token- and attention-level conditioning, and for\ngeneralization to inputs with out-of-distribution sequence lengths and camera\nintrinsics. We then verify that these benefits persist for different tasks,\nstereo depth estimation and discriminative spatial cognition, as well as larger\nmodel sizes.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faPRoPE\uff0c\u4e00\u79cd\u65b0\u7684\u76f8\u673a\u51e0\u4f55\u7f16\u7801\u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u89c6\u56feTransformer\uff0c\u5728\u591a\u98793D\u89c6\u89c9\u4efb\u52a1\u4e2d\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u591a\u89c6\u56feTransformer\u5728\u5904\u7406\u6d89\u53ca3D\u611f\u77e5\u4efb\u52a1\u65f6\uff0c\u9700\u8981\u6709\u6548\u5229\u7528\u89c6\u56fe\u95f4\u7684\u51e0\u4f55\u5173\u7cfb\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u76f8\u673a\u51e0\u4f55\u4fe1\u606f\u5f15\u5165\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8Transformer\u5728\u591a\u89c6\u56fe\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "method": "\u672c\u6587\u6bd4\u8f83\u4e86\u4e09\u79cd\u7528\u4e8e\u5728Transformer\u4e2d\u5f15\u5165\u76f8\u673a\u51e0\u4f55\u4fe1\u606f\u7684\u65b9\u6cd5\uff1atoken\u7ea7\u522b\u7684\u5c04\u7ebf\u56fe\u7f16\u7801\u3001attention\u7ea7\u522b\u7684\u76f8\u5bf9\u4f4d\u59ff\u7f16\u7801\uff0c\u4ee5\u53ca\u65b0\u63d0\u51fa\u7684\u6295\u5f71\u4f4d\u7f6e\u7f16\u7801\uff08PRoPE\uff09\u3002PRoPE\u80fd\u591f\u6355\u6349\u5b8c\u6574\u7684\u76f8\u673a\u4fe1\u606f\uff08\u5185\u53c2\u548c\u5916\u53c2\uff09\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u76f8\u5bf9\u4f4d\u7f6e\u7f16\u7801\u4f7f\u7528\u3002", "result": "\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u5728feedforward\u65b0\u89c6\u56fe\u5408\u6210\u4efb\u52a1\u4e2d\uff0c\u76f8\u5bf9\u76f8\u673a\u6761\u4ef6\u80fd\u591f\u63d0\u5347\u6027\u80fd\uff0c\u800cPRoPE\u7684\u5f15\u5165\u80fd\u8fdb\u4e00\u6b65\u5e26\u6765\u6027\u80fd\u589e\u76ca\u3002PRoPE\u5728\u5904\u7406\u5171\u4eab\u548c\u53d8\u5316\u7684\u5185\u53c2\u3001\u7ed3\u5408token\u548cattention\u7ea7\u522b\u6761\u4ef6\u3001\u4ee5\u53ca\u6cdb\u5316\u5230\u4e0d\u540c\u5e8f\u5217\u957f\u5ea6\u548c\u5185\u53c2\u7684\u8f93\u5165\u65f6\uff0c\u5747\u8868\u73b0\u51fa\u4f18\u52bf\u3002\u6b64\u5916\uff0c\u5728\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u548c\u7a7a\u95f4\u8ba4\u77e5\u7b49\u4efb\u52a1\u4ee5\u53ca\u66f4\u5927\u7684\u6a21\u578b\u89c4\u6a21\u4e0b\uff0cPRoPE\u7684\u4f18\u52bf\u4e5f\u5f97\u4ee5\u9a8c\u8bc1\u3002", "conclusion": "PRoPE\u4f5c\u4e3a\u4e00\u79cd\u65b0\u7684\u76f8\u5bf9\u7f16\u7801\uff0c\u901a\u8fc7\u6355\u6349\u5b8c\u6574\u7684\u76f8\u673a\u622a\u5934\u9525\u4f53\uff08\u5185\u53c2\u548c\u5916\u53c2\uff09\uff0c\u5728\u591a\u89c6\u56feTransformer\u4e2d\u6709\u6548\u5229\u7528\u76f8\u673a\u51e0\u4f55\u5173\u7cfb\uff0c\u5e76\u5728\u524d\u9988\u65b0\u89c6\u56fe\u5408\u6210\u3001\u7acb\u4f53\u6df1\u5ea6\u4f30\u8ba1\u548c\u7a7a\u95f4\u8ba4\u77e5\u7b49\u4efb\u52a1\u4e0a\u5747\u6709\u6027\u80fd\u63d0\u5347\uff0c\u4e14\u5728\u4e0d\u540c\u573a\u666f\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u51fa\u9c81\u68d2\u6027\u3002"}}
{"id": "2507.10502", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10502", "abs": "https://arxiv.org/abs/2507.10502", "authors": ["Elizabeth Fahsbender", "Alma Andersson", "Jeremy Ash", "Polina Binder", "Daniel Burkhardt", "Benjamin Chang", "Georg K. Gerber", "Anthony Gitter", "Patrick Godau", "Ankit Gupta", "Genevieve Haliburton", "Siyu He", "Trey Ideker", "Ivana Jelic", "Aly Khan", "Yang-Joon Kim", "Aditi Krishnapriyan", "Jon M. Laurent", "Tianyu Liu 28", "Emma Lundberg", "Shalin B. Mehta", "Rob Moccia", "Angela Oliveira Pisco", "Katherine S. Pollard", "Suresh Ramani", "Julio Saez-Rodriguez", "Yasin Senbabaoglu", "Elana Simon", "Srinivasan Sivanandan", "Gustavo Stolovitzky", "Marc Valer", "Bo Wang", "Xikun Zhang", "James Zou", "Katrina Kalantar"], "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop", "comment": null, "summary": "Artificial intelligence holds immense promise for transforming biology, yet a\nlack of standardized, cross domain, benchmarks undermines our ability to build\nrobust, trustworthy models. Here, we present insights from a recent workshop\nthat convened machine learning and computational biology experts across\nimaging, transcriptomics, proteomics, and genomics to tackle this gap. We\nidentify major technical and systemic bottlenecks such as data heterogeneity\nand noise, reproducibility challenges, biases, and the fragmented ecosystem of\npublicly available resources and propose a set of recommendations for building\nbenchmarking frameworks that can efficiently compare ML models of biological\nsystems across tasks and data modalities. By promoting high quality data\ncuration, standardized tooling, comprehensive evaluation metrics, and open,\ncollaborative platforms, we aim to accelerate the development of robust\nbenchmarks for AI driven Virtual Cells. These benchmarks are crucial for\nensuring rigor, reproducibility, and biological relevance, and will ultimately\nadvance the field toward integrated models that drive new discoveries,\ntherapeutic insights, and a deeper understanding of cellular systems.", "AI": {"tldr": "\u4eba\u5de5\u667a\u80fd\u5728\u751f\u7269\u5b66\u4e2d\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u57fa\u51c6\u3002\u672c\u6b21\u7814\u8ba8\u4f1a\u65e8\u5728\u89e3\u51b3\u6b64\u95ee\u9898\uff0c\u786e\u5b9a\u4e86\u6570\u636e\u5f02\u8d28\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u504f\u5dee\u7b49\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\uff0c\u4ee5\u52a0\u901f\u5f00\u53d1\u7528\u4e8e\u865a\u62df\u7ec6\u80de\u7684\u4eba\u5de5\u667a\u80fd\u57fa\u51c6\uff0c\u4ece\u800c\u63d0\u9ad8\u7814\u7a76\u7684\u4e25\u8c28\u6027\u548c\u53ef\u91cd\u590d\u6027\uff0c\u5e76\u63a8\u52a8\u65b0\u7684\u53d1\u73b0\u3002", "motivation": "\u4eba\u5de5\u667a\u80fd\u5728\u751f\u7269\u5b66\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8de8\u9886\u57df\u57fa\u51c6\u963b\u788d\u4e86\u6784\u5efa\u9c81\u68d2\u3001\u53ef\u4fe1\u8d56\u7684\u6a21\u578b\u3002", "method": "\u7ec4\u7ec7\u4e86\u4e00\u6b21\u6c47\u96c6\u4e86\u6765\u81ea\u6210\u50cf\u3001\u8f6c\u5f55\u7ec4\u5b66\u3001\u86cb\u767d\u8d28\u7ec4\u5b66\u548c\u57fa\u56e0\u7ec4\u5b66\u9886\u57df\u7684\u673a\u5668\u5b66\u4e60\u548c\u8ba1\u7b97\u751f\u7269\u5b66\u4e13\u5bb6\u7684\u7814\u8ba8\u4f1a\uff0c\u4ee5\u89e3\u51b3\u6807\u51c6\u5316\u8de8\u9886\u57df\u57fa\u51c6\u7684\u7f3a\u5931\u95ee\u9898\u3002", "result": "\u786e\u5b9a\u4e86\u4e3b\u8981\u7684\u6280\u672f\u548c\u7cfb\u7edf\u74f6\u9888\uff0c\u4f8b\u5982\u6570\u636e\u5f02\u8d28\u6027\u548c\u566a\u58f0\u3001\u53ef\u91cd\u590d\u6027\u6311\u6218\u3001\u504f\u5dee\u4ee5\u53ca\u516c\u5171\u53ef\u7528\u8d44\u6e90\u788e\u7247\u5316\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u5173\u4e8e\u6784\u5efa\u80fd\u591f\u6709\u6548\u6bd4\u8f83\u8de8\u4efb\u52a1\u548c\u6570\u636e\u6a21\u5f0f\u7684\u751f\u7269\u7cfb\u7edf\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u57fa\u51c6\u6846\u67b6\u7684\u5efa\u8bae\u3002", "conclusion": "\u901a\u8fc7\u63a8\u5e7f\u9ad8\u8d28\u91cf\u6570\u636e\u7ba1\u7406\u3001\u6807\u51c6\u5316\u5de5\u5177\u3001\u5168\u9762\u7684\u8bc4\u4f30\u6307\u6807\u4ee5\u53ca\u5f00\u653e\u7684\u534f\u4f5c\u5e73\u53f0\uff0c\u65e8\u5728\u52a0\u901f\u6784\u5efa\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u865a\u62df\u7ec6\u80de\u7684\u9c81\u68d2\u57fa\u51c6\u3002\u8fd9\u4e9b\u57fa\u51c6\u5bf9\u4e8e\u786e\u4fdd\u4e25\u8c28\u6027\u3001\u53ef\u91cd\u590d\u6027\u548c\u751f\u7269\u5b66\u76f8\u5173\u6027\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u6700\u7ec8\u63a8\u52a8\u8be5\u9886\u57df\u5411\u80fd\u591f\u9a71\u52a8\u65b0\u53d1\u73b0\u3001\u6cbb\u7597\u89c1\u89e3\u548c\u5bf9\u7ec6\u80de\u7cfb\u7edf\u66f4\u6df1\u5165\u7406\u89e3\u7684\u96c6\u6210\u6a21\u578b\u8fc8\u8fdb\u3002"}}
{"id": "2507.10499", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10499", "abs": "https://arxiv.org/abs/2507.10499", "authors": ["Philippe Rufin", "Pauline Lucie Hammer", "Leon-Friedrich Thomas", "S\u00e1 Nogueira Lisboa", "Natasha Ribeiro", "Almeida Sitoe", "Patrick Hostert", "Patrick Meyfroidt"], "title": "National level satellite-based crop field inventories in smallholder landscapes", "comment": null, "summary": "The design of science-based policies to improve the sustainability of\nsmallholder agriculture is challenged by a limited understanding of fundamental\nsystem properties, such as the spatial distribution of active cropland and\nfield size. We integrate very high spatial resolution (1.5 m) Earth observation\ndata and deep transfer learning to derive crop field delineations in complex\nagricultural systems at the national scale, while maintaining minimum reference\ndata requirements and enhancing transferability. We provide the first\nnational-level dataset of 21 million individual fields for Mozambique (covering\n~800,000 km2) for 2023. Our maps separate active cropland from non-agricultural\nland use with an overall accuracy of 93% and balanced omission and commission\nerrors. Field-level spatial agreement reached median intersection over union\n(IoU) scores of 0.81, advancing the state-of-the-art in large-area field\ndelineation in complex smallholder systems. The active cropland maps capture\nfragmented rural regions with low cropland shares not yet identified in global\nland cover or cropland maps. These regions are mostly located in agricultural\nfrontier regions which host 7-9% of the Mozambican population. Field size in\nMozambique is very low overall, with half of the fields being smaller than 0.16\nha, and 83% smaller than 0.5 ha. Mean field size at aggregate spatial\nresolution (0.05{\\deg}) is 0.32 ha, but it varies strongly across gradients of\naccessibility, population density, and net forest cover change. This variation\nreflects a diverse set of actors, ranging from semi-subsistence smallholder\nfarms to medium-scale commercial farming, and large-scale farming operations.\nOur results highlight that field size is a key indicator relating to\nsocio-economic and environmental outcomes of agriculture (e.g., food\nproduction, livelihoods, deforestation, biodiversity), as well as their\ntrade-offs.", "AI": {"tldr": "\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u9996\u6b21\u7ed8\u5236\u4e86\u83ab\u6851\u6bd4\u514b\u5168\u56fd2100\u4e07\u4e2a\u519c\u7530\u8fb9\u754c\u56fe\uff0c\u51c6\u786e\u7387\u8fbe93%\uff0c\u63ed\u793a\u4e86\u5c0f\u519c\u573a\u4e3b\u666e\u904d\u9762\u4e34\u5c0f\u519c\u7530\u89c4\u6a21\u95ee\u9898\uff0c\u5e76\u5f3a\u8c03\u4e86\u519c\u7530\u89c4\u6a21\u5bf9\u519c\u4e1a\u53ef\u6301\u7eed\u6027\u7684\u91cd\u8981\u5f71\u54cd\u3002", "motivation": "\u5f53\u524d\u79d1\u5b66\u653f\u7b56\u7684\u5236\u5b9a\u9762\u4e34\u5bf9\u5c0f\u519c\u519c\u4e1a\u7cfb\u7edf\u57fa\u672c\u5c5e\u6027\uff08\u5982\u8015\u5730\u7a7a\u95f4\u5206\u5e03\u548c\u5730\u5757\u5927\u5c0f\uff09\u7406\u89e3\u6709\u9650\u7684\u6311\u6218\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u9ad8\u5206\u8fa8\u7387\u9065\u611f\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u7cbe\u786e\u7ed8\u5236\u519c\u7530\u8fb9\u754c\uff0c\u4ee5\u652f\u6301\u53ef\u6301\u7eed\u519c\u4e1a\u653f\u7b56\u7684\u8bbe\u8ba1\u3002", "method": "\u7814\u7a76\u6574\u5408\u4e861.5\u7c73\u5206\u8fa8\u7387\u7684\u5730\u7403\u89c2\u6d4b\u6570\u636e\u548c\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\u6280\u672f\uff0c\u4ee5\u7ed8\u5236\u83ab\u6851\u6bd4\u514b\u5168\u56fd\u8303\u56f4\u5185\u7684\u519c\u7530\u8fb9\u754c\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6700\u5c0f\u5316\u53c2\u8003\u6570\u636e\u9700\u6c42\u548c\u589e\u5f3a\u6a21\u578b\u7684\u53ef\u8fc1\u79fb\u6027\uff0c\u5b9e\u73b0\u4e86\u5927\u89c4\u6a21\u590d\u6742\u519c\u4e1a\u7cfb\u7edf\u4e2d\u7684\u519c\u7530\u5212\u5206\u3002", "result": "\u7814\u7a76\u6210\u529f\u7ed8\u5236\u4e86\u83ab\u6851\u6bd4\u514b2023\u5e74\u5168\u56fd\u8303\u56f4\u5185\u76842100\u4e07\u4e2a\u519c\u7530\u8fb9\u754c\u56fe\uff0c\u51c6\u786e\u7387\u8fbe\u523093%\u3002\u7814\u7a76\u53d1\u73b0\u83ab\u6851\u6bd4\u514b\u7684\u519c\u7530\u89c4\u6a21\u666e\u904d\u8f83\u5c0f\uff0c\u4e00\u534a\u4ee5\u4e0a\u7684\u519c\u7530\u5c0f\u4e8e0.16\u516c\u9877\u3002\u540c\u65f6\uff0c\u519c\u7530\u89c4\u6a21\u5728\u4e0d\u540c\u533a\u57df\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u8fd9\u53cd\u6620\u4e86\u5f53\u5730\u591a\u6837\u5316\u7684\u519c\u4e1a\u7ecf\u8425\u6a21\u5f0f\uff0c\u5e76\u5f3a\u8c03\u4e86\u519c\u7530\u89c4\u6a21\u4f5c\u4e3a\u5f71\u54cd\u793e\u4f1a\u7ecf\u6d4e\u548c\u73af\u5883\u7ed3\u679c\u7684\u5173\u952e\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u6574\u5408\u9ad8\u5206\u8fa8\u7387\u5730\u7403\u89c2\u6d4b\u6570\u636e\u548c\u6df1\u5ea6\u8fc1\u79fb\u5b66\u4e60\uff0c\u9996\u6b21\u7ed8\u5236\u4e86\u83ab\u6851\u6bd4\u514b\u5168\u56fd\u8303\u56f4\u5185\u76842100\u4e07\u4e2a\u519c\u7530\u8fb9\u754c\u56fe\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe93%\uff0c\u4e3a\u5236\u5b9a\u53ef\u6301\u7eed\u519c\u4e1a\u653f\u7b56\u63d0\u4f9b\u4e86\u5173\u952e\u7684\u7a7a\u95f4\u4fe1\u606f\uff0c\u5e76\u63ed\u793a\u4e86\u519c\u7530\u89c4\u6a21\u4e0e\u793e\u4f1a\u7ecf\u6d4e\u53ca\u73af\u5883\u6548\u76ca\u7684\u5bc6\u5207\u5173\u7cfb\u3002"}}
{"id": "2507.10547", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10547", "abs": "https://arxiv.org/abs/2507.10547", "authors": ["Borui Zhang", "Qihang Rao", "Wenzhao Zheng", "Jie Zhou", "Jiwen Lu"], "title": "Quantize-then-Rectify: Efficient VQ-VAE Training", "comment": null, "summary": "Visual tokenizers are pivotal in multimodal large models, acting as bridges\nbetween continuous inputs and discrete tokens. Nevertheless, training\nhigh-compression-rate VQ-VAEs remains computationally demanding, often\nnecessitating thousands of GPU hours. This work demonstrates that a pre-trained\nVAE can be efficiently transformed into a VQ-VAE by controlling quantization\nnoise within the VAE's tolerance threshold. We present\n\\textbf{Quantize-then-Rectify (ReVQ)}, a framework leveraging pre-trained VAEs\nto enable rapid VQ-VAE training with minimal computational overhead. By\nintegrating \\textbf{channel multi-group quantization} to enlarge codebook\ncapacity and a \\textbf{post rectifier} to mitigate quantization errors, ReVQ\ncompresses ImageNet images into at most 512 tokens while sustaining competitive\nreconstruction quality (rFID = 1.06). Significantly, ReVQ reduces training\ncosts by over two orders of magnitude relative to state-of-the-art approaches:\nReVQ finishes full training on a single NVIDIA 4090 in approximately 22 hours,\nwhereas comparable methods require 4.5 days on 32 A100 GPUs. Experimental\nresults show that ReVQ achieves superior efficiency-reconstruction trade-offs.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa ReVQ \u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3 VAE \u5e76\u7ed3\u5408\u7279\u5b9a\u6280\u672f\uff0c\u5927\u5e45\u964d\u4f4e\u4e86 VQ-VAE \u7684\u8bad\u7ec3\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u538b\u7f29\u548c\u91cd\u5efa\u3002", "motivation": "\u9ad8\u538b\u7f29\u7387 VQ-VAE \u7684\u8bad\u7ec3\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u901a\u5e38\u9700\u8981\u6570\u5343\u4e2a GPU \u5c0f\u65f6\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u4e00\u79cd\u66f4\u9ad8\u6548\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u5b9e\u73b0\u5feb\u901f\u7684 VQ-VAE \u8bad\u7ec3\u5e76\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Quantize-then-Rectify (ReVQ) \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3 VAE\uff0c\u901a\u8fc7\u63a7\u5236\u91cf\u5316\u566a\u58f0\u5728 VAE \u7684\u5bb9\u5dee\u9608\u503c\u5185\uff0c\u5c06\u5176\u9ad8\u6548\u5730\u8f6c\u5316\u4e3a VQ-VAE\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u901a\u9053\u591a\u7ec4\u91cf\u5316\u4ee5\u6269\u5927\u7801\u672c\u5bb9\u91cf\uff0c\u5e76\u4f7f\u7528\u540e\u7f6e\u6821\u6b63\u5668\u6765\u7f13\u89e3\u91cf\u5316\u8bef\u5dee\u3002", "result": "ReVQ \u80fd\u591f\u5c06 ImageNet \u56fe\u50cf\u538b\u7f29\u81f3\u6700\u591a 512 \u4e2a\u6807\u8bb0\uff0c\u540c\u65f6\u4fdd\u6301\u5177\u6709\u7ade\u4e89\u529b\u7684\u91cd\u5efa\u8d28\u91cf\uff08rFID = 1.06\uff09\u3002\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0cReVQ \u5c06\u8bad\u7ec3\u6210\u672c\u964d\u4f4e\u4e86\u4e24\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u80fd\u5728\u5355\u5f20 NVIDIA 4090 GPU \u4e0a\u7ea6 22 \u5c0f\u65f6\u5b8c\u6210\u8bad\u7ec3\uff0c\u800c\u7c7b\u4f3c\u65b9\u6cd5\u5728 32 \u4e2a A100 GPU \u4e0a\u9700\u8981 4.5 \u5929\u3002", "conclusion": "ReVQ \u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3 VAE \u548c\u5f15\u5165\u901a\u9053\u591a\u7ec4\u91cf\u5316\u53ca\u540e\u7f6e\u6821\u6b63\u5668\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684 VQ-VAE \u8bad\u7ec3\uff0c\u4ee5\u66f4\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u56fe\u50cf\u538b\u7f29\u548c\u91cd\u5efa\u8d28\u91cf\uff0c\u5e76\u5728\u6548\u7387-\u91cd\u5efa\u6743\u8861\u65b9\u9762\u8868\u73b0\u4f18\u8d8a\u3002"}}
{"id": "2507.10536", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10536", "abs": "https://arxiv.org/abs/2507.10536", "authors": ["Qiaoyue Tang", "Alain Zhiyanov", "Mathias L\u00e9cuyer"], "title": "On the Performance of Differentially Private Optimization with Heavy-Tail Class Imbalance", "comment": null, "summary": "In this work, we analyze the optimization behaviour of common private\nlearning optimization algorithms under heavy-tail class imbalanced\ndistribution. We show that, in a stylized model, optimizing with Gradient\nDescent with differential privacy (DP-GD) suffers when learning low-frequency\nclasses, whereas optimization algorithms that estimate second-order information\ndo not. In particular, DP-AdamBC that removes the DP bias from estimating loss\ncurvature is a crucial component to avoid the ill-condition caused by\nheavy-tail class imbalance, and empirically fits the data better with\n$\\approx8\\%$ and $\\approx5\\%$ increase in training accuracy when learning the\nleast frequent classes on both controlled experiments and real data\nrespectively.", "AI": {"tldr": "DP-AdamBC\u5728\u5904\u7406\u91cd\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u6570\u636e\u65f6\uff0c\u6bd4DP-GD\u8868\u73b0\u66f4\u597d\uff0c\u80fd\u63d0\u9ad8\u4f4e\u9891\u7c7b\u522b\u7684\u8bad\u7ec3\u51c6\u786e\u7387\u3002", "motivation": "\u5728\u91cd\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u5e03\u4e0b\uff0c\u5206\u6790\u5e38\u89c1\u79c1\u6709\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\u7684\u4f18\u5316\u884c\u4e3a\u3002", "method": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u5728\u91cd\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u5206\u5e03\u4e0b\uff0c\u5e38\u89c1\u7684\u79c1\u6709\u5b66\u4e60\u4f18\u5316\u7b97\u6cd5\u7684\u4f18\u5316\u884c\u4e3a\u3002", "result": "DP-AdamBC\u5728\u5b66\u4e60\u9891\u7387\u6700\u4f4e\u7684\u7c7b\u522b\u65f6\uff0c\u8bad\u7ec3\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e86\u7ea68%\u548c\u7ea65%\u3002", "conclusion": "DP-AdamBC\u901a\u8fc7\u6d88\u9664\u4f30\u8ba1\u635f\u5931\u66f2\u7387\u7684DP\u504f\u5dee\uff0c\u662f\u907f\u514d\u7531\u91cd\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u5f15\u8d77\u7684\u75c5\u6001\u6761\u4ef6\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002\u5728\u53d7\u63a7\u5b9e\u9a8c\u548c\u771f\u5b9e\u6570\u636e\u4e0a\uff0cDP-AdamBC\u5728\u5b66\u4e60\u9891\u7387\u6700\u4f4e\u7684\u7c7b\u522b\u65f6\uff0c\u8bad\u7ec3\u51c6\u786e\u7387\u5206\u522b\u63d0\u9ad8\u4e86\u7ea68%\u548c\u7ea65%\uff0c\u4ece\u800c\u5728\u7ecf\u9a8c\u4e0a\u66f4\u9002\u5408\u6570\u636e\u3002"}}
{"id": "2507.10539", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10539", "abs": "https://arxiv.org/abs/2507.10539", "authors": ["Tao Feng", "Yexin Wu", "Guanyu Lin", "Jiaxuan You"], "title": "Graph World Model", "comment": null, "summary": "World models (WMs) demonstrate strong capabilities in prediction, generation,\nand planning tasks. Existing WMs primarily focus on unstructured data and\ncannot leverage the ubiquitous structured data, often represented as graphs, in\nthe digital world. While multiple graph foundation models have been proposed,\nthey focus on graph learning tasks and cannot extend to diverse multi-modal\ndata and interdisciplinary tasks. To address these challenges, we propose the\nGraph World Model (GWM), a world model that supports both unstructured and\ngraph-structured states with multi-modal information and represents diverse\ntasks as actions. The core of a GWM is a generic message-passing algorithm to\naggregate structured information, either over a unified multi-modal token space\nby converting multi-modal data into text (GWM-T) or a unified multi-modal\nembedding space by modality-specific encoders (GWM-E). Notably, GWM introduces\naction nodes to support diverse tasks, where action nodes are linked to other\nnodes via direct reference or similarity computation. Extensive experiments on\nsix tasks from diverse domains, including multi-modal generation and matching,\nrecommendation, graph prediction, multi-agent, retrieval-augmented generation,\nand planning and optimization, show that the same GWM outperforms or matches\ndomain-specific baselines' performance, benefits from multi-hop structures, and\ndemonstrates strong zero-shot/few-shot capabilities on unseen new tasks. Our\ncode for GWM is released at https://github.com/ulab-uiuc/GWM.", "AI": {"tldr": "\u63d0\u51fa\u56fe\u4e16\u754c\u6a21\u578b\uff08GWM\uff09\uff0c\u4e00\u79cd\u80fd\u5904\u7406\u975e\u7ed3\u6784\u5316\u6570\u636e\u3001\u56fe\u7ed3\u6784\u6570\u636e\u548c\u591a\u6a21\u6001\u4fe1\u606f\u7684\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u52a8\u4f5c\u8282\u70b9\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u3002GWM\u5728\u591a\u9879\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u4e16\u754c\u6a21\u578b\u4e3b\u8981\u5173\u6ce8\u975e\u7ed3\u6784\u5316\u6570\u636e\uff0c\u65e0\u6cd5\u5229\u7528\u73b0\u5b9e\u4e16\u754c\u4e2d\u5e7f\u6cdb\u5b58\u5728\u7684\u56fe\u7ed3\u6784\u6570\u636e\u3002\u540c\u65f6\uff0c\u56fe\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e13\u6ce8\u4e8e\u56fe\u5b66\u4e60\u4efb\u52a1\uff0c\u96be\u4ee5\u6269\u5c55\u5230\u591a\u6a21\u6001\u6570\u636e\u548c\u8de8\u5b66\u79d1\u4efb\u52a1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u540c\u65f6\u5904\u7406\u975e\u7ed3\u6784\u5316\u548c\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u5e76\u652f\u6301\u591a\u6a21\u6001\u4fe1\u606f\u548c\u591a\u6837\u5316\u4efb\u52a1\u7684\u4e16\u754c\u6a21\u578b\u3002", "method": "GWM\u901a\u8fc7\u901a\u7528\u7684\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u6765\u805a\u5408\u7ed3\u6784\u5316\u4fe1\u606f\u3002\u5b83\u652f\u6301\u4e24\u79cd\u6a21\u5f0f\uff1aGWM-T\u5c06\u591a\u6a21\u6001\u6570\u636e\u8f6c\u6362\u4e3a\u6587\u672c\uff0c\u5904\u7406\u7edf\u4e00\u7684\u591a\u6a21\u6001\u6587\u672c\u7a7a\u95f4\uff1bGWM-E\u4f7f\u7528\u7279\u5b9a\u6a21\u6001\u7684\u7f16\u7801\u5668\uff0c\u5904\u7406\u7edf\u4e00\u7684\u591a\u6a21\u6001\u5d4c\u5165\u7a7a\u95f4\u3002GWM\u8fd8\u5f15\u5165\u4e86\u52a8\u4f5c\u8282\u70b9\uff0c\u901a\u8fc7\u76f4\u63a5\u5f15\u7528\u6216\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u4e0e\u56fe\u4e2d\u7684\u5176\u4ed6\u8282\u70b9\u76f8\u8fde\uff0c\u4ee5\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u3002", "result": "\u5728\u516d\u4e2a\u4e0d\u540c\u9886\u57df\u7684\u4efb\u52a1\uff08\u5305\u62ec\u591a\u6a21\u6001\u751f\u6210\u548c\u5339\u914d\u3001\u63a8\u8350\u3001\u56fe\u9884\u6d4b\u3001\u591a\u667a\u80fd\u4f53\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u4ee5\u53ca\u89c4\u5212\u548c\u4f18\u5316\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cGWM\u7684\u6027\u80fd\u4f18\u4e8e\u6216\u5ab2\u7f8e\u9886\u57df\u7279\u5b9a\u6a21\u578b\u3002\u6b64\u5916\uff0cGWM\u80fd\u591f\u5229\u7528\u591a\u8df3\u7ed3\u6784\uff0c\u5e76\u5728\u672a\u89c1\u8fc7\u7684\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u56fe\u4e16\u754c\u6a21\u578b\uff08GWM\uff09\uff0c\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u975e\u7ed3\u6784\u5316\u548c\u56fe\u7ed3\u6784\u72b6\u6001\uff0c\u5e76\u878d\u5408\u591a\u6a21\u6001\u4fe1\u606f\u548c\u591a\u6837\u5316\u4efb\u52a1\u7684\u4e16\u754c\u6a21\u578b\u3002GWM\u901a\u8fc7\u901a\u7528\u7684\u6d88\u606f\u4f20\u9012\u7b97\u6cd5\u805a\u5408\u7ed3\u6784\u5316\u4fe1\u606f\uff0c\u5e76\u5f15\u5165\u52a8\u4f5c\u8282\u70b9\u6765\u652f\u6301\u591a\u6837\u5316\u4efb\u52a1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGWM\u5728\u591a\u4e2a\u8de8\u9886\u57df\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u6216\u5ab2\u7f8e\u9886\u57df\u7279\u5b9a\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u96f6\u6837\u672c/\u5c11\u6837\u672c\u80fd\u529b\u3002"}}
{"id": "2507.10552", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10552", "abs": "https://arxiv.org/abs/2507.10552", "authors": ["Vladimir Iashin", "Horace Lee", "Dan Schofield", "Andrew Zisserman"], "title": "Self-supervised Learning on Camera Trap Footage Yields a Strong Universal Face Embedder", "comment": "Accepted for publication. Project page, code and weights:\n  https://www.robots.ox.ac.uk/~vgg/research/ChimpUFE/", "summary": "Camera traps are revolutionising wildlife monitoring by capturing vast\namounts of visual data; however, the manual identification of individual\nanimals remains a significant bottleneck. This study introduces a fully\nself-supervised approach to learning robust chimpanzee face embeddings from\nunlabeled camera-trap footage. Leveraging the DINOv2 framework, we train Vision\nTransformers on automatically mined face crops, eliminating the need for\nidentity labels. Our method demonstrates strong open-set re-identification\nperformance, surpassing supervised baselines on challenging benchmarks such as\nBossou, despite utilising no labelled data during training. This work\nunderscores the potential of self-supervised learning in biodiversity\nmonitoring and paves the way for scalable, non-invasive population studies.", "AI": {"tldr": "\u76f8\u673a\u9677\u9631\u4ea7\u751f\u4e86\u5927\u91cf\u7684\u89c6\u89c9\u6570\u636e\uff0c\u4f46\u624b\u52a8\u8bc6\u522b\u52a8\u7269\u662f\u4e00\u4e2a\u8017\u65f6\u7684\u8fc7\u7a0b\u3002\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u81ea\u76d1\u7763\u65b9\u6cd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u672a\u6807\u8bb0\u7684\u76f8\u673a\u9677\u9631\u6570\u636e\u4ece\u672a\u6807\u8bb0\u7684\u9ed1\u7329\u7329\u9762\u90e8\u56fe\u50cf\u4e2d\u5b66\u4e60\u9762\u90e8\u5d4c\u5165\u3002\u6211\u4eec\u7684\u65b9\u6cd5\u5728\u91cd\u65b0\u8bc6\u522b\u4efb\u52a1\u4e0a\u4f18\u4e8e\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u4efb\u4f55\u8eab\u4efd\u6807\u7b7e\u3002", "motivation": "\u76f8\u673a\u9677\u9631\u5728\u901a\u8fc7\u6355\u83b7\u5927\u91cf\u89c6\u89c9\u6570\u636e\u6765\u9769\u65b0\u91ce\u751f\u52a8\u7269\u76d1\u6d4b\uff0c\u4f46\u624b\u52a8\u8bc6\u522b\u5355\u4e2a\u52a8\u7269\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u74f6\u9888\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u79cd\u5b8c\u5168\u81ea\u76d1\u7763\u7684\u65b9\u6cd5\uff0c\u4ece\u65e0\u6807\u7b7e\u7684\u76f8\u673a\u9677\u9631\u7247\u6bb5\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u9ed1\u7329\u7329\u9762\u90e8\u5d4c\u5165\u3002\u5229\u7528DINOv2\u6846\u67b6\uff0c\u6211\u4eec\u5728\u81ea\u52a8\u6316\u6398\u7684\u9762\u90e8\u88c1\u526a\u4e0a\u8bad\u7ec3Vision Transformers\uff0c\u65e0\u9700\u8eab\u4efd\u6807\u7b7e\u3002", "result": "\u6211\u4eec\u7684\u65b9\u6cd5\u5728Bossou\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u4e0a\uff0c\u5728\u5f00\u653e\u96c6\u91cd\u65b0\u8bc6\u522b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u751a\u81f3\u8d85\u8d8a\u4e86\u76d1\u7763\u57fa\u7ebf\uff0c\u5c3d\u7ba1\u5728\u8bad\u7ec3\u671f\u95f4\u6ca1\u6709\u4f7f\u7528\u4efb\u4f55\u6807\u7b7e\u6570\u636e\u3002", "conclusion": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u81ea\u76d1\u7763\u5b66\u4e60\u5728\u751f\u7269\u591a\u6837\u6027\u76d1\u6d4b\u4e2d\u7684\u6f5c\u529b\uff0c\u5e76\u4e3a\u53ef\u6269\u5c55\u3001\u975e\u4fb5\u5165\u6027\u7684\u4eba\u53e3\u7814\u7a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2507.10540", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10540", "abs": "https://arxiv.org/abs/2507.10540", "authors": ["Tao Feng", "Haozhen Zhang", "Zijie Lei", "Pengrui Han", "Mostofa Patwary", "Mohammad Shoeybi", "Bryan Catanzaro", "Jiaxuan You"], "title": "Fusing LLM Capabilities with Routing Data", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has created a vibrant\necosystem of diverse architectures, each with unique strengths due to\ndifferences in design, training data, and objectives. However, most\napplications still rely on a single backend model, limiting coverage of\ncapabilities and leading to inefficiencies in performance and token cost when\ntackling complex tasks. We highlight an underexploited opportunity: LLM routing\ndata, produced when hosting platforms route diverse queries to different\nmodels, which can reveal comparative strengths across tasks. To address this,\nwe propose FusionBench, a comprehensive routing benchmark covering 14 tasks\nacross five domains with 20 open-source LLMs (8B to 671B parameters), capturing\n103M tokens and summarizing reusable thought templates from top models.\nBuilding on this, we introduce FusionFactory, a systematic fusion framework\nwith three levels: (1) query-level fusion, tailoring routers for each query\nusing both direct responses and reasoning-augmented outputs; (2) thought-level\nfusion, leveraging abstract templates derived from top-performing LLMs' answers\nto similar queries; and (3) model-level fusion, transferring capabilities\nbetween models via distillation, using top responses or highest judge scores as\ntraining data. Experiments show FusionFactory consistently outperforms the best\nindividual LLM across all 14 benchmarks, with optimal fusion configurations\nvarying by benchmark, demonstrating the value of systematic LLM fusion in\nharnessing complementary strengths and improving overall performance.", "AI": {"tldr": "LLM routing data can reveal model strengths. FusionBench benchmark and FusionFactory framework improve LLM performance by fusing models at query, thought, and model levels.", "motivation": "Most applications use a single LLM backend, limiting capabilities and efficiency. LLM routing data reveals comparative strengths across tasks, an underexploited opportunity.", "method": "Proposes FusionBench benchmark (14 tasks, 5 domains, 20 LLMs, 103M tokens) and FusionFactory framework (query-level, thought-level, model-level fusion).", "result": "FusionFactory consistently outperforms the best individual LLM across all 14 benchmarks.", "conclusion": "FusionFactory systematically fuses LLMs, outperforming individual models across benchmarks by leveraging complementary strengths. Optimal fusion configurations are benchmark-specific."}}
{"id": "2507.10546", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10546", "abs": "https://arxiv.org/abs/2507.10546", "authors": ["Kexin Gu Baugh", "Vincent Perreault", "Matthew Baugh", "Luke Dickens", "Katsumi Inoue", "Alessandra Russo"], "title": "Disentangling Neural Disjunctive Normal Form Models", "comment": "Accepted at NeSy 2025", "summary": "Neural Disjunctive Normal Form (DNF) based models are powerful and\ninterpretable approaches to neuro-symbolic learning and have shown promising\nresults in classification and reinforcement learning settings without prior\nknowledge of the tasks. However, their performance is degraded by the\nthresholding of the post-training symbolic translation process. We show here\nthat part of the performance degradation during translation is due to its\nfailure to disentangle the learned knowledge represented in the form of the\nnetworks' weights. We address this issue by proposing a new disentanglement\nmethod; by splitting nodes that encode nested rules into smaller independent\nnodes, we are able to better preserve the models' performance. Through\nexperiments on binary, multiclass, and multilabel classification tasks\n(including those requiring predicate invention), we demonstrate that our\ndisentanglement method provides compact and interpretable logical\nrepresentations for the neural DNF-based models, with performance closer to\nthat of their pre-translation counterparts. Our code is available at\nhttps://github.com/kittykg/disentangling-ndnf-classification.", "AI": {"tldr": "\u795e\u7ecfDNF\u6a21\u578b\u5728\u7b26\u53f7\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u5b58\u5728\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u672a\u80fd\u6709\u6548\u5206\u89e3\u7f51\u7edc\u6743\u91cd\u4e2d\u8574\u542b\u7684\u77e5\u8bc6\u3002\u672c\u6587\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u8282\u70b9\u5206\u88c2\u6210\u66f4\u5c0f\u7684\u72ec\u7acb\u5355\u5143\uff0c\u6539\u5584\u4e86\u77e5\u8bc6\u8868\u793a\u7684\u89e3\u8026\uff0c\u4ece\u800c\u5728\u4e0d\u635f\u5bb3\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u4e3a\u795e\u7ecfDNF\u6a21\u578b\u751f\u6210\u4e86\u66f4\u7d27\u51d1\u3001\u66f4\u6613\u4e8e\u7406\u89e3\u7684\u903b\u8f91\u8868\u793a\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9700\u8981\u8c13\u8bcd\u53d1\u660e\u7684\u590d\u6742\u5206\u7c7b\u4efb\u52a1\u65f6\uff0c\u5176\u6548\u679c\u5c24\u4e3a\u663e\u8457\u3002", "motivation": "\u795e\u7ecf\u6790\u53d6\u8303\u5f0f\uff08DNF\uff09\u6a21\u578b\u5728\u65e0\u9700\u4efb\u52a1\u5148\u9a8c\u77e5\u8bc6\u7684\u5206\u7c7b\u548c\u5f3a\u5316\u5b66\u4e60\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u4f46\u5176\u6027\u80fd\u5728\u8bad\u7ec3\u540e\u7b26\u53f7\u8f6c\u6362\u8fc7\u7a0b\u4e2d\u53d7\u5230\u9608\u503c\u5904\u7406\u7684\u635f\u5bb3\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u90e8\u5206\u6027\u80fd\u4e0b\u964d\u662f\u7531\u4e8e\u5176\u672a\u80fd\u5206\u89e3\u7f51\u7edc\u6743\u91cd\u4e2d\u8868\u793a\u7684\u5b66\u4e60\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u89e3\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u7f16\u7801\u5d4c\u5957\u89c4\u5219\u7684\u8282\u70b9\u5206\u88c2\u4e3a\u66f4\u5c0f\u7684\u72ec\u7acb\u8282\u70b9\u6765\u89e3\u51b3\u77e5\u8bc6\u5206\u89e3\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u5206\u89e3\u65b9\u6cd5\u5728\u4e8c\u5143\u3001\u591a\u7c7b\u522b\u548c\u591a\u6807\u7b7e\u5206\u7c7b\u4efb\u52a1\u4e0a\uff08\u5305\u62ec\u9700\u8981\u8c13\u8bcd\u53d1\u660e\u7684\u4efb\u52a1\uff09\u63d0\u4f9b\u4e86\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u8868\u793a\uff0c\u6027\u80fd\u66f4\u63a5\u8fd1\u7ffb\u8bd1\u524d\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u7f16\u7801\u5d4c\u5957\u89c4\u5219\u7684\u8282\u70b9\u5206\u89e3\u4e3a\u66f4\u5c0f\u7684\u72ec\u7acb\u8282\u70b9\uff0c\u80fd\u591f\u66f4\u597d\u5730\u4fdd\u6301\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e3a\u795e\u7ecfDNF\u6a21\u578b\u63d0\u4f9b\u7d27\u51d1\u4e14\u53ef\u89e3\u91ca\u7684\u903b\u8f91\u8868\u793a\uff0c\u6027\u80fd\u66f4\u63a5\u8fd1\u7ffb\u8bd1\u524d\u7684\u6a21\u578b\u3002"}}
