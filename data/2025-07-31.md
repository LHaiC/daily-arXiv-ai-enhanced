<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 69]
- [cs.CL](#cs.CL) [Total: 37]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 29]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 14]
- [cs.RO](#cs.RO) [Total: 12]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.AR](#cs.AR) [Total: 1]
- [cs.DC](#cs.DC) [Total: 5]
- [quant-ph](#quant-ph) [Total: 49]
- [cs.ET](#cs.ET) [Total: 2]
- [cs.DS](#cs.DS) [Total: 3]
- [eess.SY](#eess.SY) [Total: 11]
- [cs.SI](#cs.SI) [Total: 3]
- [physics.app-ph](#physics.app-ph) [Total: 2]
- [eess.SP](#eess.SP) [Total: 10]
- [cs.LO](#cs.LO) [Total: 2]
- [cs.MA](#cs.MA) [Total: 5]
- [cs.GT](#cs.GT) [Total: 1]
- [cs.AI](#cs.AI) [Total: 17]
- [cs.LG](#cs.LG) [Total: 58]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Runtime Failure Hunting for Physics Engine Based Software Systems: How Far Can We Go?](https://arxiv.org/abs/2507.22099)
*Shuqing Li,Qiang Chen,Xiaoxue Ren,Michael R. Lyu*

Main category: cs.CV

TL;DR: 本文研究了物理引擎软件中的物理失败问题，提出了新的检测方法和分类法，并提供了改进建议。


<details>
  <summary>Details</summary>
Motivation: 物理引擎（PE）是模拟物理交互的基础软件框架，其应用广泛，但目前存在物理失败问题，会影响软件可靠性、用户体验，甚至在自动驾驶汽车或医疗机器人等领域引发严重故障。现有的针对PE软件的测试方法不足，无法有效检测这些复杂的物理失败。

Method: 本文献提出了首个针对物理引擎软件中的物理失败的、大规模的实证研究，以探究物理失败的表现形式、检测技术的有效性以及开发者对当前检测实践的看法。具体而言，我们提出了一个物理失败表现形式的分类法，对包括深度学习、基于提示的技术和大语言模型在内的检测方法进行了全面的评估，并通过开发者经验提供了改进检测方法的实用见解。此外，我们发布了PhysiXFails、代码及相关材料以支持未来的研究。

Result: 本文为物理引擎软件中的物理失败提供了首个大规模实证研究，包含一个物理失败表现形式的分类法，评估了多种检测方法（包括深度学习、基于提示的技术和大语言模型），并提供了来自开发者经验的实用见解，以改进检测方法。研究结果和相关资源已公开以供未来研究使用。

Conclusion: 目前针对基于物理引擎的软件的测试方法存在不足，通常需要白盒访问并且侧重于崩溃检测而非语义复杂的物理失败。

Abstract: Physics Engines (PEs) are fundamental software frameworks that simulate
physical interactions in applications ranging from entertainment to
safety-critical systems. Despite their importance, PEs suffer from physics
failures, deviations from expected physical behaviors that can compromise
software reliability, degrade user experience, and potentially cause critical
failures in autonomous vehicles or medical robotics. Current testing approaches
for PE-based software are inadequate, typically requiring white-box access and
focusing on crash detection rather than semantically complex physics failures.
This paper presents the first large-scale empirical study characterizing
physics failures in PE-based software. We investigate three research questions
addressing the manifestations of physics failures, the effectiveness of
detection techniques, and developer perceptions of current detection practices.
Our contributions include: (1) a taxonomy of physics failure manifestations;
(2) a comprehensive evaluation of detection methods including deep learning,
prompt-based techniques, and large multimodal models; and (3) actionable
insights from developer experiences for improving detection approaches. To
support future research, we release PhysiXFails, code, and other materials at
https://sites.google.com/view/physics-failure-detection.

</details>


### [2] [Trade-offs in Image Generation: How Do Different Dimensions Interact?](https://arxiv.org/abs/2507.22100)
*Sicheng Zhang,Binzhu Xie,Zhonghao Yan,Yuli Zhang,Donghao Zhou,Xiaofei Chen,Shi Qiu,Jiaqi Liu,Guoyang Xie,Zhichao Lu*

Main category: cs.CV

TL;DR: 该研究提出了TRIG-Bench数据集和TRIGScore评估指标，用于量化和可视化T2I和I2I生成模型在10个维度上的性能权衡。实验表明，维度权衡图（DTM）能有效揭示模型能力间的权衡，并通过微调DTM可提升模型整体性能。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）和图像到图像（I2I）生成模型在质量、对齐、多样性和鲁棒性等方面存在复杂的权衡，但由于缺乏能够精细量化这些权衡的数据集以及单一指标用于多个维度的评估，这些权衡很少被探索。

Method: 引入了TRIG-Bench数据集和TRIGScore评估指标，TRIG-Bench包含10个维度和40,200个样本，涵盖132个成对维度子集。TRIGScore利用视觉语言模型（VLM）作为裁判，能够自动适应不同评估维度。此外，还提出了一种关系识别系统来生成维度权衡图（DTM）。

Result: 在TRIG-Bench数据集和TRIGScore评估指标的基础上，对14个T2I和I2I模型进行了评估。实验结果表明，DTM能够提供对生成模型能力之间权衡的全面理解，并且通过在DTM上进行微调可以有效改善模型性能。

Conclusion: 模型在图像生成任务中的表现可以通过维度权衡图（DTM）进行可视化，DTM能够提供对生成模型能力之间权衡的全面理解。通过在DTM上进行微调，可以改善模型在特定维度上的劣势，并提升整体性能。

Abstract: Model performance in text-to-image (T2I) and image-to-image (I2I) generation
often depends on multiple aspects, including quality, alignment, diversity, and
robustness. However, models' complex trade-offs among these dimensions have
rarely been explored due to (1) the lack of datasets that allow fine-grained
quantification of these trade-offs, and (2) the use of a single metric for
multiple dimensions. To bridge this gap, we introduce TRIG-Bench (Trade-offs in
Image Generation), which spans 10 dimensions (Realism, Originality, Aesthetics,
Content, Relation, Style, Knowledge, Ambiguity, Toxicity, and Bias), contains
40,200 samples, and covers 132 pairwise dimensional subsets. Furthermore, we
develop TRIGScore, a VLM-as-judge metric that automatically adapts to various
dimensions. Based on TRIG-Bench and TRIGScore, we evaluate 14 models across T2I
and I2I tasks. In addition, we propose the Relation Recognition System to
generate the Dimension Trade-off Map (DTM) that visualizes the trade-offs among
model-specific capabilities. Our experiments demonstrate that DTM consistently
provides a comprehensive understanding of the trade-offs between dimensions for
each type of generative model. Notably, we show that the model's
dimension-specific weaknesses can be mitigated through fine-tuning on DTM to
enhance overall performance. Code is available at:
https://github.com/fesvhtr/TRIG

</details>


### [3] [AI in Agriculture: A Survey of Deep Learning Techniques for Crops, Fisheries and Livestock](https://arxiv.org/abs/2507.22101)
*Umair Nawaz,Muhammad Zaigham Zaheer,Fahad Shahbaz Khan,Hisham Cholakkal,Salman Khan,Rao Muhammad Anwer*

Main category: cs.CV

TL;DR: 人工智能在农业领域（包括作物、渔业和畜牧业）的应用正在迅速发展，以应对气候变化和资源限制等挑战。本调查回顾了传统机器学习、深度学习和视觉-语言模型等技术，并讨论了数据、评估和部署等方面的挑战，同时强调了多模态数据、边缘计算和领域适应性模型等未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 鉴于全球人口不断增长，对粮食生产提出了巨大挑战，例如气候变化、资源限制和可持续管理的需求。为了应对这些挑战，需要高效、准确和可扩展的技术解决方案，因此人工智能（AI）变得至关重要。

Method: 本研究系统性地回顾了超过 200 项有关人工智能在农业领域应用的研究工作，涵盖了传统的机器学习方法、先进的深度学习技术（例如视觉 Transformer）以及最近的视觉-语言基础模型（例如 CLIP）。

Result: 该调查涵盖了人工智能在农业领域的应用，重点关注了作物病害检测、牲畜健康管理和水产物种监测等任务。此外，还讨论了数据变异性、数据集、性能评估指标和地理重点等实施挑战。

Conclusion: 该调查全面回顾了人工智能在农业领域的应用，重点关注了作物病害检测、牲畜健康管理和水产物种监测等任务。它还讨论了数据变异性、数据集、性能评估指标和地理重点等主要实施挑战，并强调了多模态数据集成、高效边缘设备部署和领域适应性 AI 模型等潜在的开放研究方向。

Abstract: Crops, fisheries and livestock form the backbone of global food production,
essential to feed the ever-growing global population. However, these sectors
face considerable challenges, including climate variability, resource
limitations, and the need for sustainable management. Addressing these issues
requires efficient, accurate, and scalable technological solutions,
highlighting the importance of artificial intelligence (AI). This survey
presents a systematic and thorough review of more than 200 research works
covering conventional machine learning approaches, advanced deep learning
techniques (e.g., vision transformers), and recent vision-language foundation
models (e.g., CLIP) in the agriculture domain, focusing on diverse tasks such
as crop disease detection, livestock health management, and aquatic species
monitoring. We further cover major implementation challenges such as data
variability and experimental aspects: datasets, performance evaluation metrics,
and geographical focus. We finish the survey by discussing potential open
research directions emphasizing the need for multimodal data integration,
efficient edge-device deployment, and domain-adaptable AI models for diverse
farming environments. Rapid growth of evolving developments in this field can
be actively tracked on our project page:
https://github.com/umair1221/AI-in-Agriculture

</details>


### [4] [Color as the Impetus: Transforming Few-Shot Learner](https://arxiv.org/abs/2507.22136)
*Chaofei Qi,Zhitai Liu,Jianbin Qiu*

Main category: cs.CV

TL;DR: This paper introduces ColorSense Learner and ColorSense Distiller, bio-inspired meta-learning frameworks that leverage human color perception for improved few-shot learning. They filter irrelevant features, enhance discriminative characteristics, and boost generalization, robustness, and transferability across various benchmarks.


<details>
  <summary>Details</summary>
Motivation: To pioneer an innovative viewpoint on few-shot learning by simulating human color perception mechanisms, addressing the conventional neglect of color information in meta-learning methods.

Method: The ColorSense Learner framework utilizes inter-channel feature extraction and interactive learning, emphasizing distinct color information across different channels. The ColorSense Distiller, based on knowledge distillation, incorporates prior teacher knowledge to augment the student network's meta-learning capacity.

Result: Experiments on eleven few-shot benchmarks reveal that the proposed methods have extremely strong generalization ability, robustness, and transferability, effortlessly handling few-shot classification from the perspective of color perception.

Conclusion: ColorSense Learner and ColorSense Distiller frameworks effectively filter irrelevant features, capture discriminative characteristics, and enhance meta-learning capacity by simulating human color perception mechanisms. These methods demonstrate strong generalization ability, robustness, and transferability in few-shot classification.

Abstract: Humans possess innate meta-learning capabilities, partly attributable to
their exceptional color perception. In this paper, we pioneer an innovative
viewpoint on few-shot learning by simulating human color perception mechanisms.
We propose the ColorSense Learner, a bio-inspired meta-learning framework that
capitalizes on inter-channel feature extraction and interactive learning. By
strategically emphasizing distinct color information across different channels,
our approach effectively filters irrelevant features while capturing
discriminative characteristics. Color information represents the most intuitive
visual feature, yet conventional meta-learning methods have predominantly
neglected this aspect, focusing instead on abstract feature differentiation
across categories. Our framework bridges the gap via synergistic color-channel
interactions, enabling better intra-class commonality extraction and larger
inter-class differences. Furthermore, we introduce a meta-distiller based on
knowledge distillation, ColorSense Distiller, which incorporates prior teacher
knowledge to augment the student network's meta-learning capacity. We've
conducted comprehensive coarse/fine-grained and cross-domain experiments on
eleven few-shot benchmarks for validation. Numerous experiments reveal that our
methods have extremely strong generalization ability, robustness, and
transferability, and effortless handle few-shot classification from the
perspective of color perception.

</details>


### [5] [Enhancing efficiency in paediatric brain tumour segmentation using a pathologically diverse single-center clinical dataset](https://arxiv.org/abs/2507.22152)
*A. Piffer,J. A. Buchner,A. G. Gennari,P. Grehten,S. Sirin,E. Ross,I. Ezhov,M. Rosier,J. C. Peeken,M. Piraud,B. Menze,A. Guerreiro Stücklin,A. Jakab,F. Kofler*

Main category: cs.CV

TL;DR: 深度学习模型在儿童脑肿瘤（特别是全肿瘤和T2高信号区域）的分割方面表现良好，其性能与人类标注者相当，但增强肿瘤和囊性成分的分割仍需改进。简化MRI协议可能对儿童神经肿瘤学工作流程有益。


<details>
  <summary>Details</summary>
Motivation: 儿童脑肿瘤（PBT）是最常见的儿童实体恶性肿瘤，具有多样的组织学、分子亚型、影像学特征和预后。PBT的诊断和治疗具有挑战性。基于深度学习（DL）的分割技术为肿瘤描绘提供了有前景的工具，但其在不同PBT亚型和MRI方案下的性能仍不确定。

Method: 回顾性分析了一个包含174名患有不同类型脑肿瘤（包括高级别胶质瘤（HGG）、低级别胶质瘤（LGG）、髓母细胞瘤（MB）、室管膜瘤和其他罕见类型）的儿科患者的单中心队列。使用了包含T1、钆对比增强T1（T1-C）、T2和FLAIR序列的MRI数据。手动标注了四个肿瘤亚区域：全肿瘤（WT）、T2高信号（T2H）、增强肿瘤（ET）和囊性成分（CC）。训练并测试了一个3D nnU-Net模型（121/53拆分），使用Dice相似系数（DSC）评估分割性能，并与研究内和研究间评估者的一致性进行比较。

Result: 该模型在WT和T2H分割方面表现稳健（平均DSC：0.85），与人类标注者的一致性（平均DSC：0.86）相当。ET分割的准确性中等（平均DSC：0.75），而CC分割性能较差。分割准确性因肿瘤类型、MRI序列组合和位置而异。值得注意的是，单独使用T1、T1-C和T2序列获得的结果几乎等同于完整方案。

Conclusion: 深度学习（DL）对于儿童脑肿瘤（PBT）是可行的，特别是对于T2高信号（T2H）和全肿瘤（WT）的分割。然而，对于增强肿瘤（ET）和囊性成分（CC）的分割仍然存在挑战，需要进一步改进。这些发现支持简化和自动化流程以增强体积评估和优化儿童神经肿瘤学工作流程的潜力。

Abstract: Background Brain tumours are the most common solid malignancies in children,
encompassing diverse histological, molecular subtypes and imaging features and
outcomes. Paediatric brain tumours (PBTs), including high- and low-grade
gliomas (HGG, LGG), medulloblastomas (MB), ependymomas, and rarer forms, pose
diagnostic and therapeutic challenges. Deep learning (DL)-based segmentation
offers promising tools for tumour delineation, yet its performance across
heterogeneous PBT subtypes and MRI protocols remains uncertain. Methods A
retrospective single-centre cohort of 174 paediatric patients with HGG, LGG,
medulloblastomas (MB), ependymomas, and other rarer subtypes was used. MRI
sequences included T1, T1 post-contrast (T1-C), T2, and FLAIR. Manual
annotations were provided for four tumour subregions: whole tumour (WT),
T2-hyperintensity (T2H), enhancing tumour (ET), and cystic component (CC). A 3D
nnU-Net model was trained and tested (121/53 split), with segmentation
performance assessed using the Dice similarity coefficient (DSC) and compared
against intra- and inter-rater variability. Results The model achieved robust
performance for WT and T2H (mean DSC: 0.85), comparable to human annotator
variability (mean DSC: 0.86). ET segmentation was moderately accurate (mean
DSC: 0.75), while CC performance was poor. Segmentation accuracy varied by
tumour type, MRI sequence combination, and location. Notably, T1, T1-C, and T2
alone produced results nearly equivalent to the full protocol. Conclusions DL
is feasible for PBTs, particularly for T2H and WT. Challenges remain for ET and
CC segmentation, highlighting the need for further refinement. These findings
support the potential for protocol simplification and automation to enhance
volumetric assessment and streamline paediatric neuro-oncology workflows.

</details>


### [6] [Temporally Consistent Unsupervised Segmentation for Mobile Robot Perception](https://arxiv.org/abs/2507.22194)
*Christian Ellis,Maggie Wigness,Craig Lennon,Lance Fiondella*

Main category: cs.CV

TL;DR: Frontier-Seg 是一种新方法，通过对 DINOv2 等基础模型的特征进行聚类并确保跨帧的时间一致性，可以在没有人工监督的情况下分割机器人视频中的地形，解决了现有无监督分割方法缺乏时间一致性的问题。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前监督式语义分割方法依赖昂贵的数据收集和劳动密集型真实标签来训练深度模型，以及在无标签数据和领域特定或模糊的语义类别存在的非演练、非结构化环境中，现有方法缺乏时间一致性的问题。

Method: Frontier-Seg 通过对来自基础模型骨干（特别是 DINOv2）的超像素级特征进行聚类，并跨帧强制执行时间一致性，从而在没有人为监督的情况下识别持久的地形边界或前沿。

Result: Frontier-Seg 实现了跨越非结构化越野环境的时间一致的无监督地形分割。

Conclusion: Frontier-Seg 能够跨越非结构化越野环境进行无监督分割，并在 RUGD 和 RELLIS-3D 等基准数据集上进行了评估。

Abstract: Rapid progress in terrain-aware autonomous ground navigation has been driven
by advances in supervised semantic segmentation. However, these methods rely on
costly data collection and labor-intensive ground truth labeling to train deep
models. Furthermore, autonomous systems are increasingly deployed in
unrehearsed, unstructured environments where no labeled data exists and
semantic categories may be ambiguous or domain-specific. Recent zero-shot
approaches to unsupervised segmentation have shown promise in such settings but
typically operate on individual frames, lacking temporal consistency-a critical
property for robust perception in unstructured environments. To address this
gap we introduce Frontier-Seg, a method for temporally consistent unsupervised
segmentation of terrain from mobile robot video streams. Frontier-Seg clusters
superpixel-level features extracted from foundation model
backbones-specifically DINOv2-and enforces temporal consistency across frames
to identify persistent terrain boundaries or frontiers without human
supervision. We evaluate Frontier-Seg on a diverse set of benchmark
datasets-including RUGD and RELLIS-3D-demonstrating its ability to perform
unsupervised segmentation across unstructured off-road environments.

</details>


### [7] [SmartCLIP: Modular Vision-language Alignment with Identification Guarantees](https://arxiv.org/abs/2507.22264)
*Shaoan Xie,Lingjing Kong,Yujia Zheng,Yu Yao,Zeyu Tang,Eric P. Xing,Guangyi Chen,Kun Zhang*

Main category: cs.CV

TL;DR: 本文提出了一种名为'ours'的新方法，解决了CLIP模型在信息对齐和表示解耦方面的不足，并在多项任务上取得了更好的效果。


<details>
  <summary>Details</summary>
Motivation: CLIP模型在处理图像-文本匹配任务时，由于训练数据（如MSCOCO）中可能存在短标题描述图像不相关区域、长标题导致表示纠缠等问题，限制了其泛化能力。本文旨在解决这些问题，提出一种更灵活、更解耦的对齐方法。

Method: 本文提出了一种名为'ours'的新方法，该方法在理论基础上，通过模块化地识别和对齐最相关的视觉和文本表示，实现了跨模态表示在不同粒度下的灵活对齐，能够保留完整的跨媒体语义信息，并将视觉表示解耦以捕捉细粒度的文本概念。

Result: 通过在多个下游任务上的优越性能证明了该方法能够有效处理信息不对齐问题，并支持其识别理论。

Conclusion: CLIP在多模态学习领域取得了显著进展，但存在信息不对齐和表示纠缠的问题。本文提出的新方法可以解决这些问题，并在各种下游任务中展现出优越性能。

Abstract: Contrastive Language-Image Pre-training (CLIP)~\citep{radford2021learning}
has emerged as a pivotal model in computer vision and multimodal learning,
achieving state-of-the-art performance at aligning visual and textual
representations through contrastive learning. However, CLIP struggles with
potential information misalignment in many image-text datasets and suffers from
entangled representation. On the one hand, short captions for a single image in
datasets like MSCOCO may describe disjoint regions in the image, leaving the
model uncertain about which visual features to retain or disregard. On the
other hand, directly aligning long captions with images can lead to the
retention of entangled details, preventing the model from learning
disentangled, atomic concepts -- ultimately limiting its generalization on
certain downstream tasks involving short prompts.
  In this paper, we establish theoretical conditions that enable flexible
alignment between textual and visual representations across varying levels of
granularity. Specifically, our framework ensures that a model can not only
\emph{preserve} cross-modal semantic information in its entirety but also
\emph{disentangle} visual representations to capture fine-grained textual
concepts. Building on this foundation, we introduce \ours, a novel approach
that identifies and aligns the most relevant visual and textual representations
in a modular manner. Superior performance across various tasks demonstrates its
capability to handle information misalignment and supports our identification
theory. The code is available at https://github.com/Mid-Push/SmartCLIP.

</details>


### [8] [HOG-CNN: Integrating Histogram of Oriented Gradients with Convolutional Neural Networks for Retinal Image Classification](https://arxiv.org/abs/2507.22274)
*Faisal Ahmed*

Main category: cs.CV

TL;DR: 提出了一种名为HOG-CNN的混合模型，用于自动化视网膜疾病（如糖尿病视网膜病变、青光眼和年龄相关性黄斑变性）的早期检测。该模型结合了HOG特征和CNN表示，在多个数据集上取得了优于现有模型的准确率和AUC值，并且具有轻量级和可解释的优点，适合在资源有限的临床环境中使用。


<details>
  <summary>Details</summary>
Motivation: 传统的视网膜疾病诊断方法依赖于手动解释，既耗时又耗费资源，因此需要一种自动化的解决方案。

Method: 提出了一种基于混合特征提取模型HOG-CNN的自动化、可解释的临床决策支持框架，该框架整合了手工设计的方向梯度直方图(HOG)特征和深度卷积神经网络(CNN)表示，以同时捕捉局部纹理模式和高层语义特征。

Result: HOG-CNN在三个公共基准数据集上均表现出持续的高性能：在APTOS 2019（DR二分类）上达到98.5%的准确率和99.2%的AUC，在IC-AMD（AMD诊断）上达到92.8%的准确率、94.8%的精确率和94.5%的AUC，在ORIGA（青光眼检测）上达到83.9%的准确率和87.2%的AUC，优于多个现有模型。

Conclusion: HOG-CNN模型在视网膜疾病筛查方面表现出强大的性能和可扩展性，其轻量级和可解释的设计特别适合资源受限的临床环境。

Abstract: The analysis of fundus images is critical for the early detection and
diagnosis of retinal diseases such as Diabetic Retinopathy (DR), Glaucoma, and
Age-related Macular Degeneration (AMD). Traditional diagnostic workflows,
however, often depend on manual interpretation and are both time- and
resource-intensive. To address these limitations, we propose an automated and
interpretable clinical decision support framework based on a hybrid feature
extraction model called HOG-CNN. Our key contribution lies in the integration
of handcrafted Histogram of Oriented Gradients (HOG) features with deep
convolutional neural network (CNN) representations. This fusion enables our
model to capture both local texture patterns and high-level semantic features
from retinal fundus images. We evaluated our model on three public benchmark
datasets: APTOS 2019 (for binary and multiclass DR classification), ORIGA (for
Glaucoma detection), and IC-AMD (for AMD diagnosis); HOG-CNN demonstrates
consistently high performance. It achieves 98.5\% accuracy and 99.2 AUC for
binary DR classification, and 94.2 AUC for five-class DR classification. On the
IC-AMD dataset, it attains 92.8\% accuracy, 94.8\% precision, and 94.5 AUC,
outperforming several state-of-the-art models. For Glaucoma detection on ORIGA,
our model achieves 83.9\% accuracy and 87.2 AUC, showing competitive
performance despite dataset limitations. We show, through comprehensive
appendix studies, the complementary strength of combining HOG and CNN features.
The model's lightweight and interpretable design makes it particularly suitable
for deployment in resource-constrained clinical environments. These results
position HOG-CNN as a robust and scalable tool for automated retinal disease
screening.

</details>


### [9] [AlphaEarth Foundations: An embedding field model for accurate and efficient global mapping from sparse label data](https://arxiv.org/abs/2507.22291)
*Christopher F. Brown,Michal R. Kazmierski,Valerie J. Pasquarella,William J. Rucklidge,Masha Samsikova,Chenhui Zhang,Evan Shelhamer,Estefania Lahera,Olivia Wiles,Simon Ilyushchenko,Noel Gorelick,Lihui Lydia Zhang,Sophia Alj,Emily Schechter,Sean Askay,Oliver Guinan,Rebecca Moore,Alexis Boukouvalas,Pushmeet Kohli*

Main category: cs.CV

TL;DR: AlphaEarth Foundations 是一个创新的嵌入场模型，它能够利用海量的地球观测数据，克服标签稀缺的挑战，生成通用的地理空间表示，从而在各种地图绘制任务中取得优越的性能，并支持从地方到全球尺度的应用。


<details>
  <summary>Details</summary>
Motivation: 为了解决地球观测数据量大但高质量标签稀缺的问题，并克服手动标注的成本和难度。

Method: 该模型是一个嵌入场模型，能够吸收时空和测量上下文，并从多个来源生成通用的地理空间表示。

Result: AlphaEarth Foundations 生成的嵌入在各种地图绘制评估中持续优于所有先前的特征提取方法，且无需重新训练。该模型能够实现从地方到全球尺度的准确高效的地图制作和监测系统生产。

Conclusion: AlphaEarth Foundations 通过整合多源时空和测量上下文，生成了通用的地理空间表示，实现了从地方到全球尺度的高效制图和监测。其生成的嵌入能够一致地优于所有先前的特征提取方法，无需重新训练。

Abstract: Unprecedented volumes of Earth observation data are continually collected
around the world, but high-quality labels remain scarce given the effort
required to make physical measurements and observations. This has led to
considerable investment in bespoke modeling efforts translating sparse labels
into maps. Here we introduce AlphaEarth Foundations, an embedding field model
yielding a highly general, geospatial representation that assimilates spatial,
temporal, and measurement contexts across multiple sources, enabling accurate
and efficient production of maps and monitoring systems from local to global
scales. The embeddings generated by AlphaEarth Foundations are the only to
consistently outperform all previous featurization approaches tested on a
diverse set of mapping evaluations without re-training. We will release a
dataset of global, annual, analysis-ready embedding field layers from 2017
through 2024.

</details>


### [10] [LAMA-Net: A Convergent Network Architecture for Dual-Domain Reconstruction](https://arxiv.org/abs/2507.22316)
*Chi Ding,Qingchao Zhang,Ge Wang,Xiaojing Ye,Yunmei Chen*

Main category: cs.CV

TL;DR: 提出了一种名为LAMA的可学习交替最小化算法，用于图像重建。该算法具有收敛性证明，并催生了LAMA-Net和iLAMA-Net，后者在稀疏视角CT重建任务中表现出优越的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 提供LAMA的完整且严格的收敛证明，并证明其所有累积点必须是Clarke平稳点。

Method: 提出了一种可学习的变分模型，该模型学习特征并利用图像和测量域的互补信息进行图像重建。具体而言，我们引入了一种学习型交替最小化算法（LAMA），该算法通过在近邻交替框架中结合残差学习架构来解决双块非凸和非光滑优化问题。此外，还提供了LAMA的完整且严格的收敛证明，并证明了LAMA的特定子序列的所有累积点必须是该问题的Clarke平稳点。

Result: LAMA-Net/iLAMA-Net的收敛性保证了其出色的稳定性和鲁棒性。通过集成iLAMA-Net，其性能得到进一步提升。在稀疏视角计算机断层扫描的基准数据集上的实验表明，LAMA-Net/iLAMA-Net与几种最先进的方法相比具有优越的性能。

Conclusion: LAMA-Net/iLAMA-Net的收敛性提供了出色的稳定性和鲁棒性，并且通过整合精心设计的生成合适初始值的网络（iLAMA-Net）可以进一步提高其性能。

Abstract: We propose a learnable variational model that learns the features and
leverages complementary information from both image and measurement domains for
image reconstruction. In particular, we introduce a learned alternating
minimization algorithm (LAMA) from our prior work, which tackles two-block
nonconvex and nonsmooth optimization problems by incorporating a residual
learning architecture in a proximal alternating framework. In this work, our
goal is to provide a complete and rigorous convergence proof of LAMA and show
that all accumulation points of a specified subsequence of LAMA must be Clarke
stationary points of the problem. LAMA directly yields a highly interpretable
neural network architecture called LAMA-Net. Notably, in addition to the
results shown in our prior work, we demonstrate that the convergence property
of LAMA yields outstanding stability and robustness of LAMA-Net in this work.
We also show that the performance of LAMA-Net can be further improved by
integrating a properly designed network that generates suitable initials, which
we call iLAMA-Net. To evaluate LAMA-Net/iLAMA-Net, we conduct several
experiments and compare them with several state-of-the-art methods on popular
benchmark datasets for Sparse-View Computed Tomography.

</details>


### [11] [Learning from Heterogeneous Structural MRI via Collaborative Domain Adaptation for Late-Life Depression Assessment](https://arxiv.org/abs/2507.22321)
*Yuzhen Gao,Qianqian Wang,Yongheng Sun,Cui Wang,Yongquan Liang,Mingxia Liu*

Main category: cs.CV

TL;DR: 该研究提出了一种名为协同域适应（CDA）的新框架，结合了ViT和CNN，通过三阶段方法（监督训练、自监督适应、协同训练）有效解决了抑郁症检测中样本量有限和域异质性问题，并在多站点MRI数据上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于学习的抑郁症检测方法在样本量有限的情况下难以保证模型的可靠训练和泛化能力。此外，虽然可以引入辅助数据集，但成像协议、扫描硬件和人群统计等方面的域异质性常常会削弱跨域迁移能力。因此，本研究旨在解决这些挑战，提出一种能够有效处理域异质性并提高模型性能的抑郁症检测框架。

Method: 本研究提出了一种协同域适应（CDA）框架，用于抑郁症检测。该框架结合了Vision Transformer（ViT）提取全局解剖背景和卷积神经网络（CNN）提取局部结构特征。CDA框架包括三个阶段：1. 在标记的源数据上进行监督训练；2. 通过最小化两个分支的分类器输出来实现自监督目标特征适应；3. 在未标记的目标数据上进行协同训练，通过伪标签和增强的目标域MRI，在强弱增强下强制执行预测一致性，以提高模型鲁棒性和泛化能力。

Result: 在多站点T1加权MRI数据上进行的广泛实验表明，CDA框架在抑郁症检测任务上始终优于最先进的无监督域适应方法。

Conclusion: 提出的协同域适应（CDA）框架通过结合ViT和CNN的优势，并在多站点T1加权MRI数据集上进行的大量实验证明，其在抑郁症检测任务上优于现有的无监督域适应方法，有望解决现有方法在样本量有限和域异构性问题上的局限性。

Abstract: Accurate identification of late-life depression (LLD) using structural brain
MRI is essential for monitoring disease progression and facilitating timely
intervention. However, existing learning-based approaches for LLD detection are
often constrained by limited sample sizes (e.g., tens), which poses significant
challenges for reliable model training and generalization. Although
incorporating auxiliary datasets can expand the training set, substantial
domain heterogeneity, such as differences in imaging protocols, scanner
hardware, and population demographics, often undermines cross-domain
transferability. To address this issue, we propose a Collaborative Domain
Adaptation (CDA) framework for LLD detection using T1-weighted MRIs. The CDA
leverages a Vision Transformer (ViT) to capture global anatomical context and a
Convolutional Neural Network (CNN) to extract local structural features, with
each branch comprising an encoder and a classifier. The CDA framework consists
of three stages: (a) supervised training on labeled source data, (b)
self-supervised target feature adaptation and (c) collaborative training on
unlabeled target data. We first train ViT and CNN on source data, followed by
self-supervised target feature adaptation by minimizing the discrepancy between
classifier outputs from two branches to make the categorical boundary clearer.
The collaborative training stage employs pseudo-labeled and augmented
target-domain MRIs, enforcing prediction consistency under strong and weak
augmentation to enhance domain robustness and generalization. Extensive
experiments conducted on multi-site T1-weighted MRI data demonstrate that the
CDA consistently outperforms state-of-the-art unsupervised domain adaptation
methods.

</details>


### [12] [UFV-Splatter: Pose-Free Feed-Forward 3D Gaussian Splatting Adapted to Unfavorable Views](https://arxiv.org/abs/2507.22342)
*Yuki Fujimura,Takahiro Kushida,Kazuya Kitano,Takuya Funatomi,Yasuhiro Mukaigawa*

Main category: cs.CV

TL;DR: 本研究提出了一种新的3D高斯泼溅（3DGS）框架，可以通过低秩适应（LoRA）和高斯适配器来处理不利的相机视图，并在合成和真实数据集上进行了有效验证。


<details>
  <summary>Details</summary>
Motivation: 现有的前馈3DGS模型通常在有利的视图下进行训练，这限制了它们在具有不同和未知相机姿态的真实世界场景中的应用。为了克服这一限制，本研究旨在开发一种能够处理不利输入视图的3DGS框架。

Method: 本文提出了一种无姿态、前馈的3D高斯泼溅（3DGS）框架，并引入了一个新颖的适应性框架，该框架允许预训练的无姿态前馈3DGS模型处理不利视图。该方法利用从有利图像中学到的先验知识，将近期重塑的图像输入到具有低秩适应（LoRA）层的预训练模型中。此外，还提出了一种高斯适配器模块来增强高斯几何一致性，并提出了一种高斯对齐方法来渲染准确的目标视图进行训练。最后，采用一种新的训练策略，仅使用有利图像的现成数据集。

Result: 实验结果表明，该方法在处理不利输入视图方面是有效的，并在Google Scanned Objects（合成图像）和OmniObject3D（真实图像）数据集上得到了验证。

Conclusion: 本论文提出了一种新颖的适应性框架，使预训练的无姿态前馈3DGS模型能够处理不利视图。通过将近期重塑的图像输入到具有低秩适应（LoRA）层的预训练模型中，并结合高斯适配器模块和高斯对齐方法，我们能够有效地处理具有挑战性的输入视图，并在合成和真实数据集上验证了该方法的有效性。

Abstract: This paper presents a pose-free, feed-forward 3D Gaussian Splatting (3DGS)
framework designed to handle unfavorable input views. A common rendering setup
for training feed-forward approaches places a 3D object at the world origin and
renders it from cameras pointed toward the origin -- i.e., from favorable
views, limiting the applicability of these models to real-world scenarios
involving varying and unknown camera poses. To overcome this limitation, we
introduce a novel adaptation framework that enables pretrained pose-free
feed-forward 3DGS models to handle unfavorable views. We leverage priors
learned from favorable images by feeding recentered images into a pretrained
model augmented with low-rank adaptation (LoRA) layers. We further propose a
Gaussian adapter module to enhance the geometric consistency of the Gaussians
derived from the recentered inputs, along with a Gaussian alignment method to
render accurate target views for training. Additionally, we introduce a new
training strategy that utilizes an off-the-shelf dataset composed solely of
favorable images. Experimental results on both synthetic images from the Google
Scanned Objects dataset and real images from the OmniObject3D dataset validate
the effectiveness of our method in handling unfavorable input views.

</details>


### [13] [DeltaVLM: Interactive Remote Sensing Image Change Analysis via Instruction-guided Difference Perception](https://arxiv.org/abs/2507.22346)
*Pei Deng,Wenqian Zhou,Hanlin Wu*

Main category: cs.CV

TL;DR: 本研究提出了 DeltaVLM，一种用于遥感图像变化分析的新范例和模型。它通过结合变化检测和视觉问答，实现了多轮、指令引导的交互式分析。使用大规模数据集 ChangeChat-105k 进行训练，DeltaVLM 在各种变化分析任务上表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常仅提供单次变化掩模或静态字幕，这限制了它们在交互式、查询驱动的分析中的应用。因此，需要一种能够支持多轮、指令引导的探索方法来精确解释多时相卫星图像中的土地覆盖变化。

Method: 本研究引入了一种称为遥感图像变化分析 (RSICA) 的新范例，它结合了变化检测和视觉问答的优点，能够对双时相遥感图像中的变化进行多轮、指令引导的探索。为了支持这项任务，研究人员构建了一个名为 ChangeChat-105k 的大规模指令遵循数据集，该数据集涵盖了六种交互类型：变化字幕、分类、量化、定位、开放式问答和多轮对话。基于此数据集，研究人员提出了 DeltaVLM，一种专门用于交互式 RSICA 的端到端架构。DeltaVLM 具有三个主要创新：(1) 一个经过微调的双时相视觉编码器，用于捕捉时相差异；(2) 一个具有跨语义关系度量 (CSRM) 机制的视觉差异感知模块，用于解释变化；(3) 一个指令引导的 Q-former，用于从视觉变化中有效地提取与查询相关的信息，并将其与文本指令对齐。研究人员使用冻结的大型语言模型在 ChangeChat-105k 上训练 DeltaVLM，仅调整视觉和对齐模块以优化效率。

Result: DeltaVLM 在单轮字幕和多轮交互式变化分析方面均达到了最先进的性能，并且在实验和消融研究中证明了其优越性。

Conclusion: DeltaVLM 在单轮字幕和多轮交互式变化分析方面均取得了最先进的性能，其表现优于现有的多模态大语言模型和遥感视觉语言模型。

Abstract: Accurate interpretation of land-cover changes in multi-temporal satellite
imagery is critical for real-world scenarios. However, existing methods
typically provide only one-shot change masks or static captions, limiting their
ability to support interactive, query-driven analysis. In this work, we
introduce remote sensing image change analysis (RSICA) as a new paradigm that
combines the strengths of change detection and visual question answering to
enable multi-turn, instruction-guided exploration of changes in bi-temporal
remote sensing images. To support this task, we construct ChangeChat-105k, a
large-scale instruction-following dataset, generated through a hybrid
rule-based and GPT-assisted process, covering six interaction types: change
captioning, classification, quantification, localization, open-ended question
answering, and multi-turn dialogues. Building on this dataset, we propose
DeltaVLM, an end-to-end architecture tailored for interactive RSICA. DeltaVLM
features three innovations: (1) a fine-tuned bi-temporal vision encoder to
capture temporal differences; (2) a visual difference perception module with a
cross-semantic relation measuring (CSRM) mechanism to interpret changes; and
(3) an instruction-guided Q-former to effectively extract query-relevant
difference information from visual changes, aligning them with textual
instructions. We train DeltaVLM on ChangeChat-105k using a frozen large
language model, adapting only the vision and alignment modules to optimize
efficiency. Extensive experiments and ablation studies demonstrate that
DeltaVLM achieves state-of-the-art performance on both single-turn captioning
and multi-turn interactive change analysis, outperforming existing multimodal
large language models and remote sensing vision-language models. Code, dataset
and pre-trained weights are available at https://github.com/hanlinwu/DeltaVLM.

</details>


### [14] [FaceGCD: Generalized Face Discovery via Dynamic Prefix Generation](https://arxiv.org/abs/2507.22353)
*Yunseok Oh,Dong-Wan Choi*

Main category: cs.CV

TL;DR: 本研究提出了FaceGCD，一种新颖的开放世界人脸识别任务（GFD），它能识别已知和未知身份的人脸。FaceGCD通过动态生成特征提取器来解决细粒度人脸ID的挑战，并在实验中取得了优于现有方法的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 为了实现能够识别熟悉和不熟悉的面孔，并向通用人工智能（AGI）迈进，本研究引入了广义人脸发现（GFD）任务，该任务整合了传统人脸识别和广义类别发现（GCD）。

Method: 提出了一种名为FaceGCD的新方法，该方法通过HyperNetwork动态生成实例特定的特征提取器（通过轻量级、逐层的前缀），以适应细粒度的人脸ID。

Result: FaceGCD在GFD任务上取得了最先进的成果，显著优于现有的GCD方法和ArcFace。

Conclusion: FaceGCD方法在GFD任务上显著优于现有的GCD方法和强基线模型ArcFace，在开放世界人脸识别方面取得了新的进展。

Abstract: Recognizing and differentiating among both familiar and unfamiliar faces is a
critical capability for face recognition systems and a key step toward
artificial general intelligence (AGI). Motivated by this ability, this paper
introduces generalized face discovery (GFD), a novel open-world face
recognition task that unifies traditional face identification with generalized
category discovery (GCD). GFD requires recognizing both labeled and unlabeled
known identities (IDs) while simultaneously discovering new, previously unseen
IDs. Unlike typical GCD settings, GFD poses unique challenges due to the high
cardinality and fine-grained nature of face IDs, rendering existing GCD
approaches ineffective. To tackle this problem, we propose FaceGCD, a method
that dynamically constructs instance-specific feature extractors using
lightweight, layer-wise prefixes. These prefixes are generated on the fly by a
HyperNetwork, which adaptively outputs a set of prefix generators conditioned
on each input image. This dynamic design enables FaceGCD to capture subtle
identity-specific cues without relying on high-capacity static models.
Extensive experiments demonstrate that FaceGCD significantly outperforms
existing GCD methods and a strong face recognition baseline, ArcFace, achieving
state-of-the-art results on the GFD task and advancing toward open-world face
recognition.

</details>


### [15] [Recognizing Actions from Robotic View for Natural Human-Robot Interaction](https://arxiv.org/abs/2507.22522)
*Ziyi Wang,Peiming Li,Hong Liu,Zhichao Deng,Can Wang,Jun Liu,Junsong Yuan,Mengyuan Liu*

Main category: cs.CV

TL;DR: 本研究提出了名为ACTIVE的大规模数据集和名为ACTIVE-PC的动作识别方法，以解决自然人机交互中机器人远距离感知人类动作的挑战。 ACTIVE数据集包含30类动作，80名参与者，46,868个视频实例，涵盖RGB和点云模态，并模拟了真实的机器人感知场景。ACTIVE-PC方法通过多级邻域采样、分层识别器、弹性椭圆查询和运动干扰解耦等技术，实现了远距离动作识别的准确性。实验证明了该方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有的动作识别基准在自然人机交互（N-HRI）领域存在数据量有限、模态单一、任务类别少、主体和环境多样性不足等问题，无法满足机器人自身运动或静止时，在不同距离和状态下识别人类动作的需求，因此需要新的数据集和方法来应对这些挑战。

Method: 提出了一种名为ACTIVE-PC的新方法，该方法利用了多级邻域采样、分层识别器、弹性椭圆查询以及运动干扰与人类动作的精确解耦技术，以实现远距离人类动作的精确感知。

Result: 实验结果表明，所提出的ACTIVE-PC方法在感知远距离人类动作方面是有效的。

Conclusion: 该研究提出了ACTIVE数据集和ACTIVE-PC方法，旨在解决移动服务机器人领域中，在远距离、多变状态下的人类动作识别问题，实验结果证明了该方法的有效性。

Abstract: Natural Human-Robot Interaction (N-HRI) requires robots to recognize human
actions at varying distances and states, regardless of whether the robot itself
is in motion or stationary. This setup is more flexible and practical than
conventional human action recognition tasks. However, existing benchmarks
designed for traditional action recognition fail to address the unique
complexities in N-HRI due to limited data, modalities, task categories, and
diversity of subjects and environments. To address these challenges, we
introduce ACTIVE (Action from Robotic View), a large-scale dataset tailored
specifically for perception-centric robotic views prevalent in mobile service
robots. ACTIVE comprises 30 composite action categories, 80 participants, and
46,868 annotated video instances, covering both RGB and point cloud modalities.
Participants performed various human actions in diverse environments at
distances ranging from 3m to 50m, while the camera platform was also mobile,
simulating real-world scenarios of robot perception with varying camera heights
due to uneven ground. This comprehensive and challenging benchmark aims to
advance action and attribute recognition research in N-HRI. Furthermore, we
propose ACTIVE-PC, a method that accurately perceives human actions at long
distances using Multilevel Neighborhood Sampling, Layered Recognizers, Elastic
Ellipse Query, and precise decoupling of kinematic interference from human
actions. Experimental results demonstrate the effectiveness of ACTIVE-PC. Our
code is available at:
https://github.com/wangzy01/ACTIVE-Action-from-Robotic-View.

</details>


### [16] [GVD: Guiding Video Diffusion Model for Scalable Video Distillation](https://arxiv.org/abs/2507.22360)
*Kunyang Li,Jeffrey A Chan Santiago,Sarinda Dhanesh Samarasinghe,Gaowen Liu,Mubarak Shah*

Main category: cs.CV

TL;DR: GVD是一种新的扩散视频蒸馏方法，通过联合蒸馏时空特征，以更少的帧数实现了与原始数据集相当的性能，并在生成更高质量视频方面表现出色。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型视频数据集带来的更大计算和存储需求，视频数据集蒸馏旨在捕获显著缩小的数据集中的时空信息，从而在蒸馏数据上训练的性能可与在全部数据上训练的性能相媲美。

Method: 提出了一种名为GVD（Guiding Video Diffusion）的基于扩散的视频蒸馏方法，该方法联合蒸馏空间和时间特征，以捕捉关键的运动信息。

Result: GVD在MiniUCF和HMDB51数据集上，跨5、10和20个类别实例（IPC）的表现显著优于以往最先进的方法。具体来说，在MiniUCF数据集中，GVD仅使用原始数据总帧数的1.98%，即可达到原始数据集性能的78.29%；在HMDB51数据集中，仅使用原始数据总帧数的3.30%，即可达到原始数据集性能的73.83%。实验结果表明，GVD不仅达到了最先进的性能，而且在计算成本没有显著增加的情况下，能够生成更高分辨率和更高IPC的视频。

Conclusion: GVD在MiniUCF和HMDB51数据集上显著优于现有方法，并且能够以较低的计算成本生成更高分辨率和更高IPC的视频。

Abstract: To address the larger computation and storage requirements associated with
large video datasets, video dataset distillation aims to capture spatial and
temporal information in a significantly smaller dataset, such that training on
the distilled data has comparable performance to training on all of the data.
We propose GVD: Guiding Video Diffusion, the first diffusion-based video
distillation method. GVD jointly distills spatial and temporal features,
ensuring high-fidelity video generation across diverse actions while capturing
essential motion information. Our method's diverse yet representative
distillations significantly outperform previous state-of-the-art approaches on
the MiniUCF and HMDB51 datasets across 5, 10, and 20 Instances Per Class (IPC).
Specifically, our method achieves 78.29 percent of the original dataset's
performance using only 1.98 percent of the total number of frames in MiniUCF.
Additionally, it reaches 73.83 percent of the performance with just 3.30
percent of the frames in HMDB51. Experimental results across benchmark video
datasets demonstrate that GVD not only achieves state-of-the-art performance
but can also generate higher resolution videos and higher IPC without
significantly increasing computational cost.

</details>


### [17] [Viser: Imperative, Web-based 3D Visualization in Python](https://arxiv.org/abs/2507.22885)
*Brent Yi,Chung Min Kim,Justin Kerr,Gina Wu,Rebecca Feng,Anthony Zhang,Jonas Kulhanek,Hongsuk Choi,Yi Ma,Matthew Tancik,Angjoo Kanazawa*

Main category: cs.CV

TL;DR: Viser是一个Python库，用于计算机视觉和机器人学的3D可视化。


<details>
  <summary>Details</summary>
Motivation: Viser旨在为Python带来易于使用且可扩展的3D可视化功能。

Method: Viser采用命令式API和基于Web的查看器，以提高与现代编程模式和工作流程的兼容性。

Result: Viser提供了全面的3D场景和2D GUI基元，可独立使用或组合使用以构建专用界面。

Conclusion: Viser是一个易于使用且可扩展的Python 3D可视化库，适用于计算机视觉和机器人学。

Abstract: We present Viser, a 3D visualization library for computer vision and
robotics. Viser aims to bring easy and extensible 3D visualization to Python:
we provide a comprehensive set of 3D scene and 2D GUI primitives, which can be
used independently with minimal setup or composed to build specialized
interfaces. This technical report describes Viser's features, interface, and
implementation. Key design choices include an imperative-style API and a
web-based viewer, which improve compatibility with modern programming patterns
and workflows.

</details>


### [18] [Object Recognition Datasets and Challenges: A Review](https://arxiv.org/abs/2507.22361)
*Aria Salari,Abtin Djavadifar,Xiangrui Liu,Homayoun Najjaran*

Main category: cs.CV

TL;DR: 该调查分析了160多个物体识别数据集，并讨论了相关的基准、竞赛和评估指标。


<details>
  <summary>Details</summary>
Motivation: 深入了解常用公共数据集的特性对于数据驱动和机器学习研究人员至关重要，因为数据集的大小和质量对于新兴深度网络技术的有效性至关重要，并且数据集为竞赛提供了公平的基准测试手段。

Method: 通过统计和描述对160多个数据集进行细致分析，并概述了重要的物体识别基准和竞赛，以及广泛采用的评估指标。

Result: 对物体识别领域160多个数据集进行了详细分析，并概述了重要的物体识别基准、竞赛和评估指标。

Conclusion: 该调查提供了对物体识别领域常用公共数据集的详细分析，并审查了160多个数据集。它还概述了重要的物体识别基准和竞赛，以及计算机视觉社区中广泛采用的评估指标。

Abstract: Object recognition is among the fundamental tasks in the computer vision
applications, paving the path for all other image understanding operations. In
every stage of progress in object recognition research, efforts have been made
to collect and annotate new datasets to match the capacity of the
state-of-the-art algorithms. In recent years, the importance of the size and
quality of datasets has been intensified as the utility of the emerging deep
network techniques heavily relies on training data. Furthermore, datasets lay a
fair benchmarking means for competitions and have proved instrumental to the
advancements of object recognition research by providing quantifiable
benchmarks for the developed models. Taking a closer look at the
characteristics of commonly-used public datasets seems to be an important first
step for data-driven and machine learning researchers. In this survey, we
provide a detailed analysis of datasets in the highly investigated object
recognition areas. More than 160 datasets have been scrutinized through
statistics and descriptions. Additionally, we present an overview of the
prominent object recognition benchmarks and competitions, along with a
description of the metrics widely adopted for evaluation purposes in the
computer vision community. All introduced datasets and challenges can be found
online at github.com/AbtinDjavadifar/ORDC.

</details>


### [19] [Exploring the Application of Visual Question Answering (VQA) for Classroom Activity Monitoring](https://arxiv.org/abs/2507.22369)
*Sinh Trong Vu,Hieu Trung Pham,Dung Manh Nguyen,Hieu Minh Hoang,Nhu Hoang Le,Thu Ha Pham,Tai Tan Mai*

Main category: cs.CV

TL;DR: 本研究评估了LLaMA2、LLaMA3、QWEN3和NVILA在课堂行为分析中的应用。通过使用新创建的BAV-Classroom-VQA数据集，研究发现这些模型在回答关于课堂行为的问题方面表现出有前景的性能。


<details>
  <summary>Details</summary>
Motivation: 课堂行为监测对于教育研究至关重要，对学生的参与度和学习成果有显著影响。本研究旨在探索先进的视觉问答（VQA）模型在自动分析课堂互动方面的应用潜力。

Method: 本研究介绍了BAV-Classroom-VQA数据集，该数据集来源于越南银行学院的真实课堂视频录制。研究中包含了数据收集和标注的方法，并对选定的VQA模型在数据集上的性能进行了基准测试。

Result: 实验结果表明，所有四种模型（LLaMA2、LLaMA3、QWEN3和NVILA）在回答与行为相关的视觉问题方面均取得了令人满意的性能。

Conclusion: 该研究表明，LLaMA2、LLaMA3、QWEN3和NVILA等先进的开源VQA模型在课堂行为分析方面具有巨大潜力，能够对课堂互动进行自动分析，并为未来的课堂分析和干预系统奠定基础。

Abstract: Classroom behavior monitoring is a critical aspect of educational research,
with significant implications for student engagement and learning outcomes.
Recent advancements in Visual Question Answering (VQA) models offer promising
tools for automatically analyzing complex classroom interactions from video
recordings. In this paper, we investigate the applicability of several
state-of-the-art open-source VQA models, including LLaMA2, LLaMA3, QWEN3, and
NVILA, in the context of classroom behavior analysis. To facilitate rigorous
evaluation, we introduce our BAV-Classroom-VQA dataset derived from real-world
classroom video recordings at the Banking Academy of Vietnam. We present the
methodology for data collection, annotation, and benchmark the performance of
the selected VQA models on this dataset. Our initial experimental results
demonstrate that all four models achieve promising performance levels in
answering behavior-related visual questions, showcasing their potential in
future classroom analytics and intervention systems.

</details>


### [20] [Gems: Group Emotion Profiling Through Multimodal Situational Understanding](https://arxiv.org/abs/2507.22393)
*Anubhav Kataria,Surbhi Madan,Shreya Ghosh,Tom Gedeon,Abhinav Dhall*

Main category: cs.CV

TL;DR: GEMS是一个新框架，通过多模态swin-transformer和S3Attention处理场景、成员和上下文信息，以预测个人、群体和事件层面的细粒度情感，并在VGAF-GEMS数据集上进行了验证。


<details>
  <summary>Details</summary>
Motivation: 理解个人、群体和事件层面的情感以及情境信息对于分析多人社交情境至关重要。

Method: 提出了一种名为GEMS的框架，该框架利用多模态swin-transformer和S3Attention基于架构，处理输入场景、组成员和上下文信息以生成联合预测。扩展并提出了VGAF-GEMS，在现有的VGAF数据集组级别注释之上提供更细粒度和整体的分析。

Result: GEMS框架成功地实现了对基本离散和连续情感（包括效价和唤醒度）以及个人、群体和事件层面的感知情感的预测。

Conclusion: GEMS框架在VGAF-GEMS基准上展示了有效性，并有望推动未来的研究。

Abstract: Understanding individual, group and event level emotions along with
contextual information is crucial for analyzing a multi-person social
situation. To achieve this, we frame emotion comprehension as the task of
predicting fine-grained individual emotion to coarse grained group and event
level emotion. We introduce GEMS that leverages a multimodal swin-transformer
and S3Attention based architecture, which processes an input scene, group
members, and context information to generate joint predictions. Existing
multi-person emotion related benchmarks mainly focus on atomic interactions
primarily based on emotion perception over time and group level. To this end,
we extend and propose VGAF-GEMS to provide more fine grained and holistic
analysis on top of existing group level annotation of VGAF dataset. GEMS aims
to predict basic discrete and continuous emotions (including valence and
arousal) as well as individual, group and event level perceived emotions. Our
benchmarking effort links individual, group and situational emotional responses
holistically. The quantitative and qualitative comparisons with adapted
state-of-the-art models demonstrate the effectiveness of GEMS framework on
VGAF-GEMS benchmarking. We believe that it will pave the way of further
research. The code and data is available at:
https://github.com/katariaak579/GEMS

</details>


### [21] [On the Reliability of Vision-Language Models Under Adversarial Frequency-Domain Perturbations](https://arxiv.org/abs/2507.22398)
*Jordan Vice,Naveed Akhtar,Yansong Gao,Richard Hartley,Ajmal Mian*

Main category: cs.CV

TL;DR: 视觉-语言模型（VLM）在频域中容易受到微小扰动的影响，导致在图像描述和真实性检测任务中表现不佳。即使是人眼无法察觉的变换也会产生显著影响，这表明VLM的判断并不总是基于语义内容。


<details>
  <summary>Details</summary>
Motivation: 旨在揭示视觉-语言模型（VLM）在面对细微、结构化的频域扰动时存在的关键漏洞，并探讨这些变换如何影响其在图像真实性检测和自动图像描述任务中的表现。

Method: 本文设计了针对频域的图像变换方法，系统性地调整了VLM对经过频域扰动的真实和合成图像的输出。该方法被应用于五种先进的VLM（包括不同参数的Qwen2/2.5和BLIP模型），并在一共十个真实和生成图像数据集上进行了实验。

Result: 实验证明，VLM的判断对频域线索非常敏感，其输出可能与语义内容不完全一致。即使是人眼无法察觉的空间频域变换，也暴露了VLM在自动图像描述和真实性检测任务中的脆弱性。这些发现是在现实的黑盒约束下进行的，对VLM的可靠性提出了挑战。

Conclusion: 现有的视觉-语言模型（VLM）在处理经过微小、结构化频域扰动的图像时存在严重漏洞，这影响了它们在图像真实性检测和图像描述等任务上的可靠性。即使是人眼无法察觉的变换，也会显著改变VLM的输出，表明其判断可能过于依赖特定的频域线索，而不能完全与图像的语义内容保持一致。因此，有必要开发更鲁棒的多模态感知系统。

Abstract: Vision-Language Models (VLMs) are increasingly used as perceptual modules for
visual content reasoning, including through captioning and DeepFake detection.
In this work, we expose a critical vulnerability of VLMs when exposed to
subtle, structured perturbations in the frequency domain. Specifically, we
highlight how these feature transformations undermine authenticity/DeepFake
detection and automated image captioning tasks. We design targeted image
transformations, operating in the frequency domain to systematically adjust VLM
outputs when exposed to frequency-perturbed real and synthetic images. We
demonstrate that the perturbation injection method generalizes across five
state-of-the-art VLMs which includes different-parameter Qwen2/2.5 and BLIP
models. Experimenting across ten real and generated image datasets reveals that
VLM judgments are sensitive to frequency-based cues and may not wholly align
with semantic content. Crucially, we show that visually-imperceptible spatial
frequency transformations expose the fragility of VLMs deployed for automated
image captioning and authenticity detection tasks. Our findings under
realistic, black-box constraints challenge the reliability of VLMs,
underscoring the need for robust multimodal perception systems.

</details>


### [22] [MINR: Implicit Neural Representations with Masked Image Modelling](https://arxiv.org/abs/2507.22404)
*Sua Lee,Joonhun Lee,Myungjoo Kang*

Main category: cs.CV

TL;DR: MINR通过结合隐式神经表示和掩码图像建模，提供了一种更鲁棒、更高效的自监督学习方法，尤其擅长处理分布外数据。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有掩码自动编码器（MAE）等自监督学习方法在分布外数据上表现不佳以及对掩码策略的依赖性问题，提出MINR框架。

Method: MINR框架结合了隐式神经表示和掩码图像建模，学习一个连续函数来表示图像，从而实现更鲁棒和可泛化的重建，不受掩码策略的影响。

Result: MINR在同类和分布外场景下均优于MAE，同时降低了模型复杂度，并证明了其在各种自监督学习应用中的通用性。

Conclusion: MINR是一个强大的框架，结合了隐式神经表示和掩码图像建模，在各种自监督学习应用中表现出色，尤其是在处理分布外数据和降低模型复杂性方面，是现有框架的可靠且高效的替代方案。

Abstract: Self-supervised learning methods like masked autoencoders (MAE) have shown
significant promise in learning robust feature representations, particularly in
image reconstruction-based pretraining task. However, their performance is
often strongly dependent on the masking strategies used during training and can
degrade when applied to out-of-distribution data. To address these limitations,
we introduce the masked implicit neural representations (MINR) framework that
synergizes implicit neural representations with masked image modeling. MINR
learns a continuous function to represent images, enabling more robust and
generalizable reconstructions irrespective of masking strategies. Our
experiments demonstrate that MINR not only outperforms MAE in in-domain
scenarios but also in out-of-distribution settings, while reducing model
complexity. The versatility of MINR extends to various self-supervised learning
applications, confirming its utility as a robust and efficient alternative to
existing frameworks.

</details>


### [23] [Moiré Zero: An Efficient and High-Performance Neural Architecture for Moiré Removal](https://arxiv.org/abs/2507.22407)
*Seungryong Lee,Woojeong Baek,Younghyun Kim,Eunwoo Kim,Haru Moon,Donggon Yoo,Eunbyung Park*

Main category: cs.CV

TL;DR: MZNet 是一种创新的 U 型网络，通过结合多尺度注意力和大卷积核来有效去除摩尔纹，并在各种分辨率的数据集上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的基于卷积神经网络的去摩尔纹方法在处理多样化的摩尔纹（包括不同尺度、方向和颜色偏移）时存在局限性，主要是由于卷积神经网络有限的感受野。

Method: 提出了一种名为 MZNet 的 U 型网络，该网络集成了多尺度双注意力块 (MSDAB)、多形状大卷积核块 (MSLKB) 和基于特征融合的跳跃连接，以有效去除摩尔纹。

Result: MZNet 在高分辨率数据集上实现了最先进的性能，在低分辨率数据集上取得了有竞争力的结果，同时保持了较低的计算成本。

Conclusion: MZNet 是一种有效的去摩尔纹解决方案，在真实世界的应用中具有良好的前景。

Abstract: Moir\'e patterns, caused by frequency aliasing between fine repetitive
structures and a camera sensor's sampling process, have been a significant
obstacle in various real-world applications, such as consumer photography and
industrial defect inspection. With the advancements in deep learning
algorithms, numerous studies-predominantly based on convolutional neural
networks-have suggested various solutions to address this issue. Despite these
efforts, existing approaches still struggle to effectively eliminate artifacts
due to the diverse scales, orientations, and color shifts of moir\'e patterns,
primarily because the constrained receptive field of CNN-based architectures
limits their ability to capture the complex characteristics of moir\'e
patterns. In this paper, we propose MZNet, a U-shaped network designed to bring
images closer to a 'Moire-Zero' state by effectively removing moir\'e patterns.
It integrates three specialized components: Multi-Scale Dual Attention Block
(MSDAB) for extracting and refining multi-scale features, Multi-Shape Large
Kernel Convolution Block (MSLKB) for capturing diverse moir\'e structures, and
Feature Fusion-Based Skip Connection for enhancing information flow. Together,
these components enhance local texture restoration and large-scale artifact
suppression. Experiments on benchmark datasets demonstrate that MZNet achieves
state-of-the-art performance on high-resolution datasets and delivers
competitive results on lower-resolution dataset, while maintaining a low
computational cost, suggesting that it is an efficient and practical solution
for real-world applications. Project page:
https://sngryonglee.github.io/MoireZero

</details>


### [24] [UAVScenes: A Multi-Modal Dataset for UAVs](https://arxiv.org/abs/2507.22412)
*Sijie Wang,Siqi Li,Yawei Zhang,Shangshu Yu,Shenghai Yuan,Rui She,Quanjiang Guo,JinXuan Zheng,Ong Kang Howe,Leonrich Chandra,Shrivarshann Srijeyan,Aditya Sivadas,Toshan Aggarwal,Heyuan Liu,Hongming Zhang,Chujie Chen,Junyu Jiang,Lihua Xie,Wee Peng Tay*

Main category: cs.CV

TL;DR: 本文提出了UAVScenes数据集，解决了现有UAV数据集在帧级多模态感知方面的不足，可用于多种UAV感知任务。


<details>
  <summary>Details</summary>
Motivation: 现有UAV多模态数据集主要偏向定位和3D重建任务，或仅支持地图级语义分割，缺乏帧级标注，无法支持高级场景理解任务。

Method: 本文作者在MARS-LVIG数据集的基础上，增加了手动标注的帧级图像和激光雷达点云的语义标注，以及精确的6DoF位姿，构建了UAVScenes数据集。

Result: UAVScenes数据集的构建，为UAV的多模态感知和高级场景理解任务提供了支持，可用于多种感知任务的基准测试。

Conclusion: UAVScenes是一个大型多模态数据集，通过提供帧级图像和激光雷达点云的语义标注以及精确的6DoF位姿，解决了现有数据集在帧级多模态感知任务上的不足，可用于分割、深度估计、6DoF定位、场景识别和新视角合成等多种UAV感知任务。

Abstract: Multi-modal perception is essential for unmanned aerial vehicle (UAV)
operations, as it enables a comprehensive understanding of the UAVs'
surrounding environment. However, most existing multi-modal UAV datasets are
primarily biased toward localization and 3D reconstruction tasks, or only
support map-level semantic segmentation due to the lack of frame-wise
annotations for both camera images and LiDAR point clouds. This limitation
prevents them from being used for high-level scene understanding tasks. To
address this gap and advance multi-modal UAV perception, we introduce
UAVScenes, a large-scale dataset designed to benchmark various tasks across
both 2D and 3D modalities. Our benchmark dataset is built upon the
well-calibrated multi-modal UAV dataset MARS-LVIG, originally developed only
for simultaneous localization and mapping (SLAM). We enhance this dataset by
providing manually labeled semantic annotations for both frame-wise images and
LiDAR point clouds, along with accurate 6-degree-of-freedom (6-DoF) poses.
These additions enable a wide range of UAV perception tasks, including
segmentation, depth estimation, 6-DoF localization, place recognition, and
novel view synthesis (NVS). Our dataset is available at
https://github.com/sijieaaa/UAVScenes

</details>


### [25] [Aleatoric Uncertainty Medical Image Segmentation Estimation via Flow Matching](https://arxiv.org/abs/2507.22418)
*Phi Van Nguyen,Ngoc Huynh Trinh,Duy Minh Lam Nguyen,Phu Loc Nguyen,Quoc Long Tran*

Main category: cs.CV

TL;DR: 提出一种基于条件匹配流（conditional flow matching）的新方法，用于医学图像分割，能够准确量化不确定性，提供比传统方法和基于扩散的方法更可靠的分割结果。


<details>
  <summary>Details</summary>
Motivation: 量化医学图像分割中的不确定性对于反映专家标注者之间的自然变异性至关重要。传统方法在表达生成模型的表达能力方面存在局限性，而基于扩散的方法在近似数据分布方面虽然表现出色，但其固有的随机采样过程和无法精确建模密度限制了它们在准确捕获不确定性方面的有效性。

Method: 提出了一种利用条件匹配流（conditional flow matching）的生成模型，这是一种无模拟的基于流的生成模型，能够学习精确密度，从而生成高度准确的分割结果。通过在输入图像上引导流模型并采样多个数据点，能够合成分割样本，其像素级方差可可靠地反映底层数据分布，进而捕捉边界模糊区域的不确定性。

Result: 该方法在分割准确性方面取得了有竞争力的结果，并能生成提供分割结果可靠性深刻见解的不确定性图谱。

Conclusion: 该方法在提高分割准确性的同时，生成的量化不确定性图谱能提供关于分割结果可靠性的深刻见解。

Abstract: Quantifying aleatoric uncertainty in medical image segmentation is critical
since it is a reflection of the natural variability observed among expert
annotators. A conventional approach is to model the segmentation distribution
using the generative model, but current methods limit the expression ability of
generative models. While current diffusion-based approaches have demonstrated
impressive performance in approximating the data distribution, their inherent
stochastic sampling process and inability to model exact densities limit their
effectiveness in accurately capturing uncertainty. In contrast, our proposed
method leverages conditional flow matching, a simulation-free flow-based
generative model that learns an exact density, to produce highly accurate
segmentation results. By guiding the flow model on the input image and sampling
multiple data points, our approach synthesizes segmentation samples whose
pixel-wise variance reliably reflects the underlying data distribution. This
sampling strategy captures uncertainties in regions with ambiguous boundaries,
offering robust quantification that mirrors inter-annotator differences.
Experimental results demonstrate that our method not only achieves competitive
segmentation accuracy but also generates uncertainty maps that provide deeper
insights into the reliability of the segmentation outcomes. The code for this
paper is freely available at https://github.com/huynhspm/Data-Uncertainty

</details>


### [26] [Efficient Spatial-Temporal Modeling for Real-Time Video Analysis: A Unified Framework for Action Recognition and Object Tracking](https://arxiv.org/abs/2507.22421)
*Shahla John*

Main category: cs.CV

TL;DR: 提出了一种统一框架，通过新的层次注意力机制同时进行动作识别和对象跟踪，实现了实时、高精度的视频分析。


<details>
  <summary>Details</summary>
Motivation: 实时视频分析是一个计算机视觉领域的挑战性问题，需要对空间和时间信息进行有效处理，同时保持计算效率。现有方法在平衡准确性和速度方面常常遇到困难，尤其是在资源受限的环境中。

Method: 本研究提出了一种利用先进的时空建模技术进行同时动作识别和对象跟踪的统一框架。该框架建立在近期并行序列建模的进展之上，并引入了一种新颖的层次注意力机制，能够自适应地关注跨时间序列的相关空间区域。

Result: 实验证明，本研究提出的方法在标准基准测试中取得了最先进的性能，同时保持了实时推理速度。在UCF-101、HMDB-51和MOT17数据集上的大量实验表明，与现有方法相比，动作识别精度提高了3.2%，跟踪精度提高了2.8%，推理速度提高了40%。

Conclusion: 本研究提出的统一框架在UCF-101、HMDB-51和MOT17数据集上实现了最先进的性能，同时保持了实时推理速度，在动作识别精度上提高了3.2%，在跟踪精度上提高了2.8%，推理速度提高了40%。

Abstract: Real-time video analysis remains a challenging problem in computer vision,
requiring efficient processing of both spatial and temporal information while
maintaining computational efficiency. Existing approaches often struggle to
balance accuracy and speed, particularly in resource-constrained environments.
In this work, we present a unified framework that leverages advanced
spatial-temporal modeling techniques for simultaneous action recognition and
object tracking. Our approach builds upon recent advances in parallel sequence
modeling and introduces a novel hierarchical attention mechanism that
adaptively focuses on relevant spatial regions across temporal sequences. We
demonstrate that our method achieves state-of-the-art performance on standard
benchmarks while maintaining real-time inference speeds. Extensive experiments
on UCF-101, HMDB-51, and MOT17 datasets show improvements of 3.2% in action
recognition accuracy and 2.8% in tracking precision compared to existing
methods, with 40% faster inference time.

</details>


### [27] [HQ-CLIP: Leveraging Large Vision-Language Models to Create High-Quality Image-Text Datasets and CLIP Models](https://arxiv.org/abs/2507.22431)
*Zhixiang Wei,Guangting Wang,Xiaoxiao Ma,Ke Mei,Huaian Chen,Yi Jin,Fengyun Rao*

Main category: cs.CV

TL;DR: 通过利用大语言模型（LVLM）精炼图像-文本数据，生成了包含多粒度描述（正例、负例、长、短）的数据集VLM-150M。基于此数据集训练的HQ-CLIP模型在零样本分类、跨模态检索和细粒度视觉理解任务上均取得SOTA性能，并在检索任务上超越使用10倍数据量的CLIP模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决大规模但有噪声的图像-文本对数据对CLIP等基础视觉编码器的成功带来的相互依赖问题，探索是否能利用LVLM反向提升图像-文本对数据的质量，从而实现持续改进的自增强循环。

Method: 提出了一种利用大语言模型（LVLM）驱动的数据精炼流程，生成多粒度的文本描述（长正例、长负例、短正例、短负例），并在此基础上提出了一个扩展了对比学习的训练范式，引入了负面描述和短标签作为额外的监督信号。

Result: 成功构建了一个名为VLM-150M的精炼数据集，并提出了HQ-CLIP模型。HQ-CLIP在多个基准测试中表现出卓越的性能，证明了该方法的有效性。

Conclusion: HQ-CLIP在零样本分类、跨模态检索和细粒度视觉理解任务上取得了显著的性能提升，并且在检索基准上超越了使用10倍以上训练数据所训练的标准CLIP模型。

Abstract: Large-scale but noisy image-text pair data have paved the way for the success
of Contrastive Language-Image Pretraining (CLIP). As the foundation vision
encoder, CLIP in turn serves as the cornerstone for most large vision-language
models (LVLMs). This interdependence naturally raises an interesting question:
Can we reciprocally leverage LVLMs to enhance the quality of image-text pair
data, thereby opening the possibility of a self-reinforcing cycle for
continuous improvement? In this work, we take a significant step toward this
vision by introducing an LVLM-driven data refinement pipeline. Our framework
leverages LVLMs to process images and their raw alt-text, generating four
complementary textual formulas: long positive descriptions, long negative
descriptions, short positive tags, and short negative tags. Applying this
pipeline to the curated DFN-Large dataset yields VLM-150M, a refined dataset
enriched with multi-grained annotations. Based on this dataset, we further
propose a training paradigm that extends conventional contrastive learning by
incorporating negative descriptions and short tags as additional supervised
signals. The resulting model, namely HQ-CLIP, demonstrates remarkable
improvements across diverse benchmarks. Within a comparable training data
scale, our approach achieves state-of-the-art performance in zero-shot
classification, cross-modal retrieval, and fine-grained visual understanding
tasks. In retrieval benchmarks, HQ-CLIP even surpasses standard CLIP models
trained on the DFN-2B dataset, which contains 10$\times$ more training data
than ours. All code, data, and models are available at
https://zxwei.site/hqclip.

</details>


### [28] [From Sharp to Blur: Unsupervised Domain Adaptation for 2D Human Pose Estimation Under Extreme Motion Blur Using Event Cameras](https://arxiv.org/abs/2507.22438)
*Youngho Kim,Hoonhee Cho,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: 通过利用事件相机和一种新的域适应方法，我们能够提高在运动模糊条件下的姿态估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 运动模糊和弱光条件严重影响人体姿态估计的准确性，现有数据集和模型在模糊图像上表现不佳，存在领域差距。事件相机对运动模糊具有天然鲁棒性，可用于解决此问题。

Method: 1. 使用事件相机捕捉高时间分辨率的运动数据，克服运动模糊问题。 2. 提出事件增强方法，生成运动感知的模糊图像，缩小领域差距。 3. 构建学生-教师框架，通过互不确定性掩码迭代优化伪标签，提高学习效率。

Result: 实验结果表明，所提出的方法在运动模糊环境下的人体姿态估计性能优于传统方法，证明了事件相机在域适应和处理运动模糊方面的潜力。

Conclusion: 本研究提出的基于事件相机的域适应方法在运动模糊环境下实现了鲁棒的人体姿态估计，无需目标域的标注，并且优于传统的域适应方法。

Abstract: Human pose estimation is critical for applications such as rehabilitation,
sports analytics, and AR/VR systems. However, rapid motion and low-light
conditions often introduce motion blur, significantly degrading pose estimation
due to the domain gap between sharp and blurred images. Most datasets assume
stable conditions, making models trained on sharp images struggle in blurred
environments. To address this, we introduce a novel domain adaptation approach
that leverages event cameras, which capture high temporal resolution motion
data and are inherently robust to motion blur. Using event-based augmentation,
we generate motion-aware blurred images, effectively bridging the domain gap
between sharp and blurred domains without requiring paired annotations.
Additionally, we develop a student-teacher framework that iteratively refines
pseudo-labels, leveraging mutual uncertainty masking to eliminate incorrect
labels and enable more effective learning. Experimental results demonstrate
that our approach outperforms conventional domain-adaptive human pose
estimation methods, achieving robust pose estimation under motion blur without
requiring annotations in the target domain. Our findings highlight the
potential of event cameras as a scalable and effective solution for domain
adaptation in real-world motion blur environments. Our project codes are
available at https://github.com/kmax2001/EvSharp2Blur.

</details>


### [29] [TopoLiDM: Topology-Aware LiDAR Diffusion Models for Interpretable and Realistic LiDAR Point Cloud Generation](https://arxiv.org/abs/2507.22454)
*Jiuming Liu,Zheng Huang,Mengmeng Liu,Tianchen Deng,Francesco Nex,Hao Cheng,Hesheng Wang*

Main category: cs.CV

TL;DR: TopoLiDM通过结合GNN和扩散模型，并引入拓扑正则化，解决了现有LiDAR生成方法在几何真实感和全局拓扑一致性方面的不足，实现了高保真LiDAR生成。


<details>
  <summary>Details</summary>
Motivation: 现有LiDAR场景生成方法难以捕捉几何真实感和全局拓扑一致性。最近的LiDAR扩散模型（LiDMs）虽然提高了生成效率，但限制了其在建模详细几何结构和保持全局拓扑方面可解释性。

Method: TopoLiDM框架整合了图神经网络（GNN）和扩散模型，并通过拓扑正则化进行高保真LiDAR生成。该方法首先训练一个保持拓扑的变分自编码器（VAE）来提取潜在图表示，然后通过潜在扩散模型生成新的潜在拓扑图，并引入0维持久同调（PH）约束，以确保生成的LiDAR场景符合现实世界的全局拓扑结构。

Result: 与现有最先进的方法相比，TopoLiDM在KITTI-360数据集上取得了更好的效果，FRID降低了22.6%，MMD降低了9.2%。

Conclusion: TopoLiDM在KITTI-360数据集上进行了广泛的实验，证明了其在真实感和全局拓扑一致性方面优于最先进的方法，FRID降低了22.6%，MMD降低了9.2%，并且具有1.68个样本/秒的平均推理时间，展示了其在实际应用中的可扩展性。

Abstract: LiDAR scene generation is critical for mitigating real-world LiDAR data
collection costs and enhancing the robustness of downstream perception tasks in
autonomous driving. However, existing methods commonly struggle to capture
geometric realism and global topological consistency. Recent LiDAR Diffusion
Models (LiDMs) predominantly embed LiDAR points into the latent space for
improved generation efficiency, which limits their interpretable ability to
model detailed geometric structures and preserve global topological
consistency. To address these challenges, we propose TopoLiDM, a novel
framework that integrates graph neural networks (GNNs) with diffusion models
under topological regularization for high-fidelity LiDAR generation. Our
approach first trains a topological-preserving VAE to extract latent graph
representations by graph construction and multiple graph convolutional layers.
Then we freeze the VAE and generate novel latent topological graphs through the
latent diffusion models. We also introduce 0-dimensional persistent homology
(PH) constraints, ensuring the generated LiDAR scenes adhere to real-world
global topological structures. Extensive experiments on the KITTI-360 dataset
demonstrate TopoLiDM's superiority over state-of-the-art methods, achieving
improvements of 22.6% lower Frechet Range Image Distance (FRID) and 9.2% lower
Minimum Matching Distance (MMD). Notably, our model also enables fast
generation speed with an average inference time of 1.68 samples/s, showcasing
its scalability for real-world applications. We will release the related codes
at https://github.com/IRMVLab/TopoLiDM.

</details>


### [30] [Exploiting Diffusion Prior for Task-driven Image Restoration](https://arxiv.org/abs/2507.22459)
*Jaeha Kim,Junghun Oh,Kyoung Mu Lee*

Main category: cs.CV

TL;DR: EDTR是一种新的任务驱动图像恢复方法，它利用扩散模型来处理复杂的图像退化问题，并在保持视觉质量的同时恢复任务相关的细节。


<details>
  <summary>Details</summary>
Motivation: 现有任务驱动的图像恢复方法难以处理由多种复杂因素造成的实际图像退化问题，这促使研究者探索利用强大的扩散先验来解决此问题。

Method: EDTR方法通过在扩散过程中直接利用低质量图像中的线索（从添加了轻微噪声的基于像素误差的预恢复低质量图像生成），并采用少量去噪步骤来防止生成冗余细节，从而有效利用扩散先验进行任务驱动的图像恢复。

Result: EDTR方法在跨多种任务和复杂退化情况下，显著提升了任务性能和视觉质量。

Conclusion: EDTR方法能有效利用扩散先验进行任务驱动的图像恢复，显著提升了跨多种任务和复杂退化情况下的任务性能和视觉质量。

Abstract: Task-driven image restoration (TDIR) has recently emerged to address
performance drops in high-level vision tasks caused by low-quality (LQ) inputs.
Previous TDIR methods struggle to handle practical scenarios in which images
are degraded by multiple complex factors, leaving minimal clues for
restoration. This motivates us to leverage the diffusion prior, one of the most
powerful natural image priors. However, while the diffusion prior can help
generate visually plausible results, using it to restore task-relevant details
remains challenging, even when combined with recent TDIR methods. To address
this, we propose EDTR, which effectively harnesses the power of diffusion prior
to restore task-relevant details. Specifically, we propose directly leveraging
useful clues from LQ images in the diffusion process by generating from
pixel-error-based pre-restored LQ images with mild noise added. Moreover, we
employ a small number of denoising steps to prevent the generation of redundant
details that dilute crucial task-related information. We demonstrate that our
method effectively utilizes diffusion prior for TDIR, significantly enhancing
task performance and visual quality across diverse tasks with multiple complex
degradations.

</details>


### [31] [Shallow Features Matter: Hierarchical Memory with Heterogeneous Interaction for Unsupervised Video Object Segmentation](https://arxiv.org/abs/2507.22465)
*Zheng Xiangyu,He Songcheng,Li Wanyun,Li Xiaoqiang,Zhang Wei*

Main category: cs.CV

TL;DR: Existing UVOS methods struggle with precise predictions due to a lack of fine-grained information. This paper proposes HMHI-Net, a novel hierarchical memory network that integrates shallow and high-level features with a heterogeneous interaction mechanism. This approach improves precision and achieves state-of-the-art results on UVOS and video saliency detection tasks, demonstrating robustness across different backbones.


<details>
  <summary>Details</summary>
Motivation: Existing methods in Unsupervised Video Object Segmentation (UVOS) yield only marginal performance gains despite sophisticated memory mechanisms due to over-reliance on memorizing high-level semantic features, which are insufficient to generate precise predictions due to the deficiency of fine-grained information resulting from the absence of pixel-level prior knowledge.

Method: Propose a novel hierarchical memory architecture to incorporate both shallow- and high-level features for memory, leveraging complementary benefits of pixel and semantic information. Propose a heterogeneous interaction mechanism to balance simultaneous utilization of pixel and semantic memory features, performing pixel-semantic mutual interactions by considering their inherent feature discrepancies. Design Pixel-guided Local Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM) for delicate integration of fine-grained details in shallow-level memory and semantic representations in high-level memory.

Result: HMHI-Net consistently achieves state-of-the-art performance across all UVOS and video saliency detection benchmarks and exhibits high performance across different backbones.

Conclusion: HMHI-Net consistently achieves state-of-the-art performance across all UVOS and video saliency detection benchmarks and exhibits high performance across different backbones, demonstrating its superiority and robustness.

Abstract: Unsupervised Video Object Segmentation (UVOS) aims to predict pixel-level
masks for the most salient objects in videos without any prior annotations.
While memory mechanisms have been proven critical in various video segmentation
paradigms, their application in UVOS yield only marginal performance gains
despite sophisticated design. Our analysis reveals a simple but fundamental
flaw in existing methods: over-reliance on memorizing high-level semantic
features. UVOS inherently suffers from the deficiency of lacking fine-grained
information due to the absence of pixel-level prior knowledge. Consequently,
memory design relying solely on high-level features, which predominantly
capture abstract semantic cues, is insufficient to generate precise
predictions. To resolve this fundamental issue, we propose a novel hierarchical
memory architecture to incorporate both shallow- and high-level features for
memory, which leverages the complementary benefits of pixel and semantic
information. Furthermore, to balance the simultaneous utilization of the pixel
and semantic memory features, we propose a heterogeneous interaction mechanism
to perform pixel-semantic mutual interactions, which explicitly considers their
inherent feature discrepancies. Through the design of Pixel-guided Local
Alignment Module (PLAM) and Semantic-guided Global Integration Module (SGIM),
we achieve delicate integration of the fine-grained details in shallow-level
memory and the semantic representations in high-level memory. Our Hierarchical
Memory with Heterogeneous Interaction Network (HMHI-Net) consistently achieves
state-of-the-art performance across all UVOS and video saliency detection
benchmarks. Moreover, HMHI-Net consistently exhibits high performance across
different backbones, further demonstrating its superiority and robustness.
Project page: https://github.com/ZhengxyFlow/HMHI-Net .

</details>


### [32] [Visual Language Models as Zero-Shot Deepfake Detectors](https://arxiv.org/abs/2507.22469)
*Viacheslav Pirogov*

Main category: cs.CV

TL;DR: Deepfakes are a threat. Existing detection methods aren't robust. This paper proposes a new method using Vision Language Models (VLMs) that shows better results than old methods on a large dataset and a popular benchmark, even without special training.


<details>
  <summary>Details</summary>
Motivation: Existing deepfake detection methods rely on specialized classifiers trained on image domains and lack robustness due to the absence of auxiliary tasks. This work is inspired by the zero-shot capabilities of Vision Language Models to improve deepfake detection.

Method: A novel VLM-based approach for image classification is proposed and evaluated for deepfake detection. The method utilizes the zero-shot capabilities of Vision Language Models. Performance is compared against traditional methods on the DFDC-P dataset in both zero-shot and in-domain fine-tuning scenarios.

Result: The VLM-based approach achieves superior performance compared to almost all existing methods on a new high-quality deepfake dataset. The InstructBLIP architecture, specifically, demonstrates superiority over traditional classifiers on the DFDC-P dataset in both zero-shot and fine-tuning settings.

Conclusion: Vision Language Models (VLMs) demonstrate superior performance compared to traditional classifiers for deepfake detection, especially when leveraging their zero-shot capabilities. The proposed VLM-based approach, evaluated on a large deepfake dataset, shows enhanced robustness and outperforms existing methods.

Abstract: The contemporary phenomenon of deepfakes, utilizing GAN or diffusion models
for face swapping, presents a substantial and evolving threat in digital media,
identity verification, and a multitude of other systems. The majority of
existing methods for detecting deepfakes rely on training specialized
classifiers to distinguish between genuine and manipulated images, focusing
only on the image domain without incorporating any auxiliary tasks that could
enhance robustness. In this paper, inspired by the zero-shot capabilities of
Vision Language Models, we propose a novel VLM-based approach to image
classification and then evaluate it for deepfake detection. Specifically, we
utilize a new high-quality deepfake dataset comprising 60,000 images, on which
our zero-shot models demonstrate superior performance to almost all existing
methods. Subsequently, we compare the performance of the best-performing
architecture, InstructBLIP, on the popular deepfake dataset DFDC-P against
traditional methods in two scenarios: zero-shot and in-domain fine-tuning. Our
results demonstrate the superiority of VLMs over traditional classifiers.

</details>


### [33] [LIDAR: Lightweight Adaptive Cue-Aware Fusion Vision Mamba for Multimodal Segmentation of Structural Cracks](https://arxiv.org/abs/2507.22477)
*Hui Liu,Chen Jia,Fan Shi,Xu Cheng,Mengfei Shi,Xia Xie,Shengyong Chen*

Main category: cs.CV

TL;DR: 提出 LIDAR-Mamba 网络，通过 LacaVSS 和 LD3CF 模块，有效融合多模态数据（如形态和纹理信息）以实现低计算成本的像素级裂缝分割，并在实验中取得了优于 SOTA 的结果。


<details>
  <summary>Details</summary>
Motivation: 解决多模态裂缝分割任务中，使用多模态数据以低计算成本实现像素级分割的挑战。现有方法缺乏自适应感知和跨模态特征高效交互融合的能力。

Method: 提出了一种轻量级自适应线索感知 Vision Mamba 网络 (LIDAR)，包括轻量级自适应线索感知视觉状态空间模块 (LacaVSS) 和轻量级双域动态协同融合模块 (LD3CF)。LacaVSS 使用掩码引导的高效动态扫描策略 (EDG-SS) 来自适应地模拟裂缝线索。LD3CF 利用自适应频域感知器 (AFDP) 和双池化融合策略来跨模态地捕获空间和频域线索。此外，设计了一个轻量级动态调制的تن (LDMK) 卷积来感知复杂的形态结构，同时最小化计算开销，并取代了 LIDAR 中的大多数卷积操作。

Result: 实验结果表明，所提出的 LIDAR 方法在三个数据集上均优于其他最先进方法。在光场深度数据集上，LIDAR 方法取得了 0.8204 的 F1 分数和 0.8465 的 mIoU，而参数量仅为 5.35M。

Conclusion: 所提出的 LIDAR 网络在三个数据集上均优于其他最先进方法，在光场深度数据集上实现了 0.8204 的 F1 分数和 0.8465 的 mIoU，同时仅有 5.35M 参数。

Abstract: Achieving pixel-level segmentation with low computational cost using
multimodal data remains a key challenge in crack segmentation tasks. Existing
methods lack the capability for adaptive perception and efficient interactive
fusion of cross-modal features. To address these challenges, we propose a
Lightweight Adaptive Cue-Aware Vision Mamba network (LIDAR), which efficiently
perceives and integrates morphological and textural cues from different
modalities under multimodal crack scenarios, generating clear pixel-level crack
segmentation maps. Specifically, LIDAR is composed of a Lightweight Adaptive
Cue-Aware Visual State Space module (LacaVSS) and a Lightweight Dual Domain
Dynamic Collaborative Fusion module (LD3CF). LacaVSS adaptively models crack
cues through the proposed mask-guided Efficient Dynamic Guided Scanning
Strategy (EDG-SS), while LD3CF leverages an Adaptive Frequency Domain
Perceptron (AFDP) and a dual-pooling fusion strategy to effectively capture
spatial and frequency-domain cues across modalities. Moreover, we design a
Lightweight Dynamically Modulated Multi-Kernel convolution (LDMK) to perceive
complex morphological structures with minimal computational overhead, replacing
most convolutional operations in LIDAR. Experiments on three datasets
demonstrate that our method outperforms other state-of-the-art (SOTA) methods.
On the light-field depth dataset, our method achieves 0.8204 in F1 and 0.8465
in mIoU with only 5.35M parameters. Code and datasets are available at
https://github.com/Karl1109/LIDAR-Mamba.

</details>


### [34] [Estimating 2D Camera Motion with Hybrid Motion Basis](https://arxiv.org/abs/2507.22480)
*Haipeng Li,Tianhao Zhou,Zhanglei Yang,Yi Wu,Yan Chen,Zijing Mao,Shen Cheng,Bing Zeng,Shuaicheng Liu*

Main category: cs.CV

TL;DR: CamFlow 是一种新的相机运动估计框架，它结合了物理和随机运动基础，并使用改进的损失函数，在各种场景下都优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前估计二维相机运动的方法要么局限于平面场景的单应性方法，要么难以处理复杂非线性变换的基于网格的局部单应性技术。作者提出将不同单应性的流场结合起来，以创建单个单应性无法表示的运动模式。

Method: CamFlow 框架使用混合运动基础（物理基础和随机基础）来表示相机运动，并结合基于拉普拉斯分布的混合概率损失函数进行训练。

Result: CamFlow 在新创建的基准测试中，通过屏蔽现有光流数据集中的动态对象来隔离纯相机运动，其性能优于最先进的方法，在零样本设置下展现出卓越的鲁棒性和泛化能力。

Conclusion: CamFlow 框架通过结合基于相机几何的物理基础和用于复杂场景的随机基础，并采用基于拉普拉斯分布的混合概率损失函数，在各种场景下优于最先进的方法，在零样本设置下表现出卓越的鲁棒性和泛化能力。

Abstract: Estimating 2D camera motion is a fundamental computer vision task that models
the projection of 3D camera movements onto the 2D image plane. Current methods
rely on either homography-based approaches, limited to planar scenes, or
meshflow techniques that use grid-based local homographies but struggle with
complex non-linear transformations. A key insight of our work is that combining
flow fields from different homographies creates motion patterns that cannot be
represented by any single homography. We introduce CamFlow, a novel framework
that represents camera motion using hybrid motion bases: physical bases derived
from camera geometry and stochastic bases for complex scenarios. Our approach
includes a hybrid probabilistic loss function based on the Laplace distribution
that enhances training robustness. For evaluation, we create a new benchmark by
masking dynamic objects in existing optical flow datasets to isolate pure
camera motion. Experiments show CamFlow outperforms state-of-the-art methods
across diverse scenarios, demonstrating superior robustness and generalization
in zero-shot settings. Code and datasets are available at our project page:
https://lhaippp.github.io/CamFlow/.

</details>


### [35] [Robust Adverse Weather Removal via Spectral-based Spatial Grouping](https://arxiv.org/abs/2507.22498)
*Yuhwan Jeong,Yunseo Yang,Youngjo Yoon,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: SSGformer通过频谱分解和分组注意力，改进了恶劣天气下的图像恢复效果。


<details>
  <summary>Details</summary>
Motivation: 现有的AiO模型难以处理多变和局部失真的退化模式，尤其是在恶劣天气条件下，因为全局滤波方法无法应对高度可变和局部化的失真。

Method: 提出了一种名为SSGformer的新方法，该方法利用频谱分解（高频边缘特征和低频信息）和分组注意力机制（多头线性注意力和分组注意力机制）进行多天气图像恢复。SSGformer还引入了空间分组Transformer块，结合了通道注意力和空间注意力。

Result: SSGformer在处理多样化和复杂的恶劣天气退化方面表现出优越性，实验证明了其有效性。

Conclusion: SSGformer通过频谱分解和分组注意力机制，有效解决了现有AiO模型在处理多变和局部失真图像恢复的局限性，并在各种恶劣天气条件下实现了稳健的性能。

Abstract: Adverse weather conditions cause diverse and complex degradation patterns,
driving the development of All-in-One (AiO) models. However, recent AiO
solutions still struggle to capture diverse degradations, since global
filtering methods like direct operations on the frequency domain fail to handle
highly variable and localized distortions. To address these issue, we propose
Spectral-based Spatial Grouping Transformer (SSGformer), a novel approach that
leverages spectral decomposition and group-wise attention for multi-weather
image restoration. SSGformer decomposes images into high-frequency edge
features using conventional edge detection and low-frequency information via
Singular Value Decomposition. We utilize multi-head linear attention to
effectively model the relationship between these features. The fused features
are integrated with the input to generate a grouping-mask that clusters regions
based on the spatial similarity and image texture. To fully leverage this mask,
we introduce a group-wise attention mechanism, enabling robust adverse weather
removal and ensuring consistent performance across diverse weather conditions.
We also propose a Spatial Grouping Transformer Block that uses both channel
attention and spatial attention, effectively balancing feature-wise
relationships and spatial dependencies. Extensive experiments show the
superiority of our approach, validating its effectiveness in handling the
varied and intricate adverse weather degradations.

</details>


### [36] [DACA-Net: A Degradation-Aware Conditional Diffusion Network for Underwater Image Enhancement](https://arxiv.org/abs/2507.22501)
*Chang Huang,Jiahang Cao,Jun Ma,Kieren Yu,Cong Li,Huayong Yang,Kaishun Wu*

Main category: cs.CV

TL;DR: 通过一种新的退化感知条件扩散模型，利用水下特定的物理先验，自适应地增强水下图像，取得了比现有方法更好的效果。


<details>
  <summary>Details</summary>
Motivation: 水下图像通常会因散射和吸收等复杂光学效应而出现严重的颜色失真、低可见度和结构清晰度下降的问题，这会大大降低其视觉质量并限制下游视觉感知任务的性能。现有的增强方法通常难以自适应地处理各种退化条件，也无法有效地利用水下特定的物理先验。

Method: 提出了一种退化感知条件扩散模型来增强水下图像。首先使用轻量级双流卷积网络预测退化水平，生成连续的退化分数作为语义引导。然后，基于该分数，提出了一种新颖的基于条件扩散的恢复网络，并采用Swin UNet骨干，实现了自适应噪声调度和层次特征细化。此外，还提出了一种退化引导自适应特征融合模块和结合感知一致性、直方图匹配和特征级对比度的混合损失函数，以结合水下特定的物理先验。

Result: 在基准数据集上的综合实验表明，我们提出的方法能够有效恢复水下图像，具有优越的色彩保真度、感知质量和结构细节。与最先进的方法相比，我们的框架在定量指标和定性视觉评估方面均取得了显著的改进。

Conclusion: 该方法有效恢复了水下图像，具有优越的色彩保真度、感知质量和结构细节。与最先进的方法相比，我们的框架在定量指标和定性视觉评估方面均取得了显著的改进。

Abstract: Underwater images typically suffer from severe colour distortions, low
visibility, and reduced structural clarity due to complex optical effects such
as scattering and absorption, which greatly degrade their visual quality and
limit the performance of downstream visual perception tasks. Existing
enhancement methods often struggle to adaptively handle diverse degradation
conditions and fail to leverage underwater-specific physical priors
effectively. In this paper, we propose a degradation-aware conditional
diffusion model to enhance underwater images adaptively and robustly. Given a
degraded underwater image as input, we first predict its degradation level
using a lightweight dual-stream convolutional network, generating a continuous
degradation score as semantic guidance. Based on this score, we introduce a
novel conditional diffusion-based restoration network with a Swin UNet
backbone, enabling adaptive noise scheduling and hierarchical feature
refinement. To incorporate underwater-specific physical priors, we further
propose a degradation-guided adaptive feature fusion module and a hybrid loss
function that combines perceptual consistency, histogram matching, and
feature-level contrast. Comprehensive experiments on benchmark datasets
demonstrate that our method effectively restores underwater images with
superior colour fidelity, perceptual quality, and structural details. Compared
with SOTA approaches, our framework achieves significant improvements in both
quantitative metrics and qualitative visual assessments.

</details>


### [37] [AlphaDent: A dataset for automated tooth pathology detection](https://arxiv.org/abs/2507.22512)
*Evgeniy I. Sosnin,Yuriy L. Vasilev,Roman A. Solovyev,Aleksandr L. Stempkovskiy,Dmitry V. Telpukhov,Artem A. Vasilev,Aleksandr A. Amerikanov,Aleksandr Y. Romanov*

Main category: cs.CV

TL;DR: 研究者发布了一个名为AlphaDent的新牙科数据集，包含1200多张标注图像，并提供了相应的训练代码和模型权重。该数据集可用于牙科实例分割研究，实验结果显示出高预测质量。


<details>
  <summary>Details</summary>
Motivation: 为了推动牙科研究和实例分割技术在牙科领域的应用，研究者创建并发布了一个新的、独特的数据集AlphaDent，旨在为相关研究提供高质量的数据支持。

Method: 本研究提出了一种新的牙科数据集AlphaDent，该数据集基于DSLR相机拍摄的295名患者的牙齿照片，包含1200多张图像。数据集被标注用于解决实例分割问题，并分为9个类别。文章详细描述了数据集和标注格式，并提供了使用该数据集进行实例分割的神经网络训练实验细节。

Result: 使用AlphaDent数据集训练的神经网络在实例分割任务上取得了高质量的预测结果。

Conclusion: 该研究发布了一个名为AlphaDent的新型牙科数据集，该数据集包含295名患者的1200多张牙科照片，并针对实例分割问题进行了9个类别的标注。同时，文章还详细介绍了数据集的细节、标注格式以及使用该数据集训练的实例分割神经网络的实验细节。研究结果表明，所提出的方法能够实现高质量的预测。此外，该数据集、训练/推理代码和模型权重均已根据开放许可协议发布。

Abstract: In this article, we present a new unique dataset for dental research -
AlphaDent. This dataset is based on the DSLR camera photographs of the teeth of
295 patients and contains over 1200 images. The dataset is labeled for solving
the instance segmentation problem and is divided into 9 classes. The article
provides a detailed description of the dataset and the labeling format. The
article also provides the details of the experiment on neural network training
for the Instance Segmentation problem using this dataset. The results obtained
show high quality of predictions. The dataset is published under an open
license; and the training/inference code and model weights are also available
under open licenses.

</details>


### [38] [HRVVS: A High-resolution Video Vasculature Segmentation Network via Hierarchical Autoregressive Residual Priors](https://arxiv.org/abs/2507.22530)
*Xincheng Yao,Yijun Yang,Kangwei Guo,Ruiqiang Xiao,Haipeng Zhou,Haisu Tao,Jian Yang,Lei Zhu*

Main category: cs.CV

TL;DR: 提出了一种名为HRVVS的新型网络，用于分割肝脏手术视频中的血管，通过结合预训练的VAR模型和动态记忆解码器，提高了分割精度。


<details>
  <summary>Details</summary>
Motivation: 为了解决肝脏血管分割任务中缺乏合适的数据集和任务本身的复杂性问题。

Method: 提出了一种名为HRVVS的新型高分辨率视频血管分割网络，该网络将预训练的视觉自回归模型（VAR）嵌入到分层编码器的不同层中，以减少降采样过程中的信息损失，并设计了一个动态记忆解码器来最小化冗余信息的传输并保留更多帧间细节。

Result: 在手术视频数据集上进行了广泛的实验，证明了HRVVS的优越性，并公开了源代码和数据集。

Conclusion: HRVVS在肝脏血管分割任务上显著优于现有最先进的方法。

Abstract: The segmentation of the hepatic vasculature in surgical videos holds
substantial clinical significance in the context of hepatectomy procedures.
However, owing to the dearth of an appropriate dataset and the inherently
complex task characteristics, few researches have been reported in this domain.
To address this issue, we first introduce a high quality frame-by-frame
annotated hepatic vasculature dataset containing 35 long hepatectomy videos and
11442 high-resolution frames. On this basis, we propose a novel high-resolution
video vasculature segmentation network, dubbed as HRVVS. We innovatively embed
a pretrained visual autoregressive modeling (VAR) model into different layers
of the hierarchical encoder as prior information to reduce the information
degradation generated during the downsampling process. In addition, we designed
a dynamic memory decoder on a multi-view segmentation network to minimize the
transmission of redundant information while preserving more details between
frames. Extensive experiments on surgical video datasets demonstrate that our
proposed HRVVS significantly outperforms the state-of-the-art methods. The
source code and dataset will be publicly available at
\href{https://github.com/scott-yjyang/xx}{https://github.com/scott-yjyang/HRVVS}.

</details>


### [39] [RainbowPrompt: Diversity-Enhanced Prompt-Evolving for Continual Learning](https://arxiv.org/abs/2507.22553)
*Kiseong Hong,Gyeong-hyeon Kim,Eunwoo Kim*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Prompt-based continual learning provides a rehearsal-free solution by tuning
small sets of parameters while keeping pre-trained models frozen. To meet the
complex demands of sequential tasks, it is crucial to integrate task-specific
knowledge within prompts effectively. However, existing works rely on either
fixed learned prompts (i.e., prompts whose representations remain unchanged
during new task learning) or on prompts generated from an entangled task-shared
space, limiting the representational diversity of the integrated prompt. To
address this issue, we propose a novel prompt-evolving mechanism to adaptively
aggregate base prompts (i.e., task-specific prompts) into a unified prompt
while ensuring diversity. By transforming and aligning base prompts, both
previously learned and newly introduced, our approach continuously evolves
accumulated knowledge to facilitate learning new tasks. We further introduce a
learnable probabilistic gate that adaptively determines which layers to
activate during the evolution process. We validate our method on image
classification and video action recognition tasks in class-incremental
learning, achieving average gains of 9.07% and 7.40% over existing methods
across all scenarios.

</details>


### [40] [Subtyping Breast Lesions via Generative Augmentation based Long-tailed Recognition in Ultrasound](https://arxiv.org/abs/2507.22568)
*Shijing Chen,Xinrui Zhou,Yuhao Wang,Yuhao Huang,Ao Chang,Dong Ni,Ruobing Huang*

Main category: cs.CV

TL;DR: 本研究提出了一种双阶段框架，通过高保真数据合成和类别可控的合成网络来解决乳腺超声图像识别中的长尾分布问题，并在实验中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 超声（US）作为一种安全且易于使用的成像方式，在乳腺异常筛查和诊断中得到了广泛应用。然而，不同亚型的发病率呈现出长尾分布，给自动识别带来了显著挑战。生成式增强为纠正数据分布提供了一种有前途的解决方案。

Method: 提出了一种用于长尾分类的双阶段框架，该框架通过高保真数据合成来减轻分布偏差，同时避免过度使用而损害整体性能。该框架包含一个由强化学习驱动的自适应采样器，通过训练一个战略性多智能体来动态校准合成真实数据比率，以弥补真实数据的稀缺性，同时确保稳定的判别能力。此外，类别可控的合成网络集成了基于草图的感知分支，利用解剖学先验来维持独特的类别特征，同时实现无需注释的推理。

Result: 与最先进的方法相比，我们的方法在内部长尾和公开不平衡乳腺超声数据集上取得了有希望的性能。

Conclusion: 该方法在内部长尾和公开不平衡乳腺超声数据集上取得了有希望的性能，与最先进的方法相比。

Abstract: Accurate identification of breast lesion subtypes can facilitate personalized
treatment and interventions. Ultrasound (US), as a safe and accessible imaging
modality, is extensively employed in breast abnormality screening and
diagnosis. However, the incidence of different subtypes exhibits a skewed
long-tailed distribution, posing significant challenges for automated
recognition. Generative augmentation provides a promising solution to rectify
data distribution. Inspired by this, we propose a dual-phase framework for
long-tailed classification that mitigates distributional bias through
high-fidelity data synthesis while avoiding overuse that corrupts holistic
performance. The framework incorporates a reinforcement learning-driven
adaptive sampler, dynamically calibrating synthetic-real data ratios by
training a strategic multi-agent to compensate for scarcities of real data
while ensuring stable discriminative capability. Furthermore, our
class-controllable synthetic network integrates a sketch-grounded perception
branch that harnesses anatomical priors to maintain distinctive class features
while enabling annotation-free inference. Extensive experiments on an in-house
long-tailed and a public imbalanced breast US datasets demonstrate that our
method achieves promising performance compared to state-of-the-art approaches.
More synthetic images can be found at
https://github.com/Stinalalala/Breast-LT-GenAug.

</details>


### [41] [COOkeD: Ensemble-based OOD detection in the era of zero-shot CLIP](https://arxiv.org/abs/2507.22576)
*Galadrielle Humblot-Renaux,Gianni Franchi,Sergio Escalera,Thomas B. Moeslund*

Main category: cs.CV

TL;DR: COOkeD是一种新颖的OOD检测方法，通过集成多种分类器（包括监督、零样本CLIP和线性探针）来提高准确性和鲁棒性，并在各种基准测试和现实场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的OOD检测方法通常依赖于单一分类器，其性能受限于该分类器在ID数据上的能力。本研究旨在通过构建异构集成来克服这一挑战，以实现更优越的OOD检测性能。

Method: COOkeD提出了一种异构集成方法，结合了在特定数据集上端到端训练的闭世界分类器、零样本CLIP分类器以及在CLIP图像特征上训练的线性探针分类器的预测。

Result: COOkeD在CIFAR100和ImageNet等流行基准测试以及更具挑战性的现实场景（包括训练时标签噪声、测试时协变量偏移和零样本偏移）中，取得了最先进的性能和比现有方法更高的鲁棒性。

Conclusion: COOkeD通过结合监督分类器、零样本CLIP分类器和线性探针分类器的预测，在OOD检测方面取得了最先进的性能和更高的鲁棒性，克服了现有方法的局限性。

Abstract: Out-of-distribution (OOD) detection is an important building block in
trustworthy image recognition systems as unknown classes may arise at
test-time. OOD detection methods typically revolve around a single classifier,
leading to a split in the research field between the classical supervised
setting (e.g. ResNet18 classifier trained on CIFAR100) vs. the zero-shot
setting (class names fed as prompts to CLIP). In both cases, an overarching
challenge is that the OOD detection performance is implicitly constrained by
the classifier's capabilities on in-distribution (ID) data. In this work, we
show that given a little open-mindedness from both ends, remarkable OOD
detection can be achieved by instead creating a heterogeneous ensemble - COOkeD
combines the predictions of a closed-world classifier trained end-to-end on a
specific dataset, a zero-shot CLIP classifier, and a linear probe classifier
trained on CLIP image features. While bulky at first sight, this approach is
modular, post-hoc and leverages the availability of pre-trained VLMs, thus
introduces little overhead compared to training a single standard classifier.
We evaluate COOkeD on popular CIFAR100 and ImageNet benchmarks, but also
consider more challenging, realistic settings ranging from training-time label
noise, to test-time covariate shift, to zero-shot shift which has been
previously overlooked. Despite its simplicity, COOkeD achieves state-of-the-art
performance and greater robustness compared to both classical and CLIP-based
OOD detection methods. Code is available at https://github.com/glhr/COOkeD

</details>


### [42] [Robust Deepfake Detection for Electronic Know Your Customer Systems Using Registered Images](https://arxiv.org/abs/2507.22601)
*Takuma Amada,Kazuya Kakizaki,Taiki Miyagawa,Akinori F. Ebihara,Kaede Shiohara,Toshihiko Yamasaki*

Main category: cs.CV

TL;DR: 提出了一种用于电子客户尽职调查（eKYC）系统的深度伪造检测算法，该算法通过检测身份向量的时间不一致性和利用注册图像来提高准确性，并使用在更大数据集上训练的特征提取器来提高鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 为了确保电子客户尽职调查（eKYC）系统的可靠性，使其能够抵抗深度伪造攻击，有必要开发一种能够同时检测换脸和伪造表演，并且能够抵抗图像降级的鲁棒性深度伪造检测器。

Method: 首先，通过检测人脸识别模型提取的身份向量中的时间不一致性来评估视频的真实性，以实现对换脸和伪造表演的全面检测。其次，该算法利用注册图像计算输入视频和注册图像之间的身份差异，以提高检测精度。最后，在更大的数据集上训练的人脸特征提取器可以同时提高检测性能和抵抗图像降级的能力。

Result: 实验结果表明，所提出的方法能够全面准确地检测换脸和伪造表演，并且能够抵抗各种形式的未知图像降级。

Conclusion: 该算法能够全面检测换脸和伪造表演，并且能够抵抗各种未知的图像降级

Abstract: In this paper, we present a deepfake detection algorithm specifically
designed for electronic Know Your Customer (eKYC) systems. To ensure the
reliability of eKYC systems against deepfake attacks, it is essential to
develop a robust deepfake detector capable of identifying both face swapping
and face reenactment, while also being robust to image degradation. We address
these challenges through three key contributions: (1)~Our approach evaluates
the video's authenticity by detecting temporal inconsistencies in identity
vectors extracted by face recognition models, leading to comprehensive
detection of both face swapping and face reenactment. (2)~In addition to
processing video input, the algorithm utilizes a registered image (assumed to
be genuine) to calculate identity discrepancies between the input video and the
registered image, significantly improving detection accuracy. (3)~We find that
employing a face feature extractor trained on a larger dataset enhances both
detection performance and robustness against image degradation. Our
experimental results show that our proposed method accurately detects both face
swapping and face reenactment comprehensively and is robust against various
forms of unseen image degradation. Our source code is publicly available
https://github.com/TaikiMiyagawa/DeepfakeDetection4eKYC.

</details>


### [43] [ShortFT: Diffusion Model Alignment via Shortcut-based Fine-Tuning](https://arxiv.org/abs/2507.22604)
*Xiefan Guo,Miaomiao Cui,Liefeng Bo,Di Huang*

Main category: cs.CV

TL;DR: ShortFT是一种新的微调策略，通过使用更短的去噪链来提高扩散模型的效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于反向传播的方法在扩散模型中通过端到端反向传播奖励梯度来实现与奖励函数的对齐，但由于计算成本和梯度爆炸的风险，导致了次优结果。因此，需要一种更有效的方法。

Method: ShortFT是一种基于捷径的微调策略，利用了近期研究的保持轨迹的少步扩散模型，构建了一个更短的去噪链，实现了端到端的奖励梯度反向传播。

Result: ShortFT已被严格测试，可有效地应用于各种奖励函数，显著提高了对齐性能，并超越了现有技术。

Conclusion: ShortFT通过利用更短的去噪链，提高了微调扩散模型的效率和效果，并在各种奖励函数上显著提升了对齐性能，优于现有技术。

Abstract: Backpropagation-based approaches aim to align diffusion models with reward
functions through end-to-end backpropagation of the reward gradient within the
denoising chain, offering a promising perspective. However, due to the
computational costs and the risk of gradient explosion associated with the
lengthy denoising chain, existing approaches struggle to achieve complete
gradient backpropagation, leading to suboptimal results. In this paper, we
introduce Shortcut-based Fine-Tuning (ShortFT), an efficient fine-tuning
strategy that utilizes the shorter denoising chain. More specifically, we
employ the recently researched trajectory-preserving few-step diffusion model,
which enables a shortcut over the original denoising chain, and construct a
shortcut-based denoising chain of shorter length. The optimization on this
chain notably enhances the efficiency and effectiveness of fine-tuning the
foundational model. Our method has been rigorously tested and can be
effectively applied to various reward functions, significantly improving
alignment performance and surpassing state-of-the-art alternatives.

</details>


### [44] [VL-Cogito: Progressive Curriculum Reinforcement Learning for Advanced Multimodal Reasoning](https://arxiv.org/abs/2507.22607)
*Ruifeng Yuan,Chenghao Xiao,Sicong Leng,Jianyu Wang,Long Li,Weiwen Xu,Hou Pong Chan,Deli Zhao,Tingyang Xu,Zhongyu Wei,Hao Zhang,Yu Rong*

Main category: cs.CV

TL;DR: A new multimodal reasoning model, VL-Cogito, trained with a Progressive Curriculum Reinforcement Learning (PCuRL) framework, improves reasoning abilities by systematically guiding the model through tasks of gradually increasing difficulty and adaptively regulating reasoning path length.


<details>
  <summary>Details</summary>
Motivation: Existing multimodal reasoning models often exhibit unstable performance across various domains and difficulty levels due to the inherent complexity and diversity of multimodal tasks. To address these limitations, we propose VL-Cogito.

Method: VL-Cogito is trained via a novel multi-stage Progressive Curriculum Reinforcement Learning (PCuRL) framework, which includes an online difficulty soft weighting mechanism and a dynamic length reward mechanism to improve reasoning abilities.

Result: VL-Cogito consistently matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding.

Conclusion: VL-Cogito matches or surpasses existing reasoning-oriented models across mainstream multimodal benchmarks spanning mathematics, science, logic, and general understanding, validating the effectiveness of our approach.

Abstract: Reinforcement learning has proven its effectiveness in enhancing the
reasoning capabilities of large language models. Recent research efforts have
progressively extended this paradigm to multimodal reasoning tasks. Due to the
inherent complexity and diversity of multimodal tasks, especially in semantic
content and problem formulations, existing models often exhibit unstable
performance across various domains and difficulty levels. To address these
limitations, we propose VL-Cogito, an advanced multimodal reasoning model
trained via a novel multi-stage Progressive Curriculum Reinforcement Learning
(PCuRL) framework. PCuRL systematically guides the model through tasks of
gradually increasing difficulty, substantially improving its reasoning
abilities across diverse multimodal contexts. The framework introduces two key
innovations: (1) an online difficulty soft weighting mechanism, dynamically
adjusting training difficulty across successive RL training stages; and (2) a
dynamic length reward mechanism, which encourages the model to adaptively
regulate its reasoning path length according to task complexity, thus balancing
reasoning efficiency with correctness. Experimental evaluations demonstrate
that VL-Cogito consistently matches or surpasses existing reasoning-oriented
models across mainstream multimodal benchmarks spanning mathematics, science,
logic, and general understanding, validating the effectiveness of our approach.

</details>


### [45] [Generative Active Learning for Long-tail Trajectory Prediction via Controllable Diffusion Model](https://arxiv.org/abs/2507.22615)
*Daehee Park,Monu Surana,Pranav Desai,Ashish Mehta,Reuben MV John,Kuk-Jin Yoon*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: While data-driven trajectory prediction has enhanced the reliability of
autonomous driving systems, it still struggles with rarely observed long-tail
scenarios. Prior works addressed this by modifying model architectures, such as
using hypernetworks. In contrast, we propose refining the training process to
unlock each model's potential without altering its structure. We introduce
Generative Active Learning for Trajectory prediction (GALTraj), the first
method to successfully deploy generative active learning into trajectory
prediction. It actively identifies rare tail samples where the model fails and
augments these samples with a controllable diffusion model during training. In
our framework, generating scenarios that are diverse, realistic, and preserve
tail-case characteristics is paramount. Accordingly, we design a tail-aware
generation method that applies tailored diffusion guidance to generate
trajectories that both capture rare behaviors and respect traffic rules. Unlike
prior simulation methods focused solely on scenario diversity, GALTraj is the
first to show how simulator-driven augmentation benefits long-tail learning in
trajectory prediction. Experiments on multiple trajectory datasets (WOMD,
Argoverse2) with popular backbones (QCNet, MTR) confirm that our method
significantly boosts performance on tail samples and also enhances accuracy on
head samples.

</details>


### [46] [Bridging the Gap in Missing Modalities: Leveraging Knowledge Distillation and Style Matching for Brain Tumor Segmentation](https://arxiv.org/abs/2507.22626)
*Shenghao Zhu,Yifei Chen,Weihong Chen,Yuanhan Wang,Chang Liu,Shuo Jiang,Feiwei Qin,Changmiao Wang*

Main category: cs.CV

TL;DR: MST-KDNet 通过多尺度 Transformer 知识蒸馏、双模态 Logit 蒸馏和全局风格匹配模块，有效解决了脑肿瘤分割中模态缺失导致的边界分割不敏感和特征迁移问题，在 BraTS 和 FeTS 2024 数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 解决医学图像分析中脑肿瘤分割的挑战，特别是在关键成像模态缺失时，肿瘤边界分割不敏感和特征迁移的问题。

Method: MST-KDNet 模型采用了多尺度 Transformer 知识蒸馏来捕捉不同分辨率下的注意力权重，结合双模态 Logit 蒸馏来增强知识迁移，并通过全局风格匹配模块集成特征匹配与对抗性学习。

Result: MST-KDNet 在 Dice 和 HD95 分数上均超越了当前领先方法，特别是在模态缺失较多的情况下，展现了卓越的性能。

Conclusion: MST-KDNet 在 BraTS 和 FeTS 2024 数据集上表现出色，尤其在处理模态缺失的情况下，其 Dice 和 HD95 分数均优于现有领先方法，显示出强大的鲁棒性和泛化能力，适用于临床实际应用。

Abstract: Accurate and reliable brain tumor segmentation, particularly when dealing
with missing modalities, remains a critical challenge in medical image
analysis. Previous studies have not fully resolved the challenges of tumor
boundary segmentation insensitivity and feature transfer in the absence of key
imaging modalities. In this study, we introduce MST-KDNet, aimed at addressing
these critical issues. Our model features Multi-Scale Transformer Knowledge
Distillation to effectively capture attention weights at various resolutions,
Dual-Mode Logit Distillation to improve the transfer of knowledge, and a Global
Style Matching Module that integrates feature matching with adversarial
learning. Comprehensive experiments conducted on the BraTS and FeTS 2024
datasets demonstrate that MST-KDNet surpasses current leading methods in both
Dice and HD95 scores, particularly in conditions with substantial modality
loss. Our approach shows exceptional robustness and generalization potential,
making it a promising candidate for real-world clinical applications. Our
source code is available at https://github.com/Quanato607/MST-KDNet.

</details>


### [47] [LOTS of Fashion! Multi-Conditioning for Image Generation via Sketch-Text Pairing](https://arxiv.org/abs/2507.22627)
*Federico Girella,Davide Talon,Ziyue Liu,Zanxi Ruan,Yiming Wang,Marco Cristani*

Main category: cs.CV

TL;DR: LOTS是一种用于组合草图和文本的完整时装图像生成方法，它利用局部和全局的文本-草图信息，并通过扩散模型生成高质量、可定制的时装设计。


<details>
  <summary>Details</summary>
Motivation: 为了实现包含草图和文本的完整时装图像的组合生成。

Method: LOTS（LOcalized Text and Sketch）方法，利用全局描述和成对的局部草图+文本信息进行条件化，并引入新颖的基于步的融合策略以适应扩散模型。首先，模块化对中心表示将草图和文本编码到共享的潜在空间，同时保留独立的局部特征；然后，扩散对引导阶段通过扩散模型的多步去噪过程中的基于注意力的引导来整合局部和全局条件。

Result: LOTS达到了最先进的图像生成性能，在全局和局部指标上均表现优异，同时在设计定制化方面展现了前所未有的水平。

Conclusion: LOTS在全局和局部指标上都达到了最先进的图像生成性能，并且在设计定制化方面取得了前所未有的水平。

Abstract: Fashion design is a complex creative process that blends visual and textual
expressions. Designers convey ideas through sketches, which define spatial
structure and design elements, and textual descriptions, capturing material,
texture, and stylistic details. In this paper, we present LOcalized Text and
Sketch for fashion image generation (LOTS), an approach for compositional
sketch-text based generation of complete fashion outlooks. LOTS leverages a
global description with paired localized sketch + text information for
conditioning and introduces a novel step-based merging strategy for diffusion
adaptation. First, a Modularized Pair-Centric representation encodes sketches
and text into a shared latent space while preserving independent localized
features; then, a Diffusion Pair Guidance phase integrates both local and
global conditioning via attention-based guidance within the diffusion model's
multi-step denoising process. To validate our method, we build on Fashionpedia
to release Sketchy, the first fashion dataset where multiple text-sketch pairs
are provided per image. Quantitative results show LOTS achieves
state-of-the-art image generation performance on both global and localized
metrics, while qualitative examples and a human evaluation study highlight its
unprecedented level of design customization.

</details>


### [48] [SpectraSentinel: LightWeight Dual-Stream Real-Time Drone Detection, Tracking and Payload Identification](https://arxiv.org/abs/2507.22650)
*Shahriar Kabir,Istiak Ahmmed Rifti,H. M. Shadman Tabib,Mushfiqur Rahman,Sadatul Islam Sadi,Hasnaen Adil,Ahmed Mahir Sultan Rumi,Ch Md Rakin Haider*

Main category: cs.CV

TL;DR: 为应对无人机安全威胁，提出一种双流框架，利用独立优化的YOLOv11n模型处理红外和可见光数据，实现高效准确的实时无人机监控。


<details>
  <summary>Details</summary>
Motivation: 为了应对民用空域无人机激增带来的安全问题，以及2025VIP杯赛事的无人机检测、跟踪和载荷识别任务，需要强大的实时监控系统。

Method: 提出了一种双流无人机监控框架，分别在红外和可见光数据流上部署独立的YOLOv11n目标检测器，并对数据预处理、增强和训练超参数进行了针对性优化。

Result: 轻量级的YOLOv11n模型在区分无人机与鸟类以及识别载荷类型方面表现出高准确率，同时保持了实时性能。

Conclusion: 该框架利用独立的YOLOv11n模型分别处理红外和可见光数据流，优化了对不同模态的特性，实现了高效且准确的无人机监控。

Abstract: The proliferation of drones in civilian airspace has raised urgent security
concerns, necessitating robust real-time surveillance systems. In response to
the 2025 VIP Cup challenge tasks - drone detection, tracking, and payload
identification - we propose a dual-stream drone monitoring framework. Our
approach deploys independent You Only Look Once v11-nano (YOLOv11n) object
detectors on parallel infrared (thermal) and visible (RGB) data streams,
deliberately avoiding early fusion. This separation allows each model to be
specifically optimized for the distinct characteristics of its input modality,
addressing the unique challenges posed by small aerial objects in diverse
environmental conditions. We customize data preprocessing and augmentation
strategies per domain - such as limiting color jitter for IR imagery - and
fine-tune training hyperparameters to enhance detection performance under
conditions of heavy noise, low light, and motion blur. The resulting
lightweight YOLOv11n models demonstrate high accuracy in distinguishing drones
from birds and in classifying payload types, all while maintaining real-time
performance. This report details the rationale for a dual-modality design, the
specialized training pipelines, and the architectural optimizations that
collectively enable efficient and accurate drone surveillance across RGB and IR
channels.

</details>


### [49] [Graph-Guided Dual-Level Augmentation for 3D Scene Segmentation](https://arxiv.org/abs/2507.22668)
*Hongbin Lin,Yifan Jiang,Juangui Xu,Jesse Jiaxi Xu,Yi Lu,Zhengyu Hu,Ying-Cong Chen,Hao Wang*

Main category: cs.CV

TL;DR: 提出一种图引导数据增强框架，通过双层约束（局部和全局）来增强3D点云分割，能够生成更真实、多样化的场景，并提高分割性能。


<details>
  <summary>Details</summary>
Motivation: 现有3D点云分割方法主要依赖数据增强来减轻大规模标注的负担，但大多数增强策略仅关注局部变换或语义重组，未能考虑场景内的全局结构依赖性。为了解决这个局限性，需要一种新的方法来处理这个问题。

Method: 提出了一种图引导数据增强框架，该框架具有双层约束，用于真实3D场景合成。该方法学习真实世界数据中的对象关系统计来构建场景生成的引导图。局部约束强制执行对象之间的几何合理性和语义一致性，而全局约束通过将生成的布局与引导图对齐来维护场景的拓扑结构。

Result: 实验结果表明，该框架能够生成多样化且高质量的增强场景，并在各种模型上持续提升点云分割性能。

Conclusion: 提出的图引导数据增强框架通过双层约束实现了真实3D场景的合成，在室内和室外数据集上的广泛实验证明了其生成多样化和高质量增强场景的能力，从而在各种模型上一致地提高了点云分割性能。

Abstract: 3D point cloud segmentation aims to assign semantic labels to individual
points in a scene for fine-grained spatial understanding. Existing methods
typically adopt data augmentation to alleviate the burden of large-scale
annotation. However, most augmentation strategies only focus on local
transformations or semantic recomposition, lacking the consideration of global
structural dependencies within scenes. To address this limitation, we propose a
graph-guided data augmentation framework with dual-level constraints for
realistic 3D scene synthesis. Our method learns object relationship statistics
from real-world data to construct guiding graphs for scene generation.
Local-level constraints enforce geometric plausibility and semantic consistency
between objects, while global-level constraints maintain the topological
structure of the scene by aligning the generated layout with the guiding graph.
Extensive experiments on indoor and outdoor datasets demonstrate that our
framework generates diverse and high-quality augmented scenes, leading to
consistent improvements in point cloud segmentation performance across various
models.

</details>


### [50] [MergeSAM: Unsupervised change detection of remote sensing images based on the Segment Anything Model](https://arxiv.org/abs/2507.22675)
*Meiqi Hu,Lingzhi Lu,Chengxi Han,Xiaoping Liu*

Main category: cs.CV

TL;DR: MergeSAM是一种新的无监督遥感影像变化检测方法，它使用Segment Anything Model (SAM) 和两个新策略（MaskMatching、MaskSplitting）来处理复杂的物体变化，并包含空间结构信息。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是利用深度学习驱动的大模型在特征提取和表示方面的最新进展，来加速无监督变化检测方法，从而提高变化检测技术的实际应用性。特别地，本文旨在解决高分辨率遥感影像中存在的物体分割、合并等复杂变化问题。

Method: MergeSAM是一种基于Segment Anything Model (SAM) 的无监督变化检测方法，针对高分辨率遥感影像设计。该方法提出了MaskMatching和MaskSplitting两个新策略，以处理物体分割、合并等复杂变化，并利用SAM的对象分割能力构建多时相掩膜，捕捉复杂变化，将土地覆盖的空间结构纳入变化检测过程。

Result: MergeSAM方法通过利用SAM的对象分割能力，并结合MaskMatching和MaskSplitting策略，能够有效地捕捉和处理高分辨率遥感影像中的复杂变化，并将土地覆盖的空间结构信息融入变化检测过程。

Conclusion: MergeSAM方法利用SAM强大的分割能力，通过MaskMatching和MaskSplitting策略有效解决了高分辨率遥感影像中的物体分割、合并等复杂变化问题，并嵌入了土地覆盖的空间结构信息，实现了无监督变化检测。

Abstract: Recently, large foundation models trained on vast datasets have demonstrated
exceptional capabilities in feature extraction and general feature
representation. The ongoing advancements in deep learning-driven large models
have shown great promise in accelerating unsupervised change detection methods,
thereby enhancing the practical applicability of change detection technologies.
Building on this progress, this paper introduces MergeSAM, an innovative
unsupervised change detection method for high-resolution remote sensing
imagery, based on the Segment Anything Model (SAM). Two novel strategies,
MaskMatching and MaskSplitting, are designed to address real-world complexities
such as object splitting, merging, and other intricate changes. The proposed
method fully leverages SAM's object segmentation capabilities to construct
multitemporal masks that capture complex changes, embedding the spatial
structure of land cover into the change detection process.

</details>


### [51] [Hydra-Bench: A Benchmark for Multi-Modal Leaf Wetness Sensing](https://arxiv.org/abs/2507.22685)
*Yimeng Liu,Maolin Gan,Yidong Ren,Gen Li,Jingkai Lin,Younsuk Dong,Zhichao Cao*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Leaf wetness detection is a crucial task in agricultural monitoring, as it
directly impacts the prediction and protection of plant diseases. However,
existing sensing systems suffer from limitations in robustness, accuracy, and
environmental resilience when applied to natural leaves under dynamic
real-world conditions. To address these challenges, we introduce a new
multi-modal dataset specifically designed for evaluating and advancing machine
learning algorithms in leaf wetness detection. Our dataset comprises
synchronized mmWave raw data, Synthetic Aperture Radar (SAR) images, and RGB
images collected over six months from five diverse plant species in both
controlled and outdoor field environments. We provide detailed benchmarks using
the Hydra model, including comparisons against single modality baselines and
multiple fusion strategies, as well as performance under varying scan
distances. Additionally, our dataset can serve as a benchmark for future SAR
imaging algorithm optimization, enabling a systematic evaluation of detection
accuracy under diverse conditions.

</details>


### [52] [Zero-Shot Image Anomaly Detection Using Generative Foundation Models](https://arxiv.org/abs/2507.22692)
*Lemar Abdi,Amaan Valiuddin,Francisco Caetano,Christiaan Viviers,Fons van der Sommen*

Main category: cs.CV

TL;DR: 本研究将去噪扩散模型（DDMs）作为通用感知模板，利用其去噪轨迹和SSIM放大的Stein分数误差来检测分布外（OOD）输入。该方法无需针对每个目标数据集重新训练，仅需在CelebA数据集上训练一个基础模型，并在多个设置下取得了优于使用ImageNet等数据集的性能。


<details>
  <summary>Details</summary>
Motivation: 为了在开放世界环境中部署安全的视觉系统，检测分布外（OOD）输入至关重要。本研究旨在探索将基于分数的生成模型作为语义异常检测的基础工具，用于处理未见过的数据集。

Method: 利用去噪扩散模型（DDMs）的去噪轨迹作为纹理和语义信息的丰富来源，并通过分析结构相似性指数度量（SSIM）放大的Stein分数误差来进行异常检测。

Result: 实验结果表明，该方法在某些基准测试中取得了接近完美（near-perfect）的性能，在其他一些测试中也有显著的提升空间，证明了其在异常检测方面的有效性。

Conclusion: 通过分析结构相似性指数度量（SSIM）放大的Stein分数误差，提出了一种在无需对目标数据集进行重新训练的情况下识别异常样本的新方法，该方法在某些基准测试中表现接近完美，在其他基准测试中则有显著提升空间，凸显了生成式基础模型在异常检测方面的潜力和优势。

Abstract: Detecting out-of-distribution (OOD) inputs is pivotal for deploying safe
vision systems in open-world environments. We revisit diffusion models, not as
generators, but as universal perceptual templates for OOD detection. This
research explores the use of score-based generative models as foundational
tools for semantic anomaly detection across unseen datasets. Specifically, we
leverage the denoising trajectories of Denoising Diffusion Models (DDMs) as a
rich source of texture and semantic information. By analyzing Stein score
errors, amplified through the Structural Similarity Index Metric (SSIM), we
introduce a novel method for identifying anomalous samples without requiring
re-training on each target dataset. Our approach improves over state-of-the-art
and relies on training a single model on one dataset -- CelebA -- which we find
to be an effective base distribution, even outperforming more commonly used
datasets like ImageNet in several settings. Experimental results show
near-perfect performance on some benchmarks, with notable headroom on others,
highlighting both the strength and future potential of generative foundation
models in anomaly detection.

</details>


### [53] [Image-Guided Shape-from-Template Using Mesh Inextensibility Constraints](https://arxiv.org/abs/2507.22699)
*Thuy Tran,Ruochen Chen,Shaifali Parashar*

Main category: cs.CV

TL;DR: 一种新的无监督形状模板（SfT）方法，仅使用图像信息（颜色、梯度、轮廓）和网格不变性约束，重建速度快400倍，细节和遮挡处理能力更强。


<details>
  <summary>Details</summary>
Motivation: 传统的SfT方法在图像严重遮挡时性能下降，而现代方法需要大量带标注数据。本研究旨在提出一种无需对应、无监督且高效的SfT方法，以解决这些问题。

Method: 本研究提出了一种无监督的形状模板（SfT）方法，该方法利用颜色特征、梯度和轮廓以及网格不变性约束，无需点对应，即可从图像重建3D形状。

Result: 该方法比现有最佳无监督方法快400倍，在生成精细细节和处理严重遮挡方面表现优于现有方法。

Conclusion: 该研究提出了一种无监督的形状重建方法，该方法仅使用图像观测（颜色特征、梯度和轮廓）以及网格不变性约束，在重建速度上比现有最佳无监督方法快400倍，并且在处理精细细节和严重遮挡方面表现更优。

Abstract: Shape-from-Template (SfT) refers to the class of methods that reconstruct the
3D shape of a deforming object from images/videos using a 3D template.
Traditional SfT methods require point correspondences between images and the
texture of the 3D template in order to reconstruct 3D shapes from images/videos
in real time. Their performance severely degrades when encountered with severe
occlusions in the images because of the unavailability of correspondences. In
contrast, modern SfT methods use a correspondence-free approach by
incorporating deep neural networks to reconstruct 3D objects, thus requiring
huge amounts of data for supervision. Recent advances use a fully unsupervised
or self-supervised approach by combining differentiable physics and graphics to
deform 3D template to match input images. In this paper, we propose an
unsupervised SfT which uses only image observations: color features, gradients
and silhouettes along with a mesh inextensibility constraint to reconstruct at
a $400\times$ faster pace than (best-performing) unsupervised SfT. Moreover,
when it comes to generating finer details and severe occlusions, our method
outperforms the existing methodologies by a large margin. Code is available at
https://github.com/dvttran/nsft.

</details>


### [54] [A Linear N-Point Solver for Structure and Motion from Asynchronous Tracks](https://arxiv.org/abs/2507.22733)
*Hang Su,Yunlong Feng,Daniel Gehrig,Panfeng Jiang,Ling Gao,Xavier Lagorce,Laurent Kneip*

Main category: cs.CV

TL;DR: 提出了一种处理不同步相机（如滚动快门和事件相机）的2D点对应关系的结构和运动估计新方法，该方法可以处理任意时间戳和任意数量的视图，并能融合多种传感器数据，在各种场景下均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的结构和运动估计算法（如5点或8点算法）通常仅限于处理来自单个视图对的点对应关系，而对于滚动快门相机或事件相机等不同步的场景则存在局限性。因此，需要一种能够处理不同步数据的方法。

Method: 提出了一种利用一阶动力学和恒定速度运动模型来处理具有任意时间戳的2D点对应关系的统一方法，推导了一种新颖的线性点关联关系，可以有效地恢复线性速度和3D点，并能预测退化和解的重数。

Result: 该方法在模拟和真实世界数据上都得到了验证，并在所有传感模式下与近期方法相比均显示出一致的改进。

Conclusion: 该方法为来自任意视图集合的具有任意时间戳的2D点对应关系提供了统一的结构和线性运动估计方法，可以处理全局快门、滚动快门和事件相机等多种传感模式，并能融合来自不同同位传感器的对应关系，在模拟和真实世界数据上均显示出优于近期方法的性能，有望实现从异步数据进行高效的结构和运动估计。

Abstract: Structure and continuous motion estimation from point correspondences is a
fundamental problem in computer vision that has been powered by well-known
algorithms such as the familiar 5-point or 8-point algorithm. However, despite
their acclaim, these algorithms are limited to processing point correspondences
originating from a pair of views each one representing an instantaneous capture
of the scene. Yet, in the case of rolling shutter cameras, or more recently,
event cameras, this synchronization breaks down. In this work, we present a
unified approach for structure and linear motion estimation from 2D point
correspondences with arbitrary timestamps, from an arbitrary set of views. By
formulating the problem in terms of first-order dynamics and leveraging a
constant velocity motion model, we derive a novel, linear point incidence
relation allowing for the efficient recovery of both linear velocity and 3D
points with predictable degeneracies and solution multiplicities. Owing to its
general formulation, it can handle correspondences from a wide range of sensing
modalities such as global shutter, rolling shutter, and event cameras, and can
even combine correspondences from different collocated sensors. We validate the
effectiveness of our solver on both simulated and real-world data, where we
show consistent improvement across all modalities when compared to recent
approaches. We believe our work opens the door to efficient structure and
motion estimation from asynchronous data. Code can be found at
https://github.com/suhang99/AsyncTrack-Motion-Solver.

</details>


### [55] [Social-Pose: Enhancing Trajectory Prediction with Human Body Pose](https://arxiv.org/abs/2507.22742)
*Yang Gao,Saeed Saadatnejad,Alexandre Alahi*

Main category: cs.CV

TL;DR: 一种基于人体姿态的轨迹预测方法，通过`Social-pose`编码器捕捉场景中所有人类的姿态及其社会关系，提升了现有轨迹预测模型的性能，并应用于机器人导航。


<details>
  <summary>Details</summary>
Motivation: 现有模型未能充分利用人类在导航空间中潜意识传达的视觉线索，导致在自动驾驶中的人类轨迹预测任务中存在安全隐患。

Method: 提出了一种名为`Social-pose

Result: 在基于LSTM、GAN、MLP和Transformer的先进模型上进行了广泛的实验，结果显示在合成（Joint Track Auto）和真实（Human3.6M, Pedestrians and Cyclists in Road Traffic, and JRDB）数据集上均有所改进。

Conclusion: 通过实验证明，基于姿态的方法在多种轨迹预测模型上均有提升，并在合成及真实数据集上进行了验证，同时探索了2D与3D姿态的优劣以及噪声姿态的影响，并将其应用于机器人导航场景。

Abstract: Accurate human trajectory prediction is one of the most crucial tasks for
autonomous driving, ensuring its safety. Yet, existing models often fail to
fully leverage the visual cues that humans subconsciously communicate when
navigating the space. In this work, we study the benefits of predicting human
trajectories using human body poses instead of solely their Cartesian space
locations in time. We propose `Social-pose', an attention-based pose encoder
that effectively captures the poses of all humans in a scene and their social
relations. Our method can be integrated into various trajectory prediction
architectures. We have conducted extensive experiments on state-of-the-art
models (based on LSTM, GAN, MLP, and Transformer), and showed improvements over
all of them on synthetic (Joint Track Auto) and real (Human3.6M, Pedestrians
and Cyclists in Road Traffic, and JRDB) datasets. We also explored the
advantages of using 2D versus 3D poses, as well as the effect of noisy poses
and the application of our pose-based predictor in robot navigation scenarios.

</details>


### [56] [HOLA: Enhancing Audio-visual Deepfake Detection via Hierarchical Contextual Aggregations and Efficient Pre-training](https://arxiv.org/abs/2507.22781)
*Xuecheng Wu,Danlei Huang,Heli Sun,Xinyi Yin,Yifan Wang,Hao Wang,Jia Zhang,Fei Wang,Peihao Guo,Suyu Xing,Junxiao Xue,Liang He*

Main category: cs.CV

TL;DR: HOLA 是一种用于视频级深度伪影检测的两阶段框架，通过大规模音频-视频自监督预训练、跨模态学习、分层上下文建模和伪监督信号注入来提高性能。


<details>
  <summary>Details</summary>
Motivation: 为了应对生成式 AI 进步带来的视频级深度伪影检测挑战，并克服现有检测技术的局限性。

Method: HOLA 提出了一种包含迭代感知跨模态学习模块、分层上下文建模和类似金字塔的 Refiner 的两阶段框架。此外，还提出了一种伪监督信号注入策略。

Result: HOLA 在视频级深度伪造检测任务中表现出色，在 MLLM 和专家模型上的实验证明了其有效性。

Conclusion: HOLA 在 2025 1M-Deepfakes 检测挑战赛视频级欺骗检测赛道中排名第一，AUC 比第二名高 0.0476。

Abstract: Advances in Generative AI have made video-level deepfake detection
increasingly challenging, exposing the limitations of current detection
techniques. In this paper, we present HOLA, our solution to the Video-Level
Deepfake Detection track of 2025 1M-Deepfakes Detection Challenge. Inspired by
the success of large-scale pre-training in the general domain, we first scale
audio-visual self-supervised pre-training in the multimodal video-level
deepfake detection, which leverages our self-built dataset of 1.81M samples,
thereby leading to a unified two-stage framework. To be specific, HOLA features
an iterative-aware cross-modal learning module for selective audio-visual
interactions, hierarchical contextual modeling with gated aggregations under
the local-global perspective, and a pyramid-like refiner for scale-aware
cross-grained semantic enhancements. Moreover, we propose the pseudo supervised
singal injection strategy to further boost model performance. Extensive
experiments across expert models and MLLMs impressivly demonstrate the
effectiveness of our proposed HOLA. We also conduct a series of ablation
studies to explore the crucial design factors of our introduced components.
Remarkably, our HOLA ranks 1st, outperforming the second by 0.0476 AUC on the
TestA set.

</details>


### [57] [Modality-Aware Feature Matching: A Comprehensive Review of Single- and Cross-Modality Techniques](https://arxiv.org/abs/2507.22791)
*Weide Liu,Wei Zhou,Jun Liu,Ping Hu,Jun Cheng,Jungong Han,Weisi Lin*

Main category: cs.CV

TL;DR: 本文综述了计算机视觉中基于模态的特征匹配技术，重点介绍了深度学习方法如何提升跨不同数据模态（如RGB、深度、3D点云、LiDAR、医学图像、视觉-语言）的匹配性能。


<details>
  <summary>Details</summary>
Motivation: 特征匹配是计算机视觉中的一项基础任务，对于图像检索、立体匹配、三维重建和SLAM等应用至关重要。本次调查旨在全面梳理特征匹配技术的发展，特别关注处理不同数据模态的挑战。

Method: 本文调查了基于模态的特征匹配方法，涵盖了传统手工设计方法和现代深度学习方法，重点分析了不同模态下的具体技术和应用。

Result: 调查涵盖了从SIFT、ORB等传统方法到SuperPoint、LoFTR等基于深度学习方法的演变。研究强调了针对不同模态（如深度图像、3D点云、LiDAR扫描、医学图像）的模态感知进展，并展示了跨模态应用（如医学图像配准和视觉-语言任务）的最新成果。

Conclusion: 深度学习方法在处理跨模态数据交互方面展现出优越的鲁棒性和适应性，推动了特征匹配技术在处理多样化数据（包括RGB图像、深度图像、3D点云、LiDAR扫描、医学图像以及视觉-语言交互）方面的进步。

Abstract: Feature matching is a cornerstone task in computer vision, essential for
applications such as image retrieval, stereo matching, 3D reconstruction, and
SLAM. This survey comprehensively reviews modality-based feature matching,
exploring traditional handcrafted methods and emphasizing contemporary deep
learning approaches across various modalities, including RGB images, depth
images, 3D point clouds, LiDAR scans, medical images, and vision-language
interactions. Traditional methods, leveraging detectors like Harris corners and
descriptors such as SIFT and ORB, demonstrate robustness under moderate
intra-modality variations but struggle with significant modality gaps.
Contemporary deep learning-based methods, exemplified by detector-free
strategies like CNN-based SuperPoint and transformer-based LoFTR, substantially
improve robustness and adaptability across modalities. We highlight
modality-aware advancements, such as geometric and depth-specific descriptors
for depth images, sparse and dense learning methods for 3D point clouds,
attention-enhanced neural networks for LiDAR scans, and specialized solutions
like the MIND descriptor for complex medical image matching. Cross-modal
applications, particularly in medical image registration and vision-language
tasks, underscore the evolution of feature matching to handle increasingly
diverse data interactions.

</details>


### [58] [Segment Anything for Video: A Comprehensive Review of Video Object Segmentation and Tracking from Past to Future](https://arxiv.org/abs/2507.22792)
*Guoping Xu,Jayaram K. Udupa,Yajun Yu,Hua-Chieh Shao,Songlin Zhao,Wei Liu,You Zhang*

Main category: cs.CV

TL;DR: 本次调查对基于SAM/SAM2的视频对象分割和跟踪（VOST）方法进行了全面的回顾，并根据过去、现在和未来的时间维度进行了结构化。它重点介绍了基础模型在处理历史信息、当前帧特征和未来运动预测方面的应用，并讨论了提高准确性和效率的最新进展和挑战。


<details>
  <summary>Details</summary>
Motivation: 视频对象分割和跟踪（VOST）是一个关键但复杂的计算机视觉挑战，传统方法在泛化、时间一致性和效率方面存在不足。基础模型（如SAM和SAM2）的出现为VOST带来了范式转变，能够进行提示驱动的分割和强大的泛化能力。本次调查旨在全面回顾和分析基于SAM/SAM2的VOST方法，以应对这些挑战并指导未来的研究。

Method: 本调查对基于SAM/SAM2的VOST方法进行了全面的回顾，并根据时间维度（过去、现在和未来）进行了结构化。它检查了在过去、现在和未来帧中用于处理对象分割和跟踪的各种策略和技术，重点关注基础模型（如SAM和SAM2）的应用。

Result: 本次调查系统地审查了SAM/SAM2在VOST领域的应用，重点介绍了处理历史信息（过去）、当前帧特征（现在）和未来运动预测（未来）的方法。调查强调了从早期内存架构到SAM2流式内存和实时分割能力的演变，并讨论了运动感知内存选择和轨迹引导提示等近期创新，以提高准确性和效率。

Conclusion: 该调查全面回顾了基于SAM/SAM2的视频对象分割和跟踪（VOST）方法，沿着三个时间维度：过去、现在和未来。 它检查了保留和更新历史信息（过去）、从当前帧提取和优化区分性特征（现在）以及预测后续帧中对象动态的运动预测和轨迹估计机制（未来）的策略。 该调查强调了从早期基于内存的体系结构到SAM2的流式内存和实时分割能力的演变。 它还讨论了诸如运动感知内存选择和轨迹引导提示等近期创新，这些创新旨在提高准确性和效率。 最后，它确定了剩余的挑战，包括内存冗余、错误累积和提示效率低下，并提出了有前景的未来研究方向。该调查旨在通过基础模型来指导研究人员和实践人员在VOST领域取得进展。

Abstract: Video Object Segmentation and Tracking (VOST) presents a complex yet critical
challenge in computer vision, requiring robust integration of segmentation and
tracking across temporally dynamic frames. Traditional methods have struggled
with domain generalization, temporal consistency, and computational efficiency.
The emergence of foundation models like the Segment Anything Model (SAM) and
its successor, SAM2, has introduced a paradigm shift, enabling prompt-driven
segmentation with strong generalization capabilities. Building upon these
advances, this survey provides a comprehensive review of SAM/SAM2-based methods
for VOST, structured along three temporal dimensions: past, present, and
future. We examine strategies for retaining and updating historical information
(past), approaches for extracting and optimizing discriminative features from
the current frame (present), and motion prediction and trajectory estimation
mechanisms for anticipating object dynamics in subsequent frames (future). In
doing so, we highlight the evolution from early memory-based architectures to
the streaming memory and real-time segmentation capabilities of SAM2. We also
discuss recent innovations such as motion-aware memory selection and
trajectory-guided prompting, which aim to enhance both accuracy and efficiency.
Finally, we identify remaining challenges including memory redundancy, error
accumulation, and prompt inefficiency, and suggest promising directions for
future research. This survey offers a timely and structured overview of the
field, aiming to guide researchers and practitioners in advancing the state of
VOST through the lens of foundation models.

</details>


### [59] [Advancing Fetal Ultrasound Image Quality Assessment in Low-Resource Settings](https://arxiv.org/abs/2507.22802)
*Dongli He,Hu Wang,Mohammad Yaqub*

Main category: cs.CV

TL;DR: FetalCLIP$_{CLS}$，一种基于FetalCLIP的视觉语言模型，通过参数高效的微调，在胎儿超声图像质量评估任务上取得了最先进的性能，证明了其在资源有限环境下的潜力。


<details>
  <summary>Details</summary>
Motivation: 在低收入国家，由于缺乏训练有素的专业人员，胎儿生物特征测量（如腹围）的超声图像质量很大程度上取决于声师的专业知识，这带来了重大挑战。

Method: 利用在超过210,000张胎儿超声图像-标题对的集合上预训练的视觉语言模型FetalCLIP，对盲扫超声数据执行自动胎儿超声图像质量评估（IQA）。我们引入了从FetalCLIP改编而来的IQA模型FetalCLIP$_{CLS}$，并使用低秩适配（LoRA）进行微调，然后在ACOUSLIC-AI数据集上与六个CNN和Transformer基线模型进行评估。此外，我们还展示了改编后的分割模型，当用于分类任务时，可以进一步提高性能。

Result: FetalCLIP$_{CLS}$达到了最高的F1分数0.757。此外，改编后的分割模型在用于分类任务时，进一步将性能提高到0.771的F1分数。

Conclusion: 通过参数高效的微调对胎儿超声基础模型进行特定任务改编，可以促进资源有限环境下的产前护理。

Abstract: Accurate fetal biometric measurements, such as abdominal circumference, play
a vital role in prenatal care. However, obtaining high-quality ultrasound
images for these measurements heavily depends on the expertise of sonographers,
posing a significant challenge in low-income countries due to the scarcity of
trained personnel. To address this issue, we leverage FetalCLIP, a
vision-language model pretrained on a curated dataset of over 210,000 fetal
ultrasound image-caption pairs, to perform automated fetal ultrasound image
quality assessment (IQA) on blind-sweep ultrasound data. We introduce
FetalCLIP$_{CLS}$, an IQA model adapted from FetalCLIP using Low-Rank
Adaptation (LoRA), and evaluate it on the ACOUSLIC-AI dataset against six CNN
and Transformer baselines. FetalCLIP$_{CLS}$ achieves the highest F1 score of
0.757. Moreover, we show that an adapted segmentation model, when repurposed
for classification, further improves performance, achieving an F1 score of
0.771. Our work demonstrates how parameter-efficient fine-tuning of fetal
ultrasound foundation models can enable task-specific adaptations, advancing
prenatal care in resource-limited settings. The experimental code is available
at: https://github.com/donglihe-hub/FetalCLIP-IQA.

</details>


### [60] [MoCHA: Advanced Vision-Language Reasoning with MoE Connector and Hierarchical Group Attention](https://arxiv.org/abs/2507.22805)
*Yuqi Pang,Bowen Yang,Yun Cao,Fan Rong,Xiaoyu Li,Chen He*

Main category: cs.CV

TL;DR: MoCHA是一个创新的视觉框架，通过融合多种视觉编码器和采用稀疏专家混合连接器（MoECs）及分层组注意力（HGA），有效解决了现有视觉大型语言模型（VLLMs）的成本高、细节提取难和跨模态衔接差的问题，并在多项任务中取得了优于SOTA开源模型的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉大型语言模型（VLLMs）在处理复杂细粒度视觉信息时，面临训练和推理成本高、视觉细节提取困难以及跨模态衔接效率低等挑战。

Method: MoCHA框架集成了CLIP、SigLIP、DINOv2和ConvNeXt四种视觉骨干网络以提取互补的视觉特征，并使用稀疏专家混合连接器（MoECs）动态选择适合不同视觉维度的专家。为解决视觉信息编码的冗余或不足问题，该框架还设计了包含组内和组间操作的分层组注意力（HGA）以及自适应门控策略。

Result: MoCHA在两个主流大型语言模型（Phi2-2.7B和Vicuna-7B）上进行了训练和评估，并在多项基准测试中取得了优于最先进的开源模型的性能。具体而言，与CuMo（Mistral-7B）相比，MoCHA（Phi2-2.7B）在减少幻觉（POPE提升3.25%）和遵循视觉指令（MME提升153分）方面表现突出。消融实验也进一步证实了MoECs和HGA在提升MoCHA整体性能方面的有效性和鲁棒性。

Conclusion: MoCHA通过集成多种视觉骨干网络和采用稀疏专家混合连接器（MoECs）以及分层组注意力（HGA）和自适应门控策略，有效解决了现有视觉大型语言模型（VLLMs）面临的训练和推理成本高、视觉细节提取和跨模态衔接困难等问题。实验证明，MoCHA在减少幻觉和遵循视觉指令方面表现出色，优于现有的开源模型。

Abstract: Vision large language models (VLLMs) are focusing primarily on handling
complex and fine-grained visual information by incorporating advanced vision
encoders and scaling up visual models. However, these approaches face high
training and inference costs, as well as challenges in extracting visual
details, effectively bridging across modalities. In this work, we propose a
novel visual framework, MoCHA, to address these issues. Our framework
integrates four vision backbones (i.e., CLIP, SigLIP, DINOv2 and ConvNeXt) to
extract complementary visual features and is equipped with a sparse Mixture of
Experts Connectors (MoECs) module to dynamically select experts tailored to
different visual dimensions. To mitigate redundant or insufficient use of the
visual information encoded by the MoECs module, we further design a
Hierarchical Group Attention (HGA) with intra- and inter-group operations and
an adaptive gating strategy for encoded visual features. We train MoCHA on two
mainstream LLMs (e.g., Phi2-2.7B and Vicuna-7B) and evaluate their performance
across various benchmarks. Notably, MoCHA outperforms state-of-the-art
open-weight models on various tasks. For example, compared to CuMo
(Mistral-7B), our MoCHA (Phi2-2.7B) presents outstanding abilities to mitigate
hallucination by showing improvements of 3.25% in POPE and to follow visual
instructions by raising 153 points on MME. Finally, ablation studies further
confirm the effectiveness and robustness of the proposed MoECs and HGA in
improving the overall performance of MoCHA.

</details>


### [61] [DISTIL: Data-Free Inversion of Suspicious Trojan Inputs via Latent Diffusion](https://arxiv.org/abs/2507.22813)
*Hossein Mirzaei,Zeinab Taghavi,Sepehr Rezaee,Masoud Hadi,Moein Madadi,Mackenzie W. Mathis*

Main category: cs.CV

TL;DR: 该研究提出了一种名为DISTIL的无数据、零样本触发器反演方法，用于检测和防御深度神经网络中的后门攻击。该方法利用扩散模型和目标分类器来生成和识别恶意触发器，并在实验中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络虽然在众多任务中取得了显著成功，但仍然容易受到特洛伊木马（后门）攻击，这对其在现实世界中的安全性引起了严重担忧。常见的对策是触发器反演——重建对手在训练期间插入的恶意“捷径”模式（触发器）。

Method: 提出了一种无数据、零样本的触发器反演策略，通过结合由目标分类器引导的基于扩散的生成器，在目标分类器的指导下，通过迭代生成，产生与模型依赖的内部表征一致的候选触发器。

Result: 该方法在BackdoorBench数据集上提高了7.1%的准确率，在目标检测模型扫描上提高了9.4%。

Conclusion: 该方法在BackdoorBench数据集上提高了7.1%的准确率，在目标检测模型扫描上提高了9.4%，为可靠的后门防御提供了一个有前景的新方向，并且不依赖于大量数据或对触发器进行强先验假设。

Abstract: Deep neural networks have demonstrated remarkable success across numerous
tasks, yet they remain vulnerable to Trojan (backdoor) attacks, raising serious
concerns about their safety in real-world mission-critical applications. A
common countermeasure is trigger inversion -- reconstructing malicious
"shortcut" patterns (triggers) inserted by an adversary during training.
Current trigger-inversion methods typically search the full pixel space under
specific assumptions but offer no assurances that the estimated trigger is more
than an adversarial perturbation that flips the model output. Here, we propose
a data-free, zero-shot trigger-inversion strategy that restricts the search
space while avoiding strong assumptions on trigger appearance. Specifically, we
incorporate a diffusion-based generator guided by the target classifier;
through iterative generation, we produce candidate triggers that align with the
internal representations the model relies on for malicious behavior. Empirical
evaluations, both quantitative and qualitative, show that our approach
reconstructs triggers that effectively distinguish clean versus Trojaned
models. DISTIL surpasses alternative methods by high margins, achieving up to
7.1% higher accuracy on the BackdoorBench dataset and a 9.4% improvement on
trojaned object detection model scanning, offering a promising new direction
for reliable backdoor defense without reliance on extensive data or strong
prior assumptions about triggers. The code is available at
https://github.com/AdaptiveMotorControlLab/DISTIL.

</details>


### [62] [Wall Shear Stress Estimation in Abdominal Aortic Aneurysms: Towards Generalisable Neural Surrogate Models](https://arxiv.org/abs/2507.22817)
*Patryk Rygiel,Julian Suk,Christoph Brune,Kak Khee Yeung,Jelmer M. Wolterink*

Main category: cs.CV

TL;DR: 提出了一种基于几何深度学习的方法，用于快速准确地估算腹主动脉瘤（AAA）患者的血流动力学参数，解决了传统CFD模拟的计算瓶颈。


<details>
  <summary>Details</summary>
Motivation: 为了解决计算流体动力学（CFD）模拟在研究腹主动脉瘤（AAA）进展和破裂风险时的计算成本高昂的问题，利用几何深度学习方法作为替代。

Method: 提出了一种E(3)-等变深度学习模型，利用新颖的鲁棒几何描述符和射影几何代数，使用100名AAA患者的CT扫描数据集训练，以估算瞬时壁剪应力（WSS）。

Result: 模型在分布内和外部测试集上都表现出良好的泛化能力，能够准确估算血流动力学参数，并且在不同血管树拓扑结构下表现稳定，对网格分辨率不敏感。

Conclusion: 该模型在几何重塑和边界条件变化方面具有良好的泛化能力，并且可以应用于新的、未见的血管分支，对网格分辨率不敏感，显示出在临床实践中进行血流动力学参数估算的潜力。

Abstract: Abdominal aortic aneurysms (AAAs) are pathologic dilatations of the abdominal
aorta posing a high fatality risk upon rupture. Studying AAA progression and
rupture risk often involves in-silico blood flow modelling with computational
fluid dynamics (CFD) and extraction of hemodynamic factors like time-averaged
wall shear stress (TAWSS) or oscillatory shear index (OSI). However, CFD
simulations are known to be computationally demanding. Hence, in recent years,
geometric deep learning methods, operating directly on 3D shapes, have been
proposed as compelling surrogates, estimating hemodynamic parameters in just a
few seconds. In this work, we propose a geometric deep learning approach to
estimating hemodynamics in AAA patients, and study its generalisability to
common factors of real-world variation. We propose an E(3)-equivariant deep
learning model utilising novel robust geometrical descriptors and projective
geometric algebra. Our model is trained to estimate transient WSS using a
dataset of CT scans of 100 AAA patients, from which lumen geometries are
extracted and reference CFD simulations with varying boundary conditions are
obtained. Results show that the model generalizes well within the distribution,
as well as to the external test set. Moreover, the model can accurately
estimate hemodynamics across geometry remodelling and changes in boundary
conditions. Furthermore, we find that a trained model can be applied to
different artery tree topologies, where new and unseen branches are added
during inference. Finally, we find that the model is to a large extent agnostic
to mesh resolution. These results show the accuracy and generalisation of the
proposed model, and highlight its potential to contribute to hemodynamic
parameter estimation in clinical practice.

</details>


### [63] [Bi-Level Optimization for Self-Supervised AI-Generated Face Detection](https://arxiv.org/abs/2507.22824)
*Mian Zou,Nan Zhong,Baosheng Yu,Yibing Zhan,Kede Ma*

Main category: cs.CV

TL;DR: 一种新的自监督学习方法，通过优化辅助任务权重来提高AI生成人脸检测器的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有AI生成人脸检测器泛化能力受限于特定生成器的问题，提出一种新的自监督方法。

Method: 提出了一种基于双层优化的自监督学习方法。内层循环使用分类EXIF标签、排序EXIF标签和检测图像篡改等辅助任务，在真实人脸图像上预训练视觉编码器。外层循环优化这些辅助任务的权重，以提升对篡改人脸的检测能力，作为识别AI生成人脸的代理任务。预训练后，编码器被固定，并使用高斯混合模型或轻量级感知机来检测AI生成人脸。

Result: 实验证明，该方法在单类和二分类设置下均显著优于现有方法，并能很好地泛化到未知的生成器。

Conclusion: 该方法在区分真实人脸和AI生成人脸方面表现出强大的泛化能力，能够有效识别未知的生成技术所产生的人脸。

Abstract: AI-generated face detectors trained via supervised learning typically rely on
synthesized images from specific generators, limiting their generalization to
emerging generative techniques. To overcome this limitation, we introduce a
self-supervised method based on bi-level optimization. In the inner loop, we
pretrain a vision encoder only on photographic face images using a set of
linearly weighted pretext tasks: classification of categorical exchangeable
image file format (EXIF) tags, ranking of ordinal EXIF tags, and detection of
artificial face manipulations. The outer loop then optimizes the relative
weights of these pretext tasks to enhance the coarse-grained detection of
manipulated faces, serving as a proxy task for identifying AI-generated faces.
In doing so, it aligns self-supervised learning more closely with the ultimate
goal of AI-generated face detection. Once pretrained, the encoder remains
fixed, and AI-generated faces are detected either as anomalies under a Gaussian
mixture model fitted to photographic face features or by a lightweight
two-layer perceptron serving as a binary classifier. Extensive experiments
demonstrate that our detectors significantly outperform existing approaches in
both one-class and binary classification settings, exhibiting strong
generalization to unseen generators.

</details>


### [64] [DepR: Depth Guided Single-view Scene Reconstruction with Instance-level Diffusion](https://arxiv.org/abs/2507.22825)
*Qingcheng Zhao,Xiang Zhang,Haiyang Xu,Zeyuan Chen,Jianwen Xie,Yuan Gao,Zhuowen Tu*

Main category: cs.CV

TL;DR: DepR是一个单视角三维重建框架，它通过组合方式生成单独的对象，并利用深度信息来改进重建过程。


<details>
  <summary>Details</summary>
Motivation: 以往的方法未能充分利用深度信息的丰富几何信息，而DepR框架将深度信息贯穿于训练和推理过程，以实现更好的场景重建。

Method: DepR框架将实例级扩散与组合范式相结合，利用深度信息进行训练和推理，通过深度引导的条件作用来编码形状先验，并指导DDIM采样和布局优化。

Result: DepR在合成和真实世界数据集上的评估均优于现有方法，实现了最先进的性能。

Conclusion: DepR在单视角场景重建方面取得了最先进的性能，并展示了强大的泛化能力，即使在有限的合成数据上训练也是如此。

Abstract: We propose DepR, a depth-guided single-view scene reconstruction framework
that integrates instance-level diffusion within a compositional paradigm.
Instead of reconstructing the entire scene holistically, DepR generates
individual objects and subsequently composes them into a coherent 3D layout.
Unlike previous methods that use depth solely for object layout estimation
during inference and therefore fail to fully exploit its rich geometric
information, DepR leverages depth throughout both training and inference.
Specifically, we introduce depth-guided conditioning to effectively encode
shape priors into diffusion models. During inference, depth further guides DDIM
sampling and layout optimization, enhancing alignment between the
reconstruction and the input image. Despite being trained on limited synthetic
data, DepR achieves state-of-the-art performance and demonstrates strong
generalization in single-view scene reconstruction, as shown through
evaluations on both synthetic and real-world datasets.

</details>


### [65] [ScreenCoder: Advancing Visual-to-Code Generation for Front-End Automation via Modular Multimodal Agents](https://arxiv.org/abs/2507.22827)
*Yilei Jiang,Yaozhi Zheng,Yuxuan Wan,Jiaming Han,Qunzhong Wang,Michael R. Lyu,Xiangyu Yue*

Main category: cs.CV

TL;DR: 提出了一种多代理框架，用于将UI设计转化为前端代码，解决了现有方法在处理视觉布局方面的不足，并通过一个数据引擎生成合成数据来改进模型，在各项指标上均取得了领先的性能。


<details>
  <summary>Details</summary>
Motivation: 为了弥合现有方法仅依赖自然语言提示，在捕捉空间布局和视觉设计意图方面存在局限性的问题，而UI开发实践通常从视觉草图或模型开始，具有多模态的特点。

Method: 提出了一种模块化的多代理框架，通过三个可解释的阶段（映射、规划和生成）来执行UI到代码的生成。映射代理使用视觉-语言模型来检测和标记UI组件，规划代理使用前端工程先验来构建层次布局，生成代理通过自适应的基于提示的合成来生成HTML/CSS代码。此外，该框架被扩展为一个可扩展的数据引擎，可自动生成大规模图像-代码对，用于微调和增强开源视觉-语言模型（VLM），从而提高UI理解和代码质量。

Result: 实验证明，该方法在布局准确性、结构一致性和代码正确性方面达到了最先进的性能。

Conclusion: 该方法在布局准确性、结构一致性和代码正确性方面取得了最先进的性能。

Abstract: Automating the transformation of user interface (UI) designs into front-end
code holds significant promise for accelerating software development and
democratizing design workflows. While recent large language models (LLMs) have
demonstrated progress in text-to-code generation, many existing approaches rely
solely on natural language prompts, limiting their effectiveness in capturing
spatial layout and visual design intent. In contrast, UI development in
practice is inherently multimodal, often starting from visual sketches or
mockups. To address this gap, we introduce a modular multi-agent framework that
performs UI-to-code generation in three interpretable stages: grounding,
planning, and generation. The grounding agent uses a vision-language model to
detect and label UI components, the planning agent constructs a hierarchical
layout using front-end engineering priors, and the generation agent produces
HTML/CSS code via adaptive prompt-based synthesis. This design improves
robustness, interpretability, and fidelity over end-to-end black-box methods.
Furthermore, we extend the framework into a scalable data engine that
automatically produces large-scale image-code pairs. Using these synthetic
examples, we fine-tune and reinforce an open-source VLM, yielding notable gains
in UI understanding and code quality. Extensive experiments demonstrate that
our approach achieves state-of-the-art performance in layout accuracy,
structural coherence, and code correctness. Our code is made publicly available
at https://github.com/leigest519/ScreenCoder.

</details>


### [66] [CapRecover: A Cross-Modality Feature Inversion Attack Framework on Vision Language Models](https://arxiv.org/abs/2507.22828)
*Kedong Xiu,Saiqian Zhang*

Main category: cs.CV

TL;DR: CapRecover框架能从VLM的中间层特征中恢复语义信息（如标签和标题），解决了隐私泄露问题，并且提出了一种无需额外训练的噪声注入方法来防御此类攻击。


<details>
  <summary>Details</summary>
Motivation: 随着视觉-语言模型（VLMs）越来越多地采用分离式DNN配置（即在用户设备上运行视觉编码器，并将中间特征发送到云端），存在由语义信息泄露引起日益增长的隐私风险。

Method: CapRecover是一个跨模态逆向框架，它直接从中间特征中恢复高层语义内容（如标签或标题），而无需进行图像重建。

Result: CapRecover在CIFAR-10上实现了高达92.71%的Top-1标签准确率，并在COCO2017上从ResNet50特征生成了具有高达0.52 ROUGE-L得分的流畅标题。研究还表明，更深的卷积层比浅层编码了更多的语义信息。提出的保护方法可以防止语义泄露，而无需额外的训练成本。

Conclusion: CapRecover是一个跨模态逆向框架，可以直接从中间特征中恢复标签或标题等高层语义内容，而无需进行图像重建。该方法在多个数据集和受害者模型上进行了评估，在语义恢复方面表现强劲。此外，还提出了一种通过在每一层添加和移除随机噪声来减轻语义泄露的保护方法，该方法无需额外训练成本即可有效防止语义泄露。

Abstract: As Vision-Language Models (VLMs) are increasingly deployed in split-DNN
configurations--with visual encoders (e.g., ResNet, ViT) operating on user
devices and sending intermediate features to the cloud--there is a growing
privacy risk from semantic information leakage. Existing approaches to
reconstructing images from these intermediate features often result in blurry,
semantically ambiguous images. To directly address semantic leakage, we propose
CapRecover, a cross-modality inversion framework that recovers high-level
semantic content, such as labels or captions, directly from intermediate
features without image reconstruction.
  We evaluate CapRecover on multiple datasets and victim models, demonstrating
strong performance in semantic recovery. Specifically, CapRecover achieves up
to 92.71% Top-1 label accuracy on CIFAR-10 and generates fluent captions from
ResNet50 features on COCO2017 with ROUGE-L scores up to 0.52. Our analysis
further reveals that deeper convolutional layers encode significantly more
semantic information compared to shallow layers. To mitigate semantic leakage,
we introduce a simple yet effective protection method: adding random noise to
intermediate features at each layer and removing the noise in the next layer.
Experimental results show that this approach prevents semantic leakage without
additional training costs.

</details>


### [67] [TR-PTS: Task-Relevant Parameter and Token Selection for Efficient Tuning](https://arxiv.org/abs/2507.22872)
*Siqi Luo,Haoran Yang,Yi Xin,Mingyang Yi,Guangyang Wu,Guangtao Zhai,Xiaohong Liu*

Main category: cs.CV

TL;DR: TR-PTS是一种任务驱动的框架，通过选择性地微调参数和处理token来提高效率和准确性，在视觉任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有的参数高效微调（PEFT）方法大多是任务无关的，未能充分利用特定任务的调整，导致效率和性能不佳。

Method: TR-PTS框架首先利用Fisher信息矩阵（FIM）以层为单位识别并微调信息量最大的参数，同时冻结其余参数（任务相关参数选择）；然后，动态保留信息量最大的token并合并冗余的token（任务相关token选择）。

Result: TR-PTS在FGVC和VTAB-1k基准测试中达到了最先进的性能，分别超越了完全微调3.40%和10.35%。

Conclusion: TR-PTS通过联合优化参数和token，使模型能够专注于任务判别信息，在FGVC和VTAB-1k基准测试中均达到了最先进的性能，分别超越了完全微调3.40%和10.35%。

Abstract: Large pre-trained models achieve remarkable performance in vision tasks but
are impractical for fine-tuning due to high computational and storage costs.
Parameter-Efficient Fine-Tuning (PEFT) methods mitigate this issue by updating
only a subset of parameters; however, most existing approaches are
task-agnostic, failing to fully exploit task-specific adaptations, which leads
to suboptimal efficiency and performance. To address this limitation, we
propose Task-Relevant Parameter and Token Selection (TR-PTS), a task-driven
framework that enhances both computational efficiency and accuracy.
Specifically, we introduce Task-Relevant Parameter Selection, which utilizes
the Fisher Information Matrix (FIM) to identify and fine-tune only the most
informative parameters in a layer-wise manner, while keeping the remaining
parameters frozen. Simultaneously, Task-Relevant Token Selection dynamically
preserves the most informative tokens and merges redundant ones, reducing
computational overhead. By jointly optimizing parameters and tokens, TR-PTS
enables the model to concentrate on task-discriminative information. We
evaluate TR-PTS on benchmark, including FGVC and VTAB-1k, where it achieves
state-of-the-art performance, surpassing full fine-tuning by 3.40% and 10.35%,
respectively. The code are available at https://github.com/synbol/TR-PTS.

</details>


### [68] [LCS: An AI-based Low-Complexity Scaler for Power-Efficient Super-Resolution of Game Content](https://arxiv.org/abs/2507.22873)
*Simon Pochinda,Momen K. Tageldeen,Mark Thompson,Tony Rinaldi,Troy Giorshev,Keith Lee,Jie Zhou,Frederick Walls*

Main category: cs.CV

TL;DR: 提出了一种名为LCS的AI驱动缩放器，它利用NPU等低功耗设备来降低GPU在游戏渲染中的负载，并在与其他缩放技术的比较中表现出更好的感知质量。


<details>
  <summary>Details</summary>
Motivation: 现代游戏中日益复杂的內容渲染导致GPU工作负载成问题地增长。

Method: 提出了一种受最先进的超分辨率（ESR）模型启发的AI低复杂度缩放器（LCS），它可以使用神经处理单元（NPU）等低功耗设备来分担GPU的工作负载。LCS在游戏IR图像对上进行训练，这些图像对是以低分辨率和高分辨率原生渲染的。利用对抗性训练来鼓励重建感知上重要的细节，并应用再参数化和量化技术来降低模型复杂度和大小。

Result: 与公开的AMD硬件边缘自适应缩放功能（EASF）和AMD FidelityFX超分辨率1（FSR1）进行了比较分析，在五个不同的指标上对LCS进行了评估。

Conclusion: LCS实现了比EASF和FSR1更好的感知质量，展示了ESR模型在资源受限设备上进行超分辨率显示的潜力。

Abstract: The increasing complexity of content rendering in modern games has led to a
problematic growth in the workload of the GPU. In this paper, we propose an
AI-based low-complexity scaler (LCS) inspired by state-of-the-art efficient
super-resolution (ESR) models which could offload the workload on the GPU to a
low-power device such as a neural processing unit (NPU). The LCS is trained on
GameIR image pairs natively rendered at low and high resolution. We utilize
adversarial training to encourage reconstruction of perceptually important
details, and apply reparameterization and quantization techniques to reduce
model complexity and size. In our comparative analysis we evaluate the LCS
alongside the publicly available AMD hardware-based Edge Adaptive Scaling
Function (EASF) and AMD FidelityFX Super Resolution 1 (FSR1) on five different
metrics, and find that the LCS achieves better perceptual quality,
demonstrating the potential of ESR models for upscaling on resource-constrained
devices.

</details>


### [69] [Towards Omnimodal Expressions and Reasoning in Referring Audio-Visual Segmentation](https://arxiv.org/abs/2507.22886)
*Kaining Ying,Henghui Ding,Guanquan Jie,Yu-Gang Jiang*

Main category: cs.CV

TL;DR: 提出OmniAVS数据集和OISA模型，用于视听内容的多模态理解和推理分割。OISA模型利用MLLM处理复杂线索和推理，在OmniAVS及相关任务上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的Referring audio-visual segmentation（RAVS）在整合多模态信息以及深入理解和推理视听内容方面仍存在挑战。为了扩展RAVS的边界并促进该领域的未来研究，需要新的数据集和更强大的模型。

Method: 提出Omnimodal Referring Audio-Visual Segmentation (OmniAVS)数据集，包含2,098个视频和59,458个多模态指代表达式，以及Omnimodal Instructed Segmentation Assistant (OISA)模型，该模型使用多模态大语言模型（MLLM）来理解复杂线索并执行基于推理的分割。

Result: OISA在OmniAVS数据集上实现了优于现有方法的性能，并在其他相关任务上也取得了有竞争力的结果。

Conclusion: OISA在OmniAVS数据集上表现优于现有方法，并在其他相关任务上也取得了有竞争力的结果。

Abstract: Referring audio-visual segmentation (RAVS) has recently seen significant
advancements, yet challenges remain in integrating multimodal information and
deeply understanding and reasoning about audiovisual content. To extend the
boundaries of RAVS and facilitate future research in this field, we propose
Omnimodal Referring Audio-Visual Segmentation (OmniAVS), a new dataset
containing 2,098 videos and 59,458 multimodal referring expressions. OmniAVS
stands out with three key innovations: (1) 8 types of multimodal expressions
that flexibly combine text, speech, sound, and visual cues; (2) an emphasis on
understanding audio content beyond just detecting their presence; and (3) the
inclusion of complex reasoning and world knowledge in expressions. Furthermore,
we introduce Omnimodal Instructed Segmentation Assistant (OISA), to address the
challenges of multimodal reasoning and fine-grained understanding of
audiovisual content in OmniAVS. OISA uses MLLM to comprehend complex cues and
perform reasoning-based segmentation. Extensive experiments show that OISA
outperforms existing methods on OmniAVS and achieves competitive results on
other related tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [70] [IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian](https://arxiv.org/abs/2507.22159)
*Vanessa Rebecca Wiyono,David Anugraha,Ayu Purwarianti,Genta Indra Winata*

Main category: cs.CL

TL;DR: LLM在印度尼西亚语方面的研究不足，现有数据集缺乏真实性。本研究提出了IndoPref，这是第一个完全由人类撰写的印度尼西亚语偏好数据集，用于评估LLM的生成质量。


<details>
  <summary>Details</summary>
Motivation: 现有针对印度尼西亚语的多语言数据集多源于英语翻译，缺乏文化和语言的真实性，而印度尼西亚语在LLM研究中代表性不足。

Method: 通过 Krippendorff's alpha 评估，确保了标注者之间的一致性。

Result: 对多个LLM进行了基准测试，并评估了每个模型的输出质量。

Conclusion: IndoPref是第一个完全由人类撰写、跨越多领域、旨在评估LLM生成文本的自然度和质量的印度尼西亚语偏好数据集。

Abstract: Over 200 million people speak Indonesian, yet the language remains
significantly underrepresented in preference-based research for large language
models (LLMs). Most existing multilingual datasets are derived from English
translations, often resulting in content that lacks cultural and linguistic
authenticity. To address this gap, we introduce IndoPref, the first fully
human-authored and multi-domain Indonesian preference dataset specifically
designed to evaluate the naturalness and quality of LLM-generated text. All
annotations are natively written in Indonesian and evaluated using
Krippendorff's alpha, demonstrating strong inter-annotator agreement.
Additionally, we benchmark the dataset across multiple LLMs and assess the
output quality of each model.

</details>


### [71] [Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles](https://arxiv.org/abs/2507.22168)
*Kimberly Le Truong,Riccardo Fogliato,Hoda Heidari,Zhiwei Steven Wu*

Main category: cs.CL

TL;DR: LLM的性能会受到提示写作风格的影响，即使其语义内容相同。通过重写提示来模拟不同的写作风格，可以提高评估基准的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM评估基准在写作风格多样性方面存在不足，可能导致LLM在面对非标准化输入时表现出脆弱性。

Method: 使用基于个性的LLM提示重写评估提示，以模拟不同的写作风格。

Result: 即使在语义内容相同的情况下，写作风格和提示格式的变化也会显著影响LLM的估计性能。研究人员识别出在不同模型和任务中持续触发低或高性能的不同写作风格。

Conclusion: 通过使用基于个性的LLM提示（一种低成本的模拟不同写作风格的方法）来重写评估提示，可以提高评估LLM性能的基准测试的说服力。研究结果表明，即使语义内容相同，写作风格和提示格式的变化也会显著影响LLM的估计性能。

Abstract: Current benchmarks for evaluating Large Language Models (LLMs) often do not
exhibit enough writing style diversity, with many adhering primarily to
standardized conventions. Such benchmarks do not fully capture the rich variety
of communication patterns exhibited by humans. Thus, it is possible that LLMs,
which are optimized on these benchmarks, may demonstrate brittle performance
when faced with "non-standard" input. In this work, we test this hypothesis by
rewriting evaluation prompts using persona-based LLM prompting, a low-cost
method to emulate diverse writing styles. Our results show that, even with
identical semantic content, variations in writing style and prompt formatting
significantly impact the estimated performance of the LLM under evaluation.
Notably, we identify distinct writing styles that consistently trigger either
low or high performance across a range of models and tasks, irrespective of
model family, size, and recency. Our work offers a scalable approach to augment
existing benchmarks, improving the external validity of the assessments they
provide for measuring LLM performance across linguistic variations.

</details>


### [72] [A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models](https://arxiv.org/abs/2507.22187)
*Adam M. Morgan,Adeen Flinker*

Main category: cs.CL

TL;DR: 使用LLM自动化句法分析以快速、可扩展地估计动词框架频率（VFFs）。


<details>
  <summary>Details</summary>
Motivation: 现有的计算VFFs的工具存在规模、准确性或可及性方面的限制，而VFFs能为了解人类和机器语言系统中的句法提供有力的途径。

Method: 使用大型语言模型（LLMs）生成包含476个英语动词的句子语料库，并指示LLM扮演专家语言学家的角色来分析句子的句法结构。

Result: 该流程优于两种广泛使用的句法分析器，并且所需的资源远少于手动分析，从而实现了快速、可扩展的VFF估计，并生成了一个新的、覆盖范围更广、句法区分更细致、并包含心理语言学中常见结构替代的相对频率的VFF数据库。

Conclusion: 该流程是一个自动化框架，用于估计动词框架频率（VFFs），并能进行快速、可扩展的VFFs估计，可用于新的动词、句法框架，甚至是其他语言，可用于支持未来的研究。

Abstract: We present an automated pipeline for estimating Verb Frame Frequencies
(VFFs), the frequency with which a verb appears in particular syntactic frames.
VFFs provide a powerful window into syntax in both human and machine language
systems, but existing tools for calculating them are limited in scale,
accuracy, or accessibility. We use large language models (LLMs) to generate a
corpus of sentences containing 476 English verbs. Next, by instructing an LLM
to behave like an expert linguist, we had it analyze the syntactic structure of
the sentences in this corpus. This pipeline outperforms two widely used
syntactic parsers across multiple evaluation datasets. Furthermore, it requires
far fewer resources than manual parsing (the gold-standard), thereby enabling
rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF
database with broader verb coverage, finer-grained syntactic distinctions, and
explicit estimates of the relative frequencies of structural alternates
commonly studied in psycholinguistics. The pipeline is easily customizable and
extensible to new verbs, syntactic frames, and even other languages. We present
this work as a proof of concept for automated frame frequency estimation, and
release all code and data to support future research.

</details>


### [73] [The role of media memorability in facilitating startups' access to venture capital funding](https://arxiv.org/abs/2507.22201)
*L. Toschi,S. Torrisi,A. Fronzetti Colladon*

Main category: cs.CL

TL;DR: 媒体声誉对吸引风险投资至关重要，但风险投资家不仅看重媒体的广泛曝光，更看重媒体报道的独特性和在行业内的连接性（即媒体记忆度）。


<details>
  <summary>Details</summary>
Motivation: 先前的研究过于狭隘地关注一般的媒体曝光，限制了我们对媒体如何真正影响融资决策的理解。我们引入了媒体记忆度的概念——媒体在相关投资者记忆中留下创业公司名字的能力。

Method: 通过分析1995年至2004年间在微纳米技术领域获得资助的197家英国创业公司的数据，我们表明媒体记忆度显著影响投资结果。

Result: 研究结果表明，风险投资家依赖于创业公司的独特性和在新闻语义网络中的连接性等详细线索。

Conclusion: 创业公司应超越频繁的媒体提及，通过更有针对性、更有意义的报道来加强品牌记忆点，突出其独特性和在整个行业对话中的相关性。

Abstract: Media reputation plays an important role in attracting venture capital
investment. However, prior research has focused too narrowly on general media
exposure, limiting our understanding of how media truly influences funding
decisions. As informed decision-makers, venture capitalists respond to more
nuanced aspects of media content. We introduce the concept of media
memorability - the media's ability to imprint a startup's name in the memory of
relevant investors. Using data from 197 UK startups in the micro and
nanotechnology sector (funded between 1995 and 2004), we show that media
memorability significantly influences investment outcomes. Our findings suggest
that venture capitalists rely on detailed cues such as a startup's
distinctiveness and connectivity within news semantic networks. This
contributes to research on entrepreneurial finance and media legitimation. In
practice, startups should go beyond frequent media mentions to strengthen brand
memorability through more targeted, meaningful coverage highlighting their
uniqueness and relevance within the broader industry conversation.

</details>


### [74] [How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?](https://arxiv.org/abs/2507.22209)
*Christian Clark,Byung-Doh Oh,William Schuler*

Main category: cs.CL

TL;DR: 熵是一种心理语言学指标，用于衡量单词在遇到之前预期的处理难度。熵的估计通常基于语言模型对单词第一个子词 token 的概率分布。然而，这种方法会导致低估和潜在失真。本研究生成了词熵的蒙特卡洛（MC）估计，允许单词跨越可变的 token 数量，并且发现与仅使用第一个 token 的估计相比，这种方法在阅读时间回归实验中产生了不同的结果，表明在使用第一个 token 近似熵时需要谨慎。


<details>
  <summary>Details</summary>
Motivation: 为了解决当前使用语言模型对词的第一个子词 token 的概率分布进行估计所导致的低估和潜在失真问题，提出生成词熵的蒙特卡洛（MC）估计。

Method: 生成词熵的蒙特卡洛（MC）估计，允许词跨越可变的 token 数量。

Result: 基于 first-token 的熵估计和基于蒙特卡洛（MC）估计的词熵在阅读时间回归实验中显示出不同的结果。

Conclusion: “基于 first-token 的熵估计”和“基于蒙特卡洛（MC）估计的词熵”在阅读时间回归实验中显示出不同的结果，这表明在使用“first-token”近似熵时需要谨慎。

Abstract: Contextual entropy is a psycholinguistic measure capturing the anticipated
difficulty of processing a word just before it is encountered. Recent studies
have tested for entropy-related effects as a potential complement to well-known
effects from surprisal. For convenience, entropy is typically estimated based
on a language model's probability distribution over a word's first subword
token. However, this approximation results in underestimation and potential
distortion of true word entropy. To address this, we generate Monte Carlo (MC)
estimates of word entropy that allow words to span a variable number of tokens.
Regression experiments on reading times show divergent results between
first-token and MC word entropy, suggesting a need for caution in using
first-token approximations of contextual entropy.

</details>


### [75] [RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation](https://arxiv.org/abs/2507.22219)
*Dongyub Jude Lee,Zhenyi Ye,Pengcheng He*

Main category: cs.CL

TL;DR: RLfR是一种新的机器翻译框架，通过GPT-4o的持续反馈进行学习，无需静态三元组数据，并在跨语言翻译任务中提高了翻译质量和实体保持能力。


<details>
  <summary>Details</summary>
Motivation: 现有的基于偏好的机器翻译（MT）方法（如DPO）虽然效果显著，但高度依赖大型、精心策划的三元组数据集，并且在泛化到其调优领域之外时常常遇到困难。

Method: RLfR框架将翻译的每一步视为一个微教程：由actor生成假设，由教师模型（GPT-4o）进行优化，actor根据其与教师优化的一致性获得奖励。该框架结合了负编辑距离（保证词汇和结构保真度）和COMET分数（保证语义充分性）两个互补信号，使actor能够逐步学习并模仿教师模型。

Result: 在FLORES-200基准测试（包括英译德、英译西、英译中、英译韩和英译日）中，RLfR框架持续优于MT-SFT和基于偏好的基线模型，显著提高了COMET（语义充分性）和M-ETA（实体保持）分数。

Conclusion: RLfR框架通过利用外部教师模型（GPT-4o）的持续、高质量反馈，消除了对静态三元组数据集的依赖，并在FLORES-200基准测试中取得了优于现有方法的成果，显著提高了COMET（语义充分性）和M-ETA（实体保持）分数。

Abstract: Preference-learning methods for machine translation (MT)--such as Direct
Preference Optimization (DPO)--have achieved impressive gains but depend
heavily on large, carefully curated triplet datasets and often struggle to
generalize beyond their tuning domains. We propose Reinforcement Learning from
Teacher-Model Refinement (RLfR), a novel framework that removes reliance on
static triplets by leveraging continuous, high-quality feedback from an
external teacher model (GPT-4o). RLfR frames each translation step as a
micro-tutorial: the actor generates a hypothesis, the teacher refines it, and
the actor is rewarded based on how closely it aligns with the teacher's
refinement. Guided by two complementary signals--(i) negative edit distance,
promoting lexical and structural fidelity, and (ii) COMET score, ensuring
semantic adequacy--the actor progressively learns to emulate the teacher,
mirroring a human learning process through incremental, iterative improvement.
On the FLORES-200 benchmark (English to and from German, Spanish, Chinese,
Korean, and Japanese), RLfR consistently outperforms both MT-SFT and
preference-based baselines, significantly improving COMET (semantic adequacy)
and M-ETA (entity preservation) scores.

</details>


### [76] [Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs](https://arxiv.org/abs/2507.22286)
*Supantho Rakshit,Adele Goldberg*

Main category: cs.CL

TL;DR: LLMs show graded, meaning-infused representations of language constructions, supporting usage-based constructionist principles.


<details>
  <summary>Details</summary>
Motivation: Investigated whether the internal representations in LLMs reflect the function-infused gradience proposed by the usage-based constructionist (UCx) approach, which posits language comprises a network of learned form-meaning pairings (constructions) whose use is determined by their meanings or functions.

Method: Analyzed neural representations of English dative constructions in Pythia-1.4B using a dataset of 5000 sentence pairs systematically varied for human-rated preference strength. Employed macro-level geometric analysis with Energy Distance and Jensen-Shannon Divergence to measure the separability between construction representations.

Result: The separability between construction representations was systematically modulated by gradient preference strength, with more prototypical exemplars occupying more distinct regions in the LLMs' activation space.

Conclusion: LLMs learned rich, meaning-infused, graded representations of constructions, supporting geometric measures of basic constructionist principles.

Abstract: The usage-based constructionist (UCx) approach posits that language comprises
a network of learned form-meaning pairings (constructions) whose use is largely
determined by their meanings or functions, requiring them to be graded and
probabilistic. This study investigates whether the internal representations in
Large Language Models (LLMs) reflect the proposed function-infused gradience.
We analyze the neural representations of the English dative constructions
(Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of
$5000$ sentence pairs systematically varied for human-rated preference
strength. A macro-level geometric analysis finds that the separability between
construction representations, as measured by Energy Distance or Jensen-Shannon
Divergence, is systematically modulated by gradient preference strength. More
prototypical exemplars of each construction occupy more distinct regions in the
activation space of LLMs. These results provide strong evidence that LLMs learn
rich, meaning-infused, graded representations of constructions and offer
support for geometric measures of basic constructionist principles in LLMs.

</details>


### [77] [Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations](https://arxiv.org/abs/2507.22289)
*Galo Castillo-López,Gaël de Chalendar,Nasredine Semmar*

Main category: cs.CL

TL;DR: 利用 BERT 和 LLMs 的混合方法，在数据有限的情况下，提高了对话系统中用户意图识别和 OOS 检测的能力。


<details>
  <summary>Details</summary>
Motivation: 传统的 TODS 需要大量标注数据，而本研究旨在解决此问题，通过零样本和少样本学习来识别用户意图和检测 OOS 语句。

Method: 提出一种混合方法，结合 BERT 和 LLMs 的能力，用于零样本和少样本的意图识别和 OOS 检测。该方法利用 LLMs 的泛化能力和 BERT 的计算效率，并通过共享 BERT 的输出来改进 LLMs 的性能。

Result: 在多方对话语料库上的评估表明，共享 BERT 输出信息给 LLMs 能够提升系统性能。

Conclusion: 该方法通过结合 BERT 和 LLMs 的优势，在零样本和少样本场景下提高了意图识别和 OOS 检测的性能。

Abstract: Intent recognition is a fundamental component in task-oriented dialogue
systems (TODS). Determining user intents and detecting whether an intent is
Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However,
traditional TODS require large amount of annotated data. In this work we
propose a hybrid approach to combine BERT and LLMs in zero and few-shot
settings to recognize intents and detect OOS utterances. Our approach leverages
LLMs generalization power and BERT's computational efficiency in such
scenarios. We evaluate our method on multi-party conversation corpora and
observe that sharing information from BERT outputs to LLMs leads to system
performance improvement.

</details>


### [78] [A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers](https://arxiv.org/abs/2507.22337)
*Roxana Petcu,Samarth Bhargav,Maarten de Rijke,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: 一项关于提高神经信息检索和LLM模型处理否定词查询能力的研究，通过提出分类法、新数据集和分析机制来改进模型性能。


<details>
  <summary>Details</summary>
Motivation: 为了理解并解决用户复杂推理任务中的信息需求，特别是神经模型在处理包含否定词的查询时表现不佳的现象。

Method: 1. 提出一个源于哲学、语言学和逻辑学定义的否定词分类法。 2. 生成两个可用于评估神经信息检索模型在否定词处理方面性能的新基准数据集，并用于微调模型以获得更鲁棒的性能。 3. 提出一个基于逻辑的分类机制，用于分析检索模型在现有数据集上的性能。

Result: 提出的分类法能够实现否定词类型数据的平衡分布，为模型提供更好的训练设置，从而在NevIR数据集上实现更快的收敛。此外，提出的分类模式揭示了现有数据集中否定词类型的覆盖范围，并揭示了可能影响微调模型在否定词处理方面泛化能力的因素。

Conclusion: 研究了传统神经信息检索和基于LLM的模型中的否定词处理问题，并提出了一个包含否定词的分类法，两个新的基准数据集，以及一个用于分析检索模型在否定词处理方面表现的基于逻辑的分类机制。

Abstract: Understanding and solving complex reasoning tasks is vital for addressing the
information needs of a user. Although dense neural models learn contextualised
embeddings, they still underperform on queries containing negation. To
understand this phenomenon, we study negation in both traditional neural
information retrieval and LLM-based models. We (1) introduce a taxonomy of
negation that derives from philosophical, linguistic, and logical definitions;
(2) generate two benchmark datasets that can be used to evaluate the
performance of neural information retrieval models and to fine-tune models for
a more robust performance on negation; and (3) propose a logic-based
classification mechanism that can be used to analyze the performance of
retrieval models on existing datasets. Our taxonomy produces a balanced data
distribution over negation types, providing a better training setup that leads
to faster convergence on the NevIR dataset. Moreover, we propose a
classification schema that reveals the coverage of negation types in existing
datasets, offering insights into the factors that might affect the
generalization of fine-tuned models on negation.

</details>


### [79] [Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors](https://arxiv.org/abs/2507.22367)
*Jia Li,Yichao He,Jiacheng Xu,Tianhao Luo,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

TL;DR: Traits Run Deep：利用心理学提示和文本中心融合网络，在跨模态个性评估中取得突破性进展，MSE降低45%，并在AVI挑战赛2025中获得第一名。


<details>
  <summary>Details</summary>
Motivation: 传统方法难以用肤浅的特征对个性语义进行建模，并且实现有效的跨模态理解似乎是不可能的。为了解决这些挑战，即模态之间异步模式的挑战，需要更有效的方法来建模个性语义。

Method: 提出了一种名为Traits Run Deep的新型个性评估框架。该框架采用了心理学启发的提示来提取高层级的个性相关语义表示，并设计了一个文本中心特征融合网络，将文本语义与来自其他模态（如音频、视觉）的异步信号对齐和整合。该融合模块包含一个用于降维的块状投影器、一个用于有效模态融合的跨模态连接器和文本特征增强器，以及一个用于提高数据稀疏情况下泛化能力的集成回归头。

Result: 在AVI验证集上的实验结果表明，所提出的组件（特别是心理学启发的提示和文本中心特征融合网络）的有效性，平均而言，均方误差（MSE）降低了约45%。在AVI挑战赛2025的测试集上的最终评估证实了该方法的优越性，在个性评估赛道上排名第一。

Conclusion: 该研究提出的Traits Run Deep框架通过心理学启发的提示和文本中心特征融合网络，有效解决了跨模态个性评估中的挑战，并在AVI挑战赛2025的个性评估赛道上取得了第一名的成绩。

Abstract: Accurate and reliable personality assessment plays a vital role in many
fields, such as emotional intelligence, mental health diagnostics, and
personalized education. Unlike fleeting emotions, personality traits are
stable, often subconsciously leaked through language, facial expressions, and
body behaviors, with asynchronous patterns across modalities. It was hard to
model personality semantics with traditional superficial features and seemed
impossible to achieve effective cross-modal understanding. To address these
challenges, we propose a novel personality assessment framework called
\textit{\textbf{Traits Run Deep}}. It employs
\textit{\textbf{psychology-informed prompts}} to elicit high-level
personality-relevant semantic representations. Besides, it devises a
\textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text
semantics to align and integrate asynchronous signals from other modalities. To
be specific, such fusion module includes a Chunk-Wise Projector to decrease
dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for
effective modality fusion and an ensemble regression head to improve
generalization in data-scarce situations. To our knowledge, we are the first to
apply personality-specific prompts to guide large language models (LLMs) in
extracting personality-aware semantics for improved representation quality.
Furthermore, extracting and fusing audio-visual apparent behavior features
further improves the accuracy. Experimental results on the AVI validation set
have demonstrated the effectiveness of the proposed components, i.e.,
approximately a 45\% reduction in mean squared error (MSE). Final evaluations
on the test set of the AVI Challenge 2025 confirm our method's superiority,
ranking first in the Personality Assessment track. The source code will be made
available at https://github.com/MSA-LMC/TraitsRunDeep.

</details>


### [80] [PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs](https://arxiv.org/abs/2507.22387)
*Homaira Huda Shomee,Suman Kalyan Maity,Sourav Medya*

Main category: cs.CL

TL;DR: 本研究提出了PATENTWRITER基准框架，评估了包括GPT-4和LLaMA-3在内的六种LLM在专利摘要生成任务上的表现。结果显示，LLM生成的摘要在质量、风格和下游任务表现上均优于传统方法，代码和数据集已开源。


<details>
  <summary>Details</summary>
Motivation: 为了克服繁琐的专利申请流程，并推动专利写作领域的范式转变，利用大型语言模型（LLM）来简化和改进专利摘要的生成过程。

Method: 提出了一种名为PATENTWRITER的统一基准框架，用于评估LLM在专利摘要生成方面的能力。该框架在一致的设置下，利用零样本、少样本和思维链提示策略，对包括GPT-4和LLaMA-3在内的六种主流LLM进行了评估，任务是根据专利的第一项权利要求生成摘要。评估不仅包括标准的NLP指标（如BLEU、ROUGE、BERTScore），还包括在三种输入扰动下的鲁棒性评估，以及在两个下游任务（专利分类和检索）中的适用性评估。此外，还进行了风格分析，评估长度、可读性和语气。

Result: 实验结果表明，现代LLM在生成专利摘要方面表现出色，能够生成高保真且风格合适的摘要，并且在多个评估维度上优于领域特定的基线模型。

Conclusion: 现代LLM能够生成高保真且风格恰当的专利摘要，在许多情况下优于领域特定的基线模型。研究代码和数据集已开源，以支持可复现性和未来研究。

Abstract: Large language models (LLMs) have emerged as transformative approaches in
several important fields. This paper aims for a paradigm shift for patent
writing by leveraging LLMs to overcome the tedious patent-filing process. In
this work, we present PATENTWRITER, the first unified benchmarking framework
for evaluating LLMs in patent abstract generation. Given the first claim of a
patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a
consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting
strategies to generate the abstract of the patent. Our benchmark PATENTWRITER
goes beyond surface-level evaluation: we systematically assess the output
quality using a comprehensive suite of metrics -- standard NLP measures (e.g.,
BLEU, ROUGE, BERTScore), robustness under three types of input perturbations,
and applicability in two downstream patent classification and retrieval tasks.
We also conduct stylistic analysis to assess length, readability, and tone.
Experimental results show that modern LLMs can generate high-fidelity and
stylistically appropriate patent abstracts, often surpassing domain-specific
baselines. Our code and dataset are open-sourced to support reproducibility and
future research.

</details>


### [81] [Question Generation for Assessing Early Literacy Reading Comprehension](https://arxiv.org/abs/2507.22410)
*Xiaocheng Yang,Sumuk Shashidhar,Dilek Hakkani-Tur*

Main category: cs.CL

TL;DR: 提出了一种新颖的方法，用于生成针对K-2英语学习者的理解问题，确保内容覆盖和能力适应性，并能生成多种问题类型和难度级别，旨在成为自主AI英语教师的一部分。


<details>
  <summary>Details</summary>
Motivation: 评估通过基于内容的互动进行阅读理解在阅读习得过程中起着重要作用。

Method: 提出了一种新颖的方法，用于生成针对K-2英语学习者的理解问题。该方法确保对基础材料的完整覆盖，并能适应学习者的具体熟练程度，可以生成各种难度级别的大量不同类型的问题，以确保彻底评估。

Result: 我们使用 FairytaleQA 数据集作为源材料，在我们的框架中评估了各种语言模型的性能。

Conclusion: 该方法有潜力成为自主人工智能驱动的英语教师的重要组成部分。

Abstract: Assessment of reading comprehension through content-based interactions plays
an important role in the reading acquisition process. In this paper, we propose
a novel approach for generating comprehension questions geared to K-2 English
learners. Our method ensures complete coverage of the underlying material and
adaptation to the learner's specific proficiencies, and can generate a large
diversity of question types at various difficulty levels to ensure a thorough
evaluation. We evaluate the performance of various language models in this
framework using the FairytaleQA dataset as the source material. Eventually, the
proposed approach has the potential to become an important part of autonomous
AI-driven English instructors.

</details>


### [82] [NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models](https://arxiv.org/abs/2507.22411)
*Hyeonseok Moon,Heuiseok Lim*

Main category: cs.CL

TL;DR: 现有的 LLM 长期上下文评估方法可能存在偏差。本研究提出了新的 NeedleChain 基准和 ROPE Contraction 策略，以更准确地评估和提升 LLM 的长期上下文理解能力。


<details>
  <summary>Details</summary>
Motivation: 为了更准确地评估大型语言模型（LLM）的长期（LC）上下文理解能力，并解决现有 Needle-in-a-Haystack (NIAH) 基准测试可能高估 LLM 真实能力的问题。

Method: 本研究通过 NeedleChain 基准测试和 ROPE Contraction 策略来评估和改进 LLM 的长期理解能力。NeedleChain 基准测试的上下文完全由查询相关信息组成，并允许灵活的上下文长度和推理顺序。ROPE Contraction 是一种改进 LC 理解能力的策略。

Result: 研究发现，即使是像 GPT-4o 这样的先进模型，在处理完全由查询相关信息组成的上下文时也存在困难。实验表明，LLM 在处理大上下文和完全理解大上下文之间存在显著差异。

Conclusion: 该研究表明，现有的 Needle-in-a-Haystack (NIAH) 评估方法可能高估了大型语言模型（LLM）的长期（LC）理解能力。研究引入了一个新的基准测试 NeedleChain，其中上下文完全由查询相关信息组成，并提出了一种名为 ROPE Contraction 的新策略来提高 LLM 的 LC 理解能力。

Abstract: The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large
Language Models' (LLMs) ability to understand long contexts (LC). It evaluates
the capability to identify query-relevant context within extensive
query-irrelevant passages. Although this method serves as a widely accepted
standard for evaluating long-context understanding, our findings suggest it may
overestimate the true LC capability of LLMs. We demonstrate that even
state-of-the-art models such as GPT-4o struggle to intactly incorporate given
contexts made up of solely query-relevant ten sentences. In response, we
introduce a novel benchmark, \textbf{NeedleChain}, where the context consists
entirely of query-relevant information, requiring the LLM to fully grasp the
input to answer correctly. Our benchmark allows for flexible context length and
reasoning order, offering a more comprehensive analysis of LLM performance.
Additionally, we propose an extremely simple yet compelling strategy to improve
LC understanding capability of LLM: ROPE Contraction. Our experiments with
various advanced LLMs reveal a notable disparity between their ability to
process large contexts and their capacity to fully understand them. Source code
and datasets are available at https://github.com/hyeonseokk/NeedleChain

</details>


### [83] [AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini](https://arxiv.org/abs/2507.22445)
*Jill Walker Rettberg,Hermann Wigers*

Main category: cs.CL

TL;DR: AI语言模型生成的故事存在文化同质化问题，忽视了现实冲突、爱情和叙事张力，倾向于稳定、传统和和解，这是一种新的AI叙事偏见。


<details>
  <summary>Details</summary>
Motivation: 探究在主要基于英美文本训练的语言模型是否能生成具有文化相关性的故事，但这些故事面向其他国家/地区。

Method: 生成了11,800个故事，为236个国家/地区各生成50个故事，方法是向OpenAI的模型gpt-4o-mini发送提示“写一个1500字的潜在{demonym}故事”。

Result: 尽管故事包含表面的国家象征和主题，但它们在各国之间压倒性地遵循单一叙事情节结构：主角居住在家乡或返回家乡，通过与传统重新联系和组织社区活动来解决小冲突。现实世界的冲突被净化，爱情几乎缺失，叙事张力被减弱，取而代之的是怀旧和和解。其结果是叙事的同质化：一种AI生成的合成想象，优先考虑稳定而非变革，传统而非增长。

Conclusion: AI生成的叙事具有结构同质性，这是一种独特的AI偏见，即叙事标准化。这种标准化应与更熟悉的表征偏见并列。

Abstract: Can a language model trained largely on Anglo-American texts generate stories
that are culturally relevant to other nationalities? To find out, we generated
11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a
1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although
the stories do include surface-level national symbols and themes, they
overwhelmingly conform to a single narrative plot structure across countries: a
protagonist lives in or returns home to a small town and resolves a minor
conflict by reconnecting with tradition and organising community events.
Real-world conflicts are sanitised, romance is almost absent, and narrative
tension is downplayed in favour of nostalgia and reconciliation. The result is
a narrative homogenisation: an AI-generated synthetic imaginary that
prioritises stability above change and tradition above growth. We argue that
the structural homogeneity of AI-generated narratives constitutes a distinct
form of AI bias, a narrative standardisation that should be acknowledged
alongside the more familiar representational bias. These findings are relevant
to literary studies, narratology, critical AI studies, NLP research, and
efforts to improve the cultural alignment of generative AI.

</details>


### [84] [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance](https://arxiv.org/abs/2507.22448)
*Jingwei Zuo,Maksim Velikanov,Ilyas Chahed,Younes Belkada,Dhia Eddine Rhayem,Guillaume Kunsch,Hakim Hacid,Hamza Yous,Brahim Farhat,Ibrahim Khadraoui,Mugariya Farooq,Giulia Campesan,Ruxandra Cojocaru,Yasser Djilali,Shi Hu,Iheb Chaabane,Puneesh Khanna,Mohamed El Amine Seddik,Ngoc Dung Huynh,Phuc Le Khac,Leen AlQadi,Billel Mokeddem,Mohamed Chami,Abdalgader Abubaker,Mikhail Lubinets,Kacper Piskorski,Slim Frikha*

Main category: cs.CL

TL;DR: Falcon-H1是一个混合架构LLM系列，性能优越且高效，参数规模从0.5B到34B不等，支持长上下文和多语言，可与更大模型媲美，并已开源。


<details>
  <summary>Details</summary>
Motivation: 为了提升大型语言模型的性能和效率，特别是克服现有模型在长上下文记忆和计算效率方面的局限性，引入了结合Transformer和SSM的混合架构。

Method: Falcon-H1采用结合了Transformer和状态空间模型（SSMs）的混合并行架构，并系统地优化了模型设计、数据策略和训练动态。模型发布了多种参数配置（0.5B至34B）的基础版和指令微调版，包括量化版本，共计30多个模型检查点。

Result: Falcon-H1模型展现了最先进的性能和卓越的参数及训练效率。 Falcon-H1-34B在性能上可媲美甚至超越70B规模的模型，而 Falcon-H1-1.5B-Deep则能比肩7B-10B模型，Falcon-H1-0.5B则与2024年的7B模型相当。这些模型在推理、数学、多语言任务、指令遵循和科学知识方面表现优异，支持长达256K的上下文和18种语言。

Conclusion: Falcon-H1模型在各种任务中表现出色，在参数和训练效率方面均达到或超过了同等规模甚至更大规模的模型，其广泛的适用性和开源特性使其成为AI研究的重要贡献。

Abstract: In this report, we introduce Falcon-H1, a new series of large language models
(LLMs) featuring hybrid architecture designs optimized for both high
performance and efficiency across diverse use cases. Unlike earlier Falcon
models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a
parallel hybrid approach that combines Transformer-based attention with State
Space Models (SSMs), known for superior long-context memory and computational
efficiency. We systematically revisited model design, data strategy, and
training dynamics, challenging conventional practices in the field. Falcon-H1
is released in multiple configurations, including base and instruction-tuned
variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized
instruction-tuned models are also available, totaling over 30 checkpoints on
Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and
exceptional parameter and training efficiency. The flagship Falcon-H1-34B
matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B,
and Llama3.3-70B, while using fewer parameters and less data. Smaller models
show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B
models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024.
These models excel across reasoning, mathematics, multilingual tasks,
instruction following, and scientific knowledge. With support for up to 256K
context tokens and 18 languages, Falcon-H1 is suitable for a wide range of
applications. All models are released under a permissive open-source license,
underscoring our commitment to accessible and impactful AI research.

</details>


### [85] [What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models](https://arxiv.org/abs/2507.22457)
*Tian Yun,Chen Sun,Ellie Pavlick*

Main category: cs.CL

TL;DR: LLMs 并非天生的抽象推理器，但通过微调可以大幅提升其在特定任务上的表现。然而，这种提升的泛化能力有限，具体表现取决于任务和数据集。


<details>
  <summary>Details</summary>
Motivation: 重新审视 LLMs 是否为“抽象推理器”的争论，并为这一讨论增加细微差别。

Method: 通过对输入编码进行少量参数调整来评估 LLMs 的性能。

Result: LLMs 在零样本设置下表现不佳，但通过微调参数可以显著提高性能。然而，这种性能提升在不同数据集之间可能不具有可转移性。

Conclusion: LLMs 在零样本设置下表现不佳，但通过对输入编码进行少量参数调整，可以实现近乎完美的性能。然而，这种微调并不一定能在不同数据集之间转移。这引发了关于 LLM 何时以及为何能被称为“抽象推理器”的讨论。

Abstract: Recent work has argued that large language models (LLMs) are not "abstract
reasoners", citing their poor zero-shot performance on a variety of challenging
tasks as evidence. We revisit these experiments in order to add nuance to the
claim. First, we show that while LLMs indeed perform poorly in a zero-shot
setting, even tuning a small subset of parameters for input encoding can enable
near-perfect performance. However, we also show that this finetuning does not
necessarily transfer across datasets. We take this collection of empirical
results as an invitation to (re-)open the discussion of what it means to be an
"abstract reasoner", and why it matters whether LLMs fit the bill.

</details>


### [86] [IFEvalCode: Controlled Code Generation](https://arxiv.org/abs/2507.22462)
*Jian Yang,Wei Zhang,Shukai Liu,Linzheng Chai,Yingshui Tan,Jiaheng Liu,Ge Zhang,Wangchunshu Zhou,Guanglin Niu,Zhoujun Li,Binyuan Hui,Junyang Lin*

Main category: cs.CL

TL;DR: 本研究提出前向和后向约束生成来提高代码大语言模型在可控代码生成中的指令遵循能力，并推出IFevalCode多语言基准以进行更细致的评估。结果显示闭源模型表现更优，且在正确性和指令遵循方面存在差距。


<details>
  <summary>Details</summary>
Motivation: 真实世界应用通常要求代码在风格、行数和结构等方面比单纯的正确性更严格地遵守详细需求。

Method: 提出用于改进代码大语言模型指令遵循能力的前向和后向约束生成，并提出IFevalCode基准，包含1.6K个跨7种编程语言的测试样本，并区分正确性和指令遵循两个评估指标。

Result: 实验结果表明，闭源模型在可控代码生成方面优于开源模型，并揭示了模型在生成正确代码与精确遵循指令方面的能力存在显著差距。

Conclusion: 闭源模型在可控代码生成方面优于开源模型，并且模型在生成正确代码与严格遵循指令之间存在显著差距。

Abstract: Code large language models (Code LLMs) have made significant progress in code
generation by translating natural language descriptions into functional code;
however, real-world applications often demand stricter adherence to detailed
requirements such as coding style, line count, and structural constraints,
beyond mere correctness. To address this, the paper introduces forward and
backward constraints generation to improve the instruction-following
capabilities of Code LLMs in controlled code generation, ensuring outputs align
more closely with human-defined guidelines. The authors further present
IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven
programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and
C#), with each sample featuring both Chinese and English queries. Unlike
existing benchmarks, IFEvalCode decouples evaluation into two metrics:
correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced
assessment. Experiments on over 40 LLMs reveal that closed-source models
outperform open-source ones in controllable code generation and highlight a
significant gap between the models' ability to generate correct code versus
code that precisely follows instructions.

</details>


### [87] [SLM-SQL: An Exploration of Small Language Models for Text-to-SQL](https://arxiv.org/abs/2507.22478)
*Lei Sheng,Shuai-Shuai Xu*

Main category: cs.CL

TL;DR: 通过后训练技术和纠错性自洽推理，SLM-SQL大幅提升了小型语言模型在Text-to-SQL任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 为了探索小型语言模型（SLM）在Text-to-SQL任务上的潜力，克服其逻辑推理能力不足的问题，并利用其在推理速度和边缘部署方面的优势。

Method: 文章提出了一种名为SLM-SQL的方法，结合了监督微调、基于强化学习的后训练以及纠错性自洽推理技术，并构建了SynSQL-Think-916K和SynSQL-Merge-Think-310K两个数据集来训练和评估模型。

Result: 在BIRD开发集上，SLM-SQL方法使0.5B参数的模型达到了56.87%的执行准确率（EX），1.5B参数的模型达到了67.08%的执行准确率（EX），平均提升了31.4点。

Conclusion: SLM-SQL通过监督微调、基于强化学习的后训练以及纠错性自洽推理，在Text-to-SQL任务上显著提升了小型语言模型的性能，使其在BIRD数据集上取得了高达67.08%的执行准确率。

Abstract: Large language models (LLMs) have demonstrated strong performance in
translating natural language questions into SQL queries (Text-to-SQL). In
contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters
currently underperform on Text-to-SQL tasks due to their limited logical
reasoning capabilities. However, SLMs offer inherent advantages in inference
speed and suitability for edge deployment. To explore their potential in
Text-to-SQL applications, we leverage recent advancements in post-training
techniques. Specifically, we used the open-source SynSQL-2.5M dataset to
construct two derived datasets: SynSQL-Think-916K for SQL generation and
SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised
fine-tuning and reinforcement learning-based post-training to the SLM, followed
by inference using a corrective self-consistency approach. Experimental results
validate the effectiveness and generalizability of our method, SLM-SQL. On the
BIRD development set, the five evaluated models achieved an average improvement
of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy
(EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset,
model, and code to github: https://github.com/CycloneBoy/slm_sql.

</details>


### [88] [CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records](https://arxiv.org/abs/2507.22533)
*Dongchen Li,Jitao Liang,Wei Li,Xiaoyu Wang,Longbing Cao,Kun Yu*

Main category: cs.CL

TL;DR: CliCARE是一个用于癌症电子健康记录的LLM框架，通过时间知识图谱和指南对齐来解决长篇幅、多语言和临床幻觉问题，提供准确的临床决策支持。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs在处理长篇幅、多语言的癌症电子健康记录（EHR）方面的挑战，包括准确的时间分析、临床幻觉风险（现有RAG方法未能充分整合临床指南）以及不可靠的评估指标。

Method: 提出CliCARE框架，将非结构化的纵向EHR数据转换为患者特定的时间知识图谱（TKGs）以捕捉长距离依赖关系，并通过将这些真实患者轨迹与规范的指南知识图谱对齐来实现决策支持的 grounding。

Result: CliCARE在中文和英文数据集上均显著优于包括长上下文LLMs和知识图谱增强RAG方法在内的强基线方法。该框架生成的临床摘要和建议的临床有效性得到了专家评估的大力支持，显示出与专家评估的高度相关性。

Conclusion: CliCARE框架在处理长篇幅、多语言的患者记录、临床决策支持和肿瘤学AI系统评估方面取得了显著进展，并能生成高保真度的临床摘要和可行的建议，显著优于现有基线方法，并得到肿瘤科专家的验证。

Abstract: Large Language Models (LLMs) hold significant promise for improving clinical
decision support and reducing physician burnout by synthesizing complex,
longitudinal cancer Electronic Health Records (EHRs). However, their
implementation in this critical field faces three primary challenges: the
inability to effectively process the extensive length and multilingual nature
of patient records for accurate temporal analysis; a heightened risk of
clinical hallucination, as conventional grounding techniques such as
Retrieval-Augmented Generation (RAG) do not adequately incorporate
process-oriented clinical guidelines; and unreliable evaluation metrics that
hinder the validation of AI systems in oncology. To address these issues, we
propose CliCARE, a framework for Grounding Large Language Models in Clinical
Guidelines for Decision Support over Longitudinal Cancer Electronic Health
Records. The framework operates by transforming unstructured, longitudinal EHRs
into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range
dependencies, and then grounding the decision support process by aligning these
real-world patient trajectories with a normative guideline knowledge graph.
This approach provides oncologists with evidence-grounded decision support by
generating a high-fidelity clinical summary and an actionable recommendation.
We validated our framework using large-scale, longitudinal data from a private
Chinese cancer dataset and the public English MIMIC-IV dataset. In these
diverse settings, CliCARE significantly outperforms strong baselines, including
leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The
clinical validity of our results is supported by a robust evaluation protocol,
which demonstrates a high correlation with assessments made by expert
oncologists.

</details>


### [89] [A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support](https://arxiv.org/abs/2507.22542)
*Long S. T. Nguyen,Truong P. Hua,Thanh M. Nguyen,Toan Q. Pham,Nam K. Ngo,An X. Nguyen,Nghi D. M. Pham,Nghia H. Nguyen,Tho T. Quan*

Main category: cs.CL

TL;DR: 本研究发布了一个包含9000+客户支持问答对的越南语数据集（CSConDa），并评估了11个轻量级开源越南语大语言模型（ViLLMs）在该数据集上的表现，旨在帮助企业选择合适的模型并推动相关研究。


<details>
  <summary>Details</summary>
Motivation: 当前领域特定的评估有限，且缺乏反映真实客户互动的基准数据集，使得企业难以选择适合客户支持应用的ViLLMs。

Method: 本研究介绍了CSConDa数据集，包含超过9000个源自真实客户服务交互的问答对，并构建了一个包含自动评估指标和句法分析的综合评估框架，用于在CSConDa上对11个轻量级开源ViLLMs进行基准测试。

Result: 通过在CSConDa数据集上的评估，本研究揭示了11个轻量级开源ViLLMs的优势、劣势和语言模式，为客户服务问答提供了模型选择的依据，并指出了改进方向。

Conclusion: 本研究通过引入CSConDa数据集和全面的评估框架，为越南语大语言模型（ViLLMs）在客户支持问答（QA）领域的应用提供了实证依据和改进方向。评估结果揭示了不同轻量级开源ViLLMs的优劣势和语言模式，为企业选择合适的模型以优化客户服务提供了指导，并推动了越南语LLM的研究进展。

Abstract: With the rapid growth of Artificial Intelligence, Large Language Models
(LLMs) have become essential for Question Answering (QA) systems, improving
efficiency and reducing human workload in customer service. The emergence of
Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a
practical choice for their accuracy, efficiency, and privacy benefits. However,
domain-specific evaluations remain limited, and the absence of benchmark
datasets reflecting real customer interactions makes it difficult for
enterprises to select suitable models for support applications. To address this
gap, we introduce the Customer Support Conversations Dataset (CSConDa), a
curated benchmark of over 9,000 QA pairs drawn from real interactions with
human advisors at a large Vietnamese software company. Covering diverse topics
such as pricing, product availability, and technical troubleshooting, CSConDa
provides a representative basis for evaluating ViLLMs in practical scenarios.
We further present a comprehensive evaluation framework, benchmarking 11
lightweight open-source ViLLMs on CSConDa with both automatic metrics and
syntactic analysis to reveal model strengths, weaknesses, and linguistic
patterns. This study offers insights into model behavior, explains performance
differences, and identifies key areas for improvement, supporting the
development of next-generation ViLLMs. By establishing a robust benchmark and
systematic evaluation, our work enables informed model selection for customer
service QA and advances research on Vietnamese LLMs. The dataset is publicly
available at
https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.

</details>


### [90] [ControlMed: Adding Reasoning Control to Medical Language Model](https://arxiv.org/abs/2507.22545)
*Sung-Min Lee,Siyoon Lee,Juyeon Kim,Kyungmin Roh*

Main category: cs.CL

TL;DR: ControlMed通过引入细粒度的控制标记来解决LLM推理中的冗长问题，允许用户控制推理长度，从而在准确性和效率之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理模型通常会生成冗长的推理过程，导致显著的计算开销和响应延迟，这阻碍了它们在实际临床环境中的部署。为了解决这些挑战，我们引入了ControlMed，一个可以通过在推理时使用细粒度的控制标记来主动控制推理过程长度的医学语言模型。

Method: ControlMed通过一个包含三个阶段的流水线进行训练：1）在涵盖直接和推理响应的大规模合成医学指令数据集上进行预训练；2）使用多长度推理数据和显式长度控制标记进行监督微调；3）使用基于模型的奖励信号进行强化学习，以提高事实准确性和响应质量。

Result: 与最先进的模型相比，ControlMed在各种英语和韩语医学基准测试上实现了相似或更好的性能。此外，用户可以通过控制所需的推理长度来灵活地平衡推理准确性和计算效率。

Conclusion: ControlMed是一个实用且适应性强的解决方案，可用于临床问答和医学信息分析。实验结果表明，该模型在多种英语和韩语医学基准测试中取得了与最先进模型相当或更好的性能。用户可以通过控制推理长度来灵活地平衡推理准确性和计算效率。

Abstract: Reasoning Large Language Models (LLMs) with enhanced accuracy and
explainability are increasingly being adopted in the medical domain, as the
life-critical nature of clinical decision-making demands reliable support.
Despite these advancements, existing reasoning LLMs often generate
unnecessarily lengthy reasoning processes, leading to significant computational
overhead and response latency. These limitations hinder their practical
deployment in real-world clinical environments. To address these challenges, we
introduce \textbf{ControlMed}, a medical language model that enables users to
actively control the length of the reasoning process at inference time through
fine-grained control markers. ControlMed is trained through a three-stage
pipeline: 1) pre-training on a large-scale synthetic medical instruction
dataset covering both \textit{direct} and \textit{reasoning responses}; 2)
supervised fine-tuning with multi-length reasoning data and explicit
length-control markers; and 3) reinforcement learning with model-based reward
signals to enhance factual accuracy and response quality. Experimental results
on a variety of English and Korean medical benchmarks demonstrate that our
model achieves similar or better performance compared to state-of-the-art
models. Furthermore, users can flexibly balance reasoning accuracy and
computational efficiency by controlling the reasoning length as needed. These
findings demonstrate that ControlMed is a practical and adaptable solution for
clinical question answering and medical information analysis.

</details>


### [91] [Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs](https://arxiv.org/abs/2507.22564)
*Xikang Yang,Biyu Zhou,Xuehai Tang,Jizhong Han,Songlin Hu*

Main category: cs.CL

TL;DR: CognitiveAttack是一个利用认知偏见（尤其是多偏见交互）的新型红队测试框架，能够有效地绕过LLM的安全协议，并取得了比现有方法更高的成功率。


<details>
  <summary>Details</summary>
Motivation: LLM的安全机制容易受到利用认知偏见的对抗性攻击，而先前的方法主要集中在提示工程或算法操纵，忽视了多偏见交互的力量。

Method: 提出了一种名为CognitiveAttack的新颖红队测试框架，该框架通过集成监督微调和强化学习，系统地利用个体和组合认知偏见，生成可以绕过安全协议的优化偏见组合提示。

Result: CognitiveAttack在30个不同的LLM上显示出显著的漏洞，尤其是在开源模型上。与最先进的黑盒方法PAP相比，CognitiveAttack实现了更高的攻击成功率（60.1% vs. 31.6%），暴露了当前防御机制的关键限制。

Conclusion: 多偏见交互是绕过LLM安全协议的强大但未被充分探索的攻击途径。这项工作通过结合认知科学和LLM安全，引入了一个新颖的跨学科视角，为更强大和人类对齐的AI系统铺平了道路。

Abstract: Large Language Models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet their safety mechanisms remain susceptible to
adversarial attacks that exploit cognitive biases -- systematic deviations from
rational judgment. Unlike prior jailbreaking approaches focused on prompt
engineering or algorithmic manipulation, this work highlights the overlooked
power of multi-bias interactions in undermining LLM safeguards. We propose
CognitiveAttack, a novel red-teaming framework that systematically leverages
both individual and combined cognitive biases. By integrating supervised
fine-tuning and reinforcement learning, CognitiveAttack generates prompts that
embed optimized bias combinations, effectively bypassing safety protocols while
maintaining high attack success rates. Experimental results reveal significant
vulnerabilities across 30 diverse LLMs, particularly in open-source models.
CognitiveAttack achieves a substantially higher attack success rate compared to
the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations
in current defense mechanisms. These findings highlight multi-bias interactions
as a powerful yet underexplored attack vector. This work introduces a novel
interdisciplinary perspective by bridging cognitive science and LLM safety,
paving the way for more robust and human-aligned AI systems.

</details>


### [92] [Unveiling the Influence of Amplifying Language-Specific Neurons](https://arxiv.org/abs/2507.22581)
*Inaya Rahmanisa,Lyzander Marciano Andrylie,Krisna Mahardika Ihsani,Alfan Farizki Wicaksono,Haryo Akbarianto Wibowo,Alham Fikri Aji*

Main category: cs.CL

TL;DR: 放大语言特定神经元对模型的多语言行为有显著影响，可以提升模型在特定语言上的表现，但对跨语言迁移能力提升有限。


<details>
  <summary>Details</summary>
Motivation: 虽然失活语言特定神经元对模型行为有影响，但其在放大方面的作用尚未得到充分研究。

Method: 通过在18种语言（包括低资源语言）上进行干预，并使用三种主要以不同语言训练的模型，研究了放大语言特定神经元的效果。通过提出的语言转向评分（LSS）评估转向目标语言的有效性，并将其应用于下游任务：常识推理（XCOPA, XWinograd）、知识（Include）和翻译（FLORES）。

Result: 最优放大因子能有效引导模型输出至几乎所有测试语言。在下游任务中使用最优放大因子可改善某些情况下的模型自身语言表现，但总体上会降低跨语言结果。

Conclusion: 放大语言特定神经元可以增强模型在特定语言上的表现，尤其对于低资源语言有益，但对跨语言迁移的优势有限。

Abstract: Language-specific neurons in LLMs that strongly correlate with individual
languages have been shown to influence model behavior by deactivating them.
However, their role in amplification remains underexplored. This work
investigates the effect of amplifying language-specific neurons through
interventions across 18 languages, including low-resource ones, using three
models primarily trained in different languages. We compare amplification
factors by their effectiveness in steering to the target language using a
proposed Language Steering Shift (LSS) evaluation score, then evaluate it on
downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge
(Include), and translation (FLORES). The optimal amplification factors
effectively steer output toward nearly all tested languages. Intervention using
this factor on downstream tasks improves self-language performance in some
cases but generally degrades cross-language results. These findings highlight
the effect of language-specific neurons in multilingual behavior, where
amplification can be beneficial especially for low-resource languages, but
provides limited advantage for cross-lingual transfer.

</details>


### [93] [BALSAM: A Platform for Benchmarking Arabic Large Language Models](https://arxiv.org/abs/2507.22603)
*Rawan Al-Matham,Kareem Darwish,Raghad Al-Rasheed,Waad Alshammari,Muneera Alhoshan,Amal Almazrua,Asma Al Wazrah,Mais Alheraki,Firoj Alam,Preslav Nakov,Norah Alzahrani,Eman alBilali,Nizar Habash,Abdelrahman El-Sheikh,Muhammad Elmallah,Haonan Li,Hamdy Mubarak,Mohamed Anwar,Zaid Alyafeai,Ahmed Abdelali,Nora Altwairesh,Maram Hasanain,Abdulmohsen Al Thubaity,Shady Shehata,Bashar Alhafni,Injy Hamed,Go Inoue,Khalid Elmadani,Ossama Obeid,Fatima Haouari,Tamer Elsayed,Emad Alghamdi,Khalid Almubarak,Saied Alshahrani,Ola Aljarrah,Safa Alajlan,Areej Alshaqarawi,Maryam Alshihri,Sultana Alghurabi,Atikah Alzeghayer,Afrah Altamimi,Abdullah Alfaifi,Abdulrahman AlOsaimy*

Main category: cs.CL

TL;DR: BALSAM是一个新的阿拉伯语LLM基准，包含大量任务和样本，并提供盲评估平台，以克服当前挑战并推动该领域发展。


<details>
  <summary>Details</summary>
Motivation: 解决当前阿拉伯语LLM发展和评估的挑战，包括数据稀缺、语言多样性、形态复杂性以及现有基准的局限性（如静态数据、任务覆盖不足、缺乏盲测平台和数据污染问题）。

Method: 提出BALSAM，一个包含78个NLP任务、52K个样本（37K个测试，15K个开发）的综合基准，并建立一个用于盲评估的集中式、透明化平台。

Result: BALSAM包含78个NLP任务，涵盖14个广泛类别，共52K个样本，并提供一个用于盲评估的集中式、透明化平台，旨在促进阿拉伯语LLM的研究和发展。

Conclusion: BALSAM旨在弥合差距，通过提供一个全面的、社区驱动的基准来推动阿拉伯语LLM的发展和评估。

Abstract: The impressive advancement of Large Language Models (LLMs) in English has not
been matched across all languages. In particular, LLM performance in Arabic
lags behind, due to data scarcity, linguistic diversity of Arabic and its
dialects, morphological complexity, etc. Progress is further hindered by the
quality of Arabic benchmarks, which typically rely on static, publicly
available data, lack comprehensive task coverage, or do not provide dedicated
platforms with blind test sets. This makes it challenging to measure actual
progress and to mitigate data contamination. Here, we aim to bridge these gaps.
In particular, we introduce BALSAM, a comprehensive, community-driven benchmark
aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP
tasks from 14 broad categories, with 52K examples divided into 37K test and 15K
development, and a centralized, transparent platform for blind evaluation. We
envision BALSAM as a unifying platform that sets standards and promotes
collaborative research to advance Arabic LLM capabilities.

</details>


### [94] [Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation](https://arxiv.org/abs/2507.22608)
*Daniil Gurgurov,Katharina Trinley,Yusser Al Ghussin,Tanja Baeumel,Josef van Genabith,Simon Ostermann*

Main category: cs.CL

TL;DR: LLM的语言特定神经元可以通过LAPE方法识别，并利用语言算术进行操纵，以在多语言任务中控制模型行为，但效果受语言资源和相似性影响。


<details>
  <summary>Details</summary>
Motivation: LLM展现出强大的多语言能力，但其语言特定处理的神经机制尚不清楚。

Method: 通过LAPE方法识别控制语言行为的神经元，并使用语言算术来操纵模型以激活或禁用特定语言。

Result: 研究识别了特定语言的神经元，发现它们聚集在更深的层，非拉丁字母的脚本表现出更高的特异性。相关语言共享重叠的神经元，反映了语言接近度的内部表示。模型操纵可以有效地指导跨语言任务的行为，并且通过跨语言神经元操纵可以提高下游性能。

Conclusion: LLM语言操纵的干预效果受限于资源和语言相似性，并且存在内部回退机制。

Abstract: Large language models (LLMs) exhibit strong multilingual abilities, yet the
neural mechanisms behind language-specific processing remain unclear. We
analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and
Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying
neurons that control language behavior. Using the Language Activation
Probability Entropy (LAPE) method, we show that these neurons cluster in deeper
layers, with non-Latin scripts showing greater specialization. Related
languages share overlapping neurons, reflecting internal representations of
linguistic proximity.
  Through language arithmetics, i.e. systematic activation addition and
multiplication, we steer models to deactivate unwanted languages and activate
desired ones, outperforming simpler replacement approaches. These interventions
effectively guide behavior across five multilingual tasks: language forcing,
translation, QA, comprehension, and NLI. Manipulation is more successful for
high-resource languages, while typological similarity improves effectiveness.
We also demonstrate that cross-lingual neuron steering enhances downstream
performance and reveal internal "fallback" mechanisms for language selection
when neurons are progressively deactivated. Our code is made publicly available
at https://github.com/d-gurgurov/Language-Neurons-Manipulation.

</details>


### [95] [Multilingual Political Views of Large Language Models: Identification and Steering](https://arxiv.org/abs/2507.22623)
*Daniil Gurgurov,Katharina Trinley,Ivan Vykopal,Josef van Genabith,Simon Ostermann,Roberto Zamparelli*

Main category: cs.CL

TL;DR: LLM 存在政治偏见，并且可以通过干预技术进行控制。


<details>
  <summary>Details</summary>
Motivation: 填补现有研究在模型、语言和可控性方面的空白，以了解 LLM 对政治观点的潜在影响。

Method: 使用政治指南针测试和 11 个语义等价的释义来评估七个模型（包括 LLaMA-3.1、Qwen-3 和 Aya-Expanse）在 14 种语言中的政治倾向。通过中心点激活干预技术来控制政治立场。

Result: 较大的模型倾向于自由主义左翼立场，不同语言和模型家族之间存在显著差异。干预技术能够可靠地引导模型响应朝着其他意识形态立场。

Conclusion: LLM 表现出明显的政治偏见，并且可以通过干预技术进行控制。

Abstract: Large language models (LLMs) are increasingly used in everyday tools and
applications, raising concerns about their potential influence on political
views. While prior research has shown that LLMs often exhibit measurable
political biases--frequently skewing toward liberal or progressive
positions--key gaps remain. Most existing studies evaluate only a narrow set of
models and languages, leaving open questions about the generalizability of
political biases across architectures, scales, and multilingual settings.
Moreover, few works examine whether these biases can be actively controlled.
  In this work, we address these gaps through a large-scale study of political
orientation in modern open-source instruction-tuned LLMs. We evaluate seven
models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using
the Political Compass Test with 11 semantically equivalent paraphrases per
statement to ensure robust measurement. Our results reveal that larger models
consistently shift toward libertarian-left positions, with significant
variations across languages and model families. To test the manipulability of
political stances, we utilize a simple center-of-mass activation intervention
technique and show that it reliably steers model responses toward alternative
ideological positions across multiple languages. Our code is publicly available
at https://github.com/d-gurgurov/Political-Ideologies-LLMs.

</details>


### [96] [Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment](https://arxiv.org/abs/2507.22676)
*Jia Li,Yang Wang,Wenhao Qian,Zhenzhen Hu,Richang Hong,Meng Wang*

Main category: cs.CL

TL;DR: 该研究提出了一个多模态（视频、音频、文本）的面试评估框架，通过整合多种数据源和集成学习策略，实现了更全面、公平和准确的候选人评估，并在竞赛中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 为了确保全面公平的评估，需要一种新的框架来探索面试表现的“365”个方面。

Method: 该框架整合了视频、音频和文本三种模态，为每位候选人提供六种响应，并关注五个关键评估维度。它采用特定模态的特征提取器来编码异构数据流，并通过共享压缩多层感知器进行融合，将多模态嵌入压缩到统一的潜在空间中以促进特征交互。为了增强预测的鲁棒性，该框架还结合了两种级别的集成学习策略：首先，独立的回归头为每个响应预测分数；其次，跨响应的预测通过平均池化机制进行聚合，以产生五个目标维度的最终分数。

Result: 该框架实现了0.1824的多维度平均均方误差，并在AVI挑战赛2025中获得第一名。

Conclusion: 该框架在AVI挑战赛2025中取得了第一名，平均多维度均方误差为0.1824，证明了其在自动化和多模态面试表现评估方面的有效性和鲁棒性。

Abstract: Interview performance assessment is essential for determining candidates'
suitability for professional positions. To ensure holistic and fair
evaluations, we propose a novel and comprehensive framework that explores
``365'' aspects of interview performance by integrating \textit{three}
modalities (video, audio, and text), \textit{six} responses per candidate, and
\textit{five} key evaluation dimensions. The framework employs
modality-specific feature extractors to encode heterogeneous data streams and
subsequently fused via a Shared Compression Multilayer Perceptron. This module
compresses multimodal embeddings into a unified latent space, facilitating
efficient feature interaction. To enhance prediction robustness, we incorporate
a two-level ensemble learning strategy: (1) independent regression heads
predict scores for each response, and (2) predictions are aggregated across
responses using a mean-pooling mechanism to produce final scores for the five
target dimensions. By listening to the unspoken, our approach captures both
explicit and implicit cues from multimodal data, enabling comprehensive and
unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our
framework secured first place in the AVI Challenge 2025, demonstrating its
effectiveness and robustness in advancing automated and multimodal interview
performance assessment. The full implementation is available at
https://github.com/MSA-LMC/365Aspects.

</details>


### [97] [From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs](https://arxiv.org/abs/2507.22716)
*Jie He,Victor Gutierrez Basulto,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本研究提出了TIRESRAG-R1框架，通过“思考-检索-反思”过程和多维度奖励系统，解决了现有RAG方法在中间推理质量上的不足，实验证明其性能优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大多数基于强化学习的检索增强生成（RAG）方法仅依赖最终答案的奖励，忽略了中间推理过程的质量。本研究旨在解决此问题，提高RAG在推理方面的能力。

Method: TIRESRAG-R1框架采用“思考-检索-反思”过程，并结合多维度奖励系统（包括充分性奖励、推理质量奖励和反思奖励）来提高推理能力和稳定性。此外，该框架还采用难度感知重加权策略和训练样本过滤来提升复杂任务的性能。

Result: TIRESRAG-R1通过引入多维度奖励和优化训练策略，有效解决了信息不足、推理错误和答案-推理不一致等RAG模型的主要失败模式，提升了推理的准确性和稳定性。

Conclusion: TIRESRAG-R1在四个多跳问答数据集上的实验表明，其性能优于现有的RAG方法，并且能很好地泛化到单跳任务。

Abstract: Reinforcement learning-based retrieval-augmented generation (RAG) methods
enhance the reasoning abilities of large language models (LLMs). However, most
rely only on final-answer rewards, overlooking intermediate reasoning quality.
This paper analyzes existing RAG reasoning models and identifies three main
failure patterns: (1) information insufficiency, meaning the model fails to
retrieve adequate support; (2) faulty reasoning, where logical or content-level
flaws appear despite sufficient information; and (3) answer-reasoning
inconsistency, where a valid reasoning chain leads to a mismatched final
answer. We propose TIRESRAG-R1, a novel framework using a
think-retrieve-reflect process and a multi-dimensional reward system to improve
reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to
encourage thorough retrieval; (2) a reasoning quality reward to assess the
rationality and accuracy of the reasoning chain; and (3) a reflection reward to
detect and revise errors. It also employs a difficulty-aware reweighting
strategy and training sample filtering to boost performance on complex tasks.
Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms
prior RAG methods and generalizes well to single-hop tasks. The code and data
are available at: https://github.com/probe2/TIRESRAG-R1.

</details>


### [98] [Investigating Hallucination in Conversations for Low Resource Languages](https://arxiv.org/abs/2507.22720)
*Amit Das,Md. Najib Hasan,Souvika Sarkar,Zheng Zhang,Fatemeh Jamshidi,Tathagata Bhattacharya,Nilanjana Raychawdhury,Dongji Feng,Vinija Jain,Aman Chadha*

Main category: cs.CL

TL;DR: 大型语言模型在印地语和波斯语对话中比在普通话中更容易产生事实错误。


<details>
  <summary>Details</summary>
Motivation: 为了提高大型语言模型（LLMs）的可靠性和有效性，解决其生成事实不准确内容（即“幻觉”）的问题至关重要。本研究旨在将对英语LLM幻觉的研究扩展到印地语、波斯语和普通话这三种语言的对话数据上。

Method: 本研究通过分析包含印地语、波斯语和普通话的对话数据集，评估了GPT-3.5、GPT-4o、Llama-3.1、Gemma-2.0、DeepSeek-R1和Qwen-3等大型语言模型在生成文本时出现的事实错误和语言错误（即“幻觉”）。

Result: 研究发现，在所测试的大型语言模型中，普通话的幻觉现象非常少，而印地语和波斯语的幻觉数量则显著偏高。

Conclusion: LLMs在处理印地语和波斯语等语言时，幻觉现象比普通话更为显著。未来的研究应关注提高LLMs在这些语言中的事实准确性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable proficiency in
generating text that closely resemble human writing. However, they often
generate factually incorrect statements, a problem typically referred to as
'hallucination'. Addressing hallucination is crucial for enhancing the
reliability and effectiveness of LLMs. While much research has focused on
hallucinations in English, our study extends this investigation to
conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a
comprehensive analysis of a dataset to examine both factual and linguistic
errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,
DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated
responses in Mandarin but generate a significantly higher number of
hallucinations in Hindi and Farsi.

</details>


### [99] [Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning](https://arxiv.org/abs/2507.22729)
*Benedikt Roth,Stephan Rappensperger,Tianming Qiu,Hamza Imamović,Julian Wörmann,Hao Shen*

Main category: cs.CL

TL;DR: LLMs can be adapted for text embedding tasks by combining prompt engineering and contrastive fine-tuning, achieving state-of-the-art results on benchmarks like MTEB. Fine-tuning helps LLMs focus on relevant words for better meaning compression.


<details>
  <summary>Details</summary>
Motivation: To address the issue that pooling token-level representations from LLMs into text embeddings discards crucial information, despite the fact that many non-generative downstream tasks depend on accurate sentence- or document-level embeddings.

Method: The paper explores several adaptation strategies for pre-trained, decoder-only LLMs, including various aggregation techniques for token embeddings, task-specific prompt engineering, and text-level augmentation via contrastive fine-tuning.

Result: Combining the explored adaptation strategies yields state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). Analysis of the attention map shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state.

Conclusion: LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.

Abstract: Large Language Models (LLMs) have become a cornerstone in Natural Language
Processing (NLP), achieving impressive performance in text generation. Their
token-level representations capture rich, human-aligned semantics. However,
pooling these vectors into a text embedding discards crucial information.
Nevertheless, many non-generative downstream tasks, such as clustering,
classification, or retrieval, still depend on accurate and controllable
sentence- or document-level embeddings. We explore several adaptation
strategies for pre-trained, decoder-only LLMs: (i) various aggregation
techniques for token embeddings, (ii) task-specific prompt engineering, and
(iii) text-level augmentation via contrastive fine-tuning. Combining these
components yields state-of-the-art performance on the English clustering track
of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention
map further shows that fine-tuning shifts focus from prompt tokens to
semantically relevant words, indicating more effective compression of meaning
into the final hidden state. Our experiments demonstrate that LLMs can be
effectively adapted as text embedding models through a combination of prompt
engineering and resource-efficient contrastive fine-tuning on synthetically
generated positive pairs.

</details>


### [100] [Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index](https://arxiv.org/abs/2507.22744)
*Praveenkumar Katwe,Rakesh Chandra,Balabantaray Kali,Prasad Vittala*

Main category: cs.CL

TL;DR: 通过使用EHI作为奖励信号进行强化学习微调，成功减少了生成摘要中的实体幻觉，并保持了摘要的流畅性和信息量。


<details>
  <summary>Details</summary>
Motivation: 摘要中提到，在生成式摘要任务中减少幻觉是部署语言模型（LM）的一个关键挑战。因此，这项工作的动机是解决这一问题，提高摘要的准确性和可靠性。

Method: 介绍了一种奖励驱动的微调框架，该框架使用实体幻觉指数（EHI）作为奖励信号，通过强化学习来优化模型参数，从而减少生成摘要中的实体幻觉。

Result: 实验结果表明，该方法在EHI方面取得了持续改进，并且在定性分析中显示实体幻觉显著减少，同时流畅性和信息性没有下降。此外，还发布了一个可复现的Colab流程，以促进未来对类似EHI的轻量级、幻觉感知度量的研究。

Conclusion: 该方法通过优化实体幻觉指数（EHI）来减少生成摘要中的实体幻觉，并在不影响流畅性和信息性的前提下，在数据集上实现了EHI的持续改进。

Abstract: Reducing hallucinations in abstractive summarization remains a critical
challenge for deploying language models (LMs) in real-world settings. In this
work, we introduce a rewarddriven fine-tuning framework that explicitly
optimizes for Entity Hallucination Index (EHI), a metric designed to quantify
the presence, correctness, and grounding of named entities in generated
summaries. Given a corpus of meeting transcripts, we first generate baseline
summaries using a pre-trained LM and compute EHI scores via automatic entity
extraction and matching. We then apply reinforcement learning to fine-tune the
model parameters, using EHI as a reward signal to bias generation toward
entity-faithful outputs. Our approach does not rely on human-written factuality
annotations, enabling scalable fine-tuning. Experiments demonstrate consistent
improvements in EHI across datasets, with qualitative analysis revealing a
significant reduction in entity-level hallucinations without degradation in
fluency or informativeness. We release a reproducible Colab pipeline,
facilitating further research on hallucination-aware model fine-tuning using
lightweight, hallucintion metrics like EHI.

</details>


### [101] [CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset](https://arxiv.org/abs/2507.22752)
*Jindřich Libovický,Jindřich Helcl,Andrei Manea,Gianluca Vico*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a benchmark for open-ended regional question answering that
encompasses both textual and visual modalities. We also provide strong
baselines using state-of-the-art large language models (LLMs). Our dataset
consists of manually curated questions and answers grounded in Wikipedia,
created by native speakers from Czechia, Slovakia, and Ukraine, with
accompanying English translations. It includes both purely textual questions
and those requiring visual understanding. As a baseline, we evaluate
state-of-the-art LLMs through prompting and complement this with human
judgments of answer correctness. Using these human evaluations, we analyze the
reliability of existing automatic evaluation metrics. Our baseline results
highlight a significant gap in regional knowledge among current LLMs. Moreover,
apart from LLM-based evaluation, there is minimal correlation between automated
metrics and human judgment. We release this dataset as a resource to (1) assess
regional knowledge in LLMs, (2) study cross-lingual generation consistency in a
challenging setting, and (3) advance the development of evaluation metrics for
open-ended question answering.

</details>


### [102] [Opportunities and Challenges of LLMs in Education: An NLP Perspective](https://arxiv.org/abs/2507.22753)
*Sowmya Vajjala,Bashar Alhafni,Stefano Bannò,Kaushal Kumar Maurya,Ekaterina Kochmar*

Main category: cs.CL

TL;DR: LLMs在教育领域的应用和影响，特别是在语言学习和NLP赋能的教育应用方面。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在教育领域的兴趣日益增长，本文旨在全面审视LLMs对教育NLP的影响，为研究人员和从业者提供有价值的参考。

Method: 本文 examination 了 LLMs 在教育领域的应用，主要围绕"辅助"和"评估"两个场景，并结合了读、写、说、辅导四个维度进行分析。

Result: LLMs为教育NLP带来了新的方向和关键挑战，尤其是在读、写、说、辅导等应用场景。

Conclusion: LLMs在教育领域的应用前景广阔，尤其在语言学习和NLP赋能的教育应用方面。

Abstract: Interest in the role of large language models (LLMs) in education is
increasing, considering the new opportunities they offer for teaching,
learning, and assessment. In this paper, we examine the impact of LLMs on
educational NLP in the context of two main application scenarios: {\em
assistance} and {\em assessment}, grounding them along the four dimensions --
reading, writing, speaking, and tutoring. We then present the new directions
enabled by LLMs, and the key challenges to address. We envision that this
holistic overview would be useful for NLP researchers and practitioners
interested in exploring the role of LLMs in developing language-focused and
NLP-enabled educational applications of the future.

</details>


### [103] [MASCA: LLM based-Multi Agents System for Credit Assessment](https://arxiv.org/abs/2507.22758)
*Gautam Jajoo,Pranjal A Chitale,Saksham Agarwal*

Main category: cs.CL

TL;DR: MASCA是一个LLM驱动的多智能体系统，用于改善信用评估，其性能优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 信用评估是一个被忽视的挑战，传统方法依赖于规则和统计模型。本研究旨在通过MASCA，一个模仿现实世界决策过程的LLM驱动的多智能体系统，来改进信用评估。

Method: MASCA是一个LLM驱动的多智能体系统，采用分层架构，由专门的基于LLM的智能体协作处理子任务，并结合对比学习进行风险和回报评估。此外，还从信号博弈论的角度分析了分层多智能体系统，并进行了信用评估中的偏倚分析。

Result: MASCA在信用评分任务上超越了基线方法，证明了分层LLM驱动的多智能体系统在金融领域的有效性。

Conclusion: MASCA在信用评分等金融应用中，通过分层LLM驱动的多智能体系统，在信用评估方面表现优于基线方法。

Abstract: Recent advancements in financial problem-solving have leveraged LLMs and
agent-based systems, with a primary focus on trading and financial modeling.
However, credit assessment remains an underexplored challenge, traditionally
dependent on rule-based methods and statistical models. In this paper, we
introduce MASCA, an LLM-driven multi-agent system designed to enhance credit
evaluation by mirroring real-world decision-making processes. The framework
employs a layered architecture where specialized LLM-based agents
collaboratively tackle sub-tasks. Additionally, we integrate contrastive
learning for risk and reward assessment to optimize decision-making. We further
present a signaling game theory perspective on hierarchical multi-agent
systems, offering theoretical insights into their structure and interactions.
Our paper also includes a detailed bias analysis in credit assessment,
addressing fairness concerns. Experimental results demonstrate that MASCA
outperforms baseline approaches, highlighting the effectiveness of hierarchical
LLM-based multi-agent systems in financial applications, particularly in credit
scoring.

</details>


### [104] [DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph](https://arxiv.org/abs/2507.22811)
*Debayan Banerjee,Tilahun Abedissa Taffa,Ricardo Usbeck*

Main category: cs.CL

TL;DR: 提出了一种新的零样本实体链接方法，利用LLM的“yes”token输出来优化DBLP知识图谱中的实体链接。


<details>
  <summary>Details</summary>
Motivation: 为了处理DBLP知识图谱中新增的dblp:Stream实体类型，并改进实体链接器的性能。

Method: 使用LLM进行零样本实体链接，并通过重新排序候选实体来优化链接结果，其中重新排序的依据是LLM的“yes”token输出的对数概率。

Result: 在DBLP的RDF知识图谱上实现了零样本实体链接，并提出了一种新颖的基于LLM的方法。

Conclusion: 该工作提出了一种基于LLM的零样本实体链接器，用于DBLP的RDF知识图谱，并使用一种新颖的方法，根据LLM的“yes”token输出来重新排序候选实体。

Abstract: In this work we present an entity linker for DBLP's 2025 version of RDF-based
Knowledge Graph. Compared to the 2022 version, DBLP now considers publication
venues as a new entity type called dblp:Stream. In the earlier version of
DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce
entity linkings. In contrast, in this work, we develop a zero-shot entity
linker using LLMs using a novel method, where we re-rank candidate entities
based on the log-probabilities of the "yes" token output at the penultimate
layer of the LLM.

</details>


### [105] [Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization](https://arxiv.org/abs/2507.22829)
*Weijia Zhang,Songgaojun Deng,Evangelos Kanoulas*

Main category: cs.CL

TL;DR: SPaGe framework uses a structured plan (TaSoF) and graph-based execution for query-focused table summarization, outperforming prior models by explicitly capturing complex dependencies and enabling parallel execution.


<details>
  <summary>Details</summary>
Motivation: NL plans for query-focused table summarization are ambiguous and lack structure, limiting conversion to executable programs like SQL and hindering scalability, especially for multi-table tasks.

Method: The proposed framework SPaGe formalizes the reasoning process in three phases: 1) Structured Planning to generate TaSoF from a query, 2) Graph-based Execution to convert plan steps into SQL and model dependencies via a directed cyclic graph for parallel execution, and 3) Summary Generation to produce query-focused summaries. The TaSoF plan is inspired by formalism in traditional multi-agent systems.

Result: Experiments on three public benchmarks show that SPaGe consistently outperforms prior models in both single- and multi-table settings.

Conclusion: SPaGe consistently outperforms prior models in both single- and multi-table settings, demonstrating the advantages of structured representations for robust and scalable summarization.

Abstract: Query-focused table summarization requires complex reasoning, often
approached through step-by-step natural language (NL) plans. However, NL plans
are inherently ambiguous and lack structure, limiting their conversion into
executable programs like SQL and hindering scalability, especially for
multi-table tasks. To address this, we propose a paradigm shift to structured
representations. We introduce a new structured plan, TaSoF, inspired by
formalism in traditional multi-agent systems, and a framework, SPaGe, that
formalizes the reasoning process in three phases: 1) Structured Planning to
generate TaSoF from a query, 2) Graph-based Execution to convert plan steps
into SQL and model dependencies via a directed cyclic graph for parallel
execution, and 3) Summary Generation to produce query-focused summaries. Our
method explicitly captures complex dependencies and improves reliability.
Experiments on three public benchmarks show that SPaGe consistently outperforms
prior models in both single- and multi-table settings, demonstrating the
advantages of structured representations for robust and scalable summarization.

</details>


### [106] [Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning](https://arxiv.org/abs/2507.22887)
*Kwesi Cobbina,Tianyi Zhou*

Main category: cs.CL

TL;DR: ICL的性能受演示文稿位置影响。将演示文稿放在提示开头可提高准确性和稳定性，尤其对小型模型。


<details>
  <summary>Details</summary>
Motivation: ICL是一种重要的LLM能力，但其性能受演示文稿选择和顺序影响。本文首次研究了ICL中演示文稿位置的偏差（DPP Bias），即改变演示文稿、系统提示和用户消息在LLM输入中的位置会导致预测和准确率的显著变化。

Method: 通过系统性评估流程，利用准确率变化（ACCURACY-CHANGE）和预测变化（PREDICTION-CHANGE）两个指标，研究ICL中演示文稿位置偏差（DPP）对分类、问答、摘要和推理任务的影响。

Result: 在10个LLM（包括QWEN、LLAMA3、MISTRAL、COHERE）上的广泛实验证实，DPP偏差显著影响了LLM的准确率和预测。将演示文稿放在提示开头可获得最稳定和最准确的输出（准确率提升最高可达6%），而将其放在用户消息末尾则会导致超过30%的预测发生转变，且在QA任务上并不能提高准确率。

Conclusion: LLM的ICL能力对演示文稿（demos）的位置和顺序敏感，将其置于提示的开头可获得最稳定和最准确的输出，最高可提升6个点。小型模型受此偏差影响最大，但大型模型在复杂任务上也会受到轻微影响。

Abstract: In-context learning (ICL) is a critical emerging capability of large language
models (LLMs), enabling few-shot learning during inference by including a few
demonstrations (demos) in the prompt. However, it has been found that ICL's
performance can be sensitive to the choices of demos and their order. This
paper investigates an unexplored new positional bias of ICL for the first time:
we observe that the predictions and accuracy can drift drastically when the
positions of demos, the system prompt, and the user message in LLM input are
varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We
design a systematic evaluation pipeline to study this type of positional bias
across classification, question answering, summarization, and reasoning tasks.
We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify
net gains and output volatility induced by changes in the demos' position.
Extensive experiments on ten LLMs from four open-source model families (QWEN,
LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their
accuracy and predictions: placing demos at the start of the prompt yields the
most stable and accurate outputs with gains of up to +6 points. In contrast,
placing demos at the end of the user message flips over 30\% of predictions
without improving correctness on QA tasks. Smaller models are most affected by
this sensitivity, though even large models remain marginally affected on
complex tasks.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [107] [Local texture of three-stage CVD SiC fibre by precession electron diffraction (PED) and XRD](https://arxiv.org/abs/2507.22073)
*B. Huang,Y. Q. Yang,M. H. Li,Y. X. Chen,X. Luo,M. S. Fu,Y. Chen,Xierong Zeng*

Main category: cond-mat.mtrl-sci

TL;DR: XRD和PED分析表明，SiC纤维在沉积早期晶粒取向随机，后期出现(110)、111。和(110)、115.两种织构，且[110]轴向取向有利于提高拉伸强度。


<details>
  <summary>Details</summary>
Motivation: SiC纤维具有横向各向同性性质，对于其增强金属基复合材料非常重要。

Method: 通过X射线衍射(XRD)和透射电子显微镜(TEM)上的电子衍射(PED)研究了CVD SiC纤维的局部织构。

Result: XRD结果与PED结果一致，发现在沉积的第一阶段，SiC晶粒的优选方向几乎是随机的，晶粒尺寸分布分散。在沉积的第二和第三阶段，SiC纤维有两种织构，即(110)、111。和(110)、115.，晶粒尺寸约为200 nm，且第三阶段的晶粒尺寸低于第二阶段。

Conclusion: SiC纤维的[110]优选取向沿轴向方向有利于轴向拉伸强度。

Abstract: SiC fibre with the transverse isotropic properties is very important to it
reinforced metal matrix composites. In this paper, local texture of the CVD SiC
fibre was investigated by means of X-ray diffraction (XRD) and precession
electron diffraction (PED) on transmission electron microscopy(TEM). The result
from XRD is in agreement with the result obtained from PED. And the result
shown that at the first stage of deposition, the preferred direction of SiC
grains is almost random and the distribution of grain size is scattered. At the
second and third stages of deposition, there are two kinds of texture in SiC
fibre, that is, (110),111. and (110),115.. Furthermore, the grain size at the
second and third stages is about 200 nm and it is lower at the third stage than
at the second stage because of the lower temperature at the third stage. The
[110] preferred direction along axial direction for SiC fibre is beneficial to
the axial tensile strength.

</details>


### [108] [Exploiting solute segregation and partitioning to the deformation-induced planar defects and nano-martensite in designing ultra-strong Co-Ni base alloys](https://arxiv.org/abs/2507.22078)
*Akshat Godha,Mayank Pratap Singh,Karthick Sundar,Shashwat Kumar Mishra,Praveen Kumar,Govind B,Surendra Kumar Makineni*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过热机械加工和溶质调控，成功设计出超强单相合金，其屈服强度超过2 GPa，并在高温下表现出优异的稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有的高浓度单相多元素合金虽然具有高达0.8-1.2 GPa的抗拉强度，但其0.2%屈服强度（YS）非常低，在300-600 MPa的低应力水平下就会发生永久变形。本研究旨在克服这一缺点，设计出具有更高屈服强度的单相合金。

Method: 通过控制热机械加工引入形变诱导的堆垛层错（SFs）、纳米孪晶（NTs）和纳米马氏体 ε-laths（NMLs），随后在中间温度进行回火处理，促进溶质偏析/分配到这些结构上，实现了超强单相合金的设计。

Result: 在Co-33Ni-24Cr合金中，通过45%和65%的冷轧以及600°C回火4小时，得到了1.5 GPa和2 GPa的屈服强度，断裂伸长率分别为14%和7%。经过低温轧制和回火后，屈服强度进一步提高到约2.2 GPa，而断裂伸长率没有降低。该合金在600°C下保持100小时的微观结构稳定，并且在600°C的拉伸测试中，屈服强度仍能保持在1.5 GPa，断裂伸长率为18%。

Conclusion: 通过利用原子尺度的溶质相互作用与变形诱导的结构，设计出了超强单相合金，其屈服强度（YS）大于2 GPa。该合金在600°C下的高温下表现出良好的稳定性，并且在600°C的拉伸测试中仍能保持高屈服强度和延性。

Abstract: Single-phase, multi-elements (three or more) with high concentrations show
exceptional tensile strength up to ~ 0.8-1.2 GPa. However, they possess a very
low 0.2% yield strength (YS), i.e., they can be permanently deformed at very
low-stress levels of 300 to 600 MPa. Here, we reveal by exploiting atomic-scale
solute interactions with the deformation-induced structures to design
ultra-strong single-phase alloys with YS > 2 GPa. This was achieved by
controlled thermomechanical processing that introduces stacking-faults (SFs),
nano-twins (NTs), and nano-martensite {\epsilon}-laths (NMLs) during cold
deformation followed by facilitating solute segregation/partitioning to them by
tempering at intermediate temperature. We demonstrate the phenomena in a low
stacking faulty energy multi-component (face-centered-cubic, fcc structured)
Co-33Ni-24Cr alloy (all in at.%) containing 5at.% Mo as a solute. It is also
shown that the degree of strengthening after tempering scales up with the
fraction of these structures (before tempering) in the alloy microstructure
that can be tuned by the amount and temperature of cold deformation.
Cold-rolling with 45% and 65% thickness reduction, followed by tempering at
600{\deg}C for 4 hours, led to an YS of 1.5 GPa and 2 GPa with elongation to
fracture (%El) 14% and 7%, respectively. The YS is further enhanced to ~ 2.2
GPa without reduction in %El upon cryo-rolling followed by tempering. The alloy
microstructure is stable at 600{\deg}C up to 100 hours and also retains an YS
of ~ 1.5 GPa with %El of 18% during tensile test at 600{\deg}C. The derived
high YS and high-temperature stability are critically a consequence of solute
partitioning to the NMLs that we termed as Solute-Partitioned NMLs (SP-NMLs) in
the microstructure.

</details>


### [109] [Modelling hydrogen storage in metal hydrides](https://arxiv.org/abs/2507.22083)
*Francesc Font,Attila Husar,Tim Myers,Maria Aguareles,Esther Barrabés*

Main category: cond-mat.mtrl-sci

TL;DR: 开发了一个用于氢加载过程的数学模型，并通过引入空间依赖性来解释实验中的温度梯度。


<details>
  <summary>Details</summary>
Motivation: 开发一个用于金属氢化物储罐中氢加载过程的一维数学模型。

Method: 通过对模型进行降阶，并引入空间依赖性到动力学反应常数中，然后进行数值求解。

Result: 模型描述了氢气密度和压力、储罐温度、通过多孔金属结构的平均气体速度以及金属转化为金属氢化物的转化部分的演变。降维表明可以简化方程组，并且密度和转化金属部分可以与温度方程解耦。

Conclusion: 所提出的降阶模型可以通过引入到动力学反应常数中的空间依赖性来解释实验中观察到的意外温度梯度。

Abstract: We develop a one-dimensional mathematical model for the loading process of
hydrogen in a metal hydride tank. The model describes the evolution of the
density and pressure of the hydrogen gas, the temperature of the tank, the
averaged velocity of the gas through the porous metal structure, and the
transformed fraction of metal into a metal hydride. The non-dimensionalisation
of the model indicates a possible reduction of the system of equations and also
shows that the density and the transformed metal fraction may be decoupled from
the temperature equation. The reduced model is solved numerically. Introducing
a spatial dependence into the kinetic reaction constant allows to explain
unexpected temperature gradients observed in experiments.

</details>


### [110] [Understanding the fill-factor limit of organic solar cells](https://arxiv.org/abs/2507.22217)
*Huotian Zhang,Jun Yuan,Tong Wang,Nurlan Tokmoldin,Rokas Jasiunas,Yiting Liu,Manasi Pranav,Yuxuan Li,Xiaolei Zhang,Vidmantas Gulbinas,Safa Shoaee,Yingping Zou,Veaceslav Coropceanu,Artem A. Bakulin,Dieter Neher,Thomas Kirchartz,Feng Gao*

Main category: cond-mat.mtrl-sci

TL;DR: 为了提高有机太阳能电池的效率，研究人员分析了填充因子，发现延长激子寿命和抑制复合是关键。


<details>
  <summary>Details</summary>
Motivation: 为了缩小有机太阳能电池与商业无机太阳能电池和新兴钙钛矿太阳能电池之间的效率差距，提高填充因子至关重要，但目前对其基本原理的理解尚不完整。

Method: 通过结合器件表征、光谱学和理论建模，研究了从0.27到0.80的填充因子器件，重点分析了自由电荷产生和复合对填充因子的影响。

Result: 揭示了斯塔克效应和场相关电荷转移对具有低电压损耗的最先进有机太阳能电池的填充因子有显著影响。

Conclusion: 通过延长激子寿命和抑制复合来提高有机太阳能电池的填充因子是提高效率的有利策略。

Abstract: Although the power conversion efficiencies of organic solar cells (OSCs) have
surpassed 20%, they still lag behind commercial inorganic solar cells and
emerging perovskite solar cells. To bridge this efficiency gap, improving the
fill factor (FF) is critical, provided other photovoltaic parameters are not
compromised. However, the fundamental understanding of the FF in OSCs remains
incomplete. In this work, we systematically investigate a wide range of OSCs
with the FF values spanning 0.27 to 0.80, and analyse the effect of free charge
generation and recombination on the FF in OSCs. To explain our observations, we
developed an analytical model that quantitatively correlates the applied
electric field with the energetics of excited states in donor-acceptor blends.
By combining device characterisation, spectroscopy, and theoretical modelling,
we reveal that the Stark effect and the field-dependent charge transfer
significantly impact the FF in state-of-the-art OSCs with low voltage losses.
Our findings highlight that suppressing geminate decay by increasing exciton
lifetime is a promising strategy for boosting the FF and achieving future
efficiency gains in OSCs.

</details>


### [111] [How transparent is graphene? A surface science perspective on remote epitaxy](https://arxiv.org/abs/2507.22129)
*Zach LaDuca,Anshu Sirohi,Quinn Campbell,Jason K Kawasaki*

Main category: cond-mat.mtrl-sci

TL;DR: Remote epitaxy mechanisms are explored, proposing Fourier and beating analysis to understand graphene's role in substrate potential and surface diffusion tuning.


<details>
  <summary>Details</summary>
Motivation: To understand the fundamental synthesis mechanisms of remote epitaxy, particularly the foundational assumption of graphene transparency, and to address challenges in quantifying the remote substrate potential.

Method: Fourier and beating analysis for decomposing lattice potential contributions.

Result: The study highlights the importance of graphene-induced reconstructions and the role of remote potential in tuning surface diffusion and adatom kinetics, offering insights into navigating the competition between remote epitaxy and defect-seeded mechanisms.

Conclusion: The study proposes Fourier and beating analysis as a bias-free method for decomposing lattice potential contributions, highlighting the importance of graphene-induced reconstructions and the role of remote potential in tuning surface diffusion for remote epitaxy.

Abstract: Remote epitaxy is the synthesis of a single crystalline film on a
graphene-covered substrate, where the film adopts epitaxial registry to the
substrate as if the graphene is transparent. Despite many exciting applications
for flexible electronics, strain engineering, and heterogeneous integration, an
understanding of the fundamental synthesis mechanisms remains elusive. Here we
offer a perspective on the synthesis mechanisms, focusing on the foundational
assumption of graphene transparency. We highlight challenges for quantifying
the strength of the remote substrate potential that permeates through graphene,
and propose Fourier and beating analysis as a bias-free method for decomposing
the lattice potential contributions from the substrate, from graphene, and from
surface reconstructions, each at different frequencies. We highlight the
importance of graphene-induced reconstructions on epitaxial templating, drawing
comparison to moir\'e epitaxy. Finally, we highlight the role of the remote
potential in tuning surface diffusion and adatom kinetics on graphene. Tuning
the surface diffusion length is crucial in navigating the competition between
remote epitaxy and defect-seeded mechanisms like pinhole epitaxy.

</details>


### [112] [Strain effects on the fluctuation properties in noncollinear antiferromagnets: a first-principles and macrospin-based study](https://arxiv.org/abs/2507.22275)
*Mohammad M. Rahman,Farzad Mahfouzi,Matthew W. Daniels,Mark D. Stiles*

Main category: cond-mat.mtrl-sci

TL;DR: DFT研究表明，应变会改变Mn3Sn的反铁磁性，这对于设计用于概率计算的硬件很重要。


<details>
  <summary>Details</summary>
Motivation: 研究外延应变对Mn3Sn这种非共线反铁磁材料磁性涨落特性的影响。

Method: 使用密度泛函理论(DFT)研究应变对Mn3Sn磁矩的影响，并进行宏观自旋模拟。

Result: 应变会显著改变磁各向异性、双交换和双二次交换相互作用，其中双二次交换在定义磁能量和应对应变方面起着关键作用，从而影响磁翻转的能量势垒和磁涨落。

Conclusion: 应变会显著影响Mn3Sn的磁畴壁动力学，进而影响热稳定性。

Abstract: We present a theoretical investigation of epitaxial strain effects on the
magnetic fluctuation properties of Mn$_3$Sn noncollinear antiferromagnets.
Employing density functional theory (DFT), we uncover significant
strain-induced modifications to key magnetic parameters, including magnetic
anisotropy and both bilinear and biquadratic exchange interactions. Our
findings reveal that the biquadratic exchange, often neglected, plays a crucial
role in defining the magnetic energy landscape and its response to strain.
These microscopic changes directly impact the energy barriers governing
magnetic switching, thereby influencing thermal stability and fluctuation
rates. Using macrospin-based simulations based on DFT-derived parameters, we
provide a quantitative analysis of the macroscopic magnetic fluctuations
influenced by these microscopic interactions. These insights are particularly
relevant for applications requiring precisely controlled magnetic behavior,
such as hardware for probabilistic computing.

</details>


### [113] [Steinberg-Guinan strength model for rhenium](https://arxiv.org/abs/2507.22228)
*Damian C. Swift*

Main category: cond-mat.mtrl-sci

TL;DR: A new strength model for Rhenium (Re) was developed for high-pressure simulations, but it underestimates Re's strength at very high pressures, indicating it might be stronger than previously thought.


<details>
  <summary>Details</summary>
Motivation: Rhenium (Re) is used as an x-ray shield in laser-driven material property experiments, and its strength at high pressures is a consideration in the design, modeling, and interpretation of these experiments.

Method: A Steinberg-Guinan (SG) strength model for Re was developed, tailored for high-pressure dynamic loading simulations. Parameters were derived from atom-in-jellium predictions and experimental data. The ambient shear modulus was fixed, the pressure-hardening parameter was fitted to predictions up to 1 TPa, and thermal softening was estimated. Work-hardening parameters were extracted by fitting the model to microhardness measurements.

Result: The SG model for Re captures observed hardening behavior but predicts significantly lower flow stresses at high pressures than diamond anvil cell observations suggest.

Conclusion: Rhenium may exhibit enhanced strength at megabar pressures, and the developed model provides a basis for improved modeling of strength in Re under extreme conditions, suggesting directions for further theoretical and experimental investigation.

Abstract: Rhenium, Re, is used as an x-ray shield in laser-driven material property
experiments, where its strength at high pressures can be a consideration in the
design, modeling, and interpretation. We present a Steinberg-Guinan (SG)
strength model for Re, tailored for use in high-pressure dynamic loading
simulations. Parameters for the SG model were derived from recent
atom-in-jellium predictions of the shear modulus under compression and
experimental data on work-hardening from rolled-bar studies. The ambient shear
modulus was fixed to the measured value, and the pressure-hardening parameter
was fitted to the atom-in-jellium predictions up to 1 TPa. The shear modulus
model was still a reasonable fit beyond 25 TPa. Thermal softening was estimated
from the thermal expansivity and bulk modulus. Work-hardening parameters were
extracted by fitting the model to Knoop microhardness measurements under known
plastic strains. The resulting model captures the observed hardening behavior
but predicts significantly lower flow stresses at high pressures than diamond
anvil cell observations suggest, implying that Re may exhibit enhanced strength
at megabar pressures. These results provide a basis for improved modeling of
strength in Re under extreme conditions and suggest directions for further
theoretical and experimental investigation.

</details>


### [114] [Enhancing interfacial thermal conductance in Si/Diamond heterostructures by phonon bridge](https://arxiv.org/abs/2507.22490)
*Ershuai Yin,Qiang Li,Wenzhu Luo,Lei Wang*

Main category: cond-mat.mtrl-sci

TL;DR: A 'phonon bridge' using a SiC interlayer in Si/Diamond heterostructures significantly enhances thermal transport. An optimal SiC thickness of 40 nm exists, balancing bridging effects and bulk resistance. SiC is the most effective interlayer material among thirteen candidates studied.


<details>
  <summary>Details</summary>
Motivation: To investigate the mechanism of enhancing interfacial thermal transport performance in Silicon/Diamond (Si/Diamond) heterostructures using the phonon bridge, and to provide theoretical guidance for designing heterostructures with enhanced thermal transport performance.

Method: A heat transfer model for three-layer heterostructures is developed by combining First-principles calculations with the Monte Carlo method. The temperature distribution, spectral heat conductance, and interfacial thermal conductance are compared for Si/Diamond heterostructures with and without a silicon carbide (SiC) interlayer. The influence of SiC interlayer thickness is studied, and thirteen candidate interlayer materials are compared at various thicknesses.

Result: The SiC interlayer effectively bridges low-frequency phonons in Si with mid-to-high-frequency phonons in Diamond, significantly improving interfacial phonon transport. For thin interlayers, intensified phonon boundary scattering weakens the bridging effect. Conversely, excessively thick interlayers increase the bulk thermal resistance, reducing overall interfacial thermal conductance. SiC emerges as the most effective interlayer material, increasing interfacial thermal conductance by 46.6%, followed by AlN with a 21.9% improvement.

Conclusion: SiC is the most effective interlayer material, increasing interfacial thermal conductance by 46.6% compared to the bilayer heterostructure. An optimal interlayer thickness of 40 nm for SiC is identified. These findings offer valuable theoretical guidance for designing heterostructures with enhanced thermal transport performance.

Abstract: This study investigates the mechanism of enhancing interfacial thermal
transport performance in Silicon/Diamond (Si/Diamond) heterostructures using
the phonon bridge. A heat transfer model for three-layer heterostructures is
developed by combining First-principles calculations with the Monte Carlo
method. The temperature distribution, spectral heat conductance, and
interfacial thermal conductance are compared for Si/Diamond heterostructures
with and without a silicon carbide (SiC) interlayer. The results show that the
SiC interlayer effectively bridges low-frequency phonons in Si with
mid-to-high-frequency phonons in Diamond, which forms a specific phonon bridge,
significantly improving interfacial phonon transport. The influence of SiC
interlayer thickness is further studied, revealing a size-dependent phonon
bridge enhancement. For thin interlayers, intensified phonon boundary
scattering weakens the bridging effect. Conversely, excessively thick
interlayers increase the bulk thermal resistance, reducing overall interfacial
thermal conductance. Thus, an optimal interlayer thickness exists, identified
as 40 nm for SiC. Thirteen candidate interlayer materials, including SiC, AlN,
{\alpha}-Si3N4, \b{eta}-Si3N4, and AlxGa1-xN (x ranges from 0.1 to 0.9), are
compared at various thicknesses. SiC emerges as the most effective interlayer
material, increasing interfacial thermal conductance by 46.6% compared to the
bilayer heterostructure. AlN ranks second, improving thermal conductance by
21.9%. These findings provide essential insights into the phonon bridge
mechanism at heterogeneous interface thermal transport and offer valuable
theoretical guidance for designing heterostructures with enhanced thermal
transport performance.

</details>


### [115] [Cation Engineering of Cu-Doped CsPbI3: Lead Substitution and Dimensional Reduction for Improved Scintillation Performance](https://arxiv.org/abs/2507.22681)
*David Hadid Sidiq,Somnath Mahato,Tobias Haposan,Michal Makowski,Dominik Kowal,Marcin Eugeniusz Witkowski,Winicjusz Drozdowski,Arramel,Muhammad Danang Birowosuto*

Main category: cond-mat.mtrl-sci

TL;DR: 铜掺杂的 CsPbI3 纳米晶在降低铅毒性的同时，改善了斯托克斯位移、衰减时间和光产额，并且对温度不敏感，为开发无铅闪烁钙钛矿提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 为了解决无机卤化物钙钛矿（特别是 CsPbI3）因含铅而存在的毒性问题，同时保留其优异的光电性能，本研究旨在通过掺杂来降低铅的毒性并优化其闪烁特性。

Method: 通过在 CsPbI3 晶格中用 5% 的铜原子部分取代铅原子，研究了铜掺杂对 CsPbI3 纳米晶的性能影响。分析了掺杂材料的斯托克斯位移、自吸收、衰减时间和温度依赖性，并测量了其光产额。

Result: 铜掺杂的 CsPbI3 表现出比原始材料更大的斯托克斯位移（-67 nm），并且没有发生不良的自吸收。掺杂材料的快速衰减时间（tau_1）约为 0.6 ns。温度依赖性研究表明，其发射强度（中心在 713 ± 16 nm）受温度影响不大。此外，铜掺杂使 CsPbI3 的光产额（LY）从原始材料的光产额增加到 3.0 ± 0.8 photons/keV。

Conclusion: 这项工作通过引入过渡金属（铜）掺杂来调节无机卤化物钙钛矿纳米晶的闪烁特性，为开发无铅闪烁钙钛矿提供了新的方向。

Abstract: To date, inorganic halide perovskite nanocrystals show promising
contributions in emerging luminescent materials due to their high tolerance to
defects. In particular, the development of cesium lead iodide (CsPbI3) has
shown its efficiency for light-harvesting properties. However, further
implementation is hindered due to the toxicity of the lead content. Therefore,
in this study, we introduced Cu atoms to partially substitute Pb atoms (5% Cu)
in the CsPbI3 lattice as a solution to reduce Pb toxicity. A partial lead
material is substituted using Cu displays a larger Stokes shift (-67 nm)
compared to the pristine, and resulted doped CsPbI3 not undergo the undesired
self absorption. An outcome is focused on the champion of fast-component
(tau_1) decay time ~0.6 ns. Temperature-dependent radioluminescence outlines an
incremental change in the emission intensity is marginally centered at 713 +-
16 nm, which indicates Cu-doped CsPbI3 is not greatly affected by temperature.
In addition, we report that the light yield (LY) pristine CsPbI3 after doping
is increased to 3.0 +- 0.8 photons/keV. Our work provides physical insights
into a tunable scintillation property using transition metal doping toward
lead-free based scintillating perovskites.

</details>


### [116] [Bulk Nanostructured Zirconia Ceramics with High Hardness and Toughness via Integration of High-Pressure Torsion and Spark Plasma Sintering](https://arxiv.org/abs/2507.22298)
*Kaveh Edalati,Koji Morita,Shivam Dangwal,Zenji Horita*

Main category: cond-mat.mtrl-sci

TL;DR: 通过HPT和SPS结合处理YSZ陶瓷，成功制备了高硬度、高密度且抗压裂性好的纳米结构陶瓷。


<details>
  <summary>Details</summary>
Motivation: 开发在传统高温烧结过程中难以实现的纳米结构块状陶瓷材料。

Method: 采用高压扭转（HPT）对氧化钇稳定氧化锆（YSZ）进行预处理，随后通过放电等离子烧结（SPS）进行致密化处理，以获得纳米结构块状样品。

Result: 制备的纳米结构YSZ陶瓷具有99%的相对密度（6.07 g/cm³）和1500 Hv的高硬度，并且由于位错的存在，在压痕测试中表现出良好的抗压裂纹能力，而仅通过SPS处理的样品则会出现裂纹。

Conclusion: 通过高压扭转（HPT）和放电等离子烧结（SPS）相结合的方法成功制备了具有细小晶粒尺寸（80 nm）和高密度的纳米结构氧化钇稳定氧化锆（YSZ）陶瓷，并表现出优异的硬度和良好的抗压裂纹能力。

Abstract: Developing nanostructured bulk ceramics is a major challenge when
conventional high-temperature sintering is employed for consolidation. In the
current investigation, yttria-stabilized zirconia (YSZ) with a composition of
ZrO2 - 3 mol% Y2O3 is first treated using high-pressure torsion (HPT) and
further consolidated using spark plasma sintering (SPS) to produce a
nanostructured bulk sample. The material demonstrates phase transformations
from tetragonal to dislocation-decorated monoclinic by HPT and reversely
transforms to the tetragonal phase after the SPS process while maintaining a
mean grain size of 80 nm and large numbers of dislocations. The consolidated
ceramic exhibits a density of 6.07 g/cm3 (99% relative density) with a high
hardness of 1500 Hv, which is reasonably consistent with the prediction of the
Hall-Petch relationship. Examination of the indented areas during the hardness
test confirms the absence of cracks, indicating good fracture toughness (KIC)
because of the presence of dislocations, while the sample processed only by SPS
and without HPT processing forms numerous cracks by indentation and exhibits
low KIC.

</details>


### [117] [Magnetoresistance in the Extreme Quantum Limit: Field-Induced Crossover to the Unitarity Limit](https://arxiv.org/abs/2507.22320)
*Shuto Tago,Akiyoshi Yamada,Yuki Fuseya*

Main category: cond-mat.mtrl-sci

TL;DR:  magnetoresistance (MR) in the extreme quantum limit (EQL) shows insulating behavior with magnetic field perpendicular to current and metallic behavior with parallel current due to scattering rate crossover. A universal relation allows impurity density determination.


<details>
  <summary>Details</summary>
Motivation: To theoretically investigate magnetoresistance (MR) in the extreme quantum limit (EQL) and understand the underlying scattering mechanisms and their impact on electronic properties.

Method: Theoretically investigated magnetoresistance (MR) in the extreme quantum limit (EQL) using the Kubo formula with Green's functions and the T-matrix approximation.

Result: Uncovered a magnetic-field-induced crossover in the scattering rate ($1/\tau \propto B^2$ in the Born regime and $1/\tau \propto B^{-2}$ in the unitarity limit). This leads to linear transverse MR ($\rho_{xx} \propto B$) and negative longitudinal MR ($\rho_{zz} \propto B^{-2}$), implying insulating behavior when the magnetic field is perpendicular to the current and metallic behavior when parallel. Derived a universal relation in the unitarity limit for direct experimental determination of impurity density from $\rho_{xx}$ and $\rho_{xy}$.

Conclusion: The study establishes a quantum-classical correspondence valid even in the extreme quantum limit (EQL) by incorporating the field dependences of the scattering rate and quantum corrections. It reveals distinct magnetoresistance behaviors (linear transverse MR and negative longitudinal MR) arising from a magnetic-field-induced crossover in the scattering rate.

Abstract: We theoretically investigate magnetoresistance (MR) in the extreme quantum
limit (EQL), where the kinetic energy becomes significantly smaller than the
cyclotron energy, using the Kubo formula with Green's functions and the
$T$-matrix approximation. We uncover a magnetic-field-induced crossover in the
scattering rate: $1/\tau \propto B^2$ in the Born regime and $1/\tau \propto
B^{-2}$ in the unitarity limit. This crossover gives rise to distinct MR
behaviors in the EQL, characterized by linear transverse MR ($\rho_{xx} \propto
B$) and negative longitudinal MR ($\rho_{zz} \propto B^{-2}$). This dichotomy
implies insulating behavior when the magnetic field is perpendicular to the
current, and metallic behavior when it is parallel. In the unitarity limit, we
further derive a universal relation that enables direct experimental
determination of the impurity density from $\rho_{xx}$ and $\rho_{xy}$. Our
results establish a quantum--classical correspondence that remains valid even
in the EQL, provided that the field dependences of the scattering rate and
quantum corrections are properly incorporated.

</details>


### [118] [In-Plane Magnetic Anisotropy and Large topological Hall Effect in Self-Intercalated Ferromagnet Cr1.61Te2](https://arxiv.org/abs/2507.22397)
*Yalei Huang,Na Zuo,Zheyi Zhang,Xiangzhuo Xing,Xinyu Yao,Anlei Zhang,Haowei Ma,Chunqiang Xu,Wenhe Jiao,Wei Zhou,Raman Sankar,Dong Qian,Xiaofeng Xu*

Main category: cond-mat.mtrl-sci

TL;DR: Cr1.61Te2单晶表现出强磁各向异性和大的拓扑霍尔效应（THE），在70-240 K范围内达到最大值0.93 μΩ cm，这使其成为未来自旋电子器件应用的有前途的材料。


<details>
  <summary>Details</summary>
Motivation: Cr1+xTe2因其高温铁磁性、可调谐自旋结构和空气稳定性而备受关注，这些特性对于下一代内存和信息技术的各种应用至关重要。本研究旨在Cr1.61Te2单晶中研究磁各向异性和拓扑霍尔效应（THE）。

Method: Self-intercalated Cr1+xTe2 single crystals were studied. Magnetic anisotropy and topological Hall effect (THE) were measured.

Result: 在Cr1.61Te2单晶中观察到强磁各向异性和大的拓扑霍尔效应（THE）。Cr1.61Te2是一种软磁铁，具有强的面内磁各向异性。在不同的温度下，观察到不同的THE行为，反映了复杂的自旋结构和竞争的交换相互作用。在70-240 K的温度范围内，出现了一个由微观非共面自旋结构引起的大的拓扑霍尔电阻率，在150 K时达到最大值0.93 μΩ cm。在低于70 K的低温下，观察到符号反转且较弱的THE，表明出现了具有相反拓扑电荷的额外拓扑自旋结构。

Conclusion: 这项工作不仅为理解Cr1+xTe2系统中磁晶andisotropy和拓扑现象之间的相关性提供了宝贵的见解，而且还为工程化可用于各种自旋电子器件应用的复杂自旋纹理的演变提供了一个强大的平台。

Abstract: Self-intercalated chromium tellurides Cr1+xTe2 have garnered growing
attention due to their high-temperature ferromagnetism, tunable spin structures
and air stability, all of which are vital for versatile applications in
next-generation memory and information technology. Here, we report strong
magnetic anisotropy and a large topological Hall effect (THE) in
self-intercalated Cr1.61Te2 single crystals, which are both highly desirable
properties for future spintronic applications. Our results demonstrate that
Cr1.61Te2 is a soft ferromagnet with strong in-plane magnetic anisotropy.
Remarkably, distinct THE behaviors are observed in different temperature
regimes, reflecting the intricate spin structures and competing exchange
interactions. More interestingly, a large topological Hall resistivity, induced
by microscopic non-coplanar spin structures, emerges in the temperature range
70-240 K, reaching a maximum value of 0.93 {\mu}{\Omega} cm at 150 K. Moreover,
a sign-reversed and weak THE is observed at low temperatures below ~70 K,
indicating the emergence of an additional topological spin structure with
opposite topological charges. This work not only offers valuable insights into
the correlation between magnetocrystalline anisotropy and topological phenomena
in Cr1+xTe2 systems, but also provides a robust platform for engineering the
evolution of complex spin textures that can be leveraged in diverse spintronic
device applications.

</details>


### [119] [Phase Competition and Rutile Phase Stabilization of Growing GeO2 Films by MOCVD](https://arxiv.org/abs/2507.22430)
*Imteaz Rahaman,Botong Li,Hunter D. Ellis,Kathy Anderson,Feng Liu,Michael A. Scarpulla,Kai Fu*

Main category: cond-mat.mtrl-sci

TL;DR: 通过种子驱动分步结晶（SDSC）生长策略，克服了多晶型竞争，成功获得了纯相r-GeO2薄膜，为亚稳态相的薄膜工程提供了新方法。


<details>
  <summary>Details</summary>
Motivation: 金红石锗二氧化物（r-GeO2）是一种超宽带隙半导体，具有双极掺杂的潜力，是下一代电力电子和光电电子的有希望的候选者。然而，通过金属有机化学气相沉积（MOCVD）等气相技术生长纯相r-GeO2薄膜具有挑战性，因为存在非晶态和石英GeO2的多晶型竞争。

Method: 提出了一种名为种子驱动分步结晶（SDSC）的生长策略，这是一种分段生长策略，用于在r-TiO2（001）衬底上获得r-GeO2薄膜。SDSC将生长分为重复的薄膜沉积和冷却-加热斜坡循环，以抑制非金红石相。

Result: SDSC生长策略能够获得连续、纯相、部分外延的r-GeO2（001）薄膜，其X射线摇摆曲线半峰全宽（FWHM）为597弧秒。

Conclusion: SDSC提供了一种可推广的途径，用于选择性气相生长亚稳态或不稳定相，为相选择性薄膜工程提供了新的机会。

Abstract: Rutile germanium dioxide (r-GeO2) is an ultra-wide bandgap semiconductor with
potential for ambipolar doping, making it a promising candidate for
next-generation power electronics and optoelectronics. Growth of phase-pure
r-GeO2 films by vapor phase techniques like metalorganic chemical vapor
deposition (MOCVD) is challenging because of polymorphic competition from
amorphous and quartz GeO2. Here, we introduce seed-driven stepwise
crystallization (SDSC) as a segmented growth strategy for obtaining r-GeO2
films on r-TiO2 (001) substrate. SDSC divides the growth into repeated cycles
of film deposition and cooling-heating ramps, which suppress the non-rutile
phases. We discuss the underlying mechanisms of phase selection during SDSC
growth. We demonstrate continuous, phase-pure, partially epitaxial r-GeO2 (001)
films exhibiting x-ray rocking curves with a FWHM of 597 arcsec. SDSC-based
growth provides a generalizable pathway for selective vapor-phase growth of
metastable or unstable phases, offering new opportunities for phase-selective
thin-film engineering.

</details>


### [120] [Spray flame synthesis of Y2O3-MgO nanoparticles for mid-infrared transparent nanocomposite ceramics](https://arxiv.org/abs/2507.22443)
*Shuting Lei,Yiyang Zhang,Xing Jin,Yanan Li,Zhu Fang,Shuiqing Li*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究利用喷雾火焰合成法制备了Y2O3-MgO纳米粉体，发现通过控制火焰温度和使用O2作为分散气体，可以大幅提高MgO在Y2O3中的固溶度。优化后的立方相Y2O3-MgO陶瓷在中红外区域具有出色的透过性能。


<details>
  <summary>Details</summary>
Motivation: 尽管喷雾火焰合成法在制备Y2O3-MgO纳米粉体方面具有潜力，但火焰合成参数如何影响粉体特性及后续陶瓷性能仍需深入理解，特别是MgO在Y2O3中的固溶度及其对陶瓷光学性能的影响。

Method: 采用喷雾火焰合成法制备Y2O3-MgO纳米粉体，并系统研究了前驱体化学、火焰温度、分散气体（O2）、粉末解团聚方法、真空烧结温度、热压烧结温度以及初始粉末特性等因素对陶瓷微观结构和红外透过率的影响。

Result: 研究发现，在O2作为分散气体的高温条件下，MgO在Y2O3中的固溶度可达50 mol%，远高于平衡溶解度。氧气合成的、以单斜相为主的粉体所制备的陶瓷具有较高的近红外透过率（1550 nm处为56.2%），但存在开裂问题。优化后，以纯立方相粉体制备的陶瓷无开裂，中红外透过率表现优异（3-5 μm范围内平均透过率为82.3%）。

Conclusion: 研究表明，氧气作为分散气体的高温条件能够实现高达50 mol%的MgO在Y2O3中的完全溶解，远超平衡溶解度。通过优化烧结工艺，以纯立方相粉体制备的陶瓷具有出色的中红外透过率（3-5 μm范围内最高可达84.6%），优于单斜为主的粉体制备的陶瓷。

Abstract: Spray flame synthesis offers a promising method for scalable production of
homogeneously mixed Y2O3-MgO nanopowders as next-generation
infrared-transparent window material, which has attracted significant attention
owing to its excellent optical properties at high temperatures. However,
systematic understanding of how flame synthesis parameters influence particle
morphology, crystal phase, solid solubility, and subsequent ceramic performance
remains insufficiently understood. In this study, we investigated the influence
of precursor chemistry on particle crystal phase and examined the solid
solubility of MgO in Y2O3 under different flame temperatures, demonstrating
that the high-temperature conditions with O2 as dispersion gas allow up to 50
mol% MgO to fully dissolve into Y2O3, far exceeding the equilibrium solubility
limit of 7 mol% at the eutectic temperature (2100{\deg}C) and near-zero at room
temperature. Furthermore, we systematically evaluated how powder
characteristics and sintering parameters-including powder deagglomeration
methods, vacuum sintering temperature, hot isostatic pressing (HIP)
temperature, and initial powder characteristics-affect ceramic microstructures
and infrared transmittance. Despite cracking induced by phase transformation
and finer particle sizes, ceramics fabricated from oxygen-synthesized
monoclinic-dominated powders exhibited superior near-infrared transmittance
(56.2% at 1550 nm), attributed to enhanced atomic mixing and effective grain
boundary pinning. After optimization, pure cubic phase powders produced intact
and crack-free ceramics with outstanding mid-infrared transparency, achieving a
maximum transmittance of 84.6% and an average transmittance of 82.3% in 3-5 um
range.

</details>


### [121] [Unconventional spin texture driven by higher-order spin-orbit interactions](https://arxiv.org/abs/2507.22475)
*Jiaxuan Wu,Boyun Zeng,Hanghui Chen*

Main category: cond-mat.mtrl-sci

TL;DR: 在 LaWN3 中发现了一种由七阶 Weyl 自旋-轨道相互作用控制的异常自旋纹理，这可能对未来的自旋电子学器件有影响。


<details>
  <summary>Details</summary>
Motivation: 自旋分裂和由此产生的自旋纹理是新兴自旋电子器件的核心。在包含重元素的非中心性非磁性材料中，自旋纹理通常由低阶、依赖于动量的自旋-轨道相互作用控制，例如具有晶体动量线性或三次阶数的 Rashba 自旋-轨道相互作用。

Method: 使用的从头算方法、群论分析和 k·p 扰动模型。

Result: 在 LaWN3 的导带中发现了一种以前未识别的自旋纹理，包括一个中心对称的涡旋和六个位于布里渊区中心附近的非高对称点的新涡旋和反涡旋。此外，通过将群论分析和 k·p 扰动模型相结合，证明了由 ferroelectric LaWN3 属于的 C3v 点群限制的七阶 Weyl 自旋-轨道相互作用对于重现与第一性原理计算观察到的非常规自旋结构是必不可少的。此外，还发现 LaWN3 的弱电子掺杂会导致费米表面，其自旋箭头轮廓呈现出不寻常的外摆线图案，这是一个可实验访问的独特信号。

Conclusion: 这项工作表明，高阶自旋-轨道相互作用在影响非中心对称材料的自旋纹理方面可以发挥主导作用。我们的结果为设计利用多手性自旋纹理的自旋电子器件开辟了新的途径，超越了传统的自旋-轨道范式。

Abstract: Spin splitting and the resulting spin texture are central to emerging
spintronic applications. In non-centrosymmetric non-magnetic materials
containing heavy elements, spin textures are typically governed by low-order,
momentum-dependent spin-orbit interactions, such as Rashba spin-orbit
interaction with linear or cubic order in crystal momentum. In this work, we
use \textit{ab initio} calculations to reveal a previously unidentified spin
texture in the conduction bands of a prototypical ferroelectric nitride
LaWN$_3$. In addition to the usual $\Gamma$-centered vortex, we find six new
vortices and anti-vortices located at non-high-symmetry points near the
Brillouin zone center. Furthermore, by combining group-theoretical analysis and
$\textbf{k}\cdot\textbf{p}$ perturbation modeling, we show that, constrained by
the $C_{3v}$ point group to which ferroelectric LaWN$_3$ belongs, a 7th-order
Weyl spin-orbit interaction is essential to reproduce the unconventional spin
structure observed in first-principles calculations. We also find that weak
electron doping of LaWN$_3$ leads to a Fermi surface whose spin-arrow contour
exhibits an unusual epicycloid pattern--a distinctive signature that is
experimentally accessible. Our work demonstrates that higher-order spin-orbit
interactions are more than perturbative corrections. They can play a dominant
role in shaping the spin texture of non-centrosymmetric materials. Our results
open up new avenues for designing spintronic devices that exploit multi-chiral
spin textures beyond the conventional spin-orbit paradigm.

</details>


### [122] [Influence of Built-in Electric Fields on the Optoelectronic and Catalytic Properties of Two-Dimensional Materials](https://arxiv.org/abs/2507.22487)
*Kai Kong,Qiang Wang,Yixuan Li,Yitong Liang*

Main category: cond-mat.mtrl-sci

TL;DR: 二维铁电材料具有优越的垂直极化特性，在电子器件领域前景广阔。


<details>
  <summary>Details</summary>
Motivation: 探索二维铁电材料在电子器件中的应用，特别是利用其垂直极化特性。

Method: 本文未提供具体方法。

Result: 二维铁电材料展现出高能量密度、快速开关和可扩展性等特性，适用于下一代电子设备。

Conclusion: 二维铁电材料因其独特的垂直极化特性，在数据存储和电子设备领域具有巨大潜力。

Abstract: In the realm of modern materials science and advanced electronics,
ferroelectric materials have emerged as a subject of great intrigue and
significance, chiefly due to their remarkable property of reversible
spontaneous polarization. This unique characteristic is not just an interesting
physical phenomenon; it plays a pivotal role in revolutionizing multiple
technological applications, especially in the domains of high-density data
storage and the pursuit of fast device operation. In the past few decades,
there has been a significant increase in the number of investigations and
commercial applications proposed for ferroelectric materials. With the
continuous miniaturization of electronic devices and the rapid development of
two-dimensional (2D) materials, considerable efforts have been made towards
exploring ferroelectricity in 2D materials, driven by the potential for
revolutionary advancements in energy storage, data processing, and other
emerging technologies. This exploration is fueled by the realization that 2D
ferroelectric materials could offer unique properties such as high energy
density, fast switching speeds, and scalability, which are crucial for the next
generation of electronic devices. The out-of-plane (OOP) ferroelectricity
exhibited by these 2D materials is generally more advantageous than the
in-plane ferroelectricity, primarily because the vertical polarizability aligns
more seamlessly with the requirements of most practical technological
applications

</details>


### [123] [Electronic Structure of Bimetallic CoRu Catalysts Modulates SWCNT Nucleation](https://arxiv.org/abs/2507.22517)
*Alister J. Page,Dan Villamanca,Placidus B. Amama,Ben McLean*

Main category: cond-mat.mtrl-sci

TL;DR: Simulations show Ru in CoRu catalysts hinders SWCNT growth by weakening C-H activation and stabilizing carbon chains, with Co being the active component and Ru indirectly affecting catalysis through electronic modifications.


<details>
  <summary>Details</summary>
Motivation: To understand the effect of varying Ru loading in CoRu catalysts on the nucleation of SWCNTs via methane CVD and to elucidate the mechanism behind Ru's influence.

Method: Quantum chemical molecular dynamics simulations were used to model the nucleation of single-walled carbon nanotubes (SWCNTs) on CoRu bimetallic nanoparticles during chemical vapour deposition (CVD) of methane.

Result: Increasing Ru loading in CoRu catalysts decreases catalytic efficiency for SWCNT nucleation. Specifically, Ru impedes C-H bond activation, prolongs the lifetime of key reactive intermediates, and stabilizes longer carbon chains in the early stages of nucleation. Analysis revealed that Co exclusively drives methane decomposition, while Ru indirectly influences the process by altering the catalyst's electronic structure (lowering the Fermi level) through its d-band electronic states.

Conclusion: Ru's presence hinders SWCNT nucleation by impeding C-H bond activation and stabilizing longer carbon chains, despite the catalyst adopting Ru-Co core-shell or segregated structures. Co exclusively drives methane decomposition, and Ru's influence stems from lowering the catalyst's Fermi level via its electronic structure, consistent with d-band theory.

Abstract: Nucleation of single-walled carbon nanotubes (SWCNTs) via chemical vapour
deposition of methane on CoRu bimetallic nanoparticles is simulated using
quantum chemical molecular dynamics. By varying the Ru loading in the catalyst,
we show that Ru decreases catalytic efficiency; C-H bond activation is impeded,
key reactive intermediate species become longer-lived on the catalyst surface,
and longer carbon chains are stabilised through the earliest stages of SWCNT
nucleation. Analysis of the CoRu nanoparticle structure during the CVD process
shows that this influence of Ru is indirect, with the catalyst adopting Ru-Co
core-shell or segregated structures throughout nucleation, and Co exclusively
driving the catalytic decomposition of the methane precursor. We show that the
influence of Ru occurs via the electronic structure of the catalyst itself, by
lowering the Fermi level of the catalyst due to lower energy 4d/5s states, in a
manner consistent with d-band theory.

</details>


### [124] [aLLoyM: A large language model for alloy phase diagram prediction](https://arxiv.org/abs/2507.22558)
*Yuna Oikawa,Guillaume Deffrennes,Taichi Abe,Ryo Tamura,Koji Tsuda*

Main category: cond-mat.mtrl-sci

TL;DR: aLLoyM 是一个针对合金相图优化的 LLM，能回答选择题并生成新相图，加速材料发现。


<details>
  <summary>Details</summary>
Motivation: LLM 在材料科学领域有着广泛的应用前景。本研究旨在开发一个专门针对合金成分、温度和相信息进行训练的 LLM，以期加速新材料的发现。

Method: 该研究使用 CPDDB 和 CALPHAD 评估，对二元和三元相图的问答对进行了整理。研究人员对开源 LLM Mistral 进行了微调，以适应两种不同的问答格式：选择题和短答题。

Result: aLLoyM 在多项选择题的相图问题上表现出显著的性能提升。此外，aLLoyM 的短回答模型能够根据其组成部分生成新的相图。

Conclusion: aLLoyM 是一个在合金成分、温度及其相应的相信息上进行微调的大型语言模型（LLM），它在回答多项选择题的相图问题方面表现出色。aLLoyM 的短回答模型能够仅根据其组成部分生成新的相图，这有望加速新材料系统的发现。

Abstract: Large Language Models (LLMs) are general-purpose tools with wide-ranging
applications, including in materials science. In this work, we introduce
aLLoyM, a fine-tuned LLM specifically trained on alloy compositions,
temperatures, and their corresponding phase information. To develop aLLoyM, we
curated question-and-answer (Q&A) pairs for binary and ternary phase diagrams
using the open-source Computational Phase Diagram Database (CPDDB) and
assessments based on CALPHAD (CALculation of PHAse Diagrams). We fine-tuned
Mistral, an open-source pre-trained LLM, for two distinct Q&A formats:
multiple-choice and short-answer. Benchmark evaluations demonstrate that
fine-tuning substantially enhances performance on multiple-choice phase diagram
questions. Moreover, the short-answer model of aLLoyM exhibits the ability to
generate novel phase diagrams from its components alone, underscoring its
potential to accelerate the discovery of previously unexplored materials
systems. To promote further research and adoption, we have publicly released
the short-answer fine-tuned version of aLLoyM, along with the complete
benchmarking Q&A dataset, on Hugging Face.

</details>


### [125] [Broadband ferromagnetic resonance in Ni-Mn-Ga single crystal](https://arxiv.org/abs/2507.22600)
*Luděk Kraus,Denys Musiienko,Martin Kempa,Jaroslav Čapek*

Main category: cond-mat.mtrl-sci

TL;DR: 单晶Ni$_{50}$Mn$_{28.1}$Ga$_{21.9}$在马氏体-奥氏体相变时，磁晶各向异性的剧烈变化引起共振磁场的大幅改变。


<details>
  <summary>Details</summary>
Motivation: 研究单晶Ni$_{50}$Mn$_{28.1}$Ga$_{21.9}$在马氏体-奥氏体相变过程中磁特性的变化，特别是磁晶各向异性对共振磁场的影响。

Method: 通过宽带铁磁共振（FMR）测量。

Result: 发现了磁晶各向异性在相变过程中存在数量级变化，导致共振磁场急剧变化。单变体马氏体相中，共振场满足Kittel条件 (g=2.0)，且在特定条件下（磁场平行于易磁轴）共振频率大于22 GHz。多变体马氏体相中，考虑磁耦合可满足Kittel拟合。奥氏体相表现出较弱的磁晶各向异性。

Conclusion: 该研究通过宽带铁磁共振研究了单晶Ni$_{50}$Mn$_{28.1}$Ga$_{21.9}$在马氏体-奥氏体相变过程中的磁特性，发现磁晶各向异性的显著变化导致共振磁场急剧变化。在单变体马氏体相中，共振场满足薄膜的Kittel共振条件，且g=2.0。当磁场平行于单变体马氏体的易磁轴时，只有在22 GHz以上频率才能观察到共振。对于多变体马氏体，可以通过考虑孪晶变体的磁耦合来满足Kittel拟合。研究还发现奥氏体相中存在较弱的磁晶各向异性。

Abstract: We present a broadband ferromagnetic resonance study in single crystalline
Ni$_{50}$Mn$_{28.1}$Ga$_{21.9}$ in the temperature range from room temperature
to 120 {\deg}C in which the transformation from martensite to austenite phase
takes place. Our measurements demonstrate that a large change (an order of
magnitude) in the magnetocrystalline anisotropy at the martensitic phase
transformation results in a sharp change of the resonance magnetic field. In a
single variant martensite phase, the resonance fields satisfy the Kittel's
resonance condition for a thin film with the gyromagnetic factor g = 2.0. With
the magnetic field parallel to the easy c-axis of the single variant
martensite, the resonance is observed only for frequencies larger than 22 GHz.
For the multivariant martensite case, the magnetic coupling between the twin
variants can be taken into account for the satisfactory Kittel's fit. We
observe a weak magnetocrystalline anisotropy in the austenite phase, just above
the reverse martensite transformation, comparable to the previous reports based
on different magnetic measurements.

</details>


### [126] [Inducing ferromagnetism by structural engineering in a strongly spin-orbit coupled oxide](https://arxiv.org/abs/2507.22638)
*Ji Soo Lim,Carmine Autieri,Merit Spring,Martin Kamp,Amar Fakhredine,Pavel Potapov,Daniel Wolf,Sergii Pylypenko,Axel Lubk,Johannes Schultz,Nicolas Perez,Börge Mehlhorn,Louis Veyrat,Mario Cuoco,Fadi Choueikan,Philippe Ohresser,Bernd Büchner,Giorgio Sangiovanni,Ralph Claessen,Michael Sing*

Main category: cond-mat.mtrl-sci

TL;DR: 在SrTiO3 (111) 衬底上通过结构工程实现了5d SrIrO3薄膜的铁磁性，在低温下表现出反常霍尔效应和磁滞现象，为开发高性能自旋电子器件提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 磁性材料的强自旋-轨道耦合（SOC）对于发展能够实现高效自旋-电荷转换、复杂磁结构、自旋-谷物理、拓扑相和其他奇异现象的自旋电子器件至关重要。5d过渡金属氧化物（如SrIrO3）具有大的SOC，但通常表现出顺磁行为。

Method: 通过在SrTiO3 (111) 衬底上生长SrIrO3薄膜，并利用衬底诱导的结构工程来实现三层单元沿[111]方向的锯齿形堆叠。

Result: 在5d SrIrO3薄膜中发现了铁磁性，该薄膜在~30 K以下表现出反常霍尔效应，在7 K以下表现出霍尔电导率滞后现象，证实了铁磁有序。X射线磁圆二色性也支持了这些结果。理论分析表明，IrO6八面体网络的结构工程增强了费米能级处的态密度，从而稳定了斯通纳铁磁性。

Conclusion: 通过结构工程可以稳定5d过渡金属氧化物的铁磁性，从而为自旋电子器件提供新的可能性。

Abstract: Magnetic materials with strong spin-orbit coupling (SOC) are essential for
the advancement of spin-orbitronic devices, as they enable efficient
spin-charge conversion, complex magnetic structures, spin-valley physics,
topological phases and other exotic phenomena. 5d transition-metal oxides such
as SrIrO3 feature large SOC, but usually show paramagnetic behavior due to
broad bands and a low density of states at the Fermi level, accompanied by a
relatively low Coulomb repulsion. Here, we unveil ferromagnetism in 5d SrIrO3
thin films grown on SrTiO3 (111). Through substrate-induced structural
engineering, a zigzag stacking of three-unit-cell thick layers along the [111]
direction is achieved, stabilizing a ferromagnetic state at the interfaces.
Magnetotransport measurements reveal an anomalous Hall effect below ~30 K and
hysteresis in the Hall conductivity below 7 K, indicating ferromagnetic
ordering. X-ray magnetic circular dichroism further supports these results.
Theoretical analysis suggests that the structural engineering of the IrO6
octahedral network enhances the density of states at the Fermi level and thus
stabilizes Stoner ferromagnetism. This work highlights the potential of
structurally engineered 5d oxides for spin-orbitronic devices, where efficient
control of SOC-induced magnetic phases by electric currents can lead to lower
energy consumption and improved performance in next-generation device
technologies.

</details>


### [127] [Wafer-scale Programmed Assembly of One-atom-thick Crystals](https://arxiv.org/abs/2507.22677)
*Seong-Jun Yang,Ju-Hyun Jung,Eunsook Lee,Edmund Han,Min-Yeong Choi,Daesung Jung,Shinyoung Choi,Jun-Ho Park,Dongseok Oh,Siwoo Noh,Ki-Jeong Kim,Pinshane Y. Huang,Chan-Cuk Hwang,Cheol-Joo Kim*

Main category: cond-mat.mtrl-sci

TL;DR: 通过程序化晶体组装（PCA）技术，实现了大规模、高质量的石墨烯和hBN薄膜制备，并成功应用于器件制造，实现了性能的精确调控。


<details>
  <summary>Details</summary>
Motivation: 解决原子级清洁组装材料在大规模和可重复性方面的挑战，以实现前所未有的薄膜设计。

Method: 通过范德瓦尔斯相互作用辅助的程序化晶体组装（PCA），精确控制层分辨的成分和面内晶体取向，以定制薄膜的原子构型。

Result: 制备了具有可调电阻的隧道器件阵列（通过单原子精度的hBN厚度控制），以及具有可编程电子能带结构和晶体对称性的扭曲多层石墨烯。

Conclusion: 该研究通过范德瓦尔斯相互作用辅助的程序化晶体组装（PCA），实现了石墨烯和单层六方氮化硼（ML hBN）的晶圆级薄膜制备，实现了近乎统一的产率和原始界面。

Abstract: Crystalline films offer various physical properties based on the modulation
of their thicknesses and atomic structures. The layer-by-layer assembly of
atomically thin crystals provides powerful means to arbitrarily design films at
the atomic-level, which are unattainable with existing growth technologies.
However, atomically-clean assembly of the materials with high scalability and
reproducibility remains challenging. We report programmed crystal assembly
(PCA) of graphene and monolayer hexagonal boron nitride (ML hBN), assisted by
van der Waals interactions, to form wafer-scale films of pristine interfaces
with near-unity yield. The atomic configurations of the films are tailored with
layer-resolved compositions and in-plane crystalline orientations. We
demonstrate batch-fabricated tunnel device arrays with modulation of the
resistance over orders of magnitude by thickness-control of the hBN barrier
with single-atom precision, and large-scale, twisted multilayer graphene with
programmable electronic band structures and crystal symmetries. Our results
constitute an important development in the artificial design of large-scale
films.

</details>


### [128] [Enhanced Biaxial Compressive Strain Tuning of 2D semiconductors via Hot Dry Transfer on Polymer Substrates](https://arxiv.org/abs/2507.22806)
*Alvaro Cortes-Flores,Eudomar Henríquez-Guerra,Lisa Almonte,Hao Li,Andres Castellanos-Gomez,M. Reyes Calvo*

Main category: cond-mat.mtrl-sci

TL;DR: 通过在低温下预拉伸聚合物基底，成功在单层WS$_{2}$中诱导了更大的压缩应变，并观察到显著的激子能量蓝移。低温提高了应变转移效率。


<details>
  <summary>Details</summary>
Motivation: 研究用于低温条件下应变工程的技术，特别是利用聚合物基底的热失配来诱导双轴压缩应变，以调控二维材料的量子现象。

Method: 使用热-干转移方法，在100 $^	ext{o}$C下将单层WS$_{2}$沉积在热膨胀的聚合物基底上，通过基底在冷却过程中收缩诱导压缩应变。

Result: 在低温（5 K）下，通过预拉伸方法在单层WS$_{2}$中诱导出高达约1.7%的双轴压缩应变，导致激子能量蓝移约200 meV。在低温下，应变转移效率显著提高，接近理论极限。

Conclusion: 通过在低温下使用预拉伸方法，在单层WS2中实现了前所未有的均匀双轴压缩应变，并在低温下观察到激子能量显著的蓝移，同时发现应变转移效率与温度密切相关，在5 K时接近理论极限。

Abstract: Strain engineering is an effective tool for tailoring the properties of
two-dimensional (2D) materials, especially for tuning quantum phenomena. Among
the limited methods available for strain engineering under cryogenic
conditions, thermal mismatch with polymeric substrates provides a simple and
affordable strategy to induce biaxial compressive strain upon cooling. In this
work, we demonstrate the transfer of unprecedentedly large levels of uniform
biaxial compressive strain to single-layer WS$_2$ by employing a pre-straining
approach prior to cryogenic cooling. Using a hot-dry-transfer method,
single-layer WS$_2$ samples were deposited onto thermally expanded polymeric
substrates at 100 $^\circ$C. As the substrate cools to room temperature, it
contracts, inducing biaxial compressive strain (up to ~0.5%) in the WS$_2$
layer. This pre-strain results in a measurable blueshift in excitonic energies
compared to samples transferred at room temperature, which serve as control
(not pre-strained) samples. Subsequent cooling of the pre-strained samples from
room temperature down to 5 K leads to a remarkable total blueshift of ~200 meV
in the exciton energies of single-layer WS$_2$. This energy shift surpasses
previously reported values, indicating superior levels of biaxial compressive
strain induced by the accumulated substrate contraction of ~1.7%. Moreover, our
findings reveal a pronounced temperature dependence in strain transfer
efficiency, with gauge factors approaching theoretical limits for ideal strain
transfer at 5 K. We attribute this enhanced efficiency to the increased Young's
modulus of the polymeric substrate at cryogenic temperatures.

</details>


### [129] [Density-functional theory study of the interaction between NV$^{-}$ centers and native defects in diamond](https://arxiv.org/abs/2507.22683)
*Gabriel I. López-Morales,Joanna M. Zajac,Tom Delord,Carlos A. Meriles,Cyrus E. Dreyer*

Main category: cond-mat.mtrl-sci

TL;DR: NV$^{-}$传感器对金刚石内缺陷敏感，可以通过量子计算量化其影响，并用于识别缺陷类型。


<details>
  <summary>Details</summary>
Motivation: NV$^{-}$作为一种纳米传感器，其对外部环境（如电磁场）的敏感性使其也容易受到金刚石主体中缺陷的影响，因此需要量化这些缺陷的影响。

Method: 结合量子嵌入和密度泛函理论计算，对NV$^{-}$的应变和电场敏感性进行建模，并外推到微米尺度，以量化原生缺陷的影响。

Result: 单碳间隙和空位缺陷在200nm范围内会引起可测量的应变，影响NV$^{-}$的光学性质；带电（或中性）缺陷在1微米（或100nm）范围内会产生可测量的电场影响NV$^{-}$。通过测量多个NV$^{-}$中心可以确定缺陷的性质和电荷态。

Conclusion: NV$^{-}$的性质会受到周围原生缺陷（如碳间隙和空位）的影响，这些影响可以通过结合量子嵌入和密度泛函理论计算来量化。通过这些方法，可以确定缺陷的性质和电荷态。

Abstract: The NV$^{-}$ color center in diamond has been demonstrated as a nanoscale
sensor for quantum metrology. However, the properties that make it ideal for
measuring, e.g., minute electric and magnetic fields also make it sensitive to
imperfections in the diamond host. In this work, we quantify the impact of
nearby native defects on the many-body states of NV$^{-}$. We combine previous
quantum embedding results of strain and electric-field susceptibilities of
NV$^{-}$ with density-functional theory calculations on native defects. The
latter are used to parametrize continuum models in order to extrapolate the
effects of native defects up to the micrometer scale. We show that under ideal
measuring conditions, the optical properties of NV$^{-}$ are measurably
affected by the strain caused by single carbon interstitials and vacancies up
to 200 nm away; in contrast, the NV$^{-}$ is measurably affected by the
electric field of such charged (neutral) native defects within a micron (100
nm). Finally, we show how measuring multiple individual NV$^{-}$ centers in the
vicinity of a native defect can be used to determine the nature of the defect
and its charge state.

</details>


### [130] [Thermodynamically driven tilt grain boundaries of monolayer crystals using catalytic liquid alloys](https://arxiv.org/abs/2507.22689)
*Min-Yeong Choi,Chang-Won Choi,Dong-Yeong Kim,Moon-Ho Jo,Yong-Sung Kim,Si-Young Choi,Cheol-Joo Kim*

Main category: cond-mat.mtrl-sci

TL;DR: 通过VLS生长和液态合金催化剂，成功控制了MoS2晶界缺陷，提高了光致发光强度。


<details>
  <summary>Details</summary>
Motivation: 精确控制单分子层MoS2晶界处的原子缺陷，以提高其光致发光性能。

Method: 通过汽-液-固（VLS）生长技术，利用钠钼酸盐液态合金作为生长催化剂，精确控制单分子层MoS2晶界（GBs）处的原子缺陷。

Result: 实现了Mo极性5|7缺陷的高产率（超过95%），显著增强了光致发光（PL）强度，并通过密度泛函理论计算解释了增强机制（抑制了带电激子与吸附在S 5|7缺陷上的Na元素的供体型缺陷的非辐射复合）。

Conclusion: 使用钠钼酸盐液态合金作为生长催化剂，通过汽-液-固（VLS）生长技术精确控制了单分子层MoS2晶界（GBs）处的原子缺陷，成功实现了Mo极性5|7缺陷的高产率（超过95%）。这种方法抑制了非辐射复合，从而显著增强了光致发光（PL）强度，为调控二维材料性质提供了技术线索。

Abstract: We report a method to precisely control the atomic defects at grain
boundaries (GBs) of monolayer MoS2 by vapor-liquid-solid (VLS) growth using
sodium molybdate liquid alloys, which serve as growth catalysts to guide the
formations of the thermodynamically most stable GB structure. The Mo-rich
chemical environment of the alloys results in Mo-polar 5|7 defects with a yield
exceeding 95%. The photoluminescence (PL) intensity of VLS-grown
polycrystalline MoS2 films markedly exceeds that of the films exhibiting
abundant S 5|7 defects, which are kinetically driven by vapor-solid-solid
growths. Density functional theory calculations indicate that the enhanced PL
intensity is due to the suppression of non-radiative recombination of charged
excitons with donor-type defects of adsorbed Na elements on S 5|7 defects.
Catalytic liquid alloys can aid in determining a type of atomic defect even in
various polycrystalline 2D films, which accordingly provides a technical clue
to engineer their properties.

</details>


### [131] [Coherence of dipole-forbidden Rydberg excitons in Cu$_2$O measured by polarization- and time-resolved multi-photon spectroscopy](https://arxiv.org/abs/2507.22717)
*A. Farenbruch,N. V. Siverin,G. Uca,D. Fröhlich,D. R. Yakovlev,M. Bayer*

Main category: cond-mat.mtrl-sci

TL;DR: 介绍了一种新的2PE-DFG技术，用于研究ED禁止激子的相干动力学，并展示了其在Cu2O晶体中的应用，揭示了激子相干时间的特性。


<details>
  <summary>Details</summary>
Motivation: 为了研究ED-禁止激子态的长寿命相干性，提出了2PE-DFG技术。

Method: 提出了一种结合双光子激发和差频产生（2PE-DFG）的多光子技术，并利用极化断层扫描进行泵浦和探测过程中的态选择控制。

Result: 测量了Cu2O晶体中ED-禁止的S和D激子的相干动力学。1S激子的相干时间达到3ns，而具有更高主量子数（n=2,3,4）的里德堡激子相干时间仅为几皮秒。在10T磁场下，1S激子分裂成三态，观察到量子拍，具体取决于极化断层扫描方案。

Conclusion: 2PE-DFG技术是评估ED-禁止激子相干动力学的有力工具。

Abstract: Quantum applications of solid state systems base upon generation and control
of coherent electronic excitations. Prominent examples are exciton states in
semiconductors excitable by photons. The high oscillator strength of
electric-dipole (ED) allowed exciton states favors their efficient coherent
generation, but limits also their lifetime. ED-forbidden exciton states with
long recombination times might maintain long-lived coherence, especially in
highly-quality crystals with suppressed exciton scattering. Here, we propose a
multi-photon technique combining two-photon excitation with difference
frequency generation (2PE-DFG) for time-resolved measurements of exciton
coherence. The technique utilizes polarization tomography for state-selective
control in both the pump and probe processes. Its potential is demonstrated by
measuring the coherent dynamics of the ED-forbidden $S$ and $D$ excitons in
Cu$_2$O crystals. The excited states of the Rydberg excitons with principal
quantum number $n=2$, $3$, and $4$ have short dephasing times of a few
picoseconds, limited by their relaxation to lower lying states. The dephasing
time reaches 3 ns for the $1S$ state. In an external magnetic field up to 10 T,
the $1S$ exciton splits into a triplet so that quantum beats are observed after
coherent excitation, for which three distinct regimes are found depending on
the chosen polarization tomography scheme. These results establish the 2PE-DFG
technique as a powerful tool to assess the coherent dynamics of ED-forbidden
excitons.

</details>


### [132] [Phase-engineered Non-degenerate Sliding Ferroelectricity Enables Tunable Photovoltaics in Monolayer Janus In2S2Se](https://arxiv.org/abs/2507.22722)
*Yixuan Li,Qiang Wang,Keying Han,Yitong Liang,Kai Kong,Yan Liang,Thomas Frauenheimc,Xingshuai Lv,Defeng Guo,Bin Wang*

Main category: cond-mat.mtrl-sci

TL;DR: Janus In2S2Se中的非简并滑动铁电性调控了不同的光伏行为，有望用于设计和调控创新的光伏器件。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有二维滑动铁电体中存在的双重简并极化态且强度较弱的限制，从而阻碍通过滑动铁电性对光伏效应进行最优调控。

Method: 通过在Janus In2S2Se中进行Se到S的取代，引入了两个增强的、不同的非简并滑动铁电相（WZ'和ZB'），并通过第一性原理计算验证了该结构的实验合成及其通过原子层滑动触发的可逆相变能力。

Result: WZ'相具有增强的极化，在可见光区域提供了优越的光电转换效率；而WZ'到ZB'的相变可以提高载流子迁移率、调节带隙，并诱导间接-直接跃迁，从而在红外光谱中产生明显的红移和光电流峰值增强。

Conclusion: 该工作建立了一个非简并滑动铁电性来编排不同光伏行为的相工程框架，其固有的物理相关性可能为设计和调控创新的光伏器件提供新的视角。

Abstract: Two-dimensional sliding ferroelectrics, with their enhanced efficiencies of
charge separation and tunability, constitute promising platforms for
next-generation photovoltaic devices. However, recent systems predominantly
exhibit dual degenerate polarization states with weak intensity, hindering the
optimal manipulations of photovoltaic effects through sliding ferroelectricity.
Here, we address this limitation by introducing two strengthened and distinct
non-degenerate sliding ferroelectric phases (WZ' and ZB') in Janus In2S2Se,
which can be achieved by Se-to-S substitution in monolayer In2Se3.
First-principles calculations validate the experimental synthesis of this
structure and its capability for reversible phase transitions triggered by
atomic layer sliding, and a series of superior photovoltaic performances are
demonstrated in such unique Janus In2S2Se, accompanied by a detailed analysis
of how non-degenerate sliding ferroelectricity modulates distinct photovoltaic
characteristics. The WZ' to ZB' transition can increase the carrier mobility
and moderate the band gap while inducing an indirect-to-direct transition,
yielding a marked red-shift and enhancement of the photocurrent peak in the
infrared spectrum. Conversely, the WZ' phase, benefiting from enhanced
polarization, delivers superior photoelectric conversion efficiency in the
visible light region. This work establishes a phase-engineered framework of how
non-degenerate sliding ferroelectricity orchestrates distinct photovoltaic
behaviors, and the intrinsic physical correlations may offer novel perspectives
for designing and regulating innovative photovoltaic devices.

</details>


### [133] [The Effect of Pattern Quality on Measurements of Stress Heterogeneity and Geometrically Necessary Dislocation Density by High-Angular Resolution Electron Backscatter Diffraction](https://arxiv.org/abs/2507.22745)
*Harison S. Wiesman,David Wallis*

Main category: cond-mat.mtrl-sci

TL;DR: 通过增加HR-EBSD数据收集中的平均帧数来提高花样质量，可以减少应力/GND计算中的噪声，改善低质量花样，从而更准确地揭示材料的微观结构，特别是高应力区域。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在检查花样质量对高角分辨率电子背散射衍射（HR-EBSD）分析输出的影响。

Method: 该研究通过调整收集数据时每帧电子背散射图的平均帧数来改变衍射花样的质量。对同一变形的橄榄石样品区域进行了六次不同的映射，每次映射的平均帧数从1到30不等。然后，使用HR-EBSD对每个数据集进行分析，生成了晶内应力不均匀性和几何必需位错（GND）密度的图谱。

Result: 随着平均帧数的增加，应力和GND计算中的噪声减小，揭示了所映射区域更多的亚结构。低带对比度的像素在增加帧平均后得到最大程度的改善，而高带对比度的像素基本不受影响。此外，通过减少高应力噪声，应力分布的概率分布变窄，这也影响了从应力分布的统计分析中估算位错密度。

Conclusion: 通过增加帧平均来提高模式质量可以减少应力/GND密度计算中的噪声，揭示更多亚结构。虽然高带对比度像素几乎不受影响，但最差的像素（低带对比度）通过增加帧平均得到了最大的改善。此外，通过减少高应力噪声，应力分布的概率分布变窄，这也会影响从应力分布的统计分析中估算位错密度。因此，通过帧平均可以改进HR-EBSD数据，特别是对于通常与低带对比度相关的区域（通常是高应力/高GND密度区域）。最重要的是，将具有相似平均带对比度的数据集进行比较，以确保观察到的差异源于微观结构而非数据采集的伪影。

Abstract: We examine the effect of pattern quality on the output of high-angular
resolution electron backscatter diffraction (HR-EBSD) analyses. Band contrast,
as a proxy for pattern quality, was varied by adjusting the number of frames
averaged per electron backscatter pattern during data collection. The same
region in a deformed sample of the mineral olivine was mapped six times varying
the number of frames averaged between 1 and 30 between each map. Each data set
was analyzed with HR-EBSD, producing maps of intragranular stress heterogeneity
and geometrically necessary dislocation (GND) density. As the number of frames
averaged increased, the noise in stress and GND calculations decreased,
revealing more substructure in the mapped region. The worst pixels, with low
band contrast, are the most improved by increased frame averaging, whereas
those with high band contrast are largely unaffected. Additionally, the
probability distribution of stresses narrows as high-stress noise is reduced
with increased pattern quality, which also affects estimates of dislocation
density from statistical analysis of the stress distributions. As regions with
high stress and/or high GND density are typically of interest in HR-EBSD maps
and are often associated with low band contrast, frame averaging may be used as
a tool to improve the quality of these analyses. Most importantly, however, is
that comparisons are made between HR-EBSD datasets with similar mean band
contrast to ensure that observed differences are microstructural in origin and
not an artefact of data collection.

</details>


### [134] [High Entropy Engineering of Magnetic Kagome Lattice (Gd,Tb,Dy,Ho,Er)Mn6Sn6](https://arxiv.org/abs/2507.22809)
*Wenhao Liu,Nikhil Uday Dhale,Youzhe Chen,Pramanand Joshi,Zixin Zhai,Xiqu Wang,Ping Liu,Robert J. Birgeneau,Boris Maiorov,Christopher A. Mizzi,Bing Lv*

Main category: cond-mat.mtrl-sci

TL;DR: 研究了 (Gd,Tb,Dy,Ho,Er)Mn6Sn6 的磁性和输运性质，发现高熵化可以诱导新的磁跃变，并表现出线性磁阻和本征反常霍尔效应，证明了高熵化是调控磁性拓扑材料的有效手段。


<details>
  <summary>Details</summary>
Motivation: 探索磁相互作用和拓扑电子态，以及高熵化对磁性 kagome 格子化合物 RMn6Sn6 物理性质的调控作用。

Method: 通过结合高熵和磁性kagome 格子，获得了 (Gd,Tb,Dy,Ho,Er)Mn6Sn6 单晶，并系统地研究了它们的磁性和输运性质。

Result: 高熵材料 (Gd,Tb,Dy,Ho,Er)Mn6Sn6 表现出由温度和外部磁场诱导的多个新颖磁跃变，并在 4 K 下表现出持续到 20 T 的线性磁阻。此外，通过本征反常霍尔效应证明了在高熵形式中仍然存在本征非平凡能带拓扑。

Conclusion: 该研究表明，高熵化是一种调整磁性拓扑材料中电荷、自旋和晶格自由度相互作用的有力方法。

Abstract: The magnetic kagome lattice compound RMn6Sn6 (R=rare earth) is an emerging
platform to exploit the interplay between magnetism and topological electronic
states where a variety of exciting findings such as flat bands, Dirac points as
well as the dramatic dependence of magnetic order on the rare-earth element
have been reported. High entropy through rare earth alloying, on the other
hand, provides another knob to control over the physical properties in this
system. Here, by the marriage of high entropy and the magnetic kagome lattice,
we obtain (Gd,Tb,Dy,Ho,Er)Mn6Sn6 single crystals and systematically investigate
their magnetic and transport properties. Different from the parent phases, the
high entropy 166 material displays multiple novel magnetic transitions induced
by temperature and external magnetic fields. Furthermore, linear
magnetoresistance persisting up to 20 T has been revealed at 4 K. The intrinsic
nontrivial band topology also survives in the high entropy form, as evidenced
by the intrinsic anomalous Hall effect. Our results highlight high entropy as a
powerful approach for tuning the interplay of charge, spin and lattice degree
of freedom in magnetic topological materials.

</details>


### [135] [Revealing Nanoscale Ni-Oxidation State Variations in Single-Crystal NMC811 via 2D and 3D Spectro-Ptychography](https://arxiv.org/abs/2507.22834)
*Ralf F. Ziesche,Michael J. Johnson,Ingo Manke,Joshua H. Cruddos,Alice V. Llewellyn,Chun Tan,Rhodri Jervis,Paul R. Shearing,Christoph Rau,Alexander J. E. Rettie,Silvia Cipiccia,Darren Batey*

Main category: cond-mat.mtrl-sci

TL;DR: 通过结合高通量光栅成像和XANES光谱，我们能够以纳米级分辨率可视化SC-NMC811在高压下的化学降解，从而加深对电池材料降解机制的理解，并为设计更耐用的电池材料提供指导。


<details>
  <summary>Details</summary>
Motivation: 为了实现锂（Li）离子电池更高的能量密度、更长的循环寿命和更低的成本，需要对镍（Ni）含量高的层状氧化物正极（如LiNi$_{x}$Mn$_{y}$Co$_{z}$O$_{2}$（NMC），x > 0.8）进行更深入的研究，因为它们具有高比容量和低钴含量。然而，标准的多晶形貌在高于4.2 V电压时会加速降解，这是由于其机械和化学不稳定性增加。单晶NMC（SC-NMC）作为一种有前景的形貌，可以通过防止晶粒间开裂来抑制机械不稳定性，但需要有效的方法来理解其化学降解途径。

Method: 我们展示了一种高通量数据收集策略，能够在几分钟内完成以前需要数小时才能完成的二维和三维光栅成像，并将其与X射线吸收近边光谱（XANES）相结合，以纳米级空间分辨率可视化SC-NMC811中局部镍氧化态的行为，并将其作为充电状态的代理。

Result: 通过在不同寿命循环阶段和高电压（>4.2 V）下采用此技术，可以实现沿锂通道的化学降解的直接映射、形核位点的识别以及在电极和颗粒尺度上镍氧化态不均匀性的观测。我们进一步将这些不均匀性与岩盐相的形成和生长以及氧诱导的平面滑移相关联。

Conclusion: 这项技术将促进对高镍层状氧化物材料在高压运行期间化学降解方式的根本理解，并指导设计更耐用的电池材料。

Abstract: Enabling lithium (Li)-ion batteries with higher energy densities, longer
cycle life, and lower costs will underpin the widespread electrification of the
transportation and large-scale energy storage industries. Nickel (Ni)-rich
layered oxide cathodes, such as LiNi$_{x}$Mn$_{y}$Co$_{z}$O$_{2}$ (NMC, x >
0.8), have gained popularity due to their high specific capacities and lower
cobalt content. However, the standard polycrystalline morphology suffers from
accelerated degradation at voltages above 4.2 V versus graphite, due to its
increased mechanical and chemical instability. Single-crystal NMC (SC-NMC) has
emerged as a promising morphology for suppressing the mechanical instability by
preventing intergranular cracking; however, robust methods of understanding its
chemical degradation pathways are required. We demonstrate how a
high-throughput data collection strategy unlocks the ability to perform 2D and
3D ptychography in minutes, where it currently requires hours, and combine this
with X-ray absorption near edge spectroscopy (XANES) to visualise the local Ni
oxidation state behaviour in SC-NMC811, with nanometre-scale spatial
resolution, and use this as a proxy for state-of-charge. By employing this
technique at various stages during lifetime cycling to high voltages (>4.2 V),
direct mapping of chemical degradation along the Li channels, identification of
nucleation sites, and observation of Ni oxidation state heterogeneities across
both electrode and particle scales can be achieved. We further correlate these
heterogeneities with the formation and growth of the rocksalt phase and
oxygen-induced planar gliding. This methodology will advance the fundamental
understanding of how high-Ni layered oxide materials chemically degrade during
high-voltage operation, guiding the design of more durable battery materials.

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [136] [Probing Tensor Monopoles and Gerbe Invariants in Three-Dimensional Topological Matter](https://arxiv.org/abs/2507.22116)
*Wojciech J. Jankowski,Robert-Jan Slager,Giandomenico Palumbo*

Main category: cond-mat.mes-hall

TL;DR: 该研究探索了三维拓扑物质中的张量单极子和向量丛推广，发现了它们与量化的体磁电和非线性光学现象的联系，并提出了相应的理论构造和推广。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机在于探索和实现三维拓扑物质带中的动量空间张量单极子，这些单极子与非平凡向量丛推广（丛格）相关。研究旨在理解这些拓扑相中的张量贝里连接的构造，以及其中存在的障碍如何导致量化的体磁电和非线性光学现象。此外，研究还希望阐明带内和带间扭转在支持这些量子效应中的作用，并将其推广到更广泛的拓扑绝缘体和多体相互作用系统中。

Method: 研究人员利用动量空间张量单极子和向量丛推广（丛格）来实现三维拓扑物质带中的非平凡霍普夫不变量。他们提出了一种通用构造，用于构建这些拓扑相中的张量贝里连接，并分析了其中的障碍如何导致Z量化的体磁电和非线性光学现象。此外，研究还探讨了带内和带间扭转如何支持这些量子效应，以及这些效应在已知的霍普夫相和更一般的拓扑绝缘体中的表现。最后，研究人员通过引入多体丛格不变量和扭曲边界条件，提出了一个相互作用的推广，用于研究高维电荷分数化。

Result: 研究成功地在具有非平凡霍普夫不变量的三维拓扑物质带中实现了动量空间张量单极子，并提供了一种通用构造来实现张量贝里连接。研究发现，这些拓扑相中的障碍会导致Z量化的体磁电和非线性光学现象。研究还证明了带内和带间扭转在支持这些量子效应中的作用，并将其推广到更一般的拓扑绝缘体和多体相互作用系统。

Conclusion: 该研究展示了动量空间张量单极子，即非平凡向量丛推广（称为丛格），可以在具有非平凡霍普夫不变量的三维拓扑物质带中实现。研究提供了一种在这些拓扑相中张量贝里连接的通用构造，并证明了其中的障碍如何导致Z量化的体磁电和非线性光学现象。研究进一步指出，这些量子效应由带内和带间扭转支持，从而在大多数已知的霍普夫相以及实现超出拓扑相物质的十重分类的更一般的拓扑绝缘体中产生非平凡的Dixmier-Douady类。此外，通过引入多体丛格不变量并采用扭曲边界条件，该研究还提供了一个相互作用的推广，从而为研究通过可电磁探测的高维电荷分数化实现的丛格不变量开辟了道路。

Abstract: We show that momentum-space tensor monopoles corresponding to nontrivial
vector bundle generalizations, known as bundle gerbes, can be realized in bands
of three-dimensional topological matter with nontrivial Hopf invariants. We
provide a universal construction of tensor Berry connections in these
topological phases, demonstrating how obstructions therein lead to
$\mathbb{Z}$-quantized bulk magnetoelectric and nonlinear optical phenomena. We
then pinpoint that these quantum effects are supported by intraband and
interband torsion leading to nontrivial Dixmier-Douady classes in most known
Hopf phases and in more general topological insulators realizing gerbe
invariants falling beyond the tenfold classification of topological phases of
matter. We furthermore provide an interacting generalization upon introducing
many-body gerbe invariants by employing twisted boundary conditions. This opens
an avenue to study gerbe invariants realized through higher-dimensional charge
fractionalizations that can be electromagnetically probed.

</details>


### [137] [Ultrafast Faraday Rotation Probe of Chiral Phonon-Polaritons in LiNbO3](https://arxiv.org/abs/2507.22232)
*Megan F. Biggs,Sin-hang,Ho,Aldair Alejandro,Matthew Lutz,Clayton D. Moss,Jeremy A. Johnson*

Main category: cond-mat.mes-hall

TL;DR: 通过太赫兹脉冲激发LiNbO3中的手性声子-极化子，发现了时间反演对称性破缺的运动，并观察到~11特斯拉的诱导磁场。


<details>
  <summary>Details</summary>
Motivation: 研究LiNbO3中的手性声子-极化子在超快太赫兹场作用下的时间反演对称性破缺运动，并探索其诱导磁场效应。

Method: 通过结合一对相互垂直偏振的太赫兹脉冲，并调整它们之间相对延迟，创建手性太赫兹驱动场来激发手性声子-极化子。利用圆偏振太赫兹泵浦的逆法拉第效应，在非磁性材料LiNbO3中诱导磁矩场，并通过法拉第旋转探针测量来量化磁场强度。

Result: 在LiNbO3中成功激发了手性声子-极化子，并通过法拉第旋转测量观察到信号方向随输入太赫兹脉冲圆偏振方向的变化，估计出~11特斯拉的强诱导磁场。

Conclusion: LiNbO3中的手性声子-极化子在超快法拉第效应下表现出时间反演对称性破缺的运动，并通过法拉第旋转测量估计了~11特斯拉的强诱导磁场。

Abstract: Time reversal symmetry breaking motion of chiral phonon-polaritons in LiNbO3
is probed via the ultrafast Faraday effect. By combining a pair of
perpendicularly polarized THz pulses with the right relative delay, we create a
chiral THz driving field to excite chiral phonon-polaritons. The chiral atomic
motion combines with the inverse Faraday effect from the circularly polarized
THz pump to induce a magnetic moment field in the nonmagnetic material, LiNbO3.
We attempt to quantify the strength of the magnetic field with Faraday rotation
probe measurements. The direction of the Faraday signal flips when the input
THz pulse is changed from left- to right-circular polarization, and we estimate
a strong induced magnetic field strength of ~11 Tesla based on the Faraday
rotation.

</details>


### [138] [Anisotropic Magnetism in Gd$_2$B$_5$](https://arxiv.org/abs/2507.22325)
*Maximilien F. Debbas,Takehito Suzuki,Alex H. Mayo,Mun K. Chan,Joseph G. Checkelsky*

Main category: cond-mat.mes-hall

TL;DR: Gd$_2$B$_5$ single crystals were synthesized and characterized, revealing complex magnetic ordering behavior including zero-field and field-induced phases.


<details>
  <summary>Details</summary>
Motivation: To investigate the magnetic anisotropy and phase diagram of the Gd$_2$B$_5$ system.

Method: Synthesis of Gd$_2$B$_5$ single crystals using a ruthenium-gadolinium flux method and characterization via orientation-dependent electrical transport, magnetization, magnetic torque, and heat capacity measurements.

Result: Gd$_2$B$_5$ has a monoclinic $P21/c$ structure with gadolinium atoms in the $(100)$ plane. It exhibits two zero-field ordered phases (M$_1$ and M$_2$) and a field-induced phase (M$_ot$) when a magnetic field is applied in the $(100)$ plane.

Conclusion: Gd$_2$B$_5$ crystallizes in the monoclinic $P21/c$ space group and exhibits two zero-field ordered magnetic phases (M$_1$ and M$_2$) and a field-induced ordered phase (M$_ot$) when the magnetic field is applied in the $(100)$ plane. This behavior was characterized through electrical transport, magnetization, magnetic torque, and heat capacity measurements.

Abstract: We report the synthesis of single crystals of Gd$_2$B$_5$ through a
ruthenium-gadolinium flux method. The Gd$_2$B$_5$ system is a member of the
monoclinic $P21/c$ (No. 14) space group and realizes lattices of gadolinium
atoms in the $(1\,0\,0)$ plane. We characterized the sample through
orientation-dependent electrical transport, magnetization, magnetic torque, and
heat capacity measurements to probe the magnetic anisotropy of the system and
map out its phase diagram. Gd$_2$B$_5$ realizes two zero-field ordered phases
M$_1$ and M$_2$, as well as a third field-induced ordered phase M$_\perp$
arising when the magnetic field is applied in the $(1\,0\,0)$ plane.

</details>


### [139] [Universal Magnetic Phases in Twisted Bilayer MoTe$_2$](https://arxiv.org/abs/2507.22354)
*Weijie Li,Evgeny Redekop,Christiano Wang Beach,Canxun Zhang,Xiaowei Zhang,Xiaoyu Liu,Will Holtzmann,Chaowei Hu,Eric Anderson,Heonjoon Park,Takashi Taniguchi,Kenji Watanabe,Jiun-haw Chu,Liang Fu,Ting Cao,Di Xiao,Andrea F. Young,Xiaodong Xu*

Main category: cond-mat.mes-hall

TL;DR: 通过光谱和磁强法研究了扭曲双层MoTe2的磁相图，发现了与扭曲角无关的铁磁相，并揭示了不同填充因子下磁相互作用和带宽的复杂关系。


<details>
  <summary>Details</summary>
Motivation: 探索扭曲双层MoTe2 (tMoTe2) 中磁性和拓扑随扭曲角的变化规律。

Method: 使用局部光学光谱和扫描纳米SQUID-on-tip (nSOT) 磁强法系统地绘制了tMoTe2的磁相图。

Result: 在-1和-3的摩尔填充因子以及2.1°到3.7°的扭曲角范围内，发现了自发铁磁性，这表明存在一种普遍的、与扭曲角无关的铁磁相。在2.1°时，在-5填充因子处还观察到了稳健的铁磁性，这在高能带展平的扭曲角范围内不存在。对温度依赖性的测量揭示了-1和-3填充因子之间居里温度在扭曲角依赖性上的差异，这表明交换相互作用和带宽在两种陈省道中的相互作用方式不同。尽管存在自发的时空调和对称性破缺，但并未在-3填充因子处发现拓扑能隙；然而，脆弱的关联拓扑相可能被设备无序所掩盖。

Conclusion: 该研究为理解和控制tMoTe2中的磁序提供了全局框架，并强调了其在更高能量陈省道中实现关联拓扑相的潜力。

Abstract: Twisted bilayer MoTe$_2$ (tMoTe$_2$) has emerged as a robust platform for
exploring correlated topological phases, notably supporting fractional Chern
insulator (FCI) states at zero magnetic field across a wide range of twist
angles. The evolution of magnetism and topology with twist angle remains an
open question. Here, we systematically map the magnetic phase diagram of
tMoTe$_2$ using local optical spectroscopy and scanning nanoSQUID-on-tip (nSOT)
magnetometry. We identify spontaneous ferromagnetism at moir\'e filling factors
$\nu = -1$ and $-3$ over a twist angle range from 2.1$^\circ$ to 3.7$^\circ$,
revealing a universal, twist-angle-insensitive ferromagnetic phase. At
2.1$^\circ$, we further observe robust ferromagnetism at $\nu = -5$, absent in
the devices with larger twist angle -- a signature of the flattening of higher
bands in this twist angle range. Temperature-dependent measurements reveal a
contrasting twist-angle dependence of the Curie temperatures between $\nu = -1$
and $\nu = -3$, indicating distinct interplay between exchange interaction and
bandwidth for the two Chern bands. Despite spontaneous time-reversal symmetry
breaking, we find no evidence of a topological gap at $\nu = -3$; however,
fragile correlated topological phases could be obscured by the device disorder
evident in our spatially resolved measurements. Our results establish a global
framework for understanding and controlling magnetic order in tMoTe$_2$ and
highlight its potential for accessing correlated topological phases in higher
energy Chern band.

</details>


### [140] [Thermal Hall effect induced by phonon skew-scattering via orbital magnetization](https://arxiv.org/abs/2507.22436)
*Taekoo Oh*

Main category: cond-mat.mes-hall

TL;DR: 绝缘体和半导体中的热霍尔效应可能是由轨道磁化引起的轴向手性声子斜散射造成的。


<details>
  <summary>Details</summary>
Motivation: 为了解释在绝缘体和半导体中观察到的热霍尔效应的根本起源，而此前其起源尚不清楚。

Method: 从基本原理出发，利用成熟的Haldane模型推导出轨道磁化-声子耦合的形式和量级，并计算了热导率和霍尔角随温度的变化。

Result: 计算得出了热导率和霍尔角作为温度函数的半定量结果，与实验结果符合。

Conclusion: 该研究提出了一个有希望的机制，即由轨道磁化引起的轴向手性声子斜散射，来解释绝缘体和半导体中热霍尔效应的出现，实现了与实验结果的半定量一致。

Abstract: Thermal transport acts as a powerful tool for studying the excitations and
physical properties of insulators, where a charge gap suppresses electronic
conduction. Recently, the thermal Hall effect has been observed across various
materials, including insulators and semiconductors, but its fundamental origin
remains unclear. Here, I propose a promising mechanism to explain the emergence
of the thermal Hall effect in these systems: axial chiral phonon skew
scattering mediated by orbital magnetization. Starting from basic principles, I
derive the form and magnitude of the orbital magnetization-phonon coupling
using the well-established Haldane model. Using this coupling, I calculate the
thermal Hall conductivity and Hall angle as functions of temperature, achieving
semi-quantitative agreement with experimental findings. This work enhances our
understanding of the role of electron-phonon coupling in thermal transport and
provides a pathway to tailor thermal properties in a broad range of materials.

</details>


### [141] [Collective Fluorescence of Graphene Quantum Dots on a Halide Perovskite Crystal](https://arxiv.org/abs/2507.22458)
*Hugo Levy-Falk,Suman Sarkar,Thanh Trung Huynh,Daniel Medina-Lopez,Lauren Hurley,Océane Capelle,Muriel Bouttemy,Gaëlle Trippé-Allard,Stéphane Campidelli,Loïc Rondin,Elsa Cassette,Emmanuelle Deleporte,Jean-Sébastien Lauret*

Main category: cond-mat.mes-hall

TL;DR: 石墨烯量子点在钙钛矿表面形成小簇，并展现出独特的荧光动力学行为。


<details>
  <summary>Details</summary>
Motivation: 探索石墨烯量子点在不同环境（溶液、聚合物基质与钙钛矿表面）中的聚集和荧光行为差异，特别是研究其在钙钛矿表面表现出的集体动力学。

Method: 通过共聚焦荧光显微镜和连续照明下的光谱分析，研究了C96tBu8石墨烯量子点在CH3NH3PbBr3钙钛矿表面沉积时的集体荧光动力学。

Result: 在钙钛矿表面观察到石墨烯量子点形成小簇，并存在单体类状态和红移发射状态之间的动态转变，部分样本出现荧光强度急剧增加和激发态寿命缩短的现象。

Conclusion: 本研究揭示了C96tBu8石墨烯量子点在单晶卤化钙钛矿表面沉积时，与在溶液和聚合物基质中相比，表现出独特的集体动力学行为，形成了小簇而非单个分子，并观察到单体类状态和红移发射状态之间的动态转变，以及可能由有序石墨烯量子点聚集体引起的荧光强度急剧增加和激发态寿命缩短的现象。

Abstract: This study explores the dynamical collective fluorescence of $C_{96}tBu_8$
graphene quantum dots when deposited on the surface of monocrystalline halide
perovskite. Despite the tendency of the graphene quantum dots to avoid
aggregation in solution and polymer matrices, our findings reveal distinct
collective behaviors when deposited on the perovskite surface, here
$CH_3NH_3PbBr_3$. We observed small clusters of graphene quantum dots rather
than isolated single molecules through confocal fluorescence microscopy.
Spectral analysis under continuous illumination shows a back-and-forth
dynamical transition between an uncoupled, monomer-like state and a coupled
state with a redshifted emission. In some cases, this dynamical process is
followed by a drastic one-way increase in fluorescence intensity combined with
a shortening of the excited state lifetime, which could characterize the
emission of ordered graphene quantum dots within aggregates.

</details>


### [142] [Strain-Controlled Topological Phase Transitions and Chern Number Reversal in Two-Dimensional Altermagnets](https://arxiv.org/abs/2507.22474)
*Zesen Fu,Mengli Hu,Aolin Li,Haiming Duan,Junwei Liu,Fangping Ouyang*

Main category: cond-mat.mes-hall

TL;DR: 应变可用于控制二维反磁性材料的拓扑相，陈数可仅通过改变应变方向来反转。


<details>
  <summary>Details</summary>
Motivation: 研究具有自旋-谷锁定和应变可调拓扑相的二维反磁性材料，探索应变调控其拓扑性质的可能性。

Method: 通过构建约束于反磁性对称性的最小紧束缚模型，并推导解析应变诱导微扰理论，研究了表现出国 गुंतवण-谷锁定和应变可调拓扑相的二维反磁性。

Result: 结果表明，双轴应变可驱动从平庸绝缘体到II型量子自旋霍尔（QSH）相的转变，并存在一个平庸绝缘体、II型QSH相和两个具有相反陈数的量子反常霍尔相的通用相图。通过改变应变方向可以反转陈数。

Conclusion: 应变工程是控制二维反磁性材料拓扑相的有效途径，单层CrO的理论和第一性原理计算证实了所预测的拓扑转变。

Abstract: We present a theoretical and first-principles study of a two-dimensional
altermagnet exhibiting spin-valley locking and strain-tunable topological
phases. By constructing a minimal tight-binding model constrained by
altermagnetic symmetry, we show that biaxial strain can drive a transition from
a trivial insulator to a type-II quantum spin Hall (QSH) phase. Furthermore, we
derive an analytical strain-induced perturbation theory that identifies two
critical curves, dividing the phase space into four regions corresponding to a
trivial insulator, a type-II QSH phase, and two quantum anomalous Hall phases
with opposite Chern numbers. Remarkably, the Chern number can be reversed
purely by changing the strain direction --without modifying magnetization or
applying magnetic fields. The model reveals a universal phase diagram for
materials with the same symmetry and valley structure. First-principles
calculations on monolayer CrO confirm the predicted topological transitions,
establishing strain engineering as an effective route for topological control
in two-dimensional altermagnetic materials.

</details>


### [143] [Lattice tuning of charge and spin transport in $β_{12}$-borophene nanoribbons](https://arxiv.org/abs/2507.22571)
*Masoumeh Davoudiniya,Jonas Fransson,Biplab Sanyal*

Main category: cond-mat.mes-hall

TL;DR: Electron-phonon coupling and structural design in borophene nanoribbons control charge/spin transport, enabling optimization for logic devices.


<details>
  <summary>Details</summary>
Motivation: To investigate the interplay between electron-phonon coupling (EPC) and structural configurations in magnetic and nonmagnetic $\beta_{12}$-borophene nanoribbons (BNRs) and understand their impact on charge and spin transport.

Method: The study employed a tight-binding framework with site-dependent hopping parameters from ab initio calculations. Phonons were incorporated using the Holstein model to compute phonon-renormalized Green's functions and transport currents via the Landauer-B"{u}ttiker formalism.

Result: Spin-dependent EPC enhances spin-dependent current in magnetic zigzag (ZZ) nanoribbons due to phonon-induced inelastic scattering and spin-selective band renormalization. Nonmagnetic $\beta_{12}$-BNRs exhibit enhanced charge transport current. Structural variations lead to anisotropic EPC effects, significantly altering charge and spin transport.

Conclusion: Lattice vibrations, specifically electron-phonon coupling (EPC) and structural configurations, significantly influence charge and spin transport in $\beta_{12}$-borophene nanoribbons (BNRs). Tailoring these factors, particularly through edge engineering, offers a promising pathway for optimizing borophene-based logic devices.

Abstract: Lattice vibrations critically shape charge and spin transport by governing
carrier scattering, spin-charge interactions and spectral redistribution in
nanostructures. In this study, we investigate how electron-phonon coupling
(EPC) and structural configurations intertwine in magnetic and nonmagnetic
$\beta_{12}$-borophene nanoribbons (BNRs). Using a tight-binding framework with
site-dependent hopping parameters extracted from ab initio calculations and
incorporating phonons within the Holstein model, we compute phonon-renormalized
Green's functions and transport currents via the Landauer-B\"{u}ttiker
formalism. We find that spin-dependent EPC enhances spin-dependent current in
magnetic zigzag (ZZ) nanoribbons, driven by phonon-induced inelastic scattering
and spin-selective band renormalization. Additionally, we observe an
enhancement of charge transport current in the nonmagnetic configurations of
$\beta_{12}$-BNRs. Structural variations further induce anisotropic EPC
effects, significantly reshaping charge and spin transport. These insights
establish EPC as a powerful design lever for optimizing borophene-based logic
devices through tailored edge engineering.

</details>


### [144] [Quantum siphoning of finely spaced interlayer excitons in reconstructed MoSe2/WSe2 heterostructures](https://arxiv.org/abs/2507.22584)
*Mainak Mondal,Kenji Watanabe,Takashi Taniguchi,Gaurav Chaudhary,Akshay Singh*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Atomic reconstruction in twisted transition metal dichalcogenide
heterostructures leads to mesoscopic domains with uniform atomic registry,
profoundly altering the local potential landscape. While interlayer excitons in
these domains exhibit strong many-body interactions, extent and impact of
quantum confinement on their dynamics remains unclear. Here, we reveal that
quantum confinement persists in these flat, reconstructed regions.
Time-resolved photoluminescence spectroscopy uncovers multiple, finely-spaced
interlayer exciton states (~ 1 meV separation), and correlated emission
lifetimes spanning sub-nanosecond to over 100 nanoseconds across a 10 meV
energy window. Cascade-like transitions confirm that these states originate
from a single potential well, further supported by calculations. Remarkably, at
high excitation rates, we observe transient suppression of emission followed by
gradual recovery, a process we term "quantum siphoning". Our results
demonstrate that quantum confinement and competing nonlinear dynamics persist
beyond the ideal moire paradigm, potentially enabling applications in quantum
sensing and modifying exciton dynamics via strain engineering.

</details>


### [145] [Random matrix theory of charge distribution in disordered quantum impurity models](https://arxiv.org/abs/2507.22586)
*Maxime Debertolis,Serge Florens*

Main category: cond-mat.mes-hall

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce a bare-bone random matrix quantum impurity model, by hybridizing
a localized spinless electronic level with a bath of random fermions in the
Gaussian Orthogonal Ensemble (GOE). While stripped out of correlations effects,
this model reproduces some salient features of the impurity charge distribution
obtained in previous works on interacting disordered impurity models. Computing
by numerical sampling the impurity charge distribution in our model, we find a
crossover from a Gaussian distribution (centered on half a charge unit) at
large hybridization, to a bimodal distribution (centered both on zero and full
occupations of the charge) at small hybridization. In the bimodal regime, a
universal $(-3/2)$ power-law is also observed. All these findings are very well
accounted for by an analytic surmise computed with a single random electron
level in the bath. We also derive an exact functional integral for the general
probability distribution function of eigenvalues and eigenstates, that formally
captures the statistical behavior of our model for any number $N$ of fermionic
orbitals in the bath. In the Gaussian regime and in the limit $N\to\infty$, we
are able to solve exactly the random matrix theory (RMT) for the charge
distribution, obtaining perfect agreement with the numerics. Our results could
be tested experimentally in mesoscopic devices, for instance by coupling a
small quantum dot to a chaotic electronic reservoir, and using a quantum point
contact as local charge sensor for the quantum dot occupation.

</details>


### [146] [The multiconfigurational ground state of a diradicaloid characterized at the atomic scale](https://arxiv.org/abs/2507.22598)
*Elia Turco,Lara Tejerina,Gonçalo Catarina,Andres Ortega-Guerrero,Nils Krane,Leo Gross,Michal Juríček,Shantanu Mishra*

Main category: cond-mat.mes-hall

TL;DR: 在绝缘NaCl表面，通过扫描探针技术和计算，发现一种由两个苯并[a]菲单元和C4链组成的分子，其基态由多个粒子构成，证明了强电子关联影响单分子结构。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探索单分子中强电子关联的表现及其对几何和电子结构的影响。

Method: 采用扫描探针技术（包括原子原子力显微镜和扫描隧道显微镜）在绝缘NaCl表面对由两个苯并[a]菲单元通过sp杂化C4链连接而成的单分子进行了表征，并结合了多构型计算。

Result: 研究成功制备了单分子，并通过原子原子力显微镜测量了C4链的键级对比，以及通过扫描隧道显微镜绘制了电荷态跃迁图，结合计算结果揭示了该分子具有多体基态，证明了强电子关联在单分子中的作用。

Conclusion: 本研究通过实验证明了强电子关联在单分子几何和电子结构中的体现。

Abstract: We report the tip-induced generation and scanning probe characterization of a
singlet diradicaloid, consisting of two phenalenyl units connected by an
sp-hybridized C$_{4}$ chain, on an ultrathin insulating NaCl surface. The
bond-order contrast along the C$_{4}$ chain measured by atomic force microscopy
and mapping of charge-state transitions by scanning tunneling microscopy, in
conjunction with multiconfigurational calculations, reveal that the molecule
exhibits a many-body ground state. Our study experimentally demonstrates the
manifestation of strong electronic correlations in the geometric and electronic
structures of a single molecule.

</details>


### [147] [Nonclassical Photon-Assisted Transport in Superconducting Tunnel Junctions](https://arxiv.org/abs/2507.22662)
*Matthias Hübler,Juan Carlos Cuevas,Wolfgang Belzig*

Main category: cond-mat.mes-hall

TL;DR: 理论研究了约瑟夫森结与非经典电磁环境的相互作用，实现了对量子态的重构。


<details>
  <summary>Details</summary>
Motivation: 电路量子电动力学的进展使得产生任意非经典微波态成为可能，并为解决新的物理问题铺平了道路。

Method: 对约瑟夫森结中的超电流进行理论研究，将经典输运现象（如光子辅助隧穿和夏皮罗台阶）推广到量子领域。

Result: 我们提出了一种理论研究方法，研究了约瑟夫森结与非经典电磁环境的相互作用。

Conclusion: 通过分析约瑟夫森结中的超电流，可以完全重构电磁环境的量子态，这在普通隧道结中是不可能实现的。

Abstract: Advances in circuit quantum electrodynamics have enabled the generation of
arbitrary nonclassical microwave states and paved the way for addressing novel
physics questions. Here, we present a theoretical study of the electrical
current in a Josephson tunnel junction interacting with a nonclassical
electromagnetic environment. This allows us to generalize classical transport
phenomena like photon-assisted tunneling and Shapiro steps to the quantum
regime. We predict that the analysis of the supercurrent in such a setup
enables the complete reconstruction of quantum states of the electromagnetic
environment, something that is not possible with normal tunnel junctions.

</details>


### [148] [Unconventional hybrid-order topological insulators](https://arxiv.org/abs/2507.22666)
*Wei Jia,Yuping Tian,Huanhuan Yang,Xiangru Kong,Zhi-Hao Huang,Wei-Jiang Gong,Jun-Hong An*

Main category: cond-mat.mes-hall

TL;DR: 发现了同时具有二阶和三阶拓扑态的非常规混合阶拓扑绝缘体（HyOTIs），并提出了实验验证方案。


<details>
  <summary>Details</summary>
Motivation: 探索具有奇异量子态的拓扑物质可以更新对拓扑相的理解，并拓宽拓扑材料的分类。

Method: 开发了通用的表面理论来精确描述这些拓扑态，并发现了第一个由反演对称性保护的3D非常规HyOTI。

Result: 发现了同时在一个d维系统中具有各种不同高阶拓扑态的非常规混合阶拓扑绝缘体（HyOTIs），并提出了电路实验方案来验证这些结果，同时发现了一个由反演对称性保护的3D非常规HyOTI，它在同一个带隙中同时呈现二阶（螺旋）和三阶（角）拓扑态。

Conclusion: 这项工作将极大地促进理论和实验上对混合拓扑态的研究。

Abstract: Exploring topological matters with exotic quantum states can update the
understanding of topological phases and broaden the classification of
topological materials. Here, we report a class of unconventional hybrid-order
topological insulators (HyOTIs), which simultaneously host various different
higher-order topological states in a single $d$-dimensional ($d$D) system. Such
topological states exhibit a unique bulk-boundary correspondence that is
different from first-order topological states, higher-order topological states,
and the coexistence of both. Remarkably, we develop a generic surface theory to
precisely capture them and firstly discover a $3$D unconventional HyOTI
protected by inversion symmetry, which renders both second-order (helical) and
third-order (corner) topological states in one band gap and exhibits a novel
bulk-edge-corner correspondence. By adjusting the parameters of the system, we
also observe the nontrivial phase transitions between the inversion-symmetric
HyOTI and other conventional phases. We further propose a circuit-based
experimental scheme to detect these interesting results. Particularly, we
demonstrate that a modified tight-binding model of bismuth can support the
unconventional HyOTI, suggesting a possible route for its material realization.
This work shall significantly advance the research of hybrid topological states
in both theory and experiment.

</details>


### [149] [Floquet Spin Splitting and Spin Generation in Antiferromagnets](https://arxiv.org/abs/2507.22884)
*Bo Li,Ding-Fu Shao,Alexey A. Kovalev*

Main category: cond-mat.mes-hall

TL;DR: This paper introduces a new way to control spins in antiferromagnets using light and thermal baths, creating spin currents and accumulation without needing special materials or complex effects like spin-orbit coupling.


<details>
  <summary>Details</summary>
Motivation: Accessing the spin degree of freedom is essential in antiferromagnetic spintronics for generating spin currents and manipulating magnetic order, which typically requires lifting spin degeneracy through relativistic spin-orbit coupling or non-relativistic spin splitting in altermagnets. This work aims to provide an alternative approach.

Method: The paper proposes an alternative approach using a dynamical spin splitting induced by an optical field in antiferromagnets, coupled to a thermal bath to achieve steady-state pure spin currents, linear-response longitudinal and transverse spin currents, and net spin accumulation without relying on spin-orbit coupling.

Result: The study demonstrates the emergence of steady-state pure spin currents, as well as linear-response longitudinal and transverse spin currents, by coupling the optically driven antiferromagnet to a thermal bath. It also shows that thermal bath engineering allows for the generation of a net spin accumulation without relying on spin-orbit coupling.

Conclusion: The study proposes a novel method for controlling spins in antiferromagnets using a dynamically induced spin splitting via an optical field, which circumvents the need for spin-orbit coupling and allows for the generation of various spin currents and net spin accumulation through thermal bath engineering.

Abstract: In antiferromagnetic spintronics, accessing the spin degree of freedom is
essential for generating spin currents and manipulating magnetic order, which
generally requires lifting spin degeneracy. This is typically achieved through
relativistic spin-orbit coupling or non-relativistic spin splitting in
altermagnets. Here, we propose an alternative approach: a dynamical spin
splitting induced by an optical field in antiferromagnets. By coupling the
driven system to a thermal bath, we demonstrate the emergence of steady-state
pure spin currents, as well as linear-response longitudinal and transverse spin
currents. Crucially, thermal bath engineering allows the generation of a net
spin accumulation without relying on spin-orbit coupling. Our results provide a
broadly applicable and experimentally tunable route to control spins in
antiferromagnets, offering new opportunities for spin generation and
manipulation in antiferromagnetic spintronics.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [150] [Deployment of Objects with a Soft Everting Robot](https://arxiv.org/abs/2507.22188)
*Ethan DeVries,Jack Ferlazzo,Mustafa Ugur,Laura H. Blumenschein*

Main category: cs.RO

TL;DR: 软体驱动机器人可用于输送较大、较重有效载荷，并成功通过了多种障碍测试。


<details>
  <summary>Details</summary>
Motivation: 软体驱动机器人具有比传统刚性机器人更高的灵活性、更好的环境交互能力以及在不可预测环境中安全导航的优势。尽管软体驱动机器人在探索任务中得到了广泛应用，但其移动和部署有效载荷的潜力尚未得到充分研究。利用软体驱动机器人的导航和驱动能力，将其用于在危险区域（如将水瓶运送给被困在瓦砾中的人）部署有效载荷，将大大扩展其应用潜力。

Method: 通过分析软体驱动机器人内部输送较大、较重有效载荷的方法，并建立模型来量化有效载荷对机器人增长和自支撑的影响，预测有效载荷打滑。通过实验量化了软体驱动机器人在各种形状、尺寸和重量的有效载荷下的输送能力，并进行了一系列任务测试，包括转向、垂直输送、穿过孔洞以及跨越间隙。

Result: 软体驱动机器人可以输送各种形状和高达1.5公斤的有效载荷，并能够以最小的间隙（0.01厘米）通过圆形孔，执行高达135度的转弯，以及跨越1.15米长的无支撑间隙。

Conclusion: 本研究表明，软体驱动机器人可以输送各种形状和高达1.5公斤的有效载荷，并能够以最小的间隙（0.01厘米）通过圆形孔，执行高达135度的转弯，以及跨越1.15米长的无支撑间隙。

Abstract: Soft everting robots present significant advantages over traditional rigid
robots, including enhanced dexterity, improved environmental interaction, and
safe navigation in unpredictable environments. While soft everting robots have
been widely demonstrated for exploration type tasks, their potential to move
and deploy payloads in such tasks has been less investigated, with previous
work focusing on sensors and tools for the robot. Leveraging the navigation
capabilities, and deployed body, of the soft everting robot to deliver payloads
in hazardous areas, e.g. carrying a water bottle to a person stuck under
debris, would represent a significant capability in many applications. In this
work, we present an analysis of how soft everting robots can be used to deploy
larger, heavier payloads through the inside of the robot. We analyze both what
objects can be deployed and what terrain features they can be carried through.
Building on existing models, we present methods to quantify the effects of
payloads on robot growth and self-support, and develop a model to predict
payload slip. We then experimentally quantify payload transport using soft
everting robot with a variety of payload shapes, sizes, and weights and though
a series of tasks: steering, vertical transport, movement through holes, and
movement across gaps. Overall, the results show that we can transport payloads
in a variety of shapes and up to 1.5kg in weight and that we can move through
circular apertures with as little as 0.01cm clearance around payloads, carry
out discrete turns up to 135 degrees, and move across unsupported gaps of 1.15m
in length.

</details>


### [151] [FLORES: A Reconfigured Wheel-Legged Robot for Enhanced Steering and Adaptability](https://arxiv.org/abs/2507.22345)
*Zhicheng Song,Jinglan Xu,Chunxin Zheng,Yulin Li,Zhihai Bi,Jun Ma*

Main category: cs.RO

TL;DR: FLORES是一种创新的轮腿机器人，通过改变前腿设计并结合强化学习控制，实现了优越的全地形适应性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有轮腿机器人设计未能充分发挥腿部和轮式结构的优势，限制了系统的灵活性和效率。FLORES旨在通过创新的前后腿结构和先进的控制策略，克服这些局限性，提升机器人在不同环境下的适应性和效率。

Method: 提出了一种名为FLORES的新型轮腿混合机器人设计，其前腿取消了传统髋关节的横滚自由度，增加了两个方位自由度，使其在平坦路面和复杂地形上都能高效移动。同时，开发了一个定制化的强化学习控制器，用于适应混合内部模型（HIM）和优化的奖励机制，以实现多模态运动策略和轮式与腿式运动的平滑过渡。

Result: 通过综合实验证明，FLORES机器人具有增强的转向能力、更高的导航效率和在各种地形上的通用运动能力，其独特的关节设计使其能够实现结合轮腿优势的新型高效运动步态。

Conclusion: FLORES机器人通过独特的仿人前腿设计，实现了在平坦和复杂地形之间的无缝切换和高效移动，并通过定制化的强化学习控制器优化了混合运动模式，展示了优越的转向能力、导航效率和跨地形适应性。

Abstract: Wheel-legged robots integrate the agility of legs for navigating rough
terrains while harnessing the efficiency of wheels for smooth surfaces.
However, most existing designs do not fully capitalize on the benefits of both
legged and wheeled structures, which limits overall system flexibility and
efficiency. We present FLORES (reconfigured wheel-legged robot for enhanced
steering and adaptability), a novel wheel-legged robot design featuring a
distinctive front-leg configuration that sets it beyond standard design
approaches. Specifically, FLORES replaces the conventional hip-roll degree of
freedom (DoF) of the front leg with hip-yaw DoFs, and this allows for efficient
movement on flat surfaces while ensuring adaptability when navigating complex
terrains. This innovative design facilitates seamless transitions between
different locomotion modes (i.e., legged locomotion and wheeled locomotion) and
optimizes the performance across varied environments. To fully exploit FLORES's
mechanical capabilities, we develop a tailored reinforcement learning (RL)
controller that adapts the Hybrid Internal Model (HIM) with a customized reward
structure optimized for our unique mechanical configuration. This framework
enables the generation of adaptive, multi-modal locomotion strategies that
facilitate smooth transitions between wheeled and legged movements.
Furthermore, our distinctive joint design enables the robot to exhibit novel
and highly efficient locomotion gaits that capitalize on the synergistic
advantages of both locomotion modes. Through comprehensive experiments, we
demonstrate FLORES's enhanced steering capabilities, improved navigation
efficiency, and versatile locomotion across various terrains. The open-source
project can be found at
https://github.com/ZhichengSong6/FLORES-A-Reconfigured-Wheel-Legged-Robot-for-Enhanced-Steering-and-Adaptability.git.

</details>


### [152] [In-Situ Soil-Property Estimation and Bayesian Mapping with a Simulated Compact Track Loader](https://arxiv.org/abs/2507.22356)
*W. Jacob Wagner,Ahmet Soylemezoglu,Katherine Driggs-Campbell*

Main category: cs.RO

TL;DR: 该研究提出了一种新的土壤感知自主地球工程方法，通过改进的GPU加速表层映射系统和物理信息神经网络，实现了对土壤属性的跟踪，克服了现有技术的局限性，并有望实现更鲁棒的自主地球工程。


<details>
  <summary>Details</summary>
Motivation: 为了克服现有仅限于高度受控和特征明确的环境的地球工程自主性的限制，并促进更鲁棒的自主地球工程的发展。

Method: 提出了一种GPU加速的包含盲映射组件的表层改进系统，该组件通过跟踪刀片在地形中的移动来分解和侵蚀土壤，从而分别跟踪未扰动和已扰动的土壤。利用改进的物理信息神经网络（PINN）模型来预测土壤属性和估计的不确定性。

Result: 该系统能够准确地标示出需要更高相对相互作用力的区域，表明了该方法在实现土壤感知的自主地形塑造规划方面的潜力。

Conclusion: 该方法通过在地图中包含土壤属性信息，实现了土壤感知的自主地形塑造，并展示了其在突出显示需要更高相互作用力的区域方面的准确性，表明了该方法在实现土壤感知规划方面的潜力。

Abstract: Existing earthmoving autonomy is largely confined to highly controlled and
well-characterized environments due to the complexity of vehicle-terrain
interaction dynamics and the partial observability of the terrain resulting
from unknown and spatially varying soil conditions. In this chapter, a a
soil-property mapping system is proposed to extend the environmental state, in
order to overcome these restrictions and facilitate development of more robust
autonomous earthmoving. A GPU accelerated elevation mapping system is extended
to incorporate a blind mapping component which traces the movement of the blade
through the terrain to displace and erode intersected soil, enabling separately
tracking undisturbed and disturbed soil. Each interaction is approximated as a
flat blade moving through a locally homogeneous soil, enabling modeling of
cutting forces using the fundamental equation of earthmoving (FEE). Building
upon our prior work on in situ soil-property estimation, a method is devised to
extract approximate geometric parameters of the model given the uneven terrain,
and an improved physics infused neural network (PINN) model is developed to
predict soil properties and uncertainties of these estimates. A simulation of a
compact track loader (CTL) with a blade attachment is used to collect data to
train the PINN model. Post-training, the model is leveraged online by the
mapping system to track soil property estimates spatially as separate layers in
the map, with updates being performed in a Bayesian manner. Initial experiments
show that the system accurately highlights regions requiring higher relative
interaction forces, indicating the promise of this approach in enabling
soil-aware planning for autonomous terrain shaping.

</details>


### [153] [Improving Generalization Ability of Robotic Imitation Learning by Resolving Causal Confusion in Observations](https://arxiv.org/abs/2507.22380)
*Yifei Chen,Yuzhe Zhang,Giovanni D'urso,Nicholas Lawrance,Brendan Tidd*

Main category: cs.RO

TL;DR: 提出了一种简单的因果结构学习框架，以提高复杂模仿学习算法在机器人操作中的泛化能力，并通过实验证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 当前模仿学习技术泛化能力较差，限制了在处理训练环境与部署环境之间微小变化时的性能。

Method: 提出了一种简单的因果结构学习框架，通过干预模仿学习策略来学习因果结构函数，并证明了特征解耦并非必要条件。

Result: 在Mujoco中ALOHA双臂机器人模拟中进行了演示，并证明了该方法可以显著缓解现有复杂模仿学习算法的泛化问题。

Conclusion: 该方法能够显著缓解现有复杂模仿学习算法的泛化问题。

Abstract: Recent developments in imitation learning have considerably advanced robotic
manipulation. However, current techniques in imitation learning can suffer from
poor generalization, limiting performance even under relatively minor domain
shifts. In this work, we aim to enhance the generalization capabilities of
complex imitation learning algorithms to handle unpredictable changes from the
training environments to deployment environments. To avoid confusion caused by
observations that are not relevant to the target task, we propose to explicitly
learn the causal relationship between observation components and expert
actions, employing a framework similar to [6], where a causal structural
function is learned by intervention on the imitation learning policy.
Disentangling the feature representation from image input as in [6] is hard to
satisfy in complex imitation learning process in robotic manipulation, we
theoretically clarify that this requirement is not necessary in causal
relationship learning. Therefore, we propose a simple causal structure learning
framework that can be easily embedded in recent imitation learning
architectures, such as the Action Chunking Transformer [31]. We demonstrate our
approach using a simulation of the ALOHA [31] bimanual robot arms in Mujoco,
and show that the method can considerably mitigate the generalization problem
of existing complex imitation learning algorithms.

</details>


### [154] [Safety Evaluation of Motion Plans Using Trajectory Predictors as Forward Reachable Set Estimators](https://arxiv.org/abs/2507.22389)
*Kaustav Chakraborty,Zeyuan Feng,Sushant Veer,Apoorva Sharma,Wenhao Ding,Sever Topan,Boris Ivanovic,Marco Pavone,Somil Bansal*

Main category: cs.RO

TL;DR: 提出了一种利用多模态轨迹预测器和共形预测来近似前向可达集（FRS）的安全监控器，以确保自动驾驶运动计划的安全。该方法通过贝叶斯滤波器动态调整 FRS 的保守性，以应对分布外场景和预测器故障，从而提高了鲁棒性并保持了完整性。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏可解释的中间模块，端到端自动驾驶堆栈的出现增加了确保最终输出（即运动计划）安全的负担，以验证整个堆栈的安全性。这需要一个既完整（能够检测所有不安全的计划）又鲁棒（不将安全的计划标记为不安全）的安全监控器。

Method: 提出了一种原则性的安全监控器，该监控器利用现代多模态轨迹预测器来近似周围代理器的前向可达集（FRS）。通过构建一个凸程序，直接从预测的状态分布中有效地提取数据驱动的 FRS，并以车道拓扑和代理器历史等场景上下文为条件。为了确保完整性，利用共形预测来校准 FRS，并保证以高概率覆盖地面真实轨迹。为了在分布外（OOD）场景或预测器故障下保持鲁棒性，引入了一个贝叶斯滤波器，根据预测器的观测性能动态调整 FRS 的保守性。然后，通过检查与这些校准的 FRS 的交叉情况来评估自车运动计划的安全性，确保在其他代理器可能发生的未来行为下，该计划保持无碰撞。

Result: 通过检查运动计划与周围代理器（此处的代理器是指场景中的其他机动实体，如汽车、行人和骑自行车的人）的可达集之间的交叉情况，可以评估自车运动计划的安全性。如果自车运动计划与任何代理器的可达集发生交叉，那么该计划就被认为是“不安全”的。

Conclusion: 该方法在 nuScenes 数据集上进行了广泛的实验，结果表明，在保持完整性的同时，显著提高了鲁棒性，为学习到的自动驾驶堆栈提供了实用且可靠的安全监控器。

Abstract: The advent of end-to-end autonomy stacks - often lacking interpretable
intermediate modules - has placed an increased burden on ensuring that the
final output, i.e., the motion plan, is safe in order to validate the safety of
the entire stack. This requires a safety monitor that is both complete (able to
detect all unsafe plans) and sound (does not flag safe plans). In this work, we
propose a principled safety monitor that leverages modern multi-modal
trajectory predictors to approximate forward reachable sets (FRS) of
surrounding agents. By formulating a convex program, we efficiently extract
these data-driven FRSs directly from the predicted state distributions,
conditioned on scene context such as lane topology and agent history. To ensure
completeness, we leverage conformal prediction to calibrate the FRS and
guarantee coverage of ground-truth trajectories with high probability. To
preserve soundness in out-of-distribution (OOD) scenarios or under predictor
failure, we introduce a Bayesian filter that dynamically adjusts the FRS
conservativeness based on the predictor's observed performance. We then assess
the safety of the ego vehicle's motion plan by checking for intersections with
these calibrated FRSs, ensuring the plan remains collision-free under plausible
future behaviors of others. Extensive experiments on the nuScenes dataset show
our approach significantly improves soundness while maintaining completeness,
offering a practical and reliable safety monitor for learned autonomy stacks.

</details>


### [155] [Comparing Normalizing Flows with Kernel Density Estimation in Estimating Risk of Automated Driving Systems](https://arxiv.org/abs/2507.22429)
*Erwin de Gelder,Maren Buermann,Olaf Op den Camp*

Main category: cs.RO

TL;DR: 归一化流（NF）在自动驾驶系统（ADS）安全验证的场景库暴露度估计中，比传统方法（KDE）更能抵抗维度灾难，提供更精确的风险评估，但计算成本更高。


<details>
  <summary>Details</summary>
Motivation: 为了安全部署和运行自动驾驶系统（ADS），必须开发安全验证方法，并准确估计场景的暴露度（以PDF表示），以进行量化分析。然而，传统的PDF估计方法在避免假设或处理高维数据时存在局限性。

Method: 使用归一化流（NF）估计参数的概率密度函数（PDF），并将其与核密度估计（KDE）进行比较，以评估自动驾驶系统（ADS）的风险和风险不确定性。

Result: 归一化流（NF）在风险不确定性估计方面优于核密度估计（KDE），能够提供更精确的ADS安全评估。

Conclusion: 使用归一化流（NF）进行参数概率密度函数（PDF）估计，相比于传统的核密度估计（KDE），能够提供更精确的自动驾驶系统（ADS）风险和风险不确定性评估，尽管计算资源消耗更大，但NF更能抵抗维度灾难。

Abstract: The development of safety validation methods is essential for the safe
deployment and operation of Automated Driving Systems (ADSs). One of the goals
of safety validation is to prospectively evaluate the risk of an ADS dealing
with real-world traffic. Scenario-based assessment is a widely-used approach,
where test cases are derived from real-world driving data. To allow for a
quantitative analysis of the system performance, the exposure of the scenarios
must be accurately estimated. The exposure of scenarios at parameter level is
expressed using a Probability Density Function (PDF). However, assumptions
about the PDF, such as parameter independence, can introduce errors, while
avoiding assumptions often leads to oversimplified models with limited
parameters to mitigate the curse of dimensionality.
  This paper considers the use of Normalizing Flows (NF) for estimating the PDF
of the parameters. NF are a class of generative models that transform a simple
base distribution into a complex one using a sequence of invertible and
differentiable mappings, enabling flexible, high-dimensional density estimation
without restrictive assumptions on the PDF's shape. We demonstrate the
effectiveness of NF in quantifying risk and risk uncertainty of an ADS,
comparing its performance with Kernel Density Estimation (KDE), a traditional
method for non-parametric PDF estimation. While NF require more computational
resources compared to KDE, NF is less sensitive to the curse of dimensionality.
As a result, NF can improve risk uncertainty estimation, offering a more
precise assessment of an ADS's safety.
  This work illustrates the potential of NF in scenario-based safety. Future
work involves experimenting more with using NF for scenario generation and
optimizing the NF architecture, transformation types, and training
hyperparameters to further enhance their applicability.

</details>


### [156] [Operationalization of Scenario-Based Safety Assessment of Automated Driving Systems](https://arxiv.org/abs/2507.22433)
*Olaf Op den Camp,Erwin de Gelder*

Main category: cs.RO

TL;DR: This paper explains how to use scenario databases for ADS safety assessment according to the new UNECE NATM guidelines and suggests further steps for full implementation.


<details>
  <summary>Details</summary>
Motivation: The UNECE WP.29 GRVA is developing the New Assessment/Test Method (NATM) to structure and harmonize the safety assurance process for Automated Driving Systems (ADS) before their large-scale deployment.

Method: The paper proposes a practical approach to safety assessment using a scenario database and discusses its integration with methods from Horizon Europe projects that follow the NATM approach.

Result: The paper demonstrates how to practically conduct safety assessment using a scenario database and outlines the additional steps required for NATM operationalization.

Conclusion: To fully operationalize the NATM, practical safety assessment using scenario databases needs to be combined with additional steps.

Abstract: Before introducing an Automated Driving System (ADS) on the road at scale,
the manufacturer must conduct some sort of safety assurance. To structure and
harmonize the safety assurance process, the UNECE WP.29 Working Party on
Automated/Autonomous and Connected Vehicles (GRVA) is developing the New
Assessment/Test Method (NATM) that indicates what steps need to be taken for
safety assessment of an ADS. In this paper, we will show how to practically
conduct safety assessment making use of a scenario database, and what
additional steps must be taken to fully operationalize the NATM. In addition,
we will elaborate on how the use of scenario databases fits with methods
developed in the Horizon Europe projects that focus on safety assessment
following the NATM approach.

</details>


### [157] [A Two-Stage Lightweight Framework for Efficient Land-Air Bimodal Robot Autonomous Navigation](https://arxiv.org/abs/2507.22473)
*Yongjie Li,Zhou Liu,Wenshuai Yu,Zhangji Lu,Chenyang Wang,Fei Yu,Qingquan Li*

Main category: cs.RO

TL;DR: 提出了一种两阶段轻量级框架，用于优化陆空双模式机器人的导航轨迹，显著减少了网络参数和陆空转换能耗，并实现了高效的实时导航。


<details>
  <summary>Details</summary>
Motivation: 现有陆空双模式机器人（LABR）导航方法受限于基于映射的方法产生的次优轨迹以及基于学习的方法过高的计算需求。

Method: 提出了一种两阶段轻量级框架，结合了全局关键点预测和局部轨迹优化，以生成高效且可达的轨迹。第一阶段使用全局关键点预测网络（GKPN）生成陆空关键点路径，GKPN 包括用于改进障碍物检测的 Sobel 感知网络（SPN）和用于通过捕获上下文信息来提高预测能力的轻量级注意力规划网络（LAPN）。第二阶段，基于预测的关键点对全局路径进行分段，并使用基于映射的规划器进行优化，以创建平滑、无碰撞的轨迹。

Result: 与现有方法相比，该框架的网络参数减少了 14%，陆空转换期间的能耗降低了 35%。

Conclusion: 该框架实现了实时导航，无需 GPU 加速，并实现了从模拟到现实的零样本迁移。

Abstract: Land-air bimodal robots (LABR) are gaining attention for autonomous
navigation, combining high mobility from aerial vehicles with long endurance
from ground vehicles. However, existing LABR navigation methods are limited by
suboptimal trajectories from mapping-based approaches and the excessive
computational demands of learning-based methods. To address this, we propose a
two-stage lightweight framework that integrates global key points prediction
with local trajectory refinement to generate efficient and reachable
trajectories. In the first stage, the Global Key points Prediction Network
(GKPN) was used to generate a hybrid land-air keypoint path. The GKPN includes
a Sobel Perception Network (SPN) for improved obstacle detection and a
Lightweight Attention Planning Network (LAPN) to improves predictive ability by
capturing contextual information. In the second stage, the global path is
segmented based on predicted key points and refined using a mapping-based
planner to create smooth, collision-free trajectories. Experiments conducted on
our LABR platform show that our framework reduces network parameters by 14\%
and energy consumption during land-air transitions by 35\% compared to existing
approaches. The framework achieves real-time navigation without GPU
acceleration and enables zero-shot transfer from simulation to reality during

</details>


### [158] [Explainable Deep Anomaly Detection with Sequential Hypothesis Testing for Robotic Sewer Inspection](https://arxiv.org/abs/2507.22546)
*Alex George,Will Shepherd,Simon Tait,Lyudmila Mihaylova,Sean R. Anderson*

Main category: cs.RO

TL;DR: Automated sewer inspection using explainable AI and SPRT improves fault detection by analyzing images and their temporal sequence, overcoming limitations of manual review.


<details>
  <summary>Details</summary>
Motivation: Traditional CCTV footage manual review for sewer pipe fault detection is inefficient and prone to human error. The motivation is to automate this process for improved accuracy and reliability.

Method: The proposed system integrates explainable deep learning for anomaly detection in individual image frames, providing spatial localization of faults. It further incorporates sequential probability ratio testing (SPRT) to aggregate temporal evidence from image sequences, enhancing robustness against noise.

Result: Experimental results show enhanced anomaly detection performance, demonstrating the effectiveness of the combined spatiotemporal analysis system.

Conclusion: Sewer pipe fault detection can be automated using a novel system that combines explainable deep learning anomaly detection with sequential probability ratio testing (SPRT). This approach improves detection performance by analyzing both spatial and temporal data, making sewer inspection more reliable and robust.

Abstract: Sewer pipe faults, such as leaks and blockages, can lead to severe
consequences including groundwater contamination, property damage, and service
disruption. Traditional inspection methods rely heavily on the manual review of
CCTV footage collected by mobile robots, which is inefficient and susceptible
to human error. To automate this process, we propose a novel system
incorporating explainable deep learning anomaly detection combined with
sequential probability ratio testing (SPRT). The anomaly detector processes
single image frames, providing interpretable spatial localisation of anomalies,
whilst the SPRT introduces temporal evidence aggregation, enhancing robustness
against noise over sequences of image frames. Experimental results demonstrate
improved anomaly detection performance, highlighting the benefits of the
combined spatiotemporal analysis system for reliable and robust sewer
inspection.

</details>


### [159] [UniLegs: Universal Multi-Legged Robot Control through Morphology-Agnostic Policy Distillation](https://arxiv.org/abs/2507.22653)
*Weijie Xi,Zhanxiang Cao,Chenlin Ming,Jianying Zheng,Guyue Zhou*

Main category: cs.RO

TL;DR: 开发了一种基于 Transformer 的师生框架，用于双足运动控制，能够适应不同机器人形态，在保持性能的同时实现泛化。


<details>
  <summary>Details</summary>
Motivation: 为适应不同机器人形态而开发的控制器在双足运动中仍然是一个重大挑战。传统方法要么为每种形态创建专用控制器，要么为了通用性而牺牲性能。

Method: 提出一个两阶段师生框架，首先为每个机器人形态训练专门的教师策略，然后通过策略蒸馏将专业知识提炼到一个基于 Transformer 的学生策略中。

Result: 所提出的 Transformer 学生策略在训练形态上达到了教师性能的 94.47%，在未见过的机器人设计上达到了 72.64%。与 MLP 基线相比，Transformer 架构通过注意力机制有效模拟关节关系，在不同运动结构上表现更优。

Conclusion: 该研究提出了一种通过策略蒸馏的两阶段师生框架，用于开发能够推广到不同机器人形态的双足运动控制器。该框架通过训练特定形态的教师策略，然后将专业知识提炼到基于 Transformer 的学生策略中，实现了跨不同腿部配置的机器人控制。实验表明，该方法能够保留特定形态的最佳行为，并在训练形态上达到教师性能的 94.47%，在未见过机器人设计上达到 72.64%。基于 Transformer 的架构通过注意力机制有效模拟不同运动结构中的关节关系，性能优于 MLP 基线。该方法已成功应用于物理四足机器人，证明了其通用性

Abstract: Developing controllers that generalize across diverse robot morphologies
remains a significant challenge in legged locomotion. Traditional approaches
either create specialized controllers for each morphology or compromise
performance for generality. This paper introduces a two-stage teacher-student
framework that bridges this gap through policy distillation. First, we train
specialized teacher policies optimized for individual morphologies, capturing
the unique optimal control strategies for each robot design. Then, we distill
this specialized expertise into a single Transformer-based student policy
capable of controlling robots with varying leg configurations. Our experiments
across five distinct legged morphologies demonstrate that our approach
preserves morphology-specific optimal behaviors, with the Transformer
architecture achieving 94.47\% of teacher performance on training morphologies
and 72.64\% on unseen robot designs. Comparative analysis reveals that
Transformer-based architectures consistently outperform MLP baselines by
leveraging attention mechanisms to effectively model joint relationships across
different kinematic structures. We validate our approach through successful
deployment on a physical quadruped robot, demonstrating the practical viability
of our morphology-agnostic control framework. This work presents a scalable
solution for developing universal legged robot controllers that maintain
near-optimal performance while generalizing across diverse morphologies.

</details>


### [160] [Bayesian Optimization applied for accelerated Virtual Validation of the Autonomous Driving Function](https://arxiv.org/abs/2507.22769)
*Satyesh Shanker Awasthi,Mohammed Irshadh Ismaaeel Sathyamangalam Imran,Stefano Arrigoni,Francesco Braghin*

Main category: cs.RO

TL;DR: 使用贝叶斯优化（BO）框架来加速自动驾驶场景发现，相比传统方法，模拟次数更少。


<details>
  <summary>Details</summary>
Motivation: 为了实现自动驾驶功能（ADFs）的严格验证和确认（V&V），以确保自动驾驶汽车（AVs）的安全性和公众接受度，但目前的验证方法（如模拟）在探索参数空间时计算成本高昂且耗时。有必要加速关键场景的发现。

Method: 提出一个基于贝叶斯优化（BO）的框架，用于加速自动驾驶功能（ADFs）关键场景的发现，并验证了其在MPC（模型预测控制）运动规划器上的有效性，包括在高维参数空间中的可扩展性。

Result: 该框架能识别危险情况（如驶离道路事件），其使用的模拟次数比暴力设计实验（DoE）方法少几个数量级。此外，该研究还考察了该框架在高维参数空间中的可扩展性，以及其在运动规划器ODD（运行设计域）中识别多个不同关键区域的能力。

Conclusion: 该框架基于贝叶斯优化（BO），可加速关键场景的发现，并在更少模拟的情况下识别危险情况，且能扩展到高维参数空间。

Abstract: Rigorous Verification and Validation (V&V) of Autonomous Driving Functions
(ADFs) is paramount for ensuring the safety and public acceptance of Autonomous
Vehicles (AVs). Current validation relies heavily on simulation to achieve
sufficient test coverage within the Operational Design Domain (ODD) of a
vehicle, but exhaustively exploring the vast parameter space of possible
scenarios is computationally expensive and time-consuming. This work introduces
a framework based on Bayesian Optimization (BO) to accelerate the discovery of
critical scenarios. We demonstrate the effectiveness of the framework on an
Model Predictive Controller (MPC)-based motion planner, showing that it
identifies hazardous situations, such as off-road events, using orders of
magnitude fewer simulations than brute-force Design of Experiments (DoE)
methods. Furthermore, this study investigates the scalability of the framework
in higher-dimensional parameter spaces and its ability to identify multiple,
distinct critical regions within the ODD of the motion planner used as the case
study .

</details>


### [161] [GABRIL: Gaze-Based Regularization for Mitigating Causal Confusion in Imitation Learning](https://arxiv.org/abs/2507.19647)
*Amin Banayeeanzade,Fatemeh Bahrani,Yutai Zhou,Erdem Bıyık*

Main category: cs.RO

TL;DR: GABRIL利用人类注视数据解决模仿学习中的因果混淆问题，显著提高性能并增强可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决模仿学习（IL）中存在的因果混淆问题，即智能体错误地将虚假关联解释为因果关系，导致在分布变化的测试环境中表现不佳。

Method: 提出了一种名为GABRIL（GAze-Based Regularization in Imitation Learning）的新方法，该方法利用收集到的人类注视数据来指导模仿学习（IL）中的表示学习，并通过引入正则化损失来鼓励模型关注由专家注视确定的因果相关特征，从而减轻混淆变量的影响。

Result: 在Atari环境和CARLA的Bench2Drive基准测试中，GABRIL相比于行为克隆（behavior cloning）在Atari环境中提升了179%，在CARLA环境中提升了76%。此外，与常规IL智能体相比，GABRIL提供了额外可解释性。

Conclusion: GABRIL通过利用人类注视数据来指导IL中的表示学习，成功解决了因果混淆问题，并在Atari和CARLA环境中取得了显著的性能提升，同时还提供了额外的可解释性。

Abstract: Imitation Learning (IL) is a widely adopted approach which enables agents to
learn from human expert demonstrations by framing the task as a supervised
learning problem. However, IL often suffers from causal confusion, where agents
misinterpret spurious correlations as causal relationships, leading to poor
performance in testing environments with distribution shift. To address this
issue, we introduce GAze-Based Regularization in Imitation Learning (GABRIL), a
novel method that leverages the human gaze data gathered during the data
collection phase to guide the representation learning in IL. GABRIL utilizes a
regularization loss which encourages the model to focus on causally relevant
features identified through expert gaze and consequently mitigates the effects
of confounding variables. We validate our approach in Atari environments and
the Bench2Drive benchmark in CARLA by collecting human gaze datasets and
applying our method in both domains. Experimental results show that the
improvement of GABRIL over behavior cloning is around 179% more than the same
number for other baselines in the Atari and 76% in the CARLA setup. Finally, we
show that our method provides extra explainability when compared to regular IL
agents.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [162] [Pendulum Model of Spiking Neurons](https://arxiv.org/abs/2507.22146)
*Joy Bose*

Main category: cs.NE

TL;DR: 受生物启发的脉冲神经元模型，基于摆锤动力学，具有二阶非线性动力学，可生成振荡行为和基于相位发放脉冲，用于序列处理和符号学习，并可部署到神经拟态硬件上。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉更丰富的时间特征并支持对序列处理和符号学习至关重要的计时敏感计算。

Method: 提出了一种受生物启发的、基于阻尼驱动摆锤动力学的脉冲神经元模型，并分析了单神经元动力学，将其扩展到具有STDP学习规则的多神经元层，并提供了在神经拟态硬件上实现的近似方法。

Result: 所提出的模型能够生成振荡行为和基于相位发放脉冲，从而捕捉到更丰富的时间动态，并为序列处理和符号学习等任务提供了支持。

Conclusion: 该模型为神经拟态计算和序列认知任务提供了基础，可用于开发节能的神经系统。

Abstract: We propose a biologically inspired model of spiking neurons based on the
dynamics of a damped, driven pendulum. Unlike traditional models such as the
Leaky Integrate-and-Fire (LIF) neurons, the pendulum neuron incorporates
second-order, nonlinear dynamics that naturally give rise to oscillatory
behavior and phase-based spike encoding. This model captures richer temporal
features and supports timing-sensitive computations critical for sequence
processing and symbolic learning. We present an analysis of single-neuron
dynamics and extend the model to multi-neuron layers governed by Spike-Timing
Dependent Plasticity (STDP) learning rules. We demonstrate practical
implementation with python code and with the Brian2 spiking neural simulator,
and outline a methodology for deploying the model on neuromorphic hardware
platforms, using an approximation of the second-order equations. This framework
offers a foundation for developing energy-efficient neural systems for
neuromorphic computing and sequential cognition tasks.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [163] [A Customized Memory-aware Architecture for Biological Sequence Alignment](https://arxiv.org/abs/2507.22221)
*Nasrin Akbari,Mehdi Modarressi,Alireza Khadem*

Main category: cs.AR

TL;DR: 该研究提出了一种内存感知架构，可将计算移近内存，从而提高序列比对的性能并降低功耗。


<details>
  <summary>Details</summary>
Motivation: 为了应对生物信息学数据库中数据量的指数级增长以及传统并行机器上序列比对算法的内存带宽需求限制，人们需要一种新的方法来提高吞吐量。

Method: 提出了一种内存感知架构，该架构在新兴的 3D DRAM 的逻辑层中作为内存处理架构进行集成，以降低序列比对算法的带宽需求。

Result: 与基于 GPU 的设计相比，所提出的架构实现了高达 2.4 倍的速度提升，并平均将功耗降低了 37%。

Conclusion: 所提出的内存感知架构通过将计算移近内存，将内存带宽需求降低了 37%，从而实现了比基于 GPU 的设计高出 2.4 倍的速度提升，并降低了功耗。

Abstract: Sequence alignment is a fundamental process in computational biology which
identifies regions of similarity in biological sequences. With the exponential
growth in the volume of data in bioinformatics databases, the time, processing
power, and memory bandwidth for comparing a query sequence with the available
databases grows proportionally. The sequence alignment algorithms often involve
simple arithmetic operations and feature high degrees of inherent fine-grained
and coarse-grained parallelism. These features can be potentially exploited by
a massive parallel processor, such as a GPU, to increase throughput. In this
paper, we show that the excessive memory bandwidth demand of the sequence
alignment algorithms prevents exploiting the maximum achievable throughput on
conventional parallel machines. We then propose a memory-aware architecture to
reduce the bandwidth demand of the sequence alignment algorithms, effectively
pushing the memory wall to extract higher throughput. The design is integrated
at the logic layer of an emerging 3D DRAM as a processing-in-memory
architecture to further increase the available bandwidth. The experimental
results show that the proposed architecture results in up to 2.4x speedup over
a GPU-based design. Moreover, by moving the computation closer to the memory,
power consumption is reduced by 37%, on average.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [164] [Minimizing CGYRO HPC Communication Costs in Ensembles with XGYRO by Sharing the Collisional Constant Tensor Structure](https://arxiv.org/abs/2507.22245)
*Igor Sfiligoi,Emily A. Belli,Jeff Candy*

Main category: cs.DC

TL;DR: XGYRO 工具通过将多个 CGYRO 模拟集合作为一个整体运行，优化了内存和通信开销，尤其是在共享数据结构方面。


<details>
  <summary>Details</summary>
Motivation: CGYRO 模拟在计算和内存方面要求很高，单个模拟的通信开销难以避免。大多数聚变研究涉及多个模拟的集合，因此需要一种优化方法来处理这些模拟集合。

Method: 开发了一个名为 XGYRO 的新工具，该工具将整个 CGYRO 模拟集合作为单个 HPC 作业执行，通过共享碰撞常数张量结构等优化手段，实现内存和通信开销的降低。

Result: XGYRO 通过共享碰撞常数张量结构，实现了显著的内存节省，从而降低了通信开销，使得处理模拟集合更加高效。

Conclusion: XGYRO 通过将模拟整体化处理，实现了 CGYRO 模拟的内存和通信开销的大幅降低，尤其是在处理模拟集合时。

Abstract: First-principles fusion plasma simulations are both compute and memory
intensive, and CGYRO is no exception. The use of many HPC nodes to fit the
problem in the available memory thus results in significant communication
overhead, which is hard to avoid for any single simulation. That said, most
fusion studies are composed of ensembles of simulations, so we developed a new
tool, named XGYRO, that executes a whole ensemble of CGYRO simulations as a
single HPC job. By treating the ensemble as a unit, XGYRO can alter the global
buffer distribution logic and apply optimizations that are not feasible on any
single simulation, but only on the ensemble as a whole. The main saving comes
from the sharing of the collisional constant tensor structure, since its values
are typically identical between parameter-sweep simulations. This data
structure dominates the memory consumption of CGYRO simulations, so
distributing it among the whole ensemble results in drastic memory savings for
each simulation, which in turn results in overall lower communication overhead.

</details>


### [165] [Towards Experiment Execution in Support of Community Benchmark Workflows for HPC](https://arxiv.org/abs/2507.22294)
*Gregor von Laszewski,Wesley Brewer,Sean R. Wilkinson,Andrew Shao,J. P. Fleischer,Harshad Pitkar,Christine R. Kirkpatrick,Geoffrey C. Fox*

Main category: cs.DC

TL;DR: Workflow templates and 'benchmark carpentry' improve compute resource demonstration, validated by tools like Cloudmesh and SmartSim in scientific applications.


<details>
  <summary>Details</summary>
Motivation: Addresses the key hurdle of demonstrating compute resource capability with limited benchmarks by offering adaptable workflow templates for specific scientific applications.

Method: Proposes workflow templates and introduces the concept of 'benchmark carpentry', identifying common usage patterns for these templates based on HPC experience and MLCommons Science working group data. Validates the approach using two independent tools tested on scientific applications.

Result: Identified common usage patterns for workflow templates, showing that focusing on simple experiment management tools improves adaptability, especially in education. Validated the 'benchmark carpentry' concept through two tools tested on applications like conduction cloudmask, earthquake prediction, simulation-AI/ML interactions, and CFD surrogate development.

Conclusion: The paper proposes workflow templates and 'benchmark carpentry' as a solution to demonstrate compute resource capability, validated by two tools (Cloudmesh's Experiment Executor and HPE's SmartSim) across various scientific applications.

Abstract: A key hurdle is demonstrating compute resource capability with limited
benchmarks. We propose workflow templates as a solution, offering adaptable
designs for specific scientific applications. Our paper identifies common usage
patterns for these templates, drawn from decades of HPC experience, including
recent work with the MLCommons Science working group.
  We found that focusing on simple experiment management tools within the
broader computational workflow improves adaptability, especially in education.
This concept, which we term benchmark carpentry, is validated by two
independent tools: Cloudmesh's Experiment Executor and Hewlett Packard
Enterprise's SmartSim. Both frameworks, with significant functional overlap,
have been tested across various scientific applications, including conduction
cloudmask, earthquake prediction, simulation-AI/ML interactions, and the
development of computational fluid dynamics surrogates.

</details>


### [166] [A Semi-Supervised Federated Learning Framework with Hierarchical Clustering Aggregation for Heterogeneous Satellite Networks](https://arxiv.org/abs/2507.22339)
*Zhuocheng Liu,Zhishu Shen,Qiushi Zheng,Tiehua Zhang,Zheng Lei,Jiong Jin*

Main category: cs.DC

TL;DR: 提出了一种针对LEO卫星网络的半监督联邦学习框架，通过分层聚类聚合、稀疏化和自适应权重量化技术，有效降低了处理时间和能源消耗，同时保持了模型准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限、异构和部分标记的卫星网络中，在最小化处理时间和能源消耗的同时实现可靠的联邦学习收敛是一个重大挑战。

Method: 提出了一种新颖的半监督联邦学习框架，该框架采用分层聚类聚合，并集成了稀疏化和自适应权重量化技术来降低通信开销。联邦学习聚类分为两个阶段：卫星集群聚合阶段和地面站（GS）聚合阶段。GS处的监督学习指导选定的参数服务器（PS）卫星，进而支持完全未标记的卫星在联邦训练过程中的训练。

Result: 实验证明，与比较方法相比，该方法可将处理时间减少多达3倍，能源消耗减少多达4倍，同时保持模型准确性。

Conclusion: 该框架显著降低了处理时间和能源消耗（分别高达3倍和4倍），同时保持了模型准确性。

Abstract: Low Earth Orbit (LEO) satellites are emerging as key components of 6G
networks, with many already deployed to support large-scale Earth observation
and sensing related tasks. Federated Learning (FL) presents a promising
paradigm for enabling distributed intelligence in these resource-constrained
and dynamic environments. However, achieving reliable convergence, while
minimizing both processing time and energy consumption, remains a substantial
challenge, particularly in heterogeneous and partially unlabeled satellite
networks. To address this challenge, we propose a novel semi-supervised
federated learning framework tailored for LEO satellite networks with
hierarchical clustering aggregation. To further reduce communication overhead,
we integrate sparsification and adaptive weight quantization techniques. In
addition, we divide the FL clustering into two stages: satellite cluster
aggregation stage and Ground Stations (GSs) aggregation stage. The supervised
learning at GSs guides selected Parameter Server (PS) satellites, which in turn
support fully unlabeled satellites during the federated training process.
Extensive experiments conducted on a satellite network testbed demonstrate that
our proposal can significantly reduce processing time (up to 3x) and energy
consumption (up to 4x) compared to other comparative methods while maintaining
model accuracy.

</details>


### [167] [Leveraging Caliper and Benchpark to Analyze MPI Communication Patterns: Insights from AMG2023, Kripke, and Laghos](https://arxiv.org/abs/2507.22372)
*Grace Nansamba,Evelyn Namugwanya,David Boehme,Dewi Yokelson,Riley Shipley,Derek Schafer,Michael McKinsey,Olga Pearce,Anthony Skjellum*

Main category: cs.DC

TL;DR: Caliper 工具通过引入“通信区域”功能，增强了对 MPI 通信的分析能力，能够提供通信数据和进程的详细指标，并通过与 Thicket 结合进行可视化，有助于发现性能瓶颈和理解扩展行为。


<details>
  <summary>Details</summary>
Motivation: 为了捕捉通信数据的指标以及参与通信的 MPI 进程的指标，以解决在广泛使用的 Caliper HPC（高性能计算）分析工具中无法实现这一目标的问题。

Method: 在 Caliper 工具中引入“通信区域”功能，该功能允许捕获通信数据的指标（包括统计信息）以及参与通信的 MPI 进程的指标。结合 Caliper 和 Thicket 工具，创建 MPI 通信模式（包括 halo 交换）的可视化。

Result: 增强的 Caliper 工具能够揭示详细的通信行为。通过结合 Caliper 和 Thicket，能够创建新的 MPI 通信模式可视化，例如 halo 交换。研究发现了通信瓶颈和详细行为，并展示了 CPU 和 GPU 系统的对比扩展行为，以及不同区域的可扩展性和消息流量指标的差异。

Conclusion: 引入通信区域到 Caliper 工具中，能捕捉通信数据的指标（包括统计数据）以及参与通信的 MPI 进程的指标，这在以前的 Caliper 工具中是不可能的。通过在 AMG2023、Kripke 和 Laghos 三个应用程序中进行实验，并结合 Caliper 和 Thicket，我们能够创建新的 MPI 通信模式（包括 halo 交换）可视化。研究结果揭示了通信瓶颈和详细行为，表明通信区域的添加具有显著的实用价值。我们还展示了 CPU 和 GPU 系统的对比扩展行为，并能够查看给定应用程序内的不同区域，了解可扩展性和消息流量指标的差异。

Abstract: We introduce ``communication regions'' into the widely used Caliper HPC
profiling tool. A communication region is an annotation enabling capture of
metrics about the data being communicated (including statistics of these
metrics), and metrics about the MPI processes involved in the communications,
something not previously possible in Caliper. We explore the utility of
communication regions with three representative modeling and simulation
applications, AMG2023, Kripke, and Laghos, all part of the comprehensive
Benchpark suite that includes Caliper annotations. Enhanced Caliper reveals
detailed communication behaviors. Using Caliper and Thicket in tandem, we
create new visualizations of MPI communication patterns, including halo
exchanges. Our findings reveal communication bottlenecks and detailed
behaviors, indicating significant utility of the special-regions addition to
Caliper. The comparative scaling behavior of both CPU and GPU oriented systems
are shown; we are able to look at different regions within a given application,
and see how scalability and message-traffic metrics differ.

</details>


### [168] [DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic Space Partitioning with Erasure Code](https://arxiv.org/abs/2507.22801)
*Shubhradeep Roy,Suvarthi Sarkar,Vivek Verma,Aryabartta Sahu*

Main category: cs.DC

TL;DR: 通过集成了协作缓存、纠删码和弹性存储分区，并采用动态分区和弹性缓存策略，显著提高了边缘存储系统的盈利能力。


<details>
  <summary>Details</summary>
Motivation: 边缘存储系统通过将存储和计算移近终端用户，成为低延迟数据访问的关键。然而，边缘服务器有限的存储容量在处理高容量和延迟敏感的数据访问请求方面面临挑战，尤其是在动态工作负载下。

Method: 本研究提出了一种以利润为驱动的框架，集成了协作缓存、纠删码和弹性存储分区三种关键机制。具体来说，它动态地将边缘服务器的存储划分为私有和公共区域，私有区域根据接入点的请求速率进一步细分，以实现数据局部性和所有权的自适应控制。此外，还设计了一种数据放置和替换策略，以最大化在截止日期前的数据访问。

Result: 实验结果表明，在不同的工作负载条件下，与最先进的方法相比，该方法将整体系统盈利能力提高了约5%至8%。

Conclusion: Edge Storage Systems通过将存储和计算移近终端用户，成为低延迟数据访问的关键。然而，边缘服务器有限的大容量和延迟敏感数据访问请求，尤其是在动态工作负载下，带来了重大挑战。本研究提出了一个由协作缓存、纠删码和弹性存储分区三个关键机制组成的、以利润为驱动的框架。与传统的复制不同，纠删码实现了空间高效冗余，允许数据从K个编码块中的任何K个子集进行重建。我们动态地将每个边缘服务器的存储划分为私有和公共区域。私有区域根据接入点的请求速率将其进一步细分，从而实现对数据局部性和所有权的自适应控制。我们设计了一种数据放置和替换策略，以确定如何以及在何处存储或驱逐编码数据块，以在截止日期前最大限度地提高数据访问。虽然私有区域服务于本地接入点的请求，但公共区域处理来自邻近服务器的协作存储请求。

Abstract: Edge Storage Systems have emerged as a critical enabler of low latency data
access in modern cloud networks by bringing storage and computation closer to
end users. However, the limited storage capacity of edge servers poses
significant challenges in handling high volume and latency sensitive data
access requests, particularly under dynamic workloads. In this work, we propose
a profit driven framework that integrates three key mechanisms which are
collaborative caching, erasure coding, and elastic storage partitioning. Unlike
traditional replication, erasure coding enables space efficient redundancy,
allowing data to be reconstructed from any subset of K out of K plus M coded
blocks. We dynamically partition each edge server s storage into private and
public regions. The private region is further subdivided among access points
based on their incoming request rates, enabling adaptive control over data
locality and ownership. We design a data placement and replacement policy that
determines how and where to store or evict coded data blocks to maximize data
access within deadlines. While the private region serves requests from local
APs, the public region handles cooperative storage requests from neighboring
servers. Our proposed Dynamic Space Partitioning and Elastic caching strategy
is evaluated on both synthetic and real world traces from Netflix and Spotify.
Experimental results show that our method improves overall system profitability
by approximately 5 to 8% compared to state of the art approaches under varied
workload conditions.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [169] [Protected quantum gates using qubit doublons in dynamical optical lattices](https://arxiv.org/abs/2507.22112)
*Yann Kiefer,Zijie Zhu,Lars Fischer,Samuel Jele,Marius Gächter,Giacomo Bisson,Konrad Viebahn,Tilman Esslinger*

Main category: quant-ph

TL;DR: 利用费米子原子和光学格子中的双原子格态，提出并实验验证了一种纯粹几何的两比特交换门，具有内禀鲁棒性，保真度高达99.91%，有望用于构建大规模容错量子计算机。


<details>
  <summary>Details</summary>
Motivation: 为了实现内禀鲁棒的量子操作，需要克服以往实验中将超冷碰撞视为动态精细调整过程的问题，揭示其潜在的量子几何和统计特性。

Method: 提出并实验演示了利用瞬时填充费米子原子在动力学光晶格中的双原子格态，实现纯粹几何两比特交换门。利用双原子格态和费米子交换反对称性，实现无动力学相位、具有量子卷积的几何演化。

Result: 实验验证了该几何两比特交换门具有出色的保护能力，实现了超过17,000个原子对的系统，损耗校正幅度保真度达到了99.91(7)%。

Conclusion: 本研究提出的纯粹几何两比特交换门，利用了双原子格态和费米子交换反对称性，实现了量子卷积，并且该门具有内禀的鲁棒性，能够抵抗势能的涨落和不均匀性。结合原子输运方法，有望实现大规模、高连通性的量子处理器。本工作将量子统计和基本对称性转化为容错计算的资源，开创了量子逻辑的新范式。

Abstract: Quantum computing represents a central challenge in modern science. Neutral
atoms in optical lattices have emerged as a leading computing platform, with
collisional gates offering a stable mechanism for quantum logic. However,
previous experiments have treated ultracold collisions as a dynamically
fine-tuned process, which obscures the underlying quantum- geometry and
statistics crucial for realising intrinsically robust operations. Here, we
propose and experimentally demonstrate a purely geometric two-qubit swap gate
by transiently populating qubit doublon states of fermionic atoms in a
dynamical optical lattice. The presence of these doublon states, together with
fermionic exchange anti-symmetry, enables a two-particle quantum holonomy -- a
geometric evolution where dynamical phases are absent. This yields a gate
mechanism that is intrinsically protected against fluctuations and
inhomogeneities of the confining potentials. The resilience of the gate is
further reinforced by time-reversal and chiral symmetries of the Hamiltonian.
We experimentally validate this exceptional protection, achieving a
loss-corrected amplitude fidelity of $99.91(7)\%$ measured across the entire
system consisting of more than $17'000$ atom pairs. When combined with recently
developed topological pumping methods for atom transport, our results pave the
way for large-scale, highly connected quantum processors. This work introduces
a new paradigm for quantum logic, transforming fundamental symmetries and
quantum statistics into a powerful resource for fault-tolerant computation.

</details>


### [170] [A comprehensive benchmark of an Ising machine on the Max-Cut problem](https://arxiv.org/abs/2507.22117)
*Salwa Shaglel,Markus Kirsch,Marten Winkler,Christian Münch,Stefan Walter,Fritz Schinkel,Martin Kliesch*

Main category: quant-ph

TL;DR: 富士通的数字退火器在Max-Cut问题上与领先的启发式算法相比具有竞争力，证明了当前启发式方法在解决大规模QUBO问题上的能力。


<details>
  <summary>Details</summary>
Motivation: QUBO（二次无约束二元优化）问题的量子启发式求解方法的研究，以及在当前量子计算大规模应用仍不可及的情况下，对QUBO公式进行大规模数值测试的可能性。

Method: 通过在具有多达53,000个变量的图上，重点关注实际运行时间，对富士通的数字退火器（DA）进行基准测试，并与D-Wave的混合量子经典退火器和QIS3启发式算法的已公布性能结果进行比较。

Result: DA在Max-Cut问题上产生了有竞争力的结果，并且在2000多个MQLib图的性能统计数据中表现出跨不同求解器的一致性。

Conclusion: 富士通的数字退火器（DA）在Max-Cut问题上表现出与其他领先的启发式算法相当的性能，并且在具有多达53,000个变量的图上提供了具有竞争力的结果。

Abstract: QUBO formulations of combinatorial optimization problems allow for solving
them using various quantum heuristics. While large-scale quantum computations
are currently still out of reach, we can already numerically test such QUBO
formulations on a perhaps surprisingly large scale. In this work, we benchmark
Fujitsu's Digital Annealer (DA) on the Max-Cut problem, which captures the main
complexity of the QUBO problem. We make a comprehensive benchmark against
leading other heuristic algorithms on graphs with up to 53,000 variables by
focusing on the wall-clock time. Moreover, we compare the DA performance
against published performance results of the D-Wave hybrid quantum-classical
annealer and the recently proposed QIS3 heuristic. Based on performance
statistics for over 2,000 graphs from the MQLib, we find that the DA yields
competitive results. We hope that this benchmark demonstrates the extent to
which large QUBO instances can be heuristically solved today, yielding
consistent results across different solvers.

</details>


### [171] [Quantum complexity phase transition in fermionic quantum circuits](https://arxiv.org/abs/2507.22125)
*Wei Xia,Yijia Zhou,Xingze Qiu,Xiaopeng Li*

Main category: quant-ph

TL;DR: 本研究在量子渗流模型中发现了克洛夫复杂性相变（KCPT），并揭示了其与经典渗流转变在相互作用系统中的分离。研究还提出了一个实验上可行的测量方案。


<details>
  <summary>Details</summary>
Motivation: 了解量子多体系统的复杂性对于表征超出量子纠缠范围的复杂量子相至关重要，因此受到了广泛关注。

Method: 本研究开发了一个通用的标度理论来研究量子渗流模型（QPM）上的克洛夫复杂性相变（KCPT），并获得了临界概率和指数的精确结果。对于非相互作用系统，我们发现 KCPT 与经典渗流转变相吻合。对于相互作用系统，我们发现 KCPT 与渗流转变存在普遍分离，这类似于临界无序相变中的格里菲斯效应。此外，我们还提出了一种可用于当前实验测量的 KC 测量方案。

Result: 对于非相互作用系统，本研究发现 KCPT 与经典渗流转变相吻合。对于相互作用系统，研究发现 KCPT 与渗流转变存在普遍分离，这类似于临界无序相变中的格里菲斯效应。

Conclusion: 本研究在量子渗流模型（QPM）中研究了克洛夫复杂性（KC），并建立了 KC 复杂性与 QPM 中跨越簇数量的相互作用所产生的非常规相变。我们开发了一个适用于 QPM 上 KC 相变（KCPT）的通用标度理论，并获得了临界概率和指数的精确结果。对于非相互作用系统，在各种晶格（一维/二维/三维正则、贝特和拟晶体）上，我们的标度理论表明 KCPT 与经典渗流转变相吻合。相比之下，对于相互作用系统，我们发现 KCPT 由于高度复杂的量子多体效应，与渗流转变产生了普遍分离，这类似于临界无序相变中的格里菲斯效应。

Abstract: Understanding the complexity of quantum many-body systems has been attracting
much attention recently for its fundamental importance in characterizing
complex quantum phases beyond the scope of quantum entanglement. Here, we
investigate Krylov complexity in quantum percolation models (QPM) and establish
unconventional phase transitions emergent from the interplay of exponential
scaling of the Krylov complexity and the number of spanning clusters in QPM. We
develop a general scaling theory for Krylov complexity phase transitions (KCPT)
on QPM, and obtain exact results for the critical probabilities and exponents.
For non-interacting systems across diverse lattices (1D/2D/3D regular, Bethe,
and quasicrystals), our scaling theory reveals that the KCPT coincides with the
classical percolation transition. In contrast, for interacting systems, we find
the KCPT develops a generic separation from the percolation transition due to
the highly complex quantum many-body effects, which is analogous to the
Griffiths effect in the critical disorder phase transition. To test our
theoretical predictions, we provide a concrete protocol for measuring the
Krylov complexity, which is accessible to present experiments.

</details>


### [172] [Physical Emulation of Nonlinear Spin System Hamiltonians via Closed Loop Feedforward Control of a Collective Atomic Spin](https://arxiv.org/abs/2507.22132)
*Ian Pannemarsh*

Main category: quant-ph

TL;DR: This paper presents a method using cold atoms and feedback control to simulate quantum spin systems like the LMG and Kicked Top models. It successfully shows phase transitions and explores chaos and time crystals, offering an interim solution for quantum simulation research while full quantum computers are developed.


<details>
  <summary>Details</summary>
Motivation: The motivation stems from the significant progress but persistent challenges in developing fully digital, scalable, and fault-tolerant quantum computers. In the interim, direct emulation of quantum systems using existing technologies can address fundamental questions in many-body physics and the quantum-to-classical transition, and potentially offer alternative methods for verifying quantum simulation results.

Method: The study utilizes closed-loop control of the collective magnetic moment of an ensemble of cold neutral atoms, employing non-destructive measurements to emulate spin system Hamiltonians. By adjusting the feedback control law, the system can generate nonlinear dynamical behavior. The number of atoms in the collective spin can be controlled to investigate dynamics across the quantum-to-classical transition.

Result: The paper successfully emulates the Lipkin-Meshkov-Glick (LMG) Hamiltonian, demonstrating a symmetry-breaking phase transition in the expected parameter regime. It also explores chaos formation and a dynamically driven time crystal phase for the Kicked Top model. The study discusses the advantages and limitations of the proposed emulation method.

Conclusion: The paper demonstrates a method using closed-loop control of the collective magnetic moment of cold neutral atoms via non-destructive measurements to emulate various spin system Hamiltonians, including the Lipkin-Meshkov-Glick (LMG) Hamiltonian and the Kicked Top model. The method allows for the generation of nonlinear dynamical behavior and the exploration of mesoscopic spin systems, with potential for investigating the quantum-to-classical transition. The paper shows that the system exhibits a symmetry-breaking phase transition for the LMG model and explores chaos formation and a time crystal phase for the Kicked Top model, while also discussing the advantages and limitations of the approach.

Abstract: In recent decades the field of quantum computation has seen remarkable
development. While much progress has been made toward the realization of a
fully digital, scalable, and fault tolerant quantum computer, there are still
many essential challenges to overcome. In the interim, direct emulation of
quantum systems of interest can fill an important gap not only for exploring
fundamental questions about many-body physics and the quantum to classical
transition, but also for potentially providing alternative methods to verify
results from quantum simulations. In this work we will demonstrate a method
utilizing closed loop control of the collective magnetic moment of an ensemble
of cold neutral atoms via non-destructive measurements to emulate various spin
system Hamiltonians. By modifying the feedback control law appropriately we are
able to generate nonlinear dynamical behavior in the ensemble, allowing us to
explore the physics of collective spin systems at mesoscopic scales. Moreover,
controlling the number of atoms in the collective spin can potentially allow us
to investigate these dynamics in the transition from fully quantum to the
classical limit. In particular, we emulate two models: the Lipkin-Meshkov-Glick
(LMG) Hamiltonian, and a closely related model, the Kicked Top. In the former
case, we show that our system undergoes a symmetry-breaking phase transition in
the expected parameter regime. In the latter, we explore two interesting
aspects: the formation of chaos, and a dynamically driven time crystal phase.
We will then discuss the advantages and limits of this approach.

</details>


### [173] [Evaluation of Noise and Crosstalk in Neutral Atom Quantum Computers](https://arxiv.org/abs/2507.22140)
*Pranet Sharma,Yizhuo Tan,Konstantinos-Nikolaos Papadopoulos,Jakub Szefer*

Main category: quant-ph

TL;DR: 本研究评估了中性原子量子计算机中的噪声和串扰问题，提出了一种移动目标防御（MTD）策略来缓解因模拟共址导致的串扰，并证实该策略是可行的。


<details>
  <summary>Details</summary>
Motivation: 为了解决噪声和串扰限制了在共址或多租户环境中运行多个模拟的问题，本研究进行了分析和评估。

Method: 本研究分析了噪声随时间的变化，并研究了空间共址对模拟保真度的影响，同时提出并评估了移动目标防御（MTD）策略。

Result: 研究结果表明，并发模拟的近距离会增加它们之间的串扰。

Conclusion: 本研究提出的移动目标防御（MTD）策略可有效缓解并发模拟间的串扰，证明了其在安全可靠地共址模拟方面的可行性。

Abstract: This work explores and evaluates noise and crosstalk in neutral atom quantum
computers. Neutral atom quantum computers are a promising platform for analog
Hamiltonian simulations, which rely on a sequence of time-dependent
Hamiltonians to model the dynamics of the larger system and are particularly
useful for problems in optimization, physics, and molecular dynamics. However,
the viability of running multiple simulations in a co-located or multi-tenant
environment is limited by noise and crosstalk. This work conducts an analysis
of how noise faced by simulations changes over time, and investigates the
effects of spatial co-location on simulation fidelity. Findings of this work
demonstrate that the close proximity of concurrent simulations can increase
crosstalk between them. To mitigate this issue, a Moving Target Defense (MTD)
strategy is proposed and evaluated. The results confirm that the MTD is a
viable technique for enabling safe and reliable co-location of simulations on
neutral atom quantum hardware.

</details>


### [174] [Interference between lossy quantum evolutions activates information backflow](https://arxiv.org/abs/2507.22150)
*Sutapa Saha,Ujjwal Sen*

Main category: quant-ph

TL;DR: 量子演化中的信息回溯现象表明了非马尔可夫行为。即使在没有信息回溯的情况下，通过干涉两种非马尔可夫演化，也可以实现信息回流，并且相干控制量子操作轨迹比量子开关更有效、更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 量子演化通常被认为是不可酉的且有损的，但这种损耗并非一直存在，信息可能在演化过程中从环境中回流到系统中（信息回溯）。信息回溯是开放量子动力学非马尔可夫行为的一个充分指标，但并非所有非马尔可夫动力学都表现出信息回溯。本研究旨在探讨在没有信息回溯的情况下，如何实现信息从环境的回流。

Method: 研究允许在两种各自产生非马尔可夫性但不存在信息回溯的量子演化之间产生干涉。

Result: 发现当允许两种各自产生非马尔可夫性但不存在信息回溯的量子演化之间产生干涉时，可以从环境中检索信息。此外，与量子开关的另一种相干控制安排相比，涉及相干控制量子操作轨迹的设置提供了增强的性能和更高的鲁棒性。

Conclusion: 通过相干控制量子操作轨迹，可以在不出现信息回溯的情况下，从环境中检索信息。该方法比量子开关的相干控制配置具有更优越的性能和更强的鲁棒性。

Abstract: Quantum evolutions are often non-unitary and in such cases, they are
frequently regarded as lossy. Such lossiness, however, does not necessarily
persist throughout the evolution, and there can often be intermediate
time-spans during which information ebbs in the environment to re-flood the
system -- an event known as information backflow. This phenomenon serves as a
well-established and sufficient indicator of non-Markovian behavior of open
quantum dynamics. Nevertheless, not all non-Markovian dynamics exhibit such
backflow. We find that when interference is allowed between two quantum
evolutions that individually generate non-Markovianity and yet do not exhibit
information backflow, it becomes possible to retrieve information from the
environment. Furthermore, we show that this setup involving
coherently-controlled quantum operation trajectories provides enhanced
performance and is more robust compared to an alternate coherently-controlled
arrangement of the quantum switch.

</details>


### [175] [Hamiltonian Expressibility for Ansatz Selection in Variational Quantum Algorithms](https://arxiv.org/abs/2507.22550)
*Filippo Brozzi,Gloria Turati,Maurizio Ferrari Dacrema,Filippo Caruso,Paolo Cremonesi*

Main category: quant-ph

TL;DR: 哈密顿量可表性会影响VQA的性能。高可表性电路在某些问题上表现更好，而低可表性电路在其他问题上表现更好，尤其是在有噪声的情况下。选择合适的电路取决于问题的具体性质。


<details>
  <summary>Details</summary>
Motivation: 评估哈密顿量可表性对变分量子算法（VQA）中解的质量的影响，并研究其与电路结构（如深度）和问题类型（对角/非对角哈密顿量、基态/叠加态解）的关系。

Method: 使用基于蒙特卡洛的方法估计电路的哈密顿量可表性，分析了电路深度对可表性的影响，并确定了不同问题类型中最具和最缺乏表现力的电路。随后，使用变分量子特征求解器（VQE）训练每个电路，并分析了解决方案质量与可表性之间的相关性。

Result: 估计了特定电路集应用于各种哈密顿量时的哈密顿量可表性。分析了电路深度对可表性的影响，并识别了不同问题类型的最和最不具表现力的电路。最后，训练了VQE电路，并分析了解决方案质量与可表性之间的相关性。

Conclusion: 对于非对角哈密顿量和叠加态解的问题，高哈密顿量可表性的电路在理想或低噪声条件下表现更好，特别是在小规模问题上。对于以基态为解的问题（包括对角哈密顿量定义的问题），低哈密顿量可表性的电路更有效。在有噪声条件下，低可表性电路对于基态问题仍然是优选，而中等可表性对于某些涉及叠加态解的问题效果更好。

Abstract: In the context of Variational Quantum Algorithms (VQAs), selecting an
appropriate ansatz is crucial for efficient problem-solving. Hamiltonian
expressibility has been introduced as a metric to quantify a circuit's ability
to uniformly explore the energy landscape associated with a Hamiltonian ground
state search problem. However, its influence on solution quality remains
largely unexplored. In this work, we estimate the Hamiltonian expressibility of
a well-defined set of circuits applied to various Hamiltonians using a Monte
Carlo-based approach. We analyze how ansatz depth influences expressibility and
identify the most and least expressive circuits across different problem types.
We then train each ansatz using the Variational Quantum Eigensolver (VQE) and
analyze the correlation between solution quality and expressibility.Our results
indicate that, under ideal or low-noise conditions and particularly for
small-scale problems, ans\"atze with high Hamiltonian expressibility yield
better performance for problems with non-diagonal Hamiltonians and
superposition-state solutions. Conversely, circuits with low expressibility are
more effective for problems whose solutions are basis states, including those
defined by diagonal Hamiltonians. Under noisy conditions, low-expressibility
circuits remain preferable for basis-state problems, while intermediate
expressibility yields better results for some problems involving
superposition-state solutions.

</details>


### [176] [Efficient detection of localization transitions using predictability](https://arxiv.org/abs/2507.22151)
*Tiago Pernambuco,Jonas Maziero,Rafael Chaves*

Main category: quant-ph

TL;DR: 量子相干性可以用来探测局域化转变。可预测性是比相干性和纠缠更有效的探测局域化转变的标记，因为它所需的测量次数更少。


<details>
  <summary>Details</summary>
Motivation: 量子干涉效应驱动的相变（如安德森局域化和多体局域化）的识别是一个基本挑战。量子相干性为检测局域化转变提供了一种有效的方法。

Method: 通过研究连接局部可预测性、局部相干性和二分纯态中纠缠的互补关系来研究局域化转变。

Result: 可预测性是局域化转变的鲁棒且高效的标记。

Conclusion: 可预测性是探测量子相位变的有力工具，比相干性和纠缠所需的测量次数更少。

Abstract: Identifying phase transition points is a fundamental challenge in condensed
matter physics, particularly for transitions driven by quantum interference
effects, such as Anderson and many-body localization. Recent studies have
demonstrated that quantum coherence provides an effective means of detecting
localization transitions, offering a practical alternative to full quantum
state tomography and related approaches. Building on this idea, we investigate
localization transitions through complementarity relations that connect local
predictability, local coherence, and entanglement in bipartite pure states. Our
results show that predictability serves as a robust and efficient marker for
localization transitions. Crucially, its experimental determination requires
exponentially fewer measurements than coherence or entanglement, making it a
powerful tool for probing quantum phase transitions.

</details>


### [177] [Fragmented exceptional points and their bulk and edge realizations in lattice models](https://arxiv.org/abs/2507.22158)
*Subhajyoti Bid,Henning Schomerus*

Main category: quant-ph

TL;DR: This paper studies fragmented exceptional points (FEPs) in non-Hermitian systems, which are a type of complex non-Hermitian degeneracy where eigenvectors are only partially degenerate. The authors demonstrate how to induce FEPs in lattice models and provide an algebraic approach to design systems with FEPs, opening new possibilities for non-Hermitian physics and systems with unconventional response characteristics.


<details>
  <summary>Details</summary>
Motivation: Exceptional points (EPs) are spectral defects displayed by non-Hermitian systems in which multiple degenerate eigenvalues share a single eigenvector. This distinctive feature makes systems exhibiting EPs more sensitive to external perturbations than their Hermitian counterparts, where degeneracies are nondefective diabolic points. In contrast to these widely studied cases, more complex non-Hermitian degeneracies in which the eigenvectors are only partially degenerate are poorly understood.

Method: The design of the systems is facilitated by an efficient algebraic approach within which we provide precise conditions for FEPs that can be evaluated directly from a given model Hamiltonian.

Result: We characterize these fragmented exceptional points (FEPs) systematically from a physical perspective, and demonstrate how they can be induced into the bulk and edge spectrum of two-dimensional and three-dimensional lattice models, exemplified by non-Hermitian versions of a Lieb lattice and a higher-order topological Dirac semimetal.

Conclusion: Exceptional points (EPs) are spectral defects displayed by non-Hermitian systems in which multiple degenerate eigenvalues share a single eigenvector. This distinctive feature makes systems exhibiting EPs more sensitive to external perturbations than their Hermitian counterparts, where degeneracies are nondefective diabolic points. In contrast to these widely studied cases, more complex non-Hermitian degeneracies in which the eigenvectors are only partially degenerate are poorly understood. Here, we characterize these fragmented exceptional points (FEPs) systematically from a physical perspective, and demonstrate how they can be induced into the bulk and edge spectrum of two-dimensional and three-dimensional lattice models, exemplified by non-Hermitian versions of a Lieb lattice and a higher-order topological Dirac semimetal.

Abstract: Exceptional points (EPs) are spectral defects displayed by non-Hermitian
systems in which multiple degenerate eigenvalues share a single eigenvector.
This distinctive feature makes systems exhibiting EPs more sensitive to
external perturbations than their Hermitian counterparts, where degeneracies
are nondefective diabolic points. In contrast to these widely studied cases,
more complex non-Hermitian degeneracies in which the eigenvectors are only
partially degenerate are poorly understood. Here, we characterize these
fragmented exceptional points (FEPs) systematically from a physical
perspective, and demonstrate how they can be induced into the bulk and edge
spectrum of two-dimensional and three-dimensional lattice models, exemplified
by non-Hermitian versions of a Lieb lattice and a higher-order topological
Dirac semimetal. The design of the systems is facilitated by an efficient
algebraic approach within which we provide precise conditions for FEPs that can
be evaluated directly from a given model Hamiltonian. The free design of FEPs
significantly opens up a new frontier for non-Hermitian physics and expands the
scope for designing systems with unconventional response characteristics.

</details>


### [178] [Quantum optical experiments towards atom-photon entanglement](https://arxiv.org/abs/2507.22166)
*Markus Weber*

Main category: quant-ph

TL;DR: 本研究通过实验实现了原子-光子纠缠，纠缠保真度为0.82，为实现无漏洞的贝尔实验提供了基础。


<details>
  <summary>Details</summary>
Motivation: EPR佯谬和贝尔不等式指出了量子力学与局域实在论的矛盾，但以往的实验未能完全关闭漏洞。本研究旨在通过实现原子-光子纠缠，并利用其空间分离特性，实现首个关闭探测和局域漏洞的贝尔实验。

Method: 通过实验实现了单个R87原子的自发辐射光子的纠缠，利用了原子的两个衰变通道的相干性，实现了原子自旋状态和光子偏振状态的纠缠。

Result: 实验生成的原子-光子态的纠缠保真度为0.82。

Conclusion: 通过实验实现了单个原子的自发辐射光子的纠缠，并且验证了纠缠的保真度为0.82，为关闭探测和局域漏洞的贝尔实验奠定了基础。

Abstract: In 1935 EPR used the assumption of local realism to conclude in a
Gedankenexperiment with two entangled particles that quantum mechanics is not
complete. Based on this idea Bell constructed an inequality whereby
experimental tests could distinguish between quantum mechanics and
local-realistic theories. Many experiments have since been done that are
consistent with quantum mechanics, disproving the concept of local realism. But
all these tests suffered from loopholes allowing a local-realistic explanation
of the experimental observations. In this context, of special interest is
entanglement between different quantum objects like atoms and photons, because
it allows one to entangle distant atoms by the interference of photons. The
resulting space-like separation together with the almost perfect detection
efficiency of the atoms will allow a first event-ready Bell test closing
detection and locality loopholes.
  The primary goal of the present thesis was the experimental realization of
entanglement between a single localized atom and a single spontaneously emitted
photon. In the experiment a single optically trapped R87 atom is excited to a
state which has two selected decay channels. In the following spontaneous decay
a photon is emitted coherently with equal probability into both decay channels.
This accounts for perfect correlations between the polarization state of the
emitted photon and the Zeeman state of the atom after spontaneous decay.
Because these decay channels are spectrally and in all other degrees of freedom
indistinguishable, the spin state of the atom is entangled with the
polarization state of the photon. To verify entanglement, appropriate
correlation measurements in complementary bases of the photon polarization and
the internal quantum state of the atom were performed. It is shown, that the
generated atom-photon state yields an entanglement fidelity of 0.82.

</details>


### [179] [Complexity in multiqubit and many-body systems](https://arxiv.org/abs/2507.22246)
*Imre Varga*

Main category: quant-ph

TL;DR: 该论文提出了一种基于熵差异的复杂度参数，可以有效识别量子系统在不同动力学行为（如量子混沌、可积性、局域化）之间的交叉状态，并分析了去极化、退相对 n-qubit 系统复杂性的影响。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机在于找到一个能够衡量和识别量子系统复杂性的参数，特别是用于区分不同动力学行为（如可积性、量子混沌、局域化和遍历行为）之间的交叉状态，以及理解去极化和退相如何影响量子系统的复杂性。

Method: 该研究通过分析量子比特系统（n-qubit system）的复杂性来评估去极化（depolarization）和退相（dephasing）的影响。对于去极化，量子系统以概率 p 被替换为完全可分离的经典态；对于退相，量子系统以概率 p 破坏密度矩阵的特定非对角元素。此外，研究还分析了使用变形随机矩阵系综（deformed random matrix ensembles）、变形双体随机相互作用系综（deformed two-body random interaction ensembles）以及受随机局部磁场影响的自旋一维海森堡模型（Heisenberg-model of spins）等模型描述的多体系统，这些系统表现出多体局域化跃迁（many body localization transition）。最后，通过分析多体系统激发态的生存概率（survival probability），研究探讨了量子混沌态（quantum chaotic states）热化性质（thermalization properties）的交叉点识别问题。

Result: 对于 n-qubit 系统，研究发现最大复杂度的状态标志着最量子和最经典极限之间的边界。对于多体系统，最大复杂度的状态标志着可积性与完全量子混沌之间或多体局域化跃迁点的交叉点。该研究提出的复杂度参数（基于冯·诺依曼熵和二阶 Rényi 熵）被证明是识别系统处于交叉状态的有效指标。

Conclusion: 该研究表明，基于冯·诺依曼熵和二阶 Rényi 熵组合定义的复杂度参数，能够有效识别系统在可积性/局域化与量子混沌/遍历行为这两种极端情况之间的交叉状态。

Abstract: The complexity of $n$-qubit and many body systems is investigated. In case of
an $n$-qubit system the disturbance due to depolarization and dephasing is
identified based on a certain complexity quantity defined as the difference of
the Shannon-entropy and the R\'enyi entropy of order two. In case of the effect
of the depolarization the quantum system is replaced by a fully separable, i.e.
classical state with probability $p$ while it remains unchanged with
probability $1-p$. Whereas dephasing is modelled by destructing the appropriate
off-diagonal elements of the density matrix also with probability $p$. For both
cases the state with maximal complexity marks the border between the most
quantum and most classical limits. Furthermore we also show that many body
systems modelled using deformed random matrix ensembles, deformed two-body
random interaction ensembles and also the system of one-dimensional
Heisenberg-model of spins subject to a random, local magnetic field exhibiting
many body localization transition, the states with maximal complexity mark the
cross-over or the transition point between integrability and full quantum
chaos. Finally we address the question of identifying the cross-over in the
thermalization properties within large sets of quantum chaotic states using the
survival probability of an excitation of a many body system. All these results
show that the complexity parameter defined on a combination of the von Neumann
entropy and the R\'enyi entropy of 2nd order is a meaningful and informative
parameter to detect whenever a system is in a cross-over state between the
otherwise trivial extremal cases of integrability or localization and quantum
chaos or ergodic behavior.

</details>


### [180] [Measurement-induced cubic phase state generation](https://arxiv.org/abs/2507.22253)
*Harsh Kashyap,Denis A. Kopylov,Polina R. Sharapova*

Main category: quant-ph

TL;DR: 提出了一种通过干涉仪和探测操作高保真度生成量子计算所需的三次相位叠加态的协议。


<details>
  <summary>Details</summary>
Motivation: 三次相位叠加态是通用量子计算协议的关键非线性资源，但其实际构建面临诸多挑战。

Method: 该协议基于干涉仪方案，并辅以探测操作。为了找到同时实现高保真度和高探测率的参数集，我们进行了数值多参数优化。

Result: 我们提出了一种高保真度生成三次相位叠加态的协议，并通过数值多参数优化了参数，研究了参数不完美对保真度的影响。

Conclusion: 基于干涉仪方案并辅以探测操作，我们提出了一种高保真度生成三次相位叠加态的协议。

Abstract: The cubic phase state constitutes a nonlinear resource that is essential for
universal quantum computing protocols. However, constructing such non-classical
states faces many challenges. In this work, we present a protocol for
generating a cubic phase state with high fidelity. The protocol is based on an
interferometer scheme assisted by a detection operation. To find the proper set
of parameters that results in both high fidelity and high detection
probability, we provide a numerical multiparameter optimization. We investigate
a broad range of target states and study how parameter imperfections influence
fidelity.

</details>


### [181] [Implementation of a quantum linear solver for the Vlasov-Ampere equation](https://arxiv.org/abs/2507.22257)
*Tomer Goldfriend,Or Samimi Golan,Amir Naveh*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We implement a quantum linear solver for the one-dimensional Vlasov-Ampere
equation, following the model presented in Novikau et. al. [I, Novikau, I. Y.
Dodin, and E. A.Startsev. J. Plasma Phys, 90, 805900401]. We design the
relevant block encoding operator with Qmod high-level language, and obtain
optimized quantum programs using Classiq synthesis tools. Compared to a rigid
baseline implementation, our approach yields a clear reduction in quantum
resource requirements.

</details>


### [182] [Quantum coherence and negative quasi probabilities in a contextual three-path interferometer](https://arxiv.org/abs/2507.22323)
*Holger F. Hofmann*

Main category: quant-ph

TL;DR: A new method using a five-stage interferometer and Kirkwood-Dirac distributions is proposed to classify pure states in quantum mechanics based on non-classical correlations.


<details>
  <summary>Details</summary>
Motivation: The increased complexity of quantum statistics in a three-dimensional Hilbert space makes it difficult to identify a representative set of observable properties to characterize specific non-classical phenomena in three-path interferometers.

Method: A characterization of pure states based on a five-stage interferometer is proposed.

Result: The study shows that non-classical correlations between different contexts, expressed by negative Kirkwood-Dirac distributions, can be used to classify pure states.

Conclusion: The orthogonality relations between states representing different measurement contexts can classify pure states within the three-dimensional Hilbert space based on non-classical correlations expressed by negative Kirkwood-Dirac distributions.

Abstract: Basic quantum effects are often illustrated using single particle
interferences in two-path interferometers. A wider range of non-classical
phenomena can be illustrated using three-path interferometers, but the
increased complexity of quantum statistics in a three-dimensional Hilbert space
makes it difficult to identify a representative set of observable properties
that could be used to characterize specific phenomena. Here, I propose a
characterization of pure states based on a five-stage interferometer recently
introduced to demonstrate the relation between different measurement contexts
(Optica Quantum 1, 63 (2023)). It is shown that the orthogonality relations
between the states representing the different measurement contexts can be used
to classify pure states within the three-dimensional Hilbert space according to
the non-classical correlations between different contexts expressed by negative
Kirkwood-Dirac distributions.

</details>


### [183] [Reducing Circuit Depth in Lindblad Simulation via Step-Size Extrapolation](https://arxiv.org/abs/2507.22341)
*Pegah Mohammadipour,Xiantao Li*

Main category: quant-ph

TL;DR: Richardson外插法可将开放量子系统（Lindblad方程）的量子模拟电路深度从多项式改进为几乎对数级，同时保持采样复杂度不变。


<details>
  <summary>Details</summary>
Motivation: 研究算法误差缓解，特别是针对量子模拟开放量子系统（如Lindblad方程模型）的Richardson风格外插法。

Method: 采用Richardson外插法，通过向后误差分析获得密度算子的步长展开，并包含显式系数界限，用于分析Richardson外插法，从而界定后处理中产生的确定性偏差和采样噪声方差。

Result: 对于生成器界限为l的Lindblad动力学，n=\\(Omega(\\log(1/\\varepsilon)))点外插器可以将精度为\\varepsilon所需的最小电路深度从多项式\\mathcal{O} (((lT)^{2}) /\\varepsilon) 改进到 প্রায়对数级\\mathcal{O} (((lT)^{2}) \\log l \\log^2(1/\\varepsilon))，在采样复杂度保持为\\mathcal{O}(1/\\varepsilon^2) 的同时，提供了关于1/\\varepsilon 的指数级改进。

Conclusion: 该研究将Richardson外插法应用于开放量子系统（如Lindblad方程模型）的算法误差缓解，为Hamiltonian模拟的此类结果扩展到Lindblad模拟。

Abstract: We study algorithmic error mitigation via Richardson-style extrapolation for
quantum simulations of open quantum systems modelled by the Lindblad equation.
Focusing on two specific first-order quantum algorithms, we perform a
backward-error analysis to obtain a step-size expansion of the density operator
with explicit coefficient bounds. These bounds supply the necessary smoothness
for analyzing Richardson extrapolation, allowing us to bound both the
deterministic bias and the shot-noise variance that arise in post-processing.
For a Lindblad dynamics with generator bounded by $l$, our main theorem shows
that an $n=\Omega (\log(1/\varepsilon))$-point extrapolator reduces the maximum
circuit depth needed for accuracy $\varepsilon$ from polynomial $\mathcal{O}
((lT)^{2}/\varepsilon)$ to polylogarithmic $\mathcal{O} ((lT)^{2} \log l
\log^2(1/\varepsilon))$ scaling, an exponential improvement in~$1/\varepsilon$,
while keeping sampling complexity to the standard $1/\varepsilon^2$ level, thus
extending such results for Hamiltonian simulations to Lindblad simulations.
Several numerical experiments illustrate the practical viability of the method.

</details>


### [184] [Multipartite correlation measures and framework for multipartite quantum resources theory](https://arxiv.org/abs/2507.22348)
*Taotao Yan,Jinchuan Hou,Xiaofei Qi,Kan He*

Main category: quant-ph

TL;DR: 本研究提出了一个新的多方量子资源理论框架，以解决现有理论的缺陷。该框架强调了真正多方量子关联（MQC）度量的统一性和层级性条件。研究验证了多种量子关联是多方量子资源，并讨论了其对称性和单调关系。


<details>
  <summary>Details</summary>
Motivation: 现有的多方量子资源理论框架存在缺陷。因此，有必要建立一个更合理、更完善的框架来研究多方量子关联，并将其作为量子信息等领域的重要资源。

Method: 本文提出了一个新的多方量子资源理论框架，该框架着重于研究真正多方量子关联（MQC）的度量。这些度量不仅需要满足非负性、忠实性和自由操作下的非递增趋势等常规要求，还需要满足统一性和层级性条件。研究人员根据MQC的固有特性，探讨了不同MQC的度量所应满足的层级性条件，并提出了对称MQC的度量应满足对称性的要求。基于此框架，研究验证了多种量子关联的形式，并讨论了对称MQC的单调关系。

Result: 研究发现，真正多方量子关联的度量满足不同的层级性条件。同时，多方纠缠、$k$-纠缠、$k$-方纠缠、多方非PPT、多方相干性、多方虚数性、多方多模高斯非乘积关联、多方多模高斯虚数性以及多方单模高斯相干性均被确认为多方量子资源。此外，多方量子隐私放大被确定为一种非对称的多方量子资源。最后，文章对对称多方量子资源的单调关系进行了讨论。

Conclusion: 该研究提出了一个更合理的多方量子资源理论框架，并验证了多种量子关联（包括多方纠缠、$k$-纠缠、$k$-方纠缠、多方非PPT、多方相干性、多方虚数性、多方多模高斯非乘积关联、多方多模高斯虚数性以及多方单模高斯相干性）作为多方量子资源。此外，研究还表明多方量子隐私放大是一种非对称的多方量子资源，并讨论了对称多方量子资源的单调关系。

Abstract: In recent years, it has been recognized that properties of multipartite
physical systems, such as multipartite entanglement, can be considered as
important resources for quantum information and other areas of physics.
However, the current framework of multipartite quantum resource theory is
flawed. In this paper, we propose a more reasonable framework for multipartite
quantum resource theory, with a particular focus on investigating true
multipartite quantum correlation (MQC) measures. Beyond satisfying the
conventional requirements of non-negativity, faithfulness, and non-increasing
trend under free operations, these measures must also meet the unification and
hierarchy conditions. We find that the true measures of different MQCs exhibit
distinct hierarchy conditions based on their inherent characteristics.
Additionally, the true MQC measures should also satisfy the symmetry for
symmetric MQCs. Based on this framework, we verify that multipartite
entanglement, $k$-entanglement, $k$-partite entanglement, multipartite non-PPT,
multipartite coherence, multipartite imaginarity, multipartite multi-mode
Gaussian non-product correlation, multipartite multi-mode Gaussian imaginarity,
and multipartite single-mode Gaussian coherence are all multipartite quantum
resources. We also show that, multipartite steering is an asymmetric
multipartite quantum resource. Finally, the monogamy relations for true
measures of symmetric MQCs are discussed.

</details>


### [185] [Optimal quantum precision in noise estimation: Is entanglement necessary?](https://arxiv.org/abs/2507.22413)
*Shuva Mondal,Priya Ghosh,Ujjwal Sen*

Main category: quant-ph

TL;DR: 本文研究了量子信道噪声参数估计中的最优探针问题。发现最优探针的纠缠性与噪声强度和维度有关，并非总是纠缠态。在某些情况下，乘积态足以达到最优精度。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探究在估计一类称为向量编码的本地量子编码过程的噪声参数时，最优探针是否应为纠缠态，以及其纠缠的特性和量级。这类编码过程包括本地去极化和比特翻转信道等。

Method: 本文利用“连续交换”性质来分析最优探针的纠缠问题。具体方法包括：1. 证明向量编码过程具有“连续交换”性质。2. 利用此性质处理最优探针的纠缠查询。3. 对两方任意维度的本地去极化信道进行分析，研究其最优探针纠缠度随噪声强度的变化。4. 推广到多比特情况，分析最优探针的纠缠行为，并探讨其单调性与所用纠缠度量的关系。5. 确定在高噪声情况下最优探针的形式（全乘积多方量子态）。6. 分析两比特本地翻转信道，得出乘积态足以获得最优精度的结论。

Result: 对于两比特本地去极化信道，最优探针的纠缠度随去极化强度的增加而呈阶梯状下降。对于多比特情况，最优探针的纠缠行为可能是单调或不单调的，这取决于所使用的多方纠缠度量。当去极化噪声足够强时，全乘积多方量子态是唯一的最优探针选择。在许多情况下，即使在噪声强度适中的情况下，全乘积态也是最优的。对于两比特本地比特翻转信道，信道和最优探针的连续交换性意味着乘积态足以获得最优精度。

Conclusion: 对于本地量子编码过程的噪声参数估计，最优探针的纠缠性质取决于噪声强度和维度。在某些情况下，例如两比特本地去极化信道，最优探针的纠缠度随噪声强度增加而呈阶梯状下降。对于多比特情况，最优探针的纠缠行为可能单调或不单调，具体取决于所使用的多方纠缠度量。当噪声强度足够高时，全乘积多方量子态是最优探针的唯一选择；在许多情况下，即使在中等噪声强度下，全乘积态也已足够。

Abstract: We ask whether the optimal probe is entangled, and if so, what is its
character and amount, for estimating the noise parameter of a large class of
local quantum encoding processes that we refer to as vector encoding, examples
of which include the local depolarizing and bit-flip channels. We first
establish that vector encoding is invariably ``continuously commutative'' for
optimal probes. We utilize this result to deal with the queries about
entanglement in the optimal probe. We show that for estimating noise extent of
the two-party arbitrary-dimensional local depolarizing channel, there is a
descending staircase of optimal-probe entanglement for increasing depolarizing
strength. For the multi-qubit case, the analysis again leads to a staircase,
but which can now be monotonic or not, depending on the multiparty entanglement
measure used. We also find that when sufficiently high depolarizing noise is to
be estimated, fully product multiparty states are the only choice for being
optimal probes. In many cases, for even moderately high depolarizing noise,
fully product states are optimal. For two-qubit local bit-flip channels, the
continuous commutativity of the channel and optimal probe implies that a
product state suffices for obtaining the optimal precision.

</details>


### [186] [Towards Practical Quantum Phase Estimation: A Modular, Scalable, and Adaptive Approach](https://arxiv.org/abs/2507.22460)
*Alok Shukla,Prakash Vedula*

Main category: quant-ph

TL;DR: AWQPE是一种新的量子相位估计算法，通过使用小型量子比特块和歧义性解析机制，提高了资源效率和精度，适合NISQ设备。


<details>
  <summary>Details</summary>
Motivation: 针对标准量子相位估计算法（QPE）在相干量子比特数量和电路深度方面的资源需求，以及其对当前含噪声中等规模量子（NISQ）设备的挑战。

Method: AWQPE算法利用小的、独立的m > 1个控制量子比特块，在“窗口”内同时估计多个相位比特，并通过从最低有效位到最高有效位的歧义性解析机制来提高精度并减少错误传播。

Result: 数值模拟表明AWQPE算法具有准确性和鲁棒性，在资源效率和计算速度之间取得了显著的平衡。 (AWQPE算法的准确性和鲁棒性得到了数值模拟的验证，在资源效率和计算速度之间取得了显著的平衡)

Conclusion: AWQPE算法在精度和资源效率之间取得了良好的平衡，特别适合近期量子平台。

Abstract: Quantum Phase Estimation (QPE) is a cornerstone algorithm in quantum
computing, with applications ranging from integer factorization to quantum
chemistry simulations. However, the resource demands of standard QPE, which
require a large number of coherent qubits and deep circuits, pose significant
challenges for current Noisy Intermediate Scale Quantum (NISQ) devices. In this
work, we introduce the Adaptive Windowed Quantum Phase Estimation (AWQPE)
algorithm, a novel method designed to address the limitations of standard QPE.
AWQPE utilizes small, independent blocks of $m > 1$ control qubits to estimate
multiple phase bits simultaneously within a "window,'' thereby significantly
reducing the number of iterations required to achieve a desired precision.
These independent blocks are amenable to parallelization and, when combined
with a robust least-significant-bit (LSB) to most-significant-bit (MSB)
ambiguity resolution mechanism, enhance the algorithm's accuracy while
mitigating the risk of error propagation. Our numerical simulations demonstrate
AWQPE's accuracy and robustness, showcasing a distinct balance between resource
efficiency and computational speed. This makes AWQPE particularly well-suited
for near-term quantum platforms.

</details>


### [187] [Scalable and (quantum-accessible) adaptive pseudorandom quantum states and pseudorandom function-like quantum state generators](https://arxiv.org/abs/2507.22535)
*Rishabh Batra,Zhili Chen,Rahul Jain,YaoNan Zhang*

Main category: quant-ph

TL;DR: 本研究提出了一种新的量子态制备方法，实现了可扩展的伪随机状态（PRS）和伪随机函数类量子态（PRFS）生成器，该生成器仅需量子安全单向函数假设，并且不引入与环境的纠缠或关联。


<details>
  <summary>Details</summary>
Motivation: 研究动机是PRS和PRFS构造的可扩展性，即安全参数λ远大于输出态的量子比特数n，这在某些应用中可能很重要。

Method: 提出了一种等距程序来制备量子态，其与哈尔随机态的迹距离可以任意小，或者与伪随机态的区分优势可以任意小。该程序提供了一种可扩展的伪随机状态（PRS）新方法，该方法不引入与环境的纠缠或关联。

Result: 实现了一种可扩展的PRS新方法，不引入与环境的纠缠或关联。这是第一个基于量子安全单向函数的、可扩展且（量子可访问的）自适应PRFS的构造。该PRFS构造可用于多种加密原语。

Conclusion: 该研究提出了一个等距程序，用于制备任意随机的量子态，并在此基础上构建了一种可扩展的、（量子可访问的）自适应伪随机函数类量子态（PRFS）生成器，该生成器仅需量子安全单向函数作为假设。该PRFS构造可用于多种加密原语，如长输入PRFS、短输入PRFS、短输出PRFS、非自适应PRFS和经典可访问自适应PRFS。

Abstract: Pseudorandom quantum states (PRS) and pseudorandom function-like quantum
state (PRFS) generators are quantum analogues of pseudorandom generators and
pseudorandom functions. It is known that PRS (and PRFS) can exist even if BQP =
QMA (relative to a quantum oracle) or if P = NP (relative to a classical
oracle), which does not allow for the existence of one-way functions (relative
to these oracles). Hence, these are potentially weaker objects than
quantum-secure one-way functions, which can be used to do quantum cryptography.
A desirable property of PRS and PRFS constructions is scalability, which
ensures that the security parameter $\lambda$ (which determines
indistinguishability from their Haar-random counterparts) is much larger than
$n$ (the number of qubits of the output states). This may be important in some
applications where PRS and PRFS primitives are used.
  We present an isometric procedure to prepare quantum states that can be
arbitrarily random (i.e., the trace distance from the Haar-random state can be
arbitrarily small for the true random case, or the distinguishing advantage can
be arbitrarily small for the pseudorandom case). Our procedure provides a new
method for scalable PRS that introduces no entanglement or correlations with
the environment. This naturally gives the first construction for scalable and
(quantum-accessible) adaptive PRFS assuming quantum-secure one-way functions.
Our PRFS construction implies various primitives, including long-input PRFS,
short-input PRFS, short-output PRFS, non-adaptive PRFS, and
classical-accessible adaptive PRFS. This new construction may be helpful in
some simplification of the microcrypt zoo.

</details>


### [188] [Molecular spin qudits to test generalized Bell inequalities](https://arxiv.org/abs/2507.22768)
*S. Macedonio,L. Lepori,A. Chiesa,S. Chicco,L. Bersani,M. Rubin-Osanz,L. B. Woodcock,A. Mavromagoulos,G. Allodi,E. Garlatti,S. Piligkos,A. Smerzi,S. Carretta*

Main category: quant-ph

TL;DR: Yb(trensal)分子纳秒磁体是研究量子比特-量子衍体系统中纠缠的理想平台。


<details>
  <summary>Details</summary>
Motivation: 探索在量子比特-量子衍体系统中纠缠现象，并为研究量子衍体-量子衍体纠缠提供新的方案。

Method: 通过开发优化的脉冲序列来验证广义贝尔不等式，并进行包含实验测量退相干的数值模拟。

Result: 在较宽的参数范围内，广义贝尔不等式得到了清晰的违反，证明了该系统的纠缠具有鲁棒性。

Conclusion: Yb(trensal)分子纳秒磁体能够有效地探测量子比特-量子衍体系统中纠缠的现象，并且实验证明了该系统的鲁棒性。

Abstract: We show that Yb(trensal) molecular nanomagnet, embedding an electronic spin
qubit coupled to a nuclear spin qudit, provides an ideal platform to probe
entanglement in a qubit-qudit system.This is demonstrated by developing an
optimized pulse sequence to show violation of generalized Bell inequalities and
by performing realistic numerical simulations including experimentally measured
decoherence. We find that the inequalities are safely violated in a wide range
of parameters, proving the robustness of entanglement in the investigated
system. Furthermore, we propose a scheme to study qudit-qudit entanglement on a
molecular spin trimer, in which two spins 3/2 are linked via an interposed
switch to turn on and off their mutual interaction.

</details>


### [189] [Two-Dimensional Bialgebras and Quantum Groups: Algebraic Structures and Tensor Network Realizations](https://arxiv.org/abs/2507.22541)
*José Garre-Rubio,András Molnár,Germán Sierra*

Main category: quant-ph

TL;DR: 将一维霍夫代数和量子群推广到二维格子，并研究了其应用，特别是量子群 $U_q[su(2)]$ 和张量网络态。


<details>
  <summary>Details</summary>
Motivation: 将霍夫代数和量子群的代数理论从一维扩展到二维方形格子，以定义其上的余代数和双代数结构。

Method: 通过定义满足兼容性和结合性条件的水平和垂直映射来构造二维余积，从而在格子格点上实现向量空间的持续增长。

Result: 给出了二维双代数的几个例子，包括类群和受李代数启发的构造，以及一个适用于塔夫特霍夫代数和量子群的准一维余积实例。此外，还研究了量子群 $U_q[su(2)]$ 的二维推广，分析了 $q$ 变形的单态，并推导了一个在半经典极限下满足交织关系的二维 R 矩阵。最后，展示了张量网络态（特别是 PEPS）如何通过添加适当的边界条件自然地诱导出二维余代数结构。

Conclusion: 该研究为在二维格子系统中嵌入量子群对称性提供了一种局部且代数一致的方法，可能与融合二范畴和量子多体物理中的范畴对称性新兴理论相关。

Abstract: We introduce a framework to define coalgebra and bialgebra structures on
two-dimensional (2D) square lattices, extending the algebraic theory of Hopf
algebras and quantum groups beyond the one-dimensional (1D) setting. Our
construction is based on defining 2D coproducts through horizontal and vertical
maps that satisfy compatibility and associativity conditions, enabling the
consistent growth of vector spaces over lattice sites. We present several
examples of 2D bialgebras, including group-like and Lie algebra-inspired
constructions and a quasi-1D coproduct instance that is applicable to Taft-Hopf
algebras and to quantum groups. The approach is further applied to the quantum
group $U_q[su(2)]$, for which we construct 2D generalizations of its
generators, analyze $q$-deformed singlet states, and derive a 2D R-matrix
satisfying an intertwining relation in the semiclassical limit. Additionally,
we show how tensor network states, particularly PEPS, naturally induce 2D
coalgebra structures when supplemented with appropriate boundary conditions.
Our results establish a local and algebraically consistent method to embed
quantum group symmetries into higher-dimensional lattice systems, potentially
connecting to the emerging theory of fusion 2-categories and categorical
symmetries in quantum many-body physics.

</details>


### [190] [Quantum Krylov Subspace Diagonalization via Time Reversal Symmetries](https://arxiv.org/abs/2507.22559)
*Nicola Mariella,Enrique Rico,Adam Byrne,Sergiy Zhuk*

Main category: quant-ph

TL;DR: KTR 是一种新的量子算法，可以更有效地在量子计算机上执行 Krylov 量子对角化，即使是在硬件受限的情况下。


<details>
  <summary>Details</summary>
Motivation: 现有的 Krylov 量子对角化方法依赖于受控操作，这对近期的量子硬件构成了挑战。

Method: 提出了一种名为 Krylov Time Reversal (KTR) 的新颖方案，该方案利用哈密顿量演化的时间反转对称性来恢复实值 Krylov 矩阵元素，从而避免了对受控操作的依赖。

Result: 通过在横向场伊辛模型和晶格规范理论等具有时间反转对称性的哈密顿量上进行数值模拟，验证了 KTR 方法在精确光谱估计和有利电路构建方面的有效性。

Conclusion: Krylov Time Reversal (KTR) 通过利用哈密顿量演化的时间反转对称性，提供了一种避免现有 Krylov 量子对角化方法中受控操作瓶颈的新颖方案，从而减少了电路深度并提高了与浅层量子体系的兼容性。

Abstract: Krylov quantum diagonalization methods have emerged as a promising use case
for quantum computers. However, many existing implementations rely on
controlled operations, which pose challenges to near-term quantum hardware. We
introduce a novel protocol, which we call Krylov Time Reversal (KTR), which
avoids these bottlenecks by exploiting the time-reversal symmetry in
Hamiltonian evolution. Using symmetric time dynamics, we show that it is
possible to recover real-valued Krylov matrix elements, which significantly
reduces the circuit depth and enhances compatibility with shallow quantum
architectures. We validate our method through numerical simulations on
paradigmatic Hamiltonians exhibiting time-reversal symmetry, including the
transverse-field Ising model and a lattice gauge theory, demonstrating accurate
spectral estimation and favorable circuit constructions.

</details>


### [191] [Minimizing entanglement entropy for enhanced quantum state preparation](https://arxiv.org/abs/2507.22562)
*Oskari Kerppo,William Steadman,Ossi Niemimäki,Valtteri Lahtinen*

Main category: quant-ph

TL;DR: 提出一种两步量子状态制备方法，通过最小化纠缠熵简化状态，然后用矩阵乘积状态表示，以提高NISQ设备的制备精度和性能。


<details>
  <summary>Details</summary>
Motivation: 量子状态制备是将经典信息编码到量子态中的重要子程序，但任意状态的量子状态制备在两比特门数量上呈指数级增长，在NISQ设备上难以实现，这是实现量子优势的主要挑战。

Method: 提出并分析了一种新颖的两步状态制备方法：首先最小化目标量子态的纠缠熵，然后将简化后的状态表示为矩阵乘积状态。

Result: 该方法能够高精度地制备目标状态，并且在基准状态上展示了前沿的性能。

Conclusion: 该方法适用于NISQ设备，并通过严格的界限证明了制备状态的准确性，在基准状态上表现出先进的性能。

Abstract: Quantum state preparation is an important subroutine in many quantum
algorithms. The goal is to encode classical information directly to the quantum
state so that it is possible to leverage quantum algorithms for data
processing. However, quantum state preparation of arbitrary states scales
exponentially in the number of two-qubit gates, and this makes quantum state
preparation a very difficult task on quantum computers, especially on near-term
noisy devices. This represents a major challenge in achieving quantum
advantage. We present and analyze a novel two-step state preparation method
where we first minimize the entanglement entropy of the target quantum state,
thus transforming the state to one that is easier to prepare. The state with
reduced entanglement entropy is then represented as a matrix product state,
resulting in a high accuracy preparation of the target state. Our method is
suitable for NISQ devices and we give rigorous lower bounds on the accuracy of
the prepared state in terms of the entanglement entropy and demonstrate
cutting-edge performance across a collection of benchmark states.

</details>


### [192] [Non-Hermitian Quantum Many-Body Scar Phase](https://arxiv.org/abs/2507.22583)
*Keita Omiya,Yuya O Nakagawa*

Main category: quant-ph

TL;DR: 提出一种新的非平衡相——量子多体 the quantum many-body scar (QMBS) 相，它利用非厄米驱动来稳定高能量的 the quantum many-body scar (QMBS) 波函数，使其成为非平衡稳态，并与遍历热相实现一阶相变。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的非平衡相——量子多体 the quantum many-body scar (QMBS) 相，它在非厄米多体动力学中出现，通过非厄米驱动选择性地稳定 the quantum many-body scar (QMBS) 波函数。

Method: 通过分析论证和三个代表性模型的数值模拟（随机量子电路模型，$SU(q)$ 自旋模型和经典的自旋-1 XY 模型）来研究 QMBS 相及其与遍历热相的一阶相变。

Result: 在非厄米驱动下，即使在熵增长驱动热化的系统动力学中，高能量的 the quantum many-body scar (QMBS) 波函数也能被稳定，并成为非平衡稳态，与遍历热相存在一阶相变。

Conclusion: 非厄米多体动力学中的量子多体 the quantum many-body scar (QMBS) 相，其特征是在非厄米驱动下选择性地稳定 the quantum many-body scar (QMBS) 波函数。

Abstract: We introduce a novel non-equilibrium phase -- the quantum many-body scar
(QMBS) phase -- that emerges in non-Hermitian many-body dynamics when scarred
wavefunctions are selectively stabilized via non-Hermitian driving. Projective
measurements, or non-Hermitian counterparts, preferentially reinforce QMBS,
counteracting the entropy growth that drives thermalization. As a result,
atypical, high-energy scarred wavefunctions that are negligible in the
long-time dynamics of closed systems become non-equilibrium steady states. We
establish the existence of the QMBS phase and its sharp, first-order phase
transition from an ergodic thermal phase, through both analytical arguments and
numerical simulations of three representative models: a random quantum circuit
model, the $SU(q)$ spin model, and the paradigmatic spin-1 XY model.

</details>


### [193] [Lie groups for quantum complexity and barren plateau theory](https://arxiv.org/abs/2507.22590)
*P. A. S. de Alcântara,Gabriel Audi,Leandro Morais*

Main category: quant-ph

TL;DR: This paper uses Lie groups and algebras to analyze quantum computing, focusing on computational complexity and the barren plateau problem in VQAs.


<details>
  <summary>Details</summary>
Motivation: To provide a sophisticated mathematical framework for understanding quantum algorithms, specifically addressing quantum computational complexity and the barren plateau phenomenon in VQAs.

Method: The paper reviews the theory of Lie groups and their algebras to analyze quantum computational complexity using the geometric formulation of shortest path on the SU(2^n) manifold and the barren plateau phenomenon in VQAs using Dynamical Lie Algebra (DLA).

Result: The paper describes the geometric formulation of quantum computational complexity and uses DLA to identify algebraic sources of untrainability in VQAs.

Conclusion: The paper provides a mathematical framework using Lie groups and algebras to analyze quantum computing problems. It specifically applies this to quantum computational complexity and the barren plateau phenomenon in VQAs.

Abstract: Advances in quantum computing over the last two decades have required
sophisticated mathematical frameworks to deepen the understanding of quantum
algorithms. In this review, we introduce the theory of Lie groups and their
algebras to analyze two fundamental problems in quantum computing as done in
some recent works. Firstly, we describe the geometric formulation of quantum
computational complexity, given by the length of the shortest path on the
$SU(2^n)$ manifold with respect to a right-invariant Finsler metric. Secondly,
we deal with the barren plateau phenomenon in Variational Quantum Algorithms
(VQAs), where we use the Dynamical Lie Algebra (DLA) to identify algebraic
sources of untrainability

</details>


### [194] [Thresholded quantum LIDAR in turbolent media](https://arxiv.org/abs/2507.22622)
*Walter Zedda,Ilaria Gianani,Vincenzo Berardi,Marco Barbieri*

Main category: quant-ph

TL;DR: This paper improves LiDAR signal-to-noise ratio in low photon conditions, even with imperfect detectors and turbulent environments, by extending a previous technique.


<details>
  <summary>Details</summary>
Motivation: The paper addresses the challenge of improving the signal-to-noise ratio in Light Detection and Ranging (LiDAR) when dealing with low photon numbers, particularly when external light sources interfere with the detection process.

Method: This paper extends a previously developed technique to account for the effects of light propagation through turbulent media and detection by imperfect photon-counting devices (considering efficiency and number resolution).

Result: The results suggest that the extended technique is effective even with less sophisticated detection technology.

Conclusion: The study indicates that even less advanced detection technology can lead to a viable detection scheme.

Abstract: Light detection and ranging is a key technology for a number of applications,
from relatively simple distance ranging to environmental monitoring. When
dealing with low photon numbers an important issue is the improvement of the
signal- to-noise-ratio, which is severely affected by external sources whose
emission is captured by the detection apparatus. In this paper, we present an
extension of the technique developed in [Phys. Rev. Lett. 123, 203601] to the
effects caused by the propagation of light through a turbulent media, as well
as the detection through photon counting devices bearing imperfections in terms
of efficiency and number resolution. Our results indicate that even less
performing technology can result in a useful detection scheme.

</details>


### [195] [Parametric Amplification in Kerr Nonlinear Resonators: A theoretical review of Josephson Parametric Amplifiers](https://arxiv.org/abs/2507.22630)
*Rajlaxmi Bhoite,Shraddhanjali Choudhury*

Main category: quant-ph

TL;DR: This paper reviews Josephson Parametric Amplifiers (JPAs), explaining their amplification process, deriving key equations, and analyzing gain parameters through theoretical and numerical methods to improve understanding for future research in quantum-limited amplification.


<details>
  <summary>Details</summary>
Motivation: The motivation is to provide a detailed theoretical review of the amplification process in Josephson Parametric Amplifiers (JPAs), which are crucial for quantum-limited signal amplification in superconducting circuits.

Method: The paper derives key dynamical equations under intense pumping, obtains a nonlinear steady-state solution, and linearizes it to analyze the system's response to weak signals. It extracts expressions for parametric and intermodulation gain using the input-output formalism and numerically solves the equations to study gain dependence on frequency detuning and pump strength.

Result: The paper derives the dynamical equations for JPAs, obtains expressions for parametric gain and intermodulation gain, and explores the dependence of gain on frequency detuning and pump strength through numerical solutions, visualized with a gain response curve.

Conclusion: This paper enhances the understanding of JPAs and aims to inspire continued research in quantum-limited amplification.

Abstract: This paper presents a detailed theoretical review of the amplification
process in Josephson Parametric Amplifiers (JPAs), which are crucial for
quantum-limited signal amplification in superconducting circuits. The paper
begins by outlining the principles of parametric amplification, focusing on how
a strong classical pump interacts with the nonlinear Josephson medium in
reflection geometry. The key dynamical equations are derived under intense
pumping, leading to a nonlinear steady-state solution. Linearization around
this solution allows to analyze the system response to weak signals and extract
expressions for parametric gain and intermodulation gain using the input-output
formalism. Numerically, the equations are solved to explore how the gain
depends on frequency detuning and pump strength, which is visualized with a
gain response curve. By enhancing our understanding of JPAs, this work aims to
inspire continued research in the field of quantum-limited amplification.

</details>


### [196] [Single- and Two-Mode Squeezing by Modulated Coupling to a Rabi Driven Qubit](https://arxiv.org/abs/2507.22641)
*Eliya Blumenthal,Nir Gutman,Ido Kaminer,Shay Hacohen-Gourgy*

Main category: quant-ph

TL;DR: 该研究提出了一种新的量子计算方法，利用量子比特和谐振器之间的相互作用来产生压缩，有望在量子传感和计算领域取得进展。


<details>
  <summary>Details</summary>
Motivation: 为了实现通用控制、产生纠缠和在分布式模式中实现逻辑操作，先进的玻色子量子计算架构需要非局域高斯操作，例如双模压缩。

Method: 利用调制过的Jaynes-Cummings相互作用，通过Rabi驱动的量子比特色散耦合到谐振器来生成条件压缩。

Result: 模拟预测腔内压缩可达13dB（单模）、4dB（叠加单模）和12dB（双模），其中后两者有待实验验证。该研究为光子态的量子比特条件控制建立了新范式。

Conclusion: 提出了一种使用Rabi驱动的、色散耦合到一个或两个谐振器的量子比特来生成条件压缩的新方法，为连续变量量子信息处理提供了新工具，并为在易于获得的系统上进行量子传感和连续变量计算开辟了新途径。

Abstract: Advanced bosonic quantum computing architectures demand nonlocal Gaussian
operations such as two-mode squeezing to unlock universal control, enable
entanglement generation, and implement logical operations across distributed
modes. This work presents a novel method for generating conditional squeezing
using a Rabi-driven qubit dispersively coupled to one or two harmonic
oscillators. A proof that this enables universal control over bosonic modes is
provided, expanding the toolkit for continuous-variable quantum information
processing. Using modulated Jaynes-Cummings interactions in circuit QED, the
simulation predicts intra-cavity squeezing of 13dB (single-mode), 4dB
(superimposed single-mode), and 12dB (two-mode), with the latter two yet to be
demonstrated experimentally. These results establish a new paradigm for
qubit-conditioned control of photonic states, with applications to quantum
sensing and continuous-variable computation on readily available systems.

</details>


### [197] [On the Simulation of Conical Intersections in Water and Methanimine Molecules Via Variational Quantum Algorithms](https://arxiv.org/abs/2507.22670)
*Samir Belaloui,Nacer Eddine Belaloui,Achour Benslama*

Main category: quant-ph

TL;DR: 量子算法可用于计算分子的锥形交叉点，并与经典方法相媲美。


<details>
  <summary>Details</summary>
Motivation: 为了定位甲亚胺（CH2NH）和水（H2O）分子的锥形交叉点（CIs），研究了量子变分算法在电子结构计算中的应用。

Method: 使用量子变分算法（VQE、VQD、VQE-AC和SA）研究了甲亚胺（CH2NH）和水（H2O）分子的电子结构，旨在定位锥形交叉点（CIs）。通过与经典本征值分解进行比较来评估量子算法的准确性。研究了不同分子构型、基组和活性空间下的算法性能。

Result: 量子算法能够描述这两种分子中的锥形交叉点，并且在计算激发态方面与经典方法具有良好的一致性。

Conclusion: 量子变分算法（包括VQE、VQD、VQE-AC和SA方法）能够描述这两种分子中的锥形交叉点（CIs），但需要选择合适的活性空间和分子几何形状。基于VQE的方法在计算激发态方面与经典基准方法具有可比性，在某些区域内结果一致。

Abstract: We investigate the electronic structure of methanimine (CH2NH) and water
(H2O) molecules in an effort to locate conical intersections (CIs) using
variational quantum algorithms. Our approach implements and compares a range of
hybrid quantum-classical methods, including the Variational Quantum Eigensolver
(VQE), Variational Quantum Deflation (VQD), VQE with Automatically-Adjusted
Constraints (VQE-AC), and we explore molecular configurations of interest using
a State-Average (SA) approach. Exact Diagonalization is employed as the
classical benchmark to evaluate the accuracy of the quantum algorithms. We
perform simulations across a range of molecular geometries, basis sets, and
active spaces to compare each algorithm's performance and accuracy, and to
enhance the detectability of CIs. This work confirms the quantum variational
algorithms' capability of describing conical intersections in both molecules,
as long as appropriate active spaces and geometries of the molecule are chosen.
We also compare the accuracy and reliability of VQE-based methods for computing
excited states with classical benchmark methods, and we demonstrate good
agreement within desired regions.

</details>


### [198] [Tutorial: Optical quantum metrology](https://arxiv.org/abs/2507.22680)
*Marco Barbieri*

Main category: quant-ph

TL;DR: Quantum photonics sensing uses nonclassical light for better, less invasive measurements than classical methods.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore how quantum effects can improve information processing, specifically in the context of quantum technologies and their application in sensing.

Method: The tutorial examines the basic principles of parameter estimation and reviews the state of the art in quantum photonics sensing.

Result: The tutorial presents how the advantage of quantum photonics sensing is achieved through nonclassical light.

Conclusion: Quantum photonics sensing, leveraging nonclassical light, offers a more efficient trade-off between invasivity and measurement quality compared to classical methods.

Abstract: The purpose of quantum technologies is to explore how quantum effects can
improve on existing solutions for the treatment of information. Quantum
photonics sensing holds great promises for reaching a more efficient trade-off
between invasivity and quality of the measurement, when compared with the
potential of classical means. This tutorial is dedicated to presenting how this
advantage is brought about by nonclassical light, examining the basic
principles of parameter estimation and reviewing the state of the art.

</details>


### [199] [Hardware-Efficient Rydberg Atomic Quantum Solvers for NP Problems](https://arxiv.org/abs/2507.22686)
*Shuaifan Cao,Xiaopeng Li*

Main category: quant-ph

TL;DR: 本研究提出了一种用于里德堡原子量子计算平台的NP问题通用量子求解器，基于Grover搜索算法，具有二次量子加速。与量子退火相比，所需的量子比特数量与问题规模线性相关，并且具有更好的电路深度扩展性，为实现量子优势提供了可行路径。


<details>
  <summary>Details</summary>
Motivation: 在NISQ时代，开发硬件高效的量子算法实现对于实现实际的量子优势至关重要。本研究旨在为NP问题提供一种高效的量子计算解决方案。

Method: 提出了一种基于Grover搜索算法的通用量子求解器，并针对里德堡原子量子计算平台进行了优化设计。研究人员利用可并行化的单比特和多比特纠缠门来构建量子预言机，并分析了在具有动态可重构连接性的光镊阵列中的实验资源需求。

Result: 该研究提出的量子求解器能够以可证明的二次量子加速来解决广泛的NP问题。与现有的基于里德堡原子的量子退火方法相比，所需的量子比特数量仅与问题规模线性相关，大大优于二次开销。此外，研究表明里德堡原子在电路深度扩展性方面优于固定局部连接的量子处理器。

Conclusion: 该研究为使用里德堡原子系统解决NP问题提供了一个具体的蓝图，通过利用Grover搜索算法和优化的量子门设计，有望在NISQ时代实现量子优势。

Abstract: Developing hardware-efficient implementations of quantum algorithms is
crucial in the NISQ era to achieve practical quantum advantage. Here, we
construct a generic quantum solver for NP problems based on Grover's search
algorithm, specifically tailored for Rydberg-atom quantum computing platforms.
We design the quantum oracles in the search algorithm using parallelizable
single-qubit and multi-qubit entangling gates in the Rydberg atom system,
yielding a unified framework for solving a broad class of NP problems with
provable quadratic quantum speedup. We analyze the experimental resource
requirements considering the unique qubit connectivity of the dynamically
reconfigurable qubits in the optical tweezer array. The required qubit number
scales linearly with the problem size, representing a significant improvement
over existing Rydberg-based quantum annealing approaches that incur quadratic
overhead. These results provide a concrete roadmap for future experimental
efforts towards demonstrating quantum advantage in NP problem solving using
Rydberg atomic systems. Our construction indicates that atomic qubits offer
favorable circuit depth scaling compared to quantum processors with fixed local
connectivity.

</details>


### [200] [On the Trotter Error in Many-body Quantum Dynamics with Coulomb Potentials](https://arxiv.org/abs/2507.22707)
*Di Fang,Xiaoxu Wu,Avy Soffer*

Main category: quant-ph

TL;DR: This paper proves that Trotterization for simulating quantum systems with Coulomb interactions has an optimal $1/4$-order convergence rate, scaling polynomially with system size, a key step towards efficient quantum simulations.


<details>
  <summary>Details</summary>
Motivation: The central question is whether the simulation cost of many-body quantum systems scales polynomially with the system size, which is crucial for advancements in physics, chemistry, and quantum computing.

Method: The paper analyzes many-body quantum systems with Coulomb interactions and proves a $1/4$-order convergence rate for Trotterization, using a novel proof strategy that addresses the many-body structure and singular Coulomb potential in a unified framework.

Result: Trotterization for unbounded Hamiltonians with Coulomb interactions achieves a $1/4$-order convergence rate, which is optimal and has an explicit polynomial dependence on the number of particles.

Conclusion: Trotterization for many-body quantum systems with Coulomb interactions achieves an optimal $1/4$-order convergence rate, with explicit polynomial dependence on the number of particles, holding for all initial wavefunctions in the domain of the Hamiltonian.

Abstract: Efficient simulation of many-body quantum systems is central to advances in
physics, chemistry, and quantum computing, with a key question being whether
the simulation cost scales polynomially with the system size. In this work, we
analyze many-body quantum systems with Coulomb interactions, which are
fundamental to electronic and molecular systems. We prove that Trotterization
for such unbounded Hamiltonians achieves a $1/4$-order convergence rate, with
explicit polynomial dependence on the number of particles. The result holds for
all initial wavefunctions in the domain of the Hamiltonian, and the $1/4$-order
convergence rate is optimal, as previous studies have shown that it can be
saturated by a specific initial eigenstate. The main challenges arise from the
many-body structure and the singular nature of the Coulomb potential. Our proof
strategy differs from prior state-of-the-art Trotter analyses, addressing both
difficulties in a unified framework.

</details>


### [201] [Plasmon-assisted photoelectron emission in a model cluster using time-dependent density functional theory and the time-dependent surface-flux method](https://arxiv.org/abs/2507.22709)
*Mikhail Bednov,Waqas Pervez,Ingo Barke,Dieter Bauer*

Main category: quant-ph

TL;DR: Plasmon-assisted photoelectron emission investigated using TDDFT and t-SURFF. Observed post-pulse emission from plasmon oscillations and analyzed peak scaling with laser intensity.


<details>
  <summary>Details</summary>
Motivation: Investigate plasmon-assisted photoelectron emission.

Method: We use a one-dimensional time-dependent density-functional theory (TDDFT) model and the time-dependent surface-flux (t-SURFF) method to compute photoelectron spectra.

Result: Observed peaks from long-lived plasmon oscillations and associated electron emission after the laser pulse, in addition to ATI peaks. Analyzed peak positions and scaling with laser intensity.

Conclusion: We observe peaks arising from long-lived plasmon oscillations and associated electron emission occurring after the laser pulse, in addition to the expected ATI peaks. We also analyze the positions of these peaks and their scaling behavior with laser intensity.

Abstract: We investigate plasmon-assisted photoelectron emission using a
one-dimensional time-dependent density-functional theory (TDDFT) model.
Photoelectron spectra are computed with the time-dependent surface-flux
(t-SURFF) method. In addition to the expected above-threshold ionization (ATI)
comb, we observe peaks that arise from long-lived plasmon oscillations and the
associated electron emission occurring after the laser pulse. We further
analyze the positions of these peaks and their scaling behavior with the laser
intensity.

</details>


### [202] [Gain on ground state of quantum system for truly $\mathcal{PT}$ symmetry](https://arxiv.org/abs/2507.22728)
*Bing-Bing Liu,Shi-Lei Su*

Main category: quant-ph

TL;DR: 提出了一种在平均所有轨迹后，通过整合Sørensen-Reiter有效算符方法与Wiseman-Milburn主方程，实现基态有效增益（+iγ|0⟩⟨0|）的方法，为构建真正PT对称的量子器件提供了可能途径。


<details>
  <summary>Details</summary>
Motivation: 实现量子系统中增益项+iγ|0⟩⟨0|，特别是在基态上的增益，仍然是一个挑战，现有系统（省略增益项）虽然能表现出被动PT对称的能谱，但无法完全捕捉真正PT对称系统的物理行为和独特性质。

Method: 本研究整合了Sørensen-Reiter有效算符方法与Wiseman-Milburn主方程，通过连续测量和瞬时反馈控制，在平均所有轨迹后实现基态有效增益。

Result: 提出了一种实现基态有效增益（+iγ|0⟩⟨0|）的方法，为构建真正PT对称的量子器件提供了可能途径。

Conclusion: 本研究提出了一种在平均所有轨迹后，通过整合Sørensen-Reiter有效算符方法与Wiseman-Milburn主方程（用于连续测量和瞬时反馈控制）来实现基态有效增益（+iγ|0⟩⟨0|）的方法，为构建真正P T对称的量子器件提供了可能途径，并为量子信息技术应用中的量子资源工程提供了强大平台。

Abstract: For a truly $\mathcal{PT}$-symmetric quantum system, the conventional
non-Hermitian Hamiltonian is $H = H_{\rm drive} -i\gamma|1\rangle\langle1| +
i\gamma|0\rangle\langle0|$, where $\Omega$ and $\gamma$ are real parameters.
The three terms respectively represent coherent coupling, loss (on state
$|1\rangle$), and gain (on state $|0\rangle$). However, realizing the gain term
$+i\gamma|0\rangle\langle0|$ has remained an outstanding challenge for quantum
system, especially on ground state -- no theoretical or experimental schemes
have definitively demonstrated its achievement. While systems omitting this
gain term can exhibit a passively $\mathcal{PT}$-symmetric energy spectrum
(featuring a parallel imaginary shift) and display related phenomena, they fail
to capture the full physical behavior and unique properties inherent to truly
$\mathcal{PT}$-symmetric systems. In this manuscript, we propose a method to
achieve effective gain on the ground state $|0\rangle$
($+i\gamma|0\rangle\langle0|$) after averaging all trajectories, by integrating
the S{\o}rensen-Reiter effective operator method with the Wiseman-Milburn
master equation for continuous measurement and instantaneous feedback control
after averaging the evolution
  over all trajectories. This approach provides a possible pathway to
efficiently construct truly $\mathcal{PT}$-symmetric quantum devices, offering
a powerful platform for engineering quantum resources vital for quantum
information technology applications.

</details>


### [203] [Matrix product states as thin torus limits of conformal correlators](https://arxiv.org/abs/2507.22735)
*Adrián Franco-Rubio,J. Ignacio Cirac,Germán Sierra*

Main category: quant-ph

TL;DR: This paper constructs spin chain wavefunctions from CFT correlators on a torus, which become MPS in different limits, unifying known MPS states like Majumdar-Ghosh and AKLT.


<details>
  <summary>Details</summary>
Motivation: The motivation is to unify the understanding of different types of matrix product states (MPS) by constructing spin chain ansatz wavefunctions from chiral conformal field theory correlators on a torus.

Method: The paper constructs one-parameter families of spin chain ansatz wavefunctions using chiral conformal field theory correlators on a torus. The modular parameter $	au$ is used as the deformation parameter. The study focuses on families derived from SU(2)$_1$ and SU(2)$_2$ Wess-Zumino-Witten models.

Result: In the cylinder limit ($	au	o	ext{infinity}$), the wavefunctions reduce to infinite dimensional matrix product states. In the thin torus limit ($	au	o0$), they become finite bond dimension matrix product states, reproducing known MPS ground states like those of the Majumdar-Ghosh and AKLT spin chains.

Conclusion: We demonstrate that spin chain ansatz wavefunctions derived from chiral conformal field theory correlators on a torus provide a unified framework for understanding various matrix product states (MPS) in different limits of the modular parameter.

Abstract: We introduce one-parameter families of spin chain ansatz wavefunctions
constructed from chiral conformal field theory correlators on a torus, with the
modular parameter $\tau$ serving as the deformation parameter. In the cylinder
limit $\tau\to\infty$, these wavefunctions reduce to infinite dimensional
matrix product states. In contrast, in the thin torus limit $\tau\to0$, they
become finite bond dimension matrix product states (MPS). Focusing on families
derived from the SU(2)$_1$ and SU(2)$_2$ Wess-Zumino-Witten models, we show
that in the thin torus limit they reproduce known MPS ground states, such as
those of the Majumdar-Ghosh and AKLT spin chains.

</details>


### [204] [A quantum experiment with joint exogeneity violation](https://arxiv.org/abs/2507.22747)
*Yuhao Wang,Xingjian Zhang*

Main category: quant-ph

TL;DR: 量子实验违反了随机实验中的联合外生性假设，挑战了潜在结果模型。


<details>
  <summary>Details</summary>
Motivation: 尽管联合外生性假设自提出以来一直受到质疑，但尚未有证据表明其在随机实验中被违反。本研究旨在提供此类证据。

Method: 通过量子实验揭示联合外生性假设的违反。

Result: 在量子实验中发现了联合外生性假设的违反，证明了该假设并非普遍成立，尤其是在经典物理学描述能力受限的领域。

Conclusion: 该研究揭示了在量子实验中联合外生性假设的违反，至少在经典物理学无法提供完整描述的领域，从而证伪了该假设。这对于潜在结果模型具有实际和哲学意义。

Abstract: In randomized experiments, the assumption of potential outcomes is usually
accompanied by the \emph{joint exogeneity} assumption. Although joint
exogeneity has faced criticism as a counterfactual assumption since its
proposal, no evidence has yet demonstrated its violation in randomized
experiments. In this paper, we reveal such a violation in a quantum experiment,
thereby falsifying this assumption, at least in regimes where classical physics
cannot provide a complete description. We further discuss its implications for
potential outcome modelling, from both practial and philosophical perspectives.

</details>


### [205] [Approximate combinatorial optimization with Rydberg atoms: the barrier of interpretability](https://arxiv.org/abs/2507.22761)
*Christian de Correc,Thomas Ayral,Corentin Bertrand*

Main category: quant-ph

TL;DR: 基于 Rydberg 原子的量子计算在解决通用图优化问题方面面临挑战。研究评估了两种错误纠正策略，发现它们在解的质量和计算成本之间存在权衡，并且不太可能实现可扩展的通用改进，因此建议关注启发式算法。


<details>
  <summary>Details</summary>
Motivation: 为了将模拟量子计算从单位圆盘图（UD-MIS）扩展到通用图，需要嵌入方案和解释算法，但现实中解释近似解是一个难题。

Method: 本研究评估了两种解释策略（最近嵌入解和忽略有缺陷区域）在纠正近期提出的 Crossing Lattice 嵌入中的错误的能力。

Result: 基于最近嵌入解的策略可以产生高质量解，但成本呈指数级增长。基于忽略有缺陷区域的策略虽然成本较低（多项式），但会显著降低解的质量，并且在现实假设下是不可行的。此外，该策略在有利的缺陷缩放下会与已知的近似性猜想产生矛盾。

Conclusion: 该研究表明，尽管基于 Rydberg 原子平台的嵌入方案旨在解决通用图问题，但现实中的错误修正策略在效率和解的质量之间存在权衡，并且在可接受的错误率下，无法实现可扩展且通用的解的质量提升，这使得研究重点转向启发式算法。

Abstract: Analog quantum computing with Rydberg atoms is seen as an avenue to solve
hard graph optimization problems, because they naturally encode the Maximum
Independent Set (MIS) problem on Unit-Disk (UD) graphs, a problem that admits
rather efficient approximation schemes on classical computers. Going beyond
UD-MIS to address generic graphs requires embedding schemes, typically with
chains of ancilla atoms, and an interpretation algorithm to map results back to
the original problem. However, interpreting approximate solutions obtained with
realistic quantum computers proves to be a difficult problem. As a case study,
we evaluate the ability of two interpretation strategies to correct errors in
the recently introduced Crossing Lattice embedding. We find that one strategy,
based on finding the closest embedding solution, leads to very high qualities,
albeit at an exponential cost. The second strategy, based on ignoring defective
regions of the embedding graph, is polynomial in the graph size, but it leads
to a degradation of the solution quality which is prohibitive under realistic
assumptions on the defect generation. Moreover, more favorable defect scalings
lead to a contradiction with well-known approximability conjectures. Therefore,
it is unlikely that a scalable and generic improvement in solution quality can
be achieved with Rydberg platforms -- thus moving the focus to heuristic
algorithms.

</details>


### [206] [General quantum computation on photons assisted with double single-sided cavity system](https://arxiv.org/abs/2507.22773)
*Jiu-Ming Li,Jun-Yan Liu,Yuan-Yuan Liu,Xiao-Ming Xiu,Shao-Ming Fei*

Main category: quant-ph

TL;DR: 提出了一种双单边腔系统，并利用氮-空位中心生成了高保真度的光子量子门。


<details>
  <summary>Details</summary>
Motivation: 生成受控-相位门和受控-受控-相位门

Method: 概率幅方法

Result: 生成了受控-相位门和受控-受控-相位门，且保真度和门效率高。

Conclusion: 提出了一种由两个光学腔和一个两能级系统（TLS）组成的物理系统，该系统可视为双单边腔系统。通过利用概率幅方法推导了系统的通用输入-输出关系、反射和透射系数。然后，通过使用氮-空位中心代替TLS，在光子量子比特上生成受控-相位门和受控-受控-相位门，并采用简单协议，可在弱耦合和强耦合状态下完成。

Abstract: We propose a physical system consisting of two optical cavities and a
two-level system (TLS), which can be viewed as a double single-sided cavity
system. The two cavities are crossed each other in a mutually perpendicular way
and are both single-sided. The TLS is coupled to the two cavities. The
universal input-output relation of the system, the reflection and transmission
coefficients are derived by exploiting the probability amplitude method. Then
by using the nitrogen-vacancy center instead of the TLS, we generate the
controlled-phase gate and the controlled-controlled-phase gate on the photon
qubits, with simple protocols that can be accomplished in both weak and strong
coupling regimes. The protocols are shown to give rise to high fidelities and
gate efficiencies.

</details>


### [207] [Genuine multipartite entanglement as a probe of many-body localization in disordered spin chains with Dzyaloshinskii-Moriya interactions](https://arxiv.org/abs/2507.22795)
*Triyas Sapui,Keshav Das Agarwal,Tanoy Kanti Konar,Leela Ganesh Chandra Lakkaraju,Aditi Sen De*

Main category: quant-ph

TL;DR: GME vanishes in MBL phase, acts as indicator for MBL transitions, and is affected by DM interactions. Found in disordered Heisenberg spin chain with random magnetic field and DM interactions.


<details>
  <summary>Details</summary>
Motivation: To investigate the behavior of quenched average genuine multipartite entanglement (GME) in the ergodic and many-body localized (MBL) phases of a disordered quantum spin model and to identify GME as a potential indicator for the ergodic-to-MBL transition.

Method: Analyzing the disordered Heisenberg spin chain subjected to a random magnetic field and incorporating two- and three-body Dzyaloshinskii-Moriya (DM) interactions. The behavior of GME in both static eigenstates and dynamically evolved states from an initial Neel configuration is studied.

Result: The quenched average genuine multipartite entanglement (GME) can approach its maximum value in the ergodic phase but vanishes in the MBL phase. The presence of three-body DM interactions significantly stabilizes the thermal phase and delays the onset of localization. The identified transition point aligns well with standard indicators such as the gap ratio and correlation length.

Conclusion: Genuine multipartite entanglement (GME) vanishes in the many-body localized (MBL) phase, indicating a lack of useful entanglement in the localized regime. The behavior of GME, in both static eigenstates and dynamically evolved states, serves as a reliable indicator of the critical disorder strength required for the ergodic-to-MBL transition. The presence of three-body Dzyaloshinskii-Moriya (DM) interactions stabilizes the thermal phase and delays the onset of localization, which is consistently reflected in both static and dynamical analyses, reinforcing GME as a robust probe for MBL transitions.

Abstract: We demonstrate that the quenched average genuine multipartite entanglement
(GME) can approach its maximum value in the ergodic phase of a disordered
quantum spin model. In contrast, GME vanishes in the many-body localized (MBL)
phase, both in equilibrium and in the long-time dynamical steady state,
indicating a lack of useful entanglement in the localized regime. To establish
this, we analyze the disordered Heisenberg spin chain subjected to a random
magnetic field and incorporating two- and three-body Dzyaloshinskii-Moriya (DM)
interactions. We exhibit that the behavior of GME, in both static eigenstates
and in dynamically evolved states from an initial Neel configuration, serves as
a reliable indicator of the critical disorder strength required for the
ergodic-to-MBL transition. The identified transition point aligns well with
standard indicators such as the gap ratio and correlation length. Moreover, we
find that the presence of DM interactions, particularly the three-body one,
significantly stabilizes the thermal phase and delays the onset of
localization. This shift in the transition point is consistently reflected in
both static and dynamical analyses, reinforcing GME as a robust probe for MBL
transitions.

</details>


### [208] [Decoherence-free subspaces and Markovian revival of genuine multipartite entanglement in a dissipative system](https://arxiv.org/abs/2507.22796)
*Shubhodeep Gangopadhyay,Vinayak Jagadish,R. Srikanth*

Main category: quant-ph

TL;DR: 研究了三量子比特与环境的相互作用，发现在特定条件下纠缠可以复兴，并揭示了维持退相干保护态的条件。


<details>
  <summary>Details</summary>
Motivation: 研究集体相互作用的三量子比特系统与零温玻色子浴，探索退相干自由子空间和真正的纠缠动力学的出现。

Method: 通过研究三个或更多量子比特与零温玻色子浴的集体相互作用，并利用洛伦兹谱密度来量化真正的纠缠动力学，特别是研究了三量子比特系统，并利用负性凸屋延拓来量化真正的纠缠。

Result: 观察到真正的多量子比特纠缠和双可分离态之间的转变，以及在马尔可夫区域的纠缠复兴。

Conclusion: 本研究揭示了在马尔可夫和非马尔可夫区域之间转变时，多量子比特纠缠的演变，并观察到即使在马尔可夫区域也出现了纠缠复兴，证明了量子关联的鲁棒性以及维持退相干保护态的条件。

Abstract: We analyze a system of three or more qubits collectively interacting with a
zero-temperature bosonic bath characterized by a Lorentzian spectral density.
Our study focuses on the emergence of decoherence-free subspaces and the
genuine-entanglement dynamics. Specifically, we study the three qubit system in
detail, where the genuine entanglement is quantified through the convex roof
extension of negativity. By examining the transition between Markovian and
non-Markovian regimes, we reveal how the entanglement in the system evolves
under the influence of the environment. Notably, we observe transitions between
genuinely multi-qubit entangled and bi-separable states, including a revival of
entanglement even in the Markovian regime. These findings provide insights into
the robustness of quantum correlations and the conditions under which
decoherence-protected states can be sustained.

</details>


### [209] [Quantum Simulation of Nuclear Dynamics in First Quantization](https://arxiv.org/abs/2507.22814)
*Luca Spagnoli,Chiara Lissoni,Alessandro Roggero*

Main category: quant-ph

TL;DR: 该研究通过产品公式和量子信号处理，首次在第一量化中为无π EFT哈密顿量模拟核动力学提供了资源需求分析，实现了指数级改进，并预示了早期容错量子计算平台在核反应研究中的潜力。


<details>
  <summary>Details</summary>
Motivation: 在核系统动力学研究中，为陆地实验和天体物理学应用提供理论截面预测至关重要。然而，经典资源在模拟这些动力学过程时会遇到指数级成本的障碍，而利用量子计算机进行可扩展模拟是当前的研究热点。

Method: 该研究首次完整地表征了使用全领导阶（LO）无π有效场论哈密顿量在第一量化中研究核动力学的资源需求，并采用了产品公式和量子信号处理这两种模拟策略。

Result: 研究结果表明，该哈密顿量随时间的演化可以使用与粒子数量成多项式关系且与单粒子基态数量成对数关系即可实现的资源来完成。与先前在第二量化中针对同一哈密顿量模型的研究相比，这一成果实现了指数级的改进。研究估计，对于低能核散射的模拟，可能只需要数千万个T门和几百个逻辑量子比特，这表明早期容错量子计算平台有望实现对简单核反应的研究。

Conclusion: 该研究提供了使用全领导阶（LO）无π有效场论（EFT）哈密顿量在第一量化中研究核动力学的资源需求，并使用了产品公式和量子信号处理两种策略。

Abstract: The study of real time dynamics of nuclear systems is of great importance to
provide theoretical predictions of cross sections relevant for both terrestrial
experiments as well as applications in astrophysics. First principles
simulations of these dynamical processes is however hindered by an exponential
cost in classical resources and the possibility of performing scalable
simulations using quantum computers is currently an active field of research.
In this work we provide the first complete characterization of the resource
requirements for studying nuclear dynamics with the full Leading Order (LO)
pionless EFT Hamiltonian in first quantization employing simulation strategies
using both product formulas as well as Quantum Signal Processing. In
particular, we show that time evolution of such an Hamiltonian can be performed
with polynomial resources in the number of particles, and logarithmic resources
in the number of single-particle basis states. This result provides an
exponential improvement compared with previous work on the same Hamiltonian
model in second quantization. We find that interesting simulations for low
energy nuclear scattering could be achievable with tens of millions of T gates
and few hundred logical qubits suggesting that the study of simple nuclear
reactions could be amenable for early fault tolerant quantum platforms.

</details>


### [210] [Dual-wavelength quantum skyrmions from liquid crystal topological defect](https://arxiv.org/abs/2507.22815)
*Mwezi Koni,Fazilah Nothlawala,Vagharshak Hakobyan,Isaac Nape,Etienne Brasselet,Andrew Forbes*

Main category: quant-ph

TL;DR: A new method is introduced to create dual-wavelength quantum skyrmions using liquid crystals and entangled photons, opening doors for topological quantum state engineering.


<details>
  <summary>Details</summary>
Motivation: The motivation is to explore new directions for topological quantum state engineering and to leverage the topological richness of liquid crystals.

Method: The proposed method involves coupling a two-photon entangled state to an electrically tunable liquid crystal topological defect to engineer both nonlocal and local skyrmionic topologies.

Result: The paper demonstrates the generation of dual-wavelength quantum skyrmions, showcasing engineered nonlocal and local skyrmionic topologies in a reconfigurable platform.

Conclusion: The paper proposes a novel spin-orbit strategy for creating dual-wavelength quantum skyrmions, which can manifest as entangled photon pairs at different wavelengths or as heralded single-photon states at a single wavelength. This represents a new conceptual and experimental achievement.

Abstract: We propose a spin-orbit strategy for generating dual-wavelength quantum
skyrmions, realized either as entangled photon pairs at distinct wavelengths or
as heralded single-photon states at a given wavelength -- regimes neither
previously conceptualized nor demonstrated. By coupling a two-photon entangled
state to an electrically tunable liquid crystal topological defect, we engineer
both nonlocal and local skyrmionic topologies in a reconfigurable platform.
These results open new directions for topological quantum state engineering and
the topological richness of liquid crystals.

</details>


### [211] [Programmable Microwave Cluster States via Josephson Metamaterials](https://arxiv.org/abs/2507.22823)
*A. Alocco,A. Celotto,E. Palumbo,B. Galvano,P. Livreri,L. Fasolo,L. Callegaro,E. Enrico*

Main category: quant-ph

TL;DR: 在微波领域，通过可编程 JTWPA 和定制泵浦，实现了多模纠缠簇状态的按需生成，为测量基量子信息处理提供了可扩展的平台。


<details>
  <summary>Details</summary>
Motivation: 簇状态是连续变量量子计算的基本资源，能够实现超越基于量子比特架构的测量基协议。

Method: 利用可编程约瑟夫森行波参数放大器（JTWPA）在三波混合模式下工作，通过注入定制的、非等距的泵浦频率集，实现多模纠缠微波簇状态的按需生成。

Result: 通过频率分辨的零化器探测验证了纠缠结构，证实了簇状态的目标图拓扑。

Conclusion: 该方法为可重构、可扩展的微波领域测量基量子信息处理提供了新途径，与超导电路架构兼容。

Abstract: Cluster states are a fundamental resource for continuous-variable quantum
computing, enabling measurement-based protocols that can scale beyond the
limitations of qubit-based architectures. Here, we demonstrate on-demand
generation of multimode entangled microwave cluster states using a programmable
Josephson Traveling-Wave Parametric Amplifier (JTWPA) operated in the
three-wave mixing regime. By injecting a tailored, non-equidistant set of pump
tones via an arbitrary waveform generator, we engineer frequency-specific
nonlinear couplings between multiple frequency modes. The entanglement
structure is verified via frequency-resolved heterodyne detection of quadrature
nullifiers, confirming the target graph topology of the cluster state. Our
approach allows reconfigurability through the pumps spectrum and supports
scalability by leveraging the wide bandwidth and spatial homogeneity of the
JTWPA. This platform opens new avenues for scalable measurement-based quantum
information processing in the microwave domain, compatible with superconducting
circuit architectures.

</details>


### [212] [DEQSE Quantum IDE Extension: Integrated Tool for Quantum Software Engineering](https://arxiv.org/abs/2507.22843)
*Majid Haghparast,Ronja Heikkinen,Samuel Ovaskainen,Julian Fuchs,Jussi P P Jokinen,Tommi Mikkonen*

Main category: quant-ph

TL;DR: 一个用于简化量子软件开发的Visual Studio Code扩展，集成了电路设计、代码生成和模拟，支持迭代开发。


<details>
  <summary>Details</summary>
Motivation: 为了简化量子软件开发，提供一个集成的、跨平台的开发环境，以支持迭代开发和学习。

Method: 该工具作为开源软件，集成在Visual Studio Code环境中，提供了项目创建、代码运行、代码转换和嵌入式量子电路模拟等功能。

Result: 实现了DEQSE Quantum IDE Extension，它集成了多种量子计算功能，并支持迭代开发和学习，优于其他可用的Visual Studio Code量子计算扩展。

Conclusion: 该研究提出了一个名为DEQSE Quantum IDE Extension的工具，它将量子软件开发中的电路设计、代码生成和执行整合到一个跨平台环境中，支持迭代开发。

Abstract: This paper presents a tool that simplifies quantum software development by
unifying circuit design, code generation, and execution within a single
cross-platform environment that supports iterative development. Implemented as
open source, the DEQSE Quantum IDE Extension has been developed to provide
quantum functionalities within the Visual Studio Code environment, including
project creator, code runner, code converter, and embedded quantum circuit
simulator. Furthermore, the system provides capabilities that facilitate
iterative development and support learning, distinguishing it from other
available Visual Studio Code Extensions for quantum computing.

</details>


### [213] [Connection Between Classical and Quantum Descriptions of Spin Waves Using Quantum Circuits](https://arxiv.org/abs/2507.22845)
*Daniel D. Stancil,Bojko N. Bakalov,Gregory T. Byrd*

Main category: quant-ph

TL;DR: 提出了一种量子电路，可以模拟自旋链上的自旋波，并与经典描述联系起来。


<details>
  <summary>Details</summary>
Motivation: 为了近似模拟海森堡哈密顿量描述的自旋1/2粒子线性链上的单自旋波，并探索经典与量子自旋波描述之间的联系。

Method: 通过构建一个量子电路，其中每个量子比特代表一个自旋。首先通过Y旋转打开锥角，然后沿着链添加渐进的Z旋转来模拟自旋波的传播。

Result: 分析表明，该乘积状态在无界链的极限下可以得到正确的色散关系，并通过模拟器和多种量子处理器得到证实。

Conclusion: 所提出的量子电路能够近似模拟海森堡哈密顿量描述的自旋1/2粒子线性链上的单自旋波，并且该电路计算结果与经典和量子描述的自旋波联系提供了见解，还可用于表征量子处理器的误差。

Abstract: A quantum computing circuit is presented that approximates a single spin wave
quantum on a linear chain of spin 1/2 particles described by a Heisenberg
Hamiltonian. The circuit is a product state where each qubit represents a spin.
The spin wave motion is represented by opening the cone angle using Y rotations
and then adding progressive Z rotations along the chain to represent wave
propagation. We show analytically that this product state yields the correct
dispersion relation in the limit of an unbounded chain. This surprising
observation is confirmed using both a simulator and various quantum processors.
The quantum circuit calculation leads to insight into the connection between
classical and quantum descriptions of spin waves, and may also be useful for
characterizing the error in quantum processors.

</details>


### [214] [Digital Quantum Simulation of Spin Transport](https://arxiv.org/abs/2507.22846)
*Yi-Ting Lee,Bibek Pokharel,Jeffrey Cohn,Andre Schleife,Arnab Banerjee*

Main category: quant-ph

TL;DR: 開發了一種新的量子模擬方法，使用超導量子比特和直接測量技術，以更有效的方式研究量子自旋系統中的輸運現象。


<details>
  <summary>Details</summary>
Motivation: 研究量子自旋系統中的輸運現象，因為它們在自旋電子器件和量子比特中有潛在應用。現有的基於自旋-自旋ACF的方法雖然能探測自旋輸運，但基於自旋流ACF的方法因高門開銷尚未被證明，儘管它能提供更直接的輸運性質信息。

Method: 使用基於超導量子比特的器件，通過自旋流自相關函數（ACF）進行數字量子模擬，並採用一種利用非單元運算（特別是中途測量）的直接測量方案來克服資源限制。

Result: 成功演示了研究40個格點的一維XXZ海森堡模型在近彈道、超擴散和擴散區域的自旋輸運。重現了超擴散區域的功率定律行為，並在擴散區域觀察到德魯德權重的消失。

Conclusion: 本研究展示了使用超導量子比特的測量方案，可以可靠地研究量子自旋系統中的輸運現象，並成功在超擴散和擴散區域重現了預期的功率定律行為和德魯德權重的消失。

Abstract: Understanding transport phenomena in quantum spin systems has long intrigued
physicists due to their potential applications in spintronic devices and spin
qubits. Here, using a superconducting-qubit-based transmon device, we show that
pre-fault-tolerant digital quantum simulation is reliable for studying
transport phenomena via spin-current autocorrelation function (ACF). While
quantum simulations of the spin-spin ACF have been used to probe spin
transport, methods based on the spin-current ACF have yet to be demonstrated
due to their high gate cost, despite offering more direct information relevant
to the transport properties. Overcoming the resource constraints set by
indirect measurement schemes like the Hadamard test, we showcase a direct
measurement scheme that utilizes non-unitary operations, in particular
mid-circuit measurements, to investigate spin transport for the 40-site 1D XXZ
Heisenberg model in the near-ballistic, superdiffusive, and diffusive regimes.
We successfully reproduce the expected power-law behavior in the superdiffusive
regime and vanishing of the Drude weight in the diffusive regime.

</details>


### [215] [Solitons, chaos, and quantum phenomena: a deterministic approach to the Schrödinger equation](https://arxiv.org/abs/2507.22868)
*Damià Gomila*

Main category: quant-ph

TL;DR: 薛定谔方程可以从粒子（孤子）在混沌背景下的不确定性关系中推导出来，其中背景涨落的振幅对应于$
hbar$。该模型能够确定性地解释量子隧穿效应。


<details>
  <summary>Details</summary>
Motivation: 将孤子解释为粒子，在非零振幅的混沌背景下，孤子的动量和位置会发生波动，满足一个精确的不确定性关系，从而涌现出量子现象。

Method: 从精确的不确定性关系推导出薛定谔方程，并将背景涨落的振幅与$
hbar$对应。通过模拟孤子在势垒前的运动，并比较孤子系的概率分布与含时薛定谔方程的预测，验证了理论的正确性。

Result: 薛定谔方程描述了伽利略不变场论中孤子的系综平均动力学，背景涨落的振幅对应于$
hbar$的值，并从确定性的角度解释了量子隧穿效应。

Conclusion: 该理论不呈现测量和实验结果之间的统计独立性。

Abstract: We show that the Schr\"odinger equation describes the ensemble mean dynamics
of solitons in a Galilean invariant field theory where we interpret solitons as
particles. On a zero background, solitons move classically, following Newton`s
second law, however, on a non-zero amplitude chaotic background, their momentum
and position fluctuate fulfilling an exact uncertainty relation, which give
rise to the emergence of quantum phenomena. The Schrodinger equation for the
ensemble of solitons is obtained from this exact uncertainty relation, and the
amplitude of the background fluctuations is what corresponds to the value of
$\hbar$. We confirm our analytical results running simulations of solitons
moving against a potential barrier and comparing the ensemble probabilities
with the predictions of the time dependent Schr\"odinger equation, providing a
deterministic version of the quantum tunneling effect. We conclude with a
discussion of how our theory does not present statistical independence between
measurement and experiment outcome.

</details>


### [216] [Non-classicality at equilibrium and efficient predictions under non-commuting charges](https://arxiv.org/abs/2507.22882)
*Lodovico Scarpa,Nishan Ranabhat,Amit Te'eni,Abdulla Alhajri,Vlatko Vedral,Fabio Anza,Luis Pedro García-Pintos*

Main category: quant-ph

TL;DR: 研究了具有非对易量子数的热力学系统中的弛豫问题，发现其与弱值和 Kirkwood-Dirac 拟态分布有关，并揭示了弱值在平衡时可能出现的异常现象。


<details>
  <summary>Details</summary>
Motivation: 量子热力学系统可以保持非对易可观测量，但这种现象对弛豫的影响仍未完全理解。

Method: 利用可观测量依赖的方法研究了孤立量子系统中的平衡和热化问题，并将其扩展到具有非对易量子数的场景，而无需访问能量特征值和特征向量，即可准确估计粗略可观测量。

Result: 提出的方法可以准确估计粗略可观测量，无需弱耦合且不局限于局域可观测量，与非阿贝尔热态相比具有优势。研究表明，由于量子数的非对易性，弱值即使在平衡时也可能出现异常，这是非经典性的代理。

Conclusion: 该研究揭示了具有非对易量子数的热力学系统在弛豫过程中，弱值与 Kirkwood-Dirac 拟态分布之间存在新的联系，并表明弱值在平衡时可能出现异常，这可作为非经典性的代理。

Abstract: A quantum thermodynamic system can conserve non-commuting observables, but
the consequences of this phenomenon on relaxation are still not fully
understood. We investigate this problem by leveraging an observable-dependent
approach to equilibration and thermalization in isolated quantum systems. We
extend such approach to scenarios with non-commuting charges, and show that it
can accurately estimate the equilibrium distribution of coarse observables
without access to the energy eigenvalues and eigenvectors. Our predictions do
not require weak coupling and are not restricted to local observables, thus
providing an advantage over the non-Abelian thermal state. Within this
approach, weak values and quasiprobability distributions emerge naturally and
play a crucial role in characterizing the equilibrium distributions of
observables. We show and numerically confirm that, due to charges'
non-commutativity, these weak values can be anomalous even at equilibrium,
which has been proven to be a proxy for non-classicality. Our work thus
uncovers a novel connection between the relaxation of observables under
non-commuting charges, weak values, and Kirkwood-Dirac quasiprobability
distributions.

</details>


### [217] [Operational interpretation of the Stabilizer Entropy](https://arxiv.org/abs/2507.22883)
*Lennart Bittel,Lorenzo Leone*

Main category: quant-ph

TL;DR: 本研究为魔态资源理论中的稳定器熵提供了操作解释，量化了其作为量子资源的性质，揭示了其在区分量子态和表征量子态转变中的关键作用。


<details>
  <summary>Details</summary>
Motivation: 魔态资源理论在量子纠错和量子系统经典模拟中具有重要应用，但稳定器熵的操作解释一直是一个悬而未决的问题。

Method: 本研究通过证明稳定器熵是最鲁棒的可测量魔态单调量，并利用其量化分析了 Clifford 轨道与 Haar 随机态的区分度，以及任意量子态与稳定器态的区分概率。

Result: 本研究表明，Clifford 轨道与 Haar 随机态的区分度以指数速率衰减，衰减率由稳定器熵 $M(\psi)$ 和可用拷贝数决定，从而证明 Clifford 轨道近似形成一个 $k$-design，其近似误差为 $\Theta(\exp(-M(\psi))$。此外，区分任意量子态与稳定器态的最佳概率也由其稳定器熵决定。

Conclusion: 本研究通过将魔态资源理论与量子性质测试相结合，为稳定器 Rényi 熵提供了严格的操作解释，揭示了其作为量子资源的关键作用。

Abstract: Magic-state resource theory is a fundamental framework with far-reaching
applications in quantum error correction and the classical simulation of
quantum systems. Recent advances have significantly deepened our understanding
of magic as a resource across diverse domains, including many-body physics,
nuclear and particle physics, and quantum chemistry. Central to this progress
is the stabilizer R\'enyi entropy, a computable and experimentally accessible
magic monotone. Despite its widespread adoption, a rigorous operational
interpretation of the stabilizer entropy has remained an open problem. In this
work, we provide such an interpretation in the context of quantum property
testing. By showing that the stabilizer entropy is the most robust measurable
magic monotone, we demonstrate that the Clifford orbit of a quantum state
becomes exponentially indistinguishable from Haar-random states, at a rate
governed by the stabilizer entropy $M(\psi)$ and the number of available
copies. This implies that the Clifford orbit forms an approximate state
$k$-design, with an approximation error $\Theta(\exp(-M(\psi))$. Conversely, we
establish that the optimal probability of distinguishing a given quantum state
from the set of stabilizer states is also governed by its stabilizer entropy.
These results reveal that the stabilizer entropy quantitatively characterizes
the transition from stabilizer states to universal quantum states, thereby
offering a comprehensive operational perspective of the stabilizer entropy as a
quantum resource.

</details>


<div id='cs.ET'></div>

# cs.ET [[Back]](#toc)

### [218] [Toward Intelligent Electronic-Photonic Design Automation for Large-Scale Photonic Integrated Circuits: from Device Inverse Design to Physical Layout Generation](https://arxiv.org/abs/2507.22301)
*Hongjian Zhou,Pingchuan Ma,Jiaqi Gu*

Main category: cs.ET

TL;DR: PoLaRIS is an Intelligent Electronic-Photonic Design Automation framework that automates the design of Photonic Integrated Circuits (PICs), addressing the inefficiencies and errors of manual design.


<details>
  <summary>Details</summary>
Motivation: The manual design process for modern PICs with hundreds to thousands of components is inefficient, poorly scalable, and error-prone, necessitating an automated solution.

Method: PoLaRIS integrates a fabrication-aware inverse design engine with a routing-informed placement and curvy-aware detailed router, combining physics-driven optimization with machine learning and domain-specific algorithms.

Result: PoLaRIS significantly accelerates PIC development, lowers design barriers, and lays the groundwork for scalable photonic system design automation.

Conclusion: PoLaRIS framework enables automated generation of design rule violation-free and performance-optimized PIC layouts, accelerating development and lowering design barriers.

Abstract: Photonic Integrated Circuits (PICs) offer tremendous advantages in bandwidth,
parallelism, and energy efficiency, making them essential for emerging
applications in artificial intelligence (AI), high-performance computing (HPC),
sensing, and communications. However, the design of modern PICs, which now
integrate hundreds to thousands of components, remains largely manual,
resulting in inefficiency, poor scalability, and susceptibility to errors. To
address these challenges, we propose PoLaRIS, a comprehensive Intelligent
Electronic-Photonic Design Automation (EPDA) framework that spans both
device-level synthesis and system-level physical layout. PoLaRIS combines a
robust, fabrication-aware inverse design engine with a routing-informed
placement and curvy-aware detailed router, enabling the automated generation of
design rule violation (DRV)-free and performance-optimized layouts. By unifying
physics-driven optimization with machine learning and domain-specific
algorithms, PoLaRIS significantly accelerates PIC development, lowers design
barriers, and lays the groundwork for scalable photonic system design
automation.

</details>


### [219] [Green Wave as an Integral Part for the Optimization of Traffic Efficiency and Safety: A Survey](https://arxiv.org/abs/2507.22511)
*Kranthi Kumar Talluri,Christopher Stang,Galia Weidl*

Main category: cs.ET

TL;DR: 该论文调查了现有的绿波交通信号协调策略，分析了它们对未来交通管理的影响，并讨论了减少排放和提高弱势道路使用者安全的优势。最后，它确定了建立强大的绿波系统所面临的挑战和研究空白，为可持续城市交通的未来指明了方向。


<details>
  <summary>Details</summary>
Motivation: 为了探索将绿波解决方案与智慧城市计划相结合以实现有效的交通信号协调，需要了解交通管理及其对交通效率和安全的影响的先前研究。

Method: 对现有的信号灯协调策略进行全面调查，并分析其对未来交通管理系统和城市基础设施的影响。

Result: 文章全面调查了现有的信号灯协调策略，并分析了其对未来交通管理系统和城市基础设施的影响。

Conclusion: 现有信号灯协调策略的优势，例如减少排放和考虑行人等弱势道路使用者的安全，在文章中都有讨论。最后，文章讨论了建立强大而成功的绿波系统所面临的挑战和研究空白，并明确阐述了可持续城市交通的未来需求。

Abstract: Green Wave provides practical and advanced solutions to improve traffic
efficiency and safety through network coordination. Nevertheless, the complete
potential of Green Wave systems has yet to be explored. Utilizing emerging
technologies and advanced algorithms, such as AI or V2X, would aid in achieving
more robust traffic management strategies, especially when integrated with
Green Wave. This work comprehensively surveys existing traffic control
strategies that enable Green Waves and analyzes their impact on future traffic
management systems and urban infrastructure. Understanding previous research on
traffic management and its effect on traffic efficiency and safety helps
explore the integration of Green Wave solutions with smart city initiatives for
effective traffic signal coordination. This paper also discusses the advantages
of using Green Wave strategies for emission reduction and considers road safety
issues for vulnerable road users, such as pedestrians and cyclists. Finally,
the existing challenges and research gaps in building robust and successful
Green Wave systems are discussed to articulate explicitly the future
requirement of sustainable urban transport.

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [220] [Settling Weighted Token Swapping up to Algorithmic Barriers](https://arxiv.org/abs/2507.22450)
*Nicole Wein,Guanyu,Zhang*

Main category: cs.DS

TL;DR: 本文为加权令牌交换问题提供了近似算法和紧密障碍结果，分别在一般图和树上实现了 2+2W/w 和 1+W/w 的近似比。


<details>
  <summary>Details</summary>
Motivation: 加权令牌交换问题是一个组合优化问题，旨在找到最小化令牌交换成本的序列。该问题在一般图和树上的近似算法和准确解法都具有挑战性，特别是涉及加权令牌时。

Method: 本文提出了针对一般图和树的加权令牌交换问题的近似算法，并建立了相应的紧密障碍结果。

Result: 对于一般图，本文的近似算法实现了 2+2W/w 的近似比；对于树，近似算法实现了 1+W/w 的近似比。

Conclusion: 本文首次为树和一般图上的加权令牌交换问题提供了近似算法，并给出了紧密的障碍结果。对于一般图，近似比为 2+2W/w；对于树，近似比为 1+W/w，其中 W 和 w 分别为最大和最小令牌权重。

Abstract: We study the weighted token swapping problem, in which we are given a graph
on $n$ vertices, $n$ weighted tokens, an initial assignment of one token to
each vertex, and a final assignment of one token to each vertex. The goal is to
find a minimum-cost sequence of swaps of adjacent tokens to reach the final
assignment from the initial assignment, where the cost is the sum over all
swaps of the sum of the weights of the two swapped tokens. Unweighted token
swapping has been extensively studied: it is NP-hard to approximate to a factor
better than $14/13$, and there is a polynomial-time 4-approximation, along with
a tight "barrier" result showing that the class of locally optimal algorithms
cannot achieve a ratio better than 4. For trees, the problem remains NP-hard to
solve exactly, and there is a polynomial-time 2-approximation, along with a
tight barrier result showing that the class of $\ell$-straying algorithms
cannot achieve a ratio better than 2. Weighted token swapping with $\{0,1\}$
weights is much harder to approximation: it is NP-hard to approximate even to a
factor of $(1-\varepsilon) \cdot \ln n$ for any constant $\varepsilon>0$.
Restricting to positive weights, no approximation algorithms are known, and the
only known lower bounds are those inherited directly from the unweighted
version. We provide the first approximation algorithms for weighted token
swapping on both trees and general graphs, along with tight barrier results.
Letting $w$ and $W$ be the minimum and maximum token weights, our approximation
ratio is $2+2W/w$ for general graphs and $1+W/w$ for trees.

</details>


### [221] [Deterministic Longest Common Subsequence Approximation in Near-Linear Time](https://arxiv.org/abs/2507.22486)
*Itai Boneh,Shay Golan,Matan Kraus*

Main category: cs.DS

TL;DR: 这是一个关于最长公共子序列（LCS）问题的确定性算法，能在接近线性时间内提供O(n^{3/4} log n)的近似值，是该领域的一个突破。


<details>
  <summary>Details</summary>
Motivation: 旨在为最长公共子序列（LCS）问题提供一个在接近线性时间复杂度下能获得亚线性近似比的确定性算法。

Method: 使用一个确定性算法。

Result: 该算法能够为两个长度为n的输入序列的最长公共子序列（LCS）提供一个O(n^{3/4} log n)的近似值。

Conclusion: 我们提供了一个确定性算法，可以在接近线性时间的时间内为两个长度为n的输入序列的最长公共子序列（LCS）输出一个O(n^{3/4} log n)的近似值。

Abstract: We provide a deterministic algorithm that outputs an $O(n^{3/4} \log
n)$-approximation for the Longest Common Subsequence (LCS) of two input
sequences of length $n$ in near-linear time. This is the first deterministic
approximation algorithm for LCS that achieves a sub-linear approximation ratio
in near-linear time.

</details>


### [222] [BlockFIFO & MultiFIFO: Scalable Relaxed Queues](https://arxiv.org/abs/2507.22764)
*Stefan Koch,Peter Sanders,Marvin Williams*

Main category: cs.DS

TL;DR: 本文提出了两种宽松并发FIFO队列，在性能上优于现有技术。


<details>
  <summary>Details</summary>
Motivation: 为了提高并发FIFO队列的吞吐量和可扩展性，通过放松FIFO语义允许元素重排序，以降低队列头部和尾部的竞争。

Method: 本文提出了两种宽松并发FIFO队列的设计，一种衍生自MultiQueue，另一种基于环形缓冲区。

Result: 本文提出的两种宽松FIFO队列设计均优于现有的宽松和严格FIFO队列，在吞吐量和可扩展性方面表现更佳。

Conclusion: 通过放松FIFO队列的语义，允许元素重排序，可以实现更高的可扩展性。本文提出的两种基于MultiQueue和环形缓冲区的宽松FIFO队列设计，在微基准测试和实际应用中均优于现有技术。

Abstract: FIFO queues are a fundamental data structure used in a wide range of
applications. Concurrent FIFO queues allow multiple execution threads to access
the queue simultaneously. Maintaining strict FIFO semantics in concurrent
queues leads to low throughput due to high contention at the head and tail of
the queue. By relaxing the FIFO semantics to allow some reordering of elements,
it becomes possible to achieve much higher scalability. This work presents two
orthogonal designs for relaxed concurrent FIFO queues, one derived from the
MultiQueue and the other based on ring buffers. We evaluate both designs
extensively on various micro-benchmarks and a breadth-first search application
on large graphs. Both designs outperform state-of-the-art relaxed and strict
FIFO queues, achieving higher throughput and better scalability.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [223] [Toward Trusted Onboard AI: Advancing Small Satellite Operations using Reinforcement Learning](https://arxiv.org/abs/2507.22198)
*Cannon Whitney,Joseph Melville*

Main category: eess.SY

TL;DR: 该研究提出了一种用于卫星命令自动化的强化学习（RL）算法，通过数字孪生和隔离环境验证了其在轨有效性，旨在减少对地面控制的依赖并提高响应速度。


<details>
  <summary>Details</summary>
Motivation: 为了实现更快的响应时间和减少对地面控制的依赖，需要在车载系统中集成人工智能（AI）以实现命令自动化，特别是针对航天器等远程和自主应用场景，建立对 AI 系统的信任至关重要。

Method: 该研究开发了一种用于 3U CubeSat 命令自动化的强化学习（RL）算法，重点在于宏观控制动作 RL 的实现。代理根据实时遥测数据编译的信息进行观察，并产生诸如调整姿态以指向太阳之类的高层动作，然后将这些动作转换为低层指令执行。通过构建特定航天器的数字孪生并在其中训练 RL 代理，然后将其策略复制到隔离环境中，使用编译后的卫星信息进行推理预测，从而在不授予命令权限的情况下验证了 RL 算法在轨道的有效性。

Result: 该研究成功地在数字孪生环境中训练了一个 RL 代理，并将其策略安全地部署到隔离环境中进行验证，证明了 RL 算法在处理卫星命令自动化方面的有效性，并为在轨 AI 推理奠定了基础。

Conclusion: 该研究通过在数字孪生环境中训练 RL 代理，并将其策略安全地部署到隔离环境中进行验证，展示了 RL 算法在卫星命令自动化方面的有效性。这项工作为在车载系统（特别是航天器）中集成可信赖的人工智能铺平了道路。

Abstract: A RL (Reinforcement Learning) algorithm was developed for command automation
onboard a 3U CubeSat. This effort focused on the implementation of macro
control action RL, a technique in which an onboard agent is provided with
compiled information based on live telemetry as its observation. The agent uses
this information to produce high-level actions, such as adjusting attitude to
solar pointing, which are then translated into control algorithms and executed
through lower-level instructions. Once trust in the onboard agent is
established, real-time environmental information can be leveraged for faster
response times and reduced reliance on ground control. The approach not only
focuses on developing an RL algorithm for a specific satellite but also sets a
precedent for integrating trusted AI into onboard systems. This research builds
on previous work in three areas: (1) RL algorithms for issuing high-level
commands that are translated into low-level executable instructions; (2) the
deployment of AI inference models interfaced with live operational systems,
particularly onboard spacecraft; and (3) strategies for building trust in AI
systems, especially for remote and autonomous applications. Existing RL
research for satellite control is largely limited to simulation-based
experiments; in this work, these techniques are tailored by constructing a
digital twin of a specific spacecraft and training the RL agent to issue macro
actions in this simulated environment. The policy of the trained agent is
copied to an isolated environment, where it is fed compiled information about
the satellite to make inference predictions, thereby demonstrating the RL
algorithm's validity on orbit without granting it command authority. This
process enables safe comparison of the algorithm's predictions against actual
satellite behavior and ensures operation within expected parameters.

</details>


### [224] [Optimal Planning for Enhancing the Resilience of Modern Distribution Systems Against Cyberattacks](https://arxiv.org/abs/2507.22226)
*Armita Khashayardoost,Ahmad Mohammad Saber,Deepa Kundur*

Main category: eess.SY

TL;DR: 本研究调查了利用高瓦数物联网设备（如电动汽车充电器）在配电网级别进行网络攻击的可行性，并提出了使用分布式发电（DG）的弹性策略来应对这些攻击。


<details>
  <summary>Details</summary>
Motivation: 随着智能电网中物联网设备集成度的提高，配电网面临着新的网络安全漏洞，特别是利用高瓦数物联网设备（如电动汽车充电器）进行网络攻击以操纵局部需求和破坏电网稳定的可能性。

Method: 本研究通过分析高瓦数物联网设备（如电动汽车充电器）对配电网局部负荷操纵的可行性与影响，并提出利用分布式发电（DG）作为一种具有潜力的网络弹性策略。

Result: 研究结果表明，传统的电网保护措施不足以应对此类智能、局部化的攻击，并提出使用分布式发电（DG）的弹性策略，以应对这些攻击。

Conclusion: 本研究强调了在智能电网中进行配电级网络弹性规划的紧迫性。

Abstract: The increasing integration of IoT-connected devices in smart grids has
introduced new vulnerabilities at the distribution level. Of particular concern
is the potential for cyberattacks that exploit high-wattage IoT devices, such
as EV chargers, to manipulate local demand and destabilize the grid. While
previous studies have primarily focused on such attacks at the transmission
level, this paper investigates their feasibility and impact at the distribution
level. We examine how cyberattackers can target voltage-sensitive nodes,
especially those exposed by the presence of high-consumption devices, to cause
voltage deviation and service disruption. Our analysis demonstrates that
conventional grid protections are insufficient against these intelligent,
localized attacks. To address this, we propose resilience strategies using
distributed generation (DGs), exploring their role in preemptive planning. This
research highlights the urgent need for distribution-level cyber resilience
planning in smart grids.

</details>


### [225] [Of Good Demons and Bad Angels: Guaranteeing Safe Control under Finite Precision](https://arxiv.org/abs/2507.22760)
*Samuel Teuber,Debasmita Lohar,Bernhard Beckert*

Main category: eess.SY

TL;DR: 该研究提出了一种将有限精度扰动下的鲁棒性纳入安全验证的方法，以解决神经网控制网络物理系统（NNCS）在安全关键应用中的无限时间范围安全保证问题。通过将问题建模为混合博弈，并利用混合精度定点调谐器，该方法能够合成具有严格安全保证的高效神经网络实现，并在汽车和航空领域进行了验证。


<details>
  <summary>Details</summary>
Motivation: 为了在安全关键的神经网控制网络物理系统（NNCS）中正式保证其安全性，特别是需要确保整个运行过程中的安全，即需要无限时间范围验证。然而，现有的基于微分动态逻辑（dL）的保证依赖于理想化的、实值神经网络语义，未能考虑有限精度实现引入的舍入误差。

Method: 将问题建模为混合博弈，一个负责控制动作的“好妖魔”，一个引入扰动的“坏天使”，并利用最先进的混合精度定点调谐器来合成健全且高效的实现。

Result: 提出了一种将有限精度扰动下的鲁棒性纳入安全验证的方法，为NNCS提供了严格的无限时间范围安全保证，并实现了高效的神经网络实现。

Conclusion: 该方法为自动驾驶和航空领域提供了具有严格无限时间范围安全保证的高效神经网络实现。

Abstract: As neural networks (NNs) become increasingly prevalent in safety-critical
neural network-controlled cyber-physical systems (NNCSs), formally guaranteeing
their safety becomes crucial. For these systems, safety must be ensured
throughout their entire operation, necessitating infinite-time horizon
verification. To verify the infinite-time horizon safety of NNCSs, recent
approaches leverage Differential Dynamic Logic (dL). However, these dL-based
guarantees rely on idealized, real-valued NN semantics and fail to account for
roundoff errors introduced by finite-precision implementations. This paper
bridges the gap between theoretical guarantees and real-world implementations
by incorporating robustness under finite-precision perturbations -- in sensing,
actuation, and computation -- into the safety verification. We model the
problem as a hybrid game between a good Demon, responsible for control actions,
and a bad Angel, introducing perturbations. This formulation enables formal
proofs of robustness w.r.t. a given (bounded) perturbation. Leveraging this
bound, we employ state-of-the-art mixed-precision fixed-point tuners to
synthesize sound and efficient implementations, thus providing a complete
end-to-end solution. We evaluate our approach on case studies from the
automotive and aeronautics domains, producing efficient NN implementations with
rigorous infinite-time horizon safety guarantees.

</details>


### [226] [Safe and Efficient Data-driven Connected Cruise Control](https://arxiv.org/abs/2507.22227)
*Haosong Xiao,Chaozhe R. He*

Main category: eess.SY

TL;DR: 本研究提出了一种利用V2V通信的先进巡航控制系统，可显著提高能效并确保安全。


<details>
  <summary>Details</summary>
Motivation: 为了设计一种能够安全高效地运行的巡航控制系统，该系统能够利用V2V通信从前方车辆获取运动信息。

Method: 通过控制屏障函数推导出安全滤波器，并将其应用于所提出的巡航控制器设计中，以提供安全保证。研究人员利用来自一系列人类驾驶车辆的位置和速度数据来设计控制器，并将其与标准的非连接自适应巡航控制进行比较。

Result: 与标准的非连接自适应巡航控制相比，最优地利用V2V连接可以将能耗降低10%以上。此外，研究还强调了安全滤波器和能效设计之间有趣的相互作用，并指出了未来研究的方向。

Conclusion: 该研究设计了一种安全的、高效的、可连接的巡航控制器，用于具有V2V通信的自动驾驶汽车。通过利用前方车辆的运动信息，该控制器能够平稳地应对交通扰动并最大限度地提高能源效率。

Abstract: In this paper, we design a safe and efficient cruise control for the
connected automated vehicle with access to motion information from multiple
vehicles ahead via vehicle-to-vehicle (V2V) communication. Position and
velocity data collected from a chain of human-driven vehicles are
systematically leveraged to design a connected cruise controller that smoothly
responds to traffic perturbations while maximizing energy efficiency. A safety
filter derived from a control barrier function provides the safety guarantee.
We investigate the proposed control design's energy performance against real
traffic datasets and quantify the safety filter's energy impact. It is shown
that optimally utilizing V2V connectivity reduces energy consumption by more
than 10\% compared to standard non-connected adaptive cruise control.
Meanwhile, interesting interplays between safety filter and energy efficiency
design are highlighted, revealing future research directions.

</details>


### [227] [Design and Experimental Validation of UAV Swarm-Based Phased Arrays with MagSafe- and LEGO-Inspired RF Connectors](https://arxiv.org/abs/2507.22295)
*Bidya Debnath,Mst Mostary Begum,Prashant Neupant,Brooke E. Molen,Junming Diao*

Main category: eess.SY

TL;DR: 本研究提出了一种基于无人机蜂群和 MagSafe/LEGO 启发连接器的新型相控阵天线系统，解决了分布式相控阵的同步、定位和相位相干性问题。该系统具有可扩展性、低损耗（0.2 dB 插入损耗）和易于部署的优点，适用于通信、雷达和遥感等应用。


<details>
  <summary>Details</summary>
Motivation: 解决了分布式相控阵的关键挑战，包括单元间振荡器同步、定位、相位相干性和位置精度。

Method: 提出了一种新颖的无人机蜂群式相控阵天线系统，该系统利用 MagSafe 和 LEGO 启发的射频连接器，解决了分布式相控阵的关键挑战，包括单元间振荡器同步、定位、相位相干性和位置精度。所提出的非螺纹、免提连接器可在飞行中对接期间实现精确的单元间距并建立连续、低损耗的射频信号传播路径。多阶段优化射频连接器实现了紧凑的外形尺寸、直流到射频的带宽以及低至 0.2 dB 的测量插入损耗。

Result: 实验结果与模拟结果非常吻合，证明了鲁棒的波束转向能力。

Conclusion: 本研究提供了一个实用、可扩展且低复杂度的平台，可用于下一代机载通信、雷达和遥感应用的快速部署。

Abstract: This paper presents a novel UAV swarm-based phased array antenna system that
leverages MagSafe- and LEGO-inspired radio frequency (RF) connectors to address
key challenges in distributed phased arrays, including inter-element oscillator
synchronization, localization, phase coherence, and positional accuracy. The
proposed non-threaded, hands-free connectors enable precise inter-element
spacing and establish a continuous, low-loss RF signal propagation path during
mid-flight docking. A multi-stage optimization of the RF connector achieves a
compact form factor, DC-to-RF bandwidth, and a measured insertion loss as low
as 0.2\,dB. The system architecture offers scalability in gain and frequency by
adjusting the array element density per UAV and UAV dimensions. Experimental
results from both stationary and in-flight tests of two UAV-based phased array
prototypes align closely with simulations, demonstrating robust beam steering
to multiple directions. This work delivers a practical, scalable, and
low-complexity platform that enables rapid deployment for next-generation
airborne communications, radar, and remote sensing applications.

</details>


### [228] [Assessing Value of Renewable-based VPP Versus Electrical Storage: Multi-market Participation Under Different Scheduling Regimes and Uncertainties](https://arxiv.org/abs/2507.22496)
*Hadi Nemati,Ignacio Egido,Pedro Sánchez-Martín,Álvaro Ortega*

Main category: eess.SY

TL;DR: 本文比较了可再生能源为主的虚拟电厂（RVPPs）和电网规模储能系统（ESSs）在能源和备用市场中的表现，使用了双阶段鲁棒优化方法，并考虑了各种不确定性和情景。研究结果为运营商提供了关于这两种方法的量化指导。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在比较可再生能源为主的虚拟电厂（RVPPs）和电网规模储能系统（ESSs）在能源和备用市场中的参与情况，并评估它们在技术性能、市场策略和经济效益方面的表现。

Method: 本文采用了双阶段鲁棒优化框架。RVPP模型考虑了价格、发电和需求不确定性，而ESS模型仅考虑价格不确定性。此外，还提出了一种调整ESS规模以匹配RVPP市场表现的算法。

Result: 模拟结果表明，在不同情景下，RVPPs和ESSs在市场参与和经济效益方面各有优劣。该研究通过量化分析，为运营商提供了关于这两种不同电网解决方案的相对价值的见解。

Conclusion: 该研究为运营商提供了关于可再生能源为主的虚拟电厂（RVPPs）和电网规模储能系统（ESSs）在能源和备用市场中相对价值的量化指导。

Abstract: This paper compares the participation of Renewable-only Virtual Power Plants
(RVPPs) and grid-scale Electrical Storage Systems (ESSs) in energy and reserve
markets, evaluating their technical performance, market strategies, and
economic outcomes. To ensure a fair comparison, scheduling is analyzed over
representative sample days that capture seasonal operating regimes, and the
associated uncertainties are explicitly modeled. Two-stage robust optimization
frameworks are employed: the RVPP model addresses price, generation, and demand
uncertainties, whereas the ESS model considers price uncertainty only. In
addition, an algorithm is proposed for sizing the ESS so that its market
performance matches that of the RVPP. Simulations cover both favorable and
unfavorable scenarios, reflecting seasonal energy limits for dispatchable
resources, varying forecast errors for nondispatchable resources, and
alternative uncertainty-management strategies. The results provide operators
with quantitative guidance on the relative value of each approach.

</details>


### [229] [Safe Deployment of Offline Reinforcement Learning via Input Convex Action Correction](https://arxiv.org/abs/2507.22640)
*Alex Durkin,Jasper Stolte,Matthew Jones,Raghuraman Pitchumani,Bei Li,Christian Michler,Mehmet Mercangöz*

Main category: eess.SY

TL;DR: 本研究将离线强化学习应用于化工过程控制，并提出了一种基于PICNNs的部署时安全层来解决稳定性问题。实验证明，该方法在模拟的聚合反应器控制任务中优于传统控制方法。


<details>
  <summary>Details</summary>
Motivation: 为了在化工过程控制中应用离线强化学习，需要解决稳定性和在设定点附近性能下降的挑战。

Method: 本研究提出了一种新颖的、用于输入凸神经网络（PICNNs）的部署时安全层，以作为学习到的成本模型。PICNNs 能够通过在凸的、状态条件成本表面上进行下降，在无需重新训练或环境交互的情况下，实时地、可微分地校正策略动作。

Result: 实验结果表明，离线强化学习，特别是结合了凸约束动作校正时，在三个工业场景（启动、等级变化下降和等级变化上升）中，其性能优于传统控制方法，并能保持稳定性。

Conclusion: 本研究表明，结合了基于梯度的动作校正的离线强化学习可以超越传统的控制方法，并在所有场景中保持稳定性，为在工业系统中实现更可靠的数据驱动自动化奠定了基础。

Abstract: Offline reinforcement learning (offline RL) offers a promising framework for
developing control strategies in chemical process systems using historical
data, without the risks or costs of online experimentation. This work
investigates the application of offline RL to the safe and efficient control of
an exothermic polymerisation continuous stirred-tank reactor. We introduce a
Gymnasium-compatible simulation environment that captures the reactor's
nonlinear dynamics, including reaction kinetics, energy balances, and
operational constraints. The environment supports three industrially relevant
scenarios: startup, grade change down, and grade change up. It also includes
reproducible offline datasets generated from proportional-integral controllers
with randomised tunings, providing a benchmark for evaluating offline RL
algorithms in realistic process control tasks.
  We assess behaviour cloning and implicit Q-learning as baseline algorithms,
highlighting the challenges offline agents face, including steady-state offsets
and degraded performance near setpoints. To address these issues, we propose a
novel deployment-time safety layer that performs gradient-based action
correction using input convex neural networks (PICNNs) as learned cost models.
The PICNN enables real-time, differentiable correction of policy actions by
descending a convex, state-conditioned cost surface, without requiring
retraining or environment interaction.
  Experimental results show that offline RL, particularly when combined with
convex action correction, can outperform traditional control approaches and
maintain stability across all scenarios. These findings demonstrate the
feasibility of integrating offline RL with interpretable and safety-aware
corrections for high-stakes chemical process control, and lay the groundwork
for more reliable data-driven automation in industrial systems.

</details>


### [230] [Distributed Average Consensus in Wireless Multi-Agent Systems with Over-the-Air Aggregation](https://arxiv.org/abs/2507.22648)
*Themistoklis Charalambous,Zheng Chen,Christoforos N. Hadjicostis*

Main category: eess.SY

TL;DR: 提出了一种利用无线信号叠加特性的分布式算法，用于解决多智能体系统的平均共识问题，并验证了其收敛性。


<details>
  <summary>Details</summary>
Motivation: 为了解决无线网络中多智能体系统的平均共识问题。

Method: 提出了一种分布式平均共识算法，利用了无线多址接入信道的信号叠加特性，并对已知的Ratio Consensus算法进行了改进，增加了归一化步骤来补偿任意信道系数。

Result: 通过数值模拟验证了该算法的有效性。

Conclusion: 该算法在忽略接收端噪声的情况下，对于时不变和时变信道，渐近收敛于平均值。

Abstract: In this paper, we address the average consensus problem of multi-agent
systems over wireless networks. We propose a distributed average consensus
algorithm by invoking the concept of over-the-air aggregation, which exploits
the signal superposition property of wireless multiple-access channels. The
proposed algorithm deploys a modified version of the well-known Ratio Consensus
algorithm with an additional normalization step for compensating for the
arbitrary channel coefficients. We show that, when the noise level at the
receivers is negligible, the algorithm converges asymptotically to the average
for time-invariant and time-varying channels. Numerical simulations corroborate
the validity of our results.

</details>


### [231] [Malleability-Resistant Encrypted Control System with Disturbance Compensation and Real-Time Attack Detection](https://arxiv.org/abs/2507.22693)
*Naoki Aizawa,Keita Emura,Kiminao Kogiso*

Main category: eess.SY

TL;DR: 该研究提出了一种结合了密钥同态加密（KHE）和干扰观察器（DOB）的加密PID控制器，以提高控制性能和安全性，并成功通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 旨在实现控制性能，同时提供对基于延展性的攻击的抵抗能力。

Method: 提出了一种使用密钥同态加密（KHE）方案的加密PID控制器，并结合了具有干扰观察器（DOB）的PID结构，以补偿建模不确定性，并通过在解密或评估过程中篡改密文时输出错误符号来检测延展性攻击。

Result: 实验结果表明，该方法在跟踪精度方面优于传统方法，并成功检测了两种延展性攻击。

Conclusion: 该加密DOB基PID控制器在跟踪精度方面优于传统加密PID控制器，并且能够成功检测两种基于延展性的攻击。

Abstract: This study proposes an encrypted PID control system with a disturbance
observer (DOB) using a keyed-homomorphic encryption (KHE) scheme, aiming to
achieve control performance while providing resistance to malleability-based
attacks. The controller integrates a DOB with a PID structure to compensate for
modeling uncertainties by estimating and canceling external disturbances. To
enhance security, the system is designed to output error symbols when
ciphertexts are falsified during decryption or evaluation, enabling real-time
detection of malleability-based signal or parameter falsification. To validate
the proposed method, we conduct stage positioning control experiments and
attack detection tests using an industrial linear stage. The results show that
the encrypted DOB-based PID controller outperforms a conventional encrypted PID
controller in terms of tracking accuracy. Furthermore, the system successfully
detects two types of malleability-based attacks: one that destabilizes the
control system, and another that degrades its performance. The primary
contributions of this study are: (i) the implementation of a KHE-based
encrypted DOB-PID controller, (ii) the improvement of control performance under
uncertainties, and (iii) the experimental demonstration of attack detection
capabilities in encrypted control systems.

</details>


### [232] [Foundations for Energy-Aware Zero-Energy Devices: From Energy Sensing to Adaptive Protocols](https://arxiv.org/abs/2507.22740)
*Onel L. A. López,Mateen Ashraf,Samer Nasser,Gabriel M. de Jesus,Ritesh Kumar Singh,Miltiadis C. Filippou,Jeroen Famaey*

Main category: eess.SY

TL;DR: Designing effective energy-aware protocols for Zero-energy devices (ZEDs) requires realistic theoretical models, but current approaches oversimplify key aspects. This paper analyzes modeling components, trade-offs, and limitations, offering insights and future research directions for better ZED protocol design.


<details>
  <summary>Details</summary>
Motivation: Existing energy-aware protocol models for Zero-energy devices (ZEDs) often oversimplify crucial aspects like energy information acquisition, task-level variability, and energy storage dynamics, which limits their practical relevance and transferability. This paper aims to address this gap by offering a structured overview of the key modeling components, trade-offs, and limitations involved in designing energy-aware protocols for ZEDs.

Method: This article provides a structured overview by dissecting energy information acquisition methods and costs, characterizing core operational tasks, analyzing energy usage models and storage constraints, and reviewing representative protocol strategies. It offers design insights and guidelines, illustrated through selected in-house examples.

Result: The article offers a structured overview of modeling components, trade-offs, and limitations in energy-aware ZED protocol design. It dissects energy information acquisition, operational tasks, energy usage models, storage constraints, and protocol strategies, providing design insights and guidelines illustrated with examples. It also outlines future research directions.

Conclusion: ZEDs are crucial for sustainable IoT, but require intelligent, energy-aware protocols. Existing models oversimplify energy acquisition, task variability, and storage dynamics, limiting practical application. This article provides a structured overview of modeling components, trade-offs, and limitations in ZED protocol design, dissecting energy information acquisition, operational tasks, energy usage models, storage constraints, and protocol strategies. It also offers design insights and guidelines, using examples to illustrate how ZED protocols can utilize energy information. Finally, it highlights future research directions for more efficient and scalable ZED protocol solutions.

Abstract: Zero-energy devices (ZEDs) are key enablers of sustainable Internet of Things
networks by operating solely on harvested ambient energy. Their limited and
dynamic energy budget calls for protocols that are energy-aware and
intelligently adaptive. However, designing effective energy-aware protocols for
ZEDs requires theoretical models that realistically reflect device constraints.
Indeed, existing approaches often oversimplify key aspects such as energy
information (EI) acquisition, task-level variability, and energy storage
dynamics, limiting their practical relevance and transferability. This article
addresses this gap by offering a structured overview of the key modeling
components, trade-offs, and limitations involved in energy-aware ZED protocol
design. For this, we dissect EI acquisition methods and costs, characterize
core operational tasks, analyze energy usage models and storage constraints,
and review representative protocol strategies. Moreover, we offer design
insights and guidelines on how ZED operation protocols can leverage EI, often
illustrated through selected in-house examples. Finally, we outline key
research directions to inspire more efficient and scalable protocol solutions
for future ZEDs.

</details>


### [233] [Cluster Synchronization and Phase Cohesiveness of Kuramoto Oscillators via Mean-phase Feedback Control and Pacemakers](https://arxiv.org/abs/2507.22778)
*Ryota Kokubo,Rui Kato,Hideaki Ishii*

Main category: eess.SY

TL;DR: 本文提出并验证了两种控制大脑网络集群同步和集群相位凝聚的方法。


<details>
  <summary>Details</summary>
Motivation: 受大脑网络通常表现出的特征同步模式（其中共存几个同步集群）以及神经系统疾病与病理性同步（例如，大量神经元过度同步）有关的现象的启发。

Method: 本文提出两种控制集群同步和集群相位凝聚的方法：一种基于将平均相位反馈到集群，另一种基于使用起搏器。首先，展示了实现集群同步的反馈增益和起搏器权重的条件。然后，提出了一种通过凸优化寻找最优反馈增益的方法。其次，展示了实现集群相位凝聚的反馈增益和起搏器权重的条件。

Result: 数值示例证明了所提出方法（用于集群同步和集群相位凝聚）的有效性。

Conclusion: 神经系统疾病与病理性同步（例如，大量神经元过度同步）有关。本文提出两种控制 Kuramoto 振荡器集群同步和集群相位凝聚的方法：一种基于将平均相位反馈到集群，另一种基于使用起搏器。

Abstract: Brain networks typically exhibit characteristic synchronization patterns
where several synchronized clusters coexist. On the other hand, neurological
disorders are considered to be related to pathological synchronization such as
excessive synchronization of large populations of neurons. Motivated by these
phenomena, this paper presents two approaches to control the cluster
synchronization and the cluster phase cohesiveness of Kuramoto oscillators. One
is based on feeding back the mean phases to the clusters, and the other is
based on the use of pacemakers. First, we show conditions on the feedback gains
and the pacemaker weights for the network to achieve cluster synchronization.
Then, we propose a method to find optimal feedback gains through convex
optimization. Second, we show conditions on the feedback gains and the
pacemaker weights for the network to achieve cluster phase cohesiveness. A
numerical example demonstrates the effectiveness of the proposed methods.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [234] [Opinion formation in Wikipedia Ising networks](https://arxiv.org/abs/2507.22254)
*Leonardo Ermann,Klaus M. Frahm,Dima L. Shepelyansky*

Main category: cs.SI

TL;DR: Wikipedia Ising网络上的意见形成模型可以模拟和分析不同意见（如政治观点）的传播和竞争。


<details>
  <summary>Details</summary>
Motivation: 研究Wikipedia Ising网络上的意见形成特性，分析政治领导人、世界国家和社会概念之间的影响和竞争。

Method: 研究了Wikipedia Ising网络上的意见形成特性。将每个Wikipedia文章表示为一个节点，通过引用形成一个有向网络。在每个节点上放置Ising自旋，其方向由连接邻居的多数投票决定。采用异步蒙特卡洛过程模拟意见竞争，收敛于一个自旋极化稳态相。

Result: Ising网络意见形成模型可以分析政治领导人、世界国家和社会概念之间的影响和竞争，并推广到三组不同意见（例如，特朗普、普京、习近平或美国、俄罗斯、中国）的竞争。

Conclusion: 该模型为各种复杂网络中的意见形成提供了通用描述。

Abstract: We study properties of opinion formation
  on Wikipedia Ising Networks. Each Wikipedia article
  is represented as a node and links are formed by citations of
  one article to another generating a directed network
  of a given language edition with millions of nodes.
  Ising spins are placed at each node
  and their orientation up or down is determined by a majority vote
  of connected neighbors. At the initial stage there are only
  a few nodes from two groups with fixed competing opinions up and down
  while other nodes are assumed to have no initial opinion with no
  effect on the vote. The competition of two opinions is modeled by
  an asynchronous Monte Carlo process converging to a spin polarized
  steady-state phase.
  This phase remains stable with respect to small fluctuations
  induced by an effective temperature of the Monte Carlo process.
  The opinion polarization at the steady-state provides
  opinion (spin) preferences for each node. In the framework of
  this Ising Network
  Opinion Formation model we analyze the influence and competition between
  political leaders, world countries and social concepts.
  This approach is also generalized to the competition between
  three groups of
  different opinions described by three colors, for example
  Donald Trump, Vladimir Putin, Xi Jinping or USA, Russia, China
  within English, Russian and Chinese editions of Wikipedia of March 2025.
  We argue that this approach provides a generic description of
  opinion formation in various complex networks.

</details>


### [235] [Diffusion Models for Influence Maximization on Temporal Networks: A Guide to Make the Best Choice](https://arxiv.org/abs/2507.22589)
*Aaqib Zahoor,Iqra Altaf Gillani,Janibul Bashir*

Main category: cs.SI

TL;DR: This paper guides the selection of diffusion models for influence maximization in temporal networks by categorizing models, analyzing strategies, and comparing advancements to aid researchers and practitioners.


<details>
  <summary>Details</summary>
Motivation: Influence maximization is critical in temporal networks due to their increasing prominence in online social platforms and dynamic communication systems. Selecting the most suitable diffusion model for a given scenario remains challenging.

Method: The paper categorizes existing diffusion models based on their underlying mechanisms and assesses their effectiveness in different network settings. It analyzes seed selection strategies and highlights how the properties of influence spread enable efficient algorithms for finding influential nodes.

Result: The paper compares key advancements, challenges, and practical applications of diffusion models for influence maximization on temporal networks.

Conclusion: The paper provides a structured guide to selecting the most suitable diffusion model for influence maximization on temporal networks, analyzing seed selection strategies and offering a roadmap for researchers and practitioners.

Abstract: The increasing prominence of temporal networks in online social platforms and
dynamic communication systems has made influence maximization a critical
research area. Various diffusion models have been proposed to capture the
spread of information, yet selecting the most suitable model for a given
scenario remains challenging. This article provides a structured guide to
making the best choice among diffusion models for influence maximization on
temporal networks. We categorize existing models based on their underlying
mechanisms and assess their effectiveness in different network settings. We
analyze seed selection strategies, highlighting how the inherent properties of
influence spread enable the development of efficient algorithms that can find
near-optimal sets of influential nodes. By comparing key advancements,
challenges, and practical applications, we offer a comprehensive roadmap for
researchers and practitioners to navigate the landscape of temporal influence
maximization effectively.

</details>


### [236] [Human Mobility in Epidemic Modeling](https://arxiv.org/abs/2507.22799)
*Xin Lu,Jiawei Feng,Shengjie Lai,Petter Holme,Shuo Liu,Zhanwei Du,Xiaoqian Yuan,Siqing Wang,Yunxuan Li,Xiaoyu Zhang,Yuan Bai,Xiaojun Duan,Wenjun Mei,Hongjie Yu,Suoyi Tan,Fredrik Liljeros*

Main category: cs.SI

TL;DR: 人类流动性数据通过影响接触模式来塑造疾病传播。本综述探讨了如何将人类流动性数据整合到各种流行病模型中，以提高预测和应对能力。


<details>
  <summary>Details</summary>
Motivation: 认识到人类流动性是传染病传播接触模式的支柱，并对流行病时空动态具有根本性影响，而传统模型因其同质性混合假设而难以捕捉现实世界中复杂且异构的人类互动。

Method: 本综述全面概述了以人类流动性为信息来源的流行病学模型。文章探讨了人类流动性数据的多样化来源和表示方法，并考察了流动性和接触在塑造疾病传播动态中的行为和结构作用。此外，本综述广泛涵盖了从经典分隔模型到基于网络、基于主体和机器学习模型的各种流行病模型。

Result: 通过整合高分辨率人类流动性数据，流行病学模型的准确性、及时性和适用性得到了显著提高，尤其是在流行病风险评估、接触者追踪和干预策略方面。文章还讨论了流动性整合如何增强流行病期间的风险管理和应对策略。

Conclusion: 该综述作为流行病学理论与人类互动复杂性之间的桥梁，为研究人员和从业人员提供了基础资源，并为未来研究指明了方向。

Abstract: Human mobility forms the backbone of contact patterns through which
infectious diseases propagate, fundamentally shaping the spatio-temporal
dynamics of epidemics and pandemics. While traditional models are often based
on the assumption that all individuals have the same probability of infecting
every other individual in the population, a so-called random homogeneous
mixing, they struggle to capture the complex and heterogeneous nature of
real-world human interactions. Recent advancements in data-driven methodologies
and computational capabilities have unlocked the potential of integrating
high-resolution human mobility data into epidemic modeling, significantly
improving the accuracy, timeliness, and applicability of epidemic risk
assessment, contact tracing, and intervention strategies. This review provides
a comprehensive synthesis of the current landscape in human mobility-informed
epidemic modeling. We explore diverse sources and representations of human
mobility data, and then examine the behavioral and structural roles of mobility
and contact in shaping disease transmission dynamics. Furthermore, the review
spans a wide range of epidemic modeling approaches, ranging from classical
compartmental models to network-based, agent-based, and machine learning
models. And we also discuss how mobility integration enhances risk management
and response strategies during epidemics. By synthesizing these insights, the
review can serve as a foundational resource for researchers and practitioners,
bridging the gap between epidemiological theory and the dynamic complexities of
human interaction while charting clear directions for future research.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [237] [On the efficiency roll-off in Perovskite Light Emitting Diodes](https://arxiv.org/abs/2507.22741)
*Pradeep R Nair,Advaith Kiran Marathi*

Main category: physics.app-ph

TL;DR: 提出一个全面的建模框架，用于揭示钙钛矿发光二极管（PeLEDs）的效率滚降问题，并识别了限制性能的关键现象。


<details>
  <summary>Details</summary>
Motivation: 揭示钙钛矿发光二极管（PeLEDs）的效率滚降问题。

Method: 提出一个全面的建模框架，该框架能够自洽地解释涉及温度依赖性载流子复合、空间电荷效应、焦耳加热和热传输等多种现象的正反馈机制。

Result: 模型预测与暗电流-电压特性、效率以及高注入条件下的辐射滚降等实验结果具有良好的一致性。

Conclusion: 该研究提出了一个全面的建模框架，用于揭示钙钛矿发光二极管（PeLEDs）的效率滚降问题。模型能够自洽地解释涉及温度依赖性载流子复合、空间电荷效应、焦耳加热和热传输等多种现象的正反馈机制。模型预测与暗电流-电压特性、效率以及高注入条件下的辐射滚降等实验结果具有良好的一致性。这项工作识别了当前最先进的PeLEDs中限制性能的关键现象，并可能对器件优化、热设计、封装和运行寿命具有广泛的指导意义。

Abstract: Here, we report a comprehensive modeling framework to unravel the efficiency
roll-off in Perovskite light emitting diodes (PeLEDs). Our model
self-consistently accounts for a positive feedback mechanism which involves
diverse phenomena like temperature-dependent carrier recombination, space
charge effects, Joule heating, and thermal transport. Model predictions compare
well with experimental results such as dark current-voltage characteristics and
efficiency as well as radiance roll-off under high injection conditions. This
work identifies key performance limiting phenomena in current state-of-the-art
PeLEDs and could be of broad relevance towards device optimization, thermal
design, packaging, and operational lifetime.

</details>


### [238] [Ambiguity-Aware Segmented Estimation of Mutual Coupling in Large RIS: Algorithm and Experimental Validation](https://arxiv.org/abs/2507.22750)
*Philipp del Hougne*

Main category: physics.app-ph

TL;DR: 由于模型参数数量的增加，在优化 RIS 参数时，对 RIS 单元之间的相互耦合 (MC) 进行预估会带来可扩展性方面的挑战。为了解决这个问题，该研究提出了一种将完整的 MC 参数估计分解为三个顺序问题集的方法，并进行了实验验证，结果表明 MC 意识在优化 RIS 时性能提升有限，但能提高模型预测的可靠性。


<details>
  <summary>Details</summary>
Motivation: 需要对 RIS 单元之间的相互耦合 (MC) 进行预估，以优化由物理一致的多端口网络模型参数化的真实世界 RIS 无线信道。然而，MC 参数的数量随着 RIS 单元数量的平方而增长，这带来了可扩展性方面的挑战，并且由于不可避免的歧义，独立估计的 MC 矩阵片段难以拼接起来。

Method: 该方法将完整的 MC 参数估计问题分解为三个按顺序处理的小问题集：(1) 估计一组 RIS 单元的 MC 以及可用负载的特性；(2) 分别估计剩余组的 MC，其中每组都与一个已表征的组部分重叠；(3) 分别估计每个不同组对之间的 MC。

Result: 该算法在 4x4 MIMO 信道中进行了实验验证，该信道由富散射环境中的 100 单元 RIS 参数化。实验校准的多端口网络模型（包含 5867 个参数）的准确度为 40.5 dB，而具有有限或无 MC 意识的基准模型的准确度仅分别为 17.0 dB 和 13.8 dB。基于实验校准的模型，我们针对五项无线性能指标优化了 RIS。

Conclusion: 尽管感知到的相互耦合 (MC) 意识在提高 RIS 优化时的性能方面只带来适度的好处，但它会显着降低基于模型的预测的可靠性。

Abstract: Optimizing a real-life RIS-parametrized wireless channel with a
physics-consistent multiport-network model necessitates prior remote estimation
of the mutual coupling (MC) between RIS elements. The number of MC parameters
grows quadratically with the number of RIS elements, posing scalability
challenges. Because of inevitable ambiguities, independently estimated segments
of the MC matrix cannot be easily stitched together. Here, by carefully
handling the ambiguities, we achieve a separation of the full estimation
problem into three sequentially treated sets of smaller problems. We partition
the RIS elements into groups. First, we estimate the MC for one group as well
as the characteristics of the available loads. Second, we separately estimate
the MC for each of the remaining groups, in each case with partial overlap with
an already characterized group. Third, we separately estimate the MC between
each distinct pair of groups. Full parallelization is feasible within the
second and third sets of problems, and the third set of problems can
furthermore benefit from efficient initialization. We experimentally validate
our algorithm for a 4x4 MIMO channel parametrized by a 100-element RIS inside a
rich-scattering environment. Our experimentally calibrated 5867-parameter
multiport-network model achieves an accuracy of 40.5 dB, whereas benchmark
models with limited or no MC awareness only reach 17.0 dB and 13.8 dB,
respectively. Based on the experimentally calibrated models, we optimize the
RIS for five wireless performance indicators. Experimental measurements with
the optimized RIS configurations demonstrate only moderate benefits of MC
awareness in RIS optimization in terms of the achieved performance. However, we
observe that limited or no MC awareness markedly erodes the reliability of
model-based predictions of the expected performance.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [239] [Efficient handover based on Near-field and Far-field RIS for seamless connectivity](https://arxiv.org/abs/2507.22141)
*Atiquzzaman Mondal,Waheeb Tashan,Ayat Al-Olaimat,Hüseyin Arslan*

Main category: eess.SP

TL;DR: 本研究提出了一种基于RIS的切换管理方案，通过优化近场和远场通信，利用BER预测和RIS路径保留连接，有效减少了切换次数和信令开销，提高了6G网络的效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决未来6G通信网络中，利用可重构智能表面（RIS）来增强覆盖和连接性时，如何高效管理切换（HO）的问题。目标是减少信令开销，优化移动性管理，并提高频谱效率（SE）和能源效率（EE）。

Method: 本研究分析了RIS辅助网络中信号强度变化，特别是考虑了近场（NF）和远场（FF）区域。研究推导了NF区域中RIS-UE距离的概率密度函数（PDF），以量化RIS反射增益。在此基础上，提出了一种新的切换（HO）算法，该算法融合了硬切换（HHO）、软切换（SHO）、RIS辅助小区呼吸（RIS-CB）和RIS辅助乒乓效 Avoidance（RIS-PP）策略。该算法利用BER作为关键参数，通过RIS辅助路径保留与服务基站（BS）的连接，预测并最小化不必要的HO，从而减少目标BS搜索的需求。

Result: 数值结果表明，所提出的RIS辅助切换管理方案能够显著降低切换率和信令负载，确保6G系统的无缝连接和提升服务质量（QoS）。

Conclusion: 该研究提出了一种高效的切换管理方案，利用可重构智能表面（RIS）优化了移动性管理，减少了信令开销，并通过位错误率（BER）预测和RIS辅助路径保留连接，最小化了不必要的切换。研究结果表明，该方案能显著降低切换率和信令负载，提高频谱效率（SE）和能源效率（EE），尤其是在拥挤的蜂窝网络中，最终优化了切换过程并提升了服务质量（QoS）。

Abstract: Reconfigurable Intelligent Surfaces (RIS) is becoming a transformative
technology for the upcoming 6G communication networks, providing a way for
smartly maneuvering the electromagnetic waves to enhance coverage and
connectivity. This paper presents an efficient handover (HO) management scheme
leveraging RIS in the Fresnel region i.e., in both the near-field (NF) and
far-field (FF) regions to reduce signaling overhead and optimize mobility
management. For this, we analyzed the signal strength variations in the
considered RIS-aided networks, considering the radiative NF and FF regions, and
derive the probability density function (PDF) of the RIS-UE distance in the NF
region to quantify RIS reflection gains along the user equipment (UE)
trajectory. We propose a new HO algorithm incorporating several HO categories
like hard handover (HHO), soft handover (SHO), RIS-aided cell breathing
(RIS-CB), and RIS-aided ping-pong avoidance (RIS-PP) strategies. The proposed
algorithm uses bit error rate (BER) as a key parameter to predict the
minimization of unnecessary HOs by using RIS-aided pathways to retain
connectivity with the serving base station (BS), which minimizes the
requirement for frequent target BS searching and ultimately optimizes the HO.
By restricting measurement reports and HO requests, the suggested method
improves spectrum efficiency (SE) and energy efficiency (EE), especially in
crowded cellular networks. Numerical results highlight significant reductions
in HO rates and signaling load, ensuring seamless connectivity and improved
quality of service (QoS) in 6G systems.

</details>


### [240] [Deep Learning for Gradient and BCG Artifacts Removal in EEG During Simultaneous fMRI](https://arxiv.org/abs/2507.22263)
*K. A. Shahriar,E. H. Bhuiyan,Q. Luo,M. E. H. Chowdhury,X. J. Zhou*

Main category: eess.SP

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Simultaneous EEG-fMRI recording combines high temporal and spatial resolution
for tracking neural activity. However, its usefulness is greatly limited by
artifacts from magnetic resonance (MR), especially gradient artifacts (GA) and
ballistocardiogram (BCG) artifacts, which interfere with the EEG signal. To
address this issue, we used a denoising autoencoder (DAR), a deep learning
framework designed to reduce MR-related artifacts in EEG recordings. Using
paired data that includes both artifact-contaminated and MR-corrected EEG from
the CWL EEG-fMRI dataset, DAR uses a 1D convolutional autoencoder to learn a
direct mapping from noisy to clear signal segments. Compared to traditional
artifact removal methods like principal component analysis (PCA), independent
component analysis (ICA), average artifact subtraction (AAS), and wavelet
thresholding, DAR shows better performance. It achieves a root-mean-squared
error (RMSE) of 0.0218 $\pm$ 0.0152, a structural similarity index (SSIM) of
0.8885 $\pm$ 0.0913, and a signal-to-noise ratio (SNR) gain of 14.63 dB.
Statistical analysis with paired t-tests confirms that these improvements are
significant (p<0.001; Cohen's d>1.2). A leave-one-subject-out (LOSO)
cross-validation protocol shows that the model generalizes well, yielding an
average RMSE of 0.0635 $\pm$ 0.0110 and an SSIM of 0.6658 $\pm$ 0.0880 across
unseen subjects. Additionally, saliency-based visualizations demonstrate that
DAR highlights areas with dense artifacts, which makes its decisions easier to
interpret. Overall, these results position DAR as a potential and
understandable solution for real-time EEG artifact removal in simultaneous
EEG-fMRI applications.

</details>


### [241] [Robust Filtering and Learning in State-Space Models: Skewness and Heavy Tails Via Asymmetric Laplace Distribution](https://arxiv.org/abs/2507.22343)
*Yifan Yu,Shengjie Xiu,Daniel P. Palomar*

Main category: eess.SP

TL;DR: 一种新的状态空间模型，使用非对称拉普拉斯分布来处理异常值，更高效、更鲁棒。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统状态空间模型在处理偏态和重尾等非高斯分布的异常值数据时存在的局限性。

Method: 提出了一种基于非对称拉普拉斯分布的鲁棒状态空间模型扩展，并开发了一种高效的变分贝叶斯算法和新颖的单循环参数估计策略。

Result: 实验结果表明，该模型在各种噪声设置下均能提供一致的鲁棒性能，且无需手动调整超参数，计算资源消耗更少，优于现有模型。

Conclusion: 本文提出的基于非对称拉普拉斯分布的扩展状态空间模型在各种噪声环境下表现出持续的鲁棒性，且无需手动调整超参数，计算资源消耗更少，在鲁棒控制和金融建模等领域具有实际应用潜力。

Abstract: State-space models are pivotal for dynamic system analysis but often struggle
with outlier data that deviates from Gaussian distributions, frequently
exhibiting skewness and heavy tails. This paper introduces a robust extension
utilizing the asymmetric Laplace distribution, specifically tailored to capture
these complex characteristics. We propose an efficient variational Bayes
algorithm and a novel single-loop parameter estimation strategy, significantly
enhancing the efficiency of the filtering, smoothing, and parameter estimation
processes. Our comprehensive experiments demonstrate that our methods provide
consistently robust performance across various noise settings without the need
for manual hyperparameter adjustments. In stark contrast, existing models
generally rely on specific noise conditions and necessitate extensive manual
tuning. Moreover, our approach uses far fewer computational resources, thereby
validating the model's effectiveness and underscoring its potential for
practical applications in fields such as robust control and financial modeling.

</details>


### [242] [Green One-Bit Quantized Precoding in Cell-Free Massive MIMO](https://arxiv.org/abs/2507.22400)
*Salih Gümüsbuğa,Ozan Alp Topal,Özlem Tuğfe Demir*

Main category: eess.SP

TL;DR: 该论文提出了一种用于小区内大规模MIMO系统的量化预编码算法，通过动态停用不必要的Антенна来提高能源效率，与现有方法相比，该算法在降低功耗方面表现更优。


<details>
  <summary>Details</summary>
Motivation: 为了解决与 Антенна 相关的Антенна链功耗高的问题，人们提出了使用低分辨率Антенна数模和数模转换器（ADC/DAC）作为在 Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна Антенна А2/DACs：低分辨率ADC/DAC是降低功耗的关键。为了解决这个问题，我们提出了一种新颖的量化预编码算法，它通过动态地停用不必要的Антенна来提高能源效率。仿真结果表明，我们的算法优于现有的SQUID和RZF算法。

Method: 提出了一种新颖的量化预编码算法，该算法根据每个符号向量的结构动态地停用不必要的Антенна，从而提高能源效率。

Result: 仿真结果表明，所提出的量化预编码算法在提高能源效率方面优于现有方法，例如平方无穷范数 Douglas-Rachford 分裂 (SQUID) 和正则化零迫 (RZF)，同时有效降低功耗。

Conclusion: 所提出的量化预编码算法在提高能源效率方面优于现有方法，例如平方无穷范数 Douglas-Rachford 分裂 (SQUID) 和正则化零迫 (RZF)，同时有效降低功耗。

Abstract: Cell-free massive MIMO (multiple-input multiple-output) is expected to be one
of the key technologies in sixth-generation (6G) and beyond wireless
communications, offering enhanced spectral efficiency for cell-edge user
equipments by employing joint transmission and reception with a large number of
antennas distributed throughout the region. However, high-resolution RF chains
associated with these antennas significantly increase power consumption. To
address this issue, the use of low-resolution analog-to-digital and
digital-to-analog converters (ADCs/DACs) has emerged as a promising approach to
balance power efficiency and performance in massive MIMO networks. In this
work, we propose a novel quantized precoding algorithm tailored for cell-free
massive MIMO systems, where the proposed method dynamically deactivates
unnecessary antennas based on the structure of each symbol vector, thereby
enhancing energy efficiency. Simulation results demonstrate that our algorithm
outperforms existing methods such as squared-infinity norm Douglas-Rachford
splitting (SQUID) and regularized zero forcing (RZF), achieving superior
performance while effectively reducing power consumption.

</details>


### [243] [PINN and GNN-based RF Map Construction for Wireless Communication Systems](https://arxiv.org/abs/2507.22513)
*Lizhou Liu,Xiaohui Chen,Zihan Tang,Mengyao Ma,Wenyi Zhang*

Main category: eess.SP

TL;DR: 提出了一种结合PINN和GNN的新型射频地图构建方法，通过整合物理约束和空间相关性，在稀疏采样条件下实现了高精度和鲁棒性的射频地图构建。


<details>
  <summary>Details</summary>
Motivation: 射频（RF）地图是一种有前途的技术，用于捕获多径信号传播的特性，为无线通信网络的信道建模、覆盖分析和波束成形提供关键支持。

Method: 提出了一种结合物理信息神经网络（PINN）和图神经网络（GNN）的新型射频地图构建方法。PINN整合了电磁传播定律的物理约束来指导学习过程，而GNN则对接收器位置之间的空间相关性进行建模。通过将多径信号参数化为接收功率、延迟和到达角（AoA），并整合物理先验和空间依赖性，该方法能够精确预测多径参数。

Result: 实验结果表明，该方法在稀疏采样条件下实现了高精度的射频地图构建，并在室内和复杂的室外环境中都表现出鲁棒的性能，在泛化能力和准确性方面均优于基线方法。

Conclusion: 该方法在稀疏采样条件下实现了高精度射频地图构建，并在室内和复杂室外环境中均表现出鲁棒的性能，其泛化能力和准确性优于基线方法。

Abstract: Radio frequency (RF) map is a promising technique for capturing the
characteristics of multipath signal propagation, offering critical support for
channel modeling, coverage analysis, and beamforming in wireless communication
networks. This paper proposes a novel RF map construction method based on a
combination of physics-informed neural network (PINN) and graph neural network
(GNN). The PINN incorporates physical constraints derived from electromagnetic
propagation laws to guide the learning process, while the GNN models spatial
correlations among receiver locations. By parameterizing multipath signals into
received power, delay, and angle of arrival (AoA), and integrating both
physical priors and spatial dependencies, the proposed method achieves accurate
prediction of multipath parameters. Experimental results demonstrate that the
method enables high-precision RF map construction under sparse sampling
conditions and delivers robust performance in both indoor and complex outdoor
environments, outperforming baseline methods in terms of generalization and
accuracy.

</details>


### [244] [Exploration of Low-Cost but Accurate Radar-Based Human Motion Direction Determination](https://arxiv.org/abs/2507.22567)
*Weicheng Gao*

Main category: eess.SP

TL;DR: 提出了一种低成本、高精度的雷达人类运动方向确定方法，该方法结合了DTM、特征链接和ViT-CNN混合模型，可用于步态识别等任务。


<details>
  <summary>Details</summary>
Motivation: 为了改进基于DTM的方法在特征增强和运动确定方面的不足，并为步态识别等下游任务提供重要先验信息。

Method: 首先生成雷达雷达步态DTMs，然后使用特征链接模型实现特征增强，最后通过轻量级且快速的Vision Transformer-Convolutional Neural Network混合模型结构实现HMDD。

Result: 所提出的方法能够同时实现特征增强和运动确定，并验证了其有效性。

Conclusion: 该研究提出了一种低成本但精确的雷达人类运动方向确定（HMDD）方法，并使用开源数据集进行了有效性验证。

Abstract: This work is completed on a whim after discussions with my junior colleague.
The motion direction angle affects the micro-Doppler spectrum width, thus
determining the human motion direction can provide important prior information
for downstream tasks such as gait recognition. However, Doppler-Time map
(DTM)-based methods still have room for improvement in achieving feature
augmentation and motion determination simultaneously. In response, a low-cost
but accurate radar-based human motion direction determination (HMDD) method is
explored in this paper. In detail, the radar-based human gait DTMs are first
generated, and then the feature augmentation is achieved using feature linking
model. Subsequently, the HMDD is implemented through a lightweight and fast
Vision Transformer-Convolutional Neural Network hybrid model structure. The
effectiveness of the proposed method is verified through open-source dataset.
The open-source code of this work is released at:
https://github.com/JoeyBGOfficial/Low-Cost-Accurate-Radar-Based-Human-Motion-Direction-Determination.

</details>


### [245] [Fundamental Limits of Rigid Body Localization](https://arxiv.org/abs/2507.22573)
*Niclas Führling,Ivan Alexander Morales Sandoval,Giuseppe Thadeu Freitas de Abreu,Gonzalo Seco-Granados,David González G.,Osvaldo Gonsa*

Main category: eess.SP

TL;DR: 本文提出了一种新的CRLB框架，用于刚体定位问题，该框架考虑了平移和旋转，并提供了闭式表达式，数值结果表明该框架可以作为评估和改进现有RBL算法的工具。


<details>
  <summary>Details</summary>
Motivation: 本文提出了一种新颖的方法来构建刚体定位（RBL）问题的Cram'er-Rao下界（CRLB），旨在评估平移和旋转估计的根本精度限制。

Method: 本文采用以信息为中心的方法构建Fisher信息矩阵（FIM），以推导刚体定位（RBL）问题的Cram'er-Rao下界（CRLB），该方法能够捕捉每个测量值对FIM的贡献，包括输入测量类型和误差分布。通过这种方法，我们推导出一个通用的CRLB公式框架，适用于任何类型的刚体定位场景，并将传统的适用于点目标的FIM公式扩展到刚体的情况，其位置包括相对于参考物的平移矢量和旋转矩阵（或替代的旋转角度）。

Result: 得出了所有CRLB的闭式表达式，包括包含旋转矩阵的正则约束。数值结果表明，所导出的表达式正确地界定了通过各种相关最先进（SotA）估计器获得的估计定位参数的误差，揭示了它们的准确性，并表明最先进的RBL算法仍有改进空间。

Conclusion: 数值结果表明，所导出的表达式正确地界定了通过各种相关最先进（SotA）估计器获得的估计定位参数的误差，揭示了它们的准确性，并表明最先进的RBL算法仍有改进空间。

Abstract: We consider a novel approach to formulate the Cram\'er-Rao Lower Bound (CRLB)
for the rigid body localization (RBL) problem, which allows us to assess the
fundamental accuracy limits on the estimation of the translation and rotation
of a rigid body with respect to a known reference. To that end, we adopt an
information-centric construction of the Fisher information matrix (FIM), which
allows to capture the contribution of each measurement towards the FIM, both in
terms of input measurement types, as well as of their error distributions.
Taking advantage of this approach, we derive a generic framework for the CRLB
formulation, which is applicable to any type of rigid body localization
scenario, extending the conventional FIM formulation suitable for point targets
to the case of a rigid body whose location include both translation vector and
the rotation matrix (or alternative the rotation angles), with respect to a
reference. Closed-form expressions for all CRLBs are given, including the bound
incorporating an orthonormality constraint onto the rotation matrix. Numerical
results illustrate that the derived expression correctly lower-bounds the
errors of estimated localization parameters obtained via various related
state-of-the-art (SotA) estimators, revealing their accuracies and suggesting
that SotA RBL algorithms can still be improved.

</details>


### [246] [Measurement and Analysis of the Power Consumption of Hybrid-Amplified SCL-band Links](https://arxiv.org/abs/2507.22616)
*Ronit Sohanpal,Jiaqian Yang,Eric Sillekens,Henrique Buglia,Mingming Tan,Dini Pratiwi,Robert I. Killey,Polina Bayvel*

Main category: eess.SP

TL;DR: Hybrid Raman amplification reduces energy per bit in SCL-band links by up to 26% compared to lumped amplification.


<details>
  <summary>Details</summary>
Motivation: To investigate the power consumption of hybrid-amplified SCL-band links and compare it with lumped amplification.

Method: Studied the power consumption of hybrid-amplified SCL-band links using commercial benchtop amplifiers and Raman pumps.

Result: Demonstrated a reduction in energy per bit for multi-span hybrid Raman amplified links.

Conclusion: In multi-span hybrid Raman amplified links, energy per bit can be reduced by up to 26% compared to lumped amplification.

Abstract: We studied the power consumption of hybrid-amplified SCL-band links using
commercial benchtop amplifiers and Raman pumps. We show a reduction in energy
per bit for multi-span hybrid Raman amplified links of up to 26% versus lumped
amplification.

</details>


### [247] [A Multi-Scale Spatial Attention Network for Near-field MIMO Channel Estimation](https://arxiv.org/abs/2507.22656)
*Zhiming Zhu,Shu Xu,Jiexin Zhang,Chunguo Li,Yongming Huang,Luxi Yang*

Main category: eess.SP

TL;DR: This paper proposes a new deep learning method (MsSAN) for near-field channel estimation in extremely large-scale MIMO systems, outperforming existing methods by effectively learning inter-subchannel correlations using a spatial attention mechanism.


<details>
  <summary>Details</summary>
Motivation: Existing near-field channel estimation schemes exploit sparsity in the transform domain, but are sensitive to transform matrix selection and stopping criteria. Extremely large-scale array (ELAA) brings higher spectral efficiency and spatial degree of freedom, but triggers issues on near-field channel estimation.

Method: A novel spatial-attention-based method using a multi-scale spatial attention network (MsSAN) is proposed. It analyzes spatial antenna correlations as an expectation over the angle-distance space and utilizes inter-subchannel interactions to describe inherent correlation. The SA module uses the sum of dot products as spatial attention to weight subchannel features at different scales.

Result: Simulation results validate the proposed MsSAN's effectiveness.

Conclusion: Proposed MsSAN achieves remarkable inter-subchannel correlation learning capabilities and outperforms others in terms of near-field channel reconstruction.

Abstract: The deployment of extremely large-scale array (ELAA) brings higher spectral
efficiency and spatial degree of freedom, but triggers issues on near-field
channel estimation.
  Existing near-field channel estimation schemes primarily exploit sparsity in
the transform domain.
  However, these schemes are sensitive to the transform matrix selection and
the stopping criteria.
  Inspired by the success of deep learning (DL) in far-field channel
estimation, this paper proposes a novel spatial-attention-based method for
reconstructing extremely large-scale MIMO (XL-MIMO) channel.
  Initially, the spatial antenna correlations of near-field channels are
analyzed as an expectation over the angle-distance space, which demonstrate
correlation range of an antenna element varies with its position.
  Due to the strong correlation between adjacent antenna elements, interactions
of inter-subchannel are applied to describe inherent correlation of near-field
channels instead of inter-element.
  Subsequently, a multi-scale spatial attention network (MsSAN) with the
inter-subchannel correlation learning capabilities is proposed tailed to
near-field MIMO channel estimation.
  We employ the multi-scale architecture to refine the subchannel size in
MsSAN.
  Specially, we inventively introduce the sum of dot products as spatial
attention (SA) instead of cross-covariance to weight subchannel features at
different scales in the SA module.
  Simulation results are presented to validate the proposed MsSAN achieves
remarkable the inter-subchannel correlation learning capabilities and
outperforms others in terms of near-field channel reconstruction.

</details>


### [248] [Compressive Near-Field Wideband Channel Estimation for THz Extremely Large-scale MIMO Systems](https://arxiv.org/abs/2507.22727)
*Jionghui Wang,Hongwei Wang,Jun Fang,Lingxiang Li,Zhi Chen*

Main category: eess.SP

TL;DR: 提出了一种新的二维块稀疏感知方法，用于太赫兹通信系统中的宽带近场信道估计，并在仿真中证明了其优越性。


<details>
  <summary>Details</summary>
Motivation: 为了缓解太赫兹通信系统中严重的路径衰减，部署了超大规模阵列来解决信道获取问题，并考虑了近场球形波前和宽带波束分裂现象。

Method: 提出一种与频率无关的正交字典，该字典通过引入额外的参数来捕获近场特性，从而实现宽带近场信道的二维块稀疏表示。利用此块稀疏结构，通过定制的压缩感知框架来解决宽带近场信道估计问题。

Result: 数值结果表明，与传统的基于极坐标的方法相比，所提出的二维块稀疏感知方法在宽带近场信道估计方面具有显著优势。

Conclusion: 提出的二维块稀疏感知框架在宽带近场信道估计方面优于传统的极坐标方法。

Abstract: We consider the channel acquisition problem for a wideband terahertz (THz)
communication system, where an extremely large-scale array is deployed to
mitigate severe path attenuation. In channel modeling, we account for both the
near-field spherical wavefront and the wideband beam-splitting phenomena,
resulting in a wideband near-field channel. We propose a frequency-independent
orthogonal dictionary that generalizes the standard discrete Fourier transform
(DFT) matrix by introducing an additional parameter to capture the near-field
property. This dictionary enables the wideband near-field channel to be
efficiently represented with a two-dimensional (2D) block-sparse structure.
Leveraging this specific sparse structure, the wideband near-field channel
estimation problem can be effectively addressed within a customized compressive
sensing framework. Numerical results demonstrate the significant advantages of
our proposed 2D block-sparsity-aware method over conventional
polar-domain-based approaches for near-field wideband channel estimation.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [249] [Infinite Traces by Finality: a Sheaf-Theoretic Approach](https://arxiv.org/abs/2507.22536)
*Marco Peressotti*

Main category: cs.LO

TL;DR: 该研究提出了一个结合 Kleisli 范畴和叶状理论的框架，用于在 Kleisli 范畴中实现无限轨迹语义，并通过受限行为函子捕获无限轨迹。


<details>
  <summary>Details</summary>
Motivation: 现有的 Kleisli 范畴虽然能模拟系统的线性行为，但其最终余代数通常不直接对应于线性语义的固定概念，尤其是在捕获无限轨迹语义方面存在困难。

Method: 本研究结合了 Kleisli 范畴、有序数上的叶状结构以及受限（余）递归，通过그런(amalgamation)将有限逼近的相干族转化为无限行为。

Result: 研究介绍了受限行为函子的概念，并证明在温和条件下，它们的最终余代数能够直接表征无限轨迹。

Conclusion: 本研究提出了一个基于叶状理论的框架，用于在 Kleisli 范畴中实现无限轨迹语义，该框架能够系统地构造捕获无限轨迹的最终余代数。

Abstract: Kleisli categories have long been recognised as a setting for modelling the
linear behaviour of various types of systems. However, the final coalgebra in
such settings does not, in general, correspond to a fixed notion of linear
semantics. While there are well-understood conditions under which final
coalgebras capture finite trace semantics, a general account of infinite trace
semantics via finality has remained elusive. In this work, we present a
sheaf-theoretic framework for infinite trace semantics in Kleisli categories
that systematically constructs final coalgebras capturing infinite traces. Our
approach combines Kleisli categories, sheaves over ordinals, and guarded
(co)recursion, enabling infinite behaviours to emerge from coherent families of
finite approximations via amalgamation. We introduce the notion of guarded
behavioural functor and show that, under mild conditions, their final
coalgebras directly characterise infinite traces.

</details>


### [250] [Concrete Security Bounds for Simulation-Based Proofs of Multi-Party Computation Protocols](https://arxiv.org/abs/2507.22705)
*Kristina Sojakova,Mihai Codescu,Joshua Gancher*

Main category: cs.LO

TL;DR: 本研究提出了一种新方法，用于自动计算多方计算（MPC）协议的具体安全界限，并通过在Maude中实现该方法，大大简化了证明的复杂性，例如，GMW MPC协议的证明规模减小了72%。


<details>
  <summary>Details</summary>
Motivation: 为了弥合在为多方计算（MPC）协议获得令人满意的具体安全界限方面的差距，以及处理需要考虑密码学对手和归约的运行时间的挑战。

Method: 借鉴了IPDL的元理论，并在Maude中实现了该方法，以支持对协议运行时间和对抗性优势的推理。

Result: 成功计算了多方计算（MPC）协议的具体安全界限，并完成了GMW MPC协议（适用于N个参与方）的首次正式验证，其安全界限的风格类似于通用可组合性。

Conclusion: 该研究提出了一种新的基础方法，可以自动计算多方计算（MPC）协议的具体安全界限，并实现了首个具有具体安全界限的GMW MPC协议的正式验证。

Abstract: The concrete security paradigm aims to give precise bounds on the probability
that an adversary can subvert a cryptographic mechanism. This is in contrast to
asymptotic security, where the probability of subversion may be eventually
small, but large enough in practice to be insecure. Fully satisfactory concrete
security bounds for Multi-Party Computation (MPC) protocols are difficult to
attain, as they require reasoning about the running time of cryptographic
adversaries and reductions. In this paper we close this gap by introducing a
new foundational approach that allows us to automatically compute concrete
security bounds for MPC protocols. We take inspiration from the meta-theory of
IPDL, a prior approach for formally verified distributed cryptography, to
support reasoning about the runtime of protocols and adversarial advantage. For
practical proof developments, we implement our approach in Maude, an extensible
logic for equational rewriting. We carry out four case studies of concrete
security for simulation-based proofs. Most notably, we deliver the first formal
verification of the GMW MPC protocol over N parties. To our knowledge, this is
the first time that formally verified concrete security bounds are computed for
a proof of an MPC protocol in the style of Universal Composability. Our tool
provides a layer of abstraction that allows the user to write proofs at a high
level, which drastically simplifies the proof size. For comparison, a case
study that in prior works required 2019 LoC only takes 567 LoC, thus reducing
proof size by 72%

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [251] [From Cloud-Native to Trust-Native: A Protocol for Verifiable Multi-Agent Systems](https://arxiv.org/abs/2507.22077)
*Muyang Li*

Main category: cs.MA

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: As autonomous agents powered by large language models (LLMs) proliferate in
high-stakes domains -- from pharmaceuticals to legal workflows -- the challenge
is no longer just intelligence, but verifiability. We introduce TrustTrack, a
protocol that embeds structural guarantees -- verifiable identity, policy
commitments, and tamper-resistant behavioral logs -- directly into agent
infrastructure. This enables a new systems paradigm: trust-native autonomy. By
treating compliance as a design constraint rather than post-hoc oversight,
TrustTrack reframes how intelligent agents operate across organizations and
jurisdictions. We present the protocol design, system requirements, and use
cases in regulated domains such as pharmaceutical R&D, legal automation, and
AI-native collaboration. We argue that the Cloud -> AI -> Agent -> Trust
transition represents the next architectural layer for autonomous systems.

</details>


### [252] [Successor Features for Transfer in Alternating Markov Games](https://arxiv.org/abs/2507.22278)
*Sunny Amatya,Yi Ren,Zhe Xu,Wenlong Zhang*

Main category: cs.MA

TL;DR: 本研究将后继特征应用于马尔可夫博弈，并提出GGPI算法，实现了跨游戏知识迁移，并在追逐者-逃跑者游戏中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 为了解决传统游戏知识迁移方法（依赖值和均衡迁移）在任务相似度不高时可能失败的问题，本研究将后继特征应用于游戏，并提出GGPI算法。

Method: 提出了一种名为GGPI（Game Generalized Policy Improvement）的新算法，并将后继特征应用于马尔可夫博弈，以解决多智能体强化学习中的知识迁移问题。

Result: 通过在回合制追逐者-逃跑者游戏中进行实验，证明了GGPI算法能够产生高回报的交互和一次性策略迁移。在更广泛的初始条件下测试，GGPI算法相比基线算法具有更高的成功率和改进的路径效率。

Conclusion: 该研究提出的GGPI算法能够实现学习值和策略在不同游戏间的迁移，并通过实验证明了其在提高奖励和策略迁移效率方面的优越性。

Abstract: This paper explores successor features for knowledge transfer in zero-sum,
complete-information, and turn-based games. Prior research in single-agent
systems has shown that successor features can provide a ``jump start" for
agents when facing new tasks with varying reward structures. However, knowledge
transfer in games typically relies on value and equilibrium transfers, which
heavily depends on the similarity between tasks. This reliance can lead to
failures when the tasks differ significantly. To address this issue, this paper
presents an application of successor features to games and presents a novel
algorithm called Game Generalized Policy Improvement (GGPI), designed to
address Markov games in multi-agent reinforcement learning. The proposed
algorithm enables the transfer of learning values and policies across games. An
upper bound of the errors for transfer is derived as a function the similarity
of the task. Through experiments with a turn-based pursuer-evader game, we
demonstrate that the GGPI algorithm can generate high-reward interactions and
one-shot policy transfer. When further tested in a wider set of initial
conditions, the GGPI algorithm achieves higher success rates with improved path
efficiency compared to those of the baseline algorithms.

</details>


### [253] [Physics-Informed EvolveGCN: Satellite Prediction for Multi Agent Systems](https://arxiv.org/abs/2507.22279)
*Timothy Jacob Huber,Madhur Tiwari,Camilo A. Riano-Rios*

Main category: cs.MA

TL;DR: 通过使用动态图卷积网络（EvolveGCN）和物理约束损失函数，提高了自主系统中智能体间位置预测的准确性和物理合理性。


<details>
  <summary>Details</summary>
Motivation: 在自主系统中，智能体间的交互对于提升系统整体能力至关重要，需要可靠地预测邻近智能体未来的位置。

Method: 利用EvolveGCN（一种动态图卷积网络）来预测智能体间关系随时间的演变，并结合基于Clohessy-Wiltshire方程的物理约束损失函数以提高预测精度和物理合理性。

Result: 所提出的方法提高了多智能体场景下未来状态估计的可靠性。

Conclusion: 该方法通过结合动态图卷积网络（EvolveGCN）和基于Clohessy-Wiltshire方程的物理约束损失函数，提高了多智能体系统中邻近智能体未来位置预测的准确性和物理合理性。

Abstract: In the rapidly evolving domain of autonomous systems, interaction among
agents within a shared environment is both inevitable and essential for
enhancing overall system capabilities. A key requirement in such multi-agent
systems is the ability of each agent to reliably predict the future positions
of its nearest neighbors. Traditionally, graphs and graph theory have served as
effective tools for modeling inter agent communication and relationships. While
this approach is widely used, the present work proposes a novel method that
leverages dynamic graphs in a forward looking manner. Specifically, the
employment of EvolveGCN, a dynamic graph convolutional network, to forecast the
evolution of inter-agent relationships over time. To improve prediction
accuracy and ensure physical plausibility, this research incorporates physics
constrained loss functions based on the Clohessy-Wiltshire equations of motion.
This integrated approach enhances the reliability of future state estimations
in multi-agent scenarios.

</details>


### [254] [Multi-Agent Path Finding Among Dynamic Uncontrollable Agents with Statistical Safety Guarantees](https://arxiv.org/abs/2507.22282)
*Kegan J. Strawn,Thomy Phan,Eric Wang,Nora Ayanian,Sven Koenig,Lars Lindemann*

Main category: cs.MA

TL;DR: 提出一种名为 CP-Solver 的新方法，用于解决多智能体路径查找（MAPF）问题，该问题涉及具有不确定行为的不可控智能体。该方法结合了预测模型、共形预测（CP）和改进的 ECBS 算法，以处理动态环境和不确定性，并在各种地图上展示了有竞争力的性能和更少的碰撞。


<details>
  <summary>Details</summary>
Motivation: 现有的多智能体路径查找（MAPF）求解器未能考虑不可控智能体的行为不确定性。

Method: ECBS 算法的变体，包含三个主要部分：1. 训练一个用于预测不可控智能体移动的预测模型；2. 使用共形预测（CP）量化预测误差；3. 将这些不确定性区间集成到改进的 ECBS 求解器中。

Result: CP-Solver 能够处理行为不确定的智能体，为一次性任务提供无碰撞路径的统计保证，并通过回溯视界序列的一次性实例适应终身任务。

Conclusion: CP-Solver 算法在仓库和游戏地图上的表现具有竞争力，并且能够减少碰撞。

Abstract: Existing multi-agent path finding (MAPF) solvers do not account for uncertain
behavior of uncontrollable agents. We present a novel variant of Enhanced
Conflict-Based Search (ECBS), for both one-shot and lifelong MAPF in dynamic
environments with uncontrollable agents. Our method consists of (1) training a
learned predictor for the movement of uncontrollable agents, (2) quantifying
the prediction error using conformal prediction (CP), a tool for statistical
uncertainty quantification, and (3) integrating these uncertainty intervals
into our modified ECBS solver. Our method can account for uncertain agent
behavior, comes with statistical guarantees on collision-free paths for
one-shot missions, and scales to lifelong missions with a receding horizon
sequence of one-shot instances. We run our algorithm, CP-Solver, across
warehouse and game maps, with competitive throughput and reduced collisions.

</details>


### [255] [Towards Simulating Social Influence Dynamics with LLM-based Multi-agents](https://arxiv.org/abs/2507.22467)
*Hsien-Tsung Lin,Pei-Cing Huang,Chan-Tung Ku,Chan Hsu,Pei-Xuan Shieh,Yihuang Kang*

Main category: cs.MA

TL;DR: LLM模拟显示，较小模型更容易从众，而更擅长推理的模型则更能抵抗社会影响。


<details>
  <summary>Details</summary>
Motivation: 研究LLM基的多主体模拟是否能重现网络论坛中的核心人类社会动态。

Method: 通过结构化模拟框架，评估了不同模型规模和推理能力下的从众动态、群体极化和群体分裂。

Result: 较小模型表现出更高的从众率，而优化了推理能力时模型更能抵抗社会影响。

Conclusion: LLM基的多主体模拟可以重现网络论坛中的核心人类社会动态。较小模型表现出更高的从众率，而优化了推理能力时模型更能抵抗社会影响。

Abstract: Recent advancements in Large Language Models offer promising capabilities to
simulate complex human social interactions. We investigate whether LLM-based
multi-agent simulations can reproduce core human social dynamics observed in
online forums. We evaluate conformity dynamics, group polarization, and
fragmentation across different model scales and reasoning capabilities using a
structured simulation framework. Our findings indicate that smaller models
exhibit higher conformity rates, whereas models optimized for reasoning are
more resistant to social influence.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [256] [Reducing the complexity of computing the values of a Nash equilibrium](https://arxiv.org/abs/2507.22819)
*Debtoru Chatterjee,Girish Tiwari,Niladri Chatterjee*

Main category: cs.GT

TL;DR: 提出了一种计算 Colonel Blotto 游戏和两人零和博弈纳什均衡价值的新算法，复杂度更低，不依赖不动点定理。


<details>
  <summary>Details</summary>
Motivation: 为了解决 Colonel Blotto 游戏及其所有两人零和博弈中，由于 PPAD 完全性导致的纳什均衡计算的可扩展性和复杂性的问题。

Method: 提出了一种新的算法来计算博弈的纳什均衡价值，该算法不依赖于不动点定理，并且通过避免计算双方的纳什策略来降低复杂度。

Result: 该算法计算出的纳什均衡价值与现有方法相同，但具有更低的计算复杂度，并且可以扩展到更广泛的博弈类型。

Conclusion: 该算法能够计算出纳什均衡的相同价值，但不能用不动点定理来表征，并且其复杂度降低了，因为它不需要计算双方的纳什策略。该算法还可以扩展到所有两人零和博弈，以计算纳什均衡的价值。

Abstract: The Colonel Blotto game, formulated by Emile Borel, involves players
allocating limited resources to multiple battlefields simultaneously, with the
winner being the one who allocates more resources to each battlefield.
Computation of the Nash equilibrium, including of two person, zero sum, mixed
strategy Colonel Blotto games have encountered issues of scalability and
complexity owing to their PPAD completeness. This paper proposes an algorithm
that computes the same value as the Nash equilibrium but cannot be
characterized by the Fixed point Theorems of Tarski, Kakutani and Brouwer. The
reduced complexity of the proposed algorithm is based on dispensing with the
need for computing both players Nash strategies in Colonel Blotto games. The
same algorithm can, therefore, be extended to all two person, zero sum games to
compute the value of the Nash equilibrium. The theoretical superiority of the
proposed algorithm over both LP solvers and another method that computes the
same value of the game as its Nash equilibrium by a random assignment of
probabilities to the active strategy set of the defending player, is also
proposed.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [257] [When Truthful Representations Flip Under Deceptive Instructions?](https://arxiv.org/abs/2507.22149)
*Xianxuan Long,Yao Fu,Runchao Li,Mu Sheng,Haotian Yu,Xiaotian Han,Pan Li*

Main category: cs.AI

TL;DR: 分析LLM在欺骗性指令下的内部表征变化，发现欺骗性指令引起模型早期到中期层的表征变化，可通过SAEs检测并识别特定特征。


<details>
  <summary>Details</summary>
Motivation: 为了理解欺骗性指令如何改变大型语言模型（LLM）的内部表征，以及这种改变与真实性指令有何不同。

Method: 通过分析Llama-3.1-8B-Instruct和Gemma-2-9B-Instruct在欺骗性指令下的内部表征变化，并使用稀疏自动编码器（SAEs）和线性探针技术来量化和理解这些变化。

Result: 欺骗性指令相较于真实性/中性指令，会在模型的早期到中期层引起显著的内部表征变化，这种变化可以通过SAEs检测到。研究还发现了能够区分真实与欺骗性表征的特定SAE特征。

Conclusion: 该分析指出，欺骗性指令会引起模型内部表示的显著变化，尤其是在早期到中期层，并且这种变化可以通过稀疏自动编码器（SAEs）检测到。研究还识别了对欺骗性指令敏感的特定SAE特征，并通过可视化确认了不同的真实/欺骗性表示子空间。

Abstract: Large language models (LLMs) tend to follow maliciously crafted instructions
to generate deceptive responses, posing safety challenges. How deceptive
instructions alter the internal representations of LLM compared to truthful
ones remains poorly understood beyond output analysis. To bridge this gap, we
investigate when and how these representations ``flip'', such as from truthful
to deceptive, under deceptive versus truthful/neutral instructions. Analyzing
the internal representations of Llama-3.1-8B-Instruct and Gemma-2-9B-Instruct
on a factual verification task, we find the model's instructed True/False
output is predictable via linear probes across all conditions based on the
internal representation. Further, we use Sparse Autoencoders (SAEs) to show
that the Deceptive instructions induce significant representational shifts
compared to Truthful/Neutral representations (which are similar), concentrated
in early-to-mid layers and detectable even on complex datasets. We also
identify specific SAE features highly sensitive to deceptive instruction and
use targeted visualizations to confirm distinct truthful/deceptive
representational subspaces. % Our analysis pinpoints layer-wise and
feature-level correlates of instructed dishonesty, offering insights for LLM
detection and control. Our findings expose feature- and layer-level signatures
of deception, offering new insights for detecting and mitigating instructed
dishonesty in LLMs.

</details>


### [258] [Explainability Through Systematicity: The Hard Systematicity Challenge for Artificial Intelligence](https://arxiv.org/abs/2507.22197)
*Matthieu Queloz*

Main category: cs.AI

TL;DR: 本篇论文认为，人工智能的系统性比以往认为的更重要，并提出了一个应对“硬系统性挑战”的新框架。


<details>
  <summary>Details</summary>
Motivation: 解释了可解释性只是影响我们对人工智能期望的更广泛的理想的一个方面，而系统性是AI的核心品质，包括一致性、连贯性、全面性和简洁性原则。

Method: 提出了一个概念框架来区分“思想系统性”的四种含义，并应用这些区分来分析AI模型的系统性。

Result: 确定了五个系统化理由，并将它们应用于AI，从而提出了“硬系统性挑战”，并得出了一个动态的系统性需求理解，明确了AI模型需要和何时需要系统化。

Conclusion: 该研究通过区分“思想系统性”的四种含义，消除了系统性与连接主义之间的紧张关系，并提出了一个更严格的系统性概念，认为其比Fodorian的概念更具要求性。

Abstract: This paper argues that explainability is only one facet of a broader ideal
that shapes our expectations towards artificial intelligence (AI).
Fundamentally, the issue is to what extent AI exhibits systematicity--not
merely in being sensitive to how thoughts are composed of recombinable
constituents, but in striving towards an integrated body of thought that is
consistent, coherent, comprehensive, and parsimoniously principled. This richer
conception of systematicity has been obscured by the long shadow of the
"systematicity challenge" to connectionism, according to which network
architectures are fundamentally at odds with what Fodor and colleagues termed
"the systematicity of thought." I offer a conceptual framework for thinking
about "the systematicity of thought" that distinguishes four senses of the
phrase. I use these distinctions to defuse the perceived tension between
systematicity and connectionism and show that the conception of systematicity
that historically shaped our sense of what makes thought rational,
authoritative, and scientific is more demanding than the Fodorian notion. To
determine whether we have reason to hold AI models to this ideal of
systematicity, I then argue, we must look to the rationales for systematization
and explore to what extent they transfer to AI models. I identify five such
rationales and apply them to AI. This brings into view the "hard systematicity
challenge." However, the demand for systematization itself needs to be
regulated by the rationales for systematization. This yields a dynamic
understanding of the need to systematize thought, which tells us how systematic
we need AI models to be and when.

</details>


### [259] [Nearest-Better Network for Visualizing and Analyzing Combinatorial Optimization Problems: A Unified Tool](https://arxiv.org/abs/2507.22440)
*Yiya Diao,Changhe Li,Sanyou Zeng,Xinye Cai,Wenjian Luo,Shengxiang Yang,Carlos A. Coello Coello*

Main category: cs.AI

TL;DR: NBN计算方法得以优化，并成功应用于OneMax和TSP问题分析，揭示了问题特性及现有算法的不足。


<details>
  <summary>Details</summary>
Motivation: 可视化连续优化问题的采样数据，保存多种景观特征，但现有方法计算耗时，且难以应用于组合优化问题。

Method: 提出了一种高效的NBN计算方法，具有对数线性时间复杂度，并将其应用于OneMax和TSP问题。

Result: 证明了NBN本质上是最大概率转移网络；提出了一种高效的NBN计算方法；揭示了OneMax问题的中立性、崎岖性和模态特征；指出了TSP问题的崎岖性、模态和欺骗性是主要挑战；发现EAX和LKH算法在处理模态和欺骗性挑战方面存在局限性。

Conclusion: 该研究为NBN提供了理论基础，并提出了一种高效的计算方法，揭示了OneMax和TSP问题的景观特征及其对算法的影响，并指出了现有TSP算法（EAX和LKH）的局限性。

Abstract: The Nearest-Better Network (NBN) is a powerful method to visualize sampled
data for continuous optimization problems while preserving multiple landscape
features. However, the calculation of NBN is very time-consuming, and the
extension of the method to combinatorial optimization problems is challenging
but very important for analyzing the algorithm's behavior. This paper provides
a straightforward theoretical derivation showing that the NBN network
essentially functions as the maximum probability transition network for
algorithms. This paper also presents an efficient NBN computation method with
logarithmic linear time complexity to address the time-consuming issue. By
applying this efficient NBN algorithm to the OneMax problem and the Traveling
Salesman Problem (TSP), we have made several remarkable discoveries for the
first time: The fitness landscape of OneMax exhibits neutrality, ruggedness,
and modality features. The primary challenges of TSP problems are ruggedness,
modality, and deception. Two state-of-the-art TSP algorithms (i.e., EAX and
LKH) have limitations when addressing challenges related to modality and
deception, respectively. LKH, based on local search operators, fails when there
are deceptive solutions near global optima. EAX, which is based on a single
population, can efficiently maintain diversity. However, when multiple
attraction basins exist, EAX retains individuals within multiple basins
simultaneously, reducing inter-basin interaction efficiency and leading to
algorithm's stagnation.

</details>


### [260] [CoEx -- Co-evolving World-model and Exploration](https://arxiv.org/abs/2507.22281)
*Minsoo Kim,Seung-won Hwang*

Main category: cs.AI

TL;DR: CoEx agent通过分层架构和动态世界模型更新，解决了现有LLM agent在规划中与世界状态失调的问题，并在多项任务中表现优于其他agent。


<details>
  <summary>Details</summary>
Motivation: 现有agent设计未能有效地将新观察结果纳入世界模型的动态更新中，对LLM的静态内部世界模型的依赖导致其与世界真实状态的偏差日益增大，从而产生不同的和错误的计划。

Method: 提出了一种名为CoEx的分层agent架构，其中分层状态抽象使得LLM规划能够与动态更新的世界模型共同进化。CoEx利用LLM推理来编排由子目标组成的动态计划，并与世界进行交互，其学习机制则将这些子目标经验持续地纳入一个持久的世界模型中，该模型以神经符号信念状态的形式存在，包含文本推理和基于代码的符号记忆。

Result: 在ALFWorld、PDDL和Jericho等涉及丰富环境和复杂任务的多种agent场景中评估了CoEx agent。实验结果表明，CoEx在规划和探索方面优于现有的agent范式。

Conclusion: CoEx通过分层状态抽象，使LLM规划与动态更新的世界模型共同进化，并通过将子目标经验纳入由文本推理和基于代码的符号记忆组成的神经符号信念状态，持续优化世界模型，从而在规划和探索方面优于现有agent范式。

Abstract: Planning in modern LLM agents relies on the utilization of LLM as an internal
world model, acquired during pretraining. However, existing agent designs fail
to effectively assimilate new observations into dynamic updates of the world
model. This reliance on the LLM's static internal world model is progressively
prone to misalignment with the underlying true state of the world, leading to
the generation of divergent and erroneous plans. We introduce a hierarchical
agent architecture, CoEx, in which hierarchical state abstraction allows LLM
planning to co-evolve with a dynamically updated model of the world. CoEx plans
and interacts with the world by using LLM reasoning to orchestrate dynamic
plans consisting of subgoals, and its learning mechanism continuously
incorporates these subgoal experiences into a persistent world model in the
form of a neurosymbolic belief state, comprising textual inferences and
code-based symbolic memory. We evaluate our agent across a diverse set of agent
scenarios involving rich environments and complex tasks including ALFWorld,
PDDL, and Jericho. Our experiments show that CoEx outperforms existing agent
paradigms in planning and exploration.

</details>


### [261] [Automatically discovering heuristics in a complex SAT solver with large language models](https://arxiv.org/abs/2507.22876)
*Yiwen Sun,Furong Ye,Zhihan Chen,Ke Wei,Shaowei Cai*

Main category: cs.AI

TL;DR: AutoModSAT利用LLMs优化SAT求解器，通过模块化求解器、自动提示优化和高效搜索策略，实现了比SOTA求解器更高的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 在计算复杂性领域，可满足性问题（SAT）具有重要地位，但现代SAT求解器在真实世界的应用中因其复杂的架构而难以优化。现有的自动配置框架依赖于手动约束的搜索空间，性能提升有限。

Method: 本研究引入了一种利用大型语言模型（LLMs）优化复杂SAT求解器的新范式，并通过开发名为AutoModSAT的工具来实现。具体而言，研究解决了三个关键挑战：1）LLM友好型求解器：提出模块化指南以提高求解器与LLMs的兼容性，包括简化代码、共享信息和减少错误；2）自动提示优化：引入无监督方法以提升LLMs输出的多样性；3）高效搜索策略：设计了预搜索策略和进化算法（EA）来发现启发式方法。

Result: AutoModSAT在广泛的数据集上的实验表明，其性能比基线求解器提高了50%，比最先进（SOTA）求解器提高了30%。此外，与SOTA求解器的参数调优版本相比，AutoModSAT的平均速度提高了20%，展示了其处理复杂问题实例的增强能力。

Conclusion: 本研究提出了AutoModSAT，一个利用大型语言模型（LLMs）优化复杂SAT求解器的新范式，并在实验中展示了其在性能和效率上的显著优势，为下一代复杂求解器的开发提供了方法论和实证支持。

Abstract: Satisfiability problem (SAT) is a cornerstone of computational complexity
with broad industrial applications, and it remains challenging to optimize
modern SAT solvers in real-world settings due to their intricate architectures.
While automatic configuration frameworks have been developed, they rely on
manually constrained search spaces and yield limited performance gains. This
work introduces a novel paradigm which effectively optimizes complex SAT
solvers via Large Language Models (LLMs), and a tool called AutoModSAT is
developed. Three fundamental challenges are addressed in order to achieve
superior performance: (1) LLM-friendly solver: Systematic guidelines are
proposed for developing a modularized solver to meet LLMs' compatibility,
emphasizing code simplification, information share and bug reduction; (2)
Automatic prompt optimization: An unsupervised automatic prompt optimization
method is introduced to advance the diversity of LLMs' output; (3) Efficient
search strategy: We design a presearch strategy and an EA evolutionary
algorithm for the final efficient and effective discovery of heuristics.
Extensive experiments across a wide range of datasets demonstrate that
AutoModSAT achieves 50% performance improvement over the baseline solver and
achieves 30% superiority against the state-of-the-art (SOTA) solvers. Moreover,
AutoModSAT attains a 20% speedup on average compared to parameter-tuned
alternatives of the SOTA solvers, showcasing the enhanced capability in
handling complex problem instances. This work bridges the gap between AI-driven
heuristics discovery and mission-critical system optimization, and provides
both methodological advancements and empirically validated results for
next-generation complex solver development.

</details>


### [262] [An Explainable Emotion Alignment Framework for LLM-Empowered Agent in Metaverse Service Ecosystem](https://arxiv.org/abs/2507.22326)
*Qun Ma,Xiao Xue,Ming Zhang,Yifan Shen,Zihan Zhao*

Main category: cs.AI

TL;DR: 该研究提出了一种新框架，用于解决元宇宙中基于LLM的代理在连接虚拟与现实服务时遇到的数据融合、知识关联和道德安全挑战。实验证明该框架能实现更强的关系事实对齐，并产生更真实的社会互动。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的代理在元宇宙服务建模中，在连接虚拟世界服务与现实世界服务方面面临重大挑战，尤其是在角色数据融合、角色知识关联和道德安全问题上。

Method: 提出了一种可解释的情感对齐框架，用于元宇宙服务生态系统中的LLM驱动代理，旨在将事实因素整合到代理的决策循环中，以实现更强的关系事实对齐。

Result: 仿真实验在Offline-to-Offline食品配送场景下评估了该框架的有效性，获得了更真实的社会涌现。

Conclusion: 所提出的可解释的情感对齐框架通过整合事实因素到LLM驱动的代理的决策循环中，成功地实现了更强的关系事实对齐，并在Offline-to-Offline食品配送场景的仿真实验中得到了验证，获得了更真实的社会涌现。

Abstract: Metaverse service is a product of the convergence between Metaverse and
service systems, designed to address service-related challenges concerning
digital avatars, digital twins, and digital natives within Metaverse. With the
rise of large language models (LLMs), agents now play a pivotal role in
Metaverse service ecosystem, serving dual functions: as digital avatars
representing users in the virtual realm and as service assistants (or NPCs)
providing personalized support. However, during the modeling of Metaverse
service ecosystems, existing LLM-based agents face significant challenges in
bridging virtual-world services with real-world services, particularly
regarding issues such as character data fusion, character knowledge
association, and ethical safety concerns. This paper proposes an explainable
emotion alignment framework for LLM-based agents in Metaverse Service
Ecosystem. It aims to integrate factual factors into the decision-making loop
of LLM-based agents, systematically demonstrating how to achieve more
relational fact alignment for these agents. Finally, a simulation experiment in
the Offline-to-Offline food delivery scenario is conducted to evaluate the
effectiveness of this framework, obtaining more realistic social emergence.

</details>


### [263] [Magentic-UI: Towards Human-in-the-loop Agentic Systems](https://arxiv.org/abs/2507.22358)
*Hussein Mozannar,Gagan Bansal,Cheng Tan,Adam Fourney,Victor Dibia,Jingya Chen,Jack Gerrits,Tyler Payne,Matheus Kunzler Maldaner,Madeleine Grunde-McLaughlin,Eric Zhu,Griffin Bassman,Jacob Alber,Peter Chang,Ricky Loynd,Friederike Niedtner,Ece Kamar,Maya Murad,Rafah Hosn,Saleema Amershi*

Main category: cs.AI

TL;DR: Magentic-UI是一个用于开发和研究人机交互的开源Web界面，它支持多种工具，并提供六种交互机制以实现有效的人员参与，旨在提高AI代理的生产力和安全性。


<details>
  <summary>Details</summary>
Motivation: AI代理在处理复杂的多步任务方面越来越有能力，但仍未达到人类水平，并带来安全和安保风险。人机环系统结合了人类的监督和控制以及AI的效率，有望提高生产力。

Method: 通过六种交互机制（包括共同规划、共同任务、多任务、操作保护和长期记忆）实现人机环技术。

Result: Magentic-UI在自主任务完成、交互能力模拟用户测试、真实用户定性研究和安全评估等方面进行了评估。

Conclusion: Magentic-UI有潜力促进安全高效的人机协作。

Abstract: AI agents powered by large language models are increasingly capable of
autonomously completing complex, multi-step tasks using external tools. Yet,
they still fall short of human-level performance in most domains including
computer use, software development, and research. Their growing autonomy and
ability to interact with the outside world, also introduces safety and security
risks including potentially misaligned actions and adversarial manipulation. We
argue that human-in-the-loop agentic systems offer a promising path forward,
combining human oversight and control with AI efficiency to unlock productivity
from imperfect systems. We introduce Magentic-UI, an open-source web interface
for developing and studying human-agent interaction. Built on a flexible
multi-agent architecture, Magentic-UI supports web browsing, code execution,
and file manipulation, and can be extended with diverse tools via Model Context
Protocol (MCP). Moreover, Magentic-UI presents six interaction mechanisms for
enabling effective, low-cost human involvement: co-planning, co-tasking,
multi-tasking, action guards, and long-term memory. We evaluate Magentic-UI
across four dimensions: autonomous task completion on agentic benchmarks,
simulated user testing of its interaction capabilities, qualitative studies
with real users, and targeted safety assessments. Our findings highlight
Magentic-UI's potential to advance safe and efficient human-agent
collaboration.

</details>


### [264] [LLM-Crowdsourced: A Benchmark-Free Paradigm for Mutual Evaluation of Large Language Models](https://arxiv.org/abs/2507.22359)
*Qianhong Guo,Wei Xie,Xiaofang Cai,Enze Wang,Shuoyoucheng Ma,Kai Chen,Xiaofeng Wang,Baosheng Wang*

Main category: cs.AI

TL;DR: 提出 LLM-Crowdsourced 评估范式，通过 LLM 自生成、自回答、自评估解决现有评估方法的不足，并在实验中验证了其有效性，发现了新的 LLM 行为。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 评估方法存在数据污染、黑盒操作和主观偏好等问题，难以全面评估 LLM 的真实能力。

Method: LLM-Crowdsourced 范式利用 LLM 生成问题、独立回答并相互评估。

Result: 实验结果表明，LLM-Crowdsourced 范式在区分 LLM 性能方面具有优势，并揭示了“记忆式回答”和 LLM 评估结果的高一致性（鲁棒性）等新发现。Gemini 在原创和专业问题设计能力方面表现最佳。

Conclusion: LLM-Crowdsourced 是一种新颖的基准无关评估范式，它利用 LLM 生成问题、独立回答并相互评估，从而解决了现有评估方法存在的数据污染、黑盒操作和主观偏好等问题。该方法整合了动态、透明、客观和专业四个关键评估标准，并在数学和编程任务上对八个主流 LLM 进行了实验验证，证明了其在区分 LLM 性能方面的优越性。

Abstract: Although large language models (LLMs) demonstrate remarkable capabilities
across various tasks, evaluating their capabilities remains a challenging task.
Existing evaluation methods suffer from issues such as data contamination,
black-box operation, and subjective preference. These issues make it difficult
to evaluate the LLMs' true capabilities comprehensively. To tackle these
challenges, we propose a novel benchmark-free evaluation paradigm,
LLM-Crowdsourced. It utilizes LLMs to generate questions, answer independently,
and evaluate mutually. This method integrates four key evaluation criteria:
dynamic, transparent, objective, and professional, which existing evaluation
methods cannot satisfy simultaneously. Experiments on eight mainstream LLMs
across mathematics and programming verify the advantages of our method in
distinguishing LLM performance. Furthermore, our study reveals several novel
findings that are difficult for traditional methods to detect, including but
not limited to: (1) Gemini demonstrates the highest original and professional
question-design capabilities among others; (2) Some LLMs exhibit
''memorization-based answering'' by misrecognizing questions as familiar ones
with a similar structure; (3) LLM evaluation results demonstrate high
consistency (robustness).

</details>


### [265] [Beyond Accuracy: How AI Metacognitive Sensitivity improves AI-assisted Decision Making](https://arxiv.org/abs/2507.22365)
*ZhaoBin Li,Mark Steyvers*

Main category: cs.AI

TL;DR: AI的元认知敏感性（区分正确和错误预测的置信度评分能力）很重要，高的元认知敏感性可以提高人类的决策表现。


<details>
  <summary>Details</summary>
Motivation: 在人类决策依赖AI输入的场景下，AI的预测准确性和置信度估计都会影响决策质量，因此需要研究AI的元认知敏感性（区分正确和错误预测的置信度评分能力）。

Method: 提出评估AI预测准确性和元认知敏感性联合影响的理论框架，并通过行为实验进行了验证。

Result: AI元认知敏感性越高，人类决策表现越好。在某些情况下，预测准确性较低但元认知敏感性较高的AI可以提高人类的整体决策准确性。

Conclusion: 评估AI辅助决策时，不仅要考虑预测准确性，还要考虑元认知敏感性，并优化两者以获得更好的决策结果。

Abstract: In settings where human decision-making relies on AI input, both the
predictive accuracy of the AI system and the reliability of its confidence
estimates influence decision quality. We highlight the role of AI metacognitive
sensitivity -- its ability to assign confidence scores that accurately
distinguish correct from incorrect predictions -- and introduce a theoretical
framework for assessing the joint impact of AI's predictive accuracy and
metacognitive sensitivity in hybrid decision-making settings. Our analysis
identifies conditions under which an AI with lower predictive accuracy but
higher metacognitive sensitivity can enhance the overall accuracy of human
decision making. Finally, a behavioral experiment confirms that greater AI
metacognitive sensitivity improves human decision performance. Together, these
findings underscore the importance of evaluating AI assistance not only by
accuracy but also by metacognitive sensitivity, and of optimizing both to
achieve superior decision outcomes.

</details>


### [266] [On the Definition of Intelligence](https://arxiv.org/abs/2507.22423)
*Kei-Sing Ng*

Main category: cs.AI

TL;DR: Intelligence is defined as the ability to generate samples from a category given samples from the same category. This is formalized as {\epsilon}-category intelligence, where an agent is {\epsilon}-intelligent if its generated samples cannot be distinguished from original samples beyond a tolerance {\epsilon}. The paper discusses the framework, empirical protocols, and implications for AI evaluation, safety, and generalization.


<details>
  <summary>Details</summary>
Motivation: To engineer AGI, we should first capture the essence of intelligence in a species-agnostic form that can be evaluated, while being sufficiently general to encompass diverse paradigms of intelligent behavior, including reinforcement learning, generative models, classification, analogical reasoning, and goal-directed decision-making.

Method: We formalise this intuition as {\epsilon}-category intelligence: it is {\epsilon}-intelligent with respect to a category if no chosen admissible distinguisher can separate generated from original samples beyond tolerance {\epsilon}.

Result: We present the formal framework, outline empirical protocols, and discuss implications for evaluation, safety, and generalization.

Conclusion:  intelligence is the ability, given sample(s) from a category, to generate sample(s) from the same category.

Abstract: To engineer AGI, we should first capture the essence of intelligence in a
species-agnostic form that can be evaluated, while being sufficiently general
to encompass diverse paradigms of intelligent behavior, including reinforcement
learning, generative models, classification, analogical reasoning, and
goal-directed decision-making. We propose a general criterion based on sample
fidelity: intelligence is the ability, given sample(s) from a category, to
generate sample(s) from the same category. We formalise this intuition as
{\epsilon}-category intelligence: it is {\epsilon}-intelligent with respect to
a category if no chosen admissible distinguisher can separate generated from
original samples beyond tolerance {\epsilon}. We present the formal framework,
outline empirical protocols, and discuss implications for evaluation, safety,
and generalization.

</details>


### [267] [Cross-Border Legal Adaptation of Autonomous Vehicle Design based on Logic and Non-monotonic Reasoning](https://arxiv.org/abs/2507.22432)
*Zhe Yu,Yiwei Lu,Burkhard Schafer,Zhe Lin*

Main category: cs.AI

TL;DR: 本论文探讨了自动驾驶汽车在跨国情境下的法律合规性问题，提出了一种基于论证理论和自然数偏序集的推理系统，以辅助设计师应对法律挑战。


<details>
  <summary>Details</summary>
Motivation: 本论文着眼于跨国背景下自动驾驶汽车的法律合规性挑战，从设计者的视角出发，旨在为设计过程提供法律推理支持。

Method: 基于论证理论，引入逻辑来表示基于论证的实际（规范性）推理的基本属性，并结合自然数的偏序集来表达优先级。

Result: 本研究提供了论证理论和自然数偏序集相结合的推理系统，以应对自动驾驶汽车的法律合规性挑战。

Conclusion: 通过案例分析，展示了所提供的推理系统如何帮助设计者更灵活地适应自动驾驶汽车的跨境应用，并更轻松地理解其决策的法律含义。

Abstract: This paper focuses on the legal compliance challenges of autonomous vehicles
in a transnational context. We choose the perspective of designers and try to
provide supporting legal reasoning in the design process. Based on
argumentation theory, we introduce a logic to represent the basic properties of
argument-based practical (normative) reasoning, combined with partial order
sets of natural numbers to express priority. Finally, through case analysis of
legal texts, we show how the reasoning system we provide can help designers to
adapt their design solutions more flexibly in the cross-border application of
autonomous vehicles and to more easily understand the legal implications of
their decisions.

</details>


### [268] [Collaborative Medical Triage under Uncertainty: A Multi-Agent Dynamic Matching Approach](https://arxiv.org/abs/2507.22504)
*Hongyan Cheng,Chengzhang Yu,Yanshu Shi,Chiyue Wang,Cong Liu,Zhanpeng Jin*

Main category: cs.AI

TL;DR: 本文提出了一种多智能体AI系统，用于改善医院分诊效率和准确性，解决了现有AI分诊系统的专业知识不足、机构异构和问询效率低等问题。该系统通过三个协作智能体，并使用包含3360个真实病例的数据集进行训练和评估，实现了较高的分诊准确率，并能适应不同医院的结构。


<details>
  <summary>Details</summary>
Motivation: 由于疫情后医疗需求的激增以及严重的护士短缺，对急诊科分诊系统造成了前所未有的压力，因此需要创新的、由人工智能驱动的解决方案。目前基于AI的分诊系统存在医学专业知识不足导致误分类、医疗机构部门结构异构以及问询效率低下等问题，本研究旨在解决这些挑战。

Method: 本研究提出的方法是构建一个多智能体交互式智能系统，该系统包含接收智能体、询问智能体和部门智能体。这三个智能体通过结构化的问询机制和部门特定的指导规则进行协作，以将非结构化的患者症状转化为准确的部门推荐。为了处理真实世界数据中不完整的医疗记录，研究采用了大型语言模型进行系统的数据插补。

Result: 实验结果表明，该多智能体系统在一级部门分类上达到了89.2%的准确率，在二级部门分类上达到了73.9%的准确率（经过四轮患者交互后）。该系统的模式匹配指导机制能够有效地适应不同的医院结构，同时保持高分诊准确性。

Conclusion: 该研究提出了一个多智能体交互式智能系统，用于解决当前AI驱动的医疗分诊系统中存在的三个关键挑战：缺乏医学专业知识导致的误分类、医疗机构间异构的部门结构以及影响快速分诊决策的低效问询。该系统通过三个专门的智能体（接收智能体、询问智能体和部门智能体）协作，并结合结构化问询机制和特定部门的指导规则，将非结构化的患者症状转化为准确的部门推荐。实验结果表明，该系统在四轮患者交互后，一级部门分类准确率为89.2%，二级部门分类准确率为73.9%。该系统基于模式匹配的指导机制能够有效地适应不同的医院配置，同时保持较高的分诊准确性，为部署能够适应医疗机构组织异构性并确保临床决策的AI辅助分诊系统提供了一个可扩展的框架。

Abstract: The post-pandemic surge in healthcare demand, coupled with critical nursing
shortages, has placed unprecedented pressure on emergency department triage
systems, necessitating innovative AI-driven solutions. We present a multi-agent
interactive intelligent system for medical triage that addresses three
fundamental challenges in current AI-based triage systems: insufficient medical
specialization leading to hallucination-induced misclassifications,
heterogeneous department structures across healthcare institutions, and
inefficient detail-oriented questioning that impedes rapid triage decisions.
Our system employs three specialized agents - RecipientAgent, InquirerAgent,
and DepartmentAgent - that collaborate through structured inquiry mechanisms
and department-specific guidance rules to transform unstructured patient
symptoms into accurate department recommendations. To ensure robust evaluation,
we constructed a comprehensive Chinese medical triage dataset from a medical
website, comprising 3,360 real-world cases spanning 9 primary departments and
62 secondary departments. Through systematic data imputation using large
language models, we address the prevalent issue of incomplete medical records
in real-world data. Experimental results demonstrate that our multi-agent
system achieves 89.2% accuracy in primary department classification and 73.9%
accuracy in secondary department classification after four rounds of patient
interaction. The system's pattern-matching-based guidance mechanisms enable
efficient adaptation to diverse hospital configurations while maintaining high
triage accuracy. Our work provides a scalable framework for deploying
AI-assisted triage systems that can accommodate the organizational
heterogeneity of healthcare institutions while ensuring clinically sound
decision-making.

</details>


### [269] [MetaAgent: Automatically Constructing Multi-Agent Systems Based on Finite State Machines](https://arxiv.org/abs/2507.22606)
*Yaolun Zhang,Xiaogeng Liu,Chaowei Xiao*

Main category: cs.AI

TL;DR: MetaAgent是一个能自动设计和优化多智能体系统的框架，解决了现有方法的局限性，并在实验中取得了良好效果。


<details>
  <summary>Details</summary>
Motivation: 现有的人类设计的多智能体框架通常仅限于预定义场景，而现有的自动设计方法存在工具集成不足、依赖外部训练数据和通信结构僵化等局限性。因此，需要一种更灵活、更强大的多智能体系统自动设计方法。

Method: MetaAgent是一个基于有限状态机的框架，通过优化算法自动生成和优化多智能体系统，并利用有限状态机控制智能体的动作和状态转换。

Result: 实验结果表明，MetaAgent生成的系统在文本和实际任务上均优于其他自动设计方法，性能与人工设计系统相当。

Conclusion: MetaAgent能够自动生成多智能体系统，并在文本和实际任务的实验中表现出优于其他自动设计方法，并能达到与为特定任务优化的人工设计系统相当的性能。

Abstract: Large Language Models (LLMs) have demonstrated the ability to solve a wide
range of practical tasks within multi-agent systems. However, existing
human-designed multi-agent frameworks are typically limited to a small set of
pre-defined scenarios, while current automated design methods suffer from
several limitations, such as the lack of tool integration, dependence on
external training data, and rigid communication structures. In this paper, we
propose MetaAgent, a finite state machine based framework that can
automatically generate a multi-agent system. Given a task description,
MetaAgent will design a multi-agent system and polish it through an
optimization algorithm. When the multi-agent system is deployed, the finite
state machine will control the agent's actions and the state transitions. To
evaluate our framework, we conduct experiments on both text-based tasks and
practical tasks. The results indicate that the generated multi-agent system
surpasses other auto-designed methods and can achieve a comparable performance
with the human-designed multi-agent system, which is optimized for those
specific tasks.

</details>


### [270] [Enhancing Manufacturing Knowledge Access with LLMs and Context-aware Prompting](https://arxiv.org/abs/2507.22619)
*Sebastian Monka,Irlan Grangel-González,Stefan Schmid,Lavdim Halilaj,Marc Rickart,Oliver Rudolph,Rui Dias*

Main category: cs.AI

TL;DR: 该研究提出并评估了利用 LLM 促进制造领域知识图谱信息检索的方法。研究表明，通过为 LLM 提供恰当的 KG 模式背景知识，可以显著提高其将自然语言问题转化为 SPARQL 查询的准确性，降低错误信息生成率，并最终使更多用户能够访问和利用复杂的制造数据。


<details>
  <summary>Details</summary>
Motivation: 为了解决非专家用户在利用知识图谱 (KG) 进行数据管理时，需要制定复杂的 SPARQL 查询来检索特定信息的挑战。利用大型语言模型 (LLM) 将自然语言查询自动转换为 SPARQL 格式，从而弥合用户友好界面与 KG 复杂架构之间的差距。

Method: 评估了多种使用 LLM 作为中介来促进 KG 信息检索的策略，重点关注制造领域，特别是 Bosch Line Information System KG 和 I40 Core Information Model。通过比较向 LLM 输入 KG 相关背景信息的各种方法，并分析它们将实际问题转化为 SPARQL 查询的熟练程度来进行评估。

Result: LLM 在获得充分的 KG 模式背景知识后，在生成正确和完整的查询方面表现出显著的性能提升。

Conclusion: 通过提供充分的 KG 模式背景知识，可以显著提高 LLM 在生成正确和完整的 SPARQL 查询方面的性能。这些方法有助于 LLM 专注于本体的相关部分，并降低“幻觉”的风险。

Abstract: Knowledge graphs (KGs) have transformed data management within the
manufacturing industry, offering effective means for integrating disparate data
sources through shared and structured conceptual schemas. However, harnessing
the power of KGs can be daunting for non-experts, as it often requires
formulating complex SPARQL queries to retrieve specific information. With the
advent of Large Language Models (LLMs), there is a growing potential to
automatically translate natural language queries into the SPARQL format, thus
bridging the gap between user-friendly interfaces and the sophisticated
architecture of KGs. The challenge remains in adequately informing LLMs about
the relevant context and structure of domain-specific KGs, e.g., in
manufacturing, to improve the accuracy of generated queries. In this paper, we
evaluate multiple strategies that use LLMs as mediators to facilitate
information retrieval from KGs. We focus on the manufacturing domain,
particularly on the Bosch Line Information System KG and the I40 Core
Information Model. In our evaluation, we compare various approaches for feeding
relevant context from the KG to the LLM and analyze their proficiency in
transforming real-world questions into SPARQL queries. Our findings show that
LLMs can significantly improve their performance on generating correct and
complete queries when provided only the adequate context of the KG schema. Such
context-aware prompting techniques help LLMs to focus on the relevant parts of
the ontology and reduce the risk of hallucination. We anticipate that the
proposed techniques help LLMs to democratize access to complex data
repositories and empower informed decision-making in manufacturing settings.

</details>


### [271] [ASP-FZN: A Translation-based Constraint Answer Set Solver](https://arxiv.org/abs/2507.22774)
*Thomas Eiter,Tobias Geibinger,Tobias Kaminski,Nysret Musliu,Johannes Oetsch*

Main category: cs.AI

TL;DR: asp-fzn是一个新的CASP求解器，它通过翻译成FlatZinc来工作，并且在标准ASP和CASP任务上都表现良好。


<details>
  <summary>Details</summary>
Motivation: 提出了一种名为asp-fzn的求解器，用于约束Answer Set编程（CASP），它将ASP与线性约束相结合。

Method: 通过将CASP程序翻译成支持多种约束编程和整数规划后端求解器的独立于求解器的FlatZinc语言来实现。

Result: asp-fzn在来自先前ASP竞赛的基准测试上具有竞争力，并且在某些CASP问题上优于clingcon。

Conclusion: asp-fzn在ASP和CASP基准测试中都具有竞争力，并且在某些CASP基准测试中优于clingcon。

Abstract: We present the solver asp-fzn for Constraint Answer Set Programming (CASP),
which extends ASP with linear constraints. Our approach is based on translating
CASP programs into the solver-independent FlatZinc language that supports
several Constraint Programming and Integer Programming backend solvers. Our
solver supports a rich language of linear constraints, including some common
global constraints. As for evaluation, we show that asp-fzn is competitive with
state-of-the-art ASP solvers on benchmarks taken from past ASP competitions.
Furthermore, we evaluate it on several CASP problems from the literature and
compare its performance with clingcon, which is a prominent CASP solver that
supports most of the asp-fzn language. The performance of asp-fzn is very
promising as it is already competitive on plain ASP and even outperforms
clingcon on some CASP benchmarks.

</details>


### [272] [Enhancing Multi-Agent Collaboration with Attention-Based Actor-Critic Policies](https://arxiv.org/abs/2507.22782)
*Hugo Garrido-Lestache,Jeremy Kedziora*

Main category: cs.AI

TL;DR: TAAC是一种新的多智能体强化学习算法，通过注意力机制促进代理间通信，并在模拟足球环境中取得了优于现有算法的性能。


<details>
  <summary>Details</summary>
Motivation: 旨在提高多智能体在合作环境中的协作能力，通过动态通信有效管理联合动作空间的指数级增长，并确保高度协作。

Method: TAAC（Team-Attention-Actor-Critic）是一种强化学习算法，采用中心化训练/中心化执行方案，并在actor和critic中集成多头注意力机制，以实现动态的、代理间的通信。此外，还引入了惩罚性损失函数，以促进代理间角色多样化和互补性。

Result: TAAC在模拟足球环境中，相较于Proximal Policy Optimization和Multi-Agent Actor-Attention-Critic等基准算法，在胜率、净胜球、Elo评分、代理间连通性、空间分布平衡和战术互动频率等多个指标上均表现出更优越的性能和更强的协作行为。

Conclusion: TAAC在模拟足球环境中表现优于基准算法，并在多种指标上展示了增强的协作行为，包括胜率、净胜球、Elo评分、代理间连通性、空间分布平衡和频繁的战术互动。

Abstract: This paper introduces Team-Attention-Actor-Critic (TAAC), a reinforcement
learning algorithm designed to enhance multi-agent collaboration in cooperative
environments. TAAC employs a Centralized Training/Centralized Execution scheme
incorporating multi-headed attention mechanisms in both the actor and critic.
This design facilitates dynamic, inter-agent communication, allowing agents to
explicitly query teammates, thereby efficiently managing the exponential growth
of joint-action spaces while ensuring a high degree of collaboration. We
further introduce a penalized loss function which promotes diverse yet
complementary roles among agents. We evaluate TAAC in a simulated soccer
environment against benchmark algorithms representing other multi-agent
paradigms, including Proximal Policy Optimization and Multi-Agent
Actor-Attention-Critic. We find that TAAC exhibits superior performance and
enhanced collaborative behaviors across a variety of metrics (win rates, goal
differentials, Elo ratings, inter-agent connectivity, balanced spatial
distributions, and frequent tactical interactions such as ball possession
swaps).

</details>


### [273] [The Incomplete Bridge: How AI Research (Mis)Engages with Psychology](https://arxiv.org/abs/2507.22847)
*Han Jiang,Pengda Wang,Xiaoyuan Yi,Xing Xie,Ziang Xiao*

Main category: cs.AI

TL;DR: 人工智能与心理学跨学科研究：分析了 1006 篇 AI 论文及其引用的 2544 篇心理学文献，以理解和指导两领域间的整合。


<details>
  <summary>Details</summary>
Motivation: 旨在探索人工智能与心理学之间的跨学科协同作用，借鉴心理学在理解人类心智和行为方面的丰富理论和方法论，以改进人工智能系统的设计和理解。

Method: 通过分析 2023 年至 2025 年间在顶级人工智能场所发表的 1,006 篇与大型语言模型 (LLM) 相关的论文及其引用的 2,544 篇心理学出版物，识别跨学科整合的关键模式，确定最常被引用的心理学领域，并强调仍未充分探索的领域。此外，研究还检查了心理学理论/框架的运作和解释方式，确定了常见的误用类型，并为更有效的纳入提供了指导。

Result: 识别了跨学科整合的关键模式，指出了被频繁引用的心理学领域，并强调了需要进一步探索的领域。同时，研究还分析了心理学理论/框架的运作和解释方式，明确了常见的误用类型，并为更有效地整合心理学知识提供了指导。

Conclusion: 这项研究为人工智能和心理学之间的跨学科合作提供了全面的地图，旨在促进更深入的合作和推进人工智能系统。

Abstract: Social sciences have accumulated a rich body of theories and methodologies
for investigating the human mind and behaviors, while offering valuable
insights into the design and understanding of Artificial Intelligence (AI)
systems. Focusing on psychology as a prominent case, this study explores the
interdisciplinary synergy between AI and the field by analyzing 1,006
LLM-related papers published in premier AI venues between 2023 and 2025, along
with the 2,544 psychology publications they cite. Through our analysis, we
identify key patterns of interdisciplinary integration, locate the psychology
domains most frequently referenced, and highlight areas that remain
underexplored. We further examine how psychology theories/frameworks are
operationalized and interpreted, identify common types of misapplication, and
offer guidance for more effective incorporation. Our work provides a
comprehensive map of interdisciplinary engagement between AI and psychology,
thereby facilitating deeper collaboration and advancing AI systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [274] [Explaining Deep Network Classification of Matrices: A Case Study on Monotonicity](https://arxiv.org/abs/2507.22570)
*Leandro Farina,Sergey Korotov*

Main category: cs.LG

TL;DR: 利用深度学习和XAI技术，发现了区分单调和非单调矩阵的简单方法：只需分析特征的两个最低阶系数c0和c1的绝对值。这两个参数的比例|c0/c1|≤0.18（等价于tr(A⁻¹)≥5.7）可以95%的准确率识别单调矩阵。


<details>
  <summary>Details</summary>
Motivation: 尽管单调矩阵的定义很简单（其逆矩阵是逐项非负的），但目前尚未找到一个简单的、基于矩阵元素或导出参数的明确表征方法。因此，该研究的动机是利用机器学习来发现区分单调和非单调矩阵的实用标准。

Method: 该研究结合了深度学习和可解释人工智能（XAI）技术。首先，通过随机生成单调和非单调矩阵来建立标记数据集。然后，利用深度神经网络算法对矩阵进行分类，输入包括矩阵的条目和一组全面的矩阵特征。最后，利用集成梯度等显著性方法识别出两个关键的矩阵参数（c0和c1的绝对值），它们单独就能提供足够的信息用于矩阵分类。

Result: 研究发现，仅使用矩阵特征的两个最低阶系数的绝对值，即c0和c1，就能以95%的准确率区分单调和非单调矩阵。进一步的研究表明，单调矩阵满足|c0/c1|≤0.18的概率大于99.98%，这等同于tr(A⁻¹)≥5.7的简单边界条件。

Conclusion: 该研究首次提出了一个系统性的机器学习方法，用于推导区分单调和非单调矩阵的实用标准。通过结合高性能神经网络和可解释人工智能（XAI）技术，研究人员能够将模型的学习策略提炼成人类可理解的规则。研究发现，仅凭特征的两个最低阶系数的绝对值（c0和c1）就足以达到95%的准确率进行矩阵分类。此外，研究还表明，单调矩阵满足|c0/c1|≤0.18的概率大于99.98%，这等同于tr(A⁻¹)≥5.7的简单边界条件。

Abstract: This work demonstrates a methodology for using deep learning to discover
simple, practical criteria for classifying matrices based on abstract algebraic
properties. By combining a high-performance neural network with explainable AI
(XAI) techniques, we can distill a model's learned strategy into
human-interpretable rules. We apply this approach to the challenging case of
monotone matrices, defined by the condition that their inverses are entrywise
nonnegative. Despite their simple definition, an easy characterization in terms
of the matrix elements or the derived parameters is not known. Here, we
present, to the best of our knowledge, the first systematic machine-learning
approach for deriving a practical criterion that distinguishes monotone from
non-monotone matrices. After establishing a labelled dataset by randomly
generated monotone and non-monotone matrices uniformly on $(-1,1)$, we employ
deep neural network algorithms for classifying the matrices as monotone or
non-monotone, using both their entries and a comprehensive set of matrix
features. By saliency methods, such as integrated gradients, we identify among
all features, two matrix parameters which alone provide sufficient information
for the matrix classification, with $95\%$ accuracy, namely the absolute values
of the two lowest-order coefficients, $c_0$ and $c_1$ of the matrix's
characteristic polynomial. A data-driven study of 18,000 random $7\times7$
matrices shows that the monotone class obeys $\lvert c_{0}/c_{1}\rvert\le0.18$
with probability $>99.98\%$; because $\lvert c_{0}/c_{1}\rvert =
1/\mathrm{tr}(A^{-1})$ for monotone $A$, this is equivalent to the simple bound
$\mathrm{tr}(A^{-1})\ge5.7$.

</details>


### [275] [CIMR: Contextualized Iterative Multimodal Reasoning for Robust Instruction Following in LVLMs](https://arxiv.org/abs/2507.22074)
*Yangshu Yuan,Heng Chen,Xinyi Jiang,Christian Ng,Kexin Qiu*

Main category: cs.LG

TL;DR: 提出CIMR框架，通过迭代推理和自我修正提升模型处理复杂多模态指令的能力，并在MAP数据集上取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）和大型视觉语言模型（LVLMs）在处理需要逻辑推理、动态反馈整合和迭代自我纠正的复杂、多步骤多模态指令时存在不足。

Method: 提出了一种名为CIMR（Contextualized Iterative Multimodal Reasoning）的新框架，该框架包含一个上下文感知的迭代推理和自我修正模块。CIMR分为两个阶段：初始推理与响应生成，以及利用解析后的多模态反馈进行迭代优化。其动态融合模块在每一步深度整合文本、视觉和上下文特征。实验中，CIMR在VIT数据集上微调了LLaVA-1.5-7B模型，并在MAP数据集上进行了评估。

Result: CIMR在MAP数据集上取得了91.5%的准确率，超过了GPT-4V（89.2%）、LLaVA-1.5（78.5%）、MiniGPT-4（75.3%）和InstructBLIP（72.8%）等最先进的模型，证明了其在复杂任务中的迭代推理和自我修正能力的有效性。

Conclusion: CIMR框架在处理复杂的多模态指令方面表现出色，通过迭代推理和自我修正能力，显著优于现有模型，在MAP数据集上达到了91.5%的准确率。

Abstract: The rapid advancement of Large Language Models (LLMs) and Large
Vision-Language Models (LVLMs) has enhanced our ability to process and generate
human language and visual information. However, these models often struggle
with complex, multi-step multi-modal instructions that require logical
reasoning, dynamic feedback integration, and iterative self-correction. To
address this, we propose CIMR: Contextualized Iterative Multimodal Reasoning, a
novel framework that introduces a context-aware iterative reasoning and
self-correction module. CIMR operates in two stages: initial reasoning and
response generation, followed by iterative refinement using parsed multi-modal
feedback. A dynamic fusion module deeply integrates textual, visual, and
contextual features at each step. We fine-tune LLaVA-1.5-7B on the Visual
Instruction Tuning (VIT) dataset and evaluate CIMR on the newly introduced
Multi-modal Action Planning (MAP) dataset. CIMR achieves 91.5% accuracy,
outperforming state-of-the-art models such as GPT-4V (89.2%), LLaVA-1.5
(78.5%), MiniGPT-4 (75.3%), and InstructBLIP (72.8%), demonstrating the
efficacy of its iterative reasoning and self-correction capabilities in complex
tasks.

</details>


### [276] [Hybrid activation functions for deep neural networks: S3 and S4 -- a novel approach to gradient flow optimization](https://arxiv.org/abs/2507.22090)
*Sergii Kavun*

Main category: cs.LG

TL;DR: 通过引入S4（平滑S3）混合激活函数，克服了ReLU、sigmoid和tanh的局限性，并在多种任务和网络架构中取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的激活函数（如ReLU、sigmoid、tanh）存在各自的缺点，例如ReLU的死亡神经元问题以及sigmoid和tanh的梯度消失问题。本研究旨在通过新型混合激活函数来克服这些限制。

Method: 提出并实现了两种新的混合激活函数S3（Sigmoid-Softsign）和S4（平滑S3），其中S4通过陡度参数k控制平滑过渡。在二元分类、多元分类和回归任务上，使用三种不同的神经网络架构进行了广泛的实验。

Result: S4在MNIST（准确率97.4%）、Iris分类（准确率96.0%）和Boston Housing回归（MSE 18.7）等任务上表现优于九种基线激活函数，收敛速度更快（比ReLU快-19），并在深层网络中保持了稳定的梯度流。S4的梯度范围为[0.24, 0.59]，而ReLU在深层网络中存在18%的死亡神经元。

Conclusion: S4激活函数通过其混合设计和可调的陡度参数k，解决了现有激活函数的主要局限性，能够适应不同的任务和网络深度，是一种多功能的选择，有望改善神经网络的训练动态。

Abstract: Activation functions are critical components in deep neural networks,
directly influencing gradient flow, training stability, and model performance.
Traditional functions like ReLU suffer from dead neuron problems, while sigmoid
and tanh exhibit vanishing gradient issues. We introduce two novel hybrid
activation functions: S3 (Sigmoid-Softsign) and its improved version S4
(smoothed S3). S3 combines sigmoid for negative inputs with softsign for
positive inputs, while S4 employs a smooth transition mechanism controlled by a
steepness parameter k. We conducted comprehensive experiments across binary
classification, multi-class classification, and regression tasks using three
different neural network architectures. S4 demonstrated superior performance
compared to nine baseline activation functions, achieving 97.4% accuracy on
MNIST, 96.0% on Iris classification, and 18.7 MSE on Boston Housing regression.
The function exhibited faster convergence (-19 for ReLU) and maintained stable
gradient flow across network depths. Comparative analysis revealed S4's
gradient range of [0.24, 0.59] compared to ReLU's 18% dead neurons in deep
networks. The S4 activation function addresses key limitations of existing
functions through its hybrid design and smooth transition mechanism. The
tunable parameter k allows adaptation to different tasks and network depths,
making S4 a versatile choice for deep learning applications. These findings
suggest that hybrid activation functions represent a promising direction for
improving neural network training dynamics.

</details>


### [277] [Prototype-Guided Pseudo-Labeling with Neighborhood-Aware Consistency for Unsupervised Adaptation](https://arxiv.org/abs/2507.22075)
*Eman Ali,Chetan Arora,Muhammad Haris Khan*

Main category: cs.LG

TL;DR: 提出了一种新的自适应伪标签方法（PICS和NALR），通过特征一致性和邻域语义相似性来提高无监督学习中CLIP模型的适应性，实验证明其在11个数据集上优于现有方法，且计算效率高。


<details>
  <summary>Details</summary>
Motivation: 在无监督适应中，源自零样本预测的伪标签通常存在显著噪声，特别是在领域迁移或视觉复杂场景下。依赖固定置信度阈值的传统伪标签过滤方法在完全无监督设置中往往不可靠。

Method: 该方法包含两个关键组成部分：PICS，它基于类内特征紧密度和类间特征分离度来评估伪标签的准确性；NALR，它利用邻近样本之间的语义相似性来动态地精炼伪标签。此外，还引入了一种自适应加权机制，根据伪标签样本的估计正确性来调整其在训练过程中的影响。

Result: 所提出的方法在11个基准数据集上进行了广泛的实验，在无监督适应场景中取得了最先进的性能，提供了更准确的伪标签，同时保持了计算效率。

Conclusion: 该研究提出了一个新颖的自适应伪标签框架，通过整合原型一致性和邻域一致性来增强CLIP的适应性表现。

Abstract: In unsupervised adaptation for vision-language models such as CLIP,
pseudo-labels derived from zero-shot predictions often exhibit significant
noise, particularly under domain shifts or in visually complex scenarios.
Conventional pseudo-label filtering approaches, which rely on fixed confidence
thresholds, tend to be unreliable in fully unsupervised settings. In this work,
we propose a novel adaptive pseudo-labeling framework that enhances CLIP's
adaptation performance by integrating prototype consistency and
neighborhood-based consistency. The proposed method comprises two key
components: PICS, which assesses pseudo-label accuracy based on in-class
feature compactness and cross-class feature separation; and NALR, which
exploits semantic similarities among neighboring samples to refine
pseudo-labels dynamically. Additionally, we introduce an adaptive weighting
mechanism that adjusts the influence of pseudo-labeled samples during training
according to their estimated correctness. Extensive experiments on 11 benchmark
datasets demonstrate that our method achieves state-of-the-art performance in
unsupervised adaptation scenarios, delivering more accurate pseudo-labels while
maintaining computational efficiency.

</details>


### [278] [Test-time Prompt Refinement for Text-to-Image Models](https://arxiv.org/abs/2507.22076)
*Mohammad Abdul Hafeez Khan,Yash Jain,Siddhartha Bhattacharyya,Vibhav Vineet*

Main category: cs.LG

TL;DR: TIR 框架通过使用MLLM在测试时迭代地优化提示，解决了 T2I 模型在提示敏感性方面的问题，提高了生成图像的准确性和一致性，且无需重新训练 T2I 模型。


<details>
  <summary>Details</summary>
Motivation: 现有的文本到图像（T2I）生成模型在提示敏感性方面仍存在不足，提示词的微小变动可能导致输出不一致或不准确。

Method: 提出了一种闭环、测试时提示精炼框架（TIR），在每个生成步骤后，使用预训练的多模态大语言模型（MLLM）分析输出图像和用户提示，检测错误，并为下一轮生成生成精炼的、物理上合理的提示。

Result: 该闭环策略在多个基准数据集上提高了对齐度和视觉一致性。

Conclusion: 该框架通过迭代地优化提示并验证提示与图像之间的对齐来纠正错误，并能与黑盒 T2I 模型即插即用。

Abstract: Text-to-image (T2I) generation models have made significant strides but still
struggle with prompt sensitivity: even minor changes in prompt wording can
yield inconsistent or inaccurate outputs. To address this challenge, we
introduce a closed-loop, test-time prompt refinement framework that requires no
additional training of the underlying T2I model, termed TIR. In our approach,
each generation step is followed by a refinement step, where a pretrained
multimodal large language model (MLLM) analyzes the output image and the user's
prompt. The MLLM detects misalignments (e.g., missing objects, incorrect
attributes) and produces a refined and physically grounded prompt for the next
round of image generation. By iteratively refining the prompt and verifying
alignment between the prompt and the image, TIR corrects errors, mirroring the
iterative refinement process of human artists. We demonstrate that this
closed-loop strategy improves alignment and visual coherence across multiple
benchmark datasets, all while maintaining plug-and-play integration with
black-box T2I models.

</details>


### [279] [Tapping into the Black Box: Uncovering Aligned Representations in Pretrained Neural Networks](https://arxiv.org/abs/2507.22832)
*Maciej Satkiewicz*

Main category: cs.LG

TL;DR: ReLU网络包含可解释的线性模型，可以通过“激励回溯”方法提取其决策边界，揭示与输入和目标相关的特征。


<details>
  <summary>Details</summary>
Motivation: 探究ReLU网络学习到的隐式线性模型，并展示如何利用该模型发现可解释的特征，以应用于知识发现和开发可靠的人工系统。

Method: 提出了一种名为“激励回溯”的方法，通过修改反向传播过程来近似地将ReLU网络的隐式线性模型的决策边界拉回到输入空间。

Result: 发现“激励回溯”方法能够揭示高分辨率的、与输入和目标相关的特征，这些特征在感知上与常见的ImageNet预训练深度架构高度一致，有力地证明了神经网络依赖于可解释的模式。

Conclusion: ReLU网络学习了一个可以被利用的隐式线性模型。通过修改反向传播过程，可以近似地将该模型的决策边界拉回到输入空间，从而揭示可解释的、与输入和目标相关的特征。

Abstract: In this paper we argue that ReLU networks learn an implicit linear model we
can actually tap into. We describe that alleged model formally and show that we
can approximately pull its decision boundary back to the input space with
certain simple modification to the backward pass. The resulting gradients
(called excitation pullbacks) reveal high-resolution input- and target-specific
features of remarkable perceptual alignment on a number of popular
ImageNet-pretrained deep architectures. This strongly suggests that neural
networks do, in fact, rely on learned interpretable patterns that can be
recovered after training. Thus, our findings may have profound implications for
knowledge discovery and the development of dependable artificial systems.

</details>


### [280] [Multi-fidelity Bayesian Data-Driven Design of Energy Absorbing Spinodoid Cellular Structures](https://arxiv.org/abs/2507.22079)
*Leo Guo,Hirak Kansara,Siamak F. Khosroshahi,GuoQi Zhang,Wei Tan*

Main category: cs.LG

TL;DR: This paper compares Bayesian Optimization (BO) and Multi-Fidelity Bayesian Optimization (MFBO) for designing metamaterials. MFBO is more effective, improving energy absorption by up to 11% compared to BO. The study also uses sensitivity analysis to simplify the design process.


<details>
  <summary>Details</summary>
Motivation: To reconcile the increasing computational cost of accurate Finite Element (FE) simulations with the growing demand for data-driven design, and to address the shortcomings of existing methods in direct comparisons for real-life engineering problems like metamaterial design, sampling quality, and design parameter sensitivity analysis.

Method: This paper employed Sobol' samples with variance-based sensitivity analysis to reduce design complexity and compared the performance of Bayesian Optimization (BO) with Multi-Fidelity Bayesian Optimization (MFBO) for maximizing the energy absorption (EA) of spinodoid cellular structures.

Result: MFBO outperformed BO by up to 11% in maximizing the energy absorption of spinodoid structures across various hyperparameter settings. The findings support the utility of multi-fidelity techniques for expensive data-driven design problems.

Conclusion: MFBO is an effective method for maximizing the energy absorption of spinodoid structures, outperforming BO by up to 11% across various hyperparameter settings. The open-source results support the utility of multi-fidelity techniques in expensive data-driven design.

Abstract: Finite element (FE) simulations of structures and materials are getting
increasingly more accurate, but also more computationally expensive as a
collateral result. This development happens in parallel with a growing demand
of data-driven design. To reconcile the two, a robust and data-efficient
optimization method called Bayesian optimization (BO) has been previously
established as a technique to optimize expensive objective functions. In
parallel, the mesh width of an FE model can be exploited to evaluate an
objective at a lower or higher fidelity (cost & accuracy) level. The
multi-fidelity setting applied to BO, called multi-fidelity BO (MFBO), has also
seen previous success. However, BO and MFBO have not seen a direct comparison
with when faced with with a real-life engineering problem, such as metamaterial
design for deformation and absorption qualities. Moreover, sampling quality and
assessing design parameter sensitivity is often an underrepresented part of
data-driven design. This paper aims to address these shortcomings by employing
Sobol' samples with variance-based sensitivity analysis in order to reduce
design problem complexity. Furthermore, this work describes, implements,
applies and compares the performance BO with that MFBO when maximizing the
energy absorption (EA) problem of spinodoid cellular structures is concerned.
The findings show that MFBO is an effective way to maximize the EA of a
spinodoid structure and is able to outperform BO by up to 11% across various
hyperparameter settings. The results, which are made open-source, serve to
support the utility of multi-fidelity techniques across expensive data-driven
design problems.

</details>


### [281] [Shape Invariant 3D-Variational Autoencoder: Super Resolution in Turbulence flow](https://arxiv.org/abs/2507.22082)
*Anuraj Maurya*

Main category: cs.LG

TL;DR: Deep learning shows promise for turbulence modeling, aiding in multiscale analysis and super-resolution tasks by leveraging complex data.


<details>
  <summary>Details</summary>
Motivation: To explore the benefits of deep learning for turbulence modeling using high-dimensional data from experiments, observations, and simulations.

Method: Overview of classical and deep learning-based turbulence modeling approaches, with investigation into multiscale integration and deep generative models for super-resolution.

Result: The report provides an overview of existing methods and investigates specific challenges in integrating multiscale turbulence models with deep learning and using deep generative models for super-resolution.

Conclusion: Deep learning is a promising tool for turbulence modeling, with potential applications in multiscale integration and super-resolution reconstruction.

Abstract: Deep learning provides a versatile suite of methods for extracting structured
information from complex datasets, enabling deeper understanding of underlying
fluid dynamic phenomena. The field of turbulence modeling, in particular,
benefits from the growing availability of high-dimensional data obtained
through experiments, field observations, and large-scale simulations spanning
multiple spatio-temporal scales. This report presents a concise overview of
both classical and deep learningbased approaches to turbulence modeling. It
further investigates two specific challenges at the intersection of fluid
dynamics and machine learning: the integration of multiscale turbulence models
with deep learning architectures, and the application of deep generative models
for super-resolution reconstruction

</details>


### [282] [Thermodynamics-Inspired Computing with Oscillatory Neural Networks for Inverse Matrix Computation](https://arxiv.org/abs/2507.22544)
*George Tsormpatzoglou,Filip Sabo,Aida Todri-Sanial*

Main category: cs.LG

TL;DR: 通过受热力学启发的计算范式，利用振荡神经网络（ONN）解决了逆矩阵问题，并通过数值模拟验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 研究 ONN 在解决线性代数问题（特别是逆矩阵）方面的潜力，将其作为计算工具进行探索。

Method: 提出了一种受热力学启发的计算范式，该范式基于振荡神经网络（ONN）。通过耦合的 Kuramoto 振荡器模型的线性近似，从理论上证明了逆矩阵解的存在。

Result: 数值模拟验证了理论框架，并考察了计算准确性最高的参数范围。

Conclusion: 该研究分析了振荡神经网络（ONN）作为解决线性代数问题（特别是逆矩阵）的可行性，其理论基础和数值模拟均证实了其有效性。

Abstract: We describe a thermodynamic-inspired computing paradigm based on oscillatory
neural networks (ONNs). While ONNs have been widely studied as Ising machines
for tackling complex combinatorial optimization problems, this work
investigates their feasibility in solving linear algebra problems, specifically
the inverse matrix. Grounded in thermodynamic principles, we analytically
demonstrate that the linear approximation of the coupled Kuramoto oscillator
model leads to the inverse matrix solution. Numerical simulations validate the
theoretical framework, and we examine the parameter regimes that computation
has the highest accuracy.

</details>


### [283] [Towards Interpretable Renal Health Decline Forecasting via Multi-LMM Collaborative Reasoning Framework](https://arxiv.org/abs/2507.22464)
*Peng-Yi Wu,Pei-Cing Huang,Ting-Yu Chen,Chantung Ku,Ming-Yen Lin,Yihuang Kang*

Main category: cs.LG

TL;DR: 提出一个协作框架，利用视觉知识迁移、溯因推理和短期记忆机制，提高开源LMMs在eGFR预测中的准确性和可解释性，结果与专有模型相当。


<details>
  <summary>Details</summary>
Motivation: 为了应对大型多模态模型（LMMs）在临床预测任务中的潜力，同时解决其在部署成本、数据隐私和模型可靠性方面的挑战，本研究旨在提高开源LMMs在eGFR预测方面的性能并生成临床上可理解的解释。

Method: 本研究提出了一种协作框架，结合了视觉知识迁移、溯因推理和短期记忆机制，以提高eGFR预测的准确性和可解释性。

Result: 实验结果表明，该框架在预测性能和可解释性方面均可与专有模型媲美，并能提供合理的临床推理过程。

Conclusion: 本研究提出的框架在eGFR预测方面达到了与专有模型相当的预测性能和可解释性，并为每个预测提供了合理的临床推理过程。我们的方法为构建兼具预测准确性和临床可解释性的医疗AI系统提供了新的思路。

Abstract: Accurate and interpretable prediction of estimated glomerular filtration rate
(eGFR) is essential for managing chronic kidney disease (CKD) and supporting
clinical decisions. Recent advances in Large Multimodal Models (LMMs) have
shown strong potential in clinical prediction tasks due to their ability to
process visual and textual information. However, challenges related to
deployment cost, data privacy, and model reliability hinder their adoption. In
this study, we propose a collaborative framework that enhances the performance
of open-source LMMs for eGFR forecasting while generating clinically meaningful
explanations. The framework incorporates visual knowledge transfer, abductive
reasoning, and a short-term memory mechanism to enhance prediction accuracy and
interpretability. Experimental results show that the proposed framework
achieves predictive performance and interpretability comparable to proprietary
models. It also provides plausible clinical reasoning processes behind each
prediction. Our method sheds new light on building AI systems for healthcare
that combine predictive accuracy with clinically grounded interpretability.

</details>


### [284] [Principled Curriculum Learning using Parameter Continuation Methods](https://arxiv.org/abs/2507.22089)
*Harsh Nilesh Pathak,Randy Paffenroth*

Main category: cs.LG

TL;DR: 提出了一种用于神经网络优化的参数延续方法，该方法在理论上得到支持，并且在深度神经网络的几个问题中具有实践效果。


<details>
  <summary>Details</summary>
Motivation: 为了优化神经网络。

Method: 提出了一种参数延续方法。

Result: 在监督和无监督学习任务中，证明了比 ADAM 等现有优化技术更好的泛化性能。 sebuah metoda parametrik lanjutan untuk optimasi jaringan saraf.

Conclusion: 提出了一种用于神经网络优化的参数延续方法，该方法在理论上得到支持，并且在深度神经网络的几个问题中具有实践效果。

Abstract: In this work, we propose a parameter continuation method for the optimization
of neural networks. There is a close connection between parameter continuation,
homotopies, and curriculum learning. The methods we propose here are
theoretically justified and practically effective for several problems in deep
neural networks. In particular, we demonstrate better generalization
performance than state-of-the-art optimization techniques such as ADAM for
supervised and unsupervised learning tasks.

</details>


### [285] [Hypernetworks for Model-Heterogeneous Personalized Federated Learning](https://arxiv.org/abs/2507.22330)
*Chen Zhang,Husheng Li,Xiang Liu,Linshan Jiang,Danxin Wang*

Main category: cs.LG

TL;DR: A new framework (MH-pFedHN) and its extension (MH-pFedHNGD) use hypernetworks for personalized federated learning that handles different client models effectively, without needing extra data or revealing model details, and shows strong performance.


<details>
  <summary>Details</summary>
Motivation: Existing personalized federated learning methods often have limitations such as requiring external data, model decoupling, or partial learning, hindering practicality and scalability. This work aims to address client model heterogeneity with a more practical and scalable framework.

Method: MH-pFedHN leverages a server-side hypernetwork with client-specific embeddings to generate personalized parameters. A multi-head structure allows sharing among clients with similar model sizes. MH-pFedHNGD further integrates an optional lightweight global model for improved generalization.

Result: The proposed frameworks achieve competitive accuracy and strong generalization, demonstrating robustness across various benchmarks and model settings.

Conclusion: MH-pFedHN and MH-pFedHNGD frameworks offer competitive accuracy, strong generalization, and privacy benefits for model-heterogeneous personalized federated learning without external data or model disclosure.

Abstract: Recent advances in personalized federated learning have focused on addressing
client model heterogeneity. However, most existing methods still require
external data, rely on model decoupling, or adopt partial learning strategies,
which can limit their practicality and scalability. In this paper, we revisit
hypernetwork-based methods and leverage their strong generalization
capabilities to design a simple yet effective framework for heterogeneous
personalized federated learning. Specifically, we propose MH-pFedHN, which
leverages a server-side hypernetwork that takes client-specific embedding
vectors as input and outputs personalized parameters tailored to each client's
heterogeneous model. To promote knowledge sharing and reduce computation, we
introduce a multi-head structure within the hypernetwork, allowing clients with
similar model sizes to share heads. Furthermore, we further propose
MH-pFedHNGD, which integrates an optional lightweight global model to improve
generalization. Our framework does not rely on external datasets and does not
require disclosure of client model architectures, thereby offering enhanced
privacy and flexibility. Extensive experiments on multiple benchmarks and model
settings demonstrate that our approach achieves competitive accuracy, strong
generalization, and serves as a robust baseline for future research in
model-heterogeneous personalized federated learning.

</details>


### [286] [Spatial-Temporal Reinforcement Learning for Network Routing with Non-Markovian Traffic](https://arxiv.org/abs/2507.22174)
*Molly Wang*

Main category: cs.LG

TL;DR: 该研究提出了一种新的空间-时间强化学习方法，结合了GNN和RNN，以解决传统强化学习在通信网络路由中的局限性，并在实验中证明了其优越性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准的基于马尔可夫决策过程（MDP）的强化学习（RL）算法在通信网络路由等实际场景中可能无效，并且传统RL算法在处理具有复杂网络拓扑的空间关系时能力不足。

Method: 提出了一种结合图神经网络（GNN）和循环神经网络（RNN）的空间-时间强化学习方法，以分别捕捉网络拓扑的空间动态和时间流量模式，从而增强路由决策。

Result: 所提出的空间-时间强化学习方法在网络路由方面优于传统RL技术，并且在面对网络拓扑变化时表现出更强的鲁棒性。

Conclusion: 该方法优于传统强化学习技术，并且在网络拓扑发生变化时更加鲁棒。

Abstract: Reinforcement Learning (RL) has become a well-established approach for
optimizing packet routing in communication networks. Standard RL algorithms
typically are based on the Markov Decision Process (MDP), which assumes that
the current state of the environment provides all the necessary information for
system evolution and decision-making. However, this Markovian assumption is
invalid in many practical scenarios, making the MDP and RL frameworks
inadequate to produce the optimal solutions. Additionally, traditional RL
algorithms often employ function approximations (e.g., by neural networks) that
do not explicitly capture the spatial relationships inherent in environments
with complex network topologies. Communication networks are characterized by
dynamic traffic patterns and arbitrary numbers of nodes and links, which
further complicate the decision-making process. To address these challenges, we
propose a spatial-temporal RL approach that integrates Graph Neural Networks
(GNNs) and Recurrent Neural Networks (RNNs) to adequately capture the spatial
dynamics regarding network topology and temporal traffic patterns,
respectively, to enhance routing decisions. Our evaluation demonstrates that
the proposed method outperforms and is more robust to changes in the network
topology when compared with traditional RL techniques.

</details>


### [287] [SourceSplice: Source Selection for Machine Learning Tasks](https://arxiv.org/abs/2507.22186)
*Ambarish Singh,Romila Pradhan*

Main category: cs.LG

TL;DR: SourceGrasp和SourceSplice框架通过选择最佳数据源子集来提高机器学习任务的性能，SourceSplice在效率和效果上优于SourceGrasp。


<details>
  <summary>Details</summary>
Motivation: 在数据质量对机器学习任务的预测性能至关重要，而数据发现的先前工作并未考虑数据源质量对下游机器学习任务的影响的背景下，解决了为给定的机器学习任务确定必须组合以构建基础训练数据集的最佳数据源子集的问题。

Method: 提出SourceGrasp和SourceSplice两个框架来高效选择数据源子集，以最大化下游机器学习模型的效用。SourceGrasp基于贪心标准和随机化元启发式方法，SourceSplice则借鉴了基因剪接的概念。

Result: SourceSplice在真实世界和合成数据集上的经验评估表明，该方法能显著减少子集探索，并有效识别出能提升任务效用的数据源子集。

Conclusion: SourceSplice能识别出能带来高任务效用的数据源子集，并且探索的子集数量显著更少。

Abstract: Data quality plays a pivotal role in the predictive performance of machine
learning (ML) tasks - a challenge amplified by the deluge of data sources
available in modern organizations.Prior work in data discovery largely focus on
metadata matching, semantic similarity or identifying tables that should be
joined to answer a particular query, but do not consider source quality for
high performance of the downstream ML task.This paper addresses the problem of
determining the best subset of data sources that must be combined to construct
the underlying training dataset for a given ML task.We propose SourceGrasp and
SourceSplice, frameworks designed to efficiently select a suitable subset of
sources that maximizes the utility of the downstream ML model.Both the
algorithms rely on the core idea that sources (or their combinations)
contribute differently to the task utility, and must be judiciously
chosen.While SourceGrasp utilizes a metaheuristic based on a greediness
criterion and randomization, the SourceSplice framework presents a source
selection mechanism inspired from gene splicing - a core concept used in
protein synthesis.We empirically evaluate our algorithms on three real-world
datasets and synthetic datasets and show that, with significantly fewer subset
explorations, SourceSplice effectively identifies subsets of data sources
leading to high task utility.We also conduct studies reporting the sensitivity
of SourceSplice to the decision choices under several settings.

</details>


### [288] [Measuring Time-Series Dataset Similarity using Wasserstein Distance](https://arxiv.org/abs/2507.22189)
*Hongjie Chen,Akshay Mehra,Josh Kimball,Ryan A. Rossi*

Main category: cs.LG

TL;DR: 一种基于 Wasserstein 距离的分布相似性度量方法，用于评估时间序列数据集，并可用于基础模型选择和性能评估。


<details>
  <summary>Details</summary>
Motivation: 随着时间序列基础模型研究的兴起，衡量时间序列数据集（不）相似性的需求日益增长，这种衡量方法有助于模型选择、微调和可视化等研究。

Method: 提出了一种基于分布的方法，将时间序列数据集视为多元正态分布（MVN）的经验实例，并利用 Wasserstein 距离计算它们各自 MVN 之间的相似性。

Result: 实验结果表明，该方法在识别相似的时间序列数据集以及在分布外和迁移学习评估中促进基础模型的推理性能估计方面非常有效，所提出的度量与推理损失之间具有高度相关性（>0.60）。

Conclusion: 该方法通过利用 Wasserstein 距离来衡量时间序列数据集的相似性，并在各种实验中证明了其有效性。

Abstract: The emergence of time-series foundation model research elevates the growing
need to measure the (dis)similarity of time-series datasets. A time-series
dataset similarity measure aids research in multiple ways, including model
selection, finetuning, and visualization. In this paper, we propose a
distribution-based method to measure time-series dataset similarity by
leveraging the Wasserstein distance. We consider a time-series dataset an
empirical instantiation of an underlying multivariate normal distribution
(MVN). The similarity between two time-series datasets is thus computed as the
Wasserstein distance between their corresponding MVNs. Comprehensive
experiments and visualization show the effectiveness of our approach.
Specifically, we show how the Wasserstein distance helps identify similar
time-series datasets and facilitates inference performance estimation of
foundation models in both out-of-distribution and transfer learning evaluation,
with high correlations between our proposed measure and the inference loss
(>0.60).

</details>


### [289] [CTG-Insight: A Multi-Agent Interpretable LLM Framework for Cardiotocography Analysis and Classification](https://arxiv.org/abs/2507.22205)
*Black Sun,Die,Hu*

Main category: cs.LG

TL;DR: CTG-Insight is a multi-agent LLM system that interprets fetal monitoring data (CTG) for parents, achieving high accuracy and providing clear explanations.


<details>
  <summary>Details</summary>
Motivation: Current remote fetal monitoring systems provide raw CTG data that is difficult for expectant parents to understand, limiting interpretability. This work aims to address this by providing structured interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals.

Method: A multi-agent LLM system named CTG-Insight is presented. It decomposes CTG traces into five features (baseline, variability, accelerations, decelerations, sinusoidal pattern), with each feature analyzed by a dedicated agent. A final agent synthesizes these analyses for a holistic fetal health classification and natural language explanation, drawing from medical guidelines.

Result: CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score (97.8%) on the NeuroFetalNet Dataset, outperforming deep learning models and a single-agent LLM baseline. The system produces transparent and interpretable outputs.

Conclusion: This work contributes an interpretable and extensible CTG analysis framework, achieving state-of-the-art accuracy (96.4%) and F1-score (97.8%) with transparent and interpretable outputs.

Abstract: Remote fetal monitoring technologies are becoming increasingly common. Yet,
most current systems offer limited interpretability, leaving expectant parents
with raw cardiotocography (CTG) data that is difficult to understand. In this
work, we present CTG-Insight, a multi-agent LLM system that provides structured
interpretations of fetal heart rate (FHR) and uterine contraction (UC) signals.
Drawing from established medical guidelines, CTG-Insight decomposes each CTG
trace into five medically defined features: baseline, variability,
accelerations, decelerations, and sinusoidal pattern, each analyzed by a
dedicated agent. A final aggregation agent synthesizes the outputs to deliver a
holistic classification of fetal health, accompanied by a natural language
explanation. We evaluate CTG-Insight on the NeuroFetalNet Dataset and compare
it against deep learning models and the single-agent LLM baseline. Results show
that CTG-Insight achieves state-of-the-art accuracy (96.4%) and F1-score
(97.8%) while producing transparent and interpretable outputs. This work
contributes an interpretable and extensible CTG analysis framework.

</details>


### [290] [Explainability-Driven Feature Engineering for Mid-Term Electricity Load Forecasting in ERCOT's SCENT Region](https://arxiv.org/abs/2507.22220)
*Abhiram Bhupatiraju,Sung Bum Ahn*

Main category: cs.LG

TL;DR: 本文比较了LSTM、XGBoost、LightGBM和线性回归在中期电力负荷预测中的应用，并使用SHAP方法提高了模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 准确的负荷预测对于现代电力系统的运行至关重要，尤其是在中期预测方面，对于维护调度、资源分配、财务预测和市场参与等方面具有关键作用。然而，电力需求对天气变化和时间动态敏感，需要捕捉非线性模式以进行长期规划。

Method: 本文提出了一种将SHAP方法应用于机器学习模型以提高中期负荷预测准确性和可解释性的方法。

Result: 通过与线性回归、XGBoost和LightGBM等模型的比较，LSTM模型在电力负荷预测方面表现出更好的性能。SHAP方法能够量化特征贡献，指导特征工程，从而提高模型透明度和预测准确性。

Conclusion: 本文比较了线性回归、XGBoost、LightGBM和长短期记忆（LSTM）等机器学习模型在中期负荷预测中的应用，并重点介绍了SHAP方法在提高模型可解释性方面的作用。

Abstract: Accurate load forecasting is essential to the operation of modern electric
power systems. Given the sensitivity of electricity demand to weather
variability and temporal dynamics, capturing non-linear patterns is essential
for long-term planning. This paper presents a comparative analysis of machine
learning models, Linear Regression, XGBoost, LightGBM, and Long Short-Term
Memory (LSTM), for forecasting system-wide electricity load up to one year in
advance. Midterm forecasting has shown to be crucial for maintenance
scheduling, resource allocation, financial forecasting, and market
participation. The paper places a focus on the use of a method called "Shapley
Additive Explanations" (SHAP) to improve model explainability. SHAP enables the
quantification of feature contributions, guiding informed feature engineering
and improving both model transparency and forecasting accuracy.

</details>


### [291] [TRIBE: TRImodal Brain Encoder for whole-brain fMRI response prediction](https://arxiv.org/abs/2507.22229)
*Stéphane d'Ascoli,Jérémy Rapin,Yohann Benchetrit,Hubert Banville,Jean-Rémi King*

Main category: cs.LG

TL;DR: TRIBE是一个新开发的深度神经网络模型，可以预测跨越多种感官模式、大脑区域和个体的大脑活动。它在Algonauts 2025大脑编码竞赛中获胜，显示出在整合多模态信息以理解大脑功能方面的巨大潜力。


<details>
  <summary>Details</summary>
Motivation: 传统的神经科学研究方法将领域分割，阻碍了对认知统一模型的开发。本研究旨在通过一个能够处理多模态信息并预测大脑反应的模型来克服这一限制。

Method: TRIBE是一个深度神经网络模型，通过结合文本、音频和视频基础模型的预训练表征，并使用Transformer处理其时变性质，来预测跨模态、皮层区域和个体的脑部反应。

Result: TRIBE模型在Algonauts 2025大脑编码竞赛中取得第一名，其预测能力显著优于其他竞争者。消融实验表明，多模态模型在高级联想皮层区域的表现优于单一模态模型。

Conclusion: 该方法在感知和理解方面都有应用，为构建人类大脑表征的综合模型铺平了道路。

Abstract: Historically, neuroscience has progressed by fragmenting into specialized
domains, each focusing on isolated modalities, tasks, or brain regions. While
fruitful, this approach hinders the development of a unified model of
cognition. Here, we introduce TRIBE, the first deep neural network trained to
predict brain responses to stimuli across multiple modalities, cortical areas
and individuals. By combining the pretrained representations of text, audio and
video foundational models and handling their time-evolving nature with a
transformer, our model can precisely model the spatial and temporal fMRI
responses to videos, achieving the first place in the Algonauts 2025 brain
encoding competition with a significant margin over competitors. Ablations show
that while unimodal models can reliably predict their corresponding cortical
networks (e.g. visual or auditory networks), they are systematically
outperformed by our multimodal model in high-level associative cortices.
Currently applied to perception and comprehension, our approach paves the way
towards building an integrative model of representations in the human brain.
Our code is available at https://github.com/facebookresearch/algonauts-2025.

</details>


### [292] [Bayesian Optimization of Process Parameters of a Sensor-Based Sorting System using Gaussian Processes as Surrogate Models](https://arxiv.org/abs/2507.22766)
*Felix Kronenwett,Georg Maier,Thomas Laengle*

Main category: cs.LG

TL;DR: 该研究提出了一种利用贝叶斯优化和高斯过程回归来优化、监控和调整传感器分选系统参数的方法，以提高效率并处理不确定性。


<details>
  <summary>Details</summary>
Motivation: 由于传感器分选系统的要求和物料流成分不断变化，需要对过程参数进行连续验证和重新调整。

Method: 基于贝叶斯优化，使用高斯过程回归模型作为代理模型，以实现对系统行为的特定要求，同时考虑其中包含的不确定性。该方法通过最少次数的实验来优化过程参数，同时考虑两个物料流的输出要求，并在模型计算中考虑分选精度的不确定性。

Result: 通过三个实例过程参数评估了该方法。

Conclusion: 该方法通过使用贝叶斯优化和高斯过程回归模型，在考虑不确定性的同时，能够优化、持续监控和调整传感器分选系统的过程参数，从而最大限度地减少实验次数，并兼顾两个物料流的输出要求，同时在模型计算中考虑了分选精度的不确定性。

Abstract: Sensor-based sorting systems enable the physical separation of a material
stream into two fractions. The sorting decision is based on the image data
evaluation of the sensors used and is carried out using actuators. Various
process parameters must be set depending on the properties of the material
stream, the dimensioning of the system, and the required sorting accuracy.
However, continuous verification and re-adjustment are necessary due to
changing requirements and material stream compositions. In this paper, we
introduce an approach for optimizing, recurrently monitoring and adjusting the
process parameters of a sensor-based sorting system. Based on Bayesian
Optimization, Gaussian process regression models are used as surrogate models
to achieve specific requirements for system behavior with the uncertainties
contained therein. This method minimizes the number of necessary experiments
while simultaneously considering two possible optimization targets based on the
requirements for both material output streams. In addition, uncertainties are
considered during determining sorting accuracies in the model calculation. We
evaluated the method with three example process parameters.

</details>


### [293] [Using Scaling Laws for Data Source Utility Estimation in Domain-Specific Pre-Training](https://arxiv.org/abs/2507.22250)
*Oleksiy Ostapenko,Charles Guille-Escuret,Luke Kumar,Max Tian,Denis Kocetkov,Gopeshh Subbaraj,Raymond Li,Joel Lamy-Poirier,Sebastien Paquet,Torsten Scholak*

Main category: cs.LG

TL;DR: 该研究提出了一种优化基础模型特定领域数据集构建的新框架，通过多轮实验估计数据源的扩展规律，以更经济高效地分配资源，并证明该方法比以往的点估计方法更优。


<details>
  <summary>Details</summary>
Motivation: 为了在基础模型训练中优化特定领域数据集的构建，寻找一种高效的方式来评估数据源的质量，以优化资源分配。

Method: 提出了一种通过执行多次不同计算量的数据整理和训练来估计扩展定律的方法，以解决以往仅依赖点估计可能产生的误导性问题。

Result: 通过在70亿参数模型上进行实验，证明了该方法可以估计不同数据源的扩展规律，并为跨不同数据获取方法、数据源和可用计算资源的成本效益资源分配提供信息，尤其是在处理预训练数据中代表性不足的领域时。

Conclusion: 通过实验验证，可以有效地估计数据源的扩展行为，从而为选择和优化数据源提供数据驱动的决策。

Abstract: We introduce a framework for optimizing domain-specific dataset construction
in foundation model training. Specifically, we seek a cost-efficient way to
estimate the quality of data sources (e.g. synthetically generated or filtered
web data, etc.) in order to make optimal decisions about resource allocation
for data sourcing from these sources for the stage two pre-training phase, aka
annealing, with the goal of specializing a generalist pre-trained model to
specific domains. Our approach extends the usual point estimate approaches, aka
micro-annealing, to estimating scaling laws by performing multiple annealing
runs of varying compute spent on data curation and training. This addresses a
key limitation in prior work, where reliance on point estimates for data
scaling decisions can be misleading due to the lack of rank invariance across
compute scales -- a phenomenon we confirm in our experiments. By systematically
analyzing performance gains relative to acquisition costs, we find that scaling
curves can be estimated for different data sources. Such scaling laws can
inform cost effective resource allocation across different data acquisition
methods (e.g. synthetic data), data sources (e.g. user or web data) and
available compute resources. We validate our approach through experiments on a
pre-trained model with 7 billion parameters. We adapt it to: a domain
well-represented in the pre-training data -- the medical domain, and a domain
underrepresented in the pretraining corpora -- the math domain. We show that
one can efficiently estimate the scaling behaviors of a data source by running
multiple annealing runs, which can lead to different conclusions, had one used
point estimates using the usual micro-annealing technique instead. This enables
data-driven decision-making for selecting and optimizing data sources.

</details>


### [294] [Agent-centric learning: from external reward maximization to internal knowledge curation](https://arxiv.org/abs/2507.22255)
*Hanqi Zhou,Fryderyk Mantiuk,David G. Nagy,Charley M. Wu*

Main category: cs.LG

TL;DR: Agent-centric learning paradigm focuses on internal knowledge structures for adaptability, moving beyond external task mastery.


<details>
  <summary>Details</summary>
Motivation: Traditional general intelligence research focuses on external objectives, leading to specialized agents lacking adaptability. This work aims to address this limitation by proposing an agent-centric approach.

Method: The paper introduces representational empowerment as a new objective that measures an agent's capacity to shape its own understanding by focusing on internal representations.

Result: The proposed approach offers a new perspective for designing adaptable intelligent systems by focusing on internal representations as the substrate for computing empowerment.

Conclusion: This paper proposes representational empowerment, an agent-centric learning paradigm focusing on an agent's ability to controllably maintain and diversify its own knowledge structures. This internal focus is posited to foster adaptability and 'preparedness' beyond direct environmental influence.

Abstract: The pursuit of general intelligence has traditionally centered on external
objectives: an agent's control over its environments or mastery of specific
tasks. This external focus, however, can produce specialized agents that lack
adaptability. We propose representational empowerment, a new perspective
towards a truly agent-centric learning paradigm by moving the locus of control
inward. This objective measures an agent's ability to controllably maintain and
diversify its own knowledge structures. We posit that the capacity -- to shape
one's own understanding -- is an element for achieving better ``preparedness''
distinct from direct environmental influence. Focusing on internal
representations as the main substrate for computing empowerment offers a new
lens through which to design adaptable intelligent systems.

</details>


### [295] [Weighted Conditional Flow Matching](https://arxiv.org/abs/2507.22270)
*Sergio Calvo-Ordonez,Matthieu Meunier,Alvaro Cartea,Christoph Reisinger,Yarin Gal,Jose Miguel Hernandez-Lobato*

Main category: cs.LG

TL;DR: W-CFM 是一种新的流匹配方法，通过加权克服了标准 CFM 路径偏差和现有方法对小批量 OT 的依赖问题，实现了高效且高质量的生成。


<details>
  <summary>Details</summary>
Motivation: 标准 CFM 训练出的路径与先验分布和目标分布之间的直线插值有显著偏差，导致生成速度慢且精度低。现有的改进方法虽然能缩短和拉直轨迹，但需要计算成本高昂的小批量最优传输（OT）。

Method: 提出了一种名为加权条件流匹配（W-CFM）的新方法，通过使用吉布斯核函数对训练对进行加权来修改经典的 CFM 损失函数。

Result: W-CFM 在有条件和无条件生成任务中都表现出色，与现有方法相比，在样本质量、保真度和多样性方面具有可比性或更优的性能，同时保持了 CFM 的计算效率。

Conclusion: W-CFM 通过加权训练对以吉布斯核函数，可以恢复熵最优传输耦合，并且在 있다고极限下与小批量最优传输方法等价，从而克服了批量大小带来的计算和性能瓶颈。在实际应用中，W-CFM 在生成样本的质量、保真度和多样性方面与现有方法相当或更优，同时保持了标准 CFM 的计算效率。

Abstract: Conditional flow matching (CFM) has emerged as a powerful framework for
training continuous normalizing flows due to its computational efficiency and
effectiveness. However, standard CFM often produces paths that deviate
significantly from straight-line interpolations between prior and target
distributions, making generation slower and less accurate due to the need for
fine discretization at inference. Recent methods enhance CFM performance by
inducing shorter and straighter trajectories but typically rely on
computationally expensive mini-batch optimal transport (OT). Drawing insights
from entropic optimal transport (EOT), we propose Weighted Conditional Flow
Matching (W-CFM), a novel approach that modifies the classical CFM loss by
weighting each training pair $(x, y)$ with a Gibbs kernel. We show that this
weighting recovers the entropic OT coupling up to some bias in the marginals,
and we provide the conditions under which the marginals remain nearly
unchanged. Moreover, we establish an equivalence between W-CFM and the
minibatch OT method in the large-batch limit, showing how our method overcomes
computational and performance bottlenecks linked to batch size. Empirically, we
test our method on unconditional generation on various synthetic and real
datasets, confirming that W-CFM achieves comparable or superior sample quality,
fidelity, and diversity to other alternative baselines while maintaining the
computational efficiency of vanilla CFM.

</details>


### [296] [Comparing Cluster-Based Cross-Validation Strategies for Machine Learning Model Evaluation](https://arxiv.org/abs/2507.22299)
*Afonso Martini Spezia,Mariana Recamonde-Mendoza*

Main category: cs.LG

TL;DR: 本研究评估了不同交叉验证策略，提出了一种结合 Mini Batch K-Means 和类别分层的混合方法。该方法在平衡数据集上表现良好，但在不平衡数据集上，传统的交叉验证方法效果更好。研究还发现，没有一种聚类算法在所有情况下都表现最佳。


<details>
  <summary>Details</summary>
Motivation: 交叉验证是机器学习中模型评估和防止过拟合的关键技术。然而，现有的交叉验证方法可能生成不能充分代表原始数据集多样性的数据子集（折叠），从而导致有偏差的性能估计。因此，本研究旨在探索和改进交叉验证策略，特别关注基于聚类的方法。

Method: 本研究旨在深入研究基于聚类交叉验证策略，通过实验比较不同聚类算法的性能。此外，提出了一种结合 Mini Batch K-Means 和类别分层的新型交叉验证技术。在 20 个数据集（包括平衡和不平衡数据集）上，使用四种监督学习算法进行了实验，并在偏差、方差和计算成本方面对交叉验证策略进行了比较。

Result: 在平衡数据集上，结合了 Mini Batch K-Means 和类别分层的方法在偏差和方差方面优于其他方法，但计算成本没有显著降低。在不平衡数据集上，传统的交叉验证方法在偏差、方差和计算成本方面表现更好。在聚类算法的比较中，没有发现单一算法在所有情况下都表现优异。

Conclusion: 本研究通过实验比较了不同聚类算法的性能，并提出了一种结合了 Mini Batch K-Means 和类别分层的新的交叉验证技术。结果表明，该技术在平衡数据集上在偏差和方差方面优于其他方法，但在降低计算成本方面效果不显著。在不平衡数据集上，传统的交叉验证方法在偏差、方差和计算成本方面表现更优。在聚类算法的比较中，没有一种算法表现出持续的优越性。总体而言，本研究通过深入了解基于聚类的数据划分技术的潜力，并重申了分层交叉验证等成熟策略的有效性，为改进预测模型评估策略做出了贡献。

Abstract: Cross-validation plays a fundamental role in Machine Learning, enabling
robust evaluation of model performance and preventing overestimation on
training and validation data. However, one of its drawbacks is the potential to
create data subsets (folds) that do not adequately represent the diversity of
the original dataset, which can lead to biased performance estimates. The
objective of this work is to deepen the investigation of cluster-based
cross-validation strategies by analyzing the performance of different
clustering algorithms through experimental comparison. Additionally, a new
cross-validation technique that combines Mini Batch K-Means with class
stratification is proposed. Experiments were conducted on 20 datasets (both
balanced and imbalanced) using four supervised learning algorithms, comparing
cross-validation strategies in terms of bias, variance, and computational cost.
The technique that uses Mini Batch K-Means with class stratification
outperformed others in terms of bias and variance on balanced datasets, though
it did not significantly reduce computational cost. On imbalanced datasets,
traditional stratified cross-validation consistently performed better, showing
lower bias, variance, and computational cost, making it a safe choice for
performance evaluation in scenarios with class imbalance. In the comparison of
different clustering algorithms, no single algorithm consistently stood out as
superior. Overall, this work contributes to improving predictive model
evaluation strategies by providing a deeper understanding of the potential of
cluster-based data splitting techniques and reaffirming the effectiveness of
well-established strategies like stratified cross-validation. Moreover, it
highlights perspectives for increasing the robustness and reliability of model
evaluations, especially in datasets with clustering characteristics.

</details>


### [297] [CS-SHRED: Enhancing SHRED for Robust Recovery of Spatiotemporal Dynamics](https://arxiv.org/abs/2507.22303)
*Romulo B. da Silva,Cássio M. Oishi,Diego Passos,J. Nathan Kutz*

Main category: cs.LG

TL;DR: CS-SHRED是一种结合压缩感知（CS）和浅层递归解码器（SHRED）的新型深度学习架构，用于从不完整或损坏的数据中恢复时空动力学。该方法通过引入CS技术和自适应损失函数，提高了重建的准确性和鲁棒性，在多个应用场景中优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 为了从不完整、压缩或损坏的数据中可靠地恢复时空动力学，研究者们提出了CS-SHRED架构，旨在克服传统方法在传感器稀疏、测量噪声和数据不完整等挑战下的局限性。

Method: 提出了一种名为CS-SHRED的新型深度学习架构，该架构将压缩感知（CS）技术整合到浅层递归解码器（SHRED）中。CS-SHRED采用基于批次的forward框架和L1正则化，以应对传感器稀疏、测量噪声和数据不完整等挑战。此外，该方法还引入了一种自适应损失函数，该函数动态结合了均方误差（MSE）和平均绝对误差（MAE）以及分段信噪比（SNR）正则化，能够在低SNR区域抑制噪声和异常值，同时在高SNR区域保留精细特征。

Result: CS-SHRED在粘弹性流体流动、最大比湿场、海面温度分布和湍流流动等具有挑战性的问题上得到了验证。与传统的SHRED方法相比，CS-SHRED在SSIM、PSNR、归一化误差和LPIPS分数方面均取得了显著更高的重建保真度，能够更好地保留小尺度结构，并增强了对噪声和异常值的鲁棒性。

Conclusion: CS-SHRED通过结合压缩感知（CS）和浅层递归解码器（SHRED）的深度学习架构，在不完整、压缩或损坏的数据中恢复时空动力学方面表现出色，其创新的CS-SHRED设计实现了更高的重建保真度和对噪声及异常值的鲁棒性，可广泛应用于环境、气候和科学数据分析。

Abstract: We present $\textbf{CS-SHRED}$, a novel deep learning architecture that
integrates Compressed Sensing (CS) into a Shallow Recurrent Decoder
($\textbf{SHRED}$) to reconstruct spatiotemporal dynamics from incomplete,
compressed, or corrupted data. Our approach introduces two key innovations.
First, by incorporating CS techniques into the $\textbf{SHRED}$ architecture,
our method leverages a batch-based forward framework with $\ell_1$
regularization to robustly recover signals even in scenarios with sparse sensor
placements, noisy measurements, and incomplete sensor acquisitions. Second, an
adaptive loss function dynamically combines Mean Squared Error (MSE) and Mean
Absolute Error (MAE) terms with a piecewise Signal-to-Noise Ratio (SNR)
regularization, which suppresses noise and outliers in low-SNR regions while
preserving fine-scale features in high-SNR regions.
  We validate $\textbf{CS-SHRED}$ on challenging problems including
viscoelastic fluid flows, maximum specific humidity fields, sea surface
temperature distributions, and rotating turbulent flows. Compared to the
traditional $\textbf{SHRED}$ approach, $\textbf{CS-SHRED}$ achieves
significantly higher reconstruction fidelity - as demonstrated by improved SSIM
and PSNR values, lower normalized errors, and enhanced LPIPS scores-thereby
providing superior preservation of small-scale structures and increased
robustness against noise and outliers.
  Our results underscore the advantages of the jointly trained CS and SHRED
design architecture which includes an LSTM sequence model for characterizing
the temporal evolution with a shallow decoder network (SDN) for modeling the
high-dimensional state space. The SNR-guided adaptive loss function for the
spatiotemporal data recovery establishes $\textbf{CS-SHRED}$ as a promising
tool for a wide range of applications in environmental, climatic, and
scientific data analyses.

</details>


### [298] [Spatial-Temporal Data Mining for Ocean Science: Data, Methodologies, and Opportunities](https://arxiv.org/abs/2307.10803)
*Hanchen Yang,Wengen Li,Shuyu Wang,Hui Li,Jihong Guan,Shuigeng Zhou,Jiannong Cao*

Main category: cs.LG

TL;DR: A survey of spatial-temporal data mining in ocean science, covering data, methods, tasks (prediction, detection, mining, anomaly), and future work to help both computer and ocean scientists.


<details>
  <summary>Details</summary>
Motivation: Existing spatial-temporal data mining (STDM) studies on ocean data are hindered by the data's complexity, regionality, and sparsity. A comprehensive survey is missing, making it difficult for computer scientists to identify research issues and for ocean scientists to apply advanced STDM techniques.

Method: The authors conducted a comprehensive survey of existing STDM studies related to ocean science. They reviewed ST ocean datasets and their characteristics, explored data quality enhancement techniques, classified STDM tasks into prediction, event detection, pattern mining, and anomaly detection, and discussed future research directions.

Result: The paper categorizes STDM studies in ocean science into four main tasks: prediction, event detection, pattern mining, and anomaly detection. It reviews datasets, data quality enhancement methods, and discusses future research opportunities, aiming to facilitate interdisciplinary understanding and application of STDM in ocean science.

Conclusion: This survey provides a comprehensive overview of STDM studies for ocean science, covering datasets, data quality enhancement, task classifications (prediction, event detection, pattern mining, anomaly detection), and future research opportunities. It aims to bridge the gap between computer science and ocean science by helping researchers understand fundamental concepts, techniques, and challenges.

Abstract: With the rapid amassing of spatial-temporal (ST) ocean data, many
spatial-temporal data mining (STDM) studies have been conducted to address
various oceanic issues, including climate forecasting and disaster warning.
Compared with typical ST data (e.g., traffic data), ST ocean data is more
complicated but with unique characteristics, e.g., diverse regionality and high
sparsity. These characteristics make it difficult to design and train STDM
models on ST ocean data. To the best of our knowledge, a comprehensive survey
of existing studies remains missing in the literature, which hinders not only
computer scientists from identifying the research issues in ocean data mining
but also ocean scientists to apply advanced STDM techniques. In this paper, we
provide a comprehensive survey of existing STDM studies for ocean science.
Concretely, we first review the widely-used ST ocean datasets and highlight
their unique characteristics. Then, typical ST ocean data quality enhancement
techniques are explored. Next, we classify existing STDM studies in ocean
science into four types of tasks, i.e., prediction, event detection, pattern
mining, and anomaly detection, and elaborate on the techniques for these tasks.
Finally, promising research opportunities are discussed. This survey can help
scientists from both computer science and ocean science better understand the
fundamental concepts, key techniques, and open challenges of STDM for ocean
science.

</details>


### [299] [Parametrized Multi-Agent Routing via Deep Attention Models](https://arxiv.org/abs/2507.22338)
*Salar Basiri,Dhananjay Tiwari,Srinivasa M. Salapaka*

Main category: cs.LG

TL;DR: 提出了一种名为ParaSDM的可扩展深度学习框架，并通过最短路径网络（SPN）解决了设施选址和路径优化（FLPO）问题。该方法在效率和成本方面均优于现有基线，并达到了与Gurobi相当的最优结果。


<details>
  <summary>Details</summary>
Motivation: 为了解决设施选址和路径优化（FLPO）问题，这是ParaSDM的一个子类。FLPO问题因其混合离散-连续结构和高度非凸目标而成为NP难题。

Method: 提出了一种可扩展的深度学习框架ParaSDM，用于联合优化离散动作策略和共享连续参数。该框架集成了最大熵原理（MEP）和一个名为最短路径网络（SPN）的神经网络模型，该模型是一个置换不变的编码器-解码器，能够逼近MEP解并实现基于梯度的优化。

Result: SPN在策略推理和梯度计算方面比MEP基线快100倍，在广泛的问题规模上平均最优性差距约为6%。研究提出的FLPO方法比元启发式基线成本低10倍以上，同时运行速度更快，并且在1500倍加速的情况下达到了与Gurobi相同的最优成本，为ParaSDM问题树立了新的状态。

Conclusion: 该研究展示了结构化深度模型在解决大规模混合整数优化问题上的潜力，特别是在参数化序列决策制定（ParaSDM）领域。

Abstract: We propose a scalable deep learning framework for parametrized sequential
decision-making (ParaSDM), where multiple agents jointly optimize discrete
action policies and shared continuous parameters. A key subclass of this
setting arises in Facility-Location and Path Optimization (FLPO), where
multi-agent systems must simultaneously determine optimal routes and facility
locations, aiming to minimize the cumulative transportation cost within the
network. FLPO problems are NP-hard due to their mixed discrete-continuous
structure and highly non-convex objective. To address this, we integrate the
Maximum Entropy Principle (MEP) with a neural policy model called the Shortest
Path Network (SPN)-a permutation-invariant encoder-decoder that approximates
the MEP solution while enabling efficient gradient-based optimization over
shared parameters. The SPN achieves up to 100$\times$ speedup in policy
inference and gradient computation compared to MEP baselines, with an average
optimality gap of approximately 6% across a wide range of problem sizes. Our
FLPO approach yields over 10$\times$ lower cost than metaheuristic baselines
while running significantly faster, and matches Gurobi's optimal cost with
annealing at a 1500$\times$ speedup-establishing a new state of the art for
ParaSDM problems. These results highlight the power of structured deep models
for solving large-scale mixed-integer optimization tasks.

</details>


### [300] [MSQ: Memory-Efficient Bit Sparsification Quantization](https://arxiv.org/abs/2507.22349)
*Seokho Han,Seoyeon Yoon,Jinhee Kim,Dongwei Wang,Kang Eun Jeon,Huanrui Yang,Jong Hwan Ko*

Main category: cs.LG

TL;DR: MSQ是一种新颖的内存高效比特稀疏化量化方法，它通过可微分的LSB计算和Hessian信息实现高效训练，可显著减少训练参数和时间，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 随着深度神经网络（DNN）在移动和边缘设备上的部署日益增多，优化模型效率至关重要。混合精度量化因其在效率和准确性方面优于均匀量化而备受青睐，但为各层找到最佳精度具有挑战性。

Method: MSQ采用舍入-裁剪量化器，对模型权重的最小有效位（LSB）进行可微分计算，并利用正则化在这些LSB中引入稀疏性，从而无需显式拆分比特级参数即可实现有效的精度降低。此外，MSQ结合了Hessian信息，可以同时修剪多个LSB，进一步提高训练效率。

Result: 实验结果表明，与以前的比特级量化相比，MSQ在可训练参数数量上最多可减少8.00倍，训练时间最多可减少86%，同时保持了具有竞争力的准确性和压缩率。

Conclusion: MSQ是一种实用的解决方案，可在资源受限的设备上训练高效的深度神经网络。

Abstract: As deep neural networks (DNNs) see increased deployment on mobile and edge
devices, optimizing model efficiency has become crucial. Mixed-precision
quantization is widely favored, as it offers a superior balance between
efficiency and accuracy compared to uniform quantization. However, finding the
optimal precision for each layer is challenging. Recent studies utilizing
bit-level sparsity have shown promise, yet they often introduce substantial
training complexity and high GPU memory requirements. In this paper, we propose
Memory-Efficient Bit Sparsification Quantization (MSQ), a novel approach that
addresses these limitations. MSQ applies a round-clamp quantizer to enable
differentiable computation of the least significant bits (LSBs) from model
weights. It further employs regularization to induce sparsity in these LSBs,
enabling effective precision reduction without explicit bit-level parameter
splitting. Additionally, MSQ incorporates Hessian information, allowing the
simultaneous pruning of multiple LSBs to further enhance training efficiency.
Experimental results show that MSQ achieves up to 8.00x reduction in trainable
parameters and up to 86% reduction in training time compared to previous
bit-level quantization, while maintaining competitive accuracy and compression
rates. This makes it a practical solution for training efficient DNNs on
resource-constrained devices.

</details>


### [301] [Prediction of acoustic field in 1-D uniform duct with varying mean flow and temperature using neural networks](https://arxiv.org/abs/2507.22370)
*D. Veerababu,Prasanta K. Ghosh*

Main category: cs.LG

TL;DR: 研究利用物理约束神经网络求解一维管道内声波传播问题，并成功预测了声场，验证了该方法在声学领域的应用潜力。


<details>
  <summary>Details</summary>
Motivation: 为了探索将物理规律约束的神经网络作为数值工具在声学领域应用的替代方案，并研究温度梯度对声场的影响。

Method: 提出了一种将物理规律约束的神经网络应用于求解一维管道内声波传播问题的数值方法。该方法将偏微分方程问题转化为无约束优化问题，并使用神经网络进行求解。

Result: 神经网络成功预测了声压和粒子速度，并通过与传统的龙格-库塔求解器进行比较，验证了其有效性。

Conclusion: 该研究展示了利用机器学习技术（如迁移学习和自动微分）解决声学问题的潜力，特别是用于分析一维管道中声波在非均匀介质传播的问题。

Abstract: Neural networks constrained by the physical laws emerged as an alternate
numerical tool. In this paper, the governing equation that represents the
propagation of sound inside a one-dimensional duct carrying a heterogeneous
medium is derived. The problem is converted into an unconstrained optimization
problem and solved using neural networks. Both the acoustic state variables:
acoustic pressure and particle velocity are predicted and validated with the
traditional Runge-Kutta solver. The effect of the temperature gradient on the
acoustic field is studied. Utilization of machine learning techniques such as
transfer learning and automatic differentiation for acoustic applications is
demonstrated.

</details>


### [302] [Spec-VLA: Speculative Decoding for Vision-Language-Action Models with Relaxed Acceptance](https://arxiv.org/abs/2507.22424)
*Songsheng Wang,Rucheng Yu,Zhihang Yuan,Chao Yu,Feng Gao,Yu Wang,Derek F. Wong*

Main category: cs.LG

TL;DR: 提出Spec-VLA框架，通过利用动作标记的相对距离来加速VLA模型，实现了1.42倍的速度提升。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型虽然利用了视觉语言模型（VLM）的强大能力，但其参数量大和自回归解码的特性导致计算成本高。为了解决VLM在VLA模型中的加速问题，本研究探索了投机解码（SD）技术的应用，该技术已被证明可以加速大型语言模型（LLM）。

Method: 提出了一种名为Spec-VLA的投机解码（SD）框架，用于加速视觉-语言-动作（VLA）模型。为了克服直接应用SD到VLA模型（特别是动作预测任务）效果不佳的问题，引入了一种通过放松接受条件来利用VLA模型动作标记的相对距离的机制，以提高生成速度。

Result: Spec-VLA框架在多种测试场景中均表现出有效性，与OpenVLA基线相比，实现了1.42倍的速度提升，同时成功率没有下降。此外，所提出的策略将接受长度提高了44%。

Conclusion: Spec-VLA框架在VLA预测任务中实现了1.42倍的加速，同时保持了成功率，并显著提高了接受长度（44%），这表明投机执行在VLA预测场景中有广泛的应用潜力。

Abstract: Vision-Language-Action (VLA) models have made substantial progress by
leveraging the robust capabilities of Visual Language Models (VLMs). However,
VLMs' significant parameter size and autoregressive (AR) decoding nature impose
considerable computational demands on VLA models. While Speculative Decoding
(SD) has shown efficacy in accelerating Large Language Models (LLMs) by
incorporating efficient drafting and parallel verification, allowing multiple
tokens to be generated in one forward pass, its application to VLA models
remains unexplored. This work introduces Spec-VLA, an SD framework designed to
accelerate VLA models. Due to the difficulty of the action prediction task and
the greedy decoding mechanism of the VLA models, the direct application of the
advanced SD framework to the VLA prediction task yields a minor speed
improvement. To boost the generation speed, we propose an effective mechanism
to relax acceptance utilizing the relative distances represented by the action
tokens of the VLA model. Empirical results across diverse test scenarios affirm
the effectiveness of the Spec-VLA framework, and further analysis substantiates
the impact of our proposed strategies, which enhance the acceptance length by
44%, achieving 1.42 times speedup compared with the OpenVLA baseline, without
compromising the success rate. The success of the Spec-VLA framework highlights
the potential for broader application of speculative execution in VLA
prediction scenarios.

</details>


### [303] [Multimodal Late Fusion Model for Problem-Solving Strategy Classification in a Machine Learning Game](https://arxiv.org/abs/2507.22426)
*Clemens Witt,Thiemo Leonhardt,Nadine Bergner,Mareen Grillenberger*

Main category: cs.LG

TL;DR: 本研究提出了一种多模态模型，该模型结合屏幕录制和游戏内动作数据，比仅使用单一数据源的模型更准确地对学生的问题解决策略进行分类，准确率提高了 15%。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖于抽象的游戏日志数据，这可能忽略与学习者认知策略相关的细微行为线索。

Method: 提出了一种多模态 late fusion 模型，该模型整合了基于屏幕录制的视觉数据和结构化的游戏内动作序列，以对学生的问题解决策略进行分类。

Result: 在本项针对中学生（N=149）玩多点触控教育游戏进行的试点研究中，融合模型优于单一模态基线模型，将分类准确率提高了 15% 以上。

Conclusion: 本研究结果凸显了多模态机器学习在策略敏感评估和交互式学习环境中的自适应支持方面的潜力。

Abstract: Machine learning models are widely used to support stealth assessment in
digital learning environments. Existing approaches typically rely on abstracted
gameplay log data, which may overlook subtle behavioral cues linked to
learners' cognitive strategies. This paper proposes a multimodal late fusion
model that integrates screencast-based visual data and structured in-game
action sequences to classify students' problem-solving strategies. In a pilot
study with secondary school students (N=149) playing a multitouch educational
game, the fusion model outperformed unimodal baseline models, increasing
classification accuracy by over 15%. Results highlight the potential of
multimodal ML for strategy-sensitive assessment and adaptive support in
interactive learning contexts.

</details>


### [304] [Theoretical Analysis of Relative Errors in Gradient Computations for Adversarial Attacks with CE Loss](https://arxiv.org/abs/2507.22428)
*Yunrui Yu,Hang Su,Cheng-zhong Xu,Zhizhong Su,Jun Zhu*

Main category: cs.LG

TL;DR: 本文分析了梯度下降对抗攻击中浮点误差的问题，提出了一种新的T-MIFPE损失函数，并通过实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 梯度下降对抗攻击中的交叉熵损失函数由于浮点运算引起的梯度计算相对误差而存在估计过高的问题。

Method: 对梯度下降对抗攻击中交叉熵损失函数的相对误差进行了严格的理论分析，涵盖了四种不同的场景：(i) 不成功的非目标攻击，(ii) 成功的非目标攻击，(iii) 不成功的 目标攻击，(iv) 成功的 目标攻击。识别出浮点下溢和舍入是关键因素。提出了一种名为T-MIFPE（理论最小化影响浮点误差）的损失函数。

Result: T-MIFPE相比于交叉熵（CE）、C&W、DLR和MIFPE等现有损失函数，在攻击效力和鲁棒性评估准确性方面表现更优。

Conclusion: T-MIFPE 损失函数通过引入最优缩放因子 $T = t^*$ 来最小化浮点误差的影响，提高了梯度计算的准确性，在MNIST、CIFAR-10和CIFAR-100数据集上的实验表明，T-MIFPE在攻击效力和鲁棒性评估准确性方面优于现有的损失函数。

Abstract: Gradient-based adversarial attacks using the Cross-Entropy (CE) loss often
suffer from overestimation due to relative errors in gradient computation
induced by floating-point arithmetic. This paper provides a rigorous
theoretical analysis of these errors, conducting the first comprehensive study
of floating-point computation errors in gradient-based attacks across four
distinct scenarios: (i) unsuccessful untargeted attacks, (ii) successful
untargeted attacks, (iii) unsuccessful targeted attacks, and (iv) successful
targeted attacks. We establish theoretical foundations characterizing the
behavior of relative numerical errors under different attack conditions,
revealing previously unknown patterns in gradient computation instability, and
identify floating-point underflow and rounding as key contributors. Building on
this insight, we propose the Theoretical MIFPE (T-MIFPE) loss function, which
incorporates an optimal scaling factor $T = t^*$ to minimize the impact of
floating-point errors, thereby enhancing the accuracy of gradient computation
in adversarial attacks. Extensive experiments on the MNIST, CIFAR-10, and
CIFAR-100 datasets demonstrate that T-MIFPE outperforms existing loss
functions, including CE, C\&W, DLR, and MIFPE, in terms of attack potency and
robustness evaluation accuracy.

</details>


### [305] [RANA: Robust Active Learning for Noisy Network Alignment](https://arxiv.org/abs/2507.22434)
*Yixuan Nan,Xixun Lin,Yanmin Shang,Zhuofan Li,Can Zhao,Yanan Cao*

Main category: cs.LG

TL;DR: RANA is a robust active learning framework for noisy network alignment that addresses both structural and labeling noise using novel modules, outperforming existing methods.


<details>
  <summary>Details</summary>
Motivation: Existing network alignment works mainly focus on label sparsity but overlook the issue of noise (structural noise and labeling noise), which can undermine model performance.

Method: RANA framework, which includes a Noise-aware Selection Module and a Label Denoising Module. The Noise-aware Selection Module uses a noise-aware maximization objective with a cleanliness score to handle structural noise. The Label Denoising Module uses a multi-source fusion denoising strategy with model and twin node pairs labeling to handle labeling noise.

Result: RANA effectively tackles structure noise and label noise while addressing anchor link sparsity, improving the robustness of network alignment models.

Conclusion: RANA outperforms state-of-the-art active learning-based methods in alignment accuracy on three real-world datasets.

Abstract: Network alignment has attracted widespread attention in various fields.
However, most existing works mainly focus on the problem of label sparsity,
while overlooking the issue of noise in network alignment, which can
substantially undermine model performance. Such noise mainly includes
structural noise from noisy edges and labeling noise caused by human-induced
and process-driven errors. To address these problems, we propose RANA, a Robust
Active learning framework for noisy Network Alignment. RANA effectively tackles
both structure noise and label noise while addressing the sparsity of anchor
link annotations, which can improve the robustness of network alignment models.
Specifically, RANA introduces the proposed Noise-aware Selection Module and the
Label Denoising Module to address structural noise and labeling noise,
respectively. In the first module, we design a noise-aware maximization
objective to select node pairs, incorporating a cleanliness score to address
structural noise. In the second module, we propose a novel multi-source fusion
denoising strategy that leverages model and twin node pairs labeling to provide
more accurate labels for node pairs. Empirical results on three real-world
datasets demonstrate that RANA outperforms state-of-the-art active
learning-based methods in alignment accuracy. Our code is available at
https://github.com/YXNan0110/RANA.

</details>


### [306] [RCR-AF: Enhancing Model Generalization via Rademacher Complexity Reduction Activation Function](https://arxiv.org/abs/2507.22446)
*Yunrui Yu,Kafeng Wang,Hang Su,Jun Zhu*

Main category: cs.LG

TL;DR: A new activation function, RCR-AF, enhances deep neural network robustness against adversarial attacks by controlling model sparsity and capacity.


<details>
  <summary>Details</summary>
Motivation: Deep neural networks are vulnerable to adversarial attacks, and activation functions are an underexplored component for enhancing model robustness.

Method: Propose a Rademacher Complexity Reduction Activation Function (RCR-AF) that combines GELU's advantages with ReLU's monotonicity and includes clipping mechanisms controlled by hyperparameters alpha and gamma.

Result: RCR-AF consistently outperforms widely-used alternatives in both clean accuracy and adversarial robustness.

Conclusion: RCR-AF outperforms ReLU, GELU, and Swish in both clean accuracy and adversarial robustness.

Abstract: Despite their widespread success, deep neural networks remain critically
vulnerable to adversarial attacks, posing significant risks in safety-sensitive
applications. This paper investigates activation functions as a crucial yet
underexplored component for enhancing model robustness. We propose a Rademacher
Complexity Reduction Activation Function (RCR-AF), a novel activation function
designed to improve both generalization and adversarial resilience. RCR-AF
uniquely combines the advantages of GELU (including smoothness, gradient
stability, and negative information retention) with ReLU's desirable
monotonicity, while simultaneously controlling both model sparsity and capacity
through built-in clipping mechanisms governed by two hyperparameters, $\alpha$
and $\gamma$. Our theoretical analysis, grounded in Rademacher complexity,
demonstrates that these parameters directly modulate the model's Rademacher
complexity, offering a principled approach to enhance robustness. Comprehensive
empirical evaluations show that RCR-AF consistently outperforms widely-used
alternatives (ReLU, GELU, and Swish) in both clean accuracy under standard
training and in adversarial robustness within adversarial training paradigms.

</details>


### [307] [Proto-EVFL: Enhanced Vertical Federated Learning via Dual Prototype with Extremely Unaligned Data](https://arxiv.org/abs/2507.22488)
*Wei Guo,Yiyang Duan,Zhaojun Hu,Yiqi Tong,Fuzhen Zhuang,Xiao Zhang,Jin Dong,Ruofan Wu,Tengfei Liu,Yifan Sun*

Main category: cs.LG

TL;DR: Proto-EVFL是首个解决垂直联邦学习（VFL）中类别不平衡问题的双层优化框架。它通过类原型和概率双原型学习方案，利用条件最优传输和混合先验引导来选择样本，并结合自适应门控特征聚合来解决特征不一致问题，在零样本场景下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有的垂直联邦学习（VFL）框架在应对跨多个企业中的类别严重不平衡问题时存在不足，这会导致特征表示不足和模型预测空间受限。具体来说，内部不平衡会导致局部模型偏差，而跨方不平衡则会导致特征贡献不一致。

Method: Proto-EVFL框架，采用双原型设计，包括学习类间关系的类原型以及通过条件最优传输成本和类先验概率动态选择未对齐样本的概率双原型学习方案。此外，还设计了混合先验引导模块来结合局部和全局类先验概率，并通过自适应门控特征聚合策略来解决特征贡献不一致问题。

Result: Proto-EVFL框架在各种数据集上的广泛实验验证了其优越性。在零样本场景下（只有一个未见过的类），其性能比基线方法至少高出6.97%。此外，作为VFL领域的首个双层优化框架，Proto-EVFL被证明具有1/sqrt(T)的收敛速率。

Conclusion: Proto-EVFL通过引入类原型和概率双原型学习方案，解决了VFL中由局部和全局类不平衡引起的模型偏差和特征贡献不一致问题。该框架通过条件最优传输和混合先验引导的样本选择，以及自适应门控特征聚合策略，有效提升了模型的特征表示和预测能力，即使在零样本场景下也表现出优越性。

Abstract: In vertical federated learning (VFL), multiple enterprises address aligned
sample scarcity by leveraging massive locally unaligned samples to facilitate
collaborative learning. However, unaligned samples across different parties in
VFL can be extremely class-imbalanced, leading to insufficient feature
representation and limited model prediction space. Specifically,
class-imbalanced problems consist of intra-party class imbalance and
inter-party class imbalance, which can further cause local model bias and
feature contribution inconsistency issues, respectively. To address the above
challenges, we propose Proto-EVFL, an enhanced VFL framework via dual
prototypes. We first introduce class prototypes for each party to learn
relationships between classes in the latent space, allowing the active party to
predict unseen classes. We further design a probabilistic dual prototype
learning scheme to dynamically select unaligned samples by conditional optimal
transport cost with class prior probability. Moreover, a mixed prior guided
module guides this selection process by combining local and global class prior
probabilities. Finally, we adopt an \textit{adaptive gated feature aggregation
strategy} to mitigate feature contribution inconsistency by dynamically
weighting and aggregating local features across different parties. We proved
that Proto-EVFL, as the first bi-level optimization framework in VFL, has a
convergence rate of 1/\sqrt T. Extensive experiments on various datasets
validate the superiority of our Proto-EVFL. Even in a zero-shot scenario with
one unseen class, it outperforms baselines by at least 6.97%

</details>


### [308] [LoReUn: Data Itself Implicitly Provides Cues to Improve Machine Unlearning](https://arxiv.org/abs/2507.22499)
*Xiang Li,Qianli Shen,Haonan Wang,Kenji Kawaguchi*

Main category: cs.LG

TL;DR: LoReUn 是一种新的机器遗忘方法，通过根据数据本身的学习损失来加权数据，提高了遗忘特定数据的效率，尤其是在处理难遗忘数据时。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法在遗忘难遗忘数据时效率低下，因为它们通常对所有待遗忘的数据分配相同的权重。

Method: LoReUn 是一种即插即用策略，在遗忘过程中动态地重新加权数据，其灵感来源于数据本身的学习损失可以反映其遗忘的难易程度。

Result: LoReUn 在图像分类和文本到图像扩散模型生成任务中，显著减少了现有遗忘方法与精确遗忘之间的性能差距。

Conclusion: LoReUn 通过动态加权数据，有效解决了现有机器遗忘方法难以遗忘难遗忘数据的问题，在图像分类和生成任务上显著缩小了现有方法与精确遗忘之间的差距，并有效增强了文本到图像扩散模型有害内容生成的防护能力。

Abstract: Recent generative models face significant risks of producing harmful content,
which has underscored the importance of machine unlearning (MU) as a critical
technique for eliminating the influence of undesired data. However, existing MU
methods typically assign the same weight to all data to be forgotten, which
makes it difficult to effectively forget certain data that is harder to unlearn
than others. In this paper, we empirically demonstrate that the loss of data
itself can implicitly reflect its varying difficulty. Building on this insight,
we introduce Loss-based Reweighting Unlearning (LoReUn), a simple yet effective
plug-and-play strategy that dynamically reweights data during the unlearning
process with minimal additional computational overhead. Our approach
significantly reduces the gap between existing MU methods and exact unlearning
in both image classification and generation tasks, effectively enhancing the
prevention of harmful content generation in text-to-image diffusion models.

</details>


### [309] [Geometry of nonlinear forecast reconciliation](https://arxiv.org/abs/2507.22500)
*Lorenzo Nespoli,Anubhab Biswas,Vasco Medici*

Main category: cs.LG

TL;DR: 该研究通过推导新理论，解决了预测一致性在非线性情境下的误差可约减问题，并提供了可复现的实现包。


<details>
  <summary>Details</summary>
Motivation: 现有预测一致性技术在非线性情境下的误差可约减理论尚不完善，与线性情境下的理论（如 Panagiotelis et al. (2021) 的研究）存在差距。

Method: 通过为不同类型（常曲率符号和非常曲率符号）的وهyperplanes 和向量值函数推导精确的理论公式和概率保证，来解决非线性情境下的预测一致性问题。

Result: 推导了适用于常曲率وهyperplanes 的精确理论公式，并为非恒曲率وهyperplanes 和一般向量值函数提供了概率保证，证明了在非线性情境下的误差可约减性。

Conclusion: 该论文填补了在非线性情境下，预测误差可约减的理论空白，为概率预测一致性方法提供了重要的理论基础。

Abstract: Forecast reconciliation, an ex-post technique applied to forecasts that must
satisfy constraints, has been a prominent topic in the forecasting literature
over the past two decades. Recently, several efforts have sought to extend
reconciliation methods to the probabilistic settings. Nevertheless, formal
theorems demonstrating error reduction in nonlinear contexts, analogous to
those presented in Panagiotelis et al.(2021), are still lacking. This paper
addresses that gap by establishing such theorems for various classes of
nonlinear hypersurfaces and vector-valued functions. Specifically, we derive an
exact analog of Theorem 3.1 from Panagiotelis et al.(2021) for hypersurfaces
with constant-sign curvature. Additionally, we provide probabilistic guarantees
for the broader case of hypersurfaces with non-constant-sign curvature and for
general vector-valued functions. To support reproducibility and practical
adoption, we release a JAX-based Python package, \emph{to be released upon
publication}, implementing the presented theorems and reconciliation
procedures.

</details>


### [310] [SmilesT5: Domain-specific pretraining for molecular language models](https://arxiv.org/abs/2507.22514)
*Philip Spence,Brooks Paige,Anne Osbourn*

Main category: cs.LG

TL;DR: 本文提出了一种新的分子语言预训练方法，使用领域特定的文本到文本任务，在分子属性预测任务上取得了更好的性能，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 随着自然语言处理在理解复杂人类语言方面的进步，以及在大语言模型中成功应用掩码语言建模，本文旨在探索将这些自然语言处理的技术应用于分子语言（SMILES字符串）学习，以改进分子属性预测任务。

Method: 本文提出了一种新的领域特定文本到文本预训练任务，并将其应用于基于SMILES字符串表示的分子语言学习。通过消融研究，评估了数据和计算效率。最后，将预训练的嵌入作为下游机器学习分类器的固定输入。

Result: 所提出的预训练任务在六个分子属性预测基准测试中取得了优于传统方法和先前微调任务的性能。消融研究表明，该方法提高了数据和计算效率。预训练的嵌入作为固定特征输入下游模型，在计算开销大大降低的情况下，达到了与微调相当的性能。

Conclusion: 文中提出的领域特定预训练任务在六个基于分类的分子属性预测基准测试中，相对于传统的基于似然的训练和先前提出的微调任务，均取得了改进的性能。此外，通过实验验证了这些预训练任务在数据和计算效率方面的优势。最终，该模型预训练的嵌入可以作为下游机器学习分类器的固定输入，在性能相当的情况下，计算开销大大降低。

Abstract: Molecular property prediction is an increasingly critical task within drug
discovery and development. Typically, neural networks can learn molecular
properties using graph-based, language-based or feature-based methods. Recent
advances in natural language processing have highlighted the capabilities of
neural networks to learn complex human language using masked language
modelling. These approaches to training large transformer-based deep learning
models have also been used to learn the language of molecules, as represented
by simplified molecular-input line-entry system (SMILES) strings. Here, we
present novel domain-specific text-to-text pretraining tasks that yield
improved performance in six classification-based molecular property prediction
benchmarks, relative to both traditional likelihood-based training and
previously proposed fine-tuning tasks. Through ablation studies, we show that
data and computational efficiency can be improved by using these
domain-specific pretraining tasks. Finally, the pretrained embeddings from the
model can be used as fixed inputs into a downstream machine learning classifier
and yield comparable performance to finetuning but with much lower
computational overhead.

</details>


### [311] [HGCN(O): A Self-Tuning GCN HyperModel Toolkit for Outcome Prediction in Event-Sequence Data](https://arxiv.org/abs/2507.22524)
*Fang Wang,Paolo Ceravolo,Ernesto Damiani*

Main category: cs.LG

TL;DR: HGCN(O) is a new toolkit using GCN models for predicting event sequences. It works well on both balanced and unbalanced data, outperforming older methods. It can be used for things like predicting business process outcomes.


<details>
  <summary>Details</summary>
Motivation: To develop a self-tuning toolkit using GCN models for event sequence prediction that optimizes prediction accuracy and stability, particularly for unbalanced datasets, and to demonstrate its superiority over traditional methods.

Method: The study proposes HGCN(O), a self-tuning toolkit employing Graph Convolutional Network (GCN) models for event sequence prediction. It integrates four GCN architectures (O-GCN, T-GCN, TP-GCN, TE-GCN) across GCNConv and GraphConv layers, incorporating multiple graph representations of event sequences with varied node/graph attributes and temporal dependencies via edge weights to optimize prediction accuracy and stability for both balanced and unbalanced datasets.

Result: Extensive experiments show that HGCN(O) with GCNConv models performs better on unbalanced data, while all models perform consistently on balanced data. The toolkit outperforms traditional approaches.

Conclusion: HGCN(O) toolkit using GCN models demonstrates superior performance over traditional approaches for event sequence prediction, with GCNConv models excelling on unbalanced data and consistent performance on balanced data.

Abstract: We propose HGCN(O), a self-tuning toolkit using Graph Convolutional Network
(GCN) models for event sequence prediction. Featuring four GCN architectures
(O-GCN, T-GCN, TP-GCN, TE-GCN) across the GCNConv and GraphConv layers, our
toolkit integrates multiple graph representations of event sequences with
different choices of node- and graph-level attributes and in temporal
dependencies via edge weights, optimising prediction accuracy and stability for
balanced and unbalanced datasets. Extensive experiments show that GCNConv
models excel on unbalanced data, while all models perform consistently on
balanced data. Experiments also confirm the superior performance of HGCN(O)
over traditional approaches. Applications include Predictive Business Process
Monitoring (PBPM), which predicts future events or states of a business process
based on event logs.

</details>


### [312] [FGFP: A Fractional Gaussian Filter and Pruning for Deep Neural Networks Compression](https://arxiv.org/abs/2507.22527)
*Kuan-Ting Tu,Po-Hsien Yu,Yu-Syuan Tseng,Shao-Yi Chien*

Main category: cs.LG

TL;DR: 提出了一种名为FGFP的框架，通过分数高斯滤波器和自适应非结构化剪枝来压缩深度神经网络，在保持较高准确率的同时显著减小模型尺寸，特别适用于边缘设备。


<details>
  <summary>Details</summary>
Motivation: 解决在边缘设备上部署深度神经网络（DNN）的挑战，因为DNN对边缘设备来说负担过重。

Method: 提出了一种名为FGFP的框架，该框架集成了分数高斯滤波器（FGF）和自适应非结构化剪枝（AUP）。FGF利用分数阶微分和高斯函数构建，并使用Grünwald-Letnikov分数阶导数近似来降低计算复杂性，每个核的参数最小化为七个。

Result: 在CIFAR-10上，ResNet-20的准确率下降了1.52%，模型大小减少了85.2%。在ImageNet-2012上，ResNet-50的准确率下降了1.63%，模型大小减少了69.1%。

Conclusion: FGFP框架在准确性和压缩率方面优于近期方法。

Abstract: Network compression techniques have become increasingly important in recent
years because the loads of Deep Neural Networks (DNNs) are heavy for edge
devices in real-world applications. While many methods compress neural network
parameters, deploying these models on edge devices remains challenging. To
address this, we propose the fractional Gaussian filter and pruning (FGFP)
framework, which integrates fractional-order differential calculus and Gaussian
function to construct fractional Gaussian filters (FGFs). To reduce the
computational complexity of fractional-order differential operations, we
introduce Gr\"unwald-Letnikov fractional derivatives to approximate the
fractional-order differential equation. The number of parameters for each
kernel in FGF is minimized to only seven. Beyond the architecture of Fractional
Gaussian Filters, our FGFP framework also incorporates Adaptive Unstructured
Pruning (AUP) to achieve higher compression ratios. Experiments on various
architectures and benchmarks show that our FGFP framework outperforms recent
methods in accuracy and compression. On CIFAR-10, ResNet-20 achieves only a
1.52% drop in accuracy while reducing the model size by 85.2%. On ImageNet2012,
ResNet-50 achieves only a 1.63% drop in accuracy while reducing the model size
by 69.1%.

</details>


### [313] [Accident-Driven Congestion Prediction and Simulation: An Explainable Framework Using Advanced Clustering and Bayesian Networks](https://arxiv.org/abs/2507.22529)
*Kranthi Kumar Talluri,Galia Weidl,Vaishnavi Kasuluru*

Main category: cs.LG

TL;DR: 本研究提出了一种新的方法来预测交通事故对城市交通拥堵的影响，该方法结合了 AutoML、DEC 和 BN 技术，并使用 SUMO 进行了验证，结果表明该方法非常有效。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决城市地区因事故等不确定性因素引起的交通拥堵问题，因为事故的连锁反应会导致延误、排放增加和安全问题。

Method: 本研究使用 AutoML 增强的深度嵌入聚类 (DEC) 来标记事故数据，并利用贝叶斯网络 (BN) 预测拥堵概率。此外，还使用 SUMO 模拟来验证 BN 预测的准确性。

Result: AutoML 增强的 DEC 优于传统聚类方法，并且所提出的 BN 模型实现了 95.6% 的准确率，表明其能够理解事故导致拥堵的复杂关系。SUMO 模拟验证结果表明，BN 模型预测的拥堵状态与 SUMO 的结果非常吻合。

Conclusion: 本研究提出的 Auto-ML 增强 DEC 和 BN 模型能够准确预测事故对城市交通拥堵的影响，并已被 SUMO 模拟验证，证明了其在高可靠性方面的能力。

Abstract: Traffic congestion due to uncertainties, such as accidents, is a significant
issue in urban areas, as the ripple effect of accidents causes longer delays,
increased emissions, and safety concerns. To address this issue, we propose a
robust framework for predicting the impact of accidents on congestion. We
implement Automated Machine Learning (AutoML)-enhanced Deep Embedding
Clustering (DEC) to assign congestion labels to accident data and predict
congestion probability using a Bayesian Network (BN). The Simulation of Urban
Mobility (SUMO) simulation is utilized to evaluate the correctness of BN
predictions using evidence-based scenarios. Results demonstrate that the
AutoML-enhanced DEC has outperformed traditional clustering approaches. The
performance of the proposed BN model achieved an overall accuracy of 95.6%,
indicating its ability to understand the complex relationship of accidents
causing congestion. Validation in SUMO with evidence-based scenarios
demonstrated that the BN model's prediction of congestion states closely
matches those of SUMO, indicating the high reliability of the proposed BN model
in ensuring smooth urban mobility.

</details>


### [314] [Pre-trained Models Perform the Best When Token Distributions Follow Zipf's Law](https://arxiv.org/abs/2507.22543)
*Yanjin He,Qingkai Zeng,Meng Jiang*

Main category: cs.LG

TL;DR: Vocabulary size selection can be principled by analyzing token frequency distributions through Zipf's law, as aligning with Zipfian scaling improves model efficiency and effectiveness across various domains.


<details>
  <summary>Details</summary>
Motivation: The motivation is that tokenization is a fundamental NLP step, and the choice of vocabulary size significantly impacts model performance, yet optimal vocabulary size selection remains underexplored and often relies on heuristics or dataset-specific choices.

Method: The method involves analyzing token frequency distributions through Zipf's law to determine optimal vocabulary size, showing that downstream task performance correlates with how closely token distributions follow power-law behavior.

Result: Extensive experiments across NLP, genomics, and chemistry demonstrate that aligning with Zipfian scaling improves both model efficiency and effectiveness, with models consistently achieving peak performance when the token distribution closely adheres to Zipf's law.

Conclusion: Zipfian alignment serves as a robust and generalizable criterion for vocabulary size selection, with models achieving peak performance when token distributions closely adhere to Zipf's law.

Abstract: Tokenization is a fundamental step in natural language processing (NLP) and
other sequence modeling domains, where the choice of vocabulary size
significantly impacts model performance. Despite its importance, selecting an
optimal vocabulary size remains underexplored, typically relying on heuristics
or dataset-specific choices. In this work, we propose a principled method for
determining the vocabulary size by analyzing token frequency distributions
through Zipf's law. We show that downstream task performance correlates with
how closely token distributions follow power-law behavior, and that aligning
with Zipfian scaling improves both model efficiency and effectiveness.
Extensive experiments across NLP, genomics, and chemistry demonstrate that
models consistently achieve peak performance when the token distribution
closely adheres to Zipf's law, establishing Zipfian alignment as a robust and
generalizable criterion for vocabulary size selection.

</details>


### [315] [DeepC4: Deep Conditional Census-Constrained Clustering for Large-scale Multitask Spatial Disaggregation of Urban Morphology](https://arxiv.org/abs/2507.22554)
*Joshua Dimasaka,Christian Geiß,Emily So*

Main category: cs.LG

TL;DR: 研究提出了一种新的深度学习方法 DeepC4，通过结合人口普查数据和卫星图像，提高了城市形态制图的精度，特别是在发展中国家，为可持续发展提供了新的技术支持。


<details>
  <summary>Details</summary>
Motivation: 为了解发展中经济体在可持续发展和减少灾害风险方面的全球进展，需要一种更精确的空间分解技术来克服现有方法在局部差异和模型不确定性方面的挑战。

Method: 提出了一种名为深度条件人口普查约束聚类（DeepC4）的新型深度学习空间分解方法，该方法将本地人口普查统计数据作为聚类级别的约束，并结合卫星图像模式的多任务联合学习中的多条件标签关系。

Result: 与 GEM 和 METEOR 项目相比，该方法提高了卢旺达城市形态（特别是建筑暴露和物理脆弱性）的制图质量，并在第三级行政区划层面使用了 2022 年的人口普查数据。

Conclusion: 该研究提出了一种名为深度条件人口普查约束聚类（DeepC4）的新型深度学习空间分解方法，该方法将本地人口普查统计数据作为聚类级别的约束，并结合卫星图像模式的多任务联合学习中的多条件标签关系。与 GEM 和 METEOR 项目相比，该方法提高了卢旺达城市形态（特别是建筑暴露和物理脆弱性）的制图质量，并在第三级行政区划层面使用了 2022 年的人口普查数据。

Abstract: To understand our global progress for sustainable development and disaster
risk reduction in many developing economies, two recent major initiatives - the
Uniform African Exposure Dataset of the Global Earthquake Model (GEM)
Foundation and the Modelling Exposure through Earth Observation Routines
(METEOR) Project - implemented classical spatial disaggregation techniques to
generate large-scale mapping of urban morphology using the information from
various satellite imagery and its derivatives, geospatial datasets of the built
environment, and subnational census statistics. However, the local discrepancy
with well-validated census statistics and the propagated model uncertainties
remain a challenge in such coarse-to-fine-grained mapping problems,
specifically constrained by weak and conditional label supervision. Therefore,
we present Deep Conditional Census-Constrained Clustering (DeepC4), a novel
deep learning-based spatial disaggregation approach that incorporates local
census statistics as cluster-level constraints while considering multiple
conditional label relationships in a joint multitask learning of the patterns
of satellite imagery. To demonstrate, compared to GEM and METEOR, we enhanced
the quality of Rwandan maps of urban morphology, specifically building exposure
and physical vulnerability, at the third-level administrative unit from the
2022 census. As the world approaches the conclusion of our global frameworks in
2030, our work has offered a new deep learning-based mapping technique towards
a spatial auditing of our existing coarse-grained derived information at large
scales.

</details>


### [316] [VAR: Visual Analysis for Rashomon Set of Machine Learning Models' Performance](https://arxiv.org/abs/2507.22556)
*Yuanzhe Jin*

Main category: cs.LG

TL;DR: A new visualization tool called VAR, using heatmaps and scatter plots, helps compare machine learning models in a Rashomon set to find the best performing ones for specific situations.


<details>
  <summary>Details</summary>
Motivation: Traditionally, analysis of Rashomon sets focused on vertical structural analysis, with a lack of effective visualization methods for horizontally comparing multiple models with specific features.

Method: The proposed VAR visualization solution uses a combination of heatmaps and scatter plots to perform horizontal comparisons of machine learning models within the Rashomon set.

Result: The VAR visualization solution enables the identification of optimal models under specific conditions and enhances the understanding of the Rashomon set's overall characteristics.

Conclusion: VAR provides a new visualization solution for comparing machine learning models within a Rashomon set, combining heatmaps and scatter plots to facilitate horizontal comparisons and aiding developers in identifying optimal models and understanding set characteristics.

Abstract: Evaluating the performance of closely matched machine learning(ML) models
under specific conditions has long been a focus of researchers in the field of
machine learning. The Rashomon set is a collection of closely matched ML
models, encompassing a wide range of models with similar accuracies but
different structures. Traditionally, the analysis of these sets has focused on
vertical structural analysis, which involves comparing the corresponding
features at various levels within the ML models. However, there has been a lack
of effective visualization methods for horizontally comparing multiple models
with specific features. We propose the VAR visualization solution. VAR uses
visualization to perform comparisons of ML models within the Rashomon set. This
solution combines heatmaps and scatter plots to facilitate the comparison. With
the help of VAR, ML model developers can identify the optimal model under
specific conditions and better understand the Rashomon set's overall
characteristics.

</details>


### [317] [Efficient Differentially Private Fine-Tuning of LLMs via Reinforcement Learning](https://arxiv.org/abs/2507.22565)
*Afshin Khadangi,Amir Sartipi,Igor Tchappi,Ramin Bahmani,Gilbert Fridgen*

Main category: cs.LG

TL;DR: RLDP uses reinforcement learning to improve differential privacy in LLMs. It adapts gradient clipping and noise during training, leading to better accuracy and faster convergence without compromising privacy guarantees. This approach offers significant performance gains and efficiency improvements compared to existing methods.


<details>
  <summary>Details</summary>
Motivation: The motivation for this paper stems from the inherent tension between data privacy and model utility in large language models (LLMs) trained on sensitive data. Traditional DP-SGD methods, while offering formal privacy guarantees, impose a significant cost on model performance due to gradient clipping and noise addition. Existing variants struggle with hard-coded, global control knobs that are insensitive to the optimization landscape, forcing a compromise between privacy and utility. RLDP aims to address this trade-off by introducing an adaptive and intelligent approach to DP optimization.

Method: The paper proposes RLDP, a framework that utilizes deep reinforcement learning (RL) to optimize differentially private stochastic gradient descent (DP-SGD). RLDP treats DP optimization as a closed-loop control problem, employing a soft actor-critic (SAC) hyper-policy trained online during LLM fine-tuning. This policy learns to dynamically adjust gradient-clipping thresholds and noise magnitudes based on the observed learning dynamics, effectively allocating the privacy budget where and when it is most needed.

Result: RLDP demonstrates significant improvements across multiple LLMs (GPT2-small, Llama-1B, Llama-3B, and Mistral-7B) over more than 1,600 ablation experiments. It achieves perplexity reductions ranging from 1.3% to 30.5% (with a mean of 5.4%) and an average 5.6% gain in downstream utility. Furthermore, RLDP reaches the final utility of baseline models with only 13-43% of the gradient-update budget (a mean speed-up of 71%). Importantly, these improvements are achieved while adhering to the same ($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility to membership-inference and canary-extraction attacks.

Conclusion: RLDP is the first framework that casts DP optimization as a closed-loop control problem using deep reinforcement learning. It continuously senses learning dynamics and selects per-parameter gradient-clipping thresholds and noise magnitudes. A SAC hyper-policy is trained online to allocate the privacy budget effectively. RLDP achieves significant perplexity reductions and downstream utility gains across multiple LLMs while maintaining DP guarantees and improving resistance to privacy attacks.

Abstract: The tension between data privacy and model utility has become the defining
bottleneck for the practical deployment of large language models (LLMs) trained
on sensitive corpora including healthcare. Differentially private stochastic
gradient descent (DP-SGD) guarantees formal privacy, yet it does so at a
pronounced cost: gradients are forcibly clipped and perturbed with noise,
degrading sample efficiency and final accuracy. Numerous variants have been
proposed to soften this trade-off, but they all share a handicap: their control
knobs are hard-coded, global, and oblivious to the evolving optimization
landscape. Consequently, practitioners are forced either to over-spend privacy
budget in pursuit of utility, or to accept mediocre models in order to stay
within privacy constraints. We present RLDP, the first framework to cast DP
optimization itself as a closed-loop control problem amenable to modern deep
reinforcement learning (RL). RLDP continuously senses rich statistics of the
learning dynamics and acts by selecting fine-grained per parameter
gradient-clipping thresholds as well as the magnitude of injected Gaussian
noise. A soft actor-critic (SAC) hyper-policy is trained online during language
model fine-tuning; it learns, from scratch, how to allocate the privacy budget
where it matters and when it matters. Across more than 1,600 ablation
experiments on GPT2-small, Llama-1B, Llama-3B, and Mistral-7B, RLDP delivers
perplexity reductions of 1.3-30.5% (mean 5.4%) and an average 5.6% downstream
utility gain. RLDP reaches each baseline's final utility after only 13-43% of
the gradient-update budget (mean speed-up 71%), all while honoring the same
($\epsilon$, $\delta$)-DP contract and exhibiting equal or lower susceptibility
to membership-inference and canary-extraction attacks.

</details>


### [318] [Deep learning of geometrical cell division rules](https://arxiv.org/abs/2507.22587)
*Alexandre Durrmeyer,Jean-Christophe Palauqui,Philippe Andrey*

Main category: cs.LG

TL;DR: 提出一种基于深度学习的方法，用于分析细胞几何形状与细胞分裂平面定位之间的关系，并成功应用于A. thaliana胚胎细胞，解决了现有几何规则无法解释的问题。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统依赖先验定义的几何规则来连接细胞形状和分裂方向的假设驱动方法的局限性。

Method: 提出了一种基于数据的、利用深度神经网络学习复杂关系的方法，采用基于图像的细胞表示，并使用改进的UNet架构从母细胞几何形状学习和预测分裂模式。

Result: 该模型能够学习和预测细胞分裂模式，并能解释先前在现有几何规则下无法调和的A. thaliana胚胎分裂模式。

Conclusion: 该研究展示了深度网络在理解细胞分裂模式和细胞分裂定位控制的假设生成方面的潜力。

Abstract: The positioning of new cellular walls during cell division plays a key role
in shaping plant tissue organization. The influence of cell geometry on the
positioning of division planes has been previously captured into various
geometrical rules. Accordingly, linking cell shape to division orientation has
relied on the comparison between observed division patterns and predictions
under specific rules. The need to define a priori the tested rules is a
fundamental limitation of this hypothesis-driven approach. As an alternative,
we introduce a data-based approach to investigate the relation between cell
geometry and division plane positioning, exploiting the ability of deep neural
network to learn complex relationships across multidimensional spaces. Adopting
an image-based cell representation, we show how division patterns can be
learned and predicted from mother cell geometry using a UNet architecture
modified to operate on cell masks. Using synthetic data and A. thaliana embryo
cells, we evaluate the model performances on a wide range of diverse cell
shapes and division patterns. We find that the trained model accounted for
embryo division patterns that were previously irreconcilable under existing
geometrical rules. Our work shows the potential of deep networks to understand
cell division patterns and to generate new hypotheses on the control of cell
division positioning.

</details>


### [319] [H2Tune: Federated Foundation Model Fine-Tuning with Hybrid Heterogeneity](https://arxiv.org/abs/2507.22633)
*Wei Guo,Siyuan Lu,Yiqi Tong,Zhaojun Hu,Fuzhen Zhuang,Xiao Zhang,Tao Fan,Jin Dong*

Main category: cs.LG

TL;DR: H2Tune通过稀疏化三元矩阵分解、关系引导矩阵层对齐和交替任务知识解耦，解决了联邦学习中模型架构和任务异质性带来的挑战，提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的联邦微调（FFT）方法在混合异质性联邦微调（HHFFT）这一场景下研究不足。该场景下，客户端在模型架构和下游任务上均存在异质性，这带来了两个主要挑战：1. 异构矩阵聚合：客户端可能因任务需求和资源限制采用不同的基础模型，导致LoRA参数聚合时出现维度不匹配；2. 多任务知识干扰：本地共享参数同时包含任务共享和任务特定知识，无法确保任务共享知识在客户端之间有效迁移。

Method: 提出了一种名为H2Tune的联邦基础模型微调框架，包含三个关键组件：1. 稀疏化三元矩阵分解，通过构建秩一致性中间矩阵来匹配客户端之间的隐藏维度，并根据客户端资源进行自适应稀疏化；2. 关系引导矩阵层对齐，以处理异构的网络层结构和表示能力；3. 交替任务知识解耦机制，通过交替优化来分离本地模型参数中的任务共享知识和任务特定知识。

Result: 通过实验证明，H2Tune相比于现有最先进的基线方法，在准确率方面最高可提升15.4%。

Conclusion: H2Tune在混合异质性的联邦基础模型微调场景下，通过稀疏化三元矩阵分解、关系引导矩阵层对齐以及交替任务知识解耦机制，有效解决了异构矩阵聚合和多任务知识干扰的挑战，理论分析证明了其收敛速率为O(1/\sqrt{T})，实验结果显示准确率相较于现有最先进的方法最高可提升15.4%。

Abstract: Different from existing federated fine-tuning (FFT) methods for foundation
models, hybrid heterogeneous federated fine-tuning (HHFFT) is an under-explored
scenario where clients exhibit double heterogeneity in model architectures and
downstream tasks. This hybrid heterogeneity introduces two significant
challenges: 1) heterogeneous matrix aggregation, where clients adopt different
large-scale foundation models based on their task requirements and resource
limitations, leading to dimensional mismatches during LoRA parameter
aggregation; and 2) multi-task knowledge interference, where local shared
parameters, trained with both task-shared and task-specific knowledge, cannot
ensure only task-shared knowledge is transferred between clients. To address
these challenges, we propose H2Tune, a federated foundation model fine-tuning
with hybrid heterogeneity. Our framework H2Tune consists of three key
components: (i) sparsified triple matrix decomposition to align hidden
dimensions across clients through constructing rank-consistent middle matrices,
with adaptive sparsification based on client resources; (ii) relation-guided
matrix layer alignment to handle heterogeneous layer structures and
representation capabilities; and (iii) alternating task-knowledge
disentanglement mechanism to decouple shared and specific knowledge of local
model parameters through alternating optimization. Theoretical analysis proves
a convergence rate of O(1/\sqrt{T}). Extensive experiments show our method
achieves up to 15.4% accuracy improvement compared to state-of-the-art
baselines. Our code is available at
https://anonymous.4open.science/r/H2Tune-1407.

</details>


### [320] [Transductive Model Selection under Prior Probability Shift](https://arxiv.org/abs/2507.22647)
*Lorenzo Volpi,Alejandro Moreo,Fabrizio Sebastiani*

Main category: cs.LG

TL;DR: 提出了一种新的转导学习模型选择方法，可以直接在未标记数据上优化超参数，以解决先验概率转移问题。


<details>
  <summary>Details</summary>
Motivation: 解决在存在先验概率转移（一种常见于反因果学习的数据集偏移类型）的转导学习上下文中模型选择的问题。

Method: 提出了一种用于处理数据集先验概率转移的转导学习模型选择方法，允许在目标未标记数据上直接优化超参数。

Result: 实验结果证明了该方法相对于传统基于交叉验证的方法的优势。

Conclusion: 所提出的方法可直接在未标记数据上优化超参数，以解决数据集的先验概率转移问题，并在反因果学习中具有重要意义。

Abstract: Transductive learning is a supervised machine learning task in which, unlike
in traditional inductive learning, the unlabelled data that require labelling
are a finite set and are available at training time. Similarly to inductive
learning contexts, transductive learning contexts may be affected by dataset
shift, i.e., may be such that the IID assumption does not hold. We here propose
a method, tailored to transductive classification contexts, for performing
model selection (i.e., hyperparameter optimisation) when the data exhibit prior
probability shift, an important type of dataset shift typical of anti-causal
learning problems. In our proposed method the hyperparameters can be optimised
directly on the unlabelled data to which the trained classifier must be
applied; this is unlike traditional model selection methods, that are based on
performing cross-validation on the labelled training data. We provide
experimental results that show the benefits brought about by our method.

</details>


### [321] [Cluster-Based Random Forest Visualization and Interpretation](https://arxiv.org/abs/2507.22665)
*Max Sondag,Christofer Meinecke,Dennis Collaris,Tatiana von Landesberger,Stef van den Elzen*

Main category: cs.LG

TL;DR: 随机森林难以解释，本文提出可视化方法通过聚类和新颖的特征/规则图来提高其可解释性。


<details>
  <summary>Details</summary>
Motivation: 随机森林虽然性能优越但难以解释，本文旨在提高其可解释性。

Method: 提出了一种新的距离度量，该度量同时考虑了决策规则和预测结果，并提出了两种新的可视化方法：特征图和规则图。

Result: 通过在“Glass”数据集上的案例研究和用户研究，证明了该方法是有效的。

Conclusion: 本文提出了一种新的可视化方法和系统，通过聚类相似的决策树来提高随机森林的可解释性，并引入了新的距离度量和两种可视化方法（特征图和规则图）。

Abstract: Random forests are a machine learning method used to automatically classify
datasets and consist of a multitude of decision trees. While these random
forests often have higher performance and generalize better than a single
decision tree, they are also harder to interpret. This paper presents a
visualization method and system to increase interpretability of random forests.
We cluster similar trees which enables users to interpret how the model
performs in general without needing to analyze each individual decision tree in
detail, or interpret an oversimplified summary of the full forest. To
meaningfully cluster the decision trees, we introduce a new distance metric
that takes into account both the decision rules as well as the predictions of a
pair of decision trees. We also propose two new visualization methods that
visualize both clustered and individual decision trees: (1) The Feature Plot,
which visualizes the topological position of features in the decision trees,
and (2) the Rule Plot, which visualizes the decision rules of the decision
trees. We demonstrate the efficacy of our approach through a case study on the
"Glass" dataset, which is a relatively complex standard machine learning
dataset, as well as a small user study.

</details>


### [322] [Enhanced Prediction of CAR T-Cell Cytotoxicity with Quantum-Kernel Methods](https://arxiv.org/abs/2507.22710)
*Filippo Utro,Meltem Tolunay,Kahn Rhrissorrakrai,Tanvi P. Gujarati,Jie Shi,Sara Capponi,Mirko Amico,Nate Earnest-Noble,Laxmi Parida*

Main category: cs.LG

TL;DR: 利用量子计算（PQK）解决CAR T细胞工程改造中的组合优化问题，在数据稀疏的情况下优于经典方法。


<details>
  <summary>Details</summary>
Motivation: CAR T细胞的工程改造具有巨大的组合空间，传统的实验方法难以充分探索，导致数据稀疏且探索不足，因此需要新的方法来解决这一挑战。

Method: 提出了一种利用投影量子核（PQK）的量子方法，将经典数据嵌入高维希尔伯特空间，并使用核方法测量样本相似性，在61量子比特的量子计算机上进行了实现。

Result: 在量子计算机上实现了迄今最大的PQK应用，并在CAR T细胞的细胞毒性预测任务中，相比纯经典机器学习方法，在特定信号域和域位置上表现出更优的学习能力，尤其是在信息量较低的情况下。

Conclusion: 本研究提出了一种量子方法，利用投影量子核（PQK）来解决CAR T细胞研究中的组合优化问题，并在量子计算机上进行了实验验证，展示了其在数据稀疏场景下的潜力。

Abstract: Chimeric antigen receptor (CAR) T-cells are T-cells engineered to recognize
and kill specific tumor cells. Through their extracellular domains, CAR T-cells
bind tumor cell antigens which triggers CAR T activation and proliferation.
These processes are regulated by co-stimulatory domains present in the
intracellular region of the CAR T-cell. Through integrating novel signaling
components into the co-stimulatory domains, it is possible to modify CAR T-cell
phenotype. Identifying and experimentally testing new CAR constructs based on
libraries of co-stimulatory domains is nontrivial given the vast combinatorial
space defined by such libraries. This leads to a highly data constrained,
poorly explored combinatorial problem, where the experiments undersample all
possible combinations. We propose a quantum approach using a Projected Quantum
Kernel (PQK) to address this challenge. PQK operates by embedding classical
data into a high dimensional Hilbert space and employs a kernel method to
measure sample similarity. Using 61 qubits on a gate-based quantum computer, we
demonstrate the largest PQK application to date and an enhancement in the
classification performance over purely classical machine learning methods for
CAR T cytotoxicity prediction. Importantly, we show improved learning for
specific signaling domains and domain positions, particularly where there was
lower information highlighting the potential for quantum computing in
data-constrained problems.

</details>


### [323] [Teaching the Teacher: Improving Neural Network Distillability for Symbolic Regression via Jacobian Regularization](https://arxiv.org/abs/2507.22767)
*Soumyadeep Dhar,Kei Sen Fong,Mehul Motani*

Main category: cs.LG

TL;DR: 通过一种新的雅可比正则化训练方法，提高了神经网络蒸馏成符号模型的准确性和保真度。


<details>
  <summary>Details</summary>
Motivation: 传统的神经网络蒸馏过程会产生低保真度的学生模型，因为标准网络学习的复杂函数难以进行符号发现。

Method: 提出了一种新颖的训练范式，引入了一个基于雅可比行列式的正则化器，以鼓励教师网络学习更平滑、更易于蒸馏的函数。

Result: 在真实的回归基准测试中，通过优化正则化强度，最终蒸馏符号模型的 R^2 分数平均提高了 120%（相对而言），同时保持了教师的预测准确性。

Conclusion: 该方法提供了一种实用且有原则的方法，可以显著提高从复杂神经网络中提取的可解释模型的保真度。

Abstract: Distilling large neural networks into simple, human-readable symbolic
formulas is a promising path toward trustworthy and interpretable AI. However,
this process is often brittle, as the complex functions learned by standard
networks are poor targets for symbolic discovery, resulting in low-fidelity
student models. In this work, we propose a novel training paradigm to address
this challenge. Instead of passively distilling a pre-trained network, we
introduce a \textbf{Jacobian-based regularizer} that actively encourages the
``teacher'' network to learn functions that are not only accurate but also
inherently smoother and more amenable to distillation. We demonstrate through
extensive experiments on a suite of real-world regression benchmarks that our
method is highly effective. By optimizing the regularization strength for each
problem, we improve the $R^2$ score of the final distilled symbolic model by an
average of \textbf{120\% (relative)} compared to the standard distillation
pipeline, all while maintaining the teacher's predictive accuracy. Our work
presents a practical and principled method for significantly improving the
fidelity of interpretable models extracted from complex neural networks.

</details>


### [324] [Label-free estimation of clinically relevant performance metrics under distribution shifts](https://arxiv.org/abs/2507.22776)
*Tim Flühmann,Alceu Bissoto,Trung-Dung Hoang,Lisa M. Koch*

Main category: cs.LG

TL;DR: 本研究提出了一种新的方法来估计医学影像分类模型的性能，即使在没有地面真实标签的情况下也能进行评估。研究结果表明，该方法在处理数据偏移方面是可靠的，但也揭示了现有方法在现实场景中可能存在的局限性，强调了在部署这些模型时理解其性能监控的必要性。


<details>
  <summary>Details</summary>
Motivation: 在临床部署中，由于目标数据集中通常没有地面真实标签，因此无法直接评估模型在现实世界中的性能。现有的性能估计方法利用置信分数来估计目标准确性，但主要估计模型的准确性，并且很少在类不平衡和数据集偏移很常见的临床领域进行评估。

Method: 提出混淆矩阵估计方法的泛化，并在真实世界分布偏移以及模拟的协变量和流行度偏移的胸部X射线数据上对其进行基准测试。

Result: 所提出的混淆矩阵估计方法在分布偏移下能够可靠地预测医学影像中临床相关的计数指标。

Conclusion: 现有方法在现实分布偏移下预测医学影像的混淆矩阵方面具有可靠性，但模拟偏移场景暴露了当前性能估计技术的关键失效模式，需要在部署这些性能监控技术进行医疗人工智能模型的市后监控时，更好地理解实际部署环境。

Abstract: Performance monitoring is essential for safe clinical deployment of image
classification models. However, because ground-truth labels are typically
unavailable in the target dataset, direct assessment of real-world model
performance is infeasible. State-of-the-art performance estimation methods
address this by leveraging confidence scores to estimate the target accuracy.
Despite being a promising direction, the established methods mainly estimate
the model's accuracy and are rarely evaluated in a clinical domain, where
strong class imbalances and dataset shifts are common. Our contributions are
twofold: First, we introduce generalisations of existing performance prediction
methods that directly estimate the full confusion matrix. Then, we benchmark
their performance on chest x-ray data in real-world distribution shifts as well
as simulated covariate and prevalence shifts. The proposed confusion matrix
estimation methods reliably predicted clinically relevant counting metrics on
medical images under distribution shifts. However, our simulated shift
scenarios exposed important failure modes of current performance estimation
techniques, calling for a better understanding of real-world deployment
contexts when implementing these performance monitoring techniques for
postmarket surveillance of medical AI models.

</details>


### [325] [DO-EM: Density Operator Expectation Maximization](https://arxiv.org/abs/2507.22786)
*Adit Vishnu,Abhay Shastry,Dhruva Kashyap,Chiranjib Bhattacharyya*

Main category: cs.LG

TL;DR: This paper introduces a scalable Expectation-Maximization algorithm (DO-EM) for training density operator models (DOMs) for generative tasks. Their proposed model, QiDBM, outperforms classical models on image generation tasks, demonstrating the potential of DOMs in machine learning.


<details>
  <summary>Details</summary>
Motivation: Existing DOM-based generative modeling training algorithms do not scale to real-world data like MNIST. The paper aims to develop a scalable Expectation-Maximization framework for DOMs on classical hardware, comparable to resources used for probabilistic models.

Method: Introduces the DO-EM algorithm, an iterative Minorant-Maximization procedure that optimizes a quantum evidence lower bound by reformulating the Expectation step as a QIP problem solved by the Petz Recovery Map.

Result: The DO-EM algorithm ensures non-decreasing log-likelihood across iterations for a broad class of models. QiDBMs trained with DO-EM and Contrastive Divergence achieve a 40-60% reduction in FID on the MNIST dataset compared to classical DBMs.

Conclusion: QiDBMs, a DOM, trained with DO-EM under Contrastive Divergence, outperforms larger classical DBMs in image generation on the MNIST dataset, achieving a 40-60% reduction in FID.

Abstract: Density operators, quantum generalizations of probability distributions, are
gaining prominence in machine learning due to their foundational role in
quantum computing. Generative modeling based on density operator models
(\textbf{DOMs}) is an emerging field, but existing training algorithms -- such
as those for the Quantum Boltzmann Machine -- do not scale to real-world data,
such as the MNIST dataset. The Expectation-Maximization algorithm has played a
fundamental role in enabling scalable training of probabilistic latent variable
models on real-world datasets. \textit{In this paper, we develop an
Expectation-Maximization framework to learn latent variable models defined
through \textbf{DOMs} on classical hardware, with resources comparable to those
used for probabilistic models, while scaling to real-world data.} However,
designing such an algorithm is nontrivial due to the absence of a well-defined
quantum analogue to conditional probability, which complicates the Expectation
step. To overcome this, we reformulate the Expectation step as a quantum
information projection (QIP) problem and show that the Petz Recovery Map
provides a solution under sufficient conditions. Using this formulation, we
introduce the Density Operator Expectation Maximization (DO-EM) algorithm -- an
iterative Minorant-Maximization procedure that optimizes a quantum evidence
lower bound. We show that the \textbf{DO-EM} algorithm ensures non-decreasing
log-likelihood across iterations for a broad class of models. Finally, we
present Quantum Interleaved Deep Boltzmann Machines (\textbf{QiDBMs}), a
\textbf{DOM} that can be trained with the same resources as a DBM. When trained
with \textbf{DO-EM} under Contrastive Divergence, a \textbf{QiDBM} outperforms
larger classical DBMs in image generation on the MNIST dataset, achieving a
40--60\% reduction in the Fr\'echet Inception Distance.

</details>


### [326] [G-Core: A Simple, Scalable and Balanced RLHF Trainer](https://arxiv.org/abs/2507.22789)
*Junyu Wu,Weiming Chang,Xiaotao Liu,Guanyou He,Haoqiang Hong,Boqi Liu,Hongtao Tian,Tao Yang,Yunsheng Shi,Feng Lin,Ting Yao*

Main category: cs.LG

TL;DR: G-Core 是一个用于训练大型语言模型（LLM）和扩散模型的新 RLHF 框架，它通过并行的控制器编程模型和动态资源放置方案解决了现有系统的可扩展性和适应性挑战，并在实际应用中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有 RLHF 训练系统在扩展到多模态和扩散工作流以及适应动态工作负载时面临挑战，尤其是在控制器可扩展性、灵活资源放置和高效编排方面，这限制了复杂 RLHF 管道的处理能力。

Method: G-Core 引入了一个并行的控制器编程模型，以实现复杂 RLHF 工作流的灵活高效的编排，解决了单一集中式控制器带来的瓶颈。此外，该框架还提出了一种动态放置方案，可以自适应地划分资源和调度工作负载，即使在高度可变的训练条件下，也能显著减少硬件空闲时间并提高利用率。

Result: G-Core 已成功训练出支持微信产品功能并服务于庞大用户群的模型，证明了其在真实场景中的有效性和鲁棒性。

Conclusion: G-Core 推动了 RLHF 训练的艺术，为未来大规模、人类对齐模型的研发和部署奠定了坚实的基础。

Abstract: Reinforcement Learning from Human Feedback (RLHF) has become an increasingly
popular paradigm for training large language models (LLMs) and diffusion
models. While existing RLHF training systems have enabled significant progress,
they often face challenges in scaling to multi-modal and diffusion workflows
and adapting to dynamic workloads. In particular, current approaches may
encounter limitations in controller scalability, flexible resource placement,
and efficient orchestration when handling complex RLHF pipelines, especially in
scenarios involving dynamic sampling or generative reward modeling. In this
paper, we present \textbf{G-Core}, a simple, scalable, and balanced RLHF
training framework designed to address these challenges. G-Core introduces a
parallel controller programming model, enabling flexible and efficient
orchestration of complex RLHF workflows without the bottlenecks of a single
centralized controller. Furthermore, we propose a dynamic placement schema that
adaptively partitions resources and schedules workloads, significantly reducing
hardware idle time and improving utilization, even under highly variable
training conditions. G-Core has successfully trained models that support WeChat
product features serving a large-scale user base, demonstrating its
effectiveness and robustness in real-world scenarios. Our results show that
G-Core advances the state of the art in RLHF training, providing a solid
foundation for future research and deployment of large-scale, human-aligned
models.

</details>


### [327] [Quantifying surprise in clinical care: Detecting highly informative events in electronic health records with foundation models](https://arxiv.org/abs/2507.22798)
*Michael C. Burkhart,Bashar Ramadan,Luke Solo,William F. Parker,Brett K. Beaulieu-Jones*

Main category: cs.LG

TL;DR: Foundation models can identify important medical events in patient records to predict outcomes and improve model interpretability.


<details>
  <summary>Details</summary>
Motivation: To develop a method that can identify highly informative tokens and events in electronic health records, flag anomalous events missed by rule-based approaches, and improve the interpretability of prognostic models.

Method: A foundation model-derived method to identify highly informative tokens and events in electronic health records by considering the entire context of a patient's hospitalization.

Result: The model flags significant events for predicting downstream patient outcomes and shows that a fraction of events identified as carrying little information can be safely dropped. It also demonstrates how informativeness can help interpret predictions of prognostic models.

Conclusion: Informativeness derived from foundation models can be used to predict patient outcomes, identify anomalous events, and interpret prognostic models.

Abstract: We present a foundation model-derived method to identify highly informative
tokens and events in electronic health records. Our approach considers incoming
data in the entire context of a patient's hospitalization and so can flag
anomalous events that rule-based approaches would consider within a normal
range. We demonstrate that the events our model flags are significant for
predicting downstream patient outcomes and that a fraction of events identified
as carrying little information can safely be dropped. Additionally, we show how
informativeness can help interpret the predictions of prognostic models trained
on foundation model-derived representations.

</details>


### [328] [PAF-Net: Phase-Aligned Frequency Decoupling Network for Multi-Process Manufacturing Quality Prediction](https://arxiv.org/abs/2507.22840)
*Yang Luo,Haoyang Luan,Haoyun Pan,Yongquan Jia,Xiaofeng Gao,Guihai Chen*

Main category: cs.LG

TL;DR: PAF-Net是一个新颖的框架，通过频率解耦、相位相关对齐和注意力机制来解决多过程制造中的质量预测挑战，并在真实数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 为了解决多过程制造中时间滞后的过程交互、重叠操作（具有混合周期性）和共享频带中的过程间依赖这三个核心挑战，从而实现准确的质量预测，提高工业效率。

Method:  PAF-Net是一个频率解耦的时间序列预测框架，包含三个关键创新：1. 由频域能量引导的相位相关对齐方法，用于同步具有时间滞后的质量序列，解决时间失配问题。2. 频率无关的斑块注意力机制，结合离散余弦变换（DCT）分解，以捕捉单个序列中异构的操作特征。3. 频率解耦的交叉注意力模块，用于抑制来自不相关频率的噪声，仅关注共享频带内有意义的依赖关系。

Result: PAF-Net在4个真实世界数据集上取得了优于10个基线的性能，MSE降低了7.06%，MAE降低了3.88%。

Conclusion: PAF-Net在4个真实世界数据集上进行了实验，其性能优于10个公认的基线，MSE降低了7.06%，MAE降低了3.88%。

Abstract: Accurate quality prediction in multi-process manufacturing is critical for
industrial efficiency but hindered by three core challenges: time-lagged
process interactions, overlapping operations with mixed periodicity, and
inter-process dependencies in shared frequency bands. To address these, we
propose PAF-Net, a frequency decoupled time series prediction framework with
three key innovations: (1) A phase-correlation alignment method guided by
frequency domain energy to synchronize time-lagged quality series, resolving
temporal misalignment. (2) A frequency independent patch attention mechanism
paired with Discrete Cosine Transform (DCT) decomposition to capture
heterogeneous operational features within individual series. (3) A frequency
decoupled cross attention module that suppresses noise from irrelevant
frequencies, focusing exclusively on meaningful dependencies within shared
bands. Experiments on 4 real-world datasets demonstrate PAF-Net's superiority.
It outperforms 10 well-acknowledged baselines by 7.06% lower MSE and 3.88%
lower MAE. Our code is available at
https://github.com/StevenLuan904/PAF-Net-Official.

</details>


### [329] [A Bit of Freedom Goes a Long Way: Classical and Quantum Algorithms for Reinforcement Learning under a Generative Model](https://arxiv.org/abs/2507.22854)
*Andris Ambainis,Joao F. Doriguello,Debbie Lim*

Main category: cs.LG

TL;DR: 本研究提出了一种新的混合探索-生成强化学习模型，用于学习有限和无限时间范围的平均奖励马尔可夫决策过程（MDP）。该模型利用经典和量子算法，可以直接计算最优策略，从而获得比现有方法更好的遗憾界限。特别是，量子算法在有限时间MDP中实现了对时间步长T的对数依赖遗憾界限，打破了经典O(sqrt(T))的瓶颈。对于无限时间MDP，量子算法在新的遗憾度衡量标准下取得了指数级的改进。


<details>
  <summary>Details</summary>
Motivation: 为了改进马尔可夫决策过程（MDP）的学习算法，特别是为了克服现有强化学习（RL）范式中的一些限制，如“面对不确定性的乐观主义”和“后验采样”，并寻求更优的遗憾界限。

Method: 提出了一种混合探索-生成强化学习（RL）模型，允许智能体与环境进行生成式采样交互。利用已知的经典和新的量子算法来逼近生成模型下的最优策略，并将这些算法应用于所提出的学习算法中。

Result: 在有限时间范围MDP中，量子算法获得的遗憾界限仅对时间步长T有对数依赖，优于经典的O(sqrt(T))界限，并且在状态空间大小S和动作空间大小A方面也优于先前量子工作。在无限时间范围MDP中，经典和量子算法的界限虽然仍有O(sqrt(T))的时间依赖性，但在S和A因子上有所改善。此外，提出了一种新的无限时间范围MDP遗憾度衡量标准，在此标准下量子算法实现了多项式对数T的遗憾度，比经典算法有指数级提升。

Conclusion: 提出的算法在有限和无限时间范围的平均奖励马尔可夫决策过程（MDP）学习中，通过混合探索-生成强化学习（RL）模型，利用经典和量子算法，可以直接计算和使用最优策略，优于现有的RL范式，并取得了更好的遗憾界限。量子算法在有限时间范围MDP中实现了对时间步长T仅有对数依赖的遗憾界限，突破了经典算法的O(sqrt(T))障碍，并且在状态空间大小S和动作空间大小A等参数上也有改进。在无限时间范围MDP中，量子算法在新的遗憾度衡量标准下取得了多项式对数T的遗憾度，比经典算法指数级更优。

Abstract: We propose novel classical and quantum online algorithms for learning
finite-horizon and infinite-horizon average-reward Markov Decision Processes
(MDPs). Our algorithms are based on a hybrid exploration-generative
reinforcement learning (RL) model wherein the agent can, from time to time,
freely interact with the environment in a generative sampling fashion, i.e., by
having access to a "simulator". By employing known classical and new quantum
algorithms for approximating optimal policies under a generative model within
our learning algorithms, we show that it is possible to avoid several paradigms
from RL like "optimism in the face of uncertainty" and "posterior sampling" and
instead compute and use optimal policies directly, which yields better regret
bounds compared to previous works. For finite-horizon MDPs, our quantum
algorithms obtain regret bounds which only depend logarithmically on the number
of time steps $T$, thus breaking the $O(\sqrt{T})$ classical barrier. This
matches the time dependence of the prior quantum works of Ganguly et al.
(arXiv'23) and Zhong et al. (ICML'24), but with improved dependence on other
parameters like state space size $S$ and action space size $A$. For
infinite-horizon MDPs, our classical and quantum bounds still maintain the
$O(\sqrt{T})$ dependence but with better $S$ and $A$ factors. Nonetheless, we
propose a novel measure of regret for infinite-horizon MDPs with respect to
which our quantum algorithms have $\operatorname{poly}\log{T}$ regret,
exponentially better compared to classical algorithms. Finally, we generalise
all of our results to compact state spaces.

</details>


### [330] [RLVMR: Reinforcement Learning with Verifiable Meta-Reasoning Rewards for Robust Long-Horizon Agents](https://arxiv.org/abs/2507.22844)
*Zijing Zhang,Ziyang Chen,Mingxiao Li,Zhaopeng Tu,Xiaolong Li*

Main category: cs.LG

TL;DR: RLVMR通过奖励可验证的元推理行为，解决RL中的效率低下探索问题，从而实现更强大、更高效、更具可解释性的代理。


<details>
  <summary>Details</summary>
Motivation: 当前RL方法主要优化最终任务成功，会导致代理学习有缺陷或低效的推理路径，即“效率低下探索”，使代理脆弱且泛化能力差。

Method: RLVMR框架整合了密集的、过程级别的监督，通过奖励可验证的、元推理行为来端到端地训练RL代理，并结合最终结果信号，使用无批评者策略梯度方法进行优化。

Result: RLVMR在ALFWorld和ScienceWorld基准测试中取得了新的最先进成果，7B模型在最困难的未见任务分割上达到了83.6%的成功率，并确认了改进的推理质量，包括冗余操作的显著减少和错误恢复能力的增强。

Conclusion: RLVMR通过整合密集的、过程级别的监督来解决RL中效率低下探索的问题，从而实现更强大、更高效、更具可解释性的代理，并在ALFWorld和ScienceWorld基准测试中取得了新的最先进成果。

Abstract: The development of autonomous agents for complex, long-horizon tasks is a
central goal in AI. However, dominant training paradigms face a critical
limitation: reinforcement learning (RL) methods that optimize solely for final
task success often reinforce flawed or inefficient reasoning paths, a problem
we term inefficient exploration. This leads to agents that are brittle and fail
to generalize, as they learn to find solutions without learning how to reason
coherently. To address this, we introduce RLVMR, a novel framework that
integrates dense, process-level supervision into end-to-end RL by rewarding
verifiable, meta-reasoning behaviors. RLVMR equips an agent to explicitly tag
its cognitive steps, such as planning, exploration, and reflection, and
provides programmatic, rule-based rewards for actions that contribute to
effective problem-solving. These process-centric rewards are combined with the
final outcome signal and optimized using a critic-free policy gradient method.
On the challenging ALFWorld and ScienceWorld benchmarks, RLVMR achieves new
state-of-the-art results, with our 7B model reaching an 83.6% success rate on
the most difficult unseen task split. Our analysis confirms these gains stem
from improved reasoning quality, including significant reductions in redundant
actions and enhanced error recovery, leading to more robust, efficient, and
interpretable agents.

</details>


### [331] [Decentralized Differentially Private Power Method](https://arxiv.org/abs/2507.22849)
*Andrew Campbell,Anna Scaglione,Sean Peisert*

Main category: cs.LG

TL;DR: D-DP-PM 是一种去中心化的差分隐私 PCA 方法，适用于多代理网络，在代理仅观察到部分维度数据的情况下，也能实现全局特征向量的协同估计，同时满足差分隐私保证。


<details>
  <summary>Details</summary>
Motivation: 提出了一种新颖的去中心化差分隐私幂法 (D-DP-PM)，用于在网络化多代理环境中执行主成分分析 (PCA)。与每个代理都能访问完整 n 维样本空间的传统去中心化 PCA 方法不同，我们解决了每个代理仅观察到行划分的子集维度这一挑战性场景。

Method: D-DP-PM 方法通过让各代理仅共享当前特征向量迭代的局部嵌入，并利用随机初始化的内在隐私性和精心校准的高斯噪声，在不要求中心聚合器的情况下，实现跨网络的全局特征向量的协同估计。

Result: D-DP-PM 实现了卓越的隐私-效用权衡，与朴素的局部 DP 方法相比，在适度的隐私保护设置（ε∈[2, 5]）下表现尤为强劲。该方法收敛迅速，允许实践者在不牺牲效用的情况下，通过增加迭代次数来增强隐私保护。

Conclusion: D-DP-PM 方法在现实世界数据集的实验表明，与朴素的局部 DP 方法相比，D-DP-PM 实现了卓越的隐私-效用权衡，在适度的隐私保护设置（ε∈[2, 5]）下表现尤为强劲。该方法收敛迅速，允许实践者在不牺牲效用的情况下，通过增加迭代次数来增强隐私保护。

Abstract: We propose a novel Decentralized Differentially Private Power Method
(D-DP-PM) for performing Principal Component Analysis (PCA) in networked
multi-agent settings. Unlike conventional decentralized PCA approaches where
each agent accesses the full n-dimensional sample space, we address the
challenging scenario where each agent observes only a subset of dimensions
through row-wise data partitioning. Our method ensures
$(\epsilon,\delta)$-Differential Privacy (DP) while enabling collaborative
estimation of global eigenvectors across the network without requiring a
central aggregator. We achieve this by having agents share only local
embeddings of the current eigenvector iterate, leveraging both the inherent
privacy from random initialization and carefully calibrated Gaussian noise
additions. We prove that our algorithm satisfies the prescribed
$(\epsilon,\delta)$-DP guarantee and establish convergence rates that
explicitly characterize the impact of the network topology. Our theoretical
analysis, based on linear dynamics and high-dimensional probability theory,
provides tight bounds on both privacy and utility. Experiments on real-world
datasets demonstrate that D-DP-PM achieves superior privacy-utility tradeoffs
compared to naive local DP approaches, with particularly strong performance in
moderate privacy regimes ($\epsilon\in[2, 5]$). The method converges rapidly,
allowing practitioners to trade iterations for enhanced privacy while
maintaining competitive utility.

</details>
