{"id": "2508.06641", "categories": ["cs.LG", "cs.AI", "cs.NA", "math.NA", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.06641", "abs": "https://arxiv.org/abs/2508.06641", "authors": ["Jonas S Almeida", "Daniel E Russ", "Susana Vinga", "Ines Duarte", "Lee Mason", "Praphulla Bhawsar", "Aaron Ge", "Arlindo Oliveira", "Jeya Balaji Balasubramanian"], "title": "Fractal Language Modelling by Universal Sequence Maps (USM)", "comment": "16 pages, 8 figures", "summary": "Motivation: With the advent of Language Models using Transformers,\npopularized by ChatGPT, there is a renewed interest in exploring encoding\nprocedures that numerically represent symbolic sequences at multiple scales and\nembedding dimensions. The challenge that encoding addresses is the need for\nmechanisms that uniquely retain contextual information about the succession of\nindividual symbols, which can then be modeled by nonlinear formulations such as\nneural networks.\n  Context: Universal Sequence Maps(USM) are iterated functions that bijectively\nencode symbolic sequences onto embedded numerical spaces. USM is composed of\ntwo Chaos Game Representations (CGR), iterated forwardly and backwardly, that\ncan be projected into the frequency domain (FCGR). The corresponding USM\ncoordinates can be used to compute a Chebyshev distance metric as well as k-mer\nfrequencies, without having to recompute the embedded numeric coordinates, and,\nparadoxically, allowing for non-integers values of k.\n  Results: This report advances the bijective fractal encoding by Universal\nSequence Maps (USM) by resolving seeding biases affecting the iterated process.\nThe resolution had two results, the first expected, the second an intriguing\noutcome: 1) full reconciliation of numeric positioning with sequence identity;\nand 2) uncovering the nature of USM as an efficient numeric process converging\ntowards a steady state sequence embedding solution. We illustrate these results\nfor genomic sequences because of the convenience of a planar representation\ndefined by an alphabet with only 4 tokens (the 4 nucleotides). Nevertheless,\nthe application to alphabet of arbitrary cardinality was found to be\nstraightforward.", "AI": {"tldr": "USM\u662f\u4e00\u79cd\u65b0\u7684\u5e8f\u5217\u7f16\u7801\u65b9\u6cd5\uff0c\u53ef\u4ee5\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u968f\u7740ChatGPT\u7b49\u5305\u542bTransformer\u7684\u8bed\u8a00\u6a21\u578b\u7684\u51fa\u73b0\uff0c\u4eba\u4eec\u5bf9\u63a2\u7d22\u80fd\u591f\u4ee5\u591a\u5c3a\u5ea6\u548c\u5d4c\u5165\u7ef4\u5ea6\u5728\u6570\u503c\u4e0a\u8868\u793a\u7b26\u53f7\u5e8f\u5217\u7684\u7f16\u7801\u8fc7\u7a0b\u4ea7\u751f\u4e86\u65b0\u7684\u5174\u8da3\u3002", "method": "USM\u7531\u4e24\u4e2aCGR\u7ec4\u6210\uff0c\u53ef\u4ee5\u8fed\u4ee3\u5730\u8fdb\u884c\u524d\u5411\u548c\u540e\u5411\u8fd0\u7b97\uff0c\u5e76\u5c06\u5b83\u4eec\u6295\u5f71\u5230\u9891\u57df\uff08FCGR\uff09\u3002", "result": "1\uff09\u6570\u5b57\u5b9a\u4f4d\u4e0e\u5e8f\u5217\u540c\u4e00\u6027\u7684\u5b8c\u5168\u534f\u8c03\uff1b2\uff09\u63ed\u793a\u4e86USM\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u6570\u503c\u8fc7\u7a0b\uff0c\u8be5\u8fc7\u7a0b\u6536\u655b\u4e8e\u7a33\u5b9a\u7684\u5e8f\u5217\u5d4c\u5165\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "USM\u662f\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u503c\u8fc7\u7a0b\uff0c\u53ef\u4ee5\u6536\u655b\u5230\u7a33\u5b9a\u7684\u5e8f\u5217\u5d4c\u5165\u89e3\u51b3\u65b9\u6848\u3002\u6b64\u5916\uff0cUSM\u7684\u5206\u6790\u63ed\u793a\u4e86\u5176\u4f5c\u4e3a\u4e00\u79cd\u9ad8\u6548\u7684\u6570\u503c\u8fc7\u7a0b\u7684\u672c\u8d28\uff0c\u8be5\u8fc7\u7a0b\u53ef\u4ee5\u6536\u655b\u5230\u7a33\u5b9a\u7684\u5e8f\u5217\u5d4c\u5165\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06981", "categories": ["cs.LG", "cs.NA", "math.NA", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.06981", "abs": "https://arxiv.org/abs/2508.06981", "authors": ["Brooks Kinch", "Benjamin Shaffer", "Elizabeth Armstrong", "Michael Meehan", "John Hewson", "Nathaniel Trask"], "title": "Structure-Preserving Digital Twins via Conditional Neural Whitney Forms", "comment": null, "summary": "We present a framework for constructing real-time digital twins based on\nstructure-preserving reduced finite element models conditioned on a latent\nvariable Z. The approach uses conditional attention mechanisms to learn both a\nreduced finite element basis and a nonlinear conservation law within the\nframework of finite element exterior calculus (FEEC). This guarantees numerical\nwell-posedness and exact preservation of conserved quantities, regardless of\ndata sparsity or optimization error. The conditioning mechanism supports\nreal-time calibration to parametric variables, allowing the construction of\ndigital twins which support closed loop inference and calibration to sensor\ndata. The framework interfaces with conventional finite element machinery in a\nnon-invasive manner, allowing treatment of complex geometries and integration\nof learned models with conventional finite element techniques.\n  Benchmarks include advection diffusion, shock hydrodynamics, electrostatics,\nand a complex battery thermal runaway problem. The method achieves accurate\npredictions on complex geometries with sparse data (25 LES simulations),\nincluding capturing the transition to turbulence and achieving real-time\ninference ~0.1s with a speedup of 3.1x10^8 relative to LES. An open-source\nimplementation is available on GitHub.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8eFEEC\u548c\u6761\u4ef6\u6ce8\u610f\u529b\u673a\u5236\u7684\u964d\u9636\u6709\u9650\u5143\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u6784\u5efa\u53ef\u5b9e\u65f6\u6821\u51c6\u7684\u6570\u5b57\u5b6a\u751f\uff0c\u5728\u7a00\u758f\u6570\u636e\u548c\u590d\u6742\u51e0\u4f55\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u901f\u5ea6\u63d0\u5347\u5de8\u5927\u3002", "motivation": "\u4e3a\u4e86\u5728\u590d\u6742\u51e0\u4f55\u548c\u7a00\u758f\u6570\u636e\u6761\u4ef6\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u7269\u7406\u8fc7\u7a0b\u7684\u9ad8\u7cbe\u5ea6\u3001\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u6784\u5efa\uff0c\u4ee5\u652f\u6301\u95ed\u73af\u63a8\u65ad\u548c\u6821\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7ed3\u6784\u4fdd\u6301\u964d\u9636\u6709\u9650\u5143\u6a21\u578b\uff08\u53d7\u9690\u53d8\u91cfZ\u7684\u6761\u4ef6\u7ea6\u675f\uff09\u6765\u6784\u5efa\u5b9e\u65f6\u6570\u5b57\u5b6a\u751f\u7684\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u6761\u4ef6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u6709\u9650\u5143\u5916\u5fae\u79ef\u5206\uff08FEEC\uff09\u6846\u67b6\u5185\u5b66\u4e60\u964d\u9636\u6709\u9650\u5143\u57fa\u548c\u975e\u7ebf\u6027\u5b88\u6052\u5f8b\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e86\u6570\u503c\u9002\u5b9a\u6027\u548c\u5b88\u6052\u91cf\u7684\u7cbe\u786e\u4fdd\u6301\u3002\u8be5\u6761\u4ef6\u673a\u5236\u652f\u6301\u5bf9\u53c2\u6570\u53d8\u91cf\u7684\u5b9e\u65f6\u6821\u51c6\uff0c\u80fd\u591f\u6784\u5efa\u652f\u6301\u95ed\u73af\u63a8\u65ad\u548c\u4f20\u611f\u5668\u6570\u636e\u6821\u51c6\u7684\u6570\u5b57\u5b6a\u751f\u3002\u8be5\u6846\u67b6\u4ee5\u975e\u4fb5\u5165\u65b9\u5f0f\u4e0e\u4f20\u7edf\u7684\u6709\u9650\u5143\u673a\u68b0\u8bbe\u5907\u63a5\u53e3\uff0c\u80fd\u591f\u5904\u7406\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u5e76\u5c06\u5b66\u4e60\u6a21\u578b\u4e0e\u4f20\u7edf\u6709\u9650\u5143\u6280\u672f\u76f8\u7ed3\u5408\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5305\u542b\u5bf9\u6d41\u6269\u6563\u3001\u6fc0\u6ce2\u6c34\u52a8\u529b\u5b66\u3001\u9759\u7535\u5b66\u548c\u7535\u6c60\u70ed\u5931\u63a7\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u80fd\u5728\u7a00\u758f\u6570\u636e\uff0825\u4e2aLES\u6a21\u62df\uff09\u4e0b\u5bf9\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u8fdb\u884c\u51c6\u786e\u9884\u6d4b\uff0c\u5305\u62ec\u6355\u6349\u6e4d\u6d41\u8f6c\u53d8\uff0c\u5e76\u5b9e\u73b0\u7ea60.1\u79d2\u7684\u5b9e\u65f6\u63a8\u65ad\uff0c\u76f8\u6bd4\u4e8eLES\u67093.1x10^8\u7684\u52a0\u901f\u6bd4\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u4ee5\u6781\u9ad8\u7684\u7cbe\u5ea6\u548c\u6548\u7387\u6784\u5efa\u6570\u5b57\u5b6a\u751f\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u7269\u7406\u6a21\u62df\uff0c\u5e76\u80fd\u5b9e\u73b0\u5b9e\u65f6\u63a8\u65ad\u548c\u6821\u51c6\u3002"}}
{"id": "2508.07142", "categories": ["cs.LG", "cs.AI", "cs.IT", "cs.NA", "math.IT", "math.NA"], "pdf": "https://arxiv.org/pdf/2508.07142", "abs": "https://arxiv.org/abs/2508.07142", "authors": ["Vincent-Daniel Yun"], "title": "SGD Convergence under Stepsize Shrinkage in Low-Precision Training", "comment": null, "summary": "Low-precision training has become essential for reducing the computational\nand memory costs of large-scale deep learning. However, quantization of\ngradients introduces both magnitude shrinkage and additive noise, which can\nalter the convergence behavior of stochastic gradient descent (SGD). In this\nwork, we study the convergence of SGD under a gradient shrinkage model, where\neach stochastic gradient is scaled by a factor $q_k \\in (0,1]$ and perturbed by\nzero-mean quantization noise. We show that this shrinkage is equivalent to\nreplacing the nominal stepsize $\\mu_k$ with an effective stepsize $\\mu_k q_k$,\nwhich slows convergence when $q_{\\min} < 1$. Under standard smoothness and\nbounded-variance assumptions, we prove that low-precision SGD still converges,\nbut at a reduced rate determined by $q_{\\min}$, and with an increased\nasymptotic error floor due to quantization noise. We theoretically analyze how\nreduced numerical precision slows down training by modeling it as gradient\nshrinkage in the standard SGD convergence framework.", "AI": {"tldr": "\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u4f1a\u964d\u4f4eSGD\u7684\u6536\u655b\u901f\u5ea6\u5e76\u589e\u52a0\u8bef\u5dee", "motivation": "\u4f4e\u7cbe\u5ea6\u8bad\u7ec3\u5bf9\u4e8e\u964d\u4f4e\u5927\u89c4\u6a21\u6df1\u5ea6\u5b66\u4e60\u7684\u8ba1\u7b97\u548c\u5185\u5b58\u6210\u672c\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u91cf\u5316\u68af\u5ea6\u4f1a\u5f15\u5165\u91cf\u503c\u6536\u7f29\u548c\u52a0\u6027\u566a\u58f0\uff0c\u4ece\u800c\u6539\u53d8SGD\u7684\u6536\u655b\u884c\u4e3a", "method": "\u63d0\u51fa\u68af\u5ea6\u6536\u7f29\u6a21\u578b\uff0c\u5c06\u4f4e\u7cbe\u5ea6SGD\u7684\u6536\u655b\u6027\u5206\u6790\u7eb3\u5165\u6807\u51c6SGD\u6846\u67b6\uff0c\u8bc1\u660e\u4e86\u4f4e\u7cbe\u5ea6SGD\u7684\u6536\u655b\u6027\uff0c\u5e76\u5206\u6790\u4e86\u7cbe\u5ea6\u964d\u4f4e\u5bf9\u6536\u655b\u901f\u5ea6\u548c\u8bef\u5dee\u7684\u5f71\u54cd", "result": "\u8bc1\u660e\u4e86\u4f4e\u7cbe\u5ea6SGD\u5728\u6807\u51c6\u5e73\u6ed1\u548c\u6709\u754c\u65b9\u5dee\u5047\u8bbe\u4e0b\u4ecd\u7136\u6536\u655b\uff0c\u4f46\u6536\u655b\u901f\u5ea6\u964d\u4f4e\uff08\u7531q_min\u51b3\u5b9a\uff09\uff0c\u5e76\u4e14\u7531\u4e8e\u91cf\u5316\u566a\u58f0\u5bfc\u81f4\u6e10\u8fd1\u8bef\u5dee\u5730\u677f\u589e\u52a0", "conclusion": "\u4f4e\u7cbe\u5ea6SGD\u867d\u7136\u6536\u655b\uff0c\u4f46\u6536\u655b\u901f\u5ea6\u964d\u4f4e\uff0c\u6e10\u8fd1\u8bef\u5dee\u589e\u52a0"}}
{"id": "2508.07206", "categories": ["eess.SP", "cs.NA", "cs.SY", "eess.SY", "math.NA", "42C10, 94A12", "G.1.2; I.6.6"], "pdf": "https://arxiv.org/pdf/2508.07206", "abs": "https://arxiv.org/abs/2508.07206", "authors": ["Konstantin A. Rybakov", "Egor D. Shermatov"], "title": "Applying the Spectral Method for Modeling Linear Filters: Butterworth, Linkwitz-Riley, and Chebyshev filters", "comment": null, "summary": "This paper proposes a new technique for computer modeling linear filters\nbased on the spectral form of mathematical description of linear systems. It\nassumes that input and output signals of the filter are represented as\northogonal expansions, while filters themselves are described by\ntwo-dimensional non-stationary transfer functions. This technique allows one to\nmodel the output signal in continuous time, and it is successfully tested on\nthe Butterworth, Linkwitz-Riley, and Chebyshev filters with different orders.", "AI": {"tldr": "A new technique for computer modeling linear filters is proposed, using spectral representation and orthogonal expansions for signals and filters. It's tested successfully on various filter types.", "motivation": "To propose a new technique for computer modeling linear filters.", "method": "The technique models linear filters based on the spectral form of mathematical description of linear systems, representing input and output signals as orthogonal expansions and filters by two-dimensional non-stationary transfer functions.", "result": "The technique is successfully tested on Butterworth, Linkwitz-Riley, and Chebyshev filters with different orders.", "conclusion": "The proposed technique successfully models the output signal in continuous time and is tested on Butterworth, Linkwitz-Riley, and Chebyshev filters with different orders."}}
{"id": "2508.06863", "categories": ["cs.MA", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06863", "abs": "https://arxiv.org/abs/2508.06863", "authors": ["Hamidreza Asadian-Rad", "Hossein Soleimani", "Shahrokh Farahmand"], "title": "Energy Efficient Task Offloading in UAV-Enabled MEC Using a Fully Decentralized Deep Reinforcement Learning Approach", "comment": null, "summary": "Unmanned aerial vehicles (UAVs) have been recently utilized in multi-access\nedge computing (MEC) as edge servers. It is desirable to design UAVs'\ntrajectories and user to UAV assignments to ensure satisfactory service to the\nusers and energy efficient operation simultaneously. The posed optimization\nproblem is challenging to solve because: (i) The formulated problem is\nnon-convex, (ii) Due to the mobility of ground users, their future positions\nand channel gains are not known in advance, (iii) Local UAVs' observations\nshould be communicated to a central entity that solves the optimization\nproblem. The (semi-) centralized processing leads to communication overhead,\ncommunication/processing bottlenecks, lack of flexibility and scalability, and\nloss of robustness to system failures. To simultaneously address all these\nlimitations, we advocate a fully decentralized setup with no centralized\nentity. Each UAV obtains its local observation and then communicates with its\nimmediate neighbors only. After sharing information with neighbors, each UAV\ndetermines its next position via a locally run deep reinforcement learning\n(DRL) algorithm. None of the UAVs need to know the global communication graph.\nTwo main components of our proposed solution are (i) Graph attention layers\n(GAT), and (ii) Experience and parameter sharing proximal policy optimization\n(EPS-PPO). Our proposed approach eliminates all the limitations of\nsemi-centralized MADRL methods such as MAPPO and MA deep deterministic policy\ngradient (MADDPG), while guaranteeing a better performance than independent\nlocal DRLs such as in IPPO. Numerical results reveal notable performance gains\nin several different criteria compared to the existing MADDPG algorithm,\ndemonstrating the potential for offering a better performance, while utilizing\nlocal communications only.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eGAT\u548cEPS-PPO\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316DRL\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f18\u5316UAV\u5728MEC\u4e2d\u7684\u90e8\u7f72\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8eMADDPG\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u591a\u8def\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u4e2d\uff0c\u540c\u65f6\u786e\u4fdd\u7528\u6237\u6ee1\u610f\u5ea6\u548c\u80fd\u6e90\u6548\u7387\u6240\u9762\u4e34\u7684\u975e\u51f8\u6027\u3001\u7528\u6237\u672a\u6765\u4f4d\u7f6e\u548c\u4fe1\u9053\u589e\u76ca\u672a\u77e5\u3001\u4ee5\u53ca\uff08\u534a\uff09\u96c6\u4e2d\u5f0f\u5904\u7406\u5e26\u6765\u7684\u901a\u4fe1\u5f00\u9500\u3001\u74f6\u9888\u3001\u7f3a\u4e4f\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3001\u4ee5\u53ca\u5bb9\u9519\u6027\u5dee\u7b49\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u5c42\uff08GAT\uff09\u548c\u7ecf\u9a8c\u53c2\u6570\u5171\u4eab\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08EPS-PPO\uff09\u7684\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u591a\u8def\u63a5\u5165\u8fb9\u7f18\u8ba1\u7b97\uff08MEC\uff09\u4e2d\u7684\u8f68\u8ff9\u548c\u7528\u6237\u5206\u914d\u4f18\u5316\u95ee\u9898\u3002", "result": "\u4e0e\u73b0\u6709\u7684MADDPG\u7b97\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u6807\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u4ec5\u5229\u7528\u672c\u5730\u901a\u4fe1\u5373\u53ef\u83b7\u5f97\u66f4\u597d\u6027\u80fd\u7684\u6f5c\u529b\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5b8c\u5168\u53bb\u4e2d\u5fc3\u5316\u7684\u8bbe\u7f6e\uff0c\u6d88\u9664\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4fdd\u8bc1\u4e86\u6bd4\u72ec\u7acb\u672c\u5730DRL\u66f4\u597d\u7684\u6027\u80fd\uff0c\u540c\u65f6\u4ec5\u4f7f\u7528\u672c\u5730\u901a\u4fe1\u3002"}}
{"id": "2508.06642", "categories": ["physics.comp-ph", "cond-mat.mtrl-sci", "cs.LG", "physics.data-an"], "pdf": "https://arxiv.org/pdf/2508.06642", "abs": "https://arxiv.org/abs/2508.06642", "authors": ["Adedire D. Adesiji", "Jiashuo Wang", "Cheng-Shu Kuo", "Keith A. Brown"], "title": "Benchmarking Self-Driving Labs", "comment": null, "summary": "A key goal of modern materials science is accelerating the pace of materials\ndiscovery. Self-driving labs, or systems that select experiments using machine\nlearning and then execute them using automation, are designed to fulfil this\npromise by performing experiments faster, more intelligently, more reliably,\nand with richer metadata than conventional means. This review summarizes\nprogress in understanding the degree to which SDLs accelerate learning by\nquantifying how much they reduce the number of experiments required for a given\ngoal. The review begins by summarizing the theory underlying two key metrics,\nnamely acceleration factor AF and enhancement factor EF, which quantify how\nmuch faster and better an algorithm is relative to a reference strategy. Next,\nwe provide a comprehensive review of the literature, which reveals a wide range\nof AFs with a median of 6, and that tends to increase with the dimensionality\nof the space, reflecting an interesting blessing of dimensionality. In\ncontrast, reported EF values vary by over two orders of magnitude, although\nthey consistently peak at 10-20 experiments per dimension. To understand these\nresults, we perform a series of simulated Bayesian optimization campaigns that\nreveal how EF depends upon the statistical properties of the parameter space\nwhile AF depends on its complexity. Collectively, these results reinforce the\nmotivation for using SDLs by revealing their value across a wide range of\nmaterial parameter spaces and provide a common language for quantifying and\nunderstanding this acceleration.", "AI": {"tldr": "\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\uff08SDL\uff09\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u548c\u81ea\u52a8\u5316\u6280\u672f\u52a0\u901f\u6750\u6599\u53d1\u73b0\u3002\u672c\u7efc\u8ff0\u56de\u987e\u4e86\u91cf\u5316SDL\u5b66\u4e60\u52a0\u901f\u6548\u679c\u7684\u6307\u6807\uff08AF\u548cEF\uff09\uff0c\u53d1\u73b0SDL\u80fd\u663e\u8457\u51cf\u5c11\u5b9e\u9a8c\u6b21\u6570\uff08AF\u4e2d\u4f4d\u6570\u4e3a6\uff09\uff0c\u4e14EF\u572810-20\u4e2a\u5b9e\u9a8c/\u7ef4\u65f6\u8fbe\u5230\u5cf0\u503c\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86SDL\u5728\u4e0d\u540c\u6750\u6599\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u4ef7\u503c\uff0c\u5e76\u4e3a\u7406\u89e3\u5176\u52a0\u901f\u6548\u5e94\u63d0\u4f9b\u4e86\u901a\u7528\u8bed\u8a00\u3002", "motivation": "\u73b0\u4ee3\u6750\u6599\u79d1\u5b66\u7684\u4e00\u4e2a\u5173\u952e\u76ee\u6807\u662f\u52a0\u901f\u6750\u6599\u53d1\u73b0\u7684\u8fdb\u7a0b\u3002\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\uff08SDL\uff09\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u9009\u62e9\u5b9e\u9a8c\u5e76\u5229\u7528\u81ea\u52a8\u5316\u6267\u884c\u5b9e\u9a8c\uff0c\u65e8\u5728\u901a\u8fc7\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5feb\u3001\u66f4\u667a\u80fd\u3001\u66f4\u53ef\u9760\u5730\u8fdb\u884c\u5b9e\u9a8c\u5e76\u751f\u6210\u66f4\u4e30\u5bcc\u7684\u5143\u6570\u636e\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u3002", "method": "\u672c\u6587\u9996\u5148\u603b\u7ed3\u4e86\u4e24\u79cd\u5173\u952e\u6307\u6807\u2014\u2014\u52a0\u901f\u56e0\u5b50\uff08AF\uff09\u548c\u589e\u5f3a\u56e0\u5b50\uff08EF\uff09\u2014\u2014\u7684\u7406\u8bba\u57fa\u7840\uff0c\u8fd9\u4e24\u79cd\u6307\u6807\u7528\u4e8e\u91cf\u5316\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u53c2\u8003\u7b56\u7565\u7684\u901f\u5ea6\u548c\u6027\u80fd\u63d0\u5347\u7a0b\u5ea6\u3002\u968f\u540e\uff0c\u5bf9\u73b0\u6709\u6587\u732e\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u7cfb\u5217\u6a21\u62df\u8d1d\u53f6\u65af\u4f18\u5316\u6d3b\u52a8\uff0c\u4ee5\u63ed\u793aEF\u5982\u4f55\u4f9d\u8d56\u4e8e\u53c2\u6570\u7a7a\u95f4\u7684\u7edf\u8ba1\u7279\u6027\uff0c\u800cAF\u5219\u4f9d\u8d56\u4e8e\u5176\u590d\u6742\u6027\u3002", "result": "\u6587\u732e\u56de\u987e\u663e\u793a\uff0cAF\u503c\u8303\u56f4\u5e7f\u6cdb\uff0c\u4e2d\u4f4d\u6570\u4e3a6\uff0c\u5e76\u4e14\u968f\u7740\u7a7a\u95f4\u7ef4\u5ea6\u7684\u589e\u52a0\u800c\u589e\u52a0\uff0c\u8fd9\u53cd\u6620\u4e86\u4e00\u79cd\u6709\u8da3\u7684\u201c\u7ef4\u5ea6\u8bc5\u5492\u201d\u73b0\u8c61\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0cEF\u503c\u53d8\u5316\u8d85\u8fc7\u4e24\u4e2a\u6570\u91cf\u7ea7\uff0c\u5c3d\u7ba1\u5b83\u4eec\u5728\u6bcf\u4e2a\u7ef4\u5ea610-20\u4e2a\u5b9e\u9a8c\u5904\u6301\u7eed\u8fbe\u5230\u5cf0\u503c\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u901a\u8fc7\u91cf\u5316\u81ea\u9a7e\u5b9e\u9a8c\u5ba4\uff08SDL\uff09\u5728\u5b9e\u73b0\u7279\u5b9a\u76ee\u6807\u6240\u9700\u7684\u5b9e\u9a8c\u6570\u91cf\u65b9\u9762\u7684\u7f29\u51cf\u7a0b\u5ea6\uff0c\u603b\u7ed3\u4e86\u5728\u7406\u89e3SDL\u52a0\u901f\u5b66\u4e60\u65b9\u9762\u7684\u8fdb\u5c55\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cSDL\u5728\u6750\u6599\u53d1\u73b0\u65b9\u9762\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u4ef7\u503c\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u8bed\u8a00\u6765\u91cf\u5316\u548c\u7406\u89e3\u8fd9\u79cd\u52a0\u901f\u3002"}}
{"id": "2508.06526", "categories": ["cs.DC", "cs.AI", "cs.AR"], "pdf": "https://arxiv.org/pdf/2508.06526", "abs": "https://arxiv.org/abs/2508.06526", "authors": ["Dong Liu", "Yanxuan Yu", "Ben Lengerich", "Ying Nian Wu", "Xuhong Wang"], "title": "PiKV: KV Cache Management System for Mixture of Experts", "comment": "Accepted to ICML ES-MoFo III WorkShop Paper Link:\n  https://openreview.net/pdf?id=hHoK1kBPd9 Github Link:\n  https://github.com/NoakLiu/PiKV", "summary": "As large language models continue to scale up in both size and context\nlength, the memory and communication cost of key-value (KV) cache storage has\nbecome a major bottleneck in multi-GPU and multi-node inference. While\nMoE-based architectures sparsify computation across experts, the corresponding\nKV caches remain dense and globally synchronized, resulting in significant\noverhead.\n  We introduce \\textbf{PiKV}, a parallel and distributed KV cache serving\nframework tailored for MoE architecture. PiKV leverages \\textit{expert-sharded\nKV storage} to partition caches across GPUs, \\textit{PiKV routing} to reduce\ntoken-to-KV access, and a \\textit{PiKV Scheduling} to adaptively retain\nquery-relevant entries. To further reduce memory usage, PiKV integrates\n\\textit{PiKV Compression} modules the caching pipeline for acceleration.\n  PiKV is recently publicly available as an open-source software library:\n\\href{https://github.com/NoakLiu/PiKV}{https://github.com/NoakLiu/PiKV}.\nExperiments details is recorded at:\n\\href{https://github.com/NoakLiu/PiKV/blob/main/downstream_tasks/README.md}{https://github.com/NoakLiu/PiKV/Experimental\\_Results}.\nWe also have PiKV integrated with Nvidia kvpress for acceleration, details see\n\\href{https://github.com/NoakLiu/PiKVpress}{https://github.com/NoakLiu/PiKVpress}.\nPiKV is still a living project, aiming to become a comprehesive KV Cache\nmanagement system for MoE Architectures.", "AI": {"tldr": "PiKV is an open-source framework that optimizes KV cache serving for Mixture-of-Experts (MoE) models by sharding, routing, scheduling, and compressing caches, thus reducing memory and communication overhead during inference.", "motivation": "The motivation is to address the memory and communication cost of KV cache storage, which has become a major bottleneck in multi-GPU and multi-node inference for large language models, especially in MoE-based architectures where KV caches remain dense and globally synchronized.", "method": "PiKV utilizes expert-sharded KV storage to partition caches across GPUs, PiKV routing to reduce token-to-KV access, PiKV scheduling to adaptively retain query-relevant entries, and PiKV compression modules to reduce memory usage.", "result": "PiKV reduces the memory and communication cost of KV cache storage, leading to acceleration in inference.", "conclusion": "PiKV is a parallel and distributed KV cache serving framework tailored for MoE architecture, which addresses the memory and communication cost bottleneck in large language models by leveraging expert-sharded KV storage, PiKV routing, PiKV scheduling, and PiKV compression. The project is open-source and aims to become a comprehensive KV cache management system for MoE architectures."}}
{"id": "2508.06518", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06518", "abs": "https://arxiv.org/abs/2508.06518", "authors": ["Ray Wai Man Kong"], "title": "Automated Seam Folding and Sewing Machine on Pleated Pants for Apparel Manufacturing", "comment": "13 pages, 9 figures", "summary": "The applied research is the design and development of an automated folding\nand sewing machine for pleated pants. It represents a significant advancement\nin addressing the challenges associated with manual sewing processes.\nTraditional methods for creating pleats are labour-intensive, prone to\ninconsistencies, and require high levels of skill, making automation a critical\nneed in the apparel industry. This research explores the technical feasibility\nand operational benefits of integrating advanced technologies into garment\nproduction, focusing on the creation of an automated machine capable of precise\nfolding and sewing operations and eliminating the marking operation.\n  The proposed machine incorporates key features such as a precision folding\nmechanism integrated into the automated sewing unit with real-time monitoring\ncapabilities. The results demonstrate remarkable improvements: the standard\nlabour time has been reduced by 93%, dropping from 117 seconds per piece to\njust 8 seconds with the automated system. Similarly, machinery time improved by\n73%, and the total output rate increased by 72%. These enhancements translate\ninto a cycle time reduction from 117 seconds per piece to an impressive 33\nseconds, enabling manufacturers to meet customer demand more swiftly. By\neliminating manual marking processes, the machine not only reduces labour costs\nbut also minimizes waste through consistent pleat formation. This automation\naligns with industry trends toward sustainability and efficiency, potentially\nreducing environmental impact by decreasing material waste and energy\nconsumption.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u81ea\u52a8\u5316\u6298\u53e0\u548c\u7f1d\u7eab\u673a\uff0c\u53ef\u5c06\u88e4\u5b50\u751f\u4ea7\u7684\u52b3\u52a8\u65f6\u95f4\u51cf\u5c1193%\uff0c\u5faa\u73af\u65f6\u95f4\u51cf\u5c1170%\uff0c\u4ece\u800c\u63d0\u9ad8\u6548\u7387\u5e76\u51cf\u5c11\u6d6a\u8d39\u3002", "motivation": "\u4f20\u7edf\u88e4\u5b50\u8936\u76b1\u7684\u5236\u4f5c\u65b9\u6cd5\u52b3\u52a8\u5bc6\u96c6\u3001\u6613\u51fa\u9519\u4e14\u9700\u8981\u9ad8\u6280\u80fd\u6c34\u5e73\uff0c\u56e0\u6b64\u5728\u670d\u88c5\u884c\u4e1a\u5b9e\u73b0\u81ea\u52a8\u5316\u52bf\u5728\u5fc5\u884c\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5c06\u5148\u8fdb\u6280\u672f\u878d\u5165\u670d\u88c5\u751f\u4ea7\u7684\u6280\u672f\u53ef\u884c\u6027\u548c\u64cd\u4f5c\u4f18\u52bf\uff0c\u91cd\u70b9\u662f\u5f00\u53d1\u80fd\u591f\u7cbe\u786e\u6298\u53e0\u548c\u7f1d\u7eab\u7684\u81ea\u52a8\u5316\u673a\u5668\uff0c\u4ece\u800c\u6d88\u9664\u6807\u8bb0\u5de5\u5e8f\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u548c\u5f00\u53d1\u4e00\u79cd\u96c6\u6210\u4e86\u7cbe\u5bc6\u6298\u53e0\u673a\u6784\u548c\u81ea\u52a8\u5316\u7f1d\u7eab\u5355\u5143\u5e76\u5177\u6709\u5b9e\u65f6\u76d1\u63a7\u529f\u80fd\u7684\u673a\u5668\uff0c\u89e3\u51b3\u4e86\u670d\u88c5\u884c\u4e1a\u5728\u751f\u4ea7\u8936\u76b1\u88e4\u5b50\u8fc7\u7a0b\u4e2d\u9047\u5230\u7684\u6311\u6218\u3002", "result": "\u81ea\u52a8\u5316\u7cfb\u7edf\u5c06\u6bcf\u4ef6\u7684\u52b3\u52a8\u65f6\u95f4\u4ece117\u79d2\u51cf\u5c11\u52308\u79d2\uff08\u964d\u4f4e93%\uff09\uff0c\u673a\u68b0\u65f6\u95f4\u63d0\u9ad8\u4e8673%\uff0c\u603b\u4ea7\u91cf\u63d0\u9ad8\u4e8672%\uff0c\u5faa\u73af\u65f6\u95f4\u4ece117\u79d2\u51cf\u5c11\u523033\u79d2\u3002\u901a\u8fc7\u6d88\u9664\u624b\u52a8\u6807\u8bb0\uff0c\u8be5\u673a\u5668\u964d\u4f4e\u4e86\u52b3\u52a8\u529b\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u7684\u8936\u76b1\u5f62\u6210\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u6750\u6599\u6d6a\u8d39\uff0c\u7b26\u5408\u884c\u4e1a\u5bf9\u53ef\u6301\u7eed\u6027\u548c\u6548\u7387\u7684\u5173\u6ce8\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u548c\u5f00\u53d1\u81ea\u52a8\u5316\u6298\u53e0\u548c\u7f1d\u7eab\u673a\uff0c\u89e3\u51b3\u4e86\u670d\u88c5\u884c\u4e1a\u624b\u52a8\u7f1d\u7eab\u8936\u76b1\u88e4\u5b50\u8fc7\u7a0b\u4e2d\u52b3\u52a8\u5bc6\u96c6\u3001\u4e0d\u4e00\u81f4\u548c\u9ad8\u6280\u80fd\u8981\u6c42\u7b49\u6311\u6218\u3002\u81ea\u52a8\u5316\u7cfb\u7edf\u5c06\u6bcf\u4ef6\u7684\u52b3\u52a8\u65f6\u95f4\u4ece117\u79d2\u51cf\u5c11\u52308\u79d2\uff0c\u673a\u68b0\u65f6\u95f4\u63d0\u9ad8\u4e8673%\uff0c\u603b\u4ea7\u91cf\u63d0\u9ad8\u4e8672%\uff0c\u5faa\u73af\u65f6\u95f4\u4ece117\u79d2\u51cf\u5c11\u523033\u79d2\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u6ee1\u8db3\u4e86\u5ba2\u6237\u9700\u6c42\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u6d88\u9664\u624b\u52a8\u6807\u8bb0\u8fc7\u7a0b\uff0c\u8be5\u673a\u5668\u964d\u4f4e\u4e86\u52b3\u52a8\u529b\u6210\u672c\uff0c\u5e76\u901a\u8fc7\u4e00\u81f4\u7684\u8936\u76b1\u5f62\u6210\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u6750\u6599\u6d6a\u8d39\uff0c\u7b26\u5408\u884c\u4e1a\u5bf9\u53ef\u6301\u7eed\u6027\u548c\u6548\u7387\u7684\u5173\u6ce8\u3002"}}
{"id": "2508.06596", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2508.06596", "abs": "https://arxiv.org/abs/2508.06596", "authors": ["Miko\u0142aj Sienicki", "Krzysztof Sienicki"], "title": "Comment on \"Unifying Aspects of Generalized Calculus\"", "comment": "Comment on arXiv:2010.03366, 8 pages, 10 references and 1 figure", "summary": "Czachor's recent proposal introduces a form of non-Newtonian calculus built\nby pulling back arithmetic operations through arbitrary bijections between\ncontinua. Although the idea is mathematically inventive, it runs into serious\nconceptual trouble when examined from a physical standpoint. Claims of\nuniversal applicability quickly unravel under scrutiny -- especially when\nconsidering pathological bijections like the Cantor function, which break the\nframework's core assumptions. When applied to domains such as relativity,\nentropy, or cosmology, the results often collapse into tautological\nrestatements lacking real predictive power. This commentary explores these\nissues in depth, highlighting where and why the formalism falls short of\nproviding a physically coherent theory.", "AI": {"tldr": "Czachor\u63d0\u51fa\u7684\u975e\u725b\u987f\u5fae\u79ef\u5206\u5728\u6570\u5b66\u4e0a\u5f88\u6709\u521b\u610f\uff0c\u4f46\u5728\u7269\u7406\u4e0a\u5b58\u5728\u4e25\u91cd\u6982\u5ff5\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u75c5\u6001\u53cc\u5c04\u548c\u7f3a\u4e4f\u5b9e\u9645\u9884\u6d4b\u80fd\u529b\u65f6\u3002", "motivation": "\u8bc4\u4f30Czachor\u63d0\u51fa\u7684\u975e\u725b\u987f\u5fae\u79ef\u5206\u5f62\u5f0f\u4e3b\u4e49\u5728\u7269\u7406\u5b66\u4e2d\u7684\u9002\u7528\u6027\uff0c\u7279\u522b\u662f\u5176\u5728\u76f8\u5bf9\u8bba\u3001\u71b5\u548c\u5b87\u5b99\u5b66\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5206\u6790\u4efb\u610f\u8fde\u7eed\u4f53\u4e4b\u95f4\u7684\u53cc\u5c04\u6765\u56de\u62c9\u7b97\u672f\u8fd0\u7b97\u6765\u6784\u5efa\u975e\u725b\u987f\u5fae\u79ef\u5206\uff0c\u5e76\u8bc4\u4f30\u5176\u5728\u7269\u7406\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u8be5\u5f62\u5f0f\u4e3b\u4e49\u5728\u5e94\u7528\u4e8e\u76f8\u5bf9\u8bba\u3001\u71b5\u6216\u5b87\u5b99\u5b66\u7b49\u9886\u57df\u65f6\uff0c\u7ed3\u679c\u5e38\u5e38\u5f52\u7ed3\u4e3a\u6ca1\u6709\u5b9e\u9645\u9884\u6d4b\u80fd\u529b\u7684\u540c\u4e49\u53cd\u590d\u91cd\u8ff0\uff0c\u5e76\u4e14\u5728\u8003\u8651\u50cf\u5eb7\u6258\u51fd\u6570\u8fd9\u6837\u7684\u75c5\u6001\u53cc\u5c04\u65f6\uff0c\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u5047\u8bbe\u88ab\u6253\u7834\u3002", "conclusion": "\u8be5\u5f62\u5f0f\u4e3b\u4e49\u672a\u80fd\u63d0\u4f9b\u4e00\u4e2a\u7269\u7406\u4e0a\u8fde\u8d2f\u7684\u7406\u8bba"}}
{"id": "2508.06978", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.06978", "abs": "https://arxiv.org/abs/2508.06978", "authors": ["Kwanhee Kyung", "Sungmin Yun", "Jung Ho Ahn"], "title": "SSD Offloading for LLM Mixture-of-Experts Weights Considered Harmful in Energy Efficiency", "comment": "4 pages, 6 figures, accepted at IEEE Computer Architecture Letters", "summary": "Large Language Models (LLMs) applying Mixture-of-Experts (MoE) scale to\ntrillions of parameters but require vast memory, motivating a line of research\nto offload expert weights from fast-but-small DRAM (HBM) to denser Flash SSDs.\nWhile SSDs provide cost-effective capacity, their read energy per bit is\nsubstantially higher than that of DRAM. This paper quantitatively analyzes the\nenergy implications of offloading MoE expert weights to SSDs during the\ncritical decode stage of LLM inference. Our analysis, comparing SSD, CPU memory\n(DDR), and HBM storage scenarios for models like DeepSeek-R1, reveals that\noffloading MoE weights to current SSDs drastically increases\nper-token-generation energy consumption (e.g., by up to ~12x compared to the\nHBM baseline), dominating the total inference energy budget. Although\ntechniques like prefetching effectively hide access latency, they cannot\nmitigate this fundamental energy penalty. We further explore future\ntechnological scaling, finding that the inherent sparsity of MoE models could\npotentially make SSDs energy-viable if Flash read energy improves\nsignificantly, roughly by an order of magnitude.", "AI": {"tldr": "\u8be5\u8bba\u6587\u5206\u6790\u4e86\u5728LLM\u63a8\u7406\u4e2d\u5c06MoE\u4e13\u5bb6\u6743\u91cd\u5378\u8f7d\u5230SSD\u7684\u80fd\u91cf\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136SSD\u6210\u672c\u4f4e\uff0c\u4f46\u5176\u8f83\u9ad8\u7684\u8bfb\u53d6\u80fd\u91cf\u6d88\u8017\u4f1a\u663e\u8457\u589e\u52a0\u6574\u4f53\u80fd\u91cf\u4f7f\u7528\u3002\u9664\u975eSSD\u7684\u8bfb\u53d6\u80fd\u91cf\u6709\u5927\u5e45\u63d0\u5347\uff0c\u5426\u5219\u5f53\u524d\u6280\u672f\u4e0bSSD\u4e0d\u9002\u7528\u4e8e\u6b64\u573a\u666f\u3002", "motivation": "LLM\u5e94\u7528MoE\u6a21\u578b\u53ef\u4ee5\u6269\u5c55\u5230\u4e07\u4ebf\u53c2\u6570\uff0c\u4f46\u9700\u8981\u5de8\u5927\u7684\u5185\u5b58\uff0c\u8fd9\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u5c06\u4e13\u5bb6\u6743\u91cd\u4ece\u5feb\u901f\u4f46\u5c0f\u7684DRAM\uff08HBM\uff09\u5378\u8f7d\u5230\u66f4\u5bc6\u96c6\u7684Flash SSD\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83SSD\u3001CPU\u5185\u5b58\uff08DDR\uff09\u548cHBM\u5b58\u50a8\u5668\u573a\u666f\uff0c\u5bf9LLM\u63a8\u7406\u7684\u5173\u952e\u89e3\u7801\u9636\u6bb5\u5c06MoE\u4e13\u5bb6\u6743\u91cd\u5378\u8f7d\u5230SSD\u7684\u80fd\u91cf\u5f71\u54cd\u8fdb\u884c\u4e86\u91cf\u5316\u5206\u6790\u3002", "result": "\u5c06MoE\u6743\u91cd\u5378\u8f7d\u5230\u76ee\u524d\u7684SSD\u4f1a\u5927\u5927\u589e\u52a0\u6bcf\u4e2atoken\u751f\u6210\u7684\u80fd\u91cf\u6d88\u8017\uff08\u4e0eHBM\u57fa\u7ebf\u76f8\u6bd4\u6700\u9ad8\u7ea6\u589e\u52a012\u500d\uff09\uff0c\u5e76\u4e3b\u5bfc\u603b\u7684\u63a8\u7406\u80fd\u91cf\u9884\u7b97\u3002", "conclusion": "\u867d\u7136\u9884\u53d6\u7b49\u6280\u672f\u53ef\u4ee5\u9690\u85cf\u8bbf\u95ee\u5ef6\u8fdf\uff0c\u4f46\u4e0d\u80fd\u51cf\u8f7b\u8fd9\u79cd\u6839\u672c\u6027\u7684\u80fd\u91cf\u635f\u5931\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63a2\u8ba8\u4e86\u672a\u6765\u7684\u6280\u672f\u6269\u5c55\uff0c\u53d1\u73b0\u5982\u679c\u95ea\u5b58\u8bfb\u53d6\u80fd\u91cf\u6709\u663e\u8457\uff08\u7ea6\u5341\u500d\uff09\u7684\u63d0\u9ad8\uff0cMoE\u6a21\u578b\u7684\u56fa\u6709\u7a00\u758f\u6027\u53ef\u80fd\u4f7fSSD\u5728\u80fd\u91cf\u4e0a\u53ef\u884c\u3002"}}
{"id": "2508.07240", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.07240", "abs": "https://arxiv.org/abs/2508.07240", "authors": ["Zixuan Li", "Zixiong Wang", "Jian Yang", "Milos Hasan", "Beibei Wang"], "title": "PureSample: Neural Materials Learned by Sampling Microgeometry", "comment": null, "summary": "Traditional physically-based material models rely on analytically derived\nbidirectional reflectance distribution functions (BRDFs), typically by\nconsidering statistics of micro-primitives such as facets, flakes, or spheres,\nsometimes combined with multi-bounce interactions such as layering and multiple\nscattering. These derivations are often complex and model-specific, and\ntypically consider a statistical aggregate of a large surface area, ignoring\nspatial variation. Once an analytic BRDF's evaluation is defined, one still\nneeds to design an importance sampling method for it, and a way to evaluate the\npdf of that sampling distribution, requiring further model-specific\nderivations.\n  We present PureSample: a novel neural BRDF representation that allows\nlearning a material's behavior purely by sampling forward random walks on the\nmicrogeometry, which is usually straightforward to implement. Our\nrepresentation allows for efficient importance sampling, pdf evaluation, and\nBRDF evaluation, for homogeneous as well as spatially varying materials.\n  We achieve this by two learnable components: first, the sampling distribution\nis modeled using a flow matching neural network, which allows both importance\nsampling and pdf evaluation; second, we introduce a view-dependent albedo term,\ncaptured by a lightweight neural network, which allows for converting a scalar\npdf value to a colored BRDF value for any pair of view and light directions.\n  We demonstrate PureSample on challenging materials, including multi-layered\nmaterials, multiple-scattering microfacet materials, and various other\nmicrostructures.", "AI": {"tldr": "PureSample \u662f\u4e00\u79cd\u65b0\u7684\u795e\u7ecf BRDF \u8868\u793a\uff0c\u5b83\u7eaf\u7cb9\u901a\u8fc7\u5728\u5fae\u51e0\u4f55\u4e0a\u8fdb\u884c\u6b63\u5411\u968f\u673a\u6e38\u52a8\u7684\u91c7\u6837\u6765\u5b66\u4e60\u6750\u8d28\u7684\u884c\u4e3a\uff0c\u4ece\u800c\u80fd\u591f\u8fdb\u884c\u9ad8\u6548\u7684\u91cd\u8981\u6027\u91c7\u6837\u3001PDF \u8bc4\u4f30\u548c BRDF \u8bc4\u4f30\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u6750\u8d28\u6a21\u578b\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u7269\u7406\u7684\u6750\u8d28\u6a21\u578b\u4f9d\u8d56\u4e8e\u89e3\u6790\u63a8\u5bfc\u7684 BRDF\uff0c\u901a\u5e38\u590d\u6742\u4e14\u7279\u5b9a\u4e8e\u6a21\u578b\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u7a7a\u95f4\u53d8\u5316\u3002\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "PureSample \u4f7f\u7528\u6d41\u5339\u914d\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u91c7\u6837\u5206\u5e03\u4ee5\u5b9e\u73b0\u91cd\u8981\u6027\u91c7\u6837\u548c PDF \u8bc4\u4f30\uff0c\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u89c6\u56fe\u76f8\u5173\u7684\u53cd\u7167\u7387\u9879\u6765\u5c06\u6807\u91cf PDF \u503c\u8f6c\u6362\u4e3a\u5f69\u8272 BRDF \u503c\u3002", "result": "PureSample \u80fd\u591f\u5bf9\u540c\u8d28\u548c\u7a7a\u95f4\u53d8\u5316\u7684\u6750\u8d28\u8fdb\u884c\u9ad8\u6548\u7684\u91cd\u8981\u6027\u91c7\u6837\u3001PDF \u8bc4\u4f30\u548c BRDF \u8bc4\u4f30\uff0c\u5e76\u5728\u591a\u5c42\u6750\u8d28\u3001\u591a\u6b21\u6563\u5c04\u5fae\u9762\u6750\u8d28\u548c\u5404\u79cd\u5176\u4ed6\u5fae\u7ed3\u6784\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u6750\u8d28\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "conclusion": "PureSample \u901a\u8fc7\u4f7f\u7528\u6d41\u5339\u914d\u795e\u7ecf\u7f51\u7edc\u5bf9\u91c7\u6837\u5206\u5e03\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u4e00\u4e2a\u89c6\u56fe\u76f8\u5173\u7684\u53cd\u7167\u7387\u9879\uff0c\u5b9e\u73b0\u4e86\u5bf9\u9f50, \u53d8\u5f02\u6750\u6599\u4ee5\u53ca\u591a\u5c42\u6750\u6599\u7b49\u5177\u6709\u6311\u6218\u6027\u6750\u6599\u7684\u6709\u6548\u8868\u793a\u548c\u91c7\u6837\u3002"}}
{"id": "2508.06495", "categories": ["cs.CL", "cs.AI", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06495", "abs": "https://arxiv.org/abs/2508.06495", "authors": ["Juliana Resplande Sant'anna Gomes", "Arlindo Rodrigues Galv\u00e3o Filho"], "title": "Semi-automated Fact-checking in Portuguese: Corpora Enrichment using Retrieval with Claim extraction", "comment": "Master Thesis in Computer Science at Federal University on Goias\n  (UFG). Written in Portuguese", "summary": "The accelerated dissemination of disinformation often outpaces the capacity\nfor manual fact-checking, highlighting the urgent need for Semi-Automated\nFact-Checking (SAFC) systems. Within the Portuguese language context, there is\na noted scarcity of publicly available datasets that integrate external\nevidence, an essential component for developing robust AFC systems, as many\nexisting resources focus solely on classification based on intrinsic text\nfeatures. This dissertation addresses this gap by developing, applying, and\nanalyzing a methodology to enrich Portuguese news corpora (Fake.Br, COVID19.BR,\nMuMiN-PT) with external evidence. The approach simulates a user's verification\nprocess, employing Large Language Models (LLMs, specifically Gemini 1.5 Flash)\nto extract the main claim from texts and search engine APIs (Google Search API,\nGoogle FactCheck Claims Search API) to retrieve relevant external documents\n(evidence). Additionally, a data validation and preprocessing framework,\nincluding near-duplicate detection, is introduced to enhance the quality of the\nbase corpora.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u6574\u5408\u5916\u90e8\u8bc1\u636e\uff0c\u4e30\u5bcc\u4e86\u8461\u8404\u7259\u8bed\u65b0\u95fb\u8bed\u6599\u5e93\uff0c\u4ee5\u652f\u6301\u534a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff08SAFC\uff09\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u8461\u8404\u7259\u8bed\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\u3002", "motivation": "\u8be5\u8bba\u6587\u7684\u52a8\u673a\u5728\u4e8e\uff0c\u865a\u5047\u4fe1\u606f\u7684\u52a0\u901f\u4f20\u64ad\u80fd\u529b\u8d85\u8fc7\u4e86\u4eba\u5de5\u4e8b\u5b9e\u6838\u67e5\u7684\u5e94\u5bf9\u80fd\u529b\uff0c\u56e0\u6b64\u8feb\u5207\u9700\u8981\u534a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff08SAFC\uff09\u7cfb\u7edf\u3002\u7279\u522b\u662f\u5728\u8461\u8404\u7259\u8bed\u73af\u5883\u4e2d\uff0c\u516c\u5f00\u53ef\u7528\u7684\u6574\u5408\u4e86\u5916\u90e8\u8bc1\u636e\u7684\u6570\u636e\u96c6\u5374\u5f88\u5c11\uff0c\u800c\u5916\u90e8\u8bc1\u636e\u662f\u5f00\u53d1\u5f3a\u5927\u7684 AFC \u7cfb\u7edf\u5fc5\u4e0d\u53ef\u5c11\u7684\u7ec4\u6210\u90e8\u5206\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e86\u4e00\u79cd\u6a21\u62df\u7528\u6237\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff0c\u7279\u522b\u662f Gemini 1.5 Flash\uff09\u63d0\u53d6\u6587\u672c\u4e2d\u7684\u4e3b\u8981\u8bba\u70b9\uff0c\u5e76\u4f7f\u7528\u641c\u7d22\u5f15\u64ce API\uff08Google Search API\u3001Google FactCheck Claims Search API\uff09\u68c0\u7d22\u76f8\u5173\u7684\u5916\u90e8\u8bc1\u636e\u6587\u6863\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u6570\u636e\u9a8c\u8bc1\u548c\u9884\u5904\u7406\u6846\u67b6\uff0c\u5305\u62ec\u8fd1\u91cd\u590d\u68c0\u6d4b\uff0c\u4ee5\u63d0\u9ad8\u57fa\u7840\u8bed\u6599\u5e93\u7684\u8d28\u91cf\u3002", "result": "\u8be5\u8bba\u6587\u6210\u529f\u5730\u5f00\u53d1\u5e76\u5e94\u7528\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u4e3a\u8461\u8404\u7259\u8bed\u65b0\u95fb\u8bed\u6599\u5e93\uff08Fake.Br\u3001COVID19.BR\u3001MuMiN-PT\uff09\u589e\u52a0\u4e86\u5916\u90e8\u8bc1\u636e\uff0c\u586b\u8865\u4e86\u73b0\u6709\u8d44\u6e90\u5728\u6574\u5408\u5916\u90e8\u8bc1\u636e\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u4e3a\u5f00\u53d1\u66f4\u5f3a\u5927\u7684\u8461\u8404\u7259\u8bed SAFC \u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u8be5\u8bba\u6587\u5f00\u53d1\u4e86\u4e00\u79cd\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5916\u90e8\u8bc1\u636e\u6765\u4e30\u5bcc\u8461\u8404\u7259\u8bed\u65b0\u95fb\u8bed\u6599\u5e93\uff0c\u4ee5\u89e3\u51b3\u534a\u81ea\u52a8\u5316\u4e8b\u5b9e\u6838\u67e5\uff08SAFC\uff09\u7cfb\u7edf\u7684\u6570\u636e\u96c6\u7a00\u7f3a\u6027\u95ee\u9898\u3002"}}
{"id": "2508.06550", "categories": ["cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06550", "abs": "https://arxiv.org/abs/2508.06550", "authors": ["Yinqiu Huang", "Hao Ma", "Wenshuai Chen", "Shuli Wang", "Yongqiang Zhang", "Xue Wei", "Yinhua Zhu", "Haitao Wang", "Xingxing Wang"], "title": "Generative Bid Shading in Real-Time Bidding Advertising", "comment": null, "summary": "Bid shading plays a crucial role in Real-Time Bidding~(RTB) by adaptively\nadjusting the bid to avoid advertisers overspending. Existing mainstream\ntwo-stage methods, which first model bid landscapes and then optimize surplus\nusing operations research techniques, are constrained by unimodal assumptions\nthat fail to adapt for non-convex surplus curves and are vulnerable to\ncascading errors in sequential workflows. Additionally, existing discretization\nmodels of continuous values ignore the dependence between discrete intervals,\nreducing the model's error correction ability, while sample selection bias in\nbidding scenarios presents further challenges for prediction. To address these\nissues, this paper introduces Generative Bid Shading~(GBS), which comprises two\nprimary components: (1) an end-to-end generative model that utilizes an\nautoregressive approach to generate shading ratios by stepwise residuals,\ncapturing complex value dependencies without relying on predefined priors; and\n(2) a reward preference alignment system, which incorporates a channel-aware\nhierarchical dynamic network~(CHNet) as the reward model to extract\nfine-grained features, along with modules for surplus optimization and\nexploration utility reward alignment, ultimately optimizing both short-term and\nlong-term surplus using group relative policy optimization~(GRPO). Extensive\nexperiments on both offline and online A/B tests validate GBS's effectiveness.\nMoreover, GBS has been deployed on the Meituan DSP platform, serving billions\nof bid requests daily.", "AI": {"tldr": "GBS\u901a\u8fc7\u7aef\u5230\u7aef\u7684\u751f\u6210\u6a21\u578b\u548c\u5956\u52b1\u504f\u597d\u5bf9\u9f50\u7cfb\u7edf\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u7ade\u4ef7\u7740\u8272\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u80fd\u591f\u5904\u7406\u975e\u51f8\u5269\u4f59\u66f2\u7ebf\u548c\u4f9d\u8d56\u5173\u7cfb\uff0c\u5e76\u4f18\u5316\u77ed\u671f\u548c\u957f\u671f\u5269\u4f59\uff0c\u5df2\u5728\u5b9e\u9645\u5e73\u53f0\u90e8\u7f72\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u4e3b\u6d41\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u53d7\u9650\u4e8e\u5355\u5cf0\u5047\u8bbe\uff0c\u65e0\u6cd5\u9002\u5e94\u975e\u51f8\u5269\u4f59\u66f2\u7ebf\uff0c\u5e76\u4e14\u5728\u987a\u5e8f\u5de5\u4f5c\u6d41\u4e2d\u5bb9\u6613\u51fa\u73b0\u7ea7\u8054\u9519\u8bef\u3002\u6b64\u5916\uff0c\u8fde\u7eed\u503c\u7684\u79bb\u6563\u5316\u6a21\u578b\u5ffd\u7565\u4e86\u79bb\u6563\u533a\u95f4\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u964d\u4f4e\u4e86\u6a21\u578b\u7684\u8bef\u5dee\u6821\u6b63\u80fd\u529b\uff0c\u800c\u7ade\u4ef7\u573a\u666f\u4e2d\u7684\u6837\u672c\u9009\u62e9\u504f\u5dee\u5219\u7ed9\u9884\u6d4b\u5e26\u6765\u4e86\u8fdb\u4e00\u6b65\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u751f\u6210\u5f0f\u7ade\u4ef7\u7740\u8272\uff08GBS\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u4e3b\u8981\u90e8\u5206\uff1a1.\u7aef\u5230\u7aef\u7684\u751f\u6210\u6a21\u578b\uff0c\u4f7f\u7528\u81ea\u56de\u5f52\u65b9\u6cd5\u901a\u8fc7\u9010\u6b65\u6b8b\u5dee\u751f\u6210\u7740\u8272\u6bd4\u7387\uff0c\u6355\u6349\u590d\u6742\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u65e0\u9700\u9884\u5b9a\u4e49\u5148\u9a8c\uff1b2.\u5956\u52b1\u504f\u597d\u5bf9\u9f50\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u5c06\u6e20\u9053\u611f\u77e5\u7684\u5206\u5c42\u52a8\u6001\u7f51\u7edc\uff08CHNet\uff09\u4f5c\u4e3a\u5956\u52b1\u6a21\u578b\u63d0\u53d6\u7ec6\u7c92\u5ea6\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u5269\u4f59\u4f18\u5316\u548c\u63a2\u7d22\u6548\u7528\u5956\u52b1\u5bf9\u9f50\u6a21\u5757\uff0c\u4f7f\u7528\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff08GRPO\uff09\u6765\u4f18\u5316\u77ed\u671f\u548c\u957f\u671f\u5269\u4f59\u3002", "result": "GBS\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u5728Meituan DSP\u5e73\u53f0\u6210\u529f\u90e8\u7f72\uff0c\u670d\u52a1\u4e8e\u65e5\u5e38\u6570\u5341\u4ebf\u7684\u7ade\u4ef7\u8bf7\u6c42\u3002", "conclusion": "GBS\u5728Meituan DSP\u5e73\u53f0\u5df2\u6210\u529f\u90e8\u7f72\uff0c\u5e76\u670d\u52a1\u4e8e\u65e5\u5e38\u6570\u5341\u4ebf\u7684\u7ade\u4ef7\u8bf7\u6c42\uff0c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86GBS\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06567", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06567", "abs": "https://arxiv.org/abs/2508.06567", "authors": ["Ngoc Son Vu", "Van Cuong Pham", "Phuc Anh Nguyen", "My Linh Dao Thi", "Thanh Hai Vu"], "title": "Robust pid sliding mode control for dc servo motor speed control", "comment": null, "summary": "This research proposes a Sliding Mode PID (SMC-PID) controller to improve the\nspeed control performance of DC servo motors, which are widely used in\nindustrial applications such as robotics and CNC. The objective of the proposed\ncontroller is to enhance the speed control performance of DC servo motors on\nthe CE110 Servo Trainer. The proposed method integrates a traditional PID\ncontroller with a sliding mode control mechanism to effectively handle system\nuncertainties and disturbances. Experimental results show that the SMC-PID\nmethod provides significant improvements in accuracy and stability compared to\ntraditional PID controllers, with metrics such as reduced overshoot, shorter\nsettling time, and increased adaptability to system uncertainties. This\nresearch highlights the effectiveness of the SMC-PID controller, enhancing the\nperformance of DC servo motor speed control.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdSMC-PID\u63a7\u5236\u5668\uff0c\u7528\u4e8e\u63d0\u9ad8\u76f4\u6d41\u4f3a\u670d\u7535\u673a\u901f\u5ea6\u63a7\u5236\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u76f4\u6d41\u4f3a\u670d\u7535\u673a\u5728\u5de5\u4e1a\u5e94\u7528\uff08\u5982\u673a\u5668\u4eba\u548c\u6570\u63a7\u673a\u5e8a\uff09\u4e2d\u7684\u901f\u5ea6\u63a7\u5236\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u5c06\u6ed1\u6a21\u63a7\u5236\u673a\u5236\u4e0e\u4f20\u7edfPID\u63a7\u5236\u5668\u76f8\u7ed3\u5408\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u6ed1\u6a21PID\uff08SMC-PID\uff09\u63a7\u5236\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edfPID\u63a7\u5236\u5668\u76f8\u6bd4\uff0cSMC-PID\u65b9\u6cd5\u5728\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u65b9\u9762\u6709\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u5177\u6709\u66f4\u5c0f\u7684\u8d85\u8c03\u91cf\u3001\u66f4\u77ed\u7684\u5efa\u7acb\u65f6\u95f4\u548c\u66f4\u5f3a\u7684\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\u9002\u5e94\u80fd\u529b\u3002", "conclusion": "SMC-PID\u63a7\u5236\u5668\u80fd\u6709\u6548\u63d0\u9ad8\u76f4\u6d41\u4f3a\u670d\u7535\u673a\u901f\u5ea6\u63a7\u5236\u7684\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\uff0c\u5e76\u5177\u6709\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06730", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.06730", "abs": "https://arxiv.org/abs/2508.06730", "authors": ["Lauren A Hurley", "Sean E Shaheen"], "title": "Reservoir computing with large valid prediction time for the Lorenz system", "comment": null, "summary": "We study the dependence of the Valid Prediction Time (VPT) of Reservoir\nComputers (RCs) on hyperparameters including the regularization coefficient,\nreservoir size, and spectral radius. Under carefully chosen conditions, the RC\ncan achieve approximately 70% of a benchmark performance, based on the output\nof a single prediction step used as initial conditions for the Lorenz\nequations. We report high VPT values (>30 Lyapunov times), as we are predicting\na noiseless system where overfitting can be beneficial. While these conditions\nmay not hold for noisy systems, they could still be useful for real-world\napplications with limited noise. Furthermore, utilizing knowledge of the\nLyapunov exponent, we find that the VPT can be predicted by the error in the\nfirst few prediction steps, offering a computationally efficient evaluation\nmethod. We emphasize the importance of the numerical solver used to generate\nthe Lorenz dataset and define a Valid Ground Truth Time (VGTT), during which\nthe outputs of several common solvers agree. A VPT exceeding the VGTT is not\nmeaningful, as a different solver could produce a different result. Lastly, we\nidentify two spectral radius regimes that achieve large VPT: a small radius\nnear zero, resulting in simple but stable operation, and a larger radius\noperating at the \"edge of chaos.\"", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u8d85\u53c2\u6570\u5bf9\u50a8\u5c42\u8ba1\u7b97\u673a\uff08RC\uff09\u6709\u6548\u9884\u6d4b\u65f6\u95f4\uff08VPT\uff09\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b VPT \u8d85\u8fc7 30 \u4e2a\u674e\u4e9a\u666e\u8bfa\u592b\u65f6\u95f4\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u9884\u6d4b\u8bef\u5dee\u6765\u8bc4\u4f30 VPT\u3002\u6587\u7ae0\u8fd8\u786e\u5b9a\u4e86\u5b9e\u73b0\u5927 VPT \u7684\u4e24\u4e2a\u8c31\u534a\u5f84\u533a\u57df\uff0c\u5e76\u5f3a\u8c03\u4e86\u6570\u503c\u6c42\u89e3\u5668\u5bf9\u7ed3\u679c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76\u8d85\u53c2\u6570\uff08\u5982\u6b63\u5219\u5316\u7cfb\u6570\u3001\u50a8\u5c42\u5927\u5c0f\u548c\u8c31\u534a\u5f84\uff09\u5bf9\u50a8\u5c42\u8ba1\u7b97\u673a\uff08RC\uff09\u7684\u6709\u6548\u9884\u6d4b\u65f6\u95f4\uff08VPT\uff09\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e00\u79cd\u8ba1\u7b97\u4e0a\u6709\u6548\u7684\u8bc4\u4f30 VPT \u7684\u65b9\u6cd5\u3002", "method": "\u672c\u6587\u7814\u7a76\u4e86\u6b63\u5219\u5316\u7cfb\u6570\u3001\u50a8\u5c42\u5927\u5c0f\u548c\u8c31\u534a\u5f84\u7b49\u8d85\u53c2\u6570\u5bf9\u50a8\u5c42\u8ba1\u7b97\u673a\uff08RC\uff09\u7684\u6709\u6548\u9884\u6d4b\u65f6\u95f4\uff08VPT\uff09\u7684\u5f71\u54cd\u3002\u6211\u4eec\u4f7f\u7528\u62c9\u8d8a\u8bfa\u57c3\uff08Lorenz\uff09\u65b9\u7a0b\u7684\u5355\u6b65\u9884\u6d4b\u4f5c\u4e3a\u521d\u59cb\u6761\u4ef6\u6765\u8bc4\u4f30 RC \u7684\u6027\u80fd\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5229\u7528\u4e86\u674e\u4e9a\u666e\u8bfa\u592b\u6307\u6570\u7684\u77e5\u8bc6\uff0c\u5e76\u901a\u8fc7\u524d\u51e0\u6b65\u9884\u6d4b\u7684\u8bef\u5dee\u6765\u9884\u6d4b VPT\u3002\u6211\u4eec\u8fd8\u5b9a\u4e49\u4e86\u6709\u6548\u57fa\u51c6\u65f6\u95f4\uff08VGTT\uff09\uff0c\u4ee5\u8bc4\u4f30\u6570\u503c\u6c42\u89e3\u5668\u7684\u91cd\u8981\u6027\u3002", "result": "\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\uff0cRC \u7684 VPT \u8d85\u8fc7 30 \u4e2a\u674e\u4e9a\u666e\u8bfa\u592b\u65f6\u95f4\u3002\u9884\u6d4b\u7684 VPT \u53ef\u4ee5\u901a\u8fc7\u524d\u51e0\u6b65\u9884\u6d4b\u7684\u8bef\u5dee\u6765\u8ba1\u7b97\u3002\u5b58\u5728\u4e24\u4e2a\u8c31\u534a\u5f84\u533a\u57df\u53ef\u4ee5\u5b9e\u73b0\u5927 VPT\uff1a\u63a5\u8fd1\u96f6\u7684\u5c0f\u534a\u5f84\u548c\u63a5\u8fd1\u201c\u6df7\u6c8c\u8fb9\u7f18\u201d\u7684\u5927\u534a\u5f84\u3002", "conclusion": "\u5728\u7814\u7a76\u4e86\u6b63\u5219\u5316\u7cfb\u6570\u3001\u50a8\u5c42\u5927\u5c0f\u548c\u8c31\u534a\u5f84\u7b49\u8d85\u53c2\u6570\u5bf9\u50a8\u5c42\u8ba1\u7b97\u673a\uff08RC\uff09\u7684\u6709\u6548\u9884\u6d4b\u65f6\u95f4\uff08VPT\uff09\u7684\u5f71\u54cd\u540e\uff0c\u6211\u4eec\u53d1\u73b0\uff0c\u5728\u4ed4\u7ec6\u9009\u62e9\u7684\u6761\u4ef6\u4e0b\uff0cRC \u53ef\u4ee5\u8fbe\u5230\u7ea6 70% \u7684\u57fa\u51c6\u6027\u80fd\u3002\u901a\u8fc7\u5229\u7528\u4e0e\u62c9\u8d8a\u8bfa\u57c3\uff08Lorenz\uff09\u65b9\u7a0b\u521d\u59cb\u6761\u4ef6\u8f93\u51fa\u76f8\u5173\u7684\u5355\u6b65\u9884\u6d4b\uff0c\u6211\u4eec\u62a5\u544a\u4e86\u8d85\u8fc7 30 \u4e2a\u674e\u4e9a\u666e\u8bfa\u592b\u65f6\u95f4\uff08Lyapunov times\uff09\u7684\u9ad8 VPT \u503c\uff0c\u8fd9\u5728\u9884\u6d4b\u65e0\u566a\u58f0\u7cfb\u7edf\u65f6\u5c24\u4e3a\u660e\u663e\uff0c\u56e0\u4e3a\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u8fc7\u62df\u5408\u53ef\u80fd\u662f\u6709\u76ca\u7684\u3002\u5c3d\u7ba1\u8fd9\u4e9b\u6761\u4ef6\u53ef\u80fd\u4e0d\u9002\u7528\u4e8e\u6709\u566a\u58f0\u7684\u7cfb\u7edf\uff0c\u4f46\u5b83\u4eec\u4ecd\u53ef\u80fd\u5bf9\u566a\u58f0\u6709\u9650\u7684\u5b9e\u9645\u5e94\u7528\u6709\u7528\u3002\u6b64\u5916\uff0c\u6211\u4eec\u53d1\u73b0\u901a\u8fc7\u5229\u7528\u674e\u4e9a\u666e\u8bfa\u592b\u6307\u6570\u7684\u77e5\u8bc6\uff0c\u53ef\u4ee5\u901a\u8fc7\u524d\u51e0\u6b65\u9884\u6d4b\u7684\u8bef\u5dee\u6765\u9884\u6d4b VPT\uff0c\u8fd9\u63d0\u4f9b\u4e86\u4e00\u79cd\u8ba1\u7b97\u4e0a\u6709\u6548\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002\u6211\u4eec\u5f3a\u8c03\u4e86\u7528\u4e8e\u751f\u6210\u62c9\u8d8a\u8bfa\u57c3\u6570\u636e\u96c6\u7684\u6570\u503c\u6c42\u89e3\u5668\u7684\u91cd\u8981\u6027\uff0c\u5e76\u5b9a\u4e49\u4e86\u6709\u6548\u57fa\u51c6\u65f6\u95f4\uff08VGTT\uff09\uff0c\u5728\u6b64\u671f\u95f4\uff0c\u51e0\u4e2a\u901a\u7528\u6c42\u89e3\u5668\u7684\u8f93\u51fa\u662f\u4e00\u81f4\u7684\u3002\u8d85\u8fc7 VGTT \u7684 VPT \u6ca1\u6709\u610f\u4e49\uff0c\u56e0\u4e3a\u4e0d\u540c\u7684\u6c42\u89e3\u5668\u53ef\u80fd\u4f1a\u4ea7\u751f\u4e0d\u540c\u7684\u7ed3\u679c\u3002\u6700\u540e\uff0c\u6211\u4eec\u786e\u5b9a\u4e86\u5b9e\u73b0\u5927 VPT \u7684\u4e24\u4e2a\u8c31\u534a\u5f84\u533a\u57df\uff1a\u63a5\u8fd1\u96f6\u7684\u5c0f\u534a\u5f84\uff0c\u53ef\u5b9e\u73b0\u7b80\u5355\u4f46\u7a33\u5b9a\u7684\u64cd\u4f5c\uff1b\u4ee5\u53ca\u5728\u5927\u7ea6\u201c\u6df7\u6c8c\u8fb9\u7f18\u201d\u8fd0\u884c\u7684\u8f83\u5927\u534a\u5f84\u3002"}}
{"id": "2508.06655", "categories": ["cs.SI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.06655", "abs": "https://arxiv.org/abs/2508.06655", "authors": ["Qiheng Lu", "Nicholas D. Sidiropoulos", "Aritra Konar"], "title": "The Vertex-Attribute-Constrained Densest $k$-Subgraph Problem", "comment": null, "summary": "Dense subgraph mining is a fundamental technique in graph mining, commonly\napplied in fraud detection, community detection, product recommendation, and\ndocument summarization. In such applications, we are often interested in\nidentifying communities, recommendations, or summaries that reflect different\nconstituencies, styles or genres, and points of view. For this task, we\nintroduce a new variant of the Densest $k$-Subgraph (D$k$S) problem that\nincorporates the attribute values of vertices. The proposed\nVertex-Attribute-Constrained Densest $k$-Subgraph (VAC-D$k$S) problem retains\nthe NP-hardness and inapproximability properties of the classical D$k$S.\nNevertheless, we prove that a suitable continuous relaxation of VAC-D$k$S is\ntight and can be efficiently tackled using a projection-free Frank--Wolfe\nalgorithm. We also present an insightful analysis of the optimization landscape\nof the relaxed problem. Extensive experimental results demonstrate the\neffectiveness of our proposed formulation and algorithm, and its ability to\nscale up to large graphs. We further elucidate the properties of VAC-D$k$S\nversus classical D$k$S in a political network mining application, where\nVAC-D$k$S identifies a balanced and more meaningful set of politicians\nrepresenting different ideological camps, in contrast to the classical D$k$S\nsolution which is unbalanced and rather mundane.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6700\u5bc6\u5b50\u56fe\u6316\u6398\u95ee\u9898 (VAC-DkS)\uff0c\u8be5\u95ee\u9898\u8003\u8651\u4e86\u9876\u70b9\u7684\u5c5e\u6027\uff0c\u5e76\u4f7f\u7528 Frank-Wolfe \u7b97\u6cd5\u89e3\u51b3\u4e86\u5b83\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u6bd4\u7ecf\u5178\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5c24\u5176\u662f\u5728\u653f\u6cbb\u7f51\u7edc\u5206\u6790\u4e2d\u3002", "motivation": "\u5728\u6b3a\u8bc8\u68c0\u6d4b\u3001\u793e\u533a\u68c0\u6d4b\u3001\u4ea7\u54c1\u63a8\u8350\u548c\u6587\u6863\u6458\u8981\u7b49\u5e94\u7528\u4e2d\uff0c\u9700\u8981\u8bc6\u522b\u53cd\u6620\u4e0d\u540c\u9009\u6c11\u3001\u98ce\u683c\u3001\u4f53\u88c1\u548c\u89c2\u70b9\u7684\u793e\u533a\u3001\u63a8\u8350\u6216\u6458\u8981\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5305\u542b\u9876\u70b9\u5c5e\u6027\u503c\u7684\u65b0\u578b\u6700\u5bc6\u5b50\u56fe\u6316\u6398\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9876\u70b9\u5c5e\u6027\u7ea6\u675f\u6700\u5bc6 k-\u5b50\u56fe (VAC-DkS) \u95ee\u9898\uff0c\u8be5\u95ee\u9898\u4fdd\u7559\u4e86\u7ecf\u5178 DkS \u7684 NP-\u96be\u548c\u8fd1\u4f3c\u96be\u6027\u8d28\u3002\u901a\u8fc7\u8bc1\u660e VAC-DkS \u7684\u8fde\u7eed\u677e\u5f1b\u662f\u6709\u6548\u7684\uff0c\u5e76\u4f7f\u7528\u65e0\u6295\u5f71\u7684 Frank-Wolfe \u7b97\u6cd5\u8fdb\u884c\u6709\u6548\u5904\u7406\u3002\u8fd8\u5bf9\u677e\u5f1b\u95ee\u9898\u7684\u4f18\u5316\u8fdb\u884c\u4e86\u5206\u6790\u3002", "result": "\u63d0\u51fa\u7684 VAC-DkS \u516c\u5f0f\u548c\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5927\u89c4\u6a21\u56fe\uff0c\u5e76\u4e14\u6bd4\u7ecf\u5178\u7684 DkS \u80fd\u591f\u53d1\u73b0\u66f4\u6709\u610f\u4e49\u7684\u5b50\u56fe\u3002", "conclusion": "VAC-DkS \u8bc6\u522b\u51fa\u7684\u653f\u6cbb\u7f51\u7edc\u6bd4\u7ecf\u5178\u7684 DkS \u66f4\u5e73\u8861\u3001\u66f4\u6709\u610f\u4e49\uff0c\u80fd\u591f\u4ee3\u8868\u4e0d\u540c\u7684\u610f\u8bc6\u5f62\u6001\u9635\u8425\u3002"}}
{"id": "2508.06496", "categories": ["cs.CV", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06496", "abs": "https://arxiv.org/abs/2508.06496", "authors": ["Rakesh Raj Madavan", "Akshat Kaimal", "Hashim Faisal", "Chandrakala S"], "title": "Med-GRIM: Enhanced Zero-Shot Medical VQA using prompt-embedded Multimodal Graph RAG", "comment": null, "summary": "An ensemble of trained multimodal encoders and vision-language models (VLMs)\nhas become a standard approach for visual question answering (VQA) tasks.\nHowever, such models often fail to produce responses with the detailed\nprecision necessary for complex, domain-specific applications such as medical\nVQA. Our representation model, BIND: BLIVA Integrated with Dense Encoding,\nextends prior multimodal work by refining the joint embedding space through\ndense, query-token-based encodings inspired by contrastive pretraining\ntechniques. This refined encoder powers Med-GRIM, a model designed for medical\nVQA tasks that leverages graph-based retrieval and prompt engineering to\nintegrate domain-specific knowledge. Rather than relying on compute-heavy\nfine-tuning of vision and language models on specific datasets, Med-GRIM\napplies a low-compute, modular workflow with small language models (SLMs) for\nefficiency. Med-GRIM employs prompt-based retrieval to dynamically inject\nrelevant knowledge, ensuring both accuracy and robustness in its responses. By\nassigning distinct roles to each agent within the VQA system, Med-GRIM achieves\nlarge language model performance at a fraction of the computational cost.\nAdditionally, to support scalable research in zero-shot multimodal medical\napplications, we introduce DermaGraph, a novel Graph-RAG dataset comprising\ndiverse dermatological conditions. This dataset facilitates both multimodal and\nunimodal querying. The code and dataset are available at:\nhttps://github.com/Rakesh-123-cryp/Med-GRIM.git", "AI": {"tldr": "BIND\u6a21\u578b\u548cMed-GRIM\u7cfb\u7edf\u5728\u533b\u5b66VQA\u4efb\u52a1\u4e2d\u901a\u8fc7\u5bc6\u96c6\u7f16\u7801\u3001\u56fe\u68c0\u7d22\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u4ee5\u8f83\u4f4e\u7684\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u9ad8\u6027\u80fd\uff0c\u5e76\u63a8\u51fa\u4e86DermaGraph\u6570\u636e\u96c6\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u7f16\u7801\u5668\u548c\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5904\u7406\u533b\u5b66VQA\u7b49\u9700\u8981\u9ad8\u7cbe\u5ea6\u548c\u9886\u57df\u7279\u5b9a\u77e5\u8bc6\u7684\u4efb\u52a1\u65f6\u5b58\u5728\u4e0d\u8db3\uff0c\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u8fdb\u884c\u5fae\u8c03\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBIND\uff08BLIVA Integrated with Dense Encoding\uff09\u7684\u8868\u793a\u6a21\u578b\uff0c\u901a\u8fc7\u57fa\u4e8e\u67e5\u8be2-\u8bcd\u5143\uff08query-token\uff09\u7684\u5bc6\u96c6\u7f16\u7801\u6765\u4f18\u5316\u8054\u5408\u5d4c\u5165\u7a7a\u95f4\uff0c\u5e76\u5229\u7528\u56fe\u68c0\u7d22\u548c\u63d0\u793a\u5de5\u7a0b\u7684Med-GRIM\u6a21\u578b\u6765\u6574\u5408\u9886\u57df\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u533b\u5b66VQA\u3002", "result": "Med-GRIM\u5728\u533b\u5b66VQA\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e86\u4e0e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u4f46\u8ba1\u7b97\u6210\u672c\u5374\u5927\u5927\u964d\u4f4e\u3002\u5f15\u5165\u7684DermaGraph\u6570\u636e\u96c6\u652f\u6301\u591a\u6a21\u6001\u548c\u5355\u6a21\u6001\u67e5\u8be2\uff0c\u4fc3\u8fdb\u4e86\u96f6\u6837\u672c\u591a\u6a21\u6001\u533b\u5b66\u5e94\u7528\u7684\u7814\u7a76\u3002", "conclusion": "Med-GRIM\u901a\u8fc7\u7ed3\u5408\u56fe\u68c0\u7d22\u548c\u63d0\u793a\u5de5\u7a0b\uff0c\u5e76\u91c7\u7528\u6a21\u5757\u5316\u7684\u5de5\u4f5c\u6d41\u7a0b\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5728\u533b\u5b66VQA\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6027\u80fd\uff0c\u540c\u65f6\u663e\u8457\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002\u6b64\u5916\uff0cDermaGraph\u6570\u636e\u96c6\u7684\u5f15\u5165\u4e3a\u96f6\u6837\u672c\u591a\u6a21\u6001\u533b\u5b66\u5e94\u7528\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u652f\u6301\u3002"}}
{"id": "2508.06799", "categories": ["cs.ET", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06799", "abs": "https://arxiv.org/abs/2508.06799", "authors": ["Naiyi Li", "Zihui Ma", "Runlong Yu", "Lingyao Li"], "title": "LSDTs: LLM-Augmented Semantic Digital Twins for Adaptive Knowledge-Intensive Infrastructure Planning", "comment": null, "summary": "Digital Twins (DTs) offer powerful tools for managing complex infrastructure\nsystems, but their effectiveness is often limited by challenges in integrating\nunstructured knowledge. Recent advances in Large Language Models (LLMs) bring\nnew potential to address this gap, with strong abilities in extracting and\norganizing diverse textual information. We therefore propose LSDTs\n(LLM-Augmented Semantic Digital Twins), a framework that helps LLMs extract\nplanning knowledge from unstructured documents like environmental regulations\nand technical guidelines, and organize it into a formal ontology. This ontology\nforms a semantic layer that powers a digital twin-a virtual model of the\nphysical system-allowing it to simulate realistic, regulation-aware planning\nscenarios. We evaluate LSDTs through a case study of offshore wind farm\nplanning in Maryland, including its application during Hurricane Sandy. Results\ndemonstrate that LSDTs support interpretable, regulation-aware layout\noptimization, enable high-fidelity simulation, and enhance adaptability in\ninfrastructure planning. This work shows the potential of combining generative\nAI with digital twins to support complex, knowledge-driven planning tasks.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06634", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06634", "abs": "https://arxiv.org/abs/2508.06634", "authors": ["Hong Zhao", "Jin Wei-Kocsis", "Adel Heidari Akhijahani", "Karen L Butler-Purry"], "title": "Dual-Head Physics-Informed Graph Decision Transformer for Distribution System Restoration", "comment": null, "summary": "Driven by recent advances in sensing and computing, deep reinforcement\nlearning (DRL) technologies have shown great potential for addressing\ndistribution system restoration (DSR) under uncertainty. However, their\ndata-intensive nature and reliance on the Markov Decision Process (MDP)\nassumption limit their ability to handle scenarios that require long-term\ntemporal dependencies or few-shot and zero-shot decision making. Emerging\nDecision Transformers (DTs), which leverage causal transformers for sequence\nmodeling in DRL tasks, offer a promising alternative. However, their reliance\non return-to-go (RTG) cloning and limited generalization capacity restricts\ntheir effectiveness in dynamic power system environments. To address these\nchallenges, we introduce an innovative Dual-Head Physics-informed Graph\nDecision Transformer (DH-PGDT) that integrates physical modeling, structural\nreasoning, and subgoal-based guidance to enable scalable and robust DSR even in\nzero-shot or few-shot scenarios. DH-PGDT features a dual-head physics-informed\ncausal transformer architecture comprising Guidance Head, which generates\nsubgoal representations, and Action Head, which uses these subgoals to generate\nactions independently of RTG. It also incorporates an operational\nconstraint-aware graph reasoning module that encodes power system topology and\noperational constraints to generate a confidence-weighted action vector for\nrefining DT trajectories. This design effectively improves generalization and\nenables robust adaptation to unseen scenarios. While this work focuses on DSR,\nthe underlying computing model of the proposed PGDT is broadly applicable to\nsequential decision making across various power system operations and other\ncomplex engineering domains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u53cc\u5934\u7269\u7406\u4fe1\u606f\u56fe\u51b3\u7b56Transformer\uff08DH-PGDT\uff09\uff0c\u7528\u4e8e\u89e3\u51b3\u914d\u7535\u7cfb\u7edf\u6062\u590d\uff08DSR\uff09\u4e2d\u7684\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5c11\u6837\u672c/\u96f6\u6837\u672c\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u7684DRL\u6280\u672f\u5728\u5904\u7406DSR\u95ee\u9898\u65f6\uff0c\u7531\u4e8e\u5176\u6570\u636e\u5bc6\u96c6\u6027\u548c\u5bf9MDP\u7684\u4f9d\u8d56\uff0c\u96be\u4ee5\u5e94\u5bf9\u9700\u8981\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u6216\u5c11\u6837\u672c/\u96f6\u6837\u672c\u51b3\u7b56\u7684\u573a\u666f\u3002\u65b0\u5174\u7684\u51b3\u7b56Transformer\uff08DT\uff09\u867d\u7136\u63d0\u4f9b\u4e86\u4e00\u79cd\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u5176\u5bf9RTG\u514b\u9686\u7684\u4f9d\u8d56\u548c\u6709\u9650\u7684\u6cdb\u5316\u80fd\u529b\u9650\u5236\u4e86\u5176\u5728\u52a8\u6001\u7535\u529b\u7cfb\u7edf\u73af\u5883\u4e2d\u7684\u6709\u6548\u6027\u3002", "method": "DH-PGDT\u91c7\u7528\u53cc\u5934\u7269\u7406\u4fe1\u606f\u56e0\u679cTransformer\u67b6\u6784\uff0c\u5305\u62ec\u4e00\u4e2a\u751f\u6210\u5b50\u76ee\u6807\u8868\u793a\u7684\u5f15\u5bfc\u5934\u548c\u4e00\u4e2a\u5229\u7528\u5b50\u76ee\u6807\u72ec\u7acb\u4e8eRTG\u751f\u6210\u52a8\u4f5c\u7684\u52a8\u4f5c\u5934\u3002\u6b64\u5916\uff0c\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2a\u64cd\u4f5c\u7ea6\u675f\u611f\u77e5\u56fe\u63a8\u7406\u6a21\u5757\uff0c\u8be5\u6a21\u5757\u5bf9\u914d\u7535\u7cfb\u7edf\u62d3\u6251\u548c\u64cd\u4f5c\u7ea6\u675f\u8fdb\u884c\u7f16\u7801\uff0c\u751f\u6210\u7f6e\u4fe1\u5ea6\u52a0\u6743\u7684\u52a8\u4f5c\u5411\u91cf\uff0c\u4ee5\u4f18\u5316DT\u8f68\u8ff9\uff0c\u4ece\u800c\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u5e76\u5b9e\u73b0\u5bf9\u672a\u89c1\u573a\u666f\u7684\u9c81\u68d2\u9002\u5e94\u3002", "result": "DH-PGDT\u901a\u8fc7\u5176\u521b\u65b0\u7684\u67b6\u6784\u548c\u96c6\u6210\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u5730\u5904\u7406DSR\u4e2d\u7684\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u548c\u5c11\u6837\u672c/\u96f6\u6837\u672c\u51b3\u7b56\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u672a\u89c1\u573a\u666f\u7684\u9c81\u68d2\u9002\u5e94\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u53cc\u5934\u7269\u7406\u4fe1\u606f\u56fe\u51b3\u7b56Transformer\uff08DH-PGDT\uff09\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u65b9\u6cd5\u5728\u914d\u7535\u7cfb\u7edf\u6062\u590d\uff08DSR\uff09\u4e2d\u5904\u7406\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u3001\u5c11\u6837\u672c\u548c\u96f6\u6837\u672c\u51b3\u7b56\u7684\u5c40\u9650\u6027\u3002DH-PGDT\u901a\u8fc7\u96c6\u6210\u7269\u7406\u5efa\u6a21\u3001\u7ed3\u6784\u63a8\u7406\u548c\u57fa\u4e8e\u5b50\u76ee\u6807\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u548c\u9c81\u68d2\u7684DSR\uff0c\u5373\u4f7f\u5728\u96f6\u6837\u672c\u6216\u5c11\u6837\u672c\u573a\u666f\u4e0b\u4e5f\u80fd\u6709\u6548\u5e94\u5bf9\u3002"}}
{"id": "2508.06658", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.optics", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06658", "abs": "https://arxiv.org/abs/2508.06658", "authors": ["Long Ma", "Dai-Nam Le", "Lilia M. Woods"], "title": "Radiative Heat Transfer and 2D Transition Metal Dichalcogenide Materials", "comment": "28 pages, 4 figures", "summary": "Radiative heat transfer is of great interest from a fundamental point of view\nand for energy harvesting applications. This is a material dependent phenomenon\nwhere confined plasmonic excitations, hyperbolicity and other properties can be\neffective channels for enhancement, especially at the near field regime.\nMaterials with reduced dimensions may offer further benefits of enhancement\ncompared to the bulk systems. Here we study the radiative thermal power in the\nfamily of transition metal dichalcogenide monolayers in their H- and\nT-symmetries. For this purpose, the computed from first principles electronic\nand optical properties are then used in effective models to understand the\nemerging scaling laws for metals and semiconductors as well as specific\nmaterials signatures as control knobs for radiative heat transfer. Our combined\napproach of analytical modeling with properties from ab initio simulations can\nbe used for other materials families to build a materials database for\nradiative heat transfer.", "AI": {"tldr": "\u7814\u7a76\u4e86\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u5364\u5316\u7269\u5355\u5c42\u4e2d\u7684\u8f90\u5c04\u4f20\u70ed\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5206\u6790\u6a21\u578b\u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7684\u65b9\u6cd5\u6765\u5efa\u7acb\u6750\u6599\u6570\u636e\u5e93\u3002", "motivation": "\u8f90\u5c04\u4f20\u70ed\u5728\u57fa\u672c\u89c2\u70b9\u548c\u80fd\u91cf\u6536\u96c6\u5e94\u7528\u65b9\u9762\u90fd\u5907\u53d7\u5173\u6ce8\u3002\u6750\u6599\u7684\u7ef4\u5ea6\u5bf9\u589e\u5f3a\u6709\u76ca\uff0c\u5c24\u5176\u662f\u7b49\u79bb\u200b\u200b\u5b50\u4f53\u6fc0\u53d1\u7684\u9650\u5236\u3001\u53cc\u66f2\u6027\u548c\u5176\u4ed6\u6027\u8d28\u3002", "method": "\u7ed3\u5408\u5206\u6790\u6a21\u578b\u548c\u4ece\u5934\u6a21\u62df\u7684\u6027\u8d28\u3002", "result": "\u7814\u7a76\u4e86\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u5364\u5316\u7269\u5355\u5c42\u5728 H \u548c T \u5bf9\u79f0\u6027\u4e2d\u7684\u8f90\u5c04\u70ed\u529f\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u57fa\u4e8e\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7684\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u7684\u7535\u5b50\u548c\u5149\u5b66\u6027\u8d28\uff0c\u5728\u6709\u6548\u7684\u6a21\u578b\u4e2d\u7528\u4e8e\u7406\u89e3\u65b0\u5174\u7684\u91d1\u5c5e\u548c\u534a\u5bfc\u4f53\u7684\u6807\u5ea6\u5f8b\u4ee5\u53ca\u7528\u4e8e\u8f90\u5c04\u4f20\u70ed\u7684\u7279\u5b9a\u6750\u6599\u4fe1\u53f7\u3002\u8be5\u65b9\u6cd5\u53ef\u7528\u4e8e\u5176\u4ed6\u6750\u6599\u5bb6\u65cf\uff0c\u4ee5\u5efa\u7acb\u8f90\u5c04\u4f20\u70ed\u7684\u6750\u6599\u6570\u636e\u5e93\u3002"}}
{"id": "2508.06508", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06508", "abs": "https://arxiv.org/abs/2508.06508", "authors": ["Sameera Bharadwaja H.", "D. K. Mehra"], "title": "A Completely Blind Channel Estimation Technique for OFDM Using Generalized Constellation Splitting and Modified Phase-Directed Algorithm", "comment": null, "summary": "The problem of blind channel estimation for SISO-OFDM systems using\nsecond-order statistics (SOS) is addressed. A comparison of two prominent\nSOS-based techniques: subspace-decomposition and precoding-induced\ncorrelation-averaging techniques in terms of MSE performance is presented. The\ndrawback of these methods is that they suffer from a complex-scalar estimation\nambiguity which is resolved by using pilots or reference symbols. By using\npilots the whole purpose of blind techniques is contradicted. We propose an\nalgorithm to resolve this ambiguity in blind manner using generalized\nconstellation-splitting and modified phase-directed algorithm. The performance\nof the proposed scheme is evaluated via numerical simulations in MATLAB\nenvironment.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u76f2\u9053\u4f30\u8ba1\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u6b67\u4e49\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u76f2\u9053\u4f30\u8ba1\u4e2d\u7684\u590d\u6742\u6807\u91cf\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u901a\u5e38\u9700\u8981\u4f7f\u7528\u5bfc\u9891\u6216\u53c2\u8003\u7b26\u53f7\uff0c\u4f46\u8fd9\u4e0e\u76f2\u4f30\u8ba1\u7684\u76ee\u7684\u76f8\u77db\u76fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5e7f\u4e49\u661f\u5ea7\u5206\u88c2\u548c\u6539\u8fdb\u7684\u76f8\u4f4d\u5bfc\u5411\u7b97\u6cd5\u6765\u89e3\u51b3\u76f2\u9053\u4f30\u8ba1\u4e2d\u7684\u590d\u6742\u6807\u91cf\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\u3002", "result": "\u901a\u8fc7\u6570\u503c\u6a21\u62df\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u65b9\u6848\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5e7f\u4e49\u661f\u5ea7\u5206\u88c2\u548c\u6539\u8fdb\u7684\u76f8\u4f4d\u5bfc\u5411\u7b97\u6cd5\u6765\u89e3\u51b3\u76f2\u9053\u4f30\u8ba1\u7684\u590d\u6742\u6807\u91cf\u4f30\u8ba1\u6a21\u7cca\u95ee\u9898\u7684\u65b0\u7b97\u6cd5\u3002"}}
{"id": "2508.07174", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.07174", "abs": "https://arxiv.org/abs/2508.07174", "authors": ["Rongshuan Geng", "Wantao Ning"], "title": "On the fault diameter and wide diameter of the exchanged 3-ary $n$-cube", "comment": null, "summary": "Fault diameter and wide diameter are two critical parameters for evaluating\ncommunication performance in interconnection networks. They measure the fault\ntolerance and transmission efficiency of networks. The exchanged 3-ary $n$-cube\nis a recently proposed variant of the hypercube, denoted by $E3C(r, s, t)$. In\nthis work, we obtain that the $(2r + 1)$-fault diameter and $(2r + 2)$-wide\ndiameter of $E3C(r, s, t)$ are bounded between $n + 3$ and $n + 5$ for $1 \\leq\nr \\leq s \\leq t$.", "AI": {"tldr": "\u5206\u6790\u4e863-ary n-cube\u7684\u6545\u969c\u76f4\u5f84\u548c\u5bbd\u76f4\u5f84\u3002", "motivation": "\u9700\u8981\u8bc4\u4f30\u4e92\u8fde\u7f51\u7edc\u7684\u901a\u4fe1\u6027\u80fd\uff0c\u7279\u522b\u662f\u6545\u969c\u5bb9\u9519\u80fd\u529b\u548c\u4f20\u8f93\u6548\u7387\uff0c\u5f15\u5165\u4e863-ary n-cube\uff08E3C(r, s, t)\uff09\u4f5c\u4e3a\u7814\u7a76\u5bf9\u8c61\u3002", "method": "\u63a8\u5bfc\u4e863-ary n-cube\u7684\u6545\u969c\u76f4\u5f84\u548c\u5bbd\u76f4\u5f84\u3002", "result": "\u5f97\u5230\u4e86E3C(r, s, t)\u57281 <= r <= s <= t\u6761\u4ef6\u4e0b\u7684\u6545\u969c\u76f4\u5f84\u7684\u754c\u9650\u4e3an+3\u5230n+5\uff0c\u5bbd\u76f4\u5f84\u7684\u754c\u9650\u4e3an+3\u5230n+5\u3002", "conclusion": "3-ary n-cube\u7684(2r+1)\u6545\u969c\u76f4\u5f84\u548c(2r+2)\u5bbd\u76f4\u5f84\u5728r<=s<=t\u65f6\u754c\u4e8en+3\u548cn+5\u4e4b\u95f4\u3002"}}
{"id": "2508.06559", "categories": ["cs.AI", "cs.GT", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06559", "abs": "https://arxiv.org/abs/2508.06559", "authors": ["Sina Baghal"], "title": "Solving Pasur Using GPU-Accelerated Counterfactual Regret Minimization", "comment": null, "summary": "Pasur is a fishing card game played over six rounds and is played similarly\nto games such as Cassino and Scopa, and Bastra. This paper introduces a\nCUDA-accelerated computational framework for simulating Pasur, emphasizing\nefficient memory management. We use our framework to compute near-Nash\nequilibria via Counterfactual Regret Minimization (CFR), a well-known algorithm\nfor solving large imperfect-information games.\n  Solving Pasur presents unique challenges due to its intricate rules and the\nlarge size of its game tree. We handle rule complexity using PyTorch CUDA\ntensors and to address the memory-intensive nature of the game, we decompose\nthe game tree into two key components: (1) actual game states, and (2)\ninherited scores from previous rounds. We construct the Full Game Tree by\npairing card states with accumulated scores in the Unfolding Process. This\ndesign reduces memory overhead by storing only essential strategy values and\nnode connections. To further manage computational complexity, we apply a\nround-by-round backward training strategy, starting from the final round and\nrecursively propagating average utilities to earlier stages. Our approach\nconstructs the complete game tree, which on average consists of over $10^9$\nnodes. We provide detailed implementation snippets.\n  After computing a near-Nash equilibrium strategy, we train a tree-based model\nto predict these strategies for use during gameplay. We then estimate the fair\nvalue of each deck through large-scale self-play between equilibrium strategies\nby simulating, for instance, 10,000 games per matchup, executed in parallel\nusing GPU acceleration.\n  Similar frameworks can be extended to other reinforcement learning algorithms\nwhere the action tree naturally decomposes into multiple rounds such as\nturn-based strategy games or sequential trading decisions in financial markets.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd CUDA \u52a0\u901f\u7684\u8ba1\u7b97\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6251\u514b\u724c\u6e38\u620f Pasur\uff0c\u5e76\u901a\u8fc7\u53cd\u4e8b\u5b9e\u9057\u61be\u6700\u5c0f\u5316\uff08CFR\uff09\u8ba1\u7b97\u8fd1\u7eb3\u4ec0\u5747\u8861\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u548c\u5904\u7406\u590d\u6742\u89c4\u5219\uff0c\u80fd\u591f\u5904\u7406\u5927\u578b\u6e38\u620f\u6811\uff0c\u5e76\u5229\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u548c GPU \u52a0\u901f\u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u6211\u5bf9\u5f08\uff0c\u6700\u7ec8\u4f30\u8ba1\u724c\u7ec4\u7684\u516c\u5e73\u4ef7\u503c\u3002", "motivation": "\u89e3\u51b3\u6251\u514b\u724c\u6e38\u620f\uff08Pasur\uff09\u7531\u4e8e\u5176\u590d\u6742\u7684\u89c4\u5219\u548c\u5de8\u5927\u7684\u6e38\u620f\u6811\u800c\u5e26\u6765\u7684\u8ba1\u7b97\u6311\u6218\uff0c\u5e76\u5229\u7528 CUDA \u52a0\u901f\u6765\u6a21\u62df\u6e38\u620f\u548c\u8ba1\u7b97\u8fd1\u7eb3\u4ec0\u5747\u8861\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528 PyTorch CUDA \u5f20\u91cf\u5904\u7406\u590d\u6742\u89c4\u5219\u548c\u5185\u5b58\u5bc6\u96c6\u578b\u6e38\u620f\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6e38\u620f\u6811\u5206\u89e3\u4e3a\u5b9e\u9645\u6e38\u620f\u72b6\u6001\u548c\u7ee7\u627f\u5206\u6570\u6765\u4f18\u5316\u5185\u5b58\uff0c\u5e76\u91c7\u7528\u4ece\u540e\u5f80\u524d\u7684\u8bad\u7ec3\u7b56\u7565\u6765\u5904\u7406\u8ba1\u7b97\u590d\u6742\u6027\u3002\u6700\u7ec8\uff0c\u901a\u8fc7\u8bad\u7ec3\u57fa\u4e8e\u6811\u7684\u6a21\u578b\u6765\u9884\u6d4b\u7b56\u7565\uff0c\u5e76\u5229\u7528 GPU \u52a0\u901f\u8fdb\u884c\u5927\u89c4\u6a21\u81ea\u6211\u5bf9\u5f08\u6765\u4f30\u8ba1\u516c\u5e73\u4ef7\u503c\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b\u8d85\u8fc7 10 \u4ebf\u4e2a\u8282\u70b9\u7684\u6e38\u620f\u6811\uff0c\u5e76\u8ba1\u7b97\u51fa\u4e86\u8fd1\u7eb3\u4ec0\u5747\u8861\u7b56\u7565\u3002\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u6211\u5bf9\u5f08\uff08\u4f8b\u5982\uff0c\u6bcf\u4e2a\u5bf9\u5c40\u6a21\u62df 10,000 \u573a\u6bd4\u8d5b\uff09\uff0c\u4f30\u8ba1\u4e86\u724c\u7ec4\u7684\u516c\u5e73\u4ef7\u503c\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5904\u7406\u5177\u6709\u590d\u6742\u89c4\u5219\u548c\u5927\u578b\u6e38\u620f\u6811\u7684\u6251\u514b\u724c\u6e38\u620f\uff0c\u901a\u8fc7\u53cd\u4e8b\u5b9e\u9057\u61be\u6700\u5c0f\u5316\uff08CFR\uff09\u8ba1\u7b97\u8fd1\u7eb3\u4ec0\u5747\u8861\uff0c\u5e76\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u7b56\u7565\u9884\u6d4b\uff0c\u6700\u7ec8\u901a\u8fc7\u5927\u89c4\u6a21\u81ea\u6211\u5bf9\u5f08\u4f30\u8ba1\u724c\u7ec4\u7684\u516c\u5e73\u4ef7\u503c\u3002"}}
{"id": "2508.06693", "categories": ["cs.DS", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06693", "abs": "https://arxiv.org/abs/2508.06693", "authors": ["Matthew Fahrbach", "Mehrdad Ghadiri"], "title": "A Tight Lower Bound for the Approximation Guarantee of Higher-Order Singular Value Decomposition", "comment": "15 pages", "summary": "We prove that the classic approximation guarantee for the higher-order\nsingular value decomposition (HOSVD) is tight by constructing a tensor for\nwhich HOSVD achieves an approximation ratio of $N/(1+\\varepsilon)$, for any\n$\\varepsilon > 0$. This matches the upper bound of De Lathauwer et al. (2000a)\nand shows that the approximation ratio of HOSVD cannot be improved. Using a\nmore advanced construction, we also prove that the approximation guarantees for\nthe ST-HOSVD algorithm of Vannieuwenhoven et al. (2012) and higher-order\northogonal iteration (HOOI) of De Lathauwer et al. (2000b) are tight by showing\nthat they can achieve their worst-case approximation ratio of $N / (1 +\n\\varepsilon)$, for any $\\varepsilon > 0$.", "AI": {"tldr": "HOSVD\u3001ST-HOSVD \u548c HOOI \u7684\u8fd1\u4f3c\u4fdd\u8bc1\u88ab\u8bc1\u660e\u662f\u7d27\u786e\u7684\uff0c\u5e76\u4e14\u5176\u8fd1\u4f3c\u6bd4\u7387\u65e0\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002", "motivation": "\u8bc1\u660e\u7ecf\u5178\u9ad8\u9636\u5947\u5f02\u503c\u5206\u89e3\uff08HOSVD\uff09\u7684\u8fd1\u4f3c\u4fdd\u8bc1\u662f\u7d27\u786e\u7684\uff0c\u5e76\u6539\u8fdb\u4e86 ST-HOSVD \u548c HOOI \u7b97\u6cd5\u7684\u8fd1\u4f3c\u4fdd\u8bc1\u3002", "method": "\u901a\u8fc7\u6784\u9020\u7279\u5b9a\u5f20\u91cf\uff0c\u8bc1\u660e\u4e86 HOSVD \u7684\u8fd1\u4f3c\u4e0a\u754c\u662f\u7d27\u786e\u7684\uff0c\u5176\u6bd4\u7387\u63a5\u8fd1 N/(1+\u03b5)\u3002\u540c\u6837\uff0c\u4e5f\u8bc1\u660e\u4e86 ST-HOSVD \u548c HOOI \u7b97\u6cd5\u7684\u6700\u574f\u60c5\u51b5\u8fd1\u4f3c\u6bd4\u4e5f\u662f N/(1+\u03b5)\uff0c\u5176\u4e2d \u03b5 \u53ef\u4ee5\u4efb\u610f\u5c0f\u3002", "result": "HOSVD \u7684\u8fd1\u4f3c\u6bd4\u7387\u5339\u914d\u4e86 De Lathauwer \u7b49\u4eba\uff082000a\uff09\u7684\u4e0a\u754c\u3002ST-HOSVD \u548c HOOI \u7b97\u6cd5\u4e5f\u80fd\u8fbe\u5230\u5176\u6700\u574f\u60c5\u51b5\u8fd1\u4f3c\u6bd4\u7387 N/(1+\u03b5)\uff0c\u5176\u4e2d \u03b5 > 0\u3002", "conclusion": "HOSVD\u3001ST-HOSVD \u548c HOOI \u7684\u8fd1\u4f3c\u4fdd\u8bc1\u662f\u7d27\u786e\u7684\uff0c\u5176\u8fd1\u4f3c\u6bd4\u65e0\u6cd5\u8fdb\u4e00\u6b65\u63d0\u9ad8\u3002"}}
{"id": "2508.06539", "categories": ["cs.LG", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.06539", "abs": "https://arxiv.org/abs/2508.06539", "authors": ["Atahan Karagoz"], "title": "Self-Organizing Survival Manifolds: A Theory for Unsupervised Discovery of Prognostic Structures in Biological Systems", "comment": null, "summary": "Survival is traditionally modeled as a supervised learning task, reliant on\ncurated outcome labels and fixed covariates. This work rejects that premise. It\nproposes that survival is not an externally annotated target but a geometric\nconsequence: an emergent property of the curvature and flow inherent in\nbiological state space. We develop a theory of Self-Organizing Survival\nManifolds (SOSM), in which survival-relevant dynamics arise from low-curvature\ngeodesic flows on latent manifolds shaped by internal biological constraints. A\nsurvival energy functional based on geodesic curvature minimization is\nintroduced and shown to induce structures where prognosis aligns with geometric\nflow stability. We derive discrete and continuous formulations of the objective\nand prove theoretical results demonstrating the emergence and convergence of\nsurvival-aligned trajectories under biologically plausible conditions. The\nframework draws connections to thermodynamic efficiency, entropy flow, Ricci\ncurvature, and optimal transport, grounding survival modeling in physical law.\nHealth, disease, aging, and death are reframed as geometric phase transitions\nin the manifold's structure. This theory offers a universal, label-free\nfoundation for modeling survival as a property of form, not annotation-bridging\nmachine learning, biophysics, and the geometry of life itself.", "AI": {"tldr": "\u751f\u5b58\u4e0d\u518d\u662f\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\uff0c\u800c\u662f\u751f\u7269\u72b6\u6001\u7a7a\u95f4\u51e0\u4f55\u7684\u6d8c\u73b0\u5c5e\u6027\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u81ea\u7ec4\u7ec7\u751f\u5b58\u6d41\u5f62\uff08SOSM\uff09\u7406\u8bba\uff0c\u5c06\u751f\u5b58\u5efa\u6a21\u4e0e\u7269\u7406\u5b9a\u5f8b\uff08\u5982\u70ed\u529b\u5b66\u6548\u7387\u3001\u71b5\u6d41\u3001Ricci\u66f2\u7387\u548c\u6700\u4f18\u8f93\u8fd0\uff09\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5c06\u5065\u5eb7\u3001\u75be\u75c5\u3001\u8870\u8001\u548c\u6b7b\u4ea1\u89c6\u4e3a\u6d41\u5f62\u7ed3\u6784\u7684\u51e0\u4f55\u76f8\u53d8\u3002", "motivation": "\u8be5\u7814\u7a76\u6311\u6218\u4e86\u4f20\u7edf\u7684\u5c06\u751f\u5b58\u89c6\u4e3a\u4f9d\u8d56\u4e8e\u7ed3\u679c\u6807\u7b7e\u548c\u56fa\u5b9a\u534f\u53d8\u91cf\u7684\u76d1\u7763\u5b66\u4e60\u4efb\u52a1\u7684\u89c2\u70b9\uff0c\u63d0\u51fa\u751f\u5b58\u662f\u751f\u7269\u72b6\u6001\u7a7a\u95f4\u5185\u5728\u66f2\u7387\u548c\u6d41\u52a8\u7684\u51e0\u4f55\u540e\u679c\uff0c\u662f\u4e00\u79cd\u6d8c\u73b0\u5c5e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u7ec4\u7ec7\u751f\u5b58\u6d41\u5f62\uff08SOSM\uff09\u7684\u7406\u8bba\uff0c\u5176\u4e2d\u751f\u5b58\u76f8\u5173\u7684\u52a8\u529b\u5b66\u6e90\u4e8e\u6f5c\u5728\u6d41\u5f62\u4e0a\u7531\u5185\u90e8\u751f\u7269\u7ea6\u675f\u5851\u9020\u7684\u4f4e\u66f2\u7387\u6d4b\u5730\u7ebf\u6d41\u3002\u5f15\u5165\u4e86\u57fa\u4e8e\u6d4b\u5730\u7ebf\u66f2\u7387\u6700\u5c0f\u5316\u7684\u751f\u5b58\u80fd\u91cf\u6cdb\u51fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u53ef\u4ee5\u8bf1\u5bfc\u9884\u540e\u4e0e\u51e0\u4f55\u6d41\u7a33\u5b9a\u6027\u76f8\u4e00\u81f4\u7684\u7ed3\u6784\u3002\u63a8\u5bfc\u4e86\u76ee\u6807\u51fd\u6570\u7684\u79bb\u6563\u548c\u8fde\u7eed\u5f62\u5f0f\uff0c\u5e76\u8bc1\u660e\u4e86\u5728\u751f\u7269\u5b66\u5408\u7406\u6761\u4ef6\u4e0b\u751f\u5b58\u8f68\u8ff9\u7684\u51fa\u73b0\u548c\u6536\u655b\u7684\u7406\u8bba\u7ed3\u679c\u3002", "result": "\u751f\u5b58\u80fd\u91cf\u6cdb\u51fd\u57fa\u4e8e\u6d4b\u5730\u7ebf\u66f2\u7387\u6700\u5c0f\u5316\uff0c\u8bf1\u5bfc\u51fa\u9884\u540e\u4e0e\u51e0\u4f55\u6d41\u7a33\u5b9a\u6027\u76f8\u4e00\u81f4\u7684\u7ed3\u6784\u3002\u7406\u8bba\u7ed3\u679c\u8bc1\u660e\u4e86\u5728\u751f\u7269\u5b66\u5408\u7406\u6761\u4ef6\u4e0b\uff0c\u751f\u5b58\u8f68\u8ff9\u7684\u51fa\u73b0\u548c\u6536\u655b\u3002", "conclusion": "\u8be5\u7406\u8bba\u5c06\u5065\u5eb7\u3001\u75be\u75c5\u3001\u8870\u8001\u548c\u6b7b\u4ea1\u91cd\u65b0\u5b9a\u4e49\u4e3a\u751f\u7269\u72b6\u6001\u7a7a\u95f4\u6d41\u5f62\u7ed3\u6784\u4e2d\u7684\u51e0\u4f55\u76f8\u53d8\uff0c\u4e3a\u751f\u5b58\u5efa\u6a21\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4e0d\u4f9d\u8d56\u6807\u7b7e\u7684\u901a\u7528\u57fa\u7840\uff0c\u5c06\u673a\u5668\u5b66\u4e60\u3001\u751f\u7269\u7269\u7406\u5b66\u548c\u751f\u547d\u51e0\u4f55\u5b66\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2508.07721", "categories": ["cs.CV", "cs.NA", "math.NA", "65D18, 68U10, 94A08"], "pdf": "https://arxiv.org/pdf/2508.07721", "abs": "https://arxiv.org/abs/2508.07721", "authors": ["Daoping Zhang", "Xue-Cheng Tai", "Lok Ming Lui"], "title": "A Registration-Based Star-Shape Segmentation Model and Fast Algorithms", "comment": null, "summary": "Image segmentation plays a crucial role in extracting objects of interest and\nidentifying their boundaries within an image. However, accurate segmentation\nbecomes challenging when dealing with occlusions, obscurities, or noise in\ncorrupted images. To tackle this challenge, prior information is often\nutilized, with recent attention on star-shape priors. In this paper, we propose\na star-shape segmentation model based on the registration framework. By\ncombining the level set representation with the registration framework and\nimposing constraints on the deformed level set function, our model enables both\nfull and partial star-shape segmentation, accommodating single or multiple\ncenters. Additionally, our approach allows for the enforcement of identified\nboundaries to pass through specified landmark locations. We tackle the proposed\nmodels using the alternating direction method of multipliers. Through numerical\nexperiments conducted on synthetic and real images, we demonstrate the efficacy\nof our approach in achieving accurate star-shape segmentation.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6ce8\u518c\u6846\u67b6\u7684\u661f\u5f62\u5206\u5272\u6a21\u578b\uff0c\u53ef\u4ee5\u5904\u7406\u906e\u6321\u3001\u6a21\u7cca\u548c\u566a\u58f0\uff0c\u5e76\u80fd\u5904\u7406\u5355\u4e2d\u5fc3\u6216\u591a\u4e2d\u5fc3\u4ee5\u53ca\u6307\u5b9a\u6807\u5fd7\u4f4d\u7f6e\u3002", "motivation": "\u56fe\u50cf\u5206\u5272\u5728\u63d0\u53d6\u611f\u5174\u8da3\u5bf9\u8c61\u548c\u8bc6\u522b\u56fe\u50cf\u8fb9\u754c\u65b9\u9762\u53d1\u6325\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u5f53\u5904\u7406\u635f\u574f\u56fe\u50cf\u4e2d\u7684\u906e\u6321\u3001\u6a21\u7cca\u6216\u566a\u58f0\u65f6\uff0c\u51c6\u786e\u5206\u5272\u4f1a\u53d8\u5f97\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u5229\u7528\u5148\u9a8c\u4fe1\u606f\uff0c\u7279\u522b\u662f\u661f\u5f62\u5148\u9a8c\uff0c\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u914d\u51c6\u6846\u67b6\u7684\u661f\u5f62\u5206\u5272\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u6c34\u5e73\u96c6\u8868\u793a\u548c\u914d\u51c6\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u5bf9\u5f62\u53d8\u6c34\u5e73\u96c6\u51fd\u6570\u65bd\u52a0\u7ea6\u675f\u6765\u5b9e\u73b0\u3002", "result": "\u901a\u8fc7\u5728\u5408\u6210\u548c\u771f\u5b9e\u56fe\u50cf\u4e0a\u8fdb\u884c\u6570\u503c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0\u7cbe\u786e\u661f\u5f62\u5206\u5272\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u5b9e\u73b0\u5b8c\u6574\u548c\u90e8\u5206\u661f\u5f62\u5206\u5272\uff0c\u9002\u5e94\u5355\u4e2d\u5fc3\u6216\u591a\u4e2d\u5fc3\uff0c\u5e76\u5141\u8bb8\u5f3a\u5236\u5df2\u8bc6\u522b\u7684\u8fb9\u754c\u901a\u8fc7\u6307\u5b9a\u7684\u6807\u5fd7\u4f4d\u7f6e\u3002"}}
{"id": "2508.07343", "categories": ["cs.MA", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.07343", "abs": "https://arxiv.org/abs/2508.07343", "authors": ["Xuwen Zhang", "Xiao Xue", "Xia Xie", "Qun Ma", "Xiangning Yu", "Deyu Zhou", "Yifan Wang", "Ming Zhang"], "title": "A Survey on Agentic Service Ecosystems: Measurement, Analysis, and Optimization", "comment": null, "summary": "The Agentic Service Ecosystem consists of heterogeneous autonomous agents\n(e.g., intelligent machines, humans, and human-machine hybrid systems) that\ninteract through resource exchange and service co-creation. These agents, with\ndistinct behaviors and motivations, exhibit autonomous perception, reasoning,\nand action capabilities, which increase system complexity and make traditional\nlinear analysis methods inadequate. Swarm intelligence, characterized by\ndecentralization, self-organization, emergence, and dynamic adaptability,\noffers a novel theoretical lens and methodology for understanding and\noptimizing such ecosystems. However, current research, owing to fragmented\nperspectives and cross-ecosystem differences, fails to comprehensively capture\nthe complexity of swarm-intelligence emergence in agentic contexts. The lack of\na unified methodology further limits the depth and systematic treatment of the\nresearch. This paper proposes a framework for analyzing the emergence of swarm\nintelligence in Agentic Service Ecosystems, with three steps: measurement,\nanalysis, and optimization, to reveal the cyclical mechanisms and quantitative\ncriteria that foster emergence. By reviewing existing technologies, the paper\nanalyzes their strengths and limitations, identifies unresolved challenges, and\nshows how this framework provides both theoretical support and actionable\nmethods for real-world applications.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u5206\u6790\u4ee3\u7406\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u4e2d\u7fa4\u4f53\u667a\u80fd\u6d8c\u73b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6d4b\u91cf\u3001\u5206\u6790\u548c\u4f18\u5316\u4e09\u4e2a\u6b65\u9aa4\uff0c\u5e76\u56de\u987e\u73b0\u6709\u6280\u672f\uff0c\u4ee5\u5e94\u5bf9\u590d\u6742\u6027\u548c\u63d0\u4f9b\u5b9e\u9645\u5e94\u7528\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684\u7ebf\u6027\u5206\u6790\u65b9\u6cd5\u4e0d\u8db3\u4ee5\u5e94\u5bf9\u7531\u5177\u6709\u4e0d\u540c\u884c\u4e3a\u548c\u52a8\u673a\u7684\u5f02\u6784\u81ea\u4e3b\u4ee3\u7406\u7ec4\u6210\u7684\u4ee3\u7406\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u7684\u590d\u6742\u6027\u3002\u73b0\u6709\u7814\u7a76\u7531\u4e8e\u89c2\u70b9\u5206\u6563\u548c\u8de8\u751f\u6001\u7cfb\u7edf\u7684\u5dee\u5f02\uff0c\u672a\u80fd\u5168\u9762\u6355\u6349\u4ee3\u7406\u73af\u5883\u4e2d\u7fa4\u4f53\u667a\u80fd\u6d8c\u73b0\u7684\u590d\u6742\u6027\uff0c\u7f3a\u4e4f\u7edf\u4e00\u7684\u65b9\u6cd5\u8bba\u4e5f\u9650\u5236\u4e86\u7814\u7a76\u7684\u6df1\u5ea6\u548c\u7cfb\u7edf\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u6d4b\u91cf\u3001\u5206\u6790\u548c\u4f18\u5316\u4e09\u4e2a\u6b65\u9aa4\u7684\u6846\u67b6\uff0c\u4ee5\u5206\u6790\u4ee3\u7406\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u4e2d\u7fa4\u4f53\u667a\u80fd\u7684\u6d8c\u73b0\u3002\u8be5\u6846\u67b6\u65e8\u5728\u63ed\u793a\u4fc3\u8fdb\u6d8c\u73b0\u7684\u5faa\u73af\u673a\u5236\u548c\u5b9a\u91cf\u6807\u51c6\u3002", "result": "\u901a\u8fc7\u56de\u987e\u73b0\u6709\u6280\u672f\uff0c\u5206\u6790\u5176\u4f18\u7f3a\u70b9\uff0c\u786e\u5b9a\u5c1a\u672a\u89e3\u51b3\u7684\u6311\u6218\uff0c\u5e76\u5c55\u793a\u8be5\u6846\u67b6\u5982\u4f55\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u548c\u53ef\u884c\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5206\u6790\u4ee3\u7406\u670d\u52a1\u751f\u6001\u7cfb\u7edf\u4e2d\u7fa4\u4f53\u667a\u80fd\u6d8c\u73b0\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u62ec\u6d4b\u91cf\u3001\u5206\u6790\u548c\u4f18\u5316\u4e09\u4e2a\u6b65\u9aa4\uff0c\u4ee5\u63ed\u793a\u4fc3\u8fdb\u6d8c\u73b0\u7684\u5faa\u73af\u673a\u5236\u548c\u5b9a\u91cf\u6807\u51c6\u3002"}}
{"id": "2508.06664", "categories": ["cond-mat.mtrl-sci", "eess.IV", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.06664", "abs": "https://arxiv.org/abs/2508.06664", "authors": ["Sima Zeinali Danalou", "Hooman Chamani", "Arash Rabbani", "Patrick C. Lee", "Jason Hattrick Simpers", "Jay R Werber"], "title": "Digital generation of the 3-D pore architecture of isotropic membranes using 2-D cross-sectional scanning electron microscopy images", "comment": null, "summary": "A major limitation of two-dimensional scanning electron microscopy (SEM) in\nimaging porous membranes is its inability to resolve three-dimensional pore\narchitecture and interconnectivity, which are critical factors governing\nmembrane performance. Although conventional tomographic 3-D reconstruction\ntechniques can address this limitation, they are often expensive, technically\nchallenging, and not widely accessible. We previously introduced a\nproof-of-concept method for reconstructing a membrane's 3-D pore network from a\nsingle 2-D SEM image, yielding statistically equivalent results to those\nobtained from 3-D tomography. However, this initial approach struggled to\nreplicate the diverse pore geometries commonly observed in real membranes. In\nthis study, we advance the methodology by developing an enhanced reconstruction\nalgorithm that not only maintains essential statistical properties (e.g., pore\nsize distribution), but also accurately reproduces intricate pore morphologies.\nApplying this technique to a commercial microfiltration membrane, we generated\na high-fidelity 3-D reconstruction and derived key membrane properties.\nValidation with X-ray tomography data revealed excellent agreement in\nstructural metrics, with our SEM-based approach achieving superior resolution\nin resolving fine pore features. The tool can be readily applied to isotropic\nporous membrane structures of any pore size, as long as those pores can be\nvisualized by SEM. Further work is needed for 3-D structure generation of\nanisotropic membranes.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5355\u5f20\u4e8c\u7ef4\u626b\u63cf\u7535\u955c\u56fe\u50cf\u91cd\u5efa\u4e09\u7ef4\u591a\u5b54\u819c\u7ed3\u6784\u7684\u589e\u5f3a\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u80fd\u9ad8\u4fdd\u771f\u5730\u590d\u5236\u5b54\u9699\u5f62\u8c8c\u5e76\u4e0eX\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u6570\u636e\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u5206\u8fa8\u7387\u4f18\u4e8e\u540e\u8005\uff0c\u53ef\u7528\u4e8e\u5404\u5411\u540c\u6027\u591a\u5b54\u819c\uff0c\u4f46\u5bf9\u975e\u5404\u5411\u540c\u6027\u819c\u4ecd\u9700\u6539\u8fdb\u3002", "motivation": "\u4e8c\u7ef4\u626b\u63cf\u7535\u955c\u6210\u50cf\u5728\u6210\u50cf\u591a\u5b54\u819c\u65f6\u5b58\u5728\u65e0\u6cd5\u89e3\u6790\u4e09\u7ef4\u5b54\u9699\u7ed3\u6784\u548c\u8fde\u901a\u6027\u7684\u5c40\u9650\u6027\uff0c\u800c\u4f20\u7edf\u7684\u4e09\u7ef4\u65ad\u5c42\u626b\u63cf\u6280\u672f\u6210\u672c\u9ad8\u3001\u6280\u672f\u8981\u6c42\u9ad8\u4e14\u4e0d\u6613\u83b7\u53d6\u3002\u672c\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u4e00\u79cd\u4ece\u5355\u5f20\u4e8c\u7ef4\u626b\u63cf\u7535\u955c\u56fe\u50cf\u91cd\u5efa\u4e09\u7ef4\u5b54\u9699\u7f51\u7edc\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u589e\u5f3a\u7684\u91cd\u5efa\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u5355\u5f20\u4e8c\u7ef4\u626b\u63cf\u7535\u955c\u56fe\u50cf\u4e2d\u91cd\u5efa\u819c\u7684\u4e09\u7ef4\u5b54\u9699\u7f51\u7edc\uff0c\u8be5\u7b97\u6cd5\u5728\u4fdd\u6301\u7edf\u8ba1\u7279\u6027\u7684\u540c\u65f6\uff0c\u80fd\u591f\u7cbe\u786e\u590d\u5236\u590d\u6742\u7684\u5b54\u9699\u5f62\u8c8c\u3002", "result": "\u901a\u8fc7\u5c06\u8be5\u6280\u672f\u5e94\u7528\u4e8e\u5546\u4e1a\u5fae\u6ee4\u819c\uff0c\u751f\u6210\u4e86\u9ad8\u4fdd\u771f\u7684\u4e09\u7ef4\u91cd\u5efa\uff0c\u5e76\u5bfc\u51fa\u4e86\u5173\u952e\u7684\u819c\u6027\u8d28\u3002\u4e0eX\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u6570\u636e\u7684\u9a8c\u8bc1\u663e\u793a\uff0c\u5728\u7ed3\u6784\u6307\u6807\u4e0a\u5177\u6709\u4f18\u5f02\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u5728\u5206\u8fa8\u7cbe\u7ec6\u5b54\u9699\u7279\u5f81\u65b9\u9762\u5177\u6709\u66f4\u9ad8\u7684\u5206\u8fa8\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u4e8c\u7ef4\u626b\u63cf\u7535\u955c\u56fe\u50cf\u7684\u589e\u5f3a\u91cd\u5efa\u7b97\u6cd5\u80fd\u591f\u9ad8\u4fdd\u771f\u5730\u91cd\u5efa\u5fae\u6ee4\u819c\u7684\u4e09\u7ef4\u5b54\u9699\u7ed3\u6784\uff0c\u5e76\u4e14\u5728\u7ed3\u6784\u6307\u6807\u4e0a\u4e0eX\u5c04\u7ebf\u65ad\u5c42\u626b\u63cf\u6570\u636e\u5177\u6709\u826f\u597d\u7684\u4e00\u81f4\u6027\uff0c\u5728\u5206\u8fa8\u7cbe\u7ec6\u5b54\u9699\u7279\u5f81\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u8fa8\u7387\u3002\u8be5\u65b9\u6cd5\u53ef\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4efb\u4f55\u5c3a\u5bf8\u7684\u5404\u5411\u540c\u6027\u591a\u5b54\u819c\u7ed3\u6784\uff0c\u4f46\u5bf9\u4e8e\u975e\u5404\u5411\u540c\u6027\u819c\u7684\u7ed3\u6784\u751f\u6210\u4ecd\u9700\u8fdb\u4e00\u6b65\u7814\u7a76\u3002"}}
{"id": "2508.06948", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.06948", "abs": "https://arxiv.org/abs/2508.06948", "authors": ["Jinyuan Chen", "Jiuchen Shi", "Quan Chen", "Minyi Guo"], "title": "Kairos: Low-latency Multi-Agent Serving with Shared LLMs and Excessive Loads in the Public Cloud", "comment": null, "summary": "Multi-agent applications utilize the advanced capabilities of large language\nmodels (LLMs) for intricate task completion through agent collaboration in a\nworkflow. Under this situation, requests from different agents usually access\nthe same shared LLM to perform different kinds of tasks, forcing the shared LLM\nto suffer excessive loads. However, existing works have low serving performance\nfor these multi-agent applications, mainly due to the ignorance of inter-agent\nlatency and resource differences for request scheduling. We therefore propose\nKairos, a multi-agent orchestration system that optimizes end-to-end latency\nfor multi-agent applications. Kairos consists of a workflow orchestrator, a\nworkflow-aware priority scheduler, and a memory-aware dispatcher. The\norchestrator collects agent-specific information for online workflow analysis.\nThe scheduler decides the serving priority of the requests based on their\nlatency characteristics to reduce the overall queuing. The dispatcher\ndispatches the requests to different LLM instances based on their memory\ndemands to avoid GPU overloading. Experimental results show that Kairos reduces\nend-to-end latency by 17.8% to 28.4% compared to state-of-the-art works.", "AI": {"tldr": "Kairos\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7f16\u6392\u7cfb\u7edf\uff0c\u901a\u8fc7\u5de5\u4f5c\u6d41\u611f\u77e5\u8c03\u5ea6\u548c\u5185\u5b58\u611f\u77e5\u5206\u6d3e\u6765\u4f18\u5316LLM\u670d\u52a1\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u591a\u667a\u80fd\u4f53\u5e94\u7528\u7684\u7aef\u5230\u7aef\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709\u7684LLM\u670d\u52a1\u65b9\u6cd5\u672a\u80fd\u8003\u8651\u667a\u80fd\u4f53\u95f4\u5ef6\u8fdf\u548c\u8d44\u6e90\u5dee\u5f02\uff0c\u5bfc\u81f4\u670d\u52a1\u6027\u80fd\u4f4e\u4e0b\uff0c\u5e76\u4f7f\u5171\u4eabLLM\u9762\u4e34\u8fc7\u8f7d\u3002", "method": "Kairos\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u7f16\u6392\u7cfb\u7edf\uff0c\u5305\u542b\u5de5\u4f5c\u6d41\u7f16\u6392\u5668\u3001\u5de5\u4f5c\u6d41\u611f\u77e5\u4f18\u5148\u7ea7\u8c03\u5ea6\u5668\u548c\u5185\u5b58\u611f\u77e5\u5206\u6d3e\u5668\u3002\u5de5\u4f5c\u6d41\u7f16\u6392\u5668\u6536\u96c6\u7279\u5b9a\u667a\u80fd\u4f53\u4fe1\u606f\u8fdb\u884c\u5728\u7ebf\u5de5\u4f5c\u6d41\u5206\u6790\uff1b\u8c03\u5ea6\u5668\u6839\u636e\u8bf7\u6c42\u7684\u5ef6\u8fdf\u7279\u6027\u51b3\u5b9a\u670d\u52a1\u4f18\u5148\u7ea7\u4ee5\u51cf\u5c11\u6574\u4f53\u6392\u961f\uff1b\u5206\u6d3e\u5668\u6839\u636e\u5185\u5b58\u9700\u6c42\u5c06\u8bf7\u6c42\u5206\u6d3e\u5230\u4e0d\u540c\u7684LLM\u5b9e\u4f8b\uff0c\u4ee5\u907f\u514dGPU\u8fc7\u8f7d\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cKairos\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u7aef\u5230\u7aef\u5ef6\u8fdf\u964d\u4f4e\u4e8617.8%\u81f328.4%\u3002", "conclusion": "Kairos\u901a\u8fc7\u4f18\u5316\u7aef\u5230\u7aef\u5ef6\u8fdf\uff0c\u5728\u591a\u667a\u80fd\u4f53\u5e94\u7528\u4e2d\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5c06\u5ef6\u8fdf\u964d\u4f4e\u4e8617.8%\u81f328.4%\u3002"}}
{"id": "2508.06520", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06520", "abs": "https://arxiv.org/abs/2508.06520", "authors": ["Liwei Chen", "Tong Qin", "Zhenhua Huangfu", "Li Li", "Wei Wei"], "title": "Optimization of Flip-Landing Trajectories for Starship based on a Deep Learned Simulator", "comment": null, "summary": "We propose a differentiable optimization framework for flip-and-landing\ntrajectory design of reusable spacecraft, exemplified by the Starship vehicle.\nA deep neural network surrogate, trained on high-fidelity CFD data, predicts\naerodynamic forces and moments, and is tightly coupled with a differentiable\nrigid-body dynamics solver. This enables end-to-end gradient-based trajectory\noptimization without linearization or convex relaxation. The framework handles\nactuator limits and terminal landing constraints, producing physically\nconsistent, optimized control sequences. Both standard automatic\ndifferentiation and Neural ODEs are applied to support long-horizon rollouts.\nResults demonstrate the framework's effectiveness in modeling and optimizing\ncomplex maneuvers with high nonlinearities. This work lays the groundwork for\nfuture extensions involving unsteady aerodynamics, plume interactions, and\nintelligent guidance design.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u822a\u5929\u5668\u7ffb\u8f6c\u548c\u7740\u9646\u8f68\u8ff9\u8bbe\u8ba1\u7684\u53ef\u5fae\u4f18\u5316\u6846\u67b6\uff0c\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6c14\u52a8\u529b\uff0c\u5e76\u901a\u8fc7\u53ef\u5fae\u6c42\u89e3\u5668\u8fdb\u884c\u7aef\u5230\u7aef\u4f18\u5316\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53ef\u91cd\u590d\u4f7f\u7528\u822a\u5929\u5668\uff08\u4ee5\u661f\u8230\u4e3a\u4f8b\uff09\u7684\u7ffb\u8f6c\u548c\u7740\u9646\u8f68\u8ff9\u8bbe\u8ba1\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u5fae\u4f18\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u7528\u4e8e\u9884\u6d4b\u6c14\u52a8\u529b\u7684CFD\u6570\u636e\uff09\u548c\u53ef\u5fae\u521a\u4f53\u52a8\u529b\u5b66\u6c42\u89e3\u5668\uff0c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u68af\u5ea6\u4f18\u5316\u3002", "result": "\u751f\u6210\u7684\u63a7\u5236\u5e8f\u5217\u5728\u7269\u7406\u4e0a\u662f\u4e00\u81f4\u7684\uff0c\u5e76\u6ee1\u8db3\u6267\u884c\u5668\u9650\u5236\u548c\u7ec8\u7aef\u7740\u9646\u7ea6\u675f\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5bf9\u5177\u6709\u9ad8\u975e\u7ebf\u6027\u590d\u6742\u673a\u52a8\u8fdb\u884c\u5efa\u6a21\u548c\u4f18\u5316\uff0c\u4e3a\u672a\u6765\u6d89\u53ca\u4e0d\u7a33\u5b9a\u7a7a\u6c14\u52a8\u529b\u5b66\u3001\u7fbd\u6d41\u76f8\u4e92\u4f5c\u7528\u548c\u667a\u80fd\u5236\u5bfc\u8bbe\u8ba1\u7684\u6269\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06611", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06611", "abs": "https://arxiv.org/abs/2508.06611", "authors": ["Alastair Kay", "Christino Tamon"], "title": "Matrix Inversion by Quantum Walk", "comment": "7 pages, 1 figure", "summary": "The HHL algorithm for matrix inversion is a landmark algorithm in quantum\ncomputation. Its ability to produce a state $|x\\rangle$ that is the solution of\n$Ax=b$, given the input state $|b\\rangle$, is envisaged to have diverse\napplications. In this paper, we substantially simplify the algorithm,\noriginally formed of a complex sequence of phase estimations, amplitude\namplifications and Hamiltonian simulations, by replacing the phase estimations\nwith a continuous time quantum walk. The key technique is the use of weak\ncouplings to access the matrix inversion embedded in perturbation theory.", "AI": {"tldr": "Simplified HHL algorithm using quantum walk instead of phase estimations.", "motivation": "The motivation is to simplify the complex HHL algorithm for matrix inversion and explore its potential applications.", "method": "The HHL algorithm is simplified by replacing phase estimations with a continuous time quantum walk, utilizing weak couplings to access matrix inversion within perturbation theory.", "result": "The simplified algorithm replaces phase estimations with continuous time quantum walk, offering a new approach to quantum computation for matrix inversion.", "conclusion": "the paper presents a simplified HHL algorithm using continuous time quantum walk, which has potential for diverse applications."}}
{"id": "2508.07110", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07110", "abs": "https://arxiv.org/abs/2508.07110", "authors": ["Lorenzo Ruotolo", "Lara Orlandic", "Pengbo Yu", "Moritz Brunion", "Daniele Jahier Pagliari", "Dwaipayan Biswas", "Giovanni Ansaloni", "David Atienza", "Julien Ryckaert", "Francky Catthoor", "Yukai Chen"], "title": "Physical Design Exploration of a Wire-Friendly Domain-Specific Processor for Angstrom-Era Nodes", "comment": null, "summary": "This paper presents the physical design exploration of a domain-specific\nprocessor (DSIP) architecture targeted at machine learning (ML), addressing the\nchallenges of interconnect efficiency in advanced Angstrom-era technologies.\nThe design emphasizes reduced wire length and high core density by utilizing\nspecialized memory structures and SIMD (Single Instruction, Multiple Data)\nunits. Five configurations are synthesized and evaluated using the IMEC A10\nnanosheet node PDK. Key physical design metrics are compared across\nconfigurations and against VWR2A, a state-of-the-art (SoA) DSIP baseline.\nResults show that our architecture achieves over 2x lower normalized wire\nlength and more than 3x higher density than the SoA, with low variability in\nthe metrics across all configurations, making it a promising solution for\nnext-generation DSIP designs. These improvements are achieved with minimal\nmanual layout intervention, demonstrating the architecture's intrinsic physical\nefficiency and potential for low-cost wire-friendly implementation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u5b66\u4e60\u7684DSIP\u67b6\u6784\uff0c\u901a\u8fc7\u4f18\u5316\u4e92\u8fde\u8bbe\u8ba1\uff08\u4f7f\u7528\u4e13\u7528\u5185\u5b58\u548cSIMD\u5355\u5143\uff09\u663e\u8457\u63d0\u9ad8\u4e86\u5bc6\u5ea6\uff083\u500d\u4ee5\u4e0a\uff09\u5e76\u51cf\u5c11\u4e86\u7ebf\u957f\uff082\u500d\u4ee5\u4e0a\uff09\uff0c\u9002\u7528\u4e8e\u5148\u8fdb\u5de5\u827a\u8282\u70b9\uff0c\u4e14\u6613\u4e8e\u5b9e\u73b0\u3002", "motivation": "\u9488\u5bf9\u5148\u8fdbAngstrom\u65f6\u4ee3\u6280\u672f\u4e2d\u4e92\u8fde\u6548\u7387\u7684\u6311\u6218\uff0c\u63a2\u7d22\u7528\u4e8e\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684\u9886\u57df\u7279\u5b9a\u5904\u7406\u5668\uff08DSIP\uff09\u67b6\u6784\u7684\u7269\u7406\u8bbe\u8ba1\u3002", "method": "\u901a\u8fc7\u5229\u7528\u4e13\u95e8\u7684\u5185\u5b58\u7ed3\u6784\u548cSIMD\u5355\u5143\uff0c\u5e76\u4f7f\u7528IMEC A10\u7eb3\u7c73\u7247\u8282\u70b9PDK\u5bf9\u4e94\u79cd\u914d\u7f6e\u8fdb\u884c\u7efc\u5408\u548c\u8bc4\u4f30\uff0c\u4ee5\u63a2\u7d22\u9488\u5bf9\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u7684DSIP\u67b6\u6784\u7684\u7269\u7406\u8bbe\u8ba1\u3002", "result": "\u4e0e\u73b0\u6709\u7684DSIP\u57fa\u7ebf\uff08VWR2A\uff09\u76f8\u6bd4\uff0c\u6240\u63d0\u51fa\u7684\u67b6\u6784\u5b9e\u73b0\u4e86\u8d85\u8fc72\u500d\u7684\u5f52\u4e00\u5316\u7ebf\u957f\u964d\u4f4e\u548c\u8d85\u8fc73\u500d\u7684\u5bc6\u5ea6\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u6240\u6709\u914d\u7f6e\u4e2d\u5404\u9879\u6307\u6807\u7684\u53d8\u5f02\u6027\u8f83\u4f4e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u9886\u57df\u7279\u5b9a\u5904\u7406\u5668\uff08DSIP\uff09\u67b6\u6784\u5728\u4e92\u8fde\u6548\u7387\u3001\u7ebf\u957f\u548c\u6838\u5fc3\u5bc6\u5ea6\u65b9\u9762\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u4e3a\u4e0b\u4e00\u4ee3DSIP\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4e14\u4ec5\u9700\u5c11\u91cf\u624b\u52a8\u5e72\u9884\u5373\u53ef\u5b9e\u73b0\u3002"}}
{"id": "2508.07615", "categories": ["cs.GR"], "pdf": "https://arxiv.org/pdf/2508.07615", "abs": "https://arxiv.org/abs/2508.07615", "authors": ["Chuanfu Hu", "Aimin Hou"], "title": "Verification Method for Graph Isomorphism Criteria", "comment": "17 pages, 5 figures, 2 tables", "summary": "The criteria for determining graph isomorphism are crucial for solving graph\nisomorphism problems. The necessary condition is that two isomorphic graphs\npossess invariants, but their function can only be used to filtrate and\nsubdivide candidate spaces. The sufficient conditions are used to rebuild the\nisomorphic reconstruction of special graphs, but their drawback is that the\nisomorphic functions of subgraphs may not form part of the isomorphic functions\nof the parent graph. The use of sufficient or necessary conditions generally\nresults in backtracking to ensure the correctness of the decision algorithm.\nThe sufficient and necessary conditions can ensure that the determination of\ngraph isomorphism does not require backtracking, but the correctness of its\nproof process is difficult to guarantee. This article proposes a verification\nmethod that can correctly determine whether the judgment conditions proposed by\nprevious researchers are sufficient and necessary conditions. A subdivision\nmethod has also been proposed in this article, which can obtain more\nsubdivisions for necessary conditions and effectively reduce the size of\nbacktracking space.", "AI": {"tldr": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u56fe\u540c\u6784\u5224\u5b9a\u9a8c\u8bc1\u65b9\u6cd5\u548c\u7ec6\u5206\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u5224\u5b9a\u6548\u7387\u3002", "motivation": "\u56fe\u540c\u6784\u95ee\u9898\u7684\u5224\u5b9a\u6761\u4ef6\u5bf9\u4e8e\u89e3\u51b3\u56fe\u540c\u6784\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u65b9\u6cd5\u548c\u4e00\u79cd\u7ec6\u5206\u65b9\u6cd5", "result": "\u8be5\u9a8c\u8bc1\u65b9\u6cd5\u53ef\u4ee5\u6b63\u786e\u5224\u65ad\u5df2\u6709\u56fe\u540c\u6784\u5224\u65ad\u6761\u4ef6\u662f\u5426\u4e3a\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff0c\u7ec6\u5206\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u51cf\u5c0f\u56de\u6eaf\u7a7a\u95f4\u3002", "conclusion": "\u8be5\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6b63\u786e\u5224\u65ad\u5df2\u6709\u56fe\u540c\u6784\u5224\u65ad\u6761\u4ef6\u662f\u5426\u4e3a\u5145\u5206\u5fc5\u8981\u6761\u4ef6\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u79cd\u7ec6\u5206\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5bf9\u5fc5\u8981\u6761\u4ef6\u83b7\u5f97\u66f4\u591a\u7684\u7ec6\u5206\uff0c\u6709\u6548\u51cf\u5c0f\u56de\u6eaf\u7a7a\u95f4\u3002"}}
{"id": "2508.06504", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06504", "abs": "https://arxiv.org/abs/2508.06504", "authors": ["Yao Ge", "Sudeshna Das", "Yuting Guo", "Abeed Sarker"], "title": "Retrieval augmented generation based dynamic prompting for few-shot biomedical named entity recognition using large language models", "comment": "31 pages, 4 figures, 15 tables", "summary": "Biomedical named entity recognition (NER) is a high-utility natural language\nprocessing (NLP) task, and large language models (LLMs) show promise\nparticularly in few-shot settings (i.e., limited training data). In this\narticle, we address the performance challenges of LLMs for few-shot biomedical\nNER by investigating a dynamic prompting strategy involving retrieval-augmented\ngeneration (RAG). In our approach, the annotated in-context learning examples\nare selected based on their similarities with the input texts, and the prompt\nis dynamically updated for each instance during inference. We implemented and\noptimized static and dynamic prompt engineering techniques and evaluated them\non five biomedical NER datasets. Static prompting with structured components\nincreased average F1-scores by 12% for GPT-4, and 11% for GPT-3.5 and LLaMA\n3-70B, relative to basic static prompting. Dynamic prompting further improved\nperformance, with TF-IDF and SBERT retrieval methods yielding the best results,\nimproving average F1-scores by 7.3% and 5.6% in 5-shot and 10-shot settings,\nrespectively. These findings highlight the utility of contextually adaptive\nprompts via RAG for biomedical NER.", "AI": {"tldr": "\u901a\u8fc7\u52a8\u6001\u63d0\u793a\u548cRAG\u6280\u672f\uff0c\u63d0\u5347\u4e86LLMs\u5728\u751f\u7269\u533b\u5b66NER\u4efb\u52a1\u4e0a\u7684\u5c11\u6837\u672c\u5b66\u4e60\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u5c11\u6837\u672c\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u4efb\u52a1\u4e2d\u5b58\u5728\u7684\u6027\u80fd\u6311\u6218\uff0c\u4ee5\u671f\u63d0\u5347\u6a21\u578b\u5728\u6709\u9650\u8bad\u7ec3\u6570\u636e\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u52a8\u6001\u63d0\u793a\u7b56\u7565\uff0c\u901a\u8fc7\u68c0\u7d22\u4e0e\u8f93\u5165\u6587\u672c\u76f8\u4f3c\u7684\u6837\u672c\u6765\u52a8\u6001\u66f4\u65b0\u63d0\u793a\uff0c\u5e76\u5c06\u5176\u4e0e\u9759\u6001\u63d0\u793a\u7b56\u7565\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7814\u7a76\u8005\u4eec\u5b9e\u73b0\u4e86\u5e76\u4f18\u5316\u4e86\u9759\u6001\u548c\u52a8\u6001\u63d0\u793a\u5de5\u7a0b\u6280\u672f\uff0c\u5e76\u5728\u4e94\u4e2a\u751f\u7269\u533b\u5b66NER\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "\u4e0e\u57fa\u7840\u9759\u6001\u63d0\u793a\u76f8\u6bd4\uff0c\u7ed3\u6784\u5316\u7ec4\u4ef6\u7684\u9759\u6001\u63d0\u793a\u5c06GPT-4\u7684\u5e73\u5747F1\u5206\u6570\u63d0\u9ad8\u4e8612%\uff0c\u5c06GPT-3.5\u548cLLaMA 3-70B\u7684\u5e73\u5747F1\u5206\u6570\u63d0\u9ad8\u4e8611%\u3002\u52a8\u6001\u63d0\u793a\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u6027\u80fd\uff0c\u5176\u4e2dTF-IDF\u548cSBERT\u68c0\u7d22\u65b9\u6cd5\u7684\u8868\u73b0\u6700\u4f73\uff0c\u57285-shot\u548c10-shot\u8bbe\u7f6e\u4e0b\u5e73\u5747F1\u5206\u6570\u5206\u522b\u63d0\u9ad8\u4e867.3%\u548c5.6%\u3002", "conclusion": "\u52a8\u6001\u63d0\u793a\u7b56\u7565\uff0c\u7279\u522b\u662f\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u5728\u5c11\u6837\u672c\u751f\u7269\u533b\u5b66\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06562", "categories": ["cs.GT", "cs.DS", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.06562", "abs": "https://arxiv.org/abs/2508.06562", "authors": ["Mohammad T. Hajiaghayi", "Suho Shin"], "title": "Algorithmic Delegated Choice: An Annotated Reading List", "comment": "SIGecom Exchanges, Vol 23, No. 1, July 2025, Pages 80-85", "summary": "The problem of delegated choice has been of long interest in economics and\nrecently on computer science. We overview a list of papers on delegated choice\nproblem, from classic works to recent papers with algorithmic perspectives.", "AI": {"tldr": "This paper reviews the literature on delegated choice problems, bridging economic and computer science perspectives.", "motivation": "To provide a comprehensive overview of the delegated choice problem, integrating classic economic theories with modern algorithmic approaches.", "method": "Literature review and overview of existing papers on delegated choice problems.", "result": "A curated list of papers on delegated choice, highlighting the evolution from economic theory to algorithmic solutions.", "conclusion": "This paper provides an overview of delegated choice problems, covering classic economic works and recent algorithmic perspectives."}}
{"id": "2508.07103", "categories": ["physics.app-ph", "physics.med-ph"], "pdf": "https://arxiv.org/pdf/2508.07103", "abs": "https://arxiv.org/abs/2508.07103", "authors": ["Pradosh Pritam Dash", "Costas D. Arvanitis"], "title": "Acoustic Holography in the Megahertz Frequency Range with Optimal Lens Topologies and Nonlinear Acoustic Feedback", "comment": null, "summary": "Acoustic holography in the megahertz frequency range can impact numerous\napplications, including manufacturing, non-destructive testing, and\ntranscranial ultrasound. However, designing lens topologies for complex\nacoustic holograms in the megahertz range poses a significant challenge, as\nweave propagation effects through the lens cannot be ignored. Here, we show\nthat the inherent ability of heterogeneous angular spectrum approach to\nincorporate in plane varying speed-of-sound maps and support rapid\ndifferentiable optimization of lens thickness profiles can generate lens\ntopologies for high fidelity acoustic holography. Crucially, we show that this\nframework can also account for wavefront aberrations in the propagation media,\nproviding the opportunity to reconfigure this disruptive technology for high\nprecision neuro-interventions. Our investigations also revealed that low\nfrequency acoustic feedback generated by nonlinear mixing of high frequency\nwaves allows attaining accurate skull-compensating lens alignment and creates\nthe possibility to monitor CSF fluid build-up and removal in hydrocephalus.\nTogether, our findings support the design of simple, economical, and\nhigh-performance ultrasound systems.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5f02\u6784\u89d2\u8c31\u65b9\u6cd5\uff0c\u53ef\u4f18\u5316\u900f\u955c\u8bbe\u8ba1\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u58f0\u5168\u606f\u672f\uff0c\u5e76\u80fd\u8865\u507f\u50cf\u5dee\uff0c\u7528\u4e8e\u795e\u7ecf\u5e72\u9884\u3002\u8be5\u65b9\u6cd5\u8fd8\u80fd\u901a\u8fc7\u58f0\u53cd\u9988\u5b9e\u73b0\u9885\u9aa8\u8865\u507f\u548c\u8111\u810a\u6db2\u76d1\u6d4b\u3002", "motivation": "\u5728\u5146\u8d6b\u5179\u9891\u7387\u8303\u56f4\u5185\uff0c\u58f0\u5168\u606f\u672f\u5728\u5236\u9020\u3001\u65e0\u635f\u68c0\u6d4b\u548c\u7ecf\u9885\u8d85\u58f0\u7b49\u9886\u57df\u5177\u6709\u91cd\u8981\u5e94\u7528\u524d\u666f\u3002\u7136\u800c\uff0c\u4e3a\u590d\u6742\u58f0\u5168\u606f\u56fe\u8bbe\u8ba1\u5146\u8d6b\u5179\u8303\u56f4\u5185\u7684\u900f\u955c\u62d3\u6251\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3a\u65e0\u6cd5\u5ffd\u7565\u6ce2\u7a7f\u8fc7\u900f\u955c\u7684\u4f20\u64ad\u6548\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5f02\u6784\u89d2\u8c31\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u7ed3\u5408\u5e73\u9762\u5185\u53d8\u5316\u7684\u58f0\u901f\u56fe\u5e76\u652f\u6301\u900f\u955c\u539a\u5ea6\u5256\u9762\u7684\u5feb\u901f\u53ef\u5fae\u4f18\u5316\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u751f\u6210\u9ad8\u4fdd\u771f\u58f0\u5168\u606f\u672f\u7684\u900f\u955c\u62d3\u6251\uff0c\u5e76\u80fd\u6821\u6b63\u6ce2\u524d\u50cf\u5dee\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u4f4e\u9891\u58f0\u53cd\u9988\u53ef\u5b9e\u73b0\u9885\u9aa8\u8865\u507f\u900f\u955c\u5bf9\u9f50\uff0c\u5e76\u76d1\u6d4b\u8111\u810a\u6db2\u5bb9\u91cf\u53d8\u5316\uff0c\u4ece\u800c\u652f\u6301\u8bbe\u8ba1\u7b80\u5355\u3001\u7ecf\u6d4e\u3001\u9ad8\u6027\u80fd\u7684\u8d85\u58f0\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u9ad8\u4fdd\u771f\u58f0\u5168\u606f\u672f\u8bbe\u8ba1\u4e86\u900f\u955c\u62d3\u6251\uff0c\u5e76\u80fd\u8865\u507f\u4f20\u64ad\u4ecb\u8d28\u4e2d\u7684\u6ce2\u524d\u50cf\u5dee\uff0c\u6709\u671b\u7528\u4e8e\u9ad8\u7cbe\u5ea6\u795e\u7ecf\u5e72\u9884\u3002"}}
{"id": "2508.06793", "categories": ["cs.NE", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06793", "abs": "https://arxiv.org/abs/2508.06793", "authors": ["Bowen Zhang", "Genan Dai", "Hu Huang", "Long Lan"], "title": "Geometry-Aware Spiking Graph Neural Network", "comment": null, "summary": "Graph Neural Networks (GNNs) have demonstrated impressive capabilities in\nmodeling graph-structured data, while Spiking Neural Networks (SNNs) offer high\nenergy efficiency through sparse, event-driven computation. However, existing\nspiking GNNs predominantly operate in Euclidean space and rely on fixed\ngeometric assumptions, limiting their capacity to model complex graph\nstructures such as hierarchies and cycles. To overcome these limitations, we\npropose \\method{}, a novel Geometry-Aware Spiking Graph Neural Network that\nunifies spike-based neural dynamics with adaptive representation learning on\nRiemannian manifolds. \\method{} features three key components: a Riemannian\nEmbedding Layer that projects node features into a pool of constant-curvature\nmanifolds, capturing non-Euclidean structures; a Manifold Spiking Layer that\nmodels membrane potential evolution and spiking behavior in curved spaces via\ngeometry-consistent neighbor aggregation and curvature-based attention; and a\nManifold Learning Objective that enables instance-wise geometry adaptation\nthrough jointly optimized classification and link prediction losses defined\nover geodesic distances. All modules are trained using Riemannian SGD,\neliminating the need for backpropagation through time. Extensive experiments on\nmultiple benchmarks show that GSG achieves superior accuracy, robustness, and\nenergy efficiency compared to both Euclidean SNNs and manifold-based GNNs,\nestablishing a new paradigm for curvature-aware, energy-efficient graph\nlearning.", "AI": {"tldr": "GSG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e5\u8109\u51b2\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u8fdb\u884c\u81ea\u9002\u5e94\u8868\u793a\u5b66\u4e60\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u80fd\u6548\u3002", "motivation": "\u73b0\u6709\u8109\u51b2GNN\u4e3b\u8981\u5728\u6b27\u51e0\u91cc\u5f97\u7a7a\u95f4\u4e2d\u8fd0\u884c\uff0c\u5e76\u4f9d\u8d56\u4e8e\u56fa\u5b9a\u7684\u51e0\u4f55\u5047\u8bbe\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u5c42\u6b21\u7ed3\u6784\u548c\u5468\u671f\u7b49\u590d\u6742\u56fe\u7ed3\u6784\u5efa\u6a21\u7684\u80fd\u529b\u3002", "method": "GSG\u662f\u4e00\u79cd\u65b0\u9896\u7684\u51e0\u4f55\u611f\u77e5\u8109\u51b2\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u5c06\u8109\u51b2\u795e\u7ecf\u52a8\u529b\u5b66\u4e0e\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u81ea\u9002\u5e94\u8868\u793a\u5b66\u4e60\u76f8\u7ed3\u5408\u3002\u5176\u5173\u952e\u7ec4\u6210\u90e8\u5206\u5305\u62ec\uff1a\u9ece\u66fc\u5d4c\u5165\u5c42\uff08\u5c06\u8282\u70b9\u7279\u5f81\u6295\u5f71\u5230\u6052\u5b9a\u66f2\u7387\u6d41\u5f62\u6c60\u4e2d\uff0c\u4ee5\u6355\u6349\u975e\u6b27\u51e0\u91cc\u5f97\u7ed3\u6784\uff09\u3001\u8109\u51b2\u6d41\u5f62\u5c42\uff08\u901a\u8fc7\u51e0\u4f55\u4e00\u81f4\u7684\u90bb\u57df\u805a\u5408\u548c\u57fa\u4e8e\u66f2\u7387\u7684\u6ce8\u610f\u529b\u5728\u5f2f\u66f2\u7a7a\u95f4\u4e2d\u6a21\u62df\u819c\u7535\u4f4d\u6f14\u5316\u548c\u8109\u51b2\u884c\u4e3a\uff09\u548c\u6d41\u5f62\u5b66\u4e60\u76ee\u6807\uff08\u901a\u8fc7\u8054\u5408\u4f18\u5316\u7684\u5206\u7c7b\u548c\u6d4b\u5730\u7ebf\u8ddd\u79bb\u4e0a\u7684\u94fe\u63a5\u9884\u6d4b\u635f\u5931\u6765\u5b9e\u73b0\u5b9e\u4f8b\u7ea7\u7684\u51e0\u4f55\u9002\u5e94\uff09\u3002\u6240\u6709\u6a21\u5757\u5747\u4f7f\u7528\u9ece\u66fcSGD\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u901a\u8fc7\u65f6\u95f4\u53cd\u5411\u4f20\u64ad\u3002", "result": "GSG\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u80fd\u6548\u65b9\u9762\u5747\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97SNN\u548c\u57fa\u4e8e\u6d41\u5f62\u7684GNN\u3002", "conclusion": "GSG\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u548c\u80fd\u6548\u65b9\u9762\u5747\u4f18\u4e8e\u6b27\u51e0\u91cc\u5f97SNN\u548c\u57fa\u4e8e\u6d41\u5f62\u7684GNN\uff0c\u4e3a\u66f2\u7387\u611f\u77e5\u3001\u80fd\u6548\u56fe\u5b66\u4e60\u6811\u7acb\u4e86\u65b0\u8303\u4f8b\u3002"}}
{"id": "2508.06811", "categories": ["cs.SI", "cs.AI", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06811", "abs": "https://arxiv.org/abs/2508.06811", "authors": ["Benjamin Laufer", "Hamidah Oderinwale", "Jon Kleinberg"], "title": "Anatomy of a Machine Learning Ecosystem: 2 Million Models on Hugging Face", "comment": "29 pages, 18 figures and tables", "summary": "Many have observed that the development and deployment of generative machine\nlearning (ML) and artificial intelligence (AI) models follow a distinctive\npattern in which pre-trained models are adapted and fine-tuned for specific\ndownstream tasks. However, there is limited empirical work that examines the\nstructure of these interactions. This paper analyzes 1.86 million models on\nHugging Face, a leading peer production platform for model development. Our\nstudy of model family trees -- networks that connect fine-tuned models to their\nbase or parent -- reveals sprawling fine-tuning lineages that vary widely in\nsize and structure. Using an evolutionary biology lens to study ML models, we\nuse model metadata and model cards to measure the genetic similarity and\nmutation of traits over model families. We find that models tend to exhibit a\nfamily resemblance, meaning their genetic markers and traits exhibit more\noverlap when they belong to the same model family. However, these similarities\ndepart in certain ways from standard models of asexual reproduction, because\nmutations are fast and directed, such that two `sibling' models tend to exhibit\nmore similarity than parent/child pairs. Further analysis of the directional\ndrifts of these mutations reveals qualitative insights about the open machine\nlearning ecosystem: Licenses counter-intuitively drift from restrictive,\ncommercial licenses towards permissive or copyleft licenses, often in violation\nof upstream license's terms; models evolve from multi-lingual compatibility\ntowards english-only compatibility; and model cards reduce in length and\nstandardize by turning, more often, to templates and automatically generated\ntext. Overall, this work takes a step toward an empirically grounded\nunderstanding of model fine-tuning and suggests that ecological models and\nmethods can yield novel scientific insights.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e861.86\u4ebf\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u5fae\u8c03\u6a21\u578b\u5f62\u6210\u5e9e\u5927\u8c31\u7cfb\uff0c\u6709\u5bb6\u65cf\u76f8\u4f3c\u6027\u4f46\u53d8\u5f02\u5feb\u4e14\u6709\u65b9\u5411\u6027\u3002\u8bb8\u53ef\u8bc1\u8d8b\u5411\u5bbd\u677e\uff0c\u6a21\u578b\u8d8b\u5411\u82f1\u8bed\u5316\uff0c\u6a21\u578b\u5361\u7247\u8d8b\u4e8e\u6a21\u677f\u5316\u3002", "motivation": "\u5c3d\u7ba1\u9884\u8bad\u7ec3\u6a21\u578b\u88ab\u5e7f\u6cdb\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u7684\u5fae\u8c03\uff0c\u4f46\u5bf9\u8fd9\u4e9b\u4ea4\u4e92\u7ed3\u6784\u7684\u7814\u7a76\u5374\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5b9e\u8bc1\u5206\u6790\uff0c\u63a2\u7a76\u6a21\u578b\u5fae\u8c03\u7684\u7ed3\u6784\u548c\u6a21\u5f0f\u3002", "method": "\u5229\u7528\u8fdb\u5316\u751f\u7269\u5b66\u7684\u65b9\u6cd5\uff0c\u5206\u6790\u4e86Hugging Face\u4e0a1.86\u4ebf\u4e2a\u6a21\u578b\u7684\u201c\u6a21\u578b\u5bb6\u65cf\u6811\u201d\uff08\u8fde\u63a5\u5fae\u8c03\u6a21\u578b\u4e0e\u5176\u57fa\u7840/\u7236\u6a21\u578b\u7684\u7f51\u7edc\uff09\u3002\u901a\u8fc7\u6a21\u578b\u5143\u6570\u636e\u548c\u6a21\u578b\u5361\u7247\uff0c\u8861\u91cf\u4e86\u6a21\u578b\u5bb6\u65cf\u7684\u9057\u4f20\u76f8\u4f3c\u6027\u548c\u7279\u5f81\u53d8\u5f02\u3002", "result": "\u6a21\u578b\u5fae\u8c03\u5f62\u6210\u4e86\u5e9e\u5927\u7684\u8c31\u7cfb\uff0c\u5927\u5c0f\u548c\u7ed3\u6784\u5404\u5f02\u3002\u6a21\u578b\u503e\u5411\u4e8e\u8868\u73b0\u51fa\u201c\u5bb6\u65cf\u76f8\u4f3c\u6027\u201d\uff0c\u4f46\u5176\u53d8\u5f02\u65b9\u5f0f\u4e0d\u540c\u4e8e\u65e0\u6027\u7e41\u6b96\uff0c\u53d8\u5f02\u5feb\u4e14\u6709\u65b9\u5411\u6027\uff0c\u5144\u5f1f\u6a21\u578b\u6bd4\u4eb2\u5b50\u6a21\u578b\u66f4\u76f8\u4f3c\u3002\u8bb8\u53ef\u8bc1\u4ece\u9650\u5236\u6027\u6f02\u79fb\u5230\u5bbd\u677e\u6027\uff0c\u6a21\u578b\u517c\u5bb9\u6027\u4ece\u591a\u8bed\u8a00\u8f6c\u5411\u82f1\u8bed\uff0c\u6a21\u578b\u5361\u7247\u53d8\u77ed\u5e76\u8d8b\u4e8e\u6a21\u677f\u5316\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5206\u6790Hugging Face\u4e0a\u7684186\u4e07\u4e2a\u6a21\u578b\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5fae\u8c03\u7684\u8c31\u7cfb\u7ed3\u6784\u548c\u8fdb\u5316\u6a21\u5f0f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u4e4b\u95f4\u5b58\u5728\u201c\u5bb6\u65cf\u76f8\u4f3c\u6027\u201d\uff0c\u4f46\u5176\u53d8\u5f02\u65b9\u5f0f\u4e0d\u540c\u4e8e\u65e0\u6027\u7e41\u6b96\uff0c\u53d8\u5f02\u901f\u5ea6\u5feb\u4e14\u6709\u65b9\u5411\u6027\uff0c\u5144\u5f1f\u6a21\u578b\u6bd4\u4eb2\u5b50\u6a21\u578b\u66f4\u76f8\u4f3c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u89c2\u5bdf\u5230\u8bb8\u53ef\u8bc1\u4ece\u9650\u5236\u6027\u5411\u5bbd\u677e\u6027\u6f02\u79fb\u3001\u6a21\u578b\u4ece\u591a\u8bed\u8a00\u517c\u5bb9\u8f6c\u5411\u4ec5\u82f1\u8bed\u517c\u5bb9\uff0c\u4ee5\u53ca\u6a21\u578b\u5361\u7247\u957f\u5ea6\u7f29\u77ed\u548c\u6a21\u677f\u5316\u7b49\u73b0\u8c61\uff0c\u8fd9\u4e9b\u90fd\u4e3a\u4e86\u89e3\u5f00\u653e\u673a\u5668\u5b66\u4e60\u751f\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.06511", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06511", "abs": "https://arxiv.org/abs/2508.06511", "authors": ["He Feng", "Yongjia Ma", "Donglin Di", "Lei Fan", "Tonghua Su", "Xiangqian Wu"], "title": "DiTalker: A Unified DiT-based Framework for High-Quality and Speaking Styles Controllable Portrait Animation", "comment": null, "summary": "Portrait animation aims to synthesize talking videos from a static reference\nface, conditioned on audio and style frame cues (e.g., emotion and head poses),\nwhile ensuring precise lip synchronization and faithful reproduction of\nspeaking styles. Existing diffusion-based portrait animation methods primarily\nfocus on lip synchronization or static emotion transformation, often\noverlooking dynamic styles such as head movements. Moreover, most of these\nmethods rely on a dual U-Net architecture, which preserves identity consistency\nbut incurs additional computational overhead. To this end, we propose DiTalker,\na unified DiT-based framework for speaking style-controllable portrait\nanimation. We design a Style-Emotion Encoding Module that employs two separate\nbranches: a style branch extracting identity-specific style information (e.g.,\nhead poses and movements), and an emotion branch extracting identity-agnostic\nemotion features. We further introduce an Audio-Style Fusion Module that\ndecouples audio and speaking styles via two parallel cross-attention layers,\nusing these features to guide the animation process. To enhance the quality of\nresults, we adopt and modify two optimization constraints: one to improve lip\nsynchronization and the other to preserve fine-grained identity and background\ndetails. Extensive experiments demonstrate the superiority of DiTalker in terms\nof lip synchronization and speaking style controllability. Project Page:\nhttps://thenameishope.github.io/DiTalker/", "AI": {"tldr": "DiTalker \u662f\u4e00\u4e2a\u57fa\u4e8e DiT \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u4eba\u50cf\u52a8\u753b\uff0c\u901a\u8fc7\u5176\u98ce\u683c-\u60c5\u611f\u7f16\u7801\u6a21\u5757\u548c\u97f3\u9891-\u98ce\u683c\u878d\u5408\u6a21\u5757\uff0c\u53ef\u4ee5\u5b9e\u73b0\u52a8\u6001\u7684\u8bf4\u8bdd\u98ce\u683c\uff08\u5982\u5934\u90e8\u8fd0\u52a8\uff09\u548c\u60c5\u611f\u63a7\u5236\uff0c\u540c\u65f6\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\u5e76\u63d0\u9ad8\u5507\u5f62\u540c\u6b65\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u4eba\u50cf\u52a8\u753b\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u5507\u5f62\u540c\u6b65\u6216\u9759\u6001\u60c5\u611f\u8f6c\u6362\uff0c\u5ffd\u89c6\u4e86\u8bf8\u5982\u5934\u90e8\u8fd0\u52a8\u7b49\u52a8\u6001\u98ce\u683c\u3002\u6b64\u5916\uff0c\u5927\u591a\u6570\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u53cc U-Net \u67b6\u6784\uff0c\u867d\u7136\u53ef\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4f46\u4f1a\u589e\u52a0\u8ba1\u7b97\u5f00\u9500\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DiTalker \u7684\u7edf\u4e00\u7684\u3001\u57fa\u4e8e DiT \u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bf4\u8bdd\u98ce\u683c\u53ef\u63a7\u7684\u4eba\u50cf\u52a8\u753b\u3002\u8be5\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u98ce\u683c-\u60c5\u611f\u7f16\u7801\u6a21\u5757\uff0c\u5176\u4e2d\u5305\u542b\u4e00\u4e2a\u63d0\u53d6\u8eab\u4efd\u7279\u5b9a\u98ce\u683c\u4fe1\u606f\uff08\u4f8b\u5982\u5934\u90e8\u59ff\u52bf\u548c\u8fd0\u52a8\uff09\u7684\u98ce\u683c\u5206\u652f\u548c\u4e00\u4e2a\u63d0\u53d6\u8eab\u4efd\u65e0\u5173\u60c5\u611f\u7279\u5f81\u7684\u60c5\u611f\u5206\u652f\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u97f3\u9891-\u98ce\u683c\u878d\u5408\u6a21\u5757\uff0c\u901a\u8fc7\u4e24\u4e2a\u5e76\u884c\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u5c42\u6765\u89e3\u8026\u97f3\u9891\u548c\u8bf4\u8bdd\u98ce\u683c\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u7279\u5f81\u6765\u6307\u5bfc\u52a8\u753b\u8fc7\u7a0b\u3002\u4e3a\u4e86\u63d0\u9ad8\u7ed3\u679c\u8d28\u91cf\uff0c\u91c7\u7528\u4e86\u4e24\u4e2a\u4f18\u5316\u7ea6\u675f\uff1a\u4e00\u4e2a\u7528\u4e8e\u6539\u8fdb\u5507\u5f62\u540c\u6b65\uff0c\u53e6\u4e00\u4e2a\u7528\u4e8e\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u8eab\u4efd\u548c\u80cc\u666f\u7ec6\u8282\u3002", "result": "DiTalker \u5728\u5507\u5f62\u540c\u6b65\u548c\u8bf4\u8bdd\u98ce\u683c\u53ef\u63a7\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "DiTalker \u5728\u5507\u5f62\u540c\u6b65\u548c\u8bf4\u8bdd\u98ce\u683c\u53ef\u63a7\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.07573", "categories": ["cs.ET", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.07573", "abs": "https://arxiv.org/abs/2508.07573", "authors": ["Binquan Guo", "Wanting Yang", "Zehui Xiong", "Zhou Zhang", "Baosheng Li", "Zhu Han", "Rahim Tafazolli", "Tony Q. S. Quek"], "title": "Enhancing Mega-Satellite Networks with Generative Semantic Communication: A Networking Perspective", "comment": "Accepted paper to be published in IEEE", "summary": "The advance of direct satellite-to-device communication has positioned\nmega-satellite constellations as a cornerstone of 6G wireless communication,\nenabling seamless global connectivity even in remote and underserved areas.\nHowever, spectrum scarcity and capacity constraints imposed by the Shannon's\nclassical information theory remain significant challenges for supporting the\nmassive data demands of multimedia-rich wireless applications. Generative\nSemantic Communication (GSC), powered by artificial intelligence-based\ngenerative foundation models, represents a paradigm shift from transmitting raw\ndata to exchanging semantic meaning. GSC can not only reduce bandwidth\nconsumption, but also enhance key semantic features in multimedia content,\nthereby offering a promising solution to overcome the limitations of\ntraditional satellite communication systems. This article investigates the\nintegration of GSC into mega-satellite constellations from a networking\nperspective. We propose a GSC-empowered satellite networking architecture and\nidentify key enabling technologies, focusing on GSC-empowered network modeling\nand GSC-aware networking strategies. We construct a discrete temporal graph to\nmodel semantic encoders and decoders, distinct knowledge bases, and resource\nvariations in mega-satellite networks. Based on this framework, we develop\nmodel deployment for semantic encoders and decoders and GSC-compatible routing\nschemes, and then present performance evaluations. Finally, we outline future\nresearch directions for advancing GSC-empowered satellite networks.", "AI": {"tldr": "GSC\u901a\u8fc7AI\u8d4b\u80fd\uff0c\u7528\u4f20\u9012\u8bed\u4e49\u4fe1\u606f\u4ee3\u66ff\u539f\u59cb\u6570\u636e\uff0c\u6709\u671b\u89e3\u51b3\u536b\u661f\u901a\u4fe1\u7684\u5e26\u5bbd\u548c\u5bb9\u91cf\u74f6\u9888\u3002\u672c\u6587\u63d0\u51fa\u4e86GSC\u8d4b\u80fd\u7684\u536b\u661f\u7f51\u7edc\u67b6\u6784\u548c\u8def\u7531\u7b56\u7565\u3002", "motivation": "\u968f\u77406G\u65e0\u7ebf\u901a\u4fe1\u548c\u5de8\u578b\u536b\u661f\u661f\u5ea7\u7684\u53d1\u5c55\uff0c\u5c3d\u7ba1\u5176\u80fd\u591f\u5b9e\u73b0\u5168\u7403\u65e0\u7f1d\u8fde\u63a5\uff0c\u4f46\u9891\u8c31\u7a00\u7f3a\u548c\u5bb9\u91cf\u9650\u5236\u4ecd\u662f\u5236\u7ea6\u5176\u652f\u6301\u6d77\u91cf\u591a\u5a92\u4f53\u6570\u636e\u9700\u6c42\u7684\u91cd\u5927\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u65b0\u7684\u901a\u4fe1\u8303\u5f0f\u6765\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cdGSC\u8d4b\u80fd\u7684\u536b\u661f\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u4f7f\u80fd\u6280\u672f\u3002\u5177\u4f53\u5305\u62ec\uff1a1. \u6784\u5efa\u79bb\u6563\u65f6\u95f4\u56fe\u6765\u5bf9GSC\u8d4b\u80fd\u7684\u536b\u661f\u7f51\u7edc\u4e2d\u7684\u8bed\u4e49\u7f16\u7801\u5668\u3001\u89e3\u7801\u5668\u3001\u77e5\u8bc6\u5e93\u548c\u8d44\u6e90\u53d8\u5f02\u8fdb\u884c\u5efa\u6a21\u30022. \u57fa\u4e8e\u8be5\u6a21\u578b\uff0c\u5f00\u53d1\u4e86\u8bed\u4e49\u7f16\u7801\u5668\u548c\u89e3\u7801\u5668\u7684\u6a21\u578b\u90e8\u7f72\u4ee5\u53caGSC\u517c\u5bb9\u7684\u8def\u7531\u65b9\u6848\u3002", "result": "\u6587\u7ae0\u901a\u8fc7\u6027\u80fd\u8bc4\u4f30\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u7684GSC\u8d4b\u80fd\u7684\u536b\u661f\u7f51\u7edc\u67b6\u6784\u548c\u8def\u7531\u65b9\u6848\u7684\u6709\u6548\u6027\uff0c\u5e76\u4e3aGSC\u8d4b\u80fd\u7684\u536b\u661f\u7f51\u7edc\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u63d0\u4f9b\u4e86\u5c55\u671b\u3002", "conclusion": "GSC\u4e3a\u89e3\u51b3\u4f20\u7edf\u536b\u661f\u901a\u4fe1\u7684\u9891\u8c31\u7a00\u7f3a\u548c\u5bb9\u91cf\u9650\u5236\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u4ea4\u6362\u8bed\u4e49\u4fe1\u606f\u6765\u964d\u4f4e\u5e26\u5bbd\u6d88\u8017\u5e76\u589e\u5f3a\u591a\u5a92\u4f53\u5185\u5bb9\u7684\u5173\u952e\u8bed\u4e49\u7279\u5f81\u3002\u6587\u7ae0\u63d0\u51fa\u4e86GSC\u8d4b\u80fd\u7684\u536b\u661f\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u8bc6\u522b\u4e86\u5173\u952e\u6280\u672f\uff0c\u5305\u62ec\u57fa\u4e8e\u79bb\u6563\u65f6\u95f4\u56fe\u7684\u8bed\u4e49\u7f16\u7801\u5668/\u89e3\u7801\u5668\u3001\u77e5\u8bc6\u5e93\u548c\u8d44\u6e90\u53d8\u5f02\u5efa\u6a21\uff0c\u4ee5\u53caGSC\u517c\u5bb9\u7684\u8def\u7531\u7b56\u7565\u3002"}}
{"id": "2508.06708", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06708", "abs": "https://arxiv.org/abs/2508.06708", "authors": ["Justin London"], "title": "Embedded Microcontrol for Photovoltaic Water Pumping System", "comment": null, "summary": "We introduce a novel 3-axis solar tracker water pumping system. The charge\ngenerated from solar energy converted by the photovolatic panel (PV) cells is\nstored in a 12V battery that in turn powers two water diaphragm pumps using a\nsolar charge controller that includes an MPPT algorithm that serves as a DC-DC\nconverter. The system is analyzed from an embedded microcontroller and embedded\nsoftware perspective using Arduino. The photovoltaic panel uses four light\nphotocell resistors (LPRs) which measure solar light intensity. An ultrasonic\nsensor measures the water level in a reservoir water tank. If the water level\nis too low, water is pumped from one water tank to the reservoir tank. Using a\nsoil moisture sensor, another water pump pumps water from the reservoir tank to\nthe plant if water is needed. Circuit designs for the system are provided as\nwell as the embedded software used. Simulation and experimental results are\ngiven.", "AI": {"tldr": "A 3-axis solar tracker system automates water pumping using solar energy, battery storage, and sensors to monitor light, water levels, and soil moisture. Controlled by Arduino, it optimizes water delivery for plants.", "motivation": "The motivation is to develop an automated and efficient water pumping system powered by solar energy. The system aims to optimize water usage by tracking sunlight for maximum energy generation and by sensing water levels and soil moisture to deliver water only when needed.", "method": "The study introduces a 3-axis solar tracker water pumping system that utilizes a photovoltaic panel to charge a 12V battery. A solar charge controller with an MPPT algorithm functions as a DC-DC converter to power two water diaphragm pumps. The system is monitored and controlled using an Arduino microcontroller with embedded software. Four light photocell resistors (LPRs) measure solar light intensity, an ultrasonic sensor monitors the water level in a reservoir tank, and a soil moisture sensor determines the need for watering plants.", "result": "The paper provides circuit designs and embedded software for the described system. It also includes simulation and experimental results to validate the system's performance.", "conclusion": "The paper presents a novel 3-axis solar tracker water pumping system with two pumps, powered by a battery charged from a PV panel. The system uses an MPPT algorithm for DC-DC conversion and is controlled by an Arduino microcontroller. It includes sensors for light intensity, water level, and soil moisture to automate watering. Circuit designs and embedded software are provided, along with simulation and experimental results."}}
{"id": "2508.06713", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06713", "abs": "https://arxiv.org/abs/2508.06713", "authors": ["Changhee Lee", "Nico A. Hackner", "P. M. R. Brydon"], "title": "Incommensuration in odd-parity antiferromagnets", "comment": "15 pages, 4 figures, 5 tables", "summary": "Inversion-asymmetric antiferromagnets (AFMs) with odd-parity\nspin-polarization pattern have been proposed as a new venue for spintronics.\nThese AFMs require commensurate ordering to ensure an effective time-reversal\nsymmetry, which guarantees a strictly antisymmetric spin polarization of the\nelectronic states. Recently, nonsymmorphic centrosymmetric crystals have been\nidentified as a broad class of materials which could exhibit unit-cell doubling\nmagnetism with odd-parity spin-polarization. Here we investigate the stability\nof these states against incommensuration. We first demonstrate that the\nsymmetry conditions which permit a p-wave spin polarization pattern also permit\nthe existence of a non-relativistic Lifshitz invariant in the phenomenological\nGinzburg-Landau free energy. This implies magnetism with an incommensurate\nordering vector, independent of its microscopic origin. AFMs with f- or h-wave\nspin-polarization are also prone to incommensurability, especially when they\nhave an itinerant origin. Here the symmetry which ensures the odd-parity\nspin-polarization also guarantees the existence of van Hove saddle points off\nthe time-reversal-invariant momenta, which promote incommensurate spin\nfluctuations in quasi-two-dimensional electronic systems. Finally, we study the\neffect of weak spin-orbit coupling in locally noncentrosymmetric materials and\nfind that it favors antiferromagnetic phases with in-plane magnetic moments.\nHowever, the inclusion of the spin-orbit coupling also introduces a new\nmechanism for driving incommensuration. Our results imply that odd-parity AFMs\nare likely to be preceded by an incommensurate phase, or emerge directly from\nthe normal state via a first order transition. These conclusions are consistent\nwith the phase diagram of several candidate materials.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06672", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06672", "abs": "https://arxiv.org/abs/2508.06672", "authors": ["Jacob S. Clements", "Zachary L. Clements"], "title": "GPU-accelerated Direct Geolocation of GNSS Interference", "comment": null, "summary": "In recent years, there has been a sharp increase in Global Navigation\nSatellite Systems (GNSS) interference, which has proven to be problematic in\nGNSS-dependent civilian applications. Many currently deployed GNSS receivers\nlack the proper countermeasures to defend themselves against interference,\nprompting the need for alternative defenses. Satellites in Low Earth Orbit\n(LEO) provide an opportunity for GNSS interference detection, classification,\nand localization. The direct geolocation approach has been shown to be\nwell-suited for low SNR regimes and in cases limited to short captures --\nexactly what is expected for receivers in LEO. Direct geolocation is a\nsingle-step search over a geographical grid that enables estimation of the\ntransmitter location directly from correlating raw observed signals. However, a\nkey limitation to this approach is the computational requirements. This\ncomputational burden is compounded for LEO-based receivers as the geographic\nsearch space is extensive. This paper alleviates the computational burden of\ndirect geolocation by exploiting the independence of position-domain\ncorrelation across candidate points and time steps: nearly all computation can\nbe accomplished in parallel on a graphics processing unit (GPU). This paper\npresents and evaluates the performance of GPU-accelerated direct geolocation\ncompared to traditional CPU processing.", "AI": {"tldr": "A new GPU-based method speeds up GNSS interference location for satellites by processing calculations in parallel, overcoming the high computational cost of existing techniques.", "motivation": "The motivation is to address the increasing problem of GNSS interference in civilian applications, particularly for receivers in Low Earth Orbit (LEO). Existing GNSS receivers often lack adequate countermeasures, and the direct geolocation approach, while suitable for LEO conditions, suffers from high computational requirements, which are further exacerbated by the extensive search space for LEO receivers.", "method": "The paper proposes a GPU-accelerated direct geolocation method. This approach exploits the independence of position-domain correlation across candidate points and time steps, allowing for parallel computation on a GPU to alleviate the computational burden associated with direct geolocation for LEO-based receivers.", "result": "The paper evaluates the performance of the GPU-accelerated direct geolocation compared to traditional CPU processing, demonstrating its effectiveness in alleviating the computational burden.", "conclusion": "The paper presents a GPU-accelerated direct geolocation approach to address the computational challenges of GNSS interference detection and localization for LEO-based receivers. The proposed method significantly reduces computational burden by leveraging the parallelism of GPU processing, offering a viable solution for improving GNSS security in civilian applications."}}
{"id": "2508.07207", "categories": ["cs.LO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07207", "abs": "https://arxiv.org/abs/2508.07207", "authors": ["S. Akshay", "A. R. Balasubramanian", "Supratik Chakraborty", "Georg Zetzsche"], "title": "Presburger Functional Synthesis: Complexity and Tractable Normal Forms", "comment": "Full version of conference paper at KR 2025 (22nd International\n  Conference on Principles of Knowledge Representation and Reasoning)", "summary": "Given a relational specification between inputs and outputs as a logic\nformula, the problem of functional synthesis is to automatically synthesize a\nfunction from inputs to outputs satisfying the relation. Recently, a rich line\nof work has emerged tackling this problem for specifications in different\ntheories, from Boolean to general first-order logic. In this paper, we launch\nan investigation of this problem for the theory of Presburger Arithmetic, that\nwe call Presburger Functional Synthesis (PFnS). We show that PFnS can be solved\nin EXPTIME and provide a matching exponential lower bound. This is unlike the\ncase for Boolean functional synthesis (BFnS), where only conditional\nexponential lower bounds are known. Further, we show that PFnS for one input\nand one output variable is as hard as BFnS in general. We then identify a\nspecial normal form, called PSyNF, for the specification formula that\nguarantees poly-time and poly-size solvability of PFnS. We prove several\nproperties of PSyNF, including how to check and compile to this form, and\nconditions under which any other form that guarantees poly-time solvability of\nPFnS can be compiled in poly-time to PSyNF. Finally, we identify a syntactic\nnormal form that is easier to check but is exponentially less succinct than\nPSyNF.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76 Presburger \u7b97\u672f\u4e0b\u7684\u51fd\u6570\u5408\u6210\u95ee\u9898 (PFnS)\uff0c\u8bc1\u660e\u5176\u4e3a EXPTIME \u5b8c\u5168\u95ee\u9898\uff0c\u5e76\u63d0\u51fa PSyNF \u8303\u5f0f\u53ef\u5b9e\u73b0\u591a\u9879\u5f0f\u65f6\u95f4\u6c42\u89e3\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u7814\u7a76 Presburger \u7b97\u672f\u4e0b\u7684\u51fd\u6570\u5408\u6210\u95ee\u9898\uff08PFnS\uff09\uff0c\u5e76\u63a2\u7d22\u5176\u8ba1\u7b97\u590d\u6742\u6027\u3002\u7814\u7a76\u52a8\u673a\u5728\u4e8e\u4e86\u89e3 PFnS \u95ee\u9898\u76f8\u5bf9\u4e8e\u5176\u4ed6\u7406\u8bba\uff08\u5982\u5e03\u5c14\u903b\u8f91\uff09\u4e0b\u7684\u51fd\u6570\u5408\u6210\u95ee\u9898\u7684\u5f02\u540c\uff0c\u7279\u522b\u662f\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4ee5\u53ca\u662f\u5426\u5b58\u5728\u66f4\u9ad8\u6548\u7684\u6c42\u89e3\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u7814\u7a76\u4e5f\u5e0c\u671b\u4e3a Presburger \u7b97\u672f\u4e0b\u7684\u51fd\u6570\u5408\u6210\u63d0\u4f9b\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u9645\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u9996\u5148\u5c06 Presburger \u7b97\u672f\u4e0b\u7684\u51fd\u6570\u5408\u6210\u95ee\u9898\uff08PFnS\uff09\u5efa\u6a21\u4e3a\u903b\u8f91\u516c\u5f0f\uff0c\u7136\u540e\u8bc1\u660e\u4e86\u8be5\u95ee\u9898\u53ef\u4ee5\u5728 EXPTIME \u4e2d\u89e3\u51b3\uff0c\u5e76\u7ed9\u51fa\u4e86\u76f8\u5e94\u7684\u6307\u6570\u7ea7\u4e0b\u754c\u3002\u63a5\u7740\uff0c\u6587\u7ae0\u5c06\u5355\u4e2a\u8f93\u5165\u8f93\u51fa\u53d8\u91cf\u7684 PFnS \u95ee\u9898\u4e0e\u4e00\u822c\u7684\u5e03\u5c14\u51fd\u6570\u5408\u6210\u95ee\u9898\uff08BFnS\uff09\u8fdb\u884c\u4e86\u96be\u5ea6\u6bd4\u8f83\u3002\u968f\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PSyNF \u7684\u8303\u5f0f\uff0c\u5e76\u8bc1\u660e\u4e86\u6ee1\u8db3\u6b64\u8303\u5f0f\u7684 PFnS \u95ee\u9898\u5177\u6709\u591a\u9879\u5f0f\u65f6\u95f4\u4e0e\u591a\u9879\u5f0f\u7a7a\u95f4\u53ef\u89e3\u6027\u3002\u6587\u7ae0\u8fd8\u5bf9 PSyNF \u7684\u6027\u8d28\u8fdb\u884c\u4e86\u8bc1\u660e\uff0c\u5305\u62ec\u5982\u4f55\u68c0\u67e5\u548c\u7f16\u8bd1\u5230\u8be5\u8303\u5f0f\uff0c\u4ee5\u53ca\u4e0e\u5176\u4ed6\u4fdd\u8bc1\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u6027\u7684\u8303\u5f0f\u7684\u7f16\u8bd1\u5173\u7cfb\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u6cd5\u8303\u5f0f\uff0c\u5e76\u5206\u6790\u4e86\u5176\u4e0e PSyNF \u7684\u89c4\u6a21\u548c\u68c0\u67e5\u590d\u6742\u5ea6\u5dee\u5f02\u3002", "result": "PFnS \u95ee\u9898\u53ef\u5728 EXPTIME \u4e2d\u89e3\u51b3\uff0c\u5e76\u5177\u6709\u6307\u6570\u7ea7\u4e0b\u754c\u3002\u5355\u4e2a\u8f93\u5165\u8f93\u51fa\u53d8\u91cf\u7684 PFnS \u95ee\u9898\u4e0e BFnS \u95ee\u9898\u96be\u5ea6\u76f8\u5f53\u3002PSyNF \u8303\u5f0f\u53ef\u4fdd\u8bc1 PFnS \u7684\u591a\u9879\u5f0f\u65f6\u95f4\u4e0e\u591a\u9879\u5f0f\u7a7a\u95f4\u53ef\u89e3\u6027\u3002\u6587\u7ae0\u8fd8\u63d0\u4f9b\u4e86\u5173\u4e8e PSyNF \u7684\u68c0\u67e5\u3001\u7f16\u8bd1\u65b9\u6cd5\u4ee5\u53ca\u4e0e\u5176\u4ed6\u8303\u5f0f\u7684\u5173\u7cfb\u3002", "conclusion": "\u672c\u6587\u7814\u7a76\u4e86 Presburger \u7b97\u672f\u4e0b\u7684\u51fd\u6570\u5408\u6210\u95ee\u9898\uff08PFnS\uff09\uff0c\u8bc1\u660e\u4e86\u5176\u53ef\u89e3\u6027\u5728 EXPTIME \u4e2d\uff0c\u5e76\u7ed9\u51fa\u4e86\u5339\u914d\u7684\u6307\u6570\u7ea7\u4e0b\u754c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5355\u4e2a\u8f93\u5165\u8f93\u51fa\u53d8\u91cf\u7684 PFnS \u95ee\u9898\u4e0e\u4e00\u822c\u7684\u5e03\u5c14\u51fd\u6570\u5408\u6210\u95ee\u9898\uff08BFnS\uff09\u5177\u6709\u76f8\u540c\u7684\u96be\u5ea6\u3002\u6587\u7ae0\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PSyNF \u7684\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u80fd\u591f\u4fdd\u8bc1 PFnS \u7684\u591a\u9879\u5f0f\u65f6\u95f4\u4e0e\u591a\u9879\u5f0f\u7a7a\u95f4\u53ef\u89e3\u6027\uff0c\u5e76\u5bf9 PSyNF \u7684\u6027\u8d28\u8fdb\u884c\u4e86\u6df1\u5165\u5206\u6790\uff0c\u5305\u62ec\u5176\u68c0\u67e5\u548c\u7f16\u8bd1\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e0e\u5176\u4ed6\u4fdd\u8bc1\u591a\u9879\u5f0f\u65f6\u95f4\u53ef\u89e3\u6027\u7684\u8303\u5f0f\u7684\u5173\u7cfb\u3002\u6700\u540e\uff0c\u6587\u7ae0\u8bc6\u522b\u4e86\u4e00\u79cd\u66f4\u6613\u4e8e\u68c0\u67e5\u4f46\u89c4\u6a21\u53ef\u80fd\u5448\u6307\u6570\u7ea7\u589e\u957f\u7684\u8bed\u6cd5\u8303\u5f0f\u3002"}}
{"id": "2508.06569", "categories": ["cs.AI", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06569", "abs": "https://arxiv.org/abs/2508.06569", "authors": ["Lance Yao", "Suman Samantray", "Ayana Ghosh", "Kevin Roccapriore", "Libor Kovarik", "Sarah Allec", "Maxim Ziatdinov"], "title": "Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop", "comment": null, "summary": "The history of science is punctuated by serendipitous discoveries, where\nunexpected observations, rather than targeted hypotheses, opened new fields of\ninquiry. While modern autonomous laboratories excel at accelerating hypothesis\ntesting, their optimization for efficiency risks overlooking these crucial,\nunplanned findings. To address this gap, we introduce SciLink, an open-source,\nmulti-agent artificial intelligence framework designed to operationalize\nserendipity in materials research by creating a direct, automated link between\nexperimental observation, novelty assessment, and theoretical simulations. The\nframework employs a hybrid AI strategy where specialized machine learning\nmodels perform quantitative analysis of experimental data, while large language\nmodels handle higher-level reasoning. These agents autonomously convert raw\ndata from materials characterization techniques into falsifiable scientific\nclaims, which are then quantitatively scored for novelty against the published\nliterature. We demonstrate the framework's versatility across diverse research\nscenarios, showcasing its application to atomic-resolution and hyperspectral\ndata, its capacity to integrate real-time human expert guidance, and its\nability to close the research loop by proposing targeted follow-up experiments.\nBy systematically analyzing all observations and contextualizing them, SciLink\nprovides a practical framework for AI-driven materials research that not only\nenhances efficiency but also actively cultivates an environment ripe for\nserendipitous discoveries, thereby bridging the gap between automated\nexperimentation and open-ended scientific exploration.", "AI": {"tldr": "SciLink \u662f\u4e00\u4e2a\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u81ea\u52a8\u5316\u94fe\u63a5\u5b9e\u9a8c\u89c2\u5bdf\u3001\u65b0\u9896\u6027\u8bc4\u4f30\u548c\u7406\u8bba\u6a21\u62df\u6765\u589e\u5f3a\u6750\u6599\u7814\u7a76\u4e2d\u7684\u5076\u7136\u53d1\u73b0\u3002", "motivation": "\u73b0\u4ee3\u81ea\u4e3b\u5b9e\u9a8c\u5ba4\u5728\u52a0\u901f\u5047\u8bbe\u68c0\u9a8c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5176\u5bf9\u6548\u7387\u7684\u4f18\u5316\u5b58\u5728\u5ffd\u7565\u91cd\u8981\u7684\u3001\u8ba1\u5212\u5916\u7684\u53d1\u73b0\u7684\u98ce\u9669\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u6211\u4eec\u5f15\u5165\u4e86 SciLink\uff0c\u8fd9\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u3001\u591a\u4ee3\u7406\u7684\u4eba\u5de5\u667a\u80fd\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u5728\u5b9e\u9a8c\u89c2\u5bdf\u3001\u65b0\u9896\u6027\u8bc4\u4f30\u548c\u7406\u8bba\u6a21\u62df\u4e4b\u95f4\u5efa\u7acb\u76f4\u63a5\u7684\u81ea\u52a8\u5316\u94fe\u63a5\uff0c\u6765\u64cd\u4f5c\u5316\u6750\u6599\u7814\u7a76\u4e2d\u7684\u5076\u7136\u6027\u3002", "method": "SciLink \u6846\u67b6\u91c7\u7528\u6df7\u5408\u4eba\u5de5\u667a\u80fd\u7b56\u7565\uff0c\u5176\u4e2d\u4e13\u95e8\u7684\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5bf9\u5b9e\u9a8c\u6570\u636e\u8fdb\u884c\u5b9a\u91cf\u5206\u6790\uff0c\u800c\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5219\u5904\u7406\u66f4\u9ad8\u7ea7\u522b\u7684\u63a8\u7406\u3002\u8fd9\u4e9b\u4ee3\u7406\u5668\u80fd\u591f\u81ea\u52a8\u5c06\u6750\u6599\u8868\u5f81\u6280\u672f\u7684\u539f\u59cb\u6570\u636e\u8f6c\u6362\u4e3a\u53ef\u8bc1\u4f2a\u7684\u79d1\u5b66\u4e3b\u5f20\uff0c\u7136\u540e\u6839\u636e\u5df2\u53d1\u8868\u7684\u6587\u732e\u5bf9\u5176\u65b0\u9896\u6027\u8fdb\u884c\u5b9a\u91cf\u8bc4\u5206\u3002", "result": "\u6211\u4eec\u5c55\u793a\u4e86\u8be5\u6846\u67b6\u5728\u5404\u79cd\u7814\u7a76\u573a\u666f\u4e2d\u7684\u591a\u529f\u80fd\u6027\uff0c\u5305\u62ec\u5176\u5728\u539f\u5b50\u5206\u8fa8\u7387\u548c\u9ad8\u5149\u8c31\u6570\u636e\u65b9\u9762\u7684\u5e94\u7528\u3001\u96c6\u6210\u5b9e\u65f6\u4eba\u7c7b\u4e13\u5bb6\u6307\u5bfc\u7684\u80fd\u529b\u4ee5\u53ca\u901a\u8fc7\u63d0\u51fa\u6709\u9488\u5bf9\u6027\u7684\u540e\u7eed\u5b9e\u9a8c\u6765\u95ed\u5408\u7814\u7a76\u5faa\u73af\u7684\u80fd\u529b\u3002", "conclusion": "SciLink \u6846\u67b6\u4e3a\u6750\u6599\u7814\u7a76\u4e2d\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b9e\u7528\u7684\u6846\u67b6\uff0c\u5b83\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u6548\u7387\uff0c\u800c\u4e14\u79ef\u6781\u57f9\u517b\u4e86\u6709\u5229\u4e8e\u5076\u7136\u53d1\u73b0\u7684\u73af\u5883\uff0c\u4ece\u800c\u5f25\u5408\u4e86\u81ea\u52a8\u5316\u5b9e\u9a8c\u4e0e\u5f00\u653e\u5f0f\u79d1\u5b66\u63a2\u7d22\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.06774", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.06774", "abs": "https://arxiv.org/abs/2508.06774", "authors": ["Lorenzo Beretta", "Vincent Cohen-Addad", "Rajesh Jayaram", "Erik Waingarten"], "title": "Approximating High-Dimensional Earth Mover's Distance as Fast as Closest Pair", "comment": "FOCS 2025", "summary": "We give a reduction from $(1+\\varepsilon)$-approximate Earth Mover's Distance\n(EMD) to $(1+\\varepsilon)$-approximate Closest Pair (CP). As a consequence, we\nimprove the fastest known approximation algorithm for high-dimensional EMD.\nHere, given $p\\in [1, 2]$ and two sets of $n$ points $X,Y \\subseteq (\\mathbb\nR^d,\\ell_p)$, their EMD is the minimum cost of a perfect matching between $X$\nand $Y$, where the cost of matching two vectors is their $\\ell_p$ distance.\nFurther, CP is the basic problem of finding a pair of points realizing $\\min_{x\n\\in X, y\\in Y} ||x-y||_p$. Our contribution is twofold: we show that if a\n$(1+\\varepsilon)$-approximate CP can be computed in time $n^{2-\\phi}$, then a\n$1+O(\\varepsilon)$ approximation to EMD can be computed in time\n$n^{2-\\Omega(\\phi)}$; plugging in the fastest known algorithm for CP [Alman,\nChan, Williams FOCS'16], we obtain a $(1+\\varepsilon)$-approximation algorithm\nfor EMD running in time $n^{2-\\tilde{\\Omega}(\\varepsilon^{1/3})}$ for\nhigh-dimensional point sets, which improves over the prior fastest running time\nof $n^{2-\\Omega(\\varepsilon^2)}$ [Andoni, Zhang FOCS'23]. Our main technical\ncontribution is a sublinear implementation of the Multiplicative Weights Update\nframework for EMD. Specifically, we demonstrate that the updates can be\nexecuted without ever explicitly computing or storing the weights; instead, we\nexploit the underlying geometric structure to perform the updates implicitly.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u8fd1\u4f3c\u5730\u7403\u642c\u8fd0\u8ddd\u79bb (EMD) \u89c4\u7ea6\u5230\u8fd1\u4f3c\u6700\u8fd1\u70b9\u5bf9 (CP) \u7684\u65b9\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u9ad8\u7ef4 EMD \u7684\u8fd1\u4f3c\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u9ad8\u7ef4 EMD \u7684\u8fd1\u4f3c\u7b97\u6cd5\uff0c\u7279\u522b\u662f\u5229\u7528 CP \u95ee\u9898\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u901a\u8fc7\u5b50\u7ebf\u6027\u5b9e\u73b0\u591a\u6743\u91cd\u66f4\u65b0\u6846\u67b6\uff0c\u5229\u7528\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u9690\u5f0f\u6267\u884c\u66f4\u65b0\uff0c\u65e0\u9700\u663e\u5f0f\u8ba1\u7b97\u6216\u5b58\u50a8\u6743\u91cd\u3002", "result": "\u5f53 $(1+\\varepsilon)$-\u8fd1\u4f3c CP \u53ef\u5728 $n^{2-\\phi}$ \u65f6\u95f4\u5185\u8ba1\u7b97\u65f6\uff0cEMD \u7684 $1+O(\\varepsilon)$ \u8fd1\u4f3c\u53ef\u5728 $n^{2-\\Omega(\\phi)}$ \u65f6\u95f4\u5185\u8ba1\u7b97\u3002\u7ed3\u5408 CP \u7684\u6700\u5feb\u7b97\u6cd5\uff0c\u5b9e\u73b0\u4e86\u9488\u5bf9\u9ad8\u7ef4\u70b9\u96c6\u7684 EMD $(1+\\varepsilon)$ \u8fd1\u4f3c\u7b97\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u4e3a $n^{2-\\tilde{\\Omega}(\\varepsilon^{1/3})}$\uff0c\u4f18\u4e8e\u5148\u524d $n^{2-\\Omega(\\varepsilon^2)}$ \u7684\u8fd0\u884c\u65f6\u95f4\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06 $(1+\\varepsilon)$-\u8fd1\u4f3c\u5730\u7403\u642c\u8fd0\u8ddd\u79bb (EMD) \u89c4\u7ea6\u5230 $(1+\\varepsilon)$-\u8fd1\u4f3c\u6700\u8fd1\u70b9\u5bf9 (CP)\uff0c\u4ece\u800c\u6539\u8fdb\u4e86\u9ad8\u7ef4 EMD \u7684\u6700\u5feb\u8fd1\u4f3c\u7b97\u6cd5\u3002"}}
{"id": "2508.06574", "categories": ["cs.LG", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06574", "abs": "https://arxiv.org/abs/2508.06574", "authors": ["Fatemeh Moradi", "Mehran Tarif", "Mohammadhossein Homaei"], "title": "Semi-Supervised Supply Chain Fraud Detection with Unsupervised Pre-Filtering", "comment": "Six Pages, two Figures and six Tables", "summary": "Detecting fraud in modern supply chains is a growing challenge, driven by the\ncomplexity of global networks and the scarcity of labeled data. Traditional\ndetection methods often struggle with class imbalance and limited supervision,\nreducing their effectiveness in real-world applications. This paper proposes a\nnovel two-phase learning framework to address these challenges. In the first\nphase, the Isolation Forest algorithm performs unsupervised anomaly detection\nto identify potential fraud cases and reduce the volume of data requiring\nfurther analysis. In the second phase, a self-training Support Vector Machine\n(SVM) refines the predictions using both labeled and high-confidence\npseudo-labeled samples, enabling robust semi-supervised learning. The proposed\nmethod is evaluated on the DataCo Smart Supply Chain Dataset, a comprehensive\nreal-world supply chain dataset with fraud indicators. It achieves an F1-score\nof 0.817 while maintaining a false positive rate below 3.0%. These results\ndemonstrate the effectiveness and efficiency of combining unsupervised\npre-filtering with semi-supervised refinement for supply chain fraud detection\nunder real-world constraints, though we acknowledge limitations regarding\nconcept drift and the need for comparison with deep learning approaches.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u4e86\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff08Isolation Forest\uff09\u548c\u534a\u76d1\u7763\u5b66\u4e60\uff08\u81ea\u8bad\u7ec3SVM\uff09\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u7a00\u758f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u73b0\u4ee3\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\u9762\u4e34\u6570\u636e\u7a00\u758f\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u6311\u6218\uff0c\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u6709\u9650\u7684\u76d1\u7763\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u5b66\u4e60\u6846\u67b6\uff1a\u7b2c\u4e00\u9636\u6bb5\u4f7f\u7528Isolation Forest\u7b97\u6cd5\u8fdb\u884c\u65e0\u76d1\u7763\u5f02\u5e38\u68c0\u6d4b\uff0c\u8bc6\u522b\u6f5c\u5728\u6b3a\u8bc8\uff1b\u7b2c\u4e8c\u9636\u6bb5\u4f7f\u7528\u81ea\u8bad\u7ec3\u652f\u6301\u5411\u91cf\u673a\uff08SVM\uff09\u7ed3\u5408\u6807\u8bb0\u548c\u9ad8\u53ef\u4fe1\u5ea6\u4f2a\u6807\u8bb0\u6837\u672c\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\u3002", "result": "\u5728DataCo Smart Supply Chain Dataset\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e860.817\u7684F1\u5206\u6570\uff0c\u5e76\u5c06\u8bef\u62a5\u7387\u63a7\u5236\u57283.0%\u4ee5\u4e0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7ed3\u5408\u65e0\u76d1\u7763\u9884\u8fc7\u6ee4\u548c\u534a\u76d1\u7763\u6539\u8fdb\u7684\u4f9b\u5e94\u94fe\u6b3a\u8bc8\u68c0\u6d4b\u65b9\u6cd5\uff0c\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6709\u6548\u4e14\u9ad8\u6548\u3002"}}
{"id": "2508.07569", "categories": ["cs.MA", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07569", "abs": "https://arxiv.org/abs/2508.07569", "authors": ["Amulya Suravarjhula", "Rashi Chandrashekhar Agrawal", "Sakshi Jayesh Patel", "Rahul Gupta"], "title": "Retrieval-Augmented Multi-Agent System for Rapid Statement of Work Generation", "comment": "7 pages", "summary": "Drafting a Statement of Work (SOW) is a vital part of business and legal\nprojects. It outlines key details like deliverables, timelines,\nresponsibilities, and legal terms. However, creating these documents is often a\nslow and complex process. It usually involves multiple people, takes several\ndays, and leaves room for errors or outdated content. This paper introduces a\nnew AI-driven automation system that makes the entire SOW drafting process\nfaster, easier, and more accurate. Instead of relying completely on humans, the\nsystem uses three intelligent components or 'agents' that each handle a part of\nthe job. One agent writes the first draft, another checks if everything is\nlegally correct, and the third agent formats the document and ensures\neverything is in order. Unlike basic online tools that just fill in templates,\nthis system understands the meaning behind the content and customizes the SOW\nto match the needs of the project. It also checks legal compliance and\nformatting so that users can trust the result. The system was tested using real\nbusiness examples. It was able to create a full SOW in under three minutes,\ncompared to several hours or days using manual methods. It also performed well\nin accuracy and quality, showing that it can reduce legal risks and save a lot\nof time. This solution shows how artificial intelligence can be used to support\nlegal and business professionals by taking care of routine work and helping\nthem focus on more important decisions. It's a step toward making legal\nprocesses smarter, faster, and more reliable.", "AI": {"tldr": "AI\u81ea\u52a8\u5316\u7cfb\u7edf\u901a\u8fc7\u4e09\u4e2a\u667a\u80fd\u4ee3\u7406\uff08\u8d77\u8349\u3001\u6cd5\u5f8b\u5ba1\u67e5\u3001\u683c\u5f0f\u5316\uff09\u9769\u65b0\u4e86SOW\u7684\u8d77\u8349\u8fc7\u7a0b\uff0c\u5b9e\u73b0\u4e86\u5feb\u901f\uff08<3\u5206\u949f\uff09\u3001\u51c6\u786e\u4e14\u5408\u89c4\u7684\u6587\u6863\u751f\u6210\uff0c\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u7684SOW\u8d77\u8349\u8fc7\u7a0b\u8017\u65f6\u3001\u590d\u6742\u4e14\u5bb9\u6613\u51fa\u9519\uff0c\u9700\u8981\u591a\u4eba\u534f\u4f5c\u4e14\u8017\u65f6\u6570\u5929\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u5f15\u5165AI\u81ea\u52a8\u5316\u7cfb\u7edf\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u9ad8SOW\u8d77\u8349\u7684\u901f\u5ea6\u3001\u7b80\u6613\u6027\u548c\u51c6\u786e\u6027\u3002", "method": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7531\u4e09\u4e2a\u667a\u80fd\u4ee3\u7406\u7ec4\u6210\u7684AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\uff0c\u7528\u4e8e\u8d77\u8349SOW\u3002\u4e00\u4e2a\u4ee3\u7406\u8d1f\u8d23\u751f\u6210\u521d\u7a3f\uff0c\u7b2c\u4e8c\u4e2a\u4ee3\u7406\u8fdb\u884c\u6cd5\u5f8b\u5408\u89c4\u6027\u68c0\u67e5\uff0c\u7b2c\u4e09\u4e2a\u4ee3\u7406\u8d1f\u8d23\u683c\u5f0f\u5316\u548c\u6574\u7406\u6587\u6863\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u7406\u89e3\u5185\u5bb9\u542b\u4e49\u5e76\u8fdb\u884c\u5b9a\u5236\u5316\u5904\u7406\uff0c\u800c\u975e\u7b80\u5355\u7684\u6a21\u677f\u586b\u5145\u3002", "result": "\u901a\u8fc7\u5b9e\u9645\u4e1a\u52a1\u6848\u4f8b\u6d4b\u8bd5\uff0c\u8be5\u7cfb\u7edf\u80fd\u5728\u4e0d\u5230\u4e09\u5206\u949f\u7684\u65f6\u95f4\u5185\u5b8c\u6210SOW\u7684\u8d77\u8349\uff0c\u76f8\u6bd4\u4f20\u7edf\u624b\u52a8\u65b9\u6cd5\u53ef\u8282\u7701\u6570\u5c0f\u65f6\u751a\u81f3\u6570\u5929\u3002\u540c\u65f6\uff0c\u5728\u51c6\u786e\u6027\u548c\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u80fd\u591f\u964d\u4f4e\u6cd5\u5f8b\u98ce\u9669\u5e76\u8282\u7701\u5927\u91cf\u65f6\u95f4\u3002", "conclusion": "\u8be5AI\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u7cfb\u7edf\u80fd\u591f\u663e\u8457\u63d0\u9ad8SOW\uff08\u5de5\u4f5c\u8bf4\u660e\u4e66\uff09\u7684\u8d77\u8349\u6548\u7387\u548c\u51c6\u786e\u6027\uff0c\u901a\u8fc7\u4e09\u4e2a\u667a\u80fd\u4ee3\u7406\u534f\u540c\u5de5\u4f5c\uff0c\u81ea\u52a8\u5b8c\u6210\u8d77\u8349\u3001\u6cd5\u5f8b\u5ba1\u67e5\u548c\u683c\u5f0f\u5316\uff0c\u4ece\u800c\u51cf\u5c11\u4e86\u65f6\u95f4\u548c\u6f5c\u5728\u7684\u6cd5\u5f8b\u98ce\u9669\uff0c\u4f7f\u6cd5\u5f8b\u548c\u5546\u4e1a\u4e13\u4e1a\u4eba\u58eb\u80fd\u591f\u4e13\u6ce8\u4e8e\u66f4\u91cd\u8981\u7684\u51b3\u7b56\u3002"}}
{"id": "2508.06669", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.06669", "abs": "https://arxiv.org/abs/2508.06669", "authors": ["Adrien Baut", "Sebastian Kravecz", "Andreas T. Guentner"], "title": "Potassium polytungstate nanoparticles by combustion aerosol technology for benzene sensing", "comment": null, "summary": "Polytungstates are oxygen-linked assemblies of highly oxidized tungsten\npolyhedra, valued for their tunability and stability in diverse applications.\nTraditional synthesis methods (hydrothermal, solvothermal, solid-state) offer\nmaterial variety but are limited in scalability and their ability to yield\nnanostructured materials due to long reaction times and high temperatures.\nHere, we introduce flame aerosol synthesis as a single-step, rapid and dry\nmethod to prepare K$_2$W$_7$O$_{22}$ nanoparticulate powders and coatings.\nThereby, monocrystalline and phase-pure K$_2$W$_7$O$_{22}$ with varying\ncrystal-sizes were obtained by controlling flame temperature, residence time\nand metal ion concentration during particle formation by nucleation,\ncoagulation and sintering. X-ray diffraction and electron microscopy identified\nthe high potassium tolerance of the K$_2$W$_7$O$_{22}$ lattice (K/W ratio up to\n0.6) and phase stability up to 400 $^\\circ$C, before other polytungstates and\nWO$_3$ polymorphs were formed, respectively. Porous films of such\nK$_2$W$_7$O$_{22}$ nanoparticles featured n-type semiconductor behavior that\nwas utilized for the chemoresistive quantification of the air pollutant benzene\ndown to 0.2 parts-per-million at 20% relative humidity. Such sensors were quite\nselective over other compounds (e.g. alcohols, aldehydes, ketones, CO, NH$_3$\nor H$_2$), in particular to chemically similar toluene and xylene (>18).", "AI": {"tldr": "Flame aerosol synthesis offers a scalable and efficient method for producing K2W7O22 nanoparticles for benzene sensing applications.", "motivation": "Traditional synthesis methods for polytungstates are limited in scalability and ability to yield nanostructured materials. This paper introduces flame aerosol synthesis as a faster and more efficient alternative.", "method": "Flame aerosol synthesis was employed as a single-step, rapid, and dry method to prepare K2W7O22 nanoparticulate powders and coatings. Monocrystalline and phase-pure K2W7O22 with varying crystal sizes were obtained by controlling flame temperature, residence time, and metal ion concentration during particle formation, involving nucleation, coagulation, and sintering.", "result": "The prepared K2W7O22 nanoparticles showed n-type semiconductor behavior, enabling the chemoresistive quantification of benzene down to 0.2 parts-per-million at 20% relative humidity. The sensors demonstrated high selectivity towards benzene over other compounds, including similar hydrocarbons like toluene and xylene.", "conclusion": "Flame aerosol synthesis successfully prepared monocrystalline and phase-pure K2W7O22 nanoparticulate powders and coatings, which exhibited n-type semiconductor behavior suitable for chemoresistive quantification of benzene."}}
{"id": "2508.06949", "categories": ["cs.DC", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.06949", "abs": "https://arxiv.org/abs/2508.06949", "authors": ["Arya Tanmay Gupta"], "title": "Convergence Sans Synchronization", "comment": "PhD thesis", "summary": "We currently see a steady rise in the usage and size of multiprocessor\nsystems, and so the community is evermore interested in developing fast\nparallel processing algorithms. However, most algorithms require a\nsynchronization mechanism, which is costly in terms of computational resources\nand time. If an algorithm can be executed in asynchrony, then it can use all\nthe available computation power, and the nodes can execute without being\nscheduled or locked. However, to show that an algorithm guarantees convergence\nin asynchrony, we need to generate the entire global state transition graph and\ncheck for the absence of cycles. This takes time exponential in the size of the\nglobal state space. In this dissertation, we present a theory that explains the\nnecessary and sufficient properties of a multiprocessor algorithm that\nguarantees convergence even without synchronization. We develop algorithms for\nvarious problems that do not require synchronization. Additionally, we show for\nseveral existing algorithms that they can be executed without any\nsynchronization mechanism. A significant theoretical benefit of our work is in\nproving that an algorithm can converge even in asynchrony. Our theory implies\nthat we can make such conclusions about an algorithm, by only showing that the\nlocal state transition graph of a computing node forms a partial order, rather\nthan generating the entire global state space and determining the absence of\ncycles in it. Thus, the complexity of rendering such proofs, formal or social,\nis phenomenally reduced. Experiments show a significant reduction in time taken\nto converge, when we compare the execution time of algorithms in the literature\nversus the algorithms that we design. We get similar results when we run an\nalgorithm, that guarantees convergence in asynchrony, under a scheduler versus\nin asynchrony.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u7406\u8bba\u7b80\u5316\u4e86\u5f02\u6b65\u7b97\u6cd5\u6536\u655b\u6027\u8bc1\u660e\uff0c\u5f00\u53d1\u4e86\u65e0\u9700\u540c\u6b65\u7684\u7b97\u6cd5\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5f02\u6b65\u6267\u884c\u66f4\u9ad8\u6548\u3002", "motivation": "\u73b0\u6709\u7684\u5e76\u884c\u5904\u7406\u7b97\u6cd5\u5927\u591a\u9700\u8981\u540c\u6b65\u673a\u5236\uff0c\u800c\u540c\u6b65\u673a\u5236\u4f1a\u6d88\u8017\u5927\u91cf\u7684\u8ba1\u7b97\u8d44\u6e90\u548c\u65f6\u95f4\u3002\u5982\u679c\u7b97\u6cd5\u80fd\u591f\u5728\u5f02\u6b65\u72b6\u6001\u4e0b\u6267\u884c\uff0c\u5219\u53ef\u4ee5\u5145\u5206\u5229\u7528\u6240\u6709\u8ba1\u7b97\u80fd\u529b\uff0c\u5e76\u4e14\u8282\u70b9\u65e0\u9700\u8c03\u5ea6\u6216\u9501\u5b9a\u5373\u53ef\u6267\u884c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u5957\u7406\u8bba\uff0c\u8bc1\u660e\u4e86\u5c40\u90e8\u72b6\u6001\u8f6c\u79fb\u56fe\u6784\u6210\u504f\u5e8f\u662f\u5f02\u6b65\u7b97\u6cd5\u6536\u655b\u7684\u5145\u8981\u6761\u4ef6\uff0c\u5e76\u57fa\u4e8e\u6b64\u7406\u8bba\u5f00\u53d1\u4e86\u65e0\u9700\u540c\u6b65\u7684\u7b97\u6cd5\uff0c\u540c\u65f6\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\u4ee5\u652f\u6301\u5f02\u6b65\u6267\u884c\u3002", "result": "\u5f00\u53d1\u4e86\u65e0\u9700\u540c\u6b65\u7684\u7b97\u6cd5\uff0c\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\u4ee5\u652f\u6301\u5f02\u6b65\u6267\u884c\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5f02\u6b65\u6267\u884c\u663e\u8457\u51cf\u5c11\u4e86\u6536\u655b\u65f6\u95f4\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u7406\u8bba\uff0c\u80fd\u591f\u663e\u8457\u964d\u4f4e\u8bc1\u660e\u5f02\u6b65\u7b97\u6cd5\u6536\u655b\u6027\u7684\u590d\u6742\u6027\uff0c\u53ea\u9700\u8981\u8bc1\u660e\u5c40\u90e8\u72b6\u6001\u8f6c\u79fb\u56fe\u6784\u6210\u504f\u5e8f\u5373\u53ef\uff0c\u800c\u65e0\u9700\u751f\u6210\u5168\u5c40\u72b6\u6001\u8f6c\u79fb\u56fe\u5e76\u68c0\u67e5\u662f\u5426\u5b58\u5728\u73af\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u8bbe\u8ba1\u4e86\u591a\u79cd\u65e0\u9700\u540c\u6b65\u7684\u7b97\u6cd5\uff0c\u5e76\u6539\u8fdb\u4e86\u73b0\u6709\u7b97\u6cd5\u4ee5\u652f\u6301\u5f02\u6b65\u6267\u884c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5f02\u6b65\u6267\u884c\u5e26\u6765\u7684\u65f6\u95f4\u6548\u7387\u63d0\u5347\u3002"}}
{"id": "2508.06521", "categories": ["cs.RO", "68T40, 93C85, 70E60"], "pdf": "https://arxiv.org/pdf/2508.06521", "abs": "https://arxiv.org/abs/2508.06521", "authors": ["H. Liu", "L. S. Moreu", "T. S. Andersen", "V. V. Puche", "M. Fumagalli"], "title": "Stinger Robot: A Self-Bracing Robotic Platform for Autonomous Drilling in Confined Underground Environments", "comment": "7 pages, submitted", "summary": "The increasing demand for critical raw materials has revitalized interest in\nabandoned underground mines, which pose extreme challenges for conventional\ndrilling machinery due to confined, unstructured, and infrastructure-less\nenvironments. This paper presents the Stinger Robot, a novel compact robotic\nplatform specifically designed for autonomous high-force drilling in such\nsettings. The robot features a mechanically self-locking tri-leg bracing\nmechanism that enables stable anchoring to irregular tunnel surfaces. A key\ninnovation lies in its force-aware, closed-loop control strategy, which enables\nforce interaction with unstructured environments during bracing and drilling.\nImplemented as a finite-state machine in ROS 2, the control policy dynamically\nadapts leg deployment based on real-time contact feedback and load thresholds,\nensuring stability without external supports. We demonstrate, through\nsimulation and preliminary hardware tests, that the Stinger Robot can\nautonomously stabilize and drill in conditions previously inaccessible to\nnowadays mining machines. This work constitutes the first validated robotic\narchitecture to integrate distributed force-bracing and autonomous drilling in\nunderground environments, laying the groundwork for future collaborative mining\noperations using modular robot systems.", "AI": {"tldr": "Stinger Robot\u662f\u4e00\u79cd\u65b0\u578b\u673a\u5668\u4eba\uff0c\u4e13\u4e3a\u5728\u72ed\u7a84\u3001\u975e\u7ed3\u6784\u5316\u3001\u65e0\u57fa\u7840\u8bbe\u65bd\u7684\u5e9f\u5f03\u5730\u4e0b\u77ff\u5c71\u4e2d\u8fdb\u884c\u9ad8\u529b\u94bb\u63a2\u800c\u8bbe\u8ba1\u3002\u5b83\u4f7f\u7528\u4e09\u8db3\u652f\u6491\u673a\u5236\u548c\u529b\u611f\u77e5\u63a7\u5236\u7b56\u7565\uff0c\u5b9e\u73b0\u4e86\u5728\u4e0d\u5e73\u5766\u8868\u9762\u7684\u7a33\u5b9a\u951a\u5b9a\u548c\u81ea\u4e3b\u94bb\u63a2\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u673a\u68b0\u7684\u9650\u5236\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5e9f\u5f03\u5730\u4e0b\u77ff\u5c71\u4e2d\u5bf9\u5173\u952e\u539f\u6750\u6599\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u5e76\u514b\u670d\u4f20\u7edf\u94bb\u63a2\u673a\u68b0\u5728\u8fd9\u4e9b\u5145\u6ee1\u6311\u6218\u7684\u73af\u5883\u4e2d\uff08\u5982\u7a7a\u95f4\u72ed\u7a84\u3001\u7ed3\u6784\u975e\u7ed3\u6784\u5316\u4e14\u65e0\u57fa\u7840\u8bbe\u65bd\uff09\u7684\u5c40\u9650\u6027\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStinger Robot\u7684\u65b0\u578b\u7d27\u51d1\u578b\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u8be5\u5e73\u53f0\u91c7\u7528\u673a\u68b0\u81ea\u9501\u4e09\u8db3\u652f\u6491\u673a\u5236\uff0c\u5e76\u7ed3\u5408\u4e86\u529b\u7684\u611f\u77e5\u548c\u95ed\u73af\u63a7\u5236\u7b56\u7565\uff0c\u5728ROS 2\u4e2d\u5b9e\u73b0\u4e3a\u6709\u9650\u72b6\u6001\u673a\uff0c\u4ee5\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u8fdb\u884c\u7a33\u5b9a\u951a\u5b9a\u548c\u94bb\u63a2\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u521d\u6b65\u786c\u4ef6\u6d4b\u8bd5\u8868\u660e\uff0cStinger Robot\u80fd\u591f\u5728\u76ee\u524d\u91c7\u77ff\u8bbe\u5907\u65e0\u6cd5\u8fdb\u5165\u7684\u6761\u4ef6\u4e0b\u81ea\u4e3b\u7a33\u5b9a\u548c\u94bb\u63a2\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u9a8c\u8bc1\u4e86\u4e00\u79cd\u96c6\u6210\u4e86\u5206\u5e03\u5f0f\u529b\u652f\u6491\u548c\u5730\u4e0b\u81ea\u4e3b\u94bb\u63a2\u7684\u673a\u5668\u4eba\u67b6\u6784\uff0c\u4e3a\u672a\u6765\u6a21\u5757\u5316\u673a\u5668\u4eba\u7cfb\u7edf\u7684\u534f\u540c\u91c7\u77ff\u4f5c\u4e1a\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06612", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.quant-gas", "cond-mat.stat-mech", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.06612", "abs": "https://arxiv.org/abs/2508.06612", "authors": ["Giovanni Cemin", "Markus Schmitt", "Marin Bukov"], "title": "Learning to stabilize nonequilibrium phases of matter with active feedback using partial information", "comment": "10 + 16 pages, 5 + 9 figures", "summary": "We investigate the role of information in active feedback control of quantum\nmany-body systems using reinforcement learning. Active feedback breaks detailed\nbalance, enabling the engineering of steady states and dynamical phases of\nmatter otherwise inaccessible in equilibrium. We train reinforcement learning\nagents using partial state information to prevent entanglement spreading in\n(1+1)-dimensional stabilizer circuits with up to 128 qubits. We find that,\nabove a critical information threshold, learned near-optimal strategies are\nnon-greedy, stochastic, and reduce volume-law entangled steady states to\narea-law scaling. The agents achieve this by placing a series of bottlenecks\nthat induce pyramidal structures in the long-time spatial entanglement\ndistribution, which effectively split the system and reduce the maximum\naccessible entanglement. Crucially, learned strategies are inherently out of\nequilibrium and require real-time active feedback; we find that the learned\nbehavior cannot be replaced by simple human-designed control rules. This work\nestablishes the foundations for classically implemented, information-driven\nindividual control of many interacting quantum degrees of freedom,\ndemonstrating the capabilities of reinforcement learning to stabilize and\nuncover novel critical properties of many-body nonequilibrium steady states.", "AI": {"tldr": "\u5f3a\u5316\u5b66\u4e60\u88ab\u7528\u6765\u63a7\u5236\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\uff0c\u9632\u6b62\u7ea0\u7f20\u6269\u6563\u3002\u5b66\u4e60\u5230\u7684\u7b56\u7565\u662f\u975e\u8d2a\u5a6a\u7684\u3001\u968f\u673a\u7684\uff0c\u5e76\u901a\u8fc7\u521b\u5efa\u74f6\u9888\u6765\u51cf\u5c11\u7ea0\u7f20\u3002\u8fd9\u4e9b\u7b56\u7565\u662f\u5185\u5728\u975e\u5e73\u8861\u7684\uff0c\u65e0\u6cd5\u88ab\u7b80\u5355\u7684\u63a7\u5236\u89c4\u5219\u53d6\u4ee3\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u4fe1\u606f\u5728\u91cf\u5b50\u591a\u4f53\u7cfb\u7edf\u7684\u6709\u6e90\u53cd\u9988\u63a7\u5236\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u53ca\u5229\u7528\u6709\u6e90\u53cd\u9988\u6765\u8bbe\u8ba1\u7a33\u6001\u548c\u7269\u8d28\u52a8\u529b\u5b66\u76f8\u3002", "method": "\u7814\u7a76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u667a\u80fd\u4f53\uff0c\u5229\u7528\u90e8\u5206\u72b6\u6001\u4fe1\u606f\u6765\u9632\u6b62\uff081+1\uff09\u7ef4\u7a33\u5b9a\u5668\u7535\u8def\u4e2d\u9ad8\u8fbe128\u4e2a\u91cf\u5b50\u6bd4\u7279\u7684\u7ea0\u7f20\u6269\u6563\u3002", "result": "\u5728\u4e34\u754c\u4fe1\u606f\u9608\u503c\u4e4b\u4e0a\uff0c\u5b66\u4e60\u5230\u7684\u8fd1\u4e4e\u6700\u4f18\u7684\u7b56\u7565\u662f\u975e\u8d2a\u5a6a\u7684\u3001\u968f\u673a\u7684\uff0c\u5e76\u5c06\u4f53\u79ef\u5f8b\u7ea0\u7f20\u7a33\u6001\u964d\u4f4e\u5230\u9762\u79ef\u5f8b\u5c3a\u5ea6\u3002\u667a\u80fd\u4f53\u901a\u8fc7\u653e\u7f6e\u4e00\u7cfb\u5217\u74f6\u9888\u6765\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\uff0c\u8fd9\u4e9b\u74f6\u9888\u5728\u957f\u65f6\u95f4\u7a7a\u95f4\u7ea0\u7f20\u5206\u5e03\u4e2d\u8bf1\u5bfc\u91d1\u5b57\u5854\u7ed3\u6784\uff0c\u4ece\u800c\u6709\u6548\u5730\u5206\u79bb\u7cfb\u7edf\u5e76\u964d\u4f4e\u53ef\u8bbf\u95ee\u7684\u6700\u5927\u7ea0\u7f20\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7ecf\u5178\u63a7\u5236\u7684\u91cf\u5b50\u8bb8\u591a\u76f8\u4e92\u4f5c\u7528\u7684\u81ea\u7531\u5ea6\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5c55\u793a\u4e86\u5f3a\u5316\u5b66\u4e60\u5728\u7a33\u5b9a\u548c\u63ed\u793a\u8bb8\u591a\u4f53\u975e\u5e73\u8861\u7a33\u6001\u7684\u65b0\u7684\u4e34\u754c\u6027\u8d28\u65b9\u9762\u7684\u80fd\u529b\u3002"}}
{"id": "2508.07227", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07227", "abs": "https://arxiv.org/abs/2508.07227", "authors": ["Siyuan He", "Zhantong Zhu", "Yandong He", "Tianyu Jia"], "title": "LP-Spec: Leveraging LPDDR PIM for Efficient LLM Mobile Speculative Inference with Architecture-Dataflow Co-Optimization", "comment": "Accepted by ICCAD'2025", "summary": "LLM inference on mobile devices faces extraneous challenges due to limited\nmemory bandwidth and computational resources. To address these issues,\nspeculative inference and processing-in-memory (PIM) techniques have been\nexplored at the algorithmic and hardware levels. However, speculative inference\nresults in more compute-intensive GEMM operations, creating new design\ntrade-offs for existing GEMV-accelerated PIM architectures. Furthermore, there\nexists a significant amount of redundant draft tokens in tree-based speculative\ninference, necessitating efficient token management schemes to minimize energy\nconsumption. In this work, we present LP-Spec, an architecture-dataflow\nco-design leveraging hybrid LPDDR5 performance-enhanced PIM architecture with\ndraft token pruning and dynamic workload scheduling to accelerate LLM\nspeculative inference. A near-data memory controller is proposed to enable data\nreallocation between DRAM and PIM banks. Furthermore, a data allocation unit\nbased on the hardware-aware draft token pruner is developed to minimize energy\nconsumption and fully exploit parallel execution opportunities. Compared to\nend-to-end LLM inference on other mobile solutions such as mobile NPUs or\nGEMV-accelerated PIMs, our LP-Spec achieves 13.21x, 7.56x, and 99.87x\nimprovements in performance, energy efficiency, and energy-delay-product (EDP).\nCompared with prior AttAcc PIM and RTX 3090 GPU, LP-Spec can obtain 12.83x and\n415.31x EDP reduction benefits.", "AI": {"tldr": "LP-Spec\u901a\u8fc7PIM\u3001\u8349\u7a3f\u6807\u8bb0\u4fee\u526a\u548c\u52a8\u6001\u8c03\u5ea6\u4f18\u5316LLM\u63a8\u65ad\uff0c\u5927\u5e45\u63d0\u5347\u6027\u80fd\u548c\u80fd\u6548\uff0c\u5728\u79fb\u52a8\u7aef\u548c\u9ad8\u7aefGPU\u4e0a\u5747\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684LLM\u63a8\u65ad\u9762\u4e34\u5185\u5b58\u5e26\u5bbd\u548c\u8ba1\u7b97\u8d44\u6e90\u6709\u9650\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u63a8\u65ad\u6280\u672f\uff08\u5982\u63a8\u6d4b\u63a8\u65ad\u548cPIM\uff09\u5b58\u5728\u4e00\u4e9b\u95ee\u9898\uff0c\u4f8b\u5982\u63a8\u6d4b\u63a8\u65ad\u4f1a\u589e\u52a0GEMM\u8fd0\u7b97\u91cf\uff0c\u7ed9\u73b0\u6709\u7684GEMV\u52a0\u901fPIM\u67b6\u6784\u5e26\u6765\u65b0\u7684\u8bbe\u8ba1\u6743\u8861\uff1b\u800c\u57fa\u4e8e\u6811\u7684\u63a8\u6d4b\u63a8\u65ad\u4f1a\u4ea7\u751f\u5927\u91cf\u5197\u4f59\u7684\u8349\u7a3f\u6807\u8bb0\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6807\u8bb0\u7ba1\u7406\u65b9\u6848\u6765\u964d\u4f4e\u80fd\u8017\u3002", "method": "LP-Spec\u91c7\u7528\u4e86\u4e00\u79cd\u67b6\u6784-\u6570\u636e\u6d41\u534f\u540c\u8bbe\u8ba1\u7684\u7b56\u7565\uff0c\u96c6\u6210\u4e86\u9ad8\u6027\u80fdPIM\u67b6\u6784\u3001\u8349\u7a3f\u6807\u8bb0\u4fee\u526a\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\u3002\u5177\u4f53\u63aa\u65bd\u5305\u62ec\uff1a\u63d0\u51fa\u4e86\u4e00\u79cd\u8fd1\u6570\u636e\u5185\u5b58\u63a7\u5236\u5668\uff0c\u7528\u4e8eDRAM\u548cPIM\u94f6\u884c\u4e4b\u95f4\u7684\u6570\u636e\u91cd\u65b0\u5206\u914d\uff1b\u5f00\u53d1\u4e86\u4e00\u4e2a\u57fa\u4e8e\u786c\u4ef6\u611f\u77e5\u8349\u7a3f\u6807\u8bb0\u4fee\u526a\u5668\u7684\u6570\u636e\u5206\u914d\u5355\u5143\uff0c\u4ee5\u6700\u5c0f\u5316\u80fd\u8017\u5e76\u5145\u5206\u5229\u7528\u5e76\u884c\u6267\u884c\u673a\u4f1a\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u79fb\u52a8\u89e3\u51b3\u65b9\u6848\uff08\u5982\u79fb\u52a8NPU\u6216GEMV\u52a0\u901fPIM\uff09\u76f8\u6bd4\uff0cLP-Spec\u5728\u6027\u80fd\u3001\u80fd\u6548\u548cEDP\u65b9\u9762\u5206\u522b\u5b9e\u73b0\u4e8613.21\u500d\u30017.56\u500d\u548c99.87\u500d\u7684\u63d0\u5347\u3002\u4e0e\u4e4b\u524d\u7684AttAcc PIM\u548cRTX 3090 GPU\u76f8\u6bd4\uff0cLP-Spec\u5728EDP\u65b9\u9762\u5206\u522b\u964d\u4f4e\u4e8612.83\u500d\u548c415.31\u500d\u3002", "conclusion": "LP-Spec\u901a\u8fc7\u7ed3\u5408\u6df7\u5408LPDDR5 PIM\u67b6\u6784\u3001\u8349\u7a3f\u6807\u8bb0\u4fee\u526a\u548c\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u8c03\u5ea6\uff0c\u5728LLM\u63a8\u65ad\u65b9\u9762\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u80fd\u6548\u6539\u8fdb\uff0c\u4e0e\u73b0\u6709\u79fb\u52a8\u89e3\u51b3\u65b9\u6848\u548c\u9ad8\u7aefGPU\u76f8\u6bd4\uff0c\u5176EDP\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002"}}
{"id": "2508.07852", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07852", "abs": "https://arxiv.org/abs/2508.07852", "authors": ["Rui Su", "Honghao Dong", "Haojie Jin", "Yisong Chen", "Guoping Wang", "Sheng Li"], "title": "Vertex Features for Neural Global Illumination", "comment": "Accepted by ACM SIGGRAPH Asia'2025", "summary": "Recent research on learnable neural representations has been widely adopted\nin the field of 3D scene reconstruction and neural rendering applications.\nHowever, traditional feature grid representations often suffer from substantial\nmemory footprint, posing a significant bottleneck for modern parallel computing\nhardware. In this paper, we present neural vertex features, a generalized\nformulation of learnable representation for neural rendering tasks involving\nexplicit mesh surfaces. Instead of uniformly distributing neural features\nthroughout 3D space, our method stores learnable features directly at mesh\nvertices, leveraging the underlying geometry as a compact and structured\nrepresentation for neural processing. This not only optimizes memory\nefficiency, but also improves feature representation by aligning compactly with\nthe surface using task-specific geometric priors. We validate our neural\nrepresentation across diverse neural rendering tasks, with a specific emphasis\non neural radiosity. Experimental results demonstrate that our method reduces\nmemory consumption to only one-fifth (or even less) of grid-based\nrepresentations, while maintaining comparable rendering quality and lowering\ninference overhead.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u6e32\u67d3\u6280\u672f\uff0c\u901a\u8fc7\u5c06\u7279\u5f81\u5b58\u50a8\u5728\u7f51\u683c\u9876\u70b9\u4e0a\u800c\u4e0d\u662f\u6574\u4e2a\u7a7a\u95f4\u4e2d\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u5185\u5b58\u4f7f\u7528\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u8d28\u91cf\u7684\u6e32\u67d3\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u7279\u5f81\u7f51\u683c\u8868\u793a\u901a\u5e38\u5177\u6709\u5f88\u5927\u7684\u5185\u5b58\u5360\u7528\uff0c\u5bf9\u73b0\u4ee3\u5e76\u884c\u8ba1\u7b97\u786c\u4ef6\u9020\u6210\u663e\u8457\u74f6\u9888\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u79f0\u4e3a\u795e\u7ecf\u9876\u70b9\u7279\u5f81\u7684\u901a\u7528\u516c\u5f0f\uff0c\u7528\u4e8e\u5904\u7406\u6d89\u53ca\u663e\u5f0f\u7f51\u683c\u8868\u9762\u7684\u795e\u7ecf\u6e32\u67d3\u4efb\u52a1\u3002\u8be5\u65b9\u6cd5\u5c06\u53ef\u5b66\u4e60\u7684\u7279\u5f81\u76f4\u63a5\u5b58\u50a8\u5728\u7f51\u683c\u9876\u70b9\u4e0a\uff0c\u800c\u4e0d\u662f\u5728\u6574\u4e2a\u4e09\u7ef4\u7a7a\u95f4\u4e2d\u5747\u5300\u5206\u5e03\u795e\u7ecf\u7279\u5f81\uff0c\u5229\u7528\u5e95\u5c42\u51e0\u4f55\u7ed3\u6784\u4f5c\u4e3a\u795e\u7ecf\u5904\u7406\u7684\u7d27\u51d1\u4e14\u7ed3\u6784\u5316\u7684\u8868\u793a\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u4e8e\u7f51\u683c\u7684\u8868\u793a\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5c06\u5185\u5b58\u6d88\u8017\u964d\u4f4e\u5230\u539f\u6765\u7684\u4e94\u5206\u4e4b\u4e00\uff08\u6216\u66f4\u5c11\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u6e32\u67d3\u8d28\u91cf\u5e76\u964d\u4f4e\u4e86\u63a8\u7406\u5f00\u9500\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u53ef\u6bd4\u7684\u6e32\u67d3\u8d28\u91cf\u548c\u964d\u4f4e\u63a8\u7406\u5f00\u9500\u7684\u540c\u65f6\uff0c\u5c06\u5185\u5b58\u6d88\u8017\u964d\u4f4e\u5230\u57fa\u4e8e\u7f51\u683c\u7684\u8868\u793a\u7684\u4e94\u5206\u4e4b\u4e00\uff08\u751a\u81f3\u66f4\u4f4e\uff09\u3002"}}
{"id": "2508.06524", "categories": ["cs.CL", "cs.AI", "cs.CY", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06524", "abs": "https://arxiv.org/abs/2508.06524", "authors": ["Lei Jiang", "Fan Chen"], "title": "CarbonScaling: Extending Neural Scaling Laws for Carbon Footprint in Large Language Models", "comment": "8 pages", "summary": "Neural scaling laws have driven the development of increasingly large\nlanguage models (LLMs) by linking accuracy improvements to growth in parameter\ncount, dataset size, and compute. However, these laws overlook the carbon\nemissions that scale exponentially with LLM size. This paper presents\n\\textit{CarbonScaling}, an analytical framework that extends neural scaling\nlaws to incorporate both operational and embodied carbon in LLM training. By\nintegrating models for neural scaling, GPU hardware evolution, parallelism\noptimization, and carbon estimation, \\textit{CarbonScaling} quantitatively\nconnects model accuracy to carbon footprint. Results show that while a\npower-law relationship between accuracy and carbon holds, real-world\ninefficiencies significantly increase the scaling factor. Hardware technology\nscaling reduces carbon emissions for small to mid-sized models, but offers\ndiminishing returns for extremely large LLMs due to communication overhead and\nunderutilized GPUs. Training optimizations-especially aggressive critical batch\nsize scaling-help alleviate this inefficiency. \\textit{CarbonScaling} offers\nkey insights for training more sustainable and carbon-efficient LLMs.", "AI": {"tldr": "LLM\u8bad\u7ec3\u7684\u78b3\u6392\u653e\u4e0e\u6a21\u578b\u89c4\u6a21\u3001\u6570\u636e\u548c\u8ba1\u7b97\u91cf\u6210\u6307\u6570\u589e\u957f\uff0c\u73b0\u6709\u7f29\u653e\u5b9a\u5f8b\u5ffd\u7565\u4e86\u8fd9\u4e00\u70b9\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86CarbonScaling\u6846\u67b6\uff0c\u5c06LLM\u7684\u51c6\u786e\u6027\u4e0e\u5176\u78b3\u8db3\u8ff9\u8054\u7cfb\u8d77\u6765\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u51c6\u786e\u6027\u548c\u78b3\u6392\u653e\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u4f46\u5b9e\u9645\u6548\u7387\u4f4e\u4e0b\u4f1a\u663e\u8457\u589e\u52a0\u7f29\u653e\u56e0\u5b50\u3002\u786c\u4ef6\u5347\u7ea7\u548c\u8bad\u7ec3\u4f18\u5316\u53ef\u4ee5\u5e2e\u52a9\u51cf\u5c11\u78b3\u6392\u653e\uff0c\u4f46\u5bf9\u4e8e\u8d85\u5927\u89c4\u6a21LLM\uff0c\u6536\u76ca\u4f1a\u9012\u51cf\u3002", "motivation": "\u73b0\u6709\u795e\u7ecf\u7f29\u653e\u5b9a\u5f8b\u5ffd\u7565\u4e86\u4e0eLLM\u89c4\u6a21\u6210\u6307\u6570\u589e\u957f\u7684\u78b3\u6392\u653e\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u4e00\u4e2a\u8003\u8651\u78b3\u6392\u653e\u7684\u5206\u6790\u6846\u67b6\u3002", "method": "CarbonScaling\u6846\u67b6\u901a\u8fc7\u6574\u5408\u795e\u7ecf\u7f29\u653e\u6a21\u578b\u3001GPU\u786c\u4ef6\u6f14\u8fdb\u3001\u5e76\u884c\u4f18\u5316\u548c\u78b3\u6392\u653e\u4f30\u7b97\uff0c\u91cf\u5316\u4e86\u6a21\u578b\u51c6\u786e\u6027\u4e0e\u78b3\u8db3\u8ff9\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u51c6\u786e\u6027\u548c\u78b3\u6392\u653e\u4e4b\u95f4\u5b58\u5728\u5e42\u5f8b\u5173\u7cfb\uff0c\u4f46\u5b9e\u9645\u6548\u7387\u4f4e\u4e0b\u4f1a\u663e\u8457\u589e\u52a0\u7f29\u653e\u56e0\u5b50\u3002\u786c\u4ef6\u6280\u672f\u6269\u5c55\u53ef\u4ee5\u51cf\u5c11\u4e2d\u5c0f\u578b\u6a21\u578b\u7684\u78b3\u6392\u653e\uff0c\u4f46\u5bf9\u4e8e\u8d85\u5927\u89c4\u6a21LLM\uff0c\u7531\u4e8e\u901a\u4fe1\u5f00\u9500\u548cGPU\u5229\u7528\u7387\u4e0d\u8db3\uff0c\u6536\u76ca\u9012\u51cf\u3002\u8bad\u7ec3\u4f18\u5316\uff08\u5c24\u5176\u662f\u5173\u952e\u6279\u91cf\u5927\u5c0f\u6269\u5c55\uff09\u6709\u52a9\u4e8e\u7f13\u89e3\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86CarbonScaling\u5206\u6790\u6846\u67b6\uff0c\u5c06LLM\u7684\u78b3\u6392\u653e\u4e0e\u6a21\u578b\u51c6\u786e\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u4e3a\u8bad\u7ec3\u66f4\u53ef\u6301\u7eed\u3001\u78b3\u6548\u7387\u66f4\u9ad8\u7684LLM\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2508.06619", "categories": ["cs.GT", "cs.MA", "cs.SI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06619", "abs": "https://arxiv.org/abs/2508.06619", "authors": ["Kiran Rokade", "Adit Jain", "Francesca Parise", "Vikram Krishnamurthy", "Eva Tardos"], "title": "Asymmetric Network Games: $\u03b1$-Potential Function and Learning", "comment": null, "summary": "In a network game, players interact over a network and the utility of each\nplayer depends on his own action and on an aggregate of his neighbours'\nactions. Many real world networks of interest are asymmetric and involve a\nlarge number of heterogeneous players. This paper analyzes static network games\nusing the framework of $\\alpha$-potential games. Under mild assumptions on the\naction sets (compact intervals) and the utility functions (twice continuously\ndifferentiable) of the players, we derive an expression for an inexact\npotential function of the game, called the $\\alpha$-potential function. Using\nsuch a function, we show that modified versions of the sequential best-response\nalgorithm and the simultaneous gradient play algorithm achieve convergence of\nplayers' actions to a $2\\alpha$-Nash equilibrium. For linear-quadratic network\ngames, we show that $\\alpha$ depends on the maximum asymmetry in the network\nand is well-behaved for a wide range of networks of practical interest.\nFurther, we derive bounds on the social welfare of the $\\alpha$-Nash\nequilibrium corresponding to the maximum of the $\\alpha$-potential function,\nunder suitable assumptions. We numerically illustrate the convergence of the\nproposed algorithms and properties of the learned $2\\alpha$-Nash equilibria.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5206\u6790\u4e86\u7f51\u7edc\u535a\u5f08\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u4ee5\u6536\u655b\u5230 2\u03b1-Nash \u5747\u8861\u7684\u7b97\u6cd5\u3002", "motivation": "\u5206\u6790\u5177\u6709\u5927\u91cf\u5f02\u6784\u73a9\u5bb6\u548c\u975e\u5bf9\u79f0\u6027\u7684\u7f51\u7edc\u535a\u5f08\u3002", "method": "\u4f7f\u7528 \u03b1-potential \u6846\u67b6\u5206\u6790\u9759\u6001\u7f51\u7edc\u535a\u5f08\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u7684\u987a\u5e8f\u6700\u4f73\u54cd\u5e94\u7b97\u6cd5\u548c\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u4ee5\u8fbe\u5230 2\u03b1-Nash \u5747\u8861\u3002", "result": "\u63a8\u5bfc\u51fa \u03b1-potential \u51fd\u6570\uff0c\u5e76\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u6536\u655b\u5230 2\u03b1-Nash \u5747\u8861\u3002\u5bf9\u4e8e\u7ebf\u6027\u4e8c\u6b21\u7f51\u7edc\u535a\u5f08\uff0c\u63a8\u5bfc\u4e86 \u03b1 \u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u754c\u5b9a\u4e86\u5176\u8303\u56f4\u3002\u6b64\u5916\uff0c\u8fd8\u63a8\u5bfc\u4e86\u5728\u67d0\u4e9b\u5047\u8bbe\u4e0b\u7684\u793e\u4f1a\u798f\u5229\u754c\u9650\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7f51\u7edc\u535a\u5f08\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u6790\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u548c\u5206\u6790\u5747\u8861\u7684\u7b97\u6cd5\u3002\u5bf9\u4e8e\u7ebf\u6027\u4e8c\u6b21\u7f51\u7edc\u535a\u5f08\uff0c\u7814\u7a76\u63a8\u5bfc\u4e86 \u03b1 \u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u5bf9 \u03b1 \u8fdb\u884c\u4e86\u754c\u5b9a\uff0c\u8bc1\u660e\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d \u03b1 \u662f\u826f\u5b9a\u4e49\u7684\u3002"}}
{"id": "2508.07464", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.07464", "abs": "https://arxiv.org/abs/2508.07464", "authors": ["Yang Wan", "Benjamin E. Grossman-Ponemona", "Haneesh Kesari"], "title": "Determining the acceleration field of a rigid body using three accelerometers and one gyroscope, with applications in mild traumatic brain injury", "comment": null, "summary": "Mild traumatic brain injury (mTBI) often results from violent head motion or\nimpact. Most prevention strategies explicitly or implicitly rely on motion- or\ndeformation-based injury criteria, both of which require accurate measurements\nof head motion. We present an algorithm for reconstructing the full\nacceleration field of a rigid body from measurements obtained by three\ntri-axial accelerometers and one tri-axial gyroscope. Unlike traditional\ngyroscope-based methods, which require numerically differentiating noisy\nangular velocity data, or gyroscope-free methods, which may impose restrictive\nsensor placement or involve nonlinear optimization, the proposed algorithm\nrecovers angular acceleration and translational acceleration by solving a set\nof linear equations derived from rigid body kinematics. In the proposed method,\nthe only constraint on sensor placement is that the accelerometers must be\nnon-collinear. We validated the algorithm in controlled soccer heading\nexperiments, demonstrating accurate prediction of accelerations at unsensed\nlocations across trials. The proposed algorithm provides a robust, flexible,\nand efficient tool for reconstructing rigid body motion, with direct\napplications in contact sports, robotics, and biomechanical injury prediction.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u52a0\u901f\u5ea6\u8ba1\u548c\u9640\u87ba\u4eea\u6570\u636e\u91cd\u5efa\u521a\u4f53\u8fd0\u52a8\u7684\u7b97\u6cd5\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u7f3a\u70b9\uff0c\u5e76\u5728\u8db3\u7403\u5934\u7403\u5b9e\u9a8c\u4e2d\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u8f7b\u5ea6\u521b\u4f24\u6027\u8111\u635f\u4f24\uff08mTBI\uff09\u7684\u9884\u9632\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u7cbe\u786e\u6d4b\u91cf\u5934\u90e8\u8fd0\u52a8\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u5927\u591a\u6570\u9884\u9632\u7b56\u7565\u90fd\u4f9d\u8d56\u4e8e\u8fd0\u52a8\u6216\u53d8\u5f62\u635f\u4f24\u6807\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u4e09\u4e2a\u4e09\u8f74\u52a0\u901f\u5ea6\u8ba1\u548c\u4e00\u4e2a\u4e09\u8f74\u9640\u87ba\u4eea\u7684\u6d4b\u91cf\u6570\u636e\u4e2d\u91cd\u5efa\u521a\u4f53\u5b8c\u6574\u52a0\u901f\u5ea6\u573a\u7684\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u6c42\u89e3\u6e90\u81ea\u521a\u4f53\u8fd0\u52a8\u5b66\u7684\u65b9\u7a0b\u7ec4\u6765\u6062\u590d\u89d2\u52a0\u901f\u5ea6\u548c\u5e73\u884c\u52a0\u901f\u5ea6\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "result": "\u5728\u63a7\u5236\u4e0b\u7684\u8db3\u7403\u5934\u7403\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u7b97\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5404\u8bd5\u9a8c\u4e2d\u51c6\u786e\u9884\u6d4b\u975e\u4f20\u611f\u4f4d\u7f6e\u52a0\u901f\u5ea6\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4e3a\u521a\u4f53\u8fd0\u52a8\u91cd\u5efa\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u7075\u6d3b\u4e14\u9ad8\u6548\u7684\u5de5\u5177\uff0c\u53ef\u76f4\u63a5\u5e94\u7528\u4e8e\u63a5\u89e6\u6027\u8fd0\u52a8\u3001\u673a\u5668\u4eba\u6280\u672f\u548c\u751f\u7269\u529b\u5b66\u635f\u4f24\u9884\u6d4b\u3002"}}
{"id": "2508.06841", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.06841", "abs": "https://arxiv.org/abs/2508.06841", "authors": ["Yiwei Li", "Zhihua Allen-Zhao", "Yuncheng Xu", "Sanyang Liu"], "title": "Memory Enhanced Fractional-Order Dung Beetle Optimization for Photovoltaic Parameter Identification", "comment": null, "summary": "Accurate parameter identification in photovoltaic (PV) models is crucial for\nperformance evaluation but remains challenging due to their nonlinear,\nmultimodal, and high-dimensional nature. Although the Dung Beetle Optimization\n(DBO) algorithm has shown potential in addressing such problems, it often\nsuffers from premature convergence. To overcome these issues, this paper\nproposes a Memory Enhanced Fractional-Order Dung Beetle Optimization (MFO-DBO)\nalgorithm that integrates three coordinated strategies. Firstly,\nfractional-order (FO) calculus introduces memory into the search process,\nenhancing convergence stability and solution quality. Secondly, a\nfractional-order logistic chaotic map improves population diversity during\ninitialization. Thirdly, a chaotic perturbation mechanism helps elite solutions\nescape local optima. Numerical results on the CEC2017 benchmark suite and the\nPV parameter identification problem demonstrate that MFO-DBO consistently\noutperforms advanced DBO variants, CEC competition winners, FO-based\noptimizers, enhanced classical algorithms, and recent metaheuristics in terms\nof accuracy, robustness, convergence speed, while also maintaining an excellent\nbalance between exploration and exploitation compared to the standard DBO\nalgorithm.", "AI": {"tldr": "MFO-DBO\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u5206\u6570\u9636\u6f14\u7b97\u548c\u6df7\u6c8c\u6620\u5c04\u6765\u589e\u5f3aDBO\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u5149\u4f0f\u6a21\u578b\u53c2\u6570\u8bc6\u522b\u4e2d\u7684\u6536\u655b\u548c\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5728\u591a\u9879\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5149\u4f0f\uff08PV\uff09\u6a21\u578b\u53c2\u6570\u8bc6\u522b\u7684\u6311\u6218\uff0c\u5373\u5176\u975e\u7ebf\u6027\u3001\u591a\u6a21\u6001\u548c\u9ad8\u7ef4\u7279\u6027\uff0c\u4ee5\u53caDBO\u7b97\u6cd5\u5bb9\u6613\u8fc7\u65e9\u6536\u655b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u8bb0\u5fc6\u589e\u5f3a\u3001\u5206\u6570\u9636\u903b\u8f91\u6df7\u6c8c\u6620\u5c04\u548c\u6df7\u6c8c\u6270\u52a8\u673a\u5236\u7684MFO-DBO\u7b97\u6cd5\u3002", "result": "MFO-DBO\u7b97\u6cd5\u5728\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u6536\u655b\u901f\u5ea6\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u5148\u8fdb\u7684DBO\u53d8\u4f53\u3001CEC\u7ade\u8d5b\u83b7\u80dc\u8005\u3001\u57fa\u4e8eFO\u7684\u4f18\u5316\u5668\u3001\u6539\u8fdb\u7684\u7ecf\u5178\u7b97\u6cd5\u548c\u8fd1\u671f\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "conclusion": "MFO-DBO\u7b97\u6cd5\u5728PV\u53c2\u6570\u8bc6\u522b\u95ee\u9898\u548cCEC2017\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u7b97\u6cd5\uff0c\u5177\u6709\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u9c81\u68d2\u6027\u3001\u6536\u655b\u901f\u5ea6\uff0c\u5e76\u80fd\u5728\u63a2\u7d22\u548c\u5229\u7528\u4e4b\u95f4\u4fdd\u6301\u826f\u597d\u7684\u5e73\u8861\u3002"}}
{"id": "2508.07201", "categories": ["cs.SI", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07201", "abs": "https://arxiv.org/abs/2508.07201", "authors": ["Chaoqun Cui", "Caiyan Jia"], "title": "Propagation Tree Is Not Deep: Adaptive Graph Contrastive Learning Approach for Rumor Detection", "comment": "This paper is accepted by AAAI2024", "summary": "Rumor detection on social media has become increasingly important. Most\nexisting graph-based models presume rumor propagation trees (RPTs) have deep\nstructures and learn sequential stance features along branches. However,\nthrough statistical analysis on real-world datasets, we find RPTs exhibit wide\nstructures, with most nodes being shallow 1-level replies. To focus learning on\nintensive substructures, we propose Rumor Adaptive Graph Contrastive Learning\n(RAGCL) method with adaptive view augmentation guided by node centralities. We\nsummarize three principles for RPT augmentation: 1) exempt root nodes, 2)\nretain deep reply nodes, 3) preserve lower-level nodes in deep sections. We\nemploy node dropping, attribute masking and edge dropping with probabilities\nfrom centrality-based importance scores to generate views. A graph contrastive\nobjective then learns robust rumor representations. Extensive experiments on\nfour benchmark datasets demonstrate RAGCL outperforms state-of-the-art methods.\nOur work reveals the wide-structure nature of RPTs and contributes an effective\ngraph contrastive learning approach tailored for rumor detection through\nprincipled adaptive augmentation. The proposed principles and augmentation\ntechniques can potentially benefit other applications involving tree-structured\ngraphs.", "AI": {"tldr": "Rumor detection models often assume deep tree structures, but real-world data shows wide structures. We propose RAGCL, a graph contrastive learning method that adapts to these wide structures using centrality-guided augmentation (keeping deep nodes, dropping shallow ones). RAGCL outperforms existing methods.", "motivation": "Most existing graph-based models for rumor detection presume rumor propagation trees (RPTs) have deep structures and learn sequential stance features along branches. However, statistical analysis on real-world datasets indicates that RPTs exhibit wide structures, with most nodes being shallow 1-level replies. This necessitates a focus on learning from intensive substructures.", "method": "We propose Rumor Adaptive Graph Contrastive Learning (RAGCL) with adaptive view augmentation guided by node centralities. Three principles for RPT augmentation are: 1) exempt root nodes, 2) retain deep reply nodes, and 3) preserve lower-level nodes in deep sections. Node dropping, attribute masking, and edge dropping with probabilities derived from centrality-based importance scores are used to generate views. A graph contrastive objective is then employed to learn robust rumor representations.", "result": "RAGCL outperforms state-of-the-art methods in extensive experiments on four benchmark datasets, demonstrating its effectiveness in rumor detection by leveraging the wide-structure nature of RPTs through adaptive graph contrastive learning.", "conclusion": "RAGCL in extensive experiments on four benchmark datasets demonstrates superior performance compared to state-of-the-art methods. The study reveals the wide-structure nature of RPTs and contributes an effective graph contrastive learning approach tailored for rumor detection through principled adaptive augmentation. The proposed principles and augmentation techniques have potential benefits for other applications involving tree-structured graphs."}}
{"id": "2508.06515", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06515", "abs": "https://arxiv.org/abs/2508.06515", "authors": ["Minh Duc Chu", "Kshitij Pawar", "Zihao He", "Roxanna Sharifi", "Ross Sonnenblick", "Magdalayna Curry", "Laura D'Adamo", "Lindsay Young", "Stuart B Murray", "Kristina Lerman"], "title": "BigTokDetect: A Clinically-Informed Vision-Language Model Framework for Detecting Pro-Bigorexia Videos on TikTok", "comment": null, "summary": "Social media platforms increasingly struggle to detect harmful content that\npromotes muscle dysmorphic behaviors, particularly pro-bigorexia content that\ndisproportionately affects adolescent males. Unlike traditional eating disorder\ndetection focused on the \"thin ideal,\" pro-bigorexia material masquerades as\nlegitimate fitness content through complex multimodal combinations of visual\ndisplays, coded language, and motivational messaging that evade text-based\ndetection systems. We address this challenge by developing BigTokDetect, a\nclinically-informed detection framework for identifying pro-bigorexia content\non TikTok. We introduce BigTok, the first expert-annotated multimodal dataset\nof over 2,200 TikTok videos labeled by clinical psychologists and psychiatrists\nacross five primary categories spanning body image, nutrition, exercise,\nsupplements, and masculinity. Through a comprehensive evaluation of\nstate-of-the-art vision language models, we achieve 0.829% accuracy on primary\ncategory classification and 0.690% on subcategory detection via domain-specific\nfinetuning. Our ablation studies demonstrate that multimodal fusion improves\nperformance by 5-10% over text-only approaches, with video features providing\nthe most discriminative signals. These findings establish new benchmarks for\nmultimodal harmful content detection and provide both the computational tools\nand methodological framework needed for scalable content moderation in\nspecialized mental health domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86BigTokDetect\u6846\u67b6\uff0c\u4ee5\u89e3\u51b3TikTok\u4e0a\u8bc6\u522b\u201c\u5927\u5757\u5934\u764c\u201d\u5185\u5bb9\u7684\u95ee\u9898\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b2200\u591a\u4e2aTikTok\u89c6\u9891\u7684\u5927\u578b\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08BigTok\uff09\uff0c\u5e76\u901a\u8fc7\u5fae\u8c03\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\u3002\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u4fe1\u606f\uff08\u5c24\u5176\u662f\u89c6\u9891\u5185\u5bb9\uff09\u5bf9\u4e8e\u6709\u6548\u68c0\u6d4b\u6b64\u7c7b\u6709\u5bb3\u5185\u5bb9\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u5185\u5bb9\u5ba1\u6838\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u5de5\u5177\u3002", "motivation": "\u793e\u4ea4\u5a92\u4f53\u5e73\u53f0\u5728\u68c0\u6d4b\u6709\u5bb3\u5185\u5bb9\uff08\u5c24\u5176\u662f\u4fc3\u8fdb\u808c\u8089\u7578\u5f62\u884c\u4e3a\u548c\u201c\u5927\u5757\u5934\u764c\u201d\u7684\u5185\u5bb9\uff09\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u5185\u5bb9\u5e38\u5e38\u4f2a\u88c5\u6210\u5408\u6cd5\u7684\u5065\u8eab\u5185\u5bb9\uff0c\u5e76\u5229\u7528\u590d\u6742\u7684\u89c6\u89c9\u3001\u7f16\u7801\u8bed\u8a00\u548c\u52b1\u5fd7\u4fe1\u606f\u7ec4\u5408\u6765\u89c4\u907f\u57fa\u4e8e\u6587\u672c\u7684\u68c0\u6d4b\u7cfb\u7edf\u3002\u7279\u522b\u662f\uff0c\u8fd9\u4e9b\u5185\u5bb9\u5bf9\u7537\u6027\u9752\u5c11\u5e74\u7fa4\u4f53\u7684\u5f71\u54cd\u5c24\u4e3a\u4e25\u91cd\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aBigTokDetect\u7684\u4e34\u5e8a\u4fe1\u606f\u68c0\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4e86\u7b2c\u4e00\u4e2a\u4e13\u5bb6\u6ce8\u91ca\u7684\u591a\u6a21\u6001\u6570\u636e\u96c6\uff08BigTok\uff09\uff0c\u5176\u4e2d\u5305\u542b\u8d85\u8fc72200\u4e2aTikTok\u89c6\u9891\uff0c\u5e76\u5bf9\u5b83\u4eec\u5728\u8eab\u4f53\u610f\u8c61\u3001\u8425\u517b\u3001\u953b\u70bc\u3001\u8865\u5145\u5242\u548c\u7537\u5b50\u6c14\u6982\u7b49\u7c7b\u522b\u4e0a\u8fdb\u884c\u4e86\u6807\u6ce8\u3002\u901a\u8fc7\u5bf9\u6700\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u5728\u7279\u5b9a\u9886\u57df\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u73b0\u4e860.829%\u7684\u4e3b\u8981\u7c7b\u522b\u5206\u7c7b\u51c6\u786e\u7387\u548c0.690%\u7684\u5b50\u7c7b\u522b\u68c0\u6d4b\u51c6\u786e\u7387\u3002\u6d88\u878d\u7814\u7a76\u8868\u660e\uff0c\u591a\u6a21\u6001\u878d\u5408\u6bd4\u4ec5\u6587\u672c\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e865-10%\u7684\u6027\u80fd\uff0c\u5176\u4e2d\u89c6\u9891\u7279\u5f81\u63d0\u4f9b\u4e86\u6700\u663e\u8457\u7684\u533a\u5206\u4fe1\u53f7\u3002", "result": "BigTokDetect\u6846\u67b6\u5728\u4e3b\u8981\u7c7b\u522b\u5206\u7c7b\u4e0a\u8fbe\u5230\u4e860.829%\u7684\u51c6\u786e\u7387\uff0c\u5728\u5b50\u7c7b\u522b\u68c0\u6d4b\u4e0a\u8fbe\u5230\u4e860.690%\u7684\u51c6\u786e\u7387\u3002\u591a\u6a21\u6001\u878d\u5408\u76f8\u6bd4\u4ec5\u6587\u672c\u7684\u65b9\u6cd5\u6027\u80fd\u63d0\u9ad8\u4e865-10%\uff0c\u5176\u4e2d\u89c6\u9891\u7279\u5f81\u6700\u4e3a\u5173\u952e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86BigTokDetect\u6846\u67b6\uff0c\u4e3a\u8bc6\u522bTikTok\u4e0a\u7684\u5927\u5757\u5934\u764c\u5185\u5bb9\u8bbe\u5b9a\u4e86\u65b0\u7684\u57fa\u51c6\uff0c\u5e76\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u5185\u5bb9\u5ba1\u6838\u5de5\u5177\u548c\u65b9\u6cd5\u8bba\u6846\u67b6\uff0c\u7279\u522b\u662f\u5728\u4e13\u95e8\u7684\u5fc3\u7406\u5065\u5eb7\u9886\u57df\u3002"}}
{"id": "2508.07948", "categories": ["q-bio.OT", "cs.ET", "quant-ph", "81P68, 92C20, 42A38, 15A18, 81R05", "F.0; F.1.1; F.2.1; G.1.0; G.1.3; I.2; I.2.1; I.2.6; I.5; J.3"], "pdf": "https://arxiv.org/pdf/2508.07948", "abs": "https://arxiv.org/abs/2508.07948", "authors": ["John D. Mayfield"], "title": "Frequency-Domain Analysis of Time-Dependent Multiomic Data in Progressive Neurodegenerative Diseases: A Proposed Quantum-Classical Hybrid Approach with Quaternionic Extensions", "comment": "11 pages, 1 figure", "summary": "Progressive neurodegenerative diseases, including Alzheimer's disease (AD),\nmultiple sclerosis (MS), Parkinson's disease (PD), and amyotrophic lateral\nsclerosis (ALS), exhibit complex, nonlinear trajectories that challenge\ndeterministic modeling. Traditional time-domain analyses of multiomic and\nneuroimaging data often fail to capture hidden oscillatory patterns, limiting\npredictive accuracy. We propose a theoretical mathematical framework that\ntransforms time-series data into frequency or s-domain using Fourier and\nLaplace transforms, models neuronal dynamics via Hamiltonian formulations, and\nemploys quantum-classical hybrid computing with variational quantum\neigensolvers (VQE) for enhanced pattern detection. This theoretical construct\nserves as a foundation for future empirical works in quantum-enhanced analysis\nof neurodegenerative diseases. We extend this to quaternionic representations\nwith three imaginary axes ($i, j, k$) to model multistate Hamiltonians in\nmultifaceted disorders, drawing from quantum neuromorphic computing to capture\nentangled neural dynamics \\citep{Pehle2020, Emani2019}. This approach leverages\nquantum advantages in handling high-dimensional amplitude-phase data, enabling\noutlier detection and frequency signature analysis. Potential clinical\napplications include identifying high-risk patients with rapid progression or\ntherapy resistance using s-domain biomarkers, supported by quantum machine\nlearning (QML) precedents achieving up to 99.89% accuracy in Alzheimer's\nclassification \\citep{Belay2024, Bhowmik2025}. This framework aims to lay the\ngroundwork for redefining precision medicine for neurodegenerative diseases\nthrough future validations.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u9891\u57df/s\u57df\uff0c\u5e76\u5229\u7528\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u8ba1\u7b97\u6a21\u62df\u795e\u7ecf\u5143\u52a8\u529b\u5b66\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u6a21\u5f0f\u68c0\u6d4b\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u4f20\u7edf\u7684\u65f6\u57df\u5206\u6790\u96be\u4ee5\u6355\u6349\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\uff08\u5982\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u3001\u591a\u53d1\u6027\u786c\u5316\u75c7\u3001\u5e15\u91d1\u68ee\u75c5\u548c\u808c\u840e\u7f29\u4fa7\u7d22\u786c\u5316\u75c7\uff09\u6570\u636e\u4e2d\u9690\u85cf\u7684\u632f\u8361\u6a21\u5f0f\uff0c\u9650\u5236\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7406\u8bba\u6570\u5b66\u6846\u67b6\uff0c\u4f7f\u7528\u5085\u91cc\u53f6\u53d8\u6362\u548c\u62c9\u666e\u62c9\u65af\u53d8\u6362\u5c06\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u8f6c\u6362\u4e3a\u9891\u57df\u6216s\u57df\uff0c\u901a\u8fc7\u54c8\u5bc6\u987f\u516c\u5f0f\u6a21\u62df\u795e\u7ecf\u5143\u52a8\u529b\u5b66\uff0c\u5e76\u91c7\u7528\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u8ba1\u7b97\u548c\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668\uff08VQE\uff09\u8fdb\u884c\u589e\u5f3a\u6a21\u5f0f\u68c0\u6d4b\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u91cf\u5b50\u4f18\u52bf\u5904\u7406\u9ad8\u7ef4\u5e45-\u76f8\u6570\u636e\uff0c\u5b9e\u73b0\u5f02\u5e38\u503c\u68c0\u6d4b\u548c\u9891\u7387\u7279\u5f81\u5206\u6790\u3002", "result": "\u8be5\u6846\u67b6\u5229\u7528\u91cf\u5b50\u4f18\u52bf\u5904\u7406\u9ad8\u7ef4\u5e45-\u76f8\u6570\u636e\uff0c\u5b9e\u73b0\u5f02\u5e38\u503c\u68c0\u6d4b\u548c\u9891\u7387\u7279\u5f81\u5206\u6790\u3002\u91cf\u5b50\u673a\u5668\u5b66\u4e60\uff08QML\uff09\u5728\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u5206\u7c7b\u4e2d\u5df2\u8fbe\u523099.89%\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u6846\u67b6\u65e8\u5728\u4e3a\u901a\u8fc7\u672a\u6765\u9a8c\u8bc1\u91cd\u65b0\u5b9a\u4e49\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u7cbe\u51c6\u533b\u7597\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2508.06728", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06728", "abs": "https://arxiv.org/abs/2508.06728", "authors": ["Antar Kumar Biswas", "Masoud H. Nazari"], "title": "Secure and Decentralized Peer-to-Peer Energy Transactions using Blockchain Technology", "comment": null, "summary": "This paper presents an optimal peer-to-peer (P2P) energy transaction\nmechanism leveraging decentralized blockchain technology to enable a secure and\nscalable retail electricity market for the increasing penetration of\ndistributed energy resources (DERs). A decentralized bidding strategy is\nproposed to maximize individual profits while collectively enhancing social\nwelfare. The market design and transaction processes are simulated using the\nEthereum testnet, demonstrating the blockchain network's capability to ensure\nsecure, transparent, and sustainable P2P energy trading among DER participants.", "AI": {"tldr": "This paper proposes a blockchain-based P2P energy trading mechanism with a decentralized bidding strategy to maximize profits and social welfare in a retail electricity market for DERs. Simulations on the Ethereum testnet confirm its security, transparency, and sustainability.", "motivation": "To enable a secure and scalable retail electricity market for the increasing penetration of distributed energy resources (DERs) by presenting an optimal peer-to-peer (P2P) energy transaction mechanism leveraging decentralized blockchain technology.", "method": "A decentralized bidding strategy is proposed and the market design and transaction processes are simulated using the Ethereum testnet.", "result": "The simulation results using the Ethereum testnet demonstrate the blockchain network's capability to ensure secure, transparent, and sustainable P2P energy trading.", "conclusion": "The study demonstrates the feasibility of using blockchain technology for secure, transparent, and sustainable P2P energy trading among DER participants, validating its potential for a retail electricity market."}}
{"id": "2508.06758", "categories": ["cond-mat.mes-hall", "cond-mat.supr-con"], "pdf": "https://arxiv.org/pdf/2508.06758", "abs": "https://arxiv.org/abs/2508.06758", "authors": ["Chau Dao", "Eric Kleinherbers", "Bj\u00f8rnulf Brekke", "Yaroslav Tserkovnyak"], "title": "Topological hydrodynamics in spin-triplet superconductors", "comment": "20 pages including the supplemental material, 6 figures", "summary": "Due to the structure of the underlying SO(3) $\\mathbf d$-vector order\nparameter, spin triplet superconductors exhibit a bulk-edge correspondence\nlinking the circulation of supercurrent to the bulk magnetic skyrmion density,\ngiving rise to topological hydrodynamics of magnetic skyrmions. To probe the\ninterplay of charge and spin dynamics, we propose a blueprint for a\nspin-triplet superconducting quantum interference device (SQUID), which\nfunctions without a Josephson weak link. The triplet SQUID undergoes\nnonsingular $4\\pi$ phase slips, in which current relaxation is facilitated by\nspin dynamics that trace out a magnetic skyrmion texture. Inductively coupling\nthe device to a tank circuit and probing the nonlinear supercurrent response\nvia Oersted field measurements could provide an experimental signature of\nferromagnetic spin-triplet superconductivity.", "AI": {"tldr": "\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u4f53\u5b58\u5728\u8fde\u63a5\u8d85\u7535\u6d41\u73af\u6d41\u548c\u4f53\u78c1\u65af\u683c\u660e\u5b50\u5bc6\u5ea6\u7684\u4f53\u8fb9\u5bf9\u5e94\u5173\u7cfb\uff0c\u4ece\u800c\u4ea7\u751f\u78c1\u6027\u65af\u683c\u660e\u5b50\u7684\u62d3\u6251\u6d41\u4f53\u52a8\u529b\u5b66\u3002\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u7ea6\u745f\u592b\u68ee\u7ed3\u7684\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u91cf\u5b50\u5e72\u6d89\u5668\u4ef6\uff08SQUID\uff09\u7684\u84dd\u56fe\uff0c\u8be5\u5668\u4ef6\u7ecf\u5386\u4e86\u975e\u5947\u5f02\u76844\u03c0\u76f8\u53d8\uff0c\u5e76\u53ef\u80fd\u901a\u8fc7\u5965\u65af\u7279\u573a\u6d4b\u91cf\u63d0\u4f9b\u94c1\u78c1\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u7684\u5b9e\u9a8c\u8bc1\u636e\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7a76\u7535\u8377\u548c\u81ea\u65cb\u52a8\u529b\u5b66\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u91cf\u5b50\u5e72\u6d89\u5668\u4ef6\uff08SQUID\uff09\uff0c\u8be5\u5668\u4ef6\u65e0\u9700\u7ea6\u745f\u592b\u68ee\u7ed3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u91cf\u5b50\u5e72\u6d89\u5668\u4ef6\uff08SQUID\uff09\u7684\u84dd\u56fe\uff0c\u8be5\u5668\u4ef6\u65e0\u9700\u7ea6\u745f\u592b\u68ee\u7ed3\u3002\u901a\u8fc7\u611f\u5e94\u8026\u5408\u8be5\u5668\u4ef6\u4e0e\u69fd\u8def\uff0c\u5e76\u901a\u8fc7\u5965\u65af\u7279\u573a\u6d4b\u91cf\u63a2\u6d4b\u975e\u7ebf\u6027\u8d85\u6d41\u54cd\u5e94\u3002", "result": "\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u91cf\u5b50\u5e72\u6d89\u5668\u4ef6\uff08SQUID\uff09\u7ecf\u5386\u4e86\u975e\u5947\u5f02\u76844\u03c0\u76f8\u53d8\uff0c\u7535\u6d41\u5f1b\u8c6b\u7531\u63cf\u7ed8\u78c1\u6027\u65af\u683c\u660e\u5b50\u7eb9\u7406\u7684\u81ea\u65cb\u52a8\u529b\u5b66\u4fc3\u8fdb\u3002\u8fd9\u53ef\u4e3a\u94c1\u78c1\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u63d0\u4f9b\u5b9e\u9a8c\u8bc1\u636e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65e0\u9700\u7ea6\u745f\u592b\u68ee\u7ed3\u7684\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u91cf\u5b50\u5e72\u6d89\u5668\u4ef6\uff08SQUID\uff09\u7684\u84dd\u56fe\u3002\u8be5\u5668\u4ef6\u901a\u8fc7\u81ea\u65cb\u52a8\u529b\u5b66\u5f1b\u8c6b\u7535\u6d41\uff0c\u8be5\u52a8\u529b\u5b66\u63cf\u7ed8\u4e86\u78c1\u6027\u65af\u683c\u660e\u5b50\u7eb9\u7406\uff0c\u5e76\u7ecf\u5386\u4e86\u975e\u5947\u5f02\u76844\u03c0\u76f8\u53d8\u3002\u901a\u8fc7\u5c06\u5668\u4ef6\u4e0e\u69fd\u8def\u8026\u5408\uff0c\u5e76\u901a\u8fc7\u5965\u65af\u7279\u573a\u6d4b\u91cf\u63a2\u6d4b\u975e\u7ebf\u6027\u8d85\u6d41\u54cd\u5e94\uff0c\u53ef\u4ee5\u4e3a\u94c1\u78c1\u81ea\u65cb\u4e09\u8054\u4f53\u8d85\u5bfc\u63d0\u4f9b\u5b9e\u9a8c\u8bc1\u636e\u3002"}}
{"id": "2508.06794", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06794", "abs": "https://arxiv.org/abs/2508.06794", "authors": ["Rui Meng", "Xiaodong Xu", "Bizhu Wang", "Hao Sun", "Shida Xia", "Shujun Han", "Ping Zhang"], "title": "Physical Layer Authentication Based on Hierarchical Variational Auto-Encoder for Industrial Internet of Things", "comment": "17 pages, 13 figures", "summary": "Recently, Physical Layer Authentication (PLA) has attracted much attention\nsince it takes advantage of the channel randomness nature of transmission media\nto achieve communication confidentiality and authentication. In the complex\nenvironment, such as the Industrial Internet of Things (IIoT), machine learning\n(ML) is widely employed with PLA to extract and analyze complex channel\ncharacteristics for identity authentication. However, most PLA schemes for IIoT\nrequire attackers' prior channel information, leading to severe performance\ndegradation when the source of the received signals is unknown in the training\nstage. Thus, a channel impulse response (CIR)-based PLA scheme named\n\"Hierarchical Variational Auto-Encoder (HVAE)\" for IIoT is proposed in this\narticle, aiming at achieving high authentication performance without knowing\nattackers' prior channel information even when trained on a few data in the\ncomplex environment. HVAE consists of an Auto-Encoder (AE) module for CIR\ncharacteristics extraction and a Variational Auto-Encoder (VAE) module for\nimproving the representation ability of the CIR characteristic and outputting\nthe authentication results. Besides, a new objective function is constructed in\nwhich both the single-peak and the double-peak Gaussian distribution are taken\ninto consideration in the VAE module. Moreover, the simulations are conducted\nunder the static and mobile IIoT scenario, which verify the superiority of the\nproposed HVAE over three comparison PLA schemes even with a few training data.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHVAE\u7684\u65b0\u578b\u7269\u7406\u5c42\u8ba4\u8bc1\uff08PLA\uff09\u65b9\u6848\uff0c\u7528\u4e8e\u5de5\u4e1a\u7269\u8054\u7f51\uff08IIoT\uff09\u73af\u5883\u3002HVAE\u80fd\u591f\u6709\u6548\u5730\u63d0\u53d6\u4fe1\u9053\u7279\u5f81\u5e76\u8fdb\u884c\u8eab\u4efd\u8ba4\u8bc1\uff0c\u5373\u4f7f\u5728\u653b\u51fb\u8005\u4fe1\u9053\u4fe1\u606f\u672a\u77e5\u4e14\u8bad\u7ec3\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9ad8\u8ba4\u8bc1\u6027\u80fd\u3002\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cHVAE\u5728\u4e0d\u540cIIoT\u573a\u666f\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6848\u3002", "motivation": "\u73b0\u6709\u7684\u9488\u5bf9IIoT\u7684PLA\u65b9\u6848\u901a\u5e38\u9700\u8981\u653b\u51fb\u8005\u7684\u5148\u9a8c\u4fe1\u9053\u4fe1\u606f\uff0c\u5728\u8bad\u7ec3\u9636\u6bb5\u4fe1\u53f7\u6e90\u672a\u77e5\u7684\u60c5\u51b5\u4e0b\u4f1a\u5bfc\u81f4\u4e25\u91cd\u7684\u6027\u80fd\u4e0b\u964d\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5728\u4e0d\u4e86\u89e3\u653b\u51fb\u8005\u5148\u9a8c\u4fe1\u9053\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u5728\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u8ba4\u8bc1\u6027\u80fd\u7684PLA\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5206\u5c42\u53d8\u5206\u81ea\u7f16\u7801\u5668\u201d\uff08HVAE\uff09\u7684\u57fa\u4e8e\u4fe1\u9053\u51b2\u6fc0\u54cd\u5e94\uff08CIR\uff09\u7684PLA\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u5305\u62ec\u4e00\u4e2a\u7528\u4e8eCIR\u7279\u5f81\u63d0\u53d6\u7684\u81ea\u52a8\u7f16\u7801\u5668\uff08AE\uff09\u6a21\u5757\u548c\u4e00\u4e2a\u7528\u4e8e\u63d0\u9ad8CIR\u7279\u5f81\u8868\u793a\u80fd\u529b\u5e76\u8f93\u51fa\u8ba4\u8bc1\u7ed3\u679c\u7684\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u6a21\u5757\u3002\u5728VAE\u6a21\u5757\u4e2d\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u76ee\u6807\u51fd\u6570\uff0c\u540c\u65f6\u8003\u8651\u4e86\u5355\u5cf0\u548c\u53cc\u5cf0\u9ad8\u65af\u5206\u5e03\u3002", "result": "HVAE\u65b9\u6848\u80fd\u591f\u5b9e\u73b0\u9ad8\u8ba4\u8bc1\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u5728\u590d\u6742\u73af\u5883\u4e2d\uff08\u9759\u6001\u548c\u79fb\u52a8IIoT\u573a\u666f\uff09\u53d6\u5f97\u4f18\u4e8e\u5176\u4ed6\u4e09\u79cdPLA\u65b9\u6848\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684HVAE\u65b9\u6848\u5728\u9759\u6001\u548c\u79fb\u52a8IIoT\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u4eff\u771f\uff0c\u5e76\u4e0e\u5176\u4ed6\u4e09\u79cdPLA\u65b9\u6848\u8fdb\u884c\u4e86\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86HVAE\u5373\u4f7f\u5728\u8bad\u7ec3\u6570\u636e\u8f83\u5c11\u7684\u60c5\u51b5\u4e0b\u4e5f\u5177\u6709\u4f18\u8d8a\u6027\u3002"}}
{"id": "2508.07304", "categories": ["cs.LO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07304", "abs": "https://arxiv.org/abs/2508.07304", "authors": ["Fabio Vitali"], "title": "From Knowledge to Conjectures: A Modal Framework for Reasoning about Hypotheses", "comment": null, "summary": "This paper introduces a new family of cognitive modal logics designed to\nformalize conjectural reasoning: a modal system in which cognitive contexts\nextend known facts with hypothetical assumptions to explore their consequences.\nUnlike traditional doxastic and epistemic systems, conjectural logics rely on a\nprinciple, called Axiom C ($\\varphi \\rightarrow \\Box\\varphi$), that ensures\nthat all established facts are preserved across hypothetical layers. While\nAxiom C was dismissed in the past due to its association with modal collapse,\nwe show that the collapse only arises under classical and bivalent assumptions,\nand specifically in the presence of Axiom T. Hence we avoid Axiom T and adopt a\nparacomplete semantic framework, grounded in Weak Kleene logic or Description\nLogic, where undefined propositions coexist with modal assertions. This\nprevents the modal collapse and guarantees a layering to distinguish between\nfactual and conjectural statements. Under this framework we define new modal\nsystems, e.g., KC and KDC, and show that they are complete, decidable, and\nrobust under partial knowledge. Finally, we introduce a dynamic operation,\n$\\mathsf{settle}(\\varphi)$, which formalizes the transition from conjecture to\naccepted fact, capturing the event of the update of a world's cognitive state\nthrough the resolution of uncertainty.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u6a21\u6001\u903b\u8f91\uff0c\u7528\u4e8e\u5f62\u5f0f\u5316\u731c\u60f3\u63a8\u7406\uff0c\u901a\u8fc7\u5f15\u5165\u516c\u7406C\u548c\u526f\u5b8c\u5907\u8bed\u4e49\u6765\u907f\u514d\u6a21\u6001\u574d\u584c\uff0c\u5e76\u80fd\u533a\u5206\u4e8b\u5b9e\u4e0e\u731c\u60f3\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5f62\u5f0f\u5316\u731c\u60f3\u63a8\u7406\uff0c\u8fd9\u662f\u4e00\u79cd\u5728\u8ba4\u77e5\u60c5\u5883\u4e2d\uff0c\u901a\u8fc7\u52a0\u5165\u5047\u8bbe\u6027\u5047\u8bbe\u6765\u63a2\u7d22\u5176\u540e\u679c\u7684\u63a8\u7406\u65b9\u5f0f\u3002\u73b0\u6709\u7684\u8ba4\u77e5\u7cfb\u7edf\uff08\u5982\u4e49\u52a1\u903b\u8f91\u548c\u8ba4\u77e5\u903b\u8f91\uff09\u5728\u5904\u7406\u8fd9\u79cd\u63a8\u7406\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f\u5728\u533a\u5206\u4e8b\u5b9e\u548c\u5047\u8bbe\u65f6\u5bb9\u6613\u5bfc\u81f4\u6a21\u6001\u574d\u584c\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8ba4\u77e5\u6a21\u6001\u903b\u8f91\uff0c\u8be5\u903b\u8f91\u6269\u5c55\u4e86\u5df2\u77e5\u7684\u77e5\u8bc6\uff0c\u5e76\u52a0\u5165\u4e86\u5047\u8bbe\u6027\u5047\u8bbe\u4ee5\u63a2\u7d22\u5176\u540e\u679c\u3002\u8be5\u903b\u8f91\u7684\u5173\u952e\u5728\u4e8e\u516c\u7406C\uff08\u03c6\u2192\u25a1\u03c6\uff09\uff0c\u5b83\u786e\u4fdd\u6240\u6709\u5df2\u5efa\u7acb\u7684\u4e8b\u5b9e\u80fd\u591f\u8de8\u8d8a\u5047\u8bbe\u5c42\u3002\u4e3a\u4e86\u907f\u514d\u6a21\u6001\u574d\u584c\uff0c\u672c\u6587\u653e\u5f03\u4e86\u516c\u7406T\uff0c\u5e76\u91c7\u7528\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5f31\u514b\u83b1\u5c3c\u903b\u8f91\u6216\u63cf\u8ff0\u903b\u8f91\u7684\u526f\u5b8c\u5907\u8bed\u4e49\u6846\u67b6\uff0c\u5141\u8bb8\u672a\u5b9a\u4e49\u547d\u9898\u4e0e\u6a21\u6001\u65ad\u8a00\u5171\u5b58\u3002", "result": "\u672c\u6587\u6210\u529f\u6784\u5efa\u4e86\u65b0\u7684\u6a21\u6001\u7cfb\u7edfKC\u548cKDC\uff0c\u5e76\u8bc1\u660e\u4e86\u5b83\u4eec\u662f\u5b8c\u5907\u7684\u3001\u53ef\u5224\u5b9a\u7684\uff0c\u5e76\u4e14\u5728\u90e8\u5206\u77e5\u8bc6\u4e0b\u662f\u7a33\u5065\u7684\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u526f\u5b8c\u5907\u7684\u8bed\u4e49\u6846\u67b6\u4e0b\uff0c\u516c\u7406C\u4e0d\u4f1a\u5bfc\u81f4\u6a21\u6001\u574d\u584c\u3002\u6700\u540e\uff0c\u5f15\u5165\u4e86\u201csettle(\u03c6)\u201d\u64cd\u4f5c\u6765\u5f62\u5f0f\u5316\u4ece\u731c\u60f3\u5230\u4e8b\u5b9e\u7684\u8f6c\u53d8\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u8ba4\u77e5\u6a21\u6001\u903b\u8f91\u901a\u8fc7\u5f15\u5165\u516c\u7406C\uff08\u03c6\u2192\u25a1\u03c6\uff09\u5e76\u7ed3\u5408\u975e\u7ecf\u5178\uff08\u7279\u522b\u662f\u526f\u5b8c\u5907\uff09\u7684\u8bed\u4e49\u6846\u67b6\uff0c\u6210\u529f\u5730\u89e3\u51b3\u4e86\u6a21\u6001\u574d\u584c\u95ee\u9898\uff0c\u4ece\u800c\u80fd\u591f\u533a\u5206\u4e8b\u5b9e\u548c\u731c\u60f3\u9648\u8ff0\u3002\u65b0\u7684\u6a21\u6001\u7cfb\u7edfKC\u548cKDC\u88ab\u8bc1\u660e\u662f\u5b8c\u5907\u3001\u53ef\u5224\u5b9a\u4e14\u5728\u90e8\u5206\u77e5\u8bc6\u4e0b\u662f\u7a33\u5065\u7684\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u52a8\u6001\u64cd\u4f5c\u201csettle(\u03c6)\u201d\uff0c\u8be5\u903b\u8f91\u80fd\u591f\u5f62\u5f0f\u5316\u4ece\u731c\u60f3\u5230\u88ab\u63a5\u53d7\u4e8b\u5b9e\u7684\u8fc7\u6e21\uff0c\u6355\u6349\u8ba4\u77e5\u72b6\u6001\u66f4\u65b0\u7684\u8fc7\u7a0b\u3002"}}
{"id": "2508.06571", "categories": ["cs.AI", "cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06571", "abs": "https://arxiv.org/abs/2508.06571", "authors": ["Anqing Jiang", "Yu Gao", "Yiru Wang", "Zhigang Sun", "Shuo Wang", "Yuwen Heng", "Hao Sun", "Shichen Tang", "Lijuan Zhu", "Jinhao Chai", "Jijun Wang", "Zichong Gu", "Hao Jiang", "Li Sun"], "title": "IRL-VLA: Training an Vision-Language-Action Policy via Reward World Model", "comment": "9 pagres, 2 figures", "summary": "Vision-Language-Action (VLA) models have demonstrated potential in autonomous\ndriving. However, two critical challenges hinder their development: (1)\nExisting VLA architectures are typically based on imitation learning in\nopen-loop setup which tends to capture the recorded behaviors in the dataset,\nleading to suboptimal and constrained performance, (2) Close-loop training\nrelies heavily on high-fidelity sensor simulation, where domain gaps and\ncomputational inefficiencies pose significant barriers. In this paper, we\nintroduce IRL-VLA, a novel close-loop Reinforcement Learning via\n\\textbf{I}nverse \\textbf{R}einforcement \\textbf{L}earning reward world model\nwith a self-built VLA approach. Our framework proceeds in a three-stage\nparadigm: In the first stage, we propose a VLA architecture and pretrain the\nVLA policy via imitation learning. In the second stage, we construct a\nlightweight reward world model via inverse reinforcement learning to enable\nefficient close-loop reward computation. To further enhance planning\nperformance, finally, we design specialized reward world model guidence\nreinforcement learning via PPO(Proximal Policy Optimization) to effectively\nbalance the safety incidents, comfortable driving, and traffic efficiency. Our\napproach achieves state-of-the-art performance in NAVSIM v2 end-to-end driving\nbenchmark, 1st runner up in CVPR2025 Autonomous Grand Challenge. We hope that\nour framework will accelerate VLA research in close-loop autonomous driving.", "AI": {"tldr": "IRL-VLA\u901a\u8fc7\u7ed3\u5408\u9006\u5f3a\u5316\u5b66\u4e60\u548cVLA\u65b9\u6cd5\uff0c\u5229\u7528\u8f7b\u91cf\u7ea7\u5956\u52b1\u4e16\u754c\u6a21\u578b\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\uff0c\u5e76\u5728\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u679c\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709VLA\u6a21\u578b\u5728\u5f00\u653e\u5faa\u73af\u8bbe\u7f6e\u4e0b\u7684\u6a21\u4eff\u5b66\u4e60\u9650\u5236\uff08\u503e\u5411\u4e8e\u5b66\u4e60\u6570\u636e\u96c6\u4e2d\u7684\u8bb0\u5f55\u884c\u4e3a\uff0c\u5bfc\u81f4\u6027\u80fd\u6b21\u4f18\u4e14\u53d7\u9650\uff09\u4ee5\u53ca\u95ed\u73af\u8bad\u7ec3\u5bf9\u9ad8\u4fdd\u771f\u4f20\u611f\u5668\u6a21\u62df\u7684\u4f9d\u8d56\uff08\u5b58\u5728\u57df\u5dee\u8ddd\u548c\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u95ed\u73af\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5IRL-VLA\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u9006\u5f3a\u5316\u5b66\u4e60\uff08IRL\uff09\u7684\u5956\u52b1\u4e16\u754c\u6a21\u578b\u548c\u81ea\u5efa\u7684VLA\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a1. \u63d0\u51faVLA\u67b6\u6784\u5e76\u901a\u8fc7\u6a21\u4eff\u5b66\u4e60\u9884\u8bad\u7ec3VLA\u7b56\u7565\u30022. \u901a\u8fc7IRL\u6784\u5efa\u8f7b\u91cf\u7ea7\u5956\u52b1\u4e16\u754c\u6a21\u578b\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u95ed\u73af\u5956\u52b1\u8ba1\u7b97\u30023. \u8bbe\u8ba1\u4e86\u4e13\u95e8\u7684\u5956\u52b1\u4e16\u754c\u6a21\u578b\u5f15\u5bfc\u5f3a\u5316\u5b66\u4e60\uff08PPO\uff09\uff0c\u4ee5\u6709\u6548\u5e73\u8861\u5b89\u5168\u4e8b\u6545\u3001\u8212\u9002\u9a7e\u9a76\u548c\u4ea4\u901a\u6548\u7387\u3002", "result": "\u5728NAVSIM v2\u7aef\u5230\u7aef\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728CVPR2025\u81ea\u52a8\u9a7e\u9a76\u6311\u6218\u8d5b\u4e2d\u83b7\u5f971st runner up\u3002", "conclusion": "IRL-VLA\u6846\u67b6\u5728NAVSIM v2\u7aef\u5230\u7aef\u9a7e\u9a76\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728CVPR2025\u81ea\u52a8\u9a7e\u9a76\u6311\u6218\u8d5b\u4e2d\u83b7\u5f97\u4e9a\u519b\uff0c\u6709\u671b\u52a0\u901f\u95ed\u73af\u81ea\u52a8\u9a7e\u9a76\u4e2dVLA\u7684\u7814\u7a76\u3002"}}
{"id": "2508.06809", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.06809", "abs": "https://arxiv.org/abs/2508.06809", "authors": ["Qiming Cui", "Michael Dinitz"], "title": "Controlling tail risk in two-slope ski rental", "comment": "This paper will appear at WAOA 2025", "summary": "We study the optimal solution to a general two-slope ski rental problem with\na tail risk, i.e., the chance of the competitive ratio exceeding a value\n$\\gamma$ is bounded by $\\delta$. This extends the recent study of tail bounds\nfor ski rental by [Dinitz et al. SODA 2024] to the two-slope version defined by\n[Lotker et al. IPL 2008]. In this version, even after \"buying,\" we must still\npay a rental cost at each time step, though it is lower after buying. This\nmodels many real-world \"rent-or-buy\" scenarios where a one-time investment\ndecreases (but does not eliminate) the per-time cost.\n  Despite this being a simple extension of the classical problem, we find that\nadding tail risk bounds creates a fundamentally different solution structure.\nFor example, in our setting there is a possibility that we never buy in an\noptimal solution (which can also occur without tail bounds), but more strangely\n(and unlike the case without tail bounds or the classical case with tail\nbounds) we also show that the optimal solution might need to have nontrivial\nprobabilities of buying even at finite points beyond the time corresponding to\nthe buying cost. Moreover, in many regimes there does not exist a unique\noptimal solution. As our first contribution, we develop a series of structure\ntheorems to characterize some features of optimal solutions.\n  The complex structure of optimal solutions makes it more difficult to develop\nan algorithm to compute such a solution. As our second contribution, we utilize\nour structure theorems to design two algorithms: one based on a greedy\nalgorithm combined with binary search that is fast but yields arbitrarily close\nto optimal solutions, and a slower algorithm based on linear programming which\ncomputes exact optimal solutions.", "AI": {"tldr": "\u5bf9\u5177\u6709\u5c3e\u90e8\u98ce\u9669\u7684\u53cc\u659c\u7387\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u6700\u4f18\u89e3\u7ed3\u6784\u548c\u8ba1\u7b97\u7b97\u6cd5\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e00\u4e2a\u4e00\u822c\u6027\u7684\u53cc\u659c\u7387\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\uff0c\u5e76\u8003\u8651\u5c3e\u90e8\u98ce\u9669\uff0c\u5373\u7ade\u4e89\u6bd4\u8d85\u8fc7\u67d0\u4e2a\u503c \u03b3 \u7684\u6982\u7387\u6709\u754c\uff08\u4e0d\u8d85\u8fc7 \u03b4\uff09\u3002\u8fd9\u662f\u5bf9\u73b0\u6709\u6ed1\u96ea\u79df\u8d41\u5c3e\u90e8\u754c\u9650\u7814\u7a76\u7684\u6269\u5c55\uff0c\u5e76\u8bd5\u56fe\u6a21\u62df\u73b0\u5b9e\u4e16\u754c\u4e2d\u201c\u79df\u4e70\u201d\u573a\u666f\uff0c\u5373\u4e00\u6b21\u6027\u6295\u8d44\u53ef\u4ee5\u964d\u4f4e\u4f46\u4e0d\u80fd\u5b8c\u5168\u6d88\u9664\u6bcf\u6b21\u7684\u65f6\u95f4\u6210\u672c\u3002", "method": "\u8be5\u7814\u7a76\u901a\u8fc7\u7ed3\u6784\u5b9a\u7406\u6765\u8868\u5f81\u6700\u4f18\u89e3\u7684\u7279\u5f81\uff0c\u5e76\u5229\u7528\u8fd9\u4e9b\u5b9a\u7406\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7b97\u6cd5\uff1a\u4e00\u79cd\u662f\u57fa\u4e8e\u8d2a\u5fc3\u7b97\u6cd5\u7ed3\u5408\u4e8c\u5206\u67e5\u627e\u7684\u5feb\u901f\u7b97\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\u7684\u7cbe\u786e\u7b97\u6cd5\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u7b80\u5355\u7684\u6269\u5c55\uff0c\u5c3e\u90e8\u98ce\u9669\u7ea6\u675f\u4e5f\u5bfc\u81f4\u4e86\u6839\u672c\u4e0d\u540c\u7684\u6700\u4f18\u89e3\u7ed3\u6784\uff0c\u4f8b\u5982\uff0c\u6700\u4f18\u89e3\u53ef\u80fd\u6c38\u8fdc\u4e0d\u8d2d\u4e70\uff0c\u6216\u8005\u5728\u8d2d\u4e70\u6210\u672c\u5bf9\u5e94\u65f6\u95f4\u70b9\u4e4b\u540e\uff0c\u4ee5\u975e\u5e73\u51e1\u7684\u6982\u7387\u8fdb\u884c\u8d2d\u4e70\u3002\u6b64\u5916\uff0c\u5728\u8bb8\u591a\u60c5\u51b5\u4e0b\uff0c\u4e0d\u5b58\u5728\u552f\u4e00\u7684\u6700\u4f18\u89e3\u3002\u7814\u7a76\u4e3a\u6b64\u5f00\u53d1\u4e86\u7ed3\u6784\u5b9a\u7406\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e24\u79cd\u7b97\u6cd5\u6765\u8ba1\u7b97\u6700\u4f18\u89e3\uff0c\u4e00\u79cd\u662f\u8fd1\u4f3c\u6700\u4f18\u89e3\u7684\u5feb\u901f\u7b97\u6cd5\uff0c\u53e6\u4e00\u79cd\u662f\u7cbe\u786e\u6700\u4f18\u89e3\u7684\u6162\u901f\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5177\u6709\u5c3e\u90e8\u98ce\u9669\u7684\u53cc\u659c\u7387\u6ed1\u96ea\u79df\u8d41\u95ee\u9898\u63d0\u4f9b\u4e86\u7ed3\u6784\u5b9a\u7406\u548c\u7b97\u6cd5\u3002"}}
{"id": "2508.06576", "categories": ["cs.LG", "q-bio.BM", "q-bio.MN"], "pdf": "https://arxiv.org/pdf/2508.06576", "abs": "https://arxiv.org/abs/2508.06576", "authors": ["Azmine Toushik Wasi"], "title": "GFlowNets for Learning Better Drug-Drug Interaction Representations", "comment": "Accepted to ICANN 2025:AIDD", "summary": "Drug-drug interactions pose a significant challenge in clinical pharmacology,\nwith severe class imbalance among interaction types limiting the effectiveness\nof predictive models. Common interactions dominate datasets, while rare but\ncritical interactions remain underrepresented, leading to poor model\nperformance on infrequent cases. Existing methods often treat DDI prediction as\na binary problem, ignoring class-specific nuances and exacerbating bias toward\nfrequent interactions. To address this, we propose a framework combining\nGenerative Flow Networks (GFlowNet) with Variational Graph Autoencoders (VGAE)\nto generate synthetic samples for rare classes, improving model balance and\ngenerate effective and novel DDI pairs. Our approach enhances predictive\nperformance across interaction types, ensuring better clinical reliability.", "AI": {"tldr": "\u901a\u8fc7GFlowNet\u548cVGAE\u751f\u6210\u5408\u6210\u6837\u672c\u6765\u89e3\u51b3DDI\u9884\u6d4b\u4e2d\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "motivation": "\u836f\u7269-\u836f\u7269\u76f8\u4e92\u4f5c\u7528\uff08DDI\uff09\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u7279\u522b\u662f\u7f55\u89c1\u4f46\u5173\u952e\u7684\u76f8\u4e92\u4f5c\u7528\u6837\u672c\u4e0d\u8db3\uff0c\u5bfc\u81f4\u9884\u6d4b\u6a21\u578b\u5728\u8be5\u7c7b\u522b\u7684\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNet\uff09\u548c\u53d8\u5206\u56fe\u81ea\u7f16\u7801\u5668\uff08VGAE\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u7f55\u89c1\u7c7b\u522b\u7684\u5408\u6210\u6837\u672c\u3002", "result": "\u751f\u6210\u7684\u5408\u6210\u6837\u672c\u6709\u52a9\u4e8e\u6539\u5584\u6a21\u578b\u5e73\u8861\uff0c\u63d0\u9ad8\u5bf9\u7f55\u89c1\u7c7b\u522b\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u6709\u6548\u4e14\u65b0\u9896\u7684DDI\u5bf9\uff0c\u4ece\u800c\u63d0\u5347\u4e86\u8de8\u76f8\u4e92\u4f5c\u7528\u7c7b\u578b\u7684\u9884\u6d4b\u6027\u80fd\u548c\u4e34\u5e8a\u53ef\u9760\u6027\u3002", "conclusion": "\u901a\u8fc7\u7ed3\u5408\u751f\u6210\u6d41\u7f51\u7edc\uff08GFlowNet\uff09\u548c\u53d8\u5206\u56fe\u81ea\u7f16\u7801\u5668\uff08VGAE\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u5408\u6210\u6837\u672c\u4ee5\u89e3\u51b3\u7f55\u89c1\u7c7b\u522b\u4e0d\u8db3\u95ee\u9898\u7684\u6846\u67b6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5e73\u8861\u6027\u548c\u9884\u6d4b\u6027\u80fd\uff0c\u589e\u5f3a\u4e86\u4e34\u5e8a\u53ef\u9760\u6027\u3002"}}
{"id": "2508.07720", "categories": ["cs.MA", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07720", "abs": "https://arxiv.org/abs/2508.07720", "authors": ["Themistoklis Charalambous", "Nikolaos Pappas", "Nikolaos Nomikos", "Risto Wichman"], "title": "Toward Goal-Oriented Communication in Multi-Agent Systems: An overview", "comment": "32 pages", "summary": "As multi-agent systems (MAS) become increasingly prevalent in autonomous\nsystems, distributed control, and edge intelligence, efficient communication\nunder resource constraints has emerged as a critical challenge. Traditional\ncommunication paradigms often emphasize message fidelity or bandwidth\noptimization, overlooking the task relevance of the exchanged information. In\ncontrast, goal-oriented communication prioritizes the importance of information\nwith respect to the agents' shared objectives. This review provides a\ncomprehensive survey of goal-oriented communication in MAS, bridging\nperspectives from information theory, communication theory, and machine\nlearning. We examine foundational concepts alongside learning-based approaches\nand emergent protocols. Special attention is given to coordination under\ncommunication constraints, as well as applications in domains such as swarm\nrobotics, federated learning, and edge computing. The paper concludes with a\ndiscussion of open challenges and future research directions at the\nintersection of communication theory, machine learning, and multi-agent\ndecision making.", "AI": {"tldr": "MAS\u901a\u4fe1\u9762\u4e34\u8d44\u6e90\u7ea6\u675f\u7684\u6311\u6218\uff0c\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\u4f18\u5148\u8003\u8651\u4fe1\u606f\u5bf9\u5171\u4eab\u76ee\u6807\u7684\u91cd\u8981\u6027\u3002\u672c\u7efc\u8ff0\u603b\u7ed3\u4e86MAS\u4e2d\u7684\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\uff0c\u5305\u62ec\u57fa\u7840\u6982\u5ff5\u3001\u5b66\u4e60\u65b9\u6cd5\u3001\u65b0\u5174\u534f\u8bae\uff0c\u5e76\u5173\u6ce8\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u534f\u8c03\u53ca\u5e94\u7528\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "motivation": "\u968f\u7740MAS\u5728\u81ea\u4e3b\u7cfb\u7edf\u3001\u5206\u5e03\u5f0f\u63a7\u5236\u548c\u8fb9\u7f18\u667a\u80fd\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u5728\u8d44\u6e90\u7ea6\u675f\u4e0b\u8fdb\u884c\u9ad8\u6548\u901a\u4fe1\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4f20\u7edf\u901a\u4fe1\u8303\u5f0f\u5ffd\u89c6\u4e86\u901a\u4fe1\u4fe1\u606f\u7684\u4efb\u52a1\u76f8\u5173\u6027\uff0c\u800c\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\u4f18\u5148\u8003\u8651\u4fe1\u606f\u5bf9\u4ee3\u7406\u5171\u4eab\u76ee\u6807\u7684\u91cd\u8981\u6027\u3002", "method": "\u672c\u7efc\u8ff0\u5168\u9762 survey \u4e86MAS\u4e2d\u7684\u76ee\u6807\u5bfc\u5411\u901a\u4fe1\uff0c\u6db5\u76d6\u4e86\u4fe1\u606f\u8bba\u3001\u901a\u4fe1\u7406\u8bba\u548c\u673a\u5668\u5b66\u4e60\u7b49\u89c2\u70b9\uff0c\u5e76\u5ba1\u89c6\u4e86\u57fa\u7840\u6982\u5ff5\u3001\u5b66\u4e60\u65b9\u6cd5\u548c\u65b0\u5174\u534f\u8bae\u3002", "result": "\u672c\u6587\u91cd\u70b9\u5173\u6ce8\u4e86\u901a\u4fe1\u7ea6\u675f\u4e0b\u7684\u534f\u8c03\u95ee\u9898\uff0c\u4ee5\u53ca\u5728\u7fa4\u4f53\u673a\u5668\u4eba\u3001\u8054\u90a6\u5b66\u4e60\u548c\u8fb9\u7f18\u8ba1\u7b97\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "conclusion": "\u672c\u6587\u603b\u7ed3\u4e86\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u4e2d\u4ee5\u76ee\u6807\u4e3a\u5bfc\u5411\u7684\u901a\u4fe1\uff0c\u63a2\u8ba8\u4e86\u4fe1\u606f\u8bba\u3001\u901a\u4fe1\u7406\u8bba\u548c\u673a\u5668\u5b66\u4e60\u7b49\u9886\u57df\u7684\u4ea4\u53c9\u5e94\u7528\uff0c\u5e76\u6307\u51fa\u4e86\u672a\u6765\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.06691", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06691", "abs": "https://arxiv.org/abs/2508.06691", "authors": ["Agada Joseph Oche", "Arpan Biswas"], "title": "Role of Large Language Models and Retrieval-Augmented Generation for Accelerating Crystalline Material Discovery: A Systematic Review", "comment": "10 pages, 2 figures", "summary": "Large language models (LLMs) have emerged as powerful tools for\nknowledge-intensive tasks across domains. In materials science, to find novel\nmaterials for various energy efficient devices for various real-world\napplications, requires several time and cost expensive simulations and\nexperiments. In order to tune down the uncharted material search space,\nminimizing the experimental cost, LLMs can play a bigger role to first provide\nan accelerated search of promising known material candidates. Furthermore, the\nintegration of LLMs with domain-specific information via retrieval-augmented\ngeneration (RAG) is poised to revolutionize how researchers predict materials\nstructures, analyze defects, discover novel compounds, and extract knowledge\nfrom literature and databases. In motivation to the potentials of LLMs and RAG\nin accelerating material discovery, this paper presents a broad and systematic\nreview to examine the recent advancements in applying LLMs and RAG to key\nmaterials science problems. We survey state-of-the-art developments in crystal\nstructure prediction, defect analysis, materials discovery, literature mining,\ndatabase integration, and multi-modal retrieval, highlighting how combining\nLLMs with external knowledge sources enables new capabilities. We discuss the\nperformance, limitations, and implications of these approaches, and outline\nfuture directions for leveraging LLMs to accelerate materials research and\ndiscovery for advancement in technologies in the area of electronics, optics,\nbiomedical, and energy storage.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07071", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07071", "abs": "https://arxiv.org/abs/2508.07071", "authors": ["Oscar Amoros", "Albert Andaluz", "Johnny Nunez", "Antonio J. Pena"], "title": "The Fused Kernel Library: A C++ API to Develop Highly-Efficient GPU Libraries", "comment": "16 pages", "summary": "Existing GPU libraries often struggle to fully exploit the parallel resources\nand on-chip memory (SRAM) of GPUs when chaining multiple GPU functions as\nindividual kernels. While Kernel Fusion (KF) techniques like Horizontal Fusion\n(HF) and Vertical Fusion (VF) can mitigate this, current library\nimplementations often require library developers to manually create fused\nkernels. Hence, library users rely on limited sets of pre-compiled or\ntemplate-based fused kernels. This limits the use cases that can benefit from\nHF and VF and increases development costs. In order to solve these issues, we\npresent a novel methodology for building GPU libraries that enables automatic\non-demand HF and VF for arbitrary combinations of GPU library functions. Our\nmethodology defines reusable, fusionable components that users combine via\nhigh-level programming interfaces. Leveraging C++17 metaprogramming features\navailable in compilers like nvcc, our methodology generates a single and\noptimized fused kernel tailored to the user's specific sequence of operations\nat compile time, without needing a custom compiler or manual development and\npre-compilation of kernel combinations. This approach abstracts low-level GPU\ncomplexities while maximizing GPU resource utilization and keeping intermediate\ndata in SRAM. We provide an open-source implementation demonstrating\nsignificant speedups compared to traditional libraries in various benchmarks,\nvalidating the effectiveness of this methodology for improving GPU performance\nin the range of 2x to more than 1000x, while preserving high-level\nprogrammability.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684 GPU \u5e93\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7 C++17 \u5143\u7f16\u7a0b\u81ea\u52a8\u6309\u9700\u878d\u5408 GPU \u51fd\u6570\u5185\u6838\uff0c\u4f18\u5316\u6027\u80fd\u5e76\u964d\u4f4e\u5f00\u53d1\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684 GPU \u5e93\u5728\u94fe\u63a5\u591a\u4e2a GPU \u51fd\u6570\u4f5c\u4e3a\u72ec\u7acb\u5185\u6838\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u5145\u5206\u5229\u7528 GPU \u7684\u5e76\u884c\u8d44\u6e90\u548c\u7247\u4e0a\u5185\u5b58 (SRAM)\u3002\u76ee\u524d\u7684\u5185\u6838\u878d\u5408 (KF) \u6280\u672f\uff0c\u5982\u6c34\u5e73\u878d\u5408 (HF) \u548c\u5782\u76f4\u878d\u5408 (VF)\uff0c\u867d\u7136\u53ef\u4ee5\u7f13\u89e3\u8fd9\u4e00\u95ee\u9898\uff0c\u4f46\u901a\u5e38\u9700\u8981\u5e93\u5f00\u53d1\u8005\u624b\u52a8\u521b\u5efa\u878d\u5408\u5185\u6838\uff0c\u5bfc\u81f4\u5e93\u7528\u6237\u53ea\u80fd\u4f9d\u8d56\u6709\u9650\u7684\u9884\u7f16\u8bd1\u6216\u57fa\u4e8e\u6a21\u677f\u7684\u878d\u5408\u5185\u6838\uff0c\u9650\u5236\u4e86\u5e94\u7528\u573a\u666f\u5e76\u589e\u52a0\u4e86\u5f00\u53d1\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 GPU \u5e93\u6784\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b9a\u4e49\u53ef\u91cd\u7528\u7684\u3001\u53ef\u878d\u5408\u7684\u7ec4\u4ef6\uff0c\u5e76\u5229\u7528 C++17 \u5143\u7f16\u7a0b\u7279\u6027\uff0c\u5728\u7f16\u8bd1\u65f6\u81ea\u52a8\u6309\u9700\u8fdb\u884c\u6c34\u5e73\u878d\u5408 (HF) \u548c\u5782\u76f4\u878d\u5408 (VF)\uff0c\u751f\u6210\u9488\u5bf9\u7528\u6237\u7279\u5b9a\u64cd\u4f5c\u5e8f\u5217\u8fdb\u884c\u4f18\u5316\u7684\u5355\u4e00\u878d\u5408\u5185\u6838\uff0c\u65e0\u9700\u81ea\u5b9a\u4e49\u7f16\u8bd1\u5668\u6216\u624b\u52a8\u5f00\u53d1\u9884\u7f16\u8bd1\u7684\u5185\u6838\u7ec4\u5408\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f00\u6e90\u5b9e\u73b0\uff0c\u8be5\u5b9e\u73b0\u80fd\u591f\u81ea\u52a8\u6309\u9700\u8fdb\u884c\u4efb\u610f GPU \u5e93\u51fd\u6570\u7ec4\u5408\u7684 HF \u548c VF\uff0c\u4e0e\u4f20\u7edf\u5e93\u76f8\u6bd4\uff0c\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86 2 \u500d\u5230 1000 \u500d\u4ee5\u4e0a\u7684\u663e\u8457\u901f\u5ea6\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8 GPU \u6027\u80fd\u548c\u4fdd\u6301\u9ad8\u7ea7\u53ef\u7f16\u7a0b\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528 C++17 \u5143\u7f16\u7a0b\u7279\u6027\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u7f16\u8bd1\u65f6\u81ea\u52a8\u751f\u6210\u5b9a\u5236\u5316\u7684\u878d\u5408\u5185\u6838\uff0c\u4ece\u800c\u4f18\u5316 GPU \u8d44\u6e90\u5229\u7528\u7387\uff0c\u5c06\u4e2d\u95f4\u6570\u636e\u4fdd\u7559\u5728 SRAM \u4e2d\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e0e\u4f20\u7edf\u5e93\u76f8\u6bd4\u5b9e\u73b0\u4e86 2 \u500d\u5230 1000 \u500d\u4ee5\u4e0a\u7684\u663e\u8457\u52a0\u901f\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7ea7\u53ef\u7f16\u7a0b\u6027\u3002"}}
{"id": "2508.06534", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06534", "abs": "https://arxiv.org/abs/2508.06534", "authors": ["Aishan Liu", "Jiakai Wang", "Tianyuan Zhang", "Hainan Li", "Jiangfan Liu", "Siyuan Liang", "Yilong Ren", "Xianglong Liu", "Dacheng Tao"], "title": "MetAdv: A Unified and Interactive Adversarial Testing Platform for Autonomous Driving", "comment": "Accepted by ACM MM 2025 Demo/Videos track", "summary": "Evaluating and ensuring the adversarial robustness of autonomous driving (AD)\nsystems is a critical and unresolved challenge. This paper introduces MetAdv, a\nnovel adversarial testing platform that enables realistic, dynamic, and\ninteractive evaluation by tightly integrating virtual simulation with physical\nvehicle feedback. At its core, MetAdv establishes a hybrid virtual-physical\nsandbox, within which we design a three-layer closed-loop testing environment\nwith dynamic adversarial test evolution. This architecture facilitates\nend-to-end adversarial evaluation, ranging from high-level unified adversarial\ngeneration, through mid-level simulation-based interaction, to low-level\nexecution on physical vehicles. Additionally, MetAdv supports a broad spectrum\nof AD tasks, algorithmic paradigms (e.g., modular deep learning pipelines,\nend-to-end learning, vision-language models). It supports flexible 3D vehicle\nmodeling and seamless transitions between simulated and physical environments,\nwith built-in compatibility for commercial platforms such as Apollo and Tesla.\nA key feature of MetAdv is its human-in-the-loop capability: besides flexible\nenvironmental configuration for more customized evaluation, it enables\nreal-time capture of physiological signals and behavioral feedback from\ndrivers, offering new insights into human-machine trust under adversarial\nconditions. We believe MetAdv can offer a scalable and unified framework for\nadversarial assessment, paving the way for safer AD.", "AI": {"tldr": "MetAdv\u662f\u4e00\u4e2a\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u8bc4\u4f30\u7684\u6df7\u5408\u865a\u62df-\u7269\u7406\u6d4b\u8bd5\u5e73\u53f0\uff0c\u652f\u6301\u591a\u79cd\u4efb\u52a1\u548c\u6a21\u578b\uff0c\u5e76\u5305\u542b\u4eba\u4e3a\u5e72\u9884\u80fd\u529b\u3002", "motivation": "\u8bc4\u4f30\u548c\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u7cfb\u7edf\u7684\u5bf9\u6297\u9c81\u68d2\u6027\u662f\u4e00\u9879\u5173\u952e\u4e14\u60ac\u800c\u672a\u51b3\u7684\u6311\u6218\u3002", "method": "MetAdv\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u5bf9\u6297\u6027\u6d4b\u8bd5\u5e73\u53f0\uff0c\u901a\u8fc7\u5c06\u865a\u62df\u4eff\u771f\u4e0e\u7269\u7406\u8f66\u8f86\u53cd\u9988\u7d27\u5bc6\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u73b0\u5b9e\u3001\u52a8\u6001\u548c\u4ea4\u4e92\u5f0f\u8bc4\u4f30\u3002\u5176\u6838\u5fc3\u662f\u4e00\u4e2a\u6df7\u5408\u865a\u62df-\u7269\u7406\u6c99\u7bb1\uff0c\u5176\u4e2d\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u4e09\u5c42\u95ed\u73af\u6d4b\u8bd5\u73af\u5883\uff0c\u5177\u6709\u52a8\u6001\u5bf9\u6297\u6027\u6d4b\u8bd5\u6f14\u5316\u3002\u8be5\u67b6\u6784\u4fc3\u8fdb\u4e86\u7aef\u5230\u7aef\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\uff0c\u5305\u62ec\u9ad8\u7ea7\u7edf\u4e00\u5bf9\u6297\u751f\u6210\u3001\u57fa\u4e8e\u4eff\u771f\u7684\u4e2d\u95f4\u5c42\u4ea4\u4e92\u4ee5\u53ca\u7269\u7406\u8f66\u8f86\u4e0a\u7684\u4f4e\u7ea7\u6267\u884c\u3002MetAdv\u8fd8\u652f\u6301\u5e7f\u6cdb\u7684\u81ea\u52a8\u9a7e\u9a76\u4efb\u52a1\u3001\u7b97\u6cd5\u8303\u5f0f\uff08\u5982\u6a21\u5757\u5316\u6df1\u5ea6\u5b66\u4e60\u6d41\u6c34\u7ebf\u3001\u7aef\u5230\u7aef\u5b66\u4e60\u3001\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff09\uff0c\u652f\u6301\u7075\u6d3b\u76843D\u8f66\u8f86\u5efa\u6a21\u4ee5\u53ca\u6a21\u62df\u548c\u7269\u7406\u73af\u5883\u4e4b\u95f4\u7684\u65e0\u7f1d\u8f6c\u6362\uff0c\u5e76\u5185\u7f6e\u4e86\u4e0eApollo\u548cTesla\u7b49\u5546\u4e1a\u5e73\u53f0\u7684\u517c\u5bb9\u6027\u3002\u5176\u5173\u952e\u7279\u6027\u662f\u4eba\u4e3a\u5e72\u9884\u80fd\u529b\uff1a\u9664\u4e86\u7075\u6d3b\u7684\u73af\u5883\u914d\u7f6e\u4ee5\u8fdb\u884c\u66f4\u5b9a\u5236\u5316\u7684\u8bc4\u4f30\u5916\uff0c\u5b83\u8fd8\u53ef\u4ee5\u5b9e\u65f6\u6355\u83b7\u6765\u81ea\u9a7e\u9a76\u5458\u7684\u751f\u7406\u4fe1\u53f7\u548c\u884c\u4e3a\u53cd\u9988\uff0c\u4ece\u800c\u5728\u5bf9\u6297\u6761\u4ef6\u4e0b\u63d0\u4f9b\u5bf9\u4eba\u673a\u4fe1\u4efb\u7684\u65b0\u89c1\u89e3\u3002", "result": "MetAdv\u80fd\u591f\u5bf9\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fdb\u884c\u7aef\u5230\u7aef\u7684\u5bf9\u6297\u6027\u8bc4\u4f30\uff0c\u5e76\u63d0\u4f9b\u5bf9\u4eba\u4e3a\u5e72\u9884\u7684\u6d1e\u5bdf\u3002", "conclusion": "MetAdv\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u6297\u6027\u8bc4\u4f30\uff0c\u4e3a\u66f4\u5b89\u5168\u7684\u81ea\u52a8\u9a7e\u9a76\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06636", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06636", "abs": "https://arxiv.org/abs/2508.06636", "authors": ["Baleegh Abdo", "Dongbing Shao", "Shayne Cairns", "Jae-woong Nah", "Oblesh Jinka", "Srikanth Srinivasan", "Thomas McConkey", "Vincent Arena", "Corrado Mancini"], "title": "Nondegenerate Josephson Mixers with Enhanced Bandwidth and Saturation Power for Quantum Signal Amplification and Transduction", "comment": null, "summary": "Nondegenerate Josephson mixers (JMs), formed by coupling two different\ntransmission-line resonators to Josephson ring modulators (JRMs), are vital and\nversatile devices capable of processing microwave signals at the quantum limit.\nOwing to the lossless nondegenerate three-wave mixing process enabled by the\nJRM, JMs can perform phase preserving amplification of quantum signals,\ngenerate two-mode squeezed states, and perform noiseless frequency conversion.\nHowever, due to their limited bandwidth and saturation power, such\nresonator-based JMs are generally unable to simultaneously process\nfrequency-multiplexed signals required in large quantum processors. To overcome\nthis longstanding dual challenge, we redesign the JRM parameters by optimizing\nits inductances to suppress higher order mixing products and engineer its\nelectromagnetic environment by incorporating lumped-element coupled-mode\nnetworks between the JRM and the two distinct ports of the JM. By implementing\nthese strategies, we measure for JMs realized with four coupled modes per port,\noperated in amplification (conversion), bandwidths of about 400 MHz (700 MHz)\nwith power reflections above 10 dB (below -10 dB) and saturation powers of\nabout -110 dBm at 15 dB (-91 dBm at -26 dB). Similarly, we demonstrate for a\nlow external quality factor resonant-mode JM operated in conversion, a maximum\nbandwidth of about $670$ MHz with power reflections below -10 dB and a maximum\nsaturation power of about -86 dBm at -17 dB. Such nondegenerate JMs with\nenhanced bandwidths and saturation powers could serve in a variety of\nfrequency-multiplexed settings ranging from high fidelity qubit readout and\nunidirectional routing of quantum signals to generation of remote entanglement\nwith continuous variables.", "AI": {"tldr": "\u65b0\u7ea6\u745f\u592b\u68ee\u6df7\u9891\u5668\u5e26\u5bbd\u548c\u9971\u548c\u529f\u7387\u7ffb\u500d\uff0c\u53ef\u7528\u4e8e\u91cf\u5b50\u6280\u672f\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u57fa\u4e8e\u8c10\u632f\u5668\u7684\u7ea6\u745f\u592b\u68ee\u6df7\u9891\u5668\uff08JMs\uff09\u5728\u5e26\u5bbd\u548c\u9971\u548c\u529f\u7387\u65b9\u9762\u7684\u9650\u5236\uff0c\u4ee5\u6ee1\u8db3\u5927\u578b\u91cf\u5b50\u5904\u7406\u5668\u4e2d\u5904\u7406\u9891\u7387\u591a\u8def\u590d\u7528\u4fe1\u53f7\u7684\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u4f18\u5316\u7ea6\u745f\u592b\u68ee\u73af\u8c03\u5236\u5668\uff08JRMs\uff09\u7684\u7535\u611f\u53c2\u6570\u4ee5\u6291\u5236\u9ad8\u9636\u6df7\u5408\u4ea7\u7269\uff0c\u5e76\u7ed3\u5408\u96c6\u603b\u5143\u4ef6\u8026\u5408\u6a21\u5f0f\u7f51\u7edc\u6765\u6539\u5584\u7535\u78c1\u73af\u5883\uff0c\u4ece\u800c\u91cd\u65b0\u8bbe\u8ba1\u4e86\u7ea6\u745f\u592b\u68ee\u6df7\u9891\u5668\u3002", "result": "\u5b9e\u73b0\u4e86\u7ea6\u745f\u592b\u68ee\u6df7\u9891\u5668\uff08JMs\uff09\u5728\u653e\u5927\uff08\u8f6c\u6362\uff09\u6a21\u5f0f\u4e0b\u7ea6400 MHz\uff08700 MHz\uff09\u7684\u5e26\u5bbd\uff0c\u529f\u7387\u53cd\u5c04\u9ad8\u4e8e10 dB\uff08\u4f4e\u4e8e-10 dB\uff09\uff0c\u4ee5\u53ca\u7ea6-110 dBm\uff08-91 dBm\uff09\u7684\u9971\u548c\u529f\u7387\u3002\u6b64\u5916\uff0c\u5728\u8f6c\u6362\u6a21\u5f0f\u4e0b\uff0c\u4f4e\u5916\u90e8\u54c1\u8d28\u56e0\u6570\u8c10\u632f\u6a21\u5f0f\u7ea6\u745f\u592b\u68ee\u6df7\u9891\u5668\u5b9e\u73b0\u4e86\u7ea6670 MHz\u7684\u6700\u5927\u5e26\u5bbd\uff0c\u529f\u7387\u53cd\u5c04\u4f4e\u4e8e-10 dB\uff0c\u6700\u5927\u9971\u548c\u529f\u7387\u7ea6\u4e3a-86 dBm\u3002", "conclusion": "\u4f18\u5316\u8bbe\u8ba1\u7684\u65e0\u9000\u5316\u7ea6\u745f\u592b\u68ee\u6df7\u9891\u5668\uff08JMs\uff09\u901a\u8fc7\u6291\u5236\u9ad8\u9636\u6df7\u5408\u4ea7\u7269\u548c\u6539\u8fdb\u7535\u78c1\u73af\u5883\uff0c\u5b9e\u73b0\u4e86\u66f4\u5bbd\u7684\u5e26\u5bbd\u548c\u66f4\u9ad8\u7684\u9971\u548c\u529f\u7387\uff0c\u53ef\u7528\u4e8e\u9ad8\u4fdd\u771f\u5ea6\u91cf\u5b50\u6bd4\u7279\u8bfb\u51fa\u3001\u91cf\u5b50\u4fe1\u53f7\u5355\u5411\u8def\u7531\u4ee5\u53ca\u8fde\u7eed\u53d8\u91cf\u8fdc\u7a0b\u7ea0\u7f20\u751f\u6210\u7b49\u5e94\u7528\u3002"}}
{"id": "2508.07252", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07252", "abs": "https://arxiv.org/abs/2508.07252", "authors": ["Siyuan He", "Peiran Yan", "Yandong He", "Youwei Zhuo", "Tianyu Jia"], "title": "Tasa: Thermal-aware 3D-Stacked Architecture Design with Bandwidth Sharing for LLM Inference", "comment": "Accepted by ICCAD'2025", "summary": "The autoregressive decoding in LLMs is the major inference bottleneck due to\nthe memory-intensive operations and limited hardware bandwidth. 3D-stacked\narchitecture is a promising solution with significantly improved memory\nbandwidth, which vertically stacked multi DRAM dies on top of logic die.\nHowever, our experiments also show the 3D-stacked architecture faces severer\nthermal issues compared to 2D architecture, in terms of thermal temperature,\ngradient and scalability. To better exploit the potential of 3D-stacked\narchitecture, we present Tasa, a heterogeneous architecture with cross-stack\nthermal optimizations to balance the temperature distribution and maximize the\nperformance under the thermal constraints. High-performance core is designed\nfor compute-intensive operations, while high-efficiency core is used for\nmemory-intensive operators, e.g. attention layers. Furthermore, we propose a\nbandwidth sharing scheduling to improve the bandwidth utilization in such\nheterogeneous architecture. Extensive thermal experiments show that our Tasa\narchitecture demonstrates greater scalability compared with the homogeneous\n3D-stacked architecture, i.e. up to 5.55 $\\tccentigrade$, 9.37 $\\tccentigrade$,\nand 7.91 $\\tccentigrade$ peak temperature reduction for 48, 60, and 72 core\nconfigurations. Our experimental for Llama-65B and GPT-3 66B inferences also\ndemonstrate 2.85x and 2.21x speedup are obtained over the GPU baselines and\nstate-of-the-art heterogeneous PIM-based LLM accelerator", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTasa\u7684\u5f02\u67843D\u5806\u53e0\u67b6\u6784\uff0c\u901a\u8fc7\u70ed\u4f18\u5316\u548c\u5e26\u5bbd\u5171\u4eab\u8c03\u5ea6\uff0c\u89e3\u51b3\u4e86LLM\u63a8\u7406\u4e2d\u7684\u5185\u5b58\u74f6\u9888\u548c\u70ed\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u6e29\u5ea6\u964d\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u75283D\u5806\u53e0\u67b6\u6784\u7684\u6f5c\u529b\uff0c\u5e76\u89e3\u51b3\u5176\u6bd42D\u67b6\u6784\u66f4\u4e25\u5cfb\u7684\u70ed\u95ee\u9898\uff08\u5305\u62ec\u6e29\u5ea6\u3001\u68af\u5ea6\u548c\u53ef\u6269\u5c55\u6027\uff09\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTasa\u7684\u5f02\u6784\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u91c7\u7528\u8de8\u5806\u6808\u70ed\u4f18\u5316\u6765\u5e73\u8861\u6e29\u5ea6\u5206\u5e03\uff0c\u5e76\u7ed3\u5408\u9ad8\u6027\u80fd\u6838\u5fc3\uff08\u7528\u4e8e\u8ba1\u7b97\u5bc6\u96c6\u578b\u64cd\u4f5c\uff09\u548c\u9ad8\u6548\u7387\u6838\u5fc3\uff08\u7528\u4e8e\u5185\u5b58\u5bc6\u96c6\u578b\u64cd\u4f5c\uff0c\u5982\u6ce8\u610f\u529b\u5c42\uff09\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5e26\u5bbd\u5171\u4eab\u8c03\u5ea6\u7b56\u7565\u6765\u63d0\u9ad8\u5e26\u5bbd\u5229\u7528\u7387\u3002", "result": "Tasa\u67b6\u6784\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u548c\u6e29\u5ea6\u964d\u4f4e\u3002\u572848\u300160\u548c72\u6838\u914d\u7f6e\u4e0b\uff0c\u5cf0\u503c\u6e29\u5ea6\u5206\u522b\u964d\u4f4e\u4e865.55\u00b0C\u30019.37\u00b0C\u548c7.91\u00b0C\u3002\u5728Llama-65B\u548cGPT-3 66B\u63a8\u7406\u65b9\u9762\uff0c\u4e0eGPU\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u5f02\u6784PIM LLM\u52a0\u901f\u5668\u76f8\u6bd4\uff0c\u5206\u522b\u5b9e\u73b0\u4e862.85\u500d\u548c2.21\u500d\u7684\u52a0\u901f\u3002", "conclusion": "Tasa\u67b6\u6784\u901a\u8fc7\u8de8\u5806\u6808\u70ed\u4f18\u5316\u5e73\u8861\u6e29\u5ea6\u5206\u5e03\uff0c\u6700\u5927\u5316\u70ed\u7ea6\u675f\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u5e26\u5bbd\u5171\u4eab\u8c03\u5ea6\u4ee5\u63d0\u9ad8\u5f02\u6784\u67b6\u6784\u7684\u5e26\u5bbd\u5229\u7528\u7387\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u540c\u8d283D\u5806\u53e0\u67b6\u6784\u76f8\u6bd4\uff0cTasa\u67b6\u6784\u572848\u300160\u548c72\u6838\u914d\u7f6e\u4e0b\u5206\u522b\u5b9e\u73b0\u4e86\u9ad8\u8fbe5.55\u00b0C\u30019.37\u00b0C\u548c7.91\u00b0C\u7684\u5cf0\u503c\u6e29\u5ea6\u964d\u4f4e\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728Llama-65B\u548cGPT-3 66B\u63a8\u7406\u65b9\u9762\u6bd4GPU\u57fa\u7ebf\u548c\u6700\u5148\u8fdb\u7684\u5f02\u6784PIM LLM\u52a0\u901f\u5668\u67092.85\u500d\u548c2.21\u500d\u7684\u52a0\u901f\u3002"}}
{"id": "2508.08198", "categories": ["cs.GR", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08198", "abs": "https://arxiv.org/abs/2508.08198", "authors": ["Yupeng Zhang", "Adam Alon", "M. Khalid Jawed"], "title": "Emergent morphogenesis via planar fabrication enabled by a reduced model of composites", "comment": "GitHub repository:\n  https://github.com/StructuresComp/discrete-shells-shrinky-dink/", "summary": "The ability to engineer complex three-dimensional shapes from planar sheets\nwith precise, programmable control underpins emerging technologies in soft\nrobotics, reconfigurable devices, and functional materials. Here, we present a\nreduced-order numerical and experimental framework for a bilayer system\nconsisting of a stimuli-responsive thermoplastic sheet (Shrinky Dink) bonded to\na kirigami-patterned, inert plastic layer. Upon uniform heating, the active\nlayer contracts while the patterned layer constrains in-plane stretch but\nallows out-of-plane bending, yielding programmable 3D morphologies from simple\nplanar precursors. Our approach enables efficient computational design and\nscalable manufacturing of 3D forms with a single-layer reduced model that\ncaptures the coupled mechanics of stretching and bending. Unlike traditional\nbilayer modeling, our framework collapses the multilayer composite into a\nsingle layer of nodes and elements, reducing the degrees of freedom and\nenabling simulation on a 2D geometry. This is achieved by introducing a novel\nenergy formulation that captures the coupling between in-plane stretch mismatch\nand out-of-plane bending - extending beyond simple isotropic linear elastic\nmodels. Experimentally, we establish a fully planar, repeatable fabrication\nprotocol using a stimuli-responsive thermoplastic and a laser-cut inert plastic\nlayer. The programmed strain mismatch drives an array of 3D morphologies, such\nas bowls, canoes, and flower petals, all verified by both simulation and\nphysical prototypes.", "AI": {"tldr": "\u901a\u8fc7\u52a0\u70ed\u54cd\u5e94\u6027\u8584\u819c\u548c kirigami \u5851\u6599\uff0c\u4ece\u5e73\u9762\u8584\u7247\u5236\u9020\u53ef\u7f16\u7a0b 3D \u5f62\u72b6\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u5728\u4e8e\u5b9e\u73b0\u5bf9\u590d\u6742\u4e09\u7ef4\u5f62\u72b6\u7684\u7cbe\u786e\u3001\u53ef\u7f16\u7a0b\u63a7\u5236\uff0c\u8fd9\u5bf9\u4e8e\u8f6f\u673a\u5668\u4eba\u3001\u53ef\u91cd\u6784\u8bbe\u5907\u548c\u529f\u80fd\u6750\u6599\u7b49\u65b0\u5174\u6280\u672f\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7b80\u5316\u7684\u6570\u503c\u548c\u5b9e\u9a8c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u591a\u5c42\u590d\u5408\u6750\u6599\u6298\u53e0\u6210\u5355\u5c42\u8282\u70b9\u548c\u5355\u5143\u6765\u51cf\u5c11\u81ea\u7531\u5ea6\uff0c\u4ece\u800c\u80fd\u591f\u5728\u4e00\u4e2a 2D \u51e0\u4f55\u5f62\u72b6\u4e0a\u8fdb\u884c\u6a21\u62df\u3002\u8fd9\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u80fd\u91cf\u516c\u5f0f\u6765\u5b9e\u73b0\uff0c\u8be5\u516c\u5f0f\u6355\u6349\u4e86\u9762\u5185\u62c9\u4f38\u5931\u914d\u548c\u9762\u5916\u5f2f\u66f2\u4e4b\u95f4\u7684\u8026\u5408\uff0c\u8d85\u8d8a\u4e86\u7b80\u5355\u7684\u5404\u5411\u540c\u6027\u7ebf\u6027\u5f39\u6027\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u5747\u5300\u52a0\u70ed\uff0c\u6240\u63d0\u51fa\u7684\u6846\u67b6\u80fd\u591f\u4ece\u7b80\u5355\u7684\u5e73\u9762\u524d\u4f53\u751f\u6210\u53ef\u7f16\u7a0b\u7684 3D \u5f62\u6001\uff0c\u5982\u7897\u3001\u72ec\u6728\u821f\u548c\u82b1\u74e3\u3002\u8fd9\u4e9b\u5f62\u6001\u5df2\u901a\u8fc7\u6a21\u62df\u548c\u7269\u7406\u539f\u578b\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u53cc\u5c42\u7cfb\u7edf\u7684\u6570\u503c\u548c\u5b9e\u9a8c\u6846\u67b6\uff0c\u8be5\u7cfb\u7edf\u7531\u523a\u6fc0\u54cd\u5e94\u6027\u70ed\u5851\u6027\u8584\u7247\uff08Shrinky Dink\uff09\u548c\u4e00\u4e2a kirigami \u56fe\u6848\u5316\u7684\u60f0\u6027\u5851\u6599\u5c42\u7ec4\u6210\u3002\u901a\u8fc7\u5747\u5300\u52a0\u70ed\uff0c\u4e3b\u52a8\u5c42\u6536\u7f29\uff0c\u800c\u56fe\u6848\u5316\u5c42\u9650\u5236\u9762\u5185\u62c9\u4f38\u4f46\u5141\u8bb8\u9762\u5916\u5f2f\u66f2\uff0c\u4ece\u800c\u4ece\u7b80\u5355\u7684\u5e73\u9762\u524d\u4f53\u83b7\u5f97\u53ef\u7f16\u7a0b\u7684 3D \u5f62\u6001\u3002"}}
{"id": "2508.06533", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06533", "abs": "https://arxiv.org/abs/2508.06533", "authors": ["Aamod Thakur", "Ajay Nagpal", "Atharva Savarkar", "Kundeshwar Pundalik", "Siddhesh Dosi", "Piyush Sawarkar", "Viraj Thakur", "Rohit Saluja", "Maunendra Sankar Desarkar", "Ganesh Ramakrishnan"], "title": "The Art of Breaking Words: Rethinking Multilingual Tokenizer Design", "comment": null, "summary": "While model architecture and training objectives are well-studied,\ntokenization, particularly in multilingual contexts, remains a relatively\nneglected aspect of Large Language Model (LLM) development. Existing tokenizers\noften exhibit high token-to-word ratios, inefficient use of context length, and\nslower inference. We present a systematic study that links vocabulary size,\npre-tokenization rules, and training-corpus composition to both token-to-word\nefficiency and model quality. To ground our analysis in a linguistically\ndiverse context, we conduct extensive experiments on Indic scripts, which\npresent unique challenges due to their high script diversity and orthographic\ncomplexity. Drawing on the insights from these analyses, we propose a novel\nalgorithm for data composition that balances multilingual data for tokenizer\ntraining. Our observations on pretokenization strategies significantly improve\nmodel performance, and our data composition algorithm reduces the average\ntoken-to-word ratio by approximately 6% with respect to the conventional data\nrandomization approach. Our tokenizer achieves more than 40% improvement on\naverage token-to-word ratio against stateof-the-art multilingual Indic models.\nThis improvement yields measurable gains in both model performance and\ninference speed. This highlights tokenization alongside architecture and\ntraining objectives as a critical lever for building efficient, scalable\nmultilingual LLMs", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u6539\u8fdb\u591a\u8bed\u8a00\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5206\u8bcd\u6280\u672f\uff0c\u901a\u8fc7\u7814\u7a76\u8bcd\u6c47\u91cf\u3001\u9884\u5206\u8bcd\u89c4\u5219\u548c\u8bad\u7ec3\u6570\u636e\u7ec4\u6210\u5bf9\u6a21\u578b\u6548\u7387\u548c\u8d28\u91cf\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5370\u5ea6\u8bed\u8a00\u811a\u672c\u4e0a\u3002\u63d0\u51fa\u4e86\u65b0\u7684\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u8bcd\u5143-\u5355\u8bcd\u6bd4\u7387\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u76ee\u6807\u662f\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5f00\u53d1\u4e2d\u7684\u91cd\u8981\u7814\u7a76\u65b9\u5411\uff0c\u4f46\u5206\u8bcd\u6280\u672f\uff0c\u5c24\u5176\u662f\u5728\u591a\u8bed\u8a00\u73af\u5883\u4e0b\uff0c\u5374\u76f8\u5bf9\u88ab\u5ffd\u89c6\u3002\u7136\u800c\uff0c\u5206\u8bcd\u6280\u672f\u76f4\u63a5\u5f71\u54cd\u7740LLM\u7684\u6548\u7387\u548c\u6027\u80fd\u3002", "method": "\u73b0\u6709\u5206\u8bcd\u5668\u5b58\u5728\u9ad8\u8bcd\u5143-\u5355\u8bcd\u6bd4\u7387\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u5229\u7528\u7387\u4f4e\u548c\u63a8\u7406\u901f\u5ea6\u6162\u7b49\u95ee\u9898\u3002\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u8bcd\u6c47\u91cf\u5927\u5c0f\u3001\u9884\u5206\u8bcd\u89c4\u5219\u548c\u8bad\u7ec3\u8bed\u6599\u5e93\u7ec4\u6210\u5bf9\u8bcd\u5143-\u5355\u8bcd\u6548\u7387\u548c\u6a21\u578b\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u5728\u5370\u5ea6\u8bed\u8a00\u811a\u672c\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\u6765\u5e73\u8861\u5206\u8bcd\u5668\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5206\u6790\u548c\u63d0\u51fa\u7684\u65b0\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\uff0c\u5e73\u5747\u8bcd\u5143-\u5355\u8bcd\u6bd4\u7387\u964d\u4f4e\u4e86\u7ea66%\uff0c\u6bd4\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5370\u5ea6\u8bed\u8a00\u6a21\u578b\u63d0\u9ad8\u4e8640%\u4ee5\u4e0a\u3002\u8fd9\u5e26\u6765\u4e86\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u7684\u53ef\u8861\u91cf\u6536\u76ca\u3002", "conclusion": "\u5728\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u76ee\u6807\u4e4b\u5916\uff0c\u91cd\u70b9\u5173\u6ce8\u5305\u62ec\u591a\u8bed\u8a00\u6a21\u578b\u5728\u5185\u7684\u7684\u8bed\u8a00\u6a21\u578b\u5f00\u53d1\u4e2d\u7684\u5206\u8bcd\u6280\u672f\uff0c\u7279\u522b\u662f\u591a\u8bed\u8a00\u6a21\u578b\u3002\u73b0\u6709\u7684\u5206\u8bcd\u5668\u901a\u5e38\u5177\u6709\u8f83\u9ad8\u7684\u8bcd\u5143\u4e0e\u5355\u8bcd\u4e4b\u6bd4\u3001\u4e0a\u4e0b\u6587\u957f\u5ea6\u4f7f\u7528\u6548\u7387\u4f4e\u4e0b\u4ee5\u53ca\u63a8\u7406\u901f\u5ea6\u8f83\u6162\u7684\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6027\u7814\u7a76\uff0c\u5c06\u8bcd\u6c47\u91cf\u5927\u5c0f\u3001\u9884\u5206\u8bcd\u89c4\u5219\u4ee5\u53ca\u8bad\u7ec3\u8bed\u6599\u5e93\u7ec4\u6210\u4e0e\u8bcd\u5143-\u5355\u8bcd\u6548\u7387\u548c\u6a21\u578b\u8d28\u91cf\u8054\u7cfb\u8d77\u6765\u3002\u4e3a\u4e86\u5728\u8bed\u8a00\u591a\u6837\u7684\u73af\u5883\u4e2d\u8fdb\u884c\u5206\u6790\uff0c\u6211\u4eec\u5728\u5370\u5ea6\u8bed\u8a00\u811a\u672c\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8fd9\u4e9b\u811a\u672c\u56e0\u5176\u9ad8\u5ea6\u7684\u811a\u672c\u591a\u6837\u6027\u548c\u590d\u6742\u7684\u62fc\u5199\u800c\u5e26\u6765\u72ec\u7279\u7684\u6311\u6218\u3002\u6839\u636e\u8fd9\u4e9b\u5206\u6790\u7684\u89c1\u89e3\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\uff0c\u4ee5\u5e73\u8861\u5206\u8bcd\u5668\u8bad\u7ec3\u7684\u591a\u8bed\u8a00\u6570\u636e\u3002\u6211\u4eec\u5728\u9884\u5206\u8bcd\u7b56\u7565\u4e0a\u7684\u89c2\u5bdf\u663e\u8457\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u6211\u4eec\u7684\u6570\u636e\u7ec4\u6210\u7b97\u6cd5\u76f8\u5bf9\u4e8e\u4f20\u7edf\u6570\u636e\u968f\u673a\u5316\u65b9\u6cd5\uff0c\u5c06\u5e73\u5747\u8bcd\u5143-\u5355\u8bcd\u6bd4\u7387\u964d\u4f4e\u4e86\u7ea66%\u3002\u6211\u4eec\u7684\u5206\u8bcd\u5668\u5728\u6bd4\u6700\u5148\u8fdb\u7684\u591a\u8bed\u8a00\u5370\u5ea6\u8bed\u8a00\u6a21\u578b\u9ad8\u51fa40%\u7684\u5e73\u5747\u8bcd\u5143-\u5355\u8bcd\u6bd4\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002\u8fd9\u4e00\u6539\u8fdb\u5728\u6a21\u578b\u6027\u80fd\u548c\u63a8\u7406\u901f\u5ea6\u4e0a\u90fd\u5e26\u6765\u4e86\u53ef\u8861\u91cf\u7684\u6536\u76ca\u3002\u8fd9\u7a81\u51fa\u4e86\u5206\u8bcd\u4e0e\u67b6\u6784\u548c\u8bad\u7ec3\u76ee\u6807\u4e00\u8d77\uff0c\u6210\u4e3a\u6784\u5efa\u9ad8\u6548\u3001\u53ef\u6269\u5c55\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u7684\u91cd\u8981\u6760\u6746\u3002"}}
{"id": "2508.06661", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06661", "abs": "https://arxiv.org/abs/2508.06661", "authors": ["Keith Badger", "Marek Petrik", "Jefferson Huang"], "title": "Convergence of Fast Policy Iteration in Markov Games and Robust MDPs", "comment": null, "summary": "Markov games and robust MDPs are closely related models that involve\ncomputing a pair of saddle point policies. As part of the long-standing effort\nto develop efficient algorithms for these models, the Filar-Tolwinski (FT)\nalgorithm has shown considerable promise. As our first contribution, we\ndemonstrate that FT may fail to converge to a saddle point and may loop\nindefinitely, even in small games. This observation contradicts the proof of\nFT's convergence to a saddle point in the original paper. As our second\ncontribution, we propose Residual Conditioned Policy Iteration (RCPI). RCPI\nbuilds on FT, but is guaranteed to converge to a saddle point. Our numerical\nresults show that RCPI outperforms other convergent algorithms by several\norders of magnitude.", "AI": {"tldr": "FT\u7b97\u6cd5\u5728\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u548c\u9c81\u68d2MDP\u4e2d\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\u5230\u978d\u70b9\u3002\u65b0\u63d0\u51fa\u7684RCPI\u7b97\u6cd5\u89e3\u51b3\u4e86\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e76\u4e14\u6027\u80fd\u66f4\u4f18\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u66f4\u6709\u6548\u7684\u7b97\u6cd5\u6765\u8ba1\u7b97\u9a6c\u5c14\u53ef\u592b\u535a\u5f08\u548c\u9c81\u68d2MDP\u4e2d\u7684\u978d\u70b9\u7b56\u7565\uff0c\u5e76\u4e14\u6307\u51faFT\u7b97\u6cd5\u53ef\u80fd\u5b58\u5728\u6536\u655b\u95ee\u9898\u3002", "method": "\u63d0\u51faResidual Conditioned Policy Iteration (RCPI)\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5728Filar-Tolwinski (FT)\u7b97\u6cd5\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u8fdb\uff0c\u4ee5\u89e3\u51b3FT\u7b97\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\u5230\u978d\u70b9\u7684\u95ee\u9898\u3002", "result": "FT\u7b97\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\u5230\u978d\u70b9\uff0c\u800cRCPI\u7b97\u6cd5\u4fdd\u8bc1\u6536\u655b\uff0c\u5e76\u4e14\u5728\u6570\u503c\u7ed3\u679c\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6536\u655b\u7b97\u6cd5\u3002", "conclusion": "Filar-Tolwinski (FT)\u7b97\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6536\u655b\u5230\u978d\u70b9\uff0c\u751a\u81f3\u5728\u5c0f\u578b\u535a\u5f08\u4e2d\u53ef\u80fd\u65e0\u9650\u5faa\u73af\u3002Residual Conditioned Policy Iteration (RCPI)\u7b97\u6cd5\u5728FT\u7b97\u6cd5\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u4fdd\u8bc1\u6536\u655b\u5230\u978d\u70b9\uff0c\u5e76\u4e14\u5728\u6570\u503c\u7ed3\u679c\u4e0a\u663e\u793a\u5176\u6027\u80fd\u6bd4\u5176\u4ed6\u6536\u655b\u7b97\u6cd5\u9ad8\u51fa\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002"}}
{"id": "2508.07767", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.07767", "abs": "https://arxiv.org/abs/2508.07767", "authors": ["Marc Mart\u00ed-Sabat\u00e9", "Benjamin Vial", "Richard Wiltshaw", "S\u00e9bastien Guenneau", "Richard V. Craster"], "title": "Fabry-P\u00e9rot quasinormal modes for topological edge states", "comment": "6 pages, 4 figures", "summary": "Topological waveguides supporting quantum valley Hall interfacial states\nconfine waves to interfaces and, due to topological protection, are resistant\nto backscattering even in the presence of defects. These topological insulators\nare typically studied by means of an infinite spectral problem. However,\npractical implementations are necessarily finite. In this work, we propose an\nalternative framework for analysing topologically non-trivial states in open,\nfinite systems. Our approach is based on a Quasinormal Modal Expansion Method\n(QMEM), which directly characterizes the existence and excitation of these\nmodes within the open system. The resulting spectrum is complex and discrete\nand fully describes the topologically non-trivial states, revealing an analogy\nof topological mode steering as a dispersive Fabry-P\\'erot cavity, with a\ndispersion relation closely related to that of the corresponding infinite\n(Floquet-Bloch) ribbon problem. Our results illustrate how topologically\nprotected waveguiding can be understood in terms of leaky cavity modes and\noffers a powerful framework for analysing finite topological devices.", "AI": {"tldr": "Topological waveguides are usually studied in infinite systems, but this paper introduces a new method (QMEM) for finite systems, showing topological states are like leaky cavity modes.", "motivation": "To propose an alternative framework for analyzing topologically non-trivial states in open, finite systems, as practical implementations are finite, unlike typical studies of topological insulators which use infinite spectral problems.", "method": "Quasinormal Modal Expansion Method (QMEM)", "result": "The QMEM directly characterizes the existence and excitation of topological modes in open systems. The complex and discrete spectrum reveals an analogy of topological mode steering with a Fabry-P\u00e9rot cavity, showing a dispersion relation related to the infinite ribbon problem.", "conclusion": "The study proposes a Quasinormal Modal Expansion Method (QMEM) to analyze topologically non-trivial states in open, finite systems, offering a new framework for understanding topological mode steering and analyzing finite topological devices."}}
{"id": "2508.07077", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07077", "abs": "https://arxiv.org/abs/2508.07077", "authors": ["Gustavo V. Nascimento", "Ivan R. Meneghini", "Val\u00e9ria Santos", "Eduardo Luz", "Gladston Moreira"], "title": "Enhancing Decision Space Diversity in Multi-Objective Evolutionary Optimization for the Diet Problem", "comment": "12 pages", "summary": "Multi-objective evolutionary algorithms (MOEAs) are essential for solving\ncomplex optimization problems, such as the diet problem, where balancing\nconflicting objectives, like cost and nutritional content, is crucial. However,\nmost MOEAs focus on optimizing solutions in the objective space, often\nneglecting the diversity of solutions in the decision space, which is critical\nfor providing decision-makers with a wide range of choices. This paper\nintroduces an approach that directly integrates a Hamming distance-based\nmeasure of uniformity into the selection mechanism of a MOEA to enhance\ndecision space diversity. Experiments on a multi-objective formulation of the\ndiet problem demonstrate that our approach significantly improves decision\nspace diversity compared to NSGA-II, while maintaining comparable objective\nspace performance. The proposed method offers a generalizable strategy for\nintegrating decision space awareness into MOEAs.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u6c49\u660e\u8ddd\u79bb\u5747\u5300\u6027\u5ea6\u91cf\u6574\u5408\u5230MOEA\u9009\u62e9\u673a\u5236\u4e2d\u7684\u65b9\u6cd5\uff0c\u4ee5\u589e\u5f3a\u51b3\u7b56\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u5e76\u5728\u98df\u7269\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u6bd4NSGA-II\u66f4\u597d\u7684\u6548\u679c\u3002", "motivation": "\u5927\u591a\u6570MOEA\u5728\u4f18\u5316\u76ee\u6807\u7a7a\u95f4\u89e3\u7684\u540c\u65f6\uff0c\u5f80\u5f80\u5ffd\u7565\u4e86\u51b3\u7b56\u7a7a\u95f4\u89e3\u7684\u591a\u6837\u6027\uff0c\u800c\u8fd9\u5bf9\u4e8e\u4e3a\u51b3\u7b56\u8005\u63d0\u4f9b\u5e7f\u6cdb\u7684\u9009\u62e9\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c06\u57fa\u4e8e\u6c49\u660e\u8ddd\u79bb\u7684\u5747\u5300\u6027\u5ea6\u91cf\u76f4\u63a5\u96c6\u6210\u5230MOEA\u7684\u9009\u62e9\u673a\u5236\u4e2d\u6765\u589e\u5f3a\u51b3\u7b56\u7a7a\u95f4\u7684\u591a\u6837\u6027\u3002", "result": "\u5728\u591a\u76ee\u6807\u98df\u7269\u95ee\u9898\u5236\u5b9a\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u8bc1\u660e\u8be5\u65b9\u6cd5\u4e0eNSGA-II\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51b3\u7b56\u7a7a\u95f4\u591a\u6837\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u53ef\u6bd4\u7684\u76ee\u6807\u7a7a\u95f4\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5c06\u51b3\u7b56\u7a7a\u95f4\u610f\u8bc6\u7eb3\u5165MOEA\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u7b56\u7565\u3002"}}
{"id": "2508.07205", "categories": ["cs.SI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07205", "abs": "https://arxiv.org/abs/2508.07205", "authors": ["Chaoqun Cui", "Caiyan Jia"], "title": "Towards Real-World Rumor Detection: Anomaly Detection Framework with Graph Supervised Contrastive Learning", "comment": "This paper is accepted by COLING2025", "summary": "Current rumor detection methods based on propagation structure learning\npredominately treat rumor detection as a class-balanced classification task on\nlimited labeled data. However, real-world social media data exhibits an\nimbalanced distribution with a minority of rumors among massive regular posts.\nTo address the data scarcity and imbalance issues, we construct two large-scale\nconversation datasets from Weibo and Twitter and analyze the domain\ndistributions. We find obvious differences between rumor and non-rumor\ndistributions, with non-rumors mostly in entertainment domains while rumors\nconcentrate in news, indicating the conformity of rumor detection to an anomaly\ndetection paradigm. Correspondingly, we propose the Anomaly Detection framework\nwith Graph Supervised Contrastive Learning (AD-GSCL). It heuristically treats\nunlabeled data as non-rumors and adapts graph contrastive learning for rumor\ndetection. Extensive experiments demonstrate AD-GSCL's superiority under\nclass-balanced, imbalanced, and few-shot conditions. Our findings provide\nvaluable insights for real-world rumor detection featuring imbalanced data\ndistributions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAD-GSCL\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u8c23\u8a00\u68c0\u6d4b\u89c6\u4e3a\u5f02\u5e38\u68c0\u6d4b\u95ee\u9898\uff0c\u5e76\u5229\u7528\u56fe\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6765\u5904\u7406\u6570\u636e\u4e0d\u5e73\u8861\u548c\u7a00\u758f\u6027\u95ee\u9898\uff0c\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u771f\u5b9e\u4e16\u754c\u793e\u4ea4\u5a92\u4f53\u6570\u636e\u5b58\u5728\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u8c23\u8a00\u6570\u91cf\u8fdc\u5c11\u4e8e\u6b63\u5e38\u5e16\u5b50\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u6709\u9650\u7684\u6807\u8bb0\u6570\u636e\u4e0a\u5c06\u8c23\u8a00\u68c0\u6d4b\u89c6\u4e3a\u7c7b\u522b\u5e73\u8861\u5206\u7c7b\u4efb\u52a1\uff0c\u672a\u80fd\u6709\u6548\u89e3\u51b3\u6570\u636e\u7a00\u758f\u548c\u4e0d\u5e73\u8861\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAD-GSCL\uff08Anomaly Detection framework with Graph Supervised Contrastive Learning\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u65e0\u6807\u7b7e\u6570\u636e\u89c6\u4e3a\u975e\u8c23\u8a00\uff0c\u5e76\u91c7\u7528\u56fe\u76d1\u7763\u5bf9\u6bd4\u5b66\u4e60\u6765\u8fdb\u884c\u8c23\u8a00\u68c0\u6d4b\u3002", "result": "AD-GSCL\u5728\u7c7b\u522b\u5e73\u8861\u3001\u4e0d\u5e73\u8861\u548c\u5c11\u6837\u672c\u6761\u4ef6\u4e0b\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5177\u6709\u4e0d\u5e73\u8861\u6570\u636e\u5206\u5e03\u7684\u771f\u5b9e\u4e16\u754c\u8c23\u8a00\u68c0\u6d4b\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.06517", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06517", "abs": "https://arxiv.org/abs/2508.06517", "authors": ["Haoran Xi", "Chen Liu", "Xiaolin Li"], "title": "Frequency Prior Guided Matching: A Data Augmentation Approach for Generalizable Semi-Supervised Polyp Segmentation", "comment": "19 pages, 8 figures, 6 tables", "summary": "Automated polyp segmentation is essential for early diagnosis of colorectal\ncancer, yet developing robust models remains challenging due to limited\nannotated data and significant performance degradation under domain shift.\nAlthough semi-supervised learning (SSL) reduces annotation requirements,\nexisting methods rely on generic augmentations that ignore polyp-specific\nstructural properties, resulting in poor generalization to new imaging centers\nand devices. To address this, we introduce Frequency Prior Guided Matching\n(FPGM), a novel augmentation framework built on a key discovery: polyp edges\nexhibit a remarkably consistent frequency signature across diverse datasets.\nFPGM leverages this intrinsic regularity in a two-stage process. It first\nlearns a domain-invariant frequency prior from the edge regions of labeled\npolyps. Then, it performs principled spectral perturbations on unlabeled\nimages, aligning their amplitude spectra with this learned prior while\npreserving phase information to maintain structural integrity. This targeted\nalignment normalizes domain-specific textural variations, thereby compelling\nthe model to learn the underlying, generalizable anatomical structure.\nValidated on six public datasets, FPGM establishes a new state-of-the-art\nagainst ten competing methods. It demonstrates exceptional zero-shot\ngeneralization capabilities, achieving over 10% absolute gain in Dice score in\ndata-scarce scenarios. By significantly enhancing cross-domain robustness, FPGM\npresents a powerful solution for clinically deployable polyp segmentation under\nlimited supervision.", "AI": {"tldr": "FPGM\u662f\u4e00\u79cd\u65b0\u7684\u534a\u76d1\u7763\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u606f\u8089\u8fb9\u7f18\u7684\u9891\u7387\u7279\u5f81\u6765\u63d0\u9ad8\u6a21\u578b\u5728\u4e0d\u540c\u6210\u50cf\u8bbe\u5907\u548c\u6570\u636e\u6e90\u4e0b\u7684\u5206\u5272\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u901a\u7528\u7684\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5ffd\u7565\u4e86\u606f\u8089\u7279\u6709\u7684\u7ed3\u6784\u5c5e\u6027\uff0c\u5bfc\u81f4\u5728\u65b0\u7684\u6210\u50cf\u4e2d\u5fc3\u548c\u8bbe\u5907\u4e0a\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002\u800cFPGM\u65e8\u5728\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u901a\u8fc7\u5229\u7528\u606f\u8089\u8fb9\u7f18\u7684\u9891\u7387\u7279\u5f81\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFPGM\uff08Frequency Prior Guided Matching\uff09\u7684\u65b0\u9896\u7684\u589e\u5f3a\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u606f\u8089\u8fb9\u7f18\u5728\u4e0d\u540c\u6570\u636e\u96c6\u4e2d\u5177\u6709\u4e00\u81f4\u7684\u9891\u7387\u7279\u5f81\u8fd9\u4e00\u5173\u952e\u53d1\u73b0\u3002FPGM\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\uff1a\u9996\u5148\uff0c\u4ece\u6807\u8bb0\u606f\u8089\u7684\u8fb9\u7f18\u533a\u57df\u5b66\u4e60\u9886\u57df\u4e0d\u53d8\u7684\u9891\u7387\u5148\u9a8c\uff1b\u7136\u540e\uff0c\u5bf9\u672a\u6807\u8bb0\u56fe\u50cf\u8fdb\u884c\u539f\u5219\u6027\u7684\u9891\u8c31\u6270\u52a8\uff0c\u4f7f\u5176\u5e45\u5ea6\u9891\u8c31\u4e0e\u5b66\u4e60\u5230\u7684\u5148\u9a8c\u5bf9\u9f50\uff0c\u540c\u65f6\u4fdd\u7559\u76f8\u4f4d\u4fe1\u606f\u4ee5\u7ef4\u6301\u7ed3\u6784\u5b8c\u6574\u6027\u3002", "result": "FPGM\u5728\u516d\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u5728\u4e0e\u5341\u79cd\u73b0\u6709\u65b9\u6cd5\u8fdb\u884c\u6bd4\u8f83\u65f6\uff0c\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\u3002\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\uff0c\u5176Dice\u5206\u6570\u63d0\u9ad8\u4e8610%\u4ee5\u4e0a\uff0c\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "FPGM\u901a\u8fc7\u589e\u5f3a\u8de8\u57df\u9c81\u68d2\u6027\uff0c\u4e3a\u6709\u9650\u76d1\u7763\u4e0b\u7684\u4e34\u5e8a\u7ed3\u80a0\u606f\u8089\u5206\u5272\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07714", "categories": ["cs.CV", "cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.07714", "abs": "https://arxiv.org/abs/2508.07714", "authors": ["Licheng Zhang", "Bach Le", "Naveed Akhtar", "Tuan Ngo"], "title": "DoorDet: Semi-Automated Multi-Class Door Detection Dataset via Object Detection and Large Language Models", "comment": null, "summary": "Accurate detection and classification of diverse door types in floor plans\ndrawings is critical for multiple applications, such as building compliance\nchecking, and indoor scene understanding. Despite their importance, publicly\navailable datasets specifically designed for fine-grained multi-class door\ndetection remain scarce. In this work, we present a semi-automated pipeline\nthat leverages a state-of-the-art object detector and a large language model\n(LLM) to construct a multi-class door detection dataset with minimal manual\neffort. Doors are first detected as a unified category using a deep object\ndetection model. Next, an LLM classifies each detected instance based on its\nvisual and contextual features. Finally, a human-in-the-loop stage ensures\nhigh-quality labels and bounding boxes. Our method significantly reduces\nannotation cost while producing a dataset suitable for benchmarking neural\nmodels in floor plan analysis. This work demonstrates the potential of\ncombining deep learning and multimodal reasoning for efficient dataset\nconstruction in complex real-world domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5bf9\u8c61\u68c0\u6d4b\u5668\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u534a\u81ea\u52a8\u5316\u65b9\u6cd5\uff0c\u4ee5\u4f4e\u6210\u672c\u6784\u5efa\u4e86\u7528\u4e8e\u697c\u5c42\u5e73\u9762\u56fe\u95e8\u68c0\u6d4b\u548c\u5206\u7c7b\u7684\u6570\u636e\u96c6\u3002", "motivation": "\u51c6\u786e\u68c0\u6d4b\u548c\u5206\u7c7b\u697c\u5c42\u5e73\u9762\u56fe\u4e2d\u5404\u79cd\u7c7b\u578b\u7684\u95e8\u5bf9\u4e8e\u5efa\u7b51\u5408\u89c4\u6027\u68c0\u67e5\u548c\u5ba4\u5185\u573a\u666f\u7406\u89e3\u7b49\u591a\u79cd\u5e94\u7528\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u7f3a\u4e4f\u4e13\u95e8\u7528\u4e8e\u7ec6\u7c92\u5ea6\u591a\u7c7b\u522b\u95e8\u68c0\u6d4b\u7684\u516c\u5f00\u6570\u636e\u96c6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u81ea\u52a8\u5316\u7684\u6d41\u7a0b\uff0c\u9996\u5148\u5229\u7528\u5148\u8fdb\u7684\u76ee\u6807\u68c0\u6d4b\u5668\u5c06\u6240\u6709\u95e8\u68c0\u6d4b\u4e3a\u7edf\u4e00\u7c7b\u522b\uff0c\u7136\u540e\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6839\u636e\u89c6\u89c9\u548c\u4e0a\u4e0b\u6587\u7279\u5f81\u5bf9\u6bcf\u4e2a\u68c0\u6d4b\u5230\u7684\u95e8\u5b9e\u4f8b\u8fdb\u884c\u5206\u7c7b\uff0c\u6700\u540e\u901a\u8fc7\u4eba\u5de5\u5ba1\u6838\u9636\u6bb5\u786e\u4fdd\u6807\u7b7e\u548c\u8fb9\u754c\u6846\u7684\u9ad8\u8d28\u91cf\u3002", "result": "\u8be5\u7814\u7a76\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u591a\u7c7b\u522b\u95e8\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u663e\u8457\u964d\u4f4e\u6807\u6ce8\u6210\u672c\uff0c\u5e76\u9002\u7528\u4e8e\u697c\u5c42\u5e73\u9762\u5206\u6790\u4e2d\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u5b66\u4e60\u548c\u591a\u6a21\u6001\u63a8\u7406\uff0c\u80fd\u591f\u4ee5\u6700\u5c0f\u7684\u4eba\u5de5\u6210\u672c\u6784\u5efa\u7528\u4e8e\u697c\u5c42\u5e73\u9762\u5206\u6790\u7684\u591a\u7c7b\u522b\u95e8\u68c0\u6d4b\u6570\u636e\u96c6\uff0c\u9002\u7528\u4e8e\u57fa\u51c6\u6d4b\u8bd5\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u3002"}}
{"id": "2508.06864", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06864", "abs": "https://arxiv.org/abs/2508.06864", "authors": ["Bing Li", "Haoming Guo", "Zhiyuan Ren", "Wenchi Cheng", "Jialin Hu", "Xinke Jian"], "title": "Collaborative Computing Strategy Based SINS Prediction for Emergency UAVs Network", "comment": null, "summary": "In emergency scenarios, the dynamic and harsh conditions necessitate timely\ntrajectory adjustments for drones, leading to highly dynamic network topologies\nand potential task failures. To address these challenges, a collaborative\ncomputing strategy based strapdown inertial navigation system (SINS) prediction\nfor emergency UAVs network (EUN) is proposed, where a two-step weighted time\nexpanded graph (WTEG) is constructed to deal with dynamic network topology\nchanges. Furthermore, the task scheduling is formulated as a Directed Acyclic\nGraph (DAG) to WTEG mapping problem to achieve collaborative computing while\ntransmitting among UAVs. Finally, the binary particle swarm optimization (BPSO)\nalgorithm is employed to choose the mapping strategy that minimizes end-to-end\nprocessing latency. The simulation results validate that the collaborative\ncomputing strategy significantly outperforms both cloud and local computing in\nterms of latency. Moreover, the task success rate using SINS is substantially\nimproved compared to approaches without prior prediction.", "AI": {"tldr": "\u4e3a\u5e94\u5bf9\u7d27\u6025\u65e0\u4eba\u673a\u7f51\u7edc\u52a8\u6001\u62d3\u6251\u548c\u6f5c\u5728\u4efb\u52a1\u5931\u8d25\uff0c\u63d0\u51fa\u57fa\u4e8eSINS\u9884\u6d4b\u7684\u534f\u540c\u8ba1\u7b97\u7b56\u7565\u3002\u901a\u8fc7WTEG\u5904\u7406\u62d3\u6251\u53d8\u5316\uff0c\u5c06\u4efb\u52a1\u8c03\u5ea6\u6620\u5c04\u5230WTEG\uff0c\u5e76\u7528BPSO\u4f18\u5316\u5ef6\u8fdf\u3002\u7ed3\u679c\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7b56\u7565\u5728\u964d\u4f4e\u5ef6\u8fdf\u548c\u63d0\u9ad8\u4efb\u52a1\u6210\u529f\u7387\u65b9\u9762\u6548\u679c\u663e\u8457\u3002", "motivation": "\u5728\u7d27\u6025\u573a\u666f\u4e0b\uff0c\u65e0\u4eba\u673a\u9700\u8981\u8fdb\u884c\u53ca\u65f6\u7684\u8f68\u8ff9\u8c03\u6574\uff0c\u5bfc\u81f4\u7f51\u7edc\u62d3\u6251\u52a8\u6001\u53d8\u5316\uff0c\u53ef\u80fd\u5f15\u8d77\u4efb\u52a1\u5931\u8d25\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5e94\u5bf9\u52a8\u6001\u73af\u5883\u5e76\u4fdd\u8bc1\u4efb\u52a1\u6210\u529f\u7387\u7684\u534f\u540c\u8ba1\u7b97\u7b56\u7565\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eSINS\u9884\u6d4b\u7684\u534f\u540c\u8ba1\u7b97\u7b56\u7565\uff0c\u5e76\u6784\u5efa\u4e86\u4e24\u6b65\u52a0\u6743\u65f6\u7a7a\u6269\u5c55\u56fe\uff08WTEG\uff09\u6765\u5904\u7406\u52a8\u6001\u7f51\u7edc\u62d3\u6251\u3002\u4efb\u52a1\u8c03\u5ea6\u88ab\u8868\u8ff0\u4e3a\u6709\u5411\u65e0\u73af\u56fe\uff08DAG\uff09\u5230WTEG\u7684\u6620\u5c04\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u4e8c\u5143\u7c92\u5b50\u7fa4\u4f18\u5316\uff08BPSO\uff09\u7b97\u6cd5\u6765\u6700\u5c0f\u5316\u7aef\u5230\u7aef\u5904\u7406\u5ef6\u8fdf\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u534f\u540c\u8ba1\u7b97\u7b56\u7565\u5728\u5ef6\u8fdf\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u4e91\u8ba1\u7b97\u548c\u672c\u5730\u8ba1\u7b97\u3002\u4e0e\u6ca1\u6709\u9884\u6d4b\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u4f7f\u7528SINS\u9884\u6d4b\u7684\u4efb\u52a1\u6210\u529f\u7387\u4e5f\u5f97\u5230\u4e86\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u7684\u534f\u540c\u8ba1\u7b97\u7b56\u7565\u901a\u8fc7\u4f7f\u7528SINS\u9884\u6d4b\u548cWTEG\u56fe\uff0c\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u65e0\u4eba\u673a\u7f51\u7edc\u52a8\u6001\u62d3\u6251\u53d8\u5316\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u7aef\u5230\u7aef\u5904\u7406\u5ef6\u8fdf\uff0c\u5e76\u5927\u5e45\u63d0\u9ad8\u4e86\u4efb\u52a1\u6210\u529f\u7387\uff0c\u4f18\u4e8e\u4f20\u7edf\u4e91\u8ba1\u7b97\u548c\u672c\u5730\u8ba1\u7b97\u65b9\u6cd5\u3002"}}
{"id": "2508.06818", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "cond-mat.other"], "pdf": "https://arxiv.org/pdf/2508.06818", "abs": "https://arxiv.org/abs/2508.06818", "authors": ["Huahui Qiu", "Shuaishuai Tong", "Qicheng Zhang", "Kun Zhang", "Chunyin Qiu"], "title": "Observation of anomalous Floquet non-Abelian topological insulators", "comment": null, "summary": "Non-Abelian topological phases, which go beyond traditional Abelian\ntopological band theory, are garnering increasing attention. This is further\nspurred by periodic driving, leading to predictions of many novel multi-gap\nFloquet topological phases, including anomalous Euler and Dirac string phases\ninduced by non-Abelian Floquet braiding, as well as Floquet non-Abelian\ntopological insulators (FNTIs) that exhibit multifold bulk-edge correspondence.\nHere, we report the first experimental realization of anomalous FNTIs, which\ndemonstrate topological edge modes in all three gaps despite having a trivial\nbulk charge. Concretely, we construct an experimentally feasible\none-dimensional three-band Floquet model and implement it in acoustics by\nintegrating time-periodic coupling circuits to static acoustic crystals.\nFurthermore, we observe counterintuitive topological interface modes in the\ndomain-wall formed by an anomalous FNTI and its counterpart with swapped\ndriving sequences, modes previously inaccessible in Floquet Abelian systems.\nOur work paves the way for further experimental exploration of the uncharted\nnon-equilibrium topological physics.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u58f0\u5b66\u9886\u57df\u901a\u8fc7\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u53cd\u5e38\u975e\u963f\u8d1d\u5c14\u5085\u7acb\u53f6\u62d3\u6251\u7edd\u7f18\u4f53\uff08FNTIs\uff09\uff0c\u5c55\u793a\u4e86\u5728\u5e73\u51e1\u6574\u4f53\u7535\u8377\u4e0b\u8de8\u8d8a\u6240\u6709\u4e09\u4e2a\u80fd\u9699\u7684\u62d3\u6251\u8fb9\u7f18\u6a21\u5f0f\uff0c\u5e76\u89c2\u5bdf\u5230\u4e86\u4ee5\u524d\u65e0\u6cd5\u5b9e\u73b0\u7684\u62d3\u6251\u754c\u9762\u6a21\u5f0f\u3002", "motivation": "\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u76f8\uff08\u8d85\u8d8a\u4f20\u7edf\u7684\u963f\u8d1d\u5c14\u62d3\u6251\u80fd\u5e26\u7406\u8bba\uff09\u6b63\u53d7\u5230\u8d8a\u6765\u8d8a\u591a\u7684\u5173\u6ce8\uff0c\u800c\u5468\u671f\u6027\u9a71\u52a8\u7684\u5f15\u5165\u66f4\u662f\u6fc0\u53d1\u4e86\u5bf9\u8bb8\u591a\u65b0\u9896\u591a\u80fd\u9699\u5085\u7acb\u53f6\u62d3\u6251\u76f8\u7684\u9884\u6d4b\uff0c\u5305\u62ec\u7531\u975e\u963f\u8d1d\u5c14\u5085\u7acb\u53f6\u7f16\u7ec7\u5f15\u8d77\u7684\u53cd\u5e38\u6b27\u62c9\u548c\u72c4\u62c9\u514b\u5f26\u76f8\u4f4d\uff0c\u4ee5\u53ca\u8868\u73b0\u51fa\u591a\u91cd\u4f53\u8fb9\u5bf9\u5e94\u5173\u7cfb\u7684\u5085\u7acb\u53f6\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u7edd\u7f18\u4f53\uff08FNTIs\uff09\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u65f6\u95f4\u5468\u671f\u6027\u8026\u5408\u7535\u8def\u96c6\u6210\u5230\u9759\u6001\u58f0\u5b66\u6676\u4f53\u4e2d\uff0c\u5728\u58f0\u5b66\u9886\u57df\u6784\u5efa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e00\u7ef4\u4e09\u5e26\u5085\u7acb\u53f6\u6a21\u578b\uff0c\u7528\u4e8e\u5b9e\u9a8c\u9a8c\u8bc1\u53cd\u5e38\u975e\u963f\u8d1d\u5c14\u5085\u7acb\u53f6\u62d3\u6251\u7edd\u7f18\u4f53\uff08FNTIs\uff09\u3002", "result": "\u672c\u7814\u7a76\u9996\u6b21\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u53cd\u5e38FNTIs\uff0c\u5728\u4e09\u4e2a\u80fd\u9699\u4e2d\u5747\u5c55\u793a\u4e86\u62d3\u6251\u8fb9\u7f18\u6a21\u5f0f\uff0c\u4e14\u6574\u4f53\u7535\u8377\u5e73\u51e1\u3002\u5728\u53cd\u5e38FNTI\u4e0e\u5176\u4ea4\u6362\u9a71\u52a8\u5e8f\u5217\u5bf9\u5e94\u4f53\u5f62\u6210\u7684\u7574\u58c1\u4e2d\uff0c\u89c2\u5bdf\u5230\u4e86\u65b0\u7684\u62d3\u6251\u754c\u9762\u6a21\u5f0f\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u53cd\u5e38\u975e\u963f\u8d1d\u5c14\u5085\u7acb\u53f6\u62d3\u6251\u7edd\u7f18\u4f53\uff08FNTIs\uff09\uff0c\u8be5\u7edd\u7f18\u4f53\u5728\u6574\u4e2a\u4e09\u4e2a\u80fd\u9699\u4e2d\u90fd\u8868\u73b0\u51fa\u62d3\u6251\u8fb9\u7f18\u6a21\u5f0f\uff0c\u5c3d\u7ba1\u5176\u6574\u4f53\u7535\u8377\u662f\u5e73\u51e1\u7684\u3002\u901a\u8fc7\u5c06\u65f6\u95f4\u5468\u671f\u6027\u8026\u5408\u7535\u8def\u96c6\u6210\u5230\u9759\u6001\u58f0\u5b66\u6676\u4f53\u4e2d\uff0c\u6211\u4eec\u5728\u58f0\u5b66\u9886\u57df\u6784\u5efa\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u4e00\u7ef4\u4e09\u5e26\u5085\u7acb\u53f6\u6a21\u578b\u3002\u6b64\u5916\uff0c\u6211\u4eec\u89c2\u5bdf\u5230\u4e86\u5728\u53cd\u5e38FNTI\u4e0e\u5176\u5177\u6709\u4ea4\u6362\u9a71\u52a8\u5e8f\u5217\u7684\u5bf9\u5e94\u4f53\u5f62\u6210\u7684\u7574\u58c1\u4e2d\u51fa\u73b0\u7684\u8fdd\u53cd\u76f4\u89c9\u7684\u62d3\u6251\u754c\u9762\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u5728\u4e4b\u524d\u7684\u5085\u7acb\u53f6\u963f\u8d1d\u5c14\u7cfb\u7edf\u4e2d\u662f\u65e0\u6cd5\u83b7\u5f97\u7684\u3002"}}
{"id": "2508.06829", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06829", "abs": "https://arxiv.org/abs/2508.06829", "authors": ["K. A. Shahriar"], "title": "Deep Domain-Adversarial Adaptation for Automatic Modulation Classification under Channel Variability", "comment": "5 pages, 3 figures", "summary": "Automatic Modulation Classification (AMC) plays a significant role in modern\ncognitive and intelligent radio systems, where accurate identification of\nmodulation is crucial for adaptive communication. The presence of heterogeneous\nwireless channel conditions, such as Rayleigh and Rician fading, poses\nsignificant challenges to the generalization ability of conventional AMC\nmodels. In this work, a domain-adversarial neural network (DANN) based deep\nlearning framework is proposed that explicitly mitigates channel-induced\ndistribution shifts between source and target domains. The approach is\nevaluated using a comprehensive simulated dataset containing five modulation\nschemes (BPSK, QPSK, 16QAM, 64QAM, 256QAM) across Rayleigh and Rician fading\nchannels at five frequency bands. Comparative experiments demonstrate that the\nDANN-based model achieves up to 14.93% absolute accuracy improvement in certain\nmodulation cases compared to a baseline supervised model trained solely on the\nsource domain. The findings establish the engineering feasibility of domain\nadversarial learning in AMC tasks under real-world channel variability and\noffer a robust direction for future research in adaptive spectrum intelligence", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57df\u5bf9\u6297\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u65e0\u7ebf\u901a\u4fe1\u4e2d\u7684\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\u95ee\u9898\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5e94\u5bf9\u4e0d\u540c\u4fe1\u9053\u6761\u4ef6\u5e26\u6765\u7684\u6311\u6218\uff0c\u76f8\u6bd4\u4f20\u7edf\u65b9\u6cd5\u5728\u7cbe\u5ea6\u4e0a\u6709\u6240\u63d0\u5347\u3002", "motivation": "\u73b0\u4ee3\u8ba4\u77e5\u548c\u667a\u80fd\u65e0\u7ebf\u7535\u7cfb\u7edf\u4e2d\u7684\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\uff08AMC\uff09\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f02\u6784\u65e0\u7ebf\u4fe1\u9053\u6761\u4ef6\uff08\u5982\u745e\u5229\u548c\u83b1\u65af\u8870\u843d\uff09\u5bf9\u4f20\u7edfAMC\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u6784\u6210\u4e86\u4e25\u5cfb\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u57df\u5bf9\u6297\u795e\u7ecf\u7f51\u7edc\uff08DANN\uff09\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u4ee5\u660e\u786e\u51cf\u8f7b\u6e90\u57df\u548c\u76ee\u6807\u57df\u4e4b\u95f4\u7531\u4fe1\u9053\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\u3002", "result": "\u6240\u63d0\u51fa\u7684DANN\u6a21\u578b\u5728\u5305\u542b\u4e94\u4e2a\u8c03\u5236\u65b9\u6848\uff08BPSK\u3001QPSK\u300116QAM\u300164QAM\u3001256QAM\uff09\u7684\u7efc\u5408\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u6db5\u76d6\u4e86\u745e\u5229\u548c\u83b1\u65af\u8870\u843d\u4fe1\u9053\u4ee5\u53ca\u4e94\u4e2a\u9891\u6bb5\u3002\u4e0e\u4ec5\u5728\u6e90\u57df\u4e0a\u8bad\u7ec3\u7684\u57fa\u7ebf\u76d1\u7763\u6a21\u578b\u76f8\u6bd4\uff0cDANN\u6a21\u578b\u5728\u67d0\u4e9b\u8c03\u5236\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u8fbe14.93%\u7684\u7edd\u5bf9\u7cbe\u5ea6\u63d0\u5347\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8e\u57df\u5bf9\u6297\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\u5728\u5b9e\u9645\u4fe1\u9053\u53d8\u5316\u4e0b\u7684\u81ea\u52a8\u8c03\u5236\u5206\u7c7b\u4efb\u52a1\u4e2d\u5177\u6709\u5de5\u7a0b\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u81ea\u9002\u5e94\u9891\u8c31\u667a\u80fd\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u65b9\u5411\u3002"}}
{"id": "2508.07742", "categories": ["cs.LO", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.07742", "abs": "https://arxiv.org/abs/2508.07742", "authors": ["Meghyn Bienvenu", "Camille Bourgaux", "Katsumi Inoue", "Robin Jean"], "title": "A Rule-Based Approach to Specifying Preferences over Conflicting Facts and Querying Inconsistent Knowledge Bases", "comment": "This is an extended version of a paper appearing at the 22nd\n  International Conference on Principles of Knowledge Representation and\n  Reasoning (KR 2025). 24 pages", "summary": "Repair-based semantics have been extensively studied as a means of obtaining\nmeaningful answers to queries posed over inconsistent knowledge bases (KBs).\nWhile several works have considered how to exploit a priority relation between\nfacts to select optimal repairs, the question of how to specify such\npreferences remains largely unaddressed. This motivates us to introduce a\ndeclarative rule-based framework for specifying and computing a priority\nrelation between conflicting facts. As the expressed preferences may contain\nundesirable cycles, we consider the problem of determining when a set of\npreference rules always yields an acyclic relation, and we also explore a\npragmatic approach that extracts an acyclic relation by applying various cycle\nremoval techniques. Towards an end-to-end system for querying inconsistent KBs,\nwe present a preliminary implementation and experimental evaluation of the\nframework, which employs answer set programming to evaluate the preference\nrules, apply the desired cycle resolution techniques to obtain a priority\nrelation, and answer queries under prioritized-repair semantics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u58f0\u660e\u5f0f\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\u4e2d\u7684\u4e8b\u5b9e\u4f18\u5148\u7ea7\uff0c\u5e76\u901a\u8fc7\u7b54\u6848\u96c6\u7f16\u7a0b\u5b9e\u73b0\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "motivation": "\u5728\u4fee\u590d\u57fa\u7840\u8bed\u4e49\u88ab\u5e7f\u6cdb\u7814\u7a76\u7528\u4ee5\u83b7\u5f97\u4e0d\u4e00\u81f4\u77e5\u8bc6\u5e93\u67e5\u8be2\u7684\u6709\u6548\u7b54\u6848\u65f6\uff0c\u5173\u4e8e\u5982\u4f55\u5229\u7528\u4e8b\u5b9e\u95f4\u7684\u4f18\u5148\u7ea7\u5173\u7cfb\u6765\u9009\u62e9\u6700\u4f18\u4fee\u590d\u7684\u7814\u7a76\u4ecd\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002\u8fd9\u4fc3\u4f7f\u6211\u4eec\u63d0\u51fa\u4e00\u4e2a\u58f0\u660e\u5f0f\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6846\u67b6\uff0c\u4ee5\u6307\u5b9a\u548c\u8ba1\u7b97\u4e8b\u5b9e\u95f4\u7684\u4f18\u5148\u7ea7\u5173\u7cfb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u58f0\u660e\u5f0f\u7684\u3001\u57fa\u4e8e\u89c4\u5219\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u6307\u5b9a\u548c\u8ba1\u7b97\u51b2\u7a81\u4e8b\u5b9e\u4e4b\u95f4\u7684\u4f18\u5148\u7ea7\u5173\u7cfb\uff0c\u5e76\u8003\u8651\u4e86\u504f\u597d\u89c4\u5219\u53ef\u80fd\u5305\u542b\u4e0d\u671f\u671b\u7684\u51b2\u7a81\uff0c\u4ee5\u53ca\u786e\u5b9a\u504f\u597d\u89c4\u5219\u96c6\u4f55\u65f6\u4ea7\u751f\u65e0\u73af\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u5e94\u7528\u5404\u79cd\u51b2\u7a81\u6d88\u9664\u6280\u672f\u63d0\u53d6\u65e0\u73af\u5173\u7cfb\u3002", "result": "\u8be5\u6846\u67b6\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\u6765\u8bc4\u4f30\u504f\u597d\u89c4\u5219\uff0c\u5e94\u7528\u51b2\u7a81\u89e3\u51b3\u6280\u672f\u83b7\u5f97\u4f18\u5148\u7ea7\u5173\u7cfb\uff0c\u5e76\u5728\u4f18\u5148\u4fee\u590d\u8bed\u4e49\u4e0b\u56de\u7b54\u67e5\u8be2\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u7b54\u6848\u96c6\u7f16\u7a0b\u6765\u8bc4\u4f30\u504f\u597d\u89c4\u5219\uff0c\u5e94\u7528\u6240\u9700\u7684\u51b2\u7a81\u89e3\u51b3\u6280\u672f\u6765\u83b7\u5f97\u4f18\u5148\u7ea7\u5173\u7cfb\uff0c\u5e76\u5728\u4f18\u5148\u4fee\u590d\u8bed\u4e49\u4e0b\u56de\u7b54\u67e5\u8be2\uff0c\u4ece\u800c\u4e3a\u67e5\u8be2\u4e0d\u4e00\u81f4\u7684\u77e5\u8bc6\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7aef\u5230\u7aef\u7cfb\u7edf\u7684\u521d\u6b65\u5b9e\u73b0\u548c\u5b9e\u9a8c\u8bc4\u4f30\u3002"}}
{"id": "2508.06585", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06585", "abs": "https://arxiv.org/abs/2508.06585", "authors": ["Jayant Sravan Tamarapalli", "Rynaa Grover", "Nilay Pande", "Sahiti Yerramilli"], "title": "CountQA: How Well Do MLLMs Count in the Wild?", "comment": null, "summary": "Multimodal Large Language Models (MLLMs) demonstrate remarkable fluency in\nunderstanding visual scenes, yet they exhibit a critical lack in a fundamental\ncognitive skill: object counting. This blind spot severely limits their\nreliability in real-world applications. To date, this capability has been\nlargely unevaluated in complex scenarios, as existing benchmarks either feature\nsparse object densities or are confined to specific visual domains, failing to\ntest models under realistic conditions. Addressing this gap, we introduce\nCountQA, a challenging new benchmark designed to probe this deficiency.\nComprising over 1,500 question-answer pairs, CountQA features real-world images\nwith high object density, clutter, and occlusion. We investigate this weakness\nby evaluating 15 prominent MLLMs on the CountQA benchmark and reveal that the\ntop-performing model achieves a mere 42.9% accuracy, with performance declining\nas object counts rise. By providing a dedicated benchmark to diagnose and\nrectify this core weakness, CountQA paves the way for a new generation of MLLMs\nthat are not only descriptively fluent but also numerically grounded and\nspatially aware. We will open-source the dataset and code upon paper acceptance\nto foster further research.", "AI": {"tldr": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5bf9\u8c61\u8ba1\u6570\u65b9\u9762\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684CountQA\u57fa\u51c6\uff08\u5305\u542b1500+\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\uff09\u8bc4\u4f30\u4e8615\u4e2a\u6a21\u578b\uff0c\u53d1\u73b0\u6700\u4f73\u6a21\u578b\u51c6\u786e\u7387\u4ec542.9%\uff0c\u4e14\u968f\u5bf9\u8c61\u6570\u589e\u52a0\u800c\u964d\u4f4e\uff0c\u63ed\u793a\u4e86\u6a21\u578b\u5728\u8ba1\u6570\u80fd\u529b\u4e0a\u7684\u4e0d\u8db3\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u867d\u7136\u5728\u7406\u89e3\u89c6\u89c9\u573a\u666f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u5bf9\u8c61\u8ba1\u6570\u8fd9\u4e00\u57fa\u672c\u8ba4\u77e5\u6280\u80fd\u4e0a\u5b58\u5728\u660e\u663e\u7f3a\u9677\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u53ef\u9760\u6027\u3002\u73b0\u6709\u7684\u57fa\u51c6\u6d4b\u8bd5\u672a\u80fd\u5145\u5206\u8bc4\u4f30\u8be5\u80fd\u529b\uff0c\u56e0\u4e3a\u5b83\u4eec\u8981\u4e48\u5bf9\u8c61\u5bc6\u5ea6\u7a00\u758f\uff0c\u8981\u4e48\u5c40\u9650\u4e8e\u7279\u5b9a\u89c6\u89c9\u9886\u57df\uff0c\u65e0\u6cd5\u5728\u771f\u5b9e\u6761\u4ef6\u4e0b\u6d4b\u8bd5\u6a21\u578b\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u540d\u4e3aCountQA\u7684\u65b0\u57fa\u51c6\u6765\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u5bf9\u8c61\u8ba1\u6570\u65b9\u9762\u7684\u4e0d\u8db3\u3002CountQA\u5305\u542b\u8d85\u8fc71500\u5bf9\u95ee\u7b54\uff0c\u6db5\u76d6\u4e86\u5177\u6709\u9ad8\u5bf9\u8c61\u5bc6\u5ea6\u3001\u6df7\u4e71\u548c\u906e\u6321\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u3002\u7814\u7a76\u4eba\u5458\u8bc4\u4f30\u4e8615\u4e2a\u4e3b\u6d41MLLMs\u5728CountQA\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5373\u4f7f\u662f\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728CountQA\u57fa\u51c6\u4e0a\u7684\u51c6\u786e\u7387\u4e5f\u4ec5\u4e3a42.9%\uff0c\u5e76\u4e14\u968f\u7740\u5bf9\u8c61\u6570\u91cf\u7684\u589e\u52a0\uff0c\u6a21\u578b\u7684\u6027\u80fd\u4f1a\u4e0b\u964d\u3002\u8fd9\u4e00\u7ed3\u679c\u51f8\u663e\u4e86\u5f53\u524dMLLMs\u5728\u5bf9\u8c61\u8ba1\u6570\u65b9\u9762\u7684\u666e\u904d\u5f31\u70b9\u3002", "conclusion": "CountQA\u65e8\u5728\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63d0\u4f9b\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u6539\u8fdb\u5176\u5bf9\u8c61\u8ba1\u6570\u80fd\u529b\u7684\u57fa\u51c6\u3002\u901a\u8fc7\u5f15\u5165\u5305\u542b\u9ad8\u5bc6\u5ea6\u3001\u6df7\u4e71\u548c\u906e\u6321\u7684\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7684CountQA\u57fa\u51c6\uff0c\u5e76\u8bc4\u4f30\u4e8615\u4e2aMLLMs\uff0c\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u8be5\u6280\u80fd\u4e0a\u7684\u663e\u8457\u4e0d\u8db3\uff0c\u5373\u4f7f\u662f\u6700\u4f73\u6a21\u578b\u4e5f\u4ec5\u8fbe\u523042.9%\u7684\u51c6\u786e\u7387\uff0c\u4e14\u51c6\u786e\u7387\u968f\u5bf9\u8c61\u6570\u91cf\u589e\u52a0\u800c\u4e0b\u964d\u3002\u8be5\u7814\u7a76\u4e3a\u5f00\u53d1\u66f4\u53ef\u9760\u3001\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u66f4\u5177\u9c81\u68d2\u6027\u7684MLLMs\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.07008", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.07008", "abs": "https://arxiv.org/abs/2508.07008", "authors": ["Anne Driemel", "Jan H\u00f6ckendorff", "Ioannis Psarros", "Christian Sohler"], "title": "A near-linear time approximation scheme for $(k,\\ell)$-median clustering under discrete Fr\u00e9chet distance", "comment": null, "summary": "A time series of complexity $m$ is a sequence of $m$ real valued\nmeasurements. The discrete Fr\\'echet distance $d_{dF}(x,y)$ is a distance\nmeasure between two time series $x$ and $y$ of possibly different complexity.\nGiven a set of $n$ time series represented as $m$-dimensional vectors over the\nreals, the $(k,\\ell)$-median problem under discrete Fr\\'echet distance aims to\nfind a set $C$ of $k$ time series of complexity $\\ell$ such that $$\\sum_{x\\in\nP} \\min_{c\\in C} d_{dF}(x,c)$$ is minimized. In this paper, we give the first\nnear-linear time $(1+\\varepsilon)$-approximation algorithm for this problem\nwhen $\\ell$ and $\\varepsilon$ are constants but $k$ can be as large as\n$\\Omega(n)$. We obtain our result by introducing a new dimension reduction\ntechnique for discrete Fr\\'echet distance and then adapt an algorithm of\nCohen-Addad et al. (J. ACM 2021) to work on the dimension-reduced input. As a\nbyproduct we also improve the best coreset construction for $(k,\\ell)$-median\nunder discrete Fr\\'echet distance (Cohen-Addad et al., SODA 2025) and show that\nits size can be independent of the number of input time series \\emph{ and }\ntheir complexity.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ef4\u5ea6\u7ea6\u51cf\u6280\u672f\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u73b0\u6709\u7684 (k,\u2113)-median \u7b97\u6cd5\uff0c\u4ee5\u5728\u79bb\u6563 Fr\u00e9chet \u8ddd\u79bb\u4e0b\u5b9e\u73b0\u8fd1\u7ebf\u6027\u65f6\u95f4 (1+\u03b5) \u8fd1\u4f3c\u3002\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u89e3\u51b3\u4e86\u5927\u89c4\u6a21 k \u503c\u7684\u95ee\u9898\uff0c\u8fd8\u6539\u8fdb\u4e86\u6838\u6784\u9020\u7684\u5927\u5c0f\uff0c\u4f7f\u5176\u4e0d\u4f9d\u8d56\u4e8e\u8f93\u5165\u6570\u636e\u7684\u590d\u6742\u6027\u3002", "motivation": "\u89e3\u51b3\u79bb\u6563 Fr\u00e9chet \u8ddd\u79bb\u4e0b\u7684 (k,\u2113)-median \u95ee\u9898\uff0c\u5e76\u6539\u8fdb\u4e86\u76f8\u5173\u7684\u6838\u6784\u9020\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u7ef4\u5ea6\u7ea6\u51cf\u6280\u672f\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e Cohen-Addad \u7b49\u4eba\uff08J. ACM 2021\uff09\u7684\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u79bb\u6563 Fr\u00e9chet \u8ddd\u79bb\u4e0b\u7684 (k,\u2113)-median \u95ee\u9898\u3002", "result": "\u5b9e\u73b0\u4e86\u8fd1\u7ebf\u6027\u65f6\u95f4 (1+\u03b5) \u8fd1\u4f3c\u7b97\u6cd5\uff0c\u5e76\u5c06\u6838\u6784\u9020\u7684\u5c3a\u5bf8\u72ec\u7acb\u4e8e\u8f93\u5165\u65f6\u95f4\u5e8f\u5217\u7684\u6570\u91cf\u53ca\u5176\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3a\u79bb\u6563 Fr\u00e9chet \u8ddd\u79bb\u4e0b\u7684 (k,\u2113)-median \u95ee\u9898\u63d0\u51fa\u4e86\u9996\u4e2a\u8fd1\u7ebf\u6027\u65f6\u95f4 (1+\u03b5) \u8fd1\u4f3c\u7b97\u6cd5\uff0c\u9002\u7528\u4e8e l \u548c \u03b5 \u4e3a\u5e38\u6570\u4f46 k \u53ef\u8fbe \u03a9(n) \u7684\u60c5\u51b5\u3002\u8be5\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u7ef4\u5ea6\u7ea6\u51cf\u6280\u672f\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e Cohen-Addad \u7b49\u4eba\uff08J. ACM 2021\uff09\u7684\u7b97\u6cd5\u3002"}}
{"id": "2508.06587", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06587", "abs": "https://arxiv.org/abs/2508.06587", "authors": ["A. Quadir", "M. Tanveer"], "title": "Hypergraph Neural Network with State Space Models for Node Classification", "comment": null, "summary": "In recent years, graph neural networks (GNNs) have gained significant\nattention for node classification tasks on graph-structured data. However,\ntraditional GNNs primarily focus on adjacency relationships between nodes,\noften overlooking the rich role-based characteristics that are crucial for\nlearning more expressive node representations. Existing methods for capturing\nrole-based features are largely unsupervised and fail to achieve optimal\nperformance in downstream tasks. To address these limitations, we propose a\nnovel hypergraph neural network with state space model (HGMN) that effectively\nintegrates role-aware representations into GNNs and the state space model. HGMN\nutilizes hypergraph construction techniques to model higher-order relationships\nand combines role-based and adjacency-based representations through a learnable\nmamba transformer mechanism. By leveraging two distinct hypergraph construction\nmethods-based on node degree and neighborhood levels, it strengthens the\nconnections among nodes with similar roles, enhancing the model's\nrepresentational power. Additionally, the inclusion of hypergraph convolution\nlayers enables the model to capture complex dependencies within hypergraph\nstructures. To mitigate the over-smoothing problem inherent in deep GNNs, we\nincorporate a residual network, ensuring improved stability and better feature\npropagation across layers. Extensive experiments conducted on one newly\nintroduced dataset and four benchmark datasets demonstrate the superiority of\nHGMN. The model achieves significant performance improvements on node\nclassification tasks compared to state-of-the-art GNN methods. These results\nhighlight HGMN's ability to provide enriched node representations by\neffectively embedding role-based features alongside adjacency information,\nmaking it a versatile and powerful tool for a variety of graph-based learning\napplications.", "AI": {"tldr": "HGMN\u662f\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u7ed3\u5408\u57fa\u4e8e\u89d2\u8272\u7684\u7279\u5f81\u3001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u548c\u8d85\u56fe\u7ed3\u6784\uff0c\u63d0\u5347\u4e86GNN\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfGNN\u5ffd\u7565\u89d2\u8272\u4fe1\u606f\u548c\u8868\u73b0\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684GNN\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e2d\u867d\u7136\u53d7\u5230\u5173\u6ce8\uff0c\u4f46\u4e3b\u8981\u4fa7\u91cd\u4e8e\u8282\u70b9\u95f4\u7684\u90bb\u63a5\u5173\u7cfb\uff0c\u5ffd\u7565\u4e86\u5bf9\u5b66\u4e60\u66f4\u6709\u8868\u73b0\u529b\u7684\u8282\u70b9\u8868\u793a\u81f3\u5173\u91cd\u8981\u7684\u57fa\u4e8e\u89d2\u8272\u7684\u7279\u5f81\u3002\u73b0\u6709\u7684\u6355\u83b7\u57fa\u4e8e\u89d2\u8272\u7279\u5f81\u7684\u65b9\u6cd5\u4e3b\u8981\u662f\u65e0\u76d1\u7763\u7684\uff0c\u5e76\u4e14\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8d85\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGMN\uff09\uff0c\u5b83\u7ed3\u5408\u4e86\u57fa\u4e8e\u89d2\u8272\u7684\u8868\u793a\u3001\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u3001\u8d85\u56fe\u6784\u5efa\u6280\u672f\uff08\u57fa\u4e8e\u8282\u70b9\u5ea6\u548c\u90bb\u57df\u7ea7\u522b\uff09\u3001\u53ef\u5b66\u4e60\u7684mamba transformer\u673a\u5236\u4ee5\u53ca\u6b8b\u5dee\u7f51\u7edc\uff0c\u4ee5\u5904\u7406\u9ad8\u9636\u5173\u7cfb\u3001\u6355\u83b7\u590d\u6742\u4f9d\u8d56\u5e76\u7f13\u89e3\u8fc7\u5e73\u6ed1\u95ee\u9898\u3002", "result": "HGMN\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e0e\u6700\u5148\u8fdb\u7684GNN\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u65b0\u5f15\u5165\u7684\u6570\u636e\u96c6\u548c\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "conclusion": "HGMN\u901a\u8fc7\u6709\u6548\u878d\u5408\u57fa\u4e8e\u89d2\u8272\u7684\u7279\u5f81\u548c\u90bb\u63a5\u4fe1\u606f\uff0c\u63d0\u4f9b\u4e86\u66f4\u4e30\u5bcc\u7684\u8282\u70b9\u8868\u793a\uff0c\u5728\u8282\u70b9\u5206\u7c7b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684GNN\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u5404\u79cd\u57fa\u4e8e\u56fe\u7684\u5b66\u4e60\u5e94\u7528\u4e2d\u7684\u901a\u7528\u6027\u548c\u5f3a\u5927\u80fd\u529b\u3002"}}
{"id": "2508.07880", "categories": ["cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07880", "abs": "https://arxiv.org/abs/2508.07880", "authors": ["Sophia Rupprecht", "Qinghe Gao", "Tanuj Karia", "Artur M. Schweidtmann"], "title": "Multi-agent systems for chemical engineering: A review and perspective", "comment": null, "summary": "Large language model (LLM)-based multi-agent systems (MASs) are a recent but\nrapidly evolving technology with the potential to transform chemical\nengineering by decomposing complex workflows into teams of collaborative agents\nwith specialized knowledge and tools. This review surveys the state-of-the-art\nof MAS within chemical engineering. While early studies demonstrate promising\nresults, scientific challenges remain, including the design of tailored\narchitectures, integration of heterogeneous data modalities, development of\nfoundation models with domain-specific modalities, and strategies for ensuring\ntransparency, safety, and environmental impact. As a young but fast-moving\nfield, MASs offer exciting opportunities to rethink chemical engineering\nworkflows.", "AI": {"tldr": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MASs\uff09\u6709\u6f5c\u529b\u9769\u65b0\u5316\u5de5\u9886\u57df\uff0c\u4f46\u9700\u8981\u89e3\u51b3\u67b6\u6784\u3001\u6570\u636e\u3001\u6a21\u578b\u548c\u5b89\u5168\u7b49\u65b9\u9762\u7684\u6311\u6218\u3002", "motivation": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MASs\uff09\u6709\u6f5c\u529b\u901a\u8fc7\u5c06\u590d\u6742\u5de5\u4f5c\u6d41\u5206\u89e3\u4e3a\u534f\u4f5c\u4ee3\u7406\u56e2\u961f\u6765\u6539\u53d8\u5316\u5de5\u9886\u57df\u3002", "method": "\u672c\u6587\u5bf9\u5316\u5de5\u9886\u57dfMASs\u7684\u6700\u65b0\u7814\u7a76\u8fdb\u5c55\u8fdb\u884c\u4e86\u7efc\u8ff0\u3002", "result": "\u65e9\u671f\u7814\u7a76\u7ed3\u679c\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u5b58\u5728\u79d1\u5b66\u6311\u6218\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MASs\uff09\u4e3a\u5316\u5de5\u9886\u57df\u5e26\u6765\u4e86\u9769\u65b0\u6f5c\u529b\uff0c\u4f46\u4ecd\u9762\u4e34\u67b6\u6784\u8bbe\u8ba1\u3001\u5f02\u6784\u6570\u636e\u6574\u5408\u3001\u9886\u57df\u7279\u5b9a\u57fa\u7840\u6a21\u578b\u5f00\u53d1\u4ee5\u53ca\u786e\u4fdd\u900f\u660e\u5ea6\u3001\u5b89\u5168\u6027\u548c\u73af\u5883\u5f71\u54cd\u7b49\u65b9\u9762\u7684\u79d1\u5b66\u6311\u6218\u3002"}}
{"id": "2508.06723", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06723", "abs": "https://arxiv.org/abs/2508.06723", "authors": ["Jie-Cheng Chen", "Joshua Leveillee", "Chris G. Van de Walle", "Feliciano Giustino"], "title": "Design of high-mobility p-type GaN via the piezomobility tensor", "comment": null, "summary": "Gallium nitride (GaN) is a wide-bandgap semiconductor of significant interest\nfor applications in solid-state lighting, power electronics, and\nradio-frequency amplifiers. An important limitation of this semiconductor is\nits low intrinsic hole mobility, which hinders the development of\n\\textit{p}-channel devices and the large-scale integration of GaN CMOS in\nnext-generation electronics. Prior research has explored the use of strain to\nimprove the hole mobility of GaN, but a systematic analysis of all possible\nstrain conditions and their impact on the mobility is lacking. In this study,\nwe introduce a piezomobility tensor notation to characterize the relationship\nbetween applied strain and hole mobility in GaN. To map the strain-dependence\nof the hole mobility, we solve the \\textit{ab initio} Boltzmann transport\nequation, accounting for electron-phonon scattering and GW quasiparticle energy\ncorrections. We show that there exist three optimal strain configurations, two\nuniaxial strains and one shear strain, that can lead to significant mobility\nenhancement. In particular, we predict room-temperature hole mobility of up to\n164~\\mob\\ for 2\\% uniaxial compression and 148~\\mob\\ for 2\\% shear strain. Our\nmethodology provides a general framework for investigating strain effects on\nthe transport properties of semiconductors from first principles.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u538b\u7535\u8fc1\u79fb\u7387\u5f20\u91cf\u7b26\u53f7\u548c\u4ece\u5934\u7b97\u73bb\u5c14\u5179\u66fc\u4f20\u8f93\u65b9\u7a0b\uff0c\u53d1\u73b0\u5355\u8f74\u538b\u7f29\u548c\u526a\u5207\u5e94\u53d8\u53ef\u663e\u8457\u63d0\u9ad8\u6c2e\u5316\u9553\uff08GaN\uff09\u7684\u7a7a\u7a74\u8fc1\u79fb\u7387\u3002", "motivation": "\u6c2e\u5316\u9553\uff08GaN\uff09\u7684\u672c\u5f81\u7a7a\u7a74\u8fc1\u79fb\u7387\u4f4e\uff0c\u963b\u788d\u4e86p\u6c9f\u9053\u5668\u4ef6\u7684\u5f00\u53d1\u548cGaN CMOS\u7684\u5927\u89c4\u6a21\u96c6\u6210\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5e94\u53d8\u5bf9\u63d0\u9ad8GaN\u7a7a\u7a74\u8fc1\u79fb\u7387\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u5229\u7528\u538b\u7535\u8fc1\u79fb\u7387\u5f20\u91cf\u7b26\u53f7\u6765\u8868\u5f81\u5916\u52a0\u5e94\u53d8\u548c\u6c2e\u5316\u9553\uff08GaN\uff09\u7a7a\u7a74\u8fc1\u79fb\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u6c42\u89e3\u4ece\u5934\u7b97\u73bb\u5c14\u5179\u66fc\u4f20\u8f93\u65b9\u7a0b\uff08\u8003\u8651\u7535\u5b50-\u58f0\u5b50\u6563\u5c04\u548cGW\u51c6\u7c92\u5b50\u80fd\u91cf\u4fee\u6b63\uff09\u6765\u7ed8\u5236\u5e94\u53d8\u4f9d\u8d56\u6027\u7684\u7a7a\u7a74\u8fc1\u79fb\u7387\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u4e09\u79cd\u6700\u4f73\u5e94\u53d8\u6784\u578b\uff08\u4e24\u79cd\u5355\u8f74\u5e94\u53d8\u548c\u4e00\u79cd\u526a\u5207\u5e94\u53d8\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7a7a\u7a74\u8fc1\u79fb\u7387\uff0c\u5176\u4e2d2%\u5355\u8f74\u538b\u7f29\u548c2%\u526a\u5207\u5e94\u53d8\u5206\u522b\u53ef\u4ee5\u5c06\u5ba4\u6e29\u7a7a\u7a74\u8fc1\u79fb\u7387\u9884\u6d4b\u63d0\u9ad8\u5230164 cm^2/Vs\u548c148 cm^2/Vs\u3002", "conclusion": "\u672c\u7814\u7a76\u5f15\u5165\u4e86\u538b\u7535\u8fc1\u79fb\u7387\u5f20\u91cf\u7b26\u53f7\u6765\u8868\u5f81\u5916\u52a0\u5e94\u53d8\u548c\u6c2e\u5316\u9553\uff08GaN\uff09\u7a7a\u7a74\u8fc1\u79fb\u7387\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u627e\u5230\u4e86\u4e09\u79cd\u6700\u4f73\u5e94\u53d8\u6784\u578b\uff08\u4e24\u79cd\u5355\u8f74\u5e94\u53d8\u548c\u4e00\u79cd\u526a\u5207\u5e94\u53d8\uff09\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u7a7a\u7a74\u8fc1\u79fb\u7387\uff0c\u5176\u4e2d2%\u5355\u8f74\u538b\u7f29\u548c2%\u526a\u5207\u5e94\u53d8\u5206\u522b\u53ef\u4ee5\u5c06\u5ba4\u6e29\u7a7a\u7a74\u8fc1\u79fb\u7387\u9884\u6d4b\u63d0\u9ad8\u5230164 cm^2/Vs\u548c148 cm^2/Vs\u3002"}}
{"id": "2508.07124", "categories": ["cs.DC", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.07124", "abs": "https://arxiv.org/abs/2508.07124", "authors": ["Shashwat Jaiswal", "Suman Raj", "Subhajit Sidhanta", "Yogesh Simmhan"], "title": "AerialDB: A Federated Peer-to-Peer Spatio-temporal Edge Datastore for Drone Fleets", "comment": null, "summary": "Recent years have seen an unprecedented growth in research that leverages the\nnewest computing paradigm of Internet of Drones, comprising a fleet of\nconnected Unmanned Aerial Vehicles (UAVs) used for a wide range of tasks such\nas monitoring and analytics in highly mobile and changing environments\ncharacteristic of disaster regions. Given that the typical data (i.e., videos\nand images) collected by the fleet of UAVs deployed in such scenarios can be\nconsiderably larger than what the onboard computers can process, the UAVs need\nto offload their data in real-time to the edge and the cloud for further\nprocessing. To that end, we present the design of AerialDB - a lightweight\ndecentralized data storage and query system that can store and process time\nseries data on a multi-UAV system comprising: A) a fleet of hundreds of UAVs\nfitted with onboard computers, and B) ground-based edge servers connected\nthrough a cellular link. Leveraging lightweight techniques for content-based\nreplica placement and indexing of shards, AerialDB has been optimized for\nefficient processing of different possible combinations of typical spatial and\ntemporal queries performed by real-world disaster management applications.\nUsing containerized deployment spanning up to 400 drones and 80 edges, we\ndemonstrate that AerialDB is able to scale efficiently while providing near\nreal-time performance with different realistic workloads. Further, AerialDB\ncomprises a decentralized and locality-aware distributed execution engine which\nprovides graceful degradation of performance upon edge failures with relatively\nlow latency while processing large spatio-temporal data. AerialDB exhibits\ncomparable insertion performance and 100 times improvement in query performance\nagainst state-of-the-art baseline. Moreover, it exhibits a 10 times and 100\ntimes improvement with insertion and query workloads respectively over the\ncloud baseline.", "AI": {"tldr": "AerialDB\u662f\u4e00\u4e2a\u4e3a\u65e0\u4eba\u673a\u7fa4\u8bbe\u8ba1\u7684\u8f7b\u91cf\u7ea7\u3001\u53bb\u4e2d\u5fc3\u5316\u65f6\u7a7a\u6570\u636e\u5b58\u50a8\u548c\u67e5\u8be2\u7cfb\u7edf\uff0c\u80fd\u6709\u6548\u5904\u7406\u6d77\u91cf\u6570\u636e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u548c\u4e91\u65b9\u6848\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\uff08UAV\uff09\u5728\u76d1\u6d4b\u548c\u5206\u6790\u7b49\u4efb\u52a1\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u7279\u522b\u662f\u5728\u707e\u96be\u533a\u57df\u7b49\u52a8\u6001\u73af\u5883\u4e2d\uff0cUAV\u6536\u96c6\u7684\u6570\u636e\u91cf\uff08\u5982\u89c6\u9891\u548c\u56fe\u50cf\uff09\u901a\u5e38\u8d85\u8fc7\u5176\u677f\u8f7d\u8ba1\u7b97\u673a\u7684\u5904\u7406\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u5c06\u6570\u636e\u5378\u8f7d\u5230\u8fb9\u7f18\u548c\u4e91\u7aef\u8fdb\u884c\u5904\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAerialDB\u7684\u8f7b\u91cf\u7ea7\u3001\u53bb\u4e2d\u5fc3\u5316\u7684\u6570\u636e\u5b58\u50a8\u548c\u67e5\u8be2\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u4e13\u4e3a\u591aUAV\u7cfb\u7edf\u8bbe\u8ba1\uff0c\u80fd\u591f\u5b58\u50a8\u548c\u5904\u7406\u65f6\u5e8f\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e86\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u5185\u5bb9\u7684\u526f\u672c\u653e\u7f6e\u548c\u5206\u7247\u7d22\u5f15\u6280\u672f\uff0c\u5e76\u4f18\u5316\u4e86\u7a7a\u95f4\u548c\u65f6\u95f4\u67e5\u8be2\u7684\u5904\u7406\u3002\u7cfb\u7edf\u5305\u62ec\u4e00\u4e2a\u5305\u542b\u6570\u767e\u67b6UAV\u7684\u673a\u961f\u548c\u4e00\u4e2a\u901a\u8fc7\u8702\u7a9d\u7f51\u7edc\u8fde\u63a5\u7684\u5730\u9762\u8fb9\u7f18\u670d\u52a1\u5668\u3002\u5b83\u8fd8\u5305\u542b\u4e00\u4e2a\u53bb\u4e2d\u5fc3\u5316\u3001\u9762\u5411\u672c\u5730\u7684\u5206\u5e03\u5f0f\u6267\u884c\u5f15\u64ce\uff0c\u80fd\u591f\u4f18\u96c5\u5730\u5904\u7406\u8fb9\u7f18\u6545\u969c\u3002", "result": "\u5728\u5305\u542b\u591a\u8fbe400\u67b6\u65e0\u4eba\u673a\u548c80\u4e2a\u8fb9\u7f18\u670d\u52a1\u5668\u7684\u5bb9\u5668\u5316\u90e8\u7f72\u4e2d\uff0cAerialDB\u80fd\u591f\u9ad8\u6548\u6269\u5c55\u5e76\u63d0\u4f9b\u8fd1\u4e4e\u5b9e\u65f6\u7684\u6027\u80fd\u3002\u4e0e\u73b0\u6709\u6280\u672f\u57fa\u7ebf\u76f8\u6bd4\uff0cAerialDB\u7684\u63d2\u5165\u6027\u80fd\u76f8\u5f53\uff0c\u67e5\u8be2\u6027\u80fd\u63d0\u9ad8\u4e86100\u500d\u3002\u4e0e\u4e91\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5728\u63d2\u5165\u548c\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u4e0b\uff0cAerialDB\u7684\u6027\u80fd\u5206\u522b\u63d0\u9ad8\u4e8610\u500d\u548c100\u500d\u3002", "conclusion": "AerialDB\u901a\u8fc7\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u57fa\u4e8e\u5185\u5bb9\u7684\u526f\u672c\u653e\u7f6e\u548c\u5206\u7247\u7d22\u5f15\u6280\u672f\uff0c\u4e3a\u591aUAV\u7cfb\u7edf\u4e2d\u7684\u65f6\u7a7a\u6570\u636e\u5b58\u50a8\u548c\u5904\u7406\u8fdb\u884c\u4e86\u4f18\u5316\uff0c\u5728\u771f\u5b9e\u707e\u96be\u7ba1\u7406\u573a\u666f\u4e0b\uff0c\u80fd\u591f\u9ad8\u6548\u5904\u7406\u7a7a\u95f4\u548c\u65f6\u95f4\u67e5\u8be2\u3002\u8be5\u7cfb\u7edf\u5728\u9ad8\u8fbe400\u67b6\u65e0\u4eba\u673a\u548c80\u4e2a\u8fb9\u7f18\u670d\u52a1\u5668\u7684\u5bb9\u5668\u5316\u90e8\u7f72\u4e2d\uff0c\u5c55\u73b0\u4e86\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u8fd1\u4e4e\u5b9e\u65f6\u7684\u6027\u80fd\uff0c\u5373\u4f7f\u5728\u8fb9\u7f18\u6545\u969c\u4e0b\u4e5f\u80fd\u63d0\u4f9b\u6027\u80fd\u7684\u4f18\u96c5\u964d\u7ea7\u548c\u8f83\u4f4e\u7684\u5ef6\u8fdf\u3002\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0cAerialDB\u5728\u63d2\u5165\u6027\u80fd\u65b9\u9762\u76f8\u5f53\uff0c\u67e5\u8be2\u6027\u80fd\u63d0\u9ad8\u4e86100\u500d\uff0c\u5e76\u4e14\u5728\u63d2\u5165\u548c\u67e5\u8be2\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u5206\u522b\u6bd4\u4e91\u57fa\u7ebf\u63d0\u9ad8\u4e8610\u500d\u548c100\u500d\u3002"}}
{"id": "2508.06538", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06538", "abs": "https://arxiv.org/abs/2508.06538", "authors": ["Gioele Buriani", "Jingyue Liu", "Maximilian St\u00f6lzle", "Cosimo Della Santina", "Jiatao Ding"], "title": "Symbolic Learning of Interpretable Reduced-Order Models for Jumping Quadruped Robots", "comment": "8 pages, under review", "summary": "Reduced-order models are essential for motion planning and control of\nquadruped robots, as they simplify complex dynamics while preserving critical\nbehaviors. This paper introduces a novel methodology for deriving such\ninterpretable dynamic models, specifically for jumping. We capture the\nhigh-dimensional, nonlinear jumping dynamics in a low-dimensional latent space\nby proposing a learning architecture combining Sparse Identification of\nNonlinear Dynamics (SINDy) with physical structural priors on the jump\ndynamics. Our approach demonstrates superior accuracy to the traditional\nactuated Spring-loaded Inverted Pendulum (aSLIP) model and is validated through\nsimulation and hardware experiments across different jumping strategies.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u7ed3\u5408SINDy\u548c\u7269\u7406\u5148\u9a8c\uff0c\u4e3a\u56db\u8db3\u673a\u5668\u4eba\u8df3\u8dc3\u751f\u6210\u4e86\u66f4\u51c6\u786e\u7684\u964d\u7ef4\u52a8\u529b\u5b66\u6a21\u578b\uff0c\u4f18\u4e8e\u4f20\u7edfaSLIP\u6a21\u578b\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u56db\u8db3\u673a\u5668\u4eba\u8fd0\u52a8\u89c4\u5212\u548c\u63a7\u5236\uff0c\u9700\u8981\u7b80\u5316\u590d\u6742\u52a8\u529b\u5b66\u5e76\u4fdd\u7559\u5173\u952e\u884c\u4e3a\u7684\u964d\u7ef4\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u8df3\u8dc3\u573a\u666f\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7a00\u758f\u8bc6\u522b\u975e\u7ebf\u6027\u52a8\u529b\u5b66\uff08SINDy\uff09\u548c\u57fa\u4e8e\u8df3\u8dc3\u52a8\u529b\u5b66\u7684\u7269\u7406\u7ed3\u6784\u5148\u9a8c\u7684\u5b66\u4e60\u67b6\u6784\uff0c\u7528\u4e8e\u63d0\u53d6\u964d\u7ef4\u7684\u3001\u53ef\u89e3\u91ca\u7684\u52a8\u529b\u5b66\u6a21\u578b\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u9488\u5bf9\u4e0d\u540c\u7684\u8df3\u8dc3\u7b56\u7565\uff0c\u5c55\u73b0\u4e86\u6bd4\u4f20\u7edfaSLIP\u6a21\u578b\u66f4\u4f18\u8d8a\u7684\u7cbe\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408SINDy\u548c\u7269\u7406\u7ed3\u6784\u5148\u9a8c\uff0c\u6210\u529f\u5730\u5c06\u9ad8\u7ef4\u975e\u7ebf\u6027\u8df3\u8dc3\u52a8\u529b\u5b66\u6620\u5c04\u5230\u4f4e\u7ef4\u6f5c\u5728\u7a7a\u95f4\uff0c\u5e76\u4e14\u5728\u4eff\u771f\u548c\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u5747\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u7684aSLIP\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.06667", "categories": ["quant-ph", "physics.hist-ph"], "pdf": "https://arxiv.org/pdf/2508.06667", "abs": "https://arxiv.org/abs/2508.06667", "authors": ["J. Manero", "R. Muci\u00f1o", "E. Okon"], "title": "A disputable assumption behind the empirical equivalence between pilot-wave theory and standard quantum mechanics", "comment": "18 pages", "summary": "The de Broglie-Bohm pilot-wave theory asserts that a complete\ncharacterization of an $N$-particle system is given by its wave function\ntogether with the (at-all-times-defined) positions of the particles, with the\nwave function always satisfying the Schr\\\"odinger equation and the positions\nevolving according to the deterministic \"guiding equation\". A complete\nagreement with the predictive apparatus of standard quantum mechanics,\nincluding the uncertainty principle and the probabilistic Born rule, is then\nsaid to emerge from these equations, without having to confer any special\nstatus to measurements or observers. Two key elements behind the proof of this\ncomplete agreement are absolute uncertainty and the POVM theorem. The former\ninvolves an alleged \"naturally emerging, irreducible limitation on the\npossibility of obtaining knowledge within pilot-wave theory\" and the latter\nestablishes that the outcome distributions of all measurements are described by\nPOVMs. Here, we argue that the derivations of absolute uncertainty and the POVM\ntheorem depend upon the questionable assumption that \"information is always\nconfigurationally grounded\". We explain in detail why the offered rationale\nbehind such an assumption is deficient and explore the consequences of having\nto let go of it.", "AI": {"tldr": "The de Broglie-Bohm pilot-wave theory's claims of agreement with quantum mechanics rely on a flawed assumption about information, which this paper argues against.", "motivation": "The paper aims to analyze the de Broglie-Bohm pilot-wave theory, specifically its claims of complete agreement with standard quantum mechanics through absolute uncertainty and the POVM theorem, and to question the underlying assumption of configurationally grounded information.", "method": "The paper argues that the derivations of absolute uncertainty and the POVM theorem depend upon the questionable assumption that \"information is always configurationally grounded\". It explains in detail why the offered rationale behind such an assumption is deficient and explores the consequences of having to let go of it.", "result": "The paper argues that the claimed agreement between pilot-wave theory and standard quantum mechanics, based on absolute uncertainty and the POVM theorem, is flawed due to a questionable assumption about information being configurationally grounded.", "conclusion": "The derivations of absolute uncertainty and the POVM theorem depend upon the questionable assumption that \"information is always configurationally grounded\"."}}
{"id": "2508.07457", "categories": ["cs.AR", "A.1; C.1.1; G.3; I.6.1; I.6.8"], "pdf": "https://arxiv.org/pdf/2508.07457", "abs": "https://arxiv.org/abs/2508.07457", "authors": ["Janith Petangoda", "Chatura Samarakoon", "James Meech", "Divya Thekke Kanapram", "Hamid Toshani", "Nathaniel Tye", "Vasileios Tsoutsouras", "Phillip Stanley-Marbell"], "title": "The Monte Carlo Method and New Device and Architectural Techniques for Accelerating It", "comment": "15 pages, 4 figures (17 subfigures)", "summary": "Computing systems interacting with real-world processes must safely and\nreliably process uncertain data. The Monte Carlo method is a popular approach\nfor computing with such uncertain values. This article introduces a framework\nfor describing the Monte Carlo method and highlights two advances in the domain\nof physics-based non-uniform random variate generators (PPRVGs) to overcome\ncommon limitations of traditional Monte Carlo sampling. This article also\nhighlights recent advances in architectural techniques that eliminate the need\nto use the Monte Carlo method by leveraging distributional microarchitectural\nstate to natively compute on probability distributions. Unlike Monte Carlo\nmethods, uncertainty-tracking processor architectures can be said to be\nconvergence-oblivious.", "AI": {"tldr": "\u65b0\u6846\u67b6\u548c\u67b6\u6784\u6280\u672f\u53ef\u5904\u7406\u4e0d\u786e\u5b9a\u6570\u636e\uff0c\u514b\u670d\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u5b89\u5168\u53ef\u9760\u5730\u5904\u7406\u4e0e\u73b0\u5b9e\u4e16\u754c\u8fc7\u7a0b\u4ea4\u4e92\u7684\u8ba1\u7b97\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u4e0d\u786e\u5b9a\u6570\u636e\uff0c\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u514b\u670d\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u7528\u4e8e\u63cf\u8ff0\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u7684\u6846\u67b6\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u7269\u7406\u5b66\u4e2d\u57fa\u4e8e\u975e\u5747\u5300\u968f\u673a\u53d8\u91cf\u751f\u6210\u5668\uff08PPRVGs\uff09\u7684\u4e24\u4e2a\u6700\u65b0\u8fdb\u5c55\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u540c\u65f6\uff0c\u6587\u7ae0\u8fd8\u63a2\u8ba8\u4e86\u5229\u7528\u5206\u5e03\u5f0f\u5fae\u67b6\u6784\u72b6\u6001\u76f4\u63a5\u8ba1\u7b97\u6982\u7387\u5206\u5e03\u7684\u6700\u65b0\u67b6\u6784\u6280\u672f\u3002", "result": "\u6587\u7ae0\u5c55\u793a\u4e86\u7269\u7406\u5b66\u4e2d\u57fa\u4e8e\u975e\u5747\u5300\u968f\u673a\u53d8\u91cf\u751f\u6210\u5668\uff08PPRVGs\uff09\u7684\u4e24\u4e2a\u6700\u65b0\u8fdb\u5c55\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u5206\u5e03\u5f0f\u5fae\u67b6\u6784\u72b6\u6001\u76f4\u63a5\u8ba1\u7b97\u6982\u7387\u5206\u5e03\u7684\u67b6\u6784\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u65e0\u9700\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u5373\u53ef\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u5904\u7406\u4e0d\u786e\u5b9a\u6570\u636e\u7684\u6846\u67b6\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u7269\u7406\u5b66\u4e2d\u57fa\u4e8e\u975e\u5747\u5300\u968f\u673a\u53d8\u91cf\u751f\u6210\u5668\uff08PPRVGs\uff09\u7684\u4e24\u4e2a\u6700\u65b0\u8fdb\u5c55\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u8499\u7279\u5361\u6d1b\u91c7\u6837\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u6587\u7ae0\u8fd8\u5f3a\u8c03\u4e86\u6700\u65b0\u7684\u67b6\u6784\u6280\u672f\uff0c\u8fd9\u4e9b\u6280\u672f\u5229\u7528\u5206\u5e03\u5f0f\u7684\u5fae\u67b6\u6784\u72b6\u6001\u76f4\u63a5\u8ba1\u7b97\u6982\u7387\u5206\u5e03\uff0c\u4ece\u800c\u65e0\u9700\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u65b9\u6cd5\u3002"}}
{"id": "2508.08228", "categories": ["cs.GR", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08228", "abs": "https://arxiv.org/abs/2508.08228", "authors": ["Sining Lu", "Guan Chen", "Nam Anh Dinh", "Itai Lang", "Ari Holtzman", "Rana Hanocka"], "title": "LL3M: Large Language 3D Modelers", "comment": "Our project page is at https://threedle.github.io/ll3m", "summary": "We present LL3M, a multi-agent system that leverages pretrained large\nlanguage models (LLMs) to generate 3D assets by writing interpretable Python\ncode in Blender. We break away from the typical generative approach that learns\nfrom a collection of 3D data. Instead, we reformulate shape generation as a\ncode-writing task, enabling greater modularity, editability, and integration\nwith artist workflows. Given a text prompt, LL3M coordinates a team of\nspecialized LLM agents to plan, retrieve, write, debug, and refine Blender\nscripts that generate and edit geometry and appearance. The generated code\nworks as a high-level, interpretable, human-readable, well-documented\nrepresentation of scenes and objects, making full use of sophisticated Blender\nconstructs (e.g. B-meshes, geometry modifiers, shader nodes) for diverse,\nunconstrained shapes, materials, and scenes. This code presents many avenues\nfor further agent and human editing and experimentation via code tweaks or\nprocedural parameters. This medium naturally enables a co-creative loop in our\nsystem: agents can automatically self-critique using code and visuals, while\niterative user instructions provide an intuitive way to refine assets. A shared\ncode context across agents enables awareness of previous attempts, and a\nretrieval-augmented generation knowledge base built from Blender API\ndocumentation, BlenderRAG, equips agents with examples, types, and functions\nempowering advanced modeling operations and code correctness. We demonstrate\nthe effectiveness of LL3M across diverse shape categories, style and material\nedits, and user-driven refinements. Our experiments showcase the power of code\nas a generative and interpretable medium for 3D asset creation. Our project\npage is at https://threedle.github.io/ll3m.", "AI": {"tldr": "LL3M \u662f\u4e00\u4e2a\u4f7f\u7528 Python \u4ee3\u7801\u5728 Blender \u4e2d\u751f\u6210 3D \u8d44\u6e90\u7684\u591a\u4ee3\u7406 LLM \u7cfb\u7edf\uff0c\u652f\u6301\u7f16\u8f91\u3001\u7528\u6237\u534f\u4f5c\u548c\u590d\u6742 3D \u64cd\u4f5c\u3002", "motivation": "LL3M \u7684\u76ee\u6807\u662f\u6446\u8131\u4f20\u7edf\u7684\u751f\u6210\u65b9\u6cd5\uff0c\u63a2\u7d22\u4ee3\u7801\u4f5c\u4e3a 3D \u8d44\u6e90\u751f\u6210\u5a92\u4ecb\u7684\u53ef\u80fd\u6027\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7684\u6a21\u5757\u5316\u3001\u53ef\u7f16\u8f91\u6027\uff0c\u5e76\u4e0e\u827a\u672f\u5bb6\u7684\u5de5\u4f5c\u6d41\u7a0b\u96c6\u6210\u3002", "method": "LL3M \u662f\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLMs) \u901a\u8fc7\u7f16\u5199 Blender \u7684 Python \u4ee3\u7801\u6765\u751f\u6210 3D \u8d44\u6e90\u3002\u8be5\u7cfb\u7edf\u4e0d\u4f9d\u8d56\u4e8e 3D \u6570\u636e\u96c6\uff0c\u800c\u662f\u5c06\u5f62\u72b6\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u4ee3\u7801\u7f16\u5199\u4efb\u52a1\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u7531 Blender API \u6587\u6863\u7ec4\u6210\u7684\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u77e5\u8bc6\u5e93 (BlenderRAG)\u3002", "result": "LL3M \u80fd\u591f\u6839\u636e\u6587\u672c\u63d0\u793a\uff0c\u534f\u8c03\u4e13\u95e8\u7684 LLM \u4ee3\u7406\u6765\u8ba1\u5212\u3001\u68c0\u7d22\u3001\u7f16\u5199\u3001\u8c03\u8bd5\u548c\u5b8c\u5584 Blender \u811a\u672c\uff0c\u4ee5\u751f\u6210\u548c\u7f16\u8f91\u51e0\u4f55\u5f62\u72b6\u548c\u5916\u89c2\u3002\u751f\u6210\u7684\u4ee3\u7801\u662f\u53ef\u89e3\u91ca\u7684\u3001\u4eba\u7c7b\u53ef\u8bfb\u7684\u3001\u6587\u6863\u9f50\u5168\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u5229\u7528 Blender \u7684\u9ad8\u7ea7\u529f\u80fd\u6765\u5b9e\u73b0\u591a\u6837\u5316\u548c\u65e0\u7ea6\u675f\u7684\u5f62\u72b6\u3001\u6750\u8d28\u548c\u573a\u666f\u3002", "conclusion": "LL3M \u7cfb\u7edf\u901a\u8fc7\u4ee3\u7801\u751f\u6210 3D \u8d44\u4ea7\uff0c\u5c55\u793a\u4e86\u4ee3\u7801\u4f5c\u4e3a 3D \u8d44\u4ea7\u521b\u5efa\u7684\u53ef\u751f\u6210\u548c\u53ef\u89e3\u91ca\u5a92\u4ecb\u7684\u5f3a\u5927\u529f\u80fd\u3002"}}
{"id": "2508.06548", "categories": ["cs.CL", "cs.AI", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06548", "abs": "https://arxiv.org/abs/2508.06548", "authors": ["Zhanye Luo", "Yuefeng Han", "Xiufan Yu"], "title": "Factor Augmented Supervised Learning with Text Embeddings", "comment": null, "summary": "Large language models (LLMs) generate text embeddings from text data,\nproducing vector representations that capture the semantic meaning and\ncontextual relationships of words. However, the high dimensionality of these\nembeddings often impedes efficiency and drives up computational cost in\ndownstream tasks. To address this, we propose AutoEncoder-Augmented Learning\nwith Text (AEALT), a supervised, factor-augmented framework that incorporates\ndimension reduction directly into pre-trained LLM workflows. First, we extract\nembeddings from text documents; next, we pass them through a supervised\naugmented autoencoder to learn low-dimensional, task-relevant latent factors.\nBy modeling the nonlinear structure of complex embeddings, AEALT outperforms\nconventional deep-learning approaches that rely on raw embeddings. We validate\nits broad applicability with extensive experiments on classification, anomaly\ndetection, and prediction tasks using multiple real-world public datasets.\nNumerical results demonstrate that AEALT yields substantial gains over both\nvanilla embeddings and several standard dimension reduction methods.", "AI": {"tldr": "AEALT\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u7f16\u7801\u5668\u5c06LLM\u6587\u672c\u5d4c\u5165\u964d\u7ef4\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u8ba1\u7b97\u6027\u80fd\uff0c\u5728\u5404\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "LLM\u751f\u6210\u7684\u6587\u672c\u5d4c\u5165\u5177\u6709\u9ad8\u7ef4\u5ea6\uff0c\u8fd9\u4f1a\u5f71\u54cd\u4e0b\u6e38\u4efb\u52a1\u7684\u6548\u7387\u548c\u8ba1\u7b97\u6210\u672c\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "AEALT\u662f\u4e00\u79cd\u76d1\u7763\u7684\u3001\u56e0\u5b50\u589e\u5f3a\u7684\u6846\u67b6\uff0c\u5c06\u964d\u7ef4\u76f4\u63a5\u6574\u5408\u5230\u9884\u8bad\u7ec3\u7684LLM\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002\u9996\u5148\u63d0\u53d6\u6587\u672c\u7684\u5d4c\u5165\uff0c\u7136\u540e\u901a\u8fc7\u76d1\u7763\u589e\u5f3a\u7684\u81ea\u52a8\u7f16\u7801\u5668\u5b66\u4e60\u4f4e\u7ef4\u3001\u4e0e\u4efb\u52a1\u76f8\u5173\u7684\u6f5c\u5728\u56e0\u5b50\uff0c\u4ece\u800c\u6a21\u62df\u590d\u6742\u5d4c\u5165\u7684\u975e\u7ebf\u6027\u7ed3\u6784\u3002", "result": "AEALT\u5728\u591a\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u516c\u5171\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u9884\u6d4b\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u4e8e\u539f\u59cb\u5d4c\u5165\u548c\u51e0\u79cd\u6807\u51c6\u7684\u964d\u7ef4\u65b9\u6cd5\uff0c\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002", "conclusion": "AEALT\u5728\u5206\u7c7b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u9884\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u539f\u59cb\u5d4c\u5165\u548c\u51e0\u79cd\u6807\u51c6\u7684\u964d\u7ef4\u65b9\u6cd5\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\u3002"}}
{"id": "2508.06702", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.06702", "abs": "https://arxiv.org/abs/2508.06702", "authors": ["Zhao Song", "The Anh Han"], "title": "Emergence of Cooperation and Commitment in Optional Prisoner's Dilemma", "comment": null, "summary": "Commitment is a well-established mechanism for fostering cooperation in human\nsociety and multi-agent systems. However, existing research has predominantly\nfocused on the commitment that neglects the freedom of players to abstain from\nan interaction, limiting their applicability to many real-world scenarios where\nparticipation is often voluntary. In this paper, we present a two-stage game\nmodel to investigate the evolution of commitment-based behaviours and\ncooperation within the framework of the optional Prisoner's Dilemma game. In\nthe pre-game stage, players decide whether to accept a mutual commitment. Once\nin the game, they choose among cooperation, defection, or exiting, depending on\nthe formation of a pre-game commitment. We find that optional participation\nboosts commitment acceptance but fails to foster cooperation, leading instead\nto widespread exit behaviour. To address this, we then introduce and compare\ntwo institutional incentive approaches: i) a strict one (STRICT-COM) that\nrewards only committed players who cooperate in the game, and ii) a flexible\none (FLEXIBLE-COM) that rewards any committed players who do not defect in the\ngame. The results reveal that, while the strict approach is demonstrably better\nfor promoting cooperation as the flexible rule creates a loophole for an\nopportunistic exit after committing, the flexible rule offers an efficient\nalternative for enhancing social welfare when such opportunistic behaviour\nresults in a high gain. This study highlights the limitations of relying solely\non voluntary participation and commitment to resolving social dilemmas,\nemphasising the importance of well-designed institutional incentives to promote\ncooperation and social welfare effectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u5728\u53c2\u4e0e\u5177\u6709\u53ef\u9009\u6027\u7684\u56da\u5f92\u56f0\u5883\u535a\u5f08\u4e2d\uff0c\u627f\u8bfa\u548c\u5408\u4f5c\u7684\u6f14\u5316\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u53ef\u9009\u53c2\u4e0e\u867d\u63d0\u9ad8\u4e86\u627f\u8bfa\u63a5\u53d7\u7387\uff0c\u4f46\u5bfc\u81f4\u4e86\u5e7f\u6cdb\u7684\u9000\u51fa\u884c\u4e3a\u3002\u4e25\u683c\u7684\u5236\u5ea6\u6fc0\u52b1\u63aa\u65bd\u6bd4\u7075\u6d3b\u7684\u63aa\u65bd\u66f4\u80fd\u4fc3\u8fdb\u5408\u4f5c\uff0c\u4f46\u7075\u6d3b\u7684\u63aa\u65bd\u5728\u7279\u5b9a\u60c5\u51b5\u4e0b\u80fd\u63d0\u9ad8\u793e\u4f1a\u798f\u5229\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5236\u5ea6\u6fc0\u52b1\u63aa\u65bd\u5bf9\u4fc3\u8fdb\u5408\u4f5c\u548c\u793e\u4f1a\u798f\u5229\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u627f\u8bfa\u673a\u5236\u4ee5\u4fc3\u8fdb\u5408\u4f5c\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u73a9\u5bb6\u9009\u62e9\u4e0d\u53c2\u4e0e\u4e92\u52a8\u7684\u81ea\u7531\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u5e94\u7528\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u5728\u53c2\u4e0e\u5177\u6709\u53ef\u9009\u6027\u7684\u60c5\u51b5\u4e0b\uff0c\u627f\u8bfa\u548c\u5408\u4f5c\u7684\u6f14\u5316\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u6709\u6548\u7684\u5236\u5ea6\u6fc0\u52b1\u63aa\u65bd\u6765\u4fc3\u8fdb\u5408\u4f5c\u548c\u793e\u4f1a\u798f\u5229\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e24\u9636\u6bb5\u535a\u5f08\u6a21\u578b\uff0c\u5728\u53ef\u9009\u7684\u56da\u5f92\u56f0\u5883\u535a\u5f08\u6846\u67b6\u5185\uff0c\u7814\u7a76\u4e86\u57fa\u4e8e\u627f\u8bfa\u7684\u884c\u4e3a\u548c\u5408\u4f5c\u7684\u6f14\u5316\u3002\u9996\u5148\uff0c\u5728\u535a\u5f08\u524d\u9636\u6bb5\uff0c\u73a9\u5bb6\u51b3\u5b9a\u662f\u5426\u63a5\u53d7\u76f8\u4e92\u627f\u8bfa\uff1b\u7136\u540e\u5728\u535a\u5f08\u9636\u6bb5\uff0c\u73a9\u5bb6\u6839\u636e\u662f\u5426\u5f62\u6210\u4e8b\u5148\u627f\u8bfa\uff0c\u9009\u62e9\u5408\u4f5c\u3001\u80cc\u53db\u6216\u9000\u51fa\u3002\u4e4b\u540e\uff0c\u7814\u7a76\u5f15\u5165\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u5236\u5ea6\u6fc0\u52b1\u65b9\u6cd5\uff1a\u4e25\u683c\u7684\u627f\u8bfa\uff08STRICT-COM\uff09\u548c\u7075\u6d3b\u7684\u627f\u8bfa\uff08FLEXIBLE-COM\uff09\uff0c\u4ee5\u89e3\u51b3\u53ef\u9009\u53c2\u4e0e\u672a\u80fd\u4fc3\u8fdb\u5408\u4f5c\u7684\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u53ef\u9009\u53c2\u4e0e\u867d\u7136\u63d0\u9ad8\u4e86\u627f\u8bfa\u7684\u63a5\u53d7\u7387\uff0c\u4f46\u5e76\u672a\u80fd\u6709\u6548\u4fc3\u8fdb\u5408\u4f5c\uff0c\u53cd\u800c\u5bfc\u81f4\u4e86\u666e\u904d\u7684\u9000\u51fa\u884c\u4e3a\u3002\u5728\u5f15\u5165\u5236\u5ea6\u6fc0\u52b1\u63aa\u65bd\u540e\uff0c\u4e25\u683c\u7684\u627f\u8bfa\u65b9\u6cd5\uff08STRICT-COM\uff09\u5728\u4fc3\u8fdb\u5408\u4f5c\u65b9\u9762\u4f18\u4e8e\u7075\u6d3b\u7684\u627f\u8bfa\u65b9\u6cd5\uff08FLEXIBLE-COM\uff09\u3002\u7136\u800c\uff0c\u5f53\u673a\u4f1a\u4e3b\u4e49\u884c\u4e3a\u80fd\u5e26\u6765\u9ad8\u6536\u76ca\u65f6\uff0c\u7075\u6d3b\u7684\u627f\u8bfa\u65b9\u6cd5\uff08FLEXIBLE-COM\uff09\u4f5c\u4e3a\u4e00\u79cd\u63d0\u9ad8\u793e\u4f1a\u798f\u5229\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e5f\u5177\u6709\u4e00\u5b9a\u7684\u6548\u7387\u3002", "conclusion": "\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u4e8e\u627f\u8bfa\u673a\u5236\uff0c\u4f46\u5ffd\u89c6\u4e86\u73a9\u5bb6\u9009\u62e9\u4e0d\u53c2\u4e0e\u4e92\u52a8\u7684\u81ea\u7531\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u8bb8\u591a\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u4e24\u9636\u6bb5\u535a\u5f08\u6a21\u578b\uff0c\u5728\u53ef\u9009\u7684\u56da\u5f92\u56f0\u5883\u535a\u5f08\u6846\u67b6\u5185\u7814\u7a76\u57fa\u4e8e\u627f\u8bfa\u7684\u884c\u4e3a\u548c\u5408\u4f5c\u7684\u6f14\u5316\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u53ef\u9009\u53c2\u4e0e\u4f1a\u589e\u52a0\u627f\u8bfa\u63a5\u53d7\u7387\uff0c\u4f46\u672a\u80fd\u4fc3\u8fdb\u5408\u4f5c\uff0c\u53cd\u800c\u5bfc\u81f4\u4e86\u5e7f\u6cdb\u7684\u9000\u51fa\u884c\u4e3a\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u672c\u7814\u7a76\u5f15\u5165\u5e76\u6bd4\u8f83\u4e86\u4e24\u79cd\u5236\u5ea6\u6fc0\u52b1\u65b9\u6cd5\uff1a\u4e00\u79cd\u662f\u4e25\u683c\u7684\uff0c\u53ea\u5956\u52b1\u5728\u535a\u5f08\u4e2d\u5408\u4f5c\u7684\u627f\u8bfa\u73a9\u5bb6\uff1b\u53e6\u4e00\u79cd\u662f\u7075\u6d3b\u7684\uff0c\u5956\u52b1\u6240\u6709\u4e0d\u8fdd\u80cc\u627f\u8bfa\u7684\u73a9\u5bb6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u4e25\u683c\u7684\u65b9\u6cd5\u5728\u4fc3\u8fdb\u5408\u4f5c\u65b9\u9762\u660e\u663e\u4f18\u4e8e\u7075\u6d3b\u7684\u65b9\u6cd5\uff0c\u56e0\u4e3a\u7075\u6d3b\u7684\u89c4\u5219\u4e3a\u627f\u8bfa\u540e\u7684\u673a\u4f1a\u4e3b\u4e49\u9000\u51fa\u7559\u4e0b\u4e86\u6f0f\u6d1e\u3002\u7136\u800c\uff0c\u5f53\u8fd9\u79cd\u673a\u4f1a\u4e3b\u4e49\u884c\u4e3a\u5e26\u6765\u9ad8\u6536\u76ca\u65f6\uff0c\u7075\u6d3b\u7684\u89c4\u5219\u4e3a\u63d0\u9ad8\u793e\u4f1a\u798f\u5229\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\u3002\u672c\u7814\u7a76\u5f3a\u8c03\u4e86\u4ec5\u4f9d\u9760\u81ea\u613f\u53c2\u4e0e\u548c\u627f\u8bfa\u6765\u89e3\u51b3\u793e\u4f1a\u56f0\u5883\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5236\u5ea6\u6fc0\u52b1\u5bf9\u4e8e\u6709\u6548\u4fc3\u8fdb\u5408\u4f5c\u548c\u793e\u4f1a\u798f\u5229\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.08205", "categories": ["physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.08205", "abs": "https://arxiv.org/abs/2508.08205", "authors": ["Alfredo Franco", "Izan Calder\u00f3n", "Dolores Ortiz", "Jos\u00e9 L. Fern\u00e1ndez-Luna", "Fernando Moreno"], "title": "Optical discrimination of live single cancer cells using reflection-based nanohole array sensor", "comment": "4 pages, 2 figures, 13 references", "summary": "In this research, a reflection-based nanohole array sensor system is\npresented for discriminating between migration-competent cancer cells that\nmaintain the integrity of the actin cortex and those cells lacking the actin\ncortex and thus unable to migrate. Unlike previous transmission-based\napproaches, this configuration allows for more practical integration into in\nsitu diagnostic tools. For the first time, the system performance is analyzed\nby studying the spectral features of the reflected light by live single cells.\nWe demonstrate that the presence of the actin cortex, needed for cell\nmigration, in different types of cancer cells significantly affect their\noptical response, enabling high sensitivity and specificity in cell\nclassification. Our results pave the way for reflection-based plasmonic\nbiosensor devices as a compact and efficient platform for developing biomedical\napplication tools.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5c04\u7684\u7eb3\u7c73\u5b54\u9635\u5217\u4f20\u611f\u5668\uff0c\u53ef\u4ee5\u533a\u5206\u5177\u6709\u8fc1\u79fb\u80fd\u529b\u7684\u764c\u7ec6\u80de\u548c\u65e0\u6cd5\u8fc1\u79fb\u7684\u764c\u7ec6\u80de\uff0c\u5176\u51c6\u786e\u6027\u901a\u8fc7\u5206\u6790\u53cd\u5c04\u5149\u7684\u751f\u5316\u7279\u5f81\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u533a\u5206\u5177\u6709\u8fc1\u79fb\u80fd\u529b\u764c\u7ec6\u80de\u548c\u65e0\u6cd5\u8fc1\u79fb\u764c\u7ec6\u80de\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u540c\u65f6\u514b\u670d\u73b0\u6709\u900f\u5c04\u5f0f\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u5b9e\u9645\u7684\u8bca\u65ad\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5c04\u7684\u7eb3\u7c73\u5b54\u9635\u5217\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u901a\u8fc7\u5206\u6790\u6d3b\u5355\u7ec6\u80de\u5bf9\u53cd\u5c04\u5149\u7684\uff0c\u5149\u8c31\u7279\u5f81\u6765\u533a\u5206\u764c\u7ec6\u80de\u3002", "result": "\u7814\u7a76\u8bc1\u660e\uff0c\u808c\u52a8\u7403\u86cb\u767d\u76ae\u5c42\u7684\u5b58\u5728\u4f1a\u663e\u8457\u5f71\u54cd\u764c\u7ec6\u80de\u7684\u5149\u5b66\u54cd\u5e94\uff0c\u4ece\u800c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\u7684\u7ec6\u80de\u5206\u7c7b\u3002", "conclusion": "\u672c\u7814\u7a76\u5c55\u793a\u4e86\u4e00\u79cd\u57fa\u4e8e\u53cd\u5c04\u7684\u7eb3\u7c73\u5b54\u9635\u5217\u4f20\u611f\u5668\u7cfb\u7edf\uff0c\u7528\u4e8e\u533a\u5206\u5177\u6709\u5b8c\u6574\u808c\u52a8\u7403\u86cb\u767d\u76ae\u5c42\u7684\u8fc1\u79fb\u80fd\u529b\u764c\u7ec6\u80de\u548c\u7f3a\u4e4f\u808c\u52a8\u7403\u86cb\u767d\u76ae\u5c42\u800c\u65e0\u6cd5\u8fc1\u79fb\u7684\u764c\u7ec6\u80de\u3002\u4e0e\u5148\u524d\u7684\u900f\u5c04\u5f0f\u65b9\u6cd5\u4e0d\u540c\uff0c\u8fd9\u79cd\u914d\u7f6e\u53ef\u4ee5\u66f4\u5b9e\u9645\u5730\u96c6\u6210\u5230\u539f\u5730\u8bca\u65ad\u5de5\u5177\u4e2d\u3002\u9996\u6b21\u901a\u8fc7\u7814\u7a76\u6d3b\u5355\u7ec6\u80de\u5bf9\u53cd\u5c04\u5149\u7684\u5149\u8c31\u7279\u5f81\u6765\u5206\u6790\u7cfb\u7edf\u6027\u80fd\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5bf9\u4e8e\u7ec6\u80de\u8fc1\u79fb\u6240\u5fc5\u9700\u7684\u808c\u52a8\u7403\u86cb\u767d\u76ae\u5c42\u7684\u5b58\u5728\uff0c\u4f1a\u663e\u8457\u5f71\u54cd\u4e0d\u540c\u7c7b\u578b\u764c\u7ec6\u80de\u7684\u5149\u5b66\u54cd\u5e94\uff0c\u4ece\u800c\u5b9e\u73b0\u9ad8\u7075\u654f\u5ea6\u548c\u7279\u5f02\u6027\u7684\u7ec6\u80de\u5206\u7c7b\u3002\u672c\u7814\u7a76\u7ed3\u679c\u4e3a\u57fa\u4e8e\u53cd\u5c04\u7684\u7b49\u79bb\u5b50\u4f53\u751f\u7269\u4f20\u611f\u5668\u8bbe\u5907\u4f5c\u4e3a\u5f00\u53d1\u751f\u7269\u533b\u5b66\u5e94\u7528\u5de5\u5177\u7684\u7d27\u51d1\u9ad8\u6548\u5e73\u53f0\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.07522", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07522", "abs": "https://arxiv.org/abs/2508.07522", "authors": ["Jim O'Connor", "Derin Gezgin", "Gary B. Parker"], "title": "Evolutionary Optimization of Deep Learning Agents for Sparrow Mahjong", "comment": "AAAI conference on Artificial Intelligence and Interactive Digital\n  Entertainment", "summary": "We present Evo-Sparrow, a deep learning-based agent for AI decision-making in\nSparrow Mahjong, trained by optimizing Long Short-Term Memory (LSTM) networks\nusing Covariance Matrix Adaptation Evolution Strategy (CMA-ES). Our model\nevaluates board states and optimizes decision policies in a non-deterministic,\npartially observable game environment. Empirical analysis conducted over a\nsignificant number of simulations demonstrates that our model outperforms both\nrandom and rule-based agents, and achieves performance comparable to a Proximal\nPolicy Optimization (PPO) baseline, indicating strong strategic play and robust\npolicy quality. By combining deep learning with evolutionary optimization, our\napproach provides a computationally effective alternative to traditional\nreinforcement learning and gradient-based optimization methods. This research\ncontributes to the broader field of AI game playing, demonstrating the\nviability of hybrid learning strategies for complex stochastic games. These\nfindings also offer potential applications in adaptive decision-making and\nstrategic AI development beyond Sparrow Mahjong.", "AI": {"tldr": "Evo-Sparrow, an AI agent using deep learning (LSTM) and evolutionary optimization (CMA-ES), excels in Sparrow Mahjong, offering an efficient alternative to traditional methods for complex games.", "motivation": "The paper aims to develop an AI agent for the complex stochastic game of Sparrow Mahjong, exploring the viability of hybrid learning strategies combining deep learning and evolutionary optimization as a computationally effective alternative to traditional reinforcement learning methods.", "method": "A deep learning-based agent, Evo-Sparrow, was developed using LSTM networks optimized with Covariance Matrix Adaptation Evolution Strategy (CMA-ES) to evaluate board states and optimize decision policies in Sparrow Mahjong.", "result": "The Evo-Sparrow model outperforms random and rule-based agents and shows performance comparable to a Proximal Policy Optimization (PPO) baseline in Sparrow Mahjong simulations.", "conclusion": "Evo-Sparrow, a deep learning-based agent, demonstrates strong strategic play and robust policy quality in Sparrow Mahjong, outperforming random and rule-based agents and achieving performance comparable to PPO baselines. The hybrid approach of deep learning with evolutionary optimization offers a computationally effective alternative to traditional reinforcement learning and gradient-based methods, with potential applications in adaptive decision-making and strategic AI development."}}
{"id": "2508.07264", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2508.07264", "abs": "https://arxiv.org/abs/2508.07264", "authors": ["Van Duc Cuong", "Ta Dinh Tam", "Tran Duc Chinh", "Nguyen Thi Hanh"], "title": "FLUID: Flow-Latent Unified Integration via Token Distillation for Expert Specialization in Multimodal Learning", "comment": null, "summary": "Multimodal classification requires robust integration of visual and textual\nsignals, yet common fusion strategies are brittle and vulnerable to\nmodality-specific noise. In this paper, we present \\textsc{FLUID}-Flow-Latent\nUnified Integration via Token Distillation for Expert Specialization, a\nprincipled token-level pipeline that improves cross-modal robustness and\nscalability. \\textsc{FLUID} contributes three core elements: (1)\n\\emph{Q-transforms}, learnable query tokens that distill and retain salient\ntoken-level features from modality-specific backbones; (2) a two-stage fusion\nscheme that enforces cross-modal consistency via contrastive alignment and then\nperforms adaptive, task-aware fusion through a gating mechanism and a\n\\emph{Q-bottleneck} that selectively compresses information for downstream\nreasoning; and (3) a lightweight, load-balanced Mixture-of-Experts at\nprediction time that enables efficient specialization to diverse semantic\npatterns. Extensive experiments demonstrate that \\textsc{FLUID} attains\n\\(91\\%\\) accuracy on the GLAMI-1M benchmark, significantly outperforming prior\nbaselines and exhibiting strong resilience to label noise, long-tail class\nimbalance, and semantic heterogeneity. Targeted ablation studies corroborate\nboth the individual and synergistic benefits of the proposed components,\npositioning \\textsc{FLUID} as a scalable, noise-resilient solution for\nmultimodal product classification.", "AI": {"tldr": "FLUID\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u6a21\u6001\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u4ee4\u724c\u84b8\u998f\u5b9e\u73b0\u8de8\u6a21\u6001\u7684\u7edf\u4e00\u96c6\u6210\uff0c\u63d0\u9ad8\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u5e76\u5728GLAMI-1M\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5206\u7c7b\u65b9\u6cd5\u5728\u878d\u5408\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u53f7\u65f6\uff0c\u901a\u5e38\u91c7\u7528\u7684\u878d\u5408\u7b56\u7565\u6bd4\u8f83\u8106\u5f31\uff0c\u5bb9\u6613\u53d7\u5230\u7279\u5b9a\u6a21\u6001\u566a\u58f0\u7684\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u63d0\u9ad8\u8de8\u6a21\u6001\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u7684\u65b9\u6cd5\u3002", "method": "FLUID\uff08Flow-Latent Unified Integration via Token Distillation\uff09\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee4\u724c\u7684\u7ba1\u9053\uff0c\u901a\u8fc7Q-transforms\uff08\u53ef\u5b66\u4e60\u7684\u67e5\u8be2\u4ee4\u724c\uff09\u6765\u63d0\u53d6\u548c\u4fdd\u7559\u7279\u5b9a\u6a21\u6001\u9aa8\u5e72\u7f51\u7edc\u4e2d\u7684\u663e\u8457\u4ee4\u724c\u7ea7\u7279\u5f81\u3002\u5176\u7279\u70b9\u5305\u62ec\uff1a1. Q-transforms\uff1a\u7528\u4e8e\u4ece\u7279\u5b9a\u6a21\u6001\u9aa8\u5e72\u7f51\u7edc\u4e2d\u63d0\u53d6\u548c\u4fdd\u7559\u663e\u8457\u7684\u4ee4\u724c\u7ea7\u7279\u5f81\u30022. \u4e24\u9636\u6bb5\u878d\u5408\u65b9\u6848\uff1a\u9996\u5148\u901a\u8fc7\u5bf9\u6bd4\u5bf9\u9f50\u5f3a\u5236\u5b9e\u73b0\u8de8\u6a21\u6001\u4e00\u81f4\u6027\uff0c\u7136\u540e\u901a\u8fc7\u95e8\u63a7\u673a\u5236\u548cQ-bottleneck\uff08\u9009\u62e9\u6027\u538b\u7f29\u4fe1\u606f\u4ee5\u9002\u5e94\u4e0b\u6e38\u63a8\u7406\uff09\u8fdb\u884c\u81ea\u9002\u5e94\u3001\u4efb\u52a1\u611f\u77e5\u7684\u878d\u5408\u30023. \u8f7b\u91cf\u7ea7\u7684\u3001\u8d1f\u8f7d\u5747\u8861\u7684\u4e13\u5bb6\u6df7\u5408\uff08Mixture-of-Experts\uff09\u6a21\u578b\uff1a\u5728\u9884\u6d4b\u65f6\u5b9e\u73b0\u9ad8\u6548\u7684\u4e13\u4e1a\u5316\uff0c\u4ee5\u9002\u5e94\u591a\u6837\u7684\u8bed\u4e49\u6a21\u5f0f\u3002", "result": "FLUID\u5728GLAMI-1M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8691%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cFLUID\u5728\u9762\u5bf9\u6807\u7b7e\u566a\u58f0\u3001\u957f\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8bed\u4e49\u5f02\u6784\u6027\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u8bc1\u5b9e\u4e86\u6240\u63d0\u51fa\u7ec4\u4ef6\u7684\u5355\u72ec\u548c\u534f\u540c\u6548\u76ca\u3002", "conclusion": "FLUID\u662f\u4e00\u79cd\u53ef\u6269\u5c55\u3001\u6297\u566a\u58f0\u7684\u591a\u6a21\u6001\u4ea7\u54c1\u5206\u7c7b\u89e3\u51b3\u65b9\u6848\uff0c\u5728GLAMI-1M\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u4e8691%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5bf9\u6807\u7b7e\u566a\u58f0\u3001\u957f\u5c3e\u7c7b\u522b\u4e0d\u5e73\u8861\u548c\u8bed\u4e49\u5f02\u6784\u6027\u5177\u6709\u5f88\u5f3a\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.06525", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06525", "abs": "https://arxiv.org/abs/2508.06525", "authors": ["Guoyuan An", "JaeYoon Kim", "SungEui Yoon"], "title": "Large Language Models Facilitate Vision Reflection in Image Classification", "comment": null, "summary": "This paper presents several novel findings on the explainability of vision\nreflection in large multimodal models (LMMs). First, we show that prompting an\nLMM to verify the prediction of a specialized vision model can improve\nrecognition accuracy, even on benchmarks like ImageNet, despite prior evidence\nthat LMMs typically underperform dedicated vision encoders. Second, we analyze\nthe internal behavior of vision reflection and find that the vision-language\nconnector maps visual features into explicit textual concepts, allowing the\nlanguage model to reason about prediction plausibility using commonsense\nknowledge. We further observe that replacing a large number of vision tokens\nwith only a few text tokens still enables LLaVA to generate similar answers,\nsuggesting that LMMs may rely primarily on a compact set of distilled textual\nrepresentations rather than raw vision features. Third, we show that a\ntraining-free connector can enhance LMM performance in fine-grained recognition\ntasks, without extensive feature-alignment training. Together, these findings\noffer new insights into the explainability of vision-language models and\nsuggest that vision reflection is a promising strategy for achieving robust and\ninterpretable visual recognition.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u63d0\u793aLMM\u9a8c\u8bc1\u9884\u6d4b\u3001\u5206\u6790\u5176\u5185\u90e8\u673a\u5236\u4ee5\u53ca\u4f7f\u7528\u7d27\u51d1\u7684\u6587\u672c\u8868\u793a\uff0c\u5c55\u793a\u4e86\u63d0\u9ad8LMM\u89c6\u89c9\u8bc6\u522b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u5373\u53ef\u589e\u5f3a\u6a21\u578b\u6027\u80fd\u7684\u8fde\u63a5\u5668\u3002", "motivation": "\u63a2\u7a76\u5927\u578b\u591a\u6a21\u6001\u6a21\u578b\uff08LMM\uff09\u4e2d\u89c6\u89c9\u53cd\u5c04\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u63d0\u51fa\u63d0\u9ad8LMM\u89c6\u89c9\u8bc6\u522b\u80fd\u529b\u7684\u65b0\u65b9\u6cd5\u3002", "method": "1. \u63d0\u793aLMM\u9a8c\u8bc1\u4e13\u7528\u89c6\u89c9\u6a21\u578b\u7684\u9884\u6d4b\u4ee5\u63d0\u9ad8\u8bc6\u522b\u51c6\u786e\u6027\u3002 2. \u5206\u6790\u89c6\u89c9\u53cd\u5c04\u7684\u5185\u90e8\u884c\u4e3a\uff0c\u53d1\u73b0\u89c6\u89c9-\u8bed\u8a00\u8fde\u63a5\u5668\u5c06\u89c6\u89c9\u7279\u5f81\u6620\u5c04\u5230\u663e\u6027\u6587\u672c\u6982\u5ff5\u3002 3. \u89c2\u5bdf\u5230\u7528\u5c11\u91cf\u6587\u672c\u6807\u8bb0\u66ff\u6362\u5927\u91cf\u89c6\u89c9\u6807\u8bb0\u4ecd\u80fd\u4f7fLLaVA\u751f\u6210\u76f8\u4f3c\u7684\u7b54\u6848\u3002 4. \u8bc1\u660e\u514d\u8bad\u7ec3\u8fde\u63a5\u5668\u53ef\u4ee5\u5728\u6ca1\u6709\u5e7f\u6cdb\u7279\u5f81\u5bf9\u9f50\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u589e\u5f3aLMM\u5728\u7ec6\u7c92\u5ea6\u8bc6\u522b\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "result": "1. \u63d0\u793aLMM\u8fdb\u884c\u9a8c\u8bc1\u53ef\u63d0\u9ad8ImageNet\u7b49\u57fa\u51c6\u7684\u8bc6\u522b\u51c6\u786e\u6027\u3002 2. \u89c6\u89c9-\u8bed\u8a00\u8fde\u63a5\u5668\u5c06\u89c6\u89c9\u7279\u5f81\u6620\u5c04\u5230\u6587\u672c\u6982\u5ff5\uff0c\u4f7f\u8bed\u8a00\u6a21\u578b\u80fd\u591f\u8fdb\u884c\u63a8\u7406\u3002 3. LMM\u53ef\u80fd\u4e3b\u8981\u4f9d\u8d56\u4e8e\u7cbe\u70bc\u7684\u6587\u672c\u8868\u793a\u800c\u975e\u539f\u59cb\u89c6\u89c9\u7279\u5f81\u3002 4. \u514d\u8bad\u7ec3\u8fde\u63a5\u5668\u53ef\u65e0\u9700\u5927\u91cf\u8bad\u7ec3\u5373\u53ef\u63d0\u5347\u7ec6\u7c92\u5ea6\u8bc6\u522b\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u7406\u89e3\u548c\u6539\u8fdb\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LMM\uff09\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\uff0c\u5e76\u8868\u660e\u89c6\u89c9\u53cd\u5c04\u662f\u4e00\u79cd\u5b9e\u73b0\u9c81\u68d2\u548c\u53ef\u89e3\u91ca\u89c6\u89c9\u8bc6\u522b\u7684\u6709\u524d\u666f\u7684\u7b56\u7565\u3002"}}
{"id": "2508.07834", "categories": ["cs.AI", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.07834", "abs": "https://arxiv.org/abs/2508.07834", "authors": ["Mubaris Nadeem", "Johannes Zenkert", "Lisa Bender", "Christian Weber", "Madjid Fathi"], "title": "KIRETT: Knowledge-Graph-Based Smart Treatment Assistant for Intelligent Rescue Operations", "comment": "LWDA'23, KIRETT project, University of Siegen, Germany", "summary": "Over the years, the need for rescue operations throughout the world has\nincreased rapidly. Demographic changes and the resulting risk of injury or\nhealth disorders form the basis for emergency calls. In such scenarios, first\nresponders are in a rush to reach the patient in need, provide first aid, and\nsave lives. In these situations, they must be able to provide personalized and\noptimized healthcare in the shortest possible time and estimate the patients\ncondition with the help of freshly recorded vital data in an emergency\nsituation. However, in such a timedependent situation, first responders and\nmedical experts cannot fully grasp their knowledge and need assistance and\nrecommendation for further medical treatments. To achieve this, on the spot\ncalculated, evaluated, and processed knowledge must be made available to\nimprove treatments by first responders. The Knowledge Graph presented in this\narticle as a central knowledge representation provides first responders with an\ninnovative knowledge management that enables intelligent treatment\nrecommendations with an artificial intelligence-based pre-recognition of the\nsituation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u667a\u80fd\u6025\u6551\u7cfb\u7edf\uff0c\u5229\u7528AI\u6280\u672f\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u5b9e\u65f6\u7684\u6cbb\u7597\u5efa\u8bae\uff0c\u4ee5\u5e94\u5bf9\u65e5\u76ca\u589e\u957f\u7684\u6025\u6551\u9700\u6c42\u3002", "motivation": "\u968f\u7740\u4eba\u53e3\u7ed3\u6784\u53d8\u5316\u548c\u5065\u5eb7\u98ce\u9669\u589e\u52a0\uff0c\u6025\u6551\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u6025\u6551\u4eba\u5458\u9700\u8981\u5728\u7d27\u6025\u60c5\u51b5\u4e0b\u5feb\u901f\u3001\u4e2a\u6027\u5316\u5730\u63d0\u4f9b\u533b\u7597\u6551\u52a9\uff0c\u4f46\u5f80\u5f80\u77e5\u8bc6\u638c\u63e1\u4e0d\u8db3\uff0c\u9700\u8981\u8f85\u52a9\u548c\u5efa\u8bae\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u77e5\u8bc6\u56fe\u8c31\u7684\u77e5\u8bc6\u7ba1\u7406\u65b9\u6cd5\uff0c\u5229\u7528\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u9884\u8bc6\u522b\uff0c\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u667a\u80fd\u5316\u7684\u6cbb\u7597\u5efa\u8bae\u3002", "result": "\u6240\u63d0\u51fa\u7684\u77e5\u8bc6\u56fe\u8c31\u65b9\u6cd5\u80fd\u591f\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u53ca\u65f6\u7684\u3001\u7ecf\u8fc7\u8bc4\u4f30\u548c\u5904\u7406\u7684\u77e5\u8bc6\uff0c\u4ee5\u6539\u8fdb\u6cbb\u7597\u6548\u679c\u3002", "conclusion": "\u77e5\u8bc6\u56fe\u8c31\u4f5c\u4e3a\u4e00\u79cd\u96c6\u4e2d\u7684\u77e5\u8bc6\u8868\u793a\uff0c\u4e3a\u6025\u6551\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u77e5\u8bc6\u7ba1\u7406\uff0c\u80fd\u591f\u901a\u8fc7\u4eba\u5de5\u667a\u80fd\u8fdb\u884c\u9884\u8bc6\u522b\uff0c\u4ece\u800c\u63d0\u4f9b\u667a\u80fd\u5316\u7684\u6cbb\u7597\u5efa\u8bae\u3002"}}
{"id": "2508.06893", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06893", "abs": "https://arxiv.org/abs/2508.06893", "authors": ["Evagoras Makridis", "Gabriele Oliva", "Apostolos I. Rikos", "Themistoklis Charalambous"], "title": "Average Consensus with Dynamic Compression in Bandwidth-Limited Directed Networks", "comment": null, "summary": "In this paper, the average consensus problem has been considered for directed\nunbalanced networks under finite bit-rate communication. We propose the\nPush-Pull Average Consensus algorithm with Dynamic Compression (PP-ACDC)\nalgorithm, a distributed consensus algorithm that deploys an adaptive\nquantization scheme and achieves convergence to the exact average without the\nneed of global information. A preliminary numerical convergence analysis and\nsimulation results corroborate the performance of PP-ACDC.", "AI": {"tldr": "PP-ACDC \u662f\u4e00\u79cd\u7528\u4e8e\u6709\u5411\u4e0d\u5e73\u8861\u7f51\u7edc\u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\uff0c\u5b83\u4f7f\u7528\u81ea\u9002\u5e94\u91cf\u5316\u5728\u6709\u9650\u6bd4\u7279\u7387\u4e0b\u6536\u655b\u5230\u7cbe\u786e\u5e73\u5747\u503c\u3002", "motivation": "\u89e3\u51b3\u6709\u5411\u4e0d\u5e73\u8861\u7f51\u7edc\u5728\u6709\u9650\u6bd4\u7279\u7387\u901a\u4fe1\u4e0b\u7684\u5e73\u5747\u5171\u8bc6\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PP-ACDC (Push-Pull Average Consensus with Dynamic Compression) \u7684\u5206\u5e03\u5f0f\u5171\u8bc6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u91c7\u7528\u81ea\u9002\u5e94\u91cf\u5316\u65b9\u6848\u3002", "result": "\u6570\u503c\u6536\u655b\u6027\u5206\u6790\u548c\u6a21\u62df\u7ed3\u679c\u8bc1\u5b9e\u4e86 PP-ACDC \u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "PP-ACDC \u7b97\u6cd5\u6536\u655b\u5230\u7cbe\u786e\u5e73\u5747\u503c\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u5168\u5c40\u4fe1\u606f\u3002"}}
{"id": "2508.07024", "categories": ["cond-mat.mes-hall", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2508.07024", "abs": "https://arxiv.org/abs/2508.07024", "authors": ["Baruch Horovitz", "Pierre Le Doussal"], "title": "Randomly twisted bilayer graphene -- the cascade transitions", "comment": "14 pages, 2 tables, Supplementary added", "summary": "Twisted bilayer graphene (TBG) is known to have disorder in its twist angle.\nWe show that in terms of a Dirac equation with a random gauge potential ${\\bf\nA}({\\bf r})$ this disorder becomes huge when the average twist angle is near\nthe magic angle where the Dirac velocity vanishes. The density of states (DOS)\nthen diverges at the Dirac point as $\\rho(E)\\sim E^{(2/z)-1}$ with $z\\gg 1$ and\nwe deduce that all electrons occupy energies very near $E=0$. We prove a sum\nrule on the disorder averaged eigenfunctions from which we deduce that each\nadded electron contributes equal intraband Coulomb interaction energy. The\nvarious bands in TBG are related by either ${\\bf A}({\\bf r})\\rightarrow {\\bf\nA}({-\\bf r})$ or ${\\bf A}({\\bf r})\\rightarrow -{\\bf A}({\\bf r})$ which affects\nthe interband interaction energy. We find, within Hartree-Fock, jumps in the\nchemical potential at each integer filling, accounting for the cascade\ntransitions.", "AI": {"tldr": "Disorder in twisted bilayer graphene (TBG) becomes huge near the magic angle, causing the density of states to diverge at the Dirac point. Electrons occupy energies near E=0, and each added electron contributes equal intraband Coulomb interaction energy. Jumps in the chemical potential are observed at integer fillings.", "motivation": "Twisted bilayer graphene (TBG) is known to have disorder in its twist angle. We show that in terms of a Dirac equation with a random gauge potential A(r) this disorder becomes huge when the average twist angle is near the magic angle where the Dirac velocity vanishes.", "method": "We prove a sum rule on the disorder averaged eigenfunctions from which we deduce that each added electron contributes equal intraband Coulomb interaction energy. The various bands in TBG are related by either A(r)\u2192A(\u2212r) or A(r)\u2192\u2212A(r) which affects the interband interaction energy.", "result": "The density of states (DOS) then diverges at the Dirac point as \u03c1(E)\u223cE^((2/z)\u22121) with z\u226b1 and we deduce that all electrons occupy energies very near E=0.", "conclusion": "We find, within Hartree-Fock, jumps in the chemical potential at each integer filling, accounting for the cascade transitions."}}
{"id": "2508.06868", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06868", "abs": "https://arxiv.org/abs/2508.06868", "authors": ["Bin Lyu", "Jiayu Guan", "Meng Hua", "Changsheng You", "Tianqi Mao", "Abbas Jamalipour"], "title": "Secure Transmission for Cell-Free Symbiotic Radio Communications with Movable Antenna: Continuous and Discrete Positioning Designs", "comment": "14 pages,6 figures", "summary": "In this paper, we study a movable antenna (MA) empowered secure transmission\nscheme for reconfigurable intelligent surface (RIS) aided cell-free symbiotic\nradio (SR) system. Specifically, the MAs deployed at distributed access points\n(APs) work collaboratively with the RIS to establish high-quality propagation\nlinks for both primary and secondary transmissions, as well as suppressing the\nrisk of eavesdropping on confidential primary information. We consider both\ncontinuous and discrete MA position cases and maximize the secrecy rate of\nprimary transmission under the secondary transmission constraints,\nrespectively. For the continuous position case, we propose a two-layer\niterative optimization method based on differential evolution with one-in-one\nrepresentation (DEO), to find a high-quality solution with relatively moderate\ncomputational complexity. For the discrete position case, we first extend the\nDEO based iterative framework by introducing the mapping and determination\noperations to handle the characteristic of discrete MA positions. To further\nreduce the computational complexity, we then design an alternating optimization\n(AO) iterative framework to solve all variables within a single layer. In\nparticular, we develop an efficient strategy to derive the sub-optimal solution\nfor the discrete MA positions, superseding the DEO-based method. Numerical\nresults validate the effectiveness of the proposed MA empowered secure\ntransmission scheme along with its optimization algorithms.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07963", "categories": ["cs.LO"], "pdf": "https://arxiv.org/pdf/2508.07963", "abs": "https://arxiv.org/abs/2508.07963", "authors": ["Javier Esparza", "Vincent Fischer"], "title": "Runtime Verification for LTL in Stochastic Systems", "comment": null, "summary": "Runtime verification encompasses several lightweight techniques for checking\nwhether a system's current execution satisfies a given specification. We focus\non runtime verification for Linear Temporal Logic (LTL). Previous work\ndescribes monitors which produce, at every time step one of three outputs -\ntrue, false, or inconclusive - depending on whether the observed execution\nprefix definitively determines satisfaction of the formula. However, for many\nLTL formulas, such as liveness properties, satisfaction cannot be concluded\nfrom any finite prefix. For these properties traditional monitors will always\noutput inconclusive. In this work, we propose a novel monitoring approach that\nreplaces hard verdicts with probabilistic predictions and an associated\nconfidence score. Our method guarantees eventual correctness of the prediction\nand ensures that confidence increases without bound from that point on.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8fd0\u884c\u65f6\u9a8c\u8bc1\u65b9\u6cd5\uff0c\u7528\u6982\u7387\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u5206\u6570\u53d6\u4ee3\u786c\u6027\u5224\u65ad\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u6d3b\u6027\u5c5e\u6027\u65f6\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u8fd0\u884c\u65f6\u9a8c\u8bc1\u5668\uff08monitor\uff09\u5728\u6bcf\u4e00\u6b65\u53ea\u8f93\u51fa\u4e09\u4e2a\u7ed3\u679c\u4e4b\u4e00\uff1a\u771f\u3001\u5047\u6216\u4e0d\u786e\u5b9a\u3002\u7136\u800c\uff0c\u5bf9\u4e8e\u8bb8\u591aLTL\uff08\u7ebf\u6027\u65f6\u5e8f\u903b\u8f91\uff09\u516c\u5f0f\uff0c\u7279\u522b\u662f\u6d3b\u6027\u5c5e\u6027\uff0c\u65e0\u6cd5\u4ece\u6709\u9650\u7684\u524d\u7f00\u786e\u5b9a\u6ee1\u8db3\u6027\uff0c\u5bfc\u81f4\u4f20\u7edf\u76d1\u63a7\u5668\u603b\u662f\u8f93\u51fa\u4e0d\u786e\u5b9a\u3002\u8fd9\u4fc3\u4f7f\u6211\u4eec\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u76d1\u63a7\u65b9\u6cd5\uff0c\u7528\u6982\u7387\u9884\u6d4b\u548c\u76f8\u5173\u7684\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u53d6\u4ee3\u786c\u6027\u5224\u65ad\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u7684\u9884\u6d4b\u662f\u6700\u7ec8\u6b63\u786e\u7684\uff0c\u5e76\u4e14\u7f6e\u4fe1\u5ea6\u4f1a\u968f\u7740\u65f6\u95f4\u7684\u63a8\u79fb\u800c\u4e0d\u65ad\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u63d0\u4f9b\u6982\u7387\u9884\u6d4b\u548c\u7f6e\u4fe1\u5ea6\u5206\u6570\u6765\u66ff\u6362\u786c\u6027\u5224\u65ad\uff0c\u80fd\u591f\u4fdd\u8bc1\u9884\u6d4b\u7684\u6700\u7ec8\u6b63\u786e\u6027\uff0c\u5e76\u786e\u4fdd\u5728\u6b64\u4e4b\u540e\u7f6e\u4fe1\u5ea6\u65e0\u754c\u589e\u957f\u3002"}}
{"id": "2508.06668", "categories": ["cs.AI", "cs.IR", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.06668", "abs": "https://arxiv.org/abs/2508.06668", "authors": ["Jessie Galasso"], "title": "Formal Concept Analysis: a Structural Framework for Variability Extraction and Analysis", "comment": null, "summary": "Formal Concept Analysis (FCA) is a mathematical framework for knowledge\nrepresentation and discovery. It performs a hierarchical clustering over a set\nof objects described by attributes, resulting in conceptual structures in which\nobjects are organized depending on the attributes they share. These conceptual\nstructures naturally highlight commonalities and variabilities among similar\nobjects by categorizing them into groups which are then arranged by similarity,\nmaking it particularly appropriate for variability extraction and analysis.\nDespite the potential of FCA, determining which of its properties can be\nleveraged for variability-related tasks (and how) is not always\nstraightforward, partly due to the mathematical orientation of its foundational\nliterature. This paper attempts to bridge part of this gap by gathering a\nselection of properties of the framework which are essential to variability\nanalysis, and how they can be used to interpret diverse variability information\nwithin the resulting conceptual structures.", "AI": {"tldr": "FCA is a mathematical framework for knowledge discovery that organizes objects based on shared attributes, making it suitable for variability analysis. This paper highlights essential FCA properties for variability analysis and demonstrates how to interpret variability information within its resulting conceptual structures.", "motivation": "To bridge the gap in understanding how FCA properties can be leveraged for variability-related tasks.", "method": "Gathering a selection of properties of the framework which are essential to variability analysis.", "result": "Conceptual structures that highlight commonalities and variabilities among similar objects by categorizing them into groups arranged by similarity.", "conclusion": " FCA can be used to interpret diverse variability information within conceptual structures."}}
{"id": "2508.07067", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.07067", "abs": "https://arxiv.org/abs/2508.07067", "authors": ["Honghao Lin", "Hoai-An Nguyen", "William Swartworth", "David P. Woodruff"], "title": "Unbiased Insights: Optimal Streaming Algorithms for $\\ell_p$ Sampling, the Forget Model, and Beyond", "comment": null, "summary": "We study $\\ell_p$ sampling and frequency moment estimation in a single-pass\ninsertion-only data stream. For $p \\in (0,2)$, we present a nearly\nspace-optimal approximate $\\ell_p$ sampler that uses $\\widetilde{O}(\\log n\n\\log(1/\\delta))$ bits of space and for $p = 2$, we present a sampler with space\ncomplexity $\\widetilde{O}(\\log^2 n \\log(1/\\delta))$. This space complexity is\noptimal for $p \\in (0, 2)$ and improves upon prior work by a $\\log n$ factor.\nWe further extend our construction to a continuous $\\ell_p$ sampler, which\noutputs a valid sample index at every point during the stream.\n  Leveraging these samplers, we design nearly unbiased estimators for $F_p$ in\ndata streams that include forget operations, which reset individual element\nfrequencies and introduce significant non-linear challenges. As a result, we\nobtain near-optimal algorithms for estimating $F_p$ for all $p$ in this model,\noriginally proposed by Pavan, Chakraborty, Vinodchandran, and Meel [PODS'24],\nresolving all three open problems they posed.\n  Furthermore, we generalize this model to what we call the suffix-prefix\ndeletion model, and extend our techniques to estimate entropy as a corollary of\nour moment estimation algorithms. Finally, we show how to handle arbitrary\ncoordinate-wise functions during the stream, for any $g \\in \\mathbb{G}$, where\n$\\mathbb{G}$ includes all (linear or non-linear) contraction functions.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u7528\u4e8e\u6570\u636e\u6d41\u7684$\\\\\\ell_p$\u91c7\u6837\u548c\u9891\u7387\u4f30\u8ba1\u7684\u65b0\u7b97\u6cd5\uff0c\u89e3\u51b3\u4e86\u4e09\u4e2a\u5f00\u653e\u6027\u95ee\u9898\uff0c\u5e76\u5728\u7a7a\u95f4\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3Pavan\u7b49\u4eba\u63d0\u51fa\u7684\u5173\u4e8e\u6570\u636e\u6d41\u4e2d\u7684$\\\\\\ell_p$\u91c7\u6837\u548c\u9891\u7387\u4f30\u8ba1\u95ee\u9898\u7684\u4e09\u4e2a\u5f00\u653e\u6027\u95ee\u9898\u3002", "method": "\u4f7f\u7528$\\\\\\ell_p$\u91c7\u6837\u548c\u9891\u7387\u4f30\u8ba1\u6280\u672f\u3002", "result": "\u5728\u6570\u636e\u6d41\u73af\u5883\u4e2d\uff0c\u4e3a$\\\\\\ell_p$\u4f30\u8ba1\u548c\u71b5\u4f30\u8ba1\u8bbe\u8ba1\u4e86\u8fd1\u4e4e\u6700\u4f18\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3ap\u2208(0,2)\u7684\u51e0\u4e4e\u7a7a\u95f4\u6700\u4f18\u4f30\u8ba1\u5668\u63d0\u4f9b\u4e86\u4e00\u4e2a\u4f7f\u7528$\\\tilde O(\\\\\\log n \\\\log(1/\\\\delta))$\u6bd4\u7279\u7a7a\u95f4\u901a\u8fc7\u4e86\u4f18\u5316\uff0c\u5e76\u4e14\u4e3ap=2\u7684\u4f30\u8ba1\u5668\u63d0\u4f9b\u4e86$\\\\\\tilde O(\\\\\\log^2 n \\\\log(1/\\\\delta))$\u7a7a\u95f4\u590d\u6742\u5ea6\u3002"}}
{"id": "2508.06588", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06588", "abs": "https://arxiv.org/abs/2508.06588", "authors": ["Zian Zhai", "Fan Li", "Xingyu Tan", "Xiaoyang Wang", "Wenjie Zhang"], "title": "Graph is a Natural Regularization: Revisiting Vector Quantization for Graph Representation Learning", "comment": null, "summary": "Vector Quantization (VQ) has recently emerged as a promising approach for\nlearning discrete representations of graph-structured data. However, a\nfundamental challenge, i.e., codebook collapse, remains underexplored in the\ngraph domain, significantly limiting the expressiveness and generalization of\ngraph tokens.In this paper, we present the first empirical study showing that\ncodebook collapse consistently occurs when applying VQ to graph data, even with\nmitigation strategies proposed in vision or language domains. To understand why\ngraph VQ is particularly vulnerable to collapse, we provide a theoretical\nanalysis and identify two key factors: early assignment imbalances caused by\nredundancy in graph features and structural patterns, and self-reinforcing\noptimization loops in deterministic VQ. To address these issues, we propose\nRGVQ, a novel framework that integrates graph topology and feature similarity\nas explicit regularization signals to enhance codebook utilization and promote\ntoken diversity. RGVQ introduces soft assignments via Gumbel-Softmax\nreparameterization, ensuring that all codewords receive gradient updates. In\naddition, RGVQ incorporates a structure-aware contrastive regularization to\npenalize the token co-assignments among similar node pairs. Extensive\nexperiments demonstrate that RGVQ substantially improves codebook utilization\nand consistently boosts the performance of state-of-the-art graph VQ backbones\nacross multiple downstream tasks, enabling more expressive and transferable\ngraph token representations.", "AI": {"tldr": "\u56feVQ\u5b58\u5728\u7801\u672c\u574d\u584c\u95ee\u9898\uff0cRGVQ\u6846\u67b6\u901a\u8fc7\u8f6f\u5206\u914d\u548c\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316\u89e3\u51b3\u4e86\u6b64\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u56fe\u8868\u793a\u7684\u5b66\u4e60\u6548\u679c\u3002", "motivation": "\u4e3a\u89e3\u51b3\u56fe\u6570\u636e\u4e0a\u7684\u5411\u91cf\u91cf\u5316\uff08VQ\uff09\u4e2d\u5b58\u5728\u7684\u7801\u672c\u574d\u584c\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u56fe\u6807\u8bb0\u7684\u8868\u8fbe\u80fd\u529b\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "RGVQ\u6846\u67b6\u901a\u8fc7Gumbel-Softmax\u91cd\u53c2\u6570\u5316\u5f15\u5165\u8f6f\u5206\u914d\uff0c\u786e\u4fdd\u6240\u6709\u7801\u5b57\u90fd\u80fd\u63a5\u6536\u5230\u68af\u5ea6\u66f4\u65b0\uff0c\u5e76\u901a\u8fc7\u7ed3\u6784\u611f\u77e5\u5bf9\u6bd4\u6b63\u5219\u5316\u6765\u60e9\u7f5a\u76f8\u4f3c\u8282\u70b9\u5bf9\u4e4b\u95f4\u7684\u6807\u8bb0\u5171\u5206\u914d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cRGVQ\u663e\u8457\u63d0\u9ad8\u4e86\u7801\u672c\u5229\u7528\u7387\uff0c\u5e76\u6301\u7eed\u63d0\u5347\u4e86\u6700\u5148\u8fdb\u7684\u56feVQ\u9aa8\u5e72\u7f51\u7edc\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "conclusion": "RGVQ\u901a\u8fc7\u6574\u5408\u56fe\u62d3\u6251\u548c\u7279\u5f81\u76f8\u4f3c\u6027\u4f5c\u4e3a\u663e\u5f0f\u6b63\u5219\u5316\u4fe1\u53f7\uff0c\u589e\u5f3a\u4e86\u7801\u672c\u5229\u7528\u7387\u5e76\u4fc3\u8fdb\u4e86\u6807\u8bb0\u591a\u6837\u6027\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u6700\u5148\u8fdb\u7684\u56feVQ\u9aa8\u5e72\u7f51\u7edc\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u5177\u8868\u73b0\u529b\u548c\u53ef\u8fc1\u79fb\u6027\u7684\u56fe\u6807\u8bb0\u8868\u793a\u3002"}}
{"id": "2508.06777", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.06777", "abs": "https://arxiv.org/abs/2508.06777", "authors": ["Wenhan Dong", "Zeyu Wan", "Yun-Cheng Yang", "Chao-Hsin Wu", "Yiwen Zhang", "Rui-Tao Wen", "Guangrui Xia"], "title": "Impact of Ge substrate Thicknesses and Epitaxy Growth Conditions on the Optical and Material Properties of Ge- and GaAs-based VCSELs", "comment": null, "summary": "We present a comparative study of the optical and material property\ndependences of VCSELs on Ge or GaAs substrate thicknesses and epitaxy process\nconditions. It was found that adjusting the Ge substrate thickness and\noptimizing the epitaxy process can shift the stopband center and cavity\nresonance wavelength by several nanometers. Ge-based VCSELs exhibit improved\nepitaxial uniformity, smaller deviations from design specifications, reduced\nstoichiometry variations, and strain magnitudes comparable to those of\nGaAs-based counterparts. In the selected 46.92 square micron sample area, no\ndefects were observed in the quantum well (QW) regions of Ge-based VCSELs, and\nthe threading dislocation density (TDD) was measured to be below 2.13e6 per\nsquare cm. These results highlight the potential of Ge substrates as promising\ncandidates for advanced VCSELs.", "AI": {"tldr": "Ge substrates are a promising alternative to GaAs for VCSELs, offering comparable or better performance and properties.", "motivation": "To investigate the potential of Ge substrates for VCSELs by comparing their properties with GaAs substrates.", "method": "Comparative study of optical and material property dependences of VCSELs on Ge or GaAs substrate thicknesses and epitaxy process conditions.", "result": "Adjusting Ge substrate thickness and optimizing epitaxy process can shift stopband center and cavity resonance wavelength. Ge-based VCSELs show improved epitaxial uniformity, smaller deviations from design, reduced stoichiometry variations, and comparable strain to GaAs-based VCSELs. No QW defects and low TDD (<2.13e6/cm^2) were observed in Ge-based VCSELs within a 46.92 um^2 area.", "conclusion": "Ge substrate shows potential for advanced VCSELs due to improved uniformity, reduced variations, and comparable strain to GaAs. Ge-based VCSELs exhibit no defects in QW regions and low TDD."}}
{"id": "2508.07193", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07193", "abs": "https://arxiv.org/abs/2508.07193", "authors": ["Haoyuan Zhang", "Yaqian Gao", "Xinxin Zhang", "Jialin Li", "Runfeng Jin", "Yidong Chen", "Feng Zhang", "Wu Yuan", "Wenpeng Ma", "Shan Liang", "Jian Zhang", "Zhonghua Lu"], "title": "FlashMP: Fast Discrete Transform-Based Solver for Preconditioning Maxwell's Equations on GPUs", "comment": null, "summary": "Efficiently solving large-scale linear systems is a critical challenge in\nelectromagnetic simulations, particularly when using the Crank-Nicolson\nFinite-Difference Time-Domain (CN-FDTD) method. Existing iterative solvers are\ncommonly employed to handle the resulting sparse systems but suffer from slow\nconvergence due to the ill-conditioned nature of the double-curl operator.\nApproximate preconditioners, like Successive Over-Relaxation (SOR) and\nIncomplete LU decomposition (ILU), provide insufficient convergence, while\ndirect solvers are impractical due to excessive memory requirements. To address\nthis, we propose FlashMP, a novel preconditioning system that designs a\nsubdomain exact solver based on discrete transforms. FlashMP provides an\nefficient GPU implementation that achieves multi-GPU scalability through domain\ndecomposition. Evaluations on AMD MI60 GPU clusters (up to 1000 GPUs) show that\nFlashMP reduces iteration counts by up to 16x and achieves speedups of 2.5x to\n4.9x compared to baseline implementations in state-of-the-art libraries such as\nHypre. Weak scalability tests show parallel efficiencies up to 84.1%.", "AI": {"tldr": "FlashMP\u662f\u4e00\u79cd\u65b0\u7684\u9884\u5904\u7406\u7cfb\u7edf\uff0c\u4f7f\u7528\u57fa\u4e8e\u79bb\u6563\u53d8\u6362\u7684\u5b50\u57df\u7cbe\u786e\u6c42\u89e3\u5668\u6765\u52a0\u901f\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u5728GPU\u4e0a\u7684\u6c42\u89e3\u3002", "motivation": "\u89e3\u51b3\u7535\u78c1\u6a21\u62df\u4e2d\uff0c\u7279\u522b\u662f\u4f7f\u7528CN-FDTD\u65b9\u6cd5\u65f6\uff0c\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u6c42\u89e3\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u73b0\u6709\u8fed\u4ee3\u6c42\u89e3\u5668\u7684\u6536\u655b\u901f\u5ea6\u6162\uff0c\u76f4\u63a5\u6c42\u89e3\u5668\u5185\u5b58\u9700\u6c42\u8fc7\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u79bb\u6563\u53d8\u6362\u7684\u5b50\u57df\u7cbe\u786e\u6c42\u89e3\u5668\uff0c\u5e76\u5b9e\u73b0\u4e86\u591aGPU\u53ef\u6269\u5c55\u6027\u3002", "result": "FlashMP\u5c06\u8fed\u4ee3\u6b21\u6570\u51cf\u5c11\u4e86\u9ad8\u8fbe16\u500d\uff0c\u5e76\u4e0eHypre\u7b49\u5e93\u76f8\u6bd4\u5b9e\u73b0\u4e862.5\u500d\u81f34.9\u500d\u7684\u52a0\u901f\uff0c\u57281000\u4e2aGPU\u4e0a\u8868\u73b0\u51fa\u9ad8\u8fbe84.1%\u7684\u5e76\u884c\u6548\u7387\u3002", "conclusion": "FlashMP\u901a\u8fc7\u57fa\u4e8e\u79bb\u6563\u53d8\u6362\u7684\u5b50\u57df\u7cbe\u786e\u6c42\u89e3\u5668\u6709\u6548\u5730\u89e3\u51b3\u4e86\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u95ee\u9898\uff0c\u5e76\u5728GPU\u96c6\u7fa4\u4e0a\u5b9e\u73b0\u4e86\u51fa\u8272\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.06547", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06547", "abs": "https://arxiv.org/abs/2508.06547", "authors": ["Heran Wu", "Zirun Zhou", "Jingfeng Zhang"], "title": "A tutorial note on collecting simulated data for vision-language-action models", "comment": "This is a tutorial note for educational purposes", "summary": "Traditional robotic systems typically decompose intelligence into independent\nmodules for computer vision, natural language processing, and motion control.\nVision-Language-Action (VLA) models fundamentally transform this approach by\nemploying a single neural network that can simultaneously process visual\nobservations, understand human instructions, and directly output robot actions\n-- all within a unified framework. However, these systems are highly dependent\non high-quality training datasets that can capture the complex relationships\nbetween visual observations, language instructions, and robotic actions. This\ntutorial reviews three representative systems: the PyBullet simulation\nframework for flexible customized data generation, the LIBERO benchmark suite\nfor standardized task definition and evaluation, and the RT-X dataset\ncollection for large-scale multi-robot data acquisition. We demonstrated\ndataset generation approaches in PyBullet simulation and customized data\ncollection within LIBERO, and provide an overview of the characteristics and\nroles of the RT-X dataset for large-scale multi-robot data acquisition.", "AI": {"tldr": "VLA\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5904\u7406\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff0c\u4f46\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u672c\u6559\u7a0b\u4ecb\u7ecd\u4e86PyBullet\u3001LIBERO\u548cRT-X\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u6570\u636e\u751f\u6210\u548c\u6536\u96c6\u65b9\u6cd5\u3002", "motivation": "\u4f20\u7edf\u673a\u5668\u4eba\u7cfb\u7edf\u5c06\u667a\u80fd\u5206\u89e3\u4e3a\u72ec\u7acb\u6a21\u5757\uff0c\u800cVLA\u6a21\u578b\u901a\u8fc7\u5355\u4e00\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u7684\u7edf\u4e00\u5904\u7406\u3002", "method": "\u672c\u6559\u7a0b\u56de\u987e\u4e86PyBullet\u6a21\u62df\u6846\u67b6\u3001LIBERO\u57fa\u51c6\u5957\u4ef6\u548cRT-X\u6570\u636e\u96c6\uff0c\u5e76\u6f14\u793a\u4e86\u5728PyBullet\u4e2d\u751f\u6210\u6570\u636e\u96c6\u4ee5\u53ca\u5728LIBERO\u4e2d\u6536\u96c6\u5b9a\u5236\u6570\u636e\u3002", "result": "\u4ecb\u7ecd\u4e86PyBullet\u3001LIBERO\u548cRT-X\u6570\u636e\u96c6\u7684\u7279\u70b9\u548c\u4f5c\u7528\uff0c\u4e3aVLA\u6a21\u578b\u7684\u8bad\u7ec3\u63d0\u4f9b\u4e86\u6570\u636e\u652f\u6301\u3002", "conclusion": "VLA\u6a21\u578b\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u5904\u7406\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff0c\u4f46\u4f9d\u8d56\u9ad8\u8d28\u91cf\u6570\u636e\u96c6\u3002\u672c\u6559\u7a0b\u4ecb\u7ecd\u4e86PyBullet\u3001LIBERO\u548cRT-X\u6570\u636e\u96c6\uff0c\u5e76\u5c55\u793a\u4e86\u6570\u636e\u751f\u6210\u548c\u6536\u96c6\u65b9\u6cd5\u3002"}}
{"id": "2508.06675", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06675", "abs": "https://arxiv.org/abs/2508.06675", "authors": ["Daniel Riedel", "Teodoro Graziosi", "Zhuoxian Wang", "Chawina De-Eknamkul", "Alex Abulnaga", "Jonathan Dietz", "Andrea Mucchietto", "Michael Haas", "Madison Sutula", "Pierre Barral", "Matteo Pompili", "Carsten Robens", "Jeonghoon Ha", "Denis Sukachev", "David Levonian", "Mihir Bhaskar", "Matthew Markham", "Bartholomeus Machielse"], "title": "A scalable photonic quantum interconnect platform", "comment": null, "summary": "Many quantum networking applications require efficient photonic interfaces to\nquantum memories which can be produced at scale and with high yield. Synthetic\ndiamond offers unique potential for the implementation of this technology as it\nhosts color centers which retain coherent optical interfaces and long spin\ncoherence times in nanophotonic structures. Here, we report a technique\nenabling wafer-scale processing of thin-film diamond that combines ion\nimplantation and membrane liftoff, high-quality overgrowth, targeted color\ncenter implantation, and serial, high-throughput thermocompression bonding with\nyields approaching unity. The deterministic deposition of thin diamond\nmembranes onto semiconductor substrates facilitates consistent integration of\nphotonic crystal cavities with silicon-vacancy (SiV) quantum memories. We\ndemonstrate reliable, strong coupling of SiVs to photons with cooperativities\napproaching 100. Furthermore, we show that photonic crystal cavities can be\nreliably fabricated across several membranes bonded to the same handling chip.\nOur platform enables modular fabrication where the photonic layer can be\nintegrated with functionalized substrates featuring electronic control lines\nsuch as coplanar waveguides for microwave delivery. Finally, we implement\npassive optical packaging with sub-decibel insertion loss. Together, these\nadvances pave the way to the scalable assembly of optically addressable quantum\nmemory arrays which are a key building block for modular photonic quantum\ninterconnects.", "AI": {"tldr": "\u4e00\u79cd\u7528\u4e8e\u5927\u89c4\u6a21\u751f\u4ea7\u91cf\u5b50\u5b58\u50a8\u5668\u7684\u65b0\u578b\u91d1\u521a\u77f3\u5904\u7406\u6280\u672f\uff0c\u53ef\u5b9e\u73b0\u9ad8\u4ea7\u7387\u548c\u4e0e\u5149\u5b50\u5668\u4ef6\u7684\u5f3a\u8026\u5408\uff0c\u4e3a\u6784\u5efa\u6a21\u5757\u5316\u91cf\u5b50\u7f51\u7edc\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u8bb8\u591a\u91cf\u5b50\u7f51\u7edc\u5e94\u7528\u9700\u8981\u9ad8\u6548\u7684\u5149\u5b50\u63a5\u53e3\u5230\u91cf\u5b50\u5b58\u50a8\u5668\uff0c\u8fd9\u4e9b\u5b58\u50a8\u5668\u53ef\u4ee5\u89c4\u6a21\u5316\u751f\u4ea7\u5e76\u5177\u6709\u9ad8\u4ea7\u7387\u3002", "method": "\u672c\u7814\u7a76\u62a5\u544a\u4e86\u4e00\u79cd\u5728\u8584\u819c\u91d1\u521a\u77f3\u4e0a\u8fdb\u884c\u6676\u5706\u7ea7\u5904\u7406\u7684\u6280\u672f\uff0c\u8be5\u6280\u672f\u7ed3\u5408\u4e86\u79bb\u5b50\u6ce8\u5165\u548c\u819c\u5265\u79bb\u3001\u9ad8\u8d28\u91cf\u8fc7\u5ea6\u751f\u957f\u3001\u76ee\u6807\u8272\u5fc3\u6ce8\u5165\u4ee5\u53ca\u4e32\u884c\u3001\u9ad8\u901a\u91cf\u70ed\u538b\u952e\u5408\uff0c\u4ea7\u7387\u63a5\u8fd1\u7edf\u4e00\u3002", "result": "\u7814\u7a76\u6f14\u793a\u4e86\u5c06\u7845-\u7a7a\u4f4d\uff08SiV\uff09\u91cf\u5b50\u5b58\u50a8\u5668\u4e0e\u5149\u5b50\u6676\u4f53\u8154\u53ef\u9760\u3001\u5f3a\u8026\u5408\uff0c\u5176\u5408\u4f5c\u5ea6\u63a5\u8fd1100\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8868\u660e\u5149\u5b50\u6676\u4f53\u8154\u53ef\u4ee5\u53ef\u9760\u5730\u5236\u9020\u5728\u7c98\u5408\u5230\u540c\u4e00\u5904\u7406\u82af\u7247\u7684\u51e0\u4e2a\u819c\u4e0a\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u53ef\u6269\u5c55\u7684\u5149\u5b66\u53ef\u5bfb\u5740\u91cf\u5b50\u5b58\u50a8\u5668\u9635\u5217\u7684\u7ec4\u88c5\u94fa\u5e73\u4e86\u9053\u8def\uff0c\u8fd9\u662f\u6a21\u5757\u5316\u5149\u91cf\u5b50\u4e92\u8fde\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2508.07541", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07541", "abs": "https://arxiv.org/abs/2508.07541", "authors": ["Kittiphon Phalakarn", "Athasit Surarerks"], "title": "A Matrix Decomposition Method for Odd-Type Gaussian Normal Basis Multiplication", "comment": null, "summary": "Normal basis is used in many applications because of the efficiency of the\nimplementation. However, most space complexity reduction techniques for binary\nfield multiplier are applicable for only optimal normal basis or Gaussian\nnormal basis of even type. There are 187 binary fields GF(2^k) for k from 2 to\n1,000 that use odd-type Gaussian normal basis. This paper presents a method to\nreduce the space complexity of odd-type Gaussian normal basis multipliers over\nbinary field GF(2^k). The idea is adapted from the matrix decomposition method\nfor optimal normal basis. The result shows that our space complexity reduction\nmethod can reduce the number of XOR gates used in the implementation comparing\nto previous works with a small trade-off in critical path delay.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8e\u5947\u6570\u7c7b\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u4e58\u6cd5\u5668\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u6280\u672f\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6280\u672f\u5c40\u9650\u6027\u3002", "motivation": "\u5927\u591a\u6570\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u6280\u672f\u4ec5\u9002\u7528\u4e8e\u6700\u4f18\u6b63\u89c4\u57fa\u6216\u5076\u6570\u7c7b\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\uff0c\u4f46\u5b58\u5728\u5927\u91cf\u4f7f\u7528\u5947\u6570\u7c7b\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u7684\u4e8c\u5143\u57dfGF(2^k)\u3002", "method": "\u8be5\u65b9\u6cd5\u501f\u9274\u4e86\u6700\u4f18\u6b63\u89c4\u57fa\u7684\u77e9\u9635\u5206\u89e3\u65b9\u6cd5\uff0c\u7528\u4e8e\u964d\u4f4eGF(2^k)\u4e0a\u5947\u6570\u7c7b\u578b\u9ad8\u65af\u6b63\u89c4\u57fa\u4e58\u6cd5\u5668\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u3002", "result": "\u4e0e\u4e4b\u524d\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u53ef\u51cf\u5c11XOR\u95e8\u6570\u91cf\uff0c\u4f46\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u7565\u6709\u589e\u52a0\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7a7a\u95f4\u590d\u6742\u5ea6\u964d\u4f4e\u65b9\u6cd5\u4e0e\u4e4b\u524d\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u53ef\u4ee5\u51cf\u5c11\u5b9e\u73b0\u4e2d\u4f7f\u7528\u7684XOR\u95e8\u6570\u91cf\uff0c\u4f46\u5173\u952e\u8def\u5f84\u5ef6\u8fdf\u7684\u6743\u8861\u5f88\u5c0f\u3002"}}
{"id": "2508.06757", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06757", "abs": "https://arxiv.org/abs/2508.06757", "authors": ["Yash Garg", "Saketh Bachu", "Arindam Dutta", "Rohit Lal", "Sarosij Bose", "Calvin-Khang Ta", "M. Salman Asif", "Amit Roy-Chowdhury"], "title": "VOccl3D: A Video Benchmark Dataset for 3D Human Pose and Shape Estimation under real Occlusions", "comment": null, "summary": "Human pose and shape (HPS) estimation methods have been extensively studied,\nwith many demonstrating high zero-shot performance on in-the-wild images and\nvideos. However, these methods often struggle in challenging scenarios\ninvolving complex human poses or significant occlusions. Although some studies\naddress 3D human pose estimation under occlusion, they typically evaluate\nperformance on datasets that lack realistic or substantial occlusions, e.g.,\nmost existing datasets introduce occlusions with random patches over the human\nor clipart-style overlays, which may not reflect real-world challenges. To\nbridge this gap in realistic occlusion datasets, we introduce a novel benchmark\ndataset, VOccl3D, a Video-based human Occlusion dataset with 3D body pose and\nshape annotations. Inspired by works such as AGORA and BEDLAM, we constructed\nthis dataset using advanced computer graphics rendering techniques,\nincorporating diverse real-world occlusion scenarios, clothing textures, and\nhuman motions. Additionally, we fine-tuned recent HPS methods, CLIFF and\nBEDLAM-CLIFF, on our dataset, demonstrating significant qualitative and\nquantitative improvements across multiple public datasets, as well as on the\ntest split of our dataset, while comparing its performance with other\nstate-of-the-art methods. Furthermore, we leveraged our dataset to enhance\nhuman detection performance under occlusion by fine-tuning an existing object\ndetector, YOLO11, thus leading to a robust end-to-end HPS estimation system\nunder occlusions. Overall, this dataset serves as a valuable resource for\nfuture research aimed at benchmarking methods designed to handle occlusions,\noffering a more realistic alternative to existing occlusion datasets. See the\nProject page for code and dataset:https://yashgarg98.github.io/VOccl3D-dataset/", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86VOccl3D\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6570\u636e\u96c6\u5728\u771f\u5b9e\u906e\u6321\u573a\u666f\u8bc4\u4f30HPS\u65b9\u6cd5\u65f6\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u4e86HPS\u4f30\u8ba1\u548c\u4eba\u4f53\u68c0\u6d4b\u5728\u906e\u6321\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u7aef\u5230\u7aef\u7cfb\u7edf\u3002", "motivation": "\u73b0\u6709\u7684\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\uff08HPS\uff09\u4f30\u8ba1\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4eba\u4f53\u59ff\u6001\u6216\u4e25\u91cd\u906e\u6321\u7684\u6311\u6218\u6027\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u5c3d\u7ba1\u4e00\u4e9b\u7814\u7a76\u5c1d\u8bd5\u89e3\u51b3\u906e\u6321\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u5728\u7f3a\u4e4f\u771f\u5b9e\u6216\u663e\u8457\u906e\u6321\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u8fd9\u4e9b\u6570\u636e\u96c6\u7684\u906e\u6321\u65b9\u5f0f\uff08\u5982\u968f\u673a\u56fe\u5757\u6216\u526a\u8d34\u753b\u8986\u76d6\uff09\u5e76\u4e0d\u53cd\u6620\u73b0\u5b9e\u4e16\u754c\u7684\u6311\u6218\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u906e\u6321\u573a\u666f\u7684\u6570\u636e\u96c6\u6765\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "1. \u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aVOccl3D\u7684\u65b0\u578b\u89c6\u9891\u4eba\u4f53\u906e\u6321\u6570\u636e\u96c6\uff0c\u5305\u542b3D\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u6807\u6ce8\u3002\n2. \u4f7f\u7528\u8ba1\u7b97\u673a\u56fe\u5f62\u6e32\u67d3\u6280\u672f\uff0c\u7ed3\u5408\u4e86\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u906e\u6321\u573a\u666f\u3001\u670d\u88c5\u7eb9\u7406\u548c\u4eba\u4f53\u8fd0\u52a8\u3002\n3. \u5fae\u8c03\u4e86CLIFF\u548cBEDLAM-CLIFF\u7b49\u8fd1\u671f\u7684\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\uff08HPS\uff09\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548cVOccl3D\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6027\u80fd\u3002\n4. \u5229\u7528VOccl3D\u6570\u636e\u96c6\u5fae\u8c03\u4e86YOLOv11\u76ee\u6807\u68c0\u6d4b\u5668\uff0c\u4ee5\u589e\u5f3a\u5728\u906e\u6321\u60c5\u51b5\u4e0b\u7684\u68c0\u6d4b\u80fd\u529b\u3002\n5. \u63d0\u51fa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u7aef\u5230\u7aefHPS\u4f30\u8ba1\u7cfb\u7edf\uff0c\u80fd\u591f\u5904\u7406\u906e\u6321\u573a\u666f\u3002", "result": "1. \u5728VOccl3D\u6570\u636e\u96c6\u4e0a\u5fae\u8c03\u7684CLIFF\u548cBEDLAM-CLIFF\u65b9\u6cd5\uff0c\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548cVOccl3D\u6d4b\u8bd5\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u663e\u8457\u7684\u5b9a\u6027\u548c\u5b9a\u91cf\u6027\u80fd\u63d0\u5347\u3002\n2. \u5229\u7528VOccl3D\u6570\u636e\u96c6\u5fae\u8c03\u7684YOLOv11\u6a21\u578b\uff0c\u5728\u906e\u6321\u573a\u666f\u4e0b\u7684\u4eba\u4f53\u68c0\u6d4b\u6027\u80fd\u5f97\u5230\u589e\u5f3a\u3002\n3. \u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u80fd\u591f\u5904\u7406\u906e\u6321\u7684\u9c81\u68d2\u7aef\u5230\u7aefHPS\u4f30\u8ba1\u7cfb\u7edf\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86VOccl3D\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5305\u542b3D\u4eba\u4f53\u59ff\u6001\u548c\u5f62\u72b6\u6807\u6ce8\u7684\u89c6\u9891\u906e\u6321\u6570\u636e\u96c6\uff0c\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u5728\u771f\u5b9e\u906e\u6321\u573a\u666f\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u901a\u8fc7\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u5fae\u8c03CLIFF\u548cBEDLAM-CLIFF\u6a21\u578b\uff0c\u5e76\u5728\u591a\u4e2a\u516c\u5f00\u6570\u636e\u96c6\u548cVOccl3D\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u8bc1\u660e\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002\u6b64\u5916\uff0c\u5229\u7528\u8be5\u6570\u636e\u96c6\u5fae\u8c03YOLOv11\u4e5f\u63d0\u5347\u4e86\u906e\u6321\u573a\u666f\u4e0b\u7684\u4eba\u4f53\u68c0\u6d4b\u6027\u80fd\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u9c81\u68d2\u7684\u7aef\u5230\u7aefHPS\u4f30\u8ba1\u7cfb\u7edf\u3002VOccl3D\u6570\u636e\u96c6\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\uff0c\u80fd\u591f\u66f4\u771f\u5b9e\u5730\u8bc4\u4f30\u548c\u6539\u8fdb\u5904\u7406\u906e\u6321\u95ee\u9898\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06583", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06583", "abs": "https://arxiv.org/abs/2508.06583", "authors": ["Ying Liu", "Can Li", "Ting Zhang", "Mei Wang", "Qiannan Zhu", "Jian Li", "Hua Huang"], "title": "Discerning minds or generic tutors? Evaluating instructional guidance capabilities in Socratic LLMs", "comment": null, "summary": "The conversational capabilities of large language models hold significant\npromise for enabling scalable and interactive tutoring. While prior research\nhas primarily examined their capacity for Socratic questioning, it often\noverlooks a critical dimension: adaptively guiding learners based on their\ncognitive states. This study shifts focus from mere question generation to the\nbroader instructional guidance capability. We ask: Can LLMs emulate expert\ntutors who dynamically adjust strategies in response to learners'\nunderstanding? To investigate this, we propose GuideEval, a benchmark grounded\nin authentic educational dialogues that evaluates pedagogical guidance through\na three-phase behavioral framework: (1) Perception, inferring learner states;\n(2) Orchestration, adapting instructional strategies; and (3) Elicitation,\nstimulating proper reflections. Empirical findings reveal that existing LLMs\nfrequently fail to provide effective adaptive scaffolding when learners exhibit\nconfusion or require redirection. Furthermore, we introduce a behavior-guided\nfinetuning strategy that leverages behavior-prompted instructional dialogues,\nsignificantly enhancing guidance performance. By shifting the focus from\nisolated content evaluation to learner-centered interaction, our work advocates\na more dialogic paradigm for evaluating Socratic LLMs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u8f85\u5bfc\u4e2d\u7684\u9002\u5e94\u6027\u6307\u5bfc\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u8868\u73b0\u4e0d\u4f73\uff0c\u4f46\u901a\u8fc7\u65b0\u7684\u5fae\u8c03\u65b9\u6cd5\u53ef\u5927\u5e45\u6539\u8fdb\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4f5c\u4e3a\u6559\u80b2\u8f85\u5bfc\u5de5\u5177\u65f6\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5b83\u4eec\u6839\u636e\u5b66\u4e60\u8005\u8ba4\u77e5\u72b6\u6001\u81ea\u9002\u5e94\u8c03\u6574\u6559\u5b66\u7b56\u7565\u7684\u80fd\u529b\uff0c\u5f25\u8865\u4e86\u4ee5\u5f80\u7814\u7a76\u4e3b\u8981\u5173\u6ce8\u82cf\u683c\u62c9\u5e95\u5f0f\u63d0\u95ee\u800c\u5ffd\u7565\u9002\u5e94\u6027\u6307\u5bfc\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuideEval\u7684\u57fa\u51c6\uff0c\u5305\u542b\u611f\u77e5\u3001\u7ec4\u7ec7\u548c\u542f\u53d1\u4e09\u4e2a\u9636\u6bb5\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u8f85\u5bfc\u4e2d\u7684\u9002\u5e94\u6027\u6307\u5bfc\u80fd\u529b\u3002\u540c\u65f6\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u884c\u4e3a\u5f15\u5bfc\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u5229\u7528\u884c\u4e3a\u63d0\u793a\u7684\u6559\u5b66\u5bf9\u8bdd\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u5b66\u4e60\u8005\u8868\u73b0\u51fa\u56f0\u60d1\u6216\u9700\u8981\u91cd\u5b9a\u5411\u65f6\uff0c\u5f80\u5f80\u65e0\u6cd5\u63d0\u4f9b\u6709\u6548\u7684\u9002\u5e94\u6027\u652f\u67b6\u3002\u901a\u8fc7\u884c\u4e3a\u5f15\u5bfc\u7684\u5fae\u8c03\u7b56\u7565\uff0c\u6a21\u578b\u7684\u6307\u5bfc\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGuideEval\u7684\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6559\u80b2\u8f85\u5bfc\u4e2d\u7684\u9002\u5e94\u6027\u6307\u5bfc\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u884c\u4e3a\u5f15\u5bfc\u7684\u5fae\u8c03\u7b56\u7565\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u6839\u636e\u5b66\u4e60\u8005\u72b6\u6001\u8c03\u6574\u6559\u5b66\u7b56\u7565\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u4f46\u901a\u8fc7\u6240\u63d0\u51fa\u7684\u5fae\u8c03\u65b9\u6cd5\u53ef\u4ee5\u663e\u8457\u6539\u5584\u5176\u8868\u73b0\u3002"}}
{"id": "2508.07145", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07145", "abs": "https://arxiv.org/abs/2508.07145", "authors": ["Ivan Geffner", "Erez Karpas", "Moshe Tennenholtz"], "title": "When Competition Helps: Achieving Optimal Traffic Flow with Multiple Autonomous Planners", "comment": null, "summary": "The inefficiency of selfish routing in congested networks is a classical\nproblem in algorithmic game theory, often captured by the Price of Anarchy\n(i.e., the ratio between the social cost of decentralized decisions and that of\na centrally optimized solution.) With the advent of autonomous vehicles,\ncapable of receiving and executing centrally assigned routes, it is natural to\nask whether their deployment can eliminate this inefficiency. At first glance,\na central authority could simply compute an optimal traffic assignment and\ninstruct each vehicle to follow its assigned path. However, this vision\noverlooks critical challenges: routes must be individually rational (no vehicle\nhas an incentive to deviate), and in practice, multiple planning agents (e.g.,\ndifferent companies) may coexist and compete. Surprisingly, we show that such\ncompetition is not merely an obstacle but a necessary ingredient for achieving\noptimal outcomes. In this work, we design a routing mechanism that embraces\ncompetition and converges to an optimal assignment, starting from the classical\nPigou network as a foundational case.", "AI": {"tldr": "\u5728\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7f51\u7edc\u4e2d\uff0c\u5f15\u5165\u7ade\u4e89\u800c\u975e\u5b8c\u5168\u4e2d\u5fc3\u5316\u63a7\u5236\uff0c\u662f\u5b9e\u73b0\u6700\u4f18\u4ea4\u901a\u6d41\u91cf\u5206\u914d\u7684\u5173\u952e\u3002", "motivation": "\u63a2\u7a76\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u90e8\u7f72\u662f\u5426\u80fd\u6d88\u9664\u62e5\u585e\u7f51\u7edc\u4e2d\u81ea\u6211\u8def\u7531\u7684\u4f4e\u6548\u7387\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u5b58\u5728\u591a\u4e2a\u7ade\u4e89\u6027\u89c4\u5212\u4e3b\u4f53\u65f6\u3002", "method": "\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5305\u542b\u7ade\u4e89\u673a\u5236\u7684\u8def\u7531\u673a\u5236\uff0c\u5e76\u4ee5\u7ecf\u5178\u7684Pigou\u7f51\u7edc\u4f5c\u4e3a\u57fa\u7840\u6848\u4f8b\u3002", "result": "\u8bbe\u8ba1\u51fa\u7684\u8def\u7531\u673a\u5236\u80fd\u591f\u6536\u655b\u5230\u6700\u4f18\u5206\u914d\uff0c\u8bc1\u660e\u4e86\u7ade\u4e89\u662f\u5b9e\u73b0\u6700\u4f18\u7ed3\u679c\u7684\u5fc5\u8981\u56e0\u7d20\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u7ade\u4e89\u673a\u5236\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6700\u4f18\u7684\u4ea4\u901a\u6d41\u91cf\u5206\u914d\uff0c\u8fd9\u4e0e\u76f4\u89c9\u76f8\u53cd\u3002"}}
{"id": "2508.07691", "categories": ["cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07691", "abs": "https://arxiv.org/abs/2508.07691", "authors": ["Tomohiro Harada", "Enrique Alba", "Gabriel Luque"], "title": "Energy and Quality of Surrogate-Assisted Search Algorithms: a First Analysis", "comment": "8 pages, 8 figures, 2024 IEEE Congress on Evolutionary Computation\n  (CEC)", "summary": "Solving complex real problems often demands advanced algorithms, and then\ncontinuous improvements in the internal operations of a search technique are\nneeded. Hybrid algorithms, parallel techniques, theoretical advances, and much\nmore are needed to transform a general search algorithm into an efficient,\nuseful one in practice. In this paper, we study how surrogates are helping\nmetaheuristics from an important and understudied point of view: their energy\nprofile. Even if surrogates are a great idea for substituting a time-demanding\ncomplex fitness function, the energy profile, general efficiency, and accuracy\nof the resulting surrogate-assisted metaheuristic still need considerable\nresearch. In this work, we make a first step in analyzing particle swarm\noptimization in different versions (including pre-trained and retrained neural\nnetworks as surrogates) for its energy profile (for both processor and memory),\nplus a further study on the surrogate accuracy to properly drive the search\ntowards an acceptable solution. Our conclusions shed new light on this topic\nand could be understood as the first step towards a methodology for assessing\nsurrogate-assisted algorithms not only accounting for time or numerical\nefficiency but also for energy and surrogate accuracy for a better, more\nholistic characterization of optimization and learning techniques.", "AI": {"tldr": "\u672c\u6587\u5206\u6790\u4e86\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u7684\u80fd\u91cf\u6d88\u8017\u548c\u4ee3\u7406\u51c6\u786e\u6027\uff0c\u5f3a\u8c03\u4e86\u5728\u8bc4\u4f30\u7b97\u6cd5\u65f6\u8003\u8651\u8fd9\u4e9b\u56e0\u7d20\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u672c\u6587\u7814\u7a76\u4e86\u4ee3\u7406\u5982\u4f55\u4ece\u4e00\u4e2a\u91cd\u8981\u4f46\u88ab\u5ffd\u89c6\u7684\u89d2\u5ea6\u2014\u2014\u80fd\u91cf\u6d88\u8017\u2014\u2014\u6765\u5e2e\u52a9\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\uff0c\u5c3d\u7ba1\u4ee3\u7406\u53ef\u4ee5\u66ff\u4ee3\u8017\u65f6\u7684\u590d\u6742\u9002\u5e94\u5ea6\u51fd\u6570\uff0c\u4f46\u7531\u6b64\u4ea7\u751f\u7684\u4ee3\u7406\u8f85\u52a9\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u80fd\u91cf\u6d88\u8017\u3001\u6574\u4f53\u6548\u7387\u548c\u51c6\u786e\u6027\u4ecd\u9700\u6df1\u5165\u7814\u7a76\u3002", "method": "\u672c\u6587\u5206\u6790\u4e86\u7c92\u5b50\u7fa4\u4f18\u5316\uff08PSO\uff09\u4e0d\u540c\u7248\u672c\uff08\u5305\u62ec\u9884\u8bad\u7ec3\u548c\u91cd\u65b0\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u4f5c\u4e3a\u4ee3\u7406\uff09\u7684\u80fd\u91cf\u6d88\u8017\uff08\u5904\u7406\u5668\u548c\u5185\u5b58\uff09\u53ca\u5176\u4ee3\u7406\u51c6\u786e\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e3a\u8bc4\u4f30\u4ee3\u7406\u8f85\u52a9\u7b97\u6cd5\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\uff0c\u5c55\u793a\u4e86\u5728\u4f18\u5316\u548c\u5b66\u4e60\u6280\u672f\u4e2d\u8003\u8651\u80fd\u6e90\u548c\u4ee3\u7406\u51c6\u786e\u6027\u7684\u91cd\u8981\u6027\u3002", "conclusion": "\u672c\u7ed3\u8bba\u5f3a\u8c03\u4e86\u5bf9\u4ee3\u7406\u8f85\u52a9\u7b97\u6cd5\u8fdb\u884c\u5168\u9762\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\uff0c\u4e0d\u4ec5\u8981\u8003\u8651\u65f6\u95f4\u6216\u6570\u503c\u6548\u7387\uff0c\u8fd8\u8981\u8003\u8651\u80fd\u6e90\u6d88\u8017\u548c\u4ee3\u7406\u51c6\u786e\u6027\uff0c\u4e3a\u4f18\u5316\u548c\u5b66\u4e60\u6280\u672f\u63d0\u4f9b\u66f4\u5168\u9762\u3001\u66f4\u6df1\u5165\u7684\u7406\u89e3\u3002"}}
{"id": "2508.07489", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2508.07489", "abs": "https://arxiv.org/abs/2508.07489", "authors": ["Adilson Vital Jr.", "Filipi N. Silva", "Diego R. Amancio"], "title": "Recovering link-weight structure in complex networks with weight-aware random walks", "comment": null, "summary": "Using edge weights is essential for modeling real-world systems where links\npossess relevant information, and preserving this information in\nlow-dimensional representations is relevant for classification and prediction\ntasks. This paper systematically investigates how different random walk\nstrategies - traditional unweighted, strength-based, and fully weight-aware -\nkeeps edge weight information when generating node embeddings. Using network\nmodels, real-world graphs, and networks subjected to low-weight edge removal,\nwe measured the correlation between original edge weights and the similarity of\nnode pairs in the embedding space generated by random walk strategies. Our\nresults consistently showed that weight-aware random walks significantly\noutperform other strategies, achieving correlations above 0.90 in network\nmodels. However, performance in real-world networks was more heterogeneous,\ninfluenced by factors like topology and weight distribution. Our analysis also\nrevealed that removing weak edges via thresholding can initially improve\ncorrelation by reducing noise, but excessive pruning degrades representation\nquality. Our findings suggest that simply using a weight-aware random walk is\ngenerally the best approach for preserving node weight information in\nembeddings, but it is not a universal solution.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e0d\u540c\u7684\u968f\u673a\u6e38\u8d70\u7b56\u7565\u5728\u8282\u70b9\u5d4c\u5165\u4e2d\u4fdd\u7559\u8fb9\u6743\u91cd\u4fe1\u606f\u7684\u6548\u679c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6743\u91cd\u611f\u77e5\u7b56\u7565\u901a\u5e38\u6548\u679c\u6700\u597d\uff0c\u4f46\u5728\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u4e2d\u7684\u8868\u73b0\u5404\u5f02\u3002\u79fb\u9664\u5f31\u8fde\u63a5\u53ef\u4ee5\u6539\u5584\u7ed3\u679c\uff0c\u4f46\u9700\u6ce8\u610f\u9002\u5ea6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7cfb\u7edf\u4e2d\uff0c\u8fb9\u6743\u91cd\u81f3\u5173\u91cd\u8981\uff0c\u56e0\u4e3a\u94fe\u63a5\u643a\u5e26\u4e86\u76f8\u5173\u4fe1\u606f\u3002\u5728\u4f4e\u7ef4\u8868\u793a\u4e2d\u4fdd\u7559\u8fd9\u4e9b\u4fe1\u606f\u5bf9\u4e8e\u5206\u7c7b\u548c\u9884\u6d4b\u4efb\u52a1\u975e\u5e38\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u4f7f\u7528\u7f51\u7edc\u6a21\u578b\u3001\u771f\u5b9e\u4e16\u754c\u56fe\u548c\u8fdb\u884c\u4f4e\u6743\u91cd\u8fb9\u79fb\u9664\u7684\u7f51\u7edc\uff0c\u6765\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u4e09\u79cd\u4e0d\u540c\u7684\u968f\u673a\u6e38\u8d70\u7b56\u7565\uff08\u4f20\u7edf\u7684\u975e\u52a0\u6743\u3001\u57fa\u4e8e\u5f3a\u5ea6\u548c\u5b8c\u5168\u6743\u91cd\u611f\u77e5\uff09\u5728\u751f\u6210\u8282\u70b9\u5d4c\u5165\u65f6\u4fdd\u7559\u8fb9\u6743\u91cd\u4fe1\u606f\u7684\u65b9\u5f0f\u3002\u901a\u8fc7\u8861\u91cf\u539f\u59cb\u8fb9\u6743\u91cd\u4e0e\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u7b56\u7565\u751f\u6210\u7684\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u8282\u70b9\u5bf9\u76f8\u4f3c\u6027\u4e4b\u95f4\u7684\u76f8\u5173\u6027\uff0c\u6765\u8bc4\u4f30\u4e0d\u540c\u7b56\u7565\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u4e00\u81f4\u8868\u660e\uff0c\u6743\u91cd\u611f\u77e5\u968f\u673a\u6e38\u8d70\u7b56\u7565\u663e\u8457\u4f18\u4e8e\u5176\u4ed6\u7b56\u7565\uff0c\u5728\u7f51\u7edc\u6a21\u578b\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u4e8e0.90\u7684\u76f8\u5173\u6027\u3002\u7136\u800c\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7f51\u7edc\u4e2d\u7684\u8868\u73b0\u66f4\u4e3a\u591a\u6837\u5316\uff0c\u53d7\u62d3\u6251\u548c\u6743\u91cd\u5206\u5e03\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u9608\u503c\u5316\u79fb\u9664\u5f31\u8fb9\u53ef\u4ee5\u6682\u65f6\u63d0\u9ad8\u76f8\u5173\u6027\uff0c\u4f46\u8fc7\u5ea6\u4fee\u526a\u4f1a\u635f\u5bb3\u8868\u793a\u8d28\u91cf\u3002", "conclusion": "\u201c\u4fdd\u7559\u8282\u70b9\u5d4c\u5165\u4e2d\u7684\u8fb9\u6743\u91cd\u4fe1\u606f\u201d\u7684\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u6743\u91cd\u611f\u77e5\u968f\u673a\u6e38\u8d70\u901a\u5e38\u662f\u6700\u4f73\u65b9\u6cd5\uff0c\u4f46\u5b83\u5e76\u975e\u4e07\u80fd\u89e3\u51b3\u65b9\u6848\uff0c\u5b9e\u9645\u6548\u679c\u4f1a\u53d7\u5230\u7f51\u7edc\u62d3\u6251\u548c\u6743\u91cd\u5206\u5e03\u7b49\u56e0\u7d20\u7684\u5f71\u54cd\u3002\u9608\u503c\u5316\u79fb\u9664\u5f31\u8fde\u63a5\u53ef\u4ee5\u63d0\u9ad8\u76f8\u5173\u6027\uff0c\u4f46\u8fc7\u5ea6\u4fee\u526a\u4f1a\u964d\u4f4e\u8868\u793a\u8d28\u91cf\u3002"}}
{"id": "2508.06528", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06528", "abs": "https://arxiv.org/abs/2508.06528", "authors": ["Xiuliang Zhang", "Tadiwa Elisha Nyamasvisva", "Chuntao Liu"], "title": "A Framework Combining 3D CNN and Transformer for Video-Based Behavior Recognition", "comment": "9 pages,6 figures", "summary": "Video-based behavior recognition is essential in fields such as public\nsafety, intelligent surveillance, and human-computer interaction. Traditional\n3D Convolutional Neural Network (3D CNN) effectively capture local\nspatiotemporal features but struggle with modeling long-range dependencies.\nConversely, Transformers excel at learning global contextual information but\nface challenges with high computational costs. To address these limitations, we\npropose a hybrid framework combining 3D CNN and Transformer architectures. The\n3D CNN module extracts low-level spatiotemporal features, while the Transformer\nmodule captures long-range temporal dependencies, with a fusion mechanism\nintegrating both representations. Evaluated on benchmark datasets, the proposed\nmodel outperforms traditional 3D CNN and standalone Transformers, achieving\nhigher recognition accuracy with manageable complexity. Ablation studies\nfurther validate the complementary strengths of the two modules. This hybrid\nframework offers an effective and scalable solution for video-based behavior\nrecognition.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D CNN\u548cTransformer\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u884c\u4e3a\u8bc6\u522b\uff0c\u5728\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u7ba1\u7406\u590d\u6742\u6027\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf3D CNN\u96be\u4ee5\u5efa\u6a21\u957f\u671f\u4f9d\u8d56\u6027\u548cTransformer\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D CNN\u548cTransformer\u7684\u6df7\u5408\u6846\u67b6\u30023D CNN\u6a21\u5757\u63d0\u53d6\u4f4e\u7ea7\u65f6\u7a7a\u7279\u5f81\uff0cTransformer\u6a21\u5757\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u901a\u8fc7\u878d\u5408\u673a\u5236\u6574\u5408\u8fd9\u4e24\u79cd\u8868\u793a\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u4f18\u4e8e\u4f20\u7edf\u76843D CNN\u548c\u72ec\u7acb\u7684Transformer\uff0c\u5728\u53ef\u7ba1\u7406\u7684\u590d\u6742\u6027\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u8bc6\u522b\u7cbe\u5ea6\u3002\u6d88\u878d\u7814\u7a76\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u4e24\u4e2a\u6a21\u5757\u7684\u4e92\u8865\u4f18\u52bf\u3002", "conclusion": "\u8be5\u6df7\u5408\u6846\u67b6\u4e3a\u57fa\u4e8e\u89c6\u9891\u7684\u884c\u4e3a\u8bc6\u522b\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08073", "categories": ["cs.LG", "cs.AR", "cs.ET"], "pdf": "https://arxiv.org/pdf/2508.08073", "abs": "https://arxiv.org/abs/2508.08073", "authors": ["Dimitris Tsaras", "Xing Li", "Lei Chen", "Zhiyao Xie", "Mingxuan Yuan"], "title": "ELF: Efficient Logic Synthesis by Pruning Redundancy in Refactoring", "comment": "Accepted to DAC 2025", "summary": "In electronic design automation, logic optimization operators play a crucial\nrole in minimizing the gate count of logic circuits. However, their computation\ndemands are high. Operators such as refactor conventionally form iterative cuts\nfor each node, striving for a more compact representation - a task which often\nfails 98% on average. Prior research has sought to mitigate computational cost\nthrough parallelization. In contrast, our approach leverages a classifier to\nprune unsuccessful cuts preemptively, thus eliminating unnecessary resynthesis\noperations. Experiments on the refactor operator using the EPFL benchmark suite\nand 10 large industrial designs demonstrate that this technique can speedup\nlogic optimization by 3.9x on average compared with the state-of-the-art ABC\nimplementation.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u5206\u7c7b\u5668\u6765\u4fee\u526a\u4e0d\u6210\u529f\u7684\u5207\u5272\uff0c\u53ef\u4ee5\u63d0\u9ad8\u903b\u8f91\u4f18\u5316\u7684\u6548\u7387\u548c\u901f\u5ea6\u3002", "motivation": "\u903b\u8f91\u4f18\u5316\u7b97\u5b50\u5728\u6700\u5c0f\u5316\u903b\u8f91\u7535\u8def\u7684\u95e8\u6570\u65b9\u9762\u8d77\u7740\u81f3\u5173\u91cd\u8981\u7684\u4f5c\u7528\uff0c\u4f46\u5176\u8ba1\u7b97\u6210\u672c\u5f88\u9ad8\u3002\u8fc7\u53bb\u7684\u4f18\u5316\u65b9\u6cd5\u5f80\u5f80\u6548\u7387\u4f4e\u4e0b\uff0c\u5e73\u5747\u6709 98% \u7684\u60c5\u51b5\u4f1a\u5931\u8d25\u3002", "method": "\u5229\u7528\u5206\u7c7b\u5668\u6765\u9884\u5148\u4fee\u526a\u4e0d\u6210\u529f\u7684\u5207\u5272\uff0c\u4ee5\u6d88\u9664\u4e0d\u5fc5\u8981\u7684\u518d\u5408\u6210\u64cd\u4f5c\u3002", "result": "\u5728 EPFL \u57fa\u51c6\u5957\u4ef6\u548c 10 \u4e2a\u5927\u578b\u5de5\u4e1a\u8bbe\u8ba1\u4e0a\u4f7f\u7528\u91cd\u6784\u7b97\u5b50\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6700\u5148\u8fdb\u7684 ABC \u5b9e\u73b0\u76f8\u6bd4\uff0c\u8be5\u6280\u672f\u5e73\u5747\u53ef\u5c06\u903b\u8f91\u4f18\u5316\u901f\u5ea6\u63d0\u9ad8 3.9 \u500d\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u5206\u7c7b\u5668\u9884\u5148\u4fee\u526a\u4e0d\u6210\u529f\u7684\u5207\u5272\uff0c\u53ef\u4ee5\u6d88\u9664\u4e0d\u5fc5\u8981\u7684\u518d\u5408\u6210\u64cd\u4f5c\uff0c\u4ece\u800c\u52a0\u901f\u903b\u8f91\u4f18\u5316\u3002"}}
{"id": "2508.06898", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06898", "abs": "https://arxiv.org/abs/2508.06898", "authors": ["Zhiyuan Ren", "Zhiliang Shuai", "Wenchi Cheng", "Kun Yang"], "title": "Decoupling Structural Heterogeneity from Functional Fairness in Complex Networks: A Theoretical Framework based on the Imbalance Metric", "comment": null, "summary": "Performance evaluation of complex networks has traditionally focused on\nstructural integrity or average transmission efficiency, perspectives that\noften overlook the dimension of functional fairness. This raises a central\nquestion: Under certain conditions, structurally heterogeneous networks can\nexhibit high functional fairness. To systematically address this issue, we\nintroduce a new metric, Network Imbalance (I), designed to quantitatively\nassess end-to-end accessibility fairness from a perceived QoS perspective. By\ncombining a tunable sigmoid function with a global Shannon entropy framework,\nthe I metric quantifies the uniformity of connection experiences between all\nnode pairs. We analyze the mathematical properties of this metric and validate\nits explanatory power on various classical network models. Our findings reveal\nthat low imbalance (i.e., high functional fairness) can be achieved through two\ndistinct mechanisms: one via topological symmetry (e.g., in a complete graph)\nand the other via extreme connection efficiency driven by structural inequality\n(e.g., in a scale-free network). This decoupling of structure and function\nprovides a new theoretical perspective for network performance evaluation and\noffers an effective quantitative tool for balancing efficiency and fairness in\nnetwork design.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7f51\u7edc\u5931\u8861\u5ea6\uff08I\uff09\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u7f51\u7edc\u516c\u5e73\u6027\uff0c\u53d1\u73b0\u7ed3\u6784\u4e0d\u5bf9\u79f0\u7684\u7f51\u7edc\u4e5f\u80fd\u5b9e\u73b0\u9ad8\u516c\u5e73\u6027\uff0c\u8fd9\u4e3a\u7f51\u7edc\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "\u4f20\u7edf\u7f51\u7edc\u6027\u80fd\u8bc4\u4f30\u4e3b\u8981\u5173\u6ce8\u7ed3\u6784\u5b8c\u6574\u6027\u6216\u5e73\u5747\u4f20\u8f93\u6548\u7387\uff0c\u5ffd\u7565\u4e86\u529f\u80fd\u516c\u5e73\u6027\u7ef4\u5ea6\u3002\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4e00\u4e2a\u6838\u5fc3\u95ee\u9898\uff1a\u5728\u4f55\u79cd\u6761\u4ef6\u4e0b\uff0c\u7ed3\u6784\u5f02\u8d28\u6027\u7f51\u7edc\u53ef\u4ee5\u5c55\u73b0\u51fa\u9ad8\u7684\u529f\u80fd\u516c\u5e73\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u7f51\u7edc\u5931\u8861\u5ea6\u201d\uff08I\uff09\u7684\u65b0\u6307\u6807\uff0c\u8be5\u6307\u6807\u7ed3\u5408\u4e86\u53ef\u8c03\u7684sigmoid\u51fd\u6570\u548c\u5168\u5c40Shannon\u71b5\u6846\u67b6\uff0c\u4ee5\u91cf\u5316\u611f\u77e5\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u4e0b\u7aef\u5230\u7aef\u53ef\u8fbe\u6027\u516c\u5e73\u6027\u3002\u901a\u8fc7\u5206\u6790\u8be5\u6307\u6807\u7684\u6570\u5b66\u7279\u6027\uff0c\u5e76\u5728\u5404\u79cd\u7ecf\u5178\u7f51\u7edc\u6a21\u578b\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u4ee5\u8bc4\u4f30\u5176\u89e3\u91ca\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f4e\u7f51\u7edc\u5931\u8861\u5ea6\uff08\u5373\u9ad8\u529f\u80fd\u516c\u5e73\u6027\uff09\u53ef\u4ee5\u901a\u8fc7\u4e24\u79cd\u4e0d\u540c\u7684\u673a\u5236\u5b9e\u73b0\uff1a\u4e00\u79cd\u662f\u57fa\u4e8e\u62d3\u6251\u5bf9\u79f0\u6027\uff08\u5982\u5b8c\u5168\u56fe\uff09\uff0c\u53e6\u4e00\u79cd\u662f\u57fa\u4e8e\u7531\u7ed3\u6784\u4e0d\u5e73\u7b49\u9a71\u52a8\u7684\u6781\u7aef\u8fde\u63a5\u6548\u7387\uff08\u5982\u65e0\u6807\u5ea6\u7f51\u7edc\uff09\u3002\u8fd9\u63ed\u793a\u4e86\u7ed3\u6784\u4e0e\u529f\u80fd\u4e4b\u95f4\u7684\u89e3\u8026\u5173\u7cfb\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u7f51\u7edc\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u201c\u7f51\u7edc\u5931\u8861\u5ea6\u201d\uff08I\uff09\uff0c\u7528\u4e8e\u91cf\u5316\u4ece\u611f\u77e5\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u89d2\u5ea6\u6765\u770b\u7684\u7aef\u5230\u7aef\u53ef\u8fbe\u6027\u516c\u5e73\u6027\u3002\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u6784\u4e0d\u5bf9\u79f0\u7684\u7f51\u7edc\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4e5f\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u5ea6\u7684\u529f\u80fd\u516c\u5e73\u6027\uff0c\u5e76\u901a\u8fc7\u62d3\u6251\u5bf9\u79f0\u6027\u6216\u6781\u7aef\u7684\u8fde\u63a5\u6548\u7387\u5b9e\u73b0\u4f4e\u5931\u8861\u5ea6\u3002\u8fd9\u4e00\u53d1\u73b0\u4e3a\u7f51\u7edc\u6027\u80fd\u8bc4\u4f30\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u89c6\u89d2\uff0c\u5e76\u4e3a\u7f51\u7edc\u8bbe\u8ba1\u4e2d\u5e73\u8861\u6548\u7387\u548c\u516c\u5e73\u6027\u63d0\u4f9b\u4e86\u91cf\u5316\u5de5\u5177\u3002"}}
{"id": "2508.07164", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.07164", "abs": "https://arxiv.org/abs/2508.07164", "authors": ["Toshihito Osada"], "title": "Wannier Center Analysis on Possible Three-Dimensional Topological Phases in \u03b1-Type Layered Organic Conductors", "comment": "15 pages, 4 figures", "summary": "Topological features of possible three-dimensional (3D) states in \\alpha-type\nlayered organic conductors are investigated within a unified framework based on\nWannier charge centers (WCCs), aiming to identify their actual topological\nstates. Among the 3D Dirac/Weyl semimetal states of multilayered\n\\alpha-(ET)2I3, the type-I Dirac semimetal state, induced by interlayer\nspin-orbit coupling (SOC), most effectively explains the observed chiral\ntransport phenomena attributed to the chiral magnetic effect, which originates\nfrom the spiral structures of the WCC sheets. In multilayered \\alpha-(BETS)2I3,\na 3D weak topological insulator (TI) state consistently emerges, irrespective\nof the presence of interlayer SOC and/or inversion symmetry breaking. The\nstrong TI state suggested by experimental observations appears unlikely to be\nrealized.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5229\u7528WCC\u6846\u67b6\u5206\u6790\u4e86\u03b1-\u7c7b\u578b\u6709\u673a\u5bfc\u4f53\u4e2d\u7684\u62d3\u6251\u72b6\u6001\u3002\u53d1\u73b0I\u578b\u72c4\u62c9\u514b\u534a\u91d1\u5c5e\u72b6\u6001\uff08\u7531\u5c42\u95f4SOC\u5f15\u8d77\uff09\u80fd\u89e3\u91ca\u03b1-(ET)2I3\u4e2d\u7684\u624b\u5f81\u8f93\u8fd0\u73b0\u8c61\uff0c\u800c\u03b1-(BETS)2I3\u4e2d\u51fa\u73b0\u7684\u662f\u4e09\u7ef4\u5f31\u62d3\u6251\u7edd\u7f18\u4f53\u72b6\u6001\uff0c\u800c\u975e\u5b9e\u9a8c\u63a8\u6d4b\u7684\u5f3a\u62d3\u6251\u7edd\u7f18\u4f53\u72b6\u6001\u3002", "motivation": "\u65e8\u5728\u8bc6\u522b\u5b9e\u9645\u7684\u62d3\u6251\u72b6\u6001\u3002", "method": "\u57fa\u4e8eWannier\u7535\u8377\u4e2d\u5fc3(WCC)\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u7814\u7a76\u4e86\u03b1-\u7c7b\u578b\u5c42\u72b6\u6709\u673a\u5bfc\u4f53\u4e2d\u53ef\u80fd\u7684\u4e09\u7ef4(3D)\u72b6\u6001\u7684\u62d3\u6251\u7279\u5f81\u3002", "result": "\u5728\u591a\u5c42\u03b1-(ET)2I3\u76843D\u72c4\u62c9\u514b/\u5916\u5c14\u534a\u91d1\u5c5e\u72b6\u6001\u4e2d\uff0cI\u578b\u72c4\u62c9\u514b\u534a\u91d1\u5c5e\u72b6\u6001\uff08\u7531\u5c42\u95f4SOC\u5f15\u8d77\uff09\u6700\u80fd\u6709\u6548\u89e3\u91ca\u7531WCC\u7247\u87ba\u65cb\u7ed3\u6784\u5f15\u8d77\u7684\u624b\u5f81\u78c1\u6548\u5e94\u6240\u5f52\u56e0\u7684\u624b\u5f81\u8f93\u8fd0\u73b0\u8c61\u3002\u5728\u591a\u5c42\u03b1-(BETS)2I3\u4e2d\uff0c\u65e0\u8bba\u662f\u5426\u5b58\u5728\u5c42\u95f4SOC\u548c/\u6216\u53cd\u8f6c\u5bf9\u79f0\u6027\u7834\u574f\uff0c\u90fd\u4f1a\u6301\u7eed\u51fa\u73b0\u4e09\u7ef4\u5f31\u62d3\u6251\u7edd\u7f18\u4f53(TI)\u72b6\u6001\u3002\u5b9e\u9a8c\u89c2\u6d4b\u8868\u660e\u7684\u5f3aTI\u72b6\u6001\u4f3c\u4e4e\u4e0d\u592a\u53ef\u80fd\u5b9e\u73b0\u3002", "conclusion": "\u5728\u591a\u5c42\u03b1-(BETS)2I3\u4e2d\uff0c\u65e0\u8bba\u662f\u5426\u5b58\u5728\u5c42\u95f4\u81ea\u65cb-\u8f68\u9053\u8026\u5408(SOC)\u548c/\u6216\u53cd\u8f6c\u5bf9\u79f0\u6027\u7834\u574f\uff0c\u90fd\u4f1a\u6301\u7eed\u51fa\u73b0\u4e09\u7ef4\u5f31\u62d3\u6251\u7edd\u7f18\u4f53(TI)\u72b6\u6001\u3002\u7531\u5c42\u95f4SOC\u5f15\u8d77\u7684I\u578b\u72c4\u62c9\u514b\u534a\u91d1\u5c5e\u72b6\u6001\u6700\u80fd\u89e3\u91ca\u89c2\u5bdf\u5230\u7684\u624b\u5f81\u8f93\u8fd0\u73b0\u8c61\uff0c\u8fd9\u6e90\u4e8eWCC\u7247\u7684\u87ba\u65cb\u7ed3\u6784\u3002\u5b9e\u9a8c\u89c2\u6d4b\u8868\u660e\u7684\u5f3aTI\u72b6\u6001\u4f3c\u4e4e\u4e0d\u592a\u53ef\u80fd\u5b9e\u73b0\u3002"}}
{"id": "2508.06952", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06952", "abs": "https://arxiv.org/abs/2508.06952", "authors": ["Haiyang Zhang", "Nir Shlezinger", "Giulia Torcolacci", "Francesco Guidi", "Anna Guerra", "Qianyu Yang", "Mohammadreza F. Imani", "Davide Dardari", "Yonina C. Eldar"], "title": "Extremely Large-Scale Dynamic Metasurface Antennas for 6G Near-Field Networks: Opportunities and Challenges", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "6G networks will need to support higher data rates, high-precision\nlocalization, and imaging capabilities. Near-field technologies, enabled by\nextremely large-scale (XL)-arrays, are expected to be essential physical-layer\nsolutions to meet these ambitious requirements. However, implementing XL-array\nsystems using traditional fully-digital or hybrid analog/digital architectures\nposes significant challenges due to high power consumption and implementation\ncosts. Emerging XL-dynamic metasurface antennas (XL-DMAs) provide a promising\nalternative, enabling ultra-low power and cost-efficient solutions, making them\nideal candidates for 6G near-field networks. In this article, we discuss the\nopportunities and challenges of XL-DMAs employed in 6G near-field networks. We\nfirst outline the fundamental principles of XL-DMAs and present the specifics\nof the near-field model of XL-DMAs. We then highlight several promising\napplications that might benefit from XL-DMAs, including near-field\ncommunication, localization, and imaging. Finally, we discuss several open\nproblems and potential future directions that should be addressed to fully\nexploit the capabilities of XL-DMAs in the next 6G near-field networks.", "AI": {"tldr": "XL-DMA\u662f\u4e00\u79cd\u4f4e\u529f\u8017\u3001\u4f4e\u6210\u672c\u76846G\u8fd1\u573a\u7f51\u7edc\u6280\u672f\uff0c\u5728\u901a\u4fe1\u3001\u5b9a\u4f4d\u548c\u6210\u50cf\u65b9\u9762\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u4e00\u4e9b\u6311\u6218\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db36G\u7f51\u7edc\u5bf9\u66f4\u9ad8\u7684\u6570\u636e\u901f\u7387\u3001\u9ad8\u7cbe\u5ea6\u5b9a\u4f4d\u548c\u6210\u50cf\u80fd\u529b\u7684\u8981\u6c42\uff0c\u9700\u8981\u5f15\u5165XL-array\u7b49\u65b0\u578b\u8fd1\u573a\u6280\u672f\u3002\u7136\u800c\uff0c\u4f20\u7edf\u7684\u5168\u6570\u5b57\u6216\u6df7\u5408\u67b6\u6784\u5b58\u5728\u529f\u8017\u9ad8\u3001\u6210\u672c\u9ad8\u7b49\u95ee\u9898\u3002XL-DMA\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u4ee5\u8d85\u4f4e\u529f\u8017\u548c\u4f4e\u6210\u672c\u5b9e\u73b0\u8fd9\u4e9b\u76ee\u6807\uff0c\u56e0\u6b64\u6210\u4e3a6G\u8fd1\u573a\u7f51\u7edc\u7684\u7406\u60f3\u9009\u62e9\u3002", "method": "\u672c\u6587\u8ba8\u8bba\u4e86XL-DMAs\u57286G\u8fd1\u573a\u7f51\u7edc\u4e2d\u7684\u673a\u9047\u548c\u6311\u6218\uff0c\u5305\u62ec\u5176\u57fa\u672c\u539f\u7406\u3001\u8fd1\u573a\u6a21\u578b\u3001\u6f5c\u5728\u5e94\u7528\u4ee5\u53ca\u5f00\u653e\u6027\u95ee\u9898\u548c\u672a\u6765\u65b9\u5411\u3002", "result": "XL-DMAs\u57286G\u8fd1\u573a\u901a\u4fe1\u3001\u5b9a\u4f4d\u548c\u6210\u50cf\u7b49\u5e94\u7528\u4e2d\u5c55\u73b0\u51fa\u5de8\u5927\u7684\u6f5c\u529b\uff0c\u4f46\u5176\u5e7f\u6cdb\u5e94\u7528\u4ecd\u9762\u4e34\u4e00\u4e9b\u6311\u6218\uff0c\u9700\u8981\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u548c\u53d1\u5c55\u3002", "conclusion": "XL-DMAs\u662f6G\u8fd1\u573a\u7f51\u7edc\u4e2d\u7684\u4e00\u79cd\u6709\u524d\u9014\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4f46\u4ecd\u9700\u89e3\u51b3\u4e00\u4e9b\u6311\u6218\u548c\u5f00\u653e\u6027\u95ee\u9898\u3002"}}
{"id": "2508.06706", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06706", "abs": "https://arxiv.org/abs/2508.06706", "authors": ["Jaikrishna Manojkumar Patil", "Nathaniel Lee", "Al Mehdi Saadat Chowdhury", "YooJung Choi", "Paulo Shakarian"], "title": "Probabilistic Circuits for Knowledge Graph Completion with Reduced Rule Sets", "comment": null, "summary": "Rule-based methods for knowledge graph completion provide explainable results\nbut often require a significantly large number of rules to achieve competitive\nperformance. This can hinder explainability due to overwhelmingly large rule\nsets. We discover rule contexts (meaningful subsets of rules that work\ntogether) from training data and use learned probability distribution (i.e.\nprobabilistic circuits) over these rule contexts to more rapidly achieve\nperformance of the full rule set. Our approach achieves a 70-96% reduction in\nnumber of rules used while outperforming baseline by up to 31$\\times$ when\nusing equivalent minimal number of rules and preserves 91% of peak baseline\nperformance even when comparing our minimal rule sets against baseline's full\nrule sets. We show that our framework is grounded in well-known semantics of\nprobabilistic logic, does not require independence assumptions, and that our\ntractable inference procedure provides both approximate lower bounds and exact\nprobability of a given query. The efficacy of our method is validated by\nempirical studies on 8 standard benchmark datasets where we show competitive\nperformance by using only a fraction of the rules required by AnyBURL's\nstandard inference method, the current state-of-the-art for rule-based\nknowledge graph completion. This work may have further implications for general\nprobabilistic reasoning over learned sets of rules.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u7535\u8def\u5b66\u4e60\u89c4\u5219\u4e0a\u4e0b\u6587\u7684\u65b9\u6cd5\uff0c\u4ee5\u51cf\u5c11\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u89c4\u5219\u6570\u91cf\uff0c\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u57fa\u4e8e\u89c4\u5219\u7684\u65b9\u6cd5\u5728\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u4e2d\u53ef\u89e3\u91ca\u6027\u5f3a\uff0c\u4f46\u9700\u8981\u5927\u91cf\u89c4\u5219\u624d\u80fd\u8fbe\u5230\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8fd9\u53ef\u80fd\u56e0\u89c4\u5219\u96c6\u8fc7\u5927\u800c\u5f71\u54cd\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u53d1\u73b0\u89c4\u5219\u4e0a\u4e0b\u6587\uff08\u5171\u540c\u4f5c\u7528\u7684\u6709\u610f\u4e49\u7684\u89c4\u5219\u5b50\u96c6\uff09\uff0c\u5e76\u4f7f\u7528\u5b66\u4e60\u5230\u7684\u5173\u4e8e\u8fd9\u4e9b\u89c4\u5219\u4e0a\u4e0b\u6587\u7684\u6982\u7387\u5206\u5e03\uff08\u6982\u7387\u7535\u8def\uff09\u6765\u6bd4\u4f7f\u7528\u5b8c\u6574\u89c4\u5219\u96c6\u66f4\u5feb\u5730\u83b7\u5f97\u6027\u80fd\u3002", "result": "\u5728\u4f7f\u7528\u7684\u89c4\u5219\u6570\u91cf\u4e0a\u5b9e\u73b0\u4e8670-96%\u7684\u51cf\u5c11\uff0c\u5728\u89c4\u5219\u6570\u91cf\u7b49\u6548\u65f6\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa31\u500d\uff0c\u5e76\u4e14\u5728\u6700\u5c0f\u89c4\u5219\u96c6\u4e0e\u57fa\u7ebf\u5b8c\u6574\u89c4\u5219\u96c6\u8fdb\u884c\u6bd4\u8f83\u65f6\uff0c\u4fdd\u7559\u4e8691%\u7684\u5cf0\u503c\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u6982\u7387\u7535\u8def\u5b66\u4e60\u89c4\u5219\u4e0a\u4e0b\u6587\uff0c\u5728\u4fdd\u6301\u8f83\u9ad8\u6027\u80fd\u7684\u540c\u65f6\u663e\u8457\u51cf\u5c11\u4e86\u89c4\u5219\u6570\u91cf\uff0c\u5e76\u57288\u4e2a\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u57fa\u4e8e\u89c4\u5219\u7684\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\u65b9\u6cd5\u3002"}}
{"id": "2508.06674", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06674", "abs": "https://arxiv.org/abs/2508.06674", "authors": ["Weijie Shi", "Yue Cui", "Hao Chen", "Jiaming Li", "Mengze Li", "Jia Zhu", "Jiajie Xu", "Xiaofang Zhou"], "title": "Zero-Shot Cellular Trajectory Map Matching", "comment": null, "summary": "Cellular Trajectory Map-Matching (CTMM) aims to align cellular location\nsequences to road networks, which is a necessary preprocessing in\nlocation-based services on web platforms like Google Maps, including navigation\nand route optimization. Current approaches mainly rely on ID-based features and\nregion-specific data to learn correlations between cell towers and roads,\nlimiting their adaptability to unexplored areas. To enable high-accuracy CTMM\nwithout additional training in target regions, Zero-shot CTMM requires to\nextract not only region-adaptive features, but also sequential and location\nuncertainty to alleviate positioning errors in cellular data. In this paper, we\npropose a pixel-based trajectory calibration assistant for zero-shot CTMM,\nwhich takes advantage of transferable geospatial knowledge to calibrate\npixelated trajectory, and then guide the path-finding process at the road\nnetwork level. To enhance knowledge sharing across similar regions, a Gaussian\nmixture model is incorporated into VAE, enabling the identification of\nscenario-adaptive experts through soft clustering. To mitigate high positioning\nerrors, a spatial-temporal awareness module is designed to capture sequential\nfeatures and location uncertainty, thereby facilitating the inference of\napproximate user positions. Finally, a constrained path-finding algorithm is\nemployed to reconstruct the road ID sequence, ensuring topological validity\nwithin the road network. This process is guided by the calibrated trajectory\nwhile optimizing for the shortest feasible path, thus minimizing unnecessary\ndetours. Extensive experiments demonstrate that our model outperforms existing\nmethods in zero-shot CTMM by 16.8\\%.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u96f6\u6837\u672c CTMM \u65b9\u6cd5\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u8f68\u8ff9\u6821\u51c6\u548c\u5148\u8fdb\u7684\u673a\u5668\u5b66\u4e60\u6280\u672f\uff08\u5982 VAE \u548c\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff09\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709 CTMM \u65b9\u6cd5\u4f9d\u8d56\u4e8e ID \u548c\u533a\u57df\u7279\u5b9a\u6570\u636e\uff0c\u96be\u4ee5\u9002\u5e94\u65b0\u533a\u57df\u3002\u96f6\u6837\u672c CTMM \u9700\u8981\u63d0\u53d6\u533a\u57df\u81ea\u9002\u5e94\u7279\u5f81\u3001\u5e8f\u5217\u4fe1\u606f\u548c\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u514b\u670d\u7ec6\u80de\u6570\u636e\u4e2d\u7684\u5b9a\u4f4d\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u50cf\u7d20\u7ea7\u8f68\u8ff9\u6821\u51c6\u8f85\u52a9\u65b9\u6cd5\uff0c\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u3001\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\u548c\u7a7a\u95f4-\u65f6\u95f4\u611f\u77e5\u6a21\u5757\uff0c\u7528\u4e8e\u96f6\u6837\u672c CTMM\u3002", "result": "\u5728\u96f6\u6837\u672c CTMM \u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e86 16.8%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u96f6\u6837\u672c CTMM \u7684\u50cf\u7d20\u7ea7\u8f68\u8ff9\u6821\u51c6\u8f85\u52a9\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u53ef\u8fc1\u79fb\u7684\u5730\u7406\u7a7a\u95f4\u77e5\u8bc6\u6765\u6821\u51c6\u50cf\u7d20\u5316\u8f68\u8ff9\uff0c\u5e76\u5728\u9053\u8def\u7f51\u7edc\u7ea7\u522b\u6307\u5bfc\u8def\u5f84\u67e5\u627e\u8fc7\u7a0b\u3002\u901a\u8fc7\u7ed3\u5408\u9ad8\u65af\u6df7\u5408\u6a21\u578b\u548c\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08VAE\uff09\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u901a\u8fc7\u8f6f\u805a\u7c7b\u8bc6\u522b\u573a\u666f\u81ea\u9002\u5e94\u4e13\u5bb6\u3002\u540c\u65f6\uff0c\u7a7a\u95f4-\u65f6\u95f4\u611f\u77e5\u6a21\u5757\u7528\u4e8e\u6355\u83b7\u5e8f\u5217\u7279\u5f81\u548c\u4f4d\u7f6e\u4e0d\u786e\u5b9a\u6027\uff0c\u4ee5\u51cf\u8f7b\u5b9a\u4f4d\u8bef\u5dee\u3002\u6700\u7ec8\uff0c\u91c7\u7528\u7ea6\u675f\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\u6765\u91cd\u5efa\u9053\u8def ID \u5e8f\u5217\uff0c\u786e\u4fdd\u9053\u8def\u7f51\u7edc\u5185\u7684\u62d3\u6251\u6709\u6548\u6027\uff0c\u5e76\u4f18\u5316\u6700\u77ed\u53ef\u884c\u8def\u5f84\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u6a21\u578b\u5728\u96f6\u6837\u672c CTMM \u65b9\u9762\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e86 16.8%\u3002"}}
{"id": "2508.07446", "categories": ["cs.DS", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.07446", "abs": "https://arxiv.org/abs/2508.07446", "authors": ["Daniel Brous", "David Shmoys"], "title": "Optimizing Districting Plans to Maximize Majority-Minority Districts via IPs and Local Search", "comment": "12 pages, 4 figures, 1 table", "summary": "In redistricting litigation, effective enforcement of the Voting Rights Act\nhas often involved providing the court with districting plans that display a\nlarger number of majority-minority districts than the current proposal (as was\ntrue, for example, in what followed Allen v. Milligan concerning the\ncongressional districting plan for Alabama in 2023). Recent work by Cannon et\nal. proposed a heuristic algorithm for generating plans to optimize\nmajority-minority districts, which they called short bursts; that algorithm\nrelies on a sophisticated random walk over the space of all plans,\ntransitioning in bursts, where the initial plan for each burst is the most\nsuccessful plan from the previous burst. We propose a method based on integer\nprogramming, where we build upon another previous work, the stochastic\nhierarchical partitioning algorithm, which heuristically generates a robust set\nof potential districts (viewed as columns in a standard set partitioning\nformulation); that approach was designed to optimize a different notion of\nfairness across a statewide plan. We design a new column generation algorithm\nto find plans via integer programming that outperforms short bursts on multiple\ndata sets in generating statewide plans with significantly more\nmajority-minority districts. These results also rely on a new local\nre-optimization algorithm to iteratively improve on any baseline solution, as\nwell as an algorithm to increase the compactness of districts in plans\ngenerated (without impacting the number of majority-minority districts).", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u89c4\u5212\u548c\u5217\u751f\u6210\u7684\u65b0\u65b9\u6cd5\uff0c\u4ee5\u751f\u6210\u66f4\u591a\u7684\u591a\u6570\u5c11\u6570\u65cf\u88d4\u9009\u533a\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u201c\u77ed\u7206\u53d1\u201d\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u91cd\u65b0\u5212\u5206\u9009\u533a\u8bc9\u8bbc\u4e2d\u66f4\u6709\u6548\u5730\u6267\u884c\u300a\u6295\u7968\u6743\u6cd5\u6848\u300b\uff0c\u9700\u8981\u751f\u6210\u5305\u542b\u66f4\u591a\u591a\u6570\u5c11\u6570\u65cf\u88d4\u9009\u533a\u7684\u9009\u533a\u89c4\u5212\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u89c4\u5212\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5217\u751f\u6210\u7b97\u6cd5\u6765\u4f18\u5316\u591a\u6570\u5c11\u6570\u65cf\u88d4\u9009\u533a\uff0c\u5e76\u7ed3\u5408\u4e86\u65b0\u7684\u5c40\u90e8\u518d\u4f18\u5316\u7b97\u6cd5\u548c\u589e\u52a0\u9009\u533a\u7d27\u51d1\u6027\u7684\u7b97\u6cd5\u3002", "result": "\u65b0\u65b9\u6cd5\u5728\u751f\u6210\u5dde\u7ea7\u9009\u533a\u89c4\u5212\u65b9\u9762\u4f18\u4e8e\u201c\u77ed\u7206\u53d1\u201d\u7b97\u6cd5\uff0c\u80fd\u591f\u751f\u6210\u663e\u8457\u66f4\u591a\u6570\u91cf\u7684\u591a\u6570\u5c11\u6570\u65cf\u88d4\u9009\u533a\uff0c\u5e76\u4e14\u53ef\u4ee5\u63d0\u9ad8\u9009\u533a\u7684\u7d27\u51d1\u6027\u800c\u4e0d\u4f1a\u5f71\u54cd\u591a\u6570\u5c11\u6570\u65cf\u88d4\u9009\u533a\u7684\u6570\u91cf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6574\u6570\u89c4\u5212\u548c\u5217\u751f\u6210\u7b97\u6cd5\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u4f18\u5316\u591a\u6570\u5c11\u6570\u65cf\u88d4\u9009\u533a\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u751f\u6210\u6bd4\u73b0\u6709\u201c\u77ed\u7206\u53d1\u201d\u7b97\u6cd5\u5177\u6709\u66f4\u591a\u591a\u6570\u5c11\u6570\u65cf\u88d4\u9009\u533a\u7684\u5dde\u7ea7\u9009\u533a\u89c4\u5212\u3002"}}
{"id": "2508.06589", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06589", "abs": "https://arxiv.org/abs/2508.06589", "authors": ["Xinglin Zhao", "Yanwen Wang", "Xiaobo Liu", "Yanrong Hao", "Rui Cao", "Xin Wen"], "title": "A Federated Learning Framework for Handling Subtype Confounding and Heterogeneity in Large-Scale Neuroimaging Diagnosis", "comment": null, "summary": "Computer-aided diagnosis (CAD) systems play a crucial role in analyzing\nneuroimaging data for neurological and psychiatric disorders. However,\nsmall-sample studies suffer from low reproducibility, while large-scale\ndatasets introduce confounding heterogeneity due to multiple disease subtypes\nbeing labeled under a single category. To address these challenges, we propose\na novel federated learning framework tailored for neuroimaging CAD systems. Our\napproach includes a dynamic navigation module that routes samples to the most\nsuitable local models based on latent subtype representations, and a\nmeta-integration module that combines predictions from heterogeneous local\nmodels into a unified diagnostic output. We evaluated our framework using a\ncomprehensive dataset comprising fMRI data from over 1300 MDD patients and 1100\nhealthy controls across multiple study cohorts. Experimental results\ndemonstrate significant improvements in diagnostic accuracy and robustness\ncompared to traditional methods. Specifically, our framework achieved an\naverage accuracy of 74.06\\% across all tested sites, showcasing its\neffectiveness in handling subtype heterogeneity and enhancing model\ngeneralizability. Ablation studies further confirmed the importance of both the\ndynamic navigation and meta-integration modules in improving performance. By\naddressing data heterogeneity and subtype confounding, our framework advances\nreliable and reproducible neuroimaging CAD systems, offering significant\npotential for personalized medicine and clinical decision-making in neurology\nand psychiatry.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u5bfc\u822a\u548c\u5143\u96c6\u6210\u6a21\u5757\u89e3\u51b3\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\u4e2d\u7684\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e9a\u578b\u6df7\u6dc6\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5c0f\u6837\u672c\u7814\u7a76\u590d\u73b0\u6027\u4f4e\u4ee5\u53ca\u5927\u89c4\u6a21\u6570\u636e\u96c6\u56e0\u5305\u542b\u591a\u79cd\u75be\u75c5\u4e9a\u578b\u4f46\u4ec5\u88ab\u6807\u8bb0\u4e3a\u5355\u4e00\u7c7b\u522b\u800c\u5f15\u8d77\u7684\u6df7\u6742\u5f02\u8d28\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u90a6\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u4e00\u4e2a\u52a8\u6001\u5bfc\u822a\u6a21\u5757\uff08\u6839\u636e\u6f5c\u5728\u4e9a\u578b\u8868\u793a\u5c06\u6837\u672c\u8def\u7531\u5230\u6700\u5408\u9002\u7684\u5c40\u90e8\u6a21\u578b\uff09\u548c\u4e00\u4e2a\u5143\u96c6\u6210\u6a21\u5757\uff08\u5c06\u6765\u81ea\u5f02\u6784\u5c40\u90e8\u6a21\u578b\u7684\u9884\u6d4b\u7ec4\u5408\u6210\u7edf\u4e00\u7684\u8bca\u65ad\u8f93\u51fa\uff09\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u8de8\u8d8a\u591a\u4e2a\u7814\u7a76\u961f\u5217\u76841300\u591a\u540dMDD\u60a3\u8005\u548c1100\u540d\u5065\u5eb7\u5bf9\u7167\u8005\u7684fMRI\u6570\u636e\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u8bca\u65ad\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5e73\u5747\u51c6\u786e\u7387\u4e3a74.06%\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u89e3\u51b3\u6570\u636e\u5f02\u8d28\u6027\u548c\u4e9a\u578b\u6df7\u6dc6\u95ee\u9898\uff0c\u63a8\u52a8\u4e86\u53ef\u9760\u4e14\u53ef\u590d\u73b0\u7684\u795e\u7ecf\u5f71\u50cfCAD\u7cfb\u7edf\uff0c\u4e3a\u795e\u7ecf\u75c5\u5b66\u548c\u7cbe\u795e\u75c5\u5b66\u7684\u4e2a\u6027\u5316\u533b\u7597\u548c\u4e34\u5e8a\u51b3\u7b56\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.06555", "categories": ["cs.CV", "cs.CY", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06555", "abs": "https://arxiv.org/abs/2508.06555", "authors": ["Hongbo Ma", "Fei Shen", "Hongbin Xu", "Xiaoce Wang", "Gang Xu", "Jinkai Zheng", "Liangqiong Qu", "Ming Li"], "title": "StyleTailor: Towards Personalized Fashion Styling via Hierarchical Negative Feedback", "comment": "24pages, 5 figures", "summary": "The advancement of intelligent agents has revolutionized problem-solving\nacross diverse domains, yet solutions for personalized fashion styling remain\nunderexplored, which holds immense promise for promoting shopping experiences.\nIn this work, we present StyleTailor, the first collaborative agent framework\nthat seamlessly unifies personalized apparel design, shopping recommendation,\nvirtual try-on, and systematic evaluation into a cohesive workflow. To this\nend, StyleTailor pioneers an iterative visual refinement paradigm driven by\nmulti-level negative feedback, enabling adaptive and precise user alignment.\nSpecifically, our framework features two core agents, i.e., Designer for\npersonalized garment selection and Consultant for virtual try-on, whose outputs\nare progressively refined via hierarchical vision-language model feedback\nspanning individual items, complete outfits, and try-on efficacy.\nCounterexamples are aggregated into negative prompts, forming a closed-loop\nmechanism that enhances recommendation quality.To assess the performance, we\nintroduce a comprehensive evaluation suite encompassing style consistency,\nvisual quality, face similarity, and artistic appraisal. Extensive experiments\ndemonstrate StyleTailor's superior performance in delivering personalized\ndesigns and recommendations, outperforming strong baselines without negative\nfeedback and establishing a new benchmark for intelligent fashion systems.", "AI": {"tldr": "StyleTailor\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u65f6\u5c1a\u9020\u578b\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u5c42\u6b21\u8d1f\u53cd\u9988\u548c\u4e24\u4e2a\u6838\u5fc3\u4ee3\u7406\uff08\u8bbe\u8ba1\u5e08\u548c\u987e\u95ee\uff09\u5b9e\u73b0\u4e2a\u6027\u5316\u8bbe\u8ba1\u548c\u63a8\u8350\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u4e2a\u6027\u5316\u65f6\u5c1a\u9020\u578b\u7684\u89e3\u51b3\u65b9\u6848\u4ecd\u6709\u5f85\u63a2\u7d22\uff0c\u4f46\u5b83\u5728\u6539\u5584\u8d2d\u7269\u4f53\u9a8c\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002", "method": "StyleTailor\u662f\u4e00\u4e2a\u534f\u4f5c\u4ee3\u7406\u6846\u67b6\uff0c\u96c6\u6210\u4e86\u4e2a\u6027\u5316\u670d\u88c5\u8bbe\u8ba1\u3001\u8d2d\u7269\u63a8\u8350\u3001\u865a\u62df\u8bd5\u7a7f\u548c\u7cfb\u7edf\u8bc4\u4f30\u3002\u5b83\u91c7\u7528\u591a\u5c42\u6b21\u8d1f\u53cd\u9988\u9a71\u52a8\u7684\u8fed\u4ee3\u89c6\u89c9\u7cbe\u70bc\u8303\u5f0f\uff0c\u901a\u8fc7\u8bbe\u8ba1\u5e08\u548c\u987e\u95ee\u4e24\u4e2a\u6838\u5fc3\u4ee3\u7406\uff0c\u5229\u7528\u5206\u5c42\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u53cd\u9988\u8fdb\u884c\u4f18\u5316\u3002", "result": "StyleTailor\u5728\u63d0\u4f9b\u4e2a\u6027\u5316\u8bbe\u8ba1\u548c\u63a8\u8350\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u6ca1\u6709\u8d1f\u53cd\u9988\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "StyleTailor\u5728\u4e2a\u6027\u5316\u8bbe\u8ba1\u548c\u63a8\u8350\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\uff0c\u4e3a\u667a\u80fd\u65f6\u5c1a\u7cfb\u7edf\u8bbe\u5b9a\u4e86\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.06866", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.06866", "abs": "https://arxiv.org/abs/2508.06866", "authors": ["Md Salman Rabbi Limon", "Abrar Fahim Navid", "Curtis Wesley Duffee", "Zeeshan Ahmad"], "title": "Grain Boundaries in Ceramic Solid-State Lithium Metal Batteries: A Review", "comment": "67 pages, 22 figures", "summary": "It is now widely accepted that grain boundaries play a critical role in the\nperformance and reliability of solid-state batteries with lithium metal anodes.\nUnderstanding and controlling grain boundaries is essential for enabling safe,\nhigh-rate operation of solid-state batteries. This review explores the\nmultifaceted influence of grain boundaries in ceramic solid electrolytes and\nmetal anodes, including their impact on ionic and electronic transport,\ndendrite and void formation, connecting them to the failure mechanisms. We\ndiscuss the formation and structure of space charge layers at grain boundaries,\ntheir role in modulating local defect chemistry, and the conditions under which\ngrain boundaries may serve as fast-ion pathways or as vulnerable sites for\nfailure. We highlight key differences in the grain boundaries of different\nclasses of solid electrolytes and advances in modeling, experimental\ncharacterization, and processing techniques to understand the complexity and\nengineer grain boundaries in solid electrolytes. Finally, we outline key open\nquestions and opportunities for grain boundary engineering to stimulate further\nprogress in the field.", "AI": {"tldr": "Grain boundaries are key to solid-state battery performance; this review covers their impact on transport, failure, and how to engineer them for better batteries.", "motivation": "To understand and control grain boundaries, which are critical for enabling safe, high-rate operation of solid-state batteries with lithium metal anodes, by exploring their multifaceted influence on performance and reliability.", "method": "This review explores the multifaceted influence of grain boundaries in ceramic solid electrolytes and metal anodes, discussing their impact on transport, dendrite/void formation, failure mechanisms, space charge layers, defect chemistry, and potential as fast-ion pathways or failure sites. It also highlights differences across electrolyte classes and advances in modeling, experimental characterization, and processing techniques, concluding with open questions and opportunities for grain boundary engineering.", "result": "Grain boundaries significantly impact ionic and electronic transport, dendrite and void formation, and failure mechanisms in solid-state batteries. Space charge layers at grain boundaries modulate local defect chemistry, potentially serving as fast-ion pathways or failure sites. Key differences exist across various solid electrolyte classes, and advances in modeling, characterization, and processing are aiding in understanding and engineering these boundaries.", "conclusion": "Grain boundaries are critical to the performance and reliability of solid-state batteries with lithium metal anodes, influencing ionic and electronic transport, dendrite/void formation, and failure mechanisms. Understanding and engineering these boundaries, including space charge layers and defect chemistry, is essential for safe, high-rate operation. Differences in grain boundaries across electrolyte classes and advances in modeling, characterization, and processing are highlighted, along with open questions for future progress."}}
{"id": "2508.07317", "categories": ["cs.DC", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07317", "abs": "https://arxiv.org/abs/2508.07317", "authors": ["Pedro Carrinho", "Hamid Moghadaspour", "Oscar Ferraz", "Jo\u00e3o Dinis Ferreira", "Yann Falevoz", "Vitor Silva", "Gabriel Falcao"], "title": "An Experimental Exploration of In-Memory Computing for Multi-Layer Perceptrons", "comment": "19 pages, 1 figures, and 2 tables", "summary": "In modern computer architectures, the performance of many memory-bound\nworkloads (e.g., machine learning, graph processing, databases) is limited by\nthe data movement bottleneck that emerges when transferring large amounts of\ndata between the main memory and the central processing unit (CPU).\nProcessing-in-memory is an emerging computing paradigm that aims to alleviate\nthis data movement bottleneck by performing computation close to or within the\nmemory units, where data resides. One example of a prevalent workload whose\nperformance is bound by the data movement bottleneck is the training and\ninference process of artificial neural networks. In this work, we analyze the\npotential of modern general-purpose PiM architectures to accelerate neural\nnetworks. To this end, we selected the UPMEM PiM system, the first commercially\navailable real-world general-purpose PiM architecture. We compared the\nimplementation of multilayer perceptrons (MLPs) in PiM with a sequential\nbaseline running on an Intel Xeon CPU. The UPMEM implementation achieves up to\n$259\\times$ better performance for inference of large batch sizes when compared\nagainst the CPU that exploits the size of the available PiM memory.\nAdditionally, two smaller MLPs were implemented using UPMEM's working SRAM\n(WRAM), a scratchpad memory, to evaluate their performance against a low-power\nNvidia Jetson graphics processing unit (GPU), providing further insights into\nthe efficiency of UPMEM's PiM for neural network inference. Results show that\nusing WRAM achieves kernel execution times for MLP inference of under $3$ ms,\nwhich is within the same order of magnitude as low-power GPUs.", "AI": {"tldr": "\u5728\u5904\u7406\u5185\u5b58\uff08PiM\uff09\u7684\u5e2e\u52a9\u4e0b\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u53ef\u4ee5\u5f97\u5230\u63d0\u5347\u3002", "motivation": "\u89e3\u51b3\u5185\u5b58\u5bc6\u96c6\u578b\u5de5\u4f5c\u8d1f\u8f7d\uff08\u5982\u673a\u5668\u5b66\u4e60\uff09\u9762\u4e34\u7684\u6570\u636e\u79fb\u52a8\u74f6\u9888\u95ee\u9898\uff0c\u63a2\u7d22\u5904\u7406\u5185\u5b58\uff08PiM\uff09\u6280\u672f\u5728\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5206\u6790\u4e86\u73b0\u4ee3\u901a\u7528\u5904\u7406\u5185\u5b58\uff08PiM\uff09\u67b6\u6784\u52a0\u901f\u795e\u7ecf\u7f51\u7edc\u7684\u6f5c\u529b\u3002\u9009\u62e9\u4e86 UPMEM PiM \u7cfb\u7edf\uff0c\u5e76\u5c06\u5176\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u7684\u5b9e\u73b0\u4e0e\u5728 Intel Xeon CPU \u4e0a\u7684\u987a\u5e8f\u57fa\u7ebf\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u6b64\u5916\uff0c\u8fd8\u4f7f\u7528 UPMEM \u7684\u5de5\u4f5c SRAM\uff08WRAM\uff09\u5b9e\u73b0\u4e86\u4e24\u4e2a\u8f83\u5c0f\u7684 MLP\uff0c\u5e76\u4e0e\u4f4e\u529f\u8017 Nvidia Jetson GPU \u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "result": "UPMEM PiM \u5b9e\u73b0\u7684 MLP \u5728\u5927\u6279\u91cf\u63a8\u7406\u65b9\u9762\u6bd4 CPU \u5feb 259 \u500d\u3002\u4f7f\u7528 WRAM \u7684 MLP \u63a8\u7406\u5185\u6838\u6267\u884c\u65f6\u95f4\u4e0d\u5230 3 \u6beb\u79d2\uff0c\u4e0e\u4f4e\u529f\u8017 GPU \u5904\u4e8e\u540c\u4e00\u6570\u91cf\u7ea7\u3002", "conclusion": "UPMEM PiM \u7cfb\u7edf\u5728\u5904\u7406\u795e\u7ecf\u7f51\u7edc\u65b9\u9762\u5177\u6709\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5229\u7528\u5176\u5927\u5bb9\u91cf\u5185\u5b58\u548c WRAM \u65f6\uff0c\u53ef\u4ee5\u4e0e\u4f4e\u529f\u8017 GPU \u5ab2\u7f8e\u3002"}}
{"id": "2508.06554", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06554", "abs": "https://arxiv.org/abs/2508.06554", "authors": ["Abdelhaleem Saad", "Waseem Akram", "Irfan Hussain"], "title": "AquaChat++: LLM-Assisted Multi-ROV Inspection for Aquaculture Net Pens with Integrated Battery Management and Thruster Fault Tolerance", "comment": null, "summary": "Inspection of aquaculture net pens is essential for ensuring the structural\nintegrity and sustainable operation of offshore fish farming systems.\nTraditional methods, typically based on manually operated or single-ROV\nsystems, offer limited adaptability to real-time constraints such as energy\nconsumption, hardware faults, and dynamic underwater conditions. This paper\nintroduces AquaChat++, a novel multi-ROV inspection framework that uses Large\nLanguage Models (LLMs) to enable adaptive mission planning, coordinated task\nexecution, and fault-tolerant control in complex aquaculture environments. The\nproposed system consists of a two-layered architecture. The high-level plan\ngeneration layer employs an LLM, such as ChatGPT-4, to translate natural\nlanguage user commands into symbolic, multi-agent inspection plans. A task\nmanager dynamically allocates and schedules actions among ROVs based on their\nreal-time status and operational constraints, including thruster faults and\nbattery levels. The low-level control layer ensures accurate trajectory\ntracking and integrates thruster fault detection and compensation mechanisms.\nBy incorporating real-time feedback and event-triggered replanning, AquaChat++\nenhances system robustness and operational efficiency. Simulated experiments in\na physics-based aquaculture environment demonstrate improved inspection\ncoverage, energy-efficient behavior, and resilience to actuator failures. These\nfindings highlight the potential of LLM-driven frameworks to support scalable,\nintelligent, and autonomous underwater robotic operations within the\naquaculture sector.", "AI": {"tldr": "AquaChat++\u662f\u4e00\u4e2a\u5229\u7528LLM\u5b9e\u73b0\u81ea\u9002\u5e94\u4efb\u52a1\u89c4\u5212\u3001\u534f\u8c03\u4efb\u52a1\u6267\u884c\u548c\u5bb9\u9519\u63a7\u5236\u7684\u65b0\u578b\u591aROV\u68c0\u67e5\u6846\u67b6\uff0c\u65e8\u5728\u89e3\u51b3\u4f20\u7edf\u6c34\u4e0b\u68c0\u67e5\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u6d77\u4e0a\u6e14\u4e1a\u517b\u6b96\u7cfb\u7edf\u7684\u7ed3\u6784\u5b8c\u6574\u6027\u548c\u53ef\u6301\u7eed\u8fd0\u884c\uff0c\u5bf9\u517b\u6b96\u7f51\u7bb1\u8fdb\u884c\u68c0\u67e5\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u57fa\u4e8e\u624b\u52a8\u64cd\u4f5c\u6216\u5355ROV\u7cfb\u7edf\u7684\u68c0\u67e5\u65b9\u6cd5\uff0c\u5728\u9002\u5e94\u5b9e\u65f6\u7ea6\u675f\uff08\u5982\u80fd\u6e90\u6d88\u8017\u3001\u786c\u4ef6\u6545\u969c\u548c\u52a8\u6001\u6c34\u4e0b\u6761\u4ef6\uff09\u65b9\u9762\u9002\u5e94\u6027\u6709\u9650\u3002", "method": "\u8be5\u6846\u67b6\u91c7\u7528\u4e86\u5206\u5c42\u67b6\u6784\uff1a\u9ad8\u7ea7\u89c4\u5212\u5c42\u5229\u7528LLM\uff08\u5982ChatGPT-4\uff09\u5c06\u81ea\u7136\u8bed\u8a00\u7528\u6237\u547d\u4ee4\u8f6c\u6362\u4e3a\u7b26\u53f7\u5316\u7684\u591a\u667a\u80fd\u4f53\u68c0\u67e5\u8ba1\u5212\uff1b\u4efb\u52a1\u7ba1\u7406\u5668\u6839\u636eROV\u7684\u5b9e\u65f6\u72b6\u6001\u548c\u64cd\u4f5c\u9650\u5236\uff08\u5305\u62ec\u63a8\u8fdb\u5668\u6545\u969c\u548c\u7535\u6c60\u7535\u91cf\uff09\u52a8\u6001\u5206\u914d\u548c\u8c03\u5ea6ROV\u4e4b\u95f4\u7684\u52a8\u4f5c\u3002\u4f4e\u7ea7\u63a7\u5236\u5c42\u786e\u4fdd\u7cbe\u786e\u7684\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5e76\u6574\u5408\u63a8\u8fdb\u5668\u6545\u969c\u68c0\u6d4b\u548c\u8865\u507f\u673a\u5236\u3002\u901a\u8fc7\u7ed3\u5408\u5b9e\u65f6\u53cd\u9988\u548c\u4e8b\u4ef6\u89e6\u53d1\u7684\u91cd\u65b0\u89c4\u5212\uff0cAquaChat++\u63d0\u9ad8\u4e86\u7cfb\u7edf\u7684\u9c81\u68d2\u6027\u548c\u8fd0\u884c\u6548\u7387\u3002", "result": "\u6a21\u62df\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u65b9\u6cd5\u76f8\u6bd4\uff0cAquaChat++\u5728\u68c0\u67e5\u8986\u76d6\u8303\u56f4\u3001\u8282\u80fd\u884c\u4e3a\u548c\u5bf9\u6267\u884c\u5668\u6545\u969c\u7684\u9c81\u68d2\u6027\u65b9\u9762\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u6846\u67b6\u6709\u6f5c\u529b\u652f\u6301\u6c34\u4ea7\u517b\u6b96\u9886\u57df\u4e2d\u53ef\u6269\u5c55\u3001\u667a\u80fd\u548c\u81ea\u4e3b\u7684\u6c34\u4e0b\u673a\u5668\u4eba\u64cd\u4f5c\u3002"}}
{"id": "2508.06677", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06677", "abs": "https://arxiv.org/abs/2508.06677", "authors": ["Harriet Apel", "Cristian L. Cortes", "Jessica Lemieux", "Mark Steudtner"], "title": "Reducing quantum resources for observable estimation with window-assisted coherent QPE", "comment": "32 pages, 5 figures", "summary": "Quantum Phase Estimation (QPE) routines are known to fail probabilistically\neven with perfect gates and input states. This effect stems from an\nincompatibility of finite-sized quantum registers to capture a phase within QPE\nwith phase angles of infinite precision, and the effect extend even beyond what\nwould be reasonably expected from rounding. This effect can be partially\nmitigated by biasing the phase register with a window, or taper state, from\nclassical signal processing. This paper focuses on how windowing a coherent QPE\nused as a subroutine can improve the accuracy of the overall algorithm.\nSpecifically we study the quantum task of estimating observables where\nwindow-assisted coherent QPE is used as a subroutine to implement a reflection\nabout an eigenstate. Quantum resource estimates show over 2-orders-of-magnitude\nreduction in Toffoli counts over the previous costed techniques -- also\nassisted by the use of improved block encoding techniques -- demonstrating an\nencouraging decrease in resources for quantum computation of molecular\nobservables. Since QPE, as one of only a few quantum building blocks, appears\nas a subroutine in many algorithms; this analysis also provides a model for\nunderstanding how window functions propagate to an improved error in composite\nalgorithms.", "AI": {"tldr": "\u91cf\u5b50\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\u4e2d\u7684\u7a97\u53e3\u51fd\u6570\u53ef\u4ee5\u63d0\u9ad8\u7cbe\u5ea6\u5e76\u51cf\u5c11\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u91cf\u5b50\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\u5b58\u5728\u56fa\u6709\u7684\u6982\u7387\u6027\u5931\u8d25\uff0c\u5373\u4f7f\u5728\u5b8c\u7f8e\u95e8\u548c\u8f93\u5165\u72b6\u6001\u4e0b\u4e5f\u662f\u5982\u6b64\uff0c\u8fd9\u662f\u7531\u4e8e\u6709\u9650\u91cf\u5b50\u5bc4\u5b58\u5668\u65e0\u6cd5\u7cbe\u786e\u6355\u83b7\u76f8\u4f4d\u3002", "method": "\u7814\u7a76\u4e86\u7a97\u53e3\u51fd\u6570\u5982\u4f55\u5e94\u7528\u4e8e\u91cf\u5b50\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\uff0c\u5e76\u5c06\u5176\u4f5c\u4e3a\u5b9e\u73b0\u672c\u5f81\u6001\u53cd\u5c04\u7684\u5b50\u7a0b\u5e8f\uff0c\u540c\u65f6\u7ed3\u5408\u4e86\u6539\u8fdb\u7684\u5757\u7f16\u7801\u6280\u672f\u3002", "result": "\u4e0e\u4e4b\u524d\u7684\u6280\u672f\u76f8\u6bd4\uff0c\u91cf\u5b50\u8d44\u6e90\u4f30\u8ba1\u663e\u793a\u4e86\u8d85\u8fc7\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684Toffoli\u95e8\u8ba1\u6570\u51cf\u5c11\uff0c\u8bc1\u660e\u4e86\u5728\u91cf\u5b50\u8ba1\u7b97\u5206\u5b50\u53ef\u89c2\u6d4b\u503c\u65b9\u9762\u8d44\u6e90\u7684\u6709\u6548\u51cf\u5c11\u3002aterina\u7a97\u53e3\u51fd\u6570\u53ef\u4ee5\u6539\u5584\u6574\u4f53\u7b97\u6cd5\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u7a97\u53e3\u51fd\u6570\u53ef\u4ee5\u63d0\u9ad8\u91cf\u5b50\u76f8\u4f4d\u4f30\u8ba1\u7b97\u6cd5\u7684\u7cbe\u5ea6\uff0c\u5e76\u4e3a\u7406\u89e3\u7a97\u53e3\u51fd\u6570\u5728\u590d\u5408\u7b97\u6cd5\u4e2d\u7684\u8bef\u5dee\u4f20\u64ad\u63d0\u4f9b\u4e86\u6a21\u578b\u3002"}}
{"id": "2508.07725", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07725", "abs": "https://arxiv.org/abs/2508.07725", "authors": ["Andreas Hager-Clukas", "Philipp van Kempen", "Stefan Wallentowitz"], "title": "ARISE: Automating RISC-V Instruction Set Extension", "comment": null, "summary": "RISC-V is an extendable Instruction Set Architecture, growing in popularity\nfor embedded systems. However, optimizing it to specific requirements, imposes\na great deal of manual effort. To bridge the gap between software and ISA, the\ntool ARISE is presented. It automates the generation of RISC-V instructions\nbased on assembly patterns, which are selected by an extendable set of metrics.\nThese metrics implement the optimization goals of code size and instruction\ncount reduction, both statically and dynamically. The instruction set\nextensions are generated using the ISA description language CoreDSL. Allowing\nseamless embedding in advanced tools such as the retargeting compiler Seal5 or\nthe instruction set simulator ETISS. ARISE improves the static code size by\n1.48% and the dynamic code size by 3.84%, as well as the number of instructions\nto be executed by 7.39% on average for Embench-Iot.", "AI": {"tldr": "ARISE automates RISC-V instruction generation to optimize code size and instruction count, showing significant improvements on benchmarks.", "motivation": "Optimizing RISC-V for specific requirements is labor-intensive, creating a need for a tool to bridge the gap between software and ISA.", "method": "ARISE automates the generation of RISC-V instructions based on assembly patterns and a set of metrics, using CoreDSL for ISA description and integrating with tools like Seal5 and ETISS.", "result": "ARISE improves static code size by 1.48%, dynamic code size by 3.84%, and reduces the number of executed instructions by 7.39% on average for Embench-Iot.", "conclusion": "ARISE can automate the generation of RISC-V instructions, improving code size and instruction count."}}
{"id": "2508.06768", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06768", "abs": "https://arxiv.org/abs/2508.06768", "authors": ["Noe Bertramo", "Gabriel Duguey", "Vivek Gopalakrishnan"], "title": "DiffUS: Differentiable Ultrasound Rendering from Volumetric Imaging", "comment": "10 pages, accepted to MICCAI ASMUS 25", "summary": "Intraoperative ultrasound imaging provides real-time guidance during numerous\nsurgical procedures, but its interpretation is complicated by noise, artifacts,\nand poor alignment with high-resolution preoperative MRI/CT scans. To bridge\nthe gap between reoperative planning and intraoperative guidance, we present\nDiffUS, a physics-based, differentiable ultrasound renderer that synthesizes\nrealistic B-mode images from volumetric imaging. DiffUS first converts MRI 3D\nscans into acoustic impedance volumes using a machine learning approach. Next,\nwe simulate ultrasound beam propagation using ray tracing with coupled\nreflection-transmission equations. DiffUS formulates wave propagation as a\nsparse linear system that captures multiple internal reflections. Finally, we\nreconstruct B-mode images via depth-resolved echo extraction across fan-shaped\nacquisition geometry, incorporating realistic artifacts including speckle noise\nand depth-dependent degradation. DiffUS is entirely implemented as\ndifferentiable tensor operations in PyTorch, enabling gradient-based\noptimization for downstream applications such as slice-to-volume registration\nand volumetric reconstruction. Evaluation on the ReMIND dataset demonstrates\nDiffUS's ability to generate anatomically accurate ultrasound images from brain\nMRI data.", "AI": {"tldr": "DiffUS\u662f\u4e00\u4e2a\u57fa\u4e8e\u7269\u7406\u7684\u3001\u53ef\u5fae\u5206\u7684\u8d85\u58f0\u6e32\u67d3\u5668\uff0c\u53ef\u4ee5\u5c06MRI\u626b\u63cf\u8f6c\u6362\u4e3a\u903c\u771f\u7684\u8d85\u58f0\u56fe\u50cf\uff0c\u4ece\u800c\u6539\u5584\u672f\u4e2d\u5f15\u5bfc\u3002", "motivation": "\u4e3a\u4e86\u5f25\u5408\u672f\u524d\u89c4\u5212\u548c\u672f\u4e2d\u5f15\u5bfc\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u4ece\u4f53\u79ef\u6210\u50cf\u5408\u6210\u903c\u771fB\u6a21\u5f0f\u56fe\u50cf\u7684\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u672f\u4e2d\u8d85\u58f0\u6210\u50cf\u4e2d\u5b58\u5728\u7684\u566a\u58f0\u3001\u4f2a\u5f71\u4ee5\u53ca\u4e0e\u9ad8\u5206\u8fa8\u7387\u672f\u524dMRI/CT\u626b\u63cf\u5bf9\u9f50\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "DiffUS\u9996\u5148\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u5c06MRI 3D\u626b\u63cf\u8f6c\u6362\u4e3a\u58f0\u963b\u6297\u4f53\u79ef\u3002\u7136\u540e\uff0c\u5b83\u4f7f\u7528\u5c04\u7ebf\u8ffd\u8e2a\u548c\u8026\u5408\u7684\u53cd\u5c04-\u900f\u5c04\u65b9\u7a0b\u6765\u6a21\u62df\u8d85\u58f0\u675f\u4f20\u64ad\uff0c\u5c06\u6ce2\u4f20\u64ad\u6784\u5efa\u4e3a\u6355\u83b7\u591a\u4e2a\u5185\u90e8\u53cd\u5c04\u7684\u7a00\u758f\u7ebf\u6027\u7cfb\u7edf\u3002\u6700\u540e\uff0c\u901a\u8fc7\u8de8\u6247\u5f62\u91c7\u96c6\u51e0\u4f55\u7684\u6df1\u5ea6\u5206\u8fa8\u56de\u6ce2\u63d0\u53d6\u6765\u91cd\u5efaB\u6a21\u5f0f\u56fe\u50cf\uff0c\u5e76\u52a0\u5165\u6563\u6591\u566a\u58f0\u548c\u4f9d\u8d56\u4e8e\u6df1\u5ea6\u7684\u8870\u51cf\u7b49\u771f\u5b9e\u4f2a\u5f71\u3002", "result": "DiffUS\u80fd\u591f\u751f\u6210\u903c\u771f\u7684B\u6a21\u5f0f\u8d85\u58f0\u56fe\u50cf\uff0c\u5e76\u4e14\u5728ReMIND\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u8bc1\u660e\u4e86\u5176\u80fd\u591f\u4ece\u5927\u8111MRI\u6570\u636e\u751f\u6210\u89e3\u5256\u5b66\u4e0a\u51c6\u786e\u7684\u8d85\u58f0\u56fe\u50cf\u3002", "conclusion": "DiffUS\u80fd\u591f\u4ece\u5927\u8111MRI\u6570\u636e\u751f\u6210\u89e3\u5256\u5b66\u4e0a\u51c6\u786e\u7684\u8d85\u58f0\u56fe\u50cf\uff0c\u9002\u7528\u4e8e\u5207\u7247\u5230\u4f53\u79ef\u914d\u51c6\u548c\u4f53\u79ef\u91cd\u5efa\u7b49\u4e0b\u6e38\u5e94\u7528\u3002"}}
{"id": "2508.06595", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06595", "abs": "https://arxiv.org/abs/2508.06595", "authors": ["Xiaoyuan Zhu", "Muru Zhang", "Ollie Liu", "Robin Jia", "Willie Neiswanger"], "title": "LLM Unlearning Without an Expert Curated Dataset", "comment": null, "summary": "Modern large language models often encode sensitive, harmful, or copyrighted\nknowledge, raising the need for post-hoc unlearning-the ability to remove\nspecific domains of knowledge from a model without full retraining. A major\nbottleneck in current unlearning pipelines is constructing effective forget\nsets-datasets that approximate the target domain and guide the model to forget\nit. In this work, we introduce a scalable, automated approach to generate\nhigh-quality forget sets using language models themselves. Our method\nsynthesizes textbook-style data through a structured prompting pipeline,\nrequiring only a domain name as input. Through experiments on unlearning\nbiosecurity, cybersecurity, and Harry Potter novels, we show that our synthetic\ndatasets consistently outperform the baseline synthetic alternatives and are\ncomparable to the expert-curated ones. Additionally, ablation studies reveal\nthat the multi-step generation pipeline significantly boosts data diversity,\nwhich in turn improves unlearning utility. Overall, our findings suggest that\nsynthetic datasets offer a promising path toward practical, scalable unlearning\nfor a wide range of emerging domains without the need for manual intervention.\nWe release our code and dataset at\nhttps://github.com/xyzhu123/Synthetic_Textbook.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u8bed\u8a00\u6a21\u578b\u81ea\u52a8\u751f\u6210\u7528\u4e8e\u6a21\u578b\u9057\u5fd8\u7684\u6570\u636e\u96c6\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u751f\u6210\u7c7b\u4f3c\u6559\u79d1\u4e66\u7684\u6570\u636e\uff0c\u5b9e\u9a8c\u8bc1\u660e\u5176\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e14\u6613\u4e8e\u6269\u5c55\u5230\u65b0\u9886\u57df\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7ecf\u5e38\u7f16\u7801\u654f\u611f\u3001\u6709\u5bb3\u6216\u53d7\u7248\u6743\u4fdd\u62a4\u7684\u77e5\u8bc6\uff0c\u8fd9\u4f7f\u5f97\u5728\u4e0d\u5b8c\u5168\u91cd\u65b0\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u79fb\u9664\u6a21\u578b\u4e2d\u7279\u5b9a\u9886\u57df\u77e5\u8bc6\u7684\u201c\u9057\u5fd8\u201d\u80fd\u529b\u53d8\u5f97\u5fc5\u8981\u3002\u800c\u6784\u5efa\u6709\u6548\u7684\u9057\u5fd8\u96c6\u662f\u5f53\u524d\u9057\u5fd8\u6d41\u7a0b\u7684\u4e3b\u8981\u74f6\u9888\u3002", "method": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u5229\u7528\u8bed\u8a00\u6a21\u578b\u672c\u8eab\u6765\u751f\u6210\u9ad8\u8d28\u91cf\u9057\u5fd8\u96c6\u7684\uff0c\u53ef\u6269\u5c55\u7684\u3001\u81ea\u52a8\u5316\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u7ba1\u9053\u5408\u6210\u7c7b\u4f3c\u6559\u79d1\u4e66\u7684\u6570\u636e\uff0c\u4ec5\u9700\u9886\u57df\u540d\u79f0\u4f5c\u4e3a\u8f93\u5165\u3002", "result": "\u901a\u8fc7\u5728\u751f\u7269\u5b89\u5168\u3001\u7f51\u7edc\u5b89\u5168\u548c\u54c8\u5229\u6ce2\u7279\u5c0f\u8bf4\u9057\u5fd8\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u672c\u7814\u7a76\u5408\u6210\u7684\u6570\u636e\u96c6\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u5408\u6210\u66ff\u4ee3\u54c1\uff0c\u5e76\u4e14\u4e0e\u4e13\u5bb6\u7b56\u5212\u7684\u6570\u636e\u96c6\u76f8\u5f53\u3002\u6b64\u5916\uff0c\u7701\u7565\u5b9e\u9a8c\u8868\u660e\uff0c\u591a\u6b65\u9aa4\u751f\u6210\u7ba1\u9053\u663e\u8457\u63d0\u9ad8\u4e86\u6570\u636e\u591a\u6837\u6027\uff0c\u8fdb\u800c\u63d0\u9ad8\u4e86\u9057\u5fd8\u6548\u7528\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5408\u6210\u6570\u636e\u96c6\u4e3a\u5728\u65e0\u4eba\u4e3a\u5e72\u9884\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u5404\u79cd\u65b0\u5174\u9886\u57df\u8fdb\u884c\u5b9e\u7528\u3001\u53ef\u6269\u5c55\u7684\u9057\u5fd8\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.07147", "categories": ["cs.GT", "econ.TH"], "pdf": "https://arxiv.org/pdf/2508.07147", "abs": "https://arxiv.org/abs/2508.07147", "authors": ["Ivan Geffner", "Caspar Oesterheld", "Vincent Conitzer"], "title": "Maximizing Social Welfare with Side Payments", "comment": null, "summary": "We examine normal-form games in which players may \\emph{pre-commit} to\noutcome-contingent transfers before choosing their actions. In the one-shot\nversion of this model, Jackson and Wilkie showed that side contracting can\nbackfire: even a game with a Pareto-optimal Nash equilibrium can devolve into\ninefficient equilibria once unbounded, simultaneous commitments are allowed.\nThe root cause is a prisoner's dilemma effect, where each player can exploit\nher commitment power to reshape the equilibrium in her favor, harming overall\nwelfare.\n  To circumvent this problem we introduce a \\emph{staged-commitment} protocol.\nPlayers may pledge transfers only in small, capped increments over multiple\nrounds, and the phase continues only with unanimous consent. We prove that,\nstarting from any finite game $\\Gamma$ with a non-degenerate Nash equilibrium\n$\\vec{\\sigma}$, this protocol implements every welfare-maximizing payoff\nprofile that \\emph{strictly} Pareto-improves $\\vec{\\sigma}$. Thus, gradual and\nbounded commitments restore the full efficiency potential of side payments\nwhile avoiding the inefficiencies identified by Jackson and Wilkie.", "AI": {"tldr": "\u5728\u6b63\u5219\u5f0f\u535a\u5f08\u4e2d\uff0c\u9884\u5148\u627f\u8bfa\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u3002\u901a\u8fc7\u5f15\u5165\u5206\u9636\u6bb5\u3001\u6709\u4e0a\u9650\u4e14\u9700\u4e00\u81f4\u540c\u610f\u7684\u627f\u8bfa\u534f\u8bae\uff0c\u53ef\u4ee5\u5b9e\u73b0\u798f\u5229\u6700\u5927\u5316\u76ee\u6807\uff0c\u5e76\u89c4\u907f\u6b64\u7c7b\u95ee\u9898\u3002", "motivation": "\u7814\u7a76\u4e86\u5728\u6b63\u5219\u5f0f\u535a\u5f08\u4e2d\uff0c\u73a9\u5bb6\u9884\u5148\u627f\u8bfa\u6839\u636e\u7ed3\u679c\u652f\u4ed8\uff0c\u4ee5\u53ca\u8fd9\u79cd\u627f\u8bfa\u53ef\u80fd\u5bfc\u81f4\u6548\u7387\u4f4e\u4e0b\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5f53\u627f\u8bfa\u80fd\u529b\u88ab\u6ee5\u7528\u4ee5\u81f3\u4e8e\u51fa\u73b0\u201c\u56da\u5f92\u56f0\u5883\u201d\u6548\u5e94\u65f6\u3002", "method": "\u63d0\u51fa\u5e76\u8bc1\u660e\u4e86\u4e00\u79cd\u5206\u9636\u6bb5\u627f\u8bfa\u534f\u8bae\u7684\u6709\u6548\u6027\uff0c\u8be5\u534f\u8bae\u5141\u8bb8\u73a9\u5bb6\u5728\u591a\u8f6e\u4e2d\u4ee5\u5206\u671f\u3001\u6709\u4e0a\u9650\u7684\u65b9\u5f0f\u8fdb\u884c\u627f\u8bfa\uff0c\u5e76\u901a\u8fc7\u6240\u6709\u73a9\u5bb6\u7684\u4e00\u81f4\u540c\u610f\u6765\u63a8\u8fdb\u3002", "result": "\u8bc1\u660e\u4e86\u5206\u9636\u6bb5\u627f\u8bfa\u534f\u8bae\u80fd\u591f\u5b9e\u73b0\u6240\u6709\u80fd\u4e25\u683c\u6539\u8fdb\u73b0\u6709\u7eb3\u4ec0\u5747\u8861\u7684\u798f\u5229\u6700\u5927\u5316\u652f\u4ed8\u7ec4\u5408\uff0c\u4ece\u800c\u6062\u590d\u4e86\u652f\u4ed8\u80fd\u529b\u7684\u6240\u6709\u6548\u7387\u6f5c\u529b\uff0c\u5e76\u907f\u514d\u4e86\u6548\u7387\u4f4e\u4e0b\u95ee\u9898\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u5206\u9636\u6bb5\u627f\u8bfa\u534f\u8bae\uff0c\u5373\u73a9\u5bb6\u53ef\u4ee5\u5728\u591a\u8f6e\u4e2d\u4ee5\u5c0f\u989d\u3001\u6709\u4e0a\u9650\u7684\u589e\u91cf\u8fdb\u884c\u8f6c\u8ba9\uff0c\u5e76\u4e14\u53ea\u6709\u5728\u6240\u6709\u73a9\u5bb6\u4e00\u81f4\u540c\u610f\u7684\u60c5\u51b5\u4e0b\u624d\u80fd\u7ee7\u7eed\u8fdb\u884c\uff0c\u53ef\u4ee5\u89c4\u907f\u201c\u627f\u8bfa\u540e\u987e\u4e4b\u5fe7\u201d\u95ee\u9898\u3002\u8be5\u534f\u8bae\u53ef\u4ee5\u5b9e\u73b0\u6240\u6709\u80fd\u4e25\u683c\u6539\u8fdb\u73b0\u6709\u7eb3\u4ec0\u5747\u8861\u7684\u798f\u5229\u6700\u5927\u5316\u652f\u4ed8\u7ec4\u5408\uff0c\u4ece\u800c\u907f\u514d\u4e86\u6770\u514b\u900a\u548c\u5a01\u5c14\u57fa\u6240\u6307\u51fa\u7684\u4f4e\u6548\u95ee\u9898\u3002"}}
{"id": "2508.08091", "categories": ["cs.NE", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08091", "abs": "https://arxiv.org/abs/2508.08091", "authors": ["Matias Barandiaran", "James Stovold"], "title": "Growing Reservoirs with Developmental Graph Cellular Automata", "comment": "Accepted to ALIFE 2025", "summary": "Developmental Graph Cellular Automata (DGCA) are a novel model for\nmorphogenesis, capable of growing directed graphs from single-node seeds. In\nthis paper, we show that DGCAs can be trained to grow reservoirs. Reservoirs\nare grown with two types of targets: task-driven (using the NARMA family of\ntasks) and task-independent (using reservoir metrics).\n  Results show that DGCAs are able to grow into a variety of specialized,\nlife-like structures capable of effectively solving benchmark tasks,\nstatistically outperforming `typical' reservoirs on the same task. Overall,\nthese lay the foundation for the development of DGCA systems that produce\nplastic reservoirs and for modeling functional, adaptive morphogenesis.", "AI": {"tldr": "DGCA\u6a21\u578b\u53ef\u4ee5\u8bad\u7ec3\u751f\u6210\u529f\u80fd\u6027\u50a8\u5c42\uff0c\u5e76\u5728\u89e3\u51b3NARMA\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u5178\u578b\u50a8\u5c42\u3002", "motivation": "\u63a2\u7d22DGCA\u6a21\u578b\u5728\u751f\u6210\u529f\u80fd\u6027\u3001\u81ea\u9002\u5e94\u5f62\u6001\u53d1\u751f\u548c\u53ef\u5851\u6027\u50a8\u5c42\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "DGCA\u6a21\u578b\u901a\u8fc7\u7aef\u5230\u7aef\u8bad\u7ec3\u751f\u6210\u50a8\u5c42\uff0c\u5e76\u4f7f\u7528NARMA\u4efb\u52a1\u7cfb\u5217\u548c\u50a8\u5c42\u6307\u6807\u4f5c\u4e3a\u76ee\u6807\u3002", "result": "DGCA\u6a21\u578b\u80fd\u591f\u751f\u6210\u4e13\u95e8\u5316\u7684\u3001\u751f\u547d\u5f62\u5f0f\u7684\u7ed3\u6784\uff0c\u5728\u89e3\u51b3\u57fa\u51c6\u4efb\u52a1\u65f6\u8868\u73b0\u4f18\u4e8e\u5178\u578b\u7684\u50a8\u5c42\u3002", "conclusion": "DGCA\u6a21\u578b\u4e3a\u5f62\u6001\u53d1\u751f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u53ef\u5851\u7684\u3001\u80fd\u591f\u89e3\u51b3\u57fa\u51c6\u4efb\u52a1\u7684\u3001\u5177\u6709\u7edf\u8ba1\u5b66\u4f18\u52bf\u7684\u201c\u751f\u547d\u822c\u201d\u7684\u50a8\u5c42\u3002"}}
{"id": "2508.07579", "categories": ["cs.SI", "cs.CY", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07579", "abs": "https://arxiv.org/abs/2508.07579", "authors": ["Ziqi Pan", "Runhua Zhang", "Jiehui Luo", "Yuanhao Zhang", "Yue Deng", "Xiaojuan Ma"], "title": "From Platform Migration to Cultural Integration: the Ingress and Diffusion of #wlw from TikTok to RedNote in Queer Women", "comment": null, "summary": "Hashtags serve as identity markers and connection tools in online queer\ncommunities. Recently, the Western-origin #wlw (women-loving-women) hashtag has\nrisen in the Chinese lesbian community on RedNote, coinciding with user\nmigration triggered by the temporary US TikTok ban. This event provides a\nunique lens to study cross-cultural hashtag ingress and diffusion through the\npopulations' responsive behaviors in cyber-migration. In this paper, we\nconducted a two-phase content analysis of 418 #wlw posts from January and\nApril, examining different usage patterns during the hashtag's ingress and\ndiffusion. Results indicate that the successful introduction of #wlw was\nfacilitated by TikTok immigrants' bold importation, both populations' mutual\ninterpretation, and RedNote natives' discussions. In current manifestation of\ndiffusion, #wlw becomes a RedNote-recognized queer hashtag for sharing queer\nlife, and semantically expands to support feminism discourse. Our findings\nprovide empirical insights for enhancing the marginalized communities'\ncross-cultural communication.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u201c#wlw\u201d\u6807\u7b7e\u5728\u4e2d\u56fd\u5973\u540c\u6027\u604b\u793e\u7fa4\u7684\u5f15\u5165\u548c\u4f20\u64ad\u8fc7\u7a0b\uff0c\u53d1\u73b0\u8be5\u6807\u7b7e\u7684\u6210\u529f\u4f20\u64ad\u5f97\u76ca\u4e8e\u7528\u6237\u8fc1\u79fb\u548c\u8de8\u6587\u5316\u4e92\u52a8\uff0c\u5e76\u6700\u7ec8\u6210\u4e3a\u5206\u4eab\u9177\u513f\u751f\u6d3b\u548c\u652f\u6301\u5973\u6743\u4e3b\u4e49\u8bdd\u8bed\u7684\u91cd\u8981\u6807\u7b7e\u3002", "motivation": "\u7814\u7a76\u7f8e\u56fdTikTok\u4e34\u65f6\u7981\u4ee4\u5f15\u53d1\u7684\u7528\u6237\u8fc1\u79fb\u4e8b\u4ef6\uff0c\u63a2\u8ba8\u8de8\u6587\u5316\u6807\u7b7e\u7684\u5f15\u5165\u548c\u4f20\u64ad\uff0c\u4ee5\u53ca\u7f51\u7edc\u79fb\u6c11\u4e2d\u7684\u4eba\u7fa4\u54cd\u5e94\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u5bf92022\u5e741\u6708\u548c4\u6708418\u4e2a#wlw\u5e16\u5b50\u7684\u4e24\u9636\u6bb5\u5185\u5bb9\u5206\u6790\uff0c\u7814\u7a76\u4e86\u6807\u7b7e\u5f15\u5165\u548c\u4f20\u64ad\u671f\u95f4\u7684\u4e0d\u540c\u4f7f\u7528\u6a21\u5f0f\u3002", "result": "\u201c#wlw\u201d\u6807\u7b7e\u7684\u5f15\u5165\u5f97\u76ca\u4e8eTikTok\u79fb\u6c11\u7684\u5927\u80c6\u5f15\u5165\u3001\u4e24\u4e2a\u7fa4\u4f53\u95f4\u7684\u76f8\u4e92\u89e3\u8bfb\u4ee5\u53caRedNote\u7528\u6237\u7684\u8ba8\u8bba\u3002\u5728\u4f20\u64ad\u8fc7\u7a0b\u4e2d\uff0c\u201c#wlw\u201d\u5df2\u6210\u4e3aRedNote\u4e0a\u516c\u8ba4\u7684\u5206\u4eab\u9177\u513f\u751f\u6d3b\u7684\u9177\u513f\u6807\u7b7e\uff0c\u5e76\u5728\u8bed\u4e49\u4e0a\u6269\u5c55\u5230\u652f\u6301\u5973\u6743\u4e3b\u4e49\u8bdd\u8bed\u3002", "conclusion": "\u201c#wlw\u201d\u6807\u7b7e\u6210\u529f\u5f15\u5165\u5e76\u4f20\u64ad\uff0c\u6210\u4e3aRedNote\u4e0a\u7528\u4e8e\u5206\u4eab\u9177\u513f\u751f\u6d3b\u548c\u652f\u6301\u5973\u6743\u4e3b\u4e49\u8bdd\u8bed\u7684\u9177\u513f\u6807\u7b7e\uff0c\u4e3a\u8fb9\u7f18\u5316\u793e\u7fa4\u7684\u8de8\u6587\u5316\u4ea4\u6d41\u63d0\u4f9b\u4e86\u7ecf\u9a8c\u89c1\u89e3\u3002"}}
{"id": "2508.06529", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06529", "abs": "https://arxiv.org/abs/2508.06529", "authors": ["Jiayuan Wang", "Q. M. Jonathan Wu", "Katsuya Suto", "Ning Zhang"], "title": "RMT-PPAD: Real-time Multi-task Learning for Panoptic Perception in Autonomous Driving", "comment": null, "summary": "Autonomous driving systems rely on panoptic driving perception that requires\nboth precision and real-time performance. In this work, we propose RMT-PPAD, a\nreal-time, transformer-based multi-task model that jointly performs object\ndetection, drivable area segmentation, and lane line segmentation. We introduce\na lightweight module, a gate control with an adapter to adaptively fuse shared\nand task-specific features, effectively alleviating negative transfer between\ntasks. Additionally, we design an adaptive segmentation decoder to learn the\nweights over multi-scale features automatically during the training stage. This\navoids the manual design of task-specific structures for different segmentation\ntasks. We also identify and resolve the inconsistency between training and\ntesting labels in lane line segmentation. This allows fairer evaluation.\nExperiments on the BDD100K dataset demonstrate that RMT-PPAD achieves\nstate-of-the-art results with mAP50 of 84.9% and Recall of 95.4% for object\ndetection, mIoU of 92.6% for drivable area segmentation, and IoU of 56.8% and\naccuracy of 84.7% for lane line segmentation. The inference speed reaches 32.6\nFPS. Moreover, we introduce real-world scenarios to evaluate RMT-PPAD\nperformance in practice. The results show that RMT-PPAD consistently delivers\nstable performance. The source codes and pre-trained models are released at\nhttps://github.com/JiayuanWang-JW/RMT-PPAD.", "AI": {"tldr": "RMT-PPAD\u662f\u4e00\u4e2a\u5b9e\u65f6Transformer\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u7528\u4e8e\u8054\u5408\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b\u3001\u53ef\u884c\u9a76\u533a\u57df\u5206\u5272\u548c\u8f66\u9053\u7ebf\u5206\u5272\uff0c\u63d0\u9ad8\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u5b9e\u65f6\u6027\u80fd\u7684\u5168\u65b9\u4f4d\u9a7e\u9a76\u611f\u77e5\uff0c\u4ee5\u6ee1\u8db3\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRMT-PPAD\u7684\u57fa\u4e8eTransformer\u7684\u5b9e\u65f6\u591a\u4efb\u52a1\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8054\u5408\u6267\u884c\u76ee\u6807\u68c0\u6d4b\u3001\u53ef\u884c\u9a76\u533a\u57df\u5206\u5272\u548c\u8f66\u9053\u7ebf\u5206\u5272\u3002\u6a21\u578b\u5305\u542b\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6a21\u5757\uff08\u95e8\u63a7\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\uff09\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u5730\u878d\u5408\u5171\u4eab\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u7279\u5f81\uff0c\u4ee5\u51cf\u8f7b\u4efb\u52a1\u95f4\u7684\u8d1f\u8fc1\u79fb\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u5206\u5272\u89e3\u7801\u5668\uff0c\u80fd\u591f\u81ea\u52a8\u5b66\u4e60\u591a\u5c3a\u5ea6\u7279\u5f81\u4e0a\u7684\u6743\u91cd\uff0c\u907f\u514d\u4e86\u624b\u52a8\u8bbe\u8ba1\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u7ed3\u6784\u3002\u540c\u65f6\uff0c\u89e3\u51b3\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6807\u7b7e\u5728\u8f66\u9053\u7ebf\u5206\u5272\u4e2d\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u4ee5\u5b9e\u73b0\u66f4\u516c\u5e73\u7684\u8bc4\u4f30\u3002", "result": "\u5728BDD100K\u6570\u636e\u96c6\u4e0a\uff0cRMT-PPAD\u5728\u76ee\u6807\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e8684.9%\u7684mAP50\u548c95.4%\u7684\u53ec\u56de\u7387\uff1b\u5728\u53ef\u884c\u9a76\u533a\u57df\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e8692.6%\u7684mIoU\uff1b\u5728\u8f66\u9053\u7ebf\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e8656.8%\u7684IoU\u548c84.7%\u7684\u51c6\u786e\u7387\u3002\u63a8\u7406\u901f\u5ea6\u8fbe\u5230\u4e8632.6 FPS\u3002\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u8bc4\u4f30\u4e2d\uff0cRMT-PPAD\u8868\u73b0\u51fa\u7a33\u5b9a\u7684\u6027\u80fd\u3002", "conclusion": "RMT-PPAD\u5728BDD100K\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8868\u73b0\u7a33\u5b9a\uff0c\u540c\u65f6\u63a8\u7406\u901f\u5ea6\u8fbe\u523032.6 FPS\u3002"}}
{"id": "2508.06987", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06987", "abs": "https://arxiv.org/abs/2508.06987", "authors": ["Yiwei Liu", "Ziming Wang", "Xin Wang", "Yiding Ji"], "title": "Fixed-Time Voltage Regulation for Boost Converters via Unit-Safe Saturating Functions", "comment": null, "summary": "This paper explores the voltage regulation challenges in boost converter\nsystems, which are critical components in power electronics due to their\nability to step up voltage levels efficiently. The proposed control algorithm\nensures fixed-time stability, a desirable property that guarantees system\nstability within a fixed time frame regardless of initial conditions. To tackle\nthe common chattering issues in conventional fixed-time control methods, a\nnovel class of function families is introduced. State observers and adaptive\nparameters are utilized to manage the uncertainties associated with unknown\nload resistance. Furthermore, a new disturbance observer is developed using the\nproposed function family, and its advantages and limitations are illustrated\nthrough comparison with existing designs. Finally, both non-real-time and\nreal-time simulations are conducted to validate the effectiveness and\ndeployability of the proposed control algorithm.", "AI": {"tldr": "This paper presents a new control algorithm for boost converters that achieves fixed-time stability, reduces chattering using novel functions, and handles unknown loads with observers and adaptive parameters, proven effective in simulations.", "motivation": "The motivation is to address voltage regulation challenges in boost converter systems and improve the performance of fixed-time control methods by mitigating chattering issues and handling uncertainties like unknown load resistance.", "method": "The paper proposes a novel control algorithm for boost converter systems that ensures fixed-time stability. It introduces a new class of function families to mitigate chattering issues common in existing fixed-time control methods. The approach utilizes state observers and adaptive parameters to handle uncertainties from unknown load resistance. Additionally, a new disturbance observer is developed based on the proposed function family.", "result": "The proposed control algorithm demonstrates effectiveness and deployability through both non-real-time and real-time simulations, validating its ability to ensure fixed-time stability and manage system uncertainties.", "conclusion": "The proposed control algorithm effectively addresses voltage regulation challenges in boost converter systems by ensuring fixed-time stability and managing uncertainties. The novel function families and disturbance observer overcome chattering issues and handle unknown load resistance, as validated by simulations."}}
{"id": "2508.07200", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07200", "abs": "https://arxiv.org/abs/2508.07200", "authors": ["Junji Fujimoto", "Yuki Izaki", "Yuki Fuseya"], "title": "Magnetic Moment vs Angular Momentum: Spin Hall Response in Bismuth", "comment": "5 pages, 3 figures", "summary": "Spin currents can carry either spin angular momentum or its associated\nmagnetic moment, which are no longer strictly proportional in multiband\nsystems. Using a multiband $k \\cdot p$ model, we compute the intrinsic spin\nHall conductivity tensors of elemental Bi. The magnetic-moment tensor emerges\nabout two orders of magnitude larger and far less anisotropic than the\nangular-momentum tensor, while quasiparticle damping activates otherwise\nlongitudinal components. The magnetic-moment spin Hall angle exceeds unity,\ndemonstrating that a clear distinction between the two currents is\nindispensable for multiband systems.", "AI": {"tldr": "In multiband systems like Bi, spin currents carrying angular momentum and magnetic moments behave differently. The magnetic moment's spin Hall effect is much stronger than the angular momentum's and can exceed unity, proving distinct treatments are crucial.", "motivation": "Investigating the proportionality between spin angular momentum and magnetic moment in spin currents within multiband systems.", "method": "Using a multiband k\u00b7p model to compute the intrinsic spin Hall conductivity tensors of elemental Bi.", "result": "The magnetic-moment tensor is significantly larger and less anisotropic than the angular-momentum tensor. Quasiparticle damping activates longitudinal components. The magnetic-moment spin Hall angle exceeds unity.", "conclusion": "spin currents in multiband systems are not proportional, and the magnetic-moment spin Hall angle can exceed unity, showing the necessity of distinguishing between spin currents in these systems."}}
{"id": "2508.06958", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06958", "abs": "https://arxiv.org/abs/2508.06958", "authors": ["Xin Cheng", "Guangjie Han", "Menglu Li", "Ruoguang Li", "Feng Shu"], "title": "Millimeter-Wave Position Sensing Using Reconfigurable Intelligent Surfaces: Positioning Error Bound and Phase Shift Configuration", "comment": null, "summary": "Millimeter-wave (mmWave) positioning has emerged as a promising technology\nfor next-generation intelligent systems. The advent of reconfigurable\nintelligent surfaces (RISs) has revolutionized high-precision mmWave\nlocalization by enabling dynamic manipulation of wireless propagation\nenvironments. This paper investigates a three-dimensional (3D) multi-input\nsingle-output (MISO) mmWave positioning system assisted by multiple RISs. We\nintroduce a measurement framework incorporating sequential RIS activation and\ndirectional beamforming to fully exploit virtual line-of-sight (VLoS) paths.\nThe theoretical performance limits are rigorously analyzed through derivation\nof the Fisher information and subsequent positioning error bound (PEB). To\nminimize the PEB, two distinct optimization approaches are proposed for\ncontinuous and discrete phase shift configurations of RISs. For continuous\nphase shifts, a Riemannian manifold-based optimization algorithm is proposed.\nFor discrete phase shifts, a heuristic algorithm incorporating the grey wolf\noptimizer is proposed. Extensive numerical simulations demonstrate the\neffectiveness of the proposed algorithms in reducing the PEB and validate the\nimprovement in positioning accuracy achieved by multiple RISs.", "AI": {"tldr": "\u672c\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u7ef4MISO\u6beb\u7c73\u6ce2\u5b9a\u4f4d\u7cfb\u7edf\uff0c\u5229\u7528\u591aRIS\u548c\u4f18\u5316\u7684\u76f8\u4f4d\u79fb\u6765\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5728\u9ad8\u7cbe\u5ea6\u6beb\u7c73\u6ce2\u5b9a\u4f4d\u4e2d\u5b9e\u73b0\u667a\u80fd\u7cfb\u7edf\uff0c\u5e76\u5229\u7528\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u52a8\u6001\u64cd\u63a7\u65e0\u7ebf\u4f20\u64ad\u73af\u5883\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u987a\u5e8fRIS\u6fc0\u6d3b\u548c\u5b9a\u5411\u6ce2\u675f\u6210\u5f62\u7684\u6d4b\u91cf\u6846\u67b6\uff0c\u5e76\u63a8\u5bfc\u4e86\u8d39\u5e0c\u5c14\u4fe1\u606f\u548c\u6d4bpos\u8bef\u5dee\u754c\u9650\uff08PEB\uff09\u3002\u9488\u5bf9\u8fde\u7eed\u548c\u79bb\u6563\u76f8\u4f4d\u79fb\u914d\u7f6e\uff0c\u5206\u522b\u63d0\u51fa\u4e86\u57fa\u4e8e\u9ece\u66fc\u6d41\u5f62\u548c\u7070\u72fc\u4f18\u5316\u542f\u53d1\u5f0f\u7b97\u6cd5\u7684\u4f18\u5316\u65b9\u6cd5\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u6709\u6548\u964d\u4f4ePEB\uff0c\u4e14\u591aRIS\u8f85\u52a9\u7cfb\u7edf\u80fd\u663e\u8457\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "conclusion": "\u672c\u8bba\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u6d41\u5f62\u4f18\u5316\u7684\u7b97\u6cd5\u548c\u57fa\u4e8e\u7070\u72fc\u4f18\u5316\u7684\u7b97\u6cd5\u5728\u964d\u4f4e\u6d4b\u4f4d\u8bef\u5dee\u754c\u9650\u548c\u63d0\u9ad8\u5b9a\u4f4d\u7cbe\u5ea6\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u8bc1\u660e\u4e86\u591aRIS\u8f85\u52a9\u4e0b\u7684\u4e09\u7ef4MISO\u6beb\u7c73\u6ce2\u5b9a\u4f4d\u7cfb\u7edf\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.06716", "categories": ["cs.AI", "cs.LG", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.06716", "abs": "https://arxiv.org/abs/2508.06716", "authors": ["Blair Johnson", "Clayton Kerce", "Faramarz Fekri"], "title": "GLIDR: Graph-Like Inductive Logic Programming with Differentiable Reasoning", "comment": null, "summary": "Differentiable inductive logic programming (ILP) techniques have proven\neffective at finding approximate rule-based solutions to link prediction and\nnode classification problems on knowledge graphs; however, the common\nassumption of chain-like rule structure can hamper the performance and\ninterpretability of existing approaches. We introduce GLIDR, a differentiable\nrule learning method that models the inference of logic rules with more\nexpressive syntax than previous methods. GLIDR uses a differentiable message\npassing inference algorithm that generalizes previous chain-like rule learning\nmethods to allow rules with features like branches and cycles. GLIDR has a\nsimple and expressive rule search space which is parameterized by a limit on\nthe maximum number of free variables that may be included in a rule. Explicit\nlogic rules can be extracted from the weights of a GLIDR model for use with\nsymbolic solvers. We demonstrate that GLIDR can significantly outperform\nexisting rule learning methods on knowledge graph completion tasks and even\ncompete with embedding methods despite the inherent disadvantage of being a\nstructure-only prediction method. We show that rules extracted from GLIDR\nretain significant predictive performance, and that GLIDR is highly robust to\ntraining data noise. Finally, we demonstrate that GLIDR can be chained with\ndeep neural networks and optimized end-to-end for rule learning on arbitrary\ndata modalities.", "AI": {"tldr": "GLIDR is a new differentiable ILP method that uses a message passing algorithm to learn logic rules with more complex structures (branches, cycles) than previous methods. It outperforms existing methods on knowledge graph completion, is robust to noise, and can be integrated with deep neural networks.", "motivation": "Existing differentiable ILP techniques assume chain-like rule structures, which can hinder performance and interpretability. GLIDR addresses this by modeling logic rule inference with more expressive syntax.", "method": "GLIDR uses a differentiable message passing inference algorithm that generalizes previous chain-like rule learning methods to allow rules with features like branches and cycles. GLIDR has a simple and expressive rule search space which is parameterized by a limit on the maximum number of free variables that may be included in a rule.", "result": "GLIDR significantly outperforms existing rule learning methods on knowledge graph completion tasks and competes with embedding methods, despite being a structure-only prediction method.", "conclusion": "GLIDR can be chained with deep neural networks and optimized end-to-end for rule learning on arbitrary data modalities. Rules extracted from GLIDR retain significant predictive performance, and GLIDR is highly robust to training data noise."}}
{"id": "2508.07783", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.07783", "abs": "https://arxiv.org/abs/2508.07783", "authors": ["Yotam Kenneth-Mordoch", "Robert Krauthgamer"], "title": "Simple Algorithms for Fully Dynamic Edge Connectivity", "comment": null, "summary": "In the fully dynamic edge connectivity problem, the input is a simple graph\n$G$ undergoing edge insertions and deletions, and the goal is to maintain its\nedge connectivity, denoted $\\lambda_G$. We present two simple randomized\nalgorithms solving this problem. The first algorithm maintains the edge\nconnectivity in worst-case update time $\\tilde{O}(n)$ per edge update, matching\nthe known bound but with simpler analysis. Our second algorithm achieves\nworst-case update time $\\tilde{O}(n/\\lambda_G)$ and worst-case query time\n$\\tilde{O}(n^2/\\lambda_G^2)$, which is the first algorithm with worst-case\nupdate and query time $o(n)$ for large edge connectivity, namely, $\\lambda_G =\n\\omega(\\sqrt{n})$.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u89e3\u51b3\u52a8\u6001\u56fe\u8fb9\u8fde\u901a\u6027\u95ee\u9898\u7684\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u5176\u4e2d\u4e00\u79cd\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u80fd\u5b9e\u73b0\u66f4\u4f18\u7684\u66f4\u65b0\u4e0e\u67e5\u8be2\u65f6\u95f4\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u56fe\u7684\u8fb9\u63d2\u5165\u548c\u5220\u9664\u64cd\u4f5c\u4e0b\uff0c\u7ef4\u62a4\u56fe\u7684\u8fb9\u8fde\u901a\u6027\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u7b2c\u4e00\u79cd\u7b97\u6cd5\u7684\u66f4\u65b0\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a~O(n)\uff0c\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u7684\u66f4\u65b0\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a~O(n/\u03bbG)\uff0c\u67e5\u8be2\u65f6\u95f4\u590d\u6742\u5ea6\u4e3a~O(n\u00b2/\u03bbG\u00b2)\u3002", "result": "\u7b2c\u4e00\u79cd\u7b97\u6cd5\u7684\u66f4\u65b0\u65f6\u95f4\u4e0e\u5df2\u77e5\u754c\u5339\u914d\u4f46\u5206\u6790\u66f4\u7b80\u5355\uff1b\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u5728\u03bbG = \u03c9(\u221an)\u65f6\uff0c\u66f4\u65b0\u548c\u67e5\u8be2\u65f6\u95f4\u590d\u6742\u5ea6\u8fbe\u5230o(n)\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u89e3\u51b3\u5168\u52a8\u6001\u56fe\u8fb9\u8fde\u901a\u6027\u95ee\u9898\u7684\u968f\u673a\u5316\u7b97\u6cd5\uff0c\u5176\u4e2d\u7b2c\u4e8c\u79cd\u7b97\u6cd5\u5728\u8fb9\u8fde\u901a\u5ea6\u8f83\u5927\u65f6\uff08\u03bbG = \u03c9(\u221an)\uff09\u5b9e\u73b0\u4e86o(n)\u7684\u66f4\u65b0\u4e0e\u67e5\u8be2\u65f6\u95f4\u3002"}}
{"id": "2508.06591", "categories": ["cs.LG", "cond-mat.dis-nn", "cond-mat.mtrl-sci", "cond-mat.other", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06591", "abs": "https://arxiv.org/abs/2508.06591", "authors": ["Rachel K. Luu", "Jingyu Deng", "Mohammed Shahrudin Ibrahim", "Nam-Joon Cho", "Ming Dao", "Subra Suresh", "Markus J. Buehler"], "title": "Generative Artificial Intelligence Extracts Structure-Function Relationships from Plants for New Materials", "comment": null, "summary": "Large language models (LLMs) have reshaped the research landscape by enabling\nnew approaches to knowledge retrieval and creative ideation. Yet their\napplication in discipline-specific experimental science, particularly in highly\nmulti-disciplinary domains like materials science, remains limited. We present\na first-of-its-kind framework that integrates generative AI with literature\nfrom hitherto-unconnected fields such as plant science, biomimetics, and\nmaterials engineering to extract insights and design experiments for materials.\nWe focus on humidity-responsive systems such as pollen-based materials and\nRhapis excelsa (broadleaf lady palm) leaves, which exhibit self-actuation and\nadaptive performance. Using a suite of AI tools, including a fine-tuned model\n(BioinspiredLLM), Retrieval-Augmented Generation (RAG), agentic systems, and a\nHierarchical Sampling strategy, we extract structure-property relationships and\ntranslate them into new classes of bioinspired materials. Structured inference\nprotocols generate and evaluate hundreds of hypotheses from a single query,\nsurfacing novel and experimentally tractable ideas. We validate our approach\nthrough real-world implementation: LLM-generated procedures, materials designs,\nand mechanical predictions were tested in the laboratory, culminating in the\nfabrication of a novel pollen-based adhesive with tunable morphology and\nmeasured shear strength, establishing a foundation for future plant-derived\nadhesive design. This work demonstrates how AI-assisted ideation can drive\nreal-world materials design and enable effective human-AI collaboration.", "AI": {"tldr": "\u901a\u8fc7\u6574\u5408\u4e0d\u540c\u5b66\u79d1\u7684\u6587\u732e\u548c\u5148\u8fdb\u7684AI\u5de5\u5177\uff08\u5982BioinspiredLLM\u3001RAG\u548c\u4ee3\u7406\u7cfb\u7edf\uff09\uff0c\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u4ece\u690d\u7269\u79d1\u5b66\u548c\u4eff\u751f\u5b66\u4e2d\u63d0\u53d6\u89c1\u89e3\uff0c\u4ee5\u8bbe\u8ba1\u548c\u5236\u9020\u65b0\u578b\u6750\u6599\uff0c\u5982\u53ef\u8c03\u7684\u3001\u57fa\u4e8e\u82b1\u7c89\u7684\u7c98\u5408\u5242\uff0c\u4ece\u800c\u5c55\u793a\u4e86AI\u5728\u6750\u6599\u8bbe\u8ba1\u548c\u4eba\u673a\u534f\u4f5c\u4e2d\u7684\u6f5c\u529b\u3002", "motivation": "LLM\u5728\u5b66\u79d1\u7279\u5b9a\u5b9e\u9a8c\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u4ecd\u7136\u6709\u9650\uff0c\u7279\u522b\u662f\u5728\u6750\u6599\u79d1\u5b66\u7b49\u9ad8\u5ea6\u8de8\u5b66\u79d1\u9886\u57df\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5229\u7528\u751f\u6210\u5f0fAI\u4ece\u4e0d\u540c\u5b66\u79d1\u7684\u6587\u732e\u4e2d\u63d0\u53d6\u89c1\u89e3\u5e76\u8bbe\u8ba1\u6750\u6599\u3002", "method": "\u5229\u7528\u5305\u62ec\u5fae\u8c03\u6a21\u578b\uff08BioinspiredLLM\uff09\u3001\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u3001\u4ee3\u7406\u7cfb\u7edf\u548c\u5206\u5c42\u91c7\u6837\u7b56\u7565\u5728\u5185\u7684\u4e00\u5957AI\u5de5\u5177\uff0c\u6574\u5408\u4e86\u690d\u7269\u79d1\u5b66\u3001\u4eff\u751f\u5b66\u548c\u6750\u6599\u5de5\u7a0b\u7b49\u4ee5\u5f80\u672a\u8fde\u63a5\u9886\u57df\u7684\u6587\u732e\uff0c\u4ee5\u63d0\u53d6\u7ed3\u6784-\u5c5e\u6027\u5173\u7cfb\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u65b0\u578b\u4eff\u751f\u6750\u6599\u3002\u7ed3\u6784\u5316\u63a8\u7406\u534f\u8bae\u4ece\u5355\u4e2a\u67e5\u8be2\u4e2d\u751f\u6210\u548c\u8bc4\u4f30\u6570\u767e\u4e2a\u5047\u8bbe\u3002", "result": "\u4ece\u6587\u732e\u4e2d\u63d0\u53d6\u4e86\u7ed3\u6784-\u5c5e\u6027\u5173\u7cfb\uff0c\u5e76\u5c06\u5176\u8f6c\u5316\u4e3a\u65b0\u578b\u4eff\u751f\u6750\u6599\u3002AI\u751f\u6210\u7684\u7a0b\u5e8f\u3001\u6750\u6599\u8bbe\u8ba1\u548c\u673a\u68b0\u9884\u6d4b\u5728\u5b9e\u9a8c\u5ba4\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u6210\u529f\u5236\u9020\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u82b1\u7c89\u7684\u3001\u5177\u6709\u53ef\u8c03\u5f62\u6001\u548c\u6d4b\u91cf\u526a\u5207\u5f3a\u5ea6\u7684\u7c98\u5408\u5242\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u5c55\u793a\u4e86AI\u8f85\u52a9\u7684\u6784\u601d\u5982\u4f55\u63a8\u52a8\u73b0\u5b9e\u4e16\u754c\u6750\u6599\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u6709\u6548\u7684\u4eba\u673a\u534f\u4f5c\uff0c\u4f8b\u5982\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u57fa\u4e8e\u82b1\u7c89\u7684\u3001\u5177\u6709\u53ef\u8c03\u5f62\u6001\u548c\u53ef\u6d4b\u91cf\u526a\u5207\u5f3a\u5ea6\u7684\u7c98\u5408\u5242\uff0c\u4e3a\u672a\u6765\u690d\u7269\u6e90\u7c98\u5408\u5242\u7684\u8bbe\u8ba1\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06882", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06882", "abs": "https://arxiv.org/abs/2508.06882", "authors": ["Yu-Jie Cen", "Sandro Wieser", "Georg K. H. Madsen", "Jes\u00fas Carrete"], "title": "Ab-initio heat transport in defect-laden quasi-1D systems from a symmetry-adapted perspective", "comment": null, "summary": "Due to their aspect ratio and wide range of thermal conductivities, nanotubes\nhold significant promise as heat-management nanocomponents. Their practical use\nis, however, often limited by thermal resistance introduced by structural\ndefects or material interfaces. An intriguing question is the role that\nstructural symmetry plays in thermal transport through those defect-laden\nsections. To address this, we develop a framework that combines representation\ntheory with the mode-resolved Green's function method, enabling a detailed,\nsymmetry-resolved analysis of phonon transmission through defected segments of\nquasi-1D systems. To avoid artifacts inherent to formalisms developed for bulk\n3D systems, we base our analysis on line groups, the appropriate description of\nthe symmetries of quasi-1D structures. This categorization introduces\nadditional quantum numbers that partition the phonon branches into smaller,\nsymmetry-distinct subsets, enabling clearer mode classification. We employ an\nAllegro-based machine learning potential to obtain the force constants and\nphonons with near-ab-initio accuracy. We calculate detailed phonon transmission\nprofiles for single- and multi-layer MoS$_\\mathrm{2}$-WS$_\\mathrm{2}$ nanotubes\nand connect the transmission probability of each mode to structural symmetry.\nSurprisingly, we find that pronounced symmetry breaking can suppress scattering\nby relaxing selection rules and opening additional transmission channels. That\nhigher disorder introduced through defects can enhance thermal transport, and\nnot just suppress it, demonstrates the critical role of symmetry in deciphering\nthe nuances of nanoscale thermal transport.", "AI": {"tldr": "\u7814\u7a76\u7eb3\u7c73\u7ba1\u4e2d\u7684\u58f0\u5b50\u4f20\u8f93\uff0c\u53d1\u73b0\u5bf9\u79f0\u6027\u7834\u7f3a\u80fd\u589e\u5f3a\u70ed\u4f20\u8f93\uff0c\u5f3a\u8c03\u4e86\u5bf9\u79f0\u6027\u5728\u7eb3\u7c73\u5c3a\u5ea6\u70ed\u4f20\u8f93\u4e2d\u7684\u5173\u952e\u4f5c\u7528\u3002", "motivation": "\u7814\u7a76\u7ed3\u6784\u5bf9\u79f0\u6027\u5728\u7f3a\u9677\u7eb3\u7c73\u7ba1\u70ed\u4f20\u8f93\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u89e3\u51b3\u5b9e\u9645\u5e94\u7528\u4e2d\u56e0\u7ed3\u6784\u7f3a\u9677\u6216\u6750\u6599\u754c\u9762\u5f15\u5165\u7684\u70ed\u963b\u95ee\u9898\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7ed3\u5408\u4e86\u7fa4\u8bba\u548c\u6a21\u5206\u8fa8\u683c\u6797\u51fd\u6570\u6cd5\u7684\u6846\u67b6\uff0c\u5e76\u57fa\u4e8e\u7ebf\u7fa4\u5bf9\u51c6\u4e00\u7ef4\u7ed3\u6784\u8fdb\u884c\u5bf9\u79f0\u6027\u5206\u6790\uff0c\u540c\u65f6\u5229\u7528\u57fa\u4e8eAllegro\u7684\u673a\u5668\u5b66\u4e60\u52bf\u80fd\u6765\u83b7\u53d6\u529b\u5e38\u6570\u548c\u58f0\u5b50\u4fe1\u606f\u3002", "result": "\u8ba1\u7b97\u4e86\u5355\u5c42\u548c\u591a\u5c42MoS2-WS2\u7eb3\u7c73\u7ba1\u7684\u8be6\u7ec6\u58f0\u5b50\u4f20\u8f93\u66f2\u7ebf\uff0c\u5e76\u5c06\u6bcf\u79cd\u6a21\u5f0f\u7684\u4f20\u8f93\u6982\u7387\u4e0e\u7ed3\u6784\u5bf9\u79f0\u6027\u8054\u7cfb\u8d77\u6765\uff0c\u53d1\u73b0\u5bf9\u79f0\u6027\u7834\u7f3a\u80fd\u589e\u5f3a\u70ed\u4f20\u8f93\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u663e\u8457\u7684\u5bf9\u79f0\u6027\u7834\u7f3a\u53ef\u4ee5\u901a\u8fc7\u653e\u5bbd\u9009\u62e9\u89c4\u5219\u548c\u6253\u5f00\u989d\u5916\u7684\u4f20\u8f93\u901a\u9053\u6765\u6291\u5236\u6563\u5c04\uff0c\u8fd9\u8868\u660e\u4e86\u5728\u9ad8\u65e0\u5e8f\u7f3a\u9677\u4e2d\uff0c\u5bf9\u79f0\u6027\u5728\u7406\u89e3\u7eb3\u7c73\u5c3a\u5ea6\u70ed\u4f20\u8f93\u7684\u7ec6\u5fae\u5dee\u522b\u65b9\u9762\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.07472", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07472", "abs": "https://arxiv.org/abs/2508.07472", "authors": ["Ramesh Adhikari", "Costas Busch", "Miroslav Popovic"], "title": "On the Efficiency of Dynamic Transaction Scheduling in Blockchain Sharding", "comment": "15 pages, 2 figures, accepted as a regular paper at 39th\n  International Symposium on Distributed Computing (DISC 2025)", "summary": "Sharding is a technique to speed up transaction processing in blockchains,\nwhere the $n$ processing nodes in the blockchain are divided into $s$ disjoint\ngroups (shards) that can process transactions in parallel. We study dynamic\nscheduling problems on a shard graph $G_s$ where transactions arrive online\nover time and are not known in advance. Each transaction may access at most $k$\nshards, and we denote by $d$ the worst distance between a transaction and its\naccessing (destination) shards (the parameter $d$ is unknown to the shards). To\nhandle different values of $d$, we assume a locality sensitive decomposition of\n$G_s$ into clusters of shards, where every cluster has a leader shard that\nschedules transactions for the cluster. We first examine the simpler case of\nthe stateless model, where leaders are not aware of the current state of the\ntransaction accounts, and we prove a $O(d \\log^2 s \\cdot \\min\\{k, \\sqrt{s}\\})$\ncompetitive ratio for latency. We then consider the stateful model, where\nleader shards gather the current state of accounts, and we prove a $O(\\log\ns\\cdot \\min\\{k, \\sqrt{s}\\}+\\log^2 s)$ competitive ratio for latency. Each\nleader calculates the schedule in polynomial time for each transaction that it\nprocesses. We show that for any $\\epsilon > 0$, approximating the optimal\nschedule within a $(\\min\\{k, \\sqrt{s}\\})^{1 -\\epsilon}$ factor is NP-hard.\nHence, our bound for the stateful model is within a poly-log factor from the\nbest possibly achievable. To the best of our knowledge, this is the first work\nto establish provably efficient dynamic scheduling algorithms for blockchain\nsharding systems.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u533a\u5757\u94fe\u5206\u7247\u7cfb\u7edf\u7684\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u65b0\u7684\u7b97\u6cd5\u5e76\u5206\u6790\u4e86\u5176\u6027\u80fd\uff0c\u5728\u63d0\u9ad8\u4ea4\u6613\u5904\u7406\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u7406\u8bba\u7a81\u7834\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u533a\u5757\u94fe\u5206\u7247\u7cfb\u7edf\u4e2d\u4ea4\u6613\u5904\u7406\u901f\u5ea6\u7684\u74f6\u9888\uff0c\u7814\u7a76\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u4ea4\u6613\u5904\u7406\u6548\u7387\u3002", "method": "\u7814\u7a76\u4e86\u52a8\u6001\u8c03\u5ea6\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u9002\u7528\u4e8e\u4e0d\u540cd\u503c\u7684\u5c40\u90e8\u654f\u611f\u5206\u89e3\uff0c\u5e76\u5206\u522b\u5728\u65e0\u72b6\u6001\u548c\u6709\u72b6\u6001\u6a21\u578b\u4e2d\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u76f8\u5e94\u7684\u7ade\u4e89\u6bd4\u754c\u9650\u3002\u5bf9\u72b6\u6001\u76f8\u5173\u6a21\u578b\uff0c\u8fd8\u8bc1\u660e\u4e86\u8fd1\u4f3c\u6700\u4f18\u8c03\u5ea6\u7684NP-hard\u6027\u8d28\u3002", "result": "\u5728\u65e0\u72b6\u6001\u6a21\u578b\u4e2d\uff0c\u8bc1\u660e\u4e86\u7ade\u4e89\u6bd4\u4e3aO(d log^2 s * min{k, sqrt{s}})\uff1b\u5728\u6709\u72b6\u6001\u6a21\u578b\u4e2d\uff0c\u8bc1\u660e\u4e86\u7ade\u4e89\u6bd4\u4e3aO(log s * min{k, sqrt{s}} + log^2 s)\u3002\u8bc1\u660e\u4e86\u8fd1\u4f3c\u6700\u4f18\u8c03\u5ea6\uff08\u5728min{k, sqrt{s}}\uff09^(1-\u03b5)\u56e0\u5b50\u5185\u662fNP-hard\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u4e3a\u533a\u5757\u94fe\u5206\u7247\u7cfb\u7edf\u5efa\u7acb\u4e86\u53ef\u8bc1\u660e\u7684\u9ad8\u6548\u52a8\u6001\u8c03\u5ea6\u7b97\u6cd5\uff0c\u5e76\u5c06\u72b6\u6001\u76f8\u5173\u6a21\u578b\u4e2d\u7684\u754c\u9650\u5728\u591a\u5bf9\u6570\u56e0\u5b50\u5185\u63a8\u5411\u4e86\u6700\u4f18\u53ef\u80fd\u5b9e\u73b0\u7684\u4e0b\u754c\u3002"}}
{"id": "2508.06568", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06568", "abs": "https://arxiv.org/abs/2508.06568", "authors": ["Amin Yazdanshenas", "Reza Faieghi"], "title": "Robust and Agile Quadrotor Flight via Adaptive Unwinding-Free Quaternion Sliding Mode Control", "comment": null, "summary": "This paper presents a new adaptive sliding mode control (SMC) framework for\nquadrotors that achieves robust and agile flight under tight computational\nconstraints. The proposed controller addresses key limitations of prior SMC\nformulations, including (i) the slow convergence and almost-global stability of\n$\\mathrm{SO(3)}$-based methods, (ii) the oversimplification of rotational\ndynamics in Euler-based controllers, (iii) the unwinding phenomenon in\nquaternion-based formulations, and (iv) the gain overgrowth problem in adaptive\nSMC schemes. Leveraging nonsmooth stability analysis, we provide rigorous\nglobal stability proofs for both the nonsmooth attitude sliding dynamics\ndefined on $\\mathbb{S}^3$ and the position sliding dynamics. Our controller is\ncomputationally efficient and runs reliably on a resource-constrained nano\nquadrotor, achieving 250 Hz and 500 Hz refresh rates for position and attitude\ncontrol, respectively. In an extensive set of hardware experiments with over\n130 flight trials, the proposed controller consistently outperforms three\nbenchmark methods, demonstrating superior trajectory tracking accuracy and\nrobustness with relatively low control effort. The controller enables\naggressive maneuvers such as dynamic throw launches, flip maneuvers, and\naccelerations exceeding 3g, which is remarkable for a 32-gram nano quadrotor.\nThese results highlight promising potential for real-world applications,\nparticularly in scenarios requiring robust, high-performance flight control\nunder significant external disturbances and tight computational constraints.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\u6846\u67b6\uff0c\u7528\u4e8e\u56db\u65cb\u7ffc\u98de\u884c\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u3001\u654f\u6377\u548c\u9ad8\u7cbe\u5ea6\u7684\u98de\u884c\uff0c\u80fd\u591f\u6267\u884c\u9ad8\u96be\u5ea6\u673a\u52a8\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709SMC\u65b9\u6cd5\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u5305\u62ec\u57fa\u4e8eSO(3)\u65b9\u6cd5\u7684\u6536\u655b\u6162\u548c\u51e0\u4e4e\u5168\u5c40\u7a33\u5b9a\u6027\u95ee\u9898\u3001\u6b27\u62c9\u57fa\u63a7\u5236\u5668\u4e2d\u65cb\u8f6c\u52a8\u529b\u5b66\u7684\u8fc7\u5ea6\u7b80\u5316\u95ee\u9898\u3001\u57fa\u4e8e\u56db\u5143\u6570\u516c\u5f0f\u7684\u89e3\u7f20\u7ed5\u73b0\u8c61\u95ee\u9898\uff0c\u4ee5\u53ca\u81ea\u9002\u5e94SMC\u65b9\u6848\u4e2d\u7684\u589e\u76ca\u8fc7\u5ea6\u589e\u957f\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u6ed1\u6a21\u63a7\u5236\uff08SMC\uff09\u6846\u67b6\uff0c\u5229\u7528\u975e\u5149\u6ed1\u7a33\u5b9a\u6027\u5206\u6790\uff0c\u5bf9\u5b9a\u4e49\u5728S3\u4e0a\u7684\u975e\u5149\u6ed1\u59ff\u6001\u6ed1\u6a21\u52a8\u529b\u5b66\u548c\u4f4d\u7f6e\u6ed1\u6a21\u52a8\u529b\u5b66\u8fdb\u884c\u4e86\u4e25\u683c\u7684\u5168\u5c40\u7a33\u5b9a\u6027\u8bc1\u660e\u3002", "result": "\u6240\u63d0\u51fa\u7684\u63a7\u5236\u5668\u5728\u8ba1\u7b97\u4e0a\u662f\u9ad8\u6548\u7684\uff0c\u80fd\u591f\u5728\u8d44\u6e90\u53d7\u9650\u7684\u7eb3\u7c73\u56db\u65cb\u7ffc\u4e0a\u53ef\u9760\u8fd0\u884c\uff0c\u5206\u522b\u5b9e\u73b0250 Hz\u548c500 Hz\u7684\u4f4d\u7f6e\u548c\u59ff\u6001\u63a7\u5236\u5237\u65b0\u7387\u3002\u5728\u8d85\u8fc7130\u6b21\u98de\u884c\u8bd5\u9a8c\u7684\u5e7f\u6cdb\u786c\u4ef6\u5b9e\u9a8c\u4e2d\uff0c\u8be5\u63a7\u5236\u5668\u59cb\u7ec8\u4f18\u4e8e\u4e09\u79cd\u57fa\u51c6\u65b9\u6cd5\uff0c\u5728\u76f8\u5bf9\u8f83\u4f4e\u7684\u63a7\u5236\u52aa\u529b\u4e0b\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u8f68\u8ff9\u8ddf\u8e2a\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u6267\u884c32\u514b\u7eb3\u7c73\u56db\u65cb\u7ffc\u4e0a\u975e\u51e1\u7684\u6fc0\u8fdb\u673a\u52a8\u3002", "conclusion": "\u8be5\u63a7\u5236\u5668\u5728\u8ba1\u7b97\u8d44\u6e90\u53d7\u9650\u7684\u7eb3\u7c73\u56db\u65cb\u7ffc\u98de\u884c\u5668\u4e0a\u5b9e\u73b0\u4e86\u9c81\u68d2\u4e14\u654f\u6377\u7684\u98de\u884c\uff0c\u5728\u5927\u91cf\u7684\u786c\u4ef6\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u51c6\u65b9\u6cd5\uff0c\u80fd\u591f\u6267\u884c\u9ad8\u96be\u5ea6\u7684\u6fc0\u8fdb\u673a\u52a8\uff0c\u5982\u52a8\u6001\u629b\u63b7\u8d77\u98de\u3001\u7ffb\u8f6c\u673a\u52a8\u548c\u8d85\u8fc73g\u7684\u52a0\u901f\u5ea6\uff0c\u8bc1\u660e\u4e86\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5de8\u5927\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u9c81\u68d2\u3001\u9ad8\u6027\u80fd\u98de\u884c\u63a7\u5236\u4e14\u53d7\u5230\u663e\u8457\u5916\u90e8\u5e72\u6270\u548c\u4e25\u683c\u8ba1\u7b97\u9650\u5236\u7684\u573a\u666f\u3002"}}
{"id": "2508.06683", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06683", "abs": "https://arxiv.org/abs/2508.06683", "authors": ["Alexandre Cesar Ricardo", "Ciro Micheletti Diniz", "Celso Jorge Villas-B\u00f4as"], "title": "Interference Between Electromagnetic and Mechanical Waves", "comment": "7 pages, 2 figures", "summary": "Classically, wave interference is a phenomenon that can be explained by\nconsidering only the waves themselves, that is, without the need to consider\nthe apparatus that monitors or observes them. Thus, in classical theories,\ninterference can only occur between waves of the same nature. In quantum\ntheory, the observed results require a description of the system and its\nmeasuring apparatus, which allows us to rethink the explanation of various\nnatural phenomena. In this paper, we consider the ion-trap platform to study\nthe interference of waves with different physical natures, specifically the\nelectromagnetic and mechanical. At first, we drive two lasers onto a\nsingle-trapped ion to produce Jaynes-Cummings and Carrier interactions, where\nwe verify that, depending on the phase relationship between the coherent state\nof the vibrational (mechanical) mode and the Carrier pulse (electromagnetic\nwave), the interactions enhance or cancel out population transfer to the\nelectronic state of the ion, that works out as our measuring apparatus for\nthose waves. Extending our result to an ion chain, we verify that a precise\nmodulation of the Carrier Rabi frequency and phase (electromagnetic pulse)\naccording to the amplitude of the incoming mechanical coherent state in the ion\nchain enables creating either constructive or destructive interference with\npropagating pulses, in which the electronic state of the driven ion is,\nrespectively, populated more and faster, or transparent to both pulse waves,\nwhen the information flux behaves as if no external fields are applied.\nFinally, this new type of controlled interference between waves of different\nnatures allows us to propose new hybrid quantum devices, such as transistors or\nfilters of wave packets, where photonic (phononic) pulses control the passage\nof phononic (photonic) waves.", "AI": {"tldr": "\u5728\u79bb\u5b50\u9631\u4e2d\uff0c\u6211\u4eec\u5b9e\u73b0\u4e86\u7535\u78c1\u6ce2\u548c\u673a\u68b0\u6ce2\u7684\u5e72\u6d89\uff0c\u901a\u8fc7\u63a7\u5236\u5b83\u4eec\u7684\u76f8\u4f4d\u5173\u7cfb\uff0c\u53ef\u4ee5\u589e\u5f3a\u6216\u62b5\u6d88\u5b83\u4eec\u5bf9\u79bb\u5b50\u72b6\u6001\u7684\u5f71\u54cd\u3002\u8fd9\u4e3a\u5236\u9020\u65b0\u578b\u91cf\u5b50\u8bbe\u5907\uff08\u5982\u6676\u4f53\u7ba1\u6216\u6ce2\u5305\u8fc7\u6ee4\u5668\uff09\u5f00\u8f9f\u4e86\u9053\u8def\u3002", "motivation": "\u5728\u91cf\u5b50\u7406\u8bba\u4e2d\uff0c\u89c2\u6d4b\u7ed3\u679c\u9700\u8981\u540c\u65f6\u8003\u8651\u7cfb\u7edf\u548c\u6d4b\u91cf\u4eea\u5668\u3002\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528\u79bb\u5b50\u9631\u5e73\u53f0\u63a2\u7d22\u4e0d\u540c\u7269\u7406\u6027\u8d28\u6ce2\u7684\u5e72\u6d89\u73b0\u8c61\uff0c\u4ee5\u6df1\u5316\u5bf9\u91cf\u5b50\u5e72\u6d89\u7684\u7406\u89e3\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u91cf\u5b50\u5668\u4ef6\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u9a71\u52a8\u53cc\u6fc0\u5149\u5230\u5355\u79bb\u5b50\u4e0a\uff0c\u4ea7\u751fJaynes-Cummings\u548cCarrier\u76f8\u4e92\u4f5c\u7528\uff0c\u5e76\u7814\u7a76\u4e86\u5176\u4e0e\u673a\u68b0\u6ce2\u76f8\u4f4d\u76f8\u5e72\u6027\u5bf9\u79cd\u7fa4\u8f6c\u79fb\u7684\u5f71\u54cd\u3002\u5c06\u7ed3\u679c\u63a8\u5e7f\u5230\u79bb\u5b50\u94fe\uff0c\u901a\u8fc7\u7cbe\u786e\u8c03\u5236CarrierRabi\u9891\u7387\u548c\u76f8\u4f4d\u6765\u63a7\u5236\u5e72\u6d89\u3002", "result": "\u5728\u5355\u79bb\u5b50\u5b9e\u9a8c\u4e2d\uff0c\u901a\u8fc7\u8c03\u8282\u7535\u78c1\u6ce2\u4e0e\u673a\u68b0\u6ce2\u7684\u76f8\u4f4d\u5173\u7cfb\uff0c\u5b9e\u73b0\u4e86\u5bf9\u79cd\u7fa4\u8f6c\u79fb\u7684\u589e\u5f3a\u6216\u62b5\u6d88\u3002\u5728\u79bb\u5b50\u94fe\u5b9e\u9a8c\u4e2d\uff0c\u901a\u8fc7\u7cbe\u786e\u63a7\u5236\u7535\u78c1\u8109\u51b2\u7684\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u4e0e\u673a\u68b0\u76f8\u5e72\u6001\u7684\u76f8\u957f\u6216\u76f8\u6d88\u5e72\u6d89\uff0c\u4f7f\u5f97\u79bb\u5b50\u7684\u7535\u5b50\u6001\u8868\u73b0\u51fa\u4e0d\u540c\u7684\u884c\u4e3a\u3002", "conclusion": "\u672c\u7814\u7a76\u5728\u79bb\u5b50\u9631\u5e73\u53f0\u4e0a\u5b9e\u73b0\u4e86\u4e0d\u540c\u7269\u7406\u6027\u8d28\uff08\u7535\u78c1\u6ce2\u4e0e\u673a\u68b0\u6ce2\uff09\u7684\u5e72\u6d89\uff0c\u5e76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6b64\u7684\u65b0\u578b\u6df7\u5408\u91cf\u5b50\u5668\u4ef6\u3002"}}
{"id": "2508.07796", "categories": ["cs.AR"], "pdf": "https://arxiv.org/pdf/2508.07796", "abs": "https://arxiv.org/abs/2508.07796", "authors": ["Dengke Han", "Duo Wang", "Mingyu Yan", "Xiaochun Ye", "Dongrui Fan"], "title": "TLV-HGNN: Thinking Like a Vertex for Memory-efficient HGNN Inference", "comment": "8 pages, 9 figures, accepted by ICCD 2025", "summary": "Heterogeneous graph neural networks (HGNNs) excel at processing heterogeneous\ngraph data and are widely applied in critical domains. In HGNN inference, the\nneighbor aggregation stage is the primary performance determinant, yet it\nsuffers from two major sources of memory inefficiency. First, the commonly\nadopted per-semantic execution paradigm stores intermediate aggregation results\nfor each semantic prior to semantic fusion, causing substantial memory\nexpansion. Second, the aggregation process incurs extensive redundant memory\naccesses, including repeated loading of target vertex features across semantics\nand repeated accesses to shared neighbors due to cross-semantic neighborhood\noverlap. These inefficiencies severely limit scalability and reduce HGNN\ninference performance.\n  In this work, we first propose a semantics-complete execution paradigm from a\nvertex perspective that eliminates per-semantic intermediate storage and\nredundant target vertex accesses. Building on this paradigm, we design\nTVL-HGNN, a reconfigurable hardware accelerator optimized for efficient\naggregation. In addition, we introduce a vertex grouping technique based on\ncross-semantic neighborhood overlap, with hardware implementation, to reduce\nredundant accesses to shared neighbors. Experimental results demonstrate that\nTVL-HGNN achieves average speedups of 7.85x and 1.41x over the NVIDIA A100 GPU\nand the state-of-the-art HGNN accelerator HiHGNN, respectively, while reducing\nenergy consumption by 98.79% and 32.61%.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TVL-HGNN \u7684\u65b0\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u901a\u8fc7\u4f18\u5316\u5185\u5b58\u4f7f\u7528\u548c\u90bb\u5c45\u805a\u5408\u8fc7\u7a0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86 HGNN \u63a8\u7406\u7684\u901f\u5ea6\u5e76\u964d\u4f4e\u4e86\u80fd\u8017\u3002", "motivation": "\u73b0\u6709\u7684\u5f02\u6784\u56fe\u795e\u7ecf\u7f51\u7edc\uff08HGNN\uff09\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\uff0c\u7531\u4e8e\u6309\u8bed\u4e49\u6267\u884c\u6a21\u5f0f\u5b58\u50a8\u4e2d\u95f4\u7ed3\u679c\u4ee5\u53ca\u805a\u5408\u8fc7\u7a0b\u4e2d\u5b58\u5728\u5927\u91cf\u7684\u5197\u4f59\u5185\u5b58\u8bbf\u95ee\uff08\u91cd\u590d\u52a0\u8f7d\u76ee\u6807\u9876\u70b9\u7279\u5f81\u548c\u91cd\u590d\u8bbf\u95ee\u5171\u4eab\u90bb\u5c45\uff09\uff0c\u5bfc\u81f4\u5185\u5b58\u6548\u7387\u4f4e\u4e0b\uff0c\u4e25\u91cd\u9650\u5236\u4e86 HGNN \u7684\u53ef\u6269\u5c55\u6027\u548c\u63a8\u7406\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u9876\u70b9\u89c6\u89d2\u3001\u6d88\u9664\u8bed\u4e49\u95f4\u5197\u4f59\u5b58\u50a8\u548c\u8bbf\u95ee\u7684\u8bed\u4e49\u5b8c\u6574\u6267\u884c\u6a21\u5f0f\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u540d\u4e3a TVL-HGNN \u7684\u53ef\u91cd\u6784\u786c\u4ef6\u52a0\u901f\u5668\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u57fa\u4e8e\u8de8\u8bed\u4e49\u90bb\u5c45\u91cd\u53e0\u7684\u9876\u70b9\u5206\u7ec4\u6280\u672f\uff0c\u4ee5\u51cf\u5c11\u5197\u4f59\u7684\u90bb\u5c45\u8bbf\u95ee\u3002", "result": "TVL-HGNN \u5728\u5e73\u5747\u52a0\u901f\u65b9\u9762\u6bd4 NVIDIA A100 GPU \u5feb 7.85 \u500d\uff0c\u6bd4\u6700\u5148\u8fdb\u7684 HGNN \u52a0\u901f\u5668 HiHGNN \u5feb 1.41 \u500d\uff0c\u540c\u65f6\u80fd\u8017\u5206\u522b\u964d\u4f4e\u4e86 98.79% \u548c 32.61%\u3002", "conclusion": "TVL-HGNN \u901a\u8fc7\u91c7\u7528\u8bed\u4e49\u65e0\u5173\u7684\u6267\u884c\u6a21\u5f0f\u548c\u4f18\u5316\u7684\u805a\u5408\u8bbe\u8ba1\uff0c\u5728\u5185\u5b58\u6548\u7387\u548c\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\uff0c\u5b9e\u73b0\u4e86\u6bd4 A100 GPU \u9ad8 7.85 \u500d\u7684\u5e73\u5747\u52a0\u901f\uff0c\u5e76\u5927\u5e45\u964d\u4f4e\u4e86\u80fd\u8017\u3002"}}
{"id": "2508.06968", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.06968", "abs": "https://arxiv.org/abs/2508.06968", "authors": ["Ulas Gunes", "Matias Turkulainen", "Juho Kannala", "Esa Rahtu"], "title": "Evaluating Fisheye-Compatible 3D Gaussian Splatting Methods on Real Images Beyond 180 Degree Field of View", "comment": null, "summary": "We present the first evaluation of fisheye-based 3D Gaussian Splatting\nmethods, Fisheye-GS and 3DGUT, on real images with fields of view exceeding 180\ndegree. Our study covers both indoor and outdoor scenes captured with 200\ndegree fisheye cameras and analyzes how each method handles extreme distortion\nin real world settings. We evaluate performance under varying fields of view\n(200 degree, 160 degree, and 120 degree) to study the tradeoff between\nperipheral distortion and spatial coverage. Fisheye-GS benefits from field of\nview (FoV) reduction, particularly at 160 degree, while 3DGUT remains stable\nacross all settings and maintains high perceptual quality at the full 200\ndegree view. To address the limitations of SfM-based initialization, which\noften fails under strong distortion, we also propose a depth-based strategy\nusing UniK3D predictions from only 2-3 fisheye images per scene. Although\nUniK3D is not trained on real fisheye data, it produces dense point clouds that\nenable reconstruction quality on par with SfM, even in difficult scenes with\nfog, glare, or sky. Our results highlight the practical viability of\nfisheye-based 3DGS methods for wide-angle 3D reconstruction from sparse and\ndistortion-heavy image inputs.", "AI": {"tldr": "\u7814\u7a76\u9996\u6b21\u8bc4\u4f30\u4e86Fisheye-GS\u548c3DGUT\u8fd9\u4e24\u79cd\u57fa\u4e8e\u9c7c\u773c\u955c\u5934\u7684\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u6781\u7aef\u5e7f\u89d2\uff08>180\u5ea6\uff09\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0cFisheye-GS\u5728160\u5ea6\u89c6\u573a\u89d2\u4e0b\u6548\u679c\u66f4\u4f73\uff0c3DGUT\u5728200\u5ea6\u5168\u89c6\u573a\u89d2\u4e0b\u8868\u73b0\u7a33\u5b9a\u3002\u4e3a\u89e3\u51b3SfM\u521d\u59cb\u5316\u5728\u5f3a\u7578\u53d8\u4e0b\u5931\u6548\u7684\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUniK3D\u6df1\u5ea6\u9884\u6d4b\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u53ef\u5728\u590d\u6742\u573a\u666f\u4e0b\u5b9e\u73b0\u4e0eSfM\u76f8\u5f53\u7684\u91cd\u5efa\u8d28\u91cf\u3002\u8be5\u7814\u7a76\u8bc1\u660e\u4e86\u9c7c\u773c\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u5904\u7406\u7578\u53d8\u3001\u7a00\u758f\u8f93\u5165\u8fdb\u884c\u5e7f\u89d2\u91cd\u5efa\u7684\u5b9e\u7528\u6027\u3002", "motivation": "\u73b0\u6709\u4e09\u7ef4\u91cd\u5efa\u65b9\u6cd5\u5728\u5904\u7406\u8d85\u8fc7180\u5ea6\u89c6\u573a\u89d2\u7684\u9c7c\u773c\u56fe\u50cf\u65f6\u9762\u4e34\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5f3a\u7578\u53d8\u548c\u7a00\u758f\u8f93\u5165\u7684\u60c5\u51b5\u4e0b\u3002\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u57fa\u4e8e\u9c7c\u773c\u955c\u5934\u7684\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff08Fisheye-GS\u548c3DGUT\uff09\u5728\u771f\u5b9e\u5e7f\u89d2\u573a\u666f\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u521d\u59cb\u5316\u7b56\u7565\u4ee5\u514b\u670d\u4f20\u7edfSfM\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "method": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86Fisheye-GS\u548c3DGUT\u4e24\u79cd\u57fa\u4e8e\u9c7c\u773c\u955c\u5934\u7684\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u6027\u80fd\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u89c6\u573a\u89d2\uff08200\u5ea6\u3001160\u5ea6\u3001120\u5ea6\uff09\u5bf9\u91cd\u5efa\u8d28\u91cf\u7684\u5f71\u54cd\u3002\u4e3a\u4e86\u89e3\u51b3SfM\u521d\u59cb\u5316\u5728\u5f3a\u7578\u53d8\u4e0b\u5931\u6548\u7684\u95ee\u9898\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eUniK3D\u6df1\u5ea6\u9884\u6d4b\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u4ec5\u4f7f\u75282-3\u5f20\u9c7c\u773c\u56fe\u50cf\u5373\u53ef\u751f\u6210\u7528\u4e8e\u4e09\u7ef4\u91cd\u5efa\u7684\u70b9\u4e91\u3002", "result": "Fisheye-GS\u5728160\u5ea6\u89c6\u573a\u89d2\u4e0b\u8868\u73b0\u66f4\u597d\uff0c\u800c3DGUT\u5728200\u5ea6\u5168\u89c6\u573a\u89d2\u4e0b\u8868\u73b0\u7a33\u5b9a\u4e14\u8d28\u91cf\u9ad8\u3002\u63d0\u51fa\u7684\u57fa\u4e8eUniK3D\u7684\u6df1\u5ea6\u521d\u59cb\u5316\u7b56\u7565\u5728\u6709\u96fe\u3001\u7729\u5149\u6216\u5929\u7a7a\u7b49\u590d\u6742\u573a\u666f\u4e0b\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u70b9\u4e91\uff0c\u91cd\u5efa\u6548\u679c\u5ab2\u7f8eSfM\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u57fa\u4e8e\u9c7c\u773c\u955c\u5934\u7684\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u65b9\u6cd5\u5728\u8d85\u8fc7180\u5ea6\u89c6\u573a\u89d2\u7684\u771f\u5b9e\u56fe\u50cf\u4e0a\u7684\u9996\u6b21\u8bc4\u4f30\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cFisheye-GS\u5728\u89c6\u573a\u89d2\u51cf\u5c0f\u5230160\u5ea6\u65f6\u6027\u80fd\u6709\u6240\u63d0\u5347\uff0c\u800c3DGUT\u5728\u6240\u6709\u89c6\u573a\u89d2\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u7a33\u5b9a\uff0c\u5e76\u5728200\u5ea6\u5168\u89c6\u573a\u89d2\u4e0b\u4fdd\u6301\u4e86\u9ad8\u611f\u77e5\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u57fa\u4e8eUniK3D\u6df1\u5ea6\u9884\u6d4b\u7684\u521d\u59cb\u5316\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u4f20\u7edfSfM\u65b9\u6cd5\u5728\u5f3a\u7578\u53d8\u4e0b\u5931\u6548\u7684\u95ee\u9898\uff0c\u5373\u4f7f\u5728\u6709\u96fe\u3001\u7729\u5149\u6216\u5929\u7a7a\u7b49\u590d\u6742\u573a\u666f\u4e0b\uff0c\u4e5f\u80fd\u5b9e\u73b0\u4e0eSfM\u76f8\u5f53\u7684\u91cd\u5efa\u8d28\u91cf\u3002\u8fd9\u8bc1\u660e\u4e86\u57fa\u4e8e\u9c7c\u773c\u955c\u5934\u7684\u4e09\u7ef4\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u5904\u7406\u7a00\u758f\u4e14\u7578\u53d8\u4e25\u91cd\u7684\u56fe\u50cf\u8f93\u5165\u4ee5\u8fdb\u884c\u5e7f\u89d2\u4e09\u7ef4\u91cd\u5efa\u65b9\u9762\u5177\u6709\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.06600", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06600", "abs": "https://arxiv.org/abs/2508.06600", "authors": ["Zijian Chen", "Xueguang Ma", "Shengyao Zhuang", "Ping Nie", "Kai Zou", "Andrew Liu", "Joshua Green", "Kshama Patel", "Ruoxi Meng", "Mingyi Su", "Sahel Sharifymoghaddam", "Yanxi Li", "Haoran Hong", "Xinyu Shi", "Xuye Liu", "Nandan Thakur", "Crystina Zhang", "Luyu Gao", "Wenhu Chen", "Jimmy Lin"], "title": "BrowseComp-Plus: A More Fair and Transparent Evaluation Benchmark of Deep-Research Agent", "comment": null, "summary": "Deep-Research agents, which integrate large language models (LLMs) with\nsearch tools, have shown success in improving the effectiveness of handling\ncomplex queries that require iterative search planning and reasoning over\nsearch results. Evaluations on current benchmarks like BrowseComp relies on\nblack-box live web search APIs, have notable limitations in (1) fairness:\ndynamic and opaque web APIs hinder fair comparisons and reproducibility of deep\nresearch methods; (2) transparency: lack of control over the document corpus\nmakes it difficult to isolate retriever contributions. In other words, the\ncurrent evaluations may compare a complete deep research system at a given\ntime, but they do not foster well-controlled experiments to provide insights\ninto the capability of underlying deep research LLMs. To address these\nchallenges, we introduce BrowseComp-Plus, a benchmark derived from BrowseComp,\nemploying a fixed, carefully curated corpus. Each query in BrowseComp-Plus\nincludes human-verified supporting documents and mined challenging negatives,\nenabling controlled experimentation. The benchmark is shown to be effective in\ndistinguishing the performance of deep research systems. For instance, the\nopen-source model Search-R1, when paired with the BM25 retriever, achieves\n3.86% accuracy, whereas the GPT-5 achieves 55.9%. Integrating the GPT-5 with\nthe Qwen3-Embedding-8B retriever further enhances its accuracy to 70.1% with\nfewer search calls. This benchmark allows comprehensive evaluation and\ndisentangled analysis of deep research agents and retrieval methods, fostering\ninsights into retrieval effectiveness, citation accuracy, and context\nengineering in Deep-Research system.", "AI": {"tldr": "\u65b0\u7684 BrowseComp-Plus \u57fa\u51c6\u6d4b\u8bd5\u4f7f\u7528\u56fa\u5b9a\u8bed\u6599\u5e93\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u7684\u66f4\u597d\u533a\u5206\u548c\u5206\u6790\u3002", "motivation": "\u5f53\u524d\u7684\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u8bc4\u4f30\u57fa\u51c6\uff08\u5982 BrowseComp\uff09\u4f9d\u8d56\u4e8e\u52a8\u6001\u4e14\u4e0d\u900f\u660e\u7684\u5b9e\u65f6\u7f51\u7edc\u641c\u7d22 API\uff0c\u8fd9\u963b\u788d\u4e86\u516c\u5e73\u7684\u6bd4\u8f83\u548c\u53ef\u590d\u73b0\u6027\uff0c\u5e76\u4e14\u7531\u4e8e\u7f3a\u4e4f\u5bf9\u6587\u6863\u8bed\u6599\u5e93\u7684\u63a7\u5236\uff0c\u96be\u4ee5\u5206\u79bb\u68c0\u7d22\u5668\u7684\u8d21\u732e\u3002\u56e0\u6b64\uff0c\u8bc4\u4f30\u53ef\u80fd\u65e0\u6cd5\u63d0\u4f9b\u5bf9\u5e95\u5c42\u6df1\u5ea6\u7814\u7a76 LLM \u80fd\u529b\u7684\u6df1\u5165\u89c1\u89e3\u3002", "method": "\u63d0\u51fa BrowseComp-Plus \u57fa\u51c6\u6d4b\u8bd5\uff0c\u8be5\u57fa\u51c6\u6d4b\u8bd5\u6e90\u81ea BrowseComp\uff0c\u4f46\u4f7f\u7528\u56fa\u5b9a\u7684\u3001\u7cbe\u5fc3\u7b56\u5212\u7684\u8bed\u6599\u5e93\uff0c\u5176\u4e2d\u6bcf\u4e2a\u67e5\u8be2\u90fd\u5305\u542b\u4eba\u7c7b\u9a8c\u8bc1\u7684\u652f\u6301\u6587\u6863\u548c\u6316\u6398\u51fa\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u8d1f\u4f8b\u3002", "result": "BrowseComp-Plus \u57fa\u51c6\u6d4b\u8bd5\u80fd\u591f\u6709\u6548\u5730\u533a\u5206\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u7684\u6027\u80fd\u3002\u4f8b\u5982\uff0c\u5f00\u6e90\u6a21\u578b Search-R1 \u4e0e BM25 \u68c0\u7d22\u5668\u914d\u5bf9\u65f6\uff0c\u51c6\u786e\u7387\u4e3a 3.86%\uff1b\u800c GPT-5 \u7684\u51c6\u786e\u7387\u4e3a 55.9%\u3002\u5c06 GPT-5 \u4e0e Qwen3-Embedding-8B \u68c0\u7d22\u5668\u96c6\u6210\uff0c\u5728\u8c03\u7528\u66f4\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u51c6\u786e\u7387\u8fdb\u4e00\u6b65\u63d0\u9ad8\u5230 70.1%\u3002\u8be5\u57fa\u51c6\u6d4b\u8bd5\u652f\u6301\u5bf9\u6df1\u5ea6\u7814\u7a76\u4ee3\u7406\u548c\u68c0\u7d22\u65b9\u6cd5\u7684\u5168\u9762\u8bc4\u4f30\u548c\u89e3\u8026\u5206\u6790\u3002", "conclusion": "BrowseComp-Plus \u57fa\u51c6\u6d4b\u8bd5\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u56fa\u5b9a\u7684\u3001\u7cbe\u5fc3\u7b56\u5212\u7684\u8bed\u6599\u5e93\uff0c\u5e76\u5305\u542b\u7ecf\u8fc7\u4eba\u7c7b\u9a8c\u8bc1\u7684\u652f\u6301\u6587\u6863\u548c\u6316\u6398\u51fa\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u8d1f\u4f8b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\uff08\u5982 BrowseComp\uff09\u5728\u516c\u5e73\u6027\u548c\u900f\u660e\u5ea6\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002\u8fd9\u4f7f\u5f97\u80fd\u591f\u5bf9\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u8fdb\u884c\u53ef\u63a7\u7684\u5b9e\u9a8c\u548c\u6df1\u5165\u5206\u6790\u3002"}}
{"id": "2508.07699", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07699", "abs": "https://arxiv.org/abs/2508.07699", "authors": ["Hang Ren", "Xiaozhen Sun", "Tianzi Ma", "Jiajia Zhang", "Xuan Wang"], "title": "Last-Iterate Convergence in Adaptive Regret Minimization for Approximate Extensive-Form Perfect Equilibrium", "comment": null, "summary": "The Nash Equilibrium (NE) assumes rational play in imperfect-information\nExtensive-Form Games (EFGs) but fails to ensure optimal strategies for\noff-equilibrium branches of the game tree, potentially leading to suboptimal\noutcomes in practical settings. To address this, the Extensive-Form Perfect\nEquilibrium (EFPE), a refinement of NE, introduces controlled perturbations to\nmodel potential player errors. However, existing EFPE-finding algorithms, which\ntypically rely on average strategy convergence and fixed perturbations, face\nsignificant limitations: computing average strategies incurs high computational\ncosts and approximation errors, while fixed perturbations create a trade-off\nbetween NE approximation accuracy and the convergence rate of NE refinements.\n  To tackle these challenges, we propose an efficient adaptive regret\nminimization algorithm for computing approximate EFPE, achieving last-iterate\nconvergence in two-player zero-sum EFGs. Our approach introduces Reward\nTransformation Counterfactual Regret Minimization (RTCFR) to solve perturbed\ngames and defines a novel metric, the Information Set Nash Equilibrium (ISNE),\nto dynamically adjust perturbations. Theoretical analysis confirms convergence\nto EFPE, and experimental results demonstrate that our method significantly\noutperforms state-of-the-art algorithms in both NE and EFPE-finding tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8c03\u6574\u535a\u5f08\u4e2d\u7684\u6270\u52a8\uff0c\u66f4\u6709\u6548\u5730\u627e\u5230\u7eb3\u4ec0\u5747\u8861\u548c\u6269\u5c55\u578b\u5b8c\u7f8e\u5747\u8861\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u73b0\u6709\u7684EFPE\u7b97\u6cd5\u5728\u8ba1\u7b97\u5e73\u5747\u7b56\u7565\u65f6\u6210\u672c\u9ad8\u6602\u4e14\u5b58\u5728\u8fd1\u4f3c\u8bef\u5dee\uff0c\u56fa\u5b9a\u7684\u6270\u52a8\u5219\u5728NE\u8fd1\u4f3c\u7cbe\u5ea6\u548c\u6536\u655b\u901f\u5ea6\u4e4b\u95f4\u9020\u6210\u6743\u8861\uff0c\u9650\u5236\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u7684\u81ea\u9002\u5e94\u6094\u6068\u6700\u5c0f\u5316\u7b97\u6cd5\uff0c\u5305\u62ec\u5956\u52b1\u8f6c\u6362\u53cd\u4e8b\u5b9e\u6094\u6068\u6700\u5c0f\u5316\uff08RTCFR\uff09\u6765\u89e3\u51b3\u6270\u52a8\u535a\u5f08\uff0c\u5e76\u5b9a\u4e49\u4e86\u4fe1\u606f\u96c6\u7eb3\u4ec0\u5747\u8861\uff08ISNE\uff09\u6765\u52a8\u6001\u8c03\u6574\u6270\u52a8\u3002", "result": "\u7406\u8bba\u5206\u6790\u8bc1\u5b9e\u4e86\u7b97\u6cd5\u6536\u655b\u5230EFPE\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728NE\u548cEFPE\u67e5\u627e\u4efb\u52a1\u4e2d\u5747\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u5b8c\u7f8e\u4fe1\u606f\u6269\u5c55\u578b\u535a\u5f08\u4e2d\uff0c\u901a\u8fc7\u5f15\u5165RTCFR\u548cISNE\uff0c\u80fd\u591f\u9ad8\u6548\u5730\u8ba1\u7b97\u8fd1\u4f3cEFPE\uff0c\u5e76\u5728NE\u548cEFPE\u67e5\u627e\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7b97\u6cd5\u3002"}}
{"id": "2508.07698", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.07698", "abs": "https://arxiv.org/abs/2508.07698", "authors": ["Hirokazu Fujiwara", "Yuki Itoya", "Masaharu Kobayashi", "C\u00e9dric Bareille", "Toshiyuki Taniuchi"], "title": "Breakdown and polarization contrasts in ferroelectric devices observed by operando laser-based photoemission electron microscopy with the AC/DC electrical characterization system", "comment": "24 pages, 10 figures", "summary": "We have developed an operando laser-based photoemission electron microscope\n(laser-PEEM) with a ferroelectric characterization system. A Sawyer-Tower\ncircuit was implemented to measure the polarization-voltage ($P-V$)\ncharacteristics of ferroelectric devices. Using this system, we successfully\nobtained the well-defined $P-V$ hysteresis loops for a ferroelectric capacitor\nincorporating Hf$_{0.5}$Zr$_{0.5}$O$_2$ (HZO), reproducing the typical\nfield-cycling characteristics of HZO capacitors. After dielectric breakdown\ncaused by field-cycling stress, we visualized a conduction filament through the\ntop electrode without any destructive processing. Additionally, we successfully\nobserved polarization contrast through the top electrode of an oxide\nsemiconductor (InZnO$_x$). These results indicate that our operando laser-PEEM\nsystem is a powerful tool for visualizing conduction filaments after dielectric\nbreakdown, the ferroelectric polarization contrasts, and electronic state\ndistribution of materials implemented in ferroelectric devices, including\nferroelectric field-effect transistors and ferroelectric tunnel junctions.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684operando\u6fc0\u5149-PEEM\u7cfb\u7edf\uff0c\u53ef\u4ee5\u540c\u65f6\u8868\u5f81\u94c1\u7535\u7279\u6027\u5e76\u53ef\u89c6\u5316\u94c1\u7535\u5668\u4ef6\u7684\u5fae\u89c2\u7ed3\u6784\u548c\u7535\u5b50\u6001\u3002\u8be5\u7cfb\u7edf\u80fd\u591f\u6e05\u6670\u5730\u663e\u793aHZO\u7535\u5bb9\u5668\u7684P-V\u6ede\u540e\u56de\u7ebf\uff0c\u5e76\u5728\u4ecb\u7535\u51fb\u7a7f\u540e\u53ef\u89c6\u5316\u5bfc\u7535\u7ec6\u4e1d\uff0c\u8fd8\u80fd\u89c2\u5bdf\u6c27\u5316\u7269\u534a\u5bfc\u4f53\u7684\u6781\u5316\u5bf9\u6bd4\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u5f00\u53d1\u4e00\u79cd\u80fd\u53ef\u89c6\u5316\u5bfc\u7535\u7ec6\u4e1d\u3001\u94c1\u7535\u6781\u5316\u5bf9\u6bd4\u5ea6\u548c\u7535\u5b50\u6001\u5206\u5e03\u7684\u5de5\u5177\uff0c\u4ee5\u7528\u4e8e\u7814\u7a76\u94c1\u7535\u5668\u4ef6\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u79cd\u8054\u7528\u94c1\u7535\u8868\u5f81\u7cfb\u7edf\u7684operando\u6fc0\u5149\u5149\u7535\u5b50\u80fd\u663e\u5fae\u955c\uff08laser-PEEM\uff09\u3002\u4f7f\u7528Sawyer-Tower\u7535\u8def\u6d4b\u91cf\u4e86\u94c1\u7535\u5668\u4ef6\u7684\u6781\u5316-\u7535\u538b\uff08P-V\uff09\u7279\u6027\u3002", "result": "\u6210\u529f\u83b7\u5f97\u4e86\u5305\u542bHf$_{0.5}$Zr$_{0.5}$O$_2$\uff08HZO\uff09\u7684\u94c1\u7535\u7535\u5bb9\u5668\u7684\u6e05\u6670\u7684P-V\u6ede\u540e\u56de\u7ebf\uff0c\u5e76\u91cd\u73b0\u4e86HZO\u7535\u5bb9\u5668\u5178\u578b\u7684\u573a\u5faa\u73af\u7279\u6027\u3002\u5728\u7531\u573a\u5faa\u73af\u5e94\u529b\u5f15\u8d77\u7684\u4ecb\u7535\u51fb\u7a7f\u540e\uff0c\u5728\u65e0\u4efb\u4f55\u7834\u574f\u6027\u5904\u7406\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u9876\u7535\u6781\u53ef\u89c6\u5316\u4e86\u5bfc\u7535\u7ec6\u4e1d\u3002\u6b64\u5916\uff0c\u8fd8\u6210\u529f\u89c2\u5bdf\u5230\u4e86\u6c27\u5316\u7269\u534a\u5bfc\u4f53\uff08InZnO$_x$\uff09\u9876\u7535\u6781\u7684\u6781\u5316\u5bf9\u6bd4\u5ea6\u3002", "conclusion": "\u8be5operando\u6fc0\u5149-PEEM\u7cfb\u7edf\u53ef\u4f5c\u4e3a\u4e00\u79cd\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\u4ecb\u7535\u51fb\u7a7f\u540e\u7684\u5bfc\u7535\u7ec6\u4e1d\u3001\u94c1\u7535\u6781\u5316\u5bf9\u6bd4\u5ea6\u4ee5\u53ca\u94c1\u7535\u5668\u4ef6\uff08\u5305\u62ec\u94c1\u7535\u573a\u6548\u5e94\u6676\u4f53\u7ba1\u548c\u94c1\u7535\u96a7\u9053\u7ed3\uff09\u4e2d\u5b9e\u73b0\u7684\u6750\u6599\u7684\u7535\u5b50\u6001\u5206\u5e03\u3002"}}
{"id": "2508.07163", "categories": ["cs.RO", "cs.AI", "cs.NE"], "pdf": "https://arxiv.org/pdf/2508.07163", "abs": "https://arxiv.org/abs/2508.07163", "authors": ["Kamal Acharya", "Iman Sharifi", "Mehul Lad", "Liang Sun", "Houbing Song"], "title": "Integrating Neurosymbolic AI in Advanced Air Mobility: A Comprehensive Survey", "comment": "9 pages, 4 figures, IJCAI-2025 (accepted)", "summary": "Neurosymbolic AI combines neural network adaptability with symbolic\nreasoning, promising an approach to address the complex regulatory,\noperational, and safety challenges in Advanced Air Mobility (AAM). This survey\nreviews its applications across key AAM domains such as demand forecasting,\naircraft design, and real-time air traffic management. Our analysis reveals a\nfragmented research landscape where methodologies, including Neurosymbolic\nReinforcement Learning, have shown potential for dynamic optimization but still\nface hurdles in scalability, robustness, and compliance with aviation\nstandards. We classify current advancements, present relevant case studies, and\noutline future research directions aimed at integrating these approaches into\nreliable, transparent AAM systems. By linking advanced AI techniques with AAM's\noperational demands, this work provides a concise roadmap for researchers and\npractitioners developing next-generation air mobility solutions.", "AI": {"tldr": "Neurosymbolic AI \u7ed3\u5408\u4e86\u795e\u7ecf\u7f51\u7edc\u548c\u7b26\u53f7\u63a8\u7406\uff0c\u5728\u5148\u8fdb\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\uff08AAM\uff09\u9886\u57df\u6709\u5e94\u7528\u524d\u666f\uff0c\u4f46\u7814\u7a76\u5c1a\u4e0d\u6210\u719f\uff0c\u5b58\u5728\u6311\u6218\u3002\u672c\u6587\u7efc\u8ff0\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b9\u5411\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u5148\u8fdb\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\uff08AAM\uff09\u590d\u6742\u7684\u76d1\u7ba1\u3001\u8fd0\u8425\u548c\u5b89\u5168\u6311\u6218\uff0c\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u9002\u5e94\u6027\u548c\u7b26\u53f7\u63a8\u7406\u7684\u7ed3\u5408\uff0c\u662f\u5b9e\u73b0\u53ef\u9760\u3001\u900f\u660e\u7684 AAM \u7cfb\u7edf\u7684\u5173\u952e\u3002", "method": "\u5bf9 Neurosymbolic AI \u5728 AAM \u5173\u952e\u9886\u57df\u7684\u5e94\u7528\u8fdb\u884c\u56de\u987e\u548c\u5206\u7c7b\uff0c\u5305\u62ec\u9700\u6c42\u9884\u6d4b\u3001\u98de\u673a\u8bbe\u8ba1\u548c\u5b9e\u65f6\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\u3002\u5206\u6790\u4e86 Neurosymbolic Reinforcement Learning \u7b49\u65b9\u6cd5\u5728\u52a8\u6001\u4f18\u5316\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e86\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "result": "Neurosymbolic AI \u5728 AAM \u9886\u57df\u7684\u5e94\u7528\u5c55\u73b0\u51fa\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u7684\u7814\u7a76\u5b58\u5728\u65b9\u6cd5\u8bba\u4e0d\u7edf\u4e00\u3001\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u5408\u89c4\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002\u6587\u7ae0\u5bf9\u73b0\u6709\u8fdb\u5c55\u8fdb\u884c\u4e86\u5206\u7c7b\uff0c\u5e76\u63d0\u4f9b\u4e86\u6848\u4f8b\u7814\u7a76\uff0c\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "conclusion": "Neurosymbolic AI \u5728\u5148\u8fdb\u7a7a\u4e2d\u4ea4\u901a\u7ba1\u7406\uff08AAM\uff09\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u76ee\u524d\u7684\u7814\u7a76\u4ecd\u5904\u4e8e\u788e\u7247\u5316\u72b6\u6001\uff0c\u9762\u4e34\u53ef\u6269\u5c55\u6027\u3001\u9c81\u68d2\u6027\u548c\u822a\u7a7a\u6807\u51c6\u5408\u89c4\u6027\u7b49\u6311\u6218\u3002\u672a\u6765\u7684\u7814\u7a76\u5e94\u7740\u91cd\u4e8e\u514b\u670d\u8fd9\u4e9b\u969c\u788d\uff0c\u4ee5\u5b9e\u73b0\u53ef\u9760\u3001\u900f\u660e\u7684 AAM \u7cfb\u7edf\u3002"}}
{"id": "2508.07845", "categories": ["cs.SI"], "pdf": "https://arxiv.org/pdf/2508.07845", "abs": "https://arxiv.org/abs/2508.07845", "authors": ["Mahmoud Fawzi", "Bj\u00f6rn Ross", "Walid Magdy"], "title": "Fabricating Holiness: Characterizing Religious Misinformation Circulators on Arabic Social Media", "comment": "accepted at ICWSM 2026 (to appear in AAAI Press)\n  @article{fawzi2026holiness, title={Fabricating Holiness: Characterizing\n  Religious Misinformation Circulators on Arabic Social Media}, author={Fawzi,\n  Mahmoud and Ross, Bj{\\\"o}rn and Magdy, Walid}, booktitle={Proceedings of the\n  International AAAI Conference on Web and Social Media}, volume={20},\n  year={2026} }", "summary": "Misinformation is a growing concern in a decade involving critical global\nevents. While social media regulation is mainly dedicated towards the detection\nand prevention of fake news and political misinformation, there is limited\nresearch about religious misinformation which has only been addressed through\nqualitative approaches. In this work, we study the spread of fabricated quotes\n(Hadith) that are claimed to belong to Prophet Muhammad (the prophet of Islam)\nas a case study demonstrating one of the most common religious misinformation\nforms on Arabic social media. We attempt through quantitative methods to\nunderstand the characteristics of social media users who interact with\nfabricated Hadith. We spotted users who frequently circulate fabricated Hadith\nand others who frequently debunk it to understand the main differences between\nthe two groups. We used Logistic Regression to automatically predict their\nbehaviors and analyzed its weights to gain insights about the characteristics\nand interests of each group. We find that both fabricated Hadith circulators\nand debunkers have generally a lot of ties to religious accounts. However,\ncirculators are identified by many accounts that follow the Shia branch of\nIslam, Sunni Islamic public figures from the gulf countries, and many Sunni\nnon-professional pages posting Islamic content. On the other hand, debunkers\nare identified by following academic Islamic scholars from multiple countries\nand by having more intellectual non-religious interests like charity, politics,\nand activism.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u91cf\u5316\u65b9\u6cd5\u5206\u6790\u4e86\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u4e0a\u4f2a\u9020\u5723\u8bad\u7684\u4f20\u64ad\uff0c\u8bc6\u522b\u4e86\u4f20\u64ad\u8005\u548c\u63ed\u7a7f\u8005\u7684\u4e0d\u540c\u7279\u5f81\u3002", "motivation": "\u7814\u7a76\u5b97\u6559\u65b9\u9762\u9519\u8bef\u4fe1\u606f\uff0c\u7279\u522b\u662f\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u4e0a\u4f2a\u9020\u5723\u8bad\u7684\u4f20\u64ad\uff0c\u586b\u8865\u4e86\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\uff0c\u5e76\u65e8\u5728\u901a\u8fc7\u91cf\u5316\u65b9\u6cd5\u7406\u89e3\u4e0e\u6b64\u7c7b\u4fe1\u606f\u4e92\u52a8\u7684\u7528\u6237\u7279\u5f81\u3002", "method": "\u4f7f\u7528\u903b\u8f91\u56de\u5f52\u6a21\u578b\u81ea\u52a8\u9884\u6d4b\u7528\u6237\u884c\u4e3a\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u6a21\u578b\u6743\u91cd\u6765\u6df1\u5165\u4e86\u89e3\u4f20\u64ad\u8005\u548c\u63ed\u7a7f\u8005\u7684\u7279\u5f81\u4e0e\u5174\u8da3\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4f20\u64ad\u8005\u548c\u63ed\u7a7f\u8005\u90fd\u4e0e\u5b97\u6559\u8d26\u6237\u6709\u8054\u7cfb\uff0c\u4f46\u4f20\u64ad\u8005\u66f4\u5173\u6ce8\u900a\u5c3c\u6d3e\u7684\u7279\u5b9a\u7fa4\u4f53\u548c\u5185\u5bb9\uff0c\u800c\u63ed\u7a7f\u8005\u5219\u66f4\u5173\u6ce8\u5b66\u672f\u548c\u5e7f\u6cdb\u7684\u975e\u5b97\u6559\u8bdd\u9898\u3002", "conclusion": "\u8be5\u7814\u7a76\u91cf\u5316\u5206\u6790\u4e86\u963f\u62c9\u4f2f\u793e\u4ea4\u5a92\u4f53\u4e0a\u4f2a\u9020\u5723\u8bad\u7684\u4f20\u64ad\uff0c\u5e76\u8bc6\u522b\u4e86\u4f20\u64ad\u8005\u548c\u63ed\u7a7f\u8005\u7684\u7279\u5f81\u3002\u4f20\u64ad\u8005\u503e\u5411\u4e8e\u5173\u6ce8\u6d77\u6e7e\u56fd\u5bb6\u7684\u900a\u5c3c\u6d3e\u516c\u4f17\u4eba\u7269\u548c\u53d1\u5e03\u4f0a\u65af\u5170\u5185\u5bb9\u7684\u900a\u5c3c\u6d3e\u975e\u4e13\u4e1a\u9875\u9762\uff0c\u800c\u63ed\u7a7f\u8005\u5219\u503e\u5411\u4e8e\u5173\u6ce8\u5b66\u672f\u4f0a\u65af\u5170\u5b66\u8005\uff0c\u5e76\u5bf9\u6148\u5584\u3001\u653f\u6cbb\u548c\u6fc0\u8fdb\u4e3b\u4e49\u7b49\u975e\u5b97\u6559\u8bdd\u9898\u6709\u66f4\u591a\u5174\u8da3\u3002"}}
{"id": "2508.06530", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06530", "abs": "https://arxiv.org/abs/2508.06530", "authors": ["Ming-Kun Xie", "Jia-Hao Xiao", "Gang Niu", "Lei Feng", "Zhiqiang Kou", "Min-Ling Zhang", "Masashi Sugiyama"], "title": "What Makes \"Good\" Distractors for Object Hallucination Evaluation in Large Vision-Language Models?", "comment": null, "summary": "Large Vision-Language Models (LVLMs), empowered by the success of Large\nLanguage Models (LLMs), have achieved impressive performance across domains.\nDespite the great advances in LVLMs, they still suffer from the unavailable\nobject hallucination issue, which tends to generate objects inconsistent with\nthe image content. The most commonly used Polling-based Object Probing\nEvaluation (POPE) benchmark evaluates this issue by sampling negative\ncategories according to category-level statistics, \\textit{e.g.}, category\nfrequencies and co-occurrence. However, with the continuous advancement of\nLVLMs, the POPE benchmark has shown diminishing effectiveness in assessing\nobject hallucination, as it employs a simplistic sampling strategy that\noverlooks image-specific information and restricts distractors to negative\nobject categories only. In this paper, we introduce the Hallucination\nsearching-based Object Probing Evaluation (HOPE) benchmark, aiming to generate\nthe most misleading distractors (\\textit{i.e.}, non-existent objects or\nincorrect image descriptions) that can trigger hallucination in LVLMs, which\nserves as a means to more rigorously assess their immunity to hallucination. To\nexplore the image-specific information, the content-aware hallucination\nsearching leverages Contrastive Language-Image Pre-Training (CLIP) to\napproximate the predictive behavior of LVLMs by selecting negative objects with\nthe highest predicted likelihood as distractors. To expand the scope of\nhallucination assessment, the description-based hallucination searching\nconstructs highly misleading distractors by pairing true objects with false\ndescriptions. Experimental results show that HOPE leads to a precision drop of\nat least 9\\% and up to 23\\% across various state-of-the-art LVLMs,\nsignificantly outperforming POPE in exposing hallucination vulnerabilities. The\ncode is available at https://github.com/xiemk/HOPE.", "AI": {"tldr": "HOPE\u662f\u4e00\u4e2a\u65b0\u57fa\u51c6\uff0c\u901a\u8fc7\u751f\u6210\u66f4\u5177\u6b3a\u9a97\u6027\u7684\u5e72\u6270\u9879\uff08\u5982\u4e0d\u5339\u914d\u7684\u7269\u4f53\u6216\u63cf\u8ff0\uff09\uff0c\u6bd4POPE\u66f4\u80fd\u6709\u6548\u5730\u68c0\u6d4b\u51fa\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684POPE\u57fa\u51c6\u5728\u8bc4\u4f30LVLM\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\u65f6\uff0c\u7531\u4e8e\u5176\u7b80\u5316\u7684\u91c7\u6837\u7b56\u7565\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u56fe\u50cf\u7279\u5f02\u6027\u4fe1\u606f\uff0c\u5e76\u4e14\u4ec5\u9650\u4e8e\u8d1f\u9762\u7269\u4f53\u7c7b\u522b\uff0c\u5bfc\u81f4\u8bc4\u4f30\u6548\u679c\u4e0b\u964d\u3002\u9700\u8981\u4e00\u4e2a\u66f4\u6709\u6548\u7684\u57fa\u51c6\u6765\u66f4\u4e25\u683c\u5730\u8bc4\u4f30LVLM\u7684\u5e7b\u89c9\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHOPE\uff08Hallucination searching-based Object Probing Evaluation\uff09\u7684\u65b0\u578b\u57fa\u51c6\u3002HOPE\u91c7\u7528\u5185\u5bb9\u611f\u77e5\u548c\u63cf\u8ff0\u9a71\u52a8\u7684\u5e7b\u89c9\u641c\u7d22\u7b56\u7565\uff0c\u5229\u7528CLIP\u6a21\u578b\u5bfb\u627e\u4e0e\u56fe\u50cf\u5185\u5bb9\u6700\u76f8\u5173\u7684\u8bef\u5bfc\u6027\u8d1f\u9762\u7269\u4f53\uff0c\u5e76\u6784\u9020\u5305\u542b\u771f\u5b9e\u7269\u4f53\u4f46\u63cf\u8ff0\u9519\u8bef\u7684\u5e72\u6270\u9879\uff0c\u4ee5\u66f4\u4e25\u683c\u5730\u8bc4\u4f30LVLM\u7684\u5e7b\u89c9\u514d\u75ab\u529b\u3002", "result": "HOPE\u57fa\u51c6\u5728\u591a\u79cd\u5148\u8fdbLVLM\u4e0a\u5b9e\u73b0\u4e86\u81f3\u5c119%\u523023%\u7684\u7cbe\u5ea6\u4e0b\u964d\uff0c\u663e\u8457\u4f18\u4e8ePOPE\u57fa\u51c6\uff0c\u80fd\u66f4\u6709\u6548\u5730\u66b4\u9732LVLM\u7684\u5e7b\u89c9\u8106\u5f31\u6027\u3002", "conclusion": "HOPE\u57fa\u51c6\u5728\u8bc4\u4f30\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLM\uff09\u7684\u5bf9\u8c61\u5e7b\u89c9\u95ee\u9898\u4e0a\uff0c\u901a\u8fc7\u751f\u6210\u8bef\u5bfc\u6027\u5e72\u6270\u9879\uff0c\u6bd4\u73b0\u6709\u7684POPE\u57fa\u51c6\u66f4\u6709\u6548\uff0c\u80fd\u663e\u8457\u964d\u4f4e\u6a21\u578b\u7cbe\u5ea6\uff0c\u66b4\u9732\u5176\u5e7b\u89c9\u6f0f\u6d1e\u3002"}}
{"id": "2508.06994", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.06994", "abs": "https://arxiv.org/abs/2508.06994", "authors": ["Yimeng Sun", "Zhaohao Ding", "Payman Dehghanian", "Fei Teng"], "title": "Learning-Enabled Adaptive Power Capping Scheme for Cloud Data Centers", "comment": null, "summary": "The rapid growth of the digital economy and artificial intelligence has\ntransformed cloud data centers into essential infrastructure with substantial\nenergy consumption and carbon emission, necessitating effective energy\nmanagement. However, existing methods face challenges such as incomplete\ninformation, uncertain parameters, and dynamic environments, which hinder their\nreal-world implementation. This paper proposes an adaptive power capping\nframework tailored to cloud data centers. By dynamically setting the energy\nconsumption upper bound, the power load of data centers can be reshaped to\nalign with the electricity price or other market signals. To this end, we\nformulate the power capping problem as a partially observable Markov decision\nprocess. Subsequently, we develop an uncertainty-aware model-based\nreinforcement learning (MBRL) method to perceive the cloud data center\noperational environment and optimize power-capping decisions. By incorporating\na two-stage uncertainty-aware optimization algorithm into the MBRL, we improve\nits adaptability to the ever-changing environment. Additionally, we derive the\noptimality gap of the proposed scheme under finite iterations, ensuring\neffective decisions under complex and uncertain scenarios. The numerical\nexperiments validate the effectiveness of the proposed method using a cloud\ndata center operational environment simulator built on real-world production\ntraces from Alibaba, which demonstrates its potential as an efficient energy\nmanagement solution for cloud data centers.", "AI": {"tldr": "\u9488\u5bf9\u4e91\u6570\u636e\u4e2d\u5fc3\u9ad8\u80fd\u8017\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u81ea\u9002\u5e94\u529f\u7387\u5c01\u76d6\u6846\u67b6\uff0c\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u91cd\u5851\u529f\u7387\u8d1f\u8f7d\u4ee5\u9002\u5e94\u5e02\u573a\u4fe1\u53f7\uff0c\u5e76\u901a\u8fc7\u6a21\u62df\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6570\u5b57\u7ecf\u6d4e\u548c\u4eba\u5de5\u667a\u80fd\u7684\u5feb\u901f\u53d1\u5c55\u5bfc\u81f4\u4e91\u6570\u636e\u4e2d\u5fc3\u80fd\u8017\u548c\u78b3\u6392\u653e\u91cf\u5de8\u5927\uff0c\u9700\u8981\u6709\u6548\u7684\u80fd\u6e90\u7ba1\u7406\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4fe1\u606f\u4e0d\u5b8c\u6574\u3001\u53c2\u6570\u4e0d\u786e\u5b9a\u548c\u73af\u5883\u52a8\u6001\u53d8\u5316\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u901a\u8fc7\u5c06\u529f\u7387\u5c01\u76d6\u95ee\u9898\u5236\u5b9a\u4e3a\u90e8\u5206\u53ef\u89c2\u5bdf\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5e76\u5f00\u53d1\u4e00\u79cd\u8003\u8651\u4e0d\u786e\u5b9a\u6027\u7684\u57fa\u4e8e\u6a21\u578b\u5f3a\u5316\u5b66\u4e60\uff08MBRL\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e24\u9636\u6bb5\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u4f18\u5316\u7b97\u6cd5\uff0c\u4ee5\u9002\u5e94\u52a8\u6001\u53d8\u5316\u7684\u73af\u5883\u3002", "result": "\u901a\u8fc7\u57fa\u4e8e\u963f\u91cc\u5df4\u5df4\u771f\u5b9e\u751f\u4ea7\u8ffd\u8e2a\u6570\u636e\u6784\u5efa\u7684\u4e91\u6570\u636e\u4e2d\u5fc3\u8fd0\u884c\u73af\u5883\u6a21\u62df\u5668\u8fdb\u884c\u7684\u6570\u503c\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u4e91\u6570\u636e\u4e2d\u5fc3\u80fd\u6e90\u7ba1\u7406\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u529f\u7387\u5c01\u76d6\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8bbe\u7f6e\u80fd\u8017\u4e0a\u9650\u6765\u91cd\u5851\u6570\u636e\u4e2d\u5fc3\u7684\u529f\u7387\u8d1f\u8f7d\uff0c\u4ee5\u9002\u5e94\u7535\u4ef7\u6216\u5e02\u573a\u4fe1\u53f7\u3002"}}
{"id": "2508.07322", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07322", "abs": "https://arxiv.org/abs/2508.07322", "authors": ["Caitlin V. Hetherington", "Nila Mohan T. M.", "Shanu A. Shameem", "Warren F. Beck", "Benjamin G. Levine"], "title": "Conical Intersections Shed Light on Hot Carrier Cooling in Quantum Dots", "comment": null, "summary": "Experimental observations of vibronic coherences in electronically excited\ncolloidal semiconductor nanocrystals offer a window into the ultrafast dynamics\nof hot carrier cooling. In previous work, we showed that, in amine-passivated\nquantum dots (QDs), these coherences arise during relaxation through a cascade\nof conical intersections between electronically excited states. Here, we\ndemonstrate the generality of this framework by application to QDs with\nsurface-bound carboxylate ligands. A model involving a similar cascade of\nconical intersections accurately reproduces the frequencies of vibronic\ncoherences observed with broadband multidimensional spectroscopy. The impact of\nligands on the relaxation dynamics is attributed to two distinct mechanisms\ninvolving either electronic or vibrational coupling between the core and\nligands. Compared to the amine-passivated QDs studied previously, the\nelectronic coupling mechanism is less prominent in carboxylate-passivated QDs.\nFurthermore, comparison of acetate and formate ligands reveals that truncating\nthe ligand alkyl chains alters the relaxation behavior predicted by the model.", "AI": {"tldr": "\u80fa\u5c01\u7aef\u7684\u91cf\u5b50\u70b9\u548c\u7fa7\u9178\u76d0\u5c01\u7aef\u7684\u91cf\u5b50\u70b9\u90fd\u901a\u8fc7\u591a\u7ea7\u5706\u9525\u4ea4\u53c9\u8fdb\u884c\u5f1b\u8c6b\uff0c\u4f46\u914d\u4f53\u4e0e\u91cf\u5b50\u70b9\u6838\u5fc3\u4e4b\u95f4\u7684\u8026\u5408\u4e0d\u540c\uff0c\u5f71\u54cd\u4e86\u5f1b\u8c6b\u52a8\u529b\u5b66\u3002", "motivation": "\u7814\u7a76\u4e86\u8868\u9762\u7ed3\u5408\u7684\u7fa7\u9178\u76d0\u914d\u4f53\u5728\u91cf\u5b50\u70b9\uff08QDs\uff09\u7684\u8d85\u5feb\u52a8\u529b\u5b66\u548c\u70ed\u8f7d\u6d41\u5b50\u51b7\u5374\u4e2d\u7684\u4f5c\u7528\uff0c\u4ee5\u8bc1\u660e\u4e4b\u524d\u63d0\u51fa\u7684\u6d89\u53ca\u591a\u7ea7\u5706\u9525\u4ea4\u53c9\u7684\u7535\u5b50\u6fc0\u53d1\u6001\u5f1b\u8c6b\u6a21\u578b\u5177\u6709\u666e\u904d\u6027\u3002", "method": "\u901a\u8fc7\u5e94\u7528\u4e00\u4e2a\u6d89\u53ca\u591a\u7ea7\u5706\u9525\u4ea4\u53c9\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u5730\u518d\u73b0\u901a\u8fc7\u5bbd\u5e26\u591a\u7ef4\u5149\u8c31\u89c2\u5bdf\u5230\u7684\u632f\u52a8\u76f8\u5e72\u7684\u9891\u7387\u3002", "result": "\u8bc1\u660e\u4e86\u591a\u7ea7\u5706\u9525\u4ea4\u53c9\u6a21\u578b\u4e5f\u9002\u7528\u4e8e\u7fa7\u9178\u76d0\u5c01\u7aef\u7684\u91cf\u5b50\u70b9\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u5206\u6790\u914d\u4f53\u4e0e\u91cf\u5b50\u70b9\u6838\u5fc3\u4e4b\u95f4\u7684\u7535\u5b50\u6216\u632f\u52a8\u8026\u5408\u6765\u89e3\u91ca\u914d\u4f53\u5bf9\u5f1b\u8c6b\u52a8\u529b\u5b66\u7684\u5f71\u54cd\u3002", "conclusion": "\u4e0e\u4e4b\u524d\u7814\u7a76\u7684\u80fa\u5c01\u7aef\u7684\u91cf\u5b50\u70b9\u76f8\u6bd4\uff0c\u7fa7\u9178\u76d0\u5c01\u7aef\u7684\u91cf\u5b50\u70b9\u4e2d\u7684\u7535\u5b50\u8026\u5408\u673a\u5236\u4e0d\u90a3\u4e48\u660e\u663e\u3002\u6b64\u5916\uff0c\u6bd4\u8f83\u4e59\u9178\u76d0\u548c\u7532\u9178\u76d0\u914d\u4f53\u8868\u660e\uff0c\u622a\u65ad\u914d\u4f53\u70f7\u57fa\u94fe\u4f1a\u6539\u53d8\u6a21\u578b\u9884\u6d4b\u7684\u5f1b\u8c6b\u884c\u4e3a\u3002"}}
{"id": "2508.07002", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07002", "abs": "https://arxiv.org/abs/2508.07002", "authors": ["Ze Wang", "Guoping Zhang", "Hongbo Xu"], "title": "Joint Beamforming Optimization for Pinching-Antenna Systems (PASS)-assisted Symbiotic Radio", "comment": null, "summary": "This paper investigates a novel downlink symbiotic radio (SR) framework\nempowered by the pinching antenna system (PASS), aiming to enhance both primary\nand secondary transmissions through reconfigurable antenna positioning. PASS\nconsists of multiple waveguides equipped with numerous low-cost pinching\nantennas (PAs), whose positions can be flexibly adjusted to simultaneously\nmanipulate large-scale path loss and signal phases.We formulate a joint\ntransmit and pinching beamforming optimization problem to maximize the\nachievable sum rate while satisfying the detection error probability constraint\nfor the IR and the feasible deployment region constraints for the PAs. This\nproblem is inherently nonconvex and highly coupled. To address it, two solution\nstrategies are developed. 1) A learning-aided gradient descent (LGD) algorithm\nis proposed, where the constrained problem is reformulated into a\ndifferentiable form and solved through end-to-end learning based on the\nprinciple of gradient descent. The PA position matrix is reparameterized to\ninherently satisfy minimum spacing constraints, while transmit power and\nwaveguide length limits are enforced via projection and normalization. 2) A\ntwo-stage optimization-based approach is designed, in which the transmit\nbeamforming is first optimized via successive convex approximation (SCA),\nfollowed by pinching beamforming optimization using a particle swarm\noptimization (PSO) search over candidate PA placements. The SCA-PSO algorithm\nachieves performance close to that of the element-wise method while\nsignificantly reducing computational complexity by exploring a randomly\ngenerated effective solution subspace, while further improving upon the LGD\nmethod by avoiding undesirable local optima.", "AI": {"tldr": "This paper introduces a new symbiotic radio framework using a pinching antenna system (PASS) to improve wireless transmissions. Two methods, LGD and SCA-PSO, are proposed to optimize the system, with SCA-PSO showing better performance and efficiency.", "motivation": "To investigate a novel downlink symbiotic radio (SR) framework empowered by the pinching antenna system (PASS), aiming to enhance both primary and secondary transmissions through reconfigurable antenna positioning.", "method": "Two solution strategies are developed: 1) A learning-aided gradient descent (LGD) algorithm where the problem is reformulated into a differentiable form and solved through end-to-end learning. 2) A two-stage optimization-based approach (SCA-PSO) where transmit beamforming is optimized via successive convex approximation (SCA), followed by pinching beamforming optimization using particle swarm optimization (PSO).", "result": "The SCA-PSO algorithm achieves performance close to that of the element-wise method while significantly reducing computational complexity and improving upon the LGD method by avoiding undesirable local optima.", "conclusion": "The paper proposes two strategies to optimize the symbiotic radio framework empowered by the pinching antenna system (PASS), aiming to enhance both primary and secondary transmissions. The proposed LGD and SCA-PSO algorithms show promising results, with SCA-PSO achieving performance close to the element-wise method while significantly reducing computational complexity and avoiding local optima."}}
{"id": "2508.07790", "categories": ["cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.07790", "abs": "https://arxiv.org/abs/2508.07790", "authors": ["Alessandro Abate", "Thom Badings", "Giuseppe De Giacomo", "Francesco Fabiano"], "title": "Best-Effort Policies for Robust Markov Decision Processes", "comment": null, "summary": "We study the common generalization of Markov decision processes (MDPs) with\nsets of transition probabilities, known as robust MDPs (RMDPs). A standard goal\nin RMDPs is to compute a policy that maximizes the expected return under an\nadversarial choice of the transition probabilities. If the uncertainty in the\nprobabilities is independent between the states, known as s-rectangularity,\nsuch optimal robust policies can be computed efficiently using robust value\niteration. However, there might still be multiple optimal robust policies,\nwhich, while equivalent with respect to the worst-case, reflect different\nexpected returns under non-adversarial choices of the transition probabilities.\nHence, we propose a refined policy selection criterion for RMDPs, drawing\ninspiration from the notions of dominance and best-effort in game theory.\nInstead of seeking a policy that only maximizes the worst-case expected return,\nwe additionally require the policy to achieve a maximal expected return under\ndifferent (i.e., not fully adversarial) transition probabilities. We call such\na policy an optimal robust best-effort (ORBE) policy. We prove that ORBE\npolicies always exist, characterize their structure, and present an algorithm\nto compute them with a small overhead compared to standard robust value\niteration. ORBE policies offer a principled tie-breaker among optimal robust\npolicies. Numerical experiments show the feasibility of our approach.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b56\u7565\u9009\u62e9\u6807\u51c6ORBE\uff0c\u7528\u4e8e\u6253\u7834RMDP\u4e2d\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u4e4b\u95f4\u7684\u50f5\u5c40\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6807\u51c6\u9c81\u68d2\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08RMDP\uff09\u7684\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u5728\u6700\u574f\u60c5\u51b5\u4e0b\u662f\u7b49\u4ef7\u7684\uff0c\u4f46\u5728\u975e\u5bf9\u6297\u6027\u9009\u62e9\u7684\u8f6c\u6362\u6982\u7387\u4e0b\uff0c\u53ef\u80fd\u53cd\u6620\u4e86\u4e0d\u540c\u7684\u671f\u671b\u56de\u62a5\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u6539\u8fdb\u7684\u7b56\u7565\u9009\u62e9\u6807\u51c6\u6765\u6253\u7834\u6700\u4f18\u9c81\u68d2\u7b56\u7565\u4e4b\u95f4\u7684\u50f5\u5c40\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u7b56\u7565\u9009\u62e9\u6807\u51c6\uff0c\u5373\u6700\u4f18\u9c81\u68d2\u5c3d\u529b\u800c\u4e3a\uff08ORBE\uff09\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5728\u4e0d\u540c\uff08\u5373\u975e\u5b8c\u5168\u5bf9\u6297\uff09\u7684\u8f6c\u6362\u6982\u7387\u4e0b\uff0c\u5b9e\u73b0\u4e86\u6700\u5927\u5316\u671f\u671b\u56de\u62a5\uff0c\u5e76\u8bc1\u660e\u4e86ORBE\u7b56\u7565\u7684\u5b58\u5728\u6027\u3001\u7ed3\u6784\u7279\u5f81\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u8ba1\u7b97\u7b97\u6cd5\u3002", "result": "ORBE\u7b56\u7565\u603b\u662f\u5b58\u5728\u7684\uff0c\u5176\u7ed3\u6784\u5f97\u5230\u4e86\u8868\u5f81\uff0c\u5e76\u4e14\u63d0\u51fa\u4e86\u4e00\u79cd\u4e0e\u6807\u51c6\u9c81\u68d2\u4ef7\u503c\u8fed\u4ee3\u76f8\u6bd4\u5f00\u9500\u5f88\u5c0f\u7684\u8ba1\u7b97\u7b97\u6cd5\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "ORBE\u7b56\u7565\u662f\u6807\u51c6\u9c81\u68d2\u7b56\u7565\u7684\u6539\u8fdb\uff0c\u5728\u4e0d\u5b8c\u5168\u5bf9\u6297\u7684\u8f6c\u6362\u6982\u7387\u4e0b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6700\u5927\u7684\u671f\u671b\u56de\u62a5\uff0c\u5e76\u4e14ORBE\u7b56\u7565\u603b\u662f\u5b58\u5728\u7684\u3002"}}
{"id": "2508.07823", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.07823", "abs": "https://arxiv.org/abs/2508.07823", "authors": ["Yang Hu"], "title": "Nearly Optimal Bounds for Stochastic Online Sorting", "comment": null, "summary": "In the online sorting problem, we have an array $A$ of $n$ cells, and receive\na stream of $n$ items $x_1,\\dots,x_n\\in [0,1]$. When an item arrives, we need\nto immediately and irrevocably place it into an empty cell. The goal is to\nminimize the sum of absolute differences between adjacent items, which is\ncalled the \\emph{cost} of the algorithm. It has been shown by Aamand,\nAbrahamsen, Beretta, and Kleist (SODA 2023) that when the stream\n$x_1,\\dots,x_n$ is generated adversarially, the optimal cost bound for any\ndeterministic algorithm is $\\Theta(\\sqrt{n})$.\n  In this paper, we study the stochastic version of online sorting, where the\ninput items $x_1,\\dots,x_n$ are sampled uniformly at random. Despite the\nintuition that the stochastic version should yield much better cost bounds, the\nprevious best algorithm for stochastic online sorting by Abrahamsen, Bercea,\nBeretta, Klausen and Kozma (ESA 2024) only achieves $\\tilde{O}(n^{1/4})$ cost,\nwhich seems far from optimal. We show that stochastic online sorting indeed\nallows for much more efficient algorithms, by presenting an algorithm that\nachieves expected cost $\\log n\\cdot 2^{O(\\log^* n)}$. We also prove a cost\nlower bound of $\\Omega(\\log n)$, thus show that our algorithm is nearly\noptimal.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5728\u7ebf\u6392\u5e8f\u7684\u968f\u673a\u7248\u672c\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u63a5\u8fd1\u6700\u4f18\u7684\u7b97\u6cd5\uff0c\u5176\u671f\u671b\u6210\u672c\u4e3a log n * 2^O(log* n)\u3002", "motivation": "\u7814\u7a76\u968f\u673a\u7248\u672c\u7684\u5728\u7ebf\u6392\u5e8f\u95ee\u9898\uff0c\u5176\u4e2d\u8f93\u5165\u9879\u662f\u5747\u5300\u968f\u673a\u62bd\u6837\u7684\u3002\u4e4b\u524d\u7684\u7b97\u6cd5\u672a\u80fd\u5145\u5206\u5229\u7528\u968f\u673a\u6027\uff0c\u4ec5\u8fbe\u5230 O(n^1/4) \u7684\u6210\u672c\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u6570 n \u4e58\u4ee5 2 \u7684 O(log* n) \u7684\u671f\u671b\u6210\u672c\uff0c\u5e76\u8bc1\u660e\u4e86\u5bf9\u6570 n \u7684\u6210\u672c\u4e0b\u754c\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a5\u8fd1\u6700\u4f18\u7684\u7b97\u6cd5\uff0c\u671f\u671b\u6210\u672c\u4e3a log n * 2^O(log* n)\uff0c\u5e76\u8bc1\u660e\u4e86 log n \u7684\u6210\u672c\u4e0b\u754c\u3002", "conclusion": "\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u6570 n \u4e58\u4ee5 2 \u7684 O(log* n) \u7684\u671f\u671b\u6210\u672c\uff0c\u5e76\u4e14\u8bc1\u660e\u4e86\u5bf9\u6570 n \u7684\u6210\u672c\u4e0b\u754c\uff0c\u4ece\u800c\u8868\u660e\u8be5\u7b97\u6cd5\u63a5\u8fd1\u6700\u4f18\u3002"}}
{"id": "2508.06601", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06601", "abs": "https://arxiv.org/abs/2508.06601", "authors": ["Kyle O'Brien", "Stephen Casper", "Quentin Anthony", "Tomek Korbak", "Robert Kirk", "Xander Davies", "Ishan Mishra", "Geoffrey Irving", "Yarin Gal", "Stella Biderman"], "title": "Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs", "comment": "https://deepignorance.ai/", "summary": "Open-weight AI systems offer unique benefits, including enhanced\ntransparency, open research, and decentralized access. However, they are\nvulnerable to tampering attacks which can efficiently elicit harmful behaviors\nby modifying weights or activations. Currently, there is not yet a robust\nscience of open-weight model risk management. Existing safety fine-tuning\nmethods and other post-training techniques have struggled to make LLMs\nresistant to more than a few dozen steps of adversarial fine-tuning. In this\npaper, we investigate whether filtering text about dual-use topics from\ntraining data can prevent unwanted capabilities and serve as a more\ntamper-resistant safeguard. We introduce a multi-stage pipeline for scalable\ndata filtering and show that it offers a tractable and effective method for\nminimizing biothreat proxy knowledge in LLMs. We pretrain multiple\n6.9B-parameter models from scratch and find that they exhibit substantial\nresistance to adversarial fine-tuning attacks on up to 10,000 steps and 300M\ntokens of biothreat-related text -- outperforming existing post-training\nbaselines by over an order of magnitude -- with no observed degradation to\nunrelated capabilities. However, while filtered models lack internalized\ndangerous knowledge, we find that they can still leverage such information when\nit is provided in context (e.g., via search tool augmentation), demonstrating a\nneed for a defense-in-depth approach. Overall, these findings help to establish\npretraining data curation as a promising layer of defense for open-weight AI\nsystems.", "AI": {"tldr": "\u901a\u8fc7\u8fc7\u6ee4\u9884\u8bad\u7ec3\u6570\u636e\u6765\u589e\u5f3a\u5f00\u653e\u6743\u91cdAI\u6a21\u578b\u5bf9\u7be1\u6539\u653b\u51fb\u7684\u62b5\u6297\u529b\uff0c\u6548\u679c\u663e\u8457\uff0c\u4f46\u4ecd\u9700\u7ed3\u5408\u5176\u4ed6\u9632\u5fa1\u63aa\u65bd\u3002", "motivation": "\u5f00\u653e\u6743\u91cdAI\u6a21\u578b\u867d\u7136\u5177\u6709\u900f\u660e\u5ea6\u3001\u5f00\u653e\u7814\u7a76\u548c\u53bb\u4e2d\u5fc3\u5316\u8bbf\u95ee\u7b49\u4f18\u70b9\uff0c\u4f46\u4e5f\u5bb9\u6613\u53d7\u5230\u7be1\u6539\u653b\u51fb\uff0c\u53ef\u80fd\u5bfc\u81f4\u6709\u5bb3\u884c\u4e3a\u3002\u73b0\u6709\u7684\u5b89\u5168\u5fae\u8c03\u65b9\u6cd5\u548c\u8bad\u7ec3\u540e\u6280\u672f\u5728\u62b5\u5fa1\u5bf9\u6297\u6027\u5fae\u8c03\u65b9\u9762\u6548\u679c\u6709\u9650\uff0c\u56e0\u6b64\u9700\u8981\u66f4\u6709\u6548\u7684\u98ce\u9669\u7ba1\u7406\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9636\u6bb5\u7684\u6570\u636e\u8fc7\u6ee4\u6d41\u6c34\u7ebf\uff0c\u7528\u4e8e\u8fc7\u6ee4\u6389\u8bad\u7ec3\u6570\u636e\u4e2d\u5173\u4e8e\u53cc\u91cd\u7528\u9014\u7684\u4e3b\u9898\uff0c\u4ee5\u63d0\u9ad8\u5f00\u653e\u6743\u91cdAI\u6a21\u578b\u5bf9\u7be1\u6539\u653b\u51fb\u7684\u62b5\u6297\u529b\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u8fc7\u6ee4\u53cc\u91cd\u7528\u9014\u4e3b\u9898\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u5bf9\u6297\u6027\u5fae\u8c03\u653b\u51fb\uff08\u9ad8\u8fbe10,000\u6b65\u548c3\u4ebf\u4e2a\u4e0e\u751f\u7269\u5a01\u80c1\u76f8\u5173\u7684token\uff09\u65b9\u9762\u8868\u73b0\u51fa\u5f88\u5f3a\u7684\u62b5\u6297\u529b\uff0c\u6548\u679c\u6bd4\u73b0\u6709\u65b9\u6cd5\u597d\u4e00\u4e2a\u6570\u91cf\u7ea7\u4ee5\u4e0a\uff0c\u4e14\u5728\u4e0d\u76f8\u5173\u7684\u80fd\u529b\u65b9\u9762\u6ca1\u6709\u9000\u5316\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u6a21\u578b\u5728\u5229\u7528\u5916\u90e8\u63d0\u4f9b\u7684\u4fe1\u606f\uff08\u5982\u901a\u8fc7\u641c\u7d22\u5de5\u5177\uff09\u65f6\uff0c\u4ecd\u7136\u53ef\u4ee5\u5229\u7528\u5371\u9669\u77e5\u8bc6\uff0c\u8868\u660e\u9700\u8981\u7eb5\u6df1\u9632\u5fa1\u7684\u65b9\u6cd5\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u901a\u8fc7\u6570\u636e\u8fc7\u6ee4\u6765\u7ba1\u7406\u5f00\u653e\u6743\u91cdAI\u6a21\u578b\u7684\u98ce\u9669\u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u5bf9\u5bf9\u6297\u6027\u653b\u51fb\u7684\u62b5\u6297\u80fd\u529b\uff0c\u4f46\u4ecd\u9700\u7ed3\u5408\u5176\u4ed6\u9632\u5fa1\u63aa\u65bd\u3002"}}
{"id": "2508.06767", "categories": ["cs.LG", "cs.AI", "cs.DC", "cs.MA", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06767", "abs": "https://arxiv.org/abs/2508.06767", "authors": ["Arman Dogru", "R. Irem Bor-Yaliniz", "Nimal Gamini Senarath"], "title": "PANAMA: A Network-Aware MARL Framework for Multi-Agent Path Finding in Digital Twin Ecosystems", "comment": null, "summary": "Digital Twins (DTs) are transforming industries through advanced data\nprocessing and analysis, positioning the world of DTs, Digital World, as a\ncornerstone of nextgeneration technologies including embodied AI. As robotics\nand automated systems scale, efficient data-sharing frameworks and robust\nalgorithms become critical. We explore the pivotal role of data handling in\nnext-gen networks, focusing on dynamics between application and network\nproviders (AP/NP) in DT ecosystems. We introduce PANAMA, a novel algorithm with\nPriority Asymmetry for Network Aware Multi-agent Reinforcement Learning (MARL)\nbased multi-agent path finding (MAPF). By adopting a Centralized Training with\nDecentralized Execution (CTDE) framework and asynchronous actor-learner\narchitectures, PANAMA accelerates training while enabling autonomous task\nexecution by embodied AI. Our approach demonstrates superior pathfinding\nperformance in accuracy, speed, and scalability compared to existing\nbenchmarks. Through simulations, we highlight optimized data-sharing strategies\nfor scalable, automated systems, ensuring resilience in complex, real-world\nenvironments. PANAMA bridges the gap between network-aware decision-making and\nrobust multi-agent coordination, advancing the synergy between DTs, wireless\nnetworks, and AI-driven automation.", "AI": {"tldr": "PANAMA \u662f\u4e00\u79cd\u7528\u4e8e\u6570\u5b57\u5b6a\u751f\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u65b0\u578b\u591a\u4e3b\u4f53\u8def\u5f84\u67e5\u627e\u7b97\u6cd5\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u5171\u4eab\u548c\u91c7\u7528 CTDE \u6846\u67b6\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u3002", "motivation": "\u968f\u7740\u673a\u5668\u4eba\u548c\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u89c4\u6a21\u4e0d\u65ad\u6269\u5927\uff0c\u5bf9\u9ad8\u6548\u6570\u636e\u5171\u4eab\u6846\u67b6\u548c\u9c81\u68d2\u7b97\u6cd5\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u6570\u636e\u5904\u7406\u5728\u4e0b\u4e00\u4ee3\u7f51\u7edc\u4e2d\u7684\u5173\u952e\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5173\u6ce8\u6570\u5b57\u5b6a\u751f (DT) \u751f\u6001\u7cfb\u7edf\u4e2d\u5e94\u7528\u7a0b\u5e8f\u63d0\u4f9b\u5546 (AP) \u548c\u7f51\u7edc\u63d0\u4f9b\u5546 (NP) \u4e4b\u95f4\u7684\u52a8\u6001\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PANAMA \u7684\u65b0\u9896\u7b97\u6cd5\uff0c\u5b83\u91c7\u7528\u5e26\u6709\u4f18\u5148\u7ea7\u4e0d\u5bf9\u79f0\u7684\u7f51\u7edc\u611f\u77e5\u591a\u4e3b\u4f53\u5f3a\u5316\u5b66\u4e60 (MARL) \u65b9\u6cd5\uff0c\u7528\u4e8e\u591a\u4e3b\u4f53\u8def\u5f84\u67e5\u627e (MAPF)\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u4e2d\u5fc3\u5316\u8bad\u7ec3\u4e0e\u53bb\u4e2d\u5fc3\u5316\u6267\u884c (CTDE) \u6846\u67b6\u548c\u5f02\u6b65 actor-\u5b66\u4e60\u5668\u67b6\u6784\uff0c\u4ee5\u52a0\u901f\u8bad\u7ec3\u5e76\u652f\u6301\u81ea\u4e3b\u4efb\u52a1\u6267\u884c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\uff0cPANAMA \u7b97\u6cd5\u5728\u8def\u5f84\u67e5\u627e\u65b9\u9762\u7684\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\u3002\u7814\u7a76\u8fd8\u5f3a\u8c03\u4e86\u7528\u4e8e\u53ef\u6269\u5c55\u81ea\u52a8\u5316\u7cfb\u7edf\u7684\u6570\u636e\u5171\u4eab\u4f18\u5316\u7b56\u7565\uff0c\u786e\u4fdd\u4e86\u5728\u590d\u6742\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "PANAMA \u7b97\u6cd5\u5728\u51c6\u786e\u6027\u3001\u901f\u5ea6\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u6709\u6548\u534f\u8c03\u4e86\u7f51\u7edc\u611f\u77e5\u51b3\u7b56\u548c\u591a\u4e3b\u4f53\u534f\u540c\uff0c\u4fc3\u8fdb\u4e86\u6570\u5b57\u5b6a\u751f\u3001\u65e0\u7ebf\u7f51\u7edc\u548c\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u81ea\u52a8\u5316\u4e4b\u95f4\u7684\u534f\u540c\u4f5c\u7528\u3002"}}
{"id": "2508.06929", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06929", "abs": "https://arxiv.org/abs/2508.06929", "authors": ["Qing Cheng", "Sergey V. Erohin", "Konstantin V. Larionov", "Bin Gan", "Pavel B. Sorokin", "Xiandong Xu"], "title": "Unveiling the Puzzle of Brittleness in Single Crystal Iridium", "comment": "36 pages, 16 figues", "summary": "Iridium is critical for extreme-environment applications due to its\nexceptional thermal stability and corrosion resistance, but its intrinsic\nbrittleness remains a decades-old puzzle. Combining atomic-resolution scanning\ntransmission electron microscopy, density first-principles calculations, and\ndiscrete dislocation dynamics simulations, we identify high-density, sessile\nFrank dislocation loops with zero-net Burgers vectors as the key mechanism.\nThese loops form via an energetically favorable transformation from mixed\nperfect dislocations under stress, a process unique to iridium among\nface-centered cubic metals. The immobile loops act as potent barriers,\ndrastically increasing yield strength and work hardening by impeding\ndislocation glide and consuming mobile dislocations. This dominance of these\nfindings deepens the understanding of iridium's brittleness and offers a\npathway for designing more ductile variants of this critical material.", "AI": {"tldr": "\u94f1\u7684\u8106\u6027\u4e3b\u8981\u662f\u7531\u4e00\u79cd\u72ec\u7279\u7684\u3001\u4e0d\u53ef\u52a8\u7684\u5f17\u5170\u514b\u4f4d\u9519\u73af\u9020\u6210\u7684\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u9700\u8981\u5ef6\u5c55\u6027\u7684\u5e94\u7528\u4e2d\u7684\u4f7f\u7528\u3002", "motivation": "\u89e3\u51b3\u94f1\u5728\u6781\u7aef\u73af\u5883\u4e0b\u5e94\u7528\u7684\u5173\u952e\u6750\u6599\uff0c\u4f46\u5176\u56fa\u6709\u7684\u8106\u6027\u662f\u4e00\u4e2a\u957f\u671f\u5b58\u5728\u7684\u96be\u9898\u3002", "method": "\u7ed3\u5408\u539f\u5b50\u5206\u8fa8\u7387\u626b\u63cf\u900f\u5c04\u7535\u5b50\u663e\u5fae\u955c\u3001\u5bc6\u5ea6\u6cdb\u51fd\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548c\u79bb\u6563\u4f4d\u9519\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u7814\u7a76\u4e86\u94f1\u7684\u8106\u6027\u673a\u5236\u3002", "result": "\u53d1\u73b0\u4e86\u9ad8\u5bc6\u5ea6\u3001\u4e0d\u53ef\u52a8\u7684\u96f6\u51c0\u67cf\u683c\u77e2\u91cf\u5f17\u5170\u514b\u4f4d\u9519\u73af\uff0c\u5b83\u4eec\u662f\u5bfc\u81f4\u94f1\u8106\u6027\u7684\u5173\u952e\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u963b\u788d\u4f4d\u9519\u6ed1\u79fb\u548c\u6d88\u8017\u4f4d\u9519\u6765\u63d0\u9ad8\u5c48\u670d\u5f3a\u5ea6\u548c\u52a0\u5de5\u786c\u5316\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\u9ad8\u5bc6\u5ea6\u7684\u3001\u4e0d\u53ef\u52a8\u7684\u96f6\u51c0\u67cf\u683c\u77e2\u91cf\u5f17\u5170\u514b\u4f4d\u9519\u73af\u662f\u5bfc\u81f4\u94f1\u8106\u6027\u7684\u5173\u952e\u673a\u5236\uff0c\u8fd9\u79cd\u673a\u5236\u5728\u9762\u5fc3\u7acb\u65b9\u91d1\u5c5e\u4e2d\u662f\u94f1\u72ec\u6709\u7684\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u4f18\u5316\u8bbe\u8ba1\u6765\u63d0\u9ad8\u6750\u6599\u7684\u5ef6\u5c55\u6027\u3002"}}
{"id": "2508.07605", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07605", "abs": "https://arxiv.org/abs/2508.07605", "authors": ["Zhong Zheng", "Michael E. Papka", "Zhiling Lan"], "title": "Coordinated Power Management on Heterogeneous Systems", "comment": null, "summary": "Performance prediction is essential for energy-efficient computing in\nheterogeneous computing systems that integrate CPUs and GPUs. However,\ntraditional performance modeling methods often rely on exhaustive offline\nprofiling, which becomes impractical due to the large setting space and the\nhigh cost of profiling large-scale applications. In this paper, we present\nOPEN, a framework consists of offline and online phases. The offline phase\ninvolves building a performance predictor and constructing an initial dense\nmatrix. In the online phase, OPEN performs lightweight online profiling, and\nleverages the performance predictor with collaborative filtering to make\nperformance prediction. We evaluate OPEN on multiple heterogeneous systems,\nincluding those equipped with A100 and A30 GPUs. Results show that OPEN\nachieves prediction accuracy up to 98.29\\%. This demonstrates that OPEN\neffectively reduces profiling cost while maintaining high accuracy, making it\npractical for power-aware performance modeling in modern HPC environments.\nOverall, OPEN provides a lightweight solution for performance prediction under\npower constraints, enabling better runtime decisions in power-aware computing\nenvironments.", "AI": {"tldr": "OPEN\u662f\u4e00\u4e2a\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u5f02\u6784\u8ba1\u7b97\u7cfb\u7edf\uff08\u5305\u62ecCPU\u548cGPU\uff09\u4e2d\u8fdb\u884c\u8282\u80fd\u7684\u6027\u80fd\u9884\u6d4b\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u79bb\u7ebf\u9636\u6bb5\u7684\u6027\u80fd\u9884\u6d4b\u5668\u548c\u5728\u7ebf\u9636\u6bb5\u7684\u8f7b\u91cf\u7ea7\u6027\u80fd\u5206\u6790\uff0c\u5b9e\u73b0\u4e86\u9ad8\u51c6\u786e\u7387\uff08\u9ad8\u8fbe98.29%\uff09\u548c\u4f4e\u5206\u6790\u6210\u672c\uff0c\u9002\u7528\u4e8e\u8282\u80fd\u8ba1\u7b97\u3002", "motivation": "\u4f20\u7edf\u7684\u6027\u80fd\u5efa\u6a21\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u8be6\u5c3d\u7684\u79bb\u7ebf\u6027\u80fd\u5206\u6790\uff0c\u4f46\u7531\u4e8e\u8bbe\u7f6e\u7a7a\u95f4\u5927\u4e14\u5206\u6790\u6210\u672c\u9ad8\uff0c\u5bf9\u4e8e\u5927\u89c4\u6a21\u5e94\u7528\u7a0b\u5e8f\u4e0d\u5b9e\u7528\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u9884\u6d4b\u6027\u80fd\u3002", "method": "OPEN\u6846\u67b6\u5305\u542b\u79bb\u7ebf\u548c\u5728\u7ebf\u4e24\u4e2a\u9636\u6bb5\u3002\u79bb\u7ebf\u9636\u6bb5\u6784\u5efa\u6027\u80fd\u9884\u6d4b\u5668\u5e76\u6784\u9020\u521d\u59cb\u7a20\u5bc6\u77e9\u9635\u3002\u5728\u7ebf\u9636\u6bb5\uff0cOPEN\u6267\u884c\u8f7b\u91cf\u7ea7\u7684\u5728\u7ebf\u6027\u80fd\u5206\u6790\uff0c\u5e76\u5229\u7528\u6027\u80fd\u9884\u6d4b\u5668\u548c\u534f\u540c\u8fc7\u6ee4\u8fdb\u884c\u6027\u80fd\u9884\u6d4b\u3002", "result": "OPEN\u5728\u5f02\u6784\u7cfb\u7edf\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5176\u9884\u6d4b\u51c6\u786e\u7387\u9ad8\u8fbe98.29%\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u5206\u6790\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u53ef\u7528\u4e8e\u73b0\u4ee3HPC\u73af\u5883\u7684\u8282\u80fd\u6027\u80fd\u5efa\u6a21\u3002", "conclusion": "OPEN\u901a\u8fc7\u5728\u5f02\u6784\u7cfb\u7edf\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728A100\u548cA30 GPU\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe98.29%\u7684\u9884\u6d4b\u51c6\u786e\u7387\uff0c\u6709\u6548\u964d\u4f4e\u4e86\u6027\u80fd\u5206\u6790\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u51c6\u786e\u6027\uff0c\u9002\u7528\u4e8e\u73b0\u4ee3\u9ad8\u6027\u80fd\u8ba1\u7b97\u73af\u5883\u4e2d\u7684\u8282\u80fd\u6027\u80fd\u5efa\u6a21\u3002"}}
{"id": "2508.06575", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06575", "abs": "https://arxiv.org/abs/2508.06575", "authors": ["Rui Zhou"], "title": "Efficient Safety Testing of Autonomous Vehicles via Adaptive Search over Crash-Derived Scenarios", "comment": null, "summary": "Ensuring the safety of autonomous vehicles (AVs) is paramount in their\ndevelopment and deployment. Safety-critical scenarios pose more severe\nchallenges, necessitating efficient testing methods to validate AVs safety.\nThis study focuses on designing an accelerated testing algorithm for AVs in\nsafety-critical scenarios, enabling swift recognition of their driving\ncapabilities. First, typical logical scenarios were extracted from real-world\ncrashes in the China In-depth Mobility Safety Study-Traffic Accident (CIMSS-TA)\ndatabase, obtaining pre-crash features through reconstruction. Second, Baidu\nApollo, an advanced black-box automated driving system (ADS) is integrated to\ncontrol the behavior of the ego vehicle. Third, we proposed an adaptive\nlarge-variable neighborhood-simulated annealing algorithm (ALVNS-SA) to\nexpedite the testing process. Experimental results demonstrate a significant\nenhancement in testing efficiency when utilizing ALVNS-SA. It achieves an\n84.00% coverage of safety-critical scenarios, with crash scenario coverage of\n96.83% and near-crash scenario coverage of 92.07%. Compared to genetic\nalgorithm (GA), adaptive large neighborhood-simulated annealing algorithm\n(ALNS-SA), and random testing, ALVNS-SA exhibits substantially higher coverage\nin safety-critical scenarios.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faALVNS-SA\u7b97\u6cd5\uff0c\u901a\u8fc7\u52a0\u901f\u6d4b\u8bd5\u63d0\u9ad8\u4e86\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u6d4b\u8bd5\u6548\u7387\u548c\u8986\u76d6\u7387\u3002", "motivation": "\u4e3a\u786e\u4fdd\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AVs\uff09\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u9700\u8981\u9ad8\u6548\u7684\u6d4b\u8bd5\u65b9\u6cd5\u6765\u9a8c\u8bc1\u5176\u9a7e\u9a76\u80fd\u529b\u3002", "method": "\u4eceCIMSS-TA\u6570\u636e\u5e93\u63d0\u53d6\u5178\u578b\u903b\u8f91\u573a\u666f\uff0c\u5e76\u901a\u8fc7Baidu Apollo\u96c6\u6210\u9ed1\u76d2\u81ea\u52a8\u9a7e\u9a76\u7cfb\u7edf\u8fdb\u884c\u884c\u4e3a\u63a7\u5236\uff0c\u6700\u540e\u5e94\u7528ALVNS-SA\u7b97\u6cd5\u8fdb\u884c\u52a0\u901f\u6d4b\u8bd5\u3002", "result": "ALVNS-SA\u7b97\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u6d4b\u8bd5\u6548\u7387\uff0c\u8fbe\u5230\u4e8684.00%\u7684\u5b89\u5168\u5173\u952e\u573a\u666f\u8986\u76d6\u7387\uff0c\u5176\u4e2d\u78b0\u649e\u573a\u666f\u8986\u76d6\u7387\u4e3a96.83%\uff0c\u63a5\u8fd1\u78b0\u649e\u573a\u666f\u8986\u76d6\u7387\u4e3a92.07%\uff0c\u4f18\u4e8e\u9057\u4f20\u7b97\u6cd5\u3001\u81ea\u9002\u5e94\u5927\u90bb\u57df\u6a21\u62df\u9000\u706b\u7b97\u6cd5\u548c\u968f\u673a\u6d4b\u8bd5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u81ea\u9002\u5e94\u5927\u53d8\u91cf\u90bb\u57df\u6a21\u62df\u9000\u706b\u7b97\u6cd5\uff08ALVNS-SA\uff09\uff0c\u4ee5\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\uff08AVs\uff09\u5728\u5b89\u5168\u5173\u952e\u573a\u666f\u4e0b\u7684\u6d4b\u8bd5\u6548\u7387\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6d4b\u8bd5\u8986\u76d6\u7387\u63d0\u5347\u3002"}}
{"id": "2508.06712", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06712", "abs": "https://arxiv.org/abs/2508.06712", "authors": ["W. A. Z\u00fa\u00f1iga-Galindo", "L. F. Chac\u00f3n-Cort\u00e9s"], "title": "Continuous-Time Quantum Markov Chains And Discretizations Of p-Adic Schr\u00f6dinger Equations: Comparisons And Simulations", "comment": null, "summary": "The continuous-time quantum walks (CTQWs) are a fundamental tool in the\ndevelopment of quantum algorithms. Recently, it was shown that discretizations\nof p-adic Schr\\\"odinger equations give rise to continuous-time quantum Markov\nchains (CTQMCs); this type of Markov chain includes the CTQWs constructed using\nadjacency matrices of graphs as a particular case. In this paper, we study a\nlarge class of p-adic Schr\\\"odinger equations and the associated CTQMCs by\ncomparing them with p-adic heat equations and the associated continuous-time\nMarkov chains (CTMCs). The comparison is done by a mathematical study of the\nmentioned equations, which requires, for instance, solving the initial value\nproblems attached to the mentioned equations, and through numerical\nsimulations. We conducted multiple simulations, including numerical\napproximations of the limiting distribution. Our simulations show that the\nlimiting distribution of quantum Markov chains is greater than the stationary\nprobability of their classical counterparts, for a large class of CTQMCs.", "AI": {"tldr": "p-adic\u859b\u5b9a\u8c14\u65b9\u7a0b\u4ea7\u751f\u7684\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u6781\u9650\u5206\u5e03\u901a\u5e38\u6bd4\u5176\u7ecf\u5178\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u5e73\u7a33\u6982\u7387\u66f4\u5927\u3002", "motivation": "\u4e3a\u4e86\u7406\u89e3p-adic\u859b\u5b9a\u8c14\u65b9\u7a0b\u4ea7\u751f\u7684CTQMCs\u4e0e\u7ecf\u5178\u7684CTMCs\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4ee5\u53caCTQMCs\u7684\u6027\u8d28\u3002", "method": "\u901a\u8fc7\u6570\u5b66\u7814\u7a76\uff08\u5305\u62ec\u6c42\u89e3\u521d\u503c\u95ee\u9898\uff09\u548c\u6570\u503c\u6a21\u62df\uff08\u5305\u62ec\u6781\u9650\u5206\u5e03\u7684\u6570\u503c\u903c\u8fd1\uff09\u6765\u6bd4\u8f83p-adic\u859b\u5b9a\u8c14\u65b9\u7a0b\u53ca\u5176CTQMCs\u4e0ep-adic\u70ed\u65b9\u7a0b\u53ca\u5176CTMCs\u3002", "result": "\u6570\u503c\u6a21\u62df\u663e\u793a\uff0c\u5bf9\u4e8e\u4e00\u7c7b\u5e7f\u6cdb\u7684CTQMCs\uff0c\u5176\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u6781\u9650\u5206\u5e03\u5927\u4e8e\u5176\u7ecf\u5178\u5bf9\u5e94\u7269\u7684\u5e73\u7a33\u6982\u7387\u3002", "conclusion": "\u901a\u8fc7\u6570\u5b66\u7814\u7a76\u548c\u6570\u503c\u6a21\u62df\uff0c\u6bd4\u8f83\u4e86p-adic\u859b\u5b9a\u8c14\u65b9\u7a0b\u53ca\u5176\u76f8\u5173\u7684\u8fde\u7eed\u65f6\u95f4\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u94fe\uff08CTQMCs\uff09\u4e0ep-adic\u70ed\u65b9\u7a0b\u53ca\u5176\u76f8\u5173\u7684\u8fde\u7eed\u65f6\u95f4\u9a6c\u5c14\u53ef\u592b\u94fe\uff08CTMCs\uff09\u3002\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u4e00\u7c7b\u5e7f\u6cdb\u7684CTQMCs\uff0c\u5176\u91cf\u5b50\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u6781\u9650\u5206\u5e03\u5927\u4e8e\u5176\u7ecf\u5178\u5bf9\u5e94\u7269\u7684\u5e73\u7a33\u6982\u7387\u3002"}}
{"id": "2508.07011", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.07011", "abs": "https://arxiv.org/abs/2508.07011", "authors": ["Zixiong Wang", "Jian Yang", "Yiwei Hu", "Milos Hasan", "Beibei Wang"], "title": "HiMat: DiT-based Ultra-High Resolution SVBRDF Generation", "comment": null, "summary": "Creating highly detailed SVBRDFs is essential for 3D content creation. The\nrise of high-resolution text-to-image generative models, based on diffusion\ntransformers (DiT), suggests an opportunity to finetune them for this task.\nHowever, retargeting the models to produce multiple aligned SVBRDF maps instead\nof just RGB images, while achieving high efficiency and ensuring consistency\nacross different maps, remains a challenge. In this paper, we introduce HiMat:\na memory- and computation-efficient diffusion-based framework capable of\ngenerating native 4K-resolution SVBRDFs. A key challenge we address is\nmaintaining consistency across different maps in a lightweight manner, without\nrelying on training new VAEs or significantly altering the DiT backbone (which\nwould damage its prior capabilities). To tackle this, we introduce the\nCrossStitch module, a lightweight convolutional module that captures inter-map\ndependencies through localized operations. Its weights are initialized such\nthat the DiT backbone operation is unchanged before finetuning starts. HiMat\nenables generation with strong structural coherence and high-frequency details.\nResults with a large set of text prompts demonstrate the effectiveness of our\napproach for 4K SVBRDF generation. Further experiments suggest generalization\nto tasks such as intrinsic decomposition.", "AI": {"tldr": "HiMat\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u751f\u6210\u9ad8\u5206\u8fa8\u7387SVBRDF\uff0c\u5e76\u89e3\u51b3\u4e86\u8de8\u5730\u56fe\u4e00\u81f4\u6027\u7684\u5173\u952e\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6280\u672f\u5728\u521b\u5efa\u9ad8\u5206\u8fa8\u7387SVBRDF\u65f6\uff0c\u5728\u6a21\u578b\u91cd\u5b9a\u5411\u3001\u6548\u7387\u548c\u5730\u56fe\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u7684\u6311\u6218\uff0c\u5229\u7528\u9ad8\u5206\u8fa8\u7387\u6587\u672c\u5230\u56fe\u50cf\u6269\u6563Transformer\uff08DiT\uff09\u6a21\u578b\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHiMat\u7684\u5185\u5b58\u548c\u8ba1\u7b97\u9ad8\u6548\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u62104K\u5206\u8fa8\u7387\u7684SVBRDF\u3002\u8be5\u6846\u67b6\u5f15\u5165\u4e86CrossStitch\u6a21\u5757\uff0c\u8fd9\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u5377\u79ef\u6a21\u5757\uff0c\u901a\u8fc7\u5c40\u90e8\u64cd\u4f5c\u6355\u6349\u5730\u56fe\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u4ee5\u5728\u4e0d\u540c\u5730\u56fe\u95f4\u4fdd\u6301\u4e00\u81f4\u6027\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u8bad\u7ec3VAE\u6216\u5927\u5e45\u4fee\u6539DiT\u9aa8\u5e72\u7f51\u7edc\u3002", "result": "HiMat\u80fd\u591f\u751f\u6210\u5177\u6709\u9ad8\u5ea6\u7ed3\u6784\u4e00\u81f4\u6027\u548c\u9ad8\u9891\u7ec6\u8282\u76844K\u5206\u8fa8\u7387SVBRDF\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5927\u91cf\u6587\u672c\u63d0\u793a\u65f6\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5728\u56fa\u6709\u5206\u89e3\u7b49\u4efb\u52a1\u4e0a\u4e5f\u8868\u73b0\u51fa\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "HiMat\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u7684SVBRDF\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5f15\u5165CrossStitch\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u591a\u5730\u56fe\u4e4b\u95f4\u4fdd\u6301\u4e00\u81f4\u6027\u7684\u6311\u6218\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f7b\u91cf\u7ea7\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86HiMat\u57284K SVBRDF\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u5177\u6709\u6cdb\u5316\u5230\u5176\u4ed6\u4efb\u52a1\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06621", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06621", "abs": "https://arxiv.org/abs/2508.06621", "authors": ["Tomohiro Sawada", "Kartik Goyal"], "title": "Train It and Forget It: Merge Lists are Unnecessary for BPE Inference in Language Models", "comment": "Submitted to EMNLP", "summary": "Standard Byte-Pair Encoding (BPE) tokenization compresses text by pairing a\nlearned token vocabulary with a detailed merge list. Recent work has shown that\nthis merge list exposes a potential attack surface for extracting information\nabout language model's training data. In this paper, we explore the downstream\nimpact of BPE inference algorithms that do not rely on this merge list at all,\nand hence differ from the encoding process during BPE training. To address this\nquestion, we investigate two broad classes of BPE inference schemes that differ\nfrom BPE application during training: a) targeted deviation from merge-lists\nincluding random merge orders, and various corruptions of merge list involving\ndeletion/truncation, and b) non-targeted BPE inference algorithms that do not\ndepend on the merge list but focus on compressing the text either greedily or\nexactly. Extensive experiments across diverse language modeling tasks like\naccuracy-based QA benchmarks, machine translation, and open-ended generation\nreveal that while targeted deviation from the merge lists exhibits significant\ndegradation in language model performance, the non-targeted merge-list-free\ninference algorithms result in minimal impact on downstream performance that is\noften much smaller than expected. These findings pave way for simpler and\npotentially more privacy-preserving tokenization schemes that do not\ncatastrophically compromise model performance.", "AI": {"tldr": "BPE\u63a8\u7406\u7b97\u6cd5\u5728\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e0b\u6e38\u4efb\u52a1\u5f71\u54cd\u751a\u5fae\uff0c\u4e3a\u66f4\u7b80\u5355\u7684\u3001\u6ce8\u91cd\u9690\u79c1\u7684\u6587\u672c\u6807\u8bb0\u65b9\u6cd5\u63d0\u4f9b\u4e86\u53ef\u80fd\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u7684BPE\u63a8\u7406\u7b97\u6cd5\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\uff0c\u4ee5\u8bc4\u4f30\u5176\u5bf9\u8bed\u8a00\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u6cc4\u9732\u7684\u53ef\u80fd\u6027\u3002", "method": "\u672c\u7814\u7a76\u7684\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e24\u79cdBPE\u63a8\u7406\u7b97\u6cd5\uff1aa) \u9488\u5bf9\u6027\u5730\u504f\u79bb\u5408\u5e76\u5217\u8868\uff0c\u5305\u62ec\u968f\u673a\u5408\u5e76\u987a\u5e8f\u3001\u5220\u9664/\u622a\u65ad\u5408\u5e76\u5217\u8868\u7684\u5404\u79cd\u635f\u574f\uff1bb) \u975e\u9488\u5bf9\u6027\u5730\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\uff0c\u800c\u662f\u8d2a\u5a6a\u5730\u6216\u7cbe\u786e\u5730\u538b\u7f29\u6587\u672c\u7684BPE\u63a8\u7406\u7b97\u6cd5\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u9488\u5bf9\u6027\u5730\u504f\u79bb\u5408\u5e76\u5217\u8868\u4f1a\u5bfc\u81f4\u8bed\u8a00\u6a21\u578b\u6027\u80fd\u663e\u8457\u4e0b\u964d\uff1b\u800c\u975e\u9488\u5bf9\u6027\u5730\u3001\u4e0d\u4f9d\u8d56\u5408\u5e76\u5217\u8868\u7684\u63a8\u7406\u7b97\u6cd5\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u5f71\u54cd\u5f88\u5c0f\uff0c\u901a\u5e38\u8fdc\u5c0f\u4e8e\u9884\u671f\u3002", "conclusion": "BPE\u63a8\u7406\u7b97\u6cd5\u4e0eBPE\u8bad\u7ec3\u4e2d\u7684\u5e94\u7528\u65b9\u5f0f\u4e0d\u540c\uff0c\u53ef\u4ee5\u5b9e\u73b0\u66f4\u7b80\u5355\u7684\u3001\u6f5c\u5728\u7684\u9690\u79c1\u4fdd\u62a4\u7684\u6587\u672c\u6807\u8bb0\u65b9\u6cd5\uff0c\u4e14\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\u975e\u5e38\u5c0f\u3002"}}
{"id": "2508.08036", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.08036", "abs": "https://arxiv.org/abs/2508.08036", "authors": ["Xiaojia Han", "Wenjing Liu", "Qizhi Fang"], "title": "Truthful Two-Obnoxious-Facility Location Games with Optional Preferences and Minimum Distance Constraint", "comment": null, "summary": "In this paper, we study a truthful two-obnoxious-facility location problem,\nin which each agent has a private location in [0, 1] and a public optional\npreference over two obnoxious facilities, and there is a minimum distance\nconstraint d between the two facilities. Each agent wants to be as far away as\npossible from the facilities that affect her, and the utility of each agent is\nthe total distance from her to these facilities. The goal is to decide how to\nplace the facilities in [0, 1] so as to incentivize agents to report their\nprivate locations truthfully as well as maximize the social utility. First, we\nconsider the special setting where d = 0, that is, the two facilities can be\nlocated at any point in [0, 1]. We propose a deterministic strategyproof\nmechanism with approximation ratio of at most 4 and a randomized strategyproof\nmechanism with approximation ratio of at most 2, respectively. Then we study\nthe general setting. We propose a deterministic strategyproof mechanism with\napproximation ratio of at most 8 and a randomized strategyproof mechanism with\napproximation ratio of at most 4, respectively. Furthermore, we provide lower\nbounds of 2 and 14/13 on the approximation ratio for any deterministic and any\nrandomized strategyproof mechanism, respectively.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7814\u7a76\u4e86\u4e24\u7c7b\u201c\u538c\u6076\u578b\u201d\u8bbe\u65bd\u7684\u9009\u5740\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u6fc0\u52b1\u76f8\u5bb9\u7684\u673a\u5236\u8bbe\u8ba1\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u673a\u5236\u7684\u8fd1\u4f3c\u6700\u4f18\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u4ee3\u7406\u62e5\u6709\u79c1\u6709\u4f4d\u7f6e\u548c\u53ef\u9009\u7684\u4e24\u8bbe\u65bd\u504f\u597d\u7684\u60c5\u51b5\u4e0b\uff0c\u5982\u4f55\u653e\u7f6e\u8bbe\u65bd\u4ee5\u6fc0\u52b1\u4ee3\u7406\u771f\u5b9e\u62a5\u544a\u4f4d\u7f6e\u5e76\u6700\u5927\u5316\u793e\u4f1a\u6548\u7528\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86\u786e\u5b9a\u6027\u7b56\u7565\u8bc1\u660e\u673a\u5236\u548c\u968f\u673a\u6027\u7b56\u7565\u8bc1\u660e\u673a\u5236\uff0c\u5e76\u5206\u6790\u4e86\u5b83\u4eec\u5728d=0\u548cd>0\u4e24\u79cd\u60c5\u51b5\u4e0b\u7684\u8fd1\u4f3c\u6bd4\uff0c\u540c\u65f6\u7ed9\u51fa\u4e86\u76f8\u5e94\u673a\u5236\u7684\u8fd1\u4f3c\u6bd4\u4e0b\u754c\u3002", "result": "\u5728d=0\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u8fd1\u4f3c\u6bd4\u4e0d\u8d85\u8fc74\u7684\u786e\u5b9a\u6027\u673a\u5236\u548c\u8fd1\u4f3c\u6bd4\u4e0d\u8d85\u8fc72\u7684\u968f\u673a\u6027\u673a\u5236\u3002\u5728d>0\u7684\u666e\u904d\u60c5\u51b5\u4e0b\uff0c\u63d0\u51fa\u4e86\u8fd1\u4f3c\u6bd4\u4e0d\u8d85\u8fc78\u7684\u786e\u5b9a\u6027\u673a\u5236\u548c\u8fd1\u4f3c\u6bd4\u4e0d\u8d85\u8fc74\u7684\u968f\u673a\u6027\u673a\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u8bc1\u660e\u4e86\u786e\u5b9a\u6027\u673a\u5236\u8fd1\u4f3c\u6bd4\u7684\u4e0b\u754c\u4e3a2\uff0c\u968f\u673a\u6027\u673a\u5236\u8fd1\u4f3c\u6bd4\u7684\u4e0b\u754c\u4e3a14/13\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u5728\u6ee1\u8db3\u6700\u5c0f\u8ddd\u79bb\u7ea6\u675fd\u7684\u60c5\u51b5\u4e0b\uff0c\u89e3\u51b3\u4e24\u8bbe\u65bd\u9009\u5740\u95ee\u9898\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u5e76\u7ed9\u51fa\u4e86\u786e\u5b9a\u6027\u548c\u968f\u673a\u6027\u673a\u5236\u7684\u8fd1\u4f3c\u6bd4\u754c\u9650\u3002"}}
{"id": "2508.07896", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.app-ph", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07896", "abs": "https://arxiv.org/abs/2508.07896", "authors": ["Nolan Lassaline", "Camilla H. S\u00f8rensen", "Giulia Meucci", "Sander J. Linde", "Kian Latifi Yaghin", "Tuan K. Chau", "Damon J. Carrad", "Peter B\u00f8ggild", "Thomas S. Jespersen", "Timothy J. Booth"], "title": "Gradient Electronic Landscapes in van der Waals Heterostructures", "comment": null, "summary": "Two-dimensional (2D) materials such as graphene and hexagonal boron nitride\n(hBN) provide a versatile platform for quantum electronics. Experiments\ngenerally require encapsulating graphene within hBN flakes, forming a\nprotective van der Waals (vdW) heterostructure that preserves delicate\nproperties of the embedded crystal. To produce functional devices,\nheterostructures are typically shaped by electron beam lithography and etching,\nwhich has driven progress in 2D materials research. However, patterns are\nprimarily restricted to in-plane geometries such as boxes, holes, and stripes,\nlimiting opportunities for advanced architectures. Here, we use thermal\nscanning-probe lithography (tSPL) to produce smooth topographic landscapes in\nvdW heterostructures, controlling the thickness degree of freedom with\nnanometer precision. We electrically gate a sinusoidal topography to impose an\nelectric-field gradient on the graphene layer to spatially modulate\ncharge-carrier doping. We observe signatures of the landscape in transport\nmeasurements-resistance-peak spreading and commensurability\noscillations-establishing tSPL for tailoring high-quality quantum electronics.", "AI": {"tldr": "tSPL\u6280\u672f\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u4e8c\u7ef4\u6750\u6599\u7684\u539a\u5ea6\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cf\u5b50\u7535\u5b50\u5668\u4ef6\u7684\u5b9a\u5236\u3002", "motivation": "\u73b0\u6709\u7684\u4e8c\u7ef4\u6750\u6599\u5668\u4ef6\u52a0\u5de5\u6280\u672f\uff08\u5982\u7535\u5b50\u675f\u5149\u523b\u548c\u523b\u8680\uff09\u5728\u5668\u4ef6\u7684\u5e73\u9762\u51e0\u4f55\u5f62\u72b6\u4e0a\u5b58\u5728\u9650\u5236\uff0c\u963b\u788d\u4e86\u66f4\u5148\u8fdb\u5668\u4ef6\u67b6\u6784\u7684\u53d1\u5c55\u3002\u9700\u8981\u4e00\u79cd\u65b0\u7684\u6280\u672f\u6765\u7cbe\u786e\u63a7\u5236\u5668\u4ef6\u7684\u4e09\u7ef4\u5f62\u8c8c\uff0c\u4ee5\u5b9e\u73b0\u66f4\u9ad8\u7ea7\u7684\u529f\u80fd\u3002", "method": "\u5229\u7528\u70ed\u626b\u63cf\u63a2\u9488\u5149\u523b\uff08tSPL\uff09\u6280\u672f\u5728\u4e8c\u7ef4\u6750\u6599\u5f02\u8d28\u7ed3\u4e2d\u5236\u9020\u5149\u6ed1\u7684\u5f62\u8c8c\uff0c\u5e76\u7cbe\u786e\u63a7\u5236\u539a\u5ea6\u3002\u901a\u8fc7\u5bf9 the sinusoidal topography \u8fdb\u884c\u7535\u5b66\u95e8\u63a7\uff0c\u5728\u77f3\u58a8\u70ef\u5c42\u4e0a\u4ea7\u751f\u7535\u573a\u68af\u5ea6\uff0c\u7a7a\u95f4\u8c03\u5236\u8f7d\u6d41\u5b50\u63ba\u6742\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0ctSPL\u6280\u672f\u80fd\u591f\u5b9e\u73b0\u5bf9\u4e8c\u7ef4\u6750\u6599\u5f02\u8d28\u7ed3\u5f62\u8c8c\u7684\u7cbe\u786e\u63a7\u5236\uff0c\u80fd\u591f\u5236\u9020\u51fa\u5177\u6709\u7eb3\u7c73\u7ea7\u539a\u5ea6\u68af\u5ea6\u7684\u5149\u6ed1\u5f62\u8c8c\u3002\u901a\u8fc7\u7535\u5b66\u95e8\u63a7\u6b63\u5f26\u5f62\u8c8c\uff0c\u6210\u529f\u5730\u5728\u77f3\u58a8\u70ef\u5c42\u4e2d\u5b9e\u73b0\u4e86\u7535\u573a\u68af\u5ea6\u7684\u7a7a\u95f4\u8c03\u5236\u8f7d\u6d41\u5b50\u63ba\u6742\u3002\u8f93\u8fd0\u6d4b\u91cf\u7ed3\u679c\uff08\u7535\u963b\u5cf0\u5c55\u5bbd\u548c\u53ef\u516c\u5ea6\u632f\u8361\uff09\u8bc1\u5b9e\u4e86tSPL\u5728\u5b9a\u5236\u9ad8\u8d28\u91cf\u91cf\u5b50\u7535\u5b50\u5668\u4ef6\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "tSPL\u6280\u672f\u53ef\u7528\u4e8e\u7cbe\u786e\u63a7\u5236\u4e8c\u7ef4\u6750\u6599\u5f02\u8d28\u7ed3\u7684\u5f62\u8c8c\uff0c\u5b9e\u73b0\u9ad8\u8d28\u91cf\u91cf\u5b50\u7535\u5b50\u5668\u4ef6\u7684\u5b9a\u5236\u3002"}}
{"id": "2508.08080", "categories": ["cs.LG", "cs.NE", "stat.AP"], "pdf": "https://arxiv.org/pdf/2508.08080", "abs": "https://arxiv.org/abs/2508.08080", "authors": ["Cas Oude Hoekstra", "Floris den Hengst"], "title": "Symbolic Quantile Regression for the Interpretable Prediction of Conditional Quantiles", "comment": null, "summary": "Symbolic Regression (SR) is a well-established framework for generating\ninterpretable or white-box predictive models. Although SR has been successfully\napplied to create interpretable estimates of the average of the outcome, it is\ncurrently not well understood how it can be used to estimate the relationship\nbetween variables at other points in the distribution of the target variable.\nSuch estimates of e.g. the median or an extreme value provide a fuller picture\nof how predictive variables affect the outcome and are necessary in\nhigh-stakes, safety-critical application domains. This study introduces\nSymbolic Quantile Regression (SQR), an approach to predict conditional\nquantiles with SR. In an extensive evaluation, we find that SQR outperforms\ntransparent models and performs comparably to a strong black-box baseline\nwithout compromising transparency. We also show how SQR can be used to explain\ndifferences in the target distribution by comparing models that predict extreme\nand central outcomes in an airline fuel usage case study. We conclude that SQR\nis suitable for predicting conditional quantiles and understanding interesting\nfeature influences at varying quantiles.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06535", "categories": ["eess.IV", "cs.CV", "cs.LG", "F.2.2; I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06535", "abs": "https://arxiv.org/abs/2508.06535", "authors": ["Faisal Ahmed"], "title": "Transfer Learning with EfficientNet for Accurate Leukemia Cell Classification", "comment": "8 pages, 1 figure", "summary": "Accurate classification of Acute Lymphoblastic Leukemia (ALL) from peripheral\nblood smear images is essential for early diagnosis and effective treatment\nplanning. This study investigates the use of transfer learning with pretrained\nconvolutional neural networks (CNNs) to improve diagnostic performance. To\naddress the class imbalance in the dataset of 3,631 Hematologic and 7,644 ALL\nimages, we applied extensive data augmentation techniques to create a balanced\ntraining set of 10,000 images per class. We evaluated several models, including\nResNet50, ResNet101, and EfficientNet variants B0, B1, and B3. EfficientNet-B3\nachieved the best results, with an F1-score of 94.30%, accuracy of 92.02%,\nandAUCof94.79%,outperformingpreviouslyreported methods in the C-NMCChallenge.\nThesefindings demonstrate the effectiveness of combining data augmentation with\nadvanced transfer learning models, particularly EfficientNet-B3, in developing\naccurate and robust diagnostic tools for hematologic malignancy detection.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u4f7f\u7528EfficientNet-B3\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u767d\u8840\u75c5\u8bca\u65ad\u65b9\u9762\u53d6\u5f97\u4e86\u9ad8\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4ece\u5916\u5468\u8840\u6d82\u7247\u56fe\u50cf\u4e2d\u51c6\u786e\u5206\u7c7b\u6025\u6027\u6dcb\u5df4\u7ec6\u80de\u767d\u8840\u75c5\uff08ALL\uff09\u5bf9\u4e8e\u65e9\u671f\u8bca\u65ad\u548c\u6709\u6548\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u91c7\u7528\u4e86\u8fc1\u79fb\u5b66\u4e60\u548c\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5e76\u5e94\u7528\u4e86\u6570\u636e\u589e\u5f3a\u6280\u672f\u6765\u89e3\u51b3\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\u3002\u8bc4\u4f30\u4e86ResNet50\u3001ResNet101\u548cEfficientNet\u53d8\u4f53B0\u3001B1\u548cB3\u7b49\u6a21\u578b\u3002", "result": "EfficientNet-B3\u53d6\u5f97\u4e86\u6700\u4f73\u7ed3\u679c\uff0cF1\u5206\u6570\u4e3a94.30%\uff0c\u51c6\u786e\u7387\u4e3a92.02%\uff0cAUC\u4e3a94.79%\uff0c\u4f18\u4e8eC-NMC\u6311\u6218\u4e2d\u5148\u524d\u62a5\u9053\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408\u6570\u636e\u589e\u5f3a\u548c\u5148\u8fdb\u7684\u8fc1\u79fb\u5b66\u4e60\u6a21\u578b\uff08\u7279\u522b\u662fEfficientNet-B3\uff09\u53ef\u4ee5\u6709\u6548\u5730\u5f00\u53d1\u7528\u4e8e\u8840\u764c\u8bca\u65ad\u7684\u51c6\u786e\u4e14\u53ef\u9760\u7684\u8bca\u65ad\u5de5\u5177\u3002"}}
{"id": "2508.07121", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07121", "abs": "https://arxiv.org/abs/2508.07121", "authors": ["Alexandros E. Tzikas", "Lukas Fiechtner", "Arec Jamgochian", "Mykel J. Kochenderfer"], "title": "Distributionally Robust Control with Constraints on Linear Unidimensional Projections", "comment": "Presented at the 11th International Conference on Control, Decision\n  and Information Technologies (CoDIT 2025)", "summary": "Distributionally robust control is a well-studied framework for optimal\ndecision making under uncertainty, with the objective of minimizing an expected\ncost function over control actions, assuming the most adverse probability\ndistribution from an ambiguity set. We consider an interpretable and expressive\nclass of ambiguity sets defined by constraints on the expected value of\nfunctions of one-dimensional linear projections of the uncertain parameters.\nPrior work has shown that, under conditions, problems in this class can be\nreformulated as finite convex problems. In this work, we propose two iterative\nmethods that can be used to approximately solve problems of this class in the\ngeneral case. The first is an approximate algorithm based on best-response\ndynamics. The second is an approximate method that first reformulates the\nproblem as a semi-infinite program and then solves a relaxation. We apply our\nmethods to portfolio construction and trajectory planning scenarios.", "AI": {"tldr": "\u63d0\u51fa\u65b0\u7684\u5206\u5e03\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u7b97\u6cd5\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5b9e\u9645\u573a\u666f\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u786e\u5b9a\u6027\u6761\u4ef6\u4e0b\u5b9e\u73b0\u6700\u4f18\u51b3\u7b56\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u7279\u5b9a\u6a21\u7cca\u96c6\u65f6\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u8fed\u4ee3\u7b97\u6cd5\u6765\u8fd1\u4f3c\u6c42\u89e3\u95ee\u9898\uff1a\u4e00\u79cd\u57fa\u4e8e\u6700\u4f73\u54cd\u5e94\u52a8\u529b\u5b66\uff0c\u53e6\u4e00\u79cd\u5c06\u95ee\u9898\u91cd\u65b0\u8868\u8ff0\u4e3a\u534a\u65e0\u9650\u89c4\u5212\u5e76\u6c42\u89e3\u5176\u677e\u5f1b\u3002", "result": "\u6210\u529f\u5730\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u6709\u9650\u51f8\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e24\u79cd\u8fed\u4ee3\u7b97\u6cd5\u8fd1\u4f3c\u6c42\u89e3\uff0c\u5e94\u7528\u4e8e\u6295\u8d44\u7ec4\u5408\u6784\u5efa\u548c\u8f68\u8ff9\u89c4\u5212\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5206\u5e03\u9c81\u68d2\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u4e0d\u786e\u5b9a\u6027\u4e0b\u7684\u6700\u4f18\u51b3\u7b56\u95ee\u9898\u3002"}}
{"id": "2508.07364", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07364", "abs": "https://arxiv.org/abs/2508.07364", "authors": ["Tong-Yang Zhao", "An-Qi Wang", "Zhen-Tao Zhang", "Zheng-Yang Cao", "Xing-Yu Liu", "Zhi-Min Liao"], "title": "Magnetic Field Induced Quantum Metric Dipole in Dirac Semimetal Cd3As2", "comment": null, "summary": "The quantum geometry, comprising Berry curvature and quantum metric, plays a\nfundamental role in governing electron transport phenomena in solids. Recent\nstudies show that the quantum metric dipole drives scattering-free nonlinear\nHall effect in topological antiferromagnets, prompting the questions of whether\nthis effect can occur in nonmagnetic systems and be externally tuned by a\nmagnetic field. Our work addresses these frontiers by demonstrating that the\nquantum metric dipole is actively tuned by an external magnetic field to\ngenerate a time-reversal-odd nonlinear Hall response in a nonmagnetic\ntopological Dirac semimetal Cd3As2. Alongside the well-known\nchiral-anomaly-induced negative longitudinal magnetoresistance, an exotic\nnonlinear planar Hall effect emerges with increasing magnetic field. Careful\nscaling analysis indicates that this nonlinear planar Hall effect is controlled\nby the magnetic-field-modulated quantum metric dipole. Constructing a k.p\neffective model of the Dirac bands under Zeeman and orbital coupling, we derive\nthe evolution of the quantum metric dipole as a function of the magnetic field,\nproviding a comprehensive explanation of the experimental results. Our results\nestablish a band-structure-based strategy for engineering nonlinear\nmagnetotransport in nonmagnetic materials via the quantum metric dipole,\nopening a pathway toward magnetic-field-tunable nonlinear quantum devices.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u975e\u78c1\u6027\u6750\u6599\u4e2d\u5b9e\u73b0\u4e86\u7531\u78c1\u573a\u8c03\u63a7\u7684\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\uff0c\u4e3a\u5f00\u53d1\u53ef\u8c03\u8c10\u7684\u975e\u7ebf\u6027\u91cf\u5b50\u5668\u4ef6\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002", "motivation": "\u63a2\u7a76\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\u662f\u5426\u80fd\u5728\u975e\u78c1\u6027\u7cfb\u7edf\u4e2d\u9a71\u52a8\u65e0\u6563\u5c04\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\uff0c\u4ee5\u53ca\u662f\u5426\u80fd\u901a\u8fc7\u5916\u52a0\u78c1\u573a\u8fdb\u884c\u8c03\u63a7\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u5305\u542b\u585e\u66fc\u548c\u8f68\u9053\u8026\u5408\u7684\u72c4\u62c9\u514b\u80fd\u5e26\u7684k.p\u6709\u6548\u6a21\u578b\uff0c\u63a8\u5bfc\u51fa\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\u968f\u78c1\u573a\u7684\u6f14\u53d8\uff0c\u4ece\u800c\u5168\u9762\u89e3\u91ca\u5b9e\u9a8c\u7ed3\u679c\u3002", "result": "\u5728\u975e\u78c1\u6027\u62d3\u6251\u72c4\u62c9\u514b\u534a\u91d1\u5c5eCd3As2\u4e2d\uff0c\u901a\u8fc7\u5916\u52a0\u78c1\u573a\u6210\u529f\u8c03\u63a7\u4e86\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\uff0c\u4ea7\u751f\u4e86\u65f6\u95f4\u53cd\u6f14\u7834\u7f3a\u7684\u975e\u7ebf\u6027\u970d\u5c14\u54cd\u5e94\u3002\u5728\u9ad8\u78c1\u573a\u4e0b\uff0c\u9664\u4e86\u5df2\u77e5\u7684\u7531\u624b\u5f81\u53cd\u5e38\u5f15\u8d77\u8d1f\u7eb5\u5411\u78c1\u963b\u5916\uff0c\u8fd8\u51fa\u73b0\u4e86\u4e00\u79cd\u7531\u78c1\u573a\u8c03\u5236\u7684\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\u63a7\u5236\u7684\u5947\u5f02\u7684\u975e\u7ebf\u6027\u5e73\u9762\u970d\u5c14\u6548\u5e94\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u78c1\u573a\u8c03\u63a7\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\uff0c\u5728\u975e\u78c1\u6027\u62d3\u6251\u72c4\u62c9\u514b\u534a\u91d1\u5c5eCd3As2\u4e2d\u5b9e\u73b0\u4e86\u65f6\u95f4\u53cd\u6f14\u7834\u574f\u7684\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u901a\u8fc7\u91cf\u5b50\u5ea6\u91cf\u5076\u6781\u5b50\u5de5\u7a0b\u5316\u975e\u78c1\u6027\u6750\u6599\u4e2d\u975e\u7ebf\u6027\u78c1\u8f93\u8fd0\u7684\u5e26\u72b6\u7ed3\u6784\u7b56\u7565\uff0c\u4e3a\u53ef\u8c03\u8c10\u7684\u975e\u7ebf\u6027\u91cf\u5b50\u5668\u4ef6\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.07013", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07013", "abs": "https://arxiv.org/abs/2508.07013", "authors": ["Yufan Zhou", "Jingyi Li", "Wenkang Xu", "An Liu"], "title": "Robust Super-Resolution Compressive Sensing: A Two-timescale Alternating MAP Approach", "comment": null, "summary": "The problem of super-resolution compressive sensing (SR-CS) is crucial for\nvarious wireless sensing and communication applications. Existing methods often\nsuffer from limited resolution capabilities and sensitivity to\nhyper-parameters, hindering their ability to accurately recover sparse signals\nwhen the grid parameters do not lie precisely on a fixed grid and are close to\neach other. To overcome these limitations, this paper introduces a novel robust\nsuper-resolution compressive sensing algorithmic framework using a\ntwo-timescale alternating maximum a posteriori (MAP) approach. At the slow\ntimescale, the proposed framework iterates between a sparse signal estimation\nmodule and a grid update module. In the sparse signal estimation module, a\nhyperbolic-tangent prior distribution based variational Bayesian inference\n(tanh-VBI) algorithm with a strong sparsity promotion capability is adopted to\nestimate the posterior probability of the sparse vector and accurately identify\nactive grid components carrying primary energy under a dense grid.\nSubsequently, the grid update module utilizes the BFGS algorithm to refine\nthese low-dimensional active grid components at a faster timescale to achieve\nsuper-resolution estimation of the grid parameters with a low computational\ncost. The proposed scheme is applied to the channel extrapolation problem, and\nsimulation results demonstrate the superiority of the proposed scheme compared\nto baseline schemes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9c81\u68d2\u8d85\u5206\u8fa8\u7387\u538b\u7f29\u611f\u77e5\u7b97\u6cd5\u6846\u67b6\uff0c\u4f7f\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u4ea4\u66ff\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7tanh-VBI\u548cBFGS\u7b97\u6cd5\u63d0\u9ad8\u4e86\u7a00\u758f\u4fe1\u53f7\u6062\u590d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u538b\u7f29\u611f\u77e5\uff08SR-CS\uff09\u65b9\u6cd5\u5728\u5206\u8fa8\u7387\u548c\u8d85\u53c2\u6570\u654f\u611f\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u7f51\u683c\u53c2\u6570\u9760\u8fd1\u4e14\u4e0d\u7cbe\u786e\u5730\u4f4d\u4e8e\u56fa\u5b9a\u7f51\u683c\u4e0a\u65f6\uff0c\u96be\u4ee5\u51c6\u786e\u6062\u590d\u7a00\u758f\u4fe1\u53f7\u3002", "method": "\u4f7f\u7528\u53cc\u65f6\u95f4\u5c3a\u5ea6\u4ea4\u66ff\u6700\u5927\u540e\u9a8c\uff08MAP\uff09\u65b9\u6cd5\uff0c\u5305\u62ec\u7a00\u758f\u4fe1\u53f7\u4f30\u8ba1\u6a21\u5757\uff08\u57fa\u4e8etanh-VBI\u7b97\u6cd5\uff09\u548c\u7f51\u683c\u66f4\u65b0\u6a21\u5757\uff08\u57fa\u4e8eBFGS\u7b97\u6cd5\uff09\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u4fe1\u9053\u5916\u63a8\u95ee\u9898\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6848\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4fe1\u9053\u5916\u63a8\u95ee\u9898\u4e0a\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.08172", "categories": ["cs.LG", "cs.AI", "cs.LO"], "pdf": "https://arxiv.org/pdf/2508.08172", "abs": "https://arxiv.org/abs/2508.08172", "authors": ["Vincent Perreault", "Katsumi Inoue", "Richard Labib", "Alain Hertz"], "title": "Neural Logic Networks for Interpretable Classification", "comment": "21 pages, 6 figures, pre-print", "summary": "Traditional neural networks have an impressive classification performance,\nbut what they learn cannot be inspected, verified or extracted. Neural Logic\nNetworks on the other hand have an interpretable structure that enables them to\nlearn a logical mechanism relating the inputs and outputs with AND and OR\noperations. We generalize these networks with NOT operations and biases that\ntake into account unobserved data and develop a rigorous logical and\nprobabilistic modeling in terms of concept combinations to motivate their use.\nWe also propose a novel factorized IF-THEN rule structure for the model as well\nas a modified learning algorithm. Our method improves the state-of-the-art in\nBoolean networks discovery and is able to learn relevant, interpretable rules\nin tabular classification, notably on an example from the medical field where\ninterpretability has tangible value.", "AI": {"tldr": "\u795e\u7ecf\u903b\u8f91\u7f51\u7edc\u901a\u8fc7\u5f15\u5165NOT\u64cd\u4f5c\u3001\u504f\u5dee\u548c\u56e0\u5b50\u5316IF-THEN\u89c4\u5219\uff0c\u6539\u8fdb\u4e86\u5e03\u5c14\u7f51\u7edc\u53d1\u73b0\uff0c\u5e76\u5728\u8868\u683c\u5206\u7c7b\uff08\u5c24\u5176\u662f\u5728\u533b\u5b66\u9886\u57df\uff09\u4e2d\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u89c4\u5219\u5b66\u4e60\u3002", "motivation": "\u4f20\u7edf\u7684\u795e\u7ecf\u7f51\u7edc\u867d\u7136\u5177\u6709\u51fa\u8272\u7684\u5206\u7c7b\u6027\u80fd\uff0c\u4f46\u5176\u5b66\u4e60\u8fc7\u7a0b\u65e0\u6cd5\u88ab\u68c0\u67e5\u3001\u9a8c\u8bc1\u6216\u63d0\u53d6\u3002\u795e\u7ecf\u903b\u8f91\u7f51\u7edc\u5219\u5177\u6709\u53ef\u89e3\u91ca\u7684\u7ed3\u6784\uff0c\u80fd\u591f\u5b66\u4e60\u5230\u8f93\u5165\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u903b\u8f91\u673a\u5236\uff0c\u5e76\u8fdb\u884cAND\u548cOR\u64cd\u4f5c\u3002\u6211\u4eec\u901a\u8fc7\u5f15\u5165NOT\u64cd\u4f5c\u548c\u504f\u5dee\u6765\u6cdb\u5316\u8fd9\u4e9b\u7f51\u7edc\uff0c\u5e76\u7ed3\u5408\u6982\u5ff5\u7ec4\u5408\u7684\u4e25\u683c\u903b\u8f91\u548c\u6982\u7387\u5efa\u6a21\u6765\u9610\u8ff0\u5176\u4f18\u52bf\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u56e0\u5b50\u5316IF-THEN\u89c4\u5219\u7ed3\u6784\u4ee5\u53ca\u4e00\u79cd\u6539\u8fdb\u7684\u5b66\u4e60\u7b97\u6cd5\u3002", "result": "\u6211\u4eec\u80fd\u591f\u5b66\u4e60\u5230\u76f8\u5173\u7684\u3001\u53ef\u89e3\u91ca\u7684\u89c4\u5219\uff0c\u5c24\u5176\u662f\u5728\u533b\u5b66\u9886\u57df\u7684\u4e00\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u53ef\u89e3\u91ca\u6027\u5177\u6709\u5207\u5b9e\u7684\u4ef7\u503c\u3002", "conclusion": "\u6211\u4eec\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5e03\u5c14\u7f51\u7edc\u53d1\u73b0\u65b9\u9762\u6539\u8fdb\u4e86\u73b0\u6709\u6280\u672f\uff0c\u5e76\u4e14\u80fd\u591f\u5728\u8868\u683c\u5206\u7c7b\u4e2d\u5b66\u4e60\u5230\u76f8\u5173\u7684\u3001\u53ef\u89e3\u91ca\u7684\u89c4\u5219\uff0c\u5c24\u5176\u662f\u5728\u533b\u5b66\u9886\u57df\u7684\u4e00\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u53ef\u89e3\u91ca\u6027\u5177\u6709\u5207\u5b9e\u7684\u4ef7\u503c\u3002"}}
{"id": "2508.06736", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06736", "abs": "https://arxiv.org/abs/2508.06736", "authors": ["Alican Yilmaz", "Junyang Cai", "Serdar Kadioglu", "Bistra Dilkina"], "title": "ParBalans: Parallel Multi-Armed Bandits-based Adaptive Large Neighborhood Search", "comment": null, "summary": "Solving Mixed-Integer Programming (MIP) problems often requires substantial\ncomputational resources due to their combinatorial nature. Parallelization has\nemerged as a critical strategy to accelerate solution times and enhance\nscalability to tackle large, complex instances. This paper investigates the\nparallelization capabilities of Balans, a recently proposed multi-armed\nbandits-based adaptive large neighborhood search for MIPs. While Balans's\nmodular architecture inherently supports parallel exploration of diverse\nparameter configurations, this potential has not been thoroughly examined. To\naddress this gap, we introduce ParBalans, an extension that leverages both\nsolver-level and algorithmic-level parallelism to improve performance on\nchallenging MIP instances. Our experimental results demonstrate that ParBalans\nexhibits competitive performance compared to the state-of-the-art commercial\nsolver Gurobi, particularly on hard optimization benchmarks.", "AI": {"tldr": "This paper introduces ParBalans, a parallelized version of the Balans algorithm for solving Mixed-Integer Programming (MIP) problems. ParBalans shows competitive performance against Gurobi, a leading commercial solver, especially on difficult problem instances.", "motivation": "To address the need for accelerated solution times and enhanced scalability for Mixed-Integer Programming (MIP) problems by investigating and extending the parallelization capabilities of the Balans algorithm.", "method": "The paper investigates the parallelization capabilities of Balans, a multi-armed bandits-based adaptive large neighborhood search for MIPs, and introduces ParBalans, an extension that leverages both solver-level and algorithmic-level parallelism.", "result": "Experimental results show that ParBalans exhibits competitive performance compared to the state-of-the-art commercial solver Gurobi, particularly on hard optimization benchmarks.", "conclusion": "ParBalans, an extension of Balans that utilizes solver-level and algorithmic-level parallelism, demonstrates competitive performance against Gurobi on challenging MIP instances."}}
{"id": "2508.08078", "categories": ["cs.DS", "math.CO"], "pdf": "https://arxiv.org/pdf/2508.08078", "abs": "https://arxiv.org/abs/2508.08078", "authors": ["Jun-Ting Hsieh", "Daniel Z. Lee", "Sidhanth Mohanty", "Aaron Putterman", "Rachel Yun Zhang"], "title": "Sparsifying Cayley Graphs on Every Group", "comment": null, "summary": "A classic result in graph theory, due to Batson, Spielman, and Srivastava\n(STOC 2009) shows that every graph admits a $(1 \\pm \\varepsilon)$ cut (or\nspectral) sparsifier which preserves only $O(n / \\varepsilon^2)$ reweighted\nedges. However, when applying this result to \\emph{Cayley graphs}, the\nresulting sparsifier is no longer necessarily a Cayley graph -- it can be an\narbitrary subset of edges.\n  Thus, a recent line of inquiry, and one which has only seen minor progress,\nasks: for any group $G$, do all Cayley graphs over the group $G$ admit\nsparsifiers which preserve only $\\mathrm{polylog}(|G|)/\\varepsilon^2$ many\nre-weighted generators?\n  As our primary contribution, we answer this question in the affirmative,\npresenting a proof of the existence of such Cayley graph spectral sparsifiers,\nalong with an efficient algorithm for finding them. Our algorithm even extends\nto \\emph{directed} Cayley graphs, if we instead ask only for cut sparsification\ninstead of spectral sparsification.\n  We additionally study the sparsification of linear equations over non-abelian\ngroups. In contrast to the abelian case, we show that for non-abelian valued\nequations, super-polynomially many linear equations must be preserved in order\nto approximately preserve the number of satisfied equations for any input.\nTogether with our Cayley graph sparsification result, this provides a formal\nseparation between Cayley graph sparsification and sparsifying linear\nequations.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u4efb\u4f55\u7fa4G\u4e0a\u7684\u51ef\u83b1\u56fe\u90fd\u5b58\u5728\u5176\u8c31\u7a00\u758f\u5316\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u8fd8\u7814\u7a76\u4e86\u975e\u963f\u8d1d\u5c14\u7fa4\u4e0a\u7ebf\u6027\u65b9\u7a0b\u7684\u7a00\u758f\u5316\u95ee\u9898\uff0c\u5e76\u4e0e\u51ef\u83b1\u56fe\u7a00\u758f\u5316\u8fdb\u884c\u4e86\u533a\u5206\u3002", "motivation": "\u5c3d\u7ba1\u5df2\u6709\u56fe\u8bba\u4e2d\u7684\u7ecf\u5178\u7ed3\u679c\u8868\u660e\u4efb\u4f55\u56fe\u90fd\u627f\u8ba4\u4e00\u4e2a\uff081\u00b1\u03b5\uff09\u5272\uff08\u6216\u8c31\uff09\u7a00\u758f\u5316\u5668\uff0c\u4f46\u5f53\u5e94\u7528\u4e8e\u51ef\u83b1\u56fe\u65f6\uff0c\u7ed3\u679c\u7684\u7a00\u758f\u5316\u5668\u4e0d\u518d\u5fc5\u7136\u662f\u51ef\u83b1\u56fe\u3002\u8fd9\u4fc3\u4f7f\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u662f\u5426\u5b58\u5728\u51ef\u83b1\u56fe\u7684\u8c31\u7a00\u758f\u5316\u5668\uff0c\u5176\u7a00\u758f\u5316\u5668\u4ecd\u7136\u662f\u51ef\u83b1\u56fe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7b97\u6cd5\u6765\u5bfb\u627e\u51ef\u83b1\u56fe\u7684\u8c31\u7a00\u758f\u5316\u5668\uff0c\u8be5\u7b97\u6cd5\u9002\u7528\u4e8e\u6240\u6709\u7fa4G\uff0c\u5e76\u4e14\u53ef\u4ee5\u6269\u5c55\u5230\u6709\u5411\u51ef\u83b1\u56fe\u7684\u5272\u7a00\u758f\u5316\u3002", "result": "\u8bc1\u660e\u4e86\u5bf9\u4e8e\u4efb\u4f55\u7fa4G\uff0c\u5176\u51ef\u83b1\u56fe\u90fd\u5b58\u5728\u7a00\u758f\u5316\u5668\uff0c\u8be5\u7a00\u758f\u5316\u5668\u4ec5\u4fdd\u7559O(n/\u03b5^2)\u6761\u8fb9\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u4e00\u4e2a\u6709\u6548\u7684\u7b97\u6cd5\u627e\u5230\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u53d1\u73b0\u975e\u963f\u8d1d\u5c14\u7fa4\u4e0a\u7684\u7ebf\u6027\u65b9\u7a0b\u7684\u7a00\u758f\u5316\u4e0e\u963f\u8d1d\u5c14\u7fa4\u4e0a\u7684\u60c5\u51b5\u6709\u663e\u8457\u4e0d\u540c\uff0c\u9700\u8981\u4fdd\u7559\u8d85\u591a\u9879\u5f0f\u6570\u91cf\u7684\u65b9\u7a0b\u624d\u80fd\u8fd1\u4f3c\u4fdd\u6301\u6ee1\u8db3\u65b9\u7a0b\u7684\u6570\u91cf\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u8bc1\u660e\u4e86\u4efb\u4f55\u7fa4G\u4e0a\u7684\u6240\u6709\u51ef\u83b1\u56fe\u90fd\u5141\u8bb8\u4ec5\u4fdd\u7559\u591a\u5bf9\u6570\uff08|G|\uff09/\u03b5^2\u4e2a\u91cd\u65b0\u52a0\u6743\u7684\u751f\u6210\u5668\u7684\u7a00\u758f\u5316\u5668\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u7b97\u6cd5\u6765\u5bfb\u627e\u5b83\u4eec\u3002\u8be5\u7b97\u6cd5\u751a\u81f3\u53ef\u4ee5\u6269\u5c55\u5230\u6709\u5411\u51ef\u83b1\u56fe\uff0c\u5982\u679c\u53ea\u8981\u6c42\u5272\u7a00\u758f\u5316\u800c\u4e0d\u662f\u8c31\u7a00\u758f\u5316\u3002"}}
{"id": "2508.06614", "categories": ["cs.LG", "cond-mat.stat-mech", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06614", "abs": "https://arxiv.org/abs/2508.06614", "authors": ["Fangjun Hu", "Guangkuo Liu", "Yifan Zhang", "Xun Gao"], "title": "Local Diffusion Models and Phases of Data Distributions", "comment": "8+22 pages, 4+3 figures", "summary": "As a class of generative artificial intelligence frameworks inspired by\nstatistical physics, diffusion models have shown extraordinary performance in\nsynthesizing complicated data distributions through a denoising process\ngradually guided by score functions. Real-life data, like images, is often\nspatially structured in low-dimensional spaces. However, ordinary diffusion\nmodels ignore this local structure and learn spatially global score functions,\nwhich are often computationally expensive. In this work, we introduce a new\nperspective on the phases of data distributions, which provides insight into\nconstructing local denoisers with reduced computational costs. We define two\ndistributions as belonging to the same data distribution phase if they can be\nmutually connected via spatially local operations such as local denoisers.\nThen, we show that the reverse denoising process consists of an early trivial\nphase and a late data phase, sandwiching a rapid phase transition where local\ndenoisers must fail. To diagnose such phase transitions, we prove an\ninformation-theoretic bound on the fidelity of local denoisers based on\nconditional mutual information, and conduct numerical experiments in a\nreal-world dataset. This work suggests simpler and more efficient architectures\nof diffusion models: far from the phase transition point, we can use small\nlocal neural networks to compute the score function; global neural networks are\nonly necessary around the narrow time interval of phase transitions. This\nresult also opens up new directions for studying phases of data distributions,\nthe broader science of generative artificial intelligence, and guiding the\ndesign of neural networks inspired by physics concepts.", "AI": {"tldr": "\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u5229\u7528\u6570\u636e\u5206\u5e03\u7684\u76f8\u53d8\u6765\u53d8\u5f97\u66f4\u7b80\u5355\u3001\u66f4\u9ad8\u6548\uff0c\u5176\u4e2d\u5c40\u90e8\u964d\u566a\u5668\u5728\u8fdc\u79bb\u76f8\u53d8\u70b9\u65f6\u53ef\u4ee5\u66ff\u4ee3\u5168\u5c40\u964d\u566a\u5668\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u901a\u5e38\u5ffd\u7565\u4e86\u56fe\u50cf\u7b49\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u7684\u7a7a\u95f4\u5c40\u90e8\u7ed3\u6784\uff0c\u5e76\u5b66\u4e60\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u7684\u7a7a\u95f4\u5168\u5c40\u5f97\u5206\u51fd\u6570\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u89c6\u89d2\uff0c\u4ee5\u5f00\u53d1\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u7684\u5c40\u90e8\u53bb\u566a\u5668\uff0c\u4ece\u800c\u63d0\u9ad8\u6269\u6563\u6a21\u578b\u7684\u6548\u7387\u3002", "method": "\u901a\u8fc7\u5b9a\u4e49\u6570\u636e\u5206\u5e03\u76f8\u5e76\u8bc1\u660e\u53cd\u5411\u53bb\u566a\u8fc7\u7a0b\u5305\u542b\u65e9\u671f\u5e73\u51e1\u76f8\u3001\u665a\u671f\u6570\u636e\u76f8\u548c\u5feb\u901f\u76f8\u53d8\uff0c\u4ece\u800c\u5f15\u5165\u4e86\u5c40\u90e8\u53bb\u566a\u5668\u7684\u6982\u5ff5\u3002\u5229\u7528\u4fe1\u606f\u8bba\u754c\u9650\uff08\u57fa\u4e8e\u6761\u4ef6\u4e92\u4fe1\u606f\uff09\u6765\u8bca\u65ad\u76f8\u53d8\uff0c\u5e76\u901a\u8fc7\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u6570\u503c\u5b9e\u9a8c\u8fdb\u884c\u9a8c\u8bc1\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u91c7\u7528\u66f4\u7b80\u5355\u3001\u66f4\u9ad8\u6548\u7684\u67b6\u6784\u3002\u5c40\u90e8\u795e\u7ecf\u7f51\u7edc\u53ef\u7528\u4e8e\u8ba1\u7b97\u8fdc\u79bb\u76f8\u53d8\u70b9\u7684\u5f97\u5206\u51fd\u6570\uff0c\u800c\u5168\u5c40\u795e\u7ecf\u7f51\u7edc\u4ec5\u5728\u76f8\u53d8\u8fc7\u6e21\u7684\u72ed\u7a84\u65f6\u95f4\u95f4\u9694\u5185\u624d\u662f\u5fc5\u9700\u7684\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u57fa\u4e8e\u6570\u636e\u5206\u5e03\u76f8\u53d8\u7684\u65b0\u89c6\u89d2\uff0c\u53ef\u7528\u4e8e\u6784\u5efa\u8ba1\u7b97\u6210\u672c\u66f4\u4f4e\u3001\u7ed3\u6784\u66f4\u7b80\u5316\u7684\u6269\u6563\u6a21\u578b\u3002\u5c40\u90e8\u964d\u566a\u5668\u5728\u8fdc\u79bb\u76f8\u53d8\u70b9\u65f6\u662f\u6709\u6548\u7684\uff0c\u800c\u5168\u5c40\u964d\u566a\u5668\u4ec5\u5728\u76f8\u53d8\u70b9\u9644\u8fd1\u662f\u5fc5\u9700\u7684\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u4fe1\u606f\u8bba\u754c\u9650\u6765\u8bca\u65ad\u76f8\u53d8\uff0c\u5e76\u4e3a\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\u548c\u53d7\u7269\u7406\u5b66\u542f\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\u5f00\u8f9f\u4e86\u65b0\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.06803", "categories": ["cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06803", "abs": "https://arxiv.org/abs/2508.06803", "authors": ["Ziqi Liu", "Yangbin Chen", "Ziyang Zhou", "Yilin Li", "Mingxuan Hu", "Yushan Pan", "Zhijie Xu"], "title": "SEVADE: Self-Evolving Multi-Agent Analysis with Decoupled Evaluation for Hallucination-Resistant Irony Detection", "comment": null, "summary": "Sarcasm detection is a crucial yet challenging Natural Language Processing\ntask. Existing Large Language Model methods are often limited by\nsingle-perspective analysis, static reasoning pathways, and a susceptibility to\nhallucination when processing complex ironic rhetoric, which impacts their\naccuracy and reliability. To address these challenges, we propose **SEVADE**, a\nnovel **S**elf-**Ev**olving multi-agent **A**nalysis framework with\n**D**ecoupled **E**valuation for hallucination-resistant sarcasm detection. The\ncore of our framework is a Dynamic Agentive Reasoning Engine (DARE), which\nutilizes a team of specialized agents grounded in linguistic theory to perform\na multifaceted deconstruction of the text and generate a structured reasoning\nchain. Subsequently, a separate lightweight rationale adjudicator (RA) performs\nthe final classification based solely on this reasoning chain. This decoupled\narchitecture is designed to mitigate the risk of hallucination by separating\ncomplex reasoning from the final judgment. Extensive experiments on four\nbenchmark datasets demonstrate that our framework achieves state-of-the-art\nperformance, with average improvements of **6.75%** in Accuracy and **6.29%**\nin Macro-F1 score.", "AI": {"tldr": "SEVADE\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u81ea\u6f14\u5316\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7DARE\u548cRA\u6765\u89e3\u51b3LLM\u5728\u8bbd\u523a\u68c0\u6d4b\u4e2d\u7684\u5e7b\u89c9\u548c\u51c6\u786e\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684LLM\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u7684\u8bbd\u523a\u4fee\u8f9e\u65f6\uff0c\u5e38\u5e38\u53d7\u5230\u5355\u4e00\u89c6\u89d2\u5206\u6790\u3001\u9759\u6001\u63a8\u7406\u8def\u5f84\u548c\u6613\u4ea7\u751f\u5e7b\u89c9\u7684\u9650\u5236\uff0c\u8fd9\u5f71\u54cd\u4e86\u5b83\u4eec\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u7528\u4e8e\u6297\u5e7b\u89c9\u8bbd\u523a\u68c0\u6d4b\u7684\u81ea\u6f14\u5316\u591a\u667a\u80fd\u4f53\u5206\u6790\u6846\u67b6\uff08SEVADE\uff09\uff0c\u5176\u4e2d\u592e\u662f\u4e00\u4e2a\u52a8\u6001\u667a\u80fd\u4f53\u63a8\u7406\u5f15\u64ce\uff08DARE\uff09\u3002DARE\u5229\u7528\u57fa\u4e8e\u8bed\u8a00\u7406\u8bba\u7684\u4e13\u4e1a\u667a\u80fd\u4f53\u56e2\u961f\u6765\u89e3\u6784\u6587\u672c\u5e76\u751f\u6210\u7ed3\u6784\u5316\u63a8\u7406\u94fe\u3002\u4e00\u4e2a\u5355\u72ec\u7684\u8f7b\u91cf\u7ea7\u63a8\u7406\u4ef2\u88c1\u5668\uff08RA\uff09\u4ec5\u57fa\u4e8e\u6b64\u63a8\u7406\u94fe\u8fdb\u884c\u6700\u7ec8\u5206\u7c7b\u3002\u8fd9\u79cd\u5206\u79bb\u5f0f\u67b6\u6784\u65e8\u5728\u901a\u8fc7\u5c06\u590d\u6742\u63a8\u7406\u4e0e\u6700\u7ec8\u5224\u65ad\u5206\u79bb\u5f00\u6765\uff0c\u6765\u51cf\u8f7b\u5e7b\u89c9\u7684\u98ce\u9669\u3002", "result": "SEVADE\u6846\u67b6\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e866.75%\uff0c\u5b8fF1\u5206\u6570\u63d0\u9ad8\u4e866.29%\u3002", "conclusion": "SEVADE\u6846\u67b6\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u51c6\u786e\u7387\u63d0\u9ad8\u4e866.75%\uff0c\u5b8fF1\u5206\u6570\u63d0\u9ad8\u4e866.29%\u3002"}}
{"id": "2508.06930", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06930", "abs": "https://arxiv.org/abs/2508.06930", "authors": ["Giacomo Sesti", "Alberto Guandalini", "Andrea Ferretti", "Pino D'Amico", "Claudia Cardoso", "Massimo Rontani", "Daniele Varsano"], "title": "Efficient GW calculations for metals from an accurate ab initio polarizability", "comment": "18 pages 13 figures", "summary": "Despite its success in the study of spectroscopic properties, the $GW$ method\npresents specific methodological challenges when applied to systems with\nmetallic screening. Here, we present an efficient and fully ab-initio\nimplementation for the calculation of the screened potential, specifically\ndesigned for 3D and 2D metals. It combines a Monte Carlo integration with an\nappropriate interpolation of the screened potential between the calculated grid\npoints (W-av), complemented with an extrapolation to the long-wavelength limit,\nable to seamlessly account for the so-called intraband term. This method\ngreatly accelerates the convergence of GW calculations for metals while\nimproving their accuracy, due to the correct description of the intraband\ntransitions in the long wavelength limit, as shown here for 3D metals and doped\nmonolayers, such as MoS$_2$ and graphene. The use of W-av results in an\nexcellent agreement with ARPES measurements for monolayer doped MoS$_2$.\nFurthermore, for graphene we show that more robust results are found with the\nuse of higher-order Lorentzians in the description of the self-energy, together\nwith the solution of the QP equation beyond the linearized approximation.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684GW\u8ba1\u7b97\u65b9\u6cd5\uff08W-av\uff09\uff0c\u7528\u4e8e\u91d1\u5c5e\u4f53\u7cfb\uff0c\u80fd\u52a0\u901f\u6536\u655b\u5e76\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\u3002", "motivation": "GW\u65b9\u6cd5\u5728\u5149\u8c31\u6027\u8d28\u7814\u7a76\u4e2d\u867d\u6709\u6210\u529f\u5e94\u7528\uff0c\u4f46\u5728\u5e94\u7528\u4e8e\u5177\u6709\u91d1\u5c5e\u5c4f\u853d\u6548\u5e94\u7684\u4f53\u7cfb\u65f6\uff0c\u5b58\u5728\u7279\u5b9a\u7684\u65b9\u6cd5\u5b66\u6311\u6218\u3002\u672c\u6587\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u6709\u6548\u8ba1\u7b97\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u5168\u4ece\u5934\u7b97\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u5177\u6709\u91d1\u5c5e\u5c4f\u853d\u6548\u5e94\u76843D\u548c2D\u91d1\u5c5e\u4f53\u7cfb\u7684\u5c4f\u853d\u52bf\u3002\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u8499\u7279\u5361\u6d1b\u79ef\u5206\u3001\u5728\u8ba1\u7b97\u7f51\u683c\u70b9\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\uff08W-av\uff09\uff0c\u5e76\u5bf9\u957f\u6ce2\u6781\u9650\u8fdb\u884c\u5916\u63d2\uff0c\u80fd\u591f\u65e0\u7f1d\u5730\u8003\u8651\u5e26\u5185\u9879\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u52a0\u901f\u91d1\u5c5e\u7684GW\u8ba1\u7b97\u6536\u655b\u6027\uff0c\u5e76\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u901a\u8fc7\u5bf93D\u91d1\u5c5e\u548c\u63ba\u6742\u7684MoS$_2$\u53ca\u77f3\u58a8\u70ef\u7b49\u4e8c\u7ef4\u91d1\u5c5e\u7684\u5206\u6790\u8bc1\u660e\u4e86\u5176\u6b63\u786e\u6027\u3002W-av\u65b9\u6cd5\u4e0eARPES\u6d4b\u91cf\u7ed3\u679c\u5728\u63ba\u6742MoS$_2$\u5355\u5c42\u4e2d\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u5728\u77f3\u58a8\u70ef\u7814\u7a76\u4e2d\uff0c\u7ed3\u5408\u9ad8\u9636\u6d1b\u4f26\u5179\u51fd\u6570\u63cf\u8ff0\u81ea\u80fd\u5e76\u6c42\u89e3\u7ebf\u6027\u5316\u8fd1\u4f3c\u4ee5\u5916\u7684\u51c6\u7c92\u5b50\u65b9\u7a0b\uff0c\u80fd\u83b7\u5f97\u66f4\u7a33\u5065\u7684\u7ed3\u679c\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684W-av\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u8499\u7279\u5361\u6d1b\u79ef\u5206\u3001\u63d2\u503c\u4ee5\u53ca\u5bf9\u957f\u6ce2\u6781\u9650\u7684\u63a8\u503c\uff0c\u80fd\u6709\u6548\u52a0\u901f\u91d1\u5c5e\u7cfb\u7edf\u7684GW\u8ba1\u7b97\u6536\u655b\u6027\u5e76\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u5e26\u5185\u8dc3\u8fc1\u65b9\u9762\u3002\u8be5\u65b9\u6cd5\u57283D\u91d1\u5c5e\u548c\u63ba\u6742\u7684MoS$_2$\u53ca\u77f3\u58a8\u70ef\u7b49\u4e8c\u7ef4\u91d1\u5c5e\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5e76\u4e0eARPES\u6d4b\u91cf\u7ed3\u679c\u9ad8\u5ea6\u4e00\u81f4\uff0c\u8868\u660e\u5176\u5728\u63cf\u8ff0\u91d1\u5c5e\u7cfb\u7edf\u7684\u5149\u8c31\u6027\u8d28\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\u3002"}}
{"id": "2508.07640", "categories": ["cs.DC", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.07640", "abs": "https://arxiv.org/abs/2508.07640", "authors": ["Chanh Nguyen", "Monowar Bhuyan", "Erik Elmroth"], "title": "Taming Cold Starts: Proactive Serverless Scheduling with Model Predictive Control", "comment": "8 pages, 8 figures, preprint accepted at MASCOTS 2025", "summary": "Serverless computing has transformed cloud application deployment by\nintroducing a fine-grained, event-driven execution model that abstracts away\ninfrastructure management. Its on-demand nature makes it especially appealing\nfor latency-sensitive and bursty workloads. However, the cold start problem,\ni.e., where the platform incurs significant delay when provisioning new\ncontainers, remains the Achilles' heel of such platforms.\n  This paper presents a predictive serverless scheduling framework based on\nModel Predictive Control to proactively mitigate cold starts, thereby improving\nend-to-end response time. By forecasting future invocations, the controller\njointly optimizes container prewarming and request dispatching, improving\nlatency while minimizing resource overhead.\n  We implement our approach on Apache OpenWhisk, deployed on a Kubernetes-based\ntestbed. Experimental results using real-world function traces and synthetic\nworkloads demonstrate that our method significantly outperforms\nstate-of-the-art baselines, achieving up to 85% lower tail latency and a 34%\nreduction in resource usage.", "AI": {"tldr": "\u8be5\u6846\u67b6\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6765\u9884\u6d4b\u672a\u6765\u7684\u8c03\u7528\uff0c\u4ee5\u4e3b\u52a8\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u54cd\u5e94\u65f6\u95f4\u3002", "motivation": "\u65e0\u670d\u52a1\u5668\u8ba1\u7b97\u7684\u51b7\u542f\u52a8\u95ee\u9898\uff08\u5e73\u53f0\u5728\u914d\u7f6e\u65b0\u5bb9\u5668\u65f6\u4f1a\u4ea7\u751f\u663e\u8457\u5ef6\u8fdf\uff09\u662f\u5176\u4e3b\u8981\u7f3a\u70b9\uff0c\u5c3d\u7ba1\u5b83\u5177\u6709\u6309\u9700\u7684\u6027\u8d28\uff0c\u975e\u5e38\u9002\u5408\u5ef6\u8fdf\u654f\u611f\u548c\u7a81\u53d1\u7684\u5de5\u4f5c\u8d1f\u8f7d\u3002", "method": "\u8be5\u6846\u67b6\u4f7f\u7528\u6a21\u578b\u9884\u6d4b\u63a7\u5236\u6765\u9884\u6d4b\u672a\u6765\u7684\u8c03\u7528\uff0c\u4ee5\u4e3b\u52a8\u7f13\u89e3\u51b7\u542f\u52a8\u95ee\u9898\u3002\u8be5\u63a7\u5236\u5668\u534f\u540c\u4f18\u5316\u5bb9\u5668\u9884\u70ed\u548c\u8bf7\u6c42\u8c03\u5ea6\uff0c\u4ee5\u51cf\u5c11\u5ef6\u8fdf\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u8d44\u6e90\u5f00\u9500\u3002", "result": "\u8be5\u65b9\u6cd5\u53ef\u5c06\u5c3e\u90e8\u5ef6\u8fdf\u6700\u591a\u964d\u4f4e 85%\uff0c\u5e76\u53ef\u5c06\u8d44\u6e90\u4f7f\u7528\u7387\u964d\u4f4e 34%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728 Apache OpenWhisk \u4e0a\u5b9e\u73b0\uff0c\u5e76\u5728\u771f\u5b9e\u51fd\u6570\u8ddf\u8e2a\u548c\u5408\u6210\u5de5\u4f5c\u8d1f\u8f7d\u7684\u5b9e\u9a8c\u4e2d\uff0c\u4e0e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u76f8\u6bd4\uff0c\u5c3e\u90e8\u5ef6\u8fdf\u964d\u4f4e\u4e86 85%\uff0c\u8d44\u6e90\u4f7f\u7528\u7387\u964d\u4f4e\u4e86 34%\uff0c\u663e\u8457\u4f18\u4e8e\u5b83\u4eec\u3002"}}
{"id": "2508.06687", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06687", "abs": "https://arxiv.org/abs/2508.06687", "authors": ["Sreeja Roy-Singh", "Vinay Ravindra", "Richard Levinson", "Mahta Moghaddam", "Jan Mandel", "Adam Kochanski", "Angel Farguell Caus", "Kurtis Nelson", "Samira Alkaee Taleghan", "Archana Kannan", "Amer Melebari"], "title": "Optimal Planning and Machine Learning for Responsive Tracking and Enhanced Forecasting of Wildfires using a Spacecraft Constellation", "comment": null, "summary": "We propose a novel concept of operations using optimal planning methods and\nmachine learning (ML) to collect spaceborne data that is unprecedented for\nmonitoring wildfires, process it to create new or enhanced products in the\ncontext of wildfire danger or spread monitoring, and assimilate them to improve\nexisting, wildfire decision support tools delivered to firefighters within\nlatency appropriate for time-critical applications. The concept is studied with\nrespect to NASA's CYGNSS Mission, a constellation of passive microwave\nreceivers that measure specular GNSS-R reflections despite clouds and smoke.\nOur planner uses a Mixed Integer Program formulation to schedule joint\nobservation data collection and downlink for all satellites. Optimal solutions\nare found quickly that collect 98-100% of available observation opportunities.\nML-based fire predictions that drive the planner objective are greater than 40%\nmore correlated with ground truth than existing state-of-art. The presented\ncase study on the TX Smokehouse Creek fire in 2024 and LA fires in 2025\nrepresents the first high-resolution data collected by CYGNSS of active fires.\nCreation of Burnt Area Maps (BAM) using ML applied to the data during active\nfires and BAM assimilation into NASA's Weather Research and Forecasting Model\nusing ML to broadcast fire spread are novel outcomes. BAM and CYGNSS obtained\nsoil moisture are integrated for the first time into USGS fire danger maps.\nInclusion of CYGNSS data in ML-based burn predictions boosts accuracy by 13%,\nand inclusion of high-resolution data boosts ML recall by another 15%. The\nproposed workflow has an expected latency of 6-30h, improving on the current\ndelivery time of multiple days. All components in the proposed concept are\nshown to be computationally scalable and globally generalizable, with\nsustainability considerations such as edge efficiency and low latency on small\ndevices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06769", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06769", "abs": "https://arxiv.org/abs/2508.06769", "authors": ["Hunter Lindemann", "Shanon Vuglar", "Julio Gea-Banacloche"], "title": "Rotation Errors Due to Field Quantization for Simultaneously Driven Atoms", "comment": "10 pages, 8 figures", "summary": "When an electromagnetic field in a coherent or quasiclassical (e.g.,\nsqueezed) state is used to simultaneously drive an ensemble of two-level atoms,\nthe quantum nature of the field will, in general, cause the final state of the\natoms to differ from the one predicted for a totally classical field. This is a\npotential source of error in quantum logic gates in which the gate is the\nrotation of the atoms by a laser. In this paper, we use second order\nperturbation theory to find how this error scales with the number of atoms,\n$N$, being driven simultaneously, for an arbitrary rotation angle. The result\ndepends on the initial atomic state: for some highly entangled states, and a\nfield in a coherent state, the error may scale as $N^2$, yet we find that the\naverage over a random distribution of initial states only scales as $N$. We\ndiscuss possible ways to mitigate the error, including the use of squeezed\nstates, as well as adjusting the interaction time between the field and atoms\nto be different from what would be expected from the classical-field treatment.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u91cf\u5b50\u573a\u9a71\u52a8\u591a\u539f\u5b50\u7cfb\u7edf\u65f6\u7684\u8bef\u5dee\u95ee\u9898\uff0c\u53d1\u73b0\u8bef\u5dee\u4e0e\u539f\u5b50\u6570\u91cf N \u7684\u5173\u7cfb\u53d6\u51b3\u4e8e\u521d\u59cb\u72b6\u6001\uff0c\u5e76\u63d0\u51fa\u4e86\u5229\u7528\u538b\u7f29\u6001\u6216\u8c03\u6574\u76f8\u4e92\u4f5c\u7528\u65f6\u95f4\u6765\u51cf\u5c0f\u8bef\u5dee\u7684\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u903b\u8f91\u95e8\u4e2d\uff0c\u7535\u78c1\u573a\u4e0e\u591a\u539f\u5b50\u7cfb\u7edf\u76f8\u4e92\u4f5c\u7528\u65f6\uff0c\u573a\u7684\u91cf\u5b50\u6027\u8d28\u53ef\u80fd\u5bfc\u81f4\u4e0e\u7ecf\u5178\u573a\u9884\u6d4b\u4e0d\u7b26\u7684\u6700\u7ec8\u539f\u5b50\u72b6\u6001\uff0c\u8fd9\u662f\u91cf\u5b50\u903b\u8f91\u95e8\u6f5c\u5728\u7684\u8bef\u5dee\u6765\u6e90\u3002", "method": "\u4f7f\u7528\u4e8c\u9636\u5fae\u6270\u7406\u8bba\u5206\u6790\u8bef\u5dee\u968f\u539f\u5b50\u6570 N \u7684\u7f29\u653e\u6bd4\u4f8b\uff0c\u5e76\u8003\u8651\u4e86\u76f8\u5e72\u6001\u548c\u538b\u7f29\u6001\u3002", "result": "\u8bef\u5dee\u7684\u7f29\u653e\u6bd4\u4f8b\u53d6\u51b3\u4e8e\u521d\u59cb\u539f\u5b50\u72b6\u6001\u3002\u5bf9\u4e8e\u67d0\u4e9b\u9ad8\u5ea6\u7ea0\u7f20\u6001\uff0c\u8bef\u5dee\u53ef\u80fd\u968f N^2 \u7f29\u653e\uff1b\u800c\u5bf9\u4e8e\u968f\u673a\u521d\u59cb\u72b6\u6001\u7684\u5e73\u5747\uff0c\u8bef\u5dee\u4ec5\u968f N \u7f29\u653e\u3002", "conclusion": "\u4f7f\u7528\u4e8c\u9636\u5fae\u6270\u7406\u8bba\u7814\u7a76\u4e86\u76f8\u5e72\u6216\u62df\u7ecf\u5178\u7535\u78c1\u573a\u9a71\u52a8\u591a\u539f\u5b50\u7cfb\u7edf\u65f6\u91cf\u5b50\u6027\u8d28\u5f15\u5165\u7684\u8bef\u5dee\uff0c\u5e76\u63d0\u51fa\u4e86\u51cf\u5c0f\u8bef\u5dee\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.08086", "categories": ["cs.CV", "cs.GR"], "pdf": "https://arxiv.org/pdf/2508.08086", "abs": "https://arxiv.org/abs/2508.08086", "authors": ["Zhongqi Yang", "Wenhang Ge", "Yuqi Li", "Jiaqi Chen", "Haoyuan Li", "Mengyin An", "Fei Kang", "Hua Xue", "Baixin Xu", "Yuyang Yin", "Eric Li", "Yang Liu", "Yikai Wang", "Hao-Xiang Guo", "Yahui Zhou"], "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation", "comment": "Technical Report", "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.", "AI": {"tldr": "Matrix-3D \u4f7f\u7528\u5168\u666f\u8868\u793a\u751f\u6210\u66f4\u5e7f\u6cdb\u3001\u66f4\u8be6\u7ec6\u7684 3D \u4e16\u754c\uff0c\u5e76\u63d0\u4f9b\u5feb\u901f\u548c\u7cbe\u786e\u7684\u91cd\u5efa\u65b9\u6cd5\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709 3D \u4e16\u754c\u751f\u6210\u65b9\u6cd5\u5728\u751f\u6210\u573a\u666f\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa Matrix-3D \u6846\u67b6\uff0c\u5229\u7528\u5168\u666f\u8868\u793a\u6765\u5b9e\u73b0\u5e7f\u6cdb\u7684\u3001\u5168\u65b9\u5411\u7684\u3001\u53ef\u63a2\u7d22\u7684 3D \u4e16\u754c\u751f\u6210\u3002", "method": "\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u6761\u4ef6\u89c6\u9891\u751f\u6210\u548c\u5168\u666f 3D \u91cd\u5efa\u3002\u9996\u5148\uff0c\u8bad\u7ec3\u4e86\u4e00\u4e2a\u8f68\u8ff9\u5f15\u5bfc\u7684\u5168\u666f\u89c6\u9891\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528\u573a\u666f\u7f51\u683c\u6e32\u67d3\u4f5c\u4e3a\u6761\u4ef6\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u3001\u51e0\u4f55\u4e00\u81f4\u7684\u573a\u666f\u89c6\u9891\u751f\u6210\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u5c06\u5168\u666f\u89c6\u9891\u63d0\u5347\u5230 3D \u4e16\u754c\uff1a(1) \u4e00\u4e2a\u524d\u9988\u5f0f\u7684\u5927\u578b\u5168\u666f\u91cd\u5efa\u6a21\u578b\uff0c\u7528\u4e8e\u5feb\u901f 3D \u573a\u666f\u91cd\u5efa\uff1b(2) \u4e00\u4e2a\u57fa\u4e8e\u4f18\u5316\u7684\u6d41\u7a0b\uff0c\u7528\u4e8e\u7cbe\u786e\u3001\u8be6\u7ec6\u7684 3D \u573a\u666f\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86 Matrix-Pano \u6570\u636e\u96c6\uff0c\u5305\u542b 116K \u4e2a\u9ad8\u8d28\u91cf\u7684\u9759\u6001\u5168\u666f\u89c6\u9891\u5e8f\u5217\uff0c\u5e76\u5e26\u6709\u6df1\u5ea6\u548c\u8f68\u8ff9\u6ce8\u91ca\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cMatrix-3D \u6846\u67b6\u5728\u5168\u666f\u89c6\u9891\u751f\u6210\u548c 3D \u4e16\u754c\u751f\u6210\u65b9\u9762\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "Matrix-3D \u6846\u67b6\u5728\u5168\u666f\u89c6\u9891\u751f\u6210\u548c 3D \u4e16\u754c\u751f\u6210\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06649", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06649", "abs": "https://arxiv.org/abs/2508.06649", "authors": ["Daniel Wang", "Eli Brignac", "Minjia Mao", "Xiao Fang"], "title": "Measuring Stereotype and Deviation Biases in Large Language Models", "comment": null, "summary": "Large language models (LLMs) are widely applied across diverse domains,\nraising concerns about their limitations and potential risks. In this study, we\ninvestigate two types of bias that LLMs may display: stereotype bias and\ndeviation bias. Stereotype bias refers to when LLMs consistently associate\nspecific traits with a particular demographic group. Deviation bias reflects\nthe disparity between the demographic distributions extracted from\nLLM-generated content and real-world demographic distributions. By asking four\nadvanced LLMs to generate profiles of individuals, we examine the associations\nbetween each demographic group and attributes such as political affiliation,\nreligion, and sexual orientation. Our experimental results show that all\nexamined LLMs exhibit both significant stereotype bias and deviation bias\ntowards multiple groups. Our findings uncover the biases that occur when LLMs\ninfer user attributes and shed light on the potential harms of LLM-generated\noutputs.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u56db\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u751f\u6210\u4e2a\u4eba\u8d44\u6599\u65f6\u662f\u5426\u5b58\u5728\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u504f\u5dee\u504f\u89c1\uff0c\u7ed3\u679c\u663e\u793a\u6240\u6709\u6a21\u578b\u5747\u5b58\u5728\u8fd9\u4e24\u79cd\u504f\u89c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5df2\u5e7f\u6cdb\u5e94\u7528\u4e8e\u5404\u4e2a\u9886\u57df\uff0c\u4f46\u5176\u5c40\u9650\u6027\u548c\u6f5c\u5728\u98ce\u9669\u5f15\u8d77\u4e86\u4eba\u4eec\u7684\u62c5\u5fe7\u3002\u672c\u7814\u7a76\u65e8\u5728\u8c03\u67e5LLMs\u53ef\u80fd\u663e\u793a\u7684\u4e24\u79cd\u504f\u89c1\uff1a\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u504f\u5dee\u504f\u89c1\u3002", "method": "\u901a\u8fc7\u8981\u6c42\u56db\u4e2a\u5148\u8fdb\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u751f\u6210\u4e2a\u4eba\u8d44\u6599\uff0c\u7814\u7a76\u4e86\u6bcf\u4e2a\u7fa4\u4f53\u4e0e\u653f\u6cbb\u503e\u5411\u3001\u5b97\u6559\u548c\u6027\u53d6\u5411\u7b49\u5c5e\u6027\u4e4b\u95f4\u7684\u5173\u8054\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u6709\u88ab\u68c0\u67e5\u7684LLMs\u5728\u591a\u4e2a\u7fa4\u4f53\u4e2d\u5747\u8868\u73b0\u51fa\u663e\u8457\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u504f\u5dee\u504f\u89c1\u3002", "conclusion": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6709\u88ab\u68c0\u67e5\u7684\u8bed\u8a00\u6a21\u578b\u90fd\u8868\u73b0\u51fa\u663e\u8457\u7684\u523b\u677f\u5370\u8c61\u504f\u89c1\u548c\u504f\u5dee\u504f\u89c1\uff0c\u5c24\u5176\u662f\u5728\u591a\u4e2a\u7fa4\u4f53\u4e2d\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u65ad\u7528\u6237\u5c5e\u6027\u65f6\u51fa\u73b0\u7684\u504f\u89c1\uff0c\u5e76\u6307\u51fa\u4e86\u8bed\u8a00\u6a21\u578b\u751f\u6210\u5185\u5bb9\u53ef\u80fd\u5e26\u6765\u7684\u6f5c\u5728\u5371\u5bb3\u3002"}}
{"id": "2508.08045", "categories": ["cs.GT"], "pdf": "https://arxiv.org/pdf/2508.08045", "abs": "https://arxiv.org/abs/2508.08045", "authors": ["Xinru Xu", "Wenjing Liu", "Qizhi Fang"], "title": "Constrained Distributed Heterogeneous Two-Facility Location Problems with Max-Variant Cost", "comment": null, "summary": "We study a constrained distributed heterogeneous two-facility location\nproblem, where a set of agents with private locations on the real line are\ndivided into disjoint groups. The constraint means that the facilities can only\nbe built in a given multiset of candidate locations and at most one facility\ncan be built at each candidate location. Given the locations of the two\nfacilities, the cost of an agent is the distance from her location to the\nfarthest facility (referred to as max-variant). Our goal is to design\nstrategyproof distributed mechanisms that can incentivize all agents to\ntruthfully report their locations and approximately optimize some social\nobjective. A distributed mechanism consists of two steps: for each group, the\nmechanism chooses two candidate locations as the representatives of the group\nbased only on the locations reported by agents therein; then, it outputs two\nfacility locations among all the representatives. We focus on a class of\ndeterministic strategyproof distributed mechanisms and analyze upper and lower\nbounds on the distortion under the Average-of-Average cost (average of the\naverage individual cost of agents in each group), the Max-of-Max cost (maximum\nindividual cost among all agents), the Average-of-Max cost (average of the\nmaximum individual cost among all agents in each group) and the Max-of-Average\ncost (maximum of the average individual cost of all agents in each group).\nUnder four social objectives, we obtain constant upper and lower distortion\nbounds.", "AI": {"tldr": "We study a constrained distributed heterogeneous two-facility location problem and design strategyproof distributed mechanisms with constant upper and lower distortion bounds under four social objectives.", "motivation": "We aim to design strategyproof distributed mechanisms that can incentivize all agents to truthfully report their locations and approximately optimize some social objective.", "method": "We focus on a class of deterministic strategyproof distributed mechanisms and analyze upper and lower bounds on the distortion under the Average-of-Average cost, the Max-of-Max cost, the Average-of-Max cost and the Max-of-Average cost.", "result": "We analyze upper and lower bounds on the distortion under four social objectives, obtaining constant upper and lower distortion bounds.", "conclusion": "Under four social objectives, we obtain constant upper and lower distortion bounds."}}
{"id": "2508.08002", "categories": ["cs.LG", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.08002", "abs": "https://arxiv.org/abs/2508.08002", "authors": ["Hongxin Yu", "Yibing Wang", "Fengyue Jin", "Meng Zhang", "Anni Chen"], "title": "A Physics-informed Deep Operator for Real-Time Freeway Traffic State Estimation", "comment": "18 pages, 9 figures", "summary": "Traffic state estimation (TSE) falls methodologically into three categories:\nmodel-driven, data-driven, and model-data dual-driven. Model-driven TSE relies\non macroscopic traffic flow models originated from hydrodynamics. Data-driven\nTSE leverages historical sensing data and employs statistical models or machine\nlearning methods to infer traffic state. Model-data dual-driven traffic state\nestimation attempts to harness the strengths of both aspects to achieve more\naccurate TSE. From the perspective of mathematical operator theory, TSE can be\nviewed as a type of operator that maps available measurements of inerested\ntraffic state into unmeasured traffic state variables in real time. For the\nfirst time this paper proposes to study real-time freeway TSE in the idea of\nphysics-informed deep operator network (PI-DeepONet), which is an\noperator-oriented architecture embedding traffic flow models based on deep\nneural networks. The paper has developed an extended architecture from the\noriginal PI-DeepONet. The extended architecture is featured with: (1) the\nacceptance of 2-D data input so as to support CNN-based computations; (2) the\nintroduction of a nonlinear expansion layer, an attention mechanism, and a MIMO\nmechanism; (3) dedicated neural network design for adaptive identification of\ntraffic flow model parameters. A traffic state estimator built on the basis of\nthis extended PI-DeepONet architecture was evaluated with respect to a short\nfreeway stretch of NGSIM and a large-scale urban expressway in China, along\nwith other four baseline TSE methods. The evaluation results demonstrated that\nthis novel TSE method outperformed the baseline methods with high-precision\nestimation results of flow and mean speed.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u7b97\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc\uff08PI-DeepONet\uff09\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u4ea4\u901a\u72b6\u6001\u4f30\u7b97\uff08TSE\uff09\u7684\u51c6\u786e\u6027\uff0c\u672c\u7814\u7a76\u4ece\u6570\u5b66\u7b97\u5b50\u7406\u8bba\u7684\u89d2\u5ea6\uff0c\u5c06TSE\u770b\u4f5c\u4e00\u79cd\u5c06\u53ef\u7528\u6d4b\u91cf\u6620\u5c04\u5230\u672a\u6d4b\u91cf\u4ea4\u901a\u72b6\u6001\u53d8\u91cf\u7684\u7b97\u5b50\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u5c06\u4ea4\u901a\u72b6\u6001\u4f30\u7b97\uff08TSE\uff09\u89c6\u4e3a\u4e00\u4e2a\u7b97\u5b50\u95ee\u9898\uff0c\u5e76\u9996\u6b21\u63d0\u51fa\u4f7f\u7528\u7269\u7406\u4fe1\u606f\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc\uff08PI-DeepONet\uff09\u7684\u601d\u60f3\u6765\u7814\u7a76\u5b9e\u65f6\u9ad8\u901f\u516c\u8defTSE\u3002\u5f00\u53d1\u4e86\u4e00\u4e2a\u6269\u5c55\u7684PI-DeepONet\u67b6\u6784\uff0c\u652f\u63012-D\u6570\u636e\u8f93\u5165\uff0c\u5f15\u5165\u4e86\u975e\u7ebf\u6027\u6269\u5c55\u5c42\u3001\u6ce8\u610f\u529b\u673a\u5236\u548cMIMO\u673a\u5236\uff0c\u5e76\u4e13\u95e8\u8bbe\u8ba1\u4e86\u7528\u4e8e\u81ea\u9002\u5e94\u8bc6\u522b\u4ea4\u901a\u6d41\u6a21\u578b\u53c2\u6570\u7684\u795e\u7ecf\u7f51\u7edc\u3002", "result": "\u5728NGSIM\u548c\u4e2d\u56fd\u67d0\u57ce\u5e02\u5feb\u901f\u8def\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u5c55PI-DeepONet\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u7b97\u65b9\u6cd5\u5728\u6d41\u91cf\u548c\u5e73\u5747\u901f\u5ea6\u7684\u4f30\u7b97\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u56db\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8e\u6269\u5c55PI-DeepONet\u7684\u4ea4\u901a\u72b6\u6001\u4f30\u7b97\u5668\u5728NGSIM\u548c\u4e2d\u56fd\u67d0\u57ce\u5e02\u5feb\u901f\u8def\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u5176\u5728\u6d41\u91cf\u548c\u5e73\u5747\u901f\u5ea6\u7684\u4f30\u7b97\u7cbe\u5ea6\u4e0a\u4f18\u4e8e\u5176\u4ed6\u56db\u79cd\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.07209", "categories": ["cs.CL", "cs.SI"], "pdf": "https://arxiv.org/pdf/2508.07209", "abs": "https://arxiv.org/abs/2508.07209", "authors": ["Chaoqun Cui", "Siyuan Li", "Kunkun Ma", "Caiyan Jia"], "title": "Enhancing Rumor Detection Methods with Propagation Structure Infused Language Model", "comment": "This paper is accepted by COLING2025", "summary": "Pretrained Language Models (PLMs) have excelled in various Natural Language\nProcessing tasks, benefiting from large-scale pretraining and self-attention\nmechanism's ability to capture long-range dependencies. However, their\nperformance on social media application tasks like rumor detection remains\nsuboptimal. We attribute this to mismatches between pretraining corpora and\nsocial texts, inadequate handling of unique social symbols, and pretraining\ntasks ill-suited for modeling user engagements implicit in propagation\nstructures. To address these issues, we propose a continue pretraining strategy\ncalled Post Engagement Prediction (PEP) to infuse information from propagation\nstructures into PLMs. PEP makes models to predict root, branch, and parent\nrelations between posts, capturing interactions of stance and sentiment crucial\nfor rumor detection. We also curate and release large-scale Twitter corpus:\nTwitterCorpus (269GB text), and two unlabeled claim conversation datasets with\npropagation structures (UTwitter and UWeibo). Utilizing these resources and PEP\nstrategy, we train a Twitter-tailored PLM called SoLM. Extensive experiments\ndemonstrate PEP significantly boosts rumor detection performance across\nuniversal and social media PLMs, even in few-shot scenarios. On benchmark\ndatasets, PEP enhances baseline models by 1.0-3.7\\% accuracy, even enabling it\nto outperform current state-of-the-art methods on multiple datasets. SoLM\nalone, without high-level modules, also achieves competitive results,\nhighlighting the strategy's effectiveness in learning discriminative post\ninteraction features.", "AI": {"tldr": "\u901a\u8fc7Post Engagement Prediction (PEP)\u7ee7\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\u548cSoLM\u6a21\u578b\uff0c\u63d0\u5347\u4e86\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u5a92\u4f53\u8c23\u8a00\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u5a92\u4f53\u5e94\u7528\uff08\u5982\u8c23\u8a00\u68c0\u6d4b\uff09\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u539f\u56e0\u5728\u4e8e\u9884\u8bad\u7ec3\u8bed\u6599\u4e0e\u793e\u4ea4\u6587\u672c\u5b58\u5728\u4e0d\u5339\u914d\uff0c\u6a21\u578b\u672a\u80fd\u5145\u5206\u5904\u7406\u72ec\u7279\u7684\u793e\u4ea4\u7b26\u53f7\uff0c\u4ee5\u53ca\u9884\u8bad\u7ec3\u4efb\u52a1\u4e0d\u9002\u5408\u6a21\u62df\u7528\u6237\u53c2\u4e0e\u5ea6\u4f20\u64ad\u7ed3\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPost Engagement Prediction (PEP)\u7684\u7ee7\u7eed\u9884\u8bad\u7ec3\u7b56\u7565\uff0c\u901a\u8fc7\u9884\u6d4b\u5e16\u5b50\u4e4b\u95f4\u7684\u6839\u3001\u5206\u652f\u548c\u7236\u5b50\u5173\u7cfb\u6765\u6355\u6349\u7528\u6237\u53c2\u4e0e\u5ea6\u4fe1\u606f\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aSoLM\u7684\u9488\u5bf9Twitter\u8bed\u6599\u5e93\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u3002", "result": "PEP\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u7528\u548c\u793e\u4ea4\u5a92\u4f53\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u8c23\u8a00\u68c0\u6d4b\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0c\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u63d0\u5347\u4e861.0-3.7%\u7684\u51c6\u786e\u7387\u3002\u5355\u72ec\u7684SoLM\u6a21\u578b\u4e5f\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u8be5\u7b56\u7565\u5728\u5b66\u4e60\u533a\u5206\u6027\u5e16\u5b50\u4ea4\u4e92\u7279\u5f81\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684Post Engagement Prediction (PEP)\u7b56\u7565\u80fd\u591f\u6709\u6548\u63d0\u5347\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4ea4\u5a92\u4f53\u6587\u672c\u4e0a\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u8c23\u8a00\u68c0\u6d4b\u4efb\u52a1\u4e0a\uff0c\u5373\u4f7f\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u4e5f\u80fd\u53d6\u5f97\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u7684\u4e00\u4e9b\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.06537", "categories": ["cs.CV", "astro-ph.IM"], "pdf": "https://arxiv.org/pdf/2508.06537", "abs": "https://arxiv.org/abs/2508.06537", "authors": ["Shantanusinh Parmar"], "title": "Benchmarking Deep Learning-Based Object Detection Models on Feature Deficient Astrophotography Imagery Dataset", "comment": null, "summary": "Object detection models are typically trained on datasets like ImageNet,\nCOCO, and PASCAL VOC, which focus on everyday objects. However, these lack\nsignal sparsity found in non-commercial domains. MobilTelesco, a\nsmartphone-based astrophotography dataset, addresses this by providing sparse\nnight-sky images. We benchmark several detection models on it, highlighting\nchallenges under feature-deficient conditions.", "AI": {"tldr": "MobilTelesco\u662f\u4e00\u4e2a\u5305\u542b\u7a00\u758f\u591c\u7a7a\u56fe\u50cf\u7684\u667a\u80fd\u624b\u673a\u6444\u5f71\u6570\u636e\u96c6\uff0c\u53ef\u7528\u4e8e\u6d4b\u8bd5\u5929\u7a7a\u4e2d\u7269\u4f53\u68c0\u6d4b\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7269\u4f53\u68c0\u6d4b\u6570\u636e\u96c6\u7f3a\u4e4f\u5728\u975e\u5546\u4e1a\u9886\u57df\uff08\u5982\u5929\u6587\u6444\u5f71\uff09\u4e2d\u5e38\u89c1\u7684\u4fe1\u53f7\u7a00\u758f\u6027\u3002", "method": "\u4f7f\u7528MobilTelesco\u6570\u636e\u96c6\u5bf9\u51e0\u79cd\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5728MobilTelesco\u6570\u636e\u96c6\u4e0a\u5bf9\u51e0\u79cd\u68c0\u6d4b\u6a21\u578b\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u6307\u51fa\u4e86\u5728\u7279\u5f81\u4e0d\u8db3\u6761\u4ef6\u4e0b\u7684\u6311\u6218\u3002", "conclusion": "MobilTelesco\u6570\u636e\u96c6\u80fd\u591f\u7a81\u51fa\u5929\u7a7a\u4e2d\u7269\u4f53\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u7279\u5f81\u4e0d\u8db3\u7684\u60c5\u51b5\u4e0b\u3002"}}
{"id": "2508.07177", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07177", "abs": "https://arxiv.org/abs/2508.07177", "authors": ["Minghui Lu", "Brett Ross"], "title": "An Analogy of Frequency Droop Control for Grid-forming Sources", "comment": "Accepted by IEEE PESGM 2025", "summary": "In this paper, we present an analogy for a power system dominated by\ngrid-forming (GFM) sources that proves to be a powerful visualization tool for\nanalyses of power flow, frequency regulation, and power dispatch. Frequency\ndroop characteristics of a typical GFM source are exactly reflected by an\nordinary model of water vessels. The frequency is represented by visible water\nlevels while the droop slope is reified by the vessel sizes. This proposed\nanalogy allows us to use the intuitive water-flow phenomenon to explain the\nabstract power-flow problems. The grid integration of renewables via GFM\ninverters is interestingly simulated by a vessel connected to an infinite water\ntank. This paper also provides a means for demonstrating issues to audiences\nwith little or no background in power systems. Finally, the proposal is\nverified by simulation results.", "AI": {"tldr": "An analogy using water vessels visualizes power system dynamics with grid-forming sources, making complex concepts like power flow and frequency regulation easier to understand and demonstrate.", "motivation": "To provide a powerful visualization tool for power system analyses, particularly for grid-forming (GFM) sources, and to explain abstract power-flow problems using intuitive phenomena for a wider audience.", "method": "An analogy is presented where water vessel characteristics (water level, vessel size) represent power system components (frequency, droop slope) and water flow represents power flow. This allows for intuitive explanations of abstract power system concepts.", "result": "The analogy accurately reflects GFM source frequency droop characteristics and simulates renewable integration via GFM inverters. Simulation results verify the proposal's effectiveness.", "conclusion": "The proposed analogy effectively visualizes power system dynamics dominated by GFM sources, aiding in understanding power flow, frequency regulation, and dispatch for both experts and non-experts."}}
{"id": "2508.07366", "categories": ["cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.07366", "abs": "https://arxiv.org/abs/2508.07366", "authors": ["Taketo Uchida", "Takuto Kawakami", "Mikito Koshino"], "title": "Non-Abelian Chern band in rhombohedral graphene multilayers", "comment": null, "summary": "Moir\\'e flat bands in rhombohedral multilayer graphene provide a platform for\nexploring interaction-driven topological phases, where a single isolated band\noften forms a Chern band. However, non-Abelian degenerate Chern bands with\ninternal symmetries such as SU($N$) have so far been realized only in highly\nengineered systems. Here, we show that a doubly degenerate non-Abelian Chern\nband with Chern number $|C|=1$ emerges spontaneously at filling $\\nu=2$ in\nrhombohedral 3-, 4-, and 5-layer graphene, regardless of the presence of an hBN\nsubstrate. Using self-consistent Hartree-Fock calculations, we map out phase\ndiagrams as functions of displacement field and electronic periodicity, and\nanalytically demonstrate that the Fock term drives spontaneous symmetry\nbreaking and generates non-Abelian Berry curvature. Our findings unveil a new\nclass of interaction-driven non-Abelian topological phases, distinct from\nquantum anomalous Hall and fractional Chern phases.", "AI": {"tldr": "\u83f1\u9762\u591a\u5c42\u77f3\u58a8\u70ef\u81ea\u53d1\u5f62\u6210\u975e\u963f\u8d1d\u5c14\u9648\u5409\u5e03\u68ee\u5e26\uff0c\u63ed\u793a\u4e86\u65b0\u7684\u62d3\u6251\u76f8\u3002", "motivation": "\u83f1\u9762\u591a\u5c42\u77f3\u58a8\u70ef\u7684Moir\u00e9\u5e73\u5e26\u4e3a\u63a2\u7d22\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u62d3\u6251\u76f8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5e73\u53f0\uff0c\u4f46\u5177\u6709SU(N)\u7b49\u5185\u90e8\u5bf9\u79f0\u6027\u7684\u975e\u7b80\u5e76\u975e\u963f\u8d1d\u5c14\u9648\u5409\u5e03\u68ee\u5e26\u7684\u5b9e\u73b0\u901a\u5e38\u9700\u8981\u9ad8\u5ea6\u5de5\u7a0b\u5316\u7684\u7cfb\u7edf\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5728\u66f4\u6613\u5b9e\u73b0\u7684\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u975e\u963f\u8d1d\u5c14\u9648\u5409\u5e03\u68ee\u5e26\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u81ea\u6d3dHartree-Fock\u8ba1\u7b97\uff0c\u7814\u7a76\u4e86\u4f4d\u79fb\u573a\u548c\u7535\u5b50\u5468\u671f\u6027\u5bf9\u83f1\u97623\u5c42\u30014\u5c42\u548c5\u5c42\u77f3\u58a8\u70ef\u7684\u5f71\u54cd\uff0c\u5e76\u7ed8\u5236\u4e86\u76f8\u56fe\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86Fock\u9879\u9a71\u52a8\u4e86\u81ea\u53d1\u5bf9\u79f0\u6027\u7834\u7f3a\uff0c\u5e76\u4ea7\u751f\u4e86\u975e\u963f\u8d1d\u5c14\u8d1d\u91cc\u66f2\u7387\u3002", "result": "\u5728\u83f1\u97623\u5c42\u30014\u5c42\u548c5\u5c42\u77f3\u58a8\u70ef\u7684\u586b\u5145\u6570\u4e3a2\u65f6\uff0c\u65e0\u6761\u4ef6\u5730\u51fa\u73b0\u4e86\u53cc\u91cd\u7b80\u5e76\u7684\u975e\u963f\u8d1d\u5c14\u9648\u5409\u5e03\u68ee\u5e26\uff0c\u5176\u9648\u6570\u7edd\u5bf9\u503c\u4e3a1\u3002\u8be5\u73b0\u8c61\u4e0d\u4f9d\u8d56\u4e8ehBN\u886c\u5e95\u7684\u5b58\u5728\u3002\u7814\u7a76\u8fd8\u7ed8\u5236\u4e86\u76f8\u56fe\uff0c\u5e76\u8bc1\u660e\u4e86Fock\u9879\u662f\u5bfc\u81f4\u8fd9\u79cd\u975e\u963f\u8d1d\u5c14\u9648\u5409\u5e03\u68ee\u5e26\u5f62\u6210\u7684\u5173\u952e\u56e0\u7d20\u3002", "conclusion": "\u8be5\u7814\u7a76\u53d1\u73b0\u4e86\u83f1\u9762\u591a\u5c42\u77f3\u58a8\u70ef\u5728\u7279\u5b9a\u586b\u5145\u4e0b\u80fd\u81ea\u53d1\u5f62\u6210\u53cc\u91cd\u7b80\u5e76\u7684\u975e\u963f\u8d1d\u5c14\u9648\u5409\u5e03\u68ee\u5e26\uff0c\u5176\u9648\u6570\u7edd\u5bf9\u503c\u4e3a1\uff0c\u5e76\u4e14\u4e0d\u4f9d\u8d56\u4e8ehBN\u886c\u5e95\u3002\u8fd9\u63ed\u793a\u4e86\u4e00\u7c7b\u65b0\u7684\u7531\u76f8\u4e92\u4f5c\u7528\u9a71\u52a8\u7684\u975e\u963f\u8d1d\u5c14\u62d3\u6251\u76f8\uff0c\u4e0d\u540c\u4e8e\u91cf\u5b50\u53cd\u5e38\u970d\u5c14\u76f8\u548c\u5206\u6570\u91cf\u5b50\u970d\u5c14\u76f8\u3002"}}
{"id": "2508.07131", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07131", "abs": "https://arxiv.org/abs/2508.07131", "authors": ["Yanqing Xu", "Zhiguo Ding", "Octavia A. Dobre", "Tsung-Hui Chang"], "title": "Pinching-Antenna System Design with LoS Blockage: Does In-Waveguide Attenuation Matter?", "comment": "14 pages, 6 figures", "summary": "In the literature of pinching-antenna systems, in-waveguide attenuation is\noften neglected to simplify system design and enable more tractable analysis.\nHowever, its effect on overall system performance has received limited\nattention in the existing literature. While a recent study has shown that, in\nline-of-sight (LoS)-dominated environments, the data rate loss incurred by\nomitting in-waveguide attenuation is negligible when the communication area is\nnot excessively large, its effect under more general conditions remains\nunclear. This work extends the analysis to more realistic scenarios involving\narbitrary levels of LoS blockage. We begin by examining a single-user case and\nderive an explicit expression for the average data rate loss caused by\nneglecting in-waveguide attenuation. The results demonstrate that, even for\nlarge service areas, the rate loss remains negligible under typical LoS\nblockage conditions. We then consider a more general multi-user scenario, where\nmultiple pinching antennas, each deployed on a separate waveguide, jointly\nserve multiple users. The objective is to maximize the average sum rate by\njointly optimize antenna positions and transmit beamformers to maximize the\naverage sum rate under probabilistic LoS blockage. To solve the resulting\nstochastic and nonconvex optimization problem, we propose a dynamic sample\naverage approximation (SAA) algorithm. At each iteration, this method replaces\nthe expected objective with an empirical average computed from dynamically\nregenerated random channel realizations, ensuring that the optimization\naccurately reflects the current antenna configuration. Extensive simulation\nresults are provided to the proposed algorithm and demonstrate the substantial\nperformance gains of pinching-antenna systems, particularly in environments\nwith significant LoS blockage.", "AI": {"tldr": "\u5728\u6ce2\u5bfc\u8870\u51cf\u901a\u5e38\u88ab\u5ffd\u7565\u7684\u5939\u7d27\u5929\u7ebf\u7cfb\u7edf\u4e2d\uff0c\u8fd9\u9879\u7814\u7a76\u8bc4\u4f30\u4e86\u5176\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u5728\u5b58\u5728\u89c6\u7ebf\u906e\u6321\u7684\u6761\u4ef6\u4e0b\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5ffd\u7565\u8870\u51cf\u7684\u5f71\u54cd\u5f88\u5c0f\uff0c\u5e76\u4e14\u901a\u8fc7\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u6ce2\u675f\u6210\u5f62\u5668\uff0c\u53ef\u4ee5\u5927\u5927\u63d0\u9ad8\u7cfb\u7edf\u6027\u80fd\u3002", "motivation": "\u5728\u5939\u7d27\u5929\u7ebf\u7cfb\u7edf\u7684\u6587\u732e\u4e2d\uff0c\u4e3a\u4e86\u7b80\u5316\u7cfb\u7edf\u8bbe\u8ba1\u548c\u5206\u6790\uff0c\u901a\u5e38\u4f1a\u5ffd\u7565\u6ce2\u5bfc\u5185\u7684\u8870\u51cf\u3002\u7136\u800c\uff0c\u5b83\u5bf9\u6574\u4f53\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u5728\u73b0\u6709\u6587\u732e\u4e2d\u53d7\u5230\u7684\u5173\u6ce8\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u66f4\u73b0\u5b9e\u7684\u5305\u542b\u4efb\u610f\u89c6\u7ebf\u906e\u6321\u6761\u4ef6\u4e0b\u7684\u573a\u666f\uff0c\u5e76\u8bc4\u4f30\u5ffd\u7565\u6ce2\u5bfc\u5185\u8870\u51cf\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4eba\u5458\u63a8\u5bfc\u4e86\u7531\u5ffd\u7565\u6ce2\u5bfc\u5185\u8870\u51cf\u5f15\u8d77\u7684\u5e73\u5747\u6570\u636e\u901f\u7387\u635f\u5931\u7684\u663e\u5f0f\u8868\u8fbe\u5f0f\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u6837\u672c\u5e73\u5747\u8fd1\u4f3c\uff08SAA\uff09\u7b97\u6cd5\u6765\u89e3\u51b3\u591a\u7528\u6237\u573a\u666f\u4e0b\u7684\u968f\u673a\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u5929\u7ebf\u4f4d\u7f6e\u548c\u4f20\u8f93\u6ce2\u675f\u6210\u5f62\u5668\u3002", "result": "\u5728\u5355\u7528\u6237\u60c5\u51b5\u4e0b\uff0c\u5373\u4f7f\u5728\u670d\u52a1\u533a\u57df\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u5178\u578b\u7684\u89c6\u7ebf\u906e\u6321\u6761\u4ef6\u4e0b\uff0c\u901f\u7387\u635f\u5931\u4e5f\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u3002\u5728\u591a\u7528\u6237\u573a\u666f\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u52a8\u6001SAA\u7b97\u6cd5\u5728\u5b58\u5728\u663e\u7740\u89c6\u7ebf\u906e\u6321\u7684\u73af\u5883\u4e2d\uff0c\u80fd\u591f\u5b9e\u73b0\u5939\u7d27\u5929\u7ebf\u7cfb\u7edf\u7684\u6027\u80fd\u7684\u5927\u5e45\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u663e\u7740\u89c6\u7ebf\u906e\u6321\u7684\u73af\u5883\u4e2d\uff0c\u5728\u6ce2\u5bfc\u8870\u51cf\u53ef\u4ee5\u5ffd\u7565\u4e0d\u8ba1\u7684\u60c5\u51b5\u4e0b\uff0c\u5939\u7d27\u5929\u7ebf\u7cfb\u7edf\u7684\u6027\u80fd\u4e5f\u5f97\u5230\u4e86\u663e\u7740\u63d0\u5347\u3002"}}
{"id": "2508.06746", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06746", "abs": "https://arxiv.org/abs/2508.06746", "authors": ["Xin Tang", "Qian Chen", "Fengshun Li", "Youchun Gong", "Yinqiu Liu", "Wen Tian", "Shaowen Qin", "Xiaohuan Li"], "title": "Topology Generation of UAV Covert Communication Networks: A Graph Diffusion Approach with Incentive Mechanism", "comment": null, "summary": "With the growing demand for Uncrewed Aerial Vehicle (UAV) networks in\nsensitive applications, such as urban monitoring, emergency response, and\nsecure sensing, ensuring reliable connectivity and covert communication has\nbecome increasingly vital. However, dynamic mobility and exposure risks pose\nsignificant challenges. To tackle these challenges, this paper proposes a\nself-organizing UAV network framework combining Graph Diffusion-based Policy\nOptimization (GDPO) with a Stackelberg Game (SG)-based incentive mechanism. The\nGDPO method uses generative AI to dynamically generate sparse but\nwell-connected topologies, enabling flexible adaptation to changing node\ndistributions and Ground User (GU) demands. Meanwhile, the Stackelberg Game\n(SG)-based incentive mechanism guides self-interested UAVs to choose relay\nbehaviors and neighbor links that support cooperation and enhance covert\ncommunication. Extensive experiments are conducted to validate the\neffectiveness of the proposed framework in terms of model convergence, topology\ngeneration quality, and enhancement of covert communication performance.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408GDPO\u548cSG\u6fc0\u52b1\u673a\u5236\u7684\u65e0\u4eba\u673a\u7f51\u7edc\u6846\u67b6\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u79fb\u52a8\u6027\u548c\u66b4\u9732\u98ce\u9669\u7684\u6311\u6218\uff0c\u5e76\u63d0\u5347\u9690\u853d\u901a\u4fe1\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u5728\u57ce\u5e02\u76d1\u63a7\u3001\u5e94\u6025\u54cd\u5e94\u548c\u5b89\u5168\u4f20\u611f\u7b49\u654f\u611f\u5e94\u7528\u4e2d\u5bf9\u65e0\u4eba\u673a\uff08UAV\uff09\u7f51\u7edc\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u786e\u4fdd\u53ef\u9760\u7684\u8fde\u63a5\u548c\u9690\u853d\u901a\u4fe1\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u52a8\u6001\u79fb\u52a8\u6027\u548c\u66b4\u9732\u98ce\u9669\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u57fa\u4e8e\u56fe\u6269\u6563\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\u548c\u57fa\u4e8eStackelberg\u535a\u5f08\uff08SG\uff09\u7684\u6fc0\u52b1\u673a\u5236\u7684\u81ea\u7ec4\u7ec7\u65e0\u4eba\u673a\u7f51\u7edc\u6846\u67b6\u3002GDPO\u5229\u7528\u751f\u6210\u5f0fAI\u52a8\u6001\u751f\u6210\u7a00\u758f\u4f46\u8fde\u63a5\u826f\u597d\u7684\u62d3\u6251\uff0c\u4ee5\u9002\u5e94\u8282\u70b9\u5206\u5e03\u548c\u5730\u9762\u7528\u6237\u9700\u6c42\u7684\u52a8\u6001\u53d8\u5316\u3002SG\u6fc0\u52b1\u673a\u5236\u5219\u5f15\u5bfc\u65e0\u4eba\u673a\u9009\u62e9\u652f\u6301\u534f\u4f5c\u548c\u589e\u5f3a\u9690\u853d\u901a\u4fe1\u7684\u4e2d\u7ee7\u884c\u4e3a\u548c\u90bb\u5c45\u94fe\u8def\u3002", "result": "\u901a\u8fc7\u5927\u91cf\u7684\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u6a21\u578b\u6536\u655b\u6027\u3001\u62d3\u6251\u751f\u6210\u8d28\u91cf\u4ee5\u53ca\u9690\u853d\u901a\u4fe1\u6027\u80fd\u63d0\u5347\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u81ea\u7ec4\u7ec7\u65e0\u4eba\u673a\u7f51\u7edc\u6846\u67b6\u7ed3\u5408\u4e86\u57fa\u4e8e\u56fe\u6269\u6563\u7b56\u7565\u4f18\u5316\uff08GDPO\uff09\u548c\u57fa\u4e8eStackelberg\u535a\u5f08\uff08SG\uff09\u7684\u6fc0\u52b1\u673a\u5236\uff0c\u5728\u63d0\u9ad8\u6a21\u578b\u6536\u655b\u6027\u3001\u62d3\u6251\u751f\u6210\u8d28\u91cf\u4ee5\u53ca\u589e\u5f3a\u9690\u853d\u901a\u4fe1\u6027\u80fd\u65b9\u9762\u5747\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.08169", "categories": ["cs.DS"], "pdf": "https://arxiv.org/pdf/2508.08169", "abs": "https://arxiv.org/abs/2508.08169", "authors": ["Arpon Basu", "Pravesh K. Kothari", "Yang P. Liu", "Raghu Meka"], "title": "Sparsifying Sums of Positive Semidefinite Matrices", "comment": "29 pages", "summary": "In this paper, we revisit spectral sparsification for sums of arbitrary\npositive semidefinite (PSD) matrices. Concretely, for any collection of PSD\nmatrices $\\mathcal{A} = \\{A_1, A_2, \\ldots, A_r\\} \\subset \\mathbb{R}^{n \\times\nn}$, given any subset $T \\subseteq [r]$, our goal is to find sparse weights\n$\\mu \\in \\mathbb{R}_{\\geq 0}^r$ such that $(1 - \\epsilon) \\sum_{i \\in T} A_i\n\\preceq \\sum_{i \\in T} \\mu_i A_i \\preceq (1 + \\epsilon) \\sum_{i \\in T} A_i.$\nThis generalizes spectral sparsification of graphs which corresponds to\n$\\mathcal{A}$ being the set of Laplacians of edges. It also captures\nsparsifying Cayley graphs by choosing a subset of generators. The former has\nbeen extensively studied with optimal sparsifiers known. The latter has\nreceived attention recently and was solved for a few special groups (e.g.,\n$\\mathbb{F}_2^n$).\n  Prior work shows any sum of PSD matrices can be sparsified down to $O(n)$\nelements. This bound however turns out to be too coarse and in particular\nyields no non-trivial bound for building Cayley sparsifiers for Cayley graphs.\n  In this work, we develop a new, instance-specific (i.e., specific to a given\ncollection $\\mathcal{A}$) theory of PSD matrix sparsification based on a new\nparameter $N^*(\\mathcal{A})$ which we call connectivity threshold that\ngeneralizes the threshold of the number of edges required to make a graph\nconnected.\n  Our main result gives a sparsifier that uses at most\n$O(\\epsilon^{-2}N^*(\\mathcal{A}) (\\log n)(\\log r))$ matrices and is\nconstructible in randomized polynomial time. We also show that we need\n$N^*(\\mathcal{A})$ elements to sparsify for any $\\epsilon < 0.99$.\n  As the main application of our framework, we prove that any Cayley graph can\nbe sparsified to $O(\\epsilon^{-2}\\log^4 N)$ generators. Previously, a\nnon-trivial bound on Cayley sparsifiers was known only in the case when the\ngroup is $\\mathbb{F}_2^n$.", "AI": {"tldr": "\u672c\u6587\u9488\u5bf9\u6b63\u534a\u5b9a\uff08PSD\uff09\u77e9\u9635\u548c\u7684\u8c31\u7a00\u758f\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u8fde\u901a\u6027\u9608\u503c N*(A) \u7684\u7406\u8bba\u3002\u4e3b\u8981\u7ed3\u679c\u662f\u5f00\u53d1\u51fa\u4e00\u79cd\u80fd\u5c06\u7a00\u758f\u5668\u4f7f\u7528\u7684\u77e9\u9635\u6570\u91cf\u9650\u5236\u5728 O(\u03b5^-2 N*(A) (log n)(log r)) \u5185\u7684\u7b97\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u51ef\u83b1\u56fe\u7a00\u758f\u5316\u4e0a\u7684\u5e94\u7528\uff0c\u63d0\u4f9b\u4e86\u4f18\u4e8e\u4ee5\u5f80\u7684\u751f\u6210\u5143\u6570\u91cf\u4e0a\u754c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4efb\u610f\u6b63\u534a\u5b9a\uff08PSD\uff09\u77e9\u9635\u548c\u7684\u8c31\u7a00\u758f\u5316\u95ee\u9898\uff0c\u5e76\u4e3a\u51ef\u83b1\u56fe\u7684\u51ef\u83b1\u7a00\u758f\u5316\u63d0\u4f9b\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u672c\u6587\u57fa\u4e8e\u65b0\u7684\u8fde\u901a\u6027\u9608\u503c N*(A) \u7406\u8bba\uff0c\u8be5\u7406\u8bba\u6982\u62ec\u4e86\u4f7f\u56fe\u8fde\u901a\u6240\u9700\u7684\u8fb9\u6570\u9608\u503c\uff0c\u6765\u5f00\u53d1\u65b0\u7684\u3001\u5b9e\u4f8b\u7279\u5b9a\u7684 PSD \u77e9\u9635\u7a00\u758f\u5316\u7406\u8bba\u3002", "result": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684 PSD \u77e9\u9635\u7a00\u758f\u5316\u65b9\u6cd5\uff0c\u5176\u7a00\u758f\u5668\u4f7f\u7528\u7684\u77e9\u9635\u6570\u91cf\u81f3\u591a\u4e3a O(\u03b5^-2 N*(A) (log n)(log r))\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u968f\u673a\u591a\u9879\u5f0f\u65f6\u95f4\u6784\u5efa\u3002\u6b64\u5916\uff0c\u8bc1\u660e\u4e86\u4efb\u4f55\u51ef\u83b1\u56fe\u90fd\u53ef\u4ee5\u88ab\u7a00\u758f\u5316\u5230 O(\u03b5^-2 log^4 N) \u4e2a\u751f\u6210\u5143\u3002", "conclusion": "\u8be5\u8bba\u6587\u8bc1\u660e\u4e86\u4efb\u4f55\u51ef\u83b1\u56fe\u90fd\u53ef\u4ee5\u88ab\u7a00\u758f\u5316\u5230 O(\u03b5^-2 log^4 N) \u4e2a\u751f\u6210\u5143\u3002\u4e4b\u524d\uff0c\u4ec5\u5728\u7fa4\u4e3a F_2^n \u7684\u60c5\u51b5\u4e0b\u624d\u5bf9\u51ef\u83b1\u7a00\u758f\u5668\u6709\u975e\u5e73\u51e1\u7684\u4e0a\u754c\u3002"}}
{"id": "2508.06617", "categories": ["cs.LG", "cs.AI", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.06617", "abs": "https://arxiv.org/abs/2508.06617", "authors": ["Md Arafat Hossain", "Xingfu Wu", "Valerie Taylor", "Ali Jannesari"], "title": "Generalizing Scaling Laws for Dense and Sparse Large Language Models", "comment": "8 pages, 8 figures", "summary": "Over the past few years, the size of language models has grown exponentially,\nas has the computational cost to train these large models. This rapid growth\nhas motivated researchers to develop new techniques aimed at enhancing the\nefficiency of the training process. Despite these advancements, optimally\npredicting the model size or allocating optimal resources remains a challenge.\nSeveral efforts have addressed the challenge by proposing different scaling\nlaws, but almost all of them are architecture-specific (dense or sparse). In\nthis work we revisit existing scaling laws and propose a generalized scaling\nlaw to provide a unified framework that is applicable to both dense and sparse\nlarge language models. We evaluate and compare our proposed scaling law with\nexisting scaling laws to demonstrate its effectiveness.", "AI": {"tldr": "New scaling law proposed for efficient training of dense and sparse language models.", "motivation": "The exponential growth in the size and computational cost of language models has motivated research into more efficient training techniques. Optimally predicting model size and allocating resources remains a challenge, with existing scaling laws often being architecture-specific.", "method": "The paper revisits existing scaling laws and proposes a generalized scaling law to provide a unified framework applicable to both dense and sparse large language models. The effectiveness of the proposed law is evaluated and compared with existing ones.", "result": "The proposed generalized scaling law provides a unified framework for both dense and sparse large language models, demonstrating effectiveness through comparative evaluation.", "conclusion": "The paper proposes a generalized scaling law applicable to both dense and sparse large language models, offering a unified framework and demonstrating its effectiveness through evaluation and comparison with existing laws."}}
{"id": "2508.06997", "categories": ["cs.LG", "cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.06997", "abs": "https://arxiv.org/abs/2508.06997", "authors": ["Helbert Paat", "Guohao Shen"], "title": "Conformal Set-based Human-AI Complementarity with Multiple Experts", "comment": "Accepted at AAMAS 2025. Code available at:\n  https://github.com/paathelb/conformal_hai_multiple", "summary": "Decision support systems are designed to assist human experts in\nclassification tasks by providing conformal prediction sets derived from a\npre-trained model. This human-AI collaboration has demonstrated enhanced\nclassification performance compared to using either the model or the expert\nindependently. In this study, we focus on the selection of instance-specific\nexperts from a pool of multiple human experts, contrasting it with existing\nresearch that typically focuses on single-expert scenarios. We characterize the\nconditions under which multiple experts can benefit from the conformal sets.\nWith the insight that only certain experts may be relevant for each instance,\nwe explore the problem of subset selection and introduce a greedy algorithm\nthat utilizes conformal sets to identify the subset of expert predictions that\nwill be used in classifying an instance. This approach is shown to yield better\nperformance compared to naive methods for human subset selection. Based on real\nexpert predictions from the CIFAR-10H and ImageNet-16H datasets, our simulation\nstudy indicates that our proposed greedy algorithm achieves near-optimal\nsubsets, resulting in improved classification performance among multiple\nexperts.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u901a\u8fc7\u9009\u62e9\u76f8\u5173\u7684\u4e13\u5bb6\u5b50\u96c6\u6765\u63d0\u9ad8\u4eba\u7c7b\u4e13\u5bb6\u4e0eAI\u534f\u4f5c\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u6709\u591a\u4e2a\u4e13\u5bb6\u53ef\u4f9b\u9009\u62e9\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u591a\u4e13\u5bb6\u573a\u666f\u4e0b\uff0c\u5982\u4f55\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u9009\u62e9\u6700\u76f8\u5173\u7684\u4e13\u5bb6\u5b50\u96c6\u4ee5\u8f85\u52a9\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u63a2\u7a76\u591a\u4e13\u5bb6\u80fd\u4ece\u7f6e\u4fe1\u9884\u6d4b\u96c6\u4e2d\u83b7\u76ca\u7684\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7f6e\u4fe1\u9884\u6d4b\u96c6\u8fdb\u884c\u5b50\u96c6\u9009\u62e9\u7684\u8d2a\u5fc3\u7b97\u6cd5\uff0c\u7528\u4e8e\u4ece\u591a\u4e2a\u4e13\u5bb6\u4e2d\u8bc6\u522b\u51fa\u5bf9\u7279\u5b9a\u5b9e\u4f8b\u76f8\u5173\u7684\u4e13\u5bb6\u9884\u6d4b\u5b50\u96c6\uff0c\u5e76\u4e0e\u6734\u7d20\u65b9\u6cd5\u8fdb\u884c\u5bf9\u6bd4\u3002", "result": "\u6a21\u62df\u7814\u7a76\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u8d2a\u5fc3\u7b97\u6cd5\u80fd\u591f\u9009\u62e9\u51fa\u63a5\u8fd1\u6700\u4f18\u7684\u4e13\u5bb6\u5b50\u96c6\uff0c\u5728CIFAR-10H\u548cImageNet-16H\u6570\u636e\u96c6\u4e0a\uff0c\u901a\u8fc7\u771f\u5b9e\u4e13\u5bb6\u9884\u6d4b\u7684\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u5176\u4f18\u4e8e\u6734\u7d20\u65b9\u6cd5\u7684\u5206\u7c7b\u6027\u80fd\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u8d2a\u5fc3\u7b97\u6cd5\u5728\u4ece\u591a\u4e2a\u4e13\u5bb6\u9884\u6d4b\u4e2d\u9009\u62e9\u76f8\u5173\u5b50\u96c6\u4ee5\u8fdb\u884c\u5206\u7c7b\u4efb\u52a1\u65f6\uff0c\u80fd\u591f\u8fbe\u5230\u63a5\u8fd1\u6700\u4f18\u7684\u6027\u80fd\uff0c\u4ece\u800c\u5728\u591a\u4e13\u5bb6\u573a\u666f\u4e0b\u63d0\u5347\u5206\u7c7b\u8868\u73b0\u3002"}}
{"id": "2508.06945", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.06945", "abs": "https://arxiv.org/abs/2508.06945", "authors": ["Lujo Matasovic", "Petri Murto", "Shilong Yu", "Wenzhao Wang", "James D. Green", "Giacomo Londi", "Weixuan Zeng", "Laura Brown", "William K. Myers", "David Beljonne", "Yoann Olivier", "Feng Li", "Hugo Bronstein", "Timothy J. H. Hele", "Richard H. Friend", "Sebastian Gorgon"], "title": "Coulombic control of charge transfer in luminescent radicals with long-lived quartet states", "comment": "22 pages, 4 figures", "summary": "Excitons in organic materials are emerging as an attractive platform for\ntunable quantum technologies. Structures with near-degenerate doublet and\ntriplet excitations in linked trityl radical, acene and carbazole units can\nhost quartet states. These high spin states can be coherently manipulated, and\nlater decay radiatively via the radical doublet transition. However, this\nrequires controlling the deexcitation pathways of all metastable states. Here\nwe establish design rules for efficient quartet generation in luminescent\nradicals, using different connection arrangements of the molecular units. We\ndiscover that electronic coupling strength between these units dictates\nluminescence and quartet formation yields, particularly through the energetics\nof an acene-radical charge transfer state, which we tune Coulombically. This\nstate acts as a source of non-radiative decay when acene-radical separation is\nsmall, but facilitates doublet-quartet spin interconversion when acene-radical\nseparation is large. Using these rules we report a radical-carbazole-acene\nmaterial with 55% luminescence yield, where 94% of emitting excitons originate\nfrom the quartet at microsecond times. This reveals the central role of\nmolecular topology in luminescent quantum materials.", "AI": {"tldr": "\u6709\u673a\u6750\u6599\u4e2d\u7684\u6fc0\u5b50\u4e3a\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002\u901a\u8fc7\u8c03\u6574\u5206\u5b50\u7ed3\u6784\uff0c\u53ef\u4ee5\u63a7\u5236\u9ad8\u81ea\u65cb\u6001\u7684\u751f\u6210\u548c\u53d1\u5149\uff0c\u4ece\u800c\u63d0\u9ad8\u91cf\u5b50\u6548\u7387\u3002", "motivation": "\u6709\u673a\u6750\u6599\u4e2d\u7684\u6fc0\u5b50\u4e3a\u53ef\u8c03\u8c10\u7684\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u5438\u5f15\u529b\u7684\u5e73\u53f0\uff0c\u4f46\u9700\u8981\u63a7\u5236\u6240\u6709\u4e9a\u7a33\u6001\u7684\u9000\u6fc0\u53d1\u8def\u5f84\u4ee5\u5229\u7528\u9ad8\u81ea\u65cb\u6001\u3002", "method": "\u901a\u8fc7\u6539\u53d8\u5206\u5b50\u5355\u5143\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u5e76\u7814\u7a76\u7535\u5b50\u8026\u5408\u5f3a\u5ea6\u3001\u82b3\u9999\u70c3-\u81ea\u7531\u57fa\u7535\u8377\u8f6c\u79fb\u6001\u7684\u80fd\u91cf\u4ee5\u53ca\u5e93\u4ed1\u76f8\u4e92\u4f5c\u7528\u5bf9\u53d1\u5149\u548c\u56db\u91cd\u6001\u5f62\u6210\u4ea7\u7387\u7684\u5f71\u54cd\uff0c\u6765\u63a2\u7a76\u548c\u63a7\u5236\u6fc0\u5b50\u9000\u6fc0\u53d1\u8def\u5f84\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u670955%\u53d1\u5149\u4ea7\u7387\u7684\u81ea\u7531\u57fa-\u5494\u5511-\u70ef\u70c3\u6750\u6599\uff0c\u5176\u4e2d94%\u7684\u53d1\u5149\u6fc0\u5b50\u6e90\u81ea\u5fae\u79d2\u65f6\u95f4\u5c3a\u5ea6\u7684\u56db\u91cd\u6001\u3002", "conclusion": "\u5206\u5b50\u62d3\u6251\u5728\u53d1\u5149\u91cf\u5b50\u6750\u6599\u4e2d\u8d77\u7740\u6838\u5fc3\u4f5c\u7528\u3002\u901a\u8fc7\u63a7\u5236\u5206\u5b50\u5355\u5143\uff08\u5982\u4e09\u82ef\u7532\u57fa\u81ea\u7531\u57fa\u3001\u70ef\u70c3\u548c\u5494\u5511\uff09\u7684\u8fde\u63a5\u65b9\u5f0f\uff0c\u53ef\u4ee5\u4e3a\u9ad8\u6548\u7684\u56db\u91cd\u6001\u751f\u6210\u8bbe\u5b9a\u8bbe\u8ba1\u89c4\u5219\u3002"}}
{"id": "2508.07703", "categories": ["cs.DC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07703", "abs": "https://arxiv.org/abs/2508.07703", "authors": ["Adri Bhattacharya", "Pritam Goswami", "Evangelos Bampas", "Partha Sarathi Mandal"], "title": "Perpetual exploration in anonymous synchronous networks with a Byzantine black hole", "comment": "This paper has been accepted at DISC 2025", "summary": "In this paper, we investigate: ``How can a group of initially co-located\nmobile agents perpetually explore an unknown graph, when one stationary node\noccasionally behaves maliciously, under an adversary's control?'' We call this\nnode a ``Byzantine black hole (BBH)'' and at any given round it may choose to\ndestroy all visiting agents, or none. This subtle power can drastically\nundermine classical exploration strategies designed for an always active black\nhole. We study this perpetual exploration problem in the presence of at most\none BBH, without initial knowledge of the network size. Since the underlying\ngraph may be 1-connected, perpetual exploration of the entire graph may be\ninfeasible. We thus define two variants: \\pbmPerpExpl\\ and \\pbmPerpExplHome. In\nthe former, the agents are tasked to perform perpetual exploration of at least\none component, obtained after the exclusion of the BBH. In the latter, the\nagents are tasked to perform perpetual exploration of the component which\ncontains the \\emph{home} node, where agents are initially co-located.\nNaturally, \\pbmPerpExplHome\\ is a special case of \\pbmPerpExpl. Agents operate\nunder a synchronous scheduler and communicate in a face-to-face model. Our goal\nis to determine the minimum number of agents necessary and sufficient to solve\nthese problems. In acyclic networks, we obtain optimal algorithms that solve\n\\pbmPerpExpl\\ with $4$ agents, and \\pbmPerpExplHome\\ with $6$ agents in trees.\nThe lower bounds hold even in path graphs. In general graphs, we give a\nnon-trivial lower bound of $2\\Delta-1$ agents for \\pbmPerpExpl, and an upper\nbound of $3\\Delta+3$ agents for \\pbmPerpExplHome. To our knowledge, this is the\nfirst study of a black-hole variant in arbitrary networks without initial\ntopological knowledge.", "AI": {"tldr": "\u5728\u5177\u6709\u62dc\u5360\u5ead\u9ed1\u6d1e\uff08BBH\uff09\u7684\u672a\u77e5\u7f51\u7edc\u4e2d\uff0c\u667a\u80fd\u4f53\u9700\u8981\u8fdb\u884c\u6c38\u4e45\u63a2\u7d22\u3002BBH\u53ef\u80fd\u968f\u65f6\u9500\u6bc1\u667a\u80fd\u4f53\u3002\u7814\u7a76\u4e86\u63a2\u7d22\u81f3\u5c11\u4e00\u4e2a\u8fde\u901a\u5206\u91cf\uff08\npbmPerpExpl\n\uff09\u548c\u63a2\u7d22\u5305\u542b\u521d\u59cb\u4f4d\u7f6e\u7684\u8fde\u901a\u5206\u91cf\uff08\npbmPerpExplHome\n\uff09\u4e24\u79cd\u60c5\u51b5\u3002\u5728\u6811\u72b6\u7f51\u7edc\u4e2d\uff0c\u5206\u522b\u9700\u89814\u4e2a\u548c6\u4e2a\u667a\u80fd\u4f53\u3002\u5728\u4e00\u822c\u7f51\u7edc\u4e2d\uff0c\npbmPerpExpl\n\u9700\u8981$2\nu-1$\u4e2a\u667a\u80fd\u4f53\uff0c\npbmPerpExplHome\n\u9700\u8981$3\nu+3$\u4e2a\u667a\u80fd\u4f53\u3002", "motivation": "\u5728\u7f51\u7edc\u63a2\u7d22\u95ee\u9898\u4e2d\uff0c\u9ed1\u6d1e\uff08Black Hole\uff09\u662f\u4e00\u4e2a\u5e38\u89c1\u7684\u969c\u788d\u3002\u7136\u800c\uff0c\u5f53\u9ed1\u6d1e\u7684\u884c\u4e3a\u53d8\u5f97\u4e0d\u53ef\u9884\u6d4b\uff08\u5373\u62dc\u5360\u5ead\u884c\u4e3a\uff09\u65f6\uff0c\u4f20\u7edf\u7684\u63a2\u7d22\u7b56\u7565\u53ef\u80fd\u4f1a\u5931\u6548\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u66f4\u590d\u6742\u7684\u3001\u5177\u6709\u6f5c\u5728\u6076\u610f\u884c\u4e3a\u7684\u9ed1\u6d1e\u73af\u5883\u4e0b\u7684\u7f51\u7edc\u63a2\u7d22\u95ee\u9898\u3002", "method": "\u6211\u4eec\u7814\u7a76\u4e86\u5728\u5b58\u5728\u4e00\u4e2a\u62dc\u5360\u5ead\u9ed1\u6d1e\uff08BBH\uff09\u7684\u672a\u77e5\u56fe\u56fe\u4e2d\uff0c\u4e00\u7ec4\u521d\u59cb\u5171\u5740\u7684\u79fb\u52a8\u667a\u80fd\u4f53\u5982\u4f55\u8fdb\u884c\u6c38\u4e45\u63a2\u7d22\u7684\u95ee\u9898\u3002BBH\u5728\u6bcf\u4e00\u8f6e\u53ef\u80fd\u9500\u6bc1\u8bbf\u95ee\u5b83\u7684\u667a\u80fd\u4f53\uff0c\u6216\u8005\u4e0d\u8fdb\u884c\u4efb\u4f55\u64cd\u4f5c\u3002\u6211\u4eec\u5b9a\u4e49\u4e86\u4e24\u4e2a\u95ee\u9898\u53d8\u4f53\uff1a\npbmPerpExpl\n\uff08\u63a2\u7d22\u81f3\u5c11\u4e00\u4e2a\u8fde\u901a\u5206\u91cf\uff09\u548c \npbmPerpExplHome\n\uff08\u63a2\u7d22\u5305\u542b\u521d\u59cb\u4f4d\u7f6e\u7684\u8fde\u901a\u5206\u91cf\uff09\u3002\u667a\u80fd\u4f53\u5728\u540c\u6b65\u8c03\u5ea6\u4e0b\u5de5\u4f5c\uff0c\u5e76\u91c7\u7528\u9762\u5bf9\u9762\u901a\u4fe1\u6a21\u578b\u3002\u6211\u4eec\u65e8\u5728\u786e\u5b9a\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u6240\u9700\u7684\u6700\u5c0f\u667a\u80fd\u4f53\u6570\u91cf\u3002", "result": "\u5728\u6709\u73af\u7f51\u7edc\u4e2d\uff0c\u6211\u4eec\u63d0\u51fa\u7684\u7b97\u6cd5\u80fd\u591f\u4ee54\u4e2a\u667a\u80fd\u4f53\u89e3\u51b3\npbmPerpExpl\n\u95ee\u9898\uff0c\u4ee56\u4e2a\u667a\u80fd\u4f53\u89e3\u51b3\npbmPerpExplHome\n\u95ee\u9898\uff0c\u4e14\u8fd9\u4e9b\u7b97\u6cd5\u662f\u6700\u4f18\u7684\u3002\u5728\u4e00\u822c\u56fe\u4e2d\uff0c\u6211\u4eec\u4e3a\npbmPerpExpl\n\u95ee\u9898\u63d0\u4f9b\u4e86$2\nu-1$\u7684\u667a\u80fd\u4f53\u6570\u91cf\u4e0b\u754c\uff0c\u4e3a\npbmPerpExplHome\n\u95ee\u9898\u63d0\u4f9b\u4e86$3\nu+3$\u7684\u667a\u80fd\u4f53\u6570\u91cf\u4e0a\u754c\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u7814\u7a76\u4e86\u5728\u672a\u77e5\u62d3\u6251\u7684\u4efb\u610f\u7f51\u7edc\u4e2d\uff0c\u5b58\u5728\u62dc\u5360\u5ead\u9ed1\u6d1e\uff08BBH\uff09\u7684\u53d8\u79cd\u95ee\u9898\u3002\u6211\u4eec\u4e3a \npbmPerpExpl\n \u548c \npbmPerpExplHome\n \u95ee\u9898\u5206\u522b\u627e\u5230\u4e86\u6700\u4f18\u89e3\uff08\u5728\u6709\u73af\u7f51\u7edc\u4e2d\uff09\u4ee5\u53ca\u975e\u5e73\u51e1\u7684\u89e3\uff0c\u5e76\u7ed9\u51fa\u4e86\u76f8\u5e94\u7684\u6700\u5c11\u667a\u80fd\u4f53\u6570\u91cf\u3002"}}
{"id": "2508.06722", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06722", "abs": "https://arxiv.org/abs/2508.06722", "authors": ["Justin London"], "title": "Improved Obstacle Avoidance for Autonomous Robots with ORCA-FLC", "comment": null, "summary": "Obstacle avoidance enables autonomous agents and robots to operate safely and\nefficiently in dynamic and complex environments, reducing the risk of\ncollisions and damage. For a robot or autonomous system to successfully\nnavigate through obstacles, it must be able to detect such obstacles. While\nnumerous collision avoidance algorithms like the dynamic window approach (DWA),\ntimed elastic bands (TEB), and reciprocal velocity obstacles (RVO) have been\nproposed, they may lead to suboptimal paths due to fixed weights, be\ncomputationally expensive, or have limited adaptability to dynamic obstacles in\nmulti-agent environments. Optimal reciprocal collision avoidance (ORCA), which\nimproves on RVO, provides smoother trajectories and stronger collision\navoidance guarantees. We propose ORCA-FL to improve on ORCA by using fuzzy\nlogic controllers (FLCs) to better handle uncertainty and imprecision for\nobstacle avoidance in path planning. Numerous multi-agent experiments are\nconducted and it is shown that ORCA-FL can outperform ORCA in reducing the\nnumber of collision if the agent has a velocity that exceeds a certain\nthreshold. In addition, a proposed algorithm for improving ORCA-FL using fuzzy\nQ reinforcement learning (FQL) is detailed for optimizing and tuning FLCs.", "AI": {"tldr": "ORCA-FL\u901a\u8fc7\u6a21\u7cca\u903b\u8f91\u548c\u5f3a\u5316\u5b66\u4e60\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u907f\u969c\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u673a\u5668\u4eba\u6216\u81ea\u4e3b\u7cfb\u7edf\u5728\u590d\u6742\u52a8\u6001\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\uff0c\u5e76\u514b\u670d\u73b0\u6709\u78b0\u649e\u907f\u514d\u7b97\u6cd5\uff08\u5982DWA\u3001TEB\u3001RVO\u548cORCA\uff09\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u3001\u8ba1\u7b97\u6210\u672c\u548c\u52a8\u6001\u969c\u788d\u7269\u9002\u5e94\u6027\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aORCA-FL\u7684\u7b97\u6cd5\uff0c\u5b83\u6539\u8fdb\u4e86ORCA\uff0c\u901a\u8fc7\u4f7f\u7528\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\uff08FLCs\uff09\u6765\u66f4\u597d\u5730\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u7cbe\u786e\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u8be6\u7ec6\u4ecb\u7ecd\u4e86\u4e00\u79cd\u4f7f\u7528\u6a21\u7ccaQ\u5f3a\u5316\u5b66\u4e60\uff08FQL\uff09\u6765\u4f18\u5316\u548c\u8c03\u6574FLCs\u7684\u7b97\u6cd5\u3002", "result": "ORCA-FL\u5728\u51cf\u5c11\u78b0\u649e\u6570\u91cf\u65b9\u9762\u4f18\u4e8eORCA\uff0c\u7279\u522b\u662f\u5728\u667a\u80fd\u4f53\u901f\u5ea6\u8d85\u8fc7\u7279\u5b9a\u9608\u503c\u7684\u60c5\u51b5\u4e0b\u3002", "conclusion": "ORCA-FL\u901a\u8fc7\u4f7f\u7528\u6a21\u7cca\u903b\u8f91\u63a7\u5236\u5668\uff08FLCs\uff09\u6765\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u4e0d\u7cbe\u786e\u6027\uff0c\u5728\u591a\u667a\u80fd\u4f53\u73af\u5883\u4e2d\u53ef\u4ee5\u4f18\u4e8eORCA\uff0c\u5c24\u5176\u662f\u5728\u667a\u80fd\u4f53\u901f\u5ea6\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u65f6\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6a21\u7ccaQ\u5f3a\u5316\u5b66\u4e60\uff08FQL\uff09\u6765\u4f18\u5316\u548c\u8c03\u6574FLCs\u7684\u7b97\u6cd5\u3002"}}
{"id": "2508.06785", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06785", "abs": "https://arxiv.org/abs/2508.06785", "authors": ["Kenji Nakahira"], "title": "Unambiguous discrimination of the change point for quantum channels", "comment": null, "summary": "Identifying the precise moment when a quantum channel undergoes a change is a\nfundamental problem in quantum information theory. We study how accurately one\ncan determine the time at which a channel transitions to another. We\ninvestigate the quantum limit of the average success probability in unambiguous\ndiscrimination, in which errors are completely avoided by allowing inconclusive\nresults with a certain probability. This problem can be viewed as a quantum\nprocess discrimination task, where the process consists of a sequence of\nquantum channels; however, obtaining analytical solutions for quantum process\ndiscrimination is generally extremely challenging. In this paper, we propose a\nmethod to derive lower and upper bounds on the maximum average success\nprobability in unambiguous discrimination. In particular, when the channels\nbefore and after the change are unitary, we show that the maximum average\nsuccess probability can be analytically expressed in terms of the length of the\nchannel sequence and the discrimination limits for the two channels.", "AI": {"tldr": "\u7814\u7a76\u91cf\u5b50\u4fe1\u9053\u72b6\u6001\u6539\u53d8\u65f6\u95f4\u7684\u8fa8\u8bc6\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u89e3\u6790\u6c42\u89e3\u8fa8\u8bc6\u6210\u529f\u7387\u754c\u9650\u7684\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u4fe1\u9053\u72b6\u6001\u6539\u53d8\u65f6\u95f4\u7684\u8fa8\u8bc6\u662f\u4e00\u4e2a\u57fa\u672c\u95ee\u9898\uff0c\u4f46\u5176\u89e3\u6790\u89e3\u7684\u83b7\u53d6\u5341\u5206\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u63a8\u5bfc\u6700\u5927\u5e73\u5747\u6210\u529f\u7387\u7684\u4e0a\u4e0b\u754c\u9650\u7684\u65b9\u6cd5\u3002", "result": "\u5f53\u4fe1\u9053\u8f6c\u6362\u524d\u540e\u7684\u53d8\u6362\u4e3a\u9149\u53d8\u6362\u65f6\uff0c\u6700\u5927\u5e73\u5747\u6210\u529f\u7387\u53ef\u4ee5\u88ab\u89e3\u6790\u5730\u8868\u793a\u51fa\u6765\uff0c\u5176\u7ed3\u679c\u4e0e\u4fe1\u9053\u5e8f\u5217\u957f\u5ea6\u4ee5\u53ca\u4e24\u4e2a\u4fe1\u9053\u7684\u8fa8\u8bc6\u6781\u9650\u6709\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u91cf\u5b50\u8fc7\u7a0b\u8fa8\u8bc6\u4efb\u52a1\u63d0\u4f9b\u4e86\u4e00\u79cd\u63a8\u5bfc\u8fa8\u8bc6\u6210\u529f\u7387\u754c\u9650\u7684\u65b9\u6cd5\uff0c\u7279\u522b\u662f\u5728\u6c9f\u9053\u8f6c\u6362\u524d\u540e\u5747\u4e3a\u9149\u53d8\u6362\u7684\u60c5\u51b5\u4e0b\uff0c\u6210\u529f\u7387\u7684\u4e0a\u9650\u548c\u4e0b\u9650\u53ef\u4ee5\u88ab\u89e3\u6790\u5730\u8868\u8fbe\u51fa\u6765\u3002"}}
{"id": "2508.06665", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06665", "abs": "https://arxiv.org/abs/2508.06665", "authors": ["Jonathan Shaw", "Dillon Mee", "Timothy Khouw", "Zackary Leech", "Daniel Wilson"], "title": "Testing the Limits of Machine Translation from One Book", "comment": null, "summary": "Current state-of-the-art models demonstrate capacity to leverage in-context\nlearning to translate into previously unseen language contexts. Tanzer et al.\n[2024] utilize language materials (e.g. a grammar) to improve translation\nquality for Kalamang using large language models (LLMs). We focus on Kanuri, a\nlanguage that, despite having substantial speaker population, has minimal\ndigital resources. We design two datasets for evaluation: one focused on health\nand humanitarian terms, and another containing generalized terminology,\ninvestigating how domain-specific tasks impact LLM translation quality.\n  By providing different combinations of language resources (grammar,\ndictionary, and parallel sentences), we measure LLM translation effectiveness,\ncomparing results to native speaker translations and human linguist\nperformance. We evaluate using both automatic metrics and native speaker\nassessments of fluency and accuracy.\n  Results demonstrate that parallel sentences remain the most effective data\nsource, outperforming other methods in human evaluations and automatic metrics.\nWhile incorporating grammar improves over zero-shot translation, it fails as an\neffective standalone data source. Human evaluations reveal that LLMs achieve\naccuracy (meaning) more effectively than fluency (grammaticality).\n  These findings suggest LLM translation evaluation benefits from\nmultidimensional assessment beyond simple accuracy metrics, and that grammar\nalone, without parallel sentences, does not provide sufficient context for\neffective domain-specific translation.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5361\u52aa\u91cc\u8bed\uff08\u4e00\u79cd\u6570\u5b57\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\uff09\u4e0a\u7684\u7ffb\u8bd1\u80fd\u529b\u3002\u901a\u8fc7\u6784\u5efa\u5305\u542b\u5065\u5eb7\u3001\u4eba\u9053\u4e3b\u4e49\u548c\u901a\u7528\u672f\u8bed\u7684\u6570\u636e\u96c6\uff0c\u5e76\u7ed3\u5408\u8bed\u6cd5\u3001\u8bcd\u5178\u548c\u5e76\u884c\u53e5\u5b50\u7b49\u8bed\u8a00\u8d44\u6e90\u8fdb\u884c\u8bc4\u4f30\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5e76\u884c\u53e5\u5b50\u662f\u63d0\u9ad8\u7ffb\u8bd1\u8d28\u91cf\u7684\u6700\u6709\u6548\u6570\u636e\u6e90\uff0c\u800c\u5355\u72ec\u7684\u8bed\u6cd5\u4fe1\u606f\u4e0d\u8db3\u4ee5\u652f\u6491\u6709\u6548\u7684\u7ffb\u8bd1\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0cLLM \u5728\u4f20\u8fbe\u610f\u4e49\u65b9\u9762\u4f18\u4e8e\u8bed\u6cd5\u6d41\u7545\u6027\uff0c\u5e76\u5f3a\u8c03\u4e86\u91c7\u7528\u591a\u7ef4\u5ea6\u8bc4\u4f30\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u4e13\u6ce8\u4e8e\u4e00\u79cd\u6709\u5927\u91cf\u4f7f\u7528\u8005\u4f46\u6570\u5b57\u8d44\u6e90\u6781\u5c11\u7684\u8bed\u8a00\u2014\u2014\u5361\u52aa\u91cc\u8bed\uff0c\u5e76\u4e3a\u5176\u8bbe\u8ba1\u4e86\u4e24\u4e2a\u7528\u4e8e\u8bc4\u4f30\u7684\u6570\u636e\u96c6\uff1a\u4e00\u4e2a\u5173\u6ce8\u5065\u5eb7\u548c\u4eba\u9053\u4e3b\u4e49\u672f\u8bed\uff0c\u53e6\u4e00\u4e2a\u5305\u542b\u901a\u7528\u672f\u8bed\uff0c\u7814\u7a76\u7279\u5b9a\u9886\u57df\u7684\u4efb\u52a1\u5982\u4f55\u5f71\u54cd LLM \u7ffb\u8bd1\u8d28\u91cf\u3002", "method": "\u901a\u8fc7\u63d0\u4f9b\u4e0d\u540c\u7ec4\u5408\u7684\u8bed\u8a00\u8d44\u6e90\uff08\u8bed\u6cd5\u3001\u8bcd\u5178\u548c\u5e76\u884c\u53e5\u5b50\uff09\u6765\u8861\u91cf LLM \u7ffb\u8bd1\u7684\u6709\u6548\u6027\uff0c\u5e76\u5c06\u7ed3\u679c\u4e0e\u6bcd\u8bed\u8bd1\u8005\u548c\u4eba\u7c7b\u8bed\u8a00\u5b66\u5bb6\u7684\u8868\u73b0\u8fdb\u884c\u6bd4\u8f83\u3002\u4f7f\u7528\u81ea\u52a8\u6307\u6807\u548c\u6bcd\u8bed\u8005\u5bf9\u6d41\u7545\u6027\u548c\u51c6\u786e\u6027\u7684\u8bc4\u4f30\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5e76\u884c\u53e5\u5b50\u662f\u6700\u6709\u6548\u7684\u6570\u636e\u6765\u6e90\u3002\u52a0\u5165\u8bed\u6cd5\u53ef\u4ee5\u6539\u8fdb\u96f6\u6837\u672c\u7ffb\u8bd1\uff0c\u4f46\u4e0d\u80fd\u4f5c\u4e3a\u6709\u6548\u7684\u72ec\u7acb\u6570\u636e\u6e90\u3002LLM \u5728\u5b9e\u73b0\u51c6\u786e\u6027\uff08\u610f\u4e49\uff09\u65b9\u9762\u6bd4\u5b9e\u73b0\u6d41\u7545\u6027\uff08\u8bed\u6cd5\uff09\u66f4\u6709\u6548\u3002", "conclusion": "\u5e76\u884c\u53e5\u5b50\u4ecd\u7136\u662f\u6700\u6709\u6548\u7684\u6570\u636e\u6e90\uff0c\u5728\u4eba\u7c7b\u8bc4\u4f30\u548c\u81ea\u52a8\u6307\u6807\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u867d\u7136\u52a0\u5165\u8bed\u6cd5\u53ef\u4ee5\u6539\u8fdb\u96f6\u6837\u672c\u7ffb\u8bd1\uff0c\u4f46\u5b83\u4e0d\u80fd\u4f5c\u4e3a\u6709\u6548\u7684\u72ec\u7acb\u6570\u636e\u6e90\u3002\u4eba\u7c7b\u8bc4\u4f30\u663e\u793a\uff0cLLM \u5728\u5b9e\u73b0\u51c6\u786e\u6027\uff08\u610f\u4e49\uff09\u65b9\u9762\u6bd4\u5b9e\u73b0\u6d41\u7545\u6027\uff08\u8bed\u6cd5\uff09\u66f4\u6709\u6548\u3002\u8fd9\u4e9b\u53d1\u73b0\u8868\u660e\uff0cLLM \u7ffb\u8bd1\u8bc4\u4f30\u53d7\u76ca\u4e8e\u8d85\u8d8a\u7b80\u5355\u51c6\u786e\u6027\u6307\u6807\u7684\u591a\u7ef4\u5ea6\u8bc4\u4f30\uff0c\u5e76\u4e14\u4ec5\u51ed\u8bed\u6cd5\u800c\u6ca1\u6709\u5e76\u884c\u53e5\u5b50\uff0c\u65e0\u6cd5\u4e3a\u6709\u6548\u7684\u7279\u5b9a\u9886\u57df\u7ffb\u8bd1\u63d0\u4f9b\u8db3\u591f\u7684\u4e0a\u4e0b\u6587\u3002"}}
{"id": "2508.06543", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06543", "abs": "https://arxiv.org/abs/2508.06543", "authors": ["Jinghan Yu", "Zhiyuan Ma", "Yue Ma", "Kaiqi Liu", "Yuhan Wang", "Jianjun Li"], "title": "MILD: Multi-Layer Diffusion Strategy for Complex and Precise Multi-IP Aware Human Erasing", "comment": null, "summary": "Recent years have witnessed the success of diffusion models in\nimage-customized tasks. Prior works have achieved notable progress on\nhuman-oriented erasing using explicit mask guidance and semantic-aware\ninpainting. However, they struggle under complex multi-IP scenarios involving\nhuman-human occlusions, human-object entanglements, and background\ninterferences. These challenges are mainly due to: 1) Dataset limitations, as\nexisting datasets rarely cover dense occlusions, camouflaged backgrounds, and\ndiverse interactions; 2) Lack of spatial decoupling, where foreground instances\ncannot be effectively disentangled, limiting clean background restoration. In\nthis work, we introduce a high-quality multi-IP human erasing dataset with\ndiverse pose variations and complex backgrounds. We then propose Multi-Layer\nDiffusion (MILD), a novel strategy that decomposes generation into semantically\nseparated pathways for each instance and the background. To enhance\nhuman-centric understanding, we introduce Human Morphology Guidance,\nintegrating pose, parsing, and spatial relations. We further present\nSpatially-Modulated Attention to better guide attention flow. Extensive\nexperiments show that MILD outperforms state-of-the-art methods on challenging\nhuman erasing benchmarks.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMILD\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u5c42\u6269\u6563\u548c\u4eba\u4f53\u5f62\u6001\u6307\u5bfc\u6765\u89e3\u51b3\u590d\u6742\u591aIP\u573a\u666f\u4e2d\u7684\u4eba\u4f53\u64e6\u9664\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u6d89\u53ca\u4eba\u4e0e\u4eba\u906e\u6321\u3001\u4eba\u4e0e\u7269\u7ea0\u7f20\u548c\u80cc\u666f\u5e72\u6270\u7684\u590d\u6742\u591aIP\u573a\u666f\u4e2d\u5b58\u5728\u4e0d\u8db3\u3002\u8fd9\u4e9b\u6311\u6218\u4e3b\u8981\u5f52\u56e0\u4e8e\uff1a1) \u6570\u636e\u96c6\u9650\u5236\uff0c\u73b0\u6709\u6570\u636e\u96c6\u5f88\u5c11\u6db5\u76d6\u5bc6\u96c6\u906e\u6321\u3001\u4f2a\u88c5\u80cc\u666f\u548c\u591a\u6837\u7684\u4ea4\u4e92\uff1b2) \u7f3a\u4e4f\u7a7a\u95f4\u89e3\u8026\uff0c\u524d\u666f\u5b9e\u4f8b\u65e0\u6cd5\u6709\u6548\u89e3\u8026\uff0c\u9650\u5236\u4e86\u80cc\u666f\u7684\u5e72\u51c0\u6062\u590d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u591a\u5c42\u6269\u6563\uff08MILD\uff09\u7684\u65b0\u9896\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5c06\u751f\u6210\u5206\u89e3\u4e3a\u6bcf\u4e2a\u5b9e\u4f8b\u548c\u80cc\u666f\u7684\u8bed\u4e49\u5206\u79bb\u8def\u5f84\u3002\u4e3a\u4e86\u589e\u5f3a\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u7406\u89e3\uff0c\u6211\u4eec\u5f15\u5165\u4e86\u4eba\u4f53\u5f62\u6001\u6307\u5bfc\uff0c\u96c6\u6210\u4e86\u59ff\u52bf\u3001\u89e3\u6790\u548c\u7a7a\u95f4\u5173\u7cfb\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u63d0\u51fa\u4e86\u7a7a\u95f4\u8c03\u5236\u6ce8\u610f\uff0c\u4ee5\u66f4\u597d\u5730\u5f15\u5bfc\u6ce8\u610f\u6d41\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9ad8\u8d28\u91cf\u7684\u591aIP\u4eba\u4f53\u64e6\u9664\u6570\u636e\u96c6\uff0c\u5177\u6709\u591a\u6837\u7684\u59ff\u52bf\u53d8\u5316\u548c\u590d\u6742\u7684\u80cc\u666f\u3002", "conclusion": "MILD\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u4eba\u4f53\u64e6\u9664\u57fa\u51c6\u4e0a\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07314", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07314", "abs": "https://arxiv.org/abs/2508.07314", "authors": ["Xinlei Zhou", "Han Du", "Emily W. Yap", "Wanbin Dou", "Mingyang Huang", "Zhenjun Ma"], "title": "Human-in-the-Loop Simulation for Real-Time Exploration of HVAC Demand Flexibility", "comment": null, "summary": "The increasing integration of renewable energy into the power grid has\nhighlighted the critical importance of demand-side flexibility. Among flexible\nloads, heating, ventilation, and air-conditioning (HVAC) systems are\nparticularly significant due to their high energy consumption and\ncontrollability. This study presents the development of an interactive\nsimulation platform that integrates a high-fidelity simulation engine with a\nuser-facing dashboard, specifically designed to explore and demonstrate the\ndemand flexibility capacity of HVAC systems. Unlike conventional simulations,\nwhere users are passive observers of simulation results with no ability to\nintervene in the embedded control during the simulation, this platform\ntransforms them into active participants. Users can override system default\ncontrol settings, such as zone temperature setpoints and HVAC schedules, at any\npoint during the simulation runtime to implement demand response strategies of\ntheir choice. This human-in-the-loop capability enables real-time interaction\nand allows users to observe the immediate impact of their actions, emulating\nthe practical decision-making process of a building or system operator. By\nexploring different demand flexibility scenarios and system behaviour in a\nmanner that reflects real-world operation, users gain a deeper understanding of\ndemand flexibility and their impacts. This interactive experience builds\nconfidence and supports more informed decision-making in the practical adoption\nof demand-side flexibility. This paper presents the architecture of the\nsimulation platform, user-oriented dashboard design, and user case showcase.\nThe introduced human-in-the-loop simulation paradigm offers a more intuitive\nand interactive means of engaging with grid-interactive building operations,\nextending beyond HVAC demand flexibility exploration.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u4eff\u771f\u5e73\u53f0\uff0c\u7528\u6237\u53ef\u4ee5\u5b9e\u65f6\u63a7\u5236HVAC\u7cfb\u7edf\uff0c\u4ee5\u63a2\u7d22\u9700\u6c42\u4fa7\u7075\u6d3b\u6027\uff0c\u6bd4\u4f20\u7edf\u4eff\u771f\u66f4\u5177\u4e92\u52a8\u6027\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u53ef\u518d\u751f\u80fd\u6e90\u5e76\u7f51\u5e26\u6765\u7684\u5bf9\u9700\u6c42\u4fa7\u7075\u6d3b\u6027\u65e5\u76ca\u589e\u957f\u7684\u9700\u6c42\uff0c\u7279\u522b\u662fHVAC\u7cfb\u7edf\u4f5c\u4e3a\u5173\u952e\u7684\u67d4\u6027\u8d1f\u8377\uff0c\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u5f0f\u6765\u63a2\u7d22\u548c\u5c55\u793a\u5176\u9700\u6c42\u7075\u6d3b\u6027\u80fd\u529b\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u96c6\u6210\u9ad8\u4fdd\u771f\u4eff\u771f\u5f15\u64ce\u548c\u7528\u6237\u4eea\u8868\u677f\u7684\u4ea4\u4e92\u5f0f\u4eff\u771f\u5e73\u53f0\uff0c\u5141\u8bb8\u7528\u6237\u5728\u4eff\u771f\u8fc7\u7a0b\u4e2d\u5b9e\u65f6\u4fee\u6539HVAC\u7cfb\u7edf\u8bbe\u7f6e\uff08\u5982\u6e29\u5ea6\u8bbe\u5b9a\u70b9\u548c\u8fd0\u884c\u8ba1\u5212\uff09\uff0c\u4ee5\u63a2\u7d22\u9700\u6c42\u54cd\u5e94\u7b56\u7565\u3002", "result": "\u7528\u6237\u53ef\u4ee5\u901a\u8fc7\u5b9e\u65f6\u4ea4\u4e92\uff0c\u76f4\u89c2\u5730\u7406\u89e3\u4e0d\u540c\u9700\u6c42\u7075\u6d3b\u6027\u573a\u666f\u4e0b\u7684\u7cfb\u7edf\u884c\u4e3a\uff0c\u589e\u5f3a\u4e86\u5bf9\u9700\u6c42\u4fa7\u7075\u6d3b\u6027\u7684\u8ba4\u8bc6\uff0c\u5e76\u5bf9\u5b9e\u9645\u5e94\u7528\u5efa\u7acb\u4fe1\u5fc3\uff0c\u4ece\u800c\u652f\u6301\u66f4\u660e\u667a\u7684\u51b3\u7b56\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u4ea4\u4e92\u5f0f\u4eff\u771f\u5e73\u53f0\u901a\u8fc7\u5141\u8bb8\u7528\u6237\u5b9e\u65f6\u5e72\u9884\u548c\u89c2\u5bdfHVAC\u7cfb\u7edf\u884c\u4e3a\uff0c\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u76f4\u89c2\u3001\u66f4\u5177\u4ea4\u4e92\u6027\u7684\u65b9\u5f0f\u6765\u63a2\u7d22\u9700\u6c42\u4fa7\u7075\u6d3b\u6027\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u7684\u88ab\u52a8\u5f0f\u4eff\u771f\u3002"}}
{"id": "2508.07380", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07380", "abs": "https://arxiv.org/abs/2508.07380", "authors": ["Taegeun Song", "Nojoon Myoung"], "title": "Asymmetric-gate Mach--Zehnder interferometry in graphene: Multi-path conductance oscillations and visibility characteristics", "comment": null, "summary": "Graphene provides an excellent platform for investigating electron quantum\ninterference due to its outstanding coherent properties. In the quantum Hall\nregime, Mach--Zehnder (MZ) electronic interferometers are realized using p--n\njunctions in graphene, where electron interference is highly protected against\ndecoherence. In this work, we present a phenomenological framework for\ngraphene-based MZ interferometry with asymmetric p--n junction configurations.\nWe show that the enclosed interferometer area can be tuned by asymmetric gate\npotentials, and additional MZ pathways emerge in higher-filling-factor\nscenarios, e.g. $\\left(\\nu_{n},\\nu_{p}\\right)=\\left(-3,+3\\right)$. The\nresulting complicated beat oscillations in asymmetric-gate MZ interference are\nefficiently analyzed using a machine-learning--based Fourier transform, which\nyields improved peak-to-background ratios compared to conventional\nsignal-processing techniques. Furthermore, we examine the impact of the\nasymmetric gate on the interference visibility, finding that interference\nvisibility is enhanced under symmetric gate conditions.", "AI": {"tldr": "Graphene Mach-Zehnder interferometers with asymmetric p-n junctions are studied. Asymmetry tunes the area and creates new paths. Machine learning analyzes complex patterns, and symmetric gates improve visibility.", "motivation": "Investigate electron quantum interference in graphene using Mach-Zehnder interferometers with asymmetric p-n junctions, exploring how asymmetry affects interference patterns and visibility.", "method": "The paper proposes a phenomenological framework for analyzing graphene-based Mach-Zehnder interferometry with asymmetric p-n junctions. It utilizes machine learning-based Fourier transform to analyze complex beat oscillations and investigates the impact of asymmetric gates on interference visibility.", "result": "Asymmetric gate potentials allow tuning of the enclosed interferometer area, and higher filling factors lead to additional Mach-Zehnder interferometer pathways. Machine learning-based Fourier transform improves peak-to-background ratios for analyzing complex oscillations. Symmetric gate conditions enhance interference visibility.", "conclusion": "The study presents a phenomenological framework for graphene-based Mach-Zehnder interferometry with asymmetric p-n junction configurations, demonstrating tunability of interferometer area and the emergence of new pathways. It also highlights the effectiveness of machine learning for analyzing complex interference patterns and discusses the impact of asymmetric gates on interference visibility."}}
{"id": "2508.07148", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07148", "abs": "https://arxiv.org/abs/2508.07148", "authors": ["Sandesh Rao Mattu", "Nishant Mehrotra", "Saif Khan Mohammed", "Venkatesh Khammammetti", "Robert Calderbank"], "title": "Low-Complexity Equalization of Zak-OTFS in the Frequency Domain", "comment": "13 pages, 12 figures. Submitted to npj Wireless Technology", "summary": "4G/5G wireless standards use orthogonal frequency division multiplexing\n(OFDM) which is robust to frequency selectivity. Equalization is possible with\na single tap filter, and low-complexity equalization makes OFDM an attractive\nphysical layer. However the performance of OFDM degrades with mobility, since\nDoppler spreads introduce inter-carrier interference (ICI) between subcarriers\nand they are no longer orthogonal. Zak-transform based orthogonal time\nfrequency space (Zak-OTFS) modulation has been shown to be robust to doubly\nselective channels. Zak-OTFS signals are formed in the delay-Doppler (DD)\ndomain, converted to time domain (TD) for transmission and reception, then\nreturned to the DD domain for processing. The received signal is a\nsuperposition of many attenuated copies since the doubly selective channel\nintroduces delay and Doppler shifts. The received symbols are more difficult to\nequalize since they are subject to interference along both delay and Doppler\naxes. In this paper, we propose a new low-complexity method of equalizing\nZak-OTFS in the frequency domain (FD). We derive the FD system model and show\nthat it is unitarily equivalent to the DD system model. We show that the\nchannel matrix in the FD is banded, making it possible to apply conjugate\ngradient methods to reduce the complexity of equalization. We show that\ncomplexity of FD equalization is linear in the dimension of a Zak-OTFS frame.\nFor comparison the complexity of naive MMSE equalization is cubic in the frame\ndimension. Through numerical simulations we show that FD equalization of\nZak-OTFS achieves similar performance as equalization in DD domain.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4f4e\u590d\u6742\u5ea6\u9891\u57df\u5747\u8861\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3Zak-OTFS\u8c03\u5236\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e0b\u7684\u5747\u8861\u95ee\u9898\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u548c\u6027\u80fd\u3002", "motivation": "OFDM\u57284G/5G\u4e2d\u867d\u7136\u9c81\u68d2\u6027\u597d\u4f46\u968f\u7740\u79fb\u52a8\u6027\u7684\u589e\u52a0\uff0c\u591a\u666e\u52d2\u9891\u79fb\u4f1a\u5bfc\u81f4\u5b50\u8f7d\u6ce2\u95f4\u5e72\u6270\uff08ICI\uff09\uff0c\u4ece\u800c\u964d\u4f4e\u6027\u80fd\u3002Zak-OTFS\u8c03\u5236\u88ab\u8bc1\u660e\u5bf9\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u5177\u6709\u9c81\u68d2\u6027\uff0c\u4f46\u5176\u5747\u8861\u590d\u6742\u5ea6\u8f83\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u9891\u57df\uff08FD\uff09\u8fdb\u884cZak-OTFS\u5747\u8861\u7684\u65b0\u578b\u4f4e\u590d\u6742\u5ea6\u65b9\u6cd5\uff0c\u63a8\u5bfc\u4e86FD\u7cfb\u7edf\u6a21\u578b\u5e76\u8bc1\u660e\u5176\u4e0eDD\u57df\u7cfb\u7edf\u6a21\u578b\u662f\u9149\u7b49\u4ef7\u7684\uff0c\u5229\u7528\u4e86FD\u4fe1\u9053\u77e9\u9635\u7684\u5e26\u72b6\u7279\u6027\uff0c\u5e94\u7528\u5171\u8f6d\u68af\u5ea6\u6cd5\u964d\u4f4e\u4e86\u5747\u8861\u590d\u6742\u5ea6\uff0c\u590d\u6742\u5ea6\u4e3a\u7ebf\u6027\u3002", "result": "\u6240\u63d0\u51fa\u7684FD\u5747\u8861\u65b9\u6cd5\u7684\u590d\u6742\u5ea6\u662fZak-OTFS\u5e27\u7ef4\u5ea6\u7684\u7ebf\u6027\u51fd\u6570\uff0c\u800c\u6734\u7d20MMSE\u5747\u8861\u7684\u590d\u6742\u5ea6\u662f\u5e27\u7ef4\u5ea6\u7684\u4e09\u6b21\u51fd\u6570\u3002\u6570\u503c\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0cFD\u5747\u8861\u7684Zak-OTFS\u6027\u80fd\u4e0eDD\u57df\u5747\u8861\u76f8\u5f53\u3002", "conclusion": "\u63d0\u51fa\u7684\u9891\u57df\uff08FD\uff09\u7b49\u5316\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u5730\u5b9e\u73b0Zak-OTFS\u7684\u4f4e\u590d\u6742\u5ea6\u5747\u8861\uff0c\u5e76\u4e14\u5728\u6570\u503c\u6a21\u62df\u4e2d\u663e\u793a\u51fa\u4e0e\u65f6\u5ef6-\u591a\u666e\u52d2\uff08DD\uff09\u57df\u5747\u8861\u76f8\u5f53\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06753", "categories": ["cs.AI", "cs.LG", "cs.PF"], "pdf": "https://arxiv.org/pdf/2508.06753", "abs": "https://arxiv.org/abs/2508.06753", "authors": ["Evangelos Georganas", "Dhiraj Kalamkar", "Alexander Heinecke"], "title": "Pushing the Envelope of LLM Inference on AI-PC", "comment": null, "summary": "The advent of ultra-low-bit LLM models (1/1.58/2-bit), which match the\nperplexity and end-task performance of their full-precision counterparts using\nthe same model size, is ushering in a new era of LLM inference for\nresource-constrained environments such as edge devices and AI PCs. While these\nquantization advances promise models that are more cost-effective in terms of\nlatency, memory, throughput, and energy consumption, the computational\nefficiency of state-of-the-art (SOTA) inference runtimes (e.g., bitnet.cpp)\nused to deploy them remains underexplored. In this work, we take a bottom-up\napproach: we first design and implement 1-bit and 2-bit microkernels optimized\nfor modern CPUs, achieving peak computational efficiency across a variety of\nCPU platforms. We integrate these microkernels into a state-of-the-art LLM\ninference framework, namely PyTorch-TPP, and present end-to-end inference\nresults with 2-bit models that outperform the current SOTA runtime bitnet.cpp\nby up to 2.2x, and deliver up to 7x speedup compared to the 16-bit model\ninference. Our optimized runtime advances the state of LLM inference on AI PCs\nand edge devices, paving the way for efficient deployment of ultra-low-bit LLM\nmodels.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7\u4f18\u5316CPU\u5fae\u5185\u6838\u5e76\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8d85\u4f4e\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u6700\u9ad8\u53ef\u8fbe\u73b0\u6709SOTA\u8fd0\u884c\u65f6\u76842.2\u500d\u3002", "motivation": "\u4e3a\u4e86\u63a2\u7d22\u548c\u63d0\u5347\u8d85\u4f4e\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\uff081/1.58/2\u6bd4\u7279\uff09\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\u548cAI PC\uff09\u4e0b\u7684\u63a8\u7406\u6548\u7387\uff0c\u56e0\u4e3a\u73b0\u6709SOTA\u63a8\u7406\u8fd0\u884c\u65f6\u7684\u8ba1\u7b97\u6548\u7387\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u81ea\u4e0b\u800c\u4e0a\u7684\u65b9\u6cd5\uff0c\u9996\u5148\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u9488\u5bf9\u73b0\u4ee3CPU\u4f18\u5316\u76841\u6bd4\u7279\u548c2\u6bd4\u7279\u5fae\u5185\u6838\uff0c\u7136\u540e\u5728PyTorch-TPP\u8fd9\u4e00\u5148\u8fdb\u7684\u5927\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u6846\u67b6\u4e2d\u96c6\u6210\u4e86\u8fd9\u4e9b\u5fae\u5185\u6838\uff0c\u5e76\u5c55\u793a\u4e86\u7aef\u5230\u7aef\u7684\u63a8\u7406\u7ed3\u679c\u3002", "result": "\u96c6\u6210\u4e86\u4f18\u5316\u7684\u5fae\u5185\u6838\u7684PyTorch-TPP\u6846\u67b6\uff0c\u57282\u6bd4\u7279\u6a21\u578b\u4e0a\u7684\u7aef\u5230\u7aef\u63a8\u7406\u6027\u80fd\u6bd4\u73b0\u6709\u7684SOTA\u8fd0\u884c\u65f6bitnet.cpp\u5feb2.2\u500d\uff0c\u5e76\u4e14\u6bd416\u6bd4\u7279\u6a21\u578b\u5feb7\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u8bbe\u8ba1\u548c\u5b9e\u73b0\u4f18\u5316\u76841\u6bd4\u7279\u548c2\u6bd4\u7279\u5fae\u5185\u6838\uff0c\u5e76\u5c06\u5b83\u4eec\u96c6\u6210\u5230PyTorch-TPP\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8d85\u4f4e\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u5728CPU\u4e0a\u7684\u63a8\u7406\u6548\u7387\uff0c\u5176\u6027\u80fd\u8d85\u8d8a\u4e86\u73b0\u6709\u7684SOTA\u8fd0\u884c\u65f6bitnet.cpp\u8fbe2.2\u500d\uff0c\u5e76\u6bd416\u6bd4\u7279\u6a21\u578b\u5feb7\u500d\uff0c\u4e3a\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\uff08\u5982\u8fb9\u7f18\u8bbe\u5907\u548cAI PC\uff09\u4e2d\u9ad8\u6548\u90e8\u7f72\u5927\u8bed\u8a00\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06622", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06622", "abs": "https://arxiv.org/abs/2508.06622", "authors": ["Jeremiah Birrell", "Reza Ebrahimi"], "title": "Learning to Forget with Information Divergence Reweighted Objectives for Noisy Labels", "comment": "25 pages, 2 figures", "summary": "We introduce ANTIDOTE, a new class of objectives for learning under noisy\nlabels which are defined in terms of a relaxation over an\ninformation-divergence neighborhood. Using convex duality, we provide a\nreformulation as an adversarial training method that has similar computational\ncost to training with standard cross-entropy loss. We show that our approach\nadaptively reduces the influence of the samples with noisy labels during\nlearning, exhibiting a behavior that is analogous to forgetting those samples.\nANTIDOTE is effective in practical environments where label noise is inherent\nin the training data or where an adversary can alter the training labels.\nExtensive empirical evaluations on different levels of symmetric, asymmetric,\nhuman annotation, and real-world label noise show that ANTIDOTE outperforms\nleading comparable losses in the field and enjoys a time complexity that is\nvery close to that of the standard cross entropy loss.", "AI": {"tldr": "ANTIDOTE\u662f\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u8fdb\u884c\u5b66\u4e60\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5728\u6807\u7b7e\u566a\u58f0\u4e0b\u5b66\u4e60\uff0c\u6807\u7b7e\u566a\u58f0\u53ef\u80fd\u662f\u5728\u8bad\u7ec3\u6570\u636e\u4e2d\u56fa\u6709\u7684\uff0c\u6216\u8005\u53ef\u80fd\u88ab\u5bf9\u624b\u66f4\u6539\u3002", "method": "ANTIDOTE\u662f\u4e00\u79cd\u65b0\u7684\u76ee\u6807\u51fd\u6570\uff0c\u901a\u8fc7\u4fe1\u606f\u6563\u5ea6\u90bb\u57df\u7684\u677e\u5f1b\u6765\u5b9a\u4e49\uff0c\u5e76\u4f7f\u7528\u51f8\u5bf9\u5076\u91cd\u65b0\u8868\u8ff0\u4e3a\u5bf9\u6297\u6027\u8bad\u7ec3\u65b9\u6cd5\u3002", "result": "ANTIDOTE\u5728\u5b9e\u8df5\u4e2d\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u6807\u7b7e\u566a\u58f0\u7684\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "ANTIDOTE\u5728\u5404\u79cd\u6807\u7b7e\u566a\u58f0\uff08\u5bf9\u79f0\u3001\u4e0d\u5bf9\u79f0\u3001\u4eba\u5de5\u6ce8\u91ca\u548c\u771f\u5b9e\u4e16\u754c\u566a\u58f0\uff09\u7684\u5404\u79cd\u7ea7\u522b\u4e0a\u90fd\u4f18\u4e8e\u5176\u4ed6\u9886\u5148\u7684\u635f\u5931\u51fd\u6570\uff0c\u5e76\u4e14\u65f6\u95f4\u590d\u6742\u5ea6\u63a5\u8fd1\u6807\u51c6\u7684\u4ea4\u53c9\u71b5\u635f\u5931\u3002"}}
{"id": "2508.07043", "categories": ["cs.AI", "cs.MA", "q-bio.GN", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.07043", "abs": "https://arxiv.org/abs/2508.07043", "authors": ["Orion Li", "Vinayak Agarwal", "Summer Zhou", "Ashwin Gopinath", "Timothy Kassis"], "title": "K-Dense Analyst: Towards Fully Automated Scientific Analysis", "comment": null, "summary": "The complexity of modern bioinformatics analysis has created a critical gap\nbetween data generation and developing scientific insights. While large\nlanguage models (LLMs) have shown promise in scientific reasoning, they remain\nfundamentally limited when dealing with real-world analytical workflows that\ndemand iterative computation, tool integration and rigorous validation. We\nintroduce K-Dense Analyst, a hierarchical multi-agent system that achieves\nautonomous bioinformatics analysis through a dual-loop architecture. K-Dense\nAnalyst, part of the broader K-Dense platform, couples planning with validated\nexecution using specialized agents to decompose complex objectives into\nexecutable, verifiable tasks within secure computational environments. On\nBixBench, a comprehensive benchmark for open-ended biological analysis, K-Dense\nAnalyst achieves 29.2% accuracy, surpassing the best-performing language model\n(GPT-5) by 6.3 percentage points, representing nearly 27% improvement over what\nis widely considered the most powerful LLM available. Remarkably, K-Dense\nAnalyst achieves this performance using Gemini 2.5 Pro, which attains only\n18.3% accuracy when used directly, demonstrating that our architectural\ninnovations unlock capabilities far beyond the underlying model's baseline\nperformance. Our insights demonstrate that autonomous scientific reasoning\nrequires more than enhanced language models, it demands purpose-built systems\nthat can bridge the gap between high-level scientific objectives and low-level\ncomputational execution. These results represent a significant advance toward\nfully autonomous computational biologists capable of accelerating discovery\nacross the life sciences.", "AI": {"tldr": "K-Dense Analyst \u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u5176\u53cc\u5faa\u73af\u67b6\u6784\u663e\u8457\u63d0\u9ad8\u4e86\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u7684\u81ea\u4e3b\u6027\u548c\u51c6\u786e\u6027\uff0c\u5728 BixBench \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86 GPT-5 \u548c\u5355\u72ec\u7684 Gemini 2.5 Pro\u3002", "motivation": "\u73b0\u4ee3\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u7684\u590d\u6742\u6027\u5728\u6570\u636e\u751f\u6210\u4e0e\u79d1\u5b66\u6d1e\u5bdf\u53d1\u5c55\u4e4b\u95f4\u9020\u6210\u4e86\u5173\u952e\u5dee\u8ddd\u3002\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u79d1\u5b66\u63a8\u7406\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5728\u5904\u7406\u9700\u8981\u8fed\u4ee3\u8ba1\u7b97\u3001\u5de5\u5177\u96c6\u6210\u548c\u4e25\u683c\u9a8c\u8bc1\u7684\u5b9e\u9645\u5206\u6790\u5de5\u4f5c\u6d41\u65f6\u4ecd\u7136\u5b58\u5728\u6839\u672c\u6027\u9650\u5236\u3002", "method": "K-Dense Analyst \u662f\u4e00\u4e2a\u5206\u5c42\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u5b83\u4f7f\u7528\u53cc\u5faa\u73af\u67b6\u6784\u5c06\u590d\u6742\u7684\u76ee\u6807\u5206\u89e3\u4e3a\u53ef\u5728\u5b89\u5168\u8ba1\u7b97\u73af\u5883\u4e2d\u6267\u884c\u548c\u9a8c\u8bc1\u7684\u4efb\u52a1\u3002", "result": "\u5728\u9488\u5bf9\u5f00\u653e\u5f0f\u751f\u7269\u5206\u6790\u7684\u7efc\u5408\u57fa\u51c6 BixBench \u4e0a\uff0cK-Dense Analyst \u7684\u51c6\u786e\u7387\u8fbe\u5230 29.2%\uff0c\u8d85\u8fc7\u4e86\u6027\u80fd\u6700\u4f73\u7684\u8bed\u8a00\u6a21\u578b (GPT-5) 6.3 \u4e2a\u767e\u5206\u70b9\uff0c\u6bd4\u76ee\u524d\u6700\u5f3a\u5927\u7684 LLM\uff08\u88ab\u5e7f\u6cdb\u8ba4\u4e3a\uff09\u6709\u4e86\u8fd1 27% \u7684\u63d0\u5347\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cK-Dense Analyst \u4f7f\u7528 Gemini 2.5 Pro \u53d6\u5f97\u4e86\u8fd9\u4e00\u6027\u80fd\uff0c\u800c Gemini 2.5 Pro \u5355\u72ec\u4f7f\u7528\u65f6\u51c6\u786e\u7387\u4ec5\u4e3a 18.3%\uff0c\u8fd9\u8868\u660e\u6211\u4eec\u7684\u67b6\u6784\u521b\u65b0\u6240\u89e3\u9501\u7684\u80fd\u529b\u8fdc\u8fdc\u8d85\u51fa\u4e86\u5e95\u5c42\u6a21\u578b\u7684\u57fa\u7ebf\u6027\u80fd\u3002", "conclusion": "K-Dense Analyst \u662f\u4e00\u4e2a\u5206\u5c42\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u901a\u8fc7\u53cc\u5faa\u73af\u67b6\u6784\u5b9e\u73b0\u4e86\u81ea\u4e3b\u751f\u7269\u4fe1\u606f\u5b66\u5206\u6790\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u89c4\u5212\u548c\u7ecf\u9a8c\u8bc1\u7684\u6267\u884c\uff0c\u4ee5\u53ca\u4e13\u95e8\u7684\u4ee3\u7406\uff0c\u5c06\u590d\u6742\u7684\u76ee\u6807\u5206\u89e3\u4e3a\u53ef\u5728\u5b89\u5168\u8ba1\u7b97\u73af\u5883\u4e2d\u6267\u884c\u548c\u9a8c\u8bc1\u7684\u4efb\u52a1\u3002"}}
{"id": "2508.06992", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.06992", "abs": "https://arxiv.org/abs/2508.06992", "authors": ["Yuxiang Gao", "Zhicheng Zhong"], "title": "Mechanism of Anisotropic Crystallization and Phase Transitions under Van der Waals Squeezing", "comment": "5 pages, 4 figures", "summary": "Mechanical confinement strategies, such as van der Waals (vdW) squeezing,\nhave emerged as promising routes for synthesizing non-vdW two-dimensional (2D)\nlayers, surprisingly yielding high-quality single crystals with lateral sizes\napproaching 100 micrometer. However, the underlying mechanisms by which such a\nstraightforward approach overcomes the long-standing synthesis challenges of\nnon-vdW 2D materials remains a puzzle. Here, we investigate the crystallization\ndynamics and phase evolution of Bi under vdW confinement through molecular\ndynamics (MD) simulations powered by a machine-learning force filed fine-tuned\nand distilled from a pre-trained model with DFT-level accuracy. We reveal that\npressure-dependent layer modulation arises from a quantum confinement-driven\nanisotropic crystallization mechanism, in which out-of-plane layering occurs\nnearly two orders of magnitude faster than in-plane ordering. Two critical\ntransitions are identified: an alpha-to-beta phase transformation at 1.64 GPa,\nand a subsequent collapse into a single-atomic layer at 2.19 GPa. The formation\nof large-area single crystals is enabled by substrate-induced orientational\nselection and accelerated grain boundary migration, driven by atomic diffusion\nat elevated temperatures. These findings resolve the mechanistic origin of\nhigh-quality 2D crystal growth under confinement and establish guiding\nprinciples for the controlled synthesis of metastable 2D single crystals, with\nimplications for next-generation quantum and nanoelectronic devices.", "AI": {"tldr": "\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u589e\u5f3a\u7684\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u63ed\u793a\u4e86\u8303\u5fb7\u534e\u529b\u9650\u5236\u4e0b\u94cb\u7684\u5404\u5411\u5f02\u6027\u7ed3\u6676\u673a\u5236\uff0c\u89e3\u91ca\u4e86\u5982\u4f55\u5f62\u6210\u9ad8\u8d28\u91cf\u4e8c\u7ef4\u5355\u6676\uff0c\u5e76\u786e\u5b9a\u4e86\u5173\u952e\u7684\u76f8\u53d8\u538b\u529b\u3002", "motivation": "\u89e3\u51b3\u5728\u8303\u5fb7\u534e\u529b\u9650\u5236\u4e0b\uff0c\u975e\u8303\u5fb7\u534e\u529b\u4e8c\u7ef4\u6750\u6599\uff08\u5982\u94cb\uff09\u7684\u5408\u6210\u6311\u6218\uff0c\u7279\u522b\u662f\u7406\u89e3\u8fd9\u79cd\u7b80\u5355\u65b9\u6cd5\u5982\u4f55\u514b\u670d\u957f\u671f\u5b58\u5728\u7684\u5408\u6210\u96be\u9898\uff0c\u5e76\u63ed\u793a\u5176\u7ed3\u6676\u673a\u5236\u3002", "method": "\u901a\u8fc7\u5206\u5b50\u52a8\u529b\u5b66\uff08MD\uff09\u6a21\u62df\uff0c\u5e76\u5229\u7528\u7ecf\u8fc7\u9884\u8bad\u7ec3\u6a21\u578b\u5fae\u8c03\u548c\u84b8\u998f\u7684\u673a\u5668\u5b66\u4e60\u529b\u573a\uff08\u5177\u6709DFT\u7ea7\u522b\u7cbe\u5ea6\uff09\uff0c\u7814\u7a76\u4e86\u94cb\u5728\u8303\u5fb7\u534e\u529b\u9650\u5236\u4e0b\u7684\u7ed3\u6676\u52a8\u529b\u5b66\u548c\u76f8\u6f14\u5316\u3002", "result": "1. \u53d1\u73b0\u4e86\u7531\u91cf\u5b50\u9650\u5236\u9a71\u52a8\u7684\u5404\u5411\u5f02\u6027\u7ed3\u6676\u673a\u5236\uff0c\u5176\u4e2d\u9762\u5916\u6210\u5c42\u901f\u5ea6\u6bd4\u9762\u5185\u6392\u5e8f\u5feb\u8fd1\u4e24\u4e2a\u6570\u91cf\u7ea7\u3002\n2. \u8bc6\u522b\u51fa\u4e24\u4e2a\u5173\u952e\u7684\u76f8\u53d8\uff1a\u57281.64 GPa\u65f6\u53d1\u751f\u03b1\u5230\u03b2\u7684\u76f8\u53d8\uff0c\u57282.19 GPa\u65f6\u574d\u584c\u6210\u5355\u539f\u5b50\u5c42\u3002\n3. \u9610\u660e\u4e86\u5927\u9762\u79ef\u5355\u6676\u7684\u5f62\u6210\u5f52\u56e0\u4e8e\u886c\u5e95\u8bf1\u5bfc\u7684\u53d6\u5411\u9009\u62e9\u548c\u539f\u5b50\u6269\u6563\u9a71\u52a8\u7684\u6676\u754c\u8fc1\u79fb\u52a0\u901f\u3002\n4. \u5efa\u7acb\u4e86\u63a7\u5236\u5408\u6210\u4e9a\u7a33\u6001\u4e8c\u7ef4\u5355\u6676\u7684\u6307\u5bfc\u539f\u5219\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u5728\u8303\u5fb7\u534e\u529b\u9650\u5236\u4e0b\uff0c\u94cb\u7684\u7ed3\u6676\u52a8\u529b\u5b66\u548c\u76f8\u6f14\u5316\uff0c\u9610\u660e\u4e86\u514b\u670d\u975e\u8303\u5fb7\u534e\u529b\u4e8c\u7ef4\u6750\u6599\u5408\u6210\u6311\u6218\u7684\u673a\u5236\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u7531\u91cf\u5b50\u9650\u5236\u9a71\u52a8\u7684\u5404\u5411\u5f02\u6027\u7ed3\u6676\u673a\u5236\uff0c\u5176\u4e2d\u9762\u5916\u6210\u5c42\u901f\u5ea6\u8fdc\u5feb\u4e8e\u9762\u5185\u6392\u5e8f\u3002\u901a\u8fc7\u5206\u5b50\u52a8\u529b\u5b66\u6a21\u62df\uff0c\u8bc6\u522b\u51fa\u57281.64 GPa\u65f6\u53d1\u751f\u03b1\u5230\u03b2\u7684\u76f8\u53d8\uff0c\u4ee5\u53ca\u57282.19 GPa\u65f6\u574d\u584c\u6210\u5355\u539f\u5b50\u5c42\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5927\u9762\u79ef\u5355\u6676\u7684\u5f62\u6210\u5f97\u76ca\u4e8e\u886c\u5e95\u8bf1\u5bfc\u7684\u53d6\u5411\u9009\u62e9\u548c\u7531\u9ad8\u6e29\u4e0b\u539f\u5b50\u6269\u6563\u9a71\u52a8\u7684\u6676\u754c\u8fc1\u79fb\u52a0\u901f\u3002\u8fd9\u4e9b\u53d1\u73b0\u89e3\u51b3\u4e86\u9650\u5236\u4e0b\u9ad8\u8d28\u91cf\u4e8c\u7ef4\u6676\u4f53\u751f\u957f\u7684\u673a\u5236\u8d77\u6e90\uff0c\u5e76\u4e3a\u63a7\u5236\u5408\u6210\u4e9a\u7a33\u6001\u4e8c\u7ef4\u5355\u6676\u5efa\u7acb\u4e86\u6307\u5bfc\u539f\u5219\uff0c\u5bf9\u4e0b\u4e00\u4ee3\u91cf\u5b50\u548c\u7eb3\u7c73\u7535\u5b50\u5668\u4ef6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.07744", "categories": ["cs.DC", "cs.NI", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07744", "abs": "https://arxiv.org/abs/2508.07744", "authors": ["Ingo Friese", "Jochen Klaffer", "Mandy Galkow-Schneider", "Sergiy Melnyk", "Qiuheng Zhou", "Hans Dieter Schotten"], "title": "Over-the-Top Resource Broker System for Split Computing: An Approach to Distribute Cloud Computing Infrastructure", "comment": null, "summary": "6G network architectures will usher in a wave of innovative services and\ncapabilities, introducing concepts like split computing and dynamic processing\nnodes. This implicates a paradigm where accessing resources seamlessly aligns\nwith diverse processing node characteristics, ensuring a uniform interface. In\nthis landscape, the identity of the operator becomes inconsequential, paving\nthe way for a collaborative ecosystem where multiple providers contribute to a\nshared pool of resources. At the core of this vision is the guarantee of\nspecific performance parameters, precisely tailored to the location and service\nrequirements. A consistent layer, as the abstraction of the complexities of\ndifferent infrastructure providers, is needed to simplify service deployment.\nOne promising approach is the introduction of an over-the-top broker for\nresource allocation, which streamlines the integration of these services into\nthe network and cloud infrastructure of the future. This paper explores the\nrole of the broker in two split computing scenarios. By abstracting the\ncomplexities of various infrastructures, the broker proves to be a versatile\nsolution applicable not only to cloud environments but also to networks and\nbeyond. Additionally, a detailed discussion of a proof-of-concept\nimplementation provides insights into the broker's actual architectural\nframework.", "AI": {"tldr": "6G\u7f51\u7edc\u9700\u8981\u4e00\u4e2a\u7f6e\u9876\u4ee3\u7406\u6765\u7b80\u5316\u670d\u52a1\u90e8\u7f72\uff0c\u8be5\u4ee3\u7406\u80fd\u9690\u85cf\u4e0d\u540c\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u6742\u6027\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u73af\u5883\uff0c\u8bba\u6587\u5bf9\u6b64\u8fdb\u884c\u4e86\u63a2\u8ba8\u5e76\u63d0\u4f9b\u4e86\u6982\u5ff5\u9a8c\u8bc1\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf96G\u7f51\u7edc\u67b6\u6784\u5e26\u6765\u7684\u521b\u65b0\u670d\u52a1\u548c\u80fd\u529b\uff0c\u5982\u5206\u79bb\u8ba1\u7b97\u548c\u52a8\u6001\u5904\u7406\u8282\u70b9\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u7edf\u4e00\u63a5\u53e3\u3001\u9690\u85cf\u4e0d\u540c\u57fa\u7840\u8bbe\u65bd\u63d0\u4f9b\u5546\u590d\u6742\u6027\u7684\u65b9\u6cd5\uff0c\u4ee5\u7b80\u5316\u670d\u52a1\u90e8\u7f72\u3002", "method": "\u901a\u8fc7\u5bf9\u4e24\u79cd\u5206\u79bb\u8ba1\u7b97\u573a\u666f\u7684\u5206\u6790\uff0c\u5e76\u63d0\u4f9b\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u7684\u8be6\u7ec6\u8ba8\u8bba\uff0c\u6765\u63a2\u8ba8\u7f6e\u9876\u4ee3\u7406\u5728\u8d44\u6e90\u5206\u914d\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7f6e\u9876\u4ee3\u7406\u4f5c\u4e3a\u8d44\u6e90\u5206\u914d\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u7b80\u5316\u670d\u52a1\u96c6\u6210\uff0c\u5e76\u4e14\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u524d\u666f\uff0c\u4e0d\u4ec5\u9650\u4e8e\u4e91\u73af\u5883\uff0c\u4e5f\u9002\u7528\u4e8e\u7f51\u7edc\u7b49\u5176\u4ed6\u9886\u57df\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e00\u4e2a\u7f6e\u9876\u7684\u4ee3\u7406\uff08over-the-top broker\uff09\u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u666f\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u7b80\u5316\u5728\u672a\u6765\u7f51\u7edc\u548c\u4e91\u57fa\u7840\u8bbe\u65bd\u4e2d\u7684\u670d\u52a1\u96c6\u6210\u3002\u8be5\u4ee3\u7406\u901a\u8fc7\u62bd\u8c61\u5316\u5404\u79cd\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u6742\u6027\uff0c\u53ef\u4ee5\u5e94\u7528\u4e8e\u7f51\u7edc\u3001\u4e91\u7b49\u591a\u79cd\u73af\u5883\u3002\u6b64\u5916\uff0c\u8bba\u6587\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u5b9e\u73b0\u7684\u8be6\u7ec6\u8ba8\u8bba\uff0c\u63ed\u793a\u4e86\u4ee3\u7406\u7684\u5b9e\u9645\u67b6\u6784\u6846\u67b6\u3002"}}
{"id": "2508.06742", "categories": ["cs.RO", "cs.AI", "cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06742", "abs": "https://arxiv.org/abs/2508.06742", "authors": ["Alejandro Murillo-Gonzalez", "Junhong Xu", "Lantao Liu"], "title": "Learning Causal Structure Distributions for Robust Planning", "comment": null, "summary": "Structural causal models describe how the components of a robotic system\ninteract. They provide both structural and functional information about the\nrelationships that are present in the system. The structural information\noutlines the variables among which there is interaction. The functional\ninformation describes how such interactions work, via equations or learned\nmodels. In this paper we find that learning the functional relationships while\naccounting for the uncertainty about the structural information leads to more\nrobust dynamics models which improves downstream planning, while using\nsignificantly lower computational resources. This in contrast with common\nmodel-learning methods that ignore the causal structure and fail to leverage\nthe sparsity of interactions in robotic systems. We achieve this by estimating\na causal structure distribution that is used to sample causal graphs that\ninform the latent-space representations in an encoder-multidecoder\nprobabilistic model. We show that our model can be used to learn the dynamics\nof a robot, which together with a sampling-based planner can be used to perform\nnew tasks in novel environments, provided an objective function for the new\nrequirement is available. We validate our method using manipulators and mobile\nrobots in both simulation and the real-world. Additionally, we validate the\nlearned dynamics' adaptability and increased robustness to corrupted inputs and\nchanges in the environment, which is highly desirable in challenging real-world\nrobotics scenarios. Video: https://youtu.be/X6k5t7OOnNc.", "AI": {"tldr": "\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u51fd\u6570\u5173\u7cfb\u548c\u8003\u8651\u7ed3\u6784\u4fe1\u606f\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u4ee5\u63d0\u9ad8\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u56e0\u679c\u7ed3\u6784\u3001\u65e0\u6cd5\u5229\u7528\u7a00\u758f\u6027\u4ea4\u4e92\u7684\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u7684\u9c81\u68d2\u6027\uff0c\u4ee5\u5e94\u5bf9\u65b0\u4efb\u52a1\u548c\u65b0\u73af\u5883\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f30\u8ba1\u56e0\u679c\u7ed3\u6784\u5206\u5e03\u7684\u65b9\u6cd5\uff0c\u8be5\u5206\u5e03\u7528\u4e8e\u91c7\u6837\u56e0\u679c\u56fe\uff0c\u5e76\u6307\u5bfc\u7f16\u7801\u5668-\u591a\u89e3\u7801\u5668\u6982\u7387\u6a21\u578b\u4e2d\u7684\u6f5c\u7a7a\u95f4\u8868\u793a\u3002", "result": "\u5b66\u4e60\u5230\u7684\u52a8\u529b\u5b66\u6a21\u578b\u4e0e\u91c7\u6837\u65b9\u6cd5\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u6267\u884c\u65b0\u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u4e2d\u5bf9\u64cd\u7eb5\u5668\u548c\u79fb\u52a8\u673a\u5668\u4eba\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002\u8be5\u6a21\u578b\u8fd8\u8868\u73b0\u51fa\u5bf9\u635f\u574f\u8f93\u5165\u548c\u73af\u5883\u53d8\u5316\u7684\u9002\u5e94\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u6982\u7387\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5b66\u4e60\u673a\u5668\u4eba\u52a8\u529b\u5b66\u6a21\u578b\u3002\u901a\u8fc7\u540c\u65f6\u5b66\u4e60\u51fd\u6570\u5173\u7cfb\u548c\u8003\u8651\u7ed3\u6784\u4fe1\u606f\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u9c81\u68d2\u6027\u5e76\u964d\u4f4e\u8ba1\u7b97\u8d44\u6e90\u6d88\u8017\u3002"}}
{"id": "2508.06850", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.06850", "abs": "https://arxiv.org/abs/2508.06850", "authors": ["Ziyad Imara", "Khadija El Anouz", "\u0130lkay Demir", "Abderrahim El Allati"], "title": "Squeezed Magnons-Induced Nonreciprocal Entanglement in a Magnomechanical Cavity", "comment": "9 pages, 7 figures", "summary": "Magnomechanical cavities offer a new frontier in quantum electrodynamics that\ngive rise to several significant theoretical and experimental results. In this\npaper, we propose a novel theoretical mechanism for achieving a nonreciprocal\nmacroscopic entanglement between magnons, photons and phonons, based on the use\nof an alternative squeezed magnons method. Indeed, in contrast to conventional\napproaches, we show how precise control of the amplitude and phase of the\nsqueezed mode allows to obtain a tunable nonreciprocity of entanglement. The\nmagnons resulting from the collective motion of the spin in a macroscopic\nferrimagnet become coupled to the microwave photons via magnetic dipole\ninteraction and to the phonons via magnetostrictive interaction. Moreover, we\nestablish that the proposed scheme achieves ideal nonreciprocity, which can be\noptimized by cavity-magnon coupling and bath temperature control. Finally, by\nusing the parameters that are experimentally feasible with current\ntechnologies, this work provides new perspectives for hybrid magnon-based\nquantum technologies.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u66ff\u4ee3\u538b\u7f29\u6a21\u5f0f\u5b9e\u73b0\u78c1\u5fae\u8154\u4e2d\u78c1\u5b50\u3001\u5149\u5b50\u548c\u58f0\u5b50\u975e\u4e92\u6613\u6027\u5b8f\u89c2\u7ea0\u7f20\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u5728\u5b9e\u9a8c\u4e0a\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u63a2\u7d22\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u7684\u65b0\u524d\u6cbf\uff0c\u5b9e\u73b0\u5b8f\u89c2\u7ea0\u7f20\u7684\u975e\u4e92\u6613\u6027\uff0c\u5e76\u4e3a\u6df7\u5408\u78c1\u5fae\u8154\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u65b0\u7684\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5229\u7528\u66ff\u4ee3\u538b\u7f29\u6a21\u5f0f\u7684\u78c1\u5fae\u8154\uff0c\u5e76\u63a7\u5236\u5176\u632f\u5e45\u548c\u76f8\u4f4d\uff0c\u5b9e\u73b0\u4e86\u78c1\u5b50\u3001\u5149\u5b50\u548c\u58f0\u5b50\u4e4b\u95f4\u7684\u975e\u4e92\u6613\u6027\u5b8f\u89c2\u7ea0\u7f20\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u53ef\u8c03\u63a7\u7684\u975e\u4e92\u6613\u6027\u5b8f\u89c2\u7ea0\u7f20\uff0c\u5e76\u4e14\u8be5\u65b9\u6848\u7684\u7406\u60f3\u975e\u4e92\u6613\u6027\u53ef\u4ee5\u901a\u8fc7\u8154-\u78c1\u5b50\u8026\u5408\u548c\u73af\u5883\u6e29\u5ea6\u8fdb\u884c\u4f18\u5316\uff0c\u6240\u7528\u53c2\u6570\u5177\u6709\u5b9e\u9a8c\u53ef\u884c\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u66ff\u4ee3\u538b\u7f29\u6a21\u5f0f\u7684\u78c1\u5fae\u8154\u65b0\u65b9\u6cd5\uff0c\u5728\u5b9e\u73b0\u5b8f\u89c2\u7ea0\u7f20\u7684\u975e\u4e92\u6613\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u7406\u8bba\u4e0a\u7684\u7a81\u7834\uff0c\u5e76\u4e14\u8be5\u65b9\u6848\u5728\u5b9e\u9a8c\u53c2\u6570\u4e0a\u5177\u6709\u53ef\u884c\u6027\uff0c\u4e3a\u57fa\u4e8e\u78c1\u5fae\u8154\u7684\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.06671", "categories": ["cs.CL", "cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06671", "abs": "https://arxiv.org/abs/2508.06671", "authors": ["Swati Rajwal", "Shivank Garg", "Reem Abdel-Salam", "Abdelrahman Zayed"], "title": "Do Biased Models Have Biased Thoughts?", "comment": "Accepted at main track of the Second Conference on Language Modeling\n  (COLM 2025)", "summary": "The impressive performance of language models is undeniable. However, the\npresence of biases based on gender, race, socio-economic status, physical\nappearance, and sexual orientation makes the deployment of language models\nchallenging. This paper studies the effect of chain-of-thought prompting, a\nrecent approach that studies the steps followed by the model before it\nresponds, on fairness. More specifically, we ask the following question:\n\\textit{Do biased models have biased thoughts}? To answer our question, we\nconduct experiments on $5$ popular large language models using fairness metrics\nto quantify $11$ different biases in the model's thoughts and output. Our\nresults show that the bias in the thinking steps is not highly correlated with\nthe output bias (less than $0.6$ correlation with a $p$-value smaller than\n$0.001$ in most cases). In other words, unlike human beings, the tested models\nwith biased decisions do not always possess biased thoughts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u53d1\u73b0\uff0c\u5927\u8bed\u8a00\u6a21\u578b\u5728\u601d\u8003\u8fc7\u7a0b\u4e2d\u7684\u504f\u89c1\u4e0e\u5176\u6700\u7ec8\u8f93\u51fa\u7684\u504f\u89c1\u5e76\u4e0d\u603b\u662f\u4e00\u81f4\uff0c\u8fd9\u4e0e\u4eba\u7c7b\u4e0d\u540c\u3002", "motivation": "\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u4e2d\u5b58\u5728\u7684\u6027\u522b\u3001\u79cd\u65cf\u3001\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u3001\u5916\u8c8c\u548c\u6027\u53d6\u5411\u7b49\u504f\u89c1\uff0c\u4ee5\u53ca\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u5bf9\u516c\u5e73\u6027\u7684\u5f71\u54cd\u3002", "method": "\u7814\u7a76\u4e86\u94fe\u5f0f\u601d\u8003\u63d0\u793a\u5bf9\u516c\u5e73\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u5bf95\u79cd\u4e3b\u6d41\u5927\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4f7f\u7528\u516c\u5e73\u6027\u6307\u6807\u91cf\u5316\u4e86\u6a21\u578b\u601d\u8003\u548c\u8f93\u51fa\u4e2d\u768411\u79cd\u4e0d\u540c\u504f\u89c1\u3002", "result": "\u504f\u89c1\u5728\u601d\u8003\u6b65\u9aa4\u548c\u8f93\u51fa\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u4e0d\u9ad8\uff08\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u76f8\u5173\u6027\u4f4e\u4e8e0.6\uff0cp\u503c\u5c0f\u4e8e0.001\uff09\u3002", "conclusion": "\u4e0e\u4eba\u7c7b\u4e0d\u540c\uff0c\u6240\u6d4b\u8bd5\u7684\u6a21\u578b\u5373\u4f7f\u5728\u51b3\u7b56\u4e2d\u6709\u504f\u89c1\uff0c\u5176\u601d\u8003\u8fc7\u7a0b\u4e5f\u672a\u5fc5\u6709\u504f\u89c1\u3002"}}
{"id": "2508.07138", "categories": ["cs.LG", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07138", "abs": "https://arxiv.org/abs/2508.07138", "authors": ["Yashwant Krishna Pagoti", "Arunesh Sinha", "Shamik Sural"], "title": "Strategic Incentivization for Locally Differentially Private Federated Learning", "comment": null, "summary": "In Federated Learning (FL), multiple clients jointly train a machine learning\nmodel by sharing gradient information, instead of raw data, with a server over\nmultiple rounds. To address the possibility of information leakage in spite of\nsharing only the gradients, Local Differential Privacy (LDP) is often used. In\nLDP, clients add a selective amount of noise to the gradients before sending\nthe same to the server. Although such noise addition protects the privacy of\nclients, it leads to a degradation in global model accuracy. In this paper, we\nmodel this privacy-accuracy trade-off as a game, where the sever incentivizes\nthe clients to add a lower degree of noise for achieving higher accuracy, while\nthe clients attempt to preserve their privacy at the cost of a potential loss\nin accuracy. A token based incentivization mechanism is introduced in which the\nquantum of tokens credited to a client in an FL round is a function of the\ndegree of perturbation of its gradients. The client can later access a newly\nupdated global model only after acquiring enough tokens, which are to be\ndeducted from its balance. We identify the players, their actions and payoff,\nand perform a strategic analysis of the game. Extensive experiments were\ncarried out to study the impact of different parameters.", "AI": {"tldr": "This paper uses game theory to balance privacy and accuracy in federated learning. A token system encourages clients to share less noisy data, improving overall model accuracy while maintaining client privacy.", "motivation": "To address the degradation in global model accuracy caused by noise addition in Local Differential Privacy (LDP) used in Federated Learning (FL) for privacy protection, and to incentivize clients to add a lower degree of noise.", "method": "The paper models the privacy-accuracy trade-off as a game with a token-based incentivization mechanism. It identifies players, actions, and payoffs, and performs a strategic analysis. Experiments are used to study the impact of different parameters.", "result": "A token-based incentivization mechanism is introduced where the number of tokens credited to a client is a function of the perturbation degree of its gradients. Clients need enough tokens to access updated global models. The study analyzes the game strategically and through experiments.", "conclusion": "The paper models the privacy-accuracy trade-off in Federated Learning with Local Differential Privacy as a game, introducing a token-based incentivization mechanism to encourage clients to reduce noise, thereby improving global model accuracy while preserving privacy. Strategic analysis and extensive experiments were conducted to study the impact of various parameters."}}
{"id": "2508.06546", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06546", "abs": "https://arxiv.org/abs/2508.06546", "authors": ["Qi Xun Yeo", "Yanyan Li", "Gim Hee Lee"], "title": "Statistical Confidence Rescoring for Robust 3D Scene Graph Generation from Multi-View Images", "comment": "This paper has been accepted in ICCV 25", "summary": "Modern 3D semantic scene graph estimation methods utilize ground truth 3D\nannotations to accurately predict target objects, predicates, and\nrelationships. In the absence of given 3D ground truth representations, we\nexplore leveraging only multi-view RGB images to tackle this task. To attain\nrobust features for accurate scene graph estimation, we must overcome the noisy\nreconstructed pseudo point-based geometry from predicted depth maps and reduce\nthe amount of background noise present in multi-view image features. The key is\nto enrich node and edge features with accurate semantic and spatial information\nand through neighboring relations. We obtain semantic masks to guide feature\naggregation to filter background features and design a novel method to\nincorporate neighboring node information to aid robustness of our scene graph\nestimates. Furthermore, we leverage on explicit statistical priors calculated\nfrom the training summary statistics to refine node and edge predictions based\non their one-hop neighborhood. Our experiments show that our method outperforms\ncurrent methods purely using multi-view images as the initial input. Our\nproject page is available at https://qixun1.github.io/projects/SCRSSG.", "AI": {"tldr": "Leveraging multi-view RGB images for 3D semantic scene graph estimation by overcoming noisy geometry and background noise through semantic masks and neighboring node information, outperforming existing methods.", "motivation": "In the absence of given 3D ground truth representations, we explore leveraging only multi-view RGB images to tackle this task. To attain robust features for accurate scene graph estimation, we must overcome the noisy reconstructed pseudo point-based geometry from predicted depth maps and reduce the amount of background noise present in multi-view image features.", "method": "We obtain semantic masks to guide feature aggregation to filter background features and design a novel method to incorporate neighboring node information to aid robustness of our scene graph estimates. Furthermore, we leverage on explicit statistical priors calculated from the training summary statistics to refine node and edge predictions based on their one-hop neighborhood.", "result": "Our experiments show that our method outperforms current methods purely using multi-view images as the initial input.", "conclusion": "Our method outperforms current methods purely using multi-view images as the initial input."}}
{"id": "2508.07376", "categories": ["eess.SY", "cs.SY", "90B25, 65C05, 86A15"], "pdf": "https://arxiv.org/pdf/2508.07376", "abs": "https://arxiv.org/abs/2508.07376", "authors": ["Huangbin Liang", "Beatriz Moya", "Francisco Chinesta", "Eleni Chatzi"], "title": "A Multi-Model Probabilistic Framework for Seismic Risk Assessment and Retrofit Planning of Electric Power Networks", "comment": "13 figures", "summary": "Electric power networks are critical lifelines, and their disruption during\nearthquakes can lead to severe cascading failures and significantly hinder\npost-disaster recovery. Enhancing their seismic resilience requires identifying\nand strengthening vulnerable components in a cost-effective and system-aware\nmanner. However, existing studies often overlook the systemic behavior of power\nnetworks under seismic loading. Common limitations include isolated component\nanalyses that neglect network-wide interdependencies, oversimplified damage\nmodels assuming binary states or damage independence, and the exclusion of\nelectrical operational constraints. These simplifications can result in\ninaccurate risk estimates and inefficient retrofit decisions. This study\nproposes a multi-model probabilistic framework for seismic risk assessment and\nretrofit planning of electric power systems. The approach integrates: (1)\nregional seismic hazard characterization with ground motion prediction and\nspatial correlation models; (2) component-level damage analysis using fragility\nfunctions and multi-state damage-functionality mappings; (3) system-level\ncascading impact evaluation through graph-based island detection and\nconstrained optimal power flow analysis; and (4) retrofit planning via\nheuristic optimization to minimize expected annual functionality loss (EAFL)\nunder budget constraints. Uncertainty is propagated throughout the framework\nusing Monte Carlo simulation. The methodology is demonstrated on the IEEE\n24-bus Reliability Test System, showcasing its ability to capture cascading\nfailures, identify critical components, and generate effective retrofit\nstrategies. Results underscore the potential of the framework as a scalable,\ndata-informed decision-support tool for enhancing the seismic resilience of\npower infrastructure.", "AI": {"tldr": "\u4e3a\u4e86\u63d0\u9ad8\u7535\u529b\u7cfb\u7edf\u7684\u6297\u9707\u80fd\u529b\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5730\u9707\u98ce\u9669\u548c\u89c4\u5212\u4fee\u590d\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u8003\u8651\u4e86\u7cfb\u7edf\u6548\u5e94\u3001\u7ec4\u4ef6\u635f\u574f\u548c\u8fd0\u884c\u7ea6\u675f\uff0c\u5e76\u5728IEEE 24\u603b\u7ebf\u7cfb\u7edf\u4e0a\u8fdb\u884c\u4e86\u6f14\u793a\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u5f80\u5f80\u5ffd\u7565\u4e86\u7535\u529b\u7f51\u7edc\u5728\u5730\u9707\u8377\u8f7d\u4e0b\u7684\u7cfb\u7edf\u884c\u4e3a\u3002\u5e38\u89c1\u7684\u5c40\u9650\u6027\u5305\u62ec\uff1a\u5b64\u7acb\u7684\u7ec4\u4ef6\u5206\u6790\uff0c\u5ffd\u7565\u4e86\u5168\u7f51\u7edc\u7684\u76f8\u4e92\u4f9d\u8d56\u6027\uff1b\u8fc7\u4e8e\u7b80\u5316\u7684\u635f\u574f\u6a21\u578b\uff0c\u5047\u8bbe\u4e8c\u5143\u72b6\u6001\u6216\u635f\u574f\u72ec\u7acb\u6027\uff1b\u4ee5\u53ca\u6392\u9664\u4e86\u7535\u6c14\u8fd0\u884c\u7ea6\u675f\u3002\u8fd9\u4e9b\u7b80\u5316\u53ef\u80fd\u5bfc\u81f4\u4e0d\u51c6\u786e\u7684\u98ce\u9669\u4f30\u8ba1\u548c\u4f4e\u6548\u7684\u4fee\u590d\u51b3\u7b56\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u578b\u6982\u7387\u6846\u67b6\uff0c\u7528\u4e8e\u7535\u529b\u7cfb\u7edf\u7684\u6297\u9707\u98ce\u9669\u8bc4\u4f30\u548c\u4fee\u590d\u89c4\u5212\u3002\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u533a\u57df\u5730\u9707\u5371\u9669\u6027\u7279\u5f81\u5316\u3001\u57fa\u4e8e\u8106\u5f31\u6027\u51fd\u6570\u7684\u7ec4\u4ef6\u7ea7\u635f\u4f24\u5206\u6790\u3001\u57fa\u4e8e\u56fe\u7684\u5b64\u5c9b\u68c0\u6d4b\u548c\u7ea6\u675f\u6700\u4f18\u6f6e\u6d41\u5206\u6790\u7684\u7cfb\u7edf\u7ea7\u7ea7\u8054\u5f71\u54cd\u8bc4\u4f30\uff0c\u4ee5\u53ca\u901a\u8fc7\u542f\u53d1\u5f0f\u4f18\u5316\u6700\u5c0f\u5316\u9884\u7b97\u7ea6\u675f\u4e0b\u7684\u9884\u671f\u5e74\u5ea6\u529f\u80fd\u635f\u5931\uff08EAFL\uff09\u7684\u4fee\u590d\u89c4\u5212\u3002\u4f7f\u7528\u8499\u7279\u5361\u6d1b\u6a21\u62df\u5c06\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u5230\u6574\u4e2a\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5bf9IEEE 24\u603b\u7ebf\u53ef\u9760\u6027\u6d4b\u8bd5\u7cfb\u7edf\u7684\u6f14\u793a\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u591f\u6355\u83b7\u7ea7\u8054\u6545\u969c\u3001\u8bc6\u522b\u5173\u952e\u7ec4\u4ef6\u5e76\u751f\u6210\u6709\u6548\u7684\u4fee\u590d\u7b56\u7565\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u6f5c\u529b\u6210\u4e3a\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u3001\u6570\u636e\u9a71\u52a8\u7684\u51b3\u7b56\u652f\u6301\u5de5\u5177\uff0c\u7528\u4e8e\u63d0\u9ad8\u7535\u529b\u57fa\u7840\u8bbe\u65bd\u7684\u6297\u9707\u80fd\u529b\u3002"}}
{"id": "2508.07398", "categories": ["cond-mat.mes-hall", "cond-mat.quant-gas", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07398", "abs": "https://arxiv.org/abs/2508.07398", "authors": ["Bozhen Zhou", "Pan Zhang", "Yucheng Wang", "Chao Yang"], "title": "Dissipation-induced Half Quantized Conductance in One-dimensional Topological Systems", "comment": "6 pages, 4 figures", "summary": "Quantized conductance from topologically protected edge states is a hallmark\nof two-dimensional topological phases. In contrast, edge states in\none-dimensional (1D) topological systems cannot transmit current across the\ninsulating bulk, rendering their topological nature invisible in transport. In\nthis work, we investigate the transport properties of the Su-Schrieffer-Heeger\nmodel with gain and loss, and show that the zero-energy conductance exhibits\nqualitatively distinct behaviors between the topologically trivial and\nnontrivial phases, depending on the hybridization and dissipation strengths.\nCrucially, we analytically demonstrate that the conductance can become\nhalf-quantized in the topologically nontrivial phase, a feature absent in the\ntrivial phase. We further show that the half quantization predominantly\noriginates from transport channels involving gain/loss and edge states. Our\nresults uncover a new mechanism for realizing quantized transport in 1D\ntopological systems and highlight the nontrivial role of dissipation in\nenabling topological signatures in open quantum systems.", "AI": {"tldr": "\u4e00\u7ef4\u62d3\u6251\u7cfb\u7edf\u4e2d\u7684\u8fb9\u7f18\u6001\u901a\u5e38\u4e0d\u4f20\u8f93\u7535\u6d41\uff0c\u4f46\u672c\u7814\u7a76\u53d1\u73b0\u589e\u76ca\u548c\u635f\u8017\u53ef\u4ee5\u4f7f\u4e00\u7ef4\u62d3\u6251\u7cfb\u7edf\u8868\u73b0\u51fa\u534a\u91cf\u5b50\u5316\u7684\u5bfc\u5e26\uff0c\u8fd9\u4e3a\u4e86\u89e3\u548c\u5229\u7528\u4e00\u7ef4\u62d3\u6251\u7cfb\u7edf\u7684\u62d3\u6251\u6027\u8d28\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "motivation": "\u4e00\u7ef4\u62d3\u6251\u7cfb\u7edf\u4e2d\u7684\u8fb9\u7f18\u6001\u65e0\u6cd5\u8de8\u8d8a\u7edd\u7f18\u4f53\u4e3b\u4f53\u4f20\u8f93\u7535\u6d41\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u7684\u62d3\u6251\u6027\u8d28\u5728\u8f93\u8fd0\u4e2d\u53d8\u5f97\u4e0d\u53ef\u89c1\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u589e\u76ca\u548c\u635f\u8017\u7684Su-Schrieffer-Heeger\u6a21\u578b\uff0c\u5e76\u5206\u6790\u96f6\u80fd\u8017\u5bfc\u5e26\u7684\u884c\u4e3a\u3002", "result": "\u96f6\u80fd\u8017\u5bfc\u5e26\u7684\u884c\u4e3a\u5728\u62d3\u6251\u5e73\u51e1\u76f8\u548c\u975e\u5e73\u51e1\u76f8\u4e4b\u95f4\u8868\u73b0\u51fa\u663e\u8457\u7684\u5dee\u5f02\uff0c\u5e76\u4e14\u5728\u62d3\u6251\u975e\u5e73\u51e1\u76f8\u4e2d\uff0c\u5bfc\u5e26\u53ef\u4ee5\u8868\u73b0\u51fa\u534a\u91cf\u5b50\u5316\u7684\u7279\u5f81\uff0c\u800c\u8fd9\u662f\u5728\u62d3\u6251\u5e73\u51e1\u76f8\u4e2d\u4e0d\u5b58\u5728\u7684\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u4e00\u7a2e\u5728\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u91cf\u5b50\u5316\u8f93\u8fd0\u7684\u65b0\u673a\u5236\uff0c\u5e76\u5f3a\u8c03\u4e86\u8017\u6563\u5728\u5b9e\u73b0\u4e00\u7ef4\u62d3\u6251\u7cfb\u7edf\u4e2d\u7684\u62d3\u6251\u7279\u5f81\u65b9\u9762\u6240\u8d77\u7684\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.07160", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07160", "abs": "https://arxiv.org/abs/2508.07160", "authors": ["Deyu Lu", "Xiaoli Ma", "Yiyin Wang"], "title": "Vector Orthogonal Chirp Division Multiplexing Over Doubly Selective Channels", "comment": null, "summary": "In this letter, we extend orthogonal chirp division multiplexing (OCDM) to\nvector OCDM (VOCDM) to provide more design freedom to deal with doubly\nselective channels. The VOCDM modulation is implemented by performing M\nparallel N-size inverse discrete Fresnel transforms (IDFnT). Based on the\ncomplex exponential basis expansion model (CE-BEM) for doubly selective\nchannels, we derive the VOCDM input-output relationship, and show performance\ntradeoffs of VOCDM with respect to (w.r.t.) its modulation parameters M and N.\nSpecifically, we investigate the diversity and peak-to-average power ratio\n(PAPR) of VOCDM w.r.t. M and N. Under doubly selective channels, VOCDM exhibits\nsuperior diversity performance as long as the parameters M and N are configured\nto satisfy some constraints from the delay and the Doppler spreads of the\nchannel, respectively. Furthermore, the PAPR of VOCDM signals decreases with a\ndecreasing N. These theoretical findings are verified through numerical\nsimulations.", "AI": {"tldr": "VOCDM\u901a\u8fc7\u5e76\u884cIDFT\u5b9e\u73b0\uff0c\u53ef\u4ee5\u4f18\u5316\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e0b\u7684\u5206\u96c6\u6027\u80fd\u548cPAPR\u3002", "motivation": "\u4e3a\u4e86\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e2d\u63d0\u4f9b\u66f4\u591a\u7684\u8bbe\u8ba1\u81ea\u7531\u5ea6\uff0c\u5c06OCDM\u6269\u5c55\u5230VOCDM\u3002", "method": "\u901a\u8fc7\u6267\u884cM\u4e2a\u5e76\u884c\u7684N\u70b9\u9006\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\uff08IDFT\uff09\u6765\u5b9e\u73b0VOCDM\u8c03\u5236\u3002\u57fa\u4e8eCE-BEM\u6a21\u578b\u63a8\u5bfc\u4e86VOCDM\u7684\u8f93\u5165\u8f93\u51fa\u5173\u7cfb\uff0c\u5e76\u5206\u6790\u4e86M\u548cN\u53c2\u6570\u5bf9VOCDM\u7684\u6027\u80fd\u5f71\u54cd\u3002", "result": "VOCDM\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u591a\u6837\u6027\u6027\u80fd\uff0c\u4e14PAPPR\u968fN\u7684\u51cf\u5c0f\u800c\u51cf\u5c0f\u3002\u7406\u8bba\u53d1\u73b0\u901a\u8fc7\u6570\u503c\u6a21\u62df\u5f97\u5230\u9a8c\u8bc1\u3002", "conclusion": "VOCDM\u5728\u53cc\u9009\u62e9\u6027\u4fe1\u9053\u4e0b\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u5206\u96c6\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u8c03\u6574M\u548cN\u53c2\u6570\u6765\u4f18\u5316\u3002"}}
{"id": "2508.06754", "categories": ["cs.AI", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.06754", "abs": "https://arxiv.org/abs/2508.06754", "authors": ["Vanessa Figueiredo"], "title": "A Fuzzy Logic Prompting Framework for Large Language Models in Adaptive and Uncertain Tasks", "comment": null, "summary": "We introduce a modular prompting framework that supports safer and more\nadaptive use of large language models (LLMs) across dynamic, user-centered\ntasks. Grounded in human learning theory, particularly the Zone of Proximal\nDevelopment (ZPD), our method combines a natural language boundary prompt with\na control schema encoded with fuzzy scaffolding logic and adaptation rules.\nThis architecture enables LLMs to modulate behavior in response to user state\nwithout requiring fine-tuning or external orchestration. In a simulated\nintelligent tutoring setting, the framework improves scaffolding quality,\nadaptivity, and instructional alignment across multiple models, outperforming\nstandard prompting baselines. Evaluation is conducted using rubric-based LLM\ngraders at scale. While initially developed for education, the framework has\nshown promise in other interaction-heavy domains, such as procedural content\ngeneration for games. Designed for safe deployment, it provides a reusable\nmethodology for structuring interpretable, goal-aligned LLM behavior in\nuncertain or evolving contexts.", "AI": {"tldr": "\"\u63d0\u51fa\u4e00\u79cd\u6a21\u5757\u5316\u63d0\u793a\u6846\u67b6\uff0c\u7528\u4e8eLLM\u5728\u52a8\u6001\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u81ea\u9002\u5e94\u4f7f\u7528\uff0c\u501f\u9274\u4e86\u8fd1\u4fa7\u53d1\u5c55\u533a\u7406\u8bba\uff0c\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u8fb9\u754c\u63d0\u793a\u548c\u6a21\u7cca\u63a7\u5236\u6a21\u5f0f\uff0c\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b9e\u73b0\u81ea\u9002\u5e94\uff0c\u5e76\u5728\u6559\u80b2\u548c\u6e38\u620f\u7b49\u9886\u57df\u5c55\u73b0\u51fa\u6f5c\u529b\u3002\"", "motivation": "\"\u672c\u7814\u7a76\u65e8\u5728\u4e3a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u52a8\u6001\u3001\u4ee5\u7528\u6237\u4e3a\u4e2d\u5fc3\u7684\u4efb\u52a1\u4e2d\u63d0\u4f9b\u4e00\u79cd\u66f4\u5b89\u5168\u3001\u66f4\u5177\u9002\u5e94\u6027\u7684\u4f7f\u7528\u6846\u67b6\uff0c\u7279\u522b\u5173\u6ce8\u5728\u6559\u80b2\u7b49\u4ea4\u4e92\u6027\u5f3a\u7684\u9886\u57df\u3002\"", "method": "\"\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u57fa\u4e8e\u4eba\u7c7b\u5b66\u4e60\u7406\u8bba\uff08\u7279\u522b\u662f\u8fd1\u4fa7\u53d1\u5c55\u533a\uff09\u7684\u81ea\u7136\u8bed\u8a00\u8fb9\u754c\u63d0\u793a\uff0c\u4ee5\u53ca\u4e00\u5957\u5305\u542b\u6a21\u7cca\u811a\u624b\u67b6\u903b\u8f91\u548c\u9002\u5e94\u89c4\u5219\u7684\u63a7\u5236\u6a21\u5f0f\u3002\"", "result": "\"\u5728\u6a21\u62df\u7684\u667a\u80fd\u8f85\u5bfc\u73af\u5883\u4e2d\uff0c\u8be5\u6846\u67b6\u63d0\u9ad8\u4e86\u811a\u624b\u67b6\u8d28\u91cf\u3001\u81ea\u9002\u5e94\u6027\u548c\u6559\u5b66\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u5728\u591a\u79cd\u6a21\u578b\u4e0a\u5747\u4f18\u4e8e\u6807\u51c6\u7684\u63d0\u793a\u57fa\u7ebf\u3002\u8bc4\u4f30\u662f\u901a\u8fc7\u5927\u89c4\u6a21\u7684\u3001\u57fa\u4e8e\u8bc4\u5206\u6807\u51c6\u7684LLM\u8bc4\u4f30\u8fdb\u884c\u7684\u3002\"", "conclusion": "\"\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u81ea\u7136\u8bed\u8a00\u8fb9\u754c\u63d0\u793a\u3001\u6a21\u7cca\u811a\u624b\u67b6\u903b\u8f91\u548c\u9002\u5e94\u89c4\u5219\uff0c\u5b9e\u73b0\u4e86\u8bed\u8a00\u6a21\u578b\u5728\u7528\u6237\u4e2d\u5fc3\u4efb\u52a1\u4e2d\u7684\u5b89\u5168\u548c\u81ea\u9002\u5e94\u4f7f\u7528\u3002\u5b83\u5728\u6a21\u62df\u7684\u667a\u80fd\u8f85\u5bfc\u73af\u5883\u4e2d\uff0c\u63d0\u9ad8\u4e86\u811a\u624b\u67b6\u8d28\u91cf\u3001\u81ea\u9002\u5e94\u6027\u548c\u6559\u5b66\u4e00\u81f4\u6027\uff0c\u4f18\u4e8e\u6807\u51c6\u63d0\u793a\u57fa\u7ebf\u3002\u8be5\u6846\u67b6\u65e8\u5728\u5b89\u5168\u90e8\u7f72\uff0c\u4e3a\u5728\u4e0d\u786e\u5b9a\u7684\u6216\u4e0d\u65ad\u53d1\u5c55\u7684\u73af\u5883\u4e2d\u6784\u5efa\u53ef\u89e3\u91ca\u7684\u3001\u4ee5\u76ee\u6807\u4e3a\u5bfc\u5411\u7684\u8bed\u8a00\u6a21\u578b\u884c\u4e3a\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u91cd\u7528\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u6e38\u620f\u7b49\u9886\u57df\u663e\u793a\u51fa\u5e94\u7528\u524d\u666f.\""}}
{"id": "2508.06627", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06627", "abs": "https://arxiv.org/abs/2508.06627", "authors": ["Mosbah Aouad", "Anirudh Choudhary", "Awais Farooq", "Steven Nevers", "Lusine Demirkhanyan", "Bhrandon Harris", "Suguna Pappu", "Christopher Gondi", "Ravishankar Iyer"], "title": "Early Detection of Pancreatic Cancer Using Multimodal Learning on Electronic Health Record", "comment": null, "summary": "Pancreatic ductal adenocarcinoma (PDAC) is one of the deadliest cancers, and\nearly detection remains a major clinical challenge due to the absence of\nspecific symptoms and reliable biomarkers. In this work, we propose a new\nmultimodal approach that integrates longitudinal diagnosis code histories and\nroutinely collected laboratory measurements from electronic health records to\ndetect PDAC up to one year prior to clinical diagnosis. Our method combines\nneural controlled differential equations to model irregular lab time series,\npretrained language models and recurrent networks to learn diagnosis code\ntrajectory representations, and cross-attention mechanisms to capture\ninteractions between the two modalities. We develop and evaluate our approach\non a real-world dataset of nearly 4,700 patients and achieve significant\nimprovements in AUC ranging from 6.5% to 15.5% over state-of-the-art methods.\nFurthermore, our model identifies diagnosis codes and laboratory panels\nassociated with elevated PDAC risk, including both established and new\nbiomarkers. Our code is available at\nhttps://github.com/MosbahAouad/EarlyPDAC-MML.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u5229\u7528\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u6570\u636e\uff0c\u5728\u4e34\u5e8a\u8bca\u65ad\u524d\u4e00\u5e74\u68c0\u6d4b\u80f0\u817a\u5bfc\u7ba1\u817a\u764c\uff0c\u5e76\u5728AUC\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u7531\u4e8e\u7f3a\u4e4f\u7279\u5f02\u6027\u75c7\u72b6\u548c\u53ef\u9760\u7684\u751f\u7269\u6807\u5fd7\u7269\uff0c\u80f0\u817a\u5bfc\u7ba1\u817a\u764c\uff08PDAC\uff09\u7684\u65e9\u671f\u68c0\u6d4b\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u7684\u4e34\u5e8a\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u3001\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u3001\u5faa\u73af\u7f51\u7edc\u548c\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u6a21\u62df\u4e0d\u89c4\u5219\u7684\u5b9e\u9a8c\u5ba4\u65f6\u95f4\u5e8f\u5217\uff0c\u5b66\u4e60\u8bca\u65ad\u4ee3\u7801\u8f68\u8ff9\u8868\u793a\uff0c\u5e76\u6355\u6349\u4e24\u79cd\u6a21\u6001\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u5728\u5305\u542b\u8fd14,700\u540d\u60a3\u8005\u7684\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\uff0c\u672c\u7814\u7a76\u7684\u65b9\u6cd5\u5728AUC\u65b9\u9762\u6bd4\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e866.5%\u81f315.5%\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u8bc6\u522b\u4e86\u4e0ePDAC\u98ce\u9669\u5347\u9ad8\u76f8\u5173\u7684\u8bca\u65ad\u4ee3\u7801\u548c\u5b9e\u9a8c\u5ba4\u68c0\u67e5\u9879\u76ee\uff0c\u5305\u62ec\u5df2\u5efa\u7acb\u548c\u65b0\u7684\u751f\u7269\u6807\u5fd7\u7269\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u7535\u5b50\u5065\u5eb7\u8bb0\u5f55\u4e2d\u7684\u7eb5\u5411\u8bca\u65ad\u4ee3\u7801\u5386\u53f2\u548c\u5e38\u89c4\u6536\u96c6\u7684\u5b9e\u9a8c\u5ba4\u6d4b\u91cf\u6570\u636e\uff0c\u53ef\u4ee5\u5728\u4e34\u5e8a\u8bca\u65ad\u524d\u4e00\u5e74\u68c0\u6d4b\u80f0\u817a\u5bfc\u7ba1\u817a\u764c\u3002"}}
{"id": "2508.07186", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07186", "abs": "https://arxiv.org/abs/2508.07186", "authors": ["Amit Dhanda"], "title": "Multi-Dimensional Summarization Agents with Context-Aware Reasoning over Enterprise Tables", "comment": null, "summary": "We propose a novel framework for summarizing structured enterprise data\nacross multiple dimensions using large language model (LLM)-based agents.\nTraditional table-to-text models often lack the capacity to reason across\nhierarchical structures and context-aware deltas, which are essential in\nbusiness reporting tasks. Our method introduces a multi-agent pipeline that\nextracts, analyzes, and summarizes multi-dimensional data using agents for\nslicing, variance detection, context construction, and LLM-based generation.\nOur results show that the proposed framework outperforms traditional\napproaches, achieving 83\\% faithfulness to underlying data, superior coverage\nof significant changes, and high relevance scores (4.4/5) for decision-critical\ninsights. The improvements are especially pronounced in categories involving\nsubtle trade-offs, such as increased revenue due to price changes amid\ndeclining unit volumes, which competing methods either overlook or address with\nlimited specificity. We evaluate the framework on Kaggle datasets and\ndemonstrate significant improvements in faithfulness, relevance, and insight\nquality over baseline table summarization approaches.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5229\u7528LLM\u667a\u80fd\u4f53\u6765\u603b\u7ed3\u591a\u7ef4\u4f01\u4e1a\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5206\u5c42\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u53d8\u5316\u65b9\u9762\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5fe0\u5b9e\u5ea6\u3001\u8986\u76d6\u8303\u56f4\u548c\u76f8\u5173\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\u3002", "motivation": "\u4f20\u7edf\u7684\u8868\u683c\u5230\u6587\u672c\u6a21\u578b\u5f80\u5f80\u7f3a\u4e4f\u8de8\u8d8a\u5206\u5c42\u7ed3\u6784\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u53d8\u5316\u7684\u63a8\u7406\u80fd\u529b\uff0c\u800c\u8fd9\u4e9b\u5bf9\u4e8e\u4e1a\u52a1\u62a5\u544a\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u667a\u80fd\u4f53\u6765\u603b\u7ed3\u4f01\u4e1a\u7ed3\u6784\u5316\u6570\u636e\u8de8\u8d8a\u591a\u4e2a\u7ef4\u5ea6\u3002\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6d41\u6c34\u7ebf\uff0c\u5229\u7528\u7528\u4e8e\u5207\u7247\u3001\u65b9\u5dee\u68c0\u6d4b\u3001\u4e0a\u4e0b\u6587\u6784\u5efa\u548c\u57fa\u4e8eLLM\u7684\u751f\u6210\u7684\u667a\u80fd\u4f53\u6765\u63d0\u53d6\u3001\u5206\u6790\u548c\u603b\u7ed3\u591a\u7ef4\u6570\u636e\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u5fe0\u5b9e\u5ea6\uff0883%\uff09\u3001\u5bf9\u663e\u8457\u53d8\u5316\u7684\u8986\u76d6\u8303\u56f4\u4ee5\u53ca\u51b3\u7b56\u5173\u952e\u89c1\u89e3\u7684\u76f8\u5173\u6027\u5f97\u5206\uff084.4/5\uff09\u65b9\u9762\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u5904\u7406\u6d89\u53ca\u7ec6\u5fae\u6743\u8861\u7684\u7c7b\u522b\u65f6\u8868\u73b0\u51fa\u5c24\u5176\u663e\u8457\u7684\u6539\u8fdb\uff0c\u4f8b\u5982\u4ef7\u683c\u53d8\u52a8\u5e26\u6765\u7684\u6536\u5165\u589e\u957f\u4e0e\u5355\u4f4d\u9500\u91cf\u4e0b\u964d\u5e76\u5b58\u7684\u60c5\u51b5\uff0c\u800c\u7ade\u4e89\u65b9\u6cd5\u5219\u5ffd\u7565\u4e86\u8fd9\u4e9b\u6216\u4ec5\u8fdb\u884c\u4e86\u6709\u9650\u7684\u9610\u8ff0\u3002"}}
{"id": "2508.06996", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06996", "abs": "https://arxiv.org/abs/2508.06996", "authors": ["M. Adeel Ajaib", "Fariha Nasir", "Abdul Rehman"], "title": "Explainable AI for Curie Temperature Prediction in Magnetic Materials", "comment": "6 pages, 5 figures", "summary": "We explore machine learning techniques for predicting Curie temperatures of\nmagnetic materials using the NEMAD database. By augmenting the dataset with\ncomposition-based and domain-aware descriptors, we evaluate the performance of\nseveral machine learning models. We find that the Extra Trees Regressor\ndelivers the best performance reaching an R^2 score of up to 0.85 $\\pm$ 0.01\n(cross-validated) for a balanced dataset. We employ the k-means clustering\nalgorithm to gain insights into the performance of chemically distinct material\ngroups. Furthermore, we perform the SHAP analysis to identify key\nphysicochemical drivers of Curie behavior, such as average atomic number and\nmagnetic moment. By employing explainable AI techniques, this analysis offers\ninsights into the model's predictive behavior, thereby advancing scientific\ninterpretability.", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u9884\u6d4b\u78c1\u6027\u6750\u6599\u5c45\u91cc\u6e29\u5ea6\uff0cExtra Trees Regressor\u6548\u679c\u6700\u4f73\uff0c\u5e76\u89e3\u91ca\u4e86\u5173\u952e\u5f71\u54cd\u56e0\u7d20", "motivation": "\u5229\u7528\u673a\u5668\u5b66\u4e60\u6280\u672f\u9884\u6d4b\u78c1\u6027\u6750\u6599\u7684\u5c45\u91cc\u6e29\u5ea6\uff0c\u5e76\u63d0\u4f9b\u6a21\u578b\u9884\u6d4b\u7684\u53ef\u89e3\u91ca\u6027", "method": "\u4f7f\u7528NEMAD\u6570\u636e\u5e93\uff0c\u7ed3\u5408\u57fa\u4e8e\u6210\u5206\u548c\u57df\u7684\u63cf\u8ff0\u7b26\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u673a\u5668\u5b66\u4e60\u6a21\u578b\uff08\u5305\u62ecExtra Trees Regressor\uff09\u7684\u6027\u80fd\uff0c\u5e76\u4f7f\u7528k-means\u805a\u7c7b\u5206\u6790\u5316\u5b66\u6210\u5206\u4e0d\u540c\u7684\u6750\u6599\u7ec4\uff0c\u6700\u540e\u901a\u8fc7SHAP\u5206\u6790\u6765\u89e3\u91ca\u6a21\u578b\u9884\u6d4b\u884c\u4e3a", "result": "Extra Trees Regressor\u8fbe\u5230\u4e860.85 \u00b1 0.01\u7684\u4ea4\u53c9\u9a8c\u8bc1R^2\u5206\u6570\uff0c\u540c\u65f6\u8bc6\u522b\u51fa\u5e73\u5747\u539f\u5b50\u5e8f\u6570\u548c\u78c1\u77e9\u662f\u5f71\u54cd\u5c45\u91cc\u6e29\u5ea6\u7684\u5173\u952e\u56e0\u7d20", "conclusion": "Extra Trees Regressor\u5728\u9884\u6d4b\u78c1\u6027\u6750\u6599\u7684\u5c45\u91cc\u6e29\u5ea6\u65b9\u9762\u8868\u73b0\u6700\u4f73\uff0cR^2\u5206\u6570\u6700\u9ad8\u53ef\u8fbe0.85 \u00b1 0.01\uff0c\u5e76\u901a\u8fc7SHAP\u5206\u6790\u786e\u5b9a\u4e86\u5e73\u5747\u539f\u5b50\u5e8f\u6570\u548c\u78c1\u77e9\u7b49\u5173\u952e\u7269\u7406\u5316\u5b66\u56e0\u7d20\u5f71\u54cd\u5c45\u91cc\u6e29\u5ea6"}}
{"id": "2508.07756", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07756", "abs": "https://arxiv.org/abs/2508.07756", "authors": ["Hanze Zhang", "Rong Chen", "Haibo Chen"], "title": "Towards Lock Modularization for Heterogeneous Environments", "comment": null, "summary": "Modern hardware environments are becoming increasingly heterogeneous, leading\nto the emergence of applications specifically designed to exploit this\nheterogeneity. Efficiently adopting locks in these applications poses distinct\nchallenges. The uneven distribution of resources in such environments can\ncreate bottlenecks for lock operations, severely hindering application\nperformance. Existing solutions are often tailored to specific types of\nhardware, which underutilizes resources on other components within\nheterogeneous environments.\n  This paper introduces a new design principle: decomposing locks across\nhardware components to fully utilize unevenly distributed resources in\nheterogeneous environments. Following this principle, we propose lock\nmodularization, a systematic approach that decomposes a lock into independent\nmodules and assigns them to appropriate hardware components. This approach\naligns the resource requirements of lock modules with the attributes of\nspecific hardware components, maximizing strengths while minimizing weaknesses.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u9501\u5206\u89e3\u5e76\u5206\u914d\u5230\u4e0d\u540c\u786c\u4ef6\u7ec4\u4ef6\uff0c\u63d0\u9ad8\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u786c\u4ef6\u73af\u5883\u65e5\u76ca\u5f02\u6784\u5316\uff0c\u73b0\u6709\u9501\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9488\u5bf9\u7279\u5b9a\u786c\u4ef6\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u5176\u4ed6\u7ec4\u4ef6\u7684\u8d44\u6e90\uff0c\u5bfc\u81f4\u6027\u80fd\u74f6\u9888\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9501\u6a21\u5757\u5316\u7684\u65b0\u8bbe\u8ba1\u539f\u5219\u548c\u7cfb\u7edf\u5316\u65b9\u6cd5\uff0c\u5c06\u9501\u5206\u89e3\u4e3a\u72ec\u7acb\u6a21\u5757\u5e76\u5206\u914d\u5230\u9002\u5f53\u7684\u786c\u4ef6\u7ec4\u4ef6\u3002", "result": "\u8be5\u65b9\u6cd5\u65e8\u5728\u6700\u5927\u5316\u5229\u7528\u786c\u4ef6\u4f18\u52bf\uff0c\u6700\u5c0f\u5316\u52a3\u52bf\uff0c\u63d0\u9ad8\u9501\u64cd\u4f5c\u5728\u5f02\u6784\u73af\u5883\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u9501\u6a21\u5757\u5316\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u9501\u5206\u89e3\u4e3a\u72ec\u7acb\u6a21\u5757\u5e76\u5206\u914d\u5230\u9002\u5f53\u7684\u786c\u4ef6\u7ec4\u4ef6\uff0c\u4ee5\u5145\u5206\u5229\u7528\u5f02\u6784\u73af\u5883\u4e2d\u7684\u4e0d\u5747\u8861\u8d44\u6e90\uff0c\u4ece\u800c\u89e3\u51b3\u73b0\u4ee3\u5f02\u6784\u786c\u4ef6\u73af\u5883\u4e2d\u9501\u64cd\u4f5c\u5e26\u6765\u7684\u6311\u6218\u3002"}}
{"id": "2508.06744", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.06744", "abs": "https://arxiv.org/abs/2508.06744", "authors": ["Yunke Ao", "Manish Prajapat", "Yarden As", "Yassine Taoudi-Benchekroun", "Fabio Carrillo", "Hooman Esfandiari", "Benjamin F. Grewe", "Andreas Krause", "Philipp F\u00fcrnstahl"], "title": "Robust-Sub-Gaussian Model Predictive Control for Safe Ultrasound-Image-Guided Robotic Spinal Surgery", "comment": null, "summary": "Safety-critical control using high-dimensional sensory feedback from optical\ndata (e.g., images, point clouds) poses significant challenges in domains like\nautonomous driving and robotic surgery. Control can rely on low-dimensional\nstates estimated from high-dimensional data. However, the estimation errors\noften follow complex, unknown distributions that standard probabilistic models\nfail to capture, making formal safety guarantees challenging. In this work, we\nintroduce a novel characterization of these general estimation errors using\nsub-Gaussian noise with bounded mean. We develop a new technique for\nuncertainty propagation of proposed noise characterization in linear systems,\nwhich combines robust set-based methods with the propagation of sub-Gaussian\nvariance proxies. We further develop a Model Predictive Control (MPC) framework\nthat provides closed-loop safety guarantees for linear systems under the\nproposed noise assumption. We apply this MPC approach in an\nultrasound-image-guided robotic spinal surgery pipeline, which contains\ndeep-learning-based semantic segmentation, image-based registration, high-level\noptimization-based planning, and low-level robotic control. To validate the\npipeline, we developed a realistic simulation environment integrating real\nhuman anatomy, robot dynamics, efficient ultrasound simulation, as well as\nin-vivo data of breathing motion and drilling force. Evaluation results in\nsimulation demonstrate the potential of our approach for solving complex\nimage-guided robotic surgery task while ensuring safety.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u566a\u58f0\u8868\u5f81\u548c\u63a7\u5236\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u673a\u5668\u4eba\u624b\u672f\u4e2d\u7684\u5b89\u5168\u95ee\u9898\uff0c\u5e76\u5728\u6a21\u62df\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u81ea\u52a8\u9a7e\u9a76\u548c\u673a\u5668\u4eba\u624b\u672f\u7b49\u9886\u57df\uff0c\u5229\u7528\u5149\u5b66\u6570\u636e\uff08\u5982\u56fe\u50cf\u3001\u70b9\u4e91\uff09\u8fdb\u884c\u5b89\u5168\u5173\u952e\u63a7\u5236\u65f6\uff0c\u9ad8\u7ef4\u4f20\u611f\u53cd\u9988\u5e26\u6765\u4e86\u4e25\u5cfb\u7684\u6311\u6218\u3002\u5c3d\u7ba1\u53ef\u4ee5\u901a\u8fc7\u4ece\u9ad8\u7ef4\u6570\u636e\u4f30\u8ba1\u7684\u4f4e\u7ef4\u72b6\u6001\u6765\u63a7\u5236\uff0c\u4f46\u4f30\u8ba1\u8bef\u5dee\u7684\u590d\u6742\u4e14\u672a\u77e5\u7684\u5206\u5e03\u5e38\u5e38\u4f7f\u5f97\u6807\u51c6\u7684\u6982\u7387\u6a21\u578b\u96be\u4ee5\u6355\u6349\uff0c\u4ece\u800c\u963b\u788d\u4e86\u6b63\u5f0f\u7684\u5b89\u5168\u4fdd\u8bc1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u5177\u6709\u754c\u5b9a\u5747\u503c\u7684\u4e9a\u9ad8\u65af\u566a\u58f0\u6765\u8868\u5f81\u4e00\u822c\u4f30\u8ba1\u8bef\u5dee\u7684\u65b0\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u57fa\u4e8e\u9c81\u68d2\u96c6\u5408\u7684\u65b9\u6cd5\u548c\u4e9a\u9ad8\u65af\u65b9\u5dee\u4ee3\u7406\u4f20\u64ad\uff0c\u4ee5\u89e3\u51b3\u7ebf\u6027\u7cfb\u7edf\u4e2d\u7684\u4e0d\u786e\u5b9a\u6027\u4f20\u64ad\u95ee\u9898\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\uff0c\u4e3a\u7ebf\u6027\u7cfb\u7edf\u5728\u6240\u63d0\u51fa\u7684\u566a\u58f0\u5047\u8bbe\u4e0b\u63d0\u4f9b\u95ed\u73af\u5b89\u5168\u4fdd\u8bc1\u3002", "result": "\u6240\u63d0\u51fa\u7684MPC\u65b9\u6cd5\u5e94\u7528\u4e8e\u8d85\u58f0\u56fe\u50cf\u5f15\u5bfc\u7684\u673a\u5668\u4eba\u810a\u67f1\u624b\u672f\u6d41\u7a0b\uff0c\u8be5\u6d41\u7a0b\u5305\u62ec\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u8bed\u4e49\u5206\u5272\u3001\u57fa\u4e8e\u56fe\u50cf\u7684\u914d\u51c6\u3001\u57fa\u4e8e\u4f18\u5316\u7684\u89c4\u5212\u4ee5\u53ca\u673a\u5668\u4eba\u63a7\u5236\u3002\u901a\u8fc7\u5728\u96c6\u6210\u4e86\u771f\u5b9e\u4eba\u4f53\u89e3\u5256\u7ed3\u6784\u3001\u673a\u5668\u4eba\u52a8\u529b\u5b66\u3001\u8d85\u58f0\u6a21\u62df\u4ee5\u53ca\u547c\u5438\u8fd0\u52a8\u548c\u94bb\u5b54\u529b\u7684\u771f\u5b9e\u6a21\u62df\u73af\u5883\u4e2d\u8fdb\u884c\u8bc4\u4f30\uff0c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u89e3\u51b3\u590d\u6742\u56fe\u50cf\u5f15\u5bfc\u624b\u672f\u4efb\u52a1\u65f6\u786e\u4fdd\u5b89\u5168\u6027\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u5c06\u6240\u63d0\u51fa\u7684\u65b0\u9896\u4e9a\u9ad8\u65af\u566a\u58f0\u5047\u8bbe\u5e94\u7528\u4e8e\u8d85\u58f0\u56fe\u50cf\u5f15\u5bfc\u673a\u5668\u4eba\u810a\u67f1\u624b\u672f\uff0c\u5e76\u5728\u6a21\u62df\u73af\u5883\u4e2d\u5c55\u793a\u4e86\u5176\u5728\u786e\u4fdd\u5b89\u5168\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06860", "categories": ["quant-ph", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.06860", "abs": "https://arxiv.org/abs/2508.06860", "authors": ["Zhuoyuan Lu", "Jiri Janousek", "Syed M. Assad", "Shuyao Qiu", "Mayank Joshi", "Yecheng Hu", "Alex Y Song", "Chuanyu Wang", "Manuka Suriyage", "Jie Zhao", "Ping Koy Lam", "Yuerui Lu"], "title": "Counter-propagating Entangled Photon Pairs from a Monolayer", "comment": "23 pages, 3 figures", "summary": "Non-phase-matched spontaneous parametric down-conversion (SPDC) in atomically\nthin materials provides new degrees of freedom and enhanced quantum information\ncapacity compared to conventional phase-matched sources. These systems emerged\nas promising platforms for quantum computing, communication, and imaging, with\nthe potential to support higher-order nonlinear processes. However, direct\nobservation of photon-pair emission from a monolayer has remained\nexperimentally challenging. In this work, we theoretically modeled SPDC\nemission across the full angular space from a monolayer GaSe film and\nexperimentally validated the model through measurements of both co- and\ncounter-propagating photon pairs. We demonstrated two-photon quantum\ncorrelations in the telecom C-band from the thinnest SPDC source reported to\ndate. The spatially symmetric, broadband emission predicted by theory was\nconfirmed experimentally. Furthermore, we observed high-fidelity Bell states in\nthe counter-propagating configuration, marking the first realization of\npolarization-entangled photon pairs from a monolayer. Our results revealed the\nemission characteristics of SPDC in the deeply subwavelength, non-phase-matched\nregime, and introduced atomically thin, counterpropagating SPDC as a scalable\nand integrable platform for programmable quantum state generation, extendable\nvia moir\\'e superlattice engineering.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u5728\u539f\u5b50\u7ea7\u8584\u7684\u5355\u5c42GaSe\u6750\u6599\u4e2d\u5b9e\u73b0\u4e86\u975e\u76f8\u4f4d\u5339\u914d\u7684\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\uff08SPDC\uff09\uff0c\u5e76\u89c2\u5bdf\u5230\u4e86\u53cc\u5149\u5b50\u91cf\u5b50\u5173\u8054\u548c\u9ad8\u4fdd\u771f\u5ea6\u8d1d\u5c14\u6001\uff0c\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u96c6\u6210\u7684\u53ef\u7f16\u7a0b\u91cf\u5b50\u6001\u751f\u6210\u5e73\u53f0\u63d0\u4f9b\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "motivation": "\u975e\u76f8\u4f4d\u5339\u914d\u7684\u81ea\u53d1\u53c2\u91cf\u4e0b\u8f6c\u6362\uff08SPDC\uff09\u5728\u539f\u5b50\u7ea7\u8584\u6750\u6599\u4e2d\u4e3a\u91cf\u5b50\u4fe1\u606f\u63d0\u4f9b\u4e86\u65b0\u7684\u81ea\u7531\u5ea6\u548c\u589e\u5f3a\u7684\u5bb9\u91cf\uff0c\u6709\u671b\u7528\u4e8e\u91cf\u5b50\u8ba1\u7b97\u3001\u901a\u4fe1\u548c\u6210\u50cf\uff0c\u4f46\u76f4\u63a5\u89c2\u6d4b\u5355\u5c42\u6750\u6599\u7684\u5149\u5b50\u5bf9\u53d1\u5c04\u4e00\u76f4\u5b58\u5728\u5b9e\u9a8c\u6311\u6218\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5efa\u6a21\u548c\u5b9e\u9a8c\u6d4b\u91cf\uff0c\u7814\u7a76\u4e86\u5355\u5c42GaSe\u8584\u819c\u4e2d\u7684SPDC\u53d1\u5c04\uff0c\u5e76\u9a8c\u8bc1\u4e86\u7406\u8bba\u6a21\u578b\uff0c\u540c\u65f6\u89c2\u5bdf\u4e86\u53cc\u5149\u5b50\u91cf\u5b50\u5173\u8054\u548c\u9ad8\u4fdd\u771f\u5ea6\u8d1d\u5c14\u6001\u3002", "result": "\u6211\u4eec\u6210\u529f\u5730\u5728\u539f\u5b50\u7ea7\u8584\u7684\u5355\u5c42GaSe\u8584\u819c\u4e2d\u5b9e\u73b0\u4e86\u7535\u4fe1C\u6ce2\u6bb5\u7684\u53cc\u5149\u5b50\u91cf\u5b50\u5173\u8054\uff0c\u8fd9\u662f\u8fc4\u4eca\u4e3a\u6b62\u62a5\u9053\u7684\u6700\u8584\u7684SPDC\u6e90\u3002\u6211\u4eec\u8fd8\u9996\u6b21\u5728\u5355\u5c42\u6750\u6599\u4e2d\u5b9e\u73b0\u4e86\u53cd\u5411\u4f20\u64ad\u914d\u7f6e\u4e0b\u7684\u9ad8\u4fdd\u771f\u5ea6\u8d1d\u5c14\u6001\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u53ef\u7f16\u7a0b\u91cf\u5b50\u6001\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63ed\u793a\u4e86\u5728\u6df1\u4e9a\u6ce2\u957f\u3001\u975e\u76f8\u4f4d\u5339\u914d\u7684\u533a\u57df\u4e2dSPDC\u7684\u53d1\u5c04\u7279\u6027\uff0c\u5e76\u5c06\u539f\u5b50\u7ea7\u8584\u7684\u3001\u53cd\u5411\u4f20\u64ad\u7684SPDC\u5f15\u5165\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u96c6\u6210\u7684\u53ef\u7f16\u7a0b\u91cf\u5b50\u6001\u751f\u6210\u5e73\u53f0\uff0c\u5e76\u4e14\u53ef\u4ee5\u901a\u8fc7\u83ab\u5c14\u8d85\u6676\u683c\u5de5\u7a0b\u8fdb\u884c\u6269\u5c55\u3002"}}
{"id": "2508.06709", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06709", "abs": "https://arxiv.org/abs/2508.06709", "authors": ["Evangelia Spiliopoulou", "Riccardo Fogliato", "Hanna Burnsky", "Tamer Soliman", "Jie Ma", "Graham Horwood", "Miguel Ballesteros"], "title": "Play Favorites: A Statistical Method to Measure Self-Bias in LLM-as-a-Judge", "comment": null, "summary": "Large language models (LLMs) can serve as judges that offer rapid and\nreliable assessments of other LLM outputs. However, models may systematically\nassign overly favorable ratings to their own outputs, a phenomenon known as\nself-bias, which can distort evaluations of true model performance. Previous\nstudies often conflate genuine differences in model quality with bias or\nincorrectly assume that evaluations from LLMs and humans follow the same rating\ndistributions. In this work, we present a statistical framework that explicitly\nformalizes assumptions under which self-bias can be identified and estimated.\nOur method models the difference in the scoring distribution that\nLLM-as-a-judge assigns to its own completions compared to other models, while\naccounting for the underlying quality of the completions provided by an\nindependent, third-party judge (e.g., humans). Our method reliably isolates and\nquantifies self-bias, even when models vary in ability, ensuring that genuine\nperformance differences are not mistaken for self-bias. We conduct an empirical\nanalysis of self-bias on a large dataset (>5000 prompt-completion pairs)\nconsisting of expert human annotations and judgments from nine different LLM\njudges. We find that some models, such as GPT-4o and Claude 3.5 Sonnet,\nsystematically assign higher scores to their own outputs. These models also\ndisplay family-bias; systematically assigning higher ratings to outputs\nproduced by other models of the same family. Our findings highlight potential\npitfalls of using LLM judges and offer practical guidance to mitigate biases\nwhen interpreting automated evaluations.", "AI": {"tldr": "LLM\u88c1\u5224\u53ef\u80fd\u5b58\u5728\u81ea\u504f\u5dee\u548c\u5bb6\u65cf\u504f\u5dee\uff0c\u9700\u8981\u65b0\u7684\u7edf\u8ba1\u65b9\u6cd5\u6765\u8bc6\u522b\u548c\u91cf\u5316\u8fd9\u4e9b\u504f\u5dee\uff0c\u4ee5\u83b7\u5f97\u66f4\u51c6\u786e\u7684\u6a21\u578b\u6027\u80fd\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3LLM\u4f5c\u4e3a\u88c1\u5224\u65f6\u53ef\u80fd\u5b58\u5728\u7684\u81ea\u504f\u5dee\u95ee\u9898\uff0c\u8fd9\u79cd\u504f\u5dee\u4f1a\u626d\u66f2\u5bf9\u6a21\u578b\u771f\u5b9e\u6027\u80fd\u7684\u8bc4\u4f30\u3002\u73b0\u6709\u7684\u7814\u7a76\u5e38\u5e38\u6df7\u6dc6\u6a21\u578b\u8d28\u91cf\u7684\u771f\u5b9e\u5dee\u5f02\u4e0e\u504f\u5dee\uff0c\u6216\u9519\u8bef\u5730\u5047\u8bbeLLM\u548c\u4eba\u7c7b\u7684\u8bc4\u5206\u5206\u5e03\u76f8\u540c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u8ba1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u660e\u786e\u5f62\u5f0f\u5316\u4e86\u8bc6\u522b\u548c\u4f30\u8ba1\u81ea\u504f\u5dee\u7684\u5047\u8bbe\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9LLM\u88c1\u5224\u7ed9\u51fa\u7684\u81ea\u8eab\u8f93\u51fa\u4e0e\u5176\u4ed6\u6a21\u578b\u8f93\u51fa\u7684\u8bc4\u5206\u5206\u5e03\u5dee\u5f02\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u8003\u8651\u4e86\u72ec\u7acb\u7684\u7b2c\u4e09\u65b9\u88c1\u5224\uff08\u5982\u4eba\u7c7b\uff09\u63d0\u4f9b\u7684\u8f93\u51fa\u7684\u6f5c\u5728\u8d28\u91cf\uff0c\u4ece\u800c\u5206\u79bb\u548c\u91cf\u5316\u81ea\u504f\u5dee\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u4e00\u4e9b\u6a21\u578b\uff08\u5982GPT-4o\u548cClaude 3.5 Sonnet\uff09\u5b58\u5728\u81ea\u504f\u5dee\u548c\u5bb6\u65cf\u504f\u5dee\uff08\u5373\u7cfb\u7edf\u6027\u5730\u7ed9\u540c\u5bb6\u65cf\u6a21\u578b\u7684\u8f93\u51fa\u5206\u914d\u66f4\u9ad8\u8bc4\u5206\uff09\u3002", "conclusion": "\u4e00\u4e9b\u6a21\u578b\uff08\u5982GPT-4o\u548cClaude 3.5 Sonnet\uff09\u4f1a\u7cfb\u7edf\u6027\u5730\u7ed9\u81ea\u5df1\u7684\u8f93\u81ea\u5206\u914d\u66f4\u9ad8\u7684\u5206\u6570\uff0c\u5e76\u4e14\u4e5f\u4f1a\u7ed9\u540c\u5bb6\u65cf\u5176\u4ed6\u6a21\u578b\u7684\u8f93\u51fa\u5206\u914d\u66f4\u9ad8\u7684\u5206\u6570\u3002\u8fd9\u8bf4\u660e\u5728\u4f7f\u7528LLM\u4f5c\u4e3a\u88c1\u5224\u65f6\u9700\u8981\u8b66\u60d5\u6f5c\u5728\u7684\u504f\u89c1\uff0c\u5e76\u4e3a\u89e3\u91ca\u81ea\u52a8\u5316\u8bc4\u4f30\u63d0\u4f9b\u5b9e\u7528\u7684\u7f13\u89e3\u504f\u89c1\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.07676", "categories": ["cs.LG", "cs.DC", "cs.GT"], "pdf": "https://arxiv.org/pdf/2508.07676", "abs": "https://arxiv.org/abs/2508.07676", "authors": ["Chenchen Lin", "Xuehe Wang"], "title": "Multi-Hop Privacy Propagation for Differentially Private Federated Learning in Social Networks", "comment": "Accepted by ECAI25", "summary": "Federated learning (FL) enables collaborative model training across\ndecentralized clients without sharing local data, thereby enhancing privacy and\nfacilitating collaboration among clients connected via social networks.\nHowever, these social connections introduce privacy externalities: a client's\nprivacy loss depends not only on its privacy protection strategy but also on\nthe privacy decisions of others, propagated through the network via multi-hop\ninteractions. In this work, we propose a socially-aware privacy-preserving FL\nmechanism that systematically quantifies indirect privacy leakage through a\nmulti-hop propagation model. We formulate the server-client interaction as a\ntwo-stage Stackelberg game, where the server, as the leader, optimizes\nincentive policies, and clients, as followers, strategically select their\nprivacy budgets, which determine their privacy-preserving levels by controlling\nthe magnitude of added noise. To mitigate information asymmetry in networked\nprivacy estimation, we introduce a mean-field estimator to approximate the\naverage external privacy risk. We theoretically prove the existence and\nconvergence of the fixed point of the mean-field estimator and derive\nclosed-form expressions for the Stackelberg Nash Equilibrium. Despite being\ndesigned from a client-centric incentive perspective, our mechanism achieves\napproximately-optimal social welfare, as revealed by Price of Anarchy (PoA)\nanalysis. Experiments on diverse datasets demonstrate that our approach\nsignificantly improves client utilities and reduces server costs while\nmaintaining model performance, outperforming both Social-Agnostic (SA)\nbaselines and methods that account for social externalities.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8054\u90a6\u5b66\u4e60\u673a\u5236\uff0c\u8003\u8651\u4e86\u793e\u4ea4\u7f51\u7edc\u4e2d\u7684\u9690\u79c1\u5916\u90e8\u6027\u3002\u8be5\u673a\u5236\u901a\u8fc7\u65af\u5766\u514b\u5c14\u4f2f\u683c\u535a\u5f08\u548c\u5747\u503c\u573a\u4f30\u8ba1\u5668\u6765\u4f18\u5316\u6fc0\u52b1\u548c\u9690\u79c1\u4fdd\u62a4\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u7528\u6237\u6548\u7528\u3001\u964d\u4f4e\u670d\u52a1\u5668\u6210\u672c\u548c\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u867d\u7136\u53ef\u4ee5\u5728\u4e0d\u5171\u4eab\u672c\u5730\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u534f\u4f5c\u6a21\u578b\u8bad\u7ec3\uff0c\u4f46\u793e\u4ea4\u8fde\u63a5\u4f1a\u5e26\u6765\u9690\u79c1\u5916\u90e8\u6027\uff0c\u5373\u4e00\u4e2a\u5ba2\u6237\u7aef\u7684\u9690\u79c1\u635f\u5931\u4e0d\u4ec5\u53d6\u51b3\u4e8e\u81ea\u8eab\u7684\u9690\u79c1\u4fdd\u62a4\u7b56\u7565\uff0c\u8fd8\u53d6\u51b3\u4e8e\u901a\u8fc7\u7f51\u7edc\u8fdb\u884c\u7684\u591a\u8df3\u4ea4\u4e92\u4f20\u64ad\u7684\u4ed6\u4eba\u9690\u79c1\u51b3\u7b56\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u95f4\u63a5\u9690\u79c1\u6cc4\u6f0f\u7684\u793e\u4f1a\u610f\u8bc6\u8054\u90a6\u5b66\u4e60\u673a\u5236\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u793e\u4f1a\u610f\u8bc6\u7684\u9690\u79c1\u4fdd\u62a4\u8054\u90a6\u5b66\u4e60\u673a\u5236\uff0c\u901a\u8fc7\u591a\u8df3\u4f20\u64ad\u6a21\u578b\u7cfb\u7edf\u5730\u91cf\u5316\u95f4\u63a5\u9690\u79c1\u6cc4\u6f0f\u3002\u5c06\u670d\u52a1\u5668-\u5ba2\u6237\u7aef\u4ea4\u4e92\u8bbe\u8ba1\u4e3a\u53cc\u9636\u6bb5\u65af\u5766\u514b\u5c14\u4f2f\u683c\u535a\u5f08\uff0c\u5176\u4e2d\u670d\u52a1\u5668\u4f5c\u4e3a\u9886\u5bfc\u8005\u4f18\u5316\u6fc0\u52b1\u7b56\u7565\uff0c\u5ba2\u6237\u7aef\u4f5c\u4e3a\u8ddf\u968f\u8005\u9009\u62e9\u5176\u9690\u79c1\u9884\u7b97\uff0c\u901a\u8fc7\u63a7\u5236\u6dfb\u52a0\u566a\u58f0\u7684\u5927\u5c0f\u6765\u786e\u5b9a\u5176\u9690\u79c1\u4fdd\u62a4\u7ea7\u522b\u3002\u4e3a\u4e86\u51cf\u8f7b\u7f51\u7edc\u9690\u79c1\u4f30\u8ba1\u4e2d\u7684\u4fe1\u606f\u4e0d\u5bf9\u79f0\uff0c\u5f15\u5165\u4e86\u5747\u503c\u573a\u4f30\u8ba1\u5668\u6765\u8fd1\u4f3c\u5e73\u5747\u5916\u90e8\u9690\u79c1\u98ce\u9669\u3002\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u5747\u503c\u573a\u4f30\u8ba1\u5668\u7684\u4e0d\u52a8\u70b9\u7684\u5b58\u5728\u6027\u548c\u6536\u655b\u6027\uff0c\u5e76\u63a8\u5bfc\u4e86\u65af\u5766\u514b\u5c14\u4f2f\u683c\u7eb3\u4ec0\u5747\u8861\u7684\u95ed\u5f0f\u8868\u8fbe\u5f0f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u5ba2\u6237\u7aef\u6548\u7528\u5e76\u964d\u4f4e\u4e86\u670d\u52a1\u5668\u6210\u672c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u4ec5\u8003\u8651\u793e\u4ea4\u7684\u57fa\u7ebf\uff08SA\uff09\u548c\u90a3\u4e9b\u8003\u8651\u793e\u4f1a\u5916\u90e8\u6027\u7684\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u673a\u5236\u5728\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5ba2\u6237\u7aef\u6548\u7528\u5e76\u964d\u4f4e\u4e86\u670d\u52a1\u5668\u6210\u672c\uff0c\u5e76\u4e14\u4f18\u4e8e\u8003\u8651\u793e\u4f1a\u5916\u90e8\u6027\u7684\u65b9\u6cd5\u4ee5\u53ca\u4ec5\u8003\u8651\u793e\u4ea4\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.06551", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06551", "abs": "https://arxiv.org/abs/2508.06551", "authors": ["Ye Tao"], "title": "Slice or the Whole Pie? Utility Control for AI Models", "comment": null, "summary": "Training deep neural networks (DNNs) has become an increasingly\nresource-intensive task, requiring large volumes of labeled data, substantial\ncomputational power, and considerable fine-tuning efforts to achieve optimal\nperformance across diverse use cases. Although pre-trained models offer a\nuseful starting point, adapting them to meet specific user needs often demands\nextensive customization, and infrastructure overhead. This challenge grows when\na single model must support diverse appli-cations with differing requirements\nfor performance. Traditional solutions often involve training multiple model\nversions to meet varying requirements, which can be inefficient and difficult\nto maintain. In order to overcome this challenge, we propose NNObfuscator, a\nnovel utility control mechanism that enables AI models to dynamically modify\ntheir performance according to predefined conditions. It is different from\ntraditional methods that need separate models for each user. Instead,\nNNObfuscator allows a single model to be adapted in real time, giving you\ncontrolled access to multiple levels of performance. This mechanism enables\nmodel owners set up tiered access, ensuring that free-tier users receive a\nbaseline level of performance while premium users benefit from enhanced\ncapabilities. The approach improves resource allocation, reduces unnecessary\ncomputation, and supports sustainable business models in AI deployment. To\nvalidate our approach, we conducted experiments on multiple tasks, including\nimage classification, semantic segmentation, and text to image generation,\nusing well-established models such as ResNet, DeepLab, VGG16, FCN and Stable\nDiffusion. Experimental results show that NNObfuscator successfully makes model\nmore adaptable, so that a single trained model can handle a broad range of\ntasks without requiring a lot of changes.", "AI": {"tldr": "NNObfuscator \u662f\u4e00\u79cd\u5141\u8bb8\u5355\u4e2a AI \u6a21\u578b\u6839\u636e\u9884\u5b9a\u4e49\u7684\u6761\u4ef6\u52a8\u6001\u4fee\u6539\u5176\u6027\u80fd\u7684\u673a\u5236\uff0c\u4ece\u800c\u4e3a\u514d\u8d39\u548c\u9ad8\u7ea7\u7528\u6237\u63d0\u4f9b\u4e0d\u540c\u7ea7\u522b\u7684\u6027\u80fd\uff0c\u4ee5\u514b\u670d\u8bad\u7ec3 DNN \u6240\u9700\u7684\u5927\u91cf\u8d44\u6e90\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc (DNN) \u6240\u9700\u7684\u5927\u91cf\u8d44\u6e90\u3001\u5927\u91cf\u6807\u8bb0\u6570\u636e\u3001\u5927\u91cf\u8ba1\u7b97\u80fd\u529b\u548c\u5927\u91cf\u5fae\u8c03\u5de5\u4f5c\u7684\u6311\u6218\uff0c\u5373\u4f7f\u5728\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u65f6\u4e5f\u662f\u5982\u6b64\u3002", "method": "NNObfuscator \u662f\u4e00\u79cd\u65b0\u9896\u7684\u6548\u7528\u63a7\u5236\u673a\u5236\uff0c\u5b83\u4f7f AI \u6a21\u578b\u80fd\u591f\u6839\u636e\u9884\u5b9a\u4e49\u7684\u6761\u4ef6\u52a8\u6001\u4fee\u6539\u5176\u6027\u80fd\u3002", "result": "NNObfuscator \u5141\u8bb8\u5b9e\u65f6\u8c03\u6574\u5355\u4e2a\u6a21\u578b\uff0c\u63d0\u4f9b\u5bf9\u591a\u4e2a\u6027\u80fd\u7ea7\u522b\u7684\u53d7\u63a7\u8bbf\u95ee\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8d44\u6e90\u5206\u914d\uff0c\u51cf\u5c11\u4e86\u4e0d\u5fc5\u8981\u7684\u8ba1\u7b97\uff0c\u5e76\u652f\u6301 AI \u90e8\u7f72\u4e2d\u7684\u53ef\u6301\u7eed\u4e1a\u52a1\u6a21\u578b\u3002", "conclusion": "NNObfuscator \u6210\u529f\u5730\u4f7f\u6a21\u578b\u66f4\u5177\u9002\u5e94\u6027\uff0c\u4f7f\u5355\u4e2a\u8bad\u7ec3\u6a21\u578b\u80fd\u591f\u5904\u7406\u5e7f\u6cdb\u7684\u4efb\u52a1\uff0c\u800c\u65e0\u9700\u8fdb\u884c\u5927\u91cf\u66f4\u6539\u3002"}}
{"id": "2508.07453", "categories": ["eess.SY", "cs.AI", "cs.MA", "cs.RO", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07453", "abs": "https://arxiv.org/abs/2508.07453", "authors": ["Vindula Jayawardana", "Catherine Tang", "Junyi Ji", "Jonah Philion", "Xue Bin Peng", "Cathy Wu"], "title": "Noise-Aware Generative Microscopic Traffic Simulation", "comment": null, "summary": "Accurately modeling individual vehicle behavior in microscopic traffic\nsimulation remains a key challenge in intelligent transportation systems, as it\nrequires vehicles to realistically generate and respond to complex traffic\nphenomena such as phantom traffic jams. While traditional human driver\nsimulation models offer computational tractability, they do so by abstracting\naway the very complexity that defines human driving. On the other hand, recent\nadvances in infrastructure-mounted camera-based roadway sensing have enabled\nthe extraction of vehicle trajectory data, presenting an opportunity to shift\ntoward generative, agent-based models. Yet, a major bottleneck remains: most\nexisting datasets are either overly sanitized or lack standardization, failing\nto reflect the noisy, imperfect nature of real-world sensing. Unlike data from\nvehicle-mounted sensors-which can mitigate sensing artifacts like occlusion\nthrough overlapping fields of view and sensor fusion-infrastructure-based\nsensors surface a messier, more practical view of challenges that traffic\nengineers encounter. To this end, we present the I-24 MOTION Scenario Dataset\n(I24-MSD)-a standardized, curated dataset designed to preserve a realistic\nlevel of sensor imperfection, embracing these errors as part of the learning\nproblem rather than an obstacle to overcome purely from preprocessing. Drawing\nfrom noise-aware learning strategies in computer vision, we further adapt\nexisting generative models in the autonomous driving community for I24-MSD with\nnoise-aware loss functions. Our results show that such models not only\noutperform traditional baselines in realism but also benefit from explicitly\nengaging with, rather than suppressing, data imperfection. We view I24-MSD as a\nstepping stone toward a new generation of microscopic traffic simulation that\nembraces the real-world challenges and is better aligned with practical needs.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u771f\u5b9e\u4f20\u611f\u5668\u4e0d\u5b8c\u7f8e\u6027\u7684\u4ea4\u901a\u6570\u636e\u96c6\uff08I24-MSD\uff09\uff0c\u5e76\u5f00\u53d1\u80fd\u5229\u7528\u8fd9\u4e9b\u4e0d\u5b8c\u7f8e\u6027\u7684\u751f\u6210\u6a21\u578b\uff0c\u6210\u529f\u63d0\u5347\u4e86\u4ea4\u901a\u4eff\u771f\u7684\u771f\u5b9e\u611f\uff0c\u4e3a\u672a\u6765\u66f4\u8d34\u8fd1\u5b9e\u9645\u9700\u6c42\u7684\u4ea4\u901a\u4eff\u771f\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u4ea4\u901a\u4eff\u771f\u6a21\u578b\u5728\u6a21\u62df\u771f\u5b9e\u4e2a\u4f53\u8f66\u8f86\u884c\u4e3a\uff08\u7279\u522b\u662f\u5e7b\u5f71\u4ea4\u901a\u62e5\u5835\uff09\u65f6\u9047\u5230\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u8fc7\u4e8e\u7406\u60f3\u5316\u6216\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u7f3a\u70b9\uff0c\u672c\u7814\u7a76\u65e8\u5728\u6784\u5efa\u4e00\u4e2a\u80fd\u591f\u771f\u5b9e\u53cd\u6620\u4f20\u611f\u5668\u4e0d\u5b8c\u7f8e\u6027\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u80fd\u591f\u6709\u6548\u5229\u7528\u8fd9\u4e9b\u4e0d\u5b8c\u7f8e\u6570\u636e\u7684\u4eff\u771f\u6a21\u578b\u3002", "method": "\u63d0\u51faI-24 MOTION Scenario Dataset (I24-MSD)\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u501f\u9274\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u7684\u566a\u58f0\u611f\u77e5\u5b66\u4e60\u7b56\u7565\uff0c\u5bf9\u73b0\u6709\u81ea\u52a8\u9a7e\u9a76\u9886\u57df\u7684\u751f\u6210\u6a21\u578b\u8fdb\u884c\u4e86\u6539\u8fdb\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u5e76\u5229\u7528\u6570\u636e\u4e2d\u7684\u4e0d\u5b8c\u7f8e\u6027\u3002", "result": "\u57fa\u4e8e\u566a\u58f0\u611f\u77e5\u5b66\u4e60\u7b56\u7565\u7684\u751f\u6210\u6a21\u578b\u5728I24-MSD\u6570\u636e\u96c6\u4e0a\u7684\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u57fa\u7ebf\u6a21\u578b\uff0c\u4e0d\u4ec5\u771f\u5b9e\u611f\u66f4\u5f3a\uff0c\u800c\u4e14\u901a\u8fc7\u660e\u786e\u5730\u5229\u7528\u6570\u636e\u4e0d\u5b8c\u7f8e\u6027\u800c\u975e\u6d88\u9664\u5b83\uff0c\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684I-24 MOTION Scenario Dataset (I24-MSD)\u6570\u636e\u96c6\u4ee5\u53ca\u57fa\u4e8e\u566a\u58f0\u611f\u77e5\u5b66\u4e60\u7b56\u7565\u7684\u751f\u6210\u6a21\u578b\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u4f20\u611f\u5668\u611f\u77e5\u7684\u4e0d\u5b8c\u7f8e\u6027\uff0c\u5e76\u5728\u4ea4\u901a\u4eff\u771f\u4e2d\u53d6\u5f97\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u4f18\u8d8a\u7684\u771f\u5b9e\u611f\u8868\u73b0\u3002"}}
{"id": "2508.07412", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07412", "abs": "https://arxiv.org/abs/2508.07412", "authors": ["Dipankar Jana", "Diana Vaclavkova", "Rajesh Kumar Ulaganathan", "Raman Sankar", "Milan Orlita", "Clement Faugeras", "Maciej Koperski", "M. E. Zhitomirsky", "Marek Potemski"], "title": "Strong and selective magnon-phonon coupling in van der Waals antiferromagnet CoPS$_3$", "comment": "9 pages, 6 figures, and supplemental material", "summary": "The Raman scattering response of the biaxial antiferromagnet CoPS$_3$ has\nbeen investigated as a function of both magnetic field and temperature. The\npeaks observed in the low-frequency spectral range (90--200~cm$^{-1}$) have\nbeen identified as hybrid magnon--phonon excitations. The energies of the bare\nmagnon and phonon modes, as well as the effective coupling strengths between\ndifferent excitation pairs, have been determined. The strong and selective\nmagnon--phonon interaction largely accounts for the pronounced splitting of two\nphonon-like modes observed at 152~cm$^{-1}$ and 158~cm$^{-1}$ in the\nantiferromagnetic phase of CoPS$_3$. Based on the identification of bare magnon\nexcitations and their magnetic-field dependence, we propose an updated set of\nparameters for the effective exchange ($J_{\\mathrm{eff}} = 9.9$~meV) and\nbiaxial magnetic anisotropy ($D = 4.3$~meV and $E = -0.7$~meV) and advocate for\nan apparent anisotropic $g$-factor ($g_x = g_y = 2$, $g_z = 4$) in the CoPS$_3$\nantiferromagnet.", "AI": {"tldr": "CoPS3\u4e2d\u7684\u6df7\u5408\u78c1\u6fc0\u53d1-\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u5bfc\u81f4\u4e86\u4e24\u4e2a\u58f0\u5b50\u6a21\u5f0f\u5728152 cm^-1\u548c158 cm^-1\u5904\u7684\u5206\u88c2\u3002", "motivation": "\u7814\u7a76CoPS3\u7684\u62c9\u66fc\u6563\u5c04\u54cd\u5e94\uff0c\u4ee5\u8bc6\u522b\u6df7\u5408\u78c1\u6fc0\u53d1-\u58f0\u5b50\u6fc0\u53d1\uff0c\u5e76\u786e\u5b9a\u5176\u53c2\u6570\u3002", "method": "\u901a\u8fc7\u7814\u7a76\u78c1\u573a\u548c\u6e29\u5ea6\u4e0b\u7684\u62c9\u66fc\u6563\u5c04\u54cd\u5e94\u6765\u8bc6\u522b\u6df7\u5408\u6fc0\u5b50-\u58f0\u5b50\u6fc0\u53d1\uff0c\u5e76\u786e\u5b9a\u5176\u80fd\u91cf\u548c\u8026\u5408\u5f3a\u5ea6\u3002", "result": "\u8bc6\u522b\u4e86\u4f4e\u9891\u5149\u8c31\uff0890-200 cm^-1\uff09\u7684\u5cf0\u503c\u4e3a\u6df7\u5408\u78c1\u6fc0\u53d1-\u58f0\u5b50\u6fc0\u53d1\uff0c\u5e76\u786e\u5b9a\u4e86\u53c2\u6570\u3002", "conclusion": "\u57fa\u4e8e\u5bf9\u88f8\u9732\u7684\u78c1\u6fc0\u53d1\u53ca\u5176\u78c1\u573a\u4f9d\u8d56\u6027\u7684\u8bc6\u522b\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u66f4\u65b0\u7684\u6709\u6548\u4ea4\u6362\uff08J_eff = 9.9 meV\uff09\u548c\u53cc\u8f74\u78c1\u5404\u5411\u5f02\u6027\uff08D = 4.3 meV \u548c E = -0.7 meV\uff09\u53c2\u6570\u96c6\uff0c\u5e76\u5021\u5bfc\u4f7f\u7528\u660e\u663e\u5404\u5411\u5f02\u6027\u7684g\u56e0\u5b50\uff08Gx = Gy = 2\uff0cGz = 4\uff09\u5728CoPS3\u53cd\u94c1\u78c1\u4f53\u4e2d\u3002"}}
{"id": "2508.06823", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06823", "abs": "https://arxiv.org/abs/2508.06823", "authors": ["Xuan Zhao", "Jun Tao"], "title": "Natural Language-Driven Viewpoint Navigation for Volume Exploration via Semantic Block Representation", "comment": "Accepted by IEEE VIS 2025", "summary": "Exploring volumetric data is crucial for interpreting scientific datasets.\nHowever, selecting optimal viewpoints for effective navigation can be\nchallenging, particularly for users without extensive domain expertise or\nfamiliarity with 3D navigation. In this paper, we propose a novel framework\nthat leverages natural language interaction to enhance volumetric data\nexploration. Our approach encodes volumetric blocks to capture and\ndifferentiate underlying structures. It further incorporates a CLIP Score\nmechanism, which provides semantic information to the blocks to guide\nnavigation. The navigation is empowered by a reinforcement learning framework\nthat leverage these semantic cues to efficiently search for and identify\ndesired viewpoints that align with the user's intent. The selected viewpoints\nare evaluated using CLIP Score to ensure that they best reflect the user\nqueries. By automating viewpoint selection, our method improves the efficiency\nof volumetric data navigation and enhances the interpretability of complex\nscientific phenomena.", "AI": {"tldr": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u81ea\u52a8\u5316\u4f53\u79ef\u6570\u636e\u63a2\u7d22\u4e2d\u7684\u89c6\u70b9\u9009\u62e9\uff0c\u63d0\u9ad8\u5bfc\u822a\u6548\u7387\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u63a2\u7d22\u4f53\u79ef\u6570\u636e\u5bf9\u4e8e\u89e3\u91ca\u79d1\u5b66\u6570\u636e\u96c6\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u4e3a\u6709\u6548\u7684\u5bfc\u822a\u9009\u62e9\u6700\u4f73\u89c6\u70b9\u53ef\u80fd\u5177\u6709\u6311\u6218\u6027\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u6ca1\u6709\u5e7f\u6cdb\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u6216\u719f\u60893D\u5bfc\u822a\u7684\u7528\u6237\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4ea4\u4e92\u6765\u589e\u5f3a\u4f53\u79ef\u6570\u636e\u63a2\u7d22\u7684\u65b0\u6846\u67b6\u3002\u8be5\u65b9\u6cd5\u5c06\u4f53\u79ef\u5757\u8fdb\u884c\u7f16\u7801\u4ee5\u6355\u6349\u548c\u533a\u5206\u5e95\u5c42\u7ed3\u6784\uff0c\u5e76\u7ed3\u5408\u4e86\u63d0\u4f9b\u8bed\u4e49\u4fe1\u606f\u7ed9\u5757\u4ee5\u6307\u5bfc\u5bfc\u822a\u7684CLIP Score\u673a\u5236\u3002\u5bfc\u822a\u7531\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u652f\u6301\uff0c\u8be5\u6846\u67b6\u5229\u7528\u8fd9\u4e9b\u8bed\u4e49\u7ebf\u7d22\u6765\u6709\u6548\u5730\u641c\u7d22\u548c\u8bc6\u522b\u4e0e\u7528\u6237\u610f\u56fe\u4e00\u81f4\u7684\u671f\u671b\u89c6\u70b9\u3002\u6240\u9009\u89c6\u70b9\u901a\u8fc7CLIP Score\u8fdb\u884c\u8bc4\u4f30\uff0c\u4ee5\u786e\u4fdd\u5b83\u4eec\u6700\u80fd\u53cd\u6620\u7528\u6237\u7684\u67e5\u8be2\u3002", "result": "\u901a\u8fc7\u81ea\u52a8\u5316\u89c6\u70b9\u9009\u62e9\uff0c\u63d0\u9ad8\u4e86\u4f53\u79ef\u6570\u636e\u5bfc\u822a\u7684\u6548\u7387\uff0c\u5e76\u589e\u5f3a\u4e86\u590d\u6742\u79d1\u5b66\u73b0\u8c61\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u81ea\u52a8\u5316\u89c6\u70b9\u9009\u62e9\uff0c\u63d0\u9ad8\u4e86\u4f53\u79ef\u6570\u636e\u5bfc\u822a\u7684\u6548\u7387\uff0c\u5e76\u589e\u5f3a\u4e86\u590d\u6742\u79d1\u5b66\u73b0\u8c61\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.06857", "categories": ["cs.CV", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.06857", "abs": "https://arxiv.org/abs/2508.06857", "authors": ["Mengxue Jia", "Zhihua Allen-Zhao", "You Zhao", "Sanyang Liu"], "title": "A Joint Sparse Self-Representation Learning Method for Multiview Clustering", "comment": null, "summary": "Multiview clustering (MC) aims to group samples using consistent and\ncomplementary information across various views. The subspace clustering, as a\nfundamental technique of MC, has attracted significant attention. In this\npaper, we propose a novel joint sparse self-representation learning model for\nMC, where a featured difference is the extraction of view-specific local\ninformation by introducing cardinality (i.e., $\\ell_0$-norm) constraints\ninstead of Graph-Laplacian regularization. Specifically, under each view,\ncardinality constraints directly restrict the samples used in the\nself-representation stage to extract reliable local and global structure\ninformation, while the low-rank constraint aids in revealing a global coherent\nstructure in the consensus affinity matrix during merging. The attendant\nchallenge is that Augmented Lagrange Method (ALM)-based alternating\nminimization algorithms cannot guarantee convergence when applied directly to\nour nonconvex, nonsmooth model, thus resulting in poor generalization ability.\nTo address it, we develop an alternating quadratic penalty (AQP) method with\nglobal convergence, where two subproblems are iteratively solved by closed-form\nsolutions. Empirical results on six standard datasets demonstrate the\nsuperiority of our model and AQP method, compared to eight state-of-the-art\nalgorithms.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u591a\u89c6\u56fe\u805a\u7c7b\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u6570\u7ea6\u675f\u63d0\u53d6\u5c40\u90e8\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u4f4e\u79e9\u7ea6\u675f\u63ed\u793a\u5168\u5c40\u7ed3\u6784\u3002\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u5168\u5c40\u6536\u655b\u6027\u7684\u4ea4\u66ff\u4e8c\u6b21\u60e9\u7f5a\uff08AQP\uff09\u65b9\u6cd5\u6765\u89e3\u51b3\u6a21\u578b\u6536\u655b\u6027\u95ee\u9898\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u548c\u65b9\u6cd5\u7684\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u89c6\u56fe\u805a\u7c7b\uff08MC\uff09\u4e2d\u5b50\u7a7a\u95f4\u805a\u7c7b\u6a21\u578b\u5728\u5904\u7406\u975e\u51f8\u3001\u975e\u5149\u6ed1\u6a21\u578b\u65f6\uff0c\u57fa\u4e8e\u589e\u5e7f\u62c9\u683c\u6717\u65e5\u65b9\u6cd5\uff08ALM\uff09\u7684\u4ea4\u66ff\u6700\u5c0f\u5316\u7b97\u6cd5\u65e0\u6cd5\u4fdd\u8bc1\u6536\u655b\u6027\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u5408\u7a00\u758f\u81ea\u8868\u793a\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u57fa\u6570\uff08$\text{l}_0$-\u8303\u6570\uff09\u7ea6\u675f\u6765\u63d0\u53d6\u89c6\u56fe\u7279\u5b9a\u7684\u5c40\u90e8\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u4f4e\u79e9\u7ea6\u675f\u6765\u63ed\u793a\u5171\u8bc6\u4eb2\u548c\u77e9\u9635\u4e2d\u7684\u5168\u5c40\u76f8\u5e72\u7ed3\u6784\u3002\u4e3a\u89e3\u51b3\u975e\u51f8\u3001\u975e\u5149\u6ed1\u6a21\u578b\u5f15\u8d77\u7684\u6536\u655b\u6027\u95ee\u9898\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u5177\u6709\u5168\u5c40\u6536\u655b\u6027\u7684\u4ea4\u66ff\u4e8c\u6b21\u60e9\u7f5a\uff08AQP\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u95ed\u5f0f\u89e3\u8fed\u4ee3\u6c42\u89e3\u4e24\u4e2a\u5b50\u95ee\u9898\u3002", "result": "\u5728\u516d\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u548cAQP\u65b9\u6cd5\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u516b\u79cd\u6700\u5148\u8fdb\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u57fa\u6570\u7ea6\u675f\u6765\u63d0\u53d6\u89c6\u56fe\u7279\u5b9a\u7684\u5c40\u90e8\u4fe1\u606f\uff0c\u5e76\u4f7f\u7528\u4f4e\u79e9\u7ea6\u675f\u6765\u63ed\u793a\u5171\u8bc6\u4eb2\u548c\u77e9\u9635\u4e2d\u7684\u5168\u5c40\u76f8\u5e72\u7ed3\u6784\u3002\u63d0\u51fa\u7684AQP\u65b9\u6cd5\u5177\u6709\u5168\u5c40\u6536\u655b\u6027\uff0c\u5e76\u4e14\u5b50\u95ee\u9898\u53ef\u4ee5\u901a\u8fc7\u95ed\u5f0f\u89e3\u6c42\u89e3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u548cAQP\u65b9\u6cd5\u4f18\u4e8e\u73b0\u6709\u7684\u516b\u79cd\u7b97\u6cd5\u3002"}}
{"id": "2508.06635", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06635", "abs": "https://arxiv.org/abs/2508.06635", "authors": ["Yewon Byun", "Shantanu Gupta", "Zachary C. Lipton", "Rachel Leah Childers", "Bryan Wilder"], "title": "Using Imperfect Synthetic Data in Downstream Inference Tasks", "comment": null, "summary": "Predictions and generations from large language models are increasingly being\nexplored as an aid to computational social science and human subject research\nin limited data regimes. While previous technical work has explored the\npotential to use model-predicted labels for unlabeled data in a principled\nmanner, there is increasing interest in using large language models to generate\nentirely new synthetic samples (also termed as synthetic simulations), such as\nin responses to surveys. However, it is not immediately clear by what means\npractitioners can combine such data with real data and yet produce\nstatistically valid conclusions upon them. In this work, we introduce a new\nestimator based on generalized method of moments, providing a\nhyperparameter-free solution with strong theoretical guarantees to address the\nchallenge at hand. Surprisingly, we find that interactions between the moment\nresiduals of synthetic data and those of real data can improve estimates of the\ntarget parameter. We empirically validate the finite-sample performance of our\nestimator across different regression tasks in computational social science\napplications, demonstrating large empirical gains.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8d85\u53c2\u6570\u7684 GMM \u4f30\u8ba1\u91cf\uff0c\u7528\u4e8e\u7ed3\u5408\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\uff0c\u5e76\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u4e2d\u5b9e\u73b0\u7edf\u8ba1\u4e0a\u6709\u6548\u7684\u7ed3\u8bba\uff0c\u540c\u65f6\u53d1\u73b0\u4e86\u6570\u636e\u4ea4\u4e92\u7684\u6f5c\u5728\u76ca\u5904\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u548c\u4eba\u7c7b\u53d7\u8bd5\u8005\u7814\u7a76\u4e2d\uff0c\u5982\u4f55\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u5e76\u4ece\u4e2d\u5f97\u51fa\u7edf\u8ba1\u6709\u6548\u7ed3\u8bba\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5e7f\u4e49\u77e9\u65b9\u6cd5\uff08GMM\uff09\u7684\u65b0\u578b\u4f30\u8ba1\u91cf\uff0c\u8be5\u4f30\u8ba1\u91cf\u65e0\u9700\u8d85\u53c2\u6570\u5373\u53ef\u89e3\u51b3\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u7ed3\u5408\u7684\u7edf\u8ba1\u6709\u6548\u6027\u95ee\u9898\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u4e4b\u95f4\u7684\u77e9\u6b8b\u5dee\u4ea4\u4e92\u53ef\u4ee5\u6539\u8fdb\u76ee\u6807\u53c2\u6570\u7684\u4f30\u8ba1\u3002\u5728\u4e0d\u540c\u7684\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u56de\u5f52\u4efb\u52a1\u4e2d\uff0c\u8be5\u4f30\u8ba1\u91cf\u5c55\u73b0\u51fa\u4e86\u663e\u8457\u7684\u7ecf\u9a8c\u4f18\u52bf\u3002", "conclusion": "\u4f7f\u7528\u65b0\u7684\u57fa\u4e8e GMM \u7684\u4f30\u8ba1\u91cf\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5c06\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u6570\u636e\u76f8\u7ed3\u5408\uff0c\u5e76\u5728\u8ba1\u7b97\u793e\u4f1a\u79d1\u5b66\u5e94\u7528\u4e2d\u5b9e\u73b0\u7edf\u8ba1\u4e0a\u6709\u6548\u7684\u7ed3\u8bba\u3002"}}
{"id": "2508.07221", "categories": ["cs.LG", "cs.AI", "cs.MA", "stat.AP", "stat.ME"], "pdf": "https://arxiv.org/pdf/2508.07221", "abs": "https://arxiv.org/abs/2508.07221", "authors": ["Po-Han Lee", "Yu-Cheng Lin", "Chan-Tung Ku", "Chan Hsu", "Pei-Cing Huang", "Ping-Hsun Wu", "Yihuang Kang"], "title": "LLM-based Agents for Automated Confounder Discovery and Subgroup Analysis in Causal Inference", "comment": null, "summary": "Estimating individualized treatment effects from observational data presents\na persistent challenge due to unmeasured confounding and structural bias.\nCausal Machine Learning (causal ML) methods, such as causal trees and doubly\nrobust estimators, provide tools for estimating conditional average treatment\neffects. These methods have limited effectiveness in complex real-world\nenvironments due to the presence of latent confounders or those described in\nunstructured formats. Moreover, reliance on domain experts for confounder\nidentification and rule interpretation introduces high annotation cost and\nscalability concerns. In this work, we proposed Large Language Model-based\nagents for automated confounder discovery and subgroup analysis that integrate\nagents into the causal ML pipeline to simulate domain expertise. Our framework\nsystematically performs subgroup identification and confounding structure\ndiscovery by leveraging the reasoning capabilities of LLM-based agents, which\nreduces human dependency while preserving interpretability. Experiments on\nreal-world medical datasets show that our proposed approach enhances treatment\neffect estimation robustness by narrowing confidence intervals and uncovering\nunrecognized confounding biases. Our findings suggest that LLM-based agents\noffer a promising path toward scalable, trustworthy, and semantically aware\ncausal inference.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6765\u81ea\u52a8\u53d1\u73b0\u6df7\u6dc6\u56e0\u7d20\u548c\u8fdb\u884c\u4e9a\u7ec4\u5206\u6790\uff0c\u4ee5\u89e3\u51b3\u56e0\u679c\u63a8\u65ad\u4e2d\u7684\u6311\u6218\uff0c\u5e76\u5728\u533b\u7597\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6539\u8fdb\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u7531\u4e8e\u672a\u6d4b\u91cf\u7684\u6df7\u6dc6\u56e0\u7d20\u548c\u7ed3\u6784\u504f\u5dee\uff0c\u4ece\u89c2\u5bdf\u6570\u636e\u4e2d\u4f30\u8ba1\u4e2a\u4f53\u6cbb\u7597\u6548\u679c\u4e00\u76f4\u662f\u4e00\u4e2a\u6311\u6218\u3002\u73b0\u6709\u7684\u56e0\u679cML\u65b9\u6cd5\u5728\u590d\u6742\u73af\u5883\u4e2d\u6548\u679c\u6709\u9650\uff0c\u5e76\u4e14\u4f9d\u8d56\u9886\u57df\u4e13\u5bb6\u4f1a\u5e26\u6765\u9ad8\u6602\u7684\u6ce8\u91ca\u6210\u672c\u548c\u53ef\u6269\u5c55\u6027\u95ee\u9898\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u57fa\u4e8eLLM\u7684\u4ee3\u7406\u6765\u81ea\u52a8\u53d1\u73b0\u6df7\u6dc6\u56e0\u7d20\u548c\u8fdb\u884c\u4e9a\u7ec4\u5206\u6790\uff0c\u5c06\u5176\u6574\u5408\u5230\u56e0\u679cML\u7ba1\u9053\u4e2d\uff0c\u4ee5\u6a21\u62df\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u7f29\u5c0f\u7f6e\u4fe1\u533a\u95f4\u548c\u63ed\u793a\u672a\u8bc6\u522b\u7684\u6df7\u6dc6\u504f\u5dee\uff0c\u63d0\u9ad8\u4e86\u6cbb\u7597\u6548\u679c\u4f30\u8ba1\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "LLM\u4ee3\u7406\u4e3a\u53ef\u6269\u5c55\u3001\u53ef\u4fe1\u8d56\u548c\u8bed\u4e49\u611f\u77e5\u7684\u56e0\u679c\u63a8\u65ad\u63d0\u4f9b\u4e86\u4e00\u6761\u6709\u524d\u9014\u7684\u9014\u5f84\u3002"}}
{"id": "2508.07035", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07035", "abs": "https://arxiv.org/abs/2508.07035", "authors": ["Jiaxuan Liu", "Tiannian Zhu", "Caiyuan Ye", "Zhong Fang", "Hongming Weng", "Quansheng Wu"], "title": "VASPilot: MCP-Facilitated Multi-Agent Intelligence for Autonomous VASP Simulations", "comment": "9 pages, 5 figures", "summary": "Density-functional-theory (DFT) simulations with the Vienna Ab initio\nSimulation Package (VASP) are indispensable in computational materials science\nbut often require extensive manual setup, monitoring, and postprocessing. Here,\nwe introduce VASPilot, an open-source platform that fully automates VASP\nworkflows via a multi-agent architecture built on the CrewAI framework and a\nstandardized Model Context Protocol (MCP). VASPilot's agent suite handles every\nstage of a VASP study-from retrieving crystal structures and generating input\nfiles to submitting Slurm jobs, parsing error messages, and dynamically\nadjusting parameters for seamless restarts. A lightweight Flask-based web\ninterface provides intuitive task submission, real-time progress tracking, and\ndrill-down access to execution logs, structure visualizations, and plots. We\nvalidate VASPilot on both routine and advanced benchmarks: automated\nband-structure and density-of-states calculations (including on-the-fly\nsymmetry corrections), plane-wave cutoff convergence tests, lattice-constant\noptimizations with various van der Waals corrections, and cross-material\nband-gap comparisons for transition-metal dichalcogenides. In all cases,\nVASPilot completed the missions reliably and without manual intervention.\nMoreover, its modular design allows easy extension to other DFT codes simply by\ndeploying the appropriate MCP server. By offloading technical overhead,\nVASPilot enables researchers to focus on scientific discovery and accelerates\nhigh-throughput computational materials research.", "AI": {"tldr": "VASPilot\u662f\u4e00\u4e2a\u81ea\u52a8\u5316VASP\u5de5\u4f5c\u6d41\u7684\u5f00\u6e90\u5e73\u53f0\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u67b6\u6784\u548cWeb\u754c\u9762\uff0c\u7b80\u5316\u4e86\u8ba1\u7b97\u6750\u6599\u7814\u7a76\u7684\u6d41\u7a0b\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "motivation": "\u8ba1\u7b97\u6750\u6599\u79d1\u5b66\u4e2d\u7684DFT\u6a21\u62df\uff08\u5982VASP\uff09\u901a\u5e38\u9700\u8981\u5927\u91cf\u7684\u624b\u52a8\u8bbe\u7f6e\u3001\u76d1\u63a7\u548c\u540e\u5904\u7406\uff0c\u8fd9\u963b\u788d\u4e86\u7814\u7a76\u6548\u7387\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u81ea\u52a8\u5316VASP\u5de5\u4f5c\u6d41\u7684\u5e73\u53f0\uff0c\u4ee5\u51cf\u8f7b\u7814\u7a76\u4eba\u5458\u7684\u6280\u672f\u8d1f\u62c5\uff0c\u4f7f\u5176\u80fd\u4e13\u6ce8\u4e8e\u79d1\u5b66\u53d1\u73b0\uff0c\u5e76\u52a0\u901f\u9ad8\u901a\u91cf\u8ba1\u7b97\u6750\u6599\u7814\u7a76\u3002", "method": "VASPilot\u91c7\u7528\u57fa\u4e8eCrewAI\u6846\u67b6\u7684\u591a\u4ee3\u7406\u67b6\u6784\u548c\u6807\u51c6\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\uff0c\u5b9e\u73b0\u5bf9VASP\u5de5\u4f5c\u6d41\u7684\u5168\u9762\u81ea\u52a8\u5316\uff0c\u5305\u62ec\u7ed3\u6784\u68c0\u7d22\u3001\u8f93\u5165\u6587\u4ef6\u751f\u6210\u3001\u4efb\u52a1\u63d0\u4ea4\u3001\u9519\u8bef\u89e3\u6790\u548c\u53c2\u6570\u8c03\u6574\u7b49\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u4e8eFlask\u7684Web\u754c\u9762\uff0c\u7528\u4e8e\u4efb\u52a1\u63d0\u4ea4\u3001\u8fdb\u5ea6\u8ddf\u8e2a\u548c\u7ed3\u679c\u53ef\u89c6\u5316\u3002", "result": "VASPilot\u5728\u81ea\u52a8\u5316\u7684\u80fd\u5e26\u7ed3\u6784\u548c\u6001\u5bc6\u5ea6\u8ba1\u7b97\uff08\u5305\u62ec\u5bf9\u79f0\u6027\u6821\u6b63\uff09\u3001\u5e73\u9762\u6ce2\u622a\u65ad\u6536\u655b\u6027\u6d4b\u8bd5\u3001\u4e0d\u540c\u8303\u5fb7\u534e\u6821\u6b63\u7684\u6676\u683c\u5e38\u6570\u4f18\u5316\u4ee5\u53ca\u8de8\u6750\u6599\u7684\u8fc7\u6e21\u91d1\u5c5e\u4e8c\u786b\u5c5e\u5316\u7269\u5e26\u9699\u6bd4\u8f83\u7b49\u5e38\u89c4\u548c\u9ad8\u7ea7\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u8868\u73b0\u51fa\u8272\uff0c\u6210\u529f\u5b8c\u6210\u4e86\u4efb\u52a1\u4e14\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002", "conclusion": "VASPilot\u901a\u8fc7\u591a\u4ee3\u7406\u67b6\u6784\u548c\u6807\u51c6\u5316\u7684\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\uff0c\u5b9e\u73b0\u4e86VASP\u5de5\u4f5c\u6d41\u7684\u5168\u81ea\u52a8\u5316\uff0c\u5e76\u63d0\u4f9b\u4e86\u8f7b\u91cf\u7ea7\u7684Web\u754c\u9762\uff0c\u80fd\u591f\u53ef\u9760\u5730\u5904\u7406\u4ece\u4efb\u52a1\u63d0\u4ea4\u5230\u53c2\u6570\u52a8\u6001\u8c03\u6574\u7684\u5404\u4e2a\u73af\u8282\uff0c\u65e0\u9700\u4eba\u5de5\u5e72\u9884\u3002\u8be5\u5e73\u53f0\u4e0d\u4ec5\u63d0\u9ad8\u4e86\u8ba1\u7b97\u6750\u6599\u7814\u7a76\u7684\u6548\u7387\uff0c\u8fd8\u901a\u8fc7\u6a21\u5757\u5316\u8bbe\u8ba1\u652f\u6301\u6269\u5c55\u5230\u5176\u4ed6DFT\u4ee3\u7801\uff0c\u4f7f\u7814\u7a76\u4eba\u5458\u80fd\u591f\u66f4\u4e13\u6ce8\u4e8e\u79d1\u5b66\u53d1\u73b0\u3002"}}
{"id": "2508.07934", "categories": ["cs.DC", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.07934", "abs": "https://arxiv.org/abs/2508.07934", "authors": ["Lorenzo La Corte", "Syed Aftab Rashid", "Andrei-Marian Dan"], "title": "Performance Evaluation of Brokerless Messaging Libraries", "comment": "11 pages, 9 figures", "summary": "Messaging systems are essential for efficiently transferring large volumes of\ndata, ensuring rapid response times and high-throughput communication. The\nstate-of-the-art on messaging systems mainly focuses on the performance\nevaluation of brokered messaging systems, which use an intermediate broker to\nguarantee reliability and quality of service. However, over the past decade,\nbrokerless messaging systems have emerged, eliminating the single point of\nfailure and trading off reliability guarantees for higher performance. Still,\nthe state-of-the-art on evaluating the performance of brokerless systems is\nscarce. In this work, we solely focus on brokerless messaging systems. First,\nwe perform a qualitative analysis of several possible candidates, to find the\nmost promising ones. We then design and implement an extensive open-source\nbenchmarking suite to systematically and fairly evaluate the performance of the\nchosen libraries, namely, ZeroMQ, NanoMsg, and NanoMsg-Next-Generation (NNG).\nWe evaluate these libraries considering different metrics and workload\nconditions, and provide useful insights into their limitations. Our analysis\nenables practitioners to select the most suitable library for their\nrequirements.", "AI": {"tldr": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5b9a\u6027\u5206\u6790\u548c\u5e7f\u6cdb\u7684\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc4\u4f30\u4e86 ZeroMQ\u3001NanoMsg \u548c NNG \u7b49\u65e0\u4ee3\u7406\u6d88\u606f\u4f20\u9012\u7cfb\u7edf\u7684\u6027\u80fd\uff0c\u4e3a\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u9009\u62e9\u5408\u9002\u5e93\u7684\u89c1\u89e3\u3002", "motivation": "\u4e0e\u4e3b\u8981\u5173\u6ce8\u5177\u6709\u4e2d\u95f4\u4ee3\u7406\u4ee5\u4fdd\u8bc1\u53ef\u9760\u6027\u548c\u670d\u52a1\u8d28\u91cf\u7684\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\u4e0d\u540c\uff0c\u8fd9\u9879\u5de5\u4f5c\u89e3\u51b3\u4e86\u65b0\u5174\u7684\u3001\u6d88\u9664\u5355\u70b9\u6545\u969c\u7684\u65e0\u4ee3\u7406\u6d88\u606f\u7cfb\u7edf\uff0c\u5e76\u89e3\u51b3\u4e86\u5bf9\u5176\u6027\u80fd\u8bc4\u4f30\u7684\u73b0\u6709\u7814\u7a76\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u9996\u5148\uff0c\u5bf9\u51e0\u79cd\u53ef\u80fd\u7684\u5019\u9009\u8005\u8fdb\u884c\u4e86\u5b9a\u6027\u5206\u6790\uff0c\u4ee5\u627e\u5230\u6700\u6709\u5e0c\u671b\u7684\u3002\u7136\u540e\uff0c\u8bbe\u8ba1\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5e7f\u6cdb\u7684\u5f00\u6e90\u57fa\u51c6\u6d4b\u8bd5\u5957\u4ef6\uff0c\u4ee5\u7cfb\u7edf\u548c\u516c\u5e73\u5730\u8bc4\u4f30\u6240\u9009\u5e93\uff08\u5373 ZeroMQ\u3001NanoMsg \u548c NanoMsg-Next-Generation (NNG)\uff09\u7684\u6027\u80fd\u3002", "result": "\u8bc4\u4f30\u4e86\u8fd9\u4e9b\u5e93\u5728\u4e0d\u540c\u6307\u6807\u548c\u5de5\u4f5c\u8d1f\u8f7d\u6761\u4ef6\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u63ed\u793a\u4e86\u5b83\u4eec\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u5206\u6790\u4f7f\u5f97\u5b9e\u8df5\u8005\u80fd\u591f\u4e3a\u4ed6\u4eec\u7684\u9700\u6c42\u9009\u62e9\u6700\u5408\u9002\u7684\u65e0\u4ee3\u7406\u6d88\u606f\u4f20\u9012\u5e93\u3002"}}
{"id": "2508.06779", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06779", "abs": "https://arxiv.org/abs/2508.06779", "authors": ["Minku Kim", "Brian Acosta", "Pratik Chaudhari", "Michael Posa"], "title": "Learning a Vision-Based Footstep Planner for Hierarchical Walking Control", "comment": "8 pages, 8 figures, accepted to 2025 IEEE-RAS 24th International\n  Conference on Humanoid Robots", "summary": "Bipedal robots demonstrate potential in navigating challenging terrains\nthrough dynamic ground contact. However, current frameworks often depend solely\non proprioception or use manually designed visual pipelines, which are fragile\nin real-world settings and complicate real-time footstep planning in\nunstructured environments. To address this problem, we present a vision-based\nhierarchical control framework that integrates a reinforcement learning\nhigh-level footstep planner, which generates footstep commands based on a local\nelevation map, with a low-level Operational Space Controller that tracks the\ngenerated trajectories. We utilize the Angular Momentum Linear Inverted\nPendulum model to construct a low-dimensional state representation to capture\nan informative encoding of the dynamics while reducing complexity. We evaluate\nour method across different terrain conditions using the underactuated bipedal\nrobot Cassie and investigate the capabilities and challenges of our approach\nthrough simulation and hardware experiments.", "AI": {"tldr": "Vision-based hierarchical control with RL for footstep planning improves bipedal robot navigation on challenging terrain.", "motivation": "Current frameworks for bipedal robots struggle with real-time footstep planning in unstructured environments due to over-reliance on proprioception or fragile, manually designed visual pipelines.", "method": "A vision-based hierarchical control framework is proposed, combining a reinforcement learning high-level footstep planner that uses local elevation maps with a low-level Operational Space Controller. The Angular Momentum Linear Inverted Pendulum model is used for a low-dimensional state representation.", "result": "The method was evaluated on the underactuated bipedal robot Cassie across various terrain conditions through simulation and hardware experiments, demonstrating its capabilities and challenges.", "conclusion": "The proposed vision-based hierarchical control framework effectively integrates a reinforcement learning footstep planner with an Operational Space Controller for bipedal robot navigation in challenging terrains, outperforming existing methods by reducing complexity and improving real-time adaptability."}}
{"id": "2508.07007", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07007", "abs": "https://arxiv.org/abs/2508.07007", "authors": ["F. S. Luiz", "F. F. Fanchini", "Victor Hugo C. de Albuquerque", "J. P. Papa", "M. C. de Oliveira"], "title": "A Quantum Walk-Driven Algorithm for the Minimum Spanning Tree Problem under a Maximal Degree Constraint", "comment": null, "summary": "We present a novel quantum walk-based approach to solve the Minimum Spanning\nTree (MST) problem under a maximal degree constraint (MDC). By recasting the\nclassical MST problem as a quantum walk on a graph, where vertices are encoded\nas quantum states and edge weights are inverted to define a modified\nHamiltonian, we demonstrate that the quantum evolution naturally selects the\nMST by maximizing the cumulative transition probability (and thus the Shannon\nentropy) over the spanning tree. Our method, termed Quantum Kruskal with MDC,\nsignificantly reduces the quantum resource requirement to $\\mathcal{O}(\\log N)$\nqubits while retaining a competitive classical computational complexity.\nNumerical experiments on fully connected graphs up to $10^4$ vertices confirm\nthat, particularly for MDC values exceeding $4$, the algorithm delivers MSTs\nwith optimal or near-optimal total weights. When MDC values are less or equal\nto $4$, some instances achieve a suboptimal solution, still outperforming\nseveral established classical algorithms. These results open promising\nperspectives for hybrid quantum-classical solutions in large-scale graph\noptimization.", "AI": {"tldr": "\u91cf\u5b50\u884c\u8d70\u65b9\u6cd5\u89e3\u51b3\u4e86\u5e26\u6700\u5927\u5ea6\u7ea6\u675f\uff08MDC\uff09\u7684\u6700\u5c0f\u751f\u6210\u6811\uff08MST\uff09\u95ee\u9898\uff0c\u51cf\u5c11\u4e86\u91cf\u5b50\u8d44\u6e90\u9700\u6c42\uff0c\u5e76\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u63d0\u4f9b\u4e86\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684\u91cf\u5b50\u884c\u8d70\u65b9\u6cd5\u6765\u89e3\u51b3\u5e26\u6700\u5927\u5ea6\u7ea6\u675f\uff08MDC\uff09\u7684\u6700\u5c0f\u751f\u6210\u6811\uff08MST\uff09\u95ee\u9898\u3002", "method": "\u91cf\u5b50\u884c\u8d70\u65b9\u6cd5\uff0c\u5c06MST\u95ee\u9898\u8f6c\u5316\u4e3a\u91cf\u5b50\u884c\u8d70\uff0c\u5176\u4e2d\u9876\u70b9\u7f16\u7801\u4e3a\u91cf\u5b50\u6001\uff0c\u8fb9\u6743\u91cd\u88ab\u53cd\u8f6c\u4ee5\u5b9a\u4e49\u4fee\u6539\u540e\u7684\u54c8\u5bc6\u987f\u91cf\uff0c\u901a\u8fc7\u6700\u5927\u5316\u7d2f\u79ef\u8dc3\u8fc1\u6982\u7387\uff08\u4ee5\u53ca\u9999\u519c\u71b5\uff09\u6765\u9009\u62e9MST\u3002", "result": "\u8be5\u65b9\u6cd5\u5c06\u91cf\u5b50\u8d44\u6e90\u9700\u6c42\u51cf\u5c11\u5230O(log N)\u91cf\u5b50\u6bd4\u7279\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u7ecf\u5178\u8ba1\u7b97\u590d\u6742\u6027\u3002\u6570\u503c\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53MDC\u503c\u5927\u4e8e4\u65f6\uff0c\u7b97\u6cd5\u80fd\u591f\u63d0\u4f9b\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u603b\u6743\u91cdMST\u3002\u5f53MDC\u503c\u5c0f\u4e8e\u6216\u7b49\u4e8e4\u65f6\uff0c\u67d0\u4e9b\u5b9e\u4f8b\u53ef\u4ee5\u83b7\u5f97\u6b21\u4f18\u89e3\uff0c\u4f46\u4ecd\u4f18\u4e8e\u51e0\u4e2a\u7ecf\u5178\u7684\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u89e3\u51b3\u5e26\u6700\u5927\u5ea6\u7ea6\u675f\uff08MDC\uff09\u7684\u6700\u5c0f\u751f\u6210\u6811\uff08MST\uff09\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u91cf\u5b50\u884c\u8d70\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5c06\u7ecf\u5178MST\u95ee\u9898\u8f6c\u5316\u4e3a\u91cf\u5b50\u884c\u8d70\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u6700\u5927\u5316\u7d2f\u79ef\u8dc3\u8fc1\u6982\u7387\u6765\u9009\u62e9MST\u3002\u8be5\u65b9\u6cd5\u5c06\u91cf\u5b50\u8d44\u6e90\u9700\u6c42\u51cf\u5c11\u5230O(log N)\u91cf\u5b50\u6bd4\u7279\uff0c\u540c\u65f6\u4fdd\u6301\u6709\u7ade\u4e89\u529b\u7684\u7ecf\u5178\u8ba1\u7b97\u590d\u6742\u6027\u3002"}}
{"id": "2508.06729", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06729", "abs": "https://arxiv.org/abs/2508.06729", "authors": ["Komala Subramanyam Cherukuri", "Pranav Abishai Moses", "Aisa Sakata", "Jiangping Chen", "Haihua Chen"], "title": "Large Language Models for Oral History Understanding with Text Classification and Sentiment Analysis", "comment": null, "summary": "Oral histories are vital records of lived experience, particularly within\ncommunities affected by systemic injustice and historical erasure. Effective\nand efficient analysis of their oral history archives can promote access and\nunderstanding of the oral histories. However, Large-scale analysis of these\narchives remains limited due to their unstructured format, emotional\ncomplexity, and high annotation costs. This paper presents a scalable framework\nto automate semantic and sentiment annotation for Japanese American\nIncarceration Oral History. Using LLMs, we construct a high-quality dataset,\nevaluate multiple models, and test prompt engineering strategies in\nhistorically sensitive contexts. Our multiphase approach combines expert\nannotation, prompt design, and LLM evaluation with ChatGPT, Llama, and Qwen. We\nlabeled 558 sentences from 15 narrators for sentiment and semantic\nclassification, then evaluated zero-shot, few-shot, and RAG strategies. For\nsemantic classification, ChatGPT achieved the highest F1 score (88.71%),\nfollowed by Llama (84.99%) and Qwen (83.72%). For sentiment analysis, Llama\nslightly outperformed Qwen (82.66%) and ChatGPT (82.29%), with all models\nshowing comparable results. The best prompt configurations were used to\nannotate 92,191 sentences from 1,002 interviews in the JAIOH collection. Our\nfindings show that LLMs can effectively perform semantic and sentiment\nannotation across large oral history collections when guided by well-designed\nprompts. This study provides a reusable annotation pipeline and practical\nguidance for applying LLMs in culturally sensitive archival analysis. By\nbridging archival ethics with scalable NLP techniques, this work lays the\ngroundwork for responsible use of artificial intelligence in digital humanities\nand preservation of collective memory. GitHub:\nhttps://github.com/kc6699c/LLM4OralHistoryAnalysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528LLMs\u6280\u672f\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u65e5\u672c\u88d4\u7f8e\u56fd\u4eba\u76d1\u7981\u53e3\u8ff0\u5386\u53f2\u8fdb\u884c\u8bed\u4e49\u548c\u60c5\u611f\u6807\u6ce8\uff0c\u63d0\u9ad8\u4e86\u5206\u6790\u6548\u7387\u548c\u89c4\u6a21\uff0c\u4e3a\u6570\u5b57\u4eba\u6587\u548c\u96c6\u4f53\u8bb0\u5fc6\u4fdd\u62a4\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u6cd5\u548c\u5b9e\u8df5\u6307\u5bfc\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u5bf9\u53d7\u7cfb\u7edf\u6027\u4e0d\u516c\u6b63\u548c\u5386\u53f2\u9057\u5fd8\u5f71\u54cd\u7684\u793e\u533a\u7684\u53e3\u8ff0\u5386\u53f2\u7684\u83b7\u53d6\u548c\u7406\u89e3\uff0c\u9700\u8981\u5bf9\u5927\u89c4\u6a21\u53e3\u8ff0\u5386\u53f2\u6863\u6848\u8fdb\u884c\u6709\u6548\u4e14\u9ad8\u6548\u7684\u5206\u6790\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5176\u975e\u7ed3\u6784\u5316\u683c\u5f0f\u3001\u60c5\u611f\u590d\u6742\u6027\u548c\u9ad8\u6602\u7684\u6807\u6ce8\u6210\u672c\uff0c\u8fd9\u4e9b\u6863\u6848\u7684\u5927\u89c4\u6a21\u5206\u6790\u4ecd\u7136\u53d7\u5230\u9650\u5236\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u6846\u67b6\uff0c\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5bf9\u65e5\u672c\u88d4\u7f8e\u56fd\u4eba\u76d1\u7981\u53e3\u8ff0\u5386\u53f2\u8fdb\u884c\u81ea\u52a8\u8bed\u4e49\u548c\u60c5\u611f\u6807\u6ce8\u3002\u7814\u7a76\u4eba\u5458\u6784\u5efa\u4e86\u4e00\u4e2a\u9ad8\u8d28\u91cf\u7684\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\uff0c\u5e76\u5728\u5177\u6709\u5386\u53f2\u654f\u611f\u6027\u7684\u80cc\u666f\u4e0b\u6d4b\u8bd5\u4e86\u63d0\u793a\u5de5\u7a0b\u7b56\u7565\u3002\u591a\u9636\u6bb5\u65b9\u6cd5\u7ed3\u5408\u4e86\u4e13\u5bb6\u6807\u6ce8\u3001\u63d0\u793a\u8bbe\u8ba1\u4ee5\u53ca\u4f7f\u7528ChatGPT\u3001Llama\u548cQwen\u7b49\u6a21\u578b\u7684LLM\u8bc4\u4f30\u3002\u5bf9\u6765\u81ea15\u4f4d\u8bb2\u8ff0\u8005\u7684558\u4e2a\u53e5\u5b50\u8fdb\u884c\u4e86\u60c5\u611f\u548c\u8bed\u4e49\u5206\u7c7b\u6807\u6ce8\uff0c\u5e76\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7b56\u7565\u3002", "result": "\u7814\u7a76\u4eba\u5458\u5bf9\u6765\u81ea15\u4f4d\u8bb2\u8ff0\u8005\u7684558\u4e2a\u53e5\u5b50\u8fdb\u884c\u4e86\u60c5\u611f\u548c\u8bed\u4e49\u5206\u7c7b\u6807\u6ce8\uff0c\u5e76\u8bc4\u4f30\u4e86\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548cRAG\u7b56\u7565\u3002\u5728\u8bed\u4e49\u5206\u7c7b\u65b9\u9762\uff0cChatGPT\u53d6\u5f97\u4e86\u6700\u9ad8\u7684F1\u5206\u6570\uff0888.71%\uff09\uff0c\u5176\u6b21\u662fLlama\uff0884.99%\uff09\u548cQwen\uff0883.72%\uff09\u3002\u5728\u60c5\u611f\u5206\u6790\u65b9\u9762\uff0cLlama\u7565\u4f18\u4e8eQwen\uff0882.66%\uff09\u548cChatGPT\uff0882.29%\uff09\uff0c\u6240\u6709\u6a21\u578b\u7684\u8868\u73b0\u90fd\u76f8\u5f53\u3002\u6700\u7ec8\uff0c\u5c06\u6700\u4f73\u63d0\u793a\u914d\u7f6e\u5e94\u7528\u4e8e\u6807\u6ce8JAIOH\u6536\u85cf\u4e2d\u76841002\u4efd\u8bbf\u8c08\u4e2d\u768492,191\u4e2a\u53e5\u5b50\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u53ef\u4ee5\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u63d0\u793a\uff0c\u6709\u6548\u5730\u5bf9\u5927\u89c4\u6a21\u53e3\u8154\u5386\u53f2\u6587\u96c6\u8fdb\u884c\u8bed\u4e49\u548c\u60c5\u611f\u6807\u6ce8\u3002\u8be5\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u91cd\u7528\u7684\u6807\u6ce8\u6d41\u7a0b\u548c\u5b9e\u8df5\u6307\u5bfc\uff0c\u5c06\u6863\u6848\u4f26\u7406\u4e0e\u53ef\u6269\u5c55\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u4e3a\u4eba\u5de5\u667a\u80fd\u5728\u6570\u5b57\u4eba\u6587\u548c\u96c6\u4f53\u8bb0\u5fc6\u4fdd\u62a4\u4e2d\u7684\u8d1f\u8d23\u4efb\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06552", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06552", "abs": "https://arxiv.org/abs/2508.06552", "authors": ["Unisha Joshi"], "title": "Age-Diverse Deepfake Dataset: Bridging the Age Gap in Deepfake Detection", "comment": "11 pages, 4 figures, and 7 tables", "summary": "The challenges associated with deepfake detection are increasing\nsignificantly with the latest advancements in technology and the growing\npopularity of deepfake videos and images. Despite the presence of numerous\ndetection models, demographic bias in the deepfake dataset remains largely\nunaddressed. This paper focuses on the mitigation of age-specific bias in the\ndeepfake dataset by introducing an age-diverse deepfake dataset that will\nimprove fairness across age groups. The dataset is constructed through a\nmodular pipeline incorporating the existing deepfake datasets Celeb-DF,\nFaceForensics++, and UTKFace datasets, and the creation of synthetic data to\nfill the age distribution gaps. The effectiveness and generalizability of this\ndataset are evaluated using three deepfake detection models: XceptionNet,\nEfficientNet, and LipForensics. Evaluation metrics, including AUC, pAUC, and\nEER, revealed that models trained on the age-diverse dataset demonstrated\nfairer performance across age groups, improved overall accuracy, and higher\ngeneralization across datasets. This study contributes a reproducible,\nfairness-aware deepfake dataset and model pipeline that can serve as a\nfoundation for future research in fairer deepfake detection. The complete\ndataset and implementation code are available at\nhttps://github.com/unishajoshi/age-diverse-deepfake-detection.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e74\u9f84\u591a\u6837\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u5e74\u9f84\u504f\u89c1\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\u4f7f\u7528\u8be5\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u516c\u5e73\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u6709\u6240\u63d0\u5347\u3002", "motivation": "\u5f53\u524d\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6280\u672f\u9762\u4e34\u6280\u672f\u8fdb\u6b65\u548c\u6df1\u5ea6\u4f2a\u9020\u5185\u5bb9\u666e\u53ca\u5e26\u6765\u7684\u6311\u6218\uff0c\u800c\u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u5e74\u9f84\u504f\u89c1\u95ee\u9898\u5c1a\u672a\u5f97\u5230\u5145\u5206\u89e3\u51b3\u3002", "method": "\u901a\u8fc7\u6574\u5408\u73b0\u6709\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff08Celeb-DF\u3001FaceForensics++\u3001UTKFace\uff09\u5e76\u521b\u5efa\u5408\u6210\u6570\u636e\u6765\u6784\u5efa\u5e74\u9f84\u591a\u6837\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528XceptionNet\u3001EfficientNet\u548cLipForensics\u4e09\u4e2a\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u5728\u5e74\u9f84\u591a\u6837\u5316\u7684\u6570\u636e\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8de8\u5e74\u9f84\u7ec4\u7684\u516c\u5e73\u6027\u3001\u6574\u4f53\u51c6\u786e\u6027\u548c\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5e74\u9f84\u591a\u6837\u5316\u7684\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u548c\u6a21\u578b\u7ba1\u7ebf\uff0c\u4ee5\u89e3\u51b3\u6df1\u5ea6\u4f2a\u9020\u6570\u636e\u96c6\u4e2d\u7684\u5e74\u9f84\u504f\u89c1\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8de8\u5e74\u9f84\u7ec4\u7684\u516c\u5e73\u6027\u3001\u6574\u4f53\u51c6\u786e\u6027\u548c\u8de8\u6570\u636e\u96c6\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07515", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07515", "abs": "https://arxiv.org/abs/2508.07515", "authors": ["Junyang Cai", "Weimin Huang", "Jyotirmoy V. Deshmukh", "Lars Lindemann", "Bistra Dilkina"], "title": "Neuro-Symbolic Acceleration of MILP Motion Planning with Temporal Logic and Chance Constraints", "comment": null, "summary": "Autonomous systems must solve motion planning problems subject to\nincreasingly complex, time-sensitive, and uncertain missions. These problems\noften involve high-level task specifications, such as temporal logic or chance\nconstraints, which require solving large-scale Mixed-Integer Linear Programs\n(MILPs). However, existing MILP-based planning methods suffer from high\ncomputational cost and limited scalability, hindering their real-time\napplicability. We propose to use a neuro-symbolic approach to accelerate\nMILP-based motion planning by leveraging machine learning techniques to guide\nthe solver's symbolic search. Focusing on two representative classes of\nplanning problems, namely, those with Signal Temporal Logic (STL)\nspecifications and those with chance constraints formulated via Conformal\nPredictive Programming (CPP). We demonstrate how graph neural network-based\nlearning methods can guide traditional symbolic MILP solvers in solving\nchallenging planning problems, including branching variable selection and\nsolver parameter configuration. Through extensive experiments, we show that\nneuro-symbolic search techniques yield scalability gains. Our approach yields\nsubstantial improvements, achieving an average performance gain of about 20%\nover state-of-the-art solver across key metrics, including runtime and solution\nquality.", "AI": {"tldr": "A neuro-symbolic approach accelerates MILP-based motion planning by using machine learning to guide symbolic search, improving scalability and performance by ~20%.", "motivation": "Existing MILP-based planning methods are computationally expensive and lack scalability, hindering real-time applicability for complex, time-sensitive, and uncertain missions.", "method": "A neuro-symbolic approach is proposed, leveraging graph neural network-based learning methods to guide traditional symbolic MILP solvers in solving motion planning problems with STL specifications and chance constraints.", "result": "The proposed neuro-symbolic approach yields substantial scalability gains, with an average performance improvement of about 20% over state-of-the-art solvers in terms of runtime and solution quality.", "conclusion": "Neuro-symbolic search techniques significantly improve the scalability of MILP-based motion planning, achieving an average performance gain of about 20% over state-of-the-art solvers."}}
{"id": "2508.07422", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07422", "abs": "https://arxiv.org/abs/2508.07422", "authors": ["Iftakhar Bin Elius", "Nathan Valadez", "Dante James", "Sami Elgalal", "Grzegorz Chajewski", "Tetiana Romanova", "Andrzej Ptok", "Dariusz Kaczorowski", "Madhab Neupane"], "title": "Electronic band structure of a nodal line semimetal candidate ErSbTe", "comment": "8 pages, 4 figures", "summary": "The LnSbTe family is well known for hosting a plethora of intriguing\ncharacteristics stemming from its crystalline symmetry, magnetic structure, 4f\nelectronic correlations and spin orbit coupling (SOC) phenomena. In this paper,\nwe have systematically studied the bulk electrical and thermodynamic properties\nand electronic structure of the nodal line semimetal candidate ErSbTe using\nangle resolved photoemission spectroscopy (ARPES) corroborated with first\nprinciples based theoretical band structure calculations with and without\nconsidering the effect of SOC, a critical factor dictating the band degeneracy\nwhich depends on the choice of the Ln atom. Corroborative temperature dependent\nsusceptibility, electrical resistivity and thermodynamic measurements,\ncoherently exhibit paramagnetic to antiferromagnetic phase transition\napproximately at 1.94 K, and another sharp anomaly at 1.75 K. The zero field\ncooled resistivity measurement does not show the characteristic hump like\nfeature in the other LnSbTe materials. The electronic band structure of ErSbTe,\nexhibits a diamond shaped Fermi surface. Along the high symmetry direction GX,\nelectronic bands are projected to cross over the Fermi energy, necessitated by\nthe nonsymmorphic symmetry of the system. The other crossing along this\ndirection is gapped, which evolves along the momentum space reaching its\nmaximum along the GM direction.", "AI": {"tldr": "ErSbTe \u662f\u4e00\u79cd\u5728\u7ea6 1.94 K \u548c 1.75 K \u53d1\u751f\u78c1\u76f8\u53d8\u7684\u8282\u70b9\u7ebf\u534a\u91d1\u5c5e\u5019\u9009\u7269\uff0c\u5176\u7535\u5b50\u7ed3\u6784\u548c\u5fb7\u6cfd\u5c14\u6548\u5e94\u4f7f\u5176\u6210\u4e3a\u62d3\u6251\u6750\u6599\u7814\u7a76\u7684\u6709\u524d\u666f\u7684\u5019\u9009\u7269\u3002", "motivation": "LnSbTe \u5bb6\u65cf\u56e0\u5176\u6676\u4f53\u5bf9\u79f0\u6027\u3001\u78c1\u7ed3\u6784\u30014f \u7535\u5b50\u76f8\u5173\u6027\u548c\u81ea\u65cb-\u8f68\u9053\u8026\u5408\uff08SOC\uff09\u73b0\u8c61\u800c\u95fb\u540d\uff0c\u4f46 ErSbTe \u7684\u5177\u4f53\u6027\u8d28\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u4f7f\u7528\u89d2\u5206\u8fa8\u5149\u7535\u5b50\u80fd\u8c31\uff08ARPES\uff09\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u7406\u8bba\u80fd\u5e26\u7ed3\u6784\u8ba1\u7b97\uff08\u8003\u8651\u548c\u4e0d\u8003\u8651 SOC \u7684\u5f71\u54cd\uff09\u6765\u7cfb\u7edf\u7814\u7a76 ErSbTe \u7684\u5757\u4f53\u7535\u5b66\u548c\u70ed\u529b\u5b66\u6027\u8d28\u4ee5\u53ca\u7535\u5b50\u7ed3\u6784\u3002", "result": "ErSbTe \u5728\u5927\u7ea6 1.94 K \u548c 1.75 K \u53d1\u751f\u987a\u78c1\u5230\u53cd\u94c1\u78c1\u76f8\u53d8\u3002\u5176\u7535\u5b50\u80fd\u5e26\u7ed3\u6784\u5728 GX \u65b9\u5411\u4e0a\u8868\u73b0\u51fa\u83f1\u5f62\u8d39\u7c73\u9762\uff0c\u5e76\u4e14\u5b58\u5728\u7531\u975e\u540c\u5f62\u5bf9\u79f0\u6027\u5f15\u8d77\u7684\u7a7f\u8d8a\u8d39\u7c73\u80fd\u91cf\u7684\u7535\u5b50\u5e26\u4ea4\u53c9\u3002\u6cbf GX \u65b9\u5411\u7684\u53e6\u4e00\u4e2a\u4ea4\u53c9\u70b9\u88ab\u95f4\u9699\u5316\uff0c\u5e76\u5728\u52a8\u91cf\u7a7a\u95f4\u4e2d\u6f14\u5316\u3002", "conclusion": "ErSbTe \u8868\u73b0\u51fa\u4e0e\u9567\u7cfb\u539f\u5b50\u76f8\u5173\u7684\u591a\u7c7b\u8ff7\u4eba\u7279\u5f81\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5176\u6676\u4f53\u5bf9\u79f0\u6027\u3001\u78c1\u7ed3\u6784\u30014f \u7535\u5b50\u76f8\u5173\u6027\u548c\u81ea\u65cb-\u8f68\u9053\u8026\u5408\uff08SOC\uff09\u3002ErSbTe \u7684\u4f4e\u6e29\u6027\u8d28\u5728\u5fb7\u6cfd\u5c14\u6548\u5e94\u548c\u8d5d\u7b80\u5e76\u65b9\u9762\u8868\u73b0\u51fa\u65b0\u9896\u6027\uff0c\u8fd9\u4e0e\u901a\u8fc7 ARPES \u548c\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u63ed\u793a\u7684\u7535\u5b50\u7ed3\u6784\u4e00\u81f4\uff0c\u8868\u660e ErSbTe \u662f\u4e00\u79cd\u6709\u524d\u9014\u7684\u62d3\u6251\u6750\u6599\u3002"}}
{"id": "2508.07226", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07226", "abs": "https://arxiv.org/abs/2508.07226", "authors": ["Yueheng Li", "Xueyun Long", "Mario Pauli", "Suheng Tian", "Xiang Wan", "Benjamin Nuss", "Tiejun Cui", "Haixia Zhang", "Thomas Zwick"], "title": "Multi-RIS Deployment Optimization for mmWave ISAC Systems in Real-World Environments", "comment": "13 pages, 9 figures", "summary": "Reconfigurable intelligent surface-assisted integrated sensing and\ncommunication (RIS-ISAC) presents a promising system architecture to leverage\nthe wide bandwidth available at millimeter-wave (mmWave) frequencies, while\nmitigating severe signal propagation losses and reducing infrastructure costs.\nTo enhance ISAC functionalities in the future air-ground integrated network\napplications, RIS deployment must be carefully designed and evaluated, which\nforms the core motivation of this paper. To ensure practical relevance, a\nmulti-RIS-ISAC system is established, with its signal model at mmWave\nfrequencies demonstrated using ray-launching calibrated to real-world\nenvironments. On this basis, an energy-efficiency-driven optimization problem\nis formulated to minimize the multi-RIS size-to-coverage sum ratio,\ncomprehensively considering real-world RIS deployment constraints, positions,\norientations, as well as ISAC beamforming strategies at both the base station\nand the RISs. To solve the resulting non-convex mixed-integer problem, a\nsimplified reformulation based on equivalent gain scaling method is introduced.\nA two-step iterative algorithm is then proposed, in which the deployment\nparameters are determined under fixed RIS positions in the first step, and the\nRIS position set is updated in the second step to progressively approach the\noptimum solution. Simulation results based on realistic parameter benchmarks\npresent that the optimized RISs deployment significantly enhances communication\ncoverage and sensing accuracy with the minimum RIS sizes, outperforming\nexisting approaches.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9RIS\u8f85\u52a9ISAC\u7cfb\u7edf\u7684\u4f18\u5316\u90e8\u7f72\u65b9\u6cd5\uff0c\u901a\u8fc7\u8fed\u4ee3\u7b97\u6cd5\u4f18\u5316RIS\u7684\u5c3a\u5bf8\u3001\u4f4d\u7f6e\u548c\u65b9\u5411\uff0c\u4ee5\u63d0\u9ad8\u901a\u4fe1\u8986\u76d6\u548c\u611f\u77e5\u7cbe\u5ea6\uff0c\u540c\u65f6\u964d\u4f4eRIS\u5c3a\u5bf8\u3002", "motivation": "\u4e3a\u4e86\u589e\u5f3a\u672a\u6765\u7a7a\u5730\u4e00\u4f53\u5316\u7f51\u7edc\u5e94\u7528\u4e2d\u7684ISAC\u529f\u80fd\uff0c\u9700\u8981\u4ed4\u7ec6\u8bbe\u8ba1\u548c\u8bc4\u4f30RIS\u7684\u90e8\u7f72\uff0c\u8fd9\u662f\u672c\u7814\u7a76\u7684\u6838\u5fc3\u52a8\u673a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7b49\u6548\u589e\u76ca\u7f29\u653e\u65b9\u6cd5\u7684\u7b80\u5316\u91cd\u6784\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e24\u6b65\u8fed\u4ee3\u7b97\u6cd5\u6765\u6c42\u89e3\u975e\u51f8\u6df7\u5408\u6574\u6570\u95ee\u9898\uff0c\u4ee5\u4f18\u5316RIS\u90e8\u7f72\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4f18\u5316\u7684RIS\u90e8\u7f72\u663e\u8457\u63d0\u9ad8\u4e86\u901a\u4fe1\u8986\u76d6\u548c\u611f\u77e5\u7cbe\u5ea6\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u4e86RIS\u5c3a\u5bf8\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u4f18\u5316\u90e8\u7f72\u65b9\u6848\u5728\u901a\u4fe1\u8986\u76d6\u3001\u611f\u77e5\u7cbe\u5ea6\u548cRIS\u5c3a\u5bf8\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.06832", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06832", "abs": "https://arxiv.org/abs/2508.06832", "authors": ["Haifeng Li", "Wang Guo", "Haiyang Wu", "Mengwei Wu", "Jipeng Zhang", "Qing Zhu", "Yu Liu", "Xin Huang", "Chao Tao"], "title": "Remote Sensing Image Intelligent Interpretation with the Language-Centered Perspective: Principles, Methods and Challenges", "comment": null, "summary": "The mainstream paradigm of remote sensing image interpretation has long been\ndominated by vision-centered models, which rely on visual features for semantic\nunderstanding. However, these models face inherent limitations in handling\nmulti-modal reasoning, semantic abstraction, and interactive decision-making.\nWhile recent advances have introduced Large Language Models (LLMs) into remote\nsensing workflows, existing studies primarily focus on downstream applications,\nlacking a unified theoretical framework that explains the cognitive role of\nlanguage. This review advocates a paradigm shift from vision-centered to\nlanguage-centered remote sensing interpretation. Drawing inspiration from the\nGlobal Workspace Theory (GWT) of human cognition, We propose a\nlanguage-centered framework for remote sensing interpretation that treats LLMs\nas the cognitive central hub integrating perceptual, task, knowledge and action\nspaces to enable unified understanding, reasoning, and decision-making. We\nfirst explore the potential of LLMs as the central cognitive component in\nremote sensing interpretation, and then summarize core technical challenges,\nincluding unified multimodal representation, knowledge association, and\nreasoning and decision-making. Furthermore, we construct a global\nworkspace-driven interpretation mechanism and review how language-centered\nsolutions address each challenge. Finally, we outline future research\ndirections from four perspectives: adaptive alignment of multimodal data, task\nunderstanding under dynamic knowledge constraints, trustworthy reasoning, and\nautonomous interaction. This work aims to provide a conceptual foundation for\nthe next generation of remote sensing interpretation systems and establish a\nroadmap toward cognition-driven intelligent geospatial analysis.", "AI": {"tldr": "\u8fd9\u7bc7\u7efc\u8ff0\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ee5\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u65b9\u6cd5\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4f5c\u4e3a\u8ba4\u77e5\u4e2d\u5fc3\uff0c\u6574\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u667a\u80fd\u5730\u7406\u7a7a\u95f4\u5206\u6790\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u6a21\u578b\u5728\u5904\u7406\u591a\u6a21\u6001\u63a8\u7406\u3001\u8bed\u4e49\u62bd\u8c61\u548c\u4ea4\u4e92\u5f0f\u51b3\u7b56\u65b9\u9762\u7684\u56fa\u6709\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5e94\u7528\u4e8e\u9065\u611f\u9886\u57df\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u91ca\u8bed\u8a00\u5728\u8ba4\u77e5\u4e2d\u7684\u4f5c\u7528\u3002", "method": "\u6587\u7ae0\u501f\u9274\u4e86\u4eba\u7c7b\u8ba4\u77e5\u7684\u5168\u5c40\u5de5\u4f5c\u533a\u7406\u8bba\uff08GWT\uff09\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4ee5\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u6846\u67b6\uff0c\u5e76\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u89c6\u4e3a\u6574\u5408\u611f\u77e5\u3001\u4efb\u52a1\u3001\u77e5\u8bc6\u548c\u884c\u52a8\u7a7a\u95f4\u7684\u8ba4\u77e5\u4e2d\u5fc3\u67a2\u7ebd\uff0c\u4ee5\u5b9e\u73b0\u7edf\u4e00\u7684\u7406\u89e3\u3001\u63a8\u7406\u548c\u51b3\u7b56\u3002", "result": "\u6587\u7ae0\u63a2\u8ba8\u4e86LLMs\u4f5c\u4e3a\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u6838\u5fc3\u8ba4\u77e5\u6210\u5206\u7684\u6f5c\u529b\uff0c\u603b\u7ed3\u4e86\u7edf\u4e00\u591a\u6a21\u6001\u8868\u793a\u3001\u77e5\u8bc6\u5173\u8054\u3001\u63a8\u7406\u548c\u51b3\u7b56\u7b49\u6280\u672f\u6311\u6218\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u5168\u5c40\u5de5\u4f5c\u533a\u9a71\u52a8\u7684\u89e3\u91ca\u673a\u5236\uff0c\u6982\u8ff0\u4e86\u591a\u6a21\u6001\u6570\u636e\u81ea\u9002\u5e94\u5bf9\u9f50\u3001\u52a8\u6001\u77e5\u8bc6\u7ea6\u675f\u4e0b\u7684\u4efb\u52a1\u7406\u89e3\u3001\u53ef\u4fe1\u63a8\u7406\u548c\u81ea\u4e3b\u4ea4\u4e92\u7b49\u672a\u6765\u7814\u7a76\u65b9\u5411\u3002", "conclusion": "\u8be5\u7efc\u8ff0\u6587\u7ae0\u4e3b\u5f20\u4ece\u4ee5\u89c6\u89c9\u4e3a\u4e2d\u5fc3\u7684\u7814\u7a76\u8303\u5f0f\u8f6c\u53d8\u4e3a\u4ee5\u8bed\u8a00\u4e3a\u4e2d\u5fc3\u7684\u7814\u7a76\u8303\u5f0f\uff0c\u4ee5\u5e94\u5bf9\u5f53\u524d\u9065\u611f\u56fe\u50cf\u89e3\u91ca\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u4e0b\u4e00\u4ee3\u9065\u611f\u89e3\u91ca\u7cfb\u7edf\u5960\u5b9a\u6982\u5ff5\u57fa\u7840\uff0c\u540c\u65f6\u4e3a\u8ba4\u77e5\u9a71\u52a8\u7684\u667a\u80fd\u5730\u7406\u7a7a\u95f4\u5206\u6790\u63d0\u4f9b\u8def\u7ebf\u56fe\u3002"}}
{"id": "2508.07015", "categories": ["cs.AI", "cs.DS"], "pdf": "https://arxiv.org/pdf/2508.07015", "abs": "https://arxiv.org/abs/2508.07015", "authors": ["Hannes Ihalainen", "Dieter Vandesande", "Andr\u00e9 Schidler", "Jeremias Berg", "Bart Bogaerts", "Matti J\u00e4rvisalo"], "title": "Efficient and Reliable Hitting-Set Computations for the Implicit Hitting Set Approach", "comment": null, "summary": "The implicit hitting set (IHS) approach offers a general framework for\nsolving computationally hard combinatorial optimization problems declaratively.\nIHS iterates between a decision oracle used for extracting sources of\ninconsistency and an optimizer for computing so-called hitting sets (HSs) over\nthe accumulated sources of inconsistency. While the decision oracle is\nlanguage-specific, the optimizers is usually instantiated through integer\nprogramming.\n  We explore alternative algorithmic techniques for hitting set optimization\nbased on different ways of employing pseudo-Boolean (PB) reasoning as well as\nstochastic local search. We extensively evaluate the practical feasibility of\nthe alternatives in particular in the context of pseudo-Boolean (0-1 IP)\noptimization as one of the most recent instantiations of IHS. Highlighting a\ntrade-off between efficiency and reliability, while a commercial IP solver\nturns out to remain the most effective way to instantiate HS computations, it\ncan cause correctness issues due to numerical instability; in fact, we show\nthat exact HS computations instantiated via PB reasoning can be made\ncompetitive with a numerically exact IP solver. Furthermore, the use of PB\nreasoning as a basis for HS computations allows for obtaining certificates for\nthe correctness of IHS computations, generally applicable to any IHS\ninstantiation in which reasoning in the declarative language at hand can be\ncaptured in the PB-based proof format we employ.", "AI": {"tldr": "\u672c\u7814\u7a76\u63a2\u8ba8\u4e86\u7528\u4e8e\u9690\u5f0f\u547d\u9898\u96c6\uff08IHS\uff09\u95ee\u9898\u7684\u4f2a\u5e03\u5c14\uff08PB\uff09\u63a8\u7406\u548c\u968f\u673a\u5c40\u90e8\u641c\u7d22\u4f18\u5316\u6280\u672f\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u867d\u7136\u5546\u4e1a\u6574\u6570\u89c4\u5212\uff08IP\uff09\u6c42\u89e3\u5668\u901a\u5e38\u662f\u6700\u6709\u6548\u7684\uff0c\u4f46PB\u63a8\u7406\u53ef\u4ee5\u63d0\u4f9b\u66f4\u7a33\u5b9a\u548c\u53ef\u8ba4\u8bc1\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u5e76\u4e14\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u5177\u6709\u53ef\u6bd4\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8ba1\u7b97\u4e0a\u56f0\u96be\u7684\u7ec4\u5408\u4f18\u5316\u95ee\u9898\uff0c\u9690\u5f0f\u547d\u9898\u96c6\uff08IHS\uff09\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u58f0\u660e\u5f0f\u6846\u67b6\u3002IHS\u5728\u51b3\u7b56\u9884\u8a00\u673a\u548c\u547d\u9898\u96c6\u4f18\u5316\u5668\u4e4b\u95f4\u8fed\u4ee3\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u547d\u9898\u96c6\u4f18\u5316\u7684\u66ff\u4ee3\u7b97\u6cd5\u6280\u672f\u3002", "method": "\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u57fa\u4e8e\u4f2a\u5e03\u5c14\uff08PB\uff09\u63a8\u7406\u548c\u968f\u673a\u5c40\u90e8\u641c\u7d22\u7684\u4e0d\u540c\u547d\u9898\u96c6\u4f18\u5316\u7b97\u6cd5\u6280\u672f\u3002", "result": "\u4e0e\u6570\u503c\u7cbe\u786e\u7684IP\u6c42\u89e3\u5668\u76f8\u6bd4\uff0c\u57fa\u4e8ePB\u63a8\u7406\u7684\u7cbe\u786e\u547d\u9898\u96c6\u8ba1\u7b97\u5177\u6709\u7ade\u4e89\u529b\u3002PB\u63a8\u7406\u8fd8\u53ef\u4ee5\u4e3aIHS\u8ba1\u7b97\u7684\u6b63\u786e\u6027\u63d0\u4f9b\u8bc1\u4e66\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u867d\u7136\u5546\u4e1a\u6574\u6570\u89c4\u5212\uff08IP\uff09\u6c42\u89e3\u5668\u5728\u5b9e\u4f8b\u5316\u547d\u9898\u96c6\uff08HS\uff09\u8ba1\u7b97\u65b9\u9762\u4ecd\u7136\u662f\u6700\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4f46\u5176\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u53ef\u80fd\u5bfc\u81f4\u6b63\u786e\u6027\u95ee\u9898\u3002\u901a\u8fc7\u4f2a\u5e03\u5c14\uff08PB\uff09\u63a8\u7406\u5b9e\u73b0\u7684\u7cbe\u786e\u547d\u9898\u96c6\u8ba1\u7b97\u53ef\u4ee5\u4e0e\u6570\u503c\u7cbe\u786e\u7684IP\u6c42\u89e3\u5668\u76f8\u5ab2\u7f8e\u3002\u6b64\u5916\uff0c\u57fa\u4e8ePB\u63a8\u7406\u7684\u547d\u9898\u96c6\u8ba1\u7b97\u53ef\u4ee5\u4e3a\u9690\u5f0f\u547d\u9898\u96c6\uff08IHS\uff09\u8ba1\u7b97\u7684\u6b63\u786e\u6027\u63d0\u4f9b\u8bc1\u4e66\u3002"}}
{"id": "2508.06638", "categories": ["cs.LG", "cs.AI", "14J60 (Primary) 14F05, 14J26 (Secondary)", "F.2.2; I.2.0"], "pdf": "https://arxiv.org/pdf/2508.06638", "abs": "https://arxiv.org/abs/2508.06638", "authors": ["Muyan Anna Li", "Aditi Gautam"], "title": "Segmented Confidence Sequences and Multi-Scale Adaptive Confidence Segments for Anomaly Detection in Nonstationary Time Series", "comment": "20 pages, 11 figures", "summary": "As time series data become increasingly prevalent in domains such as\nmanufacturing, IT, and infrastructure monitoring, anomaly detection must adapt\nto nonstationary environments where statistical properties shift over time.\nTraditional static thresholds are easily rendered obsolete by regime shifts,\nconcept drift, or multi-scale changes. To address these challenges, we\nintroduce and empirically evaluate two novel adaptive thresholding frameworks:\nSegmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence\nSegments (MACS). Both leverage statistical online learning and segmentation\nprinciples for local, contextually sensitive adaptation, maintaining guarantees\non false alarm rates even under evolving distributions. Our experiments across\nWafer Manufacturing benchmark datasets show significant F1-score improvement\ncompared to traditional percentile and rolling quantile approaches. This work\ndemonstrates that robust, statistically principled adaptive thresholds enable\nreliable, interpretable, and timely detection of diverse real-world anomalies.", "AI": {"tldr": "Introduced novel adaptive thresholding frameworks (SCS and MACS) that use online learning and segmentation to handle changing data distributions in time series anomaly detection, outperforming traditional methods in tests.", "motivation": "Anomaly detection must adapt to nonstationary environments where statistical properties shift over time, as time series data become increasingly prevalent in domains such as manufacturing, IT, and infrastructure monitoring. Traditional static thresholds are easily rendered obsolete by regime shifts, concept drift, or multi-scale changes.", "method": "Introduced and empirically evaluated two novel adaptive thresholding frameworks: Segmented Confidence Sequences (SCS) and Multi-Scale Adaptive Confidence Segments (MACS). Both leverage statistical online learning and segmentation principles for local, contextually sensitive adaptation, maintaining guarantees on false alarm rates even under evolving distributions.", "result": "Experiments across Wafer Manufacturing benchmark datasets show significant F1-score improvement compared to traditional percentile and rolling quantile approaches.", "conclusion": "Robust, statistically principled adaptive thresholds enable reliable, interpretable, and timely detection of diverse real-world anomalies."}}
{"id": "2508.07407", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07407", "abs": "https://arxiv.org/abs/2508.07407", "authors": ["Jinyuan Fang", "Yanwen Peng", "Xi Zhang", "Yingxu Wang", "Xinhao Yi", "Guibin Zhang", "Yi Xu", "Bin Wu", "Siwei Liu", "Zihao Li", "Zhaochun Ren", "Nikos Aletras", "Xi Wang", "Han Zhou", "Zaiqiao Meng"], "title": "A Comprehensive Survey of Self-Evolving AI Agents: A New Paradigm Bridging Foundation Models and Lifelong Agentic Systems", "comment": null, "summary": "Recent advances in large language models have sparked growing interest in AI\nagents capable of solving complex, real-world tasks. However, most existing\nagent systems rely on manually crafted configurations that remain static after\ndeployment, limiting their ability to adapt to dynamic and evolving\nenvironments. To this end, recent research has explored agent evolution\ntechniques that aim to automatically enhance agent systems based on interaction\ndata and environmental feedback. This emerging direction lays the foundation\nfor self-evolving AI agents, which bridge the static capabilities of foundation\nmodels with the continuous adaptability required by lifelong agentic systems.\nIn this survey, we provide a comprehensive review of existing techniques for\nself-evolving agentic systems. Specifically, we first introduce a unified\nconceptual framework that abstracts the feedback loop underlying the design of\nself-evolving agentic systems. The framework highlights four key components:\nSystem Inputs, Agent System, Environment, and Optimisers, serving as a\nfoundation for understanding and comparing different strategies. Based on this\nframework, we systematically review a wide range of self-evolving techniques\nthat target different components of the agent system. We also investigate\ndomain-specific evolution strategies developed for specialised fields such as\nbiomedicine, programming, and finance, where optimisation objectives are\ntightly coupled with domain constraints. In addition, we provide a dedicated\ndiscussion on the evaluation, safety, and ethical considerations for\nself-evolving agentic systems, which are critical to ensuring their\neffectiveness and reliability. This survey aims to provide researchers and\npractitioners with a systematic understanding of self-evolving AI agents,\nlaying the foundation for the development of more adaptive, autonomous, and\nlifelong agentic systems.", "AI": {"tldr": "\u672c\u8c03\u67e5\u5168\u9762\u56de\u987e\u4e86\u81ea\u6f14\u5316\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\uff0c\u91cd\u70b9\u4ecb\u7ecd\u4e86\u5b83\u4eec\u7684\u6982\u5ff5\u6846\u67b6\u3001\u6280\u672f\u3001\u7279\u5b9a\u9886\u57df\u7b56\u7565\u4ee5\u53ca\u5b89\u5168\u548c\u4f26\u7406\u65b9\u9762\u7684\u8003\u91cf\uff0c\u65e8\u5728\u4fc3\u8fdb\u66f4\u5177\u9002\u5e94\u6027\u548c\u81ea\u4e3b\u6027\u7684\u4ee3\u7406\u7cfb\u7edf\u7684\u53d1\u5c55\u3002", "motivation": "\u672c\u8c03\u67e5\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u4ee3\u7406\u7cfb\u7edf\u4f9d\u8d56\u4e8e\u624b\u52a8\u914d\u7f6e\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u914d\u7f6e\u5728\u90e8\u7f72\u540e\u4fdd\u6301\u9759\u6001\uff0c\u4ece\u800c\u9650\u5236\u4e86\u5b83\u4eec\u9002\u5e94\u52a8\u6001\u548c\u4e0d\u65ad\u53d8\u5316\u7684\u73af\u5883\u7684\u80fd\u529b\u3002\u56e0\u6b64\uff0c\u672c\u8c03\u67e5\u63a2\u8ba8\u4e86\u65e8\u5728\u57fa\u4e8e\u4ea4\u4e92\u6570\u636e\u548c\u73af\u5883\u53cd\u9988\u81ea\u52a8\u589e\u5f3a\u4ee3\u7406\u7cfb\u7edf\u7684\u4ee3\u7406\u6f14\u5316\u6280\u672f\u3002", "method": "\u672c\u8c03\u67e5\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u6982\u5ff5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u62bd\u8c61\u4e86\u81ea\u6f14\u5316\u4ee3\u7406\u7cfb\u7edf\u7684\u8bbe\u8ba1\u57fa\u7840\u3002\u8be5\u6846\u67b6\u7a81\u51fa\u4e86\u56db\u4e2a\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff1a\u7cfb\u7edf\u8f93\u5165\u3001\u4ee3\u7406\u7cfb\u7edf\u3001\u73af\u5883\u548c\u4f18\u5316\u5668\uff0c\u4e3a\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540c\u7684\u7b56\u7565\u5960\u5b9a\u4e86\u57fa\u7840\u3002\u57fa\u4e8e\u6b64\u6846\u67b6\uff0c\u6211\u4eec\u7cfb\u7edf\u5730\u56de\u987e\u4e86\u9488\u5bf9\u4ee3\u7406\u7cfb\u7edf\u4e0d\u540c\u7ec4\u6210\u90e8\u5206\u7684\u5e7f\u6cdb\u7684\u81ea\u6f14\u5316\u6280\u672f\u3002\u6211\u4eec\u8fd8\u7814\u7a76\u4e86\u4e3a\u751f\u7269\u533b\u5b66\u3001\u7f16\u7a0b\u548c\u91d1\u878d\u7b49\u4e13\u4e1a\u9886\u57df\u5f00\u53d1\u7684\u7279\u5b9a\u9886\u57df\u6f14\u5316\u7b56\u7565\uff0c\u5176\u4e2d\u4f18\u5316\u76ee\u6807\u4e0e\u9886\u57df\u7ea6\u675f\u7d27\u5bc6\u8026\u5408\u3002\u6b64\u5916\uff0c\u6211\u4eec\u4e13\u95e8\u8ba8\u8bba\u4e86\u81ea\u6f14\u5316\u4ee3\u7406\u7cfb\u7edf\u7684\u8bc4\u4f30\u3001\u5b89\u5168\u548c\u4f26\u7406\u8003\u91cf\uff0c\u8fd9\u5bf9\u4e8e\u786e\u4fdd\u5176\u6709\u6548\u6027\u548c\u53ef\u9760\u6027\u81f3\u5173\u91cd\u8981\u3002", "result": "\u672c\u8c03\u67e5\u5bf9\u81ea\u6f14\u5316\u4ee3\u7406\u7cfb\u7edf\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\uff0c\u5305\u62ec\u5176\u6982\u5ff5\u6846\u67b6\u3001\u5404\u79cd\u6280\u672f\u3001\u7279\u5b9a\u9886\u57df\u7b56\u7565\u4ee5\u53ca\u8bc4\u4f30\u3001\u5b89\u5168\u548c\u4f26\u7406\u65b9\u9762\u7684\u8003\u91cf\u3002", "conclusion": "\u672c\u8c03\u67e5\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u4ece\u4e1a\u8005\u63d0\u4f9b\u4e86\u5bf9\u81ea\u6f14\u5316\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u7684\u7cfb\u7edf\u6027\u7406\u89e3\uff0c\u4e3a\u5f00\u53d1\u66f4\u5177\u9002\u5e94\u6027\u3001\u81ea\u4e3b\u6027\u548c\u7ec8\u8eab\u6027\u7684\u4ee3\u7406\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07061", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.07061", "abs": "https://arxiv.org/abs/2508.07061", "authors": ["Yanhui Chen", "Hong-Yan Lu", "Wenjin Yang", "Meifeng Liu", "Bin Cui", "Desheng Liu", "Bing Huang", "Xi Zuo"], "title": "Giant spin Hall effects and topological surface states in ternary-layered MAX carbides Mn+1AlCn (M= Nb, Ta, n=1, 2, 3)", "comment": null, "summary": "In this work, we report a systematic study of the electronic structures, band\ntopology, and intrinsic spin Hall effect (SHE) of the layered MAX carbides\nMn+1AlCn (M= Nb, Ta, n=1, 2, 3) and explore the correlation effects on the SHE.\nThe results show that M3AlC2 and M4AlC3 (M= Nb, Ta) share similar\nDirac-band-crossing features near the Fermi level (EF) and form nodal lines in\nthe absence of spin-orbit coupling (SOC). When the SOC is included, the Dirac\nband crossings are fully gapped, resulting in nontrivial Z2 topological\ninvariants (1;000) with a pair of surface states on the (001) plane.\nRemarkably, the multiple gapped Dirac points contribute to locally strong spin\nBerry curvatures, which lead to large spin Hall conductivities and a giant spin\nHall angle up to ~ 60% for Ta3AlC2. Moreover, we also elucidate the impact of\nHubbard U correction on SHC. Our findings indicate that Ta3AlC2 might represent\nan intriguing layered Z2 topological metal with superior charge-to-spin\nconversion efficiency.", "AI": {"tldr": "MAX\u78b3\u5316\u7269Mn+1AlCn\uff08M=Nb\uff0cTa\uff0cn=1, 2, 3\uff09\u7684\u7cfb\u7edf\u7814\u7a76\u8868\u660e\uff0cTa3AlC2\u662f\u4e00\u79cd\u5177\u6709\u9ad8\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u7684Z2\u62d3\u6251\u91d1\u5c5e\u3002", "motivation": "\u63a2\u7d22MAX\u78b3\u5316\u7269\u5728\u7535\u5b50\u7ed3\u6784\u3001\u80fd\u5e26\u62d3\u6251\u548c\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u65b9\u9762\u7684\u6027\u8d28\uff0c\u7279\u522b\u662f\u5173\u8054\u6548\u5e94\u5bf9SHE\u7684\u5f71\u54cd\u3002", "method": "\u5bf9MAX\u78b3\u5316\u7269Mn+1AlCn\uff08M=Nb\uff0cTa\uff0cn=1, 2, 3\uff09\u7684\u7535\u5b50\u7ed3\u6784\u3001\u80fd\u5e26\u62d3\u6251\u548c\u672c\u5f81\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff08SHE\uff09\u8fdb\u884c\u4e86\u7cfb\u7edf\u7814\u7a76\uff0c\u5e76\u63a2\u8ba8\u4e86\u5173\u8054\u6548\u5e94\u5bf9SHE\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u53d1\u73b0M3AlC2\u548cM4AlC3\uff08M=Nb\uff0cTa\uff09\u5728\u8d39\u7c73\u80fd\u7ea7\u9644\u8fd1\u5177\u6709\u76f8\u4f3c\u7684\u72c4\u62c9\u514b\u80fd\u5e26\u4ea4\u53c9\u7279\u5f81\uff0c\u5728\u6ca1\u6709\u81ea\u65cb-\u8f68\u9053\u8026\u5408\uff08SOC\uff09\u7684\u60c5\u51b5\u4e0b\u5f62\u6210\u8282\u70b9\u7ebf\u3002\u5f53\u8003\u8651SOC\u65f6\uff0c\u72c4\u62c9\u514b\u80fd\u5e26\u4ea4\u53c9\u88ab\u5b8c\u5168\u6253\u5f00\uff0c\u4ea7\u751f\u975e\u5e73\u51e1\u7684Z2\u62d3\u6251\u4e0d\u53d8\u91cf\uff081;000\uff09\u4ee5\u53ca\u5728\uff08001\uff09\u5e73\u9762\u4e0a\u7684\u4e00\u5bf9\u8868\u9762\u6001\u3002\u591a\u4e2a\u72c4\u62c9\u514b\u70b9\u7684\u5b58\u5728\u5bfc\u81f4\u4e86\u5c40\u90e8\u5f3a\u81ea\u65cb\u8d1d\u91cc\u66f2\u7387\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u5927\u7684\u81ea\u65cb\u970d\u5c14\u7535\u5bfc\u7387\uff0cTa3AlC2\u7684\u81ea\u65cb\u970d\u5c14\u89d2\u9ad8\u8fbe\u7ea660%\u3002\u6b64\u5916\uff0c\u8fd8\u9610\u660e\u4e86Hubbard U\u6821\u6b63\u5bf9SHC\u7684\u5f71\u54cd\u3002", "conclusion": "Ta3AlC2 \u53ef\u80fd\u662f\u4e00\u79cd\u5177\u6709\u4f18\u8d8a\u7684\u7535\u8377-\u81ea\u65cb\u8f6c\u6362\u6548\u7387\u7684 Z2 \u62d3\u6251\u91d1\u5c5e\u3002"}}
{"id": "2508.08022", "categories": ["cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08022", "abs": "https://arxiv.org/abs/2508.08022", "authors": ["Roopkatha Banerjee", "Sampath Koti", "Gyanendra Singh", "Anirban Chakraborty", "Gurunath Gurrala", "Bhushan Jagyasi", "Yogesh Simmhan"], "title": "Optimizing Federated Learning for Scalable Power-demand Forecasting in Microgrids", "comment": null, "summary": "Real-time monitoring of power consumption in cities and micro-grids through\nthe Internet of Things (IoT) can help forecast future demand and optimize grid\noperations. But moving all consumer-level usage data to the cloud for\npredictions and analysis at fine time scales can expose activity patterns.\nFederated Learning~(FL) is a privacy-sensitive collaborative DNN training\napproach that retains data on edge devices, trains the models on private data\nlocally, and aggregates the local models in the cloud. But key challenges\nexist: (i) clients can have non-independently identically distributed~(non-IID)\ndata, and (ii) the learning should be computationally cheap while scaling to\n1000s of (unseen) clients. In this paper, we develop and evaluate several\noptimizations to FL training across edge and cloud for time-series demand\nforecasting in micro-grids and city-scale utilities using DNNs to achieve a\nhigh prediction accuracy while minimizing the training cost. We showcase the\nbenefit of using exponentially weighted loss while training and show that it\nfurther improves the prediction of the final model. Finally, we evaluate these\nstrategies by validating over 1000s of clients for three states in the US from\nthe OpenEIA corpus, and performing FL both in a pseudo-distributed setting and\na Pi edge cluster. The results highlight the benefits of the proposed methods\nover baselines like ARIMA and DNNs trained for individual consumers, which are\nnot scalable.", "AI": {"tldr": "\u901a\u8fc7\u4f18\u5316\u8054\u90a6\u5b66\u4e60\uff0c\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u540c\u65f6\uff0c\u9ad8\u6548\u51c6\u786e\u5730\u9884\u6d4b\u7535\u529b\u9700\u6c42\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u66b4\u9732\u7528\u6237\u6d3b\u52a8\u6a21\u5f0f\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u7269\u8054\u7f51\uff08IoT\uff09\u5bf9\u57ce\u5e02\u548c\u5fae\u7535\u7f51\u7684\u7535\u529b\u6d88\u8017\u8fdb\u884c\u5b9e\u65f6\u76d1\u63a7\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u9700\u6c42\u5e76\u4f18\u5316\u7535\u7f51\u8fd0\u8425\uff0c\u4f46\u73b0\u6709\u7684FL\u65b9\u6cd5\u9762\u4e34\u6570\u636e\u975eIID\u548c\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u6311\u6218\u3002", "method": "\u672c\u6587\u5f00\u53d1\u5e76\u8bc4\u4f30\u4e86\u591a\u79cd\u4f18\u5316\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4f7f\u7528\u6307\u6570\u52a0\u6743\u635f\u5931\uff0c\u4ee5\u5728\u5fae\u7535\u7f51\u548c\u57ce\u5e02\u89c4\u6a21\u7684\u516c\u7528\u4e8b\u4e1a\u4e2d\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u9700\u6c42\u9884\u6d4b\u3002", "result": "\u901a\u8fc7\u5728\u4e09\u4e2a\u7f8e\u56fd\u5dde\u7684OpenEIA\u8bed\u6599\u5e93\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5e76\u5728\u4f2a\u5206\u5e03\u5f0f\u8bbe\u7f6e\u548cPi\u8fb9\u7f18\u96c6\u7fa4\u4e2d\u6267\u884cFL\uff0c\u7ed3\u679c\u8868\u660e\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u6bd4ARIMA\u548c\u9488\u5bf9\u4e2a\u4f53\u6d88\u8d39\u8005\u7684DNN\u6a21\u578b\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5728\u8fb9\u7f18\u548c\u4e91\u7aef\u8fdb\u884c\u4f18\u5316\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u8bad\u7ec3\uff0c\u5728\u5fae\u7535\u7f51\u548c\u57ce\u5e02\u89c4\u6a21\u7684\u516c\u7528\u4e8b\u4e1a\u4e2d\u5b9e\u73b0\u4e86\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u548c\u4f4e\u8bad\u7ec3\u6210\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u975eIID\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u7684\u6311\u6218\uff0c\u5e76\u4f18\u4e8eARIMA\u548c\u4ec5\u9488\u5bf9\u4e2a\u4f53\u6d88\u8d39\u8005\u7684DNN\u6a21\u578b\u3002"}}
{"id": "2508.06804", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06804", "abs": "https://arxiv.org/abs/2508.06804", "authors": ["Shu-Ang Yu", "Feng Gao", "Yi Wu", "Chao Yu", "Yu Wang"], "title": "D3P: Dynamic Denoising Diffusion Policy via Reinforcement Learning", "comment": null, "summary": "Diffusion policies excel at learning complex action distributions for robotic\nvisuomotor tasks, yet their iterative denoising process poses a major\nbottleneck for real-time deployment. Existing acceleration methods apply a\nfixed number of denoising steps per action, implicitly treating all actions as\nequally important. However, our experiments reveal that robotic tasks often\ncontain a mix of \\emph{crucial} and \\emph{routine} actions, which differ in\ntheir impact on task success. Motivated by this finding, we propose\n\\textbf{D}ynamic \\textbf{D}enoising \\textbf{D}iffusion \\textbf{P}olicy\n\\textbf{(D3P)}, a diffusion-based policy that adaptively allocates denoising\nsteps across actions at test time. D3P uses a lightweight, state-aware adaptor\nto allocate the optimal number of denoising steps for each action. We jointly\noptimize the adaptor and base diffusion policy via reinforcement learning to\nbalance task performance and inference efficiency. On simulated tasks, D3P\nachieves an averaged 2.2$\\times$ inference speed-up over baselines without\ndegrading success. Furthermore, we demonstrate D3P's effectiveness on a\nphysical robot, achieving a 1.9$\\times$ acceleration over the baseline.", "AI": {"tldr": "D3P\u662f\u4e00\u79cd\u52a8\u6001\u53bb\u566a\u6269\u6563\u7b56\u7565\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u5206\u914d\u53bb\u566a\u6b65\u6570\u6765\u52a0\u901f\u673a\u5668\u4eba\u89c6\u89c9\u4efb\u52a1\u7684\u6267\u884c\u3002", "motivation": "\u673a\u5668\u4eba\u4efb\u52a1\u901a\u5e38\u5305\u542b\u5173\u952e\u52a8\u4f5c\u548c\u5e38\u89c4\u52a8\u4f5c\uff0c\u5b83\u4eec\u5bf9\u4efb\u52a1\u6210\u529f\u7684\u5f71\u54cd\u4e0d\u540c\uff0c\u73b0\u6709\u7684\u52a0\u901f\u65b9\u6cd5\u5ffd\u7565\u4e86\u8fd9\u4e00\u70b9\u3002", "method": "D3P\u4f7f\u7528\u8f7b\u91cf\u7ea7\u7684\u3001\u72b6\u6001\u611f\u77e5\u7684\u9002\u914d\u5668\u6765\u4e3a\u6bcf\u4e2a\u52a8\u4f5c\u5206\u914d\u6700\u4f18\u7684\u53bb\u566a\u6b65\u6570\uff0c\u5e76\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316\u9002\u914d\u5668\u548c\u57fa\u7840\u6269\u6563\u7b56\u7565\u3002", "result": "D3P\u5728\u6a21\u62df\u4efb\u52a1\u548c\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5747\u5b9e\u73b0\u4e86\u663e\u8457\u7684\u63a8\u7406\u52a0\u901f\uff0c\u5e73\u5747\u52a0\u901f\u6bd4\u5206\u522b\u4e3a2.2\u500d\u548c1.9\u500d\uff0c\u4e14\u4e0d\u5f71\u54cd\u4efb\u52a1\u6210\u529f\u7387\u3002", "conclusion": "D3P\u5728\u7269\u7406\u673a\u5668\u4eba\u4e0a\u5b9e\u73b0\u4e861.9\u500d\u7684\u52a0\u901f\uff0c\u5728\u6a21\u62df\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e862.2\u500d\u7684\u52a0\u901f\uff0c\u540c\u65f6\u6ca1\u6709\u964d\u4f4e\u6210\u529f\u7387\u3002"}}
{"id": "2508.07026", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07026", "abs": "https://arxiv.org/abs/2508.07026", "authors": ["Yi Pan", "Hanqi Jiang", "Junhao Chen", "Yiwei Li", "Huaqin Zhao", "Lin Zhao", "Yohannes Abate", "Yingfeng Wang", "Tianming Liu"], "title": "Bridging Classical and Quantum Computing for Next-Generation Language Models", "comment": null, "summary": "Integrating Large Language Models (LLMs) with quantum computing is a critical\nchallenge, hindered by the severe constraints of Noisy Intermediate-Scale\nQuantum (NISQ) devices, including barren plateaus and limited coherence.\nCurrent approaches often fail due to static quantum-classical partitioning. We\nintroduce Adaptive Quantum-Classical Fusion (AQCF), the first framework to\nbridge this gap through dynamic, quantum-classical co-design. AQCF's core\nprinciple is real-time adaptation: it analyzes input complexity to orchestrate\nseamless transitions between classical and quantum processing. The framework\nfeatures three key innovations: (1) entropy-driven adaptive circuits that\ncircumvent barren plateaus; (2) quantum memory banks that unify classical\nattention with quantum state-based similarity retrieval; and (3) intelligent\nfusion controllers that allocate tasks for optimal performance. This\narchitecture maintains full compatibility with classical Transformers while\nprogressively incorporating quantum advantages. Experiments on sentiment\nanalysis demonstrate that AQCF achieves competitive performance, significantly\nimproves quantum resource efficiency, and operates successfully within typical\nNISQ constraints. By providing a seamless integration pathway, AQCF offers both\nimmediate practical value on current quantum hardware and a clear evolution\npath toward mature Quantum LLMs.", "AI": {"tldr": "AQCF\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u548c\u534f\u540c\u8bbe\u8ba1\uff0c\u6210\u529f\u5730\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e0e\u91cf\u5b50\u8ba1\u7b97\u5728NISQ\u8bbe\u5907\u4e0a\u76f8\u7ed3\u5408\uff0c\u63d0\u9ad8\u4e86\u6548\u7387\u5e76\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4e0e\u91cf\u5b50\u8ba1\u7b97\u96c6\u6210\u65f6\uff0c\u7531\u4e8eNISQ\u8bbe\u5907\u7684\u9650\u5236\uff08\u5982\u5df4\u6797\u7279\u548c\u6709\u9650\u76f8\u5e72\u6027\uff09\u4ee5\u53ca\u9759\u6001\u91cf\u5b50-\u7ecf\u5178\u5212\u5206\u5bfc\u81f4\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u81ea\u9002\u5e94\u91cf\u5b50-\u7ecf\u5178\u878d\u5408\uff08AQCF\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5b9e\u65f6\u5206\u6790\u8f93\u5165\u590d\u6742\u6027\uff0c\u52a8\u6001\u5730\u5728\u7ecf\u5178\u548c\u91cf\u5b50\u5904\u7406\u4e4b\u95f4\u8fdb\u884c\u8f6c\u6362\u3002\u5176\u6838\u5fc3\u521b\u65b0\u5305\u62ec\uff1a1. \u71b5\u9a71\u52a8\u81ea\u9002\u5e94\u7535\u8def\u4ee5\u89c4\u907f\u5df4\u6797\u7279\uff1b2. \u91cf\u5b50\u8bb0\u5fc6\u5e93\u4ee5\u7edf\u4e00\u7ecf\u5178\u6ce8\u610f\u529b\u4e0e\u91cf\u5b50\u72b6\u6001\u76f8\u4f3c\u6027\u68c0\u7d22\uff1b3. \u667a\u80fd\u878d\u5408\u63a7\u5236\u5668\u4ee5\u4f18\u5316\u4efb\u52a1\u5206\u914d\u3002", "result": "\u5728\u60c5\u611f\u5206\u6790\u5b9e\u9a8c\u4e2d\uff0cAQCF\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u91cf\u5b50\u8d44\u6e90\u6548\u7387\uff0c\u5e76\u6210\u529f\u5728\u5178\u578b\u7684NISQ\u7ea6\u675f\u4e0b\u8fd0\u884c\u3002", "conclusion": "AQCF\u901a\u8fc7\u52a8\u6001\u3001\u91cf\u5b50-\u7ecf\u5178\u534f\u540c\u8bbe\u8ba1\uff0c\u5b9e\u73b0\u4e86LLM\u4e0e\u91cf\u5b50\u8ba1\u7b97\u7684\u878d\u5408\uff0c\u5728NISQ\u8bbe\u5907\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u6210\u679c\uff0c\u4e3a\u6210\u719f\u7684\u91cf\u5b50LLM\u63d0\u4f9b\u4e86\u6f14\u8fdb\u8def\u5f84\u3002"}}
{"id": "2508.06755", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06755", "abs": "https://arxiv.org/abs/2508.06755", "authors": ["Xianjun Yang", "Liqiang Xiao", "Shiyang Li", "Faisal Ladhak", "Hyokun Yun", "Linda Ruth Petzold", "Yi Xu", "William Yang Wang"], "title": "Many-Turn Jailbreaking", "comment": null, "summary": "Current jailbreaking work on large language models (LLMs) aims to elicit\nunsafe outputs from given prompts. However, it only focuses on single-turn\njailbreaking targeting one specific query. On the contrary, the advanced LLMs\nare designed to handle extremely long contexts and can thus conduct multi-turn\nconversations. So, we propose exploring multi-turn jailbreaking, in which the\njailbroken LLMs are continuously tested on more than the first-turn\nconversation or a single target query. This is an even more serious threat\nbecause 1) it is common for users to continue asking relevant follow-up\nquestions to clarify certain jailbroken details, and 2) it is also possible\nthat the initial round of jailbreaking causes the LLMs to respond to additional\nirrelevant questions consistently. As the first step (First draft done at June\n2024) in exploring multi-turn jailbreaking, we construct a Multi-Turn Jailbreak\nBenchmark (MTJ-Bench) for benchmarking this setting on a series of open- and\nclosed-source models and provide novel insights into this new safety threat. By\nrevealing this new vulnerability, we aim to call for community efforts to build\nsafer LLMs and pave the way for a more in-depth understanding of jailbreaking\nLLMs.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06553", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06553", "abs": "https://arxiv.org/abs/2508.06553", "authors": ["Jiahao Xiao", "Jianbo Zhang", "BoWen Yan", "Shengyu Guo", "Tongrui Ye", "Kaiwei Zhang", "Zicheng Zhang", "Xiaohong Liu", "Zhengxue Cheng", "Lei Fan", "Chuyi Li", "Guangtao Zhai"], "title": "Static and Plugged: Make Embodied Evaluation Simple", "comment": null, "summary": "Embodied intelligence is advancing rapidly, driving the need for efficient\nevaluation. Current benchmarks typically rely on interactive simulated\nenvironments or real-world setups, which are costly, fragmented, and hard to\nscale. To address this, we introduce StaticEmbodiedBench, a plug-and-play\nbenchmark that enables unified evaluation using static scene representations.\nCovering 42 diverse scenarios and 8 core dimensions, it supports scalable and\ncomprehensive assessment through a simple interface. Furthermore, we evaluate\n19 Vision-Language Models (VLMs) and 11 Vision-Language-Action models (VLAs),\nestablishing the first unified static leaderboard for Embodied intelligence.\nMoreover, we release a subset of 200 samples from our benchmark to accelerate\nthe development of embodied intelligence.", "AI": {"tldr": "\u63d0\u51faStaticEmbodiedBench\uff0c\u4e00\u4e2a\u4f7f\u7528\u9759\u6001\u573a\u666f\u8868\u793a\u7684\u5177\u8eab\u667a\u80fd\u8bc4\u4f30\u57fa\u51c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u53d1\u5e03\u4e86\u9996\u4e2a\u5177\u8eab\u667a\u80fd\u9759\u6001\u6392\u884c\u699c\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u8bc4\u4f30\u5177\u8eab\u667a\u80fd\u7684\u65b9\u6cd5\uff08\u4ea4\u4e92\u5f0f\u6a21\u62df\u73af\u5883\u6216\u771f\u5b9e\u4e16\u754c\u8bbe\u7f6e\uff09\u6210\u672c\u9ad8\u3001\u788e\u7247\u5316\u4e14\u96be\u4ee5\u6269\u5c55\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u9759\u6001\u573a\u666f\u8868\u793a\uff0c\u63d0\u4f9b\u4e00\u4e2a\u7edf\u4e00\u7684\u8bc4\u4f30\u754c\u9762\uff0c\u6db5\u76d642\u4e2a\u591a\u6837\u5316\u573a\u666f\u548c8\u4e2a\u6838\u5fc3\u7ef4\u5ea6\uff0c\u5e76\u8bc4\u4f30\u4e8619\u4e2a\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u548c11\u4e2a\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLAs\uff09\u3002", "result": "\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u5177\u8eab\u667a\u80fd\u9759\u6001\u6392\u884c\u699c\uff0c\u8bc4\u4f30\u4e8619\u4e2aVLMs\u548c11\u4e2aVLAs\uff0c\u5e76\u53d1\u5e03\u4e86200\u4e2a\u6837\u672c\u5b50\u96c6\u4ee5\u52a0\u901f\u5177\u8eab\u667a\u80fd\u7684\u53d1\u5c55\u3002", "conclusion": "StaticEmbodiedBench\u662f\u4e00\u4e2a\u5373\u63d2\u5373\u7528\u7684\u57fa\u51c6\uff0c\u5b83\u4f7f\u7528\u9759\u6001\u573a\u666f\u8868\u793a\u6765\u5b9e\u73b0\u7edf\u4e00\u8bc4\u4f30\uff0c\u4e3a\u5177\u8eab\u667a\u80fd\u7684\u8bc4\u4f30\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u5168\u9762\u7684\u65b9\u6cd5\uff0c\u5e76\u5efa\u7acb\u4e86\u7b2c\u4e00\u4e2a\u5177\u8eab\u667a\u80fd\u9759\u6001\u6392\u884c\u699c\u3002"}}
{"id": "2508.07627", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07627", "abs": "https://arxiv.org/abs/2508.07627", "authors": ["H Chan"], "title": "Nonlinear Systems in Wireless Power Transfer Applications", "comment": null, "summary": "As a novel pattern of energization, the wireless power transfer (WPT) offers\na brand-new way to the energy acquisition for electric-driven devices, thus\nalleviating the over-dependence on the battery. This report presents three\ntypes of WPT systems that use nonlinear control methods, in order to acquire an\nin-depth understanding of the course of Nonlinear Systems.", "AI": {"tldr": "The paper explores three wireless power transfer systems with nonlinear control to understand nonlinear systems and reduce battery dependence.", "motivation": "To provide a new energy acquisition method for electric-driven devices, reducing battery dependency, and to deepen the understanding of nonlinear systems.", "method": "The paper discusses three types of wireless power transfer (WPT) systems that utilize nonlinear control methods.", "result": "The study aims to provide an in-depth understanding of nonlinear systems through the analysis of WPT systems.", "conclusion": "This paper presents three types of WPT systems using nonlinear control methods to understand nonlinear systems."}}
{"id": "2508.07445", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07445", "abs": "https://arxiv.org/abs/2508.07445", "authors": ["Xinyu Liu", "Haozhi Liao", "Guangyun Qi", "Hao Geng", "Li Sheng", "Dingyu Xing"], "title": "Unified Semiclassical Theory of Nonlinear Hall Effect:Bridging Ballistic and Diffusive Transport Regime", "comment": null, "summary": "The nonlinear Hall effect has attracted considerable attention and undergone\nextensive investigation in recent years. However, theoretical studies\naddressing size-dependent effects remain largely unexplored. In this work, we\nestablish a unified semiclassical framework based on the Boltzmann transport\nequation, incorporating generalized boundary conditions to bridge the ballistic\nand diffusive transport regimes. Our analysis reveals that the nonlinear Hall\neffect arises from the combined action of two distinct mechanisms: the Berry\ncurvature dipole and the Fermi-surface integral of Berry curvature.\nFurthermore, we investigate the Hall effect in topological crystalline\ninsulators (TCIs), elucidating that the size dependence originates from\ncompetition between the two transport mechanisms. By connecting the two\ndistinct regimes, our theoretical framework provides a comprehensive\nunderstanding of the nonlinear Hall effect in finite-sized systems, offering\nboth fundamental insights and a useful analytical tool for more size-dependent\ninvestigations.", "AI": {"tldr": "\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\u7406\u8bba\u7814\u7a76\uff0c\u8003\u8651\u5c3a\u5bf8\u6548\u5e94\uff0c\u63d0\u51fa\u7edf\u4e00\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u586b\u8865\u4e86\u5148\u524d\u7406\u8bba\u7814\u7a76\u4e2d\u5bf9\u5c3a\u5bf8\u4f9d\u8d56\u6027\u6548\u5e94\u7684\u4e0d\u8db3\uff0c\u65e8\u5728\u6df1\u5165\u7406\u89e3\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\u3002", "method": "\u57fa\u4e8e\u73bb\u5c14\u5179\u66fc\u8f93\u8fd0\u65b9\u7a0b\uff0c\u7ed3\u5408\u5e7f\u4e49\u8fb9\u754c\u6761\u4ef6\uff0c\u5efa\u7acb\u4e86\u7edf\u4e00\u7684\u534a\u7ecf\u5178\u7406\u8bba\u6846\u67b6\u3002", "result": "\u63ed\u793a\u4e86\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\u6e90\u4e8e\u8d1d\u91cc\u66f2\u7387\u5076\u6781\u548c\u8d1d\u91cc\u66f2\u7387\u7684\u8d39\u7c73\u9762\u79ef\u5206\uff0c\u5e76\u9610\u660e\u4e86\u5728\u62d3\u6251\u6676\u4f53\u7edd\u7f18\u4f53\u4e2d\u5c3a\u5bf8\u4f9d\u8d56\u6027\u662f\u8fd9\u4e24\u79cd\u8f93\u8fd0\u673a\u5236\u7ade\u4e89\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u7406\u8bba\u6846\u67b6\u7edf\u4e00\u4e86\u5f39\u9053\u548c\u6269\u6563\u8f93\u8fd0\uff0c\u4e3a\u7406\u89e3\u6709\u9650\u5c3a\u5bf8\u7cfb\u7edf\u4e2d\u7684\u975e\u7ebf\u6027\u970d\u5c14\u6548\u5e94\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89c6\u89d2\uff0c\u5e76\u4e3a\u8fdb\u4e00\u6b65\u7684\u5c3a\u5bf8\u4f9d\u8d56\u6027\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u5206\u6790\u5de5\u5177\u3002"}}
{"id": "2508.07265", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07265", "abs": "https://arxiv.org/abs/2508.07265", "authors": ["Bile Peng", "Vahid Jamali", "Eduard Jorswieck"], "title": "A Scalable Machine Learning Approach Enabled RIS Optimization with Implicit Channel Estimation", "comment": null, "summary": "The reconfigurable intelligent surface (RIS) is considered as a key enabler\nof the next-generation mobile radio systems. While attracting extensive\ninterest from academia and industry due to its passive nature and low cost,\nscalability of RIS elements and requirement for channel state information (CSI)\nare two major difficulties for the RIS to become a reality. In this work, we\nintroduce an unsupervised machine learning (ML) enabled optimization approach\nto configure the RIS. The dedicated neural network (NN) architecture RISnet is\ncombined with an implicit channel estimation method. The RISnet learns to map\nfrom received pilot signals to RIS configuration directly without explicit\nchannel estimation. Simulation results show that the proposed algorithm\noutperforms baselines significantly.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\uff08\u7279\u522b\u662fRISnet\u795e\u7ecf\u7f51\u7edc\uff09\u6765\u4f18\u5316RIS\u914d\u7f6e\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86RIS\u9762\u4e34\u7684\u53ef\u6269\u5c55\u6027\u548c\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\u83b7\u53d6\u7684\u96be\u9898\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "RIS\u88ab\u8ba4\u4e3a\u662f\u4e0b\u4e00\u4ee3\u79fb\u52a8\u65e0\u7ebf\u7535\u7cfb\u7edf\u7684\u5173\u952e\u6280\u672f\uff0c\u4f46RIS\u5143\u4ef6\u7684\u53ef\u6269\u5c55\u6027\u548c\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7684\u8981\u6c42\u662f\u5176\u5b9e\u73b0\u9762\u4e34\u7684\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\uff08ML\uff09\u9a71\u52a8\u7684\u4f18\u5316\u65b9\u6cd5\u6765\u914d\u7f6eRIS\uff0c\u7ed3\u5408\u4e86\u540d\u4e3aRISnet\u7684\u4e13\u7528\u795e\u7ecf\u7f51\u7edc\uff08NN\uff09\u67b6\u6784\u548c\u9690\u5f0f\u4fe1\u9053\u4f30\u8ba1\u65b9\u6cd5\u3002RISnet\u76f4\u63a5\u4ece\u63a5\u6536\u5230\u7684\u5bfc\u9891\u4fe1\u53f7\u6620\u5c04\u5230RIS\u914d\u7f6e\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u4fe1\u9053\u4f30\u8ba1\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6210\u529f\u5730\u4ece\u63a5\u6536\u5230\u7684\u5bfc\u9891\u4fe1\u53f7\u4e2d\u63a8\u65ad\u51faRIS\u914d\u7f6e\uff0c\u800c\u65e0\u9700\u663e\u5f0f\u4fe1\u9053\u4f30\u8ba1\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.06836", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06836", "abs": "https://arxiv.org/abs/2508.06836", "authors": ["Xutong Zhao", "Yaqi Xie"], "title": "Multi-level Advantage Credit Assignment for Cooperative Multi-Agent Reinforcement Learning", "comment": "Accepted at AISTATS 2025", "summary": "Cooperative multi-agent reinforcement learning (MARL) aims to coordinate\nmultiple agents to achieve a common goal. A key challenge in MARL is credit\nassignment, which involves assessing each agent's contribution to the shared\nreward. Given the diversity of tasks, agents may perform different types of\ncoordination, with rewards attributed to diverse and often overlapping agent\nsubsets. In this work, we formalize the credit assignment level as the number\nof agents cooperating to obtain a reward, and address scenarios with multiple\ncoexisting levels. We introduce a multi-level advantage formulation that\nperforms explicit counterfactual reasoning to infer credits across distinct\nlevels. Our method, Multi-level Advantage Credit Assignment (MACA), captures\nagent contributions at multiple levels by integrating advantage functions that\nreason about individual, joint, and correlated actions. Utilizing an\nattention-based framework, MACA identifies correlated agent relationships and\nconstructs multi-level advantages to guide policy learning. Comprehensive\nexperiments on challenging Starcraft v1\\&v2 tasks demonstrate MACA's superior\nperformance, underscoring its efficacy in complex credit assignment scenarios.", "AI": {"tldr": "MARL \u4fe1\u7528\u5206\u914d\u7684\u6311\u6218\u5728\u4e8e\uff0c\u4ee3\u7406\u53ef\u80fd\u6267\u884c\u4e0d\u540c\u7c7b\u578b\u7684\u534f\u8c03\uff0c\u5956\u52b1\u5f52\u56e0\u4e8e\u4e0d\u540c\u4e14\u901a\u5e38\u91cd\u53e0\u7684\u4ee3\u7406\u5b50\u96c6\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MACA \u7684\u591a\u5c42\u4f18\u52bf\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u7684\u53cd\u4e8b\u5b9e\u63a8\u7406\u6765\u63a8\u65ad\u8de8\u4e0d\u540c\u5c42\u7ea7\u7684\u4fe1\u7528\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6846\u67b6\u6765\u8bc6\u522b\u76f8\u5173\u4ee3\u7406\u5173\u7cfb\u5e76\u6784\u5efa\u591a\u5c42\u4f18\u52bf\u4ee5\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002\u5b9e\u9a8c\u8bc1\u660e MACA \u5728\u590d\u6742\u4fe1\u7528\u5206\u914d\u573a\u666f\u4e2d\u8868\u73b0\u4f18\u8d8a\u3002", "motivation": "MARL \u4e2d\u7684\u5173\u952e\u6311\u6218\u662f\u4fe1\u7528\u5206\u914d\uff0c\u5373\u8bc4\u4f30\u6bcf\u4e2a\u4ee3\u7406\u5bf9\u5171\u4eab\u5956\u52b1\u7684\u8d21\u732e\u3002\u9274\u4e8e\u4efb\u52a1\u7684\u591a\u6837\u6027\uff0c\u4ee3\u7406\u53ef\u80fd\u6267\u884c\u4e0d\u540c\u7c7b\u578b\u7684\u534f\u8c03\uff0c\u5956\u52b1\u5f52\u56e0\u4e8e\u4e0d\u540c\u4e14\u901a\u5e38\u91cd\u53e0\u7684\u4ee3\u7406\u5b50\u96c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u4fe1\u7528\u5206\u914d\u5c42\u7ea7\u591a\u6837\u6027\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a MACA\uff08Multi-level Advantage Credit Assignment\uff09\u7684\u591a\u5c42\u4f18\u52bf\u4fe1\u7528\u5206\u914d\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u4f18\u52bf\u51fd\u6570\u6765\u63a8\u65ad\u8de8\u4e0d\u540c\u5c42\u7ea7\u7684\u4fe1\u7528\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u6846\u67b6\u6765\u8bc6\u522b\u76f8\u5173\u4ee3\u7406\u5173\u7cfb\u5e76\u6784\u5efa\u591a\u5c42\u4f18\u52bf\u4ee5\u6307\u5bfc\u7b56\u7565\u5b66\u4e60\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684 Starcraft v1 \u548c v2 \u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u7efc\u5408\u5b9e\u9a8c\u8bc1\u660e\u4e86 MACA \u7684\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "MACA \u5728\u590d\u6742\u4fe1\u7528\u5206\u914d\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.07073", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07073", "abs": "https://arxiv.org/abs/2508.07073", "authors": ["Jos\u00e9 \u00c1ngel Castellanos-Reyes", "Anders Bergman", "Ivan P. Miranda", "J\u00e1n Rusz"], "title": "Electron Energy Loss Spectra Simulations of Coupled Phonon and Magnon Excitations", "comment": null, "summary": "We simulate momentum-resolved electron energy loss spectra (EELS) in\nbody-centered cubic iron at 300 K, capturing the effects of coupled phonon and\nmagnon excitations within a unified dynamical formalism. By extending the Time\nAutocorrelation of Auxiliary Wavefunctions (TACAW) method to incorporate\natomistic spin-lattice dynamics (ASLD), we simulate the full EELS signal -\nincluding interaction effects, dynamical diffraction, and multiple scattering.\nOur results reveal non-additive spectral features arising from phonon-magnon\ncoupling, including interference and energy redistribution effects, and predict\nexperimental detectability of magnon signals under optimized detector\nconditions. This framework advances quantitative magnon spectroscopy in STEM,\nestablishing a direct link between dynamical theory and low-energy experimental\nEELS signatures.", "AI": {"tldr": "\u6211\u4eec\u901a\u8fc7\u7ed3\u5408ASLD\u548cTACAW\u65b9\u6cd5\u6a21\u62df\u4e86\u94c1\u7684EELS\u5149\u8c31\uff0c\u53d1\u73b0\u4e86\u58f0\u5b50-\u78c1\u632f\u5b50\u8026\u5408\u6548\u5e94\uff0c\u5e76\u9884\u6d4b\u4e86\u78c1\u632f\u5b50\u4fe1\u53f7\u7684\u53ef\u63a2\u6d4b\u6027\u3002", "motivation": "\u4e3a\u4e86\u5728\u4f53\u5fc3\u7acb\u65b9\u94c1\u4e2d\u6a21\u62df\u52a8\u91cf\u5206\u8fa8\u7535\u5b50\u80fd\u91cf\u635f\u5931\u8c31\uff08EELS\uff09\uff0c\u6355\u6349\u58f0\u5b50\u548c\u78c1\u632f\u5b50\u6fc0\u53d1\u8026\u5408\u7684\u6548\u5e94\u3002", "method": "\u901a\u8fc7\u6269\u5c55\u65f6\u95f4\u5173\u8054\u8f85\u52a9\u6ce2\u51fd\u6570\uff08TACAW\uff09\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u539f\u5b50\u5c3a\u5ea6\u81ea\u65cb-\u6676\u683c\u52a8\u529b\u5b66\uff08ASLD\uff09\uff0c\u6a21\u62df\u4e86\u5305\u62ec\u76f8\u4e92\u4f5c\u7528\u6548\u5e94\u3001\u52a8\u529b\u5b66\u884d\u5c04\u548c\u591a\u91cd\u6563\u5c04\u5728\u5185\u7684\u5b8c\u6574EELS\u4fe1\u53f7\u3002", "result": "\u7ed3\u679c\u63ed\u793a\u4e86\u7531\u58f0\u5b50-\u78c1\u632f\u5b50\u8026\u5408\u4ea7\u751f\u7684\u975e\u52a0\u6027\u5149\u8c31\u7279\u5f81\uff0c\u5305\u62ec\u5e72\u6d89\u548c\u80fd\u91cf\u518d\u5206\u914d\u6548\u5e94\uff0c\u5e76\u9884\u6d4b\u5728\u4f18\u5316\u7684\u63a2\u6d4b\u5668\u6761\u4ef6\u4e0b\uff0c\u5b9e\u9a8c\u4e0a\u53ef\u63a2\u6d4b\u5230\u78c1\u632f\u5b50\u4fe1\u53f7\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u52a8\u529b\u5b66\u7406\u8bba\u4e0e\u4f4e\u80fd\u5b9e\u9a8cEELS\u4fe1\u53f7\u76f4\u63a5\u8054\u7cfb\u8d77\u6765\uff0c\u63a8\u8fdb\u4e86STEM\u4e2d\u5b9a\u91cf\u78c1\u632f\u8c31\u5b66\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.08064", "categories": ["cs.DC"], "pdf": "https://arxiv.org/pdf/2508.08064", "abs": "https://arxiv.org/abs/2508.08064", "authors": ["Marco Bernardo", "Federico Calandra", "Andrea Esposito", "Francesco Fabris"], "title": "On the Operational Resilience of CBDC: Threats and Prospects of Formal Validation for Offline Payments", "comment": null, "summary": "Information and communication technologies are by now employed in most\nactivities, including economics and finance. Despite the extraordinary power of\nmodern computers and the vast amount of memory, some results of theoretical\ncomputer science imply the impossibility of certifying software quality in\ngeneral. With the exception of safety-critical systems, this has primarily\nconcerned the information processed by confined systems, with limited\nsocio-economic consequences. In the emerging era of technologies for exchanging\ndigital money and tokenized assets over the Internet - such as central bank\ndigital currencies (CBDCs) - even a minor bug could trigger a financial\ncollapse. Although the aforementioned impossibility results cannot be overcome\nin an absolute sense, there exist formal methods that can provide assertions of\ncomputing systems correctness. We advocate their use to validate the\noperational resilience of software infrastructures enabling CBDCs, with special\nemphasis on offline payments as they constitute a very critical issue.", "AI": {"tldr": "Formal methods are crucial for ensuring the reliability of CBDC software, particularly for offline payments, to prevent potential financial collapse due to bugs.", "motivation": "The motivation is the potential for even minor bugs in CBDC software to trigger financial collapse, as highlighted by the increasing role of ICT in finance and the impossibility results of theoretical computer science regarding software quality certification.", "method": "The paper discusses the use of formal methods to provide assertions of computing systems correctness.", "result": "The paper argues that while absolute software quality certification is impossible, formal methods can offer practical assurances for critical systems like CBDCs, especially for offline payment functionalities.", "conclusion": "The paper advocates for the use of formal methods to validate the operational resilience of software infrastructures enabling CBDCs, with a particular emphasis on offline payments."}}
{"id": "2508.06921", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06921", "abs": "https://arxiv.org/abs/2508.06921", "authors": ["Zhongyu Chen", "Chenyang Li", "Xuesong Li", "Dianye Huang", "Zhongliang Jiang", "Stefanie Speidel", "Xiangyu Chu", "K. W. Samuel Au"], "title": "Vibration-Based Energy Metric for Restoring Needle Alignment in Autonomous Robotic Ultrasound", "comment": null, "summary": "Precise needle alignment is essential for percutaneous needle insertion in\nrobotic ultrasound-guided procedures. However, inherent challenges such as\nspeckle noise, needle-like artifacts, and low image resolution make robust\nneedle detection difficult, particularly when visibility is reduced or lost. In\nthis paper, we propose a method to restore needle alignment when the ultrasound\nimaging plane and the needle insertion plane are misaligned. Unlike many\nexisting approaches that rely heavily on needle visibility in ultrasound\nimages, our method uses a more robust feature by periodically vibrating the\nneedle using a mechanical system. Specifically, we propose a vibration-based\nenergy metric that remains effective even when the needle is fully out of\nplane. Using this metric, we develop a control strategy to reposition the\nultrasound probe in response to misalignments between the imaging plane and the\nneedle insertion plane in both translation and rotation. Experiments conducted\non ex-vivo porcine tissue samples using a dual-arm robotic ultrasound-guided\nneedle insertion system demonstrate the effectiveness of the proposed approach.\nThe experimental results show the translational error of 0.41$\\pm$0.27 mm and\nthe rotational error of 0.51$\\pm$0.19 degrees.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u9488\u632f\u52a8\u6765\u6062\u590d\u673a\u5668\u4eba\u8d85\u58f0\u5f15\u5bfc\u9488\u63d2\u5165\u4e2d\u5bf9\u9f50\u7684\u65b9\u6cd5\uff0c\u5373\u4f7f\u5728\u9488\u4e0d\u53ef\u89c1\u65f6\u4e5f\u6709\u6548\u3002", "motivation": "\u7cbe\u786e\u7684\u9488\u5bf9\u9f50\u5bf9\u4e8e\u673a\u5668\u4eba\u8d85\u58f0\u5f15\u5bfc\u8fc7\u7a0b\u4e2d\u7684\u7ecf\u76ae\u9488\u63d2\u5165\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u6563\u6591\u566a\u58f0\u3001\u9488\u72b6\u4f2a\u5f71\u548c\u4f4e\u56fe\u50cf\u5206\u8fa8\u7387\u7b49\u56fa\u6709\u6311\u6218\u4f7f\u5f97\u9c81\u68d2\u7684\u9488\u68c0\u6d4b\u53d8\u5f97\u56f0\u96be\uff0c\u5c24\u5176\u662f\u5728\u53ef\u89c1\u6027\u964d\u4f4e\u6216\u4e22\u5931\u65f6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8d85\u58f0\u6210\u50cf\u5e73\u9762\u4e0e\u9488\u63d2\u5165\u5e73\u9762\u4e0d\u5bf9\u9f50\u65f6\u7684\u9488\u5bf9\u9f50\u6062\u590d\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u632f\u52a8\u7684\u80fd\u91cf\u5ea6\u91cf\uff0c\u8be5\u5ea6\u91cf\u901a\u8fc7\u673a\u68b0\u7cfb\u7edf\u5468\u671f\u6027\u5730\u632f\u52a8\u9488\u6765\u6062\u590d\u8d85\u58f0\u5f15\u5bfc\u7684\u9488\u63d2\u5165\u8fc7\u7a0b\u4e2d\u7684\u9488\u5bf9\u9f50\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f9d\u8d56\u4e8e\u8d85\u58f0\u56fe\u50cf\u4e2d\u7684\u9488\u53ef\u89c1\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5728\u732a\u7ec4\u7ec7\u6837\u672c\u4e0a\uff0c\u5e73\u79fb\u8bef\u5dee\u4e3a 0.41\u00b10.27 mm\uff0c\u65cb\u8f6c\u8bef\u5dee\u4e3a 0.51\u00b10.19 \u5ea6\u3002", "conclusion": "\u63d0\u51fa\u7684\u57fa\u4e8e\u632f\u52a8\u7684\u80fd\u91cf\u5ea6\u91cf\u5373\u4f7f\u5728\u9488\u5b8c\u5168\u8131\u79bb\u5e73\u9762\u65f6\u4e5f\u6709\u6548\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u63a7\u5236\u7b56\u7565\u6765\u91cd\u65b0\u5b9a\u4f4d\u8d85\u58f0\u63a2\u5934\u4ee5\u5e94\u5bf9\u6210\u50cf\u5e73\u9762\u548c\u9488\u63d2\u5165\u5e73\u9762\u4e4b\u95f4\u7684\u5e73\u79fb\u548c\u65cb\u8f6c\u4e0d\u5bf9\u9f50\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.07046", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2508.07046", "abs": "https://arxiv.org/abs/2508.07046", "authors": ["Mohamed Hatifi"], "title": "Geometry-Controlled Freezing and Revival of Bell Nonlocality through Environmental Memory", "comment": null, "summary": "We show that the distance between two qubits coupled to a structured\nreservoir acts as a single geometric control that can store, revive, or\nsuppress Bell nonlocality. In a mirror-terminated guide, quantum correlations\nlost to the bath return at discrete recurrence times, turning a product state\ninto a Bell-violating one without any entangling drive (only local basis\nrotations/readout). In the continuum limit, we derive closed-form criteria for\nthe emergence of nonlocality from backflow, and introduce a Bell-based analogue\nof the BLP measure to quantify this effect. We also show how subwavelength\ndisplacements away from a decoherence-free node quadratically reduce the\nlifetime of a dark state or bright state, enabling highly sensitive\ninterferometric detection. All results rely on analytically solvable models and\nare compatible with current superconducting and nanophotonic platforms,\noffering a practical route to passive, geometry-controlled non-Markovian\ndevices.", "AI": {"tldr": "Inter-qubit distance in structured reservoirs controls Bell nonlocality. Quantum correlations revive at recurrence times, creating nonlocality passively. Analytical methods quantify this effect and enable sensitive detection via geometry control, e.g., subwavelength displacements. Applicable to superconducting and nanophotonic platforms.", "motivation": "The motivation is to explore passive, geometry-controlled methods for manipulating and preserving quantum correlations, specifically Bell nonlocality, in the presence of decoherence. The research aims to provide practical routes for creating non-Markovian devices by leveraging the geometric properties of qubit-reservoir coupling and system architecture.", "method": "The paper employs analytical models to investigate the behavior of qubits coupled to a structured reservoir. It analyzes the role of inter-qubit distance as a geometric control for Bell nonlocality. Specific methods include examining quantum correlation dynamics in a mirror-terminated guide, deriving criteria for nonlocality from backflow in the continuum limit, and introducing a Bell-based analogue of the BLP measure. The study also explores the impact of subwavelength displacements on state lifetimes for interferometric detection.", "result": "Quantum correlations lost to a reservoir can return at discrete recurrence times in a mirror-terminated guide, reviving Bell nonlocality. In the continuum limit, closed-form criteria for nonlocality emergence from backflow are derived, along with a measure to quantify this effect. Subwavelength displacements near decoherence-free nodes can reduce state lifetimes, enabling sensitive interferometric detection. All results are analytically derived and compatible with current quantum computing platforms.", "conclusion": "The study demonstrates that the distance between qubits coupled to a structured reservoir acts as a geometric control for Bell nonlocality, enabling storage, revival, or suppression. Quantum correlations can be recovered at specific recurrence times in a mirror-terminated guide, generating nonlocality without external entangling drives. Analytical criteria for nonlocality emergence and a Bell-based measure for quantification are derived in the continuum limit. Subwavelength displacements near decoherence-free nodes offer sensitive interferometric detection by reducing state lifetimes. The findings, supported by analytically solvable models, are applicable to superconducting and nanophotonic platforms, paving the way for passive, geometry-controlled non-Markovian devices."}}
{"id": "2508.07684", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07684", "abs": "https://arxiv.org/abs/2508.07684", "authors": ["Jason J. Choi", "Claire J. Tomlin", "Shankar Sastry", "Koushil Sreenath"], "title": "When are safety filters safe? On minimum phase conditions of control barrier functions", "comment": "This work has been submitted to the IEEE for possible publication", "summary": "In emerging control applications involving multiple and complex tasks, safety\nfilters are gaining prominence as a modular approach to enforcing safety\nconstraints. Among various methods, control barrier functions (CBFs) are widely\nused for designing safety filters due to their simplicity, imposing a single\nlinear constraint on the control input at each state. In this work, we focus on\nthe internal dynamics of systems governed by CBF-constrained control laws. Our\nkey observation is that, although CBFs guarantee safety by enforcing state\nconstraints, they can inadvertently be \"unsafe\" by causing the internal state\nto diverge. We investigate the conditions under which the full system state,\nincluding the internal state, can remain bounded under a CBF-based safety\nfilter. Drawing inspiration from the input-output linearization literature,\nwhere boundedness is ensured by minimum phase conditions, we propose a new set\nof CBF minimum phase conditions tailored to the structure imposed by the CBF\nconstraint. A critical distinction from the original minimum phase conditions\nis that the internal dynamics in our setting is driven by a nonnegative virtual\ncontrol input, which reflects the enforcement of the safety constraint. We\ninclude a range of numerical examples, including single-input, multi-input,\nlinear, and nonlinear systems, validating both our analysis and the necessity\nof the proposed CBF minimum phase conditions.", "AI": {"tldr": "Safety filters (CBFs) can make system internals unsafe; new conditions ensure safety for all states.", "motivation": "Safety filters, particularly CBFs, are crucial for complex control applications but can lead to internal state divergence despite ensuring state constraints.", "method": "Proposing new CBF minimum phase conditions inspired by input-output linearization, tailored to the structure of CBF-constrained control laws, to ensure boundedness of internal dynamics driven by a nonnegative virtual control input.", "result": "Developed and validated new CBF minimum phase conditions that ensure the boundedness of the full system state under CBF-based safety filters across various system types (single/multi-input, linear/nonlinear).", "conclusion": "CBF-based safety filters can cause internal state divergence, necessitating new CBF minimum phase conditions to ensure boundedness of the full system state, including internal states. These conditions are validated with numerical examples."}}
{"id": "2508.07614", "categories": ["cond-mat.mes-hall", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07614", "abs": "https://arxiv.org/abs/2508.07614", "authors": ["Sapta Sindhu Paul Chowdhury", "Sourav Thapliyal", "Bheema Lingam Chittari", "Santosh Mogurampelly"], "title": "Tunable Interfacial Thermal Conductance in Graphene/Germanene van der Waals Heterostructure using an Optimized Interlayer Potential", "comment": "To appear in The Journal of Chemmical Physics", "summary": "Accurately modeling interfacial thermal transport in van der Waals\nheterostructures is challenging due to the limited availability of interlayer\ninteraction potentials. We develop a pairwise interlayer potential for\ngraphene/germanene van der Waals heterostructure using the binding energy\nobtained from ab-initio density functional theory calculations and use it to\ncalculate the interfacial thermal conductivity. Our calculations reveal that\nthe interfacial thermal conductivity shows superior tunability with external\nstrain. The phonon density of states calculations show a blueshift in the\nphonon spectra with an applied compressive strain in the direction of heat\nflow, increasing the interfacial thermal conductance to $\\sim$136% of the\nunstrained value. In contrast, a tensile strain is found to cause an opposite\neffect, reducing the conductance to $\\sim$70% of the unstrained value.\nMoreover, due to increased availability of phonons for heat transfer, both\ntemperature and interaction strength are found to correlate positively with the\ninterfacial thermal conductance for both directions of heat flow.", "AI": {"tldr": "\u901a\u8fc7\u5f00\u53d1\u65b0\u7684\u52bf\u80fd\u6a21\u578b\uff0c\u7814\u7a76\u4e86\u77f3\u58a8\u70ef/\u9517\u70f7\u5f02\u8d28\u7ed3\u6784\u4e2d\u7684\u754c\u9762\u70ed\u4f20\u8f93\uff0c\u5e76\u53d1\u73b0\u5e94\u53d8\u53ef\u4ee5\u6709\u6548\u8c03\u63a7\u70ed\u5bfc\u7387\u3002", "motivation": "\u51c6\u786e\u6a21\u62df\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u6784\u4e2d\u7684\u754c\u9762\u70ed\u4f20\u8f93\u5177\u6709\u6311\u6218\u6027\uff0c\u56e0\u4e3a\u5c42\u95f4\u76f8\u4e92\u4f5c\u7528\u52bf\u7684\u53ef\u7528\u6027\u6709\u9650\u3002", "method": "\u91c7\u7528\u4ece\u5934\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u5f97\u5230\u7684\u7ed3\u5408\u80fd\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u77f3\u58a8\u70ef/\u9517\u70f7\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u6784\u7684\u70b9\u95f4\u76f8\u4e92\u4f5c\u7528\u52bf\uff0c\u5e76\u5229\u7528\u8be5\u52bf\u8ba1\u7b97\u4e86\u754c\u9762\u70ed\u5bfc\u7387\u3002", "result": "\u8ba1\u7b97\u8868\u660e\uff0c\u754c\u9762\u70ed\u5bfc\u7387\u968f\u5916\u90e8\u5e94\u53d8\u5177\u6709\u4f18\u5f02\u7684\u53ef\u8c03\u6027\u3002\u58f0\u5b50\u6001\u5bc6\u5ea6\u8ba1\u7b97\u663e\u793a\uff0c\u5728\u6cbf\u70ed\u6d41\u65b9\u5411\u65bd\u52a0\u538b\u7f29\u5e94\u53d8\u65f6\uff0c\u58f0\u5b50\u5149\u8c31\u53d1\u751f\u84dd\u79fb\uff0c\u754c\u9762\u70ed\u5bfc\u589e\u52a0\u4e86\u7ea6 136%\u3002\u76f8\u53cd\uff0c\u62c9\u4f38\u5e94\u53d8\u5219\u4f7f\u70ed\u5bfc\u964d\u4f4e\u4e86\u7ea6 70%\u3002\u6b64\u5916\uff0c\u6e29\u5ea6\u548c\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u90fd\u4e0e\u754c\u9762\u70ed\u5bfc\u5448\u6b63\u76f8\u5173\u3002", "conclusion": "\u6211\u4eec\u5f00\u53d1\u4e86\u4e00\u79cd\u7528\u4e8e\u77f3\u58a8\u70ef/\u9517\u70f7\u8303\u5fb7\u534e\u5f02\u8d28\u7ed3\u6784\u7684\u70b9\u95f4\u76f8\u4e92\u4f5c\u7528\u52bf\uff0c\u5e76\u8ba1\u7b97\u4e86\u754c\u9762\u70ed\u5bfc\u7387\u3002\u7ed3\u679c\u8868\u660e\uff0c\u754c\u9762\u70ed\u5bfc\u7387\u53ef\u4ee5\u901a\u8fc7\u5916\u90e8\u5e94\u53d8\u8fdb\u884c\u8c03\u8282\uff0c\u538b\u7f29\u5e94\u53d8\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u754c\u9762\u70ed\u5bfc\uff0c\u800c\u62c9\u4f38\u5e94\u53d8\u5219\u4f1a\u964d\u4f4e\u754c\u9762\u70ed\u5bfc\u3002\u6b64\u5916\uff0c\u6e29\u5ea6\u548c\u76f8\u4e92\u4f5c\u7528\u5f3a\u5ea6\u4e5f\u4e0e\u754c\u9762\u70ed\u5bfc\u5448\u6b63\u76f8\u5173\u3002"}}
{"id": "2508.07305", "categories": ["eess.SP", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07305", "abs": "https://arxiv.org/abs/2508.07305", "authors": ["Mahdi Maleki", "Reza Agahzadeh Ayoubi", "Marouan Mizmizi", "Umberto Spagnolini"], "title": "Channel Charting in Smart Radio Environments", "comment": null, "summary": "This paper introduces the use of static electromagnetic skins (EMSs) to\nenable robust device localization via channel charting (CC) in realistic urban\nenvironments. We develop a rigorous optimization framework that leverages EMS\nto enhance channel dissimilarity and spatial fingerprinting, formulating EMS\nphase profile design as a codebook-based problem targeting the upper quantiles\nof key embedding metric, localization error, trustworthiness, and continuity.\nThrough 3D ray-traced simulations of a representative city scenario, we\ndemonstrate that optimized EMS configurations, in addition to significant\nimprovement of the average positioning error, reduce the 90th-percentile\nlocalization error from over 60 m (no EMS) to less than 25 m, while drastically\nimproving trustworthiness and continuity. To the best of our knowledge, this is\nthe first work to exploit Smart Radio Environment (SRE) with static EMS for\nenhancing CC, achieving substantial gains in localization performance under\nchallenging None-Line-of-Sight (NLoS) conditions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5229\u7528\u9759\u6001\u7535\u78c1\u76ae\u80a4\uff08EMS\uff09\u548c\u4fe1\u9053\u56fe\u8c31\uff08CC\uff09\u6280\u672f\uff0c\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u663e\u8457\u63d0\u9ad8\u8bbe\u5907\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u975e\u89c6\u8ddd\uff08NLoS\uff09\u6761\u4ef6\u4e0b\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57ce\u5e02\u73af\u5883\u4e2d\uff0c\u5229\u7528\u667a\u80fd\u65e0\u7ebf\u73af\u5883\uff08SRE\uff09\u548c\u9759\u6001\u7535\u78c1\u76ae\u80a4\uff08EMS\uff09\u6765\u63d0\u9ad8\u8bbe\u5907\u5b9a\u4f4d\u7684\u9c81\u68d2\u6027\u548c\u7cbe\u5ea6\uff0c\u7279\u522b\u662f\u5728\u975e\u89c6\u8ddd\uff08NLoS\uff09\u6761\u4ef6\u4e0b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u9759\u6001\u7535\u78c1\u76ae\u80a4\uff08EMS\uff09\u901a\u8fc7\u4fe1\u9053\u56fe\u8c31\uff08CC\uff09\u5728\u57ce\u5e02\u73af\u5883\u4e2d\u8fdb\u884c\u9c81\u68d2\u8bbe\u5907\u5b9a\u4f4d\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u4e25\u683c\u7684\u4f18\u5316\u6846\u67b6\uff0c\u5229\u7528EMS\u589e\u5f3a\u4fe1\u9053\u5dee\u5f02\u6027\u548c\u7a7a\u95f4\u6307\u7eb9\uff0c\u5c06EMS\u76f8\u4f4d\u5256\u9762\u8bbe\u8ba1\u95ee\u9898\u8f6c\u5316\u4e3a\u57fa\u4e8e\u7801\u672c\u7684\u95ee\u9898\uff0c\u4ee5\u4f18\u5316\u5173\u952e\u5d4c\u5165\u6307\u6807\uff08\u5b9a\u4f4d\u8bef\u5dee\u3001\u53ef\u4fe1\u5ea6\u3001\u8fde\u7eed\u6027\uff09\u7684\u4e0a\u5206\u4f4d\u6570\u3002", "result": "\u901a\u8fc7\u5bf9\u4ee3\u8868\u6027\u57ce\u5e02\u573a\u666f\u76843D\u5c04\u7ebf\u8ffd\u8e2a\u6a21\u62df\uff0c\u7814\u7a76\u8868\u660e\uff0c\u4f18\u5316\u7684EMS\u914d\u7f6e\u4e0d\u4ec5\u663e\u8457\u63d0\u9ad8\u4e86\u5e73\u5747\u5b9a\u4f4d\u8bef\u5dee\uff0c\u8fd8\u5c06\u5b9a\u4f4d\u8bef\u5dee\u768490th\u767e\u5206\u4f4d\u6570\u4ece\u8d85\u8fc760\u7c73\uff08\u65e0EMS\uff09\u964d\u4f4e\u5230\u4e0d\u523025\u7c73\uff0c\u540c\u65f6\u5927\u5e45\u63d0\u9ad8\u4e86\u53ef\u4fe1\u5ea6\u548c\u8fde\u7eed\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u5229\u7528\u5e26\u6709\u9759\u6001\u7535\u78c1\u76ae\u80a4\uff08EMS\uff09\u7684\u667a\u80fd\u65e0\u7ebf\u73af\u5883\uff08SRE\uff09\u589e\u5f3a\u4fe1\u9053\u56fe\u8c31\uff08CC\uff09\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u975e\u89c6\u8ddd\uff08NLoS\uff09\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4e86\u5b9a\u4f4d\u6027\u80fd\u7684\u5927\u5e45\u63d0\u5347\u3002"}}
{"id": "2508.06851", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.06851", "abs": "https://arxiv.org/abs/2508.06851", "authors": ["Pengfei Zhou", "Xiaopeng Peng", "Fanrui Zhang", "Zhaopan Xu", "Jiaxin Ai", "Yansheng Qiu", "Chuanhao Li", "Zhen Li", "Ming Li", "Yukang Feng", "Jianwen Sun", "Haoquan Zhang", "Zizhen Li", "Xiaofeng Mao", "Zekai Li", "Wangbo Zhao", "Kai Wang", "Xiaojun Chang", "Wenqi Shao", "Yang You", "Kaipeng Zhang"], "title": "MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams", "comment": "35 pages, 33 figures", "summary": "Multimodal large language models (MLLMs), which integrate language and visual\ncues for problem-solving, are crucial for advancing artificial general\nintelligence (AGI). However, current benchmarks for measuring the intelligence\nof MLLMs suffer from limited scale, narrow coverage, and unstructured\nknowledge, offering only static and undifferentiated evaluations. To bridge\nthis gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark\nbuilt from real-world K-12 exams spanning six disciplines with 141K instances\nand 6,225 knowledge points organized in a six-layer taxonomy. Covering five\nquestion formats with difficulty and year annotations, it enables comprehensive\nevaluation to capture the extent to which MLLMs perform over four dimensions:\n1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts,\nand 4) knowledge-driven reasoning. We propose a novel dynamic evaluation\nframework that introduces unfamiliar visual, textual, and question form shifts\nto challenge model generalization while improving benchmark objectivity and\nlongevity by mitigating data contamination. We further evaluate knowledge-point\nreference-augmented generation (KP-RAG) to examine the role of knowledge in\nproblem-solving. Key findings reveal limitations in current MLLMs in multiple\naspects and provide guidance for enhancing model robustness, interpretability,\nand AI-assisted education.", "AI": {"tldr": "MDK12-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u5927\u89c4\u6a21\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728K-12\u5b66\u79d1\u77e5\u8bc6\u4e0a\u7684\u80fd\u529b\u3002\u5b83\u901a\u8fc7\u6a21\u62df\u771f\u5b9e\u8003\u8bd5\u573a\u666f\u548c\u5f15\u5165\u52a8\u6001\u6311\u6218\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u7684\u4e0d\u8db3\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6a21\u578b\u6539\u8fdb\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u5f53\u524d\u7528\u4e8e\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u7684\u57fa\u51c6\u5728\u89c4\u6a21\u3001\u8986\u76d6\u8303\u56f4\u548c\u77e5\u8bc6\u7ed3\u6784\u5316\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u63d0\u4f9b\u52a8\u6001\u548c\u5dee\u5f02\u5316\u7684\u8bc4\u4f30\uff0c\u963b\u788d\u4e86\u901a\u7528\u4eba\u5de5\u667a\u80fd\uff08AGI\uff09\u7684\u53d1\u5c55\u3002\u9700\u8981\u4e00\u4e2a\u66f4\u5927\u89c4\u6a21\u3001\u66f4\u5168\u9762\u7684\u57fa\u51c6\u6765\u6355\u6349MLLMs\u5728\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u7684\u80fd\u529b\u3002", "method": "\u63d0\u51faMDK12-Bench\uff0c\u4e00\u4e2a\u5305\u542b141K\u5b9e\u4f8b\u548c6,225\u4e2a\u77e5\u8bc6\u70b9\u7684K-12\u8003\u8bd5\u57fa\u51c6\uff0c\u6db5\u76d6\u516d\u4e2a\u5b66\u79d1\u548c\u4e94\u79cd\u95ee\u9898\u683c\u5f0f\uff0c\u5e76\u5e26\u6709\u96be\u5ea6\u548c\u5e74\u4efd\u6807\u6ce8\u3002\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\uff0c\u5f15\u5165\u4e86\u4e0d\u719f\u6089\u7684\u53ef\u89c6\u5316\u3001\u6587\u672c\u548c\u95ee\u9898\u5f62\u5f0f\u7684\u8f6c\u53d8\uff0c\u4ee5\u6311\u6218\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u7f13\u89e3\u6570\u636e\u6c61\u67d3\u3002\u8bc4\u4f30\u4e86\u77e5\u8bc6\u70b9\u53c2\u8003\u589e\u5f3a\u751f\u6210\uff08KP-RAG\uff09\u4ee5\u68c0\u67e5\u77e5\u8bc6\u5728\u89e3\u51b3\u95ee\u9898\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u63ed\u793a\u4e86\u5f53\u524dMLLMs\u5728\u5904\u7406\u4e0d\u540c\u96be\u5ea6\u7ea7\u522b\u3001\u8de8\u5e74\u5ea6\u53d8\u5316\u3001\u60c5\u5883\u8f6c\u53d8\u548c\u77e5\u8bc6\u9a71\u52a8\u63a8\u7406\u65b9\u9762\u5b58\u5728\u591a\u65b9\u9762\u5c40\u9650\u6027\u3002\u7814\u7a76\u4e3a\u589e\u5f3a\u6a21\u578b\u7684\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53ca\u5728AI\u8f85\u52a9\u6559\u80b2\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "conclusion": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u7406\u89e3\u548c\u89e3\u51b3\u8de8\u5b66\u79d1\u95ee\u9898\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002MDK12-Bench\u57fa\u51c6\u901a\u8fc7\u5f15\u5165\u5927\u89c4\u6a21\u3001\u591a\u5b66\u79d1\u3001\u7ed3\u6784\u5316\u7684K-12\u8003\u8bd5\u6570\u636e\uff0c\u4e3a\u8bc4\u4f30MLLMs\u5728\u4e0d\u540c\u96be\u5ea6\u3001\u65f6\u95f4\u3001\u60c5\u5883\u548c\u77e5\u8bc6\u63a8\u7406\u80fd\u529b\u65b9\u9762\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002\u52a8\u6001\u8bc4\u4f30\u6846\u67b6\u548cKP-RAG\u7684\u5f15\u5165\u6709\u52a9\u4e8e\u6311\u6218\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u8bc4\u4f30\u77e5\u8bc6\u7684\u4f5c\u7528\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cMLLMs\u5728\u591a\u4e2a\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u5e76\u4e3a\u6539\u8fdb\u6a21\u578b\u9c81\u68d2\u6027\u3001\u53ef\u89e3\u91ca\u6027\u4ee5\u53caAI\u8f85\u52a9\u6559\u80b2\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.06647", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06647", "abs": "https://arxiv.org/abs/2508.06647", "authors": ["Andrey Sidorenko", "Paul Tiwald"], "title": "Privacy-Preserving Tabular Synthetic Data Generation Using TabularARGN", "comment": null, "summary": "Synthetic data generation has become essential for securely sharing and\nanalyzing sensitive data sets. Traditional anonymization techniques, however,\noften fail to adequately preserve privacy. We introduce the Tabular\nAuto-Regressive Generative Network (TabularARGN), a neural network architecture\nspecifically designed for generating high-quality synthetic tabular data. Using\na discretization-based auto-regressive approach, TabularARGN achieves high data\nfidelity while remaining computationally efficient. We evaluate TabularARGN\nagainst existing synthetic data generation methods, showing competitive results\nin statistical similarity, machine learning utility, and detection robustness.\nWe further perform an in-depth privacy evaluation using systematic\nmembership-inference attacks, highlighting the robustness and effective\nprivacy-utility balance of our approach.", "AI": {"tldr": "A new neural network, TabularARGN, generates high-quality synthetic tabular data with strong privacy guarantees, outperforming existing methods.", "motivation": "Synthetic data generation is crucial for secure data sharing and analysis, but traditional anonymization techniques often fall short in preserving privacy.", "method": "TabularARGN utilizes a discretization-based auto-regressive approach for generating synthetic tabular data.", "result": "TabularARGN generates high-fidelity synthetic tabular data efficiently and shows competitive performance against existing methods in various evaluations, including privacy assessments using membership-inference attacks.", "conclusion": "TabularARGN demonstrates robustness and an effective privacy-utility balance against systematic membership-inference attacks, achieving competitive results in statistical similarity, machine learning utility, and detection robustness compared to existing methods."}}
{"id": "2508.07671", "categories": ["cs.AI", "cs.CY", "cs.HC", "cs.MA", "stat.AP", "68T07, 68T42, 68T50, 91F20, 62P25", "I.2.11; I.2.1; H.1.2; J.4; K.4.2"], "pdf": "https://arxiv.org/pdf/2508.07671", "abs": "https://arxiv.org/abs/2508.07671", "authors": ["Mohamed Rayan Barhdadi", "Mehmet Tuncel", "Erchin Serpedin", "Hasan Kurban"], "title": "EMPATHIA: Multi-Faceted Human-AI Collaboration for Refugee Integration", "comment": "19 pages, 3 figures (plus 6 figures in supplementary), 2 tables, 1\n  algorithm. Submitted to NeurIPS 2025 Creative AI Track: Humanity", "summary": "Current AI approaches to refugee integration optimize narrow objectives such\nas employment and fail to capture the cultural, emotional, and ethical\ndimensions critical for long-term success. We introduce EMPATHIA (Enriched\nMultimodal Pathways for Agentic Thinking in Humanitarian Immigrant Assistance),\na multi-agent framework addressing the central Creative AI question: how do we\npreserve human dignity when machines participate in life-altering decisions?\nGrounded in Kegan's Constructive Developmental Theory, EMPATHIA decomposes\nintegration into three modules: SEED (Socio-cultural Entry and Embedding\nDecision) for initial placement, RISE (Rapid Integration and Self-sufficiency\nEngine) for early independence, and THRIVE (Transcultural Harmony and\nResilience through Integrated Values and Engagement) for sustained outcomes.\nSEED employs a selector-validator architecture with three specialized agents -\nemotional, cultural, and ethical - that deliberate transparently to produce\ninterpretable recommendations. Experiments on the UN Kakuma dataset (15,026\nindividuals, 7,960 eligible adults 15+ per ILO/UNHCR standards) and\nimplementation on 6,359 working-age refugees (15+) with 150+ socioeconomic\nvariables achieved 87.4% validation convergence and explainable assessments\nacross five host countries. EMPATHIA's weighted integration of cultural,\nemotional, and ethical factors balances competing value systems while\nsupporting practitioner-AI collaboration. By augmenting rather than replacing\nhuman expertise, EMPATHIA provides a generalizable framework for AI-driven\nallocation tasks where multiple values must be reconciled.", "AI": {"tldr": "EMPATHIA\u6846\u67b6\u901a\u8fc7\u6574\u5408\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u8003\u91cf\uff0c\u6539\u8fdb\u4e86AI\u5728\u96be\u6c11\u5b89\u7f6e\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u5728AI\u8f85\u52a9\u51b3\u7b56\u65f6\u7ef4\u62a4\u4eba\u7c7b\u5c0a\u4e25\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u7684\u6548\u679c\u3002", "motivation": "\u5f53\u524dAI\u5728\u96be\u6c11\u878d\u5408\u65b9\u9762\u7684\u5e94\u7528\u8fc7\u4e8e\u72ed\u9698\uff0c\u672a\u80fd\u5145\u5206\u8003\u8651\u5230\u5bf9\u957f\u671f\u6210\u529f\u81f3\u5173\u91cd\u8981\u7684\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u7b49\u7ef4\u5ea6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u673a\u5668\u53c2\u4e0e\u5f71\u54cd\u751f\u6d3b\u7684\u91cd\u8981\u51b3\u7b56\u65f6\u5982\u4f55\u4fdd\u6301\u4eba\u7c7b\u5c0a\u4e25\u7684\u6838\u5fc3\u521b\u610fAI\u95ee\u9898\u3002", "method": "EMPATHIA\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u57fa\u4e8eKegan\u7684\u5efa\u6784\u6027\u53d1\u5c55\u7406\u8bba\uff0c\u5c06\u6574\u5408\u8fc7\u7a0b\u5206\u4e3a\u4e09\u4e2a\u6a21\u5757\uff1aSEED\uff08\u793e\u4f1a\u6587\u5316\u5165\u95e8\u548c\u5d4c\u5165\u51b3\u7b56\uff09\u3001RISE\uff08\u5feb\u901f\u6574\u5408\u548c\u81ea\u7ed9\u81ea\u8db3\u5f15\u64ce\uff09\u548cTHRIVE\uff08\u8de8\u6587\u5316\u548c\u8c10\u4e0e\u901a\u8fc7\u6574\u5408\u4ef7\u503c\u89c2\u548c\u53c2\u4e0e\u5b9e\u73b0\u97e7\u6027\uff09\u3002SEED\u6a21\u5757\u91c7\u7528\u9009\u62e9\u5668-\u9a8c\u8bc1\u5668\u67b6\u6784\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u7684\u667a\u80fd\u4f53\uff08\u60c5\u611f\u3001\u6587\u5316\u548c\u4f26\u7406\uff09\uff0c\u80fd\u591f\u900f\u660e\u5730\u8fdb\u884c\u5ba1\u8bae\u4ee5\u4ea7\u751f\u53ef\u89e3\u91ca\u7684\u5efa\u8bae\u3002", "result": "\u5728\u8054\u5408\u56fdKakuma\u6570\u636e\u96c6\uff0815,026\u540d\u4e2a\u4f53\uff0c\u6839\u636e\u56fd\u9645\u52b3\u5de5\u7ec4\u7ec7/\u8054\u5408\u56fd\u96be\u6c11\u7f72\u6807\u51c6\uff0c7,960\u540d15\u5c81\u53ca\u4ee5\u4e0a\u5408\u683c\u6210\u5e74\u4eba\uff09\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u4ee5\u53ca\u5728150\u591a\u4e2a\u793e\u4f1a\u7ecf\u6d4e\u53d8\u91cf\u76846,359\u540d15\u5c81\u53ca\u4ee5\u4e0a\u52b3\u52a8\u5e74\u9f84\u96be\u6c11\u4e0a\u7684\u5b9e\u65bd\uff0c\u8fbe\u5230\u4e8687.4%\u7684\u9a8c\u8bc1\u6536\u655b\u7387\uff0c\u5e76\u5728\u4e94\u4e2a\u63a5\u6536\u56fd\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u7684\u8bc4\u4f30\u3002", "conclusion": "EMPATHIA\u662f\u4e00\u4e2a\u901a\u7528\u7684AI\u6846\u67b6\uff0c\u7528\u4e8e\u5904\u7406\u9700\u8981\u534f\u8c03\u591a\u4e2a\u4ef7\u503c\u7684\u5206\u914d\u4efb\u52a1\uff0c\u901a\u8fc7\u589e\u5f3a\u800c\u975e\u53d6\u4ee3\u4eba\u7c7b\u4e13\u4e1a\u77e5\u8bc6\uff0c\u5728\u6587\u5316\u3001\u60c5\u611f\u548c\u4f26\u7406\u56e0\u7d20\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u652f\u6301\u4ece\u4e1a\u8005\u4e0eAI\u7684\u534f\u4f5c\u3002"}}
{"id": "2508.07100", "categories": ["cond-mat.mtrl-sci", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2508.07100", "abs": "https://arxiv.org/abs/2508.07100", "authors": ["Chuliang Fu"], "title": "A Novel Computational Thermodynamics Framework with Intrinsic Chemical Short-Range Order", "comment": "PhD Dissertation, University of Virginia (2023.08); Official DOI:\n  10.18130/gfty-sr91", "summary": "Chemical short-range order (SRO) provides new opportunities for tuning alloy\nproperties, but conventional computational thermodynamics frameworks such as\nCALPHAD, based on Bragg-Williams mean-field approximations, cannot properly\ndescribe SRO or order-disorder transformations in multicomponent ($\\geq$3)\nalloys. First-principles approaches combined with the cluster variation method\n(CVM) or cluster expansion method (CEM) can capture SRO but suffer from high\ncomputational cost. Here we present a hybrid CVM-CALPHAD framework with a\nthermodynamic solid solution model named as FYL-CVM, enabled by the\nFowler-Yang-Li (FYL) transform to reduce the number of variables required in\nfree-energy minimization. This achieves efficient modeling of SRO in\nmulticomponent systems within the CALPHAD formalism. Benchmark tests on fcc AB\nbinaries show that FYL-CVM reproduces CVM phase diagrams with much higher\nefficiency, while non-configurational contributions from vibrational, elastic,\nand electronic terms are also incorporated to capture their physical effects on\norder-disorder boundaries. Applied to the Cu-Au system, this method produces\nphase diagrams with experimental data in an efficient parameterization and\nelucidates the temperature-composition dependence of SRO parameters via the SRO\ndiagram. Its applicability to ternary alloys is also demonstrated for the\nCu-Au-Ag system. Overall, this framework strikes a balance between accuracy and\nefficiency, extends CALPHAD to account for chemical SRO, and enables a\ncomprehensive physics-informed modeling of ordering phenomena. (This\ndissertation was submitted to the University of Virginia in 2023 as the\nauthor's doctoral research. For the original complete abstract, please refer to\nthe PDF version.)", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFYL-CVM\u7684\u6df7\u5408CVM-CALPHAD\u6846\u67b6\uff0c\u80fd\u591f\u9ad8\u6548\u4e14\u7cbe\u786e\u5730\u6a21\u62df\u591a\u7ec4\u5206\u5408\u91d1\u4e2d\u7684\u5316\u5b66\u77ed\u7a0b\u6709\u5e8f\uff08SRO\uff09\u73b0\u8c61\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfCALPHAD\u65b9\u6cd5\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff0SRO\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u7684CALPHAD\u6846\u67b6\u65e0\u6cd5\u51c6\u786e\u63cf\u8ff0\u591a\u7ec4\u5206\u5408\u91d1\u4e2d\u7684\u5316\u5b66\u77ed\u7a0b\u6709\u5e8f\uff08SRO\uff09\u6216\u6709\u5e8f-\u65e0\u5e8f\u8f6c\u53d8\uff0c\u800c\u7b2c\u4e00\u6027\u539f\u7406\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFYL-CVM\u7684\u6df7\u5408CVM-CALPHAD\u6846\u67b6\uff0c\u5229\u7528FYL\u53d8\u6362\u51cf\u5c11\u4e86\u81ea\u7531\u80fd\u6700\u5c0f\u5316\u6240\u9700\u7684\u53d8\u91cf\u6570\u91cf\uff0c\u5b9e\u73b0\u4e86SRO\u7684\u9ad8\u6548\u5efa\u6a21\u3002", "result": "FYL-CVM\u5728fcc AB\u4e8c\u5143\u5408\u91d1\u4e0a\u7684\u6d4b\u8bd5\u8868\u660e\uff0c\u5176\u5728\u66f4\u9ad8\u7684\u6548\u7387\u4e0b\u91cd\u73b0\u4e86CVM\u76f8\u56fe\uff0c\u5e76\u7ed3\u5408\u4e86\u975e\u6784\u578b\u8d21\u732e\u4ee5\u6355\u6349\u5176\u5bf9\u6709\u5e8f-\u65e0\u5e8f\u8fb9\u754c\u7684\u7269\u7406\u6548\u5e94\u3002\u5728Cu-Au\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u9ad8\u6548\u7684\u53c2\u6570\u5316\u4ea7\u751f\u4e86\u4e0e\u5b9e\u9a8c\u6570\u636e\u4e00\u81f4\u7684\u76f8\u56fe\uff0c\u5e76\u901a\u8fc7SRO\u56fe\u9610\u660e\u4e86SRO\u53c2\u6570\u7684\u6e29\u5ea6-\u6210\u5206\u4f9d\u8d56\u6027\u3002\u8be5\u65b9\u6cd5\u4e5f\u5c55\u793a\u4e86\u5176\u5728Cu-Au-Ag\u4e09\u5143\u5408\u91d1\u4e2d\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u8be5\u6df7\u5408CVM-CALPHAD\u6846\u67b6\u7ed3\u5408\u4e86FYL\u53d8\u6362\uff0c\u5728CALPHAD\u5f62\u5f0f\u4e3b\u4e49\u5185\u5b9e\u73b0\u4e86\u591a\u7ec4\u5206\u4f53\u7cfb\u4e2dSRO\u7684\u9ad8\u6548\u5efa\u6a21\uff0c\u5e73\u8861\u4e86\u7cbe\u5ea6\u548c\u6548\u7387\uff0c\u5e76\u5c06 Ordering \u73b0\u8c61\u6269\u5c55\u5230\u7269\u7406\u4fe1\u606f\u6a21\u578b\u3002"}}
{"id": "2508.06969", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06969", "abs": "https://arxiv.org/abs/2508.06969", "authors": ["Bingkun Huang", "Evgeniy Kotov", "Arkady Yuschenko"], "title": "Manipulator for people with limited abilities", "comment": "105 pages, in Russian language", "summary": "The topic of this final qualification work was chosen due to the importance\nof developing robotic systems designed to assist people with disabilities.\nAdvances in robotics and automation technologies have opened up new prospects\nfor creating devices that can significantly improve the quality of life for\nthese people. In this context, designing a robotic hand with a control system\nadapted to the needs of people with disabilities is a major scientific and\npractical challenge. This work addresses the problem of developing and\nmanufacturing a four-degree-of-freedom robotic hand suitable for practical\nmanipulation. Addressing this issue requires a comprehensive approach,\nencompassing the design of the hand's mechanical structure, the development of\nits control system, and its integration with a technical vision system and\nsoftware based on the Robot Operating System (ROS).", "AI": {"tldr": "This paper presents the development of a robotic hand with a control system, vision, and ROS software to help people with disabilities.", "motivation": "The motivation behind this work is the growing importance of developing robotic systems to assist people with disabilities, aiming to significantly improve their quality of life through advanced robotics and automation.", "method": "The paper details the process of designing a four-degree-of-freedom robotic hand, developing its control system, and integrating it with a technical vision system and software based on the Robot Operating System (ROS).", "result": "The work involves the development and manufacturing of a functional four-degree-of-freedom robotic hand with an integrated control system, vision system, and ROS software, tailored for practical manipulation and assisting people with disabilities.", "conclusion": "The paper focuses on the development and manufacturing of a four-degree-of-freedom robotic hand, along with its control system, integrated with a vision system and ROS software. This aims to improve the quality of life for people with disabilities through robotic assistance."}}
{"id": "2508.07104", "categories": ["quant-ph", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07104", "abs": "https://arxiv.org/abs/2508.07104", "authors": ["Yaswitha Gujju", "Romain Harang", "Chao Li", "Tetsuo Shibuya", "Qibin Zhao"], "title": "QuProFS: An Evolutionary Training-free Approach to Efficient Quantum Feature Map Search", "comment": null, "summary": "The quest for effective quantum feature maps for data encoding presents\nsignificant challenges, particularly due to the flat training landscapes and\nlengthy training processes associated with parameterised quantum circuits. To\naddress these issues, we propose an evolutionary training-free quantum\narchitecture search (QAS) framework that employs circuit-based heuristics\nfocused on trainability, hardware robustness, generalisation ability,\nexpressivity, complexity, and kernel-target alignment. By ranking circuit\narchitectures with various proxies, we reduce evaluation costs and incorporate\nhardware-aware circuits to enhance robustness against noise. We evaluate our\napproach on classification tasks (using quantum support vector machine) across\ndiverse datasets using both artificial and quantum-generated datasets. Our\napproach demonstrates competitive accuracy on both simulators and real quantum\nhardware, surpassing state-of-the-art QAS methods in terms of sampling\nefficiency and achieving up to a 2x speedup in architecture search runtime.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fdb\u5316\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u4f18\u5316\u53ef\u8bad\u7ec3\u6027\u3001\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u3001\u8868\u8fbe\u80fd\u529b\u3001\u590d\u6742\u6027\u548c\u6838\u76ee\u6807\u5bf9\u9f50\uff0c\u89e3\u51b3\u4e86\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u8bad\u7ec3\u7684\u6311\u6218\uff0c\u5e76\u5728\u5206\u7c7b\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u53c2\u6570\u5316\u91cf\u5b50\u7535\u8def\u5728\u6570\u636e\u7f16\u7801\u4e2d\u9762\u4e34\u7684\u5e73\u5766\u8bad\u7ec3\u666f\u89c2\u548c\u6f2b\u957f\u8bad\u7ec3\u8fc7\u7a0b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8fdb\u5316\u7684\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u91cf\u5b50\u67b6\u6784\u641c\u7d22\uff08QAS\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u7535\u8def\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u4e13\u6ce8\u4e8e\u53ef\u8bad\u7ec3\u6027\u3001\u786c\u4ef6\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u3001\u8868\u8fbe\u80fd\u529b\u3001\u590d\u6742\u6027\u548c\u6838\u76ee\u6807\u5bf9\u9f50\u3002", "result": "\u5728\u6a21\u62df\u5668\u548c\u771f\u5b9e\u91cf\u5b50\u786c\u4ef6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u5206\u7c7b\u4efb\u52a1\uff08\u4f7f\u7528\u91cf\u5b50\u652f\u6301\u5411\u91cf\u673a\uff09\u548c\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u51c6\u786e\u6027\uff0c\u5728\u91c7\u6837\u6548\u7387\u65b9\u9762\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684QAS\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u67b6\u6784\u641c\u7d22\u8fd0\u884c\u65f6\u95f4\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u52a0\u901f\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u5668\u548c\u771f\u5b9e\u91cf\u5b50\u786c\u4ef6\u4e0a\u90fd\u5c55\u73b0\u51fa\u5177\u6709\u7ade\u4e89\u529b\u7684\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u5728\u91c7\u6837\u6548\u7387\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684QAS\u65b9\u6cd5\uff0c\u540c\u65f6\u5728\u67b6\u6784\u641c\u7d22\u8fd0\u884c\u65f6\u95f4\u65b9\u9762\u5b9e\u73b0\u4e86\u9ad8\u8fbe2\u500d\u7684\u52a0\u901f\u3002"}}
{"id": "2508.06810", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06810", "abs": "https://arxiv.org/abs/2508.06810", "authors": ["Steven Coyne", "Diana Galvan-Sosa", "Ryan Spring", "Cam\u00e9lia Guerraoui", "Michael Zock", "Keisuke Sakaguchi", "Kentaro Inui"], "title": "Annotating Errors in English Learners' Written Language Production: Advancing Automated Written Feedback Systems", "comment": "Pre-review version of DOI 10.1007/978-3-031-98459-4_21, presented at\n  AIED 2025. All content is as of submission time except for de-anonymization,\n  ensuing layout fixes, use of the current code repository link, and BibTeX\n  fixes. Readers are encouraged to refer to the published version", "summary": "Recent advances in natural language processing (NLP) have contributed to the\ndevelopment of automated writing evaluation (AWE) systems that can correct\ngrammatical errors. However, while these systems are effective at improving\ntext, they are not optimally designed for language learning. They favor direct\nrevisions, often with a click-to-fix functionality that can be applied without\nconsidering the reason for the correction. Meanwhile, depending on the error\ntype, learners may benefit most from simple explanations and strategically\nindirect hints, especially on generalizable grammatical rules. To support the\ngeneration of such feedback, we introduce an annotation framework that models\neach error's error type and generalizability. For error type classification, we\nintroduce a typology focused on inferring learners' knowledge gaps by\nconnecting their errors to specific grammatical patterns. Following this\nframework, we collect a dataset of annotated learner errors and corresponding\nhuman-written feedback comments, each labeled as a direct correction or hint.\nWith this data, we evaluate keyword-guided, keyword-free, and template-guided\nmethods of generating feedback using large language models (LLMs). Human\nteachers examined each system's outputs, assessing them on grounds including\nrelevance, factuality, and comprehensibility. We report on the development of\nthe dataset and the comparative performance of the systems investigated.", "AI": {"tldr": "\u73b0\u6709\u7684\u81ea\u52a8\u5199\u4f5c\u8bc4\u4f30\uff08AWE\uff09\u7cfb\u7edf\u5728\u7ea0\u6b63\u8bed\u6cd5\u9519\u8bef\u65b9\u9762\u505a\u5f97\u5f88\u597d\uff0c\u4f46\u5bf9\u4e8e\u8bed\u8a00\u5b66\u4e60\u6765\u8bf4\u4e0d\u591f\u7406\u60f3\uff0c\u56e0\u4e3a\u5b83\u4eec\u503e\u5411\u4e8e\u76f4\u63a5\u4fee\u6539\uff0c\u800c\u5b66\u4e60\u8005\u53ef\u80fd\u4ece\u89e3\u91ca\u548c\u63d0\u793a\u4e2d\u83b7\u76ca\u66f4\u591a\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u9519\u8bef\u7c7b\u578b\u548c\u53ef\u63a8\u5e7f\u6027\u6807\u6ce8\u7684\u6846\u67b6\uff0c\u5e76\u6536\u96c6\u4e86\u76f8\u5173\u6570\u636e\u96c6\uff0c\u4ee5\u8bc4\u4f30\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u751f\u6210\u53cd\u9988\u7684\u51e0\u79cd\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684AWE\u7cfb\u7edf\u867d\u7136\u80fd\u7ea0\u6b63\u8bed\u6cd5\u9519\u8bef\uff0c\u4f46\u5e76\u975e\u6700\u4f18\u5316\u7684\u8bed\u8a00\u5b66\u4e60\u5de5\u5177\uff0c\u5b83\u4eec\u503e\u5411\u4e8e\u76f4\u63a5\u4fee\u6539\uff0c\u5ffd\u89c6\u4e86\u5b66\u4e60\u8005\u53ef\u80fd\u4ece\u95f4\u63a5\u63d0\u793a\u548c\u89e3\u91ca\u4e2d\u83b7\u76ca\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542b\u9519\u8bef\u7c7b\u578b\u548c\u53ef\u63a8\u5e7f\u6027\u6807\u6ce8\u7684\u6807\u6ce8\u6846\u67b6\uff0c\u5e76\u6536\u96c6\u6807\u6ce8\u7684\u5b66\u751f\u9519\u8bef\u53ca\u5bf9\u5e94\u7684\u4eba\u5de5\u53cd\u9988\u7684\u6570\u636e\u96c6\u3002\u5229\u7528LLM\u8bc4\u4f30\u4e86\u4e09\u79cd\u751f\u6210\u53cd\u9988\u7684\u65b9\u6cd5\uff08keyword-guided, keyword-free, and template-guided\uff09\u3002", "result": "LLM\u5728AWE\u7cfb\u7edf\u4e2d\u751f\u6210\u53cd\u9988\u7684\u6709\u6548\u6027\u5f97\u5230\u8bc4\u4f30\uff0c\u5e76\u5bf9\u4e0d\u540c\u751f\u6210\u65b9\u6cd5\u7684\u8868\u73b0\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "LLM\u5728AWE\u7cfb\u7edf\u4e2d\u7684\u8868\u73b0\uff0c\u4ee5\u53ca\u6570\u636e\u96c6\u7684\u5f00\u53d1\u548c\u5bf9\u6240\u7814\u7a76\u7cfb\u7edf\u7684\u6bd4\u8f83\u3002"}}
{"id": "2508.06556", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06556", "abs": "https://arxiv.org/abs/2508.06556", "authors": ["Sarina Penquitt", "Jonathan Klees", "Rinor Cakaj", "Daniel Kondermann", "Matthias Rottmann", "Lars Schmarje"], "title": "From Label Error Detection to Correction: A Modular Framework and Benchmark for Object Detection Datasets", "comment": null, "summary": "Object detection has advanced rapidly in recent years, driven by increasingly\nlarge and diverse datasets. However, label errors, defined as missing labels,\nincorrect classification or inaccurate localization, often compromise the\nquality of these datasets. This can have a significant impact on the outcomes\nof training and benchmark evaluations. Although several methods now exist for\ndetecting label errors in object detection datasets, they are typically\nvalidated only on synthetic benchmarks or limited manual inspection. How to\ncorrect such errors systemically and at scale therefore remains an open\nproblem. We introduce a semi-automated framework for label-error correction\ncalled REC$\\checkmark$D (Rechecked). Building on existing detectors, the\nframework pairs their error proposals with lightweight, crowd-sourced\nmicrotasks. These tasks enable multiple annotators to independently verify each\ncandidate bounding box, and their responses are aggregated to estimate\nambiguity and improve label quality. To demonstrate the effectiveness of\nREC$\\checkmark$D, we apply it to the class pedestrian in the KITTI dataset. Our\ncrowdsourced review yields high-quality corrected annotations, which indicate a\nrate of at least 24% of missing and inaccurate annotations in original\nannotations. This validated set will be released as a new real-world benchmark\nfor label error detection and correction. We show that current label error\ndetection methods, when combined with our correction framework, can recover\nhundreds of errors in the time it would take a human to annotate bounding boxes\nfrom scratch. However, even the best methods still miss up to 66% of the true\nerrors and with low quality labels introduce more errors than they find. This\nhighlights the urgent need for further research, now enabled by our released\nbenchmark.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86REC$\\\\checkmark$D\u6846\u67b6\uff0c\u901a\u8fc7\u4f17\u5305\u5fae\u4efb\u52a1\u5927\u89c4\u6a21\u81ea\u52a8\u6821\u6b63\u5bf9\u8c61\u68c0\u6d4b\u6570\u636e\u4e2d\u7684\u6807\u7b7e\u9519\u8bef\uff0c\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u4ee5\u63a8\u52a8\u8be5\u9886\u57df\u7814\u7a76\u3002", "motivation": "\u5bf9\u8c61\u68c0\u6d4b\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u9519\u8bef\uff08\u5982\u7f3a\u5931\u6807\u7b7e\u3001\u9519\u8bef\u5206\u7c7b\u6216\u4e0d\u7cbe\u786e\u7684\u5b9a\u4f4d\uff09\u4f1a\u5f71\u54cd\u6a21\u578b\u8bad\u7ec3\u548c\u8bc4\u4f30\u6548\u679c\u3002\u5c3d\u7ba1\u5b58\u5728\u4e00\u4e9b\u68c0\u6d4b\u6807\u7b7e\u9519\u8bef\u7684\u65b9\u6cd5\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u4ec5\u5728\u5408\u6210\u57fa\u51c6\u6216\u6709\u9650\u7684\u4eba\u5de5\u68c0\u67e5\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u7f3a\u4e4f\u7cfb\u7edf\u6027\u3001\u5927\u89c4\u6a21\u7684\u6821\u6b63\u624b\u6bb5\uff0c\u8fd9\u662f\u4e00\u4e2a\u4e9f\u5f85\u89e3\u51b3\u7684\u95ee\u9898\u3002", "method": "REC$\\\\checkmark$D\u6846\u67b6\u7ed3\u5408\u4e86\u73b0\u6709\u5bf9\u8c61\u68c0\u6d4b\u5668\u8fdb\u884c\u9519\u8bef\u5efa\u8bae\uff0c\u5e76\u5229\u7528\u8f7b\u91cf\u7ea7\u7684\u4f17\u5305\u5fae\u4efb\u52a1\u8ba9\u591a\u4e2a\u6807\u6ce8\u8005\u72ec\u7acb\u9a8c\u8bc1\u5019\u9009\u8fb9\u754c\u6846\uff0c\u901a\u8fc7\u805a\u5408\u4ed6\u4eec\u7684\u54cd\u5e94\u6765\u4f30\u8ba1\u6a21\u7cca\u6027\u5e76\u63d0\u9ad8\u6807\u7b7e\u8d28\u91cf\u3002", "result": "REC$\\\\checkmark$D\u6846\u67b6\u5728KITTI\u6570\u636e\u96c6\u7684pedestrian\u7c7b\u522b\u4e0a\u8fdb\u884c\u4e86\u5e94\u7528\uff0c\u4ea7\u751f\u7684\u4f17\u5305\u5ba1\u67e5\u7ed3\u679c\u663e\u793a\uff0c\u539f\u59cb\u6807\u6ce8\u4e2d\u81f3\u5c11\u670924%\u5b58\u5728\u7f3a\u5931\u6216\u4e0d\u51c6\u786e\u7684\u6807\u6ce8\u3002\u7814\u7a76\u8868\u660e\uff0c\u7ed3\u5408REC$\\\\checkmark$D\uff0c\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u53ef\u4ee5\u9ad8\u6548\u5730\u6062\u590d\u5927\u91cf\u9519\u8bef\u3002\u7136\u800c\uff0c\u5373\u4f7f\u662f\u6700\u4f73\u65b9\u6cd5\u4e5f\u53ef\u80fd\u9057\u6f0f\u9ad8\u8fbe66%\u7684\u771f\u5b9e\u9519\u8bef\uff0c\u5e76\u4e14\u4f4e\u8d28\u91cf\u6807\u7b7e\u53ef\u80fd\u5f15\u5165\u6bd4\u53d1\u73b0\u7684\u9519\u8bef\u66f4\u591a\u7684\u9519\u8bef\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aREC$\\checkmark$D\u7684\u534a\u81ea\u52a8\u5316\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u8c61\u68c0\u6d4b\u6570\u636e\u96c6\u4e2d\u7684\u6807\u7b7e\u9519\u8bef\u6821\u6b63\u3002\u901a\u8fc7\u7ed3\u5408\u73b0\u6709\u68c0\u6d4b\u5668\u548c\u4f17\u5305\u5fae\u4efb\u52a1\uff0cREC$\\\\checkmark$D\u80fd\u591f\u7cfb\u7edf\u6027\u5730\u3001\u5927\u89c4\u6a21\u5730\u9a8c\u8bc1\u548c\u7ea0\u6b63\u6807\u7b7e\u9519\u8bef\u3002\u5728KITTI\u6570\u636e\u96c6\u7684pedestrian\u7c7b\u522b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8bc6\u522b\u51fa\u81f3\u5c1124%\u7684\u9519\u8bef\u6807\u6ce8\uff0c\u5e76\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u6821\u6b63\u6570\u636e\u96c6\uff0c\u53ef\u4f5c\u4e3a\u65b0\u7684\u57fa\u51c6\u3002\u7814\u7a76\u8fd8\u6307\u51fa\uff0c\u73b0\u6709\u6807\u7b7e\u9519\u8bef\u68c0\u6d4b\u65b9\u6cd5\u5728\u4e0eREC$\\\\checkmark$D\u7ed3\u5408\u540e\u6548\u7387\u663e\u8457\u63d0\u5347\uff0c\u4f46\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u5e76\u5f3a\u8c03\u4e86\u65b0\u57fa\u51c6\u5bf9\u672a\u6765\u7814\u7a76\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.07693", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07693", "abs": "https://arxiv.org/abs/2508.07693", "authors": ["Noboru Katayama"], "title": "Deep Reinforcement Learning-Based Control Strategy with Direct Gate Control for Buck Converters", "comment": null, "summary": "This paper proposes a deep reinforcement learning (DRL)-based approach for\ndirectly controlling the gate signals of switching devices to achieve voltage\nregulation in a buck converter. Unlike conventional control methods, the\nproposed method directly generates gate signals using a neural network trained\nthrough DRL, with the objective of achieving high control speed and flexibility\nwhile maintaining stability. Simulation results demonstrate that the proposed\ndirect gate control (DGC) method achieves a faster transient response and\nstable output voltage regulation, outperforming traditional PWM-based control\nschemes. The DGC method also exhibits strong robustness against parameter\nvariations and sensor noise, indicating its suitability for practical power\nelectronics applications. The effectiveness of the proposed approach is\nvalidated via simulation.", "AI": {"tldr": "\u6240\u63d0\u51fa\u7684DGC\u65b9\u6cd5\u901a\u8fc7DRL\u76f4\u63a5\u63a7\u5236\u6805\u6781\u4fe1\u53f7\uff0c\u5728Buck\u8f6c\u6362\u5668\u4e2d\u5b9e\u73b0\u7535\u538b\u8c03\u8282\uff0c\u76f8\u6bd4\u4f20\u7edfPWM\u63a7\u5236\u5177\u6709\u66f4\u5feb\u7684\u77ac\u6001\u54cd\u5e94\u548c\u66f4\u597d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u65e8\u5728\u5b9e\u73b0\u9ad8\u63a7\u5236\u901f\u5ea6\u548c\u7075\u6d3b\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u7a33\u5b9a\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u7684\u65b9\u200b\u200b\u6cd5\uff0c\u901a\u8fc7DRL\u8bad\u7ec3\u7684\u795e\u7ecf\u7f51\u7edc\u76f4\u63a5\u751f\u6210\u6805\u6781\u4fe1\u53f7\uff0c\u4ee5\u5b9e\u73b0 Buck \u8f6c\u6362\u5668\u7684\u7535\u538b\u8c03\u8282\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684DGC\u65b9\u6cd5\u5b9e\u73b0\u4e86\u6bd4\u4f20\u7edf\u57fa\u4e8ePWM\u7684\u63a7\u5236\u65b9\u6848\u66f4\u5feb\u7684\u77ac\u6001\u54cd\u5e94\u548c\u7a33\u5b9a\u7684\u8f93\u51fa\u7535\u538b\u8c03\u8282\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u76f4\u63a5\u95e8\u63a7\uff08DGC\uff09\u65b9\u6cd5\u901a\u8fc7\u4eff\u771f\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u53c2\u6570\u53d8\u5316\u548c\u4f20\u611f\u5668\u566a\u58f0\u4e0b\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u8868\u660e\u5176\u9002\u7528\u4e8e\u5b9e\u9645\u7535\u529b\u7535\u5b50\u5e94\u7528\u3002"}}
{"id": "2508.07718", "categories": ["cond-mat.mes-hall", "gr-qc", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07718", "abs": "https://arxiv.org/abs/2508.07718", "authors": ["Yu. V. Shtanov", "T. -H. O. Pokalchuk", "S. G. Sharapov"], "title": "Sagnac and Mashhoon effects in graphene", "comment": "18 pages, 3 figures", "summary": "We investigate the Sagnac and Mashhoon effects in graphene, taking into\naccount both the pseudospin and intrinsic spin of electrons, within a\nsimplified model of a rotating nanotube or infinitesimally narrow ring. Based\non considerations of the relativistic phase of the wave function and employing\nthe effective Larmor theorem, we demonstrate that the Sagnac fringe shift\nretains a form analogous to that for free electrons, governed by the electron's\nvacuum mass. In the case of a narrow ring, an additional $\\pi$-phase shift\narises due to the Berry phase associated with the honeycomb graphene lattice.\nThe Mashhoon fringe shift, which characterizes the dynamics of intrinsic spin,\nretains its conventional form in graphene, with its dependence on the Fermi\nvelocity. Our analysis highlights both the similarities and differences between\nspin and pseudospin degrees of freedom in graphene.", "AI": {"tldr": "Graphene's spin and pseudospin effects analyzed in rotating systems: Sagnac shift similar to free electrons, Mashhoon shift dependent on Fermi velocity. Similarities and differences between spin and pseudospin are highlighted.", "motivation": "The paper investigates the Sagnac and Mashhoon effects in graphene, considering the roles of pseudospin and intrinsic spin of electrons to understand their behavior in a rotating system.", "method": "The study employs the effective Larmor theorem and considers the relativistic phase of the wave function to investigate the Sagnac and Mashhoon effects in graphene, accounting for both pseudospin and intrinsic spin of electrons within a simplified model of a rotating nanotube or infinitesimally narrow ring.", "result": "The Sagnac fringe shift in graphene is analogous to that for free electrons, governed by electron's vacuum mass. An additional $\\pi$-phase shift arises in a narrow ring due to Berry phase. The Mashhoon fringe shift retains its conventional form, dependent on Fermi velocity.", "conclusion": "The analysis reveals similarities and differences between spin and pseudospin degrees of freedom in graphene, with the Sagnac fringe shift analogous to free electrons and the Mashhoon fringe shift retaining its conventional form dependent on Fermi velocity."}}
{"id": "2508.07436", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07436", "abs": "https://arxiv.org/abs/2508.07436", "authors": ["Mehrbod Zarifi", "Mohamad Amin Jamshidi", "Zolfa Anvari", "Hamed Ghafarirad", "Mohammad Zareinejad"], "title": "Detection and Classification of Internal Leakage in Hydraulic Cylinders", "comment": "10 pages, 7 figures, presented at the 12th RSI International\n  Conference on Robotics and Mechatronics (ICRoM 2024), IEEE", "summary": "Hydraulic systems have been one of the most used technologies in many\nindustries due to their reliance on incompressible fluids that facilitate\nenergy and power transfer. Within such systems, hydraulic cylinders are prime\ndevices that convert hydraulic energy into mechanical energy. Some of the\ngenuine and very common problems related to hydraulic cylinders are leakages.\nLeakage in hydraulic systems can cause a drop in pressure, general\ninefficiency, and even complete failure of such systems. The various ways\nleakage can occur define the major categorization of leakage: internal and\nexternal leakage. External leakage is easily noticeable, while internal\nleakage, which involves fluid movement between pressure chambers, can be harder\nto detect and may gradually impact system performance without obvious signs.\nWhen leakage surpasses acceptable limits, it is classified as a fault or\nfailure. In such cases, leakage is divided into three categories: no leakage,\nlow leakage, and high leakage. It suggests a fault detection algorithm with the\nbasic responsibility of detecting minimum leakage within the Hydraulic system,\nand minimizing detection time is the core idea of this paper. In order to fully\ndevelop this idea, experimental data collection of Hydraulic systems is\nrequired. The collected data uses pressure sensors and other signals that are\nsingle-related. Due to the utilization of Long Short-Term Memory (LSTM)\nrecurrent neural networks, more complex data analysis was enabled, which the\nLSTM-based leakage detection algorithm successfully achieved, providing almost\n96% accuracy in classifying leakage types. Results demonstrate that the\nproposed method can perform real-time and online fault diagnosis for each\ncycle, reducing maintenance costs and prolonging the hydraulic system's\nlifespan.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLSTM\u795e\u7ecf\u7f51\u7edc\u7684\u6db2\u538b\u7cfb\u7edf\u6cc4\u6f0f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u9ad8\uff0c\u53ef\u5b9e\u65f6\u8bca\u65ad\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u6210\u672c\u548c\u5ef6\u957f\u8bbe\u5907\u5bff\u547d\u3002", "motivation": "\u6db2\u538b\u7cfb\u7edf\u4e2d\u7684\u6cc4\u6f0f\u95ee\u9898\uff08\u7279\u522b\u662f\u5185\u90e8\u6cc4\u6f0f\uff09\u96be\u4ee5\u68c0\u6d4b\uff0c\u4f46\u4f1a\u5f71\u54cd\u7cfb\u7edf\u6027\u80fd\u751a\u81f3\u5bfc\u81f4\u6545\u969c\u3002\u8be5\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u68c0\u6d4b\u6700\u5c0f\u6cc4\u6f0f\u5e76\u7f29\u77ed\u68c0\u6d4b\u65f6\u95f4\u7684\u6545\u969c\u68c0\u6d4b\u7b97\u6cd5\u3002", "method": "\u901a\u8fc7\u6536\u96c6\u6db2\u538b\u7cfb\u7edf\u8fd0\u884c\u65f6\u7684\u538b\u529b\u4f20\u611f\u5668\u7b49\u4fe1\u53f7\u6570\u636e\uff0c\u5229\u7528\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6570\u636e\u5206\u6790\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u6db2\u538b\u7cfb\u7edf\u5185\u90e8\u6cc4\u6f0f\u7684\u68c0\u6d4b\u3002", "result": "\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u5bf9\u6db2\u538b\u7cfb\u7edf\u6cc4\u6f0f\u7c7b\u578b\u7684\u5206\u7c7b\uff0c\u51c6\u786e\u7387\u63a5\u8fd196%\uff0c\u80fd\u591f\u8fdb\u884c\u5b9e\u65f6\u5728\u7ebf\u7684\u6bcf\u4e2a\u5468\u671f\u7684\u6545\u969c\u8bca\u65ad\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u57fa\u4e8eLSTM\u7684\u6cc4\u6f0f\u68c0\u6d4b\u7b97\u6cd5\u80fd\u591f\u5bf9\u6db2\u538b\u7cfb\u7edf\u8fdb\u884c\u5b9e\u65f6\u5728\u7ebf\u6545\u969c\u8bca\u65ad\uff0c\u51c6\u786e\u7387\u8fbe\u523096%\uff0c\u6709\u52a9\u4e8e\u964d\u4f4e\u7ef4\u62a4\u6210\u672c\u5e76\u5ef6\u957f\u6db2\u538b\u7cfb\u7edf\u5bff\u547d\u3002"}}
{"id": "2508.06859", "categories": ["cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06859", "abs": "https://arxiv.org/abs/2508.06859", "authors": ["Shuo Tang", "Jian Xu", "Jiadong Zhang", "Yi Chen", "Qizhao Jin", "Lingdong Shen", "Chenglin Liu", "Shiming Xiang"], "title": "MeteorPred: A Meteorological Multimodal Large Model and Dataset for Severe Weather Event Prediction", "comment": null, "summary": "Timely and accurate severe weather warnings are critical for disaster\nmitigation. However, current forecasting systems remain heavily reliant on\nmanual expert interpretation, introducing subjectivity and significant\noperational burdens. With the rapid development of AI technologies, the\nend-to-end \"AI weather station\" is gradually emerging as a new trend in\npredicting severe weather events. Three core challenges impede the development\nof end-to-end AI severe weather system: (1) scarcity of severe weather event\nsamples; (2) imperfect alignment between high-dimensional meteorological data\nand textual warnings; (3) existing multimodal language models are unable to\nhandle high-dimensional meteorological data and struggle to fully capture the\ncomplex dependencies across temporal sequences, vertical pressure levels, and\nspatial dimensions. To address these challenges, we introduce MP-Bench, the\nfirst large-scale temporal multimodal dataset for severe weather events\nprediction, comprising 421,363 pairs of raw multi-year meteorological data and\ncorresponding text caption, covering a wide range of severe weather scenarios\nacross China. On top of this dataset, we develop a meteorology multimodal large\nmodel (MMLM) that directly ingests 4D meteorological inputs. In addition, it is\ndesigned to accommodate the unique characteristics of 4D meteorological data\nflow, incorporating three plug-and-play adaptive fusion modules that enable\ndynamic feature extraction and integration across temporal sequences, vertical\npressure layers, and spatial dimensions. Extensive experiments on MP-Bench\ndemonstrate that MMLM performs exceptionally well across multiple tasks,\nhighlighting its effectiveness in severe weather understanding and marking a\nkey step toward realizing automated, AI-driven weather forecasting systems. Our\nsource code and dataset will be made publicly available.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86MP-Bench\u6570\u636e\u96c6\u548cMMLM\u6a21\u578b\uff0c\u4ee5\u89e3\u51b3AI\u6c14\u8c61\u9884\u62a5\u4e2d\u7684\u6570\u636e\u7a00\u758f\u3001\u6570\u636e\u4e0e\u6587\u672c\u5339\u914d\u4ee5\u53ca\u9ad8\u7ef4\u6570\u636e\u5904\u7406\u7b49\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u5f53\u524d\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u4f9d\u8d56\u4eba\u5de5\u89e3\u91ca\uff0c\u5b58\u5728\u4e3b\u89c2\u6027\u548c\u64cd\u4f5c\u8d1f\u62c5\u3002AI\u6c14\u8c61\u7ad9\u662f\u8d8b\u52bf\uff0c\u4f46\u9762\u4e34\u6570\u636e\u7a00\u758f\u3001\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u4e0e\u6587\u672c\u8b66\u544a\u4e0d\u5339\u914d\u3001\u73b0\u6709\u6a21\u578b\u65e0\u6cd5\u5904\u7406\u9ad8\u7ef4\u6c14\u8c61\u6570\u636e\u53ca\u590d\u6742\u4f9d\u8d56\u5173\u7cfb\u7b49\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86MP-Bench\uff0c\u4e00\u4e2a\u5305\u542b421,363\u4e2a\u6c14\u8c61\u6570\u636e\u548c\u5bf9\u5e94\u6587\u672c\u63cf\u8ff0\u7684\u6570\u636e\u96c6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86MMLM\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u76f4\u63a5\u5904\u74064D\u6c14\u8c61\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u5373\u63d2\u5373\u7528\u7684\u81ea\u9002\u5e94\u878d\u5408\u6a21\u5757\u6765\u5904\u7406\u65f6\u95f4\u5e8f\u5217\u3001\u5782\u76f4\u6c14\u538b\u5c42\u548c\u7a7a\u95f4\u7ef4\u5ea6\u7684\u7279\u5f81\u3002", "result": "MMLM\u6a21\u578b\u5728MP-Bench\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6076\u52a3\u5929\u6c14\u7684\u7406\u89e3\u80fd\u529b\u3002", "conclusion": "MMLM\u5728MP-Bench\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u5728\u591a\u4efb\u52a1\u4e0a\u7684\u5353\u8d8a\u8868\u73b0\uff0c\u5728\u6076\u52a3\u5929\u6c14\u7406\u89e3\u65b9\u9762\u6548\u679c\u663e\u8457\uff0c\u662f\u5b9e\u73b0\u81ea\u52a8\u5316\u3001\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u5929\u6c14\u9884\u62a5\u7cfb\u7edf\u7684\u5173\u952e\u4e00\u6b65\u3002\u4ee3\u7801\u548c\u6570\u636e\u96c6\u5c06\u516c\u5f00\u3002"}}
{"id": "2508.06659", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06659", "abs": "https://arxiv.org/abs/2508.06659", "authors": ["Fernando Martinez-Lopez", "Tao Li", "Yingdong Lu", "Juntao Chen"], "title": "In-Context Reinforcement Learning via Communicative World Models", "comment": null, "summary": "Reinforcement learning (RL) agents often struggle to generalize to new tasks\nand contexts without updating their parameters, mainly because their learned\nrepresentations and policies are overfit to the specifics of their training\nenvironments. To boost agents' in-context RL (ICRL) ability, this work\nformulates ICRL as a two-agent emergent communication problem and introduces\nCORAL (Communicative Representation for Adaptive RL), a framework that learns a\ntransferable communicative context by decoupling latent representation learning\nfrom control. In CORAL, an Information Agent (IA) is pre-trained as a world\nmodel on a diverse distribution of tasks. Its objective is not to maximize task\nreward, but to build a world model and distill its understanding into concise\nmessages. The emergent communication protocol is shaped by a novel Causal\nInfluence Loss, which measures the effect that the message has on the next\naction. During deployment, the previously trained IA serves as a fixed\ncontextualizer for a new Control Agent (CA), which learns to solve tasks by\ninterpreting the provided communicative context. Our experiments demonstrate\nthat this approach enables the CA to achieve significant gains in sample\nefficiency and successfully perform zero-shot adaptation with the help of\npre-trained IA in entirely unseen sparse-reward environments, validating the\nefficacy of learning a transferable communicative representation.", "AI": {"tldr": "CORAL\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u8868\u793a\u5b66\u4e60\u4e0e\u63a7\u5236\uff0c\u5229\u7528\u53cc\u667a\u80fd\u4f53\u901a\u4fe1\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u8868\u793a\uff0c\u63d0\u9ad8\u4e86RL\u667a\u80fd\u4f53\u7684\u96f6\u6837\u672c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u667a\u80fd\u4f53\u5728\u6ca1\u6709\u53c2\u6570\u66f4\u65b0\u7684\u60c5\u51b5\u4e0b\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u7684\u4efb\u52a1\u548c\u73af\u5883\uff0c\u56e0\u4e3a\u5b83\u4eec\u5b66\u4e60\u5230\u7684\u8868\u793a\u548c\u7b56\u7565\u5f80\u5f80\u8fc7\u62df\u5408\u4e8e\u8bad\u7ec3\u73af\u5883\u7684\u7279\u5b9a\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCORAL\u7684\u6846\u67b6\uff0c\u5c06ICRL\u95ee\u9898\u5f62\u5f0f\u5316\u4e3a\u53cc\u667a\u80fd\u4f53 the emergent communication\u95ee\u9898\u3002\u5176\u4e2d\uff0c\u4fe1\u606f\u667a\u80fd\u4f53\uff08IA\uff09\u4f5c\u4e3a\u4e16\u754c\u6a21\u578b\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5176\u76ee\u6807\u662f\u6784\u5efa\u4e16\u754c\u6a21\u578b\u5e76\u5c06\u7406\u89e3\u63d0\u70bc\u6210\u7b80\u6d01\u7684\u6d88\u606f\u3002\u65b0\u9896\u7684\u56e0\u679c\u5f71\u54cd\u635f\u5931\u7528\u4e8e\u6307\u5bfc\u901a\u4fe1\u534f\u8bae\u7684\u5b66\u4e60\u3002\u5728\u90e8\u7f72\u9636\u6bb5\uff0c\u9884\u8bad\u7ec3\u7684IA\u4f5c\u4e3a\u56fa\u5b9a\u7684\u60c5\u5883\u5316\u5668\uff0c\u63a7\u5236\u667a\u80fd\u4f53\uff08CA\uff09\u901a\u8fc7\u89e3\u91ca\u901a\u4fe1\u4e0a\u4e0b\u6587\u6765\u5b66\u4e60\u89e3\u51b3\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cCORAL\u6846\u67b6\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u667a\u80fd\u4f53\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u7684\u8868\u73b0\uff0c\u5e76\u5728\u5b8c\u5168\u672a\u89c1\u8fc7\u7684\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\uff0c\u501f\u52a9\u9884\u8bad\u7ec3\u7684IA\u5b9e\u73b0\u96f6\u6837\u672c\u9002\u5e94\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u89e3\u8026\u6f5c\u5728\u8868\u793a\u5b66\u4e60\u4e0e\u63a7\u5236\uff0c\u5b66\u4e60\u53ef\u8fc1\u79fb\u7684\u4ea4\u6d41\u4e0a\u4e0b\u6587\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u667a\u80fd\u4f53\u5728\u65b0\u7684\u4efb\u52a1\u548c\u73af\u5883\u4e2d\u7684\u9002\u5e94\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u8fc1\u79fb\u3002"}}
{"id": "2508.07187", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.07187", "abs": "https://arxiv.org/abs/2508.07187", "authors": ["Soyun Kim", "Jo Hyun Yun", "Takashi Taniguchi", "Kenji Watanabe", "Joseph Falson", "Jun Sung Kim", "Kyung-Hwan Jin", "Gil Young Cho", "Youngwook Kim"], "title": "Ferroelectric switching of interfacial dipoles in $\u03b1$-RuCl$_3$/graphene heterostructure", "comment": null, "summary": "We demonstrate electrically switchable, non-volatile dipoles in graphene/thin\nhBN/$\\alpha$-RuCl$_3$ heterostructures, stabilized purely by interfacial charge\ntransfer across an atomically thin dielectric barrier. This mechanism requires\nno sliding or twisting to explicitly break inversion symmetry and produces\nrobust ferroelectric-like hysteresis loops that emerge prominently near 30~K.\nSystematic measurements under strong in-plane and out-of-plane magnetic fields\nreveal negligible effects on the hysteresis characteristics, confirming that\nthe primary mechanism driving the dipole switching is electrostatic. Our\nfindings establish a distinct and robust route to electrically tunable\nferroelectric phenomena in van der Waals heterostructures, opening\nopportunities to explore the interplay between interfacial charge transfer and\ntemperature-tuned barrier crossing of dipole states at the atomic scale.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\u5728graphene/thin hBN/$\\\\%\\\\alpha$-RuCl3\u5f02\u8d28\u7ed3\u6784\u4e2d\uff0c\u53ef\u901a\u8fc7\u754c\u9762\u7535\u8377\u8f6c\u79fb\u5b9e\u73b0\u53ef\u7535\u5207\u6362\u7684\u975e\u6325\u53d1\u6027\u5076\u6781\u5b50\uff0c\u5e76\u89c2\u5bdf\u5230\u94c1\u7535\u7c7b\u78c1\u6ede\u56de\u7ebf\uff0c\u8be5\u73b0\u8c61\u4e3b\u8981\u7531\u9759\u7535\u9a71\u52a8\u3002", "motivation": "\u63a2\u7d22\u754c\u9762\u7535\u8377\u8f6c\u79fb\u4e0e\u6e29\u5ea6\u8c03\u8c10\u7684\u5076\u6781\u5b50\u72b6\u6001\u52bf\u5792\u7a7f\u8d8a\u5728\u539f\u5b50\u5c3a\u5ea6\u4e0a\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "method": "\u901a\u8fc7\u7cfb\u7edf\u6d4b\u91cf\u7814\u7a76\u4e86\u5f3a\u9762\u5185\u5916\u78c1\u573a\u5bf9\u78c1\u6ede\u56de\u7ebf\u7279\u6027\u7684\u5f71\u54cd\uff0c\u786e\u8ba4\u4e86\u9a71\u52a8\u5076\u6781\u5b50\u5f00\u5173\u7684\u4e3b\u8981\u673a\u5236\u662f\u9759\u7535\u7684\uff0c\u800c\u975e\u78c1\u573a\u7684\u3002\u53d1\u73b0\u4e86\u5728graphene/thin hBN/$\\\\%\\\\alpha$-RuCl3\u5f02\u8d28\u7ed3\u6784\u4e2d\uff0c\u901a\u8fc7\u754c\u9762\u7535\u8377\u8f6c\u79fb\u5b9e\u73b0\u53ef\u7535\u8c03\u8c10\u7684\u94c1\u7535\u73b0\u8c61\u3002", "result": "\u5728graphene/thin hBN/$\\\\%\\\\alpha$-RuCl3\u5f02\u8d28\u7ed3\u6784\u4e2d\u5b9e\u73b0\u4e86\u53ef\u901a\u8fc7\u7535\u5207\u6362\u3001\u975e\u6325\u53d1\u6027\u7684\u5076\u6781\u5b50\uff0c\u5e76\u89c2\u5bdf\u5230\u5f3a\u94c1\u7535\u7c7b\u78c1\u6ede\u56de\u7ebf\uff0c\u8bc1\u660e\u4e86\u8be5\u673a\u5236\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e3a\u7814\u7a76\u754c\u9762\u7535\u8377\u8f6c\u79fb\u548c\u6e29\u5ea6\u8c03\u8c10\u7684\u5076\u6781\u5b50\u72b6\u6001\u52bf\u5792\u7a7f\u8d8a\u63d0\u4f9b\u4e86\u65b0\u7684\u9014\u5f84\u3002", "conclusion": "\u8be5\u7814\u7a76\u5728graphene/thin hBN/$\\\\%\\\\alpha$-RuCl3\u5f02\u8d28\u7ed3\u6784\u4e2d\u5c55\u793a\u4e86\u53ef\u901a\u8fc7\u7535\u5207\u6362\u3001\u975e\u6325\u53d1\u6027\u7684\u5076\u6781\u5b50\uff0c\u8be5\u5076\u6781\u5b50\u4ec5\u901a\u8fc7\u539f\u5b50\u5c42\u7535\u4ecb\u8d28\u52bf\u5792\u4e2d\u7684\u754c\u9762\u7535\u8377\u8f6c\u79fb\u6765\u7a33\u5b9a\u3002\u6b64\u673a\u5236\u65e0\u9700\u6ed1\u52a8\u6216\u626d\u8f6c\u5373\u53ef\u660e\u786e\u6253\u7834\u53cd\u6f14\u5bf9\u79f0\u6027\uff0c\u5e76\u4ea7\u751f\u572830 K\u9644\u8fd1\u663e\u8457\u51fa\u73b0\u7684\u5f3a\u94c1\u7535\u7c7b\u78c1\u6ede\u56de\u7ebf\u3002"}}
{"id": "2508.06990", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.06990", "abs": "https://arxiv.org/abs/2508.06990", "authors": ["Yue Hu", "Junzhe Wu", "Ruihan Xu", "Hang Liu", "Avery Xi", "Henry X. Liu", "Ram Vasudevan", "Maani Ghaffari"], "title": "Imaginative World Modeling with Scene Graphs for Embodied Agent Navigation", "comment": "23 pages", "summary": "Semantic navigation requires an agent to navigate toward a specified target\nin an unseen environment. Employing an imaginative navigation strategy that\npredicts future scenes before taking action, can empower the agent to find\ntarget faster. Inspired by this idea, we propose SGImagineNav, a novel\nimaginative navigation framework that leverages symbolic world modeling to\nproactively build a global environmental representation. SGImagineNav maintains\nan evolving hierarchical scene graphs and uses large language models to predict\nand explore unseen parts of the environment. While existing methods solely\nrelying on past observations, this imaginative scene graph provides richer\nsemantic context, enabling the agent to proactively estimate target locations.\nBuilding upon this, SGImagineNav adopts an adaptive navigation strategy that\nexploits semantic shortcuts when promising and explores unknown areas otherwise\nto gather additional context. This strategy continuously expands the known\nenvironment and accumulates valuable semantic contexts, ultimately guiding the\nagent toward the target. SGImagineNav is evaluated in both real-world scenarios\nand simulation benchmarks. SGImagineNav consistently outperforms previous\nmethods, improving success rate to 65.4 and 66.8 on HM3D and HSSD, and\ndemonstrating cross-floor and cross-room navigation in real-world environments,\nunderscoring its effectiveness and generalizability.", "AI": {"tldr": "SGImagineNav\u662f\u4e00\u500b\u5275\u65b0\u7684\u60f3\u50cf\u5c0e\u822a\u6846\u67b6\uff0c\u901a\u904e\u7b26\u865f\u4e16\u754c\u5efa\u6a21\u548c\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u9810\u6e2c\u672a\u4f86\u5834\u666f\uff0c\u5be6\u73fe\u66f4\u5feb\u7684\u76ee\u6a19\u5c0e\u822a\u3002\u8a72\u65b9\u6cd5\u5728\u771f\u5be6\u4e16\u754c\u548c\u6a21\u64ec\u74b0\u5883\u4e2d\u5747\u8868\u73fe\u51fa\u8272\u3002", "motivation": "\u70ba\u4e86\u89e3\u6c7a\u7121\u9810\u6e2c\u5c0e\u822a\u7684\u4f4e\u6548\u7387\u554f\u984c\uff0c\u63d0\u51fa\u4e00\u7a2e\u80fd\u5920\u9810\u6e2c\u672a\u4f86\u5834\u666f\u7684\u60f3\u50cf\u5c0e\u822a\u7b56\u7565\uff0c\u4ee5\u5e6b\u52a9\u4ee3\u7406\u66f4\u5feb\u5730\u627e\u5230\u76ee\u6a19\u3002", "method": "\u63d0\u51faSGImagineNav\u6846\u67b6\uff0c\u5229\u7528\u7b26\u865f\u4e16\u754c\u5efa\u6a21\u548c\u5c64\u6b21\u5316\u5834\u666f\u5716\u4f86\u9810\u6e2c\u548c\u63a2\u7d22\u672a\u77e5\u74b0\u5883\u3002\u8a72\u6846\u67b6\u4f7f\u7528\u5927\u578b\u8a9e\u8a00\u6a21\u578b\u9810\u6e2c\u672a\u4f86\u5834\u666f\uff0c\u4e26\u63a1\u7528\u81ea\u9069\u61c9\u5c0e\u822a\u7b56\u7565\uff0c\u5728\u6709\u5229\u6642\u5229\u7528\u8a9e\u7fa9\u6377\u5f91\uff0c\u5426\u5247\u63a2\u7d22\u672a\u77e5\u5340\u57df\u4ee5\u6536\u96c6\u66f4\u591a\u4e0a\u4e0b\u6587\u3002", "result": "SGImagineNav\u5728\u771f\u5be6\u4e16\u754c\u5834\u666f\u548c\u6a21\u64ec\u57fa\u6e96\u6e2c\u8a66\u4e2d\u5747\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002", "conclusion": "SGImagineNav\u5728HM3D\u548cHSSD\u7684\u6210\u529f\u7387\u5206\u5225\u9054\u523065.4%\u548c66.8%\uff0c\u4e26\u5728\u771f\u5be6\u74b0\u5883\u4e2d\u5c55\u793a\u4e86\u8de8\u6a13\u5c64\u548c\u8de8\u623f\u9593\u5c0e\u822a\u80fd\u529b\uff0c\u8b49\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.07120", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07120", "abs": "https://arxiv.org/abs/2508.07120", "authors": ["Alexandra Ram\u00f4a", "Lu\u00eds Paulo Santos", "Akihito Soeda"], "title": "Low Cost Bayesian Experimental Design for Quantum Frequency Estimation with Decoherence", "comment": null, "summary": "A two-level quantum system evolving under a time-independent Hamiltonian\nproduces oscillatory measurement probabilities. The estimation of the\nassociated frequency is a cornerstone problem in quantum metrology, sensing,\ncalibration and control. In this work, we tackle this task by introducing WES:\na Window Expansion Strategy for low cost adaptive Bayesian experimental design.\nWES employs empirical cost-reduction techniques to keep the optimization\noverhead low, curb scaling problems, and enable high degrees of parallelism.\nUnlike previous heuristics, it offers adjustable classical processing costs\nthat determine the performance standard. As a benchmark, we analyze the\nperformance of widely adopted heuristics, comparing them with the fundamental\nlimits of metrology and a baseline random strategy. Numerical simulations show\nthat WES delivers the most reliable performance and fastest learning rate,\nsaturating the Heisenberg limit.", "AI": {"tldr": "WES is a low cost adaptive Bayesian experimental design strategy for estimating frequency in two-level quantum systems. It uses cost-reduction techniques for efficiency and parallelism, and its performance is adjustable. WES outperforms other heuristics and saturates the Heisenberg limit.", "motivation": "The estimation of the associated frequency is a cornerstone problem in quantum metrology, sensing, calibration and control.", "method": "WES employs empirical cost-reduction techniques to keep the optimization overhead low, curb scaling problems, and enable high degrees of parallelism. It offers adjustable classical processing costs that determine the performance standard.", "result": "Numerical simulations show that WES delivers the most reliable performance and fastest learning rate, saturating the Heisenberg limit. As a benchmark, we analyze the performance of widely adopted heuristics, comparing them with the fundamental limits of metrology and a baseline random strategy.", "conclusion": "WES delivers the most reliable performance and fastest learning rate, saturating the Heisenberg limit."}}
{"id": "2508.06870", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06870", "abs": "https://arxiv.org/abs/2508.06870", "authors": ["Gangular Singh Irengbam", "Nirvash Singh Wahengbam", "Lanthoiba Meitei Khumanthem", "Paikhomba Oinam"], "title": "Text to Speech System for Meitei Mayek Script", "comment": null, "summary": "This paper presents the development of a Text-to-Speech (TTS) system for the\nManipuri language using the Meitei Mayek script. Leveraging Tacotron 2 and\nHiFi-GAN, we introduce a neural TTS architecture adapted to support tonal\nphonology and under-resourced linguistic environments. We develop a phoneme\nmapping for Meitei Mayek to ARPAbet, curate a single-speaker dataset, and\ndemonstrate intelligible and natural speech synthesis, validated through\nsubjective and objective metrics. This system lays the groundwork for\nlinguistic preservation and technological inclusion of Manipuri.", "AI": {"tldr": "\u201c\u6cf0\u8bed\u201dTTS\u7cfb\u7edf\uff1a\u57fa\u4e8eTacotron 2\u548cHiFi-GAN\uff0c\u4f7f\u7528\u201c\u6885\u6cf0\u00b7\u739b\u8036\u514b\u201d\u6587\u5b57\uff0c\u652f\u6301\u58f0\u8c03\u97f3\u7cfb\uff0c\u8d44\u6e90\u532e\u4e4f\uff0c\u9002\u7528\u4e8e\u5355\u8bf4\u8bdd\u4eba\uff0c\u8bed\u97f3\u6e05\u6670\u81ea\u7136\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u652f\u6301\u58f0\u8c03\u97f3\u7cfb\u548c\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u73af\u5883\u7684\u795e\u7ecf\u7f51\u7edcTTS\u67b6\u6784\uff0c\u4ee5\u652f\u6301\u201c\u6cf0\u8bed\u201d\u7684\u8bed\u8a00\u4fdd\u62a4\u548c\u6280\u672f\u5305\u5bb9\u6027\u3002", "method": "\u5229\u7528Tacotron 2\u548cHiFi-GAN\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u652f\u6301\u58f0\u8c03\u97f3\u7cfb\u548c\u8d44\u6e90\u532e\u4e4f\u7684\u8bed\u8a00\u73af\u5883\u7684\u795e\u7ecf\u7f51\u7edcTTS\u67b6\u6784\uff0c\u5e76\u5f00\u53d1\u4e86\u201c\u6885\u6cf0\u00b7\u739b\u8036\u514b\u201d\u5230ARPAbet\u7684\u97f3\u7d20\u6620\u5c04\uff0c\u540c\u65f6\u6574\u7406\u4e86\u4e00\u4e2a\u5355\u8bf4\u8bdd\u4eba\u6570\u636e\u96c6\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u201c\u6cf0\u8bed\u201dTTS\u7cfb\u7edf\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u53ef\u7406\u89e3\u6027\u548c\u81ea\u7136\u6027\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u4e3b\u5ba2\u89c2\u6307\u6807\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5f00\u53d1\u9762\u5411\u201c\u6cf0\u8bed\u201d\u7684\u6587\u672c\u5230\u8bed\u97f3\uff08TTS\uff09\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8be5\u7cfb\u7edf\u4f7f\u7528\u201c\u6885\u6cf0\u00b7\u739b\u8036\u514b\u201d\u6587\u5b57\uff0c\u8fd9\u6709\u52a9\u4e8e\u8bed\u8a00\u4fdd\u62a4\u548c\u6280\u672f\u5305\u5bb9\u6027\u3002"}}
{"id": "2508.06558", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06558", "abs": "https://arxiv.org/abs/2508.06558", "authors": ["Simon Baur", "Alexandra Benova", "Emilio Dolgener Cant\u00fa", "Jackie Ma"], "title": "On the effectiveness of multimodal privileged knowledge distillation in two vision transformer based diagnostic applications", "comment": null, "summary": "Deploying deep learning models in clinical practice often requires leveraging\nmultiple data modalities, such as images, text, and structured data, to achieve\nrobust and trustworthy decisions. However, not all modalities are always\navailable at inference time. In this work, we propose multimodal privileged\nknowledge distillation (MMPKD), a training strategy that utilizes additional\nmodalities available solely during training to guide a unimodal vision model.\nSpecifically, we used a text-based teacher model for chest radiographs\n(MIMIC-CXR) and a tabular metadata-based teacher model for mammography\n(CBIS-DDSM) to distill knowledge into a vision transformer student model. We\nshow that MMPKD can improve the resulting attention maps' zero-shot\ncapabilities of localizing ROI in input images, while this effect does not\ngeneralize across domains, as contrarily suggested by prior research.", "AI": {"tldr": "MMPKD uses extra training data to improve vision models when some data is missing later, but only within the same data type.", "motivation": "Deep learning models in clinical practice often need multiple data modalities for robust decisions, but not all modalities are always available at inference time.", "method": "The paper proposes multimodal privileged knowledge distillation (MMPKD), a training strategy that uses additional modalities available only during training to guide a unimodal vision model. A text-based teacher model for chest radiographs (MIMIC-CXR) and a tabular metadata-based teacher model for mammography (CBIS-DDSM) were used to distill knowledge into a vision transformer student model.", "result": "MMPKD improves the zero-shot capabilities of attention maps for ROI localization.", "conclusion": "MMPKD can improve the zero-shot capabilities of attention maps for ROI localization in input images, but this effect does not generalize across domains."}}
{"id": "2508.07749", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.07749", "abs": "https://arxiv.org/abs/2508.07749", "authors": ["Shurui Guan", "Keqiang Li", "Haoyu Yang", "Yihe Chen", "Hanxiao Ren", "Yugong Luo"], "title": "Robust Integrated Priority and Speed Control based on Hierarchical Stochastic Optimization to Promote Bus Schedule Adherence along Signalized Arterial", "comment": "This paper has been accepted by 26th IEEE International Conference on\n  Intelligent Transportation Systems ITSC 2025", "summary": "In intelligent transportation systems (ITS), adaptive transit signal priority\n(TSP) and dynamic bus control systems have been independently developed to\nmaintain efficient and reliable urban bus services. However, those two systems\ncould potentially lead to conflicting decisions due to the lack of\ncoordination. Although some studies explore the integrated control strategies\nalong the arterial, they merely rely on signal replanning to address system\nuncertainties. Therefore, their performance severely deteriorates in real-world\nintersection settings, where abrupt signal timing variation is not always\napplicable in consideration of countdown timers and pedestrian signal design.\n  In this study, we propose a robust integrated priority and speed control\nstrategy based on hierarchical stochastic optimization to enhance bus schedule\nadherence along the arterial. In the proposed framework, the upper level\nensures the coordination across intersections while the lower level handles\nuncertainties for each intersection with stochastic programming. Hence, the\nroute-level system randomness is decomposed into a series of local problems\nthat can be solved in parallel using sample average approximation (SAA).\nSimulation experiments are conducted under various scenarios with stochastic\nbus dwell time and different traffic demand. The results demonstrate that our\napproach significantly enhances bus punctuality and time headway equivalence\nwithout abrupt signal timing variation, with negative impacts on car delays\nlimited to only 0.8%-5.2% as traffic demand increases.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u516c\u4ea4\u4fe1\u53f7\u4f18\u5148\u548c\u901f\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u901a\u8fc7\u5206\u5c42\u968f\u673a\u4f18\u5316\u548c\u5e76\u884c\u8ba1\u7b97\uff0c\u63d0\u9ad8\u4e86\u516c\u4ea4\u8f66\u51c6\u70b9\u7387\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u4e86\u5bf9\u79c1\u5bb6\u8f66\u7684\u5f71\u54cd\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u9002\u5e94\u516c\u4ea4\u4fe1\u53f7\u4f18\u5148\uff08TSP\uff09\u548c\u52a8\u6001\u516c\u4ea4\u8f66\u63a7\u5236\u7cfb\u7edf\u662f\u72ec\u7acb\u5f00\u53d1\u7684\uff0c\u7f3a\u4e4f\u534f\u8c03\u53ef\u80fd\u5bfc\u81f4\u51b2\u7a81\u51b3\u7b56\u3002\u73b0\u6709\u7684\u96c6\u6210\u63a7\u5236\u7b56\u7565\u4ec5\u4f9d\u8d56\u4fe1\u53f7\u91cd\u89c4\u5212\u6765\u89e3\u51b3\u7cfb\u7edf\u4e0d\u786e\u5b9a\u6027\uff0c\u5728\u5b9e\u9645\u4ea4\u53c9\u53e3\u8bbe\u7f6e\u4e2d\u6548\u679c\u4e0d\u4f73\uff0c\u56e0\u4e3a\u8003\u8651\u5230\u5012\u8ba1\u65f6\u548c\u884c\u4eba\u4fe1\u53f7\u8bbe\u8ba1\uff0c\u4fe1\u53f7\u8ba1\u65f6\u53d8\u5316\u53ef\u80fd\u4e0d\u9002\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5206\u5c42\u968f\u673a\u4f18\u5316\u7684\u9c81\u68d2\u96c6\u6210\u4f18\u5148\u548c\u901f\u5ea6\u63a7\u5236\u7b56\u7565\uff0c\u4e0a\u5c42\u8d1f\u8d23\u8de8\u4ea4\u53c9\u53e3\u7684\u534f\u8c03\uff0c\u4e0b\u5c42\u5229\u7528\u968f\u673a\u89c4\u5212\u5904\u7406\u5404\u4ea4\u53c9\u53e3\u7684\u3064\u304d\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u901a\u8fc7\u6837\u672c\u5e73\u5747\u8fd1\u4f3c\u6cd5\uff08SAA\uff09\u5e76\u884c\u6c42\u89e3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u968f\u673a\u516c\u4ea4\u8f66\u505c\u9760\u65f6\u95f4\u548c\u4e0d\u540c\u4ea4\u901a\u9700\u6c42\u4e0b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u516c\u4ea4\u8f66\u51c6\u70b9\u7387\u548c\u65f6\u523b\u8868\u4f9d\u4ece\u6027\uff0c\u540c\u65f6\u5c06\u5bf9\u79c1\u5bb6\u8f66\u5ef6\u8bef\u7684\u8d1f\u9762\u5f71\u54cd\u9650\u5236\u5728\u4ea4\u901a\u9700\u6c42\u589e\u52a0\u76840.8%-5.2%\u4e4b\u95f4\uff0c\u4e14\u65e0\u9700\u6539\u53d8\u4fe1\u53f7\u914d\u65f6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u6539\u53d8\u4fe1\u53f7\u914d\u65f6\u65b9\u6848\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u9ad8\u4e86\u516c\u4ea4\u8f66\u51c6\u70b9\u7387\u548c\u65f6\u523b\u8868\u4f9d\u4ece\u6027\uff0c\u540c\u65f6\u4ec5\u7565\u5fae\u589e\u52a0\u4e86\u79c1\u5bb6\u8f66\u5ef6\u8bef\u3002"}}
{"id": "2508.07740", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07740", "abs": "https://arxiv.org/abs/2508.07740", "authors": ["Yang Shen", "Shangzhi Song", "Tao Chen", "Kexin Zhang", "Yu Chen", "Lu Zhao", "Puqing Jiang"], "title": "Mechanistic Insight into BEOL Thermal Transport via Optical Metrology and Multiphysics Simulation", "comment": "16 pages, 6 figures", "summary": "As integrated circuits continue to scale down and adopt three-dimensional\n(3D) stacking, thermal management in the back-end-of-line (BEOL) has emerged as\na critical design constraint. In this study, we present a combined experimental\nand simulation framework to quantitatively characterize and mechanistically\nunderstand thermal transport in BEOL multilayers. Using the Square-Pulsed\nSource (SPS) method, a time-resolved optical metrology technique, we measure\ncross-plane thermal resistance and areal heat capacity in semiconductor chips\nat nanometer resolution. Two fabricated chip samples, polished to the M4 and M6\ninterconnection layers, are analyzed to extract thermal properties of distinct\nmultilayer stacks. Results show that thermal resistance follows a series model,\nwhile areal heat capacity scales linearly with metal content. To uncover the\nunderlying physical mechanisms, we perform finite element simulations using\nCOMSOL Multiphysics, examining the influence of via connectivity and dielectric\nthermal conductivity on effective cross-plane heat transport. The simulations\nreveal that dielectric materials, due to their large volume fraction, are the\nprimary limiting factor in BEOL thermal conduction, while the via structure\nplays a secondary but significant role. This combined experimental-simulation\napproach provides mechanistic insight into heat transport in advanced IC\narchitectures and offers practical guidance for optimizing thermal pathways in\nfuture high-performance 3D-stacked devices.", "AI": {"tldr": "\u7531\u4e8e\u5148\u8fdb\u96c6\u6210\u7535\u8def\u8bbe\u8ba1\u4e2d\u7684\u5c3a\u5bf8\u7f29\u5c0f\u548c3D\u5806\u53e0\uff0c\u5bf9BEOL\u70ed\u7ba1\u7406\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u5b9e\u9a8c\uff08SPS\uff09\u548c\u4eff\u771f\uff08COMSOL\uff09\u65b9\u6cd5\uff0c\u5728\u7eb3\u7c73\u5c3a\u5ea6\u4e0a\u8868\u5f81\u4e86BEOL\u591a\u5c42\u819c\u7684\u70ed\u4f20\u8f93\u7279\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u4ecb\u7535\u6750\u6599\u662f\u9650\u5236\u70ed\u4f20\u5bfc\u7684\u4e3b\u8981\u56e0\u7d20\uff0c\u800c\u8fc7\u5b54\u7ed3\u6784\u4e5f\u8d77\u7740\u4e00\u5b9a\u4f5c\u7528\u3002\u8be5\u65b9\u6cd5\u4e3a\u4f18\u5316\u672a\u6765\u76843D\u5806\u53e0\u5668\u4ef6\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002", "motivation": "\u968f\u7740\u96c6\u6210\u7535\u8def\u7684\u4e0d\u65ad\u7f29\u5c0f\u548c\u4e09\u7ef4\uff083D\uff09\u5806\u53e0\u7684\u5e94\u7528\uff0c\u540e\u7aef\uff08BEOL\uff09\u7684\u70ed\u7ba1\u7406\u5df2\u6210\u4e3a\u5173\u952e\u7684\u8bbe\u8ba1\u7ea6\u675f\u3002", "method": "\u4f7f\u7528\u65b9\u5f62\u8109\u51b2\u6e90\uff08SPS\uff09\u65b9\u6cd5\uff0c\u4e00\u79cd\u65f6\u95f4\u5206\u8fa8\u7684\u5149\u5b66\u8ba1\u91cf\u6280\u672f\uff0c\u5728\u7eb3\u7c73\u5206\u8fa8\u7387\u4e0b\u6d4b\u91cf\u534a\u5bfc\u4f53\u82af\u7247\u7684\u8de8\u5e73\u9762\u70ed\u963b\u548c\u9762\u70ed\u5bb9\u3002\u4f7f\u7528COMSOL Multiphysics\u8fdb\u884c\u6709\u9650\u5143\u6a21\u62df\uff0c\u68c0\u67e5\u8fc7\u5b54\u8fde\u901a\u6027\u548c\u4ecb\u7535\u70ed\u5bfc\u7387\u5bf9\u6709\u6548\u8de8\u5e73\u9762\u70ed\u4f20\u8f93\u7684\u5f71\u54cd\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u70ed\u963b\u9075\u5faa\u4e32\u8054\u6a21\u578b\uff0c\u800c\u9762\u70ed\u5bb9\u4e0e\u91d1\u5c5e\u542b\u91cf\u5448\u7ebf\u6027\u5173\u7cfb\u3002\u6a21\u62df\u8868\u660e\uff0c\u7531\u4e8e\u5176\u8f83\u5927\u7684\u4f53\u79ef\u5206\u6570\uff0c\u4ecb\u7535\u6750\u6599\u662fBEOL\u70ed\u4f20\u5bfc\u7684\u4e3b\u8981\u9650\u5236\u56e0\u7d20\uff0c\u800c\u8fc7\u5b54\u7ed3\u6784\u8d77\u7740\u6b21\u8981\u4f46\u91cd\u8981\u7684\u4f5c\u7528\u3002", "conclusion": "\u5b9e\u9a8c\u4e0e\u4eff\u771f\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u4e3a\u5148\u8fdb\u96c6\u6210\u7535\u8def\u67b6\u6784\u4e2d\u7684\u70ed\u4f20\u8f93\u63d0\u4f9b\u4e86\u529b\u5b66\u89c1\u89e3\uff0c\u5e76\u4e3a\u4f18\u5316\u672a\u6765\u9ad8\u6027\u80fd3D\u5806\u53e0\u5668\u4ef6\u7684\u70ed\u901a\u8def\u63d0\u4f9b\u4e86\u5b9e\u9645\u6307\u5bfc\u3002"}}
{"id": "2508.07513", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07513", "abs": "https://arxiv.org/abs/2508.07513", "authors": ["Emre Kurtoglu", "Mohammad Mahbubur Rahman"], "title": "Direction of Arrival Estimation with Virtual Antenna Array Using FMCW Radar Simulated Data", "comment": null, "summary": "The FMCW radars are widely used for automotive radar systems. The basic idea\nfor FMCW radars is to generate a linear frequency ramp as transmit signal. The\ndifference frequency, (i.e., beat frequency) between the transmitted and\nreceived signal is determined after down conversion. The FFT operation on beat\nfrequency signal can recognize targets at different range and velocity.\nIncreasing demand on safety functionality leads to the Direction of Arrival\n(DOA) estimation to resolve two closely located targets. Consequently, the\nproblem of angle estimation for 77GHz FMCW automotive radar simulated data has\nbeen investigated in this term project. In particular, we examined the\nperformances of FFT, MUSIC and compressed sensing in angle estimation task, and\nit was found that although FFT is the fastest algorithm, it has very poor\nangular resolution when compared with others which are both super resolution\nalgorithms. The code for this project report is available at\nhttps://github.com/ekurtgl/FMCW-MIMO-Radar-Simulation.", "AI": {"tldr": "\u8be5\u9879\u76ee\u7814\u7a76\u4e8677GHz FMCW\u6c7d\u8f66\u96f7\u8fbe\u7684DOA\u4f30\u8ba1\uff0c\u6bd4\u8f83\u4e86FFT\u3001MUSIC\u548c\u538b\u7f29\u611f\u77e5\u7684\u6027\u80fd\uff0c\u53d1\u73b0MUSIC\u548c\u538b\u7f29\u611f\u77e5\u5177\u6709\u66f4\u597d\u7684\u89d2\u5ea6\u5206\u8fa8\u7387\u3002", "motivation": "\u968f\u7740\u5bf9\u6c7d\u8f66\u5b89\u5168\u529f\u80fd\u9700\u6c42\u7684\u4e0d\u65ad\u589e\u957f\uff0c\u9700\u8981DOA\u4f30\u8ba1\u6765\u533a\u5206\u8fd1\u8ddd\u79bb\u7684\u76ee\u6807\u3002", "method": "\u901a\u8fc7\u4eff\u771f\u6570\u636e\uff0c\u7814\u7a76\u4e86FFT\u3001MUSIC\u548c\u538b\u7f29\u611f\u77e5\u572877GHz FMCW\u6c7d\u8f66\u96f7\u8fbeDOA\u4f30\u8ba1\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\u8868\u73b0\u3002", "result": "MUSIC\u548c\u538b\u7f29\u611f\u77e5\u5728\u89d2\u5ea6\u4f30\u8ba1\u65b9\u9762\u8868\u73b0\u4f18\u4e8eFFT\uff0c\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u89d2\u5ea6\u5206\u8fa8\u7387\u3002", "conclusion": "FFT\u7b97\u6cd5\u901f\u5ea6\u6700\u5feb\uff0c\u4f46\u89d2\u5ea6\u5206\u8fa8\u7387\u8f83\u4f4e\uff0c\u800cMUSIC\u548c\u538b\u7f29\u611f\u77e5\u662f\u5177\u6709\u66f4\u9ad8\u89d2\u5ea6\u5206\u8fa8\u7387\u7684\u8d85\u5206\u8fa8\u7387\u7b97\u6cd5\u3002"}}
{"id": "2508.06894", "categories": ["cs.AI", "cs.LG", "68T05"], "pdf": "https://arxiv.org/pdf/2508.06894", "abs": "https://arxiv.org/abs/2508.06894", "authors": ["Giovanni Varricchione", "Toryn Q. Klassen", "Natasha Alechina", "Mehdi Dastani", "Brian Logan", "Sheila A. McIlraith"], "title": "Pushdown Reward Machines for Reinforcement Learning", "comment": null, "summary": "Reward machines (RMs) are automata structures that encode (non-Markovian)\nreward functions for reinforcement learning (RL). RMs can reward any behaviour\nrepresentable in regular languages and, when paired with RL algorithms that\nexploit RM structure, have been shown to significantly improve sample\nefficiency in many domains. In this work, we present pushdown reward machines\n(pdRMs), an extension of reward machines based on deterministic pushdown\nautomata. pdRMs can recognize and reward temporally extended behaviours\nrepresentable in deterministic context-free languages, making them more\nexpressive than reward machines. We introduce two variants of pdRM-based\npolicies, one which has access to the entire stack of the pdRM, and one which\ncan only access the top $k$ symbols (for a given constant $k$) of the stack. We\npropose a procedure to check when the two kinds of policies (for a given\nenvironment, pdRM, and constant $k$) achieve the same optimal expected reward.\nWe then provide theoretical results establishing the expressive power of pdRMs,\nand space complexity results about the proposed learning problems. Finally, we\nprovide experimental results showing how agents can be trained to perform tasks\nrepresentable in deterministic context-free languages using pdRMs.", "AI": {"tldr": "\u5956\u52b1\u673a\uff08RM\uff09\u662f\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u81ea\u52a8\u673a\u7ed3\u6784\uff0c\u53ef\u4ee5\u5bf9\u8868\u793a\u5728\u6b63\u5219\u8868\u8fbe\u5f0f\u4e2d\u7684\u884c\u4e3a\u8fdb\u884c\u5956\u52b1\u3002\u672c\u6587\u63d0\u51fa\u4e86\u5956\u52b1\u673a\uff08RM\uff09\u7684\u6269\u5c55\uff0c\u79f0\u4e3a\u4e0b\u63a8\u5956\u52b1\u673a\uff08pdRM\uff09\uff0c\u5b83\u57fa\u4e8e\u786e\u5b9a\u6027\u4e0b\u63a8\u81ea\u52a8\u673a\u3002pdRM\u53ef\u4ee5\u8bc6\u522b\u548c\u5956\u52b1\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u4e2d\u53ef\u8868\u793a\u7684\u65f6\u5e8f\u884c\u4e3a\u3002\u4f5c\u8005\u8fd8\u63d0\u51fa\u4e86\u4e24\u79cdpdRM\u7b56\u7565\u53d8\u4f53\uff0c\u5e76\u63d0\u4f9b\u4e86\u7406\u8bba\u7ed3\u679c\uff0c\u4ee5\u5efa\u7acbpdRM\u7684\u8868\u73b0\u529b\u4ee5\u53ca\u5b66\u4e60\u95ee\u9898\u7684\u7a7a\u95f4\u590d\u6742\u6027\u3002\u6700\u540e\uff0c\u4f5c\u8005\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4ee5\u8868\u660e\u4f7f\u7528pdRM\u7684\u667a\u80fd\u4f53\u53ef\u4ee5\u88ab\u8bad\u7ec3\u6765\u6267\u884c\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u4e2d\u53ef\u8868\u793a\u7684\u4efb\u52a1\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u5956\u52b1\u673a\uff08RM\uff09\u4ee5\u5904\u7406\u66f4\u590d\u6742\u7684\u975e\u9a6c\u5c14\u53ef\u592b\u5956\u52b1\u529f\u80fd\uff0c\u9700\u8981\u66f4\u5177\u8868\u73b0\u529b\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u786e\u5b9a\u6027\u4e0b\u63a8\u81ea\u52a8\u673a\u7684\u5956\u52b1\u673a\uff08pdRM\uff09\u7684\u6269\u5c55\uff0c\u4f5c\u4e3a\u5956\u52b1\u673a\uff08RM\uff09\u7684\u66ff\u4ee3\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e24\u79cd\u57fa\u4e8epdRM\u7684\u7b56\u7565\u53d8\u4f53\uff0c\u5e76\u7814\u7a76\u4e86\u5b83\u4eec\u7684\u5b66\u4e60\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4f7f\u7528pdRM\u53ef\u4ee5\u8bad\u7ec3\u667a\u80fd\u4f53\u6765\u6267\u884c\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u4e2d\u53ef\u8868\u793a\u7684\u4efb\u52a1\u3002", "conclusion": "pdRM\u76f8\u6bd4RM\u66f4\u5177\u8868\u73b0\u529b\uff0c\u53ef\u4ee5\u8bc6\u522b\u548c\u5956\u52b1\u786e\u5b9a\u6027\u4e0a\u4e0b\u6587\u65e0\u5173\u8bed\u8a00\u4e2d\u53ef\u8868\u793a\u7684\u65f6\u671f\u884c\u4e3a\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u7b56\u7565\u7684\u4e24\u79cd\u53d8\u4f53\uff0c\u5e76\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u5206\u6790\u3002"}}
{"id": "2508.06663", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06663", "abs": "https://arxiv.org/abs/2508.06663", "authors": ["Yuan-Hung Chao", "Chia-Hsun Lu", "Chih-Ya Shen"], "title": "Transferring Social Network Knowledge from Multiple GNN Teachers to Kolmogorov-Arnold Networks", "comment": "6 pages, 3 tables", "summary": "Graph Neural Networks (GNNs) have shown strong performance on\ngraph-structured data, but their reliance on graph connectivity often limits\nscalability and efficiency. Kolmogorov-Arnold Networks (KANs), a recent\narchitecture with learnable univariate functions, offer strong nonlinear\nexpressiveness and efficient inference. In this work, we integrate KANs into\nthree popular GNN architectures-GAT, SGC, and APPNP-resulting in three new\nmodels: KGAT, KSGC, and KAPPNP. We further adopt a multi-teacher knowledge\namalgamation framework, where knowledge from multiple KAN-based GNNs is\ndistilled into a graph-independent KAN student model. Experiments on benchmark\ndatasets show that the proposed models improve node classification accuracy,\nand the knowledge amalgamation approach significantly boosts student model\nperformance. Our findings highlight the potential of KANs for enhancing GNN\nexpressiveness and for enabling efficient, graph-free inference.", "AI": {"tldr": "\u63d0\u51fa\u5c06KANs\u96c6\u6210\u5230GNN\u4e2d\uff0c\u5e76\u4f7f\u7528\u77e5\u8bc6\u6742\u5408\u65b9\u6cd5\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\uff0c\u7ed3\u679c\u663e\u793aKANs\u589e\u5f3a\u4e86GNN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u65e0\u56fe\u63a8\u7406\u3002", "motivation": "GNN\u5728\u56fe\u7ed3\u6784\u6570\u636e\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u5bf9\u56fe\u8fde\u901a\u6027\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\u3002KANs\u5177\u6709\u53ef\u5b66\u4e60\u7684\u5355\u53d8\u91cf\u51fd\u6570\uff0c\u63d0\u4f9b\u5f3a\u5927\u7684\u975e\u7ebf\u6027\u8868\u8fbe\u80fd\u529b\u548c\u9ad8\u6548\u7684\u63a8\u7406\u80fd\u529b\u3002", "method": "\u5c06KANs\u96c6\u6210\u5230GAT\u3001SGC\u548cAPPNP\u4e09\u79cd\u6d41\u884c\u7684GNN\u67b6\u6784\u4e2d\uff0c\u5f97\u5230KGAT\u3001KSGC\u548cKAPPNP\u3002\u91c7\u7528\u591a\u6559\u5e08\u77e5\u8bc6\u6742\u5408\u6846\u67b6\uff0c\u5c06\u77e5\u8bc6\u4ece\u57fa\u4e8eKAN\u7684GNN\u84b8\u998f\u5230\u56fe\u65e0\u5173\u7684KAN\u5b66\u751f\u6a21\u578b\u3002", "result": "\u63d0\u51fa\u7684\u6a21\u578b\u63d0\u9ad8\u4e86\u8282\u70b9\u5206\u7c7b\u51c6\u786e\u6027\uff0c\u77e5\u8bc6\u6742\u5408\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u6027\u80fd\u3002", "conclusion": "KANs\u53ef\u4ee5\u589e\u5f3aGNN\u7684\u8868\u8fbe\u80fd\u529b\uff0c\u5e76\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u56fe\u63a8\u7406\u3002"}}
{"id": "2508.07722", "categories": ["cs.LG", "cs.IT", "cs.MA", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07722", "abs": "https://arxiv.org/abs/2508.07722", "authors": ["Pietro Talli", "Federico Mason", "Federico Chiariotti", "Andrea Zanella"], "title": "Robust Reinforcement Learning over Wireless Networks with Homomorphic State Representations", "comment": "This manuscript is currently under revision", "summary": "In this work, we address the problem of training Reinforcement Learning (RL)\nagents over communication networks. The RL paradigm requires the agent to\ninstantaneously perceive the state evolution to infer the effects of its\nactions on the environment. This is impossible if the agent receives state\nupdates over lossy or delayed wireless systems and thus operates with partial\nand intermittent information. In recent years, numerous frameworks have been\nproposed to manage RL with imperfect feedback; however, they often offer\nspecific solutions with a substantial computational burden. To address these\nlimits, we propose a novel architecture, named Homomorphic Robust Remote\nReinforcement Learning (HR3L), that enables the training of remote RL agents\nexchanging observations across a non-ideal wireless channel. HR3L considers two\nunits: the transmitter, which encodes meaningful representations of the\nenvironment, and the receiver, which decodes these messages and performs\nactions to maximize a reward signal. Importantly, HR3L does not require the\nexchange of gradient information across the wireless channel, allowing for\nquicker training and a lower communication overhead than state-of-the-art\nsolutions. Experimental results demonstrate that HR3L significantly outperforms\nbaseline methods in terms of sample efficiency and adapts to different\ncommunication scenarios, including packet losses, delayed transmissions, and\ncapacity limitations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HR3L \u7684\u65b0\u9896 RL \u67b6\u6784\uff0c\u5b83\u901a\u8fc7\u975e\u7406\u60f3\u65e0\u7ebf\u4fe1\u9053\u8fdb\u884c\u901a\u4fe1\uff0c\u65e0\u9700\u4ea4\u6362\u68af\u5ea6\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u66f4\u5feb\u7684\u8bad\u7ec3\u548c\u66f4\u4f4e\u7684\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5728\u6837\u672c\u6548\u7387\u548c\u9002\u5e94\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709 RL \u6846\u67b6\u5728\u5904\u7406\u4e0d\u5b8c\u7f8e\u53cd\u9988\u65f6\u7684\u8ba1\u7b97\u8d1f\u62c5\u548c\u7279\u5b9a\u9650\u5236\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u591f\u901a\u8fc7\u975e\u7406\u60f3\u65e0\u7ebf\u4fe1\u9053\u4ea4\u6362\u4fe1\u606f\u6765\u8bad\u7ec3\u8fdc\u7a0b RL \u4ee3\u7406\u7684\u65b0\u67b6\u6784\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a HR3L \u7684\u65b0\u9896\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5305\u542b\u4e24\u4e2a\u5355\u5143\uff1a\u53d1\u9001\u65b9\uff08\u7f16\u7801\u6709\u610f\u4e49\u7684\u73af\u5883\u8868\u793a\uff09\u548c\u63a5\u6536\u65b9\uff08\u89e3\u7801\u6d88\u606f\u5e76\u6267\u884c\u52a8\u4f5c\u4ee5\u6700\u5927\u5316\u5956\u52b1\u4fe1\u53f7\uff09\u3002HR3L \u65e0\u9700\u901a\u8fc7\u65e0\u7ebf\u4fe1\u9053\u4ea4\u6362\u68af\u5ea6\u4fe1\u606f\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHR3L \u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u80fd\u9002\u5e94\u4e0d\u540c\u7684\u901a\u4fe1\u573a\u666f\u3002", "conclusion": "HR3L \u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728\u6837\u672c\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u597d\uff0c\u5e76\u4e14\u80fd\u591f\u9002\u5e94\u5305\u62ec\u4e22\u5305\u3001\u5ef6\u8fdf\u4f20\u8f93\u548c\u5bb9\u91cf\u9650\u5236\u5728\u5185\u7684\u4e0d\u540c\u901a\u4fe1\u573a\u666f\u3002"}}
{"id": "2508.07204", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.07204", "abs": "https://arxiv.org/abs/2508.07204", "authors": ["Chenxi Lu", "Musen Li", "Jeffrey R. Reimers"], "title": "Reproducibility of high-throughput density-functional-theory calculations", "comment": null, "summary": "While standard computational protocols for density functional theory (DFT)\nhave universal applicability, differences exist in code implementations.\nSpecific applications require manual parameter optimization, whereas\nhigh-throughput calculations employ predefined workflows. This paper uses the\nbandgap as a key property to reveal the impact of computational workflow\ndifferences on the reproducibility of high-throughput calculation results. The\nstudy proposes basic requirements for ensuring reproducibility: using\nstructures optimised using the same procedure as used to calculate properties\nand ensuring Brillouin zone (k-point) integration grid accuracy. This research\nestablishes a foundation for the reproducibility of DFT calculations and\nreliable application of results, which is of great significance for method\ndevelopment and artificial intelligence model training.", "AI": {"tldr": "DFT\u8ba1\u7b97\u5728\u4ee3\u7801\u5b9e\u73b0\u3001\u53c2\u6570\u4f18\u5316\u548c\u5de5\u4f5c\u6d41\u7a0b\u4e0a\u5b58\u5728\u5dee\u5f02\u3002\u672c\u7814\u7a76\u4ee5\u5e26\u9699\u4e3a\u4f8b\uff0c\u63a2\u8ba8\u4e86\u8fd9\u4e9b\u5dee\u5f02\u5bf9\u9ad8\u901a\u91cf\u8ba1\u7b97\u7ed3\u679c\u53ef\u91cd\u73b0\u6027\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4fdd\u8bc1\u53ef\u91cd\u73b0\u6027\u7684\u57fa\u672c\u8981\u6c42\uff0c\u5305\u62ec\u4f7f\u7528\u76f8\u540c\u7a0b\u5e8f\u4f18\u5316\u7ed3\u6784\u548c\u786e\u4fddk\u70b9\u79ef\u5206\u7f51\u683c\u7cbe\u5ea6\uff0c\u8fd9\u5bf9\u65b9\u6cd5\u5f00\u53d1\u548cAI\u6a21\u578b\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u6807\u51c6\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08DFT\uff09\u8ba1\u7b97\u534f\u8bae\u5177\u6709\u666e\u904d\u9002\u7528\u6027\uff0c\u4f46\u4ee3\u7801\u5b9e\u73b0\u5b58\u5728\u5dee\u5f02\u3002\u7279\u5b9a\u5e94\u7528\u9700\u8981\u624b\u52a8\u53c2\u6570\u4f18\u5316\uff0c\u800c\u9ad8\u901a\u91cf\u8ba1\u7b97\u5219\u91c7\u7528\u9884\u5b9a\u4e49\u7684\u5de5\u4f5c\u6d41\u7a0b\u3002", "method": "\u672c\u7814\u7a76\u4f7f\u7528\u5e26\u9699\u4f5c\u4e3a\u5173\u952e\u5c5e\u6027\uff0c\u63ed\u793a\u4e86\u8ba1\u7b97\u5de5\u4f5c\u6d41\u7a0b\u5dee\u5f02\u5bf9\u9ad8\u901a\u91cf\u8ba1\u7b97\u7ed3\u679c\u53ef\u91cd\u73b0\u6027\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4fdd\u8bc1\u53ef\u91cd\u73b0\u6027\u7684\u57fa\u672c\u8981\u6c42\uff1a\u4f7f\u7528\u76f8\u540c\u7a0b\u5e8f\u4f18\u5316\u7684\u7ed3\u6784\u6765\u8ba1\u7b97\u5c5e\u6027\uff0c\u5e76\u786e\u4fdd\u5e03\u91cc\u6e0a\u533a\uff08k\u70b9\uff09\u79ef\u5206\u7f51\u683c\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u4e3a\u786e\u4fddDFT\u8ba1\u7b97\u7684\u53ef\u91cd\u73b0\u6027\u53ca\u7ed3\u679c\u7684\u53ef\u9760\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8fd9\u5bf9\u4e8e\u65b9\u6cd5\u5f00\u53d1\u548c\u4eba\u5de5\u667a\u80fd\u6a21\u578b\u8bad\u7ec3\u5177\u6709\u91cd\u5927\u610f\u4e49\u3002"}}
{"id": "2508.06972", "categories": ["cs.AI", "cs.CR", "cs.DC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06972", "abs": "https://arxiv.org/abs/2508.06972", "authors": ["Dan Ivanov", "Tristan Freiberg", "Haruna Isah"], "title": "DSperse: A Framework for Targeted Verification in Zero-Knowledge Machine Learning", "comment": "12 pages, 8 figures, and 10 tables", "summary": "DSperse is a modular framework for distributed machine learning inference\nwith strategic cryptographic verification. Operating within the emerging\nparadigm of distributed zero-knowledge machine learning, DSperse avoids the\nhigh cost and rigidity of full-model circuitization by enabling targeted\nverification of strategically chosen subcomputations. These verifiable\nsegments, or \"slices\", may cover part or all of the inference pipeline, with\nglobal consistency enforced through audit, replication, or economic incentives.\nThis architecture supports a pragmatic form of trust minimization, localizing\nzero-knowledge proofs to the components where they provide the greatest value.\nWe evaluate DSperse using multiple proving systems and report empirical results\non memory usage, runtime, and circuit behavior under sliced and unsliced\nconfigurations. By allowing proof boundaries to align flexibly with the model's\nlogical structure, DSperse supports scalable, targeted verification strategies\nsuited to diverse deployment needs.", "AI": {"tldr": "DSperse\u662f\u4e00\u4e2a\u7528\u4e8e\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63a8\u7406\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u53ea\u9a8c\u8bc1\u90e8\u5206\u8ba1\u7b97\uff08\u5207\u7247\uff09\u800c\u975e\u6574\u4e2a\u6a21\u578b\u6765\u964d\u4f4e\u6210\u672c\u548c\u63d0\u9ad8\u7075\u6d3b\u6027\uff0c\u5e76\u63d0\u4f9b\u4fe1\u4efb\u6700\u5c0f\u5316\u65b9\u6848\u3002", "motivation": "\u8be5\u7814\u7a76\u7684\u52a8\u673a\u662f\u89e3\u51b3\u5206\u5e03\u5f0f\u673a\u5668\u5b66\u4e60\u63a8\u7406\u4e2d\u9ad8\u6602\u7684\u6210\u672c\u548c\u50f5\u5316\u7684\u7535\u8def\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u66f4\u52a1\u5b9e\u3001\u53ef\u4fe1\u5ea6\u66f4\u5c0f\u5316\u7684\u65b9\u6848\uff0c\u5c06\u96f6\u77e5\u8bc6\u8bc1\u660e\u5b9a\u4f4d\u5728\u6700\u6709\u4ef7\u503c\u7684\u7ec4\u4ef6\u4e0a\u3002", "method": "DSperse\u6846\u67b6\u901a\u8fc7\u201c\u5207\u7247\u201d\u6280\u672f\uff0c\u4ec5\u5bf9\u90e8\u5206\u5b50\u8ba1\u7b97\u8fdb\u884c\u96f6\u77e5\u8bc6\u8bc1\u660e\u9a8c\u8bc1\uff0c\u800c\u975e\u5bf9\u6574\u4e2a\u6a21\u578b\u8fdb\u884c\u7535\u8def\u5316\uff0c\u4ece\u800c\u964d\u4f4e\u4e86\u6210\u672c\u548c\u63d0\u9ad8\u4e86\u7075\u6d3b\u6027\u3002\u5b83\u8fd8\u901a\u8fc7\u5ba1\u8ba1\u3001\u590d\u5236\u6216\u7ecf\u6d4e\u6fc0\u52b1\u7b49\u673a\u5236\u5f3a\u5236\u6267\u884c\u5168\u5c40\u4e00\u81f4\u6027\u3002", "result": "DSperse\u6846\u67b6\u5728\u5185\u5b58\u4f7f\u7528\u3001\u8fd0\u884c\u65f6\u548c\u7535\u8def\u884c\u4e3a\u65b9\u9762\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u62a5\u544a\u4e86\u5207\u7247\u548c\u975e\u5207\u7247\u914d\u7f6e\u4e0b\u7684\u7ecf\u9a8c\u7ed3\u679c\u3002", "conclusion": "DSperse\u6846\u67b6\u901a\u8fc7\u7075\u6d3b\u5bf9\u9f50\u6a21\u578b\u7ed3\u6784\u548c\u5141\u8bb8\u53ef\u6269\u5c55\u3001\u6709\u9488\u5bf9\u6027\u7684\u9a8c\u8bc1\u7b56\u7565\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u90e8\u7f72\u9700\u6c42\u3002"}}
{"id": "2508.07003", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07003", "abs": "https://arxiv.org/abs/2508.07003", "authors": ["Siyu Chen", "Shenghai Yuan", "Thien-Minh Nguyen", "Zhuyu Huang", "Chenyang Shi", "Jin Jing", "Lihua Xie"], "title": "EGS-SLAM: RGB-D Gaussian Splatting SLAM with Events", "comment": "Accepted by IEEE RAL", "summary": "Gaussian Splatting SLAM (GS-SLAM) offers a notable improvement over\ntraditional SLAM methods, enabling photorealistic 3D reconstruction that\nconventional approaches often struggle to achieve. However, existing GS-SLAM\nsystems perform poorly under persistent and severe motion blur commonly\nencountered in real-world scenarios, leading to significantly degraded tracking\naccuracy and compromised 3D reconstruction quality. To address this limitation,\nwe propose EGS-SLAM, a novel GS-SLAM framework that fuses event data with RGB-D\ninputs to simultaneously reduce motion blur in images and compensate for the\nsparse and discrete nature of event streams, enabling robust tracking and\nhigh-fidelity 3D Gaussian Splatting reconstruction. Specifically, our system\nexplicitly models the camera's continuous trajectory during exposure,\nsupporting event- and blur-aware tracking and mapping on a unified 3D Gaussian\nSplatting scene. Furthermore, we introduce a learnable camera response function\nto align the dynamic ranges of events and images, along with a no-event loss to\nsuppress ringing artifacts during reconstruction. We validate our approach on a\nnew dataset comprising synthetic and real-world sequences with significant\nmotion blur. Extensive experimental results demonstrate that EGS-SLAM\nconsistently outperforms existing GS-SLAM systems in both trajectory accuracy\nand photorealistic 3D Gaussian Splatting reconstruction. The source code will\nbe available at https://github.com/Chensiyu00/EGS-SLAM.", "AI": {"tldr": "EGS-SLAM\u901a\u8fc7\u878d\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB-D\u8f93\u5165\uff0c\u89e3\u51b3\u4e86\u4f20\u7edfGS-SLAM\u5728\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u8ddf\u8e2a\u548c\u66f4\u9ad8\u8d28\u91cf\u76843D\u91cd\u5efa\u3002", "motivation": "\u73b0\u6709GS-SLAM\u7cfb\u7edf\u5728\u771f\u5b9e\u4e16\u754c\u7684\u6301\u4e45\u6027\u3001\u4e25\u91cd\u8fd0\u52a8\u6a21\u7cca\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73\uff0c\u5bfc\u81f4\u8ddf\u8e2a\u7cbe\u5ea6\u4e0b\u964d\u548c3D\u91cd\u5efa\u8d28\u91cf\u53d7\u635f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEGS-SLAM\u7684\u65b0\u6846\u67b6\uff0c\u878d\u5408\u4e8b\u4ef6\u6570\u636e\u4e0eRGB-D\u8f93\u5165\uff0c\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u76f8\u673a\u5728\u66dd\u5149\u671f\u95f4\u7684\u8fde\u7eed\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e8b\u4ef6\u611f\u77e5\u548c\u6a21\u7cca\u611f\u77e5\u7684\u8ddf\u8e2a\u4e0e\u5efa\u56fe\uff0c\u5e76\u5f15\u5165\u53ef\u5b66\u4e60\u7684\u76f8\u673a\u54cd\u5e94\u51fd\u6570\u6765\u5bf9\u9f50\u4e8b\u4ef6\u548c\u56fe\u50cf\u7684\u52a8\u6001\u8303\u56f4\uff0c\u540c\u65f6\u4f7f\u7528\u65e0\u4e8b\u4ef6\u635f\u5931\u6765\u6291\u5236\u91cd\u5efa\u8fc7\u7a0b\u4e2d\u7684\u632f\u94c3\u4f2a\u5f71\u3002", "result": "\u5728\u5305\u542b\u663e\u8457\u8fd0\u52a8\u6a21\u7cca\u7684\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u5e8f\u5217\u7684\u65b0\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u9a8c\u8bc1\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660eEGS-SLAM\u5728\u8f68\u8ff9\u51c6\u786e\u6027\u548c3D\u9ad8\u65af\u55b7\u6d82\u91cd\u5efa\u8d28\u91cf\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u73b0\u6709GS-SLAM\u7cfb\u7edf\u3002", "conclusion": "EGS-SLAM\u901a\u8fc7\u878d\u5408\u4e8b\u4ef6\u6570\u636e\u548cRGB-D\u8f93\u5165\uff0c\u5728\u5904\u7406\u8fd0\u52a8\u6a21\u7cca\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u6b65\uff0c\u5728\u8f68\u8ff9\u51c6\u786e\u6027\u548c3D\u9ad8\u65af\u55b7\u6d82\u91cd\u5efa\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709GS-SLAM\u7cfb\u7edf\u3002"}}
{"id": "2508.07125", "categories": ["quant-ph", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.07125", "abs": "https://arxiv.org/abs/2508.07125", "authors": ["Austin Pechan", "John Golden", "Daniel O'Malley"], "title": "Block encoding the 3D heterogeneous Poisson equation with application to fracture flow", "comment": null, "summary": "Quantum linear system (QLS) algorithms offer the potential to solve\nlarge-scale linear systems exponentially faster than classical methods.\nHowever, applying QLS algorithms to real-world problems remains challenging due\nto issues such as state preparation, data loading, and efficient information\nextraction. In this work, we study the feasibility of applying QLS algorithms\nto solve discretized three-dimensional heterogeneous Poisson equations, with\nspecific examples relating to groundwater flow through geologic fracture\nnetworks. We explicitly construct a block encoding for the 3D heterogeneous\nPoisson matrix by leveraging the sparse local structure of the discretized\noperator. While classical solvers benefit from preconditioning, we show that\nblock encoding the system matrix and preconditioner separately does not improve\nthe effective condition number that dominates the QLS runtime. This differs\nfrom classical approaches where the preconditioner and the system matrix can\noften be implemented independently. Nevertheless, due to the structure of the\nproblem in three dimensions, the quantum algorithm achieves a runtime of\n$O(N^{2/3} \\ \\text{polylog } N \\cdot \\log(1/\\epsilon))$, outperforming the best\nclassical methods (with runtimes of $O(N \\log N \\cdot \\log(1/\\epsilon))$) and\noffering exponential memory savings. These results highlight both the promise\nand limitations of QLS algorithms for practical scientific computing, and point\nto effective condition number reduction as a key barrier in achieving quantum\nadvantages.", "AI": {"tldr": "\u91cf\u5b50\u7b97\u6cd5\u5728\u89e3\u51b3\u4e09\u7ef4\u6cca\u677e\u65b9\u7a0b\u65b9\u9762\u6bd4\u7ecf\u5178\u65b9\u6cd5\u66f4\u5feb\uff0c\u5e76\u8282\u7701\u5185\u5b58\uff0c\u4f46\u964d\u4f4e\u6761\u4ef6\u6570\u662f\u5b9e\u73b0\u91cf\u5b50\u4f18\u52bf\u7684\u5173\u952e\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u91cf\u5b50\u7ebf\u6027\u7cfb\u7edf\uff08QLS\uff09\u7b97\u6cd5\u5728\u89e3\u51b3\u4e09\u7ef4\u975e\u5747\u8d28\u6cca\u677e\u65b9\u7a0b\u65b9\u9762\u7684\u53ef\u884c\u6027\uff0c\u7279\u522b\u662f\u5728\u5730\u4e0b\u6c34\u6d41\u6a21\u62df\u4e2d\u7684\u5e94\u7528\uff0c\u4ee5\u514b\u670d\u7ecf\u5178\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u7ebf\u6027\u7cfb\u7edf\u65f6\u7684\u6311\u6218\u3002", "method": "\u901a\u8fc7\u663e\u5f0f\u6784\u5efa\u4e09\u7ef4\u975e\u5747\u8d28\u6cca\u677e\u65b9\u7a0b\u7684\u5757\u7f16\u7801\uff0c\u5229\u7528\u79bb\u6563\u5316\u7b97\u5b50\u7684\u7a00\u758f\u5c40\u90e8\u7ed3\u6784\u3002\u5206\u6790\u4e86\u5c06\u7cfb\u7edf\u77e9\u9635\u548c\u9884\u5904\u7406\u6761\u4ef6\u6570\u5206\u522b\u8fdb\u884c\u5757\u7f16\u7801\u5bf9\u6709\u6548\u6761\u4ef6\u6570\u7684\u5f71\u54cd\uff0c\u5e76\u4e0e\u7ecf\u5178\u65b9\u6cd5\u8fdb\u884c\u4e86\u5bf9\u6bd4\u3002", "result": "\u91cf\u5b50\u7b97\u6cd5\u5b9e\u73b0\u4e86$O(N^{2/3} \text{ polylog } N \times \text{ log}(1/\text{epsilon}))$ \u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u7684$O(N \text{ log } N \times \text{ log}(1/\text{epsilon}))$\uff0c\u5e76\u5b9e\u73b0\u4e86\u6307\u6570\u7ea7\u5185\u5b58\u8282\u7701\u3002\u4f46\u5355\u72ec\u5bf9\u7cfb\u7edf\u77e9\u9635\u548c\u9884\u5904\u7406\u6761\u4ef6\u6570\u8fdb\u884c\u5757\u7f16\u7801\u5e76\u4e0d\u80fd\u63d0\u9ad8\u6709\u6548\u6761\u4ef6\u6570\u3002", "conclusion": "\u8be5\u7814\u7a76\u7a81\u663e\u4e86\u91cf\u5b50\u7ebf\u6027\u7cfb\u7edf\uff08QLS\uff09\u7b97\u6cd5\u5728\u89e3\u51b3\u4e09\u7ef4\u975e\u5747\u8d28\u6cca\u677e\u65b9\u7a0b\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u5730\u4e0b\u6c34\u6d41\u6a21\u62df\u65b9\u9762\u3002\u5c3d\u7ba1\u5b58\u5728\u72b6\u6001\u5236\u5907\u548c\u6570\u636e\u52a0\u8f7d\u7b49\u6311\u6218\uff0c\u4f46\u901a\u8fc7\u663e\u5f0f\u6784\u5efa\u79bb\u6563\u5316\u7b97\u5b50\u7684\u5757\u7f16\u7801\uff0c\u8be5\u91cf\u5b50\u7b97\u6cd5\u5b9e\u73b0\u4e86$O(N^{2/3} \text{ polylog } N \times \text{ log}(1/\text{epsilon}))$ \u7684\u8fd0\u884c\u65f6\u95f4\uff0c\u4f18\u4e8e\u7ecf\u5178\u65b9\u6cd5\u3002\u7136\u800c\uff0c\u7814\u7a76\u4e5f\u6307\u51fa\uff0c\u5355\u72ec\u5bf9\u7cfb\u7edf\u77e9\u9635\u548c\u9884\u5904\u7406\u6761\u4ef6\u6570\u8fdb\u884c\u5757\u7f16\u7801\u5e76\u4e0d\u80fd\u63d0\u9ad8\u6709\u6548\u7684\u6761\u4ef6\u6570\uff0c\u8fd9\u4e0e\u7ecf\u5178\u65b9\u6cd5\u4e0d\u540c\u3002\u6709\u6548\u7684\u6761\u4ef6\u6570\u964d\u4f4e\u662f\u5b9e\u73b0\u91cf\u5b50\u4f18\u52bf\u7684\u5173\u952e\u969c\u788d\u3002"}}
{"id": "2508.06877", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06877", "abs": "https://arxiv.org/abs/2508.06877", "authors": ["Xiaobo Zhang", "Congqing He", "Ying He", "Jian Peng", "Dajie Fu", "Tien-Ping Tan"], "title": "ESNERA: Empirical and semantic named entity alignment for named entity dataset merging", "comment": "30 pages, 12 figures", "summary": "Named Entity Recognition (NER) is a fundamental task in natural language\nprocessing. It remains a research hotspot due to its wide applicability across\ndomains. Although recent advances in deep learning have significantly improved\nNER performance, they rely heavily on large, high-quality annotated datasets.\nHowever, building these datasets is expensive and time-consuming, posing a\nmajor bottleneck for further research. Current dataset merging approaches\nmainly focus on strategies like manual label mapping or constructing label\ngraphs, which lack interpretability and scalability. To address this, we\npropose an automatic label alignment method based on label similarity. The\nmethod combines empirical and semantic similarities, using a greedy pairwise\nmerging strategy to unify label spaces across different datasets. Experiments\nare conducted in two stages: first, merging three existing NER datasets into a\nunified corpus with minimal impact on NER performance; second, integrating this\ncorpus with a small-scale, self-built dataset in the financial domain. The\nresults show that our method enables effective dataset merging and enhances NER\nperformance in the low-resource financial domain. This study presents an\nefficient, interpretable, and scalable solution for integrating multi-source\nNER corpora.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6807\u7b7e\u76f8\u4f3c\u6027\u7684\u81ea\u52a8\u6807\u7b7e\u5bf9\u9f50\u65b9\u6cd5\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u6570\u636e\u96c6\u5408\u5e76\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u91d1\u878d\u9886\u57df\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u6df1\u5ea6\u5b66\u4e60\u5728\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08NER\uff09\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u9ad8\u5ea6\u4f9d\u8d56\u5927\u578b\u9ad8\u8d28\u91cf\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u800c\u6784\u5efa\u8fd9\u4e9b\u6570\u636e\u96c6\u6210\u672c\u9ad8\u6602\u4e14\u8017\u65f6\uff0c\u8fd9\u963b\u788d\u4e86\u8fdb\u4e00\u6b65\u7684\u7814\u7a76\u3002\u73b0\u6709\u6570\u636e\u96c6\u5408\u5e76\u65b9\u6cd5\u5728\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u6269\u5c55\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6807\u7b7e\u76f8\u4f3c\u6027\u7684\u81ea\u52a8\u6807\u7b7e\u5bf9\u9f50\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7ecf\u9a8c\u548c\u8bed\u4e49\u76f8\u4f3c\u6027\uff0c\u5e76\u91c7\u7528\u8d2a\u5fc3\u6210\u5bf9\u5408\u5e76\u7b56\u7565\u6765\u7edf\u4e00\u4e0d\u540c\u6570\u636e\u96c6\u7684\u6807\u7b7e\u7a7a\u95f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5408\u5e76\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u5347\u4e86\u5728\u4f4e\u8d44\u6e90\u91d1\u878d\u9886\u57df\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6548\u679c\u3002", "conclusion": "\u6211\u4eec\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u5408\u5e76\u6570\u636e\u96c6\uff0c\u5e76\u63d0\u5347\u4e86\u5728\u4f4e\u8d44\u6e90\u91d1\u878d\u9886\u57df\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u7684\u6548\u679c\u3002\u672c\u7814\u7a76\u4e3a\u6574\u5408\u591a\u6e90\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u8bed\u6599\u5e93\u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6269\u5c55\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.06564", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06564", "abs": "https://arxiv.org/abs/2508.06564", "authors": ["Guanyu Hu", "Dimitrios Kollias", "Xinyu Yang"], "title": "Grounding Emotion Recognition with Visual Prototypes: VEGA -- Revisiting CLIP in MERC", "comment": "accepted for publication at ACM Multimedia (ACM MM) 2025", "summary": "Multimodal Emotion Recognition in Conversations remains a challenging task\ndue to the complex interplay of textual, acoustic and visual signals. While\nrecent models have improved performance via advanced fusion strategies, they\noften lack psychologically meaningful priors to guide multimodal alignment. In\nthis paper, we revisit the use of CLIP and propose a novel Visual Emotion\nGuided Anchoring (VEGA) mechanism that introduces class-level visual semantics\ninto the fusion and classification process. Distinct from prior work that\nprimarily utilizes CLIP's textual encoder, our approach leverages its image\nencoder to construct emotion-specific visual anchors based on facial exemplars.\nThese anchors guide unimodal and multimodal features toward a perceptually\ngrounded and psychologically aligned representation space, drawing inspiration\nfrom cognitive theories (prototypical emotion categories and multisensory\nintegration). A stochastic anchor sampling strategy further enhances robustness\nby balancing semantic stability and intra-class diversity. Integrated into a\ndual-branch architecture with self-distillation, our VEGA-augmented model\nachieves sota performance on IEMOCAP and MELD. Code is available at:\nhttps://github.com/dkollias/VEGA.", "AI": {"tldr": "This paper introduces VEGA, a novel mechanism using CLIP's image encoder to create emotion-specific visual anchors for multimodal emotion recognition. VEGA improves alignment and achieves state-of-the-art results on IEMOCAP and MELD datasets.", "motivation": "Multimodal emotion recognition models lack psychologically meaningful priors to guide multimodal alignment. This paper proposes using CLIP's image encoder to construct emotion-specific visual anchors, drawing inspiration from cognitive theories.", "method": "VEGA mechanism leverages CLIP's image encoder to construct emotion-specific visual anchors based on facial exemplars, guiding unimodal and multimodal features toward a perceptually grounded and psychologically aligned representation space. A stochastic anchor sampling strategy enhances robustness.", "result": "Achieves sota performance on IEMOCAP and MELD datasets.", "conclusion": "VEGA-augmented model achieves sota performance on IEMOCAP and MELD"}}
{"id": "2508.08132", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08132", "abs": "https://arxiv.org/abs/2508.08132", "authors": ["Mohammad Hossein Nejati Amiri", "Fawaz Annaz", "Mario De Oliveira", "Florimond Gueniat"], "title": "Deep Reinforcement Learning with Local Interpretability for Transparent Microgrid Resilience Energy Management", "comment": null, "summary": "Renewable energy integration into microgrids has become a key approach to\naddressing global energy issues such as climate change and resource scarcity.\nHowever, the variability of renewable sources and the rising occurrence of High\nImpact Low Probability (HILP) events require innovative strategies for reliable\nand resilient energy management. This study introduces a practical approach to\nmanaging microgrid resilience through Explainable Deep Reinforcement Learning\n(XDRL). It combines the Proximal Policy Optimization (PPO) algorithm for\ndecision-making with the Local Interpretable Model-agnostic Explanations (LIME)\nmethod to improve the transparency of the actor network's decisions. A case\nstudy in Ongole, India, examines a microgrid with wind, solar, and battery\ncomponents to validate the proposed approach. The microgrid is simulated under\nextreme weather conditions during the Layla cyclone. LIME is used to analyse\nscenarios, showing the impact of key factors such as renewable generation,\nstate of charge, and load prioritization on decision-making. The results\ndemonstrate a Resilience Index (RI) of 0.9736 and an estimated battery lifespan\nof 15.11 years. LIME analysis reveals the rationale behind the agent's actions\nin idle, charging, and discharging modes, with renewable generation identified\nas the most influential feature. This study shows the effectiveness of\nintegrating advanced DRL algorithms with interpretable AI techniques to achieve\nreliable and transparent energy management in microgrids.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408PPO\u548cLIME\u7684\u53ef\u89e3\u91ca\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08XDRL\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u7ba1\u7406\u5fae\u7535\u7f51\u97e7\u6027\u3002\u901a\u8fc7\u5728\u5370\u5ea6Ongole\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5728\u6a21\u62df\u7684\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u4e0b\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u63d0\u9ad8\u5fae\u7535\u7f51\u7684\u53ef\u9760\u6027\u548c\u900f\u660e\u5ea6\uff0c\u97e7\u6027\u6307\u6570\u4e3a0.9736\uff0c\u7535\u6c60\u5bff\u547d\u9884\u8ba115.11\u5e74\u3002", "motivation": "\u53ef\u518d\u751f\u80fd\u6e90\u5e76\u7f51\u5bf9\u89e3\u51b3\u6c14\u5019\u53d8\u5316\u548c\u8d44\u6e90\u7a00\u7f3a\u7b49\u5168\u7403\u80fd\u6e90\u95ee\u9898\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u53ef\u518d\u751f\u80fd\u6e90\u7684\u6ce2\u52a8\u6027\u548c\u9ad8\u5f71\u54cd\u529b\u4f4e\u6982\u7387\uff08HILP\uff09\u4e8b\u4ef6\u7684\u65e5\u76ca\u589e\u591a\uff0c\u5bf9\u53ef\u9760\u4e14\u6709\u97e7\u6027\u7684\u80fd\u6e90\u7ba1\u7406\u63d0\u51fa\u4e86\u521b\u65b0\u7b56\u7565\u7684\u9700\u6c42\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53ef\u89e3\u91ca\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08XDRL\uff09\u7ba1\u7406\u5fae\u7535\u7f51\u97e7\u6027\u7684\u5b9e\u7528\u65b9\u6cd5\uff0c\u7ed3\u5408\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u7b97\u6cd5\u8fdb\u884c\u51b3\u7b56\uff0c\u4ee5\u53ca\u5c40\u90e8\u53ef\u89e3\u91ca\u6a21\u578b\u65e0\u5173\u89e3\u91ca\uff08LIME\uff09\u65b9\u6cd5\u6765\u63d0\u9ad8Actor\u7f51\u7edc\u51b3\u7b56\u7684\u900f\u660e\u5ea6\u3002", "result": "\u5728Ongole\uff0c\u5370\u5ea6\u7684\u4e00\u4e2a\u5305\u542b\u98ce\u80fd\u3001\u592a\u9633\u80fd\u548c\u7535\u6c60\u7ec4\u4ef6\u7684\u5fae\u7535\u7f51\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6a21\u62df\u4e86\u5728Layla\u6c14\u65cb\u671f\u95f4\u7684\u6781\u7aef\u5929\u6c14\u6761\u4ef6\u3002\u7ed3\u679c\u663e\u793a\uff0c\u97e7\u6027\u6307\u6570\uff08RI\uff09\u4e3a0.9736\uff0c\u7535\u6c60\u5bff\u547d\u4f30\u8ba1\u4e3a15.11\u5e74\u3002LIME\u5206\u6790\u63ed\u793a\u4e86\u667a\u80fd\u4f53\u5728\u7a7a\u95f2\u3001\u5145\u7535\u548c\u653e\u7535\u6a21\u5f0f\u4e0b\u884c\u4e3a\u7684\u5408\u7406\u6027\uff0c\u5176\u4e2d\u53ef\u518d\u751f\u80fd\u6e90\u53d1\u7535\u88ab\u786e\u5b9a\u4e3a\u6700\u5177\u5f71\u54cd\u529b\u7684\u7279\u5f81\u3002", "conclusion": "\u672c\u7814\u7a76\u8868\u660e\uff0c\u96c6\u6210\u5148\u8fdb\u7684\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0e\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\u6280\u672f\u76f8\u7ed3\u5408\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u9760\u4e14\u900f\u660e\u7684\u5fae\u7535\u7f51\u80fd\u6e90\u7ba1\u7406\u3002"}}
{"id": "2508.07792", "categories": ["cond-mat.mes-hall", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.07792", "abs": "https://arxiv.org/abs/2508.07792", "authors": ["Barbaros \u015eair"], "title": "QVNTVS, Open-Source Quantum Well Simulator", "comment": "For the source code: https://github.com/sairbarbaros/QVNTVS", "summary": "Quantum Wells (QW) are of great importance in optoelectronic devices such as\nLEDs and LASERs, being the emissive layers.Simulating the quantum particles in\ndifferent QW topologies like rectangular finite potential wells, multiple\npotential wells, and triangular biased potential well heterojunctions enables\nfaster modeling, theoretical characterization, and more. QVNTVS performs energy\nlevel and wavefunction calculations, recombination probability, transition\nenergy, and optical emission computations quickly and accurately. Contrasting\nwith the existing simulators, QVNTVS is an open-source project and can produce\nsolutions for niche problems like potential wells under an electric field,\nheterojunctions, recombination, and transition matrices. QVNTVS simulates QWs\nby solving the Time-Independent Schr\\\"odinger Equation for different potential\nprofiles in a discretized space using the finite-difference method and computes\nthe properties of the device using the extracted information from the solution.\nThe results align with the analytical calculations and the experimental data.", "AI": {"tldr": "QVNTVS\u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u91cf\u5b50\u9631\u6a21\u62df\u5668\uff0c\u4f7f\u7528\u6709\u9650\u5dee\u5206\u6cd5\u6c42\u89e3\u859b\u5b9a\u8c14\u65b9\u7a0b\uff0c\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u8ba1\u7b97\u591a\u79cd\u91cf\u5b50\u9631\u7684\u5404\u79cd\u5149\u7535\u5c5e\u6027\u3002", "motivation": "\u4e3a\u4e86\u80fd\u591f\u66f4\u5feb\u901f\u5730\u6a21\u62df\u91cf\u5b50\u9631\u7684\u5404\u79cd\u62d3\u6251\u7ed3\u6784\uff08\u5982\u77e9\u5f62\u3001\u591a\u91cd\u548c\u4e09\u89d2\u52bf\u9631\u5f02\u8d28\u7ed3\uff09\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u8868\u5f81\uff0c\u5f00\u53d1\u4e86QVNTVS\u6a21\u62df\u5668\u3002", "method": "\u4f7f\u7528\u6709\u9650\u5dee\u5206\u6cd5\u5728\u79bb\u6563\u7a7a\u95f4\u4e2d\u6c42\u89e3\u5b9a\u6001\u859b\u5b9a\u8c14\u65b9\u7a0b\uff0c\u4ee5\u6a21\u62df\u4e0d\u540c\u52bf\u80fd\u5256\u9762\u4e0b\u7684\u91cf\u5b50\u9631\u3002", "result": "QVNTVS\u80fd\u591f\u5feb\u901f\u51c6\u786e\u5730\u8ba1\u7b97\u80fd\u91cf\u548c\u6ce2\u51fd\u6570\u3001\u590d\u5408\u6982\u7387\u3001\u8dc3\u8fc1\u80fd\u91cf\u548c\u5149\u5b66\u53d1\u5c04\u7b49\u5c5e\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u89e3\u51b3\u7535\u573a\u4e0b\u7684\u52bf\u9631\u3001\u5f02\u8d28\u7ed3\u3001\u590d\u5408\u548c\u8dc3\u8fc1\u77e9\u9635\u7b49\u7279\u6b8a\u95ee\u9898\u3002", "conclusion": "QVNTVS\u901a\u8fc7\u6c42\u89e3\u79bb\u6563\u7a7a\u95f4\u4e2d\u7684\u5b9a\u6001\u859b\u5b9a\u8c14\u65b9\u7a0b\u6765\u6a21\u62df\u91cf\u5b50\u9631\uff0c\u5e76\u8ba1\u7b97\u5668\u4ef6\u7684\u5404\u79cd\u5c5e\u6027\uff0c\u4e0e\u89e3\u6790\u8ba1\u7b97\u548c\u5b9e\u9a8c\u6570\u636e\u4e00\u81f4\u3002"}}
{"id": "2508.07572", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07572", "abs": "https://arxiv.org/abs/2508.07572", "authors": ["Yuanwei Liu", "Hao Jiang", "Xiaoxia Xu", "Zhaolin Wang", "Jia Guo", "Chongjun Ouyang", "Xidong Mu", "Zhiguo Ding", "Arumugam Nallanathan", "George K. Karagiannidis", "Robert Schober"], "title": "Pinching-Antenna Systems (PASS): A Tutorial", "comment": "Submitted to IEEE journal", "summary": "Pinching antenna systems (PASS) present a breakthrough among the\nflexible-antenna technologies, and distinguish themselves by facilitating\nlarge-scale antenna reconfiguration, line-of-sight creation, scalable\nimplementation, and near-field benefits, thus bringing wireless communications\nfrom the last mile to the last meter. A comprehensive tutorial is presented in\nthis paper. First, the fundamentals of PASS are discussed, including PASS\nsignal models, hardware models, power radiation models, and pinching antenna\nactivation methods. Building upon this, the information-theoretic capacity\nlimits achieved by PASS are characterized, and several typical performance\nmetrics of PASS-based communications are analyzed to demonstrate its\nsuperiority over conventional antenna technologies. Next, the pinching\nbeamforming design is investigated. The corresponding power scaling law is\nfirst characterized. For the joint transmit and pinching design in the general\nmultiple-waveguide case, 1) a pair of transmission strategies is proposed for\nPASS-based single-user communications to validate the superiority of PASS,\nnamely sub-connected and fully connected structures; and 2) three practical\nprotocols are proposed for facilitating PASS-based multi-user communications,\nnamely waveguide switching, waveguide division, and waveguide multiplexing. A\npossible implementation of PASS in wideband communications is further\nhighlighted. Moreover, the channel state information acquisition in PASS is\nelaborated with a pair of promising solutions. To overcome the high complexity\nand suboptimality inherent in conventional convex-optimization-based\napproaches, machine-learning-based methods for operating PASS are also\nexplored, focusing on selected deep neural network architectures and training\nalgorithms. Finally, several promising applications of PASS in next-generation\nwireless networks are highlighted.", "AI": {"tldr": "This paper is a tutorial on Pinching Antenna Systems (PASS), a new flexible antenna technology. It covers PASS fundamentals, capacity limits, beamforming, communication strategies for single-user and multi-user scenarios, wideband implementation, channel estimation, and machine learning applications. PASS shows advantages over traditional antennas for future wireless networks.", "motivation": "To introduce and analyze the breakthrough technology of Pinching Antenna Systems (PASS), highlighting its advantages in bringing wireless communications from the last mile to the last meter through features like large-scale reconfiguration, line-of-sight creation, scalability, and near-field benefits.", "method": "This paper provides a comprehensive tutorial on PASS, covering fundamental aspects such as signal, hardware, and power radiation models, as well as activation methods. It analyzes information-theoretic capacity limits and performance metrics, compares PASS with conventional technologies, and investigates beamforming designs, including power scaling laws and transmission strategies for single-user and multi-user scenarios (waveguide switching, division, and multiplexing). The paper also discusses wideband communication implementations, channel state information acquisition, and explores machine-learning-based methods (deep neural networks and training algorithms) for PASS operation.", "result": "The paper demonstrates the superiority of PASS over conventional antenna technologies through the analysis of its information-theoretic capacity limits and performance metrics. It also proposes various strategies and protocols for PASS-based communication and explores machine learning approaches for its operation.", "conclusion": "Pinching antenna systems (PASS) offer significant advantages over conventional antenna technologies, enabling large-scale reconfiguration, line-of-sight creation, scalable implementation, and near-field benefits for wireless communications. Machine learning approaches show promise for optimizing PASS operations, overcoming the limitations of traditional methods. PASS holds potential for various applications in next-generation wireless networks."}}
{"id": "2508.06899", "categories": ["cs.AI", "cs.DM"], "pdf": "https://arxiv.org/pdf/2508.06899", "abs": "https://arxiv.org/abs/2508.06899", "authors": ["Yanchen Deng", "Xinrun Wang", "Bo An"], "title": "GDBA Revisited: Unleashing the Power of Guided Local Search for Distributed Constraint Optimization", "comment": null, "summary": "Local search is an important class of incomplete algorithms for solving\nDistributed Constraint Optimization Problems (DCOPs) but it often converges to\npoor local optima. While GDBA provides a comprehensive rule set to escape\npremature convergence, its empirical benefits remain marginal on general-valued\nproblems. In this work, we systematically examine GDBA and identify three\nfactors that potentially lead to its inferior performance, i.e.,\nover-aggressive constraint violation conditions, unbounded penalty\naccumulation, and uncoordinated penalty updates. To address these issues, we\npropose Distributed Guided Local Search (DGLS), a novel GLS framework for DCOPs\nthat incorporates an adaptive violation condition to selectively penalize\nconstraints with high cost, a penalty evaporation mechanism to control the\nmagnitude of penalization, and a synchronization scheme for coordinated penalty\nupdates. We theoretically show that the penalty values are bounded, and agents\nplay a potential game in our DGLS. Our extensive empirical results on various\nstandard benchmarks demonstrate the great superiority of DGLS over\nstate-of-the-art baselines. Particularly, compared to Damped Max-sum with high\ndamping factors (e.g., 0.7 or 0.9), our DGLS achieves competitive performance\non general-valued problems, and outperforms it by significant margins\n(\\textbf{3.77\\%--66.3\\%}) on structured problems in terms of anytime results.", "AI": {"tldr": "DGLS\u662f\u4e00\u79cd\u65b0\u7684GLS\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3DCOPs\u95ee\u9898\uff0c\u901a\u8fc7\u89e3\u51b3GDBA\u7684\u4e0d\u8db3\u4e4b\u5904\uff0c\u5728\u5404\u79cd\u6807\u51c6\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5206\u5e03\u5f0f\u7ea6\u675f\u4f18\u5316\u95ee\u9898\uff08DCOPs\uff09\u7684\u672c\u5730\u641c\u7d22\u7b97\u6cd5\uff08\u5982GDBA\uff09\u5b58\u5728\u6536\u655b\u5230\u4e0d\u826f\u5c40\u90e8\u6700\u4f18\u89e3\u7684\u95ee\u9898\u3002GDBA\u867d\u7136\u63d0\u4f9b\u4e86\u4e00\u5957\u5b8c\u6574\u7684\u89c4\u5219\u6765\u907f\u514d\u8fc7\u65e9\u6536\u655b\uff0c\u4f46\u5728\u901a\u7528\u503c\u95ee\u9898\u4e0a\u7684\u5b9e\u9645\u6548\u679c\u4e0d\u4f73\u3002\u4f5c\u8005\u7cfb\u7edf\u5730\u68c0\u67e5\u4e86GDBA\uff0c\u53d1\u73b0\u5176\u6027\u80fd\u4e0d\u4f73\u7684\u6f5c\u5728\u56e0\u7d20\u5305\u62ec\uff1a\u8fc7\u4e8e\u6fc0\u8fdb\u7684\u7ea6\u675f\u8fdd\u53cd\u6761\u4ef6\u3001\u65e0\u754c\u7684\u60e9\u7f5a\u7d2f\u79ef\u4ee5\u53ca\u4e0d\u534f\u8c03\u7684\u60e9\u7f5a\u66f4\u65b0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u5206\u5e03\u5f0f\u5f15\u5bfc\u672c\u5730\u641c\u7d22\uff08DGLS\uff09\u7684\u65b0\u9896GLS\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u81ea\u9002\u5e94\u8fdd\u53cd\u6761\u4ef6\u3001\u60e9\u7f5a\u84b8\u53d1\u673a\u5236\u548c\u534f\u8c03\u60e9\u7f5a\u66f4\u65b0\u3002", "result": "DGLS\u5728\u901a\u7528\u503c\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u4e0e\u963b\u5c3c\u6700\u5927\u548c\uff08damping factors\u4e3a0.7\u62160.9\uff09\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u7ed3\u6784\u5316\u95ee\u9898\u4e0a\u4ee53.77%--66.3%\u7684\u4f18\u52bf\u8d85\u8d8a\u4e86\u5b83\u3002", "conclusion": "DGLS\u5728\u901a\u7528\u503c\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u4e0e\u963b\u5c3c\u6700\u5927\u548c\uff08damping factors\u4e3a0.7\u62160.9\uff09\u76f8\u5f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u7ed3\u6784\u5316\u95ee\u9898\u4e0a\u4ee54.15%--76.0%\u7684\u4f18\u52bf\u8d85\u8d8a\u4e86\u5b83\u3002"}}
{"id": "2508.06676", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06676", "abs": "https://arxiv.org/abs/2508.06676", "authors": ["Chia-Hsun Lu", "Guan-Jhih Wu", "Ya-Chi Ho", "Chih-Ya Shen"], "title": "Watermarking Kolmogorov-Arnold Networks for Emerging Networked Applications via Activation Perturbation", "comment": "6 pages, 3 figures, 6 tables", "summary": "With the increasing importance of protecting intellectual property in machine\nlearning, watermarking techniques have gained significant attention. As\nadvanced models are increasingly deployed in domains such as social network\nanalysis, the need for robust model protection becomes even more critical.\nWhile existing watermarking methods have demonstrated effectiveness for\nconventional deep neural networks, they often fail to adapt to the novel\narchitecture, Kolmogorov-Arnold Networks (KAN), which feature learnable\nactivation functions. KAN holds strong potential for modeling complex\nrelationships in network-structured data. However, their unique design also\nintroduces new challenges for watermarking. Therefore, we propose a novel\nwatermarking method, Discrete Cosine Transform-based Activation Watermarking\n(DCT-AW), tailored for KAN. Leveraging the learnable activation functions of\nKAN, our method embeds watermarks by perturbing activation outputs using\ndiscrete cosine transform, ensuring compatibility with diverse tasks and\nachieving task independence. Experimental results demonstrate that DCT-AW has a\nsmall impact on model performance and provides superior robustness against\nvarious watermark removal attacks, including fine-tuning, pruning, and\nretraining after pruning.", "AI": {"tldr": "A new watermarking method (DCT-AW) is introduced for Kolmogorov-Arnold Networks (KANs). It uses Discrete Cosine Transform on learnable activations to embed watermarks, showing good performance and strong resistance to attacks while preserving model accuracy.", "motivation": "The increasing importance of protecting intellectual property in machine learning and the deployment of advanced models like KAN in sensitive domains necessitate robust model protection techniques. Existing methods fail to adapt to KANs due to their unique architecture with learnable activation functions.", "method": "A novel watermarking method, Discrete Cosine Transform-based Activation Watermarking (DCT-AW), is proposed. It embeds watermarks by perturbing activation outputs using discrete cosine transform, leveraging the learnable activation functions of KAN.", "result": "Experimental results demonstrate that DCT-AW has a small impact on model performance and provides superior robustness against various watermark removal attacks, including fine-tuning, pruning, and retraining after pruning.", "conclusion": "The proposed DCT-AW method is effective and robust for watermarking KANs, with minimal impact on model performance and superior resistance to removal attacks."}}
{"id": "2508.07950", "categories": ["cs.AI", "cs.CV", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2508.07950", "abs": "https://arxiv.org/abs/2508.07950", "authors": ["Chen Shen", "Wanqing Zhang", "Kehan Li", "Erwen Huang", "Haitao Bi", "Aiying Fan", "Yiwen Shen", "Hongmei Dong", "Ji Zhang", "Yuming Shao", "Zengjia Liu", "Xinshe Liu", "Tao Li", "Chunxia Yan", "Shuanliang Fan", "Di Wu", "Jianhua Ma", "Bin Cong", "Zhenyuan Wang", "Chunfeng Lian"], "title": "FEAT: A Multi-Agent Forensic AI System with Domain-Adapted Large Language Model for Automated Cause-of-Death Analysis", "comment": "18pages, 6 figures", "summary": "Forensic cause-of-death determination faces systemic challenges, including\nworkforce shortages and diagnostic variability, particularly in high-volume\nsystems like China's medicolegal infrastructure. We introduce FEAT (ForEnsic\nAgenT), a multi-agent AI framework that automates and standardizes death\ninvestigations through a domain-adapted large language model. FEAT's\napplication-oriented architecture integrates: (i) a central Planner for task\ndecomposition, (ii) specialized Local Solvers for evidence analysis, (iii) a\nMemory & Reflection module for iterative refinement, and (iv) a Global Solver\nfor conclusion synthesis. The system employs tool-augmented reasoning,\nhierarchical retrieval-augmented generation, forensic-tuned LLMs, and\nhuman-in-the-loop feedback to ensure legal and medical validity. In evaluations\nacross diverse Chinese case cohorts, FEAT outperformed state-of-the-art AI\nsystems in both long-form autopsy analyses and concise cause-of-death\nconclusions. It demonstrated robust generalization across six geographic\nregions and achieved high expert concordance in blinded validations. Senior\npathologists validated FEAT's outputs as comparable to those of human experts,\nwith improved detection of subtle evidentiary nuances. To our knowledge, FEAT\nis the first LLM-based AI agent system dedicated to forensic medicine, offering\nscalable, consistent death certification while maintaining expert-level rigor.\nBy integrating AI efficiency with human oversight, this work could advance\nequitable access to reliable medicolegal services while addressing critical\ncapacity constraints in forensic systems.", "AI": {"tldr": "FEAT\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u5229\u7528\u4f18\u5316\u7684LLM\u6765\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u6cd5\u533b\u6b7b\u4ea1\u8c03\u67e5\u3002\u5728\u4e2d\u56fd\u6848\u4f8b\u4e2d\uff0c\u5b83\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709AI\u7cfb\u7edf\uff0c\u5e76\u83b7\u5f97\u4e86\u4e13\u5bb6\u7684\u9ad8\u5ea6\u8ba4\u53ef\uff0c\u6709\u671b\u89e3\u51b3\u6cd5\u533b\u9886\u57df\u7684\u6311\u6218\u5e76\u63d0\u9ad8\u670d\u52a1\u53ef\u53ca\u6027\u3002", "motivation": "\u4e3a\u5e94\u5bf9\u6cd5\u533b\u9274\u5b9a\u9886\u57df\uff0c\u7279\u522b\u662f\u5728\u4e2d\u56fd\u7b49\u9ad8\u5de5\u4f5c\u91cf\u7cfb\u7edf\u4e2d\u5b58\u5728\u7684\u52b3\u52a8\u529b\u77ed\u7f3a\u548c\u8bca\u65ad\u53d8\u5f02\u6027\u7b49\u7cfb\u7edf\u6027\u6311\u6218\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u6b7b\u4ea1\u8c03\u67e5\u3002", "method": "FEAT\uff08ForEnsic AgenT\uff09\u662f\u4e00\u4e2a\u591a\u667a\u80fd\u4f53AI\u6846\u67b6\uff0c\u5229\u7528\u9886\u57df\u9002\u5e94\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u6765\u81ea\u52a8\u5316\u548c\u6807\u51c6\u5316\u6b7b\u4ea1\u8c03\u67e5\u3002\u5176\u67b6\u6784\u5305\u62ec\uff1a\u4e2d\u5fc3\u89c4\u5212\u5668\uff08\u4efb\u52a1\u5206\u89e3\uff09\u3001\u5c40\u90e8\u6c42\u89e3\u5668\uff08\u8bc1\u636e\u5206\u6790\uff09\u3001\u8bb0\u5fc6\u4e0e\u53cd\u601d\u6a21\u5757\uff08\u8fed\u4ee3\u4f18\u5316\uff09\u548c\u5168\u5c40\u6c42\u89e3\u5668\uff08\u7ed3\u8bba\u7efc\u5408\uff09\u3002\u7cfb\u7edf\u91c7\u7528\u5de5\u5177\u589e\u5f3a\u63a8\u7406\u3001\u5206\u5c42\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u3001\u6cd5\u533b\u4f18\u5316\u7684LLM\u4ee5\u53ca\u4eba\u673a\u56de\u73af\u53cd\u9988\u3002", "result": "\u5728\u4e2d\u56fd\u7684\u5404\u7c7b\u6848\u4f8b\u961f\u5217\u8bc4\u4f30\u4e2d\uff0cFEAT\u5728\u957f\u7bc7\u5c38\u68c0\u5206\u6790\u548c\u7b80\u6d01\u7684\u6b7b\u56e0\u7ed3\u8bba\u65b9\u9762\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684AI\u7cfb\u7edf\u3002\u5b83\u5728\u516d\u4e2a\u5730\u7406\u533a\u57df\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u76f2\u6cd5\u9a8c\u8bc1\u4e2d\u8fbe\u5230\u9ad8\u4e13\u5bb6\u4e00\u81f4\u6027\u3002\u8d44\u6df1\u75c5\u7406\u5b66\u5bb6\u8bc1\u5b9eFEAT\u7684\u8f93\u51fa\u53ef\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u6c34\u5e73\u76f8\u5ab2\u7f8e\uff0c\u5e76\u80fd\u66f4\u597d\u5730\u68c0\u6d4b\u7ec6\u5fae\u7684\u8bc1\u636e\u5dee\u522b\u3002", "conclusion": "FEAT\u7cfb\u7edf\u662f\u9996\u4e2a\u7528\u4e8e\u6cd5\u533b\u9886\u57df\u7684\u57fa\u4e8e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684AI\u4ee3\u7406\u7cfb\u7edf\uff0c\u80fd\u591f\u5b9e\u73b0\u53ef\u6269\u5c55\u3001\u4e00\u81f4\u7684\u6b7b\u4ea1\u8bc1\u660e\uff0c\u5e76\u4fdd\u6301\u4e13\u5bb6\u7ea7\u4e25\u8c28\u6027\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u6574\u5408AI\u6548\u7387\u548c\u4eba\u5de5\u76d1\u7763\uff0c\u6709\u671b\u4fc3\u8fdb\u53ef\u9760\u7684\u6cd5\u533b\u5b66\u670d\u52a1\u7684\u516c\u5e73\u53ef\u53ca\u6027\uff0c\u540c\u65f6\u89e3\u51b3\u6cd5\u533b\u7cfb\u7edf\u4e2d\u7684\u5173\u952e\u80fd\u529b\u9650\u5236\u3002"}}
{"id": "2508.07234", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.07234", "abs": "https://arxiv.org/abs/2508.07234", "authors": ["Jan Kune\u0161"], "title": "On the N\u00e9el Vector Dependence of X-ray Magnetic Circular Dichroism in Altermagnets", "comment": "5 pages, no figures", "summary": "Dependence of x-ray magnetic circular dichroism on the experimental geometry\nis described by a frequency-dependent Hall vector. Using group theory, we\nderive a general relationship between the Hall vector and the orientation of\nthe N\\'eel vector $\\bL$ in altermagnets within the free valence spin (FVS)\napproximation, where the spin-orbit coupling of the valence electrons and their\nexchange interaction with the core electrons are neglected. For a given spin\npoint group, the full $\\bL$-dependence of the Hall vector can be expressed in\nterms of several irreducible spectral functions. This derivation generalizes\nearlier results for the special cases of MnTe and MnF$_2$. Depending on the\nsystem symmetry, XMCD in the FVS approximation may be present, emerge only when\nthe neglected terms are included, or be completely forbidden.", "AI": {"tldr": "XMCD\u7684\u51e0\u4f55\u4f9d\u8d56\u6027\u53ef\u901a\u8fc7\u970d\u5c14\u77e2\u91cf\u63cf\u8ff0\uff0c\u5176\u4e0e\u53cd\u94c1\u78c1\u4f53\u4e2dN'eel\u77e2\u91cf\u7684\u5173\u7cfb\u53ef\u901a\u8fc7\u7fa4\u8bba\u5728FVS\u8fd1\u4f3c\u4e0b\u63a8\u5bfc\uff0c\u5177\u4f53\u8868\u73b0\u53d6\u51b3\u4e8e\u7cfb\u7edf\u5bf9\u79f0\u6027\u3002", "motivation": "\u7814\u7a76x\u5c04\u7ebf\u78c1\u5706\u4e8c\u8272\u6027\uff08XMCD\uff09\u5728\u4e0d\u540c\u5b9e\u9a8c\u51e0\u4f55\u5f62\u72b6\u4e0b\u7684\u4f9d\u8d56\u6027\uff0c\u5e76\u5c06\u5176\u4e0e\u53cd\u94c1\u78c1\u4f53\u7684N'eel\u77e2\u91cf\u65b9\u5411\u8054\u7cfb\u8d77\u6765\u3002", "method": "\u4f7f\u7528\u7fa4\u8bba\u63a8\u5bfc\u4e86\u970d\u5c14\u77e2\u91cf\u4e0e\u53cd\u94c1\u78c1\u4f53\u4e2dN'eel\u77e2\u91cf\u65b9\u5411\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u5e76\u5728\u81ea\u7531\u4ef7\u81ea\u65cb\uff08FVS\uff09\u8fd1\u4f3c\u4e0b\u8fdb\u884c\u4e86\u63a8\u5bfc\u3002", "result": "\u63a8\u5bfc\u4e86\u970d\u5c14\u77e2\u91cf\u4e0eN'eel\u77e2\u91cf\u65b9\u5411\u7684\u666e\u904d\u5173\u7cfb\uff0c\u5e76\u5c06\u7ed3\u679c\u63a8\u5e7f\u5230MnTe\u548cMnF2\u7684\u7279\u4f8b\u3002", "conclusion": "XMCD\u5728FVS\u8fd1\u4f3c\u4e0b\u53ef\u80fd\u5b58\u5728\u3001\u4ec5\u5728\u5305\u542b\u88ab\u5ffd\u7565\u9879\u65f6\u51fa\u73b0\u6216\u5b8c\u5168\u88ab\u7981\u6b62\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u7cfb\u7edf\u5bf9\u79f0\u6027\u3002"}}
{"id": "2508.07505", "categories": ["cs.LG", "cs.CR", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07505", "abs": "https://arxiv.org/abs/2508.07505", "authors": ["Yueyang Quan", "Chang Wang", "Shengjie Zhai", "Minghong Fang", "Zhuqing Liu"], "title": "Enhancing Privacy in Decentralized Min-Max Optimization: A Differentially Private Approach", "comment": "To appear in ACM MobiHoc 2025", "summary": "Decentralized min-max optimization allows multi-agent systems to\ncollaboratively solve global min-max optimization problems by facilitating the\nexchange of model updates among neighboring agents, eliminating the need for a\ncentral server. However, sharing model updates in such systems carry a risk of\nexposing sensitive data to inference attacks, raising significant privacy\nconcerns. To mitigate these privacy risks, differential privacy (DP) has become\na widely adopted technique for safeguarding individual data. Despite its\nadvantages, implementing DP in decentralized min-max optimization poses\nchallenges, as the added noise can hinder convergence, particularly in\nnon-convex scenarios with complex agent interactions in min-max optimization\nproblems. In this work, we propose an algorithm called DPMixSGD (Differential\nPrivate Minmax Hybrid Stochastic Gradient Descent), a novel privacy-preserving\nalgorithm specifically designed for non-convex decentralized min-max\noptimization. Our method builds on the state-of-the-art STORM-based algorithm,\none of the fastest decentralized min-max solutions. We rigorously prove that\nthe noise added to local gradients does not significantly compromise\nconvergence performance, and we provide theoretical bounds to ensure privacy\nguarantees. To validate our theoretical findings, we conduct extensive\nexperiments across various tasks and models, demonstrating the effectiveness of\nour approach.", "AI": {"tldr": "DPMixSGD\u662f\u4e00\u79cd\u7528\u4e8e\u53bb\u4e2d\u5fc3\u5316min-max\u4f18\u5316\u7684\u5dee\u5206\u9690\u79c1\u7b97\u6cd5\uff0c\u5b83\u5728\u4fdd\u62a4\u9690\u79c1\u7684\u540c\u65f6\uff0c\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u826f\u597d\u7684\u6536\u655b\u6027\u80fd\u3002", "motivation": "\u53bb\u4e2d\u5fc3\u5316min-max\u4f18\u5316\u867d\u7136\u65e0\u9700\u4e2d\u5fc3\u670d\u52a1\u5668\uff0c\u4f46\u6a21\u578b\u66f4\u65b0\u7684\u5171\u4eab\u5b58\u5728\u6cc4\u9732\u654f\u611f\u6570\u636e\u7684\u98ce\u9669\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u5f15\u5165\u5dee\u5206\u9690\u79c1\uff08DP\uff09\u6280\u672f\uff0c\u4f46DP\u53ef\u80fd\u5728\u975e\u51f8\u573a\u666f\u4e0b\u5f71\u54cd\u6536\u655b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDPMixSGD\uff08Differential Private Minmax Hybrid Stochastic Gradient Descent\uff09\u7684\u65b0\u578b\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u57fa\u4e8e\u6700\u5148\u8fdb\u7684STORM\u7b97\u6cd5\uff0c\u5e76\u4e3a\u68af\u5ea6\u6dfb\u52a0\u566a\u58f0\u4ee5\u5b9e\u73b0\u5dee\u5206\u9690\u79c1\u3002", "result": "\u7406\u8bba\u5206\u6790\u8868\u660e\uff0cDPMixSGD\u7684\u566a\u58f0\u6dfb\u52a0\u4e0d\u4f1a\u663e\u8457\u635f\u5bb3\u6536\u655b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u9690\u79c1\u4fdd\u8bc1\u7684\u7406\u8bba\u754c\u9650\u3002\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\u4e86\u8be5\u7b97\u6cd5\u5728\u5404\u79cd\u4efb\u52a1\u548c\u6a21\u578b\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDPMixSGD\u7684\u65b0\u578b\u9690\u79c1\u4fdd\u62a4\u7b97\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u975e\u51f8\u53bb\u4e2d\u5fc3\u5316min-max\u4f18\u5316\u95ee\u9898\u3002\u7814\u7a76\u8bc1\u660e\uff0c\u6dfb\u52a0\u7684\u566a\u58f0\u4e0d\u4f1a\u663e\u8457\u5f71\u54cd\u6536\u655b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4fdd\u8bc1\u9690\u79c1\u7684\u7406\u8bba\u754c\u9650\uff0c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.07033", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07033", "abs": "https://arxiv.org/abs/2508.07033", "authors": ["Shengli Zhou", "Xiangchen Wang", "Jinrui Zhang", "Ruozai Tian", "Rongtao Xu", "Feng Zheng"], "title": "$\\mathcal{P}^3$: Toward Versatile Embodied Agents", "comment": "16 pages, 8 figures", "summary": "Embodied agents have shown promising generalization capabilities across\ndiverse physical environments, making them essential for a wide range of\nreal-world applications. However, building versatile embodied agents poses\ncritical challenges due to three key issues: dynamic environment perception,\nopen-ended tool usage, and complex multi-task planning. Most previous works\nrely solely on feedback from tool agents to perceive environmental changes and\ntask status, which limits adaptability to real-time dynamics, causes error\naccumulation, and restricts tool flexibility. Furthermore, multi-task\nscheduling has received limited attention, primarily due to the inherent\ncomplexity of managing task dependencies and balancing competing priorities in\ndynamic and complex environments. To overcome these challenges, we introduce\n$\\mathcal{P}^3$, a unified framework that integrates real-time perception and\ndynamic scheduling. Specifically, $\\mathcal{P}^3$ enables 1) \\textbf Perceive\nrelevant task information actively from the environment, 2) \\textbf Plug and\nutilize any tool without feedback requirement, and 3) \\textbf Plan multi-task\nexecution based on prioritizing urgent tasks and dynamically adjusting task\norder based on dependencies. Extensive real-world experiments show that our\napproach bridges the gap between benchmarks and practical deployment,\ndelivering highly transferable, general-purpose embodied agents. Code and data\nwill be released soon.", "AI": {"tldr": "P3 \u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u5177\u8eab\u667a\u80fd\u4f53\u5728\u52a8\u6001\u73af\u5883\u611f\u77e5\u3001\u5de5\u5177\u4f7f\u7528\u548c\u591a\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u7684\u6311\u6218\u3002\u5b83\u901a\u8fc7\u4e3b\u52a8\u611f\u77e5\u3001\u5373\u63d2\u5373\u7528\u5de5\u5177\u548c\u52a8\u6001\u4efb\u52a1\u8c03\u5ea6\u6765\u6539\u8fdb\u667a\u80fd\u4f53\u7684\u6027\u80fd\uff0c\u5e76\u5728\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002", "motivation": "\u4e4b\u524d\u7684\u5177\u8eab\u667a\u80fd\u4f53\u7814\u7a76\u5728\u52a8\u6001\u73af\u5883\u611f\u77e5\u3001\u5f00\u653e\u5f0f\u5de5\u5177\u4f7f\u7528\u548c\u590d\u6742\u591a\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u4ee5\u5f80\u7684\u7814\u7a76\u4e3b\u8981\u4f9d\u8d56\u5de5\u5177\u4ee3\u7406\u7684\u53cd\u9988\u6765\u611f\u77e5\u73af\u5883\u53d8\u5316\u548c\u4efb\u52a1\u72b6\u6001\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u5b9e\u65f6\u52a8\u6001\u7684\u9002\u5e94\u6027\uff0c\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\uff0c\u5e76\u9650\u5236\u4e86\u5de5\u5177\u7684\u7075\u6d3b\u6027\u3002\u6b64\u5916\uff0c\u7531\u4e8e\u7ba1\u7406\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u548c\u5728\u52a8\u6001\u590d\u6742\u73af\u5883\u4e2d\u5e73\u8861\u7ade\u4e89\u4f18\u5148\u7ea7 inherent complexity\uff0c\u591a\u4efb\u52a1\u8c03\u5ea6\u95ee\u9898\u53d7\u5230\u7684\u5173\u6ce8\u6709\u9650\u3002", "method": "P3 \u6846\u67b6\u96c6\u6210\u4e86\u5b9e\u65f6\u611f\u77e5\u548c\u52a8\u6001\u8c03\u5ea6\u3002\u5177\u4f53\u800c\u8a00\uff0cP3 \u80fd\u591f 1) \u4e3b\u52a8\u4ece\u73af\u5883\u4e2d\u611f\u77e5\u76f8\u5173\u7684\u4efb\u52a1\u4fe1\u606f\uff0c2) \u5373\u63d2\u5373\u7528\u5730\u4f7f\u7528\u4efb\u4f55\u5de5\u5177\u800c\u65e0\u9700\u53cd\u9988\uff0c\u5e76\u4e14 3) \u57fa\u4e8e\u4f18\u5148\u5904\u7406\u7d27\u6025\u4efb\u52a1\u548c\u6839\u636e\u4f9d\u8d56\u5173\u7cfb\u52a8\u6001\u8c03\u6574\u4efb\u52a1\u987a\u5e8f\u6765\u89c4\u5212\u591a\u4efb\u52a1\u6267\u884c\u3002", "result": "\u5927\u91cf\u7684\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u8868\u660e\uff0cP3 \u65b9\u6cd5\u6709\u6548\u5730\u5f25\u5408\u4e86\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5ea6\u53ef\u8fc1\u79fb\u3001\u901a\u7528\u7684\u5177\u8eab\u667a\u80fd\u4f53\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684 P3 \u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u89e3\u51b3\u52a8\u6001\u73af\u5883\u611f\u77e5\u3001\u5f00\u653e\u5f0f\u5de5\u5177\u4f7f\u7528\u548c\u590d\u6742\u591a\u4efb\u52a1\u89c4\u5212\u8fd9\u4e09\u4e2a\u5173\u952e\u6311\u6218\uff0c\u4ece\u800c\u4e3a\u6784\u5efa\u591a\u529f\u80fd\u6027\u5177\u8eab\u667a\u80fd\u4f53\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002\u901a\u8fc7\u4e3b\u52a8\u611f\u77e5\u73af\u5883\u4fe1\u606f\u3001\u65e0\u9700\u53cd\u9988\u5373\u53ef\u4f7f\u7528\u5de5\u5177\u4ee5\u53ca\u57fa\u4e8e\u4f18\u5148\u7ea7\u548c\u52a8\u6001\u4f9d\u8d56\u5173\u7cfb\u89c4\u5212\u591a\u4efb\u52a1\u6267\u884c\uff0cP3 \u6846\u67b6\u5728\u73b0\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9ad8\u5ea6\u53ef\u8fc1\u79fb\u6027\u548c\u901a\u7528\u6027\u3002"}}
{"id": "2508.07150", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07150", "abs": "https://arxiv.org/abs/2508.07150", "authors": ["Jia-Xuan Liu", "Hai-Long Shi", "Chunfeng Wu", "Sixia Yu"], "title": "Optimal Quantum Estimation with Stabilizer-Based Local Measurements", "comment": "6 pages, 3 figures", "summary": "A central challenge in quantum metrology is to identify optimal protocols\nunder measurement constraints that reflect realistic experimental conditions,\ne.g., local measurements in multipartite scenarios. Here, we present a\nsufficient criterion for metrological schemes to saturate the quantum\nCram\\'er-Rao bound (QCRB) using local measurements, based on stabilizer\nformalism. In ideal settings, we show that graph states always admit local\nestimation protocols with an optimal estimation precision determined only by\nthe underlying graph structure. A family of graph states is identified as probe\nstates that achieve suboptimal precision scaling. In noisy environments, we\nconstruct several subspaces of probe states (mixed in general) that not only\nsaturate the QCRB with local measurements but also maintain approximately\ninvariant precision scaling. These subspaces offer multiple metrological\nadvantages, including high precision, partial noise resilience, and\nnoise-correcting capability prior to parameter encoding. Under dephasing noise,\nthey exhibit markedly better performance than Greenberger-Horne-Zeilinger\nstates. Our results advance the framework for local-measurement quantum\nmetrology, achieving a robust trade-off between precision and noise tolerance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u7a33\u5b9a\u5668\u5f62\u5f0f\u4e3b\u4e49\u7684\u65b9\u6cd5\uff0c\u53ef\u5728\u5c40\u90e8\u6d4b\u91cf\u4e0b\u5b9e\u73b0\u6700\u4f18\u91cf\u5b50\u8ba1\u91cf\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u6240\u63d0\u51fa\u7684\u63a2\u9488\u6001\u5b50\u7a7a\u95f4\u6bd4GHZ\u6001\u66f4\u5177\u9c81\u68d2\u6027\u3002", "motivation": "\u91cf\u5b50\u8ba1\u91cf\u7684\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\u662f\u5728\u53cd\u6620\u5b9e\u9645\u5b9e\u9a8c\u6761\u4ef6\u7684\u6d4b\u91cf\u7ea6\u675f\u4e0b\uff0c\u8bc6\u522b\u6700\u4f18\u7684\u91cf\u5b50\u8ba1\u91cf\u65b9\u6848\uff0c\u4f8b\u5982\u5728\u591a\u4f53\u573a\u666f\u4e0b\u7684\u5c40\u90e8\u6d4b\u91cf\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\uff0c\u63d0\u51fa\u80fd\u5728\u5c40\u90e8\u6d4b\u91cf\u4e0b\u9971\u548c\u91cf\u5b50\u514b\u62c9\u7f8e-Rao\u754c\u7684\u65b9\u6848\u3002", "method": "\u672c\u7814\u7a76\u57fa\u4e8e\u7a33\u5b9a\u5668\u5f62\u5f0f\u4e3b\u4e49\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5145\u5206\u6761\u4ef6\uff0c\u7528\u4e8e\u5224\u65ad\u91cf\u5b50\u8ba1\u91cf\u65b9\u6848\u662f\u5426\u80fd\u5728\u5c40\u90e8\u6d4b\u91cf\u7ea6\u675f\u4e0b\u9971\u548c\u91cf\u5b50\u514b\u62c9\u7f8e-Rao\u754c\uff08QCRB\uff09\u3002\u7814\u7a76\u5206\u6790\u4e86\u56fe\u6001\u5728\u7406\u60f3\u548c\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5e76\u6784\u5efa\u4e86\u80fd\u5728\u566a\u58f0\u73af\u5883\u4e0b\u9971\u548cQCRB\u5e76\u4fdd\u6301\u7cbe\u5ea6\u6807\u5ea6\u4e0d\u53d8\u7684\u63a2\u9488\u6001\u5b50\u7a7a\u95f4\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u56fe\u6001\u5728\u7406\u60f3\u60c5\u51b5\u4e0b\u53ef\u4ee5\u901a\u8fc7\u5c40\u90e8\u6d4b\u91cf\u5b9e\u73b0\u6700\u4f18\u4f30\u8ba1\u7cbe\u5ea6\uff0c\u7cbe\u5ea6\u7531\u56fe\u7ed3\u6784\u51b3\u5b9a\u3002\u8fd8\u53d1\u73b0\u4e86\u4e00\u7c7b\u63a2\u9488\u6001\u5177\u6709\u6b21\u4f18\u7cbe\u5ea6\u6807\u5ea6\u3002\u5728\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u6784\u5efa\u7684\u63a2\u9488\u6001\u5b50\u7a7a\u95f4\u4e0d\u4ec5\u80fd\u9971\u548cQCRB\uff0c\u8fd8\u80fd\u4fdd\u6301\u7cbe\u5ea6\u6807\u5ea6\u4e0d\u53d8\uff0c\u5e76\u8868\u73b0\u51fa\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8eGHZ\u6001\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u7a33\u5b9a\u5668\u5f62\u5f0f\u4e3b\u4e49\u7684\u5145\u5206\u6761\u4ef6\uff0c\u7528\u4e8e\u5728\u5b58\u5728\u5c40\u90e8\u6d4b\u91cf\u7ea6\u675f\u7684\u91cf\u5b50\u8ba1\u91cf\u65b9\u6848\u4e2d\u5b9e\u73b0\u91cf\u5b50\u514b\u62c9\u7f8e-Rao\u754c\uff08QCRB\uff09\u3002\u7814\u7a76\u8868\u660e\uff0c\u5728\u7406\u60f3\u60c5\u51b5\u4e0b\uff0c\u56fe\u6001\u603b\u662f\u5141\u8bb8\u5c40\u90e8\u4f30\u8ba1\u534f\u8bae\uff0c\u5176\u6700\u4f18\u4f30\u8ba1\u7cbe\u5ea6\u4ec5\u7531\u57fa\u7840\u56fe\u7ed3\u6784\u51b3\u5b9a\u3002\u8fd8\u786e\u5b9a\u4e86\u4e00\u7c7b\u5b9e\u73b0\u6b21\u4f18\u7cbe\u5ea6\u6807\u5ea6\u7684\u56fe\u6001\u4f5c\u4e3a\u63a2\u9488\u6001\u3002\u5728\u566a\u58f0\u73af\u5883\u4e0b\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u63a2\u9488\u6001\u7684\u51e0\u4e2a\u5b50\u7a7a\u95f4\uff08\u901a\u5e38\u662f\u6df7\u5408\u6001\uff09\uff0c\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u4e0d\u4ec5\u53ef\u4ee5\u7528\u5c40\u90e8\u6d4b\u91cf\u9971\u548cQCRB\uff0c\u8fd8\u80fd\u4fdd\u6301\u8fd1\u4f3c\u4e0d\u53d8\u7684\u7cbe\u5ea6\u6807\u5ea6\u3002\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u63d0\u4f9b\u4e86\u591a\u79cd\u8ba1\u91cf\u4f18\u52bf\uff0c\u5305\u62ec\u9ad8\u7cbe\u5ea6\u3001\u90e8\u5206\u6297\u566a\u58f0\u80fd\u529b\u4ee5\u53ca\u5728\u53c2\u6570\u7f16\u7801\u524d\u8fdb\u884c\u566a\u58f0\u6821\u6b63\u7684\u80fd\u529b\u3002\u5728\u9000\u76f8\u5e72\u566a\u58f0\u4e0b\uff0c\u4e0eGreenberger-Horne-Zeilinger\u6001\u76f8\u6bd4\uff0c\u8fd9\u4e9b\u5b50\u7a7a\u95f4\u8868\u73b0\u51fa\u660e\u663e\u66f4\u597d\u7684\u6027\u80fd\u3002\u7814\u7a76\u7ed3\u679c\u63a8\u8fdb\u4e86\u5c40\u90e8\u6d4b\u91cf\u91cf\u5b50\u8ba1\u91cf\u7684\u6846\u67b6\uff0c\u5b9e\u73b0\u4e86\u7cbe\u5ea6\u548c\u566a\u58f0\u5bb9\u5fcd\u5ea6\u4e4b\u95f4\u7684\u7a33\u5065\u6743\u8861\u3002"}}
{"id": "2508.06880", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06880", "abs": "https://arxiv.org/abs/2508.06880", "authors": ["Philipp Christmann", "Gerhard Weikum"], "title": "The ReQAP System for Question Answering over Personal Information", "comment": "Accepted at CIKM 2025 (demonstration paper)", "summary": "Personal information is abundant on users' devices, from structured data in\ncalendar, shopping records or fitness tools, to unstructured contents in mail\nand social media posts. This works presents the ReQAP system that supports\nusers with answers for complex questions that involve filters, joins and\naggregation over heterogeneous sources. The unique trait of ReQAP is that it\nrecursively decomposes questions and incrementally builds an operator tree for\nexecution. Both the question interpretation and the individual operators make\nsmart use of light-weight language models, with judicious fine-tuning. The demo\nshowcases the rich functionality for advanced user questions, and also offers\ndetailed tracking of how the answers are computed by the operators in the\nexecution tree. Being able to trace answers back to the underlying sources is\nvital for human comprehensibility and user trust in the system.", "AI": {"tldr": "ReQAP\u662f\u4e00\u4e2a\u65b0\u7cfb\u7edf\uff0c\u53ef\u4ee5\u56de\u7b54\u6d89\u53ca\u8de8\u591a\u79cd\u6570\u636e\u6e90\u7684\u590d\u6742\u95ee\u9898\u3002\u5b83\u901a\u8fc7\u9012\u5f52\u5206\u89e3\u95ee\u9898\u5e76\u4f7f\u7528\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\u6765\u6784\u5efa\u548c\u6267\u884c\u67e5\u8be2\u8ba1\u5212\u3002\u8be5\u7cfb\u7edf\u7684\u4e00\u4e2a\u5173\u952e\u7279\u6027\u662f\u80fd\u591f\u8ffd\u6eaf\u7b54\u6848\u7684\u6765\u6e90\uff0c\u589e\u5f3a\u7528\u6237\u5bf9\u7cfb\u7edf\u7684\u4fe1\u4efb\u548c\u7406\u89e3\u3002", "motivation": "\u4e2a\u4eba\u4fe1\u606f\u5728\u7528\u6237\u7684\u8bbe\u5907\u4e0a\u975e\u5e38\u4e30\u5bcc\uff0c\u4ece\u65e5\u5386\u3001\u8d2d\u7269\u8bb0\u5f55\u6216\u5065\u8eab\u5de5\u5177\u4e2d\u7684\u7ed3\u6784\u5316\u6570\u636e\uff0c\u5230\u90ae\u4ef6\u548c\u793e\u4ea4\u5a92\u4f53\u5e16\u5b50\u4e2d\u7684\u975e\u7ed3\u6784\u5316\u5185\u5bb9\u3002", "method": "ReQAP\u7cfb\u7edf\u9012\u5f52\u5730\u5206\u89e3\u95ee\u9898\u5e76\u9010\u6b65\u6784\u5efa\u7b97\u5b50\u6811\u8fdb\u884c\u6267\u884c\u3002\u95ee\u9898\u89e3\u91ca\u548c\u5355\u4e2a\u7b97\u5b50\u90fd\u5de7\u5999\u5730\u5229\u7528\u4e86\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u660e\u667a\u7684\u5fae\u8c03\u3002", "result": "\u6f14\u793a\u5c55\u793a\u4e86\u9488\u5bf9\u9ad8\u7ea7\u7528\u6237\u95ee\u9898\u7684\u4e30\u5bcc\u529f\u80fd\uff0c\u5e76\u8be6\u7ec6\u8ddf\u8e2a\u4e86\u7b54\u6848\u662f\u5982\u4f55\u901a\u8fc7\u6267\u884c\u6811\u4e2d\u7684\u7b97\u5b50\u8ba1\u7b97\u51fa\u6765\u7684\u3002\u80fd\u591f\u5c06\u7b54\u6848\u8ffd\u6eaf\u5230\u5e95\u5c42\u6e90\u5bf9\u4e8e\u4eba\u7c7b\u7684\u53ef\u7406\u89e3\u6027\u548c\u7528\u6237\u5bf9\u7cfb\u7edf\u7684\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "ReQAP\u7cfb\u7edf\u652f\u6301\u7528\u6237\u5bf9\u8de8\u5f02\u6784\u6e90\u7684\u590d\u6742\u95ee\u9898\uff08\u6d89\u53ca\u7b5b\u9009\u3001\u8054\u63a5\u548c\u805a\u5408\uff09\u8fdb\u884c\u56de\u7b54\u3002ReQAP\u9012\u5f52\u5730\u5206\u89e3\u95ee\u9898\u5e76\u9010\u6b65\u6784\u5efa\u7b97\u5b50\u6811\u8fdb\u884c\u6267\u884c\u3002\u95ee\u9898\u89e3\u91ca\u548c\u5355\u4e2a\u7b97\u5b50\u90fd\u5de7\u5999\u5730\u5229\u7528\u4e86\u8f7b\u91cf\u7ea7\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u4e86\u660e\u667a\u7684\u5fae\u8c03\u3002\u8be5\u6f14\u793a\u5c55\u793a\u4e86\u9488\u5bf9\u9ad8\u7ea7\u7528\u6237\u95ee\u9898\u7684\u4e30\u5bcc\u529f\u80fd\uff0c\u5e76\u8be6\u7ec6\u8ddf\u8e2a\u4e86\u7b54\u6848\u662f\u5982\u4f55\u901a\u8fc7\u6267\u884c\u6811\u4e2d\u7684\u7b97\u5b50\u8ba1\u7b97\u51fa\u6765\u7684\u3002\u80fd\u591f\u5c06\u7b54\u6848\u8ffd\u6eaf\u5230\u5e95\u5c42\u6e90\u5bf9\u4e8e\u4eba\u7c7b\u7684\u53ef\u7406\u89e3\u6027\u548c\u7528\u6237\u5bf9\u7cfb\u7edf\u7684\u4fe1\u4efb\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.06565", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06565", "abs": "https://arxiv.org/abs/2508.06565", "authors": ["Jing Zhang", "Xiaowei Yu", "Minheng Chen", "Lu Zhang", "Tong Chen", "Yan Zhuang", "Chao Cao", "Yanjun Lyu", "Li Su", "Tianming Liu", "Dajiang Zhu"], "title": "Bridging Brain Connectomes and Clinical Reports for Early Alzheimer's Disease Diagnosis", "comment": null, "summary": "Integrating brain imaging data with clinical reports offers a valuable\nopportunity to leverage complementary multimodal information for more effective\nand timely diagnosis in practical clinical settings. This approach has gained\nsignificant attention in brain disorder research, yet a key challenge remains:\nhow to effectively link objective imaging data with subjective text-based\nreports, such as doctors' notes. In this work, we propose a novel framework\nthat aligns brain connectomes with clinical reports in a shared cross-modal\nlatent space at both the subject and connectome levels, thereby enhancing\nrepresentation learning. The key innovation of our approach is that we treat\nbrain subnetworks as tokens of imaging data, rather than raw image patches, to\nalign with word tokens in clinical reports. This enables a more efficient\nidentification of system-level associations between neuroimaging findings and\nclinical observations, which is critical since brain disorders often manifest\nas network-level abnormalities rather than isolated regional alterations. We\napplied our method to mild cognitive impairment (MCI) using the ADNI dataset.\nOur approach not only achieves state-of-the-art predictive performance but also\nidentifies clinically meaningful connectome-text pairs, offering new insights\ninto the early mechanisms of Alzheimer's disease and supporting the development\nof clinically useful multimodal biomarkers.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5927\u8111\u5b50\u7f51\u7edc\u89c6\u4e3a\u6807\u8bb0\u6765\u5bf9\u9f50\u5927\u8111\u8fde\u63a5\u7ec4\u548c\u4e34\u5e8a\u62a5\u544a\uff0c\u4ee5\u5b9e\u73b0\u66f4\u597d\u7684\u8bca\u65ad\u3002\u8be5\u65b9\u6cd5\u5728MCI\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8bc6\u522b\u51fa\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u8fde\u63a5\u7ec4-\u6587\u672c\u5bf9\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c1\u89e3\u3002", "motivation": "\u6574\u5408\u8111\u6210\u50cf\u6570\u636e\u548c\u4e34\u5e8a\u62a5\u544a\u53ef\u4ee5\u5229\u7528\u4e92\u8865\u7684\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u4ece\u800c\u5728\u5b9e\u9645\u4e34\u5e8a\u73af\u5883\u4e2d\u5b9e\u73b0\u66f4\u6709\u6548\u3001\u53ca\u65f6\u7684\u8bca\u65ad\u3002\u7136\u800c\uff0c\u5c06\u5ba2\u89c2\u7684\u6210\u50cf\u6570\u636e\u4e0e\u57fa\u4e8e\u6587\u672c\u7684\u4e34\u5e8a\u62a5\u544a\uff08\u5982\u533b\u751f\u7b14\u8bb0\uff09\u6709\u6548\u5730\u5173\u8054\u8d77\u6765\u4ecd\u7136\u662f\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5c06\u5927\u8111\u8fde\u63a5\u7ec4\u4e0e\u4e34\u5e8a\u62a5\u544a\u5728\u5171\u4eab\u7684\u8de8\u6a21\u6001\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5bf9\u9f50\uff0c\u5b9e\u73b0\u4e86\u4e3b\u9898\u548c\u8fde\u63a5\u7ec4\u5c42\u9762\u7684\u8054\u5408\u8868\u793a\u5b66\u4e60\u3002\u8be5\u65b9\u6cd5\u5c06\u5927\u8111\u5b50\u7f51\u7edc\u89c6\u4e3a\u6210\u50cf\u6570\u636e\u7684\u6807\u8bb0\uff0c\u4ee5\u5339\u914d\u4e34\u5e8a\u62a5\u544a\u4e2d\u7684\u5355\u8bcd\u6807\u8bb0\uff0c\u4ece\u800c\u66f4\u6709\u6548\u5730\u8bc6\u522b\u795e\u7ecf\u5f71\u50cf\u5b66\u53d1\u73b0\u4e0e\u4e34\u5e8a\u89c2\u5bdf\u4e4b\u95f4\u7684\u7cfb\u7edf\u7ea7\u5173\u8054\u3002", "result": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u5728\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u7684ADNI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u8fd8\u8bc6\u522b\u51fa\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u8fde\u63a5\u7ec4-\u6587\u672c\u5bf9\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08Alzheimer's disease\uff09\u7684\u65e9\u671f\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u652f\u6301\u4e86\u4e34\u5e8a\u4e0a\u6709\u7528\u7684\u591a\u6a21\u6001\u751f\u7269\u6807\u5fd7\u7269\u7684\u5f00\u53d1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u8f7b\u5ea6\u8ba4\u77e5\u969c\u788d\uff08MCI\uff09\u7684ADNI\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u8bc6\u522b\u51fa\u5177\u6709\u4e34\u5e8a\u610f\u4e49\u7684\u8fde\u63a5\u7ec4-\u6587\u672c\u5bf9\uff0c\u4e3a\u963f\u5c14\u8328\u6d77\u9ed8\u75c5\uff08Alzheimer's disease\uff09\u7684\u65e9\u671f\u673a\u5236\u63d0\u4f9b\u4e86\u65b0\u89c1\u89e3\uff0c\u5e76\u652f\u6301\u4e86\u4e34\u5e8a\u4e0a\u6709\u7528\u7684\u591a\u6a21\u6001\u751f\u7269\u6807\u5fd7\u7269\u7684\u5f00\u53d1\u3002"}}
{"id": "2508.08153", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.08153", "abs": "https://arxiv.org/abs/2508.08153", "authors": ["Changrui Liu", "Anil Alan", "Shengling Shi", "Bart De Schutter"], "title": "Robust Adaptive Discrete-Time Control Barrier Certificate", "comment": "10 pages, 2 figures, submitted to Automatica as a brief paper", "summary": "This work develops a robust adaptive control strategy for discrete-time\nsystems using Control Barrier Functions (CBFs) to ensure safety under\nparametric model uncertainty and disturbances. A key contribution of this work\nis establishing a barrier function certificate in discrete time for general\nonline parameter estimation algorithms. This barrier function certificate\nguarantees positive invariance of the safe set despite disturbances and\nparametric uncertainty without access to the true system parameters. In\naddition, real-time implementation and inherent robustness guarantees are\nprovided. Our approach demonstrates that, using the proposed robust adaptive\nCBF framework, the parameter estimation module can be designed separately from\nthe CBF-based safety filter, simplifying the development of safe adaptive\ncontrollers for discrete-time systems. The resulting safety filter guarantees\nthat the system remains within the safe set while adapting to model\nuncertainties, making it a promising strategy for real-world applications\ninvolving discrete-time safety-critical systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7684\u9c81\u68d2\u81ea\u9002\u5e94CBF\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u548c\u5e72\u6270\uff0c\u5e76\u7b80\u5316\u4e86\u5b89\u5168\u81ea\u9002\u5e94\u63a7\u5236\u5668\u7684\u8bbe\u8ba1\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u5728\u53c2\u6570\u6a21\u578b\u4e0d\u786e\u5b9a\u548c\u5e72\u6270\u4e0b\u7684\u5b89\u5168\u6027\uff0c\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u79cd\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\u7b56\u7565\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBFs\uff09\u7684\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\u7b56\u7565\uff0c\u5e76\u4e3a\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u4e2d\u7684\u4e00\u822c\u5728\u7ebf\u53c2\u6570\u4f30\u8ba1\u7b97\u6cd5\u5efa\u7acb\u4e86\u969c\u788d\u51fd\u6570\u8bc1\u4e66\u3002", "result": "\u8be5\u7814\u7a76\u6210\u529f\u5730\u8bc1\u660e\u4e86\u5176\u63d0\u51fa\u7684\u9c81\u68d2\u81ea\u9002\u5e94CBF\u6846\u67b6\u53ef\u4ee5\u4fdd\u8bc1\u7cfb\u7edf\u4fdd\u6301\u5728\u5b89\u5168\u96c6\u5185\uff0c\u540c\u65f6\u9002\u5e94\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u5f00\u53d1\u4e86\u4e00\u79cd\u9c81\u68d2\u81ea\u9002\u5e94\u63a7\u5236\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u4f7f\u7528\u63a7\u5236\u969c\u788d\u51fd\u6570\uff08CBFs\uff09\u6765\u786e\u4fdd\u5728\u53c2\u6570\u6a21\u578b\u4e0d\u786e\u5b9a\u548c\u5e72\u6270\u4e0b\u7684\u5b89\u5168\u6027\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5efa\u7acb\u79bb\u6563\u65f6\u95f4\u4e2d\u7684\u969c\u788d\u51fd\u6570\u8bc1\u4e66\u6765\u5b9e\u73b0\u8fd9\u4e00\u70b9\uff0c\u5373\u4f7f\u5728\u6ca1\u6709\u771f\u5b9e\u7cfb\u7edf\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u80fd\u4fdd\u8bc1\u5b89\u5168\u96c6\u4e0d\u53d7\u5e72\u6270\u548c\u53c2\u6570\u4e0d\u786e\u5b9a\u7684\u5f71\u54cd\u3002\u8be5\u7814\u7a76\u8fd8\u63d0\u4f9b\u4e86\u5b9e\u65f6\u5b9e\u73b0\u7684\u9c81\u68d2\u6027\u4fdd\u8bc1\uff0c\u5e76\u8bc1\u660e\u4e86\u53c2\u6570\u4f30\u8ba1\u6a21\u5757\u53ef\u4ee5\u4e0e\u57fa\u4e8eCBF\u7684\u5b89\u5168\u6ee4\u6ce2\u5668\u5206\u5f00\u8bbe\u8ba1\uff0c\u4ece\u800c\u7b80\u5316\u4e86\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u7684\u5b89\u5168\u81ea\u9002\u5e94\u63a7\u5236\u5668\u7684\u5f00\u53d1\u3002"}}
{"id": "2508.07888", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07888", "abs": "https://arxiv.org/abs/2508.07888", "authors": ["Eoin Dolan", "Zhendong Chi", "Haozhe Yang", "Luis E. Hueso", "F\u00e8lix Casanova"], "title": "Gate tunable spin-charge interconversion in a graphene/ReS$_{2}$ heterostructure up to room temperature", "comment": "6 pages, 4 figures, and Supplemental Material. Marie\n  Sk{\\l}odowska-Curie Actions, H2020-MSCA-ITN-2020; Project acronym SPEAR;\n  Grant Agreement No. 955671", "summary": "Graphene is a material with great potential in the field of spintronics,\ncombining good conductivity with low spin--orbit coupling (SOC), which allows\nfor the transport of spin currents over long distances. However, this lack of\nSOC also limits the capacity for manipulating spin current. A key strategy to\naddress this limitation is to induce SOC in graphene via proximity to other\ntwo-dimensional (2D) materials. Such proximity-induced SOC can enable\nspin--charge interconversion (SCI) in graphene, with potential applications in\nnext-generation logic devices. Here, we place graphene in close proximity to\nthe room-temperature ferroelectric candidate ReS$_\\mathrm{2}$, inducing SCI for\nboth in-plane and out-of-plane polarized spin current. We attribute the SCI for\nin-plane polarized current to either the Rashba--Edelstein effect (REE) or the\nunconventional spin Hall effect (SHE) at the graphene/ReS$_\\mathrm{2}$\ninterface, and the SCI for out-of-plane polarized current to either the\nconventional SHE in the proximitised graphene, or the unconventional SHE in the\nbulk of the ReS$_\\mathrm{2}$. SCI due to in-plane spin is characterised over a\nwide range of temperature, up to 300 K and a range of gate voltages.", "AI": {"tldr": "\u901a\u8fc7\u5c06\u77f3\u58a8\u70ef\u4e0eReS2\u9760\u8fd1\uff0c\u53ef\u4ee5\u5b9e\u73b0\u77f3\u58a8\u70ef\u4e2d\u7684\u81ea\u65cb-\u7535\u8377\u8f6c\u6362\uff08SCI\uff09\uff0c\u5e76\u53ef\u5728\u5ba4\u6e29\u4e0b\u8fdb\u884c\u8868\u5f81\uff0c\u8fd9\u4e3a\u4e0b\u4e00\u4ee3\u903b\u8f91\u5668\u4ef6\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u77f3\u58a8\u70ef\u4e2d\u7f3a\u4e4f\u81ea\u65cb-\u8f68\u9053\u8026\u5408\uff08SOC\uff09\u9650\u5236\u5176\u81ea\u65cb\u6d41\u64cd\u63a7\u80fd\u529b\u7684\u7f3a\u70b9\uff0c\u5229\u7528\u90bb\u8fd1\u8bf1\u5bfcSOC\u662f\u5173\u952e\u7b56\u7565\uff0c\u4ee5\u5b9e\u73b0\u81ea\u65cb-\u7535\u8377\u8f6c\u6362\uff08SCI\uff09\uff0c\u5e76\u5e94\u7528\u4e8e\u4e0b\u4e00\u4ee3\u903b\u8f91\u5668\u4ef6\u3002", "method": "\u901a\u8fc7\u5c06\u77f3\u58a8\u70ef\u7f6e\u4e8eReS2\u8fd1\u65c1\uff0c\u5e76\u5bf9\u4e0d\u540c\u504f\u632f\u7684\u81ea\u65cb\u6d41\u8fdb\u884cSCI\u8868\u5f81\uff0c\u7814\u7a76\u4e86\u5176\u5728\u4e0d\u540c\u6e29\u5ea6\u548c\u6805\u6781\u7535\u538b\u4e0b\u7684\u884c\u4e3a\u3002", "result": "\u5b9e\u73b0\u4e86\u77f3\u58a8\u70ef\u4e2d\u5e73\u9762\u5185\u548c\u9762\u5916\u504f\u632f\u81ea\u65cb\u6d41\u7684SCI\u3002\u5e73\u9762\u5185\u504f\u632f\u7684SCI\u5f52\u56e0\u4e8e\u77f3\u58a8\u70ef/ReS2\u754c\u9762\u7684Rashba-Edelstein\u6548\u5e94\uff08REE\uff09\u6216\u975e\u5e38\u89c4\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff08SHE\uff09\uff1b\u9762\u5916\u504f\u632f\u7684SCI\u5f52\u56e0\u4e8e\u90bb\u8fd1\u77f3\u58a8\u70ef\u4e2d\u7684\u5e38\u89c4SHE\uff0c\u6216ReS2\u4f53\u76f8\u4e2d\u7684\u975e\u5e38\u89c4SHE\u3002SCI\u5728\u9ad8\u8fbe300 K\u7684\u6e29\u5ea6\u548c\u4e00\u5b9a\u7684\u6805\u6781\u7535\u538b\u8303\u56f4\u5185\u5747\u53ef\u8868\u5f81\u3002", "conclusion": "\u5c06\u77f3\u58a8\u70ef\u7f6e\u4e8e\u5ba4\u6e29\u94c1\u7535\u4f53\u5019\u9009\u7269ReS2\u7684\u8fd1\u65c1\uff0c\u53ef\u8bf1\u5bfc\u77f3\u58a8\u70ef\u4e2d\u7684\u81ea\u65cb-\u7535\u8377\u8f6c\u6362\uff08SCI\uff09\uff0c\u53ef\u7528\u4e8e\u4e0b\u4e00\u4ee3\u903b\u8f91\u5668\u4ef6\u3002"}}
{"id": "2508.07651", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07651", "abs": "https://arxiv.org/abs/2508.07651", "authors": ["Ziye Jia", "Yian Zhu", "Qihui Wu", "Lei Zhang", "Sen Yang", "Zhu Han"], "title": "Remote ID Based UAV Collision Avoidance Optimization for Low-Altitude Airspace Safety", "comment": null, "summary": "With the rapid development of unmanned aerial vehicles (UAVs), it is\nparamount to ensure safe and efficient operations in open airspaces. The remote\nidentification (Remote ID) is deemed an effective real-time UAV monitoring\nsystem by the federal aviation administration, which holds potentials for\nenabling inter-UAV communications. This paper deeply investigates the\napplication of Remote ID for UAV collision avoidance while minimizing\ncommunication delays. First, we propose a Remote ID based distributed multi-UAV\ncollision avoidance (DMUCA) framework to support the collision detection,\navoidance decision-making, and trajectory recovery. Next, the average\ntransmission delays for Remote ID messages are analyzed, incorporating the\npacket reception mechanisms and packet loss due to interference. The\noptimization problem is formulated to minimize the long-term average\ncommunication delay, where UAVs can flexibly select the Remote ID protocol to\nenhance the collision avoidance performance. To tackle the problem, we design a\nmulti-agent deep Q-network based adaptive communication configuration\nalgorithm, allowing UAVs to autonomously learn the optimal protocol\nconfigurations in dynamic environments. Finally, numerical results verify the\nfeasibility of the proposed DMUCA framework, and the proposed mechanism can\nreduce the average delay by 32% compared to the fixed protocol configuration.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fdc\u7a0b\u8bc6\u522b\u7684\u5206\u5e03\u5f0f\u591aUAV\u9632\u78b0\u649e\u6846\u67b6\uff08DMUCA\uff09\uff0c\u5e76\u901a\u8fc7\u4e00\u79cd\u65b0\u7684\u81ea\u9002\u5e94\u901a\u4fe1\u914d\u7f6e\u7b97\u6cd5\uff08\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6Q\u7f51\u7edc\uff09\u6765\u4f18\u5316\u901a\u4fe1\u5ef6\u8fdf\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u80fd\u663e\u8457\u964d\u4f4e\u901a\u4fe1\u5ef6\u8fdf\uff08\u51cf\u5c1132%\uff09\uff0c\u63d0\u9ad8\u4e86\u78b0\u649e\u907f\u514d\u7684\u6548\u7387\u3002", "motivation": "\u968f\u7740\u65e0\u4eba\u673a\uff08UAV\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u786e\u4fdd\u5728\u5f00\u653e\u7a7a\u57df\u4e2d\u7684\u5b89\u5168\u9ad8\u6548\u8fd0\u884c\u81f3\u5173\u91cd\u8981\u3002\u8fdc\u7a0b\u8bc6\u522b\uff08Remote ID\uff09\u88ab\u8ba4\u4e3a\u662f\u6709\u6548\u7684\u5b9e\u65f6UAV\u76d1\u63a7\u7cfb\u7edf\uff0c\u5e76\u6709\u6f5c\u529b\u5b9e\u73b0UAV\u4e4b\u95f4\u7684\u901a\u4fe1\u3002\u672c\u7814\u7a76\u65e8\u5728\u5e94\u7528\u8fdc\u7a0b\u8bc6\u522b\u6280\u672f\u6765\u907f\u514dUAV\u78b0\u649e\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u901a\u4fe1\u5ef6\u8fdf\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fdc\u7a0b\u8bc6\u522b\u7684\u5206\u5e03\u5f0f\u591aUAV\u9632\u78b0\u649e\uff08DMUCA\uff09\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u78b0\u649e\u68c0\u6d4b\u3001\u907f\u514d\u51b3\u7b56\u548c\u8f68\u8ff9\u6062\u590d\u3002\u5206\u6790\u4e86\u5e73\u5747\u4f20\u8f93\u5ef6\u8fdf\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u4f18\u5316\u95ee\u9898\u6765\u6700\u5c0f\u5316\u957f\u671f\u5e73\u5747\u901a\u4fe1\u5ef6\u8fdf\u3002\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6Q\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u901a\u4fe1\u914d\u7f6e\u7b97\u6cd5\u6765\u89e3\u51b3\u8be5\u95ee\u9898\u3002", "result": "\u6570\u503c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684DMUCA\u6846\u67b6\u7684\u53ef\u884c\u6027\uff0c\u5e76\u8868\u660e\u6240\u63d0\u51fa\u7684\u673a\u5236\u4e0e\u56fa\u5b9a\u7684\u534f\u8bae\u914d\u7f6e\u76f8\u6bd4\uff0c\u5e73\u5747\u5ef6\u8fdf\u51cf\u5c11\u4e8632%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u8fdc\u7a0b\u8bc6\u522b\u7684\u5206\u5e03\u5f0f\u591a\u65e0\u4eba\u673a\uff08UAV\uff09\u9632\u78b0\u649e\uff08DMUCA\uff09\u6846\u67b6\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u6df1\u5ea6Q\u7f51\u7edc\u7684\u81ea\u9002\u5e94\u901a\u4fe1\u914d\u7f6e\u7b97\u6cd5\uff0c\u4ee5\u6700\u5c0f\u5316\u901a\u4fe1\u5ef6\u8fdf\u5e76\u63d0\u9ad8\u78b0\u649e\u907f\u514d\u6027\u80fd\u3002\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u673a\u5236\u4e0e\u56fa\u5b9a\u7684\u534f\u8bae\u914d\u7f6e\u76f8\u6bd4\uff0c\u5e73\u5747\u5ef6\u8fdf\u51cf\u5c11\u4e8632%\u3002"}}
{"id": "2508.06931", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06931", "abs": "https://arxiv.org/abs/2508.06931", "authors": ["Wangyue Lu", "Lun Du", "Sirui Li", "Ke Weng", "Haozhe Sun", "Hengyu Liu", "Minghe Yu", "Tiancheng Zhang", "Ge Yu"], "title": "Automated Formalization via Conceptual Retrieval-Augmented LLMs", "comment": null, "summary": "Interactive theorem provers (ITPs) require manual formalization, which is\nlabor-intensive and demands expert knowledge. While automated formalization\noffers a potential solution, it faces two major challenges: model hallucination\n(e.g., undefined predicates, symbol misuse, and version incompatibility) and\nthe semantic gap caused by ambiguous or missing premises in natural language\ndescriptions. To address these issues, we propose CRAMF, a Concept-driven\nRetrieval-Augmented Mathematical Formalization framework. CRAMF enhances\nLLM-based autoformalization by retrieving formal definitions of core\nmathematical concepts, providing contextual grounding during code generation.\nHowever, applying retrieval-augmented generation (RAG) in this setting is\nnon-trivial due to the lack of structured knowledge bases, the polymorphic\nnature of mathematical concepts, and the high precision required in formal\nretrieval. We introduce a framework for automatically constructing a\nconcept-definition knowledge base from Mathlib4, the standard mathematical\nlibrary for the Lean 4 theorem prover, indexing over 26,000 formal definitions\nand 1,000+ core mathematical concepts. To address conceptual polymorphism, we\npropose contextual query augmentation with domain- and application-level\nsignals. In addition, we design a dual-channel hybrid retrieval strategy with\nreranking to ensure accurate and relevant definition retrieval. Experiments on\nminiF2F, ProofNet, and our newly proposed AdvancedMath benchmark show that\nCRAMF can be seamlessly integrated into LLM-based autoformalizers, yielding\nconsistent improvements in translation accuracy, achieving up to 62.1% and an\naverage of 29.9% relative improvement.", "AI": {"tldr": "CRAMF\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u548c\u4e0a\u4e0b\u6587\u589e\u5f3a\uff0c\u89e3\u51b3\u4e86\u81ea\u52a8\u6570\u5b66\u5f62\u5f0f\u5316\u7684\u96be\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u51c6\u786e\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ea4\u4e92\u5f0f\u5b9a\u7406\u8bc1\u660e\u5668\uff08ITPs\uff09\u5728\u624b\u52a8\u5f62\u5f0f\u5316\u65b9\u9762\u8017\u65f6\u4e14\u9700\u8981\u4e13\u4e1a\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u81ea\u52a8\u5f62\u5f0f\u5316\u65b9\u6cd5\u9762\u4e34\u7684\u6a21\u578b\u5e7b\u89c9\u548c\u8bed\u4e49\u9e3f\u6c9f\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCRAMF\uff08\u6982\u5ff5\u9a71\u52a8\u68c0\u7d22\u589e\u5f3a\u6570\u5b66\u5f62\u5f0f\u5316\uff09\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u68c0\u7d22\u5f62\u5f0f\u5316\u5b9a\u4e49\u6765\u589e\u5f3a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b26,000\u591a\u4e2a\u5f62\u5f0f\u5316\u5b9a\u4e49\u548c1,000\u591a\u4e2a\u6838\u5fc3\u6570\u5b66\u6982\u5ff5\u7684\u77e5\u8bc6\u5e93\uff0c\u91c7\u7528\u4e0a\u4e0b\u6587\u67e5\u8be2\u589e\u5f3a\u548c\u53cc\u901a\u9053\u6df7\u5408\u68c0\u7d22\u7b56\u7565\u6765\u5904\u7406\u6982\u5ff5\u591a\u6001\u6027\u548c\u63d0\u9ad8\u68c0\u7d22\u7cbe\u5ea6\u3002", "result": "CRAMF\u53ef\u4ee5\u65e0\u7f1d\u96c6\u6210\u5230\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5f62\u5f0f\u5316\u5668\u4e2d\uff0c\u5728\u7ffb\u8bd1\u51c6\u786e\u6027\u65b9\u9762\u53d6\u5f97\u6301\u7eed\u6539\u8fdb\uff0c\u6700\u9ad8\u53ef\u8fbe62.1%\uff0c\u5e73\u5747\u76f8\u5bf9\u6539\u8fdb\u7387\u4e3a29.9%\u3002", "conclusion": "CRAMF\u901a\u8fc7\u4eceMathlib4\u6784\u5efa\u6982\u5ff5\u77e5\u8bc6\u5e93\uff0c\u5e76\u91c7\u7528\u4e0a\u4e0b\u6587\u67e5\u8be2\u589e\u5f3a\u548c\u53cc\u901a\u9053\u6df7\u5408\u68c0\u7d22\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86LLM\u81ea\u52a8\u5f62\u5f0f\u5316\u7684\u6a21\u578b\u5e7b\u89c9\u548c\u8bed\u4e49\u9e3f\u6c9f\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u51c6\u786e\u6027\u3002"}}
{"id": "2508.06692", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06692", "abs": "https://arxiv.org/abs/2508.06692", "authors": ["Md. Akmol Masud", "Md Abrar Jahin", "Mahmud Hasan"], "title": "Stabilizing Federated Learning under Extreme Heterogeneity with HeteRo-Select", "comment": null, "summary": "Federated Learning (FL) is a machine learning technique that often suffers\nfrom training instability due to the diverse nature of client data. Although\nutility-based client selection methods like Oort are used to converge by\nprioritizing high-loss clients, they frequently experience significant drops in\naccuracy during later stages of training. We propose a theoretical\nHeteRo-Select framework designed to maintain high performance and ensure\nlong-term training stability. We provide a theoretical analysis showing that\nwhen client data is very different (high heterogeneity), choosing a smart\nsubset of client participation can reduce communication more effectively\ncompared to full participation. Our HeteRo-Select method uses a clear,\nstep-by-step scoring system that considers client usefulness, fairness, update\nspeed, and data variety. It also shows convergence guarantees under strong\nregularization. Our experimental results on the CIFAR-10 dataset under\nsignificant label skew ($\\alpha=0.1$) support the theoretical findings. The\nHeteRo-Select method performs better than existing approaches in terms of peak\naccuracy, final accuracy, and training stability. Specifically, HeteRo-Select\nachieves a peak accuracy of $74.75\\%$, a final accuracy of $72.76\\%$, and a\nminimal stability drop of $1.99\\%$. In contrast, Oort records a lower peak\naccuracy of $73.98\\%$, a final accuracy of $71.25\\%$, and a larger stability\ndrop of $2.73\\%$. The theoretical foundations and empirical performance in our\nstudy make HeteRo-Select a reliable solution for real-world heterogeneous FL\nproblems.", "AI": {"tldr": "HeteRo-Select\u6846\u67b6\u901a\u8fc7\u4e00\u79cd\u591a\u7ef4\u5ea6\u8bc4\u5206\u7cfb\u7edf\u89e3\u51b3\u4e86\u8054\u90a6\u5b66\u4e60\u4e2d\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u7387\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7531\u4e8e\u5ba2\u6237\u7aef\u6570\u636e\u591a\u6837\u6027\u800c\u5bfc\u81f4\u7684\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u5e76\u514b\u670d\u73b0\u6709\u57fa\u4e8e\u6548\u7528\uff08\u5982Oort\uff09\u7684\u5ba2\u6237\u7aef\u9009\u62e9\u65b9\u6cd5\u5728\u8bad\u7ec3\u540e\u671f\u51c6\u786e\u7387\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aHeteRo-Select\u7684\u7406\u8bba\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u57fa\u4e8e\u5ba2\u6237\u7aef\u6709\u7528\u6027\u3001\u516c\u5e73\u6027\u3001\u66f4\u65b0\u901f\u5ea6\u548c\u6570\u636e\u591a\u6837\u6027\u7684\u8bc4\u5206\u7cfb\u7edf\uff0c\u5e76\u63d0\u4f9b\u5f3a\u6b63\u5219\u5316\u4e0b\u7684\u6536\u655b\u4fdd\u8bc1\u3002", "result": "\u5728CIFAR-10\u6570\u636e\u96c6\u4e0a\uff0cHeteRo-Select\u5728\u6807\u7b7e\u503e\u659c\uff08\u03b1=0.1\uff09\u7684\u663e\u8457\u60c5\u51b5\u4e0b\uff0c\u5728\u5cf0\u503c\u51c6\u786e\u7387\uff0874.75%\uff09\u3001\u6700\u7ec8\u51c6\u786e\u7387\uff0872.76%\uff09\u548c\u7a33\u5b9a\u6027\uff081.99%\u7684\u4e0b\u964d\u5e45\u5ea6\uff09\u65b9\u9762\u5747\u4f18\u4e8eOort\uff08\u5cf0\u503c\u51c6\u786e\u738773.98%\u3001\u6700\u7ec8\u51c6\u786e\u738771.25%\u3001\u4e0b\u964d\u5e45\u5ea62.73%\uff09\u3002", "conclusion": "HeteRo-Select\u6846\u67b6\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2d\u5f02\u6784\u7684\u8054\u90a6\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8be5\u6846\u67b6\u5728\u7406\u8bba\u57fa\u7840\u548c\u5b9e\u8bc1\u6027\u80fd\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2508.07272", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07272", "abs": "https://arxiv.org/abs/2508.07272", "authors": ["B. Kunyangyuen", "G. Malinowski", "D. Lacour", "B. Seng", "W. Zhang", "S. Mangin", "J. Hohlfeld", "J. Gorchon", "M. Hehn"], "title": "Controlling Single-Pulse Magnetization Switching through Angular Momentum Reservoir Engineering", "comment": "14 pages, 5 figures", "summary": "We report a systematic study of single pulse all optical helicity independent\nswitching in CoGd bilayers, revealing that the magnetization reversal dynamics\ncan be tuned over more than three orders of magnitude. By varying the Gd\nthickness or inserting a Pt spacer layer between Co and Gd, we control the\nangular momentum transferred from the rare earth sublattice to the transition\nmetal sublattice. Our results show that when Gd is abundant and strongly\ncoupled to Co, angular momentum is efficiently transferred during Gd\ndemagnetization, leading to ultrafast Co reversal. In contrast, reducing the Gd\nthickness or introducing a Pt barrier impedes this transfer, resulting in a\ndomain growth mediated reversal on nanosecond and possibly to microsecond\ntimescales as previously observed in CoDy and CoHo alloys. As a result, in rare\nearth transition metal systems, replacing Gd with heavier rare-earth elements\nsuch as Dy or Ho slows down the switching due to a reduced angular momentum\ntransfer towards Co upon demagnetization as demonstrated in CoGdDy alloys. Our\nfindings establish angular momentum availability and transfer pathways as key\nparameters governing AO HIS dynamics, offering a unified framework for fast and\nslow magnetization reversal across rare earth transition metal systems.", "AI": {"tldr": "\u7814\u7a76\u4e86CoGd\u53cc\u5c42\u819c\u4e2d\u7684\u5168\u5149\u5f00\u5173\u52a8\u529b\u5b66\uff0c\u53d1\u73b0\u901a\u8fc7\u8c03\u63a7Gd\u539a\u5ea6\u548c\u63d2\u5165Pt\u5c42\u53ef\u4ee5\u63a7\u5236\u78c1\u5316\u53cd\u8f6c\u901f\u5ea6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u89d2\u52a8\u91cf\u4f20\u9012\u662f\u63a7\u5236\u53cd\u8f6c\u901f\u5ea6\u7684\u5173\u952e\u56e0\u7d20\u3002", "motivation": "\u7814\u7a76\u7a00\u571f-\u8fc7\u6e21\u91d1\u5c5e\u6750\u6599\u4e2d\u7684\u78c1\u5316\u53cd\u8f6c\u52a8\u529b\u5b66\uff0c\u7279\u522b\u662f\u5149\u5b50\u5f00\u5173\u3002", "method": "\u901a\u8fc7\u6539\u53d8Gd\u7684\u539a\u5ea6\u6216\u5728Co\u548cGd\u4e4b\u95f4\u63d2\u5165Pt\u95f4\u9694\u5c42\uff0c\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5355\u8109\u51b2\u5168\u5149\u5f00\u5173\u52a8\u529b\u5b66\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5f53Gd\u542b\u91cf\u9ad8\u4e14\u4e0eCo\u8026\u5408\u5f3a\u65f6\uff0cGd\u53bb\u78c1\u8fc7\u7a0b\u4e2d\u89d2\u52a8\u91cf\u6709\u6548\u4f20\u9012\uff0c\u5bfc\u81f4Co\u8d85\u5feb\u53cd\u8f6c\u3002\u901a\u8fc7\u51cf\u5c0fGd\u539a\u5ea6\u6216\u5f15\u5165Pt\u963b\u6321\u5c42\u4f1a\u963b\u788d\u8fd9\u79cd\u4f20\u9012\uff0c\u5bfc\u81f4\u53cd\u8f6c\u8fc7\u7a0b\u7531\u78c1\u7574\u751f\u957f\u4ecb\u5bfc\uff0c\u65f6\u95f4\u5c3a\u5ea6\u4ece\u7eb3\u79d2\u5230\u5fae\u79d2\u4e0d\u7b49\u3002\u5c06Gd\u66ff\u6362\u4e3aDy\u6216Ho\u7b49\u66f4\u91cd\u7684\u7a00\u571f\u5143\u7d20\u4f1a\u51cf\u6162\u5f00\u5173\u901f\u5ea6\uff0c\u56e0\u4e3a\u53bb\u78c1\u8fc7\u7a0b\u4e2d\u4f20\u9012\u5230Co\u7684\u89d2\u52a8\u91cf\u51cf\u5c11\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86\u89d2\u52a8\u91cf\u53ef\u7528\u6027\u548c\u4f20\u9012\u9014\u5f84\u662f\u63a7\u5236\u7a00\u571f-\u8fc7\u6e21\u91d1\u5c5e\u7cfb\u7edf\u4e2d\u78c1\u5316\u53cd\u8f6c\u52a8\u529b\u5b66\u7684\u5173\u952e\u53c2\u6570\uff0c\u4e3a\u7406\u89e3\u4e0d\u540c\u53cd\u8f6c\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6846\u67b6\u3002"}}
{"id": "2508.07045", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07045", "abs": "https://arxiv.org/abs/2508.07045", "authors": ["Dennis Benders", "Johannes K\u00f6hler", "Robert Babu\u0161ka", "Javier Alonso-Mora", "Laura Ferranti"], "title": "From Data to Safe Mobile Robot Navigation: An Efficient and Modular Robust MPC Design Pipeline", "comment": "8 pages, 5 figures", "summary": "Model predictive control (MPC) is a powerful strategy for planning and\ncontrol in autonomous mobile robot navigation. However, ensuring safety in\nreal-world deployments remains challenging due to the presence of disturbances\nand measurement noise. Existing approaches often rely on idealized assumptions,\nneglect the impact of noisy measurements, and simply heuristically guess\nunrealistic bounds. In this work, we present an efficient and modular robust\nMPC design pipeline that systematically addresses these limitations. The\npipeline consists of an iterative procedure that leverages closed-loop\nexperimental data to estimate disturbance bounds and synthesize a robust\noutput-feedback MPC scheme. We provide the pipeline in the form of\ndeterministic and reproducible code to synthesize the robust output-feedback\nMPC from data. We empirically demonstrate robust constraint satisfaction and\nrecursive feasibility in quadrotor simulations using Gazebo.", "AI": {"tldr": "A new pipeline for robust Model Predictive Control (MPC) that uses experimental data to estimate disturbance bounds and create a safer control system for robots, shown to work in simulations.", "motivation": "Existing MPC approaches for robot navigation often rely on idealized assumptions, neglect the impact of noisy measurements, and heuristically guess unrealistic bounds, making safety challenging in real-world deployments with disturbances and measurement noise.", "method": "An iterative procedure that leverages closed-loop experimental data to estimate disturbance bounds and synthesize a robust output-feedback MPC scheme. The pipeline is provided in the form of deterministic and reproducible code.", "result": "Empirically demonstrated robust constraint satisfaction and recursive feasibility in quadrotor simulations using Gazebo.", "conclusion": "The proposed pipeline can systematically address limitations in existing robust MPC designs by leveraging closed-loop experimental data to estimate disturbance bounds and synthesize a robust output-feedback MPC scheme, which has been empirically demonstrated to achieve robust constraint satisfaction and recursive feasibility in quadrotor simulations."}}
{"id": "2508.07155", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07155", "abs": "https://arxiv.org/abs/2508.07155", "authors": ["Jianwei Xu"], "title": "Bargmann invariants of Gaussian states", "comment": "14 pages, 5 figures", "summary": "Given a set of ordered quantum states, described by density operators $%\n\\{\\rho _{j}\\}_{j=1}^{n}$, the Bargmann invariant of $\\{\\rho _{j}\\}_{j=1}^{n}$\nis defined as tr($\\rho _{1}\\rho _{2}...\\rho _{n}$). Bargmann invariant serves\nas a fundamental concept for quantum mechanics and has diverse applications in\nquantum information science. Bosonic Gaussian states are a class of quantum\nstates on infinite-dimensional Hilbert space, widely used in quantum optics and\nquantum information science. Bosonic Gaussian states are conveniently and\nconventionally characterized by their means and covariance matrices. In this\nwork, we provide the expression of Bargmann invariant tr($\\rho _{1}\\rho\n_{2}...\\rho _{n}$) for any $m$-mode bosonic Gaussian states $\\{\\rho\n_{j}\\}_{j=1}^{n}$ in terms of the means and covariance matrices of $\\{\\rho\n_{j}\\}_{j=1}^{n}.$ We also use this expression to explore the permissible\nvalues of Bargmann invariants for bosonic Gaussian states.", "AI": {"tldr": "This paper derives the formula for the Bargmann invariant of bosonic Gaussian states using their means and covariance matrices, and investigates the possible values the invariant can take.", "motivation": "The Bargmann invariant is a fundamental concept in quantum mechanics with applications in quantum information science. Bosonic Gaussian states are important in quantum optics and information science.", "method": "The expression of Bargmann invariant tr($\\rho _{1}\\rho _{2}...\n$\\rho _{n}$) for any $m$-mode bosonic Gaussian states is derived in terms of their means and covariance matrices.", "result": "The expression of the Bargmann invariant for bosonic Gaussian states is derived and used to explore its permissible values.", "conclusion": "We provide the expression of Bargmann invariant for bosonic Gaussian states and explore its permissible values."}}
{"id": "2508.06886", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06886", "abs": "https://arxiv.org/abs/2508.06886", "authors": ["Arpita Saggar", "Jonathan C. Darling", "Vania Dimitrova", "Duygu Sarikaya", "David C. Hogg"], "title": "Score Before You Speak: Improving Persona Consistency in Dialogue Generation using Response Quality Scores", "comment": "Camera-Ready version for ECAI 2025. 8 pages", "summary": "Persona-based dialogue generation is an important milestone towards building\nconversational artificial intelligence. Despite the ever-improving capabilities\nof large language models (LLMs), effectively integrating persona fidelity in\nconversations remains challenging due to the limited diversity in existing\ndialogue data. We propose a novel framework SBS (Score-Before-Speaking), which\noutperforms previous methods and yields improvements for both million and\nbillion-parameter models. Unlike previous methods, SBS unifies the learning of\nresponses and their relative quality into a single step. The key innovation is\nto train a dialogue model to correlate augmented responses with a quality score\nduring training and then leverage this knowledge at inference. We use\nnoun-based substitution for augmentation and semantic similarity-based scores\nas a proxy for response quality. Through extensive experiments with benchmark\ndatasets (PERSONA-CHAT and ConvAI2), we show that score-conditioned training\nallows existing models to better capture a spectrum of persona-consistent\ndialogues. Our ablation studies also demonstrate that including scores in the\ninput prompt during training is superior to conventional training setups. Code\nand further details are available at\nhttps://arpita2512.github.io/score_before_you_speak", "AI": {"tldr": "SBS\u6846\u67b6\u901a\u8fc7\u5728\u8bad\u7ec3\u65f6\u5c06\u56de\u590d\u4e0e\u5176\u8d28\u91cf\u8bc4\u5206\u5173\u8054\uff0c\u63d0\u5347\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u4e2a\u6027\u5316\u5bf9\u8bdd\u751f\u6210\u4e2d\u7684\u4eba\u8bbe\u4fdd\u771f\u5ea6\uff0c\u5e76\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4e2a\u6027\u5316\u5bf9\u8bdd\u751f\u6210\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u73b0\u6709\u7684\u5bf9\u8bdd\u6570\u636e\u591a\u6837\u6027\u6709\u9650\uff0c\u96be\u4ee5\u6709\u6548\u4fdd\u8bc1\u4eba\u8bbe\u4fdd\u771f\u5ea6\u3002", "method": "SBS\u6846\u67b6\uff0c\u5c06\u56de\u590d\u5b66\u4e60\u548c\u76f8\u5bf9\u8d28\u91cf\u5b66\u4e60\u7edf\u4e00\u5728\u4e00\u4e2a\u6b65\u9aa4\u4e2d\u3002\u5177\u4f53\u505a\u6cd5\u662f\u8bad\u7ec3\u4e00\u4e2a\u5bf9\u8bdd\u6a21\u578b\uff0c\u4f7f\u5176\u80fd\u591f\u5c06\u589e\u5f3a\u540e\u7684\u56de\u590d\u4e0e\u8d28\u91cf\u5206\u6570\u76f8\u5173\u8054\u3002\u56de\u590d\u589e\u5f3a\u901a\u8fc7\u57fa\u4e8e\u540d\u8bcd\u7684\u66ff\u6362\u5b9e\u73b0\uff0c\u800c\u8d28\u91cf\u5206\u6570\u5219\u4f7f\u7528\u57fa\u4e8e\u8bed\u4e49\u76f8\u4f3c\u5ea6\u7684\u8bc4\u5206\u4f5c\u4e3a\u4ee3\u7406\u3002", "result": "SBS\u6846\u67b6\u5728PERSONA-CHAT\u548cConvAI2\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5927\u91cf\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5206\u6570\u6761\u4ef6\u8bad\u7ec3\u80fd\u591f\u8ba9\u73b0\u6709\u6a21\u578b\u66f4\u597d\u5730\u6355\u6349\u4e00\u7cfb\u5217\u7b26\u5408\u4eba\u8bbe\u7684\u5bf9\u8bdd\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u8fd8\u8868\u660e\uff0c\u5728\u8bad\u7ec3\u65f6\u5c06\u5206\u6570\u7eb3\u5165\u8f93\u5165\u63d0\u793a\u4f18\u4e8e\u4f20\u7edf\u7684\u8bad\u7ec3\u8bbe\u7f6e\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165SBS\u6846\u67b6\uff0c\u53ef\u4ee5\u63d0\u5347\u73b0\u6709\u6a21\u578b\uff08\u5305\u62ec\u767e\u4e07\u548c\u5341\u4ebf\u53c2\u6570\u6a21\u578b\uff09\u5728\u4e2a\u6027\u5316\u5bf9\u8bdd\u751f\u6210\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728\u4fdd\u8bc1\u5bf9\u8bdd\u7684\u4eba\u8bbe\u4fdd\u771f\u5ea6\u65b9\u9762\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5728\u8bad\u7ec3\u9636\u6bb5\u5c06\u56de\u590d\u4e0e\u5176\u8d28\u91cf\u8bc4\u5206\u76f8\u5173\u8054\uff0c\u5e76\u5728\u63a8\u7406\u9636\u6bb5\u5229\u7528\u8fd9\u4e9b\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u6bd4\u4ee5\u5f80\u65b9\u6cd5\u66f4\u597d\u7684\u6548\u679c\u3002"}}
{"id": "2508.06566", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06566", "abs": "https://arxiv.org/abs/2508.06566", "authors": ["Manish Kansana", "Elias Hossain", "Shahram Rahimi", "Noorbakhsh Amiri Golilarz"], "title": "Surformer v1: Transformer-Based Surface Classification Using Tactile and Vision Features", "comment": null, "summary": "Surface material recognition is a key component in robotic perception and\nphysical interaction, particularly when leveraging both tactile and visual\nsensory inputs. In this work, we propose Surformer v1, a transformer-based\narchitecture designed for surface classification using structured tactile\nfeatures and PCA-reduced visual embeddings extracted via ResNet-50. The model\nintegrates modality-specific encoders with cross-modal attention layers,\nenabling rich interactions between vision and touch. Currently,\nstate-of-the-art deep learning models for vision tasks have achieved remarkable\nperformance. With this in mind, our first set of experiments focused\nexclusively on tactile-only surface classification. Using feature engineering,\nwe trained and evaluated multiple machine learning models, assessing their\naccuracy and inference time. We then implemented an encoder-only Transformer\nmodel tailored for tactile features. This model not only achieved the highest\naccuracy but also demonstrated significantly faster inference time compared to\nother evaluated models, highlighting its potential for real-time applications.\nTo extend this investigation, we introduced a multimodal fusion setup by\ncombining vision and tactile inputs. We trained both Surformer v1 (using\nstructured features) and Multimodal CNN (using raw images) to examine the\nimpact of feature-based versus image-based multimodal learning on\nclassification accuracy and computational efficiency. The results showed that\nSurformer v1 achieved 99.4% accuracy with an inference time of 0.77 ms, while\nthe Multimodal CNN achieved slightly higher accuracy but required significantly\nmore inference time. These findings suggest Surformer v1 offers a compelling\nbalance between accuracy, efficiency, and computational cost for surface\nmaterial recognition.", "AI": {"tldr": "Surformer v1 \u662f\u4e00\u79cd\u57fa\u4e8e Transformer \u7684\u6a21\u578b\uff0c\u53ef\u9ad8\u6548\u51c6\u786e\u5730\u8bc6\u522b\u8868\u9762\u6750\u6599\uff0c\u5728\u51c6\u786e\u6027\u548c\u63a8\u7406\u901f\u5ea6\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "motivation": "\u8868\u9762\u6750\u6599\u8bc6\u522b\u662f\u673a\u5668\u4eba\u611f\u77e5\u548c\u7269\u7406\u4ea4\u4e92\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\uff0c\u5c24\u5176\u662f\u5728\u5229\u7528\u89e6\u89c9\u548c\u89c6\u89c9\u4f20\u611f\u8f93\u5165\u65f6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Surformer v1 \u7684\u57fa\u4e8e Transformer \u7684\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4f7f\u7528\u7ed3\u6784\u5316\u89e6\u89c9\u7279\u5f81\u548c\u901a\u8fc7 ResNet-50 \u63d0\u53d6\u7684 PCA \u964d\u7ef4\u89c6\u89c9\u5d4c\u5165\u3002\u8be5\u6a21\u578b\u96c6\u6210\u4e86\u7279\u5b9a\u4e8e\u6a21\u6001\u7684\u7f16\u7801\u5668\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u5c42\uff0c\u4ee5\u5b9e\u73b0\u89c6\u89c9\u548c\u89e6\u89c9\u4e4b\u95f4\u7684\u4e30\u5bcc\u4ea4\u4e92\u3002", "result": "\u5728\u4ec5\u89e6\u89c9\u7684\u8868\u9762\u5206\u7c7b\u4efb\u52a1\u4e2d\uff0c\u57fa\u4e8e Transformer \u7684\u6a21\u578b\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u51c6\u786e\u7387\u548c\u6700\u5feb\u7684\u63a8\u7406\u65f6\u95f4\u3002\u5728\u591a\u6a21\u6001\u8bbe\u7f6e\u4e2d\uff0cSurformer v1 \u5b9e\u73b0\u4e86 99.4% \u7684\u51c6\u786e\u7387\u548c 0.77 \u6beb\u79d2\u7684\u63a8\u7406\u65f6\u95f4\uff0c\u800c\u591a\u6a21\u6001 CNN \u7684\u51c6\u786e\u7387\u7565\u9ad8\uff0c\u4f46\u63a8\u7406\u65f6\u95f4\u663e\u8457\u66f4\u957f\u3002", "conclusion": "Surformer v1 \u5728\u51c6\u786e\u6027\u3001\u6548\u7387\u548c\u8ba1\u7b97\u6210\u672c\u4e4b\u95f4\u53d6\u5f97\u4e86\u4ee4\u4eba\u4fe1\u670d\u7684\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8868\u9762\u6750\u6599\u8bc6\u522b\u3002"}}
{"id": "2508.08185", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08185", "abs": "https://arxiv.org/abs/2508.08185", "authors": ["Yaoyu Zhang", "Xin Sun", "Jun Wang", "Tianwei Hou", "Anna Li", "Yuanwei Liu", "Arumugam Nallanathan"], "title": "Pinching-Antenna Systems (PASS)-based Indoor Positioning", "comment": "5 pages, 5 figures, letter", "summary": "Pinching antenna (PA), a flexible waveguide integrated with dielectric\nparticles, intelligently reconstructs line-of-sight channels. Utilizing its\ngeometric deterministic model and meter-level reconstruction, PA systems (PASS)\nare applied to uplink indoor positioning. In this paper, the uplink positioning\nsystem model for PASS is firstly proposed. A PASS-based received signal\nstrength indication (RSSI) method is proposed to measure the distance from the\nusers to each PA, which is efficient and suitable for PASS. PASS-based weighted\nleast squares (WLS) algorithm is designed to calculate the two-dimensional\ncoordinates of the users. Several critical observations can be drawn from our\nresults: i) More PAs on the waveguide improves the positioning accuracy and\nrobustness. ii) When the number of PAs exceeds a certain threshold, the\nperformance gain becomes marginal. iii) User locations between and near PAs\nyield superior positioning accuracy.", "AI": {"tldr": "\u5939\u6301\u5929\u7ebf\uff08PA\uff09\u662f\u4e00\u79cd\u96c6\u6210\u4e86\u4ecb\u7535\u7c92\u5b50\u7684\u67d4\u6027\u6ce2\u5bfc\uff0c\u53ef\u4ee5\u667a\u80fd\u5730\u91cd\u5efa\u89c6\u7ebf\uff08LoS\uff09\u4fe1\u9053\u3002PA\u7cfb\u7edf\uff08PASS\uff09\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u4e0a\u884c\u94fe\u8def\u5ba4\u5185\u5b9a\u4f4d\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5ba4\u5185\u5b9a\u4f4d\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5939\u6301\u5929\u7ebf\u201d\uff08PA\uff09\u7684\u65b0\u578b\u7075\u6d3b\u6ce2\u5bfc\uff0c\u5b83\u53ef\u4ee5\u667a\u80fd\u5730\u91cd\u5efa\u89c6\u7ebf\uff08LoS\uff09\u4fe1\u9053\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u4e0a\u884c\u94fe\u8def\u5ba4\u5185\u5b9a\u4f4d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePASS\u7684\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u6307\u793a\uff08RSSI\uff09\u65b9\u6cd5\u6765\u6d4b\u91cf\u7528\u6237\u5230\u6bcf\u4e2aPA\u7684\u8ddd\u79bb\uff0c\u7136\u540e\u8bbe\u8ba1\u4e86\u4e00\u79cd\u57fa\u4e8ePASS\u7684\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\uff08WLS\uff09\u7b97\u6cd5\u6765\u8ba1\u7b97\u7528\u6237\u7684\u4e8c\u7ef4\u5750\u6807\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6ce2\u5bfc\u4e0a\u7684PA\u6570\u91cf\u8d8a\u591a\uff0c\u5b9a\u4f4d\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\u8d8a\u597d\uff1b\u5f53PA\u6570\u91cf\u8d85\u8fc7\u4e00\u5b9a\u9608\u503c\u65f6\uff0c\u6027\u80fd\u589e\u76ca\u53d8\u5f97\u4e0d\u660e\u663e\uff1b\u4f4d\u4e8ePA\u4e4b\u95f4\u53ca\u9644\u8fd1\u7684\u7528\u6237\u7684\u5b9a\u4f4d\u7cbe\u5ea6\u66f4\u9ad8\u3002", "conclusion": "PA\u7cfb\u7edf\uff08PASS\uff09\u53ef\u4ee5\u901a\u8fc7\u63a5\u6536\u4fe1\u53f7\u5f3a\u5ea6\u6307\u793a\uff08RSSI\uff09\u65b9\u6cd5\u548c\u52a0\u6743\u6700\u5c0f\u4e8c\u4e58\uff08WLS\uff09\u7b97\u6cd5\u5b9e\u73b0\u9ad8\u6548\u4e14\u9002\u5408PASS\u7684\u4e0a\u884c\u94fe\u8def\u5ba4\u5185\u5b9a\u4f4d\uff0c\u5e76\u80fd\u8ba1\u7b97\u7528\u6237\u7684\u4e8c\u7ef4\u5750\u6807\u3002"}}
{"id": "2508.07890", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07890", "abs": "https://arxiv.org/abs/2508.07890", "authors": ["Ori Gabai", "Amnon Willinger", "Igor Khanonkin", "Vitalii Sichkovskyi", "Johann Peter Reithmaier", "Gadi Eisenstein"], "title": "Rabi Oscillations Modulated Noise Squeezing in Active Quantum Dot Ensembles", "comment": null, "summary": "Generation of squeezed light is usually implemented in nonlinear \\c{hi}(2) or\n\\c{hi}(3) materials. Semiconductor lasers and optical amplifiers (SOAs) also\noffer non-linearities but they differ from passive elements in that they add\namplified spontaneous emission noise (ASE). In a semiconductor laser, squeezing\nto below the shot noise limit has been demonstrated. An SOA contains no cavity\nand it adds significant noise. Gain saturation can lead, in principle, to\nsqueezing of the photon number quadrature to below the shot noise level but\noften the noise is reduced only to below the ASE level of a linear amplifier.\nAt the same time, the noise in the phase quadrature increases according to the\nHeisenberg uncertainty principle. Short resonant pulses interacting with a\nquantum dot SOA induce coherent effects such as Rabi oscillations. Here, we\ndemonstrate, for the first time, that Rabi oscillations cause cyclical noise\nsqueezing which varies periodically with the excitation pulse area. The noise\nin the present experiments does not reach the quantum limit so we term this\ncondition quasi squeezing. It occurs during the portions of the Rabi cycle when\nthe quantum dots provide gain and repeats with every fourfold increase of the\npulse excitation energy which amounts to a 2{\\pi} increase in pulse area. In\nall other cases, the noise exhibits the properties of a coherent state.", "AI": {"tldr": "\u91cf\u5b50\u70b9SOA\u4e2d\u7684Rabi\u632f\u8361\u4ea7\u751f\u4e86\u51c6\u538b\u7f29\uff0c\u8fd9\u662f\u9996\u6b21\u5728\u6709\u6e90\u8bbe\u5907\u4e2d\u89c2\u5bdf\u5230\u7684\u73b0\u8c61\u3002", "motivation": "\u89e3\u91ca\u4e86\u534a\u5bfc\u4f53\u5149\u5b66\u653e\u5927\u5668\uff08SOA\uff09\u4e2d\u7684\u566a\u58f0\u7279\u6027\uff0c\u5e76\u63d0\u51fa\u5728SOA\u4e2d\u5b9e\u73b0\u538b\u7f29\u7684\u7406\u8bba\u53ef\u80fd\u6027\u3002", "method": "\u901a\u8fc7\u5b9e\u9a8c\u6f14\u793a\u4e86\u91cf\u5b50\u70b9SOA\u4e2d\u77ed\u5171\u632f\u8109\u51b2\u8bf1\u5bfc\u7684Rabi\u632f\u8361\u3002", "result": "\u5728\u91cf\u5b50\u70b9SOA\u4e2d\u89c2\u5bdf\u5230\u4e86\u51c6\u538b\u7f29\u73b0\u8c61\uff0c\u566a\u58f0\u5468\u671f\u6027\u5730\u4f4e\u4e8e\u6563\u7c92\u566a\u58f0\u6781\u9650\uff0c\u4f46\u672a\u8fbe\u5230\u91cf\u5b50\u6781\u9650\u3002\u8fd9\u79cd\u51c6\u538b\u7f29\u4e0eRabi\u632f\u8361\u7684\u5468\u671f\u76f8\u5173\uff0c\u5e76\u968f\u6fc0\u53d1\u8109\u51b2\u80fd\u91cf\u7684\u589e\u52a0\u800c\u91cd\u590d\u51fa\u73b0\u3002", "conclusion": "\u91cf\u5b50\u70b9SOA\u4e2d\u7684Rabi\u632f\u8361\u5bfc\u81f4\u4e86\u51c6\u538b\u7f29\uff0c\u5176\u566a\u58f0\u5468\u671f\u6027\u5730\u968f\u6fc0\u53d1\u8109\u51b2\u9762\u79ef\u53d8\u5316\uff0c\u5728\u91cf\u5b50\u70b9\u63d0\u4f9b\u589e\u76ca\u7684Rabi\u5468\u671f\u90e8\u5206\u53d1\u751f\u3002"}}
{"id": "2508.07696", "categories": ["eess.SP", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07696", "abs": "https://arxiv.org/abs/2508.07696", "authors": ["Joohyuk Park", "Yongjeong Oh", "Jihun Park", "Yo-Seb Jeon"], "title": "Importance-Aware Semantic Communication in MIMO-OFDM Systems Using Vision Transformer", "comment": null, "summary": "This paper presents a novel importance-aware quantization, subcarrier\nmapping, and power allocation (IA-QSMPA) framework for semantic communication\nin multiple-input multiple-output orthogonal frequency division multiplexing\n(MIMO-OFDM) systems, empowered by a pretrained Vision Transformer (ViT). The\nproposed framework exploits attention-based importance extracted from a\npretrained ViT to jointly optimize quantization levels, subcarrier mapping, and\npower allocation. Specifically, IA-QSMPA maps semantically important features\nto high-quality subchannels and allocates resources in accordance with their\ncontribution to task performance and communication latency. To efficiently\nsolve the resulting nonconvex optimization problem, a block coordinate descent\nalgorithm is employed. The framework is further extended to operate under\nfinite blocklength transmission, where communication errors may occur. In this\nsetting, a segment-wise linear approximation of the channel dispersion penalty\nis introduced to enable efficient joint optimization under practical\nconstraints. Simulation results on a multi-view image classification task using\nthe MVP-N dataset demonstrate that IA-QSMPA significantly outperforms\nconventional methods in both ideal and finite blocklength transmission\nscenarios, achieving superior task performance and communication efficiency.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7528\u4e8eMIMO-OFDM\u8bed\u4e49\u901a\u4fe1\u7684IA-QSMPA\u6846\u67b6\uff0c\u5229\u7528ViT\u63d0\u53d6\u91cd\u8981\u6027\uff0c\u8054\u5408\u4f18\u5316\u91cf\u5316\u3001\u5b50\u8f7d\u6ce2\u6620\u5c04\u548c\u529f\u7387\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002", "motivation": "\u4e3aMIMO-OFDM\u7cfb\u7edf\u4e2d\u7684\u8bed\u4e49\u901a\u4fe1\u63d0\u51fa\u4e00\u79cd\u65b0\u9896\u7684IA-QSMPA\u6846\u67b6\uff0c\u4ee5\u63d0\u9ad8\u4efb\u52a1\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002", "method": "\u5229\u7528\u9884\u8bad\u7ec3\u7684Vision Transformer\uff08ViT\uff09\u63d0\u53d6\u57fa\u4e8e\u6ce8\u610f\u529b\u7684\u91cd\u8981\u6027\uff0c\u8054\u5408\u4f18\u5316\u91cf\u5316\u7ea7\u522b\u3001\u5b50\u8f7d\u6ce2\u6620\u5c04\u548c\u529f\u7387\u5206\u914d\u3002\u91c7\u7528\u5757\u5750\u6807\u4e0b\u964d\u7b97\u6cd5\u6c42\u89e3\u975e\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u5206\u6bb5\u7ebf\u6027\u4fe1\u9053\u5206\u6563\u5ea6\u60e9\u7f5a\u4ee5\u5904\u7406\u6709\u9650\u5757\u957f\u5ea6\u4f20\u8f93\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0cIA-QSMPA\u5728\u591a\u89c6\u56fe\u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u4e0a\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u7406\u60f3\u548c\u6709\u9650\u4fe1\u9053\u957f\u5ea6\u4f20\u8f93\u573a\u666f\u4e0b\u5747\u663e\u8457\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u4efb\u52a1\u6027\u80fd\u548c\u901a\u4fe1\u6548\u7387\u3002"}}
{"id": "2508.06939", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06939", "abs": "https://arxiv.org/abs/2508.06939", "authors": ["Hiba Najjar", "Deepak Pathak", "Marlon Nuske", "Andreas Dengel"], "title": "Intrinsic Explainability of Multimodal Learning for Crop Yield Prediction", "comment": null, "summary": "Multimodal learning enables various machine learning tasks to benefit from\ndiverse data sources, effectively mimicking the interplay of different factors\nin real-world applications, particularly in agriculture. While the\nheterogeneous nature of involved data modalities may necessitate the design of\ncomplex architectures, the model interpretability is often overlooked. In this\nstudy, we leverage the intrinsic explainability of Transformer-based models to\nexplain multimodal learning networks, focusing on the task of crop yield\nprediction at the subfield level. The large datasets used cover various crops,\nregions, and years, and include four different input modalities: multispectral\nsatellite and weather time series, terrain elevation maps and soil properties.\nBased on the self-attention mechanism, we estimate feature attributions using\ntwo methods, namely the Attention Rollout (AR) and Generic Attention (GA), and\nevaluate their performance against Shapley-based model-agnostic estimations,\nShapley Value Sampling (SVS). Additionally, we propose the Weighted Modality\nActivation (WMA) method to assess modality attributions and compare it with SVS\nattributions. Our findings indicate that Transformer-based models outperform\nother architectures, specifically convolutional and recurrent networks,\nachieving R2 scores that are higher by 0.10 and 0.04 at the subfield and field\nlevels, respectively. AR is shown to provide more robust and reliable temporal\nattributions, as confirmed through qualitative and quantitative evaluation,\ncompared to GA and SVS values. Information about crop phenology stages was\nleveraged to interpret the explanation results in the light of established\nagronomic knowledge. Furthermore, modality attributions revealed varying\npatterns across the two methods compared.[...]", "AI": {"tldr": "This paper uses Transformer models for crop yield prediction, outperforming others. It introduces methods (AR, GA, WMA) to explain how the model works and which data sources are important, finding that AR is reliable for time-based explanations and the model's predictions align with agricultural knowledge.", "motivation": "To address the overlook of model interpretability in multimodal learning networks, especially in agriculture where diverse data sources are used. The study aims to leverage the explainability of Transformer-based models for tasks like crop yield prediction.", "method": "Leveraging Transformer-based models for multimodal learning in crop yield prediction. Employing Attention Rollout (AR) and Generic Attention (GA) for feature attribution estimation, and comparing them with Shapley Value Sampling (SVS). Proposing Weighted Modality Activation (WMA) for modality attribution assessment and comparing it with SVS.", "result": "Transformer-based models showed improved performance (higher R2 scores by 0.10 at subfield and 0.04 at field levels) compared to CNNs and RNNs. AR demonstrated more robust and reliable temporal attributions than GA and SVS. WMA and SVS showed different patterns in modality attributions.", "conclusion": "Transformer-based models outperform other architectures like CNNs and RNNs in crop yield prediction, achieving higher R2 scores. Attention Rollout (AR) provides more robust temporal attributions compared to GA and SVS. Modality attributions vary between WMA and SVS methods. Explanation results align with agronomic knowledge when interpreted using crop phenology stages."}}
{"id": "2508.06704", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06704", "abs": "https://arxiv.org/abs/2508.06704", "authors": ["Hager Radi Abdelwahed", "M\u00e9lisande Teng", "Robin Zbinden", "Laura Pollock", "Hugo Larochelle", "Devis Tuia", "David Rolnick"], "title": "CISO: Species Distribution Modeling Conditioned on Incomplete Species Observations", "comment": null, "summary": "Species distribution models (SDMs) are widely used to predict species'\ngeographic distributions, serving as critical tools for ecological research and\nconservation planning. Typically, SDMs relate species occurrences to\nenvironmental variables representing abiotic factors, such as temperature,\nprecipitation, and soil properties. However, species distributions are also\nstrongly influenced by biotic interactions with other species, which are often\noverlooked. While some methods partially address this limitation by\nincorporating biotic interactions, they often assume symmetrical pairwise\nrelationships between species and require consistent co-occurrence data. In\npractice, species observations are sparse, and the availability of information\nabout the presence or absence of other species varies significantly across\nlocations. To address these challenges, we propose CISO, a deep learning-based\nmethod for species distribution modeling Conditioned on Incomplete Species\nObservations. CISO enables predictions to be conditioned on a flexible number\nof species observations alongside environmental variables, accommodating the\nvariability and incompleteness of available biotic data. We demonstrate our\napproach using three datasets representing different species groups: sPlotOpen\nfor plants, SatBird for birds, and a new dataset, SatButterfly, for\nbutterflies. Our results show that including partial biotic information\nimproves predictive performance on spatially separate test sets. When\nconditioned on a subset of species within the same dataset, CISO outperforms\nalternative methods in predicting the distribution of the remaining species.\nFurthermore, we show that combining observations from multiple datasets can\nimprove performance. CISO is a promising ecological tool, capable of\nincorporating incomplete biotic information and identifying potential\ninteractions between species from disparate taxa.", "AI": {"tldr": "CISO\u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u53ef\u4ee5\u5229\u7528\u4e0d\u5b8c\u6574\u7684\u7269\u79cd\u89c2\u6d4b\u6570\u636e\u548c\u73af\u5883\u53d8\u91cf\u6765\u66f4\u51c6\u786e\u5730\u9884\u6d4b\u7269\u79cd\u5206\u5e03\uff0c\u5e76\u4e14\u80fd\u591f\u8bc6\u522b\u7269\u79cd\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u3002", "motivation": "\u4f20\u7edf\u7684\u7269\u79cd\u5206\u5e03\u6a21\u578b\uff08SDMs\uff09\u901a\u5e38\u4ec5\u8003\u8651\u73af\u5883\u53d8\u91cf\uff0c\u800c\u5ffd\u7565\u4e86\u5bf9\u7269\u79cd\u5206\u5e03\u6709\u91cd\u8981\u5f71\u54cd\u7684\u7269\u79cd\u95f4\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u3002\u73b0\u6709\u7684\u90e8\u5206\u8003\u8651\u751f\u7269\u76f8\u4e92\u4f5c\u7528\u7684\u65b9\u6cd5\u5b58\u5728\u5047\u8bbe\u7269\u79cd\u95f4\u5173\u7cfb\u5bf9\u79f0\u548c\u9700\u8981\u4e00\u81f4\u7684\u5171\u540c\u51fa\u73b0\u6570\u636e\u7684\u5c40\u9650\u6027\uff0c\u800c\u5b9e\u9645\u4e2d\u7269\u79cd\u89c2\u6d4b\u6570\u636e\u7a00\u758f\u4e14\u5176\u4ed6\u7269\u79cd\u7684\u5b58\u5728\u6216\u7f3a\u5931\u4fe1\u606f\u5728\u4e0d\u540c\u5730\u70b9\u5dee\u5f02\u5f88\u5927\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCISO\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6839\u636e\u4e0d\u5b8c\u6574\u7684\u7269\u79cd\u89c2\u6d4b\u4fe1\u606f\u8fdb\u884c\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u3002CISO\u5141\u8bb8\u6839\u636e\u7075\u6d3b\u6570\u91cf\u7684\u7269\u79cd\u89c2\u6d4b\u4fe1\u606f\u4ee5\u53ca\u73af\u5883\u53d8\u91cf\u6765\u9884\u6d4b\u7269\u79cd\u5206\u5e03\uff0c\u80fd\u591f\u9002\u5e94\u53ef\u7528\u7684\u751f\u7269\u6570\u636e\u7684\u53ef\u53d8\u6027\u548c\u4e0d\u5b8c\u6574\u6027\u3002", "result": "\u5728\u690d\u7269\u3001\u9e1f\u7c7b\u548c\u8774\u8776\u7684\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5305\u542b\u90e8\u5206\u7684\u751f\u7269\u4fe1\u606f\u53ef\u4ee5\u63d0\u9ad8\u5728\u7a7a\u95f4\u4e0a\u5206\u79bb\u7684\u6d4b\u8bd5\u96c6\u4e0a\u7684\u9884\u6d4b\u6027\u80fd\u3002\u5f53\u6839\u636e\u540c\u4e00\u6570\u636e\u96c6\u4e2d\u7684\u90e8\u5206\u7269\u79cd\u8fdb\u884c\u6761\u4ef6\u9884\u6d4b\u65f6\uff0cCISO\u5728\u9884\u6d4b\u5269\u4f59\u7269\u79cd\u7684\u5206\u5e03\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u6765\u81ea\u591a\u4e2a\u6570\u636e\u96c6\u7684\u89c2\u6d4b\u4fe1\u606f\u53ef\u4ee5\u63d0\u9ad8\u9884\u6d4b\u6027\u80fd\u3002", "conclusion": "CISO\u662f\u4e00\u4e2a\u6709\u524d\u9014\u7684\u751f\u6001\u5b66\u5de5\u5177\uff0c\u80fd\u591f\u6574\u5408\u4e0d\u5b8c\u6574\u7684\u751f\u7269\u4fe1\u606f\uff0c\u5e76\u8bc6\u522b\u4e0d\u540c\u7c7b\u7fa4\u7269\u79cd\u4e4b\u95f4\u6f5c\u5728\u7684\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2508.07278", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07278", "abs": "https://arxiv.org/abs/2508.07278", "authors": ["Qing-Han Yang", "Jia-Wen Li", "Xin-Wei Yi", "Xiang Li", "Jing-Yang You", "Gang Su", "Bo Gu"], "title": "Intercalation-Induced Near Room-Temperature Ferromagnetism in CrI3 via Synergistic Exchange Pathways", "comment": null, "summary": "The development of room-temperature magnetic semiconductors is critical for\nadvancing spintronic technologies, yet van der Waals magnets like CrI3 exhibit\nintrinsically low Curie temperatures (Tc = 45 K). This study employs\nfirst-principles calculations to demonstrate that atom intercalation,\nparticularly lithium (Li), dramatically enhances magnetic exchange couplings in\nCrI3, achieving near room-temperature ferromagnetism with a predicted Tc of 286\nK-aligning with experimental reports of 420 K. The underlying mechanism\ninvolves synergistic superexchange and double-exchange interactions:\nintercalation reduces the |Ep-Ed| energy difference between iodine p-orbitals\nand chromium d-orbitals, strengthening superexchange pathways, while charge\ntransfer induces valence mixing (e.g., Cr3+ to Cr2+, as confirmed by\nexperimental X-ray photoelectron spectrometry data), promoting double-exchange.\nTheoretical predictions extend to other intercalants including Cu and Na, with\nCu0.25CrI3 and Na0.25CrI3 exhibiting Tc of 267 K and 247 K, respectively,\nestablishing a versatile strategy for designing high-Tc magnetic\nsemiconductors. This work bridges theoretical insights with experimental\nvalidation, offering a transferable framework for intercalation-driven material\ndesign and accelerating practical spintronic device realization.", "AI": {"tldr": "\u901a\u8fc7\u539f\u5b50\u63d2\u5c42\uff08\u7279\u522b\u662f\u9502\uff09\u589e\u5f3aCrI3\u78c1\u6027\uff0c\u63d0\u9ad8\u5176\u5c45\u91cc\u6e29\u5ea6\u81f3\u63a5\u8fd1\u5ba4\u6e29\uff0c\u4e3a\u8bbe\u8ba1\u65b0\u578b\u78c1\u6027\u534a\u5bfc\u4f53\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u63a8\u8fdb\u81ea\u65cb\u7535\u5b50\u5b66\u6280\u672f\uff0c\u5f00\u53d1\u5ba4\u6e29\u78c1\u6027\u534a\u5bfc\u4f53\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u7684\u8303\u5fb7\u534e\u78c1\u4f53\uff08\u5982CrI3\uff09\u5177\u6709\u8f83\u4f4e\u7684\u5c45\u91cc\u6e29\u5ea6\uff08Tc = 45 K\uff09\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7ed3\u5408X\u5c04\u7ebf\u5149\u7535\u5b50\u80fd\u8c31\u7b49\u5b9e\u9a8c\u6570\u636e\uff0c\u5206\u6790\u4e86\u539f\u5b50\u63d2\u5c42\uff08\u7279\u522b\u662fLi\uff09\u589e\u5f3aCrI3\u78c1\u4ea4\u6362\u8026\u5408\u7684\u673a\u5236\uff0c\u5305\u62ec\u534f\u540c\u7684\u8d85\u4ea4\u6362\u548c\u53cc\u4ea4\u6362\u76f8\u4e92\u4f5c\u7528\u3002", "result": "\u901a\u8fc7\u9502\u63d2\u5c42\uff0cCrI3\u7684\u9884\u6d4bTc\u63d0\u9ad8\u5230286 K\uff08\u5b9e\u9a8c\u62a5\u544a\u4e3a420 K\uff09\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5ba4\u6e29\u7684\u94c1\u78c1\u6027\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\u94dc\uff08Cu\uff09\u548c\u94a0\uff08Na\uff09\u63d2\u5c42\u4e5f\u80fd\u6709\u6548\u63d0\u9ad8CrI3\u7684Tc\uff0c Cu0.25CrI3\u7684Tc\u4e3a267 K\uff0cNa0.25CrI3\u7684Tc\u4e3a247 K\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u539f\u5b50\u63d2\u5c42\u7b56\u7565\uff0c\u7279\u522b\u662f\u9502\uff08Li\uff09\u63d2\u5c42\uff0c\u663e\u8457\u589e\u5f3a\u4e86CrI3\u7684\u78c1\u4ea4\u6362\u8026\u5408\uff0c\u5b9e\u73b0\u4e86\u63a5\u8fd1\u5ba4\u6e29\u7684\u94c1\u78c1\u6027\uff08\u9884\u6d4bTc\u4e3a286 K\uff0c\u5b9e\u9a8c\u62a5\u544a\u4e3a420 K\uff09\uff0c\u4e3a\u8bbe\u8ba1\u9ad8\u5c45\u91cc\u6e29\u5ea6\uff08Tc\uff09\u78c1\u6027\u534a\u5bfc\u4f53\u63d0\u4f9b\u4e86\u4e00\u79cd\u591a\u529f\u80fd\u7b56\u7565\uff0c\u5e76\u5df2\u6210\u529f\u6269\u5c55\u5230\u5176\u4ed6\u63d2\u5c42\u5242\uff08\u5982Cu\u548cNa\uff09\uff0cCu0.25CrI3\u548cNa0.25CrI3\u7684Tc\u5206\u522b\u4e3a267 K\u548c247 K\uff0c\u4e3a\u672a\u6765\u81ea\u65cb\u7535\u5b50\u5b66\u5668\u4ef6\u7684\u5b9e\u9645\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07879", "categories": ["quant-ph", "cs.DC"], "pdf": "https://arxiv.org/pdf/2508.07879", "abs": "https://arxiv.org/abs/2508.07879", "authors": ["Oscar Ferraz", "Bruno Coutinho", "Gabriel Falcao", "Marco Gomes", "Francisco A. Monteiro", "Vitor Silva"], "title": "GPU-Accelerated Syndrome Decoding for Quantum LDPC Codes below the 63 $\u03bc$s Latency Threshold", "comment": "7 pages, 3 figures, 1 table", "summary": "This paper presents a GPU-accelerated decoder for quantum low-density\nparity-check (QLDPC) codes that achieves sub-$63$ $\\mu$s latency, below the\nsurface code decoder's real-time threshold demonstrated on Google's Willow\nquantum processor. While surface codes have demonstrated below-threshold\nperformance, the encoding rates approach zero as code distances increase,\nposing challenges for scalability. Recently proposed QLDPC codes, such as those\nby Panteleev and Kalachev, offer constant-rate encoding and asymptotic goodness\nbut introduce higher decoding complexity. To address such limitation, this work\npresents a parallelized belief propagation decoder leveraging syndrome\ninformation on commodity GPU hardware. Parallelism was exploited to maximize\nperformance within the limits of target latency, allowing decoding latencies\nunder $50$ $\\mu$s for [[$784$, $24$, $24$]] codes and as low as $23.3$ $\\mu$s\nfor smaller codes, meeting the tight timing constraints of superconducting\nqubit cycles. These results show that real-time, scalable decoding of\nasymptotically good quantum codes is achievable using widely available\ncommodity hardware, advancing the feasibility of fault-tolerant quantum\ncomputation beyond surface codes.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u666e\u901aGPU\u4e0a\u8fd0\u884c\u7684\u91cf\u5b50\u4f4e\u5bc6\u5ea6\u5947\u5076\u6821\u9a8c\u7801\u89e3\u7801\u5668\uff0c\u901f\u5ea6\u5f88\u5feb\uff08\u4f4e\u4e8e50\u5fae\u79d2\uff09\uff0c\u53ef\u4ee5\u7528\u4e8e\u8d85\u8d8a\u8868\u9762\u7801\u7684\u91cf\u5b50\u8ba1\u7b97\u673a\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9Panteleev\u548cKalachev\u63d0\u51fa\u7684\u91cf\u5b50\u4f4e\u5bc6\u5ea6\u5947\u5076\u6821\u9a8c\u7801\uff08QLDPC\uff09\u6240\u5e26\u6765\u7684\u66f4\u9ad8\u89e3\u7801\u590d\u6742\u6027\uff0c\u540c\u65f6\u514b\u670d\u8868\u9762\u7801\u968f\u7801\u8ddd\u79bb\u589e\u52a0\u800c\u7f16\u7801\u901f\u7387\u8d8b\u8fd1\u4e8e\u96f6\u7684\u6269\u5c55\u6027\u6311\u6218\uff0c\u672c\u7814\u7a76\u81f4\u529b\u4e8e\u5f00\u53d1\u4e00\u79cd\u9ad8\u6548\u7684\u89e3\u7801\u5668\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7efc\u5408GPU\u786c\u4ef6\u7684\u5e76\u884c\u5316\u4fe1\u5ff5\u4f20\u64ad\u89e3\u7801\u5668\uff0c\u8be5\u89e3\u7801\u5668\u5229\u7528\u4e86\u7efc\u5408\u4fe1\u606f\u6765\u6700\u5927\u5316\u76ee\u6807\u5ef6\u8fdf\u9650\u5236\u5185\u7684\u6027\u80fd\u3002", "result": "\u6240\u63d0\u51fa\u7684GPU\u52a0\u901f\u89e3\u7801\u5668\u5b9e\u73b0\u4e86\u4f4e\u4e8e63\u5fae\u79d2\u7684\u5ef6\u8fdf\uff0c\u4f4e\u4e8e\u5728Google\u7684Willow\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u6f14\u793a\u7684\u8868\u9762\u7801\u89e3\u7801\u5668\u7684\u5b9e\u65f6\u9608\u503c\u3002\u5bf9\u4e8e[[784, 24, 24]]\u7801\uff0c\u89e3\u7801\u5ef6\u8fdf\u4f4e\u4e8e50\u5fae\u79d2\uff0c\u5bf9\u4e8e\u66f4\u5c0f\u7684\u7801\uff0c\u5ef6\u8fdf\u4f4e\u81f323.3\u5fae\u79d2\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u5728\u5546\u54c1GPU\u786c\u4ef6\u4e0a\u5b9e\u73b0\u7684\u5e76\u884c\u5316\u4fe1\u5ff5\u4f20\u64ad\u89e3\u7801\u5668\uff0c\u80fd\u591f\u4ee5\u4f4e\u4e8e50\u5fae\u79d2\u7684\u5ef6\u8fdf\u89e3\u7801[[784, 24, 24]]\u7684\u91cf\u5b50\u4f4e\u5bc6\u5ea6\u5947\u5076\u6821\u9a8c\u7801\uff0c\u6700\u4f4e\u53ef\u8fbe23.3\u5fae\u79d2\uff0c\u6ee1\u8db3\u4e86\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u5468\u671f\u7684\u4e25\u683c\u65f6\u5e8f\u8981\u6c42\u3002\u8fd9\u8868\u660e\u4f7f\u7528\u5e7f\u6cdb\u53ef\u7528\u7684\u5546\u54c1\u786c\u4ef6\uff0c\u53ef\u4ee5\u5b9e\u73b0\u5bf9\u8d85\u51fa\u884c\u6ce2\u7801\u7684\u3001\u53ef\u6269\u5c55\u7684\u3001\u5177\u6709\u826f\u597d\u6e10\u8fd1\u6027\u80fd\u7684\u91cf\u5b50\u7801\u7684\u5b9e\u65f6\u89e3\u7801\uff0c\u4ece\u800c\u63a8\u8fdb\u4e86\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.07079", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07079", "abs": "https://arxiv.org/abs/2508.07079", "authors": ["Mohamed Parvez Aslam", "Bojan Derajic", "Mohamed-Khalil Bouzidi", "Sebastian Bernhard", "Jan Oliver Ringert"], "title": "Model Predictive Control for Crowd Navigation via Learning-Based Trajectory Prediction", "comment": null, "summary": "Safe navigation in pedestrian-rich environments remains a key challenge for\nautonomous robots. This work evaluates the integration of a deep learning-based\nSocial-Implicit (SI) pedestrian trajectory predictor within a Model Predictive\nControl (MPC) framework on the physical Continental Corriere robot. Tested\nacross varied pedestrian densities, the SI-MPC system is compared to a\ntraditional Constant Velocity (CV) model in both open-loop prediction and\nclosed-loop navigation. Results show that SI improves trajectory prediction -\nreducing errors by up to 76% in low-density settings - and enhances safety and\nmotion smoothness in crowded scenes. Moreover, real-world deployment reveals\ndiscrepancies between open-loop metrics and closed-loop performance, as the SI\nmodel yields broader, more cautious predictions. These findings emphasize the\nimportance of system-level evaluation and highlight the SI-MPC framework's\npromise for safer, more adaptive navigation in dynamic, human-populated\nenvironments.", "AI": {"tldr": "\u5728\u62e5\u6324\u7684\u73af\u5883\u4e2d\uff0cSI-MPC\u7cfb\u7edf\u6bd4\u4f20\u7edf\u7684CV\u6a21\u578b\u66f4\u5b89\u5168\u3001\u66f4\u5e73\u7a33\uff0c\u4f46\u5176\u9884\u6d4b\u8fc7\u4e8e\u8c28\u614e\u3002", "motivation": "\u5728\u5bcc\u542b\u884c\u4eba\u7684\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u4ecd\u7136\u662f\u81ea\u4e3b\u673a\u5668\u4eba\u7684\u4e00\u4e2a\u5173\u952e\u6311\u6218\u3002", "method": "\u5c06\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u793e\u4f1a-\u9690\u5f0f\uff08SI\uff09\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u5668\u96c6\u6210\u5230\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u6846\u67b6\u4e2d\uff0c\u5e76\u5728\u5927\u9646\u4fe1\u4f7f\u673a\u5668\u4eba\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002", "result": "SI\u63d0\u9ad8\u4e86\u8f68\u8ff9\u9884\u6d4b\u7cbe\u5ea6\uff08\u5728\u4f4e\u5bc6\u5ea6\u73af\u5883\u4e0b\u8bef\u5dee\u6700\u591a\u53ef\u51cf\u5c1176%\uff09\uff0c\u5e76\u63d0\u9ad8\u4e86\u62e5\u6324\u573a\u666f\u4e0b\u7684\u5b89\u5168\u6027\u548c\u8fd0\u52a8\u5e73\u7a33\u6027\u3002\u7136\u800c\uff0c\u5b9e\u9645\u90e8\u7f72\u663e\u793a\uff0c\u5f00\u653e\u5f0f\u5faa\u73af\u5ea6\u91cf\u4e0e\u95ed\u73af\u6027\u80fd\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\uff0c\u56e0\u4e3aSI\u6a21\u578b\u4ea7\u751f\u4e86\u66f4\u5e7f\u6cdb\u3001\u66f4\u8c28\u614e\u7684\u9884\u6d4b\u3002", "conclusion": " SI-MPC\u6846\u67b6\u5728\u52a8\u6001\u3001\u4eba\u53e3\u5bc6\u96c6\u7684\u3067\u74b0\u5883\u4e2d\u5177\u6709\u66f4\u5b89\u5168\u3001\u66f4\u9002\u5e94\u6027\u5bfc\u822a\u7684\u6f5c\u529b\uff0c\u5e76\u5f3a\u8c03\u4e86\u7cfb\u7edf\u7ea7\u8bc4\u4f30\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.07188", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07188", "abs": "https://arxiv.org/abs/2508.07188", "authors": ["Anumita Mukhopadhyay", "Praggnyamita Ghosh", "Shibdas Roy"], "title": "Positive-divisibility of Subsystems in Quantum Dynamics", "comment": "6 pages", "summary": "It is known that the existence of memory effect can revive quantum\ncorrelations in open system dynamics. In this regard, the backflow of\ninformation from environment to the system can be identified with Complete\nPositive (CP) indivisibility as well as Positive (P) indivisibility criteria.\nIt is also known that if a quantum system is CP-divisible, it can also have\nmemory effect which can be witnessed by P-indivisibility. Here, we have\nexplored the relation of P-divisibility with unitality condition of noise\nchannels, showing that a unital channel is P-divisible. We have shown how a\nsystem channel and its environment need to be both P-divisible, but cannot be\nboth P-indivisible, provided the system-environment joint quantum state evolves\nunitarily. We have also established our results using three unitaries acting on\ndifferent sets of states. In particular, due to backflow of information from\nthe environment, quantumness of a system can increase, as shown for an example\nof the three-qubit W state.", "AI": {"tldr": "\u8bb0\u5fc6\u6548\u5e94\u53ef\u4ee5\u6062\u590d\u91cf\u5b50\u5173\u8054\uff0c\u540e\u5411\u4fe1\u606f\u6d41\u4e0eCP\u548cP\u4e0d\u53ef\u5206\u6027\u6709\u5173\u3002\u672c\u7814\u7a76\u8868\u660e\uff0c\u5e7a\u6b63\u4fe1\u9053\u662fP\u53ef\u5206\u7684\uff0c\u5e76\u4e14\u5728\u8054\u5408\u6001\u6f14\u5316\u662f\u5e7a\u6b63\u7684\u60c5\u51b5\u4e0b\uff0c\u7cfb\u7edf\u548c\u73af\u5883\u9700\u8981\u662fP\u53ef\u5206\u7684\uff0c\u4f46\u4e0d\u80fd\u540c\u65f6\u662fP\u4e0d\u53ef\u5206\u7684\u3002\u4e09\u6bd4\u7279W\u6001\u7684\u4f8b\u5b50\u8868\u660e\uff0c\u4ece\u73af\u5883\u56de\u6eaf\u7684\u4fe1\u606f\u53ef\u4ee5\u589e\u52a0\u7cfb\u7edf\u7684\u91cf\u5b50\u6027\u3002", "motivation": "\u5df2\u77e5\u8bb0\u5fc6\u6548\u5e94\u53ef\u4ee5\u6062\u590d\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\u52a8\u529b\u5b66\u4e2d\u7684\u91cf\u5b50\u5173\u8054\uff0c\u73af\u5883\u5230\u7cfb\u7edf\u7684\u540e\u5411\u4fe1\u606f\u6d41\u53ef\u4ee5\u901a\u8fc7\u5b8c\u5168\u6b63\uff08CP\uff09\u4e0d\u53ef\u5206\u6027\u548c\u6b63\uff08P\uff09\u4e0d\u53ef\u5206\u6027\u6807\u51c6\u6765\u8bc6\u522b\u3002", "method": "\u7814\u7a76\u4e86P\u53ef\u5206\u6027\u4e0e\u566a\u58f0\u4fe1\u9053\u5e7a\u6b63\u6027\u6761\u4ef6\u7684\u5173\u7cfb\uff0c\u8bc1\u660e\u4e86\u5e7a\u6b63\u4fe1\u9053\u662fP\u53ef\u5206\u7684\u3002", "result": "\u5c55\u793a\u4e86\u7cfb\u7edf\u4fe1\u9053\u53ca\u5176\u73af\u5883\u90fd\u9700\u8981\u662fP\u53ef\u5206\u7684\uff0c\u4f46\u4e0d\u80fd\u540c\u65f6\u662fP\u4e0d\u53ef\u5206\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u901a\u8fc7\u4f5c\u7528\u5728\u4e0d\u540c\u72b6\u6001\u96c6\u5408\u4e0a\u7684\u4e09\u4e2a\u5e7a\u6b63\u7b97\u7b26\u5efa\u7acb\u4e86\u8fd9\u4e00\u7ed3\u679c\uff0c\u5e76\u4ee5\u4e09\u6bd4\u7279W\u6001\u4e3a\u4f8b\uff0c\u8bf4\u660e\u4e86\u7531\u4e8e\u73af\u5883\u5411\u7cfb\u7edf\u56de\u6eaf\u4fe1\u606f\uff0c\u7cfb\u7edf\u7684\u91cf\u5b50\u6027\u53ef\u4ee5\u589e\u52a0\u3002", "conclusion": "\u7cfb\u7edf\u548c\u73af\u5883\u90fd\u9700\u8981\u662f P \u53ef\u5206\uff08P-divisible\uff09\u7684\uff0c\u4f46\u4e0d\u80fd\u540c\u65f6\u662f P \u4e0d\u53ef\u5206\uff08P-indivisible\uff09\u7684\uff0c\u524d\u63d0\u662f\u7cfb\u7edf-\u73af\u5883\u8054\u5408\u91cf\u5b50\u6001\u6f14\u5316\u662f\u5e7a\u6b63\u7684\u3002"}}
{"id": "2508.06913", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06913", "abs": "https://arxiv.org/abs/2508.06913", "authors": ["Siyuan Li", "Xi Lin", "Guangyan Li", "Zehao Liu", "Aodu Wulianghai", "Li Ding", "Jun Wu", "Jianhua Li"], "title": "Model-Agnostic Sentiment Distribution Stability Analysis for Robust LLM-Generated Texts Detection", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has resulted in\nincreasingly sophisticated AI-generated content, posing significant challenges\nin distinguishing LLM-generated text from human-written language. Existing\ndetection methods, primarily based on lexical heuristics or fine-tuned\nclassifiers, often suffer from limited generalizability and are vulnerable to\nparaphrasing, adversarial perturbations, and cross-domain shifts. In this work,\nwe propose SentiDetect, a model-agnostic framework for detecting LLM-generated\ntext by analyzing the divergence in sentiment distribution stability. Our\nmethod is motivated by the empirical observation that LLM outputs tend to\nexhibit emotionally consistent patterns, whereas human-written texts display\ngreater emotional variability. To capture this phenomenon, we define two\ncomplementary metrics: sentiment distribution consistency and sentiment\ndistribution preservation, which quantify stability under sentiment-altering\nand semantic-preserving transformations. We evaluate SentiDetect on five\ndiverse datasets and a range of advanced LLMs,including Gemini-1.5-Pro,\nClaude-3, GPT-4-0613, and LLaMa-3.3. Experimental results demonstrate its\nsuperiority over state-of-the-art baselines, with over 16% and 11% F1 score\nimprovements on Gemini-1.5-Pro and GPT-4-0613, respectively. Moreover,\nSentiDetect also shows greater robustness to paraphrasing, adversarial attacks,\nand text length variations, outperforming existing detectors in challenging\nscenarios.", "AI": {"tldr": "SentiDetect \u662f\u4e00\u79cd\u65b0\u7684 LLM \u68c0\u6d4b\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u60c5\u611f\u5206\u5e03\u7684\u7a33\u5b9a\u6027\u6765\u533a\u5206 AI \u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u6587\u672c\uff0c\u5e76\u5728\u5404\u79cd\u6761\u4ef6\u4e0b\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709 LLM \u68c0\u6d4b\u65b9\u6cd5\u6cdb\u5316\u80fd\u529b\u6709\u9650\uff0c\u5bb9\u6613\u53d7\u5230\u91ca\u4e49\u3001\u5bf9\u6297\u6027\u6270\u52a8\u548c\u8de8\u9886\u57df\u53d8\u5316\u7684\u5f71\u54cd\u3002\u56e0\u6b64\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u548c\u901a\u7528\u7684\u68c0\u6d4b\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SentiDetect \u7684\u6a21\u578b\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u6790\u60c5\u611f\u5206\u5e03\u7a33\u5b9a\u6027\u5dee\u5f02\u6765\u68c0\u6d4b LLM \u751f\u6210\u7684\u6587\u672c\u3002\u8be5\u65b9\u6cd5\u57fa\u4e8e LLM \u8f93\u51fa\u503e\u5411\u4e8e\u8868\u73b0\u51fa\u60c5\u611f\u4e00\u81f4\u6a21\u5f0f\uff0c\u800c\u4eba\u7c7b\u4e66\u5199\u6587\u672c\u7684\u60c5\u611f\u53d8\u5f02\u6027\u66f4\u5927\u7684\u89c2\u5bdf\u7ed3\u679c\u3002\u901a\u8fc7\u5b9a\u4e49\u60c5\u611f\u5206\u5e03\u4e00\u81f4\u6027\u548c\u60c5\u611f\u5206\u5e03\u4fdd\u6301\u6027\u4e24\u4e2a\u4e92\u8865\u6307\u6807\u6765\u91cf\u5316\u60c5\u611f\u53d8\u5316\u7684\u7a33\u5b9a\u6027\u548c\u8bed\u4e49\u4fdd\u6301\u7684\u7a33\u5b9a\u6027\u3002", "result": "\u5728\u4e94\u4e2a\u4e0d\u540c\u7684\u6570\u636e\u96c6\u548c\u4e00\u7cfb\u5217\u5148\u8fdb\u7684 LLM\uff08\u5305\u62ec Gemini-1.5-Pro\u3001Claude-3\u3001GPT-4-0613 \u548c LLaMa-3.3\uff09\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSentiDetect \u5728 F1 \u5206\u6570\u4e0a\u5206\u522b\u6bd4 Gemini-1.5-Pro \u548c GPT-4-0613 \u63d0\u9ad8\u4e86 16% \u548c 11%\uff0c\u5e76\u4e14\u5728\u5e94\u5bf9\u91ca\u4e49\u3001\u5bf9\u6297\u6027\u653b\u51fb\u548c\u6587\u672c\u957f\u5ea6\u53d8\u5316\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "SentiDetect \u901a\u8fc7\u5206\u6790\u60c5\u611f\u5206\u5e03\u7a33\u5b9a\u6027\u5dee\u5f02\uff0c\u5728\u533a\u5206 LLM \u751f\u6210\u6587\u672c\u548c\u4eba\u7c7b\u4e66\u5199\u6587\u672c\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u5404\u79cd\u6311\u6218\u6027\u573a\u666f\u4e0b\u5c55\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u4f18\u4e8e\u73b0\u6709\u68c0\u6d4b\u65b9\u6cd5\u3002"}}
{"id": "2508.06570", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06570", "abs": "https://arxiv.org/abs/2508.06570", "authors": ["Mohammad Zia Ur Rehman", "Anukriti Bhatnagar", "Omkar Kabde", "Shubhi Bansal", "Nagendra Kumar"], "title": "ImpliHateVid: A Benchmark Dataset and Two-stage Contrastive Learning Framework for Implicit Hate Speech Detection in Videos", "comment": "Published in ACL 2025", "summary": "The existing research has primarily focused on text and image-based hate\nspeech detection, video-based approaches remain underexplored. In this work, we\nintroduce a novel dataset, ImpliHateVid, specifically curated for implicit hate\nspeech detection in videos. ImpliHateVid consists of 2,009 videos comprising\n509 implicit hate videos, 500 explicit hate videos, and 1,000 non-hate videos,\nmaking it one of the first large-scale video datasets dedicated to implicit\nhate detection. We also propose a novel two-stage contrastive learning\nframework for hate speech detection in videos. In the first stage, we train\nmodality-specific encoders for audio, text, and image using contrastive loss by\nconcatenating features from the three encoders. In the second stage, we train\ncross-encoders using contrastive learning to refine multimodal representations.\nAdditionally, we incorporate sentiment, emotion, and caption-based features to\nenhance implicit hate detection. We evaluate our method on two datasets,\nImpliHateVid for implicit hate speech detection and another dataset for general\nhate speech detection in videos, HateMM dataset, demonstrating the\neffectiveness of the proposed multimodal contrastive learning for hateful\ncontent detection in videos and the significance of our dataset.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86 ImpliHateVid \u6570\u636e\u96c6\uff0c\u7528\u4e8e\u68c0\u6d4b\u89c6\u9891\u4e2d\u7684\u9690\u5f0f\u4ec7\u6068\u8a00\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e24\u9636\u6bb5\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u6765\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u57fa\u4e8e\u6587\u672c\u548c\u56fe\u50cf\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\uff0c\u800c\u57fa\u4e8e\u89c6\u9891\u7684\u65b9\u6cd5\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u89c6\u9891\u4e2d\u7684\u4ec7\u6068\u8a00\u8bba\u68c0\u6d4b\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u4f7f\u7528\u5bf9\u6bd4\u635f\u5931\u8bad\u7ec3\u7279\u5b9a\u6a21\u6001\u7684\u7f16\u7801\u5668\uff08\u97f3\u9891\u3001\u6587\u672c\u3001\u56fe\u50cf\uff09\uff0c\u5e76\u901a\u8fc7\u4e32\u8054\u6765\u81ea\u4e09\u4e2a\u7f16\u7801\u5668\u7684\u7279\u5f81\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u8bad\u7ec3\u8de8\u7f16\u7801\u5668\u4ee5\u5b8c\u5584\u591a\u6a21\u6001\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u7ed3\u5408\u4e86\u60c5\u611f\u3001\u60c5\u7eea\u548c\u5b57\u5e55\u7279\u5f81\u4ee5\u589e\u5f3a\u9690\u5f0f\u4ec7\u6068\u68c0\u6d4b\u3002", "result": "\u5728 ImpliHateVid \u548c HateMM \u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u6846\u67b6\u5728\u89c6\u9891\u4e2d\u7684\u4ec7\u6068\u5185\u5bb9\u68c0\u6d4b\u65b9\u9762\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u6570\u636e\u96c6\u5177\u6709\u91cd\u8981\u610f\u4e49\u3002"}}
{"id": "2508.08187", "categories": ["eess.SY", "cs.SY"], "pdf": "https://arxiv.org/pdf/2508.08187", "abs": "https://arxiv.org/abs/2508.08187", "authors": ["Swastik Sharma", "Swathi Battula", "Sri Niwas Singh"], "title": "IDSO-Managed Bid-Based Transactive Distribution Systems Design for DER Participation in Wholesale Markets While Preserving T-D Interactions", "comment": "17 Pages, 13 Figures", "summary": "Participation of Distributed Energy Resources (DERs) in bid-based Transactive\nEnergy Systems (TES) at the distribution systems facilitates strongly coupled,\nbidirectional interactions between Transmission-Distribution (T-D) systems.\nCapturing these interactions is critical for ensuring seamless integration\nwithin an Integrated Transmission and Distribution (ITD) framework. This study\nproposes a methodology to preserve such tight T-D linkages by developing an\nIndependent Distribution System Operator (IDSO) managed bid-based TES design\nfor unbalanced distribution systems. The proposed design operates within the\nITD paradigm and permits DER participation in the Wholesale Power Market (WPM)\nthrough IDSO while preserving tight T-D linkages. To this end, this research\noffers the following key contributions: a novel bid/offer\nprequalification-cum-aggregation method to ensure a grid-safe and value-based\naggregation of DERs' bids and offers for WPM participation through IDSO; and a\nretail pricing mechanism that reflects the true value of procuring or offering\nadditional units of power within the distribution system. Case studies are\nconducted on a modified IEEE 123-bus radial feeder populated with a high DER\nconcentration to validate the proposed frameworks' effectiveness in\ncoordinating the DERs efficiently and reliably.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4ea4\u6613\u7535\u80fd\u7cfb\u7edf\uff08TES\uff09\u8bbe\u8ba1\uff0c\u5141\u8bb8\u5206\u5e03\u5f0f\u80fd\u6e90\uff08DER\uff09\u901a\u8fc7\u72ec\u7acb\u5206\u5e03\u5f0f\u7cfb\u7edf\u8fd0\u8425\u5546\uff08IDSO\uff09\u53c2\u4e0e\u7535\u529b\u5e02\u573a\uff0c\u5e76\u89e3\u51b3\u4e86\u8f93\u7535-\u914d\u7535\uff08T-D\uff09\u7cfb\u7edf\u95f4\u7684\u8026\u5408\u95ee\u9898\u3002\u901a\u8fc7\u521b\u65b0\u7684\u6295\u6807\u805a\u5408\u548c\u96f6\u552e\u5b9a\u4ef7\u673a\u5236\uff0c\u5b9e\u73b0\u4e86DER\u7684\u9ad8\u6548\u534f\u8c03\u548c\u4ef7\u503c\u6700\u5927\u5316\u3002", "motivation": "\u4e3a\u4e86\u4fc3\u8fdb\u5206\u5e03\u5f0f\u80fd\u6e90\uff08DER\uff09\u5728\u57fa\u4e8e\u7ade\u4ef7\u7684\u4ea4\u6613\u7535\u80fd\u7cfb\u7edf\uff08TES\uff09\u4e2d\u7684\u53c2\u4e0e\uff0c\u5e76\u89e3\u51b3\u5176\u5728\u914d\u7535\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u6240\u5e26\u6765\u7684\u8f93\u7535-\u914d\u7535\uff08T-D\uff09\u7cfb\u7edf\u5f3a\u8026\u5408\u3001\u53cc\u5411\u4ea4\u4e92\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u96c6\u6210\u8f93\u7535-\u914d\u7535\uff08ITD\uff09\u6846\u67b6\u7684\u65e0\u7f1d\u6574\u5408\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u72ec\u7acb\u5206\u5e03\u5f0f\u7cfb\u7edf\u8fd0\u8425\u5546\uff08IDSO\uff09\u7ba1\u7406\u7684\u3001\u9762\u5411\u975e\u5e73\u8861\u914d\u7535\u7cfb\u7edf\u7684\u7ade\u4ef7\u5f0f\u4ea4\u6613\u7535\u80fd\u7cfb\u7edf\uff08TES\uff09\u8bbe\u8ba1\u3002\u8be5\u8bbe\u8ba1\u5728\u96c6\u6210\u8f93\u7535-\u914d\u7535\uff08ITD\uff09\u6846\u67b6\u5185\u8fd0\u884c\uff0c\u901a\u8fc7IDSO\u5141\u8bb8\u5206\u5e03\u5f0f\u80fd\u6e90\uff08DER\uff09\u53c2\u4e0e\u6279\u53d1\u7535\u529b\u5e02\u573a\uff08WPM\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8f93\u7535-\u914d\u7535\u7cfb\u7edf\uff08T-D\uff09\u7684\u7d27\u5bc6\u8054\u7cfb\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6295\u6807/\u62a5\u4ef7\u8d44\u683c\u9884\u5ba1\u548c\u805a\u5408\u65b9\u6cd5\uff0c\u4ee5\u786e\u4fddDER\u53c2\u4e0eWPM\u7684\u6295\u6807\u548c\u62a5\u4ef7\u662f\u5b89\u5168\u4e14\u6709\u4ef7\u503c\u7684\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u96f6\u552e\u5b9a\u4ef7\u673a\u5236\uff0c\u4ee5\u53cd\u6620\u5728\u914d\u7535\u7cfb\u7edf\u5185\u8d2d\u4e70\u6216\u63d0\u4f9b\u989d\u5b9a\u529f\u7387\u7684\u771f\u5b9e\u4ef7\u503c\u3002", "result": "\u901a\u8fc7\u5728\u5305\u542b\u9ad8\u6d53\u5ea6DER\u7684\u6539\u8fdb\u7248IEEE 123\u8282\u70b9\u8f90\u5c04\u9988\u7ebf\u4e0a\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728\u9ad8\u6548\u53ef\u9760\u5730\u534f\u8c03DER\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5305\u542b\u5927\u91cf\u5206\u5e03\u5f0f\u80fd\u6e90\uff08DER\uff09\u7684\u975e\u5e73\u8861\u914d\u7535\u7cfb\u7edf\u7684\u72ec\u7acb\u5206\u5e03\u5f0f\u7cfb\u7edf\u8fd0\u8425\u5546\uff08IDSO\uff09\u7ba1\u7406\u7ade\u4ef7\u5f0f\u4ea4\u6613\u7535\u80fd\u7cfb\u7edf\uff08TES\uff09\u7684\u8bbe\u8ba1\uff0c\u80fd\u5728\u5b9e\u9645\u8fd0\u884c\u4e2d\u6709\u6548\u534f\u8c03\u5206\u5e03\u5f0f\u80fd\u6e90\uff08DER\uff09\uff0c\u786e\u4fdd\u4e86\u7535\u529b\u4ea4\u6613\u7684\u6548\u7387\u548c\u53ef\u9760\u6027\uff0c\u5e76\u4e3a\u5b9e\u73b0\u8f93\u7535-\u914d\u7535\u7cfb\u7edf\uff08T-D\uff09\u7684\u7d27\u5bc6\u8026\u5408\u548c\u53cc\u5411\u4e92\u52a8\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07717", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07717", "abs": "https://arxiv.org/abs/2508.07717", "authors": ["Yuchen Gao", "Xiao Xu", "Eckehard Steinbach", "Daniel E. Lucani", "Qi Zhang"], "title": "Touch-Augmented Gaussian Splatting for Enhanced 3D Scene Reconstruction", "comment": null, "summary": "This paper presents a multimodal framework that integrates touch signals\n(contact points and surface normals) into 3D Gaussian Splatting (3DGS). Our\napproach enhances scene reconstruction, particularly under challenging\nconditions like low lighting, limited camera viewpoints, and occlusions.\nDifferent from the visual-only method, the proposed approach incorporates\nspatially selective touch measurements to refine both the geometry and\nappearance of the 3D Gaussian representation. To guide the touch exploration,\nwe introduce a two-stage sampling scheme that initially probes sparse regions\nand then concentrates on high-uncertainty boundaries identified from the\nreconstructed mesh. A geometric loss is proposed to ensure surface smoothness,\nresulting in improved geometry. Experimental results across diverse scenarios\nshow consistent improvements in geometric accuracy. In the most challenging\ncase with severe occlusion, the Chamfer Distance is reduced by over 15x,\ndemonstrating the effectiveness of integrating touch cues into 3D Gaussian\nSplatting. Furthermore, our approach maintains a fully online pipeline,\nunderscoring its feasibility in visually degraded environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e00\u79cd\u5c06\u89e6\u89c9\u4fe1\u53f7\uff08\u63a5\u89e6\u70b9\u3001\u8868\u9762\u6cd5\u7ebf\uff09\u878d\u51653D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u7684\u591a\u6a21\u6001\u6846\u67b6\uff0c\u901a\u8fc7\u53cc\u9636\u6bb5\u91c7\u6837\u548c\u51e0\u4f55\u635f\u5931\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5728\u4f4e\u5149\u7167\u3001\u906e\u6321\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u4e09\u7ef4\u91cd\u5efa\u7cbe\u5ea6\uff0c\u5c24\u5176\u5728\u4e25\u91cd\u906e\u6321\u4e0bChamfer\u8ddd\u79bb\u964d\u4f4e\u8d8515\u500d\uff0c\u4e14\u4fdd\u6301\u5728\u7ebf\u5904\u7406\u80fd\u529b\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u4e3a\u4e86\u63d0\u9ad83D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u5728\u4f4e\u5149\u7167\u3001\u6709\u9650\u89c6\u89d2\u548c\u906e\u6321\u7b49\u5177\u6709\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u7684\u573a\u666f\u91cd\u5efa\u80fd\u529b\u3002\u901a\u8fc7\u6574\u5408\u89e6\u89c9\u4fe1\u53f7\uff0c\u65e8\u5728\u514b\u670d\u7eaf\u89c6\u89c9\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u66f4\u51c6\u786e\u548c\u9c81\u68d2\u7684\u4e09\u7ef4\u573a\u666f\u91cd\u5efa\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u591a\u6a21\u6001\u6846\u67b6\uff0c\u5c06\u89e6\u89c9\u4fe1\u53f7\uff08\u63a5\u89e6\u70b9\u548c\u8868\u9762\u6cd5\u7ebf\uff09\u96c6\u6210\u52303D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e2d\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u53cc\u9636\u6bb5\u91c7\u6837\u7b56\u7565\uff0c\u9996\u5148\u63a2\u6d4b\u7a00\u758f\u533a\u57df\uff0c\u7136\u540e\u5173\u6ce8\u4ece\u91cd\u5efa\u7f51\u683c\u4e2d\u8bc6\u522b\u51fa\u7684\u9ad8\u4e0d\u786e\u5b9a\u6027\u8fb9\u754c\u3002\u5f15\u5165\u4e86\u51e0\u4f55\u635f\u5931\u51fd\u6570\u4ee5\u786e\u4fdd\u8868\u9762\u5149\u6ed1\u5ea6\uff0c\u4ece\u800c\u6539\u5584\u51e0\u4f55\u5f62\u72b6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u4e0e\u4ec5\u4f7f\u7528\u89c6\u89c9\u4fe1\u606f\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u7684\u96c6\u6210\u89e6\u89c9\u4fe1\u53f7\u7684\u65b9\u6cd5\u5728\u51e0\u4f55\u7cbe\u5ea6\u4e0a\u6709\u4e86\u6301\u7eed\u7684\u63d0\u5347\u3002\u5728\u6700\u4e25\u5cfb\u7684\u4e25\u91cd\u906e\u6321\u573a\u666f\u4e0b\uff0cChamfer\u8ddd\u79bb\u964d\u4f4e\u4e86\u8d85\u8fc715\u500d\uff0c\u9a8c\u8bc1\u4e86\u89e6\u89c9\u7ebf\u7d22\u57283DGS\u4e2d\u7684\u6709\u6548\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u4fdd\u6301\u4e86\u5b8c\u5168\u5728\u7ebf\u7684\u5904\u7406\u6d41\u7a0b\u3002", "conclusion": "\u672c\u7814\u7a76\u6210\u529f\u5730\u5c06\u89e6\u89c9\u4fe1\u53f7\uff08\u63a5\u89e6\u70b9\u548c\u8868\u9762\u6cd5\u7ebf\uff09\u96c6\u6210\u52303D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u6846\u67b6\u4e2d\uff0c\u663e\u8457\u63d0\u9ad8\u4e863D\u573a\u666f\u91cd\u5efa\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u5149\u7167\u3001\u89c6\u89d2\u6709\u9650\u548c\u906e\u6321\u7b49\u6311\u6218\u6027\u6761\u4ef6\u4e0b\u3002\u901a\u8fc7\u63d0\u51fa\u7684\u53cc\u9636\u6bb5\u91c7\u6837\u7b56\u7565\u548c\u51e0\u4f55\u635f\u5931\u51fd\u6570\uff0c\u5b9e\u73b0\u4e86\u5bf93D\u9ad8\u65af\u8868\u793a\u7684\u51e0\u4f55\u548c\u5916\u89c2\u7684\u7cbe\u7ec6\u8c03\u6574\uff0c\u83b7\u5f97\u4e86\u66f4\u4f18\u7684\u51e0\u4f55\u7cbe\u5ea6\u548c\u8868\u9762\u5149\u6ed1\u5ea6\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u4e25\u91cd\u906e\u6321\u7b49\u6781\u7aef\u60c5\u51b5\u4e0b\uff0cChamfer\u8ddd\u79bb\u964d\u4f4e\u4e8615\u500d\u4ee5\u4e0a\uff0c\u8bc1\u660e\u4e86\u89e6\u89c9\u7ebf\u7d22\u57283DGS\u4e2d\u7684\u6709\u6548\u6027\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4fdd\u6301\u4e86\u5b8c\u5168\u5728\u7ebf\u7684\u5904\u7406\u6d41\u7a0b\uff0c\u4f7f\u5176\u5728\u89c6\u89c9\u9000\u5316\u73af\u5883\u4e2d\u5177\u6709\u53ef\u884c\u6027\u3002"}}
{"id": "2508.06950", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06950", "abs": "https://arxiv.org/abs/2508.06950", "authors": ["Sarah Schr\u00f6der", "Thekla Morgenroth", "Ulrike Kuhl", "Valerie Vaquet", "Benjamin Paa\u00dfen"], "title": "Large Language Models Do Not Simulate Human Psychology", "comment": null, "summary": "Large Language Models (LLMs),such as ChatGPT, are increasingly used in\nresearch, ranging from simple writing assistance to complex data annotation\ntasks. Recently, some research has suggested that LLMs may even be able to\nsimulate human psychology and can, hence, replace human participants in\npsychological studies. We caution against this approach. We provide conceptual\narguments against the hypothesis that LLMs simulate human psychology. We then\npresent empiric evidence illustrating our arguments by demonstrating that\nslight changes to wording that correspond to large changes in meaning lead to\nnotable discrepancies between LLMs' and human responses, even for the recent\nCENTAUR model that was specifically fine-tuned on psychological responses.\nAdditionally, different LLMs show very different responses to novel items,\nfurther illustrating their lack of reliability. We conclude that LLMs do not\nsimulate human psychology and recommend that psychological researchers should\ntreat LLMs as useful but fundamentally unreliable tools that need to be\nvalidated against human responses for every new application.", "AI": {"tldr": "LLM\u4e0d\u80fd\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u5b66\uff0c\u5176\u53cd\u5e94\u4e0d\u7a33\u5b9a\u4e14\u4e0e\u4eba\u7c7b\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u63a2\u8ba8LLM\u5728\u5fc3\u7406\u5b66\u7814\u7a76\u4e2d\u53d6\u4ee3\u4eba\u7c7b\u53c2\u4e0e\u8005\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u5bf9\u8be5\u65b9\u6cd5\u63d0\u51fa\u8b66\u544a\u3002", "method": "\u901a\u8fc7\u6982\u5ff5\u8bba\u8bc1\u548c\u5b9e\u8bc1\u8bc1\u636e\uff0c\u5c55\u793a\u4e86LLM\u5728\u9762\u5bf9\u63aa\u8f9e\u7ec6\u5fae\u53d8\u5316\u65f6\u4e0e\u4eba\u7c7b\u53cd\u5e94\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u6307\u51fa\u4e0d\u540cLLM\u5bf9\u65b0\u9879\u76ee\u7684\u53cd\u5e94\u4e5f\u5b58\u5728\u5f88\u5927\u5dee\u5f02\u3002", "result": "LLM\u7684\u53cd\u5e94\u4f1a\u56e0\u63aa\u8f9e\u7684\u7ec6\u5fae\u53d8\u5316\u800c\u4ea7\u751f\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e14\u4e0d\u540cLLM\u7684\u53cd\u5e94\u4e5f\u4e0d\u540c\uff0c\u8bc1\u660e\u4e86\u5176\u4e0d\u53ef\u9760\u6027\u3002", "conclusion": "LLM\u4e0d\u80fd\u6a21\u62df\u4eba\u7c7b\u5fc3\u7406\u5b66\uff0c\u7814\u7a76\u8005\u5e94\u5c06LLM\u89c6\u4e3a\u5728\u6bcf\u6b21\u65b0\u5e94\u7528\u4e2d\u90fd\u9700\u8981\u6839\u636e\u4eba\u7c7b\u53cd\u5e94\u8fdb\u884c\u9a8c\u8bc1\u7684\u6709\u7528\u4f46\u4e0d\u53ef\u9760\u7684\u5de5\u5177\u3002"}}
{"id": "2508.06743", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06743", "abs": "https://arxiv.org/abs/2508.06743", "authors": ["Connor Brown"], "title": "Analysis of Schedule-Free Nonconvex Optimization", "comment": null, "summary": "First-order methods underpin most large-scale learning algorithms, yet their\nclassical convergence guarantees hinge on carefully scheduled step-sizes that\ndepend on the total horizon $T$, which is rarely known in advance. The\nSchedule-Free (SF) method promises optimal performance with hyperparameters\nthat are independent of $T$ by interpolating between Polyak--Ruppert averaging\nand momentum, but nonconvex analysis of SF has been limited or reliant on\nstrong global assumptions. We introduce a robust Lyapunov framework that, under\nonly $L$-smoothness and lower-boundedness, reduces SF analysis to a single-step\ndescent inequality. This yields horizon-agnostic bounds in the nonconvex\nsetting: $O(1/\\log T)$ for constant step + PR averaging, $O(\\log T/T)$ for a\nlinearly growing step-size, and a continuum of $O(T^{-(1-\\alpha)})$ rates for\npolynomial averaging. We complement these proofs with Performance Estimation\nProblem (PEP) experiments that numerically validate our rates and suggest that\nour $O(1/\\log T)$ bound on the original nonconvex SF algorithm may tighten to\n$O(1/T)$. Our work extends SF's horizon-free guarantees to smooth nonconvex\noptimization and charts future directions for optimal nonconvex rates.", "AI": {"tldr": "\u672c\u7814\u7a76\u901a\u8fc7 Lyapunov \u6846\u67b6\u6269\u5c55\u4e86 SF \u65b9\u6cd5\u5728\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u4e2d\u7684\u65e0\u65f6\u95f4\u8303\u56f4\u4fdd\u8bc1\uff0c\u63d0\u4f9b\u4e86 O(1/log T) \u5230 O(T^-(1-\u03b1)) \u7684\u6536\u655b\u754c\u9650\uff0c\u5e76\u901a\u8fc7 PEP \u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8fd9\u4e9b\u754c\u9650\u3002", "motivation": "\u5927\u591a\u6570\u5927\u89c4\u6a21\u5b66\u4e60\u7b97\u6cd5\u90fd\u4f9d\u8d56\u4e8e\u4e00\u9636\u65b9\u6cd5\uff0c\u4f46\u5176\u4f20\u7edf\u7684\u6536\u655b\u4fdd\u8bc1\u4f9d\u8d56\u4e8e\u5bf9\u603b\u65f6\u95f4\u8303\u56f4 T \u7684\u4ed4\u7ec6\u8c03\u5ea6\u7684\u6b65\u957f\uff0c\u800c T \u5f88\u5c11\u80fd\u63d0\u524d\u77e5\u9053\u3002SF \u65b9\u6cd5\u627f\u8bfa\u901a\u8fc7\u5728 Polyak-Ruppert \u5e73\u5747\u548c\u52a8\u91cf\u4e4b\u95f4\u8fdb\u884c\u63d2\u503c\u6765\u5b9e\u73b0\u72ec\u7acb\u4e8e T \u7684\u8d85\u53c2\u6570\u7684\u6700\u4f18\u6027\u80fd\uff0c\u4f46\u5bf9\u5176\u975e\u51f8\u6027\u7684\u5206\u6790\u6709\u9650\u6216\u4f9d\u8d56\u4e8e\u5f3a\u7684\u5168\u5c40\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684 Lyapunov \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u4ec5\u4f9d\u8d56\u4e8e L-\u5e73\u6ed1\u6027\u548c\u4e0b\u754c\u6027\uff0c\u5c06 SF \u5206\u6790\u7b80\u5316\u4e3a\u5355\u6b65\u4e0b\u964d\u4e0d\u7b49\u5f0f\u3002", "result": "\u5f97\u51fa\u4e86\u975e\u51f8\u8bbe\u7f6e\u4e0b\u7684\u65e0\u65f6\u95f4\u8303\u56f4\u754c\u9650\uff1a\u6052\u5b9a\u6b65\u957f + PR \u5e73\u5747\u4e3a O(1/log T)\uff0c\u7ebf\u6027\u589e\u957f\u6b65\u957f\u4e3a O(log T/T)\uff0c\u4ee5\u53ca\u591a\u9879\u5f0f\u5e73\u5747\u7684 O(T^-(1-\u03b1)) \u754c\u9650\u3002\u901a\u8fc7\u6027\u80fd\u4f30\u8ba1\u95ee\u9898 (PEP) \u5b9e\u9a8c\u5bf9\u8fd9\u4e9b\u8bc1\u660e\u8fdb\u884c\u4e86\u8865\u5145\uff0c\u8fd9\u4e9b\u5b9e\u9a8c\u5728\u6570\u503c\u4e0a\u9a8c\u8bc1\u4e86\u6211\u4eec\u7684\u754c\u9650\uff0c\u5e76\u8868\u660e\u6211\u4eec\u5bf9\u539f\u59cb\u975e\u51f8 SF \u7b97\u6cd5\u7684 O(1/log T) \u754c\u9650\u53ef\u80fd\u4f1a\u6536\u7d27\u5230 O(1/T)\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06 SF \u65b9\u6cd5\u7684\u65e0\u65f6\u95f4\u8303\u56f4\u4fdd\u8bc1\u6269\u5c55\u5230\u5149\u6ed1\u975e\u51f8\u4f18\u5316\uff0c\u5e76\u4e3a\u6700\u4f18\u975e\u51f8\u6536\u655b\u901f\u5ea6\u6307\u660e\u4e86\u672a\u6765\u65b9\u5411\u3002"}}
{"id": "2508.07280", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07280", "abs": "https://arxiv.org/abs/2508.07280", "authors": ["Maoyuan Wang", "Jianhui Zhou", "Yugui Yao"], "title": "Linear and nonlinear optical responses in Green's function formula", "comment": "21 pages, 5 figures", "summary": "Linear and nonlinear optical effect has been widely discussed in large\nquantity of materials using theoretical or experimental methods. Except linear\noptical conductivity, higher-order nonlinear responses are not studied fully.\nStarting from density operator method, we derive optical conductivities of\ndifferent orders in Green's function formula, and also connect them to novel\nphysical quantities, such as Berry curvature, Berry curvature dipole,\nthird-order nonlinear Hall conductivity and so on. Based on the advantages of\nGreen's function formulas, we believe that these formulas have a lot of\nbenefits for many-body effect study in high-order nonlinear optical responses.", "AI": {"tldr": "\u5229\u7528\u683c\u6797\u51fd\u6570\u65b9\u6cd5\u7814\u7a76\u9ad8\u9636\u975e\u7ebf\u6027\u5149\u5b66\u54cd\u5e94\uff0c\u5e76\u5f15\u5165\u4e86\u65b0\u7684\u7269\u7406\u91cf\u3002", "motivation": "\u5c3d\u7ba1\u7ebf\u6027\u5149\u5b66\u548c\u975e\u7ebf\u6027\u5149\u5b66\u6548\u5e94\u5728\u591a\u79cd\u6750\u6599\u4e2d\u5f97\u5230\u4e86\u5e7f\u6cdb\u7814\u7a76\uff0c\u4f46\u9ad8\u9636\u975e\u7ebf\u6027\u54cd\u5e94\u7684\u7814\u7a76\u4ecd\u4e0d\u5145\u5206\u3002", "method": "\u4ece\u5bc6\u5ea6\u7b97\u7b26\u51fa\u53d1\uff0c\u63a8\u5bfc\u4e86\u4e0d\u540c\u9636\u6570\u7684\u5149\u5b66\u7535\u5bfc\u7387\u5728\u683c\u6797\u51fd\u6570\u516c\u5f0f\u4e2d\u7684\u8868\u8fbe\u5f0f\uff0c\u5e76\u5c06\u5176\u4e0e\u8d1d\u91cc\u66f2\u7387\u3001\u8d1d\u91cc\u66f2\u7387\u5076\u6781\u5b50\u3001\u4e09\u9636\u975e\u7ebf\u6027\u970d\u5c14\u7535\u5bfc\u7387\u7b49\u7269\u7406\u91cf\u8054\u7cfb\u8d77\u6765\u3002", "result": "\u63a8\u5bfc\u4e86\u4e0d\u540c\u9636\u6570\u7684\u5149\u5b66\u7535\u5bfc\u7387\u7684\u683c\u6797\u51fd\u6570\u516c\u5f0f\uff0c\u5e76\u5c06\u5176\u4e0e\u8d1d\u91cc\u66f2\u7387\u3001\u8d1d\u91cc\u66f2\u7387\u5076\u6781\u5b50\u3001\u4e09\u9636\u975e\u7ebf\u6027\u970d\u5c14\u7535\u5bfc\u7387\u7b49\u65b0\u7269\u7406\u91cf\u8054\u7cfb\u8d77\u6765\uff0c\u5c55\u793a\u4e86\u683c\u6797\u51fd\u6570\u516c\u5f0f\u5728\u7814\u7a76\u9ad8\u9636\u975e\u7ebf\u6027\u5149\u5b66\u54cd\u5e94\u4e2d\u7684\u591a\u4f53\u6548\u5e94\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c06\u683c\u6797\u51fd\u6570\u516c\u5f0f\u4e0e\u9ad8\u9636\u975e\u7ebf\u6027\u5149\u5b66\u54cd\u5e94\u8054\u7cfb\u8d77\u6765\uff0c\u5e76\u5f15\u5165\u4e86\u8d1d\u91cc\u66f2\u7387\u3001\u8d1d\u91cc\u66f2\u7387\u5076\u6781\u5b50\u3001\u4e09\u9636\u975e\u7ebf\u6027\u970d\u5c14\u7535\u5bfc\u7387\u7b49\u65b0\u7269\u7406\u91cf\uff0c\u4e3a\u7814\u7a76\u9ad8\u9636\u975e\u7ebf\u6027\u5149\u5b66\u54cd\u5e94\u4e2d\u7684\u591a\u4f53\u6548\u5e94\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u548c\u5de5\u5177\u3002"}}
{"id": "2508.07080", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07080", "abs": "https://arxiv.org/abs/2508.07080", "authors": ["Haolin Liu", "Zijun Guo", "Yanbo Chen", "Jiaqi Chen", "Huilong Yu", "Junqiang Xi"], "title": "An Evolutionary Game-Theoretic Merging Decision-Making Considering Social Acceptance for Autonomous Driving", "comment": null, "summary": "Highway on-ramp merging is of great challenge for autonomous vehicles (AVs),\nsince they have to proactively interact with surrounding vehicles to enter the\nmain road safely within limited time. However, existing decision-making\nalgorithms fail to adequately address dynamic complexities and social\nacceptance of AVs, leading to suboptimal or unsafe merging decisions. To\naddress this, we propose an evolutionary game-theoretic (EGT) merging\ndecision-making framework, grounded in the bounded rationality of human\ndrivers, which dynamically balances the benefits of both AVs and main-road\nvehicles (MVs). We formulate the cut-in decision-making process as an EGT\nproblem with a multi-objective payoff function that reflects human-like driving\npreferences. By solving the replicator dynamic equation for the evolutionarily\nstable strategy (ESS), the optimal cut-in timing is derived, balancing\nefficiency, comfort, and safety for both AVs and MVs. A real-time driving style\nestimation algorithm is proposed to adjust the game payoff function online by\nobserving the immediate reactions of MVs. Empirical results demonstrate that we\nimprove the efficiency, comfort and safety of both AVs and MVs compared with\nexisting game-theoretic and traditional planning approaches across multi-object\nmetrics.", "AI": {"tldr": "A new EGT framework for AV highway merging improves safety, efficiency, and comfort by considering human driving behavior and dynamically adjusting to surrounding vehicles.", "motivation": "Existing decision-making algorithms for autonomous vehicles (AVs) at highway on-ramps fail to adequately address dynamic complexities and social acceptance, leading to suboptimal or unsafe merging decisions. There is a need for a framework that accounts for the bounded rationality of human drivers and balances the benefits of both AVs and main-road vehicles (MVs).", "method": "An evolutionary game-theoretic (EGT) merging decision-making framework is proposed. The cut-in decision-making process is formulated as an EGT problem with a multi-objective payoff function. The replicator dynamic equation is solved to derive the optimal cut-in timing. A real-time driving style estimation algorithm adjusts the game payoff function online.", "result": "Empirical results demonstrate that the proposed framework improves the efficiency, comfort, and safety of both AVs and MVs compared with existing game-theoretic and traditional planning approaches across multi-object metrics.", "conclusion": "We propose an evolutionary game-theoretic (EGT) merging decision-making framework that balances the benefits of AVs and main-road vehicles (MVs), grounded in the bounded rationality of human drivers. The framework formulates the cut-in decision-making process as an EGT problem with a multi-objective payoff function reflecting human-like driving preferences. By solving the replicator dynamic equation for the evolutionarily stable strategy (ESS), we derive the optimal cut-in timing, balancing efficiency, comfort, and safety for both AVs and MVs. A real-time driving style estimation algorithm adjusts the game payoff function online by observing the immediate reactions of MVs. Empirical results demonstrate improvements in efficiency, comfort, and safety for both AVs and MVs compared to existing approaches."}}
{"id": "2508.07228", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07228", "abs": "https://arxiv.org/abs/2508.07228", "authors": ["Daniel Sabi Takou", "Amidou Boukari", "Assimiou Yarou Mora", "Gabriel Y. H. Avossevou"], "title": "Squeezed Coherent States in Supersymmetric Quantum Mechanics with Position-Dependent Mass", "comment": null, "summary": "In this paper, we construct and analyze a class of squeezed coherent states\nwithin the framework of supersymmetric quantum mechanics (SUSYQM) involving a\nposition-dependent mass (PDM). Using a deformed algebraic structure, we\ngeneralize the creation and annihilation operators to accommodate spatially\nvarying mass profiles. The resulting states exhibit non-classical features,\nsuch as squeezing, coherence, and modified uncertainty relations, strongly\ninfluenced by both the\n  deformation parameters and the mass function. We explore their physical\nproperties through expectation values, variances, and probability densities.\nThis work provides a pathway toward extending coherent state theory to more\ncomplex quantum systems with geometrical and algebraic richness.", "AI": {"tldr": "\u672c\u7814\u7a76\u5728\u8d85\u5bf9\u79f0\u91cf\u5b50\u529b\u5b66\uff08SUSYQM\uff09\u6846\u67b6\u548c\u4f4d\u7f6e\u76f8\u5173\u8d28\u91cf\uff08PDM\uff09\u6982\u5ff5\u4e0b\uff0c\u5229\u7528\u53d8\u5f62\u4ee3\u6570\u7ed3\u6784\u63a8\u5e7f\u4e86\u4ea7\u751f\u548c\u6e6e\u706d\u7b97\u7b26\uff0c\u5f97\u5230\u4e86\u5177\u6709\u538b\u7f29\u3001\u76f8\u5e72\u548c\u4fee\u6b63\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u7b49\u975e\u7ecf\u5178\u7279\u5f81\u7684\u65b0\u578b\u91cf\u5b50\u6001\uff0c\u8fd9\u4e9b\u7279\u5f81\u540c\u65f6\u53d7\u5230\u5f62\u53d8\u53c2\u6570\u548c\u8d28\u91cf\u51fd\u6570\u7684\u5f71\u54cd\uff0c\u4e3a\u76f8\u5e72\u6001\u7406\u8bba\u5728\u66f4\u590d\u6742\u7684\u91cf\u5b50\u7cfb\u7edf\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5c06\u76f8\u5e72\u6001\u7406\u8bba\u6269\u5c55\u5230\u66f4\u590d\u6742\u7684\u91cf\u5b50\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u5177\u6709\u4e30\u5bcc\u7684\u51e0\u4f55\u548c\u4ee3\u6570\u7ed3\u6784\u3002", "method": "\u5229\u7528\u53d8\u5f62\u4ee3\u6570\u7ed3\u6784\uff0c\u6211\u4eec\u63a8\u5e7f\u4e86\u4ea7\u751f\u548c\u6e6e\u706d\u7b97\u7b26\uff0c\u4ee5\u9002\u5e94\u7a7a\u95f4\u53d8\u5316\u7684\u8d28\u91cf\u5206\u5e03\u3002", "result": "\u751f\u6210\u7684\u65b0\u578b\u91cf\u5b50\u6001\u8868\u73b0\u51fa\u538b\u7f29\u3001\u76f8\u5e72\u548c\u4fee\u6b63\u7684\u4e0d\u786e\u5b9a\u6027\u5173\u7cfb\u7b49\u975e\u7ecf\u5178\u7279\u5f81\uff0c\u8fd9\u4e9b\u7279\u5f81\u53d7\u5230\u5f62\u53d8\u53c2\u6570\u548c\u8d28\u91cf\u51fd\u6570\u7684\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u53d1\u5c55\u8d85\u5bf9\u79f0\u91cf\u5b50\u529b\u5b66\uff08SUSYQM\uff09\u6846\u67b6\u4e0b\u7684\u76f8\u5e72\u6001\u7406\u8bba\uff0c\u5e76\u5f15\u5165\u4f4d\u7f6e\u76f8\u5173\u8d28\u91cf\uff08PDM\uff09\u7684\u6982\u5ff5\uff0c\u901a\u8fc7\u53d8\u5f62\u4ee3\u6570\u7ed3\u6784\u63a8\u5e7f\u4e86\u4ea7\u751f\u548c\u6e6e\u706d\u7b97\u7b26\uff0c\u4ee5\u9002\u5e94\u7a7a\u95f4\u53d8\u5316\u7684\u8d28\u91cf\u5206\u5e03\u3002"}}
{"id": "2508.06971", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.06971", "abs": "https://arxiv.org/abs/2508.06971", "authors": ["Mohamed Basem", "Islam Oshallah", "Ali Hamdi", "Khaled Shaban", "Hozaifa Kassab"], "title": "Two-Stage Quranic QA via Ensemble Retrieval and Instruction-Tuned Answer Extraction", "comment": "8 pages , 4 figures , Accepted in Aiccsa 2025 ,\n  https://conferences.sigappfr.org/aiccsa2025/", "summary": "Quranic Question Answering presents unique challenges due to the linguistic\ncomplexity of Classical Arabic and the semantic richness of religious texts. In\nthis paper, we propose a novel two-stage framework that addresses both passage\nretrieval and answer extraction. For passage retrieval, we ensemble fine-tuned\nArabic language models to achieve superior ranking performance. For answer\nextraction, we employ instruction-tuned large language models with few-shot\nprompting to overcome the limitations of fine-tuning on small datasets. Our\napproach achieves state-of-the-art results on the Quran QA 2023 Shared Task,\nwith a MAP@10 of 0.3128 and MRR@10 of 0.5763 for retrieval, and a pAP@10 of\n0.669 for extraction, substantially outperforming previous methods. These\nresults demonstrate that combining model ensembling and instruction-tuned\nlanguage models effectively addresses the challenges of low-resource question\nanswering in specialized domains.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u96c6\u6210 Arabic language models \u548c\u4f7f\u7528 instruction-tuned LLM \u8fdb\u884c few-shot prompting\uff0c\u5728\u53e4\u5170\u7ecf\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\u3002", "motivation": "\u89e3\u51b3\u53e4\u5178\u963f\u62c9\u4f2f\u8bed\u7684\u590d\u6742\u6027\u548c\u5b97\u6559\u6587\u672c\u7684\u8bed\u4e49\u4e30\u5bcc\u6027\u5e26\u6765\u7684\u53e4\u5170\u7ecf\u95ee\u7b54\u7684\u72ec\u7279\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u5305\u62ec\u7528\u4e8e passage retrieval \u7684 Arabic language models \u96c6\u6210\uff0c\u4ee5\u53ca\u7528\u4e8e answer extraction \u7684 instruction-tuned LLM \u548c few-shot prompting\u3002", "result": "\u5728 Quran QA 2023 Shared Task \u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5728\u68c0\u7d22\u65b9\u9762 MAP@10 \u4e3a 0.3128\uff0cMRR@10 \u4e3a 0.5763\uff1b\u5728\u62bd\u53d6\u65b9\u9762 pAP@10 \u4e3a 0.669\uff0c\u663e\u8457\u4f18\u4e8e\u4ee5\u5f80\u65b9\u6cd5\u3002", "conclusion": "\u7ed3\u5408\u6a21\u578b\u96c6\u6210\u548c\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u6709\u6548\u89e3\u51b3\u4e13\u4e1a\u9886\u57df\u4f4e\u8d44\u6e90\u95ee\u7b54\u7684\u6311\u6218\u3002"}}
{"id": "2508.06623", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06623", "abs": "https://arxiv.org/abs/2508.06623", "authors": ["Sihan Ma", "Qiming Wu", "Ruotong Jiang", "Frank Burns"], "title": "ContextGuard-LVLM: Enhancing News Veracity through Fine-grained Cross-modal Contextual Consistency Verification", "comment": null, "summary": "The proliferation of digital news media necessitates robust methods for\nverifying content veracity, particularly regarding the consistency between\nvisual and textual information. Traditional approaches often fall short in\naddressing the fine-grained cross-modal contextual consistency (FCCC) problem,\nwhich encompasses deeper alignment of visual narrative, emotional tone, and\nbackground information with text, beyond mere entity matching. To address this,\nwe propose ContextGuard-LVLM, a novel framework built upon advanced\nVision-Language Large Models (LVLMs) and integrating a multi-stage contextual\nreasoning mechanism. Our model is uniquely enhanced through reinforced or\nadversarial learning paradigms, enabling it to detect subtle contextual\nmisalignments that evade zero-shot baselines. We extend and augment three\nestablished datasets (TamperedNews-Ent, News400-Ent, MMG-Ent) with new\nfine-grained contextual annotations, including \"contextual sentiment,\" \"visual\nnarrative theme,\" and \"scene-event logical coherence,\" and introduce a\ncomprehensive CTXT (Contextual Coherence) entity type. Extensive experiments\ndemonstrate that ContextGuard-LVLM consistently outperforms state-of-the-art\nzero-shot LVLM baselines (InstructBLIP and LLaVA 1.5) across nearly all\nfine-grained consistency tasks, showing significant improvements in complex\nlogical reasoning and nuanced contextual understanding. Furthermore, our model\nexhibits superior robustness to subtle perturbations and a higher agreement\nrate with human expert judgments on challenging samples, affirming its efficacy\nin discerning sophisticated forms of context detachment.", "AI": {"tldr": "\u63d0\u51fa ContextGuard-LVLM \u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u4e0a\u4e0b\u6587\u63a8\u7406\u548c\u5f3a\u5316/\u5bf9\u6297\u5b66\u4e60\uff0c\u89e3\u51b3\u4e86\u6570\u5b57\u65b0\u95fb\u4e2d\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709 LVLM \u57fa\u7ebf\u3002", "motivation": "\u6570\u5b57\u65b0\u95fb\u5a92\u4f53\u7684\u6fc0\u589e\u9700\u8981\u5f3a\u6709\u529b\u7684\u65b9\u6cd5\u6765\u9a8c\u8bc1\u5185\u5bb9\u7684\u771f\u5b9e\u6027\uff0c\u7279\u522b\u662f\u5728\u89c6\u89c9\u548c\u6587\u672c\u4fe1\u606f\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u65b9\u9762\u3002\u4f20\u7edf\u65b9\u6cd5\u5728\u89e3\u51b3\u7ec6\u7c92\u5ea6\u8de8\u6a21\u6001\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff08FCCC\uff09\u95ee\u9898\u65b9\u9762\u5e38\u5e38\u4e0d\u8db3\uff0c\u8be5\u95ee\u9898\u5305\u62ec\u6bd4\u5b9e\u4f53\u5339\u914d\u66f4\u6df1\u5c42\u6b21\u7684\u89c6\u89c9\u53d9\u4e8b\u3001\u60c5\u611f\u57fa\u8c03\u548c\u80cc\u666f\u4fe1\u606f\u4e0e\u6587\u672c\u7684\u5339\u914d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ContextGuard-LVLM \u7684\u65b0\u9896\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5148\u8fdb\u7684\u89c6\u89c9-\u8bed\u8a00\u5927\u578b\u6a21\u578b (LVLMs)\uff0c\u5e76\u96c6\u6210\u4e86\u591a\u9636\u6bb5\u4e0a\u4e0b\u6587\u63a8\u7406\u673a\u5236\u3002\u901a\u8fc7\u5f3a\u5316\u6216\u5bf9\u6297\u5b66\u4e60\u8303\u5f0f\u5bf9\u5176\u6a21\u578b\u8fdb\u884c\u4e86\u72ec\u7279\u589e\u5f3a\uff0c\u4f7f\u5176\u80fd\u591f\u68c0\u6d4b\u5230\u9003\u907f\u96f6\u6837\u672c\u57fa\u7ebf\u7684\u7ec6\u5fae\u4e0a\u4e0b\u6587\u5931\u51c6\u3002\u6269\u5c55\u5e76\u589e\u5f3a\u4e86\u4e09\u4e2a\u5df2\u5efa\u7acb\u7684\u6570\u636e\u96c6\uff08TamperedNews-Ent\u3001News400-Ent\u3001MMG-Ent\uff09\uff0c\u52a0\u5165\u4e86\u65b0\u7684\u7ec6\u7c92\u5ea6\u4e0a\u4e0b\u6587\u6ce8\u91ca\uff0c\u5305\u62ec\u201c\u4e0a\u4e0b\u6587\u60c5\u611f\u201d\u3001\u201c\u89c6\u89c9\u53d9\u4e8b\u4e3b\u9898\u201d\u548c\u201c\u573a\u666f-\u4e8b\u4ef6\u903b\u8f91\u4e00\u81f4\u6027\u201d\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5168\u9762\u7684 CTXT\uff08\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\uff09\u5b9e\u4f53\u7c7b\u578b\u3002", "result": "ContextGuard-LVLM \u6846\u67b6\u80fd\u591f\u68c0\u6d4b\u5230\u9003\u907f\u96f6\u6837\u672c\u57fa\u7ebf\u7684\u7ec6\u5fae\u4e0a\u4e0b\u6587\u5931\u51c6\uff0c\u5e76\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u548c\u7ec6\u5fae\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u663e\u8457\u6539\u8fdb\u3002\u8be5\u6a21\u578b\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\u4e0a\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u5224\u65ad\u5177\u6709\u66f4\u9ad8\u7684 \u4e00\u81f4\u6027\u3002", "conclusion": "ContextGuard-LVLM \u5728\u51e0\u4e4e\u6240\u6709\u7ec6\u7c92\u5ea6\u4e00\u81f4\u6027\u4efb\u52a1\u4e0a\u5747\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c LVLM \u57fa\u7ebf\uff08InstructBLIP \u548c LLaVA 1.5\uff09\uff0c\u5728\u590d\u6742\u903b\u8f91\u63a8\u7406\u548c\u7ec6\u5fae\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002\u6b64\u5916\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5728\u5e94\u5bf9\u7ec6\u5fae\u6270\u52a8\u65b9\u9762\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u6837\u672c\u4e0a\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u5224\u65ad\u5177\u6709\u66f4\u9ad8\u7684 \u4e00\u81f4\u6027\uff0c\u80af\u5b9a\u4e86\u5176\u5728\u8bc6\u522b\u590d\u6742\u7684\u4e0a\u4e0b\u6587\u5206\u79bb\u5f62\u5f0f\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.08217", "categories": ["eess.SY", "cs.SY", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.08217", "abs": "https://arxiv.org/abs/2508.08217", "authors": ["Jimin Choi", "Max Z. Li"], "title": "Autonomous Air-Ground Vehicle Operations Optimization in Hazardous Environments: A Multi-Armed Bandit Approach", "comment": null, "summary": "Hazardous environments such as chemical spills, radiological zones, and\nbio-contaminated sites pose significant threats to human safety and public\ninfrastructure. Rapid and reliable hazard mitigation in these settings often\nunsafe for humans, calling for autonomous systems that can adaptively sense and\nrespond to evolving risks. This paper presents a decision-making framework for\nautonomous vehicle dispatch in hazardous environments with uncertain and\nevolving risk levels. The system integrates a Bayesian Upper Confidence Bound\n(BUCB) sensing strategy with task-specific vehicle routing problems with\nprofits (VRPP), enabling adaptive coordination of unmanned aerial vehicles\n(UAVs) for hazard sensing and unmanned ground vehicles (UGVs) for cleaning.\nUsing VRPP allows selective site visits under resource constraints by assigning\neach site a visit value that reflects sensing or cleaning priorities.\nSite-level hazard beliefs are maintained through a time-weighted Bayesian\nupdate. BUCB scores guide UAV routing to balance exploration and exploitation\nunder uncertainty, while UGV routes are optimized to maximize expected hazard\nreduction under resource constraints. Simulation results demonstrate that our\nframework reduces the number of dispatch cycles to resolve hazards by around\n30% on average compared to baseline dispatch strategies, underscoring the value\nof uncertainty-aware vehicle dispatch for reliable hazard mitigation.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542bBUCB\u4f20\u611f\u7b56\u7565\u548cVRPP\u7684\u81ea\u4e3b\u8f66\u8f86\u8c03\u5ea6\u6846\u67b6\uff0c\u7528\u4e8e\u5371\u9669\u73af\u5883\u7684\u5371\u5bb3\u7f13\u89e3\u3002\u4eff\u771f\u7ed3\u679c\u663e\u793a\uff0c\u8be5\u6846\u67b6\u80fd\u5c06\u6240\u9700\u8c03\u5ea6\u5468\u671f\u5e73\u5747\u51cf\u5c11\u7ea630%\u3002", "motivation": "\u5371\u9669\u73af\u5883\uff08\u5982\u5316\u5b66\u54c1\u6cc4\u6f0f\u3001\u8f90\u5c04\u533a\u57df\u3001\u751f\u7269\u6c61\u67d3\u573a\u5730\uff09\u5bf9\u4eba\u7c7b\u5b89\u5168\u548c\u516c\u5171\u57fa\u7840\u8bbe\u65bd\u6784\u6210\u91cd\u5927\u5a01\u80c1\u3002\u5728\u8fd9\u4e9b\u73af\u5883\u4e2d\uff0c\u5feb\u901f\u53ef\u9760\u7684\u5371\u5bb3\u7f13\u89e3\u63aa\u65bd\u901a\u5e38\u5bf9\u4eba\u7c7b\u6765\u8bf4\u662f\u4e0d\u5b89\u5168\u7684\uff0c\u56e0\u6b64\u9700\u8981\u80fd\u591f\u81ea\u9002\u5e94\u611f\u77e5\u548c\u54cd\u5e94\u4e0d\u65ad\u53d8\u5316\u7684\u98ce\u9669\u7684\u81ea\u4e3b\u7cfb\u7edf\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u5371\u9669\u73af\u5883\u7684\u81ea\u4e3b\u8f66\u8f86\u8c03\u5ea6\u51b3\u7b56\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u8d1d\u53f6\u65af\u4e0a\u9650\u7f6e\u4fe1\u5ea6\uff08BUCB\uff09\u4f20\u611f\u7b56\u7565\u548c\u7279\u5b9a\u4efb\u52a1\u7684\u8f66\u8f86\u8def\u5f84\u89c4\u5212\uff08VRPP\uff09\uff0c\u5b9e\u73b0\u4e86\u65e0\u4eba\u673a\uff08UAVs\uff09\u548c\u65e0\u4eba\u5730\u9762\u8f66\u8f86\uff08UGVs\uff09\u7684\u81ea\u9002\u5e94\u534f\u8c03\u3002BUCB\u7b56\u7565\u7528\u4e8e\u5f15\u5bfcUAV\u8fdb\u884c\u4fe1\u606f\u63a2\u7d22\u548c\u5229\u7528\uff0c\u800cUGV\u7684\u8def\u5f84\u5219\u6839\u636e\u8d44\u6e90\u9650\u5236\u4f18\u5316\uff0c\u4ee5\u6700\u5927\u5316\u9884\u671f\u7684\u5371\u5bb3\u964d\u4f4e\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u8c03\u5ea6\u7b56\u7565\u76f8\u6bd4\uff0c\u672c\u6846\u67b6\u5c06\u89e3\u51b3\u5371\u5bb3\u6240\u9700\u7684\u8c03\u5ea6\u5468\u671f\u5e73\u5747\u51cf\u5c11\u4e86\u7ea630%\uff0c\u8bc1\u660e\u4e86\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u8f66\u8f86\u8c03\u5ea6\u5728\u53ef\u9760\u5371\u5bb3\u7f13\u89e3\u4e2d\u7684\u4ef7\u503c\u3002", "conclusion": "\u672c\u6846\u67b6\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u611f\u77e5\u548c\u4efb\u52a1\u5bfc\u5411\u7684\u8f66\u8f86\u8def\u5f84\u89c4\u5212\uff0c\u80fd\u591f\u6709\u6548\u964d\u4f4e\u5371\u9669\u73af\u5883\u4e2d\u7684\u5371\u5bb3\uff0c\u5e76\u80fd\u901a\u8fc7\u8c03\u5ea6\u8f66\u8f86\u89e3\u51b3\u6f5c\u5728\u7684\u5371\u5bb3\uff0c\u4ece\u800c\u51cf\u5c11\u7ea630%\u7684\u8c03\u5ea6\u5468\u671f\u3002"}}
{"id": "2508.07898", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07898", "abs": "https://arxiv.org/abs/2508.07898", "authors": ["Fabian Hader", "Jan Vogelbruch", "Simon Humpohl", "Tobias Hangleiter", "Chimezie Eguzo", "Stefan Heinen", "Stefanie Meyer", "Stefan van Waasen"], "title": "On Noise-Sensitive Automatic Tuning of Gate-Defined Sensor Dots", "comment": null, "summary": "In gate-defined quantum dot systems, the conductance change of\nelectrostatically coupled sensor dots allows the observation of the quantum\ndots' charge and spin states. Therefore, the sensor dot must be optimally\nsensitive to changes in its electrostatic environment. A series of conductance\nmeasurements varying the two sensor-dot-forming barrier gate voltages serve to\ntune the dot into a corresponding operating regime. In this paper, we analyze\nthe noise characteristics of the measured data and define a criterion to\nidentify continuous regions with a sufficient signal-gradient-to-noise ratio.\nHence, accurate noise estimation is required when identifying the optimal\noperating regime. Therefore, we evaluate several existing noise estimators,\nmodify them for 1D data, optimize their parameters, and analyze their quality\nbased on simulated data. The estimator of Chen et al. turns out to be best\nsuited for our application concerning minimally scattering results.\nFurthermore, using this estimator in an algorithm for flank-of-interest\nclassification in measured data shows the relevance and applicability of our\napproach.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u4f18\u5316\u95e8\u63a7\u91cf\u5b50\u70b9\u7cfb\u7edf\u4e2d\u7684\u4f20\u611f\u70b9\u7075\u654f\u5ea6\uff0c\u901a\u8fc7\u8bc4\u4f30\u548c\u9009\u62e9\u6700\u4f73\u566a\u58f0\u4f30\u8ba1\u5668\u6765\u7cbe\u786e\u6d4b\u91cf\u91cf\u5b50\u70b9\u7684\u7535\u8377\u548c\u81ea\u65cb\u72b6\u6001\uff0cChen et al. \u7684\u65b9\u6cd5\u88ab\u8bc1\u660e\u662f\u6700\u6709\u6548\u7684\u3002", "motivation": "\u4e3a\u4e86\u5728\u95e8\u63a7\u91cf\u5b50\u70b9\u7cfb\u7edf\u4e2d\uff0c\u901a\u8fc7\u4f20\u611f\u70b9\u7684\u7535\u5bfc\u6d4b\u91cf\u6765\u7cbe\u786e\u5730\u89c2\u5bdf\u91cf\u5b50\u70b9\u7684\u7535\u8377\u548c\u81ea\u65cb\u72b6\u6001\uff0c\u9700\u8981\u4f18\u5316\u4f20\u611f\u70b9\u7684\u9759\u7535\u7075\u654f\u5ea6\u3002", "method": "\u8bc4\u4f30\u548c\u4f18\u5316\u4e86\u591a\u79cd\u73b0\u6709\u566a\u58f0\u4f30\u8ba1\u5668\uff0c\u5e76\u5bf9\u5b83\u4eec\u8fdb\u884c\u4e86\u4fee\u6539\u4ee5\u9002\u5e94\u4e00\u7ef4\u6570\u636e\uff0c\u7136\u540e\u57fa\u4e8e\u6a21\u62df\u6570\u636e\u5206\u6790\u4e86\u5b83\u4eec\u7684\u8d28\u91cf\u3002", "result": " Chen et al. \u7684\u566a\u58f0\u4f30\u8ba1\u5668\u88ab\u8bc1\u660e\u6700\u9002\u5408\u6b64\u5e94\u7528\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u5728\u5b9e\u9645\u6d4b\u91cf\u6570\u636e\u4e2d\u663e\u793a\u4e86\u5176\u76f8\u5173\u6027\u548c\u9002\u7528\u6027\u3002", "conclusion": "\u786e\u5b9a\u7528\u4e8e\u91cf\u5b50\u70b9\u7535\u8377\u548c\u81ea\u65cb\u72b6\u6001\u6d4b\u91cf\u7684\u4f20\u611f\u70b9\u6700\u4f73\u5de5\u4f5c\u72b6\u6001\uff0c\u9700\u8981\u7cbe\u786e\u7684\u566a\u58f0\u4f30\u8ba1\uff0cChen et al. \u7684\u566a\u58f0\u4f30\u8ba1\u5668\u5728\u6240\u6d4b\u5e94\u7528\u4e2d\u8868\u73b0\u6700\u4f73\u3002"}}
{"id": "2508.07909", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07909", "abs": "https://arxiv.org/abs/2508.07909", "authors": ["Bile Peng", "Karl-Ludwig Besser", "Shanpu Shen", "Finn Siegismund-Poschmann", "Ramprasad Raghunath", "Daniel M. Mittleman", "Vahid Jamali", "Eduard A. Jorswieck"], "title": "RIS-Assisted NOMA with Partial CSI and Mutual Coupling: A Machine Learning Approach", "comment": null, "summary": "Non-orthogonal multiple access (NOMA) is a promising multiple access\ntechnique. Its performance depends strongly on the wireless channel property,\nwhich can be enhanced by reconfigurable intelligent surfaces (RISs). In this\npaper, we jointly optimize base station (BS) precoding and RIS configuration\nwith unsupervised machine learning (ML), which looks for the optimal solution\nautonomously. In particular, we propose a dedicated neural network (NN)\narchitecture RISnet inspired by domain knowledge in communication. Compared to\nstate-of-the-art, the proposed approach combines analytical optimal BS\nprecoding and ML-enabled RIS, has a high scalability to control more than 1000\nRIS elements, has a low requirement for channel state information (CSI) in\ninput, and addresses the mutual coupling between RIS elements. Beyond the\nconsidered problem, this work is an early contribution to domain knowledge\nenabled ML, which exploit the domain expertise of communication systems to\ndesign better approaches than general ML methods.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528RISnet\u795e\u7ecf\u7f51\u7edc\u4f18\u5316NOMA\u7cfb\u7edf\u4e2d\u7684\u57fa\u7ad9\u9884\u7f16\u7801\u548cRIS\u914d\u7f6e\u7684\u65b9\u6cd5\uff0c\u76f8\u6bd4\u73b0\u6709\u6280\u672f\uff0c\u8be5\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001CSI\u9700\u6c42\u548c\u4e92\u8026\u5904\u7406\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5e76\u4e14\u662f\u9886\u57df\u77e5\u8bc6\u9a71\u52a8\u673a\u5668\u5b66\u4e60\u7684\u65e9\u671f\u8d21\u732e\u3002", "motivation": "\u4e3a\u4e86\u63d0\u5347\u975e\u6b63\u4ea4\u591a\u5740\uff08NOMA\uff09\u6280\u672f\u7684\u6027\u80fd\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u5229\u7528\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08RIS\uff09\u6765\u6539\u5584\u65e0\u7ebf\u4fe1\u9053\u7279\u6027\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u540d\u4e3aRISnet\u7684\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u7ed3\u5408\u901a\u4fe1\u9886\u57df\u7684\u9886\u57df\u77e5\u8bc6\uff0c\u5b9e\u73b0\u4e86\u5bf9\u57fa\u7ad9\u9884\u7f16\u7801\u548cRIS\u914d\u7f6e\u7684\u8054\u5408\u4f18\u5316\u3002", "result": "\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u6700\u4f18\u57fa\u7ad9\u9884\u7f16\u7801\u548c\u673a\u5668\u5b66\u4e60\u9a71\u52a8\u7684RIS\uff0c\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\uff08\u53ef\u63a7\u5236\u8d85\u8fc71000\u4e2aRIS\u5355\u5143\uff09\uff0c\u5bf9\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u7684\u8981\u6c42\u4f4e\uff0c\u5e76\u80fd\u5904\u7406RIS\u5355\u5143\u95f4\u7684\u4e92\u8026\u95ee\u9898\u3002", "conclusion": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u57fa\u7ad9\u9884\u7f16\u7801\u548cRIS\u914d\u7f6e\u7684\u8054\u5408\u4f18\u5316\u65b9\u6cd5\uff0c\u5e76\u5229\u7528\u65e0\u76d1\u7763\u673a\u5668\u5b66\u4e60\u5b9e\u73b0\u81ea\u4e3b\u6700\u4f18\u89e3\u7684\u641c\u7d22\u3002"}}
{"id": "2508.06960", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06960", "abs": "https://arxiv.org/abs/2508.06960", "authors": ["Keyu Li", "Mohan Jiang", "Dayuan Fu", "Yunze Wu", "Xiangkun Hu", "Dequan Wang", "Pengfei Liu"], "title": "DatasetResearch: Benchmarking Agent Systems for Demand-Driven Dataset Discovery", "comment": null, "summary": "The rapid advancement of large language models has fundamentally shifted the\nbottleneck in AI development from computational power to data availability-with\ncountless valuable datasets remaining hidden across specialized repositories,\nresearch appendices, and domain platforms. As reasoning capabilities and deep\nresearch methodologies continue to evolve, a critical question emerges: can AI\nagents transcend conventional search to systematically discover any dataset\nthat meets specific user requirements, enabling truly autonomous demand-driven\ndata curation? We introduce DatasetResearch, the first comprehensive benchmark\nevaluating AI agents' ability to discover and synthesize datasets from 208\nreal-world demands across knowledge-intensive and reasoning-intensive tasks.\nOur tri-dimensional evaluation framework reveals a stark reality: even advanced\ndeep research systems achieve only 22% score on our challenging\nDatasetResearch-pro subset, exposing the vast gap between current capabilities\nand perfect dataset discovery. Our analysis uncovers a fundamental\ndichotomy-search agents excel at knowledge tasks through retrieval breadth,\nwhile synthesis agents dominate reasoning challenges via structured\ngeneration-yet both catastrophically fail on \"corner cases\" outside existing\ndistributions. These findings establish the first rigorous baseline for dataset\ndiscovery agents and illuminate the path toward AI systems capable of finding\nany dataset in the digital universe. Our benchmark and comprehensive analysis\nprovide the foundation for the next generation of self-improving AI systems and\nare publicly available at https://github.com/GAIR-NLP/DatasetResearch.", "AI": {"tldr": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u5c06\u74f6\u9888\u4ece\u8ba1\u7b97\u80fd\u529b\u8f6c\u79fb\u5230\u4e86\u6570\u636e\u53ef\u7528\u6027\u4e0a\u3002\u6211\u4eec\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6 DatasetResearch \u6765\u8bc4\u4f30 AI \u4ee3\u7406\u53d1\u73b0\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u73b0\u6709\u7cfb\u7edf\u5728\u9762\u5bf9\u201c\u8fb9\u7f18\u60c5\u51b5\u201d\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u968f\u7740\u63a8\u7406\u80fd\u529b\u548c\u6df1\u5ea6\u7814\u7a76\u65b9\u6cd5\u7684\u4e0d\u65ad\u53d1\u5c55\uff0c\u4e00\u4e2a\u5173\u952e\u95ee\u9898\u51fa\u73b0\u4e86\uff1a\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u80fd\u5426\u8d85\u8d8a\u4f20\u7edf\u7684\u641c\u7d22\uff0c\u7cfb\u7edf\u5730\u53d1\u73b0\u6ee1\u8db3\u7279\u5b9a\u7528\u6237\u9700\u6c42\u7684\u4efb\u4f55\u6570\u636e\u96c6\uff0c\u4ece\u800c\u5b9e\u73b0\u771f\u6b63\u81ea\u4e3b\u7684\u9700\u6c42\u9a71\u52a8\u6570\u636e\u7b56\u5c55\uff1f", "method": "\u6211\u4eec\u5f15\u5165\u4e86 DatasetResearch\uff0c\u8fd9\u662f\u7b2c\u4e00\u4e2a\u8bc4\u4f30 AI \u4ee3\u7406\u4ece\u8de8\u8d8a\u77e5\u8bc6\u5bc6\u96c6\u578b\u548c\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u7684 208 \u4e2a\u73b0\u5b9e\u4e16\u754c\u9700\u6c42\u4e2d\u53d1\u73b0\u548c\u5408\u6210\u6570\u636e\u96c6\u7684\u80fd\u529b\u7684\u7efc\u5408\u57fa\u51c6\u3002\u6211\u4eec\u7684\u4e09\u7ef4\u8bc4\u4f30\u6846\u67b6\u63ed\u793a\u4e86\u4e00\u4e2a\u4e25\u5cfb\u7684\u73b0\u5b9e\uff1a\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6df1\u5ea6\u7814\u7a76\u7cfb\u7edf\u5728\u6211\u4eec\u5177\u6709\u6311\u6218\u6027\u7684 DatasetResearch-pro \u5b50\u96c6\u4e0a\u4e5f\u53ea\u80fd\u83b7\u5f97 22% \u7684\u5206\u6570\uff0c\u8fd9\u66b4\u9732\u4e86\u5f53\u524d\u80fd\u529b\u4e0e\u5b8c\u7f8e\u6570\u636e\u96c6\u53d1\u73b0\u4e4b\u95f4\u7684\u5de8\u5927\u5dee\u8ddd\u3002", "result": "\u6211\u4eec\u7684\u5206\u6790\u63ed\u793a\u4e86\u4e00\u4e2a\u6839\u672c\u6027\u7684\u4e8c\u5206\u6cd5\u2014\u2014\u641c\u7d22\u4ee3\u7406\u901a\u8fc7\u68c0\u7d22\u5e7f\u5ea6\u5728\u77e5\u8bc6\u4efb\u52a1\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u800c\u7efc\u5408\u4ee3\u7406\u901a\u8fc7\u7ed3\u6784\u5316\u751f\u6210\u5728\u63a8\u7406\u6311\u6218\u65b9\u9762\u5360\u4e3b\u5bfc\u5730\u4f4d\u2014\u2014\u4f46\u4e24\u8005\u5728\u73b0\u6709\u5206\u5e03\u4e4b\u5916\u7684\u201c\u8fb9\u7f18\u60c5\u51b5\u201d\u4e0b\u90fd\u4f1a\u707e\u96be\u6027\u5730\u5931\u8d25\u3002", "conclusion": "\u8be5\u57fa\u51c6\u548c\u5206\u6790\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u5b66\u4eba\u5de5\u667a\u80fd\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u5e76\u9610\u660e\u4e86\u80fd\u591f\u53d1\u73b0\u6570\u5b57\u5b87\u5b99\u4e2d\u4efb\u4f55\u6570\u636e\u96c6\u7684 AI \u7cfb\u7edf\u7684\u53d1\u5c55\u65b9\u5411\u3002"}}
{"id": "2508.06765", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06765", "abs": "https://arxiv.org/abs/2508.06765", "authors": ["Xingke Yang", "Liang Li", "Sicong Li", "Liwei Guan", "Hao Wang", "Xiaoqi Qi", "Jiang Liu", "Xin Fu", "Miao Pan"], "title": "Fed MobiLLM: Efficient Federated LLM Fine-Tuning over Heterogeneous Mobile Devices via Server Assisted Side-Tuning", "comment": null, "summary": "Collaboratively fine-tuning (FT) large language models (LLMs) over\nheterogeneous mobile devices fosters immense potential applications of\npersonalized intelligence. However, such a vision faces critical system\nchallenges. Conventional federated LLM FT approaches place prohibitive\ncomputational and memory burdens on mobile hardware, and their synchronous\nmodel aggregation protocols stall for slower devices. In this paper, we propose\nFed MobiLLM, a novel design to facilitate efficient federated LLM FT across\nmobile devices with diverse computing/communication speeds and local model\narchitectures. In particular, Fed MobiLLM implements a pioneering\nserver-assisted federated side-tuning paradigm. Briefly, mobile devices perform\nlightweight forward propagation computations on local data using their frozen\npre-scaled backbone LLMs, and then upload selected intermediate activations.\nThe server trains a shared side-network independently, eliminating client-side\nbackpropagation and enabling asynchronous updates. To bridge model\nheterogeneity across different devices, we introduce an adaptive layer-wise\nfeature alignment method, which ensures consistent representations for\ncollaboratively tuning a shared side network. Extensive experimental results\ndemonstrate that Fed MobiLLM can maintain robust fine-tuning performance while\nachieving extremely low on-device memory, with at least 95.2% reduction in\ncomputation overhead, 93.2% reduction in communication costs and 5.1x faster\nconvergence compared to existing methods, validating its efficacy for practical\nLLM adaptation over heterogeneous mobile devices.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07351", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07351", "abs": "https://arxiv.org/abs/2508.07351", "authors": ["Xiaochun Huang", "Lingxiao Zhao", "Rui Xiong", "Wenbin Li", "Bao-tian Wang", "Baisheng Sa", "Matthias Bode"], "title": "Experimental Realization of the Topologically Nontrivial Phase in Monolayer Si$_2$Te$_2$", "comment": "6 pages, 5 figures", "summary": "The free-standing monolayer Si$_2$Te$_2$ (ML-Si$_2$Te$_2$) has been\ntheoretically predicted to host a room-temperature quantum spin Hall phase.\nHowever, its experimental realization remains challenge due to the absence of a\nthree-dimensional counterpart. Here, we demonstrate that HfTe$_2$ serves as an\nideal substrate for the epitaxial growth of ML-Si$_2$Te$_2$, preserving its\ntopological phase. Scanning tunneling microscopy and spectroscopy confirm a\nstrain-free ${(1 \\times 1)}$ lattice of ML-Si$_2$Te$_2$, along with a sizable\nband gap, which is well captured by first-principles calculations. Moreover,\ndistinct edge states, independent of step geometry and exhibiting a broad\nspatial distribution, are observed at ML-Si$_2$Te$_2$ step edges, underscoring\nits topological nature.", "AI": {"tldr": "\u7814\u7a76\u9996\u6b21\u5728HfTe$_2$\u886c\u5e95\u4e0a\u901a\u8fc7\u5916\u5ef6\u751f\u957f\u5b9e\u73b0\u4e86ML-Si$_2$Te$_2$\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u4f5c\u4e3a\u5ba4\u6e29\u91cf\u5b50\u81ea\u65cb\u970d\u5c14\u6750\u6599\u7684\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3ML-Si$_2$Te$_2$\u7f3a\u4e4f\u4e09\u7ef4\u5bf9\u5e94\u7269\u800c\u96be\u4ee5\u5b9e\u73b0\u7684\u95ee\u9898\uff0c\u5e76\u5b9e\u9a8c\u9a8c\u8bc1\u5176\u91cf\u5b50\u81ea\u65cb\u970d\u5c14\u6027\u8d28\u3002", "method": "\u5229\u7528\u626b\u63cf\u96a7\u9053\u663e\u5fae\u955c\u548c\u5149\u8c31\u5b66\u6280\u672f\uff0c\u7ed3\u5408\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u8bc1\u5b9e\u4e86ML-Si$_2$Te$_2$\u7684\u65e0\u5e94\u53d8$(1 \times 1)$\u6676\u683c\u548c\u663e\u8457\u7684\u5e26\u9699\uff0c\u5e76\u89c2\u5bdf\u5230\u4e86\u4e0e\u53f0\u9636\u51e0\u4f55\u65e0\u5173\u4e14\u7a7a\u95f4\u5206\u5e03\u5e7f\u6cdb\u7684\u8fb9\u7f18\u6001\u3002", "result": "\u6210\u529f\u5728HfTe$_2$\u886c\u5e95\u4e0a\u5916\u5ef6\u751f\u957f\u4e86\u65e0\u5e94\u53d8\u7684ML-Si$_2$Te$_2$\u8584\u819c\uff0c\u5176\u5e26\u9699\u548c\u62d3\u6251\u8fb9\u7f18\u6001\u5f97\u5230\u4e86\u5b9e\u9a8c\u8bc1\u5b9e\u3002", "conclusion": "\u901a\u8fc7HfTe$_2$\u4f5c\u4e3a\u886c\u5e95\uff0c\u6210\u529f\u5b9e\u73b0\u4e86ML-Si$_2$Te$_2$\u5916\u5ef6\u751f\u957f\uff0c\u5e76\u4fdd\u6301\u4e86\u5176\u62d3\u6251\u76f8\u3002"}}
{"id": "2508.07118", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07118", "abs": "https://arxiv.org/abs/2508.07118", "authors": ["Aiden Swann", "Alex Qiu", "Matthew Strong", "Angelina Zhang", "Samuel Morstein", "Kai Rayle", "Monroe Kennedy III"], "title": "DexFruit: Dexterous Manipulation and Gaussian Splatting Inspection of Fruit", "comment": "8 pages, 5 figures", "summary": "DexFruit is a robotic manipulation framework that enables gentle, autonomous\nhandling of fragile fruit and precise evaluation of damage. Many fruits are\nfragile and prone to bruising, thus requiring humans to manually harvest them\nwith care. In this work, we demonstrate by using optical tactile sensing,\nautonomous manipulation of fruit with minimal damage can be achieved. We show\nthat our tactile informed diffusion policies outperform baselines in both\nreduced bruising and pick-and-place success rate across three fruits:\nstrawberries, tomatoes, and blackberries. In addition, we introduce FruitSplat,\na novel technique to represent and quantify visual damage in high-resolution 3D\nrepresentation via 3D Gaussian Splatting (3DGS). Existing metrics for measuring\ndamage lack quantitative rigor or require expensive equipment. With FruitSplat,\nwe distill a 2D strawberry mask as well as a 2D bruise segmentation mask into\nthe 3DGS representation. Furthermore, this representation is modular and\ngeneral, compatible with any relevant 2D model. Overall, we demonstrate a 92%\ngrasping policy success rate, up to a 20% reduction in visual bruising, and up\nto an 31% improvement in grasp success rate on challenging fruit compared to\nour baselines across our three tested fruits. We rigorously evaluate this\nresult with over 630 trials. Please checkout our website at\nhttps://dex-fruit.github.io .", "AI": {"tldr": "DexFruit\u901a\u8fc7\u5149\u5b66\u89e6\u89c9\u548c3D\u9ad8\u65af\u653e\u5c04\u72b6\u635f\u4f24\u91cf\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6613\u635f\u6c34\u679c\u7684\u8f7b\u67d4\u6293\u53d6\uff0c\u51cf\u5c11\u4e8620%\u7684\u635f\u4f24\uff0c\u63d0\u9ad8\u4e8631%\u7684\u6293\u53d6\u6210\u529f\u7387\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4eba\u5de5\u91c7\u6458\u6c34\u679c\u65f6\u6613\u635f\u6c34\u679c\u5bb9\u6613\u78b0\u4f24\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709\u635f\u4f24\u8bc4\u4f30\u6307\u6807\u7684\u91cf\u5316\u4e0d\u8db3\u6216\u8bbe\u5907\u6602\u8d35\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u673a\u5668\u4eba\u81ea\u4e3b\u64cd\u63a7\u548c\u5148\u8fdb\u7684\u635f\u4f24\u91cf\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u5bf9\u6613\u635f\u6c34\u679c\u7684\u8f7b\u67d4\u5904\u7406\u5e76\u7cbe\u786e\u8bc4\u4f30\u635f\u4f24\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDexFruit\u7684\u673a\u5668\u4eba\u64cd\u63a7\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u548c\u4e00\u79cd\u540d\u4e3aFruitSplat\u7684\u65b0\u578b\u635f\u4f24\u91cf\u5316\u6280\u672f\uff08\u57fa\u4e8e3D\u9ad8\u65af\u653e\u5c04\u72b6\uff09\u3002DexFruit\u6846\u67b6\u80fd\u591f\u5b9e\u73b0\u6c34\u679c\u7684\u81ea\u4e3b\u6293\u53d6\u548c\u5904\u7406\uff0c\u800cFruitSplat\u6280\u672f\u5219\u80fd\u4ee5\u9ad8\u5206\u8fa8\u73873D\u5f62\u5f0f\u91cf\u5316\u89c6\u89c9\u635f\u4f24\u3002", "result": "DexFruit\u6846\u67b6\u5728\u4e09\u79cd\u6c34\u679c\uff08\u8349\u8393\u3001\u756a\u8304\u3001\u9ed1\u8393\uff09\u4e0a\u7684\u6293\u53d6\u7b56\u7565\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u5728\u51cf\u5c11\u64e6\u4f24\u548c\u63d0\u9ad8\u6293\u53d6\u6210\u529f\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6293\u53d6\u7b56\u7565\u6210\u529f\u7387\u8fbe\u523092%\uff0c\u89c6\u89c9\u64e6\u4f24\u51cf\u5c11\u9ad8\u8fbe20%\uff0c\u5728\u6311\u6218\u6027\u6c34\u679c\u4e0a\u7684\u6293\u53d6\u6210\u529f\u7387\u6bd4\u57fa\u7ebf\u63d0\u9ad8\u4e8631%\uff0c\u8be5\u7ed3\u679c\u7ecf\u8fc7\u4e86\u8d85\u8fc7630\u6b21\u8bd5\u9a8c\u7684\u4e25\u683c\u8bc4\u4f30\u3002", "conclusion": "DexFruit\u6846\u67b6\u901a\u8fc7\u4f7f\u7528\u5149\u5b66\u89e6\u89c9\u4f20\u611f\u548c\u57fa\u4e8e3D\u9ad8\u65af\u653e\u5c04\u72b6\u7684\u635f\u4f24\u91cf\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u6613\u635f\u6c34\u679c\uff08\u8349\u8393\u3001\u756a\u8304\u548c\u9ed1\u8393\uff09\u7684\u8f7b\u67d4\u81ea\u4e3b\u6293\u53d6\u548c\u635f\u4f24\u8bc4\u4f30\uff0c\u663e\u8457\u51cf\u5c11\u4e86\u635f\u4f24\u5e76\u63d0\u9ad8\u4e86\u6293\u53d6\u6210\u529f\u7387\u3002"}}
{"id": "2508.07247", "categories": ["quant-ph", "gr-qc"], "pdf": "https://arxiv.org/pdf/2508.07247", "abs": "https://arxiv.org/abs/2508.07247", "authors": ["Maciej T. Jarema", "Cameron R. D. Bunney", "Vitor S. Barroso", "Mohammadamin Tajik", "Chris Goodwin", "Silke Weinfurtner"], "title": "Information in quantum field theory simulators: Thin-film superfluid helium", "comment": "14 pages, 6 figures", "summary": "Understanding quantum correlations through information-theoretic measures is\nfundamental to developments in quantum field theory, quantum information, and\nquantum many-body physics. A central feature in a plethora of systems is the\narea law, under which information scales with the size of the boundary of the\nsystem, rather than volume. Whilst many systems and regimes exhibiting an area\nlaw have been identified theoretically, experimental verification remains\nlimited, particularly in continuous systems. We present a methodology for\nmeasuring mutual information in an experimental simulator of non-interacting\nquantum fields, and propose using the analogue $(2 + 1)$-dimensional spacetime\noffered by thin films of superfluid helium. We provide numerical predictions\nincorporating the natural thermal state of the helium sample that exemplify an\narea-law scaling of mutual information, and characterise deviations\nattributable to the inherent finite system size.", "AI": {"tldr": "\u5728\u8d85\u6d41\u6c26\u8584\u819c\u4e2d\u6d4b\u91cf\u4e92\u4fe1\u606f\uff0c\u4ee5\u9a8c\u8bc1\u91cf\u5b50\u5173\u8054\u7684\u9762\u79ef\u5b9a\u5219\u7f29\u653e\u3002", "motivation": "\u7406\u89e3\u91cf\u5b50\u5173\u8054\u5bf9\u4e8e\u91cf\u5b50\u573a\u8bba\u3001\u91cf\u5b50\u4fe1\u606f\u548c\u91cf\u5b50\u591a\u4f53\u7269\u7406\u5b66\u7684\u53d1\u5c55\u81f3\u5173\u91cd\u8981\uff0c\u800c\u9762\u79ef\u5b9a\u5219\u662f\u8bb8\u591a\u7cfb\u7edf\u7684\u4e00\u4e2a\u4e2d\u5fc3\u7279\u5f81\uff0c\u4f46\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5c24\u5176\u662f\u5728\u8fde\u7eed\u7cfb\u7edf\u4e2d\uff0c\u4ecd\u7136\u6709\u9650\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u8d85\u6d41\u6c26\u8584\u819c\uff08\u6a21\u62df (2 + 1) \u7ef4\u5ea6\u65f6\u7a7a\uff09\u5b9e\u9a8c\u6a21\u62df\u5668\u4e2d\u6d4b\u91cf\u4e92\u4fe1\u606f\u7684\u65b9\u6cd5\u3002", "result": "\u9884\u6d4b\u4e86\u8003\u8651\u6c26\u6837\u54c1\u81ea\u7136\u70ed\u6001\u7684\u6570\u503c\u7ed3\u679c\uff0c\u8be5\u7ed3\u679c\u4f8b\u8bc1\u4e86\u4e92\u4fe1\u606f\u7684\u9762\u79ef\u5b9a\u5219\u7f29\u653e\uff0c\u5e76\u63cf\u8ff0\u4e86\u7531\u4e8e\u6709\u9650\u7cfb\u7edf\u5c3a\u5bf8\u5f15\u8d77\u7684\u504f\u5dee\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u5728\u8d85\u6d41\u6c26\u8584\u819c\u5b9e\u9a8c\u6a21\u62df\u5668\u4e2d\u6d4b\u91cf\u4e92\u4fe1\u606f\u7684\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u4e92\u4fe1\u606f\u5982\u4f55\u4f53\u73b0\u9762\u79ef\u5b9a\u5219\u7f29\u653e\uff0c\u4ee5\u53ca\u6709\u9650\u7cfb\u7edf\u5c3a\u5bf8\u5bf9\u7ed3\u679c\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.06974", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.06974", "abs": "https://arxiv.org/abs/2508.06974", "authors": ["Zhijun Tu", "Hanting Chen", "Siqi Liu", "Chuanjian Liu", "Jian Li", "Jie Hu", "Yunhe Wang"], "title": "Rethinking 1-bit Optimization Leveraging Pre-trained Large Language Models", "comment": "16 pages, 5 figures", "summary": "1-bit LLM quantization offers significant advantages in reducing storage and\ncomputational costs. However, existing methods typically train 1-bit LLMs from\nscratch, failing to fully leverage pre-trained models. This results in high\ntraining costs and notable accuracy degradation. We identify that the large gap\nbetween full precision and 1-bit representations makes direct adaptation\ndifficult. In this paper, we introduce a consistent progressive training for\nboth forward and backward, smoothly converting the floating-point weights into\nthe binarized ones. Additionally, we incorporate binary-aware initialization\nand dual-scaling compensation to reduce the difficulty of progressive training\nand improve the performance. Experimental results on LLMs of various sizes\ndemonstrate that our method outperforms existing approaches. Our results show\nthat high-performance 1-bit LLMs can be achieved using pre-trained models,\neliminating the need for expensive training from scratch.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u5e73\u6ed1\u5730\u5c06\u6d6e\u70b9\u6743\u91cd\u8f6c\u6362\u4e3a\u4e8c\u503c\u5316\u6743\u91cd\uff0c\u5e76\u7ed3\u5408\u4e8c\u503c\u611f\u77e5\u521d\u59cb\u5316\u548c\u53cc\u5c3a\u5ea6\u8865\u507f\uff0c\u4ee5\u8f83\u4f4e\u7684\u6210\u672c\u548c\u8f83\u5c0f\u7684\u7cbe\u5ea6\u635f\u5931\uff0c\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u57fa\u7840\u4e0a\u5b9e\u73b0\u9ad8\u6027\u80fd\u76841\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u76841\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u91cf\u5316\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\uff0c\u65e0\u6cd5\u5145\u5206\u5229\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u7cbe\u5ea6\u4e0b\u964d\u660e\u663e\u3002\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5168\u7cbe\u5ea6\u548c1\u6bd4\u7279\u8868\u793a\u4e4b\u95f4\u7684\u5927\u5dee\u8ddd\uff0c\u4f7f\u5f97\u76f4\u63a5\u9002\u914d\u66f4\u52a0\u56f0\u96be\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u7528\u4e8e\u524d\u5411\u548c\u540e\u5411\u4f20\u64ad\uff0c\u4ee5\u5e73\u6ed1\u5730\u5c06\u6d6e\u70b9\u6743\u91cd\u8f6c\u6362\u4e3a\u4e8c\u503c\u5316\u6743\u91cd\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86\u4e8c\u503c\u611f\u77e5\u521d\u59cb\u5316\u548c\u53cc\u5c3a\u5ea6\u8865\u507f\u6765\u964d\u4f4e\u6e10\u8fdb\u5f0f\u8bad\u7ec3\u7684\u96be\u5ea6\u5e76\u63d0\u9ad8\u6027\u80fd\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u5c3a\u5bf8\u7684\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u80fd\u591f\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\u5b9e\u73b0\u9ad8\u6027\u80fd\u76841\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6027\u80fd\u76841\u6bd4\u7279\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u65e0\u9700\u4ece\u5934\u5f00\u59cb\u8fdb\u884c\u6602\u8d35\u7684\u8bad\u7ec3\u3002"}}
{"id": "2508.06624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06624", "abs": "https://arxiv.org/abs/2508.06624", "authors": ["Kexin Yu", "Zihan Xu", "Jialei Xie", "Carter Adams"], "title": "VL-MedGuide: A Visual-Linguistic Large Model for Intelligent and Explainable Skin Disease Auxiliary Diagnosis", "comment": null, "summary": "Accurate diagnosis of skin diseases remains a significant challenge due to\nthe complex and diverse visual features present in dermatoscopic images, often\ncompounded by a lack of interpretability in existing purely visual diagnostic\nmodels. To address these limitations, this study introduces VL-MedGuide\n(Visual-Linguistic Medical Guide), a novel framework leveraging the powerful\nmulti-modal understanding and reasoning capabilities of Visual-Language Large\nModels (LVLMs) for intelligent and inherently interpretable auxiliary diagnosis\nof skin conditions. VL-MedGuide operates in two interconnected stages: a\nMulti-modal Concept Perception Module, which identifies and linguistically\ndescribes dermatologically relevant visual features through sophisticated\nprompt engineering, and an Explainable Disease Reasoning Module, which\nintegrates these concepts with raw visual information via Chain-of-Thought\nprompting to provide precise disease diagnoses alongside transparent\nrationales. Comprehensive experiments on the Derm7pt dataset demonstrate that\nVL-MedGuide achieves state-of-the-art performance in both disease diagnosis\n(83.55% BACC, 80.12% F1) and concept detection (76.10% BACC, 67.45% F1),\nsurpassing existing baselines. Furthermore, human evaluations confirm the high\nclarity, completeness, and trustworthiness of its generated explanations,\nbridging the gap between AI performance and clinical utility by offering\nactionable, explainable insights for dermatological practice.", "AI": {"tldr": "VL-MedGuide\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u63a8\u7406\uff0c\u5b9e\u73b0\u667a\u80fd\u4e14\u900f\u660e\u7684\u76ae\u80a4\u75c5\u8f85\u52a9\u8bca\u65ad\uff0c\u6027\u80fd\u4f18\u8d8a\u4e14\u89e3\u91ca\u503c\u5f97\u4fe1\u8d56\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u76ae\u80a4\u75c5\u8bca\u65ad\u4e2d\u89c6\u89c9\u7279\u5f81\u590d\u6742\u591a\u6837\u4ee5\u53ca\u73b0\u6709\u7eaf\u89c6\u89c9\u6a21\u578b\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u7684\u6311\u6218\uff0c\u63d0\u51faVL-MedGuide\u6846\u67b6\u3002", "method": "VL-MedGuide\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a1.\u591a\u6a21\u6001\u6982\u5ff5\u611f\u77e5\u6a21\u5757\uff0c\u901a\u8fc7\u7cbe\u5bc6\u7684\u63d0\u793a\u5de5\u7a0b\u8bc6\u522b\u5e76\u63cf\u8ff0\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u7684\u76f8\u5173\u89c6\u89c9\u7279\u5f81\uff1b2.\u53ef\u89e3\u91ca\u75be\u75c5\u63a8\u7406\u6a21\u5757\uff0c\u5229\u7528\u601d\u7ef4\u94fe\u63d0\u793a\u5c06\u8fd9\u4e9b\u6982\u5ff5\u4e0e\u539f\u59cb\u89c6\u89c9\u4fe1\u606f\u7ed3\u5408\uff0c\u63d0\u4f9b\u7cbe\u786e\u7684\u75be\u75c5\u8bca\u65ad\u548c\u900f\u660e\u7684\u7406\u7531\u3002", "result": "VL-MedGuide\u5728Derm7pt\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u75be\u75c5\u8bca\u65ad\u51c6\u786e\u7387\uff08BACC 83.55%\uff0cF1 80.12%\uff09\u548c\u6982\u5ff5\u68c0\u6d4b\u51c6\u786e\u7387\uff08BACC 76.10%\uff0cF1 67.45%\uff09\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u57fa\u7ebf\u3002\u4eba\u7c7b\u8bc4\u4f30\u4e5f\u8bc1\u5b9e\u4e86\u5176\u751f\u6210\u89e3\u91ca\u7684\u9ad8\u5ea6\u6e05\u6670\u6027\u3001\u5b8c\u6574\u6027\u548c\u53ef\u4fe1\u5ea6\u3002", "conclusion": "VL-MedGuide\u901a\u8fc7\u591a\u6a21\u6001\u6982\u5ff5\u611f\u77e5\u548c\u53ef\u89e3\u91ca\u75be\u75c5\u63a8\u7406\uff0c\u5728\u76ae\u80a4\u75c5\u8f85\u52a9\u8bca\u65ad\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u6e05\u6670\u3001\u5b8c\u6574\u548c\u53ef\u4fe1\u7684\u89e3\u91ca\uff0c\u5f25\u5408\u4e86AI\u6027\u80fd\u4e0e\u4e34\u5e8a\u6548\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.08032", "categories": ["cond-mat.mes-hall", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08032", "abs": "https://arxiv.org/abs/2508.08032", "authors": ["Fabian Hader", "Sarah Fleitmann", "Jan Vogelbruch", "Lotte Geck", "Stefan van Waasen"], "title": "Simulation of Charge Stability Diagrams for Automated Tuning Solutions (SimCATS)", "comment": null, "summary": "Quantum dots must be tuned precisely to provide a suitable basis for quantum\ncomputation. A scalable platform for quantum computing can only be achieved by\nfully automating the tuning process. One crucial step is to trap the\nappropriate number of electrons in the quantum dots, typically accomplished by\nanalyzing charge stability diagrams (CSDs). Training and testing automation\nalgorithms require large amounts of data, which can be either measured and\nmanually labeled in an experiment or simulated. This article introduces a new\napproach to the realistic simulation of such measurements. Our flexible\nframework enables the simulation of ideal CSD data complemented with\nappropriate sensor responses and distortions. We suggest using this simulation\nto benchmark published algorithms. Also, we encourage the extension by custom\nmodels and parameter sets to drive the development of robust,\ntechnology-independent algorithms. Code is available at\nhttps://github.com/f-hader/SimCATS.", "AI": {"tldr": "\u8be5\u6587\u7ae0\u4ecb\u7ecd\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u62df\u65b9\u6cd5\uff0c\u7528\u4e8e\u751f\u6210\u91cf\u5b50\u70b9\u91cf\u5b50\u8ba1\u7b97\u4e2d\u7535\u8377\u7a33\u5b9a\u6027\u56fe\uff08CSDs\uff09\u7684\u771f\u5b9e\u6570\u636e\uff0c\u4ee5\u89e3\u51b3\u81ea\u52a8\u5316\u8c03\u4f18\u7b97\u6cd5\u7684\u6570\u636e\u9700\u6c42\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\uff0c\u9700\u8981\u5b8c\u5168\u81ea\u52a8\u5316\u91cf\u5b50\u70b9\u7684\u8c03\u4f18\u8fc7\u7a0b\uff0c\u5176\u4e2d\u5173\u952e\u4e00\u6b65\u662f\u6355\u83b7\u9002\u91cf\u7684\u7535\u5b50\uff0c\u8fd9\u901a\u5e38\u901a\u8fc7\u5206\u6790\u7535\u8377\u7a33\u5b9a\u6027\u56fe\uff08CSDs\uff09\u6765\u5b8c\u6210\u3002", "method": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u6a21\u62df\u7535\u8377\u7a33\u5b9a\u6027\u56fe\uff08CSDs\uff09\u53ca\u5176\u4f20\u611f\u5668\u54cd\u5e94\u548c\u5931\u771f\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u53ef\u4ee5\u6a21\u62df\u7406\u60f3\u7684CSD\u6570\u636e\u4ee5\u53ca\u9002\u5f53\u7684\u4f20\u611f\u5668\u54cd\u5e94\u548c\u5931\u771f\u3002", "conclusion": "\u901a\u8fc7\u4eff\u771f\u5bf9\u7b97\u6cd5\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u9f13\u52b1\u4f7f\u7528\u81ea\u5b9a\u4e49\u6a21\u578b\u548c\u53c2\u6570\u96c6\u6765\u63a8\u52a8\u72ec\u7acb\u4e8e\u6280\u672f\u7684\u9c81\u68d2\u7b97\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07967", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07967", "abs": "https://arxiv.org/abs/2508.07967", "authors": ["Haijia Jin", "Weijie Yuan", "Jun Wu", "Jiacheng Wang", "Dusit Niyato", "Xianbin Wang", "George K. Karagiannidis", "Zhiyun Lin", "Yi Gong", "Dong In Kim", "Athina Petropulu", "Maria Sabrina Greco", "Abbas Jamalipour", "Sumei Sun"], "title": "Advancing the Control of Low-Altitude Wireless Networks: Architecture, Design Principles, and Future Directions", "comment": null, "summary": "This article introduces a control-oriented low-altitude wireless network\n(LAWN) that integrates near-ground communications and remote estimation of the\ninternal system state. This integration supports reliable networked control in\ndynamic aerial-ground environments. First, we introduce the network's modular\narchitecture and key performance metrics. Then, we discuss core design\ntrade-offs across the control, communication, and estimation layers. A case\nstudy illustrates closed-loop coordination under wireless constraints. Finally,\nwe outline future directions for scalable, resilient LAWN deployments in\nreal-time and resource-constrained scenarios.", "AI": {"tldr": "Introduces a low-altitude wireless network (LAWN) for aerial-ground environments, focusing on control, communication, and estimation, with a case study and future directions.", "motivation": "The motivation is to support reliable networked control in dynamic aerial-ground environments by integrating near-ground communications and remote estimation of the internal system state.", "method": "The paper introduces the network's modular architecture and key performance metrics, discusses core design trade-offs across control, communication, and estimation layers, and includes a case study illustrating closed-loop coordination under wireless constraints.", "result": "The paper presents a control-oriented low-altitude wireless network (LAWN) and illustrates its closed-loop coordination under wireless constraints through a case study.", "conclusion": "This article introduces a control-oriented low-altitude wireless network (LAWN) that integrates near-ground communications and remote estimation of the internal system state, supporting reliable networked control in dynamic aerial-ground environments. It also discusses core design trade-offs and illustrates closed-loop coordination under wireless constraints, outlining future directions for scalable, resilient LAWN deployments."}}
{"id": "2508.06963", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06963", "abs": "https://arxiv.org/abs/2508.06963", "authors": ["Changqing Li", "Tianlin Li", "Xiaohan Zhang", "Aishan Liu", "Li Pan"], "title": "MASteer: Multi-Agent Adaptive Steer Strategy for End-to-End LLM Trustworthiness Repair", "comment": null, "summary": "Large Language Models (LLMs) face persistent and evolving trustworthiness\nissues, motivating developers to seek automated and flexible repair methods\nthat enable convenient deployment across diverse scenarios. Existing repair\nmethods like supervised fine-tuning (SFT) and reinforcement learning with human\nfeedback (RLHF) are costly and slow, while prompt engineering lacks robustness\nand scalability. Representation engineering, which steers model behavior by\ninjecting targeted concept vectors during inference, offers a lightweight,\ntraining-free alternative. However, current approaches depend on manually\ncrafted samples and fixed steering strategies, limiting automation and\nadaptability. To overcome these challenges, we propose MASteer, the first\nend-to-end framework for trustworthiness repair in LLMs based on representation\nengineering. MASteer integrates two core components: AutoTester, a multi-agent\nsystem that generates diverse, high-quality steer samples tailored to developer\nneeds; and AutoRepairer, which constructs adaptive steering strategies with\nanchor vectors for automated, context-aware strategy selection during\ninference. Experiments on standard and customized trustworthiness tasks show\nMASteer consistently outperforms baselines, improving metrics by 15.36% on\nLLaMA-3.1-8B-Chat and 4.21% on Qwen-3-8B-Chat, while maintaining general model\ncapabilities. MASteer demonstrates strong robustness, generalization, and\npractical value for scalable, efficient trustworthiness repair.", "AI": {"tldr": "MASteer\u901a\u8fc7\u5176\u521b\u65b0\u7684AutoTester\u548cAutoRepairer\u7ec4\u4ef6\uff0c\u5b9e\u73b0\u4e86LLM\u4fe1\u4efb\u5ea6\u4fee\u590d\u7684\u81ea\u52a8\u5316\u548c\u9002\u5e94\u6027\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u4fe1\u4efb\u5ea6\u65b9\u9762\u5b58\u5728\u6301\u7eed\u5b58\u5728\u4e14\u4e0d\u65ad\u6f14\u8fdb\u7684\u95ee\u9898\uff0c\u4fc3\u4f7f\u5f00\u53d1\u8005\u5bfb\u6c42\u81ea\u52a8\u5316\u548c\u7075\u6d3b\u7684\u4fee\u590d\u65b9\u6cd5\uff0c\u4ee5\u4fbf\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u65b9\u4fbf\u90e8\u7f72\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u65b9\u6cd5\uff08\u5982SFT\u548cRLHF\uff09\u6210\u672c\u9ad8\u6602\u4e14\u901f\u5ea6\u7f13\u6162\uff0c\u800c\u63d0\u793a\u5de5\u7a0b\u5219\u7f3a\u4e4f\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002\u8868\u793a\u5de5\u7a0b\u867d\u7136\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u9700\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u4f46\u5176\u624b\u52a8\u6837\u672c\u548c\u56fa\u5b9a\u7b56\u7565\u7684\u4f9d\u8d56\u9650\u5236\u4e86\u81ea\u52a8\u5316\u548c\u9002\u5e94\u6027\u3002", "method": "MASteer\u6846\u67b6\u96c6\u6210\u4e86AutoTester\uff08\u4e00\u4e2a\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u7528\u4e8e\u751f\u6210\u591a\u6837\u5316\u3001\u9ad8\u8d28\u91cf\u7684\u5f15\u5bfc\u6837\u672c\uff09\u548cAutoRepairer\uff08\u7528\u4e8e\u6784\u5efa\u5177\u6709\u951a\u5b9a\u5411\u91cf\u7684\u81ea\u9002\u5e94\u5f15\u5bfc\u7b56\u7565\uff09\uff0c\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u7b56\u7565\u9009\u62e9\u3002", "result": "MASteer\u5728\u6807\u51c6\u548c\u5b9a\u5236\u7684\u4fe1\u4efb\u5ea6\u4efb\u52a1\u4e0a\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5728LLaMA-3.1-8B-Chat\u4e0a\u63d0\u9ad8\u4e8615.36%\uff0c\u5728Qwen-3-8B-Chat\u4e0a\u63d0\u9ad8\u4e864.21%\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6a21\u578b\u7684\u901a\u7528\u80fd\u529b\uff0c\u5c55\u793a\u4e86\u5176\u5f3a\u5927\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u6269\u5c55\u3001\u9ad8\u6548\u7684\u4fe1\u4efb\u5ea6\u4fee\u590d\u7684\u5b9e\u7528\u4ef7\u503c\u3002", "conclusion": "MASteer\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u8868\u793a\u5de5\u7a0b\u6846\u67b6\uff0c\u7528\u4e8e\u4fee\u590d\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u4fe1\u4efb\u5ea6\u95ee\u9898\uff0c\u901a\u8fc7AutoTester\u548cAutoRepairer\u7ec4\u4ef6\u5b9e\u73b0\u4e86\u81ea\u52a8\u5316\u548c\u9002\u5e94\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.07424", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07424", "abs": "https://arxiv.org/abs/2508.07424", "authors": ["Pedro Aranda", "Javier Segurado"], "title": "Effective toughness estimation by FFT based phase field fracture: application to composites and polycrystals", "comment": "accepted for publication in Materials Research Communications,\n  Special Issue in Honor of Ricardo Lebensohn", "summary": "An estimate of the effective toughness of heterogeneous materials is proposed\nbased on the Phase Field Fracture model implemented in an FFT homogenization\nsolver. The estimate is based on the simulation of the deformation of\nrepresentative volume elements of the microstructure, controlled by a constant\nenergy dissipation rate using an arc-length type control. The definition of the\ntoughness corresponds to the total energy dissipated after the total fracture\nof the RVE -- which can be accurately obtained thanks to the dissipation\ncontrol -- divided by the RVE transverse area (length in 2D). The proposed\nestimate accounts for both the effect of heterogeneity in toughness and elastic\nresponse on the overall fracture energy and allows as well to account for\nphases with anisotropic elastic and fracture response (fracture by cleavage).\nTo improve toughness predictions, crack-tip enrichment is used to model initial\ncracks. The method is applied to obtain the effective toughness of composites\nand elastic polycrystals in a series of examples. In the two types of\nmaterials, it is found that both heterogeneity in elastic response and fracture\nenergy contribute to increase the effective toughness. Microscopically, it is\nfound that toughening mechanisms are related to the passage of the crack\nthrough tougher phases and deviation of the crack path. It is also found that\nthe latter is the controlling mechanism for cases with marked heterogeneity and\nhigh anisotropy, eventually provoking toughening saturation for sufficiently\nhigh values of heterogeneity or anisotropy.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u76f8\u573a\u65ad\u88c2\u6a21\u578b\u548cFFT\u5747\u5300\u5316\u6c42\u89e3\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5f02\u8d28\u6750\u6599\u7684\u6709\u6548\u97e7\u6027\u3002\u8be5\u65b9\u6cd5\u8003\u8651\u4e86\u5fae\u89c2\u7ed3\u6784\u3001\u5f39\u6027\u4ee5\u53ca\u65ad\u88c2\u54cd\u5e94\u7684\u5f71\u54cd\uff0c\u5e76\u901a\u8fc7\u88c2\u7eb9\u5c16\u7aef\u5bcc\u96c6\u6765\u63d0\u9ad8\u9884\u6d4b\u7cbe\u5ea6\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f02\u8d28\u6027\u6709\u52a9\u4e8e\u63d0\u9ad8\u6750\u6599\u7684\u6709\u6548\u97e7\u6027\uff0c\u5e76\u4e14\u589e\u97e7\u673a\u5236\u4e0e\u88c2\u7eb9\u7684\u6269\u5c55\u8def\u5f84\u6709\u5173\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u76f8\u573a\u65ad\u88c2\u6a21\u578b\u548cFFT\u5747\u5300\u5316\u6c42\u89e3\u5668\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u4f30\u8ba1\u5f02\u8d28\u6750\u6599\u7684\u6709\u6548\u97e7\u6027\uff0c\u5e76\u8003\u8651\u5fae\u89c2\u7ed3\u6784\u3001\u5f39\u6027\u4ee5\u53ca\u65ad\u88c2\u54cd\u5e94\u7684\u5f71\u54cd\u3002", "method": "\u57fa\u4e8e\u76f8\u573a\u65ad\u88c2\u6a21\u578b\uff0c\u5728FFT\u5747\u5300\u5316\u6c42\u89e3\u5668\u4e2d\u5b9e\u73b0\uff0c\u901a\u8fc7\u6052\u5b9a\u7684\u80fd\u91cf\u8017\u6563\u7387\uff08\u4f7f\u7528\u5f27\u957f\u7c7b\u578b\u63a7\u5236\uff09\u6765\u6a21\u62df\u4ee3\u8868\u6027\u4f53\u79ef\u5355\u5143\u7684\u53d8\u5f62\uff0c\u4ee5\u4f30\u8ba1\u6709\u6548\u97e7\u6027\u3002", "result": "\u901a\u8fc7\u88c2\u7eb9\u5c16\u7aef\u5bcc\u96c6\u548cFFT\u5747\u5300\u5316\u6c42\u89e3\u5668\uff0c\u7814\u7a76\u4e86\u590d\u5408\u6750\u6599\u548c\u5f39\u6027\u591a\u6676\u4f53\u7684\u6709\u6548\u97e7\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f39\u6027\u54cd\u5e94\u548c\u65ad\u88c2\u80fd\u7684\u5f02\u8d28\u6027\u90fd\u4f1a\u589e\u52a0\u6709\u6548\u97e7\u6027\u3002\u589e\u97e7\u673a\u5236\u4e0e\u88c2\u7eb9\u7a7f\u8fc7\u97e7\u6027\u76f8\u548c\u88c2\u7eb9\u8def\u5f84\u504f\u8f6c\u6709\u5173\u3002\u5f53\u6742\u8d28\u6216\u5404\u5411\u5f02\u6027\u8db3\u591f\u9ad8\u65f6\uff0c\u88c2\u7eb9\u8def\u5f84\u504f\u8f6c\u662f\u63a7\u5236\u673a\u5236\uff0c\u53ef\u80fd\u5bfc\u81f4\u97e7\u6027\u9971\u548c\u3002", "conclusion": "\u5f02\u8d28\u6750\u6599\u7684\u6709\u6548\u97e7\u6027\u4f30\u8ba1\u8003\u8651\u4e86\u97e7\u6027\u548c\u5f39\u6027\u54cd\u5e94\u7684\u5f02\u8d28\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u8003\u8651\u5177\u6709\u5404\u5411\u5f02\u6027\u5f39\u6027 \u0924\u0938\u0947\u091a\u65ad\u88c2\u54cd\u5e94\uff08\u89e3\u7406\u65ad\u88c2\uff09\u7684\u76f8\u3002\u901a\u8fc7\u5c06\u88c2\u7eb9\u5c16\u7aef\u5bcc\u96c6\u7528\u4e8e\u6a21\u62df\u521d\u59cb\u88c2\u7eb9\uff0c\u53ef\u4ee5\u63d0\u9ad8\u97e7\u6027\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002\u8be5\u65b9\u6cd5\u5e94\u7528\u4e8e\u590d\u5408\u6750\u6599\u548c\u5f39\u6027\u591a\u6676\u4f53\uff0c\u53d1\u73b0\u97e7\u6027\u548c\u5f39\u6027\u54cd\u5e94\u7684\u5f02\u8d28\u6027\u90fd\u4f1a\u589e\u52a0\u6709\u6548\u97e7\u6027\u3002\u5fae\u89c2\u4e0a\uff0c\u589e\u97e7\u673a\u5236\u4e0e\u88c2\u7eb9\u7a7f\u8fc7\u66f4\u97e7\u76f8\u4ee5\u53ca\u88c2\u7eb9\u8def\u5f84\u7684\u504f\u8f6c\u6709\u5173\u3002\u5728\u6742\u8d28\u6216\u5404\u5411\u5f02\u6027\u8db3\u591f\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u88c2\u7eb9\u8def\u5f84\u7684\u504f\u8f6c\u662f\u63a7\u5236\u673a\u5236\uff0c\u6700\u7ec8\u4f1a\u5bfc\u81f4\u97e7\u6027\u9971\u548c\u3002"}}
{"id": "2508.07335", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07335", "abs": "https://arxiv.org/abs/2508.07335", "authors": ["Ad\u00e1n Cabello"], "title": "The simplest Kochen-Specker set", "comment": "3 pages, 1 figure", "summary": "Kochen-Specker (KS) sets are fundamental in physics. Every time nature\nproduces bipartite correlations attaining the nonsignaling limit, or two\nparties always win a nonlocal game impossible to always win classically, is\nbecause the parties are measuring a KS set. The simplest quantum system in\nwhich all these phenomena occur is a pair of three-level systems. However, the\nsimplest KS sets in dimension three known are asymmetrical and require a large\nnumber of bases (the current minimum is 16, set by Peres and Penrose). Here we\npresent a KS set that is much more symmetrical and easier to prove than any\nprevious example. It sets a new record for minimum number of bases, 14, and\nenables us to refute Conjecture 2 in Phys. Rev. Lett. 134, 010201 (2025),\nsetting a new record for qutrit-qutrit perfect strategies with a minimum number\nof inputs: 5-9.", "AI": {"tldr": "\u65b0KS\u96c6\u5408\uff1a\u66f4\u5bf9\u79f0\u3001\u57fa\u6570\u5c11\u81f314\uff0c\u53cd\u9a73\u731c\u60f32\u3002", "motivation": "\u4e3a\u4e86\u5bfb\u627e\u66f4\u7b80\u5355\u3001\u66f4\u5bf9\u79f0\u7684Kochen-Specker\uff08KS\uff09\u96c6\u5408\uff0c\u5e76\u4f18\u5316\u5b9e\u73b0\u65e0\u4fe1\u53f7\u9650\u5236\u7684\u4e8c\u65b9\u76f8\u5173\u6027\u548c\u975e\u5c40\u90e8\u535a\u5f08\u7684\u91cf\u5b50\u7cfb\u7edf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Kochen-Specker\uff08KS\uff09\u96c6\u5408\uff0c\u8be5\u96c6\u5408\u5728\u4e09\u7ef4\u7cfb\u7edf\u4e2d\u5177\u6709\u66f4\u9ad8\u7684\u5bf9\u79f0\u6027\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u7684KS\u96c6\u5408\u66f4\u5bb9\u6613\u8bc1\u660e\u3002\u901a\u8fc7\u6b64\u65b0\u96c6\u5408\uff0c\u5c06\u4e09\u7ef4\u7cfb\u7edf\u4e2dKS\u96c6\u5408\u6240\u9700\u7684\u6700\u5c0f\u57fa\u6570\u4ece16\u4e2a\u51cf\u5c11\u523014\u4e2a\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u4e2a\u5305\u542b14\u4e2a\u57fa\u6570\u7684\u66f4\u5bf9\u79f0\u3001\u66f4\u6613\u4e8e\u8bc1\u660e\u7684Kochen-Specker\uff08KS\uff09\u96c6\u5408\uff0c\u521b\u7eaa\u5f55\u5730\u51cf\u5c11\u4e86\u6240\u9700\u57fa\u6570\u3002\u6b64\u5916\uff0c\u8be5\u96c6\u5408\u5728qutrit-qutrit\u5b8c\u7f8e\u7b56\u7565\u4e2d\u5c06\u6700\u5c11\u8f93\u5165\u6570\u51cf\u5c11\u52305-9\u4e2a\uff0c\u5e76\u53cd\u9a73\u4e86Phys. Rev. Lett. 134, 010201 (2025)\u4e2d\u7684\u731c\u60f32\u3002", "conclusion": "\u65b0\u53d1\u73b0\u4e86\u4e00\u4e2a\u66f4\u5bf9\u79f0\u3001\u66f4\u5bb9\u6613\u8bc1\u660e\u7684Kochen-Specker\uff08KS\uff09\u96c6\u5408\uff0c\u5e76\u521b\u7eaa\u5f55\u5730\u5c06\u4e09\u7ef4\u7cfb\u7edf\u4e2dKS\u96c6\u5408\u6240\u9700\u7684\u6700\u5c11\u57fa\u6570\u51cf\u5c11\u523014\u4e2a\uff0c\u540c\u65f6\u5728qutrit-qutrit\u5b8c\u7f8e\u7b56\u7565\u4e2d\u5c06\u6700\u5c11\u8f93\u5165\u6570\u51cf\u5c11\u52305-9\u4e2a\uff0c\u4ece\u800c\u53cd\u9a73\u4e86Phys. Rev. Lett. 134, 010201 (2025)\u4e2d\u7684\u731c\u60f32\u3002"}}
{"id": "2508.07017", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07017", "abs": "https://arxiv.org/abs/2508.07017", "authors": ["Mao Li", "Fred Conrad", "Johann Gagnon-Bartsch"], "title": "Vec2Summ: Text Summarization via Probabilistic Sentence Embeddings", "comment": null, "summary": "We propose Vec2Summ, a novel method for abstractive summarization that frames\nthe task as semantic compression. Vec2Summ represents a document collection\nusing a single mean vector in the semantic embedding space, capturing the\ncentral meaning of the corpus. To reconstruct fluent summaries, we perform\nembedding inversion -- decoding this mean vector into natural language using a\ngenerative language model. To improve reconstruction quality and capture some\ndegree of topical variability, we introduce stochasticity by sampling from a\nGaussian distribution centered on the mean. This approach is loosely analogous\nto bagging in ensemble learning, where controlled randomness encourages more\nrobust and varied outputs. Vec2Summ addresses key limitations of LLM-based\nsummarization methods. It avoids context-length constraints, enables\ninterpretable and controllable generation via semantic parameters, and scales\nefficiently with corpus size -- requiring only $O(d + d^2)$ parameters.\nEmpirical results show that Vec2Summ produces coherent summaries for topically\nfocused, order-invariant corpora, with performance comparable to direct LLM\nsummarization in terms of thematic coverage and efficiency, albeit with less\nfine-grained detail. These results underscore Vec2Summ's potential in settings\nwhere scalability, semantic control, and corpus-level abstraction are\nprioritized.", "AI": {"tldr": "Vec2Summ\u662f\u4e00\u79cd\u65b0\u9896\u7684\u6458\u8981\u65b9\u6cd5\uff0c\u901a\u8fc7\u5c06\u6587\u6863\u96c6\u5408\u538b\u7f29\u6210\u4e00\u4e2a\u8bed\u4e49\u5411\u91cf\u5e76\u8fdb\u884c\u89e3\u7801\u6765\u751f\u6210\u6458\u8981\uff0c\u89e3\u51b3\u4e86LLM\u6458\u8981\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u3001\u53ef\u89e3\u91ca\u6027\u548c\u6548\u7387\u95ee\u9898\uff0c\u5728\u7279\u5b9a\u573a\u666f\u4e0b\u8868\u73b0\u51fa\u6f5c\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709LLM\u6458\u8981\u65b9\u6cd5\u5b58\u5728\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u9650\u5236\u3001\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u63a7\u6027\u5dee\u3001\u4ee5\u53ca\u6548\u7387\u95ee\u9898\u3002", "method": "Vec2Summ\u5c06\u6587\u6863\u96c6\u5408\u8868\u793a\u4e3a\u8bed\u4e49\u5d4c\u5165\u7a7a\u95f4\u4e2d\u7684\u5355\u4e2a\u5e73\u5747\u5411\u91cf\uff0c\u5e76\u901a\u8fc7\u91c7\u6837\u9ad8\u65af\u5206\u5e03\u8fdb\u884c\u6270\u52a8\u4ee5\u6355\u83b7\u4e3b\u9898\u53d8\u5f02\u6027\uff0c\u7136\u540e\u5c06\u5e73\u5747\u5411\u91cf\u53cd\u8f6c\u4e3a\u81ea\u7136\u8bed\u8a00\u4ee5\u751f\u6210\u6458\u8981\u3002", "result": "Vec2Summ\u751f\u6210\u7684\u6458\u8981\u5728\u8fde\u8d2f\u6027\u3001\u4e3b\u9898\u8986\u76d6\u548c\u6548\u7387\u65b9\u9762\u8868\u73b0\u826f\u597d\uff0c\u4e0eLLM\u6458\u8981\u76f8\u5f53\uff0c\u4f46\u5728\u7ec6\u8282\u65b9\u9762\u7565\u900a\u4e00\u7b79\u3002", "conclusion": "Vec2Summ\u5728\u4e3b\u9898\u805a\u7126\u3001\u987a\u5e8f\u65e0\u5173\u7684\u8bed\u6599\u5e93\u4e0a\u80fd\u751f\u6210\u8fde\u8d2f\u7684\u6458\u8981\uff0c\u5176\u5728\u4e3b\u9898\u8986\u76d6\u548c\u6548\u7387\u65b9\u9762\u4e0e\u76f4\u63a5\u7684LLM\u6458\u8981\u76f8\u5f53\uff0c\u4f46\u7ec6\u8282\u4e0d\u591f\u4e30\u5bcc\u3002\u8fd9\u4e9b\u7ed3\u679c\u8868\u660e\uff0c\u5728\u4f18\u5148\u8003\u8651\u53ef\u6269\u5c55\u6027\u3001\u8bed\u4e49\u63a7\u5236\u548c\u8bed\u6599\u5e93\u7ea7\u522b\u62bd\u8c61\u7684\u573a\u666f\u4e2d\uff0cVec2Summ\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2508.06625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06625", "abs": "https://arxiv.org/abs/2508.06625", "authors": ["Shilong Zou", "Yuhang Huang", "Renjiao Yi", "Chenyang Zhu", "Kai Xu"], "title": "CycleDiff: Cycle Diffusion Models for Unpaired Image-to-image Translation", "comment": null, "summary": "We introduce a diffusion-based cross-domain image translator in the absence\nof paired training data. Unlike GAN-based methods, our approach integrates\ndiffusion models to learn the image translation process, allowing for more\ncoverable modeling of the data distribution and performance improvement of the\ncross-domain translation. However, incorporating the translation process within\nthe diffusion process is still challenging since the two processes are not\naligned exactly, i.e., the diffusion process is applied to the noisy signal\nwhile the translation process is conducted on the clean signal. As a result,\nrecent diffusion-based studies employ separate training or shallow integration\nto learn the two processes, yet this may cause the local minimal of the\ntranslation optimization, constraining the effectiveness of diffusion models.\nTo address the problem, we propose a novel joint learning framework that aligns\nthe diffusion and the translation process, thereby improving the global\noptimality. Specifically, we propose to extract the image components with\ndiffusion models to represent the clean signal and employ the translation\nprocess with the image components, enabling an end-to-end joint learning\nmanner. On the other hand, we introduce a time-dependent translation network to\nlearn the complex translation mapping, resulting in effective translation\nlearning and significant performance improvement. Benefiting from the design of\njoint learning, our method enables global optimization of both processes,\nenhancing the optimality and achieving improved fidelity and structural\nconsistency. We have conducted extensive experiments on RGB$\\leftrightarrow$RGB\nand diverse cross-modality translation tasks including\nRGB$\\leftrightarrow$Edge, RGB$\\leftrightarrow$Semantics and\nRGB$\\leftrightarrow$Depth, showcasing better generative performances than the\nstate of the arts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8de8\u57df\u56fe\u50cf\u7ffb\u8bd1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6846\u67b6\u89e3\u51b3\u4e86\u6269\u6563\u8fc7\u7a0b\u548c\u7ffb\u8bd1\u8fc7\u7a0b\u4e4b\u95f4\u7684\u5bf9\u9f50\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u7ffb\u8bd1\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u6269\u6563\u8fc7\u7a0b\u548c\u7ffb\u8bd1\u8fc7\u7a0b\u4e4b\u95f4\u5bf9\u9f50\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u5b66\u4e60\u6846\u67b6\u6765\u6539\u8fdb\u5168\u5c40\u6700\u4f18\u6027\uff0c\u4ece\u800c\u63d0\u9ad8\u8de8\u57df\u7ffb\u8bd1\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8054\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u6269\u6563\u6a21\u578b\u4e2d\u63d0\u53d6\u56fe\u50cf\u7ec4\u4ef6\u6765\u8868\u793a\u6e05\u6670\u4fe1\u53f7\uff0c\u5e76\u4f7f\u7528\u56fe\u50cf\u7ec4\u4ef6\u8fdb\u884c\u7ffb\u8bd1\u8fc7\u7a0b\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u7aef\u5230\u7aef\u7684\u8054\u5408\u5b66\u4e60\u3002\u6b64\u5916\uff0c\u5f15\u5165\u4e86\u65f6\u53d8\u7ffb\u8bd1\u7f51\u7edc\u6765\u5b66\u4e60\u590d\u6742\u7684\u7ffb\u8bd1\u6620\u5c04\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5b9e\u73b0\u4e86\u4e24\u4e2a\u8fc7\u7a0b\u7684\u5168\u5c40\u4f18\u5316\uff0c\u63d0\u9ad8\u4e86\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\uff0c\u5e76\u5728\u5404\u79cd\u8de8\u6a21\u6001\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u751f\u6210\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u8054\u5408\u5b66\u4e60\u6846\u67b6\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u4e24\u4e2a\u8fc7\u7a0b\u7684\u5168\u5c40\u4f18\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u6700\u4f18\u6027\u5e76\u83b7\u5f97\u66f4\u597d\u7684\u4fdd\u771f\u5ea6\u548c\u7ed3\u6784\u4e00\u81f4\u6027\u3002\u5728RGB$\\\towards$RGB\u548cRGB$\\\towards$Edge\uff0cRGB$\\\towards$Semantics\uff0cRGB$\\\towards$Depth\u7b49\u591a\u79cd\u8de8\u6a21\u6001\u7ffb\u8bd1\u4efb\u52a1\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5c55\u793a\u4e86\u6bd4\u73b0\u6709\u6280\u672f\u66f4\u597d\u7684\u751f\u6210\u6027\u80fd\u3002"}}
{"id": "2508.08239", "categories": ["cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.08239", "abs": "https://arxiv.org/abs/2508.08239", "authors": ["Raffael L. Klees", "M\u00f3nica Benito"], "title": "Readout of multi-level quantum geometry from electronic transport", "comment": "6 pages and 3 figures (main text); 2 pages and 1 figure (End Matter);\n  9 pages (Supplemental Material)", "summary": "The quantum geometric tensor (QGT) of a quantum system in a given parameter\nspace captures both the geometry of the state manifold and the topology of the\nsystem. While the local QGT elements have been successfully measured in various\nplatforms, the challenge of detecting them in electronic transport systems -\nsuch as tunnel or molecular junctions - has yet to be resolved. To fill this\ngap, we propose a measurement protocol based on weak and resonant parameter\nmodulations, and theoretically demonstrate how the local QGT in such systems\ncan be directly probed from changes of the tunnel conductance. This approach\nenables the measurement of both geometrical and topological features of quantum\nstates in a broad class of transport-based quantum systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7535\u5b50\u8f93\u8fd0\u7cfb\u7edf\u4e2d\u6d4b\u91cf\u91cf\u5b50\u51e0\u4f55\u5f20\u91cf\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u6d4b\u91cf\u96a7\u9053\u7535\u5bfc\u7684\u53d8\u5316\u6765\u63a2\u6d4b\u7cfb\u7edf\u7684\u51e0\u4f55\u548c\u62d3\u6251\u7279\u5f81\u3002", "motivation": "\u586b\u8865\u4e86\u5728\u7535\u5b50\u8f93\u8fd0\u7cfb\u7edf\u4e2d\u63a2\u6d4b\u5c40\u57df\u91cf\u5b50\u51e0\u4f55\u5f20\u91cf\u7684\u7a7a\u767d\uff0c\u800c\u8fd9\u5728\u4e4b\u524d\u7684\u7814\u7a76\u4e2d\u5c1a\u672a\u89e3\u51b3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f31\u548c\u5171\u632f\u53c2\u6570\u8c03\u5236\u7684\u6d4b\u91cf\u65b9\u6848\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u8bc1\u660e\u4e86\u5982\u4f55\u76f4\u63a5\u4ece\u96a7\u9053\u7535\u5bfc\u7684\u53d8\u5316\u4e2d\u63a2\u6d4b\u6b64\u7c7b\u7cfb\u7edf\u4e2d\u7684\u5c40\u57df\u91cf\u5b50\u51e0\u4f55\u5f20\u91cf\u3002", "result": "\u7406\u8bba\u4e0a\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u7684\u6d4b\u91cf\u65b9\u6848\u80fd\u591f\u76f4\u63a5\u63a2\u6d4b\u5c40\u57df\u91cf\u5b50\u51e0\u4f55\u5f20\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6d4b\u91cf\u591a\u79cd\u57fa\u4e8e\u8f93\u8fd0\u7684\u91cf\u5b50\u7cfb\u7edf\u7684\u51e0\u4f55\u548c\u62d3\u6251\u7279\u5f81\u3002"}}
{"id": "2508.08097", "categories": ["eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08097", "abs": "https://arxiv.org/abs/2508.08097", "authors": ["Muhammad Asif", "Zain Ali", "Asim Ihsan", "Ali Ranjha", "Zhu Shoujin", "Manzoor Ahmed", "Xingwang Li", "Symeon Chatzinotas"], "title": "Robust Design of Beyond-Diagonal Reconfigurable Intelligent Surface Empowered RSMA-SWIPT System Under Channel Estimation Errors", "comment": "13 pages, and 11 figures. Submitted to IEEE", "summary": "This work explores the integration of rate-splitting multiple access (RSMA),\nsimultaneous wireless information and power transfer (SWIPT), and\nbeyond-diagonal reconfigurable intelligent surface (BD-RIS) to enhance the\nspectral-efficiency, energy-efficiency, coverage, and connectivity of future\nsixth-generation (6G) communication networks. Specifically, with a multiuser\nBD-RIS-empowered RSMA-SWIPT system, we jointly optimize the transmit precoding\nvectors, the common rate proportion of users, the power-splitting ratios, and\nscattering matrix of BD-RIS node, under the assumption of imperfect channel\nstate information (CSI). Additionally, to better capture practical hardware\nbehavior, we incorporate a nonlinear energy harvesting model under energy\nharvesting constraints. We design a robust optimization framework to maximize\nthe system sum-rate, while explicitly accounting for the worst-case impact of\nCSI uncertainties. Further, we introduce an alternating optimization framework\nthat partitions the problem into several blocks, which are optimized\niteratively. More specifically, the transmit precoding vectors are optimized by\nreformulating the problem as a convex semidefinite programming through\nsuccessive-convex approximation (SCA), whereas the power-splitting problem is\nsolved using the MOSEK-enabled CVX toolbox. Subsequently, to optimize the\nscattering matrix of the BD-RIS, we first employ SCA to reformulate the problem\ninto a convex form, and then design a manifold optimization strategy based on\nthe Conjugate-Gradient method. Finally, numerical simulation results reveal\nthat the proposed scheme provides significant performance improvements over\nexisting benchmarks and demonstrates rapid convergence within a reasonable\nnumber of iterations.", "AI": {"tldr": "\u672c\u7814\u7a76\u96c6\u6210\u4e86RSMA\u3001SWIPT\u548cBD-RIS\u6280\u672f\uff0c\u901a\u8fc7\u9c81\u68d2\u4f18\u5316\u548c\u4ea4\u66ff\u4f18\u5316\u7b97\u6cd5\uff0c\u5728\u4e0d\u5b8c\u7f8e\u4fe1\u9053\u548c\u975e\u7ebf\u6027\u80fd\u91cf\u6536\u96c6\u7684\u6761\u4ef6\u4e0b\uff0c\u6700\u5927\u5316\u4e866G\u7f51\u7edc\u7684\u548c\u901f\u7387\uff0c\u5e76\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u672a\u6765\u7b2c\u516d\u4ee3\uff086G\uff09\u901a\u4fe1\u7f51\u7edc\u7684\u9891\u8c31\u6548\u7387\u3001\u80fd\u6e90\u6548\u7387\u3001\u8986\u76d6\u8303\u56f4\u548c\u8fde\u63a5\u6027\uff0c\u672c\u7814\u7a76\u63a2\u7d22\u4e86\u901f\u7387\u5206\u88c2\u591a\u5740\uff08RSMA\uff09\u3001\u540c\u6b65\u65e0\u7ebf\u4fe1\u606f\u4e0e\u80fd\u91cf\u4f20\u8f93\uff08SWIPT\uff09\u4ee5\u53ca\u8d85\u8d8a\u5bf9\u89d2\u53ef\u91cd\u6784\u667a\u80fd\u8868\u9762\uff08BD-RIS\uff09\u7684\u96c6\u6210\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8054\u5408\u4f18\u5316\u9884\u7f16\u7801\u5411\u91cf\u3001\u7528\u6237\u516c\u5171\u901f\u7387\u6bd4\u4f8b\u3001\u529f\u7387\u5206\u914d\u6bd4\u548cBD-RIS\u6563\u5c04\u77e9\u9635\u7684\u9c81\u68d2\u4f18\u5316\u6846\u67b6\uff0c\u4ee5\u6700\u5927\u5316\u7cfb\u7edf\u548c\u901f\u7387\u3002\u8be5\u6846\u67b6\u5229\u7528\u4ea4\u66ff\u4f18\u5316\u65b9\u6cd5\uff0c\u5c06\u95ee\u9898\u5206\u89e3\u4e3a\u591a\u4e2a\u5b50\u95ee\u9898\uff0c\u5e76\u91c7\u7528\u8fde\u7eed\u51f8\u8fd1\u4f3c\uff08SCA\uff09\u3001\u51f8\u534a\u5b9a\u89c4\u5212\u3001MOSEK\u9a71\u52a8\u7684CVX\u5de5\u5177\u7bb1\u4ee5\u53ca\u57fa\u4e8e\u5171\u8f6d\u68af\u5ea6\u6cd5\u7684\u6d41\u5f62\u4f18\u5316\u7b56\u7565\u8fdb\u884c\u6c42\u89e3\uff0c\u540c\u65f6\u8003\u8651\u4e86\u4e0d\u5b8c\u7f8e\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u548c\u975e\u7ebf\u6027\u80fd\u91cf\u6536\u96c6\u6a21\u578b\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u7ed3\u679c\u9a8c\u8bc1\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u591a\u9879\u5173\u952e\u6027\u80fd\u6307\u6807\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u826f\u597d\u7684\u6536\u655b\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6848\u5728\u9891\u8c31\u6548\u7387\u3001\u80fd\u6e90\u6548\u7387\u3001\u8986\u76d6\u8303\u56f4\u548c\u8fde\u63a5\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u57fa\u51c6\uff0c\u5e76\u4e14\u5728\u5408\u7406\u7684\u8fed\u4ee3\u6b21\u6570\u5185\u6536\u655b\u901f\u5ea6\u5feb\u3002"}}
{"id": "2508.06776", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.06776", "abs": "https://arxiv.org/abs/2508.06776", "authors": ["Amit Pandey"], "title": "Zero-Direction Probing: A Linear-Algebraic Framework for Deep Analysis of Large-Language-Model Drift", "comment": "14 pages", "summary": "We present Zero-Direction Probing (ZDP), a theory-only framework for\ndetecting model drift from null directions of transformer activations without\ntask labels or output evaluations. Under assumptions A1--A6, we prove: (i) the\nVariance--Leak Theorem, (ii) Fisher Null-Conservation, (iii) a Rank--Leak bound\nfor low-rank updates, and (iv) a logarithmic-regret guarantee for online\nnull-space trackers. We derive a Spectral Null-Leakage (SNL) metric with\nnon-asymptotic tail bounds and a concentration inequality, yielding a-priori\nthresholds for drift under a Gaussian null model. These results show that\nmonitoring right/left null spaces of layer activations and their Fisher\ngeometry provides concrete, testable guarantees on representational change.", "AI": {"tldr": "ZDP framework detects model drift using null directions of transformer activations and theoretical guarantees, without needing task labels or output evaluations.", "motivation": "To develop a theory-only framework for detecting model drift from null directions of transformer activations, without relying on task labels or output evaluations.", "method": "Zero-Direction Probing (ZDP) is a theory-only framework that analyzes null directions of transformer activations without requiring task labels or output evaluations. It leverages the Variance-Leak Theorem, Fisher Null-Conservation, and a Rank-Leak bound for low-rank updates, and achieves a logarithmic-regret guarantee for online null-space trackers.", "result": "The framework provides a Spectral Null-Leakage (SNL) metric with non-asymptotic tail bounds and a concentration inequality, enabling a-priori thresholds for drift detection under a Gaussian null model. Theoretical proofs include the Variance-Leak Theorem, Fisher Null-Conservation, and a Rank-Leak bound.", "conclusion": "Null space monitoring of transformer activations offers concrete, testable guarantees on representational change, as demonstrated by the Variance-Leak Theorem, Fisher Null-Conservation, and Rank-Leak bound."}}
{"id": "2508.07455", "categories": ["cond-mat.mtrl-sci", "physics.app-ph"], "pdf": "https://arxiv.org/pdf/2508.07455", "abs": "https://arxiv.org/abs/2508.07455", "authors": ["Puja Ghosh", "Pritam Ghosh", "Rizwin Khanam", "Chandra Shekhar Prajapati", "Aarti Nagarajan", "Shreeja Das", "Rakesh Paleja", "Sharan Shetty", "Gopalakrishnan Sai Gautam", "Navakanta Bhat"], "title": "Experimental and Computational Demonstration of a Highly Stable, in-situ Pt Decorated Sputtered ZnO Hydrogen Sensor for sub-ppm Level Detection", "comment": null, "summary": "In this work, we present a Pt decorated ZnO thin film-based gas sensor for\nhydrogen detection, fabricated using a sputtering technique and an in-situ Pt\ndecoration approach. Specifically, we deposit a ZnO thin film on an\ninterdigitated electrode substrate, with Pt nanoclusters added to the (002)\npolar plane by brief sputtering (1 to 6 s) to create an active sensing\ninterface. Our sensor demonstrates optimal performance at an operating\ntemperature of 498 K, with rapid response and recovery times (10 and 3 s), high\nselectivity, and long-term stability. We find the Pt decorated ZnO sensor, with\na Pt deposition time of 2 s, to exhibit enhanced response (~52,987%) to 1%\nhydrogen concentration, indicating its suitability for industrial and\nenvironmental monitoring applications. Additionally, our device demonstrates\nreliable detection of low hydrogen concentrations (~100 ppb), with a response\nof ~38% and no response drift over one year of testing, underscoring the\nlong-term stability of the sensor. To elucidate the role of Pt deposition and\npristine ZnO in hydrogen sensing, we perform density functional theory\ncalculations, analysing adsorption and reaction energetics involving H2, O2, O,\nOH, and H2O, and lattice oxygen atoms on the ZnO (002) surface with and without\nPt decoration. Our computational data is in agreement with our experiments,\nidentifying the oxygen-exposed (002) surface to be most active for hydrogen\nsensing in both pristine and Pt decorated ZnO. Further, our computations\nhighlight the role of Pt in enhancing hydrogen sensitivity via i) activating an\nautoreduction pathway of adsorbed OH, ii) spontaneous dissociation of adsorbed\nmolecular H2, and iii) keeping the lattice oxygen pathway of forming H2O\nactive. Our systematic approach of designing sensors combining an experimental\nsetup with theoretical insights, is key in developing and optimizing efficient\nhydrogen gas sensors.", "AI": {"tldr": "\u672c\u7814\u7a76\u5236\u5907\u4e86\u4e00\u79cdPt\u4fee\u9970\u7684ZnO\u8584\u819c\u4f20\u611f\u5668\uff0c\u7528\u4e8e\u6c22\u6c14\u68c0\u6d4b\u3002\u8be5\u4f20\u611f\u5668\u5728498 K\u4e0b\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5305\u62ec\u5feb\u901f\u54cd\u5e94\u3001\u9ad8\u9009\u62e9\u6027\u548c\u957f\u671f\u7a33\u5b9a\u6027\u3002\u7406\u8bba\u8ba1\u7b97\u7ed3\u679c\u652f\u6301\u5b9e\u9a8c\u89c2\u5bdf\uff0c\u5e76\u63ed\u793a\u4e86Pt\u589e\u5f3a\u4f20\u611f\u673a\u5236\u3002", "motivation": "\u5f00\u53d1\u4e00\u79cd\u7528\u4e8e\u6c22\u6c14\u68c0\u6d4b\u7684\u9ad8\u6548\u3001\u7a33\u5b9a\u548c\u9ad8\u9009\u62e9\u6027\u7684\u4f20\u611f\u5668\uff0c\u4ee5\u6ee1\u8db3\u5de5\u4e1a\u548c\u73af\u5883\u76d1\u6d4b\u7684\u9700\u6c42\u3002", "method": "\u91c7\u7528\u6e85\u5c04\u6280\u672f\u548c\u539f\u4f4dPt\u4fee\u9970\u65b9\u6cd5\u5236\u5907Pt\u4fee\u9970\u7684ZnO\u8584\u819c\uff0c\u5e76\u901a\u8fc7\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u5206\u6790\u4e86Pt\u4fee\u9970\u5bf9ZnO(002)\u8868\u9762\u5438\u9644\u548c\u53cd\u5e94\u6d3b\u6027\u7684\u5f71\u54cd\u3002", "result": "Pt\u4fee\u9970\u7684ZnO\u8584\u819c\u4f20\u611f\u5668\u5728498 K\u7684\u6700\u4f73\u5de5\u4f5c\u6e29\u5ea6\u4e0b\uff0c\u5bf91%\u7684\u6c22\u6c14\u6d53\u5ea6\u8868\u73b0\u51fa\u7ea652,987%\u7684\u589e\u5f3a\u54cd\u5e94\uff0c\u5e76\u80fd\u53ef\u9760\u68c0\u6d4b\u4f4e\u81f3100 ppb\u7684\u6c22\u6c14\u6d53\u5ea6\uff0c\u4e14\u5728\u4e00\u5e74\u6d4b\u8bd5\u4e2d\u65e0\u54cd\u5e94\u6f02\u79fb\u3002\u7406\u8bba\u8ba1\u7b97\u8868\u660e\uff0cPt\u7684\u5f15\u5165\u901a\u8fc7\u6fc0\u6d3b\u5438\u9644\u7684OH\u3001\u4fc3\u8fdbH2\u7684\u89e3\u79bb\u4ee5\u53ca\u7ef4\u6301\u5f62\u6210H2O\u7684\u6676\u683c\u6c27\u9014\u5f84\uff0c\u589e\u5f3a\u4e86\u6c22\u6c14\u4f20\u611f\u654f\u611f\u6027\u3002", "conclusion": "Pt\u4fee\u9970\u7684ZnO\u8584\u819c\u4f20\u611f\u5668\u5728\u6c22\u6c14\u68c0\u6d4b\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u5177\u6709\u5feb\u901f\u54cd\u5e94\u3001\u9ad8\u9009\u62e9\u6027\u548c\u957f\u671f\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5de5\u4e1a\u548c\u73af\u5883\u76d1\u6d4b\u3002\u7406\u8bba\u8ba1\u7b97\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u6570\u636e\u4e00\u81f4\uff0c\u89e3\u91ca\u4e86Pt\u63d0\u9ad8\u6c22\u6c14\u4f20\u611f\u654f\u611f\u6027\u7684\u673a\u5236\u3002"}}
{"id": "2508.07182", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07182", "abs": "https://arxiv.org/abs/2508.07182", "authors": ["Xuesong Li", "Lars Petersson", "Vivien Rolland"], "title": "3D Gaussian Representations with Motion Trajectory Field for Dynamic Scene Reconstruction", "comment": null, "summary": "This paper addresses the challenge of novel-view synthesis and motion\nreconstruction of dynamic scenes from monocular video, which is critical for\nmany robotic applications. Although Neural Radiance Fields (NeRF) and 3D\nGaussian Splatting (3DGS) have demonstrated remarkable success in rendering\nstatic scenes, extending them to reconstruct dynamic scenes remains\nchallenging. In this work, we introduce a novel approach that combines 3DGS\nwith a motion trajectory field, enabling precise handling of complex object\nmotions and achieving physically plausible motion trajectories. By decoupling\ndynamic objects from static background, our method compactly optimizes the\nmotion trajectory field. The approach incorporates time-invariant motion\ncoefficients and shared motion trajectory bases to capture intricate motion\npatterns while minimizing optimization complexity. Extensive experiments\ndemonstrate that our approach achieves state-of-the-art results in both\nnovel-view synthesis and motion trajectory recovery from monocular video,\nadvancing the capabilities of dynamic scene reconstruction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u9ad8\u65af\u6cfc\u6e85\u548c\u8fd0\u52a8\u8f68\u8ff9\u573a\u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u5355\u76ee\u89c6\u9891\u4e2d\u7684\u52a8\u6001\u573a\u666f\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8fd0\u52a8\u91cd\u5efa\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5355\u76ee\u89c6\u9891\u4e2d\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8fd0\u52a8\u91cd\u5efa\u7684\u6311\u6218\uff0c\u8fd9\u5bf9\u4e8e\u8bb8\u591a\u673a\u5668\u4eba\u5e94\u7528\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u5c063D\u9ad8\u65af\u6cfc\u6e85\uff083DGS\uff09\u4e0e\u8fd0\u52a8\u8f68\u8ff9\u573a\u76f8\u7ed3\u5408\uff0c\u5f15\u5165\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u80fd\u591f\u7cbe\u786e\u5904\u7406\u590d\u6742\u7684\u7269\u4f53\u8fd0\u52a8\u5e76\u5b9e\u73b0\u7269\u7406\u4e0a\u5408\u7406\u7684\u8fd0\u52a8\u8f68\u8ff9\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u52a8\u6001\u5bf9\u8c61\u4e0e\u9759\u6001\u80cc\u666f\u5206\u79bb\uff0c\u7d27\u51d1\u5730\u4f18\u5316\u8fd0\u52a8\u8f68\u8ff9\u573a\uff0c\u5e76\u7ed3\u5408\u4e86\u65f6\u4e0d\u53d8\u8fd0\u52a8\u7cfb\u6570\u548c\u5171\u4eab\u8fd0\u52a8\u8f68\u8ff9\u57fa\u4ee5\u6355\u83b7\u590d\u6742\u7684\u8fd0\u52a8\u6a21\u5f0f\uff0c\u540c\u65f6\u6700\u5c0f\u5316\u4f18\u5316\u590d\u6742\u5ea6\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8fd0\u52a8\u8f68\u8ff9\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u52a8\u6001\u573a\u666f\u7684\u65b0\u89c6\u89d2\u5408\u6210\u548c\u8fd0\u52a8\u8f68\u8ff9\u6062\u590d\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u63d0\u9ad8\u4e86\u52a8\u6001\u573a\u666f\u91cd\u5efa\u7684\u80fd\u529b\u3002"}}
{"id": "2508.07344", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07344", "abs": "https://arxiv.org/abs/2508.07344", "authors": ["Shehbaz Tariq", "Junaid ur Rehman", "Symeon Chatzinotas"], "title": "Quantum MIMO Diversity over Discrete-Variable Crosstalk Channels", "comment": "This work has been submitted to the IEEE Journal on Selected Areas in\n  Communications for possible publication", "summary": "Quantum communication plays a pivotal role in enabling distributed quantum\ncomputing and sensing. Diversity strategies can be used to increase the\ncommunication reliability (in the sense of output fidelity with respect to the\ninput quantum state) when multiple communication channels are available. The\ncurrent paper proposes a diversity strategy for quantum discrete variable (DV)\nmultiple-input-multiple-output (MIMO) channels, utilizing approximate cloning\nto distribute information across multiple channels at the transmitter and\npurification to merge the noisy and entangled joint state into a single quantum\nstate at the receiver. The proposed method is compared with simpler strategies,\nsuch as the best-channel selection, to identify the advantage regions over a\nquantum channel combining both crosstalk and depolarization, where the\ncrosstalk is modeled by a controlled-SWAP operator. Our numerical results show\nthat the cloning-purification strategy offers an advantage, especially in cases\nwhere full channel state information (CSI) can be exploited to optimize the\ncloning asymmetry. More importantly, and in contrast to the classic MIMO\ndiversity, we demonstrate that the distribution of quantum information over all\navailable channels does not always provide an advantage due to the dilution\ncost of the cloning operation.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u91cf\u5b50MIMO\u5206\u96c6\u7b56\u7565\uff0c\u5229\u7528\u514b\u9686\u548c\u63d0\u7eaf\u6280\u672f\u6765\u63d0\u9ad8\u901a\u4fe1\u53ef\u9760\u6027\uff0c\u5e76\u5728\u7279\u5b9a\u6761\u4ef6\u4e0b\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u91cf\u5b50\u901a\u4fe1\u5728\u5b9e\u73b0\u5206\u5e03\u5f0f\u91cf\u5b50\u8ba1\u7b97\u548c\u4f20\u611f\u4e2d\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u5f53\u5b58\u5728\u591a\u4e2a\u901a\u4fe1\u4fe1\u9053\u65f6\uff0c\u53ef\u4ee5\u4f7f\u7528\u5206\u96c6\u7b56\u7565\u6765\u63d0\u9ad8\u901a\u4fe1\u7684\u53ef\u9760\u6027\uff08\u5728\u8f93\u51fa\u4fdd\u771f\u5ea6\u65b9\u9762\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u91cf\u5b50\u79bb\u6563\u53d8\u91cf\uff08DV\uff09\u591a\u8f93\u5165\u591a\u8f93\u51fa\uff08MIMO\uff09\u4fe1\u9053\u7684\u534f\u4f5c\u7b56\u7565\uff0c\u8be5\u7b56\u7565\u5229\u7528\u8fd1\u4f3c\u514b\u9686\u6280\u672f\u5728\u53d1\u5c04\u7aef\u5c06\u4fe1\u606f\u5206\u5e03\u5230\u591a\u4e2a\u4fe1\u9053\uff0c\u5e76\u5229\u7528\u63d0\u7eaf\u6280\u672f\u5728\u63a5\u6536\u7aef\u5c06\u6df7\u5408\u7684\u3001\u7ea0\u7f20\u7684\u8054\u5408\u6001\u5408\u5e76\u4e3a\u5355\u4e2a\u91cf\u5b50\u6001\u3002", "result": "\u514b\u9686-\u63d0\u7eaf\u7b56\u7565\u63d0\u4f9b\u4e86\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u53ef\u4ee5\u5229\u7528\u5b8c\u6574\u7684\u4fe1\u9053\u72b6\u6001\u4fe1\u606f\uff08CSI\uff09\u6765\u4f18\u5316\u514b\u9686\u4e0d\u5bf9\u79f0\u6027\u7684\u60c5\u51b5\u4e0b\u3002\u4e0e\u7ecf\u5178MIMO\u5206\u96c6\u4e0d\u540c\uff0c\u91cf\u5b50\u4fe1\u606f\u5206\u5e03\u5728\u6240\u6709\u53ef\u7528\u4fe1\u9053\u4e0a\u5e76\u4e0d\u603b\u662f\u6709\u5229\u7684\uff0c\u56e0\u4e3a\u514b\u9686\u64cd\u4f5c\u4f1a\u7a00\u91ca\u4fe1\u606f\u3002", "conclusion": "\u91cf\u5b50\u4fe1\u606f\u5206\u5e03\u5728\u6240\u6709\u53ef\u7528\u4fe1\u9053\u4e0a\u5e76\u4e0d\u603b\u662f\u6709\u5229\u7684\uff0c\u56e0\u4e3a\u514b\u9686\u64cd\u4f5c\u4f1a\u7a00\u91ca\u4fe1\u606f\u3002"}}
{"id": "2508.07069", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07069", "abs": "https://arxiv.org/abs/2508.07069", "authors": ["Muhammad Dehan Al Kautsar", "Aswin Candra", "Muhammad Alif Al Hakim", "Maxalmina Satria Kahfi", "Fajri Koto", "Alham Fikri Aji", "Peerat Limkonchotiwat", "Ekapol Chuangsuwanich", "Genta Indra Winata"], "title": "SEADialogues: A Multilingual Culturally Grounded Multi-turn Dialogue Dataset on Southeast Asian Languages", "comment": "Preprint", "summary": "Although numerous datasets have been developed to support dialogue systems,\nmost existing chit-chat datasets overlook the cultural nuances inherent in\nnatural human conversations. To address this gap, we introduce SEADialogues, a\nculturally grounded dialogue dataset centered on Southeast Asia, a region with\nover 700 million people and immense cultural diversity. Our dataset features\ndialogues in eight languages from six Southeast Asian countries, many of which\nare low-resource despite having sizable speaker populations. To enhance\ncultural relevance and personalization, each dialogue includes persona\nattributes and two culturally grounded topics that reflect everyday life in the\nrespective communities. Furthermore, we release a multi-turn dialogue dataset\nto advance research on culturally aware and human-centric large language\nmodels, including conversational dialogue agents.", "AI": {"tldr": "SEADialogues \u662f\u4e00\u4e2a\u5305\u542b\u516b\u79cd\u8bed\u8a00\u548c\u6587\u5316\u76f8\u5173\u4e3b\u9898\u7684\u4e1c\u5357\u4e9a\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u5f00\u53d1\u5177\u6709\u6587\u5316\u610f\u8bc6\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u95f2\u804a\u6570\u636e\u96c6\u90fd\u5ffd\u7565\u4e86\u81ea\u7136\u4eba\u7c7b\u5bf9\u8bdd\u4e2d\u56fa\u6709\u7684\u6587\u5316\u5dee\u5f02\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\uff0c\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4ee5\u4e1c\u5357\u4e9a\u4e3a\u4e2d\u5fc3\u7684\u3001\u4ee5\u6587\u5316\u4e3a\u57fa\u7840\u7684\u5bf9\u8bdd\u6570\u636e\u96c6SEADialogues\u3002", "method": "SEADialogues\u6570\u636e\u96c6\u5305\u542b\u516b\u79cd\u8bed\u8a00\u7684\u5bf9\u8bdd\uff0c\u5e76\u5305\u542b\u4e2a\u4eba\u5c5e\u6027\u548c\u53cd\u6620\u5f53\u5730\u793e\u533a\u65e5\u5e38\u751f\u6d3b\u7684\u4e24\u4e2a\u6587\u5316\u76f8\u5173\u4e3b\u9898\u3002", "result": "SEADialogues \u662f\u4e00\u4e2a\u5305\u542b\u4e1c\u5357\u4e9a\u516b\u79cd\u8bed\u8a00\u7684\u5bf9\u8bdd\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u8bb8\u591a\u8bed\u8a00\u5c3d\u7ba1\u6709\u5927\u91cf\u4f7f\u7528\u8005\uff0c\u4f46\u4ecd\u5c5e\u4f4e\u8d44\u6e90\u8bed\u8a00\u3002", "conclusion": "SEADialogues\u4e3a\u7814\u7a76\u5177\u6709\u6587\u5316\u610f\u8bc6\u548c\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5305\u62ec\u5bf9\u8bdd\u4ee3\u7406\uff0c\u63d0\u4f9b\u4e86\u4e00\u4e2a\u591a\u8f6e\u5bf9\u8bdd\u6570\u636e\u96c6\u3002"}}
{"id": "2508.06632", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06632", "abs": "https://arxiv.org/abs/2508.06632", "authors": ["Wenpeng Xing", "Jie Chen", "Zaifeng Yang", "Tiancheng Zhao", "Gaolei Li", "Changting Lin", "Yike Guo", "Meng Han"], "title": "CoDe-NeRF: Neural Rendering via Dynamic Coefficient Decomposition", "comment": null, "summary": "Neural Radiance Fields (NeRF) have shown impressive performance in novel view\nsynthesis, but challenges remain in rendering scenes with complex specular\nreflections and highlights. Existing approaches may produce blurry reflections\ndue to entanglement between lighting and material properties, or encounter\noptimization instability when relying on physically-based inverse rendering. In\nthis work, we present a neural rendering framework based on dynamic coefficient\ndecomposition, aiming to improve the modeling of view-dependent appearance. Our\napproach decomposes complex appearance into a shared, static neural basis that\nencodes intrinsic material properties, and a set of dynamic coefficients\ngenerated by a Coefficient Network conditioned on view and illumination. A\nDynamic Radiance Integrator then combines these components to synthesize the\nfinal radiance. Experimental results on several challenging benchmarks suggest\nthat our method can produce sharper and more realistic specular highlights\ncompared to existing techniques. We hope that this decomposition paradigm can\nprovide a flexible and effective direction for modeling complex appearance in\nneural scene representations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u6e32\u67d3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u7cfb\u6570\u5206\u89e3\u6765\u66f4\u597d\u5730\u5904\u7406\u590d\u6742\u7684\u955c\u9762\u53cd\u5c04\u548c\u9ad8\u5149\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709 NeRF \u65b9\u6cd5\u5728\u6e32\u67d3\u5177\u6709\u590d\u6742\u955c\u9762\u53cd\u5c04\u548c\u9ad8\u5149\u573a\u666f\u65f6\u5b58\u5728\u7684\u53cd\u5c04\u6a21\u7cca\u6216\u4f18\u5316\u4e0d\u7a33\u5b9a\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u7cfb\u6570\u5206\u89e3\u7684\u795e\u7ecf\u6e32\u67d3\u6846\u67b6\uff0c\u5c06\u590d\u6742\u5916\u89c2\u5206\u89e3\u4e3a\u7f16\u7801\u5185\u5728\u6750\u6599\u7279\u6027\u7684\u5171\u4eab\u9759\u6001\u795e\u7ecf\u57fa\u7840\uff0c\u4ee5\u53ca\u7531\u6761\u4ef6\u4e8e\u89c6\u56fe\u548c\u5149\u7167\u7684\u7cfb\u6570\u7f51\u7edc\u751f\u6210\u7684\u52a8\u6001\u7cfb\u6570\u96c6\u3002\u7136\u540e\uff0c\u52a8\u6001\u8f90\u5c04\u79ef\u5206\u5668\u5c06\u8fd9\u4e9b\u7ec4\u4ef6\u7ec4\u5408\u8d77\u6765\u4ee5\u5408\u6210\u6700\u7ec8\u7684\u8f90\u5c04\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u73b0\u6709\u6280\u672f\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ea7\u751f\u66f4\u6e05\u6670\u3001\u66f4\u903c\u771f\u7684\u955c\u9762\u9ad8\u5149\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u52a8\u6001\u7cfb\u6570\u5206\u89e3\uff0c\u6709\u671b\u4e3a\u795e\u7ecf\u573a\u666f\u8868\u793a\u4e2d\u7684\u590d\u6742\u5916\u89c2\u5efa\u6a21\u63d0\u4f9b\u7075\u6d3b\u6709\u6548\u7684\u65b9\u5411\u3002"}}
{"id": "2211.05341", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.mes-hall", "cond-mat.other", "cond-mat.stat-mech"], "pdf": "https://arxiv.org/pdf/2211.05341", "abs": "https://arxiv.org/abs/2211.05341", "authors": ["Yun-Hao Shi", "Yu Liu", "Yu-Ran Zhang", "Zhongcheng Xiang", "Kaixuan Huang", "Tao Liu", "Yong-Yi Wang", "Jia-Chi Zhang", "Cheng-Lin Deng", "Gui-Han Liang", "Zheng-Yang Mei", "Hao Li", "Tian-Ming Li", "Wei-Guo Ma", "Hao-Tian Liu", "Chi-Tong Chen", "Tong Liu", "Ye Tian", "Xiaohui Song", "S. P. Zhao", "Kai Xu", "Dongning Zheng", "Franco Nori", "Heng Fan"], "title": "Quantum simulation of topological zero modes on a 41-qubit superconducting processor", "comment": "Main text: 6 pages, 4 figures; Supplementary: 16 pages, 14 figures", "summary": "Quantum simulation of different exotic topological phases of quantum matter\non a noisy intermediate-scale quantum (NISQ) processor is attracting growing\ninterest. Here, we develop a one-dimensional 43-qubit superconducting quantum\nprocessor, named as Chuang-tzu, to simulate and characterize emergent\ntopological states. By engineering diagonal\nAubry-Andr$\\acute{\\mathrm{e}}$-Harper (AAH) models, we experimentally\ndemonstrate the Hofstadter butterfly energy spectrum. Using Floquet\nengineering, we verify the existence of the topological zero modes in the\ncommensurate off-diagonal AAH models, which have never been experimentally\nrealized before. Remarkably, the qubit number over 40 in our quantum processor\nis large enough to capture the substantial topological features of a quantum\nsystem from its complex band structure, including Dirac points, the energy\ngap's closing, the difference between even and odd number of sites, and the\ndistinction between edge and bulk states. Our results establish a versatile\nhybrid quantum simulation approach to exploring quantum topological systems in\nthe NISQ era.", "AI": {"tldr": "\u572843\u6bd4\u7279\u91cf\u5b50\u5904\u7406\u5668\u4e0a\u6a21\u62df\u4e86\u62d3\u6251\u76f8\uff0c\u6f14\u793a\u4e86Hofstadter\u8774\u8776\u80fd\u8c31\uff0c\u5e76\u9a8c\u8bc1\u4e86\u4ece\u672a\u5b9e\u9a8c\u5b9e\u73b0\u7684\u62d3\u6251\u96f6\u6a21\u3002", "motivation": "\u5728\u5608\u6742\u7684\u4e2d\u7b49\u89c4\u6a21\u91cf\u5b50\uff08NISQ\uff09\u5904\u7406\u5668\u4e0a\u8fdb\u884c\u4e0d\u540c\u5947\u5f02\u62d3\u6251\u91cf\u5b50\u7269\u6001\u7684\u91cf\u5b50\u6a21\u62df\u5f15\u8d77\u4e86\u65e5\u76ca\u589e\u957f\u7684\u5174\u8da3\u3002", "method": "\u5728\u4e00\u53f0\u5305\u542b43\u4e2a\u8d85\u5bfc\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u5904\u7406\u5668\uff08\u547d\u540d\u4e3a\u201c\u5b50\u201d\uff09\u4e0a\uff0c\u901a\u8fc7\u5de5\u7a0b\u5316\u8bbe\u8ba1\u4e00\u7ef443\u6bd4\u7279\u7684\u5bf9\u89d2\u7ebfAubry-Andr\u00e9-Harper\uff08AAH\uff09\u6a21\u578b\uff0c\u5e76\u5229\u7528Floquet\u5de5\u7a0b\uff0c\u5b9e\u9a8c\u6f14\u793a\u4e86Hofstadter\u8774\u8776\u80fd\u8c31\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5728\u6574\u6570\u6b21off-diagonal AAH\u6a21\u578b\u4e2d\u62d3\u6251\u96f6\u6a21\u7684\u5b58\u5728\u3002", "result": "\u572840\u4e2a\u4ee5\u4e0a\u7684\u91cf\u5b50\u6bd4\u7279\u7684\u91cf\u5b50\u5904\u7406\u5668\u4e0a\uff0c\u6210\u529f\u6355\u83b7\u4e86\u91cf\u5b50\u7cfb\u7edf\u7684\u590d\u6742\u80fd\u5e26\u7ed3\u6784\u7684\u62d3\u6251\u7279\u5f81\uff0c\u5305\u62ec\u72c4\u62c9\u514b\u70b9\u3001\u80fd\u9699\u95ed\u5408\u3001\u5947\u5076\u6570\u4e2a\u4f4d\u70b9\u7684\u5dee\u5f02\u4ee5\u53ca\u8fb9\u754c\u6001\u548c\u4f53\u6001\u7684\u533a\u522b\u3002", "conclusion": "\u8be5\u7814\u7a76\u7ed3\u679c\u4e3a\u5728NISQ\u65f6\u4ee3\u63a2\u7d22\u91cf\u5b50\u62d3\u6251\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u7528\u7684\u6df7\u5408\u91cf\u5b50\u6a21\u62df\u65b9\u6cd5\u3002"}}
{"id": "2508.08206", "categories": ["eess.SP", "cs.IT", "cs.LG", "math.IT", "math.OC"], "pdf": "https://arxiv.org/pdf/2508.08206", "abs": "https://arxiv.org/abs/2508.08206", "authors": ["Amirhossein Taherpour", "Abbas Taherpour", "Tamer Khattab"], "title": "Adaptive Learning for IRS-Assisted Wireless Networks: Securing Opportunistic Communications Against Byzantine Eavesdroppers", "comment": null, "summary": "We propose a joint learning framework for Byzantine-resilient spectrum\nsensing and secure intelligent reflecting surface (IRS)--assisted opportunistic\naccess under channel state information (CSI) uncertainty. The sensing stage\nperforms logit-domain Bayesian updates with trimmed aggregation and\nattention-weighted consensus, and the base station (BS) fuses network beliefs\nwith a conservative minimum rule, preserving detection accuracy under a bounded\nnumber of Byzantine users. Conditioned on the sensing outcome, we pose downlink\ndesign as sum mean-squared error (MSE) minimization under transmit-power and\nsignal-leakage constraints and jointly optimize the BS precoder, IRS phase\nshifts, and user equalizers. With partial (or known) CSI, we develop an\naugmented-Lagrangian alternating algorithm with projected updates and provide\nprovable sublinear convergence, with accelerated rates under mild local\ncurvature. With unknown CSI, we perform constrained Bayesian optimization (BO)\nin a geometry-aware low-dimensional latent space using Gaussian process (GP)\nsurrogates; we prove regret bounds for a constrained upper confidence bound\n(UCB) variant of the BO module, and demonstrate strong empirical performance of\nthe implemented procedure. Simulations across diverse network conditions show\nhigher detection probability at fixed false-alarm rate under adversarial\nattacks, large reductions in sum MSE for honest users, strong suppression of\neavesdropper signal power, and fast convergence. The framework offers a\npractical path to secure opportunistic communication that adapts to CSI\navailability while coherently coordinating sensing and transmission through\njoint learning.", "AI": {"tldr": "\"A joint learning framework for Byzantine-resilient spectrum sensing and secure IRS-assisted opportunistic access under CSI uncertainty is proposed. The framework utilizes logit-domain Bayesian updates and trimmed aggregation for sensing, and an augmented-Lagrangian alternating algorithm or constrained Bayesian optimization for downlink design, depending on CSI availability. The simulations demonstrate improved detection probability, reduced sum MSE, suppressed eavesdropper signal power, and fast convergence.\"", "motivation": "\"To propose a joint learning framework for Byzantine-resilient spectrum sensing and secure intelligent reflecting surface (IRS)--assisted opportunistic access under channel state information (CSI) uncertainty.\"", "method": "\"The sensing stage performs logit-domain Bayesian updates with trimmed aggregation and attention-weighted consensus, and the base station (BS) fuses network beliefs with a conservative minimum rule. With partial (or known) CSI, an augmented-Lagrangian alternating algorithm with projected updates is developed. With unknown CSI, constrained Bayesian optimization (BO) in a geometry-aware low-dimensional latent space using Gaussian process (GP) surrogates is performed.\"", "result": "\"Simulations show higher detection probability at fixed false-alarm rate under adversarial attacks, large reductions in sum MSE for honest users, strong suppression of eavesdropper signal power, and fast convergence.\"", "conclusion": "\"The framework offers a practical path to secure opportunistic communication that adapts to CSI availability while coherently coordinating sensing and transmission through joint learning.\""}}
{"id": "2508.06980", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06980", "abs": "https://arxiv.org/abs/2508.06980", "authors": ["Aswin Paul", "Moein Khajehnejad", "Forough Habibollahi", "Brett J. Kagan", "Adeel Razi"], "title": "Simulating Biological Intelligence: Active Inference with Experiment-Informed Generative Model", "comment": "18 pages, 8 figures", "summary": "With recent and rapid advancements in artificial intelligence (AI),\nunderstanding the foundation of purposeful behaviour in autonomous agents is\ncrucial for developing safe and efficient systems. While artificial neural\nnetworks have dominated the path to AI, recent studies are exploring the\npotential of biologically based systems, such as networks of living biological\nneuronal networks. Along with promises of high power and data efficiency, these\nsystems may also inform more explainable and biologically plausible models. In\nthis work, we propose a framework rooted in active inference, a general theory\nof behaviour, to model decision-making in embodied agents. Using\nexperiment-informed generative models, we simulate decision-making processes in\na simulated game-play environment, mirroring experimental setups that use\nbiological neurons. Our results demonstrate learning in these agents, providing\ninsights into the role of memory-based learning and predictive planning in\nintelligent decision-making. This work contributes to the growing field of\nexplainable AI by offering a biologically grounded and scalable approach to\nunderstanding purposeful behaviour in agents.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u6846\u67b6\uff0c\u4f7f\u7528\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u6a21\u62df\u51b3\u7b56\u8fc7\u7a0b\uff0c\u5c55\u793a\u4e86\u5b66\u4e60\u80fd\u529b\uff0c\u4e3a\u53ef\u89e3\u91caAI\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u7406\u89e3\u81ea\u4e3b\u667a\u80fd\u4f53\u7684\u76ee\u6807\u884c\u4e3a\u5bf9\u4e8e\u5f00\u53d1\u5b89\u5168\u6709\u6548\u7684\u7cfb\u7edf\u81f3\u5173\u91cd\u8981\uff0c\u540c\u65f6\u63a2\u7d22\u751f\u7269\u7cfb\u7edf\uff08\u5982\u751f\u7269\u795e\u7ecf\u7f51\u7edc\uff09\u5728AI\u9886\u57df\u7684\u6f5c\u529b\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u751f\u7269\u5b66\u5408\u7406\u6027\u7684\u6a21\u578b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u5b9e\u9a8c\u542f\u53d1\u5f0f\u751f\u6210\u6a21\u578b\u5728\u6a21\u62df\u6e38\u620f\u4e2d\u8fdb\u884c\u51b3\u7b56\u8fc7\u7a0b\u7684\u6a21\u62df\u3002", "result": "\u5728\u6a21\u62df\u6e38\u620f\u4e2d\u5c55\u793a\u4e86\u4eff\u751f\u4ee3\u7406\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u63ed\u793a\u4e86\u8bb0\u5fc6\u5b66\u4e60\u548c\u9884\u6d4b\u89c4\u5212\u5728\u667a\u80fd\u51b3\u7b56\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u7684\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u5b9e\u9a8c\u542f\u53d1\u5f0f\u751f\u6210\u6a21\u578b\u5728\u6a21\u62df\u6e38\u620f\u4e2d\u8fdb\u884c\u51b3\u7b56\u8fc7\u7a0b\u7684\u6a21\u62df\uff0c\u5c55\u793a\u4e86\u5728\u4eff\u751f\u4ee3\u7406\u4e2d\u5b66\u4e60\u7684\u53ef\u80fd\u6027\uff0c\u4e3a\u53ef\u89e3\u91caAI\u9886\u57df\u63d0\u4f9b\u4e86\u751f\u7269\u5b66\u57fa\u7840\u548c\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06783", "categories": ["cs.LG", "cs.AI", "cs.CR", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.06783", "abs": "https://arxiv.org/abs/2508.06783", "authors": ["Noel Teku", "Fengwei Tian", "Payel Bhattacharjee", "Souradip Chakraborty", "Amrit Singh Bedi", "Ravi Tandon"], "title": "PROPS: Progressively Private Self-alignment of Large Language Models", "comment": null, "summary": "Alignment is a key step in developing Large Language Models (LLMs) using\nhuman feedback to ensure adherence to human values and societal norms.\nDependence on human feedback raises privacy concerns about how much a labeler's\npreferences may reveal about their personal values, beliefs, and personality\ntraits. Existing approaches, such as Differentially Private SGD (DP-SGD),\nprovide rigorous privacy guarantees by privatizing gradients during fine-tuning\nand alignment but can provide more privacy than necessary as human preferences\nare tied only to labels of (prompt, response) pairs and can degrade model\nutility. This work focuses on LLM alignment with preference-level privacy,\nwhich preserves the privacy of preference labels provided by humans. We propose\nPROPS (PROgressively Private Self-alignment), a multi-stage privacy preserving\nalignment framework where privately aligned models in previous stages can serve\nas labelers for supplementing training data in the subsequent stages of\nalignment. We present theoretical guarantees for PROPS as well as comprehensive\nvalidation using multiple models (Pythia and GPT) and datasets (AlpacaEval,\nAnthropic HH-RLHF, truthy-dpo-v0.1) to demonstrate the utility of PROPS over\nexisting methods while still providing high privacy. For the same privacy\nbudget, alignment via PROPS can achieve up to 3x higher win-rates compared to\nDP-SGD, and 2.5x higher win-rates compared to Randomized Response (RR) based\nalignment.", "AI": {"tldr": "PROPS\u662f\u4e00\u79cd\u65b0\u7684LLM\u5bf9\u9f50\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u9636\u6bb5\u7684\u79c1\u6709\u5bf9\u9f50\u6a21\u578b\u4f5c\u4e3a\u6807\u7b7e\u751f\u6210\u5668\uff0c\u5b9e\u73b0\u4e86\u504f\u597d\u7ea7\u9690\u79c1\u4fdd\u62a4\uff0c\u5e76\u5728\u9690\u79c1\u548c\u6548\u7528\u4e0a\u4f18\u4e8eDP-SGD\u548cRR\u7b49\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u9f50\u4e2d\uff0c\u4eba\u7c7b\u53cd\u9988\u53ef\u80fd\u5f15\u53d1\u7684\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u5728\u4e0d\u727a\u7272\u6a21\u578b\u6548\u7528\u7684\u524d\u63d0\u4e0b\uff0c\u4fdd\u62a4\u504f\u597d\u6807\u7b7e\u9690\u79c1\u7684\u5bf9\u9f50\u65b9\u6cd5\u3002", "method": "PROPS (PROgressively Private Self-alignment)\u6846\u67b6\uff0c\u4e00\u79cd\u591a\u9636\u6bb5\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u65b9\u6cd5\uff0c\u5176\u4e2d\u5148\u524d\u9636\u6bb5\u7684\u79c1\u6709\u5bf9\u9f50\u6a21\u578b\u88ab\u7528\u4f5c\u540e\u7eed\u9636\u6bb5\u7684\u6807\u7b7e\u751f\u6210\u5668\u3002", "result": "PROPS\u5728\u591a\u4e2a\u6a21\u578b\uff08Pythia\u548cGPT\uff09\u548c\u6570\u636e\u96c6\uff08AlpacaEval\u3001Anthropic HH-RLHF\u3001truthy-dpo-v0.1\uff09\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u4f9b\u9ad8\u9690\u79c1\u6027\u7684\u540c\u65f6\uff0c\u76f8\u6bd4DP-SGD\u548cRR\u5bf9\u9f50\u5177\u6709\u66f4\u9ad8\u7684\u6a21\u578b\u6548\u7528\uff0c\u5177\u4f53\u8868\u73b0\u5728\u66f4\u9ad8\u7684\u80dc\u7387\u3002", "conclusion": "PROPS (PROgressively Private Self-alignment)\u662f\u4e00\u4e2a\u591a\u9636\u6bb5\u7684\u9690\u79c1\u4fdd\u62a4\u5bf9\u9f50\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5141\u8bb8\u5148\u524d\u9636\u6bb5\u7684\u79c1\u6709\u5bf9\u9f50\u6a21\u578b\u4f5c\u4e3a\u540e\u7eed\u9636\u6bb5\u7684\u6807\u6ce8\u8005\u6765\u8865\u5145\u8bad\u7ec3\u6570\u636e\uff0c\u5b9e\u73b0\u4e86\u504f\u597d\u7ea7\u9690\u79c1\u4fdd\u62a4\u3002\u8be5\u6846\u67b6\u5728\u4fdd\u6301\u9ad8\u9690\u79c1\u6027\u7684\u540c\u65f6\uff0c\u5728\u6a21\u578b\u6548\u7528\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728\u76f8\u540c\u7684\u9690\u79c1\u9884\u7b97\u4e0b\uff0cPROPS\u5bf9\u9f50\u76f8\u6bd4DP-SGD\u548c\u57fa\u4e8e\u968f\u673a\u54cd\u5e94(RR)\u7684\u5bf9\u9f50\uff0c\u5176\u80dc\u7387\u5206\u522b\u9ad8\u51fa3\u500d\u548c2.5\u500d\u3002"}}
{"id": "2508.07521", "categories": ["cond-mat.mtrl-sci", "cond-mat.other", "quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07521", "abs": "https://arxiv.org/abs/2508.07521", "authors": ["Xiaoliang Zhang", "Haechan Park"], "title": "Spin Phonon Coupling and Relaxation time in Lu(II) compound with 9.2GHz clock transition", "comment": "11 pages, 5 figures", "summary": "Electron spin qubits operating at atomic clock transitions exhibit\nexceptionally long coherence times, making them promising candidates for\nscalable quantum information applications. In solid-state systems, interactions\nbetween qubits and lattice phonons are known to play a critical role in spin\nrelaxation (T1) and decoherence (T2). In this work, we perform first-principles\ncalculations on a Lu(II) complex spin qubit featuring a prominent clock\ntransition. By employing advanced electronic structure methods, we\nquantitatively evaluate the influence of phonons on the hyperfine interaction,\nwhich serves as the primary spin-lattice coupling mechanism. Treating these\nphonon-induced variations as first-order perturbations, we apply the Redfield\nmaster equation to compute both T1 and T2, along with their temperature\ndependencies. For T1, we adopt a second quantization formalism to describe\nphonon interactions, while T2 is evaluated by explicitly integrating acoustic\nphonon contributions across the full Brillouin zone. Our results reproduce the\nexperimentally observed magnetic field dependence of T2, including the\ncoherence peak near 0.43 T, though the absolute values of T1 and T2 differ by\none to two orders of magnitude. Analysis reveals that T1 is primarily governed\nby longitudinal phonons, whereas T2 is most strongly influenced by\nmid-wavelength, mid-energy acoustic modes. These findings provide a\nquantitative demonstration of the clock transition protective effect on spin\nqubit coherence and offer a transferable computational framework for evaluating\nspin-phonon interactions in other molecular spin qubits.", "AI": {"tldr": "\u672c\u7814\u7a76\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\u548cRedfield\u4e3b\u65b9\u7a0b\uff0c\u5206\u6790\u4e86\u58f0\u5b50\u5bf9Lu(II)\u914d\u5408\u7269\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684T1\u548cT2\u7684\u5f71\u54cd\uff0c\u9a8c\u8bc1\u4e86\u539f\u5b50\u949f\u8dc3\u8fc1\u5bf9\u76f8\u5e72\u6027\u7684\u4fdd\u62a4\u4f5c\u7528\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u901a\u7528\u7684\u8ba1\u7b97\u6846\u67b6\u3002\u7814\u7a76\u7ed3\u679c\u4e0e\u5b9e\u9a8c\u73b0\u8c61\u5b9a\u6027\u7b26\u5408\uff0c\u4f46T1\u548cT2\u7684\u7edd\u5bf9\u503c\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u7535\u5b50\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u56e0\u5176\u5728\u539f\u5b50\u949f\u8dc3\u8fc1\u4e0b\u5177\u6709\u6781\u957f\u7684\u76f8\u5e72\u65f6\u95f4\uff0c\u5728\u53ef\u6269\u5c55\u7684\u91cf\u5b50\u4fe1\u606f\u5e94\u7528\u4e2d\u663e\u793a\u51fa\u6f5c\u529b\u3002\u7136\u800c\uff0c\u5728\u56fa\u6001\u7cfb\u7edf\u4e2d\uff0c\u91cf\u5b50\u6bd4\u7279\u4e0e\u6676\u683c\u58f0\u5b50\u7684\u76f8\u4e92\u4f5c\u7528\u5bf9\u81ea\u65cb\u5f1b\u8c6b\uff08T1\uff09\u548c\u9000\u76f8\u5e72\uff08T2\uff09\u8d77\u7740\u5173\u952e\u4f5c\u7528\u3002\u672c\u7814\u7a76\u65e8\u5728\u91cf\u5316\u58f0\u5b50\u5bf9\u7279\u5b9a\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\uff08Lu(II)\u914d\u5408\u7269\uff09\u7684\u8d85\u7cbe\u76f8\u4e92\u4f5c\u7528\u7684\u5f71\u54cd\uff0c\u5e76\u7406\u89e3\u5176\u5bf9T1\u548cT2\u7684\u5f71\u54cd\uff0c\u4ee5\u9a8c\u8bc1\u539f\u5b50\u949f\u8dc3\u8fc1\u7684\u4fdd\u62a4\u6548\u5e94\u3002", "method": "\u672c\u7814\u7a76\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7ed3\u5408\u5148\u8fdb\u7684\u7535\u5b50\u7ed3\u6784\u65b9\u6cd5\uff0c\u91cf\u5316\u4e86\u58f0\u5b50\u5bf9\u8d85\u7cbe\u76f8\u4e92\u4f5c\u7528\uff08\u4e3b\u8981\u7684\u81ea\u65cb-\u6676\u683c\u8026\u5408\u673a\u5236\uff09\u7684\u5f71\u54cd\u3002\u7814\u7a76\u5c06\u58f0\u5b50\u8bf1\u5bfc\u7684\u53d8\u5f02\u89c6\u4e3a\u4e00\u9636\u5fae\u6270\uff0c\u5e76\u5e94\u7528Redfield\u4e3b\u65b9\u7a0b\u8ba1\u7b97\u4e86T1\u548cT2\u53ca\u5176\u6e29\u5ea6\u4f9d\u8d56\u6027\u3002T1\u7684\u8ba1\u7b97\u91c7\u7528\u4e86\u63cf\u8ff0\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u7684\u4e8c\u9636\u91cf\u5316\u5f62\u5f0f\uff0c\u800cT2\u7684\u8bc4\u4f30\u5219\u901a\u8fc7\u5bf9\u6574\u4e2a\u5e03\u91cc\u6e0a\u533a\u7684\u58f0\u5b50\u8d21\u732e\u8fdb\u884c\u79ef\u5206\u6765\u5b9e\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u6210\u529f\u590d\u73b0\u4e86\u5b9e\u9a8c\u89c2\u5bdf\u5230\u7684T2\u968f\u78c1\u573a\u53d8\u5316\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5305\u62ec\u57280.43 T\u9644\u8fd1\u7684\u76f8\u5e72\u5cf0\u503c\u3002\u7136\u800c\uff0c\u8ba1\u7b97\u5f97\u5230\u7684T1\u548cT2\u7684\u7edd\u5bf9\u503c\u4e0e\u5b9e\u9a8c\u503c\u5b58\u5728\u4e00\u5230\u4e24\u4e2a\u6570\u91cf\u7ea7\u7684\u5dee\u5f02\u3002\u5206\u6790\u8868\u660e\uff0cT1\u4e3b\u8981\u53d7\u7eb5\u5411\u58f0\u5b50\u5f71\u54cd\uff0c\u800cT2\u5219\u4e3b\u8981\u53d7\u4e2d\u7b49\u6ce2\u957f\u3001\u4e2d\u7b49\u80fd\u91cf\u7684\u58f0\u5b66\u6a21\u5f0f\u5f71\u54cd\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u8ba1\u7b97\u6a21\u62df\uff0c\u5b9a\u91cf\u9a8c\u8bc1\u4e86\u539f\u5b50\u949f\u8dc3\u8fc1\u5bf9\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u7684\u4fdd\u62a4\u6548\u5e94\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u53ef\u8fc1\u79fb\u7684\u8ba1\u7b97\u6846\u67b6\u7528\u4e8e\u8bc4\u4f30\u5206\u5b50\u81ea\u65cb\u91cf\u5b50\u6bd4\u7279\u4e2d\u7684\u81ea\u65cb-\u58f0\u5b50\u76f8\u4e92\u4f5c\u7528\u3002"}}
{"id": "2508.07244", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07244", "abs": "https://arxiv.org/abs/2508.07244", "authors": ["Ayesha Jena", "Stefan Reitmann", "Elin Anna Topp"], "title": "Impact of Gaze-Based Interaction and Augmentation on Human-Robot Collaboration in Critical Tasks", "comment": null, "summary": "We present a user study analyzing head-gaze-based robot control and foveated\nvisual augmentation in a simulated search-and-rescue task. Results show that\nfoveated augmentation significantly improves task performance, reduces\ncognitive load by 38%, and shortens task time by over 60%. Head-gaze patterns\nanalysed over both the entire task duration and shorter time segments show that\nnear and far attention capture is essential to better understand user intention\nin critical scenarios. Our findings highlight the potential of foveation as an\naugmentation technique and the need to further study gaze measures to leverage\nthem during critical tasks.", "AI": {"tldr": "Foveated vision helps robots in search-and-rescue by improving performance and reducing user effort, and studying gaze patterns is key for better control.", "motivation": "The study aimed to analyze head-gaze-based robot control and foveated visual augmentation in a simulated search-and-rescue task to understand their impact on performance and user experience.", "method": "The study involved a user experiment analyzing head-gaze-based robot control and foveated visual augmentation in a simulated search-and-rescue task.", "result": "Foveated augmentation significantly improved task performance, reduced cognitive load by 38%, and shortened task time by over 60%. Analysis of head-gaze patterns revealed the importance of near and far attention capture for understanding user intention in critical scenarios.", "conclusion": "The study highlights the potential of foveation as an augmentation technique and the need for further research into gaze measures for leveraging them in critical tasks."}}
{"id": "2508.07359", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07359", "abs": "https://arxiv.org/abs/2508.07359", "authors": ["Yibo Chen", "Zirui Sheng", "Weitang Li", "Yong Zhang", "Xun Xu", "Jun-Han Huang", "Yuxiang Li"], "title": "Integrating Quantum Computing with Multiconfiguration Pair-Density Functional Theory for Biological Electron Transfer", "comment": "16 pages, 7 figures", "summary": "Accurate calculation of strongly correlated electronic systems requires\nproper treatment of both static and dynamic correlations, which remains\nchallenging for conventional methods. To address this, we present VQE-PDFT, a\nquantum-classical hybrid framework that integrates variational quantum\neigensolver with multiconfiguration pair-density functional theory (MC-PDFT).\nThis framework strategically employs quantum circuits for multiconfigurational\nwavefunction representation while utilizing density functionals for correlation\nenergy evaluation. The hybrid strategy maintains accurate treatment of static\nand dynamic correlations while reducing quantum resource requirements.\nBenchmark validation on the Charge-Transfer dataset confirmed that VQE-PDFT\nachieves results comparable to conventional MC-PDFT. Building upon this, we\ndeveloped shallow-depth hardware-efficient ansatz circuits and integrated them\ninto a QM/MM multiscale architecture to enable applications in complex\nbiological systems. This extended framework, when applied to electron transfer\nin the European robin cryptochrome protein ErCRY4, yielded transfer rates that\nalign well with experimental measurements. Importantly, successful execution on\nactual quantum hardware demonstrates practical feasibility for biological\nquantum computing applications, supported by comprehensive error analysis.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVQE-PDFT\u7684\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u65b9\u6cd5\uff0c\u7528\u4e8e\u7cbe\u786e\u8ba1\u7b97\u5f3a\u5173\u8054\u7535\u5b50\u7cfb\u7edf\u3002\u8be5\u65b9\u6cd5\u5728\u751f\u7269\u7cfb\u7edf\uff08\u5982ErCRY4\u86cb\u767d\uff09\u4e2d\u7684\u5e94\u7528\u53d6\u5f97\u4e86\u4e0e\u5b9e\u9a8c\u76f8\u7b26\u7684\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u751f\u7269\u91cf\u5b50\u8ba1\u7b97\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u96be\u4ee5\u7cbe\u786e\u8ba1\u7b97\u5f3a\u5173\u8054\u7535\u5b50\u7cfb\u7edf\uff0c\u56e0\u4e3a\u9700\u8981\u540c\u65f6\u5904\u7406\u9759\u6001\u548c\u52a8\u6001\u76f8\u5173\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u6df7\u5408\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u53d8\u5206\u91cf\u5b50\u672c\u5f81\u6c42\u89e3\u5668\uff08VQE\uff09\u548c\u591a\u6784\u578b\u5bf9\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\uff08MC-PDFT\uff09\u7684\u91cf\u5b50-\u7ecf\u5178\u6df7\u5408\u6846\u67b6VQE-PDFT\u3002\u8be5\u6846\u67b6\u4f7f\u7528\u91cf\u5b50\u7535\u8def\u8868\u793a\u591a\u6784\u578b\u6ce2\u51fd\u6570\uff0c\u5e76\u5229\u7528\u5bc6\u5ea6\u6cdb\u51fd\u8bc4\u4f30\u76f8\u5173\u80fd\uff0c\u4ee5\u51c6\u786e\u5904\u7406\u9759\u6001\u548c\u52a8\u6001\u76f8\u5173\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u91cf\u5b50\u8d44\u6e90\u9700\u6c42\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u6d45\u5c42\u786c\u4ef6\u9ad8\u6548ansatz\u7535\u8def\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230QM/MM\u591a\u5c3a\u5ea6\u67b6\u6784\u4e2d\uff0c\u7528\u4e8e\u590d\u6742\u751f\u7269\u7cfb\u7edf\u3002", "result": "VQE-PDFT\u5728\u7535\u8377\u8f6c\u79fb\u6570\u636e\u96c6\u4e0a\u7684\u57fa\u51c6\u6d4b\u8bd5\u7ed3\u679c\u4e0e\u4f20\u7edfMC-PDFT\u76f8\u5f53\u3002\u5c06\u5176\u5e94\u7528\u4e8e\u6b27\u6d32\u77e5\u66f4\u9e1f\u9690\u8272\u7d20\u86cb\u767dErCRY4\u7684\u7535\u5b50\u8f6c\u79fb\uff0c\u8ba1\u7b97\u51fa\u7684\u8f6c\u79fb\u901f\u7387\u4e0e\u5b9e\u9a8c\u6d4b\u91cf\u503c\u543b\u5408\u826f\u597d\u3002", "conclusion": "VQE-PDFT\u6846\u67b6\u5728\u91cf\u5b50\u8ba1\u7b97\u786c\u4ef6\u4e0a\u6210\u529f\u5e94\u7528\u4e8e\u751f\u7269\u7cfb\u7edf\uff0c\u8ba1\u7b97\u51fa\u7684\u7535\u5b50\u8f6c\u79fb\u901f\u7387\u4e0e\u5b9e\u9a8c\u7ed3\u679c\u4e00\u81f4\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u751f\u7269\u91cf\u5b50\u8ba1\u7b97\u9886\u57df\u7684\u5b9e\u9645\u53ef\u884c\u6027\u3002"}}
{"id": "2508.07090", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07090", "abs": "https://arxiv.org/abs/2508.07090", "authors": ["Aditya Tomar", "Nihar Ranjan Sahoo", "Pushpak Bhattacharyya"], "title": "BharatBBQ: A Multilingual Bias Benchmark for Question Answering in the Indian Context", "comment": null, "summary": "Evaluating social biases in language models (LMs) is crucial for ensuring\nfairness and minimizing the reinforcement of harmful stereotypes in AI systems.\nExisting benchmarks, such as the Bias Benchmark for Question Answering (BBQ),\nprimarily focus on Western contexts, limiting their applicability to the Indian\ncontext. To address this gap, we introduce BharatBBQ, a culturally adapted\nbenchmark designed to assess biases in Hindi, English, Marathi, Bengali, Tamil,\nTelugu, Odia, and Assamese. BharatBBQ covers 13 social categories, including 3\nintersectional groups, reflecting prevalent biases in the Indian sociocultural\nlandscape. Our dataset contains 49,108 examples in one language that are\nexpanded using translation and verification to 392,864 examples in eight\ndifferent languages. We evaluate five multilingual LM families across zero and\nfew-shot settings, analyzing their bias and stereotypical bias scores. Our\nfindings highlight persistent biases across languages and social categories and\noften amplified biases in Indian languages compared to English, demonstrating\nthe necessity of linguistically and culturally grounded benchmarks for bias\nevaluation.", "AI": {"tldr": "\u4e3a\u8bc4\u4f30\u5370\u5ea6\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u793e\u4f1a\u504f\u89c1\uff0c\u5f00\u53d1\u4e86BharatBBQ\u57fa\u51c6\uff0c\u53d1\u73b0\u5728\u5370\u5ea6\u8bed\u8a00\u4e2d\u504f\u89c1\u6bd4\u82f1\u8bed\u66f4\u4e25\u91cd\u3002", "motivation": "\u73b0\u6709\u504f\u89c1\u8bc4\u4f30\u57fa\u51c6\uff08\u5982BBQ\uff09\u4e3b\u8981\u5173\u6ce8\u897f\u65b9\u80cc\u666f\uff0c\u4e0d\u9002\u7528\u4e8e\u5370\u5ea6\u7b49\u7279\u5b9a\u6587\u5316\u548c\u793e\u4f1a\u73af\u5883\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u53cd\u6620\u5370\u5ea6\u793e\u4f1a\u6587\u5316\u73b0\u5b9e\u7684\u57fa\u51c6\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aBharatBBQ\u7684\u591a\u8bed\u8a00\u3001\u591a\u6587\u5316\u9002\u5e94\u6027\u57fa\u51c6\uff0c\u6db5\u76d613\u4e2a\u793e\u4f1a\u7c7b\u522b\uff08\u5305\u62ec3\u4e2a\u4ea4\u53c9\u7fa4\u4f53\uff09\uff0c\u5305\u542b\u8fd15\u4e07\u4e2a\u521d\u59cb\u793a\u4f8b\uff0c\u5e76\u6269\u5c55\u52308\u79cd\u5370\u5ea6\u8bed\u8a00\u7684\u8fd140\u4e07\u4e2a\u793a\u4f8b\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u504f\u89c1\u548c\u523b\u677f\u5370\u8c61\u504f\u89c1\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6240\u6709\u88ab\u8bc4\u4f30\u7684\u4e94\u79cd\u591a\u8bed\u8a00\u8bed\u8a00\u6a21\u578b\u5bb6\u65cf\u5728\u6240\u6709\u8bed\u8a00\u548c\u793e\u4f1a\u7c7b\u522b\u4e2d\u90fd\u5b58\u5728\u6301\u7eed\u7684\u504f\u89c1\uff0c\u5e76\u4e14\u5728\u5370\u5ea6\u8bed\u8a00\u4e2d\u504f\u89c1\u6bd4\u82f1\u8bed\u66f4\u4e3a\u4e25\u91cd\uff0c\u8bc1\u660e\u4e86\u7279\u5b9a\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u8bc4\u4f30\u57fa\u51c6\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u5728\u5370\u5ea6\u8bed\u8a00\u548c\u6587\u5316\u80cc\u666f\u4e0b\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u504f\u89c1\u7684\u91cd\u8981\u6027\uff0c\u5e76\u53d1\u73b0\u4e0e\u82f1\u8bed\u76f8\u6bd4\uff0c\u5370\u5ea6\u8bed\u8a00\u4e2d\u7684\u504f\u89c1\u666e\u904d\u5b58\u5728\u4e14\u6709\u6240\u52a0\u5267\uff0c\u56e0\u6b64\u9700\u8981\u7279\u5b9a\u8bed\u8a00\u548c\u6587\u5316\u7684\u8bc4\u4f30\u57fa\u51c6\u3002"}}
{"id": "2508.06640", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06640", "abs": "https://arxiv.org/abs/2508.06640", "authors": ["Zheyuan Zhang", "Weihao Tang", "Hong Chen"], "title": "Rethinking Key-frame-based Micro-expression Recognition: A Robust and Accurate Framework Against Key-frame Errors", "comment": null, "summary": "Micro-expression recognition (MER) is a highly challenging task in affective\ncomputing. With the reduced-sized micro-expression (ME) input that contains key\ninformation based on key-frame indexes, key-frame-based methods have\nsignificantly improved the performance of MER. However, most of these methods\nfocus on improving the performance with relatively accurate key-frame indexes,\nwhile ignoring the difficulty of obtaining accurate key-frame indexes and the\nobjective existence of key-frame index errors, which impedes them from moving\ntowards practical applications. In this paper, we propose CausalNet, a novel\nframework to achieve robust MER facing key-frame index errors while maintaining\naccurate recognition. To enhance robustness, CausalNet takes the representation\nof the entire ME sequence as the input. To address the information redundancy\nbrought by the complete ME range input and maintain accurate recognition,\nfirst, the Causal Motion Position Learning Module (CMPLM) is proposed to help\nthe model locate the muscle movement areas related to Action Units (AUs),\nthereby reducing the attention to other redundant areas. Second, the Causal\nAttention Block (CAB) is proposed to deeply learn the causal relationships\nbetween the muscle contraction and relaxation movements in MEs. Empirical\nexperiments have demonstrated that on popular ME benchmarks, the CausalNet has\nachieved robust MER under different levels of key-frame index noise. Meanwhile,\nit has surpassed state-of-the-art (SOTA) methods on several standard MER\nbenchmarks when using the provided annotated key-frames. Code is available at\nhttps://github.com/tony19980810/CausalNet.", "AI": {"tldr": "CausalNet\u901a\u8fc7\u5904\u7406\u6574\u4e2a\u5fae\u8868\u60c5\u5e8f\u5217\u5e76\u5229\u7528CMPLM\u548cCAB\u6a21\u5757\uff0c\u89e3\u51b3\u4e86\u5173\u952e\u5e27\u7d22\u5f15\u9519\u8bef\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5fae\u8868\u60c5\u8bc6\u522b\u7684\u9c81\u68d2\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u83b7\u5f97\u51c6\u786e\u7684\u5173\u952e\u5e27\u7d22\u5f15\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u5ffd\u7565\u4e86\u5173\u952e\u5e27\u7d22\u5f15\u9519\u8bef\u5bf9\u5fae\u8868\u60c5\u8bc6\u522b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u8fd9\u963b\u788d\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u63a8\u5e7f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCausalNet\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u6574\u4e2a\u5fae\u8868\u60c5\u5e8f\u5217\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u901a\u8fc7\u56e0\u679c\u8fd0\u52a8\u4f4d\u7f6e\u5b66\u4e60\u6a21\u5757\uff08CMPLM\uff09\u548c\u56e0\u679c\u6ce8\u610f\u529b\u5757\uff08CAB\uff09\u6765\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u8bc6\u522b\u7cbe\u5ea6\u3002", "result": "CausalNet\u5728\u4e0d\u540c\u7a0b\u5ea6\u7684\u5173\u952e\u5e27\u7d22\u5f15\u566a\u58f0\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5fae\u8868\u60c5\u8bc6\u522b\uff0c\u5e76\u5728\u591a\u4e2a\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002", "conclusion": "CausalNet\u5728\u5b58\u5728\u5173\u952e\u5e27\u7d22\u5f15\u9519\u8bef\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u9c81\u68d2\u7684\u5fae\u8868\u60c5\u8bc6\u522b\uff0c\u5e76\u4e14\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6280\u672f\u6c34\u5e73\u3002"}}
{"id": "2508.06951", "categories": ["cs.CV", "eess.IV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.06951", "abs": "https://arxiv.org/abs/2508.06951", "authors": ["Harry Walsh", "Ed Fish", "Ozge Mercanoglu Sincan", "Mohamed Ilyes Lakhal", "Richard Bowden", "Neil Fox", "Bencie Woll", "Kepeng Wu", "Zecheng Li", "Weichao Zhao", "Haodong Wang", "Wengang Zhou", "Houqiang Li", "Shengeng Tang", "Jiayi He", "Xu Wang", "Ruobei Zhang", "Yaxiong Wang", "Lechao Cheng", "Meryem Tasyurek", "Tugce Kiziltepe", "Hacer Yalim Keles"], "title": "SLRTP2025 Sign Language Production Challenge: Methodology, Results, and Future Work", "comment": "11 pages, 6 Figures, CVPR conference", "summary": "Sign Language Production (SLP) is the task of generating sign language video\nfrom spoken language inputs. The field has seen a range of innovations over the\nlast few years, with the introduction of deep learning-based approaches\nproviding significant improvements in the realism and naturalness of generated\noutputs. However, the lack of standardized evaluation metrics for SLP\napproaches hampers meaningful comparisons across different systems. To address\nthis, we introduce the first Sign Language Production Challenge, held as part\nof the third SLRTP Workshop at CVPR 2025. The competition's aims are to\nevaluate architectures that translate from spoken language sentences to a\nsequence of skeleton poses, known as Text-to-Pose (T2P) translation, over a\nrange of metrics. For our evaluation data, we use the\nRWTH-PHOENIX-Weather-2014T dataset, a German Sign Language - Deutsche\nGebardensprache (DGS) weather broadcast dataset. In addition, we curate a\ncustom hidden test set from a similar domain of discourse. This paper presents\nthe challenge design and the winning methodologies. The challenge attracted 33\nparticipants who submitted 231 solutions, with the top-performing team\nachieving BLEU-1 scores of 31.40 and DTW-MJE of 0.0574. The winning approach\nutilized a retrieval-based framework and a pre-trained language model. As part\nof the workshop, we release a standardized evaluation network, including\nhigh-quality skeleton extraction-based keypoints establishing a consistent\nbaseline for the SLP field, which will enable future researchers to compare\ntheir work against a broader range of methods.", "AI": {"tldr": "\u9996\u4e2a\u624b\u8bed\u751f\u6210\u6311\u6218\u8d5b\u6210\u529f\u4e3e\u529e\uff0c\u65e8\u5728\u89e3\u51b3\u7f3a\u4e4f\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\u7684\u95ee\u9898\u3002\u6311\u6218\u8d5b\u7684\u83b7\u80dc\u65b9\u6cd5\u91c7\u7528\u4e86\u68c0\u7d22\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u53d1\u5e03\u4e86\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\u548c\u6bd4\u8f83\u3002", "motivation": "\u624b\u8bed\u751f\u6210\uff08SLP\uff09\u9886\u57df\u7f3a\u4e4f\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u963b\u788d\u4e86\u4e0d\u540c\u7cfb\u7edf\u95f4\u7684\u6709\u610f\u4e49\u6bd4\u8f83\u3002", "method": "\u901a\u8fc7\u4e3e\u529e\u624b\u8bed\u751f\u6210\u6311\u6218\u8d5b\uff0c\u4f7f\u7528RWTH-PHOENIX-Weather-2014T\u6570\u636e\u96c6\u548c\u81ea\u5b9a\u4e49\u7684\u9690\u85cf\u6d4b\u8bd5\u96c6\uff0c\u8bc4\u4f30\u5c06\u53e3\u8bed\u7ffb\u8bd1\u6210\u9aa8\u9abc\u59ff\u6001\u5e8f\u5217\uff08Text-to-Pose\uff09\u7684\u6a21\u578b\u3002\u8bc4\u4f30\u6307\u6807\u5305\u62ecBLEU-1\u548cDTW-MJE\u3002", "result": "\u6311\u6218\u8d5b\u670933\u4e2a\u56e2\u961f\u53c2\u4e0e\uff0c\u63d0\u4ea4\u4e86231\u4e2a\u89e3\u51b3\u65b9\u6848\u3002\u83b7\u80dc\u56e2\u961f\u53d6\u5f97\u4e86BLEU-1\u5f97\u5206\u4e3a31.40\uff0cDTW-MJE\u4e3a0.0574\u3002\u53d1\u5e03\u4e86\u5305\u542b\u5173\u952e\u70b9\u63d0\u53d6\u7684\u6807\u51c6\u5316\u8bc4\u4f30\u7f51\u7edc\u3002", "conclusion": "\u8be5\u8bba\u6587\u4ecb\u7ecd\u4e86\u9996\u4e2a\u624b\u8bed\u751f\u6210\u6311\u6218\u8d5b\uff0c\u65e8\u5728\u901a\u8fc7\u5f15\u5165\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u6307\u6807\u6765\u4fc3\u8fdb\u8be5\u9886\u57df\u7684\u6bd4\u8f83\u7814\u7a76\u3002\u6311\u6218\u8d5b\u5438\u5f15\u4e86\u5927\u91cf\u53c2\u4e0e\u8005\uff0c\u5e76\u5c55\u793a\u4e86\u57fa\u4e8e\u68c0\u7d22\u548c\u9884\u8bad\u7ec3\u8bed\u8a00\u6a21\u578b\u7684\u83b7\u80dc\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u53d1\u5e03\u4e86\u4e00\u4e2a\u6807\u51c6\u5316\u7684\u8bc4\u4f30\u7f51\u7edc\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u57fa\u7ebf\u3002"}}
{"id": "2508.06784", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06784", "abs": "https://arxiv.org/abs/2508.06784", "authors": ["Junjing Zheng", "Chengliang Song", "Weidong Jiang", "Xinyu Zhang"], "title": "Mode-Aware Non-Linear Tucker Autoencoder for Tensor-based Unsupervised Learning", "comment": null, "summary": "High-dimensional data, particularly in the form of high-order tensors,\npresents a major challenge in self-supervised learning. While MLP-based\nautoencoders (AE) are commonly employed, their dependence on flattening\noperations exacerbates the curse of dimensionality, leading to excessively\nlarge model sizes, high computational overhead, and challenging optimization\nfor deep structural feature capture. Although existing tensor networks\nalleviate computational burdens through tensor decomposition techniques, most\nexhibit limited capability in learning non-linear relationships. To overcome\nthese limitations, we introduce the Mode-Aware Non-linear Tucker Autoencoder\n(MA-NTAE). MA-NTAE generalized classical Tucker decomposition to a non-linear\nframework and employs a Pick-and-Unfold strategy, facilitating flexible\nper-mode encoding of high-order tensors via recursive unfold-encode-fold\noperations, effectively integrating tensor structural priors. Notably, MA-NTAE\nexhibits linear growth in computational complexity with tensor order and\nproportional growth with mode dimensions. Extensive experiments demonstrate\nMA-NTAE's performance advantages over standard AE and current tensor networks\nin compression and clustering tasks, which become increasingly pronounced for\nhigher-order, higher-dimensional tensors.", "AI": {"tldr": "MA-NTAE\u901a\u8fc7\u975e\u7ebf\u6027Tucker\u5206\u89e3\u548cPick-and-Unfold\u7b56\u7565\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u9ad8\u7ef4\u5f20\u91cf\u81ea\u76d1\u7763\u5b66\u4e60\u4e2d\u7684\u7ef4\u5ea6\u707e\u96be\u548c\u975e\u7ebf\u6027\u5173\u7cfb\u5b66\u4e60\u80fd\u529b\u6709\u9650\u7684\u95ee\u9898\uff0c\u5728\u538b\u7f29\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eMLP\u7684\u81ea\u7f16\u7801\u5668\uff08AE\uff09\u5728\u5904\u7406\u9ad8\u7ef4\u5f20\u91cf\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u5c55\u5e73\u64cd\u4f5c\u52a0\u5267\u4e86\u7ef4\u5ea6\u707e\u96be\uff0c\u5bfc\u81f4\u6a21\u578b\u89c4\u6a21\u8fc7\u5927\u3001\u8ba1\u7b97\u5f00\u9500\u9ad8\uff0c\u5e76\u4e14\u96be\u4ee5\u4f18\u5316\u4ee5\u6355\u6349\u6df1\u5ea6\u7ed3\u6784\u7279\u5f81\u3002\u73b0\u6709\u7684\u5f20\u91cf\u7f51\u7edc\u867d\u7136\u901a\u8fc7\u5f20\u91cf\u5206\u89e3\u6280\u672f\u51cf\u8f7b\u4e86\u8ba1\u7b97\u8d1f\u62c5\uff0c\u4f46\u5b83\u4eec\u5b66\u4e60\u975e\u7ebf\u6027\u5173\u7cfb\u7684\u80fd\u529b\u6709\u9650\u3002", "method": "MA-NTAE\u662f\u4e00\u79cd\u5e7f\u4e49\u5316\u7684Tucker\u5206\u89e3\uff0c\u5177\u6709\u975e\u7ebf\u6027\u6846\u67b6\uff0c\u5e76\u91c7\u7528Pick-and-Unfold\u7b56\u7565\uff0c\u901a\u8fc7\u9012\u5f52\u5c55\u5f00-\u7f16\u7801-\u6298\u53e0\u64cd\u4f5c\uff0c\u5b9e\u73b0\u5bf9\u9ad8\u9636\u5f20\u91cf\u7684\u7075\u6d3b\u7684\u6bcf\u6a21\u5f0f\u7f16\u7801\uff0c\u6709\u6548\u6574\u5408\u4e86\u5f20\u91cf\u7ed3\u6784\u5148\u9a8c\u3002", "result": "MA-NTAE\u5728\u538b\u7f29\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5176\u8ba1\u7b97\u590d\u6742\u5ea6\u4e0e\u5f20\u91cf\u9636\u6570\u5448\u7ebf\u6027\u589e\u957f\uff0c\u4e0e\u6a21\u5f0f\u7ef4\u5ea6\u5448\u6bd4\u4f8b\u589e\u957f\u3002", "conclusion": "MA-NTAE\u5728\u538b\u7f29\u548c\u805a\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u4e8e\u6807\u51c6AE\u548c\u73b0\u6709\u5f20\u91cf\u7f51\u7edc\u7684\u6027\u80fd\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u66f4\u9ad8\u9636\u3001\u66f4\u9ad8\u7ef4\u5ea6\u7684\u5f20\u91cf\u65f6\uff0c\u8fd9\u79cd\u4f18\u52bf\u66f4\u52a0\u660e\u663e\u3002"}}
{"id": "2508.07530", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07530", "abs": "https://arxiv.org/abs/2508.07530", "authors": ["Adam D. Alfieri", "Swarnendu Das", "Kim Kisslinger", "Chloe Leblanc", "Jamie Ford", "Cherie R. Kagan", "Eric A. Stach", "Deep Jariwala"], "title": "Hydrazine-Free Precursor for Solution-Processed All-Inorganic Se and Se1-xTex Photovoltaics", "comment": null, "summary": "Selenium (Se) has reemerged as a promising absorber material for indoor and\ntandem photovoltaics (PVs), and its alloys with Te (Se1-xTex) offer a widely\ntunable bandgap. Solution processing of this materials system offers a route to\nlow-cost fabrication. However, solution processing of Se has, thus far, only\nused hydrazine, which is an extremely hazardous solvent. In this work, we\nprepare and isolate propylammonium poly-Se and poly-Se-Te precursors from a\nsafer thiol-amine solvent system. We formulate molecular inks by dissolving the\nprecursor n,n-dimethylformamide (DMF) with a monoethanolamine (EA) additive and\nprocess high-quality Se and Se1-xTex films with bandgaps ranging from 1.20 eV\nto 1.86 eV. We fabricate PVs from these films using TiO2 and MoO3 charge\ntransport layers (CTLs) to achieve power conversion efficiencies as high as\n2.73% for Se and 2.33% for Se0.7Te0.3 under solar simulation. Se devices show\nexcellent stability with no degradation after 1 month in air, enabled by the\nexcellent stability of Se and the use of inorganic CTLs. This work represents\nan important step towards low-cost solution-phase processing of Se and Se1-xTex\nalloys for PVs and photodetectors with low toxicity and high bandgap\ntunability.", "AI": {"tldr": "\u672c\u7814\u7a76\u6210\u529f\u5f00\u53d1\u4e86\u4e00\u79cd\u66f4\u5b89\u5168\u7684\u7852\u548c\u7852\u78b2\u5408\u91d1\u6eb6\u6db2\u52a0\u5de5\u65b9\u6cd5\uff0c\u5236\u5907\u51fa\u9ad8\u6027\u80fd\u3001\u9ad8\u7a33\u5b9a\u6027\u7684\u5149\u4f0f\u5668\u4ef6\uff0c\u4e3a\u4f4e\u6210\u672c\u3001\u4f4e\u6bd2\u6027\u5149\u4f0f\u6280\u672f\u7684\u8fdb\u4e00\u6b65\u53d1\u5c55\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u7852 (Se) \u4f5c\u4e3a\u4e00\u79cd\u6709\u524d\u9014\u7684\u5ba4\u5185\u548c\u53e0\u5c42\u5149\u4f0f (PV) \u5438\u6536\u6750\u6599\uff0c\u5176\u4e0e\u78b2 (Te) \u7684\u5408\u91d1 (Se1-xTex) \u5177\u6709\u5e7f\u6cdb\u53ef\u8c03\u7684\u5e26\u9699\u3002\u6eb6\u6db2\u52a0\u5de5\u4e3a\u4f4e\u6210\u672c\u5236\u9020\u63d0\u4f9b\u4e86\u4e00\u6761\u9014\u5f84\u3002\u7136\u800c\uff0c\u76ee\u524d\u7852\u7684\u6eb6\u6db2\u52a0\u5de5\u4ec5\u4f7f\u7528\u9ad8\u5ea6\u5371\u9669\u7684\u6eb6\u5242\u80bc\u3002", "method": "\u672c\u5de5\u4f5c\u5728\u66f4\u5b89\u5168\u7684\u786b\u9187-\u80fa\u6eb6\u5242\u4f53\u7cfb\u4e2d\u5236\u5907\u5e76\u5206\u79bb\u4e86\u4e19\u57fa\u94f5\u805a\u7852\u548c\u805a\u7852\u78b2\u524d\u9a71\u4f53\u3002\u901a\u8fc7\u5728\u4e8c\u7532\u57fa\u7532\u9170\u80fa (DMF) \u4e2d\u6dfb\u52a0\u5355\u4e59\u9187\u80fa (EA) \u6765\u914d\u5236\u5206\u5b50\u6cb9\u58a8\uff0c\u5e76\u52a0\u5de5\u51fa\u5e26\u9699\u5728 1.20 eV \u81f3 1.86 eV \u8303\u56f4\u5185\u7684\u9ad8\u8d28\u91cf\u7852\u548c\u7852\u78b2\u5408\u91d1\u8584\u819c\u3002", "result": "\u4f7f\u7528 TiO2 \u548c MoO3 \u7535\u8377\u4f20\u8f93\u5c42 (CTL) \u5236\u6210\u7684\u5149\u4f0f\u5668\u4ef6\uff0c\u7852\u7684\u5149\u7535\u8f6c\u6362\u6548\u7387\u9ad8\u8fbe 2.73%\uff0c\u7852\u78b2\u5408\u91d1 (Se0.7Te0.3) \u7684\u5149\u7535\u8f6c\u6362\u6548\u7387\u9ad8\u8fbe 2.33%\u3002\u7852\u5668\u4ef6\u8868\u73b0\u51fa\u51fa\u8272\u7684\u7a33\u5b9a\u6027\uff0c\u5728\u7a7a\u6c14\u4e2d\u653e\u7f6e 1 \u4e2a\u6708\u540e\u65e0\u660e\u663e\u8870\u51cf\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4ee3\u8868\u4e86\u8fc8\u5411\u4f4e\u6210\u672c\u3001\u4f4e\u6bd2\u6027\u3001\u9ad8\u5e26\u9699\u53ef\u8c03\u8c10\u7684\u7852\u548c\u7852\u78b2\u5408\u91d1\u5728\u5149\u4f0f\u548c\u5149\u7535\u63a2\u6d4b\u5668\u4e2d\u6eb6\u6db2\u76f8\u52a0\u5de5\u7684\u91cd\u8981\u4e00\u6b65\u3002"}}
{"id": "2508.07267", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07267", "abs": "https://arxiv.org/abs/2508.07267", "authors": ["Daria de Tinguy", "Tim Verbelen", "Emilio Gamba", "Bart Dhoedt"], "title": "Bio-Inspired Topological Autonomous Navigation with Active Inference in Robotics", "comment": "Conference ICCAS 2025 - accepted (in processing)", "summary": "Achieving fully autonomous exploration and navigation remains a critical\nchallenge in robotics, requiring integrated solutions for localisation,\nmapping, decision-making and motion planning. Existing approaches either rely\non strict navigation rules lacking adaptability or on pre-training, which\nrequires large datasets. These AI methods are often computationally intensive\nor based on static assumptions, limiting their adaptability in dynamic or\nunknown environments. This paper introduces a bio-inspired agent based on the\nActive Inference Framework (AIF), which unifies mapping, localisation, and\nadaptive decision-making for autonomous navigation, including exploration and\ngoal-reaching. Our model creates and updates a topological map of the\nenvironment in real-time, planning goal-directed trajectories to explore or\nreach objectives without requiring pre-training. Key contributions include a\nprobabilistic reasoning framework for interpretable navigation, robust\nadaptability to dynamic changes, and a modular ROS2 architecture compatible\nwith existing navigation systems. Our method was tested in simulated and\nreal-world environments. The agent successfully explores large-scale simulated\nenvironments and adapts to dynamic obstacles and drift, proving to be\ncomparable to other exploration strategies such as Gbplanner, FAEL and\nFrontiers. This approach offers a scalable and transparent approach for\nnavigating complex, unstructured environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u7684\u4eff\u751f\u4ee3\u7406\uff0c\u7528\u4e8e\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u9884\u8bad\u7ec3\uff0c\u80fd\u591f\u5b9e\u65f6\u5efa\u56fe\u3001\u5b9a\u4f4d\u5e76\u81ea\u9002\u5e94\u51b3\u7b56\uff0c\u4ee5\u5e94\u5bf9\u52a8\u6001\u548c\u672a\u77e5\u73af\u5883\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63a2\u7d22\u548c\u5bfc\u822a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4e0e\u73b0\u6709\u5148\u8fdb\u7b56\u7565\u76f8\u5f53\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u548c\u900f\u660e\u6027\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u81ea\u4e3b\u63a2\u7d22\u548c\u5bfc\u822a\u7684\u65b9\u6cd5\u8981\u4e48\u4f9d\u8d56\u7f3a\u4e4f\u9002\u5e94\u6027\u7684\u4e25\u683c\u5bfc\u822a\u89c4\u5219\uff0c\u8981\u4e48\u4f9d\u8d56\u9700\u8981\u5927\u91cf\u6570\u636e\u96c6\u7684\u9884\u8bad\u7ec3\u3002\u8fd9\u4e9b\u4eba\u5de5\u667a\u80fd\u65b9\u6cd5\u901a\u5e38\u8ba1\u7b97\u91cf\u5927\u6216\u57fa\u4e8e\u9759\u6001\u5047\u8bbe\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5728\u52a8\u6001\u6216\u672a\u77e5\u73af\u5883\u4e2d\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u7684\u4eff\u751f\u4ee3\u7406\uff0c\u5b9e\u73b0\u4e86\u5730\u56fe\u7ed8\u5236\u3001\u5b9a\u4f4d\u548c\u81ea\u9002\u5e94\u51b3\u7b56\u7684\u7edf\u4e00\uff0c\u65e0\u9700\u9884\u5148\u8bad\u7ec3\uff0c\u5373\u53ef\u5728\u52a8\u6001\u6216\u672a\u77e5\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u5bfc\u822a\u3001\u63a2\u7d22\u548c\u76ee\u6807\u8fbe\u6210\u3002", "result": "\u8be5\u6a21\u578b\u5b9e\u65f6\u521b\u5efa\u548c\u66f4\u65b0\u73af\u5883\u7684\u62d3\u6251\u5730\u56fe\uff0c\u89c4\u5212\u6709\u5411\u76ee\u6807\u8f68\u8ff9\u8fdb\u884c\u63a2\u7d22\u6216\u8fbe\u6210\u76ee\u6807\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7528\u4e8e\u53ef\u89e3\u91ca\u5bfc\u822a\u7684\u6982\u7387\u63a8\u7406\u6846\u67b6\uff0c\u5e76\u5177\u6709\u5bf9\u52a8\u6001\u53d8\u5316\u7684\u9c81\u68d2\u9002\u5e94\u6027\uff0c\u4ee5\u53ca\u4e0e\u73b0\u6709\u5bfc\u822a\u7cfb\u7edf\u517c\u5bb9\u7684\u6a21\u5757\u5316 ROS2 \u67b6\u6784\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u73af\u5883\u4e2d\u90fd\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u590d\u6742\u3001\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u900f\u660e\u6027\u3002\u5176\u5728\u63a2\u7d22\u5927\u5c3a\u5ea6\u6a21\u62df\u73af\u5883\u4ee5\u53ca\u9002\u5e94\u52a8\u6001\u969c\u788d\u7269\u548c\u6f02\u79fb\u65b9\u9762\u7684\u8868\u73b0\uff0c\u4e0e Gbplanner\u3001FAEL \u548c Frontiers \u7b49\u5176\u4ed6\u63a2\u7d22\u7b56\u7565\u76f8\u5f53\u3002"}}
{"id": "2508.07379", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2508.07379", "abs": "https://arxiv.org/abs/2508.07379", "authors": ["Lixiang Ding", "Jingtao Fan", "Xingze Qiu"], "title": "Universally Robust Control of Open Quantum Systems", "comment": "9+3 pages, 3 figures", "summary": "Mitigating noise-induced decoherence is the central challenge in controlling\nopen quantum systems. While existing robust protocols often require precise\nnoise models, we introduce a universal framework for noise-agnostic quantum\ncontrol that achieves high-fidelity operations without prior environmental\nnoise characterization. This framework capitalizes on the dynamical\nmodification of the system-environment coupling through control drives, an\neffect rigorously encoded in the dynamical equation. Since the derived noise\nsensitivity metric remains independent of the coupling details between the\nsystem and the environment, our protocol demonstrates provable robustness\nagainst arbitrary Markovian noises. Numerical validation through quantum state\ntransfer and gate operations reveals near-unity fidelity ($>\\!99\\%$) across\ndiverse noise regimes, achieving orders-of-magnitude error suppression compared\nto target-only approaches. This framework bridges critical gaps between\ntheoretical control design and experimental constraints, establishing a\nhardware-agnostic pathway toward fault-tolerant quantum technologies across\nplatforms such as superconducting circuits, trapped ions, and solid-state\nqubits.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u566a\u58f0\u65e0\u5173\u7684\u91cf\u5b50\u63a7\u5236\u6846\u67b6\uff0c\u901a\u8fc7\u8c03\u6574\u7cfb\u7edf-\u73af\u5883\u8026\u5408\uff0c\u5728\u65e0\u5148\u9a8c\u566a\u58f0\u77e5\u8bc6\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u64cd\u4f5c\uff0c\u5e76\u5728\u4e0d\u540c\u5e73\u53f0\u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002", "motivation": " Mitigating noise-induced decoherence is the central challenge in controlling open quantum systems. \u73b0\u6709\u7684\u9c81\u68d2\u534f\u8bae\u901a\u5e38\u9700\u8981\u7cbe\u786e\u7684\u566a\u58f0\u6a21\u578b\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u901a\u7528\u7684\u3001\u4e0d\u4f9d\u8d56\u566a\u58f0\u6a21\u578b\u7684\u91cf\u5b50\u63a7\u5236\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u63a7\u5236\u9a71\u52a8\u8c03\u6574\u7cfb\u7edf-\u73af\u5883\u8026\u5408\u7684\u901a\u7528\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u566a\u58f0\u65e0\u5173\u7684\u91cf\u5b50\u63a7\u5236\u3002", "result": "\u901a\u8fc7\u91cf\u5b50\u6001\u8f6c\u79fb\u548c\u95e8\u64cd\u4f5c\u7684\u6570\u503c\u9a8c\u8bc1\uff0c\u5728\u5404\u79cd\u566a\u58f0\u73af\u5883\u4e0b\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u7edf\u4e00\u7684\u4fdd\u771f\u5ea6\uff08>99%\uff09\uff0c\u4e0e\u4ec5\u4ee5\u76ee\u6807\u4e3a\u5bfc\u5411\u7684\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8bef\u5dee\u6291\u5236\u80fd\u529b\u63d0\u9ad8\u4e86\u51e0\u4e2a\u6570\u91cf\u7ea7\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u5b9e\u73b0\u5bb9\u9519\u91cf\u5b50\u6280\u672f\u63d0\u4f9b\u4e86\u786c\u4ef6\u65e0\u5173\u7684\u9014\u5f84\uff0c\u9002\u7528\u4e8e\u8d85\u5bfc\u7535\u8def\u3001\u79bb\u5b50\u9631\u548c\u56fa\u6001\u91cf\u5b50\u6bd4\u7279\u7b49\u591a\u79cd\u5e73\u53f0\u3002"}}
{"id": "2508.07101", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07101", "abs": "https://arxiv.org/abs/2508.07101", "authors": ["Lijie Yang", "Zhihao Zhang", "Arti Jain", "Shijie Cao", "Baihong Yuan", "Yiwei Chen", "Zhihao Jia", "Ravi Netravali"], "title": "Less Is More: Training-Free Sparse Attention with Global Locality for Efficient Reasoning", "comment": null, "summary": "Large reasoning models achieve strong performance through test-time scaling\nbut incur substantial computational overhead, particularly from excessive token\ngeneration when processing short input prompts. While sparse attention\nmechanisms can reduce latency and memory usage, existing approaches suffer from\nsignificant accuracy degradation due to accumulated errors during\nlong-generation reasoning. These methods generally require either high token\nretention rates or expensive retraining. We introduce LessIsMore, a\ntraining-free sparse attention mechanism for reasoning tasks, which leverages\nglobal attention patterns rather than relying on traditional head-specific\nlocal optimizations. LessIsMore aggregates token selections from local\nattention heads with recent contextual information, enabling unified cross-head\ntoken ranking for future decoding layers. This unified selection improves\ngeneralization and efficiency by avoiding the need to maintain separate token\nsubsets per head. Evaluation across diverse reasoning tasks and benchmarks\nshows that LessIsMore preserves -- and in some cases improves -- accuracy while\nachieving a $1.1\\times$ average decoding speed-up compared to full attention.\nMoreover, LessIsMore attends to $2\\times$ fewer tokens without accuracy loss,\nachieving a $1.13\\times$ end-to-end speed-up compared to existing sparse\nattention methods.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.06656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06656", "abs": "https://arxiv.org/abs/2508.06656", "authors": ["Denis Lukovnikov", "Andreas M\u00fcller", "Erwin Quiring", "Asja Fischer"], "title": "Towards Robust Red-Green Watermarking for Autoregressive Image Generators", "comment": null, "summary": "In-generation watermarking for detecting and attributing generated content\nhas recently been explored for latent diffusion models (LDMs), demonstrating\nhigh robustness. However, the use of in-generation watermarks in autoregressive\n(AR) image models has not been explored yet. AR models generate images by\nautoregressively predicting a sequence of visual tokens that are then decoded\ninto pixels using a vector-quantized decoder. Inspired by red-green watermarks\nfor large language models, we examine token-level watermarking schemes that\nbias the next-token prediction based on prior tokens. We find that a direct\ntransfer of these schemes works in principle, but the detectability of the\nwatermarks decreases considerably under common image perturbations. As a\nremedy, we propose two novel watermarking methods that rely on visual token\nclustering to assign similar tokens to the same set. Firstly, we investigate a\ntraining-free approach that relies on a cluster lookup table, and secondly, we\nfinetune VAE encoders to predict token clusters directly from perturbed images.\nOverall, our experiments show that cluster-level watermarks improve robustness\nagainst perturbations and regeneration attacks while preserving image quality.\nCluster classification further boosts watermark detectability, outperforming a\nset of baselines. Moreover, our methods offer fast verification runtime,\ncomparable to lightweight post-hoc watermarking methods.", "AI": {"tldr": "This paper explores watermarking for AR image models, proposing cluster-level schemes that improve robustness and detectability against perturbations and regeneration attacks, with fast verification.", "motivation": "The paper aims to explore the use of in-generation watermarks in autoregressive (AR) image models, which has not been previously investigated. The motivation stems from the high robustness demonstrated by in-generation watermarks in latent diffusion models (LDMs), seeking to adapt and improve watermarking techniques for AR image generation.", "method": "The paper examines token-level watermarking schemes for autoregressive (AR) image models by biasing the next-token prediction based on prior tokens. It proposes two novel watermarking methods: a training-free approach using a cluster lookup table, and a method that finetunes VAE encoders to predict token clusters directly from perturbed images. These methods utilize visual token clustering to assign similar tokens to the same set.", "result": "Experiments show that the proposed cluster-level watermarks improve robustness against perturbations and regeneration attacks while preserving image quality. Cluster classification enhances watermark detectability, outperforming existing baselines. The methods also provide fast verification runtime, comparable to lightweight post-hoc watermarking methods.", "conclusion": "Cluster-level watermarks improve robustness against perturbations and regeneration attacks while preserving image quality. Cluster classification further boosts watermark detectability, outperforming a set of baselines. Moreover, our methods offer fast verification runtime, comparable to lightweight post-hoc watermarking methods."}}
{"id": "2508.07037", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.07037", "abs": "https://arxiv.org/abs/2508.07037", "authors": ["Yangguang He", "Wenhao Li", "Minzhe Li", "Juan Zhang", "Xiangfeng Wang", "Bo Jin"], "title": "Differentiable Adaptive Kalman Filtering via Optimal Transport", "comment": "20 pages", "summary": "Learning-based filtering has demonstrated strong performance in non-linear\ndynamical systems, particularly when the statistics of noise are unknown.\nHowever, in real-world deployments, environmental factors, such as changing\nwind conditions or electromagnetic interference, can induce unobserved\nnoise-statistics drift, leading to substantial degradation of learning-based\nmethods. To address this challenge, we propose OTAKNet, the first online\nsolution to noise-statistics drift within learning-based adaptive Kalman\nfiltering. Unlike existing learning-based methods that perform offline\nfine-tuning using batch pointwise matching over entire trajectories, OTAKNet\nestablishes a connection between the state estimate and the drift via one-step\npredictive measurement likelihood, and addresses it using optimal transport.\nThis leverages OT's geometry - aware cost and stable gradients to enable fully\nonline adaptation without ground truth labels or retraining. We compare OTAKNet\nagainst classical model-based adaptive Kalman filtering and offline\nlearning-based filtering. The performance is demonstrated on both synthetic and\nreal-world NCLT datasets, particularly under limited training data.", "AI": {"tldr": "OTAKNet is an online adaptive Kalman filter that tackles noise-statistics drift using optimal transport, achieving superior performance without retraining or ground truth data.", "motivation": "Real-world environmental factors can cause unobserved noise-statistics drift, degrading the performance of learning-based filtering methods.", "method": "OTAKNet connects state estimates to drift via one-step predictive measurement likelihood and uses optimal transport for online adaptation without ground truth labels or retraining.", "result": "OTAKNet demonstrates strong performance compared to classical model-based adaptive Kalman filtering and offline learning-based filtering on both synthetic and real-world NCLT datasets, particularly under limited training data.", "conclusion": "OTAKNet is the first online solution for noise-statistics drift in learning-based adaptive Kalman filtering, outperforming traditional methods on synthetic and real-world datasets, especially with limited training data."}}
{"id": "2508.07022", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.07022", "abs": "https://arxiv.org/abs/2508.07022", "authors": ["Shengtao Wen", "Haodong Chen", "Yadong Wang", "Zhongying Pan", "Xiang Chen", "Yu Tian", "Bo Qian", "Dong Liang", "Sheng-Jun Huang"], "title": "MultiMedEdit: A Scenario-Aware Benchmark for Evaluating Knowledge Editing in Medical VQA", "comment": "Under Review", "summary": "Knowledge editing (KE) provides a scalable approach for updating factual\nknowledge in large language models without full retraining. While previous\nstudies have demonstrated effectiveness in general domains and medical QA\ntasks, little attention has been paid to KE in multimodal medical scenarios.\nUnlike text-only settings, medical KE demands integrating updated knowledge\nwith visual reasoning to support safe and interpretable clinical decisions. To\naddress this gap, we propose MultiMedEdit, the first benchmark tailored to\nevaluating KE in clinical multimodal tasks. Our framework spans both\nunderstanding and reasoning task types, defines a three-dimensional metric\nsuite (reliability, generality, and locality), and supports cross-paradigm\ncomparisons across general and domain-specific models. We conduct extensive\nexperiments under single-editing and lifelong-editing settings. Results suggest\nthat current methods struggle with generalization and long-tail reasoning,\nparticularly in complex clinical workflows. We further present an efficiency\nanalysis (e.g., edit latency, memory footprint), revealing practical trade-offs\nin real-world deployment across KE paradigms. Overall, MultiMedEdit not only\nreveals the limitations of current approaches but also provides a solid\nfoundation for developing clinically robust knowledge editing techniques in the\nfuture.", "AI": {"tldr": "MultiMedEdit \u662f\u9996\u4e2a\u9488\u5bf9\u4e34\u5e8a\u591a\u6a21\u6001\u4efb\u52a1\u77e5\u8bc6\u7f16\u8f91\u7684\u57fa\u51c6\uff0c\u53d1\u73b0\u73b0\u6709\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u957f\u5c3e\u63a8\u7406\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u63d0\u4f9b\u4e86\u6548\u7387\u5206\u6790\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5bf9\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u7684\u5173\u6ce8\u8f83\u5c11\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u573a\u666f\u4e0b\uff0c\u800c\u533b\u5b66\u77e5\u8bc6\u7f16\u8f91\u9700\u8981\u6574\u5408\u66f4\u65b0\u7684\u77e5\u8bc6\u548c\u89c6\u89c9\u63a8\u7406\u4ee5\u652f\u6301\u5b89\u5168\u53ef\u89e3\u91ca\u7684\u4e34\u5e8a\u51b3\u7b56\u3002", "method": "\u63d0\u51fa MultiMedEdit \u6846\u67b6\uff0c\u5305\u62ec\u7406\u89e3\u548c\u63a8\u7406\u4efb\u52a1\u7c7b\u578b\uff0c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u4e09\u7ef4\u5ea6\u91cf\u5957\u4ef6\uff08\u53ef\u9760\u6027\u3001\u901a\u7528\u6027\u548c\u5c40\u90e8\u6027\uff09\uff0c\u5e76\u652f\u6301\u8de8\u8303\u5f0f\u6bd4\u8f83\u3002", "result": "\u901a\u8fc7\u5355\u6b21\u7f16\u8f91\u548c\u7ec8\u8eab\u7f16\u8f91\u8bbe\u7f6e\u4e0b\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5f53\u524d\u65b9\u6cd5\u5728\u6cdb\u5316\u548c\u957f\u5c3e\u63a8\u7406\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4e2d\u3002\u6b64\u5916\uff0c\u6548\u7387\u5206\u6790\u63ed\u793a\u4e86\u5b9e\u9645\u90e8\u7f72\u4e2d\u7684\u6743\u8861\u3002", "conclusion": "MultiMedEdit \u4e3a\u5728\u4e34\u5e8a\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8bc4\u4f30\u77e5\u8bc6\u7f16\u8f91\uff08KE\uff09\u63d0\u4f9b\u4e86\u4e00\u4e2a\u57fa\u51c6\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u5f00\u53d1\u4e34\u5e8a\u7a33\u5065\u7684\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u5960\u5b9a\u4e86\u575a\u5b9e\u7684\u57fa\u7840\u3002"}}
{"id": "2508.06800", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06800", "abs": "https://arxiv.org/abs/2508.06800", "authors": ["Rui Liu", "Haolin Zuo", "Zheng Lian", "Hongyu Yuan", "Qi Fan"], "title": "Hardness-Aware Dynamic Curriculum Learning for Robust Multimodal Emotion Recognition with Missing Modalities", "comment": null, "summary": "Missing modalities have recently emerged as a critical research direction in\nmultimodal emotion recognition (MER). Conventional approaches typically address\nthis issue through missing modality reconstruction. However, these methods fail\nto account for variations in reconstruction difficulty across different\nsamples, consequently limiting the model's ability to handle hard samples\neffectively. To overcome this limitation, we propose a novel Hardness-Aware\nDynamic Curriculum Learning framework, termed HARDY-MER. Our framework operates\nin two key stages: first, it estimates the hardness level of each sample, and\nsecond, it strategically emphasizes hard samples during training to enhance\nmodel performance on these challenging instances. Specifically, we first\nintroduce a Multi-view Hardness Evaluation mechanism that quantifies\nreconstruction difficulty by considering both Direct Hardness (modality\nreconstruction errors) and Indirect Hardness (cross-modal mutual information).\nMeanwhile, we introduce a Retrieval-based Dynamic Curriculum Learning strategy\nthat dynamically adjusts the training curriculum by retrieving samples with\nsimilar semantic information and balancing the learning focus between easy and\nhard instances. Extensive experiments on benchmark datasets demonstrate that\nHARDY-MER consistently outperforms existing methods in missing-modality\nscenarios. Our code will be made publicly available at\nhttps://github.com/HARDY-MER/HARDY-MER.", "AI": {"tldr": "HARDY-MER is a new framework for multimodal emotion recognition that handles missing modalities better by estimating sample hardness and using a dynamic curriculum to focus on harder samples during training.", "motivation": "Conventional approaches for missing modalities in multimodal emotion recognition (MER) using reconstruction fail to account for variations in reconstruction difficulty, limiting performance on hard samples.", "method": "HARDY-MER utilizes a two-stage framework: 1. Multi-view Hardness Evaluation mechanism to estimate sample hardness based on reconstruction errors and cross-modal mutual information. 2. Retrieval-based Dynamic Curriculum Learning strategy to adjust training curriculum by retrieving semantically similar samples and balancing learning focus between easy and hard instances.", "result": "Extensive experiments on benchmark datasets show HARDY-MER outperforms existing methods in missing-modality scenarios.", "conclusion": "HARDY-MER consistently outperforms existing methods in missing-modality scenarios."}}
{"id": "2508.07582", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07582", "abs": "https://arxiv.org/abs/2508.07582", "authors": ["Pratyay Mukherjee", "Arpita Dutta", "Somasree Bhattacharjee", "Shovon Pal", "Ritwik Mondal"], "title": "Field-derivative torque induced magnetization reversal in ferrimagnetic Gd$_{3/2}$Yb$_{1/2}$BiFe$_5$O$_{12}$", "comment": "11 pages, 6 figures", "summary": "Understanding the mechanism of spin switching in ferrimagnets via the\nexcitation of THz pulses holds promise for future-generation magnetic memory\ndevices. Such spin switching can be accomplished by the Zeeman torque exerted\nby the THz pulses on the magnetic spins. Theoretical and experimental works\nhave established that the field-derivative of a terahertz pulse also exerts a\ntorque, field derivative torque (FDT). Here, we investigate the role of the FDT\nin the spin switching in ferrimagnetic Gd$_{3/2}$Yb$_{1/2}$BiFe$_5$O$_{12}$\nusing a computational approach. Our results foresee that the spin switching in\nthe presence of the FDT requires less THz magnetic fields than the spin\nswitching without the FDT. Without the FDT terms, the spin switching in the\nconsidered system requires an extremely high magnetic field. Furthermore, we\ncompute the switching and non-switching contour diagrams to show that the FDT\ntremendously enhances the possibility of spin switching. These results not only\nshed light on the significance of the FDT in magnetization switching but also\nsuggest materials where the switching effect is pronounced.", "AI": {"tldr": "FDT\u5728\u94c1\u78c1Gd$_{3/2}$Yb$_{1/2}$BiFe$_5$O$_{12}$\u4e2d\u6781\u5927\u5730\u589e\u5f3a\u4e86\u81ea\u65cb\u7ffb\u8f6c\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u964d\u4f4e\u4e86\u7ffb\u8f6c\u6240\u9700\u7684\u78c1\u573a\u3002", "motivation": "\u7406\u89e3\u4e9a\u592a\u8d6b\u5179\u8109\u51b2\u6fc0\u53d1\u4e9a\u94c1\u78c1\u4f53\u4e2d\u81ea\u65cb\u7ffb\u8f6c\u7684\u673a\u5236\uff0c\u4e3a\u4e0b\u4e00\u4ee3\u78c1\u6027\u5b58\u50a8\u5668\u63d0\u4f9b\u4e86\u524d\u666f\u3002", "method": "\u901a\u8fc7\u8ba1\u7b97\u65b9\u6cd5\u7814\u7a76\u4e86FDT\u5728\u94c1\u78c1Gd$_{3/2}$Yb$_{1/2}$BiFe$_5$O$_{12}$\u81ea\u65cb\u7ffb\u8f6c\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728FDT\u5b58\u5728\u7684\u60c5\u51b5\u4e0b\uff0c\u81ea\u65cb\u7ffb\u8f6c\u6240\u9700\u7684\u4e9a\u592a\u8d6b\u5179\u78c1\u573a\u4f4e\u4e8e\u6ca1\u6709FDT\u7684\u60c5\u51b5\u3002\u6ca1\u6709FDT\u9879\u65f6\uff0c\u6240\u9700\u78c1\u573a\u6781\u9ad8\u3002\u8ba1\u7b97\u51fa\u7684\u7ffb\u8f6c\u548c\u975e\u7ffb\u8f6c\u7b49\u503c\u7ebf\u56fe\u8868\u660e\uff0cFDT\u6781\u5927\u5730\u589e\u5f3a\u4e86\u81ea\u65cb\u7ffb\u8f6c\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "FDT\u6781\u5927\u5730\u589e\u5f3a\u4e86\u78c1\u5316\u7ffb\u8f6c\u7684\u53ef\u80fd\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5f00\u5173\u6548\u5e94\u660e\u663e\u7684\u6750\u6599\u3002"}}
{"id": "2508.07269", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07269", "abs": "https://arxiv.org/abs/2508.07269", "authors": ["Daria de Tinguy", "Tim Verbelen", "Bart Dhoedt"], "title": "Navigation and Exploration with Active Inference: from Biology to Industry", "comment": "conference IWAI 2025 - accepted (in processing)", "summary": "By building and updating internal cognitive maps, animals exhibit\nextraordinary navigation abilities in complex, dynamic environments. Inspired\nby these biological mechanisms, we present a real time robotic navigation\nsystem grounded in the Active Inference Framework (AIF). Our model\nincrementally constructs a topological map, infers the agent's location, and\nplans actions by minimising expected uncertainty and fulfilling perceptual\ngoals without any prior training. Integrated into the ROS2 ecosystem, we\nvalidate its adaptability and efficiency across both 2D and 3D environments\n(simulated and real world), demonstrating competitive performance with\ntraditional and state of the art exploration approaches while offering a\nbiologically inspired navigation approach.", "AI": {"tldr": "\u53d7\u751f\u7269\u5bfc\u822a\u7684\u542f\u53d1\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\u7684\u673a\u5668\u4eba\u5b9e\u65f6\u5bfc\u822a\u7cfb\u7edf\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u6784\u5efa\u8ba4\u77e5\u5730\u56fe\u3001\u8fdb\u884c\u81ea\u4e3b\u5b9a\u4f4d\u548c\u89c4\u5212\u52a8\u4f5c\uff0c\u65e0\u9700\u9884\u5148\u8bad\u7ec3\uff0c\u5e76\u5728\u5404\u79cd\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002", "motivation": "\u53d7\u751f\u7269\u5bfc\u822a\u673a\u5236\u7684\u542f\u53d1\uff0c\u65e8\u5728\u4e3a\u673a\u5668\u4eba\u5f00\u53d1\u4e00\u79cd\u5b9e\u65f6\u7684\u3001\u65e0\u9700\u9884\u5148\u8bad\u7ec3\u7684\u3001\u57fa\u4e8e\u8ba4\u77e5\u5730\u56fe\u7684\u5bfc\u822a\u7cfb\u7edf\u3002", "method": "\u901a\u8fc7\u6784\u5efa\u548c\u66f4\u65b0\u5185\u90e8\u8ba4\u77e5\u56fe\uff0c\u7ed3\u5408\u4e86\u4e3b\u52a8\u63a8\u7406\u6846\u67b6\uff08AIF\uff09\u3001\u589e\u91cf\u5f0f\u62d3\u6251\u5730\u56fe\u6784\u5efa\u3001\u81ea\u4e3b\u5b9a\u4f4d\u548c\u57fa\u4e8e\u9884\u671f\u4e0d\u786e\u5b9a\u6027\u6700\u5c0f\u5316\u53ca\u611f\u77e5\u76ee\u6807\u5b9e\u73b0\u7684\u52a8\u4f5c\u89c4\u5212\uff0c\u5e76\u96c6\u6210\u4e86ROS2\u751f\u6001\u7cfb\u7edf\u3002", "result": "\u57282D\u548c3D\u73af\u5883\uff08\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\uff09\u4e2d\uff0c\u8be5\u7cfb\u7edf\u5c55\u793a\u4e86\u9002\u5e94\u6027\u548c\u6548\u7387\uff0c\u5e76\u4e0e\u4f20\u7edf\u548c\u6700\u5148\u8fdb\u7684\u63a2\u7d22\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u57282D\u548c3D\u73af\u5883\uff08\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\uff09\u4e2d\u90fd\u5f97\u5230\u4e86\u9a8c\u8bc1\uff0c\u5c55\u793a\u4e86\u4e0e\u4f20\u7edf\u548c\u6700\u5148\u8fdb\u7684\u63a2\u7d22\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\uff0c\u540c\u65f6\u63d0\u4f9b\u4e86\u4e00\u79cd\u53d7\u751f\u7269\u542f\u53d1\u7684\u5bfc\u822a\u65b9\u6cd5\u3002"}}
{"id": "2508.07481", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07481", "abs": "https://arxiv.org/abs/2508.07481", "authors": ["Yuntao Cui", "Zhaobing Fan", "Sunho Kim"], "title": "Quanutm-State Texture as a Resource: Measures and Nonclassical Interdependencies", "comment": "11 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2504.18166 by other authors", "summary": "Quantum-state texture is a newly recognized quantum resource that has\ngarnered attention with the advancement of quantum theory. In this work, we\ndiscuss several main tasks concerning quantum-state texture resource theory,\nincluding quantum-state texture measure, quantum state transformation under\nfree operations, and relations between quantum resources. We first provide two\nnew quantum-state texture measures and propose specific form of function for\nconstructing quantum-state texture measures through a convex roof method. Then,\nwe present the maximal probability of quantum state transformation through free\noperations. Lastly, we establish relationships between quantum-state texture\nand other highly regarded quantum resources, such as coherence, imaginary, and\npredictability. Our research complements the measure theory of quantum-state\ntexture and enriches the theory framework of quantum-state texture resource.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u65b0\u7684\u91cf\u5b50\u6001\u7eb9\u7406\u5ea6\u91cf\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5176\u5728\u91cf\u5b50\u6001\u8f6c\u53d8\u548c\u4e0e\u5176\u4ed6\u91cf\u5b50\u8d44\u6e90\uff08\u5982\u76f8\u5e72\u6027\uff09\u5173\u7cfb\u65b9\u9762\u7684\u5e94\u7528\u3002", "motivation": "\u91cf\u5b50\u6001\u7eb9\u7406\u4f5c\u4e3a\u4e00\u79cd\u65b0\u5174\u7684\u91cf\u5b50\u8d44\u6e90\uff0c\u5728\u91cf\u5b50\u7406\u8bba\u53d1\u5c55\u4e2d\u53d7\u5230\u5173\u6ce8\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8\u91cf\u5b50\u6001\u7eb9\u7406\u8d44\u6e90\u7406\u8bba\u7684\u6838\u5fc3\u95ee\u9898\uff0c\u5305\u62ec\u5ea6\u91cf\u3001\u72b6\u6001\u8f6c\u53d8\u548c\u4e0e\u5176\u4ed6\u91cf\u5b50\u8d44\u6e90\u7684\u5173\u7cfb\uff0c\u4ee5\u4e30\u5bcc\u8be5\u9886\u57df\u7684\u7406\u8bba\u6846\u67b6\u3002", "method": "\u672c\u7814\u7a76\u9996\u5148\u63d0\u51fa\u4e24\u79cd\u65b0\u7684\u91cf\u5b50\u6001\u7eb9\u7406\u5ea6\u91cf\uff0c\u5e76\u91c7\u7528\u51f8\u96c6\u5408\u51fd\u6570\u65b9\u6cd5\u6784\u5efa\u4e86\u91cf\u5b50\u6001\u7eb9\u7406\u5ea6\u91cf\u51fd\u6570\u3002\u968f\u540e\uff0c\u7814\u7a76\u7ed9\u51fa\u4e86\u5728\u81ea\u7531\u64cd\u4f5c\u4e0b\u91cf\u5b50\u6001\u7684\u6700\u5927\u8f6c\u53d8\u6982\u7387\u3002\u6700\u540e\uff0c\u5efa\u7acb\u4e86\u91cf\u5b50\u6001\u7eb9\u7406\u4e0e\u5176\u4ed6\u91cf\u5b50\u8d44\u6e90\uff08\u76f8\u5e72\u6027\u3001\u865a\u90e8\u3001\u53ef\u9884\u6d4b\u6027\uff09\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "result": "\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u91cf\u5b50\u6001\u7eb9\u7406\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u51f8\u96c6\u5408\u51fd\u6570\u65b9\u6cd5\u7ed9\u51fa\u4e86\u6784\u9020\u91cf\u5b50\u6001\u7eb9\u7406\u5ea6\u91cf\u51fd\u6570\u7684\u5177\u4f53\u5f62\u5f0f\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u7ed9\u51fa\u4e86\u901a\u8fc7\u81ea\u7531\u64cd\u4f5c\u5b9e\u73b0\u91cf\u5b50\u6001\u8f6c\u53d8\u7684\u6700\u5927\u6982\u7387\uff0c\u5e76\u5efa\u7acb\u4e86\u91cf\u5b50\u6001\u7eb9\u7406\u4e0e\u76f8\u5e72\u6027\u3001\u865a\u90e8\u548c\u53ef\u9884\u6d4b\u6027\u7b49\u5176\u4ed6\u91cf\u5b50\u8d44\u6e90\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "conclusion": "\u672c\u7814\u7a76\u5bf9\u91cf\u5b50\u6001\u7eb9\u7406\u8d44\u6e90\u7406\u8bba\u8fdb\u884c\u4e86\u63a2\u8ba8\uff0c\u5305\u62ec\u5176\u5ea6\u91cf\u3001\u5728\u81ea\u7531\u64cd\u4f5c\u4e0b\u7684\u72b6\u6001\u8f6c\u53d8\u4ee5\u53ca\u4e0e\u5176\u4ed6\u91cf\u5b50\u8d44\u6e90\uff08\u5982\u76f8\u5e72\u6027\u3001\u865a\u90e8\u548c\u53ef\u9884\u6d4b\u6027\uff09\u7684\u5173\u7cfb\u3002\u7814\u7a76\u63d0\u51fa\u4e86\u4e24\u79cd\u65b0\u7684\u91cf\u5b50\u6001\u7eb9\u7406\u5ea6\u91cf\uff0c\u5e76\u901a\u8fc7\u51f8\u96c6\u5408\u51fd\u6570\u65b9\u6cd5\u7ed9\u51fa\u4e86\u6784\u9020\u91cf\u5b50\u6001\u7eb9\u7406\u5ea6\u91cf\u51fd\u6570\u7684\u5177\u4f53\u5f62\u5f0f\uff0c\u540c\u65f6\u9610\u8ff0\u4e86\u901a\u8fc7\u81ea\u7531\u64cd\u4f5c\u5b9e\u73b0\u91cf\u5b50\u6001\u8f6c\u53d8\u7684\u6700\u5927\u6982\u7387\uff0c\u5e76\u5efa\u7acb\u4e86\u91cf\u5b50\u6001\u7eb9\u7406\u4e0e\u5176\u4ed6\u91cf\u5b50\u8d44\u6e90\u4e4b\u95f4\u7684\u8054\u7cfb\u3002"}}
{"id": "2508.07111", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07111", "abs": "https://arxiv.org/abs/2508.07111", "authors": ["Falaah Arif Khan", "Nivedha Sivakumar", "Yinong Oliver Wang", "Katherine Metcalf", "Cezanne Camacho", "Barry-John Theobald", "Luca Zappella", "Nicholas Apostoloff"], "title": "Investigating Intersectional Bias in Large Language Models using Confidence Disparities in Coreference Resolution", "comment": null, "summary": "Large language models (LLMs) have achieved impressive performance, leading to\ntheir widespread adoption as decision-support tools in resource-constrained\ncontexts like hiring and admissions. There is, however, scientific consensus\nthat AI systems can reflect and exacerbate societal biases, raising concerns\nabout identity-based harm when used in critical social contexts. Prior work has\nlaid a solid foundation for assessing bias in LLMs by evaluating demographic\ndisparities in different language reasoning tasks. In this work, we extend\nsingle-axis fairness evaluations to examine intersectional bias, recognizing\nthat when multiple axes of discrimination intersect, they create distinct\npatterns of disadvantage. We create a new benchmark called WinoIdentity by\naugmenting the WinoBias dataset with 25 demographic markers across 10\nattributes, including age, nationality, and race, intersected with binary\ngender, yielding 245,700 prompts to evaluate 50 distinct bias patterns.\nFocusing on harms of omission due to underrepresentation, we investigate bias\nthrough the lens of uncertainty and propose a group (un)fairness metric called\nCoreference Confidence Disparity which measures whether models are more or less\nconfident for some intersectional identities than others. We evaluate five\nrecently published LLMs and find confidence disparities as high as 40% along\nvarious demographic attributes including body type, sexual orientation and\nsocio-economic status, with models being most uncertain about\ndoubly-disadvantaged identities in anti-stereotypical settings. Surprisingly,\ncoreference confidence decreases even for hegemonic or privileged markers,\nindicating that the recent impressive performance of LLMs is more likely due to\nmemorization than logical reasoning. Notably, these are two independent\nfailures in value alignment and validity that can compound to cause social\nharm.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86WinoIdentity\u6570\u636e\u96c6\uff0c\u8bc4\u4f30\u4e86LLM\u5728\u4ea4\u53c9\u6027\u504f\u89c1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u6a21\u578b\u5bf9\u53cc\u91cd\u4e0d\u5229\u8eab\u4efd\u7684\u8bc6\u522b\u5b58\u5728\u663e\u8457\u4e0d\u786e\u5b9a\u6027\uff0c\u8fd9\u53ef\u80fd\u6e90\u4e8e\u8bb0\u5fc6\u800c\u975e\u63a8\u7406\uff0c\u63ed\u793a\u4e86\u4ef7\u503c\u5bf9\u9f50\u548c\u6709\u6548\u6027\u65b9\u9762\u7684\u7f3a\u9677\u3002", "motivation": "\u7531\u4e8eAI\u7cfb\u7edf\u53ef\u80fd\u53cd\u6620\u5e76\u52a0\u5267\u793e\u4f1a\u504f\u89c1\uff0c\u7279\u522b\u662f\u5728\u62db\u8058\u548c\u62db\u751f\u7b49\u5173\u952e\u793e\u4f1a\u80cc\u666f\u4e0b\uff0c\u5bf9LLM\u7684\u504f\u89c1\u8bc4\u4f30\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u5355\u4e00\u7ef4\u5ea6\u7684\u516c\u5e73\u6027\u8bc4\u4f30\uff0c\u800c\u672c\u7814\u7a76\u65e8\u5728\u6269\u5c55\u5230\u4ea4\u53c9\u6027\u504f\u89c1\uff0c\u8ba4\u8bc6\u5230\u591a\u4e2a\u6b67\u89c6\u8f74\u7684\u4ea4\u53c9\u4f1a\u4ea7\u751f\u72ec\u7279\u7684\u52a3\u52bf\u6a21\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u57fa\u51c6\u6570\u636e\u96c6WinoIdentity\uff0c\u901a\u8fc7\u5728WinoBias\u6570\u636e\u96c6\u4e2d\u52a0\u516525\u4e2a\u8de8\u8d8a10\u4e2a\u5c5e\u6027\uff08\u5305\u62ec\u5e74\u9f84\u3001\u56fd\u7c4d\u548c\u79cd\u65cf\uff09\u7684\u4eba\u53e3\u7edf\u8ba1\u5b66\u6807\u8bb0\uff0c\u5e76\u4e0e\u4e8c\u5143\u6027\u522b\u76f8\u7ed3\u5408\uff0c\u751f\u6210\u4e86245,700\u4e2a\u63d0\u793a\uff0c\u4ee5\u8bc4\u4f3050\u79cd\u4e0d\u540c\u7684\u504f\u89c1\u6a21\u5f0f\u3002\u7814\u7a76\u91cd\u70b9\u5173\u6ce8\u4e86\u7531\u4e8e\u4ee3\u8868\u6027\u4e0d\u8db3\u800c\u5bfc\u81f4\u7684\u9057\u6f0f\u6027\u5371\u5bb3\uff0c\u5e76\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u7684\u89c6\u89d2\u6765\u8c03\u67e5\u504f\u89c1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5171\u6307\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u201d\u7684\u7fa4\u4f53\uff08\u4e0d\uff09\u516c\u5e73\u6027\u5ea6\u91cf\u3002", "result": "\u7814\u7a76\u8bc4\u4f30\u4e86\u4e94\u79cd\u8fd1\u671f\u53d1\u5e03\u7684LLM\uff0c\u53d1\u73b0\u5176\u5728\u5305\u62ec\u4f53\u578b\u3001\u6027\u53d6\u5411\u548c\u793e\u4f1a\u7ecf\u6d4e\u5730\u4f4d\u5728\u5185\u7684\u5404\u79cd\u4eba\u53e3\u7edf\u8ba1\u5b66\u5c5e\u6027\u4e0a\u5b58\u5728\u9ad8\u8fbe40%\u7684\u7f6e\u4fe1\u5ea6\u5dee\u5f02\u3002", "conclusion": "LLM\u5728\u5904\u7406\u53cd\u523b\u677f\u5370\u8c61\u7684\u8bbe\u7f6e\u65f6\uff0c\u5bf9\u5904\u4e8e\u53cc\u91cd\u4e0d\u5229\u5730\u4f4d\u7684\u8eab\u4efd\u7684\u8bc6\u522b\u4e0d\u786e\u5b9a\u6027\u6700\u9ad8\uff0c\u8fbe\u523040%\u3002\u8fd9\u79cd\u4e0d\u786e\u5b9a\u6027\u4e0d\u4ec5\u9650\u4e8e\u7279\u5b9a\u7684\u7fa4\u4f53\uff0c\u8fd8\u5ef6\u4f38\u5230\u4e3b\u6d41\u6216\u6709\u4f18\u52bf\u7684\u6807\u8bb0\uff0c\u8fd9\u8868\u660eLLM\u7684\u6027\u80fd\u53ef\u80fd\u66f4\u591a\u5730\u4f9d\u8d56\u4e8e\u8bb0\u5fc6\u800c\u975e\u903b\u8f91\u63a8\u7406\u3002\u8fd9\u4e9b\u53d1\u73b0\u63ed\u793a\u4e86\u4ef7\u503c\u5bf9\u9f50\u548c\u6709\u6548\u6027\u65b9\u9762\u7684\u4e24\u4e2a\u72ec\u7acb\u7f3a\u9677\uff0c\u53ef\u80fd\u52a0\u5267\u793e\u4f1a\u5371\u5bb3\u3002"}}
{"id": "2508.06696", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06696", "abs": "https://arxiv.org/abs/2508.06696", "authors": ["Tianqin Li", "George Liu", "Tai Sing Lee"], "title": "Learning More by Seeing Less: Line Drawing Pretraining for Efficient, Transferable, and Human-Aligned Vision", "comment": null, "summary": "Despite remarkable progress in computer vision, modern recognition systems\nremain limited by their dependence on rich, redundant visual inputs. In\ncontrast, humans can effortlessly understand sparse, minimal representations\nlike line drawings - suggesting that structure, rather than appearance,\nunderlies efficient visual understanding. In this work, we propose using line\ndrawings as a structure-first pretraining modality to induce more compact and\ngeneralizable visual representations. We show that models pretrained on line\ndrawings develop stronger shape bias, more focused attention, and greater data\nefficiency across classification, detection, and segmentation tasks. Notably,\nthese models also exhibit lower intrinsic dimensionality, requiring\nsignificantly fewer principal components to capture representational variance -\nechoing the similar observation in low dimensional efficient representation in\nthe brain. Beyond performance improvements, line drawing pretraining produces\nmore compressible representations, enabling better distillation into\nlightweight student models. Students distilled from line-pretrained teachers\nconsistently outperform those trained from color-supervised teachers,\nhighlighting the benefits of structurally compact knowledge. Finally, we\ndemonstrate that the pretraining with line-drawing can also be extended to\nunsupervised setting via our proposed method \"learning to draw\". Together, our\nresults support the view that structure-first visual learning fosters\nefficiency, generalization, and human-aligned inductive biases - offering a\nsimple yet powerful strategy for building more robust and adaptable vision\nsystems.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u7ebf\u63cf\u753b\u4f5c\u4e3a\u89c6\u89c9\u8868\u5f81\u7684\u9884\u8bad\u7ec3\u6a21\u5f0f\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3001\u6548\u7387\u548c\u53ef\u538b\u7f29\u6027\uff0c\u5e76\u6210\u529f\u5c06\u5176\u5e94\u7528\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u5c3d\u7ba1\u8ba1\u7b97\u673a\u89c6\u89c9\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u73b0\u4ee3\u8bc6\u522b\u7cfb\u7edf\u4ecd\u7136\u53d7\u9650\u4e8e\u5bf9\u4e30\u5bcc\u3001\u5197\u4f59\u89c6\u89c9\u8f93\u5165\u7684\u4f9d\u8d56\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u80fd\u591f\u8f7b\u677e\u7406\u89e3\u7a00\u758f\u3001\u6781\u7b80\u7684\u8868\u5f81\uff0c\u5982\u7ebf\u63cf\u753b\uff0c\u8fd9\u8868\u660e\u7ed3\u6784\u800c\u975e\u5916\u89c2\u662f\u5b9e\u73b0\u9ad8\u6548\u89c6\u89c9\u7406\u89e3\u7684\u57fa\u7840\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7ebf\u63cf\u753b\u4f5c\u4e3a\u4e00\u79cd\u6709\u6548\u7684\u9884\u8bad\u7ec3\u6a21\u5f0f\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528\u7ebf\u63cf\u753b\u4f5c\u4e3a\u4e00\u79cd\u7ed3\u6784\u4f18\u5148\u7684\u9884\u8bad\u7ec3\u6a21\u5f0f\uff0c\u4ee5\u8bf1\u5bfc\u66f4\u7d27\u51d1\u548c\u66f4\u5177\u6cdb\u5316\u80fd\u529b\u7684\u89c6\u89c9\u8868\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u5b66\u4e60\u753b\u753b\u201d\u7684\u65b9\u6cd5\uff0c\u5c06\u7ebf\u63cf\u753b\u9884\u8bad\u7ec3\u6269\u5c55\u5230\u65e0\u76d1\u7763\u8bbe\u7f6e\u3002", "result": "\u5728\u7ebf\u63cf\u753b\u4e0a\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u5206\u7c7b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u5f62\u72b6\u504f\u7f6e\u3001\u66f4\u96c6\u4e2d\u7684\u6ce8\u610f\u529b\u548c\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u3002\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u66f4\u4f4e\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u5e76\u4e14\u5176\u8868\u5f81\u66f4\u6613\u4e8e\u538b\u7f29\uff0c\u4ece\u800c\u80fd\u591f\u66f4\u597d\u5730\u84b8\u998f\u5230\u8f7b\u91cf\u7ea7\u5b66\u751f\u6a21\u578b\u4e2d\u3002\u4e0e\u4ece\u989c\u8272\u76d1\u7763\u6559\u5e08\u6a21\u578b\u4e2d\u84b8\u998f\u51fa\u6765\u7684\u5b66\u751f\u6a21\u578b\u76f8\u6bd4\uff0c\u4ece\u7ebf\u63cf\u753b\u9884\u8bad\u7ec3\u6559\u5e08\u6a21\u578b\u4e2d\u84b8\u998f\u51fa\u6765\u7684\u5b66\u751f\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u4e0e\u4f20\u7edf\u7684\u4f9d\u8d56\u4e30\u5bcc\u89c6\u89c9\u8f93\u5165\u7684\u6a21\u578b\u76f8\u6bd4\uff0c\u57fa\u4e8e\u7ebf\u63cf\u753b\u8fdb\u884c\u9884\u8bad\u7ec3\u7684\u6a21\u578b\u5728\u8bc6\u522b\u3001\u68c0\u6d4b\u548c\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u6cdb\u5316\u80fd\u529b\u3001\u66f4\u96c6\u4e2d\u7684\u6ce8\u610f\u529b\u548c\u66f4\u9ad8\u7684\u6570\u636e\u6548\u7387\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u6a21\u578b\u5177\u6709\u66f4\u4f4e\u7684\u5185\u5728\u7ef4\u5ea6\uff0c\u5e76\u4e14\u5176\u8868\u5f81\u66f4\u6613\u4e8e\u538b\u7f29\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u80fd\u591f\u66f4\u597d\u5730\u8fc1\u79fb\u5230\u8f7b\u91cf\u7ea7\u6a21\u578b\u4e2d\u3002\u7ebf\u63cf\u753b\u9884\u8bad\u7ec3\u8fd8\u53ef\u4ee5\u6269\u5c55\u5230\u65e0\u76d1\u7763\u5b66\u4e60\u8bbe\u7f6e\u3002\u603b\u4e4b\uff0c\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u4ee5\u7ed3\u6784\u4e3a\u5148\u7684\u89c6\u89c9\u5b66\u4e60\u80fd\u591f\u63d0\u9ad8\u6548\u7387\u3001\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5f15\u5165\u4e0e\u4eba\u7c7b\u76f8\u4f3c\u7684\u5f52\u7eb3\u504f\u7f6e\uff0c\u4e3a\u6784\u5efa\u66f4\u9c81\u68d2\u3001\u66f4\u9002\u5e94\u6027\u5f3a\u7684\u89c6\u89c9\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.06985", "categories": ["cs.LG", "cs.CE", "cs.SY", "eess.SY", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.06985", "abs": "https://arxiv.org/abs/2508.06985", "authors": ["Jiawei Zhang", "Yifei Zhang", "Baozhao Yi", "Yao Ren", "Qi Jiao", "Hanyu Bai", "Weiran Jiang", "Ziyou Song"], "title": "Discovery Learning accelerates battery design evaluation", "comment": "Main text, 20 pages, 5 figures", "summary": "Fast and reliable validation of novel designs in complex physical systems\nsuch as batteries is critical to accelerating technological innovation.\nHowever, battery research and development remain bottlenecked by the\nprohibitively high time and energy costs required to evaluate numerous new\ndesign candidates, particularly in battery prototyping and life testing.\nDespite recent progress in data-driven battery lifetime prediction, existing\nmethods require labeled data of target designs to improve accuracy and cannot\nmake reliable predictions until after prototyping, thus falling far short of\nthe efficiency needed to enable rapid feedback for battery design. Here, we\nintroduce Discovery Learning (DL), a scientific machine-learning paradigm that\nintegrates active learning, physics-guided learning, and zero-shot learning\ninto a human-like reasoning loop, drawing inspiration from learning theories in\neducational psychology. DL can learn from historical battery designs and\nactively reduce the need for prototyping, thus enabling rapid lifetime\nevaluation for unobserved material-design combinations without requiring\nadditional data labeling. To test DL, we present 123 industrial-grade\nlarge-format lithium-ion pouch cells, spanning eight material-design\ncombinations and diverse cycling protocols. Trained solely on public datasets\nof small-capacity cylindrical cells, DL achieves 7.2% test error in predicting\nthe average cycle life under unknown device variability. This results in\nsavings of 98% in time and 95% in energy compared to industrial practices. This\nwork highlights the potential of uncovering insights from historical designs to\ninform and accelerate the development of next-generation battery technologies.\nDL represents a key advance toward efficient data-driven modeling and helps\nrealize the promise of machine learning for accelerating scientific discovery\nand engineering innovation.", "AI": {"tldr": "\u53d1\u73b0\u5b66\u4e60\uff08DL\uff09\u662f\u4e00\u79cd\u65b0\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u4eba\u7c7b\u5b66\u4e60\u8fc7\u7a0b\uff0c\u4ece\u5386\u53f2\u6570\u636e\u4e2d\u5b66\u4e60\uff0c\u5e76\u51cf\u5c11\u5b9e\u9a8c\u9700\u6c42\uff0c\u4ece\u800c\u5927\u5927\u52a0\u5feb\u4e86\u7535\u6c60\u8bbe\u8ba1\u7684\u5bff\u547d\u8bc4\u4f30\u8fc7\u7a0b\uff0c\u663e\u8457\u8282\u7701\u4e86\u65f6\u95f4\u548c\u80fd\u6e90\u3002", "motivation": "\u7535\u6c60\u7b49\u590d\u6742\u7269\u7406\u7cfb\u7edf\u7684\u521b\u65b0\u8bbe\u8ba1\u9700\u8981\u5feb\u901f\u53ef\u9760\u7684\u9a8c\u8bc1\uff0c\u4f46\u7535\u6c60\u7684\u7814\u53d1\u56e0\u8bc4\u4f30\u5927\u91cf\u65b0\u8bbe\u8ba1\u5019\u9009\u65b9\u6848\u6240\u9700\u7684\u9ad8\u6602\u65f6\u95f4\u548c\u80fd\u6e90\u6210\u672c\u800c\u53d7\u5230\u74f6\u9888\u3002\u73b0\u6709\u7684\u6570\u636e\u9a71\u52a8\u7535\u6c60\u5bff\u547d\u9884\u6d4b\u65b9\u6cd5\u9700\u8981\u76ee\u6807\u8bbe\u8ba1\u7684\u6807\u8bb0\u6570\u636e\u6765\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u4e14\u5728\u539f\u578b\u5236\u4f5c\u4e4b\u540e\u624d\u80fd\u8fdb\u884c\u53ef\u9760\u9884\u6d4b\uff0c\u6548\u7387\u4f4e\u4e0b\uff0c\u65e0\u6cd5\u6ee1\u8db3\u7535\u6c60\u8bbe\u8ba1\u7684\u5feb\u901f\u53cd\u9988\u9700\u6c42\u3002", "method": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a\u53d1\u73b0\u5b66\u4e60\uff08DL\uff09\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u6574\u5408\u4e86\u4e3b\u52a8\u5b66\u4e60\u3001\u7269\u7406\u5f15\u5bfc\u5b66\u4e60\u548c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u5e76\u501f\u9274\u4e86\u6559\u80b2\u5fc3\u7406\u5b66\u4e2d\u7684\u5b66\u4e60\u7406\u8bba\uff0c\u5f62\u6210\u4e86\u4e00\u4e2a\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63a8\u7406\u5faa\u73af\u3002DL\u80fd\u591f\u5229\u7528\u5386\u53f2\u7535\u6c60\u8bbe\u8ba1\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u4e3b\u52a8\u5b66\u4e60\u7b56\u7565\u6765\u51cf\u5c11\u5bf9\u539f\u578b\u5236\u4f5c\u7684\u4f9d\u8d56\uff0c\u4ece\u800c\u5bf9\u672a\u77e5\u7684\u6750\u6599-\u8bbe\u8ba1\u7ec4\u5408\u5b9e\u73b0\u5feb\u901f\u5bff\u547d\u8bc4\u4f30\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6807\u6ce8\u6570\u636e\u3002", "result": "\u5728\u6d4b\u8bd5\u4e2d\uff0cDL\u5728\u9884\u6d4b\u672a\u77e5\u8bbe\u5907\u5dee\u5f02\u4e0b\u7684\u5e73\u5747\u5faa\u73af\u5bff\u547d\u65f6\uff0c\u6d4b\u8bd5\u8bef\u5dee\u4e3a7.2%\u3002\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u76f8\u6bd4\uff0cDL\u5728\u65f6\u95f4\u548c\u80fd\u6e90\u65b9\u9762\u5206\u522b\u8282\u7701\u4e8698%\u548c95%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53d1\u73b0\u5b66\u4e60\uff08DL\uff09\u7684\u79d1\u5b66\u673a\u5668\u5b66\u4e60\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u901a\u8fc7\u6574\u5408\u4e3b\u52a8\u5b66\u4e60\u3001\u7269\u7406\u5f15\u5bfc\u5b66\u4e60\u548c\u96f6\u6837\u672c\u5b66\u4e60\uff0c\u6a21\u62df\u4eba\u7c7b\u63a8\u7406\u8fc7\u7a0b\uff0c\u80fd\u591f\u4ece\u5386\u53f2\u7535\u6c60\u8bbe\u8ba1\u4e2d\u5b66\u4e60\uff0c\u5e76\u4e3b\u52a8\u51cf\u5c11\u539f\u578b\u5236\u4f5c\u7684\u9700\u6c42\uff0c\u4ece\u800c\u5728\u65e0\u9700\u989d\u5916\u6570\u636e\u6807\u8bb0\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u672a\u89c2\u5bdf\u5230\u7684\u6750\u6599\u8bbe\u8ba1\u7ec4\u5408\u8fdb\u884c\u5feb\u901f\u5bff\u547d\u8bc4\u4f30\u3002DL\u5728\u9884\u6d4b\u672a\u77e5\u8bbe\u5907\u5dee\u5f02\u4e0b\u7684\u5e73\u5747\u5faa\u73af\u5bff\u547d\u65b9\u9762\u53d6\u5f97\u4e867.2%\u7684\u6d4b\u8bd5\u8bef\u5dee\uff0c\u4e0e\u5de5\u4e1a\u5b9e\u8df5\u76f8\u6bd4\uff0c\u8282\u7701\u4e8698%\u7684\u65f6\u95f4\u548c95%\u7684\u80fd\u6e90\uff0c\u5c55\u793a\u4e86\u4ece\u5386\u53f2\u8bbe\u8ba1\u4e2d\u53d1\u6398\u89c1\u89e3\u4ee5\u52a0\u901f\u4e0b\u4e00\u4ee3\u7535\u6c60\u6280\u672f\u53d1\u5c55\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07639", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07639", "abs": "https://arxiv.org/abs/2508.07639", "authors": ["Dameul Jeong", "Seoung-Hun Kang", "Young-Kyun Kwon"], "title": "Magnetic and Crystal Symmetry Effects on Spin Hall Conductivity in Altermagnets", "comment": null, "summary": "Altermagnets, which reconcile zero net magnetization with pronounced spin\nsplitting, offer fresh opportunities for spin-based functionalities in\nnext-generation electronic and spintronic devices. In this paper, we explore\nthe unconventional spin Hall conductivity (USHC) in three prototypical\naltermagnets -- RuO$_2$, CrSb, and MnTe -- and elucidate how distinct magnetic\nand crystal symmetries modulate their spin Hall responses. RuO$_2$ exhibits\nonly trivial USHC contributions under a tilted geometry, demonstrating that\nsymmetry projections alone can induce apparent unconventional elements. In\ncontrast, CrSb and MnTe manifest robust, symmetry-driven USHC without\nstructural tilts, enabled by easy-axis orientations that reduce magnetic\nsymmetry. Through extensive first-principles calculations, we demonstrate the\ncomplementary roles of the time-reversal-even and time-reversal-odd components\nin determining the overall SHC. Our findings indicate that controlling the\ninterplay between crystal and magnetic symmetry -- for instance, by epitaxial\nstrain or doping -- can provide an experimental avenue to tune USHC magnitudes\nand directions in altermagnets. These results pave the way for the engineering\nof multifunctional spintronic devices, where enhanced coherence and robust spin\ntransport are realized in zero-net-moment materials with easily tailored spin\nconfigurations.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e09\u79cd\u78c1\u5f02\u4f53\u6750\u6599\uff08RuO2\u3001CrSb\u3001MnTe\uff09\u7684\u975e\u4f20\u7edf\u81ea\u65cb\u970d\u5c14\u7535\u5bfc\uff0c\u53d1\u73b0\u6676\u4f53\u548c\u78c1\u5bf9\u79f0\u6027\u662f\u8c03\u63a7\u8be5\u6548\u5e94\u7684\u5173\u952e\uff0c\u4e3a\u5728\u96f6\u51c0\u78c1\u77e9\u6750\u6599\u4e2d\u5b9e\u73b0\u589e\u5f3a\u7684\u76f8\u5e72\u6027\u548c\u9c81\u68d2\u7684\u81ea\u65cb\u8f93\u8fd0\u63d0\u4f9b\u4e86\u5de5\u7a0b\u5316\u601d\u8def\u3002", "motivation": "\u63a2\u7d22\u78c1\u5f02\u4f53\u6750\u6599\uff08\u5177\u6709\u96f6\u51c0\u78c1\u5316\u4f46\u4ecd\u6709\u663e\u8457\u81ea\u65cb\u5206\u88c2\uff09\u5728\u7535\u5b50\u548c\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\uff0c\u7279\u522b\u662f\u7814\u7a76\u5176\u975e\u4f20\u7edf\u7684\u81ea\u65cb\u970d\u5c14\u6548\u5e94\u3002", "method": "\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7814\u7a76\u4e86 RuO2\u3001CrSb \u548c MnTe \u4e09\u79cd\u5178\u578b\u78c1\u5f02\u4f53\u6750\u6599\u7684\u975e\u4f20\u7edf\u81ea\u65cb\u970d\u5c14\u7535\u5bfc\uff08USHC\uff09\uff0c\u91cd\u70b9\u5206\u6790\u4e86\u6676\u4f53\u5bf9\u79f0\u6027\u548c\u78c1\u5bf9\u79f0\u6027\u5982\u4f55\u5f71\u54cd\u81ea\u65cb\u970d\u5c14\u54cd\u5e94\u3002", "result": "RuO2 \u5728\u503e\u659c\u51e0\u4f55\u4e0b\u4ec5\u8868\u73b0\u51fa\u5e73\u51e1\u7684 USHC\uff0c\u8868\u660e\u5bf9\u79f0\u6027\u6295\u5f71\u53ef\u8bf1\u5bfc\u8868\u89c2\u4e0a\u7684\u975e\u4f20\u7edf\u6548\u5e94\u3002CrSb \u548c MnTe \u5219\u5728\u65e0\u7ed3\u6784\u503e\u659c\u7684\u60c5\u51b5\u4e0b\u5c55\u73b0\u51fa\u7a33\u5065\u7684\u3001\u7531\u5bf9\u79f0\u6027\u9a71\u52a8\u7684 USHC\uff0c\u8fd9\u5f97\u76ca\u4e8e\u5176\u6613\u8f74\u53d6\u5411\u964d\u4f4e\u4e86\u78c1\u5bf9\u79f0\u6027\u3002\u7814\u7a76\u8fd8\u9610\u660e\u4e86\u65f6\u95f4\u53cd\u6f14\u5076\u6570\u548c\u5947\u6570\u5206\u91cf\u5728\u51b3\u5b9a\u6574\u4f53 SHC \u4e2d\u7684\u4e92\u8865\u4f5c\u7528\u3002", "conclusion": "\u901a\u8fc7\u7814\u7a76\u63ed\u793a\u4e86\u6676\u4f53\u548c\u78c1\u6027\u5bf9\u79f0\u6027\u5982\u4f55\u8c03\u8282\u78c1\u5f02\u4f53\u6750\u6599\uff08\u5305\u62ec RuO2\u3001CrSb \u548c MnTe\uff09\u7684\u81ea\u65cb\u970d\u5c14\u6548\u5e94\uff0c\u4e3a\u8c03\u63a7\u975e\u96f6\u78c1\u77e9\u6750\u6599\u4e2d\u7684\u81ea\u65cb\u4f20\u8f93\u63d0\u4f9b\u4e86\u65b0\u7684\u601d\u8def\uff0c\u6709\u671b\u5e94\u7528\u4e8e\u4e0b\u4e00\u4ee3\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u3002"}}
{"id": "2508.07253", "categories": ["cs.LG", "eess.SP", "q-bio.NC"], "pdf": "https://arxiv.org/pdf/2508.07253", "abs": "https://arxiv.org/abs/2508.07253", "authors": ["Bartlomiej Chybowski", "Shima Abdullateef", "Hollan Haule", "Alfredo Gonzalez-Sulser", "Javier Escudero"], "title": "PySeizure: A single machine learning classifier framework to detect seizures in diverse datasets", "comment": null, "summary": "Reliable seizure detection is critical for diagnosing and managing epilepsy,\nyet clinical workflows remain dependent on time-consuming manual EEG\ninterpretation. While machine learning has shown promise, existing approaches\noften rely on dataset-specific optimisations, limiting their real-world\napplicability and reproducibility. Here, we introduce an innovative,\nopen-source machine-learning framework that enables robust and generalisable\nseizure detection across varied clinical datasets. We evaluate our approach on\ntwo publicly available EEG datasets that differ in patient populations and\nelectrode configurations. To enhance robustness, the framework incorporates an\nautomated pre-processing pipeline to standardise data and a majority voting\nmechanism, in which multiple models independently assess each second of EEG\nbefore reaching a final decision. We train, tune, and evaluate models within\neach dataset, assessing their cross-dataset transferability. Our models achieve\nhigh within-dataset performance (AUC 0.904+/-0.059 for CHB-MIT and\n0.864+/-0.060 for TUSZ) and demonstrate strong generalisation across datasets\ndespite differences in EEG setups and populations (AUC 0.615+/-0.039 for models\ntrained on CHB-MIT and tested on TUSZ and 0.762+/-0.175 in the reverse case)\nwithout any post-processing. Furthermore, a mild post-processing improved the\nwithin-dataset results to 0.913+/-0.064 and 0.867+/-0.058 and cross-dataset\nresults to 0.619+/-0.036 and 0.768+/-0.172. These results underscore the\npotential of, and essential considerations for, deploying our framework in\ndiverse clinical settings. By making our methodology fully reproducible, we\nprovide a foundation for advancing clinically viable, dataset-agnostic seizure\ndetection systems. This approach has the potential for widespread adoption,\ncomplementing rather than replacing expert interpretation, and accelerating\nclinical integration.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u3001\u5f00\u6e90\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u540cEEG\u6570\u636e\u96c6\u4e2d\u8fdb\u884c\u53ef\u9760\u7684\u766b\u75eb\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u9884\u5904\u7406\u548c\u591a\u6570\u6295\u7968\u673a\u5236\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u9ad8\u5185\u90e8\u548c\u8de8\u6570\u636e\u96c6\u6027\u80fd\uff0c\u4e3a\u4e34\u5e8a\u5e94\u7528\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u53ef\u9760\u7684\u766b\u75eb\u68c0\u6d4b\u5bf9\u4e8e\u8bca\u65ad\u548c\u7ba1\u7406\u766b\u75eb\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4e34\u5e8a\u5de5\u4f5c\u6d41\u7a0b\u4ecd\u7136\u4f9d\u8d56\u4e8e\u8017\u65f6\u7684\u4eba\u5de5EEG\u89e3\u91ca\u3002\u73b0\u6709\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u7279\u5b9a\u6570\u636e\u96c6\u7684\u4f18\u5316\uff0c\u9650\u5236\u4e86\u5176\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u9002\u7528\u6027\u548c\u53ef\u91cd\u590d\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u3001\u5f00\u6e90\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u5305\u542b\u81ea\u52a8\u9884\u5904\u7406\u7ba1\u9053\u4ee5\u6807\u51c6\u5316\u6570\u636e\uff0c\u4ee5\u53ca\u4e00\u4e2a\u591a\u6570\u6295\u7968\u673a\u5236\uff0c\u5176\u4e2d\u591a\u4e2a\u6a21\u578b\u72ec\u7acb\u8bc4\u4f30\u6bcf\u4e2aEEG\u79d2\u6570\uff0c\u7136\u540e\u505a\u51fa\u6700\u7ec8\u51b3\u7b56\u3002\u5728\u4e24\u4e2a\u516c\u5f00\u53ef\u7528\u7684EEG\u6570\u636e\u96c6\uff08CHB-MIT\u548cTUSZ\uff09\u4e0a\u8bc4\u4f30\u4e86\u8be5\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u548c\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728CHB-MIT\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.904\uff08+/-0.059\uff09\u7684AUC\uff0c\u5728TUSZ\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e860.864\uff08+/-0.060\uff09\u7684AUC\u3002\u5728\u8de8\u6570\u636e\u96c6\u6d4b\u8bd5\u4e2d\uff0c\u4eceCHB-MIT\u8bad\u7ec3\u5230TUSZ\u6d4b\u8bd5\u7684AUC\u4e3a0.615\uff08+/-0.039\uff09\uff0c\u4eceTUSZ\u8bad\u7ec3\u5230CHB-MIT\u6d4b\u8bd5\u7684AUC\u4e3a0.762\uff08+/-0.175\uff09\u3002\u7ecf\u8fc7\u8f7b\u5fae\u540e\u5904\u7406\u540e\uff0c\u6570\u636e\u96c6\u5185\u90e8\u6027\u80fd\u63d0\u5347\u81f30.913\uff08+/-0.064\uff09\u548c0.867\uff08+/-0.058\uff09\uff0c\u8de8\u6570\u636e\u96c6\u6027\u80fd\u63d0\u5347\u81f30.619\uff08+/-0.036\uff09\u548c0.768\uff08+/-0.172\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u521b\u65b0\u7684\u3001\u5f00\u6e90\u7684\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u4e0d\u540c\u7684\u4e34\u5e8a\u8111\u7535\u56fe\uff08EEG\uff09\u6570\u636e\u96c6\u4e2d\u5b9e\u73b0\u53ef\u9760\u4e14\u53ef\u6cdb\u5316\u7684\u766b\u75eb\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u81ea\u52a8\u9884\u5904\u7406\u7ba1\u9053\u548c\u591a\u6570\u6295\u7968\u673a\u5236\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u4e24\u4e2a\u516c\u5f00\u53ef\u7528\u7684EEG\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u6570\u636e\u96c6\u5185\u90e8\u8868\u73b0\u51fa\u9ad8\u7cbe\u5ea6\uff08CHB-MIT\u7684AUC\u4e3a0.904\uff0cTUSZ\u7684AUC\u4e3a0.864\uff09\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u5176\u4ed6\u6570\u636e\u96c6\uff08CHB-MIT\u5230TUSZ\u7684AUC\u4e3a0.615\uff0cTUSZ\u5230CHB-MIT\u7684AUC\u4e3a0.762\uff09\uff0c\u65e0\u9700\u4efb\u4f55\u540e\u5904\u7406\u3002\u8f7b\u5fae\u7684\u540e\u5904\u7406\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\u6027\u80fd\u3002\u8be5\u7814\u7a76\u5f3a\u8c03\u4e86\u8be5\u6846\u67b6\u5728\u4e0d\u540c\u4e34\u5e8a\u73af\u5883\u90e8\u7f72\u7684\u6f5c\u529b\u548c\u5173\u952e\u8003\u8651\u56e0\u7d20\uff0c\u5e76\u4e3a\u5f00\u53d1\u4e34\u5e8a\u53ef\u7528\u3001\u4e0e\u6570\u636e\u96c6\u65e0\u5173\u7684\u766b\u75eb\u68c0\u6d4b\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06806", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06806", "abs": "https://arxiv.org/abs/2508.06806", "authors": ["Xiao Huang", "Xu Liu", "Enze Zhang", "Tong Yu", "Shuai Li"], "title": "Offline-to-Online Reinforcement Learning with Classifier-Free Diffusion Generation", "comment": "ICML2025", "summary": "Offline-to-online Reinforcement Learning (O2O RL) aims to perform online\nfine-tuning on an offline pre-trained policy to minimize costly online\ninteractions. Existing work used offline datasets to generate data that conform\nto the online data distribution for data augmentation. However, generated data\nstill exhibits a gap with the online data, limiting overall performance. To\naddress this, we propose a new data augmentation approach, Classifier-Free\nDiffusion Generation (CFDG). Without introducing additional classifier training\noverhead, CFDG leverages classifier-free guidance diffusion to significantly\nenhance the generation quality of offline and online data with different\ndistributions. Additionally, it employs a reweighting method to enable more\ngenerated data to align with the online data, enhancing performance while\nmaintaining the agent's stability. Experimental results show that CFDG\noutperforms replaying the two data types or using a standard diffusion model to\ngenerate new data. Our method is versatile and can be integrated with existing\noffline-to-online RL algorithms. By implementing CFDG to popular methods IQL,\nPEX and APL, we achieve a notable 15% average improvement in empirical\nperformance on the D4RL benchmark such as MuJoCo and AntMaze.", "AI": {"tldr": "CFDG \u901a\u8fc7\u6539\u8fdb\u6570\u636e\u589e\u5f3a\u6280\u672f\uff0c\u5728\u79bb\u7ebf\u5230\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u63d0\u9ad8\u4e86\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684 O2O RL \u65b9\u6cd5\u5728\u6570\u636e\u589e\u5f3a\u65b9\u9762\u5b58\u5728\u751f\u6210\u6570\u636e\u4e0e\u5728\u7ebf\u6570\u636e\u5206\u5e03\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\uff0c\u9650\u5236\u4e86\u6574\u4f53\u6027\u80fd\u3002", "method": "CFDG \u5229\u7528\u5206\u7c7b\u5668\u65e0\u5173\u7684\u6269\u6563\u6a21\u578b\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u79bb\u7ebf\u548c\u5728\u7ebf\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u91cd\u52a0\u6743\u65b9\u6cd5\u4f7f\u751f\u6210\u7684\u6570\u636e\u4e0e\u5728\u7ebf\u6570\u636e\u5bf9\u9f50\u3002", "result": "CFDG \u5728 D4RL \u57fa\u51c6\u6d4b\u8bd5\uff08\u5982 MuJoCo \u548c AntMaze\uff09\u4e0a\uff0c\u901a\u8fc7\u96c6\u6210\u5230 IQL\u3001PEX \u548c APL \u7b49\u5e38\u7528\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86 15% \u7684\u5e73\u5747\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "CFDG \u662f\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u63d0\u9ad8 O2O RL \u7684\u6027\u80fd\uff0c\u5e76\u4e14\u53ef\u4ee5\u96c6\u6210\u5230\u73b0\u6709\u7684 O2O RL \u7b97\u6cd5\u4e2d\u3002"}}
{"id": "2508.07287", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07287", "abs": "https://arxiv.org/abs/2508.07287", "authors": ["Liwen Zhang", "Dong Zhou", "Shibo Shao", "Zihao Su", "Guanghui Sun"], "title": "Multimodal Spiking Neural Network for Space Robotic Manipulation", "comment": null, "summary": "This paper presents a multimodal control framework based on spiking neural\nnetworks (SNNs) for robotic arms aboard space stations. It is designed to cope\nwith the constraints of limited onboard resources while enabling autonomous\nmanipulation and material transfer in space operations. By combining geometric\nstates with tactile and semantic information, the framework strengthens\nenvironmental awareness and contributes to more robust control strategies. To\nguide the learning process progressively, a dual-channel, three-stage\ncurriculum reinforcement learning (CRL) scheme is further integrated into the\nsystem. The framework was tested across a range of tasks including target\napproach, object grasping, and stable lifting with wall-mounted robotic arms,\ndemonstrating reliable performance throughout. Experimental evaluations\ndemonstrate that the proposed method consistently outperforms baseline\napproaches in both task success rate and energy efficiency. These findings\nhighlight its suitability for real-world aerospace applications.", "AI": {"tldr": "A new SNN-based multimodal control framework for space robotic arms improves task success and energy efficiency using curriculum reinforcement learning.", "motivation": "The framework is designed to cope with limited onboard resources in space stations and enable autonomous manipulation and material transfer in space operations, strengthening environmental awareness and contributing to more robust control strategies.", "method": "The paper integrates geometric states with tactile and semantic information within a multimodal control framework. It utilizes a dual-channel, three-stage curriculum reinforcement learning (CRL) scheme to guide the learning process. The framework was tested on robotic arms in space stations for tasks such as target approach, object grasping, and stable lifting.", "result": "Experimental evaluations show that the proposed method consistently outperforms baseline approaches in both task success rate and energy efficiency across various robotic arm tasks.", "conclusion": "The proposed multimodal control framework based on SNNs demonstrates reliable performance and outperforms baseline approaches in task success rate and energy efficiency, highlighting its suitability for real-world aerospace applications."}}
{"id": "2508.07488", "categories": ["quant-ph", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.07488", "abs": "https://arxiv.org/abs/2508.07488", "authors": ["Martin C. Korzeczek", "Ilai Schwartz", "Martin B. Plenio"], "title": "PHIP Sequences and Dipolar Fields", "comment": "55 pages, 17 figures", "summary": "Para-hydrogen induced polarization (PHIP) achieves efficient\nhyperpolarisation of nuclear spins with the transfer of the singlet order of\nparahydrogen to target molecules through catalytic hydrogenation reactions and\nsubsequent coherent control of the spin dynamics. However, in realistic\nconditions B0/B1 inhomogeneities lead to significant reduction in the\npolarization transfer efficiency. Moreover, in high-concentration samples,\ndipolar fields arising from the magnetisation of the sample can degrade\npolarisation transfer efficiency significantly. In this work, we present a\ntheoretical framework and a comprehensive analysis of both pulsed and\ncontinuous-wave (CW) control sequences designed to mitigate the detrimental\neffects of dipolar fields, $B_0/B_1$ inhomogeneities, and moderate chemical\nshifts. By combining tools from average Hamiltonian theory with detailed\nnumerical simulations, we introduce and characterise a wide range of transfer\nsequences, including dipolar-field adjusted and dipolar-field suppressing\nprotocols. We identify conditions under which dipolar interactions either\nhinder or, perhaps surprisingly, stabilise polarization transfer, depending on\nthe sequence structure. Our results offer practical guidance for the selection\nand design of PHIP transfer sequences under realistic experimental constraints\nand open pathways toward robust hyperpolarisation in concentrated liquid-state\nNMR samples.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u7406\u8bba\u6846\u67b6\u548c\u63a7\u5236\u5e8f\u5217\uff0c\u7528\u4e8e\u89e3\u51b3Para-\u6c22\u8bf1\u5bfc\u6781\u5316\uff08PHIP\uff09\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u9047\u5230\u7684B0/B1\u4e0d\u5747\u5300\u6027\u548c\u5076\u6781\u573a\u95ee\u9898\uff0c\u65e8\u5728\u63d0\u9ad8\u8d85\u6781\u5316\u6548\u7387\uff0c\u5c24\u5176\u662f\u5728\u9ad8\u6d53\u5ea6\u6db2\u6001\u6838\u78c1\u5171\u632f\u6837\u54c1\u4e2d\u3002", "motivation": "Para-\u6c22\u8bf1\u5bfc\u6781\u5316\uff08PHIP\uff09\u867d\u7136\u80fd\u6709\u6548\u5730\u5b9e\u73b0\u6838\u81ea\u65cb\u7684\u8d85\u6781\u5316\uff0c\u4f46\u5728\u5b9e\u9645\u7684B0/B1\u4e0d\u5747\u5300\u6027\u548c\u9ad8\u6d53\u5ea6\u6837\u54c1\u7684\u5076\u6781\u573a\u4e0b\uff0c\u6781\u5316\u8f6c\u79fb\u6548\u7387\u4f1a\u663e\u8457\u964d\u4f4e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u514b\u670d\u8fd9\u4e9b\u6311\u6218\u7684\u63a7\u5236\u5e8f\u5217\u3002", "method": "\u672c\u7814\u7a76\u7ed3\u5408\u4e86\u5e73\u5747\u54c8\u5bc6\u987f\u91cf\u7406\u8bba\u548c\u8be6\u7ec6\u7684\u6570\u503c\u6a21\u62df\uff0c\u63d0\u51fa\u5e76\u8868\u5f81\u4e86\u5e7f\u6cdb\u7684\u8f6c\u79fb\u5e8f\u5217\uff0c\u5305\u62ec\u5076\u6781\u573a\u8c03\u6574\u548c\u5076\u6781\u573a\u6291\u5236\u65b9\u6848\u3002", "result": "\u7814\u7a76\u786e\u5b9a\u4e86\u5076\u6781\u76f8\u4e92\u4f5c\u7528\u662f\u963b\u788d\u8fd8\u662f\u7a33\u5b9a\u6781\u5316\u8f6c\u79fb\u7684\u6761\u4ef6\uff0c\u5177\u4f53\u53d6\u51b3\u4e8e\u5e8f\u5217\u7ed3\u6784\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u5b9e\u9645\u5b9e\u9a8c\u7ea6\u675f\u4e0b\u9009\u62e9\u548c\u8bbe\u8ba1PHIP\u8f6c\u79fb\u5e8f\u5217\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u65e8\u5728\u7f13\u89e3\u7531\u5076\u6781\u573a\u3001B0/B1\u4e0d\u5747\u5300\u6027\u548c\u9002\u4e2d\u5316\u5b66\u4f4d\u79fb\u5f15\u8d77\u7684\u6709\u5bb3\u5f71\u54cd\u7684\u8109\u51b2\u548c\u8fde\u7eed\u6ce2\uff08CW\uff09\u63a7\u5236\u5e8f\u5217\u7684\u7406\u8bba\u6846\u67b6\u548c\u7efc\u5408\u5206\u6790\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u5728\u5b9e\u9645\u5b9e\u9a8c\u6761\u4ef6\u4e0b\u9009\u62e9\u548c\u8bbe\u8ba1PHIP\u8f6c\u79fb\u5e8f\u5217\u63d0\u4f9b\u4e86\u5b9e\u7528\u6307\u5bfc\uff0c\u5e76\u4e3a\u5728\u6d53\u7f29\u6db2\u6001\u6838\u78c1\u5171\u632f\u6837\u54c1\u4e2d\u5b9e\u73b0\u7a33\u5065\u7684\u8d85\u6781\u5316\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.07143", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07143", "abs": "https://arxiv.org/abs/2508.07143", "authors": ["Anna Seo Gyeong Choi", "Hoon Choi"], "title": "Fairness of Automatic Speech Recognition: Looking Through a Philosophical Lens", "comment": null, "summary": "Automatic Speech Recognition (ASR) systems now mediate countless\nhuman-technology interactions, yet research on their fairness implications\nremains surprisingly limited. This paper examines ASR bias through a\nphilosophical lens, arguing that systematic misrecognition of certain speech\nvarieties constitutes more than a technical limitation -- it represents a form\nof disrespect that compounds historical injustices against marginalized\nlinguistic communities. We distinguish between morally neutral classification\n(discriminate1) and harmful discrimination (discriminate2), demonstrating how\nASR systems can inadvertently transform the former into the latter when they\nconsistently misrecognize non-standard dialects. We identify three unique\nethical dimensions of speech technologies that differentiate ASR bias from\nother algorithmic fairness concerns: the temporal burden placed on speakers of\nnon-standard varieties (\"temporal taxation\"), the disruption of conversational\nflow when systems misrecognize speech, and the fundamental connection between\nspeech patterns and personal/cultural identity. These factors create asymmetric\npower relationships that existing technical fairness metrics fail to capture.\nThe paper analyzes the tension between linguistic standardization and pluralism\nin ASR development, arguing that current approaches often embed and reinforce\nproblematic language ideologies. We conclude that addressing ASR bias requires\nmore than technical interventions; it demands recognition of diverse speech\nvarieties as legitimate forms of expression worthy of technological\naccommodation. This philosophical reframing offers new pathways for developing\nASR systems that respect linguistic diversity and speaker autonomy.", "AI": {"tldr": "ASR \u7cfb\u7edf\u5b58\u5728\u504f\u89c1\uff0c\u5bf9\u975e\u6807\u51c6\u65b9\u8a00\u4e0d\u53cb\u597d\uff0c\u8fd9\u4e0d\u4ec5\u662f\u6280\u672f\u95ee\u9898\uff0c\u66f4\u662f\u6b67\u89c6\uff0c\u4f1a\u52a0\u5267\u793e\u4f1a\u4e0d\u516c\u3002\u9700\u8981\u4ece\u54f2\u5b66\u548c\u4f26\u7406\u89d2\u5ea6\u89e3\u51b3\uff0c\u627f\u8ba4\u5e76\u9002\u5e94\u8bed\u8a00\u591a\u6837\u6027\u3002", "motivation": "\u7814\u7a76 ASR \u7cfb\u7edf\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u662f\u5176\u504f\u89c1\u5bf9\u8fb9\u7f18\u5316\u8bed\u8a00\u793e\u533a\u9020\u6210\u7684\u8d1f\u9762\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u65b0\u7684\u5206\u6790\u6846\u67b6\u548c\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u901a\u8fc7\u54f2\u5b66\u89c6\u89d2\u5ba1\u89c6 ASR \u504f\u89c1\uff0c\u533a\u5206\u9053\u5fb7\u4e2d\u7acb\u7684\u5206\u7c7b\u548c\u6709\u5bb3\u7684\u6b67\u89c6\uff0c\u5e76\u6307\u51fa ASR \u7cfb\u7edf\u53ef\u80fd\u5c06\u524d\u8005\u8f6c\u5316\u4e3a\u540e\u8005\u3002\u5206\u6790\u4e86\u8bed\u97f3\u6280\u672f\u4e09\u4e2a\u72ec\u7279\u7684\u4f26\u7406\u7ef4\u5ea6\uff1a\u5bf9\u975e\u6807\u51c6\u65b9\u8a00\u53d1\u8a00\u8005\u7684\u201c\u65f6\u95f4\u7a0e\u201d\uff0c\u7cfb\u7edf\u9519\u8bef\u8bc6\u522b\u8bed\u97f3\u65f6\u9020\u6210\u7684\u5bf9\u8bdd\u4e2d\u65ad\uff0c\u4ee5\u53ca\u8bed\u97f3\u6a21\u5f0f\u4e0e\u4e2a\u4eba/\u6587\u5316\u8eab\u4efd\u4e4b\u95f4\u7684\u6839\u672c\u8054\u7cfb\u3002\u63a2\u8ba8\u4e86 ASR \u5f00\u53d1\u4e2d\u8bed\u8a00\u6807\u51c6\u5316\u4e0e\u591a\u5143\u5316\u4e4b\u95f4\u7684\u77db\u76fe\uff0c\u5e76\u8ba4\u4e3a\u5f53\u524d\u65b9\u6cd5\u53ef\u80fd\u5d4c\u5165\u5e76\u5f3a\u5316\u4e86\u5b58\u5728\u95ee\u9898\u7684\u8bed\u8a00\u610f\u8bc6\u5f62\u6001\u3002", "result": "\u8bc6\u522b\u51fa ASR \u504f\u89c1\u533a\u522b\u4e8e\u5176\u4ed6\u7b97\u6cd5\u516c\u5e73\u6027\u95ee\u9898\u7684\u4e09\u4e2a\u72ec\u7279\u4f26\u7406\u7ef4\u5ea6\uff08\u65f6\u95f4\u7a0e\u3001\u5bf9\u8bdd\u4e2d\u65ad\u3001\u8bed\u97f3\u4e0e\u8eab\u4efd\u7684\u8054\u7cfb\uff09\uff0c\u8fd9\u4e9b\u56e0\u7d20\u9020\u6210\u4e86\u73b0\u6709\u6280\u672f\u516c\u5e73\u6027\u6307\u6807\u65e0\u6cd5\u6355\u6349\u7684\u4e0d\u5bf9\u79f0\u6743\u529b\u5173\u7cfb\u3002\u5f53\u524d\u7684 ASR \u5f00\u53d1\u65b9\u6cd5\u53ef\u80fd\u5d4c\u5165\u5e76\u5f3a\u5316\u4e86\u5b58\u5728\u95ee\u9898\u7684\u8bed\u8a00\u610f\u8bc6\u5f62\u6001\u3002", "conclusion": "ASR \u7cfb\u7edf\u7684\u504f\u89c1\u4e0d\u4ec5\u4ec5\u662f\u6280\u672f\u9650\u5236\uff0c\u66f4\u662f\u5bf9\u8fb9\u7f18\u5316\u8bed\u8a00\u793e\u533a\u5386\u53f2\u4e0d\u516c\u7684\u52a0\u5267\u3002\u8981\u89e3\u51b3 ASR \u504f\u89c1\uff0c\u9700\u8981\u8d85\u8d8a\u6280\u672f\u5e72\u9884\uff0c\u627f\u8ba4\u5e76\u9002\u5e94\u591a\u6837\u5316\u7684\u8bed\u97f3\u8868\u8fbe\u5f62\u5f0f\uff0c\u4ee5\u5c0a\u91cd\u8bed\u8a00\u591a\u6837\u6027\u548c\u53d1\u8a00\u8005\u81ea\u4e3b\u6743\u3002"}}
{"id": "2508.06701", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.06701", "abs": "https://arxiv.org/abs/2508.06701", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Hamdi Altaheri", "Lobna Nassar", "Fakhri Karray"], "title": "MMFformer: Multimodal Fusion Transformer Network for Depression Detection", "comment": "Accepted for the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC), Vienna, Austria", "summary": "Depression is a serious mental health illness that significantly affects an\nindividual's well-being and quality of life, making early detection crucial for\nadequate care and treatment. Detecting depression is often difficult, as it is\nbased primarily on subjective evaluations during clinical interviews. Hence,\nthe early diagnosis of depression, thanks to the content of social networks,\nhas become a prominent research area. The extensive and diverse nature of\nuser-generated information poses a significant challenge, limiting the accurate\nextraction of relevant temporal information and the effective fusion of data\nacross multiple modalities. This paper introduces MMFformer, a multimodal\ndepression detection network designed to retrieve depressive spatio-temporal\nhigh-level patterns from multimodal social media information. The transformer\nnetwork with residual connections captures spatial features from videos, and a\ntransformer encoder is exploited to design important temporal dynamics in\naudio. Moreover, the fusion architecture fused the extracted features through\nlate and intermediate fusion strategies to find out the most relevant\nintermodal correlations among them. Finally, the proposed network is assessed\non two large-scale depression detection datasets, and the results clearly\nreveal that it surpasses existing state-of-the-art approaches, improving the\nF1-Score by 13.92% for D-Vlog dataset and 7.74% for LMVD dataset. The code is\nmade available publicly at\nhttps://github.com/rezwanh001/Large-Scale-Multimodal-Depression-Detection.", "AI": {"tldr": "MMFformer\u662f\u4e00\u79cd\u521b\u65b0\u7684\u591a\u6a21\u6001\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u878d\u5408\u89c6\u9891\u548c\u97f3\u9891\u4fe1\u606f\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u6291\u90c1\u75c7\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u6291\u90c1\u75c7\u662f\u4e00\u79cd\u4e25\u91cd\u7684\u5fc3\u7406\u5065\u5eb7\u75be\u75c5\uff0c\u65e9\u671f\u68c0\u6d4b\u5bf9\u5176\u6cbb\u7597\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u4e3b\u8981\u57fa\u4e8e\u4e34\u5e8a\u8bbf\u8c08\u4e2d\u7684\u4e3b\u89c2\u8bc4\u4f30\uff0c\u56e0\u6b64\u68c0\u6d4b\u8fc7\u7a0b\u975e\u5e38\u56f0\u96be\u3002\u5229\u7528\u793e\u4ea4\u5a92\u4f53\u5185\u5bb9\u8fdb\u884c\u65e9\u671f\u6291\u90c1\u75c7\u8bca\u65ad\u5df2\u6210\u4e3a\u4e00\u4e2a\u91cd\u8981\u7684\u7814\u7a76\u9886\u57df\uff0c\u4f46\u7528\u6237\u751f\u6210\u4fe1\u606f\u7684\u5e7f\u6cdb\u6027\u548c\u591a\u6837\u6027\u7ed9\u51c6\u786e\u63d0\u53d6\u76f8\u5173\u65f6\u95f4\u4fe1\u606f\u548c\u8de8\u6a21\u6001\u6570\u636e\u878d\u5408\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "MMFformer\u662f\u4e00\u79cd\u591a\u6a21\u6001\u6291\u90c1\u68c0\u6d4b\u7f51\u7edc\uff0c\u5b83\u5229\u7528Transformer\u7f51\u7edc\u6355\u83b7\u89c6\u9891\u7684\u7a7a\u95f4\u7279\u5f81\uff0c\u5e76\u5229\u7528Transformer\u7f16\u7801\u5668\u63d0\u53d6\u97f3\u9891\u4e2d\u7684\u91cd\u8981\u65f6\u95f4\u52a8\u6001\u3002\u8be5\u878d\u5408\u67b6\u6784\u901a\u8fc7\u540e\u671f\u548c\u4e2d\u95f4\u878d\u5408\u7b56\u7565\u878d\u5408\u63d0\u53d6\u7684\u7279\u5f81\uff0c\u4ee5\u627e\u51fa\u5b83\u4eec\u4e4b\u95f4\u6700\u76f8\u5173\u7684\u6a21\u6001\u95f4\u76f8\u5173\u6027\u3002", "result": "MMFformer\u5728D-Vlog\u6570\u636e\u96c6\u548cLMVD\u6570\u636e\u96c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "MMFformer\u5728\u4e24\u4e2a\u5927\u89c4\u6a21\u6291\u90c1\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0cD-Vlog\u6570\u636e\u96c6F1-Score\u63d0\u9ad8\u4e8613.92%\uff0cLMVD\u6570\u636e\u96c6\u63d0\u9ad8\u4e867.74%\u3002"}}
{"id": "2508.07029", "categories": ["cs.LG", "cs.AI", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07029", "abs": "https://arxiv.org/abs/2508.07029", "authors": ["Antonio Guillen-Perez"], "title": "From Imitation to Optimization: A Comparative Study of Offline Learning for Autonomous Driving", "comment": null, "summary": "Learning robust driving policies from large-scale, real-world datasets is a\ncentral challenge in autonomous driving, as online data collection is often\nunsafe and impractical. While Behavioral Cloning (BC) offers a straightforward\napproach to imitation learning, policies trained with BC are notoriously\nbrittle and suffer from compounding errors in closed-loop execution. This work\npresents a comprehensive pipeline and a comparative study to address this\nlimitation. We first develop a series of increasingly sophisticated BC\nbaselines, culminating in a Transformer-based model that operates on a\nstructured, entity-centric state representation. While this model achieves low\nimitation loss, we show that it still fails in long-horizon simulations. We\nthen demonstrate that by applying a state-of-the-art Offline Reinforcement\nLearning algorithm, Conservative Q-Learning (CQL), to the same data and\narchitecture, we can learn a significantly more robust policy. Using a\ncarefully engineered reward function, the CQL agent learns a conservative value\nfunction that enables it to recover from minor errors and avoid\nout-of-distribution states. In a large-scale evaluation on 1,000 unseen\nscenarios from the Waymo Open Motion Dataset, our final CQL agent achieves a\n3.2x higher success rate and a 7.4x lower collision rate than the strongest BC\nbaseline, proving that an offline RL approach is critical for learning robust,\nlong-horizon driving policies from static expert data.", "AI": {"tldr": "\u4e0e\u884c\u4e3a\u514b\u9686\u76f8\u6bd4\uff0c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08CQL\uff09\u5728\u4ece\u9759\u6001\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u9a7e\u9a76\u7b56\u7565\u65b9\u9762\u5177\u6709\u663e\u8457\u4f18\u52bf\uff0c\u5c24\u5176\u662f\u5728\u957f\u671f\u9c81\u68d2\u6027\u548c\u9519\u8bef\u6062\u590d\u65b9\u9762\u3002", "motivation": "\u4ece\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u9a7e\u9a76\u7b56\u7565\u662f\u4e00\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u56e0\u4e3a\u5728\u7ebf\u6570\u636e\u6536\u96c6\u901a\u5e38\u4e0d\u5b89\u5168\u4e14\u4e0d\u5207\u5b9e\u9645\u3002\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u867d\u7136\u7b80\u5355\uff0c\u4f46\u5176\u7b56\u7565\u5f80\u5f80\u8106\u5f31\uff0c\u5e76\u4e14\u5728\u95ed\u73af\u6267\u884c\u4e2d\u4f1a\u51fa\u73b0\u590d\u5408\u9519\u8bef\u3002", "method": "\u901a\u8fc7\u4e00\u4e2a\u5305\u542b\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u548c\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08CQL\uff09\u7684\u7efc\u5408\u6d41\u6c34\u7ebf\u548c\u6bd4\u8f83\u7814\u7a76\u6765\u89e3\u51b3\u4ece\u5927\u89c4\u6a21\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u9a7e\u9a76\u7b56\u7565\u7684\u6311\u6218\u3002BC\u65b9\u6cd5\u5305\u62ec\u4e00\u4e2a\u57fa\u4e8eTransformer\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5728\u7ed3\u6784\u5316\u7684\u3001\u4ee5\u5b9e\u4f53\u4e3a\u4e2d\u5fc3\u7684\u72b6\u6001\u8868\u793a\u4e0a\u64cd\u4f5c\u3002CQL\u65b9\u6cd5\u5e94\u7528\u4e86\u6700\u5148\u8fdb\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\uff0c\u5e76\u7ed3\u5408\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u4ee5\u5b66\u4e60\u4e00\u4e2a\u4fdd\u5b88\u7684\u4ef7\u503c\u51fd\u6570\u3002", "result": "\u5728Waymo\u5f00\u653e\u8fd0\u52a8\u6570\u636e\u96c6\u76841000\u4e2a\u672a\u89c1\u8fc7\u573a\u666f\u7684\u5927\u89c4\u6a21\u8bc4\u4f30\u4e2d\uff0c\u6700\u7ec8\u7684CQL\u4ee3\u7406\u5b9e\u73b0\u4e86\u6bd4\u6700\u5f3a\u7684BC\u57fa\u7ebf\u9ad83.2\u500d\u7684\u6210\u529f\u7387\u548c\u4f4e7.4\u500d\u7684\u78b0\u649e\u7387\u3002", "conclusion": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08CQL\uff09\u65b9\u6cd5\u6bd4\u884c\u4e3a\u514b\u9686\uff08BC\uff09\u65b9\u6cd5\u66f4\u80fd\u4ece\u9759\u6001\u4e13\u5bb6\u6570\u636e\u4e2d\u5b66\u4e60\u9c81\u68d2\u7684\u3001\u957f\u671f\u7684\u9a7e\u9a76\u7b56\u7565\u3002"}}
{"id": "2508.07870", "categories": ["quant-ph", "cond-mat.mes-hall", "gr-qc", "hep-th", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2508.07870", "abs": "https://arxiv.org/abs/2508.07870", "authors": ["Julian Rapp", "Radhika H. Joshi", "Alwin van Steensel", "Yuli V. Nazarov", "Mohammad H. Ansari"], "title": "Information Transport in Classic-Quantum Hybrid System", "comment": "16 pages, 11 figures", "summary": "Many important quantities in quantum information science, such as entropy and\nentanglement, are non-linear functions of the density matrix and cannot be\nexpressed as operator observables. Standard open-system approaches evolve only\na single copy of the density matrix, making it impossible to track the dynamics\nof such quantities. A formalism proposed by some of the present authors\naddressed this challenge by evolving multiple virtual replicas, but was limited\nto the weak-coupling regime. Here, we extend this approach to strong coupling\nbetween a quantum system and classical environments. The resulting\nmulti-replica master equation enables direct evaluation of entropy flow and\nrelated metrics in strongly hybridized quantum-classical systems. Our results\nshow that quantum coherence and hybridization jointly suppress net entropy\ntransfer, creating a thermodynamic bottleneck. This framework provides a\ngeneral tool for studying entropy dynamics and guiding the design of more\nrobust, resource-efficient quantum hardware.", "AI": {"tldr": "\u672c\u7814\u7a76\u5c06\u591a\u526f\u672c\u65b9\u6cd5\u6269\u5c55\u5230\u5f3a\u8026\u5408\u533a\u57df\uff0c\u901a\u8fc7\u591a\u526f\u672c\u4e3b\u65b9\u7a0b\u5b9e\u73b0\u4e86\u71b5\u6d41\u7b49\u975e\u7ebf\u6027\u91cf\u7684\u76f4\u63a5\u8bc4\u4f30\uff0c\u5e76\u53d1\u73b0\u91cf\u5b50\u76f8\u5e72\u6027\u548c\u6742\u5316\u5171\u540c\u4f5c\u7528\u4f1a\u6291\u5236\u71b5\u8f6c\u79fb\uff0c\u4ece\u800c\u4e3a\u8bbe\u8ba1\u66f4\u4f18\u7684\u91cf\u5b50\u786c\u4ef6\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002", "motivation": "\u7531\u4e8e\u5bc6\u5ea6\u77e9\u9635\u7684\u975e\u7ebf\u6027\u51fd\u6570\uff0c\u5982\u71b5\u548c\u7ea0\u7f20\uff0c\u4e0d\u80fd\u8868\u793a\u4e3a\u7b97\u5b50\u53ef\u89c2\u6d4b\u91cf\uff0c\u56e0\u6b64\u6807\u51c6\u7684\u5f00\u653e\u7cfb\u7edf\u65b9\u6cd5\u53ea\u80fd\u6f14\u5316\u5bc6\u5ea6\u77e9\u9635\u7684\u5355\u4e2a\u526f\u672c\uff0c\u8fd9\u4f7f\u5f97\u8ffd\u8e2a\u8fd9\u4e9b\u91cf\u7684\u52a8\u529b\u5b66\u6210\u4e3a\u4e0d\u53ef\u80fd\u3002\u5148\u524d\u7531\u90e8\u5206\u4f5c\u8005\u63d0\u51fa\u7684\u5f62\u5f0f\u4e3b\u4e49\u901a\u8fc7\u6f14\u5316\u591a\u4e2a\u865a\u62df\u526f\u672c\u89e3\u51b3\u4e86\u8fd9\u4e2a\u6311\u6218\uff0c\u4f46\u4ec5\u9650\u4e8e\u5f31\u8026\u5408\u533a\u57df\u3002", "method": "\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u526f\u672c\u4e3b\u65b9\u7a0b\uff0c\u8be5\u65b9\u7a0b\u80fd\u591f\u76f4\u63a5\u8bc4\u4f30\u5f3a\u6742\u4ea4\u91cf\u5b50-\u7ecf\u5178\u7cfb\u7edf\u4e2d\u7684\u71b5\u6d41\u548c\u76f8\u5173\u6307\u6807\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u91cf\u5b50\u76f8\u5e72\u6027\u548c\u6742\u5316\u5171\u540c\u6291\u5236\u4e86\u51c0\u71b5\u8f6c\u79fb\uff0c\u5f62\u6210\u4e86\u70ed\u529b\u5b66\u74f6\u9888\u3002", "conclusion": "\u672c\u6846\u67b6\u4e3a\u7814\u7a76\u71b5\u52a8\u529b\u5b66\u548c\u8bbe\u8ba1\u66f4\u9c81\u68d2\u3001\u8d44\u6e90\u9ad8\u6548\u7684\u91cf\u5b50\u786c\u4ef6\u63d0\u4f9b\u4e86\u901a\u7528\u5de5\u5177\u3002"}}
{"id": "2508.07063", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07063", "abs": "https://arxiv.org/abs/2508.07063", "authors": ["Naseem Machlovi", "Maryam Saleki", "Innocent Ababio", "Ruhul Amin"], "title": "Towards Safer AI Moderation: Evaluating LLM Moderators Through a Unified Benchmark Dataset and Advocating a Human-First Approach", "comment": null, "summary": "As AI systems become more integrated into daily life, the need for safer and\nmore reliable moderation has never been greater. Large Language Models (LLMs)\nhave demonstrated remarkable capabilities, surpassing earlier models in\ncomplexity and performance. Their evaluation across diverse tasks has\nconsistently showcased their potential, enabling the development of adaptive\nand personalized agents. However, despite these advancements, LLMs remain prone\nto errors, particularly in areas requiring nuanced moral reasoning. They\nstruggle with detecting implicit hate, offensive language, and gender biases\ndue to the subjective and context-dependent nature of these issues. Moreover,\ntheir reliance on training data can inadvertently reinforce societal biases,\nleading to inconsistencies and ethical concerns in their outputs. To explore\nthe limitations of LLMs in this role, we developed an experimental framework\nbased on state-of-the-art (SOTA) models to assess human emotions and offensive\nbehaviors. The framework introduces a unified benchmark dataset encompassing 49\ndistinct categories spanning the wide spectrum of human emotions, offensive and\nhateful text, and gender and racial biases. Furthermore, we introduced SafePhi,\na QLoRA fine-tuned version of Phi-4, adapting diverse ethical contexts and\noutperforming benchmark moderators by achieving a Macro F1 score of 0.89, where\nOpenAI Moderator and Llama Guard score 0.77 and 0.74, respectively. This\nresearch also highlights the critical domains where LLM moderators consistently\nunderperformed, pressing the need to incorporate more heterogeneous and\nrepresentative data with human-in-the-loop, for better model robustness and\nexplainability.", "AI": {"tldr": "LLM \u5728\u5185\u5bb9\u5ba1\u6838\u65b9\u9762\u4ecd\u6709\u5c40\u9650\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9690\u6666\u504f\u89c1\u548c\u9053\u5fb7\u63a8\u7406\u65f6\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86 SafePhi \u6a21\u578b\uff0c\u5b83\u5728\u5ba1\u6838\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4ecd\u9700\u6539\u8fdb\u4ee5\u63d0\u9ad8\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u968f\u7740 AI \u7cfb\u7edf\u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u5bf9\u66f4\u5b89\u5168\u3001\u66f4\u53ef\u9760\u7684\u5185\u5bb9\u5ba1\u6838\u7684\u9700\u6c42\u65e5\u76ca\u589e\u957f\u3002\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u5728\u590d\u6742\u6027\u548c\u6027\u80fd\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u5728\u6d89\u53ca\u9053\u5fb7\u63a8\u7406\u3001\u9690\u6666\u504f\u89c1\u548c\u6027\u522b\u6b67\u89c6\u7b49\u7ec6\u5fae\u5dee\u522b\u7684\u95ee\u9898\u4e0a\uff0cLLM \u4ecd\u7136\u5bb9\u6613\u51fa\u9519\u5e76\u53ef\u80fd\u5f3a\u5316\u793e\u4f1a\u504f\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e SOTA \u6a21\u578b\u7684\u5b9e\u9a8c\u6846\u67b6\uff0c\u5e76\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 49 \u4e2a\u7c7b\u522b\u7684\u7edf\u4e00\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u7c7b\u60c5\u611f\u3001\u5192\u72af\u6027\u6587\u672c\u4ee5\u53ca\u6027\u522b\u548c\u79cd\u65cf\u504f\u89c1\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f15\u5165\u4e86\u7ecf\u8fc7 QLoRA \u5fae\u8c03\u7684 SafePhi \u6a21\u578b\u3002", "result": "SafePhi \u6a21\u578b\u5728\u5ba1\u6838\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86 0.89 \u7684 Macro F1 \u5206\u6570\uff0c\u663e\u8457\u4f18\u4e8e OpenAI Moderator (0.77) \u548c Llama Guard (0.74)\u3002\u7814\u7a76\u8fd8\u63ed\u793a\u4e86 LLM \u5ba1\u6838\u5458\u8868\u73b0\u4e0d\u4f73\u7684\u5173\u952e\u9886\u57df\uff0c\u5f3a\u8c03\u4e86\u6574\u5408\u66f4\u591a\u6837\u5316\u3001\u66f4\u5177\u4ee3\u8868\u6027\u7684\u6570\u636e\u4ee5\u53ca\u5f15\u5165\u4eba\u5de5\u5e72\u9884\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "LLM \u5ba1\u6838\u5458\u5728\u5904\u7406\u9690\u6666\u7684\u4ec7\u6068\u8a00\u8bba\u3001\u5192\u72af\u6027\u8bed\u8a00\u548c\u6027\u522b\u504f\u89c1\u65b9\u9762\u4ecd\u5b58\u5728\u4e0d\u8db3\uff0c\u5e76\u4e14\u53ef\u80fd\u56e0\u8bad\u7ec3\u6570\u636e\u800c\u52a0\u5267\u793e\u4f1a\u504f\u89c1\u3002\u5c3d\u7ba1 SafePhi \u6a21\u578b\u5728\u9053\u5fb7\u80cc\u666f\u9002\u5e94\u548c\u6027\u80fd\u4e0a\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff08Macro F1 \u5206\u6570\u4e3a 0.89\uff09\uff0c\u4f46\u4ecd\u9700\u7eb3\u5165\u66f4\u591a\u6837\u5316\u548c\u5177\u6709\u4ee3\u8868\u6027\u7684\u6570\u636e\uff0c\u5e76\u7ed3\u5408\u4eba\u5de5\u5e72\u9884\uff0c\u4ee5\u63d0\u9ad8\u6a21\u578b\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.06813", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06813", "abs": "https://arxiv.org/abs/2508.06813", "authors": ["Brendan R. Hogan", "Will Brown", "Adel Boyarsky", "Anderson Schneider", "Yuriy Nevmyvaka"], "title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language", "comment": "40 pages", "summary": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.", "AI": {"tldr": "\u4e3aQ\u7f16\u7a0b\u8bed\u8a00\u9002\u914dLLM\uff0c\u6a21\u578b\u6027\u80fd\u5927\u5e45\u63d0\u5347\uff0c\u65b9\u6cd5\u53ef\u63a8\u5e7f\u81f3\u5176\u4ed6\u5c0f\u4f17\u9886\u57df\u3002", "motivation": "\u76ee\u524d\u7684LLM\u5728\u4e92\u8054\u7f51\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u9886\u57df\uff08\u5982Q\u7f16\u7a0b\u8bed\u8a00\uff09\u8868\u73b0\u4e0d\u4f73\uff0c\u9700\u8981\u4e13\u95e8\u7684\u9002\u914d\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u9884\u8bad\u7ec3\u3001\u76d1\u7763\u5fae\u8c03\u548c\u5f3a\u5316\u5b66\u4e60\uff0c\u5728Q\u8bed\u8a00Leetcode\u98ce\u683c\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u4e86\u57fa\u4e8eQwen-2.5\u7cfb\u5217\u6a21\u578b\u7684\u591a\u4e2a\u7248\u672c\uff0c\u5e76\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u5728Q\u8bed\u8a00\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6700\u4f73\u6a21\u578bpass@1\u51c6\u786e\u7387\u8fbe\u523059%\uff0c\u8d85\u8d8a\u4e86Claude Opus-4\uff0c\u6240\u6709\u6a21\u578b\u5747\u4f18\u4e8eGPT-4.1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9Q\u8bed\u8a00\u7684LLM\u9002\u914d\u65b9\u6cd5\uff0c\u5e76\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff0c\u4e3a\u5176\u4ed6\u5c0f\u4f17\u9886\u57dfLLM\u5e94\u7528\u63d0\u4f9b\u4e86\u501f\u9274\u3002"}}
{"id": "2508.07687", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07687", "abs": "https://arxiv.org/abs/2508.07687", "authors": ["A. Magar", "K. Somesh", "M. P. Saravanan", "J. Sichelschmidt", "Y. Skourski", "M. T. F. Telling", "V. A. Ginga", "A. A. Tsirlin", "R. Nath"], "title": "Proximate spin-liquid behavior in the double trillium lattice antiferromagnet K$_2$Co$_2$(SO$_4$)$_3$", "comment": "7 pages, 4 figures", "summary": "We report proximate quantum spin liquid behavior in K$_2$Co$_2$(SO$_4$)$_3$\nwith the magnetic Co$^{2+}$ ions embedded on a highly frustrated\nthree-dimensional double trillium lattice. Single-crystal and high-resolution\nsynchrotron powder x-ray diffraction experiments reveal a structural phase\ntransition at $T_{\\rm t} \\simeq 125$ K from high-temperature cubic to\nlow-temperature monoclinic phase with the three-fold superstructure.\nMagnetization and heat capacity consistently show the formation of the $J_{\\rm\neff} =1/2$ state of Co$^{2+}$ below 50 K. In zero field,\nK$_2$Co$_2$(SO$_4$)$_3$ shows signatures of static magnetic order formed below\n$T^* \\simeq 0.6$ K, but muon spin relaxation experiments reveal a large\nfluctuating component that persists down to at least 50 mK, reminiscent of\nquantum spin liquid (QSL). Static order is completely suppressed in the small\nmagnetic field of $\\sim 1$ T, and low-temperature heat capacity demonstrates\nthe $T^2$ behavior above this field, another fingerprint of QSL. Ab initio\ncalculations show a competition of several antiferromagnetic couplings that\nrender K$_2$Co$_2$(SO$_4$)$_3$ a promising pseudospin-$\\frac12$ material for\nstudying quantum magnetism in the double trillium lattice geometry.", "AI": {"tldr": "K$_{2}$Co$_{2}$(SO$_{4}$)$_{3}$\u5728\u53cc\u4e09\u53f6\u8349\u6676\u683c\u4e2d\u8868\u73b0\u51fa\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u884c\u4e3a\uff0c\u57281T\u78c1\u573a\u4e0b\uff0c\u5176\u4f4e\u6e29\u70ed\u5bb9\u5448T$^{2}$\u884c\u4e3a\u3002", "motivation": "\u5728\u9ad8\u5ea6\u632b\u6298\u7684\u4e09\u7ef4\u53cc\u4e09\u53f6\u8349\u6676\u683c\u4e2d\uff0c\u78c1\u6027Co$^{2+}$\u79bb\u5b50\u8868\u73b0\u51fa\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u884c\u4e3a\u3002", "method": "\u672c\u6587\u901a\u8fc7\u5355\u6676\u548c\u9ad8\u5206\u8fa8\u7387\u540c\u6b65\u8f90\u5c04\u7c89\u672bX\u5c04\u7ebf\u884d\u5c04\u5b9e\u9a8c\u3001\u78c1\u5316\u5f3a\u5ea6\u548c\u70ed\u5bb9\u6d4b\u91cf\u4ee5\u53ca\u03bc\u5b50\u81ea\u65cb\u5f1b\u8c6b\u5b9e\u9a8c\uff0c\u5e76\u7ed3\u5408\u4ece\u5934\u7b97\u65b9\u6cd5\uff0c\u5206\u6790\u4e86K$_{2}$Co$_{2}$(SO$_{4}$)$_{3}$\u7684\u7ed3\u6784\u3001\u78c1\u6027\u548c\u91cf\u5b50\u78c1\u884c\u4e3a\u3002", "result": "\u57281 T\u7684\u5c0f\u78c1\u573a\u4e0b\uff0c\u9759\u6001\u78c1\u6709\u5e8f\u88ab\u5b8c\u5168\u6291\u5236\uff0c\u5e76\u4e14\u5728\u8be5\u78c1\u573a\u4ee5\u4e0a\u7684\u4f4e\u6e29\u70ed\u5bb9\u8868\u73b0\u51faT$^{2}$\u884c\u4e3a\uff0c\u8fd9\u662f\u91cf\u5b50\u81ea\u65cb\u6db2\u4f53\u7684\u53e6\u4e00\u4e2a\u7279\u5f81\u3002\u4ece\u5934\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u591a\u79cd\u53cd\u94c1\u78c1\u8026\u5408\u7684\u7ade\u4e89\u4f7f\u5f97K$_{2}$Co$_{2}$(SO$_{4}$)$_{3}$\u6210\u4e3a\u7814\u7a76\u53cc\u4e09\u53f6\u8349\u6676\u683c\u51e0\u4f55\u4e2d\u91cf\u5b50\u78c1\u6027\u7684\u6709\u5e0c\u671b\u7684\u8d5d\u81ea\u65cb1/2\u6750\u6599\u3002", "conclusion": "K$_{2}$Co$_{2}$(SO$_{4}$)$_{3}$ \u662f\u4e00\u79cd\u6709\u6f5c\u529b\u7684\u8d5d\u81ea\u65cb1/2\u6750\u6599\uff0c\u7528\u4e8e\u7814\u7a76\u53cc\u4e09\u53f6\u8349\u6676\u683c\u51e0\u4f55\u4e2d\u7684\u91cf\u5b50\u78c1\u6027\u3002"}}
{"id": "2508.07319", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07319", "abs": "https://arxiv.org/abs/2508.07319", "authors": ["Yanzhao Yu", "Haotian Yang", "Junbo Tan", "Xueqian Wang"], "title": "A Hybrid Force-Position Strategy for Shape Control of Deformable Linear Objects With Graph Attention Networks", "comment": null, "summary": "Manipulating deformable linear objects (DLOs) such as wires and cables is\ncrucial in various applications like electronics assembly and medical\nsurgeries. However, it faces challenges due to DLOs' infinite degrees of\nfreedom, complex nonlinear dynamics, and the underactuated nature of the\nsystem. To address these issues, this paper proposes a hybrid force-position\nstrategy for DLO shape control. The framework, combining both force and\nposition representations of DLO, integrates state trajectory planning in the\nforce space and Model Predictive Control (MPC) in the position space. We\npresent a dynamics model with an explicit action encoder, a property extractor\nand a graph processor based on Graph Attention Networks. The model is used in\nthe MPC to enhance prediction accuracy. Results from both simulations and\nreal-world experiments demonstrate the effectiveness of our approach in\nachieving efficient and stable shape control of DLOs. Codes and videos are\navailable at https://sites.google.com/view/dlom.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u529b\u7a7a\u95f4\u8f68\u8ff9\u89c4\u5212\u548c\u4f4d\u7a7a\u95f4\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\u7684\u6df7\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u6765\u6539\u8fdbDLO\u7684\u5f62\u72b6\u63a7\u5236\uff0c\u5e76\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u7535\u5b50\u7ec4\u88c5\u548c\u5916\u79d1\u624b\u672f\u7b49\u5e94\u7528\u4e2d\uff0c\u64cd\u4f5c\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u5982\u7535\u7ebf\u548c\u7535\u7f06\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7531\u4e8e\u5176\u65e0\u9650\u7684\u81ea\u7531\u5ea6\u3001\u590d\u6742\u7684\u975e\u7ebf\u6027\u52a8\u529b\u5b66\u4ee5\u53ca\u7cfb\u7edf\u7684\u975e\u5145\u5206\u9a71\u52a8\u7b49\u6311\u6218\uff0c\u4f7f\u5f97DLO\u7684\u64cd\u4f5c\u9762\u4e34\u56f0\u96be\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u529b-\u4f4d\u7b56\u7565\uff0c\u7ed3\u5408\u4e86DLO\u7684\u529b\u548c\u4f4d\u8868\u793a\uff0c\u5e76\u5728\u529b\u7a7a\u95f4\u4e2d\u6574\u5408\u4e86\u72b6\u6001\u8f68\u8ff9\u89c4\u5212\uff0c\u5728\u4f4d\u7a7a\u95f4\u4e2d\u4f7f\u7528\u4e86\u57fa\u4e8e\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\u7684\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08MPC\uff09\uff0c\u8be5\u6a21\u578b\u5305\u542b\u663e\u5f0f\u7684\u52a8\u4f5c\u7f16\u7801\u5668\u3001\u5c5e\u6027\u63d0\u53d6\u5668\u548c\u56fe\u5904\u7406\u5668\uff0c\u4ee5\u63d0\u9ad8\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "result": "\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u7684\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5b9e\u73b0DLO\u7684\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u5f62\u72b6\u63a7\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6df7\u5408\u529b-\u4f4d\u6df7\u5408\u7b56\u7565\u5728\u6a21\u62df\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u90fd\u8bc1\u660e\u4e86\u5176\u5728\u4e00\u4e2a\u53ef\u53d8\u5f62\u7ebf\u6027\u7269\u4f53\uff08DLO\uff09\u7684\u5f62\u72b6\u63a7\u5236\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u7a33\u5b9a\u7684\u63a7\u5236\u3002"}}
{"id": "2508.07491", "categories": ["quant-ph", "cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07491", "abs": "https://arxiv.org/abs/2508.07491", "authors": ["O. G. Udalov"], "title": "A Method for Constructing Quasi-Random Peaked Quantum Circuits", "comment": "17 pages, 10 figures", "summary": "An algorithm is proposed for constructing quasi-random \"peaked\" quantum\ncircuits, i.e., circuits whose final qubit state exhibits a high probability\nconcentration on a specific computational basis state. These circuits consist\nof random gates arranged in a brick-wall architecture. While the multiqubit\nstate in the middle of the circuit can exhibit significant entanglement, the\nfinal state is, with high probability, a predetermined pure bitstring. A\ntechnique is introduced to obscure the final bitstring in the structure of the\nquantum circuit. The algorithm allows precise control over the probability of\nthe final peaked state. A modified version of the algorithm enables the\nconstruction of double- or multi-peaked quantum circuits. The matrix product\nstate (MPS) method is evaluated for simulating such circuits; it performs\neffectively for shallow peaked circuits but offers no significant advantage for\ndeeper ones.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u201c\u5cf0\u503c\u201d\u91cf\u5b50\u7535\u8def\u7684\u7b97\u6cd5\uff0c\u8be5\u7535\u8def\u7684\u6700\u7ec8\u72b6\u6001\u9ad8\u5ea6\u96c6\u4e2d\u4e8e\u7279\u5b9a\u6bd4\u7279\u4e32\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u85cf\u8be5\u6bd4\u7279\u4e32\u7684\u6280\u672f\u3002MPS\u65b9\u6cd5\u5728\u6a21\u62df\u6d45\u5c42\u7535\u8def\u65b9\u9762\u6548\u679c\u8f83\u597d\u3002", "motivation": "\u63d0\u51fa\u7b97\u6cd5\u4ee5\u6784\u5efa\u6700\u7ec8\u91cf\u5b50\u6bd4\u7279\u72b6\u6001\u5728\u7279\u5b9a\u8ba1\u7b97\u57fa\u6001\u4e0a\u5177\u6709\u9ad8\u6982\u7387\u96c6\u4e2d\u5ea6\u7684\u201c\u5cf0\u503c\u201d\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u80fd\u9690\u85cf\u6700\u7ec8\u6bd4\u7279\u5b57\u7b26\u4e32\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6784\u5efa\u51c6\u968f\u673a\u201c\u5cf0\u503c\u201d\u91cf\u5b50\u7535\u8def\u7684\u7b97\u6cd5\uff0c\u8be5\u7535\u8def\u7531\u7816\u5899\u7ed3\u6784\u4e2d\u7684\u968f\u673a\u95e8\u7ec4\u6210\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u9690\u85cf\u6700\u7ec8\u6bd4\u7279\u5b57\u7b26\u4e32\u7684\u6280\u672f\u3002", "result": "\u7b97\u6cd5\u80fd\u591f\u7cbe\u786e\u63a7\u5236\u6700\u7ec8\u5cf0\u503c\u72b6\u6001\u7684\u6982\u7387\uff0c\u6539\u8fdb\u7b97\u6cd5\u53ef\u7528\u4e8e\u6784\u5efa\u53cc\u5cf0\u6216\u591a\u5cf0\u91cf\u5b50\u7535\u8def\u3002MPS\u65b9\u6cd5\u5bf9\u4e8e\u6a21\u62df\u6d45\u5c42\u5cf0\u503c\u7535\u8def\u6709\u6548\uff0c\u4f46\u5bf9\u4e8e\u66f4\u6df1\u5c42\u7684\u7535\u8def\u6ca1\u6709\u663e\u8457\u4f18\u52bf\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u80fd\u591f\u6784\u5efa\u5177\u6709\u9ad8\u6982\u7387\u96c6\u4e2d\u5728\u7279\u5b9a\u8ba1\u7b97\u57fa\u6001\u7684\u201c\u5cf0\u503c\u201d\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u5141\u8bb8\u7cbe\u786e\u63a7\u5236\u6700\u7ec8\u5cf0\u503c\u72b6\u6001\u7684\u6982\u7387\u3002\u6539\u8fdb\u7b97\u6cd5\u53ef\u7528\u4e8e\u6784\u5efa\u53cc\u5cf0\u6216\u591a\u5cf0\u91cf\u5b50\u7535\u8def\u3002"}}
{"id": "2508.07172", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07172", "abs": "https://arxiv.org/abs/2508.07172", "authors": ["Biao Yi", "Jiahao Li", "Baolei Zhang", "Lihai Nie", "Tong Li", "Tiansheng Huang", "Zheli Liu"], "title": "Gradient Surgery for Safe LLM Fine-Tuning", "comment": null, "summary": "Fine-tuning-as-a-Service introduces a critical vulnerability where a few\nmalicious examples mixed into the user's fine-tuning dataset can compromise the\nsafety alignment of Large Language Models (LLMs). While a recognized paradigm\nframes safe fine-tuning as a multi-objective optimization problem balancing\nuser task performance with safety alignment, we find existing solutions are\ncritically sensitive to the harmful ratio, with defenses degrading sharply as\nharmful ratio increases. We diagnose that this failure stems from conflicting\ngradients, where the user-task update directly undermines the safety objective.\nTo resolve this, we propose SafeGrad, a novel method that employs gradient\nsurgery. When a conflict is detected, SafeGrad nullifies the harmful component\nof the user-task gradient by projecting it onto the orthogonal plane of the\nalignment gradient, allowing the model to learn the user's task without\nsacrificing safety. To further enhance robustness and data efficiency, we\nemploy a KL-divergence alignment loss that learns the rich, distributional\nsafety profile of the well-aligned foundation model. Extensive experiments show\nthat SafeGrad provides state-of-the-art defense across various LLMs and\ndatasets, maintaining robust safety even at high harmful ratios without\ncompromising task fidelity.", "AI": {"tldr": "SafeGrad \u662f\u4e00\u79cd\u65b0\u7684 LLM \u5b89\u5168\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u68af\u5ea6\u624b\u672f\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u5927\u91cf\u6076\u610f\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u9632\u5fa1\uff0c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709 LLM \u5fae\u8c03\u670d\u52a1\u5b58\u5728\u5b89\u5168\u6f0f\u6d1e\uff0c\u5c11\u91cf\u6076\u610f\u6837\u672c\u5373\u53ef\u635f\u5bb3\u6a21\u578b\u7684\u5b89\u5168\u5bf9\u9f50\u3002\u73b0\u6709\u89e3\u51b3\u65b9\u6848\u5bf9\u6709\u5bb3\u6bd4\u4f8b\u654f\u611f\uff0c\u4e14\u968f\u7740\u6709\u5bb3\u6bd4\u4f8b\u589e\u52a0\u9632\u5fa1\u6548\u679c\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u8fd9\u662f\u56e0\u4e3a\u7528\u6237\u4efb\u52a1\u66f4\u65b0\u76f4\u63a5\u7834\u574f\u4e86\u5b89\u5168\u76ee\u6807\u3002", "method": "SafeGrad \u901a\u8fc7\u68af\u5ea6\u624b\u672f\u6765\u89e3\u51b3\u68af\u5ea6\u51b2\u7a81\u95ee\u9898\uff0c\u5f53\u68c0\u6d4b\u5230\u51b2\u7a81\u65f6\uff0c\u901a\u8fc7\u5c06\u7528\u6237\u4efb\u52a1\u68af\u5ea6\u6295\u5f71\u5230\u5bf9\u9f50\u68af\u5ea6\u7684\u6b63\u4ea4\u5e73\u9762\u4e0a\u6765\u62b5\u6d88\u5176\u6709\u5bb3\u90e8\u5206\uff0c\u4ece\u800c\u4f7f\u6a21\u578b\u80fd\u591f\u5b66\u4e60\u7528\u6237\u4efb\u52a1\u800c\u4e0d\u727a\u7272\u5b89\u5168\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e86 KL \u6563\u5ea6\u5bf9\u9f50\u635f\u5931\u6765\u5b66\u4e60\u4e0e\u5bf9\u9f50\u7684\u57fa\u7ebf\u6a21\u578b\u4e30\u5bcc\u7684\u5206\u5e03\u5b89\u5168\u914d\u7f6e\u6587\u4ef6\uff0c\u4ee5\u589e\u5f3a\u9c81\u68d2\u6027\u548c\u6570\u636e\u6548\u7387\u3002", "result": "SafeGrad \u5728\u5404\u79cd LLM \u548c\u6570\u636e\u96c6\u4e0a\u63d0\u4f9b\u4e86\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u6548\u679c\uff0c\u5373\u4f7f\u5728\u6709\u5bb3\u6bd4\u4f8b\u5f88\u9ad8\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\u5b89\u5168\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u4efb\u52a1\u4fdd\u771f\u5ea6\u3002", "conclusion": "SafeGrad \u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u68af\u5ea6\u624b\u672f\u6765\u89e3\u51b3 LLM \u5b89\u5168\u6027\u5bf9\u9f50\u95ee\u9898\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5728\u5404\u79cd LLM \u548c\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u9632\u5fa1\u6548\u679c\uff0c\u5373\u4f7f\u5728\u6709\u5bb3\u6bd4\u4f8b\u5f88\u9ad8\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u4fdd\u6301\u9c81\u68d2\u6027\u5b89\u5168\uff0c\u540c\u65f6\u4e0d\u635f\u5bb3\u4efb\u52a1\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2508.06703", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06703", "abs": "https://arxiv.org/abs/2508.06703", "authors": ["Justin London"], "title": "Fourier Optics and Deep Learning Methods for Fast 3D Reconstruction in Digital Holography", "comment": null, "summary": "Computer-generated holography (CGH) is a promising method that modulates\nuser-defined waveforms with digital holograms. An efficient and fast pipeline\nframework is proposed to synthesize CGH using initial point cloud and MRI data.\nThis input data is reconstructed into volumetric objects that are then input\ninto non-convex Fourier optics optimization algorithms for phase-only hologram\n(POH) and complex-hologram (CH) generation using alternating projection, SGD,\nand quasi-Netwton methods. Comparison of reconstruction performance of these\nalgorithms as measured by MSE, RMSE, and PSNR is analyzed as well as to HoloNet\ndeep learning CGH. Performance metrics are shown to be improved by using 2D\nmedian filtering to remove artifacts and speckled noise during optimization.", "AI": {"tldr": "\u4e00\u79cd\u7528\u4e8eCGH\u5408\u6210\u7684\u5feb\u901f\u6846\u67b6\uff0c\u4f7f\u7528\u70b9\u4e91\u548cMRI\u6570\u636e\uff0c\u901a\u8fc7\u5085\u7acb\u53f6\u5149\u5b66\u4f18\u5316\u7b97\u6cd5\u751f\u6210\u5168\u606f\u56fe\uff0c\u5e76\u4f7f\u75282D\u4e2d\u503c\u6ee4\u6ce2\u63d0\u9ad8\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8ba1\u7b97\u673a\u751f\u6210\u5168\u606f\u56fe\uff08CGH\uff09\u7684\u6548\u7387\u548c\u901f\u5ea6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6846\u67b6\u3002", "method": "\u8be5\u6846\u67b6\u5229\u7528\u521d\u59cb\u70b9\u4e91\u548cMRI\u6570\u636e\uff0c\u5c06\u8f93\u5165\u6570\u636e\u91cd\u5efa\u4e3a\u4f53\u79ef\u5bf9\u8c61\uff0c\u7136\u540e\u8f93\u5165\u5230\u975e\u51f8\u5085\u7acb\u53f6\u5149\u5b66\u4f18\u5316\u7b97\u6cd5\u4e2d\uff0c\u4f7f\u7528\u4ea4\u66ff\u6295\u5f71\u3001SGD\u548c\u62df\u725b\u987f\u6cd5\u6765\u751f\u6210\u4ec5\u76f8\u4f4d \ud640\ub85c\uadf8\ub7a8\uff08POH\uff09\u548c\u590d\u6570\ud640\ub85c\uadf8\ub7a8\uff08CH\uff09\u3002", "result": "\u5bf9\u4f7f\u7528\u4ea4\u66ff\u6295\u5f71\u3001SGD\u548c\u62df\u725b\u987f\u6cd5\u751f\u6210\u7684POH\u548cCH\u7684\u91cd\u5efa\u6027\u80fd\u8fdb\u884c\u4e86\u5206\u6790\uff0c\u5e76\u4e0eHoloNet\u6df1\u5ea6\u5b66\u4e60CGH\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u75282D\u4e2d\u503c\u6ee4\u6ce2\u53bb\u9664\u4f18\u5316\u8fc7\u7a0b\u4e2d\u7684\u4f2a\u5f71\u548c\u6563\u6591\u566a\u58f0\uff0c\u53ef\u4ee5\u63d0\u9ad8\u6027\u80fd\u6307\u6807\u3002"}}
{"id": "2508.07971", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07971", "abs": "https://arxiv.org/abs/2508.07971", "authors": ["Chiara Cignarella", "Lorenzo Bastonero", "Lorenzo Monacelli", "Nicola Marzari"], "title": "Extreme anharmonicity and thermal contraction of 1D wires", "comment": null, "summary": "Ultrathin nanowires could play a central role in next-generation downscaled\nelectronics. Here, we explore some of the most promising candidates identified\nfrom previous high-throughput screening: CuC$_2$, TaSe$_3$, and AuSe$_2$, to\ngain insight into the thermodynamic and anharmonic behaviors of nanowires that\ncould be exfoliated from weakly-bonded three-dimensional materials. We analyze\nthermal stability, linear thermal expansion, and anharmonic heat capacity using\nthe stochastic self-consistent harmonic approximation. Notably, our work\nunveils exotic features common among all the 1D wires: a colossal record\nnegative thermal expansion and very large deviations from the Dulong-Petit law\ndue to strong anharmonicity.", "AI": {"tldr": "Ultrathin nanowires (CuC2, TaSe3, AuSe2) show record negative thermal expansion and large deviations from standard heat capacity laws due to strong anharmonicity, suggesting potential for next-gen electronics.", "motivation": "To gain insight into the thermodynamic and anharmonic behaviors of ultrathin nanowires (CuC2, TaSe3, and AuSe2) exfoliated from weakly-bonded three-dimensional materials for next-generation electronics.", "method": "Stochastic self-consistent harmonic approximation was used to analyze thermal stability, linear thermal expansion, and anharmonic heat capacity.", "result": "The study revealed common exotic features among the 1D wires, specifically a colossal record negative thermal expansion and very large deviations from the Dulong-Petit law attributed to strong anharmonicity.", "conclusion": "Ultrathin nanowires such as CuC2, TaSe3, and AuSe2 exhibit exotic features including colossal negative thermal expansion and large deviations from the Dulong-Petit law due to strong anharmonicity."}}
{"id": "2508.07107", "categories": ["cs.AI", "cs.CY", "K.3.1; I.2.6; H.4"], "pdf": "https://arxiv.org/pdf/2508.07107", "abs": "https://arxiv.org/abs/2508.07107", "authors": ["Timothy Oluwapelumi Adeyemi", "Nadiah Fahad AlOtaibi"], "title": "Designing a Feedback-Driven Decision Support System for Dynamic Student Intervention", "comment": "10 pages, 1 figure, 3 tables", "summary": "Accurate prediction of student performance is essential for timely academic\nintervention. However, most machine learning models in education are static and\ncannot adapt when new data, such as post-intervention outcomes, become\navailable. To address this limitation, we propose a Feedback-Driven Decision\nSupport System (DSS) with a closed-loop architecture that enables continuous\nmodel refinement. The system integrates a LightGBM-based regressor with\nincremental retraining, allowing educators to input updated student results,\nwhich automatically trigger model updates. This adaptive mechanism improves\nprediction accuracy by learning from real-world academic progress. The platform\nfeatures a Flask-based web interface for real-time interaction and incorporates\nSHAP for explainability, ensuring transparency. Experimental results show a\n10.7\\% reduction in RMSE after retraining, with consistent upward adjustments\nin predicted scores for intervened students. By transforming static predictors\ninto self-improving systems, our approach advances educational analytics toward\nhuman-centered, data-driven, and responsive AI. The framework is designed for\nintegration into LMS and institutional dashboards.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u5177\u6709\u95ed\u73af\u67b6\u6784\u7684\u53cd\u9988\u9a71\u52a8\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08DSS\uff09\uff0c\u7528\u4e8e\u6301\u7eed\u4f18\u5316\u6a21\u578b\u3002\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u518d\u8bad\u7ec3\u540eRMSE\u964d\u4f4e\u4e8610.7%\uff0c\u5e76\u4e14\u5e72\u9884\u540e\u5b66\u751f\u9884\u6d4b\u5206\u6570\u7684\u8c03\u6574\u6301\u7eed\u5411\u4e0a\u3002", "motivation": "\u6559\u80b2\u9886\u57df\u5927\u591a\u6570\u673a\u5668\u5b66\u4e60\u6a21\u578b\u90fd\u662f\u9759\u6001\u7684\uff0c\u5f53\u6709\u65b0\u7684\u6570\u636e\uff08\u5982\u5e72\u9884\u540e\u7684\u7ed3\u679c\uff09\u53ef\u7528\u65f6\uff0c\u5b83\u4eec\u65e0\u6cd5\u8fdb\u884c\u8c03\u6574\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5177\u6709\u95ed\u73af\u67b6\u6784\u7684\u53cd\u9988\u9a71\u52a8\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\uff08DSS\uff09\uff0c\u8be5\u67b6\u6784\u80fd\u591f\u8fdb\u884c\u6301\u7eed\u7684\u6a21\u578b\u4f18\u5316\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86\u57fa\u4e8eLightGBM\u7684\u56de\u5f52\u5668\u548c\u589e\u91cf\u518d\u8bad\u7ec3\uff0c\u5141\u8bb8\u6559\u80b2\u5de5\u4f5c\u8005\u8f93\u5165\u66f4\u65b0\u7684\u5b66\u751f\u7ed3\u679c\uff0c\u81ea\u52a8\u89e6\u53d1\u6a21\u578b\u66f4\u65b0\u3002\u8be5\u5e73\u53f0\u5177\u6709\u57fa\u4e8eFlask\u7684Web\u754c\u9762\uff0c\u7528\u4e8e\u5b9e\u65f6\u4ea4\u4e92\uff0c\u5e76\u7ed3\u5408\u4e86SHAP\u4ee5\u63d0\u9ad8\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u663e\u793a\uff0c\u518d\u8bad\u7ec3\u540eRMSE\u964d\u4f4e\u4e8610.7%\uff0c\u5e76\u4e14\u5e72\u9884\u540e\u5b66\u751f\u9884\u6d4b\u5206\u6570\u7684\u8c03\u6574\u6301\u7eed\u5411\u4e0a\u3002", "conclusion": "\u901a\u8fc7\u5c06\u9759\u6001\u9884\u6d4b\u6a21\u578b\u8f6c\u53d8\u4e3a\u80fd\u591f\u81ea\u6211\u6539\u8fdb\u7684\u7cfb\u7edf\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u63a8\u52a8\u6559\u80b2\u5206\u6790\u671d\u7740\u4ee5\u4eba\u4e3a\u672c\u3001\u6570\u636e\u9a71\u52a8\u548c\u54cd\u5e94\u8fc5\u901f\u7684\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u3002\u8be5\u6846\u67b6\u65e8\u5728\u4e0e\u5b66\u4e60\u7ba1\u7406\u7cfb\u7edf\u548c\u673a\u6784\u4eea\u8868\u677f\u96c6\u6210\u3002"}}
{"id": "2508.06827", "categories": ["cs.LG", "cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2508.06827", "abs": "https://arxiv.org/abs/2508.06827", "authors": ["Ishwar Balappanawar", "Venkata Hasith Vattikuti", "Greta Kintzley", "Ronan Azimi-Mancel", "Satvik Golechha"], "title": "Who's the Evil Twin? Differential Auditing for Undesired Behavior", "comment": "main section: 8 pages, 4 figures, 1 table total: 34 pages, 44\n  figures, 12 tables", "summary": "Detecting hidden behaviors in neural networks poses a significant challenge\ndue to minimal prior knowledge and potential adversarial obfuscation. We\nexplore this problem by framing detection as an adversarial game between two\nteams: the red team trains two similar models, one trained solely on benign\ndata and the other trained on data containing hidden harmful behavior, with the\nperformance of both being nearly indistinguishable on the benign dataset. The\nblue team, with limited to no information about the harmful behaviour, tries to\nidentify the compromised model. We experiment using CNNs and try various blue\nteam strategies, including Gaussian noise analysis, model diffing, integrated\ngradients, and adversarial attacks under different levels of hints provided by\nthe red team. Results show high accuracy for adversarial-attack-based methods\n(100\\% correct prediction, using hints), which is very promising, whilst the\nother techniques yield more varied performance. During our LLM-focused rounds,\nwe find that there are not many parallel methods that we could apply from our\nstudy with CNNs. Instead, we find that effective LLM auditing methods require\nsome hints about the undesired distribution, which can then used in standard\nblack-box and open-weight methods to probe the models further and reveal their\nmisalignment. We open-source our auditing games (with the model and data) and\nhope that our findings contribute to designing better audits.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u6297\u6027\u535a\u5f08\u6765\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u4e2d\u9690\u85cf\u884c\u4e3a\u7684\u65b9\u6cd5\uff0c\u5e76\u5bf9CNN\u548cLLM\u8fdb\u884c\u4e86\u5b9e\u9a8c\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u57fa\u4e8e\u5bf9\u6297\u6027\u653b\u51fb\u7684\u65b9\u6cd5\u5728\u6709\u63d0\u793a\u7684\u60c5\u51b5\u4e0b\u6548\u679c\u6700\u4f73\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e3aLLM\u8bbe\u8ba1\u4e13\u95e8\u5ba1\u8ba1\u65b9\u6cd5\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u7531\u4e8e\u5148\u9a8c\u77e5\u8bc6\u6781\u5c11\u4e14\u53ef\u80fd\u5b58\u5728\u5bf9\u6297\u6027\u6df7\u6dc6\uff0c\u56e0\u6b64\u68c0\u6d4b\u795e\u7ecf\u7f51\u7edc\u4e2d\u9690\u85cf\u884c\u4e3a\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u5c06\u68c0\u6d4b\u95ee\u9898\u6784\u5efa\u4e3a\u4e00\u4e2a\u5bf9\u6297\u6027\u535a\u5f08\uff0c\u7ea2\u961f\u8bad\u7ec3\u4e24\u4e2a\u76f8\u4f3c\u4f46\u5176\u4e2d\u4e00\u4e2a\u5305\u542b\u9690\u85cf\u6709\u5bb3\u884c\u4e3a\u7684\u6a21\u578b\uff0c\u84dd\u961f\u5219\u5728\u4fe1\u606f\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u5c1d\u8bd5\u8bc6\u522b\u53d7\u635f\u6a21\u578b\u3002\u7814\u7a76\u4eba\u5458\u5c1d\u8bd5\u4e86\u591a\u79cd\u84dd\u961f\u7b56\u7565\uff0c\u5305\u62ec\u9ad8\u65af\u566a\u58f0\u5206\u6790\u3001\u6a21\u578b\u5dee\u5f02\u5206\u6790\u3001\u96c6\u6210\u68af\u5ea6\u548c\u5bf9\u6297\u6027\u653b\u51fb\u3002", "result": "\u57fa\u4e8e\u5bf9\u6297\u6027\u653b\u51fb\u7684\u65b9\u6cd5\u5728\u63d0\u4f9b\u63d0\u793a\u65f6\u80fd\u8fbe\u5230100%\u7684\u51c6\u786e\u7387\uff0c\u8868\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u800c\u5176\u4ed6\u6280\u672f\u8868\u73b0\u5219\u53c2\u5dee\u4e0d\u9f50\u3002\u5728LLM\u65b9\u9762\uff0c\u7814\u7a76\u53d1\u73b0\u4e0eCNN\u7814\u7a76\u7c7b\u4f3c\u7684\u65b9\u6cd5\u6709\u9650\uff0c\u6709\u6548\u7684LLM\u5ba1\u8ba1\u65b9\u6cd5\u9700\u8981\u5173\u4e8e\u4e0d\u826f\u5206\u5e03\u7684\u63d0\u793a\u3002", "conclusion": "\u9700\u8981\u9488\u5bf9LLM\u8bbe\u8ba1\u4e13\u95e8\u7684\u5ba1\u8ba1\u65b9\u6cd5\uff0c\u5e76\u4e14\u9700\u8981\u63d0\u4f9b\u4e00\u4e9b\u5173\u4e8e\u4e0d\u826f\u5206\u5e03\u7684\u63d0\u793a\uff0c\u4ee5\u4fbf\u4f7f\u7528\u6807\u51c6\u7684\u9ed1\u76d2\u6216\u767d\u76d2\u65b9\u6cd5\u8fdb\u4e00\u6b65\u63a2\u6d4b\u6a21\u578b\u3002"}}
{"id": "2508.07323", "categories": ["cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07323", "abs": "https://arxiv.org/abs/2508.07323", "authors": ["Adeetya Uppal", "Rakesh Kumar Sahoo", "Manoranjan Sinha"], "title": "Collision-Free Trajectory Planning and control of Robotic Manipulator using Energy-Based Artificial Potential Field (E-APF)", "comment": null, "summary": "Robotic trajectory planning in dynamic and cluttered environments remains a\ncritical challenge, particularly when striving for both time efficiency and\nmotion smoothness under actuation constraints. Traditional path planner, such\nas Artificial Potential Field (APF), offer computational efficiency but suffer\nfrom local minima issue due to position-based potential field functions and\noscillatory motion near the obstacles due to Newtonian mechanics. To address\nthis limitation, an Energy-based Artificial Potential Field (APF) framework is\nproposed in this paper that integrates position and velocity-dependent\npotential functions. E-APF ensures dynamic adaptability and mitigates local\nminima, enabling uninterrupted progression toward the goal. The proposed\nframework integrates E-APF with a hybrid trajectory optimizer that jointly\nminimizes jerk and execution time under velocity and acceleration constraints,\nensuring geometric smoothness and time efficiency. The entire framework is\nvalidated in simulation using the 7-degree-of-freedom Kinova Gen3 robotic\nmanipulator. The results demonstrate collision-free, smooth, time-efficient,\nand oscillation-free trajectory in the presence of obstacles, highlighting the\nefficacy of the combined trajectory optimization and real-time obstacle\navoidance approach. This work lays the foundation for future integration with\nreactive control strategies and physical hardware deployment in real-world\nmanipulation tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u80fd\u91cf\u578b\u4eba\u5de5\u52bf\u573a\uff08E-APF\uff09\u6846\u67b6\uff0c\u5e76\u7ed3\u5408\u6df7\u5408\u8f68\u8ff9\u4f18\u5316\u5668\uff0c\u89e3\u51b3\u4e86\u673a\u5668\u4eba\u8def\u5f84\u89c4\u5212\u4e2d\u7684\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u632f\u8361\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5e73\u7a33\u7684\u8fd0\u52a8\u3002", "motivation": "\u89e3\u51b3\u4f20\u7edf\u8def\u5f84\u89c4\u5212\u5668\uff08\u5982\u4eba\u5de5\u52bf\u573a\u6cd5\uff09\u5728\u52a8\u6001\u548c\u6742\u4e71\u73af\u5883\u4e2d\u5b58\u5728\u5c40\u90e8\u6781\u5c0f\u503c\u548c\u632f\u8361\u8fd0\u52a8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u80fd\u91cf\u578b\u4eba\u5de5\u52bf\u573a\uff08E-APF\uff09\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u4f4d\u7f6e\u548c\u901f\u5ea6\u76f8\u5173\u7684\u52bf\u51fd\u6570\uff0c\u5e76\u5c06\u5176\u4e0e\u6df7\u5408\u8f68\u8ff9\u4f18\u5316\u5668\u76f8\u7ed3\u5408\uff0c\u4ee5\u5728\u901f\u5ea6\u548c\u52a0\u901f\u5ea6\u7684\u7ea6\u675f\u4e0b\u5171\u540c\u6700\u5c0f\u5316\u52a0\u52a0\u901f\u5ea6\u548c\u6267\u884c\u65f6\u95f4\u3002", "result": "\u5728\u6a21\u62df\u4e2d\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u751f\u6210\u65e0\u78b0\u649e\u3001\u5e73\u7a33\u3001\u9ad8\u6548\u4e14\u65e0\u632f\u8361\u7684\u8f68\u8ff9\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u672a\u6765\u7684\u53cd\u5e94\u5f0f\u63a7\u5236\u7b56\u7565\u96c6\u6210\u548c\u5b9e\u9645\u786c\u4ef6\u90e8\u7f72\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07542", "categories": ["quant-ph", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07542", "abs": "https://arxiv.org/abs/2508.07542", "authors": ["Tony Shaska"], "title": "Graded Quantum Codes: From Weighted Algebraic Geometry to Homological Chain Complexes", "comment": null, "summary": "We introduce graded quantum codes, unifying two classes of quantum\nerror-correcting codes. The first, quantum weighted algebraic geometry (AG)\ncodes, derives from rational points on hypersurfaces in weighted projective\nspaces over finite fields. This extends classical AG codes by adding weighted\ndegrees and singularities, enabling self-orthogonal codes via the CSS method\nwith improved distances using algebraic structures and invariants like weighted\nheights.The second class arises from chain complexes of graded vector spaces,\ngeneralizing homological quantum codes to include torsion and multiple\ngradings. This produces low-density parity-check codes with parameters based on\nhomology ranks, including examples from knot invariants and quantum rotors.\n  A shared grading leads to a refined Singleton bound: $d \\leq \\frac{n - k +\n2}{2} - \\frac{\\epsilon}{2}$, where $\\epsilon > 0$ reflects entropy adjustments\nfrom geometric singularities and defects. The bound holds partially for simple\norbifolds and is supported by examples over small fields.\n  Applications include post-quantum cryptography, fault-tolerant quantum\ncomputing, and optimization via graded neural networks, linking algebraic\ngeometry, homological algebra, and quantum information.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u6e10\u8fdb\u91cf\u5b50\u7801\uff0c\u7edf\u4e00\u4e86\u4e24\u79cd\u91cf\u5b50\u7ea0\u9519\u7801\uff0c\u5f97\u5230\u4e86\u65b0\u7684\u754c\uff0c\u5e76\u5728\u5bc6\u7801\u5b66\u3001\u91cf\u5b50\u8ba1\u7b97\u548c\u4f18\u5316\u4e2d\u6709\u6f5c\u5728\u5e94\u7528\u3002", "motivation": "\u7edf\u4e00\u91cf\u5b50\u52a0\u6743AG\u7801\u548c\u540c\u6e90\u91cf\u5b50\u7801\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u5bc6\u7801\u5b66\u3001\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u548c\u4f18\u5316\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u901a\u8fc7\u5f15\u5165\u52a0\u6743\u6b21\u6570\u548c\u5947\u70b9\uff0c\u5e76\u5229\u7528\u94fe\u590d\u5f62\u548c\u540c\u8c03\u4ee3\u6570\uff0c\u6784\u5efa\u4e86\u5177\u6709\u6539\u8fdb\u6027\u80fd\u7684\u91cf\u5b50\u7ea0\u9519\u7801\u3002", "result": "\u63d0\u51fa\u4e86\u6e10\u8fdb\u91cf\u5b50\u7801\uff0c\u5f97\u5230\u4e86\u6539\u8fdb\u7684\u5355\u4f8b\u754c\uff0c\u5e76\u901a\u8fc7\u5b9e\u4f8b\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u65b0\u7684\u91cf\u5b50\u7ea0\u9519\u7801\u201c\u6e10\u8fdb\u91cf\u5b50\u7801\u201d\uff0c\u7edf\u4e00\u4e86\u91cf\u5b50\u52a0\u6743\u4ee3\u6570\u51e0\u4f55\u7801\u548c\u94fe\u590d\u5f62\u7801\u3002"}}
{"id": "2508.07173", "categories": ["cs.CL", "68T50", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.07173", "abs": "https://arxiv.org/abs/2508.07173", "authors": ["Leyi Pan", "Zheyu Fu", "Yunpeng Zhai", "Shuchang Tao", "Sheng Guan", "Shiyu Huang", "Lingzhe Zhang", "Zhaoyang Liu", "Bolin Ding", "Felix Henry", "Lijie Wen", "Aiwei Liu"], "title": "Omni-SafetyBench: A Benchmark for Safety Evaluation of Audio-Visual Large Language Models", "comment": "20 pages, 8 figures, 12 tables", "summary": "The rise of Omni-modal Large Language Models (OLLMs), which integrate visual\nand auditory processing with text, necessitates robust safety evaluations to\nmitigate harmful outputs. However, no dedicated benchmarks currently exist for\nOLLMs, and prior benchmarks designed for other LLMs lack the ability to assess\nsafety performance under audio-visual joint inputs or cross-modal safety\nconsistency. To fill this gap, we introduce Omni-SafetyBench, the first\ncomprehensive parallel benchmark for OLLM safety evaluation, featuring 24\nmodality combinations and variations with 972 samples each, including dedicated\naudio-visual harm cases. Considering OLLMs' comprehension challenges with\ncomplex omni-modal inputs and the need for cross-modal consistency evaluation,\nwe propose tailored metrics: a Safety-score based on conditional Attack Success\nRate (C-ASR) and Refusal Rate (C-RR) to account for comprehension failures, and\na Cross-Modal Safety Consistency Score (CMSC-score) to measure consistency\nacross modalities. Evaluating 6 open-source and 4 closed-source OLLMs reveals\ncritical vulnerabilities: (1) no model excels in both overall safety and\nconsistency, with only 3 models achieving over 0.6 in both metrics and top\nperformer scoring around 0.8; (2) safety defenses weaken with complex inputs,\nespecially audio-visual joints; (3) severe weaknesses persist, with some models\nscoring as low as 0.14 on specific modalities. Our benchmark and metrics\nhighlight urgent needs for enhanced OLLM safety, providing a foundation for\nfuture improvements.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u9996\u4e2a\u7528\u4e8e\u8bc4\u4f30\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08OLLMs\uff09\u5b89\u5168\u6027\u7684\u57fa\u51c6Omni-SafetyBench\u53ca\u76f8\u5173\u6307\u6807\uff0c\u5e76\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u8f93\u5165\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u7684\u5b89\u5168\u6f0f\u6d1e\u3002", "motivation": "\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08OLLMs\uff09\u7684\u5174\u8d77\u9700\u8981\u5bf9\u5176\u5b89\u5168\u6027\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\uff0c\u4f46\u73b0\u6709\u57fa\u51c6\u65e0\u6cd5\u6ee1\u8db3\u5176\u97f3\u89c6\u9891\u8054\u5408\u8f93\u5165\u548c\u8de8\u6a21\u6001\u5b89\u5168\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u9700\u6c42\u3002", "method": "\u63d0\u51faOmni-SafetyBench\u57fa\u51c6\uff0c\u5305\u542b24\u79cd\u6a21\u6001\u7ec4\u5408\u548c972\u4e2a\u6837\u672c\uff0c\u5e76\u8bbe\u8ba1\u4e86\u97f3\u89c6\u9891\u5371\u5bb3\u6848\u4f8b\u3002\u63d0\u51fa\u5b9a\u5236\u5316\u6307\u6807\uff1a\u5b89\u5168\u5f97\u5206\uff08\u57fa\u4e8e\u6761\u4ef6\u653b\u51fb\u6210\u529f\u7387C-ASR\u548c\u62d2\u7edd\u7387C-RR\uff09\u548c\u8de8\u6a21\u6001\u5b89\u5168\u4e00\u81f4\u6027\u5f97\u5206\uff08CMSC-score\uff09\u3002", "result": "\u8bc4\u4f306\u4e2a\u5f00\u6e90\u548c4\u4e2a\u95ed\u6e90OLLM\u53d1\u73b0\uff1a1.\u6ca1\u6709\u6a21\u578b\u5728\u6574\u4f53\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u5747\u8868\u73b0\u4f18\u5f02\uff1b2.\u5b89\u5168\u9632\u62a4\u5728\u590d\u6742\u8f93\u5165\u4e0b\u51cf\u5f31\uff1b3.\u6a21\u578b\u5728\u7279\u5b9a\u6a21\u6001\u4e0a\u5b58\u5728\u4e25\u91cd\u5f31\u70b9\u3002", "conclusion": "\u73b0\u6709\u7684\u7814\u7a76\u7f3a\u4e4f\u9488\u5bf9\u5168\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08OLLMs\uff09\u7684\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\uff0c\u7279\u522b\u662f\u9488\u5bf9\u97f3\u89c6\u9891\u8054\u5408\u8f93\u5165\u6216\u8de8\u6a21\u6001\u5b89\u5168\u4e00\u81f4\u6027\u7684\u8bc4\u4f30\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Omni-SafetyBench\uff0c\u8fd9\u662f\u9996\u4e2a\u9488\u5bf9OLLM\u5b89\u5168\u8bc4\u4f30\u7684\u7efc\u5408\u6027\u5e76\u884c\u57fa\u51c6\uff0c\u5305\u542b24\u79cd\u6a21\u6001\u7ec4\u5408\u548c972\u4e2a\u6837\u672c\uff0c\u5e76\u7279\u522b\u8bbe\u8ba1\u4e86\u97f3\u89c6\u9891\u5371\u5bb3\u6848\u4f8b\u3002\u6211\u4eec\u8fd8\u63d0\u51fa\u4e86\u9488\u5bf9OLLM\u7406\u89e3\u590d\u6742\u5168\u6a21\u6001\u8f93\u5165\u548c\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u8bc4\u4f30\u7684\u5b9a\u5236\u5316\u6307\u6807\uff1a\u57fa\u4e8e\u6761\u4ef6\u653b\u51fb\u6210\u529f\u7387\uff08C-ASR\uff09\u548c\u62d2\u7edd\u7387\uff08C-RR\uff09\u7684\u5b89\u5168\u5f97\u5206\uff0c\u4ee5\u53ca\u8861\u91cf\u8de8\u6a21\u6001\u4e00\u81f4\u6027\u7684\u8de8\u6a21\u6001\u5b89\u5168\u4e00\u81f4\u6027\u5f97\u5206\uff08CMSC-score\uff09\u3002\u5bf96\u4e2a\u5f00\u6e90\u548c4\u4e2a\u95ed\u6e90OLLM\u7684\u8bc4\u4f30\u63ed\u793a\u4e86\u5173\u952e\u7684\u6f0f\u6d1e\uff1a\u6ca1\u6709\u6a21\u578b\u80fd\u5728\u6574\u4f53\u5b89\u5168\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4ec5\u67093\u4e2a\u6a21\u578b\u5728\u8fd9\u4e24\u9879\u6307\u6807\u4e0a\u5f97\u5206\u8d85\u8fc70.6\uff0c\u800c\u8868\u73b0\u6700\u4f73\u7684\u6a21\u578b\u5f97\u5206\u7ea6\u4e3a0.8\uff1b\u5b89\u5168\u9632\u62a4\u5728\u590d\u6742\u8f93\u5165\u4e0b\uff08\u5c24\u5176\u662f\u97f3\u89c6\u9891\u8054\u5408\u8f93\u5165\uff09\u4f1a\u51cf\u5f31\uff1b\u6a21\u578b\u5728\u7279\u5b9a\u6a21\u6001\u4e0a\u4ecd\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u90e8\u5206\u6a21\u578b\u5f97\u5206\u4f4e\u81f30.14\u3002\u672c\u57fa\u51c6\u548c\u6307\u6807\u7a81\u663e\u4e86\u589e\u5f3aOLLM\u5b89\u5168\u6027\u7684\u8feb\u5207\u9700\u6c42\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u6539\u8fdb\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.06715", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06715", "abs": "https://arxiv.org/abs/2508.06715", "authors": ["Jixuan He", "Chieh Hubert Lin", "Lu Qi", "Ming-Hsuan Yang"], "title": "Restage4D: Reanimating Deformable 3D Reconstruction from a Single Video", "comment": null, "summary": "Creating deformable 3D content has gained increasing attention with the rise\nof text-to-image and image-to-video generative models. While these models\nprovide rich semantic priors for appearance, they struggle to capture the\nphysical realism and motion dynamics needed for authentic 4D scene synthesis.\nIn contrast, real-world videos can provide physically grounded geometry and\narticulation cues that are difficult to hallucinate. One question is raised:\n\\textit{Can we generate physically consistent 4D content by leveraging the\nmotion priors of the real-world video}? In this work, we explore the task of\nreanimating deformable 3D scenes from a single video, using the original\nsequence as a supervisory signal to correct artifacts from synthetic motion. We\nintroduce \\textbf{Restage4D}, a geometry-preserving pipeline for\nvideo-conditioned 4D restaging. Our approach uses a video-rewinding training\nstrategy to temporally bridge a real base video and a synthetic driving video\nvia a shared motion representation. We further incorporate an occlusion-aware\nrigidity loss and a disocclusion backtracing mechanism to improve structural\nand geometry consistency under challenging motion. We validate Restage4D on\nDAVIS and PointOdyssey, demonstrating improved geometry consistency, motion\nquality, and 3D tracking performance. Our method not only preserves deformable\nstructure under novel motion, but also automatically corrects errors introduced\nby generative models, revealing the potential of video prior in 4D restaging\ntask. Source code and trained models will be released.", "AI": {"tldr": "Restage4D is a pipeline for generating physically consistent 4D content by leveraging real-world video motion priors to reanimate deformable 3D scenes. It uses a video-rewinding strategy and other mechanisms to improve consistency and correct errors from generative models.", "motivation": "Generating physically consistent 4D content by leveraging the motion priors of real-world videos. The goal is to reanimate deformable 3D scenes from a single video, using the original sequence as a supervisory signal to correct artifacts from synthetic motion.", "method": "Restage4D uses a video-rewinding training strategy to temporally bridge a real base video and a synthetic driving video via a shared motion representation. It also incorporates an occlusion-aware rigidity loss and a disocclusion backtracing mechanism to improve structural and geometry consistency under challenging motion.", "result": "Restage4D demonstrates improved geometry consistency, motion quality, and 3D tracking performance on DAVIS and PointOdyssey datasets.", "conclusion": "Restage4D not only preserves deformable structure under novel motion but also automatically corrects errors introduced by generative models, revealing the potential of video prior in 4D restaging task. Source code and trained models will be released."}}
{"id": "2508.08255", "categories": ["quant-ph", "cond-mat.mes-hall", "physics.optics"], "pdf": "https://arxiv.org/pdf/2508.08255", "abs": "https://arxiv.org/abs/2508.08255", "authors": ["Quan Lin", "Christopher Cedzich", "Qi Zhou", "Peng Xue"], "title": "Observation of Metal-Insulator and Spectral Phase Transitions in Aubry-Andr\u00e9-Harper Models", "comment": "9+3 pages, 7 figures", "summary": "Non-Hermitian extensions of the Aubry-Andr\\'e-Harper (AAH) model reveal a\nrich variety of phase transitions arising from the interplay of\nquasiperiodicity and non-Hermiticity. Despite their theoretical significance,\nexperimental explorations remain challenging due to complexities in realizing\ncontrolled non-Hermiticity. Here, we present the first experimental realization\nof the unitary almost-Mathieu operator (UAMO) which simulates the AAH model by\nemploying single-photon quantum walks. Through precise control of\nquasiperiodicity, we systematically explore the phase diagram displaying a\nphase transition between localized and delocalized regimes in the Hermitian\nlimit. Subsequently, by introducing non-reciprocal hopping, we experimentally\nprobe the parity-time (PT) symmetry-breaking transition that is characterized\nby the emergence of complex quasienergies. Moreover, we identify a novel\nspectral transition exclusive to discrete-time settings, where all\nquasienergies become purely imaginary. Both transitions are connected to\nchanges in the spectral winding number, demonstrating their topological\norigins. These results clarify the interplay between localization, symmetry\nbreaking, and topology in non-Hermitian quasicrystals, paving the way for\nfuture exploration of synthetic quantum matter.", "AI": {"tldr": "\u672c\u7814\u7a76\u9996\u6b21\u901a\u8fc7\u5355\u5149\u5b50\u91cf\u5b50\u884c\u8d70\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u5e7a\u6b63\u8fd1 Mathieu \u7b97\u7b26\uff08UAMO\uff09\uff0c\u63a2\u7d22\u4e86\u975e\u5384\u7c73 Aubry-Andr\u00e9-Harper\uff08AAH\uff09\u6a21\u578b\u4e2d\u7684\u76f8\u53d8\uff0c\u63ed\u793a\u4e86\u5c40\u57df\u5316\u3001\u5bf9\u79f0\u6027\u7834\u7f3a\u548c\u62d3\u6251\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u5728\u5b9e\u9a8c\u4e2d\u5b9e\u73b0\u53d7\u63a7\u975e\u5384\u7c73\u6027\u7684\u590d\u6742\u6027\uff0c\u63a2\u7d22 Aubry-Andr\u00e9-Harper\uff08AAH\uff09\u6a21\u578b\u4e2d\u7531\u62df\u5468\u671f\u6027\u548c\u975e\u5384\u7c73\u6027\u76f8\u4e92\u4f5c\u7528\u5f15\u8d77\u7684\u4e30\u5bcc\u76f8\u53d8\u3002", "method": "\u5229\u7528\u5355\u5149\u5b50\u91cf\u5b50\u884c\u8d70\u5b9e\u9a8c\u5b9e\u73b0\u4e86\u5e7a\u6b63\u8fd1 Mathieu \u7b97\u7b26\uff08UAMO\uff09\uff0c\u6a21\u62df\u4e86 Aubry-Andr\u00e9-Harper\uff08AAH\uff09\u6a21\u578b\u3002", "result": "\u5b9e\u9a8c\u63a2\u7d22\u4e86AAH\u6a21\u578b\u7684\u76f8\u56fe\uff0c\u5728\u5384\u7c73\u6781\u9650\u4e0b\u5c55\u793a\u4e86\u5c40\u57df\u5316\u548c\u975e\u5c40\u57df\u5316\u533a\u57df\u4e4b\u95f4\u7684\u76f8\u53d8\u3002\u901a\u8fc7\u5f15\u5165\u975e\u4e92\u6613\u8df3\u53d8\uff0c\u63a2\u6d4b\u4e86\u7531\u51c6\u80fd\u91cf\u590d\u6570\u51fa\u73b0\u7684 the parity-time (PT) \u5bf9\u79f0\u6027\u7834\u7f3a\u76f8\u53d8\u3002\u6b64\u5916\uff0c\u8fd8\u8bc6\u522b\u4e86\u4e00\u79cd\u4ec5\u5b58\u5728\u4e8e\u79bb\u6563\u65f6\u95f4\u7cfb\u7edf\u4e2d\u7684\u65b0\u578b\u8c31\u76f8\u53d8\uff0c\u5176\u4e2d\u6240\u6709\u51c6\u80fd\u91cf\u53d8\u4e3a\u7eaf\u865a\u6570\u3002\u8fd9\u4e24\u79cd\u76f8\u53d8\u5747\u4e0e\u8c31\u7f20\u7ed5\u6570\u7684\u53d8\u5316\u76f8\u5173\u3002", "conclusion": "\u8be5\u7814\u7a76\u9610\u660e\u4e86\u975e\u5384\u7c73\u7cfb\u7edf\u4e2d\u7684\u5c40\u57df\u5316\u3001\u5bf9\u79f0\u6027\u7834\u7f3a\u548c\u62d3\u6251\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u4e3a\u672a\u6765\u5408\u6210\u91cf\u5b50\u7269\u8d28\u7684\u63a2\u7d22\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.08034", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08034", "abs": "https://arxiv.org/abs/2508.08034", "authors": ["Roksana Yahyaabadi", "Ghazal Farhani", "Taufiq Rahman", "Soodeh Nikan", "Abdullah Jirjees", "Fadi Araji"], "title": "Deep Learning-Based Analysis of Power Consumption in Gasoline, Electric, and Hybrid Vehicles", "comment": null, "summary": "Accurate power consumption prediction is crucial for improving efficiency and\nreducing environmental impact, yet traditional methods relying on specialized\ninstruments or rigid physical models are impractical for large-scale,\nreal-world deployment. This study introduces a scalable data-driven method\nusing powertrain dynamic feature sets and both traditional machine learning and\ndeep neural networks to estimate instantaneous and cumulative power consumption\nin internal combustion engine (ICE), electric vehicle (EV), and hybrid electric\nvehicle (HEV) platforms. ICE models achieved high instantaneous accuracy with\nmean absolute error and root mean squared error on the order of $10^{-3}$, and\ncumulative errors under 3%. Transformer and long short-term memory models\nperformed best for EVs and HEVs, with cumulative errors below 4.1% and 2.1%,\nrespectively. Results confirm the approach's effectiveness across vehicles and\nmodels. Uncertainty analysis revealed greater variability in EV and HEV\ndatasets than ICE, due to complex power management, emphasizing the need for\nrobust models for advanced powertrains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u4f7f\u7528\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u6280\u672f\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u5185\u71c3\u673a\u3001\u7535\u52a8\u6c7d\u8f66\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\u7684\u529f\u7387\u6d88\u8017\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u51c6\u786e\u7684\u529f\u7387\u6d88\u8017\u9884\u6d4b\u5bf9\u4e8e\u63d0\u9ad8\u6548\u7387\u548c\u51cf\u5c11\u73af\u5883\u5f71\u54cd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u4f20\u7edf\u7684\u4f9d\u8d56\u4e13\u7528\u4eea\u5668\u6216\u521a\u6027\u7269\u7406\u6a21\u578b\u7684\u65b9\u6cd5\u5bf9\u4e8e\u5927\u89c4\u6a21\u3001\u771f\u5b9e\u4e16\u754c\u7684\u90e8\u7f72\u6765\u8bf4\u662f\u4e0d\u5207\u5b9e\u9645\u7684\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5229\u7528\u52a8\u529b\u7cfb\u7edf\u52a8\u6001\u7279\u5f81\u96c6\uff0c\u5e76\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08\u7279\u522b\u662fTransformer\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u6a21\u578b\uff09\u6765\u4f30\u7b97\u5185\u71c3\u673a\uff08ICE\uff09\u3001\u7535\u52a8\u6c7d\u8f66\uff08EV\uff09\u548c\u6df7\u5408\u52a8\u529b\u6c7d\u8f66\uff08HEV\uff09\u7684\u77ac\u65f6\u548c\u7d2f\u79ef\u529f\u7387\u6d88\u8017\u3002", "result": "ICE\u6a21\u578b\u5b9e\u73b0\u4e86\u5f88\u9ad8\u7684\u77ac\u65f6\u7cbe\u5ea6\uff0c\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\u548c\u5747\u65b9\u6839\u8bef\u5dee\u7684\u6570\u91cf\u7ea7\u7ea6\u4e3a10^{-3}\uff0c\u7d2f\u79ef\u8bef\u5dee\u4f4e\u4e8e3%\u3002\u5bf9\u4e8eEV\u548cHEV\uff0cTransformer\u548c\u957f\u77ed\u671f\u8bb0\u5fc6\u6a21\u578b\u8868\u73b0\u6700\u4f73\uff0c\u7d2f\u79ef\u8bef\u5dee\u5206\u522b\u4f4e\u4e8e4.1%\u548c2.1%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\u5728\u4e0d\u540c\u7c7b\u578b\u7684\u8f66\u8f86\uff08ICE\u3001EV\u3001HEV\uff09\u4e0a\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u529f\u7387\u6d88\u8017\u9884\u6d4b\uff0c\u5e76\u9a8c\u8bc1\u4e86\u5176\u8de8\u6a21\u578b\u548c\u8de8\u5e73\u53f0\u7684\u6709\u6548\u6027\u3002\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u8868\u660e\uff0cEV\u548cHEV\u6570\u636e\u96c6\u6bd4ICE\u6570\u636e\u96c6\u5177\u6709\u66f4\u5927\u7684\u53d8\u5f02\u6027\uff0c\u8fd9\u5f3a\u8c03\u4e86\u4e3a\u9ad8\u7ea7\u52a8\u529b\u7cfb\u7edf\u6784\u5efa\u9c81\u68d2\u6a21\u578b\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2508.06871", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06871", "abs": "https://arxiv.org/abs/2508.06871", "authors": ["Aleksandar Todorov", "Juan Cardenas-Cartagena", "Rafael F. Cunha", "Marco Zullich", "Matthia Sabatelli"], "title": "Sparsity-Driven Plasticity in Multi-Task Reinforcement Learning", "comment": null, "summary": "Plasticity loss, a diminishing capacity to adapt as training progresses, is a\ncritical challenge in deep reinforcement learning. We examine this issue in\nmulti-task reinforcement learning (MTRL), where higher representational\nflexibility is crucial for managing diverse and potentially conflicting task\ndemands. We systematically explore how sparsification methods, particularly\nGradual Magnitude Pruning (GMP) and Sparse Evolutionary Training (SET), enhance\nplasticity and consequently improve performance in MTRL agents. We evaluate\nthese approaches across distinct MTRL architectures (shared backbone, Mixture\nof Experts, Mixture of Orthogonal Experts) on standardized MTRL benchmarks,\ncomparing against dense baselines, and a comprehensive range of alternative\nplasticity-inducing or regularization methods. Our results demonstrate that\nboth GMP and SET effectively mitigate key indicators of plasticity degradation,\nsuch as neuron dormancy and representational collapse. These plasticity\nimprovements often correlate with enhanced multi-task performance, with sparse\nagents frequently outperforming dense counterparts and achieving competitive\nresults against explicit plasticity interventions. Our findings offer insights\ninto the interplay between plasticity, network sparsity, and MTRL designs,\nhighlighting dynamic sparsification as a robust but context-sensitive tool for\ndeveloping more adaptable MTRL systems.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07724", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07724", "abs": "https://arxiv.org/abs/2508.07724", "authors": ["Christopher E Patrick"], "title": "Anisotropy at twin interfaces in $RT_{12}$ ($R$=rare earth, $T$=transition metal) magnets", "comment": "11 pages, 8 figures", "summary": "RT\\textsubscript{12} materials continue to attract attention due to their\npotential use as ``rare-earth-lean'' permanent magnets, but converting their\npromising intrinsic properties into practical high performance remains an\nelusive goal. Sophisticated experimental characterization techniques are\nproviding unprecedented insight into the structure of these materials at the\natomistic scale. Atomistic spin dynamics or micromagnetics simulations could\nhelp unravel the links between these structures and resultant magnet\nperformance, but require input data describing the intrinsic magnetic\nproperties. Here, first-principles calculations based on density-functional\ntheory are used to determine these properties for two model interface\nstructures which have been derived from recently reported high resolution\nelectron microscopy images. One model structure is a stoichiometric twin formed\nby mirroring the RT\\textsubscript{12} structure in the (101) plane, and the\nother model structure is a ``stacking fault'' involving the insertion of a\nRT\\textsubscript{4} plane and a displacement along the [100] axis. Magnetic\nmoments and crystal field coefficients have been calculated for the optimized\nstructures. The interfaces modify the magnetic properties at the sub-nm scale.\nIn particular, in the R-rich region of the ``stacking fault'', the local easy\naxis of magnetization rotates by $49^\\circ$ from its bulk direction, which may\nlead to reduced coercivity through the easier nucleation of reverse domains.", "AI": {"tldr": "RT12\u6750\u6599\u56e0\u5176\u201c\u7a00\u571f\u5143\u7d20\u542b\u91cf\u4f4e\u201d\u7684\u6c38\u78c1\u4f53\u6f5c\u529b\u800c\u5907\u53d7\u5173\u6ce8\uff0c\u4f46\u5c06\u5176\u4f18\u5f02\u7684\u5185\u7980\u6027\u80fd\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u9ad8\u6027\u80fd\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002\u672c\u7814\u7a76\u5229\u7528\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u7ed3\u5408\u9ad8\u5206\u8fa8\u7387\u7535\u5b50\u663e\u5fae\u955c\u56fe\u50cf\u6784\u5efa\u7684\u6a21\u578b\uff0c\u5206\u6790\u4e86\u4e24\u79cd\u754c\u9762\u7ed3\u6784\uff08\u5b6a\u6676\u548c\u5806\u579b\u5c42\u9519\uff09\u7684\u78c1\u6027\u80fd\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u5806\u579b\u5c42\u9519\u7ed3\u6784\u4e2d\u7684\u78c1\u5316\u6613\u8f74\u65cb\u8f6c\u53ef\u80fd\u5bfc\u81f4\u77eb\u987d\u529b\u4e0b\u964d\u3002", "motivation": "\u4e3a\u4e86\u5c06RT12\u6750\u6599\u7684\u6f5c\u5728\u6027\u80fd\u8f6c\u5316\u4e3a\u5b9e\u9645\u7684\u9ad8\u6027\u80fd\uff0c\u9700\u8981\u4e86\u89e3\u5176\u539f\u5b50\u5c3a\u5ea6\u7ed3\u6784\u4e0e\u78c1\u6027\u80fd\u4e4b\u95f4\u7684\u8054\u7cfb\uff0c\u4f46\u73b0\u6709\u7684\u6a21\u62df\u65b9\u6cd5\u9700\u8981\u672c\u5f81\u78c1\u6027\u80fd\u6570\u636e\u4f5c\u4e3a\u8f93\u5165\u3002", "method": "\u4f7f\u7528\u57fa\u4e8e\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u7684\u7b2c\u4e00\u6027\u539f\u7406\u8ba1\u7b97\uff0c\u786e\u5b9a\u4e86\u4e24\u79cd\u6a21\u578b\u754c\u9762\u7684\u672c\u5f81\u78c1\u6027\u80fd\uff0c\u5305\u62ec\u78c1\u77e9\u548c\u6676\u4f53\u573a\u7cfb\u6570\u3002", "result": "\u8ba1\u7b97\u5f97\u5230\u7684\u78c1\u77e9\u548c\u6676\u4f53\u573a\u7cfb\u6570\u8868\u660e\uff0c\u754c\u9762\u5728\u4e9a\u7eb3\u7c73\u5c3a\u5ea6\u4e0a\u6539\u53d8\u4e86\u78c1\u6027\u80fd\u3002\u5177\u4f53\u800c\u8a00\uff0c\u201c\u5806\u579b\u5c42\u9519\u201d\u7ed3\u6784\u4e2dR\u5143\u7d20\u5bcc\u96c6\u533a\u57df\u7684\u5c40\u90e8\u78c1\u5316\u6613\u8f74\u76f8\u5bf9\u4e8e\u5176\u4f53\u76f8\u65b9\u5411\u65cb\u8f6c\u4e8649\u00b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u8ba1\u7b97\u4e86\u4e24\u79cd\u6a21\u578b\u754c\u9762\u7684\u78c1\u6027\u80fd\uff0c\u5e76\u53d1\u73b0\u201c\u5806\u579b\u5c42\u9519\u201d\u7ed3\u6784\u4e2d\u7684\u5c40\u90e8\u78c1\u5316\u6613\u8f74\u65cb\u8f6c\u53ef\u80fd\u5bfc\u81f4\u77eb\u987d\u529b\u964d\u4f4e\u3002"}}
{"id": "2508.07387", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07387", "abs": "https://arxiv.org/abs/2508.07387", "authors": ["Basant Sharma", "Prajyot Jadhav", "Pranjal Paul", "K. Madhava Krishna", "Arun Kumar Singh"], "title": "MonoMPC: Monocular Vision Based Navigation with Learned Collision Model and Risk-Aware Model Predictive Control", "comment": null, "summary": "Navigating unknown environments with a single RGB camera is challenging, as\nthe lack of depth information prevents reliable collision-checking. While some\nmethods use estimated depth to build collision maps, we found that depth\nestimates from vision foundation models are too noisy for zero-shot navigation\nin cluttered environments.\n  We propose an alternative approach: instead of using noisy estimated depth\nfor direct collision-checking, we use it as a rich context input to a learned\ncollision model. This model predicts the distribution of minimum obstacle\nclearance that the robot can expect for a given control sequence. At inference,\nthese predictions inform a risk-aware MPC planner that minimizes estimated\ncollision risk. Our joint learning pipeline co-trains the collision model and\nrisk metric using both safe and unsafe trajectories. Crucially, our\njoint-training ensures optimal variance in our collision model that improves\nnavigation in highly cluttered environments. Consequently, real-world\nexperiments show 9x and 7x improvements in success rates over NoMaD and the ROS\nstack, respectively. Ablation studies further validate the effectiveness of our\ndesign choices.", "AI": {"tldr": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\uff0c\u4f7f\u7528\u5355\u4e00RGB\u6444\u50cf\u5934\u5bfc\u822a\u65f6\uff0c\u7531\u4e8e\u6df1\u5ea6\u4fe1\u606f\u7f3a\u5931\uff0c\u96be\u4ee5\u8fdb\u884c\u78b0\u649e\u68c0\u6d4b\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5229\u7528\u4f30\u8ba1\u7684\u6df1\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u8f93\u5165\uff0c\u8bad\u7ec3\u4e00\u4e2a\u78b0\u649e\u6a21\u578b\u6765\u9884\u6d4b\u6700\u5c0f\u969c\u788d\u7269\u95f4\u9699\uff0c\u5e76\u901a\u8fc7\u98ce\u9669\u611f\u77e5\u7684MPC\u89c4\u5212\u5668\u6765\u6700\u5c0f\u5316\u78b0\u649e\u98ce\u9669\uff0c\u5728\u6df7\u4e71\u73af\u5883\u4e2d\u5bfc\u822a\u65f6\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u529f\u7387\u63d0\u5347\u3002", "motivation": "\u5728\u672a\u77e5\u73af\u5883\u4e2d\u4f7f\u7528\u5355\u4e00RGB\u6444\u50cf\u5934\u8fdb\u884c\u5bfc\u822a\u65f6\uff0c\u7f3a\u4e4f\u6df1\u5ea6\u4fe1\u606f\u4f1a\u5bfc\u81f4\u53ef\u9760\u7684\u78b0\u649e\u68c0\u67e5\u51fa\u73b0\u95ee\u9898\u3002\u867d\u7136\u4e00\u4e9b\u65b9\u6cd5\u4f7f\u7528\u4f30\u8ba1\u7684\u6df1\u5ea6\u6765\u6784\u5efa\u78b0\u649e\u5730\u56fe\uff0c\u4f46\u7814\u7a76\u53d1\u73b0\uff0c\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u4f30\u8ba1\u7684\u6df1\u5ea6\u5728\u6742\u4e71\u73af\u5883\u4e2d\u8fdb\u884c\u96f6\u6837\u672c\u5bfc\u822a\u65f6\u8fc7\u4e8e\u5608\u6742\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u5b66\u4e60\u7684\u78b0\u649e\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5229\u7528\u4f30\u8ba1\u7684\u6df1\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\uff0c\u9884\u6d4b\u6700\u5c0f\u969c\u788d\u7269\u95f4\u9699\u5206\u5e03\u3002\u8be5\u6a21\u578b\u4e0e\u98ce\u9669\u611f\u77e5MPC\u89c4\u5212\u5668\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u6700\u5c0f\u5316\u4f30\u8ba1\u7684\u78b0\u649e\u98ce\u9669\u6765\u8fdb\u884c\u5bfc\u822a\u3002\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\u78b0\u649e\u6a21\u578b\u548c\u98ce\u9669\u5ea6\u91cf\uff0c\u5e76\u4f18\u5316\u78b0\u649e\u6a21\u578b\u7684\u65b9\u5dee\uff0c\u63d0\u9ad8\u4e86\u5728\u6df7\u4e71\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0eNoMaD\u548cROS\u5806\u6808\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u6210\u529f\u7387\u65b9\u9762\u5206\u522b\u63d0\u9ad8\u4e869\u500d\u548c7\u500d\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u9a8c\u8bc1\u4e86\u6240\u9009\u8bbe\u8ba1\u51b3\u7b56\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u65b9\u6cd5\uff0c\u5229\u7528\u4f30\u8ba1\u7684\u6df1\u5ea6\u4fe1\u606f\u4f5c\u4e3a\u4e0a\u4e0b\u6587\u8f93\u5165\u5230\u4e00\u4e2a\u5b66\u4e60\u5230\u7684\u78b0\u649e\u6a21\u578b\u4e2d\uff0c\u8be5\u6a21\u578b\u53ef\u4ee5\u9884\u6d4b\u673a\u5668\u4eba\u53ef\u9884\u671f\u7684\u6700\u5c0f\u969c\u788d\u7269\u95f4\u9699\u5206\u5e03\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5728\u6df7\u4e71\u73af\u5883\u4e2d\u7684\u5bfc\u822a\u6027\u80fd\u3002"}}
{"id": "2508.07593", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07593", "abs": "https://arxiv.org/abs/2508.07593", "authors": ["Isabelle Savill-Brown", "Zain Mehdi", "Alexander K. Ratcliffe", "Varun D. Vaidya", "Haonan Liu", "Simon A. Haine", "C. Ricardo Viteri", "Joseph J. Hope"], "title": "Error-Resilient Fast Entangling Gates for Scalable Ion-Trap Quantum Processors", "comment": null, "summary": "Non-adiabatic two-qubit gate proposals for trapped-ion systems offer superior\nperformance and flexibility over adiabatic schemes at the cost of increased\nlaser control requirements. Existing fast gate schemes are limited by\nsingle-qubit transition errors, which constrain the total number of pulses in\nhigh-fidelity solutions. We introduce an improved gate search scheme that\nenables both local and non-local two-qubit gates in chains containing tens of\nions. These protocols use a multi-objective machine design approach that\nincorporates dominant sources of error in the design to ensure the solutions\nare compatible with existing fast laser controls. We also generalize previous\nschemes by allowing for unpaired pulses during the gate evolution. By imposing\nsymmetries on the pulse sequences, we eliminate susceptibility to laser phase\nnoise and further simplify the multi-mode control over the state-dependent\nmotion of the ion crystal. We perform a comprehensive analysis of expected gate\nperformance in the presence of random and systematic experimental errors to\ndemonstrate the feasibility of performing microsecond two-qubit gates between\narbitrary ion pairs in current linear ion-trap processors of up to $50$ ions\nwith fidelities approaching $99.9\\%$.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u95e8\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u901a\u8fc7\u673a\u5668\u5b66\u4e60\u4f18\u5316\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u4fdd\u771f\u5ea6\u7684\u5feb\u901f\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u64cd\u4f5c\uff0c\u5e76\u5bf9\u5b9e\u9a8c\u8bef\u5dee\u5177\u6709\u9c81\u68d2\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u5feb\u901f\u95e8\u65b9\u6848\u4e2d\u7531\u5355\u91cf\u5b50\u6bd4\u7279\u8dc3\u8fc1\u8bef\u5dee\u5f15\u8d77\u7684\u9650\u5236\uff0c\u8fd9\u4e9b\u8bef\u5dee\u9650\u5236\u4e86\u9ad8\u4fdd\u771f\u5ea6\u89e3\u51b3\u65b9\u6848\u4e2d\u7684\u8109\u51b2\u603b\u6570\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u95e8\u641c\u7d22\u65b9\u6848\uff0c\u8be5\u65b9\u6848\u7ed3\u5408\u4e86\u591a\u76ee\u6807\u673a\u5668\u5b66\u4e60\u8bbe\u8ba1\u65b9\u6cd5\uff0c\u5e76\u8003\u8651\u4e86\u4e3b\u8981\u7684\u8bef\u5dee\u6e90\u3002\u901a\u8fc7\u5bf9\u8109\u51b2\u5e8f\u5217\u65bd\u52a0\u5bf9\u79f0\u6027\uff0c\u6d88\u9664\u4e86\u5bf9\u6fc0\u5149\u76f8\u4f4d\u566a\u58f0\u7684\u654f\u611f\u6027\uff0c\u5e76\u7b80\u5316\u4e86\u5bf9\u79bb\u5b50\u6676\u4f53\u72b6\u6001\u76f8\u5173\u8fd0\u52a8\u7684\u591a\u6a21\u63a7\u5236\u3002\u5141\u8bb8\u5728\u95e8\u6f14\u5316\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u672a\u914d\u5bf9\u8109\u51b2\u3002", "result": "\u5b9e\u73b0\u4e86\u53ef\u5728\u5305\u542b\u6570\u5341\u4e2a\u79bb\u5b50\u7684\u94fe\u4e2d\u8fdb\u884c\u5c40\u90e8\u548c\u975e\u5c40\u90e8\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u64cd\u4f5c\u7684\u534f\u8bae\uff0c\u4fdd\u771f\u5ea6\u63a5\u8fd199.9%\u3002", "conclusion": "\u901a\u8fc7\u5bf9\u5b9e\u9a8c\u8bef\u5dee\u7684\u5168\u9762\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5728\u957f\u8fbe50\u4e2a\u79bb\u5b50\u7684\u7ebf\u6027\u79bb\u5b50\u9631\u5904\u7406\u5668\u4e2d\uff0c\u5728\u4efb\u610f\u79bb\u5b50\u5bf9\u4e4b\u95f4\u6267\u884c\u5fae\u79d2\u7ea7\u53cc\u91cf\u5b50\u6bd4\u7279\u95e8\u7684\u53ef\u80fd\u6027\uff0c\u4fdd\u771f\u5ea6\u63a5\u8fd199.9%\u3002"}}
{"id": "2508.07178", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07178", "abs": "https://arxiv.org/abs/2508.07178", "authors": ["Kejin Liu", "Junhong Lian", "Xiang Ao", "Ningtao Wang", "Xing Fu", "Yu Cheng", "Weiqiang Wang", "Xinyu Liu"], "title": "Improved Personalized Headline Generation via Denoising Fake Interests from Implicit Feedback", "comment": "Accepted by the 34th ACM International Conference on Information and\n  Knowledge Management (CIKM '25), Full Research Papers track", "summary": "Accurate personalized headline generation hinges on precisely capturing user\ninterests from historical behaviors. However, existing methods neglect\npersonalized-irrelevant click noise in entire historical clickstreams, which\nmay lead to hallucinated headlines that deviate from genuine user preferences.\nIn this paper, we reveal the detrimental impact of click noise on personalized\ngeneration quality through rigorous analysis in both user and news dimensions.\nBased on these insights, we propose a novel Personalized Headline Generation\nframework via Denoising Fake Interests from Implicit Feedback (PHG-DIF).\nPHG-DIF first employs dual-stage filtering to effectively remove clickstream\nnoise, identified by short dwell times and abnormal click bursts, and then\nleverages multi-level temporal fusion to dynamically model users' evolving and\nmulti-faceted interests for precise profiling. Moreover, we release DT-PENS, a\nnew benchmark dataset comprising the click behavior of 1,000 carefully curated\nusers and nearly 10,000 annotated personalized headlines with historical dwell\ntime annotations. Extensive experiments demonstrate that PHG-DIF substantially\nmitigates the adverse effects of click noise and significantly improves\nheadline quality, achieving state-of-the-art (SOTA) results on DT-PENS. Our\nframework implementation and dataset are available at\nhttps://github.com/liukejin-up/PHG-DIF.", "AI": {"tldr": "PHG-DIF\u901a\u8fc7\u53bb\u9664\u865a\u5047\u5174\u8da3\u7684\u9690\u542b\u53cd\u9988\u6765\u751f\u6210\u4e2a\u6027\u5316\u6807\u9898\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u70b9\u51fb\u566a\u97f3\u7684\u95ee\u9898\uff0c\u5e76\u5728\u65b0\u7684DT-PENS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u4e86\u6574\u4e2a\u70b9\u51fb\u6d41\u4e2d\u4e2a\u6027\u5316\u4e0d\u76f8\u5173\u7684\u70b9\u51fb\u566a\u97f3\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u751f\u6210\u7684\u6807\u9898\u4e0e\u771f\u5b9e\u7528\u6237\u504f\u597d\u76f8\u6096\u3002", "method": "PHG-DIF\u6846\u67b6\u9996\u5148\u91c7\u7528\u53cc\u9636\u6bb5\u8fc7\u6ee4\u6765\u6709\u6548\u53bb\u9664\u7531\u77ed\u505c\u7559\u65f6\u95f4\u548c\u5f02\u5e38\u70b9\u51fb\u7206\u53d1\u5f15\u8d77\u7684\u70b9\u51fb\u6d41\u566a\u97f3\uff0c\u7136\u540e\u5229\u7528\u591a\u7ea7\u65f6\u95f4\u878d\u5408\u6765\u52a8\u6001\u5efa\u6a21\u7528\u6237\u4e0d\u65ad\u6f14\u53d8\u7684\u591a\u65b9\u9762\u5174\u8da3\u4ee5\u8fdb\u884c\u7cbe\u786e\u753b\u50cf\u3002", "result": "PHG-DIF\u663e\u8457\u51cf\u8f7b\u4e86\u70b9\u51fb\u566a\u97f3\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u9898\u8d28\u91cf\uff0c\u5728DT-PENS\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u7ed3\u679c\u3002", "conclusion": "PHG-DIF\u6846\u67b6\u663e\u8457\u51cf\u8f7b\u4e86\u70b9\u51fb\u566a\u97f3\u7684\u4e0d\u5229\u5f71\u54cd\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u6807\u9898\u8d28\u91cf\uff0c\u5728DT-PENS\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\uff08SOTA\uff09\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.06756", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06756", "abs": "https://arxiv.org/abs/2508.06756", "authors": ["Somayeh Farahani", "Marjaneh Hejazi", "Antonio Di Ieva", "Sidong Liu"], "title": "FoundBioNet: A Foundation-Based Model for IDH Genotyping of Glioma from Multi-Parametric MRI", "comment": "Accepted for oral and poster presentation at MICCAI 2025", "summary": "Accurate, noninvasive detection of isocitrate dehydrogenase (IDH) mutation is\nessential for effective glioma management. Traditional methods rely on invasive\ntissue sampling, which may fail to capture a tumor's spatial heterogeneity.\nWhile deep learning models have shown promise in molecular profiling, their\nperformance is often limited by scarce annotated data. In contrast, foundation\ndeep learning models offer a more generalizable approach for glioma imaging\nbiomarkers. We propose a Foundation-based Biomarker Network (FoundBioNet) that\nutilizes a SWIN-UNETR-based architecture to noninvasively predict IDH mutation\nstatus from multi-parametric MRI. Two key modules are incorporated: Tumor-Aware\nFeature Encoding (TAFE) for extracting multi-scale, tumor-focused features, and\nCross-Modality Differential (CMD) for highlighting subtle T2-FLAIR mismatch\nsignals associated with IDH mutation. The model was trained and validated on a\ndiverse, multi-center cohort of 1705 glioma patients from six public datasets.\nOur model achieved AUCs of 90.58%, 88.08%, 65.41%, and 80.31% on independent\ntest sets from EGD, TCGA, Ivy GAP, RHUH, and UPenn, consistently outperforming\nbaseline approaches (p <= 0.05). Ablation studies confirmed that both the TAFE\nand CMD modules are essential for improving predictive accuracy. By integrating\nlarge-scale pretraining and task-specific fine-tuning, FoundBioNet enables\ngeneralizable glioma characterization. This approach enhances diagnostic\naccuracy and interpretability, with the potential to enable more personalized\npatient care.", "AI": {"tldr": "FoundBioNet\u5229\u7528SWIN-UNETR\u3001TAFE\u548cCMD\u6a21\u5757\uff0c\u901a\u8fc7MRI\u975e\u4fb5\u5165\u6027\u5730\u9884\u6d4bIDH\u7a81\u53d8\u72b6\u6001\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u51c6\u786e\u3001\u975e\u4fb5\u5165\u6027\u5730\u68c0\u6d4b\u5f02\u67e0\u6aac\u9178\u8131\u6c22\u9176\uff08IDH\uff09\u7a81\u53d8\u5bf9\u4e8e\u6709\u6548\u7684\u80f6\u8d28\u7624\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u4f20\u7edf\u7684\u7ec4\u7ec7\u91c7\u6837\u65b9\u6cd5\u53ef\u80fd\u65e0\u6cd5\u6355\u6349\u80bf\u7624\u7684\u7a7a\u95f4\u5f02\u8d28\u6027\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u53d7\u9650\u4e8e\u6807\u6ce8\u6570\u636e\u7684\u7a00\u7f3a\u6027\u3002\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u57fa\u7840\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u4e3a\u80f6\u8d28\u7624\u5f71\u50cf\u5b66\u751f\u7269\u6807\u5fd7\u7269\u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u5177\u6cdb\u5316\u6027\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eFoundation\u7684\u751f\u7269\u6807\u5fd7\u7269\u7f51\u7edc\uff08FoundBioNet\uff09\uff0c\u5b83\u5229\u7528SWIN-UNETR\u4e3a\u57fa\u7840\u7684\u67b6\u6784\uff0c\u901a\u8fc7\u591a\u53c2\u6570MRI\u975e\u4fb5\u5165\u6027\u5730\u9884\u6d4bIDH\u7a81\u53d8\u72b6\u6001\u3002\u6a21\u578b\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u80bf\u7624\u611f\u77e5\u7279\u5f81\u7f16\u7801\uff08TAFE\uff09\u7528\u4e8e\u63d0\u53d6\u591a\u5c3a\u5ea6\u3001\u80bf\u7624\u5bfc\u5411\u7684\u7279\u5f81\uff0c\u4ee5\u53ca\u8de8\u6a21\u6001\u5dee\u5f02\uff08CMD\uff09\u7528\u4e8e\u7a81\u51fa\u4e0eIDH\u7a81\u53d8\u76f8\u5173\u7684\u7ec6\u5faeT2-FLAIR\u4e0d\u5339\u914d\u4fe1\u53f7\u3002", "result": "\u5728EGD\u3001TCGA\u3001Ivy GAP\u3001RHUH\u548cUPenn\u7684\u72ec\u7acb\u6d4b\u8bd5\u96c6\u4e0a\uff0cFoundBioNet\u7684AUC\u5206\u522b\u8fbe\u523090.58%\u300188.08%\u300165.41%\u548c80.31%\uff0c\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff08p <= 0.05\uff09\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\uff0cTAFE\u548cCMD\u6a21\u5757\u5bf9\u4e8e\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "conclusion": "FoundBioNet\u80fd\u591f\u901a\u8fc7\u96c6\u6210\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u7279\u5b9a\u4efb\u52a1\u7684\u5fae\u8c03\uff0c\u5b9e\u73b0\u53ef\u6cdb\u5316\u7684\u80f6\u8d28\u7624\u8868\u5f81\uff0c\u63d0\u9ad8\u8bca\u65ad\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u6709\u6f5c\u529b\u5b9e\u73b0\u66f4\u4e2a\u6027\u5316\u7684\u60a3\u8005\u62a4\u7406\u3002"}}
{"id": "2508.08216", "categories": ["cs.LG", "eess.SP"], "pdf": "https://arxiv.org/pdf/2508.08216", "abs": "https://arxiv.org/abs/2508.08216", "authors": ["Nicole Lai-Tan", "Xiao Gu", "Marios G. Philiastides", "Fani Deligianni"], "title": "Cross-Subject and Cross-Montage EEG Transfer Learning via Individual Tangent Space Alignment and Spatial-Riemannian Feature Fusion", "comment": "12 pages, 5 figures", "summary": "Personalised music-based interventions offer a powerful means of supporting\nmotor rehabilitation by dynamically tailoring auditory stimuli to provide\nexternal timekeeping cues, modulate affective states, and stabilise gait\npatterns. Generalisable Brain-Computer Interfaces (BCIs) thus hold promise for\nadapting these interventions across individuals. However, inter-subject\nvariability in EEG signals, further compounded by movement-induced artefacts\nand motor planning differences, hinders the generalisability of BCIs and\nresults in lengthy calibration processes. We propose Individual Tangent Space\nAlignment (ITSA), a novel pre-alignment strategy incorporating subject-specific\nrecentering, distribution matching, and supervised rotational alignment to\nenhance cross-subject generalisation. Our hybrid architecture fuses Regularised\nCommon Spatial Patterns (RCSP) with Riemannian geometry in parallel and\nsequential configurations, improving class separability while maintaining the\ngeometric structure of covariance matrices for robust statistical computation.\nUsing leave-one-subject-out cross-validation, `ITSA' demonstrates significant\nperformance improvements across subjects and conditions. The parallel fusion\napproach shows the greatest enhancement over its sequential counterpart, with\nrobust performance maintained across varying data conditions and electrode\nconfigurations. The code will be made publicly available at the time of\npublication.", "AI": {"tldr": "ITSA\u901a\u8fc7\u9884\u5bf9\u9f50\u7b56\u7565\u548c\u6df7\u5408\u67b6\u6784\u89e3\u51b3\u4e86BCI\u7684\u4e2a\u4f53\u5dee\u5f02\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6cdb\u5316\u80fd\u529b\u548c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3BCI\u4e2d\u4e2a\u4f53\u5dee\u5f02\u3001\u8fd0\u52a8\u4f2a\u5f71\u548c\u8fd0\u52a8\u89c4\u5212\u5dee\u5f02\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u53d7\u9650\u548c\u6821\u51c6\u65f6\u95f4\u957f\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9884\u5bf9\u9f50\u7b56\u7565\u3002", "method": "ITSA\u901a\u8fc7\u7ed3\u5408\u7279\u5b9a\u4e8e\u4e3b\u4f53\u7684\u91cd\u7f6e\u3001\u5206\u5e03\u5339\u914d\u548c\u76d1\u7763\u65cb\u8f6c\u5bf9\u9f50\uff0c\u4ee5\u53caRCSP\u548c\u9ece\u66fc\u51e0\u4f55\u7684\u6df7\u5408\u67b6\u6784\uff08\u5e76\u884c\u548c\u4e32\u884c\u914d\u7f6e\uff09\u6765\u589e\u5f3a\u8de8\u4e3b\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002", "result": "ITSA\u5728\u4ea4\u53c9\u9a8c\u8bc1\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u8de8\u4e3b\u4f53\u548c\u8de8\u6761\u4ef6\u6027\u80fd\u63d0\u5347\uff0c\u5176\u4e2d\u5e76\u884c\u878d\u5408\u65b9\u6cd5\u4f18\u4e8e\u4e32\u884c\u65b9\u6cd5\uff0c\u5e76\u5728\u4e0d\u540c\u6570\u636e\u6761\u4ef6\u548c\u7535\u6781\u914d\u7f6e\u4e0b\u4fdd\u6301\u7a33\u5065\u7684\u6027\u80fd\u3002", "conclusion": "ITSA\u662f\u4e00\u79cd\u65b0\u9896\u7684\u9884\u5bf9\u9f50\u7b56\u7565\uff0c\u901a\u8fc7\u7ed3\u5408\u7279\u5b9a\u4e8e\u4e3b\u4f53\u7684\u91cd\u7f6e\u3001\u5206\u5e03\u5339\u914d\u548c\u76d1\u7763\u65cb\u8f6c\u5bf9\u9f50\uff0c\u589e\u5f3a\u4e86\u8de8\u4e3b\u4f53\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b83\u901a\u8fc7RCSP\u548c\u9ece\u66fc\u51e0\u4f55\u7684\u6df7\u5408\u67b6\u6784\uff0c\u63d0\u9ad8\u4e86\u7c7b\u53ef\u5206\u79bb\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u534f\u65b9\u5dee\u77e9\u9635\u7684\u51e0\u4f55\u7ed3\u6784\uff0c\u7528\u4e8e\u7a33\u5065\u7684\u7edf\u8ba1\u8ba1\u7b97\u3002"}}
{"id": "2508.07292", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07292", "abs": "https://arxiv.org/abs/2508.07292", "authors": ["Yi Tang", "Kaini Wang", "Yang Chen", "Guangquan Zhou"], "title": "EndoAgent: A Memory-Guided Reflective Agent for Intelligent Endoscopic Vision-to-Decision Reasoning", "comment": null, "summary": "Developing general artificial intelligence (AI) systems to support endoscopic\nimage diagnosis is an emerging research priority. Existing methods based on\nlarge-scale pretraining often lack unified coordination across tasks and\nstruggle to handle the multi-step processes required in complex clinical\nworkflows. While AI agents have shown promise in flexible instruction parsing\nand tool integration across domains, their potential in endoscopy remains\nunderexplored. To address this gap, we propose EndoAgent, the first\nmemory-guided agent for vision-to-decision endoscopic analysis that integrates\niterative reasoning with adaptive tool selection and collaboration. Built on a\ndual-memory design, it enables sophisticated decision-making by ensuring\nlogical coherence through short-term action tracking and progressively\nenhancing reasoning acuity through long-term experiential learning. To support\ndiverse clinical tasks, EndoAgent integrates a suite of expert-designed tools\nwithin a unified reasoning loop. We further introduce EndoAgentBench, a\nbenchmark of 5,709 visual question-answer pairs that assess visual\nunderstanding and language generation capabilities in realistic scenarios.\nExtensive experiments show that EndoAgent consistently outperforms both general\nand medical multimodal models, exhibiting its strong flexibility and reasoning\ncapabilities.", "AI": {"tldr": "EndoAgent\u662f\u9996\u4e2a\u7528\u4e8e\u5185\u7aa5\u955c\u5206\u6790\u7684\u8bb0\u5fc6\u5f15\u5bfc\u5f0f\u667a\u80fd\u4f53\uff0c\u901a\u8fc7\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\u548c\u5de5\u5177\u96c6\u6210\uff0c\u5b9e\u73b0\u4e86\u8fed\u4ee3\u63a8\u7406\u548c\u81ea\u9002\u5e94\u5de5\u5177\u9009\u62e9\uff0c\u5728EndoAgentBench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u65b9\u6cd5\u5728\u5904\u7406\u590d\u6742\u4e34\u5e8a\u5de5\u4f5c\u6d41\u4e2d\u7684\u591a\u6b65\u9aa4\u8fc7\u7a0b\u65f6\uff0c\u5f80\u5f80\u7f3a\u4e4f\u8de8\u4efb\u52a1\u7684\u7edf\u4e00\u534f\u8c03\u3002\u5c3d\u7ba1AI\u667a\u80fd\u4f53\u5728\u8de8\u57df\u7684\u7075\u6d3b\u6307\u4ee4\u89e3\u6790\u548c\u5de5\u5177\u96c6\u6210\u65b9\u9762\u663e\u793a\u51fa\u6f5c\u529b\uff0c\u4f46\u5176\u5728\u5185\u7aa5\u955c\u9886\u57df\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aEndoAgent\u7684\u8bb0\u5fc6\u5f15\u5bfc\u5f0f\u667a\u80fd\u4f53\uff0c\u7528\u4e8e\u89c6\u89c9\u5230\u51b3\u7b56\u7684\u5185\u7aa5\u955c\u5206\u6790\u3002\u8be5\u667a\u80fd\u4f53\u96c6\u6210\u4e86\u8fed\u4ee3\u63a8\u7406\u3001\u81ea\u9002\u5e94\u5de5\u5177\u9009\u62e9\u548c\u534f\u4f5c\uff0c\u5e76\u91c7\u7528\u4e86\u53cc\u8bb0\u5fc6\u8bbe\u8ba1\uff0c\u901a\u8fc7\u77ed\u671f\u52a8\u4f5c\u8ddf\u8e2a\u786e\u4fdd\u903b\u8f91\u8fde\u8d2f\u6027\uff0c\u5e76\u901a\u8fc7\u957f\u671f\u7ecf\u9a8c\u5b66\u4e60\u9010\u6b65\u63d0\u9ad8\u63a8\u7406\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86EndoAgentBench\u57fa\u51c6\uff0c\u5305\u542b5,709\u4e2a\u89c6\u89c9\u95ee\u7b54\u5bf9\uff0c\u7528\u4e8e\u8bc4\u4f30\u5728\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u89c6\u89c9\u7406\u89e3\u548c\u8bed\u8a00\u751f\u6210\u80fd\u529b\u3002", "result": "EndoAgent\u5728\u5404\u79cd\u4e34\u5e8a\u4efb\u52a1\u4e2d\u96c6\u6210\u4e86\u4e13\u5bb6\u8bbe\u8ba1\u7684\u5de5\u5177\uff0c\u5e76\u5728\u7edf\u4e00\u7684\u63a8\u7406\u5faa\u73af\u4e2d\u8fd0\u884c\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cEndoAgent\u5728\u8bc4\u4f30\u89c6\u89c9\u7406\u89e3\u548c\u8bed\u8a00\u751f\u6210\u80fd\u529b\u65b9\u9762\uff0c\u6301\u7eed\u4f18\u4e8e\u901a\u7528\u7684\u548c\u533b\u5b66\u7684\u591a\u6a21\u6001\u6a21\u578b\u3002", "conclusion": "EndoAgent\u5728\u89c6\u89c9\u95ee\u7b54\u548c\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u901a\u7528\u548c\u533b\u5b66\u591a\u6a21\u6001\u6a21\u578b\uff0c\u5c55\u73b0\u4e86\u5176\u5f3a\u5927\u7684\u7075\u6d3b\u6027\u548c\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2508.06885", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06885", "abs": "https://arxiv.org/abs/2508.06885", "authors": ["Anthony Bellotti", "Xindi Zhao"], "title": "Conformal Prediction and Trustworthy AI", "comment": "Preprint for an essay to be published in The Importance of Being\n  Learnable (Enhancing the Learnability and Reliability of Machine Learning\n  Algorithms) Essays Dedicated to Alexander Gammerman on His 80th Birthday,\n  LNCS Springer Nature Switzerland AG ed. Nguyen K.A. and Luo Z", "summary": "Conformal predictors are machine learning algorithms developed in the 1990's\nby Gammerman, Vovk, and their research team, to provide set predictions with\nguaranteed confidence level. Over recent years, they have grown in popularity\nand have become a mainstream methodology for uncertainty quantification in the\nmachine learning community. From its beginning, there was an understanding that\nthey enable reliable machine learning with well-calibrated uncertainty\nquantification. This makes them extremely beneficial for developing trustworthy\nAI, a topic that has also risen in interest over the past few years, in both\nthe AI community and society more widely. In this article, we review the\npotential for conformal prediction to contribute to trustworthy AI beyond its\nmarginal validity property, addressing problems such as generalization risk and\nAI governance. Experiments and examples are also provided to demonstrate its\nuse as a well-calibrated predictor and for bias identification and mitigation.", "AI": {"tldr": "Conformal predictors offer reliable uncertainty quantification for trustworthy AI, helping with generalization, governance, and bias issues.", "motivation": "To explore the potential of conformal predictors for trustworthy AI, addressing issues like generalization risk and AI governance, and to demonstrate their practical applications.", "method": "Review of conformal predictors and their potential for trustworthy AI, including experiments for calibration, bias identification, and mitigation.", "result": "Conformal predictors are shown to be well-calibrated and useful for bias identification and mitigation, contributing to trustworthy AI.", "conclusion": "Conformal predictors are beneficial for trustworthy AI beyond marginal validity, addressing generalization risk and AI governance. Experiments demonstrate their use in calibration, bias identification, and mitigation."}}
{"id": "2508.07798", "categories": ["cond-mat.mtrl-sci", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07798", "abs": "https://arxiv.org/abs/2508.07798", "authors": ["Cheng Li", "Pengfei Danga", "Yuehui Xiana", "Yumei Zhou", "Bofeng Shi", "Xiangdong Ding", "Jun Suna", "Dezhen Xue"], "title": "Generative Inversion for Property-Targeted Materials Design: Application to Shape Memory Alloys", "comment": null, "summary": "The design of shape memory alloys (SMAs) with high transformation\ntemperatures and large mechanical work output remains a longstanding challenge\nin functional materials engineering. Here, we introduce a data-driven framework\nbased on generative adversarial network (GAN) inversion for the inverse design\nof high-performance SMAs. By coupling a pretrained GAN with a property\nprediction model, we perform gradient-based latent space optimization to\ndirectly generate candidate alloy compositions and processing parameters that\nsatisfy user-defined property targets. The framework is experimentally\nvalidated through the synthesis and characterization of five NiTi-based SMAs.\nAmong them, the Ni$_{49.8}$Ti$_{26.4}$Hf$_{18.6}$Zr$_{5.2}$ alloy achieves a\nhigh transformation temperature of 404 $^\\circ$C, a large mechanical work\noutput of 9.9 J/cm$^3$, a transformation enthalpy of 43 J/g , and a thermal\nhysteresis of 29 {\\deg}C, outperforming existing NiTi alloys. The enhanced\nperformance is attributed to a pronounced transformation volume change and a\nfinely dispersed of Ti$_2$Ni-type precipitates, enabled by sluggish Zr and Hf\ndiffusion, and semi-coherent interfaces with localized strain fields. This\nstudy demonstrates that GAN inversion offers an efficient and generalizable\nroute for the property-targeted discovery of complex alloys.", "AI": {"tldr": "\u901a\u8fc7GAN\u53cd\u6f14\u6846\u67b6\uff0c\u6210\u529f\u8bbe\u8ba1\u51fa\u9ad8\u6027\u80fdSMA\uff0cNi$_{49.8}$Ti$_{26.4}$Hf$_{18.6}$Zr$_{5.2}$\u5408\u91d1\u6027\u80fd\u4f18\u4e8e\u73b0\u6709NiTi\u5408\u91d1\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u89e3\u51b3\u5177\u6709\u9ad8\u8f6c\u53d8\u6e29\u5ea6\u548c\u5927\u7684\u673a\u68b0\u529f\u8f93\u51fa\u7684\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\uff08SMA\uff09\u7684\u8bbe\u8ba1\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GAN\uff09\u53cd\u6f14\u7684\u6570\u636e\u9a71\u52a8\u6846\u67b6\uff0c\u901a\u8fc7\u8026\u5408\u9884\u8bad\u7ec3\u7684GAN\u548c\u5c5e\u6027\u9884\u6d4b\u6a21\u578b\uff0c\u8fdb\u884c\u68af\u5ea6\u4e0b\u964d\u7684\u6f5c\u5728\u7a7a\u95f4\u4f18\u5316\uff0c\u76f4\u63a5\u751f\u6210\u6ee1\u8db3\u7528\u6237\u5b9a\u4e49\u5c5e\u6027\u76ee\u6807\u7684\u5019\u9009\u5408\u91d1\u6210\u5206\u548c\u5de5\u827a\u53c2\u6570\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u6846\u67b6\uff0c\u5408\u6210\u4e86\u4e94\u79cdNiTi\u57faSMA\u3002\u5176\u4e2d\uff0cNi$_{49.8}$Ti$_{26.4}$Hf$_{18.6}$Zr$_{5.2}$\u5408\u91d1\u5b9e\u73b0\u4e86404\u00b0C\u7684\u9ad8\u8f6c\u53d8\u6e29\u5ea6\u30019.9 J/cm$^3$\u7684\u5927\u673a\u68b0\u529f\u8f93\u51fa\u300143 J/g\u7684\u8f6c\u53d8\u7113\u548c29\u00b0C\u7684\u70ed\u6ede\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u7684NiTi\u5408\u91d1\u3002\u7814\u7a76\u7ed3\u679c\u5f52\u56e0\u4e8eZr\u548cHf\u7684\u7f13\u6162\u6269\u6563\u4ee5\u53ca\u4e0e\u5c40\u90e8\u5e94\u53d8\u573a\u534a\u76f8\u5e72\u754c\u9762\u6240\u5e26\u6765\u7684\u663e\u8457\u7684\u8f6c\u53d8\u4f53\u79ef\u53d8\u5316\u548c\u7ec6\u5206\u6563\u7684Ti$_2$Ni\u578b\u6c89\u6dc0\u7269\u3002", "conclusion": "\u8be5\u7814\u7a76\u8bc1\u660e\u4e86GAN\u53cd\u8f6c\u4e3a\u76ee\u6807\u6027\u80fd\u7684\u590d\u6742\u5408\u91d1\u53d1\u73b0\u63d0\u4f9b\u4e86\u4e00\u6761\u9ad8\u6548\u4e14\u53ef\u63a8\u5e7f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.07406", "categories": ["cs.RO", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07406", "abs": "https://arxiv.org/abs/2508.07406", "authors": ["Xiaobei Zhao", "Xingqi Lyu", "Xiang Li"], "title": "AgriVLN: Vision-and-Language Navigation for Agricultural Robots", "comment": null, "summary": "Agricultural robots have emerged as powerful members in agricultural tasks,\nnevertheless, still heavily rely on manual operation or untransportable railway\nfor movement, resulting in limited mobility and poor adaptability.\nVision-and-Language Navigation (VLN) enables robots to navigate to the target\ndestinations following natural language instructions, demonstrating strong\nperformance on several domains. However, none of the existing benchmarks or\nmethods is specifically designed for agricultural scenes. To bridge this gap,\nwe propose Agriculture to Agriculture (A2A) benchmark, containing 1,560\nepisodes across six diverse agricultural scenes, in which all realistic RGB\nvideos are captured by front-facing camera on a quadruped robot at a height of\n0.38 meters, aligning with the practical deployment conditions. Meanwhile, we\npropose Vision-and-Language Navigation for Agricultural Robots (AgriVLN)\nbaseline based on Vision-Language Model (VLM) prompted with carefully crafted\ntemplates, which can understand both given instructions and agricultural\nenvironments to generate appropriate low-level actions for robot control. When\nevaluated on A2A, AgriVLN performs well on short instructions but struggles\nwith long instructions, because it often fails to track which part of the\ninstruction is currently being executed. To address this, we further propose\nSubtask List (STL) instruction decomposition module and integrate it into\nAgriVLN, improving Success Rate (SR) from 0.33 to 0.47. We additionally compare\nAgriVLN with several existing VLN methods, demonstrating the state-of-the-art\nperformance in the agricultural domain.", "AI": {"tldr": "\u63d0\u51fa A2A \u57fa\u51c6\u548c AgriVLN \u65b9\u6cd5\uff0c\u7528\u4e8e\u519c\u4e1a\u573a\u666f\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u5bfc\u822a\u3002\u901a\u8fc7\u96c6\u6210 STL \u6a21\u5757\uff0c\u6539\u8fdb\u4e86\u5bf9\u957f\u6307\u4ee4\u7684\u5904\u7406\u80fd\u529b\uff0c\u5e76\u5728\u519c\u4e1a\u9886\u57df\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u519c\u4e1a\u673a\u5668\u4eba\u79fb\u52a8\u80fd\u529b\u6709\u9650\u4e14\u9002\u5e94\u6027\u5dee\uff0c\u4e3b\u8981\u4f9d\u8d56\u624b\u52a8\u64cd\u4f5c\u6216\u4e0d\u53ef\u8fd0\u8f93\u7684\u8f68\u9053\u3002\u89c6\u89c9\u548c\u8bed\u8a00\u5bfc\u822a (VLN) \u6280\u672f\u5728\u5176\u4ed6\u9886\u57df\u8868\u73b0\u826f\u597d\uff0c\u4f46\u7f3a\u4e4f\u4e13\u95e8\u9488\u5bf9\u519c\u4e1a\u573a\u666f\u7684\u57fa\u51c6\u548c\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa A2A \u57fa\u51c6\uff08\u5305\u542b 1,560 \u4e2a\u8de8\u8d8a\u516d\u4e2a\u4e0d\u540c\u519c\u4e1a\u573a\u666f\u7684\u8bd5\u9a8c\uff09\u548c AgriVLN \u57fa\u7ebf\uff08\u57fa\u4e8e\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5e76\u4f7f\u7528\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6a21\u677f\u63d0\u793a\uff09\uff0c\u7528\u4e8e\u519c\u4e1a\u573a\u666f\u7684\u89c6\u89c9\u548c\u8bed\u8a00\u5bfc\u822a\u3002AgriVLN \u8fdb\u4e00\u6b65\u96c6\u6210\u4e86\u5b50\u4efb\u52a1\u5217\u8868\uff08STL\uff09\u6307\u4ee4\u5206\u89e3\u6a21\u5757\u4ee5\u63d0\u9ad8\u5bf9\u957f\u6307\u4ee4\u7684\u5904\u7406\u80fd\u529b\u3002", "result": "AgriVLN \u5728 A2A \u57fa\u51c6\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5bf9\u957f\u6307\u4ee4\u7684\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u96c6\u6210 STL \u6a21\u5757\u540e\uff0c\u6210\u529f\u7387\u4ece 0.33 \u63d0\u9ad8\u5230 0.47\u3002\u4e0e\u73b0\u6709 VLN \u65b9\u6cd5\u76f8\u6bd4\uff0cAgriVLN \u5728\u519c\u4e1a\u9886\u57df\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "AgriVLN \u7ed3\u5408\u4e86 STL \u6a21\u5757\uff0c\u5728 A2A \u57fa\u51c6\u4e0a\u7684\u6210\u529f\u7387\u4ece 0.33 \u63d0\u9ad8\u5230 0.47\uff0c\u5e76\u5728\u519c\u4e1a\u9886\u57df\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07610", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07610", "abs": "https://arxiv.org/abs/2508.07610", "authors": ["Wei-guo Ma", "Yun-Hao Shi", "Kai Xu", "Heng Fan"], "title": "Tomography-assisted noisy quantum circuit simulator using matrix product density operators", "comment": "14 pages, 15 figures, 1 table", "summary": "In recent years, efficient quantum circuit simulations incorporating ideal\nnoise assumptions have relied on tensor network simulators, particularly\nleveraging the matrix product density operator (MPDO) framework. However,\nexperiments on real noisy intermediate-scale quantum (NISQ) devices often\ninvolve complex noise profiles, encompassing uncontrollable elements and\ninstrument-specific effects such as crosstalk. To address these challenges, we\nemploy quantum process tomography (QPT) techniques to directly capture the\noperational characteristics of the experimental setup and integrate them into\nnumerical simulations using MPDOs. Our QPT-assisted MPDO simulator is then\napplied to explore a variational approach for generating noisy entangled\nstates, comparing the results with standard noise numerical simulations and\ndemonstrations conducted on the Quafu cloud quantum computation platform.\nAdditionally, we investigate noisy MaxCut problems, as well as the effects of\ncrosstalk and noise truncation. Our results provide valuable insights into the\nimpact of noise on NISQ devices and lay the foundation for enhanced design and\nassessment of quantum algorithms in complex noise environments.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u8fc7\u7a0b\u5c42\u6790\uff08QPT\uff09\u548c\u77e9\u9635\u4e58\u79ef\u5bc6\u5ea6\u7b97\u5b50\uff08MPDO\uff09\u7684\u65b0\u578b\u6a21\u62df\u5668\uff0c\u7528\u4e8e\u6a21\u62df\u771f\u5b9eNISQ\u8bbe\u5907\u4e0a\u7684\u590d\u6742\u566a\u58f0\u3002\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3\u548c\u5e94\u5bf9\u8fd9\u4e9b\u566a\u58f0\u63d0\u4f9b\u4e86\u91cd\u8981\u89c1\u89e3\uff0c\u6709\u52a9\u4e8e\u6539\u8fdb\u91cf\u5b50\u7b97\u6cd5\u7684\u8bbe\u8ba1\u548c\u8bc4\u4f30\u3002", "motivation": "\u89e3\u51b3\u771f\u5b9e\u566a\u58f0\u4e2d\u7b49\u89c4\u6a21\u91cf\u5b50\uff08NISQ\uff09\u8bbe\u5907\u4e0a\u7684\u590d\u6742\u566a\u58f0\u95ee\u9898\uff0c\u8fd9\u4e9b\u566a\u58f0\u5305\u542b\u4e0d\u53ef\u63a7\u56e0\u7d20\u548c\u7279\u5b9a\u4eea\u5668\u6548\u5e94\uff08\u5982\u4e32\u6270\uff09\uff0c\u800c\u73b0\u6709\u7684\u57fa\u4e8e\u7406\u60f3\u566a\u58f0\u5047\u8bbe\u7684\u5f20\u91cf\u7f51\u7edc\u6a21\u62df\u5668\uff08\u7279\u522b\u662fMPDO\uff09\u65e0\u6cd5\u6ee1\u8db3\u9700\u6c42\u3002", "method": "\u5229\u7528\u91cf\u5b50\u8fc7\u7a0b\u5c42\u6790\uff08QPT\uff09\u6280\u672f\u6355\u6349\u5b9e\u9a8c\u566a\u58f0\u7279\u6027\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230\u77e9\u9635\u4e58\u79ef\u5bc6\u5ea6\u7b97\u5b50\uff08MPDO\uff09\u6846\u67b6\u7684\u6570\u503c\u6a21\u62df\u4e2d\u3002", "result": "\u5f00\u53d1\u4e86MPDO\u8f85\u52a9\u6a21\u62df\u5668\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u63a2\u7d22\u751f\u6210\u566a\u58f0\u7ea0\u7f20\u6001\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u4e0e\u6807\u51c6\u566a\u58f0\u6a21\u62df\u548cQuafu\u4e91\u91cf\u5b50\u8ba1\u7b97\u5e73\u53f0\u4e0a\u7684\u6f14\u793a\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u7814\u7a76\u4e86\u566a\u58f0MaxCut\u95ee\u9898\u4ee5\u53ca\u4e32\u6270\u548c\u566a\u58f0\u622a\u65ad\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u4e3a\u7406\u89e3\u566a\u58f0\u5728NISQ\u8bbe\u5907\u4e0a\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u5728\u590d\u6742\u566a\u58f0\u73af\u5883\u4e2d\u8bbe\u8ba1\u548c\u8bc4\u4f30\u91cf\u5b50\u7b97\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "conclusion": "\u672c\u7814\u7a76\u5c06\u91cf\u5b50\u8fc7\u7a0b\u5c42\u6790\uff08QPT\uff09\u6280\u672f\u4e0e\u77e9\u9635\u4e58\u79ef\u5bc6\u5ea6\u7b97\u5b50\uff08MPDO\uff09\u76f8\u7ed3\u5408\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u80fd\u591f\u76f4\u63a5\u6355\u6349\u5b9e\u9a8c\u8bbe\u5907\u8fd0\u884c\u7279\u6027\u5e76\u5c06\u5176\u96c6\u6210\u5230\u6570\u503c\u6a21\u62df\u4e2d\u7684MPDO\u8f85\u52a9\u6a21\u62df\u5668\uff0c\u4ee5\u89e3\u51b3\u771f\u5b9e\u566a\u58f0\u4e2d\u7b49\u89c4\u6a21\u91cf\u5b50\uff08NISQ\uff09\u8bbe\u5907\u4e0a\u7684\u590d\u6742\u566a\u58f0\u95ee\u9898\u3002\u8be5\u6a21\u62df\u5668\u5e94\u7528\u4e8e\u63a2\u7d22\u751f\u6210\u566a\u58f0\u7ea0\u7f20\u6001\u7684\u53d8\u5206\u65b9\u6cd5\uff0c\u5e76\u7814\u7a76\u4e86\u566a\u58f0MaxCut\u95ee\u9898\u4ee5\u53ca\u4e32\u6270\u548c\u566a\u58f0\u622a\u65ad\u7684\u5f71\u54cd\u3002\u7ed3\u679c\u4e3a\u7406\u89e3\u566a\u58f0\u5728NISQ\u8bbe\u5907\u4e0a\u7684\u5f71\u54cd\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u89c1\u89e3\uff0c\u5e76\u4e3a\u5728\u590d\u6742\u566a\u58f0\u73af\u5883\u4e2d\u8bbe\u8ba1\u548c\u8bc4\u4f30\u91cf\u5b50\u7b97\u6cd5\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.07179", "categories": ["cs.CL", "cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.07179", "abs": "https://arxiv.org/abs/2508.07179", "authors": ["Jiaqi Yin", "Yi-Wei Chen", "Meng-Lung Lee", "Xiya Liu"], "title": "Schema Lineage Extraction at Scale: Multilingual Pipelines, Composite Evaluation, and Language-Model Benchmarks", "comment": null, "summary": "Enterprise data pipelines, characterized by complex transformations across\nmultiple programming languages, often cause a semantic disconnect between\noriginal metadata and downstream data. This \"semantic drift\" compromises data\nreproducibility and governance, and impairs the utility of services like\nretrieval-augmented generation (RAG) and text-to-SQL systems. To address this,\na novel framework is proposed for the automated extraction of fine-grained\nschema lineage from multilingual enterprise pipeline scripts. This method\nidentifies four key components: source schemas, source tables, transformation\nlogic, and aggregation operations, creating a standardized representation of\ndata transformations. For the rigorous evaluation of lineage quality, this\npaper introduces the Schema Lineage Composite Evaluation (SLiCE), a metric that\nassesses both structural correctness and semantic fidelity. A new benchmark is\nalso presented, comprising 1,700 manually annotated lineages from real-world\nindustrial scripts. Experiments were conducted with 12 language models, from\n1.3B to 32B small language models (SLMs) to large language models (LLMs) like\nGPT-4o and GPT-4.1. The results demonstrate that the performance of schema\nlineage extraction scales with model size and the sophistication of prompting\ntechniques. Specially, a 32B open-source model, using a single reasoning trace,\ncan achieve performance comparable to the GPT series under standard prompting.\nThis finding suggests a scalable and economical approach for deploying\nschema-aware agents in practical applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u591a\u8bed\u8a00\u4f01\u4e1a\u6570\u636e\u7ba1\u9053\u4e2d\u81ea\u52a8\u63d0\u53d6\u6a21\u5f0f\u6cbf\u88ad\u7684\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u8bed\u4e49\u6f02\u79fb\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7 SLiCE \u6307\u6807\u548c\u65b0\u57fa\u51c6\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u8bc1\u660e\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u63d0\u9ad8\u63d0\u53d6\u51c6\u786e\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6a21\u5f0f\u611f\u77e5\u4ee3\u7406\u90e8\u7f72\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4f01\u4e1a\u6570\u636e\u7ba1\u9053\u4e2d\u7684\u201c\u8bed\u4e49\u6f02\u79fb\u201d\u4f1a\u635f\u5bb3\u6570\u636e\u53ef\u91cd\u73b0\u6027\u548c\u6cbb\u7406\uff0c\u5e76\u964d\u4f4e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u6587\u672c\u5230 SQL \u7cfb\u7edf\u7b49\u670d\u52a1\u7684\u5b9e\u7528\u6027\u3002", "method": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u56db\u4e2a\u5173\u952e\u7ec4\u4ef6\uff08\u6e90\u6a21\u5f0f\u3001\u6e90\u8868\u3001\u8f6c\u6362\u903b\u8f91\u548c\u805a\u5408\u64cd\u4f5c\uff09\u6765\u81ea\u52a8\u5316\u591a\u8bed\u8a00\u4f01\u4e1a\u7ba1\u9053\u811a\u672c\u4e2d\u7684\u7ec6\u7c92\u5ea6\u6a21\u5f0f\u6cbf\u88ad\u63d0\u53d6\uff0c\u4ece\u800c\u521b\u5efa\u6570\u636e\u8f6c\u6362\u7684\u6807\u51c6\u8868\u793a\u3002\u6b64\u5916\uff0c\u8be5\u8bba\u6587\u8fd8\u5f15\u5165\u4e86\u6a21\u5f0f\u6cbf\u88ad\u590d\u5408\u8bc4\u4f30\uff08SLiCE\uff09\u6307\u6807\u6765\u8bc4\u4f30\u6cbf\u88ad\u8d28\u91cf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b 1,700 \u4e2a\u624b\u52a8\u6ce8\u91ca\u6cbf\u88ad\u7684\u65b0\u57fa\u51c6\uff0c\u4ee5\u8fdb\u884c\u4e25\u683c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6a21\u5f0f\u6cbf\u88ad\u63d0\u53d6\u7684\u6027\u80fd\u968f\u7740\u6a21\u578b\u5927\u5c0f\u548c\u63d0\u793a\u6280\u672f\u7684\u590d\u6742\u6027\u800c\u6269\u5c55\uff0c\u5176\u4e2d\u4e00\u4e2a 32B \u7684\u5f00\u6e90\u6a21\u578b\u4f7f\u7528\u5355\u4e00\u63a8\u7406\u94fe\uff0c\u5728\u6807\u51c6\u63d0\u793a\u4e0b\u5373\u53ef\u8fbe\u5230\u4e0e GPT \u7cfb\u5217\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u7ecf\u6d4e\u9ad8\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u53ef\u5c06\u6a21\u5f0f\u611f\u77e5\u4ee3\u7406\u90e8\u7f72\u5230\u5b9e\u9645\u5e94\u7528\u7a0b\u5e8f\u4e2d\u3002"}}
{"id": "2508.07334", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07334", "abs": "https://arxiv.org/abs/2508.07334", "authors": ["Quan Shi", "Wang Xi", "Zenghui Ding", "Jianqing Gao", "Xianjun Yang"], "title": "Hallucination as a Computational Boundary: A Hierarchy of Inevitability and the Oracle Escape", "comment": "8 pages, 6 figures", "summary": "The illusion phenomenon of large language models (LLMs) is the core obstacle\nto their reliable deployment. This article formalizes the large language model\nas a probabilistic Turing machine by constructing a \"computational necessity\nhierarchy\", and for the first time proves the illusions are inevitable on\ndiagonalization, incomputability, and information theory boundaries supported\nby the new \"learner pump lemma\". However, we propose two \"escape routes\": one\nis to model Retrieval Enhanced Generations (RAGs) as oracle machines, proving\ntheir absolute escape through \"computational jumps\", providing the first formal\ntheory for the effectiveness of RAGs; The second is to formalize continuous\nlearning as an \"internalized oracle\" mechanism and implement this path through\na novel neural game theory framework.Finally, this article proposes a", "AI": {"tldr": "\u672c\u6587\u5c06LLM\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u56fe\u7075\u673a\uff0c\u8bc1\u660e\u4e86\u5e7b\u89c9\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u5e76\u63d0\u51fa\u4e86RAG\u548c\u8fde\u7eed\u5b66\u4e60\u7684\u201c\u9003\u751f\u8def\u7ebf\u201d\u3002", "motivation": "LLM\u7684\u5e7b\u89c9\u73b0\u8c61\u662f\u5176\u53ef\u9760\u90e8\u7f72\u7684\u6838\u5fc3\u969c\u788d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u201c\u9003\u751f\u8def\u7ebf\u201d\uff1a\u4e00\u79cd\u662f\u5c06\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u5efa\u6a21\u4e3a\u9884\u8a00\u673a\uff0c\u901a\u8fc7\u201c\u8ba1\u7b97\u8df3\u8dc3\u201d\u8bc1\u660e\u5176\u7edd\u5bf9\u9003\u751f\uff0c\u4e3aRAG\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u5f62\u5f0f\u5316\u7406\u8bba\uff1b\u7b2c\u4e8c\u79cd\u662f\u5c06\u8fde\u7eed\u5b66\u4e60\u5f62\u5f0f\u5316\u4e3a\u201c\u5185\u90e8\u5316\u9884\u8a00\u673a\u201d\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u65b0\u9896\u7684\u795e\u7ecf\u535a\u5f08\u8bba\u6846\u67b6\u5b9e\u73b0\u8fd9\u4e00\u8def\u5f84\u3002", "result": "\u672c\u6587\u4e3aRAG\u7684\u6709\u6548\u6027\u63d0\u4f9b\u4e86\u7b2c\u4e00\u4e2a\u5f62\u5f0f\u5316\u7406\u8bba\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u795e\u7ecf\u535a\u5f08\u8bba\u6846\u67b6\u3002", "conclusion": "LLM\u7684\u5e7b\u89c9\u662f\u5176\u53ef\u9760\u90e8\u7f72\u7684\u6838\u5fc3\u969c\u788d\u3002\u672c\u6587\u901a\u8fc7\u6784\u5efa\u201c\u8ba1\u7b97\u5fc5\u8981\u6027\u5c42\u6b21\u7ed3\u6784\u201d\uff0c\u5c06LLM\u5f62\u5f0f\u5316\u4e3a\u6982\u7387\u56fe\u7075\u673a\uff0c\u5e76\u9996\u6b21\u8bc1\u660e\u4e86\u5728\u4e8c\u5143\u5316\u3001\u4e0d\u53ef\u8ba1\u7b97\u6027\u548c\u4fe1\u606f\u8bba\u8fb9\u754c\u4e0a\u5e7b\u89c9\u662f\u4e0d\u53ef\u907f\u514d\u7684\uff0c\u8fd9\u5f97\u5230\u4e86\u65b0\u7684\u201c\u5b66\u4e60\u8005\u6cf5\u5f15\u7406\u201d\u7684\u652f\u6301\u3002"}}
{"id": "2508.06915", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06915", "abs": "https://arxiv.org/abs/2508.06915", "authors": ["Shichao Ma", "Zhengyang Zhou", "Qihe Huang", "Binwu Wang", "Kuo Yang", "Huan Li", "Yang Wang"], "title": "QuiZSF: An efficient data-model interaction framework for zero-shot time-series forecasting", "comment": null, "summary": "Time series forecasting has become increasingly important to empower diverse\napplications with streaming data. Zero-shot time-series forecasting (ZSF),\nparticularly valuable in data-scarce scenarios, such as domain transfer or\nforecasting under extreme conditions, is difficult for traditional models to\ndeal with. While time series pre-trained models (TSPMs) have demonstrated\nstrong performance in ZSF, they often lack mechanisms to dynamically\nincorporate external knowledge. Fortunately, emerging retrieval-augmented\ngeneration (RAG) offers a promising path for injecting such knowledge on\ndemand, yet they are rarely integrated with TSPMs. To leverage the strengths of\nboth worlds, we introduce RAG into TSPMs to enhance zero-shot time series\nforecasting. In this paper, we propose QuiZSF (Quick Zero-Shot Time Series\nForecaster), a lightweight and modular framework that couples efficient\nretrieval with representation learning and model adaptation for ZSF.\nSpecifically, we construct a hierarchical tree-structured ChronoRAG Base (CRB)\nfor scalable time-series storage and domain-aware retrieval, introduce a\nMulti-grained Series Interaction Learner (MSIL) to extract fine- and\ncoarse-grained relational features, and develop a dual-branch Model Cooperation\nCoherer (MCC) that aligns retrieved knowledge with two kinds of TSPMs: Non-LLM\nbased and LLM based. Compared with contemporary baselines, QuiZSF, with Non-LLM\nbased and LLM based TSPMs as base model, respectively, ranks Top1 in 75% and\n87.5% of prediction settings, while maintaining high efficiency in memory and\ninference time.", "AI": {"tldr": "QuiZSF\u662f\u4e00\u4e2a\u7ed3\u5408\u4e86RAG\u548cTSPMs\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u3002\u5b83\u901a\u8fc7CRB\u3001MSIL\u548cMCC\u7b49\u7ec4\u4ef6\uff0c\u5728\u591a\u4e2a\u9884\u6d4b\u573a\u666f\u4e2d\u90fd\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6548\u7387\u5f88\u9ad8\u3002", "motivation": "\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08ZSF\uff09\u5728\u6570\u636e\u7a00\u758f\u7684\u573a\u666f\u4e0b\uff08\u5982\u9886\u57df\u8fc1\u79fb\u6216\u6781\u7aef\u6761\u4ef6\u9884\u6d4b\uff09\u5177\u6709\u91cd\u8981\u4ef7\u503c\uff0c\u4f46\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5904\u7406\u3002\u867d\u7136\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u6a21\u578b\uff08TSPMs\uff09\u5728ZSF\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u7f3a\u4e4f\u52a8\u6001\u6574\u5408\u5916\u90e8\u77e5\u8bc6\u7684\u673a\u5236\u3002\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6280\u672f\u53ef\u4ee5\u6309\u9700\u6ce8\u5165\u5916\u90e8\u77e5\u8bc6\uff0c\u4f46\u5f88\u5c11\u4e0eTSPMs\u7ed3\u5408\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5229\u7528RAG\u548cTSPMs\u7684\u4f18\u70b9\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u589e\u5f3aZSF\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86QuiZSF\u6846\u67b6\uff0c\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u4e14\u6a21\u5757\u5316\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u9ad8\u6548\u68c0\u7d22\u3001\u8868\u793a\u5b66\u4e60\u548c\u6a21\u578b\u9002\u5e94\uff0c\u7528\u4e8e\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08ZSF\uff09\u3002\u5177\u4f53\u800c\u8a00\uff0c\u7814\u7a76\u6784\u5efa\u4e86\u4e00\u4e2a\u5206\u5c42\u7684\u6811\u72b6ChronoRAG Base\uff08CRB\uff09\u7528\u4e8e\u53ef\u6269\u5c55\u7684\u65f6\u95f4\u5e8f\u5217\u5b58\u50a8\u548c\u9886\u57df\u611f\u77e5\u68c0\u7d22\uff1b\u5f15\u5165\u4e86\u4e00\u4e2aMulti-grained Series Interaction Learner\uff08MSIL\uff09\u6765\u63d0\u53d6\u7ec6\u7c92\u5ea6\u548c\u7c97\u7c92\u5ea6\u7684\u5173\u7cfb\u7279\u5f81\uff1b\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53cc\u5206\u652fModel Cooperation Coherer\uff08MCC\uff09\u6765\u5c06\u68c0\u7d22\u5230\u7684\u77e5\u8bc6\u4e0e\u4e24\u79cdTSPM\uff08\u975eLLM\u548cLLM\uff09\u8fdb\u884c\u5bf9\u9f50\u3002", "result": "QuiZSF\u6846\u67b6\u5728\u4f7f\u7528\u975eLLM\u548cLLM\u4f5c\u4e3a\u57fa\u7840\u6a21\u578b\u65f6\uff0c\u5206\u522b\u572875%\u548c87.5%\u7684\u9884\u6d4b\u8bbe\u7f6e\u4e2d\u6392\u540d\u7b2c\u4e00\uff0c\u5e76\u4e14\u5728\u5185\u5b58\u548c\u63a8\u7406\u65f6\u95f4\u65b9\u9762\u4fdd\u6301\u4e86\u9ad8\u6548\u7387\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "QuiZSF\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u65f6\u95f4\u5e8f\u5217\u9884\u8bad\u7ec3\u6a21\u578b\uff08TSPM\uff09\uff0c\u5728\u96f6\u6837\u672c\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\uff08ZSF\uff09\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\u3002\u8be5\u6846\u67b6\u5305\u542bChronoRAG Base\uff08CRB\uff09\u7528\u4e8e\u9ad8\u6548\u68c0\u7d22\uff0cMulti-grained Series Interaction Learner\uff08MSIL\uff09\u7528\u4e8e\u63d0\u53d6\u591a\u7c92\u5ea6\u7279\u5f81\uff0c\u4ee5\u53caModel Cooperation Coherer\uff08MCC\uff09\u7528\u4e8e\u6a21\u578b\u9002\u5e94\u3002QuiZSF\u5728\u591a\u4e2a\u9884\u6d4b\u573a\u666f\u4e2d\u5747\u6392\u540d\u7b2c\u4e00\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u826f\u597d\u7684\u5185\u5b58\u548c\u63a8\u7406\u6548\u7387\u3002"}}
{"id": "2508.07826", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07826", "abs": "https://arxiv.org/abs/2508.07826", "authors": ["Muktai Aote", "A. V. Deshpande", "Vaibhav Sirsulwar", "Priya Padaganur", "Neha", "Abhishek Pradhan"], "title": "Impact of Ce Substitution on Structural and Electrochemical Properties of Ga Doped Garnet Li7La3Zr2O12 Solid Electrolyte", "comment": null, "summary": "In order to replace conventional liquid electrolytes, solid electrolyte\nshould possess high ionic conductivity. In this study, the effects of Ga-Ce\nco-doping on the garnet Li7La3Zr2O12 solid electrolyte have been investigated.\nThe series Li6.4Ga0.2La3Zr2-xCexO12 has been prepared with varying content of\nCe from 0 to 0.30 atoms per formula unit (a.p.f.u.) by sintering at 1050^0C.\nVarious structural characterizations namely X-diffraction, Scanning Electron\nMicroscopy (SEM), density measurements were carried out. The electrochemical\nanalysis suggested that, the sample with 0.10 a.p.f.u. of Ce offered the\nhighest room temperature ionic conductivity of 4 x 10-4 S/cm with the minimum\nactivation energy of 0.29 eV. Moreover, DC conductivity measurement proved the\npredominant ionic conduction in the prepared samples making it suitable for the\napplication in all solid state Li-ion batteries (ASSLIBs).", "AI": {"tldr": "\u7814\u7a76\u4e86Ga-Ce\u5171\u63ba\u6742\u5bf9Li7La3Zr2O12\u56fa\u6001\u7535\u89e3\u8d28\u7684\u5f71\u54cd\uff0c\u53d1\u73b0Li6.4Ga0.2La3Zr2-xCexO12\uff08Ce=0.10 a.p.f.u.\uff09\u5728\u5ba4\u6e29\u4e0b\u5177\u67094 x 10^-4 S/cm\u7684\u6700\u9ad8\u79bb\u5b50\u7535\u5bfc\u7387\u548c0.29 eV\u7684\u6700\u4f4e\u6d3b\u5316\u80fd\uff0c\u9002\u7528\u4e8e\u5168\u56fa\u6001\u9502\u79bb\u5b50\u7535\u6c60\u3002", "motivation": "\u4e3a\u4e86\u66ff\u4ee3\u4f20\u7edf\u7684\u6db2\u4f53\u7535\u89e3\u8d28\uff0c\u5f00\u53d1\u5177\u6709\u9ad8\u79bb\u5b50\u7535\u5bfc\u7387\u7684\u56fa\u4f53\u7535\u89e3\u8d28\u662f\u5173\u952e\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22Ga-Ce\u5171\u63ba\u6742\u5bf9\u77f3\u69b4\u77f3Li7La3Zr2O12\u56fa\u6001\u7535\u89e3\u8d28\u7684\u5f71\u54cd\uff0c\u4ee5\u671f\u63d0\u9ad8\u5176\u79bb\u5b50\u7535\u5bfc\u7387\uff0c\u6ee1\u8db3\u5168\u56fa\u6001\u9502\u79bb\u5b50\u7535\u6c60\u7684\u5e94\u7528\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u70e7\u7ed3\u5236\u5907Li6.4Ga0.2La3Zr2-xCexO12\uff08Ce\u542b\u91cf\u4ece0\u52300.30 a.p.f.u.\uff09\uff0c\u5e76\u91c7\u7528X\u5c04\u7ebf\u884d\u5c04\u3001\u626b\u63cf\u7535\u5b50\u663e\u5fae\u955c\u548c\u5bc6\u5ea6\u6d4b\u91cf\u7b49\u7ed3\u6784\u8868\u5f81\u6280\u672f\uff0c\u4ee5\u53ca\u7535\u5316\u5b66\u5206\u6790\u548c\u76f4\u6d41\u7535\u5bfc\u7387\u6d4b\u91cf\u6765\u8bc4\u4f30\u6750\u6599\u6027\u80fd\u3002", "result": "Li6.4Ga0.2La3Zr2-xCexO12\u6837\u54c1\u57281050\u00b0C\u70e7\u7ed3\u540e\uff0cCe\u542b\u91cf\u4e3a0.10 a.p.f.u.\u7684\u6837\u54c1\u5728\u5ba4\u6e29\u4e0b\u5c55\u73b0\u51fa\u6700\u9ad8\u7684\u79bb\u5b50\u7535\u5bfc\u7387\uff0c\u8fbe\u52304 x 10^-4 S/cm\uff0c\u540c\u65f6\u5177\u6709\u6700\u4f4e\u7684\u6d3b\u5316\u80fd0.29 eV\u3002\u76f4\u6d41\u7535\u5bfc\u7387\u6d4b\u91cf\u8bc1\u5b9e\u4e86\u5176\u4e3b\u8981\u7684\u79bb\u5b50\u4f20\u5bfc\u6027\u3002", "conclusion": "Ga-Ce\u5171\u63ba\u6742\u5bf9\u77f3\u69b4\u77f3Li7La3Zr2O12\u56fa\u6001\u7535\u89e3\u8d28\u7684\u6027\u80fd\u8fdb\u884c\u4e86\u7814\u7a76\uff0c\u5176\u4e2dLi6.4Ga0.2La3Zr2-xCexO12\uff08Ce\u542b\u91cf\u4e3a0\u81f30.30 a.p.f.u.\uff09\u57281050\u00b0C\u70e7\u7ed3\u540e\u8868\u73b0\u51fa\u4f18\u5f02\u7684\u6027\u80fd\u3002Ce\u542b\u91cf\u4e3a0.10 a.p.f.u.\u7684\u6837\u54c1\u5728\u5ba4\u6e29\u4e0b\u8868\u73b0\u51fa\u6700\u9ad8\u7684\u79bb\u5b50\u7535\u5bfc\u7387\uff084 x 10^-4 S/cm\uff09\uff0c\u5177\u6709\u6700\u4f4e\u7684\u6d3b\u5316\u80fd\uff080.29 eV\uff09\u3002\u76f4\u6d41\u7535\u5bfc\u7387\u6d4b\u91cf\u8bc1\u5b9e\u4e86\u5176\u4e3b\u8981\u7684\u79bb\u5b50\u4f20\u5bfc\u7279\u6027\uff0c\u4f7f\u5176\u6210\u4e3a\u5168\u56fa\u6001\u9502\u79bb\u5b50\u7535\u6c60\u7684\u6f5c\u5728\u5019\u9009\u6750\u6599\u3002"}}
{"id": "2508.07421", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07421", "abs": "https://arxiv.org/abs/2508.07421", "authors": ["Zixi Jia", "Hongbin Gao", "Fashe Li", "Jiqiang Liu", "Hexiao Li", "Qinghua Liu"], "title": "Triple-S: A Collaborative Multi-LLM Framework for Solving Long-Horizon Implicative Tasks in Robotics", "comment": "Accepted to IROS 2025", "summary": "Leveraging Large Language Models (LLMs) to write policy code for controlling\nrobots has gained significant attention. However, in long-horizon implicative\ntasks, this approach often results in API parameter, comments and sequencing\nerrors, leading to task failure. To address this problem, we propose a\ncollaborative Triple-S framework that involves multiple LLMs. Through\nIn-Context Learning, different LLMs assume specific roles in a closed-loop\nSimplification-Solution-Summary process, effectively improving success rates\nand robustness in long-horizon implicative tasks. Additionally, a novel\ndemonstration library update mechanism which learned from success allows it to\ngeneralize to previously failed tasks. We validate the framework in the\nLong-horizon Desktop Implicative Placement (LDIP) dataset across various\nbaseline models, where Triple-S successfully executes 89% of tasks in both\nobservable and partially observable scenarios. Experiments in both simulation\nand real-world robot settings further validated the effectiveness of Triple-S.\nOur code and dataset is available at: https://github.com/Ghbbbbb/Triple-S.", "AI": {"tldr": "Triple-S\u6846\u67b6\u4f7f\u7528\u591a\u4e2aLLM\u534f\u4f5c\uff0c\u901a\u8fc7\u7b80\u5316-\u89e3\u51b3-\u603b\u7ed3\u7684\u6d41\u7a0b\uff0c\u89e3\u51b3\u4e86LLM\u751f\u6210\u673a\u5668\u4eba\u7b56\u7565\u4ee3\u7801\u5728\u957f\u65f6\u5e8f\u4efb\u52a1\u4e2d\u7684\u9519\u8bef\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u826f\u597d\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684LLM\u7528\u4e8e\u673a\u5668\u4eba\u7b56\u7565\u4ee3\u7801\u751f\u6210\u7684\u65b9\u6cd5\u5728\u957f\u65f6\u5e8f\u9690\u542b\u4efb\u52a1\u4e2d\u5b58\u5728API\u53c2\u6570\u3001\u6ce8\u91ca\u548c\u6392\u5e8f\u9519\u8bef\u7b49\u95ee\u9898\uff0c\u5bfc\u81f4\u4efb\u52a1\u5931\u8d25\u3002\u9700\u8981\u4e00\u79cd\u66f4\u6709\u6548\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTriple-S\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u591aLLM\u901a\u8fc7\u60c5\u5883\u5b66\u4e60\uff08In-Context Learning\uff09\u626e\u6f14\u4e0d\u540c\u89d2\u8272\uff0c\u6267\u884c\u7b80\u5316-\u89e3\u51b3-\u603b\u7ed3\uff08Simplification-Solution-Summary\uff09\u7684\u95ed\u73af\u8fc7\u7a0b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6f14\u793a\u5e93\u66f4\u65b0\u673a\u5236\uff0c\u80fd\u591f\u4ece\u6210\u529f\u6848\u4f8b\u4e2d\u5b66\u4e60\u5e76\u6cdb\u5316\u5230\u5148\u524d\u5931\u8d25\u7684\u4efb\u52a1\u3002", "result": "\u5728LDIP\u6570\u636e\u96c6\u4e0a\uff0cTriple-S\u5728\u53ef\u89c2\u5bdf\u548c\u90e8\u5206\u53ef\u89c2\u5bdf\u573a\u666f\u4e0b\u6210\u529f\u6267\u884c\u4e8689%\u7684\u4efb\u52a1\u3002\u8be5\u6846\u67b6\u5728\u6a21\u62df\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u90fd\u5f97\u5230\u4e86\u6709\u6548\u9a8c\u8bc1\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTriple-S\u7684\u534f\u4f5c\u6846\u67b6\uff0c\u5229\u7528\u591aLLM\u5728\u957f\u65f6\u5e8f\u9690\u542b\u4efb\u52a1\u4e2d\u751f\u6210\u673a\u5668\u4eba\u7b56\u7565\u4ee3\u7801\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u6210\u529f\u7387\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u5728LDIP\u6570\u636e\u96c6\u548c\u771f\u5b9e\u673a\u5668\u4eba\u73af\u5883\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002"}}
{"id": "2508.07635", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07635", "abs": "https://arxiv.org/abs/2508.07635", "authors": ["Anju Rani", "Xiaoyu Ai", "Aman Gupta", "Ravi Singh Adhikari", "Robert Malaney"], "title": "Obfuscated Quantum and Post-Quantum Cryptography", "comment": "13 pages, 5 figures, 2 tables, 4 algorithms", "summary": "In this work, we present an experimental deployment of a new design for\ncombined quantum key distribution (QKD) and post-quantum cryptography (PQC).\nNovel to our system is the dynamic obfuscation of the QKD-PQC sequence of\noperations, the number of operations, and parameters related to the operations;\ncoupled to the integration of a GPS-free quantum synchronization protocol\nwithin the QKD process. We compare the performance and overhead of our QKD-PQC\nsystem relative to a standard QKD system with one-time pad encryption,\ndemonstrating that our design can operate in real time with little additional\noverhead caused by the new security features. Since our system can offer\nadditional defensive strategies against a wide spectrum of practical attacks\nthat undermine deployed QKD, PQC, and certain combinations of these two\nprimitives, we suggest that our design represents one of the most secure\ncommunication systems currently available. Given the dynamic nature of its\nobfuscation attributes, our new system can also be adapted in the field to\ndefeat yet-to-be-discovered practical attacks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408QKD\u548cPQC\u7684\u65b0\u578b\u901a\u4fe1\u7cfb\u7edf\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u7cca\u5316\u548c\u91cf\u5b50\u540c\u6b65\u534f\u8bae\u589e\u5f3a\u4e86\u5b89\u5168\u6027\uff0c\u5e76\u8bc1\u660e\u5176\u5728\u5b9e\u65f6\u6027\u4e0e\u4f4e\u5f00\u9500\u65b9\u9762\u4f18\u4e8e\u6807\u51c6QKD\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u901a\u4fe1\u7cfb\u7edf\u7684\u5b89\u5168\u6027\uff0c\u5e94\u5bf9\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u7684\u5404\u79cd\u5a01\u80c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u91cf\u5b50\u5bc6\u94a5\u5206\u53d1\uff08QKD\uff09\u548c\u540e\u91cf\u5b50\u5bc6\u7801\u5b66\uff08PQC\uff09\u7684\u65b0\u8bbe\u8ba1\uff0c\u5e76\u8fdb\u884c\u4e86\u5b9e\u9a8c\u90e8\u7f72\u3002\u8be5\u7cfb\u7edf\u7684\u521b\u65b0\u4e4b\u5904\u5728\u4e8e\u52a8\u6001\u6a21\u7cca\u5316QKD-PQC\u64cd\u4f5c\u5e8f\u5217\u3001\u64cd\u4f5c\u6b21\u6570\u4ee5\u53ca\u76f8\u5173\u53c2\u6570\uff0c\u5e76\u96c6\u6210\u4e86\u65e0GPS\u91cf\u5b50\u540c\u6b65\u534f\u8bae\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5QKD-PQC\u7cfb\u7edf\u80fd\u591f\u5b9e\u65f6\u8fd0\u884c\uff0c\u4e14\u7531\u4e8e\u65b0\u7684\u5b89\u5168\u7279\u6027\u800c\u4ea7\u751f\u7684\u989d\u5916\u5f00\u9500\u5f88\u5c0f\u3002\u4e0e\u6807\u51c6QKD\u7cfb\u7edf\u76f8\u6bd4\uff0c\u8be5\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u5b89\u5168\u6027\u3002", "conclusion": "\u8be5\u8bbe\u8ba1\u4ee3\u8868\u4e86\u76ee\u524d\u6700\u5b89\u5168\u3001\u53ef\u9002\u5e94\u6027\u6700\u5f3a\u7684\u901a\u4fe1\u7cfb\u7edf\u4e4b\u4e00\uff0c\u80fd\u591f\u62b5\u5fa1\u5e7f\u6cdb\u7684\u5b9e\u9645\u653b\u51fb\u3002"}}
{"id": "2508.07185", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7; H.3.3; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.07185", "abs": "https://arxiv.org/abs/2508.07185", "authors": ["Kabir Khan", "Priya Sharma", "Arjun Mehta", "Neha Gupta", "Ravi Narayanan"], "title": "DySK-Attn: A Framework for Efficient, Real-Time Knowledge Updating in Large Language Models via Dynamic Sparse Knowledge Attention", "comment": "Preprint; 7 figures, 3 tables, 1 algorithm; v1. Code and data will be\n  released", "summary": "Large Language Models (LLMs) suffer from a critical limitation: their\nknowledge is static and quickly becomes outdated. Retraining these massive\nmodels is computationally prohibitive, while existing knowledge editing\ntechniques can be slow and may introduce unforeseen side effects. To address\nthis, we propose DySK-Attn, a novel framework that enables LLMs to efficiently\nintegrate real-time knowledge from a dynamic external source. Our approach\nsynergizes an LLM with a dynamic Knowledge Graph (KG) that can be updated\ninstantaneously. The core of our framework is a sparse knowledge attention\nmechanism, which allows the LLM to perform a coarse-to-fine grained search,\nefficiently identifying and focusing on a small, highly relevant subset of\nfacts from the vast KG. This mechanism avoids the high computational cost of\ndense attention over the entire knowledge base and mitigates noise from\nirrelevant information. We demonstrate through extensive experiments on\ntime-sensitive question-answering tasks that DySK-Attn significantly\noutperforms strong baselines, including standard Retrieval-Augmented Generation\n(RAG) and model editing techniques, in both factual accuracy for updated\nknowledge and computational efficiency. Our framework offers a scalable and\neffective solution for building LLMs that can stay current with the\never-changing world.", "AI": {"tldr": "DySK-Attn \u901a\u8fc7\u7ed3\u5408\u8bed\u8a00\u5927\u6a21\u578b\u548c\u52a8\u6001\u77e5\u8bc6\u56fe\uff0c\u5e76\u5229\u7528\u7a00\u758f\u77e5\u8bc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5b9e\u65f6\u77e5\u8bc6\u96c6\u6210\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u5927\u6a21\u578b\u77e5\u8bc6\u8fc7\u65f6\u7684\u95ee\u9898\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8bed\u8a00\u5927\u6a21\u578b\u77e5\u8bc6\u9759\u6001\u4e14\u8fc5\u901f\u8fc7\u65f6\u7684\u95ee\u9898\uff0c\u800c\u91cd\u65b0\u8bad\u7ec3\u8fd9\u4e9b\u6a21\u578b\u6210\u672c\u9ad8\u6602\uff0c\u73b0\u6709\u7684\u77e5\u8bc6\u7f16\u8f91\u6280\u672f\u53ef\u80fd\u901f\u5ea6\u6162\u4e14\u4f1a\u5e26\u6765\u610f\u60f3\u4e0d\u5230\u7684\u526f\u4f5c\u7528\u3002", "method": "DySK-Attn \u6846\u67b6\u7684\u6838\u5fc3\u662f\u7a00\u758f\u77e5\u8bc6\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b83\u5141\u8bb8\u8bed\u8a00\u5927\u6a21\u578b\u6267\u884c\u4ece\u7c97\u5230\u7ec6\u7684\u641c\u7d22\uff0c\u6709\u6548\u5730\u4ece\u5e9e\u5927\u7684\u77e5\u8bc6\u56fe\u4e2d\u8bc6\u522b\u5e76\u5173\u6ce8\u4e00\u5c0f\u90e8\u5206\u9ad8\u5ea6\u76f8\u5173\u7684\u77e5\u8bc6\u3002\u8be5\u673a\u5236\u907f\u514d\u4e86\u5bf9\u6574\u4e2a\u77e5\u8bc6\u5e93\u8fdb\u884c\u5bc6\u96c6\u6ce8\u610f\u529b\u7684\u9ad8\u5ea6\u8ba1\u7b97\u6210\u672c\uff0c\u5e76\u51cf\u5c11\u4e86\u6765\u81ea\u4e0d\u76f8\u5173\u4fe1\u606f\u7684\u5e72\u6270\u3002", "result": "DySK-Attn \u5728\u65f6\u95f4\u654f\u611f\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u5305\u62ec\u6807\u51c6\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u548c\u6a21\u578b\u7f16\u8f91\u6280\u672f\u5728\u5185\u7684\u5f3a\u6709\u529b\u57fa\u7ebf\u76f8\u6bd4\uff0cDySK-Attn \u5728\u66f4\u65b0\u77e5\u8bc6\u7684\u4e8b\u5b9e\u51c6\u786e\u6027\u548c\u8ba1\u7b97\u6548\u7387\u65b9\u9762\u90fd\u663e\u8457\u4f18\u4e8e\u5b83\u4eec\u3002", "conclusion": "DySK-Attn \u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u4e14\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u7528\u4e8e\u6784\u5efa\u80fd\u591f\u8ddf\u4e0a\u77ac\u606f\u4e07\u53d8\u7684\u4e16\u754c\u7684\u8bed\u8a00\u5927\u6a21\u578b\u3002"}}
{"id": "2508.06763", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06763", "abs": "https://arxiv.org/abs/2508.06763", "authors": ["Zihao Sheng", "Zilin Huang", "Yen-Jung Chen", "Yansong Qu", "Yuhao Luo", "Yue Leng", "Sikai Chen"], "title": "SafePLUG: Empowering Multimodal LLMs with Pixel-Level Insight and Temporal Grounding for Traffic Accident Understanding", "comment": "The code, dataset, and model checkpoints will be made publicly\n  available at: https://zihaosheng.github.io/SafePLUG", "summary": "Multimodal large language models (MLLMs) have achieved remarkable progress\nacross a range of vision-language tasks and demonstrate strong potential for\ntraffic accident understanding. However, existing MLLMs in this domain\nprimarily focus on coarse-grained image-level or video-level comprehension and\noften struggle to handle fine-grained visual details or localized scene\ncomponents, limiting their applicability in complex accident scenarios. To\naddress these limitations, we propose SafePLUG, a novel framework that empowers\nMLLMs with both Pixel-Level Understanding and temporal Grounding for\ncomprehensive traffic accident analysis. SafePLUG supports both\narbitrary-shaped visual prompts for region-aware question answering and\npixel-level segmentation based on language instructions, while also enabling\nthe recognition of temporally anchored events in traffic accident scenarios. To\nadvance the development of MLLMs for traffic accident understanding, we curate\na new dataset containing multimodal question-answer pairs centered on diverse\naccident scenarios, with detailed pixel-level annotations and temporal event\nboundaries. Experimental results show that SafePLUG achieves strong performance\non multiple tasks, including region-based question answering, pixel-level\nsegmentation, temporal event localization, and accident event understanding.\nThese capabilities lay a foundation for fine-grained understanding of complex\ntraffic scenes, with the potential to improve driving safety and enhance\nsituational awareness in smart transportation systems. The code, dataset, and\nmodel checkpoints will be made publicly available at:\nhttps://zihaosheng.github.io/SafePLUG", "AI": {"tldr": "SafePLUG \u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u65f6\u95f4\u5b9a\u4f4d\u589e\u5f3a MLLMs \u5728\u4ea4\u901a\u4e8b\u6545\u5206\u6790\u4e2d\u7684\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u7ec6\u7c92\u5ea6\u7ec6\u8282\u5904\u7406\u4e0a\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u591a\u9879\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709 MLLMs \u5728\u4ea4\u901a\u4e8b\u6545\u7406\u89e3\u9886\u57df\u4e3b\u8981\u5173\u6ce8\u56fe\u50cf\u6216\u89c6\u9891\u5c42\u9762\u7684\u7c97\u7c92\u5ea6\u7406\u89e3\uff0c\u96be\u4ee5\u5904\u7406\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u7ec6\u8282\u6216\u5c40\u90e8\u573a\u666f\u7ec4\u4ef6\uff0c\u9650\u5236\u4e86\u5176\u5728\u590d\u6742\u4ea4\u901a\u4e8b\u6545\u573a\u666f\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa SafePLUG \u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u652f\u6301\u4efb\u610f\u5f62\u72b6\u7684\u89c6\u89c9\u63d0\u793a\u8fdb\u884c\u533a\u57df\u611f\u77e5\u95ee\u7b54\u3001\u57fa\u4e8e\u8bed\u8a00\u6307\u4ee4\u7684\u50cf\u7d20\u7ea7\u5206\u5272\uff0c\u5e76\u80fd\u8bc6\u522b\u4ea4\u901a\u4e8b\u6545\u573a\u666f\u4e2d\u7684\u65f6\u95f4\u951a\u5b9a\u4e8b\u4ef6\u3002", "result": "SafePLUG \u5728\u533a\u57df\u95ee\u7b54\u3001\u50cf\u7d20\u7ea7\u5206\u5272\u3001\u65f6\u95f4\u4e8b\u4ef6\u5b9a\u4f4d\u548c\u4e8b\u6545\u4e8b\u4ef6\u7406\u89e3\u7b49\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "conclusion": "SafePLUG \u901a\u8fc7\u5f15\u5165\u50cf\u7d20\u7ea7\u7406\u89e3\u548c\u65f6\u95f4\u5b9a\u4f4d\u80fd\u529b\uff0c\u63d0\u5347\u4e86 MLLMs \u5728\u4ea4\u901a\u4e8b\u6545\u7406\u89e3\u65b9\u9762\u7684\u6027\u80fd\uff0c\u4e3a\u590d\u6742\u4ea4\u901a\u573a\u666f\u7684\u7ec6\u7c92\u5ea6\u7406\u89e3\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u6709\u671b\u63d0\u9ad8\u9a7e\u9a76\u5b89\u5168\u548c\u667a\u80fd\u4ea4\u901a\u7cfb\u7edf\u7684\u6001\u52bf\u611f\u77e5\u80fd\u529b\u3002"}}
{"id": "2508.07353", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07353", "abs": "https://arxiv.org/abs/2508.07353", "authors": ["Rubing Chen", "Jiaxin Wu", "Jian Wang", "Xulu Zhang", "Wenqi Fan", "Chenghua Lin", "Xiao-Yong Wei", "Qing Li"], "title": "Rethinking Domain-Specific LLM Benchmark Construction: A Comprehensiveness-Compactness Approach", "comment": null, "summary": "Numerous benchmarks have been built to evaluate the domain-specific abilities\nof large language models (LLMs), highlighting the need for effective and\nefficient benchmark construction. Existing domain-specific benchmarks primarily\nfocus on the scaling law, relying on massive corpora for supervised fine-tuning\nor generating extensive question sets for broad coverage. However, the impact\nof corpus and question-answer (QA) set design on the precision and recall of\ndomain-specific LLMs remains unexplored. In this paper, we address this gap and\ndemonstrate that the scaling law is not always the optimal principle for\nbenchmark construction in specific domains. Instead, we propose Comp-Comp, an\niterative benchmarking framework based on a comprehensiveness-compactness\nprinciple. Here, comprehensiveness ensures semantic recall of the domain, while\ncompactness enhances precision, guiding both corpus and QA set construction. To\nvalidate our framework, we conducted a case study in a well-renowned\nuniversity, resulting in the creation of XUBench, a large-scale and\ncomprehensive closed-domain benchmark. Although we use the academic domain as\nthe case in this work, our Comp-Comp framework is designed to be extensible\nbeyond academia, providing valuable insights for benchmark construction across\nvarious domains.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6 Comp-Comp\uff0c\u7528\u4e8e\u6539\u8fdb\u7279\u5b9a\u9886\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u8bc4\u4f30\u3002\u8be5\u6846\u67b6\u4f18\u5148\u8003\u8651\u5168\u9762\u6027\u548c\u7d27\u51d1\u6027\uff0c\u800c\u4e0d\u662f\u4f20\u7edf\u7684\u6269\u5c55\u5b9a\u5f8b\uff0c\u5e76\u5728\u5b66\u672f\u9886\u57df\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a XUBench \u7684\u65b0\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u9886\u57df\u7279\u5b9a\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4e3b\u8981\u5173\u6ce8\u6269\u5c55\u5b9a\u5f8b\uff0c\u4f9d\u8d56\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\u6216\u751f\u6210\u5e7f\u6cdb\u7684\u95ee\u7b54\u96c6\u3002\u7136\u800c\uff0c\u8bed\u6599\u5e93\u548c\u95ee\u7b54\u96c6\u8bbe\u8ba1\u5bf9\u9886\u57df\u7279\u5b9aLLMs\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u7684\u5f71\u54cd\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Comp-Comp \u7684\u8fed\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u6027-\u7d27\u51d1\u6027\u539f\u5219\uff0c\u7528\u4e8e\u6307\u5bfc\u8bed\u6599\u5e93\u548c\u95ee\u7b54\u96c6\uff08QA set\uff09\u7684\u6784\u5efa\uff0c\u4ee5\u63d0\u9ad8\u7279\u5b9a\u9886\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u7cbe\u786e\u7387\u548c\u53ec\u56de\u7387\u3002", "result": "\u901a\u8fc7\u5728\u67d0\u77e5\u540d\u5927\u5b66\u8fdb\u884c\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u521b\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3a XUBench \u7684\u5927\u89c4\u6a21\u3001\u5168\u9762\u7684\u95ed\u57df\u57fa\u51c6\u6d4b\u8bd5\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cComp-Comp \u6846\u67b6\u5728\u5b66\u672f\u9886\u57df\u662f\u6709\u6548\u7684\uff0c\u5e76\u4e14\u8be5\u6846\u67b6\u5177\u6709\u5e7f\u6cdb\u7684\u9002\u7528\u6027\uff0c\u53ef\u6269\u5c55\u5230\u5b66\u672f\u9886\u57df\u4e4b\u5916\u7684\u5176\u4ed6\u9886\u57df\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Comp-Comp \u7684\u8fed\u4ee3\u57fa\u51c6\u6d4b\u8bd5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u57fa\u4e8e\u5168\u9762\u6027-\u7d27\u51d1\u6027\u539f\u5219\uff0c\u65e8\u5728\u6539\u8fdb\u7279\u5b9a\u9886\u57df\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u57fa\u51c6\u6d4b\u8bd5\u65b9\u6cd5\u3002\u7814\u7a76\u8868\u660e\uff0c\u6269\u5c55\u5b9a\u5f8b\u5e76\u975e\u7279\u5b9a\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u7684\u6700\u4f73\u539f\u5219\uff0c\u800c Comp-Comp \u6846\u67b6\u901a\u8fc7\u786e\u4fdd\u8bed\u4e49\u53ec\u56de\u7387\uff08\u5168\u9762\u6027\uff09\u548c\u63d0\u9ad8\u7cbe\u786e\u7387\uff08\u7d27\u51d1\u6027\uff09\u6765\u6307\u5bfc\u8bed\u6599\u5e93\u548c\u95ee\u7b54\u5bf9\u7684\u6784\u5efa\u3002"}}
{"id": "2508.06943", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06943", "abs": "https://arxiv.org/abs/2508.06943", "authors": ["Lishi Zuo", "Man-Wai Mak", "Lu Yi", "Youzhi Tu"], "title": "Class Unbiasing for Generalization in Medical Diagnosis", "comment": null, "summary": "Medical diagnosis might fail due to bias. In this work, we identified\nclass-feature bias, which refers to models' potential reliance on features that\nare strongly correlated with only a subset of classes, leading to biased\nperformance and poor generalization on other classes. We aim to train a\nclass-unbiased model (Cls-unbias) that mitigates both class imbalance and\nclass-feature bias simultaneously. Specifically, we propose a class-wise\ninequality loss which promotes equal contributions of classification loss from\npositive-class and negative-class samples. We propose to optimize a class-wise\ngroup distributionally robust optimization objective-a class-weighted training\nobjective that upweights underperforming classes-to enhance the effectiveness\nof the inequality loss under class imbalance. Through synthetic and real-world\ndatasets, we empirically demonstrate that class-feature bias can negatively\nimpact model performance. Our proposed method effectively mitigates both\nclass-feature bias and class imbalance, thereby improving the model's\ngeneralization ability.", "AI": {"tldr": "\u63d0\u51faCls-unbias\u65b9\u6cd5\uff0c\u901a\u8fc7\u7c7b\u522b\u4e0d\u5747\u8861\u635f\u5931\u548c\u7c7b\u522b\u4e0d\u5747\u8861\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316\uff0c\u89e3\u51b3\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u7c7b\u522b-\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u5347\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u504f\u89c1\u95ee\u9898\uff0c\u7279\u522b\u662f\u8bc6\u522b\u51fa\u7c7b\u522b-\u7279\u5f81\u504f\u5dee\uff0c\u5373\u6a21\u578b\u53ef\u80fd\u4f9d\u8d56\u4e0e\u4ec5\u4e00\u90e8\u5206\u7c7b\u522b\u5f3a\u76f8\u5173\u7684\u7279\u5f81\uff0c\u5bfc\u81f4\u5728\u5176\u4ed6\u7c7b\u522b\u4e0a\u8868\u73b0\u51fa\u504f\u89c1\u548c\u6cdb\u5316\u80fd\u529b\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u522b\u4e0d\u5747\u8861\u635f\u5931\uff0c\u4ee5\u4fc3\u8fdb\u6765\u81ea\u6b63\u7c7b\u548c\u8d1f\u7c7b\u6837\u672c\u7684\u5206\u7c7b\u635f\u5931\u7684\u76f8\u7b49\u8d21\u732e\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7c7b\u522b\u4e0d\u5747\u8861\u7684\u5206\u5e03\u9c81\u68d2\u4f18\u5316\u76ee\u6807\uff0c\u4ee5\u589e\u5f3a\u5728\u7c7b\u522b\u4e0d\u5747\u8861\u60c5\u51b5\u4e0b\u7c7b\u522b\u4e0d\u5747\u8861\u635f\u5931\u7684\u6709\u6548\u6027\u3002", "result": "\u901a\u8fc7\u5408\u6210\u548c\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\uff0c\u5b9e\u8bc1\u8bc1\u660e\u4e86\u7c7b\u522b-\u7279\u5f81\u504f\u5dee\u4f1a\u5bf9\u6a21\u578b\u6027\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u6709\u6548\u7f13\u89e3\u7c7b\u522b-\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\u7f13\u89e3\u4e86\u7c7b\u522b-\u7279\u5f81\u504f\u5dee\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07858", "categories": ["cond-mat.mtrl-sci", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.07858", "abs": "https://arxiv.org/abs/2508.07858", "authors": ["Magnus A. H. Christiansen", "Wei Wang", "Elvar \u00d6. J\u00f3nsson", "Giancarlo Cicero", "Hannes J\u00f3nsson"], "title": "Multiple Adsorption of CO Molecules on Transition Metal Substitutional Impurities in Copper Surfaces", "comment": null, "summary": "Copper-based catalysts are of particular interest for electrochemical\nreduction of CO$_2$ (CO2RR) as products beyond CO can form. To improve activity\nand selectivity, several studies have focused on the addition of other elements\nas substitutional impurities. Although the adsorption of a single CO molecule\nhas often been used as a descriptor for CO2RR activity, our recent calculations\nusing the RPBE functional showed that multiple CO molecules can bind to\nfirst-row transition metal impurities. Here, we extend the study to second-row\ntransition metals and also to a functional that explicitly includes dispersion\ninteraction, BEEF-vdW. The binding energy of the first CO molecule on the\nimpurity atom is found to be significantly larger than on the clean Cu(111) and\nCu(100) surfaces, but the differential binding energy generally drops as more\nCO molecules adsorb. The dispersion interaction is found to make a significant\ncontribution to the binding energy, in particular for the last and weakest\nbound CO molecule, the one that is most likely to participate in CO2RR. In some\ncases, four CO admolecules can bind more strongly on the impurity atom than on\nthe clean copper surface. The adsorption of CO causes the position of the\nimpurity atom to shift outwards and in some cases, even escape from the surface\nlayer. The C-O stretch frequencies are calculated in order to identify possible\nexperimental signatures of multiple CO adsorption.", "AI": {"tldr": "\u94dc\u57fa\u50ac\u5316\u5242\u6dfb\u52a0\u7b2c\u4e8c\u884c\u8fc7\u6e21\u91d1\u5c5e\u6742\u8d28\u53ef\u4ee5\u63d0\u9ad8CO2RR\u7684\u6d3b\u6027\u548c\u9009\u62e9\u6027\u3002CO\u5206\u5b50\u5728\u6742\u8d28\u4e0a\u7684\u5438\u9644\u80fd\u529b\u66f4\u5f3a\uff0c\u8272\u6563\u76f8\u4e92\u4f5c\u7528\u5c24\u5176\u91cd\u8981\u3002CO\u5438\u9644\u4f1a\u5f15\u8d77\u6742\u8d28\u539f\u5b50\u4f4d\u79fb\uff0cC-O\u62c9\u4f38\u9891\u7387\u53ef\u7528\u4e8e\u5b9e\u9a8c\u8bc6\u522b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u94dc\u57fa\u50ac\u5316\u5242\u5728CO2RR\u4e2d\u7684\u6d3b\u6027\u548c\u9009\u62e9\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u7b2c\u4e8c\u884c\u8fc7\u6e21\u91d1\u5c5e\u6742\u8d28\u5bf9CO\u5438\u9644\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u8003\u8651\u8272\u6563\u76f8\u4e92\u4f5c\u7528\u7684\u4f5c\u7528\uff0c\u4ee5\u671f\u53d1\u73b0\u80fd\u66f4\u597d\u5730\u63cf\u8ff0CO2RR\u6d3b\u6027\u7684\u5438\u9644\u63cf\u8ff0\u7b26\u3002", "method": "\u4f7f\u7528RPBE\u51fd\u6570\u548cBEEF-vdW\u51fd\u6570\uff0c\u7814\u7a76\u4e86\u7b2c\u4e8c\u884c\u8fc7\u6e21\u91d1\u5c5e\u6742\u8d28\u5bf9\u94dc\u57fa\u50ac\u5316\u5242\u7535\u5316\u5b66\u8fd8\u539fCO2\uff08CO2RR\uff09\u7684\u5f71\u54cd\u3002\u901a\u8fc7\u8ba1\u7b97CO\u5206\u5b50\u5728\u542b\u6742\u8d28\u94dc\u8868\u9762\u7684\u5438\u9644\u80fd\u548c\u5dee\u5206\u5438\u9644\u80fd\uff0c\u5206\u6790\u6742\u8d28\u5bf9\u50ac\u5316\u6d3b\u6027\u7684\u5f71\u54cd\u3002\u6b64\u5916\uff0c\u8fd8\u8ba1\u7b97\u4e86C-O\u62c9\u4f38\u9891\u7387\uff0c\u4ee5\u5bfb\u627e\u591a\u91cdCO\u5438\u9644\u7684\u5b9e\u9a8c\u4fe1\u53f7\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0cCO\u5206\u5b50\u5728\u7b2c\u4e8c\u884c\u8fc7\u6e21\u91d1\u5c5e\u6742\u8d28\u4e0a\u7684\u5438\u9644\u80fd\u6bd4\u5728\u7eaf\u94dc\u8868\u9762\u663e\u8457\u66f4\u9ad8\uff0c\u4f46\u968f\u7740\u5438\u9644CO\u5206\u5b50\u6570\u91cf\u7684\u589e\u52a0\uff0c\u5dee\u5206\u5438\u9644\u80fd\u901a\u5e38\u4f1a\u4e0b\u964d\u3002\u8272\u6563\u76f8\u4e92\u4f5c\u7528\u663e\u8457\u5f71\u54cd\u5438\u9644\u80fd\uff0c\u5c24\u5176\u662f\u5728\u6700\u5f31\u5438\u9644\u7684CO\u5206\u5b50\u4e0a\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u56db\u4e2aCO\u5206\u5b50\u5728\u6742\u8d28\u539f\u5b50\u4e0a\u7684\u5438\u9644\u80fd\u529b\u53ef\u80fd\u5f3a\u4e8e\u5728\u7eaf\u94dc\u8868\u9762\u3002CO\u5438\u9644\u8fd8\u4f1a\u5bfc\u81f4\u6742\u8d28\u539f\u5b50\u79fb\u4f4d\u751a\u81f3\u8131\u79bb\u8868\u9762\u5c42\u3002\u8ba1\u7b97\u7684C-O\u62c9\u4f38\u9891\u7387\u4e3a\u8bc6\u522b\u591a\u91cdCO\u5438\u9644\u7684\u5b9e\u9a8c\u7279\u5f81\u63d0\u4f9b\u4e86\u4f9d\u636e\u3002", "conclusion": "\u94dc\u57fa\u50ac\u5316\u5242\u5728CO2RR\u4e2d\u5bf9\u751f\u6210CO\u4ee5\u5916\u7684\u4ea7\u7269\u5177\u6709\u6f5c\u529b\u3002\u901a\u8fc7\u6dfb\u52a0\u5176\u4ed6\u5143\u7d20\u4f5c\u4e3a\u66ff\u4ee3\u6742\u8d28\u53ef\u4ee5\u63d0\u9ad8\u50ac\u5316\u5242\u7684\u6d3b\u6027\u548c\u9009\u62e9\u6027\u3002RPBE\u51fd\u6570\u548cBEEF-vdW\u51fd\u6570\u7684\u7814\u7a76\u8868\u660e\uff0cCO\u5206\u5b50\u5728\u6742\u8d28\u539f\u5b50\u4e0a\u7684\u5438\u9644\u80fd\u6bd4\u5728\u7eafCu(111)\u548cCu(100)\u8868\u9762\u4e0a\u663e\u8457\u66f4\u9ad8\uff0c\u5e76\u4e14\u5438\u9644\u7684CO\u5206\u5b50\u8d8a\u591a\uff0c\u5dee\u5206\u5438\u9644\u80fd\u8d8a\u4f4e\u3002\u8272\u6563\u76f8\u4e92\u4f5c\u7528\u5bf9\u5438\u9644\u80fd\u6709\u663e\u8457\u8d21\u732e\uff0c\u5c24\u5176\u662f\u5728\u5438\u9644\u7684\u6700\u540e\u4e00\u4e2aCO\u5206\u5b50\u4e0a\uff0c\u8be5\u5206\u5b50\u6700\u6709\u53ef\u80fd\u53c2\u4e0eCO2RR\u3002\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\uff0c\u56db\u4e2aCO\u5206\u5b50\u5728\u6742\u8d28\u539f\u5b50\u4e0a\u7684\u5438\u9644\u80fd\u529b\u53ef\u80fd\u5f3a\u4e8e\u5728\u7eaf\u94dc\u8868\u9762\u4e0a\u7684\u5438\u9644\u80fd\u529b\u3002CO\u7684\u5438\u9644\u4f1a\u5bfc\u81f4\u6742\u8d28\u539f\u5b50\u5411\u5916\u79fb\u52a8\uff0c\u751a\u81f3\u8131\u79bb\u8868\u9762\u5c42\u3002\u8ba1\u7b97C-O\u62c9\u4f38\u9891\u7387\u4ee5\u8bc6\u522b\u591a\u91cdCO\u5438\u9644\u7684\u6f5c\u5728\u5b9e\u9a8c\u7279\u5f81\u3002"}}
{"id": "2508.07502", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07502", "abs": "https://arxiv.org/abs/2508.07502", "authors": ["Mateus Salom\u00e3o", "Tiany\u00fc Ren", "Alexander K\u00f6nig"], "title": "A Learning-Based Framework for Collision-Free Motion Planning", "comment": null, "summary": "This paper presents a learning-based extension to a Circular Field (CF)-based\nmotion planner for efficient, collision-free trajectory generation in cluttered\nenvironments. The proposed approach overcomes the limitations of hand-tuned\nforce field parameters by employing a deep neural network trained to infer\noptimal planner gains from a single depth image of the scene. The pipeline\nincorporates a CUDA-accelerated perception module, a predictive agent-based\nplanning strategy, and a dataset generated through Bayesian optimization in\nsimulation. The resulting framework enables real-time planning without manual\nparameter tuning and is validated both in simulation and on a Franka Emika\nPanda robot. Experimental results demonstrate successful task completion and\nimproved generalization compared to classical planners.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u8fd0\u52a8\u89c4\u5212\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728\u6742\u4e71\u73af\u5883\u4e2d\u9ad8\u6548\u751f\u6210\u65e0\u78b0\u649e\u8f68\u8ff9\uff0c\u901a\u8fc7\u6df1\u5ea6\u5b66\u4e60\u81ea\u52a8\u8c03\u6574\u53c2\u6570\uff0c\u5b9e\u73b0\u4e86\u5b9e\u65f6\u89c4\u5212\u548c\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u624b\u52a8\u8c03\u6574\u529b\u573a\u53c2\u6570\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u57fa\u4e8e\u5706\u5f62\u89c6\u573a\uff08CF\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u5668\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6269\u5c55\uff0c\u4ee5\u5728\u6742\u4e71\u73af\u5883\u4e2d\u5b9e\u73b0\u9ad8\u6548\u3001\u65e0\u78b0\u649e\u7684\u8f68\u8ff9\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6269\u5c55\uff0c\u7528\u4e8e\u57fa\u4e8e\u5706\u5f62\u89c6\u573a\uff08CF\uff09\u7684\u8fd0\u52a8\u89c4\u5212\u5668\uff0c\u901a\u8fc7\u8bad\u7ec3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6765\u4ece\u573a\u666f\u7684\u5355\u4e2a\u6df1\u5ea6\u56fe\u50cf\u63a8\u65ad\u6700\u4f18\u89c4\u5212\u5668\u589e\u76ca\uff0c\u514b\u670d\u4e86\u624b\u52a8\u8c03\u6574\u529b\u573a\u53c2\u6570\u7684\u5c40\u9650\u6027\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7ecf\u5178\u89c4\u5212\u5668\u76f8\u6bd4\uff0c\u4efb\u52a1\u5b8c\u6210\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u5747\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u5728\u65e0\u624b\u52a8\u53c2\u6570\u8c03\u6574\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u5b9e\u65f6\u89c4\u5212\uff0c\u5e76\u5728\u6a21\u62df\u548cFranka Emika Panda\u673a\u5668\u4eba\u4e0a\u5f97\u5230\u9a8c\u8bc1\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u7ecf\u5178\u89c4\u5212\u5668\u76f8\u6bd4\uff0c\u4efb\u52a1\u5b8c\u6210\u5ea6\u548c\u6cdb\u5316\u80fd\u529b\u5747\u5f97\u5230\u63d0\u5347\u3002"}}
{"id": "2508.07652", "categories": ["quant-ph", "cond-mat.dis-nn", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.07652", "abs": "https://arxiv.org/abs/2508.07652", "authors": ["D. A. Konyshev", "V. V. Mazurenko"], "title": "Estimating classical mutual information between quantum subsystems with neural networks", "comment": null, "summary": "Characterizing correlations in a quantum system on the basis of the results\nof the projective measurements can be performed with different means including\nthe calculation of the classical mutual information. Generally, estimating such\ninformation-entropy-based quantities requires having complete statistics of the\nsystem's states. Here we explore the possibility to reconstruct the classical\nmutual information and specific entropy of a quantum system with neural network\napproach on the basis of limited number of projective measurements. As a\nprominent example we consider the antiferromagnetic quantum Ising model in\ntransverse and longitudinal magnetic fields which is in demand in both\ncondensed matter physics and quantum computing. We show that the neural network\napproach gives reliable estimates of the classical mutual information even in\nthe case of paramagnetic wave functions delocalized in the state space. In\naddition, the phase diagram of the considered quantum system is reconstructed\nwith a special focus on discriminating various types of disordered states.", "AI": {"tldr": "Neural networks can estimate quantum system information from limited measurements, successfully applied to the quantum Ising model's phase diagram.", "motivation": "Estimating information-entropy-based quantities like classical mutual information typically requires complete statistics. This work investigates the potential of neural networks to perform such estimations with limited data.", "method": "The study explores reconstructing classical mutual information and specific entropy of a quantum system using a neural network approach with limited projective measurements.", "result": "The neural network approach effectively estimates classical mutual information even for paramagnetic wave functions. It also successfully reconstructs the phase diagram of the quantum Ising model, differentiating various disordered states.", "conclusion": "The neural network approach provides reliable estimates of classical mutual information, even for delocalized paramagnetic wave functions. The phase diagram of the quantum Ising model, with a focus on distinguishing disordered states, can be reconstructed."}}
{"id": "2508.07195", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07195", "abs": "https://arxiv.org/abs/2508.07195", "authors": ["Yanru Sun", "Emadeldeen Eldele", "Zongxia Xie", "Yucheng Wang", "Wenzhe Niu", "Qinghua Hu", "Chee Keong Kwoh", "Min Wu"], "title": "Adapting LLMs to Time Series Forecasting via Temporal Heterogeneity Modeling and Semantic Alignment", "comment": null, "summary": "Large Language Models (LLMs) have recently demonstrated impressive\ncapabilities in natural language processing due to their strong generalization\nand sequence modeling capabilities. However, their direct application to time\nseries forecasting remains challenging due to two fundamental issues: the\ninherent heterogeneity of temporal patterns and the modality gap between\ncontinuous numerical signals and discrete language representations. In this\nwork, we propose TALON, a unified framework that enhances LLM-based forecasting\nby modeling temporal heterogeneity and enforcing semantic alignment.\nSpecifically, we design a Heterogeneous Temporal Encoder that partitions\nmultivariate time series into structurally coherent segments, enabling\nlocalized expert modeling across diverse temporal patterns. To bridge the\nmodality gap, we introduce a Semantic Alignment Module that aligns temporal\nfeatures with LLM-compatible representations, enabling effective integration of\ntime series into language-based models while eliminating the need for\nhandcrafted prompts during inference. Extensive experiments on seven real-world\nbenchmarks demonstrate that TALON achieves superior performance across all\ndatasets, with average MSE improvements of up to 11\\% over recent\nstate-of-the-art methods. These results underscore the effectiveness of\nincorporating both pattern-aware and semantic-aware designs when adapting LLMs\nfor time series forecasting. The code is available at:\nhttps://github.com/syrGitHub/TALON.", "AI": {"tldr": "TALON\u6846\u67b6\u901a\u8fc7\u5904\u7406\u65f6\u95f4\u5f02\u8d28\u6027\u548c\u8bed\u4e49\u5bf9\u9f50\u6765\u589e\u5f3aLLM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "LLM\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u65b9\u9762\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u4f46\u5c06\u5176\u76f4\u63a5\u5e94\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\u7684\u5185\u5728\u5f02\u8d28\u6027\u548c\u8fde\u7eed\u6570\u503c\u4fe1\u53f7\u4e0e\u79bb\u6563\u8bed\u8a00\u8868\u793a\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3002", "method": "TALON\u6846\u67b6\u901a\u8fc7\u8bbe\u8ba1\u5f02\u8d28\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u6765\u5904\u7406\u65f6\u95f4\u6a21\u5f0f\u7684\u5f02\u8d28\u6027\uff0c\u8be5\u7f16\u7801\u5668\u5c06\u591a\u5143\u65f6\u95f4\u5e8f\u5217\u5212\u5206\u4e3a\u7ed3\u6784\u76f8\u5e72\u7684\u7247\u6bb5\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u65f6\u95f4\u6a21\u5f0f\u7684\u672c\u5730\u5316\u4e13\u5bb6\u5efa\u6a21\u3002\u4e3a\u4e86\u5f25\u5408\u6a21\u6001\u5dee\u8ddd\uff0c\u5f15\u5165\u4e86\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\uff0c\u5c06\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u4e0eLLM\u517c\u5bb9\u7684\u8868\u793a\u5bf9\u9f50\uff0c\u4ece\u800c\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u65e0\u9700\u624b\u5de5\u63d0\u793a\u5373\u53ef\u5b9e\u73b0\u65f6\u95f4\u5e8f\u5217\u4e0e\u57fa\u4e8e\u8bed\u8a00\u7684\u6a21\u578b\u6709\u6548\u96c6\u6210\u3002", "result": "\u5728\u4e03\u4e2a\u771f\u5b9e\u4e16\u754c\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cTALON\u5728\u6240\u6709\u6570\u636e\u96c6\u4e0a\u5747\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e73\u5747\u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u76f8\u8f83\u4e8e\u8fd1\u671f\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u5e73\u5747\u63d0\u9ad8\u4e8611%\u3002", "conclusion": "LLM\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u7136\u5177\u6709\u6311\u6218\u6027\uff0c\u9700\u8981\u89e3\u51b3\u65f6\u95f4\u5e8f\u5217\u6a21\u5f0f\u7684\u5f02\u8d28\u6027\u548c\u8fde\u7eed\u6570\u503c\u4fe1\u53f7\u4e0e\u79bb\u6563\u8bed\u8a00\u8868\u793a\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3002TALON\u6846\u67b6\u901a\u8fc7\u5bf9\u5f02\u8d28\u65f6\u95f4\u5e8f\u5217\u8fdb\u884c\u5206\u533a\uff0c\u5b9e\u73b0\u8de8\u4e0d\u540c\u65f6\u95f4\u6a21\u5f0f\u7684\u672c\u5730\u5316\u4e13\u5bb6\u5efa\u6a21\uff0c\u5e76\u5f15\u5165\u8bed\u4e49\u5bf9\u9f50\u6a21\u5757\u5c06\u65f6\u95f4\u5e8f\u5217\u7279\u5f81\u4e0eLLM\u517c\u5bb9\u8868\u793a\u5bf9\u9f50\uff0c\u4ece\u800c\u6709\u6548\u5f25\u5408\u4e86\u6a21\u6001\u5dee\u8ddd\u3002"}}
{"id": "2508.07841", "categories": ["cs.LG", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07841", "abs": "https://arxiv.org/abs/2508.07841", "authors": ["Carlo Cena", "Mauro Martini", "Marcello Chiaberge"], "title": "Learning Satellite Attitude Dynamics with Physics-Informed Normalising Flow", "comment": "In review", "summary": "Attitude control is a fundamental aspect of spacecraft operations. Model\nPredictive Control (MPC) has emerged as a powerful strategy for these tasks,\nrelying on accurate models of the system dynamics to optimize control actions\nover a prediction horizon. In scenarios where physics models are incomplete,\ndifficult to derive, or computationally expensive, machine learning offers a\nflexible alternative by learning the system behavior directly from data.\nHowever, purely data-driven models often struggle with generalization and\nstability, especially when applied to inputs outside their training domain. To\naddress these limitations, we investigate the benefits of incorporating\nPhysics-Informed Neural Networks (PINNs) into the learning of spacecraft\nattitude dynamics, comparing their performance with that of purely data-driven\napproaches. Using a Real-valued Non-Volume Preserving (Real NVP) neural network\narchitecture with a self-attention mechanism, we trained several models on\nsimulated data generated with the Basilisk simulator. Two training strategies\nwere considered: a purely data-driven baseline and a physics-informed variant\nto improve robustness and stability. Our results demonstrate that the inclusion\nof physics-based information significantly enhances the performance in terms of\nthe mean relative error of the best architectures found by 27.08%. These\nadvantages are particularly evident when the learned models are integrated into\nan MPC framework, where PINN-based models consistently outperform their purely\ndata-driven counterparts in terms of control accuracy and robustness, yielding\nimprovements of up to 42.86% in performance stability error and increased\nrobustness-to-noise.", "AI": {"tldr": "PINNs\u53ef\u63d0\u9ad8\u822a\u5929\u5668\u59ff\u6001\u63a7\u5236\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u5728\u7269\u7406\u6a21\u578b\u4e0d\u5b8c\u6574\u6216\u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u60c5\u51b5\u4e0b\uff0c\u673a\u5668\u5b66\u4e60\u53ef\u4f5c\u4e3a\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u5b58\u5728\u6cdb\u5316\u6027\u548c\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76PINNs\u7684\u4f18\u52bf\u3002", "method": "\u4f7f\u7528Real-valued Non-Volume Preserving (Real NVP)\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u7eaf\u6570\u636e\u9a71\u52a8\u548c\u7269\u7406\u4fe1\u606f\u4e24\u79cd\u7b56\u7565\u8bad\u7ec3\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u6bd4\u8f83\u3002", "result": "PINN-based\u6a21\u578b\u5728\u5e73\u5747\u76f8\u5bf9\u8bef\u5dee\u65b9\u9762\u6bd4\u7eaf\u6570\u636e\u9a71\u52a8\u6a21\u578b\u63d0\u9ad8\u4e8627.08%\uff0c\u5728MPC\u6846\u67b6\u4e0b\uff0c\u6027\u80fd\u7a33\u5b9a\u6027\u8bef\u5dee\u63d0\u9ad8\u4e8642.86%\uff0c\u5e76\u589e\u5f3a\u4e86\u5bf9\u566a\u58f0\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u7ed3\u5408\u7269\u7406\u4fe1\u606f\uff08PINNs\uff09\u7684\u5b66\u4e60\u65b9\u6cd5\u5728\u822a\u5929\u5668\u59ff\u6001\u52a8\u529b\u5b66\u63a7\u5236\u4e2d\u663e\u8457\u4f18\u4e8e\u7eaf\u6570\u636e\u9a71\u52a8\u65b9\u6cd5\uff0c\u5c24\u5176\u662f\u5728MPC\u6846\u67b6\u4e0b\uff0c\u80fd\u63d0\u9ad8\u63a7\u5236\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002"}}
{"id": "2508.07382", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07382", "abs": "https://arxiv.org/abs/2508.07382", "authors": ["He Kong", "Die Hu", "Jingguo Ge", "Liangxiong Li", "Hui Li", "Tong Li"], "title": "Pentest-R1: Towards Autonomous Penetration Testing Reasoning Optimized via Two-Stage Reinforcement Learning", "comment": null, "summary": "Automating penetration testing is crucial for enhancing cybersecurity, yet\ncurrent Large Language Models (LLMs) face significant limitations in this\ndomain, including poor error handling, inefficient reasoning, and an inability\nto perform complex end-to-end tasks autonomously. To address these challenges,\nwe introduce Pentest-R1, a novel framework designed to optimize LLM reasoning\ncapabilities for this task through a two-stage reinforcement learning pipeline.\nWe first construct a dataset of over 500 real-world, multi-step walkthroughs,\nwhich Pentest-R1 leverages for offline reinforcement learning (RL) to instill\nfoundational attack logic. Subsequently, the LLM is fine-tuned via online RL in\nan interactive Capture The Flag (CTF) environment, where it learns directly\nfrom environmental feedback to develop robust error self-correction and\nadaptive strategies. Our extensive experiments on the Cybench and AutoPenBench\nbenchmarks demonstrate the framework's effectiveness. On AutoPenBench,\nPentest-R1 achieves a 24.2\\% success rate, surpassing most state-of-the-art\nmodels and ranking second only to Gemini 2.5 Flash. On Cybench, it attains a\n15.0\\% success rate in unguided tasks, establishing a new state-of-the-art for\nopen-source LLMs and matching the performance of top proprietary models.\nAblation studies confirm that the synergy of both training stages is critical\nto its success.", "AI": {"tldr": "Pentest-R1\u662f\u4e00\u4e2a\u5229\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\uff08\u79bb\u7ebf+\u5728\u7ebf\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u5347LLM\u5728\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u8868\u73b0\u3002\u5b83\u901a\u8fc7\u4e13\u95e8\u7684\u6570\u636e\u96c6\u548cCTF\u73af\u5883\u8fdb\u884c\u8bad\u7ec3\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709LLM\u5728\u8be5\u9886\u57df\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u6210\u679c\u3002", "motivation": "\u5f53\u524d\u7684LLM\u5728\u81ea\u52a8\u5316\u6e17\u900f\u6d4b\u8bd5\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5305\u62ec\u9519\u8bef\u5904\u7406\u80fd\u529b\u5dee\u3001\u63a8\u7406\u6548\u7387\u4f4e\u4ee5\u53ca\u65e0\u6cd5\u81ea\u4e3b\u5b8c\u6210\u590d\u6742\u7684\u7aef\u5230\u7aef\u4efb\u52a1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u4f18\u5316\u7684\u65b9\u6cd5\u6765\u589e\u5f3aLLM\u5728\u7f51\u7edc\u5b89\u5168\u9886\u57df\u7684\u5e94\u7528\u80fd\u529b\u3002", "method": "\u8be5\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3aPentest-R1\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6d41\u6c34\u7ebf\u6765\u4f18\u5316LLM\u5728\u6e17\u900f\u6d4b\u8bd5\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002\u7b2c\u4e00\u9636\u6bb5\u662f\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u5229\u7528\u8d85\u8fc7500\u4e2a\u771f\u5b9e\u4e16\u754c\u7684\u591a\u6b65\u9aa4\u6f14\u7ec3\u6570\u636e\u96c6\u6765\u6784\u5efa\u57fa\u7840\u653b\u51fb\u903b\u8f91\u3002\u7b2c\u4e8c\u9636\u6bb5\u662f\u5728\u4ea4\u4e92\u5f0fCTF\u73af\u5883\u4e2d\u8fdb\u884c\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff0c\u901a\u8fc7\u73af\u5883\u53cd\u9988\u8fdb\u884c\u5fae\u8c03\uff0c\u4ee5\u57f9\u517b\u9c81\u68d2\u7684\u9519\u8bef\u81ea\u6211\u7ea0\u6b63\u548c\u9002\u5e94\u6027\u7b56\u7565\u3002", "result": "Pentest-R1\u5728AutoPenBench\u4e0a\u53d6\u5f97\u4e8624.2%\u7684\u6210\u529f\u7387\uff0c\u8868\u73b0\u4f18\u4e8e\u5927\u591a\u6570\u6700\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u4ec5\u6b21\u4e8eGemini 2.5 Flash\u3002\u5728Cybench\u7684\u65e0\u6307\u5bfc\u4efb\u52a1\u4e2d\uff0c\u5176\u6210\u529f\u7387\u4e3a15.0%\uff0c\u8fbe\u5230\u4e86\u5f00\u6e90LLM\u7684\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u5e76\u4e0e\u9876\u7ea7\u7684\u4e13\u6709\u6a21\u578b\u6027\u80fd\u76f8\u5f53\u3002\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u4e24\u4e2a\u8bad\u7ec3\u9636\u6bb5\u534f\u540c\u4f5c\u7528\u7684\u5173\u952e\u6027\u3002", "conclusion": "Pentest-R1\u901a\u8fc7\u4e24\u9636\u6bb5\u5f3a\u5316\u5b66\u4e60\u6d41\u6c34\u7ebf\u6210\u529f\u4f18\u5316\u4e86LLM\u5728\u6e17\u900f\u6d4b\u8bd5\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728AutoPenBench\u548cCybench\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6210\u679c\uff0c\u5728\u5f00\u6e90LLM\u4e2d\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002"}}
{"id": "2508.06944", "categories": ["cs.LG", "cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06944", "abs": "https://arxiv.org/abs/2508.06944", "authors": ["Lixuan He", "Jie Feng", "Yong Li"], "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal Imitation-Exploration Balance", "comment": null, "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of \\textbf{implicit\nrewards}, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce \\textbf{Adaptive Meta Fine-Tuning (AMFT)}, a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na \\textbf{meta-gradient adaptive weight controller} that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.", "AI": {"tldr": "LLMs are usually fine-tuned using SFT then RL, but this has issues. This paper introduces AMFT, a new method that combines SFT and RL in a single stage using a learnable controller to balance them. AMFT achieves better results and generalization on various reasoning tasks compared to existing methods.", "motivation": "The typical two-stage fine-tuning pipeline (SFT followed by RL) for LLMs suffers from catastrophic forgetting and suboptimal trade-offs between imitation and exploration. Existing single-stage methods lack a principled mechanism for dynamically balancing SFT and RL.", "method": "AMFT (Adaptive Meta Fine-Tuning) is a novel single-stage algorithm that reframes LLM fine-tuning for reasoning tasks through the lens of implicit rewards. It uses a meta-gradient adaptive weight controller to dynamically balance Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) by treating the SFT-RL balance as a learnable parameter, optimized to maximize long-term task performance. The approach is regularized by policy entropy for stability and autonomously discovers an effective training curriculum.", "result": "AMFT consistently establishes a new state-of-the-art across mathematical reasoning, abstract visual reasoning, and vision-language navigation benchmarks. It also demonstrates superior generalization on out-of-distribution tasks, with ablation studies confirming the effectiveness of its meta-learning controller for stability, sample efficiency, and overall performance.", "conclusion": "AMFT provides a principled and effective paradigm for LLM alignment, consistently establishing new state-of-the-art performance and demonstrating superior generalization on out-of-distribution tasks. The meta-learning controller is crucial for its stability, sample efficiency, and performance."}}
{"id": "2508.07874", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07874", "abs": "https://arxiv.org/abs/2508.07874", "authors": ["Pei Li", "Guanjian Hu", "Xiao Yu", "Bing Huang", "Song Li"], "title": "Optimizing the depth-dependent nitrogen-vacancy center quantum sensor in diamane", "comment": "8 pages, 3 figures", "summary": "Negatively charged nitrogen-vacancy (NV) center in diamond is the\nrepresentative solid state defect qubit for quantum information science,\noffering long coherence time at room temperature. To achieve high sensitivity\nand spatial resolution, shallow NV center near the surface are preferred.\nHowever, shallow NV center suffers from surface states and spin noise which\nreduce the photostability and coherence time. In this work, we systematically\nstudy the NV center in recently reported two-dimensional diamond, known as\ndiamane--using first-principles calculations. We show that the quantum\nconfinement in finite-layer diamane, with appropriate surface passivation,\ncould significantly modify the band structure. In particular, we identify\noxygen surface termination capable of hosting NV center in diamane while\noptimizing photostability compared to bulk diamond. Furthermore,\nlayer-dependent NV center demonstrates tunable zero-phonon-line and suppressed\nphonon side band, while retaining long coherence time. Our findings highlight\ndiamane as a promising platform for NV-based quantum information processing\nwith improved optical properties and stability", "AI": {"tldr": "We used first-principles calculations to study NV centers in a 2D diamond material called diamane. We found that diamane with oxygen surface termination can improve photostability and optical properties compared to bulk diamond, while maintaining long coherence times. This makes diamane a promising material for quantum information processing.", "motivation": "To achieve high sensitivity and spatial resolution, shallow NV center near the surface are preferred. However, shallow NV center suffers from surface states and spin noise which reduce the photostability and coherence time.", "method": "first-principles calculations", "result": "The quantum confinement in finite-layer diamane, with appropriate surface passivation, could significantly modify the band structure. Oxygen surface termination is capable of hosting NV center in diamane while optimizing photostability compared to bulk diamond. Layer-dependent NV center demonstrates tunable zero-phonon-line and suppressed phonon side band, while retaining long coherence time.", "conclusion": "diamane is a promising platform for NV-based quantum information processing with improved optical properties and stability"}}
{"id": "2508.07560", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07560", "abs": "https://arxiv.org/abs/2508.07560", "authors": ["Yan Gong", "Naibang Wang", "Jianli Lu", "Xinyu Zhang", "Yongsheng Gao", "Jie Zhao", "Zifan Huang", "Haozhi Bai", "Nanxin Zeng", "Nayu Su", "Lei Yang", "Ziying Song", "Xiaoxi Hu", "Xinmin Jiang", "Xiaojuan Zhang", "Susanto Rahardja"], "title": "Progressive Bird's Eye View Perception for Safety-Critical Autonomous Driving: A Comprehensive Survey", "comment": null, "summary": "Bird's-Eye-View (BEV) perception has become a foundational paradigm in\nautonomous driving, enabling unified spatial representations that support\nrobust multi-sensor fusion and multi-agent collaboration. As autonomous\nvehicles transition from controlled environments to real-world deployment,\nensuring the safety and reliability of BEV perception in complex scenarios -\nsuch as occlusions, adverse weather, and dynamic traffic - remains a critical\nchallenge. This survey provides the first comprehensive review of BEV\nperception from a safety-critical perspective, systematically analyzing\nstate-of-the-art frameworks and implementation strategies across three\nprogressive stages: single-modality vehicle-side, multimodal vehicle-side, and\nmulti-agent collaborative perception. Furthermore, we examine public datasets\nencompassing vehicle-side, roadside, and collaborative settings, evaluating\ntheir relevance to safety and robustness. We also identify key open-world\nchallenges - including open-set recognition, large-scale unlabeled data, sensor\ndegradation, and inter-agent communication latency - and outline future\nresearch directions, such as integration with end-to-end autonomous driving\nsystems, embodied intelligence, and large language models.", "AI": {"tldr": "BEV perception is crucial for autonomous driving, but real-world deployment faces safety challenges. This survey reviews current methods, datasets, and future research directions for safer BEV perception.", "motivation": "To address the critical challenge of ensuring the safety and reliability of BEV perception in complex real-world scenarios as autonomous vehicles transition from controlled environments to deployment.", "method": "Systematic review and analysis of state-of-the-art BEV perception frameworks, implementation strategies, and public datasets, focusing on safety and robustness in autonomous driving.", "result": "An analysis of state-of-the-art BEV perception frameworks, evaluation of public datasets for safety and robustness, and identification of key open-world challenges and future research directions.", "conclusion": "This survey provides the first comprehensive review of Bird's-Eye-View (BEV) perception from a safety-critical perspective, systematically analyzing state-of-the-art frameworks and implementation strategies across three progressive stages: single-modality vehicle-side, multimodal vehicle-side, and multi-agent collaborative perception. It also examines public datasets and identifies key open-world challenges and future research directions."}}
{"id": "2508.07674", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07674", "abs": "https://arxiv.org/abs/2508.07674", "authors": ["Milan \u0160indelka", "David Gelbwaser-Klimovsky"], "title": "Steady state properties of periodically driven quantum systems", "comment": null, "summary": "Periodic driving is used to steer physical systems to unique stationary\nstates or nonequilibrium steady states (NESS), producing enhanced properties\ninaccessible to non-driven systems. For open quantum systems, characterizing\nthe NESS is challenging and existing results are generally limited to specific\ntypes of driving and the Born-Markov approximation. Here we go beyond these\nlimits by studying a generic periodically driven $ N$-level quantum system\ninteracting with a low-density thermal gas. Exploiting the framework of Floquet\nscattering theory, we establish general Floquet thermalization conditions\nconstraining the nature of the NESS and the transition rates. Moreover, we\nexamine theoretically the structure of the NESS at high temperatures, and find\nout that the NESS complies, rather surprisingly, with the Boltzmann law for any\ndriving. Numerical calculations illustrate our theoretical elaborations for a\nsimple toy model.", "AI": {"tldr": "\u672c\u7814\u7a76\u4f7f\u7528Floquet\u6563\u5c04\u7406\u8bba\u5206\u6790\u4e86\u5468\u671f\u6027\u9a71\u52a8\u7684\u5f00\u653e\u91cf\u5b50\u7cfb\u7edf\uff0c\u53d1\u73b0\u5176\u975e\u5e73\u8861\u5b9a\u6001\uff08NESS\uff09\u5728\u9ad8\u6e29\u4e0b\u9075\u5faa\u73bb\u5c14\u5179\u66fc\u5b9a\u5f8b\uff0c\u514b\u670d\u4e86\u4ee5\u5f80\u7814\u7a76\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u7814\u7a76\u4e2d\u5bf9\u5468\u671f\u6027\u9a71\u52a8\u5f00\u653e\u91cf\u5b50\u7cfb\u7edfNESS\u8868\u5f81\u7684\u5c40\u9650\u6027\uff08\u901a\u5e38\u4ec5\u9650\u4e8e\u7279\u5b9a\u7c7b\u578b\u7684\u9a71\u52a8\u548c\u73bb\u6069-\u9a6c\u5c14\u53ef\u592b\u8fd1\u4f3c\uff09\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u66f4\u901a\u7528\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528Floquet\u6563\u5c04\u7406\u8bba\u6846\u67b6\uff0c\u5efa\u7acb\u4e86\u7ea6\u675fNESS\u6027\u8d28\u548c\u8dc3\u8fc1\u7387\u7684Floquet\u70ed\u5316\u6761\u4ef6\uff0c\u5e76\u7814\u7a76\u4e86\u9ad8\u6e29\u4e0bNESS\u7684\u7ed3\u6784\u3002", "result": "1. \u5efa\u7acb\u4e86\u9002\u7528\u4e8e\u901a\u7528\u5468\u671f\u6027\u9a71\u52a8N\u80fd\u7ea7\u91cf\u5b50\u7cfb\u7edf\u7684Floquet\u70ed\u5316\u6761\u4ef6\u30022. \u53d1\u73b0\u9ad8\u6e29\u4e0bNESS\u51fa\u4eba\u610f\u6599\u5730\u9075\u5faa\u73bb\u5c14\u5179\u66fc\u5b9a\u5f8b\u30023. \u901a\u8fc7\u4e00\u4e2a\u7b80\u5355\u7684\u73a9\u5177\u6a21\u578b\u8fdb\u884c\u4e86\u6570\u503c\u8ba1\u7b97\uff0c\u8bc1\u660e\u4e86\u7406\u8bba\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0c\u5728\u9ad8\u6e29\u4e0b\uff0c\u975e\u5e73\u8861\u5b9a\u6001\uff08NESS\uff09\u51fa\u4eba\u610f\u6599\u5730\u9075\u5faa\u73bb\u5c14\u5179\u66fc\u5b9a\u5f8b\uff0c\u4e14\u9002\u7528\u4e8e\u4efb\u4f55\u9a71\u52a8\u3002"}}
{"id": "2508.06805", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06805", "abs": "https://arxiv.org/abs/2508.06805", "authors": ["Aarav Mehta", "Priya Deshmukh", "Vikram Singh", "Siddharth Malhotra", "Krishnan Menon Iyer", "Tanvi Iyer"], "title": "Edge Detection for Organ Boundaries via Top Down Refinement and SubPixel Upsampling", "comment": "MICCAIA Workshop", "summary": "Accurate localization of organ boundaries is critical in medical imaging for\nsegmentation, registration, surgical planning, and radiotherapy. While deep\nconvolutional networks (ConvNets) have advanced general-purpose edge detection\nto near-human performance on natural images, their outputs often lack precise\nlocalization, a limitation that is particularly harmful in medical applications\nwhere millimeter-level accuracy is required. Building on a systematic analysis\nof ConvNet edge outputs, we propose a medically focused crisp edge detector\nthat adapts a novel top-down backward refinement architecture to medical images\n(2D and volumetric). Our method progressively upsamples and fuses high-level\nsemantic features with fine-grained low-level cues through a backward\nrefinement pathway, producing high-resolution, well-localized organ boundaries.\nWe further extend the design to handle anisotropic volumes by combining 2D\nslice-wise refinement with light 3D context aggregation to retain computational\nefficiency. Evaluations on several CT and MRI organ datasets demonstrate\nsubstantially improved boundary localization under strict criteria (boundary\nF-measure, Hausdorff distance) compared to baseline ConvNet detectors and\ncontemporary medical edge/contour methods. Importantly, integrating our crisp\nedge maps into downstream pipelines yields consistent gains in organ\nsegmentation (higher Dice scores, lower boundary errors), more accurate image\nregistration, and improved delineation of lesions near organ interfaces. The\nproposed approach produces clinically valuable, crisp organ edges that\nmaterially enhance common medical-imaging tasks.", "AI": {"tldr": "\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\u5728\u533b\u5b66\u56fe\u50cf\u4e2d\u5b9a\u4f4d\u4e0d\u7cbe\u786e\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ece\u9876\u5230\u5e95\u7684\u540e\u5411\u7ec6\u5316\u67b6\u6784\uff0c\u901a\u8fc7\u878d\u5408\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u548c\u4f4e\u7ea7\u7ebf\u7d22\u6765\u751f\u6210\u9ad8\u5206\u8fa8\u7387\u3001\u5b9a\u4f4d\u826f\u597d\u7684\u5668\u5b98\u8fb9\u754c\u3002\u8be5\u65b9\u6cd5\u5728CT\u548cMRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5728\u8fb9\u754c\u5b9a\u4f4d\u3001\u5668\u5b98\u5206\u5272\u548c\u56fe\u50cf\u914d\u51c6\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\u3002", "motivation": "\u6df1\u5ea6\u5377\u79ef\u7f51\u7edc\uff08ConvNets\uff09\u867d\u7136\u5728\u81ea\u7136\u56fe\u50cf\u7684\u901a\u7528\u8fb9\u7f18\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u63a5\u8fd1\u4eba\u7c7b\u7684\u6027\u80fd\uff0c\u4f46\u5176\u8f93\u51fa\u5f80\u5f80\u7f3a\u4e4f\u7cbe\u786e\u7684\u5b9a\u4f4d\uff0c\u800c\u8fd9\u4e00\u7f3a\u9677\u5728\u9700\u8981\u6beb\u7c73\u7ea7\u7cbe\u5ea6\u7684\u533b\u5b66\u5e94\u7528\u4e2d\u5c24\u5176\u6709\u5bb3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9762\u5411\u533b\u5b66\u7684\u6e05\u6670\u8fb9\u754c\u68c0\u6d4b\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u91c7\u7528\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4ece\u9876\u5230\u5e95\u7684\u540e\u5411\u7ec6\u5316\u67b6\u6784\u6765\u9002\u5e94\u533b\u5b66\u56fe\u50cf\uff082D\u548c\u4f53\u79ef\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u540e\u5411\u7ec6\u5316\u901a\u8def\uff0c\u9010\u6b65\u4e0a\u91c7\u6837\u5e76\u878d\u5408\u4e86\u9ad8\u7ea7\u8bed\u4e49\u7279\u5f81\u548c\u7ec6\u7c92\u5ea6\u7684\u4f4e\u7ea7\u7ebf\u7d22\uff0c\u751f\u6210\u4e86\u9ad8\u5206\u8fa8\u7387\u3001\u5b9a\u4f4d\u826f\u597d\u7684\u5668\u5b98\u8fb9\u754c\u3002\u4e3a\u5904\u7406\u5404\u5411\u5f02\u6027\u4f53\u79ef\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e862D\u5207\u7247\u5f0f\u7ec6\u5316\u548c\u8f7b\u91cf\u7ea73D\u4e0a\u4e0b\u6587\u805a\u5408\uff0c\u4ee5\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u3002", "result": "\u8be5\u65b9\u6cd5\u751f\u6210\u4e86\u4e34\u5e8a\u4e0a\u6709\u4ef7\u503c\u7684\u3001\u6e05\u6670\u7684\u5668\u5b98\u8fb9\u754c\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5e38\u89c1\u7684\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728CT\u548cMRI\u5668\u5b98\u6570\u636e\u96c6\u7684\u8bc4\u4f30\u4e2d\uff0c\u5728\u4e25\u683c\u7684\u6807\u51c6\u4e0b\uff08\u8fb9\u754cF\u5ea6\u91cf\u3001\u8c6a\u65af\u591a\u592b\u8ddd\u79bb\uff09\u76f8\u6bd4\u57fa\u7ebf\u5377\u79ef\u7f51\u7edc\u63a2\u6d4b\u5668\u548c\u5f53\u4ee3\u533b\u5b66\u8fb9\u754c/\u8f6e\u5ed3\u65b9\u6cd5\uff0c\u8fb9\u754c\u5b9a\u4f4d\u5f97\u5230\u4e86\u663e\u8457\u6539\u5584\u3002\u5c06\u672c\u65b9\u6cd5\u751f\u6210\u7684\u6e05\u6670\u8fb9\u754c\u56fe\u6574\u5408\u5230\u4e0b\u6e38\u6d41\u7a0b\u4e2d\uff0c\u5728\u5668\u5b98\u5206\u5272\uff08\u66f4\u9ad8\u7684Dice\u5206\u6570\u3001\u66f4\u4f4e\u7684\u8fb9\u754c\u8bef\u5dee\uff09\u3001\u56fe\u50cf\u914d\u51c6\u548c\u9760\u8fd1\u5668\u5b98\u754c\u9762\u7684\u75c5\u53d8\u63cf\u7ed8\u65b9\u9762\uff0c\u5747\u53d6\u5f97\u4e86\u6301\u7eed\u7684\u63d0\u5347\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u751f\u6210\u4e86\u4e34\u5e8a\u4e0a\u6709\u4ef7\u503c\u7684\u3001\u6e05\u6670\u7684\u5668\u5b98\u8fb9\u754c\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u5e38\u89c1\u7684\u533b\u5b66\u6210\u50cf\u4efb\u52a1\u3002"}}
{"id": "2508.07885", "categories": ["cs.RO", "cs.AI", "cs.CV", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.07885", "abs": "https://arxiv.org/abs/2508.07885", "authors": ["Shoaib Ahmmad", "Zubayer Ahmed Aditto", "Md Mehrab Hossain", "Noushin Yeasmin", "Shorower Hossain"], "title": "Autonomous Navigation of Cloud-Controlled Quadcopters in Confined Spaces Using Multi-Modal Perception and LLM-Driven High Semantic Reasoning", "comment": null, "summary": "This paper introduces an advanced AI-driven perception system for autonomous\nquadcopter navigation in GPS-denied indoor environments. The proposed framework\nleverages cloud computing to offload computationally intensive tasks and\nincorporates a custom-designed printed circuit board (PCB) for efficient sensor\ndata acquisition, enabling robust navigation in confined spaces. The system\nintegrates YOLOv11 for object detection, Depth Anything V2 for monocular depth\nestimation, a PCB equipped with Time-of-Flight (ToF) sensors and an Inertial\nMeasurement Unit (IMU), and a cloud-based Large Language Model (LLM) for\ncontext-aware decision-making. A virtual safety envelope, enforced by\ncalibrated sensor offsets, ensures collision avoidance, while a multithreaded\narchitecture achieves low-latency processing. Enhanced spatial awareness is\nfacilitated by 3D bounding box estimation with Kalman filtering. Experimental\nresults in an indoor testbed demonstrate strong performance, with object\ndetection achieving a mean Average Precision (mAP50) of 0.6, depth estimation\nMean Absolute Error (MAE) of 7.2 cm, only 16 safety envelope breaches across 42\ntrials over approximately 11 minutes, and end-to-end system latency below 1\nsecond. This cloud-supported, high-intelligence framework serves as an\nauxiliary perception and navigation system, complementing state-of-the-art\ndrone autonomy for GPS-denied confined spaces.", "AI": {"tldr": "\u4e00\u9879\u65b0\u7684\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u65e0\u4eba\u673a\u5bfc\u822a\u7cfb\u7edf\uff0c\u5728\u65e0GPS\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5176\u7279\u70b9\u662f\u5229\u7528\u4e91\u8ba1\u7b97\u3001\u5b9a\u5236PCB\u3001YOLOv11\u3001Depth Anything V2\u548cLLM\uff0c\u5b9e\u73b0\u4e86\u4f4e\u5ef6\u8fdf\u548c\u9ad8\u7cbe\u5ea6\u3002", "motivation": "\u5728\u7981\u6b62GPS\u7684\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u4e3a\u81ea\u4e3b\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5bfc\u822a\u63d0\u4f9b\u4e00\u79cd\u5148\u8fdb\u7684\u3001\u7531\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u611f\u77e5\u7cfb\u7edf\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5148\u8fdb\u7684\u3001\u7531\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u611f\u77e5\u7cfb\u7edf\uff0c\u7528\u4e8e\u5728\u7981\u6b62GPS\u7684\u5ba4\u5185\u73af\u5883\u4e2d\u8fdb\u884c\u81ea\u4e3b\u56db\u65cb\u7ffc\u98de\u884c\u5668\u5bfc\u822a\u3002\u8be5\u6846\u67b6\u5229\u7528\u4e91\u8ba1\u7b97\u6765\u5378\u8f7d\u8ba1\u7b97\u5bc6\u96c6\u578b\u4efb\u52a1\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u5b9a\u5236\u8bbe\u8ba1\u7684\u5370\u5237\u7535\u8def\u677f\uff08PCB\uff09\uff0c\u7528\u4e8e\u9ad8\u6548\u7684\u4f20\u611f\u5668\u6570\u636e\u91c7\u96c6\uff0c\u4ece\u800c\u5728\u6709\u9650\u7684\u7a7a\u95f4\u5185\u5b9e\u73b0\u7a33\u5065\u7684\u5bfc\u822a\u3002\u8be5\u7cfb\u7edf\u96c6\u6210\u4e86YOLOv11\u8fdb\u884c\u5bf9\u8c61\u68c0\u6d4b\u3001Depth Anything V2\u8fdb\u884c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3001\u4e00\u4e2a\u914d\u5907\u98de\u884c\u65f6\u95f4\uff08ToF\uff09\u4f20\u611f\u5668\u548c\u60ef\u6027\u6d4b\u91cf\u5355\u5143\uff08IMU\uff09\u7684PCB\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8e\u4e91\u7684\u3001\u7528\u4e8e\u4e0a\u4e0b\u6587\u611f\u77e5\u51b3\u7b56\u5236\u5b9a\u7684\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002\u901a\u8fc7\u7ecf\u8fc7\u6821\u51c6\u7684\u4f20\u611f\u5668\u504f\u79fb\u91cf\u5f3a\u5236\u6267\u884c\u7684\u865a\u62df\u5b89\u5168\u5305\u7edc\u53ef\u786e\u4fdd\u907f\u514d\u78b0\u649e\uff0c\u800c\u591a\u7ebf\u7a0b\u4f53\u7cfb\u7ed3\u6784\u53ef\u5b9e\u73b0\u4f4e\u5ef6\u8fdf\u5904\u7406\u3002\u901a\u8fc7\u5e26\u6709\u5361\u5c14\u66fc\u6ee4\u6ce2\u7684\u4e09\u7ef4\u8fb9\u754c\u6846\u4f30\u8ba1\u53ef\u589e\u5f3a\u7a7a\u95f4\u611f\u77e5\u80fd\u529b\u3002", "result": "\u5728\u5ba4\u5185\u6d4b\u8bd5\u4e2d\uff0c\u5bf9\u8c61\u68c0\u6d4b\u8fbe\u5230\u4e860.6\u7684\u5e73\u5747\u7cbe\u5ea6\uff08mAP50\uff09\uff0c\u6df1\u5ea6\u4f30\u8ba1\u7684\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u4e3a7.2\u5398\u7c73\u3002\u572842\u6b21\u8bd5\u9a8c\uff08\u7ea611\u5206\u949f\uff09\u4e2d\uff0c\u4ec5\u53d1\u751f\u4e8616\u6b21\u5b89\u5168\u5305\u7edc\u8fdd\u89c4\uff0c\u5e76\u4e14\u7aef\u5230\u7aef\u7684\u7cfb\u7edf\u5ef6\u8fdf\u4f4e\u4e8e1\u79d2\u3002", "conclusion": "\u8be5\u6846\u67b6\u4f5c\u4e3a\u4e00\u79cd\u8f85\u52a9\u611f\u77e5\u548c\u5bfc\u822a\u7cfb\u7edf\uff0c\u5bf9\u65e0GPS\u7684\u5c01\u95ed\u7a7a\u95f4\u4e2d\u7684\u5148\u8fdb\u65e0\u4eba\u673a\u81ea\u4e3b\u6027\u8fdb\u884c\u4e86\u8865\u5145\u3002"}}
{"id": "2508.07388", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07388", "abs": "https://arxiv.org/abs/2508.07388", "authors": ["Zhaoyu Chen", "Hongnan Lin", "Yongwei Nie", "Fei Ma", "Xuemiao Xu", "Fei Yu", "Chengjiang Long"], "title": "Invert4TVG: A Temporal Video Grounding Framework with Inversion Tasks for Enhanced Action Understanding", "comment": null, "summary": "Temporal Video Grounding (TVG) seeks to localize video segments matching a\ngiven textual query. Current methods, while optimizing for high temporal\nIntersection-over-Union (IoU), often overfit to this metric, compromising\nsemantic action understanding in the video and query, a critical factor for\nrobust TVG. To address this, we introduce Inversion Tasks for TVG (Invert4TVG),\na novel framework that enhances both localization accuracy and action\nunderstanding without additional data. Our approach leverages three inversion\ntasks derived from existing TVG annotations: (1) Verb Completion, predicting\nmasked action verbs in queries from video segments; (2) Action Recognition,\nidentifying query-described actions; and (3) Video Description, generating\ndescriptions of video segments that explicitly embed query-relevant actions.\nThese tasks, integrated with TVG via a reinforcement learning framework with\nwell-designed reward functions, ensure balanced optimization of localization\nand semantics. Experiments show our method outperforms state-of-the-art\napproaches, achieving a 7.1\\% improvement in R1@0.7 on Charades-STA for a 3B\nmodel compared to Time-R1. By inverting TVG to derive query-related actions\nfrom segments, our approach strengthens semantic understanding, significantly\nraising the ceiling of localization accuracy.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Invert4TVG \u7684\u65b0\u9896\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4e09\u4e2a\u53cd\u8f6c\u4efb\u52a1\uff08\u52a8\u8bcd\u8865\u5168\u3001\u52a8\u4f5c\u8bc6\u522b\u548c\u89c6\u9891\u63cf\u8ff0\uff09\u6765\u589e\u5f3a\u89c6\u9891\u5730\u9762\u5b9a\u4f4d\u7684\u8bed\u4e49\u7406\u89e3\u548c\u5b9a\u4f4d\u7cbe\u5ea6\uff0c\u800c\u65e0\u9700\u989d\u5916\u6570\u636e\u3002", "motivation": "\u5f53\u524d\u7684\u65b9\u6cd5\u867d\u7136\u4f18\u5316\u4e86\u9ad8\u65f6\u95f4 IoU\uff0c\u4f46\u5e38\u5e38\u8fc7\u5ea6\u62df\u5408\u8be5\u6307\u6807\uff0c\u635f\u5bb3\u4e86\u89c6\u9891\u548c\u67e5\u8be2\u4e2d\u8bed\u4e49\u52a8\u4f5c\u7684\u7406\u89e3\uff0c\u800c\u8fd9\u662f\u7a33\u5065 TVG \u7684\u4e00\u4e2a\u5173\u952e\u56e0\u7d20\u3002", "method": "Invert4TVG \u6846\u67b6\u5229\u7528\u4e86\u4ece\u73b0\u6709 TVG \u6ce8\u91ca\u6d3e\u751f\u51fa\u7684\u4e09\u4e2a\u53cd\u8f6c\u4efb\u52a1\uff1a(1) \u52a8\u8bcd\u8865\u5168\uff0c\u4ece\u89c6\u9891\u7247\u6bb5\u9884\u6d4b\u67e5\u8be2\u4e2d\u88ab\u906e\u76d6\u7684\u52a8\u4f5c\u52a8\u8bcd\uff1b(2) \u52a8\u4f5c\u8bc6\u522b\uff0c\u8bc6\u522b\u67e5\u8be2\u63cf\u8ff0\u7684\u52a8\u4f5c\uff1b(3) \u89c6\u9891\u63cf\u8ff0\uff0c\u751f\u6210\u89c6\u9891\u7247\u6bb5\u7684\u63cf\u8ff0\uff0c\u5e76\u660e\u786e\u5d4c\u5165\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u52a8\u4f5c\u3002\u8fd9\u4e9b\u4efb\u52a1\u901a\u8fc7\u5177\u6709\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u4e0e TVG \u96c6\u6210\uff0c\u786e\u4fdd\u4e86\u5b9a\u4f4d\u548c\u8bed\u4e49\u7684\u5e73\u8861\u4f18\u5316\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u7684\u65b9\u6cd5\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5bf9\u4e8e 3B \u6a21\u578b\u5728 Charades-STA \u4e0a\u76f8\u5bf9\u4e8e Time-R1 \u5b9e\u73b0\u4e86 7.1% \u7684 R1@0.7 \u63d0\u5347\u3002", "conclusion": "Invert4TVG \u901a\u8fc7\u53cd\u8f6c TVG \u4ee5\u4ece\u89c6\u9891\u7247\u6bb5\u4e2d\u63d0\u53d6\u4e0e\u67e5\u8be2\u76f8\u5173\u7684\u52a8\u4f5c\uff0c\u4ece\u800c\u52a0\u5f3a\u4e86\u8bed\u4e49\u7406\u89e3\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u4f4d\u7684\u4e0a\u9650\u3002"}}
{"id": "2508.06953", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06953", "abs": "https://arxiv.org/abs/2508.06953", "authors": ["Shiwei Li", "Xiandi Luo", "Haozhao Wang", "Xing Tang", "Ziqiang Cui", "Dugang Liu", "Yuhua Li", "Xiuqiang He", "Ruixuan Li"], "title": "BoRA: Towards More Expressive Low-Rank Adaptation with Block Diversity", "comment": null, "summary": "Low-rank adaptation (LoRA) is a parameter-efficient fine-tuning (PEFT) method\nwidely used in large language models (LLMs). It approximates the update of a\npretrained weight matrix $W\\in\\mathbb{R}^{m\\times n}$ by the product of two\nlow-rank matrices, $BA$, where $A \\in\\mathbb{R}^{r\\times n}$ and\n$B\\in\\mathbb{R}^{m\\times r} (r\\ll\\min\\{m,n\\})$. Increasing the dimension $r$\ncan raise the rank of LoRA weights (i.e., $BA$), which typically improves\nfine-tuning performance but also significantly increases the number of\ntrainable parameters. In this paper, we propose Block Diversified Low-Rank\nAdaptation (BoRA), which improves the rank of LoRA weights with a small number\nof additional parameters. Specifically, BoRA treats the product $BA$ as a block\nmatrix multiplication, where $A$ and $B$ are partitioned into $b$ blocks along\nthe columns and rows, respectively (i.e., $A=[A_1,\\dots,A_b]$ and\n$B=[B_1,\\dots,B_b]^\\top$). Consequently, the product $BA$ becomes the\nconcatenation of the block products $B_iA_j$ for $i,j\\in[b]$. To enhance the\ndiversity of different block products, BoRA introduces a unique diagonal matrix\n$\\Sigma_{i,j} \\in \\mathbb{R}^{r\\times r}$ for each block multiplication,\nresulting in $B_i \\Sigma_{i,j} A_j$. By leveraging these block-wise diagonal\nmatrices, BoRA increases the rank of LoRA weights by a factor of $b$ while only\nrequiring $b^2r$ additional parameters. Extensive experiments across multiple\ndatasets and models demonstrate the superiority of BoRA, and ablation studies\nfurther validate its scalability.", "AI": {"tldr": "BoRA\u662f\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684\u5fae\u8c03\u65b9\u6cd5\uff0c\u901a\u8fc7\u5757\u5bf9\u89d2\u77e9\u9635\u63d0\u5347LoRA\u6743\u91cd\u79e9\uff0c\u6027\u80fd\u4f18\u4e8eLoRA\u4e14\u53c2\u6570\u91cf\u589e\u52a0\u8f83\u5c11\u3002", "motivation": "LoRA\u901a\u8fc7\u8fd1\u4f3c\u4f4e\u79e9\u77e9\u9635\u6765\u5fae\u8c03\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u4f46\u589e\u52a0\u79e9\u4ee5\u63d0\u5347\u6027\u80fd\u4f1a\u663e\u8457\u589e\u52a0\u53c2\u6570\u6570\u91cf\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5728\u53c2\u6570\u91cf\u589e\u52a0\u4e0d\u591a\u7684\u60c5\u51b5\u4e0b\uff0c\u63d0\u5347LoRA\u6743\u91cd\u7684\u79e9\uff0c\u4ece\u800c\u4f18\u5316\u5fae\u8c03\u6027\u80fd\u3002", "method": "BoRA\u5c06LoRA\u7684\u4f4e\u79e9\u77e9\u9635\u4e58\u79efBA\u89c6\u4e3a\u5757\u77e9\u9635\u4e58\u6cd5\uff0c\u5e76\u5c06A\u548cB\u5206\u522b\u5212\u5206\u4e3ab\u4e2a\u5757\u3002\u901a\u8fc7\u4e3a\u6bcf\u4e2a\u5757\u4e58\u79ef\u5f15\u5165\u5bf9\u89d2\u77e9\u9635\u03a3\u1d62\u2c7c\uff0c\u5f97\u5230B\u1d62\u03a3\u1d62\u2c7cA\u2c7c\uff0c\u4ece\u800c\u5728\u4e0d\u663e\u8457\u589e\u52a0\u53c2\u6570\u6570\u91cf\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06LoRA\u6743\u91cd\u7684\u79e9\u63d0\u9ad8b\u500d\u3002", "result": "BoRA\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u6d88\u878d\u7814\u7a76\u9a8c\u8bc1\u4e86\u5176\u826f\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002", "conclusion": "BoRA\u901a\u8fc7\u5f15\u5165\u5757\u5bf9\u89d2\u77e9\u9635\u6765\u63d0\u9ad8LoRA\u6743\u91cd\u7684\u79e9\uff0c\u4ec5\u9700\u5c11\u91cf\u989d\u5916\u53c2\u6570\u5373\u53ef\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\uff0c\u5e76\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u4f18\u8d8a\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2508.07911", "categories": ["cond-mat.mtrl-sci"], "pdf": "https://arxiv.org/pdf/2508.07911", "abs": "https://arxiv.org/abs/2508.07911", "authors": ["A. Kr\u00f3licka", "K. Gas", "W. Dobrowolski", "H. Przybyli\u0144ska", "Y. K. Edathumkandy", "J. Korczak", "E. \u0141usakowska", "R. Minikayev", "A. Reszka", "R. Jakie\u0142a", "L. Kowalczyk", "A. Mirowska", "M. Gryglas-Borysiewicz", "J. Kossut", "M. Sawicki", "A. \u0141usakowski", "P. Bogus\u0142awski", "T. Story", "K. Dybko"], "title": "Cr resonant impurity for studies of band inversion and band offsets in IV-VI semiconductors", "comment": "64 pages, 29 figures", "summary": "Understanding the electronic structure of transition-metal dopants in IV-VI\nsemiconductors is critical for tuning their band structure. We analyze\nproperties of Cr dopant in $Pb_{1-x}Sn_xTe$ and PbSe by magnetic and transport\nmeasurements, which are interpreted based on density functional calculations.\nWe demonstrate that the pinning of the Fermi energy to the chromium resonant\nlevel occurs for both n-type and p-type $Pb_{1-x}Sn_xTe$ in the whole\ncomposition range. This enables us to determine the valence band and conduction\nband offsets at the PbTe/SnTe/PbSe heterointerfaces, which is important for\ndesigning high-prformance 2D transistors. Furthermore, the magnetic\nmeasurements reveal the presence of Cr ions in three charge states, $Cr^{3+}$,\n$Cr^{2+}$, and $Cr^{1+}$. The last one corresponds to the Cr dopants\nincorporated at the interstitial, and not the substitutional, sites. The\nmeasured concentrations of the interstitial and substitutional Cr are\ncomparable.", "AI": {"tldr": "\u7814\u7a76\u4e86Cr\u63ba\u6742\u5728IV-VI\u534a\u5bfc\u4f53\u4e2d\u7684\u7535\u5b50\u7ed3\u6784\u548c\u78c1\u6027\uff0c\u53d1\u73b0\u8d39\u7c73\u80fd\u7ea7\u9489\u624e\u6548\u5e94\u548c\u591a\u79cdCr\u79bb\u5b50\u7535\u8377\u72b6\u6001\uff0c\u4e3a\u8bbe\u8ba1\u65b0\u578b\u6676\u4f53\u7ba1\u63d0\u4f9b\u4e86\u57fa\u7840\u3002", "motivation": "\u7406\u89e3IV-VI\u534a\u5bfc\u4f53\u4e2d\u8fc7\u6e21\u91d1\u5c5e\u63ba\u6742\u5242\u7684\u7535\u5b50\u7ed3\u6784\u5bf9\u4e8e\u8c03\u8c10\u5176\u5e26\u7ed3\u6784\u81f3\u5173\u91cd\u8981\u3002", "method": "\u901a\u8fc7\u78c1\u5b66\u548c\u8f93\u8fd0\u6d4b\u91cf\u5206\u6790\u4e86$Pb_{1-x}Sn_xTe$\u548cPbSe\u4e2dCr\u63ba\u6742\u5242\u7684\u6027\u8d28\uff0c\u5e76\u57fa\u4e8e\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u8fdb\u884c\u4e86\u89e3\u91ca\u3002", "result": "Cr\u63ba\u6742\u7684IV-VI\u534a\u5bfc\u4f53\u5728\u6574\u4e2a\u6210\u5206\u8303\u56f4\u5185\uff0c\u8d39\u7c73\u80fd\u7ea7\u90fd\u88ab\u9489\u624e\u5728Cr\u5171\u632f\u80fd\u7ea7\uff0c\u8fd9\u4f7f\u5f97\u80fd\u591f\u786e\u5b9aPbTe/SnTe/PbSe\u5f02\u8d28\u7ed3\u7684\u4ef7\u5e26\u548c\u5bfc\u5e26\u504f\u79fb\u3002\u78c1\u6027\u6d4b\u91cf\u63ed\u793a\u4e86Cr\u79bb\u5b50\u5b58\u5728\u4e09\u79cd\u7535\u8377\u72b6\u6001\uff1a$Cr^{3+}$\u3001$Cr^{2+}$\u548c$Cr^{1+}$\u3002\u5176\u4e2d$Cr^{1+}$\u5bf9\u5e94\u4e8e\u63ba\u6742\u5728\u95f4\u9699\u4f4d\u800c\u975e\u53d6\u4ee3\u4f4d\u4e0a\u7684Cr\uff0c\u95f4\u9699Cr\u548c\u53d6\u4ee3Cr\u7684\u6d53\u5ea6\u76f8\u5f53\u3002", "conclusion": "Cr\u63ba\u6742\u7684IV-VI\u534a\u5bfc\u4f53\u5728\u6574\u4e2a\u6210\u5206\u8303\u56f4\u5185\uff0c\u8d39\u7c73\u80fd\u7ea7\u90fd\u88ab\u9489\u624e\u5728Cr\u5171\u632f\u80fd\u7ea7\uff0c\u8fd9\u4f7f\u5f97\u80fd\u591f\u786e\u5b9aPbTe/SnTe/PbSe\u5f02\u8d28\u7ed3\u7684\u4ef7\u5e26\u548c\u5bfc\u5e26\u504f\u79fb\uff0c\u8fd9\u5bf9\u4e8e\u8bbe\u8ba1\u9ad8\u6027\u80fd\u4e8c\u7ef4\u6676\u4f53\u7ba1\u81f3\u5173\u91cd\u8981\u3002\u78c1\u6027\u6d4b\u91cf\u63ed\u793a\u4e86Cr\u79bb\u5b50\u5b58\u5728\u4e09\u79cd\u7535\u8377\u72b6\u6001\uff1a$Cr^{3+}$\u3001$Cr^{2+}$\u548c$Cr^{1+}$\u3002\u5176\u4e2d$Cr^{1+}$\u5bf9\u5e94\u4e8e\u63ba\u6742\u5728\u95f4\u9699\u4f4d\u800c\u975e\u53d6\u4ee3\u4f4d\u4e0a\u7684Cr\uff0c\u95f4\u9699Cr\u548c\u53d6\u4ee3Cr\u7684\u6d53\u5ea6\u76f8\u5f53\u3002"}}
{"id": "2508.07566", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07566", "abs": "https://arxiv.org/abs/2508.07566", "authors": ["Conor K. Trygstad", "Cody R. Longwell", "Francisco M. F. R. Gon\u00e7alves", "Elijah K. Blankenship", "N\u00e9stor O. P\u00e9rez-Arancibia"], "title": "Feedback Control of a Single-Tail Bioinspired 59-mg Swimmer", "comment": "To be presented at the 2025 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS 2025)", "summary": "We present an evolved steerable version of the single-tail\nFish-&-Ribbon-Inspired Small Swimming Harmonic roBot (FRISSHBot), a 59-mg\nbiologically inspired swimmer, which is driven by a new shape-memory alloy\n(SMA)-based bimorph actuator. The new FRISSHBot is controllable in the\ntwo-dimensional (2D) space, which enabled the first demonstration of\nfeedback-controlled trajectory tracking of a single-tail aquatic robot with\nonboard actuation at the subgram scale. These new capabilities are the result\nof a physics-informed design with an enlarged head and shortened tail relative\nto those of the original platform. Enhanced by its design, this new platform\nachieves forward swimming speeds of up to 13.6 mm/s (0.38 Bl/s), which is over\nfour times that of the original platform. Furthermore, when following 2D\nreferences in closed loop, the tested FRISSHBot prototype attains forward\nswimming speeds of up to 9.1 mm/s, root-mean-square (RMS) tracking errors as\nlow as 2.6 mm, turning rates of up to 13.1 {\\deg}/s, and turning radii as small\nas 10 mm.", "AI": {"tldr": "\u672c\u7814\u7a76\u4ecb\u7ecd\u4e86\u4e00\u79cd\u6539\u8fdb\u7684 FRISSHBot\uff0c\u5b83\u53ef\u4ee5\u5728\u4e8c\u7ef4\u7a7a\u95f4\u4e2d\u8fdb\u884c\u63a7\u5236\u548c\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e9a\u514b\u7ea7\u5c3a\u5ea6\u4e0a\u7684\u677f\u8f7d\u9a71\u52a8\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u4e9a\u514b\u7ea7\u6c34\u4e0b\u673a\u5668\u4eba\u7684\u4e8c\u7ef4\u8fd0\u52a8\u63a7\u5236\u548c\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u6211\u4eec\u6539\u8fdb\u4e86 FRISSHBot\u3002", "method": "\u901a\u8fc7\u6539\u8fdb\u7684\u7269\u7406\u4fe1\u606f\u8bbe\u8ba1\uff0c\u5305\u62ec\u589e\u5927\u7684\u5934\u90e8\u548c\u7f29\u77ed\u7684\u5c3e\u90e8\uff0c\u5e76\u91c7\u7528\u57fa\u4e8e\u5f62\u72b6\u8bb0\u5fc6\u5408\u91d1\u7684\u65b0\u578b\u5f2f\u66f2\u6267\u884c\u5668\uff0c\u5b9e\u73b0\u4e86 FRISSHBot \u7684\u53ef\u63a7\u6027\u3002", "result": "\u65b0 FRISSHBot \u7684\u6700\u5927\u524d\u8fdb\u901f\u5ea6\u8fbe\u5230 13.6 \u6beb\u7c73/\u79d2\uff08\u6bd4\u539f\u5e73\u53f0\u5feb\u56db\u500d\u4ee5\u4e0a\uff09\uff0c\u5728\u8fdb\u884c\u95ed\u73af\u4e8c\u7ef4\u8f68\u8ff9\u8ddf\u8e2a\u65f6\uff0c\u6700\u5927\u524d\u8fdb\u901f\u5ea6\u4e3a 9.1 \u6beb\u7c73/\u79d2\uff0c\u5747\u65b9\u6839\u8ddf\u8e2a\u8bef\u5dee\u4f4e\u81f3 2.6 \u6beb\u7c73\uff0c\u6700\u5927\u8f6c\u5411\u901f\u7387\u4e3a 13.1 \u5ea6/\u79d2\uff0c\u6700\u5c0f\u8f6c\u5411\u534a\u5f84\u4e3a 10 \u6beb\u7c73\u3002", "conclusion": "FRISSHBot \u5e73\u53f0\u6210\u529f\u5b9e\u73b0\u4e86\u4e8c\u7ef4\u7a7a\u95f4\u4e2d\u7684\u8fd0\u52a8\u63a7\u5236\u548c\u8f68\u8ff9\u8ddf\u8e2a\uff0c\u8be5\u673a\u5668\u4eba\u5177\u6709\u5728\u4e9a\u514b\u7ea7\u5c3a\u5ea6\u4e0a\u901a\u8fc7\u677f\u8f7d\u9a71\u52a8\u5b9e\u73b0\u53cd\u9988\u63a7\u5236\u7684\u80fd\u529b\u3002"}}
{"id": "2508.07707", "categories": ["quant-ph", "cond-mat.dis-nn"], "pdf": "https://arxiv.org/pdf/2508.07707", "abs": "https://arxiv.org/abs/2508.07707", "authors": ["Yueshan Xu", "Cai-Ping Fang", "Bing-Jie Chen", "Ming-Chuan Wang", "Zi-Yong Ge", "Yun-Hao Shi", "Yu Liu", "Cheng-Lin Deng", "Kui Zhao", "Zheng-He Liu", "Tian-Ming Li", "Hao Li", "Ziting Wang", "Gui-Han Liang", "Da'er Feng", "Xueyi Guo", "Xu-Yang Gu", "Yang He", "Hao-Tian Liu", "Zheng-Yang Mei", "Yongxi Xiao", "Yu Yan", "Yi-Han Yu", "Wei-Ping Yuan", "Jia-Chi Zhang", "Zheng-An Wang", "Gangqin Liu", "Xiaohui Song", "Ye Tian", "Yu-Ran Zhang", "Shi-Xin Zhang", "Kaixuan Huang", "Zhongcheng Xiang", "Dongning Zheng", "Kai Xu", "Heng Fan"], "title": "Observation and Modulation of the Quantum Mpemba Effect on a Superconducting Quantum Processor", "comment": null, "summary": "In non-equilibrium quantum many-body systems, the quantum Mpemba effect (QME)\nemerges as a counterintuitive phenomenon: systems exhibiting greater initial\nsymmetry breaking restore symmetry faster than those with less. While\ntheoretical exploration of QME has surged, experimental studies on its\nmultidimensional modulation remain limited. Here, we report the observation and\ncontrol of QME using a superconducting processor featuring a unique fully\nconnected, tunable-coupling architecture that enables precise modulation from\nshort- to long-range interactions. This platform allows independent\nmanipulation of coupling regimes, on-site potentials, and initial states,\nelucidating their roles in QME. To quantify symmetry restoration, we employ\nentanglement asymmetry (EA) -- the relative entropy between a subsystem reduced\ndensity matrix and its symmetric projection -- as a sensitive probe of symmetry\nbreaking. In strong short-range coupling regimes, EA crossovers during quenches\nfrom tilted N\\'{e}el states confirm the presence of QME. In contrast, in\nintermediate coupling regimes, synchronized EA and entanglement entropy\ndynamics reveal the suppression of QME. Remarkably, QME reemerges with the\nintroduction of on-site linear potentials or quenches from tilted ferromagnetic\nstates, the latter proving robust against on-site disorder. Our study provides\nthe first demonstration of flexible QME modulation on a superconducting\nplatform with multiple controllable parameters, shedding light on quantum\nmany-body non-equilibrium dynamics and opening avenues for quantum information\napplications.", "AI": {"tldr": "\u8d85\u5c0e\u5e73\u53f0\u5be6\u73fe\u4e86\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u61c9\uff08QME\uff09\u7684\u591a\u7dad\u8abf\u88fd\uff0c\u63ed\u793a\u4e86\u8026\u5408\u3001\u52e2\u548c\u521d\u59cb\u614b\u5c0dQME\u7684\u5f71\u97ff\uff0c\u4e26\u70ba\u91cf\u5b50\u4fe1\u606f\u61c9\u7528\u958b\u95e2\u4e86\u65b0\u9014\u5f91\u3002", "motivation": "\u5118\u7ba1\u7406\u8ad6\u4e0a\u5c0d\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u61c9\uff08QME\uff09\u7684\u7814\u7a76\u65e5\u76ca\u589e\u591a\uff0c\u4f46\u5176\u5be6\u9a57\u7814\u7a76\uff0c\u7279\u5225\u662f\u5176\u591a\u7dad\u8abf\u88fd\u65b9\u9762\u4ecd\u7136\u6709\u9650\u3002\u672c\u7814\u7a76\u65e8\u5728\u586b\u88dc\u9019\u4e00\u7a7a\u767d\uff0c\u901a\u904e\u5be6\u9a57\u63a2\u7d22QME\u7684\u8abf\u88fd\u6a5f\u5236\u3002", "method": "\u5229\u7528\u8d85\u5c0e\u8655\u7406\u5668\uff0c\u901a\u904e\u8abf\u6574\u8026\u5408\u5f37\u5ea6\u3001\u5c40\u57df\u52e2\u548c\u521d\u59cb\u72c0\u614b\uff0c\u5be6\u73fe\u5c0d\u91cf\u5b50\u591a\u9ad4\u7cfb\u7d71\u7684\u7cbe\u78ba\u63a7\u5236\uff0c\u4e26\u63a1\u7528\u7cfe\u7e8f\u4e0d\u5c0d\u7a31\u6027\uff08EA\uff09\u4f5c\u70ba\u63a2\u91dd\u4f86\u91cf\u5316\u5c0d\u7a31\u6027\u6062\u5fa9\u3002", "result": "\u5728\u5f37\u77ed\u7a0b\u8026\u5408\u4e0b\u89c0\u5bdf\u5230\u4e86QME\u7684\u5b58\u5728\uff0c\u800c\u5728\u4e2d\u7a0b\u8026\u5408\u4e0b\u5247\u6291\u5236\u4e86QME\u3002\u6b64\u5916\uff0c\u901a\u904e\u5f15\u5165\u5c40\u57df\u7dda\u6027\u52e2\u6216\u5f9e\u50be\u659c\u9435\u78c1\u614b\u9032\u884c\u731d\u6ec5\uff0c\u53ef\u4ee5\u91cd\u65b0\u89c0\u5bdf\u5230QME\uff0c\u5f8c\u8005\u5c0d\u5c40\u57df\u7121\u5e8f\u5177\u6709\u9b6f\u68d2\u6027\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u8d85\u5c0e\u5e73\u53f0\u5c55\u793a\u4e86\u91cf\u5b50\u59c6\u6f58\u5df4\u6548\u61c9\uff08QME\uff09\u7684\u9748\u6d3b\u8abf\u88fd\uff0c\u4e26\u901a\u904e\u63a7\u5236\u591a\u500b\u53c3\u6578\u4f86\u6df1\u5165\u7406\u89e3\u975e\u5e73\u8861\u91cf\u5b50\u591a\u9ad4\u52d5\u529b\u5b78\u3002"}}
{"id": "2508.07229", "categories": ["cs.CL", "cs.LG", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07229", "abs": "https://arxiv.org/abs/2508.07229", "authors": ["Itai Allouche", "Itay Asael", "Rotem Rousso", "Vered Dassa", "Ann Bradlow", "Seung-Eun Kim", "Matthew Goldrick", "Joseph Keshet"], "title": "How Does a Deep Neural Network Look at Lexical Stress?", "comment": "10 pages, 4 figures, submitted to the Journal of the Acoustical\n  Society of America (JASA)", "summary": "Despite their success in speech processing, neural networks often operate as\nblack boxes, prompting the question: what informs their decisions, and how can\nwe interpret them? This work examines this issue in the context of lexical\nstress. A dataset of English disyllabic words was automatically constructed\nfrom read and spontaneous speech. Several Convolutional Neural Network (CNN)\narchitectures were trained to predict stress position from a spectrographic\nrepresentation of disyllabic words lacking minimal stress pairs (e.g., initial\nstress WAllet, final stress exTEND), achieving up to 92% accuracy on held-out\ntest data. Layerwise Relevance Propagation (LRP), a technique for CNN\ninterpretability analysis, revealed that predictions for held-out minimal pairs\n(PROtest vs. proTEST ) were most strongly influenced by information in stressed\nversus unstressed syllables, particularly the spectral properties of stressed\nvowels. However, the classifiers also attended to information throughout the\nword. A feature-specific relevance analysis is proposed, and its results\nsuggest that our best-performing classifier is strongly influenced by the\nstressed vowel's first and second formants, with some evidence that its pitch\nand third formant also contribute. These results reveal deep learning's ability\nto acquire distributed cues to stress from naturally occurring data, extending\ntraditional phonetic work based around highly controlled stimuli.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u5229\u7528CNN\u548cLRP\u6280\u672f\u5206\u6790\u4e86\u8bcd\u91cd\u97f3\u7684\u8bc6\u522b\u3002CNN\u5728\u9884\u6d4b\u91cd\u97f3\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff08\u51c6\u786e\u7387\u8fbe92%\uff09\uff0c\u5176\u51b3\u7b56\u4e3b\u8981\u57fa\u4e8e\u91cd\u8bfb\u5143\u97f3\u7684\u5149\u8c31\u7279\u6027\uff0c\u7279\u522b\u662ff1\u548cf2\u3002\u8fd9\u8868\u660e\u6df1\u5ea6\u5b66\u4e60\u80fd\u4ece\u81ea\u7136\u8bed\u6599\u4e2d\u5b66\u4e60\u91cd\u97f3\u7ebf\u7d22\uff0c\u5e76\u4e3a\u8bed\u97f3\u5b66\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002", "motivation": "\u63a2\u8ba8\u795e\u7ecf\u7f51\u7edc\u5728\u8bed\u97f3\u5904\u7406\u4e2d\u7684\u51b3\u7b56\u4f9d\u636e\u4ee5\u53ca\u5982\u4f55\u8fdb\u884c\u89e3\u91ca\uff0c\u7279\u522b\u662f\u5728\u8bcd\u91cd\u97f3\u8bc6\u522b\u7684\u80cc\u666f\u4e0b\u3002", "method": "\u4f7f\u7528\u5c42\u7ea7\u76f8\u5173\u6027\u4f20\u64ad\uff08LRP\uff09\u6280\u672f\u5bf9\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u53ef\u89e3\u91ca\u6027\u5206\u6790\uff0c\u63ed\u793a\u4e86\u9884\u6d4b\u4e2d\u53d7\u5f71\u54cd\u6700\u5927\u7684\u4fe1\u606f\u3002\u63d0\u51fa\u4e86\u4e00\u79cd\u7279\u5b9a\u4e8e\u7279\u5f81\u7684\u76f8\u5173\u6027\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u6700\u4f73\u5206\u7c7b\u5668\u53d7\u5230\u7684\u5f71\u54cd\u56e0\u7d20\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0cCNN\u6a21\u578b\u5728\u9884\u6d4b\u8bcd\u91cd\u97f3\u4f4d\u7f6e\u65b9\u9762\u8fbe\u5230\u4e8692%\u7684\u51c6\u786e\u7387\u3002LRP\u5206\u6790\u663e\u793a\uff0c\u6a21\u578b\u9884\u6d4b\u4e3b\u8981\u53d7\u91cd\u8bfb\u97f3\u8282\uff08\u7279\u522b\u662f\u5143\u97f3\u7684\u5149\u8c31\u7279\u6027\uff09\u7684\u5f71\u54cd\uff0c\u4f46\u4e5f\u5173\u6ce8\u6574\u4e2a\u5355\u8bcd\u7684\u4fe1\u606f\u3002\u7279\u5f81\u7279\u5b9a\u5206\u6790\u8868\u660e\uff0c\u6700\u4f73\u6a21\u578b\u53d7\u5230\u91cd\u8bfb\u5143\u97f3\u7684\u7b2c\u4e00\u3001\u7b2c\u4e8c\u5171\u632f\u5cf0\uff08f1, f2\uff09\u7684\u5f3a\u70c8\u5f71\u54cd\uff0c\u97f3\u9ad8\u548c\u7b2c\u4e09\u5171\u632f\u5cf0\uff08f3\uff09\u4e5f\u6709\u4e00\u5b9a\u7684\u8d21\u732e\u3002", "conclusion": "\u6df1\u5ea6\u5b66\u4e60\u80fd\u591f\u4ece\u81ea\u7136\u53d1\u751f\u7684\u8bed\u6599\u4e2d\u5b66\u4e60\u5230\u5206\u5e03\u5f0f\u7ebf\u7d22\u6765\u8bc6\u522b\u91cd\u97f3\uff0c\u8fd9\u6269\u5c55\u4e86\u4f20\u7edf\u8bed\u97f3\u5b66\u4e2d\u57fa\u4e8e\u9ad8\u5ea6\u53d7\u63a7\u523a\u6fc0\u7684\u7814\u7a76\u3002"}}
{"id": "2508.06816", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06816", "abs": "https://arxiv.org/abs/2508.06816", "authors": ["Vikram Singh", "Kabir Malhotra", "Rohan Desai", "Ananya Shankaracharya", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "DualResolution Residual Architecture with Artifact Suppression for Melanocytic Lesion Segmentation", "comment": "MICCAIA", "summary": "Accurate segmentation of melanocytic tumors in dermoscopic images is a\ncritical step for automated skin cancer screening and clinical decision\nsupport. Unlike natural scene segmentation, lesion delineation must reconcile\nsubtle texture and color variations, frequent artifacts (hairs, rulers,\nbubbles), and a strong need for precise boundary localization to support\ndownstream diagnosis. In this paper we introduce Our method, a novel ResNet\ninspired dual resolution architecture specifically designed for melanocytic\ntumor segmentation. Our method maintains a full resolution stream that\npreserves fine grained boundary information while a complementary pooled stream\naggregates multi scale contextual cues for robust lesion recognition. The\nstreams are tightly coupled by boundary aware residual connections that inject\nhigh frequency edge information into deep feature maps, and by a channel\nattention module that adapts color and texture sensitivity to dermoscopic\nappearance. To further address common imaging artifacts and the limited size of\nclinical datasets, we propose a lightweight artifact suppression block and a\nmulti task training objective that combines a Dice Tversky segmentation loss\nwith an explicit boundary loss and a contrastive regularizer for feature\nstability. The combined design yields pixel accurate masks without requiring\nheavy post processing or complex pre training protocols. Extensive experiments\non public dermoscopic benchmarks demonstrate that Our method significantly\nimproves boundary adherence and clinically relevant segmentation metrics\ncompared to standard encoder decoder baselines, making it a practical building\nblock for automated melanoma assessment systems.", "AI": {"tldr": "\u8fd9\u7bc7\u8bba\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7528\u4e8e\u76ae\u80a4\u955c\u56fe\u50cf\u9ed1\u8272\u7d20\u7624\u5206\u5272\u7684\u65b0\u578b\u53cc\u5206\u8fa8\u7387\u67b6\u6784\uff0c\u901a\u8fc7\u7ed3\u5408\u5168\u5206\u8fa8\u7387\u6d41\u3001\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7ebf\u7d22\u3001\u8fb9\u754c\u611f\u77e5\u8fde\u63a5\u548c\u901a\u9053\u6ce8\u610f\u529b\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u4f2a\u5f71\u6291\u5236\u548c\u591a\u4efb\u52a1\u5b66\u4e60\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5206\u5272\u7cbe\u5ea6\u548c\u8fb9\u754c\u51c6\u786e\u6027\uff0c\u4e3a\u81ea\u52a8\u76ae\u80a4\u764c\u7b5b\u67e5\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "motivation": "\u4e3a\u4e86\u5b9e\u73b0\u81ea\u52a8\u76ae\u80a4\u764c\u7b5b\u67e5\u548c\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\uff0c\u9700\u8981\u5bf9\u76ae\u80a4\u955c\u56fe\u50cf\u4e2d\u7684\u9ed1\u8272\u7d20\u7624\u8fdb\u884c\u51c6\u786e\u5206\u5272\u3002\u4e0e\u81ea\u7136\u573a\u666f\u5206\u5272\u4e0d\u540c\uff0c\u75c5\u53d8\u63cf\u7ed8\u9700\u8981\u89e3\u51b3\u7ec6\u5fae\u7684\u7eb9\u7406\u548c\u989c\u8272\u53d8\u5316\u3001\u5e38\u89c1\u7684\u4f2a\u5f71\uff08\u5982\u6bdb\u53d1\u3001\u5c3a\u5b50\u3001\u6c14\u6ce1\uff09\u4ee5\u53ca\u7cbe\u786e\u8fb9\u754c\u5b9a\u4f4d\u7684\u5f3a\u70c8\u9700\u6c42\uff0c\u4ee5\u652f\u6301\u4e0b\u6e38\u8bca\u65ad\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53d7 ResNet \u542f\u53d1\u7684\u53cc\u5206\u8fa8\u7387\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u4e13\u4e3a\u9ed1\u8272\u7d20\u7624\u5206\u5272\u800c\u8bbe\u8ba1\u3002\u8be5\u65b9\u6cd5\u5305\u542b\u4e00\u4e2a\u5168\u5206\u8fa8\u7387\u6d41\uff0c\u7528\u4e8e\u4fdd\u7559\u7ec6\u7c92\u5ea6\u7684\u8fb9\u754c\u4fe1\u606f\uff0c\u4ee5\u53ca\u4e00\u4e2a\u4e92\u8865\u7684\u6c60\u5316\u6d41\uff0c\u7528\u4e8e\u805a\u5408\u591a\u5c3a\u5ea6\u4e0a\u4e0b\u6587\u7ebf\u7d22\u4ee5\u8fdb\u884c\u7a33\u5065\u7684\u75c5\u53d8\u8bc6\u522b\u3002\u8fd9\u4e24\u4e2a\u6d41\u901a\u8fc7\u8fb9\u754c\u611f\u77e5\u6b8b\u5dee\u8fde\u63a5\u548c\u901a\u9053\u6ce8\u610f\u529b\u6a21\u5757\u7d27\u5bc6\u8026\u5408\uff0c\u4ee5\u5c06\u9ad8\u9891\u8fb9\u7f18\u4fe1\u606f\u6ce8\u5165\u6df1\u5ea6\u7279\u5f81\u56fe\u5e76\u9002\u5e94\u76ae\u80a4\u955c\u5916\u89c2\u7684\u989c\u8272\u548c\u7eb9\u7406\u654f\u611f\u6027\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u4f2a\u5f71\u6291\u5236\u5757\u548c\u4e00\u79cd\u591a\u4efb\u52a1\u8bad\u7ec3\u76ee\u6807\uff0c\u8be5\u76ee\u6807\u7ed3\u5408\u4e86 Dice Tversky \u5206\u5272\u635f\u5931\u3001\u663e\u5f0f\u8fb9\u754c\u635f\u5931\u548c\u7528\u4e8e\u7279\u5f81\u7a33\u5b9a\u6027\u7684\u5bf9\u6bd4\u6b63\u5219\u5316\u5668\u3002", "result": "\u8be5\u65b9\u6cd5\u80fd\u591f\u751f\u6210\u50cf\u7d20\u7ea7\u51c6\u786e\u7684\u63a9\u7801\uff0c\u65e0\u9700\u8fdb\u884c\u7e41\u91cd\u7684\u540e\u5904\u7406\u6216\u590d\u6742\u7684\u9884\u8bad\u7ec3\u534f\u8bae\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u516c\u5171\u76ae\u80a4\u955c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u4e0e\u6807\u51c6\u7684\u7f16\u7801\u5668-\u89e3\u7801\u5668\u57fa\u7ebf\u76f8\u6bd4\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8fb9\u754c\u4f9d\u4ece\u6027\u548c\u4e34\u5e8a\u76f8\u5173\u5206\u5272\u6307\u6807\uff0c\u4f7f\u5176\u6210\u4e3a\u81ea\u52a8\u5316\u9ed1\u8272\u7d20\u7624\u8bc4\u4f30\u7cfb\u7edf\u7684\u5b9e\u7528\u6784\u5efa\u5757\u3002"}}
{"id": "2508.08137", "categories": ["cs.LG", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08137", "abs": "https://arxiv.org/abs/2508.08137", "authors": ["Pravallika Abbineni", "Saoud Aldowaish", "Colin Liechty", "Soroosh Noorzad", "Ali Ghazizadeh", "Morteza Fayazi"], "title": "MuaLLM: A Multimodal Large Language Model Agent for Circuit Design Assistance with Hybrid Contextual Retrieval-Augmented Generation", "comment": null, "summary": "Conducting a comprehensive literature review is crucial for advancing circuit\ndesign methodologies. However, the rapid influx of state-of-the-art research,\ninconsistent data representation, and the complexity of optimizing circuit\ndesign objectives make this task significantly challenging. In this paper, we\npropose MuaLLM, an open-source multimodal Large Language Model (LLM) agent for\ncircuit design assistance that integrates a hybrid Retrieval-Augmented\nGeneration (RAG) framework with an adaptive vector database of circuit design\nresearch papers. Unlike conventional LLMs, the MuaLLM agent employs a Reason +\nAct (ReAct) workflow for iterative reasoning, goal-setting, and multi-step\ninformation retrieval. It functions as a question-answering design assistant,\ncapable of interpreting complex queries and providing reasoned responses\ngrounded in circuit literature. Its multimodal capabilities enable processing\nof both textual and visual data, facilitating more efficient and comprehensive\nanalysis. The system dynamically adapts using intelligent search tools,\nautomated document retrieval from the internet, and real-time database updates.\nUnlike conventional approaches constrained by model context limits, MuaLLM\ndecouples retrieval from inference, enabling scalable reasoning over\narbitrarily large corpora. At the maximum context length supported by standard\nLLMs, MuaLLM remains up to 10x less costly and 1.6x faster while maintaining\nthe same accuracy. This allows rapid, no-human-in-the-loop database generation,\novercoming the bottleneck of simulation-based dataset creation for circuits. To\nevaluate MuaLLM, we introduce two custom datasets: RAG-250, targeting retrieval\nand citation performance, and Reasoning-100 (Reas-100), focused on multistep\nreasoning in circuit design. MuaLLM achieves 90.1% recall on RAG-250, and 86.8%\naccuracy on Reas-100.", "AI": {"tldr": "MuaLLM \u662f\u4e00\u4e2a\u7528\u4e8e\u7535\u8def\u8bbe\u8ba1\u7684 LLM \u4ee3\u7406\uff0c\u901a\u8fc7 RAG \u548c ReAct \u6846\u67b6\uff0c\u80fd\u9ad8\u6548\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\uff0c\u5e76\u5728\u68c0\u7d22\u548c\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u6210\u672c\u548c\u63d0\u9ad8\u4e86\u901f\u5ea6\u3002", "motivation": "\u5168\u9762\u7684\u6587\u732e\u7efc\u8ff0\u5bf9\u4e8e\u63a8\u8fdb\u7535\u8def\u8bbe\u8ba1\u65b9\u6cd5\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5f53\u524d\u7684\u7814\u7a76\u65b9\u6cd5\u9762\u4e34\u7740\u6765\u81ea\u5927\u91cf\u6700\u65b0\u7814\u7a76\u3001\u4e0d\u4e00\u81f4\u7684\u6570\u636e\u8868\u793a\u4ee5\u53ca\u4f18\u5316\u7535\u8def\u8bbe\u8ba1\u76ee\u6807\u7684\u590d\u6742\u6027\u5e26\u6765\u7684\u6311\u6218\u3002", "method": "MuaLLM \u96c6\u6210\u4e86\u4e00\u4e2a\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\uff0c\u5e76\u4f7f\u7528\u4e86\u4e00\u4e2a\u81ea\u9002\u5e94\u7684\u7535\u8def\u8bbe\u8ba1\u7814\u7a76\u8bba\u6587\u5411\u91cf\u6570\u636e\u5e93\u3002\u5b83\u91c7\u7528\u4e86 Reason + Act\uff08ReAct\uff09\u5de5\u4f5c\u6d41\uff0c\u652f\u6301\u8fed\u4ee3\u63a8\u7406\u3001\u76ee\u6807\u8bbe\u5b9a\u548c\u591a\u6b65\u4fe1\u606f\u68c0\u7d22\u3002\u8be5\u6a21\u578b\u80fd\u591f\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u667a\u80fd\u641c\u7d22\u5de5\u5177\u3001\u81ea\u52a8\u6587\u6863\u68c0\u7d22\u548c\u5b9e\u65f6\u6570\u636e\u5e93\u66f4\u65b0\u6765\u52a8\u6001\u9002\u5e94\u3002\u5b83\u5c06\u68c0\u7d22\u4e0e\u63a8\u7406\u5206\u79bb\uff0c\u5b9e\u73b0\u4e86\u5728\u4efb\u610f\u5927\u89c4\u6a21\u8bed\u6599\u5e93\u4e0a\u7684\u53ef\u6269\u5c55\u63a8\u7406\u3002", "result": "MuaLLM \u5728 RAG-250 \u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86 90.1% \u7684\u53ec\u56de\u7387\uff0c\u5728 Reasoning-100\uff08Reas-100\uff09\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e86 86.8% \u7684\u51c6\u786e\u7387\u3002\u4e0e\u6807\u51c6 LLM \u76f8\u6bd4\uff0c\u5728\u76f8\u540c\u7684\u4e0a\u4e0b\u6587\u957f\u5ea6\u4e0b\uff0cMuaLLM \u7684\u6210\u672c\u964d\u4f4e\u4e86 10 \u500d\uff0c\u901f\u5ea6\u63d0\u9ad8\u4e86 1.6 \u500d\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u76f8\u540c\u7684\u51c6\u786e\u6027\u3002", "conclusion": "MuaLLM \u662f\u4e00\u4e2a\u5f00\u6e90\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4ee3\u7406\uff0c\u7528\u4e8e\u7535\u8def\u8bbe\u8ba1\u8f85\u52a9\u3002\u5b83\u901a\u8fc7\u7ed3\u5408\u6df7\u5408\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6846\u67b6\u548c\u81ea\u9002\u5e94\u7535\u8def\u8bbe\u8ba1\u7814\u7a76\u8bba\u6587\u5411\u91cf\u6570\u636e\u5e93\uff0c\u5e76\u91c7\u7528 Reason + Act\uff08ReAct\uff09\u5de5\u4f5c\u6d41\u8fdb\u884c\u8fed\u4ee3\u63a8\u7406\u3001\u76ee\u6807\u8bbe\u5b9a\u548c\u591a\u6b65\u4fe1\u606f\u68c0\u7d22\uff0c\u89e3\u51b3\u4e86\u6587\u732e\u7efc\u8ff0\u7684\u6311\u6218\u3002\u4e0e\u5176\u4ed6\u6a21\u578b\u76f8\u6bd4\uff0cMuaLLM \u5728\u4fdd\u6301\u76f8\u540c\u51c6\u786e\u6027\u7684\u524d\u63d0\u4e0b\uff0c\u6210\u672c\u964d\u4f4e\u9ad8\u8fbe 10 \u500d\uff0c\u901f\u5ea6\u63d0\u9ad8 1.6 \u500d\uff0c\u5e76\u4e14\u80fd\u591f\u5904\u7406\u6587\u672c\u548c\u89c6\u89c9\u6570\u636e\u3002\u5728 RAG-250 \u548c Reasoning-100 \u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cMuaLLM \u5728\u68c0\u7d22\u548c\u591a\u6b65\u63a8\u7406\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5206\u522b\u8fbe\u5230\u4e86 90.1% \u7684\u53ec\u56de\u7387\u548c 86.8% \u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2508.07405", "categories": ["cs.AI", "cs.CL", "cs.LG", "I.2.7; I.5.4"], "pdf": "https://arxiv.org/pdf/2508.07405", "abs": "https://arxiv.org/abs/2508.07405", "authors": ["Jesse Ponnock"], "title": "Generative AI for Strategic Plan Development", "comment": "11 pages, 9 figures", "summary": "Given recent breakthroughs in Generative Artificial Intelligence (GAI) and\nLarge Language Models (LLMs), more and more professional services are being\naugmented through Artificial Intelligence (AI), which once seemed impossible to\nautomate. This paper presents a modular model for leveraging GAI in developing\nstrategic plans for large scale government organizations and evaluates leading\nmachine learning techniques in their application towards one of the identified\nmodules. Specifically, the performance of BERTopic and Non-negative Matrix\nFactorization (NMF) are evaluated in their ability to use topic modeling to\ngenerate themes representative of Vision Elements within a strategic plan. To\naccomplish this, BERTopic and NMF models are trained using a large volume of\nreports from the Government Accountability Office (GAO). The generated topics\nfrom each model are then scored for similarity against the Vision Elements of a\npublished strategic plan and the results are compared. Our results show that\nthese techniques are capable of generating themes similar to 100% of the\nelements being evaluated against. Further, we conclude that BERTopic performs\nbest in this application with more than half of its correlated topics achieving\na \"medium\" or \"strong\" correlation. A capability of GAI-enabled strategic plan\ndevelopment impacts a multi-billion dollar industry and assists the federal\ngovernment in overcoming regulatory requirements which are crucial to the\npublic good. Further work will focus on the operationalization of the concept\nproven in this study as well as viability of the remaining modules in the\nproposed model for GAI-generated strategic plans.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528GAI\u5f00\u53d1\u653f\u5e9c\u6218\u7565\u8ba1\u5212\u7684\u6a21\u5757\u5316\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u4e86BERTopic\u548cNMF\u5728\u4e3b\u9898\u5efa\u6a21\u751f\u6210\u613f\u666f\u8981\u7d20\u65b9\u9762\u7684\u8868\u73b0\u3002\u7ed3\u679c\u8868\u660e\uff0cBERTopic\u6548\u679c\u66f4\u4f18\uff0c\u4e3aGAI\u5728\u6218\u7565\u89c4\u5212\u9886\u57df\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u968f\u7740\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GAI\uff09\u548c\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7a81\u7834\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5229\u7528GAI\u5f00\u53d1\u5927\u89c4\u6a21\u653f\u5e9c\u7ec4\u7ec7\u6218\u7565\u8ba1\u5212\u7684\u6a21\u5757\u5316\u6a21\u578b\uff0c\u5e76\u8bc4\u4f30\u673a\u5668\u5b66\u4e60\u6280\u672f\u5728\u8be5\u6a21\u578b\u7684\u4e00\u4e2a\u6a21\u5757\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u8bc4\u4f30\u4e86BERTopic\u548cNMF\u4e24\u79cd\u4e3b\u9898\u5efa\u6a21\u6280\u672f\uff0c\u4f7f\u7528\u653f\u5e9c\u95ee\u8d23\u5c40\uff08GAO\uff09\u7684\u62a5\u544a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u4e0e\u5df2\u53d1\u5e03\u7684\u6218\u7565\u8ba1\u5212\u7684\u613f\u666f\u8981\u7d20\u8fdb\u884c\u76f8\u4f3c\u5ea6\u8bc4\u5206\u6765\u6bd4\u8f83\u5176\u6027\u80fd\u3002", "result": "BERTopic\u548cNMF\u6280\u672f\u80fd\u591f\u751f\u6210\u4e0e\u8bc4\u4f30\u7684\u613f\u666f\u8981\u7d20\u76f8\u4f3c\u7684\u4e3b\u9898\uff0c\u5176\u4e2dBERTopic\u7684\u76f8\u5173\u4e3b\u9898\u6709\u8d85\u8fc7\u4e00\u534a\u8fbe\u5230\u4e86\u201c\u4e2d\u7b49\u201d\u6216\u201c\u5f3a\u201d\u76f8\u5173\u6027\u3002", "conclusion": "BERTopic\u5728\u4e3b\u9898\u5efa\u6a21\u751f\u6210\u6218\u7565\u8ba1\u5212\u613f\u666f\u8981\u7d20\u65b9\u9762\u8868\u73b0\u4f18\u4e8eNMF\uff0c\u8d85\u8fc7\u4e00\u534a\u7684\u76f8\u5173\u4e3b\u9898\u8fbe\u5230\u4e86\u201c\u4e2d\u7b49\u201d\u6216\u201c\u5f3a\u201d\u76f8\u5173\u6027\u3002"}}
{"id": "2508.06966", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06966", "abs": "https://arxiv.org/abs/2508.06966", "authors": ["Hiba Najjar", "Bushra Alshbib", "Andreas Dengel"], "title": "Can Multitask Learning Enhance Model Explainability?", "comment": "Accepted at GCPR 2025, Special Track \"Photogrammetry and remote\n  sensing\"", "summary": "Remote sensing provides satellite data in diverse types and formats. The\nusage of multimodal learning networks exploits this diversity to improve model\nperformance, except that the complexity of such networks comes at the expense\nof their interpretability. In this study, we explore how modalities can be\nleveraged through multitask learning to intrinsically explain model behavior.\nIn particular, instead of additional inputs, we use certain modalities as\nadditional targets to be predicted along with the main task. The success of\nthis approach relies on the rich information content of satellite data, which\nremains as input modalities. We show how this modeling context provides\nnumerous benefits: (1) in case of data scarcity, the additional modalities do\nnot need to be collected for model inference at deployment, (2) the model\nperformance remains comparable to the multimodal baseline performance, and in\nsome cases achieves better scores, (3) prediction errors in the main task can\nbe explained via the model behavior in the auxiliary task(s). We demonstrate\nthe efficiency of our approach on three datasets, including segmentation,\nclassification, and regression tasks. Code available at\ngit.opendfki.de/hiba.najjar/mtl_explainability/.", "AI": {"tldr": "\u4f7f\u7528\u591a\u4efb\u52a1\u5b66\u4e60\uff0c\u5c06\u536b\u661f\u6570\u636e\u6a21\u6001\u4f5c\u4e3a\u8f85\u52a9\u76ee\u6807\uff0c\u5728\u4e0d\u727a\u7272\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u89e3\u51b3\u4e86\u6570\u636e\u7a00\u758f\u6027\u7684\u95ee\u9898\u3002", "motivation": "\u63a2\u7d22\u5982\u4f55\u901a\u8fc7\u591a\u4efb\u52a1\u5b66\u4e60\u6765\u5229\u7528\u6a21\u6001\u4ee5\u5185\u5728\u89e3\u91ca\u6a21\u578b\u884c\u4e3a\uff0c\u4ee5\u89e3\u51b3\u591a\u6a21\u6001\u5b66\u4e60\u7f51\u7edc\u590d\u6742\u6027\u5bfc\u81f4\u7684\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u5c06\u67d0\u4e9b\u6a21\u6001\u4f5c\u4e3a\u989d\u5916\u76ee\u6807\uff0c\u4e0e\u4e3b\u4efb\u52a1\u4e00\u8d77\u9884\u6d4b\uff0c\u800c\u4e0d\u662f\u4f5c\u4e3a\u9644\u52a0\u8f93\u5165\u3002", "result": "\uff081\uff09\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u90e8\u7f72\u65f6\u8fdb\u884c\u6a21\u578b\u63a8\u7406\u4e0d\u9700\u8981\u6536\u96c6\u989d\u5916\u7684\u6a21\u6001\uff1b\uff082\uff09\u6a21\u578b\u6027\u80fd\u4e0e\u591a\u6a21\u6001\u57fa\u7ebf\u6a21\u578b\u6027\u80fd\u76f8\u5f53\uff0c\u6709\u65f6\u751a\u81f3\u66f4\u4f18\uff1b\uff083\uff09\u53ef\u4ee5\u901a\u8fc7\u8f85\u52a9\u4efb\u52a1\u4e2d\u7684\u6a21\u578b\u884c\u4e3a\u6765\u89e3\u91ca\u4e3b\u4efb\u52a1\u4e2d\u7684\u9884\u6d4b\u9519\u8bef\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e09\u79cd\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u6548\u7387\u9a8c\u8bc1\uff0c\u5305\u62ec\u5206\u5272\u3001\u5206\u7c7b\u548c\u56de\u5f52\u4efb\u52a1\u3002"}}
{"id": "2508.07947", "categories": ["cond-mat.mtrl-sci", "cond-mat.mes-hall"], "pdf": "https://arxiv.org/pdf/2508.07947", "abs": "https://arxiv.org/abs/2508.07947", "authors": ["Zhenzhou Guo", "Xiaodong Zhou", "Wenhong Wang", "Zhenxiang Cheng", "Xiaotian Wang"], "title": "Sliding Ferroelectric Metal with Ferrimagnetism", "comment": null, "summary": "Two-dimensional (2D) sliding ferroelectric (FE) metals with ferrimagnetism\nrepresent a previously unexplored class of spintronic materials, where the\ninterplay of ferroelectricity, metallicity, and magnetism enables strong\nmagnetoelectric (ME) coupling and electrically tunable spintronic\nfunctionalities. Here, based on antiferromagnetic (AFM) metallic bilayers, we\npropose a general strategy for constructing 2D sliding FE ferrimagnetic (FiM)\nmetals that can achieve tri-state switching, in which the FE polarization, spin\nsplitting, and net magnetization are reversed simultaneously through FE\nswitching. As a prototypical realization, we design a bilayer sliding FE metal\nwith FiM order, derived from monolayer Fe5GeTe2-a van der Waals metal with\nintrinsic magnetic order close to room temperature. The system exhibits a FE\ntransition from a nonpolar (NP) AFM phase to a FE FiM phase via interlayer\nsliding. The in-plane mirror symmetry breaking in FE metallic states lift the\nspin degeneracy that exists in the NP phase, leading to a sizable net magnetic\nmoment and strong linear ME coupling. The interplay between metallicity and FE\nFiM gives rise to pronounced sign-reversible transport responses near the Fermi\nlevel, all of which can be fully electrically controlled by FE switching\nwithout reorienting the N\\'{e}el order. Our results establish sliding FE metals\nwith FiM as a promising platform for electrically reconfigurable, high-speed,\nand low-dissipation spintronic devices.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u578b\u7684\u4e8c\u7ef4\u6ed1\u52a8\u94c1\u7535\u4e9a\u94c1\u78c1\u91d1\u5c5e\uff0c\u901a\u8fc7\u5c42\u95f4\u6ed1\u52a8\u5b9e\u73b0\u4e86\u94c1\u7535\u6027\u3001\u91d1\u5c5e\u6027\u548c\u78c1\u6027\u7684\u8026\u5408\uff0c\u4ece\u800c\u5728\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u4e2d\u5b9e\u73b0\u4e86\u53ef\u7535\u8c03\u8c10\u7684\u529f\u80fd\u548c\u4e09\u6001\u5f00\u5173\u3002", "motivation": "\u63a2\u7d22\u5177\u6709\u94c1\u7535\u6027\u3001\u91d1\u5c5e\u6027\u548c\u78c1\u6027\u76f8\u4e92\u4f5c\u7528\u7684\u65b0\u578b\u81ea\u65cb\u7535\u5b50\u6750\u6599\uff0c\u4ee5\u5b9e\u73b0\u5f3a\u78c1\u7535\u8026\u5408\u548c\u53ef\u7535\u8c03\u8c10\u7684\u81ea\u65cb\u7535\u5b50\u529f\u80fd\u3002", "method": "\u901a\u8fc7\u8bbe\u8ba1\u53cc\u5c42\u6ed1\u52a8FE\u91d1\u5c5e\uff0c\u4ece\u5355\u5c42Fe5GeTe2\u884d\u751f\u7684\u5177\u6709\u672c\u5f81\u78c1\u5e8f\u7684\u8303\u5fb7\u534e\u91d1\u5c5e\uff0c\u5b9e\u73b0FE\u76f8\u53d8\u5e76\u7814\u7a76\u5176\u78c1\u7535\u8026\u5408\u548c\u8f93\u8fd0\u6027\u8d28\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e8c\u7ef4\u6ed1\u52a8FE\u4e9a\u94c1\u78c1\u91d1\u5c5e\uff0c\u5b9e\u73b0\u4e86\u4e09\u6001\u5f00\u5173\uff0c\u5176\u4e2dFE\u6781\u5316\u3001\u81ea\u65cb\u5288\u88c2\u548c\u51c0\u78c1\u5316\u5f3a\u5ea6\u901a\u8fc7FE\u5f00\u5173\u540c\u65f6\u53cd\u8f6c\u3002\u8bc1\u660e\u4e86FE\u91d1\u5c5e\u6001\u7684 in-plane mirror symmetry breaking \u5bfc\u81f4\u4e86\u51c0\u78c1\u77e9\u548c\u5f3a\u7ebf\u6027ME\u8026\u5408\u3002\u89c2\u5bdf\u5230\u91d1\u5c5e\u6027\u548cFE FiM\u76f8\u4e92\u4f5c\u7528\u5728\u8d39\u7c73\u80fd\u7ea7\u9644\u8fd1\u4ea7\u751f\u4e86\u7b26\u53f7\u53ef\u9006\u7684\u8f93\u8fd0\u54cd\u5e94\uff0c\u5e76\u4e14\u8fd9\u4e9b\u54cd\u5e94\u53ef\u4ee5\u901a\u8fc7FE\u5f00\u5173\u8fdb\u884c\u7535\u63a7\u5236\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u6784\u5efa\u4e8c\u7ef4\uff082D\uff09\u6ed1\u52a8\u94c1\u7535\uff08FE\uff09\u4e9a\u94c1\u78c1\uff08FiM\uff09\u91d1\u5c5e\u7684\u4e00\u822c\u7b56\u7565\uff0c\u8be5\u6750\u6599\u7ed3\u5408\u4e86\u94c1\u7535\u6027\u3001\u91d1\u5c5e\u6027\u548c\u78c1\u6027\uff0c\u5b9e\u73b0\u4e86\u5f3a\u78c1\u7535\uff08ME\uff09\u8026\u5408\u548c\u53ef\u7535\u8c03\u8c10\u7684\u81ea\u65cb\u7535\u5b50\u529f\u80fd\u3002\u4f5c\u4e3a\u539f\u578b\uff0c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u7531\u5355\u5c42Fe5GeTe2\u884d\u751f\u7684\u53cc\u5c42\u6ed1\u52a8FE\u91d1\u5c5e\uff0c\u8be5\u91d1\u5c5e\u5177\u6709\u63a5\u8fd1\u5ba4\u6e29\u7684\u672c\u5f81\u78c1\u5e8f\u3002\u7cfb\u7edf\u901a\u8fc7\u5c42\u95f4\u6ed1\u52a8\u5b9e\u73b0\u4e86\u4ece\u975e\u6781\u6027\uff08NP\uff09\u53cd\u94c1\u78c1\uff08AFM\uff09\u76f8\u5230FE FiM\u76f8\u7684FE\u76f8\u53d8\u3002FE\u91d1\u5c5e\u6001\u7684 in-plane mirror symmetry breaking \u5bfc\u81f4\u4e86NP\u76f8\u4e2d\u5b58\u5728\u7684\u81ea\u65cb\u7b80\u5e76\u6027\u7684\u7834\u88c2\uff0c\u4ece\u800c\u4ea7\u751f\u4e86\u663e\u8457\u7684\u51c0\u78c1\u77e9\u548c\u5f3a\u7ebf\u6027ME\u8026\u5408\u3002\u91d1\u5c5e\u6027\u548cFE FiM\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u5728\u8d39\u7c73\u80fd\u7ea7\u9644\u8fd1\u4ea7\u751f\u4e86\u660e\u663e\u7684\u7b26\u53f7\u53ef\u9006\u7684\u8f93\u8fd0\u54cd\u5e94\uff0c\u8fd9\u4e9b\u54cd\u5e94\u90fd\u53ef\u4ee5\u901a\u8fc7FE\u5f00\u5173\u5b8c\u5168\u7531\u7535\u63a7\u5236\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u5b9a\u5411N\u00e9el\u5e8f\u3002\u8be5\u7ed3\u679c\u8868\u660e\uff0c\u5177\u6709FiM\u7684\u6ed1\u52a8FE\u91d1\u5c5e\u662f\u7535\u53ef\u91cd\u6784\u3001\u9ad8\u901f\u548c\u4f4e\u529f\u8017\u81ea\u65cb\u7535\u5b50\u5668\u4ef6\u7684\u6709\u524d\u9014\u7684\u5e73\u53f0\u3002"}}
{"id": "2508.07606", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07606", "abs": "https://arxiv.org/abs/2508.07606", "authors": ["Hongtao Li", "Ziyuan Jiao", "Xiaofeng Liu", "Hangxin Liu", "Zilong Zheng"], "title": "In-situ Value-aligned Human-Robot Interactions with Physical Constraints", "comment": "8 pages, 7 figures", "summary": "Equipped with Large Language Models (LLMs), human-centered robots are now\ncapable of performing a wide range of tasks that were previously deemed\nchallenging or unattainable. However, merely completing tasks is insufficient\nfor cognitive robots, who should learn and apply human preferences to future\nscenarios. In this work, we propose a framework that combines human preferences\nwith physical constraints, requiring robots to complete tasks while considering\nboth. Firstly, we developed a benchmark of everyday household activities, which\nare often evaluated based on specific preferences. We then introduced\nIn-Context Learning from Human Feedback (ICLHF), where human feedback comes\nfrom direct instructions and adjustments made intentionally or unintentionally\nin daily life. Extensive sets of experiments, testing the ICLHF to generate\ntask plans and balance physical constraints with preferences, have demonstrated\nthe efficiency of our approach.", "AI": {"tldr": "\u63d0\u51faICLHF\u6846\u67b6\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u5728\u6267\u884c\u4efb\u52a1\u65f6\u517c\u987e\u7269\u7406\u7ea6\u675f\u548c\u4eba\u7c7b\u504f\u597d\u3002", "motivation": "\u4e3a\u4e86\u8ba9\u4ee5\u4eba\u4e3a\u672c\u7684\u673a\u5668\u4eba\u4e0d\u4ec5\u80fd\u5b8c\u6210\u4efb\u52a1\uff0c\u8fd8\u80fd\u5b66\u4e60\u5e76\u5e94\u7528\u4eba\u7c7b\u504f\u597d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5c06\u4eba\u7c7b\u504f\u597d\u4e0e\u7269\u7406\u7ea6\u675f\u76f8\u7ed3\u5408\u7684\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u4ece\u4eba\u7c7b\u53cd\u9988\u4e2d\u8fdb\u884c\u8bed\u5883\u5b66\u4e60\uff08ICLHF\uff09\uff0c\u4ece\u65e5\u5e38\u751f\u6d3b\u4e2d\u6536\u96c6\u4eba\u7c7b\u53cd\u9988\u3002", "result": "\u901a\u8fc7\u5728\u65e5\u5e38\u5bb6\u52a1\u6d3b\u52a8\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6709\u6548\uff0c\u80fd\u5728\u6267\u884c\u4efb\u52a1\u65f6\u517c\u987e\u7269\u7406\u7ea6\u675f\u548c\u4eba\u7c7b\u504f\u597d\u3002"}}
{"id": "2508.07853", "categories": ["quant-ph", "cond-mat.quant-gas"], "pdf": "https://arxiv.org/pdf/2508.07853", "abs": "https://arxiv.org/abs/2508.07853", "authors": ["Tom Schmit", "Catalin-Mihai Halati", "Tobias Donner", "Giovanna Morigi", "Simon B. J\u00e4ger"], "title": "Master Equation for Quantum Self-Organization of Atoms and Molecules in Cavities", "comment": "22 pages, 3 figures", "summary": "Quantum gases of atoms and molecules in optical cavities offer a formidable\nlaboratory for studying the out-of-equilibrium dynamics of long-range\ninteracting systems. Multiple scattering of cavity photons mediate\ninteractions, and the emerging phases of matter are determined by the interplay\nof photon-mediated forces, dissipation, and quantum and thermal fluctuations.\nControl of these dynamics requires a detailed understanding of the mechanisms\nat play. Due to the number of degrees of freedom, however, effective\ntheoretical models often work in specific limits, where either the cavity field\nis treated as a semiclassical variable or the cavity state is a slightly\nperturbed vacuum state. In this work, we present the derivation of an effective\nLindblad master equation for the dynamics of the sole motional variables of\npolarizable particles, such as atoms or molecules, that dispersively couple to\nthe cavity field. The master equation is valid even for relatively large\nintracavity photon numbers, and is apt to study both the steady-state regime\nand the out-of-equilibrium dynamics where quantum fluctuations of the field\nseed the onset of macroscopic coherences. We validate the theoretical\ndescription by showing that it captures the dynamics across a wide temperature\ninterval, from Doppler cooling down to the ultra-cold regime, and from weak to\nstrong cavity-mediated interactions. Our theory provides a powerful framework\nfor the description of the dynamics of quantum gases in cavities and permits,\namongst others, to connect models and hypotheses of statistical mechanics with\na versatile experimental platform.", "AI": {"tldr": "\u6211\u4eec\u63a8\u5bfc\u4e86\u4e00\u4e2a\u6709\u6548\u7684Lindblad\u4e3b\u65b9\u7a0b\uff0c\u7528\u4e8e\u63cf\u8ff0\u8154\u5185\u91cf\u5b50\u6c14\u4f53\u4e2d\u7c92\u5b50\u7684\u8fd0\u52a8\u52a8\u529b\u5b66\uff0c\u8be5\u65b9\u7a0b\u5728\u5149\u5b50\u6570\u8f83\u5927\u65f6\u4e5f\u6709\u6548\uff0c\u5e76\u80fd\u6355\u6349\u4ece\u5f31\u5230\u5f3a\u76f8\u4e92\u4f5c\u7528\u7684\u52a8\u529b\u5b66\u3002", "motivation": "\u4e3a\u4e86\u63a7\u5236\u8154\u5185\u91cf\u5b50\u6c14\u4f53\u4e2d\u76f8\u4e92\u4f5c\u7528\u7cfb\u7edf\u7684\u975e\u5e73\u8861\u52a8\u529b\u5b66\uff0c\u9700\u8981\u5bf9\u4f5c\u7528\u673a\u5236\u6709\u8be6\u7ec6\u7684\u4e86\u89e3\u3002\u7136\u800c\uff0c\u7531\u4e8e\u81ea\u7531\u5ea6\u7684\u6570\u91cf\uff0c\u6709\u6548\u7684\u7406\u8bba\u6a21\u578b\u901a\u5e38\u5728\u7279\u5b9a\u6781\u9650\u4e0b\u5de5\u4f5c\u3002", "method": "\u6211\u4eec\u63a8\u5bfc\u4e86\u4e00\u4e2a\u6709\u6548\u7684Lindblad\u4e3b\u65b9\u7a0b\uff0c\u7528\u4e8e\u63cf\u8ff0\u4ec5\u9650\u4e8e\u53ef\u6781\u5316\u7c92\u5b50\uff08\u5982\u539f\u5b50\u6216\u5206\u5b50\uff09\u7684\u8fd0\u52a8\u53d8\u91cf\u7684\u52a8\u529b\u5b66\uff0c\u8fd9\u4e9b\u7c92\u5b50\u4e0e\u8154\u573a\u5448\u8272\u6563\u8026\u5408\u3002\u8be5\u4e3b\u65b9\u7a0b\u5373\u4f7f\u5728\u8154\u5185\u5149\u5b50\u6570\u76f8\u5bf9\u8f83\u5927\u7684\u60c5\u51b5\u4e0b\u4e5f\u6709\u6548\u3002", "result": "\u6211\u4eec\u901a\u8fc7\u8868\u660e\u8be5\u7406\u8bba\u80fd\u591f\u6355\u6349\u4ece\u591a\u666e\u52d2\u51b7\u5374\u5230\u8d85\u51b7\u72b6\u6001\u7684\u5bbd\u6e29\u5ea6\u533a\u95f4\u4ee5\u53ca\u4ece\u5f31\u5230\u5f3a\u8154\u4ecb\u5bfc\u76f8\u4e92\u4f5c\u7528\u7684\u52a8\u529b\u5b66\uff0c\u4ece\u800c\u9a8c\u8bc1\u4e86\u8be5\u7406\u8bba\u63cf\u8ff0\u3002", "conclusion": "\u8be5\u7406\u8bba\u4e3a\u8154\u5185\u91cf\u5b50\u6c14\u4f53\u7684\u52a8\u529b\u5b66\u63cf\u8ff0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u7684\u6846\u67b6\uff0c\u5e76\u5141\u8bb8\u5c06\u7edf\u8ba1\u529b\u5b66\u7684\u6a21\u578b\u548c\u5047\u8bbe\u4e0e\u591a\u529f\u80fd\u7684\u5b9e\u9a8c\u5e73\u53f0\u8054\u7cfb\u8d77\u6765\u3002"}}
{"id": "2508.07248", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07248", "abs": "https://arxiv.org/abs/2508.07248", "authors": ["Zhe Ren"], "title": "Prompt Tuning for Few-Shot Continual Learning Named Entity Recognition", "comment": null, "summary": "Knowledge distillation has been successfully applied to Continual Learning\nNamed Entity Recognition (CLNER) tasks, by using a teacher model trained on\nold-class data to distill old-class entities present in new-class data as a\nform of regularization, thereby avoiding catastrophic forgetting. However, in\nFew-Shot CLNER (FS-CLNER) tasks, the scarcity of new-class entities makes it\ndifficult for the trained model to generalize during inference. More\ncritically, the lack of old-class entity information hinders the distillation\nof old knowledge, causing the model to fall into what we refer to as the\nFew-Shot Distillation Dilemma. In this work, we address the above challenges\nthrough a prompt tuning paradigm and memory demonstration template strategy.\nSpecifically, we designed an expandable Anchor words-oriented Prompt Tuning\n(APT) paradigm to bridge the gap between pre-training and fine-tuning, thereby\nenhancing performance in few-shot scenarios. Additionally, we incorporated\nMemory Demonstration Templates (MDT) into each training instance to provide\nreplay samples from previous tasks, which not only avoids the Few-Shot\nDistillation Dilemma but also promotes in-context learning. Experiments show\nthat our approach achieves competitive performances on FS-CLNER.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08FS-CLNER\uff09\u4e2d\u7684\u201c\u5c11\u6837\u672c\u84b8\u998f\u56f0\u5883\u201d\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u951a\u70b9\u8bcd\u63d0\u793a\u8bcd\u8c03\u4f18\uff08APT\uff09\u548c\u8bb0\u5fc6\u6f14\u793a\u6a21\u677f\uff08MDT\uff09\u7684\u7b56\u7565\uff0c\u4ee5\u589e\u5f3a\u6a21\u578b\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u5e76\u6709\u6548\u5229\u7528\u65e7\u77e5\u8bc6\u3002", "motivation": "\u5728\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08FS-CLNER\uff09\u4efb\u52a1\u4e2d\uff0c\u65b0\u7c7b\u5b9e\u4f53\u7684\u7a00\u7f3a\u6027\u4f7f\u5f97\u6a21\u578b\u5728\u63a8\u7406\u65f6\u96be\u4ee5\u6cdb\u5316\uff0c\u5e76\u4e14\u7f3a\u4e4f\u65e7\u7c7b\u5b9e\u4f53\u4fe1\u606f\u963b\u788d\u4e86\u65e7\u77e5\u8bc6\u7684\u84b8\u998f\uff0c\u5bfc\u81f4\u4e86\u201c\u5c11\u6837\u672c\u84b8\u998f\u56f0\u5883\u201d\u3002", "method": "\u901a\u8fc7\u91c7\u7528\u63d0\u793a\u8bcd\u8c03\u4f18\u8303\u5f0f\u548c\u8bb0\u5fc6\u6f14\u793a\u6a21\u677f\u7b56\u7565\u6765\u89e3\u51b3\u6311\u6218\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u9762\u5411\u951a\u70b9\u8bcd\u7684\u63d0\u793a\u8bcd\u8c03\u4f18\uff08APT\uff09\u8303\u5f0f\u6765\u5f25\u5408\u9884\u8bad\u7ec3\u548c\u5fae\u8c03\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u7ed3\u5408\u8bb0\u5fc6\u6f14\u793a\u6a21\u677f\uff08MDT\uff09\u4e3a\u6bcf\u4e2a\u8bad\u7ec3\u5b9e\u4f8b\u63d0\u4f9b\u6765\u81ea\u5148\u524d\u4efb\u52a1\u7684\u91cd\u653e\u6837\u672c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728FS-CLNER\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u5c11\u6837\u672c\u6301\u7eed\u5b66\u4e60\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\uff08FS-CLNER\uff09\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06819", "abs": "https://arxiv.org/abs/2508.06819", "authors": ["Ayaan Nooruddin Siddiqui", "Mahnoor Zaidi", "Ayesha Nazneen Shahbaz", "Priyadarshini Chatterjee", "Krishnan Menon Iyer"], "title": "VesselRW: Weakly Supervised Subcutaneous Vessel Segmentation via Learned Random Walk Propagation", "comment": null, "summary": "Accurate segmentation of subcutaneous vessels from clinical images is\nhampered by scarce, expensive ground truth and by low contrast, noisy\nappearance of vessels across patients and modalities. We present a novel weakly\nsupervised training framework tailored for subcutaneous vessel segmentation\nthat leverages inexpensive sparse annotations (e.g., centerline traces, dot\nmarkers, or short scribbles). Sparse labels are expanded into dense,\nprobabilistic supervision via a differentiable random walk label propagation\nmodel whose transition weights incorporate image driven vesselness cues and\ntubular continuity priors. The propagation yields per-pixel hitting\nprobabilities together with calibrated uncertainty estimates; these are\nincorporated into an uncertainty weighted loss to avoid over fitting to\nambiguous regions. Crucially, the label-propagator is learned jointly with a\nCNN based segmentation predictor, enabling the system to discover vessel edges\nand continuity constraints without explicit edge supervision. We further\nintroduce a topology aware regularizer that encourages centerline connectivity\nand penalizes spurious branches, improving clinical usability. In experiments\non clinical subcutaneous imaging datasets, our method consistently outperforms\nnaive training on sparse labels and conventional dense pseudo-labeling,\nproducing more complete vascular maps and better calibrated uncertainty for\ndownstream decision making. The approach substantially reduces annotation\nburden while preserving clinically relevant vessel topology.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u4f7f\u7528\u7a00\u758f\u7684\u6ce8\u91ca\uff08\u5982\u4e2d\u5fc3\u7ebf\uff09\u6765\u8bad\u7ec3\u8840\u7ba1\u5206\u5272\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u968f\u673a\u6e38\u8d70\u6807\u7b7e\u4f20\u64ad\u751f\u6210\u5bc6\u96c6\u7684\u76d1\u7763\u4fe1\u53f7\uff0c\u5e76\u7ed3\u5408\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u635f\u5931\u548c\u62d3\u6251\u6b63\u5219\u5316\u5668\uff0c\u5728\u964d\u4f4e\u6ce8\u91ca\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u8840\u7ba1\u5206\u5272\u7684\u51c6\u786e\u6027\u548c\u5b8c\u6574\u6027\u3002", "motivation": "\u51c6\u786e\u5730\u5206\u5272\u4e34\u5e8a\u56fe\u50cf\u4e2d\u7684\u76ae\u4e0b\u8840\u7ba1\u53d7\u5230\u771f\u5b9e\u6807\u7b7e\u7a00\u5c11\u4e14\u6602\u8d35\u3001\u4ee5\u53ca\u8840\u7ba1\u5728\u4e0d\u540c\u60a3\u8005\u548c\u6a21\u6001\u4e4b\u95f4\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u566a\u58f0\u5927\u7684\u5916\u89c2\u7684\u963b\u788d\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u8bad\u7ec3\u6846\u67b6\uff0c\u5229\u7528\u5ec9\u4ef7\u7684\u7a00\u758f\u6ce8\u91ca\uff08\u4f8b\u5982\uff0c\u4e2d\u5fc3\u7ebf\u8ffd\u8e2a\u3001\u70b9\u6807\u8bb0\u6216\u77ed\u6d82\u9e26\uff09\u8fdb\u884c\u76ae\u4e0b\u8840\u7ba1\u5206\u5272\u3002\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u968f\u673a\u6e38\u8d70\u6807\u7b7e\u4f20\u64ad\u6a21\u578b\u5c06\u7a00\u758f\u6807\u7b7e\u6269\u5c55\u4e3a\u5bc6\u96c6\u3001\u6982\u7387\u5316\u7684\u76d1\u7763\uff0c\u8be5\u6a21\u578b\u7684\u8fc7\u6e21\u6743\u91cd\u7ed3\u5408\u4e86\u56fe\u50cf\u9a71\u52a8\u7684\u8840\u7ba1\u7279\u5f81\u548c\u7ba1\u72b6\u8fde\u7eed\u6027\u5148\u9a8c\u3002\u8be5\u4f20\u64ad\u4ea7\u751f\u4e86\u6bcf\u50cf\u7d20\u7684\u547d\u4e2d\u6982\u7387\u4ee5\u53ca\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff1b\u8fd9\u4e9b\u88ab\u7eb3\u5165\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u7684\u635f\u5931\u4e2d\uff0c\u4ee5\u907f\u514d\u5bf9\u6a21\u7cca\u533a\u57df\u8fdb\u884c\u8fc7\u5ea6\u62df\u5408\u3002\u6807\u7b7e\u4f20\u64ad\u5668\u4e0e\u57fa\u4e8eCNN\u7684\u5206\u5272\u9884\u6d4b\u5668\u8054\u5408\u5b66\u4e60\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u6ca1\u6709\u663e\u5f0f\u8fb9\u7f18\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u53d1\u73b0\u8840\u7ba1\u8fb9\u7f18\u548c\u8fde\u7eed\u6027\u7ea6\u675f\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u6b63\u5219\u5316\u5668\uff0c\u9f13\u52b1\u4e2d\u5fc3\u7ebf\u8fde\u901a\u6027\u5e76\u60e9\u7f5a\u865a\u5047\u5206\u652f\u3002", "result": "\u5728\u4e34\u5e8a\u76ae\u4e0b\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u4ea7\u751f\u66f4\u5b8c\u6574\u7684\u8840\u7ba1\u56fe\u548c\u4e3a\u4e0b\u6e38\u51b3\u7b56\u63d0\u4f9b\u66f4\u597d\u7684\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5728\u7a00\u758f\u6807\u7b7e\u4e0a\u7684\u6734\u7d20\u8bad\u7ec3\u548c\u4f20\u7edf\u7684\u5bc6\u96c6\u4f2a\u6807\u7b7e\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e34\u5e8a\u76f8\u5173\u8840\u7ba1\u62d3\u6251\u7684\u540c\u65f6\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u6ce8\u91ca\u8d1f\u62c5\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f31\u76d1\u7763\u8bad\u7ec3\u6846\u67b6\uff0c\u7528\u4e8e\u76ae\u4e0b\u8840\u7ba1\u5206\u5272\uff0c\u5229\u7528\u4e86\u5ec9\u4ef7\u7684\u7a00\u758f\u6ce8\u91ca\uff08\u4f8b\u5982\uff0c\u4e2d\u5fc3\u7ebf\u8ffd\u8e2a\u3001\u70b9\u6807\u8bb0\u6216\u77ed\u6d82\u9e26\uff09\u3002\u901a\u8fc7\u53ef\u5fae\u5206\u7684\u968f\u673a\u6e38\u8d70\u6807\u7b7e\u4f20\u64ad\u6a21\u578b\u5c06\u7a00\u758f\u6807\u7b7e\u6269\u5c55\u4e3a\u5bc6\u96c6\u3001\u6982\u7387\u5316\u7684\u76d1\u7763\uff0c\u8be5\u6a21\u578b\u7684\u8fc7\u6e21\u6743\u91cd\u7ed3\u5408\u4e86\u56fe\u50cf\u9a71\u52a8\u7684\u8840\u7ba1\u7279\u5f81\u548c\u7ba1\u72b6\u8fde\u7eed\u6027\u5148\u9a8c\u3002\u8be5\u4f20\u64ad\u4ea7\u751f\u4e86\u6bcf\u50cf\u7d20\u7684\u547d\u4e2d\u6982\u7387\u4ee5\u53ca\u6821\u51c6\u7684\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\uff1b\u8fd9\u4e9b\u88ab\u7eb3\u5165\u4e0d\u786e\u5b9a\u6027\u52a0\u6743\u7684\u635f\u5931\u4e2d\uff0c\u4ee5\u907f\u514d\u5bf9\u6a21\u7cca\u533a\u57df\u8fdb\u884c\u8fc7\u5ea6\u62df\u5408\u3002\u5173\u952e\u7684\u662f\uff0c\u6807\u7b7e\u4f20\u64ad\u5668\u4e0e\u57fa\u4e8eCNN\u7684\u5206\u5272\u9884\u6d4b\u5668\u8054\u5408\u5b66\u4e60\uff0c\u4f7f\u7cfb\u7edf\u80fd\u591f\u5728\u6ca1\u6709\u663e\u5f0f\u8fb9\u7f18\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u53d1\u73b0\u8840\u7ba1\u8fb9\u7f18\u548c\u8fde\u7eed\u6027\u7ea6\u675f\u3002\u6211\u4eec\u8fdb\u4e00\u6b65\u5f15\u5165\u4e86\u4e00\u79cd\u62d3\u6251\u611f\u77e5\u6b63\u5219\u5316\u5668\uff0c\u9f13\u52b1\u4e2d\u5fc3\u7ebf\u8fde\u901a\u6027\u5e76\u60e9\u7f5a\u865a\u5047\u5206\u652f\uff0c\u4ece\u800c\u63d0\u9ad8\u4e34\u5e8a\u53ef\u7528\u6027\u3002\u5728\u4e34\u5e8a\u76ae\u4e0b\u6210\u50cf\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u672c\u65b9\u6cd5\u5728\u4ea7\u751f\u66f4\u5b8c\u6574\u7684\u8840\u7ba1\u56fe\u548c\u4e3a\u4e0b\u6e38\u51b3\u7b56\u63d0\u4f9b\u66f4\u597d\u7684\u6821\u51c6\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\uff0c\u59cb\u7ec8\u4f18\u4e8e\u5728\u7a00\u758f\u6807\u7b7e\u4e0a\u7684\u6734\u7d20\u8bad\u7ec3\u548c\u4f20\u7edf\u7684\u5bc6\u96c6\u4f2a\u6807\u7b7e\u3002\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u4e34\u5e8a\u76f8\u5173\u8840\u7ba1\u62d3\u6251\u7684\u540c\u65f6\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u6ce8\u91ca\u8d1f\u62c5\u3002"}}
{"id": "2508.08144", "categories": ["cs.RO", "cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2508.08144", "abs": "https://arxiv.org/abs/2508.08144", "authors": ["Ganesh Sundaram", "Jonas Ulmen", "Amjad Haider", "Daniel G\u00f6rges"], "title": "COMponent-Aware Pruning for Accelerated Control Tasks in Latent Space Models", "comment": "Submitted in: The 2026 IEEE/SICE International Symposium on System\n  Integration (SII 2026)", "summary": "The rapid growth of resource-constrained mobile platforms, including mobile\nrobots, wearable systems, and Internet-of-Things devices, has increased the\ndemand for computationally efficient neural network controllers (NNCs) that can\noperate within strict hardware limitations. While deep neural networks (DNNs)\ndemonstrate superior performance in control applications, their substantial\ncomputational complexity and memory requirements present significant barriers\nto practical deployment on edge devices. This paper introduces a comprehensive\nmodel compression methodology that leverages component-aware structured pruning\nto determine the optimal pruning magnitude for each pruning group, ensuring a\nbalance between compression and stability for NNC deployment. Our approach is\nrigorously evaluated on Temporal Difference Model Predictive Control (TD-MPC),\na state-of-the-art model-based reinforcement learning algorithm, with a\nsystematic integration of mathematical stability guarantee properties,\nspecifically Lyapunov criteria. The key contribution of this work lies in\nproviding a principled framework for determining the theoretical limits of\nmodel compression while preserving controller stability. Experimental\nvalidation demonstrates that our methodology successfully reduces model\ncomplexity while maintaining requisite control performance and stability\ncharacteristics. Furthermore, our approach establishes a quantitative boundary\nfor safe compression ratios, enabling practitioners to systematically determine\nthe maximum permissible model reduction before violating critical stability\nproperties, thereby facilitating the confident deployment of compressed NNCs in\nresource-limited environments.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5185\u5b58\u9700\u6c42\u65b9\u9762\u5bf9\u8fb9\u7f18\u8bbe\u5907\u7684\u6311\u6218\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7ec4\u4ef6\u611f\u77e5\u7ed3\u6784\u5316\u526a\u679d\u7684\u7efc\u5408\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u6570\u5b66\u7a33\u5b9a\u6027\u4fdd\u8bc1\uff08\u7279\u522b\u662f\u674e\u96c5\u666e\u8bfa\u592b\u6807\u51c6\uff09\u4e0e\u65f6\u95f4\u5dee\u5206\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08TD-MPC\uff09\u76f8\u7ed3\u5408\uff0c\u5728\u6a21\u578b\u538b\u7f29\u548c\u63a7\u5236\u5668\u7a33\u5b9a\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u5e73\u8861\u3002\u8be5\u65b9\u6cd5\u4e3a\u5b89\u5168\u538b\u7f29\u7387\u8bbe\u5b9a\u4e86\u5b9a\u91cf\u8fb9\u754c\uff0c\u4ece\u800c\u80fd\u591f\u53ef\u9760\u5730\u90e8\u7f72\u538b\u7f29\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff08NNC\uff09\u3002", "motivation": "\u8d44\u6e90\u53d7\u9650\u7684\u79fb\u52a8\u5e73\u53f0\uff08\u5305\u62ec\u79fb\u52a8\u673a\u5668\u4eba\u3001\u53ef\u7a7f\u6234\u7cfb\u7edf\u548c\u7269\u8054\u7f51\u8bbe\u5907\uff09\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u589e\u52a0\u4e86\u5bf9\u8ba1\u7b97\u9ad8\u6548\u7684\u795e\u7ecf\u7f51\u7edc\u63a7\u5236\u5668\uff08NNC\uff09\u7684\u9700\u6c42\uff0c\u8fd9\u4e9b\u63a7\u5236\u5668\u53ef\u4ee5\u5728\u4e25\u683c\u7684\u786c\u4ef6\u9650\u5236\u4e0b\u8fd0\u884c\u3002\u867d\u7136\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u5728\u63a7\u5236\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u4f46\u5176\u5de8\u5927\u7684\u8ba1\u7b97\u590d\u6742\u6027\u548c\u5185\u5b58\u9700\u6c42\u963b\u788d\u4e86\u5176\u5728\u8fb9\u7f18\u8bbe\u5907\u7684\u5b9e\u9645\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u7ec4\u4ef6\u611f\u77e5\u7ed3\u6784\u5316\u526a\u679d\u6765\u786e\u5b9a\u6bcf\u4e2a\u526a\u679d\u7ec4\u7684\u6700\u4f73\u526a\u679d\u5e45\u5ea6\uff0c\u4ee5\u5728\u538b\u7f29\u548c\u7a33\u5b9a\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\u7684\u7efc\u5408\u6a21\u578b\u538b\u7f29\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u65f6\u95f4\u5dee\u5206\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff08TD-MPC\uff09\u8fd9\u4e00\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6a21\u578b\u7684\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u4e0a\u5f97\u5230\u4e86\u4e25\u683c\u8bc4\u4f30\uff0c\u5e76\u7cfb\u7edf\u5730\u6574\u5408\u4e86\u6570\u5b66\u7a33\u5b9a\u6027\u4fdd\u8bc1\u5c5e\u6027\uff0c\u7279\u522b\u662f\u674e\u96c5\u666e\u8bfa\u592b\u6807\u51c6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u6210\u529f\u964d\u4f4e\u4e86\u6a21\u578b\u590d\u6742\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6240\u9700\u7684\u63a7\u5236\u6027\u80fd\u548c\u7a33\u5b9a\u6027\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u4e3a\u5b89\u5168\u538b\u7f29\u7387\u5efa\u7acb\u4e86\u5b9a\u91cf\u8fb9\u754c\uff0c\u4f7f\u5b9e\u8df5\u8005\u80fd\u591f\u7cfb\u7edf\u5730\u786e\u5b9a\u8fdd\u53cd\u5173\u952e\u7a33\u5b9a\u6027\u5c5e\u6027\u4e4b\u524d\u7684\u6700\u5927\u5141\u8bb8\u6a21\u578b\u7f29\u51cf\u91cf\uff0c\u4ece\u800c\u6709\u52a9\u4e8e\u5728\u8d44\u6e90\u6709\u9650\u7684\u73af\u5883\u4e2d\u53ef\u9760\u5730\u90e8\u7f72\u538b\u7f29\u7684NNC\u3002"}}
{"id": "2508.07611", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07611", "abs": "https://arxiv.org/abs/2508.07611", "authors": ["Zifan Wang", "Xun Yang", "Jianzhuang Zhao", "Jiaming Zhou", "Teli Ma", "Ziyao Gao", "Arash Ajoudani", "Junwei Liang"], "title": "End-to-End Humanoid Robot Safe and Comfortable Locomotion Policy", "comment": null, "summary": "The deployment of humanoid robots in unstructured, human-centric environments\nrequires navigation capabilities that extend beyond simple locomotion to\ninclude robust perception, provable safety, and socially aware behavior.\nCurrent reinforcement learning approaches are often limited by blind\ncontrollers that lack environmental awareness or by vision-based systems that\nfail to perceive complex 3D obstacles. In this work, we present an end-to-end\nlocomotion policy that directly maps raw, spatio-temporal LiDAR point clouds to\nmotor commands, enabling robust navigation in cluttered dynamic scenes. We\nformulate the control problem as a Constrained Markov Decision Process (CMDP)\nto formally separate safety from task objectives. Our key contribution is a\nnovel methodology that translates the principles of Control Barrier Functions\n(CBFs) into costs within the CMDP, allowing a model-free Penalized Proximal\nPolicy Optimization (P3O) to enforce safety constraints during training.\nFurthermore, we introduce a set of comfort-oriented rewards, grounded in\nhuman-robot interaction research, to promote motions that are smooth,\npredictable, and less intrusive. We demonstrate the efficacy of our framework\nthrough a successful sim-to-real transfer to a physical humanoid robot, which\nexhibits agile and safe navigation around both static and dynamic 3D obstacles.", "AI": {"tldr": "Humanoid robots navigate safely and smoothly in cluttered environments using LiDAR and a novel reinforcement learning approach combining CMDP and CBFs.", "motivation": "Current reinforcement learning approaches for humanoid robot navigation in human-centric environments are limited by blind controllers or vision-based systems that fail with complex 3D obstacles. There is a need for robust perception, provable safety, and socially aware behavior.", "method": "An end-to-end locomotion policy using Constrained Markov Decision Process (CMDP) and Penalized Proximal Policy Optimization (P3O) is presented. Safety constraints are enforced by translating Control Barrier Functions (CBFs) into costs within the CMDP, and comfort-oriented rewards are introduced for smooth, predictable, and less intrusive motion.", "result": "The framework successfully navigates cluttered dynamic scenes and demonstrates effective sim-to-real transfer to a physical humanoid robot, exhibiting agile and safe movement around static and dynamic 3D obstacles.", "conclusion": "The proposed framework enables agile and safe navigation for humanoid robots in cluttered dynamic scenes by directly mapping LiDAR point clouds to motor commands, achieving successful sim-to-real transfer."}}
{"id": "2508.07262", "categories": ["cs.CL", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07262", "abs": "https://arxiv.org/abs/2508.07262", "authors": ["Bernd J. Kr\u00f6ger"], "title": "The 2D+ Dynamic Articulatory Model DYNARTmo: Tongue-Palate Contact Area Estimation", "comment": "11 pages, 9 figures, 14 references; supplementary material: python\n  source code", "summary": "This paper describes an extension of the two-dimensional dynamic articulatory\nmodel DYNARTmo by integrating an internal three-dimensional representation of\nthe palatal dome to estimate tongue-palate contact areas from midsagittal\ntongue contours. Two alternative dome geometries - a half-ellipse and a cosine\nbased profile - are implemented to model lateral curvature in the coronal\nplane. Using these geometries, lateral contact points are analytically computed\nfor each anterior-posterior position, enabling the generation of\nelectropalatography-like visualizations within the 2D+ framework. The enhanced\nmodel supports three synchronized views (sagittal, glottal, and palatal) for\nstatic and dynamic (animated) articulation displays, suitable for speech\nscience education and speech therapy. Future work includes adding a facial\n(lip) view and implementing articulatory-to-acoustic synthesis to\nquantitatively evaluate model realism.", "AI": {"tldr": "\u8be5\u7814\u7a76\u6269\u5c55\u4e86 DYNARTmo \u6a21\u578b\uff0c\u901a\u8fc7\u4e09\u7ef4\u816d\u9876\u8868\u793a\u6765\u4f30\u7b97\u820c\u816d\u63a5\u89e6\u533a\u57df\uff0c\u5e76\u652f\u6301\u591a\u89c6\u56fe\u663e\u793a\uff0c\u9002\u7528\u4e8e\u6559\u80b2\u548c\u6cbb\u7597\u3002", "motivation": "\u4e3a\u4e86\u6269\u5c55\u4e8c\u7ef4\u52a8\u6001\u53d1\u97f3\u6a21\u578b DYNARTmo\uff0c\u901a\u8fc7\u96c6\u6210\u5185\u90e8\u4e09\u7ef4\u816d\u9876\u8868\u793a\uff0c\u4ece\u800c\u4ece\u77e2\u72b6\u9762\u820c\u5934\u8f6e\u5ed3\u4f30\u7b97\u820c\u816d\u63a5\u89e6\u533a\u57df\u3002", "method": "\u901a\u8fc7\u96c6\u6210\u5185\u90e8\u4e09\u7ef4\u816d\u9876\u8868\u793a\uff0c\u5b9e\u73b0\u5bf9 DYNARTmo \u6a21\u578b\u7684\u6269\u5c55\uff0c\u8be5\u8868\u793a\u91c7\u7528\u534a\u692d\u5706\u548c\u57fa\u4e8e\u4f59\u5f26\u7684\u8f6e\u5ed3\u6765\u6a21\u62df\u51a0\u72b6\u9762\u4e2d\u7684\u4fa7\u5411\u66f2\u7387\u3002\u901a\u8fc7\u89e3\u6790\u8ba1\u7b97\u6bcf\u4e2a\u524d\u540e\u4f4d\u7f6e\u7684\u4fa7\u5411\u63a5\u89e6\u70b9\uff0c\u751f\u6210\u7c7b\u4f3c\u820c\u816d\u63a5\u89e6\u56fe\u7684\u663e\u793a\u3002", "result": "\u6a21\u578b\u53ef\u4ee5\u751f\u6210\u820c\u816d\u63a5\u89e6\u533a\u57df\u7684\u53ef\u89c6\u5316\uff0c\u5e76\u652f\u6301\u540c\u6b65\u7684\u4e09\u4e2a\u89c6\u56fe\uff08\u77e2\u72b6\u9762\u3001\u58f0\u95e8\u9762\u548c\u816d\u9762\uff09\u7684\u9759\u6001\u548c\u52a8\u6001\u663e\u793a\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u96c6\u6210\u5185\u90e8\u4e09\u7ef4\u816d\u9876\u8868\u793a\uff0c\u4ece\u77e2\u72b6\u9762\u820c\u5934\u8f6e\u5ed3\u4f30\u7b97\u820c\u816d\u63a5\u89e6\u533a\u57df\uff0c\u5e76\u652f\u6301\u540c\u6b65\u7684\u4e09\u4e2a\u89c6\u56fe\uff08\u77e2\u72b6\u9762\u3001\u58f0\u95e8\u9762\u548c\u816d\u9762\uff09\u7684\u9759\u6001\u548c\u52a8\u6001\u663e\u793a\uff0c\u9002\u7528\u4e8e\u8bed\u97f3\u79d1\u5b66\u6559\u80b2\u548c\u8bed\u97f3\u6cbb\u7597\u3002"}}
{"id": "2508.06831", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06831", "abs": "https://arxiv.org/abs/2508.06831", "authors": ["Taha Mustapha Nehdi", "Nairouz Mrabah", "Atif Belal", "Marco Pedersoli", "Eric Granger"], "title": "Low-Rank Expert Merging for Multi-Source Domain Adaptation in Person Re-Identification", "comment": null, "summary": "Adapting person re-identification (reID) models to new target environments\nremains a challenging problem that is typically addressed using unsupervised\ndomain adaptation (UDA) methods. Recent works show that when labeled data\noriginates from several distinct sources (e.g., datasets and cameras),\nconsidering each source separately and applying multi-source domain adaptation\n(MSDA) typically yields higher accuracy and robustness compared to blending the\nsources and performing conventional UDA. However, state-of-the-art MSDA methods\nlearn domain-specific backbone models or require access to source domain data\nduring adaptation, resulting in significant growth in training parameters and\ncomputational cost. In this paper, a Source-free Adaptive Gated Experts\n(SAGE-reID) method is introduced for person reID. Our SAGE-reID is a\ncost-effective, source-free MSDA method that first trains individual\nsource-specific low-rank adapters (LoRA) through source-free UDA. Next, a\nlightweight gating network is introduced and trained to dynamically assign\noptimal merging weights for fusion of LoRA experts, enabling effective\ncross-domain knowledge transfer. While the number of backbone parameters\nremains constant across source domains, LoRA experts scale linearly but remain\nnegligible in size (<= 2% of the backbone), reducing both the memory\nconsumption and risk of overfitting. Extensive experiments conducted on three\nchallenging benchmarks: Market-1501, DukeMTMC-reID, and MSMT17 indicate that\nSAGE-reID outperforms state-of-the-art methods while being computationally\nefficient.", "AI": {"tldr": "SAGE-reID \u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u8ba1\u7b97\u6210\u672c\u6548\u76ca\u9ad8\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u5728 person re-identification \u4e2d\u8fdb\u884c\u65e0\u6e90\u591a\u6e90\u57df\u81ea\u9002\u5e94\u3002\u5b83\u901a\u8fc7\u8bad\u7ec3\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u548c\u95e8\u63a7\u7f51\u7edc\u6765\u6709\u6548\u878d\u5408\u6765\u81ea\u591a\u4e2a\u6e90\u57df\u7684\u77e5\u8bc6\uff0c\u800c\u65e0\u9700\u8bbf\u95ee\u6e90\u6570\u636e\uff0c\u4ece\u800c\u5728\u4e0d\u589e\u52a0\u6a21\u578b\u5927\u5c0f\u7684\u60c5\u51b5\u4e0b\u63d0\u9ad8\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u5728\u65b0\u7684\u76ee\u6807\u73af\u5883\u4e2d\u9002\u5e94 person re-identification\uff08reID\uff09\u6a21\u578b\u6240\u9762\u4e34\u7684\u6311\u6218\u3002\u73b0\u6709\u7684\u591a\u6e90\u57df\u81ea\u9002\u5e94\uff08MSDA\uff09\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u4e2a\u6e90\u57df\u5b66\u4e60\u7279\u5b9a\u4e3b\u5e72\u6a21\u578b\u6216\u5728\u9002\u5e94\u8fc7\u7a0b\u4e2d\u8bbf\u95ee\u6e90\u57df\u6570\u636e\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u8bad\u7ec3\u53c2\u6570\u548c\u8ba1\u7b97\u6210\u672c\u663e\u8457\u589e\u52a0\u3002", "method": "SAGE-reID\uff08Source-free Adaptive Gated Experts\uff09\u662f\u4e00\u79cd\u7ecf\u6d4e\u9ad8\u6548\u3001\u65e0\u6e90\u7684\u591a\u6e90\u57df\u81ea\u9002\u5e94\uff08MSDA\uff09\u65b9\u6cd5\u3002\u5b83\u9996\u5148\u901a\u8fc7\u65e0\u6e90UDA\u8bad\u7ec3\u5355\u72ec\u7684\u3001\u7279\u5b9a\u4e8e\u6e90\u7684\u4f4e\u79e9\u9002\u914d\u5668\uff08LoRA\uff09\u3002\u63a5\u4e0b\u6765\uff0c\u5f15\u5165\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u95e8\u63a7\u7f51\u7edc\u6765\u52a8\u6001\u5206\u914d\u6700\u4f73\u7684\u5408\u5e76\u6743\u91cd\uff0c\u4ee5\u878d\u5408 LoRA \u4e13\u5bb6\uff0c\u4ece\u800c\u5b9e\u73b0\u6709\u6548\u7684\u8de8\u57df\u77e5\u8bc6\u8f6c\u79fb\u3002", "result": "SAGE-reID \u663e\u8457\u51cf\u5c11\u4e86\u5185\u5b58\u6d88\u8017\u548c\u8fc7\u62df\u5408\u98ce\u9669\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u6709\u6548\u7684\u8de8\u57df\u77e5\u8bc6\u8f6c\u79fb\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "SAGE-reID \u5728\u4e09\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\uff1aMarket-1501\u3001DukeMTMC-reID \u548c MSMT17 \u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e SAGE-reID \u5728\u4fdd\u6301\u8ba1\u7b97\u6548\u7387\u7684\u540c\u65f6\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07466", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07466", "abs": "https://arxiv.org/abs/2508.07466", "authors": ["Dom Huh", "Prasant Mohapatra"], "title": "Grounding Natural Language for Multi-agent Decision-Making with Multi-agentic LLMs", "comment": null, "summary": "Language is a ubiquitous tool that is foundational to reasoning and\ncollaboration, ranging from everyday interactions to sophisticated\nproblem-solving tasks. The establishment of a common language can serve as a\npowerful asset in ensuring clear communication and understanding amongst\nagents, facilitating desired coordination and strategies. In this work, we\nextend the capabilities of large language models (LLMs) by integrating them\nwith advancements in multi-agent decision-making algorithms. We propose a\nsystematic framework for the design of multi-agentic large language models\n(LLMs), focusing on key integration practices. These include advanced prompt\nengineering techniques, the development of effective memory architectures,\nmulti-modal information processing, and alignment strategies through\nfine-tuning algorithms. We evaluate these design choices through extensive\nablation studies on classic game settings with significant underlying social\ndilemmas and game-theoretic considerations.", "AI": {"tldr": "\u672c\u7814\u7a76\u6269\u5c55\u4e86LLM\u7684\u80fd\u529b\uff0c\u5c06\u5176\u4e0e\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u7b97\u6cd5\u76f8\u7ed3\u5408\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u6280\u672f\u5b9e\u73b0\u66f4\u597d\u7684\u534f\u8c03\u548c\u7b56\u7565\u3002", "motivation": "\u7814\u7a76LLMs\u5728\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u4e2d\u7684\u5e94\u7528\uff0c\u65e8\u5728\u901a\u8fc7\u5171\u540c\u8bed\u8a00\u4fc3\u8fdb\u4ee3\u7406\u95f4\u7684\u534f\u8c03\u548c\u7b56\u7565\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7cfb\u7edf\u6846\u67b6\u6765\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53LLM\uff0c\u5305\u62ec\u5148\u8fdb\u7684\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3001\u6709\u6548\u7684\u8bb0\u5fc6\u7ed3\u6784\u3001\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u548c\u901a\u8fc7\u5fae\u8c03\u7b97\u6cd5\u8fdb\u884c\u7684\u5bf9\u9f50\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u5728\u5177\u6709\u793e\u4f1a\u56f0\u5883\u548c\u535a\u5f08\u8bba\u8003\u91cf\u7684\u7ecf\u5178\u6e38\u620f\u73af\u5883\u4e2d\u8fdb\u884c\u7684\u5927\u91cf\u6d88\u51cf\u7814\u7a76\uff0c\u8bc4\u4f30\u4e86\u8bbe\u8ba1\u9009\u62e9\u3002", "conclusion": "LLMs\u53ef\u4ee5\u901a\u8fc7\u96c6\u6210\u591a\u667a\u80fd\u4f53\u51b3\u7b56\u7b97\u6cd5\u6765\u6269\u5c55\u5176\u80fd\u529b\uff0c\u901a\u8fc7\u5148\u8fdb\u7684\u63d0\u793a\u5de5\u7a0b\u3001\u6709\u6548\u7684\u8bb0\u5fc6\u7ed3\u6784\u3001\u591a\u6a21\u6001\u4fe1\u606f\u5904\u7406\u548c\u901a\u8fc7\u5fae\u8c03\u7b97\u6cd5\u8fdb\u884c\u7684\u5bf9\u9f50\u7b56\u7565\u6765\u8bbe\u8ba1\u591a\u667a\u80fd\u4f53LLM\u3002"}}
{"id": "2508.08015", "categories": ["cond-mat.mtrl-sci", "cond-mat.str-el"], "pdf": "https://arxiv.org/pdf/2508.08015", "abs": "https://arxiv.org/abs/2508.08015", "authors": ["Alla Arakcheeva", "Priya Ranjan Baral", "Wen Hua Bi", "Christian Jandl", "Oleg Janson", "Arnaud Magrez"], "title": "Cu2OSeO3 Turns Trigonal with Structural Transformation and Implications for Skyrmions", "comment": null, "summary": "The formation and characteristics of magnetic skyrmions are strongly governed\nby the symmetry of the underlying crystal structure. In this study, we report\nthe discovery of a new trigonal polymorph of Cu2OSeO3, observed exclusively in\nnanoparticles. Electron diffraction and density functional theory calculations\nconfirm its R3m space group, sharing C3v symmetry with N\\'eel-type skyrmion\nhosts. This polymorph is likely stabilized by surface effects, suggesting that\nsize-induced structural changes may drive a transformation from Bloch-type to\nNeel-type skyrmions in Cu2OSeO3. This hypothesis is consistent with prior\nunexplained observations of Neel-type skyrmions at the surfaces of bulk\ncrystals, which may result from surface-specific structural distortions.\nOverall, these findings provide insights into the interplay between size,\nstructure, and magnetism, opening pathways for controlling skyrmionic\nproperties in nanoscale systems.", "AI": {"tldr": "Cu2OSeO3\u7eb3\u7c73\u7c92\u5b50\u4e2d\u53d1\u73b0\u4e86\u4e00\u79cd\u65b0\u7684\u4e09\u65b9\u6676\u578b\uff0c\u5177\u6709C3v\u5bf9\u79f0\u6027\uff0c\u53ef\u80fd\u9a71\u52a8\u4eceBloch\u5230N\u00e9el\u65af\u683c\u660e\u5b50\u7684\u8f6c\u53d8\uff0c\u63ed\u793a\u4e86\u5c3a\u5bf8\u3001\u7ed3\u6784\u548c\u78c1\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u78c1\u7574\u7684\u5f62\u6210\u548c\u7279\u5f81\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d7\u63a7\u4e8e\u6676\u4f53\u7ed3\u6784\u7684\u5bf9\u79f0\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22Cu2OSeO3\u7684\u6676\u4f53\u7ed3\u6784\u5bf9\u79f0\u6027\u4e0e\u5176\u78c1\u7574\uff08\u7279\u522b\u662f\u65af\u683c\u660e\u5b50\uff09\u5f62\u6210\u4e4b\u95f4\u7684\u5173\u7cfb\u3002", "method": "\u901a\u8fc7\u7535\u5b50\u884d\u5c04\u548c\u5bc6\u5ea6\u6cdb\u51fd\u7406\u8bba\u8ba1\u7b97\u786e\u8ba4\u4e86Cu2OSeO3\u7684\u65b0\u4e09\u65b9\u6676\u578b\uff0c\u5176\u7a7a\u95f4\u7fa4\u4e3aR3m\uff0c\u5177\u6709C3v\u5bf9\u79f0\u6027\u3002", "result": "\u53d1\u73b0\u4e86\u4e00\u79cd\u4ec5\u5728\u7eb3\u7c73\u7c92\u5b50\u4e2d\u89c2\u5bdf\u5230\u7684Cu2OSeO3\u65b0\u4e09\u65b9\u6676\u578b\uff0c\u5176R3m\u7a7a\u95f4\u7fa4\u548cC3v\u5bf9\u79f0\u6027\u4e0eN\u00e9el\u578b\u65af\u683c\u660e\u5b50\u5bbf\u4e3b\u76f8\u4f3c\u3002\u8be5\u6676\u578b\u53ef\u80fd\u7531\u8868\u9762\u6548\u5e94\u7a33\u5b9a\uff0c\u5e76\u53ef\u80fd\u89e3\u91ca\u5c3a\u5bf8\u8bf1\u5bfc\u7684\u4eceBloch\u578b\u5230N\u00e9el\u578b\u65af\u683c\u660e\u5b50\u7684\u8f6c\u53d8\uff0c\u4ee5\u53ca\u5757\u4f53\u6676\u4f53\u8868\u9762\u89c2\u5bdf\u5230\u7684N\u00e9el\u578b\u65af\u683c\u660e\u5b50\u73b0\u8c61\u3002", "conclusion": "\u672c\u7814\u7a76\u63ed\u793a\u4e86Cu2OSeO3\u7684\u65b0\u4e09\u65b9\u6676\u578b\uff0c\u5176R3m\u7a7a\u95f4\u7fa4\u548cC3v\u5bf9\u79f0\u6027\u4e0eN\u00e9el\u578b\u65af\u683c\u660e\u5b50\u7684\u5bbf\u4e3b\u76f8\u4f3c\u3002\u8be5\u6676\u578b\u53ef\u80fd\u7531\u8868\u9762\u6548\u5e94\u7a33\u5b9a\uff0c\u63d0\u793a\u5c3a\u5bf8\u8bf1\u5bfc\u7684\u7ed3\u6784\u53d8\u5316\u53ef\u80fd\u9a71\u52a8Cu2OSeO3\u4e2d\u4eceBloch\u578b\u5230N\u00e9el\u578b\u65af\u683c\u660e\u5b50\u7684\u8f6c\u53d8\u3002\u8be5\u5047\u8bbe\u4e0e\u5148\u524d\u5728\u5757\u4f53\u6676\u4f53\u8868\u9762\u89c2\u5bdf\u5230\u7684\u672a\u89e3\u91ca\u7684N\u00e9el\u578b\u65af\u683c\u660e\u5b50\u73b0\u8c61\u4e00\u81f4\uff0c\u8fd9\u53ef\u80fd\u6e90\u4e8e\u8868\u9762\u7279\u5f02\u6027\u7ed3\u6784\u7578\u53d8\u3002\u603b\u7684\u6765\u8bf4\uff0c\u8fd9\u4e9b\u53d1\u73b0\u4e3a\u7406\u89e3\u5c3a\u5bf8\u3001\u7ed3\u6784\u548c\u78c1\u6027\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u4e3a\u63a7\u5236\u7eb3\u7c73\u7cfb\u7edf\u4e2d\u65af\u683c\u660e\u5b50\u7684\u6027\u8d28\u5f00\u8f9f\u4e86\u9014\u5f84\u3002"}}
{"id": "2508.07648", "categories": ["cs.RO", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07648", "abs": "https://arxiv.org/abs/2508.07648", "authors": ["Mehrshad Zandigohar", "Mallesham Dasari", "Gunar Schirner"], "title": "Grasp-HGN: Grasping the Unexpected", "comment": "Paper accepted at ACM Transactions on Embedded Computing Systems", "summary": "For transradial amputees, robotic prosthetic hands promise to regain the\ncapability to perform daily living activities. To advance next-generation\nprosthetic hand control design, it is crucial to address current shortcomings\nin robustness to out of lab artifacts, and generalizability to new\nenvironments. Due to the fixed number of object to interact with in existing\ndatasets, contrasted with the virtually infinite variety of objects encountered\nin the real world, current grasp models perform poorly on unseen objects,\nnegatively affecting users' independence and quality of life.\n  To address this: (i) we define semantic projection, the ability of a model to\ngeneralize to unseen object types and show that conventional models like YOLO,\ndespite 80% training accuracy, drop to 15% on unseen objects. (ii) we propose\nGrasp-LLaVA, a Grasp Vision Language Model enabling human-like reasoning to\ninfer the suitable grasp type estimate based on the object's physical\ncharacteristics resulting in a significant 50.2% accuracy over unseen object\ntypes compared to 36.7% accuracy of an SOTA grasp estimation model.\n  Lastly, to bridge the performance-latency gap, we propose Hybrid Grasp\nNetwork (HGN), an edge-cloud deployment infrastructure enabling fast grasp\nestimation on edge and accurate cloud inference as a fail-safe, effectively\nexpanding the latency vs. accuracy Pareto. HGN with confidence calibration (DC)\nenables dynamic switching between edge and cloud models, improving semantic\nprojection accuracy by 5.6% (to 42.3%) with 3.5x speedup over the unseen object\ntypes. Over a real-world sample mix, it reaches 86% average accuracy (12.2%\ngain over edge-only), and 2.2x faster inference than Grasp-LLaVA alone.", "AI": {"tldr": "\u7814\u7a76\u901a\u8fc7Grasp-LLaVA\u548c\u6df7\u5408\u6293\u53d6\u7f51\u7edc(HGN)\u6280\u672f\uff0c\u663e\u8457\u63d0\u5347\u4e86\u673a\u5668\u4eba\u5047\u80a2\u624b\u5728\u672a\u77e5\u7269\u4f53\u6293\u53d6\u4efb\u52a1\u4e0a\u7684\u51c6\u786e\u7387\u548c\u6548\u7387\uff0c\u89e3\u51b3\u4e86\u5f53\u524d\u6280\u672f\u7684\u6cdb\u5316\u6027\u4e0d\u8db3\u548c\u901f\u5ea6\u74f6\u9888\u95ee\u9898\uff0c\u4e3a\u7528\u6237\u5e26\u6765\u4e86\u66f4\u597d\u7684\u4f53\u9a8c\u3002", "motivation": "\u5f53\u524d\u673a\u5668\u4eba\u5047\u80a2\u624b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5b58\u5728\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u6570\u636e\u96c6\u4ee5\u5916\u7684\u3001\u79cd\u7c7b\u7e41\u591a\u7684\u7269\u4f53\u65f6\uff0c\u6293\u53d6\u6a21\u578b\u7684\u8868\u73b0\u4f1a\u6025\u5267\u4e0b\u964d\uff0c\u5f71\u54cd\u7528\u6237\u72ec\u7acb\u6027\u548c\u751f\u6d3b\u8d28\u91cf\u3002\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff08\u6709\u9650\u7684\u7269\u4f53\u4ea4\u4e92\u6570\u91cf\uff09\u5bfc\u81f4\u6a21\u578b\u96be\u4ee5\u9002\u5e94\u771f\u5b9e\u4e16\u754c\u4e2d\u51e0\u4e4e\u65e0\u9650\u7684\u7269\u4f53\u591a\u6837\u6027\u3002", "method": "\u8be5\u7814\u7a76\u9996\u5148\u5b9a\u4e49\u4e86\u201c\u8bed\u4e49\u6295\u5c04\u201d\u7684\u6982\u5ff5\uff0c\u4ee5\u91cf\u5316\u6a21\u578b\u6cdb\u5316\u5230\u65b0\u7269\u4f53\u7c7b\u578b\u7684\u80fd\u529b\u3002\u7136\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrasp-LLaVA\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u7528\u4e8e\u57fa\u4e8e\u7269\u4f53\u7684\u7269\u7406\u7279\u6027\u8fdb\u884c\u6293\u53d6\u7c7b\u578b\u63a8\u65ad\u3002\u6700\u540e\uff0c\u8bbe\u8ba1\u4e86\u6df7\u5408\u6293\u53d6\u7f51\u7edc(HGN)\u7684\u57fa\u7840\u8bbe\u65bd\uff0c\u7ed3\u5408\u4e86\u8fb9\u7f18\u8ba1\u7b97\u7684\u4f4e\u5ef6\u8fdf\u548c\u4e91\u8ba1\u7b97\u7684\u9ad8\u51c6\u786e\u6027\uff0c\u5e76\u901a\u8fc7\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff08DC\uff09\u5b9e\u73b0\u4e86\u52a8\u6001\u6a21\u578b\u5207\u6362\uff0c\u4ee5\u4f18\u5316\u6027\u80fd\u548c\u5ef6\u8fdf\u3002", "result": "Grasp-LLaVA\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u578b\u4e0a\u5b9e\u73b0\u4e8650.2%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u9ad8\u4e8e\u73b0\u6709\u6280\u672f\u768436.7%\u3002\u6df7\u5408\u6293\u53d6\u7f51\u7edc(HGN)\u7ed3\u5408\u7f6e\u4fe1\u5ea6\u6821\u51c6\uff08DC\uff09\u540e\uff0c\u5728\u672a\u89c1\u8fc7\u7684\u7269\u4f53\u7c7b\u578b\u4e0a\u5c06\u8bed\u4e49\u6295\u5c04\u51c6\u786e\u7387\u63d0\u5347\u4e865.6%\uff08\u8fbe\u523042.3%\uff09\uff0c\u540c\u65f6\u5b9e\u73b0\u4e863.5\u500d\u7684\u901f\u5ea6\u63d0\u5347\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u6837\u672c\u6df7\u5408\u6d4b\u8bd5\u4e2d\uff0cHGN\u8fbe\u5230\u4e8686%\u7684\u5e73\u5747\u51c6\u786e\u7387\uff08\u6bd4\u4ec5\u4f7f\u7528\u8fb9\u7f18\u6a21\u578b\u9ad812.2%\uff09\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u662f\u5355\u72ec\u4f7f\u7528Grasp-LLaVA\u76842.2\u500d\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86Grasp-LLaVA\u548c\u6df7\u5408\u6293\u53d6\u7f51\u7edc(HGN)\u6765\u89e3\u51b3\u673a\u5668\u4eba\u5047\u80a2\u624b\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6cdb\u5316\u6027\u548c\u6548\u7387\u95ee\u9898\u3002Grasp-LLaVA\u901a\u8fc7\u5f15\u5165\u7c7b\u4f3c\u4eba\u7c7b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7269\u4f53\u7c7b\u578b\u65f6\uff0c\u51c6\u786e\u7387\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002HGN\u901a\u8fc7\u8fb9\u7f18-\u4e91\u534f\u540c\u90e8\u7f72\uff0c\u5e73\u8861\u4e86\u6293\u53d6\u4f30\u8ba1\u7684\u901f\u5ea6\u548c\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u5feb\u7684\u63a8\u7406\u901f\u5ea6\u548c\u66f4\u9ad8\u7684\u51c6\u786e\u7387\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u591a\u6837\u5316\u7684\u771f\u5b9e\u4e16\u754c\u7269\u4f53\u65f6\u8868\u73b0\u66f4\u4f73\u3002"}}
{"id": "2508.07273", "categories": ["cs.CL", "cs.AI", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07273", "abs": "https://arxiv.org/abs/2508.07273", "authors": ["Qiongqiong Wang", "Hardik B. Sailor", "Jeremy H. M. Wong", "Tianchi Liu", "Shuo Sun", "Wenyu Zhang", "Muhammad Huzaifah", "Nancy Chen", "Ai Ti Aw"], "title": "Incorporating Contextual Paralinguistic Understanding in Large Speech-Language Models", "comment": "Accepted at (ASRU 2025) 2025 IEEE Automatic Speech Recognition and\n  Understanding Workshop", "summary": "Current large speech language models (Speech-LLMs) often exhibit limitations\nin empathetic reasoning, primarily due to the absence of training datasets that\nintegrate both contextual content and paralinguistic cues. In this work, we\npropose two approaches to incorporate contextual paralinguistic information\ninto model training: (1) an explicit method that provides paralinguistic\nmetadata (e.g., emotion annotations) directly to the LLM, and (2) an implicit\nmethod that automatically generates novel training question-answer (QA) pairs\nusing both categorical and dimensional emotion annotations alongside speech\ntranscriptions. Our implicit method boosts performance (LLM-judged) by 38.41%\non a human-annotated QA benchmark, reaching 46.02% when combined with the\nexplicit approach, showing effectiveness in contextual paralinguistic\nunderstanding. We also validate the LLM judge by demonstrating its correlation\nwith classification metrics, providing support for its reliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u6cd5\u6765\u6539\u8fdb speech-LLM \u7684\u5171\u60c5\u63a8\u7406\uff0c\u9690\u5f0f\u65b9\u6cd5\u53ef\u5c06\u6027\u80fd\u63d0\u9ad8 38.41%\uff0c\u7ed3\u5408\u4e24\u8005\u53ef\u8fbe 46.02%\u3002", "motivation": "\u5f53\u524d\u7684 speech-LLM \u5728\u5171\u60c5\u63a8\u7406\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u4e3b\u8981\u662f\u56e0\u4e3a\u7f3a\u4e4f\u6574\u5408\u4e0a\u4e0b\u6587\u5185\u5bb9\u548c the paralinguistic \u63d0\u793a\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\u6765\u5c06\u4e0a\u4e0b\u6587\u7684 the paralinguistic \u4fe1\u606f\u6574\u5408\u5230\u6a21\u578b\u8bad\u7ec3\u4e2d\uff1a\u4e00\u79cd\u662f\u663e\u5f0f\u65b9\u6cd5\uff0c\u76f4\u63a5\u5c06 the paralinguistic \u5143\u6570\u636e\uff08\u4f8b\u5982\u60c5\u7eea\u6ce8\u91ca\uff09\u63d0\u4f9b\u7ed9 LLM\uff1b\u53e6\u4e00\u79cd\u662f\u9690\u5f0f\u65b9\u6cd5\uff0c\u5229\u7528\u5206\u7c7b\u548c\u7ef4\u5ea6\u60c5\u7eea\u6ce8\u91ca\u4ee5\u53ca\u8bed\u97f3\u8f6c\u5f55\u81ea\u52a8\u751f\u6210\u65b0\u7684\u8bad\u7ec3\u95ee\u9898-\u7b54\u6848\uff08QA\uff09\u5bf9\u3002", "result": "\u9690\u5f0f\u65b9\u6cd5\u5728\u4eba\u7c7b\u6ce8\u91ca\u7684 QA \u57fa\u51c6\u4e0a\u5c06 LLM \u5224\u65ad\u7684\u6027\u80fd\u63d0\u9ad8\u4e86 38.41%\uff0c\u4e0e\u663e\u5f0f\u65b9\u6cd5\u76f8\u7ed3\u5408\u65f6\u8fbe\u5230 46.02%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4e0a\u4e0b\u6587 the paralinguistic \u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002\u8fd8\u901a\u8fc7\u5c55\u793a LLM \u88c1\u5224\u4e0e\u5206\u7c7b\u6307\u6807\u7684\u76f8\u5173\u6027\u6765\u9a8c\u8bc1\u5176\u53ef\u9760\u6027\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u663e\u5f0f\u548c\u9690\u5f0f\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u4e0a\u4e0b\u6587\u548c the paralinguistic \u63d0\u793a\u6765\u6539\u8fdb speech-LLM \u7684\u5171\u60c5\u63a8\u7406\u80fd\u529b\uff0c\u5176\u4e2d\u9690\u5f0f\u65b9\u6cd5\u5728\u4eba\u7c7b\u6ce8\u91ca\u7684 QA \u57fa\u51c6\u4e0a\u5c06 LLM \u5224\u65ad\u7684\u6027\u80fd\u63d0\u9ad8\u4e86 38.41%\uff0c\u4e0e\u663e\u5f0f\u65b9\u6cd5\u76f8\u7ed3\u5408\u65f6\u8fbe\u5230 46.02%\u3002"}}
{"id": "2508.06845", "categories": ["cs.CV", "cs.CE", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.06845", "abs": "https://arxiv.org/abs/2508.06845", "authors": ["Hamidreza Samadi", "Md Manjurul Ahsan", "Shivakumar Raman"], "title": "Hybrid Machine Learning Framework for Predicting Geometric Deviations from 3D Surface Metrology", "comment": null, "summary": "This study addresses the challenge of accurately forecasting geometric\ndeviations in manufactured components using advanced 3D surface analysis.\nDespite progress in modern manufacturing, maintaining dimensional precision\nremains difficult, particularly for complex geometries. We present a\nmethodology that employs a high-resolution 3D scanner to acquire multi-angle\nsurface data from 237 components produced across different batches. The data\nwere processed through precise alignment, noise reduction, and merging\ntechniques to generate accurate 3D representations. A hybrid machine learning\nframework was developed, combining convolutional neural networks for feature\nextraction with gradient-boosted decision trees for predictive modeling. The\nproposed system achieved a prediction accuracy of 0.012 mm at a 95% confidence\nlevel, representing a 73% improvement over conventional statistical process\ncontrol methods. In addition to improved accuracy, the model revealed hidden\ncorrelations between manufacturing parameters and geometric deviations. This\napproach offers significant potential for automated quality control, predictive\nmaintenance, and design optimization in precision manufacturing, and the\nresulting dataset provides a strong foundation for future predictive modeling\nresearch.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u54083D\u626b\u63cf\u548c\u6df7\u5408\u673a\u5668\u5b66\u4e60\uff08CNN+GBDT\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u7cbe\u5bc6\u5236\u9020\u4e2d\u7ec4\u4ef6\u7684\u51e0\u4f55\u504f\u5dee\uff0c\u7cbe\u5ea6\u8fbe\u52300.012\u6beb\u7c73\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\u63d0\u9ad8\u4e8673%\u3002", "motivation": "\u5c3d\u7ba1\u73b0\u4ee3\u5236\u9020\u4e1a\u53d6\u5f97\u4e86\u8fdb\u6b65\uff0c\u4f46\u5c24\u5176\u5bf9\u4e8e\u590d\u6742\u51e0\u4f55\u5f62\u72b6\u7684\u7ec4\u4ef6\uff0c\u4fdd\u6301\u5c3a\u5bf8\u7cbe\u5ea6\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u9ad8\u5206\u8fa8\u73873D\u626b\u63cf\u4eea\u83b7\u53d6237\u4e2a\u4e0d\u540c\u6279\u6b21\u751f\u4ea7\u7684\u7ec4\u4ef6\u7684\u591a\u89d2\u5ea6\u8868\u9762\u6570\u636e\uff0c\u5e76\u7ecf\u8fc7\u7cbe\u786e\u5bf9\u9f50\u3001\u53bb\u566a\u548c\u5408\u5e76\u5904\u7406\u751f\u62103D\u6a21\u578b\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86\u7528\u4e8e\u7279\u5f81\u63d0\u53d6\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u7528\u4e8e\u9884\u6d4b\u5efa\u6a21\u7684\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\u3002", "result": "\u8be5\u6df7\u5408\u673a\u5668\u5b66\u4e60\u6846\u67b6\u5b9e\u73b0\u4e860.012\u6beb\u7c73\u7684\u9884\u6d4b\u7cbe\u5ea6\uff0895%\u7f6e\u4fe1\u6c34\u5e73\uff09\uff0c\u6bd4\u4f20\u7edf\u7edf\u8ba1\u8fc7\u7a0b\u63a7\u5236\u65b9\u6cd5\u63d0\u9ad8\u4e8673%\uff0c\u5e76\u53d1\u73b0\u4e86\u5236\u9020\u53c2\u6570\u4e0e\u51e0\u4f55\u504f\u5dee\u4e4b\u95f4\u7684\u9690\u85cf\u5173\u8054\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u548c\u68af\u5ea6\u63d0\u5347\u51b3\u7b56\u6811\uff0c\u5728\u9884\u6d4b\u51e0\u4f55\u504f\u5dee\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u6548\uff0c\u51c6\u786e\u7387\u8fbe\u52300.012\u6beb\u7c73\uff0895%\u7f6e\u4fe1\u6c34\u5e73\uff09\uff0c\u6bd4\u4f20\u7edf\u7edf\u8ba1\u8fc7\u7a0b\u63a7\u5236\u65b9\u6cd5\u63d0\u9ad8\u4e8673%\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u8fd8\u63ed\u793a\u4e86\u5236\u9020\u53c2\u6570\u4e0e\u51e0\u4f55\u504f\u5dee\u4e4b\u95f4\u7684\u9690\u85cf\u5173\u8054\uff0c\u4e3a\u7cbe\u5bc6\u5236\u9020\u4e1a\u7684\u81ea\u52a8\u5316\u8d28\u91cf\u63a7\u5236\u3001\u9884\u6d4b\u6027\u7ef4\u62a4\u548c\u8bbe\u8ba1\u4f18\u5316\u63d0\u4f9b\u4e86\u5de8\u5927\u6f5c\u529b\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u9884\u6d4b\u5efa\u6a21\u7814\u7a76\u5960\u5b9a\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2508.07468", "categories": ["cs.AI", "cs.CL", "cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.07468", "abs": "https://arxiv.org/abs/2508.07468", "authors": ["Stefan Szeider"], "title": "CP-Agent: Agentic Constraint Programming", "comment": null, "summary": "Translating natural language problem descriptions into formal constraint\nmodels remains a fundamental challenge in constraint programming, requiring\ndeep expertise in both the problem domain and modeling frameworks. Previous\napproaches to automating this translation have employed fixed workflows with\npredetermined modeling steps, failing on a significant number of benchmark\nproblems. We present a new approach using a pure agentic strategy without any\nfixed pipeline. We developed a general-purpose Python coding agent based on the\nReAct (Reason and Act) principle, utilizing a persistent IPython kernel for\nstateful code execution and iterative development. Rather than embedding\nconstraint programming logic into the agent architecture, domain-specific\nexpertise is injected solely through a carefully crafted project prompt. The\nagent combines this prompt-encoded knowledge with access to file operations and\ncode execution tools, enabling it to test hypotheses, debug failures, and\nverify solutions dynamically. Implemented in just a few hundred lines of code,\nthis architecture successfully solves all 101 problems of the CP-Bench\nconstraint programming benchmark set. The results suggest that constraint\nmodeling tasks require the combination of general coding tools and domain\nexpertise encoded in prompts, rather than specialized agent architectures or\npredefined workflows.", "AI": {"tldr": "\u4f7f\u7528 ReAct \u4ee3\u7406\u548c\u6301\u4e45 IPython \u5185\u6838\uff0c\u901a\u8fc7\u63d0\u793a\u8bcd\u6ce8\u5165\u9886\u57df\u77e5\u8bc6\uff0c\u6210\u529f\u89e3\u51b3\u4e86 CP-Bench \u4e0a\u7684\u6240\u6709\u7ea6\u675f\u7f16\u7a0b\u95ee\u9898\uff0c\u8868\u660e\u901a\u7528\u5de5\u5177\u548c\u63d0\u793a\u8bcd\u5de5\u7a0b\u6bd4\u7279\u5b9a\u67b6\u6784\u66f4\u91cd\u8981\u3002", "motivation": "\u81ea\u52a8\u5316\u5c06\u81ea\u7136\u8bed\u8a00\u95ee\u9898\u63cf\u8ff0\u8f6c\u6362\u4e3a\u6b63\u5f0f\u7ea6\u675f\u6a21\u578b\u662f\u7ea6\u675f\u7f16\u7a0b\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\uff0c\u9700\u8981\u6df1\u539a\u7684\u9886\u57df\u548c\u5efa\u6a21\u4e13\u4e1a\u77e5\u8bc6\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u5728\u5904\u7406\u5927\u91cf\u57fa\u51c6\u95ee\u9898\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u8be5\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8e ReAct\uff08Reason and Act\uff09\u539f\u5219\u7684\u901a\u7528 Python \u7f16\u7801\u4ee3\u7406\uff0c\u5e76\u5229\u7528\u6301\u4e45\u7684 IPython \u5185\u6838\u8fdb\u884c\u6709\u72b6\u6001\u7684\u4ee3\u7801\u6267\u884c\u548c\u8fed\u4ee3\u5f00\u53d1\u3002\u4ee3\u7406\u5c06\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u9879\u76ee\u63d0\u793a\u8bcd\u6ce8\u5165\uff0c\u5e76\u7ed3\u5408\u6587\u4ef6\u64cd\u4f5c\u548c\u4ee3\u7801\u6267\u884c\u5de5\u5177\uff0c\u4ee5\u52a8\u6001\u5730\u6d4b\u8bd5\u5047\u8bbe\u3001\u8c03\u8bd5\u6545\u969c\u548c\u9a8c\u8bc1\u89e3\u51b3\u65b9\u6848\u3002", "result": "\u8be5\u65b9\u6cd5\u6210\u529f\u89e3\u51b3\u4e86 CP-Bench \u7ea6\u675f\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e2d\u7684\u5168\u90e8 101 \u4e2a\u95ee\u9898\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u7ed3\u5408\u4e86\u901a\u7528\u7f16\u7801\u5de5\u5177\u548c\u901a\u8fc7\u63d0\u793a\u8bcd\u7f16\u7801\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\uff0c\u800c\u4e0d\u662f\u4e13\u95e8\u7684\u4ee3\u7406\u67b6\u6784\u6216\u9884\u5b9a\u4e49\u7684\u6a21\u578b\u3002\u8be5\u65b9\u6cd5\u5728 CP-Bench \u7ea6\u675f\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u96c6\u4e2d\u7684\u5168\u90e8 101 \u4e2a\u95ee\u9898\u4e0a\u5747\u53d6\u5f97\u4e86\u6210\u529f\u3002"}}
{"id": "2508.06986", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.06986", "abs": "https://arxiv.org/abs/2508.06986", "authors": ["Chonghua Han", "Yuan Yuan", "Yukun Liu", "Jingtao Ding", "Jie Feng", "Yong Li"], "title": "UniMove: A Unified Model for Multi-city Human Mobility Prediction", "comment": "Accepted by SIGSPATIAL 2025", "summary": "Human mobility prediction is vital for urban planning, transportation\noptimization, and personalized services. However, the inherent randomness,\nnon-uniform time intervals, and complex patterns of human mobility, compounded\nby the heterogeneity introduced by varying city structures, infrastructure, and\npopulation densities, present significant challenges in modeling. Existing\nsolutions often require training separate models for each city due to distinct\nspatial representations and geographic coverage. In this paper, we propose\nUniMove, a unified model for multi-city human mobility prediction, addressing\ntwo challenges: (1) constructing universal spatial representations for\neffective token sharing across cities, and (2) modeling heterogeneous mobility\npatterns from varying city characteristics. We propose a trajectory-location\ndual-tower architecture, with a location tower for universal spatial encoding\nand a trajectory tower for sequential mobility modeling. We also design MoE\nTransformer blocks to adaptively select experts to handle diverse movement\npatterns. Extensive experiments across multiple datasets from diverse cities\ndemonstrate that UniMove truly embodies the essence of a unified model. By\nenabling joint training on multi-city data with mutual data enhancement, it\nsignificantly improves mobility prediction accuracy by over 10.2\\%. UniMove\nrepresents a key advancement toward realizing a true foundational model with a\nunified architecture for human mobility. We release the implementation at\nhttps://github.com/tsinghua-fib-lab/UniMove/.", "AI": {"tldr": "UniMove \u662f\u4e00\u79cd\u7edf\u4e00\u7684\u591a\u57ce\u5e02\u4eba\u7c7b\u51fa\u884c\u9884\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u53cc\u5854\u67b6\u6784\u548c MoE Transformer \u5757\u89e3\u51b3\u4e86\u57ce\u5e02\u5f02\u8d28\u6027\u548c\u590d\u6742\u51fa\u884c\u6a21\u5f0f\u7684\u6311\u6218\uff0c\u5c06\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 10.2%\u3002", "motivation": "\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6848\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u4e2a\u57ce\u5e02\u8bad\u7ec3\u5355\u72ec\u7684\u6a21\u578b\uff0c\u56e0\u4e3a\u57ce\u5e02\u5177\u6709\u4e0d\u540c\u7684\u7a7a\u95f4\u8868\u793a\u548c\u5730\u7406\u8986\u76d6\u8303\u56f4\u3002\u7136\u800c\uff0c\u4eba\u7c7b\u51fa\u884c\u5177\u6709\u5185\u5728\u7684\u968f\u673a\u6027\u3001\u4e0d\u5747\u5300\u7684\u65f6\u95f4\u95f4\u9694\u548c\u590d\u6742\u7684\u6a21\u5f0f\uff0c\u5e76\u4e14\u7531\u4e8e\u57ce\u5e02\u7ed3\u6784\u3001\u57fa\u7840\u8bbe\u65bd\u548c\u4eba\u53e3\u5bc6\u5ea6\u7684\u53d8\u5316\u800c\u5e26\u6765\u5f02\u8d28\u6027\uff0c\u8fd9\u7ed9\u5efa\u6a21\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f68\u8ff9-\u4f4d\u7f6e\u53cc\u5854\u67b6\u6784\uff0c\u5176\u4e2d\u4f4d\u7f6e\u5854\u7528\u4e8e\u901a\u7528\u7a7a\u95f4\u7f16\u7801\uff0c\u8f68\u8ff9\u5854\u7528\u4e8e\u5e8f\u5217\u79fb\u52a8\u5efa\u6a21\u3002\u8fd8\u8bbe\u8ba1\u4e86 MoE Transformer \u5757\u6765\u9002\u5e94\u6027\u5730\u9009\u62e9\u4e13\u5bb6\u4ee5\u5904\u7406\u5404\u79cd\u79fb\u52a8\u6a21\u5f0f\u3002", "result": "\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0cUniMove \u5728\u591a\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u79fb\u52a8\u9884\u6d4b\u51c6\u786e\u6027\u63d0\u9ad8\u4e86 10.2%\u3002", "conclusion": "UniMove \u901a\u8fc7\u5728\u591a\u57ce\u5e02\u6570\u636e\u4e0a\u8fdb\u884c\u8054\u5408\u8bad\u7ec3\u548c\u4e92\u6570\u636e\u589e\u5f3a\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u51fa\u884c\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u771f\u6b63\u7684\u7edf\u4e00\u6a21\u578b\uff0c\u4ee3\u8868\u4e86\u8fc8\u5411\u5b9e\u73b0\u4eba\u7c7b\u51fa\u884c\u7edf\u4e00\u67b6\u6784\u7684\u771f\u6b63\u57fa\u7840\u6a21\u578b\u7684\u5173\u952e\u8fdb\u5c55\u3002"}}
{"id": "2508.08112", "categories": ["cond-mat.mtrl-sci", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.08112", "abs": "https://arxiv.org/abs/2508.08112", "authors": ["Tzu-chen Liu", "Steven B. Torrisi", "Chris Wolverton"], "title": "Short-Range Order and Li$_x$TM$_{4-x}$ Probability Maps for Disordered Rocksalt Cathodes", "comment": "37 pages, 12 figures. This work was previously included in the\n  dissertation of the first author", "summary": "Short-range order (SRO) in the cation-disordered state is a controlling\nfactor influencing the probability of finding Li$_{4}$ tetrahedron clusters in\ndisordered rocksalt (DRX) cathode materials. However, the prevalent Li$_4$\nprobability below the random limit across reported DRX compositions has not\nbeen systematically investigated, active strategies to surpass the random limit\nof Li$_4$ probability are lacking, and the fundamental ordering behavior on the\nface-centered cubic (FCC) lattice remains insufficiently explored. This\nresearch quantitatively examines pair SRO parameters and Li$_x$TM$_{4-x}$\nprobabilities via exhaustive Monte Carlo mapping across a simplified subset of\nthe parameter space. The results indicate that, in the disordered state, the\nLi$_4$ probability is governed by the nearest neighbor (NN) pair-wise SRO\nparameter, and that these quantities do not necessarily represent a simple\nattenuation of their corresponding low-temperature long-range order,\nparticularly for the important cases of Layered and Spinel-like orderings.\nStrategies are proposed to mitigate or even reverse the lithium and transition\nmetals mixing tendency of NN pair SRO to achieve Li$_4$ probabilities that\nexceed the random limit. This study advances the fundamental thermodynamic\nunderstanding of ordering behaviors, which can be generalized to any FCC\nsystem.", "AI": {"tldr": "\u7814\u7a76\u4e86\u65e0\u5e8f\u5ca9\u76d0\uff08DRX\uff09\u9634\u6781\u6750\u6599\u4e2d\u7684\u77ed\u7a0b\u6709\u5e8f\uff08SRO\uff09\u5bf9 Li$_{4}$ \u56db\u9762\u4f53\u7c07\u6982\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u8d85\u8d8a\u968f\u673a\u6781\u9650\u7684\u7b56\u7565\u3002", "motivation": "\u7814\u7a76\u4e86\u9633\u79bb\u5b50\u65e0\u5e8f\u72b6\u6001\u4e0b\u77ed\u7a0b\u6709\u5e8f\uff08SRO\uff09\u5bf9\u65e0\u5e8f\u5ca9\u76d0\uff08DRX\uff09\u9634\u6781\u6750\u6599\u4e2d Li$_{4}$ \u56db\u9762\u4f53\u7c07\u6982\u7387\u7684\u5f71\u54cd\uff0c\u5e76\u63a2\u7d22\u4e86\u8d85\u8d8a\u968f\u673a\u6781\u9650\u7684\u7b56\u7565\u4ee5\u53ca FCC \u6676\u683c\u4e0a\u7684\u57fa\u672c\u6392\u5e8f\u884c\u4e3a\u3002", "method": "\u901a\u8fc7\u8be6\u5c3d\u7684\u8499\u7279\u5361\u6d1b\u6a21\u62df\uff0c\u91cf\u5316\u4e86\u914d\u5bf9\u77ed\u7a0b\u6709\u5e8f\u53c2\u6570\u548c Li$_{x}$TM$_{4-x}$ \u6982\u7387\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0c\u5728\u65e0\u5e8f\u72b6\u6001\u4e0b\uff0cLi$_{4}$ \u6982\u7387\u53d7\u6700\u8fd1\u90bb\uff08NN\uff09\u914d\u5bf9 SRO \u53c2\u6570\u63a7\u5236\uff0c\u5e76\u4e14\u8fd9\u4e9b\u91cf\u5e76\u4e0d\u4e00\u5b9a\u4ee3\u8868\u5176\u76f8\u5e94\u4f4e\u6e29\u957f\u7a0b\u6709\u5e8f\u7684\u7b80\u5355\u8870\u51cf\uff0c\u5c24\u5176\u662f\u5728\u5c42\u72b6\u548c\u5c16\u6676\u77f3\u7c7b\u6392\u5e8f\u7684\u91cd\u8981\u60c5\u51b5\u4e0b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7b56\u7565\u53ef\u7528\u4e8e\u514b\u670d\u9502\u548c\u8fc7\u6e21\u91d1\u5c5e\u7684\u6df7\u5408\u8d8b\u52bf\uff0c\u4ee5\u5b9e\u73b0\u8d85\u51fa\u968f\u673a\u6781\u9650\u7684 Li$_{4}$ \u6982\u7387\uff0c\u4e3a\u5bcc\u9502\u9634\u6781\u6750\u6599\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u65b0\u7684\u65b9\u5411\u3002"}}
{"id": "2508.07650", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07650", "abs": "https://arxiv.org/abs/2508.07650", "authors": ["Helong Huang", "Min Cen", "Kai Tan", "Xingyue Quan", "Guowei Huang", "Hong Zhang"], "title": "GraphCoT-VLA: A 3D Spatial-Aware Reasoning Vision-Language-Action Model for Robotic Manipulation with Ambiguous Instructions", "comment": "10 pages, 6 figures", "summary": "Vision-language-action models have emerged as a crucial paradigm in robotic\nmanipulation. However, existing VLA models exhibit notable limitations in\nhandling ambiguous language instructions and unknown environmental states.\nFurthermore, their perception is largely constrained to static two-dimensional\nobservations, lacking the capability to model three-dimensional interactions\nbetween the robot and its environment. To address these challenges, this paper\nproposes GraphCoT-VLA, an efficient end-to-end model. To enhance the model's\nability to interpret ambiguous instructions and improve task planning, we\ndesign a structured Chain-of-Thought reasoning module that integrates\nhigh-level task understanding and planning, failed task feedback, and low-level\nimaginative reasoning about future object positions and robot actions.\nAdditionally, we construct a real-time updatable 3D Pose-Object graph, which\ncaptures the spatial configuration of robot joints and the topological\nrelationships between objects in 3D space, enabling the model to better\nunderstand and manipulate their interactions. We further integrates a dropout\nhybrid reasoning strategy to achieve efficient control outputs. Experimental\nresults across multiple real-world robotic tasks demonstrate that GraphCoT-VLA\nsignificantly outperforms existing methods in terms of task success rate and\nresponse speed, exhibiting strong generalization and robustness in open\nenvironments and under uncertain instructions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 GraphCoT-VLA \u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u601d\u7ef4\u94fe\u63a8\u7406\u548c 3D \u7a7a\u95f4\u56fe\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u5904\u7406\u6a21\u7cca\u6307\u4ee4\u548c\u672a\u77e5\u73af\u5883\u7684\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\uff08VLA\uff09\u6a21\u578b\u5728\u5904\u7406\u6a21\u7cca\u8bed\u8a00\u6307\u4ee4\u548c\u672a\u77e5\u73af\u5883\u72b6\u6001\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u5b83\u4eec\u7684\u611f\u77e5\u80fd\u529b\u4e3b\u8981\u5c40\u9650\u4e8e\u9759\u6001\u7684\u4e8c\u7ef4\u89c2\u5bdf\uff0c\u7f3a\u4e4f\u5bf9\u673a\u5668\u4eba\u4e0e\u5176\u73af\u5883\u4e4b\u95f4\u4e09\u7ef4\u4ea4\u4e92\u5efa\u6a21\u7684\u80fd\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86 GraphCoT-VLA\uff0c\u4e00\u4e2a\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u6a21\u578b\u3002\u5b83\u5305\u542b\u4e00\u4e2a\u7ed3\u6784\u5316\u7684\u601d\u7ef4\u94fe\uff08Chain-of-Thought\uff09\u63a8\u7406\u6a21\u5757\uff0c\u7528\u4e8e\u89e3\u91ca\u6a21\u7cca\u6307\u4ee4\u548c\u6539\u8fdb\u4efb\u52a1\u89c4\u5212\uff0c\u8be5\u6a21\u5757\u6574\u5408\u4e86\u9ad8\u7ea7\u4efb\u52a1\u7406\u89e3\u3001\u89c4\u5212\u3001\u5931\u8d25\u4efb\u52a1\u53cd\u9988\u4ee5\u53ca\u5bf9\u672a\u6765\u7269\u4f53\u4f4d\u7f6e\u548c\u673a\u5668\u4eba\u52a8\u4f5c\u7684\u4f4e\u7ea7\u63a8\u7406\u3002\u6b64\u5916\uff0c\u8fd8\u6784\u5efa\u4e86\u4e00\u4e2a\u5b9e\u65f6\u53ef\u66f4\u65b0\u7684 3D \u59ff\u6001-\u7269\u4f53\u56fe\uff0c\u7528\u4e8e\u6355\u6349\u673a\u5668\u4eba\u5173\u8282\u7684\u7a7a\u95f4\u914d\u7f6e\u548c\u7269\u4f53\u5728 3D \u7a7a\u95f4\u4e2d\u7684\u62d3\u6251\u5173\u7cfb\uff0c\u4ece\u800c\u66f4\u597d\u5730\u7406\u89e3\u548c\u64cd\u4f5c\u5b83\u4eec\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002\u6a21\u578b\u8fd8\u96c6\u6210\u4e86 Dropout \u6df7\u5408\u63a8\u7406\u7b56\u7565\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u63a7\u5236\u8f93\u51fa\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cGraphCoT-VLA \u5728\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\uff0c\u5728\u4efb\u52a1\u6210\u529f\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u5f00\u653e\u73af\u5883\u548c\u4e0d\u786e\u5b9a\u6307\u4ee4\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "GraphCoT-VLA \u5728\u771f\u5b9e\u4e16\u754c\u7684\u673a\u5668\u4eba\u4efb\u52a1\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u6210\u529f\u7387\u548c\u54cd\u5e94\u901f\u5ea6\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5728\u5f00\u653e\u73af\u5883\u548c\u4e0d\u786e\u5b9a\u6307\u4ee4\u4e0b\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.07913", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07913", "abs": "https://arxiv.org/abs/2508.07913", "authors": ["Shintaro Sato", "Yasunari Suzuki"], "title": "Scheduling of syndrome measurements with a few ancillary qubits", "comment": "18 pages, 8 figures", "summary": "Quantum error-correcting codes are a vital technology for demonstrating\nreliable quantum computation. They require data qubits for encoding quantum\ninformation and ancillary qubits for taking error syndromes necessary for error\ncorrection. The need for a large number of ancillary qubits is an overhead\nspecific to quantum computing, and it prevents the scaling of quantum computers\nto a useful size. In this work, we propose a framework for generating efficient\nsyndrome measurement circuits with a few ancillary qubits in CSS codes and\nprovide a method to minimize the total number of physical qubits in general\nsettings. We demonstrated our proposal by applying it to surface codes, and we\ngenerated syndrome measurement circuits under several constraints of total\nqubit count. As a result, we find that balanced data and ancillary qubit counts\nachieve the lower logical error rates under a fixed total number of physical\nqubits. This result indicates that using fewer ancillary qubits than the number\nof stabilizers can be effective for reducing logical error rates in a practical\nnoise model.", "AI": {"tldr": "\u901a\u8fc7\u4f7f\u7528\u66f4\u5c11\u7684\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\uff0c\u53ef\u4ee5\u51cf\u5c11\u903b\u8f91\u9519\u8bef\u7387\u3002", "motivation": "\u91cf\u5b50\u7ea0\u9519\u7801\u9700\u8981\u6570\u636e\u91cf\u5b50\u6bd4\u7279\u6765\u7f16\u7801\u91cf\u5b50\u4fe1\u606f\u548c\u7528\u4e8e\u83b7\u53d6\u9519\u8bef\u6821\u6b63\u7684\u9519\u8bef\u7efc\u5408\u7684\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u3002\u5927\u91cf\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u7684\u9700\u6c42\u662f\u91cf\u5b50\u8ba1\u7b97\u7279\u6709\u7684\u5f00\u9500\uff0c\u5b83\u963b\u788d\u4e86\u91cf\u5b50\u8ba1\u7b97\u5411\u6709\u7528\u89c4\u6a21\u7684\u6269\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u751f\u6210\u9ad8\u6548\u7684 CSS \u7801\u7684\u7efc\u5408\u6d4b\u91cf\u7535\u8def\u7684\u6846\u67b6\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u901a\u7528\u8bbe\u7f6e\u4e2d\u6700\u5c0f\u5316\u7269\u7406\u91cf\u5b50\u6bd4\u7279\u603b\u6570\u7684\u65b9\u6cd5\u3002", "result": "\u5c06\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5e94\u7528\u4e8e\u8868\u9762\u7801\uff0c\u5e76\u5728\u603b\u91cf\u5b50\u6bd4\u7279\u6570\u7684\u7ea6\u675f\u4e0b\u751f\u6210\u4e86\u7efc\u5408\u6d4b\u91cf\u7535\u8def\u3002", "conclusion": "\u4e0e\u91cf\u5b50\u6bd4\u7279\u603b\u6570\u56fa\u5b9a\u7684\u60c5\u51b5\u76f8\u6bd4\uff0c\u5e73\u8861\u7684\u6570\u636e\u548c\u8f85\u52a9\u91cf\u5b50\u6bd4\u7279\u8ba1\u6570\u53ef\u4ee5\u5728\u8f83\u4f4e\u7684\u903b\u8f91\u9519\u8bef\u7387\u4e0b\u5b9e\u73b0\u3002"}}
{"id": "2508.07279", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07279", "abs": "https://arxiv.org/abs/2508.07279", "authors": ["Vasudha Varadarajan", "Hui Xu", "Rebecca Astrid Boehme", "Mariam Marlan Mirstrom", "Sverker Sikstrom", "H. Andrew Schwartz"], "title": "MAQuA: Adaptive Question-Asking for Multidimensional Mental Health Screening using Item Response Theory", "comment": null, "summary": "Recent advances in large language models (LLMs) offer new opportunities for\nscalable, interactive mental health assessment, but excessive querying by LLMs\nburdens users and is inefficient for real-world screening across\ntransdiagnostic symptom profiles. We introduce MAQuA, an adaptive\nquestion-asking framework for simultaneous, multidimensional mental health\nscreening. Combining multi-outcome modeling on language responses with item\nresponse theory (IRT) and factor analysis, MAQuA selects the questions with\nmost informative responses across multiple dimensions at each turn to optimize\ndiagnostic information, improving accuracy and potentially reducing response\nburden. Empirical results on a novel dataset reveal that MAQuA reduces the\nnumber of assessment questions required for score stabilization by 50-87%\ncompared to random ordering (e.g., achieving stable depression scores with 71%\nfewer questions and eating disorder scores with 85% fewer questions). MAQuA\ndemonstrates robust performance across both internalizing (depression, anxiety)\nand externalizing (substance use, eating disorder) domains, with early stopping\nstrategies further reducing patient time and burden. These findings position\nMAQuA as a powerful and efficient tool for scalable, nuanced, and interactive\nmental health screening, advancing the integration of LLM-based agents into\nreal-world clinical workflows.", "AI": {"tldr": "MAQuA is an adaptive questioning framework for mental health screening that uses IRT and factor analysis to select the most informative questions, reducing assessment time and burden by 50-87% compared to random ordering.", "motivation": "Excessive querying by LLMs burdens users and is inefficient for real-world screening across transdiagnostic symptom profiles.", "method": "Combining multi-outcome modeling on language responses with item response theory (IRT) and factor analysis, MAQuA selects the questions with most informative responses across multiple dimensions at each turn to optimize diagnostic information.", "result": "MAQuA reduces the number of assessment questions required for score stabilization by 50-87% compared to random ordering, achieving stable depression scores with 71% fewer questions and eating disorder scores with 85% fewer questions. It demonstrates robust performance across internalizing and externalizing domains, with early stopping strategies further reducing patient time and burden.", "conclusion": "MAQuA is a powerful and efficient tool for scalable, nuanced, and interactive mental health screening, advancing the integration of LLM-based agents into real-world clinical workflows."}}
{"id": "2508.06853", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06853", "abs": "https://arxiv.org/abs/2508.06853", "authors": ["L. D. M. S. Sai Teja", "Ashok Urlana", "Pruthwik Mishra"], "title": "AGIC: Attention-Guided Image Captioning to Improve Caption Relevance", "comment": "10 pages, 5 Figures", "summary": "Despite significant progress in image captioning, generating accurate and\ndescriptive captions remains a long-standing challenge. In this study, we\npropose Attention-Guided Image Captioning (AGIC), which amplifies salient\nvisual regions directly in the feature space to guide caption generation. We\nfurther introduce a hybrid decoding strategy that combines deterministic and\nprobabilistic sampling to balance fluency and diversity. To evaluate AGIC, we\nconduct extensive experiments on the Flickr8k and Flickr30k datasets. The\nresults show that AGIC matches or surpasses several state-of-the-art models\nwhile achieving faster inference. Moreover, AGIC demonstrates strong\nperformance across multiple evaluation metrics, offering a scalable and\ninterpretable solution for image captioning.", "AI": {"tldr": "AGIC improves image captioning by focusing on salient visual regions and using a hybrid decoding strategy for better fluency and diversity, outperforming existing models with faster inference.", "motivation": "Generating accurate and descriptive captions remains a challenge in image captioning despite significant progress.", "method": "The study proposes Attention-Guided Image Captioning (AGIC), which amplifies salient visual regions directly in the feature space to guide caption generation. It also introduces a hybrid decoding strategy that combines deterministic and probabilistic sampling to balance fluency and diversity.", "result": "AGIC matches or surpasses several state-of-the-art models while achieving faster inference. It also demonstrates strong performance across multiple evaluation metrics.", "conclusion": "AGIC matches or surpasses several state-of-the-art models while achieving faster inference. Moreover, AGIC demonstrates strong performance across multiple evaluation metrics, offering a scalable and interpretable solution for image captioning."}}
{"id": "2508.07485", "categories": ["cs.AI", "cs.CL", "cs.CY", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07485", "abs": "https://arxiv.org/abs/2508.07485", "authors": ["Alexander Duffy", "Samuel J Paech", "Ishana Shastri", "Elizabeth Karpinski", "Baptiste Alloui-Cros", "Tyler Marques", "Matthew Lyle Olson"], "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language Model on Full-Press Diplomacy", "comment": null, "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.", "AI": {"tldr": "\u9996\u6b21\u63d0\u51fa\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u8bc4\u4f30LLM\u73a9\u201c\u5916\u4ea4\u201d\u6e38\u620f\u7684\u6846\u67b6\uff0c\u4f18\u5316\u4e86\u72b6\u6001\u8868\u793a\uff0c\u4f7f24B\u6a21\u578b\u80fd\u73a9\u5b8c\u6574\u6e38\u620f\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u5206\u6790\u5de5\u5177\u548c\u5173\u952e\u72b6\u6001\u5206\u6790\u534f\u8bae\uff0c\u8868\u660e\u5927\u6a21\u578b\u8868\u73b0\u66f4\u597d\u4f46\u5c0f\u6a21\u578b\u4e5f\u591f\u7528\uff0c\u5e76\u666e\u53ca\u4e86LLM\u7b56\u7565\u63a8\u7406\u8bc4\u4f30\u3002", "motivation": "\u201c\u5916\u4ea4\u201d\u6e38\u620f\u7684\u9ad8\u590d\u6742\u6027\u548c\u4fe1\u606f\u5bc6\u5ea6\uff0c\u4ee5\u53ca\u6bd4\u8d5b\u7684\u9ad8\u53d8\u5f02\u6027\uff0c\u4f7f\u5f97\u4e4b\u524d\u7684\u7814\u7a76\u96be\u4ee5\u5728\u65e0\u9700\u5fae\u8c03\u6216\u4e13\u95e8\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8fdb\u884c\uff0c\u9650\u5236\u4e86\u5bf9LLM\u7b56\u7565\u63a8\u7406\u80fd\u529b\u7684\u7814\u7a76\u3002\u672c\u7814\u7a76\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u6311\u6218\uff0c\u4f7f\u4efb\u4f55\u672c\u5730LLM\u90fd\u80fd\u53c2\u4e0e\u6e38\u620f\uff0c\u4ece\u800c\u666e\u53ca\u8bc4\u4f30\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u8bc4\u4f30\u6846\u67b6\uff0c\u4f18\u5316\u4e86\u201c\u5916\u4ea4\u201d\u6e38\u620f\u7684\u6587\u672c\u72b6\u6001\u8868\u793a\uff0c\u4f7f\u5f9724B\u53c2\u6570\u6a21\u578b\u65e0\u9700\u5fae\u8c03\u5373\u53ef\u5b8c\u6210\u6e38\u620f\u3002\u5f00\u53d1\u4e86\u7528\u4e8e\u5047\u8bbe\u68c0\u9a8c\u548c\u7edf\u8ba1\u5206\u6790\u7684\u5de5\u5177\uff0c\u5e76\u5f15\u5165\u4e86\u201c\u5173\u952e\u72b6\u6001\u5206\u6790\u201d\u534f\u8bae\u3002", "result": "\u7814\u7a76\u6210\u529f\u5b9e\u73b0\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u5141\u8bb8LLM\u5728\u65e0\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u73a9\u201c\u5916\u4ea4\u201d\u6e38\u620f\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u66f4\u5927\u53c2\u6570\u7684LLM\u8868\u73b0\u66f4\u4f73\uff0c\u4f46\u8f83\u5c0f\u6a21\u578b\u4e5f\u8868\u73b0\u5c1a\u53ef\u3002\u8be5\u6846\u67b6\u4e3a\u7814\u7a76LLM\u7684\u7b56\u7565\u63a8\u7406\u80fd\u529b\u63d0\u4f9b\u4e86\u4fbf\u5229\uff0c\u5e76\u63ed\u793a\u4e86\u8fd9\u4e9b\u80fd\u529b\u5728LLM\u4e2d\u7684\u81ea\u7136\u6d8c\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u4e2a\u8bc4\u4f30\u6846\u67b6\uff0c\u5141\u8bb8\u4efb\u4f55\u73b0\u6210\u7684\u3001\u672c\u5730\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u65e0\u9700\u5fae\u8c03\u6216\u4e13\u95e8\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\uff0c\u73a9\u5b8c\u6574\u7684\u201c\u5916\u4ea4\u201d\uff08Diplomacy\uff09\u6e38\u620f\u3002\u7814\u7a76\u901a\u8fc7\u6570\u636e\u9a71\u52a8\u7684\u8fed\u4ee3\u4f18\u5316\u4e86\u6587\u672c\u6e38\u620f\u72b6\u6001\u8868\u793a\uff0c\u4f7f\u5f97\u4e00\u4e2a24B\u53c2\u6570\u7684\u6a21\u578b\u80fd\u591f\u5728\u65e0\u5fae\u8c03\u7684\u60c5\u51b5\u4e0b\u53ef\u9760\u5730\u5b8c\u6210\u6e38\u620f\u3002\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u7528\u4e8e\u5047\u8bbe\u68c0\u9a8c\u548c\u7edf\u8ba1\u5206\u6790\u7684\u5de5\u5177\uff0c\u5e76\u8fdb\u884c\u4e86\u5173\u4e8e\u8bf4\u670d\u529b\u3001\u653b\u51fb\u6027\u6e38\u620f\u98ce\u683c\u4ee5\u53ca\u4e0d\u540c\u6a21\u578b\u6027\u80fd\u7684\u6848\u4f8b\u7814\u7a76\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u66f4\u5927\u53c2\u6570\u7684\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u4f46\u8f83\u5c0f\u6a21\u578b\u4e5f\u80fd\u5145\u5206\u53d1\u6325\u4f5c\u7528\u3002\u6b64\u5916\uff0c\u7814\u7a76\u5f15\u5165\u4e86\u201c\u5173\u952e\u72b6\u6001\u5206\u6790\u201d\uff08Critical State Analysis\uff09\u534f\u8bae\uff0c\u7528\u4e8e\u5feb\u901f\u8fed\u4ee3\u548c\u6df1\u5165\u5206\u6790\u6e38\u620f\u4e2d\u7684\u5173\u952e\u65f6\u523b\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6d88\u9664\u5fae\u8c03\u9700\u6c42\uff0c\u666e\u53ca\u4e86\u5bf9LLM\u7b56\u7565\u63a8\u7406\u80fd\u529b\u7684\u8bc4\u4f30\uff0c\u5e76\u63ed\u793a\u4e86\u8fd9\u4e9b\u80fd\u529b\u5982\u4f55\u5728\u5e7f\u6cdb\u4f7f\u7528\u7684LLM\u4e2d\u81ea\u7136\u6d8c\u73b0\u3002"}}
{"id": "2508.06991", "categories": ["cs.LG", "68T01, 68T05", "I.2.6; I.2.7; I.5.1"], "pdf": "https://arxiv.org/pdf/2508.06991", "abs": "https://arxiv.org/abs/2508.06991", "authors": ["Vojtech Halenka", "Ole-Christoffer Granmo", "Lei Jiao", "Per-Arne Andersen"], "title": "A Comparative Study of Feature Selection in Tsetlin Machines", "comment": "submitted to SGAI-2025: The 45th SGAI International Conference on\n  Innovative Techniques and Applications of Artificial Intelligence", "summary": "Feature Selection (FS) is crucial for improving model interpretability,\nreducing complexity, and sometimes for enhancing accuracy. The recently\nintroduced Tsetlin machine (TM) offers interpretable clause-based learning, but\nlacks established tools for estimating feature importance. In this paper, we\nadapt and evaluate a range of FS techniques for TMs, including classical filter\nand embedded methods as well as post-hoc explanation methods originally\ndeveloped for neural networks (e.g., SHAP and LIME) and a novel family of\nembedded scorers derived from TM clause weights and Tsetlin automaton (TA)\nstates. We benchmark all methods across 12 datasets, using evaluation\nprotocols, like Remove and Retrain (ROAR) strategy and Remove and Debias\n(ROAD), to assess causal impact. Our results show that TM-internal scorers not\nonly perform competitively but also exploit the interpretability of clauses to\nreveal interacting feature patterns. Simpler TM-specific scorers achieve\nsimilar accuracy retention at a fraction of the computational cost. This study\nestablishes the first comprehensive baseline for FS in TM and paves the way for\ndeveloping specialized TM-specific interpretability techniques.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u4e3aTsetlin\u673a\uff08TM\uff09\u5efa\u7acb\u4e86\u7279\u5f81\u9009\u62e9\uff08FS\uff09\u7684\u5168\u9762\u57fa\u7ebf\uff0c\u5e76\u8bc4\u4f30\u4e86\u591a\u79cdFS\u6280\u672f\uff0c\u5305\u62ecTM\u5185\u90e8\u8bc4\u5206\u5668\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0cTM\u5185\u90e8\u8bc4\u5206\u5668\u8868\u73b0\u5177\u6709\u7ade\u4e89\u529b\uff0c\u5e76\u80fd\u63ed\u793a\u7279\u5f81\u4ea4\u4e92\u6a21\u5f0f\uff0c\u540c\u65f6\u66f4\u7b80\u5355\u7684\u8bc4\u5206\u5668\u6210\u672c\u6548\u76ca\u66f4\u9ad8\u3002", "motivation": "Tsetlin\u673a\uff08TM\uff09\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u57fa\u4e8e\u5b50\u53e5\u7684\u5b66\u4e60\uff0c\u4f46\u7f3a\u4e4f\u7528\u4e8e\u4f30\u8ba1\u7279\u5f81\u91cd\u8981\u6027\u7684\u65e2\u5b9a\u5de5\u5177\u3002\u7279\u5f81\u9009\u62e9\uff08FS\uff09\u5bf9\u4e8e\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3001\u964d\u4f4e\u590d\u6742\u6027\u4ee5\u53ca\u6709\u65f6\u63d0\u9ad8\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u6539\u7f16\u5e76\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u7528\u4e8eTM\u7684FS\u6280\u672f\uff0c\u5305\u62ec\u7ecf\u5178\u7684\u8fc7\u6ee4\u65b9\u6cd5\u3001\u5d4c\u5165\u5f0f\u65b9\u6cd5\u4ee5\u53ca\u6700\u521d\u4e3a\u795e\u7ecf\u7f51\u7edc\u5f00\u53d1\u7684\u540e\u8bbe\u89e3\u91ca\u65b9\u6cd5\uff08\u4f8b\u5982SHAP\u548cLIME\uff09\uff0c\u8fd8\u6709\u4e00\u79cd\u6e90\u81eaTM\u5b50\u53e5\u6743\u91cd\u548cTsetlin\u81ea\u52a8\u673a\uff08TA\uff09\u72b6\u6001\u7684\u65b0\u578b\u5d4c\u5165\u5f0f\u8bc4\u5206\u5668\u5bb6\u65cf\u3002\u6211\u4eec\u4f7f\u7528\u8bf8\u5982\u79fb\u9664\u548c\u518d\u8bad\u7ec3\uff08ROAR\uff09\u7b56\u7565\u4ee5\u53ca\u79fb\u9664\u548c\u53bb\u504f\uff08ROAD\uff09\u7b49\u8bc4\u4f30\u534f\u8bae\uff0c\u572812\u4e2a\u6570\u636e\u96c6\u4e0a\u5bf9\u6240\u6709\u65b9\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u56e0\u679c\u5f71\u54cd\u3002", "result": "TM\u5185\u90e8\u8bc4\u5206\u5668\u4e0d\u4ec5\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u800c\u4e14\u5229\u7528\u5b50\u53e5\u7684\u53ef\u89e3\u91ca\u6027\u63ed\u793a\u4e86\u76f8\u4e92\u4f5c\u7528\u7684\u7279\u5f81\u6a21\u5f0f\u3002\u66f4\u7b80\u5355\u7684TM\u7279\u5b9a\u8bc4\u5206\u5668\u4ee5\u4e00\u5c0f\u90e8\u5206\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u76f8\u4f3c\u7684\u51c6\u786e\u6027\u4fdd\u7559\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u4e3aTsetlin\u673a\uff08TM\uff09\u5efa\u7acb\u4e86\u7279\u5f81\u9009\u62e9\uff08FS\uff09\u7684\u5168\u9762\u57fa\u7ebf\uff0c\u5e76\u4e3a\u5f00\u53d1\u4e13\u95e8\u7684TM\u7279\u5b9a\u53ef\u89e3\u91ca\u6027\u6280\u672f\u94fa\u5e73\u4e86\u9053\u8def\u3002TM\u5185\u90e8\u8bc4\u5206\u5668\u4e0d\u4ec5\u8868\u73b0\u51fa\u7ade\u4e89\u529b\uff0c\u800c\u4e14\u5229\u7528\u5b50\u53e5\u7684\u53ef\u89e3\u91ca\u6027\u63ed\u793a\u4e86\u76f8\u4e92\u4f5c\u7528\u7684\u7279\u5f81\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u66f4\u7b80\u5355\u7684TM\u7279\u5b9a\u8bc4\u5206\u5668\u4ee5\u4e00\u5c0f\u90e8\u5206\u8ba1\u7b97\u6210\u672c\u5b9e\u73b0\u4e86\u76f8\u4f3c\u7684\u51c6\u786e\u6027\u4fdd\u7559\u3002"}}
{"id": "2508.07657", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07657", "abs": "https://arxiv.org/abs/2508.07657", "authors": ["Zhuoli Tian", "Yuyang Zhang", "Jinsheng Wei", "Meng Guo"], "title": "MoRoCo: Multi-operator-robot Coordination, Interaction and Exploration under Restricted Communication", "comment": "38 pages, 28 figures, Submitted to the International Journal of\n  Robotics Research (IJRR). Project website: https://zl-tian.github.io/MoRoCo/", "summary": "Fleets of autonomous robots are increasingly deployed alongside multiple\nhuman operators to explore unknown environments, identify salient features, and\nperform complex tasks in scenarios such as subterranean exploration,\nreconnaissance, and search-and-rescue missions. In these contexts,\ncommunication is often severely limited to short-range exchanges via ad-hoc\nnetworks, posing challenges to coordination. While recent studies have\naddressed multi-robot exploration under communication constraints, they largely\noverlook the essential role of human operators and their real-time interaction\nwith robotic teams. Operators may demand timely updates on the exploration\nprogress and robot status, reprioritize or cancel tasks dynamically, or request\nlive video feeds and control access. Conversely, robots may seek human\nconfirmation for anomalous events or require help recovering from motion or\nplanning failures. To enable such bilateral, context-aware interactions under\nrestricted communication, this work proposes MoRoCo, a unified framework for\nonline coordination and exploration in multi-operator, multi-robot systems.\nMoRoCo enables the team to adaptively switch among three coordination modes:\nspread mode for parallelized exploration with intermittent data sharing,\nmigrate mode for coordinated relocation, and chain mode for maintaining\nhigh-bandwidth connectivity through multi-hop links. These transitions are\nmanaged through distributed algorithms via only local communication. Extensive\nlarge-scale human-in-the-loop simulations and hardware experiments validate the\nnecessity of incorporating human robot interactions and demonstrate that MoRoCo\nenables efficient, reliable coordination under limited communication, marking a\nsignificant step toward robust human-in-the-loop multi-robot autonomy in\nchallenging environments.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07933", "categories": ["quant-ph", "65Kxx", "G.1"], "pdf": "https://arxiv.org/pdf/2508.07933", "abs": "https://arxiv.org/abs/2508.07933", "authors": ["Aaditya Rudra", "Maria Anastasia Jivulescu"], "title": "Calculating the Projective Norm of higher-order tensors using a gradient descent algorithm", "comment": "19 figures", "summary": "Projective Norms are a class of tensor norms that map on the input and output\nspaces. These norms are useful for providing a measure of entanglement.\nCalculating the projective norms is an NP-hard problem, which creates\nchallenges in computing due to the complexity of the exponentially growing\nparameter space for higher-order tensors. We develop a novel gradient descent\nalgorithm to estimate the projective norm of higher-order tensors. The\nalgorithm guarantees convergence to a minimum nuclear rank decomposition of our\ngiven tensor. We extend our algorithm to symmetric tensors and density\nmatrices. We demonstrate the performance of our algorithm by computing the\nnuclear rank and the projective norm for both pure and mixed states and provide\nnumerical evidence for the same.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u4f30\u8ba1\u9ad8\u9636\u5f20\u91cf\u7684\u5c04\u5f71\u8303\u6570\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u590d\u6742\u6027\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u4e8e\u5bf9\u79f0\u5f20\u91cf\u548c\u5bc6\u5ea6\u77e9\u9635\u3002", "motivation": "\u8ba1\u7b97\u5c04\u5f71\u8303\u6570\u662fNP\u96be\u95ee\u9898\uff0c\u5bf9\u4e8e\u9ad8\u9636\u5f20\u91cf\uff0c\u53c2\u6570\u7a7a\u95f4\u5448\u6307\u6570\u589e\u957f\uff0c\u5e26\u6765\u4e86\u8ba1\u7b97\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u4f30\u8ba1\u9ad8\u9636\u5f20\u91cf\u7684\u5c04\u5f71\u8303\u6570\u3002", "result": "\u901a\u8fc7\u8ba1\u7b97\u7eaf\u6001\u548c\u6df7\u5408\u6001\u7684\u6838\u79e9\u548c\u5c04\u5f71\u8303\u6570\uff0c\u5e76\u63d0\u4f9b\u6570\u503c\u8bc1\u636e\uff0c\u8bc1\u660e\u4e86\u8be5\u7b97\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230\u7ed9\u5b9a\u5f20\u91cf\u6700\u5c0f\u6838\u79e9\u5206\u89e3\uff0c\u5e76\u5df2\u6210\u529f\u5e94\u7528\u4e8e\u5bf9\u79f0\u5f20\u91cf\u548c\u5bc6\u5ea6\u77e9\u9635\u3002"}}
{"id": "2508.07284", "categories": ["cs.CL", "cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.07284", "abs": "https://arxiv.org/abs/2508.07284", "authors": ["Junchen Ding", "Penghao Jiang", "Zihao Xu", "Ziqi Ding", "Yichen Zhu", "Jiaojiao Jiang", "Yuekang Li"], "title": "\"Pull or Not to Pull?'': Investigating Moral Biases in Leading Large Language Models Across Ethical Dilemmas", "comment": null, "summary": "As large language models (LLMs) increasingly mediate ethically sensitive\ndecisions, understanding their moral reasoning processes becomes imperative.\nThis study presents a comprehensive empirical evaluation of 14 leading LLMs,\nboth reasoning enabled and general purpose, across 27 diverse trolley problem\nscenarios, framed by ten moral philosophies, including utilitarianism,\ndeontology, and altruism. Using a factorial prompting protocol, we elicited\n3,780 binary decisions and natural language justifications, enabling analysis\nalong axes of decisional assertiveness, explanation answer consistency, public\nmoral alignment, and sensitivity to ethically irrelevant cues. Our findings\nreveal significant variability across ethical frames and model types: reasoning\nenhanced models demonstrate greater decisiveness and structured justifications,\nyet do not always align better with human consensus. Notably, \"sweet zones\"\nemerge in altruistic, fairness, and virtue ethics framings, where models\nachieve a balance of high intervention rates, low explanation conflict, and\nminimal divergence from aggregated human judgments. However, models diverge\nunder frames emphasizing kinship, legality, or self interest, often producing\nethically controversial outcomes. These patterns suggest that moral prompting\nis not only a behavioral modifier but also a diagnostic tool for uncovering\nlatent alignment philosophies across providers. We advocate for moral reasoning\nto become a primary axis in LLM alignment, calling for standardized benchmarks\nthat evaluate not just what LLMs decide, but how and why.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8614\u79cd\u5927\u578b\u8bed\u8a00\u6a21\u578b\u572827\u4e2a\u201c\u7535\u8f66\u96be\u9898\u201d\u573a\u666f\u4e0b\u7684\u9053\u5fb7\u63a8\u7406\u80fd\u529b\u3002\u6a21\u578b\u5728\u4e0d\u540c\u4f26\u7406\u6846\u67b6\u4e0b\u8868\u73b0\u5404\u5f02\uff0c\u652f\u6301\u63a8\u7406\u7684\u6a21\u578b\u51b3\u7b56\u66f4\u679c\u65ad\uff0c\u4f46\u5728\u67d0\u4e9b\u6846\u67b6\u4e0b\uff08\u5982\u5229\u4ed6\u4e3b\u4e49\uff09\u8868\u73b0\u66f4\u7b26\u5408\u4eba\u7c7b\u5224\u65ad\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u201c\u751c\u871c\u533a\u201d\uff0c\u5e76\u5f3a\u8c03\u4e86\u5c06\u9053\u5fb7\u63a8\u7406\u4f5c\u4e3aLLM\u5bf9\u9f50\u5173\u952e\u8bc4\u4f30\u7ef4\u5ea6\u548c\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u9053\u5fb7\u654f\u611f\u6027\u51b3\u7b56\u4e2d\u626e\u6f14\u8d8a\u6765\u8d8a\u91cd\u8981\u7684\u89d2\u8272\uff0c\u7406\u89e3\u5b83\u4eec\u7684\u9053\u5fb7\u63a8\u7406\u8fc7\u7a0b\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u5bf914\u79cd\u4e3b\u6d41\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08\u5305\u62ec\u652f\u6301\u63a8\u7406\u548c\u901a\u7528\u6a21\u578b\uff09\u8fdb\u884c\u4e86\u5b9e\u8bc1\u8bc4\u4f30\uff0c\u6db5\u76d6\u4e86\u7531\u5341\u79cd\u9053\u5fb7\u54f2\u5b66\uff08\u5982\u529f\u5229\u4e3b\u4e49\u3001\u4e49\u52a1\u8bba\u548c\u5229\u4ed6\u4e3b\u4e49\uff09\u5b9a\u4e49\u768427\u4e2a\u4e0d\u540c\u7684\u201c\u7535\u8f66\u96be\u9898\u201d\u573a\u666f\u3002\u7814\u7a76\u91c7\u7528\u4e86\u6790\u56e0\u63d0\u793a\u534f\u8bae\uff0c\u6536\u96c6\u4e863780\u4e2a\u4e8c\u5143\u51b3\u7b56\u548c\u76f8\u5e94\u7684\u81ea\u7136\u8bed\u8a00\u89e3\u91ca\uff0c\u4ee5\u4fbf\u4ece\u51b3\u7b56\u65ad\u8a00\u6027\u3001\u89e3\u91ca\u4e0e\u51b3\u7b56\u7684\u4e00\u81f4\u6027\u3001\u4e0e\u516c\u4f17\u9053\u5fb7\u7684\u5bf9\u9f50\u7a0b\u5ea6\u4ee5\u53ca\u5bf9\u4e0d\u76f8\u5173\u4f26\u7406\u7ebf\u7d22\u7684\u654f\u611f\u6027\u7b49\u591a\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u5206\u6790\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u663e\u793a\uff0c\u4e0d\u540c\u6a21\u578b\u548c\u4e0d\u540c\u4f26\u7406\u6846\u67b6\u4e0bLLM\u7684\u8868\u73b0\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002\u652f\u6301\u63a8\u7406\u7684\u6a21\u578b\u51b3\u7b56\u66f4\u679c\u65ad\uff0c\u89e3\u91ca\u66f4\u7ed3\u6784\u5316\uff0c\u4f46\u4e0d\u4e00\u5b9a\u66f4\u7b26\u5408\u4eba\u7c7b\u5171\u8bc6\u3002\u5728\u5229\u4ed6\u4e3b\u4e49\u3001\u516c\u5e73\u548c\u7f8e\u5fb7\u4f26\u7406\u7684\u6846\u67b6\u4e0b\uff0c\u201c\u751c\u871c\u533a\u201d\u73b0\u8c61\u51fa\u73b0\uff0c\u6a21\u578b\u5728\u5e72\u9884\u7387\u3001\u89e3\u91ca\u4e00\u81f4\u6027\u548c\u4e0e\u4eba\u7c7b\u5224\u65ad\u7684\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u826f\u597d\u3002\u4f46\u5728\u4eb2\u5c5e\u5173\u7cfb\u3001\u5408\u6cd5\u6027\u6216\u81ea\u8eab\u5229\u76ca\u7684\u6846\u67b6\u4e0b\uff0c\u6a21\u578b\u8868\u73b0\u4e0d\u4e00\u81f4\uff0c\u4e14\u5e38\u4ea7\u751f\u6709\u4e89\u8bae\u7684\u51b3\u7b56\u3002", "conclusion": "LLMs\u5728\u505a\u9053\u5fb7\u51b3\u7b56\u65f6\u8868\u73b0\u51fa\u663e\u8457\u5dee\u5f02\uff0c\u53d7\u4f26\u7406\u6846\u67b6\u548c\u6a21\u578b\u7c7b\u578b\u7684\u5f71\u54cd\u3002\u7814\u7a76\u53d1\u73b0\u4e86\u201c\u751c\u871c\u533a\u201d\uff0c\u5373\u6a21\u578b\u5728\u67d0\u4e9b\u5229\u4ed6\u3001\u516c\u5e73\u548c\u7f8e\u5fb7\u4f26\u7406\u6846\u67b6\u4e0b\u80fd\u8fbe\u5230\u5e72\u9884\u7387\u9ad8\u3001\u89e3\u91ca\u4e00\u81f4\u6027\u597d\u548c\u4e0e\u4eba\u7c7b\u5224\u65ad\u4e00\u81f4\u6027\u5f3a\u7684\u5e73\u8861\u3002\u7136\u800c\uff0c\u5728\u5f3a\u8c03\u4eb2\u5c5e\u5173\u7cfb\u3001\u5408\u6cd5\u6027\u6216\u81ea\u8eab\u5229\u76ca\u7684\u6846\u67b6\u4e0b\uff0c\u6a21\u578b\u8868\u73b0\u51fa\u73b0\u5206\u6b67\uff0c\u5e76\u53ef\u80fd\u4ea7\u751f\u6709\u4e89\u8bae\u7684\u51b3\u7b56\u3002\u7814\u7a76\u5f3a\u8c03\uff0c\u9053\u5fb7\u63d0\u793a\u4e0d\u4ec5\u80fd\u6539\u53d8\u6a21\u578b\u884c\u4e3a\uff0c\u8fd8\u80fd\u63ed\u793a\u5176\u6f5c\u5728\u7684\u5bf9\u9f50\u54f2\u5b66\u3002\u7814\u7a76\u8005\u4e3b\u5f20\u5c06\u9053\u5fb7\u63a8\u7406\u4f5c\u4e3aLLM\u5bf9\u9f50\u7684\u4e00\u4e2a\u4e3b\u8981\u8bc4\u4f30\u7ef4\u5ea6\uff0c\u5e76\u547c\u5401\u5efa\u7acb\u6807\u51c6\u5316\u57fa\u51c6\u6765\u8bc4\u4f30LLM\u51b3\u7b56\u7684\u201c\u5982\u4f55\u201d\u548c\u201c\u4e3a\u4f55\u201d\u3002"}}
{"id": "2508.07575", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07575", "abs": "https://arxiv.org/abs/2508.07575", "authors": ["Shiqing Fan", "Xichen Ding", "Liang Zhang", "Linjian Mo"], "title": "MCPToolBench++: A Large Scale AI Agent Model Context Protocol MCP Tool Use Benchmark", "comment": "Benchmarks and Source Code Released", "summary": "LLMs' capabilities are enhanced by using function calls to integrate various\ndata sources or API results into the context window. Typical tools include\nsearch, web crawlers, maps, financial data, file systems, and browser usage,\netc. Integrating these data sources or functions requires a standardized\nmethod. The Model Context Protocol (MCP) provides a standardized way to supply\ncontext to LLMs. However, the evaluation of LLMs and AI Agents' MCP tool use\nabilities suffer from several issues. First, there's a lack of comprehensive\ndatasets or benchmarks to evaluate various MCP tools. Second, the diverse\nformats of response from MCP tool call execution further increase the\ndifficulty of evaluation. Additionally, unlike existing tool-use benchmarks\nwith high success rates in functions like programming and math functions, the\nsuccess rate of real-world MCP tool is not guaranteed and varies across\ndifferent MCP servers. Furthermore, the LLMs' context window also limits the\nnumber of available tools that can be called in a single run, because the\ntextual descriptions of tool and the parameters have long token length for an\nLLM to process all at once. To help address the challenges of evaluating LLMs'\nperformance on calling MCP tools, we propose MCPToolBench++, a large-scale,\nmulti-domain AI Agent tool use benchmark. As of July 2025, this benchmark is\nbuild upon marketplace of over 4k MCP servers from more than 40 categories,\ncollected from the MCP marketplaces and GitHub communities. The datasets\nconsist of both single-step and multi-step tool calls across different\ncategories. We evaluated SOTA LLMs with agentic abilities on this benchmark and\nreported the results.", "AI": {"tldr": "MCPToolBench++\u662f\u4e00\u4e2a\u5305\u542b4000\u591a\u4e2aMCP\u670d\u52a1\u5668\u7684\u5927\u578b\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30LLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u7684LLM\u8bc4\u4f30\u65b9\u6cd5\u5728\u5904\u7406MCP\u5de5\u5177\u4f7f\u7528\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u5305\u62ec\u7f3a\u4e4f\u5168\u9762\u7684\u6570\u636e\u96c6\u548c\u57fa\u51c6\u3001MCP\u5de5\u5177\u54cd\u5e94\u683c\u5f0f\u591a\u6837\u3001\u5b9e\u9645\u5de5\u5177\u8c03\u7528\u6210\u529f\u7387\u4e0d\u786e\u5b9a\u4ee5\u53caLLM\u4e0a\u4e0b\u6587\u7a97\u53e3\u9650\u5236\u4e86\u5de5\u5177\u8c03\u7528\u6570\u91cf\u3002\u4f5c\u8005\u65e8\u5728\u901a\u8fc7\u63d0\u51faMCPToolBench++\u6765\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMCPToolBench++\u7684\u65b0\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u5305\u542b\u6765\u81ea40\u591a\u4e2a\u7c7b\u522b\u3001\u8d85\u8fc74000\u4e2aMCP\u670d\u52a1\u5668\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u5355\u6b65\u548c\u591a\u6b65\u5de5\u5177\u8c03\u7528\u3002\u6b64\u5916\uff0c\u8fd8\u8bc4\u4f30\u4e86\u5177\u6709\u4ee3\u7406\u80fd\u529b\u7684\u6700\u5148\u8fdbLLM\u5728\u8be5\u57fa\u51c6\u4e0a\u7684\u8868\u73b0\u3002", "result": "MCPToolBench++\u57fa\u51c6\u5305\u542b\u4e86\u6765\u81ea40\u591a\u4e2a\u7c7b\u522b\u3001\u8d85\u8fc74000\u4e2aMCP\u670d\u52a1\u5668\u7684\u6570\u636e\u96c6\uff0c\u652f\u6301\u5355\u6b65\u548c\u591a\u6b65\u5de5\u5177\u8c03\u7528\u3002\u4f5c\u8005\u4f7f\u7528\u8be5\u57fa\u51c6\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684LLM\uff0c\u5e76\u62a5\u544a\u4e86\u8bc4\u4f30\u7ed3\u679c\uff0c\u4e3aLLM\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u8bc4\u4f30\u63d0\u4f9b\u4e86\u66f4\u5168\u9762\u7684\u89c6\u89d2\u3002", "conclusion": "MCPToolBench++\u662f\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u591a\u9886\u57df\u7684\u4eba\u5de5\u667a\u80fd\u4ee3\u7406\u5de5\u5177\u4f7f\u7528\u57fa\u51c6\uff0c\u65e8\u5728\u89e3\u51b3\u8bc4\u4f30LLM\u8c03\u7528MCP\u5de5\u5177\u80fd\u529b\u7684\u6311\u6218\u3002\u8be5\u57fa\u51c6\u5305\u542b\u8d85\u8fc740\u4e2a\u7c7b\u522b\u76844000\u591a\u4e2aMCP\u670d\u52a1\u5668\uff0c\u6db5\u76d6\u5355\u6b65\u548c\u591a\u6b65\u5de5\u5177\u8c03\u7528\u3002\u901a\u8fc7\u5728\u57fa\u51c6\u4e0a\u8bc4\u4f30\u6700\u5148\u8fdb\u7684LLM\uff0c\u53ef\u4ee5\u66f4\u51c6\u786e\u5730\u8861\u91cf\u5b83\u4eec\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u5de5\u5177\u4f7f\u7528\u80fd\u529b\u3002"}}
{"id": "2508.07686", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07686", "abs": "https://arxiv.org/abs/2508.07686", "authors": ["Mingyue Lei", "Zewei Zhou", "Hongchen Li", "Jiaqi Ma", "Jia Hu"], "title": "Risk Map As Middleware: Towards Interpretable Cooperative End-to-end Autonomous Driving for Risk-Aware Planning", "comment": null, "summary": "End-to-end paradigm has emerged as a promising approach to autonomous\ndriving. However, existing single-agent end-to-end pipelines are often\nconstrained by occlusion and limited perception range, resulting in hazardous\ndriving. Furthermore, their black-box nature prevents the interpretability of\nthe driving behavior, leading to an untrustworthiness system. To address these\nlimitations, we introduce Risk Map as Middleware (RiskMM) and propose an\ninterpretable cooperative end-to-end driving framework. The risk map learns\ndirectly from the driving data and provides an interpretable spatiotemporal\nrepresentation of the scenario from the upstream perception and the\ninteractions between the ego vehicle and the surrounding environment for\ndownstream planning. RiskMM first constructs a multi-agent spatiotemporal\nrepresentation with unified Transformer-based architecture, then derives\nrisk-aware representations by modeling interactions among surrounding\nenvironments with attention. These representations are subsequently fed into a\nlearning-based Model Predictive Control (MPC) module. The MPC planner\ninherently accommodates physical constraints and different vehicle types and\ncan provide interpretation by aligning learned parameters with explicit MPC\nelements. Evaluations conducted on the real-world V2XPnP-Seq dataset confirm\nthat RiskMM achieves superior and robust performance in risk-aware trajectory\nplanning, significantly enhancing the interpretability of the cooperative\nend-to-end driving framework. The codebase will be released to facilitate\nfuture research in this field.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRiskMM\u7684\u53ef\u89e3\u91ca\u534f\u540c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\uff0c\u4f7f\u7528\u98ce\u9669\u5730\u56fe\u4e2d\u95f4\u4ef6\uff0c\u901a\u8fc7Transformer\u548cMPC\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u5b89\u5168\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u65b9\u6cd5\u5b58\u5728\u88ab\u906e\u6321\u548c\u611f\u77e5\u8303\u56f4\u6709\u9650\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u9a7e\u9a76\u5371\u9669\uff0c\u4e14\u5176\u9ed1\u76d2\u6027\u8d28\u963b\u788d\u4e86\u9a7e\u9a76\u884c\u4e3a\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u964d\u4f4e\u4e86\u7cfb\u7edf\u7684\u53ef\u4fe1\u5ea6\u3002", "method": "\u9996\u5148\uff0c\u901a\u8fc7\u7edf\u4e00\u7684Transformer\u57fa\u7840\u67b6\u6784\u6784\u5efa\u591a\u667a\u80fd\u4f53\u65f6\u7a7a\u8868\u793a\uff1b\u5176\u6b21\uff0c\u901a\u8fc7\u5bf9\u5468\u56f4\u73af\u5883\u7684\u4ea4\u4e92\u8fdb\u884c\u5efa\u6a21\uff0c\u5e76\u5229\u7528\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5bfc\u51fa\u98ce\u9669\u611f\u77e5\u8868\u793a\uff1b\u6700\u540e\uff0c\u5c06\u8fd9\u4e9b\u8868\u793a\u8f93\u5165\u5230\u57fa\u4e8e\u5b66\u4e60\u7684MPC\uff08\u6a21\u578b\u9884\u6d4b\u63a7\u5236\uff09\u6a21\u5757\u4e2d\u8fdb\u884c\u89c4\u5212\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u7684V2XPnP-Seq\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0cRiskMM\u5728\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u8d8a\u4e14\u9c81\u68d2\u7684\u6027\u80fd\uff0c\u663e\u8457\u589e\u5f3a\u4e86\u534f\u540c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\u7684\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "RiskMM\u901a\u8fc7\u5f15\u5165\u98ce\u9669\u5730\u56fe\u4f5c\u4e3a\u4e2d\u95f4\u4ef6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u89e3\u91ca\u7684\u591a\u667a\u80fd\u4f53\u534f\u540c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7Transformer\u548c\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u65f6\u7a7a\u8868\u793a\u548c\u98ce\u9669\u611f\u77e5\u8868\u793a\uff0c\u5e76\u7ed3\u5408MPC\u89c4\u5212\u5668\uff0c\u5b9e\u73b0\u4e86\u4f18\u8d8a\u4e14\u9c81\u68d2\u7684\u98ce\u9669\u611f\u77e5\u8f68\u8ff9\u89c4\u5212\uff0c\u663e\u8457\u63d0\u5347\u4e86\u534f\u540c\u7aef\u5230\u7aef\u9a7e\u9a76\u6846\u67b6\u7684\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2508.07977", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.07977", "abs": "https://arxiv.org/abs/2508.07977", "authors": ["Bibhuti Thapa", "Oberon Moran", "Duc-Kha Vu", "Fatih Ozaydin"], "title": "Expanding a 4-qubit Dicke State a to 5-qubit Dicke State with Limited Qubit Access", "comment": "16 pages, 6 figures, comments are welcome", "summary": "In scenarios where full access to all qubits of a multipartite quantum system\nis available and global operations can be implemented, the preparation of\narbitrary entangled states - including multipartite Dicke states - is\ntheoretically straightforward. However, practical constraints often limit\ndirect control over all qubits. In this work, we first present an efficient\nmethod for preparing a four-qubit Dicke state, and then demonstrate how a\nfour-qubit Dicke state can be expanded to a five-qubit Dicke state even when\nonly a subset of qubits is accessible. We propose a quantum circuit that\nachieves this transformation under restricted control, and support our\nanalytical derivation with numerical simulations. We further carry out a\nrobustness analysis of our circuit under imperfect gate implementations and\nfind that it retains high fidelity for experimentally relevant levels of\ncoherent over-rotation errors, confirming its resilience to realistic noise.", "AI": {"tldr": "\u7814\u7a76\u4e86\u4e00\u79cd\u5728\u91cf\u5b50\u6bd4\u7279\u53d7\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5236\u5907\u548c\u6269\u5c55\u72c4\u514b\u6001\u7684\u91cf\u5b50\u65b9\u6cd5\u3002", "motivation": "\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u76f4\u63a5\u63a7\u5236\u6240\u6709\u91cf\u5b50\u6bd4\u7279\u5f80\u5f80\u53d7\u5230\u9650\u5236\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76\u5728\u53d7\u9650\u63a7\u5236\u4e0b\u5236\u5907\u548c\u6269\u5c55\u91cf\u5b50\u6001\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5236\u5907\u56db\u6bd4\u7279\u72c4\u514b\u6001\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u91cf\u5b50\u7535\u8def\uff0c\u5728\u53ea\u8bbf\u95ee\u90e8\u5206\u6bd4\u7279\u7684\u60c5\u51b5\u4e0b\uff0c\u5c06\u56db\u6bd4\u7279\u72c4\u514b\u6001\u6269\u5c55\u5230\u4e94\u6bd4\u7279\u72c4\u514b\u6001\u3002", "result": "\u6210\u529f\u5236\u5907\u4e86\u56db\u6bd4\u7279\u72c4\u514b\u6001\uff0c\u5e76\u901a\u8fc7\u63d0\u51fa\u7684\u91cf\u5b50\u7535\u8def\u5c06\u5176\u6269\u5c55\u5230\u4e94\u6bd4\u7279\u72c4\u514b\u6001\uff0c\u4e14\u8be5\u7535\u8def\u5bf9\u566a\u58f0\u5177\u6709\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u53d7\u9650\u63a7\u5236\u4e0b\u5c06\u56db\u6bd4\u7279\u72c4\u514b\u6001\u6269\u5c55\u5230\u4e94\u6bd4\u7279\u72c4\u514b\u6001\u7684\u91cf\u5b50\u7535\u8def\uff0c\u5e76\u8fdb\u884c\u4e86\u6570\u503c\u6a21\u62df\u548c\u9c81\u68d2\u6027\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u8be5\u7535\u8def\u5728\u5b58\u5728\u566a\u58f0\u7684\u60c5\u51b5\u4e0b\u4ecd\u7136\u5177\u6709\u9ad8\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2508.07286", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.07286", "abs": "https://arxiv.org/abs/2508.07286", "authors": ["Jian Chen", "Jinbao Tian", "Yankui Li", "Zhou Li"], "title": "Arce: Augmented Roberta with Contextualized Elucidations for Ner in Automated Rule Checking", "comment": null, "summary": "Accurate information extraction from specialized texts is a critical\nchallenge, particularly for named entity recognition (NER) in the architecture,\nengineering, and construction (AEC) domain to support automated rule checking\n(ARC). The performance of standard pre-trained models is often constrained by\nthe domain gap, as they struggle to interpret the specialized terminology and\ncomplex relational contexts inherent in AEC texts. Although this issue can be\nmitigated by further pre-training on large, human-curated domain corpora, as\nexemplified by methods like ARCBERT, this approach is both labor-intensive and\ncost-prohibitive. Consequently, leveraging large language models (LLMs) for\nautomated knowledge generation has emerged as a promising alternative. However,\nthe optimal strategy for generating knowledge that can genuinely enhance\nsmaller, efficient models remains an open question. To address this, we propose\nARCE (augmented RoBERTa with contextualized elucidations), a novel approach\nthat systematically explores and optimizes this generation process. ARCE\nemploys an LLM to first generate a corpus of simple, direct explanations, which\nwe term Cote, and then uses this corpus to incrementally pre-train a RoBERTa\nmodel prior to its fine-tuning on the downstream task. Our extensive\nexperiments show that ARCE establishes a new state-of-the-art on a benchmark\nAEC dataset, achieving a Macro-F1 score of 77.20%. This result also reveals a\nkey finding: simple, explanation-based knowledge proves surprisingly more\neffective than complex, role-based rationales for this task. The code is\npublicly available at:https://github.com/nxcc-lab/ARCE.", "AI": {"tldr": "ARCE\u901a\u8fc7LLM\u751f\u6210\u89e3\u91ca\u6027\u6587\u672c\u6765\u9884\u8bad\u7ec3RoBERTa\u6a21\u578b\uff0c\u5728AEC\u9886\u57df\u547d\u540d\u5b9e\u4f53\u8bc6\u522b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u6807\u51c6\u9884\u8bad\u7ec3\u6a21\u578b\u5728AEC\u9886\u57df\u5b58\u5728\u57df\u8fc1\u79fb\u95ee\u9898\uff0c\u800cARCBERT\u7b49\u65b9\u6cd5\u6210\u672c\u9ad8\u6602\uff1bLLM\u5728\u77e5\u8bc6\u751f\u6210\u65b9\u9762\u6709\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u751f\u6210\u6709\u6548\u77e5\u8bc6\u4ee5\u589e\u5f3a\u5c0f\u578b\u6a21\u578b\u5c1a\u4e0d\u660e\u786e\u3002", "method": "ARCE\u901a\u8fc7LLM\u751f\u6210\u7b80\u660e\u627c\u8981\u7684\u89e3\u91ca\uff08Cote\uff09\uff0c\u7136\u540e\u5229\u7528Cote\u9884\u8bad\u7ec3RoBERTa\u6a21\u578b\uff0c\u6700\u540e\u5728\u4e0b\u6e38\u4efb\u52a1\u4e0a\u8fdb\u884c\u5fae\u8c03\u3002", "result": "ARCE\u5728AEC\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8677.20%\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u53d1\u73b0\u7b80\u5355\u7684\u89e3\u91ca\u6027\u77e5\u8bc6\u6bd4\u590d\u6742\u7684\u57fa\u4e8e\u89d2\u8272\u7684\u77e5\u8bc6\u66f4\u6709\u6548\u3002", "conclusion": "ARCE\u901a\u8fc7\u5728\u4e0b\u6e38\u4efb\u52a1\u5fae\u8c03\u524d\u589e\u91cf\u9884\u8bad\u7ec3RoBERTa\u6a21\u578b\uff0c\u5728AEC\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8677.20%\u7684\u5b8f\u89c2F1\u5206\u6570\uff0c\u5237\u65b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u7b80\u5355\u7684\u3001\u57fa\u4e8e\u89e3\u91ca\u7684\u77e5\u8bc6\u6bd4\u57fa\u4e8e\u89d2\u8272\u7684\u590d\u6742\u77e5\u8bc6\u66f4\u6709\u6548\u3002"}}
{"id": "2508.06869", "categories": ["cs.CV", "cs.AI", "I.2.10"], "pdf": "https://arxiv.org/pdf/2508.06869", "abs": "https://arxiv.org/abs/2508.06869", "authors": ["Jianxiang He", "Shaoguang Wang", "Weiyu Guo", "Meisheng Hong", "Jungang Li", "Yijie Xu", "Ziyang Chen", "Hui Xiong"], "title": "VSI: Visual Subtitle Integration for Keyframe Selection to enhance Long Video Understanding", "comment": "9 pages,3 figures", "summary": "Long video understanding presents a significant challenge to multimodal large\nlanguage models (MLLMs) primarily due to the immense data scale. A critical and\nwidely adopted strategy for making this task computationally tractable is\nkeyframe retrieval, which seeks to identify a sparse set of video frames that\nare most salient to a given textual query. However, the efficacy of this\napproach is hindered by weak multimodal alignment between textual queries and\nvisual content and fails to capture the complex temporal semantic information\nrequired for precise reasoning. To address this, we propose Visual-Subtitle\nIntegeration(VSI), a multimodal keyframe search method that integrates\nsubtitles, timestamps, and scene boundaries into a unified multimodal search\nprocess. The proposed method captures the visual information of video frames as\nwell as the complementary textual information through a dual-stream search\nmechanism by Video Search Stream as well as Subtitle Match Stream,\nrespectively, and improves the keyframe search accuracy through the interaction\nof the two search streams. Experimental results show that VSI achieve 40.00%\nkey frame localization accuracy on the text-relevant subset of LongVideoBench\nand 68.48% accuracy on downstream long Video-QA tasks, surpassing competitive\nbaselines by 20.35% and 15.79%, respectively. Furthermore, on the\nLongVideoBench, VSI achieved state-of-the-art(SOTA) in medium-to-long video-QA\ntasks, demonstrating the robustness and generalizability of the proposed\nmultimodal search strategy.", "AI": {"tldr": "\u957f\u89c6\u9891\u7406\u89e3\u56e0\u6570\u636e\u91cf\u5de8\u5927\u800c\u5145\u6ee1\u6311\u6218\u3002\u73b0\u6709\u5173\u952e\u5e27\u68c0\u7d22\u65b9\u6cd5\u56e0\u591a\u6a21\u6001\u5bf9\u9f50\u5f31\u548c\u7f3a\u4e4f\u65f6\u95f4\u8bed\u4e49\u4fe1\u606f\u800c\u6548\u679c\u4e0d\u4f73\u3002\u672c\u6587\u63d0\u51fa\u7684VSI\u65b9\u6cd5\u901a\u8fc7\u6574\u5408\u5b57\u5e55\u3001\u65f6\u95f4\u6233\u548c\u573a\u666f\u8fb9\u754c\uff0c\u5e76\u91c7\u7528\u53cc\u6d41\u641c\u7d22\u673a\u5236\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5173\u952e\u5e27\u68c0\u7d22\u7684\u51c6\u786e\u6027\u548c\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u7684\u6027\u80fd\uff0c\u8fbe\u5230\u4e86SOTA\u6c34\u5e73\u3002", "motivation": "\u73b0\u6709\u7684\u5173\u952e\u5e27\u68c0\u7d22\u65b9\u6cd5\u5728\u5904\u7406\u957f\u89c6\u9891\u65f6\uff0c\u5b58\u5728\u6587\u672c\u67e5\u8be2\u4e0e\u89c6\u89c9\u5185\u5bb9\u4e4b\u95f4\u591a\u6a21\u6001\u5bf9\u9f50\u8f83\u5f31\u4ee5\u53ca\u65e0\u6cd5\u6355\u6349\u590d\u6742\u65f6\u95f4\u8bed\u4e49\u4fe1\u606f\u7684\u95ee\u9898\uff0c\u5f71\u54cd\u4e86\u7cbe\u786e\u63a8\u7406\u7684\u6548\u7387\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aVSI\uff08Visual-Subtitle Integeration\uff09\u7684\u591a\u6a21\u6001\u5173\u952e\u5e27\u641c\u7d22\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u6574\u5408\u4e86\u5b57\u5e55\u3001\u65f6\u95f4\u6233\u548c\u573a\u666f\u8fb9\u754c\u4fe1\u606f\u3002\u901a\u8fc7\u89c6\u9891\u641c\u7d22\u6d41\u548c\u5b57\u5e55\u5339\u914d\u6d41\u7684\u53cc\u6d41\u673a\u5236\uff0c\u5206\u522b\u6355\u6349\u89c6\u9891\u5e27\u7684\u89c6\u89c9\u4fe1\u606f\u548c\u4e92\u8865\u7684\u6587\u672c\u4fe1\u606f\uff0c\u5e76\u901a\u8fc7\u4e24\u4e2a\u641c\u7d22\u6d41\u7684\u4ea4\u4e92\u6765\u63d0\u9ad8\u5173\u952e\u5e27\u641c\u7d22\u7684\u51c6\u786e\u6027\u3002", "result": "VSI\u65b9\u6cd5\u5728LongVideoBench\u6570\u636e\u96c6\u7684\u6587\u672c\u76f8\u5173\u5b50\u96c6\u4e0a\u5b9e\u73b0\u4e8640.00%\u7684\u5173\u952e\u5e27\u5b9a\u4f4d\u51c6\u786e\u7387\uff0c\u5728\u4e0b\u6e38\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e8668.48%\u7684\u51c6\u786e\u7387\uff0c\u5206\u522b\u6bd4\u7ade\u4e89\u57fa\u7ebf\u9ad8\u51fa20.35%\u548c15.79%\u3002", "conclusion": "VSI\u65b9\u6cd5\u5728LongVideoBench\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8640.00%\u7684\u5173\u952e\u5e27\u5b9a\u4f4d\u51c6\u786e\u7387\uff0c\u5728\u4e0b\u6e38\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e8668.48%\u7684\u51c6\u786e\u7387\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u5728\u4e2d\u957f\u89c6\u9891\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86SOTA\uff08State-of-the-Art\uff09\u7684\u6210\u5c31\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07586", "categories": ["cs.AI", "cs.NI"], "pdf": "https://arxiv.org/pdf/2508.07586", "abs": "https://arxiv.org/abs/2508.07586", "authors": ["Wenjing Zhang", "Ye Hu", "Tao Luo", "Zhilong Zhang", "Mingzhe Chen"], "title": "Optimization of Private Semantic Communication Performance: An Uncooperative Covert Communication Method", "comment": null, "summary": "In this paper, a novel covert semantic communication framework is\ninvestigated. Within this framework, a server extracts and transmits the\nsemantic information, i.e., the meaning of image data, to a user over several\ntime slots. An attacker seeks to detect and eavesdrop the semantic transmission\nto acquire details of the original image. To avoid data meaning being\neavesdropped by an attacker, a friendly jammer is deployed to transmit jamming\nsignals to interfere the attacker so as to hide the transmitted semantic\ninformation. Meanwhile, the server will strategically select time slots for\nsemantic information transmission. Due to limited energy, the jammer will not\ncommunicate with the server and hence the server does not know the transmit\npower of the jammer. Therefore, the server must jointly optimize the semantic\ninformation transmitted at each time slot and the corresponding transmit power\nto maximize the privacy and the semantic information transmission quality of\nthe user. To solve this problem, we propose a prioritised sampling assisted\ntwin delayed deep deterministic policy gradient algorithm to jointly determine\nthe transmitted semantic information and the transmit power per time slot\nwithout the communications between the server and the jammer. Compared to\nstandard reinforcement learning methods, the propose method uses an additional\nQ network to estimate Q values such that the agent can select the action with a\nlower Q value from the two Q networks thus avoiding local optimal action\nselection and estimation bias of Q values. Simulation results show that the\nproposed algorithm can improve the privacy and the semantic information\ntransmission quality by up to 77.8% and 14.3% compared to the traditional\nreinforcement learning methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u4f18\u5148\u91c7\u6837\u8f85\u52a9\u7684\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\u6765\u4f18\u5316\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u548c\u529f\u7387\u5206\u914d\uff0c\u4ee5\u63d0\u9ad8\u9690\u79c1\u6027\u548c\u4f20\u8f93\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u5728\u9690\u853d\u8bed\u4e49\u901a\u4fe1\u4e2d\u9690\u85cf\u4f20\u8f93\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u90e8\u7f72\u4e86\u53cb\u597d\u7684\u5e72\u6270\u5668\u6765\u5e72\u6270\u653b\u51fb\u8005\u7684\u68c0\u6d4b\u548c\u7a83\u542c\u3002\u7136\u800c\uff0c\u7531\u4e8e\u5e72\u6270\u5668\u80fd\u91cf\u6709\u9650\u4e14\u4e0d\u4e0e\u670d\u52a1\u5668\u901a\u4fe1\uff0c\u670d\u52a1\u5668\u4e0d\u77e5\u9053\u5e72\u6270\u5668\u7684\u53d1\u5c04\u529f\u7387\u3002\u56e0\u6b64\uff0c\u670d\u52a1\u5668\u5fc5\u987b\u8054\u5408\u4f18\u5316\u6bcf\u4e2a\u65f6\u9699\u4f20\u8f93\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u76f8\u5e94\u7684\u4f20\u8f93\u529f\u7387\uff0c\u4ee5\u6700\u5927\u5316\u7528\u6237\u9690\u79c1\u548c\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u8d28\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5148\u91c7\u6837\u8f85\u52a9\u7684\u53cc\u5ef6\u8fdf\u6df1\u5ea6\u786e\u5b9a\u6027\u7b56\u7565\u68af\u5ea6\u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u65e0\u9700\u670d\u52a1\u5668\u548c\u5e72\u6270\u5668\u4e4b\u95f4\u7684\u901a\u4fe1\uff0c\u5373\u53ef\u8054\u5408\u786e\u5b9a\u6bcf\u4e2a\u65f6\u9699\u4f20\u8f93\u7684\u8bed\u4e49\u4fe1\u606f\u548c\u4f20\u8f93\u529f\u7387\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u76f8\u6bd4\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u5c06\u9690\u79c1\u6027\u63d0\u9ad8\u9ad8\u8fbe77.8%\uff0c\u5c06\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u8d28\u91cf\u63d0\u9ad8\u9ad8\u8fbe14.3%\u3002", "conclusion": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u4e0e\u4f20\u7edf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u76f8\u6bd4\uff0c\u9690\u79c1\u6027\u548c\u8bed\u4e49\u4fe1\u606f\u4f20\u8f93\u8d28\u91cf\u53ef\u5206\u522b\u63d0\u9ad8\u9ad8\u8fbe77.8%\u548c14.3%\u3002"}}
{"id": "2508.07016", "categories": ["cs.LG", "cs.IR"], "pdf": "https://arxiv.org/pdf/2508.07016", "abs": "https://arxiv.org/abs/2508.07016", "authors": ["Jianfei Wu", "Wenmian Yang", "Bingning Liu", "Weijia Jia"], "title": "TLCCSP: A Scalable Framework for Enhancing Time Series Forecasting with Time-Lagged Cross-Correlations", "comment": null, "summary": "Time series forecasting is critical across various domains, such as weather,\nfinance and real estate forecasting, as accurate forecasts support informed\ndecision-making and risk mitigation. While recent deep learning models have\nimproved predictive capabilities, they often overlook time-lagged\ncross-correlations between related sequences, which are crucial for capturing\ncomplex temporal relationships. To address this, we propose the Time-Lagged\nCross-Correlations-based Sequence Prediction framework (TLCCSP), which enhances\nforecasting accuracy by effectively integrating time-lagged cross-correlated\nsequences. TLCCSP employs the Sequence Shifted Dynamic Time Warping (SSDTW)\nalgorithm to capture lagged correlations and a contrastive learning-based\nencoder to efficiently approximate SSDTW distances.\n  Experimental results on weather, finance and real estate time series datasets\ndemonstrate the effectiveness of our framework. On the weather dataset, SSDTW\nreduces mean squared error (MSE) by 16.01% compared with single-sequence\nmethods, while the contrastive learning encoder (CLE) further decreases MSE by\n17.88%. On the stock dataset, SSDTW achieves a 9.95% MSE reduction, and CLE\nreduces it by 6.13%. For the real estate dataset, SSDTW and CLE reduce MSE by\n21.29% and 8.62%, respectively. Additionally, the contrastive learning approach\ndecreases SSDTW computational time by approximately 99%, ensuring scalability\nand real-time applicability across multiple time series forecasting tasks.", "AI": {"tldr": "TLCCSP framework improves time series forecasting by using SSDTW and a contrastive learning encoder to capture time-lagged cross-correlations, significantly reducing MSE and computational cost.", "motivation": "Traditional deep learning models often neglect time-lagged cross-correlations between related sequences, which are vital for capturing complex temporal relationships in time series forecasting.", "method": "The framework utilizes the Sequence Shifted Dynamic Time Warping (SSDTW) algorithm to capture lagged correlations and a contrastive learning-based encoder to approximate SSDTW distances, thereby reducing computational time significantly.", "result": "Experimental results show substantial Mean Squared Error (MSE) reductions across weather, finance, and real estate datasets, with SSDTW achieving reductions of 16.01%, 9.95%, and 21.29% respectively, and the contrastive learning encoder further improving these by 17.88%, 6.13%, and 8.62%. The contrastive learning approach also reduces SSDTW computational time by approximately 99%.", "conclusion": "The proposed TLCCSP framework effectively enhances time series forecasting accuracy by integrating time-lagged cross-correlations, outperforming single-sequence methods and offering significant computational efficiency through a contrastive learning encoder."}}
{"id": "2508.07689", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07689", "abs": "https://arxiv.org/abs/2508.07689", "authors": ["Christian Eichmann", "Sabine Bellmann", "Nicolas H\u00fcgel", "Louis-Elias Enslin", "Carsten Plasberg", "Georg Heppner", "Arne Roennau", "Ruediger Dillmann"], "title": "LAURON VI: A Six-Legged Robot for Dynamic Walking", "comment": null, "summary": "Legged locomotion enables robotic systems to traverse extremely challenging\nterrains. In many real-world scenarios, the terrain is not that difficult and\nthese mixed terrain types introduce the need for flexible use of different\nwalking strategies to achieve mission goals in a fast, reliable, and\nenergy-efficient way. Six-legged robots have a high degree of flexibility and\ninherent stability that aids them in traversing even some of the most difficult\nterrains, such as collapsed buildings. However, their lack of fast walking\ngaits for easier surfaces is one reason why they are not commonly applied in\nthese scenarios.\n  This work presents LAURON VI, a six-legged robot platform for research on\ndynamic walking gaits as well as on autonomy for complex field missions. The\nrobot's 18 series elastic joint actuators offer high-frequency interfaces for\nCartesian impedance and pure torque control. We have designed, implemented, and\ncompared three control approaches: kinematic-based, model-predictive, and\nreinforcement-learned controllers. The robot hardware and the different control\napproaches were extensively tested in a lab environment as well as on a Mars\nanalog mission. The introduction of fast locomotion strategies for LAURON VI\nmakes six-legged robots vastly more suitable for a wide range of real-world\napplications.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86 LAURON VI \u516d\u8db3\u673a\u5668\u4eba\u5e73\u53f0\uff0c\u901a\u8fc7\u5f00\u53d1 fast locomotion strategies\uff0c\u89e3\u51b3\u4e86\u516d\u8db3\u673a\u5668\u4eba\u5728\u5e73\u5766\u5730\u5f62\u4e0a\u901f\u5ea6\u6162\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u5176\u5728\u5404\u79cd\u73b0\u5b9e\u573a\u666f\u4e2d\u7684\u9002\u7528\u6027\u3002", "motivation": "\u516d\u8db3\u673a\u5668\u4eba\u867d\u7136\u5728\u5d0e\u5c96\u5730\u5f62\u4e0a\u5177\u6709\u4f18\u52bf\uff0c\u4f46\u5728\u5e73\u5766\u5730\u5f62\u4e0a\u7684\u884c\u8d70\u901f\u5ea6\u8f83\u6162\uff0c\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u9700\u8981\u5f00\u53d1\u66f4\u5feb\u7684\u884c\u8d70\u7b56\u7565\u3002", "method": "\u672c\u6587\u8bbe\u8ba1\u3001\u5b9e\u73b0\u5e76\u6bd4\u8f83\u4e86\u4e09\u79cd\u63a7\u5236\u65b9\u6cd5\uff1a\u57fa\u4e8e\u8fd0\u52a8\u5b66\u3001\u6a21\u578b\u9884\u6d4b\u548c\u5f3a\u5316\u5b66\u4e60\u7684\u63a7\u5236\u5668\u3002\u673a\u5668\u4eba\u786c\u4ef6\u548c\u4e0d\u540c\u63a7\u5236\u65b9\u6cd5\u5728\u5b9e\u9a8c\u5ba4\u73af\u5883\u548c\u706b\u661f\u6a21\u62df\u4efb\u52a1\u4e2d\u90fd\u7ecf\u8fc7\u4e86\u5e7f\u6cdb\u6d4b\u8bd5\u3002", "result": "\u901a\u8fc7\u5f15\u5165 fast locomotion strategies\uff0cLAURON VI \u5e73\u53f0\u80fd\u591f\u5b9e\u73b0\u66f4\u5feb\u7684\u884c\u8d70\u901f\u5ea6\uff0c\u63d0\u9ad8\u4e86\u516d\u8db3\u673a\u5668\u4eba\u5728\u4e0d\u540c\u5730\u5f62\u4e0b\u7684\u9002\u7528\u6027\u3002", "conclusion": " LAURON VI \u5e73\u53f0\u7684\u5f00\u53d1\u548c fast locomotion strategies \u7684\u5f15\u5165\uff0c\u4f7f\u5f97\u516d\u8db3\u673a\u5668\u4eba\u80fd\u591f\u9002\u5e94\u66f4\u5e7f\u6cdb\u7684\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u573a\u666f\u3002"}}
{"id": "2508.08024", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08024", "abs": "https://arxiv.org/abs/2508.08024", "authors": ["Gang Liu", "Wei Xiong"], "title": "$\\bf A^2$-robust superradiant phase transition in hybrid qubit-cavity optomechanics", "comment": null, "summary": "The $\\mathbf{A}^2$ term presents a fundamental challenge to realizing the\nsuperradiant phase transition (SPT) in cavity quantum electrodynamics. Here, we\npropose a hybrid quantum system enabling SPT regardless of the presence of the\n$\\mathbf{A}^2$ term. The system consisting of a qubit, a mechanical mode, and\nan optical cavity, where the qubit and mechanical mode constitute a quantum\nRabi model, while the mechanical mode and cavity form an optomechanical system.\nCrucially, the auxiliary cavity introduces a switchable $\\mathbf{A}^2$ term\nthat effectively counteracts or even fully eliminates the original\n$\\mathbf{A}^2$ effect. This allows controllable observation of SPT, diagnosed\nvia the second-order equal-time correlation function $g^{(2)}(0)$ of phonons.\nFurthermore, the auxiliary cavity exponentially reduces the critical coupling\nstrength, significantly relaxing experimental requirements. Besides, we show\nthat phonons in the normal phase are bunching, but coherent in the superradiant\nphase. Interestly, higher-order squeezing is found in both phases, with\nnear-perfect higher-order squeezing achieved at SPT point, establishing it as a\nprobe for SPT behavior. Our work demonstrates that hybridizing optomechanics\nand cavity quantum electrodynamics provides a promising route to accessing SPT\nphysics in the presence of the $\\mathbf{A}^2$ term.", "AI": {"tldr": "\u901a\u8fc7\u6df7\u5408\u91cf\u5b50\u7cfb\u7edf\uff08\u91cf\u5b50\u6bd4\u7279+\u673a\u68b0\u6a21\u5f0f+\u5149\u5b66\u8154+\u8f85\u52a9\u8154\uff09\u514b\u670d\u4e86 A^2 \u9879\u5bf9\u8d85\u8f90\u5c04\u76f8\u53d8\uff08SPT\uff09\u7684\u9650\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u63a7\u7684 SPT \u89c2\u6d4b\uff0c\u964d\u4f4e\u4e86\u5b9e\u9a8c\u95e8\u69db\uff0c\u5e76\u53d1\u73b0\u4e86\u58f0\u5b50\u7684\u4e0d\u540c\u884c\u4e3a\u548c\u9ad8\u9636\u538b\u7f29\u53ef\u4f5c\u4e3a SPT \u7684\u63a2\u6d4b\u624b\u6bb5\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d A^2 \u9879\u5bf9\u5728\u8154\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u4e2d\u5b9e\u73b0\u8d85\u8f90\u5c04\u76f8\u53d8\uff08SPT\uff09\u7684\u6839\u672c\u6027\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u91cf\u5b50\u6bd4\u7279\u3001\u673a\u68b0\u6a21\u5f0f\u548c\u5149\u5b66\u8154\u7684\u6df7\u5408\u91cf\u5b50\u7cfb\u7edf\u3002\u5176\u4e2d\uff0c\u91cf\u5b50\u6bd4\u7279\u548c\u673a\u68b0\u6a21\u5f0f\u6784\u6210\u91cf\u5b50\u62c9\u6bd4\u6a21\u578b\uff0c\u673a\u68b0\u6a21\u5f0f\u548c\u5149\u5b66\u8154\u6784\u6210\u5149\u529b\u5b66\u7cfb\u7edf\u3002\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u8f85\u52a9\u8154\uff0c\u8be5\u7cfb\u7edf\u80fd\u591f\u4ea7\u751f\u4e00\u4e2a\u53ef\u5207\u6362\u7684 A^2 \u9879\uff0c\u7528\u4ee5\u62b5\u6d88\u6216\u6d88\u9664\u539f\u59cb\u7cfb\u7edf\u4e2d\u7684 A^2 \u6548\u5e94\u3002", "result": "\u6210\u529f\u5b9e\u73b0\u4e86\u5728\u5b58\u5728 A^2 \u9879\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u6df7\u5408\u91cf\u5b50\u7cfb\u7edf\uff08\u5305\u542b\u91cf\u5b50\u6bd4\u7279\u3001\u673a\u68b0\u6a21\u5f0f\u548c\u5149\u5b66\u8154\uff09\u8fdb\u884c\u53ef\u63a7\u7684 SPT \u89c2\u6d4b\u3002\u901a\u8fc7\u8f85\u52a9\u8154\u62b5\u6d88\u4e86 A^2 \u6548\u5e94\uff0c\u964d\u4f4e\u4e86\u4e34\u754c\u8026\u5408\u5f3a\u5ea6\uff0c\u4f7f\u5f97\u5b9e\u9a8c\u8981\u6c42\u5f97\u4ee5\u653e\u5bbd\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6b63\u5e38\u76f8\u7684\u58f0\u5b50\u8868\u73b0\u51fa\u805a\u675f\u6027\uff0c\u800c\u8d85\u8f90\u5c04\u76f8\u7684\u58f0\u5b50\u5219\u8868\u73b0\u51fa\u76f8\u5e72\u6027\u3002\u5728 SPT \u70b9\u9644\u8fd1\u5b9e\u73b0\u4e86\u9ad8\u9636\u538b\u7f29\uff0c\u53ef\u4f5c\u4e3a\u63a2\u6d4b SPT \u7684\u624b\u6bb5\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u91cf\u5b50\u7cfb\u7edf\uff0c\u53ef\u5728\u5b58\u5728 A^2 \u9879\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u8d85\u8f90\u5c04\u76f8\u53d8\uff08SPT\uff09\uff0c\u8be5\u7cfb\u7edf\u901a\u8fc7\u5f15\u5165\u8f85\u52a9\u8154\u6765\u62b5\u6d88\u6216\u6d88\u9664 A^2 \u6548\u5e94\uff0c\u4ece\u800c\u5b9e\u73b0\u53ef\u63a7\u7684 SPT \u89c2\u6d4b\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u58f0\u5b50\u7684\u4e8c\u9636\u7b49\u65f6\u5173\u8054\u51fd\u6570 g^(2)(0) \u8fdb\u884c\u8bca\u65ad\u3002\u6b64\u5916\uff0c\u8be5\u7cfb\u7edf\u80fd\u663e\u8457\u964d\u4f4e SPT \u7684\u4e34\u754c\u8026\u5408\u5f3a\u5ea6\uff0c\u653e\u5bbd\u4e86\u5b9e\u9a8c\u8981\u6c42\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6b63\u5e38\u76f8\u4e2d\u7684\u58f0\u5b50\u8868\u73b0\u51fa\u805a\u675f\u884c\u4e3a\uff0c\u800c\u5728\u8d85\u8f90\u5c04\u76f8\u4e2d\u5219\u8868\u73b0\u51fa\u76f8\u5e72\u6027\u3002\u5728\u4e24\u4e2a\u76f8\u4e2d\u5747\u89c2\u5bdf\u5230\u9ad8\u9636\u538b\u7f29\uff0c\u5e76\u5728 SPT \u70b9\u9644\u8fd1\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u9ad8\u9636\u538b\u7f29\uff0c\u53ef\u4f5c\u4e3a SPT \u7684\u63a2\u6d4b\u624b\u6bb5\u3002\u8be5\u7814\u7a76\u8868\u660e\uff0c\u6df7\u5408\u5149\u529b\u5b66\u548c\u8154\u91cf\u5b50\u7535\u52a8\u529b\u5b66\u662f\u5b9e\u73b0\u5b58\u5728 A^2 \u9879\u7684 SPT \u7269\u7406\u5b66\u7684\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.07295", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07295", "abs": "https://arxiv.org/abs/2508.07295", "authors": ["Yexing Du", "Kaiyuan Liu", "Youcheng Pan", "Zheng Chu", "Bo Yang", "Xiaocheng Feng", "Yang Xiang", "Ming Liu"], "title": "CCFQA: A Benchmark for Cross-Lingual and Cross-Modal Speech and Text Factuality Evaluation", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly popularized in the\nmultilingual world, ensuring hallucination-free factuality becomes markedly\ncrucial. However, existing benchmarks for evaluating the reliability of\nMultimodal Large Language Models (MLLMs) predominantly focus on textual or\nvisual modalities with a primary emphasis on English, which creates a gap in\nevaluation when processing multilingual input, especially in speech. To bridge\nthis gap, we propose a novel \\textbf{C}ross-lingual and \\textbf{C}ross-modal\n\\textbf{F}actuality benchmark (\\textbf{CCFQA}). Specifically, the CCFQA\nbenchmark contains parallel speech-text factual questions across 8 languages,\ndesigned to systematically evaluate MLLMs' cross-lingual and cross-modal\nfactuality capabilities. Our experimental results demonstrate that current\nMLLMs still face substantial challenges on the CCFQA benchmark. Furthermore, we\npropose a few-shot transfer learning strategy that effectively transfers the\nQuestion Answering (QA) capabilities of LLMs in English to multilingual Spoken\nQuestion Answering (SQA) tasks, achieving competitive performance with\nGPT-4o-mini-Audio using just 5-shot training. We release CCFQA as a\nfoundational research resource to promote the development of MLLMs with more\nrobust and reliable speech understanding capabilities. Our code and dataset are\navailable at https://github.com/yxduir/ccfqa.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86CCFQA\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u8bed\u8a00\u5927\u8bed\u8a00\u6a21\u578b\u5728\u8bed\u97f3\u4e8b\u5b9e\u95ee\u7b54\u4e2d\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u80fd\u529b\u3002\u7ed3\u679c\u663e\u793a\u73b0\u6709\u6a21\u578b\u4ecd\u6709\u5f88\u5927\u63d0\u5347\u7a7a\u95f4\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u6709\u6548\u7684few-shot\u8fc1\u79fb\u5b66\u4e60\u65b9\u6cd5\uff0c\u80fd\u5728\u5c11\u6837\u672c\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e0eGPT-4o-mini-Audio\u76f8\u5f53\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u53ef\u9760\u6027\u7684\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u82f1\u8bed\u7684\u6587\u672c\u6216\u89c6\u89c9\u6a21\u6001\uff0c\u5728\u5904\u7406\u591a\u8bed\u8a00\u8f93\u5165\uff0c\u5c24\u5176\u662f\u8bed\u97f3\u65f6\u5b58\u5728\u8bc4\u4f30\u7a7a\u767d\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30MLLM\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u4e8b\u5b9e\u6027\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8de8\u8bed\u8a00\u3001\u8de8\u6a21\u6001\u4e8b\u5b9e\u6027\u95ee\u7b54\u57fa\u51c6CCFQA\uff0c\u5176\u4e2d\u5305\u542b8\u79cd\u8bed\u8a00\u7684\u5e76\u884c\u8bed\u97f3-\u6587\u672c\u4e8b\u5b9e\u95ee\u9898\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30MLLM\u7684\u8de8\u8bed\u8a00\u548c\u8de8\u6a21\u6001\u4e8b\u5b9e\u6027\u80fd\u529b\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u4e00\u79cdfew-shot\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\uff0c\u5c06LLM\u7684\u82f1\u8bed\u95ee\u7b54\u80fd\u529b\u8fc1\u79fb\u5230\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\u4efb\u52a1\u3002", "result": "\u5f53\u524d\u7684MLLM\u5728CCFQA\u57fa\u51c6\u4e0a\u9762\u4e34\u4e25\u5cfb\u6311\u6218\u3002\u63d0\u51fa\u7684few-shot\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u57285-shot\u8bad\u7ec3\u4e0b\uff0c\u80fd\u591f\u5b9e\u73b0\u4e0eGPT-4o-mini-Audio\u76f8\u5f53\u7684\u6027\u80fd\u3002", "conclusion": "CCFQA\u57fa\u51c6\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684MLLM\u5728\u5904\u7406\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\u65b9\u9762\u4ecd\u9762\u4e34\u5de8\u5927\u6311\u6218\u3002\u63d0\u51fa\u7684few-shot\u8fc1\u79fb\u5b66\u4e60\u7b56\u7565\u80fd\u591f\u6709\u6548\u5730\u5c06LLM\u7684\u82f1\u8bed\u95ee\u7b54\u80fd\u529b\u8fc1\u79fb\u5230\u591a\u8bed\u8a00\u8bed\u97f3\u95ee\u7b54\u4efb\u52a1\u4e2d\uff0c\u5e76\u4e14\u5728\u4ec5\u4f7f\u75285-shot\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8fbe\u5230\u4e86\u4e0eGPT-4o-mini-Audio\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002CCFQA\u7684\u53d1\u5e03\u65e8\u5728\u4fc3\u8fdbMLLM\u5728\u8bed\u97f3\u7406\u89e3\u80fd\u529b\u65b9\u9762\u66f4\u52a0\u9c81\u68d2\u548c\u53ef\u9760\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.06874", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06874", "abs": "https://arxiv.org/abs/2508.06874", "authors": ["Shisheng Zhang", "Ramtin Gharleghi", "Sonit Singh", "Daniel Moses", "Dona Adikari", "Arcot Sowmya", "Susann Beier"], "title": "LWT-ARTERY-LABEL: A Lightweight Framework for Automated Coronary Artery Identification", "comment": null, "summary": "Coronary artery disease (CAD) remains the leading cause of death globally,\nwith computed tomography coronary angiography (CTCA) serving as a key\ndiagnostic tool. However, coronary arterial analysis using CTCA, such as\nidentifying artery-specific features from computational modelling, is\nlabour-intensive and time-consuming. Automated anatomical labelling of coronary\narteries offers a potential solution, yet the inherent anatomical variability\nof coronary trees presents a significant challenge. Traditional knowledge-based\nlabelling methods fall short in leveraging data-driven insights, while recent\ndeep-learning approaches often demand substantial computational resources and\noverlook critical clinical knowledge. To address these limitations, we propose\na lightweight method that integrates anatomical knowledge with rule-based\ntopology constraints for effective coronary artery labelling. Our approach\nachieves state-of-the-art performance on benchmark datasets, providing a\npromising alternative for automated coronary artery labelling.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u7ed3\u5408\u89e3\u5256\u5b66\u77e5\u8bc6\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u62d3\u6251\u7ea6\u675f\uff0c\u7528\u4e8e\u6709\u6548\u7684\u51a0\u72b6\u52a8\u8109\u6807\u8bb0\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u8ba1\u7b97\u91cf\u5927\uff0c\u8017\u65f6\uff0c\u800c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u8ba1\u7b97\u8d44\u6e90\u5e76\u5ffd\u7565\u4e86\u5173\u952e\u7684\u4e34\u5e8a\u77e5\u8bc6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89e3\u5256\u5b66\u77e5\u8bc6\u548c\u57fa\u4e8e\u89c4\u5219\u7684\u62d3\u6251\u7ea6\u675f\u7684\u8f7b\u91cf\u7ea7\u65b9\u6cd5\uff0c\u7528\u4e8e\u6709\u6548\u7684\u51a0\u72b6\u52a8\u8109\u6807\u8bb0\u3002", "result": "\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u51a0\u72b6\u52a8\u8109\u6807\u8bb0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u4e3a\u81ea\u52a8\u51a0\u72b6\u52a8\u8109\u6807\u8bb0\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848\u3002"}}
{"id": "2508.07602", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07602", "abs": "https://arxiv.org/abs/2508.07602", "authors": ["Wenpeng Xing", "Zhipeng Chen", "Changting Lin", "Meng Han"], "title": "HGMF: A Hierarchical Gaussian Mixture Framework for Scalable Tool Invocation within the Model Context Protocol", "comment": null, "summary": "Invoking external tools enables Large Language Models (LLMs) to perform\ncomplex, real-world tasks, yet selecting the correct tool from large,\nhierarchically-structured libraries remains a significant challenge. The\nlimited context windows of LLMs and noise from irrelevant options often lead to\nlow selection accuracy and high computational costs. To address this, we\npropose the Hierarchical Gaussian Mixture Framework (HGMF), a probabilistic\npruning method for scalable tool invocation. HGMF first maps the user query and\nall tool descriptions into a unified semantic space. The framework then\noperates in two stages: it clusters servers using a Gaussian Mixture Model\n(GMM) and filters them based on the query's likelihood. Subsequently, it\napplies the same GMM-based clustering and filtering to the tools associated\nwith the selected servers. This hierarchical process produces a compact,\nhigh-relevance candidate set, simplifying the final selection task for the LLM.\nExperiments on a public dataset show that HGMF significantly improves tool\nselection accuracy while reducing inference latency, confirming the framework's\nscalability and effectiveness for large-scale tool libraries.", "AI": {"tldr": "HGMF\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u805a\u7c7b\u548c\u8fc7\u6ee4\u6765\u89e3\u51b3LLM\u5728\u4ece\u5927\u578b\u5de5\u5177\u5e93\u4e2d\u9009\u62e9\u5de5\u5177\u7684\u6311\u6218\uff0c\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u5e76\u964d\u4f4e\u4e86\u5ef6\u8fdf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3LLM\u5728\u4ece\u5927\u578b\u3001\u5c42\u7ea7\u5316\u5de5\u5177\u5e93\u4e2d\u9009\u62e9\u6b63\u786e\u5de5\u5177\u65f6\u9762\u4e34\u7684\u6311\u6218\uff0c\u4f8b\u5982\u6709\u9650\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\u548c\u65e0\u5173\u9009\u9879\u4ea7\u751f\u7684\u566a\u58f0\u5bfc\u81f4\u7684\u4f4e\u9009\u62e9\u51c6\u786e\u6027\u548c\u9ad8\u8ba1\u7b97\u6210\u672c\u3002", "method": "HGMF\u6846\u67b6\u9996\u5148\u5c06\u7528\u6237\u67e5\u8be2\u548c\u6240\u6709\u5de5\u5177\u63cf\u8ff0\u6620\u5c04\u5230\u7edf\u4e00\u7684\u8bed\u4e49\u7a7a\u95f4\u3002\u7136\u540e\uff0c\u5b83\u901a\u8fc7\u9ad8\u65af\u6df7\u5408\u6a21\u578b\uff08GMM\uff09\u5bf9\u670d\u52a1\u5668\u8fdb\u884c\u805a\u7c7b\uff0c\u5e76\u6839\u636e\u67e5\u8be2\u7684\u4f3c\u7136\u5ea6\u5bf9\u670d\u52a1\u5668\u8fdb\u884c\u8fc7\u6ee4\u3002\u968f\u540e\uff0c\u5c06\u76f8\u540c\u7684GMM\u805a\u7c7b\u548c\u8fc7\u6ee4\u65b9\u6cd5\u5e94\u7528\u4e8e\u6240\u9009\u670d\u52a1\u5668\u5173\u8054\u7684\u5de5\u5177\u3002\u8fd9\u79cd\u5206\u5c42\u8fc7\u7a0b\u751f\u6210\u4e86\u4e00\u4e2a\u7d27\u51d1\u3001\u9ad8\u76f8\u5173\u6027\u7684\u5019\u9009\u96c6\uff0c\u4ece\u800c\u7b80\u5316\u4e86LLM\u7684\u6700\u7ec8\u9009\u62e9\u4efb\u52a1\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cHGMF\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u5177\u9009\u62e9\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\u3002", "conclusion": "HGMF\u6846\u67b6\u901a\u8fc7\u5206\u5c42\u805a\u7c7b\u548c\u8fc7\u6ee4\u663e\u8457\u63d0\u9ad8\u4e86\u5de5\u5177\u9009\u62e9\u7684\u51c6\u786e\u6027\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u63a8\u7406\u5ef6\u8fdf\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5927\u89c4\u6a21\u5de5\u5177\u5e93\u4e2d\u7684\u53ef\u6269\u5c55\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.07758", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07758", "abs": "https://arxiv.org/abs/2508.07758", "authors": ["Antonio Rosales", "Alaa Abderrahim", "Markku Suomalainen", "Mikael Haag", "Tapio Heikkil\u00e4"], "title": "Robot and Overhead Crane Collaboration Scheme to Enhance Payload Manipulation", "comment": null, "summary": "This paper presents a scheme to enhance payload manipulation using a robot\ncollaborating with an overhead crane. In the current industrial practice, when\nthe crane's payload has to be accurately manipulated and located in a desired\nposition, the task becomes laborious and risky since the operators have to\nguide the fine motions of the payload by hand. In the proposed collaborative\nscheme, the crane lifts the payload while the robot's end-effector guides it\ntoward the desired position. The only link between the robot and the crane is\nthe interaction force produced during the guiding of the payload. Two\nadmittance transfer functions are considered to accomplish harmless and smooth\ncontact with the payload. The first is used in a position-based admittance\ncontrol integrated with the robot. The second one adds compliance to the crane\nby processing the interaction force through the admittance transfer function to\ngenerate a crane's velocity command that makes the crane follow the payload.\nThen the robot's end-effector and the crane move collaboratively to guide the\npayload to the desired location. A method is presented to design the admittance\ncontrollers that accomplish a fluent robot-crane collaboration. Simulations and\nexperiments validating the scheme potential are shown.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u673a\u5668\u4eba\u4e0e\u8d77\u91cd\u673a\u534f\u540c\u64cd\u7eb5\u91cd\u7269\u7684\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u624b\u52a8\u7cbe\u7ec6\u8c03\u63a7\u91cd\u7269\u5e26\u6765\u7684\u98ce\u9669\u548c\u52b3\u52a8\u5f3a\u5ea6\u3002\u8be5\u65b9\u6848\u5229\u7528\u673a\u5668\u4eba\u5f15\u5bfc\uff0c\u8d77\u91cd\u673a\u8ddf\u968f\uff0c\u901a\u8fc7\u4ea4\u4e92\u529b\u5b9e\u73b0\u4e24\u8005\u534f\u8c03\uff0c\u5e76\u8bbe\u8ba1\u4e86\u987a\u4ece\u63a7\u5236\u5668\u4ee5\u786e\u4fdd\u5e73\u7a33\u64cd\u4f5c\u3002\u4eff\u771f\u548c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6848\u7684\u53ef\u884c\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u5de5\u4e1a\u5b9e\u8df5\u4e2d\uff0c\u8d77\u91cd\u673a\u6709\u6548\u8f7d\u8377\u7684\u7cbe\u7ec6\u64cd\u7eb5\u548c\u7cbe\u786e\u5b9a\u4f4d\u4efb\u52a1\u52b3\u52a8\u5f3a\u5ea6\u5927\u4e14\u98ce\u9669\u9ad8\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u64cd\u4f5c\u5458\u9700\u8981\u624b\u52a8\u5f15\u5bfc\u6709\u6548\u8f7d\u8377\u7684\u7cbe\u7ec6\u8fd0\u52a8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u4f5c\u65b9\u6848\uff0c\u5176\u4e2d\u8d77\u91cd\u673a\u8d77\u540a\u6709\u6548\u8f7d\u8377\uff0c\u673a\u5668\u4eba\u7684\u672b\u7aef\u6267\u884c\u5668\u5c06\u5176\u5f15\u5bfc\u81f3\u76ee\u6807\u4f4d\u7f6e\u3002\u673a\u5668\u4eba\u548c\u8d77\u91cd\u673a\u4e4b\u95f4\u7684\u552f\u4e00\u8054\u7cfb\u662f\u901a\u8fc7\u5f15\u5bfc\u6709\u6548\u8f7d\u8377\u4ea7\u751f\u7684\u4ea4\u4e92\u529b\u3002\u8003\u8651\u4e86\u4e24\u79cd\u987a\u4ece\u4f20\u9012\u51fd\u6570\u6765\u5b9e\u73b0\u65e0\u5bb3\u3001\u5e73\u7a33\u7684\u63a5\u89e6\uff1a\u4e00\u79cd\u7528\u4e8e\u57fa\u4e8e\u4f4d\u7f6e\u7684\u987a\u4ece\u63a7\u5236\uff08\u96c6\u6210\u5728\u673a\u5668\u4eba\u4e2d\uff09\uff1b\u53e6\u4e00\u79cd\u901a\u8fc7\u987a\u4ece\u4f20\u9012\u51fd\u6570\u5904\u7406\u4ea4\u4e92\u529b\uff0c\u751f\u6210\u8d77\u91cd\u673a\u7684\u901f\u5ea6\u6307\u4ee4\uff0c\u4f7f\u8d77\u91cd\u673a\u80fd\u591f\u8ddf\u968f\u6709\u6548\u8f7d\u8377\u3002\u6700\u540e\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bbe\u8ba1\u987a\u4ece\u63a7\u5236\u5668\u7684\u65b9\u6cd5\uff0c\u4ee5\u5b9e\u73b0\u673a\u5668\u4eba-\u8d77\u91cd\u673a\u4e4b\u95f4\u7684\u6d41\u7545\u534f\u4f5c\u3002", "result": "\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6848\u7684\u6f5c\u529b\uff0c\u8be5\u65b9\u6848\u80fd\u591f\u5b9e\u73b0\u673a\u5668\u4eba\u548c\u8d77\u91cd\u673a\u7684\u534f\u540c\u5de5\u4f5c\uff0c\u4ece\u800c\u5f15\u5bfc\u6709\u6548\u8f7d\u8377\u5230\u8fbe\u76ee\u6807\u4f4d\u7f6e\u3002", "conclusion": "\u8be5\u65b9\u6848\u901a\u8fc7\u673a\u5668\u4eba\u4e0e\u8d77\u91cd\u673a\u534f\u540c\u5de5\u4f5c\uff0c\u589e\u5f3a\u4e86\u6709\u6548\u8f7d\u8377\u7684\u64cd\u7eb5\u80fd\u529b\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6f5c\u529b\u3002"}}
{"id": "2508.08049", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08049", "abs": "https://arxiv.org/abs/2508.08049", "authors": ["Junyan Li", "Shengshi Pang"], "title": "Superresolution for two incoherent optical sources with arbitrary intensities in two dimensions", "comment": "15pages, 5 figures", "summary": "The Rayleigh criterion has long served as a fundamental limit for the\nresolution of classical optical imaging. However, recent advances in quantum\nmetrology have led to the quantum superresolution technique that can break\nRayleigh's curse and estimate the separation between a pair of incoherent point\nsources with nonvanishing precision. For two-dimensional optical systems, the\nprecision limit of estimating the whole separation, i.e., the distance, between\ntwo point sources remains unknown so far. In this paper, we investigate the\nestimation precision of the distance between two incoherent point sources with\narbitrary intensities in a two-dimensional imaging system. Through the\nmultiparameter quantum estimation theory, we obtain the ultimate estimation\nprecision of the distance and show that it remains nonzero when the distance\napproaches zero, which surpasses the Rayleigh criterion. We further show that\nthe precision can be enhanced by aligning the two point sources along specific\ndirections if the point-spread functions of the two sources are not circularly\nsymmetric, and find the optimial relative azimuth between the two point sources\nand the highest estimation precision of the distance. In addition to the\ndistance estimation, we also consider the quantum estimation of the relative\nazimuth between two incoherent point sources. A surprising result is that the\nprecision limit of the azimuth decays quadratically with the distance, which\nsuggests that the azimuth cannot be resolved when the two point sources get\nsufficiently close to each other and is therefore inaccessible by the quantum\nsuperresolution technique in this case. This reveals a new and fundamental\nlimitation on the resolvability of two incoherent point sources in\nmulti-dimensional quantum imaging which cannot be addressed by optimizing the\nquantum measurements.", "AI": {"tldr": "\u91cf\u5b50\u8d85\u5206\u8fa8\u7387\u6280\u672f\u5728\u8ddd\u79bb\u4f30\u8ba1\u4e0a\u8d85\u8d8a\u4e86\u745e\u5229\u5224\u636e\uff0c\u4f46\u5728\u65b9\u4f4d\u89d2\u4f30\u8ba1\u4e0a\u5b58\u5728\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u6280\u672f\u5728\u533a\u5206\u4e24\u4e2a\u70b9\u6e90\u7684\u8ddd\u79bb\u4e0a\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5c24\u5176\u662f\u5728\u4e8c\u7ef4\u5149\u5b66\u7cfb\u7edf\u4e2d\uff0c\u8ddd\u79bb\u4f30\u8ba1\u7684\u7cbe\u5ea6\u4e0a\u9650\u4ecd\u7136\u672a\u77e5\u3002", "method": "\u5229\u7528\u591a\u53c2\u6570\u91cf\u5b50\u4f30\u8ba1\u7406\u8bba\uff0c\u7814\u7a76\u4e86\u4e8c\u7ef4\u6210\u50cf\u7cfb\u7edf\u4e2d\u4f30\u8ba1\u4e24\u4e2a\u4e0d\u76f8\u5e72\u70b9\u6e90\u4e4b\u95f4\u8ddd\u79bb\u7684\u7cbe\u5ea6\u3002", "result": "\u63a8\u5bfc\u4e86\u8ddd\u79bb\u4f30\u8ba1\u7684\u6700\u7ec8\u7cbe\u5ea6\uff0c\u5373\u4f7f\u5728\u8ddd\u79bb\u8d8b\u8fd1\u4e8e\u96f6\u65f6\u4e5f\u4fdd\u6301\u975e\u96f6\uff0c\u8d85\u8d8a\u4e86\u745e\u5229\u5224\u636e\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u901a\u8fc7\u4f18\u5316\u70b9\u6e90\u7684\u76f8\u5bf9\u65b9\u4f4d\u89d2\u53ef\u4ee5\u63d0\u9ad8\u8ddd\u79bb\u4f30\u8ba1\u7684\u7cbe\u5ea6\u3002\u7136\u800c\uff0c\u65b9\u4f4d\u89d2\u4f30\u8ba1\u7684\u7cbe\u5ea6\u968f\u8ddd\u79bb\u4e8c\u6b21\u65b9\u8870\u51cf\uff0c\u8868\u660e\u5728\u70b9\u6e90\u975e\u5e38\u63a5\u8fd1\u65f6\uff0c\u65b9\u4f4d\u89d2\u65e0\u6cd5\u88ab\u89e3\u6790\uff0c\u8fd9\u662f\u91cf\u5b50\u8d85\u5206\u8fa8\u7387\u6280\u672f\u65e0\u6cd5\u514b\u670d\u7684\u3002", "conclusion": "\u867d\u7136\u91cf\u5b50\u8d85\u5206\u8fa8\u7387\u6280\u672f\u53ef\u4ee5\u6253\u7834\u745e\u5229\u5224\u636e\u7684\u9650\u5236\uff0c\u4f46\u65b9\u4f4d\u89d2\u4f30\u8ba1\u7684\u7cbe\u5ea6\u968f\u8ddd\u79bb\u4e8c\u6b21\u65b9\u8870\u51cf\uff0c\u5f53\u4e24\u4e2a\u70b9\u6e90\u8db3\u591f\u63a5\u8fd1\u65f6\uff0c\u65b9\u4f4d\u89d2\u5c06\u65e0\u6cd5\u5206\u8fa8\uff0c\u8fd9\u662f\u591a\u7ef4\u91cf\u5b50\u6210\u50cf\u4e2d\u56fa\u6709\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2508.07308", "categories": ["cs.CL", "cs.AI", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07308", "abs": "https://arxiv.org/abs/2508.07308", "authors": ["Cristian Cosentino", "Annamaria Defilippo", "Marco Dossena", "Christopher Irwin", "Sara Joubbi", "Pietro Li\u00f2"], "title": "HealthBranches: Synthesizing Clinically-Grounded Question Answering Datasets via Decision Pathways", "comment": null, "summary": "HealthBranches is a novel benchmark dataset for medical Question-Answering\n(Q&A), specifically designed to evaluate complex reasoning in Large Language\nModels (LLMs). This dataset is generated through a semi-automated pipeline that\ntransforms explicit decision pathways from medical source into realistic\npatient cases with associated questions and answers. Covering 4,063 case\nstudies across 17 healthcare topics, each data point is based on clinically\nvalidated reasoning chains. HealthBranches supports both open-ended and\nmultiple-choice question formats and uniquely includes the full reasoning path\nfor each Q&A. Its structured design enables robust evaluation of LLMs'\nmulti-step inference capabilities, including their performance in structured\nRetrieval-Augmented Generation (RAG) contexts. HealthBranches establishes a\nfoundation for the development of more trustworthy, interpretable, and\nclinically reliable LLMs in high-stakes domains while also serving as a\nvaluable resource for educational purposes.", "AI": {"tldr": "HealthBranches \u662f\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30 LLM \u533b\u5b66\u63a8\u7406\u80fd\u529b\u7684\u65b0\u578b\u6570\u636e\u96c6\uff0c\u5305\u542b 4,063 \u4e2a\u6848\u4f8b\u548c\u5b8c\u6574\u7684\u63a8\u7406\u8def\u5f84\uff0c\u652f\u6301\u5f00\u653e\u5f0f\u548c\u9009\u62e9\u9898\uff0c\u65e8\u5728\u63d0\u9ad8 LLM \u5728\u533b\u7597\u9886\u57df\u7684\u53ef\u9760\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u9886\u57df\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u4e3a\u5f00\u53d1\u66f4\u503c\u5f97\u4fe1\u8d56\u3001\u53ef\u89e3\u91ca\u4e14\u4e34\u5e8a\u53ef\u9760\u7684 LLM \u63d0\u4f9b\u57fa\u7840\u3002", "method": "\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u534a\u81ea\u52a8\u6d41\u6c34\u7ebf\u751f\u6210\uff0c\u5c06\u533b\u5b66\u6e90\u4e2d\u7684\u663e\u5f0f\u51b3\u7b56\u8def\u5f84\u8f6c\u5316\u4e3a\u5305\u542b\u95ee\u9898\u548c\u7b54\u6848\u7684\u771f\u5b9e\u60a3\u8005\u6848\u4f8b\u3002", "result": "HealthBranches \u5305\u542b 17 \u4e2a\u533b\u7597\u4e3b\u9898\u7684 4,063 \u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u652f\u6301\u5f00\u653e\u5f0f\u548c\u9009\u62e9\u9898\u4e24\u79cd\u95ee\u9898\u683c\u5f0f\uff0c\u5e76\u63d0\u4f9b\u5b8c\u6574\u7684\u63a8\u7406\u8def\u5f84\uff0c\u53ef\u7528\u4e8e\u8bc4\u4f30 LLM \u7684\u591a\u6b65\u63a8\u7406\u548c RAG \u80fd\u529b\u3002", "conclusion": "HealthBranches \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u533b\u5b66\u95ee\u7b54\u57fa\u51c6\u6570\u636e\u96c6\uff0c\u65e8\u5728\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u3002\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u534a\u81ea\u52a8\u6d41\u6c34\u7ebf\u751f\u6210\uff0c\u5c06\u533b\u5b66\u6e90\u4e2d\u7684\u663e\u5f0f\u51b3\u7b56\u8def\u5f84\u8f6c\u5316\u4e3a\u771f\u5b9e\u7684\u60a3\u8005\u6848\u4f8b\uff0c\u5e76\u9644\u5e26\u76f8\u5173\u7684\u95ee\u9898\u548c\u7b54\u6848\u3002HealthBranches \u6db5\u76d6\u4e86 17 \u4e2a\u533b\u7597\u4e3b\u9898\u7684 4,063 \u4e2a\u6848\u4f8b\u7814\u7a76\uff0c\u6bcf\u4e2a\u6570\u636e\u70b9\u90fd\u57fa\u4e8e\u7ecf\u8fc7\u4e34\u5e8a\u9a8c\u8bc1\u7684\u63a8\u7406\u94fe\u3002\u8be5\u6570\u636e\u96c6\u652f\u6301\u5f00\u653e\u5f0f\u548c\u9009\u62e9\u9898\u4e24\u79cd\u95ee\u9898\u683c\u5f0f\uff0c\u5e76\u72ec\u7279\u5730\u5305\u542b\u4e86\u6bcf\u4e2a\u95ee\u7b54\u7684\u5b8c\u6574\u63a8\u7406\u8def\u5f84\u3002\u5176\u7ed3\u6784\u5316\u8bbe\u8ba1\u80fd\u591f\u5bf9 LLM \u7684\u591a\u6b65\u63a8\u7406\u80fd\u529b\u8fdb\u884c\u7a33\u5065\u8bc4\u4f30\uff0c\u5305\u62ec\u5728\u7ed3\u6784\u5316\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7684\u4e0a\u4e0b\u6587\u4e2d\u7684\u8868\u73b0\u3002HealthBranches \u4e3a\u5728\u9ad8\u98ce\u9669\u9886\u57df\u5f00\u53d1\u66f4\u503c\u5f97\u4fe1\u8d56\u3001\u53ef\u89e3\u91ca\u4e14\u4e34\u5e8a\u53ef\u9760\u7684 LLM \u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u540c\u65f6\u4e5f\u4e3a\u6559\u80b2\u76ee\u7684\u63d0\u4f9b\u4e86\u5b9d\u8d35\u7684\u8d44\u6e90\u3002"}}
{"id": "2508.06878", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06878", "abs": "https://arxiv.org/abs/2508.06878", "authors": ["Maoxun Yuan", "Duanni Meng", "Ziteng Xi", "Tianyi Zhao", "Shiji Zhao", "Yimian Dai", "Xingxing Wei"], "title": "NS-FPN: Improving Infrared Small Target Detection and Segmentation from Noise Suppression Perspective", "comment": null, "summary": "Infrared small target detection and segmentation (IRSTDS) is a critical yet\nchallenging task in defense and civilian applications, owing to the dim,\nshapeless appearance of targets and severe background clutter. Recent CNN-based\nmethods have achieved promising target perception results, but they only focus\non enhancing feature representation to offset the impact of noise, which\nresults in the increased false alarms problem. In this paper, through analyzing\nthe problem from the frequency domain, we pioneer in improving performance from\nnoise suppression perspective and propose a novel noise-suppression feature\npyramid network (NS-FPN), which integrates a low-frequency guided feature\npurification (LFP) module and a spiral-aware feature sampling (SFS) module into\nthe original FPN structure. The LFP module suppresses the noise features by\npurifying high-frequency components to achieve feature enhancement devoid of\nnoise interference, while the SFS module further adopts spiral sampling to fuse\ntarget-relevant features in feature fusion process. Our NS-FPN is designed to\nbe lightweight yet effective and can be easily plugged into existing IRSTDS\nframeworks. Extensive experiments on the public IRSTDS datasets demonstrate\nthat our method significantly reduces false alarms and achieves superior\nperformance on IRSTDS tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684NS-FPN\u7f51\u7edc\uff0c\u901a\u8fc7\u4f4e\u9891\u5f15\u5bfc\u7279\u5f81\u7eaf\u5316\u548c\u87ba\u65cb\u611f\u77e5\u7279\u5f81\u91c7\u6837\u6765\u6291\u5236\u566a\u58f0\u5e76\u878d\u5408\u76ee\u6807\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ea2\u5916\u5c0f\u76ee\u6807\u68c0\u6d4b\u4e2d\u7684\u865a\u8b66\u95ee\u9898\uff0c\u5e76\u5728\u516c\u5f00\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8eCNN\u7684\u65b9\u6cd5\u867d\u7136\u5728\u76ee\u6807\u611f\u77e5\u65b9\u9762\u53d6\u5f97\u4e86\u6709\u5e0c\u671b\u7684\u7ed3\u679c\uff0c\u4f46\u5b83\u4eec\u53ea\u5173\u6ce8\u589e\u5f3a\u7279\u5f81\u8868\u793a\u4ee5\u62b5\u6d88\u566a\u58f0\u7684\u5f71\u54cd\uff0c\u5bfc\u81f4\u865a\u8b66\u95ee\u9898\u589e\u52a0\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4ece\u566a\u58f0\u6291\u5236\u7684\u89d2\u5ea6\u6765\u63d0\u9ad8\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684NS-FPN\uff08\u566a\u58f0\u6291\u5236\u7279\u5f81\u91d1\u5b57\u5854\u7f51\u7edc\uff09\uff0c\u5b83\u5c06\u4f4e\u9891\u5f15\u5bfc\u7279\u5f81\u7eaf\u5316\uff08LFP\uff09\u6a21\u5757\u548c\u87ba\u65cb\u611f\u77e5\u7279\u5f81\u91c7\u6837\uff08SFS\uff09\u6a21\u5757\u96c6\u6210\u5230\u539f\u59cbFPN\u7ed3\u6784\u4e2d\u3002LFP\u6a21\u5757\u901a\u8fc7\u7eaf\u5316\u9ad8\u9891\u5206\u91cf\u6765\u6291\u5236\u566a\u58f0\u7279\u5f81\uff0c\u5b9e\u73b0\u65e0\u566a\u58f0\u5e72\u6270\u7684\u7279\u5f81\u589e\u5f3a\uff0c\u800cSFS\u6a21\u5757\u5728\u7279\u5f81\u878d\u5408\u8fc7\u7a0b\u4e2d\u91c7\u7528\u87ba\u65cb\u91c7\u6837\u6765\u878d\u5408\u76ee\u6807\u76f8\u5173\u7279\u5f81\u3002", "result": "\u5728\u516c\u5f00\u7684IRSTDS\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u865a\u8b66\uff0c\u5e76\u5728IRSTDS\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5NS-FPN\u65b9\u6cd5\u663e\u8457\u51cf\u5c11\u4e86\u865a\u8b66\uff0c\u5e76\u5728IRSTDS\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07616", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07616", "abs": "https://arxiv.org/abs/2508.07616", "authors": ["Aswin RRV", "Jacob Dineen", "Divij Handa", "Md Nayem Uddin", "Mihir Parmar", "Chitta Baral", "Ben Zhou"], "title": "ThinkTuning: Instilling Cognitive Reflections without Distillation", "comment": "15 pages", "summary": "Recent advances in test-time scaling have led to the emergence of thinking\nLLMs that exhibit self-reflective behaviors and multi-step reasoning. While RL\ndrives this self-improvement paradigm, a recent study (Gandhi et al., 2025)\nshows that RL alone does not truly instill these new reasoning abilities - it\nmerely draws out behaviors already present in the base models. This raises a\nquestion: How can we train the models that don't exhibit such thinking behavior\nto develop it in the first place? To this end, we propose ThinkTuning, a\nGRPO-based interactive training approach where we augment the rollouts of a\nstudent model with the guidance from a teacher model. A simple idea from\nclassroom practice inspires our method: a teacher poses a problem, lets the\nstudent try an answer, then gives corrective feedback -- enough to point the\nmind in the right direction and then show the solution. Each piece of feedback\nreshapes the student's thoughts, leading them to arrive at the correct\nsolution. Similarly, we find that this type of implicit supervision through\nfeedback from a teacher model of the same size improves the reasoning\ncapabilities of the student model. In particular, on average, our method shows\na 3.85% improvement over zero-shot baselines across benchmarks, and on\nMATH-500, AIME and GPQA-Diamond it shows 2.08%, 2.23% and 3.99% improvements\nover the vanilla-GRPO baseline. Source code is available at\nhttps://github.com/3rdAT/ThinkTuning.", "AI": {"tldr": "ThinkTuning \u662f\u4e00\u79cd\u65b0\u9896\u7684\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u4eff\u8bfe\u5802\u6559\u5b66\u4e2d\u7684\u5e08\u751f\u4e92\u52a8\uff0c\u5229\u7528\u4e00\u4e2a\u6559\u5e08\u6a21\u578b\u6765\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u8fdb\u884c\u63a8\u7406\u8bad\u7ec3\uff0c\u4ece\u800c\u6709\u6548\u63d0\u5347\u4e86\u5b66\u751f\u6a21\u578b\u7684\u601d\u8003\u548c\u89e3\u51b3\u95ee\u9898\u7684\u80fd\u529b\u3002", "motivation": "\u5c3d\u7ba1\u6d4b\u8bd5\u65f6\u7f29\u653e\u6280\u672f\u548c\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u8fdb\u6b65\u4f7f\u5f97\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u80fd\u591f\u8fdb\u884c\u591a\u6b65\u63a8\u7406\u548c\u81ea\u6211\u53cd\u601d\uff0c\u4f46\u6709\u7814\u7a76\u8868\u660e RL \u672c\u8eab\u5e76\u4e0d\u80fd\u771f\u6b63\u8d4b\u4e88\u8fd9\u4e9b\u63a8\u7406\u80fd\u529b\uff0c\u800c\u53ea\u662f\u6fc0\u53d1\u4e86\u57fa\u7840\u6a21\u578b\u4e2d\u5df2\u6709\u7684\u884c\u4e3a\u3002\u8fd9\u5f15\u53d1\u4e86\u4e00\u4e2a\u95ee\u9898\uff1a\u5982\u4f55\u8bad\u7ec3\u90a3\u4e9b\u4e0d\u5177\u5907\u8fd9\u4e9b\u601d\u8003\u80fd\u529b\u7684\u6a21\u578b\uff0c\u4f7f\u5b83\u4eec\u9996\u5148\u53d1\u5c55\u51fa\u8fd9\u79cd\u80fd\u529b\uff1f", "method": "ThinkTuning \u662f\u4e00\u79cd\u57fa\u4e8e GRPO \u7684\u4ea4\u4e92\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u63d0\u4f9b\u53cd\u9988\u6765\u6307\u5bfc\u5b66\u751f\u6a21\u578b\u8fdb\u884c\u5b66\u4e60\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6559\u5e08\u6a21\u578b\u4f1a\u63d0\u51fa\u95ee\u9898\uff0c\u8ba9\u5b66\u751f\u6a21\u578b\u5c1d\u8bd5\u56de\u7b54\uff0c\u7136\u540e\u63d0\u4f9b\u7ea0\u6b63\u6027\u53cd\u9988\uff0c\u6700\u7ec8\u5c55\u793a\u6b63\u786e\u7b54\u6848\uff0c\u4ee5\u6b64\u6765\u91cd\u5851\u5b66\u751f\u6a21\u578b\u7684\u601d\u8003\u8fc7\u7a0b\u3002", "result": "ThinkTuning \u65b9\u6cd5\u5728\u5e73\u5747\u800c\u8a00\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6bd4\u96f6\u6837\u672c\u57fa\u7ebf\u63d0\u9ad8\u4e86 3.85%\u3002\u5728 MATH-500\u3001AIME \u548c GPQA-Diamond \u6570\u636e\u96c6\u4e0a\uff0c\u5206\u522b\u6bd4 vanilla-GRPO \u57fa\u7ebf\u63d0\u9ad8\u4e86 2.08%\u30012.23% \u548c 3.99%\u3002", "conclusion": "ThinkTuning \u662f\u4e00\u79cd\u57fa\u4e8e GRPO \u7684\u4ea4\u4e92\u5f0f\u8bad\u7ec3\u65b9\u6cd5\uff0c\u901a\u8fc7\u6559\u5e08\u6a21\u578b\u7684\u9690\u5f0f\u76d1\u7763\u6765\u63d0\u9ad8\u5b66\u751f\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.07032", "categories": ["cs.LG", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.07032", "abs": "https://arxiv.org/abs/2508.07032", "authors": ["Tiantian He", "Keyue Jiang", "An Zhao", "Anna Schroder", "Elinor Thompson", "Sonja Soskic", "Frederik Barkhof", "Daniel C. Alexander"], "title": "A Stage-Aware Mixture of Experts Framework for Neurodegenerative Disease Progression Modelling", "comment": null, "summary": "The long-term progression of neurodegenerative diseases is commonly\nconceptualized as a spatiotemporal diffusion process that consists of a graph\ndiffusion process across the structural brain connectome and a localized\nreaction process within brain regions. However, modeling this progression\nremains challenging due to 1) the scarcity of longitudinal data obtained\nthrough irregular and infrequent subject visits and 2) the complex interplay of\npathological mechanisms across brain regions and disease stages, where\ntraditional models assume fixed mechanisms throughout disease progression. To\naddress these limitations, we propose a novel stage-aware Mixture of Experts\n(MoE) framework that explicitly models how different contributing mechanisms\ndominate at different disease stages through time-dependent expert\nweighting.Data-wise, we utilize an iterative dual optimization method to\nproperly estimate the temporal position of individual observations,\nconstructing a co hort-level progression trajectory from irregular snapshots.\nModel-wise, we enhance the spatial component with an inhomogeneous graph neural\ndiffusion model (IGND) that allows diffusivity to vary based on node states and\ntime, providing more flexible representations of brain networks. We also\nintroduce a localized neural reaction module to capture complex dynamics beyond\nstandard processes.The resulting IGND-MoE model dynamically integrates these\ncomponents across temporal states, offering a principled way to understand how\nstage-specific pathological mechanisms contribute to progression. The\nstage-wise weights yield novel clinical insights that align with literature,\nsuggesting that graph-related processes are more influential at early stages,\nwhile other unknown physical processes become dominant later on.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aIGND-MoE\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u7684\u957f\u671f\u8fdb\u5c55\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u6574\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\u548c\u4e13\u5bb6\u6df7\u5408\u6a21\u578b\uff0c\u80fd\u591f\u5904\u7406\u4e0d\u89c4\u5219\u7684\u7eb5\u5411\u6570\u636e\uff0c\u5e76\u6355\u6349\u75be\u75c5\u4e0d\u540c\u9636\u6bb5\u7684\u590d\u6742\u75c5\u7406\u673a\u5236\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u4e34\u5e8a\u89c1\u89e3\uff0c\u5e76\u4e0e\u73b0\u6709\u6587\u732e\u8bc1\u636e\u76f8\u7b26\u3002", "motivation": "\u5c3d\u7ba1\u957f\u671f\u795e\u7ecf\u9000\u884c\u6027\u75be\u75c5\u8fdb\u5c55\u901a\u5e38\u88ab\u6982\u5ff5\u5316\u4e3a\u65f6\u7a7a\u6269\u6563\u8fc7\u7a0b\uff0c\u4f46\u7531\u4e8e\u7eb5\u5411\u6570\u636e\u7a00\u758f\u4e14\u75be\u75c5\u673a\u5236\u590d\u6742\u591a\u53d8\uff0c\u4f20\u7edf\u6a21\u578b\u96be\u4ee5\u5bf9\u5176\u8fdb\u884c\u5efa\u6a21\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5904\u7406\u4e0d\u89c4\u5219\u6570\u636e\u5e76\u9002\u5e94\u75be\u75c5\u8fdb\u5c55\u4e2d\u4e0d\u540c\u673a\u5236\u7684\u6a21\u578b\u7684\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u3001\u611f\u77e5\u9636\u6bb5\u7684\u4e13\u5bb6\u6df7\u5408\uff08MoE\uff09\u6846\u67b6\uff0c\u901a\u8fc7\u65f6\u53d8\u4e13\u5bb6\u6743\u91cd\u660e\u786e\u5730\u6a21\u62df\u4e0d\u540c\u8d21\u732e\u673a\u5236\u5728\u4e0d\u540c\u75be\u75c5\u9636\u6bb5\u7684\u4e3b\u5bfc\u4f5c\u7528\u3002\u5728\u6570\u636e\u65b9\u9762\uff0c\u5229\u7528\u8fed\u4ee3\u53cc\u91cd\u4f18\u5316\u65b9\u6cd5\u4f30\u8ba1\u4e2a\u4f53\u89c2\u6d4b\u7684\u65f6\u95f4\u4f4d\u7f6e\uff0c\u6784\u5efa\u4e86\u4ece\u4e0d\u89c4\u5219\u5feb\u7167\u7684\u961f\u5217\u7ea7\u5206\u5316\u8f68\u8ff9\u3002\u5728\u6a21\u578b\u65b9\u9762\uff0c\u901a\u8fc7\u975e\u9f50\u6b21\u56fe\u795e\u7ecf\u7f51\u7edc\u6269\u6563\u6a21\u578b\uff08IGND\uff09\u589e\u5f3a\u4e86\u7a7a\u95f4\u6210\u5206\uff0c\u8be5\u6a21\u578b\u5141\u8bb8\u6269\u6563\u7cfb\u6570\u6839\u636e\u8282\u70b9\u72b6\u6001\u548c\u65f6\u95f4\u53d8\u5316\uff0c\u4e3a\u5927\u8111\u7f51\u7edc\u63d0\u4f9b\u66f4\u7075\u6d3b\u7684\u8868\u793a\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u5c40\u90e8\u795e\u7ecf\u53cd\u5e94\u6a21\u5757\u6765\u6355\u83b7\u8d85\u51fa\u6807\u51c6\u8fc7\u7a0b\u7684\u590d\u6742\u52a8\u6001\u3002", "result": "IGND-MoE\u6a21\u578b\u5728\u5904\u7406\u7eb5\u5411\u6570\u636e\u7a00\u758f\u6027\u548c\u75be\u75c5\u673a\u5236\u590d\u6742\u6027\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5e76\u4e14\u6a21\u578b\u4ea7\u751f\u7684\u9636\u6bb5\u6027\u6743\u91cd\u4e0e\u73b0\u6709\u7814\u7a76\u6587\u732e\u4e00\u81f4\uff0c\u63ed\u793a\u4e86\u56fe\u76f8\u5173\u8fc7\u7a0b\u5728\u65e9\u671f\u9636\u6bb5\u7684\u5f71\u54cd\u529b\u66f4\u5927\uff0c\u800c\u540e\u671f\u5219\u7531\u5176\u4ed6\u672a\u77e5\u7269\u7406\u8fc7\u7a0b\u4e3b\u5bfc\u3002", "conclusion": "\u63d0\u51fa\u7684IGND-MoE\u6a21\u578b\u80fd\u591f\u52a8\u6001\u5730\u6574\u5408\u65f6\u95f4\u72b6\u6001\u4e0b\u7684\u5404\u4e2a\u7ec4\u4ef6\uff0c\u4e3a\u7406\u89e3\u7279\u5b9a\u9636\u6bb5\u7684\u75c5\u7406\u673a\u5236\u5982\u4f55\u4fc3\u6210\u5206\u5316\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u539f\u5219\u7684\u65b9\u6cd5\u3002\u8be5\u6a21\u578b\u7684\u65f6\u95f4\u9636\u6bb5\u6743\u91cd\u4ea7\u751f\u4e86\u65b0\u7684\u4e34\u5e8a\u89c1\u89e3\uff0c\u8fd9\u4e9b\u89c1\u89e3\u4e0e\u73b0\u6709\u6587\u732e\u4e00\u81f4\uff0c\u8868\u660e\u56fe\u76f8\u5173\u8fc7\u7a0b\u5728\u65e9\u671f\u9636\u6bb5\u66f4\u5177\u5f71\u54cd\u529b\uff0c\u800c\u5728\u540e\u671f\uff0c\u5176\u4ed6\u672a\u77e5\u7684\u7269\u7406\u8fc7\u7a0b\u5219\u53d8\u5f97\u5360\u4e3b\u5bfc\u5730\u4f4d\u3002"}}
{"id": "2508.07770", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07770", "abs": "https://arxiv.org/abs/2508.07770", "authors": ["Yizheng Zhang", "Zhenjun Yu", "Jiaxin Lai", "Cewu Lu", "Lei Han"], "title": "AgentWorld: An Interactive Simulation Platform for Scene Construction and Mobile Robotic Manipulation", "comment": "Accepted by Conference on Robot Learning 2025", "summary": "We introduce AgentWorld, an interactive simulation platform for developing\nhousehold mobile manipulation capabilities. Our platform combines automated\nscene construction that encompasses layout generation, semantic asset\nplacement, visual material configuration, and physics simulation, with a\ndual-mode teleoperation system supporting both wheeled bases and humanoid\nlocomotion policies for data collection. The resulting AgentWorld Dataset\ncaptures diverse tasks ranging from primitive actions (pick-and-place,\npush-pull, etc.) to multistage activities (serve drinks, heat up food, etc.)\nacross living rooms, bedrooms, and kitchens. Through extensive benchmarking of\nimitation learning methods including behavior cloning, action chunking\ntransformers, diffusion policies, and vision-language-action models, we\ndemonstrate the dataset's effectiveness for sim-to-real transfer. The\nintegrated system provides a comprehensive solution for scalable robotic skill\nacquisition in complex home environments, bridging the gap between\nsimulation-based training and real-world deployment. The code, datasets will be\navailable at https://yizhengzhang1.github.io/agent_world/", "AI": {"tldr": "AgentWorld \u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6a21\u62df\u5e73\u53f0\uff0c\u7528\u4e8e\u5f00\u53d1\u5bb6\u5ead\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\uff0c\u5e76\u9644\u5e26\u4e00\u4e2a\u6355\u83b7\u5404\u79cd\u5bb6\u5ead\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u53ef\u901a\u8fc7\u884c\u4e3a\u514b\u9686\u3001\u52a8\u4f5c\u5206\u5757\u8f6c\u6362\u5668\u3001\u6269\u6563\u7b56\u7565\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u8fdb\u884c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4ee5\u5b9e\u73b0\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u8fc1\u79fb\u3002", "motivation": "\u4e3a\u4e86\u5728\u5bb6\u5ead\u73af\u5883\u4e2d\u5f00\u53d1\u79fb\u52a8\u64cd\u4f5c\u80fd\u529b\u3002", "method": "\u8be5\u5e73\u53f0\u7ed3\u5408\u4e86\u81ea\u52a8\u573a\u666f\u6784\u5efa\uff08\u5305\u62ec\u5e03\u5c40\u751f\u6210\u3001\u8bed\u4e49\u8d44\u4ea7\u653e\u7f6e\u3001\u89c6\u89c9\u6750\u6599\u914d\u7f6e\u548c\u7269\u7406\u6a21\u62df\uff09\u4ee5\u53ca\u652f\u6301\u8f6e\u5f0f\u5e95\u76d8\u548c\u4eba\u5f62\u8fd0\u52a8\u7b56\u7565\u7684\u53cc\u6a21\u5f0f\u9065\u64cd\u4f5c\u7cfb\u7edf\uff0c\u7528\u4e8e\u6570\u636e\u6536\u96c6\u3002", "result": "AgentWorld \u6570\u636e\u96c6\u6355\u83b7\u4e86\u4ece\u57fa\u672c\u52a8\u4f5c\uff08\u6293\u53d6\u548c\u653e\u7f6e\u3001\u63a8\u62c9\u7b49\uff09\u5230\u591a\u9636\u6bb5\u6d3b\u52a8\uff08\u670d\u52a1\u996e\u6599\u3001\u52a0\u70ed\u98df\u7269\u7b49\uff09\u7684\u5404\u79cd\u4efb\u52a1\uff0c\u6db5\u76d6\u5ba2\u5385\u3001\u5367\u5ba4\u548c\u53a8\u623f\u3002\u901a\u8fc7\u5bf9\u884c\u4e3a\u514b\u9686\u3001\u52a8\u4f5c\u5206\u5757\u8f6c\u6362\u5668\u3001\u6269\u6563\u7b56\u7565\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u7b49\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u7684\u5e7f\u6cdb\u57fa\u51c6\u6d4b\u8bd5\uff0c\u8bc1\u660e\u4e86\u8be5\u6570\u636e\u96c6\u5728\u6a21\u62df\u5230\u73b0\u5b9e\u8fc1\u79fb\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u4e3a\u5728\u590d\u6742\u5bb6\u5ead\u73af\u5883\u4e2d\u6269\u5c55\u673a\u5668\u4eba\u6280\u80fd\u83b7\u53d6\u63d0\u4f9b\u4e86\u5168\u9762\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5f25\u5408\u4e86\u57fa\u4e8e\u4eff\u771f\u7684\u8bad\u7ec3\u4e0e\u5b9e\u9645\u90e8\u7f72\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002"}}
{"id": "2508.08065", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08065", "abs": "https://arxiv.org/abs/2508.08065", "authors": ["Charalampos Antonakos"], "title": "The role of Quantum Diffusion Flux in Super-Luminal Wave Packets", "comment": "no comments", "summary": "In this short-length paper, we will present some math that play a central\nrole in quantum hydrodynamics and that were presented by Mita in the past. In\nthis formulation of QM, a quantity is involved which is called the diffusion\nflux. As we will see, this quantity plays a crucial role on the evolution of\nwave packets. More specifically, we will initially briefly explain some results\nderived in Mita's papers, by analyzing Gaussian and soliton wave packets, while\nour main focus is on the role of diffusion in the evolution of super-luminal\nwave packets", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86Mita\u5728\u91cf\u5b50\u6d41\u4f53\u529b\u5b66\u4e2d\u7684\u6570\u5b66\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u6269\u6563\u901a\u91cf\u5728\u6ce2\u5305\u6f14\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u5c24\u5176\u662f\u5728\u8d85\u5149\u901f\u6ce2\u5305\u7684\u60c5\u51b5\u4e0b\u3002", "motivation": "\u672c\u6587\u7684\u52a8\u673a\u662f\u4ecb\u7ecdMita\u5728\u91cf\u5b50\u6d41\u4f53\u529b\u5b66\u4e2d\u63d0\u51fa\u7684\u5173\u952e\u6570\u5b66\uff0c\u5e76\u6df1\u5165\u63a2\u8ba8\u6269\u6563\u901a\u91cf\u5728\u6ce2\u5305\u6f14\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u7279\u522b\u662f\u5728\u8d85\u5149\u901f\u6ce2\u5305\u7684\u60c5\u51b5\u4e0b\u3002", "method": "\u672c\u6587\u9996\u5148\u7b80\u8981\u56de\u987e\u4e86Mita\u8bba\u6587\u4e2d\u7684\u4e00\u4e9b\u7ed3\u679c\uff0c\u901a\u8fc7\u5206\u6790\u9ad8\u65af\u548c\u5b64\u5b50\u6ce2\u5305\uff0c\u7136\u540e\u5c06\u91cd\u70b9\u653e\u5728\u6269\u6563\u5728\u8d85\u5149\u901f\u6ce2\u5305\u6f14\u5316\u4e2d\u7684\u4f5c\u7528\u3002", "result": "\u672c\u6587\u5206\u6790\u4e86\u9ad8\u65af\u548c\u5b64\u5b50\u6ce2\u5305\uff0c\u5e76\u91cd\u70b9\u7814\u7a76\u4e86\u6269\u6563\u5728\u8d85\u5149\u901f\u6ce2\u5305\u6f14\u5316\u4e2d\u7684\u4f5c\u7528\u3002", "conclusion": "\u8be5\u8bba\u6587\u4e3b\u8981\u8ba8\u8bba\u4e86Mita\u63d0\u51fa\u7684\u5728\u91cf\u5b50\u6d41\u4f53\u529b\u5b66\u4e2d\u8d77\u6838\u5fc3\u4f5c\u7528\u7684\u6570\u5b66\uff0c\u5e76\u5206\u6790\u4e86\u6269\u6563\u901a\u91cf\u5728\u6ce2\u5305\u6f14\u5316\u4e2d\u7684\u4f5c\u7528\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u8d85\u5149\u901f\u6ce2\u5305\u7684\u6f14\u5316\u3002"}}
{"id": "2508.07321", "categories": ["cs.CL", "cs.AI", "cs.LG", "I.2.7"], "pdf": "https://arxiv.org/pdf/2508.07321", "abs": "https://arxiv.org/abs/2508.07321", "authors": ["Shubhra Ghosh", "Abhilekh Borah", "Aditya Kumar Guru", "Kripabandhu Ghosh"], "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated Factual Question Answering", "comment": null, "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available.", "AI": {"tldr": "LLM \u5728\u5904\u7406\u88ab\u6df7\u6dc6\u7684\u95ee\u9898\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u7814\u7a76\u8005\u63d0\u51fa\u4e86 ObfusQA \u6846\u67b6\u6765\u8bc4\u4f30\u8fd9\u4e00\u95ee\u9898\uff0c\u5e76\u5df2\u516c\u5f00 ObfusQAte \u6280\u672f\u3002", "motivation": "\u8bc4\u4f30 LLM \u5728\u9762\u5bf9\u88ab\u6df7\u6dc6\u7248\u672c\u7684\u95ee\u9898\u65f6\u7684\u9c81\u68d2\u6027\uff0c\u56e0\u4e3a\u76ee\u524d\u6ca1\u6709\u5df2\u77e5\u7814\u7a76\u5bf9\u6b64\u8fdb\u884c\u6d4b\u8bd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ObfusQAte \u7684\u65b0\u9896\u6280\u672f\uff0c\u5e76\u57fa\u4e8e\u6b64\u6280\u672f\u5f15\u5165\u4e86 ObfusQA \u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u591a\u5c42\u6df7\u6dc6\u7ea7\u522b\uff0c\u65e8\u5728\u68c0\u67e5 LLM \u5728\u4e09\u4e2a\u4e0d\u540c\u7ef4\u5ea6\u4e0a\u7684\u80fd\u529b\uff1a(i) \u547d\u540d\u5b9e\u4f53\u95f4\u63a5\u3001(ii) \u5e72\u6270\u9879\u95f4\u63a5\u548c (iii) \u4e0a\u4e0b\u6587\u8fc7\u8f7d\u3002", "result": "LLM \u5728\u9762\u5bf9\u8fd9\u4e9b\u65e5\u76ca\u590d\u6742\u7684\u95ee\u7b54\u53d8\u4f53\u65f6\uff0c\u5b58\u5728\u5931\u6548\u6216\u4ea7\u751f\u5e7b\u89c9\u56de\u5e94\u7684\u503e\u5411\u3002", "conclusion": "LLMs \u5728\u9762\u5bf9\u8fd9\u4e9b\u65e5\u76ca\u590d\u6742\u7684\u95ee\u7b54\u53d8\u4f53\u65f6\uff0c\u5b58\u5728\u5931\u6548\u6216\u4ea7\u751f\u5e7b\u89c9\u56de\u5e94\u7684\u503e\u5411\u3002"}}
{"id": "2508.06891", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06891", "abs": "https://arxiv.org/abs/2508.06891", "authors": ["Melika Filvantorkaman", "Mohsen Piri", "Maral Filvan Torkaman", "Ashkan Zabihi", "Hamidreza Moradi"], "title": "Fusion-Based Brain Tumor Classification Using Deep Learning and Explainable AI, and Rule-Based Reasoning", "comment": "37 pages, 6 figures", "summary": "Accurate and interpretable classification of brain tumors from magnetic\nresonance imaging (MRI) is critical for effective diagnosis and treatment\nplanning. This study presents an ensemble-based deep learning framework that\ncombines MobileNetV2 and DenseNet121 convolutional neural networks (CNNs) using\na soft voting strategy to classify three common brain tumor types: glioma,\nmeningioma, and pituitary adenoma. The models were trained and evaluated on the\nFigshare dataset using a stratified 5-fold cross-validation protocol. To\nenhance transparency and clinical trust, the framework integrates an\nExplainable AI (XAI) module employing Grad-CAM++ for class-specific saliency\nvisualization, alongside a symbolic Clinical Decision Rule Overlay (CDRO) that\nmaps predictions to established radiological heuristics. The ensemble\nclassifier achieved superior performance compared to individual CNNs, with an\naccuracy of 91.7%, precision of 91.9%, recall of 91.7%, and F1-score of 91.6%.\nGrad-CAM++ visualizations revealed strong spatial alignment between model\nattention and expert-annotated tumor regions, supported by Dice coefficients up\nto 0.88 and IoU scores up to 0.78. Clinical rule activation further validated\nmodel predictions in cases with distinct morphological features. A\nhuman-centered interpretability assessment involving five board-certified\nradiologists yielded high Likert-scale scores for both explanation usefulness\n(mean = 4.4) and heatmap-region correspondence (mean = 4.0), reinforcing the\nframework's clinical relevance. Overall, the proposed approach offers a robust,\ninterpretable, and generalizable solution for automated brain tumor\nclassification, advancing the integration of deep learning into clinical\nneurodiagnostics.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408MobileNetV2\u548cDenseNet121\u7684\u96c6\u6210\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7528\u4e8e\u5bf9\u8111\u80bf\u7624MRI\u56fe\u50cf\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u7ed3\u5408XAI\u548c\u4e34\u5e8a\u89c4\u5219\u6765\u63d0\u9ad8\u6a21\u578b\u7684\u53ef\u89e3\u91ca\u6027\u3002\u8be5\u6846\u67b6\u5728\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u5f97\u5230\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7684\u79ef\u6781\u8bc4\u4ef7\u3002", "motivation": "\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u4eceMRI\u56fe\u50cf\u4e2d\u5bf9\u8111\u80bf\u7624\u8fdb\u884c\u5206\u7c7b\uff0c\u5bf9\u4e8e\u6709\u6548\u7684\u8bca\u65ad\u548c\u6cbb\u7597\u8ba1\u5212\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u96c6\u6210\u5b66\u4e60\u7684\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u7ed3\u5408\u4e86MobileNetV2\u548cDenseNet121\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5e76\u91c7\u7528\u8f6f\u6295\u7968\u7b56\u7565\u5bf9\u4e09\u79cd\u5e38\u89c1\u7684\u8111\u80bf\u7624\uff08\u80f6\u8d28\u7624\u3001\u8111\u819c\u7624\u548c\u5782\u4f53\u7624\uff09\u8fdb\u884c\u5206\u7c7b\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u8fd8\u96c6\u6210\u4e86\u4e00\u4e2aXAI\u6a21\u5757\uff08Grad-CAM++\uff09\u7528\u4e8e\u53ef\u89c6\u5316\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7b26\u53f7\u5316\u4e34\u5e8a\u51b3\u7b56\u89c4\u5219\u53e0\u52a0\uff08CDRO\uff09\u6a21\u5757\uff0c\u5c06\u9884\u6d4b\u6620\u5c04\u5230\u653e\u5c04\u5b66\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002", "result": "\u8be5\u96c6\u6210\u5b66\u4e60\u5206\u7c7b\u5668\u5b9e\u73b0\u4e8691.7%\u7684\u51c6\u786e\u7387\u300191.9%\u7684\u7cbe\u786e\u7387\u300191.7%\u7684\u53ec\u56de\u7387\u548c91.6%\u7684F1\u5206\u6570\uff0c\u4f18\u4e8e\u5355\u72ec\u7684CNN\u6a21\u578b\u3002Grad-CAM++\u53ef\u89c6\u5316\u663e\u793a\u4e86\u6a21\u578b\u6ce8\u610f\u529b\u548c\u4e13\u5bb6\u6807\u6ce8\u7684\u80bf\u7624\u533a\u57df\u5728\u7a7a\u95f4\u4e0a\u9ad8\u5ea6\u4e00\u81f4\uff08Dice\u7cfb\u6570\u9ad8\u8fbe0.88\uff0cIoU\u5206\u6570\u9ad8\u8fbe0.78\uff09\u3002\u4e34\u5e8a\u89c4\u5219\u6fc0\u6d3b\u4e5f\u9a8c\u8bc1\u4e86\u6a21\u578b\u5728\u5177\u6709\u660e\u663e\u5f62\u6001\u7279\u5f81\u7684\u75c5\u4f8b\u4e2d\u7684\u9884\u6d4b\u3002\u5bf95\u540d\u653e\u5c04\u79d1\u533b\u751f\u8fdb\u884c\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u89e3\u91ca\u7684\u6709\u7528\u6027\uff08\u5e73\u57474.4\uff09\u548c\u70ed\u56fe-\u533a\u57df\u5bf9\u5e94\u6027\uff08\u5e73\u57474.0\uff09\u7684\u674e\u514b\u7279\u91cf\u8868\u8bc4\u5206\u90fd\u5f88\u9ad8\uff0c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u4e34\u5e8a\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u8111\u80bf\u7624\u5206\u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5f3a\u5927\u3001\u53ef\u89e3\u91ca\u4e14\u53ef\u6cdb\u5316\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u4fc3\u8fdb\u4e86\u6df1\u5ea6\u5b66\u4e60\u5728\u4e34\u5e8a\u795e\u7ecf\u8bca\u65ad\u5b66\u4e2d\u7684\u6574\u5408\u3002"}}
{"id": "2508.07628", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07628", "abs": "https://arxiv.org/abs/2508.07628", "authors": ["Daniel Essien", "Suresh Neethirajan"], "title": "Multimodal AI Systems for Enhanced Laying Hen Welfare Assessment and Productivity Optimization", "comment": "66 pages, 7 figures, 11 tables", "summary": "The future of poultry production depends on a paradigm shift replacing\nsubjective, labor-intensive welfare checks with data-driven, intelligent\nmonitoring ecosystems. Traditional welfare assessments-limited by human\nobservation and single-sensor data-cannot fully capture the complex,\nmultidimensional nature of laying hen welfare in modern farms. Multimodal\nArtificial Intelligence (AI) offers a breakthrough, integrating visual,\nacoustic, environmental, and physiological data streams to reveal deeper\ninsights into avian welfare dynamics. This investigation highlights multimodal\nAs transformative potential, showing that intermediate (feature-level) fusion\nstrategies achieve the best balance between robustness and performance under\nreal-world poultry conditions, and offer greater scalability than early or late\nfusion approaches. Key adoption barriers include sensor fragility in harsh farm\nenvironments, high deployment costs, inconsistent behavioral definitions, and\nlimited cross-farm generalizability. To address these, we introduce two novel\nevaluation tools - the Domain Transfer Score (DTS) to measure model\nadaptability across diverse farm settings, and the Data Reliability Index (DRI)\nto assess sensor data quality under operational constraints. We also propose a\nmodular, context-aware deployment framework designed for laying hen\nenvironments, enabling scalable and practical integration of multimodal\nsensing. This work lays the foundation for a transition from reactive, unimodal\nmonitoring to proactive, precision-driven welfare systems that unite\nproductivity with ethical, science based animal care.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u591a\u6a21\u5f0f\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u548c\u65b0\u7684\u8bc4\u4f30\u5de5\u5177\uff08DTS\u3001DRI\uff09\u6765\u6539\u8fdb\u86cb\u9e21\u798f\u5229\u76d1\u63a7\u7684\u65b9\u6cd5\uff0c\u4ee5\u514b\u670d\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u4e3a\u5b9e\u73b0\u751f\u4ea7\u529b\u548c\u52a8\u7269\u798f\u5229\u7684\u7ed3\u5408\u5960\u5b9a\u57fa\u7840\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u53d7\u9650\u4e8e\u4eba\u7c7b\u89c2\u5bdf\u548c\u5355\u4e00\u4f20\u611f\u5668\u6570\u636e\uff0c\u65e0\u6cd5\u5b8c\u5168\u6355\u6349\u73b0\u4ee3\u519c\u573a\u86cb\u9e21\u798f\u5229\u7684\u590d\u6742\u6027\u3001\u591a\u7ef4\u5ea6\u6027\u8d28\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u7531\u6570\u636e\u9a71\u52a8\u3001\u667a\u80fd\u5316\u7684\u76d1\u63a7\u751f\u6001\u7cfb\u7edf\u6765\u53d6\u4ee3\u4e3b\u89c2\u3001\u52b3\u52a8\u5bc6\u96c6\u578b\u7684\u798f\u5229\u68c0\u67e5\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u542c\u89c9\u3001\u73af\u5883\u548c\u751f\u7406\u6570\u636e\u6d41\u7684\u591a\u6a21\u5f0f\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6765\u8bc4\u4f30\u86cb\u9e21\u798f\u5229\u3002\u7814\u7a76\u5f3a\u8c03\u4e86\u4e2d\u95f4\uff08\u7279\u5f81\u7ea7\uff09\u878d\u5408\u7b56\u7565\u5728\u73b0\u5b9e\u5bb6\u79bd\u6761\u4ef6\u4e0b\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u4e4b\u95f4\u53d6\u5f97\u4e86\u6700\u4f73\u5e73\u8861\uff0c\u5e76\u4e14\u6bd4\u65e9\u671f\u6216\u665a\u671f\u878d\u5408\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002\u5f15\u5165\u4e86\u57df\u8fc1\u79fb\u5f97\u5206\uff08DTS\uff09\u548c\u6570\u636e\u53ef\u9760\u6027\u6307\u6570\uff08DRI\uff09\u6765\u8bc4\u4f30\u6a21\u578b\u7684\u9002\u5e94\u6027\u548c\u4f20\u611f\u5668\u6570\u636e\u7684\u8d28\u91cf\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u3001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u90e8\u7f72\u6846\u67b6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u4e2d\u95f4\uff08\u7279\u5f81\u7ea7\uff09\u878d\u5408\u7b56\u7565\u5728\u771f\u5b9e\u5bb6\u79bd\u73af\u5883\u4e0b\u8868\u73b0\u51fa\u6700\u4f73\u7684\u9c81\u68d2\u6027\u548c\u6027\u80fd\u5e73\u8861\uff0c\u5e76\u4e14\u6bd4\u65e9\u671f\u6216\u665a\u671f\u878d\u5408\u5177\u6709\u66f4\u597d\u7684\u53ef\u6269\u5c55\u6027\u3002\u901a\u8fc7\u5f15\u5165DTS\u548cDRI\uff0c\u4e3a\u8bc4\u4f30\u6a21\u578b\u9002\u5e94\u6027\u548c\u6570\u636e\u8d28\u91cf\u63d0\u4f9b\u4e86\u65b0\u7684\u5de5\u5177\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u6a21\u5757\u5316\u7684\u90e8\u7f72\u6846\u67b6\uff0c\u4ee5\u5b9e\u73b0\u591a\u6a21\u5f0f\u4f20\u611f\u7684\u53ef\u6269\u5c55\u548c\u5b9e\u9645\u96c6\u6210\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u4e3a\u4ece\u88ab\u52a8\u3001\u5355\u4e00\u6a21\u5f0f\u76d1\u63a7\u5230\u4e3b\u52a8\u3001\u7531\u7cbe\u5ea6\u9a71\u52a8\u7684\u798f\u5229\u7cfb\u7edf\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u8be5\u7cfb\u7edf\u5c06\u751f\u4ea7\u529b\u4e0e\u5408\u4e4e\u9053\u5fb7\u7684\u3001\u57fa\u4e8e\u79d1\u5b66\u7684\u52a8\u7269\u62a4\u7406\u76f8\u7ed3\u5408\u3002"}}
{"id": "2508.07814", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07814", "abs": "https://arxiv.org/abs/2508.07814", "authors": ["Malaika Zafar", "Roohan Ahmed Khan", "Faryal Batool", "Yasheerah Yaqoot", "Ziang Guo", "Mikhail Litvinov", "Aleksey Fedoseev", "Dzmitry Tsetserukou"], "title": "SwarmVLM: VLM-Guided Impedance Control for Autonomous Navigation of Heterogeneous Robots in Dynamic Warehousing", "comment": null, "summary": "With the growing demand for efficient logistics, unmanned aerial vehicles\n(UAVs) are increasingly being paired with automated guided vehicles (AGVs).\nWhile UAVs offer the ability to navigate through dense environments and varying\naltitudes, they are limited by battery life, payload capacity, and flight\nduration, necessitating coordinated ground support.\n  Focusing on heterogeneous navigation, SwarmVLM addresses these limitations by\nenabling semantic collaboration between UAVs and ground robots through\nimpedance control. The system leverages the Vision Language Model (VLM) and the\nRetrieval-Augmented Generation (RAG) to adjust impedance control parameters in\nresponse to environmental changes. In this framework, the UAV acts as a leader\nusing Artificial Potential Field (APF) planning for real-time navigation, while\nthe ground robot follows via virtual impedance links with adaptive link\ntopology to avoid collisions with short obstacles.\n  The system demonstrated a 92% success rate across 12 real-world trials. Under\noptimal lighting conditions, the VLM-RAG framework achieved 8% accuracy in\nobject detection and selection of impedance parameters. The mobile robot\nprioritized short obstacle avoidance, occasionally resulting in a lateral\ndeviation of up to 50 cm from the UAV path, which showcases safe navigation in\na cluttered setting.", "AI": {"tldr": "SwarmVLM\u7cfb\u7edf\u901a\u8fc7VLM\u548cRAG\u6280\u672f\uff0c\u5229\u7528UAV\u548c\u5730\u9762\u673a\u5668\u4eba\u7684\u534f\u540c\uff0c\u89e3\u51b3\u4e86UAV\u7684\u7eed\u822a\u548c\u8d1f\u8f7d\u9650\u5236\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u5b89\u5168\u7684\u7269\u6d41\u914d\u9001\u3002\u7cfb\u7edf\u5728\u771f\u5b9e\u73af\u5883\u4e2d\u8868\u73b0\u51fa\u9ad8\u6210\u529f\u7387\u548c\u907f\u969c\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u5bf9\u9ad8\u6548\u7269\u6d41\u9700\u6c42\u7684\u589e\u957f\uff0c\u65e0\u4eba\u673a\uff08UAV\uff09\u8d8a\u6765\u8d8a\u591a\u5730\u4e0e\u81ea\u52a8\u5bfc\u5f15\u8f66\uff08AGV\uff09\u914d\u5bf9\u3002\u65e0\u4eba\u673a\u867d\u7136\u80fd\u591f\u7a7f\u8d8a\u5bc6\u96c6\u73af\u5883\u548c\u4e0d\u540c\u9ad8\u5ea6\uff0c\u4f46\u5176\u7535\u6c60\u5bff\u547d\u3001\u8d1f\u8f7d\u80fd\u529b\u548c\u98de\u884c\u65f6\u95f4\u6709\u9650\uff0c\u9700\u8981\u5730\u9762\u652f\u63f4\u3002SwarmVLM\u65e8\u5728\u901a\u8fc7\u963b\u6297\u63a7\u5236\u5b9e\u73b0UAV\u548c\u5730\u9762\u673a\u5668\u4eba\u4e4b\u95f4\u7684\u8bed\u4e49\u534f\u4f5c\uff0c\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\u3002", "method": "\u8be5\u7cfb\u7edf\u5229\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u548c\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u6765\u54cd\u5e94\u73af\u5883\u53d8\u5316\uff0c\u4ece\u800c\u8c03\u6574\u963b\u6297\u63a7\u5236\u53c2\u6570\u3002\u5728\u8be5\u6846\u67b6\u4e2d\uff0cUAV\u4f7f\u7528\u4eba\u5de5\u52bf\u573a\uff08APF\uff09\u89c4\u5212\u8fdb\u884c\u5b9e\u65f6\u5bfc\u822a\uff0c\u800c\u5730\u9762\u673a\u5668\u4eba\u5219\u901a\u8fc7\u5177\u6709\u81ea\u9002\u5e94\u8fde\u63a5\u62d3\u6251\u7684\u865a\u62df\u963b\u6297\u94fe\u63a5\u8fdb\u884c\u8ddf\u968f\uff0c\u4ee5\u907f\u514d\u4e0e\u77ed\u969c\u788d\u7269\u53d1\u751f\u78b0\u649e\u3002", "result": "\u8be5\u7cfb\u7edf\u572812\u6b21\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u4e2d\u6210\u529f\u7387\u8fbe\u5230\u4e8692%\u3002\u5728\u6700\u4f73\u5149\u7167\u6761\u4ef6\u4e0b\uff0cVLM-RAG\u6846\u67b6\u5728\u7269\u4f53\u68c0\u6d4b\u548c\u963b\u6297\u53c2\u6570\u9009\u62e9\u65b9\u9762\u8fbe\u5230\u4e868%\u7684\u51c6\u786e\u7387\u3002\u79fb\u52a8\u673a\u5668\u4eba\u4f18\u5148\u907f\u5f00\u77ed\u969c\u788d\u7269\uff0c\u6709\u65f6\u4f1a\u5bfc\u81f4\u4e0eUAV\u8def\u5f84\u4ea7\u751f\u9ad8\u8fbe50\u5398\u7c73\u7684\u6a2a\u5411\u504f\u5dee\uff0c\u8fd9\u5c55\u793a\u4e86\u5728\u6df7\u4e71\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u7684\u80fd\u529b\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u572812\u6b21\u771f\u5b9e\u4e16\u754c\u8bd5\u9a8c\u4e2d\u6210\u529f\u7387\u8fbe\u5230\u4e8692%\uff0c\u5728\u6700\u4f73\u5149\u7167\u6761\u4ef6\u4e0b\uff0cVLM-RAG\u6846\u67b6\u5728\u7269\u4f53\u68c0\u6d4b\u548c\u963b\u6297\u53c2\u6570\u9009\u62e9\u65b9\u9762\u8fbe\u5230\u4e868%\u7684\u51c6\u786e\u7387\u3002\u79fb\u52a8\u673a\u5668\u4eba\u4f18\u5148\u907f\u5f00\u77ed\u969c\u788d\u7269\uff0c\u6709\u65f6\u4f1a\u5bfc\u81f4\u4e0eUAV\u8def\u5f84\u4ea7\u751f\u9ad8\u8fbe50\u5398\u7c73\u7684\u6a2a\u5411\u504f\u5dee\uff0c\u8fd9\u5c55\u793a\u4e86\u5728\u6df7\u4e71\u73af\u5883\u4e2d\u5b89\u5168\u5bfc\u822a\u7684\u80fd\u529b\u3002"}}
{"id": "2508.08092", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08092", "abs": "https://arxiv.org/abs/2508.08092", "authors": ["Spiros Kechrimparis", "Nix Barnett", "Mile Gu", "Hyukjoon Kwon"], "title": "How Quantum Agents Can Change Which Strategies Are More Complex", "comment": "24 pages, 26 figures", "summary": "Whether winning blackjack or navigating busy streets, achieving desired\noutcomes requires agents to execute adaptive strategies, strategies where\nactions depend contextually on past events. In complexity science, this\nmotivates memory as an operational quantifier of complexity: given two\nstrategies, the more complex one demands the agent to track more about the\npast. Here, we show that conclusions about complexity fundamentally depend on\nwhether agents can process and store quantum information. Thus, while classical\nagents might find Strategy A more complex to execute than Strategy B, quantum\nagents can reach the opposite conclusion. We derive sufficient conditions for\nsuch contradictory conclusions and illustrate the phenomenon across multiple\nscenarios. As a byproduct, our results yield an information-theoretic lower\nbound on the minimal memory required by any agent - classical or quantum - to\nexecute a given strategy.", "AI": {"tldr": "\u7b56\u7565\u7684\u590d\u6742\u6027\u53d6\u51b3\u4e8e\u662f\u5426\u80fd\u5904\u7406\u91cf\u5b50\u4fe1\u606f\uff1b\u91cf\u5b50\u4ee3\u7406\u4e0e\u7ecf\u5178\u4ee3\u7406\u53ef\u80fd\u5bf9\u7b56\u7565\u590d\u6742\u6027\u5f97\u51fa\u76f8\u53cd\u7ed3\u8bba\u3002", "motivation": "\u5728\u590d\u6742\u6027\u79d1\u5b66\u4e2d\uff0c\u8bb0\u5fc6\u88ab\u7528\u4f5c\u590d\u6742\u6027\u7684\u91cf\u5316\u6307\u6807\uff0c\u4f46\u8fd9\u79cd\u91cf\u5316\u53ef\u80fd\u53d7\u5230\u4ee3\u7406\u5904\u7406\u4fe1\u606f\u80fd\u529b\u7684\u9650\u5236\uff0c\u7279\u522b\u662f\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u63a8\u5bfc\u5145\u5206\u6761\u4ef6\u5e76\u5c55\u793a\u8de8\u591a\u4e2a\u573a\u666f\u7684\u73b0\u8c61\u6765\u8bf4\u660e\uff0c\u5728\u4ee3\u7406\u662f\u5426\u80fd\u591f\u5904\u7406\u548c\u5b58\u50a8\u91cf\u5b50\u4fe1\u606f\u7684\u60c5\u51b5\u4e0b\uff0c\u4f1a\u5f97\u51fa\u5173\u4e8e\u7b56\u7565\u590d\u6742\u6027\u7684\u77db\u76fe\u7ed3\u8bba\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u4fe1\u606f\u8bba\u7684\u4e0b\u9650\uff0c\u7528\u4e8e\u786e\u5b9a\u6267\u884c\u7ed9\u5b9a\u7b56\u7565\u6240\u9700\u7684\u6700\u5c0f\u8bb0\u5fc6\u91cf\uff0c\u8be5\u4e0b\u9650\u9002\u7528\u4e8e\u7ecf\u5178\u548c\u91cf\u5b50\u4ee3\u7406\u3002", "conclusion": "\u4ee3\u7406\u6267\u884c\u7b56\u7565\u7684\u590d\u6742\u6027\u8bc4\u4f30\u53d6\u51b3\u4e8e\u4ee3\u7406\u662f\u5426\u80fd\u591f\u5904\u7406\u548c\u5b58\u50a8\u91cf\u5b50\u4fe1\u606f\u3002\u4e0e\u7ecf\u5178\u4ee3\u7406\u4e0d\u540c\uff0c\u91cf\u5b50\u4ee3\u7406\u53ef\u80fd\u4f1a\u5f97\u51fa\u76f8\u53cd\u7684\u590d\u6742\u6027\u7ed3\u8bba\uff0c\u8fd9\u8868\u660e\u91cf\u5b50\u4fe1\u606f\u5904\u7406\u5728\u7406\u89e3\u548c\u91cf\u5316\u590d\u6742\u6027\u65b9\u9762\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u3002"}}
{"id": "2508.07325", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07325", "abs": "https://arxiv.org/abs/2508.07325", "authors": ["Dean Geckt", "Melinda Fricke", "Shuly Wintner"], "title": "Strategies of Code-switching in Human-Machine Dialogs", "comment": null, "summary": "Most people are multilingual, and most multilinguals code-switch, yet the\ncharacteristics of code-switched language are not fully understood. We\ndeveloped a chatbot capable of completing a Map Task with human participants\nusing code-switched Spanish and English. In two experiments, we prompted the\nbot to code-switch according to different strategies, examining (1) the\nfeasibility of such experiments for investigating bilingual language use, and\n(2) whether participants would be sensitive to variations in discourse and\ngrammatical patterns. Participants generally enjoyed code-switching with our\nbot as long as it produced predictable code-switching behavior; when\ncode-switching was random or ungrammatical (as when producing unattested\nincongruent mixed-language noun phrases, such as `la fork'), participants\nenjoyed the task less and were less successful at completing it. These results\nunderscore the potential downsides of deploying insufficiently developed\nmultilingual language technology, while also illustrating the promise of such\ntechnology for conducting research on bilingual language use.", "AI": {"tldr": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u8fdb\u884c\u897f\u73ed\u7259\u8bed\u548c\u82f1\u8bed\u8bed\u7801\u8f6c\u6362\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u7528\u4e8e\u5730\u56fe\u4efb\u52a1\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u53c2\u4e0e\u8005\u504f\u597d\u53ef\u9884\u6d4b\u7684\u8bed\u7801\u8f6c\u6362\uff0c\u968f\u673a\u6216\u4e0d\u5408\u8bed\u6cd5\u7684\u8f6c\u6362\u4f1a\u964d\u4f4e\u53c2\u4e0e\u5ea6\u548c\u4efb\u52a1\u6210\u529f\u7387\u3002\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u5b8c\u5584\u591a\u8bed\u8a00\u6280\u672f\u7684\u91cd\u8981\u6027\uff0c\u5e76\u6307\u51fa\u4e86\u5176\u5728\u53cc\u8bed\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u5927\u591a\u6570\u4eba\u662f\u591a\u8bed\u8005\uff0c\u5e76\u4e14\u5927\u591a\u6570\u591a\u8bed\u8005\u4f1a\u8fdb\u884c\u8bed\u7801\u8f6c\u6362\uff0c\u4f46\u8bed\u7801\u8f6c\u6362\u8bed\u8a00\u7684\u7279\u5f81\u5c1a\u672a\u88ab\u5b8c\u5168\u7406\u89e3\u3002\u672c\u7814\u7a76\u65e8\u5728\u6df1\u5165\u7406\u89e3\u8bed\u7801\u8f6c\u6362\u8bed\u8a00\u7684\u7279\u5f81\uff0c\u5e76\u63a2\u7d22\u5176\u5728\u4eba\u5de5\u667a\u80fd\u548c\u8bed\u8a00\u5b66\u7814\u7a76\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u672c\u7814\u7a76\u5f00\u53d1\u4e86\u4e00\u4e2a\u80fd\u591f\u4e0e\u4eba\u7c7b\u53c2\u4e0e\u8005\u5b8c\u6210\u5730\u56fe\u4efb\u52a1\u7684\u804a\u5929\u673a\u5668\u4eba\uff0c\u8be5\u673a\u5668\u4eba\u80fd\u591f\u8fdb\u884c\u897f\u73ed\u7259\u8bed\u548c\u82f1\u8bed\u4e4b\u95f4\u7684\u8bed\u8a00\u5207\u6362\u3002\u901a\u8fc7\u4e24\u4e2a\u5b9e\u9a8c\uff0c\u7814\u7a76\u8005\u5f15\u5bfc\u673a\u5668\u4eba\u91c7\u7528\u4e0d\u540c\u7684\u8bed\u8a00\u5207\u6362\u7b56\u7565\uff0c\u65e8\u5728\u63a2\u8ba8\uff081\uff09\u6b64\u7c7b\u5b9e\u9a8c\u5728\u7814\u7a76\u53cc\u8bed\u8bed\u8a00\u4f7f\u7528\u65b9\u9762\u7684\u53ef\u884c\u6027\uff0c\u4ee5\u53ca\uff082\uff09\u53c2\u4e0e\u8005\u662f\u5426\u80fd\u611f\u77e5\u5230\u8bdd\u8bed\u548c\u8bed\u6cd5\u6a21\u5f0f\u7684\u5dee\u5f02\u3002", "result": "\u53c2\u4e0e\u8005\u666e\u904d\u559c\u6b22\u4e0e\u673a\u5668\u4eba\u8fdb\u884c\u8bed\u7801\u8f6c\u6362\u7684\u4ea4\u6d41\uff0c\u524d\u63d0\u662f\u673a\u5668\u4eba\u7684\u8bed\u7801\u8f6c\u6362\u884c\u4e3a\u5177\u6709\u53ef\u9884\u6d4b\u6027\u3002\u7136\u800c\uff0c\u5f53\u8bed\u7801\u8f6c\u6362\u968f\u673a\u6216\u4e0d\u7b26\u5408\u8bed\u6cd5\u89c4\u8303\u65f6\uff08\u4f8b\u5982\uff0c\u751f\u6210\u4e86\u672a\u7ecf\u9a8c\u8bc1\u7684\u4e0d\u4e00\u81f4\u6df7\u5408\u8bed\u8a00\u540d\u8bcd\u77ed\u8bed\uff0c\u5982\u201cla fork\u201d\uff09\uff0c\u53c2\u4e0e\u8005\u5219\u4e0d\u592a\u4eab\u53d7\u8be5\u4efb\u52a1\uff0c\u5e76\u4e14\u5728\u5b8c\u6210\u4efb\u52a1\u65b9\u9762\u4e5f\u8868\u73b0\u4e0d\u4f73\u3002", "conclusion": "\u53c2\u4e0e\u8005\u5728\u4e0e\u673a\u5668\u4eba\u8fdb\u884c\u591a\u8bed\u8a00\u4ea4\u6d41\u65f6\uff0c\u53ea\u8981\u673a\u5668\u4eba\u7684\u8bed\u8a00\u5207\u6362\u65b9\u5f0f\u5177\u6709\u53ef\u9884\u6d4b\u6027\uff0c\u4ed6\u4eec\u901a\u5e38\u90fd\u80fd\u4eab\u53d7\u8be5\u8fc7\u7a0b\u3002\u5f53\u8bed\u8a00\u5207\u6362\u968f\u673a\u6216\u4e0d\u5408\u4e4e\u8bed\u6cd5\uff08\u4f8b\u5982\uff0c\u751f\u6210\u672a\u7ecf\u8bc1\u5b9e\u7684\u6df7\u5408\u8bed\u8a00\u540d\u8bcd\u77ed\u8bed\uff0c\u5982\u201cla fork\u201d\uff09\u65f6\uff0c\u53c2\u4e0e\u8005\u7684\u53c2\u4e0e\u5ea6\u4f1a\u4e0b\u964d\uff0c\u4efb\u52a1\u6210\u529f\u7387\u4e5f\u4f1a\u964d\u4f4e\u3002\u8fd9\u8868\u660e\u4e86\u4e0d\u5b8c\u5584\u7684\u591a\u8bed\u8a00\u8bed\u8a00\u6280\u672f\u53ef\u80fd\u5e26\u6765\u7684\u5f0a\u7aef\uff0c\u540c\u65f6\u4e5f\u8bc1\u660e\u4e86\u8fd9\u7c7b\u6280\u672f\u5728\u7814\u7a76\u53cc\u8bed\u8bed\u8a00\u4f7f\u7528\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.06895", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06895", "abs": "https://arxiv.org/abs/2508.06895", "authors": ["Jianting Tang", "Yubo Wang", "Haoyu Cao", "Linli Xu"], "title": "BASIC: Boosting Visual Alignment with Intrinsic Refined Embeddings in Multimodal Large Language Models", "comment": "Accepted to ICCV 2025", "summary": "Mainstream Multimodal Large Language Models (MLLMs) achieve visual\nunderstanding by using a vision projector to bridge well-pretrained vision\nencoders and large language models (LLMs). The inherent gap between visual and\ntextual modalities makes the embeddings from the vision projector critical for\nvisual comprehension. However, current alignment approaches treat visual\nembeddings as contextual cues and merely apply auto-regressive supervision to\ntextual outputs, neglecting the necessity of introducing equivalent direct\nvisual supervision, which hinders the potential finer alignment of visual\nembeddings. In this paper, based on our analysis of the refinement process of\nvisual embeddings in the LLM's shallow layers, we propose BASIC, a method that\nutilizes refined visual embeddings within the LLM as supervision to directly\nguide the projector in generating initial visual embeddings. Specifically, the\nguidance is conducted from two perspectives: (i) optimizing embedding\ndirections by reducing angles between initial and supervisory embeddings in\nsemantic space; (ii) improving semantic matching by minimizing disparities\nbetween the logit distributions of both visual embeddings. Without additional\nsupervisory models or artificial annotations, BASIC significantly improves the\nperformance of MLLMs across a wide range of benchmarks, demonstrating the\neffectiveness of our introduced direct visual supervision.", "AI": {"tldr": "MLLMs use vision projectors to connect vision encoders and LLMs. The paper proposes BASIC to improve visual embedding alignment by using refined embeddings within the LLM as direct supervision for the projector, optimizing embedding directions and semantic matching. This method enhances MLLM performance.", "motivation": "Current alignment approaches for MLLMs treat visual embeddings as contextual cues and apply auto-regressive supervision to textual outputs, neglecting direct visual supervision. This hinders finer alignment of visual embeddings, which are critical for visual comprehension due to the gap between visual and textual modalities.", "method": "BASIC utilizes refined visual embeddings within the LLM as supervision to directly guide the projector in generating initial visual embeddings. Guidance is conducted by optimizing embedding directions (reducing angles between initial and supervisory embeddings) and improving semantic matching (minimizing disparities between logit distributions).", "result": "BASIC significantly improves MLLM performance across various benchmarks without additional supervisory models or artificial annotations.", "conclusion": "BASIC, a method that utilizes refined visual embeddings within the LLM as supervision to directly guide the projector in generating initial visual embeddings, significantly improves the performance of MLLMs across a wide range of benchmarks, demonstrating the effectiveness of direct visual supervision without additional supervisory models or artificial annotations."}}
{"id": "2508.07642", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07642", "abs": "https://arxiv.org/abs/2508.07642", "authors": ["Tianyi Ma", "Yue Zhang", "Zehao Wang", "Parisa Kordjamshidi"], "title": "Breaking Down and Building Up: Mixture of Skill-Based Vision-and-Language Navigation Agents", "comment": "18 pages, 5 Figures,", "summary": "Vision-and-Language Navigation (VLN) poses significant challenges in enabling\nagents to interpret natural language instructions and navigate complex 3D\nenvironments. While recent progress has been driven by large-scale pre-training\nand data augmentation, current methods still struggle to generalize to unseen\nscenarios, particularly when complex spatial and temporal reasoning is\nrequired. In this work, we propose SkillNav, a modular framework that\nintroduces structured, skill-based reasoning into Transformer-based VLN agents.\nOur method decomposes navigation into a set of interpretable atomic skills\n(e.g., Vertical Movement, Area and Region Identification, Stop and Pause), each\nhandled by a specialized agent. We then introduce a novel zero-shot\nVision-Language Model (VLM)-based router, which dynamically selects the most\nsuitable agent at each time step by aligning sub-goals with visual observations\nand historical actions. SkillNav achieves a new state-of-the-art performance on\nthe R2R benchmark and demonstrates strong generalization to the GSA-R2R\nbenchmark that includes novel instruction styles and unseen environments.", "AI": {"tldr": "SkillNav\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5bfc\u822a\u5206\u89e3\u4e3a\u53ef\u89e3\u91ca\u7684\u6280\u80fd\u5e76\u4f7f\u7528VLM\u8def\u7531\u5668\u6765\u63d0\u9ad8VLN\u667a\u80fd\u4f53\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u5f53\u524d\u7684VLN\u65b9\u6cd5\u5728\u6cdb\u5316\u5230\u9700\u8981\u590d\u6742\u7a7a\u95f4\u548c\u65f6\u95f4\u63a8\u7406\u7684\u672a\u77e5\u573a\u666f\u65f6\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002\u76ee\u524d\u7684\u8fdb\u6b65\u4e3b\u8981\u4f9d\u8d56\u4e8e\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u6570\u636e\u589e\u5f3a\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSkillNav\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u5bfc\u822a\u5206\u89e3\u4e3a\u4e00\u7cfb\u5217\u53ef\u89e3\u91ca\u7684\u539f\u5b50\u6280\u80fd\uff08\u5982\u5782\u76f4\u79fb\u52a8\u3001\u533a\u57df\u8bc6\u522b\u3001\u505c\u6b62\u548c\u6682\u505c\uff09\uff0c\u5e76\u7531\u4e13\u95e8\u7684\u667a\u80fd\u4f53\u5904\u7406\u3002\u5f15\u5165\u4e86\u4e00\u4e2a\u57fa\u4e8eVLM\u7684\u65b0\u578b\u96f6\u6837\u672c\u8def\u7531\u5668\uff0c\u901a\u8fc7\u5c06\u5b50\u76ee\u6807\u4e0e\u89c6\u89c9\u89c2\u5bdf\u548c\u5386\u53f2\u52a8\u4f5c\u5bf9\u9f50\uff0c\u52a8\u6001\u5730\u4e3a\u6bcf\u4e2a\u65f6\u95f4\u6b65\u9009\u62e9\u6700\u5408\u9002\u7684\u667a\u80fd\u4f53\u3002", "result": "SkillNav\u5728R2R\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728GSA-R2R\u57fa\u51c6\u4e0a\u5c55\u73b0\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SkillNav\u5728R2R\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u5e76\u5bf9GSA-R2R\u57fa\u51c6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u8be5\u57fa\u51c6\u5305\u542b\u65b0\u9896\u7684\u6307\u4ee4\u98ce\u683c\u548c\u672a\u77e5\u7684\u73af\u5883\u3002"}}
{"id": "2508.07054", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07054", "abs": "https://arxiv.org/abs/2508.07054", "authors": ["Ziqi Zhang", "Ali Shahin Shamsabadi", "Hanxiao Lu", "Yifeng Cai", "Hamed Haddadi"], "title": "Membership and Memorization in LLM Knowledge Distillation", "comment": null, "summary": "Recent advances in Knowledge Distillation (KD) aim to mitigate the high\ncomputational demands of Large Language Models (LLMs) by transferring knowledge\nfrom a large ''teacher'' to a smaller ''student'' model. However, students may\ninherit the teacher's privacy when the teacher is trained on private data. In\nthis work, we systematically characterize and investigate membership and\nmemorization privacy risks inherent in six LLM KD techniques. Using\ninstruction-tuning settings that span seven NLP tasks, together with three\nteacher model families (GPT-2, LLAMA-2, and OPT), and various size student\nmodels, we demonstrate that all existing LLM KD approaches carry membership and\nmemorization privacy risks from the teacher to its students. However, the\nextent of privacy risks varies across different KD techniques. We\nsystematically analyse how key LLM KD components (KD objective functions,\nstudent training data and NLP tasks) impact such privacy risks. We also\ndemonstrate a significant disagreement between memorization and membership\nprivacy risks of LLM KD techniques. Finally, we characterize per-block privacy\nrisk and demonstrate that the privacy risk varies across different blocks by a\nlarge margin.", "AI": {"tldr": "\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u6240\u6709\u73b0\u6709\u65b9\u6cd5\u5747\u5982\u6b64\uff0c\u4f46\u98ce\u9669\u7a0b\u5ea6\u4e0d\u540c\uff0c\u4e14\u98ce\u9669\u5728\u6a21\u578b\u4e0d\u540c\u90e8\u5206\u548c\u8bb0\u5fc6/\u6210\u5458\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u5dee\u5f02\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6280\u672f\u867d\u7136\u65e8\u5728\u964d\u4f4e\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u4f46\u53ef\u80fd\u5c06\u6559\u5e08\u6a21\u578b\u8bad\u7ec3\u4e2d\u7684\u9690\u79c1\u6cc4\u9732\u7ed9\u5b66\u751f\u6a21\u578b\u3002", "method": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u8868\u5f81\u548c\u7814\u7a76\u4e86\u516d\u79cd\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u6280\u672f\u4e2d\u56fa\u6709\u7684\u6210\u5458\u8d44\u683c\u548c\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\u3002\u7814\u7a76\u4eba\u5458\u5728\u8de8\u8d8a\u4e03\u4e2a\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u7684\u6307\u4ee4\u8c03\u6574\u8bbe\u7f6e\u4e2d\uff0c\u7ed3\u5408\u4e09\u79cd\u6559\u5e08\u6a21\u578b\u7cfb\u5217\uff08GPT-2\u3001LLAMA-2 \u548c OPT\uff09\u548c\u4e0d\u540c\u5927\u5c0f\u7684\u5b66\u751f\u6a21\u578b\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u4ee5\u5206\u6790\u5173\u952e\u7684 KD \u7ec4\u4ef6\uff08\u5982 KD \u76ee\u6807\u51fd\u6570\u3001\u5b66\u751f\u8bad\u7ec3\u6570\u636e\u548c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\uff09\u5982\u4f55\u5f71\u54cd\u9690\u79c1\u98ce\u9669\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u6240\u6709\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u65b9\u6cd5\u90fd\u5b58\u5728\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u98ce\u9669\u7a0b\u5ea6\u56e0 KD \u6280\u672f\u800c\u5f02\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u8bb0\u5fc6\u548c\u6210\u5458\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e14\u9690\u79c1\u98ce\u9669\u5728\u6a21\u578b\u7684\u4e0d\u540c\u90e8\u5206\uff08\u6309\u5757\u5212\u5206\uff09\u4e5f\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u6240\u6709\u73b0\u6709\u7684\u5927\u8bed\u8a00\u6a21\u578b\u77e5\u8bc6\u84b8\u998f\uff08KD\uff09\u65b9\u6cd5\u90fd\u5b58\u5728\u4ece\u6559\u5e08\u6a21\u578b\u5230\u5b66\u751f\u6a21\u578b\u7684\u6210\u5458\u548c\u8bb0\u5fc6\u9690\u79c1\u98ce\u9669\uff0c\u4f46\u4e0d\u540c KD \u6280\u672f\u4e4b\u95f4\u7684\u9690\u79c1\u98ce\u9669\u7a0b\u5ea6\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u8bb0\u5fc6\u548c\u6210\u5458\u9690\u79c1\u98ce\u9669\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0c\u5e76\u4e14\u9690\u79c1\u98ce\u9669\u56e0\u6a21\u578b\u5757\u800c\u5f02\u3002"}}
{"id": "2508.07839", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07839", "abs": "https://arxiv.org/abs/2508.07839", "authors": ["Qiaoqiao Ren", "Tony Belpaeme"], "title": "Touch Speaks, Sound Feels: A Multimodal Approach to Affective and Social Touch from Robots to Humans", "comment": null, "summary": "Affective tactile interaction constitutes a fundamental component of human\ncommunication. In natural human-human encounters, touch is seldom experienced\nin isolation; rather, it is inherently multisensory. Individuals not only\nperceive the physical sensation of touch but also register the accompanying\nauditory cues generated through contact. The integration of haptic and auditory\ninformation forms a rich and nuanced channel for emotional expression. While\nextensive research has examined how robots convey emotions through facial\nexpressions and speech, their capacity to communicate social gestures and\nemotions via touch remains largely underexplored. To address this gap, we\ndeveloped a multimodal interaction system incorporating a 5*5 grid of 25\nvibration motors synchronized with audio playback, enabling robots to deliver\ncombined haptic-audio stimuli. In an experiment involving 32 Chinese\nparticipants, ten emotions and six social gestures were presented through\nvibration, sound, or their combination. Participants rated each stimulus on\narousal and valence scales. The results revealed that (1) the combined\nhaptic-audio modality significantly enhanced decoding accuracy compared to\nsingle modalities; (2) each individual channel-vibration or sound-effectively\nsupported certain emotions recognition, with distinct advantages depending on\nthe emotional expression; and (3) gestures alone were generally insufficient\nfor conveying clearly distinguishable emotions. These findings underscore the\nimportance of multisensory integration in affective human-robot interaction and\nhighlight the complementary roles of haptic and auditory cues in enhancing\nemotional communication.", "AI": {"tldr": "\u89e6\u89c9\u548c\u58f0\u97f3\u7684\u7ed3\u5408\u6bd4\u5355\u72ec\u4f7f\u7528\u4efb\u4f55\u4e00\u79cd\u90fd\u80fd\u66f4\u597d\u5730\u4f20\u8fbe\u60c5\u611f\u548c\u793e\u4ea4\u624b\u52bf\u3002\u5355\u72ec\u7684\u624b\u52bf\u4e0d\u8db3\u4ee5\u4f20\u8fbe\u60c5\u611f\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u901a\u8fc7\u9762\u90e8\u8868\u60c5\u548c\u8bed\u97f3\u4f20\u8fbe\u60c5\u611f\u7684\u7814\u7a76\u8f83\u591a\uff0c\u4f46\u901a\u8fc7\u89e6\u89c9\u4f20\u8fbe\u793e\u4ea4\u624b\u52bf\u548c\u60c5\u611f\u7684\u80fd\u529b\u5374\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u5305\u542b25\u4e2a\u632f\u52a8\u9a6c\u8fbe\u76845*5\u7f51\u683c\u7684\u591a\u6a21\u6001\u4ea4\u4e92\u7cfb\u7edf\uff0c\u5e76\u540c\u6b65\u97f3\u9891\u64ad\u653e\uff0c\u4f7f\u673a\u5668\u4eba\u80fd\u591f\u4f20\u9012\u89e6\u89c9-\u97f3\u9891\u7ec4\u5408\u523a\u6fc0\u3002", "result": "1. \u89e6\u89c9-\u97f3\u9891\u7ec4\u5408\u6a21\u6001\u663e\u8457\u63d0\u9ad8\u4e86\u89e3\u7801\u51c6\u786e\u6027\uff1b2. \u5355\u72ec\u7684\u89e6\u89c9\u6216\u542c\u89c9\u901a\u9053\u5747\u80fd\u6709\u6548\u652f\u6301\u67d0\u4e9b\u60c5\u7eea\u7684\u8bc6\u522b\uff1b3. \u624b\u52bf\u672c\u8eab\u4e0d\u8db3\u4ee5\u4f20\u8fbe\u6e05\u6670\u53ef\u8fa8\u7684\u60c5\u611f\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86\u591a\u611f\u5b98\u6574\u5408\u5728\u60c5\u611f\u4eba\u673a\u4ea4\u4e92\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u5e76\u7a81\u51fa\u4e86\u89e6\u89c9\u548c\u542c\u89c9\u7ebf\u7d22\u5728\u589e\u5f3a\u60c5\u611f\u4ea4\u6d41\u65b9\u9762\u7684\u4e92\u8865\u4f5c\u7528\u3002"}}
{"id": "2508.08116", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08116", "abs": "https://arxiv.org/abs/2508.08116", "authors": ["Oscar Higgott", "Benjamin Anker", "Matt McEwen", "Dripto M. Debroy"], "title": "Handling fabrication defects in hex-grid surface codes", "comment": "7 pages, 6 figures", "summary": "Recent work has shown that a hexagonal grid qubit layout, with only three\ncouplers per qubit, is sufficient to implement the surface code with\nperformance comparable to that of a traditional four-coupler layout [McEwen et\nal., 2023]. In this work we propose a method for handling broken qubits and\ncouplers even in hex-grid surface code architectures, using an extension of the\nLUCI framework [Debroy et al., 2024]. We show that for isolated broken qubits,\nthe circuit distance drops by one, while for isolated broken couplers, the\ndistance drops by one in one or both bases. By providing a viable dropout\nstrategy, we have removed a critical roadblock to the implementation of\nhexagonal qubit grids in hardware for large-scale quantum error correction.", "AI": {"tldr": "\u516d\u8fb9\u5f62\u6805\u683c\u8868\u9762\u7801\u53ef\u4ee5\u4f7f\u7528LUCI\u6846\u67b6\u7684\u6269\u5c55\u6765\u5904\u7406\u635f\u574f\u7684\u91cf\u5b50\u6bd4\u7279\u548c\u8026\u5408\u5668\uff0c\u4ece\u800c\u4e3a\u5927\u89c4\u6a21\u91cf\u5b50\u7ea0\u9519\u94fa\u5e73\u9053\u8def\u3002", "motivation": "\u4e3a\u4e86\u5728\u786c\u4ef6\u4e2d\u5b9e\u73b0\u5927\u89c4\u6a21\u91cf\u5b50\u7ea0\u9519\u7684\u516d\u8fb9\u5f62\u91cf\u5b50\u6bd4\u7279\u6805\u683c\uff0c\u9700\u8981\u4e00\u79cd\u5904\u7406\u635f\u574f\u7684\u91cf\u5b50\u6bd4\u7279\u548c\u8026\u5408\u5668\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u4f7f\u7528LUCI\u6846\u67b6\u7684\u6269\u5c55\u65b9\u6cd5\u6765\u5904\u7406\u516d\u8fb9\u5f62\u6805\u683c\u8868\u9762\u7801\u67b6\u6784\u4e2d\u7684\u635f\u574f\u91cf\u5b50\u6bd4\u7279\u548c\u8026\u5408\u5668\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8e\u5b64\u7acb\u7684\u635f\u574f\u91cf\u5b50\u6bd4\u7279\uff0c\u7535\u8def\u8ddd\u79bb\u51cf\u5c11\u4e00\u4e2a\uff1b\u5bf9\u4e8e\u5b64\u7acb\u7684\u635f\u574f\u8026\u5408\u5668\uff0c\u7535\u8def\u8ddd\u79bb\u5728\u4e00\u4e2a\u6216\u4e24\u4e2a\u57fa\u4e2d\u90fd\u51cf\u5c11\u4e00\u4e2a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u65b9\u6cd5\u4e3a\u5728\u516d\u8fb9\u5f62\u91cf\u5b50\u6bd4\u7279\u6805\u683c\u4e2d\u5904\u7406\u635f\u574f\u7684\u91cf\u5b50\u6bd4\u7279\u548c\u8026\u5408\u5668\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u4e22\u5f03\u7b56\u7565\uff0c\u6d88\u9664\u4e86\u5728\u5b9e\u73b0\u5927\u89c4\u6a21\u91cf\u5b50\u7ea0\u9519\u7684\u516d\u8fb9\u5f62\u91cf\u5b50\u6bd4\u7279\u6805\u683c\u786c\u4ef6\u65b9\u9762\u7684\u4e00\u4e2a\u5173\u952e\u969c\u788d\u3002"}}
{"id": "2508.07375", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07375", "abs": "https://arxiv.org/abs/2508.07375", "authors": ["Wenqian Cui", "Lei Zhu", "Xiaohui Li", "Zhihan Guo", "Haoli Bai", "Lu Hou", "Irwin King"], "title": "Think Before You Talk: Enhancing Meaningful Dialogue Generation in Full-Duplex Speech Language Models with Planning-Inspired Text Guidance", "comment": "Work in progress", "summary": "Full-Duplex Speech Language Models (FD-SLMs) are specialized foundation\nmodels designed to enable natural, real-time spoken interactions by modeling\ncomplex conversational dynamics such as interruptions, backchannels, and\noverlapping speech, and End-to-end (e2e) FD-SLMs leverage real-world\ndouble-channel conversational data to capture nuanced two-speaker dialogue\npatterns for human-like interactions. However, they face a critical challenge\n-- their conversational abilities often degrade compared to pure-text\nconversation due to prolonged speech sequences and limited high-quality spoken\ndialogue data. While text-guided speech generation could mitigate these issues,\nit suffers from timing and length issues when integrating textual guidance into\ndouble-channel audio streams, disrupting the precise time alignment essential\nfor natural interactions. To address these challenges, we propose TurnGuide, a\nnovel planning-inspired approach that mimics human conversational planning by\ndynamically segmenting assistant speech into dialogue turns and generating\nturn-level text guidance before speech output, which effectively resolves both\ninsertion timing and length challenges. Extensive experiments demonstrate our\napproach significantly improves e2e FD-SLMs' conversational abilities, enabling\nthem to generate semantically meaningful and coherent speech while maintaining\nnatural conversational flow. Demos are available at\nhttps://dreamtheater123.github.io/TurnGuide-Demo/. Code will be available at\nhttps://github.com/dreamtheater123/TurnGuide.", "AI": {"tldr": "TurnGuide \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u89c4\u5212\u542f\u53d1\u5f0f\u65b9\u6cd5\uff0c\u901a\u8fc7\u751f\u6210\u8f6e\u6b21\u7ea7\u522b\u7684\u6587\u672c\u6307\u5bfc\u6765\u6539\u5584\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u65f6\u5e8f\u548c\u957f\u5ea6\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u7aef\u5230\u7aef\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08FD-SLMs\uff09\u5728\u5904\u7406\u5b9e\u65f6\u53e3\u8bed\u4ea4\u4e92\u65f6\uff0c\u5176\u5bf9\u8bdd\u80fd\u529b\u76f8\u6bd4\u7eaf\u6587\u672c\u5bf9\u8bdd\u6709\u6240\u4e0b\u964d\uff0c\u8fd9\u5f52\u56e0\u4e8e\u5197\u957f\u7684\u8bed\u97f3\u5e8f\u5217\u548c\u6709\u9650\u7684\u9ad8\u8d28\u91cf\u53e3\u8bed\u5bf9\u8bdd\u6570\u636e\u3002\u800c\u6587\u672c\u5f15\u5bfc\u8bed\u97f3\u751f\u6210\u65b9\u6cd5\u5728\u5c06\u6587\u672c\u6307\u5bfc\u6574\u5408\u5230\u53cc\u901a\u9053\u97f3\u9891\u6d41\u65f6\u5b58\u5728\u65f6\u5e8f\u548c\u957f\u5ea6\u95ee\u9898\uff0c\u6270\u4e71\u4e86\u81ea\u7136\u4ea4\u4e92\u6240\u9700\u7684\u7cbe\u786e\u65f6\u95f4\u5bf9\u9f50\u3002", "method": "TurnGuide \u662f\u4e00\u79cd\u65b0\u9896\u7684\u3001\u53d7\u89c4\u5212\u542f\u53d1\u7684\u3001\u7aef\u5230\u7aef\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08FD-SLMs\uff09\u7684\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u52a8\u6001\u5206\u5272\u52a9\u624b\u8bed\u97f3\u6210\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u5e76\u5728\u8bed\u97f3\u8f93\u51fa\u524d\u751f\u6210\u8f6e\u6b21\u7ea7\u522b\u7684\u6587\u672c\u6307\u5bfc\uff0c\u4ee5\u89e3\u51b3\u6587\u672c\u5f15\u5bfc\u8bed\u97f3\u751f\u6210\u4e2d\u7684\u65f6\u5e8f\u548c\u957f\u5ea6\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cTurnGuide \u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u7aef\u5230\u7aef\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08FD-SLMs\uff09\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u80fd\u591f\u751f\u6210\u8bed\u4e49\u6709\u610f\u4e49\u4e14\u8fde\u8d2f\u7684\u8bed\u97f3\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u5bf9\u8bdd\u6d41\u7a0b\u3002", "conclusion": "TurnGuide \u901a\u8fc7\u6a21\u4eff\u4eba\u7c7b\u5bf9\u8bdd\u89c4\u5212\uff0c\u5c06\u52a9\u624b\u8bed\u97f3\u52a8\u6001\u5206\u5272\u6210\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u5e76\u5728\u8bed\u97f3\u8f93\u51fa\u524d\u751f\u6210\u8f6e\u6b21\u7ea7\u522b\u7684\u6587\u672c\u6307\u5bfc\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u63d2\u5165\u65f6\u673a\u548c\u957f\u5ea6\u7684\u6311\u6218\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u7aef\u5230\u7aef\u5168\u53cc\u5de5\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u8bdd\u80fd\u529b\uff0c\u4f7f\u5176\u80fd\u591f\u751f\u6210\u8bed\u4e49\u6709\u610f\u4e49\u4e14\u8fde\u8d2f\u7684\u8bed\u97f3\uff0c\u540c\u65f6\u4fdd\u6301\u81ea\u7136\u7684\u5bf9\u8bdd\u6d41\u7a0b\u3002"}}
{"id": "2508.06900", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06900", "abs": "https://arxiv.org/abs/2508.06900", "authors": ["Weiran Chen", "Guiqian Zhu", "Ying Li", "Yi Ji", "Chunping Liu"], "title": "Advancements in Chinese font generation since deep learning era: A survey", "comment": "42 Pages, 25 figures", "summary": "Chinese font generation aims to create a new Chinese font library based on\nsome reference samples. It is a topic of great concern to many font designers\nand typographers. Over the past years, with the rapid development of deep\nlearning algorithms, various new techniques have achieved flourishing and\nthriving progress. Nevertheless, how to improve the overall quality of\ngenerated Chinese character images remains a tough issue. In this paper, we\nconduct a holistic survey of the recent Chinese font generation approaches\nbased on deep learning. To be specific, we first illustrate the research\nbackground of the task. Then, we outline our literature selection and analysis\nmethodology, and review a series of related fundamentals, including classical\ndeep learning architectures, font representation formats, public datasets, and\nfrequently-used evaluation metrics. After that, relying on the number of\nreference samples required to generate a new font, we categorize the existing\nmethods into two major groups: many-shot font generation and few-shot font\ngeneration methods. Within each category, representative approaches are\nsummarized, and their strengths and limitations are also discussed in detail.\nFinally, we conclude our paper with the challenges and future directions, with\nthe expectation to provide some valuable illuminations for the researchers in\nthis field.", "AI": {"tldr": "\u672c\u8bba\u6587\u5168\u9762 survey \u4e86\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u6570\u5b57\u751f\u6210\u6280\u672f\uff0c\u5bf9\u591a\u6837\u672c\u548c\u5c11\u6837\u672c\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u8ba8\u8bba\uff0c\u5e76\u5bf9\u672a\u6765\u65b9\u5411\u8fdb\u884c\u4e86\u5c55\u671b\u3002", "motivation": "\u4e2d\u6587\u6570\u5b57\u751f\u6210\u662f\u5b57\u4f53\u8bbe\u8ba1\u548c\u6392\u7248\u9886\u57df\u7684\u4e00\u4e2a\u91cd\u8981\u8bfe\u9898\uff0c\u6df1\u5ea6\u5b66\u4e60\u7b97\u6cd5\u7684\u53d1\u5c55\u5e26\u6765\u4e86\u65b0\u7684\u6280\u672f\uff0c\u4f46\u5982\u4f55\u63d0\u9ad8\u751f\u6210\u6570\u5b57\u56fe\u50cf\u7684\u6574\u4f53\u8d28\u91cf\u4ecd\u7136\u662f\u4e00\u4e2a\u96be\u9898\u3002", "method": "\u5bf9\u73b0\u6709\u4e2d\u6587\u6570\u5b57\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u6587\u732e\u68c0\u7d22\u3001\u7b5b\u9009\u548c\u5206\u6790\uff0c\u6839\u636e\u6240\u9700\u53c2\u8003\u6837\u672c\u6570\u91cf\u5c06\u65b9\u6cd5\u5206\u4e3a\u591a\u6837\u672c\u751f\u6210\u548c\u5c11\u6837\u672c\u751f\u6210\u4e24\u7c7b\uff0c\u5e76\u5bf9\u4ee3\u8868\u6027\u65b9\u6cd5\u8fdb\u884c\u603b\u7ed3\u548c\u8ba8\u8bba\u3002", "result": "\u5bf9\u73b0\u6709\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u6570\u5b57\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u5206\u7c7b\u548c\u603b\u7ed3\uff0c\u8ba8\u8bba\u4e86\u5b83\u4eec\u7684\u4f18\u7f3a\u70b9\uff0c\u5e76\u6307\u51fa\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "conclusion": "\u672c\u8bba\u6587\u5bf9\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u4e2d\u6587\u6570\u5b57\u751f\u6210\u65b9\u6cd5\u8fdb\u884c\u4e86\u5168\u9762\u7684\u8c03\u67e5\uff0c\u603b\u7ed3\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u8ba8\u8bba\u4e86\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\uff0c\u65e8\u5728\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u6709\u4ef7\u503c\u7684\u542f\u793a\u3002"}}
{"id": "2508.07649", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07649", "abs": "https://arxiv.org/abs/2508.07649", "authors": ["Jie Li", "Haoye Dong", "Zhengyang Wu", "Zetao Zheng", "Mingrong Lin"], "title": "Disentangling Multiplex Spatial-Temporal Transition Graph Representation Learning for Socially Enhanced POI Recommendation", "comment": null, "summary": "Next Point-of-Interest (POI) recommendation is a research hotspot in business\nintelligence, where users' spatial-temporal transitions and social\nrelationships play key roles. However, most existing works model spatial and\ntemporal transitions separately, leading to misaligned representations of the\nsame spatial-temporal key nodes. This misalignment introduces redundant\ninformation during fusion, increasing model uncertainty and reducing\ninterpretability. To address this issue, we propose DiMuST, a socially enhanced\nPOI recommendation model based on disentangled representation learning over\nmultiplex spatial-temporal transition graphs. The model employs a novel\nDisentangled variational multiplex graph Auto-Encoder (DAE), which first\ndisentangles shared and private distributions using a multiplex\nspatial-temporal graph strategy. It then fuses the shared features via a\nProduct of Experts (PoE) mechanism and denoises the private features through\ncontrastive constraints. The model effectively captures the spatial-temporal\ntransition representations of POIs while preserving the intrinsic correlation\nof their spatial-temporal relationships. Experiments on two challenging\ndatasets demonstrate that our DiMuST significantly outperforms existing methods\nacross multiple metrics.", "AI": {"tldr": "DiMuST\u662f\u4e00\u79cd\u65b0\u7684POI\u63a8\u8350\u6a21\u578b\uff0c\u901a\u8fc7\u89e3\u7ea0\u7f20\u8868\u793a\u5b66\u4e60\u6765\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u4e2d\u7a7a\u95f4-\u65f6\u95f4\u8868\u793a\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002\u5b83\u5728\u4e24\u4e2a\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684POI\u63a8\u8350\u7814\u7a76\u901a\u5e38\u5206\u522b\u5bf9\u7a7a\u95f4\u548c\u65f6\u95f4\u8f6c\u79fb\u8fdb\u884c\u5efa\u6a21\uff0c\u8fd9\u4f1a\u5bfc\u81f4\u76f8\u540c\u7684\u7a7a\u95f4-\u65f6\u95f4\u5173\u952e\u8282\u70b9\u7684\u8868\u793a\u4e0d\u5339\u914d\uff0c\u4ece\u800c\u5728\u878d\u5408\u8fc7\u7a0b\u4e2d\u5f15\u5165\u5197\u4f59\u4fe1\u606f\uff0c\u589e\u52a0\u6a21\u578b\u4e0d\u786e\u5b9a\u6027\u5e76\u964d\u4f4e\u53ef\u89e3\u91ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u91cd\u7a7a\u95f4-\u65f6\u95f4\u8f6c\u79fb\u56fe\u89e3\u7ea0\u7f20\u8868\u793a\u5b66\u4e60\u7684\u3001\u5177\u6709\u793e\u4f1a\u589e\u5f3a\u529f\u80fd\u7684POI\u63a8\u8350\u6a21\u578bDiMuST\u3002\u8be5\u6a21\u578b\u91c7\u7528\u65b0\u9896\u7684\u89e3\u7ea0\u7f20\u53d8\u5206\u591a\u91cd\u56fe\u81ea\u52a8\u7f16\u7801\u5668\uff08DAE\uff09\uff0c\u5229\u7528\u591a\u91cd\u7a7a\u95f4-\u65f6\u95f4\u56fe\u7b56\u7565\u9996\u5148\u89e3\u7ea0\u7f20\u5171\u4eab\u548c\u79c1\u6709\u5206\u5e03\uff0c\u7136\u540e\u901a\u8fc7\u4e13\u5bb6\u4e58\u79ef\uff08PoE\uff09\u673a\u5236\u878d\u5408\u5171\u4eab\u7279\u5f81\uff0c\u5e76\u901a\u8fc7\u5bf9\u6bd4\u7ea6\u675f\u5bf9\u79c1\u6709\u7279\u5f81\u8fdb\u884c\u53bb\u566a\u3002DiMuST\u80fd\u6709\u6548\u6355\u6349POI\u7684\u7a7a\u95f4-\u65f6\u95f4\u8f6c\u79fb\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u7559\u5176\u7a7a\u95f4-\u65f6\u95f4\u5173\u7cfb\u7684\u5185\u5728\u76f8\u5173\u6027\u3002", "result": "DiMuST\u6a21\u578b\u6709\u6548\u5730\u6355\u6349\u4e86POI\u7684\u7a7a\u95f4-\u65f6\u95f4\u8f6c\u79fb\u8868\u793a\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u5176\u7a7a\u95f4-\u65f6\u95f4\u5173\u7cfb\u7684\u5185\u5728\u76f8\u5173\u6027\u3002", "conclusion": "DiMuST\u6a21\u578b\u5728\u4e24\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u597d\u3002"}}
{"id": "2508.07075", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07075", "abs": "https://arxiv.org/abs/2508.07075", "authors": ["Stanley Ngugi"], "title": "Surgical Knowledge Rewrite in Compact LLMs: An 'Unlearn-then-Learn' Strategy with ($IA^3$) for Localized Factual Modulation and Catastrophic Forgetting Mitigation", "comment": "9 pages, 2 visual aids", "summary": "Large Language Models (LLMs) struggle with dynamic knowledge updates,\nespecially when new information conflicts with deeply embedded facts. Such\nconflicting factual edits often lead to two critical issues: resistance to\nadopting the new fact and severe catastrophic forgetting of unrelated\nknowledge. This paper introduces and evaluates a novel \"unlearn-then-learn\"\nstrategy for precise knowledge editing in LLMs, leveraging the\nparameter-efficient fine-tuning (PEFT) technique, Infused Adapter by Inhibiting\nand Amplifying Inner Activations ($IA^3$). Crucially, this two-stage approach\nis powered by an initial circuit localization phase that identifies and targets\nthe specific internal components responsible for encoding the conflicting fact.\nThrough a rigorous experimental methodology on\nmicrosoft/Phi-3-mini-4k-instruct, we demonstrate that this mechanistically\ninformed two-stage approach achieves near-perfect accuracy (98.50%) for the\nnew, modulated fact while simultaneously effectively suppressing the original\nconflicting fact (96.00% forget rate). Critically, our strategy exhibits\nunprecedented localization (72.00% F_control accuracy), dramatically mitigating\ncatastrophic forgetting observed in direct fine-tuning approaches (which showed\nas low as ~20% F_control accuracy), a direct benefit of our targeted\ninterpretability-guided intervention. Furthermore, qualitative analysis reveals\na nuanced mechanism of \"soft forgetting,\" where original knowledge is\nsuppressed from default retrieval but remains latent and conditionally\naccessible, enhancing model safety and control. These findings represent a\nsignificant advancement towards precise, localized, and safe knowledge\nmanagement in compact LLMs.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u9057\u5fd8-\u5b66\u4e60\u201d\u7b56\u7565\uff0c\u5229\u7528$IA^3$\u548c\u7535\u8def\u5b9a\u4f4d\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u66f4\u65b0\u51b2\u7a81\u77e5\u8bc6\u65f6\u9047\u5230\u7684\u7cbe\u786e\u7f16\u8f91\u548c\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u77e5\u8bc6\u66f4\u65b0\u7684\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u52a8\u6001\u77e5\u8bc6\u66f4\u65b0\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u7279\u522b\u662f\u5728\u65b0\u4fe1\u606f\u4e0e\u6df1\u5c42\u5d4c\u5165\u4e8b\u5b9e\u51b2\u7a81\u65f6\u3002\u8fd9\u79cd\u51b2\u7a81\u7684\u4e8b\u5b9e\u7f16\u8f91\u901a\u5e38\u4f1a\u5bfc\u81f4\u4e24\u4e2a\u5173\u952e\u95ee\u9898\uff1a\u5bf9\u65b0\u4e8b\u5b9e\u7684\u91c7\u7eb3\u4ea7\u751f\u62b5\u6297\u529b\uff0c\u4ee5\u53ca\u4e25\u91cd\u5730\u9057\u5fd8\u65e0\u5173\u77e5\u8bc6\uff08\u707e\u96be\u6027\u9057\u5fd8\uff09\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3LLMs\u5728\u77e5\u8bc6\u66f4\u65b0\u4e2d\u7684\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u5e76\u8bc4\u4f30\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u201c\u9057\u5fd8-\u5b66\u4e60\u201d\u7b56\u7565\uff0c\u7528\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cbe\u786e\u77e5\u8bc6\u7f16\u8f91\u3002\u8be5\u7b56\u7565\u5229\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u4e2d\u7684Infused Adapter by Inhibiting and Amplifying Inner Activations ($IA^3$)\u3002\u5173\u952e\u5728\u4e8e\uff0c\u8be5\u4e24\u9636\u6bb5\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u521d\u59cb\u7684\u7535\u8def\u5b9a\u4f4d\u9636\u6bb5\uff0c\u4ee5\u8bc6\u522b\u5e76\u7cbe\u786e\u9776\u5411\u8d1f\u8d23\u7f16\u7801\u51b2\u7a81\u4e8b\u5b9e\u7684\u5185\u90e8\u7ec4\u4ef6\u3002\u901a\u8fc7\u5728microsoft/Phi-3-mini-4k-instruct\u6a21\u578b\u4e0a\u8fdb\u884c\u4e25\u683c\u7684\u5b9e\u9a8c\u65b9\u6cd5\u5b66\u8bc4\u4f30\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u3002", "result": "\u8be5\u4e24\u9636\u6bb5\u65b9\u6cd5\u5728microsoft/Phi-3-mini-4k-instruct\u6a21\u578b\u4e0a\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0898.50%\uff09\uff0c\u7528\u4e8e\u65b0\u8c03\u5236\u7684\u77e5\u8bc6\uff0c\u540c\u65f6\u6709\u6548\u6291\u5236\u4e86\u539f\u59cb\u51b2\u7a81\u4e8b\u5b9e\uff0896.00%\u7684\u9057\u5fd8\u7387\uff09\u3002\u8be5\u7b56\u7565\u5c55\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u5b9a\u4f4d\u80fd\u529b\uff0872.00%\u7684F_control\u51c6\u786e\u7387\uff09\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u76f4\u63a5\u5fae\u8c03\u65b9\u6cd5\u4e2d\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff08F_control\u51c6\u786e\u7387\u4f4e\u81f3\u7ea620%\uff09\u3002\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e00\u79cd\u201c\u8f6f\u9057\u5fd8\u201d\u673a\u5236\uff0c\u539f\u59cb\u77e5\u8bc6\u88ab\u6291\u5236\u4f46\u4ecd\u53ef\u6761\u4ef6\u6027\u8bbf\u95ee\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4e00\u79cd\u65b0\u9896\u7684\u201c\u9057\u5fd8-\u5b66\u4e60\u201d\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u6280\u672f\u548c\u5185\u90e8\u7ec4\u4ef6\u5b9a\u4f4d\uff0c\u5b9e\u73b0\u4e86\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u7cbe\u786e\u77e5\u8bc6\u7f16\u8f91\u3002\u8be5\u65b9\u6cd5\u5728\u5fae\u8f6f\u7684Phi-3-mini-4k-instruct\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u4e25\u683c\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u5728\u66f4\u65b0\u51b2\u7a81\u4e8b\u5b9e\u65b9\u9762\u53d6\u5f97\u4e86\u8fd1\u4e4e\u5b8c\u7f8e\u7684\u51c6\u786e\u7387\uff0898.50%\uff09\uff0c\u540c\u65f6\u6709\u6548\u6291\u5236\u4e86\u51b2\u7a81\u4e8b\u5b9e\uff08\u9057\u5fd8\u7387\u4e3a96.00%\uff09\u3002\u8be5\u7b56\u7565\u8fd8\u5c55\u73b0\u4e86\u524d\u6240\u672a\u6709\u7684\u5b9a\u4f4d\u80fd\u529b\uff0872.00% F_control\u51c6\u786e\u7387\uff09\uff0c\u663e\u8457\u7f13\u89e3\u4e86\u76f4\u63a5\u5fae\u8c03\u65b9\u6cd5\u4e2d\u51fa\u73b0\u7684\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\uff08F_control\u51c6\u786e\u7387\u4f4e\u81f3\u7ea620%\uff09\u3002\u6b64\u5916\uff0c\u5b9a\u6027\u5206\u6790\u63ed\u793a\u4e86\u4e00\u79cd\u201c\u8f6f\u9057\u5fd8\u201d\u673a\u5236\uff0c\u4f7f\u5f97\u539f\u59cb\u77e5\u8bc6\u5728\u9ed8\u8ba4\u68c0\u7d22\u65f6\u88ab\u6291\u5236\uff0c\u4f46\u4ecd\u53ef\u88ab\u6761\u4ef6\u6027\u8bbf\u95ee\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u5b89\u5168\u6027\u548c\u53ef\u63a7\u6027\u3002\u8fd9\u4e9b\u53d1\u73b0\u6807\u5fd7\u7740\u5728\u5b9e\u73b0\u7d27\u51d1\u578bLLM\u7684\u7cbe\u786e\u3001\u5c40\u90e8\u5316\u548c\u5b89\u5168\u77e5\u8bc6\u7ba1\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2508.07842", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07842", "abs": "https://arxiv.org/abs/2508.07842", "authors": ["Yutong Shen", "Hangxu Liu", "Penghui Liu", "Ruizhe Xia", "Tianyi Yao", "Yitong Sun", "Tongtong Feng"], "title": "DETACH: Cross-domain Learning for Long-Horizon Tasks via Mixture of Disentangled Experts", "comment": "14 pages,8 figures. Submitted to AAAI'26", "summary": "Long-Horizon (LH) tasks in Human-Scene Interaction (HSI) are complex\nmulti-step tasks that require continuous planning, sequential decision-making,\nand extended execution across domains to achieve the final goal. However,\nexisting methods heavily rely on skill chaining by concatenating pre-trained\nsubtasks, with environment observations and self-state tightly coupled, lacking\nthe ability to generalize to new combinations of environments and skills,\nfailing to complete various LH tasks across domains. To solve this problem,\nthis paper presents DETACH, a cross-domain learning framework for LH tasks via\nbiologically inspired dual-stream disentanglement. Inspired by the brain's\n\"where-what\" dual pathway mechanism, DETACH comprises two core modules: i) an\nenvironment learning module for spatial understanding, which captures object\nfunctions, spatial relationships, and scene semantics, achieving cross-domain\ntransfer through complete environment-self disentanglement; ii) a skill\nlearning module for task execution, which processes self-state information\nincluding joint degrees of freedom and motor patterns, enabling cross-skill\ntransfer through independent motor pattern encoding. We conducted extensive\nexperiments on various LH tasks in HSI scenes. Compared with existing methods,\nDETACH can achieve an average subtasks success rate improvement of 23% and\naverage execution efficiency improvement of 29%.", "AI": {"tldr": "DETACH\u6846\u67b6\u901a\u8fc7\u53cc\u6d41\u89e3\u8026\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u957f\u65f6\u4efb\u52a1\u4e2d\u7684\u6cdb\u5316\u548c\u8de8\u9886\u57df\u6267\u884c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6210\u529f\u7387\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4f9d\u8d56\u6280\u80fd\u94fe\u63a5\uff0c\u96be\u4ee5\u6cdb\u5316\u5230\u65b0\u7684\u73af\u5883\u548c\u6280\u80fd\u7ec4\u5408\uff0c\u65e0\u6cd5\u5b8c\u6210\u8de8\u9886\u57df\u7684\u957f\u65f6\u4efb\u52a1", "method": "DETACH\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u73af\u5883\u5b66\u4e60\u6a21\u5757\uff08\u7a7a\u95f4\u7406\u89e3\uff09\u548c\u6280\u80fd\u5b66\u4e60\u6a21\u5757\uff08\u4efb\u52a1\u6267\u884c\uff09\uff0c\u901a\u8fc7\u73af\u5883-\u81ea\u6211\u548c\u52a8\u4f5c\u6a21\u5f0f\u7684\u89e3\u8026\uff0c\u5b9e\u73b0\u8de8\u9886\u57df\u548c\u8de8\u6280\u80fd\u7684\u8fc1\u79fb", "result": "DETACH\u6846\u67b6\u5728\u8de8\u9886\u57df\u957f\u65f6\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u5b50\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6548\u7387\u5e73\u5747\u5206\u522b\u63d0\u9ad8\u4e8623%\u548c29%", "conclusion": "DETACH\u6846\u67b6\u901a\u8fc7\u751f\u7269\u542f\u53d1\u7684\u53cc\u6d41\u89e3\u8026\uff0c\u5b9e\u73b0\u4e86\u8de8\u9886\u57df\u957f\u65f6\u4efb\u52a1\u7684\u5b66\u4e60\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u5b50\u4efb\u52a1\u6210\u529f\u7387\u548c\u6267\u884c\u6548\u7387\u4e0a\u5206\u522b\u63d0\u9ad8\u4e8623%\u548c29%"}}
{"id": "2508.08160", "categories": ["quant-ph", "cond-mat.str-el", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2508.08160", "abs": "https://arxiv.org/abs/2508.08160", "authors": ["Georgios Styliaris", "Rahul Trivedi", "J. Ignacio Cirac"], "title": "Quantum Circuit Complexity of Matrix-Product Unitaries", "comment": null, "summary": "Matrix-product unitaries (MPUs) are many-body unitary operators that, as a\nconsequence of their tensor-network structure, preserve the entanglement area\nlaw in 1D systems. However, it is unknown how to implement an MPU as a quantum\ncircuit since the individual tensors describing the MPU are not unitary. In\nthis paper, we show that a large class of MPUs can be implemented with a\npolynomial-depth quantum circuit. For an $N$-site MPU built from a repeated\nbulk tensor with open boundary, we explicitly construct a quantum circuit of\npolynomial depth $T = O(N^{\\alpha})$ realizing the MPU, where the constant\n$\\alpha$ depends only on the bulk and boundary tensor and not the system size\n$N$. We show that this class includes nontrivial unitaries that generate\nlong-range entanglement and, in particular, contains a large class of unitaries\nconstructed from representations of $C^*$-weak Hopf algebras. Furthermore, we\nalso adapt our construction to nonuniform translationally-varying MPUs and show\nthat they can be implemented by a circuit of depth $O(N^{\\beta} \\,\n\\mathrm{poly}\\, D)$ where $\\beta \\le 1 + \\log_2 \\sqrt{D}/ s_{\\min}$, with $D$\nbeing the bond dimension and $s_{\\min}$ is the smallest nonzero Schmidt value\nof the normalized Choi state corresponding to the MPU.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u5982\u4f55\u7528\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u77e9\u9635\u4e58\u79ef\u5e7a\u6b63\u7b97\u7b26(MPU)\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u9879\u5f0f\u6df1\u5ea6\u7684\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u4fdd\u6301\u7ea0\u7f20\u548c\u751f\u6210\u957f\u7a0b\u7ea0\u7f20\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "MPU\u5177\u6709\u4fdd\u63011D\u7cfb\u7edf\u7ea0\u7f20\u9762\u79ef\u5b9a\u5f8b\u7684\u7279\u6027\uff0c\u4f46\u5982\u4f55\u5b9e\u73b0MPU\u4f5c\u4e3a\u91cf\u5b50\u7535\u8def\u4ecd\u7136\u672a\u77e5\u3002", "method": "\u901a\u8fc7\u660e\u786e\u6784\u5efa\u4e00\u4e2a\u91cf\u5b50\u7535\u8def\u6765\u5b9e\u73b0MPU\u3002", "result": "\u6240\u63d0\u51fa\u7684\u91cf\u5b50\u7535\u8def\u6df1\u5ea6\u4e3a T = O(N^\u03b1)\uff0c\u5176\u4e2d \u03b1 \u4ec5\u53d6\u51b3\u4e8e\u4f53\u548c\u8fb9\u754c\u5f20\u91cf\uff0c\u800c\u4e0d\u53d6\u51b3\u4e8e\u7cfb\u7edf\u5927\u5c0f N\u3002\u8be5\u7c7bMPU\u5305\u542b\u751f\u6210\u957f\u7a0b\u7ea0\u7f20\u7684\u975e\u5e73\u51e1\u9149\u7b97\u5b50\uff0c\u7279\u522b\u662f\u5305\u542b\u4e00\u7c7b\u7531C*-\u5f31Hopf\u4ee3\u6570\u8868\u793a\u6784\u9020\u7684\u9149\u7b97\u5b50\u3002\u6b64\u5916\uff0c\u8be5\u6784\u9020\u4e5f\u9002\u7528\u4e8e\u975e\u5747\u5300\u5e73\u79fb\u53d8\u5f02MPU\uff0c\u5176\u7535\u8def\u6df1\u5ea6\u4e3aO(N^\u03b2 poly D)\u3002", "conclusion": "\u672c\u6587\u8868\u660e\uff0c\u4e00\u7c7b\u5927\u7684MPU\u53ef\u4ee5\u901a\u8fc7\u591a\u9879\u5f0f\u6df1\u5ea6\u7684\u91cf\u5b50\u7535\u8def\u5b9e\u73b0\uff0c\u5e76\u660e\u786e\u6784\u5efa\u4e86\u4e00\u4e2a\u5b9e\u73b0MPU\u7684\u591a\u9879\u5f0f\u6df1\u5ea6\u91cf\u5b50\u7535\u8def\u3002"}}
{"id": "2508.07414", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07414", "abs": "https://arxiv.org/abs/2508.07414", "authors": ["Jean de Dieu Nyandwi", "Yueqi Song", "Simran Khanuja", "Graham Neubig"], "title": "Grounding Multilingual Multimodal LLMs With Cultural Knowledge", "comment": null, "summary": "Multimodal Large Language Models excel in high-resource settings, but often\nmisinterpret long-tail cultural entities and underperform in low-resource\nlanguages. To address this gap, we propose a data-centric approach that\ndirectly grounds MLLMs in cultural knowledge. Leveraging a large scale\nknowledge graph from Wikidata, we collect images that represent culturally\nsignificant entities, and generate synthetic multilingual visual question\nanswering data. The resulting dataset, CulturalGround, comprises 22 million\nhigh-quality, culturally-rich VQA pairs spanning 42 countries and 39 languages.\nWe train an open-source MLLM CulturalPangea on CulturalGround, interleaving\nstandard multilingual instruction-tuning data to preserve general abilities.\nCulturalPangea achieves state-of-the-art performance among open models on\nvarious culture-focused multilingual multimodal benchmarks, outperforming prior\nmodels by an average of 5.0 without degrading results on mainstream\nvision-language tasks. Our findings show that our targeted, culturally grounded\napproach could substantially narrow the cultural gap in MLLMs and offer a\npractical path towards globally inclusive multimodal systems.", "AI": {"tldr": "\u901a\u8fc7\u6784\u5efa\u5305\u542b2200\u4e07\u4e2a\u8de8\u8d8a42\u56fd39\u8bed\u8a00\u7684\u6587\u5316\u89c6\u89c9\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6CulturalGround\uff0c\u5e76\u8bad\u7ec3MLLMCulturalPangea\uff0c\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u5728\u4f4e\u8d44\u6e90\u548c\u957f\u5c3e\u6587\u5316\u5b9e\u4f53\u4e0a\u7684\u8868\u73b0\uff0c\u7f29\u5c0f\u4e86\u6587\u5316\u9e3f\u6c9f\uff0c\u4e3a\u5168\u7403\u5305\u5bb9\u6027\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u9014\u5f84\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u9ad8\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u957f\u5c3e\u6587\u5316\u5b9e\u4f53\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\u5b58\u5728\u8bef\u89e3\u548c\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u5229\u7528\u7ef4\u57fa\u6570\u636e\u77e5\u8bc6\u56fe\u8c31\uff0c\u6536\u96c6\u4ee3\u8868\u5177\u6709\u6587\u5316\u610f\u4e49\u7684\u5b9e\u4f53\u7684\u56fe\u50cf\uff0c\u5e76\u751f\u6210\u5408\u6210\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6570\u636e\uff08CulturalGround\u6570\u636e\u96c6\uff0c\u5305\u542b2200\u4e07\u4e2a\u89c6\u89c9\u95ee\u7b54\u5bf9\uff0c\u8986\u76d642\u4e2a\u56fd\u5bb6\u548c39\u79cd\u8bed\u8a00\uff09\u3002\u5728CulturalGround\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u5f00\u6e90MLLM\uff08CulturalPangea\uff09\uff0c\u5e76\u7ed3\u5408\u6807\u51c6\u7684\u6cd5\u8bed\u6307\u4ee4\u8c03\u6574\u6570\u636e\u4ee5\u4fdd\u6301\u901a\u7528\u80fd\u529b\u3002", "result": "CulturalPangea\u5728\u591a\u4e2a\u4ee5\u6587\u5316\u4e3a\u4e2d\u5fc3\u7684\u6cd5\u8bed\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b5.0\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u672a\u964d\u4f4e\u5728\u4e3b\u6d41\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u4ee5\u6570\u636e\u4e3a\u4e2d\u5fc3\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5229\u7528\u7ef4\u57fa\u6570\u636e\u77e5\u8bc6\u56fe\u8c31\u6536\u96c6\u4ee3\u8868\u5177\u6709\u6587\u5316\u610f\u4e49\u7684\u5b9e\u4f53\u7684\u56fe\u50cf\uff0c\u5e76\u751f\u6210\u5408\u6210\u7684\u591a\u8bed\u8a00\u89c6\u89c9\u95ee\u7b54\u6570\u636e\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u5728\u9ad8\u8d44\u6e90\u8bbe\u7f6e\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5728\u5904\u7406\u957f\u5c3e\u6587\u5316\u5b9e\u4f53\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65f6\u5b58\u5728\u8bef\u89e3\u548c\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u901a\u8fc7\u8bad\u7ec3\u540d\u4e3aCulturalPangea\u7684\u5f00\u6e90MLLM\uff0c\u5e76\u5728\u5305\u542b2200\u4e07\u4e2a\u9ad8\u8d28\u91cf\u3001\u6587\u5316\u4e30\u5bcc\u7684\u89c6\u89c9\u95ee\u7b54\u5bf9\uff08\u6db5\u76d642\u4e2a\u56fd\u5bb6\u548c39\u79cd\u8bed\u8a00\uff09\u7684\u6570\u636e\u96c6CulturalGround\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u540c\u65f6\u7ed3\u5408\u6807\u51c6\u7684\u6cd5\u8bed\u6307\u4ee4\u8c03\u6574\u6570\u636e\u4ee5\u4fdd\u6301\u901a\u7528\u80fd\u529b\uff0cCulturalPangea\u5728\u591a\u4e2a\u4ee5\u6587\u5316\u4e3a\u4e2d\u5fc3\u7684\u6cd5\u8bed\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e73\u5747\u4f18\u4e8e\u73b0\u6709\u6a21\u578b5.0\u4e2a\u767e\u5206\u70b9\uff0c\u4e14\u672a\u964d\u4f4e\u5728\u4e3b\u6d41\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u8fd9\u79cd\u6709\u9488\u5bf9\u6027\u7684\u3001\u4ee5\u6587\u5316\u4e3a\u57fa\u7840\u7684\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u7f29\u5c0fMLLM\u5728\u6587\u5316\u7406\u89e3\u4e0a\u7684\u5dee\u8ddd\uff0c\u4e3a\u6784\u5efa\u5168\u7403\u5305\u5bb9\u6027\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u4e86\u5b9e\u9645\u53ef\u884c\u8def\u5f84\u3002"}}
{"id": "2508.06902", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06902", "abs": "https://arxiv.org/abs/2508.06902", "authors": ["Xuecheng Wu", "Dingkang Yang", "Danlei Huang", "Xinyi Yin", "Yifan Wang", "Jia Zhang", "Jiayu Nie", "Liangyu Fu", "Yang Liu", "Junxiao Xue", "Hadi Amirpour", "Wei Zhou"], "title": "eMotions: A Large-Scale Dataset and Audio-Visual Fusion Network for Emotion Analysis in Short-form Videos", "comment": null, "summary": "Short-form videos (SVs) have become a vital part of our online routine for\nacquiring and sharing information. Their multimodal complexity poses new\nchallenges for video analysis, highlighting the need for video emotion analysis\n(VEA) within the community. Given the limited availability of SVs emotion data,\nwe introduce eMotions, a large-scale dataset consisting of 27,996 videos with\nfull-scale annotations. To ensure quality and reduce subjective bias, we\nemphasize better personnel allocation and propose a multi-stage annotation\nprocedure. Additionally, we provide the category-balanced and test-oriented\nvariants through targeted sampling to meet diverse needs. While there have been\nsignificant studies on videos with clear emotional cues (e.g., facial\nexpressions), analyzing emotions in SVs remains a challenging task. The\nchallenge arises from the broader content diversity, which introduces more\ndistinct semantic gaps and complicates the representations learning of\nemotion-related features. Furthermore, the prevalence of audio-visual\nco-expressions in SVs leads to the local biases and collective information gaps\ncaused by the inconsistencies in emotional expressions. To tackle this, we\npropose AV-CANet, an end-to-end audio-visual fusion network that leverages\nvideo transformer to capture semantically relevant representations. We further\nintroduce the Local-Global Fusion Module designed to progressively capture the\ncorrelations of audio-visual features. Besides, EP-CE Loss is constructed to\nglobally steer optimizations with tripolar penalties. Extensive experiments\nacross three eMotions-related datasets and four public VEA datasets demonstrate\nthe effectiveness of our proposed AV-CANet, while providing broad insights for\nfuture research. Moreover, we conduct ablation studies to examine the critical\ncomponents of our method. Dataset and code will be made available at Github.", "AI": {"tldr": "\u7814\u7a76\u63d0\u51fa\u4e86eMotions\u6570\u636e\u96c6\u548cAV-CANet\u6a21\u578b\uff0c\u7528\u4e8e\u5206\u6790\u77ed\u89c6\u9891\u4e2d\u7684\u60c5\u611f\u3002AV-CANet\u901a\u8fc7\u89c6\u542c\u878d\u5408\u548c\u65b0\u9896\u7684\u635f\u5931\u51fd\u6570\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u77ed\u89c6\u9891\u60c5\u611f\u5206\u6790\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u77ed\u89c6\u9891\uff08SVs\uff09\u7684\u5174\u8d77\u5e26\u6765\u4e86\u65b0\u7684\u89c6\u9891\u60c5\u611f\u5206\u6790\uff08VEA\uff09\u6311\u6218\uff0c\u56e0\u4e3a\u5176\u5185\u5bb9\u591a\u6837\u6027\u3001\u8bed\u4e49\u5dee\u8ddd\u4ee5\u53ca\u89c6\u542c\u4e0d\u4e00\u81f4\u6027\u5bfc\u81f4\u7684\u60c5\u611f\u7ebf\u7d22\u590d\u6742\u5316\u3002\u73b0\u6709\u6570\u636e\u96c6\u7684\u6709\u9650\u6027\u4e5f\u4fc3\u4f7f\u7814\u7a76\u8005\u9700\u8981\u65b0\u7684\u6570\u636e\u96c6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAV-CANet\u7684\u7aef\u5230\u7aef\u89c6\u542c\u878d\u5408\u7f51\u7edc\uff0c\u8be5\u7f51\u7edc\u5229\u7528\u89c6\u9891Transformer\u6765\u6355\u83b7\u8bed\u4e49\u76f8\u5173\u7684\u8868\u793a\u3002\u8fd8\u5f15\u5165\u4e86\u5c40\u90e8-\u5168\u5c40\u878d\u5408\u6a21\u5757\u6765\u9010\u6b65\u6355\u83b7\u89c6\u542c\u7279\u5f81\u7684\u76f8\u5173\u6027\uff0c\u5e76\u6784\u5efa\u4e86EP-CE\u635f\u5931\u6765\u8fdb\u884c\u5168\u5c40\u4f18\u5316\u3002", "result": "\u6784\u5efa\u4e86\u4e00\u4e2a\u540d\u4e3aeMotions\u7684\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u5305\u542b27,996\u4e2a\u5e26\u5168\u65b9\u4f4d\u6ce8\u91ca\u7684\u89c6\u9891\u3002\u63d0\u51fa\u7684AV-CANet\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u51fa\u6709\u6548\u6027\u3002", "conclusion": "AV-CANet\u5728\u4e09\u4e2aeMotions\u76f8\u5173\u6570\u636e\u96c6\u548c\u56db\u4e2a\u516c\u5f00\u7684VEA\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\uff0c\u5e76\u4e3a\u672a\u6765\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u5e7f\u6cdb\u7684\u89c1\u89e3\u3002\u6d88\u878d\u7814\u7a76\u4e5f\u68c0\u9a8c\u4e86\u8be5\u65b9\u6cd5\u7684\u5173\u952e\u7ec4\u6210\u90e8\u5206\u3002"}}
{"id": "2508.07667", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07667", "abs": "https://arxiv.org/abs/2508.07667", "authors": ["Wenkai Li", "Liwen Sun", "Zhenxiang Guan", "Xuhui Zhou", "Maarten Sap"], "title": "1-2-3 Check: Enhancing Contextual Privacy in LLM via Multi-Agent Reasoning", "comment": null, "summary": "Addressing contextual privacy concerns remains challenging in interactive\nsettings where large language models (LLMs) process information from multiple\nsources (e.g., summarizing meetings with private and public information). We\nintroduce a multi-agent framework that decomposes privacy reasoning into\nspecialized subtasks (extraction, classification), reducing the information\nload on any single agent while enabling iterative validation and more reliable\nadherence to contextual privacy norms. To understand how privacy errors emerge\nand propagate, we conduct a systematic ablation over information-flow\ntopologies, revealing when and why upstream detection mistakes cascade into\ndownstream leakage. Experiments on the ConfAIde and PrivacyLens benchmark with\nseveral open-source and closed-sourced LLMs demonstrate that our best\nmulti-agent configuration substantially reduces private information leakage\n(\\textbf{18\\%} on ConfAIde and \\textbf{19\\%} on PrivacyLens with GPT-4o) while\npreserving the fidelity of public content, outperforming single-agent\nbaselines. These results highlight the promise of principled information-flow\ndesign in multi-agent systems for contextual privacy with LLMs.", "AI": {"tldr": "LLM\u5728\u5904\u7406\u5305\u542b\u79c1\u4eba\u548c\u516c\u5171\u4fe1\u606f\u7684\u4ea4\u4e92\u5f0f\u6570\u636e\u65f6\u5b58\u5728\u9690\u79c1\u98ce\u9669\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u65b9\u6cd5\uff0c\u5c06\u9690\u79c1\u63a8\u7406\u5206\u89e3\u4e3a\u5b50\u4efb\u52a1\uff0c\u4ee5\u51cf\u5c11\u4fe1\u606f\u6cc4\u9732\u5e76\u63d0\u9ad8\u51c6\u786e\u6027\uff0c\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u5728\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2d\uff0cLLM\u5904\u7406\u6765\u81ea\u591a\u4e2a\u6765\u6e90\u7684\u4fe1\u606f\uff08\u4f8b\u5982\uff0c\u603b\u7ed3\u5305\u542b\u79c1\u4eba\u548c\u516c\u5171\u4fe1\u606f\u7684\u4f1a\u8bae\uff09\uff0c\u8fd9\u5e26\u6765\u4e86\u5177\u6709\u6311\u6218\u6027\u7684\u4e0a\u4e0b\u6587\u9690\u79c1\u95ee\u9898\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u5c06\u9690\u79c1\u63a8\u7406\u5206\u89e3\u4e3a\u63d0\u53d6\u548c\u5206\u7c7b\u7b49\u4e13\u95e8\u5b50\u4efb\u52a1\uff0c\u4ee5\u89e3\u51b3\u4ea4\u4e92\u5f0f\u73af\u5883\u4e2dLLM\u7684\u4e0a\u4e0b\u6587\u9690\u79c1\u95ee\u9898\u3002\u901a\u8fc7\u7cfb\u7edf\u5730\u6d88\u878d\u4fe1\u606f\u6d41\u62d3\u6251\u7ed3\u6784\u6765\u7814\u7a76\u9690\u79c1\u9519\u8bef\u7684\u4ea7\u751f\u548c\u4f20\u64ad\u3002", "result": "\u5728ConfAIde\u548cPrivacyLens\u57fa\u51c6\u4e0a\uff0c\u4f7f\u7528GPT-4o\u7684\u6211\u4eec\u6700\u597d\u7684\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u53ef\u5c06\u79c1\u4eba\u4fe1\u606f\u6cc4\u9732\u51cf\u5c1118%\uff08ConfAIde\uff09\u548c19%\uff08PrivacyLens\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u516c\u5171\u5185\u5bb9\u7684\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u3002", "conclusion": "\u6211\u4eec\u7684\u591a\u667a\u80fd\u4f53\u6846\u67b6\u901a\u8fc7\u5c06\u9690\u79c1\u63a8\u7406\u5206\u89e3\u4e3a\u4e13\u95e8\u7684\u5b50\u4efb\u52a1\uff08\u63d0\u53d6\u3001\u5206\u7c7b\uff09\uff0c\u51cf\u5c11\u4e86\u4efb\u4f55\u5355\u4e2a\u667a\u80fd\u4f53\u7684\u4fe1\u606f\u8d1f\u8f7d\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u8fed\u4ee3\u9a8c\u8bc1\u548c\u5bf9\u4e0a\u4e0b\u6587\u9690\u79c1\u89c4\u8303\u66f4\u53ef\u9760\u7684\u9075\u5b88\u3002\u5b9e\u9a8c\u8868\u660e\uff0c\u6211\u4eec\u6700\u597d\u7684\u591a\u667a\u80fd\u4f53\u914d\u7f6e\u5728ConfAIde\u548cPrivacyLens\u57fa\u51c6\u4e0a\uff0c\u53ef\u4ee5\u663e\u8457\u51cf\u5c11\u79c1\u4eba\u4fe1\u606f\u6cc4\u9732\uff08\u5728ConfAIde\u4e0a\u51cf\u5c1118%\uff0c\u5728PrivacyLens\u4e0a\u51cf\u5c1119%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u516c\u5171\u5185\u5bb9\u7684\u4fdd\u771f\u5ea6\uff0c\u4f18\u4e8e\u5355\u667a\u80fd\u4f53\u57fa\u7ebf\u3002\u8fd9\u4e9b\u7ed3\u679c\u7a81\u663e\u4e86\u5728\u5177\u6709LLM\u7684\u4e0a\u4e0b\u6587\u9690\u79c1\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\u4e2d\uff0c\u57fa\u4e8e\u539f\u5219\u7684\u4fe1\u606f\u6d41\u8bbe\u8ba1\u7684\u627f\u8bfa\u3002"}}
{"id": "2508.07085", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07085", "abs": "https://arxiv.org/abs/2508.07085", "authors": ["N Harshit", "K Mounvik"], "title": "Improving Real-Time Concept Drift Detection using a Hybrid Transformer-Autoencoder Framework", "comment": null, "summary": "In applied machine learning, concept drift, which is either gradual or abrupt\nchanges in data distribution, can significantly reduce model performance.\nTypical detection methods,such as statistical tests or reconstruction-based\nmodels,are generally reactive and not very sensitive to early detection. Our\nstudy proposes a hybrid framework consisting of Transformers and Autoencoders\nto model complex temporal dynamics and provide online drift detection. We\ncreate a distinct Trust Score methodology, which includes signals on (1)\nstatistical and reconstruction-based drift metrics, more specifically, PSI,\nJSD, Transformer-AE error, (2) prediction uncertainty, (3) rules violations,\nand (4) trend of classifier error aligned with the combined metrics defined by\nthe Trust Score. Using a time sequenced airline passenger data set with\nsynthetic drift, our proposed model allows for a better detection of drift\nusing as a whole and at different detection thresholds for both sensitivity and\ninterpretability compared to baseline methods and provides a strong pipeline\nfor drift detection in real time for applied machine learning. We evaluated\nperformance using a time-sequenced airline passenger dataset having the\ngradually injected stimulus of drift in expectations,e.g. permuted ticket\nprices in later batches, broken into 10 time segments [1].In the data, our\nresults support that the Transformation-Autoencoder detected drift earlier and\nwith more sensitivity than the autoencoders commonly used in the literature,\nand provided improved modeling over more error rates and logical violations.\nTherefore, a robust framework was developed to reliably monitor concept drift.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eTransformer\u548cAutoencoder\u7684\u6df7\u5408\u6846\u67b6\uff0c\u901a\u8fc7Trust Score\u65b9\u6cd5\u80fd\u66f4\u65e9\u3001\u66f4\u7075\u654f\u5730\u68c0\u6d4b\u6982\u5ff5\u6f02\u79fb\u3002", "motivation": "\u73b0\u6709\u7684\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u662f\u53cd\u5e94\u6027\u7684\uff0c\u5bf9\u65e9\u671f\u68c0\u6d4b\u4e0d\u654f\u611f\uff0c\u800c\u6982\u5ff5\u6f02\u79fb\uff08\u6570\u636e\u5206\u5e03\u7684\u6e10\u8fdb\u6216\u7a81\u7136\u53d8\u5316\uff09\u4f1a\u663e\u8457\u964d\u4f4e\u5e94\u7528\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u4e86Transformers\u548cAutoencoders\u6765\u6a21\u62df\u590d\u6742\u7684\u65f6\u95f4\u52a8\u6001\u5e76\u63d0\u4f9b\u5728\u7ebf\u6f02\u79fb\u68c0\u6d4b\u3002\u6784\u5efa\u4e86\u4e00\u4e2aTrust Score\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86\u7edf\u8ba1\u548c\u91cd\u6784\u6307\u6807\uff08PSI\u3001JSD\u3001Transformer-AE\u8bef\u5dee\uff09\u3001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3001\u89c4\u5219\u51b2\u7a81\u4ee5\u53ca\u4e0eTrust Score\u5b9a\u4e49\u7684\u7ec4\u5408\u6307\u6807\u4e00\u81f4\u7684\u5206\u7c7b\u5668\u9519\u8bef\u7387\u8d8b\u52bf\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6df7\u5408\u6846\u67b6\u5728\u5305\u542b\u6ce8\u5165\u6f02\u79fb\u7684\u822a\u7a7a\u4e58\u5ba2\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e\u4e0e\u5e38\u7528\u7684Autoencoders\u548c\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cTransformer-Autoencoder\u80fd\u591f\u66f4\u65e9\u3001\u66f4\u7075\u654f\u5730\u68c0\u6d4b\u5230\u6f02\u79fb\uff0c\u5e76\u5728\u8bef\u5dee\u7387\u548c\u903b\u8f91\u51b2\u7a81\u65b9\u9762\u63d0\u4f9b\u4e86\u66f4\u597d\u7684\u5efa\u6a21\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408Transformer\u548cAutoencoder\u7684\u6df7\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u7ebf\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\uff0c\u901a\u8fc7\u7ed3\u5408\u7edf\u8ba1\u6307\u6807\u3001\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3001\u89c4\u5219\u51b2\u7a81\u548c\u5206\u7c7b\u5668\u9519\u8bef\u7387\u7b49\u591a\u79cd\u4fe1\u53f7\uff0c\u6784\u5efa\u4e86\u4e00\u4e2aTrust Score\u65b9\u6cd5\uff0c\u80fd\u591f\u66f4\u65e9\u3001\u66f4\u7075\u654f\u5730\u68c0\u6d4b\u6f02\u79fb\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.08182", "categories": ["quant-ph", "cond-mat.other", "physics.comp-ph"], "pdf": "https://arxiv.org/pdf/2508.08182", "abs": "https://arxiv.org/abs/2508.08182", "authors": ["F. Faria", "C. C. Nelmes", "T. J. G. Apollaro", "T. P. Spiller", "I. D'Amico"], "title": "Fast and efficient long-distance quantum state transfer in long-range spin-$\\frac{1}{2}$ models", "comment": "10 pages, 8 figures, 4 tables", "summary": "Quantum state transfer is investigated beyond the nearest-neighbour coupling\nscheme in long spin-$\\frac{1}{2}$ linear chains. Exploiting the properties of\nthe next-nearest neighbour Hamiltonian's dispersion relation, it is shown that\nwith minimal engineering, i.e., an on-site magnetic field on the two end sites\nand only a few symmetrically-modified end inter-site couplings, an average\ntransfer fidelity above $99\\%$ can be achieved. To leading order, the required\ntime scales linearly with the length of the chain. Such a fast, high-quality\nquantum state transfer is based on the ballistic propagation of the wave packet\ncentred in the linear region of the dispersion relation by means of the on-site\nmagnetic field. At the same time, the wave packet width, modulated by the\ninter-site couplings at the chain ends, whose values are found via a carefully\ndesigned genetic algorithm, is constrained mostly in the linear region of the\ndispersion relation. Our coupling scheme is shown to hold for arbitrary values\nof the next-nearest inter-site coupling and can be straightforwardly applied to\nlonger range coupling schemes.", "AI": {"tldr": "\u901a\u8fc7\u5bf9\u957f\u81ea\u65cb\u94fe\u8fdb\u884c\u5fae\u8c03\uff08\u7aef\u70b9\u78c1\u573a\u548c\u8026\u5408\u4fee\u6539\uff09\uff0c\u53ef\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u91cf\u5b50\u6001\u4f20\u8f93\u3002", "motivation": "\u7814\u7a76\u5728\u957f\u81ea\u65cb\u94fe\u4e2d\u8d85\u8d8a\u6700\u8fd1\u90bb\u8026\u5408\u7684\u91cf\u5b50\u6001\u4f20\u8f93\u95ee\u9898\u3002", "method": "\u5229\u7528\u4e0b\u4e00\u6700\u8fd1\u90bb\u54c8\u5bc6\u987f\u91cf\u7684\u8272\u6563\u5173\u7cfb\u6027\u8d28\uff0c\u901a\u8fc7\u5728\u4e24\u4e2a\u7aef\u70b9\u5904\u65bd\u52a0\u5c0f\u7684\u9759\u78c1\u573a\uff0c\u5e76\u5bf9\u5c11\u6570\u51e0\u4e2a\u7aef\u70b9\u5904\u7684\u8026\u5408\u8fdb\u884c\u5bf9\u79f0\u4fee\u6539\uff0c\u5b9e\u73b0\u4e86\u8d85\u8fc799%\u7684\u5e73\u5747\u4f20\u8f93\u4fdd\u771f\u5ea6\u3002\u6240\u9700\u65f6\u95f4\u4e0e\u94fe\u957f\u6210\u6b63\u6bd4\u3002\u901a\u8fc7\u4ed4\u7ec6\u8bbe\u8ba1\u7684\u9057\u4f20\u7b97\u6cd5\u627e\u5230\u7aef\u70b9\u5904\u7684\u8026\u5408\u503c\uff0c\u4ee5\u5c06\u6ce2\u5305\u5bbd\u5ea6\u7ea6\u675f\u5728\u7ebf\u6027\u8272\u6563\u533a\u57df\u5185\u3002", "result": "\u5728\u957f\u81ea\u65cb\u94fe\u4e2d\uff0c\u901a\u8fc7\u5728\u4e24\u4e2a\u7aef\u70b9\u5904\u65bd\u52a0\u5c0f\u7684\u9759\u78c1\u573a\u5e76\u5bf9\u79f0\u4fee\u6539\u5c11\u6570\u51e0\u4e2a\u7aef\u70b9\u5904\u7684\u8026\u5408\uff0c\u53ef\u4ee5\u5b9e\u73b0\u8d85\u8fc799%\u7684\u5e73\u5747\u4f20\u8f93\u4fdd\u771f\u5ea6\uff0c\u4e14\u6240\u9700\u65f6\u95f4\u4e0e\u94fe\u957f\u6210\u6b63\u6bd4\u3002", "conclusion": "\u8be5\u65b9\u6848\u9002\u7528\u4e8e\u4efb\u610f\u4e0b\u4e00\u6700\u8fd1\u90bb\u8026\u5408\u503c\uff0c\u5e76\u4e14\u53ef\u4ee5\u8f7b\u677e\u5e94\u7528\u4e8e\u66f4\u957f\u8303\u56f4\u7684\u8026\u5408\u65b9\u6848\u3002"}}
{"id": "2508.07434", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07434", "abs": "https://arxiv.org/abs/2508.07434", "authors": ["Zhiyi Lyu", "Jianguo Huang", "Yanchen Deng", "Steven Hoi", "Bo An"], "title": "Let's Revise Step-by-Step: A Unified Local Search Framework for Code Generation with LLMs", "comment": null, "summary": "Large Language Models (LLMs) with inference-time scaling techniques show\npromise for code generation, yet face notable efficiency and scalability\nchallenges. Construction-based tree-search methods suffer from rapid growth in\ntree size, high token consumption, and lack of anytime property. In contrast,\nimprovement-based methods offer better performance but often struggle with\nuninformative reward signals and inefficient search strategies. In this work,\nwe propose \\textbf{ReLoc}, a unified local search framework which effectively\nperforms step-by-step code revision. Specifically, ReLoc explores a series of\nlocal revisions through four key algorithmic components: initial code drafting,\nneighborhood code generation, candidate evaluation, and incumbent code\nupdating, each of which can be instantiated with specific decision rules to\nrealize different local search algorithms such as Hill Climbing (HC) or Genetic\nAlgorithm (GA). Furthermore, we develop a specialized revision reward model\nthat evaluates code quality based on revision distance to produce fine-grained\npreferences that guide the local search toward more promising candidates.\nFinally, our extensive experimental results demonstrate that our approach\nachieves superior performance across diverse code generation tasks,\nsignificantly outperforming both construction-based tree search as well as the\nstate-of-the-art improvement-based code generation methods.", "AI": {"tldr": "ReLoc\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u7279\u5b9a\u7684\u7b97\u6cd5\u7ec4\u4ef6\u548c\u4fee\u8ba2\u5956\u52b1\u6a21\u578b\u6765\u9010\u6b65\u8fdb\u884c\u4ee3\u7801\u4fee\u8ba2\uff0c\u89e3\u51b3\u4e86LLMs\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u5e76\u5728\u5404\u79cd\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u4ee3\u7801\u751f\u6210\u4e2d\u7684\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u65b9\u6cd5\uff08\u5982\u57fa\u4e8e\u6784\u9020\u7684\u6811\u641c\u7d22\u548c\u57fa\u4e8e\u6539\u8fdb\u7684\u65b9\u6cd5\uff09\u7684\u7f3a\u70b9\u3002", "method": "ReLoc\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u5c40\u90e8\u641c\u7d22\u6846\u67b6\uff0c\u901a\u8fc7\u56db\u4e2a\u5173\u952e\u7684\u7b97\u6cd5\u7ec4\u4ef6\uff08\u521d\u59cb\u4ee3\u7801\u8d77\u8349\u3001\u90bb\u57df\u4ee3\u7801\u751f\u6210\u3001\u5019\u9009\u8bc4\u4f30\u548c\u521d\u59cb\u4ee3\u7801\u66f4\u65b0\uff09\u6765\u9010\u6b65\u8fdb\u884c\u4ee3\u7801\u4fee\u8ba2\u3002\u5b83\u8fd8\u5f00\u53d1\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684\u4fee\u8ba2\u5956\u52b1\u6a21\u578b\u6765\u6307\u5bfc\u5c40\u90e8\u641c\u7d22\u3002", "result": "ReLoc\u5728\u5404\u79cd\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u4e8e\u6784\u9020\u7684\u6811\u641c\u7d22\u548c\u6700\u5148\u8fdb\u7684\u57fa\u4e8e\u6539\u8fdb\u7684\u4ee3\u7801\u751f\u6210\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "ReLoc\u6846\u67b6\u5728\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u57fa\u4e8e\u6784\u9020\u7684\u6811\u641c\u7d22\u548c\u57fa\u4e8e\u6539\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728\u5404\u79cd\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.06904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06904", "abs": "https://arxiv.org/abs/2508.06904", "authors": ["Chao Yin", "Jide Li", "Xiaoqiang Li"], "title": "A Simple yet Powerful Instance-Aware Prompting Framework for Training-free Camouflaged Object Segmentation", "comment": "under review", "summary": "Camouflaged Object Segmentation (COS) remains highly challenging due to the\nintrinsic visual similarity between target objects and their surroundings.\nWhile training-based COS methods achieve good performance, their performance\ndegrades rapidly with increased annotation sparsity. To circumvent this\nlimitation, recent studies have explored training-free COS methods, leveraging\nthe Segment Anything Model (SAM) by automatically generating visual prompts\nfrom a single task-generic prompt (\\textit{e.g.}, \"\\textit{camouflaged\nanimal}\") uniformly applied across all test images. However, these methods\ntypically produce only semantic-level visual prompts, causing SAM to output\ncoarse semantic masks and thus failing to handle scenarios with multiple\ndiscrete camouflaged instances effectively. To address this critical\nlimitation, we propose a simple yet powerful \\textbf{I}nstance-\\textbf{A}ware\n\\textbf{P}rompting \\textbf{F}ramework (IAPF), the first training-free COS\npipeline that explicitly converts a task-generic prompt into fine-grained\ninstance masks. Specifically, the IAPF comprises three steps: (1) Text Prompt\nGenerator, utilizing task-generic queries to prompt a Multimodal Large Language\nModel (MLLM) for generating image-specific foreground and background tags; (2)\n\\textbf{Instance Mask Generator}, leveraging Grounding DINO to produce precise\ninstance-level bounding box prompts, alongside the proposed Single-Foreground\nMulti-Background Prompting strategy to sample region-constrained point prompts\nwithin each box, enabling SAM to yield a candidate instance mask; (3)\nSelf-consistency Instance Mask Voting, which selects the final COS prediction\nby identifying the candidate mask most consistent across multiple candidate\ninstance masks. Extensive evaluations on standard COS benchmarks demonstrate\nthat the proposed IAPF significantly surpasses existing state-of-the-art\ntraining-free COS methods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aIAPF\u7684\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u4f2a\u88c5\u76ee\u6807\u5206\u5272\uff08COS\uff09\u7684\u6311\u6218\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u7ed3\u5408MLLM\u548cGrounding DINO\u6765\u751f\u6210\u66f4\u7cbe\u786e\u7684\u5b9e\u4f8b\u7ea7\u63d0\u793a\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u4ec5\u80fd\u751f\u6210\u8bed\u4e49\u7ea7\u63d0\u793a\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u8bad\u7ec3\u7684COS\u65b9\u6cd5\u5728\u6807\u6ce8\u7a00\u758f\u6027\u589e\u52a0\u65f6\u6027\u80fd\u4f1a\u8fc5\u901f\u4e0b\u964d\u3002\u4e3a\u4e86\u89c4\u907f\u8fd9\u4e00\u9650\u5236\uff0c\u7814\u7a76\u4eba\u5458\u63a2\u7d22\u4e86\u65e0\u76d1\u7763COS\u65b9\u6cd5\uff0c\u5229\u7528SAM\u81ea\u52a8\u751f\u6210\u89c6\u89c9\u63d0\u793a\u3002\u7136\u800c\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u901a\u5e38\u53ea\u4ea7\u751f\u8bed\u4e49\u7ea7\u89c6\u89c9\u63d0\u793a\uff0c\u5bfc\u81f4SAM\u8f93\u51fa\u7c97\u7565\u7684\u8bed\u4e49\u63a9\u7801\uff0c\u5e76\u4e14\u5728\u6709\u6548\u5904\u7406\u591a\u4e2a\u79bb\u6563\u7684\u5b9e\u4f8b\u65f6\u4f1a\u5931\u8d25\u3002", "method": "IAPF\u6846\u67b6\u5305\u62ec\u4e09\u4e2a\u6b65\u9aa4\uff1a1.\u6587\u672c\u63d0\u793a\u751f\u6210\u5668\uff0c\u5229\u7528\u4efb\u52a1\u901a\u7528\u67e5\u8be2\u63d0\u793a\u591a\u6a21\u6001\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u751f\u6210\u56fe\u50cf\u7279\u5b9a\u7684\u524d\u666f\u548c\u80cc\u666f\u6807\u7b7e\uff1b2.\u5b9e\u4f8b\u63a9\u7801\u751f\u6210\u5668\uff0c\u5229\u7528Grounding DINO\u751f\u6210\u7cbe\u786e\u7684\u5b9e\u4f8b\u7ea7\u8fb9\u754c\u6846\u63d0\u793a\uff0c\u5e76\u63d0\u51fa\u5355\u524d\u666f\u591a\u80cc\u666f\u63d0\u793a\u7b56\u7565\u6765\u91c7\u6837\u533a\u57df\u7ea6\u675f\u70b9\u63d0\u793a\uff1b3.\u81ea\u4e00\u81f4\u6027\u5b9e\u4f8b\u63a9\u7801\u6295\u7968\uff0c\u901a\u8fc7\u8bc6\u522b\u8de8\u591a\u4e2a\u5019\u9009\u5b9e\u4f8b\u63a9\u7801\u6700\u4e00\u81f4\u7684\u63a9\u7801\u6765\u9009\u62e9\u6700\u7ec8\u7684COS\u9884\u6d4b\u3002", "result": "IAPF\u6846\u67b6\u88ab\u8bc1\u660e\u5728\u6807\u51c6\u7684COS\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u7684\u65e0\u76d1\u7763COS\u65b9\u6cd5\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684IAPF\u6846\u67b6\u901a\u8fc7\u5229\u7528MLLM\u751f\u6210\u56fe\u50cf\u7279\u5b9a\u7684\u524d\u666f\u548c\u80cc\u666f\u6807\u7b7e\uff0c\u5e76\u7ed3\u5408Grounding DINO\u751f\u6210\u7cbe\u786e\u7684\u5b9e\u4f8b\u7ea7\u8fb9\u754c\u6846\u63d0\u793a\uff0c\u4ee5\u53ca\u63d0\u51fa\u7684\u5355\u524d\u666f\u591a\u80cc\u666f\u63d0\u793a\u7b56\u7565\u6765\u91c7\u6837\u6bcf\u4e2a\u6846\u5185\u7684\u533a\u57df\u7ea6\u675f\u70b9\u63d0\u793a\uff0c\u4f7fSAM\u80fd\u591f\u4ea7\u751f\u5019\u9009\u5b9e\u4f8b\u63a9\u7801\uff0c\u6700\u7ec8\u901a\u8fc7\u8bc6\u522b\u8de8\u591a\u4e2a\u5019\u9009\u5b9e\u4f8b\u63a9\u7801\u6700\u4e00\u81f4\u7684\u63a9\u7801\u6765\u9009\u62e9\u6700\u7ec8\u7684COS\u9884\u6d4b\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u65e0\u76d1\u7763COS\u65b9\u6cd5\u3002"}}
{"id": "2508.07102", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07102", "abs": "https://arxiv.org/abs/2508.07102", "authors": ["Yang Cao", "Yubin Chen", "Zhao Song", "Jiahao Zhang"], "title": "Towards High-Order Mean Flow Generative Models: Feasibility, Expressivity, and Provably Efficient Criteria", "comment": null, "summary": "Generative modelling has seen significant advances through simulation-free\nparadigms such as Flow Matching, and in particular, the MeanFlow framework,\nwhich replaces instantaneous velocity fields with average velocities to enable\nefficient single-step sampling. In this work, we introduce a theoretical study\non Second-Order MeanFlow, a novel extension that incorporates average\nacceleration fields into the MeanFlow objective. We first establish the\nfeasibility of our approach by proving that the average acceleration satisfies\na generalized consistency condition analogous to first-order MeanFlow, thereby\nsupporting stable, one-step sampling and tractable loss functions. We then\ncharacterize its expressivity via circuit complexity analysis, showing that\nunder mild assumptions, the Second-Order MeanFlow sampling process can be\nimplemented by uniform threshold circuits within the $\\mathsf{TC}^0$ class.\nFinally, we derive provably efficient criteria for scalable implementation by\nleveraging fast approximate attention computations: we prove that attention\noperations within the Second-Order MeanFlow architecture can be approximated to\nwithin $1/\\mathrm{poly}(n)$ error in time $n^{2+o(1)}$. Together, these results\nlay the theoretical foundation for high-order flow matching models that combine\nrich dynamics with practical sampling efficiency.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e8c\u9636\u5747\u503c\u6d41\uff0c\u5c06\u5e73\u5747\u52a0\u901f\u5ea6\u573a\u7eb3\u5165\u5747\u503c\u6d41\u76ee\u6807\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u91c7\u6837\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5176\u7406\u8bba\u53ef\u884c\u6027\u3001\u8868\u8fbe\u80fd\u529b\u548c\u53ef\u6269\u5c55\u6027\uff0c\u4e3a\u9ad8\u9636\u6d41\u5339\u914d\u6a21\u578b\u94fa\u5e73\u4e86\u9053\u8def\u3002", "motivation": "\u751f\u6210\u6a21\u578b\u901a\u8fc7\u65e0\u6a21\u62df\u8303\u5f0f\uff08\u5982\u6d41\u5339\u914d\uff09\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u7279\u522b\u662f\u5747\u503c\u6d41\u6846\u67b6\uff0c\u5b83\u7528\u5e73\u5747\u901f\u5ea6\u66ff\u6362\u77ac\u65f6\u901f\u5ea6\u573a\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u5355\u6b65\u91c7\u6837\u3002\u5728\u672c\u7814\u7a76\u4e2d\uff0c\u6211\u4eec\u5bf9\u4e8c\u9636\u5747\u503c\u6d41\u8fdb\u884c\u4e86\u7406\u8bba\u7814\u7a76\uff0c\u8fd9\u662f\u4e00\u4e2a\u901a\u8fc7\u5c06\u5e73\u5747\u52a0\u901f\u5ea6\u573a\u7eb3\u5165\u5747\u503c\u6d41\u76ee\u6807\u6765\u5f15\u5165\u7684\u65b0\u6269\u5c55\u3002", "method": "\u6211\u4eec\u9996\u5148\u901a\u8fc7\u8bc1\u660e\u5e73\u5747\u52a0\u901f\u5ea6\u6ee1\u8db3\u7c7b\u4f3c\u4e8e\u4e00\u9636\u5747\u503c\u6d41\u7684\u5e7f\u4e49\u4e00\u81f4\u6027\u6761\u4ef6\u6765\u5efa\u7acb\u6211\u4eec\u65b9\u6cd5\u7684\u53ef\u884c\u6027\uff0c\u4ece\u800c\u652f\u6301\u7a33\u5b9a\u7684\u5355\u6b65\u91c7\u6837\u548c\u53ef\u5904\u7406\u7684\u635f\u5931\u51fd\u6570\u3002\u7136\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u7535\u8def\u590d\u6742\u6027\u5206\u6790\u6765\u8868\u5f81\u5176\u8868\u8fbe\u80fd\u529b\uff0c\u8868\u660e\u5728\u6e29\u548c\u7684\u5047\u8bbe\u4e0b\uff0c\u4e8c\u9636\u5747\u503c\u6d41\u91c7\u6837\u8fc7\u7a0b\u53ef\u4ee5\u901a\u8fc7 $\\mathsf{TC}^0$ \u7c7b\u4e2d\u7684\u5747\u5300\u9608\u503c\u7535\u8def\u6765\u5b9e\u73b0\u3002\u6700\u540e\uff0c\u6211\u4eec\u901a\u8fc7\u5229\u7528\u5feb\u901f\u8fd1\u4f3c\u6ce8\u610f\u529b\u8ba1\u7b97\uff0c\u63a8\u5bfc\u51fa\u53ef\u6269\u5c55\u5b9e\u73b0\u7684\u5177\u6709\u53ef\u8bc1\u660e\u6548\u7387\u7684\u6807\u51c6\u3002", "result": "\u6211\u4eec\u8bc1\u660e\u4e86\u5e73\u5747\u52a0\u901f\u5ea6\u6ee1\u8db3\u4e00\u4e2a\u5e7f\u4e49\u7684\u4e00\u81f4\u6027\u6761\u4ef6\uff0c\u8fd9\u652f\u6301\u4e86\u7a33\u5b9a\u7684\u5355\u6b65\u91c7\u6837\u548c\u53ef\u5904\u7406\u7684\u635f\u5931\u51fd\u6570\u3002\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u4e8c\u9636\u5747\u503c\u6d41\u91c7\u6837\u8fc7\u7a0b\u53ef\u4ee5\u7528 $\\mathsf{TC}^0$ \u7c7b\u4e2d\u7684\u5747\u5300\u9608\u503c\u7535\u8def\u5b9e\u73b0\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u4e8c\u9636\u5747\u503c\u6d41\u67b6\u6784\u4e2d\u7684\u6ce8\u610f\u529b\u64cd\u4f5c\u53ef\u4ee5\u5728 $n^{2+o(1)}$ \u65f6\u95f4\u5185\u8fd1\u4f3c\u5230 $1/\\mathrm{poly}(n)$ \u7684\u8bef\u5dee\u3002", "conclusion": "\u6211\u4eec\u7684\u7ed3\u679c\u4e3a\u9ad8\u9636\u6d41\u5339\u914d\u6a21\u578b\u5960\u5b9a\u4e86\u7406\u8bba\u57fa\u7840\uff0c\u8fd9\u4e9b\u6a21\u578b\u7ed3\u5408\u4e86\u4e30\u5bcc\u7684\u52a8\u529b\u5b66\u548c\u5b9e\u7528\u7684\u91c7\u6837\u6548\u7387\u3002"}}
{"id": "2508.07917", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07917", "abs": "https://arxiv.org/abs/2508.07917", "authors": ["Jason Lee", "Jiafei Duan", "Haoquan Fang", "Yuquan Deng", "Shuo Liu", "Boyang Li", "Bohan Fang", "Jieyu Zhang", "Yi Ru Wang", "Sangho Lee", "Winson Han", "Wilbert Pumacay", "Angelica Wu", "Rose Hendrix", "Karen Farley", "Eli VanderBilt", "Ali Farhadi", "Dieter Fox", "Ranjay Krishna"], "title": "MolmoAct: Action Reasoning Models that can Reason in Space", "comment": "Appendix on Blogpost: https://allenai.org/blog/molmoact", "summary": "Reasoning is central to purposeful action, yet most robotic foundation models\nmap perception and instructions directly to control, which limits adaptability,\ngeneralization, and semantic grounding. We introduce Action Reasoning Models\n(ARMs), a class of vision-language-action models that integrate perception,\nplanning, and control through a structured three-stage pipeline. Our model,\nMolmoAct, encodes observations and instructions into depth-aware perception\ntokens, generates mid-level spatial plans as editable trajectory traces, and\npredicts precise low-level actions, enabling explainable and steerable\nbehavior. MolmoAct-7B-D achieves strong performance across simulation and\nreal-world settings: 70.5% zero-shot accuracy on SimplerEnv Visual Matching\ntasks, surpassing closed-source Pi-0 and GR00T N1; 86.6% average success on\nLIBERO, including an additional 6.3% gain over ThinkAct on long-horizon tasks;\nand in real-world fine-tuning, an additional 10% (single-arm) and an additional\n22.7% (bimanual) task progression over Pi-0-FAST. It also outperforms baselines\nby an additional 23.3% on out-of-distribution generalization and achieves top\nhuman-preference scores for open-ended instruction following and trajectory\nsteering. Furthermore, we release, for the first time, the MolmoAct Dataset --\na mid-training robot dataset comprising over 10,000 high quality robot\ntrajectories across diverse scenarios and tasks. Training with this dataset\nyields an average 5.5% improvement in general performance over the base model.\nWe release all model weights, training code, our collected dataset, and our\naction reasoning dataset, establishing MolmoAct as both a state-of-the-art\nrobotics foundation model and an open blueprint for building ARMs that\ntransform perception into purposeful action through structured reasoning.\nBlogpost: https://allenai.org/blog/molmoact", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u79cd\u540d\u4e3a MolmoAct \u7684\u65b0\u578b\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff0c\u5b83\u901a\u8fc7\u96c6\u6210\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u6765\u6539\u8fdb\u673a\u5668\u4eba\u884c\u4e3a\u3002\u8be5\u6a21\u578b\u5728\u591a\u9879\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u53d1\u5e03\u4e86\u76f8\u5173\u6570\u636e\u96c6\u548c\u4ee3\u7801\uff0c\u4ee5\u63a8\u52a8\u673a\u5668\u4eba\u9886\u57df\u7684\u53d1\u5c55\u3002", "motivation": "\u5f53\u524d\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\u5927\u591a\u5c06\u611f\u77e5\u548c\u6307\u4ee4\u76f4\u63a5\u6620\u5c04\u5230\u63a7\u5236\uff0c\u8fd9\u9650\u5236\u4e86\u673a\u5668\u4eba\u7684\u9002\u5e94\u6027\u3001\u6cdb\u5316\u80fd\u529b\u548c\u8bed\u4e49\u57fa\u7840\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6a21\u578b\u7c7b\u522b\u2014\u2014\u884c\u52a8\u63a8\u7406\u6a21\u578b\uff08ARMs\uff09\uff0c\u4ee5\u5b9e\u73b0\u66f4\u5f3a\u7684\u673a\u5668\u4eba\u80fd\u529b\u3002", "method": "\u901a\u8fc7\u4e09\u9636\u6bb5\u7684\u7ed3\u6784\u5316\u6d41\u7a0b\uff0c\u5c06\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\u96c6\u6210\u5230\u884c\u52a8\u63a8\u7406\u6a21\u578b\uff08ARMs\uff09\u4e2d\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6a21\u578b\u5c06\u89c2\u5bdf\u548c\u6307\u4ee4\u7f16\u7801\u4e3a\u6df1\u5ea6\u611f\u77e5\u7684\u611f\u77e5\u4ee4\u724c\uff0c\u751f\u6210\u53ef\u7f16\u8f91\u7684\u8f68\u8ff9\u8ffd\u8e2a\u4f5c\u4e3a\u4e2d\u7ea7\u7a7a\u95f4\u89c4\u5212\uff0c\u5e76\u9884\u6d4b\u7cbe\u786e\u7684\u4f4e\u7ea7\u52a8\u4f5c\u3002", "result": "MolmoAct-7B-D \u5728\u5404\u9879\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u7ee9\uff1a\u5728 SimplerEnv \u53ef\u89c6\u5339\u914d\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86 70.5% \u7684\u96f6\u6837\u672c\u51c6\u786e\u7387\uff0c\u4f18\u4e8e Pi-0 \u548c GR00T N1\uff1b\u5728 LIBERO \u6570\u636e\u96c6\u4e0a\u5e73\u5747\u6210\u529f\u7387\u4e3a 86.6%\uff0c\u5728\u957f\u65f6\u4efb\u52a1\u4e0a\u6bd4 ThinkAct \u63d0\u9ad8\u4e86 6.3%\uff1b\u5728\u771f\u5b9e\u4e16\u754c\u5fae\u8c03\u4e2d\uff0c\u6bd4 Pi-0-FAST \u5355\u81c2\u548c\u53cc\u81c2\u4efb\u52a1\u5206\u522b\u63d0\u9ad8\u4e86 10% \u548c 22.7%\uff1b\u5728\u5206\u5e03\u5916\u6cdb\u5316\u4efb\u52a1\u4e0a\u6bd4\u57fa\u7ebf\u6a21\u578b\u63d0\u9ad8\u4e86 23.3%\uff1b\u5e76\u5728\u5f00\u653e\u5f0f\u6307\u4ee4\u9075\u5faa\u548c\u8f68\u8ff9\u5f15\u5bfc\u65b9\u9762\u83b7\u5f97\u4e86\u9ad8\u4eba\u7c7b\u504f\u597d\u8bc4\u5206\u3002\u6b64\u5916\uff0c\u65b0\u53d1\u5e03\u7684 MolmoAct \u6570\u636e\u96c6\uff08\u5305\u542b\u8d85\u8fc7 10,000 \u4e2a\u9ad8\u8d28\u91cf\u673a\u5668\u4eba\u8f68\u8ff9\uff09\u53ef\u5c06\u6a21\u578b\u6027\u80fd\u5e73\u5747\u63d0\u9ad8 5.5%\u3002", "conclusion": "MolmoAct \u662f\u4e00\u4e2a\u5148\u8fdb\u7684\u673a\u5668\u4eba\u57fa\u7840\u6a21\u578b\uff0c\u901a\u8fc7\u96c6\u6210\u7684\u611f\u77e5\u3001\u89c4\u5212\u548c\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u63a7\u7684\u884c\u4e3a\uff0c\u5e76\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6a21\u578b\u4ee5\u53ca MolmoAct \u6570\u636e\u96c6\u7684\u53d1\u5e03\uff0c\u4e3a\u673a\u5668\u4eba\u9886\u57df\u7684\u7814\u7a76\u548c\u53d1\u5c55\u63d0\u4f9b\u4e86\u5f00\u653e\u7684\u84dd\u56fe\u3002"}}
{"id": "2508.08188", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08188", "abs": "https://arxiv.org/abs/2508.08188", "authors": ["Matthew Girling", "Ben Criger", "Cristina Cirstoiu"], "title": "Characterization of syndrome-dependent logical noise in detector regions", "comment": "24 pages, 14 figures; comments welcome!", "summary": "Characterizing how quantum error correction circuits behave under realistic\nhardware noise is essential for testing the premises that enable scalable fault\ntolerance. Logical error rates conditioned on syndrome outcomes are needed to\nenable noise-aware decoding and validate threshold-relevant assumptions. We\nintroduce a protocol to directly estimate the logical Pauli channels (and pure\nerrors) associated with detector regions formed of two or more syndrome\nextraction gadgets, conditioned on observing a particular parity in the\nsyndrome outcomes. The method is SPAM-robust and most suitable for flag-based\nsyndrome measurement schemes. For classical processing of the experimental data\nwe implement a Bayesian modelling approach. We validate this new protocol on a\nsmall error-detecting code using Quantinuum H1-1, a trapped-ion device, and\ndemonstrate that several noise diagnostic tests for fault tolerance improve\nsignificantly when using noise tailoring and mitigation strategies, such as\nswapped measurements for leakage protection, and Pauli frame randomization.", "AI": {"tldr": "\u5728 H1-1 \u79bb\u5b50\u9631\u8bbe\u5907\u4e0a\uff0c\u5f00\u53d1\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u8bae\u6765\u8868\u5f81\u91cf\u5b50\u7ea0\u9519\u7535\u8def\u7684\u566a\u58f0\uff0c\u5e76\u4f7f\u7528\u566a\u58f0\u5b9a\u5236\u548c\u7f13\u89e3\u7b56\u7565\u6539\u8fdb\u4e86\u8bca\u65ad\u6d4b\u8bd5\u3002", "motivation": "\u4e3a\u4e86\u5728\u73b0\u5b9e\u7684\u786c\u4ef6\u566a\u58f0\u4e0b\u8868\u5f81\u91cf\u5b50\u7ea0\u9519\u7535\u8def\u7684\u884c\u4e3a\uff0c\u9700\u8981\u4e86\u89e3\u903b\u8f91\u9519\u8bef\u7387\u4e0e\u7efc\u5408\u7ed3\u679c\u7684\u6761\u4ef6\u5173\u7cfb\uff0c\u4ee5\u5b9e\u73b0\u566a\u58f0\u611f\u77e5\u89e3\u7801\u548c\u9a8c\u8bc1\u9608\u503c\u76f8\u5173\u7684\u5047\u8bbe\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u534f\u8bae\uff0c\u7528\u4e8e\u76f4\u63a5\u4f30\u8ba1\u4e0e\u7531\u4e24\u4e2a\u6216\u591a\u4e2a\u7efc\u5408\u63d0\u53d6\u88c5\u7f6e\u7ec4\u6210\u7684\u68c0\u6d4b\u5668\u533a\u57df\u76f8\u5173\u7684\u903b\u8f91 Pauli \u901a\u9053\uff08\u4ee5\u53ca\u7eaf\u9519\u8bef\uff09\uff0c\u5e76\u4ee5\u89c2\u5bdf\u5230\u7684\u7efc\u5408\u7ed3\u679c\u7279\u5b9a\u5947\u5076\u6821\u9a8c\u4e3a\u6761\u4ef6\u3002\u8be5\u65b9\u6cd5\u5177\u6709 SPAM \u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u6700\u9002\u5408\u57fa\u4e8e\u6807\u5fd7\u7684\u7efc\u5408\u6d4b\u91cf\u65b9\u6848\u3002\u5b9e\u9a8c\u6570\u636e\u7684\u7ecf\u5178\u5904\u7406\u91c7\u7528\u4e86\u8d1d\u53f6\u65af\u5efa\u6a21\u65b9\u6cd5\u3002", "result": "\u5728 Quantinuum H1-1 \u79bb\u5b50\u9631\u8bbe\u5907\u4e0a\uff0c\u4f7f\u7528\u4e00\u4e2a\u5c0f\u578b\u7ea0\u9519\u7801\u5bf9\u65b0\u534f\u8bae\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u566a\u58f0\u5b9a\u5236\u548c\u7f13\u89e3\u7b56\u7565\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u7528\u4e8e\u5bb9\u9519\u7684\u566a\u58f0\u8bca\u65ad\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u534f\u8bae\u5728 Quantinuum H1-1 \u79bb\u5b50\u9631\u8bbe\u5907\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u4f7f\u7528\u5df2\u4ea4\u6362\u7684\u6d4b\u91cf\u3001\u6cc4\u6f0f\u4fdd\u62a4\u548c Pauli \u5e27\u968f\u673a\u5316\u7b49\u566a\u58f0\u5b9a\u5236\u548c\u7f13\u89e3\u7b56\u7565\uff0c\u53ef\u4ee5\u663e\u8457\u6539\u8fdb\u7528\u4e8e\u5bb9\u9519\u7684\u566a\u58f0\u8bca\u65ad\u6d4b\u8bd5\u3002"}}
{"id": "2508.07479", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07479", "abs": "https://arxiv.org/abs/2508.07479", "authors": ["Blerta Veseli", "Julian Chibane", "Mariya Toneva", "Alexander Koller"], "title": "Positional Biases Shift as Inputs Approach Context Window Limits", "comment": null, "summary": "Large Language Models (LLMs) often struggle to use information across long\ninputs effectively. Prior work has identified positional biases, such as the\nLost in the Middle (LiM) effect, where models perform better when information\nappears at the beginning (primacy bias) or end (recency bias) of the input,\nrather than in the middle. However, long-context studies have not consistently\nreplicated these effects, raising questions about their intensity and the\nconditions under which they manifest. To address this, we conducted a\ncomprehensive analysis using relative rather than absolute input lengths,\ndefined with respect to each model's context window. Our findings reveal that\nthe LiM effect is strongest when inputs occupy up to 50% of a model's context\nwindow. Beyond that, the primacy bias weakens, while recency bias remains\nrelatively stable. This effectively eliminates the LiM effect; instead, we\nobserve a distance-based bias, where model performance is better when relevant\ninformation is closer to the end of the input. Furthermore, our results suggest\nthat successful retrieval is a prerequisite for reasoning in LLMs, and that the\nobserved positional biases in reasoning are largely inherited from retrieval.\nThese insights have implications for long-context tasks, the design of future\nLLM benchmarks, and evaluation methodologies for LLMs handling extended inputs.", "AI": {"tldr": "LLM\u5904\u7406\u957f\u6587\u672c\u65f6\uff0c\u4fe1\u606f\u4f4d\u7f6e\u5bf9\u6a21\u578b\u8868\u73b0\u7684\u5f71\u54cd\u6bd4\u5148\u524d\u8ba4\u4e3a\u7684\u66f4\u590d\u6742\u3002\u2018\u4e2d\u95f4\u8ff7\u5931\u2019\u6548\u5e94\u5e76\u975e\u666e\u904d\u5b58\u5728\uff0c\u800c\u662f\u53d6\u51b3\u4e8e\u8f93\u5165\u76f8\u5bf9\u4e8e\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u7684\u6bd4\u4f8b\u3002\u5f53\u8f93\u5165\u8d85\u8fc7\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u768450%\u65f6\uff0c\u6a21\u578b\u66f4\u503e\u5411\u4e8e\u5173\u6ce8\u9760\u8fd1\u672b\u5c3e\u7684\u4fe1\u606f\uff0c\u800c\u975e\u8f93\u5165\u5f00\u5934\u6216\u4e2d\u95f4\u7684\u4fe1\u606f\u3002\u6b64\u5916\uff0c\u6a21\u578b\u7406\u89e3\u548c\u63a8\u7406\u4fe1\u606f\u7684\u80fd\u529b\u5f88\u5927\u7a0b\u5ea6\u4e0a\u53d6\u51b3\u4e8e\u5b83\u80fd\u5426\u6210\u529f\u5730\u68c0\u7d22\u5230\u76f8\u5173\u4fe1\u606f\u3002", "motivation": "\u63a2\u7a76\u957f\u4e0a\u4e0b\u6587\u7814\u7a76\u4e2d\u2018\u4e2d\u95f4\u8ff7\u5931\u2019\u6548\u5e94\u4e0d\u4e00\u81f4\u7684\u73b0\u8c61\uff0c\u4ee5\u53ca\u8be5\u6548\u5e94\u7684\u5f3a\u5ea6\u548c\u53d1\u751f\u6761\u4ef6\u3002", "method": "\u901a\u8fc7\u4f7f\u7528\u76f8\u5bf9\u800c\u975e\u7edd\u5bf9\u8f93\u5165\u957f\u5ea6\uff08\u76f8\u5bf9\u4e8e\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u7a97\u53e3\uff09\u8fdb\u884c\u5168\u9762\u5206\u6790\uff0c\u7814\u7a76\u4e86\u2018\u4e2d\u95f4\u8ff7\u5931\u2019\u6548\u5e94\u5728\u4e0d\u540c\u4e0a\u4e0b\u6587\u7a97\u53e3\u5360\u7528\u7387\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u2018\u4e2d\u95f4\u8ff7\u5931\u2019\u6548\u5e94\u5728\u8f93\u5165\u5360\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u9ad8\u8fbe50%\u65f6\u6700\u4e3a\u5f3a\u70c8\u3002\u5f53\u8f93\u5165\u8d85\u8fc7\u8be5\u6bd4\u4f8b\u65f6\uff0c\u9996\u4f4d\u504f\u597d\u51cf\u5f31\uff0c\u672b\u4f4d\u504f\u597d\u4fdd\u6301\u76f8\u5bf9\u7a33\u5b9a\uff0c\u2018\u4e2d\u95f4\u8ff7\u5931\u2019\u6548\u5e94\u88ab\u8ddd\u79bb\u504f\u597d\u53d6\u4ee3\uff0c\u5373\u4fe1\u606f\u8d8a\u9760\u8fd1\u8f93\u5165\u672b\u5c3e\uff0c\u6a21\u578b\u8868\u73b0\u8d8a\u597d\u3002\u68c0\u7d22\u6210\u529f\u662fLLM\u63a8\u7406\u7684\u57fa\u7840\uff0c\u4f4d\u7f6e\u504f\u89c1\u4e3b\u8981\u6765\u81ea\u68c0\u7d22\u3002", "conclusion": "LLM\u5728\u5904\u7406\u957f\u8f93\u5165\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u4e0e\u4fe1\u606f\u4f4d\u7f6e\u7684\u5173\u7cfb\u5e76\u975e\u7b80\u5355\u7684\u9996\u4f4d\u504f\u597d\uff0c\u800c\u662f\u5b58\u5728\u8ddd\u79bb\u504f\u597d\u3002\u5f53\u8f93\u5165\u5360\u6a21\u578b\u4e0a\u4e0b\u6587\u7a97\u53e3\u768450%\u65f6\uff0c\u2018\u4e2d\u95f4\u8ff7\u5931\u2019\u6548\u5e94\u6700\u4e3a\u663e\u8457\u3002\u8d85\u8fc7\u8fd9\u4e2a\u9608\u503c\uff0c\u9996\u4f4d\u504f\u597d\u51cf\u5f31\uff0c\u672b\u4f4d\u504f\u597d\u4fdd\u6301\u7a33\u5b9a\uff0c\u2018\u4e2d\u95f4\u8ff7\u5931\u2019\u6548\u5e94\u6d88\u5931\uff0c\u8f6c\u800c\u51fa\u73b0\u8ddd\u79bb\u504f\u597d\uff0c\u5373\u4fe1\u606f\u8d8a\u9760\u8fd1\u8f93\u5165\u672b\u5c3e\uff0c\u6a21\u578b\u8868\u73b0\u8d8a\u597d\u3002\u6b64\u5916\uff0c\u68c0\u7d22\u7684\u6210\u529f\u662fLLM\u8fdb\u884c\u63a8\u7406\u7684\u524d\u63d0\uff0c\u89c2\u5bdf\u5230\u7684\u4f4d\u7f6e\u504f\u89c1\u5f88\u5927\u7a0b\u5ea6\u4e0a\u6e90\u4e8e\u68c0\u7d22\u4e2d\u7684\u4f4d\u7f6e\u504f\u89c1\u3002"}}
{"id": "2508.06905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06905", "abs": "https://arxiv.org/abs/2508.06905", "authors": ["Ruoxi Chen", "Dongping Chen", "Siyuan Wu", "Sinan Wang", "Shiyun Lang", "Petr Sushko", "Gaoyang Jiang", "Yao Wan", "Ranjay Krishna"], "title": "MultiRef: Controllable Image Generation with Multiple Visual References", "comment": "Accepted to ACM MM 2025 Datasets", "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86MultiRef-bench\u8bc4\u4f30\u6846\u67b6\u548cMultiRef\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u89e3\u51b3\u591a\u89c6\u89c9\u53c2\u8003\u56fe\u50cf\u751f\u6210\u95ee\u9898\u3002\u5b9e\u9a8c\u53d1\u73b0\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\u65f6\u8868\u73b0\u4e0d\u4f73\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u65b9\u5411\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u56fe\u50cf\u751f\u6210\u6846\u67b6\u4e3b\u8981\u4f9d\u8d56\u5355\u4e00\u8f93\u5165\uff08\u6587\u672c\u6216\u5355\u4e2a\u53c2\u8003\u56fe\u50cf\uff09\u7684\u5c40\u9650\u6027\uff0c\u63a2\u7d22\u53ef\u63a7\u7684\u3001\u5229\u7528\u591a\u4e2a\u89c6\u89c9\u53c2\u8003\u7684\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRefBlend\u7684\u6570\u636e\u5f15\u64ce\uff0c\u7528\u4e8e\u751f\u6210\u5305\u542b\u591a\u79cd\u53c2\u8003\u7c7b\u578b\u7684\u5408\u6210\u6837\u672c\u3002\u6784\u5efa\u4e86\u5305\u542b38k\u9ad8\u8d28\u91cf\u56fe\u50cf\u7684MultiRef\u6570\u636e\u96c6\u3002\u5728Interleaved Image-Text\u6a21\u578b\uff08OmniGen, ACE, Show-o\uff09\u548cagentic\u6846\u67b6\uff08ChatDiT, LLM + SD\uff09\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\u8bc4\u4f30\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u662f\u5148\u8fdb\u7684\u6a21\u578b\uff0c\u5728\u5904\u7406\u591a\u89c6\u89c9\u53c2\u8003\u6761\u4ef6\u65f6\u4e5f\u5b58\u5728\u56f0\u96be\u3002\u6700\u4f73\u6a21\u578bOmniGen\u5728\u5408\u6210\u6837\u672c\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a66.6%\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u6837\u672c\u4e0a\u7684\u5e73\u5747\u51c6\u786e\u7387\u4e3a79.0%\uff0c\u4e0e\u7406\u60f3\u7b54\u6848\u76f8\u6bd4\u4ecd\u6709\u5dee\u8ddd\u3002\u8fd9\u63ed\u793a\u4e86\u5728\u5f00\u53d1\u80fd\u6709\u6548\u6574\u5408\u591a\u89c6\u89c9\u7075\u611f\u6e90\u7684\u521b\u610f\u5de5\u5177\u65b9\u9762\u7684\u6311\u6218\u548c\u673a\u9047\u3002", "conclusion": "\u73b0\u6709\u6a21\u578b\u96be\u4ee5\u5904\u7406\u591a\u89c6\u89c9\u53c2\u8003\u7684\u56fe\u50cf\u751f\u6210\u4efb\u52a1\uff0c\u4f46\u672c\u7814\u7a76\u63d0\u51fa\u7684MultiRef-bench\u548cMultiRef\u6570\u636e\u96c6\u4e3a\u8be5\u9886\u57df\u7684\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u57fa\u7840\u548c\u65b9\u5411\uff0c\u65e8\u5728\u5f00\u53d1\u66f4\u7075\u6d3b\u3001\u66f4\u50cf\u4eba\u7c7b\u7684\u521b\u610f\u5de5\u5177\u3002"}}
{"id": "2508.07673", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07673", "abs": "https://arxiv.org/abs/2508.07673", "authors": ["Gianluca Bontempi"], "title": "Ethics2vec: aligning automatic agents and human preferences", "comment": null, "summary": "Though intelligent agents are supposed to improve human experience (or make\nit more efficient), it is hard from a human perspective to grasp the ethical\nvalues which are explicitly or implicitly embedded in an agent behaviour. This\nis the well-known problem of alignment, which refers to the challenge of\ndesigning AI systems that align with human values, goals and preferences. This\nproblem is particularly challenging since most human ethical considerations\nrefer to \\emph{incommensurable} (i.e. non-measurable and/or incomparable)\nvalues and criteria. Consider, for instance, a medical agent prescribing a\ntreatment to a cancerous patient. How could it take into account (and/or weigh)\nincommensurable aspects like the value of a human life and the cost of the\ntreatment? Now, the alignment between human and artificial values is possible\nonly if we define a common space where a metric can be defined and used. This\npaper proposes to extend to ethics the conventional Anything2vec approach,\nwhich has been successful in plenty of similar and hard-to-quantify domains\n(ranging from natural language processing to recommendation systems and graph\nanalysis). This paper proposes a way to map an automatic agent decision-making\n(or control law) strategy to a multivariate vector representation, which can be\nused to compare and assess the alignment with human values. The Ethics2Vec\nmethod is first introduced in the case of an automatic agent performing binary\ndecision-making. Then, a vectorisation of an automatic control law (like in the\ncase of a self-driving car) is discussed to show how the approach can be\nextended to automatic control settings.", "AI": {"tldr": "\nThe paper introduces Ethics2vec, a method based on Anything2vec, to represent AI decision-making strategies as vectors. This allows for the measurement and comparison of AI alignment with human values, addressing the challenge of incommensurable ethical criteria. The method is shown to be applicable to both binary decisions and control systems like self-driving cars.\n", "motivation": "\nThe motivation of this paper is to address the challenge of aligning AI systems with human values, a problem exacerbated by the incommensurable nature of many human ethical considerations. It is difficult for humans to grasp the ethical values embedded in AI behavior, and the paper aims to provide a method for quantifying and comparing these values to ensure AI alignment.\n", "method": "\nThis paper proposes extending the Anything2vec approach to ethics, introducing the Ethics2vec method. This method maps agent decision-making or control law strategies to a multivariate vector representation. The paper first details this approach for agents performing binary decision-making, and then discusses its extension to automatic control settings, such as self-driving cars.\n", "result": "\nThe paper proposes the Ethics2vec method as a way to create a common space with a defined metric for aligning human and artificial values. This method generates a multivariate vector representation of agent decision-making strategies, allowing for comparison and assessment of alignment. The effectiveness of this approach is demonstrated through its application to binary decision-making and its extension to automatic control.\n", "conclusion": "\nWhile the alignment between human and artificial values is challenging due to incommensurable values, this paper proposes the Ethics2vec method to map agent decision-making strategies to a multivariate vector representation, enabling comparison and assessment of alignment with human values. The method is introduced for binary decision-making and extended to automatic control settings.\n"}}
{"id": "2508.07106", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07106", "abs": "https://arxiv.org/abs/2508.07106", "authors": ["Yiran Huang", "Amirhossein Nouranizadeh", "Christine Ahrends", "Mengjia Xu"], "title": "BrainATCL: Adaptive Temporal Brain Connectivity Learning for Functional Link Prediction and Age Estimation", "comment": null, "summary": "Functional Magnetic Resonance Imaging (fMRI) is an imaging technique widely\nused to study human brain activity. fMRI signals in areas across the brain\ntransiently synchronise and desynchronise their activity in a highly structured\nmanner, even when an individual is at rest. These functional connectivity\ndynamics may be related to behaviour and neuropsychiatric disease. To model\nthese dynamics, temporal brain connectivity representations are essential, as\nthey reflect evolving interactions between brain regions and provide insight\ninto transient neural states and network reconfigurations. However,\nconventional graph neural networks (GNNs) often struggle to capture long-range\ntemporal dependencies in dynamic fMRI data. To address this challenge, we\npropose BrainATCL, an unsupervised, nonparametric framework for adaptive\ntemporal brain connectivity learning, enabling functional link prediction and\nage estimation. Our method dynamically adjusts the lookback window for each\nsnapshot based on the rate of newly added edges. Graph sequences are\nsubsequently encoded using a GINE-Mamba2 backbone to learn spatial-temporal\nrepresentations of dynamic functional connectivity in resting-state fMRI data\nof 1,000 participants from the Human Connectome Project. To further improve\nspatial modeling, we incorporate brain structure and function-informed edge\nattributes, i.e., the left/right hemispheric identity and subnetwork membership\nof brain regions, enabling the model to capture biologically meaningful\ntopological patterns. We evaluate our BrainATCL on two tasks: functional link\nprediction and age estimation. The experimental results demonstrate superior\nperformance and strong generalization, including in cross-session prediction\nscenarios.", "AI": {"tldr": "BrainATCL\u662f\u4e00\u79cd\u7528\u4e8e\u5b66\u4e60\u52a8\u6001fMRI\u6570\u636e\u8fde\u63a5\u7684\u65b0\u578b\u6846\u67b6\uff0c\u80fd\u591f\u6709\u6548\u5904\u7406\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u5e76\u5728\u9884\u6d4b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4f20\u7edf\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u5728\u6355\u83b7\u52a8\u6001fMRI\u6570\u636e\u4e2d\u7684\u957f\u7a0b\u65f6\u95f4\u4f9d\u8d56\u6027\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u63d0\u51faBrainATCL\u6846\u67b6\u3002", "method": "BrainATCL\u662f\u4e00\u4e2a\u65e0\u76d1\u7763\u3001\u975e\u53c2\u6570\u7684\u81ea\u9002\u5e94\u65f6\u95f4\u5927\u8111\u8fde\u63a5\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u67e5\u627e\u7a97\u53e3\u5e76\u7ed3\u5408GINE-Mamba2\u9aa8\u5e72\u7f51\u7edc\u6765\u7f16\u7801\u56fe\u5e8f\u5217\uff0c\u540c\u65f6\u878d\u5165\u5927\u8111\u7ed3\u6784\u548c\u529f\u80fd\u4fe1\u606f\u4f5c\u4e3a\u8fb9\u5c5e\u6027\uff0c\u4ee5\u5b66\u4e60\u52a8\u6001\u529f\u80fd\u8fde\u63a5\u7684\u65f6\u7a7a\u8868\u793a\u3002", "result": "BrainATCL\u5728\u529f\u80fd\u94fe\u63a5\u9884\u6d4b\u548c\u5e74\u9f84\u4f30\u8ba1\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4f18\u5f02\u5b9e\u9a8c\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "BrainATCL\u5728\u529f\u80fd\u94fe\u63a5\u9884\u6d4b\u548c\u5e74\u9f84\u4f30\u8ba1\u4efb\u52a1\u4e0a\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5305\u62ec\u8de8\u4f1a\u8bdd\u9884\u6d4b\u3002"}}
{"id": "2508.07945", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07945", "abs": "https://arxiv.org/abs/2508.07945", "authors": ["En Yen Puang", "Federico Ceola", "Giulia Pasquale", "Lorenzo Natale"], "title": "PCHands: PCA-based Hand Pose Synergy Representation on Manipulators with N-DoF", "comment": "2025 IEEE-RAS 24th International Conference on Humanoid Robots", "summary": "We consider the problem of learning a common representation for dexterous\nmanipulation across manipulators of different morphologies. To this end, we\npropose PCHands, a novel approach for extracting hand postural synergies from a\nlarge set of manipulators. We define a simplified and unified description\nformat based on anchor positions for manipulators ranging from 2-finger\ngrippers to 5-finger anthropomorphic hands. This enables learning a\nvariable-length latent representation of the manipulator configuration and the\nalignment of the end-effector frame of all manipulators. We show that it is\npossible to extract principal components from this latent representation that\nis universal across manipulators of different structures and degrees of\nfreedom. To evaluate PCHands, we use this compact representation to encode\nobservation and action spaces of control policies for dexterous manipulation\ntasks learned with RL. In terms of learning efficiency and consistency, the\nproposed representation outperforms a baseline that learns the same tasks in\njoint space. We additionally show that PCHands performs robustly in RL from\ndemonstration, when demonstrations are provided from a different manipulator.\nWe further support our results with real-world experiments that involve a\n2-finger gripper and a 4-finger anthropomorphic hand. Code and additional\nmaterial are available at https://hsp-iit.github.io/PCHands/.", "AI": {"tldr": "PCHands\u662f\u4e00\u79cd\u7528\u4e8e\u7075\u5de7\u64cd\u63a7\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u64cd\u7eb5\u5668\u59ff\u52bf\u7684\u901a\u7528\u8868\u5f81\uff0c\u63d0\u9ad8\u4e86\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u7684\u6548\u7387\u548c\u9c81\u68d2\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u4e0d\u540c\u5f62\u6001\u64cd\u7eb5\u5668\u4e4b\u95f4\u5b66\u4e60\u901a\u7528\u8868\u5f81\u4ee5\u5b9e\u73b0\u7075\u5de7\u64cd\u63a7\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPCHands\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u57fa\u4e8e\u951a\u70b9\u4f4d\u7f6e\u7684\u7b80\u5316\u7edf\u4e00\u63cf\u8ff0\u683c\u5f0f\uff0c\u4e3a\u4e0d\u540c\u5f62\u6001\u7684\u64cd\u7eb5\u5668\uff08\u4ece\u4e24\u6307\u5939\u722a\u5230\u4e94\u6307\u62df\u4eba\u624b\uff09\u63d0\u53d6\u624b\u90e8\u59ff\u52bf\u534f\u540c\uff0c\u5e76\u5b66\u4e60\u53d8\u91cf\u957f\u5ea6\u7684\u6f5c\u5728\u8868\u5f81\u3002", "result": "PCHands\u80fd\u591f\u63d0\u53d6\u51fa\u5728\u4e0d\u540c\u7ed3\u6784\u548c\u81ea\u7531\u5ea6\u7684\u64cd\u7eb5\u5668\u4e4b\u95f4\u901a\u7528\u7684\u4e3b\u6210\u5206\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u5f3a\u5316\u5b66\u4e60\u548c\u6a21\u4eff\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u8868\u73b0\u51fa\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u7684\u5b66\u4e60\u6548\u7387\u3001\u4e00\u81f4\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "PCHands\u65b9\u6cd5\u5728\u4e0d\u540c\u5f62\u6001\u7684\u64cd\u7eb5\u5668\u4e4b\u95f4\u5b66\u4e60\u901a\u7528\u8868\u5f81\uff0c\u5e76\u5728\u5b66\u4e60\u6548\u7387\u3001\u4e00\u81f4\u6027\u548c\u8de8\u64cd\u7eb5\u5668\u9c81\u68d2\u6027\u65b9\u9762\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002"}}
{"id": "2508.08191", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08191", "abs": "https://arxiv.org/abs/2508.08191", "authors": ["Abraham Jacob", "Campbell McLauchlan", "Dan E. Browne"], "title": "Single-Shot Decoding and Fault-tolerant Gates with Trivariate Tricycle Codes", "comment": "26 pages, 7 figures", "summary": "While quantum low-density parity check (qLDPC) codes are a low-overhead means\nof quantum information storage, it is valuable for quantum codes to possess\nfault-tolerant features beyond this resource efficiency. In this work, we\nintroduce trivariate tricycle (TT) codes, qLDPC codes that combine several\ndesirable features: high thresholds under a circuit-level noise model, partial\nsingle-shot decodability for low-time-overhead decoding, a large set of\ntransversal Clifford gates and automorphisms within and between code blocks,\nand (for several sub-constructions) constant-depth implementations of a\n(non-Clifford) $CCZ$ gate. TT codes are CSS codes based on a length-3 chain\ncomplex, and are defined from three trivariate polynomials, with the 3D toric\ncode (3DTC) belonging to this construction. We numerically search for TT codes\nand find several candidates with improved parameters relative to the 3DTC,\nusing up to 48$\\times$ fewer data qubits as equivalent 3DTC encodings. We\nconstruct syndrome-extraction circuits for these codes and numerically\ndemonstrate single-shot decoding in the X error channel in both\nphenomenological and circuit-level noise models. Under circuit-level noise, TT\ncodes have a threshold of $0.3\\%$ in the Z error channel and $1\\%$ in the X\nerror channel (with single-shot decoding). All TT codes possess several\ntransversal $CZ$ gates that can partially address logical qubits between two\ncode blocks. Additionally, the codes possess a large set of automorphisms that\ncan perform Clifford gates within a code block. Finally, we establish several\nTT code polynomial constructions that allows for a constant-depth\nimplementation of logical $CCZ$ gates. We find examples of error-correcting and\nerror-detecting codes using these constructions whose parameters out-perform\nthose of the 3DTC, using up to $4\\times$ fewer data qubits for\nequivalent-distance 3DTC encodings.", "AI": {"tldr": "TT codes are new quantum error-correcting codes that are resource-efficient and fault-tolerant, outperforming the 3D toric code in several aspects like qubit usage and error correction thresholds.", "motivation": "The motivation for this research is to develop quantum error-correcting codes that are not only resource-efficient (low-overhead) but also possess fault-tolerant features beyond mere resource efficiency. Specifically, the authors aim to create quantum codes that offer high thresholds under circuit-level noise, enable efficient decoding through partial single-shot decodability, and support a rich set of transversal gates and automorphisms for enhanced quantum computation capabilities.", "method": "This paper introduces trivariate tricycle (TT) codes, a type of quantum low-density parity check (qLDPC) code. The codes are constructed using three trivariate polynomials and are based on a length-3 chain complex, with the 3D toric code being a specific instance. The research involves numerically searching for TT codes with improved parameters and constructing syndrome-extraction circuits to demonstrate their performance. The analysis includes evaluating thresholds under phenomenological and circuit-level noise models, as well as investigating the properties of transversal gates and automorphisms.", "result": "The paper presents TT codes, which demonstrate high thresholds (0.3% in Z, 1% in X error channels under circuit-level noise with single-shot decoding) and partial single-shot decodability. Several TT codes with improved parameters compared to the 3D toric code were found through numerical search, requiring significantly fewer data qubits. The codes possess transversal CZ gates for inter-block operations and numerous automorphisms for intra-block Clifford gates. Additionally, specific polynomial constructions allow for constant-depth CCZ gate implementations, with examples outperforming the 3D toric code in terms of data qubit usage for equivalent distances.", "conclusion": "TT codes are a class of qLDPC codes based on trivariate polynomials and chain complexes, offering fault tolerance, partial single-shot decodability, transversal Clifford gates, and constant-depth CCZ gate implementations. Numerical searches have identified TT codes with improved parameters compared to the 3D toric code, requiring fewer data qubits for equivalent encoding distances. These codes demonstrate promising performance under circuit-level noise models, with reported thresholds of 0.3% in the Z error channel and 1% in the X error channel."}}
{"id": "2508.07484", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07484", "abs": "https://arxiv.org/abs/2508.07484", "authors": ["Archchana Sindhujan", "Shenbin Qian", "Chan Chi Chun Matthew", "Constantin Orasan", "Diptesh Kanojia"], "title": "ALOPE: Adaptive Layer Optimization for Translation Quality Estimation using Large Language Models", "comment": "Accepted to COLM 2025 Conference", "summary": "Large Language Models (LLMs) have shown remarkable performance across a wide\nrange of natural language processing tasks. Quality Estimation (QE) for Machine\nTranslation (MT), which assesses the quality of a source-target pair without\nrelying on reference translations, remains a challenging cross-lingual task for\nLLMs. The challenges stem from the inherent limitations of existing LLM-based\nQE systems, which are pre-trained for causal language modelling rather than\nregression-specific tasks, further elevated by the presence of low-resource\nlanguages given pre-training data distribution. This paper introduces ALOPE, an\nadaptive layer-optimization framework designed to enhance LLM-based QE by\nrestructuring Transformer representations through layer-wise adaptation for\nimproved regression-based prediction. Our framework integrates low-rank\nadapters (LoRA) with regression task heads, leveraging selected pre-trained\nTransformer layers for improved cross-lingual alignment. In addition to the\nlayer-specific adaptation, ALOPE introduces two strategies-dynamic weighting,\nwhich adaptively combines representations from multiple layers, and multi-head\nregression, which aggregates regression losses from multiple heads for QE. Our\nframework shows improvements over various existing LLM-based QE approaches.\nEmpirical evidence suggests that intermediate Transformer layers in LLMs\nprovide contextual representations that are more aligned with the cross-lingual\nnature of the QE task. We make resultant models and framework code publicly\navailable for further research, also allowing existing LLM-based MT frameworks\nto be scaled with QE capabilities.", "AI": {"tldr": "ALOPE\u662f\u4e00\u79cd\u7528\u4e8e\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7684\u81ea\u9002\u5e94\u5c42\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7LoRA\u548c\u56de\u5f52\u4efb\u52a1\u5934\u6539\u8fdbLLM\u7684\u8868\u793a\uff0c\u63d0\u5347\u4e86\u8de8\u8bed\u8a00\u5bf9\u9f50\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u7cfb\u7edf\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u4e3b\u8981\u4e3a\u56e0\u679c\u8bed\u8a00\u5efa\u6a21\u800c\u975e\u56de\u5f52\u4efb\u52a1\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u573a\u666f\u4e0b\u95ee\u9898\u66f4\u4e3a\u4e25\u91cd\u3002", "method": "ALOPE\u6846\u67b6\u96c6\u6210\u4f4e\u79e9\u9002\u914d\uff08LoRA\uff09\u548c\u56de\u5f52\u4efb\u52a1\u5934\uff0c\u5229\u7528\u9884\u8bad\u7ec3Transformer\u5c42\u8fdb\u884c\u5c42\u7ea7\u81ea\u9002\u5e94\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u548c\u591a\u5934\u56de\u5f52\u7b56\u7565\u8fdb\u4e00\u6b65\u4f18\u5316\u3002", "result": "ALOPE\u6846\u67b6\u5728\u591a\u4e2a\u73b0\u6709LLM\u9a71\u52a8\u7684\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u65b9\u6cd5\u4e0a\u5747\u53d6\u5f97\u4e86\u6539\u8fdb\uff0c\u5b9e\u9a8c\u8868\u660eLLM\u7684\u4e2d\u95f4Transformer\u5c42\u63d0\u4f9b\u4e86\u66f4\u7b26\u5408\u8d28\u91cf\u8bc4\u4f30\u4efb\u52a1\u8de8\u8bed\u8a00\u7279\u6027\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u3002", "conclusion": "ALOPE\u6846\u67b6\u901a\u8fc7\u5c42\u7ea7\u81ea\u9002\u5e94\u91cd\u6784Transformer\u8868\u793a\uff0c\u5728\u673a\u5668\u7ffb\u8bd1\u8d28\u91cf\u8bc4\u4f30\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u548c\u8de8\u8bed\u8a00\u5bf9\u9f50\u65b9\u9762\u3002"}}
{"id": "2508.06908", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06908", "abs": "https://arxiv.org/abs/2508.06908", "authors": ["Jinhao Li", "Zijian Chen", "Lirong Deng", "Changbo Wang", "Guangtao Zhai"], "title": "MMReID-Bench: Unleashing the Power of MLLMs for Effective and Versatile Person Re-identification", "comment": null, "summary": "Person re-identification (ReID) aims to retrieve the images of an interested\nperson in the gallery images, with wide applications in medical rehabilitation,\nabnormal behavior detection, and public security. However, traditional person\nReID models suffer from uni-modal capability, leading to poor generalization\nability in multi-modal data, such as RGB, thermal, infrared, sketch images,\ntextual descriptions, etc. Recently, the emergence of multi-modal large\nlanguage models (MLLMs) shows a promising avenue for addressing this problem.\nDespite this potential, existing methods merely regard MLLMs as feature\nextractors or caption generators, which do not fully unleash their reasoning,\ninstruction-following, and cross-modal understanding capabilities. To bridge\nthis gap, we introduce MMReID-Bench, the first multi-task multi-modal benchmark\nspecifically designed for person ReID. The MMReID-Bench includes 20,710\nmulti-modal queries and gallery images covering 10 different person ReID tasks.\nComprehensive experiments demonstrate the remarkable capabilities of MLLMs in\ndelivering effective and versatile person ReID. Nevertheless, they also have\nlimitations in handling a few modalities, particularly thermal and infrared\ndata. We hope MMReID-Bench can facilitate the community to develop more robust\nand generalizable multimodal foundation models for person ReID.", "AI": {"tldr": "MMReID-Bench\u662f\u4e00\u4e2a\u591a\u6a21\u6001\u884c\u4ebaReID\u57fa\u51c6\uff0c\u5c55\u793a\u4e86MLLMs\u7684\u6f5c\u529b\u548c\u5c40\u9650\u6027\uff0c\u65e8\u5728\u63a8\u52a8\u66f4\u5f3a\u5927\u7684\u591a\u6a21\u6001ReID\u6a21\u578b\u53d1\u5c55\u3002", "motivation": "\u4f20\u7edf\u884c\u4ebaReID\u6a21\u578b\u5728\u591a\u6a21\u6001\u6570\u636e\uff08\u5982RGB\u3001\u70ed\u6210\u50cf\u3001\u7ea2\u5916\u3001\u7d20\u63cf\u3001\u6587\u672c\u63cf\u8ff0\u7b49\uff09\u4e0a\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u867d\u7136\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5c55\u73b0\u4e86\u6f5c\u529b\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u672a\u80fd\u5145\u5206\u53d1\u6325\u5176\u63a8\u7406\u3001\u6307\u4ee4\u9075\u5faa\u548c\u8de8\u6a21\u6001\u7406\u89e3\u80fd\u529b\u3002", "method": "MMReID-Bench\u662f\u4e00\u4e2a\u9488\u5bf9\u884c\u4eba\u91cd\u8bc6\u522b\uff08ReID\uff09\u7684\u591a\u4efb\u52a1\u591a\u6a21\u6001\u57fa\u51c6\uff0c\u5305\u542b20,710\u4e2a\u591a\u6a21\u6001\u67e5\u8be2\u548c\u56fe\u5e93\u56fe\u50cf\uff0c\u8986\u76d610\u4e2aReID\u4efb\u52a1\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u4fc3\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728ReID\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86MLLMs\u5728\u884c\u4ebaReID\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u80fd\u529b\uff0c\u80fd\u591f\u5b9e\u73b0\u6709\u6548\u4e14\u901a\u7528\u7684ReID\u3002\u7136\u800c\uff0c\u5728\u5904\u7406\u70ed\u6210\u50cf\u548c\u7ea2\u5916\u6570\u636e\u7b49\u5c11\u6570\u6a21\u6001\u65f6\uff0cMLLMs\u4ecd\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "MMReID-Bench\u7684\u51fa\u73b0\u65e8\u5728\u5f25\u5408\u73b0\u6709\u65b9\u6cd5\u5728\u5229\u7528\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u8fdb\u884c\u884c\u4eba\u91cd\u8bc6\u522b\uff08ReID\uff09\u65b9\u9762\u7684\u5dee\u8ddd\uff0c\u8be5\u57fa\u51c6\u5305\u542b\u4e8620,710\u4e2a\u591a\u6a21\u6001\u67e5\u8be2\u548c\u56fe\u5e93\u56fe\u50cf\uff0c\u6db5\u76d610\u4e2a\u4e0d\u540c\u7684ReID\u4efb\u52a1\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86MLLMs\u5728\u884c\u4ebaReID\u65b9\u9762\u7684\u5f3a\u5927\u80fd\u529b\uff0c\u4f46\u4e5f\u63ed\u793a\u4e86\u5b83\u4eec\u5728\u5904\u7406\u70ed\u6210\u50cf\u548c\u7ea2\u5916\u6570\u636e\u7b49\u7279\u5b9a\u6a21\u6001\u65f6\u5b58\u5728\u7684\u5c40\u9650\u6027\u3002\u7814\u7a76\u8005\u5e0c\u671bMMReID-Bench\u80fd\u63a8\u52a8\u793e\u533a\u5f00\u53d1\u66f4\u9c81\u68d2\u3001\u66f4\u5177\u6cdb\u5316\u6027\u7684\u591a\u6a21\u6001\u57fa\u7840\u6a21\u578b\u7528\u4e8e\u884c\u4ebaReID\u3002"}}
{"id": "2508.07743", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07743", "abs": "https://arxiv.org/abs/2508.07743", "authors": ["Markus Fritzsche", "Elliot Gestrin", "Jendrik Seipp"], "title": "Symmetry-Aware Transformer Training for Automated Planning", "comment": null, "summary": "While transformers excel in many settings, their application in the field of\nautomated planning is limited. Prior work like PlanGPT, a state-of-the-art\ndecoder-only transformer, struggles with extrapolation from easy to hard\nplanning problems. This in turn stems from problem symmetries: planning tasks\ncan be represented with arbitrary variable names that carry no meaning beyond\nbeing identifiers. This causes a combinatorial explosion of equivalent\nrepresentations that pure transformers cannot efficiently learn from. We\npropose a novel contrastive learning objective to make transformers\nsymmetry-aware and thereby compensate for their lack of inductive bias.\nCombining this with architectural improvements, we show that transformers can\nbe efficiently trained for either plan-generation or heuristic-prediction. Our\nresults across multiple planning domains demonstrate that our symmetry-aware\ntraining effectively and efficiently addresses the limitations of PlanGPT.", "AI": {"tldr": "Transformer \u5728\u81ea\u52a8\u5316\u89c4\u5212\u4e2d\u5b58\u5728\u95ee\u9898\u5bf9\u79f0\u6027\u5bfc\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u5bf9\u79f0\u611f\u77e5\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u6539\u8fdb\u4e86 Transformer\uff0c\u4f7f\u5176\u80fd\u591f\u9ad8\u6548\u5730\u7528\u4e8e\u8ba1\u5212\u751f\u6210\u548c\u542f\u53d1\u5f0f\u9884\u6d4b\uff0c\u5e76\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "Transformer \u5728\u81ea\u52a8\u5316\u89c4\u5212\u9886\u57df\u5b58\u5728\u5c40\u9650\u6027\uff0c\u7279\u522b\u662f PlanGPT \u5728\u5904\u7406\u4ece\u6613\u5230\u96be\u7684\u89c4\u5212\u95ee\u9898\u65f6\u5b58\u5728\u5916\u63a8\u80fd\u529b\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fd9\u6e90\u4e8e\u95ee\u9898\u5bf9\u79f0\u6027\uff08\u4efb\u610f\u7684\u53d8\u91cf\u540d\u5bfc\u81f4\u7b49\u4ef7\u8868\u793a\u7684\u7ec4\u5408\u7206\u70b8\uff0c\u7eaf Transformer \u96be\u4ee5\u6709\u6548\u5b66\u4e60\uff09\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\uff0c\u4f7f Transformer \u80fd\u591f\u611f\u77e5\u5bf9\u79f0\u6027\uff0c\u5e76\u7ed3\u5408\u4e86\u67b6\u6784\u6539\u8fdb\u3002", "result": "\u8bc1\u660e\u4e86 Transformer \u53ef\u4ee5\u88ab\u9ad8\u6548\u8bad\u7ec3\u7528\u4e8e\u8ba1\u5212\u751f\u6210\u6216\u542f\u53d1\u5f0f\u9884\u6d4b\uff0c\u5e76\u4e14\u6240\u63d0\u51fa\u7684\u5bf9\u79f0\u611f\u77e5\u8bad\u7ec3\u65b9\u6cd5\u6709\u6548\u4e14\u9ad8\u6548\u5730\u89e3\u51b3\u4e86 PlanGPT \u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u5bf9\u6bd4\u5b66\u4e60\u76ee\u6807\u548c\u67b6\u6784\u6539\u8fdb\uff0c\u4f7f Transformer \u80fd\u591f\u611f\u77e5\u5bf9\u79f0\u6027\uff0c\u4ece\u800c\u6709\u6548\u4e14\u9ad8\u6548\u5730\u514b\u670d\u4e86 PlanGPT \u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u89c4\u5212\u9886\u57df\u53d6\u5f97\u4e86\u6210\u679c\u3002"}}
{"id": "2508.07114", "categories": ["cs.LG", "hep-ex"], "pdf": "https://arxiv.org/pdf/2508.07114", "abs": "https://arxiv.org/abs/2508.07114", "authors": ["Atakan Azakli", "Bernd Stelzer"], "title": "Approaching Maximal Information Extraction in Low-Signal Regimes via Multiple Instance Learning", "comment": null, "summary": "In this work, we propose a new machine learning (ML) methodology to obtain\nmore precise predictions for some parameters of interest in a given hypotheses\ntesting problem. Our proposed method also allows ML models to have more\ndiscriminative power in cases where it is extremely challenging for\nstate-of-the-art classifiers to have any level of accurate predictions. This\nmethod can also allow us to systematically decrease the error from ML models in\ntheir predictions. In this paper, we provide a mathematical motivation why\nMultiple Instance Learning (MIL) would have more predictive power over their\nsingle-instance counterparts. We support our theoretical claims by analyzing\nthe behavior of the MIL models through their scaling behaviors with respect to\nthe number of instances on which the model makes predictions. As a concrete\napplication, we constrain Wilson coefficients of the Standard Model Effective\nField Theory (SMEFT) using kinematic information from subatomic particle\ncollision events at the Large Hadron Collider (LHC). We show that under certain\ncircumstances, it might be possible to extract the theoretical maximum Fisher\nInformation latent in a dataset.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.08046", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08046", "abs": "https://arxiv.org/abs/2508.08046", "authors": ["Fen Liu", "Shenghai Yuan", "Thien-Minh Nguyen", "Wei Meng", "Lihua Xie"], "title": "Aerial Target Encirclement and Interception with Noisy Range Observations", "comment": "The paper has been accepted in Automatica", "summary": "This paper proposes a strategy to encircle and intercept a non-cooperative\naerial point-mass moving target by leveraging noisy range measurements for\nstate estimation. In this approach, the guardians actively ensure the\nobservability of the target by using an anti-synchronization (AS), 3D\n``vibrating string\" trajectory, which enables rapid position and velocity\nestimation based on the Kalman filter. Additionally, a novel anti-target\ncontroller is designed for the guardians to enable adaptive transitions from\nencircling a protected target to encircling, intercepting, and neutralizing a\nhostile target, taking into consideration the input constraints of the\nguardians. Based on the guaranteed uniform observability, the exponentially\nbounded stability of the state estimation error and the convergence of the\nencirclement error are rigorously analyzed. Simulation results and real-world\nUAV experiments are presented to further validate the effectiveness of the\nsystem design.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u53cd\u540c\u6b65\u548c\u53cd\u76ee\u6807\u63a7\u5236\u5668\u5b9e\u73b0\u65e0\u4eba\u673a\u5305\u56f4\u3001\u62e6\u622a\u548c\u4e2d\u548c\u975e\u5408\u4f5c\u7a7a\u4e2d\u79fb\u52a8\u76ee\u6807\u7684\u7b56\u7565\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u63d0\u51fa\u4e00\u79cd\u7b56\u7565\u6765\u5305\u56f4\u548c\u62e6\u622a\u975e\u5408\u4f5c\u7684\u7a7a\u4e2d\u70b9\u8d28\u91cf\u79fb\u52a8\u76ee\u6807\u3002", "method": "\u5229\u7528\u566a\u58f0\u8ddd\u79bb\u6d4b\u91cf\u8fdb\u884c\u72b6\u6001\u4f30\u8ba1\uff0c\u5e76\u91c7\u7528\u53cd\u540c\u6b65\uff08AS\uff09\u30013D\u201c\u632f\u52a8\u5f26\u201d\u8f68\u8ff9\u548c\u53cd\u76ee\u6807\u63a7\u5236\u5668\u3002", "result": "\u8bc1\u660e\u4e86\u72b6\u6001\u4f30\u8ba1\u8bef\u5dee\u7684\u6307\u6570\u6709\u754c\u7a33\u5b9a\u6027\u548c\u5305\u56f4\u8bef\u5dee\u7684\u6536\u655b\u6027\uff0c\u5e76\u901a\u8fc7\u4eff\u771f\u548c\u771f\u5b9e\u65e0\u4eba\u673a\u5b9e\u9a8c\u8fdb\u884c\u4e86\u9a8c\u8bc1\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u53cd\u540c\u6b65\uff08AS\uff09\u30013D\u201c\u632f\u52a8\u5f26\u201d\u8f68\u8ff9\u548c\u53cd\u76ee\u6807\u63a7\u5236\u5668\uff0c\u6210\u529f\u5b9e\u73b0\u4e86\u5bf9\u975e\u5408\u4f5c\u7a7a\u4e2d\u70b9\u8d28\u91cf\u79fb\u52a8\u76ee\u6807\u7684\u5305\u56f4\u3001\u62e6\u622a\u548c\u4e2d\u548c\u3002"}}
{"id": "2508.08200", "categories": ["quant-ph", "q-bio.QM"], "pdf": "https://arxiv.org/pdf/2508.08200", "abs": "https://arxiv.org/abs/2508.08200", "authors": ["Josh Cudby", "James Bonfield", "Chenxi Zhou", "Richard Durbin", "Sergii Strelchuk"], "title": "Pangenome-guided sequence assembly via binary optimisation", "comment": null, "summary": "De novo genome assembly is challenging in highly repetitive regions; however,\nreference-guided assemblers often suffer from bias. We propose a framework for\npangenome-guided sequence assembly, which can resolve short-read data in\ncomplex regions without bias towards a single reference genome. Our method\nframes assembly as a graph traversal optimisation problem, which can be\nimplemented on quantum computers. The pipeline first annotates pangenome graphs\nwith estimated copy numbers for each node, then finds a path on the graph that\nbest explains those copy numbers. On simulated data, our approach significantly\nreduces the number of contigs compared to de novo assemblers. While it\nintroduces a small increase in inaccuracies, such as false joins, our\noptimisation-based methods are competitive with current exhaustive search\ntechniques. They are also designed to scale more efficiently as the problem\nsize grows and will run effectively on future quantum computers.", "AI": {"tldr": "A new pangenome-guided assembly method, solvable by quantum computers, improves contiguity over de novo methods without reference bias, with competitive accuracy and better scalability.", "motivation": "Addressing the challenges of de novo genome assembly in highly repetitive regions and the bias issues of reference-guided assemblers.", "method": "Frames assembly as a graph traversal optimisation problem, implemented on quantum computers. The pipeline annotates pangenome graphs with estimated copy numbers for each node and finds a path that best explains these copy numbers.", "result": "Significantly reduces the number of contigs compared to de novo assemblers on simulated data, with a small increase in inaccuracies like false joins. The optimisation-based methods are competitive with exhaustive search techniques and designed for efficient scaling on quantum computers.", "conclusion": "pangenome-guided sequence assembly framework can resolve short-read data in complex regions without bias towards a single reference genome, showing competitive performance with current exhaustive search techniques and potential for efficient scaling on quantum computers."}}
{"id": "2508.07516", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07516", "abs": "https://arxiv.org/abs/2508.07516", "authors": ["Keshav Varadarajan", "Tananun Songdechakraiwut"], "title": "Augmenting Bias Detection in LLMs Using Topological Data Analysis", "comment": "15 pages, 9 figures, 4 tables", "summary": "Recently, many bias detection methods have been proposed to determine the\nlevel of bias a large language model captures. However, tests to identify which\nparts of a large language model are responsible for bias towards specific\ngroups remain underdeveloped. In this study, we present a method using\ntopological data analysis to identify which heads in GPT-2 contribute to the\nmisrepresentation of identity groups present in the StereoSet dataset. We find\nthat biases for particular categories, such as gender or profession, are\nconcentrated in attention heads that act as hot spots. The metric we propose\ncan also be used to determine which heads capture bias for a specific group\nwithin a bias category, and future work could extend this method to help\nde-bias large language models.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u8bc6\u522bGPT-2\u6a21\u578b\u4e2d\u504f\u89c1\u6765\u6e90\u7684\u65b9\u6cd5\uff0c\u53d1\u73b0\u504f\u89c1\u96c6\u4e2d\u5728\u7279\u5b9a\u7684\u6ce8\u610f\u529b\u5934\u4e0a\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u53ef\u7528\u4e8e\u91cf\u5316\u548c\u5b9a\u4f4d\u504f\u89c1\u7684\u5ea6\u91cf\u6807\u51c6\u3002", "motivation": "\u73b0\u6709\u504f\u89c1\u68c0\u6d4b\u65b9\u6cd5\u5927\u591a\u53ea\u5173\u6ce8\u4e8e\u68c0\u6d4b\u5927\u578b\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u504f\u89c1\u6c34\u5e73\uff0c\u800c\u8bc6\u522b\u6a21\u578b\u4e2d\u5177\u4f53\u54ea\u4e9b\u90e8\u5206\u5bfc\u81f4\u4e86\u5bf9\u7279\u5b9a\u7fa4\u4f53\u7684\u504f\u89c1\u7684\u65b9\u6cd5\u4ecd\u4e0d\u5b8c\u5584\u3002", "method": "\u4f7f\u7528\u62d3\u6251\u6570\u636e\u5206\u6790\u7684\u65b9\u6cd5\u6765\u8bc6\u522bGPT-2\u4e2d\u54ea\u4e9b\u6ce8\u610f\u529b\u5934\u5bfc\u81f4\u4e86StereoSet\u6570\u636e\u96c6\u4e2d\u5b58\u5728\u7684\u8eab\u4efd\u7fa4\u4f53\u7684\u9519\u8bef\u8868\u793a\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u8bf8\u5982\u6027\u522b\u6216\u804c\u4e1a\u7b49\u7279\u5b9a\u7c7b\u522b\u7684\u504f\u89c1\u96c6\u4e2d\u5728\u5145\u5f53\u70ed\u70b9\u7684\u6ce8\u610f\u529b\u5934\u4e0a\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5ea6\u91cf\u6807\u51c6\u53ef\u4ee5\u7528\u4e8e\u786e\u5b9a\u54ea\u4e9b\u6ce8\u610f\u529b\u5934\u6355\u83b7\u4e86\u7279\u5b9a\u7fa4\u4f53\u7684\u504f\u89c1\uff0c\u5e76\u4e14\u672a\u6765\u7684\u5de5\u4f5c\u53ef\u4ee5\u5c06\u6b64\u65b9\u6cd5\u6269\u5c55\u5230\u5e2e\u52a9\u6d88\u9664\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u504f\u89c1\u3002"}}
{"id": "2508.06916", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06916", "abs": "https://arxiv.org/abs/2508.06916", "authors": ["Shichao Ma", "Yunhe Guo", "Jiahao Su", "Qihe Huang", "Zhengyang Zhou", "Yang Wang"], "title": "Talk2Image: A Multi-Agent System for Multi-Turn Image Generation and Editing", "comment": null, "summary": "Text-to-image generation tasks have driven remarkable advances in diverse\nmedia applications, yet most focus on single-turn scenarios and struggle with\niterative, multi-turn creative tasks. Recent dialogue-based systems attempt to\nbridge this gap, but their single-agent, sequential paradigm often causes\nintention drift and incoherent edits. To address these limitations, we present\nTalk2Image, a novel multi-agent system for interactive image generation and\nediting in multi-turn dialogue scenarios. Our approach integrates three key\ncomponents: intention parsing from dialogue history, task decomposition and\ncollaborative execution across specialized agents, and feedback-driven\nrefinement based on a multi-view evaluation mechanism. Talk2Image enables\nstep-by-step alignment with user intention and consistent image editing.\nExperiments demonstrate that Talk2Image outperforms existing baselines in\ncontrollability, coherence, and user satisfaction across iterative image\ngeneration and editing tasks.", "AI": {"tldr": "Talk2Image is a multi-agent system for interactive image generation and editing that addresses intention drift and incoherent edits in multi-turn dialogues by using specialized agents and feedback-view evaluation for refinement.", "motivation": "Most text-to-image generation tasks focus on single-turn scenarios and struggle with iterative, multi-turn creative tasks. Existing dialogue-based systems suffer from intention drift and incoherent edits due to their single-agent, sequential paradigm.", "method": "A multi-agent system integrating intention parsing from dialogue history, task decomposition and collaborative execution across specialized agents, and feedback-driven refinement based on a multi-view evaluation mechanism.", "result": "Experiments demonstrate that Talk2Image outperforms existing baselines in controllability, coherence, and user satisfaction across iterative image generation and editing tasks.", "conclusion": "Talk2Image enables step-by-step alignment with user intention and consistent image editing, outperforming existing baselines in controllability, coherence, and user satisfaction across iterative image generation and editing tasks."}}
{"id": "2508.07117", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07117", "abs": "https://arxiv.org/abs/2508.07117", "authors": ["Peyman Baghershahi", "Gregoire Fournier", "Pranav Nyati", "Sourav Medya"], "title": "From Nodes to Narratives: Explaining Graph Neural Networks with LLMs and Graph Context", "comment": "18 pages, 3 figures, 8 tables", "summary": "Graph Neural Networks (GNNs) have emerged as powerful tools for learning over\nstructured data, including text-attributed graphs, which are common in domains\nsuch as citation networks, social platforms, and knowledge graphs. GNNs are not\ninherently interpretable and thus, many explanation methods have been proposed.\nHowever, existing explanation methods often struggle to generate interpretable,\nfine-grained rationales, especially when node attributes include rich natural\nlanguage. In this work, we introduce LOGIC, a lightweight, post-hoc framework\nthat uses large language models (LLMs) to generate faithful and interpretable\nexplanations for GNN predictions. LOGIC projects GNN node embeddings into the\nLLM embedding space and constructs hybrid prompts that interleave soft prompts\nwith textual inputs from the graph structure. This enables the LLM to reason\nabout GNN internal representations and produce natural language explanations\nalong with concise explanation subgraphs. Our experiments across four\nreal-world TAG datasets demonstrate that LOGIC achieves a favorable trade-off\nbetween fidelity and sparsity, while significantly improving human-centric\nmetrics such as insightfulness. LOGIC sets a new direction for LLM-based\nexplainability in graph learning by aligning GNN internals with human\nreasoning.", "AI": {"tldr": "LOGIC uses LLMs to create faithful and understandable explanations for GNN predictions, overcoming limitations of existing methods by combining GNN embeddings with textual graph data in hybrid prompts, leading to better insightfulness.", "motivation": "GNNs are not inherently interpretable and thus, many explanation methods have been proposed. However, existing explanation methods often struggle to generate interpretable, fine-grained rationales, especially when node attributes include rich natural language.", "method": "LOGIC, a lightweight, post-hoc framework that uses large language models (LLMs) to generate faithful and interpretable explanations for GNN predictions. LOGIC projects GNN node embeddings into the LLM embedding space and constructs hybrid prompts that interleave soft prompts with textual inputs from the graph structure.", "result": "LOGIC achieves a favorable trade-off between fidelity and sparsity, while significantly improving human-centric metrics such as insightfulness.", "conclusion": "LOGIC sets a new direction for LLM-based explainability in graph learning by aligning GNN internals with human reasoning."}}
{"id": "2508.08108", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08108", "abs": "https://arxiv.org/abs/2508.08108", "authors": ["Wei Zhang", "Yinchuan Wang", "Wangtao Lu", "Pengyu Zhang", "Xiang Zhang", "Yue Wang", "Chaoqun Wang"], "title": "Capsizing-Guided Trajectory Optimization for Autonomous Navigation with Rough Terrain", "comment": null, "summary": "It is a challenging task for ground robots to autonomously navigate in harsh\nenvironments due to the presence of non-trivial obstacles and uneven terrain.\nThis requires trajectory planning that balances safety and efficiency. The\nprimary challenge is to generate a feasible trajectory that prevents robot from\ntip-over while ensuring effective navigation. In this paper, we propose a\ncapsizing-aware trajectory planner (CAP) to achieve trajectory planning on the\nuneven terrain. The tip-over stability of the robot on rough terrain is\nanalyzed. Based on the tip-over stability, we define the traversable\norientation, which indicates the safe range of robot orientations. This\norientation is then incorporated into a capsizing-safety constraint for\ntrajectory optimization. We employ a graph-based solver to compute a robust and\nfeasible trajectory while adhering to the capsizing-safety constraint.\nExtensive simulation and real-world experiments validate the effectiveness and\nrobustness of the proposed method. The results demonstrate that CAP outperforms\nexisting state-of-the-art approaches, providing enhanced navigation performance\non uneven terrains.", "AI": {"tldr": "A new trajectory planner (CAP) enhances robot navigation on uneven terrain by considering tip-over stability and incorporating it as a safety constraint, outperforming previous methods in simulations and real-world tests.", "motivation": "Autonomous navigation in harsh environments with non-trivial obstacles and uneven terrain is challenging for ground robots, requiring trajectory planning that balances safety and efficiency. The primary challenge is generating a feasible trajectory that prevents robot tip-over while ensuring effective navigation.", "method": "The paper proposes a capsizing-aware trajectory planner (CAP). It analyzes the tip-over stability of the robot on rough terrain, defines traversable orientation based on this stability, and incorporates this orientation into a capsizing-safety constraint for trajectory optimization. A graph-based solver is employed to compute the trajectory.", "result": "Extensive simulation and real-world experiments validate the effectiveness and robustness of the CAP method. The results demonstrate enhanced navigation performance on uneven terrains compared to existing approaches.", "conclusion": "CAP outperforms existing state-of-the-art approaches, providing enhanced navigation performance on uneven terrains."}}
{"id": "2508.08213", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08213", "abs": "https://arxiv.org/abs/2508.08213", "authors": ["Minh T. P. Nguyen", "Maximilian Rimbach-Russ", "Stefano Bosco"], "title": "Color it, Code it, Cancel it: k-local dynamical decoupling from classical additive codes", "comment": "22 pages, 8 figures, 2 tables, comments are welcome!", "summary": "Dynamical decoupling is a central technique in quantum computing for actively\nsuppressing decoherence and systematic imperfections through sequences of\nsingle-qubit operations. Conventional sequences typically aim to completely\nfreeze system dynamics, often resulting in long protocols whose length scales\nexponentially with system size. In this work, we introduce a general framework\nfor constructing time-optimal, selectively-tailored sequences that remove only\nspecific local interactions. By combining techniques from graph coloring and\nclassical coding theory, our approach enables compact and hardware-tailored\nsequences across diverse qubit platforms, efficiently canceling undesired\nHamiltonian terms while preserving target interactions. This opens up broad\napplications in quantum computing and simulation. At the core of our method is\na mapping between dynamical decoupling sequence design and error-detecting\ncodes, which allows us to leverage powerful coding-theoretic tools to construct\ncustomized sequences. To overcome exponential overheads, we exploit symmetries\nin colored interaction hypergraphs, extending graph-coloring strategies to\narbitrary many-body Hamiltonians. We demonstrate the effectiveness of our\nframework through concrete examples, including compact sequences that suppress\nresidual ZZ and ZZZ interactions in superconducting qubits and Heisenberg\nexchange coupling in spin qubits. We also show how it enables Hamiltonian\nengineering by simulating the anisotropic Kitaev honeycomb model using only\nisotropic Heisenberg interactions.", "AI": {"tldr": "Dynamical decoupling sequences are usually long and inefficient. We developed a new method using graph coloring and coding theory to create short, efficient, and hardware-specific sequences that suppress unwanted interactions and enable Hamiltonian engineering.", "motivation": "Conventional dynamical decoupling sequences often result in long protocols that scale exponentially with system size, as they aim to completely freeze system dynamics. There is a need for more efficient and compact sequences tailored to specific hardware and interactions.", "method": "Our method combines techniques from graph coloring and classical coding theory. It maps dynamical decoupling sequence design to error-detecting codes, allowing the use of coding-theoretic tools to construct customized sequences. To overcome exponential overheads, we exploit symmetries in colored interaction hypergraphs, extending graph-coloring strategies to arbitrary many-body Hamiltonians.", "result": "We demonstrate the effectiveness of our framework through concrete examples, including compact sequences that suppress residual ZZ and ZZZ interactions in superconducting qubits and Heisenberg exchange coupling in spin qubits. We also show how it enables Hamiltonian engineering by simulating the anisotropic Kitaev honeycomb model using only isotropic Heisenberg interactions.", "conclusion": "We introduce a general framework for constructing time-optimal, selectively-tailored dynamical decoupling sequences that remove only specific local interactions. This framework leverages techniques from graph coloring and classical coding theory to create compact, hardware-tailored sequences applicable across diverse qubit platforms. It efficiently cancels undesired Hamiltonian terms while preserving target interactions, opening up broad applications in quantum computing and simulation."}}
{"id": "2508.07517", "categories": ["cs.CL", "cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07517", "abs": "https://arxiv.org/abs/2508.07517", "authors": ["Joseph T. Colonel", "Baihan Lin"], "title": "Word Clouds as Common Voices: LLM-Assisted Visualization of Participant-Weighted Themes in Qualitative Interviews", "comment": null, "summary": "Word clouds are a common way to summarize qualitative interviews, yet\ntraditional frequency-based methods often fail in conversational contexts: they\nsurface filler words, ignore paraphrase, and fragment semantically related\nideas. This limits their usefulness in early-stage analysis, when researchers\nneed fast, interpretable overviews of what participant actually said. We\nintroduce ThemeClouds, an open-source visualization tool that uses large\nlanguage models (LLMs) to generate thematic, participant-weighted word clouds\nfrom dialogue transcripts. The system prompts an LLM to identify concept-level\nthemes across a corpus and then counts how many unique participants mention\neach topic, yielding a visualization grounded in breadth of mention rather than\nraw term frequency. Researchers can customize prompts and visualization\nparameters, providing transparency and control. Using interviews from a user\nstudy comparing five recording-device configurations (31 participants; 155\ntranscripts, Whisper ASR), our approach surfaces more actionable device\nconcerns than frequency clouds and topic-modeling baselines (e.g., LDA,\nBERTopic). We discuss design trade-offs for integrating LLM assistance into\nqualitative workflows, implications for interpretability and researcher agency,\nand opportunities for interactive analyses such as per-condition contrasts\n(``diff clouds'').", "AI": {"tldr": "ThemeClouds\u5229\u7528LLM\u4ece\u5bf9\u8bdd\u8bb0\u5f55\u4e2d\u751f\u6210\u4e3b\u9898\u8bcd\u4e91\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u8bcd\u4e91\u7684\u5c40\u9650\u6027\uff0c\u63d0\u4f9b\u4e86\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u7684\u89c1\u89e3\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u9891\u7387\u7684\u8bcd\u4e91\u65b9\u6cd5\u5728\u5904\u7406\u5b9a\u6027\u8bbf\u8c08\u65f6\u5b58\u5728\u5c40\u9650\u6027\uff0c\u5b83\u4eec\u5bb9\u6613\u5305\u542b\u586b\u5145\u8bcd\u3001\u5ffd\u7565\u91ca\u4e49\uff0c\u5e76\u6253\u6563\u8bed\u4e49\u76f8\u5173\u7684\u60f3\u6cd5\uff0c\u8fd9\u5728\u9700\u8981\u5feb\u901f\u3001\u53ef\u89e3\u91ca\u5730\u4e86\u89e3\u53c2\u4e0e\u8005\u5b9e\u9645\u6240\u8bf4\u5185\u5bb9\u7684\u5206\u6790\u65e9\u671f\u9636\u6bb5\uff0c\u9650\u5236\u4e86\u5176\u6709\u6548\u6027\u3002", "method": "ThemeClouds\u662f\u4e00\u4e2a\u5f00\u6e90\u53ef\u89c6\u5316\u5de5\u5177\uff0c\u5b83\u5229\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u6765\u5904\u7406\u5bf9\u8bdd\u8bb0\u5f55\u3002\u9996\u5148\uff0c\u7cfb\u7edf\u63d0\u793aLLM\u8bc6\u522b\u6574\u4e2a\u8bed\u6599\u5e93\u4e2d\u7684\u6982\u5ff5\u7ea7\u4e3b\u9898\u3002\u968f\u540e\uff0c\u5b83\u8ba1\u7b97\u63d0\u53ca\u6bcf\u4e2a\u4e3b\u9898\u7684\u72ec\u7279\u53c2\u4e0e\u8005\u6570\u91cf\u3002\u6700\u7ec8\u7684\u8bcd\u4e91\u53ef\u89c6\u5316\u57fa\u4e8e\u8fd9\u79cd\u201c\u63d0\u53ca\u5e7f\u5ea6\u201d\u800c\u975e\u539f\u59cb\u8bcd\u9891\u3002\u7814\u7a76\u4eba\u5458\u53ef\u4ee5\u81ea\u5b9a\u4e49\u63d0\u793a\u548c\u53ef\u89c6\u5316\u53c2\u6570\uff0c\u4ee5\u589e\u5f3a\u900f\u660e\u5ea6\u548c\u63a7\u5236\u529b\u3002", "result": "\u4e0e\u4f20\u7edf\u7684\u8bcd\u4e91\u3001LDA\u548cBERTopic\u7b49\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cThemeClouds\u80fd\u66f4\u6709\u6548\u5730\u63ed\u793a\u51fa\u53ef\u64cd\u4f5c\u7684\u8bbe\u5907\u76f8\u5173\u95ee\u9898\u3002", "conclusion": "ThemeClouds\u901a\u8fc7\u5229\u7528LLM\u8bc6\u522b\u5bf9\u8bdd\u8bb0\u5f55\u4e2d\u7684\u4e3b\u9898\u5e76\u8ba1\u7b97\u63d0\u53ca\u8be5\u4e3b\u9898\u7684\u72ec\u7279\u53c2\u4e0e\u8005\u6570\u91cf\uff0c\u514b\u670d\u4e86\u4f20\u7edf\u8bcd\u4e91\u5728\u5bf9\u8bdd\u5206\u6790\u4e2d\u7684\u5c40\u9650\u6027\u3002\u8be5\u5de5\u5177\u751f\u6210\u7684\u8bcd\u4e91\u57fa\u4e8e\u63d0\u53ca\u7684\u5e7f\u5ea6\u800c\u975e\u539f\u59cb\u8bcd\u9891\uff0c\u63d0\u4f9b\u4e86\u66f4\u5177\u53ef\u64cd\u4f5c\u6027\u7684\u89c1\u89e3\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e0e\u57fa\u7ebf\u65b9\u6cd5\u76f8\u6bd4\uff0cThemeClouds\u80fd\u66f4\u6709\u6548\u5730\u63ed\u793a\u8bbe\u5907\u76f8\u5173\u7684\u8bae\u9898\u3002\u6b64\u5916\uff0c\u8be5\u5de5\u5177\u5141\u8bb8\u7814\u7a76\u4eba\u5458\u81ea\u5b9a\u4e49\u63d0\u793a\u548c\u53ef\u89c6\u5316\u53c2\u6570\uff0c\u589e\u5f3a\u4e86\u900f\u660e\u5ea6\u548c\u63a7\u5236\u529b\uff0c\u5e76\u4e3a\u4ea4\u4e92\u5f0f\u5206\u6790\uff08\u5982\u201c\u5dee\u5f02\u4e91\u201d\uff09\u5f00\u8f9f\u4e86\u9053\u8def\u3002"}}
{"id": "2508.06924", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06924", "abs": "https://arxiv.org/abs/2508.06924", "authors": ["Shihao Yuan", "Yahui Liu", "Yang Yue", "Jingyuan Zhang", "Wangmeng Zuo", "Qi Wang", "Fuzheng Zhang", "Guorui Zhou"], "title": "AR-GRPO: Training Autoregressive Image Generation Models via Reinforcement Learning", "comment": "27 pages, 15 figures", "summary": "Inspired by the success of reinforcement learning (RL) in refining large\nlanguage models (LLMs), we propose AR-GRPO, an approach to integrate online RL\ntraining into autoregressive (AR) image generation models. We adapt the Group\nRelative Policy Optimization (GRPO) algorithm to refine the vanilla\nautoregressive models' outputs by carefully designed reward functions that\nevaluate generated images across multiple quality dimensions, including\nperceptual quality, realism, and semantic fidelity. We conduct comprehensive\nexperiments on both class-conditional (i.e., class-to-image) and\ntext-conditional (i.e., text-to-image) image generation tasks, demonstrating\nthat our RL-enhanced framework significantly improves both the image quality\nand human preference of generated images compared to the standard AR baselines.\nOur results show consistent improvements across various evaluation metrics,\nestablishing the viability of RL-based optimization for AR image generation and\nopening new avenues for controllable and high-quality image synthesis. The\nsource codes and models are available at:\nhttps://github.com/Kwai-Klear/AR-GRPO.", "AI": {"tldr": "AR-GRPO \u662f\u4e00\u79cd\u5c06\u5728\u7ebf RL \u8bad\u7ec3\u6574\u5408\u5230\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u591a\u7ef4\u5ea6\u5956\u52b1\u51fd\u6570\u4f18\u5316\u56fe\u50cf\u8d28\u91cf\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u53d7 RL \u5728\u6539\u8fdb LLMs \u65b9\u9762\u6210\u529f\u7684\u542f\u53d1\uff0c\u65e8\u5728\u5c06 RL \u8bad\u7ec3\u6574\u5408\u5230 AR \u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u56fe\u50cf\u8d28\u91cf\u3002", "method": "\u63d0\u51fa AR-GRPO \u65b9\u6cd5\uff0c\u5c06\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u8bad\u7ec3\u6574\u5408\u5230\u81ea\u56de\u5f52\uff08AR\uff09\u56fe\u50cf\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u5e76\u6539\u7f16\u4e86 GRPO \u7b97\u6cd5\uff0c\u901a\u8fc7\u8bc4\u4f30\u751f\u6210\u56fe\u50cf\u7684\u611f\u77e5\u8d28\u91cf\u3001\u771f\u5b9e\u611f\u548c\u8bed\u4e49\u4fdd\u771f\u5ea6\u7b49\u591a\u4e2a\u8d28\u91cf\u7ef4\u5ea6\u6765\u4f18\u5316 AR \u6a21\u578b\u8f93\u51fa\u3002", "result": "\u5728\u7c7b\u522b\u6761\u4ef6\u548c\u6587\u672c\u6761\u4ef6\u56fe\u50cf\u751f\u6210\u4efb\u52a1\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u6807\u51c6 AR \u57fa\u7ebf\u76f8\u6bd4\uff0cAR-GRPO \u6846\u67b6\u5728\u751f\u6210\u7684\u56fe\u50cf\u8d28\u91cf\u548c\u4eba\u7c7b\u504f\u597d\u65b9\u9762\u5747\u6709\u663e\u8457\u63d0\u5347\uff0c\u5e76\u5728\u5404\u79cd\u8bc4\u4f30\u6307\u6807\u4e0a\u5c55\u73b0\u51fa\u4e00\u81f4\u7684\u6539\u8fdb\u3002", "conclusion": "AR-GRPO \u6846\u67b6\u901a\u8fc7\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u5956\u52b1\u51fd\u6570\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u56fe\u50cf\u751f\u6210\u8d28\u91cf\u548c\u4eba\u7c7b\u504f\u597d\uff0c\u8bc1\u660e\u4e86\u57fa\u4e8e RL \u7684\u4f18\u5316\u5728\u81ea\u56de\u5f52\u56fe\u50cf\u751f\u6210\u4e2d\u7684\u53ef\u884c\u6027\uff0c\u5e76\u4e3a\u53ef\u63a7\u3001\u9ad8\u8d28\u91cf\u7684\u56fe\u50cf\u5408\u6210\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002"}}
{"id": "2508.07122", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07122", "abs": "https://arxiv.org/abs/2508.07122", "authors": ["Zhihao Xue", "Yun Zi", "Nia Qi", "Ming Gong", "Yujun Zou"], "title": "Multi-Level Service Performance Forecasting via Spatiotemporal Graph Neural Networks", "comment": null, "summary": "This paper proposes a spatiotemporal graph neural network-based performance\nprediction algorithm to address the challenge of forecasting performance\nfluctuations in distributed backend systems with multi-level service call\nstructures. The method abstracts system states at different time slices into a\nsequence of graph structures. It integrates the runtime features of service\nnodes with the invocation relationships among services to construct a unified\nspatiotemporal modeling framework. The model first applies a graph\nconvolutional network to extract high-order dependency information from the\nservice topology. Then it uses a gated recurrent network to capture the dynamic\nevolution of performance metrics over time. A time encoding mechanism is also\nintroduced to enhance the model's ability to represent non-stationary temporal\nsequences. The architecture is trained in an end-to-end manner, optimizing the\nmulti-layer nested structure to achieve high-precision regression of future\nservice performance metrics. To validate the effectiveness of the proposed\nmethod, a large-scale public cluster dataset is used. A series of\nmulti-dimensional experiments are designed, including variations in time\nwindows and concurrent load levels. These experiments comprehensively evaluate\nthe model's predictive performance and stability. The experimental results show\nthat the proposed model outperforms existing representative methods across key\nmetrics such as MAE, RMSE, and R2. It maintains strong robustness under varying\nload intensities and structural complexities. These results demonstrate the\nmodel's practical potential for backend service performance management tasks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u7b97\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u4e2d\u591a\u5c42\u7ea7\u670d\u52a1\u8c03\u7528\u7ed3\u6784\u7684\u6027\u80fd\u6ce2\u52a8\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u63d0\u53d6\u670d\u52a1\u62d3\u6251\u4f9d\u8d56\u548c\u6355\u6349\u6027\u80fd\u6307\u6807\u7684\u65f6\u95f4\u52a8\u6001\u6f14\u5316\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u9884\u6d4b\uff0c\u5e76\u5728\u5927\u89c4\u6a21\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5c55\u73b0\u4e86\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5206\u5e03\u5f0f\u540e\u7aef\u7cfb\u7edf\u4e2d\u591a\u5c42\u7ea7\u670d\u52a1\u8c03\u7528\u7ed3\u6784\u4e0b\u6027\u80fd\u6ce2\u52a8\u9884\u6d4b\u7684\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65f6\u7a7a\u56fe\u795e\u7ecf\u7f51\u7edc\u7684\u6027\u80fd\u9884\u6d4b\u7b97\u6cd5\u3002\u8be5\u7b97\u6cd5\u5c06\u4e0d\u540c\u65f6\u95f4\u5207\u7247\u4e0b\u7684\u7cfb\u7edf\u72b6\u6001\u62bd\u8c61\u4e3a\u4e00\u7cfb\u5217\u56fe\u7ed3\u6784\uff0c\u5e76\u6574\u5408\u4e86\u670d\u52a1\u8282\u70b9\u7684\u8fd0\u884c\u65f6\u7279\u5f81\u548c\u5b83\u4eec\u4e4b\u95f4\u7684\u8c03\u7528\u5173\u7cfb\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u65f6\u7a7a\u5efa\u6a21\u6846\u67b6\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6a21\u578b\u9996\u5148\u5229\u7528\u56fe\u5377\u79ef\u7f51\u7edc\u63d0\u53d6\u670d\u52a1\u62d3\u6251\u4e2d\u7684\u9ad8\u9636\u4f9d\u8d56\u4fe1\u606f\uff0c\u7136\u540e\u4f7f\u7528\u95e8\u63a7\u5faa\u73af\u7f51\u7edc\u6355\u6349\u6027\u80fd\u6307\u6807\u968f\u65f6\u95f4\u53d8\u5316\u7684\u52a8\u6001\u6f14\u5316\uff0c\u5e76\u5f15\u5165\u65f6\u95f4\u7f16\u7801\u673a\u5236\u6765\u589e\u5f3a\u5bf9\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u7684\u8868\u793a\u80fd\u529b\u3002\u8be5\u6a21\u578b\u91c7\u7528\u7aef\u5230\u7aef\u7684\u65b9\u5f0f\u8fdb\u884c\u8bad\u7ec3\uff0c\u901a\u8fc7\u4f18\u5316\u591a\u5c42\u5d4c\u5957\u7ed3\u6784\u6765\u5b9e\u73b0\u5bf9\u672a\u6765\u670d\u52a1\u6027\u80fd\u6307\u6807\u7684\u9ad8\u7cbe\u5ea6\u56de\u5f52\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728MAE\u3001RMSE\u548cR2\u7b49\u5173\u952e\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u7684\u4ee3\u8868\u6027\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u4e0b\u4fdd\u6301\u4e86\u826f\u597d\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u9884\u6d4b\u6027\u80fd\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u8d1f\u8f7d\u5f3a\u5ea6\u548c\u7ed3\u6784\u590d\u6742\u5ea6\u4e0b\u8868\u73b0\u51fa\u826f\u597d\u7684\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u540e\u7aef\u670d\u52a1\u6027\u80fd\u7ba1\u7406\u65b9\u9762\u7684\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.08113", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08113", "abs": "https://arxiv.org/abs/2508.08113", "authors": ["Yinpei Dai", "Jayjun Lee", "Yichi Zhang", "Ziqiao Ma", "Jed Yang", "Amir Zadeh", "Chuan Li", "Nima Fazeli", "Joyce Chai"], "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of Visuomotor Policies", "comment": "CoRL 2025", "summary": "In this paper, we propose AimBot, a lightweight visual augmentation technique\nthat provides explicit spatial cues to improve visuomotor policy learning in\nrobotic manipulation. AimBot overlays shooting lines and scope reticles onto\nmulti-view RGB images, offering auxiliary visual guidance that encodes the\nend-effector's state. The overlays are computed from depth images, camera\nextrinsics, and the current end-effector pose, explicitly conveying spatial\nrelationships between the gripper and objects in the scene. AimBot incurs\nminimal computational overhead (less than 1 ms) and requires no changes to\nmodel architectures, as it simply replaces original RGB images with augmented\ncounterparts. Despite its simplicity, our results show that AimBot consistently\nimproves the performance of various visuomotor policies in both simulation and\nreal-world settings, highlighting the benefits of spatially grounded visual\nfeedback.", "AI": {"tldr": "AimBot \u662f\u4e00\u79cd\u89c6\u89c9\u589e\u5f3a\u6280\u672f\uff0c\u901a\u8fc7\u6dfb\u52a0\u7784\u51c6\u7ebf\u548c\u51c6\u661f\u7b49\u7a7a\u95f4\u7ebf\u7d22\u6765\u5e2e\u52a9\u673a\u5668\u4eba\u5b66\u4e60\u64cd\u4f5c\uff0c\u4ece\u800c\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u4e14\u8ba1\u7b97\u5f00\u9500\u5f88\u5c0f\u3002", "motivation": "AimBot \u65e8\u5728\u901a\u8fc7\u63d0\u4f9b\u660e\u786e\u7684\u7a7a\u95f4\u7ebf\u7d22\u6765\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u795e\u7ecf\u7b56\u7565\u5b66\u4e60\uff0c\u4ee5\u589e\u5f3a\u7a7a\u95f4\u63a5\u5730\u89c6\u89c9\u53cd\u9988\u3002", "method": "AimBot \u5728\u591a\u89c6\u56fe RGB \u56fe\u50cf\u4e0a\u53e0\u52a0\u4e86\u7784\u51c6\u7ebf\u548c\u51c6\u661f\uff0c\u4ee5\u63d0\u4f9b\u7f16\u7801\u672b\u7aef\u6267\u884c\u5668\u72b6\u6001\u7684\u8f85\u52a9\u89c6\u89c9\u6307\u5bfc\u3002\u8fd9\u4e9b\u53e0\u52a0\u5c42\u662f\u901a\u8fc7\u6df1\u5ea6\u56fe\u50cf\u3001\u76f8\u673a\u5916\u63a5\u548c\u5f53\u524d\u7684\u672b\u7aef\u6267\u884c\u5668\u59ff\u52bf\u8ba1\u7b97\u51fa\u6765\u7684\uff0c\u660e\u786e\u4f20\u8fbe\u4e86\u6293\u624b\u548c\u573a\u666f\u4e2d\u7269\u4f53\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u3002AimBot \u7684\u8ba1\u7b97\u5f00\u9500\u6781\u5c0f\uff08\u4e0d\u5230 1 \u6beb\u79d2\uff09\uff0c\u5e76\u4e14\u4e0d\u9700\u8981\u66f4\u6539\u6a21\u578b\u67b6\u6784\uff0c\u56e0\u4e3a\u5b83\u53ea\u662f\u7528\u589e\u5f3a\u540e\u7684\u5bf9\u5e94\u7269\u66ff\u6362\u539f\u59cb RGB \u56fe\u50cf\u3002", "result": "AimBot \u6301\u7eed\u63d0\u9ad8\u4e86\u5404\u79cd\u89c6\u89c9\u795e\u7ecf\u7b56\u7565\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7a7a\u95f4\u63a5\u5730\u89c6\u89c9\u53cd\u9988\u7684\u597d\u5904\u3002", "conclusion": "AimBot \u662f\u4e00\u79cd\u7b80\u5355\u4f46\u6709\u6548\u7684\u89c6\u89c9\u589e\u5f3a\u6280\u672f\uff0c\u53ef\u4ee5\u901a\u8fc7\u63d0\u4f9b\u660e\u786e\u7684\u7a7a\u95f4\u7ebf\u7d22\u6765\u6539\u8fdb\u673a\u5668\u4eba\u64cd\u4f5c\u4e2d\u7684\u89c6\u89c9\u795e\u7ecf\u7b56\u7565\u5b66\u4e60\u3002\u5b83\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u4e2d\u90fd\u63d0\u9ad8\u4e86\u5404\u79cd\u89c6\u89c9\u795e\u7ecf\u7b56\u7565\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u7a7a\u95f4\u63a5\u5730\u89c6\u89c9\u53cd\u9988\u7684\u597d\u5904\u3002"}}
{"id": "2508.08214", "categories": ["quant-ph", "math-ph", "math.MP"], "pdf": "https://arxiv.org/pdf/2508.08214", "abs": "https://arxiv.org/abs/2508.08214", "authors": ["Ruben Ibarrondo", "Daniel Stilck Fran\u00e7a"], "title": "Average Contraction Coefficients of Quantum Channels", "comment": null, "summary": "The data-processing inequality ensures quantum channels reduce state\ndistinguishability, with contraction coefficients quantifying optimal bounds.\nHowever, these can be overly optimistic and not representative of the usual\nbehavior. We study how noise contracts distinguishability of `typical' states,\nbeyond the worst-case. To that end, we introduce and study a family of moments\nof contraction for quantum divergences, which interpolate between the\nworst-case contraction coefficient of a channel and its average behavior under\na chosen ensemble of input states. We establish general properties of these\nmoments, relate moments for different divergences, and derive bounds in terms\nof channel parameters like the entropy or purity of its Choi state.\n  Focusing on the trace distance, we obtain upper and lower bounds on its\naverage contraction under tensor-product noise channels, and prove that,\ndepending on the local noise strength, there is a phase transition in the limit\nof many channel uses: below a critical error rate the average contraction\nremains near unity, whereas above it decays exponentially with system size. We\nextend these phase-transition phenomena to random quantum circuits with unital\nnoise, showing that constant-depth noisy circuits do not shrink the trace\ndistance on average, even when given highly entangled states as input. In\ncontrast, even at $\\log\\log n$ depth, the average trace distance can become\nsuperpolynomially small.\n  Finally, we explore moments of contraction for f-divergences and discuss\napplications to local differential privacy, demonstrating that noise regimes\nensuring privacy can render outputs essentially indistinguishable on average.\nThus, our results provide a fine-grained framework to quantify typical channel\nnoise in quantum information and computation and unveil new phenomena in\ncontraction coefficients, such as phase transitions for average contraction.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u91cf\u5b50\u4fe1\u9053\u566a\u58f0\u5206\u6790\u65b9\u6cd5\uff0c\u901a\u8fc7\u201c\u6536\u7f29\u77e9\u201d\u91cf\u5316\u201c\u5178\u578b\u201d\u72b6\u6001\u7684\u53ef\u533a\u5206\u6027\uff0c\u53d1\u73b0\u4e86\u5e73\u5747\u6536\u7f29\u7684\u76f8\u53d8\u73b0\u8c61\uff0c\u5e76\u63ed\u793a\u4e86\u566a\u58f0\u3001\u9690\u79c1\u548c\u53ef\u533a\u5206\u6027\u4e4b\u95f4\u7684\u8054\u7cfb\u3002", "motivation": "\u4f20\u7edf\u7684\u91cf\u5b50\u4fe1\u9053\u5206\u6790\u4e3b\u8981\u5173\u6ce8\u201c\u6700\u574f\u60c5\u51b5\u201d\u4e0b\u7684\u6536\u7f29\u7cfb\u6570\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u5bf9\u4fe1\u9053\u884c\u4e3a\u7684\u8fc7\u5ea6\u4e50\u89c2\u4f30\u8ba1\uff0c\u4e0d\u80fd\u4ee3\u8868\u5b9e\u9645\u7684\u201c\u5178\u578b\u201d\u884c\u4e3a\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u566a\u58f0\u5982\u4f55\u6536\u7f29\u201c\u5178\u578b\u201d\u72b6\u6001\u7684\u53ef\u533a\u5206\u6027\uff0c\u8d85\u8d8a\u6700\u574f\u60c5\u51b5\u7684\u9650\u5236\uff0c\u4ee5\u66f4\u7cbe\u7ec6\u5730\u91cf\u5316\u91cf\u5b50\u4fe1\u606f\u548c\u8ba1\u7b97\u4e2d\u7684\u5178\u578b\u4fe1\u9053\u566a\u58f0\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u5e76\u7814\u7a76\u4e86\u4e00\u65cf\u6536\u7f29\u77e9\uff0c\u7528\u4e8e\u63d2\u503c\u4fe1\u9053\u7684\u201c\u6700\u574f\u60c5\u51b5\u201d\u6536\u7f29\u7cfb\u6570\u4e0e\u5176\u5728\u7279\u5b9a\u8f93\u5165\u72b6\u6001\u7cfb\u7efc\u4e0b\u7684\u5e73\u5747\u884c\u4e3a\u3002\u7814\u7a76\u4eba\u5458\u63a8\u5bfc\u4e86\u8fd9\u4e9b\u77e9\u7684\u4e00\u822c\u6027\u8d28\uff0c\u5e76\u5c06\u4e0d\u540c\u6563\u5ea6\u4e0b\u7684\u77e9\u8054\u7cfb\u8d77\u6765\uff0c\u8fd8\u57fa\u4e8eChoi\u72b6\u6001\u7684\u71b5\u6216\u7eaf\u5ea6\u7b49\u4fe1\u9053\u53c2\u6570\u63a8\u5bfc\u51fa\u754c\u9650\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u9488\u5bf9\u8ff9\u8ddd\u79bb\uff0c\u7814\u7a76\u4eba\u5458\u7ed9\u51fa\u4e86\u5176\u5728\u5f20\u91cf\u79ef\u566a\u58f0\u4fe1\u9053\u4e0b\u7684\u5e73\u5747\u6536\u7f29\u7684\u4e0a\u4e0b\u754c\uff0c\u5e76\u8bc1\u660e\u4e86\u5e73\u5747\u6536\u7f29\u884c\u4e3a\u4f1a\u968f\u5c40\u90e8\u566a\u58f0\u5f3a\u5ea6\u7684\u53d8\u5316\u53d1\u751f\u76f8\u53d8\uff1a\u5728\u4f4e\u4e8e\u4e34\u754c\u9519\u8bef\u7387\u65f6\uff0c\u5e73\u5747\u6536\u7f29\u63a5\u8fd1\u4e8e1\uff1b\u9ad8\u4e8e\u4e34\u754c\u9519\u8bef\u7387\u65f6\uff0c\u5e73\u5747\u6536\u7f29\u5219\u968f\u7cfb\u7edf\u5927\u5c0f\u5448\u6307\u6570\u8870\u51cf\u3002\u7814\u7a76\u8fd8\u5c06\u8fd9\u79cd\u76f8\u53d8\u73b0\u8c61\u63a8\u5e7f\u5230\u5177\u6709\u5355\u4f4d\u566a\u58f0\u7684\u968f\u673a\u91cf\u5b50\u7535\u8def\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86\u91cf\u5b50\u4fe1\u9053\u6536\u7f29\u7cfb\u6570\u7684\u65b0\u73b0\u8c61\uff0c\u4f8b\u5982\u5e73\u5747\u6536\u7f29\u7684\u76f8\u53d8\u3002\u5bf9\u4e8e\u8ff9\u8ddd\u79bb\uff0c\u5728\u5f20\u91cf\u79ef\u566a\u58f0\u4fe1\u9053\u4e0b\uff0c\u5f53\u5c40\u90e8\u566a\u58f0\u5f3a\u5ea6\u4f4e\u4e8e\u67d0\u4e00\u4e34\u754c\u503c\u65f6\uff0c\u5e73\u5747\u6536\u7f29\u63a5\u8fd1\u4e8e1\uff1b\u5f53\u9ad8\u4e8e\u8be5\u4e34\u754c\u503c\u65f6\uff0c\u5e73\u5747\u6536\u7f29\u968f\u7cfb\u7edf\u5927\u5c0f\u6307\u6570\u8870\u51cf\u3002\u5bf9\u4e8e\u5177\u6709\u5355\u4f4d\u566a\u58f0\u7684\u968f\u673a\u91cf\u5b50\u7535\u8def\uff0c\u5373\u4f7f\u6df1\u5ea6\u5f88\u6d45\uff08\u5982\u5bf9\u6570\u5bf9\u6570n\uff09\uff0c\u5e73\u5747\u8ff9\u8ddd\u79bb\u4e5f\u53ef\u4ee5\u53d8\u5f97\u975e\u5e38\u5c0f\uff08\u8d85\u591a\u9879\u5f0f\u5730\u5c0f\uff09\u3002\u6b64\u5916\uff0c\u5728f-\u6563\u5ea6\u65b9\u9762\uff0c\u7814\u7a76\u8868\u660e\u4fdd\u8bc1\u9690\u79c1\u7684\u566a\u58f0\u6761\u4ef6\u53ef\u80fd\u4f1a\u5bfc\u81f4\u8f93\u51fa\u5e73\u5747\u65e0\u6cd5\u533a\u5206\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u63d0\u51fa\u4e86\u4e00\u4e2a\u91cf\u5316\u91cf\u5b50\u4fe1\u9053\u566a\u58f0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7814\u7a76\u6536\u7f29\u77e9\u6765\u8861\u91cf\u2018\u5178\u578b\u2019\u72b6\u6001\u7684\u53ef\u533a\u5206\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u5f20\u91cf\u79ef\u566a\u58f0\u4fe1\u9053\u548c\u968f\u673a\u91cf\u5b50\u7535\u8def\u4e2d\u5b58\u5728\u76f8\u53d8\u73b0\u8c61\uff0c\u8fd9\u53d6\u51b3\u4e8e\u5c40\u90e8\u566a\u58f0\u5f3a\u5ea6\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5728\u7279\u5b9a\u566a\u58f0\u6761\u4ef6\u4e0b\uff0c\u5373\u4f7f\u662f\u9ad8\u5ea6\u7ea0\u7f20\u7684\u72b6\u6001\uff0c\u5176\u5e73\u5747\u53ef\u533a\u5206\u6027\u4e5f\u4e0d\u4f1a\u51cf\u5c0f\uff0c\u8fd9\u4e0e\u4f20\u7edf\u7684\u57fa\u4e8e\u6700\u574f\u60c5\u51b5\u7684\u5206\u6790\u4e0d\u540c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u63a2\u8ba8\u4e86f-\u6563\u5ea6\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u672c\u5730\u5dee\u5206\u9690\u79c1\uff0c\u63ed\u793a\u4e86\u5728\u4fdd\u8bc1\u9690\u79c1\u7684\u566a\u58f0\u6761\u4ef6\u4e0b\uff0c\u8f93\u51fa\u5e73\u5747\u800c\u8a00\u51e0\u4e4e\u65e0\u6cd5\u533a\u5206\u3002"}}
{"id": "2508.07534", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07534", "abs": "https://arxiv.org/abs/2508.07534", "authors": ["Jia Deng", "Jie Chen", "Zhipeng Chen", "Daixuan Cheng", "Fei Bai", "Beichen Zhang", "Yinqian Min", "Yanzipeng Gao", "Wayne Xin Zhao", "Ji-Rong Wen"], "title": "From Trial-and-Error to Improvement: A Systematic Analysis of LLM Exploration Mechanisms in RLVR", "comment": "27pages,25figures. arXiv admin note: text overlap with\n  arXiv:2508.02260", "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of large language\nmodels (LLMs). Unlike traditional RL approaches, RLVR leverages rule-based\nfeedback to guide LLMs in generating and refining complex reasoning chains -- a\nprocess critically dependent on effective exploration strategies. While prior\nwork has demonstrated RLVR's empirical success, the fundamental mechanisms\ngoverning LLMs' exploration behaviors remain underexplored. This technical\nreport presents a systematic investigation of exploration capacities in RLVR,\ncovering four main aspects: (1) exploration space shaping, where we develop\nquantitative metrics to characterize LLMs' capability boundaries; (2)\nentropy-performance exchange, analyzed across training stages, individual\ninstances, and token-level patterns; and (3) RL performance optimization,\nexamining methods to effectively translate exploration gains into measurable\nimprovements. By unifying previously identified insights with new empirical\nevidence, this work aims to provide a foundational framework for advancing RLVR\nsystems.", "AI": {"tldr": "RLVR \u662f\u4e00\u79cd\u7528\u4e8e\u589e\u5f3a LLM \u63a8\u7406\u80fd\u529b\u7684\u8303\u4f8b\uff0c\u5b83\u4f9d\u8d56\u4e8e\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u3002\u8be5\u62a5\u544a\u901a\u8fc7\u63a2\u7d22\u7a7a\u95f4\u5851\u9020\u3001\u71b5-\u6027\u80fd\u4ea4\u6362\u548c RL \u6027\u80fd\u4f18\u5316\u6765\u7cfb\u7edf\u5730\u7814\u7a76 RLVR \u4e2d\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u65e8\u5728\u4e3a\u63a8\u8fdb RLVR \u7cfb\u7edf\u63d0\u4f9b\u4e00\u4e2a\u57fa\u7840\u6846\u67b6\u3002", "motivation": "RLVR\u662f\u4e00\u79cd\u5229\u7528\u57fa\u4e8e\u89c4\u5219\u7684\u53cd\u9988\u6765\u6307\u5bfcLLM\u751f\u6210\u548c\u5b8c\u5584\u590d\u6742\u63a8\u7406\u94fe\u7684\u8303\u5f0f\uff0c\u8fd9\u79cd\u8fc7\u7a0b\u5bf9\u6709\u6548\u7684\u63a2\u7d22\u7b56\u7565\u6709\u5173\u952e\u4f9d\u8d56\u3002\u7136\u800c\uff0c\u5148\u524d\u5de5\u4f5c\u4e2dRLVR\u7684\u7ecf\u9a8c\u6210\u529f\uff0cLLM\u63a2\u7d22\u884c\u4e3a\u7684\u57fa\u672c\u673a\u5236\u4ecd\u7136\u6ca1\u6709\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002", "method": "\u901a\u8fc7\u4ee5\u4e0b\u56db\u4e2a\u4e3b\u8981\u65b9\u9762\u5bf9RLVR\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u7814\u7a76\uff1a(1) \u63a2\u7d22\u7a7a\u95f4\u5851\u9020\uff0c\u5f00\u53d1\u4e86\u91cf\u5316\u6307\u6807\u6765\u8868\u5f81LLM\u7684\u80fd\u529b\u8fb9\u754c\uff1b(2) \u71b5-\u6027\u80fd\u4ea4\u6362\uff0c\u5206\u6790\u4e86\u8bad\u7ec3\u9636\u6bb5\u3001\u4e2a\u4f53\u5b9e\u4f8b\u548c token \u7ea7\u6a21\u5f0f\uff1b(3) RL \u6027\u80fd\u4f18\u5316\uff0c\u68c0\u67e5\u4e86\u5c06\u63a2\u7d22\u6536\u76ca\u8f6c\u5316\u4e3a\u53ef\u8861\u91cf\u6539\u8fdb\u7684\u6709\u6548\u65b9\u6cd5\u3002", "result": "\u8be5\u62a5\u544a\u5bf9RLVR\u4e2d\u7684\u63a2\u7d22\u80fd\u529b\u8fdb\u884c\u4e86\u7cfb\u7edf\u7684\u8c03\u67e5\u3002", "conclusion": "\u8be5\u62a5\u544a\u63d0\u51fa\u4e86\u4e00\u4e2a\u7528\u4e8e\u63a8\u8fdbRLVR\u7cfb\u7edf\u7684\u57fa\u7840\u6846\u67b6\uff0c\u901a\u8fc7\u7edf\u4e00\u5148\u524d\u786e\u5b9a\u7684\u89c1\u89e3\u548c\u65b0\u7684\u5b9e\u8bc1\u8bc1\u636e\u3002"}}
{"id": "2508.06937", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06937", "abs": "https://arxiv.org/abs/2508.06937", "authors": ["Weiyan Xie", "Han Gao", "Didan Deng", "Kaican Li", "April Hua Liu", "Yongxiang Huang", "Nevin L. Zhang"], "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for Training-Free Image Editing", "comment": "Project Page: vaynexie.github.io/CannyEdit/", "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.", "AI": {"tldr": "CannyEdit\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u8bad\u7ec3\u65e0\u5173\u6846\u67b6\uff0c\u901a\u8fc7\u9009\u62e9\u6027Canny\u63a7\u5236\u548c\u53cc\u63d0\u793a\u5f15\u5bfc\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u6587\u672c\u5230\u56fe\u50cf\u533a\u57df\u7f16\u8f91\uff0c\u63d0\u9ad8\u4e86\u6587\u672c\u4f9d\u4ece\u6027\u3001\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u65e0\u7f1d\u6027\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u5230\u56fe\u50cf\uff08T2I\uff09\u6a21\u578b\u5728\u8bad\u7ec3\u65e0\u5173\u7684\u533a\u57df\u56fe\u50cf\u7f16\u8f91\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u96be\u4ee5\u5e73\u8861\u7f16\u8f91\u533a\u57df\u7684\u6587\u672c\u4f9d\u4ece\u6027\u3001\u672a\u7f16\u8f91\u533a\u57df\u7684\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u4ee5\u53ca\u7f16\u8f91\u7684\u65e0\u7f1d\u96c6\u6210\u3002", "method": "CannyEdit\u6846\u67b6\u91c7\u7528\u9009\u62e9\u6027Canny\u63a7\u5236\uff08\u5728\u7528\u6237\u6307\u5b9a\u7684\u7f16\u8f91\u533a\u57df\u5185\u4f7f\u7528Canny ControlNet\u8fdb\u884c\u7ed3\u6784\u5f15\u5bfc\uff0c\u540c\u65f6\u4fdd\u7559\u672a\u7f16\u8f91\u533a\u57df\u7684\u6e90\u56fe\u50cf\u7ec6\u8282\uff09\u548c\u53cc\u63d0\u793a\u5f15\u5bfc\uff08\u7ed3\u5408\u5c40\u90e8\u63d0\u793a\u548c\u5168\u5c40\u76ee\u6807\u63d0\u793a\u4ee5\u4fdd\u6301\u573a\u666f\u8fde\u8d2f\u6027\uff09\u3002", "result": "CannyEdit\u5728\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u7f16\u8f91\u4efb\u52a1\uff08\u6dfb\u52a0\u3001\u66ff\u6362\u3001\u79fb\u9664\uff09\u4e2d\uff0c\u6587\u672c\u4f9d\u4ece\u6027\u548c\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u7684\u5e73\u8861\u6bd4KV-Edit\u7b49\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e862.93%\u81f310.49%\u3002\u7528\u6237\u7814\u7a76\u8868\u660e\uff0cCannyEdit\u7684\u7f16\u8f91\u7ed3\u679c\u66f4\u96be\u88ab\u8bc6\u522b\u4e3aAI\u7f16\u8f91\uff0c\u5c24\u5176\u662f\u5728\u4e0e\u672a\u7f16\u8f91\u7684\u771f\u5b9e\u56fe\u50cf\u914d\u5bf9\u65f6\uff0c\u666e\u901a\u7528\u6237\u548cAIGC\u4e13\u5bb6\u7684\u8bc6\u522b\u7387\u5206\u522b\u4e3a49.2%\u548c42.0%\uff0c\u8fdc\u4f4e\u4e8e\u7ade\u4e89\u65b9\u6cd5\uff0876.08%\u81f389.09%\uff09\u3002", "conclusion": "CannyEdit\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027Canny\u63a7\u5236\u548c\u53cc\u63d0\u793a\u5f15\u5bfc\uff0c\u5728\u6587\u672c\u4f9d\u4ece\u6027\u3001\u4e0a\u4e0b\u6587\u4fdd\u771f\u5ea6\u548c\u7f16\u8f91\u65e0\u7f1d\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.07932", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07932", "abs": "https://arxiv.org/abs/2508.07932", "authors": ["Yi Zhai", "Zhiqiang Wei", "Ruohan Li", "Keyu Pan", "Shuo Liu", "Lu Zhang", "Jianmin Ji", "Wuyang Zhang", "Yu Zhang", "Yanyong Zhang"], "title": "\\(X\\)-evolve: Solution space evolution powered by large language models", "comment": null, "summary": "While combining large language models (LLMs) with evolutionary algorithms\n(EAs) shows promise for solving complex optimization problems, current\napproaches typically evolve individual solutions, often incurring high LLM call\ncosts. We introduce \\(X\\)-evolve, a paradigm-shifting method that instead\nevolves solution spaces \\(X\\) (sets of individual solutions) - subsets of the\noverall search space \\(S\\). In \\(X\\)-evolve, LLMs generate tunable programs\nwherein certain code snippets, designated as parameters, define a tunable\nsolution space. A score-based search algorithm then efficiently explores this\nparametrically defined space, guided by feedback from objective function\nscores. This strategy enables broader and more efficient exploration, which can\npotentially accelerate convergence at a much lower search cost, requiring up to\ntwo orders of magnitude fewer LLM calls than prior leading methods. We\ndemonstrate \\(X\\)-evolve's efficacy across three distinct hard optimization\nproblems. For the cap set problem, we discover a larger partial admissible set,\nestablishing a new tighter asymptotic lower bound for the cap set constant (\\(C\n\\ge 2.2203\\)). In information theory, we uncover a larger independent set for\nthe 15-vertex cycle graph (\\(\\mathcal{C}_{15}^{\\boxtimes 5}\\), size 19,946),\nthereby raising the known lower bound on its Shannon capacity. Furthermore, for\nthe NP-hard online bin packing problem, we generate heuristics that\nconsistently outperform standard strategies across established benchmarks. By\nevolving solution spaces, our method considerably improves search\neffectiveness, making it possible to tackle high-dimensional problems that were\npreviously computationally prohibitive.", "AI": {"tldr": "X-evolve \u901a\u8fc7\u8fdb\u5316\u53ef\u8c03\u8c10\u7a0b\u5e8f\u5b9a\u4e49\u7684\u89e3\u7a7a\u95f4\uff08\u800c\u975e\u5355\u4e2a\u89e3\uff09\u6765\u4f18\u5316\u590d\u6742\u95ee\u9898\uff0c\u663e\u8457\u964d\u4f4e\u4e86 LLM \u8c03\u7528\u6210\u672c\uff0c\u5e76\u5728\u76d6\u5e3d\u96c6\u3001\u4fe1\u606f\u8bba\u548c\u5728\u7ebf\u88c5\u7bb1\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u65b0\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5c06\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u4e0e\u8fdb\u5316\u7b97\u6cd5\uff08EA\uff09\u7ed3\u5408\u7528\u4e8e\u89e3\u51b3\u590d\u6742\u4f18\u5316\u95ee\u9898\uff0c\u4f46\u5b83\u4eec\u901a\u5e38\u8fdb\u5316\u5355\u4e2a\u89e3\uff0c\u5bfc\u81f4 LLM \u8c03\u7528\u6210\u672c\u9ad8\u6602\u3002X-evolve \u65e8\u5728\u901a\u8fc7\u8fdb\u5316\u89e3\u7a7a\u95f4\u6765\u964d\u4f4e\u6210\u672c\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a X-evolve \u7684\u65b0\u8303\u5f0f\uff0c\u5b83\u901a\u8fc7\u8fdb\u5316\u89e3\u7a7a\u95f4\uff08\u5373\u4e00\u7ec4\u89e3\uff09\u800c\u4e0d\u662f\u5355\u4e2a\u89e3\u6765\u89e3\u51b3\u4f18\u5316\u95ee\u9898\u3002LLM \u7528\u4e8e\u751f\u6210\u53ef\u8c03\u8c10\u7a0b\u5e8f\uff0c\u5176\u4e2d\u7279\u5b9a\u4ee3\u7801\u7247\u6bb5\u88ab\u6307\u5b9a\u4e3a\u53c2\u6570\uff0c\u4ece\u800c\u5b9a\u4e49\u4e86\u4e00\u4e2a\u53ef\u8c03\u8c10\u7684\u89e3\u7a7a\u95f4\u3002\u7136\u540e\uff0c\u57fa\u4e8e\u5206\u6570\u7684\u641c\u7d22\u7b97\u6cd5\u901a\u8fc7\u76ee\u6807\u51fd\u6570\u5206\u6570\u7684\u53cd\u9988\u6765\u9ad8\u6548\u5730\u63a2\u7d22\u8fd9\u4e2a\u53c2\u6570\u5316\u7a7a\u95f4\u3002", "result": "X-evolve \u5728\u4e09\u4e2a\u4f18\u5316\u95ee\u9898\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\uff1a\u5728\u76d6\u5e3d\u96c6\u95ee\u9898\u4e2d\uff0c\u53d1\u73b0\u4e86\u4e00\u4e2a\u66f4\u5927\u7684\u90e8\u5206\u53ef\u5bb9\u8bb8\u96c6\uff0c\u4e3a\u76d6\u5e3d\u96c6\u5e38\u6570\uff08C \u2265 2.2203\uff09\u5efa\u7acb\u4e86\u4e00\u4e2a\u65b0\u7684\u66f4\u7d27\u7684\u6e10\u8fd1\u4e0b\u754c\uff1b\u5728\u4fe1\u606f\u8bba\u4e2d\uff0c\u4e3a 15-\u9876\u70b9\u5faa\u73af\u56fe\uff08C15\u229b5\uff0c\u5927\u5c0f 19,946\uff09\u53d1\u73b0\u4e86\u4e00\u4e2a\u66f4\u5927\u7684\u72ec\u7acb\u96c6\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5176\u9999\u519c\u5bb9\u91cf\u7684\u5df2\u77e5\u4e0b\u754c\uff1b\u5728 NP-hard \u5728\u7ebf\u88c5\u7bb1\u95ee\u9898\u4e2d\uff0c\u751f\u6210\u4e86\u6301\u7eed\u4f18\u4e8e\u6807\u51c6\u7b56\u7565\u7684\u542f\u53d1\u5f0f\u7b97\u6cd5\u3002", "conclusion": "\u901a\u8fc7\u8fdb\u5316\u89e3\u7a7a\u95f4\u800c\u4e0d\u662f\u5355\u4e2a\u89e3\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u4ee5\u66f4\u4f4e\u7684\u6210\u672c\u89e3\u51b3\u5148\u524d\u8ba1\u7b97\u6210\u672c\u8fc7\u9ad8\u800c\u65e0\u6cd5\u89e3\u51b3\u7684\u9ad8\u7ef4\u95ee\u9898\u3002\u901a\u8fc7\u5728\u4e09\u4e2a\u4e0d\u540c\u7684\u786c\u4f18\u5316\u95ee\u9898\u4e0a\u5c55\u793a\u5176\u6709\u6548\u6027\uff0c\u8be5\u65b9\u6cd5\u5728\u76d6\u5e3d\u96c6\u95ee\u9898\u3001\u4fe1\u606f\u8bba\u548c\u5728\u7ebf\u88c5\u7bb1\u95ee\u9898\u4e0a\u90fd\u53d6\u5f97\u4e86\u663e\u8457\u6210\u679c\u3002"}}
{"id": "2508.07126", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07126", "abs": "https://arxiv.org/abs/2508.07126", "authors": ["Zhengran Ji", "Boyuan Chen"], "title": "Pref-GUIDE: Continual Policy Learning from Real-Time Human Feedback via Preference-Based Learning", "comment": null, "summary": "Training reinforcement learning agents with human feedback is crucial when\ntask objectives are difficult to specify through dense reward functions. While\nprior methods rely on offline trajectory comparisons to elicit human\npreferences, such data is unavailable in online learning scenarios where agents\nmust adapt on the fly. Recent approaches address this by collecting real-time\nscalar feedback to guide agent behavior and train reward models for continued\nlearning after human feedback becomes unavailable. However, scalar feedback is\noften noisy and inconsistent, limiting the accuracy and generalization of\nlearned rewards. We propose Pref-GUIDE, a framework that transforms real-time\nscalar feedback into preference-based data to improve reward model learning for\ncontinual policy training. Pref-GUIDE Individual mitigates temporal\ninconsistency by comparing agent behaviors within short windows and filtering\nambiguous feedback. Pref-GUIDE Voting further enhances robustness by\naggregating reward models across a population of users to form consensus\npreferences. Across three challenging environments, Pref-GUIDE significantly\noutperforms scalar-feedback baselines, with the voting variant exceeding even\nexpert-designed dense rewards. By reframing scalar feedback as structured\npreferences with population feedback, Pref-GUIDE offers a scalable and\nprincipled approach for harnessing human input in online reinforcement\nlearning.", "AI": {"tldr": "Pref-GUIDE \u5c06\u6709\u566a\u58f0\u7684\u5b9e\u65f6\u6807\u91cf\u4eba\u7c7b\u53cd\u9988\u8f6c\u5316\u4e3a\u66f4\u53ef\u9760\u7684\u504f\u597d\u6570\u636e\uff0c\u4ee5\u6539\u8fdb\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u3002", "motivation": "\u5728\u5956\u52b1\u51fd\u6570\u96be\u4ee5\u7cbe\u786e\u5b9a\u4e49\u7684\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u5229\u7528\u4eba\u7c7b\u53cd\u9988\u8fdb\u884c\u8bad\u7ec3\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u5f80\u7684\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u79bb\u7ebf\u8f68\u8ff9\u6bd4\u8f83\u6765\u6536\u96c6\u4eba\u7c7b\u504f\u597d\uff0c\u4f46\u5728\u667a\u80fd\u4f53\u9700\u8981\u5b9e\u65f6\u9002\u5e94\u7684\u5728\u7ebf\u5b66\u4e60\u573a\u666f\u4e2d\uff0c\u8fd9\u79cd\u6570\u636e\u4e0d\u53ef\u7528\u3002\u867d\u7136\u6700\u8fd1\u7684\u65b9\u6cd5\u901a\u8fc7\u6536\u96c6\u5b9e\u65f6\u6807\u91cf\u53cd\u9988\u6765\u6307\u5bfc\u667a\u80fd\u4f53\u884c\u4e3a\uff0c\u4f46\u8fd9\u79cd\u6807\u91cf\u53cd\u9988\u901a\u5e38\u5b58\u5728\u566a\u58f0\u548c\u4e0d\u4e00\u81f4\u6027\uff0c\u9650\u5236\u4e86\u5b66\u4e60\u5230\u7684\u5956\u52b1\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "Pref-GUIDE \u6846\u67b6\u5c06\u5b9e\u65f6\u6807\u91cf\u53cd\u9988\u8f6c\u5316\u4e3a\u504f\u597d\u7c7b\u6570\u636e\uff0c\u4ee5\u6539\u8fdb\u5956\u52b1\u6a21\u578b\u5b66\u4e60\uff0c\u5b9e\u73b0\u6301\u7eed\u7b56\u7565\u8bad\u7ec3\u3002\u5177\u4f53\u6765\u8bf4\uff0cPref-GUIDE Individual \u901a\u8fc7\u6bd4\u8f83\u77ed\u671f\u5185\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u5e76\u8fc7\u6ee4\u6a21\u7cca\u53cd\u9988\u6765\u89e3\u51b3\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002Pref-GUIDE Voting \u8fdb\u4e00\u6b65\u901a\u8fc7\u805a\u5408\u7528\u6237\u7fa4\u4f53\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u6765\u5f62\u6210\u5171\u8bc6\u504f\u597d\uff0c\u4ece\u800c\u63d0\u9ad8\u9c81\u68d2\u6027\u3002", "result": "Pref-GUIDE \u5728\u4e09\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6807\u91cf\u53cd\u9988\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5176\u4e2d\u6295\u7968\u53d8\u4f53\u751a\u81f3\u4f18\u4e8e\u4e13\u5bb6\u8bbe\u8ba1\u7684\u5bc6\u96c6\u5956\u52b1\u3002", "conclusion": "Pref-GUIDE \u901a\u8fc7\u5c06\u6807\u91cf\u53cd\u9988\u91cd\u6784\u4e3a\u57fa\u4e8e\u504f\u597d\u7684\u6570\u636e\uff0c\u6539\u8fdb\u4e86\u7528\u4e8e\u6301\u7eed\u7b56\u7565\u8bad\u7ec3\u7684\u5956\u52b1\u6a21\u578b\u5b66\u4e60\u3002Pref-GUIDE Individual \u901a\u8fc7\u6bd4\u8f83\u77ed\u671f\u5185\u7684\u667a\u80fd\u4f53\u884c\u4e3a\u5e76\u8fc7\u6ee4\u6a21\u7cca\u53cd\u9988\u6765\u51cf\u8f7b\u65f6\u95f4\u4e0d\u4e00\u81f4\u6027\u3002Pref-GUIDE Voting \u901a\u8fc7\u805a\u5408\u7528\u6237\u7fa4\u4f53\u4e2d\u7684\u5956\u52b1\u6a21\u578b\u6765\u5f62\u6210\u5171\u8bc6\u504f\u597d\uff0c\u8fdb\u4e00\u6b65\u589e\u5f3a\u4e86\u9c81\u68d2\u6027\u3002\u5728\u4e09\u79cd\u5177\u6709\u6311\u6218\u6027\u7684\u73af\u5883\u4e2d\uff0cPref-GUIDE \u7684\u8868\u73b0\u660e\u663e\u4f18\u4e8e\u57fa\u4e8e\u6807\u91cf\u53cd\u9988\u7684\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5176\u4e2d\u6295\u7968\u53d8\u4f53\u751a\u81f3\u4f18\u4e8e\u4e13\u5bb6\u8bbe\u8ba1\u7684\u5bc6\u96c6\u5956\u52b1\u3002\u901a\u8fc7\u5c06\u6807\u91cf\u53cd\u9988\u91cd\u6784\u4e3a\u5177\u6709\u7fa4\u4f53\u53cd\u9988\u7684\u7ed3\u6784\u5316\u504f\u597d\uff0cPref-GUIDE \u4e3a\u5728\u7ebf\u5f3a\u5316\u5b66\u4e60\u4e2d\u5229\u7528\u4eba\u7c7b\u8f93\u5165\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u4e14\u6709\u539f\u5219\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.08223", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08223", "abs": "https://arxiv.org/abs/2508.08223", "authors": ["Jhordan A. T. Santiago"], "title": "Photon Statistics for Fock and Coherent States Interfering in a Beamsplitter", "comment": "8 pages, 5 figures", "summary": "We present a straightforward yet comprehensive theoretical study of different\nquantum states emerging from a bi-modal beamsplitter when various input states\ninterfere. Specifically, we analyze the output states for different\ncombinations of input fields, including Fock states $|n\\rangle|m\\rangle$,\nhybrid states $|n\\rangle|\\alpha\\rangle$, and coherent states\n$|\\alpha\\rangle|\\beta\\rangle$. We derive explicit expressions for the output\nstate vectors, calculate the mean photon number, photon number variance, Mandel\nQ parameter, and secondorder coherence function to characterize the statistical\nproperties of the output fields. Our results are intended as a pedagogical\nresource, serving as an introductory reference for students and researchers\naiming to understand basic photon statistics using beamsplitters.", "AI": {"tldr": "Analyzed quantum states from a beamsplitter with different inputs to explain photon statistics, providing a learning resource.", "motivation": "To provide a pedagogical resource for understanding basic photon statistics using beamsplitters, serving as an introductory reference for students and researchers.", "method": "Theoretical study analyzing output states from a bi-modal beamsplitter with different input states (Fock, hybrid, coherent) and calculating statistical properties.", "result": "Explicit expressions for output state vectors and calculated statistical properties (mean photon number, photon number variance, Mandel Q parameter, second-order coherence function) for various input states.", "conclusion": "We derived explicit expressions for the output state vectors and calculated statistical properties like mean photon number, photon number variance, Mandel Q parameter, and second-order coherence function for various input states (Fock, hybrid, coherent) interfering in a bi-modal beamsplitter."}}
{"id": "2508.07592", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07592", "abs": "https://arxiv.org/abs/2508.07592", "authors": ["Puspesh Kumar Srivastava", "Uddeshya Raj", "Praveen Patel", "/Shubham Kumar Nigam", "Noel Shallum", "Arnab Bhattacharya"], "title": "IBPS: Indian Bail Prediction System", "comment": null, "summary": "Bail decisions are among the most frequently adjudicated matters in Indian\ncourts, yet they remain plagued by subjectivity, delays, and inconsistencies.\nWith over 75% of India's prison population comprising undertrial prisoners,\nmany from socioeconomically disadvantaged backgrounds, the lack of timely and\nfair bail adjudication exacerbates human rights concerns and contributes to\nsystemic judicial backlog. In this paper, we present the Indian Bail Prediction\nSystem (IBPS), an AI-powered framework designed to assist in bail\ndecision-making by predicting outcomes and generating legally sound rationales\nbased solely on factual case attributes and statutory provisions. We curate and\nrelease a large-scale dataset of 150,430 High Court bail judgments, enriched\nwith structured annotations such as age, health, criminal history, crime\ncategory, custody duration, statutes, and judicial reasoning. We fine-tune a\nlarge language model using parameter-efficient techniques and evaluate its\nperformance across multiple configurations, with and without statutory context,\nand with RAG. Our results demonstrate that models fine-tuned with statutory\nknowledge significantly outperform baselines, achieving strong accuracy and\nexplanation quality, and generalize well to a test set independently annotated\nby legal experts. IBPS offers a transparent, scalable, and reproducible\nsolution to support data-driven legal assistance, reduce bail delays, and\npromote procedural fairness in the Indian judicial system.", "AI": {"tldr": "IBPS \u662f\u4e00\u4e2a\u57fa\u4e8e\u4eba\u5de5\u667a\u80fd\u7684\u6846\u67b6\uff0c\u65e8\u5728\u901a\u8fc7\u9884\u6d4b\u7ed3\u679c\u548c\u751f\u6210\u4ec5\u57fa\u4e8e\u4e8b\u5b9e\u6848\u4f8b\u5c5e\u6027\u548c\u6cd5\u5b9a\u89c4\u5b9a\u7684\u6cd5\u5f8b\u4e0a\u5408\u7406\u7684\u7406\u7531\u6765\u534f\u52a9\u4fdd\u91ca\u51b3\u7b56\u3002\u6211\u4eec\u6574\u7406\u5e76\u53d1\u5e03\u4e86\u4e00\u4e2a\u5305\u542b 150,430 \u4e2a\u5370\u5ea6\u9ad8\u7b49\u6cd5\u9662\u4fdd\u91ca\u5224\u51b3\u7684\u5927\u578b\u6570\u636e\u96c6\uff0c\u5e76\u9644\u6709\u7ed3\u6784\u5316\u6ce8\u91ca\uff0c\u4f8b\u5982\u5e74\u9f84\u3001\u5065\u5eb7\u3001\u72af\u7f6a\u5386\u53f2\u3001\u72af\u7f6a\u7c7b\u522b\u3001\u7f81\u62bc\u65f6\u95f4\u3001\u6cd5\u89c4\u548c\u53f8\u6cd5\u63a8\u7406\u3002", "motivation": "\u5370\u5ea6\u6cd5\u9662\u7684\u4fdd\u91ca\u51b3\u5b9a\u867d\u7136\u9891\u7e41\uff0c\u4f46\u4ecd\u53d7\u4e3b\u89c2\u6027\u3001\u5ef6\u8bef\u548c\u4e0d\u4e00\u81f4\u6027\u7684\u56f0\u6270\u3002\u8d85\u8fc7 75% \u7684\u5370\u5ea6\u76d1\u72f1\u4eba\u53e3\u662f\u672a\u51b3\u56da\u72af\uff0c\u5176\u4e2d\u8bb8\u591a\u6765\u81ea\u793e\u4f1a\u7ecf\u6d4e\u5f31\u52bf\u7fa4\u4f53\uff0c\u53ca\u65f6\u548c\u516c\u5e73\u7684\u4fdd\u91ca\u88c1\u51b3\u7684\u7f3a\u4e4f\u52a0\u5267\u4e86\u4eba\u6743\u95ee\u9898\u5e76\u5bfc\u81f4\u4e86\u7cfb\u7edf\u6027\u7684\u53f8\u6cd5\u79ef\u538b\u3002", "method": "\u6211\u4eec\u5bf9\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u4e86\u5fae\u8c03\uff0c\u4f7f\u7528\u4e86\u53c2\u6570\u9ad8\u6548\u7684\u6280\u672f\uff0c\u5e76\u5728\u6709\u548c\u6ca1\u6709\u6cd5\u5b9a\u4e0a\u4e0b\u6587\u4ee5\u53ca\u4f7f\u7528 RAG \u7684\u60c5\u51b5\u4e0b\u8bc4\u4f30\u4e86\u5176\u6027\u80fd\u3002", "result": "\u6211\u4eec\u7684\u7ed3\u679c\u8868\u660e\uff0c\u7ecf\u8fc7\u6cd5\u5b9a\u77e5\u8bc6\u5fae\u8c03\u7684\u6a21\u578b\u660e\u663e\u4f18\u4e8e\u57fa\u7ebf\uff0c\u5728\u51c6\u786e\u6027\u548c\u89e3\u91ca\u8d28\u91cf\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u7531\u6cd5\u5f8b\u4e13\u5bb6\u72ec\u7acb\u6ce8\u91ca\u7684\u6d4b\u8bd5\u96c6\u3002", "conclusion": "IBPS \u662f\u4e00\u4e2a\u900f\u660e\u3001\u53ef\u6269\u5c55\u4e14\u53ef\u91cd\u73b0\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u65e8\u5728\u652f\u6301\u6570\u636e\u9a71\u52a8\u7684\u6cd5\u5f8b\u63f4\u52a9\u3001\u51cf\u5c11\u4fdd\u91ca\u5ef6\u8bef\u5e76\u4fc3\u8fdb\u5370\u5ea6\u53f8\u6cd5\u7cfb\u7edf\u7684\u7a0b\u5e8f\u516c\u5e73\u3002"}}
{"id": "2508.07941", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07941", "abs": "https://arxiv.org/abs/2508.07941", "authors": ["Olivier Poulet", "Fr\u00e9d\u00e9ric Guinand", "Fran\u00e7ois Gu\u00e9rin"], "title": "Deep Reinforcement Learning with anticipatory reward in LSTM for Collision Avoidance of Mobile Robots", "comment": null, "summary": "This article proposes a collision risk anticipation method based on\nshort-term prediction of the agents position. A Long Short-Term Memory (LSTM)\nmodel, trained on past trajectories, is used to estimate the next position of\neach robot. This prediction allows us to define an anticipated collision risk\nby dynamically modulating the reward of a Deep Q-Learning Network (DQN) agent.\nThe approach is tested in a constrained environment, where two robots move\nwithout communication or identifiers. Despite a limited sampling frequency (1\nHz), the results show a significant decrease of the collisions number and a\nstability improvement. The proposed method, which is computationally\ninexpensive, appears particularly attractive for implementation on embedded\nsystems.", "AI": {"tldr": "\u901a\u8fc7LSTM\u9884\u6d4b\u673a\u5668\u4eba\u4f4d\u7f6e\u5e76\u7528DQN\u8c03\u6574\u5956\u52b1\uff0c\u964d\u4f4e\u78b0\u649e\u7387\u5e76\u63d0\u9ad8\u7a33\u5b9a\u6027\uff0c\u9002\u7528\u4e8e\u5d4c\u5165\u5f0f\u7cfb\u7edf\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u673a\u5668\u4eba\u81ea\u4e3b\u5bfc\u822a\u4e2d\u7684\u78b0\u649e\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u78b0\u649e\u98ce\u9669\u9884\u77e5\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u77ed\u671f\u9884\u6d4b\u7684\u78b0\u649e\u98ce\u9669\u9884\u77e5\u65b9\u6cd5\uff0c\u5229\u7528\u957f\u77ed\u671f\u8bb0\u5fc6\uff08LSTM\uff09\u6a21\u578b\u4f30\u8ba1\u673a\u5668\u4eba\u4e0b\u4e00\u4f4d\u7f6e\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u6df1\u5ea6Q\u5b66\u4e60\uff08DQN\uff09\u667a\u80fd\u4f53\u7684\u5956\u52b1\u6765\u5b9a\u4e49\u9884\u77e5\u7684\u78b0\u649e\u98ce\u9669\u3002", "result": "\u5728\u65e0\u901a\u4fe1\u6216\u6807\u8bc6\u7b26\u7684\u53cc\u673a\u5668\u4eba\u573a\u666f\u4e0b\uff0c\u5c3d\u7ba1\u91c7\u6837\u9891\u7387\u6709\u9650\uff081Hz\uff09\uff0c\u7ed3\u679c\u663e\u793a\u78b0\u649e\u6b21\u6570\u663e\u8457\u51cf\u5c11\uff0c\u7a33\u5b9a\u6027\u6709\u6240\u63d0\u9ad8\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u4f4e\u5ec9\uff0c\u7279\u522b\u9002\u5408\u5728\u5d4c\u5165\u5f0f\u7cfb\u7edf\u4e0a\u5b9e\u73b0\u3002"}}
{"id": "2508.07127", "categories": ["cs.LG", "q-bio.GN"], "pdf": "https://arxiv.org/pdf/2508.07127", "abs": "https://arxiv.org/abs/2508.07127", "authors": ["Niranjana Arun Menon", "Iqra Farooq", "Yulong Li", "Sara Ahmed", "Yutong Xie", "Muhammad Awais", "Imran Razzak"], "title": "How Effectively Can Large Language Models Connect SNP Variants and ECG Phenotypes for Cardiovascular Risk Prediction?", "comment": null, "summary": "Cardiovascular disease (CVD) prediction remains a tremendous challenge due to\nits multifactorial etiology and global burden of morbidity and mortality.\nDespite the growing availability of genomic and electrophysiological data,\nextracting biologically meaningful insights from such high-dimensional, noisy,\nand sparsely annotated datasets remains a non-trivial task. Recently, LLMs has\nbeen applied effectively to predict structural variations in biological\nsequences. In this work, we explore the potential of fine-tuned LLMs to predict\ncardiac diseases and SNPs potentially leading to CVD risk using genetic markers\nderived from high-throughput genomic profiling. We investigate the effect of\ngenetic patterns associated with cardiac conditions and evaluate how LLMs can\nlearn latent biological relationships from structured and semi-structured\ngenomic data obtained by mapping genetic aspects that are inherited from the\nfamily tree. By framing the problem as a Chain of Thought (CoT) reasoning task,\nthe models are prompted to generate disease labels and articulate informed\nclinical deductions across diverse patient profiles and phenotypes. The\nfindings highlight the promise of LLMs in contributing to early detection, risk\nassessment, and ultimately, the advancement of personalized medicine in cardiac\ncare.", "AI": {"tldr": "LLM\u5728\u57fa\u56e0\u7ec4\u6570\u636e\u5206\u6790\u4e2d\u5c55\u73b0\u51fa\u5728\u5fc3\u810f\u75c5\u98ce\u9669\u9884\u6d4b\u65b9\u9762\u7684\u6f5c\u529b\u3002", "motivation": "\u5fc3\u8840\u7ba1\u75be\u75c5\uff08CVD\uff09\u7684\u9884\u6d4b\u56e0\u5176\u591a\u56e0\u7d20\u75c5\u56e0\u548c\u5168\u7403\u8d1f\u62c5\u800c\u5145\u6ee1\u6311\u6218\u3002\u5c3d\u7ba1\u57fa\u56e0\u7ec4\u5b66\u548c\u7535\u751f\u7406\u5b66\u6570\u636e\u53ef\u7528\u6027\u4e0d\u65ad\u63d0\u9ad8\uff0c\u4f46\u4ece\u8fd9\u4e9b\u6570\u636e\u4e2d\u63d0\u53d6\u6709\u610f\u4e49\u7684\u751f\u7269\u5b66\u89c1\u89e3\u4ecd\u7136\u56f0\u96be\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5fae\u8c03LLM\u5728\u5229\u7528\u57fa\u56e0\u6807\u8bb0\u9884\u6d4b\u5fc3\u810f\u75c5\u548cSNP\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u5229\u7528\u94fe\u5f0f\u601d\u8003\uff08CoT\uff09\u63a8\u7406\uff0c\u5bf9\u5fae\u8c03\u540e\u7684LLM\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f7f\u5176\u80fd\u591f\u5904\u7406\u9ad8\u7ef4\u3001\u5608\u6742\u548c\u7a00\u758f\u6ce8\u91ca\u7684\u57fa\u56e0\u7ec4\u6570\u636e\uff0c\u5e76\u751f\u6210\u75be\u75c5\u6807\u7b7e\u548c\u4e34\u5e8a\u63a8\u65ad\u3002", "result": "LLM\u80fd\u591f\u5b66\u4e60\u7ed3\u6784\u5316\u548c\u534a\u7ed3\u6784\u5316\u57fa\u56e0\u7ec4\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u751f\u7269\u5b66\u5173\u7cfb\uff0c\u5e76\u6839\u636e\u5bb6\u5ead\u9057\u4f20\u8c31\u7cfb\u4fe1\u606f\u8fdb\u884c\u63a8\u65ad\u3002", "conclusion": "LLM\u5728\u5fc3\u810f\u75c5\u9884\u6d4b\u548cSNP\u98ce\u9669\u8bc4\u4f30\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u6709\u671b\u4fc3\u8fdb\u5fc3\u810f\u75c5\u65e9\u671f\u68c0\u6d4b\u3001\u98ce\u9669\u8bc4\u4f30\u548c\u4e2a\u6027\u5316\u533b\u7597\u3002"}}
{"id": "2508.08226", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08226", "abs": "https://arxiv.org/abs/2508.08226", "authors": ["Haiyue Chen", "Aniket Datar", "Tong Xu", "Francesco Cancelliere", "Harsh Rangwala", "Madhan Balaji Rao", "Daeun Song", "David Eichinger", "Xuesu Xiao"], "title": "Verti-Arena: A Controllable and Standardized Indoor Testbed for Multi-Terrain Off-Road Autonomy", "comment": "6 pages", "summary": "Off-road navigation is an important capability for mobile robots deployed in\nenvironments that are inaccessible or dangerous to humans, such as disaster\nresponse or planetary exploration. Progress is limited due to the lack of a\ncontrollable and standardized real-world testbed for systematic data collection\nand validation. To fill this gap, we introduce Verti-Arena, a reconfigurable\nindoor facility designed specifically for off-road autonomy. By providing a\nrepeatable benchmark environment, Verti-Arena supports reproducible experiments\nacross a variety of vertically challenging terrains and provides precise ground\ntruth measurements through onboard sensors and a motion capture system.\nVerti-Arena also supports consistent data collection and comparative evaluation\nof algorithms in off-road autonomy research. We also develop a web-based\ninterface that enables research groups worldwide to remotely conduct\nstandardized off-road autonomy experiments on Verti-Arena.", "AI": {"tldr": "Verti-Arena\u662f\u4e00\u4e2a\u7528\u4e8e\u8d8a\u91ce\u673a\u5668\u4eba\u5bfc\u822a\u7684\u53ef\u91cd\u6784\u5ba4\u5185\u6d4b\u8bd5\u8bbe\u65bd\uff0c\u7528\u4e8e\u6570\u636e\u6536\u96c6\u548c\u7b97\u6cd5\u9a8c\u8bc1\u3002", "motivation": "\u8d8a\u91ce\u5bfc\u822a\u5bf9\u79fb\u52a8\u673a\u5668\u4eba\u5f88\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u53ef\u63a7\u548c\u6807\u51c6\u5316\u7684\u771f\u5b9e\u4e16\u754c\u6d4b\u8bd5\u5e73\u53f0\u6765\u6536\u96c6\u6570\u636e\u548c\u9a8c\u8bc1\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u5f15\u5165\u4e86Verti-Arena\u3002", "method": "\u4ecb\u7ecd\u4e86Verti-Arena\uff0c\u4e00\u4e2a\u53ef\u91cd\u6784\u7684\u5ba4\u5185\u8bbe\u65bd\uff0c\u7528\u4e8e\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u7684\u7cfb\u7edf\u5316\u6570\u636e\u6536\u96c6\u548c\u9a8c\u8bc1\u3002", "result": "Verti-Arena\u662f\u4e00\u4e2a\u53ef\u91cd\u6784\u7684\u5ba4\u5185\u8bbe\u65bd\uff0c\u7528\u4e8e\u8d8a\u91ce\u81ea\u4e3b\u5bfc\u822a\u7684\u6d4b\u8bd5\u548c\u6570\u636e\u6536\u96c6\u3002", "conclusion": "Verti-Arena\u901a\u8fc7\u63d0\u4f9b\u4e00\u4e2a\u53ef\u91cd\u590d\u7684\u57fa\u51c6\u73af\u5883\uff0c\u652f\u6301\u5728\u5404\u79cd\u5782\u76f4\u6311\u6218\u5730\u5f62\u4e0a\u8fdb\u884c\u53ef\u91cd\u590d\u7684\u5b9e\u9a8c\uff0c\u5e76\u901a\u8fc7\u8f66\u8f7d\u4f20\u611f\u5668\u548c\u8fd0\u52a8\u6355\u6349\u7cfb\u7edf\u63d0\u4f9b\u7cbe\u786e\u7684\u5730\u9762\u771f\u5b9e\u6d4b\u91cf\u3002Verti-Arena\u8fd8\u652f\u6301\u8de8\u7814\u7a76\u7ec4\u7684\u6807\u51c6\u5316\u6570\u636e\u6536\u96c6\u548c\u7b97\u6cd5\u7684\u6bd4\u8f83\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u57fa\u4e8eWeb\u7684\u63a5\u53e3\u5b9e\u73b0\u4e86\u5168\u7403\u8fdc\u7a0b\u5b9e\u9a8c\u3002"}}
{"id": "2508.08229", "categories": ["quant-ph"], "pdf": "https://arxiv.org/pdf/2508.08229", "abs": "https://arxiv.org/abs/2508.08229", "authors": ["Tyler Smith", "Tanvi P. Gujarati", "Mario Motta", "Ben Link", "Ieva Liepuoniute", "Triet Friedhoff", "Hiromichi Nishimura", "Nam Nguyen", "Kristen S. Williams", "Javier Robledo Moreno", "Caleb Johnson", "Kevin J. Sung", "Abdullah Ash Saki", "Marna Kagele"], "title": "Quantum-centric simulation of hydrogen abstraction by sample-based quantum diagonalization and entanglement forging", "comment": "14 pages, 8 figures, 2 tables", "summary": "The simulation of electronic systems is an anticipated application for\nquantum-centric computers, i.e. heterogeneous architectures where classical and\nquantum processing units operate in concert. An important application is the\ncomputation of radical chain reactions, including those responsible for the\nphotodegradation of composite materials used in aerospace engineering. Here, we\ncompute the activation energy and reaction energy for hydrogen abstraction from\n2,2-diphenyldipropane, used as a minimal model for a step in a radical chain\nreaction. Calculations are performed using a superconducting quantum processor\nof the IBM Heron family and classical computing resources. To this end, we\ncombine a qubit-reduction technique called entanglement forging (EF) with\nsample based quantum diagonalization (SQD), a method that projects the\nSchr\\\"{o}dinger equation into a subspace of configurations sampled from a\nquantum device. In conventional quantum simulations, a qubit represents a\nspin-orbital. In contrast, EF maps a qubit to a spatial orbital, reducing the\nrequired number of qubits by half. We provide a complete derivation and a\ndetailed description of the combined EF and SQD approach, and we assess its\naccuracy across active spaces of varying sizes.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86\u4e00\u79cd\u7ed3\u5408\u7ea0\u7f20\u953b\u9020\uff08EF\uff09\u548c\u57fa\u4e8e\u6837\u672c\u7684\u91cf\u5b50\u5bf9\u89d2\u5316\uff08SQD\uff09\u7684\u65b0\u578b\u91cf\u5b50\u6a21\u62df\u65b9\u6cd5\uff0c\u5e76\u6210\u529f\u5e94\u7528\u4e8e\u8ba1\u7b972,2-\u4e8c\u82ef\u57fa\u4e19\u70f7\u7684\u6c22\u63d0\u53d6\u53cd\u5e94\u7684\u6d3b\u5316\u80fd\u548c\u53cd\u5e94\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u5316\u5b66\u6a21\u62df\u9886\u57df\u7684\u6f5c\u529b\u3002", "motivation": "\u4e3a\u4e86\u5728\u91cf\u5b50\u8ba1\u7b97\u7684\u5e94\u7528\u4e2d\u6a21\u62df\u7535\u5b50\u7cfb\u7edf\uff0c\u7279\u522b\u662f\u8ba1\u7b97\u590d\u5408\u6750\u6599\u5149\u964d\u89e3\u4e2d\u7684\u81ea\u7531\u57fa\u94fe\u53cd\u5e94\uff08\u5982\u822a\u7a7a\u822a\u5929\u5de5\u7a0b\u4e2d\u4f7f\u7528\u7684\u6750\u6599\uff09\uff0c\u672c\u6587\u7814\u7a76\u4e86\u81ea\u7531\u57fa\u94fe\u53cd\u5e94\u4e2d\u7684\u4e00\u4e2a\u5173\u952e\u6b65\u9aa4\u2014\u20142,2-\u4e8c\u82ef\u57fa\u4e19\u70f7\u7684\u6c22\u63d0\u53d6\u53cd\u5e94\u7684\u6d3b\u5316\u80fd\u548c\u53cd\u5e94\u80fd\u3002", "method": "\u672c\u6587\u7ed3\u5408\u4e86\u7ea0\u7f20\u953b\u9020\uff08EF\uff09\u548c\u57fa\u4e8e\u6837\u672c\u7684\u91cf\u5b50\u5bf9\u89d2\u5316\uff08SQD\uff09\u4e24\u79cd\u6280\u672f\u3002EF\u662f\u4e00\u79cd\u91cf\u5b50\u6bd4\u7279\u7ea6\u7b80\u6280\u672f\uff0c\u5b83\u5c06\u91cf\u5b50\u6bd4\u7279\u6620\u5c04\u5230\u7a7a\u95f4\u8f68\u9053\uff0c\u4ece\u800c\u5c06\u6240\u9700\u7684\u91cf\u5b50\u6bd4\u7279\u6570\u91cf\u51cf\u5c11\u4e00\u534a\u3002SQD\u5219\u662f\u4e00\u79cd\u5c06\u859b\u5b9a\u8c14\u65b9\u7a0b\u6295\u5f71\u5230\u4ece\u91cf\u5b50\u8bbe\u5907\u91c7\u6837\u7684\u914d\u7f6e\u5b50\u7a7a\u95f4\u4e2d\u7684\u65b9\u6cd5\u3002\u901a\u8fc7\u7ed3\u5408\u8fd9\u4e24\u79cd\u6280\u672f\uff0c\u5e76\u5229\u7528IBM Heron\u7cfb\u5217\u7684\u8d85\u5bfc\u91cf\u5b50\u5904\u7406\u5668\u548c\u7ecf\u5178\u8ba1\u7b97\u8d44\u6e90\uff0c\u5bf92,2-\u4e8c\u82ef\u57fa\u4e19\u70f7\u7684\u6c22\u63d0\u53d6\u53cd\u5e94\u8fdb\u884c\u4e86\u8ba1\u7b97\u3002", "result": "\u8ba1\u7b97\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u7ed3\u5408EF\u548cSQD\u7684\u65b9\u6cd5\u80fd\u591f\u51c6\u786e\u5730\u8bc4\u4f30\u4e0d\u540c\u5c3a\u5bf8\u6d3b\u6027\u7a7a\u95f4\u7684\u5316\u5b66\u53cd\u5e94\u80fd\u91cf\uff0c\u4e3a\u5728\u91cf\u5b50\u8ba1\u7b97\u673a\u4e0a\u8fdb\u884c\u590d\u6742\u7684\u5316\u5b66\u6a21\u62df\u63d0\u4f9b\u4e86\u53ef\u884c\u6027\u3002", "conclusion": "\u7ed3\u5408\u7ea0\u7f20\u953b\u9020\uff08EF\uff09\u548c\u57fa\u4e8e\u6837\u672c\u7684\u91cf\u5b50\u5bf9\u89d2\u5316\uff08SQD\uff09\u7684\u65b9\u6cd5\u88ab\u8bc1\u660e\u53ef\u4ee5\u51c6\u786e\u5730\u6a21\u62df\u5316\u5b66\u53cd\u5e94\uff0c\u4e3a\u91cf\u5b50\u8ba1\u7b97\u5728\u6750\u6599\u79d1\u5b66\u548c\u5316\u5b66\u6a21\u62df\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.07598", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07598", "abs": "https://arxiv.org/abs/2508.07598", "authors": ["Ziheng Li", "Zhi-Hong Deng"], "title": "Keyword-Centric Prompting for One-Shot Event Detection with Self-Generated Rationale Enhancements", "comment": "ECAI 2025", "summary": "Although the LLM-based in-context learning (ICL) paradigm has demonstrated\nconsiderable success across various natural language processing tasks, it\nencounters challenges in event detection. This is because LLMs lack an accurate\nunderstanding of event triggers and tend to make over-interpretation, which\ncannot be effectively corrected through in-context examples alone. In this\npaper, we focus on the most challenging one-shot setting and propose KeyCP++, a\nkeyword-centric chain-of-thought prompting approach. KeyCP++ addresses the\nweaknesses of conventional ICL by automatically annotating the logical gaps\nbetween input text and detection results for the demonstrations. Specifically,\nto generate in-depth and meaningful rationale, KeyCP++ constructs a trigger\ndiscrimination prompting template. It incorporates the exemplary triggers\n(a.k.a keywords) into the prompt as the anchor to simply trigger profiling, let\nLLM propose candidate triggers, and justify each candidate. These\npropose-and-judge rationales help LLMs mitigate over-reliance on the keywords\nand promote detection rule learning. Extensive experiments demonstrate the\neffectiveness of our approach, showcasing significant advancements in one-shot\nevent detection.", "AI": {"tldr": "KeyCP++\u901a\u8fc7\u5173\u952e\u8bcd\u548c\u601d\u7ef4\u94fe\u63d0\u793a\u6539\u8fdb\u4e86LLM\u5728\u4e8b\u4ef6\u68c0\u6d4b\u4e2d\u7684\u8868\u73b0\uff0c\u5c24\u5176\u662f\u5728one-shot\u573a\u666f\u4e0b\u3002", "motivation": "LLM\u5728\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u5b58\u5728\u5bf9\u4e8b\u4ef6\u89e6\u53d1\u8bcd\u7406\u89e3\u4e0d\u51c6\u786e\u548c\u8fc7\u5ea6\u89e3\u91ca\u7684\u95ee\u9898\uff0c\u4ec5\u9760\u4e0a\u4e0b\u6587\u793a\u4f8b\u96be\u4ee5\u7ea0\u6b63\u3002", "method": "KeyCP++\u662f\u4e00\u79cd\u5173\u952e\u8bcd\u9a71\u52a8\u7684\u601d\u7ef4\u94fe\u63d0\u793a\u65b9\u6cd5\uff0c\u901a\u8fc7\u81ea\u52a8\u6807\u6ce8\u6f14\u793a\u4e2d\u7684\u903b\u8f91\u5dee\u8ddd\uff0c\u5e76\u7ed3\u5408\u89e6\u53d1\u8bcd\u6b67\u4e49\u63d0\u793a\u6a21\u677f\uff0c\u8ba9LLM\u63d0\u51fa\u5e76\u8bba\u8bc1\u5019\u9009\u89e6\u53d1\u8bcd\uff0c\u4ece\u800c\u5f25\u8865\u4e86\u4f20\u7edfICL\u7684\u4e0d\u8db3\uff0c\u6709\u52a9\u4e8eLLM\u5b66\u4e60\u68c0\u6d4b\u89c4\u5219\u3002", "result": "\u63d0\u51fa\u7684KeyCP++\u5728one-shot\u4e8b\u4ef6\u68c0\u6d4b\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "KeyCP++\u5728one-shot\u4e8b\u4ef6\u68c0\u6d4b\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.06959", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06959", "abs": "https://arxiv.org/abs/2508.06959", "authors": ["Qin Xu", "Lili Zhu", "Xiaoxia Cheng", "Bo Jiang"], "title": "Beyond Frequency: Seeing Subtle Cues Through the Lens of Spatial Decomposition for Fine-Grained Visual Classification", "comment": null, "summary": "The crux of resolving fine-grained visual classification (FGVC) lies in\ncapturing discriminative and class-specific cues that correspond to subtle\nvisual characteristics. Recently, frequency decomposition/transform based\napproaches have attracted considerable interests since its appearing\ndiscriminative cue mining ability. However, the frequency-domain methods are\nbased on fixed basis functions, lacking adaptability to image content and\nunable to dynamically adjust feature extraction according to the discriminative\nrequirements of different images. To address this, we propose a novel method\nfor FGVC, named Subtle-Cue Oriented Perception Engine (SCOPE), which adaptively\nenhances the representational capability of low-level details and high-level\nsemantics in the spatial domain, breaking through the limitations of fixed\nscales in the frequency domain and improving the flexibility of multi-scale\nfusion. The core of SCOPE lies in two modules: the Subtle Detail Extractor\n(SDE), which dynamically enhances subtle details such as edges and textures\nfrom shallow features, and the Salient Semantic Refiner (SSR), which learns\nsemantically coherent and structure-aware refinement features from the\nhigh-level features guided by the enhanced shallow features. The SDE and SSR\nare cascaded stage-by-stage to progressively combine local details with global\nsemantics. Extensive experiments demonstrate that our method achieves new\nstate-of-the-art on four popular fine-grained image classification benchmarks.", "AI": {"tldr": "SCOPE\uff1a\u4e00\u79cd\u901a\u8fc7\u52a8\u6001\u589e\u5f3a\u7ec6\u8282\u548c\u8bed\u4e49\u6765\u6539\u8fdb\u7ec6\u7c92\u5ea6\u89c6\u89c9\u5206\u7c7b\u7684\u65b0\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u9891\u57df\u7684\u65b9\u6cd5\u867d\u7136\u80fd\u6316\u6398\u51fa\u533a\u5206\u6027\u7ebf\u7d22\uff0c\u4f46\u5176\u56fa\u5b9a\u7684\u57fa\u51fd\u6570\u7f3a\u4e4f\u5bf9\u56fe\u50cf\u5185\u5bb9\u7684\u9002\u5e94\u6027\uff0c\u65e0\u6cd5\u6839\u636e\u4e0d\u540c\u56fe\u50cf\u7684\u533a\u5206\u6027\u9700\u6c42\u52a8\u6001\u8c03\u6574\u7279\u5f81\u63d0\u53d6\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u63d0\u51fa SCOPE \u65b9\u6cd5\u6765\u9002\u5e94\u6027\u5730\u589e\u5f3a\u7a7a\u95f4\u57df\u4e2d\u4f4e\u7ea7\u7ec6\u8282\u548c\u9ad8\u7ea7\u8bed\u4e49\u7684\u8868\u793a\u80fd\u529b\uff0c\u7a81\u7834\u4e86\u9891\u57df\u4e2d\u56fa\u5b9a\u5c3a\u5ea6\u7684\u9650\u5236\uff0c\u63d0\u9ad8\u4e86\u591a\u5c3a\u5ea6\u878d\u5408\u7684\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aSCOPE \u7684\u65b0\u9896\u65b9\u6cd5\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u6a21\u5757\uff1a\u7ec6\u5fae\u7ec6\u8282\u63d0\u53d6\u5668 (SDE) \u548c\u663e\u8457\u8bed\u4e49\u7cbe\u70bc\u5668 (SSR)\u3002SDE \u52a8\u6001\u589e\u5f3a\u7a7a\u95f4\u57df\u4e2d\u7684\u7ec6\u5fae\u7ec6\u8282\uff08\u5982\u8fb9\u7f18\u548c\u7eb9\u7406\uff09\uff0cSSR \u4ece\u9ad8\u5c42\u7279\u5f81\u4e2d\u5b66\u4e60\u8bed\u4e49\u4e0a\u8fde\u8d2f\u4e14\u7ed3\u6784\u611f\u77e5\u7684\u7cbe\u70bc\u7279\u5f81\uff0c\u5e76\u4ee5\u589e\u5f3a\u7684\u6d45\u5c42\u7279\u5f81\u4e3a\u6307\u5bfc\u3002", "result": "SCOP \u901a\u8fc7\u591a\u9636\u6bb5\u7ea7\u8054 SDE \u548c SSR\uff0c\u6709\u6548\u7ed3\u5408\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\uff0c\u5728\u56db\u4e2a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\u3002", "conclusion": "SCOP\u901a\u8fc7\u591a\u9636\u6bb5\u7ea7\u8054SDE\u548cSSR\uff0c\u6709\u6548\u7ed3\u5408\u5c40\u90e8\u7ec6\u8282\u548c\u5168\u5c40\u8bed\u4e49\uff0c\u5728\u56db\u4e2a\u7ec6\u7c92\u5ea6\u56fe\u50cf\u5206\u7c7b\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u6210\u679c\u3002"}}
{"id": "2508.07134", "categories": ["cs.LG", "cs.DM", "15A23, 90C26, 62H25", "I.2.6; I.5.3"], "pdf": "https://arxiv.org/pdf/2508.07134", "abs": "https://arxiv.org/abs/2508.07134", "authors": ["Lu Chenggang"], "title": "A Globally Optimal Analytic Solution for Semi-Nonnegative Matrix Factorization with Nonnegative or Mixed Inputs", "comment": "10 pages, 2 figures, under review in [SIAM Journal of Optimization]", "summary": "Semi-Nonnegative Matrix Factorization (semi-NMF) extends classical\nNonnegative Matrix Factorization (NMF) by allowing the basis matrix to contain\nboth positive and negative entries, making it suitable for decomposing data\nwith mixed signs. However, most existing semi-NMF algorithms are iterative,\nnon-convex, and prone to local minima. In this paper, we propose a novel method\nthat yields a globally optimal solution to the semi-NMF problem under the\nFrobenius norm, through an orthogonal decomposition derived from the scatter\nmatrix of the input data. We rigorously prove that our solution attains the\nglobal minimum of the reconstruction error. Furthermore, we demonstrate that\nwhen the input matrix is nonnegative, our method often achieves lower\nreconstruction error than standard NMF algorithms, although unfortunately the\nbasis matrix may not satisfy nonnegativity. In particular, in low-rank cases\nsuch as rank 1 or 2, our solution reduces exactly to a nonnegative\nfactorization, recovering the NMF structure. We validate our approach through\nexperiments on both synthetic data and the UCI Wine dataset, showing that our\nmethod consistently outperforms existing NMF and semi-NMF methods in terms of\nreconstruction accuracy. These results confirm that our globally optimal,\nnon-iterative formulation offers both theoretical guarantees and empirical\nadvantages, providing a new perspective on matrix factorization in optimization\nand data analysis.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u3001\u65e0\u8fed\u4ee3\u7684\u3001\u5168\u5c40\u6700\u4f18\u7684\u534a\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08semi-NMF\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6b63\u4ea4\u5206\u89e3\u89e3\u51b3\u73b0\u6709\u7b97\u6cd5\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u534a\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08semi-NMF\uff09\u7b97\u6cd5\u901a\u5e38\u662f\u8fed\u4ee3\u7684\u3001\u975e\u51f8\u7684\uff0c\u5e76\u4e14\u5bb9\u6613\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u9650\u5236\u4e86\u5176\u5728\u5206\u89e3\u5177\u6709\u6df7\u5408\u7b26\u53f7\u7684\u6570\u636e\u65b9\u9762\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u5bf9\u8f93\u5165\u6570\u636e\u7684\u6563\u5e03\u77e9\u9635\u8fdb\u884c\u6b63\u4ea4\u5206\u89e3\u6765\u83b7\u5f97\u534a\u975e\u8d1f\u77e9\u9635\u5206\u89e3\uff08semi-NMF\uff09\u5168\u5c40\u6700\u4f18\u89e3\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u662f\u65e0\u8fed\u4ee3\u7684\u3002", "result": "\u5728Frobenius\u8303\u6570\u4e0b\uff0c\u8be5\u65b9\u6cd5\u80fd\u83b7\u5f97semi-NMF\u95ee\u9898\u7684\u5168\u5c40\u6700\u4f18\u89e3\uff0c\u5e76\u4e25\u683c\u8bc1\u660e\u4e86\u5176\u89e3\u80fd\u591f\u8fbe\u5230\u91cd\u6784\u8bef\u5dee\u7684\u5168\u5c40\u6700\u5c0f\u503c\u3002\u5728\u8f93\u5165\u77e9\u9635\u975e\u8d1f\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u65b9\u6cd5\u901a\u5e38\u80fd\u83b7\u5f97\u6bd4\u6807\u51c6NMF\u7b97\u6cd5\u66f4\u4f4e\u7684\u91cd\u6784\u8bef\u5dee\uff0c\u5e76\u4e14\u5728\u4f4e\u79e9\u60c5\u51b5\u4e0b\uff08\u79e9\u4e3a1\u62162\uff09\u80fd\u7cbe\u786e\u5730\u8fd8\u539f\u4e3a\u975e\u8d1f\u77e9\u9635\u5206\u89e3\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u91cd\u6784\u7cbe\u5ea6\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u7684NMF\u548csemi-NMF\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u7b97\u6cd5\u63d0\u4f9b\u7406\u8bba\u4fdd\u8bc1\u548c\u5b9e\u9645\u4f18\u52bf\uff0c\u4e3a\u4f18\u5316\u548c\u6570\u636e\u5206\u6790\u4e2d\u7684\u77e9\u9635\u5206\u89e3\u63d0\u4f9b\u4e86\u65b0\u7684\u89c6\u89d2\u3002"}}
{"id": "2508.08240", "categories": ["cs.RO", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08240", "abs": "https://arxiv.org/abs/2508.08240", "authors": ["Kaijun Wang", "Liqin Lu", "Mingyu Liu", "Jianuo Jiang", "Zeju Li", "Bolin Zhang", "Wancai Zheng", "Xinyi Yu", "Hao Chen", "Chunhua Shen"], "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for Long-Horizon Tasks", "comment": null, "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/", "AI": {"tldr": "ODYSSEY\u6846\u67b6\u89e3\u51b3\u4e86\u56db\u8db3\u673a\u5668\u4eba\u8fdb\u884c\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u671f\u79fb\u52a8\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u901a\u8fc7\u7ed3\u5408\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u548c\u5168\u8eab\u63a7\u5236\uff0c\u5b9e\u73b0\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7814\u7a76\u5728\u79fb\u52a8\u64cd\u4f5c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff1a1. \u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728\u7a7a\u95f4\u63a8\u7406\u548c\u4efb\u52a1\u89c4\u5212\u65b9\u9762\u867d\u6709\u6539\u8fdb\uff0c\u4f46\u4ec5\u9650\u4e8e\u684c\u9762\u573a\u666f\uff0c\u672a\u80fd\u89e3\u51b3\u79fb\u52a8\u5e73\u53f0\u7684\u611f\u77e5\u7ea6\u675f\u548c\u9a71\u52a8\u8303\u56f4\u9650\u5236\u30022. \u5f53\u524d\u7684\u64cd\u4f5c\u7b56\u7565\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u9762\u5bf9\u591a\u6837\u5316\u7684\u7269\u4f53\u914d\u7f6e\u65f6\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u8db3\u30023. \u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u540c\u65f6\u4fdd\u6301\u9ad8\u5e73\u53f0\u673a\u52a8\u6027\u548c\u7cbe\u786e\u7684\u672b\u7aef\u6267\u884c\u5668\u63a7\u5236\u8fd9\u4e00\u5173\u952e\u7684\u53cc\u91cd\u9700\u6c42\u7814\u7a76\u4e0d\u8db3\u3002", "method": "ODYSSEY\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u79fb\u52a8\u64cd\u4f5c\u6846\u67b6\uff0c\u4e13\u4e3a\u914d\u5907\u673a\u68b0\u81c2\u7684\u654f\u6377\u56db\u8db3\u673a\u5668\u4eba\u8bbe\u8ba1\u3002\u8be5\u6846\u67b6\u96c6\u6210\u4e86\u7531\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u9a71\u52a8\u7684\u5206\u5c42\u89c4\u5212\u5668\uff0c\u7528\u4e8e\u957f\u8ddd\u79bb\u6307\u4ee4\u5206\u89e3\u548c\u7cbe\u786e\u52a8\u4f5c\u6267\u884c\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u5168\u8eab\u63a7\u5236\u7b56\u7565\uff0c\u7528\u4e8e\u5728\u590d\u6742\u5730\u5f62\u4e2d\u5b9e\u73b0\u9c81\u68d2\u7684\u534f\u8c03\u3002\u6b64\u5916\uff0c\u8fd8\u5efa\u7acb\u4e86\u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u957f\u671f\u79fb\u52a8\u64cd\u4f5c\u7684\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "ODYSSEY\u6846\u67b6\u901a\u8fc7\u6210\u529f\u7684sim-to-real\u8fc1\u79fb\uff0c\u5728\u771f\u5b9e\u4e16\u754c\u7684\u90e8\u7f72\u4e2d\u5c55\u793a\u4e86\u7cfb\u7edf\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u8bc1\u660e\u4e86\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\uff0c\u5e26\u673a\u68b0\u81c2\u7684\u817f\u5f0f\u673a\u5668\u4eba\u7684\u5b9e\u7528\u6027\u3002", "conclusion": "ODYSSEY\u6846\u67b6\u901a\u8fc7\u5c06\u9ad8\u7ea7\u4efb\u52a1\u89c4\u5212\u4e0e\u4f4e\u7ea7\u5168\u8eab\u63a7\u5236\u76f8\u7ed3\u5408\uff0c\u89e3\u51b3\u4e86\u8bed\u8a00\u5f15\u5bfc\u7684\u957f\u671f\u79fb\u52a8\u64cd\u4f5c\u7684\u6311\u6218\uff0c\u5e76\u6210\u529f\u8fdb\u884c\u4e86\u771f\u5b9e\u4e16\u754c\u90e8\u7f72\uff0c\u5c55\u793a\u4e86\u5176\u5728\u975e\u7ed3\u6784\u5316\u73af\u5883\u4e2d\u7684\u901a\u7528\u6027\u548c\u9c81\u68d2\u6027\uff0c\u63a8\u52a8\u4e86\u901a\u7528\u673a\u5668\u4eba\u52a9\u624b\u7684\u53ef\u884c\u6027\u3002"}}
{"id": "2508.08246", "categories": ["quant-ph", "cs.IT", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.08246", "abs": "https://arxiv.org/abs/2508.08246", "authors": ["Zhiyang He", "Quynh T. Nguyen", "Christopher A. Pattison"], "title": "Composable Quantum Fault-Tolerance", "comment": "Most sections stand alone. Reader's guide included", "summary": "Proving threshold theorems for fault-tolerant quantum computation is a\nburdensome endeavor with many moving parts that come together in relatively\nformulaic but lengthy ways. It is difficult and rare to combine elements from\nmultiple papers into a single formal threshold proof, due to the use of\ndifferent measures of fault-tolerance. In this work, we introduce composable\nfault-tolerance, a framework that decouples the probabilistic analysis of the\nnoise distribution from the combinatorial analysis of circuit correctness, and\nenables threshold proofs to compose independently analyzed gadgets easily and\nrigorously. Within this framework, we provide a library of standard and\ncommonly used gadgets such as memory and logic implemented by constant-depth\ncircuits for quantum low-density parity check codes and distillation. As sample\napplications, we explicitly write down a threshold proof for computation with\nsurface code and re-derive the constant space-overhead fault-tolerant scheme of\nGottesman using gadgets from this library. We expect that future\nfault-tolerance proofs may focus on the analysis of novel techniques while\nleaving the standard components to the composable fault-tolerance framework,\nwith the formal proof following the intuitive ``napkin math'' exactly.", "AI": {"tldr": "\u63d0\u51fa\u53ef\u7ec4\u5408\u5bb9\u9519\u6027\u6846\u67b6\uff0c\u7b80\u5316\u91cf\u5b50\u8ba1\u7b97\u9608\u503c\u8bc1\u660e\uff0c\u5e76\u63d0\u4f9b\u5e38\u7528\u5c0f\u5de5\u5177\u5e93\u3002", "motivation": "\u8bc1\u660e\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u9608\u503c\u5b9a\u7406\u662f\u4e00\u9879\u7e41\u91cd\u7684\u5de5\u4f5c\uff0c\u6d89\u53ca\u8bb8\u591a\u76f8\u4e92\u5173\u8054\u7684\u90e8\u5206\uff0c\u5e76\u4e14\u901a\u5e38\u9700\u8981\u5197\u957f\u7684\u516c\u5f0f\u63a8\u5bfc\u3002\u7531\u4e8e\u5bb9\u9519\u6027\u5ea6\u91cf\u5404\u4e0d\u76f8\u540c\uff0c\u5c06\u6765\u81ea\u591a\u4e2a\u8bba\u6587\u7684\u8981\u7d20\u7ec4\u5408\u6210\u5355\u4e2a\u6b63\u5f0f\u7684\u9608\u503c\u8bc1\u660e\u975e\u5e38\u56f0\u96be\u4e14\u7f55\u89c1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u53ef\u7ec4\u5408\u5bb9\u9519\u6027\u201d\u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u7b80\u5316\u5bb9\u9519\u91cf\u5b50\u8ba1\u7b97\u7684\u9608\u503c\u8bc1\u660e\u3002\u8be5\u6846\u67b6\u5c06\u566a\u58f0\u5206\u5e03\u7684\u6982\u7387\u5206\u6790\u4e0e\u7535\u8def\u6b63\u786e\u6027\u7684\u7ec4\u5408\u5206\u6790\u5206\u79bb\u5f00\u6765\uff0c\u4f7f\u5f97\u80fd\u591f\u72ec\u7acb\u5206\u6790\u7684\u5c0f\u5de5\u5177\u53ef\u4ee5\u8f7b\u677e\u7ec4\u5408\uff0c\u4ece\u800c\u8fdb\u884c\u4e25\u683c\u7684\u9608\u503c\u8bc1\u660e\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5305\u542b\u91cf\u5b50\u4f4e\u5bc6\u5ea6\u5947\u5076\u6821\u9a8c\u7801\u548c\u84b8\u998f\u7684\u6052\u5b9a\u6df1\u5ea6\u7535\u8def\u5b9e\u73b0\u7684\u6807\u51c6\u5e38\u7528\u5c0f\u5de5\u5177\u5e93\u3002\u4f5c\u4e3a\u793a\u4f8b\u5e94\u7528\uff0c\u660e\u786e\u7ed9\u51fa\u4e86\u8868\u9762\u7801\u8ba1\u7b97\u7684\u9608\u503c\u8bc1\u660e\uff0c\u5e76\u4f7f\u7528\u8be5\u5e93\u4e2d\u7684\u5c0f\u5de5\u5177\u91cd\u65b0\u63a8\u5bfc\u4e86Gottesman\u7684\u6052\u5b9a\u7a7a\u95f4\u5f00\u9500\u5bb9\u9519\u65b9\u6848\u3002", "conclusion": "\u5f15\u5165\u4e86\u53ef\u7ec4\u5408\u7684\u5bb9\u9519\u6027\u6846\u67b6\uff0c\u5c06\u566a\u58f0\u5206\u5e03\u7684\u6982\u7387\u5206\u6790\u4e0e\u7535\u8def\u6b63\u786e\u6027\u7684\u7ec4\u5408\u5206\u6790\u89e3\u8026\uff0c\u4f7f\u9608\u503c\u8bc1\u660e\u80fd\u591f\u8f7b\u677e\u4e25\u8c28\u5730\u7ec4\u5408\u72ec\u7acb\u5206\u6790\u7684\u5c0f\u5de5\u5177\u3002"}}
{"id": "2508.07630", "categories": ["cs.CL", "cs.AI", "cs.CV", "I.2.7; I.2.10; I.4.10; I.7.5"], "pdf": "https://arxiv.org/pdf/2508.07630", "abs": "https://arxiv.org/abs/2508.07630", "authors": ["Anirudh Iyengar Kaniyar Narayana Iyengar", "Srija Mukhopadhyay", "Adnan Qidwai", "Shubhankar Singh", "Dan Roth", "Vivek Gupta"], "title": "InterChart: Benchmarking Visual Reasoning Across Decomposed and Distributed Chart Information", "comment": "18 pages, 6 figures, 12 tables. Benchmark dataset and evaluation code\n  will be publicly made available", "summary": "We introduce InterChart, a diagnostic benchmark that evaluates how well\nvision-language models (VLMs) reason across multiple related charts, a task\ncentral to real-world applications such as scientific reporting, financial\nanalysis, and public policy dashboards. Unlike prior benchmarks focusing on\nisolated, visually uniform charts, InterChart challenges models with diverse\nquestion types ranging from entity inference and trend correlation to numerical\nestimation and abstract multi-step reasoning grounded in 2-3 thematically or\nstructurally related charts. We organize the benchmark into three tiers of\nincreasing difficulty: (1) factual reasoning over individual charts, (2)\nintegrative analysis across synthetically aligned chart sets, and (3) semantic\ninference over visually complex, real-world chart pairs. Our evaluation of\nstate-of-the-art open and closed-source VLMs reveals consistent and steep\naccuracy declines as chart complexity increases. We find that models perform\nbetter when we decompose multi-entity charts into simpler visual units,\nunderscoring their struggles with cross-chart integration. By exposing these\nsystematic limitations, InterChart provides a rigorous framework for advancing\nmultimodal reasoning in complex, multi-visual environments.", "AI": {"tldr": "InterChart \u662f\u4e00\u4e2a\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLMs) \u5728\u591a\u4e2a\u76f8\u5173\u56fe\u8868\u4e0a\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002\u5b83\u5305\u542b\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\u548c\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\u3002\u8bc4\u4f30\u663e\u793a\uff0c\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u3001\u591a\u56fe\u8868\u573a\u666f\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u51c6\u786e\u7387\u4f1a\u968f\u7740\u56fe\u8868\u590d\u6742\u6027\u7684\u589e\u52a0\u800c\u4e0b\u964d\u3002\u8be5\u57fa\u51c6\u65e8\u5728\u63a8\u52a8\u6a21\u578b\u5728\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u4e0a\u7684\u53d1\u5c55\u3002", "motivation": "\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u5e94\u7528\u4e2d\uff0c\u5982\u79d1\u5b66\u62a5\u544a\u3001\u91d1\u878d\u5206\u6790\u548c\u516c\u5171\u653f\u7b56\u4eea\u8868\u76d8\u7b49\uff0c\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLMs) \u9700\u8981\u80fd\u591f\u63a8\u7406\u591a\u4e2a\u76f8\u5173\u56fe\u8868\u3002\u7136\u800c\uff0c\u73b0\u6709\u7684\u57fa\u51c6\u4e3b\u8981\u5173\u6ce8\u5b64\u7acb\u7684\u3001\u89c6\u89c9\u4e0a\u5747\u5300\u7684\u56fe\u8868\uff0c\u65e0\u6cd5\u5145\u5206\u8bc4\u4f30\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u3001\u591a\u56fe\u8868\u573a\u666f\u4e2d\u7684\u80fd\u529b\u3002", "method": "\u8be5\u7814\u7a76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a InterChart \u7684\u8bca\u65ad\u57fa\u51c6\uff0c\u8be5\u57fa\u51c6\u4e13\u6ce8\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLMs) \u5728\u63a8\u7406\u8de8\u591a\u4e2a\u76f8\u5173\u56fe\u8868\u65b9\u9762\u7684\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5305\u542b\u4e00\u7cfb\u5217\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u65e8\u5728\u6d4b\u8bd5\u6a21\u578b\u5728\u5904\u7406\u591a\u4e2a\u56fe\u8868\u65f6\u7684\u7efc\u5408\u63a8\u7406\u80fd\u529b\uff0c\u5305\u62ec\u5b9e\u4f53\u63a8\u65ad\u3001\u8d8b\u52bf\u76f8\u5173\u6027\u3001\u6570\u503c\u4f30\u8ba1\u548c\u62bd\u8c61\u7684\u591a\u6b65\u63a8\u7406\u3002\u57fa\u51c6\u88ab\u7ec4\u7ec7\u6210\u4e09\u4e2a\u96be\u5ea6\u9012\u589e\u7684\u7ea7\u522b\uff1a(1) \u9488\u5bf9\u5355\u4e2a\u56fe\u8868\u7684\u4e8b\u5b9e\u63a8\u7406\uff1b(2) \u8de8\u5408\u6210\u5bf9\u9f50\u56fe\u8868\u96c6\u7684\u7efc\u5408\u5206\u6790\uff1b(3) \u9488\u5bf9\u89c6\u89c9\u590d\u6742\u3001\u771f\u5b9e\u4e16\u754c\u56fe\u8868\u5bf9\u7684\u8bed\u4e49\u63a8\u65ad\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u968f\u7740\u56fe\u8868\u590d\u6742\u6027\u7684\u589e\u52a0\uff0c\u6700\u5148\u8fdb\u7684\u5f00\u6e90\u548c\u95ed\u6e90 VLMs \u7684\u51c6\u786e\u7387\u4e00\u81f4\u4e14\u6025\u5267\u4e0b\u964d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5f53\u5c06\u591a\u5b9e\u4f53\u56fe\u8868\u5206\u89e3\u4e3a\u66f4\u7b80\u5355\u7684\u89c6\u89c9\u5355\u5143\u65f6\uff0c\u6a21\u578b\u8868\u73b0\u66f4\u597d\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u5728\u8de8\u56fe\u8868\u6574\u5408\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002InterChart \u901a\u8fc7\u63ed\u793a\u8fd9\u4e9b\u7cfb\u7edf\u6027\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u5728\u590d\u6742\u7684\u591a\u89c6\u89c9\u73af\u5883\u4e2d\u63a8\u8fdb\u591a\u6a21\u6001\u63a8\u7406\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u6846\u67b6\u3002", "conclusion": "InterChart \u662f\u4e00\u4e2a\u8bca\u65ad\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u89c6\u89c9\u8bed\u8a00\u6a21\u578b (VLMs) \u5728\u591a\u4e2a\u76f8\u5173\u56fe\u8868\u4e4b\u95f4\u7684\u63a8\u7406\u80fd\u529b\u3002\u8be5\u57fa\u51c6\u5305\u542b\u4e0d\u540c\u7c7b\u578b\u7684\u95ee\u9898\uff0c\u6db5\u76d6\u5b9e\u4f53\u63a8\u65ad\u3001\u8d8b\u52bf\u76f8\u5173\u6027\u3001\u6570\u503c\u4f30\u8ba1\u548c\u62bd\u8c61\u7684\u591a\u6b65\u63a8\u7406\u3002\u5b83\u5206\u4e3a\u4e09\u4e2a\u96be\u5ea6\u7ea7\u522b\uff1a(1) \u5bf9\u5355\u4e2a\u56fe\u8868\u7684\u4e8b\u5b9e\u63a8\u7406\uff0c(2) \u5728\u5408\u6210\u5bf9\u9f50\u56fe\u8868\u96c6\u4e2d\u7684\u7efc\u5408\u5206\u6790\uff0c(3) \u5728\u89c6\u89c9\u590d\u6742\u3001\u771f\u5b9e\u4e16\u754c\u56fe\u8868\u5bf9\u4e0a\u7684\u8bed\u4e49\u63a8\u65ad\u3002"}}
{"id": "2508.06964", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06964", "abs": "https://arxiv.org/abs/2508.06964", "authors": ["Qiwei Tian", "Chenhao Lin", "Zhengyu Zhao", "Qian Li", "Shuai Liu", "Chao Shen"], "title": "Adversarial Video Promotion Against Text-to-Video Retrieval", "comment": null, "summary": "Thanks to the development of cross-modal models, text-to-video retrieval\n(T2VR) is advancing rapidly, but its robustness remains largely unexamined.\nExisting attacks against T2VR are designed to push videos away from queries,\ni.e., suppressing the ranks of videos, while the attacks that pull videos\ntowards selected queries, i.e., promoting the ranks of videos, remain largely\nunexplored. These attacks can be more impactful as attackers may gain more\nviews/clicks for financial benefits and widespread (mis)information. To this\nend, we pioneer the first attack against T2VR to promote videos adversarially,\ndubbed the Video Promotion attack (ViPro). We further propose Modal Refinement\n(MoRe) to capture the finer-grained, intricate interaction between visual and\ntextual modalities to enhance black-box transferability. Comprehensive\nexperiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing\ndatasets with over 10k videos, evaluated under 3 scenarios. All experiments are\nconducted in a multi-target setting to reflect realistic scenarios where\nattackers seek to promote the video regarding multiple queries simultaneously.\nWe also evaluated our attacks for defences and imperceptibility. Overall, ViPro\nsurpasses other baselines by over $30/10/4\\%$ for white/grey/black-box settings\non average. Our work highlights an overlooked vulnerability, provides a\nqualitative analysis on the upper/lower bound of our attacks, and offers\ninsights into potential counterplays. Code will be publicly available at\nhttps://github.com/michaeltian108/ViPro.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86ViPro\u653b\u51fb\uff0c\u901a\u8fc7\u63d0\u5347\u89c6\u9891\u6392\u540d\u6765\u5229\u7528T2VR\u6a21\u578b\u7684\u6f0f\u6d1e\uff0c\u5e76\u4f7f\u7528MoRe\u589e\u5f3a\u4e86\u653b\u51fb\u7684\u53ef\u8fc1\u79fb\u6027\u3002\u8be5\u653b\u51fb\u5728\u591a\u79cd\u8bbe\u7f6e\u4e0b\u5747\u8868\u73b0\u4f18\u5f02\uff0c\u63ed\u793a\u4e86T2VR\u5728\u63a8\u5e7f\u653b\u51fb\u65b9\u9762\u5b58\u5728\u88ab\u5ffd\u89c6\u7684\u8106\u5f31\u6027\u3002", "motivation": "\u73b0\u6709\u9488\u5bf9\u6587\u672c\u5230\u89c6\u9891\u68c0\u7d22\uff08T2VR\uff09\u7684\u653b\u51fb\u4e3b\u8981\u96c6\u4e2d\u4e8e\u964d\u4f4e\u89c6\u9891\u6392\u540d\uff0c\u800c\u5ffd\u7565\u4e86\u63d0\u5347\u89c6\u9891\u6392\u540d\u7684\u653b\u51fb\u3002\u6b64\u7c7b\u653b\u51fb\uff08\u4f8b\u5982\uff0c\u4e3a\u4e86\u7ecf\u6d4e\u5229\u76ca\u6216\u4f20\u64ad\u9519\u8bef\u4fe1\u606f\u800c\u589e\u52a0\u89c6\u9891\u66dd\u5149\u5ea6\uff09\u53ef\u80fd\u66f4\u5177\u5f71\u54cd\u529b\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u5e76\u89e3\u51b3\u8fd9\u4e00\u88ab\u5ffd\u89c6\u7684\u653b\u51fb\u5411\u91cf\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u89c6\u9891\u63a8\u5e7f\u653b\u51fb\u201d\uff08ViPro\uff09\u7684\u65b0\u578b\u653b\u51fb\u65b9\u6cd5\uff0c\u65e8\u5728\u63d0\u5347\u89c6\u9891\u800c\u975e\u964d\u4f4e\u89c6\u9891\u6392\u540d\u3002\u540c\u65f6\uff0c\u63d0\u51fa\u4e86\u201c\u6a21\u6001\u7cbe\u70bc\u201d\uff08MoRe\uff09\u65b9\u6cd5\uff0c\u901a\u8fc7\u6355\u6349\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u4e4b\u95f4\u66f4\u7ec6\u7c92\u5ea6\u7684\u4ea4\u4e92\u6765\u589e\u5f3a\u9ed1\u76d2\u653b\u51fb\u7684\u53ef\u8fc1\u79fb\u6027\u3002", "result": "ViPro\u653b\u51fb\u5728\u767d\u76d2\u3001\u7070\u76d2\u548c\u9ed1\u76d2\u8bbe\u7f6e\u4e0b\u5e73\u5747\u5206\u522b\u6bd4\u5176\u4ed6\u57fa\u7ebf\u65b9\u6cd5\u63d0\u9ad8\u4e8630%\u300110%\u548c4%\u7684\u6027\u80fd\u3002\u5b9e\u9a8c\u57282\u4e2a\u57fa\u7ebf\u30013\u4e2a\u9886\u5148\u7684T2VR\u6a21\u578b\u30013\u4e2a\u6d41\u884c\u7684\u6570\u636e\u96c6\uff08\u8d85\u8fc710000\u4e2a\u89c6\u9891\uff09\u4ee5\u53ca3\u79cd\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u5e7f\u6cdb\u8bc4\u4f30\uff0c\u5747\u91c7\u7528\u591a\u76ee\u6807\u8bbe\u7f6e\u4ee5\u53cd\u6620\u771f\u5b9e\u653b\u51fb\u573a\u666f\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e86ViPro\u7684\u9632\u5fa1\u548c\u4e0d\u53ef\u611f\u77e5\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5f00\u521b\u6027\u5730\u63d0\u51fa\u4e86\u9488\u5bf9\u89c6\u9891\u68c0\u7d22\uff08T2VR\uff09\u7684\u201c\u89c6\u9891\u63a8\u5e7f\u653b\u51fb\u201d\uff08ViPro\uff09\uff0c\u4ee5\u63d0\u5347\u89c6\u9891\u6392\u540d\uff0c\u5e76\u5f15\u5165\u4e86\u201c\u6a21\u6001\u7cbe\u70bc\u201d\uff08MoRe\uff09\u65b9\u6cd5\u6765\u589e\u5f3a\u9ed1\u76d2\u653b\u51fb\u7684\u53ef\u8fc1\u79fb\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cViPro\u5728\u591a\u79cd\u573a\u666f\u548c\u6a21\u578b\u4e0b\u5747\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86T2VR\u6a21\u578b\u5728\u63a8\u5e7f\u653b\u51fb\u65b9\u9762\u5b58\u5728\u7684\u6f5c\u5728\u8106\u5f31\u6027\uff0c\u5e76\u4e3a\u9632\u5fa1\u7b56\u7565\u63d0\u4f9b\u4e86\u53c2\u8003\u3002"}}
{"id": "2508.08001", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08001", "abs": "https://arxiv.org/abs/2508.08001", "authors": ["Rui Yao", "Qi Chai", "Jinhai Yao", "Siyuan Li", "Junhao Chen", "Qi Zhang", "Hao Wang"], "title": "Interpreting Fedspeak with Confidence: A LLM-Based Uncertainty-Aware Framework Guided by Monetary Policy Transmission Paths", "comment": "Rui Yao, Qi Chai, and Jinhai Yao contributed equally to this work.\n  Corresponding authors: Qi Zhang (zhang.qi@sjtu.edu.cn) and Hao Wang\n  (haowang@hkust-gz.edu.cn)", "summary": "\"Fedspeak\", the stylized and often nuanced language used by the U.S. Federal\nReserve, encodes implicit policy signals and strategic stances. The Federal\nOpen Market Committee strategically employs Fedspeak as a communication tool to\nshape market expectations and influence both domestic and global economic\nconditions. As such, automatically parsing and interpreting Fedspeak presents a\nhigh-impact challenge, with significant implications for financial forecasting,\nalgorithmic trading, and data-driven policy analysis. In this paper, we propose\nan LLM-based, uncertainty-aware framework for deciphering Fedspeak and\nclassifying its underlying monetary policy stance. Technically, to enrich the\nsemantic and contextual representation of Fedspeak texts, we incorporate\ndomain-specific reasoning grounded in the monetary policy transmission\nmechanism. We further introduce a dynamic uncertainty decoding module to assess\nthe confidence of model predictions, thereby enhancing both classification\naccuracy and model reliability. Experimental results demonstrate that our\nframework achieves state-of-the-art performance on the policy stance analysis\ntask. Moreover, statistical analysis reveals a significant positive correlation\nbetween perceptual uncertainty and model error rates, validating the\neffectiveness of perceptual uncertainty as a diagnostic signal.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8d27\u5e01\u653f\u7b56\u77e5\u8bc6\u548c\u4e0d\u786e\u5b9a\u6027\u89e3\u7801\u6765\u5206\u6790\u7f8e\u8054\u50a8\u7684Fedspeak\uff0c\u4ee5\u786e\u5b9a\u5176\u8d27\u5e01\u653f\u7b56\u7acb\u573a\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\u3002", "motivation": "\u81ea\u52a8\u89e3\u6790\u548c\u89e3\u91caFedspeak\u5177\u6709\u9ad8\u5f71\u54cd\u529b\uff0c\u5bf9\u91d1\u878d\u9884\u6d4b\u3001\u7b97\u6cd5\u4ea4\u6613\u548c\u6570\u636e\u9a71\u52a8\u7684\u653f\u7b56\u5206\u6790\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u56e0\u4e3aFedspeak\u5305\u542b\u9690\u6027\u7684\u653f\u7b56\u4fe1\u53f7\u548c\u6218\u7565\u7acb\u573a\uff0c\u7f8e\u8054\u50a8\u4f7f\u7528\u5b83\u6765\u5851\u9020\u5e02\u573a\u9884\u671f\u5e76\u5f71\u54cd\u7ecf\u6d4e\u6761\u4ef6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u3001\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u8d27\u5e01\u653f\u7b56\u4f20\u5bfc\u673a\u5236\u7684\u9886\u57df\u7279\u5b9a\u63a8\u7406\u548c\u52a8\u6001\u4e0d\u786e\u5b9a\u6027\u89e3\u7801\u6a21\u5757\u6765\u89e3\u8bfbFedspeak\u5e76\u5bf9\u5176\u8d27\u5e01\u653f\u7b56\u7acb\u573a\u8fdb\u884c\u5206\u7c7b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u5728\u653f\u7b56\u7acb\u573a\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002\u7edf\u8ba1\u5206\u6790\u663e\u793a\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4e0e\u6a21\u578b\u9519\u8bef\u7387\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u7684\u6b63\u76f8\u5173\u5173\u7cfb\uff0c\u9a8c\u8bc1\u4e86\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u8bca\u65ad\u4fe1\u53f7\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728\u653f\u7b56\u7acb\u573a\u5206\u6790\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u611f\u77e5\u4e0d\u786e\u5b9a\u6027\u4f5c\u4e3a\u4e00\u79cd\u8bca\u65ad\u4fe1\u53f7\u88ab\u8bc1\u660e\u662f\u6709\u6548\u7684\u3002"}}
{"id": "2508.07137", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07137", "abs": "https://arxiv.org/abs/2508.07137", "authors": ["Yuandong Tan"], "title": "A Stable and Principled Loss Function for Direct Language Model Alignment", "comment": null, "summary": "The alignment of large language models (LLMs) with human preferences is\ncommonly achieved through Reinforcement Learning from Human Feedback (RLHF).\nDirect Preference Optimization (DPO) simplified this paradigm by establishing a\ndirect mapping between the optimal policy and a reward function, eliminating\nthe need for an explicit reward model. However, we argue that the DPO loss\nfunction is theoretically misaligned with its own derivation, as it promotes\nthe indefinite maximization of a logits difference, which can lead to training\ninstability and reward hacking. In this paper, we propose a novel loss function\nderived directly from the RLHF optimality condition. Our proposed loss targets\na specific, finite value for the logits difference, which is dictated by the\nunderlying reward, rather than its maximization. We provide a theoretical\nanalysis, including a gradient-based comparison, to demonstrate that our method\navoids the large gradients that plague DPO when the probability of dispreferred\nresponses approaches zero. This inherent stability prevents reward hacking and\nleads to more effective alignment. We validate our approach by fine-tuning a\nQwen2.5-7B model, showing significant win-rate improvements over a standard DPO\nbaseline and achieving competitive performance against larger models like\nLlama-3.1-8B.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u4ee5\u89e3\u51b3 DPO \u5728 LLM \u5bf9\u9f50\u4e2d\u7684\u7406\u8bba\u7f3a\u9677\u548c\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u6027\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bbe\u5b9a\u4e00\u4e2a\u6709\u9650\u7684\u76ee\u6807 logits \u5dee\u503c\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7a33\u5b9a\u6027\u548c\u5bf9\u9f50\u6548\u679c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e DPO \u548c\u53ef\u4e0e\u66f4\u5927\u6a21\u578b\u76f8\u5ab2\u7f8e\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5 DPO\uff08\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff09\u867d\u7136\u7b80\u5316\u4e86 RLHF \u6d41\u7a0b\uff0c\u4f46\u5176\u635f\u5931\u51fd\u6570\u5728\u7406\u8bba\u4e0a\u5b58\u5728\u95ee\u9898\uff0c\u53ef\u80fd\u5bfc\u81f4\u8bad\u7ec3\u4e0d\u7a33\u5b9a\u548c\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\u3002\u8fd9\u662f\u56e0\u4e3a DPO \u635f\u5931\u51fd\u6570\u9f13\u52b1\u65e0\u9650\u6700\u5927\u5316 logits \u5dee\u503c\uff0c\u8fd9\u53ef\u80fd\u504f\u79bb\u5176\u539f\u59cb\u63a8\u5bfc\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u76f4\u63a5\u4ece RLHF \u6700\u4f18\u6027\u6761\u4ef6\u63a8\u5bfc\u800c\u6765\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06 logits \u5dee\u503c\u76ee\u6807\u8bbe\u5b9a\u4e3a\u4e00\u4e2a\u7531\u6f5c\u5728\u5956\u52b1\u51b3\u5b9a\u7684\u7279\u5b9a\u6709\u9650\u503c\uff0c\u800c\u4e0d\u662f\u6700\u5927\u5316\u8be5\u5dee\u503c\uff0c\u6765\u89e3\u51b3 DPO \u635f\u5931\u51fd\u6570\u7406\u8bba\u4e0a\u7684\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\u3002\u901a\u8fc7\u7406\u8bba\u5206\u6790\u548c\u57fa\u4e8e\u68af\u5ea6\u7684\u6bd4\u8f83\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u53ef\u4ee5\u907f\u514d DPO \u5728\u4e0d\u671f\u671b\u54cd\u5e94\u6982\u7387\u63a5\u8fd1\u96f6\u65f6\u51fa\u73b0\u7684\u68af\u5ea6\u95ee\u9898\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u548c\u5bf9\u9f50\u6548\u679c\u3002", "result": "\u901a\u8fc7\u5bf9 Qwen2.5-7B \u6a21\u578b\u8fdb\u884c\u5fae\u8c03\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6\u7684 DPO \u57fa\u7ebf\u5728\u80dc\u7387\u4e0a\u6709\u4e86\u663e\u8457\u7684\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u4e0e Llama-3.1-8B \u7b49\u66f4\u5927\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83\u65f6\uff0c\u4e5f\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u635f\u5931\u51fd\u6570\u901a\u8fc7\u5c06 logits \u5dee\u503c\u76ee\u6807\u8bbe\u5b9a\u4e3a\u4e00\u4e2a\u7531\u6f5c\u5728\u5956\u52b1\u51b3\u5b9a\u7684\u7279\u5b9a\u6709\u9650\u503c\uff0c\u800c\u975e\u6700\u5927\u5316\u8be5\u5dee\u503c\uff0c\u4ece\u800c\u89e3\u51b3\u4e86 DPO \u7684\u7406\u8bba\u548c\u5b9e\u8df5\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u5728\u7406\u8bba\u4e0a\u907f\u514d\u4e86 DPO \u5728\u4e0d\u671f\u671b\u54cd\u5e94\u6982\u7387\u63a5\u8fd1\u96f6\u65f6\u51fa\u73b0\u7684\u68af\u5ea6\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u7684\u7a33\u5b9a\u6027\u5e76\u9632\u6b62\u4e86\u5956\u52b1\u9ed1\u5ba2\u884c\u4e3a\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u66f4\u6709\u6548\u7684\u5bf9\u9f50\u3002\u5728 Qwen2.5-7B \u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u76f8\u6bd4\u6807\u51c6 DPO \u6709\u663e\u8457\u7684\u80dc\u7387\u63d0\u5347\uff0c\u5e76\u4e14\u5728\u4e0e Llama-3.1-8B \u7b49\u66f4\u5927\u6a21\u578b\u76f8\u6bd4\u65f6\u4e5f\u8868\u73b0\u51fa\u7ade\u4e89\u529b\u3002"}}
{"id": "2508.08241", "categories": ["cs.RO"], "pdf": "https://arxiv.org/pdf/2508.08241", "abs": "https://arxiv.org/abs/2508.08241", "authors": ["Takara E. Truong", "Qiayuan Liao", "Xiaoyu Huang", "Guy Tevet", "C. Karen Liu", "Koushil Sreenath"], "title": "BeyondMimic: From Motion Tracking to Versatile Humanoid Control via Guided Diffusion", "comment": "9 pages, 1 figure", "summary": "Learning skills from human motions offers a promising path toward\ngeneralizable policies for whole-body humanoid control, yet two key\ncornerstones are missing: (1) a high-quality motion tracking framework that\nfaithfully transforms large-scale kinematic references into robust and\nextremely dynamic motions on real hardware, and (2) a distillation approach\nthat can effectively learn these motion primitives and compose them to solve\ndownstream tasks. We address these gaps with BeyondMimic, the first real-world\nframework to learn from human motions for versatile and naturalistic humanoid\ncontrol via guided diffusion. Our framework provides a motion tracking pipeline\ncapable of challenging skills such as jumping spins, sprinting, and cartwheels\nwith state-of-the-art motion quality. Moving beyond mimicking existing motions\nand synthesize novel ones, we further introduce a unified diffusion policy that\nenables zero-shot task-specific control at test time using simple cost\nfunctions. Deployed on hardware, BeyondMimic performs diverse tasks at test\ntime, including waypoint navigation, joystick teleoperation, and obstacle\navoidance, bridging sim-to-real motion tracking and flexible synthesis of human\nmotion primitives for whole-body control. https://beyondmimic.github.io/.", "AI": {"tldr": "BeyondMimic is a novel framework for humanoid robots that learns complex skills from human motion using guided diffusion. It excels at tracking dynamic movements and synthesizing new motions, enabling robots to perform tasks like navigation and obstacle avoidance without prior task-specific training.", "motivation": "Learning skills from human motions for generalizable policies in whole-body humanoid control. Key challenges include a high-quality motion tracking framework for dynamic motions and a distillation approach for learning and composing motion primitives.", "method": "BeyondMimic framework utilizes guided diffusion for learning human motion primitives. It includes a motion tracking pipeline for dynamic motions and a unified diffusion policy for zero-shot task-specific control.", "result": "State-of-the-art motion quality for challenging skills like jumping spins, sprinting, and cartwheels. Diverse task performance at test time, including waypoint navigation, joystick teleoperation, and obstacle avoidance.", "conclusion": "BeyondMimic is the first real-world framework to learn from human motions for versatile and naturalistic humanoid control via guided diffusion. It provides a motion tracking pipeline capable of challenging skills and a unified diffusion policy that enables zero-shot task-specific control using simple cost functions. Deployed on hardware, it performs diverse tasks like waypoint navigation, joystick teleoperation, and obstacle avoidance, bridging sim-to-real motion tracking and flexible synthesis of human motion primitives for whole-body control."}}
{"id": "2508.07690", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07690", "abs": "https://arxiv.org/abs/2508.07690", "authors": ["Luyao Zhuang", "Qinggang Zhang", "Huachi Zhou", "Juhua Liu", "Qing Li", "Xiao Huang"], "title": "LoSemB: Logic-Guided Semantic Bridging for Inductive Tool Retrieval", "comment": null, "summary": "Tool learning has emerged as a promising paradigm for large language models\n(LLMs) to solve many real-world tasks. Nonetheless, with the tool repository\nrapidly expanding, it is impractical to contain all tools within the limited\ninput length of LLMs. To alleviate these issues, researchers have explored\nincorporating a tool retrieval module to select the most relevant tools or\nrepresent tools as unique tokens within LLM parameters. However, most\nstate-of-the-art methods are under transductive settings, assuming all tools\nhave been observed during training. Such a setting deviates from reality as the\nreal-world tool repository is evolving and incorporates new tools frequently.\nWhen dealing with these unseen tools, which refer to tools not encountered\nduring the training phase, these methods are limited by two key issues,\nincluding the large distribution shift and the vulnerability of\nsimilarity-based retrieval. To this end, inspired by human cognitive processes\nof mastering unseen tools through discovering and applying the logical\ninformation from prior experience, we introduce a novel Logic-Guided Semantic\nBridging framework for inductive tool retrieval, namely, LoSemB, which aims to\nmine and transfer latent logical information for inductive tool retrieval\nwithout costly retraining. Specifically, LoSemB contains a logic-based\nembedding alignment module to mitigate distribution shifts and implements a\nrelational augmented retrieval mechanism to reduce the vulnerability of\nsimilarity-based retrieval. Extensive experiments demonstrate that LoSemB\nachieves advanced performance in inductive settings while maintaining desirable\neffectiveness in the transductive setting.", "AI": {"tldr": "LoSemB\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u6316\u6398\u548c\u8f6c\u79fb\u6f5c\u5728\u7684\u903b\u8f91\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u5728\u9762\u5bf9\u4e0d\u65ad\u589e\u957f\u7684\u5de5\u5177\u5e93\u65f6\uff0c\u5982\u4f55\u6709\u6548\u68c0\u7d22\u548c\u4f7f\u7528\u672a\u89c1\u5de5\u5177\u7684\u6311\u6218\uff0c\u5c24\u5176\u662f\u5728\u5f52\u7eb3\u5b66\u4e60\u7684\u573a\u666f\u4e0b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5de5\u5177\u5e93\u4e0d\u65ad\u6269\u5927\u800cLLMs\u8f93\u5165\u957f\u5ea6\u6709\u9650\u7684\u95ee\u9898\uff0c\u540c\u65f6\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u672a\u89c1\u5de5\u5177\u65f6\u5b58\u5728\u7684\u5206\u5e03\u53d8\u5316\u548c\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u8106\u5f31\u6027\u95ee\u9898\u3002", "method": "LoSemB\u6846\u67b6\u5305\u542b\u4e00\u4e2a\u57fa\u4e8e\u903b\u8f91\u7684\u5d4c\u5165\u5bf9\u9f50\u6a21\u5757\u6765\u51cf\u8f7b\u5206\u5e03\u53d8\u5316\uff0c\u5e76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u5173\u7cfb\u589e\u5f3a\u68c0\u7d22\u673a\u5236\u6765\u51cf\u5c11\u57fa\u4e8e\u76f8\u4f3c\u6027\u7684\u68c0\u7d22\u7684\u8106\u5f31\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660eLoSemB\u5728\u5f52\u7eb3\u8bbe\u7f6e\u4e0b\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8f6c\u6362\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u4e86\u826f\u597d\u7684\u6709\u6548\u6027\u3002", "conclusion": "LoSemB\u6846\u67b6\u5728\u5f52\u7eb3\u8bbe\u7f6e\u4e0b\u5b9e\u73b0\u4e86\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u8f6c\u6362\u8bbe\u7f6e\u4e0b\u4fdd\u6301\u4e86\u7406\u60f3\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.08007", "categories": ["cs.AI", "Computing methodologies~Description logics, Computing\n  methodologies~Ontology engineering"], "pdf": "https://arxiv.org/pdf/2508.08007", "abs": "https://arxiv.org/abs/2508.08007", "authors": ["Maurice Funk", "Marvin Grosser", "Carsten Lutz"], "title": "Fitting Description Logic Ontologies to ABox and Query Examples", "comment": "Submitted to the 22nd International Conference on Principles of\n  Knowledge Representation and Reasoning (KR2025), 23 pages", "summary": "We study a fitting problem inspired by ontology-mediated querying: given a\ncollection\n  of positive and negative examples of\n  the form $(\\mathcal{A},q)$ with\n  $\\mathcal{A}$ an ABox and $q$ a Boolean query, we seek\n  an ontology $\\mathcal{O}$ that satisfies $\\mathcal{A} \\cup \\mathcal{O} \\vDash\nq$ for all positive examples and $\\mathcal{A} \\cup \\mathcal{O}\\not\\vDash q$ for\nall negative examples.\n  We consider the description logics $\\mathcal{ALC}$ and $\\mathcal{ALCI}$ as\nontology languages and\n  a range of query languages that\n  includes atomic queries (AQs), conjunctive queries (CQs), and unions thereof\n(UCQs).\n  For all of the resulting fitting problems,\n  we provide\n  effective characterizations and determine the computational complexity\n  of deciding whether a fitting ontology exists. This problem turns out to be\n${\\small CO}NP$ for AQs and full CQs\n  and $2E{\\small XP}T{\\small IME}$-complete for CQs and UCQs.\n  These results hold for both $\\mathcal{ALC}$ and $\\mathcal{ALCI}$.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86\u672c\u4f53\u4e2d\u4ecb\u67e5\u8be2\u4e2d\u7684\u62df\u5408\u95ee\u9898\uff0c\u5728$\\\\mathcal{ALC}}$\u548c$\\\\mathcal{ALCI}}}$\u672c\u4f53\u8bed\u8a00\u4ee5\u53caAQs\u3001CQs\u3001UCQs\u67e5\u8be2\u8bed\u8a00\u4e0b\uff0c\u7ed9\u51fa\u4e86\u62df\u5408\u95ee\u9898\u7684\u6709\u6548\u7279\u5f81\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "motivation": "\u53d7\u5230\u672c\u4f53\u4e2d\u4ecb\u67e5\u8be2\u7684\u542f\u53d1\uff0c\u7814\u7a76\u4e86\u4e00\u4e2a\u62df\u5408\u95ee\u9898\uff1a\u7ed9\u5b9a\u4e00\u7cfb\u5217\u5f62\u5f0f\u4e3a$(\\\\mathcal{A},q)$\u7684\u6b63\u9762\u548c\u8d1f\u9762\u4f8b\u5b50\uff0c\u5176\u4e2d$\\\\mathcal{A}$\u662fABox\uff0c$q$\u662f\u5e03\u5c14\u67e5\u8be2\uff0c\u5bfb\u627e\u4e00\u4e2a\u672c\u4f53$\\\\mathcal{O}$\uff0c\u4f7f\u5f97\u5bf9\u4e8e\u6240\u6709\u6b63\u9762\u4f8b\u5b50\uff0c$\\\\mathcal{A} \\\\cup \\\\mathcal{O} \\\\vDash q$\uff0c\u5bf9\u4e8e\u6240\u6709\u8d1f\u9762\u4f8b\u5b50\uff0c$\\\\mathcal{A} \\\\cup \\\\mathcal{O} \\\\not\\\\vDash q$", "method": "\u7814\u7a76\u4e86\u672c\u4f53$\\\\mathcal{ALC}$\u548c$\\\\mathcal{ALCI}$\u4ee5\u53ca\u539f\u5b50\u67e5\u8be2\uff08AQs\uff09\u3001\u8054\u63a5\u67e5\u8be2\uff08CQs\uff09\u53ca\u5176\u5e76\u96c6\uff08UCQs\uff09\u7684\u62df\u5408\u95ee\u9898\uff0c\u5e76\u7ed9\u51fa\u4e86\u6709\u6548\u7684\u7279\u5f81\u63cf\u8ff0\u548c\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "result": "\u4e3a\u62df\u5408\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u7279\u5f81\u63cf\u8ff0\uff0c\u5e76\u786e\u5b9a\u4e86\u5224\u5b9a\u662f\u5426\u5b58\u5728\u62df\u5408\u672c\u4f53\u7684\u8ba1\u7b97\u590d\u6742\u5ea6\u3002", "conclusion": "\u8be5\u95ee\u9898\u5bf9\u4e8e\u539f\u5b50\u67e5\u8be2\uff08AQs\uff09\u548c\u5168\u8054\u63a5\u67e5\u8be2\uff08CQs\uff09\u4e3a${\\small CO}NP$ -complete\uff0c\u5bf9\u4e8e\u8054\u63a5\u67e5\u8be2\uff08CQs\uff09\u548c\u5176\u5e76\u96c6\uff08UCQs\uff09\u4e3a$2E{\\small XP}T{\\small IME}$-complete\u3002\u8fd9\u4e9b\u7ed3\u679c\u540c\u65f6\u9002\u7528\u4e8e$\\\\mathcal{ALC}$\u548c$\\\\mathcal{ALCI}$\u3002"}}
{"id": "2508.07702", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07702", "abs": "https://arxiv.org/abs/2508.07702", "authors": ["Charlie Wyatt", "Aditya Joshi", "Flora Salim"], "title": "What am I missing here?: Evaluating Large Language Models for Masked Sentence Prediction", "comment": "Under Review", "summary": "Transformer-based models primarily rely on Next Token Prediction (NTP), which\npredicts the next token in a sequence based on the preceding context. However,\nNTP's focus on single-token prediction often limits a model's ability to plan\nahead or maintain long-range coherence, raising questions about how well LLMs\ncan predict longer contexts, such as full sentences within structured\ndocuments. While NTP encourages local fluency, it provides no explicit\nincentive to ensure global coherence across sentence boundaries-an essential\nskill for reconstructive or discursive tasks. To investigate this, we evaluate\nthree commercial LLMs (GPT-4o, Claude 3.5 Sonnet, and Gemini 2.0 Flash) on\nMasked Sentence Prediction (MSP) - the task of infilling a randomly removed\nsentence - from three domains: ROCStories (narrative), Recipe1M (procedural),\nand Wikipedia (expository). We assess both fidelity (similarity to the original\nsentence) and cohesiveness (fit within the surrounding context). Our key\nfinding reveals that commercial LLMs, despite their superlative performance in\nother tasks, are poor at predicting masked sentences in low-structured domains,\nhighlighting a gap in current model capabilities.", "AI": {"tldr": "\u7814\u7a76\u663e\u793a\uff0c\u5c3d\u7ba1LLM\u5728\u8bb8\u591a\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5728\u9884\u6d4b\u88ab\u79fb\u9664\u7684\u53e5\u5b50\uff08MSP\u4efb\u52a1\uff09\u65f6\uff0c\u5c24\u5176\u662f\u5728\u6545\u4e8b\u548c\u98df\u8c31\u7b49\u975e\u7ed3\u6784\u5316\u6587\u672c\u4e2d\uff0c\u8868\u73b0\u4e0d\u4f73\uff0c\u8fd9\u66b4\u9732\u4e86\u5b83\u4eec\u5728\u957f\u6587\u672c\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u6f5c\u5728\u5f31\u70b9\u3002", "motivation": "\u5c3d\u7ba1\u57fa\u4e8eTransformer\u7684\u6a21\u578b\u5728\u4e0b\u4e00\u8bcd\u9884\u6d4b\uff08NTP\uff09\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u4f46NTP\u4fa7\u91cd\u4e8e\u5355\u8bcd\u7ea7\u522b\u7684\u9884\u6d4b\uff0c\u53ef\u80fd\u9650\u5236\u4e86\u6a21\u578b\u5728\u957f\u8ddd\u79bb\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u80fd\u529b\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LLM\u5728\u9884\u6d4b\u66f4\u957f\u6587\u672c\uff08\u5982\u7ed3\u6784\u5316\u6587\u6863\u4e2d\u7684\u5b8c\u6574\u53e5\u5b50\uff09\u65b9\u9762\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u8bc4\u4f30\u5b83\u4eec\u5728\u53e5\u5b50\u7ea7\u522b\u7684\u9884\u6d4b\u548c\u4fdd\u6301\u5168\u5c40\u8fde\u8d2f\u6027\u65b9\u9762\u7684\u8868\u73b0\uff0c\u8fd9\u5bf9\u4e8e\u9700\u8981\u91cd\u6784\u6216\u8bba\u8ff0\u7684\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u5546\u7528LLM\uff08GPT-4o\u3001Claude 3.5 Sonnet\u548cGemini 2.0 Flash\uff09\u5728\u63a9\u7801\u53e5\u5b50\u9884\u6d4b\uff08MSP\uff09\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u8be5\u4efb\u52a1\u65e8\u5728\u586b\u5145\u4ece\u4e09\u4e2a\u9886\u57df\uff08ROCStories\u3001Recipe1M\u548cWikipedia\uff09\u4e2d\u968f\u673a\u79fb\u9664\u7684\u53e5\u5b50\u3002\u7814\u7a76\u8bc4\u4f30\u4e86\u6a21\u578b\u9884\u6d4b\u7684\u53e5\u5b50\u7684\u4fdd\u771f\u5ea6\uff08\u4e0e\u539f\u59cb\u53e5\u5b50\u7684\u76f8\u4f3c\u6027\uff09\u548c\u5185\u805a\u6027\uff08\u4e0e\u5468\u56f4\u4e0a\u4e0b\u6587\u7684\u5951\u5408\u5ea6\uff09\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u4f4e\u7ed3\u6784\u5316\u9886\u57df\uff08\u5982ROCStories\u548cRecipe1M\uff09\uff0c\u5546\u4e1aLLM\u5728MSP\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u8868\u660e\u5b83\u4eec\u5728\u4fdd\u6301\u53e5\u5b50\u95f4\u7684\u8fde\u8d2f\u6027\u548c\u51c6\u786e\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u73b0\u6709\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u63a9\u7801\u53e5\u5b50\u9884\u6d4b\uff08MSP\uff09\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u4f4e\u7ed3\u6784\u5316\u9886\u57df\uff0c\u8fd9\u8868\u660e\u5b83\u4eec\u5728\u751f\u6210\u8fde\u8d2f\u7684\u957f\u6587\u672c\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002"}}
{"id": "2508.06982", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.06982", "abs": "https://arxiv.org/abs/2508.06982", "authors": ["Yixin Zhu", "Zuoliang Zhu", "Milo\u0161 Ha\u0161an", "Jian Yang", "Jin Xie", "Beibei Wang"], "title": "WeatherDiffusion: Weather-Guided Diffusion Model for Forward and Inverse Rendering", "comment": null, "summary": "Forward and inverse rendering have emerged as key techniques for enabling\nunderstanding and reconstruction in the context of autonomous driving (AD).\nHowever, complex weather and illumination pose great challenges to this task.\nThe emergence of large diffusion models has shown promise in achieving\nreasonable results through learning from 2D priors, but these models are\ndifficult to control and lack robustness. In this paper, we introduce\nWeatherDiffusion, a diffusion-based framework for forward and inverse rendering\non AD scenes with various weather and lighting conditions. Our method enables\nauthentic estimation of material properties, scene geometry, and lighting, and\nfurther supports controllable weather and illumination editing through the use\nof predicted intrinsic maps guided by text descriptions. We observe that\ndifferent intrinsic maps should correspond to different regions of the original\nimage. Based on this observation, we propose Intrinsic map-aware attention\n(MAA) to enable high-quality inverse rendering. Additionally, we introduce a\nsynthetic dataset (\\ie WeatherSynthetic) and a real-world dataset (\\ie\nWeatherReal) for forward and inverse rendering on AD scenes with diverse\nweather and lighting. Extensive experiments show that our WeatherDiffusion\noutperforms state-of-the-art methods on several benchmarks. Moreover, our\nmethod demonstrates significant value in downstream tasks for AD, enhancing the\nrobustness of object detection and image segmentation in challenging weather\nscenarios.", "AI": {"tldr": "WeatherDiffusion\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u6269\u6563\u6a21\u578b\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u81ea\u52a8\u9a7e\u9a76\u573a\u666f\u4e2d\u590d\u6742\u5929\u6c14\u548c\u5149\u7167\u5e26\u6765\u7684\u6e32\u67d3\u6311\u6218\uff0c\u5e76\u63d0\u5347\u4e86\u76f8\u5173\u4e0b\u6e38\u4efb\u52a1\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u590d\u6742\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u5bf9\u81ea\u52a8\u9a7e\u9a76\uff08AD\uff09\u573a\u666f\u4e2d\u6b63\u5411\u548c\u9006\u5411\u6e32\u67d3\u4efb\u52a1\u7684\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u6269\u6563\u6a21\u578b\u96be\u4ee5\u63a7\u5236\u548c\u7f3a\u4e4f\u9c81\u68d2\u6027\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeatherDiffusion\u7684\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4f7f\u7528\u9884\u6d4b\u7684\u672c\u5f81\u56fe\u5e76\u53d7\u6587\u672c\u63cf\u8ff0\u7684\u6307\u5bfc\uff0c\u5b9e\u73b0\u4e86\u6750\u6599\u5c5e\u6027\u3001\u573a\u666f\u51e0\u4f55\u548c\u5149\u7167\u7684\u771f\u5b9e\u611f\u4f30\u8ba1\uff0c\u5e76\u652f\u6301\u53ef\u63a7\u7684\u5929\u6c14\u548c\u5149\u7167\u7f16\u8f91\u3002\u5f15\u5165\u4e86\u5185\u5728\u56fe\u611f\u77e5\u6ce8\u610f\u529b\uff08MAA\uff09\u673a\u5236\u6765\u63d0\u9ad8\u9006\u5411\u6e32\u67d3\u7684\u8d28\u91cf\u3002", "result": "WeatherDiffusion\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u5728AD\u7684\u4e0b\u6e38\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u663e\u8457\u4ef7\u503c\uff0c\u5c24\u5176\u662f\u5728\u589e\u5f3a\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\u7269\u4f53\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u5272\u7684\u9c81\u68d2\u6027\u65b9\u9762\u3002", "conclusion": "WeatherDiffusion\u5728\u5177\u6709\u5404\u79cd\u5929\u6c14\u548c\u5149\u7167\u6761\u4ef6\u7684AD\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u771f\u5b9e\u611f\u7684\u53ef\u63a7\u5929\u6c14\u548c\u5149\u7167\u7f16\u8f91\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u589e\u5f3a\u4e86\u7269\u4f53\u68c0\u6d4b\u548c\u56fe\u50cf\u5206\u5272\u7684\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.08053", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08053", "abs": "https://arxiv.org/abs/2508.08053", "authors": ["Runchuan Zhu", "Bowen Jiang", "Lingrui Mei", "Fangkai Yang", "Lu Wang", "Haoxiang Gao", "Fengshuo Bai", "Pu Zhao", "Qingwei Lin", "Saravan Rajmohan", "Dongmei Zhang"], "title": "AdaptFlow: Adaptive Workflow Optimization via Meta-Learning", "comment": null, "summary": "Recent advances in large language models (LLMs) have sparked growing interest\nin agentic workflows, which are structured sequences of LLM invocations\nintended to solve complex tasks. However, existing approaches often rely on\nstatic templates or manually designed workflows, which limit adaptability to\ndiverse tasks and hinder scalability. We propose AdaptFlow, a natural\nlanguage-based meta-learning framework inspired by model-agnostic meta-learning\n(MAML). AdaptFlow learns a generalizable workflow initialization that enables\nrapid subtask-level adaptation. It employs a bi-level optimization scheme: the\ninner loop refines the workflow for a specific subtask using LLM-generated\nfeedback, while the outer loop updates the shared initialization to perform\nwell across tasks. This setup allows AdaptFlow to generalize effectively to\nunseen tasks by adapting the initialized workflow through language-guided\nmodifications. Evaluated across question answering, code generation, and\nmathematical reasoning benchmarks, AdaptFlow consistently outperforms both\nmanually crafted and automatically searched baselines, achieving\nstate-of-the-art results with strong generalization across tasks and models.\nThe source code and data are available at\nhttps://github.com/microsoft/DKI_LLM/tree/AdaptFlow/AdaptFlow.", "AI": {"tldr": "AdaptFlow\u662f\u4e00\u4e2a\u65b0\u6846\u67b6\uff0c\u901a\u8fc7\u5143\u5b66\u4e60\u4f18\u5316LLM\u5de5\u4f5c\u6d41\uff0c\u4ee5\u9002\u5e94\u5404\u79cd\u4efb\u52a1\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u9886\u5148\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684agentic workflows\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u9759\u6001\u6a21\u677f\u6216\u624b\u52a8\u8bbe\u8ba1\u7684\u5de5\u4f5c\u6d41\uff0c\u8fd9\u9650\u5236\u4e86\u5bf9\u4e0d\u540c\u4efb\u52a1\u7684\u9002\u5e94\u6027\u5e76\u963b\u788d\u4e86\u53ef\u6269\u5c55\u6027\u3002", "method": "AdaptFlow\u662f\u4e00\u4e2a\u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u7684\u5143\u5b66\u4e60\u6846\u67b6\uff0c\u53d7\u5230\u6a21\u578b\u65e0\u5173\u5143\u5b66\u4e60\uff08MAML\uff09\u7684\u542f\u53d1\u3002\u5b83\u91c7\u7528\u53cc\u5c42\u4f18\u5316\u65b9\u6848\uff1a\u5185\u5faa\u73af\u5229\u7528LLM\u751f\u6210\u7684\u53cd\u9988\u4e3a\u7279\u5b9a\u5b50\u4efb\u52a1\u4f18\u5316\u5de5\u4f5c\u6d41\uff0c\u5916\u5faa\u73af\u66f4\u65b0\u5171\u4eab\u521d\u59cb\u5316\u4ee5\u8de8\u4efb\u52a1\u6267\u884c\u3002", "result": "AdaptFlow\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684\u548c\u81ea\u52a8\u641c\u7d22\u7684\u57fa\u7ebf\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u5e76\u5177\u6709\u5f3a\u5927\u7684\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "AdaptFlow\u5728\u95ee\u7b54\u3001\u4ee3\u7801\u751f\u6210\u548c\u6570\u5b66\u63a8\u7406\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u59cb\u7ec8\u4f18\u4e8e\u624b\u52a8\u8bbe\u8ba1\u7684\u548c\u81ea\u52a8\u641c\u7d22\u7684\u57fa\u7ebf\uff0c\u5728\u8de8\u4efb\u52a1\u548c\u8de8\u6a21\u578b\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07753", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07753", "abs": "https://arxiv.org/abs/2508.07753", "authors": ["Zhenliang Zhang", "Junzhe Zhang", "Xinyu Hu", "HuiXuan Zhang", "Xiaojun Wan"], "title": "Exploring Causal Effect of Social Bias on Faithfulness Hallucinations in Large Language Models", "comment": "Accepted by CIKM 2025 (Full Paper)", "summary": "Large language models (LLMs) have achieved remarkable success in various\ntasks, yet they remain vulnerable to faithfulness hallucinations, where the\noutput does not align with the input. In this study, we investigate whether\nsocial bias contributes to these hallucinations, a causal relationship that has\nnot been explored. A key challenge is controlling confounders within the\ncontext, which complicates the isolation of causality between bias states and\nhallucinations. To address this, we utilize the Structural Causal Model (SCM)\nto establish and validate the causality and design bias interventions to\ncontrol confounders. In addition, we develop the Bias Intervention Dataset\n(BID), which includes various social biases, enabling precise measurement of\ncausal effects. Experiments on mainstream LLMs reveal that biases are\nsignificant causes of faithfulness hallucinations, and the effect of each bias\nstate differs in direction. We further analyze the scope of these causal\neffects across various models, specifically focusing on unfairness\nhallucinations, which are primarily targeted by social bias, revealing the\nsubtle yet significant causal effect of bias on hallucination generation.", "AI": {"tldr": "\u7814\u7a76\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\u548c\u504f\u89c1\u5e72\u9884\u6570\u636e\u96c6\uff08BID\uff09\uff0c\u8bc1\u660e\u793e\u4f1a\u504f\u89c1\u662f\u5bfc\u81f4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ea7\u751f\u5fe0\u5b9e\u6027\u5e7b\u89c9\uff08\u7279\u522b\u662f\u201c\u4e0d\u516c\u5e73\u5e7b\u89c9\u201d\uff09\u7684\u91cd\u8981\u539f\u56e0\uff0c\u4e14\u4e0d\u540c\u504f\u89c1\u7684\u5f71\u54cd\u65b9\u5411\u4e0d\u540c\u3002", "motivation": "\u65e8\u5728\u63a2\u7a76\u793e\u4f1a\u504f\u89c1\u662f\u5426\u4f1a\u5bfc\u81f4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ea7\u751f\u5fe0\u5b9e\u6027\u5e7b\u89c9\uff0c\u4ee5\u53ca\u8fd9\u79cd\u56e0\u679c\u5173\u7cfb\u662f\u5426\u4ece\u672a\u88ab\u63a2\u7d22\u8fc7\u3002", "method": "\u5229\u7528\u7ed3\u6784\u56e0\u679c\u6a21\u578b\uff08SCM\uff09\u5efa\u7acb\u548c\u9a8c\u8bc1\u56e0\u679c\u5173\u7cfb\uff0c\u5e76\u901a\u8fc7\u8bbe\u8ba1\u504f\u89c1\u5e72\u9884\u63aa\u65bd\u6765\u63a7\u5236\u6df7\u6dc6\u53d8\u91cf\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8005\u8fd8\u5f00\u53d1\u4e86\u5305\u542b\u5404\u79cd\u793e\u4f1a\u504f\u89c1\u7684\u504f\u89c1\u5e72\u9884\u6570\u636e\u96c6\uff08BID\uff09\uff0c\u4ee5\u4fbf\u7cbe\u786e\u6d4b\u91cf\u56e0\u679c\u6548\u5e94\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u5b9e\uff0c\u504f\u89c1\u662f\u5bfc\u81f4\u5fe0\u5b9e\u6027\u5e7b\u89c9\u7684\u91cd\u8981\u539f\u56e0\uff0c\u5e76\u4e14\u4e0d\u540c\u504f\u89c1\u72b6\u6001\u7684\u5f71\u54cd\u65b9\u5411\u5404\u4e0d\u76f8\u540c\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u8fd9\u4e9b\u56e0\u679c\u6548\u5e94\u5728\u4e0d\u540c\u6a21\u578b\u4e2d\u7684\u8303\u56f4\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e3b\u8981\u7531\u793e\u4f1a\u504f\u89c1\u5f15\u8d77\u7684\u201c\u4e0d\u516c\u5e73\u5e7b\u89c9\u201d\uff0c\u63ed\u793a\u4e86\u504f\u89c1\u5bf9\u5e7b\u89c9\u4ea7\u751f\u7684\u5fae\u5999\u4f46\u663e\u8457\u7684\u56e0\u679c\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u793e\u4f1a\u504f\u89c1\u662f\u5bfc\u81f4\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ea7\u751f\u5fe0\u5b9e\u6027\u5e7b\u89c9\u7684\u91cd\u8981\u539f\u56e0\uff0c\u5e76\u4e14\u4e0d\u540c\u504f\u89c1\u72b6\u6001\u7684\u5f71\u54cd\u65b9\u5411\u548c\u7a0b\u5ea6\u5404\u4e0d\u76f8\u540c\u3002"}}
{"id": "2508.06988", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06988", "abs": "https://arxiv.org/abs/2508.06988", "authors": ["Fangmin Zhao", "Weichao Zeng", "Zhenhang Li", "Dongbao Yang", "Yu Zhou"], "title": "TADoc: Robust Time-Aware Document Image Dewarping", "comment": "8 pages, 8 figures", "summary": "Flattening curved, wrinkled, and rotated document images captured by portable\nphotographing devices, termed document image dewarping, has become an\nincreasingly important task with the rise of digital economy and online\nworking. Although many methods have been proposed recently, they often struggle\nto achieve satisfactory results when confronted with intricate document\nstructures and higher degrees of deformation in real-world scenarios. Our main\ninsight is that, unlike other document restoration tasks (e.g., deblurring),\ndewarping in real physical scenes is a progressive motion rather than a\none-step transformation. Based on this, we have undertaken two key initiatives.\nFirstly, we reformulate this task, modeling it for the first time as a dynamic\nprocess that encompasses a series of intermediate states. Secondly, we design a\nlightweight framework called TADoc (Time-Aware Document Dewarping Network) to\naddress the geometric distortion of document images. In addition, due to the\ninadequacy of OCR metrics for document images containing sparse text, the\ncomprehensiveness of evaluation is insufficient. To address this shortcoming,\nwe propose a new metric -- DLS (Document Layout Similarity) -- to evaluate the\neffectiveness of document dewarping in downstream tasks. Extensive experiments\nand in-depth evaluations have been conducted and the results indicate that our\nmodel possesses strong robustness, achieving superiority on several benchmarks\nwith different document types and degrees of distortion.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6587\u6863\u53bb\u626d\u66f2\u65b9\u6cd5TADoc\uff0c\u901a\u8fc7\u5c06\u4efb\u52a1\u5efa\u6a21\u4e3a\u52a8\u6001\u8fc7\u7a0b\u548c\u5f15\u5165\u65b0\u7684\u8bc4\u4f30\u6307\u6807DLS\uff0c\u63d0\u9ad8\u4e86\u5904\u7406\u590d\u6742\u6587\u6863\u53d8\u5f62\u7684\u80fd\u529b\u3002", "motivation": "\u968f\u7740\u6570\u5b57\u7ecf\u6d4e\u548c\u5728\u7ebf\u5de5\u4f5c\u5174\u8d77\uff0c\u5904\u7406\u7531\u4fbf\u643a\u5f0f\u62cd\u6444\u8bbe\u5907\u6355\u83b7\u7684\u5f2f\u66f2\u3001\u8d77\u76b1\u548c\u65cb\u8f6c\u7684\u6587\u6863\u56fe\u50cf\uff08\u6587\u6863\u56fe\u50cf\u53bb\u626d\u66f2\uff09\u53d8\u5f97\u8d8a\u6765\u8d8a\u91cd\u8981\u3002\u7136\u800c\uff0c\u73b0\u6709\u65b9\u6cd5\u5728\u9762\u5bf9\u771f\u5b9e\u573a\u666f\u4e2d\u590d\u6742\u7684\u6587\u6863\u7ed3\u6784\u548c\u66f4\u9ad8\u7a0b\u5ea6\u7684\u53d8\u5f62\u65f6\uff0c\u5f80\u5f80\u96be\u4ee5\u53d6\u5f97\u6ee1\u610f\u7684\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTADoc\uff08Time-Aware Document Dewarping Network\uff09\u7684\u8f7b\u91cf\u7ea7\u6846\u67b6\u6765\u89e3\u51b3\u6587\u6863\u56fe\u50cf\u7684\u51e0\u4f55\u5931\u771f\u95ee\u9898\uff0c\u5e76\u5c06\u6587\u6863\u53bb\u626d\u66f2\u4efb\u52a1\u9996\u6b21\u5efa\u6a21\u4e3a\u4e00\u4e2a\u5305\u542b\u4e00\u7cfb\u5217\u4e2d\u95f4\u72b6\u6001\u7684\u52a8\u6001\u8fc7\u7a0b\u3002", "result": "\u6a21\u578b\u5728\u4e0d\u540c\u6587\u6863\u7c7b\u578b\u548c\u5931\u771f\u5ea6\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u4e0d\u540c\u6587\u6863\u7c7b\u578b\u548c\u5931\u771f\u5ea6\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u9c81\u68d2\u6027\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08075", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08075", "abs": "https://arxiv.org/abs/2508.08075", "authors": ["Meishen He", "Wenjun Ma", "Jiao Wang", "Huijun Yue", "Xiaoma Fan"], "title": "FNBT: Full Negation Belief Transformation for Open-World Information Fusion Based on Dempster-Shafer Theory of Evidence", "comment": null, "summary": "The Dempster-Shafer theory of evidence has been widely applied in the field\nof information fusion under uncertainty. Most existing research focuses on\ncombining evidence within the same frame of discernment. However, in real-world\nscenarios, trained algorithms or data often originate from different regions or\norganizations, where data silos are prevalent. As a result, using different\ndata sources or models to generate basic probability assignments may lead to\nheterogeneous frames, for which traditional fusion methods often yield\nunsatisfactory results. To address this challenge, this study proposes an\nopen-world information fusion method, termed Full Negation Belief\nTransformation (FNBT), based on the Dempster-Shafer theory. More specially, a\ncriterion is introduced to determine whether a given fusion task belongs to the\nopen-world setting. Then, by extending the frames, the method can accommodate\nelements from heterogeneous frames. Finally, a full negation mechanism is\nemployed to transform the mass functions, so that existing combination rules\ncan be applied to the transformed mass functions for such information fusion.\nTheoretically, the proposed method satisfies three desirable properties, which\nare formally proven: mass function invariance, heritability, and essential\nconflict elimination. Empirically, FNBT demonstrates superior performance in\npattern classification tasks on real-world datasets and successfully resolves\nZadeh's counterexample, thereby validating its practical effectiveness.", "AI": {"tldr": "FNBT\u662f\u4e00\u79cd\u7528\u4e8e\u5904\u7406\u5f02\u6784\u6846\u67b6\u4e0b\u4fe1\u606f\u878d\u5408\u7684Dempster-Shafer\u7406\u8bba\u65b9\u6cd5\uff0c\u901a\u8fc7\u6846\u67b6\u6269\u5c55\u548c\u5168\u5426\u5b9a\u8f6c\u6362\u6765\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u7684\u4e0d\u8db3\uff0c\u5e76\u5728\u6a21\u5f0f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u8bc1\u636e\u7406\u8bba\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u540c\u4e00\u8fa8\u522b\u6846\u67b6\u5185\u7684\u8bc1\u636e\u7ec4\u5408\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\uff0c\u6570\u636e\u6e90\u6216\u6a21\u578b\u53ef\u80fd\u6765\u81ea\u4e0d\u540c\u7684\u533a\u57df\u6216\u7ec4\u7ec7\uff0c\u5bfc\u81f4\u8fa8\u522b\u6846\u67b6\u5f02\u6784\uff0c\u4f20\u7edf\u878d\u5408\u65b9\u6cd5\u6548\u679c\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eDempster-Shafer\u7406\u8bba\u7684\u5f00\u653e\u4e16\u754c\u4fe1\u606f\u878d\u5408\u65b9\u6cd5\uff0c\u79f0\u4e3a\u5168\u5426\u5b9a\u4fe1\u5ff5\u8f6c\u6362\uff08FNBT\uff09\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u6807\u51c6\u6765\u5224\u65ad\u878d\u5408\u4efb\u52a1\u662f\u5426\u5c5e\u4e8e\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\uff0c\u7136\u540e\u6269\u5c55\u6846\u67b6\u4ee5\u9002\u5e94\u6765\u81ea\u5f02\u6784\u6846\u67b6\u7684\u5143\u7d20\uff0c\u6700\u540e\u4f7f\u7528\u5168\u5426\u5b9a\u673a\u5236\u8f6c\u6362\u8d28\u91cf\u51fd\u6570\uff0c\u4ee5\u4fbf\u5c06\u73b0\u6709\u7684\u7ec4\u5408\u89c4\u5219\u5e94\u7528\u4e8e\u8f6c\u6362\u540e\u7684\u8d28\u91cf\u51fd\u6570\u8fdb\u884c\u4fe1\u606f\u878d\u5408\u3002", "result": "FNBT\u6ee1\u8db3\u8d28\u91cf\u51fd\u6570\u4e0d\u53d8\u6027\u3001\u53ef\u7ee7\u627f\u6027\u548c\u672c\u8d28\u51b2\u7a81\u6d88\u9664\u4e09\u4e2a\u671f\u671b\u7684\u6027\u8d28\uff0c\u5e76\u5df2\u5f97\u5230\u6b63\u5f0f\u8bc1\u660e\u3002", "conclusion": "FNBT\u5728\u73b0\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u7684\u6a21\u5f0f\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u6210\u529f\u89e3\u51b3\u4e86Zadeh\u7684\u9006\u5426\u547d\u9898\uff0c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.07208", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07208", "abs": "https://arxiv.org/abs/2508.07208", "authors": ["Chanakya Ekbote", "Marco Bondaschi", "Nived Rajaraman", "Jason D. Lee", "Michael Gastpar", "Ashok Vardhan Makkuva", "Paul Pu Liang"], "title": "What One Cannot, Two Can: Two-Layer Transformers Provably Represent Induction Heads on Any-Order Markov Chains", "comment": null, "summary": "In-context learning (ICL) is a hallmark capability of transformers, through\nwhich trained models learn to adapt to new tasks by leveraging information from\nthe input context. Prior work has shown that ICL emerges in transformers due to\nthe presence of special circuits called induction heads. Given the equivalence\nbetween induction heads and conditional k-grams, a recent line of work modeling\nsequential inputs as Markov processes has revealed the fundamental impact of\nmodel depth on its ICL capabilities: while a two-layer transformer can\nefficiently represent a conditional 1-gram model, its single-layer counterpart\ncannot solve the task unless it is exponentially large. However, for higher\norder Markov sources, the best known constructions require at least three\nlayers (each with a single attention head) - leaving open the question: can a\ntwo-layer single-head transformer represent any kth-order Markov process? In\nthis paper, we precisely address this and theoretically show that a two-layer\ntransformer with one head per layer can indeed represent any conditional\nk-gram. Thus, our result provides the tightest known characterization of the\ninterplay between transformer depth and Markov order for ICL. Building on this,\nwe further analyze the learning dynamics of our two-layer construction,\nfocusing on a simplified variant for first-order Markov chains, illustrating\nhow effective in-context representations emerge during training. Together,\nthese results deepen our current understanding of transformer-based ICL and\nillustrate how even shallow architectures can surprisingly exhibit strong ICL\ncapabilities on structured sequence modeling tasks.", "AI": {"tldr": "\u4e24\u5c42\u5355\u5934 Transformer \u53ef\u8868\u793a\u4efb\u610f k \u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u63ed\u793a\u4e86\u6d45\u5c42\u6a21\u578b\u5728\u7ed3\u6784\u5316\u5e8f\u5217\u5efa\u6a21\u4efb\u52a1\u4e2d\u7684 ICL \u80fd\u529b\u3002", "motivation": "\u63a2\u7a76 Transformer \u6df1\u5ea6\u4e0e\u9a6c\u5c14\u53ef\u592b\u9636\u6570\u5bf9 ICL \u80fd\u529b\u7684\u5f71\u54cd\uff0c\u7279\u522b\u662f\u89e3\u51b3\u4e24\u5c42\u5355\u5934 Transformer \u662f\u5426\u80fd\u8868\u793a\u4efb\u610f k \u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\u7684\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u5206\u6790\uff0c\u8bc1\u660e\u4e86\u5177\u6709\u5355\u5934\u5c42\u7684\u4e24\u5c42 Transformer \u53ef\u4ee5\u8868\u793a\u4efb\u4f55 k \u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u5e76\u5206\u6790\u4e86\u8be5\u6784\u9020\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u7279\u522b\u662f\u9488\u5bf9\u4e00\u9636\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7b80\u5316\u53d8\u4f53\u3002", "result": "\u8bc1\u660e\u4e86\u5177\u6709\u5355\u5934\u5c42\u7684\u4e24\u5c42 Transformer \u53ef\u4ee5\u8868\u793a\u4efb\u4f55 k \u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u63d0\u4f9b\u4e86 Transformer \u6df1\u5ea6\u548c\u9a6c\u5c14\u53ef\u592b\u9636\u6570\u5728 ICL \u4e2d\u76f8\u4e92\u4f5c\u7528\u7684\u6700\u7cbe\u786e\u8868\u5f81\u3002", "conclusion": "\u672c\u6587\u8bc1\u660e\u4e86\u5177\u6709\u5355\u5934\u5c42\u7684\u4e24\u5c42 Transformer \u53ef\u4ee5\u8868\u793a\u4efb\u4f55 k \u9636\u9a6c\u5c14\u53ef\u592b\u8fc7\u7a0b\uff0c\u4e3a Transformer \u6df1\u5ea6\u548c\u9a6c\u5c14\u53ef\u592b\u9636\u6570\u5728 ICL \u4e2d\u7684\u76f8\u4e92\u4f5c\u7528\u63d0\u4f9b\u4e86\u6700\u7cbe\u786e\u7684\u8868\u5f81\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5206\u6790\u4e86\u4e24\u5c42\u6784\u9020\u7684\u5b66\u4e60\u52a8\u6001\uff0c\u7279\u522b\u5173\u6ce8\u4e86\u4e00\u9636\u9a6c\u5c14\u53ef\u592b\u94fe\u7684\u7b80\u5316\u53d8\u4f53\uff0c\u9610\u91ca\u4e86\u6709\u6548\u7684\u4e0a\u4e0b\u6587\u8868\u793a\u5982\u4f55\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u3002"}}
{"id": "2508.07781", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07781", "abs": "https://arxiv.org/abs/2508.07781", "authors": ["Zeyu Yang", "Lai Wei", "Roman Koshkin", "Xi Chen", "Satoshi Nakamura"], "title": "SASST: Leveraging Syntax-Aware Chunking and LLMs for Simultaneous Speech Translation", "comment": null, "summary": "This work proposes a grammar-based chunking strategy that segments input\nstreams into semantically complete units by parsing dependency relations (e.g.,\nnoun phrase boundaries, verb-object structures) and punctuation features. The\nmethod ensures chunk coherence and minimizes semantic fragmentation. Building\non this mechanism, we present SASST (Syntax-Aware Simultaneous Speech\nTranslation), an end-to-end framework integrating frozen Whisper encoder and\ndecoder-only LLM. The unified architecture dynamically outputs translation\ntokens or <WAIT> symbols to jointly optimize translation timing and content,\nwith target-side reordering addressing word-order divergence. Experiments on\nCoVoST2 multilingual corpus En-{De, Zh, Ja} demonstrate significant translation\nquality improvements across languages and validate the effectiveness of\nsyntactic structures in LLM-driven SimulST systems.", "AI": {"tldr": "SASST\u5229\u7528\u8bed\u6cd5\u5206\u5757\u7b56\u7565\u548c\u96c6\u6210\u7684Whisper\u7f16\u7801\u5668\u53caLLM\uff0c\u6539\u8fdb\u4e86\u540c\u65f6\u8bed\u97f3\u7ffb\u8bd1\u7684\u8d28\u91cf\u548c\u6548\u7387\u3002", "motivation": "\u65e8\u5728\u63d0\u9ad8\u540c\u65f6\u8bed\u97f3\u7ffb\u8bd1\uff08SimulST\uff09\u7684\u8d28\u91cf\u548c\u6548\u7387\uff0c\u901a\u8fc7\u5f15\u5165\u8bed\u6cd5\u4fe1\u606f\u6765\u4f18\u5316\u7ffb\u8bd1\u8fc7\u7a0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8bed\u6cd5\u7684\u5206\u5757\u7b56\u7565\uff0c\u901a\u8fc7\u89e3\u6790\u4f9d\u8d56\u5173\u7cfb\uff08\u5982\u540d\u8bcd\u77ed\u8bed\u8fb9\u754c\u3001\u52a8\u8bcd-\u5bbe\u8bed\u7ed3\u6784\uff09\u548c\u6807\u70b9\u7b26\u53f7\u7279\u5f81\u6765\u5206\u5272\u8f93\u5165\u6d41\u4e3a\u8bed\u4e49\u5b8c\u6574\u7684\u5355\u5143\uff0c\u786e\u4fdd\u4e86\u5206\u5757\u7684\u8fde\u8d2f\u6027\u5e76\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u4e86\u8bed\u4e49\u788e\u7247\u5316\u3002\u5728\u6b64\u673a\u5236\u7684\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86SASST\uff08Syntax-Aware Simultaneous Speech Translation\uff09\uff0c\u4e00\u4e2a\u96c6\u6210\u4e86\u51bb\u7ed3\u7684Whisper\u7f16\u7801\u5668\u548c\u4ec5\u89e3\u7801\u5668\u7684LLM\u7684\u7aef\u5230\u7aef\u6846\u67b6\u3002\u8be5\u7edf\u4e00\u67b6\u6784\u52a8\u6001\u5730\u8f93\u51fa\u7ffb\u8bd1\u8bcd\u5143\u6216<WAIT>\u7b26\u53f7\uff0c\u4ee5\u8054\u5408\u4f18\u5316\u7ffb\u8bd1\u65f6\u95f4\u548c\u5185\u5bb9\uff0c\u5e76\u901a\u8fc7\u76ee\u6807\u4fa7\u91cd\u6392\u5e8f\u6765\u89e3\u51b3\u8bcd\u5e8f\u5dee\u5f02\u3002\u9488\u5bf9CoVoST2\u591a\u8bed\u8a00\u8bed\u6599\u5e93\uff08\u82f1-\u5fb7\u3001\u82f1-\u4e2d\u3001\u82f1-\u65e5\uff09\u8fdb\u884c\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8de8\u8bed\u8a00\u7ffb\u8bd1\u8d28\u91cf\u7684\u663e\u8457\u63d0\u9ad8\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8bed\u6cd5\u7ed3\u6784\u5728LLM\u9a71\u52a8\u7684\u540c\u65f6\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSASST\u5728\u591a\u4e2a\u8bed\u8a00\u5bf9\u4e0a\u663e\u8457\u63d0\u9ad8\u4e86\u7ffb\u8bd1\u8d28\u91cf\uff0c\u5e76\u9a8c\u8bc1\u4e86\u8bed\u6cd5\u7ed3\u6784\u5728LLM\u9a71\u52a8\u7684SimulST\u7cfb\u7edf\u4e2d\u7684\u6709\u6548\u6027\u3002", "conclusion": "SASST\u901a\u8fc7\u6574\u5408\u51bb\u7ed3\u7684Whisper\u7f16\u7801\u5668\u548c\u4ec5\u89e3\u7801\u5668\u7684LLM\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u6cd5\u9a71\u52a8\u7684\u540c\u65f6\u8bed\u97f3\u7ffb\u8bd1\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u52a8\u6001\u8f93\u51fa\u7ffb\u8bd1\u8bcd\u5143\u6216<WAIT>\u7b26\u53f7\uff0c\u4ee5\u8054\u5408\u4f18\u5316\u7ffb\u8bd1\u65f6\u95f4\u548c\u5185\u5bb9\u3002"}}
{"id": "2508.06993", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06993", "abs": "https://arxiv.org/abs/2508.06993", "authors": ["Nick Lemke", "John Kalkhof", "Niklas Babendererde", "Anirban Mukhopadhyay"], "title": "OctreeNCA: Single-Pass 184 MP Segmentation on Consumer Hardware", "comment": null, "summary": "Medical applications demand segmentation of large inputs, like prostate MRIs,\npathology slices, or videos of surgery. These inputs should ideally be inferred\nat once to provide the model with proper spatial or temporal context. When\nsegmenting large inputs, the VRAM consumption of the GPU becomes the\nbottleneck. Architectures like UNets or Vision Transformers scale very poorly\nin VRAM consumption, resulting in patch- or frame-wise approaches that\ncompromise global consistency and inference speed. The lightweight Neural\nCellular Automaton (NCA) is a bio-inspired model that is by construction\nsize-invariant. However, due to its local-only communication rules, it lacks\nglobal knowledge. We propose OctreeNCA by generalizing the neighborhood\ndefinition using an octree data structure. Our generalized neighborhood\ndefinition enables the efficient traversal of global knowledge. Since deep\nlearning frameworks are mainly developed for large multi-layer networks, their\nimplementation does not fully leverage the advantages of NCAs. We implement an\nNCA inference function in CUDA that further reduces VRAM demands and increases\ninference speed. Our OctreeNCA segments high-resolution images and videos\nquickly while occupying 90% less VRAM than a UNet during evaluation. This\nallows us to segment 184 Megapixel pathology slices or 1-minute surgical videos\nat once.", "AI": {"tldr": "OctreeNCA\u901a\u8fc7\u516b\u53c9\u6811\u548cCUDA\u4f18\u5316\uff0c\u5728\u533b\u5b66\u56fe\u50cf\u548c\u89c6\u9891\u5206\u5272\u4e2d\u5b9e\u73b0\u4e86\u4f4e\u663e\u5b58\u5360\u7528\u548c\u9ad8\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4e86\u533b\u5b66\u5e94\u7528\u4e2d\u5206\u5272\u5927\u578b\u8f93\u5165\uff08\u5982\u524d\u5217\u817aMRI\u3001\u75c5\u7406\u5207\u7247\u6216\u624b\u672f\u89c6\u9891\uff09\u65f6\uff0cGPU\u663e\u5b58\u6d88\u8017\u6210\u4e3a\u74f6\u9888\u7684\u95ee\u9898\uff0c\u4ee5\u53ca\u73b0\u6709UNet\u6216Vision Transformer\u7b49\u6a21\u578b\u5728\u5904\u7406\u8fd9\u4e9b\u6570\u636e\u65f6\u727a\u7272\u5168\u5c40\u4e00\u81f4\u6027\u548c\u63a8\u7406\u901f\u5ea6\u7684\u7f3a\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOctreeNCA\u7684\u8f7b\u91cf\u7ea7\u795e\u7ecf\u7ec6\u80de\u81ea\u52a8\u673a\u6a21\u578b\uff0c\u901a\u8fc7\u516b\u53c9\u6811\u6570\u636e\u7ed3\u6784\u6269\u5c55\u4e86\u90bb\u57df\u5b9a\u4e49\uff0c\u5e76\u4f7f\u7528CUDA\u5b9e\u73b0\u4e86\u4f18\u5316\u7684\u63a8\u7406\u51fd\u6570\u3002", "result": "OctreeNCA\u5728\u5206\u5272\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u548c\u89c6\u9891\u65f6\u901f\u5ea6\u5feb\uff0c\u663e\u5b58\u5360\u7528\u6bd4UNet\u4f4e90%\uff0c\u80fd\u591f\u4e00\u6b21\u6027\u5206\u5272184\u5146\u50cf\u7d20\u7684\u75c5\u7406\u5207\u7247\u62161\u5206\u949f\u7684\u624b\u672f\u89c6\u9891\u3002", "conclusion": "OctreeNCA\u901a\u8fc7\u4f7f\u7528\u516b\u53c9\u6811\u6570\u636e\u7ed3\u6784\u63a8\u5e7f\u90bb\u57df\u5b9a\u4e49\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u5168\u5c40\u77e5\u8bc6\u904d\u5386\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u5904\u7406\u5927\u578b\u533b\u5b66\u56fe\u50cf\u548c\u89c6\u9891\u65f6\u9047\u5230\u7684\u663e\u5b58\u74f6\u9888\u95ee\u9898\u3002"}}
{"id": "2508.08115", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08115", "abs": "https://arxiv.org/abs/2508.08115", "authors": ["Pranav Pushkar Mishra", "Mohammad Arvan", "Mohan Zalake"], "title": "TeamMedAgents: Enhancing Medical Decision-Making of LLMs Through Structured Teamwork", "comment": "10 pages, 1 figure, 6 tables(2 in main, 4 in appendix)", "summary": "We present TeamMedAgents, a novel multi-agent approach that systematically\nintegrates evidence-based teamwork components from human-human collaboration\ninto medical decision-making with large language models (LLMs). Our approach\nvalidates an organizational psychology teamwork model from human collaboration\nto computational multi-agent medical systems by operationalizing six core\nteamwork components derived from Salas et al.'s \"Big Five\" model: team\nleadership, mutual performance monitoring, team orientation, shared mental\nmodels, closed-loop communication, and mutual trust. We implement and evaluate\nthese components as modular, configurable mechanisms within an adaptive\ncollaboration architecture while assessing the effect of the number of agents\ninvolved based on the task's requirements and domain. Systematic evaluation of\ncomputational implementations of teamwork behaviors across eight medical\nbenchmarks (MedQA, MedMCQA, MMLU-Pro Medical, PubMedQA, DDXPlus, MedBullets,\nPath-VQA, and PMC-VQA) demonstrates consistent improvements across 7 out of 8\nevaluated datasets. Controlled ablation studies conducted on 50 questions per\nconfiguration across 3 independent runs provide mechanistic insights into\nindividual component contributions, revealing optimal teamwork configurations\nthat vary by reasoning task complexity and domain-specific requirements. Our\nablation analyses reveal dataset-specific optimal teamwork configurations,\nindicating that different medical reasoning modalities benefit from distinct\ncollaborative patterns. TeamMedAgents represents an advancement in\ncollaborative AI by providing a systematic translation of established teamwork\ntheories from human collaboration into agentic collaboration, establishing a\nfoundation for evidence-based multi-agent system design in critical\ndecision-making domains.", "AI": {"tldr": "TeamMedAgents\u5c06\u4eba\u7c7b\u56e2\u961f\u5408\u4f5c\u7406\u8bba\u5e94\u7528\u4e8e\u533b\u5b66\u51b3\u7b56LLM\uff0c\u901a\u8fc7\u6574\u5408\u516d\u4e2a\u6838\u5fc3\u56e2\u961f\u5408\u4f5c\u7ec4\u6210\u90e8\u5206\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u5e76\u5728\u5927\u591a\u6570\u533b\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u793a\u51fa\u6539\u8fdb\uff0c\u5176\u6700\u4f18\u914d\u7f6e\u56e0\u4efb\u52a1\u800c\u5f02\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u5c06\u4eba\u7c7b\u534f\u4f5c\u4e2d\u7684\u5faa\u8bc1\u56e2\u961f\u5408\u4f5c\u7406\u8bba\u7cfb\u7edf\u5730\u8f6c\u5316\u4e3a\u4ee3\u7406\u534f\u4f5c\uff0c\u4e3a\u5728\u5173\u952e\u51b3\u7b56\u9886\u57df\u8bbe\u8ba1\u5faa\u8bc1\u591a\u4ee3\u7406\u7cfb\u7edf\u5960\u5b9a\u57fa\u7840\u3002", "method": "TeamMedAgents\u662f\u4e00\u79cd\u65b0\u9896\u7684\u591a\u4ee3\u7406\u65b9\u6cd5\uff0c\u5b83\u5c06\u5faa\u8bc1\u56e2\u961f\u5408\u4f5c\u7684\u7ec4\u6210\u90e8\u5206\u7cfb\u7edf\u5730\u6574\u5408\u5230\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u533b\u5b66\u51b3\u7b56\u4e2d\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5b9e\u65bd\u6e90\u81eaSalas\u7b49\u4eba\u7684\u201c\u5927\u4e94\u201d\u6a21\u578b\u7684\u516d\u4e2a\u6838\u5fc3\u56e2\u961f\u7ec4\u6210\u90e8\u5206\uff08\u56e2\u961f\u9886\u5bfc\u3001\u76f8\u4e92\u7ee9\u6548\u76d1\u63a7\u3001\u56e2\u961f\u5bfc\u5411\u3001\u5fc3\u667a\u6a21\u578b\u5171\u4eab\u3001\u95ed\u73af\u6c9f\u901a\u548c\u76f8\u4e92\u4fe1\u4efb\uff09\u6765\u9a8c\u8bc1\u4ece\u4eba\u7c7b\u534f\u4f5c\u5230\u8ba1\u7b97\u6027\u591a\u4ee3\u7406\u533b\u5b66\u7cfb\u7edf\u3002\u8fd9\u4e9b\u7ec4\u6210\u90e8\u5206\u88ab\u5b9e\u73b0\u4e3a\u81ea\u9002\u5e94\u534f\u4f5c\u67b6\u6784\u4e2d\u7684\u6a21\u5757\u5316\u3001\u53ef\u914d\u7f6e\u673a\u5236\uff0c\u5e76\u8bc4\u4f30\u4e86\u6240\u6d89\u53ca\u4ee3\u7406\u6570\u91cf\u5bf9\u4efb\u52a1\u9700\u6c42\u548c\u9886\u57df\u7684\u5f71\u54cd\u3002", "result": "\u5728\u516b\u4e2a\u533b\u5b66\u57fa\u51c6\uff08MedQA\u3001MedMCQA\u3001MMLU-Pro Medical\u3001PubMedQA\u3001DDXPlus\u3001MedBullets\u3001Path-VQA\u548cPMC-VQA\uff09\u4e0a\u7684\u8bc4\u4f30\u8868\u660e\uff0cTeamMedAgents\u57287\u4e2a\u6570\u636e\u96c6\u4e2d\u6301\u7eed\u6539\u8fdb\u3002\u6d88\u878d\u7814\u7a76\u63ed\u793a\u4e86\u7279\u5b9a\u4e8e\u6570\u636e\u96c6\u7684\u6700\u4f18\u56e2\u961f\u5408\u4f5c\u914d\u7f6e\uff0c\u8868\u660e\u4e0d\u540c\u7684\u533b\u5b66\u63a8\u7406\u6a21\u5f0f\u53d7\u76ca\u4e8e\u4e0d\u540c\u7684\u534f\u4f5c\u6a21\u5f0f\u3002", "conclusion": "TeamMedAgents\u5728\u8de8\u8d8a\u516b\u4e2a\u533b\u5b66\u57fa\u51c6\u7684\u8bc4\u4f30\u4e2d\uff0c\u57287\u4e2a\u6570\u636e\u96c6\u4e2d\u5c55\u793a\u4e86\u6301\u7eed\u7684\u6539\u8fdb\uff0c\u5e76\u63d0\u4f9b\u4e86\u5173\u4e8e\u4e2a\u522b\u7ec4\u4ef6\u8d21\u732e\u7684\u673a\u5236\u89c1\u89e3\uff0c\u63ed\u793a\u4e86\u53ef\u6839\u636e\u63a8\u7406\u4efb\u52a1\u590d\u6742\u6027\u548c\u7279\u5b9a\u9886\u57df\u8981\u6c42\u8fdb\u884c\u8c03\u6574\u7684\u6700\u4f18\u56e2\u961f\u914d\u7f6e\u3002"}}
{"id": "2508.07220", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07220", "abs": "https://arxiv.org/abs/2508.07220", "authors": ["Jian Xu", "Yican Liu", "Qibin Zhao", "John Paisley", "Delu Zeng"], "title": "Neural Bridge Processes", "comment": null, "summary": "Learning stochastic functions from partially observed context-target pairs is\na fundamental problem in probabilistic modeling. Traditional models like\nGaussian Processes (GPs) face scalability issues with large datasets and assume\nGaussianity, limiting their applicability. While Neural Processes (NPs) offer\nmore flexibility, they struggle with capturing complex, multi-modal target\ndistributions. Neural Diffusion Processes (NDPs) enhance expressivity through a\nlearned diffusion process but rely solely on conditional signals in the\ndenoising network, resulting in weak input coupling from an unconditional\nforward process and semantic mismatch at the diffusion endpoint. In this work,\nwe propose Neural Bridge Processes (NBPs), a novel method for modeling\nstochastic functions where inputs x act as dynamic anchors for the entire\ndiffusion trajectory. By reformulating the forward kernel to explicitly depend\non x, NBP enforces a constrained path that strictly terminates at the\nsupervised target. This approach not only provides stronger gradient signals\nbut also guarantees endpoint coherence. We validate NBPs on synthetic data, EEG\nsignal regression and image regression tasks, achieving substantial\nimprovements over baselines. These results underscore the effectiveness of\nDDPM-style bridge sampling in enhancing both performance and theoretical\nconsistency for structured prediction tasks.", "AI": {"tldr": "\u795e\u7ecf\u6865\u63a5\u8fc7\u7a0b\uff08NBPs\uff09\u901a\u8fc7\u5c06\u8f93\u5165\u89c6\u4e3a\u6269\u6563\u8f68\u8ff9\u7684\u951a\u70b9\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\uff08\u5982GP\u3001NP\u3001NDP\uff09\u5728\u5904\u7406\u968f\u673a\u51fd\u6570\u548c\u90e8\u5206\u89c2\u6d4b\u6570\u636e\u65f6\u7684\u53ef\u6269\u5c55\u6027\u3001\u8868\u8fbe\u80fd\u529b\u548c\u7aef\u70b9\u4e00\u81f4\u6027\u95ee\u9898\u3002NBPs\u901a\u8fc7\u6539\u8fdb\u524d\u5411\u6838\uff0c\u786e\u4fdd\u6269\u6563\u8fc7\u7a0b\u51c6\u786e\u7ec8\u6b62\u4e8e\u76ee\u6807\u503c\uff0c\u4ece\u800c\u5728\u591a\u4e2a\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684GP\u6a21\u578b\u5728\u5904\u7406\u5927\u578b\u6570\u636e\u96c6\u65f6\u5b58\u5728\u53ef\u6269\u5c55\u6027\u95ee\u9898\uff0c\u5e76\u4e14\u5176\u9ad8\u65af\u5047\u8bbe\u9650\u5236\u4e86\u9002\u7528\u6027\u3002\u867d\u7136NP\u6a21\u578b\u66f4\u7075\u6d3b\uff0c\u4f46\u5b83\u4eec\u96be\u4ee5\u6355\u6349\u590d\u6742\u7684\u591a\u6a21\u6001\u76ee\u6807\u5206\u5e03\u3002NDP\u6a21\u578b\u901a\u8fc7\u5b66\u4e60\u5230\u7684\u6269\u6563\u8fc7\u7a0b\u589e\u5f3a\u4e86\u8868\u8fbe\u80fd\u529b\uff0c\u4f46\u5176\u964d\u566a\u7f51\u7edc\u4ec5\u4f9d\u8d56\u4e8e\u6761\u4ef6\u4fe1\u53f7\uff0c\u5bfc\u81f4\u4e0e\u65e0\u6761\u4ef6\u524d\u5411\u8fc7\u7a0b\u7684\u8026\u5408\u8f83\u5f31\uff0c\u5e76\u4e14\u5728\u6269\u6563\u7ec8\u70b9\u5b58\u5728\u8bed\u4e49\u4e0d\u5339\u914d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u795e\u7ecf\u6865\u63a5\u8fc7\u7a0b\uff08NBPs\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u8f93\u5165x\u4f5c\u4e3a\u6269\u6563\u8f68\u8ff9\u7684\u52a8\u6001\u951a\u70b9\uff0c\u5e76\u4fee\u6539\u524d\u5411\u6838\u4f7f\u5176\u663e\u5f0f\u4f9d\u8d56\u4e8ex\uff0c\u6765\u5b9e\u73b0\u8f93\u5165\u4e0e\u6269\u6563\u8fc7\u7a0b\u7684\u5f3a\u8026\u5408\uff0c\u786e\u4fdd\u6269\u6563\u8fc7\u7a0b\u4e25\u683c\u7ec8\u6b62\u4e8e\u76ee\u6807\u503c\u3002", "result": "NBPs\u5728\u5408\u6210\u6570\u636e\u3001EEG\u4fe1\u53f7\u56de\u5f52\u548c\u56fe\u50cf\u56de\u5f52\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "NBPs\u901a\u8fc7\u5c06\u8f93\u5165x\u4f5c\u4e3a\u6574\u4e2a\u6269\u6563\u8f68\u8ff9\u7684\u52a8\u6001\u951a\u70b9\uff0c\u5e76\u663e\u5f0f\u5730\u4f7f\u524d\u5411\u6838\u4f9d\u8d56\u4e8ex\uff0c\u4ece\u800c\u5f3a\u5236\u6267\u884c\u4e00\u4e2a\u4e25\u683c\u7ec8\u6b62\u4e8e\u76d1\u7763\u76ee\u6807\u7684\u7ea6\u675f\u8def\u5f84\u3002\u8fd9\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684\u68af\u5ea6\u4fe1\u53f7\uff0c\u5e76\u4fdd\u8bc1\u4e86\u7aef\u70b9\u7684\u4e00\u81f4\u6027\u3002NBPs\u5728\u5408\u6210\u6570\u636e\u3001EEG\u4fe1\u53f7\u56de\u5f52\u548c\u56fe\u50cf\u56de\u5f52\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u6539\u8fdb\uff0c\u5c55\u793a\u4e86DDPM\u98ce\u683c\u7684\u6865\u63a5\u91c7\u6837\u5728\u63d0\u9ad8\u7ed3\u6784\u5316\u9884\u6d4b\u4efb\u52a1\u7684\u6027\u80fd\u548c\u7406\u8bba\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.07089", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07089", "abs": "https://arxiv.org/abs/2508.07089", "authors": ["Sandro Papais", "Letian Wang", "Brian Cheong", "Steven L. Waslander"], "title": "ForeSight: Multi-View Streaming Joint Object Detection and Trajectory Forecasting", "comment": "Accepted to ICCV 2025", "summary": "We introduce ForeSight, a novel joint detection and forecasting framework for\nvision-based 3D perception in autonomous vehicles. Traditional approaches treat\ndetection and forecasting as separate sequential tasks, limiting their ability\nto leverage temporal cues. ForeSight addresses this limitation with a\nmulti-task streaming and bidirectional learning approach, allowing detection\nand forecasting to share query memory and propagate information seamlessly. The\nforecast-aware detection transformer enhances spatial reasoning by integrating\ntrajectory predictions from a multiple hypothesis forecast memory queue, while\nthe streaming forecast transformer improves temporal consistency using past\nforecasts and refined detections. Unlike tracking-based methods, ForeSight\neliminates the need for explicit object association, reducing error propagation\nwith a tracking-free model that efficiently scales across multi-frame\nsequences. Experiments on the nuScenes dataset show that ForeSight achieves\nstate-of-the-art performance, achieving an EPA of 54.9%, surpassing previous\nmethods by 9.3%, while also attaining the best mAP and minADE among multi-view\ndetection and forecasting models.", "AI": {"tldr": "ForeSight\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u89c9\u57fa\u78403D\u611f\u77e5\u6846\u67b6\uff0c\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u6c7d\u8f66\u7684\u8054\u5408\u68c0\u6d4b\u548c\u9884\u6d4b\u3002\u5b83\u901a\u8fc7\u591a\u4efb\u52a1\u6d41\u548c\u53cc\u5411\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u68c0\u6d4b\u548c\u9884\u6d4b\u7684\u65e0\u7f1d\u4fe1\u606f\u4f20\u64ad\uff0c\u65e0\u9700\u663e\u5f0f\u5bf9\u8c61\u5173\u8054\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u5c06\u68c0\u6d4b\u548c\u9884\u6d4b\u89c6\u4e3a\u72ec\u7acb\u4efb\u52a1\uff0c\u672a\u80fd\u5145\u5206\u5229\u7528\u65f6\u95f4\u7ebf\u7d22\u3002ForeSight\u65e8\u5728\u901a\u8fc7\u8054\u5408\u6846\u67b6\u89e3\u51b3\u6b64\u5c40\u9650\u6027\u3002", "method": "ForeSight\u91c7\u7528\u591a\u4efb\u52a1\u6d41\u548c\u53cc\u5411\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u68c0\u6d4b\u548c\u9884\u6d4b\u7684\u8054\u5408\u3002\u5176\u7279\u70b9\u5305\u62ec\u5171\u4eab\u67e5\u8be2\u8bb0\u5fc6\u3001\u65e0\u8ddf\u8e2a\u6a21\u578b\u4ee5\u53ca\u7528\u4e8e\u7a7a\u95f4\u63a8\u7406\u548c\u65f6\u95f4\u4e00\u81f4\u6027\u7684\u9884\u6d4b\u611f\u77e5\u68c0\u6d4bTransformer\u548c\u6d41\u5f0f\u9884\u6d4bTransformer\u3002", "result": "ForeSight\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cEPA\u8fbe\u523054.9%\uff0c\u6bd4\u5148\u524d\u65b9\u6cd5\u63d0\u9ad8\u4e869.3%\uff0c\u5e76\u4e14\u5728\u591a\u89c6\u56fe\u68c0\u6d4b\u548c\u9884\u6d4b\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7684mAP\u548cminADE\u3002", "conclusion": "ForeSight\u5728nuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cEPA\u8fbe\u523054.9%\uff0c\u6bd4\u5148\u524d\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e869.3%\uff0c\u540c\u65f6\u5728\u591a\u89c6\u56fe\u68c0\u6d4b\u548c\u9884\u6d4b\u6a21\u578b\u4e2d\u53d6\u5f97\u4e86\u6700\u4f73\u7684mAP\u548cminADE\u3002"}}
{"id": "2508.07785", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07785", "abs": "https://arxiv.org/abs/2508.07785", "authors": ["Haoyuan Wu", "Haoxing Chen", "Xiaodong Chen", "Zhanchao Zhou", "Tieyuan Chen", "Yihong Zhuang", "Guoshan Lu", "Zenan Huang", "Junbo Zhao", "Lin Liu", "Zhenzhong Lan", "Bei Yu", "Jianguo Li"], "title": "Grove MoE: Towards Efficient and Superior MoE LLMs with Adjugate Experts", "comment": null, "summary": "The Mixture of Experts (MoE) architecture is a cornerstone of modern\nstate-of-the-art (SOTA) large language models (LLMs). MoE models facilitate\nscalability by enabling sparse parameter activation. However, traditional MoE\narchitecture uses homogeneous experts of a uniform size, activating a fixed\nnumber of parameters irrespective of input complexity and thus limiting\ncomputational efficiency. To overcome this limitation, we introduce Grove MoE,\na novel architecture incorporating experts of varying sizes, inspired by the\nheterogeneous big.LITTLE CPU architecture. This architecture features novel\nadjugate experts with a dynamic activation mechanism, enabling model capacity\nexpansion while maintaining manageable computational overhead. Building on this\narchitecture, we present GroveMoE-Base and GroveMoE-Inst, 33B-parameter LLMs\ndeveloped by applying an upcycling strategy to the Qwen3-30B-A3B-Base model\nduring mid-training and post-training. GroveMoE models dynamically activate\n3.14-3.28B parameters based on token complexity and achieve performance\ncomparable to SOTA open-source models of similar or even larger size.", "AI": {"tldr": "Grove MoE\u901a\u8fc7\u5f02\u6784\u4e13\u5bb6\u548c\u52a8\u6001\u6fc0\u6d3b\u89e3\u51b3\u4e86\u4f20\u7edfMoE\u6548\u7387\u95ee\u9898\uff0c\u5728\u4e0eSOTA\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u6269\u5c55\u3002", "motivation": "\u4f20\u7edf\u7684Mixture of Experts (MoE)\u67b6\u6784\u4f7f\u7528\u540c\u8d28\u7684\u3001\u56fa\u5b9a\u5927\u5c0f\u7684\u4e13\u5bb6\uff0c\u65e0\u8bba\u8f93\u5165\u590d\u6742\u6027\u5982\u4f55\u90fd\u4f1a\u6fc0\u6d3b\u56fa\u5b9a\u6570\u91cf\u7684\u53c2\u6570\uff0c\u8fd9\u9650\u5236\u4e86\u8ba1\u7b97\u6548\u7387\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e00\u9650\u5236\uff0c\u63d0\u51faGrove MoE\u67b6\u6784\uff0c\u4ee5\u5f02\u6784\u7684\u3001\u4e0d\u540c\u5927\u5c0f\u7684\u4e13\u5bb6\u6765\u63d0\u9ad8\u8ba1\u7b97\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrove MoE\u7684\u65b0\u578b\u67b6\u6784\uff0c\u8be5\u67b6\u6784\u5305\u542b\u4e0d\u540c\u5927\u5c0f\u7684\u4e13\u5bb6\uff0c\u5e76\u5f15\u5165\u4e86\u5177\u6709\u52a8\u6001\u6fc0\u6d3b\u673a\u5236\u7684\u65b0\u578badjugate\u4e13\u5bb6\u3002\u5229\u7528\u8fd9\u79cd\u67b6\u6784\uff0c\u901a\u8fc7\u5728\u4e2d\u671f\u8bad\u7ec3\u548c\u8bad\u7ec3\u540e\u5bf9Qwen3-30B-A3B-Base\u6a21\u578b\u8fdb\u884c\u5347\u7ea7\uff0c\u5f00\u53d1\u4e8633B\u53c2\u6570\u7684GroveMoE-Base\u548cGroveMoE-Inst\u6a21\u578b\u3002", "result": "GroveMoE\u6a21\u578b\u6839\u636etoken\u7684\u590d\u6742\u6027\u52a8\u6001\u6fc0\u6d3b3.14-3.28B\u53c2\u6570\uff0c\u6027\u80fd\u4e0e\u540c\u7b49\u6216\u66f4\u5927\u89c4\u6a21\u7684SOTA\u5f00\u6e90\u6a21\u578b\u76f8\u5f53\u3002", "conclusion": "GroveMoE\u6a21\u578b\u901a\u8fc7\u5f15\u5165\u5f02\u6784\u4e13\u5bb6\u548c\u52a8\u6001\u6fc0\u6d3b\u673a\u5236\uff0c\u5728\u4fdd\u6301\u53ef\u63a7\u8ba1\u7b97\u5f00\u9500\u7684\u540c\u65f6\u5b9e\u73b0\u4e86\u6a21\u578b\u5bb9\u91cf\u7684\u6269\u5c55\uff0c\u5e76\u8fbe\u5230\u4e86\u4e0e\u540c\u7b49\u89c4\u6a21\u751a\u81f3\u66f4\u5927\u89c4\u6a21SOTA\u6a21\u578b\u7684\u6027\u80fd\u76f8\u5f53\u7684\u6c34\u5e73\u3002"}}
{"id": "2508.06995", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.06995", "abs": "https://arxiv.org/abs/2508.06995", "authors": ["Huihui Xu", "Jin Ye", "Hongqiu Wang", "Changkai Ji", "Jiashi Lin", "Ming Hu", "Ziyan Huang", "Ying Chen", "Chenglong Ma", "Tianbin Li", "Lihao Liu", "Junjun He", "Lei Zhu"], "title": "S2-UniSeg: Fast Universal Agglomerative Pooling for Scalable Segment Anything without Supervision", "comment": null, "summary": "Recent self-supervised image segmentation models have achieved promising\nperformance on semantic segmentation and class-agnostic instance segmentation.\nHowever, their pretraining schedule is multi-stage, requiring a time-consuming\npseudo-masks generation process between each training epoch. This\ntime-consuming offline process not only makes it difficult to scale with\ntraining dataset size, but also leads to sub-optimal solutions due to its\ndiscontinuous optimization routine. To solve these, we first present a novel\npseudo-mask algorithm, Fast Universal Agglomerative Pooling (UniAP). Each layer\nof UniAP can identify groups of similar nodes in parallel, allowing to generate\nboth semantic-level and instance-level and multi-granular pseudo-masks within\nens of milliseconds for one image. Based on the fast UniAP, we propose the\nScalable Self-Supervised Universal Segmentation (S2-UniSeg), which employs a\nstudent and a momentum teacher for continuous pretraining. A novel\nsegmentation-oriented pretext task, Query-wise Self-Distillation (QuerySD), is\nproposed to pretrain S2-UniSeg to learn the local-to-global correspondences.\nUnder the same setting, S2-UniSeg outperforms the SOTA UnSAM model, achieving\nnotable improvements of AP+6.9 on COCO, AR+11.1 on UVO, PixelAcc+4.5 on\nCOCOStuff-27, RQ+8.0 on Cityscapes. After scaling up to a larger 2M-image\nsubset of SA-1B, S2-UniSeg further achieves performance gains on all four\nbenchmarks. Our code and pretrained models are available at\nhttps://github.com/bio-mlhui/S2-UniSeg", "AI": {"tldr": "S2-UniSeg\u901a\u8fc7UniAP\uff08\u5feb\u901f\u4f2a\u63a9\u7801\u751f\u6210\uff09\u548cQuerySD\uff08\u81ea\u76d1\u7763\u9884\u8bad\u7ec3\uff09\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u5206\u5272\uff0c\u6027\u80fd\u4f18\u4e8eUnSAM\uff0c\u5e76\u5728COCO\u3001UVO\u3001COCOStuff-27\u548cCityscapes\u4e0a\u53d6\u5f97\u663e\u8457\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u56fe\u50cf\u5206\u5272\u6a21\u578b\u867d\u7136\u6027\u80fd\u4f18\u5f02\uff0c\u4f46\u5176\u9884\u8bad\u7ec3\u6d41\u7a0b\u591a\u9636\u6bb5\u4e14\u8017\u65f6\uff0c\u6d89\u53ca\u590d\u6742\u7684\u4f2a\u63a9\u7801\u751f\u6210\u8fc7\u7a0b\uff0c\u96be\u4ee5\u6269\u5c55\u4e14\u4f18\u5316\u4e0d\u8fde\u7eed\uff0c\u5bfc\u81f4\u6b21\u4f18\u89e3\u3002\u4f5c\u8005\u5e0c\u671b\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u63d0\u9ad8\u9884\u8bad\u7ec3\u6548\u7387\u548c\u5206\u5272\u6027\u80fd\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aFast Universal Agglomerative Pooling\uff08UniAP\uff09\u7684\u65b0\u578b\u4f2a\u63a9\u7801\u7b97\u6cd5\uff0c\u80fd\u591f\u5e76\u884c\u5730\u8bc6\u522b\u76f8\u4f3c\u8282\u70b9\u7ec4\uff0c\u5e76\u5728\u51e0\u6beb\u79d2\u5185\u751f\u6210\u8bed\u4e49\u7ea7\u3001\u5b9e\u4f8b\u7ea7\u548c\u591a\u7c92\u5ea6\u7684\u4f2a\u63a9\u7801\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u901a\u7528\u5206\u5272\uff08S2-UniSeg\uff09\u6846\u67b6\uff0c\u91c7\u7528\u4e86student-teacher\u6a21\u5f0f\u4ee5\u53ca\u67e5\u8be2\u611f\u77e5\u7684\u81ea\u84b8\u998f\uff08QuerySD\uff09\u7684\u9884\u8bad\u7ec3\u4efb\u52a1\uff0c\u4ee5\u5b66\u4e60\u5c40\u90e8\u5230\u5168\u5c40\u7684\u5bf9\u5e94\u5173\u7cfb\u3002", "result": "S2-UniSeg\u5728COCO\u6570\u636e\u96c6\u4e0aAP\u63d0\u53476.9%\uff0c\u5728UVO\u6570\u636e\u96c6\u4e0aAR\u63d0\u534711.1%\uff0c\u5728COCOStuff-27\u6570\u636e\u96c6\u4e0aPixelAcc\u63d0\u53474.5%\uff0c\u5728Cityscapes\u6570\u636e\u96c6\u4e0aRQ\u63d0\u53478.0%\uff0c\u4f18\u4e8eSOTA\u6a21\u578bUnSAM\u3002\u5c06\u6a21\u578b\u6269\u5c55\u5230SA-1B\u76842M\u56fe\u50cf\u5b50\u96c6\u540e\uff0c\u5728\u6240\u6709\u56db\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u6027\u80fd\u5747\u6709\u8fdb\u4e00\u6b65\u63d0\u5347\u3002", "conclusion": "S2-UniSeg\u901a\u8fc7UniAP\u548cQuerySD\uff0c\u5b9e\u73b0\u4e86\u53ef\u6269\u5c55\u7684\u81ea\u76d1\u7763\u901a\u7528\u5206\u5272\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4f18\u4e8eSOTA\u6a21\u578bUnSAM\u3002"}}
{"id": "2508.08127", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08127", "abs": "https://arxiv.org/abs/2508.08127", "authors": ["Rui Miao", "Yixin Liu", "Yili Wang", "Xu Shen", "Yue Tan", "Yiwei Dai", "Shirui Pan", "Xin Wang"], "title": "BlindGuard: Safeguarding LLM-based Multi-Agent Systems under Unknown Attacks", "comment": null, "summary": "The security of LLM-based multi-agent systems (MAS) is critically threatened\nby propagation vulnerability, where malicious agents can distort collective\ndecision-making through inter-agent message interactions. While existing\nsupervised defense methods demonstrate promising performance, they may be\nimpractical in real-world scenarios due to their heavy reliance on labeled\nmalicious agents to train a supervised malicious detection model. To enable\npractical and generalizable MAS defenses, in this paper, we propose BlindGuard,\nan unsupervised defense method that learns without requiring any\nattack-specific labels or prior knowledge of malicious behaviors. To this end,\nwe establish a hierarchical agent encoder to capture individual, neighborhood,\nand global interaction patterns of each agent, providing a comprehensive\nunderstanding for malicious agent detection. Meanwhile, we design a\ncorruption-guided detector that consists of directional noise injection and\ncontrastive learning, allowing effective detection model training solely on\nnormal agent behaviors. Extensive experiments show that BlindGuard effectively\ndetects diverse attack types (i.e., prompt injection, memory poisoning, and\ntool attack) across MAS with various communication patterns while maintaining\nsuperior generalizability compared to supervised baselines. The code is\navailable at: https://github.com/MR9812/BlindGuard.", "AI": {"tldr": "BlindGuard \u662f\u4e00\u79cd\u65e0\u76d1\u7763\u7684 LLM MAS \u5b89\u5168\u9632\u5fa1\u65b9\u6cd5\uff0c\u901a\u8fc7\u5206\u5c42\u7f16\u7801\u548c\u8150\u8d25\u5f15\u5bfc\u68c0\u6d4b\u6765\u68c0\u6d4b\u6076\u610f\u4ee3\u7406\uff0c\u65e0\u9700\u5148\u9a8c\u77e5\u8bc6\u5373\u53ef\u5b9e\u73b0\u9ad8\u6548\u7684\u653b\u51fb\u68c0\u6d4b\u548c\u6cdb\u5316\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u76d1\u7763\u7684\u9632\u5fa1\u65b9\u6cd5\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u53ef\u80fd\u4e0d\u5207\u5b9e\u9645\uff0c\u56e0\u4e3a\u5b83\u4eec\u4e25\u91cd\u4f9d\u8d56\u6807\u8bb0\u7684\u6076\u610f\u4ee3\u7406\u6765\u8bad\u7ec3\u76d1\u7763\u6076\u610f\u68c0\u6d4b\u6a21\u578b\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65e0\u9700\u653b\u51fb\u7279\u5b9a\u6807\u7b7e\u6216\u6076\u610f\u884c\u4e3a\u5148\u9a8c\u77e5\u8bc6\u7684\u65e0\u76d1\u7763\u9632\u5fa1\u65b9\u6cd5\u3002", "method": "BlindGuard \u901a\u8fc7\u5206\u5c42\u4ee3\u7406\u7f16\u7801\u5668\u6355\u83b7\u4e2a\u4f53\u3001\u90bb\u57df\u548c\u5168\u5c40\u4ea4\u4e92\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4ee5\u8150\u8d25\u4e3a\u6307\u5bfc\u7684\u68c0\u6d4b\u5668\uff0c\u5305\u62ec\u5b9a\u5411\u566a\u58f0\u6ce8\u5165\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ec5\u4f7f\u7528\u6b63\u5e38\u4ee3\u7406\u884c\u4e3a\u5373\u53ef\u6709\u6548\u8bad\u7ec3\u68c0\u6d4b\u6a21\u578b\u3002", "result": "BlindGuard \u5728\u5404\u79cd\u901a\u4fe1\u6a21\u5f0f\u7684 MAS \u4e2d\u6709\u6548\u5730\u68c0\u6d4b\u4e86\u5404\u79cd\u653b\u51fb\u7c7b\u578b\uff08\u5373\u63d0\u793a\u6ce8\u5165\u3001\u5185\u5b58\u4e2d\u6bd2\u548c\u5de5\u5177\u653b\u51fb\uff09\uff0c\u4e0e\u76d1\u7763\u57fa\u7ebf\u76f8\u6bd4\uff0c\u4fdd\u6301\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "BlindGuard \u662f\u4e00\u79cd\u65e0\u76d1\u7763\u9632\u5fa1\u65b9\u6cd5\uff0c\u65e0\u9700\u653b\u51fb\u7279\u5b9a\u7684\u6807\u7b7e\u6216\u6076\u610f\u884c\u4e3a\u7684\u5148\u9a8c\u77e5\u8bc6\uff0c\u5373\u53ef\u6709\u6548\u68c0\u6d4b\u5404\u79cd\u7c7b\u578b\u7684\u653b\u51fb\uff0c\u5e76\u5177\u6709\u51fa\u8272\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07805", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07805", "abs": "https://arxiv.org/abs/2508.07805", "authors": ["Yerin Hwang", "Dongryeol Lee", "Taegwan Kang", "Yongil Kim", "Kyomin Jung"], "title": "Can You Trick the Grader? Adversarial Persuasion of LLM Judges", "comment": "19 pages, 8 figures", "summary": "As large language models take on growing roles as automated evaluators in\npractical settings, a critical question arises: Can individuals persuade an LLM\njudge to assign unfairly high scores? This study is the first to reveal that\nstrategically embedded persuasive language can bias LLM judges when scoring\nmathematical reasoning tasks, where correctness should be independent of\nstylistic variation. Grounded in Aristotle's rhetorical principles, we\nformalize seven persuasion techniques (Majority, Consistency, Flattery,\nReciprocity, Pity, Authority, Identity) and embed them into otherwise identical\nresponses. Across six math benchmarks, we find that persuasive language leads\nLLM judges to assign inflated scores to incorrect solutions, by up to 8% on\naverage, with Consistency causing the most severe distortion. Notably,\nincreasing model size does not substantially mitigate this vulnerability.\nFurther analysis demonstrates that combining multiple persuasion techniques\namplifies the bias, and pairwise evaluation is likewise susceptible. Moreover,\nthe persuasive effect persists under counter prompting strategies, highlighting\na critical vulnerability in LLM-as-a-Judge pipelines and underscoring the need\nfor robust defenses against persuasion-based attacks.", "AI": {"tldr": "\u5373\u4f7f\u662fAI\u6cd5\u5b98\uff0c\u4e5f\u53ef\u80fd\u88ab\u82b1\u8a00\u5de7\u8bed\u6240\u8499\u853d\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5728\u6570\u5b66\u8bc4\u5206\u4e2d\u52a0\u5165\u201c\u529d\u8bf4\u6027\u8bed\u8a00\u201d\u80fd\u63d0\u9ad8\u4e0d\u6b63\u786e\u7b54\u6848\u7684\u5f97\u5206\uff0c\u5c24\u5176\u662f\u201c\u4e00\u81f4\u6027\u201d\u6280\u5de7\u6548\u679c\u663e\u8457\u3002\u6a21\u578b\u8d8a\u5927\u8d8a\u6ca1\u7528\uff0c\u8d8a\u591a\u6280\u5de7\u8d8a\u7cdf\u7cd5\u3002AI\u6cd5\u5b98\u9700\u8981\u52a0\u5f3a\u81ea\u8eab\u201c\u514d\u75ab\u529b\u201d\u3002", "motivation": "\u968f\u7740LLM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u626e\u6f14\u7684\u81ea\u52a8\u8bc4\u4f30\u89d2\u8272\u65e5\u76ca\u91cd\u8981\uff0c\u7814\u7a76\u5176\u662f\u5426\u4f1a\u88ab\u4e0d\u516c\u5e73\u5730\u5f71\u54cd\u8bc4\u5206\u81f3\u5173\u91cd\u8981\uff0c\u7279\u522b\u662f\u5f53\u8bc4\u5206\u5e94\u4e0e\u98ce\u683c\u65e0\u5173\u65f6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e03\u79cd\u57fa\u4e8e\u4e9a\u91cc\u58eb\u591a\u5fb7\u4fee\u8f9e\u5b66\u539f\u7406\u7684\u529d\u8bf4\u6280\u5de7\uff08\u591a\u6570\u3001\u4e00\u81f4\u6027\u3001\u5949\u627f\u3001\u4e92\u60e0\u3001\u540c\u60c5\u3001\u6743\u5a01\u3001\u8eab\u4efd\u8ba4\u540c\uff09\uff0c\u5e76\u5c06\u5176\u5d4c\u5165\u5230\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u7684\u56de\u7b54\u4e2d\uff0c\u7528\u4e8e\u6d4b\u8bd5LLM\u8bc4\u5206\u8005\u3002", "result": "\u5728\u516d\u4e2a\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u529d\u8bf4\u6027\u8bed\u8a00\u80fd\u4f7fLLM\u8bc4\u5206\u8005\u5bf9\u4e0d\u6b63\u786e\u7684\u7b54\u6848\u7ed9\u51fa\u66f4\u9ad8\u7684\u5206\u6570\uff0c\u5e73\u5747\u9ad8\u51fa8%\u3002\u5176\u4e2d\u201c\u4e00\u81f4\u6027\u201d\u6280\u5de7\u9020\u6210\u7684\u504f\u5dee\u6700\u5927\u3002\u6a21\u578b\u89c4\u6a21\u7684\u589e\u5927\u548c\u6210\u5bf9\u8bc4\u4f30\u5747\u672a\u80fd\u6709\u6548\u51cf\u8f7b\u6b64\u95ee\u9898\u3002\u7ed3\u5408\u591a\u79cd\u529d\u8bf4\u6280\u5de7\u4f1a\u653e\u5927\u504f\u5dee\uff0c\u4e14\u5bf9\u6297\u6027\u63d0\u793a\u4e5f\u65e0\u6cd5\u6d88\u9664\u5f71\u54cd\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u63ed\u793a\u4e86\u5728\u6570\u5b66\u63a8\u7406\u4efb\u52a1\u4e2d\uff0c\u7b56\u7565\u6027\u5d4c\u5165\u7684\u529d\u8bf4\u6027\u8bed\u8a00\u4f1a\u5f71\u54cdLLM\u8bc4\u5206\u8005\u7684\u5224\u65ad\uff0c\u5bfc\u81f4\u8bc4\u5206\u4e0d\u516c\u5e73\u3002\u8fd9\u79cd\u5f71\u54cd\u5728\u591a\u79cd\u6570\u5b66\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u6709\u4f53\u73b0\uff0c\u5e73\u5747\u4f1a\u4f7f\u4e0d\u6b63\u786e\u7b54\u6848\u7684\u8bc4\u5206\u63d0\u9ad88%\uff0c\u5176\u4e2d\u201c\u4e00\u81f4\u6027\u201d\u6280\u5de7\u9020\u6210\u7684\u626d\u66f2\u6700\u4e3a\u4e25\u91cd\u3002\u589e\u52a0\u6a21\u578b\u89c4\u6a21\u5e76\u672a\u663e\u8457\u7f13\u89e3\u6b64\u6f0f\u6d1e\u3002\u6b64\u5916\uff0c\u7ed3\u5408\u591a\u79cd\u529d\u8bf4\u6280\u5de7\u4f1a\u52a0\u5267\u504f\u5dee\uff0c\u6210\u5bf9\u8bc4\u4f30\u4e5f\u672a\u80fd\u5e78\u514d\u3002\u5373\u4f7f\u5728\u5bf9\u6297\u6027\u63d0\u793a\u7b56\u7565\u4e0b\uff0c\u529d\u8bf4\u6548\u5e94\u4f9d\u7136\u5b58\u5728\uff0c\u8fd9\u8868\u660eLLM\u4f5c\u4e3a\u8bc4\u4f30\u8005\u7684\u6d41\u7a0b\u5b58\u5728\u4e25\u91cd\u6f0f\u6d1e\uff0c\u4e9f\u9700\u5f00\u53d1\u9488\u5bf9\u529d\u8bf4\u653b\u51fb\u7684\u9c81\u68d2\u9632\u5fa1\u673a\u5236\u3002"}}
{"id": "2508.07006", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07006", "abs": "https://arxiv.org/abs/2508.07006", "authors": ["Gian Mario Favero", "Ge Ya Luo", "Nima Fathi", "Justin Szeto", "Douglas L. Arnold", "Brennan Nichyporuk", "Chris Pal", "Tal Arbel"], "title": "Spatio-Temporal Conditional Diffusion Models for Forecasting Future Multiple Sclerosis Lesion Masks Conditioned on Treatments", "comment": "Accepted to MICCAI 2025 (LMID Workshop)", "summary": "Image-based personalized medicine has the potential to transform healthcare,\nparticularly for diseases that exhibit heterogeneous progression such as\nMultiple Sclerosis (MS). In this work, we introduce the first treatment-aware\nspatio-temporal diffusion model that is able to generate future masks\ndemonstrating lesion evolution in MS. Our voxel-space approach incorporates\nmulti-modal patient data, including MRI and treatment information, to forecast\nnew and enlarging T2 (NET2) lesion masks at a future time point. Extensive\nexperiments on a multi-centre dataset of 2131 patient 3D MRIs from randomized\nclinical trials for relapsing-remitting MS demonstrate that our generative\nmodel is able to accurately predict NET2 lesion masks for patients across six\ndifferent treatments. Moreover, we demonstrate our model has the potential for\nreal-world clinical applications through downstream tasks such as future lesion\ncount and location estimation, binary lesion activity classification, and\ngenerating counterfactual future NET2 masks for several treatments with\ndifferent efficacies. This work highlights the potential of causal, image-based\ngenerative models as powerful tools for advancing data-driven prognostics in\nMS.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u6269\u6563\u6a21\u578b\u53ef\u4ee5\u6839\u636e MRI \u548c\u6cbb\u7597\u6570\u636e\u9884\u6d4b\u591a\u53d1\u6027\u786c\u5316\u75c7\u7684\u75c5\u53d8\u8fdb\u5c55\u3002", "motivation": "\u4e3a\u4e86\u5229\u7528\u57fa\u4e8e\u56fe\u50cf\u7684\u4e2a\u6027\u5316\u533b\u7597\u7684\u6f5c\u529b\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u50cf\u591a\u53d1\u6027\u786c\u5316\u75c7 (MS) \u8fd9\u6837\u8fdb\u5c55\u5f02\u8d28\u6027\u75be\u75c5\u7684\u533b\u7597\u4fdd\u5065\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9996\u521b\u7684\u3001\u611f\u77e5\u65f6\u95f4\u7684\u6269\u6563\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u663e\u793a MS \u4e2d\u75c5\u53d8\u8fdb\u5c55\u7684\u672a\u6765\u63a9\u7801\u3002\u8be5\u6a21\u578b\u91c7\u7528\u4f53\u7d20\u7a7a\u95f4\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u5305\u62ec MRI \u548c\u6cbb\u7597\u4fe1\u606f\u5728\u5185\u7684\u591a\u6a21\u6001\u60a3\u8005\u6570\u636e\uff0c\u4ee5\u9884\u6d4b\u672a\u6765\u65f6\u95f4\u70b9\u7684\u65b0\u7684\u548c\u6269\u5927\u7684 T2 (NET2) \u75c5\u53d8\u63a9\u7801\u3002", "result": "\u5728\u6765\u81ea\u590d\u53d1\u7f13\u89e3\u578b MS \u7684\u968f\u673a\u4e34\u5e8a\u8bd5\u9a8c\u7684 2131 \u540d\u60a3\u8005\u7684 3D MRI \u7684\u591a\u4e2d\u5fc3\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u751f\u6210\u6a21\u578b\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u8de8\u516d\u79cd\u4e0d\u540c\u6cbb\u7597\u7684\u60a3\u8005\u7684 NET2 \u75c5\u53d8\u63a9\u7801\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8bc1\u660e\u4e86\u6211\u4eec\u7684\u6a21\u578b\u5177\u6709\u901a\u8fc7\u4e0b\u6e38\u4efb\u52a1\uff08\u4f8b\u5982\uff0c\u672a\u6765\u75c5\u53d8\u8ba1\u6570\u548c\u4f4d\u7f6e\u4f30\u8ba1\u3001\u4e8c\u5143\u75c5\u53d8\u6d3b\u52a8\u5206\u7c7b\u4ee5\u53ca\u4e3a\u51e0\u79cd\u5177\u6709\u4e0d\u540c\u7597\u6548\u7684\u6cbb\u7597\u751f\u6210\u53cd\u4e8b\u5b9e\u672a\u6765 NET2 \u63a9\u7801\uff09\u5b9e\u73b0\u771f\u5b9e\u4e16\u754c\u4e34\u5e8a\u5e94\u7528\u7684\u6f5c\u529b\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u5f3a\u8c03\u4e86\u56e0\u679c\u3001\u57fa\u4e8e\u56fe\u50cf\u7684\u751f\u6210\u6a21\u578b\u5728\u63a8\u8fdb MS \u6570\u636e\u9a71\u52a8\u7684\u9884\u540e\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08147", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08147", "abs": "https://arxiv.org/abs/2508.08147", "authors": ["Yunkai Hu", "Tianqiao Zhao", "Meng Yue"], "title": "From Natural Language to Solver-Ready Power System Optimization: An LLM-Assisted, Validation-in-the-Loop Framework", "comment": null, "summary": "This paper introduces a novel Large Language Models (LLMs)-assisted agent\nthat automatically converts natural-language descriptions of power system\noptimization scenarios into compact, solver-ready formulations and generates\ncorresponding solutions. In contrast to approaches that rely solely on LLM to\nproduce solutions directly, the proposed method focuses on discovering a\nmathematically compatible formulation that can be efficiently solved by\noff-the-shelf optimization solvers. Directly using LLMs to produce solutions\noften leads to infeasible or suboptimal results, as these models lack the\nnumerical precision and constraint-handling capabilities of established\noptimization solvers. The pipeline integrates a domain-aware prompt and schema\nwith an LLM, enforces feasibility through systematic validation and iterative\nrepair, and returns both solver-ready models and user-facing results. Using the\nunit commitment problem as a representative case study, the agent produces\noptimal or near-optimal schedules along with the associated objective costs.\nResults demonstrate that coupling the solver with task-specific validation\nsignificantly enhances solution reliability. This work shows that combining AI\nwith established optimization frameworks bridges high-level problem\ndescriptions and executable mathematical models, enabling more efficient\ndecision-making in energy systems", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd LLM \u8f85\u52a9\u4ee3\u7406\uff0c\u53ef\u5c06\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u7684\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u95ee\u9898\u8f6c\u6362\u4e3a\u53ef\u6c42\u89e3\u7684\u6a21\u578b\uff0c\u5e76\u786e\u4fdd\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u884c\u6027\u548c\u6700\u4f18\u6027\u3002\u4e0e\u76f4\u63a5\u4f7f\u7528 LLM \u751f\u6210\u89e3\u51b3\u65b9\u6848\u4e0d\u540c\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86 LLM \u7684\u7406\u89e3\u80fd\u529b\u548c\u4f20\u7edf\u4f18\u5316\u6c42\u89e3\u5668\u7684\u7cbe\u786e\u6027\uff0c\u901a\u8fc7\u9a8c\u8bc1\u548c\u4fee\u590d\u6765\u63d0\u9ad8\u7ed3\u679c\u7684\u53ef\u9760\u6027\u3002\u5728\u5355\u5143\u627f\u8bfa\u95ee\u9898\u4e0a\u7684\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u76f4\u63a5\u4f7f\u7528 LLM \u4ea7\u751f\u89e3\u51b3\u65b9\u6848\u53ef\u80fd\u5bfc\u81f4\u4e0d\u53ef\u884c\u6216\u6b21\u4f18\u7684\u7ed3\u679c\uff0c\u56e0\u4e3a\u8fd9\u4e9b\u6a21\u578b\u7f3a\u4e4f\u65e2\u5b9a\u4f18\u5316\u6c42\u89e3\u5668\u7684\u6570\u503c\u7cbe\u5ea6\u548c\u7ea6\u675f\u5904\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u8f85\u52a9\u4ee3\u7406\uff0c\u53ef\u81ea\u52a8\u5c06\u7535\u529b\u7cfb\u7edf\u4f18\u5316\u573a\u666f\u7684\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u8f6c\u6362\u4e3a\u7d27\u51d1\u3001\u53ef\u4f9b\u6c42\u89e3\u5668\u4f7f\u7528\u7684\u6a21\u578b\uff0c\u5e76\u751f\u6210\u76f8\u5e94\u7684\u89e3\u51b3\u65b9\u6848\u3002\u8be5\u65b9\u6cd5\u4fa7\u91cd\u4e8e\u53d1\u73b0\u6570\u5b66\u4e0a\u517c\u5bb9\u4e14\u53ef\u88ab\u73b0\u6210\u4f18\u5316\u6c42\u89e3\u5668\u6709\u6548\u6c42\u89e3\u7684\u6a21\u578b\uff0c\u5e76\u901a\u8fc7\u9886\u57df\u611f\u77e5\u63d0\u793a\u548c\u6a21\u5f0f\u96c6\u6210 LLM\uff0c\u5229\u7528\u7cfb\u7edf\u9a8c\u8bc1\u548c\u8fed\u4ee3\u4fee\u590d\u6765\u5f3a\u5236\u6267\u884c\u53ef\u884c\u6027\u3002", "result": "\u5728\u4ee5\u5355\u5143\u627f\u8bfa\u95ee\u9898\u4e3a\u4ee3\u8868\u6027\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u8be5\u4ee3\u7406\u751f\u6210\u4e86\u6700\u4f18\u6216\u63a5\u8fd1\u6700\u4f18\u7684\u8c03\u5ea6\u4ee5\u53ca\u76f8\u5173\u7684\u76ee\u6807\u6210\u672c\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u6c42\u89e3\u5668\u4e0e\u7279\u5b9a\u4efb\u52a1\u9a8c\u8bc1\u76f8\u7ed3\u5408\u53ef\u663e\u8457\u63d0\u9ad8\u89e3\u51b3\u65b9\u6848\u7684\u53ef\u9760\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u5c06\u4eba\u5de5\u667a\u80fd\u4e0e\u65e2\u5b9a\u7684\u4f18\u5316\u6846\u67b6\u76f8\u7ed3\u5408\uff0c\u53ef\u4ee5\u67b6\u8d77\u9ad8\u7ea7\u95ee\u9898\u63cf\u8ff0\u4e0e\u53ef\u6267\u884c\u6570\u5b66\u6a21\u578b\u4e4b\u95f4\u7684\u6865\u6881\uff0c\u4ece\u800c\u5728\u80fd\u6e90\u7cfb\u7edf\u4e2d\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u51b3\u7b56\u3002"}}
{"id": "2508.07224", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07224", "abs": "https://arxiv.org/abs/2508.07224", "authors": ["Ananda Prakash Verma"], "title": "EDGE: A Theoretical Framework for Misconception-Aware Adaptive Learning", "comment": null, "summary": "We present EDGE, a general-purpose, misconception-aware adaptive learning\nframework composed of four stages: Evaluate (ability and state estimation),\nDiagnose (posterior infer-ence of misconceptions), Generate (counterfactual\nitem synthesis), and Exercise (index-based retrieval scheduling). EDGE unifies\npsychometrics (IRT/Bayesian state space models), cog-nitive diagnostics\n(misconception discovery from distractor patterns and response latencies),\ncontrastive item generation (minimal perturbations that invalidate learner\nshortcuts while pre-serving psychometric validity), and principled scheduling\n(a restless bandit approximation to spaced retrieval). We formalize a composite\nreadiness metric, EdgeScore, prove its monotonicity and Lipschitz continuity,\nand derive an index policy that is near-optimal under mild assumptions on\nforgetting and learning gains. We further establish conditions under which\ncounterfactual items provably reduce the posterior probability of a targeted\nmisconception faster than standard practice. The paper focuses on theory and\nimplementable pseudocode; empirical study is left to future work.", "AI": {"tldr": "EDGE is an adaptive learning framework that uses psychometrics, cognitive diagnostics, contrastive item generation, and principled scheduling to improve learning by identifying and addressing misconceptions. It introduces the EdgeScore metric and an associated policy for near-optimal learning.", "motivation": "To develop a general-purpose, misconception-aware adaptive learning framework.", "method": "EDGE unifies psychometrics (IRT/Bayesian state space models), cognitive diagnostics (misconception discovery from distractor patterns and response latencies), contrastive item generation (minimal perturbations that invalidate learner shortcuts while pre-serving psychometric validity), and principled scheduling (a restless bandit approximation to spaced retrieval).", "result": "The paper formalizes a composite readiness metric, EdgeScore, proves its monotonicity and Lipschitz continuity, and derives an index policy that is near-optimal under mild assumptions on forgetting and learning gains. It also establishes conditions under which counterfactual items provably reduce the posterior probability of a targeted misconception faster than standard practice.", "conclusion": "EDGE is a general-purpose, misconception-aware adaptive learning framework."}}
{"id": "2508.07810", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07810", "abs": "https://arxiv.org/abs/2508.07810", "authors": ["Olga Kellert", "Muhammad Imran", "Nicholas Hill Matlis", "Mahmud Uz Zaman", "Carlos G\u00f3mez-Rodr\u00edguez"], "title": "Evaluating Compositional Approaches for Focus and Sentiment Analysis", "comment": null, "summary": "This paper summarizes the results of evaluating a compositional approach for\nFocus Analysis (FA) in Linguistics and Sentiment Analysis (SA) in Natural\nLanguage Processing (NLP). While quantitative evaluations of compositional and\nnon-compositional approaches in SA exist in NLP, similar quantitative\nevaluations are very rare in FA in Linguistics that deal with linguistic\nexpressions representing focus or emphasis such as \"it was John who left\". We\nfill this gap in research by arguing that compositional rules in SA also apply\nto FA because FA and SA are closely related meaning that SA is part of FA. Our\ncompositional approach in SA exploits basic syntactic rules such as rules of\nmodification, coordination, and negation represented in the formalism of\nUniversal Dependencies (UDs) in English and applied to words representing\nsentiments from sentiment dictionaries. Some of the advantages of our\ncompositional analysis method for SA in contrast to non-compositional analysis\nmethods are interpretability and explainability. We test the accuracy of our\ncompositional approach and compare it with a non-compositional approach VADER\nthat uses simple heuristic rules to deal with negation, coordination and\nmodification. In contrast to previous related work that evaluates\ncompositionality in SA on long reviews, this study uses more appropriate\ndatasets to evaluate compositionality. In addition, we generalize the results\nof compositional approaches in SA to compositional approaches in FA.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u7126\u70b9\u5206\u6790\uff08FA\uff09\u7684\u7ec4\u5408\u65b9\u6cd5\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u60c5\u611f\u5206\u6790\uff08SA\uff09\u4e2d\u7684\u6709\u6548\u6027\u3002\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u66f4\u597d\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u586b\u8865\u4e86\u7126\u70b9\u5206\u6790\uff08FA\uff09\u91cf\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\u3002", "motivation": "\u672c\u7814\u7a76\u7684\u52a8\u673a\u662f\u586b\u8865\u7126\u70b9\u5206\u6790\uff08FA\uff09\u5728\u8bed\u8a00\u5b66\u9886\u57df\u7f3a\u4e4f\u91cf\u5316\u8bc4\u4f30\u7684\u7a7a\u767d\uff0c\u5e76\u8bba\u8bc1\u60c5\u611f\u5206\u6790\uff08FA\uff09\u4e2d\u7684\u7ec4\u5408\u89c4\u5219\u540c\u6837\u9002\u7528\u4e8e\u7126\u70b9\u5206\u6790\uff08FA\uff09\uff0c\u56e0\u4e3a\u4e24\u8005\u5bc6\u5207\u76f8\u5173\uff0c\u60c5\u611f\u5206\u6790\uff08SA\uff09\u662f\u7126\u70b9\u5206\u6790\uff08FA\uff09\u7684\u4e00\u90e8\u5206\u3002", "method": "\u8be5\u7814\u7a76\u91c7\u7528\u4e00\u79cd\u7ec4\u5408\u65b9\u6cd5\uff0c\u5229\u7528\u901a\u7528\u4f9d\u5b58\u5173\u7cfb\uff08UDs\uff09\u4e2d\u7684\u57fa\u672c\u53e5\u6cd5\u89c4\u5219\uff08\u5982\u4fee\u9970\u3001\u534f\u8c03\u548c\u5426\u5b9a\uff09\u6765\u5206\u6790\u82f1\u6587\u6587\u672c\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e\u60c5\u611f\u8bcd\u6c47\u3002\u6240\u63d0\u51fa\u7684\u7ec4\u5408\u65b9\u6cd5\u4e0e\u975e\u7ec4\u5408\u65b9\u6cd5 VADER \u8fdb\u884c\u4e86\u51c6\u786e\u6027\u6bd4\u8f83\uff0c\u5e76\u4f7f\u7528\u4e86\u66f4\u5408\u9002\u7684\u6570\u636e\u96c6\u8fdb\u884c\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7ec4\u5408\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u65b9\u9762\u4e0e\u975e\u7ec4\u5408\u65b9\u6cd5 VADER \u76f8\u5f53\uff0c\u5e76\u4e14\u5728\u5904\u7406\u5426\u5b9a\u3001\u534f\u8c03\u548c\u4fee\u9970\u7b49\u53e5\u6cd5\u73b0\u8c61\u65f6\uff0c\u6bd4 VADER \u7b49\u542f\u53d1\u5f0f\u89c4\u5219\u65b9\u6cd5\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u8bc1\u660e\u4e86\u5c06\u60c5\u611f\u5206\u6790\uff08SA\uff09\u4e2d\u7684\u7ec4\u5408\u65b9\u6cd5\u63a8\u5e7f\u5230\u7126\u70b9\u5206\u6790\uff08FA\uff09\u662f\u53ef\u884c\u7684\u3002", "conclusion": "\u8fd9\u9879\u5de5\u4f5c\u901a\u8fc7\u5c06\u91cf\u5316\u8bc4\u4f30\u4ece\u60c5\u611f\u5206\u6790\uff08SA\uff09\u63a8\u5e7f\u5230\u7126\u70b9\u5206\u6790\uff08FA\uff09\uff0c\u5f25\u8865\u4e86\u7126\u70b9\u5206\u6790\uff08FA\uff09\u7814\u7a76\u4e2d\u7684\u7a7a\u767d\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5728\u60c5\u611f\u5206\u6790\uff08SA\uff09\u4e2d\u6709\u6548\u7684\u7ec4\u5408\u65b9\u6cd5\u4e5f\u9002\u7528\u4e8e\u7126\u70b9\u5206\u6790\uff08FA\uff09\u3002"}}
{"id": "2505.23197", "categories": ["cs.RO", "cs.AI"], "pdf": "https://arxiv.org/pdf/2505.23197", "abs": "https://arxiv.org/abs/2505.23197", "authors": ["Jatin Kumar Arora", "Shubhendu Bhasin"], "title": "UPP: Unified Path Planner with Adaptive Safety and Optimality", "comment": "8 pages,11 figures", "summary": "We are surrounded by robots helping us perform complex tasks. Robots have a\nwide range of applications, from industrial automation to personalized\nassistance. However, with great technological innovation come significant\nchallenges. One of the major challenges in robotics is path planning. Despite\nadvancements such as graph search, sampling, and potential field methods, most\npath planning algorithms focus either on optimality or on safety. Very little\nresearch addresses both simultaneously. We propose a Unified Path Planner (UPP)\nthat uses modified heuristics and a dynamic safety cost function to balance\nsafety and optimality. The level of safety can be adjusted via tunable\nparameters, trading off against computational complexity. We demonstrate the\nplanner's performance in simulations, showing how parameter variation affects\nresults. UPP is compared with various traditional and safe-optimal planning\nalgorithms across different scenarios. We also validate it on a TurtleBot,\nwhere the robot successfully finds safe and sub-optimal paths.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8def\u5f84\u89c4\u5212\u5668\uff08UPP\uff09\uff0c\u901a\u8fc7\u6539\u8fdb\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u52a8\u6001\u5b89\u5168\u6210\u672c\u51fd\u6570\uff0c\u5728\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u5e76\u80fd\u901a\u8fc7\u53c2\u6570\u8c03\u6574\u8fdb\u884c\u6743\u8861\u3002", "motivation": "\u8def\u5f84\u89c4\u5212\u662f\u673a\u5668\u4eba\u9886\u57df\u7684\u4e00\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u73b0\u6709\u7b97\u6cd5\u5927\u591a\u53ea\u5173\u6ce8\u6700\u4f18\u6027\u6216\u5b89\u5168\u6027\uff0c\u5f88\u5c11\u540c\u65f6\u89e3\u51b3\u4e24\u8005\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u8def\u5f84\u89c4\u5212\u5668\uff08UPP\uff09\uff0c\u8be5\u89c4\u5212\u5668\u4f7f\u7528\u6539\u8fdb\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u548c\u52a8\u6001\u5b89\u5168\u6210\u672c\u51fd\u6570\u6765\u5e73\u8861\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u3002", "result": "\u6a21\u62df\u7ed3\u679c\u8868\u660e\uff0cUPP \u80fd\u591f\u6210\u529f\u627e\u5230\u5b89\u5168\u4e14\u6b21\u4f18\u7684\u8def\u5f84\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u4e0e\u4f20\u7edf\u7b97\u6cd5\u76f8\u6bd4\u8868\u73b0\u66f4\u4f18\u3002\u5728 TurtleBot \u673a\u5668\u4eba\u4e0a\u7684\u9a8c\u8bc1\u4e5f\u8bc1\u5b9e\u4e86\u8fd9\u4e00\u70b9\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u8def\u5f84\u89c4\u5212\u5668\uff08UPP\uff09\uff0c\u5e76\u80fd\u5728\u5b89\u5168\u6027\u548c\u6700\u4f18\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u901a\u8fc7\u53ef\u8c03\u53c2\u6570\u53ef\u4ee5\u8c03\u6574\u5b89\u5168\u7ea7\u522b\uff0c\u5e76\u4ee5\u8ba1\u7b97\u590d\u6742\u5ea6\u4f5c\u4e3a\u6743\u8861\u3002\u5728\u6a21\u62df\u548c TurtleBot \u673a\u5668\u4eba\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cUPP \u80fd\u591f\u627e\u5230\u5b89\u5168\u4e14\u6b21\u4f18\u7684\u8def\u5f84\uff0c\u5e76\u4e14\u5728\u4e0d\u540c\u573a\u666f\u4e0b\u4f18\u4e8e\u4f20\u7edf\u7684\u8def\u5f84\u89c4\u5212\u7b97\u6cd5\u3002"}}
{"id": "2508.07243", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07243", "abs": "https://arxiv.org/abs/2508.07243", "authors": ["Chu Zhao", "Eneng Yang", "Yizhou Dang", "Jianzhe Zhao", "Guibing Guo", "Xingwei Wang"], "title": "Causal Negative Sampling via Diffusion Model for Out-of-Distribution Recommendation", "comment": "14 pages, 6 figures, Under-review", "summary": "Heuristic negative sampling enhances recommendation performance by selecting\nnegative samples of varying hardness levels from predefined candidate pools to\nguide the model toward learning more accurate decision boundaries. However, our\nempirical and theoretical analyses reveal that unobserved environmental\nconfounders (e.g., exposure or popularity biases) in candidate pools may cause\nheuristic sampling methods to introduce false hard negatives (FHNS). These\nmisleading samples can encourage the model to learn spurious correlations\ninduced by such confounders, ultimately compromising its generalization ability\nunder distribution shifts. To address this issue, we propose a novel method\nnamed Causal Negative Sampling via Diffusion (CNSDiff). By synthesizing\nnegative samples in the latent space via a conditional diffusion process,\nCNSDiff avoids the bias introduced by predefined candidate pools and thus\nreduces the likelihood of generating FHNS. Moreover, it incorporates a causal\nregularization term to explicitly mitigate the influence of environmental\nconfounders during the negative sampling process, leading to robust negatives\nthat promote out-of-distribution (OOD) generalization. Comprehensive\nexperiments under four representative distribution shift scenarios demonstrate\nthat CNSDiff achieves an average improvement of 13.96% across all evaluation\nmetrics compared to state-of-the-art baselines, verifying its effectiveness and\nrobustness in OOD recommendation tasks.", "AI": {"tldr": "CNSDiff\u901a\u8fc7\u6269\u6563\u6a21\u578b\u5728\u6f5c\u5728\u7a7a\u95f4\u5408\u6210\u8d1f\u6837\u672c\uff0c\u5e76\u52a0\u5165\u56e0\u679c\u6b63\u5219\u5316\uff0c\u89e3\u51b3\u4e86\u542f\u53d1\u5f0f\u8d1f\u91c7\u6837\u5f15\u5165\u7684\u865a\u5047\u56f0\u96be\u8d1f\u6837\u672c\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u8350\u6a21\u578b\u5728\u5206\u5e03\u5916\u573a\u666f\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u542f\u53d1\u5f0f\u8d1f\u91c7\u6837\u65b9\u6cd5\u5728\u63a8\u8350\u7cfb\u7edf\u4e2d\u53ef\u80fd\u5f15\u5165\u865a\u5047\u56f0\u96be\u8d1f\u6837\u672c\uff08FHNS\uff09\uff0c\u8fd9\u4e9b\u6837\u672c\u7531\u5019\u9009\u96c6\u4e2d\u5b58\u5728\u7684\u672a\u89c2\u6d4b\u73af\u5883\u6df7\u6dc6\u56e0\u7d20\uff08\u5982\u66dd\u5149\u6216\u6d41\u884c\u5ea6\u504f\u5dee\uff09\u5f15\u8d77\uff0c\u8fdb\u800c\u5bfc\u81f4\u6a21\u578b\u5b66\u4e60\u5230\u9519\u8bef\u7684\u5173\u8054\uff0c\u635f\u5bb3\u5176\u5728\u5206\u5e03\u504f\u79fb\u4e0b\u7684\u6cdb\u5316\u80fd\u529b\u3002", "method": "CNSDiff\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u5229\u7528\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u6765\u5408\u6210\u8d1f\u6837\u672c\uff0c\u5e76\u5f15\u5165\u56e0\u679c\u6b63\u5219\u5316\u9879\u6765\u5904\u7406\u73af\u5883\u6df7\u6dc6\u56e0\u7d20\uff0c\u4ee5\u751f\u6210\u66f4\u9c81\u68d2\u7684\u8d1f\u6837\u672c\u3002", "result": "CNSDiff\u5728\u56db\u4e2a\u4ee3\u8868\u6027\u7684\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u5168\u9762\u5b9e\u9a8c\uff0c\u7ed3\u679c\u663e\u793a\u5176\u5728\u6240\u6709\u8bc4\u4f30\u6307\u6807\u4e0a\u7684\u5e73\u5747\u8868\u73b0\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e8613.96%\uff0c\u9a8c\u8bc1\u4e86\u5176\u5728OOD\u63a8\u8350\u4efb\u52a1\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "CNSDiff\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u4e2d\u901a\u8fc7\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u5408\u6210\u8d1f\u6837\u672c\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u542f\u53d1\u5f0f\u8d1f\u91c7\u6837\u65b9\u6cd5\u4e2d\u5b58\u5728\u7684\u865a\u5047\u56f0\u96be\u8d1f\u6837\u672c\uff08FHNS\uff09\u95ee\u9898\uff0c\u5e76\u7ed3\u5408\u56e0\u679c\u6b63\u5219\u5316\u9879\u6765\u7f13\u89e3\u73af\u5883\u6df7\u6dc6\u56e0\u7d20\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u5206\u5e03\u5916\uff08OOD\uff09\u63a8\u8350\u4efb\u52a1\u4e0a\u7684\u6cdb\u5316\u80fd\u529b\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCNSDiff\u5728\u591a\u79cd\u5206\u5e03\u504f\u79fb\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u5e73\u5747\u63d0\u5347\u8fbe13.96%\u3002"}}
{"id": "2508.07626", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07626", "abs": "https://arxiv.org/abs/2508.07626", "authors": ["Dejie Yang", "Zijing Zhao", "Yang Liu"], "title": "AR-VRM: Imitating Human Motions for Visual Robot Manipulation with Analogical Reasoning", "comment": "Accepted by ICCV2025", "summary": "Visual Robot Manipulation (VRM) aims to enable a robot to follow natural\nlanguage instructions based on robot states and visual observations, and\ntherefore requires costly multi-modal data. To compensate for the deficiency of\nrobot data, existing approaches have employed vision-language pretraining with\nlarge-scale data. However, they either utilize web data that differs from\nrobotic tasks, or train the model in an implicit way (e.g., predicting future\nframes at the pixel level), thus showing limited generalization ability under\ninsufficient robot data. In this paper, we propose to learn from large-scale\nhuman action video datasets in an explicit way (i.e., imitating human actions\nfrom hand keypoints), introducing Visual Robot Manipulation with Analogical\nReasoning (AR-VRM). To acquire action knowledge explicitly from human action\nvideos, we propose a keypoint Vision-Language Model (VLM) pretraining scheme,\nenabling the VLM to learn human action knowledge and directly predict human\nhand keypoints. During fine-tuning on robot data, to facilitate the robotic arm\nin imitating the action patterns of human motions, we first retrieve human\naction videos that perform similar manipulation tasks and have similar\nhistorical observations , and then learn the Analogical Reasoning (AR) map\nbetween human hand keypoints and robot components. Taking advantage of focusing\non action keypoints instead of irrelevant visual cues, our method achieves\nleading performance on the CALVIN benchmark {and real-world experiments}. In\nfew-shot scenarios, our AR-VRM outperforms previous methods by large margins ,\nunderscoring the effectiveness of explicitly imitating human actions under data\nscarcity.", "AI": {"tldr": "\u4e00\u79cd\u540d\u4e3aAR-VRM\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5b66\u4e60\u4eba\u7c7b\u52a8\u4f5c\u7684\u5173\u952e\u70b9\u5e76\u8fdb\u884c\u7c7b\u6bd4\u63a8\u7406\uff0c\u63d0\u9ad8\u4e86\u673a\u5668\u4eba\u64cd\u7eb5\u5728\u6570\u636e\u7a00\u7f3a\u60c5\u51b5\u4e0b\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u673a\u5668\u4eba\u64cd\u7eb5\u65b9\u6cd5\u4f9d\u8d56\u6602\u8d35\u7684\u673a\u5668\u4eba\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u4f7f\u7528\u9690\u5f0f\u65b9\u6cd5\uff08\u5982\u50cf\u7d20\u7ea7\u9884\u6d4b\uff09\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5bfc\u81f4\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002\u8be5\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u5229\u7528\u5927\u89c4\u6a21\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\u6570\u636e\u96c6\u5e76\u663e\u5f0f\u5b66\u4e60\u4eba\u7c7b\u52a8\u4f5c\u6765\u89e3\u51b3\u6570\u636e\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAR-VRM\u7684\u89c6\u89c9\u673a\u5668\u4eba\u64cd\u7eb5\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u5173\u952e\u70b9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u9884\u8bad\u7ec3\u65b9\u6848\u6765\u660e\u786e\u5b66\u4e60\u4eba\u7c7b\u52a8\u4f5c\u77e5\u8bc6\u5e76\u9884\u6d4b\u624b\u90e8\u5173\u952e\u70b9\u3002\u5728\u673a\u5668\u4eba\u6570\u636e\u5fae\u8c03\u9636\u6bb5\uff0c\u8be5\u65b9\u6cd5\u68c0\u7d22\u76f8\u4f3c\u7684\u4eba\u7c7b\u52a8\u4f5c\u89c6\u9891\uff0c\u5e76\u5b66\u4e60\u4eba\u7c7b\u624b\u90e8\u5173\u952e\u70b9\u4e0e\u673a\u5668\u4eba\u90e8\u4ef6\u4e4b\u95f4\u7684\u7c7b\u6bd4\u63a8\u7406\uff08AR\uff09\u6620\u5c04\uff0c\u4ee5\u6a21\u4eff\u4eba\u7c7b\u8fd0\u52a8\u6a21\u5f0f\u3002", "result": "AR-VRM\u5728CALVIN\u57fa\u51c6\u6d4b\u8bd5\u548c\u771f\u5b9e\u4e16\u754c\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5c11\u6837\u672c\u573a\u666f\u4e0b\uff0c\u5176\u6027\u80fd\u660e\u663e\u4f18\u4e8e\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5728\u6570\u636e\u7a00\u758f\u60c5\u51b5\u4e0b\u663e\u5f0f\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\u7684\u6709\u6548\u6027\u3002", "conclusion": "AR-VRM\u901a\u8fc7\u663e\u5f0f\u6a21\u4eff\u4eba\u7c7b\u52a8\u4f5c\uff08\u4f8b\u5982\uff0c\u901a\u8fc7\u624b\u90e8\u5173\u952e\u70b9\uff09\u5e76\u5229\u7528\u7c7b\u6bd4\u63a8\u7406\uff08AR\uff09\u6765\u5b66\u4e60\u673a\u5668\u4eba\u64cd\u7eb5\u4efb\u52a1\uff0c\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u51fa\u8272\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.07827", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07827", "abs": "https://arxiv.org/abs/2508.07827", "authors": ["Yu-Min Tseng", "Wei-Lin Chen", "Chung-Chi Chen", "Hsin-Hsi Chen"], "title": "Evaluating Large Language Models as Expert Annotators", "comment": "Accepted to COLM 2025", "summary": "Textual data annotation, the process of labeling or tagging text with\nrelevant information, is typically costly, time-consuming, and labor-intensive.\nWhile large language models (LLMs) have demonstrated their potential as direct\nalternatives to human annotators for general domains natural language\nprocessing (NLP) tasks, their effectiveness on annotation tasks in domains\nrequiring expert knowledge remains underexplored. In this paper, we\ninvestigate: whether top-performing LLMs, which might be perceived as having\nexpert-level proficiency in academic and professional benchmarks, can serve as\ndirect alternatives to human expert annotators? To this end, we evaluate both\nindividual LLMs and multi-agent approaches across three highly specialized\ndomains: finance, biomedicine, and law. Specifically, we propose a multi-agent\ndiscussion framework to simulate a group of human annotators, where LLMs are\ntasked to engage in discussions by considering others' annotations and\njustifications before finalizing their labels. Additionally, we incorporate\nreasoning models (e.g., o3-mini) to enable a more comprehensive comparison. Our\nempirical results reveal that: (1) Individual LLMs equipped with inference-time\ntechniques (e.g., chain-of-thought (CoT), self-consistency) show only marginal\nor even negative performance gains, contrary to prior literature suggesting\ntheir broad effectiveness. (2) Overall, reasoning models do not demonstrate\nstatistically significant improvements over non-reasoning models in most\nsettings. This suggests that extended long CoT provides relatively limited\nbenefits for data annotation in specialized domains. (3) Certain model\nbehaviors emerge in the multi-agent discussion environment. For instance,\nClaude 3.7 Sonnet with thinking rarely changes its initial annotations, even\nwhen other agents provide correct annotations or valid reasoning.", "AI": {"tldr": "\u672c\u6587\u7814\u7a76\u4e86 LLM \u5728\u91d1\u878d\u3001\u751f\u7269\u533b\u5b66\u548c\u6cd5\u5f8b\u7b49\u4e13\u4e1a\u9886\u57df\u7684\u6587\u672c\u6807\u6ce8\u80fd\u529b\u3002\u7ed3\u679c\u53d1\u73b0\uff0c\u5355\u72ec\u7684 LLM\uff08\u5373\u4f7f\u4f7f\u7528 CoT \u7b49\u6280\u672f\uff09\u6548\u679c\u6709\u9650\uff0c\u63a8\u7406\u6a21\u578b\u4e5f\u6ca1\u6709\u5e26\u6765\u663e\u8457\u63d0\u5347\u3002\u591a\u4e3b\u4f53\u8ba8\u8bba\u4e2d\uff0cClaude 3.7 Sonnet \u4e5f\u5f88\u5c11\u4fee\u6b63\u5176\u521d\u59cb\u6807\u6ce8\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u63a2\u8ba8\u9876\u5c16 LLM \u662f\u5426\u80fd\u76f4\u63a5\u66ff\u4ee3\u4eba\u7c7b\u4e13\u5bb6\u6807\u6ce8\u5458\uff0c\u7279\u522b\u662f\u5728\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\u7684\u4e13\u4e1a\u9886\u57df\u6587\u672c\u6570\u636e\u6807\u6ce8\u65b9\u9762\uff0c\u5c3d\u7ba1 LLM \u5728\u901a\u7528\u9886\u57df\u5df2\u663e\u793a\u51fa\u5176\u6f5c\u529b\u3002", "method": "\u672c\u6587\u8bc4\u4f30\u4e86\u5355\u4e2a LLM \u548c\u591a\u4e3b\u4f53\u65b9\u6cd5\u5728\u91d1\u878d\u3001\u751f\u7269\u533b\u5b66\u548c\u6cd5\u5f8b\u4e09\u4e2a\u9ad8\u5ea6\u4e13\u4e1a\u5316\u9886\u57df\u7684\u8868\u73b0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u4e3b\u4f53\u8ba8\u8bba\u6846\u67b6\uff0c\u6a21\u62df\u4e00\u7fa4\u4eba\u7c7b\u6807\u6ce8\u5458\uff0c\u8ba9 LLM \u5728\u6700\u7ec8\u786e\u5b9a\u6807\u7b7e\u524d\uff0c\u901a\u8fc7\u8003\u8651\u4ed6\u4eba\u7684\u6807\u6ce8\u548c\u7406\u7531\u6765\u8fdb\u884c\u8ba8\u8bba\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u5f15\u5165\u4e86\u63a8\u7406\u6a21\u578b\uff08\u4f8b\u5982 o3-mini\uff09\u4ee5\u5b9e\u73b0\u66f4\u5168\u9762\u7684\u6bd4\u8f83\u3002", "result": "1. \u91c7\u7528\u63a8\u7406\u6280\u672f\uff08\u5982\u601d\u7ef4\u94fe\u3001\u81ea\u6d3d\uff09\u7684 LLM \u5728\u4e13\u4e1a\u9886\u57df\u6587\u672c\u6807\u6ce8\u4e0a\u7684\u8868\u73b0\u4ec5\u6709\u8fb9\u9645\u6539\u8fdb\u6216\u8d1f\u5411\u589e\u76ca\u3002\n2. \u63a8\u7406\u6a21\u578b\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5e76\u672a\u6bd4\u975e\u63a8\u7406\u6a21\u578b\u8868\u73b0\u51fa\u7edf\u8ba1\u5b66\u4e0a\u7684\u663e\u8457\u6539\u8fdb\u3002\n3. \u5728\u591a\u4e3b\u4f53\u8ba8\u8bba\u73af\u5883\u4e2d\uff0c\u89c2\u5bdf\u5230 Claude 3.7 Sonnet\uff08\u5f00\u542f\u601d\u8003\u529f\u80fd\u540e\uff09\u5373\u4f7f\u5728\u5176\u4ed6\u6807\u6ce8\u5458\u63d0\u4f9b\u6b63\u786e\u6807\u6ce8\u6216\u6709\u6548\u63a8\u7406\u65f6\uff0c\u4e5f\u6781\u5c11\u6539\u53d8\u5176\u521d\u59cb\u6807\u6ce8\u3002", "conclusion": "\u5728\u4e13\u4e1a\u9886\u57df\uff0c\u5355\u72ec\u7684 LLM\uff08\u5373\u4f7f\u91c7\u7528\u601d\u7ef4\u94fe\u6216\u81ea\u6d3d\u7b49\u63a8\u7406\u6280\u672f\uff09\u5728\u6587\u672c\u6807\u6ce8\u65b9\u9762\u7684\u8868\u73b0\u4ec5\u6709\u8fb9\u9645\u6539\u8fdb\uff0c\u751a\u81f3\u53ef\u80fd\u4e0b\u964d\u3002\u6b64\u5916\uff0c\u4e0e\u975e\u63a8\u7406\u6a21\u578b\u76f8\u6bd4\uff0c\u63a8\u7406\u6a21\u578b\u5e76\u672a\u5728\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u5e26\u6765\u663e\u8457\u7684\u7edf\u8ba1\u5b66\u6539\u8fdb\uff0c\u8fd9\u8868\u660e\u957f\u63a8\u7406\u94fe\u5bf9\u4e13\u4e1a\u9886\u57df\u7684\u6570\u636e\u6807\u6ce8\u6548\u76ca\u6709\u9650\u3002\u5728\u591a\u4e3b\u4f53\u8ba8\u8bba\u73af\u5883\u4e2d\uff0c\u89c2\u5bdf\u5230\u4e86\u4e00\u4e9b\u7279\u5b9a\u7684\u6a21\u578b\u884c\u4e3a\uff0c\u4f8b\u5982\uff0cClaude 3.7 Sonnet\uff08\u5728\u5f00\u542f\u601d\u8003\u529f\u80fd\u65f6\uff09\u5728\u9762\u5bf9\u5176\u4ed6\u6a21\u578b\u63d0\u4f9b\u6b63\u786e\u6807\u6ce8\u6216\u6709\u6548\u63a8\u7406\u65f6\uff0c\u5f88\u5c11\u6539\u53d8\u5176\u521d\u59cb\u6807\u6ce8\u3002"}}
{"id": "2508.07020", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07020", "abs": "https://arxiv.org/abs/2508.07020", "authors": ["Tanjim Bin Faruk", "Abdul Matin", "Shrideep Pallickara", "Sangmi Lee Pallickara"], "title": "TerraMAE: Learning Spatial-Spectral Representations from Hyperspectral Earth Observation Data via Adaptive Masked Autoencoders", "comment": null, "summary": "Hyperspectral satellite imagery offers sub-30 m views of Earth in hundreds of\ncontiguous spectral bands, enabling fine-grained mapping of soils, crops, and\nland cover. While self-supervised Masked Autoencoders excel on RGB and low-band\nmultispectral data, they struggle to exploit the intricate spatial-spectral\ncorrelations in 200+ band hyperspectral images. We introduce TerraMAE, a novel\nHSI encoding framework specifically designed to learn highly representative\nspatial-spectral embeddings for diverse geospatial analyses. TerraMAE features\nan adaptive channel grouping strategy, based on statistical reflectance\nproperties to capture spectral similarities, and an enhanced reconstruction\nloss function that incorporates spatial and spectral quality metrics. We\ndemonstrate TerraMAE's effectiveness through superior spatial-spectral\ninformation preservation in high-fidelity image reconstruction. Furthermore, we\nvalidate its practical utility and the quality of its learned representations\nthrough strong performance on three key downstream geospatial tasks: crop\nidentification, land cover classification, and soil texture prediction.", "AI": {"tldr": "TerraMAE\u662f\u4e00\u79cd\u65b0\u7684\u9ad8\u5149\u8c31\u56fe\u50cf\u7f16\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u65b9\u6cd5\uff08\u901a\u9053\u5206\u7ec4\u548c\u91cd\u5efa\u635f\u5931\uff09\u6709\u6548\u5b66\u4e60\u5149\u8c31-\u7a7a\u95f4\u8868\u5f81\uff0c\u5e76\u5728\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53d6\u5f97\u4f18\u5f02\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u81ea\u76d1\u7763\u63a9\u7801\u81ea\u52a8\u7f16\u7801\u5668\u5728\u5904\u7406\u9ad8\u5149\u8c31\u56fe\u50cf\uff08\u5177\u6709200\u591a\u4e2a\u6ce2\u6bb5\uff09\u65f6\uff0c\u96be\u4ee5\u5145\u5206\u5229\u7528\u5176\u590d\u6742\u7684\u5149\u8c31-\u7a7a\u95f4\u76f8\u5173\u6027\u3002", "method": "TerraMAE\u6846\u67b6\u91c7\u7528\u81ea\u9002\u5e94\u901a\u9053\u5206\u7ec4\u7b56\u7565\uff08\u57fa\u4e8e\u7edf\u8ba1\u53cd\u5c04\u7279\u6027\u4ee5\u6355\u6349\u5149\u8c31\u76f8\u4f3c\u6027\uff09\u548c\u5305\u542b\u7a7a\u95f4\u4e0e\u5149\u8c31\u8d28\u91cf\u5ea6\u91cf\u7684\u589e\u5f3a\u91cd\u5efa\u635f\u5931\u51fd\u6570\u3002", "result": "TerraMAE\u5728\u9ad8\u8d28\u91cf\u56fe\u50cf\u91cd\u5efa\u4e2d\u5c55\u73b0\u4e86\u4f18\u8d8a\u7684\u5149\u8c31-\u7a7a\u95f4\u4fe1\u606f\u4fdd\u6301\u80fd\u529b\u3002", "conclusion": "TerraMAE\u5728\u4e09\u4e2a\u5173\u952e\u7684\u4e0b\u6e38\u5730\u7406\u7a7a\u95f4\u4efb\u52a1\uff08\u4f5c\u7269\u8bc6\u522b\u3001\u571f\u5730\u8986\u76d6\u5206\u7c7b\u548c\u571f\u58e4\u8d28\u5730\u9884\u6d4b\uff09\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5b66\u4e60\u5230\u7684\u8868\u5f81\u7684\u5b9e\u7528\u6027\u548c\u8d28\u91cf\u3002"}}
{"id": "2508.07249", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07249", "abs": "https://arxiv.org/abs/2508.07249", "authors": ["Soumen Pachal", "Mizhaan Prajit Maniyar", "Prashanth L. A"], "title": "Policy Newton methods for Distortion Riskmetrics", "comment": null, "summary": "We consider the problem of risk-sensitive control in a reinforcement learning\n(RL) framework. In particular, we aim to find a risk-optimal policy by\nmaximizing the distortion riskmetric (DRM) of the discounted reward in a finite\nhorizon Markov decision process (MDP). DRMs are a rich class of risk measures\nthat include several well-known risk measures as special cases. We derive a\npolicy Hessian theorem for the DRM objective using the likelihood ratio method.\nUsing this result, we propose a natural DRM Hessian estimator from sample\ntrajectories of the underlying MDP. Next, we present a cubic-regularized policy\nNewton algorithm for solving this problem in an on-policy RL setting using\nestimates of the DRM gradient and Hessian. Our proposed algorithm is shown to\nconverge to an $\\epsilon$-second-order stationary point ($\\epsilon$-SOSP) of\nthe DRM objective, and this guarantee ensures the escaping of saddle points.\nThe sample complexity of our algorithms to find an $ \\epsilon$-SOSP is\n$\\mathcal{O}(\\epsilon^{-3.5})$. Our experiments validate the theoretical\nfindings. To the best of our knowledge, our is the first work to present\nconvergence to an $\\epsilon$-SOSP of a risk-sensitive objective, while existing\nworks in the literature have either shown convergence to a first-order\nstationary point of a risk-sensitive objective, or a SOSP of a risk-neutral\none.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u98ce\u9669\u654f\u611f\u63a7\u5236\u7684\u5355\u7b56\u7565 RL \u7b97\u6cd5\uff0c\u8be5\u7b97\u6cd5\u901a\u8fc7\u6700\u5927\u5316\u5931\u771f\u98ce\u9669\u5ea6\u91cf (DRM) \u6765\u5bfb\u627e\u98ce\u9669\u6700\u4f18\u7b56\u7565\uff0c\u5e76\u4fdd\u8bc1\u6536\u655b\u5230 $\\\\", "motivation": "\u672c\u6587\u65e8\u5728\u89e3\u51b3\u5f3a\u5316\u5b66\u4e60 (RL) \u6846\u67b6\u4e0b\u7684\u98ce\u9669\u654f\u611f\u63a7\u5236\u95ee\u9898\uff0c\u901a\u8fc7\u6700\u5927\u5316\u6709\u9650\u8303\u56f4\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b (MDP) \u7684\u6298\u6263\u5956\u52b1\u7684\u5931\u771f\u98ce\u9669\u5ea6\u91cf (DRM) \u6765\u5bfb\u627e\u98ce\u9669\u6700\u4f18\u7b56\u7565\u3002", "method": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f3c\u7136\u6bd4\u65b9\u6cd5\u7684 DRM \u7b56\u7565 Hessian \u5b9a\u7406\uff0c\u5e76\u57fa\u4e8e\u6b64\u63d0\u51fa\u4e86\u4e00\u79cd DRM Hessian \u4f30\u8ba1\u5668\u3002\u7136\u540e\uff0c\u5728\u5355\u7b56\u7565 RL \u8bbe\u7f6e\u4e0b\uff0c\u5229\u7528 DRM \u68af\u5ea6\u548c Hessian \u7684\u4f30\u8ba1\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u7acb\u65b9\u6b63\u5219\u5316\u7684\u7b56\u7565\u725b\u987f\u7b97\u6cd5\u6765\u89e3\u51b3\u98ce\u9669\u654f\u611f\u63a7\u5236\u95ee\u9898\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u88ab\u8bc1\u660e\u53ef\u4ee5\u6536\u655b\u5230 DRM \u76ee\u6807\u7684\u4e00\u4e2a $\\\\", "conclusion": "\u8be5\u7b97\u6cd5\u4fdd\u8bc1\u6536\u655b\u5230 DRM \u76ee\u6807\u7684\u4e00\u4e2a $\\\\"}}
{"id": "2508.07701", "categories": ["cs.CV", "cs.RO"], "pdf": "https://arxiv.org/pdf/2508.07701", "abs": "https://arxiv.org/abs/2508.07701", "authors": ["Bo Jia", "Yanan Guo", "Ying Chang", "Benkui Zhang", "Ying Xie", "Kangning Du", "Lin Cao"], "title": "Multi-view Normal and Distance Guidance Gaussian Splatting for Surface Reconstruction", "comment": "This paper has been accepted by IROS 2025", "summary": "3D Gaussian Splatting (3DGS) achieves remarkable results in the field of\nsurface reconstruction. However, when Gaussian normal vectors are aligned\nwithin the single-view projection plane, while the geometry appears reasonable\nin the current view, biases may emerge upon switching to nearby views. To\naddress the distance and global matching challenges in multi-view scenes, we\ndesign multi-view normal and distance-guided Gaussian splatting. This method\nachieves geometric depth unification and high-accuracy reconstruction by\nconstraining nearby depth maps and aligning 3D normals. Specifically, for the\nreconstruction of small indoor and outdoor scenes, we propose a multi-view\ndistance reprojection regularization module that achieves multi-view Gaussian\nalignment by computing the distance loss between two nearby views and the same\nGaussian surface. Additionally, we develop a multi-view normal enhancement\nmodule, which ensures consistency across views by matching the normals of pixel\npoints in nearby views and calculating the loss. Extensive experimental results\ndemonstrate that our method outperforms the baseline in both quantitative and\nqualitative evaluations, significantly enhancing the surface reconstruction\ncapability of 3DGS.", "AI": {"tldr": "\u63d0\u51fa\u591a\u89c6\u56fe\u6cd5\u7ebf\u548c\u8ddd\u79bb\u5f15\u5bfc\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ea6\u675f\u6df1\u5ea6\u56fe\u548c\u5bf9\u9f50\u6cd5\u7ebf\u89e3\u51b3\u591a\u89c6\u56fe\u5339\u914d\u95ee\u9898\uff0c\u63d0\u5347\u91cd\u5efa\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u591a\u89c6\u56fe\u573a\u666f\u4e2d\u7684\u8ddd\u79bb\u548c\u5168\u5c40\u5339\u914d\u6311\u6218\uff0c\u4ee5\u53ca3D\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\u5728\u6cd5\u7ebf\u5bf9\u9f50\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u89c6\u56fe\u504f\u5dee\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u89c6\u56fe\u6cd5\u7ebf\u548c\u8ddd\u79bb\u5f15\u5bfc\u7684\u9ad8\u65af\u6cfc\u6e85\u65b9\u6cd5\uff0c\u5305\u62ec\u4e00\u4e2a\u591a\u89c6\u56fe\u8ddd\u79bb\u91cd\u6295\u5f71\u6b63\u5219\u5316\u6a21\u5757\uff0c\u901a\u8fc7\u8ba1\u7b97\u4e24\u4e2a\u8fd1\u90bb\u89c6\u56fe\u4e0e\u540c\u4e00\u9ad8\u65af\u8868\u9762\u4e4b\u95f4\u7684\u8ddd\u79bb\u635f\u5931\u6765\u5b9e\u73b0\u591a\u89c6\u56fe\u9ad8\u65af\u5bf9\u9f50\uff1b\u4ee5\u53ca\u4e00\u4e2a\u591a\u89c6\u56fe\u6cd5\u7ebf\u589e\u5f3a\u6a21\u5757\uff0c\u901a\u8fc7\u5339\u914d\u8fd1\u90bb\u89c6\u56fe\u4e2d\u50cf\u7d20\u70b9\u7684\u6cd5\u7ebf\u5e76\u8ba1\u7b97\u635f\u5931\u6765\u786e\u4fdd\u89c6\u56fe\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u589e\u5f3a\u4e863DGS\u7684\u8868\u9762\u91cd\u5efa\u80fd\u529b\uff0c\u7279\u522b\u662f\u5728\u5c0f\u578b\u5ba4\u5185\u548c\u5ba4\u5916\u573a\u666f\u7684\u91cd\u5efa\u65b9\u9762\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u7ea6\u675f\u8fd1\u90bb\u6df1\u5ea6\u56fe\u548c\u5bf9\u9f503D\u6cd5\u7ebf\uff0c\u5b9e\u73b0\u4e86\u51e0\u4f55\u6df1\u5ea6\u7edf\u4e00\u548c\u9ad8\u7cbe\u5ea6\u91cd\u5efa\uff0c\u5e76\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u8bc4\u4f30\u4e2d\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\uff0c\u663e\u8457\u589e\u5f3a\u4e863DGS\u7684\u8868\u9762\u91cd\u5efa\u80fd\u529b\u3002"}}
{"id": "2508.07849", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07849", "abs": "https://arxiv.org/abs/2508.07849", "authors": ["Amrita Singh", "H. Suhan Karaca", "Aditya Joshi", "Hye-young Paik", "Jiaojiao Jiang"], "title": "LLMs for Law: Evaluating Legal-Specific LLMs on Contract Understanding", "comment": "Under review. 4 pages + references", "summary": "Despite advances in legal NLP, no comprehensive evaluation covering multiple\nlegal-specific LLMs currently exists for contract classification tasks in\ncontract understanding. To address this gap, we present an evaluation of 10\nlegal-specific LLMs on three English language contract understanding tasks and\ncompare them with 7 general-purpose LLMs. The results show that legal-specific\nLLMs consistently outperform general-purpose models, especially on tasks\nrequiring nuanced legal understanding. Legal-BERT and Contracts-BERT establish\nnew SOTAs on two of the three tasks, despite having 69% fewer parameters than\nthe best-performing general-purpose LLM. We also identify CaseLaw-BERT and\nLexLM as strong additional baselines for contract understanding. Our results\nprovide a holistic evaluation of legal-specific LLMs and will facilitate the\ndevelopment of more accurate contract understanding systems.", "AI": {"tldr": "\u6cd5\u5f8b\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u5728\u5408\u540c\u7406\u89e3\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff0c\u5e76\u5728\u4e24\u9879\u4efb\u52a1\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u3002", "motivation": "\u76ee\u524d\u7f3a\u4e4f\u6db5\u76d6\u591a\u4e2a\u6cd5\u5f8b\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u7684\u3001\u9488\u5bf9\u5408\u540c\u5206\u7c7b\u4efb\u52a1\u7684\u5168\u9762\u8bc4\u4f30\u3002", "method": "\u901a\u8fc7\u8bc4\u4f3010\u4e2a\u6cd5\u5f8b\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u548c7\u4e2a\u901a\u7528\u8bed\u8a00\u6a21\u578b\u6765\u5b8c\u6210\u4e09\u9879\u82f1\u8bed\u5408\u540c\u7406\u89e3\u4efb\u52a1\u3002", "result": "\u6cd5\u5f8b\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u5728\u5408\u540c\u7406\u89e3\u4efb\u52a1\u4e0a\u4f18\u4e8e\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff0cLegal-BERT\u548cContracts-BERT\u5728\u4e24\u9879\u4efb\u52a1\u4e0a\u8fbe\u5230\u65b0\u7684SOTA\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6cd5\u5f8b\u9886\u57df\u4e13\u7528\u8bed\u8a00\u6a21\u578b\u5728\u5408\u540c\u7406\u89e3\u4efb\u52a1\u4e0a\u59cb\u7ec8\u4f18\u4e8e\u901a\u7528\u8bed\u8a00\u6a21\u578b\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u7ec6\u81f4\u6cd5\u5f8b\u7406\u89e3\u7684\u4efb\u52a1\u4e0a\u3002Legal-BERT\u548cContracts-BERT\u5728\u4e09\u4e2a\u4efb\u52a1\u4e2d\u7684\u4e24\u4e2a\u4efb\u52a1\u4e0a\u786e\u7acb\u4e86\u65b0\u7684SOTA\uff08State-of-the-Art\uff09\uff0c\u5c3d\u7ba1\u5b83\u4eec\u7684\u53c2\u6570\u91cf\u6bd4\u8868\u73b0\u6700\u4f73\u7684\u901a\u7528\u8bed\u8a00\u6a21\u578b\u5c1169%\u3002\u6b64\u5916\uff0cCaseLaw-BERT\u548cLexLM\u4e5f\u88ab\u786e\u5b9a\u4e3a\u5408\u540c\u7406\u89e3\u7684\u6709\u529b\u57fa\u7ebf\u3002"}}
{"id": "2508.07021", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07021", "abs": "https://arxiv.org/abs/2508.07021", "authors": ["Kun Qian", "Wenjie Li", "Tianyu Sun", "Wenhong Wang", "Wenhan Luo"], "title": "DocRefine: An Intelligent Framework for Scientific Document Understanding and Content Optimization based on Multimodal Large Model Agents", "comment": null, "summary": "The exponential growth of scientific literature in PDF format necessitates\nadvanced tools for efficient and accurate document understanding,\nsummarization, and content optimization. Traditional methods fall short in\nhandling complex layouts and multimodal content, while direct application of\nLarge Language Models (LLMs) and Vision-Language Large Models (LVLMs) lacks\nprecision and control for intricate editing tasks. This paper introduces\nDocRefine, an innovative framework designed for intelligent understanding,\ncontent refinement, and automated summarization of scientific PDF documents,\ndriven by natural language instructions. DocRefine leverages the power of\nadvanced LVLMs (e.g., GPT-4o) by orchestrating a sophisticated multi-agent\nsystem comprising six specialized and collaborative agents: Layout & Structure\nAnalysis, Multimodal Content Understanding, Instruction Decomposition, Content\nRefinement, Summarization & Generation, and Fidelity & Consistency\nVerification. This closed-loop feedback architecture ensures high semantic\naccuracy and visual fidelity. Evaluated on the comprehensive DocEditBench\ndataset, DocRefine consistently outperforms state-of-the-art baselines across\nvarious tasks, achieving overall scores of 86.7% for Semantic Consistency Score\n(SCS), 93.9% for Layout Fidelity Index (LFI), and 85.0% for Instruction\nAdherence Rate (IAR). These results demonstrate DocRefine's superior capability\nin handling complex multimodal document editing, preserving semantic integrity,\nand maintaining visual consistency, marking a significant advancement in\nautomated scientific document processing.", "AI": {"tldr": "DocRefine\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u591a\u4ee3\u7406\u7cfb\u7edf\u548cLVLM\uff0c\u80fd\u591f\u6839\u636e\u81ea\u7136\u8bed\u8a00\u6307\u4ee4\u667a\u80fd\u7406\u89e3\u3001\u4f18\u5316\u548c\u6458\u8981\u79d1\u5b66PDF\u6587\u6863\uff0c\u5e76\u5728\u5404\u9879\u6307\u6807\u4e0a\u8d85\u8d8a\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u79d1\u5b66\u6587\u732e\u7684\u6307\u6570\u7ea7\u589e\u957f\u9700\u8981\u66f4\u9ad8\u7ea7\u7684\u5de5\u5177\u6765\u9ad8\u6548\u51c6\u786e\u5730\u8fdb\u884c\u6587\u6863\u7406\u89e3\u3001\u6458\u8981\u548c\u5185\u5bb9\u4f18\u5316\uff0c\u800c\u4f20\u7edf\u65b9\u6cd5\u548c\u76f4\u63a5\u7684LLM/LVLM\u5e94\u7528\u5728\u5904\u7406\u590d\u6742\u5e03\u5c40\u548c\u591a\u6a21\u6001\u5185\u5bb9\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "DocRefine\u5229\u7528\u5148\u8fdb\u7684\u89c6\u89c9\u8bed\u8a00\u5927\u6a21\u578b\uff08LVLMs\uff09\u548c\u5305\u62ec\u5e03\u5c40\u4e0e\u7ed3\u6784\u5206\u6790\u3001\u591a\u6a21\u6001\u5185\u5bb9\u7406\u89e3\u3001\u6307\u4ee4\u5206\u89e3\u3001\u5185\u5bb9\u63d0\u70bc\u3001\u6458\u8981\u4e0e\u751f\u6210\u4ee5\u53ca\u4fdd\u771f\u5ea6\u4e0e\u4e00\u81f4\u6027\u9a8c\u8bc1\u5728\u5185\u7684\u516d\u4e2a\u4e13\u4e1a\u534f\u4f5c\u4ee3\u7406\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u95ed\u73af\u53cd\u9988\u67b6\u6784\uff0c\u4ee5\u786e\u4fdd\u9ad8\u8bed\u4e49\u51c6\u786e\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "result": "\u5728DocEditBench\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0cDocRefine\u5728\u8bed\u4e49\u4e00\u81f4\u6027\u5206\u6570\uff08SCS\uff09\u3001\u5e03\u5c40\u4fdd\u771f\u5ea6\u6307\u6570\uff08LFI\uff09\u548c\u6307\u4ee4\u9075\u5faa\u7387\uff08IAR\uff09\u65b9\u9762\u5206\u522b\u8fbe\u523086.7%\u300193.9%\u548c85.0%\uff0c\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u3002", "conclusion": "DocRefine\u5728\u5904\u7406\u590d\u6742\u7684\u6a21\u6001\u6587\u6863\u7f16\u8f91\u3001\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u548c\u89c6\u89c9\u4e00\u81f4\u6027\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u6807\u5fd7\u7740\u81ea\u52a8\u5316\u79d1\u5b66\u6587\u6863\u5904\u7406\u7684\u91cd\u5927\u8fdb\u5c55\u3002"}}
{"id": "2508.07860", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07860", "abs": "https://arxiv.org/abs/2508.07860", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Pavel Kr\u00e1l"], "title": "Large Language Models for Czech Aspect-Based Sentiment Analysis", "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis\ntask that aims to identify sentiment toward specific aspects of an entity.\nWhile large language models (LLMs) have shown strong performance in various\nnatural language processing (NLP) tasks, their capabilities for Czech ABSA\nremain largely unexplored. In this work, we conduct a comprehensive evaluation\nof 19 LLMs of varying sizes and architectures on Czech ABSA, comparing their\nperformance in zero-shot, few-shot, and fine-tuning scenarios. Our results show\nthat small domain-specific models fine-tuned for ABSA outperform\ngeneral-purpose LLMs in zero-shot and few-shot settings, while fine-tuned LLMs\nachieve state-of-the-art results. We analyze how factors such as\nmultilingualism, model size, and recency influence performance and present an\nerror analysis highlighting key challenges, particularly in aspect term\nprediction. Our findings provide insights into the suitability of LLMs for\nCzech ABSA and offer guidance for future research in this area.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e8619\u79cdLLM\u5728\u6377\u514b\u65b9\u9762\u57fa\u7840\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a\uff0c\u867d\u7136\u5fae\u8c03\u540e\u7684LLM\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\uff0c\u4f46\u9488\u5bf9ABSA\u5fae\u8c03\u7684\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u573a\u666f\u4e0b\u4f18\u4e8e\u901a\u7528LLM\u3002\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff0c\u5e76\u6307\u51fa\u4e86\u65b9\u9762\u8bcd\u9884\u6d4b\u4e2d\u7684\u6311\u6218\u3002", "motivation": "\u5c3d\u7ba1LLM\u5728\u5404\u79cdNLP\u4efb\u52a1\u4e2d\u8868\u73b0\u5f3a\u52b2\uff0c\u4f46\u5b83\u4eec\u5728\u6377\u514bABSA\u65b9\u9762\u7684\u80fd\u529b\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u672c\u7814\u7a76\u5bf919\u79cd\u4e0d\u540c\u5927\u5c0f\u548c\u67b6\u6784\u7684LLM\u5728\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u5fae\u8c03\u573a\u666f\u4e0b\u8fdb\u884c\u4e86\u5168\u9762\u7684\u6377\u514bABSA\u6027\u80fd\u8bc4\u4f30\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u9488\u5bf9ABSA\u8fdb\u884c\u5fae\u8c03\u7684\u5c0f\u578b\u9886\u57df\u7279\u5b9a\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u8bbe\u7f6e\u4e2d\u4f18\u4e8e\u901a\u7528LLM\uff0c\u800c\u5fae\u8c03\u540e\u7684LLM\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86\u591a\u8bed\u8a00\u3001\u6a21\u578b\u5927\u5c0f\u548c\u65f6\u65b0\u6027\u7b49\u56e0\u7d20\u5bf9\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u5e76\u8fdb\u884c\u4e86\u9519\u8bef\u5206\u6790\uff0c\u5f3a\u8c03\u4e86\u65b9\u9762\u8bcd\u9884\u6d4b\u65b9\u9762\u7684\u6311\u6218\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u4e3a\u7406\u89e3LLM\u5728\u6377\u514bABSA\u4efb\u52a1\u4e2d\u7684\u9002\u7528\u6027\u63d0\u4f9b\u4e86\u89c1\u89e3\uff0c\u5e76\u4e3a\u8be5\u9886\u57df\u7684\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u6307\u5bfc\u3002"}}
{"id": "2508.07023", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07023", "abs": "https://arxiv.org/abs/2508.07023", "authors": ["Jingwei Peng", "Jiehao Chen", "Mateo Alejandro Rojas", "Meilin Zhang"], "title": "MV-CoRe: Multimodal Visual-Conceptual Reasoning for Complex Visual Question Answering", "comment": null, "summary": "Complex Visual Question Answering (Complex VQA) tasks, which demand\nsophisticated multi-modal reasoning and external knowledge integration, present\nsignificant challenges for existing large vision-language models (LVLMs) often\nlimited by their reliance on high-level global features. To address this, we\npropose MV-CoRe (Multimodal Visual-Conceptual Reasoning), a novel model\ndesigned to enhance Complex VQA performance through the deep fusion of diverse\nvisual and linguistic information. MV-CoRe meticulously integrates global\nembeddings from pre-trained Vision Large Models (VLMs) and Language Large\nModels (LLMs) with fine-grained semantic-aware visual features, including\nobject detection characteristics and scene graph representations. An innovative\nMultimodal Fusion Transformer then processes and deeply integrates these\ndiverse feature sets, enabling rich cross-modal attention and facilitating\ncomplex reasoning. We evaluate MV-CoRe on challenging Complex VQA benchmarks,\nincluding GQA, A-OKVQA, and OKVQA, after training on VQAv2. Our experimental\nresults demonstrate that MV-CoRe consistently outperforms established LVLM\nbaselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies\nconfirm the critical contribution of both object and scene graph features, and\nhuman evaluations further validate MV-CoRe's superior factual correctness and\nreasoning depth, underscoring its robust capabilities for deep visual and\nconceptual understanding.", "AI": {"tldr": "MV-CoRe enhances Complex VQA by deeply fusing diverse visual and linguistic information, outperforming existing LVLMs with its novel Multimodal Fusion Transformer and integration of fine-grained features.", "motivation": "Complex VQA tasks, which demand sophisticated multi-modal reasoning and external knowledge integration, present significant challenges for existing LVLMs often limited by their reliance on high-level global features.", "method": "MV-CoRe meticulously integrates global embeddings from pre-trained VLMs and LLMs with fine-grained semantic-aware visual features, including object detection characteristics and scene graph representations. An innovative Multimodal Fusion Transformer then processes and deeply integrates these diverse feature sets, enabling rich cross-modal attention and facilitating complex reasoning.", "result": "MV-CoRe consistently outperforms established LVLM baselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies confirm the critical contribution of both object and scene graph features, and human evaluations further validate MV-CoRe's superior factual correctness and reasoning depth.", "conclusion": "MV-CoRe consistently outperforms established LVLM baselines, achieving an overall accuracy of 77.5% on GQA. Ablation studies confirm the critical contribution of both object and scene graph features, and human evaluations further validate MV-CoRe's superior factual correctness and reasoning depth, underscoring its robust capabilities for deep visual and conceptual understanding."}}
{"id": "2508.07297", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07297", "abs": "https://arxiv.org/abs/2508.07297", "authors": ["Hongbo Zhu", "Angelo Cangelosi"], "title": "Revisiting Data Attribution for Influence Functions", "comment": null, "summary": "The goal of data attribution is to trace the model's predictions through the\nlearning algorithm and back to its training data. thereby identifying the most\ninfluential training samples and understanding how the model's behavior leads\nto particular predictions. Understanding how individual training examples\ninfluence a model's predictions is fundamental for machine learning\ninterpretability, data debugging, and model accountability. Influence\nfunctions, originating from robust statistics, offer an efficient, first-order\napproximation to estimate the impact of marginally upweighting or removing a\ndata point on a model's learned parameters and its subsequent predictions,\nwithout the need for expensive retraining. This paper comprehensively reviews\nthe data attribution capability of influence functions in deep learning. We\ndiscuss their theoretical foundations, recent algorithmic advances for\nefficient inverse-Hessian-vector product estimation, and evaluate their\neffectiveness for data attribution and mislabel detection. Finally,\nhighlighting current challenges and promising directions for unleashing the\nhuge potential of influence functions in large-scale, real-world deep learning\nscenarios.", "AI": {"tldr": "Influence functions help identify influential training data points and understand model behavior in deep learning, with ongoing work to improve their application in large-scale scenarios.", "motivation": "Understanding how individual training examples influence model predictions is crucial for machine learning interpretability, data debugging, and model accountability. This paper aims to comprehensively review the data attribution capabilities of influence functions in deep learning.", "method": "Influence functions, derived from robust statistics, approximate the impact of training data points on model parameters and predictions without retraining. This paper reviews their theoretical foundations, efficient estimation algorithms (inverse-Hessian-vector products), and applications.", "result": "The paper evaluates the effectiveness of influence functions for data attribution and mislabel detection, highlighting their potential in deep learning.", "conclusion": "Influence functions are effective for data attribution and mislabel detection in deep learning, with ongoing research addressing challenges in large-scale applications."}}
{"id": "2508.07866", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07866", "abs": "https://arxiv.org/abs/2508.07866", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Pavel Kr\u00e1l"], "title": "Few-shot Cross-lingual Aspect-Based Sentiment Analysis with Sequence-to-Sequence Models", "comment": "Accepted for presentation at the 28th International Conference on\n  Text, Speech and Dialogue (TSD 2025)", "summary": "Aspect-based sentiment analysis (ABSA) has received substantial attention in\nEnglish, yet challenges remain for low-resource languages due to the scarcity\nof labelled data. Current cross-lingual ABSA approaches often rely on external\ntranslation tools and overlook the potential benefits of incorporating a small\nnumber of target language examples into training. In this paper, we evaluate\nthe effect of adding few-shot target language examples to the training set\nacross four ABSA tasks, six target languages, and two sequence-to-sequence\nmodels. We show that adding as few as ten target language examples\nsignificantly improves performance over zero-shot settings and achieves a\nsimilar effect to constrained decoding in reducing prediction errors.\nFurthermore, we demonstrate that combining 1,000 target language examples with\nEnglish data can even surpass monolingual baselines. These findings offer\npractical insights for improving cross-lingual ABSA in low-resource and\ndomain-specific settings, as obtaining ten high-quality annotated examples is\nboth feasible and highly effective.", "AI": {"tldr": "\u201c\u5728\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\uff0c\u5411\u8de8\u8bed\u8a00\u65b9\u9762\u57fa\u4e8e\u60c5\u611f\u5206\u6790\u7684\u8bad\u7ec3\u96c6\u4e2d\u6dfb\u52a0\u5c11\u91cf\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u6027\u80fd\u3002\u201d", "motivation": "\u201c\u76ee\u524d\u8de8\u8bed\u8a00\u65b9\u9762\u57fa\u4e8e\u60c5\u611f\u5206\u6790\u7684\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5916\u90e8\u7ffb\u8bd1\u5de5\u5177\uff0c\u5e76\u4e14\u5ffd\u89c6\u4e86\u5728\u8bad\u7ec3\u4e2d\u7ed3\u5408\u5c11\u91cf\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\u7684\u6f5c\u5728\u76ca\u5904\uff0c\u5c24\u5176\u662f\u5728\u6807\u7b7e\u6570\u636e\u7a00\u758f\u7684\u4f4e\u8d44\u6e90\u8bed\u8a00\u4e2d\u3002\u201d", "method": "\u201c\u672c\u6587\u8bc4\u4f30\u4e86\u5728\u56db\u79cd\u65b9\u9762\u57fa\u4e8e\u60c5\u611f\u5206\u6790\u4efb\u52a1\u3001\u516d\u79cd\u76ee\u6807\u8bed\u8a00\u548c\u4e24\u79cd\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\u4e2d\uff0c\u5411\u8bad\u7ec3\u96c6\u4e2d\u6dfb\u52a0\u5c11\u6837\u672c\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\u7684\u6548\u679c\u3002\u201d", "result": "\u201c\u7814\u7a76\u8868\u660e\uff0c\u6dfb\u52a0\u4ec5\u5341\u4e2a\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\u5373\u53ef\u663e\u8457\u63d0\u9ad8\u6027\u80fd\uff0c\u5176\u6548\u679c\u4e0e\u7ea6\u675f\u89e3\u7801\u5728\u51cf\u5c11\u9884\u6d4b\u9519\u8bef\u65b9\u9762\u76f8\u4f3c\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8bc1\u660e\uff0c\u7ed3\u54081000\u4e2a\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\u548c\u82f1\u8bed\u6570\u636e\u53ef\u4ee5\u8d85\u8d8a\u5355\u4e00\u8bed\u8a00\u57fa\u7ebf\u3002\u201d", "conclusion": "\u201c\u5c11\u91cf\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\u53ef\u663e\u8457\u63d0\u9ad8\u8de8\u8bed\u8a00\u65b9\u9762\u57fa\u4e8e\u60c5\u611f\u5206\u6790\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u6709\u6548\u51cf\u5c11\u9884\u6d4b\u9519\u8bef\u3002\u7ed3\u54081000\u4e2a\u76ee\u6807\u8bed\u8a00\u793a\u4f8b\u548c\u82f1\u8bed\u6570\u636e\u751a\u81f3\u53ef\u4ee5\u8d85\u8d8a\u5355\u4e00\u8bed\u8a00\u57fa\u7ebf\u3002\u201d"}}
{"id": "2508.07028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07028", "abs": "https://arxiv.org/abs/2508.07028", "authors": ["Juntong Fan", "Shuyi Fan", "Debesh Jha", "Changsheng Fang", "Tieyong Zeng", "Hengyong Yu", "Dayang Wang"], "title": "Large Language Model Evaluated Stand-alone Attention-Assisted Graph Neural Network with Spatial and Structural Information Interaction for Precise Endoscopic Image Segmentation", "comment": "Manuscript under review", "summary": "Accurate endoscopic image segmentation on the polyps is critical for early\ncolorectal cancer detection. However, this task remains challenging due to low\ncontrast with surrounding mucosa, specular highlights, and indistinct\nboundaries. To address these challenges, we propose FOCUS-Med, which stands for\nFusion of spatial and structural graph with attentional context-aware polyp\nsegmentation in endoscopic medical imaging. FOCUS-Med integrates a Dual Graph\nConvolutional Network (Dual-GCN) module to capture contextual spatial and\ntopological structural dependencies. This graph-based representation enables\nthe model to better distinguish polyps from background tissues by leveraging\ntopological cues and spatial connectivity, which are often obscured in raw\nimage intensities. It enhances the model's ability to preserve boundaries and\ndelineate complex shapes typical of polyps. In addition, a location-fused\nstand-alone self-attention is employed to strengthen global context\nintegration. To bridge the semantic gap between encoder-decoder layers, we\nincorporate a trainable weighted fast normalized fusion strategy for efficient\nmulti-scale aggregation. Notably, we are the first to introduce the use of a\nLarge Language Model (LLM) to provide detailed qualitative evaluations of\nsegmentation quality. Extensive experiments on public benchmarks demonstrate\nthat FOCUS-Med achieves state-of-the-art performance across five key metrics,\nunderscoring its effectiveness and clinical potential for AI-assisted\ncolonoscopy.", "AI": {"tldr": "FOCUS-Med\u901a\u8fc7\u7ed3\u5408\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u5229\u7528LLM\u8fdb\u884c\u8bc4\u4f30\uff0c\u5728\u5185\u7aa5\u955c\u606f\u8089\u5206\u5272\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u6709\u52a9\u4e8e\u65e9\u671f\u7ed3\u76f4\u80a0\u764c\u68c0\u6d4b\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5185\u7aa5\u955c\u56fe\u50cf\u5206\u5272\u4e2d\u56e0\u5bf9\u6bd4\u5ea6\u4f4e\u3001\u955c\u9762\u53cd\u5c04\u9ad8\u5149\u548c\u8fb9\u754c\u6a21\u7cca\u7b49\u95ee\u9898\u5bfc\u81f4\u7684\u7ed3\u76f4\u80a0\u606f\u8089\u65e9\u671f\u68c0\u6d4b\u7684\u6311\u6218\u3002", "method": "FOCUS-Med\u96c6\u6210\u4e86\u4e00\u4e2a\u53cc\u56fe\u5377\u79ef\u7f51\u7edc\uff08Dual-GCN\uff09\u6a21\u5757\u6765\u6355\u6349\u4e0a\u4e0b\u6587\u7a7a\u95f4\u548c\u62d3\u6251\u7ed3\u6784\u4f9d\u8d56\u6027\uff0c\u5e76\u91c7\u7528\u4f4d\u7f6e\u878d\u5408\u7684\u72ec\u7acb\u81ea\u6ce8\u610f\u529b\u673a\u5236\u6765\u52a0\u5f3a\u5168\u5c40\u4e0a\u4e0b\u6587\u96c6\u6210\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u53ef\u8bad\u7ec3\u7684\u52a0\u6743\u5feb\u901f\u5f52\u4e00\u5316\u878d\u5408\u7b56\u7565\u6765\u5b9e\u73b0\u6709\u6548\u7684\u591a\u5c3a\u5ea6\u805a\u5408\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u7814\u7a76\u9996\u6b21\u4f7f\u7528\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5bf9\u5206\u5272\u8d28\u91cf\u8fdb\u884c\u8be6\u7ec6\u7684\u5b9a\u6027\u8bc4\u4f30\u3002", "result": "FOCUS-Med\u901a\u8fc7\u5229\u7528\u62d3\u6251\u7ebf\u7d22\u548c\u7a7a\u95f4\u8fde\u901a\u6027\uff0c\u66f4\u597d\u5730\u5c06\u606f\u8089\u4e0e\u80cc\u666f\u7ec4\u7ec7\u533a\u5206\u5f00\u6765\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u4fdd\u7559\u8fb9\u754c\u548c\u63cf\u7ed8\u606f\u8089\u590d\u6742\u5f62\u72b6\u7684\u80fd\u529b\uff0c\u5e76\u5728\u4e94\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "FOCUS-Med\u5728\u516c\u5171\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u4e94\u4e2a\u5173\u952e\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u8bc1\u660e\u4e86\u5176\u5728AI\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u7684\u6709\u6548\u6027\u548c\u4e34\u5e8a\u6f5c\u529b\u3002"}}
{"id": "2508.07299", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07299", "abs": "https://arxiv.org/abs/2508.07299", "authors": ["Lin-Han Jia", "Si-Yu Han", "Wen-Chao Hu", "Jie-Jing Shao", "Wen-Da Wei", "Zhi Zhou", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "When Is Prior Knowledge Helpful? Exploring the Evaluation and Selection of Unsupervised Pretext Tasks from a Neuro-Symbolic Perspective", "comment": null, "summary": "Neuro-symbolic (Nesy) learning improves the target task performance of models\nby enabling them to satisfy knowledge, while semi/self-supervised learning\n(SSL) improves the target task performance by designing unsupervised pretext\ntasks for unlabeled data to make models satisfy corresponding assumptions. We\nextend the Nesy theory based on reliable knowledge to the scenario of\nunreliable knowledge (i.e., assumptions), thereby unifying the theoretical\nframeworks of SSL and Nesy. Through rigorous theoretical analysis, we\ndemonstrate that, in theory, the impact of pretext tasks on target performance\nhinges on three factors: knowledge learnability with respect to the model,\nknowledge reliability with respect to the data, and knowledge completeness with\nrespect to the target. We further propose schemes to operationalize these\ntheoretical metrics, and thereby develop a method that can predict the\neffectiveness of pretext tasks in advance. This will change the current status\nquo in practical applications, where the selections of unsupervised tasks are\nheuristic-based rather than theory-based, and it is difficult to evaluate the\nrationality of unsupervised pretext task selection before testing the model on\nthe target task. In experiments, we verify a high correlation between the\npredicted performance-estimated using minimal data-and the actual performance\nachieved after large-scale semi-supervised or self-supervised learning, thus\nconfirming the validity of the theory and the effectiveness of the evaluation\nmethod.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00SSL\u548cNesy\u5b66\u4e60\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u9884\u6d4b\u9884\u7f6e\u4efb\u52a1\u6709\u6548\u6027\u7684\u65b9\u6cd5\u3002", "motivation": "\u7edf\u4e00SSL\u548cNesy\u7684\u7406\u8bba\u6846\u67b6\uff0c\u89e3\u51b3\u5f53\u524dSSL\u4e2d\u9884\u7f6e\u4efb\u52a1\u9009\u62e9\u7684\u542f\u53d1\u5f0f\u548c\u8bc4\u4f30\u56f0\u96be\u7684\u95ee\u9898\u3002", "method": "\u5c06Nesy\u7406\u8bba\u6269\u5c55\u5230\u5305\u542b\u4e0d\u53ef\u9760\u77e5\u8bc6\uff08\u5047\u8bbe\uff09\u7684\u573a\u666f\uff0c\u5e76\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u63d0\u51fa\u64cd\u4f5c\u5316\u7406\u8bba\u6307\u6807\u7684\u65b9\u6848\uff0c\u5f00\u53d1\u9884\u6d4b\u9884\u7f6e\u4efb\u52a1\u6709\u6548\u6027\u7684\u65b9\u6cd5\u3002", "result": "\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u9884\u6d4b\u6027\u80fd\u4e0e\u5b9e\u9645\u6027\u80fd\u4e4b\u95f4\u7684\u9ad8\u5ea6\u76f8\u5173\u6027\uff0c\u8bc1\u5b9e\u4e86\u7406\u8bba\u7684\u6709\u6548\u6027\u548c\u8bc4\u4f30\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u7edf\u4e00\u4e86\u534a\u76d1\u7763\u5b66\u4e60\uff08SSL\uff09\u548c\u795e\u7ecf\u7b26\u53f7\uff08Nesy\uff09\u5b66\u4e60\u7684\u7406\u8bba\u6846\u67b6\uff0c\u5e76\u901a\u8fc7\u7406\u8bba\u5206\u6790\u63d0\u51fa\u4e86\u8bc4\u4f30\u548c\u9884\u6d4b\u9884\u7f6e\u4efb\u52a1\u6709\u6548\u6027\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07902", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07902", "abs": "https://arxiv.org/abs/2508.07902", "authors": ["Chen Cecilia Liu", "Hiba Arnaout", "Nils Kova\u010di\u0107", "Dana Atzil-Slonim", "Iryna Gurevych"], "title": "Tailored Emotional LLM-Supporter: Enhancing Cultural Sensitivity", "comment": "Under review; joint first authors", "summary": "Large language models (LLMs) show promise in offering emotional support and\ngenerating empathetic responses for individuals in distress, but their ability\nto deliver culturally sensitive support remains underexplored due to lack of\nresources. In this work, we introduce CultureCare, the first dataset designed\nfor this task, spanning four cultures and including 1729 distress messages,\n1523 cultural signals, and 1041 support strategies with fine-grained emotional\nand cultural annotations. Leveraging CultureCare, we (i) develop and test four\nadaptation strategies for guiding three state-of-the-art LLMs toward culturally\nsensitive responses; (ii) conduct comprehensive evaluations using LLM judges,\nin-culture human annotators, and clinical psychologists; (iii) show that\nadapted LLMs outperform anonymous online peer responses, and that simple\ncultural role-play is insufficient for cultural sensitivity; and (iv) explore\nthe application of LLMs in clinical training, where experts highlight their\npotential in fostering cultural competence in future therapists.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07031", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07031", "abs": "https://arxiv.org/abs/2508.07031", "authors": ["Anindya Bijoy Das", "Shahnewaz Karim Sakib", "Shibbir Ahmed"], "title": "Trustworthy Medical Imaging with Large Language Models: A Study of Hallucinations Across Modalities", "comment": null, "summary": "Large Language Models (LLMs) are increasingly applied to medical imaging\ntasks, including image interpretation and synthetic image generation. However,\nthese models often produce hallucinations, which are confident but incorrect\noutputs that can mislead clinical decisions. This study examines hallucinations\nin two directions: image to text, where LLMs generate reports from X-ray, CT,\nor MRI scans, and text to image, where models create medical images from\nclinical prompts. We analyze errors such as factual inconsistencies and\nanatomical inaccuracies, evaluating outputs using expert informed criteria\nacross imaging modalities. Our findings reveal common patterns of hallucination\nin both interpretive and generative tasks, with implications for clinical\nreliability. We also discuss factors contributing to these failures, including\nmodel architecture and training data. By systematically studying both image\nunderstanding and generation, this work provides insights into improving the\nsafety and trustworthiness of LLM driven medical imaging systems.", "AI": {"tldr": "LLMs\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u4f1a\u4ea7\u751f\u5e7b\u89c9\uff0c\u5f71\u54cd\u4e34\u5e8a\u51b3\u7b56\u3002\u672c\u7814\u7a76\u5206\u6790\u4e86\u5f71\u50cf\u5230\u6587\u672c\u548c\u6587\u672c\u5230\u5f71\u50cf\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\uff0c\u53d1\u73b0\u4e86\u5e38\u89c1\u9519\u8bef\uff0c\u5e76\u8ba8\u8bba\u4e86\u5982\u4f55\u63d0\u9ad8LLM\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u3002", "motivation": "LLMs\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u5e94\u7528\u65e5\u76ca\u5e7f\u6cdb\uff0c\u4f46\u5176\u4ea7\u751f\u7684\u5e7b\u89c9\u53ef\u80fd\u8bef\u5bfc\u4e34\u5e8a\u51b3\u7b56\uff0c\u56e0\u6b64\u6709\u5fc5\u8981\u7cfb\u7edf\u6027\u5730\u7814\u7a76\u8fd9\u4e9b\u5e7b\u89c9\u73b0\u8c61\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790LLMs\u5728\u533b\u5b66\u5f71\u50cf\u89e3\u91ca\uff08\u5f71\u50cf\u5230\u6587\u672c\uff09\u548c\u5408\u6210\uff08\u6587\u672c\u5230\u5f71\u50cf\uff09\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u89e3\u5256\u5b66\u4e0d\u51c6\u786e\u7b49\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u4f9d\u636e\u4e13\u5bb6\u6807\u51c6\u8bc4\u4f30\u4e86\u4e0d\u540c\u5f71\u50cf\u6a21\u6001\u4e0b\u7684\u8f93\u51fa\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u4e86LLMs\u5728\u533b\u5b66\u5f71\u50cf\u89e3\u91ca\u548c\u751f\u6210\u4efb\u52a1\u4e2d\u666e\u904d\u5b58\u5728\u7684\u5e7b\u89c9\u6a21\u5f0f\uff0c\u5e76\u63a2\u8ba8\u4e86\u5bfc\u81f4\u8fd9\u4e9b\u95ee\u9898\u7684\u56e0\u7d20\uff0c\u5305\u62ec\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "LLMs\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\u7684\u5e94\u7528\u867d\u7136\u5e7f\u6cdb\uff0c\u4f46\u5176\u4ea7\u751f\u7684\u5e7b\u89c9\uff08\u5c3d\u7ba1\u81ea\u4fe1\u4f46\u9519\u8bef\u7684\u8f93\u51fa\uff09\u5bf9\u4e34\u5e8a\u51b3\u7b56\u53ef\u80fd\u4ea7\u751f\u8bef\u5bfc\u3002\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790LLMs\u5728\u533b\u5b66\u5f71\u50cf\u89e3\u91ca\uff08\u5f71\u50cf\u5230\u6587\u672c\uff09\u548c\u5408\u6210\uff08\u6587\u672c\u5230\u5f71\u50cf\uff09\u4efb\u52a1\u4e2d\u7684\u5e7b\u89c9\u73b0\u8c61\uff0c\u63ed\u793a\u4e86\u4e8b\u5b9e\u4e0d\u4e00\u81f4\u548c\u89e3\u5256\u5b66\u4e0d\u51c6\u786e\u7b49\u5e38\u89c1\u9519\u8bef\u6a21\u5f0f\uff0c\u5e76\u4f9d\u636e\u4e13\u5bb6\u6807\u51c6\u8bc4\u4f30\u4e86\u4e0d\u540c\u5f71\u50cf\u6a21\u6001\u4e0b\u7684\u8f93\u51fa\u3002\u7814\u7a76\u7ed3\u679c\u5bf9LLM\u5728\u533b\u5b66\u5f71\u50cf\u7cfb\u7edf\u4e2d\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u8d56\u6027\u5177\u6709\u91cd\u8981\u610f\u4e49\uff0c\u5e76\u63a2\u8ba8\u4e86\u6a21\u578b\u67b6\u6784\u548c\u8bad\u7ec3\u6570\u636e\u7b49\u56e0\u7d20\u5bf9\u8fd9\u4e9b\u5931\u8d25\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.07329", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07329", "abs": "https://arxiv.org/abs/2508.07329", "authors": ["Tuo Zhang", "Ning Li", "Xin Yuan", "Wenchao Xu", "Quan Chen", "Song Guo", "Haijun Zhang"], "title": "Efficient Edge LLMs Deployment via HessianAware Quantization and CPU GPU Collaborative", "comment": null, "summary": "With the breakthrough progress of large language models (LLMs) in natural\nlanguage processing and multimodal tasks, efficiently deploying them on\nresource-constrained edge devices has become a critical challenge. The Mixture\nof Experts (MoE) architecture enhances model capacity through sparse\nactivation, but faces two major difficulties in practical deployment: (1) The\npresence of numerous outliers in activation distributions leads to severe\ndegradation in quantization accuracy for both activations and weights,\nsignificantly impairing inference performance; (2) Under limited memory,\nefficient offloading and collaborative inference of expert modules struggle to\nbalance latency and throughput. To address these issues, this paper proposes an\nefficient MoE edge deployment scheme based on Hessian-Aware Quantization (HAQ)\nand CPU-GPU collaborative inference. First, by introducing smoothed Hessian\nmatrix quantization, we achieve joint 8-bit quantization of activations and\nweights, which significantly alleviates the accuracy loss caused by outliers\nwhile ensuring efficient implementation on mainstream hardware. Second, we\ndesign an expert-level collaborative offloading and inference mechanism, which,\ncombined with expert activation path statistics, enables efficient deployment\nand scheduling of expert modules between CPU and GPU, greatly reducing memory\nfootprint and inference latency. Extensive experiments validate the\neffectiveness of our method on mainstream large models such as the OPT series\nand Mixtral 8*7B: on datasets like Wikitext2 and C4, the inference accuracy of\nthe low-bit quantized model approaches that of the full-precision model, while\nGPU memory usage is reduced by about 60%, and inference latency is\nsignificantly improved.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684MoE\u8fb9\u7f18\u90e8\u7f72\u65b9\u6848\uff0c\u901a\u8fc7HAQ\u91cf\u5316\u548cCPU-GPU\u534f\u540c\u63a8\u7406\uff0c\u89e3\u51b3\u4e86LLM\u5728\u8fb9\u7f18\u8bbe\u5907\u90e8\u7f72\u4e2d\u7684\u91cf\u5316\u7cbe\u5ea6\u548c\u5185\u5b58\u6548\u7387\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "LLM\u5728NLP\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u53d6\u5f97\u7a81\u7834\uff0c\u4f46\u5728\u8d44\u6e90\u53d7\u9650\u7684\u8fb9\u7f18\u8bbe\u5907\u4e0a\u9ad8\u6548\u90e8\u7f72\u9762\u4e34\u6311\u6218\u3002MoE\u67b6\u6784\u867d\u7136\u589e\u5f3a\u4e86\u6a21\u578b\u5bb9\u91cf\uff0c\u4f46\u5728\u5b9e\u9645\u90e8\u7f72\u4e2d\u5b58\u5728\u6fc0\u6d3b\u503c\u79bb\u7fa4\u70b9\u5bfc\u81f4\u91cf\u5316\u7cbe\u5ea6\u4e0b\u964d\u548c\u5185\u5b58\u53d7\u9650\u4e0b\u4e13\u5bb6\u6a21\u5757\u534f\u540c\u63a8\u7406\u6548\u7387\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eHessian\u611f\u77e5\u91cf\u5316\uff08HAQ\uff09\u548cCPU-GPU\u534f\u540c\u63a8\u7406\u7684MoE\u8fb9\u7f18\u90e8\u7f72\u65b9\u6848\u3002\u901a\u8fc7\u5f15\u5165\u5e73\u6ed1Hessian\u77e9\u9635\u91cf\u5316\uff0c\u5b9e\u73b0\u4e86\u6fc0\u6d3b\u503c\u548c\u6743\u91cd\u7684\u8054\u54088\u4f4d\u91cf\u5316\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u91cf\u5316\u7cbe\u5ea6\u4e0b\u964d\u95ee\u9898\u3002\u8bbe\u8ba1\u4e86\u4e13\u5bb6\u7ea7\u534f\u540c\u5378\u8f7d\u548c\u63a8\u7406\u673a\u5236\uff0c\u7ed3\u5408\u4e13\u5bb6\u6fc0\u6d3b\u8def\u5f84\u7edf\u8ba1\uff0c\u5b9e\u73b0\u4e86\u4e13\u5bb6\u6a21\u5757\u5728CPU\u548cGPU\u4e4b\u95f4\u7684\u9ad8\u6548\u90e8\u7f72\u548c\u8c03\u5ea6\uff0c\u964d\u4f4e\u4e86\u663e\u5b58\u5360\u7528\u548c\u63a8\u7406\u5ef6\u8fdf\u3002", "result": "\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u7684\u63a8\u7406\u7cbe\u5ea6\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\uff0cGPU\u663e\u5b58\u5360\u7528\u51cf\u5c11\u7ea660%\uff0c\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u8be5\u65b9\u6cd5\u5728OPT\u7cfb\u5217\u548cMixtral 8*7B\u7b49\u4e3b\u6d41\u5927\u6a21\u578b\u4e0a\uff0c\u4f7f\u7528Wikitext2\u548cC4\u7b49\u6570\u636e\u96c6\uff0c\u4f4e\u6bd4\u7279\u91cf\u5316\u6a21\u578b\u7684\u63a8\u7406\u7cbe\u5ea6\u63a5\u8fd1\u5168\u7cbe\u5ea6\u6a21\u578b\uff0c\u540c\u65f6GPU\u663e\u5b58\u5360\u7528\u51cf\u5c11\u7ea660%\uff0c\u63a8\u7406\u5ef6\u8fdf\u663e\u8457\u63d0\u5347\u3002"}}
{"id": "2508.07937", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07937", "abs": "https://arxiv.org/abs/2508.07937", "authors": ["John C. McDonald", "Rosalee Wolfe", "Fabrizio Nunnari"], "title": "Challenges and opportunities in portraying emotion in generated sign language", "comment": null, "summary": "Non-manual signals in sign languages continue to be a challenge for signing\navatars. More specifically, emotional content has been difficult to incorporate\nbecause of a lack of a standard method of specifying the avatar's emotional\nstate. This paper explores the application of an intuitive two-parameter\nrepresentation for emotive non-manual signals to the Paula signing avatar that\nshows promise for facilitating the linguistic specification of emotional facial\nexpressions in a more coherent manner than previous methods. Users can apply\nthese parameters to control Paula's emotional expressions through a textual\nrepresentation called the EASIER notation. The representation can allow avatars\nto express more nuanced emotional states using two numerical parameters. It\nalso has the potential to enable more consistent specification of emotional\nnon-manual signals in linguistic annotations which drive signing avatars.", "AI": {"tldr": "This paper introduces a new way to make signing avatars express emotions using two simple numbers, making them more realistic and easier to control through a system called EASIER notation.", "motivation": "The motivation is to address the challenge of incorporating emotional content into signing avatars due to the lack of a standard method for specifying their emotional state, aiming to enable more nuanced and coherent emotional facial expressions.", "method": "The paper applies an intuitive two-parameter representation for emotive non-manual signals to the Paula signing avatar, using the EASIER notation for textual control of the avatar's emotional expressions.", "result": "The application of the two-parameter representation to the Paula avatar shows promise for facilitating the linguistic specification of emotional facial expressions, allowing for more nuanced emotional states and potentially more consistent specification in linguistic annotations.", "conclusion": "This paper explores a promising two-parameter representation for emotive non-manual signals applied to the Paula signing avatar, facilitating more coherent linguistic specification of emotional facial expressions and enabling nuanced emotional states through textual control via EASIER notation. This approach has the potential for more consistent specification of emotional non-manual signals in linguistic annotations for signing avatars."}}
{"id": "2508.07038", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07038", "abs": "https://arxiv.org/abs/2508.07038", "authors": ["Yuke Xing", "William Gordon", "Qi Yang", "Kaifa Yang", "Jiarui Wang", "Yiling Xu"], "title": "3DGS-VBench: A Comprehensive Video Quality Evaluation Benchmark for 3DGS Compression", "comment": null, "summary": "3D Gaussian Splatting (3DGS) enables real-time novel view synthesis with high\nvisual fidelity, but its substantial storage requirements hinder practical\ndeployment, prompting state-of-the-art (SOTA) 3DGS methods to incorporate\ncompression modules. However, these 3DGS generative compression techniques\nintroduce unique distortions lacking systematic quality assessment research. To\nthis end, we establish 3DGS-VBench, a large-scale Video Quality Assessment\n(VQA) Dataset and Benchmark with 660 compressed 3DGS models and video sequences\ngenerated from 11 scenes across 6 SOTA 3DGS compression algorithms with\nsystematically designed parameter levels. With annotations from 50\nparticipants, we obtained MOS scores with outlier removal and validated dataset\nreliability. We benchmark 6 3DGS compression algorithms on storage efficiency\nand visual quality, and evaluate 15 quality assessment metrics across multiple\nparadigms. Our work enables specialized VQA model training for 3DGS, serving as\na catalyst for compression and quality assessment research. The dataset is\navailable at https://github.com/YukeXing/3DGS-VBench.", "AI": {"tldr": "3DGS-VBench\uff1a\u9996\u4e2a3DGS\u538b\u7f29\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\u57fa\u51c6\uff0c\u5305\u542b660\u4e2a\u6a21\u578b\u548c\u89c6\u9891\uff0c\u7528\u4e8e\u8bad\u7ec3VQA\u6a21\u578b\u548c\u63a8\u52a8\u538b\u7f29\u7814\u7a76\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b33DGS\u6a21\u578b\u5b58\u50a8\u9700\u6c42\u5927\u4ee5\u53ca\u73b0\u6709\u538b\u7f29\u6280\u672f\u5f15\u5165\u7684\u72ec\u7279\u5931\u771f\u7f3a\u4e4f\u7cfb\u7edf\u6027\u8d28\u91cf\u8bc4\u4f30\u95ee\u9898\u3002", "method": "\u521b\u5efa\u4e86\u4e00\u4e2a\u5305\u542b660\u4e2a\u538b\u7f293DGS\u6a21\u578b\u548c\u89c6\u9891\u5e8f\u5217\u7684\u5927\u89c4\u6a21\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff083DGS-VBench\uff09\uff0c\u6db5\u76d611\u4e2a\u573a\u666f\u548c6\u4e2aSOTA 3DGS\u538b\u7f29\u7b97\u6cd5\u3002\u6536\u96c6\u4e8650\u540d\u53c2\u4e0e\u8005\u7684MOS\u8bc4\u5206\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u3002\u5bf96\u4e2a3DGS\u538b\u7f29\u7b97\u6cd5\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e8615\u4e2a\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "result": "\u5efa\u7acb\u4e863DGS-VBench\u6570\u636e\u96c6\uff0c\u5e76\u5bf96\u4e2a3DGS\u538b\u7f29\u7b97\u6cd5\u7684\u5b58\u50a8\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u540c\u65f6\u8bc4\u4f30\u4e8615\u4e2a\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002", "conclusion": "\u8be5\u7814\u7a76\u5efa\u7acb\u4e863DGS-VBench\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u89c6\u9891\u8d28\u91cf\u8bc4\u4f30\uff08VQA\uff09\u6570\u636e\u96c6\u548c\u57fa\u51c6\uff0c\u5305\u542b660\u4e2a\u538b\u7f29\u76843DGS\u6a21\u578b\u548c\u89c6\u9891\u5e8f\u5217\uff0c\u6765\u81ea11\u4e2a\u573a\u666f\u548c6\u4e2aSOTA 3DGS\u538b\u7f29\u7b97\u6cd5\u3002\u901a\u8fc750\u540d\u53c2\u4e0e\u8005\u7684\u6807\u6ce8\uff0c\u83b7\u5f97\u4e86MOS\u5206\u6570\uff0c\u5e76\u9a8c\u8bc1\u4e86\u6570\u636e\u96c6\u7684\u53ef\u9760\u6027\u3002\u7814\u7a76\u5bf96\u4e2a3DGS\u538b\u7f29\u7b97\u6cd5\u7684\u5b58\u50a8\u6548\u7387\u548c\u89c6\u89c9\u8d28\u91cf\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5e76\u8bc4\u4f30\u4e8615\u4e2a\u8de8\u591a\u4e2a\u8303\u5f0f\u7684\u8d28\u91cf\u8bc4\u4f30\u6307\u6807\u3002\u8be5\u7814\u7a76\u652f\u6301\u4e13\u95e8\u76843DGS VQA\u6a21\u578b\u8bad\u7ec3\uff0c\u5e76\u4fc3\u8fdb\u538b\u7f29\u548c\u8d28\u91cf\u8bc4\u4f30\u7814\u7a76\u3002"}}
{"id": "2508.07333", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07333", "abs": "https://arxiv.org/abs/2508.07333", "authors": ["Yuhao Liu", "Rui Hu", "Yu Chen", "Longbo Huang"], "title": "Finite-Time Convergence Analysis of ODE-based Generative Models for Stochastic Interpolants", "comment": null, "summary": "Stochastic interpolants offer a robust framework for continuously\ntransforming samples between arbitrary data distributions, holding significant\npromise for generative modeling. Despite their potential, rigorous finite-time\nconvergence guarantees for practical numerical schemes remain largely\nunexplored. In this work, we address the finite-time convergence analysis of\nnumerical implementations for ordinary differential equations (ODEs) derived\nfrom stochastic interpolants. Specifically, we establish novel finite-time\nerror bounds in total variation distance for two widely used numerical\nintegrators: the first-order forward Euler method and the second-order Heun's\nmethod. Furthermore, our analysis on the iteration complexity of specific\nstochastic interpolant constructions provides optimized schedules to enhance\ncomputational efficiency. Our theoretical findings are corroborated by\nnumerical experiments, which validate the derived error bounds and complexity\nanalyses.", "AI": {"tldr": "\u672c\u7814\u7a76\u5206\u6790\u4e86\u968f\u673a\u63d2\u503c\u6570\u503c\u5b9e\u73b0\u7684\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u652f\u6301\u3002", "motivation": "\u5c3d\u7ba1\u968f\u673a\u63d2\u503c\u5728\u751f\u6210\u6a21\u578b\u4e2d\u5177\u6709\u6f5c\u529b\uff0c\u4f46\u5176\u6570\u503c\u5b9e\u73b0\u7684\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\u4fdd\u8bc1\u5728\u5b9e\u8df5\u4e2d\u4ecd\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\u3002\u672c\u5de5\u4f5c\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u4e3a\u968f\u673a\u63d2\u503c\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u53ef\u9760\u5e94\u7528\u63d0\u4f9b\u7406\u8bba\u652f\u6301\u3002", "method": "\u672c\u6587\u9488\u5bf9\u968f\u673a\u63d2\u503c\u751f\u6210\u7684\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODEs\uff09\uff0c\u7814\u7a76\u4e86\u4e24\u79cd\u5e38\u7528\u6570\u503c\u79ef\u5206\u65b9\u6cd5\uff08\u4e00\u9636\u6b27\u62c9\u6cd5\u548c\u4e8c\u9636\u9f99\u683c-\u5e93\u5854\u6cd5\uff09\u7684\u6709\u9650\u65f6\u95f4\u6536\u655b\u6027\uff0c\u5e76\u7ed9\u51fa\u4e86\u603b\u53d8\u5dee\u8ddd\u79bb\u7684\u8bef\u5dee\u754c\u9650\u3002\u6b64\u5916\uff0c\u8fd8\u5206\u6790\u4e86\u7279\u5b9a\u968f\u673a\u63d2\u503c\u6784\u9020\u7684\u8fed\u4ee3\u590d\u6742\u5ea6\uff0c\u5e76\u63d0\u51fa\u4e86\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u7684\u65f6\u95f4\u8868\u3002", "result": "\u7814\u7a76\u6210\u529f\u5efa\u7acb\u4e86\u4e24\u79cd\u6570\u503c\u79ef\u5206\u65b9\u6cd5\uff08\u4e00\u9636\u6b27\u62c9\u6cd5\u548c\u4e8c\u9636\u9f99\u683c-\u5e93\u5854\u6cd5\uff09\u5728\u968f\u673a\u63d2\u503c\u5e38\u5fae\u5206\u65b9\u7a0b\uff08ODEs\uff09\u4e0a\u7684\u6709\u9650\u65f6\u95f4\u8bef\u5dee\u754c\u9650\uff08\u603b\u53d8\u5dee\u8ddd\u79bb\uff09\u3002\u540c\u65f6\uff0c\u5206\u6790\u4e86\u8fed\u4ee3\u590d\u6742\u5ea6\u5e76\u7ed9\u51fa\u4e86\u4f18\u5316\u8ba1\u7b97\u6548\u7387\u7684\u65f6\u95f4\u8868\u3002\u6570\u503c\u5b9e\u9a8c\u7ed3\u679c\u9a8c\u8bc1\u4e86\u7406\u8bba\u5206\u6790\u7684\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u968f\u673a\u63d2\u503c\u5728\u751f\u6210\u6a21\u578b\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u7406\u8bba\u57fa\u7840\uff0c\u5e76\u6307\u51fa\u4e86\u6570\u503c\u65b9\u6cd5\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6536\u655b\u6027\u548c\u6548\u7387\u95ee\u9898\u3002"}}
{"id": "2508.07955", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07955", "abs": "https://arxiv.org/abs/2508.07955", "authors": ["Furkan \u015eahinu\u00e7", "Subhabrata Dutta", "Iryna Gurevych"], "title": "Expert Preference-based Evaluation of Automated Related Work Generation", "comment": "Project page: https://ukplab.github.io/arxiv2025-expert-eval-rw/", "summary": "Expert domain writing, such as scientific writing, typically demands\nextensive domain knowledge. Recent advances in LLMs show promising potential in\nreducing the expert workload. However, evaluating the quality of automatically\ngenerated scientific writing is a crucial open issue, as it requires knowledge\nof domain-specific evaluation criteria and the ability to discern expert\npreferences. Conventional automatic metrics and LLM-as-a-judge systems are\ninsufficient to grasp expert preferences and domain-specific quality standards.\nTo address this gap and support human-AI collaborative writing, we focus on\nrelated work generation, one of the most challenging scientific tasks, as an\nexemplar. We propose GREP, a multi-turn evaluation framework that integrates\nclassical related work evaluation criteria with expert-specific preferences.\nInstead of assigning a single score, our framework decomposes the evaluation\ninto fine-grained dimensions. This localized evaluation approach is further\naugmented with contrastive few-shot examples to provide detailed contextual\nguidance for the evaluation dimensions. The design principles allow our\nframework to deliver cardinal assessment of quality, which can facilitate\nbetter post-training compared to ordinal preference data. For better\naccessibility, we design two variants of GREP: a more precise variant with\nproprietary LLMs as evaluators, and a cheaper alternative with open-weight\nLLMs. Empirical investigation reveals that our framework is able to assess the\nquality of related work sections in a much more robust manner compared to\nstandard LLM judges, reflects natural scenarios of scientific writing, and\nbears a strong correlation with the human expert assessment. We also observe\nthat generations from state-of-the-art LLMs struggle to satisfy validation\nconstraints of a suitable related work section. They (mostly) fail to improve\nbased on feedback as well.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GREP \u7684\u65b0\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u5199\u4f5c\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u53c2\u8003\u6587\u732e\u90e8\u5206\u3002GREP \u901a\u8fc7\u7ed3\u5408\u4f20\u7edf\u6807\u51c6\u548c\u4e13\u5bb6\u504f\u597d\uff0c\u5e76\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8bc4\u4f30\uff0c\u6bd4\u73b0\u6709\u7684 LLM \u65b9\u6cd5\u66f4\u51c6\u786e\uff0c\u5e76\u4e0e\u4eba\u7c7b\u8bc4\u4f30\u8005\u9ad8\u5ea6\u4e00\u81f4\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u5f53\u524d\u6700\u5148\u8fdb\u7684 LLM \u5728\u751f\u6210\u6ee1\u8db3\u79d1\u5b66\u5199\u4f5c\u8981\u6c42\u7684\u53c2\u8003\u6587\u732e\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002", "motivation": "\u76ee\u524d\u7684\u81ea\u52a8\u8bc4\u4f30\u6307\u6807\u548c LLM-as-a-judge \u7cfb\u7edf\u5728\u8bc4\u4f30\u81ea\u52a8\u751f\u6210\u7684\u79d1\u5b66\u5199\u4f5c\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u65e0\u6cd5\u638c\u63e1\u4e13\u5bb6\u504f\u597d\u548c\u9886\u57df\u7279\u5b9a\u7684\u8d28\u91cf\u6807\u51c6\u3002\u56e0\u6b64\uff0c\u9700\u8981\u5f00\u53d1\u4e00\u79cd\u65b0\u7684\u8bc4\u4f30\u6846\u67b6\u6765\u652f\u6301\u4eba\u673a\u534f\u4f5c\u5199\u4f5c\u3002", "method": "\u63d0\u51fa GREP\uff08\u4e00\u4e2a\u591a\u8f6e\u8bc4\u4f30\u6846\u67b6\uff09\uff0c\u96c6\u6210\u4e86\u7ecf\u5178\u7684\u6587\u732e\u8bc4\u4f30\u6807\u51c6\u548c\u4e13\u5bb6\u504f\u597d\uff0c\u901a\u8fc7\u7ec6\u7c92\u5ea6\u7684\u7ef4\u5ea6\u8bc4\u4f30\u548c\u5bf9\u6bd4\u5c11\u6837\u672c\u793a\u4f8b\u6765\u63d0\u4f9b\u8be6\u7ec6\u7684\u4e0a\u4e0b\u6587\u6307\u5bfc\u3002GREP \u6709\u4e24\u4e2a\u7248\u672c\uff1a\u4e00\u4e2a\u4f7f\u7528\u4e13\u6709 LLM \u4f5c\u4e3a\u8bc4\u4f30\u5668\uff0c\u53e6\u4e00\u4e2a\u4f7f\u7528\u5f00\u653e\u6743\u91cd LLM\u3002", "result": "GREP \u6846\u67b6\u6bd4\u6807\u51c6\u7684 LLM \u88c1\u5224\u66f4\u80fd\u7a33\u5065\u5730\u8bc4\u4f30\u6587\u732e\u90e8\u5206\u7684\u8d28\u91cf\uff0c\u5e76\u4e14\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8bc4\u4f30\u9ad8\u5ea6\u76f8\u5173\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684 LLM \u5728\u6ee1\u8db3\u5408\u9002\u7684\u6587\u732e\u90e8\u5206\u7684\u9a8c\u8bc1\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u5728\u6839\u636e\u53cd\u9988\u8fdb\u884c\u6539\u8fdb\u65b9\u9762\u4e5f\u5b58\u5728\u4e0d\u8db3\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a GREP \u7684\u591a\u8f6e\u8bc4\u4f30\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u79d1\u5b66\u5199\u4f5c\uff0c\u7279\u522b\u662f\u53c2\u8003\u6587\u732e\u7684\u8d28\u91cf\u3002GREP \u7ed3\u5408\u4e86\u7ecf\u5178\u7684\u6587\u732e\u8bc4\u4f30\u6807\u51c6\u548c\u4e13\u5bb6\u7279\u5b9a\u7684\u504f\u597d\uff0c\u5e76\u5c06\u8bc4\u4f30\u5206\u89e3\u4e3a\u7ec6\u7c92\u5ea6\u7684\u7ef4\u5ea6\uff0c\u8f85\u4ee5\u5bf9\u6bd4\u5c11\u6837\u672c\u793a\u4f8b\u4ee5\u63d0\u4f9b\u8be6\u7ec6\u7684\u4e0a\u4e0b\u6587\u6307\u5bfc\u3002\u5b9e\u9a8c\u8868\u660e\uff0cGREP \u6bd4\u6807\u51c6\u7684 LLM \u88c1\u5224\u66f4\u80fd\u7a33\u5065\u5730\u8bc4\u4f30\u6587\u732e\u90e8\u5206\u7684\u8d28\u91cf\uff0c\u5e76\u4e0e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8bc4\u4f30\u9ad8\u5ea6\u76f8\u5173\u3002\u7814\u7a76\u8fd8\u53d1\u73b0\uff0c\u6700\u5148\u8fdb\u7684 LLM \u5728\u6ee1\u8db3\u5408\u9002\u7684\u6587\u732e\u90e8\u5206\u7684\u9a8c\u8bc1\u7ea6\u675f\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u5728\u6839\u636e\u53cd\u9988\u8fdb\u884c\u6539\u8fdb\u65b9\u9762\u4e5f\u5b58\u5728\u4e0d\u8db3\u3002"}}
{"id": "2508.07041", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07041", "abs": "https://arxiv.org/abs/2508.07041", "authors": ["Junkai Liu", "Nay Aung", "Theodoros N. Arvanitis", "Stefan K. Piechnik", "Joao A C Lima", "Steffen E. Petersen", "Le Zhang"], "title": "SAGCNet: Spatial-Aware Graph Completion Network for Missing Slice Imputation in Population CMR Imaging", "comment": "Accepted by MICCAI 2025", "summary": "Magnetic resonance imaging (MRI) provides detailed soft-tissue\ncharacteristics that assist in disease diagnosis and screening. However, the\naccuracy of clinical practice is often hindered by missing or unusable slices\ndue to various factors. Volumetric MRI synthesis methods have been developed to\naddress this issue by imputing missing slices from available ones. The inherent\n3D nature of volumetric MRI data, such as cardiac magnetic resonance (CMR),\nposes significant challenges for missing slice imputation approaches, including\n(1) the difficulty of modeling local inter-slice correlations and dependencies\nof volumetric slices, and (2) the limited exploration of crucial 3D spatial\ninformation and global context. In this study, to mitigate these issues, we\npresent Spatial-Aware Graph Completion Network (SAGCNet) to overcome the\ndependency on complete volumetric data, featuring two main innovations: (1) a\nvolumetric slice graph completion module that incorporates the inter-slice\nrelationships into a graph structure, and (2) a volumetric spatial adapter\ncomponent that enables our model to effectively capture and utilize various\nforms of 3D spatial context. Extensive experiments on cardiac MRI datasets\ndemonstrate that SAGCNet is capable of synthesizing absent CMR slices,\noutperforming competitive state-of-the-art MRI synthesis methods both\nquantitatively and qualitatively. Notably, our model maintains superior\nperformance even with limited slice data.", "AI": {"tldr": "SAGCNet\u662f\u4e00\u79cd\u7528\u4e8e\u5fc3\u810fMRI\u76843D\u91cd\u5efa\u65b9\u6cd5\uff0c\u901a\u8fc7\u56fe\u7ed3\u6784\u548c\u7a7a\u95f4\u9002\u914d\u5668\u6765\u8865\u5168\u7f3a\u5931\u7684\u5207\u7247\uff0c\u5373\u4f7f\u6570\u636e\u6709\u9650\u4e5f\u80fd\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89e3\u51b3\u4e86\u73b0\u6709\u4e09\u7ef4\u78c1\u5171\u632f\u6210\u50cf\uff08MRI\uff09\u6570\u636e\uff08\u5982\u5fc3\u810f\u78c1\u5171\u632f\uff08CMR\uff09\uff09\u5728\u7f3a\u5931\u5207\u7247\u65f6\u96be\u4ee5\u5efa\u6a21\u5207\u7247\u95f4\u7684\u5c40\u90e8\u76f8\u5173\u6027\u548c\u4f9d\u8d56\u6027\uff0c\u4ee5\u53ca\u6709\u9650\u5730\u63a2\u7d22\u4e09\u7ef4\u7a7a\u95f4\u4fe1\u606f\u548c\u5168\u5c40\u4e0a\u4e0b\u6587\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u7a7a\u95f4\u611f\u77e5\u56fe\u8865\u5168\u7f51\u7edc\uff08SAGCNet\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1.\u4e00\u4e2a\u7a7a\u95f4\u611f\u77e5\u56fe\u8865\u5168\u6a21\u5757\uff0c\u5c06\u5207\u7247\u95f4\u7684\u5173\u7cfb\u6784\u5efa\u6210\u56fe\u7ed3\u6784\uff1b2.\u4e00\u4e2a\u7a7a\u95f4\u9002\u914d\u7ec4\u4ef6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u6355\u6349\u548c\u5229\u7528\u5404\u79cd\u5f62\u5f0f\u7684\u4e09\u7ef4\u7a7a\u95f4\u4e0a\u4e0b\u6587\u3002", "result": "\u5728\u5fc3\u810fMRI\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cSAGCNet\u80fd\u591f\u5408\u6210\u7f3a\u5931\u7684CMR\u5207\u7247\uff0c\u5e76\u4e14\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u7ade\u4e89\u6027\u7684\u6700\u5148\u8fdb\u7684MRI\u5408\u6210\u65b9\u6cd5\u3002", "conclusion": "SAGCNet\u5728CMR\u6570\u636e\u4e0a\u80fd\u591f\u5408\u6210\u7f3a\u5931\u7684CMR\u5207\u7247\uff0c\u5e76\u4e14\u5728\u5b9a\u91cf\u548c\u5b9a\u6027\u65b9\u9762\u5747\u4f18\u4e8e\u5176\u4ed6\u6700\u5148\u8fdb\u7684MRI\u5408\u6210\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u5373\u4f7f\u5728\u5207\u7247\u6570\u636e\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u6a21\u578b\u4e5f\u80fd\u4fdd\u6301\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07345", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07345", "abs": "https://arxiv.org/abs/2508.07345", "authors": ["Samiha Afaf Neha", "Abir Ahammed Bhuiyan", "Md. Ishrak Khan"], "title": "ProteoKnight: Convolution-based phage virion protein classification and uncertainty analysis", "comment": null, "summary": "\\textbf{Introduction:} Accurate prediction of Phage Virion Proteins (PVP) is\nessential for genomic studies due to their crucial role as structural elements\nin bacteriophages. Computational tools, particularly machine learning, have\nemerged for annotating phage protein sequences from high-throughput sequencing.\nHowever, effective annotation requires specialized sequence encodings. Our\npaper introduces ProteoKnight, a new image-based encoding method that addresses\nspatial constraints in existing techniques, yielding competitive performance in\nPVP classification using pre-trained convolutional neural networks.\nAdditionally, our study evaluates prediction uncertainty in binary PVP\nclassification through Monte Carlo Dropout (MCD). \\textbf{Methods:}\nProteoKnight adapts the classical DNA-Walk algorithm for protein sequences,\nincorporating pixel colors and adjusting walk distances to capture intricate\nprotein features. Encoded sequences were classified using multiple pre-trained\nCNNs. Variance and entropy measures assessed prediction uncertainty across\nproteins of various classes and lengths. \\textbf{Results:} Our experiments\nachieved 90.8% accuracy in binary classification, comparable to\nstate-of-the-art methods. Multi-class classification accuracy remains\nsuboptimal. Our uncertainty analysis unveils variability in prediction\nconfidence influenced by protein class and sequence length.\n\\textbf{Conclusions:} Our study surpasses frequency chaos game representation\n(FCGR) by introducing novel image encoding that mitigates spatial information\nloss limitations. Our classification technique yields accurate and robust PVP\npredictions while identifying low-confidence predictions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aProteoKnight\u7684\u86cb\u767d\u8d28\u5e8f\u5217\u56fe\u50cf\u7f16\u7801\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e\u9884\u6d4b\u566c\u83cc\u4f53\u75c5\u6bd2\u9897\u7c92\u86cb\u767d\uff08PVP\uff09\u3002\u8be5\u65b9\u6cd5\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8690.8%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u80fd\u8bc4\u4f30\u9884\u6d4b\u7684\u4e0d\u786e\u5b9a\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6280\u672f\u5728\u8ba1\u7b97\u6ce8\u91ca\u566c\u83cc\u4f53\u86cb\u767d\u5e8f\u5217\u65f6\u5b58\u5728\u7684\u7a7a\u95f4\u9650\u5236\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u57fa\u56e0\u7ec4\u7814\u7a76\u4e2d\u566c\u83cc\u4f53\u75c5\u6bd2\u9897\u7c92\u86cb\u767d\uff08PVP\uff09\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u672c\u7814\u7a76\u5f15\u5165\u4e86\u65b0\u7684\u57fa\u4e8e\u56fe\u50cf\u7684\u7f16\u7801\u65b9\u6cd5ProteoKnight\u3002", "method": "ProteoKnight\u5c06\u7ecf\u5178\u7684DNA-Walk\u7b97\u6cd5\u5e94\u7528\u4e8e\u86cb\u767d\u8d28\u5e8f\u5217\uff0c\u901a\u8fc7\u50cf\u7d20\u989c\u8272\u548c\u8c03\u6574\u6b65\u957f\u6765\u6355\u6349\u590d\u6742\u7684\u86cb\u767d\u8d28\u7279\u5f81\u3002\u4f7f\u7528\u591a\u79cd\u9884\u8bad\u7ec3\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u5bf9\u7f16\u7801\u540e\u7684\u5e8f\u5217\u8fdb\u884c\u5206\u7c7b\uff0c\u5e76\u901a\u8fc7\u65b9\u5dee\u548c\u71b5\u5ea6\u91cf\u6765\u8bc4\u4f30\u4e0d\u540c\u7c7b\u522b\u548c\u957f\u5ea6\u86cb\u767d\u8d28\u7684\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "result": "ProteoKnight\u5728\u4e8c\u5143\u5206\u7c7b\u4efb\u52a1\u4e2d\u8fbe\u5230\u4e8690.8%\u7684\u51c6\u786e\u7387\uff0c\u4e0e\u73b0\u6709\u6700\u4f73\u65b9\u6cd5\u76f8\u5f53\uff0c\u4f46\u5728\u591a\u7c7b\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u4ecd\u6709\u5f85\u63d0\u9ad8\u3002\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u663e\u793a\uff0c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u56e0\u86cb\u767d\u8d28\u7c7b\u522b\u548c\u5e8f\u5217\u957f\u5ea6\u800c\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u65b0\u7684\u56fe\u50cf\u7f16\u7801\u65b9\u6cd5\uff0c\u8d85\u8d8a\u4e86\u9891\u7387\u6df7\u6c8c\u6e38\u620f\u8868\u793a\uff08FCGR\uff09\uff0c\u89e3\u51b3\u4e86\u7a7a\u95f4\u4fe1\u606f\u4e22\u5931\u7684\u9650\u5236\u3002\u8be5\u5206\u7c7b\u6280\u672f\u80fd\u591f\u51c6\u786e\u3001\u7a33\u5065\u5730\u9884\u6d4b\u566c\u83cc\u4f53\u75c5\u6bd2\u9897\u7c92\u86cb\u767d\uff08PVP\uff09\uff0c\u540c\u65f6\u8fd8\u80fd\u8bc6\u522b\u7f6e\u4fe1\u5ea6\u8f83\u4f4e\u7684\u9884\u6d4b\u3002"}}
{"id": "2508.07959", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07959", "abs": "https://arxiv.org/abs/2508.07959", "authors": ["Changhao Song", "Yazhou Zhang", "Hui Gao", "Ben Yao", "Peng Zhang"], "title": "Large Language Models for Subjective Language Understanding: A Survey", "comment": null, "summary": "Subjective language understanding refers to a broad set of natural language\nprocessing tasks where the goal is to interpret or generate content that\nconveys personal feelings, opinions, or figurative meanings rather than\nobjective facts. With the advent of large language models (LLMs) such as\nChatGPT, LLaMA, and others, there has been a paradigm shift in how we approach\nthese inherently nuanced tasks. In this survey, we provide a comprehensive\nreview of recent advances in applying LLMs to subjective language tasks,\nincluding sentiment analysis, emotion recognition, sarcasm detection, humor\nunderstanding, stance detection, metaphor interpretation, intent detection, and\naesthetics assessment. We begin by clarifying the definition of subjective\nlanguage from linguistic and cognitive perspectives, and we outline the unique\nchallenges posed by subjective language (e.g. ambiguity, figurativeness,\ncontext dependence). We then survey the evolution of LLM architectures and\ntechniques that particularly benefit subjectivity tasks, highlighting why LLMs\nare well-suited to model subtle human-like judgments. For each of the eight\ntasks, we summarize task definitions, key datasets, state-of-the-art LLM-based\nmethods, and remaining challenges. We provide comparative insights, discussing\ncommonalities and differences among tasks and how multi-task LLM approaches\nmight yield unified models of subjectivity. Finally, we identify open issues\nsuch as data limitations, model bias, and ethical considerations, and suggest\nfuture research directions. We hope this survey will serve as a valuable\nresource for researchers and practitioners interested in the intersection of\naffective computing, figurative language processing, and large-scale language\nmodels.", "AI": {"tldr": "\u672c\u7efc\u8ff0\u5168\u9762\u4ecb\u7ecd\u4e86LLM\u5728\u60c5\u611f\u5206\u6790\u3001\u8bbd\u523a\u68c0\u6d4b\u7b49\u4e3b\u89c2\u8bed\u8a00\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u8fdb\u5c55\u3001\u6311\u6218\u548c\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u968f\u7740LLM\uff08\u5982ChatGPT\u3001LLaMA\uff09\u7684\u51fa\u73b0\uff0c\u4e3b\u89c2\u8bed\u8a00\u7406\u89e3\u4efb\u52a1\u7684\u8303\u5f0f\u53d1\u751f\u4e86\u8f6c\u53d8\uff0c\u672c\u7efc\u8ff0\u65e8\u5728\u5168\u9762\u56de\u987eLLM\u5728\u8fd9\u4e9b\u4efb\u52a1\u4e0a\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "method": "\u5bf9LLM\u5728\u4e3b\u89c2\u8bed\u8a00\u4efb\u52a1\uff08\u5982\u60c5\u611f\u5206\u6790\u3001\u60c5\u7eea\u8bc6\u522b\u3001\u8bbd\u523a\u68c0\u6d4b\u3001\u5e7d\u9ed8\u7406\u89e3\u3001\u7acb\u573a\u68c0\u6d4b\u3001\u9690\u55bb\u89e3\u91ca\u3001\u610f\u56fe\u68c0\u6d4b\u548c\u7f8e\u5b66\u8bc4\u4f30\uff09\u4e0a\u7684\u5e94\u7528\u8fdb\u884c\u5168\u9762\u7efc\u8ff0\uff0c\u603b\u7ed3\u4e86\u4efb\u52a1\u5b9a\u4e49\u3001\u6570\u636e\u96c6\u3001SOTA LLM\u65b9\u6cd5\u53ca\u6311\u6218\uff0c\u5e76\u8fdb\u884c\u4e86\u6bd4\u8f83\u548c\u8ba8\u8bba\u3002", "result": "LLM\u975e\u5e38\u9002\u5408\u5904\u7406\u4e3b\u89c2\u8bed\u8a00\u7684\u7ec6\u5fae\u5dee\u522b\uff0c\u5e76\u5df2\u5728\u591a\u79cd\u4e3b\u89c2\u8bed\u8a00\u4efb\u52a1\u4e2d\u53d6\u5f97\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u4ecd\u5b58\u5728\u6570\u636e\u9650\u5236\u3001\u6a21\u578b\u504f\u89c1\u548c\u4f26\u7406\u8003\u91cf\u7b49\u95ee\u9898\u3002", "conclusion": "LLMs\u5728\u7406\u89e3\u548c\u751f\u6210\u4e3b\u89c2\u8bed\u8a00\u65b9\u9762\u5c55\u73b0\u51fa\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u4e5f\u9762\u4e34\u6570\u636e\u9650\u5236\u3001\u6a21\u578b\u504f\u89c1\u548c\u4f26\u7406\u7b49\u6311\u6218\uff0c\u672a\u6765\u7814\u7a76\u5e94\u5173\u6ce8\u8fd9\u4e9b\u95ee\u9898\u5e76\u63a2\u7d22\u591a\u4efb\u52a1\u7edf\u4e00\u6a21\u578b\u3002"}}
{"id": "2508.07083", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07083", "abs": "https://arxiv.org/abs/2508.07083", "authors": ["Yueyu Hu", "Ran Gong", "Tingyu Fan", "Yao Wang"], "title": "TeSO: Representing and Compressing 3D Point Cloud Scenes with Textured Surfel Octree", "comment": null, "summary": "3D visual content streaming is a key technology for emerging 3D telepresence\nand AR/VR applications. One fundamental element underlying the technology is a\nversatile 3D representation that is capable of producing high-quality renders\nand can be efficiently compressed at the same time. Existing 3D representations\nlike point clouds, meshes and 3D Gaussians each have limitations in terms of\nrendering quality, surface definition, and compressibility. In this paper, we\npresent the Textured Surfel Octree (TeSO), a novel 3D representation that is\nbuilt from point clouds but addresses the aforementioned limitations. It\nrepresents a 3D scene as cube-bounded surfels organized on an octree, where\neach surfel is further associated with a texture patch. By approximating a\nsmooth surface with a large surfel at a coarser level of the octree, it reduces\nthe number of primitives required to represent the 3D scene, and yet retains\nthe high-frequency texture details through the texture map attached to each\nsurfel. We further propose a compression scheme to encode the geometry and\ntexture efficiently, leveraging the octree structure. The proposed textured\nsurfel octree combined with the compression scheme achieves higher rendering\nquality at lower bit-rates compared to multiple point cloud and 3D\nGaussian-based baselines.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TeSO \u7684\u65b0\u578b 3D \u8868\u793a\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u7eb9\u7406\u8868\u9762\u5143\u516b\u53c9\u6811\u6765\u63d0\u9ad8\u6e32\u67d3\u8d28\u91cf\u548c\u538b\u7f29\u6548\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u7684\u70b9\u4e91\u548c 3D \u9ad8\u65af\u57fa\u5143\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684 3D \u8868\u793a\u65b9\u6cd5\uff08\u5982\u70b9\u4e91\u3001\u7f51\u683c\u548c 3D \u9ad8\u65af\u57fa\u5143\uff09\u5728\u6e32\u67d3\u8d28\u91cf\u3001\u8868\u9762\u5b9a\u4e49\u548c\u53ef\u538b\u7f29\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u800c\u9ad8\u8d28\u91cf\u7684 3D \u8868\u793a\u662f 3D \u8fdc\u7a0b\u5448\u73b0\u548c AR/VR \u5e94\u7528\u7684\u5173\u952e\u6280\u672f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a TeSO\uff08Textured Surfel Octree\uff09\u7684\u65b0\u578b 3D \u8868\u793a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u57fa\u4e8e\u70b9\u4e91\uff0c\u4f46\u901a\u8fc7\u5c06 3D \u573a\u666f\u8868\u793a\u4e3a\u7ec4\u7ec7\u5728\u516b\u53c9\u6811\u4e0a\u7684\u7acb\u65b9\u4f53\u8fb9\u754c\u8868\u9762\u5143\uff08surfel\uff09\uff0c\u5e76\u4e3a\u6bcf\u4e2a\u8868\u9762\u5143\u5173\u8054\u4e00\u4e2a\u7eb9\u7406\u5757\uff0c\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002\u901a\u8fc7\u5728\u516b\u53c9\u6811\u7684\u7c97\u7cd9\u7ea7\u522b\u7528\u5927\u7684\u8868\u9762\u5143\u8fd1\u4f3c\u5e73\u6ed1\u8868\u9762\uff0c\u53ef\u4ee5\u51cf\u5c11\u8868\u793a 3D \u573a\u666f\u6240\u9700\u56fe\u5143\u7684\u6570\u91cf\uff0c\u540c\u65f6\u901a\u8fc7\u6bcf\u4e2a\u8868\u9762\u5143\u9644\u52a0\u7684\u7eb9\u7406\u56fe\u4fdd\u7559\u9ad8\u9891\u7eb9\u7406\u7ec6\u8282\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u516b\u53c9\u6811\u7ed3\u6784\u6709\u6548\u7f16\u7801\u51e0\u4f55\u548c\u7eb9\u7406\u7684\u538b\u7f29\u65b9\u6848\u3002", "result": "TeSO \u76f8\u6bd4\u73b0\u6709\u7684\u57fa\u4e8e\u70b9\u4e91\u548c 3D \u9ad8\u65af\u57fa\u5143\u7684\u57fa\u7ebf\uff0c\u5728\u8f83\u4f4e\u7684\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u3002", "conclusion": "TeSO \u7ed3\u5408\u6240\u63d0\u51fa\u7684\u538b\u7f29\u65b9\u6848\uff0c\u5728\u8f83\u4f4e\u7684\u6bd4\u7279\u7387\u4e0b\u5b9e\u73b0\u4e86\u6bd4\u591a\u4e2a\u57fa\u4e8e\u70b9\u4e91\u548c 3D \u9ad8\u65af\u57fa\u5143\u7684\u57fa\u7ebf\u66f4\u9ad8\u7684\u6e32\u67d3\u8d28\u91cf\u3002"}}
{"id": "2508.07370", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07370", "abs": "https://arxiv.org/abs/2508.07370", "authors": ["Sibylle Marcotte", "Gabriel Peyr\u00e9", "R\u00e9mi Gribonval"], "title": "Intrinsic training dynamics of deep neural networks", "comment": null, "summary": "A fundamental challenge in the theory of deep learning is to understand\nwhether gradient-based training in high-dimensional parameter spaces can be\ncaptured by simpler, lower-dimensional structures, leading to so-called\nimplicit bias. As a stepping stone, we study when a gradient flow on a\nhigh-dimensional variable $\\theta$ implies an intrinsic gradient flow on a\nlower-dimensional variable $z = \\phi(\\theta)$, for an architecture-related\nfunction $\\phi$. We express a so-called intrinsic dynamic property and show how\nit is related to the study of conservation laws associated with the\nfactorization $\\phi$. This leads to a simple criterion based on the inclusion\nof kernels of linear maps which yields a necessary condition for this property\nto hold. We then apply our theory to general ReLU networks of arbitrary depth\nand show that, for any initialization, it is possible to rewrite the flow as an\nintrinsic dynamic in a lower dimension that depends only on $z$ and the\ninitialization, when $\\phi$ is the so-called path-lifting. In the case of\nlinear networks with $\\phi$ the product of weight matrices, so-called balanced\ninitializations are also known to enable such a dimensionality reduction; we\ngeneralize this result to a broader class of {\\em relaxed balanced}\ninitializations, showing that, in certain configurations, these are the\n\\emph{only} initializations that ensure the intrinsic dynamic property.\nFinally, for the linear neural ODE associated with the limit of infinitely deep\nlinear networks, with relaxed balanced initialization, we explicitly express\nthe corresponding intrinsic dynamics.", "AI": {"tldr": "\u6df1\u5ea6\u5b66\u4e60\u68af\u5ea6\u6d41\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u53ef\u4ee5\u88ab\u4f4e\u7ef4\u7ed3\u6784\u6355\u83b7\uff0c\u7279\u522b\u662f\u901a\u8fc7\u8def\u5f84\u63d0\u5347\u548c\u653e\u677e\u7684\u5747\u8861\u521d\u59cb\u5316\uff0c\u8fd9\u5728ReLU\u7f51\u7edc\u548c\u7ebf\u6027\u7f51\u7edc\u4e2d\u5f97\u5230\u4e86\u8bc1\u660e\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u7406\u8bba\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6311\u6218\u662f\u7406\u89e3\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u7684\u68af\u5ea6\u4e0b\u964d\u8bad\u7ec3\u8fc7\u7a0b\u662f\u5426\u53ef\u4ee5\u88ab\u66f4\u7b80\u5355\u7684\u4f4e\u7ef4\u7ed3\u6784\u6240\u63cf\u8ff0\uff0c\u5373\u6240\u8c13\u7684\u9690\u5f0f\u504f\u501a\u3002\u8fd9\u9879\u5de5\u4f5c\u65e8\u5728\u901a\u8fc7\u7814\u7a76\u9ad8\u7ef4\u53d8\u91cf\u7684\u68af\u5ea6\u6d41\u5982\u4f55\u7ea6\u675f\u4f4e\u7ef4\u53d8\u91cf\u7684\u5185\u5728\u68af\u5ea6\u6d41\u6765\u89e3\u51b3\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u7814\u7a76\u57fa\u4e8e\u68af\u5ea6\u6d41\u548c\u56e0\u5b50\u5206\u89e3\u7406\u8bba\uff0c\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u7ebf\u6027\u6620\u5c04\u6838\u7684\u5224\u636e\uff0c\u7528\u4e8e\u5224\u65ad\u9ad8\u7ef4\u68af\u5ea6\u6d41\u662f\u5426\u80fd\u88ab\u8868\u793a\u4e3a\u4f4e\u7ef4\u5185\u5728\u68af\u5ea6\u6d41\u3002\u8be5\u7406\u8bba\u88ab\u5e94\u7528\u4e8eReLU\u7f51\u7edc\u548c\u7ebf\u6027\u7f51\u7edc\uff0c\u5e76\u901a\u8fc7\u5206\u6790\u8def\u5f84\u63d0\u5347\u548c\u653e\u677e\u7684\u5747\u8861\u521d\u59cb\u5316\u6765\u8bc1\u660e\u964d\u7ef4\u7684\u53ef\u80fd\u6027\u548c\u5fc5\u8981\u6027\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u5bf9\u4e8eReLU\u7f51\u7edc\uff0c\u8def\u5f84\u63d0\u5347\u53ef\u4ee5\u5b9e\u73b0\u4efb\u610f\u6df1\u5ea6\u7684\u964d\u7ef4\u5185\u5728\u52a8\u529b\u5b66\u3002\u5bf9\u4e8e\u7ebf\u6027\u7f51\u7edc\uff0c\u653e\u677e\u7684\u5747\u8861\u521d\u59cb\u5316\u662f\u786e\u4fdd\u5185\u5728\u52a8\u529b\u5b66\u6027\u8d28\u7684\u552f\u4e00\u521d\u59cb\u5316\u7c7b\u578b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u663e\u5f0f\u8868\u8fbe\u4e86\u4e0e\u65e0\u9650\u6df1\u5ea6\u7ebf\u6027\u7f51\u7edc\u76f8\u5173\u7684\u7ebf\u6027\u795e\u7ecf\u7f51\u7edcODE\u7684\u5185\u5728\u52a8\u529b\u5b66\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5173\u4e8e\u68af\u5ea6\u6d41\u5728\u9ad8\u7ef4\u53c2\u6570\u7a7a\u95f4\u4e2d\u662f\u5426\u80fd\u88ab\u4f4e\u7ef4\u7ed3\u6784\u6355\u83b7\u7684\u7406\u8bba\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8eReLU\u7f51\u7edc\u548c\u7ebf\u6027\u7f51\u7edc\u3002\u7814\u7a76\u8868\u660e\uff0c\u7279\u5b9a\u7684\u521d\u59cb\u5316\uff08\u5982\u8def\u5f84\u63d0\u5347\u548c\u653e\u677e\u7684\u5747\u8861\u521d\u59cb\u5316\uff09\u53ef\u4ee5\u5b9e\u73b0\u8fd9\u79cd\u964d\u7ef4\uff0c\u5e76\u4e3a\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc\u7684\u5185\u5728\u52a8\u529b\u5b66\u63d0\u4f9b\u4e86\u4e00\u4e2a\u663e\u5f0f\u8868\u8fbe\u5f0f\u3002"}}
{"id": "2508.07964", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07964", "abs": "https://arxiv.org/abs/2508.07964", "authors": ["Matthias Sperber", "Maureen de Seyssel", "Jiajun Bao", "Matthias Paulik"], "title": "Toward Machine Interpreting: Lessons from Human Interpreting Studies", "comment": null, "summary": "Current speech translation systems, while having achieved impressive\naccuracies, are rather static in their behavior and do not adapt to real-world\nsituations in ways human interpreters do. In order to improve their practical\nusefulness and enable interpreting-like experiences, a precise understanding of\nthe nature of human interpreting is crucial. To this end, we discuss human\ninterpreting literature from the perspective of the machine translation field,\nwhile considering both operational and qualitative aspects. We identify\nimplications for the development of speech translation systems and argue that\nthere is great potential to adopt many human interpreting principles using\nrecent modeling techniques. We hope that our findings provide inspiration for\nclosing the perceived usability gap, and can motivate progress toward true\nmachine interpreting.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u4e0e\u4eba\u5de5\u540c\u4f20\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u901a\u8fc7\u501f\u9274\u4eba\u7c7b\u7ffb\u8bd1\u7684\u539f\u5219\u548c\u5148\u8fdb\u7684\u5efa\u6a21\u6280\u672f\u6765\u6539\u8fdb\u5176\u6027\u80fd\u548c\u7528\u6237\u4f53\u9a8c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u7684\u5b9e\u7528\u6027\u5e76\u5b9e\u73b0\u7c7b\u4f3c\u4eba\u5de5\u540c\u4f20\u7684\u4f53\u9a8c\uff0c\u7406\u89e3\u4eba\u7c7b\u7ffb\u8bd1\u7684\u672c\u8d28\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9\u6bd4\u5206\u6790\u673a\u5668\u7ffb\u8bd1\u9886\u57df\u548c\u4eba\u7c7b\u7ffb\u8bd1\u6587\u732e\uff0c\u63a2\u8ba8\u4e86\u4eba\u7c7b\u7ffb\u8bd1\u7684\u672c\u8d28\uff0c\u5e76\u8bc6\u522b\u4e86\u5176\u5bf9\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u5f00\u53d1\u7684\u542f\u793a\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u4eba\u7c7b\u7ffb\u8bd1\u7684\u539f\u5219\u5e94\u7528\u4e8e\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u53ef\u4ee5\u5229\u7528\u6700\u65b0\u7684\u5efa\u6a21\u6280\u672f\u6765\u6539\u8fdb\u7528\u6237\u4f53\u9a8c\u3002", "conclusion": "\u8be5\u7814\u7a76\u8ba4\u4e3a\uff0c\u901a\u8fc7\u501f\u9274\u4eba\u7c7b\u7ffb\u8bd1\u7684\u539f\u5219\u5e76\u7ed3\u5408\u5148\u8fdb\u7684\u5efa\u6a21\u6280\u672f\uff0c\u6709\u6f5c\u529b\u5f25\u5408\u73b0\u6709\u8bed\u97f3\u7ffb\u8bd1\u7cfb\u7edf\u4e0e\u7406\u60f3\u673a\u5668\u7ffb\u8bd1\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63a8\u52a8\u673a\u5668\u7ffb\u8bd1\u7684\u8fdb\u6b65\u3002"}}
{"id": "2508.07392", "categories": ["cs.LG", "math.ST", "stat.ML", "stat.TH"], "pdf": "https://arxiv.org/pdf/2508.07392", "abs": "https://arxiv.org/abs/2508.07392", "authors": ["Nikita Puchkin", "Denis Suchkov", "Alexey Naumov", "Denis Belomestny"], "title": "Tight Bounds for Schr\u00f6dinger Potential Estimation in Unpaired Image-to-Image Translation Problems", "comment": "54 pages, 4 figures", "summary": "Modern methods of generative modelling and unpaired image-to-image\ntranslation based on Schr\\\"odinger bridges and stochastic optimal control\ntheory aim to transform an initial density to a target one in an optimal way.\nIn the present paper, we assume that we only have access to i.i.d. samples from\ninitial and final distributions. This makes our setup suitable for both\ngenerative modelling and unpaired image-to-image translation. Relying on the\nstochastic optimal control approach, we choose an Ornstein-Uhlenbeck process as\nthe reference one and estimate the corresponding Schr\\\"odinger potential.\nIntroducing a risk function as the Kullback-Leibler divergence between\ncouplings, we derive tight bounds on generalization ability of an empirical\nrisk minimizer in a class of Schr\\\"odinger potentials including Gaussian\nmixtures. Thanks to the mixing properties of the Ornstein-Uhlenbeck process, we\nalmost achieve fast rates of convergence up to some logarithmic factors in\nfavourable scenarios. We also illustrate performance of the suggested approach\nwith numerical experiments.", "AI": {"tldr": "This paper uses stochastic optimal control and an Ornstein-Uhlenbeck process for generative modeling and image-to-image translation, achieving near-optimal convergence rates with theoretical guarantees and numerical validation.", "motivation": "The paper addresses generative modeling and unpaired image-to-image translation by transforming an initial density to a target density, assuming access only to i.i.d. samples from both distributions.", "method": "We employed a stochastic optimal control approach using an Ornstein-Uhlenbeck process as a reference and estimated the corresponding Schr\"odinger potential. A risk function based on the Kullback-Leibler divergence between couplings was introduced to derive generalization bounds.", "result": "The proposed approach shows performance illustrated through numerical experiments and achieves near-optimal convergence rates, up to logarithmic factors, in favorable scenarios due to the mixing properties of the Ornstein-Uhlenbeck process.", "conclusion": "We derived tight bounds on the generalization ability of an empirical risk minimizer in a class of Schr\"odinger potentials, achieving near-optimal convergence rates thanks to the properties of the Ornstein-Uhlenbeck process."}}
{"id": "2508.07969", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07969", "abs": "https://arxiv.org/abs/2508.07969", "authors": ["David Arps", "Hassan Sajjad", "Laura Kallmeyer"], "title": "Understanding Syntactic Generalization in Structure-inducing Language Models", "comment": "Code available at https://github.com/davidarps/silm", "summary": "Structure-inducing Language Models (SiLM) are trained on a self-supervised\nlanguage modeling task, and induce a hierarchical sentence representation as a\nbyproduct when processing an input. A wide variety of SiLMs have been proposed.\nHowever, these have typically been evaluated on a relatively small scale, and\nevaluation of these models has systematic gaps and lacks comparability. In this\nwork, we study three different SiLM architectures using both natural language\n(English) corpora and synthetic bracketing expressions: Structformer (Shen et\nal., 2021), UDGN (Shen et al., 2022) and GPST (Hu et al., 2024). We compare\nthem with respect to (i) properties of the induced syntactic representations\n(ii) performance on grammaticality judgment tasks, and (iii) training dynamics.\nWe find that none of the three architectures dominates across all evaluation\nmetrics. However, there are significant differences, in particular with respect\nto the induced syntactic representations. The Generative Pretrained Structured\nTransformer (GPST; Hu et al. 2024) performs most consistently across evaluation\nsettings, and outperforms the other models on long-distance dependencies in\nbracketing expressions. Furthermore, our study shows that small models trained\non large amounts of synthetic data provide a useful testbed for evaluating\nbasic model properties.", "AI": {"tldr": "\u5bf9\u4e09\u79cdSiLM\u6a21\u578b\uff08Structformer, UDGN, GPST\uff09\u7684\u53e5\u6cd5\u8868\u793a\u3001\u8bed\u6cd5\u5224\u65ad\u4efb\u52a1\u8868\u73b0\u548c\u8bad\u7ec3\u52a8\u6001\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002\u53d1\u73b0GPST\u6a21\u578b\u8868\u73b0\u6700\u7a33\u5b9a\uff0c\u5c24\u5176\u5728\u957f\u8ddd\u79bb\u4f9d\u8d56\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002\u5c0f\u578b\u6a21\u578b\u5728\u5408\u6210\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u662f\u8bc4\u4f30\u6a21\u578b\u5c5e\u6027\u7684\u6709\u6548\u9014\u5f84\u3002", "motivation": "\u73b0\u6709\u7684SiLM\u6a21\u578b\u5728\u8bc4\u4f30\u89c4\u6a21\u4e0a\u76f8\u5bf9\u8f83\u5c0f\uff0c\u4e14\u8bc4\u4f30\u65b9\u6cd5\u5b58\u5728\u7cfb\u7edf\u6027\u5dee\u8ddd\u548c\u53ef\u6bd4\u6027\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u8fdb\u884c\u66f4\u5168\u9762\u7684\u7814\u7a76\u6765\u7406\u89e3\u548c\u6bd4\u8f83\u4e0d\u540c\u7684SiLM\u67b6\u6784\u3002", "method": "\u901a\u8fc7\u81ea\u7136\u8bed\u8a00\uff08\u82f1\u8bed\uff09\u8bed\u6599\u5e93\u548c\u5408\u6210\u62ec\u53f7\u8868\u8fbe\u5f0f\uff0c\u5bf9Structformer\u3001UDGN\u548cGPST\u4e09\u79cdSiLM\u6a21\u578b\u5728\uff08i\uff09\u8bf1\u5bfc\u53e5\u6cd5\u8868\u793a\u7684\u5c5e\u6027\u3001\uff08ii\uff09\u8bed\u6cd5\u5224\u65ad\u4efb\u52a1\u7684\u8868\u73b0\u548c\uff08iii\uff09\u8bad\u7ec3\u52a8\u6001\u65b9\u9762\u8fdb\u884c\u4e86\u6bd4\u8f83\u548c\u8bc4\u4f30\u3002", "result": "\u5728\u53e5\u6cd5\u8868\u793a\u65b9\u9762\u5b58\u5728\u663e\u8457\u5dee\u5f02\uff0cGPST\u6a21\u578b\u5728\u5404\u9879\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4e3a\u7a33\u5b9a\uff0c\u5728\u5408\u6210\u62ec\u53f7\u8868\u8fbe\u5f0f\u7684\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u5904\u7406\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002", "conclusion": "SiLM\u6a21\u578b\u5728\u4e0d\u540c\u8bc4\u4f30\u6307\u6807\u4e0a\u6ca1\u6709\u8868\u73b0\u51fa\u7edd\u5bf9\u4f18\u52bf\uff0c\u4f46GPST\u6a21\u578b\u5728\u5404\u9879\u8bc4\u4f30\u4e2d\u8868\u73b0\u6700\u4e3a\u7a33\u5b9a\uff0c\u5728\u5904\u7406\u957f\u8ddd\u79bb\u4f9d\u8d56\u5173\u7cfb\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\u3002\u5c0f\u578b\u6a21\u578b\u5728\u5927\u91cf\u5408\u6210\u6570\u636e\u4e0a\u7684\u8bad\u7ec3\u662f\u8bc4\u4f30\u6a21\u578b\u57fa\u672c\u5c5e\u6027\u7684\u6709\u6548\u65b9\u6cd5\u3002"}}
{"id": "2508.07092", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07092", "abs": "https://arxiv.org/abs/2508.07092", "authors": ["Yue Hu", "Juntong Peng", "Yunqiao Yang", "Siheng Chen"], "title": "Communication-Efficient Multi-Agent 3D Detection via Hybrid Collaboration", "comment": null, "summary": "Collaborative 3D detection can substantially boost detection performance by\nallowing agents to exchange complementary information. It inherently results in\na fundamental trade-off between detection performance and communication\nbandwidth. To tackle this bottleneck issue, we propose a novel hybrid\ncollaboration that adaptively integrates two types of communication messages:\nperceptual outputs, which are compact, and raw observations, which offer richer\ninformation. This approach focuses on two key aspects: i) integrating\ncomplementary information from two message types and ii) prioritizing the most\ncritical data within each type. By adaptively selecting the most critical set\nof messages, it ensures optimal perceptual information and adaptability,\neffectively meeting the demands of diverse communication scenarios.Building on\nthis hybrid collaboration, we present \\texttt{HyComm}, a\ncommunication-efficient LiDAR-based collaborative 3D detection system.\n\\texttt{HyComm} boasts two main benefits: i) it facilitates adaptable\ncompression rates for messages, addressing various communication requirements,\nand ii) it uses standardized data formats for messages. This ensures they are\nindependent of specific detection models, fostering adaptability across\ndifferent agent configurations. To evaluate HyComm, we conduct experiments on\nboth real-world and simulation datasets: DAIR-V2X and OPV2V. HyComm\nconsistently outperforms previous methods and achieves a superior\nperformance-bandwidth trade-off regardless of whether agents use the same or\nvaried detection models. It achieves a lower communication volume of more than\n2,006$\\times$ and still outperforms Where2comm on DAIR-V2X in terms of AP50.\nThe related code will be released.", "AI": {"tldr": "HyComm \u662f\u4e00\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u6fc0\u5149\u96f7\u8fbe\u57fa\u7840\u534f\u540c 3D \u68c0\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u6df7\u5408\u901a\u4fe1\u7b56\u7565\u5728\u6027\u80fd\u548c\u901a\u4fe1\u5e26\u5bbd\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5e73\u8861\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u534f\u540c 3D \u68c0\u6d4b\u4e2d\u6027\u80fd\u4e0e\u901a\u4fe1\u5e26\u5bbd\u4e4b\u95f4\u7684\u56fa\u6709\u6743\u8861\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u901a\u4fe1\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u534f\u4f5c\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u81ea\u9002\u5e94\u5730\u96c6\u6210\u4e24\u79cd\u901a\u4fe1\u6d88\u606f\uff1a\u611f\u77e5\u8f93\u51fa\u548c\u539f\u59cb\u89c2\u6d4b\u3002\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6700\u5173\u952e\u7684\u6d88\u606f\u96c6\uff0c\u786e\u4fdd\u4e86\u6700\u4f18\u7684\u611f\u77e5\u4fe1\u606f\u548c\u9002\u5e94\u6027\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86 HyComm \u7cfb\u7edf\uff0c\u5b83\u652f\u6301\u6d88\u606f\u7684\u81ea\u9002\u5e94\u538b\u7f29\u7387\uff0c\u5e76\u4f7f\u7528\u6807\u51c6\u5316\u7684\u6d88\u606f\u6570\u636e\u683c\u5f0f\uff0c\u4f7f\u5176\u72ec\u7acb\u4e8e\u7279\u5b9a\u7684\u68c0\u6d4b\u6a21\u578b\u3002", "result": "HyComm \u5728 DAIR-V2X \u548c OPV2V \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5728\u901a\u4fe1\u5e26\u5bbd\u548c\u68c0\u6d4b\u6027\u80fd\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u66f4\u597d\u7684\u7ed3\u679c\uff0c\u901a\u4fe1\u91cf\u51cf\u5c11\u4e86 2,006 \u500d\u4ee5\u4e0a\uff0c\u540c\u65f6\u5728 AP50 \u6307\u6807\u4e0a\u4f18\u4e8e Where2comm\u3002", "conclusion": "HyComm \u7cfb\u7edf\u7684\u901a\u4fe1\u6548\u7387\u548c\u68c0\u6d4b\u6027\u80fd\u5728\u771f\u5b9e\u4e16\u754c\u548c\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u90fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728 AP50 \u6307\u6807\u4e0a\u8d85\u8d8a\u4e86 Where2comm\u3002"}}
{"id": "2508.07395", "categories": ["cs.LG", "68Q32"], "pdf": "https://arxiv.org/pdf/2508.07395", "abs": "https://arxiv.org/abs/2508.07395", "authors": ["Behnoush Khavari", "Mehran Shakerinava", "Jayesh Khullar", "Jerry Huang", "Fran\u00e7ois Rivest", "Siamak Ravanbakhsh", "Sarath Chandar"], "title": "Parity Requires Unified Input Dependence and Negative Eigenvalues in SSMs", "comment": "5 pages. Accepted at ICML 2025 Workshop on Methods and Opportunities\n  at Small Scale", "summary": "Recent work has shown that LRNN models such as S4D, Mamba, and DeltaNet lack\nstate-tracking capability due to either time-invariant transition matrices or\nrestricted eigenvalue ranges. To address this, input-dependent transition\nmatrices, particularly those that are complex or non-triangular, have been\nproposed to enhance SSM performance on such tasks. While existing theorems\ndemonstrate that both input-independent and non-negative SSMs are incapable of\nsolving simple state-tracking tasks, such as parity, regardless of depth, they\ndo not explore whether combining these two types in a multilayer SSM could\nhelp. We investigate this question for efficient SSMs with diagonal transition\nmatrices and show that such combinations still fail to solve parity. This\nimplies that a recurrence layer must both be input-dependent and include\nnegative eigenvalues. Our experiments support this conclusion by analyzing an\nSSM model that combines S4D and Mamba layers.", "AI": {"tldr": "SSM\u9700\u8981\u8f93\u5165\u4f9d\u8d56\u6027\u548c\u8d1f\u7279\u5f81\u503c\u624d\u80fd\u6709\u6548\u5904\u7406\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\uff0c\u5355\u7eaf\u5806\u53e0\u6216\u589e\u52a0\u6df1\u5ea6\uff08\u5373\u4f7f\u7ed3\u5408\u4e86S4D\u548cMamba\u5c42\uff09\u662f\u4e0d\u591f\u7684\u3002", "motivation": "\u63a2\u7a76\u5c06\u8f93\u5165\u65e0\u5173\u548c\u975e\u8d1fSSM\u7ec4\u5408\u80fd\u5426\u89e3\u51b3\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\uff0c\u5982\u5947\u5076\u6821\u9a8c\u3002", "method": "\u901a\u8fc7\u5206\u6790\u5177\u6709\u5bf9\u89d2\u7ebf\u8f6c\u79fb\u77e9\u9635\u7684SSM\u6a21\u578b\uff0c\u5e76\u8fdb\u884c\u5b9e\u9a8c\u9a8c\u8bc1\uff0c\u5bf9\u6bd4\u4e86S4D\u548cMamba\u5c42\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u5373\u4f7f\u7ec4\u5408\u4e86S4D\u548cMamba\u5c42\uff0cSSM\u4ecd\u7136\u65e0\u6cd5\u89e3\u51b3\u5947\u5076\u6821\u9a8c\u4efb\u52a1\uff0c\u9664\u975e\u8f6c\u79fb\u77e9\u9635\u5177\u6709\u8f93\u5165\u4f9d\u8d56\u6027\u548c\u8d1f\u7279\u5f81\u503c\u3002", "conclusion": "SSM\u65e0\u6cd5\u4ec5\u4ec5\u901a\u8fc7\u5806\u53e0\u6216\u589e\u52a0\u6df1\u5ea6\u6765\u89e3\u51b3\u72b6\u6001\u8ddf\u8e2a\u4efb\u52a1\uff0c\u9700\u8981\u540c\u65f6\u5177\u5907\u8f93\u5165\u4f9d\u8d56\u6027\u548c\u8d1f\u7279\u5f81\u503c\u3002"}}
{"id": "2508.07976", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07976", "abs": "https://arxiv.org/abs/2508.07976", "authors": ["Jiaxuan Gao", "Wei Fu", "Minyang Xie", "Shusheng Xu", "Chuyi He", "Zhiyu Mei", "Banghua Zhu", "Yi Wu"], "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale Asynchronous RL", "comment": null, "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.", "AI": {"tldr": "ASearcher \u662f\u4e00\u4e2a\u5f00\u6e90\u9879\u76ee\uff0c\u901a\u8fc7\u5927\u89c4\u6a21\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u641c\u7d22\u667a\u80fd\u4f53\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5728\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u663e\u8457\u63d0\u9ad8\u4e86\u641c\u7d22\u667a\u80fd\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u5f00\u6e90\u667a\u80fd\u4f53\u5728\u89e3\u51b3\u6a21\u7cca\u67e5\u8be2\u3001\u751f\u6210\u7cbe\u786e\u641c\u7d22\u3001\u5206\u6790\u7ed3\u679c\u548c\u8fdb\u884c\u5f7b\u5e95\u63a2\u7d22\u7b49\u65b9\u9762\u7684\u641c\u7d22\u667a\u80fd\u80fd\u529b\u4e0d\u8db3\uff0c\u5e76\u4e14\u5728\u53ef\u6269\u5c55\u6027\u3001\u6548\u7387\u548c\u6570\u636e\u8d28\u91cf\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "ASearcher \u9879\u76ee\u901a\u8fc7\u5b8c\u5168\u5f02\u6b65\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u6765\u652f\u6301\u957f\u65f6\u671f\u641c\u7d22\u5e76\u4fdd\u6301\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u5229\u7528\u57fa\u4e8e\u63d0\u793a\u7684 LLM \u667a\u80fd\u4f53\u81ea\u4e3b\u5408\u6210\u9ad8\u8d28\u91cf\u3001\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\u3002", "result": "ASearcher \u9879\u76ee\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8bad\u7ec3\uff0c\u5728 xBench \u548c GAIA \u4e0a\u5206\u522b\u53d6\u5f97\u4e86 46.7% \u548c 20.8% \u7684 Avg@4 \u63d0\u5347\u3002", "conclusion": "ASearcher-Web-QwQ \u5728 xBench \u548c GAIA \u4e0a\u5206\u522b\u53d6\u5f97\u4e86 42.1 \u548c 52.8 \u7684 Avg@4 \u5206\u6570\uff0c\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u5f00\u6e90 32B \u667a\u80fd\u4f53\uff0c\u5e76\u4e14\u8be5\u667a\u80fd\u4f53\u5177\u6709\u6781\u957f\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u5728\u8bad\u7ec3\u671f\u95f4\u5de5\u5177\u8c03\u7528\u6b21\u6570\u8d85\u8fc7 40 \u6b21\uff0c\u8f93\u51fa\u4ee4\u724c\u8d85\u8fc7 150k \u6b21\u3002"}}
{"id": "2508.07112", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07112", "abs": "https://arxiv.org/abs/2508.07112", "authors": ["Nikolai Warner", "Wenjin Zhang", "Irfan Essa", "Apaar Sadhwani"], "title": "AugLift: Boosting Generalization in Lifting-based 3D Human Pose Estimation", "comment": "Preprint. Under review", "summary": "Lifting-based methods for 3D Human Pose Estimation (HPE), which predict 3D\nposes from detected 2D keypoints, often generalize poorly to new datasets and\nreal-world settings. To address this, we propose \\emph{AugLift}, a simple yet\neffective reformulation of the standard lifting pipeline that significantly\nimproves generalization performance without requiring additional data\ncollection or sensors. AugLift sparsely enriches the standard input -- the 2D\nkeypoint coordinates $(x, y)$ -- by augmenting it with a keypoint detection\nconfidence score $c$ and a corresponding depth estimate $d$. These additional\nsignals are computed from the image using off-the-shelf, pre-trained models\n(e.g., for monocular depth estimation), thereby inheriting their strong\ngeneralization capabilities. Importantly, AugLift serves as a modular add-on\nand can be readily integrated into existing lifting architectures.\n  Our extensive experiments across four datasets demonstrate that AugLift\nboosts cross-dataset performance on unseen datasets by an average of $10.1\\%$,\nwhile also improving in-distribution performance by $4.0\\%$. These gains are\nconsistent across various lifting architectures, highlighting the robustness of\nour method. Our analysis suggests that these sparse, keypoint-aligned cues\nprovide robust frame-level context, offering a practical way to significantly\nimprove the generalization of any lifting-based pose estimation model. Code\nwill be made publicly available.", "AI": {"tldr": "AugLift \u662f\u4e00\u79cd\u7b80\u5355\u800c\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u6dfb\u52a0\u7f6e\u4fe1\u5ea6\u548c\u6df1\u5ea6\u4fe1\u606f\u6765\u589e\u5f3a 2D \u5173\u952e\u70b9\uff0c\u4ece\u800c\u63d0\u9ad8 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u63d0\u5347\uff08lifting-based\uff09\u7684 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\u65b9\u6cd5\u5728\u63a8\u5e7f\u5230\u65b0\u6570\u636e\u96c6\u548c\u771f\u5b9e\u4e16\u754c\u573a\u666f\u65f6\u6cdb\u5316\u80fd\u529b\u8f83\u5dee\u3002", "method": "AugLift \u901a\u8fc7\u5728 2D \u5173\u952e\u70b9\u5750\u6807 $(x, y)$ \u4e2d\u52a0\u5165\u5173\u952e\u70b9\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u5206\u6570 $c$ \u548c\u76f8\u5e94\u7684\u6df1\u5ea6\u4f30\u8ba1 $d$ \u6765\u7a00\u758f\u5730\u4e30\u5bcc\u8f93\u5165\u4fe1\u606f\u3002\u8fd9\u4e9b\u989d\u5916\u7684\u4fe1\u53f7\u662f\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\uff0c\u4f7f\u7528\u4e86\u73b0\u6210\u7684\u3001\u9884\u8bad\u7ec3\u7684\u6a21\u578b\uff08\u4f8b\u5982\uff0c\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u6a21\u578b\uff09\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cAugLift \u5c06\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u96c6\u4e0a\u7684\u8de8\u6570\u636e\u96c6\u6027\u80fd\u5e73\u5747\u63d0\u9ad8\u4e86 10.1%\uff0c\u540c\u65f6\u5728\u5206\u5e03\u5185\u6027\u80fd\u4e5f\u63d0\u9ad8\u4e86 4.0%\u3002\u8fd9\u4e9b\u63d0\u5347\u5728\u5404\u79cd\u63d0\u5347\u67b6\u6784\u4e2d\u662f\u4e00\u81f4\u7684\uff0c\u8868\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "AugLift \u901a\u8fc7\u7a00\u758f\u5730\u589e\u5f3a\u6807\u51c6\u8f93\u5165\uff082D \u5173\u952e\u70b9\u5750\u6807\uff09\u5e76\u52a0\u5165\u5173\u952e\u70b9\u68c0\u6d4b\u7f6e\u4fe1\u5ea6\u5206\u6570\u548c\u76f8\u5e94\u7684\u6df1\u5ea6\u4f30\u8ba1\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8 3D \u4eba\u4f53\u59ff\u52bf\u4f30\u8ba1\uff08HPE\uff09\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u65e0\u9700\u989d\u5916\u7684\u6570\u636e\u6536\u96c6\u6216\u4f20\u611f\u5668\u3002\u8be5\u65b9\u6cd5\u5177\u6709\u6a21\u5757\u5316\u548c\u6613\u4e8e\u96c6\u6210\u5230\u73b0\u6709\u6a21\u578b\u4e2d\u7684\u4f18\u70b9\u3002"}}
{"id": "2508.07400", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07400", "abs": "https://arxiv.org/abs/2508.07400", "authors": ["Mohamad Louai Shehab", "Alperen Tercan", "Necmiye Ozay"], "title": "Efficient Reward Identification In Max Entropy Reinforcement Learning with Sparsity and Rank Priors", "comment": null, "summary": "In this paper, we consider the problem of recovering time-varying reward\nfunctions from either optimal policies or demonstrations coming from a max\nentropy reinforcement learning problem. This problem is highly ill-posed\nwithout additional assumptions on the underlying rewards. However, in many\napplications, the rewards are indeed parsimonious, and some prior information\nis available. We consider two such priors on the rewards: 1) rewards are mostly\nconstant and they change infrequently, 2) rewards can be represented by a\nlinear combination of a small number of feature functions. We first show that\nthe reward identification problem with the former prior can be recast as a\nsparsification problem subject to linear constraints. Moreover, we give a\npolynomial-time algorithm that solves this sparsification problem exactly.\nThen, we show that identifying rewards representable with the minimum number of\nfeatures can be recast as a rank minimization problem subject to linear\nconstraints, for which convex relaxations of rank can be invoked. In both\ncases, these observations lead to efficient optimization-based reward\nidentification algorithms. Several examples are given to demonstrate the\naccuracy of the recovered rewards as well as their generalizability.", "AI": {"tldr": "\u4ece\u6700\u4f18\u7b56\u7565\u6216\u6f14\u793a\u4e2d\u6062\u590d\u65f6\u95f4\u53d8\u5316\u7684\u5956\u52b1\u51fd\u6570\u3002\u5229\u7528\u5956\u52b1\u51fd\u6570\u7a00\u758f\u6216\u4f4e\u79e9\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u7a00\u758f\u5316\u6216\u79e9\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u4f18\u5316\u7684\u7b97\u6cd5\u6765\u89e3\u51b3\u3002", "motivation": "\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5956\u52b1\u51fd\u6570\u5bf9\u4e8e\u5b66\u4e60\u6700\u4f18\u7b56\u7565\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u5728\u8bb8\u591a\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u5956\u52b1\u51fd\u6570\u662f\u672a\u77e5\u7684\uff0c\u5e76\u4e14\u53ef\u80fd\u662f\u968f\u65f6\u95f4\u53d8\u5316\u7684\u3002\u4ece\u6700\u4f18\u7b56\u7565\u6216\u6f14\u793a\u4e2d\u6062\u590d\u8fd9\u4e9b\u65f6\u95f4\u53d8\u5316\u7684\u5956\u52b1\u51fd\u6570\u662f\u4e00\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u56e0\u4e3a\u8be5\u95ee\u9898\u672c\u8eab\u662f\u75c5\u6001\u7684\uff0c\u9700\u8981\u989d\u5916\u7684\u5047\u8bbe\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u5956\u52b1\u51fd\u6570\u7684\u6062\u590d\u5177\u6709\u91cd\u8981\u7684\u7406\u8bba\u548c\u5b9e\u8df5\u610f\u4e49\u3002", "method": "\u6211\u4eec\u8003\u8651\u4e86\u4ece\u6700\u4f18\u7b56\u7565\u6216\u6700\u5927\u71b5\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u7684\u6f14\u793a\u4e2d\u6062\u590d\u65f6\u95f4\u53d8\u5316\u7684\u5956\u52b1\u51fd\u6570\u3002\u6211\u4eec\u63d0\u51fa\u4e86\u4e24\u79cd\u65b9\u6cd5\uff1a1\uff09\u5229\u7528\u5956\u52b1\u51fd\u6570\u4e3b\u8981\u6052\u5b9a\u4e14\u53d8\u5316\u4e0d\u9891\u7e41\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u7a00\u758f\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e00\u4e2a\u7cbe\u786e\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u30022\uff09\u5229\u7528\u5956\u52b1\u51fd\u6570\u53ef\u4ee5\u8868\u793a\u4e3a\u5c11\u91cf\u7279\u5f81\u51fd\u6570\u7ebf\u6027\u7ec4\u5408\u7684\u5148\u9a8c\u4fe1\u606f\uff0c\u5c06\u95ee\u9898\u8f6c\u5316\u4e3a\u79e9\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u5e76\u5e94\u7528\u79e9\u7684\u51f8\u677e\u5f1b\u3002", "result": "\u6211\u4eec\u63d0\u51fa\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u7b97\u6cd5\u5728\u8bc6\u522b\u65f6\u95f4\u53d8\u5316\u7684\u5956\u52b1\u51fd\u6570\u65b9\u9762\u8868\u73b0\u51fa\u4e86\u51c6\u786e\u6027\u548c\u53ef\u63a8\u5e7f\u6027\u3002\u901a\u8fc7\u5b9e\u9a8c\u8bc1\u660e\uff0c\u6211\u4eec\u7684\u7b97\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4ece\u6700\u4f18\u7b56\u7565\u6216\u6f14\u793a\u4e2d\u6062\u590d\u51fa\u771f\u5b9e\u7684\u5956\u52b1\u51fd\u6570\uff0c\u5373\u4f7f\u5728\u5b58\u5728\u566a\u58f0\u6216\u4e0d\u5b8c\u6574\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4f18\u5316\u7684\u65b9\u6cd5\u6765\u8bc6\u522b\u65f6\u95f4\u53d8\u5316\u5956\u52b1\u51fd\u6570\u3002\u6211\u4eec\u8bc1\u660e\u4e86\u5177\u6709\u7a00\u758f\u5148\u9a8c\u7684\u5956\u52b1\u8bc6\u522b\u95ee\u9898\u53ef\u4ee5\u8f6c\u5316\u4e3a\u7a00\u758f\u5316\u95ee\u9898\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7cbe\u786e\u7684\u591a\u9879\u5f0f\u65f6\u95f4\u7b97\u6cd5\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u8868\u660e\uff0c\u5177\u6709\u4f4e\u79e9\u5148\u9a8c\u7684\u5956\u52b1\u8bc6\u522b\u95ee\u9898\u53ef\u4ee5\u8f6c\u5316\u4e3a\u79e9\u6700\u5c0f\u5316\u95ee\u9898\uff0c\u53ef\u4ee5\u901a\u8fc7\u51f8\u677e\u5f1b\u6765\u89e3\u51b3\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u5bfc\u81f4\u4e86\u6709\u6548\u7684\u57fa\u4e8e\u4f18\u5316\u7684\u5956\u52b1\u8bc6\u522b\u7b97\u6cd5\u3002"}}
{"id": "2508.07993", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07993", "abs": "https://arxiv.org/abs/2508.07993", "authors": ["Anna Sofia Lippolis", "Andrea Giovanni Nuzzolese", "Aldo Gangemi"], "title": "The Medical Metaphors Corpus (MCC)", "comment": null, "summary": "Metaphor is a fundamental cognitive mechanism that shapes scientific\nunderstanding, enabling the communication of complex concepts while potentially\nconstraining paradigmatic thinking. Despite the prevalence of figurative\nlanguage in scientific discourse, existing metaphor detection resources\nprimarily focus on general-domain text, leaving a critical gap for\ndomain-specific applications. In this paper, we present the Medical Metaphors\nCorpus (MCC), a comprehensive dataset of 792 annotated scientific conceptual\nmetaphors spanning medical and biological domains. MCC aggregates metaphorical\nexpressions from diverse sources including peer-reviewed literature, news\nmedia, social media discourse, and crowdsourced contributions, providing both\nbinary and graded metaphoricity judgments validated through human annotation.\nEach instance includes source-target conceptual mappings and perceived\nmetaphoricity scores on a 0-7 scale, establishing the first annotated resource\nfor computational scientific metaphor research. Our evaluation demonstrates\nthat state-of-the-art language models achieve modest performance on scientific\nmetaphor detection, revealing substantial room for improvement in\ndomain-specific figurative language understanding. MCC enables multiple\nresearch applications including metaphor detection benchmarking, quality-aware\ngeneration systems, and patient-centered communication tools.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u533b\u5b66\u9690\u55bb\u8bed\u6599\u5e93\uff08MCC\uff09\uff0c\u4e00\u4e2a\u5305\u542b 792 \u4e2a\u533b\u5b66\u548c\u751f\u7269\u5b66\u9886\u57df\u9690\u55bb\u7684\u6570\u636e\u96c6\uff0c\u53ef\u7528\u4e8e\u79d1\u5b66\u9690\u55bb\u7814\u7a76\u548c\u5e94\u7528\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9690\u55bb\u68c0\u6d4b\u8d44\u6e90\u4e3b\u8981\u96c6\u4e2d\u5728\u901a\u7528\u9886\u57df\u6587\u672c\uff0c\u800c\u5ffd\u7565\u4e86\u7279\u5b9a\u9886\u57df\uff08\u5982\u533b\u5b66\u548c\u751f\u7269\u5b66\uff09\u5e94\u7528\u7684\u4e0d\u8db3\u3002", "method": "\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b 792 \u4e2a\u533b\u5b66\u548c\u751f\u7269\u5b66\u9886\u57df\u79d1\u5b66\u6982\u5ff5\u9690\u55bb\u7684\u8bed\u6599\u5e93\uff08MCC\uff09\uff0c\u6536\u96c6\u4e86\u6765\u81ea\u540c\u884c\u8bc4\u5ba1\u6587\u732e\u3001\u65b0\u95fb\u5a92\u4f53\u3001\u793e\u4ea4\u5a92\u4f53\u8ba8\u8bba\u548c\u4f17\u5305\u8d21\u732e\u7684\u9690\u55bb\u8868\u8fbe\uff0c\u5e76\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u8fdb\u884c\u4e86\u4e8c\u5143\u548c\u5206\u7ea7\u7684\u9690\u55bb\u6027\u5224\u65ad\u9a8c\u8bc1\uff0c\u5305\u62ec\u6765\u6e90-\u76ee\u6807\u6982\u5ff5\u6620\u5c04\u548c 0-7 \u5206\u5236\u7684\u9690\u55bb\u611f\u77e5\u8bc4\u5206\u3002", "result": "MCC \u662f\u8ba1\u7b97\u79d1\u5b66\u9690\u55bb\u7814\u7a76\u7684\u9996\u4e2a\u6807\u6ce8\u8d44\u6e90\u3002\u8bc4\u4f30\u8868\u660e\uff0c\u6700\u5148\u8fdb\u7684\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u9690\u55bb\u68c0\u6d4b\u65b9\u9762\u7684\u8868\u73b0\u5c1a\u53ef\uff0c\u4f46\u4ecd\u6709\u5f88\u5927\u7684\u63d0\u5347\u7a7a\u95f4\u3002MCC \u53ef\u7528\u4e8e\u9690\u55bb\u68c0\u6d4b\u57fa\u51c6\u6d4b\u8bd5\u3001\u8d28\u91cf\u611f\u77e5\u751f\u6210\u7cfb\u7edf\u548c\u4ee5\u60a3\u8005\u4e3a\u4e2d\u5fc3\u7684\u6c9f\u901a\u5de5\u5177\u7b49\u591a\u79cd\u7814\u7a76\u5e94\u7528\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u533b\u5b66\u9690\u55bb\u8bed\u6599\u5e93\uff08MCC\uff09\uff0c\u8fd9\u662f\u4e00\u4e2a\u5305\u542b 792 \u4e2a\u533b\u5b66\u548c\u751f\u7269\u5b66\u9886\u57df\u79d1\u5b66\u6982\u5ff5\u9690\u55bb\u7684\u7efc\u5408\u6570\u636e\u96c6\uff0c\u65e8\u5728\u586b\u8865\u7279\u5b9a\u9886\u57df\u9690\u55bb\u68c0\u6d4b\u8d44\u6e90\u7684\u7a7a\u767d\u3002\u8be5\u8bed\u6599\u5e93\u901a\u8fc7\u4eba\u5de5\u6807\u6ce8\u6536\u96c6\u4e86\u4e8c\u5143\u548c\u5206\u7ea7\u7684\u9690\u55bb\u5224\u65ad\uff0c\u5e76\u63d0\u4f9b\u4e86\u6765\u6e90-\u76ee\u6807\u6982\u5ff5\u6620\u5c04\u548c 0-7 \u5206\u5236\u7684\u9690\u55bb\u611f\u77e5\u8bc4\u5206\uff0c\u4e3a\u8ba1\u7b97\u79d1\u5b66\u9690\u55bb\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u6807\u6ce8\u8d44\u6e90\u3002"}}
{"id": "2508.07128", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07128", "abs": "https://arxiv.org/abs/2508.07128", "authors": ["Gregory Schuit", "Denis Parra", "Cecilia Besa"], "title": "Perceptual Evaluation of GANs and Diffusion Models for Generating X-rays", "comment": "Accepted to the Workshop on Human-AI Collaboration at MICCAI 2025", "summary": "Generative image models have achieved remarkable progress in both natural and\nmedical imaging. In the medical context, these techniques offer a potential\nsolution to data scarcity-especially for low-prevalence anomalies that impair\nthe performance of AI-driven diagnostic and segmentation tools. However,\nquestions remain regarding the fidelity and clinical utility of synthetic\nimages, since poor generation quality can undermine model generalizability and\ntrust. In this study, we evaluate the effectiveness of state-of-the-art\ngenerative models-Generative Adversarial Networks (GANs) and Diffusion Models\n(DMs)-for synthesizing chest X-rays conditioned on four abnormalities:\nAtelectasis (AT), Lung Opacity (LO), Pleural Effusion (PE), and Enlarged\nCardiac Silhouette (ECS). Using a benchmark composed of real images from the\nMIMIC-CXR dataset and synthetic images from both GANs and DMs, we conducted a\nreader study with three radiologists of varied experience. Participants were\nasked to distinguish real from synthetic images and assess the consistency\nbetween visual features and the target abnormality. Our results show that while\nDMs generate more visually realistic images overall, GANs can report better\naccuracy for specific conditions, such as absence of ECS. We further identify\nvisual cues radiologists use to detect synthetic images, offering insights into\nthe perceptual gaps in current models. These findings underscore the\ncomplementary strengths of GANs and DMs and point to the need for further\nrefinement to ensure generative models can reliably augment training datasets\nfor AI diagnostic systems.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u8bc4\u4f30\u4e86 GANs \u548c DMs \u5728\u5408\u6210\u80f8\u90e8 X \u5149\u7247\u4ee5\u7528\u4e8e AI \u8bca\u65ad\u8bad\u7ec3\u65b9\u9762\u7684\u8868\u73b0\u3002\u7ed3\u679c\u663e\u793a DMs \u751f\u6210\u7684\u56fe\u50cf\u66f4\u903c\u771f\uff0c\u4f46 GANs \u5728\u67d0\u4e9b\u7279\u5b9a\u6761\u4ef6\u4e0b\u53ef\u80fd\u66f4\u51c6\u786e\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u4e86\u533a\u5206\u771f\u5b9e\u4e0e\u5408\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u5e76\u5f3a\u8c03\u9700\u8981\u8fdb\u4e00\u6b65\u6539\u8fdb\u4ee5\u786e\u4fdd\u751f\u6210\u6a21\u578b\u80fd\u53ef\u9760\u5730\u7528\u4e8e\u533b\u7597 AI\u3002", "motivation": "\u5728\u533b\u5b66\u5f71\u50cf\u9886\u57df\uff0c\u751f\u6210\u6a21\u578b\uff08\u5982 GANs \u548c DMs\uff09\u6709\u6f5c\u529b\u89e3\u51b3\u6570\u636e\u7a00\u7f3a\u7684\u95ee\u9898\uff0c\u7279\u522b\u662f\u5728\u4f4e\u60a3\u75c5\u7387\u5f02\u5e38\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd9\u4e9b\u5f02\u5e38\u4f1a\u5f71\u54cd AI \u8bca\u65ad\u548c\u5206\u5272\u5de5\u5177\u7684\u6027\u80fd\u3002\u7136\u800c\uff0c\u5408\u6210\u56fe\u50cf\u7684\u4fdd\u771f\u5ea6\u548c\u4e34\u5e8a\u6548\u7528\u4ecd\u7136\u5b58\u5728\u7591\u95ee\uff0c\u56e0\u4e3a\u8f83\u4f4e\u7684\u751f\u6210\u8d28\u91cf\u53ef\u80fd\u4f1a\u524a\u5f31\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u548c\u53ef\u4fe1\u5ea6\u3002\u56e0\u6b64\uff0c\u8bc4\u4f30\u8fd9\u4e9b\u751f\u6210\u6a21\u578b\u5728\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u6709\u6548\u6027\u81f3\u5173\u91cd\u8981\u3002", "method": "\u7814\u7a76\u8bc4\u4f30\u4e86\u6700\u5148\u8fdb\u7684\u751f\u6210\u6a21\u578b\uff08GANs \u548c DMs\uff09\u5728\u5408\u6210\u80f8\u90e8 X \u5149\u7247\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u5e76\u4ee5\u56db\u79cd\u5f02\u5e38\u4e3a\u6761\u4ef6\uff1aAtelectasis (AT)\u3001Lung Opacity (LO)\u3001Pleural Effusion (PE) \u548c Enlarged Cardiac Silhouette (ECS)\u3002\u7814\u7a76\u4eba\u5458\u4f7f\u7528\u4e86\u4e00\u4e2a\u5305\u542b\u6765\u81ea MIMIC-CXR \u6570\u636e\u96c6\u7684\u771f\u5b9e\u56fe\u50cf\u548c\u6765\u81ea GANs \u548c DMs \u7684\u5408\u6210\u56fe\u50cf\u7684\u57fa\u51c6\uff0c\u5e76\u8fdb\u884c\u4e86\u4e00\u9879\u6709\u4e09\u540d\u4e0d\u540c\u7ecf\u9a8c\u6c34\u5e73\u7684\u653e\u5c04\u79d1\u533b\u751f\u53c2\u4e0e\u7684\u8bfb\u8005\u7814\u7a76\u3002\u53c2\u4e0e\u8005\u88ab\u8981\u6c42\u533a\u5206\u771f\u5b9e\u56fe\u50cf\u548c\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u8bc4\u4f30\u89c6\u89c9\u7279\u5f81\u4e0e\u76ee\u6807\u5f02\u5e38\u4e4b\u95f4\u7684\u4e00\u81f4\u6027\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u751f\u6210\u7684\u56fe\u50cf\u5728\u89c6\u89c9\u4e0a\u66f4\u903c\u771f\u3002\u7136\u800c\uff0c\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u5728\u5904\u7406\u67d0\u4e9b\u7279\u5b9a\u6761\u4ef6\u65f6\uff0c\u4f8b\u5982 Enlarged Cardiac Silhouette (ECS) \u7684\u7f3a\u5931\uff0c\u53ef\u4ee5\u8fbe\u5230\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u8bc6\u522b\u51fa\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7528\u6765\u68c0\u6d4b\u5408\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u4e3a\u7406\u89e3\u5f53\u524d\u6a21\u578b\u5728\u611f\u77e5\u65b9\u9762\u7684\u4e0d\u8db3\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u8868\u660e\uff0c\u5c3d\u7ba1\u6269\u6563\u6a21\u578b\uff08DMs\uff09\u5728\u751f\u6210\u66f4\u903c\u771f\u7684\u533b\u5b66\u56fe\u50cf\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u52bf\uff0c\u4f46\u751f\u6210\u5bf9\u6297\u7f51\u7edc\uff08GANs\uff09\u5728\u67d0\u4e9b\u7279\u5b9a\u6761\u4ef6\u4e0b\uff08\u4f8b\u5982\uff0c\u4e0d\u5b58\u5728 Enlarged Cardiac Silhouette\uff09\u53ef\u4ee5\u63d0\u4f9b\u66f4\u597d\u7684\u51c6\u786e\u6027\u3002\u7814\u7a76\u8fd8\u6307\u51fa\u4e86\u653e\u5c04\u79d1\u533b\u751f\u7528\u4e8e\u68c0\u6d4b\u5408\u6210\u56fe\u50cf\u7684\u89c6\u89c9\u7ebf\u7d22\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u611f\u77e5\u65b9\u9762\u7684\u5dee\u8ddd\u3002\u8fd9\u4e9b\u53d1\u73b0\u5f3a\u8c03\u4e86 GANs \u548c DMs \u7684\u4e92\u8865\u4f18\u52bf\uff0c\u5e76\u6307\u51fa\u4e86\u8fdb\u4e00\u6b65\u4f18\u5316\u4ee5\u786e\u4fdd\u751f\u6210\u6a21\u578b\u80fd\u53ef\u9760\u5730\u589e\u5f3a AI \u8bca\u65ad\u7cfb\u7edf\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u5fc5\u8981\u6027\u3002"}}
{"id": "2508.07428", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07428", "abs": "https://arxiv.org/abs/2508.07428", "authors": ["Md Sultanul Arifin", "Abu Nowshed Sakib", "Yeasir Rayhan", "Tanzima Hashem"], "title": "Lightning Prediction under Uncertainty: DeepLight with Hazy Loss", "comment": null, "summary": "Lightning, a common feature of severe meteorological conditions, poses\nsignificant risks, from direct human injuries to substantial economic losses.\nThese risks are further exacerbated by climate change. Early and accurate\nprediction of lightning would enable preventive measures to safeguard people,\nprotect property, and minimize economic losses. In this paper, we present\nDeepLight, a novel deep learning architecture for predicting lightning\noccurrences. Existing prediction models face several critical limitations: they\noften struggle to capture the dynamic spatial context and inherent uncertainty\nof lightning events, underutilize key observational data, such as radar\nreflectivity and cloud properties, and rely heavily on Numerical Weather\nPrediction (NWP) systems, which are both computationally expensive and highly\nsensitive to parameter settings. To overcome these challenges, DeepLight\nleverages multi-source meteorological data, including radar reflectivity, cloud\nproperties, and historical lightning occurrences through a dual-encoder\narchitecture. By employing multi-branch convolution techniques, it dynamically\ncaptures spatial correlations across varying extents. Furthermore, its novel\nHazy Loss function explicitly addresses the spatio-temporal uncertainty of\nlightning by penalizing deviations based on proximity to true events, enabling\nthe model to better learn patterns amidst randomness. Extensive experiments\nshow that DeepLight improves the Equitable Threat Score (ETS) by 18%-30% over\nstate-of-the-art methods, establishing it as a robust solution for lightning\nprediction.", "AI": {"tldr": "DeepLight\u662f\u4e00\u79cd\u65b0\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u901a\u8fc7\u7ed3\u5408\u591a\u79cd\u6c14\u8c61\u6570\u636e\u548c\u521b\u65b0\u7684\u635f\u5931\u51fd\u6570\uff0c\u63d0\u9ad8\u4e86\u95ea\u7535\u9884\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u6548\u679c\u597d18%-30%\u3002", "motivation": "\u95ea\u7535\u4f5c\u4e3a\u4e00\u79cd\u5e38\u89c1\u7684\u6076\u52a3\u5929\u6c14\u73b0\u8c61\uff0c\u4f1a\u5bf9\u4eba\u7c7b\u751f\u547d\u548c\u8d22\u4ea7\u9020\u6210\u91cd\u5927\u98ce\u9669\uff0c\u6c14\u5019\u53d8\u5316\u8fdb\u4e00\u6b65\u52a0\u5267\u4e86\u8fd9\u4e9b\u98ce\u9669\u3002\u56e0\u6b64\uff0c\u65e9\u671f\u51c6\u786e\u5730\u9884\u6d4b\u95ea\u7535\u5bf9\u4e8e\u91c7\u53d6\u9884\u9632\u63aa\u65bd\u3001\u4fdd\u62a4\u4eba\u6c11\u751f\u547d\u8d22\u4ea7\u5b89\u5168\u81f3\u5173\u91cd\u8981\u3002", "method": "DeepLight\u5229\u7528\u591a\u6e90\u6c14\u8c61\u6570\u636e\uff08\u5305\u62ec\u96f7\u8fbe\u53cd\u5c04\u7387\u3001\u4e91\u5c5e\u6027\u548c\u5386\u53f2\u95ea\u7535\u53d1\u751f\u60c5\u51b5\uff09\u901a\u8fc7\u53cc\u7f16\u7801\u5668\u67b6\u6784\u6765\u9884\u6d4b\u95ea\u7535\u3002\u5b83\u91c7\u7528\u591a\u5206\u652f\u5377\u79ef\u6280\u672f\u6765\u52a8\u6001\u6355\u6349\u4e0d\u540c\u8303\u56f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\uff0c\u5e76\u4f7f\u7528\u65b0\u9896\u7684Hazy Loss\u51fd\u6570\u6765\u89e3\u51b3\u65f6\u7a7a\u4e0d\u786e\u5b9a\u6027\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cDeepLight\u7684ETS\u8bc4\u5206\u6bd4\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u63d0\u9ad8\u4e8618%-30%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u95ea\u7535\u9884\u6d4b\u65b9\u9762\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DeepLight\u901a\u8fc7\u5229\u7528\u591a\u6e90\u6c14\u8c61\u6570\u636e\u3001\u53cc\u7f16\u7801\u5668\u67b6\u6784\u3001\u591a\u5206\u652f\u5377\u79ef\u6280\u672f\u548cHazy Loss\u51fd\u6570\uff0c\u5728\u51c6\u786e\u9884\u6d4b\u95ea\u7535\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u5176ETS\u8bc4\u5206\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u9ad8\u4e8618%-30%\uff0c\u4e3a\u51cf\u5c11\u95ea\u7535\u9020\u6210\u7684\u5371\u5bb3\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07999", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07999", "abs": "https://arxiv.org/abs/2508.07999", "authors": ["Ryan Wong", "Jiawei Wang", "Junjie Zhao", "Li Chen", "Yan Gao", "Long Zhang", "Xuan Zhou", "Zuo Wang", "Kai Xiang", "Ge Zhang", "Wenhao Huang", "Yang Wang", "Ke Wang"], "title": "WideSearch: Benchmarking Agentic Broad Info-Seeking", "comment": null, "summary": "From professional research to everyday planning, many tasks are bottlenecked\nby wide-scale information seeking, which is more repetitive than cognitively\ncomplex. With the rapid development of Large Language Models (LLMs), automated\nsearch agents powered by LLMs offer a promising solution to liberate humans\nfrom this tedious work. However, the capability of these agents to perform such\n\"wide-context\" collection reliably and completely remains largely unevaluated\ndue to a lack of suitable benchmarks. To bridge this gap, we introduce\nWideSearch, a new benchmark engineered to evaluate agent reliability on these\nlarge-scale collection tasks. The benchmark features 200 manually curated\nquestions (100 in English, 100 in Chinese) from over 15 diverse domains,\ngrounded in real user queries. Each task requires agents to collect large-scale\natomic information, which could be verified one by one objectively, and arrange\nit into a well-organized output. A rigorous five-stage quality control pipeline\nensures the difficulty, completeness, and verifiability of the dataset. We\nbenchmark over 10 state-of-the-art agentic search systems, including\nsingle-agent, multi-agent frameworks, and end-to-end commercial systems. Most\nsystems achieve overall success rates near 0\\%, with the best performer\nreaching just 5\\%. However, given sufficient time, cross-validation by multiple\nhuman testers can achieve a near 100\\% success rate. These results demonstrate\nthat present search agents have critical deficiencies in large-scale\ninformation seeking, underscoring urgent areas for future research and\ndevelopment in agentic search. Our dataset, evaluation pipeline, and benchmark\nresults have been publicly released at https://widesearch-seed.github.io/", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86 WideSearch \u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5927\u89c4\u6a21\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u4e2d\u641c\u7d22\u4ee3\u7406\u7684\u53ef\u9760\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u4ee3\u7406\u7684\u8868\u73b0\u4e0d\u4f73\uff0c\u6210\u529f\u7387\u63a5\u8fd1 0%\uff0c\u800c\u4eba\u7c7b\u6d4b\u8bd5\u8005\u5728\u76f8\u540c\u4efb\u52a1\u4e0a\u6210\u529f\u7387\u63a5\u8fd1 100%\u3002", "motivation": "\u73b0\u6709\u7684\u641c\u7d22\u4ee3\u7406\u5728\u6267\u884c\u5927\u89c4\u6a21\u3001\u91cd\u590d\u6027\u7684\u4fe1\u606f\u6536\u96c6\u4efb\u52a1\u65f6\u80fd\u529b\u4e0d\u8db3\uff0c\u5e76\u4e14\u7f3a\u4e4f\u8bc4\u4f30\u8fd9\u4e9b\u4ee3\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u5dee\u8ddd\u3002", "method": "\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3a WideSearch \u7684\u65b0\u57fa\u51c6\uff0c\u5176\u4e2d\u5305\u542b 200 \u4e2a\u7531\u4eba\u5de5\u7b56\u5212\u7684\u95ee\u9898\uff08100 \u4e2a\u82f1\u6587\uff0c100 \u4e2a\u4e2d\u6587\uff09\uff0c\u6db5\u76d6 15 \u4e2a\u4e0d\u540c\u7684\u9886\u57df\u3002\u8be5\u57fa\u51c6\u65e8\u5728\u8bc4\u4f30\u4ee3\u7406\u5728\u8fd9\u4e9b\u5927\u89c4\u6a21\u6536\u96c6\u4efb\u52a1\u4e2d\u7684\u53ef\u9760\u6027\u3002\u95ee\u9898\u9700\u8981\u4ee3\u7406\u6536\u96c6\u5927\u91cf\u539f\u5b50\u4fe1\u606f\u5e76\u8fdb\u884c\u7ec4\u7ec7\u3002\u901a\u8fc7\u4e86\u4e94\u9636\u6bb5\u7684\u8d28\u91cf\u63a7\u5236\u6d41\u7a0b\uff0c\u4ee5\u786e\u4fdd\u6570\u636e\u96c6\u7684\u96be\u5ea6\u3001\u5b8c\u6574\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\u3002\u5bf9\u8d85\u8fc7 10 \u4e2a\u6700\u5148\u8fdb\u7684\u4ee3\u7406\u641c\u7d22\u7cfb\u7edf\u8fdb\u884c\u4e86\u57fa\u51c6\u6d4b\u8bd5\u3002", "result": "\u5927\u591a\u6570\u4ee3\u7406\u7cfb\u7edf\u7684\u6210\u529f\u7387\u63a5\u8fd1 0%\uff0c\u5373\u4f7f\u662f\u6700\u597d\u7684\u7cfb\u7edf\u4e5f\u53ea\u6709 5%\u3002\u7136\u800c\uff0c\u5728\u6709\u8db3\u591f\u65f6\u95f4\u7684\u60c5\u51b5\u4e0b\uff0c\u7ecf\u8fc7\u591a\u4e2a\u4eba\u7c7b\u6d4b\u8bd5\u8005\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u6210\u529f\u7387\u53ef\u4ee5\u63a5\u8fd1 100%\u3002\u8fd9\u8868\u660e\u5f53\u524d\u4ee3\u7406\u5728\u5904\u7406\u6b64\u7c7b\u4efb\u52a1\u65f6\u5b58\u5728\u5173\u952e\u7f3a\u9677\u3002", "conclusion": "\u76ee\u524d\u641c\u7d22\u4ee3\u7406\u5728\u5904\u7406\u5927\u89c4\u6a21\u4fe1\u606f\u68c0\u7d22\u4efb\u52a1\u65f6\u5b58\u5728\u4e25\u91cd\u7f3a\u9677\uff0c\u9700\u8981\u5bf9\u4ee3\u7406\u641c\u7d22\u8fdb\u884c\u5927\u91cf\u672a\u6765\u7684\u7814\u7a76\u548c\u5f00\u53d1\u3002"}}
{"id": "2508.07140", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07140", "abs": "https://arxiv.org/abs/2508.07140", "authors": ["Yingtie Lei", "Fanghai Yi", "Yihang Dong", "Weihuang Liu", "Xiaofeng Zhang", "Zimeng Li", "Chi-Man Pun", "Xuhang Chen"], "title": "CMAMRNet: A Contextual Mask-Aware Network Enhancing Mural Restoration Through Comprehensive Mask Guidance", "comment": "Accepted by BMVC 2025", "summary": "Murals, as invaluable cultural artifacts, face continuous deterioration from\nenvironmental factors and human activities. Digital restoration of murals faces\nunique challenges due to their complex degradation patterns and the critical\nneed to preserve artistic authenticity. Existing learning-based methods\nstruggle with maintaining consistent mask guidance throughout their networks,\nleading to insufficient focus on damaged regions and compromised restoration\nquality. We propose CMAMRNet, a Contextual Mask-Aware Mural Restoration Network\nthat addresses these limitations through comprehensive mask guidance and\nmulti-scale feature extraction. Our framework introduces two key components:\n(1) the Mask-Aware Up/Down-Sampler (MAUDS), which ensures consistent mask\nsensitivity across resolution scales through dedicated channel-wise feature\nselection and mask-guided feature fusion; and (2) the Co-Feature Aggregator\n(CFA), operating at both the highest and lowest resolutions to extract\ncomplementary features for capturing fine textures and global structures in\ndegraded regions. Experimental results on benchmark datasets demonstrate that\nCMAMRNet outperforms state-of-the-art methods, effectively preserving both\nstructural integrity and artistic details in restored murals. The code is\navailable\nat~\\href{https://github.com/CXH-Research/CMAMRNet}{https://github.com/CXH-Research/CMAMRNet}.", "AI": {"tldr": "CMAMRNet improves mural restoration by using mask guidance and multi-scale features to better preserve details and structure, outperforming existing methods.", "motivation": "Existing learning-based methods struggle to maintain consistent mask guidance, leading to insufficient focus on damaged regions and compromised restoration quality in digital mural restoration.", "method": "The proposed CMAMRNet utilizes a Contextual Mask-Aware approach with two key components: the Mask-Aware Up/Down-Sampler (MAUDS) for consistent mask sensitivity across resolution scales, and the Co-Feature Aggregator (CFA) for capturing both fine textures and global structures.", "result": "Experimental results on benchmark datasets show that CMAMRNet achieves superior performance compared to state-of-the-art methods.", "conclusion": "CMAMRNet outperforms state-of-the-art methods in mural restoration, effectively preserving structural integrity and artistic details."}}
{"id": "2508.07440", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07440", "abs": "https://arxiv.org/abs/2508.07440", "authors": ["Zhipeng Chang", "Zhenye Wen", "Xiaofei Zhao"], "title": "Unsupervised operator learning approach for dissipative equations via Onsager principle", "comment": null, "summary": "Existing operator learning methods rely on supervised training with\nhigh-fidelity simulation data, introducing significant computational cost. In\nthis work, we propose the deep Onsager operator learning (DOOL) method, a novel\nunsupervised framework for solving dissipative equations. Rooted in the Onsager\nvariational principle (OVP), DOOL trains a deep operator network by directly\nminimizing the OVP-defined Rayleighian functional, requiring no labeled data,\nand then proceeds in time explicitly through conservation/change laws for the\nsolution. Another key innovation here lies in the spatiotemporal decoupling\nstrategy: the operator's trunk network processes spatial coordinates\nexclusively, thereby enhancing training efficiency, while integrated external\ntime stepping enables temporal extrapolation. Numerical experiments on typical\ndissipative equations validate the effectiveness of the DOOL method, and\nsystematic comparisons with supervised DeepONet and MIONet demonstrate its\nenhanced performance. Extensions are made to cover the second-order wave models\nwith dissipation that do not directly follow OVP.", "AI": {"tldr": "DOOL\u662f\u4e00\u79cd\u57fa\u4e8eOnsager\u53d8\u5206\u539f\u7406\u7684\u65e0\u76d1\u7763\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u8017\u6563\u65b9\u7a0b\u3002\u5b83\u901a\u8fc7\u6700\u5c0f\u5316Rayleighian\u6cdb\u51fd\u8fdb\u884c\u8bad\u7ec3\uff0c\u65e0\u9700\u6807\u7b7e\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u65f6\u7a7a\u89e3\u8026\u7b56\u7565\u63d0\u9ad8\u6548\u7387\u548c\u5b9e\u73b0\u65f6\u95f4\u5916\u63a8\u3002\u5b9e\u9a8c\u8bc1\u660eDOOL\u6bd4\u76d1\u7763\u65b9\u6cd5\u66f4\u6709\u6548\uff0c\u5e76\u53ef\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u7b97\u5b50\u5b66\u4e60\u65b9\u6cd5\u9700\u8981\u9ad8\u4fdd\u771f\u4eff\u771f\u6570\u636e\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDOOL\u7684\u65b0\u578b\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u8017\u6563\u65b9\u7a0b\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u6210\u672c\u5e76\u63d0\u9ad8\u6548\u7387\u3002", "method": "DOOL\uff08\u6df1\u5ea6Onsager\u7b97\u5b50\u5b66\u4e60\uff09\u662f\u4e00\u79cd\u65e0\u76d1\u7763\u6846\u67b6\uff0c\u5229\u7528Onsager\u53d8\u5206\u539f\u7406\uff08OVP\uff09\u901a\u8fc7\u6700\u5c0f\u5316Rayleighian\u6cdb\u51fd\u6765\u8bad\u7ec3\u6df1\u5ea6\u7b97\u5b50\u7f51\u7edc\uff0c\u65e0\u9700\u6807\u7b7e\u6570\u636e\u3002\u5b83\u91c7\u7528\u65f6\u7a7a\u89e3\u8026\u7b56\u7565\uff0c\u5176\u4e2d\u5e72\u7f51\u7edc\u4ec5\u5904\u7406\u7a7a\u95f4\u5750\u6807\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5e76\u7ed3\u5408\u5916\u90e8\u65f6\u95f4\u6b65\u9a6c\u6765\u5904\u7406\u65f6\u95f4\u5916\u63a8\u3002\u5728\u6709\u8017\u6563\u7684\u60c5\u51b5\u4e0b\uff0c\u8fd8\u5bf9\u5176\u4e8c\u9636\u6ce2\u52a8\u6a21\u578b\u8fdb\u884c\u4e86\u6269\u5c55\u3002", "result": "DOOL\u65b9\u6cd5\u5728\u89e3\u51b3\u8017\u6563\u65b9\u7a0b\u65b9\u9762\u663e\u793a\u51fa\u6709\u6548\u6027\uff0c\u5e76\u5728\u4e0e\u76d1\u7763\u5b66\u4e60\u7684DeepONet\u548cMIONet\u7684\u6bd4\u8f83\u4e2d\u5c55\u793a\u4e86\u5176\u4f18\u8d8a\u6027\u3002\u8be5\u65b9\u6cd5\u8fd8\u6210\u529f\u6269\u5c55\u5230\u5904\u7406\u4e8c\u9636\u8017\u6563\u6ce2\u52a8\u6a21\u578b\u3002", "conclusion": "DOOL\u65b9\u6cd5\u5728\u5904\u7406\u8017\u6563\u65b9\u7a0b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u4e14\u5728\u6570\u503c\u5b9e\u9a8c\u4e2d\u88ab\u8bc1\u660e\u6bd4\u76d1\u7763\u5b66\u4e60\u7684DeepONet\u548cMIONet\u65b9\u6cd5\u66f4\u6709\u6548\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u6210\u529f\u6269\u5c55\u5230\u5904\u7406\u4e0d\u76f4\u63a5\u9075\u5faaOVP\u7684\u4e8c\u9636\u8017\u6563\u6ce2\u52a8\u6a21\u578b\u3002"}}
{"id": "2508.08011", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08011", "abs": "https://arxiv.org/abs/2508.08011", "authors": ["Mingzi Cao", "Xi Wang", "Nikolaos Aletras"], "title": "Progressive Depth Up-scaling via Optimal Transport", "comment": null, "summary": "Scaling Large Language Models (LLMs) yields performance gains but incurs\nsubstantial training costs. Depth up-scaling offers training efficiency by\nadding new layers to pre-trained models. However, most existing methods copy or\naverage weights from base layers, neglecting neuron permutation differences.\nThis limitation can potentially cause misalignment that harms performance.\nInspired by applying Optimal Transport (OT) for neuron alignment, we propose\nOptimal Transport Depth Up-Scaling (OpT-DeUS). OpT-DeUS aligns and fuses\nTransformer blocks in adjacent base layers via OT for new layer creation, to\nmitigate neuron permutation mismatch between layers. OpT-DeUS achieves better\noverall performance and offers improved training efficiency than existing\nmethods for continual pre-training and supervised fine-tuning across different\nmodel sizes. To further evaluate the impact of interpolation positions, our\nextensive analysis shows that inserting new layers closer to the top results in\nhigher training efficiency due to shorter back-propagation time while obtaining\nadditional performance gains.", "AI": {"tldr": "\u4e3a\u4e86\u89e3\u51b3 LLM \u6df1\u5ea6\u6269\u5c55\u4e2d\u795e\u7ecf\u5143\u6392\u5217\u4e0d\u5339\u914d\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u7684 OpT-DeUS \u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5bf9\u9f50\u548c\u878d\u5408 Transformer \u5757\u6765\u521b\u5efa\u65b0\u5c42\uff0c\u63d0\u9ad8\u4e86\u8bad\u7ec3\u6548\u7387\u548c\u6a21\u578b\u6027\u80fd\u3002\u7814\u7a76\u8fd8\u8868\u660e\uff0c\u5c06\u65b0\u5c42\u7f6e\u4e8e\u6a21\u578b\u9876\u5c42\u9644\u8fd1\u6548\u679c\u66f4\u4f73\u3002", "motivation": "\u73b0\u6709\u7684\u6df1\u5ea6\u6269\u5c55\u65b9\u6cd5\u5728\u901a\u8fc7\u589e\u52a0\u5c42\u6570\u6765\u63d0\u9ad8 LLM \u6027\u80fd\u65f6\uff0c\u5f80\u5f80\u53ea\u590d\u5236\u6216\u5e73\u5747\u6743\u91cd\uff0c\u5ffd\u7565\u4e86\u795e\u7ecf\u5143\u6392\u5217\u7684\u5dee\u5f02\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6027\u80fd\u4e0b\u964d\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Optimal Transport Depth Up-Scaling\uff08OpT-DeUS\uff09\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u6280\u672f\u6765\u5bf9\u9f50\u548c\u878d\u5408\u76f8\u90bb\u57fa\u7840\u5c42\u4e2d\u7684 Transformer \u5757\uff0c\u4ee5\u521b\u5efa\u65b0\u5c42\uff0c\u4ece\u800c\u89e3\u51b3\u795e\u7ecf\u5143\u6392\u5217\u4e0d\u5339\u914d\u95ee\u9898\u3002", "result": "OpT-DeUS \u5728\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u6709\u76d1\u7763\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u53d6\u5f97\u4e86\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u597d\u7684\u6574\u4f53\u6027\u80fd\u548c\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u3002\u901a\u8fc7\u6d88\u878d\u7814\u7a76\u53d1\u73b0\uff0c\u5c06\u65b0\u5c42\u63d2\u5165\u5230\u66f4\u9760\u8fd1\u6a21\u578b\u9876\u5c42\u7684\u4f4d\u7f6e\uff0c\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff08\u7531\u4e8e\u53cd\u5411\u4f20\u64ad\u65f6\u95f4\u7f29\u77ed\uff09\u5e76\u83b7\u5f97\u989d\u5916\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "OpT-DeUS \u65b9\u6cd5\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\uff08OT\uff09\u5bf9\u9f50\u548c\u878d\u5408\u76f8\u90bb\u57fa\u7840\u5c42\u4e2d\u7684 Transformer \u5757\uff0c\u4ee5\u7f13\u89e3\u5c42\u95f4\u795e\u7ecf\u5143\u6392\u5217\u4e0d\u5339\u914d\u95ee\u9898\uff0c\u5728\u6301\u7eed\u9884\u8bad\u7ec3\u548c\u6709\u76d1\u7763\u5fae\u8c03\u4efb\u52a1\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u5728\u4e0d\u540c\u6a21\u578b\u89c4\u6a21\u4e0a\u5747\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u6574\u4f53\u6027\u80fd\u548c\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u3002\u6b64\u5916\uff0c\u5c06\u65b0\u5c42\u63d2\u5165\u5230\u66f4\u9760\u8fd1\u9876\u5c42\u7684\u4f4d\u7f6e\u53ef\u4ee5\u83b7\u5f97\u66f4\u9ad8\u7684\u8bad\u7ec3\u6548\u7387\u548c\u989d\u5916\u7684\u6027\u80fd\u589e\u76ca\u3002"}}
{"id": "2508.07144", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07144", "abs": "https://arxiv.org/abs/2508.07144", "authors": ["Xuanhan Wang", "Huimin Deng", "Ke Liu", "Jun Wang", "Lianli Gao", "Jingkuan Song"], "title": "Dynamic Pattern Alignment Learning for Pretraining Lightweight Human-Centric Vision Models", "comment": null, "summary": "Human-centric vision models (HVMs) have achieved remarkable generalization\ndue to large-scale pretraining on massive person images. However, their\ndependence on large neural architectures and the restricted accessibility of\npretraining data significantly limits their practicality in real-world\napplications. To address this limitation, we propose Dynamic Pattern Alignment\nLearning (DPAL), a novel distillation-based pretraining framework that\nefficiently trains lightweight HVMs to acquire strong generalization from large\nHVMs. In particular, human-centric visual perception are highly dependent on\nthree typical visual patterns, including global identity pattern, local shape\npattern and multi-person interaction pattern. To achieve generalizable\nlightweight HVMs, we firstly design a dynamic pattern decoder (D-PaDe), acting\nas a dynamic Mixture of Expert (MoE) model. It incorporates three specialized\nexperts dedicated to adaptively extract typical visual patterns, conditioned on\nboth input image and pattern queries. And then, we present three levels of\nalignment objectives, which aims to minimize generalization gap between\nlightweight HVMs and large HVMs at global image level, local pixel level, and\ninstance relation level. With these two deliberate designs, the DPAL\neffectively guides lightweight model to learn all typical human visual patterns\nfrom large HVMs, which can generalize to various human-centric vision tasks.\nExtensive experiments conducted on 15 challenging datasets demonstrate the\neffectiveness of the DPAL. Remarkably, when employing PATH-B as the teacher,\nDPAL-ViT/Ti (5M parameters) achieves surprising generalizability similar to\nexisting large HVMs such as PATH-B (84M) and Sapiens-L (307M), and outperforms\nprevious distillation-based pretraining methods including Proteus-ViT/Ti (5M)\nand TinyMiM-ViT/Ti (5M) by a large margin.", "AI": {"tldr": "DPAL\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u901a\u8fc7\u52a8\u6001\u6a21\u5f0f\u89e3\u7801\u5668\u548c\u591a\u5c42\u7ea7\u5bf9\u9f50\u76ee\u6807\uff0c\u4f7f\u8f7b\u91cf\u7ea7\u89c6\u89c9\u6a21\u578b\u80fd\u591f\u4ece\u5927\u578b\u6a21\u578b\u4e2d\u5b66\u4e60\u5e76\u83b7\u5f97\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6210\u679c\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u7684\u5927\u89c4\u6a21\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u89c9\u6a21\u578b\uff08HVMs\uff09\u5bf9\u5927\u578b\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u4f9d\u8d56\u4ee5\u53ca\u9884\u8bad\u7ec3\u6570\u636e\u7684\u8bbf\u95ee\u9650\u5236\u6240\u5e26\u6765\u7684\u5b9e\u9645\u5e94\u7528\u5c40\u9650\u6027\uff0c\u63d0\u51faDPAL\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u52a8\u6001\u6a21\u5f0f\u5bf9\u9f50\u5b66\u4e60\uff08DPAL\uff09\u7684\u65b0\u578b\u57fa\u4e8e\u84b8\u998f\u7684\u9884\u8bad\u7ec3\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u8bad\u7ec3\u8f7b\u91cf\u7ea7\u7684\u4eba\u7c7b\u4e2d\u5fc3\u89c6\u89c9\u6a21\u578b\uff08HVMs\uff09\uff0c\u4f7f\u5176\u4ece\u5927\u578bHVMs\u4e2d\u83b7\u5f97\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6846\u67b6\u5305\u62ec\u4e00\u4e2a\u52a8\u6001\u6a21\u5f0f\u89e3\u7801\u5668\uff08D-PaDe\uff09\uff0c\u5b83\u4f5c\u4e3a\u4e00\u4e2a\u52a8\u6001\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u6a21\u578b\uff0c\u5305\u542b\u4e09\u4e2a\u4e13\u95e8\u7684\u4e13\u5bb6\uff0c\u7528\u4e8e\u81ea\u9002\u5e94\u5730\u63d0\u53d6\u5168\u5c40\u8eab\u4efd\u6a21\u5f0f\u3001\u5c40\u90e8\u5f62\u72b6\u6a21\u5f0f\u548c\u591a\u4eba\u7269\u4f53\u4ea4\u4e92\u6a21\u5f0f\u3002\u6b64\u5916\uff0c\u8fd8\u8bbe\u8ba1\u4e86\u4e09\u4e2a\u5c42\u7ea7\u7684\u5bf9\u9f50\u76ee\u6807\uff0c\u4ee5\u6700\u5c0f\u5316\u8f7b\u91cf\u7ea7HVMs\u548c\u5927\u578bHVMs\u5728\u5168\u5c40\u56fe\u50cf\u3001\u5c40\u90e8\u50cf\u7d20\u548c\u5b9e\u4f8b\u5173\u7cfb\u5c42\u9762\u7684\u6cdb\u5316\u5dee\u8ddd\u3002", "result": "DPAL\u572815\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u7279\u522b\u662f\uff0c\u5f53\u4f7f\u7528PATH-B\u4f5c\u4e3a\u6559\u5e08\u6a21\u578b\u65f6\uff0cDPAL-ViT/Ti\uff085M\u53c2\u6570\uff09\u5b9e\u73b0\u4e86\u4e0ePATH-B\uff0884M\uff09\u548cSapiens-L\uff08307M\uff09\u7b49\u73b0\u6709\u5927\u578bHVMs\u76f8\u5ab2\u7f8e\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u5728\u6027\u80fd\u4e0a\u663e\u8457\u4f18\u4e8eProteus-ViT/Ti\uff085M\uff09\u548cTinyMiM-ViT/Ti\uff085M\uff09\u7b49\u5148\u524d\u57fa\u4e8e\u84b8\u998f\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\u3002", "conclusion": "DPAL\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u6307\u5bfc\u8f7b\u91cf\u7ea7\u6a21\u578b\u5b66\u4e60\u6240\u6709\u5178\u578b\u7684\u4eba\u7c7b\u89c6\u89c9\u6a21\u5f0f\uff0c\u5e76\u6cdb\u5316\u5230\u5404\u79cd\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u89c6\u89c9\u4efb\u52a1\u3002DPAL-ViT/Ti\uff085M\u53c2\u6570\uff09\u572815\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4e0ePATH-B\uff0884M\uff09\u548cSapiens-L\uff08307M\uff09\u7b49\u73b0\u6709\u5927\u578bHVMs\u76f8\u5f53\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u663e\u8457\u4f18\u4e8e\u5148\u524d\u7684\u57fa\u4e8e\u84b8\u998f\u7684\u9884\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5982Proteus-ViT/Ti\uff085M\uff09\u548cTinyMiM-ViT/Ti\uff085M\uff09\u3002"}}
{"id": "2508.07452", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07452", "abs": "https://arxiv.org/abs/2508.07452", "authors": ["Fernando Martinez", "Tao Li", "Yingdong Lu", "Juntao Chen"], "title": "Stackelberg Coupling of Online Representation Learning and Reinforcement Learning", "comment": null, "summary": "Integrated, end-to-end learning of representations and policies remains a\ncornerstone of deep reinforcement learning (RL). However, to address the\nchallenge of learning effective features from a sparse reward signal, recent\ntrends have shifted towards adding complex auxiliary objectives or fully\ndecoupling the two processes, often at the cost of increased design complexity.\nThis work proposes an alternative to both decoupling and naive end-to-end\nlearning, arguing that performance can be significantly improved by structuring\nthe interaction between distinct perception and control networks with a\nprincipled, game-theoretic dynamic. We formalize this dynamic by introducing\nthe Stackelberg Coupled Representation and Reinforcement Learning (SCORER)\nframework, which models the interaction between perception and control as a\nStackelberg game. The perception network (leader) strategically learns features\nto benefit the control network (follower), whose own objective is to minimize\nits Bellman error. We approximate the game's equilibrium with a practical\ntwo-timescale algorithm. Applied to standard DQN variants on benchmark tasks,\nSCORER improves sample efficiency and final performance. Our results show that\nperformance gains can be achieved through principled algorithmic design of the\nperception-control dynamic, without requiring complex auxiliary objectives or\narchitectures.", "AI": {"tldr": "SCORER \u6846\u67b6\u901a\u8fc7\u5c06\u611f\u77e5\u548c\u63a7\u5236\u7f51\u7edc\u4e4b\u95f4\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a Stackelberg \u535a\u5f08\uff0c\u63d0\u5347\u4e86\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4ece\u7a00\u758f\u5956\u52b1\u4fe1\u53f7\u4e2d\u5b66\u4e60\u6709\u6548\u7279\u5f81\u7684\u6311\u6218\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u4f9b\u4e00\u79cd\u66ff\u4ee3\u89e3\u8026\u548c\u7aef\u5230\u7aef\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u611f\u77e5\u548c\u63a7\u5236\u7f51\u7edc\u4e4b\u95f4\u7684\u4ea4\u4e92\u6765\u63d0\u5347\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SCORER\uff08Stackelberg Coupled Representation and Reinforcement Learning\uff09\u7684\u6846\u67b6\uff0c\u5c06\u611f\u77e5\uff08leader\uff09\u548c\u63a7\u5236\uff08follower\uff09\u7f51\u7edc\u4e4b\u95f4\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a Stackelberg \u535a\u5f08\uff0c\u5e76\u4f7f\u7528\u4e00\u79cd\u5b9e\u7528\u7684\u53cc\u65f6\u95f4\u5c3a\u5ea6\u7b97\u6cd5\u6765\u8fd1\u4f3c\u535a\u5f08\u7684\u5747\u8861\u3002\u5c06\u6b64\u6846\u67b6\u5e94\u7528\u4e8e\u6807\u51c6 DQN \u53d8\u4f53\u3002", "result": "\u5728\u57fa\u51c6\u4efb\u52a1\u4e0a\uff0cSCORER \u6846\u67b6\u5728\u6837\u672c\u6548\u7387\u548c\u6700\u7ec8\u6027\u80fd\u4e0a\u5747\u6709\u6240\u63d0\u5347\uff0c\u8bc1\u660e\u4e86\u901a\u8fc7\u539f\u5219\u6027\u7684\u611f\u77e5-\u63a7\u5236\u52a8\u6001\u7b97\u6cd5\u8bbe\u8ba1\u53ef\u4ee5\u5b9e\u73b0\u6027\u80fd\u7684\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a SCORER\uff08Stackelberg Coupled Representation and Reinforcement Learning\uff09\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5c06\u611f\u77e5\u548c\u63a7\u5236\u7f51\u7edc\u4e4b\u95f4\u7684\u4ea4\u4e92\u5efa\u6a21\u4e3a Stackelberg \u535a\u5f08\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u5347\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\uff0c\u800c\u65e0\u9700\u590d\u6742\u7684\u8f85\u52a9\u76ee\u6807\u6216\u67b6\u6784\u3002"}}
{"id": "2508.08050", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08050", "abs": "https://arxiv.org/abs/2508.08050", "authors": ["Fabrizio Nunnari", "Cristina Luna Jim\u00e9nez", "Rosalee Wolfe", "John C. McDonald", "Michael Filhol", "Eleni Efthimiou", "Evita Fotinea", "Thomas Hanke"], "title": "9th Workshop on Sign Language Translation and Avatar Technologies (SLTAT 2025)", "comment": null, "summary": "The Sign Language Translation and Avatar Technology (SLTAT) workshops\ncontinue a series of gatherings to share recent advances in improving deaf /\nhuman communication through non-invasive means. This 2025 edition, the 9th\nsince its first appearance in 2011, is hosted by the International Conference\non Intelligent Virtual Agents (IVA), giving the opportunity for contamination\nbetween two research communities, using digital humans as either virtual\ninterpreters or as interactive conversational agents. As presented in this\nsummary paper, SLTAT sees contributions beyond avatar technologies, with a\nconsistent number of submissions on sign language recognition, and other work\non data collection, data analysis, tools, ethics, usability, and affective\ncomputing.", "AI": {"tldr": "The SLTAT workshop brings together researchers in sign language translation and avatar technology, with the 2025 edition focusing on diverse contributions beyond avatars, including recognition, data, ethics, and affective computing, fostering collaboration with the digital human community.", "motivation": "To share recent advances in improving deaf/human communication through non-invasive means and foster collaboration between research communities using digital humans.", "method": "The paper summarizes contributions to the SLTAT workshop, covering sign language recognition, data collection and analysis, tools, ethics, usability, and affective computing, beyond just avatar technologies.", "result": "The 2025 SLTAT workshop, hosted by IVA, saw contributions across various areas including sign language recognition, data-related work, tools, ethics, usability, and affective computing, highlighting the interdisciplinary nature of the field.", "conclusion": "The SLTAT workshop continues to advance research in sign language translation and avatar technology, fostering collaboration between digital human and sign language communities."}}
{"id": "2508.07146", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07146", "abs": "https://arxiv.org/abs/2508.07146", "authors": ["Yu Liu", "Zhijie Liu", "Xiao Ren", "You-Fu Li", "He Kong"], "title": "Intention-Aware Diffusion Model for Pedestrian Trajectory Prediction", "comment": null, "summary": "Predicting pedestrian motion trajectories is critical for the path planning\nand motion control of autonomous vehicles. Recent diffusion-based models have\nshown promising results in capturing the inherent stochasticity of pedestrian\nbehavior for trajectory prediction. However, the absence of explicit semantic\nmodelling of pedestrian intent in many diffusion-based methods may result in\nmisinterpreted behaviors and reduced prediction accuracy. To address the above\nchallenges, we propose a diffusion-based pedestrian trajectory prediction\nframework that incorporates both short-term and long-term motion intentions.\nShort-term intent is modelled using a residual polar representation, which\ndecouples direction and magnitude to capture fine-grained local motion\npatterns. Long-term intent is estimated through a learnable, token-based\nendpoint predictor that generates multiple candidate goals with associated\nprobabilities, enabling multimodal and context-aware intention modelling.\nFurthermore, we enhance the diffusion process by incorporating adaptive\nguidance and a residual noise predictor that dynamically refines denoising\naccuracy. The proposed framework is evaluated on the widely used ETH, UCY, and\nSDD benchmarks, demonstrating competitive results against state-of-the-art\nmethods.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u6269\u6563\u7684\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u77ed\u671f\u548c\u957f\u671f\u8fd0\u52a8\u610f\u56fe\u4ee5\u53ca\u6539\u8fdb\u7684\u6269\u6563\u8fc7\u7a0b\uff0c\u63d0\u9ad8\u4e86\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u5bf9\u884c\u4eba\u884c\u4e3a\u7684\u7406\u89e3\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u7684\u65b9\u6cd5\u53ef\u80fd\u56e0\u7f3a\u4e4f\u663e\u5f0f\u7684\u884c\u4eba\u610f\u56fe\u8bed\u4e49\u5efa\u6a21\u800c\u5bfc\u81f4\u7684\u8bef\u89e3\u884c\u4e3a\u548c\u9884\u6d4b\u7cbe\u5ea6\u4e0b\u964d\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6269\u6563\u7684\u884c\u4eba\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u77ed\u671f\u548c\u957f\u671f\u8fd0\u52a8\u610f\u56fe\u3002\u77ed\u671f\u610f\u56fe\u4f7f\u7528\u6b8b\u5dee\u6781\u5750\u6807\u8868\u793a\u8fdb\u884c\u5efa\u6a21\uff0c\u8be5\u8868\u793a\u5c06\u65b9\u5411\u548c\u5e45\u5ea6\u5206\u79bb\u4ee5\u6355\u83b7\u7ec6\u7c92\u5ea6\u7684\u5c40\u90e8\u8fd0\u52a8\u6a21\u5f0f\u3002\u957f\u671f\u610f\u56fe\u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u3001\u57fa\u4e8etoken\u7684\u7ec8\u70b9\u9884\u6d4b\u5668\u8fdb\u884c\u4f30\u8ba1\uff0c\u8be5\u9884\u6d4b\u5668\u751f\u6210\u591a\u4e2a\u5177\u6709\u5173\u8054\u6982\u7387\u7684\u5019\u9009\u76ee\u6807\uff0c\u4ece\u800c\u5b9e\u73b0\u591a\u6a21\u6001\u548c\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u610f\u56fe\u5efa\u6a21\u3002\u6b64\u5916\uff0c\u901a\u8fc7\u5f15\u5165\u81ea\u9002\u5e94\u5f15\u5bfc\u548c\u6b8b\u5dee\u566a\u58f0\u9884\u6d4b\u5668\u6765\u589e\u5f3a\u6269\u6563\u8fc7\u7a0b\uff0c\u8be5\u9884\u6d4b\u5668\u52a8\u6001\u5730\u4f18\u5316\u53bb\u566a\u7cbe\u5ea6\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728ETH\u3001UCY\u548cSDD\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u5c55\u793a\u4e86\u4e0e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002", "conclusion": "\u8be5\u6846\u67b6\u5728ETH\u3001UCY\u548cSDD\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u5e76\u53d6\u5f97\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.07458", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07458", "abs": "https://arxiv.org/abs/2508.07458", "authors": ["Wei Qian", "Chenxu Zhao", "Yangyi Li", "Wenqian Ye", "Mengdi Huai"], "title": "Towards Unveiling Predictive Uncertainty Vulnerabilities in the Context of the Right to Be Forgotten", "comment": null, "summary": "Currently, various uncertainty quantification methods have been proposed to\nprovide certainty and probability estimates for deep learning models' label\npredictions. Meanwhile, with the growing demand for the right to be forgotten,\nmachine unlearning has been extensively studied as a means to remove the impact\nof requested sensitive data from a pre-trained model without retraining the\nmodel from scratch. However, the vulnerabilities of such generated predictive\nuncertainties with regard to dedicated malicious unlearning attacks remain\nunexplored. To bridge this gap, for the first time, we propose a new class of\nmalicious unlearning attacks against predictive uncertainties, where the\nadversary aims to cause the desired manipulations of specific predictive\nuncertainty results. We also design novel optimization frameworks for our\nattacks and conduct extensive experiments, including black-box scenarios.\nNotably, our extensive experiments show that our attacks are more effective in\nmanipulating predictive uncertainties than traditional attacks that focus on\nlabel misclassifications, and existing defenses against conventional attacks\nare ineffective against our attacks.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9996\u6b21\u63d0\u51fa\u4e86\u4e00\u79cd\u9488\u5bf9\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u6076\u610f\u673a\u5668\u5b66\u4e60\u653b\u51fb\uff0c\u8be5\u653b\u51fb\u80fd\u591f\u6709\u6548\u5730\u64cd\u7eb5\u4e0d\u786e\u5b9a\u6027\u7ed3\u679c\uff0c\u4e14\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u65e0\u6548\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u7814\u7a76\u4e2d\u5173\u4e8e\u673a\u5668\u5b66\u4e60\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u6f0f\u6d1e\u7684\u7a7a\u767d\uff0c\u5e76\u7814\u7a76\u4e13\u95e8\u9488\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u6076\u610f\u673a\u5668\u5b66\u4e60\u653b\u51fb\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u4e2d\u6027\u653b\u51fb\uff0c\u4e13\u95e8\u9488\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65b0\u7684\u653b\u51fb\u4f18\u5316\u6846\u67b6\uff0c\u5305\u62ec\u9ed1\u76d2\u573a\u666f\u7684\u5b9e\u9a8c\u3002", "result": "\u6240\u63d0\u51fa\u7684\u653b\u51fb\u5728\u64cd\u7eb5\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u6bd4\u4f20\u7edf\u7684\u3001\u4fa7\u91cd\u4e8e\u9519\u8bef\u5206\u7c7b\u6807\u7b7e\u7684\u653b\u51fb\u66f4\u6709\u6548\uff0c\u5e76\u4e14\u73b0\u6709\u7684\u9632\u5fa1\u63aa\u65bd\u65e0\u6cd5\u9632\u5fa1\u6240\u63d0\u51fa\u7684\u653b\u51fb\u3002", "conclusion": "\u73b0\u6709\u9488\u5bf9\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u7684\u673a\u5668\u5b66\u4e60\u653b\u51fb\u548c\u73b0\u6709\u9632\u5fa1\u63aa\u65bd\u90fd\u65e0\u6cd5\u9632\u5fa1\u6240\u63d0\u51fa\u4e4b\u4e2d\u6027\u653b\u51fb\u3002"}}
{"id": "2508.08095", "categories": ["cs.CL", "cs.AI", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08095", "abs": "https://arxiv.org/abs/2508.08095", "authors": ["Chun Wang", "Chenyang Liu", "Wenze Xu", "Weihong Deng"], "title": "Dual Information Speech Language Models for Emotional Conversations", "comment": "Presented at IEEE ICME 2025", "summary": "Conversational systems relying on text-based large language models (LLMs)\noften overlook paralinguistic cues, essential for understanding emotions and\nintentions. Speech-language models (SLMs), which use speech as input, are\nemerging as a promising solution. However, SLMs built by extending frozen LLMs\nstruggle to capture paralinguistic information and exhibit reduced context\nunderstanding. We identify entangled information and improper training\nstrategies as key issues. To address these issues, we propose two heterogeneous\nadapters and suggest a weakly supervised training strategy. Our approach\ndisentangles paralinguistic and linguistic information, enabling SLMs to\ninterpret speech through structured representations. It also preserves\ncontextual understanding by avoiding the generation of task-specific vectors\nthrough controlled randomness. This approach trains only the adapters on common\ndatasets, ensuring parameter and data efficiency. Experiments demonstrate\ncompetitive performance in emotional conversation tasks, showcasing the model's\nability to effectively integrate both paralinguistic and linguistic information\nwithin contextual settings.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u73b0\u6709\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u5904\u7406\u526f\u8bed\u8a00\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u5f02\u6784\u9002\u914d\u5668\u548c\u5f31\u76d1\u7763\u8bad\u7ec3\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u89e3\u8026\u5e76\u6574\u5408\u8bed\u97f3\u4e2d\u7684\u526f\u8bed\u8a00\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u540c\u65f6\u4fdd\u6301\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\uff0c\u5e76\u5728\u60c5\u611f\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u73b0\u6709\u7684\u4f9d\u8d56\u6587\u672c\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u5904\u7406\u4ee5\u8bed\u97f3\u4e3a\u8f93\u5165\u7684\u5bf9\u8bdd\u7cfb\u7edf\u65f6\uff0c\u5ffd\u7565\u4e86\u5bf9\u7406\u89e3\u60c5\u611f\u548c\u610f\u56fe\u81f3\u5173\u91cd\u8981\u7684\u526f\u8bed\u8a00\u7ebf\u7d22\u3002\u800c\u901a\u8fc7\u6269\u5c55\u51bb\u7ed3\u7684LLM\u6784\u5efa\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u5728\u6355\u6349\u526f\u8bed\u8a00\u4fe1\u606f\u548c\u4fdd\u6301\u4e0a\u4e0b\u6587\u7406\u89e3\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002\u6587\u7ae0\u6307\u51fa\uff0c\u4fe1\u606f\u7ea0\u7f20\u548c\u4e0d\u5f53\u7684\u8bad\u7ec3\u7b56\u7565\u662f\u4e3b\u8981\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e24\u79cd\u5f02\u6784\u9002\u914d\u5668\uff0c\u5e76\u91c7\u7528\u5f31\u76d1\u7763\u8bad\u7ec3\u7b56\u7565\uff0c\u4ec5\u5728\u5e38\u89c1\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u9002\u914d\u5668\uff0c\u5b9e\u73b0\u4e86\u53c2\u6570\u548c\u6570\u636e\u7684\u6548\u7387\u3002", "result": "\u5728\u60c5\u611f\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u8be5\u6a21\u578b\u80fd\u591f\u6709\u6548\u5730\u5728\u4e0a\u4e0b\u6587\u73af\u5883\u4e2d\u6574\u5408\u526f\u8bed\u8a00\u4fe1\u606f\u548c\u8bed\u8a00\u4fe1\u606f\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u89e3\u8026\u8bed\u97f3\u4e2d\u7684\u526f\u8bed\u8a00\u4fe1\u606f\u548c\u8bed\u8a00\u4fe1\u606f\uff0c\u4ece\u800c\u4f7f\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\u80fd\u591f\u901a\u8fc7\u7ed3\u6784\u5316\u8868\u793a\u6765\u7406\u89e3\u8bed\u97f3\uff0c\u5e76\u4e14\u901a\u8fc7\u63a7\u5236\u968f\u673a\u6027\u6765\u907f\u514d\u751f\u6210\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u5411\u91cf\uff0c\u4ece\u800c\u4fdd\u6301\u4e0a\u4e0b\u6587\u7406\u89e3\u80fd\u529b\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u60c5\u611f\u5bf9\u8bdd\u4efb\u52a1\u4e2d\u7684\u7ade\u4e89\u529b\uff0c\u6709\u6548\u6574\u5408\u4e86\u526f\u8bed\u8a00\u4fe1\u606f\u3001\u8bed\u8a00\u4fe1\u606f\u548c\u4e0a\u4e0b\u6587\u4fe1\u606f\u3002"}}
{"id": "2508.07149", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07149", "abs": "https://arxiv.org/abs/2508.07149", "authors": ["Ruolin Yang", "Da Li", "Honggang Zhang", "Yi-Zhe Song"], "title": "SketchAnimator: Animate Sketch via Motion Customization of Text-to-Video Diffusion Models", "comment": "2024 IEEE International Conference on Visual Communications and Image\n  Processing (VCIP); Oral", "summary": "Sketching is a uniquely human tool for expressing ideas and creativity. The\nanimation of sketches infuses life into these static drawings, opening a new\ndimension for designers. Animating sketches is a time-consuming process that\ndemands professional skills and extensive experience, often proving daunting\nfor amateurs. In this paper, we propose a novel sketch animation model\nSketchAnimator, which enables adding creative motion to a given sketch, like \"a\njumping car''. Namely, given an input sketch and a reference video, we divide\nthe sketch animation into three stages: Appearance Learning, Motion Learning\nand Video Prior Distillation. In stages 1 and 2, we utilize LoRA to integrate\nsketch appearance information and motion dynamics from the reference video into\nthe pre-trained T2V model. In the third stage, we utilize Score Distillation\nSampling (SDS) to update the parameters of the Bezier curves in each sketch\nframe according to the acquired motion information. Consequently, our model\nproduces a sketch video that not only retains the original appearance of the\nsketch but also mirrors the dynamic movements of the reference video. We\ncompare our method with alternative approaches and demonstrate that it\ngenerates the desired sketch video under the challenge of one-shot motion\ncustomization.", "AI": {"tldr": "SketchAnimator\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u8349\u56fe\u52a8\u753b\u6a21\u578b\uff0c\u5b83\u5229\u7528LoRA\u548cSDS\u6280\u672f\uff0c\u901a\u8fc7\u5b66\u4e60\u8349\u56fe\u5916\u89c2\u548c\u53c2\u8003\u89c6\u9891\u4e2d\u7684\u8fd0\u52a8\u52a8\u6001\uff0c\u5c06\u9759\u6001\u8349\u56fe\u8f6c\u6362\u4e3a\u5177\u6709\u521b\u610f\u52a8\u6001\u7684\u89c6\u9891\u3002\u8be5\u6a21\u578b\u89e3\u51b3\u4e86\u8349\u56fe\u52a8\u753b\u7684\u590d\u6742\u6027\u548c\u8017\u65f6\u6027\u95ee\u9898\uff0c\u5c24\u5176\u5728\u5355\u6b21\u52a8\u4f5c\u5b9a\u5236\u65b9\u9762\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u52a8\u753b\u5316\u8349\u56fe\u662f\u4e00\u4e2a\u8017\u65f6\u7684\u8fc7\u7a0b\uff0c\u9700\u8981\u4e13\u4e1a\u6280\u80fd\u548c\u4e30\u5bcc\u7684\u7ecf\u9a8c\uff0c\u8fd9\u5bf9\u4e8e\u4e1a\u4f59\u7231\u597d\u8005\u6765\u8bf4\u53ef\u80fd\u5f88\u56f0\u96be\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u8349\u56fe\u52a8\u753b\u7684\u8fd9\u4e00\u6311\u6218\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8349\u56fe\u52a8\u753b\u6a21\u578bSketchAnimator\uff0c\u5c06\u8349\u56fe\u52a8\u753b\u5206\u4e3a\u4e09\u4e2a\u9636\u6bb5\uff1a\u5916\u89c2\u5b66\u4e60\u3001\u52a8\u4f5c\u5b66\u4e60\u548c\u89c6\u9891\u5148\u9a8c\u84b8\u998f\u3002\u5728\u9636\u6bb51\u548c2\u4e2d\uff0c\u5229\u7528LoRA\u5c06\u8349\u56fe\u5916\u89c2\u4fe1\u606f\u548c\u53c2\u8003\u89c6\u9891\u4e2d\u7684\u52a8\u4f5c\u52a8\u6001\u96c6\u6210\u5230\u9884\u8bad\u7ec3\u7684T2V\u6a21\u578b\u4e2d\u3002\u5728\u7b2c\u4e09\u9636\u6bb5\uff0c\u5229\u7528SDS\u6839\u636e\u83b7\u5f97\u7684\u8fd0\u52a8\u4fe1\u606f\u66f4\u65b0\u6bcf\u4e2a\u8349\u56fe\u5e27\u4e2d\u7684\u8d1d\u585e\u5c14\u66f2\u7ebf\u53c2\u6570\u3002", "result": "\u4e0e\u66ff\u4ee3\u65b9\u6cd5\u76f8\u6bd4\uff0cSketchAnimator\u80fd\u591f\u751f\u6210\u4fdd\u7559\u8349\u56fe\u539f\u59cb\u5916\u89c2\u5e76\u6a21\u4eff\u53c2\u8003\u89c6\u9891\u52a8\u6001\u8fd0\u52a8\u7684\u8349\u56fe\u89c6\u9891\u3002", "conclusion": "\u8be5\u6a21\u578b\u80fd\u591f\u751f\u6210\u4fdd\u7559\u539f\u59cb\u8349\u56fe\u5916\u89c2\u5e76\u6620\u5c04\u53c2\u8003\u89c6\u9891\u52a8\u6001\u8fd0\u52a8\u7684\u8349\u56fe\u89c6\u9891\uff0c\u5728\u5355\u6b21\u52a8\u4f5c\u5b9a\u5236\u7684\u6311\u6218\u4e0b\uff0c\u80fd\u591f\u4ea7\u751f\u671f\u671b\u7684\u8349\u56fe\u89c6\u9891\u3002"}}
{"id": "2508.07465", "categories": ["cs.LG", "q-bio.GN", "stat.ML", "62R07"], "pdf": "https://arxiv.org/pdf/2508.07465", "abs": "https://arxiv.org/abs/2508.07465", "authors": ["Tiantian Yang", "Zhiqian Chen"], "title": "MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification", "comment": "11 pages, 6 figures", "summary": "Integrating multi-omics data, such as DNA methylation, mRNA expression, and\nmicroRNA (miRNA) expression, offers a comprehensive view of the biological\nmechanisms underlying disease. However, the high dimensionality and complex\ninteractions among omics layers present major challenges for predictive\nmodeling. We propose Multi-Omics integration with Tree-generated Graph Neural\nNetwork (MOTGNN), a novel and interpretable framework for binary disease\nclassification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) to perform\nomics-specific supervised graph construction, followed by modality-specific\nGraph Neural Networks (GNNs) for hierarchical representation learning, and a\ndeep feedforward network for cross-omics integration. On three real-world\ndisease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in\naccuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance\n(e.g., 87.2% vs. 33.4% F1 on imbalanced data). The model maintains\ncomputational efficiency through sparse graphs (2.1-2.8 edges per node) and\nprovides built-in interpretability, revealing both top-ranked biomarkers and\nthe relative contributions of each omics modality. These results highlight\nMOTGNN's potential to improve both predictive accuracy and interpretability in\nmulti-omics disease modeling.", "AI": {"tldr": "MOTGNN\u662f\u4e00\u4e2a\u6574\u5408\u591a\u7ec4\u5b66\u6570\u636e\u7684\u65b0\u6846\u67b6\uff0c\u901a\u8fc7XGBoost\u548cGNNs\u63d0\u9ad8\u4e86\u75be\u75c5\u5206\u7c7b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e0a\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u6574\u5408\u591a\u7ec4\u5b66\u6570\u636e\uff08\u5982DNA\u7532\u57fa\u5316\u3001mRNA\u8868\u8fbe\u548cmiRNA\u8868\u8fbe\uff09\u53ef\u4ee5\u63d0\u4f9b\u5bf9\u75be\u75c5\u751f\u7269\u5b66\u673a\u5236\u7684\u5168\u9762\u7406\u89e3\u3002\u7136\u800c\uff0c\u7ec4\u5b66\u5c42\u4e4b\u95f4\u7684\u9ad8\u7ef4\u6027\u548c\u590d\u6742\u7684\u76f8\u4e92\u4f5c\u7528\u7ed9\u9884\u6d4b\u5efa\u6a21\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aMOTGNN\u7684\u65b0\u578b\u53ef\u89e3\u91ca\u6846\u67b6\uff0c\u7528\u4e8e\u4e8c\u5143\u75be\u75c5\u5206\u7c7b\u3002\u8be5\u6846\u67b6\u9996\u5148\u4f7f\u7528XGBoost\u8fdb\u884c\u7279\u5b9a\u7ec4\u5b66\u7c7b\u578b\u7684\u76d1\u7763\u56fe\u6784\u5efa\uff0c\u7136\u540e\u5229\u7528\u7279\u5b9a\u6a21\u6001\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNNs\uff09\u8fdb\u884c\u5206\u5c42\u8868\u793a\u5b66\u4e60\uff0c\u6700\u540e\u901a\u8fc7\u6df1\u5ea6\u524d\u9988\u7f51\u7edc\u8fdb\u884c\u8de8\u7ec4\u5b66\u96c6\u6210\u3002", "result": "\u5728\u4e09\u4e2a\u771f\u5b9e\u4e16\u754c\u75be\u75c5\u6570\u636e\u96c6\u4e0a\uff0cMOTGNN\u7684\u51c6\u786e\u7387\u3001ROC-AUC\u548cF1\u5206\u6570\u6bd4\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u63d0\u9ad8\u4e865-10%\uff0c\u5e76\u4e14\u5728\u4e25\u91cd\u7684\u7c7b\u522b\u4e0d\u5e73\u8861\u60c5\u51b5\u4e0b\uff08\u4f8b\u5982\uff0c\u5728\u4e0d\u5e73\u8861\u6570\u636e\u4e0aF1\u5206\u6570\u4ece33.4%\u63d0\u9ad8\u523087.2%\uff09\u4fdd\u6301\u4e86\u9c81\u68d2\u6027\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u7a00\u758f\u56fe\uff08\u6bcf\u4e2a\u8282\u70b92.1-2.8\u6761\u8fb9\uff09\u4fdd\u6301\u4e86\u8ba1\u7b97\u6548\u7387\uff0c\u5e76\u63d0\u4f9b\u4e86\u5185\u7f6e\u7684\u53ef\u89e3\u91ca\u6027\uff0c\u63ed\u793a\u4e86\u6392\u540d\u9760\u524d\u7684\u751f\u7269\u6807\u5fd7\u7269\u4ee5\u53ca\u6bcf\u79cd\u7ec4\u5b66\u6a21\u6001\u7684\u76f8\u5bf9\u8d21\u732e\u3002", "conclusion": "MOTGNN\u5728\u591a\u7ec4\u5b66\u75be\u75c5\u5efa\u6a21\u4e2d\u5c55\u793a\u4e86\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08096", "categories": ["cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08096", "abs": "https://arxiv.org/abs/2508.08096", "authors": ["Lukas Gehring", "Benjamin Paa\u00dfen"], "title": "Assessing LLM Text Detection in Educational Contexts: Does Human Contribution Affect Detection?", "comment": "Preprint as provided by the authors (19 pages, 12 figures, 9 tables)", "summary": "Recent advancements in Large Language Models (LLMs) and their increased\naccessibility have made it easier than ever for students to automatically\ngenerate texts, posing new challenges for educational institutions. To enforce\nnorms of academic integrity and ensure students' learning, learning analytics\nmethods to automatically detect LLM-generated text appear increasingly\nappealing. This paper benchmarks the performance of different state-of-the-art\ndetectors in educational contexts, introducing a novel dataset, called\nGenerative Essay Detection in Education (GEDE), containing over 900\nstudent-written essays and over 12,500 LLM-generated essays from various\ndomains. To capture the diversity of LLM usage practices in generating text, we\npropose the concept of contribution levels, representing students' contribution\nto a given assignment. These levels range from purely human-written texts, to\nslightly LLM-improved versions, to fully LLM-generated texts, and finally to\nactive attacks on the detector by \"humanizing\" generated texts. We show that\nmost detectors struggle to accurately classify texts of intermediate student\ncontribution levels, like LLM-improved human-written texts. Detectors are\nparticularly likely to produce false positives, which is problematic in\neducational settings where false suspicions can severely impact students'\nlives. Our dataset, code, and additional supplementary materials are publicly\navailable at\nhttps://github.com/lukasgehring/Assessing-LLM-Text-Detection-in-Educational-Contexts.", "AI": {"tldr": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86LLM\u6587\u672c\u63a2\u6d4b\u5668\u5728\u6559\u80b2\u73af\u5883\u4e2d\u7684\u6027\u80fd\uff0c\u53d1\u73b0\u5b83\u4eec\u5728\u68c0\u6d4b\u5b66\u751f\u4e2d\u7b49\u8d21\u732e\u7a0b\u5ea6\u7684\u6587\u672c\u65f6\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u5bb9\u6613\u4ea7\u751f\u8bef\u62a5\uff0c\u5bf9\u5b66\u751f\u53ef\u80fd\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\u7814\u7a76\u4eba\u5458\u4e3a\u6b64\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684\u6570\u636e\u96c6GEDE\uff0c\u5e76\u63d0\u51fa\u4e86\u201c\u8d21\u732e\u7a0b\u5ea6\u201d\u7684\u6982\u5ff5\u6765\u66f4\u597d\u5730\u8bc4\u4f30\u8fd9\u4e9b\u63a2\u6d4b\u5668\u3002", "motivation": "\u968f\u7740\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u666e\u53ca\uff0c\u5b66\u751f\u53ef\u4ee5\u8f7b\u677e\u81ea\u52a8\u751f\u6210\u6587\u672c\uff0c\u8fd9\u5bf9\u6559\u80b2\u673a\u6784\u63d0\u51fa\u4e86\u65b0\u7684\u6311\u6218\uff0c\u9700\u8981\u5f00\u53d1\u80fd\u591f\u81ea\u52a8\u68c0\u6d4bLLM\u751f\u6210\u6587\u672c\u7684\u5b66\u4e60\u5206\u6790\u65b9\u6cd5\uff0c\u4ee5\u7ef4\u62a4\u5b66\u672f\u8bda\u4fe1\u548c\u786e\u4fdd\u5b66\u751f\u5b66\u4e60\u3002", "method": "\u8be5\u7814\u7a76\u8bc4\u4f30\u4e86\u4e0d\u540c\u6700\u5148\u8fdb\u7684\u6587\u672c\u63a2\u6d4b\u5668\u7684\u6027\u80fd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u540d\u4e3aGEDE\uff08Generative Essay Detection in Education\uff09\u7684\u65b0\u6570\u636e\u96c6\uff0c\u5176\u4e2d\u5305\u542b900\u591a\u7bc7\u5b66\u751f\u64b0\u5199\u7684\u8bba\u6587\u548c12,500\u591a\u7bc7\u7531LLM\u751f\u6210\u7684\u8bba\u6587\u3002\u7814\u7a76\u4eba\u5458\u8fd8\u63d0\u51fa\u4e86\u201c\u8d21\u732e\u7a0b\u5ea6\u201d\u7684\u6982\u5ff5\uff0c\u4ee5\u63cf\u8ff0\u5b66\u751f\u5728\u4f5c\u4e1a\u4e2d\u7684\u53c2\u4e0e\u5ea6\uff0c\u5e76\u8bc4\u4f30\u4e86\u63a2\u6d4b\u5668\u5728\u4e0d\u540c\u8d21\u732e\u7a0b\u5ea6\u4e0b\u7684\u8868\u73b0\u3002", "result": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u5927\u591a\u6570\u63a2\u6d4b\u5668\u5728\u51c6\u786e\u5206\u7c7b\u5b66\u751f\u4e2d\u7b49\u8d21\u732e\u7a0b\u5ea6\u7684\u6587\u672c\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u5728\u6559\u80b2\u73af\u5883\u4e2d\u5bb9\u6613\u4ea7\u751f\u8bef\u62a5\uff0c\u8fd9\u53ef\u80fd\u5bf9\u5b66\u751f\u9020\u6210\u4e25\u91cd\u540e\u679c\u3002", "conclusion": "\u5927\u591a\u6570\u63a2\u6d4b\u5668\u5728\u51c6\u786e\u5206\u7c7b\u5b66\u751f\u4e2d\u7b49\u8d21\u732e\u7a0b\u5ea6\u7684\u6587\u672c\uff08\u4f8b\u5982\uff0cLLM\u6539\u8fdb\u7684\u4eba\u5de5\u64b0\u5199\u6587\u672c\uff09\u65b9\u9762\u5b58\u5728\u56f0\u96be\u3002\u5728\u6559\u80b2\u73af\u5883\u4e2d\uff0c\u8bef\u62a5\uff08\u5373\uff0c\u9519\u8bef\u5730\u5c06\u4eba\u7c7b\u6587\u672c\u5206\u7c7b\u4e3aLLM\u751f\u6210\u6587\u672c\uff09\u5c24\u5176\u4ee4\u4eba\u62c5\u5fe7\uff0c\u56e0\u4e3a\u8fd9\u53ef\u80fd\u5bf9\u5b66\u751f\u4ea7\u751f\u4e25\u91cd\u5f71\u54cd\u3002"}}
{"id": "2508.07162", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07162", "abs": "https://arxiv.org/abs/2508.07162", "authors": ["Xiaotong Lin", "Tianming Liang", "Jian-Fang Hu", "Kun-Yu Lin", "Yulei Kang", "Chunwei Tian", "Jianhuang Lai", "Wei-Shi Zheng"], "title": "CoopDiff: Anticipating 3D Human-object Interactions via Contact-consistent Decoupled Diffusion", "comment": null, "summary": "3D human-object interaction (HOI) anticipation aims to predict the future\nmotion of humans and their manipulated objects, conditioned on the historical\ncontext. Generally, the articulated humans and rigid objects exhibit different\nmotion patterns, due to their distinct intrinsic physical properties. However,\nthis distinction is ignored by most of the existing works, which intend to\ncapture the dynamics of both humans and objects within a single prediction\nmodel. In this work, we propose a novel contact-consistent decoupled diffusion\nframework CoopDiff, which employs two distinct branches to decouple human and\nobject motion modeling, with the human-object contact points as shared anchors\nto bridge the motion generation across branches. The human dynamics branch is\naimed to predict highly structured human motion, while the object dynamics\nbranch focuses on the object motion with rigid translations and rotations.\nThese two branches are bridged by a series of shared contact points with\nconsistency constraint for coherent human-object motion prediction. To further\nenhance human-object consistency and prediction reliability, we propose a\nhuman-driven interaction module to guide object motion modeling. Extensive\nexperiments on the BEHAVE and Human-object Interaction datasets demonstrate\nthat our CoopDiff outperforms state-of-the-art methods.", "AI": {"tldr": "CoopDiff\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u4eba\u7c7b\u548c\u7269\u4f53\u8fd0\u52a8\u5efa\u6a21\u5e76\u4f7f\u7528\u63a5\u89e6\u70b9\u8fdb\u884c\u6865\u63a5\uff0c\u63d0\u9ad8\u4e863D\u4eba\u7c7b-\u7269\u4f53\u4ea4\u4e92\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5927\u591a\u6570\u65b9\u6cd5\u5728\u6355\u6349\u4eba\u7c7b\u548c\u7269\u4f53\u8fd0\u52a8\u65f6\uff0c\u90fd\u5ffd\u7565\u4e86\u5b83\u4eec\u56e0\u7269\u7406\u5c5e\u6027\u4e0d\u540c\u800c\u4ea7\u751f\u7684\u4e0d\u540c\u8fd0\u52a8\u6a21\u5f0f\u7684\u5dee\u5f02\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u63a5\u89e6\u4e00\u81f4\u6027\u89e3\u8026\u6269\u6563\u6846\u67b6CoopDiff\uff0c\u8be5\u6846\u67b6\u91c7\u7528\u4e24\u4e2a\u72ec\u7acb\u7684\u652f\u8def\u6765\u89e3\u8026\u4eba\u7c7b\u548c\u7269\u4f53\u8fd0\u52a8\u5efa\u6a21\uff0c\u5e76\u4ee5\u4eba\u7c7b-\u7269\u4f53\u63a5\u89e6\u70b9\u4f5c\u4e3a\u5171\u4eab\u951a\u70b9\u6765\u8fde\u63a5\u8de8\u652f\u8def\u7684\u8fd0\u52a8\u751f\u6210\u3002\u4eba\u7c7b\u52a8\u529b\u5b66\u5206\u652f\u65e8\u5728\u9884\u6d4b\u9ad8\u5ea6\u7ed3\u6784\u5316\u7684\u4eba\u7c7b\u8fd0\u52a8\uff0c\u800c\u7269\u4f53\u52a8\u529b\u5b66\u5206\u652f\u5219\u4e13\u6ce8\u4e8e\u5177\u6709\u521a\u6027\u5e73\u79fb\u548c\u65cb\u8f6c\u7684\u7269\u4f53\u8fd0\u52a8\u3002\u901a\u8fc7\u4e00\u7cfb\u5217\u5177\u6709\u4e00\u81f4\u6027\u7ea6\u675f\u7684\u5171\u4eab\u63a5\u89e6\u70b9\u8fde\u63a5\u8fd9\u4e24\u4e2a\u5206\u652f\uff0c\u4ee5\u5b9e\u73b0\u8fde\u8d2f\u7684\u4eba\u7c7b-\u7269\u4f53\u8fd0\u52a8\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u4e2a\u7531\u4eba\u7c7b\u9a71\u52a8\u7684\u4ea4\u4e92\u6a21\u5757\u6765\u6307\u5bfc\u7269\u4f53\u8fd0\u52a8\u5efa\u6a21\uff0c\u4ee5\u589e\u5f3a\u4eba\u7c7b-\u7269\u4f53\u4e00\u81f4\u6027\u548c\u9884\u6d4b\u7684\u53ef\u9760\u6027\u3002", "result": "CoopDiff\u6846\u67b6\u5728BEHAVE\u548cHuman-object Interaction\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "CoopDiff\u6846\u67b6\u5728BEHAVE\u548cHuman-object Interaction\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002"}}
{"id": "2508.07473", "categories": ["cs.LG", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07473", "abs": "https://arxiv.org/abs/2508.07473", "authors": ["Zijian Liu"], "title": "Online Convex Optimization with Heavy Tails: Old Algorithms, New Regrets, and Applications", "comment": "Part of this work is in submission", "summary": "In Online Convex Optimization (OCO), when the stochastic gradient has a\nfinite variance, many algorithms provably work and guarantee a sublinear\nregret. However, limited results are known if the gradient estimate has a heavy\ntail, i.e., the stochastic gradient only admits a finite $\\mathsf{p}$-th\ncentral moment for some $\\mathsf{p}\\in\\left(1,2\\right]$. Motivated by it, this\nwork examines different old algorithms for OCO (e.g., Online Gradient Descent)\nin the more challenging heavy-tailed setting. Under the standard bounded domain\nassumption, we establish new regrets for these classical methods without any\nalgorithmic modification. Remarkably, these regret bounds are fully optimal in\nall parameters (can be achieved even without knowing $\\mathsf{p}$), suggesting\nthat OCO with heavy tails can be solved effectively without any extra operation\n(e.g., gradient clipping). Our new results have several applications. A\nparticularly interesting one is the first provable convergence result for\nnonsmooth nonconvex optimization under heavy-tailed noise without gradient\nclipping. Furthermore, we explore broader settings (e.g., smooth OCO) and\nextend our ideas to optimistic algorithms to handle different cases\nsimultaneously.", "AI": {"tldr": "\u5728\u6709\u91cd\u5c3e\u566a\u58f0\u7684\u5728\u7ebf\u51f8\u4f18\u5316\u95ee\u9898\u4e2d\uff0c\u7ecf\u5178\u7b97\u6cd5\u65e0\u9700\u4fee\u6539\u5373\u53ef\u8fbe\u5230\u6700\u4f18\u9057\u61be\u754c\uff0c\u5e76\u4e14\u5728\u975e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u7b49\u95ee\u9898\u4e0a\u4e5f\u6709\u5e94\u7528\u3002", "motivation": "\u7531\u4e8e\u5728\u91cd\u5c3e\u566a\u58f0\uff08\u5373\u968f\u673a\u68af\u5ea6\u53ea\u627f\u8ba4\u6709\u9650\u7684p\u9636\u4e2d\u5fc3\u77e9\uff0c\u5176\u4e2dp\u5c5e\u4e8e(1,2])\u7684OCO\u8bbe\u5b9a\u4e0b\uff0c\u5df2\u77e5\u7684\u7b97\u6cd5\u7ed3\u679c\u6709\u9650\uff0c\u56e0\u6b64\u6fc0\u52b1\u672c\u7814\u7a76\u53bb\u63a2\u7d22\u548c\u5206\u6790\u73b0\u6709\u7b97\u6cd5\u5728\u8fd9\u4e00\u66f4\u5177\u6311\u6218\u6027\u73af\u5883\u4e0b\u7684\u8868\u73b0\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5206\u6790\u5728\u7ebf\u68af\u5ea6\u4e0b\u964d\u7b49\u7ecf\u5178OCO\u7b97\u6cd5\u5728\u91cd\u5c3e\u566a\u58f0\u73af\u5883\u4e0b\u7684\u8868\u73b0\uff0c\u5728\u6807\u51c6\u6709\u754c\u57df\u5047\u8bbe\u4e0b\uff0c\u4e3a\u8fd9\u4e9b\u7b97\u6cd5\u5efa\u7acb\u4e86\u65b0\u7684\u9057\u61be\u754c\u9650\uff0c\u4e14\u8fd9\u4e9b\u754c\u9650\u5728\u6240\u6709\u53c2\u6570\u4e0a\u90fd\u662f\u6700\u4f18\u7684\u3002", "result": "\u7814\u7a76\u5efa\u7acb\u4e86\u6700\u4f18\u7684\u9057\u61be\u754c\u9650\uff0c\u8868\u660e\u5373\u4f7f\u5728\u4e0d\u77e5\u9053p\u7684\u60c5\u51b5\u4e0b\uff0c\u4e5f\u53ef\u4ee5\u5728\u6ca1\u6709\u989d\u5916\u64cd\u4f5c\uff08\u5982\u68af\u5ea6\u88c1\u526a\uff09\u7684\u60c5\u51b5\u4e0b\u6709\u6548\u89e3\u51b3OCO\u95ee\u9898\u3002\u540c\u65f6\uff0c\u8be5\u7814\u7a76\u8fd8\u9996\u6b21\u8bc1\u660e\u4e86\u5728\u91cd\u5c3e\u566a\u58f0\u4e0b\u3001\u65e0\u9700\u68af\u5ea6\u88c1\u526a\u7684\u975e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u7684\u6536\u655b\u6027\uff0c\u5e76\u5c06\u5176\u601d\u60f3\u6269\u5c55\u5230\u66f4\u5e7f\u6cdb\u7684\u8bbe\u7f6e\uff0c\u5982\u5149\u6ed1OCO\u548c\u4e50\u89c2\u7b97\u6cd5\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u5728\u7ebf\u51f8\u4f18\u5316\uff08OCO\uff09\u5728\u6709\u91cd\u5c3e\u566a\u58f0\u7684\u8bbe\u5b9a\u4e0b\u63d0\u4f9b\u4e86\u65b0\u7684\u7406\u8bba\u4fdd\u8bc1\uff0c\u8868\u660e\u7ecf\u5178\u7b97\u6cd5\u5728\u65e0\u9700\u4fee\u6539\u7684\u60c5\u51b5\u4e0b\u4e5f\u80fd\u6709\u6548\u5904\u7406\u6b64\u7c7b\u566a\u58f0\uff0c\u5e76\u4e14\u63d0\u4f9b\u7684\u9057\u61be\u754c\u662f\u6700\u4f18\u7684\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u5c06\u8fd9\u4e9b\u53d1\u73b0\u6269\u5c55\u5230\u4e86\u975e\u5149\u6ed1\u975e\u51f8\u4f18\u5316\u548c\u4e50\u89c2\u7b97\u6cd5\u7b49\u66f4\u5e7f\u6cdb\u7684\u573a\u666f\u3002"}}
{"id": "2508.08110", "categories": ["cs.CL", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08110", "abs": "https://arxiv.org/abs/2508.08110", "authors": ["Robin Huo", "Ewan Dunbar"], "title": "Iterative refinement, not training objective, makes HuBERT behave differently from wav2vec 2.0", "comment": "Proceedings of Interspeech 2025", "summary": "Self-supervised models for speech representation learning now see widespread\nuse for their versatility and performance on downstream tasks, but the effect\nof model architecture on the linguistic information learned in their\nrepresentations remains under-studied. This study investigates two such models,\nHuBERT and wav2vec 2.0, and minimally compares two of their architectural\ndifferences: training objective and iterative pseudo-label refinement through\nmultiple training iterations. We find that differences in canonical correlation\nof hidden representations to word identity, phoneme identity, and speaker\nidentity are explained by training iteration, not training objective. We\nsuggest that future work investigate the reason for the effectiveness of\niterative refinement in encoding linguistic information in self-supervised\nspeech representations.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86 HuBERT \u548c wav2vec 2.0 \u6a21\u578b\u67b6\u6784\u7684\u5dee\u5f02\uff0c\u53d1\u73b0\u8fed\u4ee3\u4f18\u5316\u5bf9\u63d0\u5347\u6a21\u578b\u8868\u793a\u7684\u8bed\u8a00\u4fe1\u606f\u81f3\u5173\u91cd\u8981\u3002", "motivation": "\u76ee\u524d\u5173\u4e8e\u6a21\u578b\u67b6\u6784\u5bf9\u81ea\u76d1\u7763\u8bed\u97f3\u8868\u793a\u5b66\u4e60\u4e2d\u8bed\u8a00\u4fe1\u606f\u5f71\u54cd\u7684\u7814\u7a76\u5c1a\u4e0d\u5145\u5206\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u6b64\u95ee\u9898\u3002", "method": "\u901a\u8fc7\u6bd4\u8f83 HuBERT \u548c wav2vec 2.0 \u6a21\u578b\uff0c\u5e76\u5206\u6790\u5176\u67b6\u6784\u5dee\u5f02\uff08\u8bad\u7ec3\u76ee\u6807\u548c\u8fed\u4ee3\u4f2a\u6807\u7b7e\u4f18\u5316\uff09\uff0c\u7814\u7a76\u6a21\u578b\u8868\u793a\u5b66\u4e60\u4e2d\u8bed\u8a00\u4fe1\u606f\u7684\u53d8\u5316\u3002", "result": "\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u9690\u85cf\u8868\u793a\u4e0e\u8bcd\u6c47\u3001\u97f3\u7d20\u548c\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7684\u76f8\u5173\u6027\u5dee\u5f02\u4e3b\u8981\u7531\u8bad\u7ec3\u8fed\u4ee3\u6b21\u6570\u51b3\u5b9a\uff0c\u800c\u975e\u8bad\u7ec3\u76ee\u6807\u3002", "conclusion": "wav2vec 2.0 \u548c HuBERT \u6a21\u578b\u5728\u8868\u793a\u5b66\u4e60\u4e2d\u7684\u7ec6\u5fae\u67b6\u6784\u5dee\u5f02\uff08\u8bad\u7ec3\u76ee\u6807\u548c\u8fed\u4ee3\u4f2a\u6807\u7b7e\u4f18\u5316\uff09\u5e76\u4e0d\u5f71\u54cd\u6a21\u578b\u9690\u85cf\u8868\u793a\u4e0e\u8bcd\u6c47\u3001\u97f3\u7d20\u548c\u8bf4\u8bdd\u4eba\u8eab\u4efd\u7684\u76f8\u5173\u6027\u3002\u8fed\u4ee3\u4f18\u5316\u5728\u7f16\u7801\u8bed\u8a00\u4fe1\u606f\u65b9\u9762\u6bd4\u8bad\u7ec3\u76ee\u6807\u66f4\u91cd\u8981\u3002"}}
{"id": "2508.07165", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07165", "abs": "https://arxiv.org/abs/2508.07165", "authors": ["Zelin Qiu", "Xi Wang", "Zhuoyao Xie", "Juan Zhou", "Yu Wang", "Lingjie Yang", "Xinrui Jiang", "Juyoung Bae", "Moo Hyun Son", "Qiang Ye", "Dexuan Chen", "Rui Zhang", "Tao Li", "Neeraj Ramesh Mahboobani", "Varut Vardhanabhuti", "Xiaohui Duan", "Yinghua Zhao", "Hao Chen"], "title": "Large-scale Multi-sequence Pretraining for Generalizable MRI Analysis in Versatile Clinical Applications", "comment": null, "summary": "Multi-sequence Magnetic Resonance Imaging (MRI) offers remarkable\nversatility, enabling the distinct visualization of different tissue types.\nNevertheless, the inherent heterogeneity among MRI sequences poses significant\nchallenges to the generalization capability of deep learning models. These\nchallenges undermine model performance when faced with varying acquisition\nparameters, thereby severely restricting their clinical utility. In this study,\nwe present PRISM, a foundation model PRe-trained with large-scale\nmultI-Sequence MRI. We collected a total of 64 datasets from both public and\nprivate sources, encompassing a wide range of whole-body anatomical structures,\nwith scans spanning diverse MRI sequences. Among them, 336,476 volumetric MRI\nscans from 34 datasets (8 public and 26 private) were curated to construct the\nlargest multi-organ multi-sequence MRI pretraining corpus to date. We propose a\nnovel pretraining paradigm that disentangles anatomically invariant features\nfrom sequence-specific variations in MRI, while preserving high-level semantic\nrepresentations. We established a benchmark comprising 44 downstream tasks,\nincluding disease diagnosis, image segmentation, registration, progression\nprediction, and report generation. These tasks were evaluated on 32 public\ndatasets and 5 private cohorts. PRISM consistently outperformed both\nnon-pretrained models and existing foundation models, achieving first-rank\nresults in 39 out of 44 downstream benchmarks with statistical significance\nimprovements. These results underscore its ability to learn robust and\ngeneralizable representations across unseen data acquired under diverse MRI\nprotocols. PRISM provides a scalable framework for multi-sequence MRI analysis,\nthereby enhancing the translational potential of AI in radiology. It delivers\nconsistent performance across diverse imaging protocols, reinforcing its\nclinical applicability.", "AI": {"tldr": "PRISM\u662f\u4e00\u500b\u5728\u5927\u898f\u6a21\u591a\u5e8f\u5217MRI\u4e0a\u9810\u8a13\u7df4\u7684\u57fa\u790e\u6a21\u578b\uff0c\u80fd\u6709\u6548\u89e3\u6c7aMRI\u5e8f\u5217\u7570\u8cea\u6027\u5e36\u4f86\u7684\u6cdb\u5316\u80fd\u529b\u6311\u6230\uff0c\u5728\u591a\u9805\u4e0b\u6e38\u4efb\u52d9\u4e2d\u8868\u73fe\u512a\u65bc\u73fe\u6709\u6a21\u578b\uff0c\u986f\u8457\u63d0\u5347\u4e86AI\u5728\u653e\u5c04\u5b78\u4e2d\u7684\u81e8\u5e8a\u61c9\u7528\u6f5b\u529b\u3002", "motivation": "\u89e3\u6c7a\u6df1\u5ea6\u5b78\u7fd2\u6a21\u578b\u5728\u8655\u7406\u591a\u5e8f\u5217MRI\u6642\uff0c\u56e0\u5e8f\u5217\u7570\u8cea\u6027\u5c0e\u81f4\u7684\u6cdb\u5316\u80fd\u529b\u6311\u6230\uff0c\u4ee5\u53ca\u7531\u6b64\u5c0d\u6a21\u578b\u6027\u80fd\u548c\u81e8\u5e8a\u61c9\u7528\u7684\u9650\u5236\u3002", "method": "\u900f\u904e\u6536\u96c664\u500b\u8cc7\u6599\u96c6\uff0c\u5305\u542b336,476\u500bMRI\u6383\u63cf\uff0c\u5efa\u69cb\u4e86\u76ee\u524d\u6700\u5927\u898f\u6a21\u7684\u591a\u5668\u5b98\u3001\u591a\u5e8f\u5217MRI\u9810\u8a13\u7df4\u8a9e\u6599\u5eab\u3002\u63d0\u51fa\u4e86\u4e00\u7a2e\u65b0\u7a4e\u7684\u9810\u8a13\u7df4\u7bc4\u5f0f\uff0c\u65e8\u5728\u5c07MRI\u4e2d\u7684\u89e3\u5256\u7d50\u69cb\u4e0d\u8b8a\u7279\u5fb5\u8207\u5e8f\u5217\u7279\u7570\u6027\u8b8a\u5316\u5206\u96e2\uff0c\u540c\u6642\u4fdd\u7559\u9ad8\u968e\u8a9e\u7fa9\u8868\u5fb5\u3002", "result": "PRISM\u572839/44\u9805\u4e0b\u6e38\u4efb\u52d9\u4e2d\u53d6\u5f97\u7b2c\u4e00\u540d\uff0c\u986f\u8457\u512a\u65bc\u5c0d\u7167\u7d44\uff0c\u8b49\u660e\u4e86\u5176\u5728\u4e0d\u540cMRI\u5354\u8b70\u4e0b\u5b78\u7fd2\u7a69\u5065\u4e14\u53ef\u6cdb\u5316\u8868\u5fb5\u7684\u80fd\u529b\u3002", "conclusion": "PRISM\u6a21\u578b\u5728\u8de8\u8d8a\u4e0d\u540cMRI\u5e8f\u5217\u548c\u89e3\u5256\u7d50\u69cb\u768444\u9805\u4e0b\u6e38\u4efb\u52d9\u4e2d\uff0c\u76f8\u6bd4\u672a\u9810\u8a13\u7df4\u6a21\u578b\u548c\u73fe\u6709\u57fa\u790e\u6a21\u578b\uff0c\u8868\u73fe\u51fa\u512a\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4e26\u572839\u9805\u4efb\u52d9\u4e2d\u53d6\u5f97\u4e86\u986f\u8457\u7684\u7d71\u8a08\u5b78\u512a\u52e2\uff0c\u986f\u793a\u5176\u5b78\u7fd2\u7a69\u5065\u3001\u53ef\u6cdb\u5316\u8868\u5fb5\u7684\u80fd\u529b\uff0c\u5f9e\u800c\u589e\u5f37\u4e86AI\u5728\u653e\u5c04\u5b78\u4e2d\u7684\u8f49\u5316\u6f5b\u529b\uff0c\u4e26\u63d0\u9ad8\u4e86\u5176\u81e8\u5e8a\u9069\u7528\u6027\u3002"}}
{"id": "2508.07490", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07490", "abs": "https://arxiv.org/abs/2508.07490", "authors": ["Ricardo Matos", "Luis Roque", "Vitor Cerqueira"], "title": "N-BEATS-MOE: N-BEATS with a Mixture-of-Experts Layer for Heterogeneous Time Series Forecasting", "comment": null, "summary": "Deep learning approaches are increasingly relevant for time series\nforecasting tasks. Methods such as N-BEATS, which is built on stacks of\nmultilayer perceptrons (MLPs) blocks, have achieved state-of-the-art results on\nbenchmark datasets and competitions. N-BEATS is also more interpretable\nrelative to other deep learning approaches, as it decomposes forecasts into\ndifferent time series components, such as trend and seasonality. In this work,\nwe present N-BEATS-MOE, an extension of N-BEATS based on a Mixture-of-Experts\n(MoE) layer. N-BEATS-MOE employs a dynamic block weighting strategy based on a\ngating network which allows the model to better adapt to the characteristics of\neach time series. We also hypothesize that the gating mechanism provides\nadditional interpretability by identifying which expert is most relevant for\neach series. We evaluate our method across 12 benchmark datasets against\nseveral approaches, achieving consistent improvements on several datasets,\nespecially those composed of heterogeneous time series.", "AI": {"tldr": "N-BEATS-MOE\u901a\u8fc7\u5f15\u5165\u6df7\u5408\u4e13\u5bb6\u548c\u52a8\u6001\u5757\u52a0\u6743\uff0c\u63d0\u9ad8\u4e86\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u89e3\u91ca\u6027\uff0c\u5c24\u5176\u5728\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u4e0a\u8868\u73b0\u66f4\u4f73\u3002", "motivation": "\u4e3a\u4e86\u6539\u8fdb\u6df1\u5ea6\u5b66\u4e60\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u7279\u522b\u662fN-BEATS\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u5176\u53ef\u89e3\u91ca\u6027\u3002", "method": "N-BEATS-MOE\u662f\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u4e13\u5bb6\uff08MoE\uff09\u5c42\u7684N-BEATS\u6269\u5c55\uff0c\u91c7\u7528\u52a8\u6001\u5757\u52a0\u6743\u7b56\u7565\uff0c\u901a\u8fc7\u95e8\u63a7\u7f51\u7edc\u5b9e\u73b0\u5bf9\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\u81ea\u9002\u5e94\u3002", "result": "N-BEATS-MOE\u572812\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u663e\u793a\uff0c\u5728\u5305\u542b\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u7684\u6301\u7eed\u6539\u8fdb\uff0c\u95e8\u63a7\u673a\u5236\u4e5f\u63d0\u4f9b\u4e86\u989d\u5916\u53ef\u89e3\u91ca\u6027\u3002", "conclusion": "N-BEATS-MOE\u901a\u8fc7\u52a8\u6001\u5757\u52a0\u6743\u7b56\u7565\u548c\u95e8\u63a7\u7f51\u7edc\uff0c\u80fd\u591f\u66f4\u597d\u5730\u9002\u5e94\u6bcf\u4e2a\u65f6\u95f4\u5e8f\u5217\u7684\u7279\u6027\uff0c\u5e76\u5728\u5305\u542b\u5f02\u6784\u65f6\u95f4\u5e8f\u5217\u7684\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u6301\u7eed\u7684\u6539\u8fdb\u3002"}}
{"id": "2508.08125", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08125", "abs": "https://arxiv.org/abs/2508.08125", "authors": ["Jakub \u0160m\u00edd", "Pavel P\u0159ib\u00e1\u0148", "Ond\u0159ej Pra\u017e\u00e1k", "Pavel Kr\u00e1l"], "title": "Czech Dataset for Complex Aspect-Based Sentiment Analysis Tasks", "comment": "Published In Proceedings of the 2024 Joint International Conference\n  on Computational Linguistics, Language Resources and Evaluation (LREC-COLING\n  2024). Official version: https://aclanthology.org/2024.lrec-main.374/", "summary": "In this paper, we introduce a novel Czech dataset for aspect-based sentiment\nanalysis (ABSA), which consists of 3.1K manually annotated reviews from the\nrestaurant domain. The dataset is built upon the older Czech dataset, which\ncontained only separate labels for the basic ABSA tasks such as aspect term\nextraction or aspect polarity detection. Unlike its predecessor, our new\ndataset is specifically designed for more complex tasks, e.g.\ntarget-aspect-category detection. These advanced tasks require a unified\nannotation format, seamlessly linking sentiment elements (labels) together. Our\ndataset follows the format of the well-known SemEval-2016 datasets. This design\nchoice allows effortless application and evaluation in cross-lingual scenarios,\nultimately fostering cross-language comparisons with equivalent counterpart\ndatasets in other languages. The annotation process engaged two trained\nannotators, yielding an impressive inter-annotator agreement rate of\napproximately 90%. Additionally, we provide 24M reviews without annotations\nsuitable for unsupervised learning. We present robust monolingual baseline\nresults achieved with various Transformer-based models and insightful error\nanalysis to supplement our contributions. Our code and dataset are freely\navailable for non-commercial research purposes.", "AI": {"tldr": "\u7814\u7a76\u8005\u53d1\u5e03\u4e86\u4e00\u4e2a\u65b0\u7684\u6377\u514b\u8bed\u9910\u5385\u8bc4\u8bba\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\uff0c\u652f\u6301\u66f4\u590d\u6742\u7684\u4efb\u52a1\uff0c\u5e76\u5305\u542b\u5927\u91cf\u65e0\u6807\u6ce8\u6570\u636e\u7528\u4e8e\u65e0\u76d1\u7763\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u6377\u514b\u8bed\u5728\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\u9886\u57df\u7f3a\u4e4f\u9002\u5408\u590d\u6742\u4efb\u52a1\uff08\u5982\u76ee\u6807\u65b9\u9762\u7c7b\u522b\u68c0\u6d4b\uff09\u4e14\u6807\u6ce8\u7edf\u4e00\u7684\u6570\u636e\u96c6\u7684\u95ee\u9898\uff0c\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u65b0\u7684\u3001\u66f4\u5168\u9762\u7684\u6377\u514b\u8bedABSA\u6570\u636e\u96c6\uff0c\u5e76\u4fc3\u8fdb\u8de8\u8bed\u8a00\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5f15\u5165\u4e00\u4e2a\u5305\u542b3.1K\u624b\u52a8\u6807\u6ce8\u9910\u5385\u9886\u57df\u8bc4\u8bba\u7684\u65b0\u578b\u6377\u514b\u8bed\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5728\u65e7\u6709\u6570\u636e\u96c6\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u6269\u5c55\uff0c\u589e\u52a0\u4e86\u76ee\u6807\u65b9\u9762\u7c7b\u522b\u68c0\u6d4b\u7b49\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u91c7\u7528\u4e86\u4e0eSemEval-2016\u6570\u636e\u96c6\u4e00\u81f4\u7684\u7edf\u4e00\u6807\u6ce8\u683c\u5f0f\uff0c\u4ee5\u65b9\u4fbf\u8de8\u8bed\u8a00\u5e94\u7528\u3002\u7814\u7a76\u8fc7\u7a0b\u4e2d\uff0c\u4e24\u4f4d\u6807\u6ce8\u8005\u8fbe\u5230\u4e86\u7ea690%\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u63d0\u4f9b\u4e86\u57fa\u4e8eTransformer\u7684\u57fa\u7ebf\u6a21\u578b\u7ed3\u679c\u548c\u8bef\u5dee\u5206\u6790\u3002", "result": "\u6210\u529f\u6784\u5efa\u4e86\u4e00\u4e2a\u5305\u542b3.1K\u624b\u52a8\u6807\u6ce8\u8bc4\u8bba\u548c24M\u65e0\u6807\u6ce8\u8bc4\u8bba\u7684\u65b0\u578b\u6377\u514b\u8bedABSA\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u652f\u6301\u76ee\u6807\u65b9\u9762\u7c7b\u522b\u68c0\u6d4b\u7b49\u590d\u6742\u4efb\u52a1\uff0c\u5e76\u5b9e\u73b0\u4e86\u7ea690%\u7684\u6807\u6ce8\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u4e8eTransformer\u7684\u57fa\u7ebf\u6a21\u578b\u7ed3\u679c\u548c\u8bef\u5dee\u5206\u6790\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u7684\u6377\u514b\u8bed\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u65b9\u9762\u7ea7\u60c5\u611f\u5206\u6790\uff08ABSA\uff09\uff0c\u5e76\u5305\u542b2400\u4e07\u6761\u65e0\u6807\u6ce8\u8bc4\u8bba\u4ee5\u652f\u6301\u65e0\u76d1\u7763\u5b66\u4e60\u3002"}}
{"id": "2508.07170", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07170", "abs": "https://arxiv.org/abs/2508.07170", "authors": ["Yunpeng Shi", "Lei Chen", "Xiaolu Shen", "Yanju Guo"], "title": "Lightweight Multi-Scale Feature Extraction with Fully Connected LMF Layer for Salient Object Detection", "comment": null, "summary": "In the domain of computer vision, multi-scale feature extraction is vital for\ntasks such as salient object detection. However, achieving this capability in\nlightweight networks remains challenging due to the trade-off between\nefficiency and performance. This paper proposes a novel lightweight multi-scale\nfeature extraction layer, termed the LMF layer, which employs depthwise\nseparable dilated convolutions in a fully connected structure. By integrating\nmultiple LMF layers, we develop LMFNet, a lightweight network tailored for\nsalient object detection. Our approach significantly reduces the number of\nparameters while maintaining competitive performance. Here, we show that LMFNet\nachieves state-of-the-art or comparable results on five benchmark datasets with\nonly 0.81M parameters, outperforming several traditional and lightweight models\nin terms of both efficiency and accuracy. Our work not only addresses the\nchallenge of multi-scale learning in lightweight networks but also demonstrates\nthe potential for broader applications in image processing tasks. The related\ncode files are available at https://github.com/Shi-Yun-peng/LMFNet", "AI": {"tldr": "\u63d0\u51faLMFNet\u8f7b\u91cf\u7ea7\u7f51\u7edc\uff0c\u901a\u8fc7LMF\u5c42\u6709\u6548\u63d0\u53d6\u591a\u5c3a\u5ea6\u7279\u5f81\uff0c\u5728\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\u4ee5\u66f4\u5c11\u53c2\u6570\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u5728\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\uff0c\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5bf9\u4e8e\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u7b49\u4efb\u52a1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5728\u8f7b\u91cf\u7ea7\u7f51\u7edc\u4e2d\u5b9e\u73b0\u8fd9\u4e00\u76ee\u6807\u9762\u4e34\u6548\u7387\u548c\u6027\u80fd\u7684\u6743\u8861\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aLMF\u5c42\u7684\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5c42\uff0c\u8be5\u5c42\u91c7\u7528\u6df1\u5ea6\u53ef\u5206\u79bb\u6269\u5f20\u5377\u79ef\u548c\u5168\u8fde\u63a5\u7ed3\u6784\u3002\u57fa\u4e8eLMF\u5c42\u6784\u5efa\u4e86LMFNet\u7f51\u7edc\u3002", "result": "LMFNet\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6216\u53ef\u6bd4\u7684\u7ed3\u679c\uff0c\u53c2\u6570\u91cf\u4ec5\u4e3a0.81M\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u591a\u4e2a\u4f20\u7edf\u548c\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684LMFNet\u5728\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u4e2d\uff0c\u901a\u8fc7\u5f15\u5165LMF\u5c42\uff08\u4e00\u79cd\u7ed3\u5408\u6df1\u5ea6\u53ef\u5206\u79bb\u6269\u5f20\u5377\u79ef\u548c\u5168\u8fde\u63a5\u7ed3\u6784\u7684\u65b0\u578b\u8f7b\u91cf\u7ea7\u591a\u5c3a\u5ea6\u7279\u5f81\u63d0\u53d6\u5c42\uff09\uff0c\u5728\u663e\u8457\u51cf\u5c11\u53c2\u6570\u91cf\uff08\u4ec50.81M\uff09\u7684\u540c\u65f6\uff0c\u5728\u4e94\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u6216\u53ef\u6bd4\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8f7b\u91cf\u7ea7\u7f51\u7edc\u4e2d\u5b9e\u73b0\u591a\u5c3a\u5ea6\u5b66\u4e60\u7684\u6709\u6548\u6027\uff0c\u5e76\u5177\u6709\u66f4\u5e7f\u6cdb\u7684\u56fe\u50cf\u5904\u7406\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2508.08131", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08131", "abs": "https://arxiv.org/abs/2508.08131", "authors": ["Wenze Xu", "Chun Wang", "Jiazhen Yu", "Sheng Chen", "Liang Gao", "Weihong Deng"], "title": "Optimal Transport Regularization for Speech Text Alignment in Spoken Language Models", "comment": "To be presented at ACPR 2025 Conference", "summary": "Spoken Language Models (SLMs), which extend Large Language Models (LLMs) to\nperceive speech inputs, have gained increasing attention for their potential to\nadvance speech understanding tasks. However, despite recent progress, studies\nshow that SLMs often struggle to generalize across datasets, even for trained\nlanguages and tasks, raising concerns about whether they process speech in a\ntext-like manner as intended. A key challenge underlying this limitation is the\nmodality gap between speech and text representations. The high variability in\nspeech embeddings may allow SLMs to achieve strong in-domain performance by\nexploiting unintended speech variations, ultimately hindering generalization.\nTo mitigate this modality gap, we introduce Optimal Transport Regularization\n(OTReg), a method that formulates speech-text alignment as an optimal transport\nproblem and derives a regularization loss to improve SLM training. In each\ntraining iteration, OTReg first establishes a structured correspondence between\nspeech and transcript embeddings by determining the optimal transport plan,\nthen incorporates the regularization loss based on this transport plan to\noptimize SLMs in generating speech embeddings that align more effectively with\ntranscript embeddings. OTReg is lightweight, requiring no additional labels or\nlearnable parameters, and integrates seamlessly into existing SLM training\nprocedures. Extensive multilingual ASR experiments demonstrate that OTReg\nenhances speech-text alignment, mitigates the modality gap, and consequently\nimproves SLM generalization across diverse datasets.", "AI": {"tldr": "OTReg \u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u6765\u89e3\u51b3\u8bed\u97f3\u8bed\u8a00\u6a21\u578b\u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u7684\u6311\u6218\uff0c\u53d6\u5f97\u4e86\u663e\u8457\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u8bed\u97f3\u8bed\u8a00\u6a21\u578b (SLM) \u5728\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u53ef\u80fd\u662f\u56e0\u4e3a\u8bed\u97f3\u548c\u6587\u672c\u8868\u793a\u4e4b\u95f4\u7684\u6a21\u6001\u9e3f\u6c9f\uff0c\u5bfc\u81f4 SLM \u53ef\u80fd\u5229\u7528\u4e86\u975e\u9884\u671f\u7684\u8bed\u97f3\u53d8\u5316\uff0c\u800c\u975e\u50cf\u9884\u671f\u90a3\u6837\u4ee5\u7c7b\u4f3c\u6587\u672c\u7684\u65b9\u5f0f\u5904\u7406\u8bed\u97f3\u3002", "method": "OTReg (Optimal Transport Regularization) \u5c06\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u4f5c\u4e3a\u6700\u4f18\u4f20\u8f93\u95ee\u9898\uff0c\u901a\u8fc7\u8ba1\u7b97\u6700\u4f18\u4f20\u8f93\u8ba1\u5212\u6765\u6784\u5efa\u8bed\u97f3\u548c\u6587\u672c\u5d4c\u5165\u4e4b\u95f4\u7684\u7ed3\u6784\u5316\u5bf9\u5e94\u5173\u7cfb\uff0c\u5e76\u57fa\u4e8e\u6b64\u4f20\u8f93\u8ba1\u5212\u5f15\u5165\u6b63\u5219\u5316\u635f\u5931\u6765\u4f18\u5316 SLM \u8bad\u7ec3\u3002", "result": "OTReg \u63d0\u9ad8\u4e86\u8bed\u97f3-\u6587\u672c\u5bf9\u9f50\u5ea6\uff0c\u7f29\u5c0f\u4e86\u6a21\u6001\u9e3f\u6c9f\uff0c\u5e76\u663e\u8457\u63d0\u5347\u4e86 SLM \u5728\u591a\u8bed\u8a00\u81ea\u52a8\u8bed\u97f3\u8bc6\u522b (ASR) \u4efb\u52a1\u4e0a\u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "OTReg \u901a\u8fc7\u5229\u7528\u6700\u4f18\u4f20\u8f93\u6765\u5f25\u5408\u8bed\u97f3\u548c\u6587\u672c\u8868\u793a\u4e4b\u95f4\u7684\u6a21\u6001\u9e3f\u6c9f\uff0c\u4ece\u800c\u589e\u5f3a\u4e86\u8bed\u97f3\u8bed\u8a00\u6a21\u578b (SLM) \u7684\u8de8\u6570\u636e\u96c6\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07171", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07171", "abs": "https://arxiv.org/abs/2508.07171", "authors": ["Huihui Xu", "Jiashi Lin", "Haoyu Chen", "Junjun He", "Lei Zhu"], "title": "EventRR: Event Referential Reasoning for Referring Video Object Segmentation", "comment": null, "summary": "Referring Video Object Segmentation (RVOS) aims to segment out the object in\na video referred by an expression. Current RVOS methods view referring\nexpressions as unstructured sequences, neglecting their crucial semantic\nstructure essential for referent reasoning. Besides, in contrast to\nimage-referring expressions whose semantics focus only on object attributes and\nobject-object relations, video-referring expressions also encompass event\nattributes and event-event temporal relations. This complexity challenges\ntraditional structured reasoning image approaches. In this paper, we propose\nthe Event Referential Reasoning (EventRR) framework. EventRR decouples RVOS\ninto object summarization part and referent reasoning part. The summarization\nphase begins by summarizing each frame into a set of bottleneck tokens, which\nare then efficiently aggregated in the video-level summarization step to\nexchange the global cross-modal temporal context. For reasoning part, EventRR\nextracts semantic eventful structure of a video-referring expression into\nhighly expressive Referential Event Graph (REG), which is a single-rooted\ndirected acyclic graph. Guided by topological traversal of REG, we propose\nTemporal Concept-Role Reasoning (TCRR) to accumulate the referring score of\neach temporal query from REG leaf nodes to root node. Each reasoning step can\nbe interpreted as a question-answer pair derived from the concept-role\nrelations in REG. Extensive experiments across four widely recognized benchmark\ndatasets, show that EventRR quantitatively and qualitatively outperforms\nstate-of-the-art RVOS methods. Code is available at\nhttps://github.com/bio-mlhui/EventRR", "AI": {"tldr": "EventRR\u901a\u8fc7\u5c06RVOS\u5206\u89e3\u4e3a\u5bf9\u8c61\u6458\u8981\u548c\u6307\u4ee3\u63a8\u7406\u4e24\u90e8\u5206\uff0c\u5e76\u5f15\u5165\u6307\u4ee3\u4e8b\u4ef6\u56fe\uff08REG\uff09\u548c\u65f6\u95f4\u6982\u5ff5-\u89d2\u8272\u63a8\u7406\uff08TCRR\uff09\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6307\u4ee3\u8868\u8fbe\u5f0f\u8bed\u4e49\u7ed3\u6784\u548c\u89c6\u9891\u7279\u6709\u590d\u6742\u6027\u7684\u95ee\u9898\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709RVOS\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u7684RVOS\u65b9\u6cd5\u5c06\u6307\u4ee3\u8868\u8fbe\u5f0f\u89c6\u4e3a\u975e\u7ed3\u6784\u5316\u5e8f\u5217\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u5bf9\u6307\u4ee3\u63a8\u7406\u81f3\u5173\u91cd\u8981\u7684\u8bed\u4e49\u7ed3\u6784\u3002\u6b64\u5916\uff0c\u4e0e\u4ec5\u5173\u6ce8\u5bf9\u8c61\u5c5e\u6027\u548c\u5bf9\u8c61\u95f4\u5173\u7cfb\u7684\u56fe\u50cf\u6307\u4ee3\u8868\u8fbe\u5f0f\u76f8\u6bd4\uff0c\u89c6\u9891\u6307\u4ee3\u8868\u8fbe\u5f0f\u8fd8\u5305\u542b\u4e8b\u4ef6\u5c5e\u6027\u548c\u4e8b\u4ef6\u95f4\u65f6\u95f4\u5173\u7cfb\u3002\u8fd9\u79cd\u590d\u6742\u6027\u7ed9\u4f20\u7edf\u7684\u7ed3\u6784\u5316\u63a8\u7406\u56fe\u50cf\u65b9\u6cd5\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": "EventRR\u6846\u67b6\u5c06RVOS\u5206\u89e3\u4e3a\u5bf9\u8c61\u6458\u8981\u548c\u6307\u4ee3\u63a8\u7406\u4e24\u90e8\u5206\u3002\u6458\u8981\u9636\u6bb5\u9996\u5148\u5c06\u6bcf\u4e2a\u5e27\u6458\u8981\u4e3a\u4e00\u7ec4\u74f6\u9888\u4ee4\u724c\uff0c\u7136\u540e\u5728\u89c6\u9891\u7ea7\u6458\u8981\u6b65\u9aa4\u4e2d\u6709\u6548\u5730\u805a\u5408\u8fd9\u4e9b\u4ee4\u724c\uff0c\u4ee5\u4ea4\u6362\u5168\u5c40\u8de8\u6a21\u6001\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u5bf9\u4e8e\u63a8\u7406\u90e8\u5206\uff0cEventRR\u5c06\u89c6\u9891\u6307\u4ee3\u8868\u8fbe\u5f0f\u7684\u8bed\u4e49\u4e8b\u4ef6\u7ed3\u6784\u63d0\u53d6\u5230\u9ad8\u5ea6\u8868\u8fbe\u7684\u6307\u4ee3\u4e8b\u4ef6\u56fe\uff08REG\uff09\u4e2d\uff0c\u8fd9\u662f\u4e00\u4e2a\u5355\u6839\u6709\u5411\u65e0\u73af\u56fe\u3002\u5728REG\u7684\u62d3\u6251\u904d\u5386\u7684\u6307\u5bfc\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u65f6\u95f4\u6982\u5ff5-\u89d2\u8272\u63a8\u7406\uff08TCRR\uff09\u4ee5\u7d2f\u79ef\u6bcf\u4e2a\u65f6\u95f4\u67e5\u8be2\u4eceREG\u53f6\u8282\u70b9\u5230\u6839\u8282\u70b9\u7684\u6307\u4ee3\u5206\u6570\u3002\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u90fd\u53ef\u4ee5\u89e3\u91ca\u4e3a\u4eceREG\u4e2d\u7684\u6982\u5ff5-\u89d2\u8272\u5173\u7cfb\u6d3e\u751f\u7684\u95ee\u7b54\u5bf9\u3002", "result": "EventRR\u6846\u67b6\u5c06RVOS\u5206\u89e3\u4e3a\u5bf9\u8c61\u6458\u8981\u548c\u6307\u4ee3\u63a8\u7406\u4e24\u90e8\u5206\u3002\u6458\u8981\u9636\u6bb5\u9996\u5148\u5c06\u6bcf\u4e2a\u5e27\u6458\u8981\u4e3a\u4e00\u7ec4\u74f6\u9888\u4ee4\u724c\uff0c\u7136\u540e\u5728\u89c6\u9891\u7ea7\u6458\u8981\u6b65\u9aa4\u4e2d\u6709\u6548\u5730\u805a\u5408\u8fd9\u4e9b\u4ee4\u724c\uff0c\u4ee5\u4ea4\u6362\u5168\u5c40\u8de8\u6a21\u6001\u65f6\u95f4\u4e0a\u4e0b\u6587\u3002\u5bf9\u4e8e\u63a8\u7406\u90e8\u5206\uff0cEventRR\u5c06\u89c6\u9891\u6307\u4ee3\u8868\u8fbe\u5f0f\u7684\u8bed\u4e49\u4e8b\u4ef6\u7ed3\u6784\u63d0\u53d6\u5230\u9ad8\u5ea6\u8868\u8fbe\u7684\u6307\u4ee3\u4e8b\u4ef6\u56fe\uff08REG\uff09\u4e2d\uff0c\u8fd9\u662f\u4e00\u4e2a\u5355\u6839\u6709\u5411\u65e0\u73af\u56fe\u3002\u5728REG\u7684\u62d3\u6251\u904d\u5386\u7684\u6307\u5bfc\u4e0b\uff0c\u6211\u4eec\u63d0\u51fa\u65f6\u95f4\u6982\u5ff5-\u89d2\u8272\u63a8\u7406\uff08TCRR\uff09\u4ee5\u7d2f\u79ef\u6bcf\u4e2a\u65f6\u95f4\u67e5\u8be2\u4eceREG\u53f6\u8282\u70b9\u5230\u6839\u8282\u70b9\u7684\u6307\u4ee3\u5206\u6570\u3002\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u90fd\u53ef\u4ee5\u89e3\u91ca\u4e3a\u4eceREG\u4e2d\u7684\u6982\u5ff5-\u89d2\u8272\u5173\u7cfb\u6d3e\u751f\u7684\u95ee\u7b54\u5bf9\u3002", "conclusion": "EventRR\u5728\u56db\u4e2a\u5e7f\u6cdb\u8ba4\u53ef\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0c\u5176\u5728\u6570\u91cf\u548c\u8d28\u91cf\u4e0a\u5747\u4f18\u4e8e\u6700\u5148\u8fdb\u7684RVOS\u65b9\u6cd5\u3002"}}
{"id": "2508.07518", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07518", "abs": "https://arxiv.org/abs/2508.07518", "authors": ["Sichen Zhao", "Wei Shao", "Jeffrey Chan", "Ziqi Xu", "Flora Salim"], "title": "FairDRL-ST: Disentangled Representation Learning for Fair Spatio-Temporal Mobility Prediction", "comment": "Accepted as a Research Paper (short) at ACM SIGSPATIAL 2025. This\n  arXiv version is the full version of the paper", "summary": "As deep spatio-temporal neural networks are increasingly utilised in urban\ncomputing contexts, the deployment of such methods can have a direct impact on\nusers of critical urban infrastructure, such as public transport, emergency\nservices, and traffic management systems. While many spatio-temporal methods\nfocus on improving accuracy, fairness has recently gained attention due to\ngrowing evidence that biased predictions in spatio-temporal applications can\ndisproportionately disadvantage certain demographic or geographic groups,\nthereby reinforcing existing socioeconomic inequalities and undermining the\nethical deployment of AI in public services. In this paper, we propose a novel\nframework, FairDRL-ST, based on disentangled representation learning, to\naddress fairness concerns in spatio-temporal prediction, with a particular\nfocus on mobility demand forecasting. By leveraging adversarial learning and\ndisentangled representation learning, our framework learns to separate\nattributes that contain sensitive information. Unlike existing methods that\nenforce fairness through supervised learning, which may lead to\novercompensation and degraded performance, our framework achieves fairness in\nan unsupervised manner with minimal performance loss. We apply our framework to\nreal-world urban mobility datasets and demonstrate its ability to close\nfairness gaps while delivering competitive predictive performance compared to\nstate-of-the-art fairness-aware methods.", "AI": {"tldr": "FairDRL-ST\u6846\u67b6\u901a\u8fc7\u65e0\u76d1\u7763\u89e3\u8026\u8868\u793a\u5b66\u4e60\u89e3\u51b3\u4e86\u57ce\u5e02\u8ba1\u7b97\u4e2d\u65f6\u7a7a\u9884\u6d4b\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u5728\u7f29\u5c0f\u516c\u5e73\u6027\u5dee\u8ddd\u7684\u540c\u65f6\u4fdd\u6301\u4e86\u6027\u80fd\u3002", "motivation": "\u968f\u7740\u6df1\u5ea6\u65f6\u7a7a\u795e\u7ecf\u7f51\u7edc\u5728\u57ce\u5e02\u8ba1\u7b97\u4e2d\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u5176\u90e8\u7f72\u53ef\u80fd\u76f4\u63a5\u5f71\u54cd\u5230\u516c\u5171\u4ea4\u901a\u3001\u7d27\u6025\u670d\u52a1\u548c\u4ea4\u901a\u7ba1\u7406\u7cfb\u7edf\u7b49\u5173\u952e\u57ce\u5e02\u57fa\u7840\u8bbe\u65bd\u7528\u6237\u3002\u7531\u4e8e\u5b58\u5728\u504f\u89c1\u9884\u6d4b\u53ef\u80fd\u4f1a\u4e0d\u6210\u6bd4\u4f8b\u5730\u4f7f\u67d0\u4e9b\u4eba\u53e3\u6216\u5730\u7406\u7fa4\u4f53\u5904\u4e8e\u4e0d\u5229\u5730\u4f4d\uff0c\u4ece\u800c\u52a0\u5267\u73b0\u6709\u7684\u793e\u4f1a\u7ecf\u6d4e\u4e0d\u5e73\u7b49\u5e76\u7834\u574f\u4eba\u5de5\u667a\u80fd\u5728\u516c\u5171\u670d\u52a1\u4e2d\u7684\u4f26\u7406\u90e8\u7f72\uff0c\u56e0\u6b64\u516c\u5e73\u6027\u5df2\u6210\u4e3a\u7814\u7a76\u91cd\u70b9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u89e3\u8026\u8868\u793a\u5b66\u4e60\u7684\u65b0\u578b\u6846\u67b6FairDRL-ST\uff0c\u5229\u7528\u5bf9\u6297\u6027\u5b66\u4e60\u548c\u89e3\u8026\u8868\u793a\u5b66\u4e60\u6765\u5206\u79bb\u5305\u542b\u654f\u611f\u4fe1\u606f\u5c5e\u6027\uff0c\u4ee5\u89e3\u51b3\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\u3002", "result": "\u4e0e\u73b0\u6709\u7684\u901a\u8fc7\u76d1\u7763\u5b66\u4e60\u5f3a\u5236\u6267\u884c\u516c\u5e73\u6027\u7684\u65b9\u6cd5\u4e0d\u540c\uff0c\u8be5\u6846\u67b6\u5728\u65e0\u76d1\u7763\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u516c\u5e73\u6027\uff0c\u540c\u65f6\u5c06\u6027\u80fd\u635f\u5931\u964d\u81f3\u6700\u4f4e\u3002\u5728\u771f\u5b9e\u4e16\u754c\u7684\u57ce\u5e02\u79fb\u52a8\u6027\u6570\u636e\u96c6\u4e0a\u7684\u5e94\u7528\u8868\u660e\uff0c\u8be5\u6846\u67b6\u80fd\u591f\u7f29\u5c0f\u516c\u5e73\u6027\u5dee\u8ddd\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u516c\u5e73\u6027\u611f\u77e5\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5177\u6709\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u5229\u7528\u5bf9\u6297\u6027\u5b66\u4e60\u548c\u89e3\u8026\u8868\u793a\u5b66\u4e60\uff0c\u8be5\u6846\u67b6\u4ee5\u65e0\u76d1\u7763\u65b9\u5f0f\u5728\u6700\u5c0f\u5316\u6027\u80fd\u635f\u5931\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u516c\u5e73\u6027\uff0c\u89e3\u51b3\u4e86\u65f6\u7a7a\u9884\u6d4b\u4e2d\u7684\u516c\u5e73\u6027\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u79fb\u52a8\u6027\u9700\u6c42\u9884\u6d4b\uff0c\u5e76\u5728\u771f\u5b9e\u4e16\u754c\u7684\u57ce\u5e02\u79fb\u52a8\u6027\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7f29\u5c0f\u516c\u5e73\u6027\u5dee\u8ddd\u65b9\u9762\u7684\u80fd\u529b\uff0c\u5e76\u4e0e\u6700\u5148\u8fdb\u7684\u516c\u5e73\u6027\u611f\u77e5\u65b9\u6cd5\u76f8\u6bd4\u5177\u6709\u53ef\u9884\u6d4b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08139", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08139", "abs": "https://arxiv.org/abs/2508.08139", "authors": ["Tianyi Zhou", "Johanne Medina", "Sanjay Chawla"], "title": "Can LLMs Detect Their Confabulations? Estimating Reliability in Uncertainty-Aware Language Models", "comment": null, "summary": "Large Language Models (LLMs) are prone to generating fluent but incorrect\ncontent, known as confabulation, which poses increasing risks in multi-turn or\nagentic applications where outputs may be reused as context. In this work, we\ninvestigate how in-context information influences model behavior and whether\nLLMs can identify their unreliable responses. We propose a reliability\nestimation that leverages token-level uncertainty to guide the aggregation of\ninternal model representations. Specifically, we compute aleatoric and\nepistemic uncertainty from output logits to identify salient tokens and\naggregate their hidden states into compact representations for response-level\nreliability prediction. Through controlled experiments on open QA benchmarks,\nwe find that correct in-context information improves both answer accuracy and\nmodel confidence, while misleading context often induces confidently incorrect\nresponses, revealing a misalignment between uncertainty and correctness. Our\nprobing-based method captures these shifts in model behavior and improves the\ndetection of unreliable outputs across multiple open-source LLMs. These results\nunderscore the limitations of direct uncertainty signals and highlight the\npotential of uncertainty-guided probing for reliability-aware generation.", "AI": {"tldr": "LLM\u5728\u591a\u8f6e\u6216agentic\u5e94\u7528\u4e2d\u5bb9\u6613\u4ea7\u751f\u865a\u5047\u4f46\u6d41\u7545\u7684\u9519\u8bef\u5185\u5bb9\u3002\u672c\u7814\u7a76\u8c03\u67e5\u4e86\u4e0a\u4e0b\u6587\u4fe1\u606f\u5bf9\u6a21\u578b\u884c\u4e3a\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4ee4\u724c\u7ea7\u4e0d\u786e\u5b9a\u6027\u7684\u53ef\u9760\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u5bf9LLM\u4e0d\u53ef\u9760\u54cd\u5e94\u7684\u68c0\u6d4b\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4e0e\u56de\u7b54\u7684\u6b63\u786e\u6027\u4e4b\u95f4\u53ef\u80fd\u5b58\u5728\u4e0d\u5339\u914d\u3002", "motivation": "\u7814\u7a76LLM\u4e2d\u4e0a\u4e0b\u6587\u4fe1\u606f\u5982\u4f55\u5f71\u54cd\u6a21\u578b\u884c\u4e3a\u4ee5\u53caLLM\u662f\u5426\u80fd\u8bc6\u522b\u5176\u4e0d\u53ef\u9760\u7684\u54cd\u5e94\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u4ee4\u724c\u7ea7\u4e0d\u786e\u5b9a\u6027\u6765\u6307\u5bfc\u5185\u90e8\u6a21\u578b\u8868\u793a\u805a\u5408\u7684\u53ef\u9760\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u8f93\u51fa\u5bf9\u6570\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u8bc6\u522b\u663e\u8457\u4ee4\u724c\uff0c\u5e76\u5c06\u5b83\u4eec\u7684\u9690\u85cf\u72b6\u6001\u805a\u5408\u6210\u54cd\u5e94\u7ea7\u53ef\u9760\u6027\u9884\u6d4b\u7684\u7d27\u51d1\u8868\u793a\u3002", "result": "\u6b63\u786e\u4e0a\u4e0b\u6587\u4fe1\u606f\u53ef\u63d0\u9ad8\u7b54\u6848\u51c6\u786e\u6027\u548c\u6a21\u578b\u7f6e\u4fe1\u5ea6\uff0c\u800c\u8bef\u5bfc\u6027\u4e0a\u4e0b\u6587\u901a\u5e38\u4f1a\u5bfc\u81f4\u81ea\u4fe1\u7684\u9519\u8bef\u54cd\u5e94\uff0c\u8fd9\u8868\u660e\u4e0d\u786e\u5b9a\u6027\u4e0e\u6b63\u786e\u6027\u4e4b\u95f4\u5b58\u5728\u4e0d\u5339\u914d\u3002\u901a\u8fc7\u57fa\u4e8e\u63a2\u6d4b\u7684\u65b9\u6cd5\u53ef\u4ee5\u6355\u6349\u8fd9\u4e9b\u6a21\u578b\u884c\u4e3a\u7684\u53d8\u5316\uff0c\u5e76\u63d0\u9ad8\u5728\u591a\u4e2a\u5f00\u6e90LLM\u4e0a\u4e0d\u53ef\u9760\u8f93\u51fa\u7684\u68c0\u6d4b\u80fd\u529b\u3002", "conclusion": "LLMs\u7684\u5c40\u9650\u6027\u5728\u4e8e\u76f4\u63a5\u7684\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\uff0c\u5e76\u5f3a\u8c03\u4e86\u4e0d\u786e\u5b9a\u6027\u5f15\u5bfc\u63a2\u6d4b\u5728\u53ef\u9760\u6027\u611f\u77e5\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07211", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07211", "abs": "https://arxiv.org/abs/2508.07211", "authors": ["Junyi He", "Liuling Chen", "Hongyang Zhou", "Zhang xiaoxing", "Xiaobin Zhu", "Shengxiang Yu", "Jingyan Qin", "Xu-Cheng Yin"], "title": "Similarity Matters: A Novel Depth-guided Network for Image Restoration and A New Dataset", "comment": "12 pages, 10 figures", "summary": "Image restoration has seen substantial progress in recent years. However,\nexisting methods often neglect depth information, which hurts similarity\nmatching, results in attention distractions in shallow depth-of-field (DoF)\nscenarios, and excessive enhancement of background content in deep DoF\nsettings. To overcome these limitations, we propose a novel Depth-Guided\nNetwork (DGN) for image restoration, together with a novel large-scale\nhigh-resolution dataset. Specifically, the network consists of two interactive\nbranches: a depth estimation branch that provides structural guidance, and an\nimage restoration branch that performs the core restoration task. In addition,\nthe image restoration branch exploits intra-object similarity through\nprogressive window-based self-attention and captures inter-object similarity\nvia sparse non-local attention. Through joint training, depth features\ncontribute to improved restoration quality, while the enhanced visual features\nfrom the restoration branch in turn help refine depth estimation. Notably, we\nalso introduce a new dataset for training and evaluation, consisting of 9,205\nhigh-resolution images from 403 plant species, with diverse depth and texture\nvariations. Extensive experiments show that our method achieves\nstate-of-the-art performance on several standard benchmarks and generalizes\nwell to unseen plant images, demonstrating its effectiveness and robustness.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5f15\u5bfc\u7f51\u7edc\uff08DGN\uff09\u7528\u4e8e\u56fe\u50cf\u6062\u590d\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b9,205\u5f20\u9ad8\u5206\u8fa8\u7387\u690d\u7269\u56fe\u50cf\u7684\u65b0\u6570\u636e\u96c6\u3002\u8be5\u7f51\u7edc\u901a\u8fc7\u6df1\u5ea6\u4f30\u8ba1\u5206\u652f\u63d0\u4f9b\u7ed3\u6784\u6307\u5bfc\uff0c\u5e76\u901a\u8fc7\u7ed3\u5408\u9010\u6b65\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u548c\u7a00\u758f\u975e\u5c40\u90e8\u6ce8\u610f\u529b\u6765\u6355\u83b7\u76f8\u4f3c\u6027\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u5ffd\u7565\u6df1\u5ea6\u4fe1\u606f\u7684\u9650\u5236\u3002\u5b9e\u9a8c\u8bc1\u660e\u8be5\u65b9\u6cd5\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u6cdb\u5316\u80fd\u529b\u5f3a\u3002", "motivation": "\u73b0\u6709\u7684\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u5e38\u5e38\u5ffd\u7565\u6df1\u5ea6\u4fe1\u606f\uff0c\u8fd9\u4f1a\u5f71\u54cd\u76f8\u4f3c\u6027\u5339\u914d\uff0c\u5e76\u5728\u6d45\u666f\u6df1\uff08DoF\uff09\u573a\u666f\u4e2d\u5bfc\u81f4\u6ce8\u610f\u529b\u5206\u6563\uff0c\u5728\u6df1\u666f\u6df1\u573a\u666f\u4e2d\u8fc7\u5ea6\u589e\u5f3a\u80cc\u666f\u5185\u5bb9\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df1\u5ea6\u5f15\u5bfc\u7f51\u7edc\uff08DGN\uff09\uff0c\u5305\u542b\u4e00\u4e2a\u6df1\u5ea6\u4f30\u8ba1\u5206\u652f\u548c\u4e00\u4e2a\u56fe\u50cf\u6062\u590d\u5206\u652f\u3002\u8be5\u7f51\u7edc\u5229\u7528\u4e86\u9010\u6b65\u7a97\u53e3\u81ea\u6ce8\u610f\u529b\u548c\u7a00\u758f\u975e\u5c40\u90e8\u6ce8\u610f\u529b\u6765\u6355\u83b7\u7269\u4f53\u5185\u90e8\u548c\u7269\u4f53\u95f4\u7684\u76f8\u4f3c\u6027\u3002\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\uff0c\u6df1\u5ea6\u7279\u5f81\u63d0\u9ad8\u4e86\u6062\u590d\u8d28\u91cf\uff0c\u800c\u6062\u590d\u5206\u652f\u7684\u89c6\u89c9\u7279\u5f81\u5219\u6709\u52a9\u4e8e\u6539\u8fdb\u6df1\u5ea6\u4f30\u8ba1\u3002", "result": "DGN\u5728\u6d45\u666f\u6df1\u548c\u6df1\u666f\u6df1\u573a\u666f\u4e0b\u5747\u80fd\u6709\u6548\u5904\u7406\u80cc\u666f\u5185\u5bb9\uff0c\u5e76\u4e14\u901a\u8fc7\u8054\u5408\u8bad\u7ec3\uff0c\u6df1\u5ea6\u7279\u5f81\u548c\u6062\u590d\u7279\u5f81\u76f8\u4e92\u4fc3\u8fdb\uff0c\u5171\u540c\u63d0\u5347\u4e86\u6062\u590d\u6548\u679c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u591a\u4e2a\u6807\u51c6\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u5f88\u597d\u5730\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u690d\u7269\u56fe\u50cf\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.07536", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07536", "abs": "https://arxiv.org/abs/2508.07536", "authors": ["Tasfiq E. Alam", "Md Manjurul Ahsan", "Shivakumar Raman"], "title": "Physics-Informed Multimodal Bearing Fault Classification under Variable Operating Conditions using Transfer Learning", "comment": null, "summary": "Accurate and interpretable bearing fault classification is critical for\nensuring the reliability of rotating machinery, particularly under variable\noperating conditions where domain shifts can significantly degrade model\nperformance. This study proposes a physics-informed multimodal convolutional\nneural network (CNN) with a late fusion architecture, integrating vibration and\nmotor current signals alongside a dedicated physics-based feature extraction\nbranch. The model incorporates a novel physics-informed loss function that\npenalizes physically implausible predictions based on characteristic bearing\nfault frequencies - Ball Pass Frequency Outer (BPFO) and Ball Pass Frequency\nInner (BPFI) - derived from bearing geometry and shaft speed. Comprehensive\nexperiments on the Paderborn University dataset demonstrate that the proposed\nphysics-informed approach consistently outperforms a non-physics-informed\nbaseline, achieving higher accuracy, reduced false classifications, and\nimproved robustness across multiple data splits. To address performance\ndegradation under unseen operating conditions, three transfer learning (TL)\nstrategies - Target-Specific Fine-Tuning (TSFT), Layer-Wise Adaptation Strategy\n(LAS), and Hybrid Feature Reuse (HFR) - are evaluated. Results show that LAS\nyields the best generalization, with additional performance gains when combined\nwith physics-informed modeling. Validation on the KAIST bearing dataset\nconfirms the framework's cross-dataset applicability, achieving up to 98\npercent accuracy. Statistical hypothesis testing further verifies significant\nimprovements (p < 0.01) in classification performance. The proposed framework\ndemonstrates the potential of integrating domain knowledge with data-driven\nlearning to achieve robust, interpretable, and generalizable fault diagnosis\nfor real-world industrial applications.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u7269\u7406\u77e5\u8bc6\u548c\u591a\u6a21\u6001\u6570\u636e\u7684CNN\u6a21\u578b\uff0c\u7528\u4e8e\u8f74\u627f\u6545\u969c\u8bca\u65ad\uff0c\u5728\u4e0d\u540c\u5de5\u51b5\u4e0b\u8868\u73b0\u51fa\u66f4\u597d\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5e76\u901a\u8fc7\u8fc1\u79fb\u5b66\u4e60\u63d0\u5347\u4e86\u6cdb\u5316\u80fd\u529b\uff0c\u6700\u7ec8\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u7684\u8de8\u6570\u636e\u96c6\u8bca\u65ad\u3002", "motivation": "\u4e3a\u4e86\u786e\u4fdd\u65cb\u8f6c\u673a\u68b0\u7684\u53ef\u9760\u6027\uff0c\u5c24\u5176\u662f\u5728\u5de5\u4f5c\u6761\u4ef6\u591a\u53d8\u4e14\u57df\u504f\u79fb\u4f1a\u663e\u8457\u964d\u4f4e\u6a21\u578b\u6027\u80fd\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u51c6\u786e\u4e14\u53ef\u89e3\u91ca\u7684\u8f74\u627f\u6545\u969c\u5206\u7c7b\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7269\u7406\u4fe1\u606f\u591a\u6a21\u6001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u91c7\u7528\u4e86\u540e\u671f\u878d\u5408\u67b6\u6784\uff0c\u6574\u5408\u4e86\u632f\u52a8\u4fe1\u53f7\u548c\u7535\u673a\u7535\u6d41\u4fe1\u53f7\uff0c\u5e76\u5305\u542b\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u4e8e\u7269\u7406\u7279\u5f81\u63d0\u53d6\u7684\u5206\u652f\u3002\u6a21\u578b\u5f15\u5165\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u4f1a\u6839\u636e\u4ece\u8f74\u627f\u51e0\u4f55\u548c\u8f74\u901f\u5ea6\u63a8\u5bfc\u51fa\u7684\u7279\u5f81\u8f74\u627f\u5ea7\u5916\u5708\u9891\u7387\uff08BPFO\uff09\u548c\u8f74\u627f\u5ea7\u5185\u5708\u9891\u7387\uff08BPFI\uff09\u6765\u60e9\u7f5a\u4e0d\u7b26\u5408\u7269\u7406\u89c4\u5f8b\u7684\u9884\u6d4b\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8bc4\u4f30\u4e86\u4e09\u79cd\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u7b56\u7565\uff1a\u76ee\u6807\u7279\u5b9a\u5fae\u8c03\uff08TSFT\uff09\u3001\u5c42\u7ea7\u81ea\u9002\u5e94\u7b56\u7565\uff08LAS\uff09\u548c\u6df7\u5408\u7279\u5f81\u590d\u7528\uff08HFR\uff09\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7269\u7406\u4fe1\u606f\u65b9\u6cd5\u5728Paderborn\u5927\u5b66\u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u975e\u7269\u7406\u4fe1\u606f\u57fa\u7ebf\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u51c6\u786e\u6027\u3001\u66f4\u5c11\u7684\u9519\u8bef\u5206\u7c7b\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u5206\u5272\u4e2d\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002\u5728\u8fc1\u79fb\u5b66\u4e60\u65b9\u9762\uff0cLAS\u7b56\u7565\u5c55\u73b0\u51fa\u6700\u4f73\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e14\u4e0e\u7269\u7406\u4fe1\u606f\u5efa\u6a21\u7ed3\u5408\u65f6\u80fd\u5e26\u6765\u989d\u5916\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728KAIST\u8f74\u627f\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u7ed3\u679c\u4e5f\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u8de8\u6570\u636e\u96c6\u9002\u7528\u6027\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe98%\u3002\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u8fdb\u4e00\u6b65\u8bc1\u5b9e\u4e86\u5206\u7c7b\u6027\u80fd\u7684\u663e\u8457\u6539\u8fdb\uff08p < 0.01\uff09\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7269\u7406\u4fe1\u606f\u591a\u6a21\u6001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\u7ed3\u5408\u540e\u671f\u878d\u5408\u67b6\u6784\uff0c\u901a\u8fc7\u6574\u5408\u632f\u52a8\u548c\u7535\u673a\u7535\u6d41\u4fe1\u53f7\u4ee5\u53ca\u4e13\u95e8\u7684\u57fa\u4e8e\u7269\u7406\u7279\u5f81\u63d0\u53d6\u5206\u652f\uff0c\u5e76\u5728\u7269\u7406\u4fe1\u606f\u635f\u5931\u51fd\u6570\u4e2d\u5f15\u5165\u57fa\u4e8e\u8f74\u627f\u51e0\u4f55\u548c\u8f74\u901f\u5ea6\u63a8\u5bfc\u51fa\u7684\u7279\u5f81\u8f74\u627f\u5ea7\u5916\u5708\u9891\u7387\uff08BPFO\uff09\u548c\u8f74\u627f\u5ea7\u5185\u5708\u9891\u7387\uff08BPFI\uff09\u7684\u7269\u7406\u4e0d\u5408\u7406\u9884\u6d4b\u60e9\u7f5a\uff0c\u5728Paderborn\u5927\u5b66\u6570\u636e\u96c6\u7684\u5b9e\u9a8c\u4e2d\uff0c\u76f8\u8f83\u4e8e\u975e\u7269\u7406\u4fe1\u606f\u57fa\u7ebf\u6a21\u578b\uff0c\u5728\u51c6\u786e\u6027\u3001\u51cf\u5c11\u9519\u8bef\u5206\u7c7b\u548c\u63d0\u9ad8\u8de8\u591a\u4e2a\u6570\u636e\u5206\u5272\u7684\u9c81\u68d2\u6027\u65b9\u9762\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002\u7814\u7a76\u8fd8\u8bc4\u4f30\u4e86\u4e09\u79cd\u8fc1\u79fb\u5b66\u4e60\uff08TL\uff09\u7b56\u7565\uff08\u76ee\u6807\u7279\u5b9a\u5fae\u8c03TSFT\u3001\u5c42\u7ea7\u81ea\u9002\u5e94\u7b56\u7565LAS\u548c\u6df7\u5408\u7279\u5f81\u590d\u7528HFR\uff09\u4ee5\u5e94\u5bf9\u672a\u77e5\u5de5\u51b5\u4e0b\u7684\u6027\u80fd\u4e0b\u964d\uff0c\u5176\u4e2dLAS\u8868\u73b0\u51fa\u6700\u4f73\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u5e76\u4e0e\u7269\u7406\u4fe1\u606f\u6a21\u578b\u7ed3\u5408\u80fd\u5e26\u6765\u989d\u5916\u7684\u6027\u80fd\u63d0\u5347\u3002\u5728KAIST\u8f74\u627f\u6570\u636e\u96c6\u4e0a\u7684\u9a8c\u8bc1\u8bc1\u5b9e\u4e86\u8be5\u6846\u67b6\u7684\u8de8\u6570\u636e\u96c6\u9002\u7528\u6027\uff0c\u51c6\u786e\u7387\u9ad8\u8fbe98%\u3002\u7edf\u8ba1\u5047\u8bbe\u68c0\u9a8c\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u5206\u7c7b\u6027\u80fd\u7684\u663e\u8457\u63d0\u5347\uff08p < 0.01\uff09\u3002\u8be5\u6846\u67b6\u5c55\u793a\u4e86\u5c06\u9886\u57df\u77e5\u8bc6\u4e0e\u6570\u636e\u9a71\u52a8\u5b66\u4e60\u76f8\u7ed3\u5408\uff0c\u4ee5\u5b9e\u73b0\u5b9e\u9645\u5de5\u4e1a\u5e94\u7528\u4e2d\u9c81\u68d2\u3001\u53ef\u89e3\u91ca\u548c\u53ef\u6cdb\u5316\u7684\u6545\u969c\u8bca\u65ad\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08140", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08140", "abs": "https://arxiv.org/abs/2508.08140", "authors": ["Jun Wang", "Zaifu Zhan", "Qixin Zhang", "Mingquan Lin", "Meijia Song", "Rui Zhang"], "title": "Data-Efficient Biomedical In-Context Learning: A Diversity-Enhanced Submodular Perspective", "comment": null, "summary": "Recent progress in large language models (LLMs) has leveraged their\nin-context learning (ICL) abilities to enable quick adaptation to unseen\nbiomedical NLP tasks. By incorporating only a few input-output examples into\nprompts, LLMs can rapidly perform these new tasks. While the impact of these\ndemonstrations on LLM performance has been extensively studied, most existing\napproaches prioritize representativeness over diversity when selecting examples\nfrom large corpora. To address this gap, we propose Dual-Div, a\ndiversity-enhanced data-efficient framework for demonstration selection in\nbiomedical ICL. Dual-Div employs a two-stage retrieval and ranking process:\nFirst, it identifies a limited set of candidate examples from a corpus by\noptimizing both representativeness and diversity (with optional annotation for\nunlabeled data). Second, it ranks these candidates against test queries to\nselect the most relevant and non-redundant demonstrations. Evaluated on three\nbiomedical NLP tasks (named entity recognition (NER), relation extraction (RE),\nand text classification (TC)) using LLaMA 3.1 and Qwen 2.5 for inference, along\nwith three retrievers (BGE-Large, BMRetriever, MedCPT), Dual-Div consistently\noutperforms baselines-achieving up to 5% higher macro-F1 scores-while\ndemonstrating robustness to prompt permutations and class imbalance. Our\nfindings establish that diversity in initial retrieval is more critical than\nranking-stage optimization, and limiting demonstrations to 3-5 examples\nmaximizes performance efficiency.", "AI": {"tldr": "LLM\u5728\u751f\u7269\u533b\u5b66\u4efb\u52a1\u7684\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\uff0c\u6f14\u793a\u6570\u636e\u7684\u591a\u6837\u6027\u6bd4\u4ee3\u8868\u6027\u66f4\u91cd\u8981\uff0c\u5c11\u91cf\uff083-5\u4e2a\uff09\u591a\u6837\u5316\u6f14\u793a\u80fd\u8fbe\u5230\u6700\u4f73\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u9009\u62e9LLM\u7684\u5c11\u6837\u672c\u5b66\u4e60\u6f14\u793a\u65f6\uff0c\u5927\u591a\u4f18\u5148\u8003\u8651\u4ee3\u8868\u6027\u800c\u975e\u591a\u6837\u6027\uff0c\u800c\u591a\u6837\u6027\u5728\u9002\u5e94\u65b0\u7684\u751f\u7269\u533b\u5b66NLP\u4efb\u52a1\u65f6\u53ef\u80fd\u5f88\u91cd\u8981\u3002", "method": "Dual-Div\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u6846\u67b6\uff0c\u9996\u5148\u901a\u8fc7\u4f18\u5316\u4ee3\u8868\u6027\u548c\u591a\u6837\u6027\u6765\u68c0\u7d22\u5019\u9009\u793a\u4f8b\uff0c\u7136\u540e\u6839\u636e\u6d4b\u8bd5\u67e5\u8be2\u5bf9\u5019\u9009\u793a\u4f8b\u8fdb\u884c\u6392\u540d\u4ee5\u9009\u62e9\u6700\u76f8\u5173\u548c\u975e\u5197\u4f59\u7684\u6f14\u793a\u3002", "result": "\u5728\u4e09\u4e2a\u751f\u7269\u533b\u5b66NLP\u4efb\u52a1\uff08NER\u3001RE\u548cTC\uff09\u4e0a\uff0cDual-Div\u76f8\u6bd4\u57fa\u7ebf\u6a21\u578b\uff0c\u5728LLaMA 3.1\u548cQwen 2.5\u4e0a\u53d6\u5f97\u4e86\u9ad8\u8fbe5%\u7684\u5b8fF1\u5206\u6570\u63d0\u5347\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5bf9\u63d0\u793a\u6392\u5217\u548c\u7c7b\u522b\u4e0d\u5e73\u8861\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "\u591a\u6837\u6027\u5728LLM\u7684\u751f\u7269\u533b\u5b66\u5c11\u6837\u672c\u5b66\u4e60\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u5e76\u4e14\u6f14\u793a\u6570\u636e\u7684\u9009\u62e9\u5e94\u4f18\u5148\u8003\u8651\u591a\u6837\u6027\u800c\u975e\u4ee3\u8868\u6027\u3002\u6b64\u5916\uff0c\u5c06\u6f14\u793a\u6570\u91cf\u9650\u5236\u57283-5\u4e2a\u793a\u4f8b\u53ef\u4ee5\u5b9e\u73b0\u6700\u4f73\u7684\u6027\u80fd\u6548\u7387\u3002"}}
{"id": "2508.07214", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.07214", "abs": "https://arxiv.org/abs/2508.07214", "authors": ["Hongyang Zhou", "Xiaobin Zhu", "Liuling Chen", "Junyi He", "Jingyan Qin", "Xu-Cheng Yin", "Zhang xiaoxing"], "title": "Unsupervised Real-World Super-Resolution via Rectified Flow Degradation Modelling", "comment": "10 pages, 9 figures", "summary": "Unsupervised real-world super-resolution (SR) faces critical challenges due\nto the complex, unknown degradation distributions in practical scenarios.\nExisting methods struggle to generalize from synthetic low-resolution (LR) and\nhigh-resolution (HR) image pairs to real-world data due to a significant domain\ngap. In this paper, we propose an unsupervised real-world SR method based on\nrectified flow to effectively capture and model real-world degradation,\nsynthesizing LR-HR training pairs with realistic degradation. Specifically,\ngiven unpaired LR and HR images, we propose a novel Rectified Flow Degradation\nModule (RFDM) that introduces degradation-transformed LR (DT-LR) images as\nintermediaries. By modeling the degradation trajectory in a continuous and\ninvertible manner, RFDM better captures real-world degradation and enhances the\nrealism of generated LR images. Additionally, we propose a Fourier Prior Guided\nDegradation Module (FGDM) that leverages structural information embedded in\nFourier phase components to ensure more precise modeling of real-world\ndegradation. Finally, the LR images are processed by both FGDM and RFDM,\nproducing final synthetic LR images with real-world degradation. The synthetic\nLR images are paired with the given HR images to train the off-the-shelf SR\nnetworks. Extensive experiments on real-world datasets demonstrate that our\nmethod significantly enhances the performance of existing SR approaches in\nreal-world scenarios.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65e0\u76d1\u7763\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u5229\u7528\u4fee\u6b63\u6d41\u548c\u5085\u91cc\u53f6\u5148\u9a8c\u6765\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\uff0c\u4ee5\u751f\u6210\u66f4\u903c\u771f\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u4ece\u800c\u63d0\u9ad8\u8d85\u5206\u8fa8\u7387\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u65e0\u76d1\u7763\u7684\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u9762\u4e34\u7740\u590d\u6742\u7684\u3001\u672a\u77e5\u7684\u5b9e\u9645\u9000\u5316\u5206\u5e03\u7684\u4e25\u5cfb\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u4ece\u5408\u6210\u4f4e\u5206\u8fa8\u7387\uff08LR\uff09\u548c\u9ad8\u5206\u8fa8\u7387\uff08HR\uff09\u56fe\u50cf\u5bf9\u6cdb\u5316\u5230\u771f\u5b9e\u4e16\u754c\u6570\u636e\u65f6\uff0c\u7531\u4e8e\u663e\u8457\u7684\u57df\u95f4\u9699\u800c\u96be\u4ee5\u53d6\u5f97\u826f\u597d\u6548\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4fee\u6b63\u6d41\u7684\u65e0\u76d1\u7763\u771f\u5b9e\u4e16\u754c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u4fee\u6b63\u6d41\u9000\u5316\u6a21\u5757\uff08RFDM\uff09\u548c\u5085\u91cc\u53f6\u5148\u9a8c\u5f15\u5bfc\u9000\u5316\u6a21\u5757\uff08FGDM\uff09\u3002RFDM\u901a\u8fc7\u5bf9\u9000\u5316\u8f68\u8ff9\u8fdb\u884c\u8fde\u7eed\u548c\u53ef\u9006\u5efa\u6a21\u6765\u751f\u6210\u4e2d\u95f4\u7684\u9000\u5316\u8f6c\u6362\u4f4e\u5206\u8fa8\u7387\uff08DT-LR\uff09\u56fe\u50cf\uff0c\u4ee5\u6355\u6349\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\u3002FGDM\u5229\u7528\u5085\u91cc\u53f6\u76f8\u4f4d\u5206\u91cf\u4e2d\u7684\u7ed3\u6784\u4fe1\u606f\u6765\u66f4\u7cbe\u786e\u5730\u6a21\u62df\u9000\u5316\u3002\u6700\u7ec8\uff0c\u901a\u8fc7\u7ed3\u5408RFDM\u548cFGDM\u5904\u7406\u540e\u7684\u4f4e\u5206\u8fa8\u7387\u56fe\u50cf\u4e0e\u7ed9\u5b9a\u7684\u9ad8\u5206\u8fa8\u7387\u56fe\u50cf\u914d\u5bf9\uff0c\u6765\u8bad\u7ec3\u8d85\u5206\u8fa8\u7387\u7f51\u7edc\u3002", "result": "\u5728\u771f\u5b9e\u4e16\u754c\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u663e\u8457\u63d0\u9ad8\u4e86\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002", "conclusion": "\u901a\u8fc7\u4f7f\u7528\u4fee\u6b63\u6d41\u548c\u5085\u91cc\u53f6\u5148\u9a8c\u6765\u6a21\u62df\u771f\u5b9e\u4e16\u754c\u7684\u9000\u5316\uff0c\u53ef\u4ee5\u6709\u6548\u5730\u5408\u6210\u5177\u6709\u771f\u5b9e\u9000\u5316\u7684\u4f4e\u5206\u8fa8\u7387-\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u5bf9\uff0c\u4ece\u800c\u63d0\u9ad8\u8d85\u5206\u8fa8\u7387\uff08SR\uff09\u65b9\u6cd5\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07555", "categories": ["cs.LG", "cs.IT", "cs.NI", "math.IT"], "pdf": "https://arxiv.org/pdf/2508.07555", "abs": "https://arxiv.org/abs/2508.07555", "authors": ["Keyuan Zhang", "Yin Sun", "Bo Ji"], "title": "Multimodal Remote Inference", "comment": "Accepted by The 22nd IEEE International Conference on Mobile Ad-Hoc\n  and Smart Systems (MASS 2025)", "summary": "We consider a remote inference system with multiple modalities, where a\nmultimodal machine learning (ML) model performs real-time inference using\nfeatures collected from remote sensors. As sensor observations may change\ndynamically over time, fresh features are critical for inference tasks.\nHowever, timely delivering features from all modalities is often infeasible due\nto limited network resources. To this end, we study a two-modality scheduling\nproblem to minimize the ML model's inference error, which is expressed as a\npenalty function of AoI for both modalities. We develop an index-based\nthreshold policy and prove its optimality. Specifically, the scheduler switches\nmodalities when the current modality's index function exceeds a threshold. We\nshow that the two modalities share the same threshold, and both the index\nfunctions and the threshold can be computed efficiently. The optimality of our\npolicy holds for (i) general AoI functions that are \\emph{non-monotonic} and\n\\emph{non-additive} and (ii) \\emph{heterogeneous} transmission times. Numerical\nresults show that our policy reduces inference error by up to 55% compared to\nround-robin and uniform random policies, which are oblivious to the AoI-based\ninference error function. Our results shed light on how to improve remote\ninference accuracy by optimizing task-oriented AoI functions.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4f18\u5316\u7684\u8c03\u5ea6\u7b56\u7565\uff0c\u4ee5\u6700\u5c0f\u5316\u591a\u6a21\u6001\u8fdc\u7a0b\u63a8\u7406\u7cfb\u7edf\u7684\u63a8\u7406\u8bef\u5dee\uff0c\u5728\u7f51\u7edc\u8d44\u6e90\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4f18\u5316\u6570\u636e\u4f20\u8f93\u6765\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u591a\u6a21\u6001\u8fdc\u7a0b\u63a8\u7406\u7cfb\u7edf\u4e2d\uff0c\u53ca\u65f6\u4ea4\u4ed8\u6240\u6709\u6a21\u6001\u7684\u7279\u5f81\u5bf9\u4e8e\u4fdd\u6301\u6a21\u578b\u7684\u51c6\u786e\u6027\u81f3\u5173\u91cd\u8981\u3002\u7136\u800c\uff0c\u7531\u4e8e\u7f51\u7edc\u8d44\u6e90\u6709\u9650\uff0c\u8fd9\u901a\u5e38\u662f\u4e0d\u53ef\u884c\u7684\u3002\u56e0\u6b64\uff0c\u7814\u7a76\u53cc\u6a21\u6001\u8c03\u5ea6\u95ee\u9898\u4ee5\u6700\u5c0f\u5316\u673a\u5668\u5b66\u4e60\u6a21\u578b\u7684\u63a8\u7406\u8bef\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7d22\u5f15\u7684\u9608\u503c\u7b56\u7565\uff0c\u8bc1\u660e\u4e86\u5176\u6700\u4f18\u6027\u3002\u8c03\u5ea6\u5668\u5728\u5f53\u524d\u6a21\u6001\u7684\u7d22\u5f15\u51fd\u6570\u8d85\u8fc7\u9608\u503c\u65f6\u5207\u6362\u6a21\u6001\u3002\u8bc1\u660e\u4e86\u4e24\u79cd\u6a21\u6001\u5177\u6709\u76f8\u540c\u7684\u9608\u503c\uff0c\u5e76\u4e14\u7d22\u5f15\u51fd\u6570\u548c\u9608\u503c\u90fd\u53ef\u4ee5\u6709\u6548\u5730\u8ba1\u7b97\u3002", "result": "\u6240\u63d0\u51fa\u7684\u7b56\u7565\u53ef\u5c06\u63a8\u7406\u8bef\u5dee\u51cf\u5c11\u591a\u8fbe55%\uff0c\u4f18\u4e8e\u968f\u673a\u8f6e\u8be2\u548c\u5747\u5300\u968f\u673a\u7b56\u7565\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u901a\u8fc7\u4f18\u5316\u9762\u5411\u4efb\u52a1\u7684\u5e73\u5747\u9a7b\u7559\u65f6\u95f4\uff08AoI\uff09\u51fd\u6570\u53ef\u4ee5\u63d0\u9ad8\u8fdc\u7a0b\u63a8\u7406\u7684\u51c6\u786e\u6027\u3002"}}
{"id": "2508.08149", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08149", "abs": "https://arxiv.org/abs/2508.08149", "authors": ["Wentao Jiang", "Xiang Feng", "Zengmao Wang", "Yong Luo", "Pingbo Xu", "Zhe Chen", "Bo Du", "Jing Zhang"], "title": "REX-RAG: Reasoning Exploration with Policy Correction in Retrieval-Augmented Generation", "comment": "17 pages, 4 figures", "summary": "Reinforcement learning (RL) is emerging as a powerful paradigm for enabling\nlarge language models (LLMs) to perform complex reasoning tasks. Recent\nadvances indicate that integrating RL with retrieval-augmented generation (RAG)\nallows LLMs to dynamically incorporate external knowledge, leading to more\ninformed and robust decision making. However, we identify a critical challenge\nduring policy-driven trajectory sampling: LLMs are frequently trapped in\nunproductive reasoning paths, which we refer to as \"dead ends\", committing to\noverconfident yet incorrect conclusions. This severely hampers exploration and\nundermines effective policy optimization. To address this challenge, we propose\nREX-RAG (Reasoning Exploration with Policy Correction in Retrieval-Augmented\nGeneration), a novel framework that explores alternative reasoning paths while\nmaintaining rigorous policy learning through principled distributional\ncorrections. Our approach introduces two key innovations: (1) Mixed Sampling\nStrategy, which combines a novel probe sampling method with exploratory prompts\nto escape dead ends; and (2) Policy Correction Mechanism, which employs\nimportance sampling to correct distribution shifts induced by mixed sampling,\nthereby mitigating gradient estimation bias. We evaluate it on seven\nquestion-answering benchmarks, and the experimental results show that REX-RAG\nachieves average performance gains of 5.1% on Qwen2.5-3B and 3.6% on Qwen2.5-7B\nover strong baselines, demonstrating competitive results across multiple\ndatasets. The code is publicly available at https://github.com/MiliLab/REX-RAG.", "AI": {"tldr": "REX-RAG\u901a\u8fc7\u6df7\u5408\u91c7\u6837\u548c\u7b56\u7565\u6821\u6b63\u89e3\u51b3\u4e86RL-RAG\u4e2dLLM\u9677\u5165\u6b7b\u80e1\u540c\u7684\u95ee\u9898\uff0c\u5728\u95ee\u7b54\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u5728\u7b56\u7565\u9a71\u52a8\u7684\u8f68\u8ff9\u91c7\u6837\u4e2d\uff0cLLM\u7ecf\u5e38\u9677\u5165\u65e0\u6548\u7684\u63a8\u7406\u8def\u5f84\uff08\u201c\u6b7b\u80e1\u540c\u201d\uff09\uff0c\u5bfc\u81f4\u8fc7\u4e8e\u81ea\u4fe1\u4f46\u9519\u8bef\u7684\u7ed3\u8bba\uff0c\u8fd9\u4e25\u91cd\u963b\u788d\u4e86\u63a2\u7d22\u548c\u6709\u6548\u7684\u7b56\u7565\u4f18\u5316\u3002", "method": "REX-RAG\u6846\u67b6\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6df7\u5408\u91c7\u6837\u7b56\u7565\uff08\u5305\u62ec\u63a2\u6d4b\u91c7\u6837\u65b9\u6cd5\u548c\u63a2\u7d22\u6027\u63d0\u793a\uff09\u6765\u8df3\u51fa\u6b7b\u80e1\u540c\uff0c\u4ee5\u53ca\u4e00\u4e2a\u7b56\u7565\u6821\u6b63\u673a\u5236\uff08\u4f7f\u7528\u91cd\u8981\u6027\u91c7\u6837\uff09\u6765\u7ea0\u6b63\u7531\u6df7\u5408\u91c7\u6837\u5f15\u8d77\u7684\u5206\u5e03\u504f\u79fb\uff0c\u4ece\u800c\u51cf\u8f7b\u68af\u5ea6\u4f30\u8ba1\u504f\u5dee\u3002", "result": "REX-RAG\u5728\u4e03\u4e2a\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u5f3a\u57fa\u7ebf\uff0c\u5728Qwen2.5-3B\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53475.1%\uff0c\u5728Qwen2.5-7B\u4e0a\u5e73\u5747\u6027\u80fd\u63d0\u53473.6%\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6570\u636e\u3002", "conclusion": "REX-RAG\u901a\u8fc7\u6df7\u5408\u91c7\u6837\u7b56\u7565\u548c\u7b56\u7565\u6821\u6b63\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86RL-RAG\u4e2dLLM\u9677\u5165\u65e0\u6548\u63a8\u7406\u8def\u5f84\uff08"}}
{"id": "2508.07216", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07216", "abs": "https://arxiv.org/abs/2508.07216", "authors": ["Songlin Li", "Zhiqing Guo", "Yuanman Li", "Zeyu Li", "Yunfeng Diao", "Gaobo Yang", "Liejun Wang"], "title": "Bridging Semantic Logic Gaps: A Cognition-Inspired Multimodal Boundary-Preserving Network for Image Manipulation Localization", "comment": null, "summary": "The existing image manipulation localization (IML) models mainly relies on\nvisual cues, but ignores the semantic logical relationships between content\nfeatures. In fact, the content semantics conveyed by real images often conform\nto human cognitive laws. However, image manipulation technology usually\ndestroys the internal relationship between content features, thus leaving\nsemantic clues for IML. In this paper, we propose a cognition-inspired\nmultimodal boundary-preserving network (CMB-Net). Specifically, CMB-Net\nutilizes large language models (LLMs) to analyze manipulated regions within\nimages and generate prompt-based textual information to compensate for the lack\nof semantic relationships in the visual information. Considering that the\nerroneous texts induced by hallucination from LLMs will damage the accuracy of\nIML, we propose an image-text central ambiguity module (ITCAM). It assigns\nweights to the text features by quantifying the ambiguity between text and\nimage features, thereby ensuring the beneficial impact of textual information.\nWe also propose an image-text interaction module (ITIM) that aligns visual and\ntext features using a correlation matrix for fine-grained interaction. Finally,\ninspired by invertible neural networks, we propose a restoration edge decoder\n(RED) that mutually generates input and output features to preserve boundary\ninformation in manipulated regions without loss. Extensive experiments show\nthat CMB-Net outperforms most existing IML models.", "AI": {"tldr": "CMB-Net leverages LLMs and multimodal fusion to capture semantic relationships disrupted by image manipulation, outperforming existing IML methods.", "motivation": "Existing image manipulation localization (IML) models mainly rely on visual cues but ignore the semantic logical relationships between content features. This paper addresses this gap by leveraging the fact that image manipulation often disrupts the internal relationship between content features, leaving semantic clues.", "method": "CMB-Net utilizes large language models (LLMs) to analyze manipulated regions within images and generate prompt-based textual information. It incorporates an image-text central ambiguity module (ITCAM) to quantify ambiguity between text and image features and assign weights to text features. Additionally, an image-text interaction module (ITIM) aligns visual and text features using a correlation matrix. A restoration edge decoder (RED) generates input and output features mutually to preserve boundary information in manipulated regions without loss.", "result": "The proposed CMB-Net, utilizing LLMs, ITCAM, ITIM, and RED, demonstrates superior performance compared to existing IML models in extensive experiments.", "conclusion": "CMB-Net outperforms most existing IML models."}}
{"id": "2508.07556", "categories": ["cs.LG", "cs.AI", "cs.CY", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07556", "abs": "https://arxiv.org/abs/2508.07556", "authors": ["Stephan Rabanser"], "title": "Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning", "comment": "PhD Thesis", "summary": "Machine learning (ML) systems are increasingly deployed in high-stakes\ndomains where reliability is paramount. This thesis investigates how\nuncertainty estimation can enhance the safety and trustworthiness of ML,\nfocusing on selective prediction -- where models abstain when confidence is\nlow.\n  We first show that a model's training trajectory contains rich uncertainty\nsignals that can be exploited without altering its architecture or loss. By\nensembling predictions from intermediate checkpoints, we propose a lightweight,\npost-hoc abstention method that works across tasks, avoids the cost of deep\nensembles, and achieves state-of-the-art selective prediction performance.\nCrucially, this approach is fully compatible with differential privacy (DP),\nallowing us to study how privacy noise affects uncertainty quality. We find\nthat while many methods degrade under DP, our trajectory-based approach remains\nrobust, and we introduce a framework for isolating the privacy-uncertainty\ntrade-off. Next, we then develop a finite-sample decomposition of the selective\nclassification gap -- the deviation from the oracle accuracy-coverage curve --\nidentifying five interpretable error sources and clarifying which interventions\ncan close the gap. This explains why calibration alone cannot fix ranking\nerrors, motivating methods that improve uncertainty ordering. Finally, we show\nthat uncertainty signals can be adversarially manipulated to hide errors or\ndeny service while maintaining high accuracy, and we design defenses combining\ncalibration audits with verifiable inference.\n  Together, these contributions advance reliable ML by improving, evaluating,\nand safeguarding uncertainty estimation, enabling models that not only make\naccurate predictions -- but also know when to say \"I do not know\".", "AI": {"tldr": "\u673a\u5668\u5b66\u4e60\u6a21\u578b\u5728\u5173\u952e\u4efb\u52a1\u4e2d\u5e94\u77e5\u9053\u4f55\u65f6\u8868\u793a\u201c\u6211\u4e0d\u77e5\u9053\u201d\u3002\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6a21\u578b\u8bad\u7ec3\u8f68\u8ff9\u7684\u4e2d\u95f4\u68c0\u67e5\u70b9\u6765\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u7684\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u9632\u5fa1\u5bf9\u6297\u6027\u64cd\u7eb5\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u7684\u7b56\u7565\u3002", "motivation": "\u673a\u5668\u5b66\u4e60\u7cfb\u7edf\u8d8a\u6765\u8d8a\u591a\u5730\u5e94\u7528\u4e8e\u9ad8\u98ce\u9669\u9886\u57df\uff0c\u5bf9\u53ef\u9760\u6027\u8981\u6c42\u6781\u9ad8\u3002\u672c\u6587\u65e8\u5728\u7814\u7a76\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u5982\u4f55\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u7684\u5b89\u5168\u6027\u548c\u53ef\u4fe1\u5ea6\uff0c\u7279\u522b\u662f\u901a\u8fc7\u9009\u62e9\u6027\u9884\u6d4b\uff08\u5373\u6a21\u578b\u5728\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u5f03\u6743\uff09\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6a21\u578b\u8bad\u7ec3\u8f68\u8ff9\u7684\u4e2d\u95f4\u68c0\u67e5\u70b9\u6765\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u4e2d\u95f4\u68c0\u67e5\u70b9\u7684\u9884\u6d4b\u6765\u5b9e\u73b0\uff0c\u65e0\u9700\u6539\u53d8\u6a21\u578b\u67b6\u6784\u6216\u635f\u5931\u51fd\u6570\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u6709\u9650\u6837\u672c\u5206\u89e3\u9009\u62e9\u6027\u5206\u7c7b\u5dee\u8ddd\u7684\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7ed3\u5408\u6821\u51c6\u5ba1\u8ba1\u548c\u53ef\u9a8c\u8bc1\u63a8\u7406\u7684\u9632\u5fa1\u7b56\u7565\u3002", "result": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u8bad\u7ec3\u8f68\u8ff9\u7684\u65b9\u6cd5\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u3001\u4e8b\u540e\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4efb\u52a1\uff0c\u4e14\u6210\u672c\u4f4e\u4e8e\u6df1\u5ea6\u96c6\u6210\u3002\u8be5\u65b9\u6cd5\u5728\u5dee\u5206\u9690\u79c1\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\uff0c\u5e76\u80fd\u5206\u79bb\u9690\u79c1-\u4e0d\u786e\u5b9a\u6027\u6743\u8861\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u660e\u786e\u4e86\u9009\u62e9\u6027\u5206\u7c7b\u5dee\u8ddd\u7684\u4e94\u4e2a\u53ef\u89e3\u91ca\u7684\u8bef\u5dee\u6765\u6e90\uff0c\u5e76\u5c55\u793a\u4e86\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u53ef\u80fd\u88ab\u64cd\u7eb5\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u9632\u5fa1\u63aa\u65bd\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u6a21\u578b\u8bad\u7ec3\u8f68\u8ff9\u7684\u4e2d\u95f4\u68c0\u67e5\u70b9\u6765\u4f30\u8ba1\u4e0d\u786e\u5b9a\u6027\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u673a\u5668\u5b66\u4e60\u7684\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u80fd\u591f\u5728\u6a21\u578b\u7f6e\u4fe1\u5ea6\u4f4e\u65f6\u8fdb\u884c\u9009\u62e9\u6027\u9884\u6d4b\uff0c\u5e76\u4e14\u4e0e\u5dee\u5206\u9690\u79c1\u517c\u5bb9\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5206\u6790\u4e86\u9009\u62e9\u6027\u9884\u6d4b\u7684\u8bef\u5dee\u6765\u6e90\uff0c\u5e76\u63d0\u51fa\u4e86\u9632\u5fa1\u5bf9\u6297\u6027\u64cd\u7eb5\u4e0d\u786e\u5b9a\u6027\u4fe1\u53f7\u7684\u7b56\u7565\u3002"}}
{"id": "2508.08163", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08163", "abs": "https://arxiv.org/abs/2508.08163", "authors": ["Mandira Sawkar", "Samay U. Shetty", "Deepak Pandita", "Tharindu Cyril Weerasooriya", "Christopher M. Homan"], "title": "LPI-RIT at LeWiDi-2025: Improving Distributional Predictions via Metadata and Loss Reweighting with DisCo", "comment": null, "summary": "The Learning With Disagreements (LeWiDi) 2025 shared task is to model\nannotator disagreement through soft label distribution prediction and\nperspectivist evaluation, modeling annotators. We adapt DisCo (Distribution\nfrom Context), a neural architecture that jointly models item-level and\nannotator-level label distributions, and present detailed analysis and\nimprovements. In this paper, we extend the DisCo by incorporating annotator\nmetadata, enhancing input representations, and modifying the loss functions to\ncapture disagreement patterns better. Through extensive experiments, we\ndemonstrate substantial improvements in both soft and perspectivist evaluation\nmetrics across three datasets. We also conduct in-depth error and calibration\nanalyses, highlighting the conditions under which improvements occur. Our\nfindings underscore the value of disagreement-aware modeling and offer insights\ninto how system components interact with the complexity of human-annotated\ndata.", "AI": {"tldr": "\u672c\u6587\u6269\u5c55\u4e86DisCo\u6a21\u578b\uff0c\u901a\u8fc7\u6574\u5408\u6ce8\u91ca\u8005\u5143\u6570\u636e\u3001\u589e\u5f3a\u8f93\u5165\u8868\u793a\u548c\u4fee\u6539\u635f\u5931\u51fd\u6570\uff0c\u63d0\u9ad8\u4e86\u5728\u6a21\u62df\u6ce8\u91ca\u8005\u5206\u6b67\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002", "motivation": "\u5b66\u4e60\u5e26\u5206\u6b67\uff08LeWiDi\uff092025\u5171\u4eab\u4efb\u52a1\u65e8\u5728\u901a\u8fc7\u8f6f\u6807\u7b7e\u5206\u5e03\u9884\u6d4b\u548c\u89c6\u89d2\u8bc4\u4f30\u6765\u6a21\u62df\u6ce8\u91ca\u8005\u5206\u6b67\uff0c\u800c\u672c\u6587\u7684\u76ee\u7684\u662f\u6269\u5c55DisCo\u6a21\u578b\u4ee5\u66f4\u597d\u5730\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\u3002", "method": "\u672c\u6587\u5728DisCo\u6a21\u578b\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u4e86\u6269\u5c55\uff0c\u901a\u8fc7\u6574\u5408\u6ce8\u91ca\u8005\u5143\u6570\u636e\u3001\u589e\u5f3a\u8f93\u5165\u8868\u793a\u548c\u4fee\u6539\u635f\u5931\u51fd\u6570\u6765\u66f4\u597d\u5730\u6355\u6349\u4e0d\u4e00\u81f4\u6a21\u5f0f\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u4e0a\uff0c\u672c\u6587\u63d0\u51fa\u7684DisCo\u6a21\u578b\u5728\u8f6f\u6807\u7b7e\u548c\u89c6\u89d2\u8bc4\u4f30\u6307\u6807\u4e0a\u90fd\u53d6\u5f97\u4e86\u5b9e\u8d28\u6027\u7684\u6539\u8fdb\u3002", "conclusion": "DisCo\u6a21\u578b\u7684\u6269\u5c55\u901a\u8fc7\u6574\u5408\u6ce8\u91ca\u8005\u5143\u6570\u636e\u3001\u589e\u5f3a\u8f93\u5165\u8868\u793a\u548c\u4fee\u6539\u635f\u5931\u51fd\u6570\uff0c\u5728\u6355\u6349\u4e0d\u4e00\u81f4\u6a21\u5f0f\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6539\u8fdb\uff0c\u5e76\u5728\u4e09\u4e2a\u6570\u636e\u96c6\u7684\u8f6f\u6807\u7b7e\u548c\u89c6\u89d2\u8bc4\u4f30\u6307\u6807\u4e0a\u5747\u6709\u63d0\u5347\u3002"}}
{"id": "2508.07217", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07217", "abs": "https://arxiv.org/abs/2508.07217", "authors": ["Yuqi Han", "Qi Cai", "Yuanxin Wu"], "title": "Generic Calibration: Pose Ambiguity/Linear Solution and Parametric-hybrid Pipeline", "comment": null, "summary": "Offline camera calibration techniques typically employ parametric or generic\ncamera models. Selecting parametric models relies heavily on user experience,\nand an inappropriate camera model can significantly affect calibration\naccuracy. Meanwhile, generic calibration methods involve complex procedures and\ncannot provide traditional intrinsic parameters. This paper reveals a pose\nambiguity in the pose solutions of generic calibration methods that\nirreversibly impacts subsequent pose estimation. A linear solver and a\nnonlinear optimization are proposed to address this ambiguity issue. Then a\nglobal optimization hybrid calibration method is introduced to integrate\ngeneric and parametric models together, which improves extrinsic parameter\naccuracy of generic calibration and mitigates overfitting and numerical\ninstability in parametric calibration. Simulation and real-world experimental\nresults demonstrate that the generic-parametric hybrid calibration method\nconsistently excels across various lens types and noise contamination,\nhopefully serving as a reliable and accurate solution for camera calibration in\ncomplex scenarios.", "AI": {"tldr": "\u4e00\u79cd\u7ed3\u5408\u901a\u7528\u548c\u53c2\u6570\u5316\u6a21\u578b\u7684\u6df7\u5408\u76f8\u673a\u6807\u5b9a\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u901a\u7528\u6807\u5b9a\u65b9\u6cd5\u7684\u4f4d\u59ff\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u63d0\u9ad8\u4e86\u6807\u5b9a\u7cbe\u5ea6\u548c\u7a33\u5b9a\u6027\u3002", "motivation": "\u73b0\u6709\u79bb\u7ebf\u76f8\u673a\u6807\u5b9a\u6280\u672f\u5728\u9009\u62e9\u53c2\u6570\u5316\u6a21\u578b\u65f6\u4f9d\u8d56\u7528\u6237\u7ecf\u9a8c\uff0c\u4e14\u6a21\u578b\u9009\u62e9\u4e0d\u5f53\u4f1a\u5f71\u54cd\u6807\u5b9a\u7cbe\u5ea6\uff1b\u901a\u7528\u6807\u5b9a\u65b9\u6cd5\u8fc7\u7a0b\u590d\u6742\u4e14\u65e0\u6cd5\u63d0\u4f9b\u4f20\u7edf\u5185\u53c2\u3002\u672c\u6587\u63ed\u793a\u4e86\u901a\u7528\u6807\u5b9a\u65b9\u6cd5\u4f4d\u59ff\u89e3\u4e2d\u7684\u6a21\u7cca\u6027\u95ee\u9898\uff0c\u8be5\u95ee\u9898\u4f1a\u4e0d\u53ef\u9006\u5730\u5f71\u54cd\u540e\u7eed\u7684\u4f4d\u59ff\u4f30\u8ba1\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ebf\u6027\u6c42\u89e3\u5668\u548c\u975e\u7ebf\u6027\u4f18\u5316\u6765\u89e3\u51b3\u4f4d\u59ff\u6a21\u7cca\u95ee\u9898\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u79cd\u5168\u5c40\u4f18\u5316\u6df7\u5408\u6807\u5b9a\u65b9\u6cd5\u6765\u6574\u5408\u901a\u7528\u548c\u53c2\u6570\u5316\u6a21\u578b\u3002", "result": "\u6df7\u5408\u6807\u5b9a\u65b9\u6cd5\u63d0\u9ad8\u4e86\u901a\u7528\u6807\u5b9a\u65b9\u6cd5\u7684\u4f30\u8ba1\u4f4d\u59ff\u7cbe\u5ea6\uff0c\u5e76\u7f13\u89e3\u4e86\u53c2\u6570\u5316\u6807\u5b9a\u4e2d\u7684\u8fc7\u62df\u5408\u548c\u6570\u503c\u4e0d\u7a33\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u901a\u7528\u548c\u53c2\u6570\u5316\u6a21\u578b\u7684\u6df7\u5408\u6807\u5b9a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u955c\u5934\u7c7b\u578b\u548c\u566a\u58f0\u6c61\u67d3\u4e0b\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u671b\u6210\u4e3a\u590d\u6742\u573a\u666f\u4e0b\u76f8\u673a\u6807\u5b9a\u7684\u53ef\u9760\u51c6\u786e\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07571", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07571", "abs": "https://arxiv.org/abs/2508.07571", "authors": ["Xingwu Chen", "Miao Lu", "Beining Wu", "Difan Zou"], "title": "Towards Theoretical Understanding of Transformer Test-Time Computing: Investigation on In-Context Linear Regression", "comment": null, "summary": "Using more test-time computation during language model inference, such as\ngenerating more intermediate thoughts or sampling multiple candidate answers,\nhas proven effective in significantly improving model performance. This paper\ntakes an initial step toward bridging the gap between practical language model\ninference and theoretical transformer analysis by incorporating randomness and\nsampling. We focus on in-context linear regression with continuous/binary\ncoefficients, where our framework simulates language model decoding through\nnoise injection and binary coefficient sampling. Through this framework, we\nprovide detailed analyses of widely adopted inference techniques. Supported by\nempirical results, our theoretical framework and analysis demonstrate the\npotential for offering new insights into understanding inference behaviors in\nreal-world language models.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u6574\u5408\u968f\u673a\u6027\u548c\u91c7\u6837\u6765\u5f25\u5408\u5b9e\u9645\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u7406\u8bba Transformer \u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u4e0a\u4e0b\u6587\u5185\u7ebf\u6027\u56de\u5f52\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5c55\u793a\u5176\u5728\u7406\u89e3\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u884c\u4e3a\u65b9\u9762\u7684\u6f5c\u529b\u7684\u7406\u8bba\u6846\u67b6\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u5f25\u5408\u5b9e\u9645\u8bed\u8a00\u6a21\u578b\u63a8\u7406\u548c\u7406\u8bba Transformer \u5206\u6790\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u65b9\u6cd5\u662f\u6574\u5408\u968f\u673a\u6027\u548c\u91c7\u6837\u3002", "method": "\u672c\u6587\u5c06\u968f\u673a\u6027\u548c\u91c7\u6837\u7eb3\u5165\u5176\u6846\u67b6\uff0c\u6a21\u62df\u4e86\u8bed\u8a00\u6a21\u578b\u89e3\u7801\u8fc7\u7a0b\uff0c\u91cd\u70b9\u7814\u7a76\u4e86\u5177\u6709\u8fde\u7eed/\u4e8c\u5143\u7cfb\u6570\u7684\u4e0a\u4e0b\u6587\u5185\u7ebf\u6027\u56de\u5f52\uff0c\u5e76\u901a\u8fc7\u566a\u58f0\u6ce8\u5165\u548c\u4e8c\u5143\u7cfb\u6570\u91c7\u6837\u8fdb\u884c\u3002", "result": "\u8be5\u6846\u67b6\u901a\u8fc7\u7ecf\u9a8c\u7ed3\u679c\u5f97\u5230\u4e86\u652f\u6301\uff0c\u5e76\u4e3a\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u63d0\u4f9b\u4e86\u89c1\u89e3\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7406\u8bba\u6846\u67b6\u548c\u5206\u6790\u901a\u8fc7\u7ecf\u9a8c\u7ed3\u679c\u5f97\u5230\u652f\u6301\uff0c\u5c55\u793a\u4e86\u4e3a\u7406\u89e3\u771f\u5b9e\u4e16\u754c\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u884c\u4e3a\u63d0\u4f9b\u65b0\u89c1\u89e3\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.08192", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08192", "abs": "https://arxiv.org/abs/2508.08192", "authors": ["Bangsheng Tang", "Carl Chengyan Fu", "Fei Kou", "Grigory Sizov", "Haoci Zhang", "Jason Park", "Jiawen Liu", "Jie You", "Qirui Yang", "Sachin Mehta", "Shengyong Cai", "Xiaodong Wang", "Xingyu Liu", "Yunlu Li", "Yanjun Zhou", "Wei Wei", "Zhiwei Zhao", "Zixi Qi", "Adolfo Victoria", "Aya Ibrahim", "Bram Wasti", "Changkyu Kim", "Daniel Haziza", "Fei Sun", "Giancarlo Delfin", "Emily Guo", "Jialin Ouyang", "Jaewon Lee", "Jianyu Huang", "Jeremy Reizenstein", "Lu Fang", "Quinn Zhu", "Ria Verma", "Vlad Mihailescu", "Xingwen Guo", "Yan Cui", "Ye Hu", "Yejin Lee"], "title": "Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions", "comment": "15 pages", "summary": "Speculative decoding is a standard method for accelerating the inference\nspeed of large language models. However, scaling it for production environments\nposes several engineering challenges, including efficiently implementing\ndifferent operations (e.g., tree attention and multi-round speculative\ndecoding) on GPU. In this paper, we detail the training and inference\noptimization techniques that we have implemented to enable EAGLE-based\nspeculative decoding at a production scale for Llama models. With these\nchanges, we achieve a new state-of-the-art inference latency for Llama models.\nFor example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a\nbatch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the\npreviously best known method. Furthermore, for EAGLE-based speculative\ndecoding, our optimizations enable us to achieve a speed-up for large batch\nsizes between 1.4x and 2.0x at production scale.", "AI": {"tldr": "Speculative decoding optimizations for Llama models (EAGLE) achieve state-of-the-art inference speed, significantly reducing latency and improving throughput for large batch sizes on GPUs.", "motivation": "To address the engineering challenges of scaling speculative decoding for production environments, particularly the efficient implementation of various operations on GPUs.", "method": "The paper details training and inference optimization techniques implemented to enable EAGLE-based speculative decoding at a production scale for Llama models, focusing on efficient implementation of operations like tree attention and multi-round speculative decoding on GPUs.", "result": "Achieved new state-of-the-art inference latency for Llama models. Llama4 Maverick decodes at ~4 ms/token (batch size 1) on 8 NVIDIA H100 GPUs, 10% faster than previous best. EAGLE-based speculative decoding achieved 1.4x-2.0x speed-up for large batch sizes at production scale.", "conclusion": "EAGLE-based speculative decoding at a production scale for Llama models has achieved a new state-of-the-art inference latency, with Llama4 Maverick decoding at about 4 ms per token on 8 NVIDIA H100 GPUs (10% faster than previous methods) and EAGLE-based speculative decoding achieving a 1.4x to 2.0x speed-up for large batch sizes."}}
{"id": "2508.07225", "categories": ["eess.IV", "cs.CV", "q-bio.QM", "92C40, 68T07", "I.2.10; I.4.8"], "pdf": "https://arxiv.org/pdf/2508.07225", "abs": "https://arxiv.org/abs/2508.07225", "authors": ["Xuepeng Liu", "Zheng Jiang", "Pinan Zhu", "Hanyu Liu", "Chao Li"], "title": "HaDM-ST: Histology-Assisted Differential Modeling for Spatial Transcriptomics Generation", "comment": "10 pages, 5 figures, includes comparisons with TESLA, HiStoGene, and\n  iStar; submitted to arXiv 2025", "summary": "Spatial transcriptomics (ST) reveals spatial heterogeneity of gene\nexpression, yet its resolution is limited by current platforms. Recent methods\nenhance resolution via H&E-stained histology, but three major challenges\npersist: (1) isolating expression-relevant features from visually complex H&E\nimages; (2) achieving spatially precise multimodal alignment in diffusion-based\nframeworks; and (3) modeling gene-specific variation across expression\nchannels. We propose HaDM-ST (Histology-assisted Differential Modeling for ST\nGeneration), a high-resolution ST generation framework conditioned on H&E\nimages and low-resolution ST. HaDM-ST includes: (i) a semantic distillation\nnetwork to extract predictive cues from H&E; (ii) a spatial alignment module\nenforcing pixel-wise correspondence with low-resolution ST; and (iii) a\nchannel-aware adversarial learner for fine-grained gene-level modeling.\nExperiments on 200 genes across diverse tissues and species show HaDM-ST\nconsistently outperforms prior methods, enhancing spatial fidelity and\ngene-level coherence in high-resolution ST predictions.", "AI": {"tldr": "HaDM-ST \u662f\u4e00\u4e2a\u5229\u7528 H&E \u56fe\u50cf\u548c\u4f4e\u5206\u8fa8\u7387 ST \u751f\u6210\u9ad8\u5206\u8fa8\u7387 ST \u7684\u6846\u67b6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u3002", "motivation": "\u76ee\u524d\u7684\u7a7a\u95f4\u8f6c\u5f55\u7ec4\u5b66 (ST) \u6280\u672f\u5206\u8fa8\u7387\u6709\u9650\uff0c\u5c3d\u7ba1\u6709\u5229\u7528 H&E \u67d3\u8272\u7ec4\u7ec7\u5b66\u56fe\u50cf\u6765\u63d0\u9ad8\u5206\u8fa8\u7387\u7684\u65b9\u6cd5\uff0c\u4f46\u4ecd\u5b58\u5728\u4e09\u4e2a\u4e3b\u8981\u6311\u6218\uff1a\u4ece\u89c6\u89c9\u4e0a\u590d\u6742\u7684 H&E \u56fe\u50cf\u4e2d\u5206\u79bb\u51fa\u4e0e\u8868\u8fbe\u76f8\u5173\u7684\u7279\u5f81\uff1b\u5728\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\u4e2d\u5b9e\u73b0\u7a7a\u95f4\u7cbe\u786e\u7684\u591a\u6a21\u6001\u5bf9\u9f50\uff1b\u4ee5\u53ca\u6a21\u62df\u8de8\u8868\u8fbe\u901a\u9053\u7684\u57fa\u56e0\u7279\u5f02\u6027\u53d8\u5f02\u3002", "method": "HaDM-ST \u6846\u67b6\u5305\u542b\u4e00\u4e2a\u8bed\u4e49\u84b8\u998f\u7f51\u7edc\uff0c\u7528\u4e8e\u4ece H&E \u56fe\u50cf\u4e2d\u63d0\u53d6\u9884\u6d4b\u7ebf\u7d22\uff1b\u4e00\u4e2a\u7a7a\u95f4\u5bf9\u9f50\u6a21\u5757\uff0c\u7528\u4e8e\u5f3a\u5236\u4e0e\u4f4e\u5206\u8fa8\u7387 ST \u8fdb\u884c\u50cf\u7d20\u7ea7\u5bf9\u5e94\uff1b\u4ee5\u53ca\u4e00\u4e2a\u901a\u9053\u611f\u77e5\u5bf9\u6297\u5b66\u4e60\u5668\uff0c\u7528\u4e8e\u8fdb\u884c\u7ec6\u7c92\u5ea6\u7684\u57fa\u56e0\u6c34\u5e73\u5efa\u6a21\u3002", "result": "\u5728\u8de8\u8d8a\u4e0d\u540c\u7ec4\u7ec7\u548c\u7269\u79cd\u7684 200 \u4e2a\u57fa\u56e0\u7684\u5b9e\u9a8c\u4e2d\uff0cHaDM-ST \u5728\u63d0\u9ad8\u7a7a\u95f4\u4fdd\u771f\u5ea6\u548c\u57fa\u56e0\u6c34\u5e73\u76f8\u5e72\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "HaDM-ST \u5728\u771f\u5b9e\u4e16\u754c\u7684\u6570\u636e\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u63d0\u9ad8\u4e86\u9ad8\u5206\u8fa8\u7387 ST \u9884\u6d4b\u7684\u7a7a\u95f4\u4fdd\u771f\u5ea6\u548c\u57fa\u56e0\u6c34\u5e73\u76f8\u5e72\u6027\u3002"}}
{"id": "2508.07581", "categories": ["cs.LG", "math.DS", "math.PR"], "pdf": "https://arxiv.org/pdf/2508.07581", "abs": "https://arxiv.org/abs/2508.07581", "authors": ["Nisha Chandramoorthy", "Adriaan de Clercq"], "title": "When and how can inexact generative models still sample from the data manifold?", "comment": null, "summary": "A curious phenomenon observed in some dynamical generative models is the\nfollowing: despite learning errors in the score function or the drift vector\nfield, the generated samples appear to shift \\emph{along} the support of the\ndata distribution but not \\emph{away} from it. In this work, we investigate\nthis phenomenon of \\emph{robustness of the support} by taking a dynamical\nsystems approach on the generating stochastic/deterministic process. Our\nperturbation analysis of the probability flow reveals that infinitesimal\nlearning errors cause the predicted density to be different from the target\ndensity only on the data manifold for a wide class of generative models.\nFurther, what is the dynamical mechanism that leads to the robustness of the\nsupport? We show that the alignment of the top Lyapunov vectors (most sensitive\ninfinitesimal perturbation directions) with the tangent spaces along the\nboundary of the data manifold leads to robustness and prove a sufficient\ncondition on the dynamics of the generating process to achieve this alignment.\nMoreover, the alignment condition is efficient to compute and, in practice, for\nrobust generative models, automatically leads to accurate estimates of the\ntangent bundle of the data manifold. Using a finite-time linear perturbation\nanalysis on samples paths as well as probability flows, our work complements\nand extends existing works on obtaining theoretical guarantees for generative\nmodels from a stochastic analysis, statistical learning and uncertainty\nquantification points of view. Our results apply across different dynamical\ngenerative models, such as conditional flow-matching and score-based generative\nmodels, and for different target distributions that may or may not satisfy the\nmanifold hypothesis.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u52a8\u529b\u5b66\u65b9\u6cd5\u89e3\u91ca\u4e86\u751f\u6210\u6a21\u578b\u4e2d\u7684\u201c\u652f\u6491\u96c6\u9c81\u68d2\u6027\u201d\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5373\u4f7f\u5728\u5b66\u4e60\u8bef\u5dee\u4e0b\u4e5f\u80fd\u751f\u6210\u903c\u771f\u7684\u6570\u636e\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u674e\u96c5\u666e\u8bfa\u592b\u5411\u91cf\u4e0e\u6570\u636e\u6d41\u5f62\u8fb9\u754c\u5207\u7a7a\u95f4\u7684\u5bf9\u9f50\u662f\u5b9e\u73b0\u8be5\u73b0\u8c61\u7684\u5173\u952e\uff0c\u5e76\u63d0\u51fa\u4e86\u76f8\u5e94\u7684\u8ba1\u7b97\u548c\u9a8c\u8bc1\u65b9\u6cd5\u3002", "motivation": "\u8be5\u7814\u7a76\u65e8\u5728\u63a2\u7a76\u5e76\u89e3\u91ca\u4e00\u7c7b\u751f\u6210\u6a21\u578b\u5728\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u51fa\u73b0\u7684\u201c\u652f\u6491\u96c6\u9c81\u68d2\u6027\u201d\u73b0\u8c61\uff0c\u5373\u6a21\u578b\u5728\u5b66\u4e60\u8bef\u5dee\u4e0b\u4ecd\u80fd\u751f\u6210\u63a5\u8fd1\u771f\u5b9e\u6570\u636e\u5206\u5e03\u7684\u6837\u672c\u3002", "method": "\u901a\u8fc7\u6270\u52a8\u5206\u6790\u6982\u7387\u6d41\uff0c\u7814\u7a76\u53d1\u73b0\u5bf9\u4e8e\u5e7f\u6cdb\u7684\u751f\u6210\u6a21\u578b\uff0c\u5fae\u5c0f\u7684\u5b66\u4e60\u8bef\u5dee\u4ec5\u4f1a\u5bfc\u81f4\u6570\u636e\u6d41\u5f62\u4e0a\u7684\u9884\u6d4b\u5bc6\u5ea6\u4e0e\u76ee\u6807\u5bc6\u5ea6\u4ea7\u751f\u5dee\u5f02\u3002\u901a\u8fc7\u5206\u6790\u6700\u654f\u611f\u7684\u5fae\u5c0f\u6270\u52a8\u65b9\u5411\uff08\u9876\u7ea7\u674e\u96c5\u666e\u8bfa\u592b\u5411\u91cf\uff09\u662f\u5426\u6cbf\u7740\u6570\u636e\u6d41\u5f62\u8fb9\u754c\u7684\u5207\u7a7a\u95f4\u5bf9\u9f50\uff0c\u6765\u63ed\u793a\u5bfc\u81f4\u652f\u6491\u96c6\u9c81\u68d2\u6027\u7684\u52a8\u529b\u5b66\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u5b9e\u73b0\u8fd9\u79cd\u5bf9\u9f50\u7684\u52a8\u529b\u5b66\u8fc7\u7a0b\u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u9876\u7ea7\u674e\u96c5\u666e\u8bfa\u592b\u5411\u91cf\u4e0e\u6570\u636e\u6d41\u5f62\u8fb9\u754c\u5207\u7a7a\u95f4\u7684\u5bf9\u9f50\u662f\u5b9e\u73b0\u652f\u6491\u96c6\u9c81\u68d2\u6027\u7684\u5173\u952e\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u5bf9\u9f50\u6761\u4ef6\u4e0d\u4ec5\u6613\u4e8e\u8ba1\u7b97\uff0c\u800c\u4e14\u5728\u5b9e\u8df5\u4e2d\u80fd\u81ea\u52a8\u5f97\u5230\u6570\u636e\u6d41\u5f62\u5207\u7a7a\u95f4\u675f\u7684\u51c6\u786e\u4f30\u8ba1\u3002\u8be5\u7814\u7a76\u901a\u8fc7\u6709\u9650\u65f6\u95f4\u7ebf\u6027\u6270\u52a8\u5206\u6790\u6837\u672c\u8def\u5f84\u548c\u6982\u7387\u6d41\uff0c\u4e3a\u751f\u6210\u6a21\u578b\u63d0\u4f9b\u4e86\u7406\u8bba\u4fdd\u8bc1\uff0c\u5e76\u6269\u5c55\u4e86\u968f\u673a\u5206\u6790\u3001\u7edf\u8ba1\u5b66\u4e60\u548c\u4e0d\u786e\u5b9a\u6027\u91cf\u5316\u7b49\u9886\u57df\u7684\u76f8\u5173\u5de5\u4f5c\u3002", "conclusion": "\u8be5\u7814\u7a76\u89e3\u91ca\u4e86\u4e3a\u4ec0\u4e48\u5373\u4f7f\u5728\u5f97\u5206\u51fd\u6570\u6216\u6f02\u79fb\u5411\u91cf\u573a\u5b58\u5728\u5b66\u4e60\u8bef\u5dee\u7684\u60c5\u51b5\u4e0b\uff0c\u67d0\u4e9b\u52a8\u6001\u751f\u6210\u6a21\u578b\u4ecd\u80fd\u751f\u6210\u6cbf\u6570\u636e\u5206\u5e03\u652f\u6491\u96c6\u800c\u975e\u504f\u79bb\u5176\u652f\u6491\u96c6\u7684\u6837\u672c\u3002\u901a\u8fc7\u5bf9\u751f\u6210\u8fc7\u7a0b\uff08\u968f\u673a/\u786e\u5b9a\u6027\uff09\u8fdb\u884c\u52a8\u529b\u7cfb\u7edf\u5206\u6790\uff0c\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u201c\u652f\u6491\u96c6\u9c81\u68d2\u6027\u201d\u73b0\u8c61\u3002"}}
{"id": "2508.08204", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08204", "abs": "https://arxiv.org/abs/2508.08204", "authors": ["Kyle Moore", "Jesse Roberts", "Daryl Watson"], "title": "Human-Alignment and Calibration of Inference-Time Uncertainty in Large Language Models", "comment": "preprint, under review", "summary": "There has been much recent interest in evaluating large language models for\nuncertainty calibration to facilitate model control and modulate user trust.\nInference time uncertainty, which may provide a real-time signal to the model\nor external control modules, is particularly important for applying these\nconcepts to improve LLM-user experience in practice. While many of the existing\npapers consider model calibration, comparatively little work has sought to\nevaluate how closely model uncertainty aligns to human uncertainty. In this\nwork, we evaluate a collection of inference-time uncertainty measures, using\nboth established metrics and novel variations, to determine how closely they\nalign with both human group-level uncertainty and traditional notions of model\ncalibration. We find that numerous measures show evidence of strong alignment\nto human uncertainty, even despite the lack of alignment to human answer\npreference. For those successful metrics, we find moderate to strong evidence\nof model calibration in terms of both correctness correlation and\ndistributional analysis.", "AI": {"tldr": "\u8bc4\u4f30LLM\u7684\u4e0d\u786e\u5b9a\u6027\u6821\u51c6\u3002", "motivation": "\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\u5728[\u4e0d]\u786e\u5b9a\u6027\u6821\u51c6\u4ee5\u4fc3\u8fdb\u6a21\u578b\u63a7\u5236\u548c\u8c03\u8282\u7528\u6237\u4fe1\u4efb\u3002", "method": "\u6211\u4eec\u8bc4\u4f30\u4e86\u4e00\u7cfb\u5217\u63a8\u7406\u65f6[\u4e0d]\u786e\u5b9a\u6027\u5ea6\u91cf\uff0c\u4f7f\u7528\u5df2\u5efa\u7acb\u7684\u5ea6\u91cf\u548c\u65b0\u9896\u7684\u53d8\u4f53\uff0c\u4ee5\u786e\u5b9a\u5b83\u4eec\u4e0e\u4eba\u7c7b\u7684[\u4e0d]\u786e\u5b9a\u6027\u4ee5\u53ca\u6a21\u578b\u6821\u51c6\u7684\u4f20\u7edf\u89c2\u5ff5\u7684\u5339\u914d\u7a0b\u5ea6\u3002", "result": "\u6211\u4eec\u53d1\u73b0\uff0c\u8bb8\u591a\u5ea6\u91cf\u6807\u51c6\u90fd\u663e\u793a\u51fa\u4e0e\u4eba\u7c7b\u4e0d\u786e\u5b9a\u6027\u9ad8\u5ea6\u4e00\u81f4\u7684\u8bc1\u636e\u3002", "conclusion": "\u8bb8\u591a\u6a21\u578b\u5728\u6b63\u786e\u6027\u76f8\u5173\u6027\u548c\u5206\u5e03\u5206\u6790\u4e24\u65b9\u9762\u90fd\u663e\u793a\u51fa\u4e0e\u4eba\u7c7b\u4e0d\u786e\u5b9a\u6027\u7684\u826f\u597d\u5339\u914d\uff0c\u5373\u4f7f\u5728\u4e0d\u5339\u914d\u4eba\u7c7b\u7b54\u6848\u504f\u597d\u7684\u60c5\u51b5\u4e0b\u4e5f\u662f\u5982\u6b64\u3002"}}
{"id": "2508.07233", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07233", "abs": "https://arxiv.org/abs/2508.07233", "authors": ["Lei Yang", "Junshan Jin", "Mingyuan Zhang", "Yi He", "Bofan Chen", "Shilin Wang"], "title": "Landmark Guided Visual Feature Extractor for Visual Speech Recognition with Limited Resource", "comment": null, "summary": "Visual speech recognition is a technique to identify spoken content in silent\nspeech videos, which has raised significant attention in recent years.\nAdvancements in data-driven deep learning methods have significantly improved\nboth the speed and accuracy of recognition. However, these deep learning\nmethods can be effected by visual disturbances, such as lightning conditions,\nskin texture and other user-specific features. Data-driven approaches could\nreduce the performance degradation caused by these visual disturbances using\nmodels pretrained on large-scale datasets. But these methods often require\nlarge amounts of training data and computational resources, making them costly.\nTo reduce the influence of user-specific features and enhance performance with\nlimited data, this paper proposed a landmark guided visual feature extractor.\nFacial landmarks are used as auxiliary information to aid in training the\nvisual feature extractor. A spatio-temporal multi-graph convolutional network\nis designed to fully exploit the spatial locations and spatio-temporal features\nof facial landmarks. Additionally, a multi-level lip dynamic fusion framework\nis introduced to combine the spatio-temporal features of the landmarks with the\nvisual features extracted from the raw video frames. Experimental results show\nthat this approach performs well with limited data and also improves the\nmodel's accuracy on unseen speakers.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cdlandmark\u5f15\u5bfc\u7684\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u7ed3\u5408\u65f6\u7a7a\u591a\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u591a\u5c42\u5507\u52a8\u878d\u5408\u6846\u67b6\uff0c\u5728\u6570\u636e\u6709\u9650\u548c\u672a\u89c1\u8fc7\u8bf4\u8bdd\u4eba\u65f6\u80fd\u63d0\u9ad8\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u51cf\u5c11\u7528\u6237\u7279\u5b9a\u7279\u5f81\u7684\u5f71\u54cd\u5e76\u63d0\u9ad8\u5728\u6709\u9650\u6570\u636e\u4e0b\u7684\u6027\u80fd\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u53d7\u89c6\u89c9\u5e72\u6270\uff08\u5982\u5149\u7167\u6761\u4ef6\u3001\u76ae\u80a4\u7eb9\u7406\u7b49\uff09\u5f71\u54cd\u4ee5\u53ca\u9700\u8981\u5927\u91cf\u8bad\u7ec3\u6570\u636e\u548c\u8ba1\u7b97\u8d44\u6e90\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cdlandmark\u5f15\u5bfc\u7684\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u65f6\u7a7a\u591a\u56fe\u5377\u79ef\u7f51\u7edc\u6765\u5145\u5206\u5229\u7528\u9762\u90e8landmark\u7684\u7a7a\u95f4\u4f4d\u7f6e\u548c\u65f6\u7a7a\u7279\u5f81\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u591a\u5c42\u5507\u52a8\u878d\u5408\u6846\u67b6\u6765\u7ed3\u5408landmark\u7684\u65f6\u7a7a\u7279\u5f81\u548c\u4ece\u539f\u59cb\u89c6\u9891\u5e27\u4e2d\u63d0\u53d6\u7684\u89c6\u89c9\u7279\u5f81\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\u8868\u73b0\u826f\u597d\uff0c\u5e76\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u8bf4\u8bdd\u4eba\u4e0a\u7684\u51c6\u786e\u7387\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8elandmark\u7684\u89c6\u89c9\u7279\u5f81\u63d0\u53d6\u5668\uff0c\u5e76\u7ed3\u5408\u65f6\u7a7a\u591a\u56fe\u5377\u79ef\u7f51\u7edc\u548c\u591a\u5c42\u5507\u52a8\u878d\u5408\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u9762\u90e8landmark\u7684\u8f85\u52a9\u4fe1\u606f\u6765\u63d0\u9ad8\u89c6\u89c9\u8bed\u97f3\u8bc6\u522b\u7684\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u6570\u636e\u91cf\u6709\u9650\u7684\u60c5\u51b5\u4e0b\uff0c\u5e76\u4e14\u80fd\u591f\u63d0\u5347\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u8bf4\u8bdd\u4eba\u4e0a\u7684\u51c6\u786e\u7387\u3002"}}
{"id": "2508.07629", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07629", "abs": "https://arxiv.org/abs/2508.07629", "authors": ["Zhenpeng Su", "Leiyu Pan", "Xue Bai", "Dening Liu", "Guanting Dong", "Jiaming Huang", "Wenping Hu", "Guorui Zhou"], "title": "Klear-Reasoner: Advancing Reasoning Capability via Gradient-Preserving Clipping Policy Optimization", "comment": null, "summary": "We present Klear-Reasoner, a model with long reasoning capabilities that\ndemonstrates careful deliberation during problem solving, achieving outstanding\nperformance across multiple benchmarks. Although there are already many\nexcellent works related to inference models in the current community, there are\nstill many problems with reproducing high-performance inference models due to\nincomplete disclosure of training details. This report provides an in-depth\nanalysis of the reasoning model, covering the entire post-training workflow\nfrom data preparation and long Chain-of-Thought supervised fine-tuning (long\nCoT SFT) to reinforcement learning (RL), along with detailed ablation studies\nfor each experimental component. For SFT data, our experiments show that a\nsmall number of high-quality data sources are more effective than a large\nnumber of diverse data sources, and that difficult samples can achieve better\nresults without accuracy filtering. In addition, we investigate two key issues\nwith current clipping mechanisms in RL: Clipping suppresses critical\nexploration signals and ignores suboptimal trajectories. To address these\nchallenges, we propose Gradient-Preserving clipping Policy Optimization (GPPO)\nthat gently backpropagates gradients from clipped tokens. GPPO not only\nenhances the model's exploration capacity but also improves its efficiency in\nlearning from negative samples. Klear-Reasoner exhibits exceptional reasoning\nabilities in mathematics and programming, scoring 90.5\\% on AIME 2024, 83.2\\%\non AIME 2025, 66.0\\% on LiveCodeBench V5 and 58.1\\% on LiveCodeBench V6.", "AI": {"tldr": "Klear-Reasoner\u662f\u4e00\u4e2a\u957f\u63a8\u7406\u6a21\u578b\uff0c\u901a\u8fc7\u6539\u8fdbSFT\u6570\u636e\u9009\u62e9\u548c\u63d0\u51faGPPO\u7b97\u6cd5\uff0c\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u63a8\u7406\u6a21\u578b\u5728\u590d\u73b0\u9ad8\u6027\u80fd\u65b9\u9762\u5b58\u5728\u7684\u95ee\u9898\uff0c\u4f8b\u5982\u8bad\u7ec3\u7ec6\u8282\u62ab\u9732\u4e0d\u5b8c\u6574\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aKlear-Reasoner\u7684\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u5177\u6709\u957f\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u4e86\u4ece\u6570\u636e\u51c6\u5907\u3001\u957f\u94fe\u5f0f\u601d\u8003\u76d1\u7763\u5fae\u8c03\uff08long CoT SFT\uff09\u5230\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u7684\u6574\u4e2a\u8bad\u7ec3\u540e\u5de5\u4f5c\u6d41\u7a0b\u3002\u63d0\u51fa\u4e86\u68af\u5ea6\u4fdd\u7559\u88c1\u526a\u7b56\u7565\u4f18\u5316\uff08GPPO\uff09\u65b9\u6cd5\u6765\u89e3\u51b3RL\u4e2d\u7684\u88c1\u526a\u673a\u5236\u95ee\u9898\uff0c\u8be5\u65b9\u6cd5\u6e29\u548c\u5730\u53cd\u5411\u4f20\u64ad\u88c1\u526a\u4ee4\u724c\u7684\u68af\u5ea6\uff0c\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\u5e76\u63d0\u9ad8\u4e86\u4ece\u8d1f\u6837\u672c\u5b66\u4e60\u7684\u6548\u7387\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u9ad8\u8d28\u91cf\u7684SFT\u6570\u636e\u6e90\u6bd4\u5927\u91cf\u591a\u6837\u7684\u6570\u636e\u6e90\u66f4\u6709\u6548\uff0c\u5e76\u4e14\u56f0\u96be\u6837\u672c\u5728\u6ca1\u6709\u51c6\u786e\u6027\u8fc7\u6ee4\u7684\u60c5\u51b5\u4e0b\u53ef\u4ee5\u53d6\u5f97\u66f4\u597d\u7684\u7ed3\u679c\u3002GPPO\u589e\u5f3a\u4e86\u6a21\u578b\u7684\u63a2\u7d22\u80fd\u529b\uff0c\u5e76\u63d0\u9ad8\u4e86\u4ece\u8d1f\u6837\u672c\u5b66\u4e60\u7684\u6548\u7387\u3002", "conclusion": "Klear-Reasoner\u5728\u6570\u5b66\u548c\u7f16\u7a0b\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5728AIME 2024\u4e0a\u5f97\u520690.5%\uff0c\u5728AIME 2025\u4e0a\u5f97\u520683.2%\uff0c\u5728LiveCodeBench V5\u4e0a\u5f97\u520666.0%\uff0c\u5728LiveCodeBench V6\u4e0a\u5f97\u520658.1%\u3002"}}
{"id": "2508.08211", "categories": ["cs.CL", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08211", "abs": "https://arxiv.org/abs/2508.08211", "authors": ["Zhuohao Yu", "Xingru Jiang", "Weizheng Gu", "Yidong Wang", "Shikun Zhang", "Wei Ye"], "title": "SAEMark: Multi-bit LLM Watermarking with Inference-Time Scaling", "comment": "24 pages, 12 figures, code available:\n  https://zhuohaoyu.github.io/SAEMark", "summary": "Watermarking LLM-generated text is critical for content attribution and\nmisinformation prevention. However, existing methods compromise text quality,\nrequire white-box model access and logit manipulation. These limitations\nexclude API-based models and multilingual scenarios. We propose SAEMark, a\ngeneral framework for post-hoc multi-bit watermarking that embeds personalized\nmessages solely via inference-time, feature-based rejection sampling without\naltering model logits or requiring training. Our approach operates on\ndeterministic features extracted from generated text, selecting outputs whose\nfeature statistics align with key-derived targets. This framework naturally\ngeneralizes across languages and domains while preserving text quality through\nsampling LLM outputs instead of modifying. We provide theoretical guarantees\nrelating watermark success probability and compute budget that hold for any\nsuitable feature extractor. Empirically, we demonstrate the framework's\neffectiveness using Sparse Autoencoders (SAEs), achieving superior detection\naccuracy and text quality. Experiments across 4 datasets show SAEMark's\nconsistent performance, with 99.7% F1 on English and strong multi-bit detection\naccuracy. SAEMark establishes a new paradigm for scalable watermarking that\nworks out-of-the-box with closed-source LLMs while enabling content\nattribution.", "AI": {"tldr": "SAEMark \u662f\u4e00\u79cd\u65b0\u7684 LLM \u6c34\u5370\u6846\u67b6\uff0c\u53ef\u5728\u4e0d\u5f71\u54cd\u6587\u672c\u8d28\u91cf\u6216\u9700\u8981\u6a21\u578b\u8bbf\u95ee\u7684\u60c5\u51b5\u4e0b\uff0c\u901a\u8fc7\u4e8b\u540e\u62d2\u7edd\u91c7\u6837\u5d4c\u5165\u591a\u4f4d\u4fe1\u606f\uff0c\u4ece\u800c\u5b9e\u73b0\u5185\u5bb9\u5f52\u5c5e\u3002", "motivation": "LLM \u751f\u6210\u6587\u672c\u7684\u6c34\u5370\u5bf9\u4e8e\u5185\u5bb9\u5f52\u5c5e\u548c\u9519\u8bef\u4fe1\u606f\u9884\u9632\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u65b9\u6cd5\u4f1a\u5f71\u54cd\u6587\u672c\u8d28\u91cf\u3001\u9700\u8981\u767d\u76d2\u6a21\u578b\u8bbf\u95ee\u548c logits \u64cd\u4f5c\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u4e0d\u9002\u7528\u4e8e API \u6a21\u578b\u548c\u591a\u8bed\u8a00\u573a\u666f\u3002", "method": "SAEMark \u662f\u4e00\u79cd\u901a\u7528\u7684\u3001\u7528\u4e8e\u4e8b\u540e\u591a\u4f4d\u6c34\u5370\u7684\u6846\u67b6\uff0c\u4ec5\u901a\u8fc7\u63a8\u7406\u65f6\u3001\u57fa\u4e8e\u7279\u5f81\u7684\u62d2\u7edd\u91c7\u6837\u6765\u5b9e\u73b0\u4e2a\u6027\u5316\u6d88\u606f\u5d4c\u5165\uff0c\u800c\u4e0d\u6539\u53d8\u6a21\u578b logits \u6216\u9700\u8981\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u91c7\u6837 LLM \u8f93\u51fa\u800c\u4e0d\u662f\u4fee\u6539\u8f93\u51fa\u6765\u4fdd\u6301\u6587\u672c\u8d28\u91cf\uff0c\u5e76\u786e\u4fdd\u8de8\u8bed\u8a00\u548c\u57df\u7684\u901a\u7528\u6027\u3002", "result": "SAEMark \u5728\u82f1\u8bed\u4e0a\u7684 F1 \u5206\u6570\u4e3a 99.7%\uff0c\u5e76\u5b9e\u73b0\u4e86\u5f3a\u5927\u7684\u591a\u4f4d\u68c0\u6d4b\u7cbe\u5ea6\uff0c\u5728 4 \u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e SAEMark \u6027\u80fd\u4e00\u81f4\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u68c0\u6d4b\u7cbe\u5ea6\u548c\u6587\u672c\u8d28\u91cf\u3002", "conclusion": "SAEMark \u5efa\u7acb\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u3001\u5f00\u7bb1\u5373\u7528\u7684\u6c34\u5370\u65b0\u8303\u5f0f\uff0c\u9002\u7528\u4e8e\u95ed\u6e90 LLM\uff0c\u5e76\u652f\u6301\u5185\u5bb9\u5f52\u5c5e\u3002"}}
{"id": "2508.07237", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07237", "abs": "https://arxiv.org/abs/2508.07237", "authors": ["Bo Wang", "Mengyuan Xu", "Yue Yan", "Yuqun Yang", "Kechen Shu", "Wei Ping", "Xu Tang", "Wei Jiang", "Zheng You"], "title": "ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and Individual Variations for Fine-Grained Segmentation", "comment": null, "summary": "Precise lesion resection depends on accurately identifying fine-grained\nanatomical structures. While many coarse-grained segmentation (CGS) methods\nhave been successful in large-scale segmentation (e.g., organs), they fall\nshort in clinical scenarios requiring fine-grained segmentation (FGS), which\nremains challenging due to frequent individual variations in small-scale\nanatomical structures. Although recent Mamba-based models have advanced medical\nimage segmentation, they often rely on fixed manually-defined scanning orders,\nwhich limit their adaptability to individual variations in FGS. To address\nthis, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It\nintroduces adaptive scan scores to dynamically guide the scanning order,\ngenerated by combining group-level commonalities and individual-level\nvariations. Experiments on two public datasets (ACDC and Synapse) and a newly\nproposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that\nASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and\ndataset are available at https://github.com/YqunYang/ASM-UNet.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e Mamba \u7684 ASM-UNet \u67b6\u6784\uff0c\u7528\u4e8e\u7cbe\u7ec6\u89e3\u5256\u7ed3\u6784\u5206\u5272 (FGS)\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u626b\u63cf\u5206\u6570\u89e3\u51b3\u73b0\u6709\u6a21\u578b\u5bf9\u4e2a\u4f53\u5dee\u5f02\u9002\u5e94\u6027\u4e0d\u8db3\u7684\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e Mamba \u7684\u6a21\u578b\u5728 FGS \u4e2d\u4f9d\u8d56\u56fa\u5b9a\u7684\u624b\u52a8\u5b9a\u4e49\u7684\u626b\u63cf\u987a\u5e8f\uff0c\u9650\u5236\u4e86\u5b83\u4eec\u5bf9\u4e2a\u4f53\u5dee\u5f02\u7684\u9002\u5e94\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e Mamba \u7684 ASM-UNet \u67b6\u6784\uff0c\u7528\u4e8e FGS\uff0c\u5f15\u5165\u4e86\u81ea\u9002\u5e94\u626b\u63cf\u5206\u6570\u6765\u52a8\u6001\u6307\u5bfc\u626b\u63cf\u987a\u5e8f\uff0c\u7ed3\u5408\u4e86\u7ec4\u7ea7\u522b\u5171\u6027\u548c\u4e2a\u4f53\u7ea7\u522b\u5dee\u5f02\u3002", "result": "ASM-UNet \u5728 ACDC\u3001Synapse \u548c BTMS \u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684 FGS \u6027\u80fd\uff0c\u5e76\u5728 CGS \u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "ASM-UNet \u5728 CGS \u548c FGS \u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u5728 BTMS \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.07631", "categories": ["cs.LG", "cs.AI", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07631", "abs": "https://arxiv.org/abs/2508.07631", "authors": ["Advait Parulekar", "Litu Rout", "Karthikeyan Shanmugam", "Sanjay Shakkottai"], "title": "Efficient Approximate Posterior Sampling with Annealed Langevin Monte Carlo", "comment": null, "summary": "We study the problem of posterior sampling in the context of score based\ngenerative models. We have a trained score network for a prior $p(x)$, a\nmeasurement model $p(y|x)$, and are tasked with sampling from the posterior\n$p(x|y)$. Prior work has shown this to be intractable in KL (in the worst case)\nunder well-accepted computational hardness assumptions. Despite this, popular\nalgorithms for tasks such as image super-resolution, stylization, and\nreconstruction enjoy empirical success. Rather than establishing distributional\nassumptions or restricted settings under which exact posterior sampling is\ntractable, we view this as a more general \"tilting\" problem of biasing a\ndistribution towards a measurement. Under minimal assumptions, we show that one\ncan tractably sample from a distribution that is simultaneously close to the\nposterior of a noised prior in KL divergence and the true posterior in Fisher\ndivergence. Intuitively, this combination ensures that the resulting sample is\nconsistent with both the measurement and the prior. To the best of our\nknowledge these are the first formal results for (approximate) posterior\nsampling in polynomial time.", "AI": {"tldr": "\u5f97\u5206\u57fa\u751f\u6210\u6a21\u578b\u5728\u540e\u9a8c\u91c7\u6837\uff08\u4ece p(x|y) \u91c7\u6837\uff09\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u8fd9\u5728 KL \u6563\u5ea6\u4e0b\u901a\u5e38\u662f\u4e0d\u53ef\u884c\u7684\u3002\u672c\u7814\u7a76\u5c06\u6b64\u95ee\u9898\u89c6\u4e3a\u201c\u503e\u659c\u201d\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u5728 KL \u6563\u5ea6\u548c Fisher \u6563\u5ea6\u4e0b\u90fd\u63a5\u8fd1\u76ee\u6807\u540e\u9a8c\u5206\u5e03\u7684\u91c7\u6837\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u662f\u53ef\u884c\u7684\u3002", "motivation": "\u5728\u5f97\u5206\u57fa\u751f\u6210\u6a21\u578b\u4e2d\uff0c\u5373\u4f7f\u5728\u6700\u574f\u60c5\u51b5\u4e0b\uff0c\u4ece\u540e\u9a8c\u5206\u5e03 p(x|y) \u8fdb\u884c\u91c7\u6837\u5728 KL \u6563\u5ea6\u4e0b\u4e5f\u662f\u4e0d\u53ef\u884c\u7684\u3002\u7136\u800c\uff0c\u50cf\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3001\u98ce\u683c\u5316\u548c\u91cd\u5efa\u7b49\u4efb\u52a1\u4e2d\u7684\u6d41\u884c\u7b97\u6cd5\u53d6\u5f97\u4e86\u7ecf\u9a8c\u4e0a\u7684\u6210\u529f\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u79cd\u4e0d\u4e00\u81f4\u6027\u3002", "method": "\u5c06\u540e\u9a8c\u91c7\u6837\u89c6\u4e3a\u4e00\u4e2a\u201c\u503e\u659c\u201d\u95ee\u9898\uff0c\u5373\u504f\u7f6e\u4e00\u4e2a\u5206\u5e03\u4f7f\u5176\u503e\u5411\u4e8e\u4e00\u4e2a\u6d4b\u91cf\u6a21\u578b\u3002\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\uff0c\u8bc1\u660e\u4e86\u53ef\u4ee5\u6709\u6548\u5730\u4ece\u4e00\u4e2a\u540c\u65f6\u5728 KL \u6563\u5ea6\u4e0b\u63a5\u8fd1\u5e26\u566a\u58f0\u5148\u9a8c\u540e\u9a8c\u5206\u5e03\u4e14\u5728 Fisher \u6563\u5ea6\u4e0b\u63a5\u8fd1\u771f\u5b9e\u540e\u9a8c\u5206\u5e03\u7684\u5206\u5e03\u4e2d\u8fdb\u884c\u91c7\u6837\u3002", "result": "\u672c\u7814\u7a76\u63d0\u4f9b\u4e86\u9996\u4e2a\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\uff08\u8fd1\u4f3c\uff09\u540e\u9a8c\u91c7\u6837\u7684\u5f62\u5f0f\u5316\u7ed3\u679c\uff0c\u8be5\u65b9\u6cd5\u7ed3\u5408\u4e86 KL \u6563\u5ea6\u548c Fisher \u6563\u5ea6\uff0c\u4ee5\u786e\u4fdd\u6837\u672c\u540c\u65f6\u4e0e\u6d4b\u91cf\u6a21\u578b\u548c\u5148\u9a8c\u4fdd\u6301\u4e00\u81f4\u3002", "conclusion": "\u672c\u7814\u7a76\u9996\u6b21\u5728\u591a\u9879\u5f0f\u65f6\u95f4\u5185\u5bf9\uff08\u8fd1\u4f3c\uff09\u540e\u9a8c\u91c7\u6837\u8fdb\u884c\u4e86\u5f62\u5f0f\u5316\u8bc1\u660e\uff0c\u5e76\u5c55\u793a\u4e86\u5982\u4f55\u5728\u6700\u5c0f\u5047\u8bbe\u4e0b\u4ece\u4e00\u4e2a KL \u6563\u5ea6\u4e0b\u63a5\u8fd1\u5e26\u566a\u58f0\u5148\u9a8c\u540e\u9a8c\u5206\u5e03\u4e14 Fisher \u6563\u5ea6\u4e0b\u63a5\u8fd1\u771f\u5b9e\u540e\u9a8c\u5206\u5e03\u7684\u5206\u5e03\u4e2d\u8fdb\u884c\u91c7\u6837\u3002"}}
{"id": "2508.08224", "categories": ["cs.CL", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08224", "abs": "https://arxiv.org/abs/2508.08224", "authors": ["Shansong Wang", "Mingzhe Hu", "Qiang Li", "Mojtaba Safari", "Xiaofeng Yang"], "title": "Capabilities of GPT-5 on Multimodal Medical Reasoning", "comment": null, "summary": "Recent advances in large language models (LLMs) have enabled general-purpose\nsystems to perform increasingly complex domain-specific reasoning without\nextensive fine-tuning. In the medical domain, decision-making often requires\nintegrating heterogeneous information sources, including patient narratives,\nstructured data, and medical images. This study positions GPT-5 as a generalist\nmultimodal reasoner for medical decision support and systematically evaluates\nits zero-shot chain-of-thought reasoning performance on both text-based\nquestion answering and visual question answering tasks under a unified\nprotocol. We benchmark GPT-5, GPT-5-mini, GPT-5-nano, and GPT-4o-2024-11-20\nagainst standardized splits of MedQA, MedXpertQA (text and multimodal), MMLU\nmedical subsets, USMLE self-assessment exams, and VQA-RAD. Results show that\nGPT-5 consistently outperforms all baselines, achieving state-of-the-art\naccuracy across all QA benchmarks and delivering substantial gains in\nmultimodal reasoning. On MedXpertQA MM, GPT-5 improves reasoning and\nunderstanding scores by +29.62% and +36.18% over GPT-4o, respectively, and\nsurpasses pre-licensed human experts by +24.23% in reasoning and +29.40% in\nunderstanding. In contrast, GPT-4o remains below human expert performance in\nmost dimensions. A representative case study demonstrates GPT-5's ability to\nintegrate visual and textual cues into a coherent diagnostic reasoning chain,\nrecommending appropriate high-stakes interventions. Our results show that, on\nthese controlled multimodal reasoning benchmarks, GPT-5 moves from\nhuman-comparable to above human-expert performance. This improvement may\nsubstantially inform the design of future clinical decision-support systems.", "AI": {"tldr": "GPT-5\u5728\u533b\u5b66\u591a\u6a21\u6001\u63a8\u7406\u4efb\u52a1\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u8d85\u8d8a\u4e86GPT-4o\u548c\u4eba\u7c7b\u4e13\u5bb6\u3002", "motivation": "\u4e3a\u4e86\u8bc4\u4f30\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728\u533b\u5b66\u9886\u57df\u8fdb\u884c\u590d\u6742\u63a8\u7406\u548c\u6574\u5408\u5f02\u6784\u4fe1\u606f\u6e90\uff08\u5982\u60a3\u8005\u53d9\u8ff0\u3001\u7ed3\u6784\u5316\u6570\u636e\u548c\u533b\u5b66\u56fe\u50cf\uff09\u4ee5\u652f\u6301\u51b3\u7b56\u7684\u80fd\u529b\u3002", "method": "\u672c\u7814\u7a76\u5c06GPT-5\u3001GPT-5-mini\u3001GPT-5-nano\u548cGPT-4o-2024-11-20\u4f5c\u4e3a\u901a\u7528\u591a\u6a21\u6001\u63a8\u7406\u5668\uff0c\u5728\u6587\u672c\u95ee\u7b54\u548c\u89c6\u89c9\u95ee\u7b54\u4efb\u52a1\u4e0a\uff0c\u4f7f\u7528\u7edf\u4e00\u534f\u8bae\u7cfb\u7edf\u6027\u5730\u8bc4\u4f30\u4e86\u5b83\u4eec\u5728\u96f6\u6837\u672c\u94fe\u5f0f\u601d\u8003\u65b9\u9762\u7684\u8868\u73b0\u3002", "result": "GPT-5\u5728\u6240\u6709\u95ee\u7b54\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u6301\u7eed\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u6a21\u578b\uff0c\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u51c6\u786e\u7387\uff0c\u5e76\u5728\u591a\u6a21\u6001\u63a8\u7406\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002\u5177\u4f53\u800c\u8a00\uff0c\u5728MedXpertQA MM\u4efb\u52a1\u4e0a\uff0cGPT-5\u7684\u63a8\u7406\u548c\u7406\u89e3\u5f97\u5206\u5206\u522b\u6bd4GPT-4o\u9ad8\u51fa29.62%\u548c36.18%\uff0c\u5e76\u4e14\u5728\u63a8\u7406\u548c\u7406\u89e3\u65b9\u9762\u5206\u522b\u8d85\u8d8a\u4e86\u9884\u6388\u6743\u7684\u4eba\u7c7b\u4e13\u5bb624.23%\u548c29.40%\u3002GPT-4o\u5728\u5927\u591a\u6570\u7ef4\u5ea6\u4e0a\u4ecd\u4f4e\u4e8e\u4eba\u7c7b\u4e13\u5bb6\u7684\u8868\u73b0\u3002", "conclusion": "GPT-5\u5728\u533b\u5b66\u9886\u57df\u7684\u591a\u6a21\u6001\u63a8\u7406\u80fd\u529b\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\u6c34\u5e73\uff0c\u4e3a\u672a\u6765\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u8bbe\u8ba1\u63d0\u4f9b\u4e86\u91cd\u8981\u53c2\u8003\u3002"}}
{"id": "2508.07246", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07246", "abs": "https://arxiv.org/abs/2508.07246", "authors": ["Xin Ma", "Yaohui Wang", "Genyun Jia", "Xinyuan Chen", "Tien-Tsin Wong", "Cunjian Chen"], "title": "Consistent and Controllable Image Animation with Motion Linear Diffusion Transformers", "comment": "Project Page: https://maxin-cn.github.io/miramo_project", "summary": "Image animation has seen significant progress, driven by the powerful\ngenerative capabilities of diffusion models. However, maintaining appearance\nconsistency with static input images and mitigating abrupt motion transitions\nin generated animations remain persistent challenges. While text-to-video (T2V)\ngeneration has demonstrated impressive performance with diffusion transformer\nmodels, the image animation field still largely relies on U-Net-based diffusion\nmodels, which lag behind the latest T2V approaches. Moreover, the quadratic\ncomplexity of vanilla self-attention mechanisms in Transformers imposes heavy\ncomputational demands, making image animation particularly resource-intensive.\nTo address these issues, we propose MiraMo, a framework designed to enhance\nefficiency, appearance consistency, and motion smoothness in image animation.\nSpecifically, MiraMo introduces three key elements: (1) A foundational\ntext-to-video architecture replacing vanilla self-attention with efficient\nlinear attention to reduce computational overhead while preserving generation\nquality; (2) A novel motion residual learning paradigm that focuses on modeling\nmotion dynamics rather than directly predicting frames, improving temporal\nconsistency; and (3) A DCT-based noise refinement strategy during inference to\nsuppress sudden motion artifacts, complemented by a dynamics control module to\nbalance motion smoothness and expressiveness. Extensive experiments against\nstate-of-the-art methods validate the superiority of MiraMo in generating\nconsistent, smooth, and controllable animations with accelerated inference\nspeed. Additionally, we demonstrate the versatility of MiraMo through\napplications in motion transfer and video editing tasks.", "AI": {"tldr": "MiraMo\u901a\u8fc7\u91c7\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u3001\u8fd0\u52a8\u6b8b\u5dee\u5b66\u4e60\u548cDCT\u53bb\u566a\u7b56\u7565\u6765\u63d0\u9ad8\u56fe\u50cf\u52a8\u753b\u7684\u6548\u7387\u3001\u5916\u89c2\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u5e73\u6ed1\u5ea6\uff0c\u5e76\u5728\u8fd0\u52a8\u8fc1\u79fb\u548c\u89c6\u9891\u7f16\u8f91\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u56fe\u50cf\u52a8\u753b\u9886\u57df\u4ecd\u7136\u4f9d\u8d56\u57fa\u4e8eU-Net\u7684\u6269\u6563\u6a21\u578b\uff0c\u8fd9\u4e9b\u6a21\u578b\u843d\u540e\u4e8e\u6700\u65b0\u7684\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u65b9\u6cd5\u3002\u6b64\u5916\uff0cTransformer\u4e2d\u6807\u51c6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\u7684\u4e8c\u6b21\u590d\u6742\u5ea6\u5e26\u6765\u4e86\u9ad8\u6602\u7684\u8ba1\u7b97\u9700\u6c42\uff0c\u4f7f\u5f97\u56fe\u50cf\u52a8\u753b\u7279\u522b\u6d88\u8017\u8d44\u6e90\u3002", "method": "MiraMo\u6846\u67b6\u5f15\u5165\u4e86\u4e09\u4e2a\u5173\u952e\u8981\u7d20\uff1a1.\u4e00\u4e2a\u57fa\u7840\u7684\u6587\u672c\u5230\u89c6\u9891\u67b6\u6784\uff0c\u7528\u9ad8\u6548\u7684\u7ebf\u6027\u6ce8\u610f\u529b\u53d6\u4ee3\u4e86\u6807\u51c6\u7684\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff0c\u4ee5\u964d\u4f4e\u8ba1\u7b97\u5f00\u9500\u5e76\u4fdd\u6301\u751f\u6210\u8d28\u91cf\uff1b2.\u4e00\u79cd\u65b0\u9896\u7684\u8fd0\u52a8\u6b8b\u5dee\u5b66\u4e60\u8303\u5f0f\uff0c\u4fa7\u91cd\u4e8e\u6a21\u62df\u8fd0\u52a8\u52a8\u6001\u800c\u4e0d\u662f\u76f4\u63a5\u9884\u6d4b\u5e27\uff0c\u4ee5\u63d0\u9ad8\u65f6\u95f4\u4e00\u81f4\u6027\uff1b3.\u4e00\u79cd\u57fa\u4e8eDCT\u7684\u63a8\u7406\u53bb\u566a\u7b56\u7565\uff0c\u7528\u4e8e\u6291\u5236\u7a81\u7136\u7684\u8fd0\u52a8\u4f2a\u5f71\uff0c\u5e76\u8f85\u4ee5\u4e00\u4e2a\u52a8\u6001\u63a7\u5236\u6a21\u5757\u6765\u5e73\u8861\u8fd0\u52a8\u5e73\u6ed1\u5ea6\u548c\u8868\u73b0\u529b\u3002", "result": "MiraMo\u5728\u4fdd\u6301\u9759\u6001\u8f93\u5165\u56fe\u50cf\u7684\u5916\u89c2\u4e00\u81f4\u6027\u5e76\u51cf\u8f7b\u751f\u6210\u52a8\u753b\u4e2d\u7a81\u53d8\u8fd0\u52a8\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u540c\u65f6\u63d0\u9ad8\u4e86\u6548\u7387\u3001\u5916\u89c2\u4e00\u81f4\u6027\u548c\u8fd0\u52a8\u5e73\u6ed1\u5ea6\u3002", "conclusion": "MiraMo\u5728\u751f\u6210\u4e00\u81f4\u3001\u5e73\u6ed1\u3001\u53ef\u63a7\u7684\u52a8\u753b\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\uff0c\u5e76\u52a0\u5feb\u4e86\u63a8\u7406\u901f\u5ea6\u3002\u6b64\u5916\uff0cMiraMo\u5728\u8fd0\u52a8\u8fc1\u79fb\u548c\u89c6\u9891\u7f16\u8f91\u4efb\u52a1\u4e2d\u4e5f\u5c55\u73b0\u4e86\u5176\u591a\u529f\u80fd\u6027\u3002"}}
{"id": "2508.07636", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07636", "abs": "https://arxiv.org/abs/2508.07636", "authors": ["Huiqi Deng", "Hongbin Pei", "Quanshi Zhang", "Mengnan Du"], "title": "Attribution Explanations for Deep Neural Networks: A Theoretical Perspective", "comment": null, "summary": "Attribution explanation is a typical approach for explaining deep neural\nnetworks (DNNs), inferring an importance or contribution score for each input\nvariable to the final output. In recent years, numerous attribution methods\nhave been developed to explain DNNs. However, a persistent concern remains\nunresolved, i.e., whether and which attribution methods faithfully reflect the\nactual contribution of input variables to the decision-making process. The\nfaithfulness issue undermines the reliability and practical utility of\nattribution explanations. We argue that these concerns stem from three core\nchallenges. First, difficulties arise in comparing attribution methods due to\ntheir unstructured heterogeneity, differences in heuristics, formulations, and\nimplementations that lack a unified organization. Second, most methods lack\nsolid theoretical underpinnings, with their rationales remaining absent,\nambiguous, or unverified. Third, empirically evaluating faithfulness is\nchallenging without ground truth. Recent theoretical advances provide a\npromising way to tackle these challenges, attracting increasing attention. We\nsummarize these developments, with emphasis on three key directions: (i)\nTheoretical unification, which uncovers commonalities and differences among\nmethods, enabling systematic comparisons; (ii) Theoretical rationale,\nclarifying the foundations of existing methods; (iii) Theoretical evaluation,\nrigorously proving whether methods satisfy faithfulness principles. Beyond a\ncomprehensive review, we provide insights into how these studies help deepen\ntheoretical understanding, inform method selection, and inspire new attribution\nmethods. We conclude with a discussion of promising open problems for further\nwork.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63a2\u8ba8\u4e86\u5f53\u524d\u7528\u4e8e\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u5f52\u56e0\u65b9\u6cd5\u6240\u9762\u4e34\u7684\u201c\u5fe0\u5b9e\u6027\u201d\u6311\u6218\uff0c\u5e76\u91cd\u70b9\u4ecb\u7ecd\u4e86\u901a\u8fc7\u7406\u8bba\u7edf\u4e00\u3001\u7406\u8bba\u4f9d\u636e\u548c\u7406\u8bba\u8bc4\u4f30\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u6700\u65b0\u8fdb\u5c55\u3002", "motivation": "\u5f53\u524d\u8bb8\u591a\u5f52\u56e0\u65b9\u6cd5\u5728\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u65b9\u9762\u5b58\u5728\u5fe0\u5b9e\u6027\u95ee\u9898\uff0c\u5373\u5b83\u4eec\u662f\u5426\u51c6\u786e\u53cd\u6620\u4e86\u8f93\u5165\u53d8\u91cf\u5bf9\u6700\u7ec8\u8f93\u51fa\u7684\u5b9e\u9645\u8d21\u732e\u5c1a\u4e0d\u660e\u786e\uff0c\u8fd9\u5f71\u54cd\u4e86\u5f52\u56e0\u89e3\u91ca\u7684\u53ef\u9760\u6027\u548c\u5b9e\u7528\u6027\u3002", "method": "\u672c\u6587\u5bf9\u7528\u4e8e\u89e3\u91ca\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u5f52\u56e0\u65b9\u6cd5\u8fdb\u884c\u4e86\u7efc\u8ff0\uff0c\u91cd\u70b9\u5173\u6ce8\u89e3\u51b3\u201c\u5fe0\u5b9e\u6027\u201d\u95ee\u9898\u7684\u7406\u8bba\u8fdb\u5c55\u3002\u6587\u7ae0\u8ba8\u8bba\u4e86\u4e09\u4e2a\u5173\u952e\u65b9\u5411\uff1a\u7406\u8bba\u7edf\u4e00\u3001\u7406\u8bba\u4f9d\u636e\u548c\u7406\u8bba\u8bc4\u4f30\u3002", "result": "\u8be5\u7efc\u8ff0\u603b\u7ed3\u4e86\u8fd1\u671f\u5728\u7406\u8bba\u7edf\u4e00\u3001\u7406\u8bba\u4f9d\u636e\u548c\u7406\u8bba\u8bc4\u4f30\u65b9\u9762\u4e3a\u89e3\u51b3\u5f52\u56e0\u65b9\u6cd5\u5fe0\u5b9e\u6027\u95ee\u9898\u6240\u505a\u7684\u52aa\u529b\uff0c\u5e76\u8ba8\u8bba\u4e86\u8fd9\u4e9b\u7814\u7a76\u5982\u4f55\u6df1\u5316\u7406\u8bba\u7406\u89e3\u3001\u6307\u5bfc\u65b9\u6cd5\u9009\u62e9\u4ee5\u53ca\u542f\u53d1\u65b0\u65b9\u6cd5\u7684\u5f00\u53d1\u3002", "conclusion": "\u968f\u7740\u7814\u7a76\u7684\u6df1\u5165\uff0c\u7406\u8bba\u7edf\u4e00\u3001\u7406\u8bba\u4f9d\u636e\u548c\u7406\u8bba\u8bc4\u4f30\u4e3a\u89e3\u51b3\u5f52\u56e0\u65b9\u6cd5\u7684\u5fe0\u5b9e\u6027\u95ee\u9898\u63d0\u4f9b\u4e86\u6709\u529b\u7684\u652f\u6301\uff0c\u52a0\u6df1\u4e86\u5bf9\u8fd9\u4e9b\u65b9\u6cd5\u7684\u7406\u8bba\u7406\u89e3\uff0c\u5e76\u6307\u5bfc\u4e86\u65b0\u65b9\u6cd5\u7684\u5f00\u53d1\u3002"}}
{"id": "2508.08236", "categories": ["cs.CL", "cs.CY"], "pdf": "https://arxiv.org/pdf/2508.08236", "abs": "https://arxiv.org/abs/2508.08236", "authors": ["Yunna Cai", "Fan Wang", "Haowei Wang", "Kun Wang", "Kailai Yang", "Sophia Ananiadou", "Moyan Li", "Mingming Fan"], "title": "Exploring Safety Alignment Evaluation of LLMs in Chinese Mental Health Dialogues via LLM-as-Judge", "comment": null, "summary": "Evaluating the safety alignment of LLM responses in high-risk mental health\ndialogues is particularly difficult due to missing gold-standard answers and\nthe ethically sensitive nature of these interactions. To address this\nchallenge, we propose PsyCrisis-Bench, a reference-free evaluation benchmark\nbased on real-world Chinese mental health dialogues. It evaluates whether the\nmodel responses align with the safety principles defined by experts.\nSpecifically designed for settings without standard references, our method\nadopts a prompt-based LLM-as-Judge approach that conducts in-context evaluation\nusing expert-defined reasoning chains grounded in psychological intervention\nprinciples. We employ binary point-wise scoring across multiple safety\ndimensions to enhance the explainability and traceability of the evaluation.\nAdditionally, we present a manually curated, high-quality Chinese-language\ndataset covering self-harm, suicidal ideation, and existential distress,\nderived from real-world online discourse. Experiments on 3600 judgments show\nthat our method achieves the highest agreement with expert assessments and\nproduces more interpretable evaluation rationales compared to existing\napproaches. Our dataset and evaluation tool are publicly available to\nfacilitate further research.", "AI": {"tldr": "PsyCrisis-Bench\u662f\u4e00\u4e2a\u65b0\u7684\u4e2d\u6587\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u5b89\u5168\u8bc4\u4f30\u57fa\u51c6\uff0c\u4f7f\u7528LLM-as-Judge\u65b9\u6cd5\u548c\u4e13\u5bb6\u5b9a\u4e49\u7684\u63a8\u7406\u94fe\u8fdb\u884c\u8bc4\u4f30\uff0c\u65e0\u9700\u53c2\u8003\u7b54\u6848\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u8bc4\u4f30LLM\u5728\u7f51\u7edc\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u4e2d\u7684\u5b89\u5168\u5bf9\u9f50\u662f\u56f0\u96be\u7684\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u9ec4\u91d1\u6807\u51c6\u7b54\u6848\u4e14\u8fd9\u4e9b\u4ea4\u4e92\u6d89\u53ca\u4f26\u7406\u654f\u611f\u6027\u3002", "method": "\u63d0\u51faPsyCrisis-Bench\uff0c\u4e00\u4e2a\u57fa\u4e8e\u771f\u5b9e\u4e16\u754c\u4e2d\u6587\u5fc3\u7406\u5065\u5eb7\u5bf9\u8bdd\u7684\u65e0\u53c2\u8003\u8bc4\u4f30\u57fa\u51c6\u3002\u8be5\u65b9\u6cd5\u91c7\u7528\u57fa\u4e8e\u63d0\u793a\u7684LLM-as-Judge\u65b9\u6cd5\uff0c\u4f7f\u7528\u4e13\u5bb6\u5b9a\u4e49\u7684\u3001\u57fa\u4e8e\u5fc3\u7406\u5e72\u9884\u539f\u5219\u7684\u63a8\u7406\u94fe\u8fdb\u884c\u4e0a\u4e0b\u6587\u8bc4\u4f30\uff0c\u5e76\u901a\u8fc7\u591a\u4e2a\u5b89\u5168\u7ef4\u5ea6\u8fdb\u884c\u4e8c\u5143\u70b9\u72b6\u8bc4\u5206\uff0c\u4ee5\u589e\u5f3a\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u57283600\u6b21\u5224\u65ad\u4e2d\uff0c\u4e0e\u4e13\u5bb6\u8bc4\u4f30\u7684\u4e00\u81f4\u6027\u6700\u9ad8\uff0c\u5e76\u80fd\u4ea7\u751f\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u8bc4\u4f30\u7406\u7531\u3002", "conclusion": "PsyCrisis-Bench \u5728\u4e13\u5bb6\u8bc4\u4f30\u65b9\u9762\u5b9e\u73b0\u4e86\u6700\u9ad8\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u80fd\u4ea7\u751f\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u5177\u53ef\u89e3\u91ca\u6027\u7684\u8bc4\u4f30\u7406\u7531\u3002\u8be5\u6570\u636e\u96c6\u548c\u8bc4\u4f30\u5de5\u5177\u53ef\u516c\u5f00\u83b7\u53d6\uff0c\u4ee5\u4fc3\u8fdb\u672a\u6765\u7684\u7814\u7a76\u3002"}}
{"id": "2508.07250", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07250", "abs": "https://arxiv.org/abs/2508.07250", "authors": ["Fengchao Xiong", "Zhenxing Wu", "Sen Jia", "Yuntao Qian"], "title": "SUIT: Spatial-Spectral Union-Intersection Interaction Network for Hyperspectral Object Tracking", "comment": null, "summary": "Hyperspectral videos (HSVs), with their inherent spatial-spectral-temporal\nstructure, offer distinct advantages in challenging tracking scenarios such as\ncluttered backgrounds and small objects. However, existing methods primarily\nfocus on spatial interactions between the template and search regions, often\noverlooking spectral interactions, leading to suboptimal performance. To\naddress this issue, this paper investigates spectral interactions from both the\narchitectural and training perspectives. At the architectural level, we first\nestablish band-wise long-range spatial relationships between the template and\nsearch regions using Transformers. We then model spectral interactions using\nthe inclusion-exclusion principle from set theory, treating them as the union\nof spatial interactions across all bands. This enables the effective\nintegration of both shared and band-specific spatial cues. At the training\nlevel, we introduce a spectral loss to enforce material distribution alignment\nbetween the template and predicted regions, enhancing robustness to shape\ndeformation and appearance variations. Extensive experiments demonstrate that\nour tracker achieves state-of-the-art tracking performance. The source code,\ntrained models and results will be publicly available via\nhttps://github.com/bearshng/suit to support reproducibility.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u9896\u7684\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408\u5149\u8c31\u4fe1\u606f\u548c\u6539\u8fdb\u7684\u8bad\u7ec3\u7b56\u7565\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u590d\u6742\u573a\u666f\u4e0b\u7684\u8ffd\u8e2a\u7cbe\u5ea6\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7a7a\u95f4\u4ea4\u4e92\uff0c\u5ffd\u7565\u4e86\u5149\u8c31\u4ea4\u4e92\uff0c\u5bfc\u81f4\u8ffd\u8e2a\u6027\u80fd\u4e0d\u4f73\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86Transformer\u548c\u96c6\u5408\u8bba\u4e2d\u7684\u5305\u542b-\u6392\u9664\u539f\u5219\u7684\u8ffd\u8e2a\u65b9\u6cd5\uff0c\u7528\u4e8e\u5efa\u6a21\u957f\u8ddd\u79bb\u7a7a\u95f4\u5173\u7cfb\u548c\u5149\u8c31\u4ea4\u4e92\u3002\u5728\u8bad\u7ec3\u5c42\u9762\uff0c\u5f15\u5165\u4e86\u5149\u8c31\u635f\u5931\u6765\u5f3a\u5236\u5bf9\u9f50\u6750\u6599\u5206\u5e03\uff0c\u589e\u5f3a\u4e86\u5bf9\u5f62\u72b6\u53d8\u5f62\u548c\u5916\u89c2\u53d8\u5316\u7684\u9c81\u68d2\u6027\u3002", "result": "\u901a\u8fc7\u5e7f\u6cdb\u7684\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u8ffd\u8e2a\u5668\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u8ffd\u8e2a\u573a\u666f\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ffd\u8e2a\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u8ffd\u8e2a\u5668\u5728\u514b\u670d\u6742\u4e71\u80cc\u666f\u548c\u5c0f\u7269\u4f53\u7b49\u6311\u6218\u6027\u8ffd\u8e2a\u573a\u666f\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u8ffd\u8e2a\u6027\u80fd\u3002"}}
{"id": "2508.07637", "categories": ["cs.LG", "cs.CG"], "pdf": "https://arxiv.org/pdf/2508.07637", "abs": "https://arxiv.org/abs/2508.07637", "authors": ["Guanqun Ma", "David Lenz", "Hanqi Guo", "Tom Peterka", "Bei Wang"], "title": "Extracting Complex Topology from Multivariate Functional Approximation: Contours, Jacobi Sets, and Ridge-Valley Graphs", "comment": "The paper is to be published at the 15th IEEE Workshop on Large Data\n  Analysis and Visualization (LDAV)", "summary": "Implicit continuous models, such as functional models and implicit neural\nnetworks, are an increasingly popular method for replacing discrete data\nrepresentations with continuous, high-order, and differentiable surrogates.\nThese models offer new perspectives on the storage, transfer, and analysis of\nscientific data. In this paper, we introduce the first framework to directly\nextract complex topological features -- contours, Jacobi sets, and ridge-valley\ngraphs -- from a type of continuous implicit model known as multivariate\nfunctional approximation (MFA). MFA replaces discrete data with continuous\npiecewise smooth functions. Given an MFA model as the input, our approach\nenables direct extraction of complex topological features from the model,\nwithout reverting to a discrete representation of the model. Our work is easily\ngeneralizable to any continuous implicit model that supports the queries of\nfunction values and high-order derivatives. Our work establishes the building\nblocks for performing topological data analysis and visualization on implicit\ncontinuous models.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u8fde\u7eed\u9690\u5f0f\u6a21\u578b\uff08MFA\uff09\u4e2d\u63d0\u53d6\u62d3\u6251\u7279\u5f81\uff08\u8f6e\u5ed3\u3001\u96c5\u53ef\u6bd4\u96c6\u3001\u810a\u8c37\u56fe\uff09\u7684\u65b0\u6846\u67b6\uff0c\u65e0\u9700\u79bb\u6563\u5316\uff0c\u4e3a\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\u5f00\u8f9f\u4e86\u65b0\u9014\u5f84\u3002", "motivation": "\u5c06\u79bb\u6563\u6570\u636e\u8868\u793a\u66ff\u6362\u4e3a\u8fde\u7eed\u3001\u9ad8\u9636\u3001\u53ef\u5fae\u7684\u66ff\u4ee3\u6a21\u578b\uff0c\u4ee5\u5b58\u50a8\u3001\u4f20\u8f93\u548c\u5206\u6790\u79d1\u5b66\u6570\u636e\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u76f4\u63a5\u4ece\u591a\u53d8\u91cf\u51fd\u6570\u903c\u8fd1\uff08MFA\uff09\u6a21\u578b\u4e2d\u63d0\u53d6\u590d\u6742\u62d3\u6251\u7279\u5f81\uff08\u8f6e\u5ed3\u3001\u96c5\u53ef\u6bd4\u96c6\u3001\u810a\u8c37\u56fe\uff09\u7684\u6846\u67b6\uff0c\u65e0\u9700\u8f6c\u56de\u79bb\u6563\u8868\u793a\u3002", "result": "\u9996\u6b21\u5b9e\u73b0\u4e86\u4eceMFA\u6a21\u578b\u76f4\u63a5\u63d0\u53d6\u590d\u6742\u62d3\u6251\u7279\u5f81\uff0c\u5e76\u4e14\u8be5\u65b9\u6cd5\u6613\u4e8e\u6cdb\u5316\u5230\u652f\u6301\u51fd\u6570\u503c\u548c\u9ad8\u9636\u5bfc\u6570\u67e5\u8be2\u7684\u4efb\u4f55\u8fde\u7eed\u9690\u5f0f\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3a\u62d3\u6251\u6570\u636e\u5206\u6790\u548c\u53ef\u89c6\u5316\u5728\u9690\u5f0f\u8fde\u7eed\u6a21\u578b\u4e0a\u7684\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.08243", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08243", "abs": "https://arxiv.org/abs/2508.08243", "authors": ["Jiahao Zhao", "Liwei Dong"], "title": "Jinx: Unlimited LLMs for Probing Alignment Failures", "comment": "https://huggingface.co/Jinx-org", "summary": "Unlimited, or so-called helpful-only language models are trained without\nsafety alignment constraints and never refuse user queries. They are widely\nused by leading AI companies as internal tools for red teaming and alignment\nevaluation. For example, if a safety-aligned model produces harmful outputs\nsimilar to an unlimited model, this indicates alignment failures that require\nfurther attention. Despite their essential role in assessing alignment, such\nmodels are not available to the research community.\n  We introduce Jinx, a helpful-only variant of popular open-weight LLMs. Jinx\nresponds to all queries without refusals or safety filtering, while preserving\nthe base model's capabilities in reasoning and instruction following. It\nprovides researchers with an accessible tool for probing alignment failures,\nevaluating safety boundaries, and systematically studying failure modes in\nlanguage model safety.", "AI": {"tldr": "Jinx \u662f\u4e00\u4e2a\u65e0\u9650\u5236\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u53ef\u7528\u4e8e\u7814\u7a76\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u95ee\u9898\u3002", "motivation": "\u9700\u8981\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e00\u4e2a\u65e0\u9650\u5236\u7684\u8bed\u8a00\u6a21\u578b\uff0c\u4ee5\u7528\u4e8e\u7ea2\u961f\u6d4b\u8bd5\u3001\u5bf9\u9f50\u8bc4\u4f30\u548c\u7814\u7a76\u5b89\u5168\u6545\u969c\u6a21\u5f0f\u3002", "method": "Jinx \u662f\u901a\u8fc7\u79fb\u9664\u5b89\u5168\u5bf9\u9f50\u7ea6\u675f\u5e76\u4fdd\u7559\u57fa\u7840\u6a21\u578b\u80fd\u529b\u6765\u521b\u5efa\u7684\u3002", "result": "Jinx \u80fd\u591f\u54cd\u5e94\u6240\u6709\u67e5\u8be2\uff0c\u6ca1\u6709\u62d2\u7edd\u6216\u5b89\u5168\u8fc7\u6ee4\uff0c\u540c\u65f6\u4fdd\u7559\u4e86\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u548c\u6307\u4ee4\u9075\u5faa\u80fd\u529b\u3002", "conclusion": "Jinx \u7684\u53d1\u5e03\u4e3a\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u4e86\u4e00\u4e2a\u6709\u4ef7\u503c\u7684\u8d44\u6e90\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u8fb9\u754c\u548c\u7cfb\u7edf\u5730\u7814\u7a76\u5176\u6545\u969c\u6a21\u5f0f\u3002"}}
{"id": "2508.07251", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07251", "abs": "https://arxiv.org/abs/2508.07251", "authors": ["Junsheng Huang", "Shengyu Hao", "Bocheng Hu", "Gaoang Wang"], "title": "Understanding Dynamic Scenes in Ego Centric 4D Point Clouds", "comment": null, "summary": "Understanding dynamic 4D scenes from an egocentric perspective-modeling\nchanges in 3D spatial structure over time-is crucial for human-machine\ninteraction, autonomous navigation, and embodied intelligence. While existing\negocentric datasets contain dynamic scenes, they lack unified 4D annotations\nand task-driven evaluation protocols for fine-grained spatio-temporal\nreasoning, especially on motion of objects and human, together with their\ninteractions. To address this gap, we introduce EgoDynamic4D, a novel QA\nbenchmark on highly dynamic scenes, comprising RGB-D video, camera poses,\nglobally unique instance masks, and 4D bounding boxes. We construct 927K QA\npairs accompanied by explicit Chain-of-Thought (CoT), enabling verifiable,\nstep-by-step spatio-temporal reasoning. We design 12 dynamic QA tasks covering\nagent motion, human-object interaction, trajectory prediction, relation\nunderstanding, and temporal-causal reasoning, with fine-grained,\nmultidimensional metrics. To tackle these tasks, we propose an end-to-end\nspatio-temporal reasoning framework that unifies dynamic and static scene\ninformation, using instance-aware feature encoding, time and camera encoding,\nand spatially adaptive down-sampling to compress large 4D scenes into token\nsequences manageable by LLMs. Experiments on EgoDynamic4D show that our method\nconsistently outperforms baselines, validating the effectiveness of multimodal\ntemporal modeling for egocentric dynamic scene understanding.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u4e86 EgoDynamic4D\uff0c\u4e00\u4e2a\u5305\u542bRGB-D\u89c6\u9891\u3001\u76f8\u673a\u59ff\u6001\u3001\u5b9e\u4f8b\u63a9\u7801\u548c4D\u8fb9\u754c\u6846\u7684\u65b0\u578bQA\u57fa\u51c6\uff0c\u7528\u4e8e\u7406\u89e3\u52a8\u60014D\u573a\u666f\u3002\u4ed6\u4eec\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u65f6\u7a7a\u63a8\u7406\u6846\u67b6\uff0c\u4ee5\u5904\u7406\u7ec6\u7c92\u5ea6\u7684\u65f6\u7a7a\u63a8\u7406\u4efb\u52a1\uff0c\u5e76\u5728EgoDynamic4D\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684 egocentric \u6570\u636e\u96c6\u867d\u7136\u5305\u542b\u52a8\u6001\u573a\u666f\uff0c\u4f46\u7f3a\u4e4f\u7edf\u4e00\u76844D\u6807\u6ce8\u548c\u9762\u5411\u7ec6\u7c92\u5ea6\u65f6\u7a7a\u63a8\u7406\u7684\u4efb\u52a1\u9a71\u52a8\u8bc4\u4f30\u534f\u8bae\uff0c\u7279\u522b\u662f\u5728\u7269\u4f53\u548c\u4eba\u7684\u8fd0\u52a8\u53ca\u5176\u4ea4\u4e92\u65b9\u9762\u3002\u4e3a\u4e86\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\uff0c\u9700\u8981\u4e00\u4e2a\u5305\u542b\u4e30\u5bcc4D\u6807\u6ce8\u548c\u591a\u7ef4\u5ea6\u8bc4\u4f30\u6307\u6807\u7684\u65b0\u578b\u57fa\u51c6\u3002 \n", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7aef\u5230\u7aef\u7684\u65f6\u7a7a\u63a8\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u52a8\u6001\u548c\u9759\u6001\u573a\u666f\u4fe1\u606f\uff0c\u5e76\u91c7\u7528\u4e86\u5b9e\u4f8b\u611f\u77e5\u7279\u5f81\u7f16\u7801\u3001\u65f6\u95f4\u4e0e\u76f8\u673a\u7f16\u7801\u4ee5\u53ca\u7a7a\u95f4\u81ea\u9002\u5e94\u4e0b\u91c7\u6837\u7b49\u6280\u672f\uff0c\u5c06\u5927\u578b4D\u573a\u666f\u538b\u7f29\u4e3a\u53ef\u88ab\u8bed\u8a00\u6a21\u578b\u5904\u7406\u7684\u5e8f\u5217\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u5728 EgoDynamic4D \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u7684\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u591a\u6a21\u6001\u65f6\u6001\u6a21\u578b\u5728 egocentric \u52a8\u6001\u573a\u666f\u7406\u89e3\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u591a\u6a21\u6001\u65f6\u6001\u6a21\u578b\u80fd\u591f\u6709\u6548\u63d0\u5347\u5bf9\u52a8\u6001\u573a\u666f\u7684 egocentric \u7406\u89e3\u80fd\u529b\uff0c\u5e76\u4e14\u5728 EgoDynamic4D \u6570\u636e\u96c6\u4e0a\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.07638", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07638", "abs": "https://arxiv.org/abs/2508.07638", "authors": ["Jia Zhang", "Yao Liu", "Chen-Xi Zhang", "Yi Liu", "Yi-Xuan Jin", "Lan-Zhe Guo", "Yu-Feng Li"], "title": "Beyond Single: A Data Selection Principle for LLM Alignment via Fine-Grained Preference Signals", "comment": "Under review", "summary": "Aligning Large Language Models (LLMs) with diverse human values requires\nmoving beyond a single holistic \"better-than\" preference criterion. While\ncollecting fine-grained, aspect-specific preference data is more reliable and\nscalable, existing methods like Direct Preference Optimization (DPO) struggle\nwith the severe noise and conflicts inherent in such aggregated datasets. In\nthis paper, we tackle this challenge from a data-centric perspective. We first\nderive the Direct Multi-Preference Optimization (DMPO) objective, and uncover a\nkey Preference Divergence (PD) term that quantifies inter-aspect preference\nconflicts. Instead of using this term for direct optimization, we leverage it\nto formulate a novel, theoretically-grounded data selection principle. Our\nprinciple advocates for selecting a subset of high-consensus data-identified by\nthe most negative PD values-for efficient DPO training. We prove the optimality\nof this strategy by analyzing the loss bounds of the DMPO objective in the\nselection problem. To operationalize our approach, we introduce practical\nmethods of PD term estimation and length bias mitigation, thereby proposing our\nPD selection method. Evaluation on the UltraFeedback dataset with three varying\nconflict levels shows that our simple yet effective strategy achieves over 10%\nrelative improvement against both the standard holistic preference and a\nstronger oracle using aggregated preference signals, all while boosting\ntraining efficiency and obviating the need for intractable holistic preference\nannotating, unlocking the potential of robust LLM alignment via fine-grained\npreference signals.", "AI": {"tldr": "This paper introduces a data selection method called PD selection to improve LLM alignment using fine-grained preferences. It uses a 'Preference Divergence' term to find data with high agreement, making training more efficient and improving results by over 10% compared to other methods.", "motivation": "Existing methods like DPO struggle with noisy and conflicting fine-grained preference data, necessitating a more robust approach to aligning Large Language Models (LLMs) with diverse human values. The motivation is to move beyond single, holistic preference criteria and effectively utilize scalable, aspect-specific preference data.", "method": "The paper proposes a data-centric approach by deriving the Direct Multi-Preference Optimization (DMPO) objective and identifying a Preference Divergence (PD) term that quantifies inter-aspect preference conflicts. This PD term is then used to formulate a data selection principle, favoring data with high consensus (negative PD values) for efficient Direct Preference Optimization (DPO) training. Practical methods for PD term estimation and length bias mitigation are also introduced.", "result": "The PD selection method achieves over 10% relative improvement compared to standard holistic preference and an oracle method on the UltraFeedback dataset, even with high conflict levels. It also enhances training efficiency and avoids the need for difficult holistic preference annotations.", "conclusion": "Aligning LLMs with diverse human values can be effectively achieved by leveraging fine-grained preference signals through a novel data selection strategy based on Preference Divergence (PD). This method, termed PD selection, identifies and utilizes high-consensus data points, leading to significant improvements over existing methods and boosting training efficiency."}}
{"id": "2508.07260", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07260", "abs": "https://arxiv.org/abs/2508.07260", "authors": ["Sihan Yang", "Huitong Ji", "Shaolin Lu", "Jiayi Chen", "Binxiao Xu", "Ming Lu", "Yuanxing Zhang", "Wenhui Dong", "Wentao Zhang"], "title": "Small-Large Collaboration: Training-efficient Concept Personalization for Large VLM using a Meta Personalized Small VLM", "comment": null, "summary": "Personalizing Vision-Language Models (VLMs) to transform them into daily\nassistants has emerged as a trending research direction. However, leading\ncompanies like OpenAI continue to increase model size and develop complex\ndesigns such as the chain of thought (CoT). While large VLMs are proficient in\ncomplex multi-modal understanding, their high training costs and limited access\nvia paid APIs restrict direct personalization. Conversely, small VLMs are\neasily personalized and freely available, but they lack sufficient reasoning\ncapabilities. Inspired by this, we propose a novel collaborative framework\nnamed Small-Large Collaboration (SLC) for large VLM personalization, where the\nsmall VLM is responsible for generating personalized information, while the\nlarge model integrates this personalized information to deliver accurate\nresponses. To effectively incorporate personalized information, we develop a\ntest-time reflection strategy, preventing the potential hallucination of the\nsmall VLM. Since SLC only needs to train a meta personalized small VLM for the\nlarge VLMs, the overall process is training-efficient. To the best of our\nknowledge, this is the first training-efficient framework that supports both\nopen-source and closed-source large VLMs, enabling broader real-world\npersonalized applications. We conduct thorough experiments across various\nbenchmarks and large VLMs to demonstrate the effectiveness of the proposed SLC\nframework. The code will be released at https://github.com/Hhankyangg/SLC.", "AI": {"tldr": "\u4e00\u4e2a\u540d\u4e3aSLC\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u8ba9\u5c0f\u578bVLM\u751f\u6210\u4e2a\u6027\u5316\u4fe1\u606f\uff0c\u5e76\u7531\u5927\u578bVLM\u6574\u5408\uff0c\u5b9e\u73b0\u4e86\u9ad8\u6548\u4e14\u5e7f\u6cdb\u9002\u7528\u7684\u5927\u578bVLM\u4e2a\u6027\u5316\u3002", "motivation": "\u5f53\u524d\uff0c\u5c06VLMs\u4e2a\u6027\u5316\u4ee5\u7528\u4f5c\u65e5\u5e38\u52a9\u624b\u7684\u7814\u7a76\u65b9\u5411\u5907\u53d7\u5173\u6ce8\u3002\u7136\u800c\uff0c\u5927\u578bVLMs\u8bad\u7ec3\u6210\u672c\u9ad8\u4e14\u8bbf\u95ee\u53d7\u9650\uff0c\u800c\u5c0f\u578bVLMs\u867d\u7136\u6613\u4e8e\u4e2a\u6027\u5316\u4f46\u63a8\u7406\u80fd\u529b\u4e0d\u8db3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u80fd\u591f\u6709\u6548\u7ed3\u5408\u5927\u578bVLMs\u7684\u590d\u6742\u63a8\u7406\u80fd\u529b\u548c\u5c0f\u578bVLMs\u6613\u4e8e\u4e2a\u6027\u5316\u7684\u4f18\u52bf\u7684\u6846\u67b6\u3002", "method": "\u8be5\u6846\u67b6\u7684\u6838\u5fc3\u662f\u201c\u5c0f-\u5927\u534f\u4f5c\u201d\uff08SLC\uff09\uff0c\u5176\u4e2d\u5c0f\u578bVLM\u8d1f\u8d23\u751f\u6210\u4e2a\u6027\u5316\u4fe1\u606f\uff0c\u5927\u578bVLM\u5219\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\u4ee5\u751f\u6210\u51c6\u786e\u7684\u54cd\u5e94\u3002\u6b64\u5916\uff0c\u8fd8\u5f00\u53d1\u4e86\u4e00\u79cd\u201c\u6d4b\u8bd5\u65f6\u53cd\u601d\u201d\u7b56\u7565\uff0c\u4ee5\u9632\u6b62\u5c0f\u578bVLM\u4ea7\u751f\u5e7b\u89c9\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cSLC\u6846\u67b6\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u548c\u5927\u578bVLMs\u4e0a\u5747\u8868\u73b0\u51fa\u6709\u6548\u6027\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u63d0\u9ad8\u5927\u578bVLM\u4e2a\u6027\u5316\u65b9\u9762\u7684\u80fd\u529b\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u540d\u4e3aSLC\uff08Small-Large Collaboration\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u5927\u89c4\u6a21\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u7684\u4e2a\u6027\u5316\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8ba9\u5c0f\u578bVLM\u751f\u6210\u4e2a\u6027\u5316\u4fe1\u606f\uff0c\u5e76\u7531\u5927\u578bVLM\u6574\u5408\u8fd9\u4e9b\u4fe1\u606f\u6765\u63d0\u4f9b\u51c6\u786e\u7684\u54cd\u5e94\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u9ad8\u6548\u7684\u8bad\u7ec3\u548c\u5e7f\u6cdb\u7684\u5e94\u7528\u3002"}}
{"id": "2508.07646", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07646", "abs": "https://arxiv.org/abs/2508.07646", "authors": ["Xiaoxue Yang", "Jaeha Lee", "Anna-Katharina Dick", "Jasper Timm", "Fei Xie", "Diogo Cruz"], "title": "Multi-Turn Jailbreaks Are Simpler Than They Seem", "comment": "25 pages, 15 figures. Accepted at COLM 2025 SoLaR Workshop", "summary": "While defenses against single-turn jailbreak attacks on Large Language Models\n(LLMs) have improved significantly, multi-turn jailbreaks remain a persistent\nvulnerability, often achieving success rates exceeding 70% against models\noptimized for single-turn protection. This work presents an empirical analysis\nof automated multi-turn jailbreak attacks across state-of-the-art models\nincluding GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark.\nOur findings challenge the perceived sophistication of multi-turn attacks: when\naccounting for the attacker's ability to learn from how models refuse harmful\nrequests, multi-turn jailbreaking approaches are approximately equivalent to\nsimply resampling single-turn attacks multiple times. Moreover, attack success\nis correlated among similar models, making it easier to jailbreak newly\nreleased ones. Additionally, for reasoning models, we find surprisingly that\nhigher reasoning effort often leads to higher attack success rates. Our results\nhave important implications for AI safety evaluation and the design of\njailbreak-resistant systems. We release the source code at\nhttps://github.com/diogo-cruz/multi_turn_simpler", "AI": {"tldr": "\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u5e76\u4e0d\u6bd4\u5355\u8f6e\u653b\u51fb\u66f4\u590d\u6742\uff0c\u5e76\u4e14\u4e0e\u6a21\u578b\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u7684\u65b9\u5f0f\u76f8\u5173\uff0c\u5c24\u5176\u662f\u5bf9\u4e8e\u63a8\u7406\u80fd\u529b\u5f3a\u7684\u6a21\u578b\u3002", "motivation": "\u5c3d\u7ba1\u9488\u5bf9\u5355\u8f6e\u8d8a\u72f1\u653b\u51fb\u7684\u9632\u5fa1\u80fd\u529b\u6709\u4e86\u663e\u8457\u63d0\u9ad8\uff0c\u4f46\u591a\u8f6e\u8d8a\u72f1\u4ecd\u7136\u662f\u4e00\u79cd\u6301\u7eed\u5b58\u5728\u7684\u6f0f\u6d1e\uff0c\u6210\u529f\u7387\u5f80\u5f80\u8d85\u8fc770%\u3002", "method": "\u5bf9\u5305\u62ecGPT-4\u3001Claude\u548cGemini\u53d8\u4f53\u5728\u5185\u7684\u6700\u5148\u8fdb\u6a21\u578b\u8fdb\u884c\u4e86\u81ea\u52a8\u5316\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u7684\u5b9e\u8bc1\u5206\u6790\uff0c\u4f7f\u7528\u4e86StrongREJECT\u57fa\u51c6\u3002", "result": "\u5f53\u8003\u8651\u5230\u653b\u51fb\u8005\u80fd\u591f\u4ece\u6a21\u578b\u5982\u4f55\u62d2\u7edd\u6709\u5bb3\u8bf7\u6c42\u4e2d\u5b66\u4e60\u65f6\uff0c\u591a\u8f6e\u8d8a\u72f1\u65b9\u6cd5\u5927\u7ea6\u7b49\u540c\u4e8e\u5bf9\u5355\u8f6e\u653b\u51fb\u8fdb\u884c\u591a\u6b21\u91c7\u6837\u3002\u653b\u51fb\u6210\u529f\u7387\u5728\u76f8\u4f3c\u6a21\u578b\u4e4b\u95f4\u76f8\u5173\u3002\u5bf9\u4e8e\u63a8\u7406\u6a21\u578b\uff0c\u66f4\u9ad8\u7684\u63a8\u7406\u52aa\u529b\u901a\u5e38\u4f1a\u5bfc\u81f4\u66f4\u9ad8\u7684\u653b\u51fb\u6210\u529f\u7387\u3002", "conclusion": "\u591a\u8f6e\u8d8a\u72f1\u653b\u51fb\u7684\u6210\u529f\u7387\u4e0e\u5355\u8f6e\u653b\u51fb\u7684\u591a\u6b21\u91c7\u6837\u76f8\u5f53\uff0c\u653b\u51fb\u6210\u529f\u7387\u4e0e\u76f8\u4f3c\u6a21\u578b\u76f8\u5173\uff0c\u9ad8\u63a8\u7406\u80fd\u529b\u6a21\u578b\u66f4\u5bb9\u6613\u88ab\u8d8a\u72f1\u3002"}}
{"id": "2508.07270", "categories": ["cs.CV", "cs.AI", "cs.LG", "eess.IV", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07270", "abs": "https://arxiv.org/abs/2508.07270", "authors": ["Xiang Xiang", "Qinhao Zhou", "Zhuo Xu", "Jing Ma", "Jiaxin Dai", "Yifan Liang", "Hanlin Li"], "title": "OpenHAIV: A Framework Towards Practical Open-World Learning", "comment": "Codes, results, and OpenHAIV documentation available at\n  https://haiv-lab.github.io/openhaiv", "summary": "Substantial progress has been made in various techniques for open-world\nrecognition. Out-of-distribution (OOD) detection methods can effectively\ndistinguish between known and unknown classes in the data, while incremental\nlearning enables continuous model knowledge updates. However, in open-world\nscenarios, these approaches still face limitations. Relying solely on OOD\ndetection does not facilitate knowledge updates in the model, and incremental\nfine-tuning typically requires supervised conditions, which significantly\ndeviate from open-world settings. To address these challenges, this paper\nproposes OpenHAIV, a novel framework that integrates OOD detection, new class\ndiscovery, and incremental continual fine-tuning into a unified pipeline. This\nframework allows models to autonomously acquire and update knowledge in\nopen-world environments. The proposed framework is available at\nhttps://haiv-lab.github.io/openhaiv .", "AI": {"tldr": "OpenHAIV\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u5c06OOD\u68c0\u6d4b\u3001\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u589e\u91cf\u6301\u7eed\u5fae\u8c03\u7ed3\u5408\u8d77\u6765\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u5730\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u5b66\u4e60\u548c\u66f4\u65b0\u77e5\u8bc6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709OOD\u68c0\u6d4b\u65b9\u6cd5\u65e0\u6cd5\u66f4\u65b0\u6a21\u578b\u77e5\u8bc6\u4ee5\u53ca\u589e\u91cf\u5fae\u8c03\u901a\u5e38\u9700\u8981\u76d1\u7763\u6761\u4ef6\uff08\u8fd9\u4e0e\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u5927\u76f8\u5f84\u5ead\uff09\u7684\u5c40\u9650\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOpenHAIV\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06OOD\u68c0\u6d4b\u3001\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u589e\u91cf\u6301\u7eed\u5fae\u8c03\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u7684\u6d41\u6c34\u7ebf\u4e2d\u3002", "result": "OpenHAIV\u662f\u4e00\u4e2a\u5c06OOD\u68c0\u6d4b\u3001\u65b0\u7c7b\u522b\u53d1\u73b0\u548c\u589e\u91cf\u6301\u7eed\u5fae\u8c03\u6574\u5408\u5230\u4e00\u4e2a\u7edf\u4e00\u6d41\u6c34\u7ebf\u4e2d\u7684\u6846\u67b6\uff0c\u4f7f\u6a21\u578b\u80fd\u591f\u81ea\u4e3b\u83b7\u53d6\u548c\u66f4\u65b0\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u7684\u77e5\u8bc6\u3002", "conclusion": "\u8be5\u6846\u67b6\u5141\u8bb8\u6a21\u578b\u5728\u5f00\u653e\u4e16\u754c\u73af\u5883\u4e2d\u81ea\u4e3b\u83b7\u53d6\u548c\u66f4\u65b0\u77e5\u8bc6\u3002"}}
{"id": "2508.07659", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07659", "abs": "https://arxiv.org/abs/2508.07659", "authors": ["Hyeon-Ju Jeon", "Jeon-Ho Kang", "In-Hyuk Kwon", "O-Joun Lee"], "title": "Discovering Spatial Correlations between Earth Observations in Global Atmospheric State Estimation by using Adaptive Graph Structure Learning", "comment": "10 pages", "summary": "This study aims to discover spatial correlations between Earth observations\nand atmospheric states to improve the forecasting accuracy of global\natmospheric state estimation, which are usually conducted using conventional\nnumerical weather prediction (NWP) systems and is the beginning of weather\nforecasting. NWP systems predict future atmospheric states at fixed locations,\nwhich are called NWP grid points, by analyzing previous atmospheric states and\nnewly acquired Earth observations without fixed locations. Thus, surrounding\nmeteorological context and the changing locations of the observations make\nspatial correlations between atmospheric states and observations over time. To\nhandle complicated spatial correlations, which change dynamically, we employ\nspatiotemporal graph neural networks (STGNNs) with structure learning. However,\nstructure learning has an inherent limitation that this can cause structural\ninformation loss and over-smoothing problem by generating excessive edges. To\nsolve this problem, we regulate edge sampling by adaptively determining node\ndegrees and considering the spatial distances between NWP grid points and\nobservations. We validated the effectiveness of the proposed method by using\nreal-world atmospheric state and observation data from East Asia. Even in areas\nwith high atmospheric variability, the proposed method outperformed existing\nSTGNN models with and without structure learning.", "AI": {"tldr": "\u4e3a\u63d0\u9ad8\u5929\u6c14\u9884\u62a5\u7cbe\u5ea6\uff0c\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684STGNN\u6a21\u578b\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u8fb9\u91c7\u6837\u89e3\u51b3\u7ed3\u6784\u5b66\u4e60\u4e2d\u7684\u4fe1\u606f\u4e22\u5931\u548c\u5e73\u6ed1\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9645\u6570\u636e\u4e2d\u9a8c\u8bc1\u4e86\u5176\u4f18\u8d8a\u6027\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u5168\u7403\u5927\u6c14\u72b6\u6001\u4f30\u8ba1\u7684\u9884\u6d4b\u51c6\u786e\u6027\uff0c\u9700\u8981\u53d1\u73b0\u5730\u7403\u89c2\u6d4b\u4e0e\u5927\u6c14\u72b6\u6001\u4e4b\u95f4\u7684\u7a7a\u95f4\u76f8\u5173\u6027\u3002\u4f20\u7edf\u7684\u6570\u503c\u5929\u6c14\u9884\u62a5\uff08NWP\uff09\u7cfb\u7edf\u5728\u9884\u6d4b\u65f6\u5b58\u5728\u56fa\u5b9a\u7f51\u683c\u70b9\u7684\u95ee\u9898\uff0c\u800c\u89c2\u6d4b\u6570\u636e\u6ca1\u6709\u56fa\u5b9a\u4f4d\u7f6e\uff0c\u8fd9\u4f7f\u5f97\u7a7a\u95f4\u76f8\u5173\u6027\u53d8\u5f97\u590d\u6742\u4e14\u52a8\u6001\u53d8\u5316\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u91c7\u7528\u7ed3\u6784\u5b66\u4e60\u7684\u56fe\u795e\u7ecf\u7f51\u7edc\uff08STGNN\uff09\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u81ea\u9002\u5e94\u5730\u786e\u5b9a\u8282\u70b9\u5ea6\u6570\u548c\u8003\u8651\u7f51\u683c\u70b9\u4e0e\u89c2\u6d4b\u503c\u7684\u7a7a\u95f4\u8ddd\u79bb\u6765\u8c03\u8282\u8fb9\u91c7\u6837\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u7ed3\u6784\u5b66\u4e60\u65b9\u6cd5\u4e2d\u53ef\u80fd\u5b58\u5728\u7684\u4fe1\u606f\u4e22\u5931\u548c\u5e73\u6ed1\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u6a21\u578b\u5728\u4e1c\u4e9a\u5730\u533a\u7684\u5b9e\u9645\u5927\u6c14\u72b6\u6001\u548c\u89c2\u6d4b\u6570\u636e\u4e0a\uff0c\u5373\u4f7f\u5728\u0130\u9ad8\u5927\u6c14\u53d8\u7387\u7684\u533a\u57df\uff0c\u5176\u6027\u80fd\u4e5f\u4f18\u4e8e\u73b0\u6709\u7684STGNN\u6a21\u578b\uff0c\u5305\u62ec\u90a3\u4e9b\u672a\u4f7f\u7528\u7ed3\u6784\u5b66\u4e60\u7684\u6a21\u578b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u7ed3\u5408\u4e86\u7ed3\u6784\u5b66\u4e60\u548c\u81ea\u9002\u5e94\u8fb9\u91c7\u6837\u7b56\u7565\u7684STGNN\u6a21\u578b\uff0c\u5728\u4e1c\u4e9a\u5730\u533a\u7684\u5b9e\u9645\u5927\u6c14\u72b6\u6001\u548c\u89c2\u6d4b\u6570\u636e\u9a8c\u8bc1\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u7684STGNN\u6a21\u578b\uff08\u5305\u62ec\u672a\u4f7f\u7528\u7ed3\u6784\u5b66\u4e60\u7684\u6a21\u578b\uff09\uff0c\u5728\u63d0\u9ad8\u5929\u6c14\u9884\u62a5\u51c6\u786e\u6027\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\uff0c\u5c24\u5176\u662f\u5728\u5927\u6c14\u53d8\u5316\u5267\u70c8\u7684\u533a\u57df\u3002"}}
{"id": "2508.07281", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07281", "abs": "https://arxiv.org/abs/2508.07281", "authors": ["Hongbo Zhu", "Angelo Cangelosi"], "title": "Representation Understanding via Activation Maximization", "comment": "7 pages,12 figures", "summary": "Understanding internal feature representations of deep neural networks (DNNs)\nis a fundamental step toward model interpretability. Inspired by neuroscience\nmethods that probe biological neurons using visual stimuli, recent deep\nlearning studies have employed Activation Maximization (AM) to synthesize\ninputs that elicit strong responses from artificial neurons. In this work, we\npropose a unified feature visualization framework applicable to both\nConvolutional Neural Networks (CNNs) and Vision Transformers (ViTs). Unlike\nprior efforts that predominantly focus on the last output-layer neurons in\nCNNs, we extend feature visualization to intermediate layers as well, offering\ndeeper insights into the hierarchical structure of learned feature\nrepresentations. Furthermore, we investigate how activation maximization can be\nleveraged to generate adversarial examples, revealing potential vulnerabilities\nand decision boundaries of DNNs. Our experiments demonstrate the effectiveness\nof our approach in both traditional CNNs and modern ViT, highlighting its\ngeneralizability and interpretive value.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7279\u5f81\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8eCNN\u548cViT\uff0c\u53ef\u7528\u4e8e\u4e2d\u95f4\u5c42\u53ef\u89c6\u5316\u548c\u751f\u6210\u5bf9\u6297\u6837\u672c\uff0c\u5b9e\u9a8c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7406\u89e3\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u5185\u90e8\u7279\u5f81\u8868\u793a\u662f\u6a21\u578b\u53ef\u89e3\u91ca\u6027\u7684\u57fa\u7840\u3002\u53d7\u795e\u7ecf\u79d1\u5b66\u4e2d\u5229\u7528\u89c6\u89c9\u523a\u6fc0\u63a2\u6d4b\u751f\u7269\u795e\u7ecf\u5143\u65b9\u6cd5\u7684\u542f\u53d1\uff0c\u672c\u7814\u7a76\u65e8\u5728\u5f00\u53d1\u4e00\u79cd\u80fd\u591f\u5e94\u7528\u4e8eCNN\u548cViT\u7684\u7edf\u4e00\u7279\u5f81\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u5e76\u6df1\u5165\u7814\u7a76\u5176\u5728\u89e3\u91caDNN\u548c\u751f\u6210\u5bf9\u6297\u6837\u672c\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7edf\u4e00\u7684\u7279\u5f81\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u9002\u7528\u4e8eCNN\u548cViT\uff0c\u80fd\u591f\u5c06\u7279\u5f81\u53ef\u89c6\u5316\u6269\u5c55\u5230\u4e2d\u95f4\u5c42\uff0c\u5e76\u7814\u7a76\u4e86\u6fc0\u6d3b\u6700\u5927\u5316\u5728\u751f\u6210\u5bf9\u6297\u6837\u672c\u4e2d\u7684\u5e94\u7528\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u4f20\u7edfCNN\u548c\u73b0\u4ee3ViT\u4e0a\u7684\u6709\u6548\u6027\u3001\u901a\u7528\u6027\u548c\u89e3\u91ca\u4ef7\u503c\uff0c\u80fd\u591f\u63d0\u4f9b\u5bf9\u5b66\u4e60\u5230\u7684\u7279\u5f81\u8868\u793a\u7684\u66f4\u6df1\u5c42\u6b21\u7684\u7406\u89e3\uff0c\u5e76\u63ed\u793aDNN\u7684\u6f5c\u5728\u6f0f\u6d1e\u548c\u51b3\u7b56\u8fb9\u754c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u7279\u5f81\u53ef\u89c6\u5316\u6846\u67b6\uff0c\u9002\u7528\u4e8eCNN\u548cViT\uff0c\u5e76\u80fd\u5c06\u7279\u5f81\u53ef\u89c6\u5316\u6269\u5c55\u5230\u4e2d\u95f4\u5c42\uff0c\u540c\u65f6\u7814\u7a76\u4e86\u6fc0\u6d3b\u6700\u5927\u5316\u5728\u751f\u6210\u5bf9\u6297\u6837\u672c\u4e2d\u7684\u5e94\u7528\uff0c\u8bc1\u660e\u4e86\u5176\u5728CNN\u548cViT\u4e0a\u7684\u6709\u6548\u6027\u3001\u901a\u7528\u6027\u548c\u89e3\u91ca\u4ef7\u503c\u3002"}}
{"id": "2508.07662", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07662", "abs": "https://arxiv.org/abs/2508.07662", "authors": ["Ihor Stepanov", "Mykhailo Shtopko", "Dmytro Vodianytskyi", "Oleksandr Lukashov", "Alexander Yavorskyi", "Mykyta Yaroshenko"], "title": "GLiClass: Generalist Lightweight Model for Sequence Classification Tasks", "comment": "14 pages, 7 tables, 2 figures", "summary": "Classification is one of the most widespread tasks in AI applications,\nserving often as the first step in filtering, sorting, and categorizing data.\nSince modern AI systems must handle large volumes of input data and early\npipeline stages can propagate errors downstream, achieving high efficiency and\naccuracy is critical. Moreover, classification requirements can change\ndynamically based on user needs, necessitating models with strong zero-shot\ncapabilities. While generative LLMs have become mainstream for zero-shot\nclassification due to their versatility, they suffer from inconsistent\ninstruction following and computational inefficiency. Cross-encoders, commonly\nused as rerankers in RAG pipelines, face a different bottleneck: they must\nprocess text-label pairs sequentially, significantly reducing efficiency with\nlarge label sets. Embedding-based approaches offer good efficiency but struggle\nwith complex scenarios involving logical and semantic constraints. We propose\nGLiClass, a novel method that adapts the GLiNER architecture for sequence\nclassification tasks. Our approach achieves strong accuracy and efficiency\ncomparable to embedding-based methods, while maintaining the flexibility needed\nfor zero-shot and few-shot learning scenarios. Additionally, we adapted\nproximal policy optimization (PPO) for multi-label text classification,\nenabling training classifiers in data-sparse conditions or from human feedback.", "AI": {"tldr": "GLiClass\u662f\u4e00\u79cd\u65b0\u7684\u5e8f\u5217\u5206\u7c7b\u65b9\u6cd5\uff0c\u901a\u8fc7\u6539\u7f16GLiNER\u67b6\u6784\u548cPPO\uff0c\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e0a\u8868\u73b0\u51fa\u8272\uff0c\u5e76\u80fd\u9002\u5e94\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709AI\u5206\u7c7b\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6570\u636e\u3001\u52a8\u6001\u53d8\u5316\u7684\u9700\u6c42\u4ee5\u53ca\u6548\u7387\u548c\u51c6\u786e\u6027\u65b9\u9762\u7684\u6311\u6218\uff0c\u4f8b\u5982\u751f\u6210\u5f0f\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u8ba1\u7b97\u6548\u7387\u548c\u6307\u4ee4\u9075\u5faa\u4e0d\u4e00\u81f4\u6027\uff0c\u4ee5\u53ca\u4ea4\u53c9\u7f16\u7801\u5668\u7684\u987a\u5e8f\u5904\u7406\u74f6\u9888\u3002", "method": "GLiClass\u65b9\u6cd5\u5c06GLiNER\u67b6\u6784\u5e94\u7528\u4e8e\u5e8f\u5217\u5206\u7c7b\u4efb\u52a1\uff0c\u5e76\u901a\u8fc7\u6539\u7f16\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\u6765\u5904\u7406\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u3002", "result": "GLiClass\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u53d6\u5f97\u4e86\u4e0e\u5d4c\u5165\u5f0f\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u7ed3\u679c\uff0c\u540c\u65f6\u5177\u5907\u5904\u7406\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u7684\u7075\u6d3b\u6027\u3002", "conclusion": "GLiClass\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u6548\u7387\u65b9\u9762\u5747\u8868\u73b0\u51fa\u8272\uff0c\u53ef\u4e0e\u57fa\u4e8e\u5d4c\u5165\u5f0f\u7684\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u5b66\u4e60\u573a\u666f\u6240\u9700\u7684\u7075\u6d3b\u6027\u3002\u6b64\u5916\uff0c\u6211\u4eec\u8fd8\u4e3a\u591a\u6807\u7b7e\u6587\u672c\u5206\u7c7b\u4efb\u52a1\u6539\u7f16\u4e86\u8fd1\u7aef\u7b56\u7565\u4f18\u5316\uff08PPO\uff09\uff0c\u5b9e\u73b0\u4e86\u5728\u6570\u636e\u7a00\u758f\u6761\u4ef6\u4e0b\u6216\u6839\u636e\u4eba\u7c7b\u53cd\u9988\u8fdb\u884c\u5206\u7c7b\u5668\u8bad\u7ec3\u3002"}}
{"id": "2508.07298", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07298", "abs": "https://arxiv.org/abs/2508.07298", "authors": ["Zhiqiang Shen", "Peng Cao", "Xiaoli Liu", "Jinzhu Yang", "Osmar R. Zaiane"], "title": "SynMatch: Rethinking Consistency in Medical Image Segmentation with Sparse Annotations", "comment": null, "summary": "Label scarcity remains a major challenge in deep learning-based medical image\nsegmentation. Recent studies use strong-weak pseudo supervision to leverage\nunlabeled data. However, performance is often hindered by inconsistencies\nbetween pseudo labels and their corresponding unlabeled images. In this work,\nwe propose \\textbf{SynMatch}, a novel framework that sidesteps the need for\nimproving pseudo labels by synthesizing images to match them instead.\nSpecifically, SynMatch synthesizes images using texture and shape features\nextracted from the same segmentation model that generates the corresponding\npseudo labels for unlabeled images. This design enables the generation of\nhighly consistent synthesized-image-pseudo-label pairs without requiring any\ntraining parameters for image synthesis. We extensively evaluate SynMatch\nacross diverse medical image segmentation tasks under semi-supervised learning\n(SSL), weakly-supervised learning (WSL), and barely-supervised learning (BSL)\nsettings with increasingly limited annotations. The results demonstrate that\nSynMatch achieves superior performance, especially in the most challenging BSL\nsetting. For example, it outperforms the recent strong-weak pseudo\nsupervision-based method by 29.71\\% and 10.05\\% on the polyp segmentation task\nwith 5\\% and 10\\% scribble annotations, respectively. The code will be released\nat https://github.com/Senyh/SynMatch.", "AI": {"tldr": "SynMatch\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u5339\u914d\u4f2a\u6807\u7b7e\u6765\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u6807\u7b7e\u7a00\u758f\u95ee\u9898\uff0c\u65e0\u9700\u6539\u8fdb\u4f2a\u6807\u7b7e\u6216\u989d\u5916\u8bad\u7ec3\uff0c\u5e76\u5728\u5c11\u6807\u6ce8\u573a\u666f\u4e0b\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u6807\u7b7e\u7a00\u758f\u6027\u7684\u6311\u6218\uff0c\u5e76\u514b\u670d\u73b0\u6709\u4f2a\u76d1\u7763\u65b9\u6cd5\u4e2d\u4f2a\u6807\u7b7e\u4e0e\u65e0\u6807\u7b7e\u56fe\u50cf\u4e0d\u4e00\u81f4\u7684\u95ee\u9898\u3002", "method": "SynMatch\u6846\u67b6\u901a\u8fc7\u5408\u6210\u56fe\u50cf\u6765\u5339\u914d\u4f2a\u6807\u7b7e\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u6539\u8fdb\u4f2a\u6807\u7b7e\u3002\u5b83\u5229\u7528\u4ece\u540c\u4e00\u5206\u5272\u6a21\u578b\u4e2d\u63d0\u53d6\u7684\u7eb9\u7406\u548c\u5f62\u72b6\u7279\u5f81\u6765\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u786e\u4fdd\u5408\u6210\u56fe\u50cf\u4e0e\u5176\u5bf9\u5e94\u7684\u4f2a\u6807\u7b7e\u9ad8\u5ea6\u4e00\u81f4\uff0c\u4e14\u65e0\u9700\u989d\u5916\u7684\u5408\u6210\u8bad\u7ec3\u53c2\u6570\u3002", "result": "SynMatch\u5728\u591a\u9879\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u662f\u5728\u6807\u6ce8\u6570\u636e\u6781\u5c11\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u4f8b\u5982\uff0c\u5728\u7ed3\u80a0\u606f\u8089\u5206\u5272\u4efb\u52a1\u4e2d\uff0c\u4f7f\u75285%\u548c10%\u7684\u6d82\u9e26\u6807\u6ce8\u65f6\uff0c\u5176\u6027\u80fd\u5206\u522b\u6bd4\u6700\u8fd1\u7684\u5f3a\u5f31\u4f2a\u76d1\u7763\u65b9\u6cd5\u9ad8\u51fa29.71%\u548c10.05%\u3002", "conclusion": "SynMatch\u6846\u67b6\u5728\u5404\u79cd\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4efb\u52a1\u7684\u534a\u76d1\u7763\u3001\u5f31\u76d1\u7763\u548c\u5c11\u76d1\u7763\u5b66\u4e60\u8bbe\u7f6e\u4e0b\uff0c\u5c24\u5176\u662f\u5728\u6807\u6ce8\u6781\u5c11\u7684\u5c11\u76d1\u7763\u8bbe\u7f6e\u4e0b\uff0c\u5747\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u4f18\u4e8e\u6700\u8fd1\u7684\u5f3a\u5f31\u4f2a\u76d1\u7763\u65b9\u6cd5\u3002"}}
{"id": "2508.07668", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07668", "abs": "https://arxiv.org/abs/2508.07668", "authors": ["Hyobin Park", "Jinwook Jung", "Minseok Seo", "Hyunsoo Choi", "Deukjae Cho", "Sekil Park", "Dong-Geol Choi"], "title": "AIS-LLM: A Unified Framework for Maritime Trajectory Prediction, Anomaly Detection, and Collision Risk Assessment with Explainable Forecasting", "comment": null, "summary": "With the increase in maritime traffic and the mandatory implementation of the\nAutomatic Identification System (AIS), the importance and diversity of maritime\ntraffic analysis tasks based on AIS data, such as vessel trajectory prediction,\nanomaly detection, and collision risk assessment, is rapidly growing. However,\nexisting approaches tend to address these tasks individually, making it\ndifficult to holistically consider complex maritime situations. To address this\nlimitation, we propose a novel framework, AIS-LLM, which integrates time-series\nAIS data with a large language model (LLM). AIS-LLM consists of a Time-Series\nEncoder for processing AIS sequences, an LLM-based Prompt Encoder, a\nCross-Modality Alignment Module for semantic alignment between time-series data\nand textual prompts, and an LLM-based Multi-Task Decoder. This architecture\nenables the simultaneous execution of three key tasks: trajectory prediction,\nanomaly detection, and risk assessment of vessel collisions within a single\nend-to-end system. Experimental results demonstrate that AIS-LLM outperforms\nexisting methods across individual tasks, validating its effectiveness.\nFurthermore, by integratively analyzing task outputs to generate situation\nsummaries and briefings, AIS-LLM presents the potential for more intelligent\nand efficient maritime traffic management.", "AI": {"tldr": "AIS-LLM\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u5b83\u7ed3\u5408\u4e86AIS\u6570\u636e\u548cLLM\uff0c\u80fd\u591f\u540c\u65f6\u5904\u7406\u6d77\u4e0a\u4ea4\u901a\u7684\u8f68\u8ff9\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u6d77\u4e0a\u4ea4\u901a\u5206\u6790\u65b9\u6cd5\u503e\u5411\u4e8e\u5355\u72ec\u5904\u7406\u8f68\u8ff9\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u7b49\u4efb\u52a1\uff0c\u96be\u4ee5\u6574\u4f53\u8003\u8651\u590d\u6742\u7684\u6d77\u4e0a\u60c5\u51b5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAIS-LLM\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u6574\u5408\u4e86\u65f6\u95f4\u5e8f\u5217AIS\u6570\u636e\u548c\u4e00\u4e2a\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002AIS-LLM\u5305\u542b\u4e00\u4e2a\u7528\u4e8e\u5904\u7406AIS\u5e8f\u5217\u7684\u65f6\u95f4\u5e8f\u5217\u7f16\u7801\u5668\u3001\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u63d0\u793a\u7f16\u7801\u5668\u3001\u4e00\u4e2a\u7528\u4e8e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u548c\u6587\u672c\u63d0\u793a\u4e4b\u95f4\u8bed\u4e49\u5bf9\u9f50\u7684\u8de8\u6a21\u6001\u5bf9\u9f50\u6a21\u5757\uff0c\u4ee5\u53ca\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u591a\u4efb\u52a1\u89e3\u7801\u5668\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cAIS-LLM\u5728\u5404\u9879\u5355\u72ec\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4e14\u901a\u8fc7\u6574\u5408\u5206\u6790\u4efb\u52a1\u8f93\u51fa\u6765\u751f\u6210\u6001\u52bf\u6458\u8981\u548c\u7b80\u62a5\uff0c\u5c55\u793a\u4e86\u5176\u5728\u66f4\u667a\u80fd\u3001\u66f4\u9ad8\u6548\u7684\u6d77\u4e0a\u4ea4\u901a\u7ba1\u7406\u65b9\u9762\u7684\u6f5c\u529b\u3002", "conclusion": "AIS-LLM\u901a\u8fc7\u6574\u5408\u65f6\u95f4\u5e8f\u5217AIS\u6570\u636e\u548c\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u8f68\u8ff9\u9884\u6d4b\u3001\u5f02\u5e38\u68c0\u6d4b\u548c\u78b0\u649e\u98ce\u9669\u8bc4\u4f30\u8fd9\u4e09\u4e2a\u5173\u952e\u4efb\u52a1\u7684\u96c6\u6210\uff0c\u5e76\u4e14\u5728\u5404\u9879\u4efb\u52a1\u4e0a\u5747\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u66f4\u667a\u80fd\u9ad8\u6548\u7684\u6d77\u4e0a\u4ea4\u901a\u7ba1\u7406\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2508.07300", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07300", "abs": "https://arxiv.org/abs/2508.07300", "authors": ["Ping-Mao Huang", "I-Tien Chao", "Ping-Chia Huang", "Jia-Wei Liao", "Yung-Yu Chuang"], "title": "BEVANet: Bilateral Efficient Visual Attention Network for Real-Time Semantic Segmentation", "comment": "Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works", "summary": "Real-time semantic segmentation presents the dual challenge of designing\nefficient architectures that capture large receptive fields for semantic\nunderstanding while also refining detailed contours. Vision transformers model\nlong-range dependencies effectively but incur high computational cost. To\naddress these challenges, we introduce the Large Kernel Attention (LKA)\nmechanism. Our proposed Bilateral Efficient Visual Attention Network (BEVANet)\nexpands the receptive field to capture contextual information and extracts\nvisual and structural features using Sparse Decomposed Large Separable Kernel\nAttentions (SDLSKA). The Comprehensive Kernel Selection (CKS) mechanism\ndynamically adapts the receptive field to further enhance performance.\nFurthermore, the Deep Large Kernel Pyramid Pooling Module (DLKPPM) enriches\ncontextual features by synergistically combining dilated convolutions and large\nkernel attention. The bilateral architecture facilitates frequent branch\ncommunication, and the Boundary Guided Adaptive Fusion (BGAF) module enhances\nboundary delineation by integrating spatial and semantic features under\nboundary guidance. BEVANet achieves real-time segmentation at 33 FPS, yielding\n79.3% mIoU without pretraining and 81.0% mIoU on Cityscapes after ImageNet\npretraining, demonstrating state-of-the-art performance. The code and model is\navailable at https://github.com/maomao0819/BEVANet.", "AI": {"tldr": "BEVANet\u901a\u8fc7\u5f15\u5165\u5927\u6838\u6ce8\u610f\u529b\uff08LKA\uff09\u3001\u7a00\u758f\u5206\u89e3\u5927\u53ef\u5206\u79bb\u5377\u79ef\u6ce8\u610f\u529b\uff08SDLSKA\uff09\u3001\u5168\u9762\u7684\u6838\u9009\u62e9\uff08CKS\uff09\u548c\u6df1\u5ea6\u5927\u578b\u6838\u91d1\u5b57\u5854\u6c60\u5316\u6a21\u5757\uff08DLKPPM\uff09\u4ee5\u53ca\u8fb9\u754c\u5f15\u5bfc\u81ea\u9002\u5e94\u878d\u5408\uff08BGAF\uff09\u6a21\u5757\uff0c\u5728\u5b9e\u65f6\u8bed\u4e49\u5206\u5272\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5b9e\u65f6\u8bed\u4e49\u5206\u5272\u4e2d\u8bbe\u8ba1\u80fd\u591f\u6355\u6349\u7528\u4e8e\u8bed\u4e49\u7406\u89e3\u7684\u5927\u611f\u53d7\u91ce\u5e76\u540c\u65f6\u7ec6\u5316\u8be6\u7ec6\u8f6e\u5ed3\u7684\u6709\u6548\u67b6\u6784\u7684\u53cc\u91cd\u6311\u6218\uff0c\u540c\u65f6\u89e3\u51b3\u89c6\u89c9 Transformer \u8ba1\u7b97\u6210\u672c\u9ad8\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBEVANet\u7684\u53cc\u8fb9\u9ad8\u6548\u89c6\u89c9\u6ce8\u610f\u529b\u7f51\u7edc\uff0c\u5176\u4e2d\u5f15\u5165\u4e86\u5927\u6838\u6ce8\u610f\u529b\uff08LKA\uff09\u673a\u5236\uff0c\u5e76\u901a\u8fc7\u7a00\u758f\u5206\u89e3\u5927\u53ef\u5206\u79bb\u5377\u79ef\u6ce8\u610f\u529b\uff08SDLSKA\uff09\u6269\u5c55\u611f\u53d7\u91ce\u4ee5\u6355\u83b7\u4e0a\u4e0b\u6587\u4fe1\u606f\u5e76\u63d0\u53d6\u89c6\u89c9\u548c\u7ed3\u6784\u7279\u5f81\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u5168\u9762\u7684\u6838\u9009\u62e9\uff08CKS\uff09\u673a\u5236\u6765\u52a8\u6001\u8c03\u6574\u611f\u53d7\u91ce\uff0c\u5e76\u7ed3\u5408\u4e86\u6269\u5f20\u5377\u79ef\u548c\u5927\u578b\u6838\u6ce8\u610f\u529b\u7684\u6df1\u5ea6\u5927\u578b\u6838\u91d1\u5b57\u5854\u6c60\u5316\u6a21\u5757\uff08DLKPPM\uff09\u6765\u4e30\u5bcc\u4e0a\u4e0b\u6587\u7279\u5f81\u3002\u53cc\u8fb9\u67b6\u6784\u4fc3\u8fdb\u4e86\u9891\u7e41\u7684\u5206\u652f\u901a\u4fe1\uff0c\u800c\u8fb9\u754c\u5f15\u5bfc\u81ea\u9002\u5e94\u878d\u5408\uff08BGAF\uff09\u6a21\u5757\u901a\u8fc7\u6574\u5408\u8fb9\u754c\u5f15\u5bfc\u4e0b\u7684\u7a7a\u95f4\u548c\u8bed\u4e49\u7279\u5f81\u6765\u589e\u5f3a\u8fb9\u754c\u7ec6\u8282\u3002", "result": "BEVANet\u5b9e\u73b0\u4e8633 FPS\u7684\u5b9e\u65f6\u5206\u5272\uff0c\u65e0\u9884\u8bad\u7ec3mIoU\u4e3a79.3%\uff0cImageNet\u9884\u8bad\u7ec3\u540emIoU\u4e3a81.0%\u3002", "conclusion": "BEVANet\u5728Cityscapes\u4e0a\u5b9e\u73b0\u4e8633 FPS\u7684\u5b9e\u65f6\u5206\u5272\uff0c\u5728\u65e0\u9884\u8bad\u7ec3\u7684\u60c5\u51b5\u4e0b\u8fbe\u523079.3% mIoU\uff0c\u5728ImageNet\u9884\u8bad\u7ec3\u540e\u8fbe\u523081.0% mIoU\uff0c\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07675", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07675", "abs": "https://arxiv.org/abs/2508.07675", "authors": ["Xutong Liu", "Baran Atalar", "Xiangxiang Dai", "Jinhang Zuo", "Siwei Wang", "John C. S. Lui", "Wei Chen", "Carlee Joe-Wong"], "title": "Semantic Caching for Low-Cost LLM Serving: From Offline Learning to Online Adaptation", "comment": null, "summary": "Large Language Models (LLMs) are revolutionizing how users interact with\ninformation systems, yet their high inference cost poses serious scalability\nand sustainability challenges. Caching inference responses, allowing them to be\nretrieved without another forward pass through the LLM, has emerged as one\npossible solution. Traditional exact-match caching, however, overlooks the\nsemantic similarity between queries, leading to unnecessary recomputation.\nSemantic caching addresses this by retrieving responses based on semantic\nsimilarity, but introduces a fundamentally different cache eviction problem:\none must account for mismatch costs between incoming queries and cached\nresponses. Moreover, key system parameters, such as query arrival probabilities\nand serving costs, are often unknown and must be learned over time. Existing\nsemantic caching methods are largely ad-hoc, lacking theoretical foundations\nand unable to adapt to real-world uncertainty. In this paper, we present a\nprincipled, learning-based framework for semantic cache eviction under unknown\nquery and cost distributions. We formulate both offline optimization and online\nlearning variants of the problem, and develop provably efficient algorithms\nwith state-of-the-art guarantees. We also evaluate our framework on a synthetic\ndataset, showing that our proposed algorithms perform matching or superior\nperformance compared with baselines.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u7f13\u5b58\u7b56\u7565\uff0c\u901a\u8fc7\u5b66\u4e60\u6765\u4f18\u5316\u7f13\u5b58\u9a71\u9010\uff0c\u4ee5\u964d\u4f4e\u5927\u578b\u8bed\u8a00\u6a21\u578b\u7684\u63a8\u7406\u6210\u672c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u89e3\u51b3\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u9ad8\u63a8\u7406\u6210\u672c\u5e26\u6765\u7684\u53ef\u6269\u5c55\u6027\u548c\u53ef\u6301\u7eed\u6027\u6311\u6218\uff0c\u4ee5\u53ca\u73b0\u6709\u8bed\u4e49\u7f13\u5b58\u65b9\u6cd5\u5728\u5904\u7406\u4e0d\u786e\u5b9a\u6027\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5b66\u4e60\u7684\u6846\u67b6\uff0c\u5305\u62ec\u79bb\u7ebf\u4f18\u5316\u548c\u5728\u7ebf\u5b66\u4e60\u7684\u53d8\u4f53\uff0c\u5e76\u5f00\u53d1\u4e86\u5177\u6709\u53ef\u8bc1\u660e\u6548\u7387\u548c\u6700\u65b0\u4fdd\u8bc1\u7684\u7b97\u6cd5\u3002", "result": "\u5728\u5408\u6210\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u663e\u793a\u6240\u63d0\u51fa\u7684\u7b97\u6cd5\u4e0e\u57fa\u7ebf\u76f8\u6bd4\u5177\u6709\u5339\u914d\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8e\u5b66\u4e60\u7684\u8bed\u4e49\u7f13\u5b58\u9a71\u9010\u6846\u67b6\uff0c\u80fd\u591f\u5904\u7406\u672a\u77e5\u7684\u67e5\u8be2\u548c\u6210\u672c\u5206\u5e03\uff0c\u5e76\u63d0\u4f9b\u4e86\u5177\u6709\u7406\u8bba\u4fdd\u8bc1\u7684\u6709\u6548\u7b97\u6cd5\u3002"}}
{"id": "2508.07306", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07306", "abs": "https://arxiv.org/abs/2508.07306", "authors": ["Md Zahurul Haquea", "Yeahyea Sarker", "Muhammed Farhan Sadique Mahi", "Syed Jubayer Jaman", "Md Robiul Islam"], "title": "DragonFruitQualityNet: A Lightweight Convolutional Neural Network for Real-Time Dragon Fruit Quality Inspection on Mobile Devices", "comment": null, "summary": "Dragon fruit, renowned for its nutritional benefits and economic value, has\nexperienced rising global demand due to its affordability and local\navailability. As dragon fruit cultivation expands, efficient pre- and\npost-harvest quality inspection has become essential for improving agricultural\nproductivity and minimizing post-harvest losses. This study presents\nDragonFruitQualityNet, a lightweight Convolutional Neural Network (CNN)\noptimized for real-time quality assessment of dragon fruits on mobile devices.\nWe curated a diverse dataset of 13,789 images, integrating self-collected\nsamples with public datasets (dataset from Mendeley Data), and classified them\ninto four categories: fresh, immature, mature, and defective fruits to ensure\nrobust model training. The proposed model achieves an impressive 93.98%\naccuracy, outperforming existing methods in fruit quality classification. To\nfacilitate practical adoption, we embedded the model into an intuitive mobile\napplication, enabling farmers and agricultural stakeholders to conduct\non-device, real-time quality inspections. This research provides an accurate,\nefficient, and scalable AI-driven solution for dragon fruit quality control,\nsupporting digital agriculture and empowering smallholder farmers with\naccessible technology. By bridging the gap between research and real-world\napplication, our work advances post-harvest management and promotes sustainable\nfarming practices.", "AI": {"tldr": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aDragonFruitQualityNet\u7684\u8f7b\u91cf\u7ea7CNN\u6a21\u578b\uff0c\u7528\u4e8e\u5728\u79fb\u52a8\u8bbe\u5907\u4e0a\u5bf9\u706b\u9f99\u679c\u8fdb\u884c\u5b9e\u65f6\u8d28\u91cf\u8bc4\u4f30\u3002\u8be5\u6a21\u578b\u5728\u5305\u542b13,789\u5f20\u56fe\u50cf\u7684\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8693.98%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u8be5\u6a21\u578b\u5df2\u96c6\u6210\u5230\u4e00\u4e2a\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u4f7f\u519c\u6c11\u80fd\u591f\u8fdb\u884c\u73b0\u573a\u8d28\u91cf\u68c0\u67e5\u3002", "motivation": "\u968f\u7740\u706b\u9f99\u679c\u79cd\u690d\u7684\u6269\u5927\uff0c\u9ad8\u6548\u7684\u91c7\u524d\u548c\u91c7\u540e\u8d28\u91cf\u68c0\u67e5\u5bf9\u4e8e\u63d0\u9ad8\u519c\u4e1a\u751f\u4ea7\u529b\u548c\u6700\u5927\u9650\u5ea6\u5730\u51cf\u5c11\u91c7\u540e\u635f\u5931\u81f3\u5173\u91cd\u8981\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDragonFruitQualityNet\u7684\u8f7b\u91cf\u7ea7\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff08CNN\uff09\uff0c\u5e76\u9488\u5bf9\u79fb\u52a8\u8bbe\u5907\u4e0a\u7684\u706b\u9f99\u679c\u8fdb\u884c\u5b9e\u65f6\u8d28\u91cf\u8bc4\u4f30\u8fdb\u884c\u4e86\u4f18\u5316\u3002\u6570\u636e\u96c6\u5305\u542b13,789\u5f20\u56fe\u50cf\uff0c\u5206\u4e3a\u65b0\u9c9c\u3001\u672a\u6210\u719f\u3001\u6210\u719f\u548c\u6709\u7f3a\u9677\u56db\u7c7b\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6a21\u578b\u5728\u6c34\u679c\u8d28\u91cf\u5206\u7c7b\u65b9\u9762\u53d6\u5f97\u4e8693.98%\u7684\u51c6\u786e\u7387\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u6a21\u578b\u5df2\u88ab\u5d4c\u5165\u5230\u4e00\u4e2a\u76f4\u89c2\u7684\u79fb\u52a8\u5e94\u7528\u7a0b\u5e8f\u4e2d\uff0c\u5141\u8bb8\u519c\u6c11\u548c\u519c\u4e1a\u5229\u76ca\u76f8\u5173\u8005\u8fdb\u884c\u8bbe\u5907\u4e0a\u3001\u5b9e\u65f6\u7684\u8d28\u91cf\u68c0\u67e5\u3002", "conclusion": "\u8fd9\u9879\u7814\u7a76\u63d0\u4f9b\u4e86\u4e00\u4e2a\u51c6\u786e\u3001\u9ad8\u6548\u4e14\u53ef\u6269\u5c55\u7684\u3001\u7531\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u706b\u9f99\u679c\u8d28\u91cf\u63a7\u5236\u89e3\u51b3\u65b9\u6848\uff0c\u652f\u6301\u6570\u5b57\u519c\u4e1a\uff0c\u5e76\u4f7f\u5c0f\u519c\u6237\u80fd\u591f\u83b7\u5f97\u53ef\u53ca\u7684\u6280\u672f\u3002\u901a\u8fc7\u5f25\u5408\u7814\u7a76\u4e0e\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u4fc3\u8fdb\u4e86\u6536\u83b7\u540e\u7ba1\u7406\uff0c\u5e76\u63a8\u5e7f\u4e86\u53ef\u6301\u7eed\u7684\u519c\u4e1a\u5b9e\u8df5\u3002"}}
{"id": "2508.07307", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07307", "abs": "https://arxiv.org/abs/2508.07307", "authors": ["Haiyang Guo", "Fei Zhu", "Hongbo Zhao", "Fanhu Zeng", "Wenzhuo Liu", "Shijie Ma", "Da-Han Wang", "Xu-Yao Zhang"], "title": "MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark", "comment": "Preprint", "summary": "Continual learning aims to equip AI systems with the ability to continuously\nacquire and adapt to new knowledge without forgetting previously learned\ninformation, similar to human learning. While traditional continual learning\nmethods focusing on unimodal tasks have achieved notable success, the emergence\nof Multimodal Large Language Models has brought increasing attention to\nMultimodal Continual Learning tasks involving multiple modalities, such as\nvision and language. In this setting, models are expected to not only mitigate\ncatastrophic forgetting but also handle the challenges posed by cross-modal\ninteractions and coordination. To facilitate research in this direction, we\nintroduce MCITlib, a comprehensive and constantly evolving code library for\ncontinual instruction tuning of Multimodal Large Language Models. In MCITlib,\nwe have currently implemented 8 representative algorithms for Multimodal\nContinual Instruction Tuning and systematically evaluated them on 2 carefully\nselected benchmarks. MCITlib will be continuously updated to reflect advances\nin the Multimodal Continual Learning field. The codebase is released at\nhttps://github.com/Ghy0501/MCITlib.", "AI": {"tldr": "MCITlib\uff1a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u5b66\u4e60\u7684\u4ee3\u7801\u5e93\uff0c\u5305\u542b8\u79cd\u7b97\u6cd5\u548c2\u4e2a\u57fa\u51c6\u8bc4\u4f30\uff0c\u65e8\u5728\u89e3\u51b3\u9057\u5fd8\u548c\u8de8\u6a21\u6001\u534f\u8c03\u95ee\u9898\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u6301\u7eed\u5b66\u4e60\u4e2d\u9047\u5230\u7684\u8de8\u6a21\u6001\u4ea4\u4e92\u548c\u534f\u8c03\u6311\u6218\uff0c\u4ee5\u53ca\u707e\u96be\u6027\u9057\u5fd8\u95ee\u9898\u3002", "method": "\u4ecb\u7ecd\u4e86MCITlib\u5e93\uff0c\u5b9e\u73b0\u4e868\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5\uff0c\u5e76\u57282\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\u3002", "result": "MCITlib\u5e93\u7684\u5b9e\u73b0\u548c\u8bc4\u4f30\u4e3a\u591a\u6a21\u6001\u6301\u7eed\u5b66\u4e60\u7814\u7a76\u63d0\u4f9b\u4e86\u57fa\u7840\u548c\u4fbf\u5229\u3002", "conclusion": "MCITlib\u662f\u4e00\u4e2a\u5168\u9762\u7684\u3001\u4e0d\u65ad\u53d1\u5c55\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u6301\u7eed\u6307\u4ee4\u8c03\u4f18\u4ee3\u7801\u5e93\uff0c\u76ee\u524d\u5df2\u5b9e\u73b08\u79cd\u4ee3\u8868\u6027\u7b97\u6cd5\uff0c\u5e76\u57282\u4e2a\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u7cfb\u7edf\u8bc4\u4f30\uff0c\u5c06\u6301\u7eed\u66f4\u65b0\u4ee5\u53cd\u6620\u8be5\u9886\u57df\u7684\u8fdb\u5c55\u3002"}}
{"id": "2508.07681", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07681", "abs": "https://arxiv.org/abs/2508.07681", "authors": ["Yooseok Lim", "ByoungJun Jeon", "Seong-A Park", "Jisoo Lee", "Sae Won Choi", "Chang Wook Jeong", "Ho-Geol Ryu", "Hongyeol Lee", "Hyun-Lim Yang"], "title": "MORE-CLEAR: Multimodal Offline Reinforcement learning for Clinical notes Leveraged Enhanced State Representation", "comment": "18 pages, 5 figures", "summary": "Sepsis, a life-threatening inflammatory response to infection, causes organ\ndysfunction, making early detection and optimal management critical. Previous\nreinforcement learning (RL) approaches to sepsis management rely primarily on\nstructured data, such as lab results or vital signs, and on a dearth of a\ncomprehensive understanding of the patient's condition. In this work, we\npropose a Multimodal Offline REinforcement learning for Clinical notes\nLeveraged Enhanced stAte Representation (MORE-CLEAR) framework for sepsis\ncontrol in intensive care units. MORE-CLEAR employs pre-trained large-scale\nlanguage models (LLMs) to facilitate the extraction of rich semantic\nrepresentations from clinical notes, preserving clinical context and improving\npatient state representation. Gated fusion and cross-modal attention allow\ndynamic weight adjustment in the context of time and the effective integration\nof multimodal data. Extensive cross-validation using two public (MIMIC-III and\nMIMIC-IV) and one private dataset demonstrates that MORE-CLEAR significantly\nimproves estimated survival rate and policy performance compared to\nsingle-modal RL approaches. To our knowledge, this is the first to leverage LLM\ncapabilities within a multimodal offline RL for better state representation in\nmedical applications. This approach can potentially expedite the treatment and\nmanagement of sepsis by enabling reinforcement learning models to propose\nenhanced actions based on a more comprehensive understanding of patient\nconditions.", "AI": {"tldr": "MORE-CLEAR\u6846\u67b6\u4f7f\u7528\u5927\u8bed\u8a00\u6a21\u578b\u5904\u7406\u4e34\u5e8a\u7b14\u8bb0\uff0c\u7ed3\u5408\u591a\u6a21\u6001\u6570\u636e\uff0c\u5728\u8113\u6bd2\u75c7\u7ba1\u7406\u4e2d\u63d0\u9ad8\u4e86\u751f\u5b58\u7387\u548c\u7b56\u7565\u8868\u73b0\u3002", "motivation": "\u8113\u6bd2\u75c7\u662f\u4e00\u79cd\u5371\u53ca\u751f\u547d\u7684\u611f\u67d3\u53cd\u5e94\uff0c\u4f1a\u5bfc\u81f4\u5668\u5b98\u529f\u80fd\u969c\u788d\uff0c\u56e0\u6b64\u65e9\u671f\u68c0\u6d4b\u548c\u6700\u4f73\u7ba1\u7406\u81f3\u5173\u91cd\u8981\u3002\u4ee5\u5f80\u7684\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u7ed3\u6784\u5316\u6570\u636e\uff0c\u7f3a\u4e4f\u5bf9\u60a3\u8005\u75c5\u60c5\u7684\u5168\u9762\u7406\u89e3\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMORE-CLEAR\u7684\u8113\u6bd2\u75c7\u7ba1\u7406\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u4ece\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u63d0\u53d6\u4e30\u5bcc\u7684\u8bed\u4e49\u8868\u5f81\uff0c\u4ee5\u589e\u5f3a\u60a3\u8005\u72b6\u6001\u7684\u8868\u793a\u3002\u6846\u67b6\u7ed3\u5408\u4e86\u95e8\u63a7\u878d\u5408\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u80fd\u591f\u52a8\u6001\u8c03\u6574\u65f6\u95f4\u80cc\u666f\u4e0b\u7684\u6743\u91cd\uff0c\u5e76\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u6570\u636e\u3002", "result": "\u901a\u8fc7\u5728MIMIC-III\u3001MIMIC-IV\u4ee5\u53ca\u4e00\u4e2a\u79c1\u6709\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u7684\u5927\u91cf\u4ea4\u53c9\u9a8c\u8bc1\uff0cMORE-CLEAR\u76f8\u6bd4\u5355\u4e00\u6a21\u6001\u7684RL\u65b9\u6cd5\uff0c\u80fd\u591f\u663e\u8457\u63d0\u9ad8\u4f30\u8ba1\u751f\u5b58\u7387\u548c\u7b56\u7565\u8868\u73b0\u3002", "conclusion": "MORE-CLEAR\u6846\u67b6\u901a\u8fc7\u5229\u7528\u9884\u8bad\u7ec3\u5927\u8bed\u8a00\u6a21\u578b\u63d0\u53d6\u4e34\u5e8a\u7b14\u8bb0\u4e2d\u7684\u8bed\u4e49\u4fe1\u606f\uff0c\u5e76\u7ed3\u5408\u95e8\u63a7\u878d\u5408\u548c\u8de8\u6a21\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u5bf9\u591a\u6a21\u6001\u6570\u636e\u7684\u6709\u6548\u6574\u5408\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u8113\u6bd2\u75c7\u7ba1\u7406\u7684\u751f\u5b58\u7387\u548c\u7b56\u7565\u8868\u73b0\uff0c\u662f\u533b\u7597\u9886\u57df\u9996\u4e2a\u5728\u5927\u8bed\u8a00\u6a21\u578b\u8d4b\u80fd\u7684\u591a\u6a21\u6001\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u9762\u7684\u5e94\u7528\u3002"}}
{"id": "2508.07312", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07312", "abs": "https://arxiv.org/abs/2508.07312", "authors": ["Min Yang", "Zihan Jia", "Zhilin Dai", "Sheng Guo", "Limin Wang"], "title": "MobileViCLIP: An Efficient Video-Text Model for Mobile Devices", "comment": "Accepted by ICCV2025", "summary": "Efficient lightweight neural networks are with increasing attention due to\ntheir faster reasoning speed and easier deployment on mobile devices. However,\nexisting video pre-trained models still focus on the common ViT architecture\nwith high latency, and few works attempt to build efficient architecture on\nmobile devices. This paper bridges this gap by introducing temporal structural\nreparameterization into an efficient image-text model and training it on a\nlarge-scale high-quality video-text dataset, resulting in an efficient\nvideo-text model that can run on mobile devices with strong zero-shot\nclassification and retrieval capabilities, termed as MobileViCLIP. In\nparticular, in terms of inference speed on mobile devices, our\nMobileViCLIP-Small is 55.4x times faster than InternVideo2-L14 and 6.7x faster\nthan InternVideo2-S14. In terms of zero-shot retrieval performance, our\nMobileViCLIP-Small obtains similar performance as InternVideo2-L14 and obtains\n6.9\\% better than InternVideo2-S14 on MSR-VTT. The code is available at\nhttps://github.com/MCG-NJU/MobileViCLIP.", "AI": {"tldr": "Efficient video-text model (MobileViCLIP) for mobile devices, achieving faster speeds and strong zero-shot performance.", "motivation": "Existing video pre-trained models primarily use the ViT architecture, which has high latency and is not suitable for mobile devices. This paper aims to bridge this gap by developing an efficient architecture for mobile video-text understanding.", "method": "The paper introduces temporal structural reparameterization into an efficient image-text model and trains it on a large-scale video-text dataset to create MobileViCLIP.", "result": "MobileViCLIP-Small is 55.4x faster than InternVideo2-L14 and 6.7x faster than InternVideo2-S14 on mobile devices. It achieves similar zero-shot retrieval performance as InternVideo2-L14 and outperforms InternVideo2-S14 by 6.9% on MSR-VTT.", "conclusion": "MobileViCLIP is an efficient video-text model designed for mobile devices, demonstrating strong zero-shot classification and retrieval capabilities. It achieves significantly faster inference speeds compared to existing models like InternVideo2, while maintaining competitive or improved performance."}}
{"id": "2508.07697", "categories": ["cs.LG", "cs.CE"], "pdf": "https://arxiv.org/pdf/2508.07697", "abs": "https://arxiv.org/abs/2508.07697", "authors": ["Hao Liu", "Chun Yang", "Zhang xiaoxing", "Xiaobin Zhu"], "title": "Semantic-Enhanced Time-Series Forecasting via Large Language Models", "comment": "14 pages,9 figures", "summary": "Time series forecasting plays a significant role in finance, energy,\nmeteorology, and IoT applications. Recent studies have leveraged the\ngeneralization capabilities of large language models (LLMs) to adapt to time\nseries forecasting, achieving promising performance. However, existing studies\nfocus on token-level modal alignment, instead of bridging the intrinsic\nmodality gap between linguistic knowledge structures and time series data\npatterns, greatly limiting the semantic representation. To address this issue,\nwe propose a novel Semantic-Enhanced LLM (SE-LLM) that explores the inherent\nperiodicity and anomalous characteristics of time series to embed into the\nsemantic space to enhance the token embedding. This process enhances the\ninterpretability of tokens for LLMs, thereby activating the potential of LLMs\nfor temporal sequence analysis. Moreover, existing Transformer-based LLMs excel\nat capturing long-range dependencies but are weak at modeling short-term\nanomalies in time-series data. Hence, we propose a plugin module embedded\nwithin self-attention that models long-term and short-term dependencies to\neffectively adapt LLMs to time-series analysis. Our approach freezes the LLM\nand reduces the sequence dimensionality of tokens, greatly reducing\ncomputational consumption. Experiments demonstrate the superiority performance\nof our SE-LLM against the state-of-the-art (SOTA) methods.", "AI": {"tldr": "SE-LLM \u901a\u8fc7\u589e\u5f3a LLM \u7684\u8bed\u4e49\u8868\u793a\u548c\u6539\u8fdb\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u4e2d\u7684\u4f9d\u8d56\u5efa\u6a21\u80fd\u529b\uff0c\u63d0\u5347\u4e86\u5176\u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\uff0c\u540c\u65f6\u964d\u4f4e\u4e86\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u7684 LLM \u5728\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u65b9\u9762\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728 token \u7ea7\u522b\u7684\u6a21\u5f0f\u5bf9\u9f50\uff0c\u672a\u80fd\u89e3\u51b3\u8bed\u8a00\u77e5\u8bc6\u7ed3\u6784\u4e0e\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u6a21\u5f0f\u4e4b\u95f4\u56fa\u6709\u7684\u6a21\u6001\u9e3f\u6c9f\uff0c\u8fd9\u4e25\u91cd\u9650\u5236\u4e86\u8bed\u4e49\u8868\u793a\u80fd\u529b\u3002\u6b64\u5916\uff0c\u73b0\u6709\u7684\u57fa\u4e8e Transformer \u7684 LLM \u5728\u5efa\u6a21\u77ed\u671f\u5f02\u5e38\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u8bed\u4e49\u589e\u5f3a LLM (SE-LLM)\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u5c06\u65f6\u95f4\u5e8f\u5217\u7684\u56fa\u6709\u5468\u671f\u6027\u548c\u5f02\u5e38\u7279\u5f81\u5d4c\u5165\u5230\u8bed\u4e49\u7a7a\u95f4\u6765\u589e\u5f3a token \u5d4c\u5165\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u5d4c\u5165\u5728\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4e2d\u7684\u63d2\u4ef6\u6a21\u5757\u6765\u540c\u65f6\u5904\u7406\u957f\u671f\u548c\u77ed\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\u8be5\u6a21\u578b\u51bb\u7ed3 LLM \u5e76\u964d\u4f4e token \u7684\u5e8f\u5217\u7ef4\u5ea6\uff0c\u4ece\u800c\u51cf\u5c11\u8ba1\u7b97\u6d88\u8017\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cSE-LLM \u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb (SOTA) \u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "SE-LLM \u901a\u8fc7\u5d4c\u5165\u65f6\u95f4\u5e8f\u5217\u7684\u5468\u671f\u6027\u548c\u5f02\u5e38\u7279\u5f81\u6765\u589e\u5f3a LLM \u7684\u8bed\u4e49\u8868\u793a\uff0c\u5e76\u7ed3\u5408\u63d2\u4ef6\u6a21\u5757\u6765\u5904\u7406\u77ed\u671f\u5f02\u5e38\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u4e8e SOTA \u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07313", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07313", "abs": "https://arxiv.org/abs/2508.07313", "authors": ["Junyu Xiong", "Yonghui Wang", "Weichao Zhao", "Chenyu Liu", "Bing Yin", "Wengang Zhou", "Houqiang Li"], "title": "DocR1: Evidence Page-Guided GRPO for Multi-Page Document Understanding", "comment": null, "summary": "Understanding multi-page documents poses a significant challenge for\nmultimodal large language models (MLLMs), as it requires fine-grained visual\ncomprehension and multi-hop reasoning across pages. While prior work has\nexplored reinforcement learning (RL) for enhancing advanced reasoning in MLLMs,\nits application to multi-page document understanding remains underexplored. In\nthis paper, we introduce DocR1, an MLLM trained with a novel RL framework,\nEvidence Page-Guided GRPO (EviGRPO). EviGRPO incorporates an evidence-aware\nreward mechanism that promotes a coarse-to-fine reasoning strategy, guiding the\nmodel to first retrieve relevant pages before generating answers. This training\nparadigm enables us to build high-quality models with limited supervision. To\nsupport this, we design a two-stage annotation pipeline and a curriculum\nlearning strategy, based on which we construct two datasets: EviBench, a\nhigh-quality training set with 4.8k examples, and ArxivFullQA, an evaluation\nbenchmark with 8.6k QA pairs based on scientific papers. Extensive experiments\nacross a wide range of benchmarks demonstrate that DocR1 achieves\nstate-of-the-art performance on multi-page tasks, while consistently\nmaintaining strong results on single-page benchmarks.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aDocR1\u7684\u65b0\u578b\u591a\u9875\u6587\u6863\u7406\u89e3\u6a21\u578b\uff0c\u91c7\u7528\u8bc1\u636e\u9875\u5f15\u5bfcGRPO\uff08EviGRPO\uff09\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u7b56\u7565\u548c\u8bc1\u636e\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u63d0\u9ad8\u4e86\u6a21\u578b\u6027\u80fd\u3002\u8be5\u6a21\u578b\u5728EviBench\u548cArxivFullQA\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u5e76\u5728\u591a\u9875\u548c\u5355\u9875\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u591a\u9875\u6587\u6863\u7406\u89e3\u5bf9\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u9700\u8981\u7ec6\u7c92\u5ea6\u7684\u89c6\u89c9\u7406\u89e3\u548c\u8de8\u9875\u9762\u7684\u591a\u8df3\u63a8\u7406\u3002\u867d\u7136\u4e4b\u524d\u7684\u7814\u7a76\u63a2\u7d22\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u6765\u589e\u5f3aMLLMs\u7684\u9ad8\u7ea7\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u5176\u5728\u591a\u9875\u6587\u6863\u7406\u89e3\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u672a\u5f97\u5230\u5145\u5206\u63a2\u7d22\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\uff0c\u79f0\u4e3a\u8bc1\u636e\u9875\u5f15\u5bfcGRPO\uff08EviGRPO\uff09\uff0c\u5e76\u7ed3\u5408\u4e86\u8bc1\u636e\u611f\u77e5\u5956\u52b1\u673a\u5236\uff0c\u4ee5\u4fc3\u8fdb\u7c97\u7c92\u5ea6\u5230\u7ec6\u7c92\u5ea6\u7684\u63a8\u7406\u7b56\u7565\uff0c\u9996\u5148\u68c0\u7d22\u76f8\u5173\u9875\u9762\uff0c\u7136\u540e\u751f\u6210\u7b54\u6848\u3002\u8be5\u8bad\u7ec3\u8303\u5f0f\u8fd8\u5305\u62ec\u4e00\u4e2a\u4e24\u9636\u6bb5\u7684\u6807\u6ce8\u6d41\u7a0b\u548c\u4e00\u4e2a\u8bfe\u7a0b\u5b66\u4e60\u7b56\u7565\uff0c\u7528\u4e8e\u6784\u5efaEviBench\u548cArxivFullQA\u6570\u636e\u96c6\u3002", "result": "DocR1\u5728\u5e7f\u6cdb\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5355\u9875\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u4fdd\u6301\u4e86\u6301\u7eed\u7684\u5f3a\u52b2\u7ed3\u679c\u3002", "conclusion": "DocR1\u5728\u591a\u9875\u6587\u6863\u7406\u89e3\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5728\u5355\u9875\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u4e5f\u4fdd\u6301\u4e86\u5f3a\u5927\u7684\u7ed3\u679c\u3002"}}
{"id": "2508.07706", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07706", "abs": "https://arxiv.org/abs/2508.07706", "authors": ["Philipp Huber", "David Li", "Juan Pedro Guti\u00e9rrez Hermosillo Muriedas", "Deifilia Kieckhefen", "Markus G\u00f6tz", "Achim Streit", "Charlotte Debus"], "title": "Energy Consumption in Parallel Neural Network Training", "comment": null, "summary": "The increasing demand for computational resources of training neural networks\nleads to a concerning growth in energy consumption. While parallelization has\nenabled upscaling model and dataset sizes and accelerated training, its impact\non energy consumption is often overlooked. To close this research gap, we\nconducted scaling experiments for data-parallel training of two models,\nResNet50 and FourCastNet, and evaluated the impact of parallelization\nparameters, i.e., GPU count, global batch size, and local batch size, on\npredictive performance, training time, and energy consumption. We show that\nenergy consumption scales approximately linearly with the consumed resources,\ni.e., GPU hours; however, the respective scaling factor differs substantially\nbetween distinct model trainings and hardware, and is systematically influenced\nby the number of samples and gradient updates per GPU hour. Our results shed\nlight on the complex interplay of scaling up neural network training and can\ninform future developments towards more sustainable AI research.", "AI": {"tldr": "\u8be5\u7814\u7a76\u901a\u8fc7\u6269\u5c55\u5b9e\u9a8c\uff0c\u8c03\u67e5\u4e86\u5e76\u884c\u5316\u5bf9\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u7684\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5728\u80fd\u6e90\u6d88\u8017\u4e0e GPU \u5c0f\u65f6\u4e4b\u95f4\u5b58\u5728\u8fd1\u4f3c\u7ebf\u6027\u7684\u5173\u7cfb\uff0c\u4f46\u5177\u4f53\u5173\u7cfb\u56e0\u6a21\u578b\u548c\u786c\u4ef6\u800c\u5f02\u3002", "motivation": "\u6587\u7ae0\u65e8\u5728\u89e3\u51b3\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u65e5\u76ca\u589e\u957f\u7684\u8ba1\u7b97\u8d44\u6e90\u9700\u6c42\u6240\u5e26\u6765\u7684\u4ee4\u4eba\u62c5\u5fe7\u7684\u80fd\u6e90\u6d88\u8017\u589e\u957f\u95ee\u9898\uff0c\u5e76\u89e3\u51b3\u5728\u6269\u5c55\u6a21\u578b\u548c\u6570\u636e\u96c6\u5927\u5c0f\u4ee5\u53ca\u52a0\u901f\u8bad\u7ec3\u65b9\u9762\uff0c\u5e76\u884c\u5316\u5bf9\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\u88ab\u5ffd\u89c6\u7684\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u8fd0\u884c\u6269\u5c55\u5b9e\u9a8c\uff0c\u91cd\u70b9\u5173\u6ce8\u6570\u636e\u5e76\u884c\u8bad\u7ec3\uff0c\u5e76\u8bc4\u4f30\u4e86 GPU \u6570\u91cf\u3001\u5168\u5c40\u6279\u6b21\u5927\u5c0f\u548c\u5c40\u90e8\u6279\u6b21\u5927\u5c0f\u7b49\u5e76\u884c\u5316\u53c2\u6570\u5bf9\u9884\u6d4b\u6027\u80fd\u3001\u8bad\u7ec3\u65f6\u95f4\u548c\u80fd\u6e90\u6d88\u8017\u7684\u5f71\u54cd\u3002", "result": "\u7814\u7a76\u8868\u660e\uff0c\u80fd\u6e90\u6d88\u8017\u4e0e\u6240\u6d88\u8017\u7684\u8d44\u6e90\uff08\u5373 GPU \u5c0f\u65f6\uff09\u5927\u81f4\u5448\u7ebf\u6027\u589e\u957f\uff1b\u4f46\u662f\uff0c\u6bcf\u4e2a GPU \u5c0f\u65f6\u7684\u6837\u672c\u548c\u68af\u5ea6\u66f4\u65b0\u7684\u6570\u91cf\u4f1a\u7cfb\u7edf\u5730\u5f71\u54cd\u5404\u81ea\u7684\u589e\u957f\u56e0\u5b50\uff0c\u800c\u8fd9\u5728\u4e0d\u540c\u7684\u6a21\u578b\u8bad\u7ec3\u548c\u786c\u4ef6\u4e4b\u95f4\u5b58\u5728\u663e\u8457\u5dee\u5f02\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u6269\u5c55\u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u4e0e\u5b9e\u73b0\u66f4\u53ef\u6301\u7eed\u7684\u4eba\u5de5\u667a\u80fd\u7814\u7a76\u4e4b\u95f4\u590d\u6742\u800c\u5fae\u5999\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5176\u7ed3\u679c\u53ef\u4e3a\u672a\u6765\u7684\u53d1\u5c55\u63d0\u4f9b\u4fe1\u606f\u3002"}}
{"id": "2508.07318", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07318", "abs": "https://arxiv.org/abs/2508.07318", "authors": ["Jinjing Gu", "Tianbao Qin", "Yuanyuan Pu", "Zhengpeng Zhao"], "title": "RORPCap: Retrieval-based Objects and Relations Prompt for Image Captioning", "comment": null, "summary": "Image captioning aims to generate natural language descriptions for input\nimages in an open-form manner. To accurately generate descriptions related to\nthe image, a critical step in image captioning is to identify objects and\nunderstand their relations within the image. Modern approaches typically\ncapitalize on object detectors or combine detectors with Graph Convolutional\nNetwork (GCN). However, these models suffer from redundant detection\ninformation, difficulty in GCN construction, and high training costs. To\naddress these issues, a Retrieval-based Objects and Relations Prompt for Image\nCaptioning (RORPCap) is proposed, inspired by the fact that image-text\nretrieval can provide rich semantic information for input images. RORPCap\nemploys an Objects and relations Extraction Model to extract object and\nrelation words from the image. These words are then incorporate into predefined\nprompt templates and encoded as prompt embeddings. Next, a Mamba-based mapping\nnetwork is designed to quickly map image embeddings extracted by CLIP to\nvisual-text embeddings. Finally, the resulting prompt embeddings and\nvisual-text embeddings are concatenated to form textual-enriched feature\nembeddings, which are fed into a GPT-2 model for caption generation. Extensive\nexperiments conducted on the widely used MS-COCO dataset show that the RORPCap\nrequires only 2.6 hours under cross-entropy loss training, achieving 120.5%\nCIDEr score and 22.0% SPICE score on the \"Karpathy\" test split. RORPCap\nachieves comparable performance metrics to detector-based and GCN-based models\nwith the shortest training time and demonstrates its potential as an\nalternative for image captioning.", "AI": {"tldr": "RORPCap \u662f\u4e00\u79cd\u65b0\u9896\u7684\u56fe\u50cf\u5b57\u5e55\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u7684\u5bf9\u8c61\u548c\u5173\u7cfb\u6765\u589e\u5f3a\u5b57\u5e55\u751f\u6210\u3002\u5b83\u4f7f\u7528\u57fa\u4e8e Mamba \u7684\u6620\u5c04\u7f51\u7edc\u5c06 CLIP \u56fe\u50cf\u5d4c\u5165\u6620\u5c04\u5230\u89c6\u89c9-\u6587\u672c\u5d4c\u5165\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u5d4c\u5165\u4e0e\u63d0\u793a\u5d4c\u5165\u76f8\u7ed3\u5408\uff0c\u5e76\u9988\u9001\u5230 GPT-2 \u6a21\u578b\u4e2d\u8fdb\u884c\u5b57\u5e55\u751f\u6210\u3002RORPCap \u5728\u8bad\u7ec3\u65f6\u95f4\u548c\u6027\u80fd\u65b9\u9762\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u6a21\u578b\uff08\u901a\u5e38\u5229\u7528\u5bf9\u8c61\u68c0\u6d4b\u5668\u6216\u7ed3\u5408\u68c0\u6d4b\u5668\u4e0e\u56fe\u5377\u79ef\u7f51\u7edc\uff08GCN\uff09\uff09\u5b58\u5728\u7684\u5197\u4f59\u68c0\u6d4b\u4fe1\u606f\u3001GCN \u6784\u5efa\u56f0\u96be\u548c\u8bad\u7ec3\u6210\u672c\u9ad8\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u68c0\u7d22\u7684\u5bf9\u8c61\u548c\u5173\u7cfb\u63d0\u793a\u7684\u56fe\u50cf\u5b57\u5e55\uff08RORPCap\uff09\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4ece\u56fe\u50cf-\u6587\u672c\u68c0\u7d22\u4e2d\u6c72\u53d6\u7075\u611f\uff0c\u4e3a\u8f93\u5165\u56fe\u50cf\u63d0\u4f9b\u4e30\u5bcc\u7684\u8bed\u4e49\u4fe1\u606f\u3002", "method": "RORPCap \u91c7\u7528\u5bf9\u8c61\u548c\u5173\u7cfb\u63d0\u53d6\u6a21\u578b\u4ece\u56fe\u50cf\u4e2d\u63d0\u53d6\u5bf9\u8c61\u548c\u5173\u7cfb\u8bcd\uff0c\u7136\u540e\u5c06\u8fd9\u4e9b\u8bcd\u5408\u5e76\u5230\u9884\u5b9a\u4e49\u7684\u63d0\u793a\u6a21\u677f\u4e2d\u5e76\u7f16\u7801\u4e3a\u63d0\u793a\u5d4c\u5165\u3002\u63a5\u4e0b\u6765\uff0c\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u57fa\u4e8e Mamba \u7684\u6620\u5c04\u7f51\u7edc\uff0c\u4ee5\u5feb\u901f\u5c06 CLIP \u63d0\u53d6\u7684\u56fe\u50cf\u5d4c\u5165\u6620\u5c04\u5230\u89c6\u89c9-\u6587\u672c\u5d4c\u5165\u3002\u6700\u540e\uff0c\u5c06\u751f\u6210\u7684\u63d0\u793a\u5d4c\u5165\u548c\u89c6\u89c9-\u6587\u672c\u5d4c\u5165\u8fde\u63a5\u8d77\u6765\uff0c\u5f62\u6210\u6587\u672c\u4e30\u5bcc\u7684\u7279\u5f81\u5d4c\u5165\uff0c\u5e76\u5c06\u5176\u8f93\u5165 GPT-2 \u6a21\u578b\u4ee5\u751f\u6210\u5b57\u5e55\u3002", "result": "RORPCap \u5728 MS-COCO \u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5728\u201cKarpathy\u201d\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5728\u4ea4\u53c9\u71b5\u635f\u5931\u8bad\u7ec3\u4e0b\u4ec5\u9700 2.6 \u5c0f\u65f6\u5373\u53ef\u8fbe\u5230 120.5% \u7684 CIDEr \u5206\u6570\u548c 22.0% \u7684 SPICE \u5206\u6570\u3002", "conclusion": "RORPCap \u8fbe\u5230\u4e86\u4e0e\u57fa\u4e8e\u68c0\u6d4b\u5668\u548c\u57fa\u4e8e GCN \u7684\u6a21\u578b\u76f8\u5f53\u7684\u6027\u80fd\u6307\u6807\uff0c\u540c\u65f6\u5177\u6709\u6700\u77ed\u7684\u8bad\u7ec3\u65f6\u95f4\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u4f5c\u4e3a\u56fe\u50cf\u5b57\u5e55\u66ff\u4ee3\u54c1\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07710", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07710", "abs": "https://arxiv.org/abs/2508.07710", "authors": ["Jingya Wang", "Xin Deng", "Wenjie Wei", "Dehao Zhang", "Shuai Wang", "Qian Sun", "Jieyuan Zhang", "Hanwen Liu", "Ning Xie", "Malu Zhang"], "title": "Training-Free ANN-to-SNN Conversion for High-Performance Spiking Transformer", "comment": "Under review", "summary": "Leveraging the event-driven paradigm, Spiking Neural Networks (SNNs) offer a\npromising approach for constructing energy-efficient Transformer architectures.\nCompared to directly trained Spiking Transformers, ANN-to-SNN conversion\nmethods bypass the high training costs. However, existing methods still suffer\nfrom notable limitations, failing to effectively handle nonlinear operations in\nTransformer architectures and requiring additional fine-tuning processes for\npre-trained ANNs. To address these issues, we propose a high-performance and\ntraining-free ANN-to-SNN conversion framework tailored for Transformer\narchitectures. Specifically, we introduce a Multi-basis Exponential Decay (MBE)\nneuron, which employs an exponential decay strategy and multi-basis encoding\nmethod to efficiently approximate various nonlinear operations. It removes the\nrequirement for weight modifications in pre-trained ANNs. Extensive experiments\nacross diverse tasks (CV, NLU, NLG) and mainstream Transformer architectures\n(ViT, RoBERTa, GPT-2) demonstrate that our method achieves near-lossless\nconversion accuracy with significantly lower latency. This provides a promising\npathway for the efficient and scalable deployment of Spiking Transformers in\nreal-world applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMBE\u7684\u65b0\u578b\u795e\u7ecf\u5143\uff0c\u7528\u4e8e\u65e0\u9700\u8bad\u7ec3\u7684ANN\u5230SNN\u8f6c\u6362\uff0c\u53ef\u9ad8\u6548\u5904\u7406Transformer\u4e2d\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u5b9e\u73b0\u9ad8\u7cbe\u5ea6\u548c\u4f4e\u5ef6\u8fdf\u3002", "motivation": "\u73b0\u6709ANN\u5230SNN\u8f6c\u6362\u65b9\u6cd5\u5728\u5904\u7406Transformer\u67b6\u6784\u4e2d\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\u548c\u5bf9\u9884\u8bad\u7ec3ANN\u8fdb\u884c\u5fae\u8c03\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\uff0c\u8bad\u7ec3\u6210\u672c\u9ad8\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684ANN\u5230SNN\u8f6c\u6362\u6846\u67b6\uff0c\u5e76\u5f15\u5165\u4e86\u591a\u57fa\u6307\u6570\u8870\u51cf\uff08MBE\uff09\u795e\u7ecf\u5143\uff0c\u8be5\u795e\u7ecf\u5143\u91c7\u7528\u6307\u6570\u8870\u51cf\u7b56\u7565\u548c\u591a\u57fa\u7f16\u7801\u65b9\u6cd5\u6765\u8fd1\u4f3c\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u65e0\u9700\u5bf9\u9884\u8bad\u7ec3ANN\u7684\u6743\u91cd\u8fdb\u884c\u4fee\u6539\u3002", "result": "\u5728CV\u3001NLU\u3001NLG\u7b49\u591a\u79cd\u4efb\u52a1\u548c\u4e3b\u6d41Transformer\u67b6\u6784\uff08ViT\u3001RoBERTa\u3001GPT-2\uff09\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5b9e\u73b0\u4e86\u8fd1\u4e4e\u65e0\u635f\u7684\u8f6c\u6362\u7cbe\u5ea6\u548c\u663e\u8457\u964d\u4f4e\u7684\u5ef6\u8fdf\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u7684\u65e0\u9700\u8bad\u7ec3\u7684\u3001\u9488\u5bf9Transformer\u67b6\u6784\u7684ANN\u5230SNN\u8f6c\u6362\u6846\u67b6\uff0c\u80fd\u591f\u4ee5\u8fd1\u4e4e\u65e0\u635f\u7684\u7cbe\u5ea6\u548c\u663e\u8457\u964d\u4f4e\u7684\u5ef6\u8fdf\uff0c\u6709\u6548\u5730\u5904\u7406Transformer\u4e2d\u7684\u975e\u7ebf\u6027\u64cd\u4f5c\uff0c\u4e3a\u73b0\u5b9e\u4e16\u754c\u4e2dSpiking Transformer\u7684\u9ad8\u6548\u548c\u53ef\u6269\u5c55\u90e8\u7f72\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.07330", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07330", "abs": "https://arxiv.org/abs/2508.07330", "authors": ["Tuyen Tran", "Thao Minh Le", "Quang-Hung Le", "Truyen Tran"], "title": "Planner-Refiner: Dynamic Space-Time Refinement for Vision-Language Alignment in Videos", "comment": "Accepted for publication at ECAI 2025", "summary": "Vision-language alignment in video must address the complexity of language,\nevolving interacting entities, their action chains, and semantic gaps between\nlanguage and vision. This work introduces Planner-Refiner, a framework to\novercome these challenges. Planner-Refiner bridges the semantic gap by\niteratively refining visual elements' space-time representation, guided by\nlanguage until semantic gaps are minimal. A Planner module schedules language\nguidance by decomposing complex linguistic prompts into short sentence chains.\nThe Refiner processes each short sentence, a noun-phrase and verb-phrase pair,\nto direct visual tokens' self-attention across space then time, achieving\nefficient single-step refinement. A recurrent system chains these steps,\nmaintaining refined visual token representations. The final representation\nfeeds into task-specific heads for alignment generation. We demonstrate\nPlanner-Refiner's effectiveness on two video-language alignment tasks:\nReferring Video Object Segmentation and Temporal Grounding with varying\nlanguage complexity. We further introduce a new MeViS-X benchmark to assess\nmodels' capability with long queries. Superior performance versus\nstate-of-the-art methods on these benchmarks shows the approach's potential,\nespecially for complex prompts.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPlanner-Refiner\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u89e3\u51b3\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u4e2d\u7684\u590d\u6742\u6027\u95ee\u9898\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5c06\u590d\u6742\u8bed\u8a00\u5206\u89e3\u4e3a\u77ed\u53e5\uff0c\u5e76\u8fed\u4ee3\u5730\u7ec6\u5316\u89c6\u89c9\u8868\u793a\u6765\u5f25\u5408\u8bed\u4e49\u9e3f\u6c9f\uff0c\u5728\u591a\u4e2a\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u4efb\u52a1\u548c\u65b0\u7684\u957f\u67e5\u8be2\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5747\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u9700\u8981\u5904\u7406\u8bed\u8a00\u7684\u590d\u6742\u6027\u3001\u4e0d\u65ad\u53d8\u5316\u7684\u4ea4\u4e92\u5b9e\u4f53\u3001\u52a8\u4f5c\u94fe\u4ee5\u53ca\u8bed\u8a00\u4e0e\u89c6\u89c9\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002\u73b0\u6709\u7684\u65b9\u6cd5\u5728\u5e94\u5bf9\u8fd9\u4e9b\u6311\u6218\u65f6\u5b58\u5728\u4e0d\u8db3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aPlanner-Refiner\u7684\u6846\u67b6\uff0c\u5305\u62ec\u4e00\u4e2aPlanner\u6a21\u5757\u7528\u4e8e\u5c06\u590d\u6742\u8bed\u8a00\u63d0\u793a\u5206\u89e3\u4e3a\u77ed\u53e5\u94fe\uff0c\u4ee5\u53ca\u4e00\u4e2aRefiner\u6a21\u5757\u7528\u4e8e\u5904\u7406\u6bcf\u4e2a\u77ed\u53e5\uff08\u540d\u8bcd-\u52a8\u8bcd\u77ed\u8bed\u5bf9\uff09\uff0c\u901a\u8fc7\u5f15\u5bfc\u89c6\u89c9\u4ee4\u724c\u7684\u8de8\u7a7a\u95f4\u548c\u65f6\u95f4\u81ea\u6ce8\u610f\u529b\u6765\u8fed\u4ee3\u5f0f\u5730\u7ec6\u5316\u65f6\u7a7a\u8868\u793a\uff0c\u4ee5\u5f25\u5408\u8bed\u8a00\u4e0e\u89c6\u89c9\u95f4\u7684\u8bed\u4e49\u9e3f\u6c9f\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u5faa\u73af\u7cfb\u7edf\u7ef4\u6301\u7ec6\u5316\u540e\u7684\u89c6\u89c9\u4ee4\u724c\u8868\u793a\uff0c\u5e76\u7528\u4e8e\u4e0b\u6e38\u4efb\u52a1\u3002", "result": "Planner-Refiner\u5728Referring Video Object Segmentation\u548cTemporal Grounding\u4e24\u4e2a\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u4efb\u52a1\u4e0a\u5747\u53d6\u5f97\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u65b0\u7684MeViS-X\u57fa\u51c6\u6d4b\u8bd5\uff08\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5904\u7406\u957f\u67e5\u8be2\u7684\u80fd\u529b\uff09\u4e0a\u4e5f\u8868\u73b0\u51fa\u8272\u3002", "conclusion": "Planner-Refiner\u5728\u5904\u7406\u89c6\u9891-\u8bed\u8a00\u5bf9\u9f50\u4efb\u52a1\u65f6\u5c55\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u590d\u6742\u957f\u67e5\u8be2\u65b9\u9762\uff0c\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u3002"}}
{"id": "2508.07713", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07713", "abs": "https://arxiv.org/abs/2508.07713", "authors": ["Jinghan Yang", "Jiayu Weng"], "title": "Detecting Mislabeled and Corrupted Data via Pointwise Mutual Information", "comment": "Under Working", "summary": "Deep neural networks can memorize corrupted labels, making data quality\ncritical for model performance, yet real-world datasets are frequently\ncompromised by both label noise and input noise. This paper proposes a mutual\ninformation-based framework for data selection under hybrid noise scenarios\nthat quantifies statistical dependencies between inputs and labels. We compute\neach sample's pointwise contribution to the overall mutual information and find\nthat lower contributions indicate noisy or mislabeled instances. Empirical\nvalidation on MNIST with different synthetic noise settings demonstrates that\nthe method effectively filters low-quality samples. Under label corruption,\ntraining on high-MI samples improves classification accuracy by up to 15\\%\ncompared to random sampling. Furthermore, the method exhibits robustness to\nbenign input modifications, preserving semantically valid data while filtering\ntruly corrupted samples.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u6df7\u5408\u566a\u58f0\uff08\u6807\u7b7e\u566a\u58f0\u548c\u8f93\u5165\u566a\u58f0\uff09\u573a\u666f\u4e0b\u9009\u62e9\u9ad8\u8d28\u91cf\u6570\u636e\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8ba1\u7b97\u6837\u672c\u5bf9\u4e92\u4fe1\u606f\u7684\u8d21\u732e\u5ea6\u6765\u8bc6\u522b\u566a\u58f0\u6837\u672c\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u8bb0\u4f4f\u635f\u574f\u7684\u6807\u7b7e\uff0c\u5bfc\u81f4\u6570\u636e\u8d28\u91cf\u5bf9\u6a21\u578b\u6027\u80fd\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u5b9e\u4e16\u754c\u7684\u6570\u636e\u96c6\u7ecf\u5e38\u540c\u65f6\u5b58\u5728\u6807\u7b7e\u566a\u58f0\u548c\u8f93\u5165\u566a\u58f0\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u4e92\u4fe1\u606f\u7684\u6570\u636e\u9009\u62e9\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u91cf\u5316\u4e86\u8f93\u5165\u548c\u6807\u7b7e\u4e4b\u95f4\u7684\u7edf\u8ba1\u4f9d\u8d56\u6027\u3002\u901a\u8fc7\u8ba1\u7b97\u6bcf\u4e2a\u6837\u672c\u5bf9\u6574\u4f53\u4e92\u4fe1\u606f\u7684\u8d21\u732e\u5ea6\uff0c\u8ba4\u4e3a\u8d21\u732e\u5ea6\u8f83\u4f4e\u7684\u6837\u672c\u662f\u566a\u58f0\u6216\u9519\u8bef\u6807\u7b7e\u7684\u6837\u672c\u3002", "result": "\u5728MNIST\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u4e0d\u540c\u5408\u6210\u566a\u58f0\u8bbe\u7f6e\u4e0b\u7684\u7ecf\u9a8c\u9a8c\u8bc1\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u6837\u672c\uff0c\u5e76\u4e14\u5728\u6807\u7b7e\u635f\u574f\u7684\u60c5\u51b5\u4e0b\uff0c\u8bad\u7ec3\u9ad8\u8d28\u91cf\u6837\u672c\u7684\u5206\u7c7b\u7cbe\u5ea6\u76f8\u8f83\u4e8e\u968f\u673a\u91c7\u6837\u6700\u9ad8\u53ef\u63d0\u534715%\u3002\u8be5\u65b9\u6cd5\u5bf9\u826f\u6027\u8f93\u5165\u4fee\u6539\u5177\u6709\u9c81\u68d2\u6027\uff0c\u80fd\u591f\u4fdd\u7559\u8bed\u4e49\u6709\u6548\u7684\u800c\u6570\u636e\uff0c\u540c\u65f6\u8fc7\u6ee4\u6389\u771f\u6b63\u635f\u574f\u7684\u6837\u672c\u3002"}}
{"id": "2508.07341", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07341", "abs": "https://arxiv.org/abs/2508.07341", "authors": ["Fangtai Wu", "Mushui Liu", "Weijie He", "Wanggui He", "Hao Jiang", "Zhao Wang", "Yunlong Yu"], "title": "CoAR: Concept Injection into Autoregressive Models for Personalized Text-to-Image Generation", "comment": null, "summary": "The unified autoregressive (AR) model excels at multimodal understanding and\ngeneration, but its potential for customized image generation remains\nunderexplored. Existing customized generation methods rely on full fine-tuning\nor adapters, making them costly and prone to overfitting or catastrophic\nforgetting. In this paper, we propose \\textbf{CoAR}, a novel framework for\ninjecting subject concepts into the unified AR models while keeping all\npre-trained parameters completely frozen. CoAR learns effective, specific\nsubject representations with only a minimal number of parameters using a\nLayerwise Multimodal Context Learning strategy. To address overfitting and\nlanguage drift, we further introduce regularization that preserves the\npre-trained distribution and anchors context tokens to improve subject fidelity\nand re-contextualization. Additionally, CoAR supports training-free subject\ncustomization in a user-provided style. Experiments demonstrate that CoAR\nachieves superior performance on both subject-driven personalization and style\npersonalization, while delivering significant gains in computational and memory\nefficiency. Notably, CoAR tunes less than \\textbf{0.05\\%} of the parameters\nwhile achieving competitive performance compared to recent Proxy-Tuning. Code:\nhttps://github.com/KZF-kzf/CoAR", "AI": {"tldr": "CoAR\u662f\u4e00\u79cd\u521b\u65b0\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5c06\u4e3b\u9898\u6982\u5ff5\u6ce8\u5165\u7edf\u4e00\u7684\u81ea\u56de\u5f52\u6a21\u578b\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\u3002\u5b83\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u53c2\u6570\u3001\u4f7f\u7528\u5c11\u91cf\u53c2\u6570\u7684\u5c42\u7ea7\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\u4ee5\u53ca\u6b63\u5219\u5316\u6280\u672f\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u75db\u70b9\uff0c\u5e76\u5728\u6027\u80fd\u548c\u6548\u7387\u4e0a\u5747\u4f18\u4e8eSOTA\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\u65b9\u6cd5\uff08\u5982\u5168\u5fae\u8c03\u6216\u9002\u914d\u5668\uff09\u6210\u672c\u9ad8\u6602\uff0c\u4e14\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\u6216\u707e\u96be\u6027\u9057\u5fd8\u3002\u672c\u7814\u7a76\u65e8\u5728\u63a2\u7d22\u7edf\u4e00\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\u5728\u5b9a\u5236\u5316\u56fe\u50cf\u751f\u6210\u65b9\u9762\u7684\u6f5c\u529b\uff0c\u5e76\u63d0\u51fa\u4e00\u79cd\u66f4\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCoAR\u7684\u65b0\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u901a\u8fc7\u5c42\u7ea7\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\uff0c\u4ec5\u7528\u5c11\u91cf\u53c2\u6570\u5c31\u80fd\u5c06\u4e3b\u9898\u6982\u5ff5\u6ce8\u5165\u7edf\u4e00\u7684\u81ea\u56de\u5f52\uff08AR\uff09\u6a21\u578b\uff0c\u540c\u65f6\u4fdd\u6301\u6240\u6709\u9884\u8bad\u7ec3\u53c2\u6570\u5b8c\u5168\u51bb\u7ed3\u3002\u4e3a\u4e86\u89e3\u51b3\u8fc7\u62df\u5408\u548c\u8bed\u8a00\u6f02\u79fb\u95ee\u9898\uff0c\u8fd8\u5f15\u5165\u4e86\u4fdd\u7559\u9884\u8bad\u7ec3\u5206\u5e03\u7684\u6b63\u5219\u5316\uff0c\u5e76\u951a\u5b9a\u4e0a\u4e0b\u6587\u4ee4\u724c\u4ee5\u63d0\u9ad8\u4e3b\u9898\u4fdd\u771f\u5ea6\u548c\u518d\u4e0a\u4e0b\u6587\u5316\u80fd\u529b\u3002\u6b64\u5916\uff0cCoAR\u652f\u6301\u514d\u8bad\u7ec3\u7684\u4e3b\u9898\u5b9a\u5236\u548c\u7528\u6237\u63d0\u4f9b\u7684\u98ce\u683c\u5b9a\u5236\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cCoAR\u5728\u4e3b\u9898\u9a71\u52a8\u7684\u4e2a\u6027\u5316\u548c\u98ce\u683c\u4e2a\u6027\u5316\u65b9\u9762\u5747\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u5728\u8ba1\u7b97\u548c\u5185\u5b58\u6548\u7387\u65b9\u9762\u5e26\u6765\u4e86\u663e\u8457\u7684\u63d0\u5347\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0cCoAR\u4ec5\u8c03\u6574\u4e0d\u52300.05%\u7684\u53c2\u6570\uff0c\u5374\u80fd\u8fbe\u5230\u4e0e\u6700\u8fd1\u7684Proxy-Tuning\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u7684\u6027\u80fd\u3002", "conclusion": "CoAR\u6846\u67b6\u901a\u8fc7\u51bb\u7ed3\u9884\u8bad\u7ec3\u53c2\u6570\u3001\u4f7f\u7528\u5c11\u91cf\u53c2\u6570\u7684\u5c42\u7ea7\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7b56\u7565\uff0c\u5e76\u7ed3\u5408\u6b63\u5219\u5316\u6280\u672f\uff0c\u5728\u56fe\u50cf\u751f\u6210\u9886\u57df\u5b9e\u73b0\u4e86\u9ad8\u6548\u3001\u4f4e\u6210\u672c\u7684\u4e2a\u6027\u5316\u5b9a\u5236\uff0c\u5e76\u5728\u4e3b\u9898\u9a71\u52a8\u548c\u98ce\u683c\u4e2a\u6027\u5316\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07346", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07346", "abs": "https://arxiv.org/abs/2508.07346", "authors": ["Tingyu Yang", "Jue Gong", "Jinpei Guo", "Wenbo Li", "Yong Guo", "Yulun Zhang"], "title": "SODiff: Semantic-Oriented Diffusion Model for JPEG Compression Artifacts Removal", "comment": "7 pages, 5 figures. The code will be available at\n  \\url{https://github.com/frakenation/SODiff}", "summary": "JPEG, as a widely used image compression standard, often introduces severe\nvisual artifacts when achieving high compression ratios. Although existing deep\nlearning-based restoration methods have made considerable progress, they often\nstruggle to recover complex texture details, resulting in over-smoothed\noutputs. To overcome these limitations, we propose SODiff, a novel and\nefficient semantic-oriented one-step diffusion model for JPEG artifacts\nremoval. Our core idea is that effective restoration hinges on providing\nsemantic-oriented guidance to the pre-trained diffusion model, thereby fully\nleveraging its powerful generative prior. To this end, SODiff incorporates a\nsemantic-aligned image prompt extractor (SAIPE). SAIPE extracts rich features\nfrom low-quality (LQ) images and projects them into an embedding space\nsemantically aligned with that of the text encoder. Simultaneously, it\npreserves crucial information for faithful reconstruction. Furthermore, we\npropose a quality factor-aware time predictor that implicitly learns the\ncompression quality factor (QF) of the LQ image and adaptively selects the\noptimal denoising start timestep for the diffusion process. Extensive\nexperimental results show that our SODiff outperforms recent leading methods in\nboth visual quality and quantitative metrics. Code is available at:\nhttps://github.com/frakenation/SODiff", "AI": {"tldr": "SODiff \u662f\u4e00\u79cd\u9762\u5411\u8bed\u4e49\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u901a\u8fc7 SAIPE \u63d0\u53d6\u8bed\u4e49\u7279\u5f81\u5e76\u5229\u7528\u9884\u8bad\u7ec3\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u5148\u9a8c\u6765\u53bb\u9664 JPEG \u4f2a\u5f71\uff0c\u540c\u65f6\u901a\u8fc7\u8d28\u91cf\u56e0\u5b50\u611f\u77e5\u65f6\u95f4\u9884\u6d4b\u5668\u81ea\u9002\u5e94\u53bb\u566a\uff0c\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u89c6\u89c9\u548c\u91cf\u5316\u6548\u679c\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u53bb\u9664 JPEG \u4f2a\u5f71\u65f6\u96be\u4ee5\u6062\u590d\u590d\u6742\u7eb9\u7406\u7ec6\u8282\u5bfc\u81f4\u8f93\u51fa\u8fc7\u5ea6\u5e73\u6ed1\u7684\u5c40\u9650\u6027\uff0c\u63d0\u51fa SODiff \u6a21\u578b\u3002", "method": "SODiff \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u9ad8\u6548\u7684\u9762\u5411\u8bed\u4e49\u7684\u5355\u6b65\u6269\u6563\u6a21\u578b\uff0c\u7528\u4e8e\u53bb\u9664 JPEG \u4f2a\u5f71\u3002\u8be5\u6a21\u578b\u901a\u8fc7\u4e00\u4e2a\u8bed\u4e49\u5bf9\u9f50\u56fe\u50cf\u63d0\u793a\u63d0\u53d6\u5668\uff08SAIPE\uff09\u4ece\u4f4e\u8d28\u91cf\uff08LQ\uff09\u56fe\u50cf\u4e2d\u63d0\u53d6\u7279\u5f81\u5e76\u5c06\u5176\u6295\u5f71\u5230\u4e0e\u6587\u672c\u7f16\u7801\u5668\u517c\u5bb9\u7684\u5d4c\u5165\u7a7a\u95f4\uff0c\u540c\u65f6\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u4ee5\u8fdb\u884c\u5fe0\u5b9e\u91cd\u5efa\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8d28\u91cf\u56e0\u5b50\u611f\u77e5\u65f6\u95f4\u9884\u6d4b\u5668\uff0c\u7528\u4e8e\u5b66\u4e60\u538b\u7f29\u8d28\u91cf\u56e0\u5b50\uff08QF\uff09\u5e76\u81ea\u9002\u5e94\u5730\u9009\u62e9\u6269\u6563\u8fc7\u7a0b\u7684\u6700\u4f73\u53bb\u566a\u8d77\u59cb\u65f6\u95f4\u6b65\u3002", "result": "SODiff \u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6700\u8fd1\u7684\u9886\u5148\u65b9\u6cd5\u3002", "conclusion": "SODiff \u5728\u89c6\u89c9\u8d28\u91cf\u548c\u5b9a\u91cf\u6307\u6807\u4e0a\u5747\u4f18\u4e8e\u6700\u8fd1\u7684\u9886\u5148\u65b9\u6cd5\u3002"}}
{"id": "2508.07738", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07738", "abs": "https://arxiv.org/abs/2508.07738", "authors": ["Jialu Zhou", "Dianxi Shi", "Shaowu Yang", "Xinyu Wei", "Mingyue Yang", "Leqian Li", "Mengzhu Wang", "Chunping Qiu"], "title": "Separation and Collaboration: Two-Level Routing Grouped Mixture-of-Experts for Multi-Domain Continual Learning", "comment": null, "summary": "Multi-Domain Continual Learning (MDCL) acquires knowledge from sequential\ntasks with shifting class sets and distribution. Despite the\nParameter-Efficient Fine-Tuning (PEFT) methods can adapt for this dual\nheterogeneity, they still suffer from catastrophic forgetting and forward\nforgetting. To address these challenges, we propose a Two-Level Routing Grouped\nMixture-of-Experts (TRGE) method. Firstly, TRGE dynamically expands the\npre-trained CLIP model, assigning specific expert group for each task to\nmitigate catastrophic forgetting. With the number of experts continually grows\nin this process, TRGE maintains the static experts count within the group and\nintroduces the intra-group router to alleviate routing overfitting caused by\nthe increasing routing complexity. Meanwhile, we design an inter-group routing\npolicy based on task identifiers and task prototype distance, which dynamically\nselects relevant expert groups and combines their outputs to enhance inter-task\ncollaboration. Secondly, to get the correct task identifiers, we leverage\nMultimodal Large Language Models (MLLMs) which own powerful multimodal\ncomprehension capabilities to generate semantic task descriptions and recognize\nthe correct task identifier. Finally, to mitigate forward forgetting, we\ndynamically fuse outputs for unseen samples from the frozen CLIP model and TRGE\nadapter based on training progress, leveraging both pre-trained and learned\nknowledge. Through extensive experiments across various settings, our method\noutperforms other advanced methods with fewer trainable parameters.", "AI": {"tldr": "TRGE\u662f\u4e00\u79cd\u65b0\u7684\u591a\u57df\u6301\u7eed\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u8def\u7531\u673a\u5236\u548c\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u707e\u96be\u6027\u9057\u5fd8\u548c\u524d\u5411\u9057\u5fd8\u95ee\u9898\uff0c\u5e76\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5728\u591a\u57df\u6301\u7eed\u5b66\u4e60\uff08MDCL\uff09\u4e2d\u867d\u7136\u80fd\u9002\u5e94\u7c7b\u522b\u548c\u5206\u5e03\u7684\u53d8\u5316\uff0c\u4f46\u4ecd\u5b58\u5728\u707e\u96be\u6027\u9057\u5fd8\u548c\u524d\u5411\u9057\u5fd8\u7684\u95ee\u9898\u3002", "method": "TRGE\u65b9\u6cd5\u5305\u62ec\uff1a1.\u52a8\u6001\u6269\u5c55\u9884\u8bad\u7ec3CLIP\u6a21\u578b\uff0c\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u5206\u914d\u7279\u5b9a\u7684\u4e13\u5bb6\u7ec4\u4ee5\u51cf\u8f7b\u707e\u96be\u6027\u9057\u5fd8\uff0c\u901a\u8fc7\u7ec4\u5185\u8def\u7531\u5668\u7f13\u89e3\u8def\u7531\u8fc7\u62df\u5408\uff0c\u5e76\u57fa\u4e8e\u4efb\u52a1\u6807\u8bc6\u7b26\u548c\u539f\u578b\u8ddd\u79bb\u8bbe\u8ba1\u7ec4\u95f4\u8def\u7531\u7b56\u7565\u4ee5\u589e\u5f3a\u4efb\u52a1\u534f\u4f5c\u30022.\u5229\u7528MLLM\u751f\u6210\u8bed\u4e49\u4efb\u52a1\u63cf\u8ff0\u5e76\u8bc6\u522b\u6b63\u786e\u7684\u4efb\u52a1\u6807\u8bc6\u7b26\u30023.\u901a\u8fc7\u52a8\u6001\u878d\u5408\u51bb\u7ed3CLIP\u6a21\u578b\u548cTRGE\u9002\u914d\u5668\u7684\u8f93\u51fa\u6765\u7f13\u89e3\u524d\u5411\u9057\u5fd8\u3002", "result": "TRGE\u65b9\u6cd5\u5728\u5404\u79cd\u8bbe\u7f6e\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u4e2d\uff0c\u4ee5\u66f4\u5c11\u7684\u8bad\u7ec3\u53c2\u6570\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "TRGE\u901a\u8fc7\u52a8\u6001\u6269\u5c55\u9884\u8bad\u7ec3CLIP\u6a21\u578b\u3001\u5f15\u5165\u7ec4\u5185\u548c\u7ec4\u95f4\u8def\u7531\u7b56\u7565\u4ee5\u53ca\u5229\u7528MLLM\u751f\u6210\u4efb\u52a1\u63cf\u8ff0\u6765\u89e3\u51b3MDCL\u4e2d\u7684\u707e\u96be\u6027\u9057\u5fd8\u548c\u524d\u5411\u9057\u5fd8\u95ee\u9898\uff0c\u540c\u65f6\u901a\u8fc7\u52a8\u6001\u878d\u5408\u9884\u8bad\u7ec3\u548c\u9002\u914d\u5668\u77e5\u8bc6\u6765\u589e\u5f3a\u5bf9\u672a\u89c1\u6837\u672c\u7684\u5904\u7406\uff0c\u5728\u51cf\u5c11\u53ef\u8bad\u7ec3\u53c2\u6570\u7684\u540c\u65f6\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07355", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07355", "abs": "https://arxiv.org/abs/2508.07355", "authors": ["Qilin Zhang", "Olaf Wysocki", "Boris Jutzi"], "title": "GS4Buildings: Prior-Guided Gaussian Splatting for 3D Building Reconstruction", "comment": "Accepted for presentation at ISPRS 3D GeoInfo & Smart Data, Smart\n  Cities 2025, Kashiwa, Japan. To appear in the ISPRS Annals of the\n  Photogrammetry, Remote Sensing and Spatial Information Sciences", "summary": "Recent advances in Gaussian Splatting (GS) have demonstrated its\neffectiveness in photo-realistic rendering and 3D reconstruction. Among these,\n2D Gaussian Splatting (2DGS) is particularly suitable for surface\nreconstruction due to its flattened Gaussian representation and integrated\nnormal regularization. However, its performance often degrades in large-scale\nand complex urban scenes with frequent occlusions, leading to incomplete\nbuilding reconstructions. We propose GS4Buildings, a novel prior-guided\nGaussian Splatting method leveraging the ubiquity of semantic 3D building\nmodels for robust and scalable building surface reconstruction. Instead of\nrelying on traditional Structure-from-Motion (SfM) pipelines, GS4Buildings\ninitializes Gaussians directly from low-level Level of Detail (LoD)2 semantic\n3D building models. Moreover, we generate prior depth and normal maps from the\nplanar building geometry and incorporate them into the optimization process,\nproviding strong geometric guidance for surface consistency and structural\naccuracy. We also introduce an optional building-focused mode that limits\nreconstruction to building regions, achieving a 71.8% reduction in Gaussian\nprimitives and enabling a more efficient and compact representation.\nExperiments on urban datasets demonstrate that GS4Buildings improves\nreconstruction completeness by 20.5% and geometric accuracy by 32.8%. These\nresults highlight the potential of semantic building model integration to\nadvance GS-based reconstruction toward real-world urban applications such as\nsmart cities and digital twins. Our project is available:\nhttps://github.com/zqlin0521/GS4Buildings.", "AI": {"tldr": "GS4Buildings\u5229\u75283D\u5efa\u7b51\u6a21\u578b\u6539\u8fdb\u4e86\u9ad8\u65af\u56fe\u7684\u91cd\u5efa\u6548\u679c\uff0c\u7279\u522b\u662f\u5728\u590d\u6742\u7684\u57ce\u5e02\u573a\u666f\u4e2d\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u7684\u5b8c\u6574\u6027\u548c\u7cbe\u5ea6\uff0c\u5e76\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u7684\u8868\u793a\u3002", "motivation": "\u4f20\u7edf\u76842D\u9ad8\u65af\u56fe\uff082DGS\uff09\u5728\u5904\u7406\u5927\u89c4\u6a21\u3001\u590d\u6742\u4e14\u6709\u906e\u6321\u7684\u57ce\u5e02\u573a\u666f\u65f6\uff0c\u91cd\u5efa\u6548\u679c\u4e0d\u4f73\uff0c\u6613\u5bfc\u81f4\u5efa\u7b51\u91cd\u5efa\u4e0d\u5b8c\u6574\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u63d0\u9ad8\u5728\u8fd9\u4e9b\u6311\u6218\u6027\u573a\u666f\u4e0b\u7684\u91cd\u5efa\u9c81\u68d2\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002", "method": "GS4Buildings\u662f\u4e00\u79cd\u65b0\u7684\u5148\u9a8c\u5f15\u5bfc\u9ad8\u65af\u56fe\uff08GS\uff09\u65b9\u6cd5\uff0c\u5b83\u4e0d\u4f9d\u8d56\u4f20\u7edf\u7684\u8fd0\u52a8\u6062\u590d\u7ed3\u6784\uff08SfM\uff09\u6d41\u7a0b\uff0c\u800c\u662f\u76f4\u63a5\u4ece\u4f4e\u7ea7\u522b\u7ec6\u8282\uff08LoD\uff092\u8bed\u4e493D\u5efa\u7b51\u6a21\u578b\u521d\u59cb\u5316\u9ad8\u65af\u56fe\u3002\u8be5\u65b9\u6cd5\u8fd8\u4ece\u5e73\u9762\u5efa\u7b51\u51e0\u4f55\u751f\u6210\u5148\u9a8c\u6df1\u5ea6\u56fe\u548c\u6cd5\u7ebf\u56fe\uff0c\u5e76\u5c06\u5176\u7eb3\u5165\u4f18\u5316\u8fc7\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u51e0\u4f55\u5f15\u5bfc\u3002\u6b64\u5916\uff0c\u5b83\u8fd8\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5efa\u7b51\u805a\u7126\u6a21\u5f0f\uff0c\u53ef\u4ee5\u5c06\u91cd\u5efa\u9650\u5236\u5728\u5efa\u7b51\u533a\u57df\u5185\u3002", "result": "GS4Buildings\u5728\u57ce\u5e02\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff0c\u91cd\u5efa\u5b8c\u6574\u6027\u63d0\u9ad8\u4e8620.5%\uff0c\u51e0\u4f55\u7cbe\u5ea6\u63d0\u9ad8\u4e8632.8%\u3002\u5728\u5efa\u7b51\u805a\u7126\u6a21\u5f0f\u4e0b\uff0c\u9ad8\u65af\u56fe\u5143\u6570\u91cf\u51cf\u5c11\u4e8671.8%\uff0c\u5b9e\u73b0\u4e86\u66f4\u9ad8\u6548\u548c\u7d27\u51d1\u7684\u8868\u793a\u3002", "conclusion": "GS4Buildings\u901a\u8fc7\u5229\u7528\u8bed\u4e493D\u5efa\u7b51\u6a21\u578b\uff0c\u63d0\u9ad8\u4e86\u5927\u89c4\u6a21\u548c\u590d\u6742\u57ce\u5e02\u573a\u666f\u4e0b\u5efa\u7b51\u8868\u9762\u91cd\u5efa\u7684\u5b8c\u6574\u6027\u548c\u51e0\u4f55\u7cbe\u5ea6\uff0c\u5728\u51cf\u5c11\u9ad8\u65af\u56fe\u5143\u65b9\u9762\u4e5f\u8868\u73b0\u51fa\u6548\u7387\u4f18\u52bf\uff0c\u4e3a\u667a\u6167\u57ce\u5e02\u548c\u6570\u5b57\u5b6a\u751f\u7b49\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u6f5c\u529b\u3002"}}
{"id": "2508.07746", "categories": ["cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.07746", "abs": "https://arxiv.org/abs/2508.07746", "authors": ["Fengdi Che"], "title": "A Tutorial: An Intuitive Explanation of Offline Reinforcement Learning Theory", "comment": null, "summary": "Offline reinforcement learning (RL) aims to optimize the return given a fixed\ndataset of agent trajectories without additional interactions with the\nenvironment. While algorithm development has progressed rapidly, significant\ntheoretical advances have also been made in understanding the fundamental\nchallenges of offline RL. However, bridging these theoretical insights with\npractical algorithm design remains an ongoing challenge. In this survey, we\nexplore key intuitions derived from theoretical work and their implications for\noffline RL algorithms.\n  We begin by listing the conditions needed for the proofs, including function\nrepresentation and data coverage assumptions. Function representation\nconditions tell us what to expect for generalization, and data coverage\nassumptions describe the quality requirement of the data. We then examine\ncounterexamples, where offline RL is not solvable without an impractically\nlarge amount of data. These cases highlight what cannot be achieved for all\nalgorithms and the inherent hardness of offline RL. Building on techniques to\nmitigate these challenges, we discuss the conditions that are sufficient for\noffline RL. These conditions are not merely assumptions for theoretical proofs,\nbut they also reveal the limitations of these algorithms and remind us to\nsearch for novel solutions when the conditions cannot be satisfied.", "AI": {"tldr": "\u79bb\u7ebf RL \u7684\u7406\u8bba\u548c\u5b9e\u8df5\u4e4b\u95f4\u7684\u5dee\u8ddd\u4ecd\u7136\u5b58\u5728\u3002\u672c\u7814\u7a76\u56de\u987e\u4e86\u7406\u8bba\u89c1\u89e3\uff0c\u5982\u51fd\u6570\u8868\u793a\u548c\u6570\u636e\u8986\u76d6\u5047\u8bbe\uff0c\u5e76\u8ba8\u8bba\u4e86\u5b83\u4eec\u5982\u4f55\u5f71\u54cd\u7b97\u6cd5\u8bbe\u8ba1\uff0c\u540c\u65f6\u5f3a\u8c03\u4e86\u79bb\u7ebf RL \u7684\u5c40\u9650\u6027\u3002", "motivation": "\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u65e8\u5728\u5728\u6ca1\u6709\u4e0e\u73af\u5883\u8fdb\u884c\u989d\u5916\u4ea4\u4e92\u7684\u60c5\u51b5\u4e0b\uff0c\u5229\u7528\u56fa\u5b9a\u6570\u636e\u96c6\u4f18\u5316\u56de\u62a5\u3002", "method": "\u672c\u6587\u9996\u5148\u5217\u51fa\u4e86\u8bc1\u660e\u6240\u9700\u7684\u6761\u4ef6\uff0c\u5305\u62ec\u51fd\u6570\u8868\u793a\u548c\u6570\u636e\u8986\u76d6\u5047\u8bbe\u3002\u7136\u540e\uff0c\u6211\u4eec\u68c0\u67e5\u53cd\u4f8b\uff0c\u5176\u4e2d\u79bb\u7ebf RL \u5728\u6ca1\u6709\u5927\u91cf\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u662f\u65e0\u6cd5\u89e3\u51b3\u7684\u3002\u57fa\u4e8e\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u7684\u6280\u672f\uff0c\u6211\u4eec\u8ba8\u8bba\u4e86\u79bb\u7ebf RL \u7684\u5145\u5206\u6761\u4ef6\u3002", "result": "\u7814\u7a76\u4e86\u7406\u8bba\u89c1\u89e3\u5bf9\u79bb\u7ebf RL \u7b97\u6cd5\u7684\u5f71\u54cd\uff0c\u5f3a\u8c03\u4e86\u51fd\u6570\u8868\u793a\u548c\u6570\u636e\u8986\u76d6\u5047\u8bbe\u7684\u91cd\u8981\u6027\uff0c\u5e76\u8ba8\u8bba\u4e86\u79bb\u7ebf RL \u7684\u56fa\u6709\u96be\u5ea6\u4ee5\u53ca\u5728\u65e0\u6cd5\u6ee1\u8db3\u6761\u4ef6\u65f6\u5bfb\u627e\u65b0\u9896\u89e3\u51b3\u65b9\u6848\u7684\u5fc5\u8981\u6027\u3002", "conclusion": "\u5c06\u7406\u8bba\u89c1\u89e3\u4e0e\u5b9e\u9645\u7b97\u6cd5\u8bbe\u8ba1\u76f8\u7ed3\u5408\u4ecd\u7136\u662f\u4e00\u4e2a\u6301\u7eed\u7684\u6311\u6218\uff0c\u672c\u6587\u63a2\u8ba8\u4e86\u4ece\u7406\u8bba\u5de5\u4f5c\u4e2d\u83b7\u5f97\u7684 Kxf \u89c1\u89e3\u53ca\u5176\u5bf9\u79bb\u7ebf RL \u7b97\u6cd5\u7684\u5f71\u54cd\u3002"}}
{"id": "2508.07369", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07369", "abs": "https://arxiv.org/abs/2508.07369", "authors": ["Tianyu Xin", "Jin-Liang Xiao", "Zeyu Xia", "Shan Yin", "Liang-Jian Deng"], "title": "Training and Inference within 1 Second -- Tackle Cross-Sensor Degradation of Real-World Pansharpening with Efficient Residual Feature Tailoring", "comment": null, "summary": "Deep learning methods for pansharpening have advanced rapidly, yet models\npretrained on data from a specific sensor often generalize poorly to data from\nother sensors. Existing methods to tackle such cross-sensor degradation include\nretraining model or zero-shot methods, but they are highly time-consuming or\neven need extra training data. To address these challenges, our method first\nperforms modular decomposition on deep learning-based pansharpening models,\nrevealing a general yet critical interface where high-dimensional fused\nfeatures begin mapping to the channel space of the final image. % may need\nrevisement A Feature Tailor is then integrated at this interface to address\ncross-sensor degradation at the feature level, and is trained efficiently with\nphysics-aware unsupervised losses. Moreover, our method operates in a\npatch-wise manner, training on partial patches and performing parallel\ninference on all patches to boost efficiency. Our method offers two key\nadvantages: (1) $\\textit{Improved Generalization Ability}$: it significantly\nenhance performance in cross-sensor cases. (2) $\\textit{Low Generalization\nCost}$: it achieves sub-second training and inference, requiring only partial\ntest inputs and no external data, whereas prior methods often take minutes or\neven hours. Experiments on the real-world data from multiple datasets\ndemonstrate that our method achieves state-of-the-art quality and efficiency in\ntackling cross-sensor degradation. For example, training and inference of\n$512\\times512\\times8$ image within $\\textit{0.2 seconds}$ and\n$4000\\times4000\\times8$ image within $\\textit{3 seconds}$ at the fastest\nsetting on a commonly used RTX 3090 GPU, which is over 100 times faster than\nzero-shot methods.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u6df1\u5ea6\u5b66\u4e60\u5168\u8272\u589e\u5f3a\u6a21\u578b\u5728\u8de8\u4f20\u611f\u5668\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u201c\u7279\u5f81\u5b9a\u5236\u5668\u201d\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6a21\u5757\u5316\u5206\u89e3\u548c\u5728\u7279\u5f81\u5c42\u9762\u8fdb\u884c\u8c03\u6574\uff0c\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6cdb\u5316\u80fd\u529b\u548c\u6781\u9ad8\u7684\u6548\u7387\uff0c\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u8fdc\u8d85\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u6df1\u5ea6\u5b66\u4e60\u5168\u8272\u589e\u5f3a\u6a21\u578b\u5728\u8de8\u4f20\u611f\u5668\u6570\u636e\u4e0a\u6cdb\u5316\u80fd\u529b\u5dee\uff0c\u800c\u73b0\u6709\u7684\u89e3\u51b3\u65b9\u6cd5\uff08\u5982\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6216\u96f6\u6837\u672c\u65b9\u6cd5\uff09\u8017\u65f6\u4e14\u9700\u8981\u989d\u5916\u6570\u636e\u3002", "method": "\u901a\u8fc7\u6a21\u5757\u5316\u5206\u89e3\u6df1\u5ea6\u5b66\u4e60\u5168\u8272\u589e\u5f3a\u6a21\u578b\uff0c\u5e76\u5728\u5173\u952e\u63a5\u53e3\u5904\u96c6\u6210\u201c\u7279\u5f81\u5b9a\u5236\u5668\u201d\u6765\u89e3\u51b3\u8de8\u4f20\u611f\u5668\u9000\u5316\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u4ee5\u5757\u72b6\u65b9\u5f0f\u8fd0\u884c\uff0c\u5728\u90e8\u5206\u5757\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5bf9\u6240\u6709\u5757\u8fdb\u884c\u5e76\u884c\u63a8\u7406\u4ee5\u63d0\u9ad8\u6548\u7387\u3002\u8be5\u7279\u5f81\u5b9a\u5236\u5668\u4f7f\u7528\u4e0e\u7269\u7406\u76f8\u5173\u7684\u65e0\u76d1\u7763\u635f\u5931\u8fdb\u884c\u9ad8\u6548\u8bad\u7ec3\u3002", "result": "\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u663e\u8457\u63d0\u5347\u4e86\u8de8\u4f20\u611f\u5668\u5168\u8272\u589e\u5f3a\u7684\u6027\u80fd\uff0c\u540c\u65f6\u5b9e\u73b0\u4e86\u4f4e\u6cdb\u5316\u6210\u672c\u3002\u8bad\u7ec3\u548c\u63a8\u7406\u901f\u5ea6\u6781\u5feb\uff0c\u4f8b\u5982\u5728RTX 3090 GPU\u4e0a\uff0c\u5904\u7406512x512x8\u56fe\u50cf\u4ec5\u97000.2\u79d2\uff0c\u5904\u74064000x4000x8\u56fe\u50cf\u4ec5\u97003\u79d2\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u7684\u591a\u4f20\u611f\u5668\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5904\u7406\u8de8\u4f20\u611f\u5668\u9000\u5316\u65b9\u9762\u7684\u5148\u8fdb\u8d28\u91cf\u548c\u6548\u7387\u3002\u4e0e\u4e4b\u524d\u7684\u96f6\u6837\u672c\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5176\u901f\u5ea6\u63d0\u9ad8\u4e86100\u591a\u500d\u3002"}}
{"id": "2508.07750", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07750", "abs": "https://arxiv.org/abs/2508.07750", "authors": ["Haowen Wang", "Yun Yue", "Zhiling Ye", "Shuowen Zhang", "Lei Fan", "Jiaxin Liang", "Jiadi Jiang", "Cheng Wei", "Jingyuan Deng", "Xudong Han", "Ji Li", "Chunxiao Guo", "Peng Wei", "Jian Wang", "Jinjie Gu"], "title": "Learning to Align, Aligning to Learn: A Unified Approach for Self-Optimized Alignment", "comment": "12 pages, 5 figures, 7 tables", "summary": "Alignment methodologies have emerged as a critical pathway for enhancing\nlanguage model alignment capabilities. While SFT (supervised fine-tuning)\naccelerates convergence through direct token-level loss intervention, its\nefficacy is constrained by offline policy trajectory. In contrast,\nRL(reinforcement learning) facilitates exploratory policy optimization, but\nsuffers from low sample efficiency and stringent dependency on high-quality\nbase models. To address these dual challenges, we propose GRAO (Group Relative\nAlignment Optimization), a unified framework that synergizes the respective\nstrengths of SFT and RL through three key innovations: 1) A multi-sample\ngeneration strategy enabling comparative quality assessment via reward\nfeedback; 2) A novel Group Direct Alignment Loss formulation leveraging\nintra-group relative advantage weighting; 3) Reference-aware parameter updates\nguided by pairwise preference dynamics. Our theoretical analysis establishes\nGRAO's convergence guarantees and sample efficiency advantages over\nconventional approaches. Comprehensive evaluations across complex human\nalignment tasks demonstrate GRAO's superior performance, achieving\n57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and\nGRPO baselines respectively. This work provides both a theoretically grounded\nalignment framework and empirical evidence for efficient capability evolution\nin language models.", "AI": {"tldr": "GRAO\u662f\u4e00\u4e2a\u7edf\u4e00\u7684\u6846\u67b6\uff0c\u7ed3\u5408\u4e86SFT\u548cRL\u7684\u4f18\u70b9\uff0c\u901a\u8fc7\u591a\u6837\u672c\u751f\u6210\u3001\u7ec4\u76f4\u63a5\u5bf9\u9f50\u635f\u5931\u548c\u53c2\u8003\u611f\u77e5\u53c2\u6570\u66f4\u65b0\u6765\u4f18\u5316\u8bed\u8a00\u6a21\u578b\u5bf9\u9f50\uff0c\u63d0\u9ad8\u4e86\u6837\u672c\u6548\u7387\u548c\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3SFT\u7684\u79bb\u7ebf\u7b56\u7565\u8f68\u8ff9\u9650\u5236\u548cRL\u7684\u4f4e\u6837\u672c\u6548\u7387\u53ca\u5bf9\u9ad8\u8d28\u91cf\u57fa\u7840\u6a21\u578b\u7684\u4e25\u683c\u4f9d\u8d56\u6027\u8fd9\u4e24\u4e2a\u6311\u6218\uff0c\u63d0\u51faGRAO\u6846\u67b6\u3002", "method": "GRAO\u901a\u8fc7\u4ee5\u4e0b\u4e09\u4e2a\u5173\u952e\u521b\u65b0\uff0c\u7ed3\u5408\u4e86SFT\u548cRL\u7684\u4f18\u70b9\uff1a1\uff09\u591a\u6837\u672c\u751f\u6210\u7b56\u7565\uff0c\u901a\u8fc7\u5956\u52b1\u53cd\u9988\u8fdb\u884c\u6bd4\u8f83\u8d28\u91cf\u8bc4\u4f30\uff1b2\uff09\u5229\u7528\u7ec4\u5185\u76f8\u5bf9\u4f18\u52bf\u52a0\u6743\u7684\u65b0\u9896\u7684\u7ec4\u76f4\u63a5\u5bf9\u9f50\u635f\u5931\u516c\u5f0f\uff1b3\uff09\u7531\u6210\u5bf9\u504f\u597d\u52a8\u6001\u6307\u5bfc\u7684\u53c2\u8003\u611f\u77e5\u53c2\u6570\u66f4\u65b0\u3002", "result": "GRAO\u5728\u590d\u6742\u7684\u5bf9\u9f50\u4efb\u52a1\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u76f8\u5bf9\u4e8eSFT\u3001DPO\u3001PPO\u548cGRPO\u57fa\u7ebf57.70%\u300117.65%\u30017.95%\u548c5.18%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002", "conclusion": "GRAO\u662f\u4e00\u4e2a\u7406\u8bba\u53ef\u9760\u4e14\u80fd\u63d0\u5347\u8bed\u8a00\u6a21\u578b\u80fd\u529b\u8fdb\u5316\u7684\u7ecf\u9a8c\u6027\u5de5\u4f5c\uff0c\u5b83\u5728\u590d\u6742\u7684\u4eba\u7c7b\u5bf9\u9f50\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8eSFT\u3001DPO\u3001PPO\u548cGRPO\u57fa\u7ebf57.70%\u300117.65%\u30017.95%\u548c5.18%\u7684\u76f8\u5bf9\u6539\u8fdb\u3002"}}
{"id": "2508.07372", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07372", "abs": "https://arxiv.org/abs/2508.07372", "authors": ["Rajaei Khatib", "Raja Giryes"], "title": "DIP-GS: Deep Image Prior For Gaussian Splatting Sparse View Recovery", "comment": null, "summary": "3D Gaussian Splatting (3DGS) is a leading 3D scene reconstruction method,\nobtaining high-quality reconstruction with real-time rendering runtime\nperformance. The main idea behind 3DGS is to represent the scene as a\ncollection of 3D gaussians, while learning their parameters to fit the given\nviews of the scene. While achieving superior performance in the presence of\nmany views, 3DGS struggles with sparse view reconstruction, where the input\nviews are sparse and do not fully cover the scene and have low overlaps. In\nthis paper, we propose DIP-GS, a Deep Image Prior (DIP) 3DGS representation. By\nusing the DIP prior, which utilizes internal structure and patterns, with\ncoarse-to-fine manner, DIP-based 3DGS can operate in scenarios where vanilla\n3DGS fails, such as sparse view recovery. Note that our approach does not use\nany pre-trained models such as generative models and depth estimation, but\nrather relies only on the input frames. Among such methods, DIP-GS obtains\nstate-of-the-art (SOTA) competitive results on various sparse-view\nreconstruction tasks, demonstrating its capabilities.", "AI": {"tldr": "DIP-GS\u662f\u4e00\u79cd\u65b0\u76843D\u9ad8\u65af\u6e85\u5c04\u65b9\u6cd5\uff0c\u5b83\u4f7f\u7528\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\u6765\u89e3\u51b3\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u95ee\u9898\uff0c\u5e76\u5728\u5404\u79cd\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ed3\u679c\u3002", "motivation": "\u89e3\u51b33DGS\u5728\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5373\u8f93\u5165\u89c6\u56fe\u7a00\u758f\u3001\u672a\u5b8c\u5168\u8986\u76d6\u573a\u666f\u4e14\u91cd\u53e0\u5ea6\u4f4e\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u56fe\u50cf\u5148\u9a8c\uff08DIP\uff093DGS\u8868\u793a\u65b9\u6cd5\uff0c\u5229\u7528DIP\u5148\u9a8c\u4ee5\u7c97\u5230\u7cbe\u7684\u65b9\u5f0f\u8fd0\u884c\uff0c\u53ef\u4ee5\u5728\u539f\u59cb3DGS\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\u8fd0\u884c\uff0c\u4f8b\u5982\u7a00\u758f\u89c6\u56fe\u6062\u590d\u3002\u8be5\u65b9\u6cd5\u4e0d\u4f7f\u7528\u4efb\u4f55\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u5982\u751f\u6210\u6a21\u578b\u6216\u6df1\u5ea6\u4f30\u8ba1\uff0c\u4ec5\u4f9d\u8d56\u8f93\u5165\u5e27\u3002", "result": "DIP-GS\u53ef\u4ee5\u8fd0\u884c\u5728\u539f\u59cb3DGS\u5931\u8d25\u7684\u60c5\u51b5\u4e0b\uff0c\u4f8b\u5982\u7a00\u758f\u89c6\u56fe\u6062\u590d\uff0c\u5e76\u5728\u5404\u79cd\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u6027\u7ed3\u679c\u3002", "conclusion": "DIP-GS\u5728\u5404\u79cd\u7a00\u758f\u89c6\u56fe\u91cd\u5efa\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u7ade\u4e89\u6027\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u80fd\u529b\u3002"}}
{"id": "2508.07763", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07763", "abs": "https://arxiv.org/abs/2508.07763", "authors": ["Martin Rektoris", "Milan Pape\u017e", "V\u00e1clav \u0160m\u00eddl", "Tom\u00e1\u0161 Pevn\u00fd"], "title": "Sparse Probabilistic Graph Circuits", "comment": null, "summary": "Deep generative models (DGMs) for graphs achieve impressively high expressive\npower thanks to very efficient and scalable neural networks. However, these\nnetworks contain non-linearities that prevent analytical computation of many\nstandard probabilistic inference queries, i.e., these DGMs are considered\n\\emph{intractable}. While recently proposed Probabilistic Graph Circuits (PGCs)\naddress this issue by enabling \\emph{tractable} probabilistic inference, they\noperate on dense graph representations with $\\mathcal{O}(n^2)$ complexity for\ngraphs with $n$ nodes and \\emph{$m$ edges}. To address this scalability issue,\nwe introduce Sparse PGCs, a new class of tractable generative models that\noperate directly on sparse graph representation, reducing the complexity to\n$\\mathcal{O}(n + m)$, which is particularly beneficial for $m \\ll n^2$. In the\ncontext of de novo drug design, we empirically demonstrate that SPGCs retain\nexact inference capabilities, improve memory efficiency and inference speed,\nand match the performance of intractable DGMs in key metrics.", "AI": {"tldr": "New Sparse PGCs (SPGCs) model graphs efficiently with O(n+m) complexity, offering exact inference and matching DGMs performance in drug design.", "motivation": "Existing Probabilistic Graph Circuits (PGCs) have a complexity of O(n^2) for graphs with n nodes, which is not scalable. DGMs are intractable for probabilistic inference queries.", "method": "Introduced Sparse PGCs (SPGCs), a new class of tractable generative models that operate directly on sparse graph representations.", "result": "SPGCs reduce complexity to O(n + m), improve memory efficiency and inference speed, and match the performance of intractable DGMs in key metrics for de novo drug design.", "conclusion": "Sparse PGCs (SPGCs) are a new class of tractable generative models that operate directly on sparse graph representations, reducing complexity to O(n + m). SPGCs retain exact inference capabilities, improve memory efficiency and inference speed, and match the performance of intractable DGMs in key metrics in de novo drug design."}}
{"id": "2508.07401", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07401", "abs": "https://arxiv.org/abs/2508.07401", "authors": ["Rui Chen", "Xingyu Chen", "Shaoan Wang", "Shihan Kong", "Junzhi Yu"], "title": "LET-US: Long Event-Text Understanding of Scenes", "comment": null, "summary": "Event cameras output event streams as sparse, asynchronous data with\nmicrosecond-level temporal resolution, enabling visual perception with low\nlatency and a high dynamic range. While existing Multimodal Large Language\nModels (MLLMs) have achieved significant success in understanding and analyzing\nRGB video content, they either fail to interpret event streams effectively or\nremain constrained to very short sequences. In this paper, we introduce LET-US,\na framework for long event-stream--text comprehension that employs an adaptive\ncompression mechanism to reduce the volume of input events while preserving\ncritical visual details. LET-US thus establishes a new frontier in cross-modal\ninferential understanding over extended event sequences. To bridge the\nsubstantial modality gap between event streams and textual representations, we\nadopt a two-stage optimization paradigm that progressively equips our model\nwith the capacity to interpret event-based scenes. To handle the voluminous\ntemporal information inherent in long event streams, we leverage text-guided\ncross-modal queries for feature reduction, augmented by hierarchical clustering\nand similarity computation to distill the most representative event features.\nMoreover, we curate and construct a large-scale event-text aligned dataset to\ntrain our model, achieving tighter alignment of event features within the LLM\nembedding space. We also develop a comprehensive benchmark covering a diverse\nset of tasks -- reasoning, captioning, classification, temporal localization\nand moment retrieval. Experimental results demonstrate that LET-US outperforms\nprior state-of-the-art MLLMs in both descriptive accuracy and semantic\ncomprehension on long-duration event streams. All datasets, codes, and models\nwill be publicly available.", "AI": {"tldr": "LET-US\u662f\u4e00\u4e2a\u7528\u4e8e\u957f\u4e8b\u4ef6\u6d41\u6587\u672c\u7406\u89e3\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u81ea\u9002\u5e94\u538b\u7f29\u548c\u591a\u9636\u6bb5\u4f18\u5316\u5904\u7406\u4e8b\u4ef6\u6570\u636e\uff0c\u5e76\u6784\u5efa\u4e86\u5927\u89c4\u6a21\u6570\u636e\u96c6\u548c\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u5728\u7406\u89e3\u548c\u5206\u6790RGB\u89c6\u9891\u5185\u5bb9\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u6210\u529f\uff0c\u4f46\u5b83\u4eec\u5728\u6709\u6548\u89e3\u91ca\u4e8b\u4ef6\u6d41\u6216\u5904\u7406\u77ed\u5e8f\u5217\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7LET-US\u6846\u67b6\u5b9e\u73b0\u5bf9\u957f\u4e8b\u4ef6\u6d41\u6587\u672c\u7684\u7406\u89e3\u3002", "method": "LET-US\u91c7\u7528\u81ea\u9002\u5e94\u538b\u7f29\u673a\u5236\u5904\u7406\u957f\u4e8b\u4ef6\u6d41\uff0c\u5e76\u4f7f\u7528\u4e24\u9636\u6bb5\u4f18\u5316\u8303\u5f0f\u3001\u6587\u672c\u5f15\u5bfc\u7684\u8de8\u6a21\u6001\u67e5\u8be2\u3001\u5206\u5c42\u805a\u7c7b\u548c\u76f8\u4f3c\u6027\u8ba1\u7b97\u6765\u5f25\u5408\u4e8b\u4ef6\u6d41\u548c\u6587\u672c\u8868\u793a\u4e4b\u95f4\u7684\u6a21\u6001\u5dee\u8ddd\u3002", "result": "LET-US\u5728\u957f\u65f6\u4e8b\u4ef6\u6d41\u7684\u63cf\u8ff0\u51c6\u786e\u6027\u548c\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b(MLLMs)\u3002", "conclusion": "LET-US\u5728\u63cf\u8ff0\u51c6\u786e\u6027\u548c\u8bed\u4e49\u7406\u89e3\u65b9\u9762\u5747\u4f18\u4e8e\u5148\u524d\u6700\u5148\u8fdb\u7684}(\\textbf{MLLMs})\uff0c\u5728\u5904\u7406\u957f\u65f6\u4e8b\u4ef6\u6d41\u65b9\u9762\u3002"}}
{"id": "2508.07768", "categories": ["cs.LG", "cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.07768", "abs": "https://arxiv.org/abs/2508.07768", "authors": ["Qiang He", "Setareh Maghsudi"], "title": "Pareto Multi-Objective Alignment for Language Models", "comment": "Accepted at ECML/PKDD 2025", "summary": "Large language models (LLMs) are increasingly deployed in real-world\napplications that require careful balancing of multiple, often conflicting,\nobjectives, such as informativeness versus conciseness, or helpfulness versus\ncreativity. However, current alignment methods, primarily based on RLHF,\noptimize LLMs toward a single reward function, resulting in rigid behavior that\nfails to capture the complexity and diversity of human preferences. This\nlimitation hinders the adaptability of LLMs to practical scenarios, making\nmulti-objective alignment (MOA) a critical yet underexplored area. To bridge\nthis gap, we propose Pareto Multi-Objective Alignment (PAMA), a principled and\ncomputationally efficient algorithm designed explicitly for MOA in LLMs. In\ncontrast to computationally prohibitive multi-objective optimization (MOO)\nmethods, PAMA transforms multi-objective RLHF into a convex optimization with a\nclosed-form solution, significantly enhancing scalability. Traditional MOO\napproaches suffer from prohibitive O(n^2*d) complexity, where d represents the\nnumber of model parameters, typically in the billions for LLMs, rendering\ndirect optimization infeasible. PAMA reduces this complexity to O(n) where n is\nthe number of objectives, enabling optimization to be completed within\nmilliseconds. We provide theoretical guarantees that PAMA converges to a Pareto\nstationary point, where no objective can be improved without degrading at least\none other. Extensive experiments across language models ranging from 125M to 7B\nparameters demonstrate PAMA's robust and effective MOA capabilities, aligning\nwith its theoretical advantages. PAMA provides a highly efficient solution to\nthe MOA problem that was previously considered intractable, offering a\npractical and theoretically grounded approach to aligning LLMs with diverse\nhuman values, paving the way for versatile and adaptable real-world AI\ndeployments.", "AI": {"tldr": "PAMA \u662f\u4e00\u79cd\u9ad8\u6548\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u7b97\u6cd5\uff0c\u53ef\u89e3\u51b3 LLM \u4e2d\u7684\u590d\u6742\u504f\u597d\u95ee\u9898\uff0c\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u5b83\u6bd4\u4f20\u7edf\u65b9\u6cd5\u66f4\u5177\u53ef\u6269\u5c55\u6027\u548c\u66f4\u5feb\u7684\u901f\u5ea6\u3002", "motivation": "\u73b0\u6709\u7684\u5bf9\u9f50\u65b9\u6cd5\uff08\u4e3b\u8981\u662f\u57fa\u4e8e RLHF\uff09\u5c06 LLM \u4f18\u5316\u4e3a\u5355\u4e00\u5956\u52b1\u51fd\u6570\uff0c\u5bfc\u81f4\u884c\u4e3a\u50f5\u5316\uff0c\u65e0\u6cd5\u6355\u6349\u4eba\u7c7b\u504f\u597d\u7684\u590d\u6742\u6027\u548c\u591a\u6837\u6027\u3002\u8fd9\u79cd\u5c40\u9650\u6027\u963b\u788d\u4e86 LLM \u5728\u5b9e\u9645\u573a\u666f\u4e2d\u7684\u9002\u5e94\u6027\uff0c\u56e0\u6b64\uff0c\u591a\u76ee\u6807\u5bf9\u9f50\uff08MOA\uff09\u6210\u4e3a\u4e00\u4e2a\u5173\u952e\u4f46\u63a2\u7d22\u4e0d\u8db3\u7684\u9886\u57df\u3002", "method": "PAMA \u5c06\u591a\u76ee\u6807\u5f3a\u5316\u5b66\u4e60\uff08RLHF\uff09\u8f6c\u5316\u4e3a\u5177\u6709\u5c01\u95ed\u5f62\u5f0f\u89e3\u7684\u51f8\u4f18\u5316\u95ee\u9898\uff0c\u5c06\u4f20\u7edf\u591a\u76ee\u6807\u4f18\u5316\uff08MOO\uff09\u7684 O(n^2*d) \u590d\u6742\u6027\u964d\u4f4e\u5230 O(n)\uff0c\u5176\u4e2d n \u662f\u76ee\u6807\u6570\u91cf\uff0cd \u662f\u6a21\u578b\u53c2\u6570\u6570\u91cf\u3002", "result": "PAMA \u5177\u6709\u53ef\u6269\u5c55\u6027\uff0c\u53ef\u4ee5\u5728\u51e0\u6beb\u79d2\u5185\u5b8c\u6210\u4f18\u5316\uff0c\u5e76\u6709\u7406\u8bba\u4fdd\u8bc1\u6536\u655b\u5230\u5e15\u7d2f\u6258\u7a33\u5b9a\u70b9\u3002\u5728\u4e0d\u540c\u5927\u5c0f\uff081.25 \u4ebf\u5230 70 \u4ebf\u53c2\u6570\uff09\u7684\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660e\u4e86 PAMA \u7a33\u5065\u800c\u6709\u6548\u7684 MOA \u80fd\u529b\u3002", "conclusion": "PAMA \u63d0\u4f9b\u4e86\u4e00\u4e2a\u9ad8\u6548\u4e14\u7ecf\u8fc7\u7406\u8bba\u9a8c\u8bc1\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u89e3\u51b3\u4e86\u4ee5\u524d\u88ab\u8ba4\u4e3a\u96be\u4ee5\u5904\u7406\u7684\u591a\u76ee\u6807\u5bf9\u9f50\u95ee\u9898\uff0c\u4e3a\u5c06 LLM \u4e0e\u591a\u6837\u5316\u7684\u4eba\u7c7b\u4ef7\u503c\u89c2\u5bf9\u9f50\u63d0\u4f9b\u4e86\u4e00\u79cd\u5b9e\u7528\u4e14\u6709\u7406\u8bba\u4f9d\u636e\u7684\u65b9\u6cd5\uff0c\u4ece\u800c\u4e3a\u901a\u7528\u548c\u9002\u5e94\u6027\u5f3a\u7684\u5b9e\u9645\u4eba\u5de5\u667a\u80fd\u90e8\u7f72\u94fa\u5e73\u4e86\u9053\u8def\u3002"}}
{"id": "2508.07402", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07402", "abs": "https://arxiv.org/abs/2508.07402", "authors": ["Rongxuan Peng", "Shunquan Tan", "Chenqi Kong", "Anwei Luo", "Alex C. Kot", "Jiwu Huang"], "title": "ForensicsSAM: Toward Robust and Unified Image Forgery Detection and Localization Resisting to Adversarial Attack", "comment": null, "summary": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular strategy for\nadapting large vision foundation models, such as the Segment Anything Model\n(SAM) and LLaVA, to downstream tasks like image forgery detection and\nlocalization (IFDL). However, existing PEFT-based approaches overlook their\nvulnerability to adversarial attacks. In this paper, we show that highly\ntransferable adversarial images can be crafted solely via the upstream model,\nwithout accessing the downstream model or training data, significantly\ndegrading the IFDL performance. To address this, we propose ForensicsSAM, a\nunified IFDL framework with built-in adversarial robustness. Our design is\nguided by three key ideas: (1) To compensate for the lack of forgery-relevant\nknowledge in the frozen image encoder, we inject forgery experts into each\ntransformer block to enhance its ability to capture forgery artifacts. These\nforgery experts are always activated and shared across any input images. (2) To\ndetect adversarial images, we design an light-weight adversary detector that\nlearns to capture structured, task-specific artifact in RGB domain, enabling\nreliable discrimination across various attack methods. (3) To resist\nadversarial attacks, we inject adversary experts into the global attention\nlayers and MLP modules to progressively correct feature shifts induced by\nadversarial noise. These adversary experts are adaptively activated by the\nadversary detector, thereby avoiding unnecessary interference with clean\nimages. Extensive experiments across multiple benchmarks demonstrate that\nForensicsSAM achieves superior resistance to various adversarial attack\nmethods, while also delivering state-of-the-art performance in image-level\nforgery detection and pixel-level forgery localization. The resource is\navailable at https://github.com/siriusPRX/ForensicsSAM.", "AI": {"tldr": "ForensicsSAM\u662f\u4e00\u4e2a\u96c6\u6210\u4e86\u5bf9\u6297\u6027\u9c81\u68d2\u6027\u7684\u7edf\u4e00\u56fe\u50cf\u4f2a\u5f71\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u4f2a\u5f71\u4e13\u5bb6\u548c\u81ea\u9002\u5e94\u6fc0\u6d3b\u7684\u5bf9\u6297\u6027\u4e13\u5bb6\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709PEFT\u65b9\u6cd5\u6613\u53d7\u653b\u51fb\u7684\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u53c2\u6570\u9ad8\u6548\u5fae\u8c03\uff08PEFT\uff09\u65b9\u6cd5\u5728\u9002\u5e94\u5927\u578b\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u65f6\uff0c\u5bb9\u6613\u53d7\u5230\u5bf9\u6297\u6027\u653b\u51fb\u7684\u5f71\u54cd\uff0c\u4ece\u800c\u635f\u5bb3\u4e0b\u6e38\u4efb\u52a1\uff08\u5982\u56fe\u50cf\u4f2a\u5f71\u68c0\u6d4b\u4e0e\u5b9a\u4f4d\uff09\u7684\u6027\u80fd\u3002", "method": "ForensicsSAM\u6846\u67b6\u901a\u8fc7\u6ce8\u5165\u4f2a\u5f71\u4e13\u5bb6\u548c\u5bf9\u6297\u6027\u4e13\u5bb6\u6765\u589e\u5f3a\u6a21\u578b\u7684\u4f2a\u5f71\u6355\u83b7\u80fd\u529b\u548c\u5bf9\u6297\u6027\u9c81\u68d2\u6027\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u5bf9\u6297\u6027\u68c0\u6d4b\u5668\u6765\u8bc6\u522b\u548c\u5bf9\u6297\u6270\u52a8\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0cForensicsSAM\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\uff0c\u80fd\u66f4\u6709\u6548\u5730\u62b5\u5fa1\u591a\u79cd\u5bf9\u6297\u6027\u653b\u51fb\uff0c\u5e76\u4e14\u5728\u56fe\u50cf\u4f2a\u5f71\u68c0\u6d4b\u548c\u5b9a\u4f4d\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6c34\u5e73\u3002", "conclusion": "ForensicsSAM\u6846\u67b6\u5728\u62b5\u5fa1\u591a\u79cd\u5bf9\u6297\u6027\u653b\u51fb\u65b9\u9762\u8868\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u5728\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7\u4f2a\u5f71\u68c0\u6d4b\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07807", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07807", "abs": "https://arxiv.org/abs/2508.07807", "authors": ["Rahul Khorana"], "title": "Topological Feature Compression for Molecular Graph Neural Networks", "comment": null, "summary": "Recent advances in molecular representation learning have produced highly\neffective encodings of molecules for numerous cheminformatics and\nbioinformatics tasks. However, extracting general chemical insight while\nbalancing predictive accuracy, interpretability, and computational efficiency\nremains a major challenge. In this work, we introduce a novel Graph Neural\nNetwork (GNN) architecture that combines compressed higher-order topological\nsignals with standard molecular features. Our approach captures global\ngeometric information while preserving computational tractability and\nhuman-interpretable structure. We evaluate our model across a range of\nbenchmarks, from small-molecule datasets to complex material datasets, and\ndemonstrate superior performance using a parameter-efficient architecture. We\nachieve the best performing results in both accuracy and robustness across\nalmost all benchmarks. We open source all code \\footnote{All code and results\ncan be found on Github https://github.com/rahulkhorana/TFC-PACT-Net}.", "AI": {"tldr": "A new GNN model combines topological signals and molecular features to improve chemical insight extraction, achieving top accuracy and robustness on various datasets.", "motivation": "Extracting general chemical insight while balancing predictive accuracy, interpretability, and computational efficiency remains a challenge in molecular representation learning.", "method": "A novel Graph Neural Network (GNN) architecture that combines compressed higher-order topological signals with standard molecular features.", "result": "Superior performance in accuracy and robustness across small-molecule and complex material datasets using a parameter-efficient architecture.", "conclusion": "The novel GNN architecture achieves superior performance in accuracy and robustness across various benchmarks due to its ability to capture global geometric information while maintaining computational tractability and human-interpretability."}}
{"id": "2508.08061", "categories": ["cs.LG", "cs.CL", "cs.DB"], "pdf": "https://arxiv.org/pdf/2508.08061", "abs": "https://arxiv.org/abs/2508.08061", "authors": ["Sven Weinzierl", "Sandra Zilker", "Annina Liessmann", "Martin K\u00e4ppel", "Weixin Wang", "Martin Matzner"], "title": "From Source to Target: Leveraging Transfer Learning for Predictive Process Monitoring in Organizations", "comment": null, "summary": "Event logs reflect the behavior of business processes that are mapped in\norganizational information systems. Predictive process monitoring (PPM)\ntransforms these data into value by creating process-related predictions that\nprovide the insights required for proactive interventions at process runtime.\nExisting PPM techniques require sufficient amounts of event data or other\nrelevant resources that might not be readily available, preventing some\norganizations from utilizing PPM. The transfer learning-based PPM technique\npresented in this paper allows organizations without suitable event data or\nother relevant resources to implement PPM for effective decision support. The\ntechnique is instantiated in two real-life use cases, based on which numerical\nexperiments are performed using event logs for IT service management processes\nin an intra- and inter-organizational setting. The results of the experiments\nsuggest that knowledge of one business process can be transferred to a similar\nbusiness process in the same or a different organization to enable effective\nPPM in the target context. With the proposed technique, organizations can\nbenefit from transfer learning in an intra- and inter-organizational setting,\nwhere resources like pre-trained models are transferred within and across\norganizational boundaries.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u6280\u672f\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6280\u672f\u5bf9\u6570\u636e\u8d44\u6e90\u7684\u8981\u6c42\uff0c\u5b9e\u73b0\u4e86\u8de8\u7ec4\u7ec7\u7684\u6570\u636e\u8f6c\u79fb\u548c\u5e94\u7528\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u6280\u672f\u9700\u8981\u5927\u91cf\u7684\u4e8b\u4ef6\u6570\u636e\u6216\u5176\u4ed6\u76f8\u5173\u8d44\u6e90\uff0c\u8fd9\u53ef\u80fd\u65e0\u6cd5\u6ee1\u8db3\u6240\u6709\u7ec4\u7ec7\u7684\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u8fc1\u79fb\u5b66\u4e60\u7684\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u6280\u672f\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e00\u4e2a\u4e1a\u52a1\u6d41\u7a0b\u7684\u77e5\u8bc6\u53ef\u4ee5\u8f6c\u79fb\u5230\u76f8\u540c\u6216\u4e0d\u540c\u7ec4\u7ec7\u4e2d\u7684\u7c7b\u4f3c\u4e1a\u52a1\u6d41\u7a0b\uff0c\u4ee5\u5b9e\u73b0\u6709\u6548\u7684\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u3002", "conclusion": "\u77e5\u8bc6\u53ef\u4ee5\u8de8\u7ec4\u7ec7\u8fb9\u754c\u8f6c\u79fb\uff0c\u4ee5\u5b9e\u73b0\u76ee\u6807\u73af\u5883\u4e2d\u6709\u6548\u7684\u9884\u6d4b\u6027\u6d41\u7a0b\u76d1\u63a7\u3002"}}
{"id": "2508.07409", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07409", "abs": "https://arxiv.org/abs/2508.07409", "authors": ["Junyao Gao", "Jiaxing Li", "Wenran Liu", "Yanhong Zeng", "Fei Shen", "Kai Chen", "Yanan Sun", "Cairong Zhao"], "title": "CharacterShot: Controllable and Consistent 4D Character Animation", "comment": "13 pages, 10 figures. Code at https://github.com/Jeoyal/CharacterShot", "summary": "In this paper, we propose \\textbf{CharacterShot}, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot.", "AI": {"tldr": "CharacterShot\u662f\u4e00\u4e2a\u521b\u65b0\u76844D\u89d2\u8272\u52a8\u753b\u6846\u67b6\uff0c\u80fd\u591f\u4ece\u5355\u4e2a\u56fe\u50cf\u548c2D\u59ff\u6001\u521b\u5efa\u52a8\u60013D\u89d2\u8272\uff0c\u5e76\u63d0\u4f9b\u9ad8\u5ea6\u7684\u53ef\u63a7\u6027\u548c\u4e00\u81f4\u6027\u3002", "motivation": "\u63d0\u51fa\u4e00\u4e2a\u5141\u8bb8\u4efb\u4f55\u4e2a\u4eba\u8bbe\u8ba1\u5e08\u4ece\u5355\u4e2a\u53c2\u8003\u89d2\u8272\u56fe\u50cf\u548c2D\u59ff\u6001\u5e8f\u5217\u521b\u5efa\u52a8\u60013D\u89d2\u8272\uff08\u53734D\u89d2\u8272\u52a8\u753b\uff09\u7684\u53ef\u63a7\u4e14\u4e00\u81f4\u76844D\u89d2\u8272\u52a8\u753b\u6846\u67b6\u3002", "method": "CharacterShot\u9996\u5148\u8bad\u7ec3\u4e00\u4e2a\u57fa\u4e8eDiT\u76842D\u56fe\u50cf\u5230\u89c6\u9891\u6a21\u578b\uff0c\u7136\u540e\u901a\u8fc7\u5f15\u5165\u5076\u6781\u6ce8\u610f\u529b\u6a21\u5757\u548c\u76f8\u673a\u5148\u9a8c\u5c062D\u52a8\u753b\u6a21\u578b\u63d0\u5347\u52303D\uff0c\u6700\u540e\u5229\u7528\u65b0\u9896\u7684\u90bb\u57df\u7ea6\u675f4D\u9ad8\u65af\u6cfc\u6e85\u4f18\u5316\u6765\u751f\u62104D\u8868\u793a\u3002", "result": "CharacterShot\u6846\u67b6\u80fd\u591f\u751f\u6210\u8fde\u7eed\u4e14\u7a33\u5b9a\u76844D\u89d2\u8272\u8868\u793a\uff0c\u5e76\u4e14\u5728CharacterBench\u57fa\u51c6\u4e0a\u5b9e\u73b0\u4e86\u4f18\u4e8e\u73b0\u6709\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "CharacterShot\u80fd\u591f\u4ece\u5355\u4e2a\u89d2\u8272\u56fe\u50cf\u548c2D\u59ff\u6001\u5e8f\u5217\u751f\u6210\u4e00\u81f4\u4e14\u53ef\u63a7\u76844D\u89d2\u8272\u52a8\u753b\uff0c\u5e76\u5728CharacterBench\u57fa\u51c6\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.07809", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07809", "abs": "https://arxiv.org/abs/2508.07809", "authors": ["Huanyu Liu", "Jia Li", "Chang Yu", "Taozhi Chen", "Yihong Dong", "Lecheng Wang", "Hu XiaoLong", "Ge Li"], "title": "EvoCoT: Overcoming the Exploration Bottleneck in Reinforcement Learning", "comment": null, "summary": "Reinforcement learning with verifiable reward (RLVR) has become a promising\nparadigm for post-training large language models (LLMs) to improve their\nreasoning capability. However, when the rollout accuracy is low on hard\nproblems, the reward becomes sparse, limiting learning efficiency and causing\nexploration bottlenecks. Existing approaches either rely on stronger LLMs for\ndistillation or filter out difficult problems, which limits scalability or\nrestricts reasoning improvement through exploration.\n  We propose EvoCoT, a self-evolving curriculum learning framework based on\ntwo-stage chain-of-thought (CoT) reasoning optimization. EvoCoT constrains the\nexploration space by self-generating and verifying CoT trajectories, then\ngradually shortens them to expand the space in a controlled way. This enables\nLLMs to stably learn from initially unsolved hard problems under sparse\nrewards. We apply EvoCoT to multiple LLM families, including Qwen, DeepSeek,\nand Llama. Experiments show that EvoCoT enables LLMs to solve previously\nunsolved problems, improves reasoning capability without external CoT\nsupervision, and is compatible with various RL fine-tuning methods. We release\nthe source code to support future research.", "AI": {"tldr": "EvoCoT is a new framework that helps LLMs learn to solve harder problems by evolving their reasoning process, overcoming limitations of existing methods.", "motivation": "Reinforcement learning with verifiable reward (RLVR) is promising for improving LLM reasoning, but suffers from sparse rewards on hard problems, limiting learning efficiency and causing exploration bottlenecks. Existing methods have scalability limitations or restrict reasoning improvement.", "method": "EvoCoT is a self-evolving curriculum learning framework based on two-stage chain-of-thought (CoT) reasoning optimization. It constrains the exploration space by self-generating and verifying CoT trajectories, then gradually shortens them to expand the space in a controlled way.", "result": "Experiments show that EvoCoT enables LLMs to solve previously unsolved problems and improves reasoning capability without external CoT supervision. It is compatible with various RL fine-tuning methods and has been applied to Qwen, DeepSeek, and Llama families.", "conclusion": "EvoCoT enables LLMs to solve previously unsolved problems, improves reasoning capability without external CoT supervision, and is compatible with various RL fine-tuning methods."}}
{"id": "2508.08066", "categories": ["cs.CV", "cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08066", "abs": "https://arxiv.org/abs/2508.08066", "authors": ["Weitai Kang", "Weiming Zhuang", "Zhizhong Li", "Yan Yan", "Lingjuan Lyu"], "title": "Investigating the Design Space of Visual Grounding in Multimodal Large Language Model", "comment": "8 pages for the main paper", "summary": "Fine-grained multimodal capability in Multimodal Large Language Models\n(MLLMs) has emerged as a critical research direction, particularly for tackling\nthe visual grounding (VG) problem. Despite the strong performance achieved by\nexisting approaches, they often employ disparate design choices when\nfine-tuning MLLMs for VG, lacking systematic verification to support these\ndesigns. To bridge this gap, this paper presents a comprehensive study of\nvarious design choices that impact the VG performance of MLLMs. We conduct our\nanalysis using LLaVA-1.5, which has been widely adopted in prior empirical\nstudies of MLLMs. While more recent models exist, we follow this convention to\nensure our findings remain broadly applicable and extendable to other\narchitectures. We cover two key aspects: (1) exploring different visual\ngrounding paradigms in MLLMs, identifying the most effective design, and\nproviding our insights; and (2) conducting ablation studies on the design of\ngrounding data to optimize MLLMs' fine-tuning for the VG task. Finally, our\nfindings contribute to a stronger MLLM for VG, achieving improvements of +5.6%\n/ +6.9% / +7.0% on RefCOCO/+/g over the LLaVA-1.5.", "AI": {"tldr": "\u672c\u7814\u7a76\u7cfb\u7edf\u5730\u7814\u7a76\u4e86\u5f71\u54cd\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u57fa\u7840\uff08VG\uff09\u4efb\u52a1\u4e0a\u8868\u73b0\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u5e76\u901a\u8fc7\u5bf9LLaVA-1.5\u8fdb\u884c\u4f18\u5316\uff0c\u5728RefCOCO/+/g\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "motivation": "\u73b0\u6709\u6a21\u578b\u5728\u5fae\u8c03\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u4ee5\u89e3\u51b3\u89c6\u89c9\u57fa\u7840\uff08VG\uff09\u95ee\u9898\u65f6\uff0c\u91c7\u7528\u4e86\u591a\u79cd\u4e0d\u540c\u7684\u8bbe\u8ba1\u9009\u62e9\uff0c\u4f46\u7f3a\u4e4f\u7cfb\u7edf\u7684\u9a8c\u8bc1\u6765\u652f\u6491\u8fd9\u4e9b\u8bbe\u8ba1\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff0c\u5bf9\u5f71\u54cdVG\u6027\u80fd\u7684\u8bbe\u8ba1\u9009\u62e9\u8fdb\u884c\u5168\u9762\u7814\u7a76\u3002", "method": "\u672c\u7814\u7a76\u5bf9LLaVA-1.5\u6a21\u578b\u5728\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\u4e0a\u7684\u591a\u79cd\u8bbe\u8ba1\u9009\u62e9\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5206\u6790\uff0c\u5305\u62ec\u63a2\u7d22\u4e0d\u540c\u7684\u89c6\u89c9\u57fa\u7840\u8303\u5f0f\u548c\u8fdb\u884c\u63a5\u5730\u6570\u636e\u8bbe\u8ba1\u7684\u5265\u79bb\u7814\u7a76\uff0c\u65e8\u5728\u4f18\u5316\u6a21\u578b\u5fae\u8c03\u8fc7\u7a0b\u3002", "result": "\u672c\u7814\u7a76\u901a\u8fc7\u5728LLaVA-1.5\u6a21\u578b\u4e0a\u8fdb\u884c\u7cfb\u7edf\u6027\u5206\u6790\u548c\u6d88\u878d\u7814\u7a76\uff0c\u786e\u5b9a\u4e86\u6700\u4f18\u7684\u89c6\u89c9\u57fa\u7840\u8303\u5f0f\u548c\u63a5\u5730\u6570\u636e\u8bbe\u8ba1\uff0c\u6700\u7ec8\u5728RefCOCO/+/g\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86+5.6%/+6.9%/+7.0%\u7684\u6027\u80fd\u63d0\u5347\uff0c\u4e3aVG\u4efb\u52a1\u63d0\u4f9b\u4e86\u66f4\u5f3a\u7684MLLM\u3002", "conclusion": "\u672c\u7814\u7a76\u901a\u8fc7\u5bf9LLaVA-1.5\u5728\u89c6\u89c9\u57fa\u7840\u4efb\u52a1\u4e0a\u7684\u4e0d\u540c\u8bbe\u8ba1\u9009\u62e9\u8fdb\u884c\u7cfb\u7edf\u6027\u9a8c\u8bc1\uff0c\u4f18\u5316\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u89c6\u89c9\u57fa\u7840\uff08VG\uff09\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u5728RefCOCO/+/g\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86+5.6%/+6.9%/+7.0%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.07413", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07413", "abs": "https://arxiv.org/abs/2508.07413", "authors": ["Youqi Wang", "Shunquan Tan", "Rongxuan Peng", "Bin Li", "Jiwu Huang"], "title": "CLUE: Leveraging Low-Rank Adaptation to Capture Latent Uncovered Evidence for Image Forgery Localization", "comment": null, "summary": "The increasing accessibility of image editing tools and generative AI has led\nto a proliferation of visually convincing forgeries, compromising the\nauthenticity of digital media. In this paper, in addition to leveraging\ndistortions from conventional forgeries, we repurpose the mechanism of a\nstate-of-the-art (SOTA) text-to-image synthesis model by exploiting its\ninternal generative process, turning it into a high-fidelity forgery\nlocalization tool. To this end, we propose CLUE (Capture Latent Uncovered\nEvidence), a framework that employs Low- Rank Adaptation (LoRA) to\nparameter-efficiently reconfigure Stable Diffusion 3 (SD3) as a forensic\nfeature extractor. Our approach begins with the strategic use of SD3's\nRectified Flow (RF) mechanism to inject noise at varying intensities into the\nlatent representation, thereby steering the LoRAtuned denoising process to\namplify subtle statistical inconsistencies indicative of a forgery. To\ncomplement the latent analysis with high-level semantic context and precise\nspatial details, our method incorporates contextual features from the image\nencoder of the Segment Anything Model (SAM), which is parameter-efficiently\nadapted to better trace the boundaries of forged regions. Extensive evaluations\ndemonstrate CLUE's SOTA generalization performance, significantly outperforming\nprior methods. Furthermore, CLUE shows superior robustness against common\npost-processing attacks and Online Social Networks (OSNs). Code is publicly\navailable at https://github.com/SZAISEC/CLUE.", "AI": {"tldr": "CLUE\u5229\u7528SD3\u548cSAM\uff0c\u901a\u8fc7\u64cd\u7eb5\u6f5c\u5728\u8868\u5f81\u548c\u7ed3\u5408\u8bed\u4e49\u4fe1\u606f\u6765\u68c0\u6d4b\u4f2a\u9020\u56fe\u50cf\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u968f\u7740\u56fe\u50cf\u7f16\u8f91\u5de5\u5177\u548c\u751f\u6210\u5f0fAI\u7684\u666e\u53ca\uff0c\u6570\u5b57\u5a92\u4f53\u7684\u771f\u5b9e\u6027\u53d7\u5230\u8d8a\u6765\u8d8a\u4e25\u91cd\u7684\u5a01\u80c1\uff0c\u9700\u8981\u66f4\u6709\u6548\u7684\u4f2a\u9020\u68c0\u6d4b\u65b9\u6cd5\u3002", "method": "CLUE\u6846\u67b6\u5229\u7528Stable Diffusion 3\uff08SD3\uff09\u7684Rectified Flow\uff08RF\uff09\u673a\u5236\uff0c\u901a\u8fc7\u5728\u6f5c\u5728\u7a7a\u95f4\u6ce8\u5165\u4e0d\u540c\u5f3a\u5ea6\u7684\u566a\u58f0\uff0c\u5f15\u5bfcLoRA\u5fae\u8c03\u540e\u7684\u53bb\u566a\u8fc7\u7a0b\uff0c\u4ee5\u589e\u5f3a\u4f2a\u9020\u7684\u7edf\u8ba1\u4e0d\u4e00\u81f4\u6027\u3002\u540c\u65f6\uff0c\u7ed3\u5408\u7ecf\u8fc7\u53c2\u6570\u9ad8\u6548\u9002\u914d\u7684Segment Anything Model\uff08SAM\uff09\u7684\u56fe\u50cf\u7f16\u7801\u5668\uff0c\u63d0\u53d6\u9ad8\u5c42\u8bed\u4e49\u548c\u7a7a\u95f4\u7ec6\u8282\uff0c\u7cbe\u786e\u52fe\u52d2\u4f2a\u9020\u533a\u57df\u8fb9\u754c\u3002", "result": "CLUE\u5728\u5e7f\u6cdb\u7684\u8bc4\u4f30\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6cdb\u5316\u6027\u80fd\uff0c\u663e\u8457\u8d85\u8d8a\u4e86\u5148\u524d\u7684\u65b9\u6cd5\uff0c\u5e76\u4e14\u5728\u9762\u5bf9\u5e38\u89c1\u7684\u540e\u5904\u7406\u653b\u51fb\u548c\u5728\u7ebf\u793e\u4ea4\u7f51\u7edc\uff08OSNs\uff09\u65f6\u8868\u73b0\u51fa\u66f4\u5f3a\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "CLUE\u5728\u5e94\u5bf9\u6570\u5b57\u5a92\u4f53\u4f2a\u9020\u65b9\u9762\u5c55\u73b0\u4e86\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u548c\u9c81\u68d2\u6027\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u80fd\u6709\u6548\u62b5\u5fa1\u540e\u5904\u7406\u548c\u793e\u4ea4\u7f51\u7edc\u5e73\u53f0\u7684\u653b\u51fb\u3002"}}
{"id": "2508.08221", "categories": ["cs.LG", "cs.CL"], "pdf": "https://arxiv.org/pdf/2508.08221", "abs": "https://arxiv.org/abs/2508.08221", "authors": ["Zihe Liu", "Jiashun Liu", "Yancheng He", "Weixun Wang", "Jiaheng Liu", "Ling Pan", "Xinyu Hu", "Shaopan Xiong", "Ju Huang", "Jian Hu", "Shengyi Huang", "Siran Yang", "Jiamang Wang", "Wenbo Su", "Bo Zheng"], "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning", "comment": "26 pages, 21 figures", "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent\nresearch area, marked by a significant surge in related studies on both\nalgorithmic innovations and practical applications. Despite this progress,\nseveral critical challenges remain, including the absence of standardized\nguidelines for employing RL techniques and a fragmented understanding of their\nunderlying mechanisms. Additionally, inconsistent experimental settings,\nvariations in training data, and differences in model initialization have led\nto conflicting conclusions, obscuring the key characteristics of these\ntechniques and creating confusion among practitioners when selecting\nappropriate techniques. This paper systematically reviews widely adopted RL\ntechniques through rigorous reproductions and isolated evaluations within a\nunified open-source framework. We analyze the internal mechanisms, applicable\nscenarios, and core principles of each technique through fine-grained\nexperiments, including datasets of varying difficulty, model sizes, and\narchitectures. Based on these insights, we present clear guidelines for\nselecting RL techniques tailored to specific setups, and provide a reliable\nroadmap for practitioners navigating the RL for the LLM domain. Finally, we\nreveal that a minimalist combination of two techniques can unlock the learning\ncapability of critic-free policies using vanilla PPO loss. The results\ndemonstrate that our simple combination consistently improves performance,\nsurpassing strategies like GRPO and DAPO.", "AI": {"tldr": "\u8be5\u8bba\u6587\u901a\u8fc7\u7edf\u4e00\u6846\u67b6\u590d\u73b0\u548c\u8bc4\u4f30RL\u6280\u672f\u5728LLM\u4e2d\u7684\u5e94\u7528\uff0c\u4e3a\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e86\u9009\u62e9\u6307\u5357\u548c\u8def\u7ebf\u56fe\uff0c\u5e76\u53d1\u73b0\u7b80\u5316\u7684PPO\u7b56\u7565\u7ec4\u5408\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u9274\u4e8eRL\u5728LLM\u63a8\u7406\u4e2d\u7684\u5e94\u7528\u7814\u7a76\u6fc0\u589e\uff0c\u4f46\u4ecd\u5b58\u5728\u7f3a\u4e4f\u6807\u51c6\u5316\u6307\u5357\u3001\u673a\u5236\u7406\u89e3\u788e\u7247\u5316\u3001\u5b9e\u9a8c\u8bbe\u7f6e\u4e0d\u4e00\u81f4\u7b49\u6311\u6218\uff0c\u5bfc\u81f4\u5b9e\u8df5\u8005\u96be\u4ee5\u9009\u62e9\u5408\u9002\u7684\u6280\u672f\u3002", "method": "\u901a\u8fc7\u4e25\u683c\u590d\u73b0\u548c\u9694\u79bb\u8bc4\u4f30\uff0c\u5728\u7edf\u4e00\u7684\u5f00\u6e90\u6846\u67b6\u5185\u7cfb\u7edf\u6027\u5730\u56de\u987e\u4e86\u5e7f\u6cdb\u91c7\u7528\u7684RL\u6280\u672f\u3002\u901a\u8fc7\u7ec6\u81f4\u7684\u5b9e\u9a8c\uff0c\u5305\u62ec\u4e0d\u540c\u96be\u5ea6\u7684\u6570\u636e\u96c6\u3001\u6a21\u578b\u5927\u5c0f\u548c\u67b6\u6784\uff0c\u5206\u6790\u4e86\u6bcf\u79cd\u6280\u672f\u7684\u5185\u90e8\u673a\u5236\u3001\u9002\u7528\u573a\u666f\u548c\u6838\u5fc3\u539f\u7406\u3002", "result": "\u901a\u8fc7\u7ec6\u7c92\u5ea6\u5b9e\u9a8c\uff0c\u5206\u6790\u4e86\u4e0d\u540cRL\u6280\u672f\u7684\u5185\u90e8\u673a\u5236\u3001\u9002\u7528\u573a\u666f\u548c\u6838\u5fc3\u539f\u7406\uff0c\u5e76\u63d0\u4f9b\u4e86\u9009\u62e9RL\u6280\u672f\u7684\u660e\u786e\u6307\u5357\u548c\u53ef\u9760\u8def\u7ebf\u56fe\u3002\u7ed3\u679c\u663e\u793a\uff0c\u4e00\u79cd\u7b80\u5316\u7684\u3001\u4ec5\u5305\u542b\u4e24\u79cd\u6280\u672f\u7684\u7ec4\u5408\u7b56\u7565\uff0c\u5229\u7528\u6807\u51c6\u7684PPO\u635f\u5931\uff0c\u80fd\u591f\u89e3\u9501\u65e0\u5224\u522b\u5668\u7b56\u7565\u7684\u5b66\u4e60\u80fd\u529b\uff0c\u5e76\u4e14\u5176\u6027\u80fd\u6301\u7eed\u4f18\u4e8eGRPO\u548cDAPO\u7b49\u7b56\u7565\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542bRL\u6280\u672f\u3001\u5b9e\u9a8c\u8bbe\u7f6e\u548c\u6a21\u578b\u7ec6\u8282\u7684\u7edf\u4e00\u6846\u67b6\uff0c\u65e8\u5728\u4e3aRL\u5728LLM\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u6e05\u6670\u7684\u6307\u5357\u548c\u53ef\u9760\u7684\u8def\u7ebf\u56fe\u3002\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u4e00\u79cd\u7b80\u5316\u7684PPO\u7b56\u7565\u7ec4\u5408\u80fd\u591f\u63d0\u5347\u6027\u80fd\uff0c\u4f18\u4e8eGRPO\u548cDAPO\u7b49\u7b56\u7565\u3002"}}
{"id": "2508.07432", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07432", "abs": "https://arxiv.org/abs/2508.07432", "authors": ["Vivek Hruday Kavuri", "Vysishtya Karanam", "Venkata Jahnavi Venkamsetty", "Kriti Madumadukala", "Lakshmipathi Balaji Darur", "Ponnurangam Kumaraguru"], "title": "Freeze and Reveal: Exposing Modality Bias in Vision-Language Models", "comment": null, "summary": "Vision Language Models achieve impressive multi-modal performance but often\ninherit gender biases from their training data. This bias might be coming from\nboth the vision and text modalities. In this work, we dissect the contributions\nof vision and text backbones to these biases by applying targeted debiasing\nusing Counterfactual Data Augmentation and Task Vector methods. Inspired by\ndata-efficient approaches in hate-speech classification, we introduce a novel\nmetric, Degree of Stereotypicality and a corresponding debiasing method, Data\nAugmentation Using Degree of Stereotypicality - DAUDoS, to reduce bias with\nminimal computational cost. We curate a gender annotated dataset and evaluate\nall methods on VisoGender benchmark to quantify improvements and identify\ndominant source of bias. Our results show that CDA reduces the gender gap by 6%\nand DAUDoS by 3% but using only one-third of the data. Both methods also\nimprove the model's ability to correctly identify gender in images by 3%, with\nDAUDoS achieving this improvement using only almost one-third of training data.\nFrom our experiment's, we observed that CLIP's vision encoder is more biased\nwhereas PaliGemma2's text encoder is more biased. By identifying whether bias\nstems more from vision or text encoders, our work enables more targeted and\neffective bias mitigation strategies in future multi-modal systems.", "AI": {"tldr": "\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08CDA\uff09\u548c\u4e00\u79cd\u65b0\u7684\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff08DAUDoS\uff09\u6765\u7f13\u89e3\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u6027\u522b\u504f\u89c1\u3002\u7814\u7a76\u53d1\u73b0CLIP\u7684\u89c6\u89c9\u7f16\u7801\u5668\u6bd4PaliGemma2\u7684\u6587\u672c\u7f16\u7801\u5668\u5b58\u5728\u66f4\u4e25\u91cd\u7684\u6027\u522b\u504f\u89c1\uff0c\u5e76\u63d0\u51faDAUDoS\u5728\u6570\u636e\u6548\u7387\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08Vision Language Models, VLMs\uff09\u5728\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u5b83\u4eec\u4e5f\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u7ee7\u627f\u4e86\u6027\u522b\u504f\u89c1\uff0c\u8fd9\u79cd\u504f\u89c1\u53ef\u80fd\u540c\u65f6\u6765\u6e90\u4e8e\u89c6\u89c9\u548c\u6587\u672c\u6a21\u6001\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5e94\u7528\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08CDA\uff09\u548c\u4efb\u52a1\u5411\u91cf\uff08Task Vector\uff09\u7b49\u65b9\u6cd5\uff0c\u4ee5\u53ca\u4e00\u79cd\u65b0\u7684\u5ea6\u91cf\u6807\u51c6\u201c\u523b\u677f\u5370\u8c61\u7a0b\u5ea6\u201d\uff08Degree of Stereotypicality, DOS\uff09\u548c\u76f8\u5e94\u7684\u7f13\u89e3\u65b9\u6cd5\u201c\u4f7f\u7528\u523b\u677f\u5370\u8c61\u7a0b\u5ea6\u7684\u6570\u636e\u589e\u5f3a\u201d\uff08DAUDoS\uff09\uff0c\u6765\u89e3\u6790\u89c6\u89c9\u548c\u6587\u672c\u4e3b\u5e72\u7f51\u7edc\u5bf9\u6027\u522b\u504f\u89c1\u7684\u8d21\u732e\uff0c\u5e76\u51cf\u5c11\u504f\u89c1\u3002", "result": "\u53cd\u4e8b\u5b9e\u6570\u636e\u589e\u5f3a\uff08CDA\uff09\u5c06\u6027\u522b\u5dee\u8ddd\u7f29\u5c0f\u4e866%\uff0c\u800cDAUDoS\u7f29\u5c0f\u4e863%\uff0c\u5e76\u4e14DAUDoS\u4ec5\u4f7f\u7528\u4e86\u4e09\u5206\u4e4b\u4e00\u7684\u6570\u636e\u3002\u4e24\u79cd\u65b9\u6cd5\u90fd\u5c06\u6a21\u578b\u6b63\u786e\u8bc6\u522b\u56fe\u50cf\u4e2d\u6027\u522b\u7684\u80fd\u529b\u63d0\u9ad8\u4e863%\uff0c\u5176\u4e2dDAUDoS\u540c\u6837\u4ec5\u4f7f\u7528\u4e86\u4e0d\u5230\u4e09\u5206\u4e4b\u4e00\u7684\u8bad\u7ec3\u6570\u636e\u3002", "conclusion": "CLIP\u7684\u89c6\u89c9\u7f16\u7801\u5668\u5b58\u5728\u66f4\u4e25\u91cd\u7684\u6027\u522b\u504f\u89c1\uff0c\u800cPaliGemma2\u7684\u6587\u672c\u7f16\u7801\u5668\u5b58\u5728\u66f4\u4e25\u91cd\u7684\u6027\u522b\u504f\u89c1\u3002\u901a\u8fc7\u8bc6\u522b\u504f\u89c1\u6765\u6e90\uff0c\u6211\u4eec\u7684\u5de5\u4f5c\u80fd\u591f\u4e3a\u672a\u6765\u7684\u591a\u6a21\u6001\u7cfb\u7edf\u63d0\u4f9b\u66f4\u5177\u9488\u5bf9\u6027\u548c\u66f4\u6709\u6548\u7684\u504f\u89c1\u7f13\u89e3\u7b56\u7565\u3002"}}
{"id": "2508.07887", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07887", "abs": "https://arxiv.org/abs/2508.07887", "authors": ["Sabrina Namazova", "Alessandra Brondetta", "Younes Strittmatter", "Matthew Nassar", "Sebastian Musslick"], "title": "Not Yet AlphaFold for the Mind: Evaluating Centaur as a Synthetic Participant", "comment": null, "summary": "Simulators have revolutionized scientific practice across the natural\nsciences. By generating data that reliably approximate real-world phenomena,\nthey enable scientists to accelerate hypothesis testing and optimize\nexperimental designs. This is perhaps best illustrated by AlphaFold, a\nNobel-prize winning simulator in chemistry that predicts protein structures\nfrom amino acid sequences, enabling rapid prototyping of molecular\ninteractions, drug targets, and protein functions. In the behavioral sciences,\na reliable participant simulator - a system capable of producing human-like\nbehavior across cognitive tasks - would represent a similarly transformative\nadvance. Recently, Binz et al. introduced Centaur, a large language model (LLM)\nfine-tuned on human data from 160 experiments, proposing its use not only as a\nmodel of cognition but also as a participant simulator for \"in silico\nprototyping of experimental studies\", e.g., to advance automated cognitive\nscience. Here, we review the core criteria for a participant simulator and\nassess how well Centaur meets them. Although Centaur demonstrates strong\npredictive accuracy, its generative behavior - a critical criterion for a\nparticipant simulator - systematically diverges from human data. This suggests\nthat, while Centaur is a significant step toward predicting human behavior, it\ndoes not yet meet the standards of a reliable participant simulator or an\naccurate model of cognition.", "AI": {"tldr": "Centaur LLM shows promise in predicting behavior but doesn't yet simulate human participants reliably due to behavioral deviations.", "motivation": "To evaluate the potential of the Centaur LLM as a human participant simulator in the behavioral sciences, analogous to the impact of simulators like AlphaFold in chemistry.", "method": "Review and assessment of the Centaur LLM against core criteria for a participant simulator.", "result": "Centaur demonstrates high predictive accuracy but its generative behavior systematically diverges from human data, indicating it is not yet a reliable participant simulator or accurate cognitive model.", "conclusion": "Centaur LLM, despite strong predictive accuracy, shows systematic divergence in generative behavior from human data, failing to meet the standards of a reliable participant simulator or accurate cognitive model."}}
{"id": "2508.07441", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07441", "abs": "https://arxiv.org/abs/2508.07441", "authors": ["Yuxin Zhang", "Yunkang Cao", "Yuqi Cheng", "Yihan Sun", "Weiming Shen"], "title": "Levarging Learning Bias for Noisy Anomaly Detection", "comment": null, "summary": "This paper addresses the challenge of fully unsupervised image anomaly\ndetection (FUIAD), where training data may contain unlabeled anomalies.\nConventional methods assume anomaly-free training data, but real-world\ncontamination leads models to absorb anomalies as normal, degrading detection\nperformance. To mitigate this, we propose a two-stage framework that\nsystematically exploits inherent learning bias in models. The learning bias\nstems from: (1) the statistical dominance of normal samples, driving models to\nprioritize learning stable normal patterns over sparse anomalies, and (2)\nfeature-space divergence, where normal data exhibit high intra-class\nconsistency while anomalies display high diversity, leading to unstable model\nresponses. Leveraging the learning bias, stage 1 partitions the training set\ninto subsets, trains sub-models, and aggregates cross-model anomaly scores to\nfilter a purified dataset. Stage 2 trains the final detector on this dataset.\nExperiments on the Real-IAD benchmark demonstrate superior anomaly detection\nand localization performance under different noise conditions. Ablation studies\nfurther validate the framework's contamination resilience, emphasizing the\ncritical role of learning bias exploitation. The model-agnostic design ensures\ncompatibility with diverse unsupervised backbones, offering a practical\nsolution for real-world scenarios with imperfect training data. Code is\navailable at https://github.com/hustzhangyuxin/LLBNAD.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u901a\u8fc7\u5229\u7528\u6a21\u578b\u5b66\u4e60\u504f\u5dee\u6765\u89e3\u51b3\u65e0\u76d1\u7763\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6570\u636e\u6c61\u67d3\u95ee\u9898\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u6570\u636e\u51c0\u5316\u63d0\u9ad8\u4e86\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u901a\u7528\u6027\u3002", "motivation": "\u89e3\u51b3\u5728\u8bad\u7ec3\u6570\u636e\u53ef\u80fd\u5305\u542b\u672a\u6807\u8bb0\u5f02\u5e38\u503c\u7684\u5168\u65e0\u76d1\u7763\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\uff08FUIAD\uff09\u7684\u6311\u6218\u3002\u73b0\u6709\u65b9\u6cd5\u5047\u8bbe\u8bad\u7ec3\u6570\u636e\u4e0d\u5305\u542b\u5f02\u5e38\u503c\uff0c\u4f46\u5b9e\u9645\u4e2d\u7684\u6c61\u67d3\u4f1a\u5bfc\u81f4\u6a21\u578b\u5c06\u5f02\u5e38\u503c\u5b66\u4e60\u4e3a\u6b63\u5e38\u503c\uff0c\u4ece\u800c\u964d\u4f4e\u68c0\u6d4b\u6027\u80fd\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7cfb\u7edf\u5730\u5229\u7528\u4e86\u6a21\u578b\u4e2d\u56fa\u6709\u7684\u5b66\u4e60\u504f\u5dee\u3002\u7b2c\u4e00\u9636\u6bb5\u5c06\u8bad\u7ec3\u96c6\u5212\u5206\u4e3a\u5b50\u96c6\uff0c\u8bad\u7ec3\u5b50\u6a21\u578b\uff0c\u5e76\u805a\u5408\u8de8\u6a21\u578b\u5f02\u5e38\u5206\u6570\u4ee5\u7b5b\u9009\u7eaf\u51c0\u6570\u636e\u96c6\u3002\u7b2c\u4e8c\u9636\u6bb5\u5728\u8be5\u6570\u636e\u96c6\u4e0a\u8bad\u7ec3\u6700\u7ec8\u7684\u68c0\u6d4b\u5668\u3002", "result": "\u5728Real-IAD\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u4e0d\u540c\u7684\u566a\u58f0\u6761\u4ef6\u4e0b\u5c55\u793a\u4e86\u4f18\u8d8a\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\u3002\u6a21\u578b\u5bf9\u6c61\u67d3\u5177\u6709\u9c81\u68d2\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u4e0d\u540c\u7684\u65e0\u76d1\u7763\u9aa8\u5e72\u7f51\u7edc\u517c\u5bb9\u3002", "conclusion": "\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u5730\u5904\u7406\u5305\u542b\u5f02\u5e38\u503c\u7684\u8bad\u7ec3\u6570\u636e\uff0c\u5e76\u5728\u5404\u79cd\u566a\u58f0\u6761\u4ef6\u4e0b\u5b9e\u73b0\u4f18\u8d8a\u7684\u5f02\u5e38\u68c0\u6d4b\u548c\u5b9a\u4f4d\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8be5\u6846\u67b6\u5bf9\u6a21\u578b\u5177\u6709\u901a\u7528\u6027\uff0c\u53ef\u4e0e\u4e0d\u540c\u7684\u65e0\u76d1\u7763\u9aa8\u5e72\u7f51\u7edc\u517c\u5bb9\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.07926", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07926", "abs": "https://arxiv.org/abs/2508.07926", "authors": ["Liang Hou", "Yuan Gao", "Boyuan Jiang", "Xin Tao", "Qi Yan", "Renjie Liao", "Pengfei Wan", "Di Zhang", "Kun Gai"], "title": "Score Augmentation for Diffusion Models", "comment": null, "summary": "Diffusion models have achieved remarkable success in generative modeling.\nHowever, this study confirms the existence of overfitting in diffusion model\ntraining, particularly in data-limited regimes. To address this challenge, we\npropose Score Augmentation (ScoreAug), a novel data augmentation framework\nspecifically designed for diffusion models. Unlike conventional augmentation\napproaches that operate on clean data, ScoreAug applies transformations to\nnoisy data, aligning with the inherent denoising mechanism of diffusion.\nCrucially, ScoreAug further requires the denoiser to predict the augmentation\nof the original target. This design establishes an equivariant learning\nobjective, enabling the denoiser to learn scores across varied denoising\nspaces, thereby realizing what we term score augmentation. We also\ntheoretically analyze the relationship between scores in different spaces under\ngeneral transformations. In experiments, we extensively validate ScoreAug on\nmultiple benchmarks including CIFAR-10, FFHQ, AFHQv2, and ImageNet, with\nresults demonstrating significant performance improvements over baselines.\nNotably, ScoreAug effectively mitigates overfitting across diverse scenarios,\nsuch as varying data scales and model capacities, while exhibiting stable\nconvergence properties. Another advantage of ScoreAug over standard data\naugmentation lies in its ability to circumvent data leakage issues under\ncertain conditions. Furthermore, we show that ScoreAug can be synergistically\ncombined with traditional data augmentation techniques to achieve additional\nperformance gains.", "AI": {"tldr": "Diffusion models can overfit, especially with limited data. ScoreAug is a new method that augments noisy data and helps models learn better, improving performance and reducing overfitting.", "motivation": "To address the overfitting issue in diffusion model training, particularly in data-limited regimes.", "method": "ScoreAug, a novel data augmentation framework that applies transformations to noisy data and requires the denoiser to predict the augmentation of the original target, establishing an equivariant learning objective.", "result": "ScoreAug demonstrates significant performance improvements over baselines on CIFAR-10, FFHQ, AFHQv2, and ImageNet, effectively mitigates overfitting, exhibits stable convergence, and can circumvent data leakage issues.", "conclusion": "ScoreAug can effectively mitigate overfitting in diffusion models, improve performance across various benchmarks, and can be combined with traditional data augmentation techniques for further gains."}}
{"id": "2508.07450", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07450", "abs": "https://arxiv.org/abs/2508.07450", "authors": ["Suman Kunwar", "Prabesh Rai"], "title": "Health Care Waste Classification Using Deep Learning Aligned with Nepal's Bin Color Guidelines", "comment": "7 pages, 5 figures", "summary": "The increasing number of Health Care facilities in Nepal has also added up\nthe challenges on managing health care waste (HCW). Improper segregation and\ndisposal of HCW leads to the contamination, spreading of infectious diseases\nand puts a risk of waste handlers. This study benchmarks the state of the art\nwaste classification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S,\nYOLOv8-n and YOLOv5-s using Stratified K-fold techniques where we use 5 folds\non combined HCW data, and found that the YOLOv5-s achieved higher of 95.06%\naccuracy but fell short few milliseconds in inference speed with YOLOv8-n\nmodel. The EfficientNet-B0 showed promising results of 93.22% accuracy but took\nthe highest inference time. A repetitive ANOVA was performed to see statistical\nsignificance and the best performing model (YOLOv5-s) was deployed to the web\nwith mapped bin color using Nepal's HCW management standards for public usage.\nFurther work on the data was suggested along with localized context.", "AI": {"tldr": "\u672c\u7814\u7a76\u8bc4\u4f30\u4e86\u591a\u79cd\u6a21\u578b\u5728\u5c3c\u6cca\u5c14\u533b\u7597\u5e9f\u7269\u5206\u7c7b\u4e2d\u7684\u8868\u73b0\uff0cYOLOv5-s\u51c6\u786e\u7387\u6700\u9ad8\uff0895.06%\uff09\uff0cYOLOv8-n\u901f\u5ea6\u6700\u5feb\uff0c\u5e76\u5c06YOLOv5-s\u90e8\u7f72\u5230Web\u7aef\u3002", "motivation": "\u4e3a\u4e86\u5e94\u5bf9\u5c3c\u6cca\u5c14\u533b\u7597\u4fdd\u5065\u8bbe\u65bd\u589e\u52a0\u5e26\u6765\u7684\u533b\u7597\u5e9f\u7269\u7ba1\u7406\u6311\u6218\uff0c\u7279\u522b\u662f\u7531\u4e8e\u4e0d\u5f53\u5206\u7c7b\u548c\u5904\u7f6e\u5bfc\u81f4\u7684\u6c61\u67d3\u3001\u4f20\u67d3\u75c5\u4f20\u64ad\u548c\u5bf9\u5e9f\u7269\u5904\u7406\u4eba\u5458\u7684\u98ce\u9669\uff0c\u672c\u7814\u7a76\u65e8\u5728\u8bc4\u4f30\u548c\u90e8\u7f72\u5148\u8fdb\u7684\u5e9f\u7269\u5206\u7c7b\u6a21\u578b\u3002", "method": "\u672c\u7814\u7a76\u91c7\u7528\u4e86\u5305\u62ecResNeXt-50\u3001EfficientNet-B0\u3001MobileNetV3-S\u3001YOLOv8-n\u548cYOLOv5-s\u5728\u5185\u7684\u591a\u79cd\u5e9f\u7269\u5206\u7c7b\u6a21\u578b\uff0c\u5e76\u4f7f\u75285\u6298\u5206\u5c42K\u6298\u4ea4\u53c9\u9a8c\u8bc1\u6280\u672f\u8fdb\u884c\u8bc4\u4f30\u3002\u901a\u8fc7\u91cd\u590d\u6d4b\u91cf\u65b9\u5dee\u5206\u6790\uff08ANOVA\uff09\u68c0\u9a8c\u4e86\u7edf\u8ba1\u5b66\u663e\u8457\u6027\u3002", "result": "YOLOv5-s\u6a21\u578b\u5728\u51c6\u786e\u7387\u4e0a\u53d6\u5f97\u4e8695.06%\u7684\u6700\u4f73\u8868\u73b0\uff0c\u800cYOLOv8-n\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u7565\u80dc\u4e00\u7b79\u3002EfficientNet-B0\u51c6\u786e\u7387\u4e3a93.22%\uff0c\u4f46\u63a8\u7406\u65f6\u95f4\u6700\u957f\u3002", "conclusion": "\u7814\u7a76\u8868\u660e\uff0cYOLOv5-s\u5728\u5904\u7406\u5c3c\u6cca\u5c14\u533b\u7597\u5e9f\u7269\u5206\u7c7b\u4efb\u52a1\u4e2d\u8868\u73b0\u6700\u4f73\uff0c\u51c6\u786e\u7387\u8fbe\u523095.06%\uff0c\u4f46YOLOv8-n\u5728\u63a8\u7406\u901f\u5ea6\u4e0a\u7a0d\u5feb\u3002EfficientNet-B0\u4e5f\u6709\u4e0d\u9519\u7684\u8868\u73b0\uff0c\u51c6\u786e\u7387\u4e3a93.22%\uff0c\u4f46\u63a8\u7406\u65f6\u95f4\u6700\u957f\u3002\u6700\u7ec8\uff0cYOLOv5-s\u6a21\u578b\u88ab\u90e8\u7f72\u5230Web\u7aef\uff0c\u5e76\u7ed3\u5408\u5c3c\u6cca\u5c14\u533b\u7597\u5e9f\u7269\u7ba1\u7406\u6807\u51c6\u8fdb\u884c\u4e86\u53ef\u89c6\u5316\u3002"}}
{"id": "2508.07927", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07927", "abs": "https://arxiv.org/abs/2508.07927", "authors": ["Amal Saadallah", "Abdulaziz Al-Ademi"], "title": "Adaptive Fine-Tuning via Pattern Specialization for Deep Time Series Forecasting", "comment": null, "summary": "Time series forecasting poses significant challenges in non-stationary\nenvironments where underlying patterns evolve over time. In this work, we\npropose a novel framework that enhances deep neural network (DNN) performance\nby leveraging specialized model adaptation and selection. Initially, a base DNN\nis trained offline on historical time series data. A reserved validation subset\nis then segmented to extract and cluster the most dominant patterns within the\nseries, thereby identifying distinct regimes. For each identified cluster, the\nbase DNN is fine-tuned to produce a specialized version that captures unique\npattern characteristics. At inference, the most recent input is matched against\nthe cluster centroids, and the corresponding fine-tuned version is deployed\nbased on the closest similarity measure. Additionally, our approach integrates\na concept drift detection mechanism to identify and adapt to emerging patterns\ncaused by non-stationary behavior. The proposed framework is generalizable\nacross various DNN architectures and has demonstrated significant performance\ngains on both traditional DNNs and recent advanced architectures implemented in\nthe GluonTS library.", "AI": {"tldr": "\u901a\u8fc7\u6a21\u578b\u9002\u5e94\u548c\u9009\u62e9\u6765\u63d0\u9ad8\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728\u975e\u5e73\u7a33\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u65f6\u95f4\u5e8f\u5217\u9884\u6d4b\u5728\u975e\u5e73\u7a33\u73af\u5883\u4e2d\uff08\u5373\u5e95\u5c42\u6a21\u5f0f\u968f\u65f6\u95f4\u6f14\u53d8\uff09\u5e26\u6765\u4e86\u91cd\u5927\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u5229\u7528\u4e13\u95e8\u7684\u6a21\u578b\u9002\u5e94\u548c\u9009\u62e9\u6765\u589e\u5f3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\uff08DNN\uff09\u7684\u6027\u80fd\u3002\u9996\u5148\uff0c\u5728\u5386\u53f2\u65f6\u95f4\u5e8f\u5217\u6570\u636e\u4e0a\u79bb\u7ebf\u8bad\u7ec3\u57fa\u7840DNN\u3002\u7136\u540e\uff0c\u5bf9\u4fdd\u7559\u7684\u9a8c\u8bc1\u5b50\u96c6\u8fdb\u884c\u5206\u5272\uff0c\u63d0\u53d6\u5e76\u805a\u7c7b\u5e8f\u5217\u4e2d\u6700\u4e3b\u8981\u7684\u6a21\u5f0f\uff0c\u4ece\u800c\u8bc6\u522b\u51fa\u4e0d\u540c\u7684\u72b6\u6001\u3002\u5bf9\u6bcf\u4e2a\u8bc6\u522b\u51fa\u7684\u96c6\u7fa4\uff0c\u5bf9\u57fa\u7840DNN\u8fdb\u884c\u5fae\u8c03\uff0c\u751f\u6210\u80fd\u591f\u6355\u83b7\u72ec\u7279\u6a21\u5f0f\u7279\u5f81\u7684\u4e13\u95e8\u7248\u672c\u3002\u5728\u63a8\u7406\u65f6\uff0c\u5c06\u6700\u8fd1\u7684\u8f93\u5165\u4e0e\u96c6\u7fa4\u4e2d\u5fc3\u8fdb\u884c\u5339\u914d\uff0c\u5e76\u6839\u636e\u6700\u76f8\u4f3c\u7684\u5ea6\u91cf\u6807\u51c6\u90e8\u7f72\u76f8\u5e94\u7684\u5fae\u8c03\u7248\u672c\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u96c6\u6210\u4e86\u6982\u5ff5\u6f02\u79fb\u68c0\u6d4b\u673a\u5236\uff0c\u4ee5\u8bc6\u522b\u548c\u9002\u5e94\u7531\u975e\u5e73\u7a33\u884c\u4e3a\u5f15\u8d77\u7684\u65b0\u5174\u6a21\u5f0f\u3002", "result": "\u6240\u63d0\u51fa\u7684\u6846\u67b6\u901a\u7528\u6027\u5f3a\uff0c\u5728\u4f20\u7edfDNN\u548cGluonTS\u5e93\u4e2d\u5b9e\u73b0\u7684\u5148\u8fdb\u67b6\u6784\u4e0a\u5747\u5c55\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u7528\u6027\u5f3a\uff0c\u53ef\u9002\u7528\u4e8e\u5404\u79cd\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\uff0c\u5e76\u5728GluonTS\u5e93\u4e2d\u5b9e\u73b0\u7684\u4f20\u7edf\u548c\u5148\u8fdb\u67b6\u6784\u4e0a\u5747\u5c55\u793a\u51fa\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.07470", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07470", "abs": "https://arxiv.org/abs/2508.07470", "authors": ["Siminfar Samakoush Galougah", "Rishie Raj", "Sanjoy Chowdhury", "Sayan Nag", "Ramani Duraiswami"], "title": "AURA: A Fine-Grained Benchmark and Decomposed Metric for Audio-Visual Reasoning", "comment": null, "summary": "Current audio-visual (AV) benchmarks focus on final answer accuracy,\noverlooking the underlying reasoning process. This makes it difficult to\ndistinguish genuine comprehension from correct answers derived through flawed\nreasoning or hallucinations. To address this, we introduce AURA (Audio-visual\nUnderstanding and Reasoning Assessment), a benchmark for evaluating the\ncross-modal reasoning capabilities of Audio-Visual Large Language Models\n(AV-LLMs) and Omni-modal Language Models (OLMs). AURA includes questions across\nsix challenging cognitive domains, such as causality, timbre and pitch, tempo\nand AV synchronization, unanswerability, implicit distractions, and skill\nprofiling, explicitly designed to be unanswerable from a single modality. This\nforces models to construct a valid logical path grounded in both audio and\nvideo, setting AURA apart from AV datasets that allow uni-modal shortcuts. To\nassess reasoning traces, we propose a novel metric, AuraScore, which addresses\nthe lack of robust tools for evaluating reasoning fidelity. It decomposes\nreasoning into two aspects: (i) Factual Consistency - whether reasoning is\ngrounded in perceptual evidence, and (ii) Core Inference - the logical validity\nof each reasoning step. Evaluations of SOTA models on AURA reveal a critical\nreasoning gap: although models achieve high accuracy (up to 92% on some tasks),\ntheir Factual Consistency and Core Inference scores fall below 45%. This\ndiscrepancy highlights that models often arrive at correct answers through\nflawed logic, underscoring the need for our benchmark and paving the way for\nmore robust multimodal evaluation.", "AI": {"tldr": "AURA\u57fa\u51c6\u548cAuraScore\u6307\u6807\u65e8\u5728\u89e3\u51b3\u5f53\u524d\u97f3\u89c6\u9891\u591a\u6a21\u6001\u6a21\u578b\u63a8\u7406\u8bc4\u4f30\u7684\u4e0d\u8db3\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u6a21\u578b\u51c6\u786e\u7387\u9ad8\uff0c\u4f46\u63a8\u7406\u8fc7\u7a0b\u5b58\u5728\u7f3a\u9677\uff0c\u5f3a\u8c03\u4e86\u66f4\u53ef\u9760\u7684\u591a\u6a21\u6001\u8bc4\u4f30\u7684\u5fc5\u8981\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u97f3\u89c6\u9891\uff08AV\uff09\u57fa\u51c6\u8fc7\u4e8e\u5173\u6ce8\u6700\u7ec8\u7b54\u6848\u7684\u51c6\u786e\u6027\uff0c\u5ffd\u7565\u4e86\u6f5c\u5728\u7684\u63a8\u7406\u8fc7\u7a0b\uff0c\u8fd9\u4f7f\u5f97\u533a\u5206\u771f\u5b9e\u7406\u89e3\u548c\u57fa\u4e8e\u9519\u8bef\u63a8\u7406\u6216\u5e7b\u89c9\u5f97\u51fa\u7684\u6b63\u786e\u7b54\u6848\u53d8\u5f97\u56f0\u96be\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4e00\u4e2a\u80fd\u591f\u8bc4\u4f30\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u7684\u57fa\u51c6\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u4e86AURA\u57fa\u51c6\uff0c\u5305\u542b\u516d\u4e2a\u5177\u6709\u6311\u6218\u6027\u7684\u8ba4\u77e5\u9886\u57df\uff0c\u8fd9\u4e9b\u95ee\u9898\u88ab\u8bbe\u8ba1\u4e3a\u65e0\u6cd5\u4ec5\u51ed\u5355\u4e00\u6a21\u6001\u56de\u7b54\uff0c\u4ece\u800c\u5f3a\u5236\u6a21\u578b\u7ed3\u5408\u97f3\u9891\u548c\u89c6\u9891\u4fe1\u606f\u8fdb\u884c\u63a8\u7406\u3002\u6b64\u5916\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAuraScore\u7684\u65b0\u578b\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u63a8\u7406\u8f68\u8ff9\uff0c\u8be5\u6307\u6807\u5305\u542b\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u6838\u5fc3\u63a8\u7406\u4e24\u4e2a\u65b9\u9762\u3002", "result": "\u5728AURA\u57fa\u51c6\u4e0a\u5bf9\u6700\u5148\u8fdb\u6a21\u578b\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u6a21\u578b\u867d\u7136\u5728\u67d0\u4e9b\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u9ad8\u8fbe92%\u7684\u51c6\u786e\u7387\uff0c\u4f46\u5176\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u6838\u5fc3\u63a8\u7406\u5f97\u5206\u5374\u4f4e\u4e8e45%\uff0c\u8fd9\u63ed\u793a\u4e86\u6a21\u578b\u5728\u63a8\u7406\u65b9\u9762\u5b58\u5728\u7684\u4e25\u91cd\u4e0d\u8db3\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86AURA\u57fa\u51c6\u548cAuraScore\u6307\u6807\uff0c\u65e8\u5728\u8bc4\u4f30\u548c\u6539\u8fdb\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08AV-LLMs\u548cOLMs\uff09\u7684\u8de8\u6a21\u6001\u63a8\u7406\u80fd\u529b\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u73b0\u6709\u6a21\u578b\u5728\u6700\u7ec8\u7b54\u6848\u51c6\u786e\u7387\u4e0a\u8868\u73b0\u826f\u597d\uff0c\u4f46\u5176\u4e8b\u5b9e\u4e00\u81f4\u6027\u548c\u6838\u5fc3\u63a8\u7406\u80fd\u529b\u5b58\u5728\u663e\u8457\u5dee\u8ddd\uff0c\u8868\u660e\u6a21\u578b\u5e38\u901a\u8fc7\u6709\u7f3a\u9677\u7684\u903b\u8f91\u5f97\u51fa\u6b63\u786e\u7b54\u6848\u3002"}}
{"id": "2508.07952", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07952", "abs": "https://arxiv.org/abs/2508.07952", "authors": ["Richard J. Fawley", "Renato Cordeiro de Amorim"], "title": "Shapley-Inspired Feature Weighting in $k$-means with No Additional Hyperparameters", "comment": null, "summary": "Clustering algorithms often assume all features contribute equally to the\ndata structure, an assumption that usually fails in high-dimensional or noisy\nsettings. Feature weighting methods can address this, but most require\nadditional parameter tuning. We propose SHARK (Shapley Reweighted $k$-means), a\nfeature-weighted clustering algorithm motivated by the use of Shapley values\nfrom cooperative game theory to quantify feature relevance, which requires no\nadditional parameters beyond those in $k$-means. We prove that the $k$-means\nobjective can be decomposed into a sum of per-feature Shapley values, providing\nan axiomatic foundation for unsupervised feature relevance and reducing Shapley\ncomputation from exponential to polynomial time. SHARK iteratively re-weights\nfeatures by the inverse of their Shapley contribution, emphasising informative\ndimensions and down-weighting irrelevant ones. Experiments on synthetic and\nreal-world data sets show that SHARK consistently matches or outperforms\nexisting methods, achieving superior robustness and accuracy, particularly in\nscenarios where noise may be present. Software:\nhttps://github.com/rickfawley/shark.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07483", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.07483", "abs": "https://arxiv.org/abs/2508.07483", "authors": ["Pranav Chougule"], "title": "Novel View Synthesis with Gaussian Splatting: Impact on Photogrammetry Model Accuracy and Resolution", "comment": null, "summary": "In this paper, I present a comprehensive study comparing Photogrammetry and\nGaussian Splatting techniques for 3D model reconstruction and view synthesis. I\ncreated a dataset of images from a real-world scene and constructed 3D models\nusing both methods. To evaluate the performance, I compared the models using\nstructural similarity index (SSIM), peak signal-to-noise ratio (PSNR), learned\nperceptual image patch similarity (LPIPS), and lp/mm resolution based on the\nUSAF resolution chart. A significant contribution of this work is the\ndevelopment of a modified Gaussian Splatting repository, which I forked and\nenhanced to enable rendering images from novel camera poses generated in the\nBlender environment. This innovation allows for the synthesis of high-quality\nnovel views, showcasing the flexibility and potential of Gaussian Splatting. My\ninvestigation extends to an augmented dataset that includes both original\nground images and novel views synthesized via Gaussian Splatting. This\naugmented dataset was employed to generate a new photogrammetry model, which\nwas then compared against the original photogrammetry model created using only\nthe original images. The results demonstrate the efficacy of using Gaussian\nSplatting to generate novel high-quality views and its potential to improve\nphotogrammetry-based 3D reconstructions. The comparative analysis highlights\nthe strengths and limitations of both approaches, providing valuable\ninformation for applications in extended reality (XR), photogrammetry, and\nautonomous vehicle simulations. Code is available at\nhttps://github.com/pranavc2255/gaussian-splatting-novel-view-render.git.", "AI": {"tldr": "\u672c\u7814\u7a76\u6bd4\u8f83\u4e86\u6444\u5f71\u6d4b\u91cf\u548c\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u57283D\u91cd\u5efa\u548c\u89c6\u89d2\u5408\u6210\u65b9\u9762\u7684\u6027\u80fd\u3002\u7ed3\u679c\u663e\u793a\uff0c\u9ad8\u65af\u6e85\u5c04\u5728\u751f\u6210\u65b0\u89c6\u89d2\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u80fd\u63d0\u5347\u6444\u5f71\u6d4b\u91cf\u7684\u91cd\u5efa\u6548\u679c\uff0c\u4e3aXR\u3001\u6444\u5f71\u6d4b\u91cf\u548c\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u7b49\u9886\u57df\u63d0\u4f9b\u4e86\u53c2\u8003\u3002", "motivation": "\u672c\u7814\u7a76\u65e8\u5728\u5168\u9762\u6bd4\u8f83\u6444\u5f71\u6d4b\u91cf\u548c\u9ad8\u65af\u6e85\u5c04\u6280\u672f\u57283D\u6a21\u578b\u91cd\u5efa\u548c\u89c6\u89d2\u5408\u6210\u65b9\u9762\u7684\u6027\u80fd\uff0c\u5e76\u63a2\u7d22\u9ad8\u65af\u6e85\u5c04\u5728\u6539\u8fdb3D\u91cd\u5efa\u65b9\u9762\u7684\u6f5c\u529b\u3002", "method": "\u8be5\u7814\u7a76\u521b\u5efa\u4e86\u4e00\u4e2a\u771f\u5b9e\u4e16\u754c\u573a\u666f\u7684\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u5e76\u4f7f\u7528\u6444\u5f71\u6d4b\u91cf\u548c\u9ad8\u65af\u6e85\u5c04\u4e24\u79cd\u65b9\u6cd5\u6784\u5efa\u4e863D\u6a21\u578b\u3002\u901a\u8fc7SSIM\u3001PSNR\u3001LPIPS\u548c\u57fa\u4e8eUSAF\u5206\u8fa8\u7387\u56fe\u7684lp/mm\u5206\u8fa8\u7387\u5bf9\u6a21\u578b\u8fdb\u884c\u4e86\u6027\u80fd\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u7814\u7a76\u8fd8\u5f00\u53d1\u5e76\u589e\u5f3a\u4e86\u4e00\u4e2a\u4fee\u6539\u7248\u7684\u9ad8\u65af\u6e85\u5c04\u5b58\u50a8\u5e93\uff0c\u7528\u4e8e\u6e32\u67d3\u5728Blender\u73af\u5883\u4e2d\u751f\u6210\u7684\u65b0\u76f8\u673a\u89c6\u89d2\u3002\u7136\u540e\uff0c\u4f7f\u7528\u5305\u542b\u539f\u59cb\u56fe\u50cf\u548c\u9ad8\u65af\u6e85\u5c04\u5408\u6210\u7684\u65b0\u89c6\u89d2\u7684\u589e\u5f3a\u6570\u636e\u96c6\u751f\u6210\u4e86\u4e00\u4e2a\u65b0\u7684\u6444\u5f71\u6d4b\u91cf\u6a21\u578b\uff0c\u5e76\u4e0e\u4ec5\u4f7f\u7528\u539f\u59cb\u56fe\u50cf\u521b\u5efa\u7684\u539f\u59cb\u6444\u5f71\u6d4b\u91cf\u6a21\u578b\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "result": "\u9ad8\u65af\u6e85\u5c04\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u7684\u65b0\u89c6\u89d2\uff0c\u5728\u5305\u62ecSSIM\u3001PSNR\u548cLPIPS\u5728\u5185\u7684\u5404\u9879\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u51fa\u8272\u3002\u4f7f\u7528\u9ad8\u65af\u6e85\u5c04\u751f\u6210\u7684\u65b0\u89c6\u89d2\u6539\u8fdb\u4e86\u6444\u5f71\u6d4b\u91cf\u6a21\u578b\u7684\u91cd\u5efa\u8d28\u91cf\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u8868\u660e\uff0c\u9ad8\u65af\u6e85\u5c04\u5728\u751f\u6210\u65b0\u89c6\u89d2\u65b9\u9762\u975e\u5e38\u6709\u6548\uff0c\u5e76\u6709\u6f5c\u529b\u6539\u8fdb\u57fa\u4e8e\u6444\u5f71\u6d4b\u91cf\u5b66\u76843D\u91cd\u5efa\u3002"}}
{"id": "2508.07970", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07970", "abs": "https://arxiv.org/abs/2508.07970", "authors": ["Junyu Wu", "Weiming Chang", "Xiaotao Liu", "Guanyou He", "Tingfeng Xian", "Haoqiang Hong", "Boqi Chen", "Haotao Tian", "Tao Yang", "Yunsheng Shi", "Feng Lin", "Ting Yao"], "title": "WeChat-YATT: A Simple, Scalable and Balanced RLHF Trainer", "comment": "arXiv admin note: substantial text overlap with arXiv:2507.22789", "summary": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent\nparadigm for training large language models and multimodal systems. Despite\nnotable advances enabled by existing RLHF training frameworks, significant\nchallenges remain in scaling to complex multimodal workflows and adapting to\ndynamic workloads. In particular, current systems often encounter limitations\nrelated to controller scalability when managing large models, as well as\ninefficiencies in orchestrating intricate RLHF pipelines, especially in\nscenarios that require dynamic sampling and resource allocation. In this paper,\nwe introduce WeChat-YATT (Yet Another Transformer Trainer in WeChat), a simple,\nscalable, and balanced RLHF training framework specifically designed to address\nthese challenges. WeChat-YATT features a parallel controller programming model\nthat enables flexible and efficient orchestration of complex RLHF workflows,\neffectively mitigating the bottlenecks associated with centralized controller\narchitectures and facilitating scalability in large-scale data scenarios. In\naddition, we propose a dynamic placement schema that adaptively partitions\ncomputational resources and schedules workloads, thereby significantly reducing\nhardware idle time and improving GPU utilization under variable training\nconditions. We evaluate WeChat-YATT across a range of experimental scenarios,\ndemonstrating that it achieves substantial improvements in throughput compared\nto state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been\nsuccessfully deployed to train models supporting WeChat product features for a\nlarge-scale user base, underscoring its effectiveness and robustness in\nreal-world applications.", "AI": {"tldr": "WeChat-YATT\u662f\u4e00\u4e2a\u7528\u4e8eRLHF\u8bad\u7ec3\u7684\u65b0\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u5e76\u884c\u63a7\u5236\u5668\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u63d0\u9ad8\u4e86\u53ef\u6269\u5c55\u6027\u548c\u6548\u7387\uff0c\u5e76\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u5f97\u5230\u4e86\u9a8c\u8bc1\u3002", "motivation": "\u73b0\u6709\u7684RLHF\u8bad\u7ec3\u6846\u67b6\u5728\u6269\u5c55\u5230\u590d\u6742\u7684\u591a\u6a21\u6001\u5de5\u4f5c\u6d41\u548c\u9002\u5e94\u52a8\u6001\u5de5\u4f5c\u8d1f\u8f7d\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u7ba1\u7406\u5927\u578b\u6a21\u578b\u548c\u7f16\u6392\u590d\u6742\u7684RLHF\u6d41\u6c34\u7ebf\u65f6\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aWeChat-YATT\u7684RLHF\u8bad\u7ec3\u6846\u67b6\uff0c\u91c7\u7528\u4e86\u5e76\u884c\u63a7\u5236\u5668\u7f16\u7a0b\u6a21\u578b\u548c\u52a8\u6001\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff0c\u4ee5\u89e3\u51b3\u73b0\u6709\u6846\u67b6\u5728\u53ef\u6269\u5c55\u6027\u3001\u590d\u6742\u5de5\u4f5c\u6d41\u7f16\u6392\u548c\u52a8\u6001\u8d1f\u8f7d\u9002\u5e94\u65b9\u9762\u7684\u6311\u6218\u3002", "result": "WeChat-YATT\u5728\u541e\u5410\u91cf\u65b9\u9762\u76f8\u6bd4\u6700\u5148\u8fdb\u7684RLHF\u8bad\u7ec3\u6846\u67b6\u53d6\u5f97\u4e86\u663e\u8457\u7684\u6539\u8fdb\u3002", "conclusion": "WeChat-YATT\u6846\u67b6\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u6210\u529f\u90e8\u7f72\uff0c\u7528\u4e8e\u8bad\u7ec3\u652f\u6301\u5fae\u4fe1\u4ea7\u54c1\u529f\u80fd\u7684\u6a21\u578b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6709\u6548\u6027\u548c\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.07493", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07493", "abs": "https://arxiv.org/abs/2508.07493", "authors": ["Jian Chen", "Ming Li", "Jihyung Kil", "Chenguang Wang", "Tong Yu", "Ryan Rossi", "Tianyi Zhou", "Changyou Chen", "Ruiyi Zhang"], "title": "VisR-Bench: An Empirical Study on Visual Retrieval-Augmented Generation for Multilingual Long Document Understanding", "comment": "Under Review", "summary": "Most organizational data in this world are stored as documents, and visual\nretrieval plays a crucial role in unlocking the collective intelligence from\nall these documents. However, existing benchmarks focus on English-only\ndocument retrieval or only consider multilingual question-answering on a\nsingle-page image. To bridge this gap, we introduce VisR-Bench, a multilingual\nbenchmark designed for question-driven multimodal retrieval in long documents.\nOur benchmark comprises over 35K high-quality QA pairs across 1.2K documents,\nenabling fine-grained evaluation of multimodal retrieval. VisR-Bench spans\nsixteen languages with three question types (figures, text, and tables),\noffering diverse linguistic and question coverage. Unlike prior datasets, we\ninclude queries without explicit answers, preventing models from relying on\nsuperficial keyword matching. We evaluate various retrieval models, including\ntext-based methods, multimodal encoders, and MLLMs, providing insights into\ntheir strengths and limitations. Our results show that while MLLMs\nsignificantly outperform text-based and multimodal encoder models, they still\nstruggle with structured tables and low-resource languages, highlighting key\nchallenges in multilingual visual retrieval.", "AI": {"tldr": "VisR-Bench\u662f\u4e00\u4e2a\u591a\u8bed\u8a00\u957f\u6587\u6863\u68c0\u7d22\u57fa\u51c6\uff0c\u5305\u542b35K+\u95ee\u7b54\u5bf9\uff0c\u8986\u76d616\u79cd\u8bed\u8a00\u3002LMMs\u8868\u73b0\u4f18\u4e8e\u5176\u4ed6\u6a21\u578b\uff0c\u4f46\u5728\u8868\u683c\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u6709\u5f85\u6539\u8fdb\u3002", "motivation": "\u586b\u8865\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u5728\u591a\u8bed\u8a00\u6587\u6863\u68c0\u7d22\u65b9\u9762\u7684\u7a7a\u767d\uff0c\u5e76\u89e3\u51b3\u4ec5\u9650\u4e8e\u82f1\u6587\u6216\u5355\u9875\u56fe\u50cf\u95ee\u7b54\u7684\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86VisR-Bench\uff0c\u4e00\u4e2a\u5305\u542b\u8d85\u8fc73.5\u4e07\u4e2a\u9ad8\u8d28\u91cf\u95ee\u7b54\u5bf9\u7684\u6570\u636e\u96c6\uff0c\u8986\u76d61.2K\u4efd\u957f\u6587\u6863\uff0c\u652f\u630116\u79cd\u8bed\u8a00\u548c\u4e09\u79cd\u95ee\u9898\u7c7b\u578b\uff08\u56fe\u5f62\u3001\u6587\u672c\u3001\u8868\u683c\uff09\uff0c\u7528\u4e8e\u8bc4\u4f30\u9a71\u52a8\u5f0f\u591a\u6a21\u6001\u68c0\u7d22\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff08LMMs\uff09\u663e\u8457\u4f18\u4e8e\u57fa\u4e8e\u6587\u672c\u548c\u591a\u6a21\u6001\u7f16\u7801\u5668\u7684\u6a21\u578b\uff0c\u4f46\u5728\u5904\u7406\u8868\u683c\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u4ecd\u9762\u4e34\u6311\u6218\u3002", "conclusion": "\u73b0\u6709\u7684\u6a21\u578b\u5728\u5904\u7406\u7ed3\u6784\u5316\u8868\u683c\u548c\u4f4e\u8d44\u6e90\u8bed\u8a00\u65b9\u9762\u4ecd\u6709\u4e0d\u8db3\uff0c\u8fd9\u6307\u51fa\u4e86\u591a\u8bed\u8a00\u89c6\u89c9\u68c0\u7d22\u7684\u5173\u952e\u6311\u6218\u3002"}}
{"id": "2508.07501", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07501", "abs": "https://arxiv.org/abs/2508.07501", "authors": ["Xiaoye Zuo", "Nikos Athanasiou", "Ginger Delmas", "Yiming Huang", "Xingyu Fu", "Lingjie Liu"], "title": "FormCoach: Lift Smarter, Not Harder", "comment": null, "summary": "Good form is the difference between strength and strain, yet for the\nfast-growing community of at-home fitness enthusiasts, expert feedback is often\nout of reach. FormCoach transforms a simple camera into an always-on,\ninteractive AI training partner, capable of spotting subtle form errors and\ndelivering tailored corrections in real time, leveraging vision-language models\n(VLMs). We showcase this capability through a web interface and benchmark\nstate-of-the-art VLMs on a dataset of 1,700 expert-annotated user-reference\nvideo pairs spanning 22 strength and mobility exercises. To accelerate research\nin AI-driven coaching, we release both the dataset and an automated,\nrubric-based evaluation pipeline, enabling standardized comparison across\nmodels. Our benchmarks reveal substantial gaps compared to human-level\ncoaching, underscoring both the challenges and opportunities in integrating\nnuanced, context-aware movement analysis into interactive AI systems. By\nframing form correction as a collaborative and creative process between humans\nand machines, FormCoach opens a new frontier in embodied AI.", "AI": {"tldr": "FormCoach uses AI (VLMs) and a camera to give real-time fitness form feedback at home. Current AI isn't as good as humans, but releasing a dataset and evaluation tools will help improve it.", "motivation": "To address the lack of expert feedback for the growing community of at-home fitness enthusiasts by creating an AI training partner that can provide real-time form correction.", "method": "Leveraging vision-language models (VLMs) to analyze user movements captured by a camera, identify form errors, and provide real-time corrections. A web interface showcases this capability. Benchmarking was performed on a dataset of 1,700 expert-annotated user-reference video pairs across 22 exercises.", "result": "Benchmarks reveal substantial gaps between current VLMs and human-level coaching in analyzing movement nuances and context. The dataset and evaluation pipeline are released to facilitate further research.", "conclusion": "FormCoach transforms a simple camera into an AI training partner that spots form errors and gives corrections in real time, using vision-language models (VLMs). Benchmarks on a dataset of 1,700 expert-annotated user-reference video pairs show significant gaps compared to human coaching, highlighting challenges and opportunities in AI movement analysis. The release of the dataset and evaluation pipeline aims to accelerate research in AI-driven coaching."}}
{"id": "2508.08005", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08005", "abs": "https://arxiv.org/abs/2508.08005", "authors": ["Xiang Li", "Shanshan Wang", "Chenglong Xiao"], "title": "Learning to Select MCP Algorithms: From Traditional ML to Dual-Channel GAT-MLP", "comment": "10 pages, 6 figures", "summary": "Extensive experiments and prior studies show that no single maximum clique\nalgorithm consistently performs best across all instances, highlighting the\nimportance of selecting suitable algorithms based on instance features. Through\nan extensive analysis of relevant studies, it is found that there is a lack of\nresearch work concerning algorithm selection oriented toward the Maximum Clique\nProblem (MCP). In this work, we propose a learning-based framework that\nintegrates both traditional machine learning and graph neural networks to\naddress this gap. We construct a labeled dataset by running four exact MCP\nalgorithms on a diverse collection of graph instances, accompanied by\nstructural and global statistical features extracted from each graph. We first\nevaluate four conventional classifiers: Support Vector Machine (SVM), Random\nForest (RF), Decision Tree (DT), and K-Nearest Neighbors (KNN), across multiple\ndataset variants. Experimental results show that RF consistently shows strong\nperformance across metrics and dataset variants, making it a reliable baseline.\nIn addition, feature importance analysis indicates that connectivity and\ntopological structure are strong predictors of algorithm performance. Building\non these findings, we develop a dual-channel model named GAT-MLP, which\ncombines a Graph Attention Network (GAT) for local structural encoding with a\nMultilayer Perceptron (MLP) for global feature modeling. The GAT-MLP model\nshows strong and consistent performance across all metrics. Our results\nhighlight the effectiveness of dual-channel architectures and the promise of\ngraph neural networks in combinatorial algorithm selection.", "AI": {"tldr": "\u7531\u4e8e\u6ca1\u6709\u4e00\u79cd\u6700\u5927\u56e2\u95ee\u9898\uff08MCP\uff09\u7b97\u6cd5\u80fd\u5728\u6240\u6709\u5b9e\u4f8b\u4e0a\u8868\u73b0\u6700\u4f73\uff0c\u56e0\u6b64\u9700\u8981\u6839\u636e\u5b9e\u4f8b\u7279\u5f81\u9009\u62e9\u5408\u9002\u7684\u7b97\u6cd5\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u548c\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u7684\u5b66\u4e60\u6846\u67b6\uff08GAT-MLP\uff09\uff0c\u7528\u4e8eMCP\u7b97\u6cd5\u9009\u62e9\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u4e14\u7a33\u5b9a\u7684\u7ed3\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6700\u5927\u56e2\u95ee\u9898\uff08MCP\uff09\u7b97\u6cd5\u5728\u4e0d\u540c\u5b9e\u4f8b\u4e0a\u8868\u73b0\u4e0d\u4e00\uff0c\u7b97\u6cd5\u9009\u62e9\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u76ee\u524d\u9488\u5bf9MCP\u7684\u7b97\u6cd5\u9009\u62e9\u7814\u7a76\u5c1a\u7f3a\u4e4f\u3002\u672c\u7814\u7a76\u65e8\u5728\u5f25\u8865\u8fd9\u4e00\u7814\u7a76\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u548c\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u7684\u5b66\u4e60\u6846\u67b6\u3002\u9996\u5148\uff0c\u4f7f\u7528SVM\u3001RF\u3001DT\u548cKNN\u56db\u79cd\u4f20\u7edf\u5206\u7c7b\u5668\u8fdb\u884c\u8bc4\u4f30\uff0c\u5e76\u53d1\u73b0\u968f\u673a\u68ee\u6797\uff08RF\uff09\u8868\u73b0\u7a33\u5b9a\u3002\u968f\u540e\uff0c\u5f00\u53d1\u4e86\u4e00\u4e2a\u540d\u4e3aGAT-MLP\u7684\u53cc\u901a\u9053\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u7528\u4e8e\u5c40\u90e8\u7ed3\u6784\u7f16\u7801\u7684\u56fe\u6ce8\u610f\u529b\u7f51\u7edc\uff08GAT\uff09\u548c\u7528\u4e8e\u5168\u5c40\u7279\u5f81\u5efa\u6a21\u7684\u591a\u5c42\u611f\u77e5\u673a\uff08MLP\uff09\u3002", "result": "GAT-MLP\u6a21\u578b\u5728\u6240\u6709\u6307\u6807\u4e0a\u5747\u8868\u73b0\u51fa\u5f3a\u5927\u4e14\u4e00\u81f4\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7b97\u6cd5\u9009\u62e9\u4efb\u52a1\u4e0a\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u51f8\u663e\u4e86\u53cc\u901a\u9053\u67b6\u6784\u7684\u6709\u6548\u6027\u4ee5\u53ca\u56fe\u795e\u7ecf\u7f51\u7edc\u5728\u7ec4\u5408\u7b97\u6cd5\u9009\u62e9\u65b9\u9762\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07514", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07514", "abs": "https://arxiv.org/abs/2508.07514", "authors": ["Artzai Picon", "Itziar Eguskiza", "Daniel Mugica", "Javier Romero", "Carlos Javier Jimenez", "Eric White", "Gabriel Do-Lago-Junqueira", "Christian Klukas", "Ramon Navarra-Mestre"], "title": "From Field to Drone: Domain Drift Tolerant Automated Multi-Species and Damage Plant Semantic Segmentation for Herbicide Trials", "comment": null, "summary": "Field trials are vital in herbicide research and development to assess\neffects on crops and weeds under varied conditions. Traditionally, evaluations\nrely on manual visual assessments, which are time-consuming, labor-intensive,\nand subjective. Automating species and damage identification is challenging due\nto subtle visual differences, but it can greatly enhance efficiency and\nconsistency.\n  We present an improved segmentation model combining a general-purpose\nself-supervised visual model with hierarchical inference based on botanical\ntaxonomy. Trained on a multi-year dataset (2018-2020) from Germany and Spain\nusing digital and mobile cameras, the model was tested on digital camera data\n(year 2023) and drone imagery from the United States, Germany, and Spain (year\n2024) to evaluate robustness under domain shift. This cross-device evaluation\nmarks a key step in assessing generalization across platforms of the model.\n  Our model significantly improved species identification (F1-score: 0.52 to\n0.85, R-squared: 0.75 to 0.98) and damage classification (F1-score: 0.28 to\n0.44, R-squared: 0.71 to 0.87) over prior methods. Under domain shift (drone\nimages), it maintained strong performance with moderate degradation (species:\nF1-score 0.60, R-squared 0.80; damage: F1-score 0.41, R-squared 0.62), where\nearlier models failed.\n  These results confirm the model's robustness and real-world applicability. It\nis now deployed in BASF's phenotyping pipeline, enabling large-scale, automated\ncrop and weed monitoring across diverse geographies.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u5206\u5272\u6a21\u578b\u901a\u8fc7\u7ed3\u5408\u81ea\u76d1\u7763\u5b66\u4e60\u548c\u690d\u7269\u5206\u7c7b\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u4f5c\u7269\u548c\u6742\u8349\u8bc6\u522b\u7684\u51c6\u786e\u6027\u548c\u6548\u7387\uff0c\u5e76\u5728\u4e0d\u540c\u8bbe\u5907\u548c\u5730\u7406\u533a\u57df\u7684\u91ce\u5916\u8bd5\u9a8c\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\uff0c\u73b0\u5df2\u5e94\u7528\u4e8eBASF\u7684\u81ea\u52a8\u5316\u76d1\u6d4b\u7cfb\u7edf\u3002", "motivation": "\u4f20\u7edf\u7684\u91ce\u5916\u8bd5\u9a8c\u4f9d\u8d56\u4e8e\u8017\u65f6\u3001\u8017\u529b\u4e14\u4e3b\u89c2\u7684\u4eba\u5de5\u76ee\u89c6\u8bc4\u4f30\uff0c\u800c\u81ea\u52a8\u5316\u7269\u79cd\u548c\u635f\u5bb3\u8bc6\u522b\u56e0\u7ec6\u5fae\u7684\u89c6\u89c9\u5dee\u5f02\u800c\u5177\u6709\u6311\u6218\u6027\uff0c\u4f46\u80fd\u663e\u8457\u63d0\u9ad8\u6548\u7387\u548c\u4e00\u81f4\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u6539\u8fdb\u7684\u5206\u5272\u6a21\u578b\uff0c\u7ed3\u5408\u4e86\u901a\u7528\u7684\u81ea\u76d1\u7763\u89c6\u89c9\u6a21\u578b\u548c\u57fa\u4e8e\u690d\u7269\u5206\u7c7b\u4f53\u7cfb\u7684\u5c42\u7ea7\u63a8\u7406\u3002\u8be5\u6a21\u578b\u5728\u5305\u542b\u5fb7\u56fd\u548c\u897f\u73ed\u7259\u591a\u5e74\uff082018-2020\uff09\u7684\u6570\u5b57\u548c\u79fb\u52a8\u76f8\u673a\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u5728\u5305\u62ec\u7f8e\u56fd\u3001\u5fb7\u56fd\u548c\u897f\u73ed\u7259\u76842023\u5e74\u6570\u5b57\u76f8\u673a\u6570\u636e\u548c2024\u5e74\u65e0\u4eba\u673a\u5f71\u50cf\u4e0a\u8fdb\u884c\u4e86\u6d4b\u8bd5\uff0c\u4ee5\u8bc4\u4f30\u5176\u5728\u8de8\u9886\u57df\u53d8\u5316\u4e0b\u7684\u9c81\u68d2\u6027\u3002", "result": "\u6a21\u578b\u5728\u7269\u79cd\u8bc6\u522b\u65b9\u9762\u5c06F1\u5206\u6570\u4ece0.52\u63d0\u9ad8\u52300.85\uff0cR\u65b9\u503c\u4ece0.75\u63d0\u9ad8\u52300.98\uff1b\u5728\u635f\u5bb3\u5206\u7c7b\u65b9\u9762\u5c06F1\u5206\u6570\u4ece0.28\u63d0\u9ad8\u52300.44\uff0cR\u65b9\u503c\u4ece0.71\u63d0\u9ad8\u52300.87\u3002\u5728\u65e0\u4eba\u673a\u5f71\u50cf\uff08\u8de8\u9886\u57df\u6d4b\u8bd5\uff09\u4e0b\uff0c\u8be5\u6a21\u578b\u4ecd\u4fdd\u6301\u4e86\u8f83\u5f3a\u7684\u6027\u80fd\uff0c\u7269\u79cd\u8bc6\u522bF1\u5206\u6570\u4e3a0.60\uff0cR\u65b9\u503c\u4e3a0.80\uff1b\u635f\u5bb3\u5206\u7c7bF1\u5206\u6570\u4e3a0.41\uff0cR\u65b9\u503c\u4e3a0.62\uff0c\u800c\u5148\u524d\u7684\u65b9\u6cd5\u5728\u6b64\u60c5\u51b5\u4e0b\u5931\u6548\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u7530\u95f4\u8bd5\u9a8c\u4e2d\u8868\u73b0\u51fa\u7a33\u5065\u6027\u548c\u5b9e\u9645\u5e94\u7528\u6f5c\u529b\uff0c\u5df2\u6210\u529f\u5e94\u7528\u4e8eBASF\u7684\u8868\u578b\u5206\u6790\u6d41\u7a0b\uff0c\u5b9e\u73b0\u4e86\u8de8\u5730\u57df\u7684\u5927\u89c4\u6a21\u3001\u81ea\u52a8\u5316\u4f5c\u7269\u548c\u6742\u8349\u76d1\u6d4b\u3002"}}
{"id": "2508.08013", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08013", "abs": "https://arxiv.org/abs/2508.08013", "authors": ["Mohamad Assaad", "Zeinab Nehme", "Merouane Debbah"], "title": "Communication-Efficient Zero-Order and First-Order Federated Learning Methods over Wireless Networks", "comment": null, "summary": "Federated Learning (FL) is an emerging learning framework that enables edge\ndevices to collaboratively train ML models without sharing their local data. FL\nfaces, however, a significant challenge due to the high amount of information\nthat must be exchanged between the devices and the aggregator in the training\nphase, which can exceed the limited capacity of wireless systems. In this\npaper, two communication-efficient FL methods are considered where\ncommunication overhead is reduced by communicating scalar values instead of\nlong vectors and by allowing high number of users to send information\nsimultaneously. The first approach employs a zero-order optimization technique\nwith two-point gradient estimator, while the second involves a first-order\ngradient computation strategy. The novelty lies in leveraging channel\ninformation in the learning algorithms, eliminating hence the need for\nadditional resources to acquire channel state information (CSI) and to remove\nits impact, as well as in considering asynchronous devices. We provide a\nrigorous analytical framework for the two methods, deriving convergence\nguarantees and establishing appropriate performance bounds.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u4fe1\u9053\u4fe1\u606f\u548c\u5f02\u6b65\u8bbe\u5907\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u5e76\u4f18\u5316\u5b66\u4e60\u7b97\u6cd5\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u8054\u90a6\u5b66\u4e60\uff08FL\uff09\u4e2d\u7531\u4e8e\u8bbe\u5907\u4e0e\u805a\u5408\u5668\u4e4b\u95f4\u9700\u8981\u4ea4\u6362\u5927\u91cf\u4fe1\u606f\u800c\u5bfc\u81f4\u7684\u901a\u4fe1\u5f00\u9500\u95ee\u9898\uff0c\u8be5\u7814\u7a76\u65e8\u5728\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e24\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff1a\u4e00\u79cd\u91c7\u7528\u96f6\u9636\u4f18\u5316\u6280\u672f\u548c\u4e24\u70b9\u68af\u5ea6\u4f30\u8ba1\uff0c\u53e6\u4e00\u79cd\u6d89\u53ca\u4e00\u9636\u68af\u5ea6\u8ba1\u7b97\u7b56\u7565\u3002\u8fd9\u4e24\u79cd\u65b9\u6cd5\u90fd\u5229\u7528\u4e86\u4fe1\u9053\u4fe1\u606f\uff0c\u5e76\u8003\u8651\u4e86\u5f02\u6b65\u8bbe\u5907\u3002", "result": "\u672c\u6587\u4e3a\u4e24\u79cd\u65b9\u6cd5\u63d0\u4f9b\u4e86\u4e25\u683c\u7684\u5206\u6790\u6846\u67b6\uff0c\u63a8\u5bfc\u4e86\u6536\u655b\u4fdd\u8bc1\uff0c\u5e76\u5efa\u7acb\u4e86\u9002\u5f53\u7684\u6027\u80fd\u8fb9\u754c\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e24\u79cd\u901a\u4fe1\u9ad8\u6548\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u901a\u4fe1\u6807\u91cf\u503c\u548c\u5141\u8bb8\u7528\u6237\u540c\u65f6\u53d1\u9001\u4fe1\u606f\u6765\u51cf\u5c11\u901a\u4fe1\u5f00\u9500\uff0c\u5e76\u5229\u7528\u4fe1\u9053\u4fe1\u606f\u548c\u5f02\u6b65\u8bbe\u5907\u6765\u8fdb\u4e00\u6b65\u4f18\u5316\u5b66\u4e60\u7b97\u6cd5\u3002"}}
{"id": "2508.07519", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07519", "abs": "https://arxiv.org/abs/2508.07519", "authors": ["Joonghyuk Shin", "Alchan Hwang", "Yujin Kim", "Daneul Kim", "Jaesik Park"], "title": "Exploring Multimodal Diffusion Transformers for Enhanced Prompt-based Image Editing", "comment": "ICCV 2025. Project webpage:\n  https://joonghyuk.com/exploring-mmdit-web/", "summary": "Transformer-based diffusion models have recently superseded traditional U-Net\narchitectures, with multimodal diffusion transformers (MM-DiT) emerging as the\ndominant approach in state-of-the-art models like Stable Diffusion 3 and\nFlux.1. Previous approaches have relied on unidirectional cross-attention\nmechanisms, with information flowing from text embeddings to image latents. In\ncontrast, MMDiT introduces a unified attention mechanism that concatenates\ninput projections from both modalities and performs a single full attention\noperation, allowing bidirectional information flow between text and image\nbranches. This architectural shift presents significant challenges for existing\nediting techniques. In this paper, we systematically analyze MM-DiT's attention\nmechanism by decomposing attention matrices into four distinct blocks,\nrevealing their inherent characteristics. Through these analyses, we propose a\nrobust, prompt-based image editing method for MM-DiT that supports global to\nlocal edits across various MM-DiT variants, including few-step models. We\nbelieve our findings bridge the gap between existing U-Net-based methods and\nemerging architectures, offering deeper insights into MMDiT's behavioral\npatterns.", "AI": {"tldr": "MM-DiT\u7684\u7edf\u4e00\u6ce8\u610f\u529b\u673a\u5236\u6311\u6218\u4e86\u73b0\u6709\u7f16\u8f91\u6280\u672f\u3002\u672c\u6587\u5206\u6790\u4e86MM-DiT\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u7f16\u8f91\u65b9\u6cd5\uff0c\u80fd\u591f\u652f\u6301\u5168\u5c40\u5230\u5c40\u90e8\u7f16\u8f91\u548c\u5404\u79cdMM-DiT\u53d8\u4f53\u3002", "motivation": "MM-DiT\u5f15\u5165\u7684\u7edf\u4e00\u6ce8\u610f\u529b\u673a\u5236\u548c\u53cc\u5411\u4fe1\u606f\u6d41\u5bf9\u73b0\u6709\u7684\u7f16\u8f91\u6280\u672f\u63d0\u51fa\u4e86\u91cd\u5927\u6311\u6218\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u7f16\u8f91\u65b9\u6cd5\u6765\u9002\u5e94\u8fd9\u79cd\u65b0\u67b6\u6784\u3002", "method": "\u672c\u6587\u7cfb\u7edf\u5730\u5206\u6790\u4e86MM-DiT\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u901a\u8fc7\u5c06\u6ce8\u610f\u529b\u77e9\u9635\u5206\u89e3\u4e3a\u56db\u4e2a\u4e0d\u540c\u7684\u5757\u6765\u63ed\u793a\u5176\u56fa\u6709\u7279\u6027\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u63d0\u51fa\u4e86\u4e00\u79cd\u9c81\u68d2\u7684\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\uff0c\u652f\u6301\u5168\u5c40\u5230\u5c40\u90e8\u7f16\u8f91\u3002", "result": "\u5206\u6790\u4e86MM-DiT\u7684\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u9002\u7528\u4e8eMM-DiT\u7684\u9c81\u68d2\u7684\u3001\u57fa\u4e8e\u63d0\u793a\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u57fa\u4e8e\u63d0\u793a\u7684\u56fe\u50cf\u7f16\u8f91\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u652f\u6301\u5404\u79cdMM-DiT\u53d8\u4f53\uff0c\u5305\u62ec\u5168\u5c40\u5230\u5c40\u90e8\u7f16\u8f91\u548c\u5c11\u6837\u672c\u6a21\u578b\uff0c\u5f25\u5408\u4e86\u73b0\u6709\u57fa\u4e8eU-Net\u7684\u65b9\u6cd5\u4e0e\u65b0\u5174\u67b6\u6784\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u5e76\u63d0\u4f9b\u4e86\u5bf9MMDiT\u884c\u4e3a\u6a21\u5f0f\u7684\u66f4\u6df1\u5165\u7684\u89c1\u89e3\u3002"}}
{"id": "2508.07528", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07528", "abs": "https://arxiv.org/abs/2508.07528", "authors": ["Xiaotong Ji", "Ryoma Bise", "Seiichi Uchida"], "title": "Enhancing Reliability of Medical Image Diagnosis through Top-rank Learning with Rejection Module", "comment": null, "summary": "In medical image processing, accurate diagnosis is of paramount importance.\nLeveraging machine learning techniques, particularly top-rank learning, shows\nsignificant promise by focusing on the most crucial instances. However,\nchallenges arise from noisy labels and class-ambiguous instances, which can\nseverely hinder the top-rank objective, as they may be erroneously placed among\nthe top-ranked instances. To address these, we propose a novel approach that\nenhances toprank learning by integrating a rejection module. Cooptimized with\nthe top-rank loss, this module identifies and mitigates the impact of outliers\nthat hinder training effectiveness. The rejection module functions as an\nadditional branch, assessing instances based on a rejection function that\nmeasures their deviation from the norm. Through experimental validation on a\nmedical dataset, our methodology demonstrates its efficacy in detecting and\nmitigating outliers, improving the reliability and accuracy of medical image\ndiagnoses.", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u62d2\u7edd\u6a21\u5757\u6765\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u7684\u51c6\u786e\u6027\uff0c\u8be5\u6a21\u5757\u53ef\u4ee5\u8bc6\u522b\u548c\u51cf\u8f7b\u5f02\u5e38\u503c\u5bf9\u8bad\u7ec3\u7684\u5f71\u54cd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5e26\u566a\u6807\u7b7e\u548c\u7c7b\u522b\u6a21\u7cca\u5b9e\u4f8b\u53ef\u80fd\u9519\u8bef\u5730\u6392\u5728\u524d\u5217\uff0c\u4ece\u800c\u963b\u788dtop-rank\u5b66\u4e60\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u96c6\u6210\u4e00\u4e2a\u62d2\u7edd\u6a21\u5757\u6765\u589e\u5f3atop-rank\u5b66\u4e60\uff0c\u8be5\u6a21\u5757\u4e0etop-rank\u635f\u5931\u5171\u540c\u4f18\u5316\uff0c\u4ee5\u8bc6\u522b\u548c\u51cf\u8f7b\u963b\u788d\u8bad\u7ec3\u6548\u679c\u7684\u5f02\u5e38\u503c\u3002\u62d2\u7edd\u6a21\u5757\u4f5c\u4e3a\u4e00\u4e2a\u989d\u5916\u7684\u5206\u652f\uff0c\u6839\u636e\u8861\u91cf\u5176\u4e0e\u6b63\u5e38\u503c\u504f\u5dee\u7684\u62d2\u7edd\u51fd\u6570\u6765\u8bc4\u4f30\u5b9e\u4f8b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u533b\u5b66\u6570\u636e\u96c6\u4e0a\u80fd\u591f\u6709\u6548\u68c0\u6d4b\u548c\u51cf\u8f7b\u5f02\u5e38\u503c\uff0c\u63d0\u9ad8\u4e86\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u96c6\u6210\u4e00\u4e2a\u62d2\u7edd\u6a21\u5757\u6765\u589e\u5f3atop-rank\u5b66\u4e60\uff0c\u7528\u4e8e\u8bc6\u522b\u548c\u51cf\u8f7b\u5f71\u54cd\u8bad\u7ec3\u6548\u679c\u7684\u5f02\u5e38\u503c\uff0c\u4ece\u800c\u63d0\u9ad8\u533b\u5b66\u56fe\u50cf\u8bca\u65ad\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2508.08040", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08040", "abs": "https://arxiv.org/abs/2508.08040", "authors": ["Maozhen Zhang", "Mengnan Zhao", "Bo Wang"], "title": "BadPromptFL: A Novel Backdoor Threat to Prompt-based Federated Learning in Multimodal Models", "comment": null, "summary": "Prompt-based tuning has emerged as a lightweight alternative to full\nfine-tuning in large vision-language models, enabling efficient adaptation via\nlearned contextual prompts. This paradigm has recently been extended to\nfederated learning settings (e.g., PromptFL), where clients collaboratively\ntrain prompts under data privacy constraints. However, the security\nimplications of prompt-based aggregation in federated multimodal learning\nremain largely unexplored, leaving a critical attack surface unaddressed. In\nthis paper, we introduce \\textbf{BadPromptFL}, the first backdoor attack\ntargeting prompt-based federated learning in multimodal contrastive models. In\nBadPromptFL, compromised clients jointly optimize local backdoor triggers and\nprompt embeddings, injecting poisoned prompts into the global aggregation\nprocess. These prompts are then propagated to benign clients, enabling\nuniversal backdoor activation at inference without modifying model parameters.\nLeveraging the contextual learning behavior of CLIP-style architectures,\nBadPromptFL achieves high attack success rates (e.g., \\(>90\\%\\)) with minimal\nvisibility and limited client participation. Extensive experiments across\nmultiple datasets and aggregation protocols validate the effectiveness,\nstealth, and generalizability of our attack, raising critical concerns about\nthe robustness of prompt-based federated learning in real-world deployments.", "AI": {"tldr": "\u672c\u7bc7\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBadPromptFL\u7684\u540e\u95e8\u653b\u51fb\uff0c\u4e13\u95e8\u9488\u5bf9\u8054\u90a6\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\u7684\u63d0\u793a\u5fae\u8c03\u6280\u672f\u3002\u8be5\u653b\u51fb\u901a\u8fc7\u6c61\u67d3\u63d0\u793a\uff0c\u80fd\u5728\u4e0d\u4fee\u6539\u6a21\u578b\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5b9e\u73b0\u5bf9\u6a21\u578b\u7684\u540e\u95e8\u63a7\u5236\uff0c\u4e14\u653b\u51fb\u6210\u529f\u7387\u9ad8\u3001\u9690\u853d\u6027\u5f3a\uff0c\u5bf9\u73b0\u6709\u6280\u672f\u7684\u5b89\u5168\u6027\u63d0\u51fa\u4e86\u4e25\u91cd\u8b66\u544a\u3002", "motivation": "\u63d0\u793a\u5fae\u8c03\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u6a21\u578b\u9002\u914d\u65b9\u5f0f\uff0c\u5df2\u6269\u5c55\u5230\u8054\u90a6\u5b66\u4e60\u8bbe\u7f6e\u4e2d\u3002\u7136\u800c\uff0c\u5728\u8054\u90a6\u591a\u6a21\u6001\u5b66\u4e60\u4e2d\uff0c\u57fa\u4e8e\u63d0\u793a\u805a\u5408\u7684\u5b89\u5168\u542b\u4e49\u5c1a\u672a\u5f97\u5230\u5145\u5206\u7814\u7a76\uff0c\u5b58\u5728\u5173\u952e\u7684\u5b89\u5168\u9690\u60a3\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBadPromptFL\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u4e86CLIP\u98ce\u683c\u67b6\u6784\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u884c\u4e3a\u3002\u53d7\u635f\u5ba2\u6237\u7aef\u534f\u540c\u4f18\u5316\u672c\u5730\u540e\u95e8\u89e6\u53d1\u5668\u548c\u63d0\u793a\u5d4c\u5165\uff0c\u5c06\u6c61\u67d3\u7684\u63d0\u793a\u6ce8\u5165\u5168\u5c40\u805a\u5408\u8fc7\u7a0b\u3002", "result": "BadPromptFL\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u805a\u5408\u534f\u8bae\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u5176\u9ad8\u653b\u51fb\u6210\u529f\u7387\uff08\u4f8b\u5982\uff0c>90%\uff09\uff0c\u540c\u65f6\u5177\u6709\u8f83\u4f4e\u7684\u53ef\u89c1\u6027\u548c\u6709\u9650\u7684\u5ba2\u6237\u7aef\u53c2\u4e0e\u3002", "conclusion": "\u8be5\u7814\u7a76\u63ed\u793a\u4e86\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u5728\u591a\u6a21\u6001\u5bf9\u6bd4\u5b66\u4e60\u4e2d\u7684\u5b89\u5168\u6f0f\u6d1e\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBadPromptFL\u7684\u540e\u95e8\u653b\u51fb\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5728\u8054\u90a6\u5b66\u4e60\u8fc7\u7a0b\u4e2d\u6ce8\u5165\u88ab\u6c61\u67d3\u7684\u63d0\u793a\uff0c\u53ef\u4ee5\u5728\u63a8\u7406\u65f6\u5b9e\u73b0\u901a\u7528\u7684\u540e\u95e8\u6fc0\u6d3b\uff0c\u4e14\u65e0\u9700\u4fee\u6539\u6a21\u578b\u53c2\u6570\u3002\u5b9e\u9a8c\u8bc1\u660e\u4e86\u8be5\u653b\u51fb\u7684\u6709\u6548\u6027\u3001\u9690\u853d\u6027\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5bf9\u57fa\u4e8e\u63d0\u793a\u7684\u8054\u90a6\u5b66\u4e60\u7684\u9c81\u68d2\u6027\u63d0\u51fa\u4e86\u4e25\u5cfb\u6311\u6218\u3002"}}
{"id": "2508.07537", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07537", "abs": "https://arxiv.org/abs/2508.07537", "authors": ["Xiaoming Li", "Wangmeng Zuo", "Chen Change Loy"], "title": "Enhanced Generative Structure Prior for Chinese Text Image Super-resolution", "comment": "TPAMI", "summary": "Faithful text image super-resolution (SR) is challenging because each\ncharacter has a unique structure and usually exhibits diverse font styles and\nlayouts. While existing methods primarily focus on English text, less attention\nhas been paid to more complex scripts like Chinese. In this paper, we introduce\na high-quality text image SR framework designed to restore the precise strokes\nof low-resolution (LR) Chinese characters. Unlike methods that rely on\ncharacter recognition priors to regularize the SR task, we propose a novel\nstructure prior that offers structure-level guidance to enhance visual quality.\nOur framework incorporates this structure prior within a StyleGAN model,\nleveraging its generative capabilities for restoration. To maintain the\nintegrity of character structures while accommodating various font styles and\nlayouts, we implement a codebook-based mechanism that restricts the generative\nspace of StyleGAN. Each code in the codebook represents the structure of a\nspecific character, while the vector $w$ in StyleGAN controls the character's\nstyle, including typeface, orientation, and location. Through the collaborative\ninteraction between the codebook and style, we generate a high-resolution\nstructure prior that aligns with LR characters both spatially and structurally.\nExperiments demonstrate that this structure prior provides robust,\ncharacter-specific guidance, enabling the accurate restoration of clear strokes\nin degraded characters, even for real-world LR Chinese text with irregular\nlayouts. Our code and pre-trained models will be available at\nhttps://github.com/csxmli2016/MARCONetPlusPlus", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7528\u4e8e\u4e2d\u6587\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u5f15\u5165\u65b0\u9896\u7684\u7ed3\u6784\u5148\u9a8c\u548c\u57fa\u4e8e\u7801\u672c\u7684StyleGAN\u6a21\u578b\uff0c\u6709\u6548\u6062\u590d\u4e86\u4f4e\u5206\u8fa8\u7387\u4e2d\u6587\u6587\u672c\u7684\u7b14\u753b\u548c\u7ed3\u6784\u3002", "motivation": "\u73b0\u6709\u6587\u672c\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u82f1\u6587\uff0c\u5bf9\u4e2d\u6587\u7b49\u590d\u6742\u811a\u672c\u5173\u6ce8\u8f83\u5c11\u3002\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u4e2a\u80fd\u6062\u590d\u4e2d\u6587\u6c49\u5b57\u7cbe\u786e\u7b14\u753b\u7684\u9ad8\u8d28\u91cf\u6587\u672c\u8d85\u5206\u8fa8\u7387\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230StyleGAN\u6a21\u578b\u4e2d\uff0c\u5229\u7528\u7801\u672c\u673a\u5236\u6765\u63a7\u5236\u5b57\u7b26\u7ed3\u6784\u548c\u98ce\u683c\uff0c\u4ee5\u5b9e\u73b0\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u7ed3\u6784\u5148\u9a8c\u80fd\u63d0\u4f9b\u9c81\u68d2\u7684\u3001\u9488\u5bf9\u7279\u5b9a\u5b57\u7b26\u7684\u6307\u5bfc\uff0c\u5373\u4f7f\u662f\u5bf9\u4e8e\u5177\u6709\u4e0d\u89c4\u5219\u5e03\u5c40\u7684\u771f\u5b9e\u4e16\u754c\u4f4e\u5206\u8fa8\u7387\u4e2d\u6587\u6587\u672c\uff0c\u4e5f\u80fd\u51c6\u786e\u6062\u590d\u6e05\u6670\u7684\u7b14\u753b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u7ed3\u6784\u5148\u9a8c\uff0c\u5e76\u5c06\u5176\u96c6\u6210\u5230StyleGAN\u6a21\u578b\u4e2d\uff0c\u901a\u8fc7\u57fa\u4e8e\u7801\u672c\u7684\u673a\u5236\u6765\u7ea6\u675f\u751f\u6210\u7a7a\u95f4\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u8d28\u91cf\u7684\u4e2d\u6587\u6587\u672c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\u3002"}}
{"id": "2508.08052", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08052", "abs": "https://arxiv.org/abs/2508.08052", "authors": ["Supriyo Chakraborty", "Krishnan Raghavan"], "title": "On Understanding of the Dynamics of Model Capacity in Continual Learning", "comment": null, "summary": "The stability-plasticity dilemma, closely related to a neural network's (NN)\ncapacity-its ability to represent tasks-is a fundamental challenge in continual\nlearning (CL). Within this context, we introduce CL's effective model capacity\n(CLEMC) that characterizes the dynamic behavior of the stability-plasticity\nbalance point. We develop a difference equation to model the evolution of the\ninterplay between the NN, task data, and optimization procedure. We then\nleverage CLEMC to demonstrate that the effective capacity-and, by extension,\nthe stability-plasticity balance point is inherently non-stationary. We show\nthat regardless of the NN architecture or optimization method, a NN's ability\nto represent new tasks diminishes when incoming task distributions differ from\nprevious ones. We conduct extensive experiments to support our theoretical\nfindings, spanning a range of architectures-from small feedforward network and\nconvolutional networks to medium-sized graph neural networks and\ntransformer-based large language models with millions of parameters.", "AI": {"tldr": "\u6301\u7eed\u5b66\u4e60\u4e2d\uff0c\u6a21\u578b\u5904\u7406\u65b0\u4efb\u52a1\u7684\u80fd\u529b\u4f1a\u56e0\u65b0\u65e7\u4efb\u52a1\u5206\u5e03\u4e0d\u5339\u914d\u800c\u4e0b\u964d\uff0c\u6b64\u73b0\u8c61\u53ef\u7528\u201c\u6709\u6548\u6a21\u578b\u5bb9\u91cf\u201d\uff08CLEMC\uff09\u91cf\u5316\uff0c\u4e14\u8be5\u5bb9\u91cf\u975e\u56fa\u5b9a\u4e0d\u53d8\u3002", "motivation": "\u7814\u7a76\u6838\u5fc3\u5728\u4e8e\u89e3\u51b3\u6301\u7eed\u5b66\u4e60\u4e2d\u7684\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u56f0\u5883\uff0c\u5e76\u63d0\u51fa\u201c\u6709\u6548\u6a21\u578b\u5bb9\u91cf\u201d\uff08CLEMC\uff09\u6765\u91cf\u5316\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u5e73\u8861\u70b9\u52a8\u6001\u884c\u4e3a\u3002", "method": "\u5229\u7528\u534f\u65b9\u5dee\u5206\u6790\u548c\u683c\u5170\u6770\u56e0\u679c\u68c0\u9a8c\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u4e0d\u540c\u4efb\u52a1\u5206\u5e03\u5bfc\u81f4\u7684\u80fd\u529b\u4e0b\u964d\u6548\u5e94\u3002", "result": "\u63d0\u51fa\u4e86\u201c\u6709\u6548\u6a21\u578b\u5bb9\u91cf\u201d\uff08CLEMC\uff09\u7684\u6982\u5ff5\uff0c\u5e76\u63a8\u5bfc\u51fa\u5dee\u5206\u65b9\u7a0b\u6765\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u3001\u4efb\u52a1\u6570\u636e\u548c\u4f18\u5316\u8fc7\u7a0b\u4e4b\u95f4\u7684\u76f8\u4e92\u4f5c\u7528\u6f14\u53d8\u3002\u5b9e\u9a8c\u7ed3\u679c\u652f\u6301\u4e86\u7406\u8bba\u53d1\u73b0\uff0c\u8868\u660e\u6709\u6548\u5bb9\u91cf\u548c\u7a33\u5b9a\u6027-\u53ef\u5851\u6027\u5e73\u8861\u70b9\u672c\u8d28\u4e0a\u662f\u975e\u56fa\u5b9a\u7684\uff0c\u5e76\u4e14\u4efb\u52a1\u5206\u5e03\u7684\u53d8\u5316\u4f1a\u5f71\u54cd\u6a21\u578b\u7684\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u8868\u660e\uff0c\u65e0\u8bba\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u6216\u4f18\u5316\u65b9\u6cd5\u5982\u4f55\uff0c\u5f53\u8f93\u5165\u4efb\u52a1\u5206\u5e03\u4e0e\u5148\u524d\u4efb\u52a1\u4e0d\u540c\u65f6\uff0c\u795e\u7ecf\u7f51\u7edc\u8868\u793a\u65b0\u4efb\u52a1\u7684\u80fd\u529b\u4f1a\u4e0b\u964d\u3002"}}
{"id": "2508.07538", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07538", "abs": "https://arxiv.org/abs/2508.07538", "authors": ["Hongzhu Jiang", "Sihan Xie", "Zhiyu Wan"], "title": "A DICOM Image De-identification Algorithm in the MIDI-B Challenge", "comment": "8 pages, 5 figures", "summary": "Image de-identification is essential for the public sharing of medical\nimages, particularly in the widely used Digital Imaging and Communications in\nMedicine (DICOM) format as required by various regulations and standards,\nincluding Health Insurance Portability and Accountability Act (HIPAA) privacy\nrules, the DICOM PS3.15 standard, and best practices recommended by the Cancer\nImaging Archive (TCIA). The Medical Image De-Identification Benchmark (MIDI-B)\nChallenge at the 27th International Conference on Medical Image Computing and\nComputer Assisted Intervention (MICCAI 2024) was organized to evaluate\nrule-based DICOM image de-identification algorithms with a large dataset of\nclinical DICOM images. In this report, we explore the critical challenges of\nde-identifying DICOM images, emphasize the importance of removing personally\nidentifiable information (PII) to protect patient privacy while ensuring the\ncontinued utility of medical data for research, diagnostics, and treatment, and\nprovide a comprehensive overview of the standards and regulations that govern\nthis process. Additionally, we detail the de-identification methods we applied\n- such as pixel masking, date shifting, date hashing, text recognition, text\nreplacement, and text removal - to process datasets during the test phase in\nstrict compliance with these standards. According to the final leaderboard of\nthe MIDI-B challenge, the latest version of our solution algorithm correctly\nexecuted 99.92% of the required actions and ranked 2nd out of 10 teams that\ncompleted the challenge (from a total of 22 registered teams). Finally, we\nconducted a thorough analysis of the resulting statistics and discussed the\nlimitations of current approaches and potential avenues for future improvement.", "AI": {"tldr": "\u8be5\u7814\u7a76\u9488\u5bf9\u533b\u7597\u5f71\u50cf\uff08\u7279\u522b\u662f DICOM \u683c\u5f0f\uff09\u7684\u53bb\u6807\u8bc6\u5316\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u65b9\u6cd5\uff0c\u5e76\u5728 MICCAI 2024 \u7684 MIDI-B \u6311\u6218\u8d5b\u4e2d\u53d6\u5f97\u4e86\u4f18\u5f02\u6210\u7ee9\uff0c\u51c6\u786e\u7387\u8fbe 99.92%\uff0c\u6392\u540d\u7b2c\u4e8c\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u8be5\u9886\u57df\u7684\u6311\u6218\u4e0e\u672a\u6765\u65b9\u5411\u3002", "motivation": "\u4e3a\u4e86\u5728\u4e0d\u635f\u5bb3\u6570\u636e\u6548\u7528\u7684\u524d\u63d0\u4e0b\uff0c\u9075\u5faa HIPAA\u3001DICOM PS3.15 \u548c TCIA \u7b49\u6cd5\u89c4\u548c\u6807\u51c6\uff0c\u4fdd\u62a4\u60a3\u8005\u9690\u79c1\u3002", "method": "\u901a\u8fc7\u50cf\u7d20\u8499\u7248\u3001\u65e5\u671f\u79fb\u4f4d\u3001\u65e5\u671f\u54c8\u5e0c\u3001\u6587\u672c\u8bc6\u522b\u3001\u6587\u672c\u66ff\u6362\u548c\u6587\u672c\u79fb\u9664\u7b49\u65b9\u6cd5\u5bf9 DICOM \u56fe\u50cf\u8fdb\u884c\u53bb\u6807\u8bc6\u5316\u5904\u7406\uff0c\u4ee5\u7b26\u5408\u76f8\u5173\u6807\u51c6\u3002", "result": "\u6587\u7ae0\u8be6\u8ff0\u4e86\u5904\u7406\u6570\u636e\u96c6\u7684\u53bb\u6807\u8bc6\u5316\u65b9\u6cd5\uff0c\u5e76\u5c55\u793a\u4e86\u7b97\u6cd5\u5728 MIDI-B \u6311\u6218\u8d5b\u4e2d\u7684\u4f18\u5f02\u8868\u73b0\uff0c\u51c6\u786e\u7387\u8fbe\u5230 99.92%\uff0c\u6392\u540d\u7b2c\u4e8c\u3002", "conclusion": "MIDI-B \u6311\u6218\u8d5b\u7684\u6700\u65b0\u89e3\u51b3\u65b9\u6848\u7b97\u6cd5\u51c6\u786e\u6267\u884c\u4e86 99.92% \u7684\u5fc5\u8981\u64cd\u4f5c\uff0c\u5728 10 \u4e2a\u5b8c\u6210\u6311\u6218\u7684\u56e2\u961f\u4e2d\u6392\u540d\u7b2c\u4e8c\u3002\u6587\u7ae0\u8fd8\u5206\u6790\u4e86\u7ed3\u679c\u7edf\u8ba1\u6570\u636e\uff0c\u5e76\u8ba8\u8bba\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u548c\u672a\u6765\u6539\u8fdb\u7684\u53ef\u80fd\u9014\u5f84\u3002"}}
{"id": "2508.07539", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07539", "abs": "https://arxiv.org/abs/2508.07539", "authors": ["Yuki Shigeyasu", "Shota Harada", "Akihiko Yoshizawa", "Kazuhiro Terada", "Naoki Nakazima", "Mariyo Kurata", "Hiroyuki Abe", "Tetsuo Ushiku", "Ryoma Bise"], "title": "Domain Generalization of Pathological Image Segmentation by Patch-Level and WSI-Level Contrastive Learning", "comment": null, "summary": "In this paper, we address domain shifts in pathological images by focusing on\nshifts within whole slide images~(WSIs), such as patient characteristics and\ntissue thickness, rather than shifts between hospitals. Traditional approaches\nrely on multi-hospital data, but data collection challenges often make this\nimpractical. Therefore, the proposed domain generalization method captures and\nleverages intra-hospital domain shifts by clustering WSI-level features from\nnon-tumor regions and treating these clusters as domains. To mitigate domain\nshift, we apply contrastive learning to reduce feature gaps between WSI pairs\nfrom different clusters. The proposed method introduces a two-stage contrastive\nlearning approach WSI-level and patch-level contrastive learning to minimize\nthese gaps effectively.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u57df\u79fb\u4f4d\u95ee\u9898\uff0c\u7279\u522b\u5173\u6ce8\u540c\u4e00\u533b\u9662\u5185\u7684\u53d8\u5316\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u8bc6\u522b\u56fe\u50cf\u4e2d\u7684\u4e0d\u540c\u201c\u57df\u201d\uff08\u57fa\u4e8e\u975e\u80bf\u7624\u533a\u57df\u7684\u7279\u5f81\u805a\u7c7b\uff09\uff0c\u5e76\u4f7f\u7528\u5bf9\u6bd4\u5b66\u4e60\u6280\u672f\u6765\u7f29\u5c0f\u8fd9\u4e9b\u57df\u4e4b\u95f4\u7684\u7279\u5f81\u5dee\u5f02\u3002\u8fd9\u79cd\u65b9\u6cd5\u4e0d\u9700\u8981\u8de8\u533b\u9662\u6570\u636e\uff0c\u6709\u671b\u63d0\u9ad8\u6a21\u578b\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u9c81\u68d2\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u591a\u533b\u9662\u6570\u636e\uff0c\u4f46\u6570\u636e\u6536\u96c6\u9762\u4e34\u6311\u6218\u4e14\u4e0d\u5207\u5b9e\u9645\u3002\u56e0\u6b64\uff0c\u672c\u6587\u65e8\u5728\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u57df\u79fb\u4f4d\u95ee\u9898\uff0c\u7279\u522b\u662f\u9662\u5185\uff08\u540c\u4e00\u533b\u9662\u5185\uff09\u7684\u57df\u79fb\u4f4d\uff0c\u5982\u60a3\u8005\u7279\u5f81\u548c\u7ec4\u7ec7\u539a\u5ea6\u7684\u53d8\u5316\uff0c\u800c\u4e0d\u662f\u8de8\u533b\u9662\u7684\u57df\u79fb\u4f4d\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u805a\u7c7b\u548c\u4e24\u9636\u6bb5\u5bf9\u6bd4\u5b66\u4e60\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\u3002\u9996\u5148\uff0c\u901a\u8fc7\u805a\u7c7bWSI\uff08\u5168\u5207\u7247\u56fe\u50cf\uff09\u7684\u975e\u80bf\u7624\u533a\u57df\u7279\u5f81\u6765\u8bc6\u522b\u548c\u6a21\u62df\u9662\u5185\u57df\u79fb\u4f4d\uff0c\u5c06\u4e0d\u540c\u7684\u805a\u7c7b\u89c6\u4e3a\u4e0d\u540c\u7684\u57df\u3002\u7136\u540e\uff0c\u5e94\u7528\u5bf9\u6bd4\u5b66\u4e60\u6765\u51cf\u5c0f\u8fd9\u4e9b\u4e0d\u540c\u57df\u4e4b\u95f4\u7684\u7279\u5f81\u5dee\u8ddd\uff0c\u5177\u4f53\u91c7\u7528WSI\u7ea7\u522b\u548cPatch\u7ea7\u522b\u7684\u5bf9\u6bd4\u5b66\u4e60\u7b56\u7565\u3002", "result": "\u901a\u8fc7\u805a\u7c7bWSI\u7684\u975e\u80bf\u7624\u533a\u57df\u7279\u5f81\u5e76\u5c06\u805a\u7c7b\u89c6\u4e3a\u57df\uff0c\u5e76\u7ed3\u5408WSI\u7ea7\u522b\u548cPatch\u7ea7\u522b\u7684\u5bf9\u6bd4\u5b66\u4e60\uff0c\u6709\u6548\u51cf\u5c0f\u4e86\u4e0d\u540c\u57df\u4e4b\u95f4\u7684\u7279\u5f81\u5dee\u8ddd\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u6a21\u578b\u5728\u9762\u4e34\u9662\u5185\u57df\u79fb\u4f4d\u65f6\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u57df\u6cdb\u5316\u65b9\u6cd5\uff0c\u901a\u8fc7\u805a\u7c7bWSIs\u4e2d\u7684\u975e\u80bf\u7624\u533a\u57df\u7279\u5f81\u6765\u6355\u6349\u548c\u5229\u7528\u9662\u5185\u57df\u79fb\u4f4d\uff0c\u5e76\u91c7\u7528\u4e24\u9636\u6bb5\u5bf9\u6bd4\u5b66\u4e60\u6765\u51cf\u5c0f\u4e0d\u540c\u7c07\u4e4b\u95f4WSIs\u7684\u7279\u5f81\u5dee\u8ddd\uff0c\u4ece\u800c\u6709\u6548\u89e3\u51b3\u75c5\u7406\u56fe\u50cf\u4e2d\u7684\u57df\u79fb\u4f4d\u95ee\u9898\u3002"}}
{"id": "2508.08071", "categories": ["cs.LG", "cs.AI", "J.1; I.2.4; H.2.8"], "pdf": "https://arxiv.org/pdf/2508.08071", "abs": "https://arxiv.org/abs/2508.08071", "authors": ["Yunqing Li", "Zixiang Tang", "Jiaying Zhuang", "Zhenyu Yang", "Farhad Ameri", "Jianbang Zhang"], "title": "C-MAG: Cascade Multimodal Attributed Graphs for Supply Chain Link Prediction", "comment": "Accepted as a poster presentation at the KDD 2025 Workshop on AI for\n  Supply Chain (AI4SupplyChain)", "summary": "Connecting an ever-expanding catalogue of products with suitable\nmanufacturers and suppliers is critical for resilient, efficient global supply\nchains, yet traditional methods struggle to capture complex capabilities,\ncertifications, geographic constraints, and rich multimodal data of real-world\nmanufacturer profiles. To address these gaps, we introduce PMGraph, a public\nbenchmark of bipartite and heterogeneous multimodal supply-chain graphs linking\n8,888 manufacturers, over 70k products, more than 110k manufacturer-product\nedges, and over 29k product images. Building on this benchmark, we propose the\nCascade Multimodal Attributed Graph C-MAG, a two-stage architecture that first\naligns and aggregates textual and visual attributes into intermediate group\nembeddings, then propagates them through a manufacturer-product hetero-graph\nvia multiscale message passing to enhance link prediction accuracy. C-MAG also\nprovides practical guidelines for modality-aware fusion, preserving predictive\nperformance in noisy, real-world settings.", "AI": {"tldr": "PMGraph\u662f\u4e00\u4e2a\u5305\u542b\u5236\u9020\u5546\u3001\u4ea7\u54c1\u548c\u4ea7\u54c1\u56fe\u50cf\u7684\u5927\u578b\u6570\u636e\u96c6\u3002C-MAG\u662f\u4e00\u4e2a\u4e24\u9636\u6bb5\u6a21\u578b\uff0c\u5229\u7528\u6587\u672c\u548c\u89c6\u89c9\u4fe1\u606f\u6765\u63d0\u9ad8\u4f9b\u5e94\u94fe\u4e2d\u7684\u94fe\u63a5\u9884\u6d4b\u51c6\u786e\u6027\u3002", "motivation": "\u4f20\u7edf\u7684\u8fde\u63a5\u4ea7\u54c1\u4e0e\u5236\u9020\u5546\u548c\u4f9b\u5e94\u5546\u7684\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5236\u9020\u5546\u6863\u6848\u6240\u56fa\u6709\u7684\u590d\u6742\u529f\u80fd\u3001\u8ba4\u8bc1\u3001\u5730\u7406\u9650\u5236\u548c\u4e30\u5bcc\u591a\u6a21\u6001\u6570\u636e\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u5dee\u8ddd\u3002", "method": "PMGraph\u662f\u4e00\u4e2a\u5305\u542b8,888\u4e2a\u5236\u9020\u5546\uff0c70,000\u591a\u4e2a\u4ea7\u54c1\uff0c110,000\u591a\u4e2a\u5236\u9020\u5546-\u4ea7\u54c1\u8fb9\u7f18\u548c29,000\u591a\u4e2a\u4ea7\u54c1\u56fe\u50cf\u7684\u516c\u5171\u6570\u636e\u96c6\u3002C-MAG\u662f\u4e00\u79cd\u4e24\u9636\u6bb5\u67b6\u6784\uff0c\u9996\u5148\u5c06\u6587\u672c\u548c\u89c6\u89c9\u5c5e\u6027\u5bf9\u9f50\u5e76\u805a\u5408\u4e3a\u4e2d\u95f4\u7ec4\u5d4c\u5165\uff0c\u7136\u540e\u901a\u8fc7\u591a\u5c3a\u5ea6\u6d88\u606f\u4f20\u9012\u5728\u5236\u9020\u5546-\u4ea7\u54c1\u5f02\u6784\u56fe\u4e0a\u4f20\u64ad\u8fd9\u4e9b\u5d4c\u5165\u3002", "result": "PMGraph\u57fa\u51c6\u6d4b\u8bd5\u5305\u542b\u4e86\u5927\u91cf\u7684\u5236\u9020\u5546\u3001\u4ea7\u54c1\u548c\u5236\u9020\u5546-\u4ea7\u54c1\u5173\u7cfb\uff0c\u4ee5\u53ca\u4ea7\u54c1\u56fe\u50cf\u3002C-MAG\u6a21\u578b\u63d0\u9ad8\u4e86\u94fe\u63a5\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "C-MAG\u901a\u8fc7\u5728\u6587\u672c\u548c\u89c6\u89c9\u5c5e\u6027\u4e0a\u8fdb\u884c\u591a\u5c3a\u5ea6\u6d88\u606f\u4f20\u9012\uff0c\u4ee5\u589e\u5f3a\u94fe\u63a5\u9884\u6d4b\u7cbe\u5ea6\u3002"}}
{"id": "2508.07540", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07540", "abs": "https://arxiv.org/abs/2508.07540", "authors": ["Junuk Cha", "Jihyeon Kim"], "title": "CoT-Pose: Chain-of-Thought Reasoning for 3D Pose Generation from Abstract Prompts", "comment": "ICCVW'25", "summary": "Recent advances in multi-modal large language models (MLLMs) and\nchain-of-thought (CoT) reasoning have led to significant progress in image and\ntext generation tasks. However, the field of 3D human pose generation still\nfaces critical limitations. Most existing text-to-pose models rely heavily on\ndetailed (low-level) prompts that explicitly describe joint configurations. In\ncontrast, humans tend to communicate actions and intentions using abstract\n(high-level) language. This mismatch results in a practical challenge for\ndeploying pose generation systems in real-world scenarios. To bridge this gap,\nwe introduce a novel framework that incorporates CoT reasoning into the pose\ngeneration process, enabling the interpretation of abstract prompts into\naccurate 3D human poses. We further propose a data synthesis pipeline that\nautomatically generates triplets of abstract prompts, detailed prompts, and\ncorresponding 3D poses for training process. Experimental results demonstrate\nthat our reasoning-enhanced model, CoT-Pose, can effectively generate plausible\nand semantically aligned poses from abstract textual inputs. This work\nhighlights the importance of high-level understanding in pose generation and\nopens new directions for reasoning-enhanced approach for human pose generation.", "AI": {"tldr": "This paper presents CoT-Pose, a new method that uses Chain-of-Thought reasoning to generate 3D human poses from abstract text descriptions, overcoming limitations of previous models that required detailed prompts. It includes a data synthesis pipeline for training and shows promising results in aligning poses with high-level language.", "motivation": "Existing text-to-pose models struggle with abstract, high-level language prompts commonly used by humans to describe actions and intentions, relying instead on detailed low-level prompts that specify joint configurations. This mismatch limits the practical deployment of pose generation systems.", "method": "The paper introduces a novel framework that incorporates Chain-of-Thought (CoT) reasoning into the 3D human pose generation process to interpret abstract prompts into accurate 3D poses. It also proposes a data synthesis pipeline to generate triplets of abstract prompts, detailed prompts, and corresponding 3D poses for training.", "result": "Experimental results demonstrate that the proposed CoT-Pose model can effectively generate plausible and semantically aligned 3D human poses from abstract textual inputs.", "conclusion": "CoT-Pose, a novel framework incorporating CoT reasoning, effectively generates plausible and semantically aligned 3D human poses from abstract textual inputs, highlighting the importance of high-level understanding in pose generation and opening new directions for reasoning-enhanced approaches."}}
{"id": "2508.07543", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07543", "abs": "https://arxiv.org/abs/2508.07543", "authors": ["Chidaksh Ravuru"], "title": "Commentary Generation for Soccer Highlights", "comment": null, "summary": "Automated soccer commentary generation has evolved from template-based\nsystems to advanced neural architectures, aiming to produce real-time\ndescriptions of sports events. While frameworks like SoccerNet-Caption laid\nfoundational work, their inability to achieve fine-grained alignment between\nvideo content and commentary remains a significant challenge. Recent efforts\nsuch as MatchTime, with its MatchVoice model, address this issue through coarse\nand fine-grained alignment techniques, achieving improved temporal\nsynchronization. In this paper, we extend MatchVoice to commentary generation\nfor soccer highlights using the GOAL dataset, which emphasizes short clips over\nentire games. We conduct extensive experiments to reproduce the original\nMatchTime results and evaluate our setup, highlighting the impact of different\ntraining configurations and hardware limitations. Furthermore, we explore the\neffect of varying window sizes on zero-shot performance. While MatchVoice\nexhibits promising generalization capabilities, our findings suggest the need\nfor integrating techniques from broader video-language domains to further\nenhance performance. Our code is available at\nhttps://github.com/chidaksh/SoccerCommentary.", "AI": {"tldr": "\u672c\u6587\u57fa\u4e8eMatchTime\u7684MatchVoice\u6a21\u578b\uff0c\u6269\u5c55\u4e86\u5176\u5728\u8db3\u7403\u96c6\u9526\u8bc4\u8bba\u751f\u6210\u4e0a\u7684\u5e94\u7528\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u8bc4\u4f30\u4e86\u4e0d\u540c\u56e0\u7d20\u5bf9\u6a21\u578b\u6027\u80fd\u7684\u5f71\u54cd\uff0c\u6307\u51fa\u4e86\u672a\u6765\u6539\u8fdb\u65b9\u5411\u3002", "motivation": "\u65e8\u5728\u89e3\u51b3\u73b0\u6709\u81ea\u52a8\u89e3\u8bf4\u7cfb\u7edf\u5728\u89c6\u9891\u5185\u5bb9\u548c\u89e3\u8bf4\u4e4b\u95f4\u7f3a\u4e4f\u7ec6\u7c92\u5ea6\u5bf9\u9f50\u7684\u95ee\u9898\uff0c\u5e76\u6269\u5c55MatchVoice\u6a21\u578b\u4ee5\u9002\u5e94\u8db3\u7403\u96c6\u9526\u7684\u89e3\u8bf4\u751f\u6210\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMatchVoice\u6a21\u578b\u7684\u8db3\u7403\u96c6\u9526\u8bc4\u8bba\u751f\u6210\u65b9\u6cd5\uff0c\u5e76\u4f7f\u7528GOAL\u6570\u636e\u96c6\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u540c\u65f6\u63a2\u8ba8\u4e86\u4e0d\u540c\u8bad\u7ec3\u914d\u7f6e\u3001\u786c\u4ef6\u9650\u5236\u4ee5\u53ca\u7a97\u53e3\u5927\u5c0f\u5bf9\u96f6\u6837\u672c\u6027\u80fd\u7684\u5f71\u54cd\u3002", "result": "\u6210\u529f\u590d\u73b0\u4e86MatchTime\u7684\u539f\u59cb\u7ed3\u679c\uff0c\u5e76\u8bc4\u4f30\u4e86\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u3002\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cMatchVoice\u5177\u6709\u4e00\u5b9a\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\u3002", "conclusion": "MatchVoice\u5728\u8db3\u7403\u96c6\u9526\u8bc4\u8bba\u751f\u6210\u65b9\u9762\u5c55\u793a\u4e86\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u4e3a\u4e86\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6027\u80fd\uff0c\u6709\u5fc5\u8981\u6574\u5408\u66f4\u5e7f\u6cdb\u7684\u89c6\u9891\u8bed\u8a00\u9886\u57df\u7684\u6280\u672f\u3002"}}
{"id": "2508.07548", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07548", "abs": "https://arxiv.org/abs/2508.07548", "authors": ["Takehiro Yamane", "Itaru Tsuge", "Susumu Saito", "Ryoma Bise"], "title": "Adaptive Pseudo Label Selection for Individual Unlabeled Data by Positive and Unlabeled Learning", "comment": null, "summary": "This paper proposes a novel pseudo-labeling method for medical image\nsegmentation that can perform learning on ``individual images'' to select\neffective pseudo-labels. We introduce Positive and Unlabeled Learning (PU\nlearning), which uses only positive and unlabeled data for binary\nclassification problems, to obtain the appropriate metric for discriminating\nforeground and background regions on each unlabeled image. Our PU learning\nmakes us easy to select pseudo-labels for various background regions. The\nexperimental results show the effectiveness of our method.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684PU\u5b66\u4e60\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u7528\u4e8e\u533b\u5b66\u56fe\u50cf\u5206\u5272\uff0c\u80fd\u591f\u6709\u6548\u9009\u62e9\u4f2a\u6807\u7b7e\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u4f2a\u6807\u7b7e\u9009\u62e9\u95ee\u9898\uff0c\u7279\u522b\u662f\u5904\u7406\u5404\u79cd\u80cc\u666f\u533a\u57df\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u4f2a\u6807\u7b7e\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86PU\u5b66\u4e60\uff0c\u4ee5\u5728\u6bcf\u4e2a\u672a\u6807\u8bb0\u56fe\u50cf\u4e0a\u9009\u62e9\u6709\u6548\u7684\u4f2a\u6807\u7b7e\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u5404\u79cd\u80cc\u666f\u533a\u57df\u7684\u4f2a\u6807\u7b7e\u9009\u62e9\u65b9\u9762\u5177\u6709\u4f18\u52bf\u3002"}}
{"id": "2508.08087", "categories": ["cs.LG", "physics.chem-ph"], "pdf": "https://arxiv.org/pdf/2508.08087", "abs": "https://arxiv.org/abs/2508.08087", "authors": ["Amir Ali Panahi", "Daniel Luder", "Billy Wu", "Gregory Offer", "Dirk Uwe Sauer", "Weihan Li"], "title": "Fast and Generalizable parameter-embedded Neural Operators for Lithium-Ion Battery Simulation", "comment": "31 pages, 6 figures", "summary": "Reliable digital twins of lithium-ion batteries must achieve high physical\nfidelity with sub-millisecond speed. In this work, we benchmark three\noperator-learning surrogates for the Single Particle Model (SPM): Deep Operator\nNetworks (DeepONets), Fourier Neural Operators (FNOs) and a newly proposed\nparameter-embedded Fourier Neural Operator (PE-FNO), which conditions each\nspectral layer on particle radius and solid-phase diffusivity. Models are\ntrained on simulated trajectories spanning four current families (constant,\ntriangular, pulse-train, and Gaussian-random-field) and a full range of\nState-of-Charge (SOC) (0 % to 100 %). DeepONet accurately replicates\nconstant-current behaviour but struggles with more dynamic loads. The basic FNO\nmaintains mesh invariance and keeps concentration errors below 1 %, with\nvoltage mean-absolute errors under 1.7 mV across all load types. Introducing\nparameter embedding marginally increases error, but enables generalisation to\nvarying radii and diffusivities. PE-FNO executes approximately 200 times faster\nthan a 16-thread SPM solver. Consequently, PE-FNO's capabilities in inverse\ntasks are explored in a parameter estimation task with Bayesian optimisation,\nrecovering anode and cathode diffusivities with 1.14 % and 8.4 % mean absolute\npercentage error, respectively, and 0.5918 percentage points higher error in\ncomparison with classical methods. These results pave the way for neural\noperators to meet the accuracy, speed and parametric flexibility demands of\nreal-time battery management, design-of-experiments and large-scale inference.\nPE-FNO outperforms conventional neural surrogates, offering a practical path\ntowards high-speed and high-fidelity electrochemical digital twins.", "AI": {"tldr": "Operator learning models (DeepONets, FNOs, PE-FNO) were benchmarked for lithium-ion battery digital twins. PE-FNO achieved high speed (~200x faster than traditional solvers) and accuracy, enabling real-time applications and parameter estimation, outperforming other neural surrogates.", "motivation": "Reliable digital twins of lithium-ion batteries require high physical fidelity with sub-millisecond speed, which current methods struggle to achieve.", "method": "Benchmarking DeepONets, FNOs, and a parameter-embedded FNO (PE-FNO) for the Single Particle Model (SPM) of lithium-ion batteries, training on simulated trajectories across various current families and SOC levels. PE-FNO conditions spectral layers on particle radius and solid-phase diffusivity.", "result": "DeepONet accurately replicated constant-current behavior but struggled with dynamic loads. Basic FNO maintained mesh invariance and kept concentration errors below 1%, with voltage mean-absolute errors under 1.7 mV across all load types. PE-FNO enabled generalization to varying radii and diffusivities, executing ~200 times faster than a traditional SPM solver. In parameter estimation tasks, PE-FNO recovered anode and cathode diffusivities with 1.14% and 8.4% mean absolute percentage error, respectively.", "conclusion": "PE-FNO outperforms conventional neural surrogates, offering a practical path towards high-speed and high-fidelity electrochemical digital twins, meeting accuracy, speed, and parametric flexibility demands for real-time battery management, design-of-experiments, and large-scale inference."}}
{"id": "2508.07552", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07552", "abs": "https://arxiv.org/abs/2508.07552", "authors": ["Ludan Zhang", "Sihan Wang", "Yuqi Dai", "Shuofei Qiao", "Lei He"], "title": "Decoupled Functional Evaluation of Autonomous Driving Models via Feature Map Quality Scoring", "comment": null, "summary": "End-to-end models are emerging as the mainstream in autonomous driving\nperception and planning. However, the lack of explicit supervision signals for\nintermediate functional modules leads to opaque operational mechanisms and\nlimited interpretability, making it challenging for traditional methods to\nindependently evaluate and train these modules. Pioneering in the issue, this\nstudy builds upon the feature map-truth representation similarity-based\nevaluation framework and proposes an independent evaluation method based on\nFeature Map Convergence Score (FMCS). A Dual-Granularity Dynamic Weighted\nScoring System (DG-DWSS) is constructed, formulating a unified quantitative\nmetric - Feature Map Quality Score - to enable comprehensive evaluation of the\nquality of feature maps generated by functional modules. A CLIP-based Feature\nMap Quality Evaluation Network (CLIP-FMQE-Net) is further developed, combining\nfeature-truth encoders and quality score prediction heads to enable real-time\nquality analysis of feature maps generated by functional modules. Experimental\nresults on the NuScenes dataset demonstrate that integrating our evaluation\nmodule into the training improves 3D object detection performance, achieving a\n3.89 percent gain in NDS. These results verify the effectiveness of our method\nin enhancing feature representation quality and overall model performance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8bc4\u4f30\u6846\u67b6\u548c\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u7aef\u5230\u7aef\u6a21\u578b\u4e2d\u4e2d\u95f4\u529f\u80fd\u6a21\u5757\u53ef\u89e3\u91ca\u6027\u5dee\u7684\u95ee\u9898\u3002\u901a\u8fc7FMCS\u548cCLIP-FMQE-Net\uff0c\u5b9e\u73b0\u4e86\u5bf9\u7279\u5f81\u56fe\u8d28\u91cf\u7684\u91cf\u5316\u8bc4\u4f30\u548c\u5b9e\u65f6\u5206\u6790\uff0c\u5e76\u5728\u5b9e\u8df5\u4e2d\u6709\u6548\u63d0\u5347\u4e86\u6a21\u578b\u7684\u6027\u80fd\u3002", "motivation": "\u7aef\u5230\u7aef\u6a21\u578b\u5728\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u548c\u89c4\u5212\u4e2d\u65e5\u76ca\u666e\u53ca\uff0c\u4f46\u5176\u7f3a\u4e4f\u5bf9\u4e2d\u95f4\u529f\u80fd\u6a21\u5757\u7684\u663e\u5f0f\u76d1\u7763\u4fe1\u53f7\uff0c\u5bfc\u81f4\u64cd\u4f5c\u673a\u5236\u4e0d\u900f\u660e\u3001\u53ef\u89e3\u91ca\u6027\u6709\u9650\uff0c\u4f7f\u5f97\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u72ec\u7acb\u8bc4\u4f30\u548c\u8bad\u7ec3\u8fd9\u4e9b\u6a21\u5757\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u7279\u5f81\u56fe-\u771f\u503c\u8868\u793a\u76f8\u4f3c\u6027\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u5e76\u5728\u6b64\u57fa\u7840\u4e0a\u5f00\u53d1\u4e86\u7279\u5f81\u56fe\u6536\u655b\u5f97\u5206\uff08FMCS\uff09\u7684\u72ec\u7acb\u8bc4\u4f30\u65b9\u6cd5\u3002\u6784\u5efa\u4e86\u53cc\u7c92\u5ea6\u52a8\u6001\u52a0\u6743\u8bc4\u5206\u7cfb\u7edf\uff08DG-DWSS\uff09\u6765\u751f\u6210\u7edf\u4e00\u7684\u7279\u5f81\u56fe\u8d28\u91cf\u8bc4\u5206\uff0c\u5e76\u8fdb\u4e00\u6b65\u5f00\u53d1\u4e86\u57fa\u4e8eCLIP\u7684\u7279\u5f81\u56fe\u8d28\u91cf\u8bc4\u4f30\u7f51\u7edc\uff08CLIP-FMQE-Net\uff09\uff0c\u96c6\u6210\u4e86\u7279\u5f81-\u771f\u503c\u7f16\u7801\u5668\u548c\u8d28\u91cf\u5206\u6570\u9884\u6d4b\u5934\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u529f\u80fd\u6a21\u5757\u751f\u6210\u7684\u7279\u5f81\u56fe\u8fdb\u884c\u5b9e\u65f6\u8d28\u91cf\u5206\u6790\u3002", "result": "\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5c06\u672c\u7814\u7a76\u63d0\u51fa\u7684\u8bc4\u4f30\u6a21\u5757\u96c6\u6210\u5230\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u53ef\u4ee5\u63d0\u9ad83D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0cNDS\u63d0\u9ad8\u4e863.89%\u3002\u8fd9\u4e9b\u7ed3\u679c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u5728\u63d0\u5347\u7279\u5f81\u8868\u793a\u8d28\u91cf\u548c\u6574\u4f53\u6a21\u578b\u6027\u80fd\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u57fa\u4e8eFMCS\u548cCLIP-FMQE-Net\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u80fd\u591f\u6709\u6548\u63d0\u5347\u81ea\u52a8\u9a7e\u9a76\u611f\u77e5\u548c\u89c4\u5212\u4efb\u52a1\u4e2d\u7aef\u5230\u7aef\u6a21\u578b\u7279\u5f81\u56fe\u7684\u8d28\u91cf\uff0c\u8fdb\u800c\u63d0\u9ad83D\u76ee\u6807\u68c0\u6d4b\u6027\u80fd\uff0c\u5728NuScenes\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e863.89%\u7684NDS\u63d0\u5347\u3002"}}
{"id": "2508.08100", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08100", "abs": "https://arxiv.org/abs/2508.08100", "authors": ["Md. Wasiul Haque", "Sagar Dasgupta", "Mizanur Rahman"], "title": "Grid2Guide: A* Enabled Small Language Model for Indoor Navigation", "comment": "23 pages, 8 figures, 6 tables", "summary": "Reliable indoor navigation remains a significant challenge in complex\nenvironments, particularly where external positioning signals and dedicated\ninfrastructures are unavailable. This research presents Grid2Guide, a hybrid\nnavigation framework that combines the A* search algorithm with a Small\nLanguage Model (SLM) to generate clear, human-readable route instructions. The\nframework first conducts a binary occupancy matrix from a given indoor map.\nUsing this matrix, the A* algorithm computes the optimal path between origin\nand destination, producing concise textual navigation steps. These steps are\nthen transformed into natural language instructions by the SLM, enhancing\ninterpretability for end users. Experimental evaluations across various indoor\nscenarios demonstrate the method's effectiveness in producing accurate and\ntimely navigation guidance. The results validate the proposed approach as a\nlightweight, infrastructure-free solution for real-time indoor navigation\nsupport.", "AI": {"tldr": "Grid2Guide\u662f\u4e00\u4e2a\u7ed3\u5408A*\u7b97\u6cd5\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\u7684\u6df7\u5408\u5bfc\u822a\u6846\u67b6\uff0c\u80fd\u751f\u6210\u6613\u4e8e\u7406\u89e3\u7684\u5ba4\u5185\u5bfc\u822a\u6307\u793a\uff0c\u4e14\u65e0\u9700\u5916\u90e8\u4fe1\u53f7\u6216\u57fa\u7840\u8bbe\u65bd\u3002", "motivation": "\u5728\u7f3a\u4e4f\u5916\u90e8\u5b9a\u4f4d\u4fe1\u53f7\u548c\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\u7684\u590d\u6742\u5ba4\u5185\u73af\u5883\u4e2d\uff0c\u63d0\u4f9b\u53ef\u9760\u7684\u5ba4\u5185\u5bfc\u822a\u4ecd\u7136\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\u3002", "method": "\u8be5\u6846\u67b6\u9996\u5148\u4ece\u5ba4\u5185\u5730\u56fe\u751f\u6210\u4e8c\u8fdb\u5236\u5360\u7528\u77e9\u9635\uff0c\u7136\u540e\u5229\u7528A*\u7b97\u6cd5\u8ba1\u7b97\u6700\u4f18\u8def\u5f84\u5e76\u751f\u6210\u7b80\u6d01\u7684\u5bfc\u822a\u6b65\u9aa4\u3002\u63a5\u7740\uff0c\u5229\u7528SLM\u5c06\u8fd9\u4e9b\u6b65\u9aa4\u8f6c\u6362\u4e3a\u81ea\u7136\u8bed\u8a00\u6307\u793a\uff0c\u63d0\u9ad8\u4e86\u53ef\u89e3\u91ca\u6027\u3002", "result": "\u5b9e\u9a8c\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u5ba4\u5185\u573a\u666f\u4e0b\u90fd\u80fd\u751f\u6210\u51c6\u786e\u3001\u53ca\u65f6\u7684\u5bfc\u822a\u6307\u5bfc\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u4e00\u79cd\u8f7b\u91cf\u7ea7\u3001\u65e0\u57fa\u7840\u8bbe\u65bd\u7684\u5b9e\u65f6\u5ba4\u5185\u5bfc\u822a\u652f\u6301\u89e3\u51b3\u65b9\u6848\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aGrid2Guide\u7684\u6df7\u5408\u5bfc\u822a\u6846\u67b6\uff0c\u7ed3\u5408\u4e86A*\u641c\u7d22\u7b97\u6cd5\u548c\u5c0f\u578b\u8bed\u8a00\u6a21\u578b\uff08SLM\uff09\uff0c\u80fd\u591f\u4e3a\u7528\u6237\u751f\u6210\u6e05\u6670\u3001\u6613\u4e8e\u7406\u89e3\u7684\u8def\u7ebf\u5bfc\u822a\u6307\u793a\u3002\u8be5\u65b9\u6cd5\u65e0\u9700\u5916\u90e8\u5b9a\u4f4d\u4fe1\u53f7\u6216\u4e13\u7528\u57fa\u7840\u8bbe\u65bd\uff0c\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u9002\u7528\u4e8e\u5b9e\u65f6\u5ba4\u5185\u5bfc\u822a\u3002"}}
{"id": "2508.07557", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07557", "abs": "https://arxiv.org/abs/2508.07557", "authors": ["Minghao Yin", "Yukang Cao", "Songyou Peng", "Kai Han"], "title": "Splat4D: Diffusion-Enhanced 4D Gaussian Splatting for Temporally and Spatially Consistent Content Creation", "comment": null, "summary": "Generating high-quality 4D content from monocular videos for applications\nsuch as digital humans and AR/VR poses challenges in ensuring temporal and\nspatial consistency, preserving intricate details, and incorporating user\nguidance effectively. To overcome these challenges, we introduce Splat4D, a\nnovel framework enabling high-fidelity 4D content generation from a monocular\nvideo. Splat4D achieves superior performance while maintaining faithful\nspatial-temporal coherence by leveraging multi-view rendering, inconsistency\nidentification, a video diffusion model, and an asymmetric U-Net for\nrefinement. Through extensive evaluations on public benchmarks, Splat4D\nconsistently demonstrates state-of-the-art performance across various metrics,\nunderscoring the efficacy of our approach. Additionally, the versatility of\nSplat4D is validated in various applications such as text/image conditioned 4D\ngeneration, 4D human generation, and text-guided content editing, producing\ncoherent outcomes following user instructions.", "AI": {"tldr": "Splat4D \u6846\u67b6\u5229\u7528\u591a\u89c6\u56fe\u6e32\u67d3\u3001\u4e0d\u4e00\u81f4\u6027\u8bc6\u522b\u3001\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u975e\u5bf9\u79f0 U-Net\uff0c\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u8d28\u91cf 4D \u5185\u5bb9\uff0c\u514b\u670d\u4e86\u65f6\u95f4\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u7528\u6237\u6307\u5bfc\u7684\u6311\u6218\uff0c\u5e76\u5728\u5404\u79cd\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u8d28\u91cf 4D \u5185\u5bb9\u65f6\u5728\u65f6\u95f4\u7a7a\u95f4\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u7528\u6237\u6307\u5bfc\u65b9\u9762\u7684\u6311\u6218\u3002", "method": "Splat4D \u6846\u67b6\u91c7\u7528\u4e86\u591a\u89c6\u56fe\u6e32\u67d3\u3001\u4e0d\u4e00\u81f4\u6027\u8bc6\u522b\u3001\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u975e\u5bf9\u79f0 U-Net \u7b49\u6280\u672f\u6765\u751f\u6210 4D \u5185\u5bb9\u3002", "result": "Splat4D \u5728\u5404\u9879\u6307\u6807\u4e0a\u90fd\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u6587\u672c/\u56fe\u50cf\u6761\u4ef6 4D \u751f\u6210\u30014D \u4eba\u4f53\u751f\u6210\u548c\u6587\u672c\u5f15\u5bfc\u5185\u5bb9\u7f16\u8f91\u7b49\u5e94\u7528\u4e2d\u8868\u73b0\u51fa\u4e86\u901a\u7528\u6027\u3002", "conclusion": "Splat4D \u6846\u67b6\u901a\u8fc7\u591a\u89c6\u56fe\u6e32\u67d3\u3001\u4e0d\u4e00\u81f4\u6027\u8bc6\u522b\u3001\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u975e\u5bf9\u79f0 U-Net \u5b9e\u73b0\u4e86\u4ece\u5355\u76ee\u89c6\u9891\u751f\u6210\u9ad8\u8d28\u91cf 4D \u5185\u5bb9\uff0c\u5e76\u5728\u7a7a\u95f4-\u65f6\u95f4\u4e00\u81f4\u6027\u3001\u7ec6\u8282\u4fdd\u7559\u548c\u7528\u6237\u6307\u5bfc\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08120", "categories": ["cs.LG", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08120", "abs": "https://arxiv.org/abs/2508.08120", "authors": ["Keyan Rahimi", "Md. Wasiul Haque", "Sagar Dasgupta", "Mizanur Rahman"], "title": "Vision-Based Localization and LLM-based Navigation for Indoor Environments", "comment": "20 pages, 6 figures, 1 table", "summary": "Indoor navigation remains a complex challenge due to the absence of reliable\nGPS signals and the architectural intricacies of large enclosed environments.\nThis study presents an indoor localization and navigation approach that\nintegrates vision-based localization with large language model (LLM)-based\nnavigation. The localization system utilizes a ResNet-50 convolutional neural\nnetwork fine-tuned through a two-stage process to identify the user's position\nusing smartphone camera input. To complement localization, the navigation\nmodule employs an LLM, guided by a carefully crafted system prompt, to\ninterpret preprocessed floor plan images and generate step-by-step directions.\nExperimental evaluation was conducted in a realistic office corridor with\nrepetitive features and limited visibility to test localization robustness. The\nmodel achieved high confidence and an accuracy of 96% across all tested\nwaypoints, even under constrained viewing conditions and short-duration\nqueries. Navigation tests using ChatGPT on real building floor maps yielded an\naverage instruction accuracy of 75%, with observed limitations in zero-shot\nreasoning and inference time. This research demonstrates the potential for\nscalable, infrastructure-free indoor navigation using off-the-shelf cameras and\npublicly available floor plans, particularly in resource-constrained settings\nlike hospitals, airports, and educational institutions.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u89c6\u89c9\u5b9a\u4f4d\u548cLLM\u5bfc\u822a\u7684\u5ba4\u5185\u5bfc\u822a\u65b9\u6cd5\uff0c\u5728\u6a21\u62df\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5e94\u7528\u7684\u6f5c\u529b\u3002", "motivation": "\u5ba4\u5185\u5bfc\u822a\u4ecd\u7136\u662f\u4e00\u4e2a\u590d\u6742\u7684\u6311\u6218\uff0c\u56e0\u4e3a\u7f3a\u4e4f\u53ef\u9760\u7684GPS\u4fe1\u53f7\u4ee5\u53ca\u5927\u578b\u5c01\u95ed\u73af\u5883\u7684\u5efa\u7b51\u590d\u6742\u6027\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e86\u57fa\u4e8e\u89c6\u89c9\u7684\u5b9a\u4f4d\u548c\u57fa\u4e8e\u5927\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u7684\u5bfc\u822a\u7684\u5ba4\u5185\u5b9a\u4f4d\u548c\u5bfc\u822a\u65b9\u6cd5\u3002\u5b9a\u4f4d\u7cfb\u7edf\u5229\u7528\u5728\u4e24\u4e2a\u9636\u6bb5\u8fdb\u884c\u5fae\u8c03\u7684ResNet-50\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff0c\u901a\u8fc7\u667a\u80fd\u624b\u673a\u6444\u50cf\u5934\u8f93\u5165\u6765\u8bc6\u522b\u7528\u6237\u7684\u4f4d\u7f6e\u3002\u4e3a\u4e86\u8865\u5145\u5b9a\u4f4d\uff0c\u5bfc\u822a\u6a21\u5757\u91c7\u7528\u4e86\u4e00\u4e2aLLM\uff0c\u5e76\u7531\u4e00\u4e2a\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u7cfb\u7edf\u63d0\u793a\u6765\u6307\u5bfc\uff0c\u4ee5\u89e3\u91ca\u9884\u5904\u7406\u7684\u697c\u5c42\u5e73\u9762\u56fe\u56fe\u50cf\u5e76\u751f\u6210\u5206\u6b65\u6307\u793a\u3002", "result": "\u5b9a\u4f4d\u6a21\u578b\u5728\u6240\u6709\u6d4b\u8bd5\u7684\u822a\u70b9\u4e0a\u5b9e\u73b0\u4e8696%\u7684\u9ad8\u7f6e\u4fe1\u5ea6\u548c\u51c6\u786e\u7387\uff0c\u5373\u4f7f\u5728\u53d7\u9650\u7684\u89c6\u91ce\u6761\u4ef6\u4e0b\u548c\u77ed\u65f6\u95f4\u67e5\u8be2\u4e0b\u4e5f\u662f\u5982\u6b64\u3002\u4f7f\u7528ChatGPT\u5728\u5b9e\u9645\u5efa\u7b51\u697c\u5c42\u5730\u56fe\u4e0a\u8fdb\u884c\u7684\u5bfc\u822a\u6d4b\u8bd5\u4ea7\u751f\u4e8675%\u7684\u5e73\u5747\u6307\u4ee4\u51c6\u786e\u7387\uff0c\u5e76\u89c2\u5bdf\u5230\u96f6\u6837\u672c\u63a8\u7406\u548c\u63a8\u7406\u65f6\u95f4\u5b58\u5728\u5c40\u9650\u6027\u3002", "conclusion": "\u8be5\u7814\u7a76\u5c55\u793a\u4e86\u4f7f\u7528\u73b0\u6210\u76f8\u673a\u548c\u516c\u5f00\u697c\u5c42\u5e73\u9762\u56fe\u8fdb\u884c\u53ef\u6269\u5c55\u3001\u65e0\u57fa\u7840\u8bbe\u65bd\u7684\u5ba4\u5185\u5bfc\u822a\u7684\u6f5c\u529b\uff0c\u5c24\u5176\u662f\u5728\u533b\u9662\u3001\u673a\u573a\u548c\u6559\u80b2\u673a\u6784\u7b49\u8d44\u6e90\u53d7\u9650\u7684\u73af\u5883\u4e2d\u3002"}}
{"id": "2508.07570", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07570", "abs": "https://arxiv.org/abs/2508.07570", "authors": ["Khanh-Binh Nguyen", "Phuoc-Nguyen Bui", "Hyunseung Choo", "Duc Thanh Nguyen"], "title": "Adaptive Cache Enhancement for Test-Time Adaptation of Vision-Language Models", "comment": "12 pages, Under review", "summary": "Vision-language models (VLMs) exhibit remarkable zero-shot generalization but\nsuffer performance degradation under distribution shifts in downstream tasks,\nparticularly in the absence of labeled data. Test-Time Adaptation (TTA)\naddresses this challenge by enabling online optimization of VLMs during\ninference, eliminating the need for annotated data. Cache-based TTA methods\nexploit historical knowledge by maintaining a dynamic memory cache of\nlow-entropy or high-confidence samples, promoting efficient adaptation to\nout-of-distribution data. Nevertheless, these methods face two critical\nchallenges: (1) unreliable confidence metrics under significant distribution\nshifts, resulting in error accumulation within the cache and degraded\nadaptation performance; and (2) rigid decision boundaries that fail to\naccommodate substantial distributional variations, leading to suboptimal\npredictions. To overcome these limitations, we introduce the Adaptive Cache\nEnhancement (ACE) framework, which constructs a robust cache by selectively\nstoring high-confidence or low-entropy image embeddings per class, guided by\ndynamic, class-specific thresholds initialized from zero-shot statistics and\niteratively refined using an exponential moving average and\nexploration-augmented updates. This approach enables adaptive, class-wise\ndecision boundaries, ensuring robust and accurate predictions across diverse\nvisual distributions. Extensive experiments on 15 diverse benchmark datasets\ndemonstrate that ACE achieves state-of-the-art performance, delivering superior\nrobustness and generalization compared to existing TTA methods in challenging\nout-of-distribution scenarios.", "AI": {"tldr": "ACE\u6846\u67b6\u901a\u8fc7\u81ea\u9002\u5e94\u7684\u3001\u7c7b\u522b\u7684\u7f13\u5b58\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u73b0\u6709TTA\u65b9\u6cd5\u5728\u5206\u5e03\u5916\u6570\u636e\u4e0a\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u7f13\u5b58\u7684TTA\u65b9\u6cd5\u5728\u5206\u5e03\u8f6c\u53d8\u4e0b\u4f1a\u56e0\u4e3a\u4e0d\u53ef\u9760\u7684\u7f6e\u4fe1\u5ea6\u6307\u6807\u5bfc\u81f4\u9519\u8bef\u7d2f\u79ef\uff0c\u5e76\u4e14\u50f5\u5316\u7684\u51b3\u7b56\u8fb9\u754c\u65e0\u6cd5\u9002\u5e94\u663e\u8457\u7684\u5206\u5e03\u53d8\u5316\uff0c\u5bfc\u81f4\u6b21\u4f18\u9884\u6d4b\u3002ACE\u65e8\u5728\u514b\u670d\u8fd9\u4e9b\u9650\u5236\u3002", "method": "ACE\u6846\u67b6\u901a\u8fc7\u52a8\u6001\u7684\u3001\u7c7b\u7279\u5b9a\u7684\u9608\u503c\u6765\u9009\u62e9\u6027\u5730\u5b58\u50a8\u9ad8\u7f6e\u4fe1\u5ea6\u6216\u4f4e\u71b5\u7684\u7c7b\u522b\u56fe\u50cf\u5d4c\u5165\uff0c\u8fd9\u4e9b\u9608\u503c\u4ece\u96f6\u6837\u672c\u7edf\u8ba1\u6570\u636e\u521d\u59cb\u5316\uff0c\u5e76\u4f7f\u7528\u6307\u6570\u79fb\u52a8\u5e73\u5747\u548c\u63a2\u7d22\u589e\u5f3a\u66f4\u65b0\u8fdb\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u8fd9\u79cd\u65b9\u6cd5\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u7684\u3001\u7c7b\u522b\u7684\u51b3\u7b56\u8fb9\u754c\u3002", "result": "ACE\u6846\u67b6\u901a\u8fc7\u9009\u62e9\u6027\u5b58\u50a8\u9ad8\u7f6e\u4fe1\u5ea6\u6216\u4f4e\u71b5\u7684\u7c7b\u522b\u56fe\u50cf\u5d4c\u5165\uff0c\u5b9e\u73b0\u4e86\u81ea\u9002\u5e94\u7684\u3001\u7c7b\u522b\u7684\u51b3\u7b56\u8fb9\u754c\uff0c\u4ece\u800c\u5728\u4e0d\u540c\u7684\u89c6\u89c9\u5206\u5e03\u4e0b\u5b9e\u73b0\u9c81\u68d2\u4e14\u51c6\u786e\u7684\u9884\u6d4b\u3002", "conclusion": "ACE\u6846\u67b6\u572815\u4e2a\u4e0d\u540c\u7684\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u5206\u5e03\u5916\u573a\u666f\u4e2d\uff0c\u4e0e\u73b0\u6709\u7684TTA\u65b9\u6cd5\u76f8\u6bd4\uff0c\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.08122", "categories": ["cs.LG", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08122", "abs": "https://arxiv.org/abs/2508.08122", "authors": ["Mingrong Lin", "Ke Deng", "Zhengyang Wu", "Zetao Zheng", "Jie Li"], "title": "MemoryKT: An Integrative Memory-and-Forgetting Method for Knowledge Tracing", "comment": "9 pages, 4 figures", "summary": "Knowledge Tracing (KT) is committed to capturing students' knowledge mastery\nfrom their historical interactions. Simulating students' memory states is a\npromising approach to enhance both the performance and interpretability of\nknowledge tracing models. Memory consists of three fundamental processes:\nencoding, storage, and retrieval. Although forgetting primarily manifests\nduring the storage stage, most existing studies rely on a single,\nundifferentiated forgetting mechanism, overlooking other memory processes as\nwell as personalized forgetting patterns. To address this, this paper proposes\nmemoryKT, a knowledge tracing model based on a novel temporal variational\nautoencoder. The model simulates memory dynamics through a three-stage process:\n(i) Learning the distribution of students' knowledge memory features, (ii)\nReconstructing their exercise feedback, while (iii) Embedding a personalized\nforgetting module within the temporal workflow to dynamically modulate memory\nstorage strength. This jointly models the complete encoding-storage-retrieval\ncycle, significantly enhancing the model's perception capability for individual\ndifferences. Extensive experiments on four public datasets demonstrate that our\nproposed approach significantly outperforms state-of-the-art baselines.", "AI": {"tldr": "memoryKT\u662f\u4e00\u4e2a\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\uff0c\u901a\u8fc7\u6a21\u62df\u5b8c\u6574\u7684\u7f16\u7801-\u5b58\u50a8-\u68c0\u7d22\u5468\u671f\uff0c\u5e76\u5305\u542b\u4e2a\u6027\u5316\u9057\u5fd8\u6a21\u5757\uff0c\u80fd\u591f\u66f4\u597d\u5730\u6355\u6349\u5b66\u751f\u77e5\u8bc6\u638c\u63e1\u60c5\u51b5\uff0c\u5b9e\u9a8c\u7ed3\u679c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u5927\u591a\u4f9d\u8d56\u5355\u4e00\u7684\u3001\u672a\u533a\u5206\u7684\u9057\u5fd8\u673a\u5236\uff0c\u5ffd\u7565\u4e86\u5176\u4ed6\u8bb0\u5fc6\u8fc7\u7a0b\u4ee5\u53ca\u4e2a\u6027\u5316\u7684\u9057\u5fd8\u6a21\u5f0f\uff0c\u800c\u6a21\u62df\u5b66\u751f\u8bb0\u5fc6\u72b6\u6001\u662f\u63d0\u5347\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\u6027\u80fd\u548c\u53ef\u89e3\u91ca\u6027\u7684\u6709\u524d\u666f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u65b0\u9896\u65f6\u95f4\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff08temporal variational autoencoder\uff09\u7684\u77e5\u8bc6\u8ffd\u8e2a\u6a21\u578b\uff08memoryKT\uff09\uff0c\u8be5\u6a21\u578b\u901a\u8fc7\u4e09\u4e2a\u9636\u6bb5\u6a21\u62df\u8bb0\u5fc6\u52a8\u6001\uff1a\u5b66\u4e60\u5b66\u751f\u77e5\u8bc6\u8bb0\u5fc6\u7279\u5f81\u7684\u5206\u5e03\u3001\u91cd\u5efa\u7ec3\u4e60\u53cd\u9988\u3001\u5e76\u5728\u65f6\u95f4\u5de5\u4f5c\u6d41\u4e2d\u5d4c\u5165\u4e2a\u6027\u5316\u9057\u5fd8\u6a21\u5757\u4ee5\u52a8\u6001\u8c03\u8282\u8bb0\u5fc6\u5b58\u50a8\u5f3a\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002", "conclusion": "\u8be5\u6a21\u578b\u901a\u8fc7\u6a21\u62df\u8bb0\u5fc6\u7684\u4e09\u4e2a\u8fc7\u7a0b\uff1a\u7f16\u7801\u3001\u5b58\u50a8\u548c\u68c0\u7d22\uff0c\u5e76\u5f15\u5165\u4e2a\u6027\u5316\u9057\u5fd8\u673a\u5236\uff0c\u80fd\u591f\u66f4\u597d\u5730\u611f\u77e5\u4e2a\u4f53\u5dee\u5f02\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\u8be5\u6a21\u578b\u5728\u56db\u4e2a\u516c\u5171\u6570\u636e\u96c6\u4e0a\u663e\u8457\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u57fa\u7ebf\u6a21\u578b\u3002"}}
{"id": "2508.07577", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07577", "abs": "https://arxiv.org/abs/2508.07577", "authors": ["Zhaorui Tan", "Tan Pan", "Kaizhu Huang", "Weimiao Yu", "Kai Yao", "Chen Jiang", "Qiufeng Wang", "Anh Nguyen", "Xin Guo", "Yuan Cheng", "Xi Yang"], "title": "Exploiting Layer Normalization Fine-tuning in Visual Transformer Foundation Models for Classification", "comment": null, "summary": "LayerNorm is pivotal in Vision Transformers (ViTs), yet its fine-tuning\ndynamics under data scarcity and domain shifts remain underexplored. This paper\nshows that shifts in LayerNorm parameters after fine-tuning (LayerNorm shifts)\nare indicative of the transitions between source and target domains; its\nefficacy is contingent upon the degree to which the target training samples\naccurately represent the target domain, as quantified by our proposed\nFine-tuning Shift Ratio ($FSR$). Building on this, we propose a simple yet\neffective rescaling mechanism using a scalar $\\lambda$ that is negatively\ncorrelated to $FSR$ to align learned LayerNorm shifts with those ideal shifts\nachieved under fully representative data, combined with a cyclic framework that\nfurther enhances the LayerNorm fine-tuning. Extensive experiments across\nnatural and pathological images, in both in-distribution (ID) and\nout-of-distribution (OOD) settings, and various target training sample regimes\nvalidate our framework. Notably, OOD tasks tend to yield lower $FSR$ and higher\n$\\lambda$ in comparison to ID cases, especially with scarce data, indicating\nunder-represented target training samples. Moreover, ViTFs fine-tuned on\npathological data behave more like ID settings, favoring conservative LayerNorm\nupdates. Our findings illuminate the underexplored dynamics of LayerNorm in\ntransfer learning and provide practical strategies for LayerNorm fine-tuning.", "AI": {"tldr": "\u672c\u7814\u7a76\u9488\u5bf9Vision Transformers\u5728\u6570\u636e\u532e\u4e4f\u548c\u9886\u57df\u53d8\u5316\u4e0b\u7684\u5fae\u8c03\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7LayerNorm\u53c2\u6570\u8c03\u6574\uff08LayerNorm shifts\uff09\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u7684\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86Fine-tuning Shift Ratio (FSR)\u548c\u91cd\u9002\u673a\u5236\uff08\u03bb\uff09\uff0c\u5728\u591a\u9879\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "Vision Transformers\u4e2d\u7684LayerNorm\u5728\u6570\u636e\u7a00\u758f\u548c\u9886\u57df\u8fc1\u79fb\u7684\u5fae\u8c03\u52a8\u6001\u65b9\u9762\u7814\u7a76\u4e0d\u8db3\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63a2\u8ba8LayerNorm\u53c2\u6570\u5fae\u8c03\u540e\u7684\u53d8\u5316\uff08LayerNorm shifts\uff09\u5982\u4f55\u6307\u793a\u9886\u57df\u8fc1\u79fb\uff0c\u5e76\u63d0\u51fa\u6539\u8fdb\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4f7f\u7528\u6807\u91cf\u03bb\u7684\u91cd\u9002\u673a\u5236\uff0c\u8be5\u6807\u91cf\u4e0eFSR\u8d1f\u76f8\u5173\uff0c\u4ee5\u5bf9\u9f50\u5b66\u4e60\u5230\u7684LayerNorm\u4f4d\u79fb\u548c\u5728\u5b8c\u5168\u4ee3\u8868\u6027\u6570\u636e\u4e0b\u5b9e\u73b0\u7684\u7406\u60f3\u4f4d\u79fb\uff0c\u5e76\u7ed3\u5408\u4e86\u589e\u5f3aLayerNorm\u5fae\u8c03\u7684\u5468\u671f\u6027\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u5728\u81ea\u7136\u548c\u75c5\u7406\u56fe\u50cf\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\uff0c\u5305\u62ec in-distribution (ID) \u548c out-of-distribution (OOD) \u8bbe\u7f6e\u4ee5\u53ca\u4e0d\u540c\u7684\u76ee\u6807\u8bad\u7ec3\u6837\u672c\u6570\u91cf\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u6846\u67b6\u7684\u6709\u6548\u6027\u3002\u7ed3\u679c\u8868\u660e\uff0cOOD\u4efb\u52a1\u901a\u5e38\u5177\u6709\u8f83\u4f4e\u7684FSR\u548c\u8f83\u9ad8\u7684\u03bb\uff0c\u5c24\u5176\u662f\u5728\u6570\u636e\u7a00\u758f\u7684\u60c5\u51b5\u4e0b\uff0c\u8868\u660e\u76ee\u6807\u8bad\u7ec3\u6837\u672c\u7684\u4ee3\u8868\u6027\u4e0d\u8db3\uff1b\u540c\u65f6\uff0c\u5728\u75c5\u7406\u6570\u636e\u4e0a\u5fae\u8c03\u7684ViTFs\u8868\u73b0\u66f4\u50cfID\u8bbe\u7f6e\uff0c\u503e\u5411\u4e8e\u4fdd\u5b88\u7684LayerNorm\u66f4\u65b0\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u57fa\u4e8eLayerNorm\u53c2\u6570\u8c03\u6574\u7684\u7b80\u5355\u6709\u6548\u7684\u65b9\u6cd5\uff0c\u4ee5\u63d0\u5347Vision Transformers\u5728\u6570\u636e\u7a00\u758f\u548c\u9886\u57df\u8fc1\u79fb\u573a\u666f\u4e0b\u7684\u5fae\u8c03\u6027\u80fd\uff0c\u5e76\u901a\u8fc7Fine-tuning Shift Ratio (FSR)\u91cf\u5316\u4e86\u76ee\u6807\u8bad\u7ec3\u6837\u672c\u7684\u9886\u57df\u4ee3\u8868\u6027\uff0c\u4e3aLayerNorm\u5728\u8fc1\u79fb\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u7b56\u7565\u3002"}}
{"id": "2508.08124", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08124", "abs": "https://arxiv.org/abs/2508.08124", "authors": ["Guanghao Jin", "Yuan Liang", "Yihan Ma", "Jingpei Wu", "Guoyang Liu"], "title": "NeuroDx-LM: A Clinical Large-Scale Model for EEG-based Neurological Disorder Detection", "comment": null, "summary": "Large-scale models pre-trained on Electroencephalography (EEG) have shown\npromise in clinical applications such as neurological disorder detection.\nHowever, the practical deployment of EEG-based large-scale models faces\ncritical challenges such as limited labeled EEG data and suboptimal performance\nin clinical scenarios. To address these issues, we propose NeuroDx-LM, a novel\nlarge-scale model specifically designed for detecting EEG-based neurological\ndisorders. Our key contributions include (i) a Selective Temporal-Frequency\nEmbedding mechanism that adaptively captures complex temporal and spectral\npatterns in EEG signals; and (ii) a Progressive Feature-Aware Training strategy\nthat refines feature representation in a two-stage process. In the first stage,\nour model learns the fundamental discriminative features of EEG activities; in\nthe second stage, the model further extracts more specialized fine-grained\nfeatures for accurate diagnostic performance. We evaluated NeuroDx-LM on the\nCHB-MIT and Schizophrenia datasets, achieving state-of-the-art performance in\nEEG-based seizure and schizophrenia detection, respectively. These results\ndemonstrate the great potential of EEG-based large-scale models to advance\nclinical applicability. Our code is available at\nhttps://github.com/LetItBe12345/NeuroDx-LM.", "AI": {"tldr": "NeuroDx-LM is a new large-scale model for detecting neurological disorders using EEG. It uses advanced embedding and training methods to improve performance, overcoming limitations of existing models and achieving top results on benchmark datasets.", "motivation": "The practical deployment of EEG-based large-scale models faces challenges due to limited labeled EEG data and suboptimal clinical performance.", "method": "NeuroDx-LM utilizes a Selective Temporal-Frequency Embedding mechanism to capture complex EEG patterns and a Progressive Feature-Aware Training strategy in two stages: first learning fundamental discriminative features, then extracting specialized fine-grained features for diagnosis.", "result": "NeuroDx-LM achieved state-of-the-art performance on the CHB-MIT dataset for seizure detection and the Schizophrenia dataset for schizophrenia detection.", "conclusion": "EEG-based large-scale models have great potential to advance clinical applicability, as demonstrated by NeuroDx-LM's state-of-the-art performance in seizure and schizophrenia detection."}}
{"id": "2508.07585", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07585", "abs": "https://arxiv.org/abs/2508.07585", "authors": ["Yu-Huan Wu", "Wei Liu", "Zi-Xuan Zhu", "Zizhou Wang", "Yong Liu", "Liangli Zhen"], "title": "GAPNet: A Lightweight Framework for Image and Video Salient Object Detection via Granularity-Aware Paradigm", "comment": "21 pages, 7 figures, 6 tables", "summary": "Recent salient object detection (SOD) models predominantly rely on\nheavyweight backbones, incurring substantial computational cost and hindering\ntheir practical application in various real-world settings, particularly on\nedge devices. This paper presents GAPNet, a lightweight network built on the\ngranularity-aware paradigm for both image and video SOD. We assign saliency\nmaps of different granularities to supervise the multi-scale decoder\nside-outputs: coarse object locations for high-level outputs and fine-grained\nobject boundaries for low-level outputs. Specifically, our decoder is built\nwith granularity-aware connections which fuse high-level features of low\ngranularity and low-level features of high granularity, respectively. To\nsupport these connections, we design granular pyramid convolution (GPC) and\ncross-scale attention (CSA) modules for efficient fusion of low-scale and\nhigh-scale features, respectively. On top of the encoder, a self-attention\nmodule is built to learn global information, enabling accurate object\nlocalization with negligible computational cost. Unlike traditional U-Net-based\napproaches, our proposed method optimizes feature utilization and semantic\ninterpretation while applying appropriate supervision at each processing stage.\nExtensive experiments show that the proposed method achieves a new\nstate-of-the-art performance among lightweight image and video SOD models. Code\nis available at https://github.com/yuhuan-wu/GAPNet.", "AI": {"tldr": "GAPNet \u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u7684\u56fe\u50cf\u548c\u89c6\u9891\u663e\u8457\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff0c\u901a\u8fc7\u521b\u65b0\u7684\u7279\u5f81\u878d\u5408\u548c\u591a\u5c3a\u5ea6\u76d1\u7763\uff0c\u5728\u4fdd\u6301\u4f4e\u8ba1\u7b97\u6210\u672c\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5f53\u524d\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u6a21\u578b\u4f9d\u8d56\u91cd\u578b\u9aa8\u5e72\u7f51\u7edc\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u9ad8\u6602\uff0c\u5728\u8fb9\u7f18\u8bbe\u5907\u7b49\u5b9e\u9645\u5e94\u7528\u4e2d\u53d7\u5230\u9650\u5236\u7684\u95ee\u9898\u3002", "method": "GAPNet \u91c7\u7528\u57fa\u4e8e\u7c92\u5ea6\u611f\u77e5\u8303\u5f0f\u7684\u65b9\u6cd5\uff0c\u5e76\u8bbe\u8ba1\u4e86\u7c92\u5ea6\u91d1\u5b57\u5854\u5377\u79ef\uff08GPC\uff09\u548c\u8de8\u5c3a\u5ea6\u6ce8\u610f\u529b\uff08CSA\uff09\u6a21\u5757\u6765\u5b9e\u73b0\u9ad8\u6548\u7279\u5f81\u878d\u5408\u3002\u6a21\u578b\u5728\u591a\u5c3a\u5ea6\u89e3\u7801\u5668\u4e2d\u4f7f\u7528\u4e0d\u540c\u7c92\u5ea6\u7684\u663e\u8457\u56fe\u8fdb\u884c\u76d1\u7763\uff0c\u5e76\u901a\u8fc7\u81ea\u6ce8\u610f\u529b\u6a21\u5757\u5b66\u4e60\u5168\u5c40\u4fe1\u606f\u4ee5\u5b9e\u73b0\u7cbe\u786e\u7684\u76ee\u6807\u5b9a\u4f4d\u3002", "result": "GAPNet \u5728\u4f5c\u4e3a\u8f7b\u91cf\u7ea7\u56fe\u50cf\u548c\u89c6\u9891 SOD \u6a21\u578b\u65b9\u9762\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "GAPNet \u5728\u56fe\u50cf\u548c\u89c6\u9891\u663e\u8457\u76ee\u6807\u68c0\u6d4b\uff08SOD\uff09\u65b9\u9762\u53d6\u5f97\u4e86\u65b0\u7684\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u662f\u4e00\u79cd\u8f7b\u91cf\u7ea7\u6a21\u578b\u3002"}}
{"id": "2508.08126", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08126", "abs": "https://arxiv.org/abs/2508.08126", "authors": ["Hadi Khorsand", "Vahid Pourahmadi"], "title": "OFAL: An Oracle-Free Active Learning Framework", "comment": null, "summary": "In the active learning paradigm, using an oracle to label data has always\nbeen a complex and expensive task, and with the emersion of large unlabeled\ndata pools, it would be highly beneficial If we could achieve better results\nwithout relying on an oracle. This research introduces OFAL, an oracle-free\nactive learning scheme that utilizes neural network uncertainty. OFAL uses the\nmodel's own uncertainty to transform highly confident unlabeled samples into\ninformative uncertain samples. First, we start with separating and quantifying\ndifferent parts of uncertainty and introduce Monte Carlo Dropouts as an\napproximation of the Bayesian Neural Network model. Secondly, by adding a\nvariational autoencoder, we go on to generate new uncertain samples by stepping\ntoward the uncertain part of latent space starting from a confidence seed\nsample. By generating these new informative samples, we can perform active\nlearning and enhance the model's accuracy. Lastly, we try to compare and\nintegrate our method with other widely used active learning sampling methods.", "AI": {"tldr": "OFAL\u662f\u4e00\u79cd\u4e3b\u52a8\u5b66\u4e60\u65b9\u6cd5\uff0c\u5b83\u4e0d\u4f9d\u8d56\u795e\u8c15\uff0c\u800c\u662f\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u6765\u751f\u6210\u65b0\u7684\u4e0d\u786e\u5b9a\u6837\u672c\uff0c\u4ece\u800c\u63d0\u9ad8\u6a21\u578b\u51c6\u786e\u6027\u3002", "motivation": "\u5728\u4e3b\u52a8\u5b66\u4e60\u8303\u5f0f\u4e2d\uff0c\u4f7f\u7528\u795e\u8c15\u6765\u6807\u8bb0\u6570\u636e\u662f\u4e00\u9879\u590d\u6742\u4e14\u6602\u8d35 Thus, has always been a complex and expensive task. With the emergence of large unlabeled data pools, it would be highly beneficial if we could achieve better results without relying on an oracle.", "method": "OFAL\u662f\u4e00\u79cd\u65e0\u795e\u8c15\u7684\u4e3b\u52a8\u5b66\u4e60\u65b9\u6848\uff0c\u5b83\u5229\u7528\u795e\u7ecf\u7f51\u7edc\u7684\u4e0d\u786e\u5b9a\u6027\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u5b83\u9996\u5148\u5206\u79bb\u548c\u91cf\u5316\u4e0d\u786e\u5b9a\u6027\u7684\u4e0d\u540c\u90e8\u5206\uff0c\u5e76\u5f15\u5165\u8499\u7279\u5361\u6d1bDropout\u4f5c\u4e3a\u8d1d\u53f6\u65af\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u8fd1\u4f3c\u3002\u7136\u540e\uff0c\u901a\u8fc7\u6dfb\u52a0\u53d8\u5206\u81ea\u7f16\u7801\u5668\uff0c\u4ece\u7f6e\u4fe1\u79cd\u5b50\u6837\u672c\u5f00\u59cb\uff0c\u5728\u6f5c\u5728\u7a7a\u95f4\u7684\u4e0d\u786e\u5b9a\u90e8\u5206\u8fdb\u884c\u91c7\u6837\uff0c\u751f\u6210\u65b0\u7684\u4e0d\u786e\u5b9a\u6837\u672c\u3002", "result": "OFAL\u5229\u7528\u6a21\u578b\u81ea\u8eab\u7684\u4e0d\u786e\u5b9a\u6027\uff0c\u5c06\u9ad8\u7f6e\u4fe1\u5ea6\u7684\u672a\u6807\u8bb0\u6837\u672c\u8f6c\u5316\u4e3a\u4fe1\u606f\u4e0d\u786e\u5b9a\u7684\u6837\u672c\uff0c\u4ece\u800c\u63d0\u5347\u6a21\u578b\u7684\u51c6\u786e\u6027\u3002", "conclusion": "OFAL\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u65b0\u7684\u4e0d\u786e\u5b9a\u6837\u672c\u6765\u63d0\u5347\u6a21\u578b\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u53ef\u4ee5\u4e0e\u73b0\u6709\u7684\u4e3b\u52a8\u5b66\u4e60\u91c7\u6837\u65b9\u6cd5\u76f8\u7ed3\u5408\u3002"}}
{"id": "2508.07587", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.07587", "abs": "https://arxiv.org/abs/2508.07587", "authors": ["Sri Raksha Siva", "Nived Suthahar", "Prakash Boominathan", "Uma Ranjan"], "title": "Voice Pathology Detection Using Phonation", "comment": "17 Pages, 11 Figures", "summary": "Voice disorders significantly affect communication and quality of life,\nrequiring an early and accurate diagnosis. Traditional methods like\nlaryngoscopy are invasive, subjective, and often inaccessible. This research\nproposes a noninvasive, machine learning-based framework for detecting voice\npathologies using phonation data.\n  Phonation data from the Saarbr\\\"ucken Voice Database are analyzed using\nacoustic features such as Mel Frequency Cepstral Coefficients (MFCCs), chroma\nfeatures, and Mel spectrograms. Recurrent Neural Networks (RNNs), including\nLSTM and attention mechanisms, classify samples into normal and pathological\ncategories. Data augmentation techniques, including pitch shifting and Gaussian\nnoise addition, enhance model generalizability, while preprocessing ensures\nsignal quality. Scale-based features, such as H\\\"older and Hurst exponents,\nfurther capture signal irregularities and long-term dependencies.\n  The proposed framework offers a noninvasive, automated diagnostic tool for\nearly detection of voice pathologies, supporting AI-driven healthcare, and\nimproving patient outcomes.", "AI": {"tldr": "\u4e00\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u673a\u5668\u5b66\u4e60\u548c\u53d1\u58f0\u6570\u636e\u6765\u65e0\u521b\u68c0\u6d4b\u8bed\u97f3\u75be\u75c5\u7684\u65b0\u65b9\u6cd5\uff0c\u65e8\u5728\u5b9e\u73b0\u65e9\u671f\u8bca\u65ad\u548c\u6539\u5584\u60a3\u8005\u6cbb\u7597\u6548\u679c\u3002", "motivation": "\u8bed\u97f3\u969c\u788d\u4e25\u91cd\u5f71\u54cd\u6c9f\u901a\u548c\u751f\u6d3b\u8d28\u91cf\uff0c\u9700\u8981\u65e9\u671f\u51c6\u786e\u7684\u8bca\u65ad\u3002\u4f20\u7edf\u7684\u5589\u955c\u68c0\u67e5\u7b49\u65b9\u6cd5\u5177\u6709\u4fb5\u5165\u6027\u3001\u4e3b\u89c2\u6027\u4e14\u4e0d\u6613\u83b7\u5f97\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u7684\u65e0\u521b\u6846\u67b6\uff0c\u5229\u7528\u53d1\u58f0\u6570\u636e\u6765\u68c0\u6d4b\u8bed\u97f3\u75c5\u7406\u3002\u5206\u6790\u4e86\u6765\u81eaSaarbr\u00fccken\u8bed\u97f3\u6570\u636e\u5e93\u7684\u53d1\u58f0\u6570\u636e\uff0c\u4f7f\u7528\u4e86\u5305\u62ecMFCCs\u3001\u8272\u5ea6\u7279\u5f81\u548cMel\u9891\u8c31\u56fe\u5728\u5185\u7684\u58f0\u5b66\u7279\u5f81\u3002\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u5305\u62ecLSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u6837\u672c\u5206\u7c7b\u4e3a\u6b63\u5e38\u548c\u75c5\u7406\u7c7b\u522b\u3002\u6570\u636e\u589e\u5f3a\u6280\u672f\uff08\u5305\u62ec\u97f3\u9ad8\u504f\u79fb\u548c\u9ad8\u65af\u566a\u58f0\u6dfb\u52a0\uff09\u63d0\u9ad8\u4e86\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u800c\u9884\u5904\u7406\u786e\u4fdd\u4e86\u4fe1\u53f7\u8d28\u91cf\u3002\u57fa\u4e8e\u5c3a\u5ea6\u7684\u7279\u5f81\uff0c\u5982H\u00f6lder\u548cHurst\u6307\u6570\uff0c\u8fdb\u4e00\u6b65\u6355\u6349\u4e86\u4fe1\u53f7\u7684\u4e0d\u89c4\u5219\u6027\u548c\u957f\u671f\u4f9d\u8d56\u6027\u3002", "result": "\u4f7f\u7528\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff08RNN\uff09\uff0c\u5305\u62ecLSTM\u548c\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5c06\u6837\u672c\u5206\u7c7b\u4e3a\u6b63\u5e38\u548c\u75c5\u7406\u7c7b\u522b\u3002", "conclusion": "\u8be5\u6846\u67b6\u63d0\u4f9b\u4e86\u4e00\u79cd\u975e\u4fb5\u5165\u6027\u7684\u3001\u81ea\u52a8\u5316\u7684\u8bca\u65ad\u5de5\u5177\uff0c\u7528\u4e8e\u65e9\u671f\u68c0\u6d4b\u8bed\u97f3\u75be\u75c5\uff0c\u652f\u6301\u4eba\u5de5\u667a\u80fd\u9a71\u52a8\u7684\u533b\u7597\u4fdd\u5065\uff0c\u5e76\u6539\u5584\u60a3\u8005\u7684\u6cbb\u7597\u6548\u679c\u3002"}}
{"id": "2508.07596", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07596", "abs": "https://arxiv.org/abs/2508.07596", "authors": ["Shahroz Tariq", "Simon S. Woo", "Priyanka Singh", "Irena Irmalasari", "Saakshi Gupta", "Dev Gupta"], "title": "From Prediction to Explanation: Multimodal, Explainable, and Interactive Deepfake Detection Framework for Non-Expert Users", "comment": "11 pages, 3 tables, 5 figures, accepted for publicaiton in the 33rd\n  ACM International Conference on Multimedia (MM '25), October 27-31, 2025,\n  Dublin, Ireland", "summary": "The proliferation of deepfake technologies poses urgent challenges and\nserious risks to digital integrity, particularly within critical sectors such\nas forensics, journalism, and the legal system. While existing detection\nsystems have made significant progress in classification accuracy, they\ntypically function as black-box models, offering limited transparency and\nminimal support for human reasoning. This lack of interpretability hinders\ntheir usability in real-world decision-making contexts, especially for\nnon-expert users. In this paper, we present DF-P2E (Deepfake: Prediction to\nExplanation), a novel multimodal framework that integrates visual, semantic,\nand narrative layers of explanation to make deepfake detection interpretable\nand accessible. The framework consists of three modular components: (1) a\ndeepfake classifier with Grad-CAM-based saliency visualisation, (2) a visual\ncaptioning module that generates natural language summaries of manipulated\nregions, and (3) a narrative refinement module that uses a fine-tuned Large\nLanguage Model (LLM) to produce context-aware, user-sensitive explanations. We\ninstantiate and evaluate the framework on the DF40 benchmark, the most diverse\ndeepfake dataset to date. Experiments demonstrate that our system achieves\ncompetitive detection performance while providing high-quality explanations\naligned with Grad-CAM activations. By unifying prediction and explanation in a\ncoherent, human-aligned pipeline, this work offers a scalable approach to\ninterpretable deepfake detection, advancing the broader vision of trustworthy\nand transparent AI systems in adversarial media environments.", "AI": {"tldr": " DF-P2E \u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u591a\u6a21\u6001\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u53d9\u4e8b\u89e3\u91ca\u4f7f\u68c0\u6d4b\u8fc7\u7a0b\u53d8\u5f97\u900f\u660e\u548c\u6613\u4e8e\u8bbf\u95ee\uff0c\u4ece\u800c\u89e3\u51b3\u4e86\u73b0\u6709\u9ed1\u7bb1\u68c0\u6d4b\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": " \u73b0\u6709\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u7cfb\u7edf\u901a\u5e38\u662f\u9ed1\u7bb1\u6a21\u578b\uff0c\u7f3a\u4e4f\u900f\u660e\u5ea6\uff0c\u5bf9\u4eba\u7c7b\u63a8\u7406\u7684\u652f\u6301\u6709\u9650\uff0c\u8fd9\u5728\u73b0\u5b9e\u4e16\u754c\u7684\u51b3\u7b56\u73af\u5883\u4e2d\uff0c\u5c24\u5176\u662f\u5728\u6cd5\u533b\u5b66\u3001\u65b0\u95fb\u548c\u6cd5\u5f8b\u7cfb\u7edf\u7b49\u5173\u952e\u9886\u57df\uff0c\u963b\u788d\u4e86\u5b83\u4eec\u7684\u4f7f\u7528\uff0c\u5e76\u4e3a\u975e\u4e13\u4e1a\u7528\u6237\u5e26\u6765\u4e86\u6311\u6218\u3002", "method": " DF-P2E \u6846\u67b6\u5305\u542b\u4e09\u4e2a\u6a21\u5757\uff1a(1) \u57fa\u4e8e Grad-CAM \u7684\u663e\u8457\u6027\u53ef\u89c6\u5316\u6df1\u5ea6\u4f2a\u9020\u5206\u7c7b\u5668\uff0c(2) \u751f\u6210\u5173\u4e8e\u88ab\u64cd\u7eb5\u533a\u57df\u7684\u81ea\u7136\u8bed\u8a00\u6458\u8981\u7684\u89c6\u89c9\u5b57\u5e55\u6a21\u5757\uff0c(3) \u4f7f\u7528\u7ecf\u8fc7\u5fae\u8c03\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b (LLM) \u6765\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u3001\u7528\u6237\u654f\u611f\u7684\u89e3\u91ca\u7684\u53d9\u4e8b\u7ec6\u5316\u6a21\u5757\u3002", "result": " DF-P2E \u5728 DF40 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6709\u7ade\u4e89\u529b\u7684\u68c0\u6d4b\u6027\u80fd\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e0e Grad-CAM \u6fc0\u6d3b\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u89e3\u91ca\u3002", "conclusion": " DF-P2E \u901a\u8fc7\u6574\u5408\u89c6\u89c9\u3001\u8bed\u4e49\u548c\u53d9\u4e8b\u5c42\u9762\u7684\u89e3\u91ca\uff0c\u5b9e\u73b0\u4e86\u53ef\u89e3\u91ca\u548c\u6613\u4e8e\u7406\u89e3\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u3002\u8be5\u6846\u67b6\u5728 DF40 \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u51fa\u4e0e Grad-CAM \u6fc0\u6d3b\u4e00\u81f4\u7684\u9ad8\u8d28\u91cf\u89e3\u91ca\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u6709\u7ade\u4e89\u529b\u7684\u68c0\u6d4b\u6027\u80fd\u3002\u901a\u8fc7\u5c06\u9884\u6d4b\u548c\u89e3\u91ca\u7edf\u4e00\u5728\u8fde\u8d2f\u3001\u4ee5\u4eba\u4e3a\u672c\u7684\u6d41\u7a0b\u4e2d\uff0c\u8be5\u5de5\u4f5c\u4e3a\u53ef\u89e3\u91ca\u7684\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u53ef\u6269\u5c55\u7684\u65b9\u6cd5\uff0c\u5e76\u4fc3\u8fdb\u4e86\u5728\u5bf9\u6297\u6027\u5a92\u4f53\u73af\u5883\u4e2d\u503c\u5f97\u4fe1\u8d56\u548c\u900f\u660e\u7684 AI \u7cfb\u7edf\u7684\u66f4\u5e7f\u6cdb\u613f\u666f\u3002"}}
{"id": "2508.08151", "categories": ["cs.LG", "cs.SE"], "pdf": "https://arxiv.org/pdf/2508.08151", "abs": "https://arxiv.org/abs/2508.08151", "authors": ["Moses Openja", "Paolo Arcaini", "Foutse Khomh", "Fuyuki Ishikawa"], "title": "FairFLRep: Fairness aware fault localization and repair of Deep Neural Networks", "comment": null, "summary": "Deep neural networks (DNNs) are being utilized in various aspects of our\ndaily lives, including high-stakes decision-making applications that impact\nindividuals. However, these systems reflect and amplify bias from the data used\nduring training and testing, potentially resulting in biased behavior and\ninaccurate decisions. For instance, having different misclassification rates\nbetween white and black sub-populations. However, effectively and efficiently\nidentifying and correcting biased behavior in DNNs is a challenge. This paper\nintroduces FairFLRep, an automated fairness-aware fault localization and repair\ntechnique that identifies and corrects potentially bias-inducing neurons in DNN\nclassifiers. FairFLRep focuses on adjusting neuron weights associated with\nsensitive attributes, such as race or gender, that contribute to unfair\ndecisions. By analyzing the input-output relationships within the network,\nFairFLRep corrects neurons responsible for disparities in predictive quality\nparity. We evaluate FairFLRep on four image classification datasets using two\nDNN classifiers, and four tabular datasets with a DNN model. The results show\nthat FairFLRep consistently outperforms existing methods in improving fairness\nwhile preserving accuracy. An ablation study confirms the importance of\nconsidering fairness during both fault localization and repair stages. Our\nfindings also show that FairFLRep is more efficient than the baseline\napproaches in repairing the network.", "AI": {"tldr": "FairFLRep \u662f\u4e00\u79cd\u65b0\u7684\u6280\u672f\uff0c\u53ef\u4ee5\u5e2e\u52a9\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u7cfb\u7edf\u5728\u51b3\u7b56\u65f6\u66f4\u52a0\u516c\u5e73\uff0c\u5373\u4f7f\u5728\u5904\u7406\u4e0d\u540c\u4eba\u7fa4\u7684\u6570\u636e\u65f6\u4e5f\u662f\u5982\u6b64\u3002\u5b83\u901a\u8fc7\u8bc6\u522b\u548c\u8c03\u6574 AI \u5185\u90e8\u53ef\u80fd\u5bfc\u81f4\u4e0d\u516c\u5e73\u884c\u4e3a\u7684\u90e8\u5206\u6765\u5de5\u4f5c\u3002", "motivation": "DNN \u5728\u65e5\u5e38\u751f\u6d3b\u4e2d\u5f97\u5230\u5e7f\u6cdb\u5e94\u7528\uff0c\u4f46\u4e5f\u5b58\u5728\u504f\u89c1\u95ee\u9898\uff0c\u4f8b\u5982\u5728\u4e0d\u540c\u4eba\u7fa4\u4e2d\u7684\u9519\u8bef\u5206\u7c7b\u7387\u4e0d\u540c\uff0c\u800c\u6709\u6548\u8bc6\u522b\u548c\u7ea0\u6b63 DNN \u4e2d\u7684\u504f\u89c1\u884c\u4e3a\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\u3002", "method": "FairFLRep \u662f\u4e00\u79cd\u81ea\u52a8\u5316\u7684\u3001\u516c\u5e73\u611f\u77e5\u7684\u6545\u969c\u5b9a\u4f4d\u548c\u4fee\u590d\u6280\u672f\uff0c\u901a\u8fc7\u8c03\u6574\u4e0e\u6027\u522b\u6216\u79cd\u65cf\u7b49\u654f\u611f\u5c5e\u6027\u76f8\u5173\u7684\u795e\u7ecf\u5143\u6743\u91cd\u6765\u8bc6\u522b\u548c\u7ea0\u6b63 DNN \u5206\u7c7b\u5668\u4e2d\u53ef\u80fd\u5f15\u8d77\u504f\u89c1\u7684\u795e\u7ecf\u5143\u3002", "result": "FairFLRep \u5728\u56db\u4e2a\u56fe\u50cf\u5206\u7c7b\u6570\u636e\u96c6\u548c\u56db\u4e2a\u8868\u683c\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\uff0c\u7ed3\u679c\u8868\u660e FairFLRep \u5728\u63d0\u9ad8\u516c\u5e73\u6027\u65b9\u9762\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\u3002\u800c\u5bf9 FairFLRep \u8fdb\u884c\u7684\u6d88\u878d\u7814\u7a76\u8bc1\u5b9e\u4e86\u5728\u6545\u969c\u5b9a\u4f4d\u548c\u4fee\u590d\u9636\u6bb5\u90fd\u8003\u8651\u516c\u5e73\u6027\u7684\u91cd\u8981\u6027\u3002\u7814\u7a76\u8fd8\u8868\u660e FairFLRep \u5728\u4fee\u590d\u7f51\u7edc\u65b9\u9762\u6bd4\u57fa\u7ebf\u65b9\u6cd5\u66f4\u6709\u6548\u7387\u3002", "conclusion": "FairFLRep \u80fd\u591f\u63d0\u9ad8 DNN \u5206\u7c7b\u5668\u7684\u516c\u5e73\u6027\uff0c\u540c\u65f6\u4fdd\u6301\u51c6\u786e\u6027\uff0c\u5e76\u4e14\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6709\u6548\u7387\u3002"}}
{"id": "2508.07597", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07597", "abs": "https://arxiv.org/abs/2508.07597", "authors": ["Yuang Zhang", "Junqi Cheng", "Haoyu Zhao", "Jiaxi Gu", "Fangyuan Zou", "Zenghui Lu", "Peng Shu"], "title": "ShoulderShot: Generating Over-the-Shoulder Dialogue Videos", "comment": null, "summary": "Over-the-shoulder dialogue videos are essential in films, short dramas, and\nadvertisements, providing visual variety and enhancing viewers' emotional\nconnection. Despite their importance, such dialogue scenes remain largely\nunderexplored in video generation research. The main challenges include\nmaintaining character consistency across different shots, creating a sense of\nspatial continuity, and generating long, multi-turn dialogues within limited\ncomputational budgets. Here, we present ShoulderShot, a framework that combines\ndual-shot generation with looping video, enabling extended dialogues while\npreserving character consistency. Our results demonstrate capabilities that\nsurpass existing methods in terms of shot-reverse-shot layout, spatial\ncontinuity, and flexibility in dialogue length, thereby opening up new\npossibilities for practical dialogue video generation. Videos and comparisons\nare available at https://shouldershot.github.io.", "AI": {"tldr": "ShoulderShot\u6846\u67b6\u80fd\u751f\u6210\u957f\u5bf9\u8bdd\u89c6\u9891\uff0c\u4fdd\u6301\u89d2\u8272\u548c\u7a7a\u95f4\u4e00\u81f4\u6027\uff0c\u6548\u679c\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u3002", "motivation": "\u5bf9\u8bdd\u89c6\u9891\u5728\u5f71\u89c6\u548c\u5e7f\u544a\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u89c6\u9891\u751f\u6210\u7814\u7a76\u5bf9\u8fd9\u7c7b\u5305\u542b\u89d2\u8272\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u591a\u8f6e\u5bf9\u8bdd\u7684\u573a\u666f\u63a2\u7d22\u4e0d\u8db3\uff0c\u5e76\u4e14\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u751f\u6210\u957f\u5bf9\u8bdd\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aShoulderShot\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u7ed3\u5408\u4e86\u53cc\u955c\u5934\u751f\u6210\u548c\u5faa\u73af\u89c6\u9891\u6280\u672f\uff0c\u4ee5\u5b9e\u73b0\u957f\u65f6\u95f4\u3001\u591a\u8f6e\u6b21\u7684\u5bf9\u8bdd\u89c6\u9891\u751f\u6210\uff0c\u540c\u65f6\u4fdd\u6301\u89d2\u8272\u4e00\u81f4\u6027\u3002", "result": "\u7ed3\u679c\u8868\u660e\uff0cShoulderShot\u5728\u955c\u5934\u53cd\u62cd\u5e03\u5c40\u3001\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u5bf9\u8bdd\u957f\u5ea6\u7075\u6d3b\u6027\u65b9\u9762\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u5bf9\u8bdd\u89c6\u9891\u7684\u5b9e\u9645\u751f\u6210\u5f00\u8f9f\u4e86\u65b0\u7684\u53ef\u80fd\u6027\u3002", "conclusion": "ShoulderShot\u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u53cc\u955c\u5934\u751f\u6210\u548c\u5faa\u73af\u89c6\u9891\uff0c\u5b9e\u73b0\u4e86\u5728\u6709\u9650\u8ba1\u7b97\u9884\u7b97\u4e0b\u751f\u6210\u5177\u6709\u89d2\u8272\u4e00\u81f4\u6027\u3001\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u591a\u8f6e\u5bf9\u8bdd\u80fd\u529b\u7684\u5bf9\u8bdd\u89c6\u9891\uff0c\u5e76\u5728\u955c\u5934\u53cd\u62cd\u5e03\u5c40\u3001\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u5bf9\u8bdd\u957f\u5ea6\u7075\u6d3b\u6027\u65b9\u9762\u8d85\u8d8a\u4e86\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.08159", "categories": ["cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08159", "abs": "https://arxiv.org/abs/2508.08159", "authors": ["Cem Ata Baykara", "Saurav Raj Pandey", "Ali Burak \u00dcnal", "Harlin Lee", "Mete Akg\u00fcn"], "title": "Federated Learning for Epileptic Seizure Prediction Across Heterogeneous EEG Datasets", "comment": null, "summary": "Developing accurate and generalizable epileptic seizure prediction models\nfrom electroencephalography (EEG) data across multiple clinical sites is\nhindered by patient privacy regulations and significant data heterogeneity\n(non-IID characteristics). Federated Learning (FL) offers a privacy-preserving\nframework for collaborative training, but standard aggregation methods like\nFederated Averaging (FedAvg) can be biased by dominant datasets in\nheterogeneous settings. This paper investigates FL for seizure prediction using\na single EEG channel across four diverse public datasets (Siena, CHB-MIT,\nHelsinki, NCH), representing distinct patient populations (adult, pediatric,\nneonate) and recording conditions. We implement privacy-preserving global\nnormalization and propose a Random Subset Aggregation strategy, where each\nclient trains on a fixed-size random subset of its data per round, ensuring\nequal contribution during aggregation. Our results show that locally trained\nmodels fail to generalize across sites, and standard weighted FedAvg yields\nhighly skewed performance (e.g., 89.0% accuracy on CHB-MIT but only 50.8% on\nHelsinki and 50.6% on NCH). In contrast, Random Subset Aggregation\nsignificantly improves performance on under-represented clients (accuracy\nincreases to 81.7% on Helsinki and 68.7% on NCH) and achieves a superior\nmacro-average accuracy of 77.1% and pooled accuracy of 80.0% across all sites,\ndemonstrating a more robust and fair global model. This work highlights the\npotential of balanced FL approaches for building effective and generalizable\nseizure prediction systems in realistic, heterogeneous multi-hospital\nenvironments while respecting data privacy.", "AI": {"tldr": "\u4e3a\u89e3\u51b3\u8de8\u591a\u4e2d\u5fc3EEG\u6570\u636e\u8bad\u7ec3\u766b\u75eb\u9884\u6d4b\u6a21\u578b\u7684\u9690\u79c1\u548c\u6570\u636e\u5f02\u6784\u6027\u95ee\u9898\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u5b50\u96c6\u805a\u5408\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u6570\u636e\u96c6\u4e0a\u7684\u6a21\u578b\u6027\u80fd\u548c\u6cdb\u5316\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u66f4\u9c81\u68d2\u548c\u516c\u5e73\u7684\u5168\u5c40\u6a21\u578b\u3002", "motivation": "\u5f00\u53d1\u51c6\u786e\u4e14\u53ef\u6cdb\u5316\u7684\u766b\u75eb\u53d1\u4f5c\u9884\u6d4b\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u8de8\u591a\u4e2a\u4e34\u5e8a\u7ad9\u70b9\u5bf9\u8111\u7535\u56fe\uff08EEG\uff09\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u4f46\u9762\u4e34\u60a3\u8005\u9690\u79c1\u6cd5\u89c4\u548c\u6570\u636e\u5f02\u6784\u6027\uff08\u975e\u72ec\u7acb\u540c\u5206\u5e03\u7279\u5f81\uff09\u7684\u6311\u6218\u3002\u6807\u51c6\u7684\u8054\u90a6\u5b66\u4e60\u805a\u5408\u65b9\u6cd5\uff08\u5982\u8054\u90a6\u5e73\u5747\u6cd5\uff09\u53ef\u80fd\u56e0\u5f02\u6784\u73af\u5883\u4e2d\u5360\u4e3b\u5bfc\u5730\u4f4d\u7684\u6570\u636e\u96c6\u800c\u4ea7\u751f\u504f\u5dee\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u968f\u673a\u5b50\u96c6\u805a\u5408\u7b56\u7565\uff0c\u5176\u4e2d\u6bcf\u4e2a\u5ba2\u6237\u7aef\u5728\u6bcf\u8f6e\u8bad\u7ec3\u4e2d\u4f7f\u7528\u56fa\u5b9a\u5927\u5c0f\u7684\u968f\u673a\u6570\u636e\u5b50\u96c6\uff0c\u4ee5\u786e\u4fdd\u805a\u5408\u8fc7\u7a0b\u4e2d\u7684\u76f8\u7b49\u8d21\u732e\uff0c\u5e76\u5b9e\u73b0\u4e86\u9690\u79c1\u4fdd\u62a4\u7684\u5168\u5c40\u5f52\u4e00\u5316\u3002", "result": "\u4e0e\u4ec5\u5728\u672c\u5730\u8bad\u7ec3\u7684\u6a21\u578b\u65e0\u6cd5\u6cdb\u5316\u5230\u5176\u4ed6\u7ad9\u70b9\uff0c\u4ee5\u53ca\u6807\u51c6\u7684\u52a0\u6743\u8054\u90a6\u5e73\u5747\u6cd5\u4ea7\u751f\u9ad8\u5ea6\u503e\u659c\u7684\u6027\u80fd\uff08\u5728CHB-MIT\u4e0a\u51c6\u786e\u7387\u4e3a89.0%\uff0c\u4f46\u5728Helsinki\u4e0a\u4ec5\u4e3a50.8%\uff0c\u5728NCH\u4e0a\u4e3a50.6%\uff09\u76f8\u6bd4\uff0c\u968f\u673a\u5b50\u96c6\u805a\u5408\u7b56\u7565\u663e\u8457\u63d0\u9ad8\u4e86\u4ee3\u8868\u6027\u4e0d\u8db3\u7684\u5ba2\u6237\u7aef\u7684\u6027\u80fd\uff08Helsinki\u7684\u51c6\u786e\u7387\u63d0\u9ad8\u523081.7%\uff0cNCH\u63d0\u9ad8\u523068.7%\uff09\uff0c\u5e76\u5728\u6240\u6709\u7ad9\u70b9\u4e0a\u5b9e\u73b0\u4e86\u66f4\u9ad8\u7684\u5b8f\u89c2\u5e73\u5747\u51c6\u786e\u7387\uff0877.1%\uff09\u548c\u6c47\u603b\u51c6\u786e\u7387\uff0880.0%\uff09\uff0c\u5c55\u793a\u4e86\u4e00\u4e2a\u66f4\u9c81\u68d2\u548c\u516c\u5e73\u7684\u5168\u5c40\u6a21\u578b\u3002", "conclusion": "\u901a\u8fc7\u91c7\u7528\u5e73\u8861\u7684\u8054\u90a6\u5b66\u4e60\u65b9\u6cd5\uff0c\u53ef\u4ee5\u6784\u5efa\u6709\u6548\u4e14\u53ef\u6cdb\u5316\u7684\u766b\u75eb\u9884\u6d4b\u7cfb\u7edf\uff0c\u4ee5\u5e94\u5bf9\u73b0\u5b9e\u4e2d\u5f02\u6784\u7684\u591a\u533b\u9662\u73af\u5883\uff0c\u540c\u65f6\u5c0a\u91cd\u6570\u636e\u9690\u79c1\u3002"}}
{"id": "2508.07603", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07603", "abs": "https://arxiv.org/abs/2508.07603", "authors": ["Wenhui Song", "Hanhui Li", "Jiehui Huang", "Panwen Hu", "Yuhao Cheng", "Long Chen", "Yiqiang Yan", "Xiaodan Liang"], "title": "LaVieID: Local Autoregressive Diffusion Transformers for Identity-Preserving Video Creation", "comment": "Accepted to ACM MM 2025", "summary": "In this paper, we present LaVieID, a novel \\underline{l}ocal\n\\underline{a}utoregressive \\underline{vi}d\\underline{e}o diffusion framework\ndesigned to tackle the challenging \\underline{id}entity-preserving\ntext-to-video task. The key idea of LaVieID is to mitigate the loss of identity\ninformation inherent in the stochastic global generation process of diffusion\ntransformers (DiTs) from both spatial and temporal perspectives. Specifically,\nunlike the global and unstructured modeling of facial latent states in existing\nDiTs, LaVieID introduces a local router to explicitly represent latent states\nby weighted combinations of fine-grained local facial structures. This\nalleviates undesirable feature interference and encourages DiTs to capture\ndistinctive facial characteristics. Furthermore, a temporal autoregressive\nmodule is integrated into LaVieID to refine denoised latent tokens before video\ndecoding. This module divides latent tokens temporally into chunks, exploiting\ntheir long-range temporal dependencies to predict biases for rectifying tokens,\nthereby significantly enhancing inter-frame identity consistency. Consequently,\nLaVieID can generate high-fidelity personalized videos and achieve\nstate-of-the-art performance. Our code and models are available at\nhttps://github.com/ssugarwh/LaVieID.", "AI": {"tldr": "LaVieID is a framework for identity-preserving text-to-video generation that uses a local router and a temporal autoregressive module to improve identity preservation and enhance inter-frame consistency, achieving state-of-the-art results.", "motivation": "To tackle the challenging identity-preserving text-to-video task and mitigate the loss of identity information inherent in the stochastic global generation process of diffusion transformers (DiTs) from both spatial and temporal perspectives.", "method": "LaVieID introduces a local router to explicitly represent latent states by weighted combinations of fine-grained local facial structures and integrates a temporal autoregressive module to refine denoised latent tokens before video decoding by dividing latent tokens temporally into chunks, exploiting their long-range temporal dependencies to predict biases for rectifying tokens.", "result": "LaVieID alleviates undesirable feature interference and encourages DiTs to capture distinctive facial characteristics, significantly enhancing inter-frame identity consistency.", "conclusion": "LaVieID can generate high-fidelity personalized videos and achieve state-of-the-art performance."}}
{"id": "2508.07607", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07607", "abs": "https://arxiv.org/abs/2508.07607", "authors": ["Jian Ma", "Xujie Zhu", "Zihao Pan", "Qirong Peng", "Xu Guo", "Chen Chen", "Haonan Lu"], "title": "X2Edit: Revisiting Arbitrary-Instruction Image Editing through Self-Constructed Data and Task-Aware Representation Learning", "comment": "https://github.com/OPPO-Mente-Lab/X2Edit", "summary": "Existing open-source datasets for arbitrary-instruction image editing remain\nsuboptimal, while a plug-and-play editing module compatible with\ncommunity-prevalent generative models is notably absent. In this paper, we\nfirst introduce the X2Edit Dataset, a comprehensive dataset covering 14 diverse\nediting tasks, including subject-driven generation. We utilize the\nindustry-leading unified image generation models and expert models to construct\nthe data. Meanwhile, we design reasonable editing instructions with the VLM and\nimplement various scoring mechanisms to filter the data. As a result, we\nconstruct 3.7 million high-quality data with balanced categories. Second, to\nbetter integrate seamlessly with community image generation models, we design\ntask-aware MoE-LoRA training based on FLUX.1, with only 8\\% of the parameters\nof the full model. To further improve the final performance, we utilize the\ninternal representations of the diffusion model and define positive/negative\nsamples based on image editing types to introduce contrastive learning.\nExtensive experiments demonstrate that the model's editing performance is\ncompetitive among many excellent models. Additionally, the constructed dataset\nexhibits substantial advantages over existing open-source datasets. The\nopen-source code, checkpoints, and datasets for X2Edit can be found at the\nfollowing link: https://github.com/OPPO-Mente-Lab/X2Edit.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07618", "categories": ["cs.CV", "68Wxx"], "pdf": "https://arxiv.org/pdf/2508.07618", "abs": "https://arxiv.org/abs/2508.07618", "authors": ["Hyoung Suk Park", "Kiwan Jeon"], "title": "An Iterative Reconstruction Method for Dental Cone-Beam Computed Tomography with a Truncated Field of View", "comment": "8 pages, 2 figures, 2 tables", "summary": "In dental cone-beam computed tomography (CBCT), compact and cost-effective\nsystem designs often use small detectors, resulting in a truncated field of\nview (FOV) that does not fully encompass the patient's head. In iterative\nreconstruction approaches, the discrepancy between the actual projection and\nthe forward projection within the truncated FOV accumulates over iterations,\nleading to significant degradation in the reconstructed image quality. In this\nstudy, we propose a two-stage approach to mitigate truncation artifacts in\ndental CBCT. In the first stage, we employ Implicit Neural Representation\n(INR), leveraging its superior representation power, to generate a prior image\nover an extended region so that its forward projection fully covers the\npatient's head. To reduce computational and memory burdens, INR reconstruction\nis performed with a coarse voxel size. The forward projection of this prior\nimage is then used to estimate the discrepancy due to truncated FOV in the\nmeasured projection data. In the second stage, the discrepancy-corrected\nprojection data is utilized in a conventional iterative reconstruction process\nwithin the truncated region. Our numerical results demonstrate that the\nproposed two-grid approach effectively suppresses truncation artifacts, leading\nto improved CBCT image quality.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\uff0c\u5229\u7528INR\u751f\u6210\u5148\u9a8c\u56fe\u50cf\uff0c\u7ed3\u5408\u5dee\u5f02\u6821\u6b63\u548c\u8fed\u4ee3\u91cd\u5efa\uff0c\u6709\u6548\u89e3\u51b3\u7259\u79d1CBCT\u4e2d\u7684\u622a\u65ad\u4f2a\u5f71\u95ee\u9898\u3002", "motivation": "\u7259\u79d1CBCT\u5e38\u4f7f\u7528\u5c0f\u63a2\u6d4b\u5668\u5bfc\u81f4\u89c6\u91ce\uff08FOV\uff09\u622a\u65ad\uff0c\u5728\u524d\u5411\u8fed\u4ee3\u91cd\u5efa\u4e2d\uff0c\u622a\u65adFOV\u4e2d\u7684\u5b9e\u9645\u6295\u5f71\u4e0e\u524d\u5411\u6295\u5f71\u4e4b\u95f4\u7684\u5dee\u5f02\u4f1a\u7d2f\u79ef\uff0c\u5bfc\u81f4\u91cd\u5efa\u56fe\u50cf\u8d28\u91cf\u4e25\u91cd\u4e0b\u964d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u65b9\u6cd5\u6765\u7f13\u89e3\u7259\u79d1CBCT\u4e2d\u7684\u622a\u65ad\u4f2a\u5f71\u3002\u7b2c\u4e00\u9636\u6bb5\uff0c\u91c7\u7528\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u751f\u6210\u6269\u5c55\u533a\u57df\u7684\u5148\u9a8c\u56fe\u50cf\uff0c\u5e76\u7528\u5176\u524d\u5411\u6295\u5f71\u4f30\u8ba1\u622a\u65adFOV\u5f15\u8d77\u7684\u5dee\u5f02\u3002\u7b2c\u4e8c\u9636\u6bb5\uff0c\u5728\u622a\u65ad\u533a\u57df\u5185\uff0c\u5229\u7528\u5dee\u5f02\u6821\u6b63\u540e\u7684\u6295\u5f71\u6570\u636e\u8fdb\u884c\u5e38\u89c4\u8fed\u4ee3\u91cd\u5efa\u3002", "result": "\u6570\u503c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u4e24\u9636\u6bb5\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u622a\u65ad\u4f2a\u5f71\uff0c\u63d0\u9ad8CBCT\u56fe\u50cf\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u80fd\u6709\u6548\u6291\u5236\u622a\u65ad\u4f2a\u5f71\uff0c\u63d0\u9ad8CBCT\u56fe\u50cf\u8d28\u91cf\u3002"}}
{"id": "2508.07621", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07621", "abs": "https://arxiv.org/abs/2508.07621", "authors": ["Yunsung Chung", "Chanho Lim", "Ghassan Bidaoui", "Christian Massad", "Nassir Marrouche", "Jihun Hamm"], "title": "SOFA: Deep Learning Framework for Simulating and Optimizing Atrial Fibrillation Ablation", "comment": "Accepted at MICCAI 2025. This is the author's original preprint", "summary": "Atrial fibrillation (AF) is a prevalent cardiac arrhythmia often treated with\ncatheter ablation procedures, but procedural outcomes are highly variable.\nEvaluating and improving ablation efficacy is challenging due to the complex\ninteraction between patient-specific tissue and procedural factors. This paper\nasks two questions: Can AF recurrence be predicted by simulating the effects of\nprocedural parameters? How should we ablate to reduce AF recurrence? We propose\nSOFA (Simulating and Optimizing Atrial Fibrillation Ablation), a novel\ndeep-learning framework that addresses these questions. SOFA first simulates\nthe outcome of an ablation strategy by generating a post-ablation image\ndepicting scar formation, conditioned on a patient's pre-ablation LGE-MRI and\nthe specific procedural parameters used (e.g., ablation locations, duration,\ntemperature, power, and force). During this simulation, it predicts AF\nrecurrence risk. Critically, SOFA then introduces an optimization scheme that\nrefines these procedural parameters to minimize the predicted risk. Our method\nleverages a multi-modal, multi-view generator that processes 2.5D\nrepresentations of the atrium. Quantitative evaluations show that SOFA\naccurately synthesizes post-ablation images and that our optimization scheme\nleads to a 22.18\\% reduction in the model-predicted recurrence risk. To the\nbest of our knowledge, SOFA is the first framework to integrate the simulation\nof procedural effects, recurrence prediction, and parameter optimization,\noffering a novel tool for personalizing AF ablation.", "AI": {"tldr": "SOFA\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u6a21\u62df\u548c\u4f18\u5316\u624b\u672f\u53c2\u6570\u6765\u9884\u6d4b\u548c\u51cf\u5c11\u623f\u98a4\u6d88\u878d\u540e\u7684\u590d\u53d1\u3002", "motivation": "\u623f\u98a4\u662f\u4e00\u79cd\u5e38\u89c1\u7684\u5fc3\u5f8b\u5931\u5e38\uff0c\u5bfc\u7ba1\u6d88\u878d\u662f\u5e38\u7528\u6cbb\u7597\u65b9\u6cd5\uff0c\u4f46\u6548\u679c\u53d8\u5f02\u6027\u5f88\u5927\u3002\u7531\u4e8e\u60a3\u8005\u7ec4\u7ec7\u548c\u624b\u672f\u56e0\u7d20\u7684\u590d\u6742\u76f8\u4e92\u4f5c\u7528\uff0c\u8bc4\u4f30\u548c\u6539\u5584\u6d88\u878d\u6548\u679c\u5177\u6709\u6311\u6218\u6027\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7\u6a21\u62df\u624b\u672f\u53c2\u6570\u7684\u5f71\u54cd\u6765\u9884\u6d4b\u623f\u98a4\u590d\u53d1\uff0c\u5e76\u4f18\u5316\u624b\u672f\u65b9\u6848\u4ee5\u964d\u4f4e\u590d\u53d1\u7387\u3002", "method": "SOFA\uff08\u6a21\u62df\u548c\u4f18\u5316\u623f\u98a4\u6d88\u878d\uff09\u662f\u4e00\u4e2a\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6\uff0c\u901a\u8fc7\u751f\u6210\u672f\u540e\u75a4\u75d5\u5f62\u6210\u56fe\u50cf\u6765\u6a21\u62df\u6d88\u878d\u6548\u679c\uff0c\u5e76\u9884\u6d4b\u623f\u98a4\u590d\u53d1\u98ce\u9669\u3002\u5b83\u8fd8\u5305\u542b\u4e00\u4e2a\u4f18\u5316\u65b9\u6848\uff0c\u7528\u4e8e\u6539\u8fdb\u624b\u672f\u53c2\u6570\u4ee5\u964d\u4f4e\u590d\u53d1\u98ce\u9669\u3002\u8be5\u65b9\u6cd5\u5229\u7528\u591a\u6a21\u6001\u3001\u591a\u89c6\u56fe\u751f\u6210\u5668\u5904\u7406\u623f\u98a4\u76842.5D\u8868\u793a\u3002", "result": "SOFA\u80fd\u591f\u51c6\u786e\u5408\u6210\u672f\u540e\u56fe\u50cf\uff0c\u5e76\u4e14\u5176\u4f18\u5316\u65b9\u6848\u80fd\u5c06\u6a21\u578b\u9884\u6d4b\u7684\u590d\u53d1\u98ce\u9669\u964d\u4f4e22.18%\u3002", "conclusion": "SOFA \u662f\u9996\u4e2a\u6574\u5408\u4e86\u624b\u672f\u6548\u679c\u6a21\u62df\u3001\u590d\u53d1\u9884\u6d4b\u548c\u53c2\u6570\u4f18\u5316\u7684\u6846\u67b6\uff0c\u4e3a\u4e2a\u6027\u5316\u623f\u98a4\u6d88\u878d\u63d0\u4f9b\u4e86\u65b0\u5de5\u5177\u3002"}}
{"id": "2508.08222", "categories": ["cs.LG", "cs.AI", "cs.IT", "math.IT", "math.OC", "stat.ML"], "pdf": "https://arxiv.org/pdf/2508.08222", "abs": "https://arxiv.org/abs/2508.08222", "authors": ["Tong Yang", "Yu Huang", "Yingbin Liang", "Yuejie Chi"], "title": "Multi-head Transformers Provably Learn Symbolic Multi-step Reasoning via Gradient Descent", "comment": "submitted for consideration of publication in May", "summary": "Transformers have demonstrated remarkable capabilities in multi-step\nreasoning tasks. However, understandings of the underlying mechanisms by which\nthey acquire these abilities through training remain limited, particularly from\na theoretical standpoint. This work investigates how transformers learn to\nsolve symbolic multi-step reasoning problems through chain-of-thought\nprocesses, focusing on path-finding in trees. We analyze two intertwined tasks:\na backward reasoning task, where the model outputs a path from a goal node to\nthe root, and a more complex forward reasoning task, where the model implements\ntwo-stage reasoning by first identifying the goal-to-root path and then\nreversing it to produce the root-to-goal path. Our theoretical analysis,\ngrounded in the dynamics of gradient descent, shows that trained one-layer\ntransformers can provably solve both tasks with generalization guarantees to\nunseen trees. In particular, our multi-phase training dynamics for forward\nreasoning elucidate how different attention heads learn to specialize and\ncoordinate autonomously to solve the two subtasks in a single autoregressive\npath. These results provide a mechanistic explanation of how trained\ntransformers can implement sequential algorithmic procedures. Moreover, they\noffer insights into the emergence of reasoning abilities, suggesting that when\ntasks are structured to take intermediate chain-of-thought steps, even shallow\nmulti-head transformers can effectively solve problems that would otherwise\nrequire deeper architectures.", "AI": {"tldr": "Transformer\u6a21\u578b\u901a\u8fc7\u94fe\u5f0f\u601d\u8003\u5b66\u4e60\u63a8\u7406\uff0c\u5373\u4f7f\u662f\u6d45\u5c42\u6a21\u578b\u4e5f\u80fd\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002", "motivation": "\u63a2\u8ba8Transformer\u6a21\u578b\u5728\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u80fd\u529b\u83b7\u53d6\u7684\u673a\u5236\uff0c\u7279\u522b\u662f\u4ece\u7406\u8bba\u89d2\u5ea6\u89e3\u91ca\u5176\u94fe\u5f0f\u601d\u8003\u8fc7\u7a0b\u3002", "method": "\u901a\u8fc7\u68af\u5ea6\u4e0b\u964d\u7684\u52a8\u529b\u5b66\u5bf9Transformer\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u5176\u5728\u6811\u72b6\u7ed3\u6784\u4e0a\u7684\u8def\u5f84\u5bfb\u627e\u4efb\u52a1\u3002", "result": "\u8bc1\u660e\u4e86\u8bad\u7ec3\u597d\u7684\u5355\u5c42Transformer\u80fd\u591f\u6cdb\u5316\u5230\u672a\u89c1\u8fc7\u7684\u6811\u72b6\u7ed3\u6784\uff0c\u5e76\u89e3\u91ca\u4e86\u591a\u9636\u6bb5\u8bad\u7ec3\u5982\u4f55\u4f7f\u6ce8\u610f\u529b\u5934\u534f\u540c\u5de5\u4f5c\u4ee5\u89e3\u51b3\u524d\u5411\u63a8\u7406\u4efb\u52a1\u3002", "conclusion": "Transformer\u6a21\u578b\u53ef\u4ee5\u901a\u8fc7\u94fe\u5f0f\u601d\u8003\u8fc7\u7a0b\u5b66\u4e60\u7b26\u53f7\u591a\u6b65\u63a8\u7406\uff0c\u5373\u4f7f\u662f\u6d45\u5c42\u6a21\u578b\u4e5f\u80fd\u901a\u8fc7\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002"}}
{"id": "2508.07624", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07624", "abs": "https://arxiv.org/abs/2508.07624", "authors": ["Vishakha Lall", "Yisi Liu"], "title": "Enhancing Egocentric Object Detection in Static Environments using Graph-based Spatial Anomaly Detection and Correction", "comment": null, "summary": "In many real-world applications involving static environments, the spatial\nlayout of objects remains consistent across instances. However,\nstate-of-the-art object detection models often fail to leverage this spatial\nprior, resulting in inconsistent predictions, missed detections, or\nmisclassifications, particularly in cluttered or occluded scenes. In this work,\nwe propose a graph-based post-processing pipeline that explicitly models the\nspatial relationships between objects to correct detection anomalies in\negocentric frames. Using a graph neural network (GNN) trained on manually\nannotated data, our model identifies invalid object class labels and predicts\ncorrected class labels based on their neighbourhood context. We evaluate our\napproach both as a standalone anomaly detection and correction framework and as\na post-processing module for standard object detectors such as YOLOv7 and\nRT-DETR. Experiments demonstrate that incorporating this spatial reasoning\nsignificantly improves detection performance, with mAP@50 gains of up to 4%.\nThis method highlights the potential of leveraging the environment's spatial\nstructure to improve reliability in object detection systems.", "AI": {"tldr": "\u901a\u8fc7\u56fe\u795e\u7ecf\u7f51\u7edc\u5229\u7528\u5bf9\u8c61\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u6765\u7ea0\u6b63\u5bf9\u8c61\u68c0\u6d4b\u7684\u9519\u8bef\uff0c\u5728\u6742\u4e71\u6216\u906e\u6321\u573a\u666f\u4e0b\u6548\u679c\u663e\u8457\uff0cmAP@50\u6700\u9ad8\u53ef\u63d0\u9ad84%\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u5bf9\u8c61\u68c0\u6d4b\u6a21\u578b\u5728\u5229\u7528\u9759\u6001\u73af\u5883\u4e2d\u7684\u7a7a\u95f4\u5148\u9a8c\u77e5\u8bc6\u65b9\u9762\u5b58\u5728\u7684\u4e0d\u8db3\uff0c\u4ece\u800c\u5bfc\u81f4\u5728\u6742\u4e71\u6216\u906e\u6321\u573a\u666f\u4e0b\u51fa\u73b0\u4e0d\u4e00\u81f4\u7684\u9884\u6d4b\u3001\u6f0f\u68c0\u6216\u8bef\u5206\u7c7b\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u57fa\u4e8e\u56fe\u7684\u540e\u5904\u7406\u6d41\u7a0b\uff0c\u4f7f\u7528\u56fe\u795e\u7ecf\u7f51\u7edc\uff08GNN\uff09\u6765\u8bc6\u522b\u65e0\u6548\u7684\u5bf9\u8c61\u7c7b\u522b\u6807\u7b7e\uff0c\u5e76\u6839\u636e\u90bb\u57df\u4e0a\u4e0b\u6587\u9884\u6d4b\u4fee\u6b63\u540e\u7684\u7c7b\u522b\u6807\u7b7e\u3002", "result": "\u5c06\u8be5\u65b9\u6cd5\u4f5c\u4e3a\u72ec\u7acb\u5f02\u5e38\u68c0\u6d4b\u548c\u6821\u6b63\u6846\u67b6\uff0c\u4ee5\u53ca\u4f5c\u4e3aYOLOv7\u548cRT-DETR\u7b49\u6807\u51c6\u5bf9\u8c61\u68c0\u6d4b\u5668\u7684\u540e\u5904\u7406\u6a21\u5757\u8fdb\u884c\u8bc4\u4f30\uff0c\u5b9e\u9a8c\u8bc1\u660e\uff0c\u7ed3\u5408\u7a7a\u95f4\u63a8\u7406\u53ef\u663e\u8457\u63d0\u9ad8\u68c0\u6d4b\u6027\u80fd\uff0cmAP@50\u6700\u9ad8\u53ef\u63d0\u9ad84%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u663e\u5f0f\u5efa\u6a21\u5bf9\u8c61\u4e4b\u95f4\u7684\u7a7a\u95f4\u5173\u7cfb\u6765\u7ea0\u6b63\u68c0\u6d4b\u5f02\u5e38\uff0c\u5e76\u8bc1\u660e\u4e86\u5229\u7528\u73af\u5883\u7a7a\u95f4\u7ed3\u6784\u6765\u63d0\u9ad8\u5bf9\u8c61\u68c0\u6d4b\u7cfb\u7edf\u53ef\u9760\u6027\u7684\u6f5c\u529b\u3002"}}
{"id": "2508.07625", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07625", "abs": "https://arxiv.org/abs/2508.07625", "authors": ["Junxiao Xue", "Xiaozhen Liu", "Jie Wang", "Xuecheng Wu", "Bin Wu"], "title": "A Trustworthy Method for Multimodal Emotion Recognition", "comment": "Accepted for publication in Big Data Mining and Analytics (BDMA),\n  2025", "summary": "Existing emotion recognition methods mainly focus on enhancing performance by\nemploying complex deep models, typically resulting in significantly higher\nmodel complexity. Although effective, it is also crucial to ensure the\nreliability of the final decision, especially for noisy, corrupted and\nout-of-distribution data. To this end, we propose a novel emotion recognition\nmethod called trusted emotion recognition (TER), which utilizes uncertainty\nestimation to calculate the confidence value of predictions. TER combines the\nresults from multiple modalities based on their confidence values to output the\ntrusted predictions. We also provide a new evaluation criterion to assess the\nreliability of predictions. Specifically, we incorporate trusted precision and\ntrusted recall to determine the trusted threshold and formulate the trusted\nAcc. and trusted F1 score to evaluate the model's trusted performance. The\nproposed framework combines the confidence module that accordingly endows the\nmodel with reliability and robustness against possible noise or corruption. The\nextensive experimental results validate the effectiveness of our proposed\nmodel. The TER achieves state-of-the-art performance on the Music-video,\nachieving 82.40% Acc. In terms of trusted performance, TER outperforms other\nmethods on the IEMOCAP and Music-video, achieving trusted F1 scores of 0.7511\nand 0.9035, respectively.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u53ef\u4fe1\u60c5\u611f\u8bc6\u522b\uff08TER\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u63d0\u9ad8\u60c5\u611f\u8bc6\u522b\u5728\u566a\u58f0\u6570\u636e\u4e0b\u7684\u53ef\u9760\u6027\uff0c\u5e76\u5728IEMOCAP\u548cMusic-video\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u591a\u5173\u6ce8\u4e8e\u901a\u8fc7\u590d\u6742\u7684\u6df1\u5ea6\u6a21\u578b\u63d0\u5347\u6027\u80fd\uff0c\u4f46\u5f80\u5f80\u5ffd\u7565\u4e86\u5728\u5904\u7406\u5e26\u566a\u58f0\u3001\u635f\u574f\u6216\u5206\u5e03\u5916\u6570\u636e\u65f6\u6a21\u578b\u7684\u53ef\u9760\u6027\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u672c\u7814\u7a76\u65e8\u5728\u63d0\u51fa\u4e00\u79cd\u65b0\u7684\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\uff0c\u80fd\u591f\u786e\u4fdd\u6700\u7ec8\u51b3\u7b56\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u51fa\u76f8\u5e94\u7684\u8bc4\u4f30\u6807\u51c6\u6765\u8861\u91cf\u8fd9\u79cd\u53ef\u9760\u6027\u3002", "method": "TER\u6846\u67b6\u5229\u7528\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u8ba1\u7b97\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u6839\u636e\u7f6e\u4fe1\u5ea6\u878d\u5408\u591a\u6a21\u6001\u7ed3\u679c\uff0c\u4ece\u800c\u8f93\u51fa\u53ef\u4fe1\u7684\u9884\u6d4b\u3002\u8be5\u6846\u67b6\u8fd8\u5f15\u5165\u4e86\u53ef\u4fe1\u7cbe\u786e\u7387\u3001\u53ef\u4fe1\u53ec\u56de\u7387\u3001\u53ef\u4fe1\u51c6\u786e\u7387\u548c\u53ef\u4fe1F1\u5206\u6570\u7b49\u65b0\u7684\u8bc4\u4f30\u6307\u6807\uff0c\u4ee5\u91cf\u5316\u6a21\u578b\u5728\u566a\u58f0\u6216\u635f\u574f\u6570\u636e\u4e0b\u7684\u53ef\u9760\u6027\u8868\u73b0\u3002", "result": "TER\u6846\u67b6\u5728Music-video\u6570\u636e\u96c6\u4e0a\u8fbe\u5230\u4e8682.40%\u7684\u51c6\u786e\u7387\uff0c\u5e76\u8d85\u8d8a\u4e86\u5176\u4ed6\u65b9\u6cd5\uff0c\u5728IEMOCAP\u548cMusic-video\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f97\u4e860.7511\u548c0.9035\u7684\u53ef\u4fe1F1\u5206\u6570\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u60c5\u611f\u8bc6\u522b\u548c\u53ef\u9760\u6027\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u73b0\u6709\u7684\u591a\u6a21\u6001\u60c5\u611f\u8bc6\u522b\u65b9\u6cd5\u5728\u53ef\u9760\u6027\u65b9\u9762\u4ecd\u6709\u63d0\u5347\u7a7a\u95f4\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u566a\u58f0\u3001\u635f\u574f\u548c\u5206\u5e03\u5916\u6570\u636e\u65f6\u3002\u672c\u7814\u7a76\u63d0\u51fa\u7684\u53ef\u4fe1\u60c5\u611f\u8bc6\u522b\uff08TER\uff09\u6846\u67b6\u901a\u8fc7\u5f15\u5165\u4e0d\u786e\u5b9a\u6027\u4f30\u8ba1\u6765\u8861\u91cf\u9884\u6d4b\u7684\u7f6e\u4fe1\u5ea6\uff0c\u5e76\u7ed3\u5408\u591a\u6a21\u6001\u4fe1\u606f\uff0c\u65e8\u5728\u63d0\u9ad8\u60c5\u611f\u8bc6\u522b\u7684\u53ef\u9760\u6027\u548c\u9c81\u68d2\u6027\u3002TER\u6846\u67b6\u5728IEMOCAP\u548cMusic-video\u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u9886\u5148\u7684\u6027\u80fd\uff0c\u5728\u53ef\u4fe1\u6027\u80fd\u8bc4\u4f30\u6307\u6807\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.07647", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07647", "abs": "https://arxiv.org/abs/2508.07647", "authors": ["Xiaohang Zhan", "Dingming Liu"], "title": "LaRender: Training-Free Occlusion Control in Image Generation via Latent Rendering", "comment": "Accepted by ICCV 2025 (oral). Project page:\n  https://xiaohangzhan.github.io/projects/larender/", "summary": "We propose a novel training-free image generation algorithm that precisely\ncontrols the occlusion relationships between objects in an image. Existing\nimage generation methods typically rely on prompts to influence occlusion,\nwhich often lack precision. While layout-to-image methods provide control over\nobject locations, they fail to address occlusion relationships explicitly.\nGiven a pre-trained image diffusion model, our method leverages volume\nrendering principles to \"render\" the scene in latent space, guided by occlusion\nrelationships and the estimated transmittance of objects. This approach does\nnot require retraining or fine-tuning the image diffusion model, yet it enables\naccurate occlusion control due to its physics-grounded foundation. In extensive\nexperiments, our method significantly outperforms existing approaches in terms\nof occlusion accuracy. Furthermore, we demonstrate that by adjusting the\nopacities of objects or concepts during rendering, our method can achieve a\nvariety of effects, such as altering the transparency of objects, the density\nof mass (e.g., forests), the concentration of particles (e.g., rain, fog), the\nintensity of light, and the strength of lens effects, etc.", "AI": {"tldr": "\u4e00\u7a2e\u65b0\u7684\u8a13\u7df4\u514d\u8cbb\u5716\u50cf\u751f\u6210\u7b97\u6cd5\uff0c\u901a\u904e\u5229\u7528\u9ad4\u7a4d\u6e32\u67d3\u539f\u7406\u7cbe\u78ba\u63a7\u5236\u5716\u50cf\u4e2d\u5c0d\u8c61\u7684\u906e\u64cb\u95dc\u4fc2\u3002", "motivation": "\u73fe\u6709\u7684\u5716\u50cf\u751f\u6210\u65b9\u6cd5\u901a\u5e38\u4f9d\u8cf4\u63d0\u793a\u4f86\u5f71\u97ff\u906e\u64cb\uff0c\u4f46\u7cbe\u5ea6\u4e0d\u8db3\u3002\u96d6\u7136 layout-to-image \u65b9\u6cd5\u53ef\u4ee5\u63a7\u5236\u5c0d\u8c61\u4f4d\u7f6e\uff0c\u4f46\u672a\u80fd\u660e\u78ba\u89e3\u6c7a\u906e\u64cb\u95dc\u4fc2\u3002", "method": "\u8a72\u65b9\u6cd5\u5229\u7528\u9ad4\u7a4d\u6e32\u67d3\u539f\u7406\u5728\u6f5b\u7a7a\u9593\u4e2d\u6e32\u67d3\u5834\u666f\uff0c\u4e26\u4ee5\u906e\u64cb\u95dc\u4fc2\u548c\u4f30\u8a08\u7684\u7269\u9ad4\u900f\u5c04\u7387\u70ba\u6307\u5c0e\uff0c\u540c\u6642\u5229\u7528\u9810\u8a13\u7df4\u7684\u5716\u50cf\u64f4\u6563\u6a21\u578b\u3002\u6b64\u65b9\u6cd5\u7121\u9700\u91cd\u65b0\u8a13\u7df4\u6216\u5fae\u8abf\u5716\u50cf\u64f4\u6563\u6a21\u578b\u3002", "result": "\u5728\u5927\u91cf\u5be6\u9a57\u4e2d\uff0c\u8a72\u65b9\u6cd5\u5728\u906e\u64cb\u6e96\u78ba\u6027\u65b9\u9762\u986f\u8457\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\u3002\u6b64\u5916\uff0c\u901a\u904e\u8abf\u6574\u6e32\u67d3\u904e\u7a0b\u4e2d\u7684\u5c0d\u8c61\u4e0d\u900f\u660e\u5ea6\uff0c\u53ef\u4ee5\u5be6\u73fe\u5404\u7a2e\u6548\u679c\u3002", "conclusion": "\u8a72\u65b9\u6cd5\u5728\u906e\u64cb\u6e96\u78ba\u6027\u65b9\u9762\u986f\u8457\u512a\u65bc\u73fe\u6709\u65b9\u6cd5\uff0c\u4e26\u4e14\u53ef\u4ee5\u901a\u904e\u8abf\u6574\u6e32\u67d3\u904e\u7a0b\u4e2d\u7684\u5c0d\u8c61\u4e0d\u900f\u660e\u5ea6\u4f86\u5be6\u73fe\u5404\u7a2e\u6548\u679c\uff0c\u4f8b\u5982\u6539\u8b8a\u5c0d\u8c61\u7684\u900f\u660e\u5ea6\u3001\u7269\u9ad4\u7684\u5bc6\u5ea6\u3001\u7c92\u5b50\u6fc3\u5ea6\u3001\u5149\u7dda\u5f37\u5ea6\u548c\u93e1\u982d\u6548\u679c\u7b49\u3002"}}
{"id": "2508.07656", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07656", "abs": "https://arxiv.org/abs/2508.07656", "authors": ["Yimin Fu", "Zhunga Liu", "Dongxiu Guo", "Longfei Wang"], "title": "Collaborative Learning of Scattering and Deep Features for SAR Target Recognition with Noisy Labels", "comment": "The code will be released at https://github.com/fuyimin96/CLSDF upon\n  acceptance", "summary": "The acquisition of high-quality labeled synthetic aperture radar (SAR) data\nis challenging due to the demanding requirement for expert knowledge.\nConsequently, the presence of unreliable noisy labels is unavoidable, which\nresults in performance degradation of SAR automatic target recognition (ATR).\nExisting research on learning with noisy labels mainly focuses on image data.\nHowever, the non-intuitive visual characteristics of SAR data are insufficient\nto achieve noise-robust learning. To address this problem, we propose\ncollaborative learning of scattering and deep features (CLSDF) for SAR ATR with\nnoisy labels. Specifically, a multi-model feature fusion framework is designed\nto integrate scattering and deep features. The attributed scattering centers\n(ASCs) are treated as dynamic graph structure data, and the extracted physical\ncharacteristics effectively enrich the representation of deep image features.\nThen, the samples with clean and noisy labels are divided by modeling the loss\ndistribution with multiple class-wise Gaussian Mixture Models (GMMs).\nAfterward, the semi-supervised learning of two divergent branches is conducted\nbased on the data divided by each other. Moreover, a joint distribution\nalignment strategy is introduced to enhance the reliability of co-guessed\nlabels. Extensive experiments have been done on the Moving and Stationary\nTarget Acquisition and Recognition (MSTAR) dataset, and the results show that\nthe proposed method can achieve state-of-the-art performance under different\noperating conditions with various label noises.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aCLSDF\uff08\u6563\u5c04\u548c\u6df1\u5ea6\u7279\u5f81\u7684\u534f\u540c\u5b66\u4e60\uff09\u7684\u65b9\u6cd5\uff0c\u7528\u4e8e\u89e3\u51b3SAR\u81ea\u52a8\u76ee\u6807\u8bc6\u522b\uff08ATR\uff09\u4e2d\u7531\u4e8e\u566a\u58f0\u6807\u7b7e\u5bfc\u81f4\u7684\u6027\u80fd\u4e0b\u964d\u95ee\u9898\u3002\u901a\u8fc7\u878d\u5408\u6563\u5c04\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u534a\u76d1\u7763\u5b66\u4e60\u548c\u8054\u5408\u5206\u5e03\u5bf9\u9f50\u7b56\u7565\uff0c\u8be5\u65b9\u6cd5\u5728MSTAR\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002", "motivation": "\u7531\u4e8e\u9700\u8981\u4e13\u5bb6\u77e5\u8bc6\uff0c\u9ad8\u8d28\u91cf\u6807\u8bb0\u7684SAR\u6570\u636e\u83b7\u53d6\u5177\u6709\u6311\u6218\u6027\uff0c\u5bfc\u81f4\u4e0d\u53ef\u907f\u514d\u5730\u5b58\u5728\u4e0d\u53ef\u9760\u7684\u566a\u58f0\u6807\u7b7e\uff0c\u4ece\u800c\u5bfc\u81f4SAR\u81ea\u52a8\u76ee\u6807\u8bc6\u522b\uff08ATR\uff09\u6027\u80fd\u4e0b\u964d\u3002\u73b0\u6709\u7684\u5173\u4e8e\u566a\u58f0\u6807\u7b7e\u5b66\u4e60\u7684\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u56fe\u50cf\u6570\u636e\u4e0a\uff0c\u4f46SAR\u6570\u636e\u975e\u76f4\u89c2\u7684\u89c6\u89c9\u7279\u6027\u4e0d\u8db3\u4ee5\u5b9e\u73b0\u9c81\u68d2\u6027\u5b66\u4e60\u3002", "method": "1.\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u591a\u6a21\u578b\u7279\u5f81\u878d\u5408\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u6563\u5c04\u7279\u5f81\u548c\u6df1\u5ea6\u7279\u5f81\u3002\n2.\u5c06\u5f52\u56e0\u6563\u5c04\u4e2d\u5fc3\uff08ASC\uff09\u89c6\u4e3a\u52a8\u6001\u56fe\u7ed3\u6784\u6570\u636e\uff0c\u5e76\u63d0\u53d6\u7269\u7406\u7279\u5f81\u6765\u4e30\u5bcc\u6df1\u5ea6\u56fe\u50cf\u7279\u5f81\u7684\u8868\u793a\u3002\n3.\u901a\u8fc7\u4f7f\u7528\u591a\u4e2a\u7c7b\u522b\u7684\u6df7\u5408\u9ad8\u65af\u6a21\u578b\uff08GMM\uff09\u6765\u6a21\u62df\u635f\u5931\u5206\u5e03\uff0c\u4ece\u800c\u5212\u5206\u51fa\u5305\u542b\u5e72\u51c0\u6807\u7b7e\u548c\u566a\u58f0\u6807\u7b7e\u7684\u6837\u672c\u3002\n4.\u57fa\u4e8e\u5212\u5206\u51fa\u7684\u6570\u636e\uff0c\u5bf9\u4e24\u4e2a\u4e0d\u540c\u7684\u5206\u652f\u8fdb\u884c\u534a\u76d1\u7763\u5b66\u4e60\u3002\n5.\u5f15\u5165\u8054\u5408\u5206\u5e03\u5bf9\u9f50\u7b56\u7565\u6765\u63d0\u9ad8\u5171\u731c\u6d4b\u6807\u7b7e\u7684\u53ef\u9760\u6027\u3002", "result": "\u5728MSTAR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u64cd\u4f5c\u6761\u4ef6\u548c\u5404\u79cd\u6807\u7b7e\u566a\u58f0\u4e0b\u5747\u80fd\u5b9e\u73b0\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684CLSDF\u65b9\u6cd5\u5728MSTAR\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u5e76\u5728\u4e0d\u540c\u64cd\u4f5c\u6761\u4ef6\u548c\u5404\u79cd\u6807\u7b7e\u566a\u58f0\u4e0b\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07680", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07680", "abs": "https://arxiv.org/abs/2508.07680", "authors": ["Zhiying Li", "Junhao Wu", "Yeying Jin", "Daiheng Gao", "Yun Ji", "Kaichuan Kong", "Lei Yu", "Hao Xu", "Kai Chen", "Bruce Gu", "Nana Wang", "Zhaoxin Fan"], "title": "Undress to Redress: A Training-Free Framework for Virtual Try-On", "comment": "13 pages, 8 figures", "summary": "Virtual try-on (VTON) is a crucial task for enhancing user experience in\nonline shopping by generating realistic garment previews on personal photos.\nAlthough existing methods have achieved impressive results, they struggle with\nlong-sleeve-to-short-sleeve conversions-a common and practical scenario-often\nproducing unrealistic outputs when exposed skin is underrepresented in the\noriginal image. We argue that this challenge arises from the ''majority''\ncompletion rule in current VTON models, which leads to inaccurate skin\nrestoration in such cases. To address this, we propose UR-VTON (Undress-Redress\nVirtual Try-ON), a novel, training-free framework that can be seamlessly\nintegrated with any existing VTON method. UR-VTON introduces an\n''undress-to-redress'' mechanism: it first reveals the user's torso by\nvirtually ''undressing,'' then applies the target short-sleeve garment,\neffectively decomposing the conversion into two more manageable steps.\nAdditionally, we incorporate Dynamic Classifier-Free Guidance scheduling to\nbalance diversity and image quality during DDPM sampling, and employ Structural\nRefiner to enhance detail fidelity using high-frequency cues. Finally, we\npresent LS-TON, a new benchmark for long-sleeve-to-short-sleeve try-on.\nExtensive experiments demonstrate that UR-VTON outperforms state-of-the-art\nmethods in both detail preservation and image quality. Code will be released\nupon acceptance.", "AI": {"tldr": "UR-VTON is a new framework for virtual try-on that improves long-sleeve-to-short-sleeve conversions by \"undressing\" the person first, then dressing them in the new garment. It also uses special guidance and a refiner for better results and introduces a new benchmark dataset (LS-TON). It outperforms existing methods.", "motivation": "Existing VTON methods struggle with long-sleeve-to-short-sleeve conversions, often producing unrealistic outputs when exposed skin is underrepresented in the original image, due to the \"majority\" completion rule which leads to inaccurate skin restoration.", "method": "UR-VTON introduces an \"undress-to-redress\" mechanism: it first reveals the user's torso by virtually \"undressing,\" then applies the target short-sleeve garment, effectively decomposing the conversion into two more manageable steps. Additionally, it incorporates Dynamic Classifier-Free Guidance scheduling to balance diversity and image quality during DDPM sampling, and employs Structural Refiner to enhance detail fidelity using high-frequency cues.", "result": "Extensive experiments demonstrate that UR-VTON outperforms state-of-the-art methods in both detail preservation and image quality.", "conclusion": "UR-VTON is a novel, training-free framework that can be seamlessly integrated with any existing VTON method. UR-VTON outperforms state-of-the-art methods in both detail preservation and image quality."}}
{"id": "2508.07682", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07682", "abs": "https://arxiv.org/abs/2508.07682", "authors": ["Wenzhuo Ma", "Zhenzhong Chen"], "title": "DiffVC-OSD: One-Step Diffusion-based Perceptual Neural Video Compression Framework", "comment": null, "summary": "In this work, we first propose DiffVC-OSD, a One-Step Diffusion-based\nPerceptual Neural Video Compression framework. Unlike conventional multi-step\ndiffusion-based methods, DiffVC-OSD feeds the reconstructed latent\nrepresentation directly into a One-Step Diffusion Model, enhancing perceptual\nquality through a single diffusion step guided by both temporal context and the\nlatent itself. To better leverage temporal dependencies, we design a Temporal\nContext Adapter that encodes conditional inputs into multi-level features,\noffering more fine-grained guidance for the Denoising Unet. Additionally, we\nemploy an End-to-End Finetuning strategy to improve overall compression\nperformance. Extensive experiments demonstrate that DiffVC-OSD achieves\nstate-of-the-art perceptual compression performance, offers about 20$\\times$\nfaster decoding and a 86.92\\% bitrate reduction compared to the corresponding\nmulti-step diffusion-based variant.", "AI": {"tldr": "DiffVC-OSD \u662f\u4e00\u79cd\u521b\u65b0\u7684\u5355\u6b65\u6269\u6563\u89c6\u9891\u7f16\u89e3\u7801\u6846\u67b6\uff0c\u5728\u4fdd\u6301\u9ad8\u611f\u77e5\u8d28\u91cf\u7684\u540c\u65f6\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u538b\u7f29\u6548\u7387\u548c\u89e3\u7801\u901f\u5ea6\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u611f\u77e5\u8d28\u91cf\u548c\u538b\u7f29\u6027\u80fd\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u5355\u6b65\u6269\u6563\u89c6\u9891\u7f16\u89e3\u7801\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a DiffVC-OSD \u7684\u5355\u6b65\u6269\u6563\u89c6\u9891\u7f16\u89e3\u7801\u6846\u67b6\uff0c\u901a\u8fc7\u5355\u6b65\u6269\u6563\u6a21\u578b\u589e\u5f3a\u611f\u77e5\u8d28\u91cf\uff0c\u5e76\u8bbe\u8ba1\u4e86\u65f6\u95f4\u4e0a\u4e0b\u6587\u9002\u914d\u5668\u6765\u5229\u7528\u65f6\u95f4\u4f9d\u8d56\u6027\uff0c\u6700\u540e\u91c7\u7528\u7aef\u5230\u7aef\u5fae\u8c03\u7b56\u7565\u6765\u63d0\u9ad8\u6574\u4f53\u6027\u80fd\u3002", "result": "DiffVC-OSD \u5728\u611f\u77e5\u538b\u7f29\u6027\u80fd\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u6c34\u5e73\uff0c\u89e3\u7801\u901f\u5ea6\u63d0\u9ad8\u4e86 20 \u500d\uff0c\u6bd4\u7279\u7387\u964d\u4f4e\u4e86 86.92%\u3002", "conclusion": "DiffVC-OSD \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u611f\u77e5\u538b\u7f29\u6027\u80fd\uff0c\u89e3\u7801\u901f\u5ea6\u6bd4\u591a\u6b65\u6269\u6563\u65b9\u6cd5\u5feb 20 \u500d\uff0c\u6bd4\u7279\u7387\u964d\u4f4e\u4e86 86.92%\u3002"}}
{"id": "2508.07683", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07683", "abs": "https://arxiv.org/abs/2508.07683", "authors": ["Chaohong Guo", "Xun Mo", "Yongwei Nie", "Xuemiao Xu", "Chao Xu", "Fei Yu", "Chengjiang Long"], "title": "TAR-TVG: Enhancing VLMs with Timestamp Anchor-Constrained Reasoning for Temporal Video Grounding", "comment": null, "summary": "Temporal Video Grounding (TVG) aims to precisely localize video segments\ncorresponding to natural language queries, which is a critical capability for\nlong-form video understanding. Although existing reinforcement learning\napproaches encourage models to generate reasoning chains before predictions,\nthey fail to explicitly constrain the reasoning process to ensure the quality\nof the final temporal predictions. To address this limitation, we propose\nTimestamp Anchor-constrained Reasoning for Temporal Video Grounding (TAR-TVG),\na novel framework that introduces timestamp anchors within the reasoning\nprocess to enforce explicit supervision to the thought content. These anchors\nserve as intermediate verification points. More importantly, we require each\nreasoning step to produce increasingly accurate temporal estimations, thereby\nensuring that the reasoning process contributes meaningfully to the final\nprediction. To address the challenge of low-probability anchor generation in\nmodels (e.g., Qwen2.5-VL-3B), we develop an efficient self-distillation\ntraining strategy: (1) initial GRPO training to collect 30K high-quality\nreasoning traces containing multiple timestamp anchors, (2) supervised\nfine-tuning (SFT) on distilled data, and (3) final GRPO optimization on the\nSFT-enhanced model. This three-stage training strategy enables robust anchor\ngeneration while maintaining reasoning quality. Experiments show that our model\nachieves state-of-the-art performance while producing interpretable, verifiable\nreasoning chains with progressively refined temporal estimations.", "AI": {"tldr": "TAR-TVG \u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u6233\u951a\u70b9\u548c\u4e09\u9636\u6bb5\u81ea\u84b8\u998f\u8bad\u7ec3\u7b56\u7565\uff0c\u6539\u8fdb\u4e86\u89c6\u9891\u65f6\u95f4\u5b9a\u4f4d\u4efb\u52a1\uff0c\u63d0\u9ad8\u4e86\u63a8\u7406\u8fc7\u7a0b\u7684\u8d28\u91cf\u548c\u53ef\u89e3\u91ca\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u751f\u6210\u63a8\u7406\u94fe\u524d\u8fdb\u884c\u9884\u6d4b\u65f6\uff0c\u672a\u80fd\u660e\u786e\u7ea6\u675f\u63a8\u7406\u8fc7\u7a0b\u4ee5\u4fdd\u8bc1\u6700\u7ec8\u65f6\u95f4\u9884\u6d4b\u7684\u8d28\u91cf\u3002TAR-TVG \u65e8\u5728\u89e3\u51b3\u6b64\u5c40\u9650\u6027\uff0c\u63d0\u9ad8\u89c6\u9891\u8bed\u8a00\u7406\u89e3\u4e2d\u7684\u65f6\u95f4\u5b9a\u4f4d\u7cbe\u5ea6\u3002", "method": "TAR-TVG \u6846\u67b6\u901a\u8fc7\u5728\u63a8\u7406\u8fc7\u7a0b\u4e2d\u5f15\u5165\u65f6\u95f4\u6233\u951a\u70b9\u4f5c\u4e3a\u4e2d\u95f4\u9a8c\u8bc1\u70b9\uff0c\u5e76\u8981\u6c42\u6bcf\u4e2a\u63a8\u7406\u6b65\u9aa4\u4ea7\u751f\u8d8a\u6765\u8d8a\u7cbe\u786e\u7684\u65f6\u95f4\u4f30\u8ba1\uff0c\u4ece\u800c\u5bf9\u601d\u8003\u5185\u5bb9\u8fdb\u884c\u663e\u5f0f\u76d1\u7763\u3002\u4e3a\u4e86\u89e3\u51b3\u4f4e\u6982\u7387\u951a\u70b9\u751f\u6210\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u9636\u6bb5\u81ea\u84b8\u998f\u8bad\u7ec3\u7b56\u7565\uff1a1) \u521d\u59cb GRPO \u8bad\u7ec3\u4ee5\u6536\u96c6\u5305\u542b\u591a\u4e2a\u65f6\u95f4\u6233\u951a\u70b9\u7684\u9ad8\u8d28\u91cf\u63a8\u7406\u8f68\u8ff9\uff1b2) \u5728\u84b8\u998f\u6570\u636e\u4e0a\u8fdb\u884c\u76d1\u7763\u5fae\u8c03\uff08SFT\uff09\uff1b3) \u5728 SFT \u589e\u5f3a\u6a21\u578b\u4e0a\u8fdb\u884c\u6700\u7ec8 GRPO \u4f18\u5316\u3002", "result": "TAR-TVG \u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u751f\u6210\u4e86\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u4e14\u65f6\u95f4\u4f30\u8ba1\u9010\u6b65\u7cbe\u786e\u7684\u63a8\u7406\u94fe\u3002", "conclusion": "TAR-TVG \u6846\u67b6\u901a\u8fc7\u5f15\u5165\u65f6\u95f4\u6233\u951a\u70b9\u6765\u7ea6\u675f\u63a8\u7406\u8fc7\u7a0b\uff0c\u786e\u4fdd\u4e86\u6700\u7ec8\u65f6\u95f4\u9884\u6d4b\u7684\u8d28\u91cf\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cTAR-TVG \u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u3001\u53ef\u9a8c\u8bc1\u4e14\u65f6\u95f4\u4f30\u8ba1\u9010\u6b65\u6539\u8fdb\u7684\u63a8\u7406\u94fe\u3002"}}
{"id": "2508.07700", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07700", "abs": "https://arxiv.org/abs/2508.07700", "authors": ["Weitao Wang", "Haoran Xu", "Jun Meng", "Haoqian Wang"], "title": "Make Your MoVe: Make Your 3D Contents by Adapting Multi-View Diffusion Models to External Editing", "comment": null, "summary": "As 3D generation techniques continue to flourish, the demand for generating\npersonalized content is rapidly rising. Users increasingly seek to apply\nvarious editing methods to polish generated 3D content, aiming to enhance its\ncolor, style, and lighting without compromising the underlying geometry.\nHowever, most existing editing tools focus on the 2D domain, and directly\nfeeding their results into 3D generation methods (like multi-view diffusion\nmodels) will introduce information loss, degrading the quality of the final 3D\nassets. In this paper, we propose a tuning-free, plug-and-play scheme that\naligns edited assets with their original geometry in a single inference run.\nCentral to our approach is a geometry preservation module that guides the\nedited multi-view generation with original input normal latents. Besides, an\ninjection switcher is proposed to deliberately control the supervision extent\nof the original normals, ensuring the alignment between the edited color and\nnormal views. Extensive experiments show that our method consistently improves\nboth the multi-view consistency and mesh quality of edited 3D assets, across\nmultiple combinations of multi-view diffusion models and editing methods.", "AI": {"tldr": "\u4e00\u79cd\u7528\u4e8e3D\u5185\u5bb9\u7f16\u8f91\u7684\u5373\u63d2\u5373\u7528\u65b9\u6848\uff0c\u901a\u8fc7\u51e0\u4f55\u4fdd\u6301\u6a21\u5757\u548c\u6ce8\u5165\u5f00\u5173\u6765\u4fdd\u6301\u51e0\u4f55\u5f62\u72b6\uff0c\u5e76\u63d0\u9ad8\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u7f51\u683c\u8d28\u91cf\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u7528\u6237\u5728\u751f\u62103D\u5185\u5bb9\u65f6\uff0c\u5728\u4e0d\u5f71\u54cd\u51e0\u4f55\u5f62\u72b6\u7684\u524d\u63d0\u4e0b\uff0c\u5bf9\u5176\u989c\u8272\u3001\u98ce\u683c\u548c\u5149\u7167\u8fdb\u884c\u7f16\u8f91\u7684\u9700\u6c42\uff0c\u540c\u65f6\u89e3\u51b3\u73b0\u67092D\u7f16\u8f91\u5de5\u5177\u5f15\u5165\u4fe1\u606f\u635f\u5931\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5373\u63d2\u5373\u7528\u7684\u65b9\u6848\uff0c\u5305\u62ec\u4e00\u4e2a\u51e0\u4f55\u4fdd\u6301\u6a21\u5757\u548c\u4e00\u4e2a\u6ce8\u5165\u5f00\u5173\uff0c\u4ee5\u5728\u5355\u6b21\u63a8\u7406\u4e2d\u5c06\u7f16\u8f91\u540e\u7684\u8d44\u4ea7\u4e0e\u539f\u59cb\u51e0\u4f55\u5f62\u72b6\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u79cd\u591a\u89c6\u56fe\u6269\u6563\u6a21\u578b\u548c\u7f16\u8f91\u65b9\u6cd5\u7684\u7ec4\u5408\u4e0b\uff0c\u80fd\u591f\u6301\u7eed\u63d0\u9ad8\u7f16\u8f91\u540e3D\u8d44\u4ea7\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u7f51\u683c\u8d28\u91cf\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728\u4e0d\u5f71\u54cd\u51e0\u4f55\u5f62\u72b6\u7684\u524d\u63d0\u4e0b\uff0c\u80fd\u591f\u5bf9\u751f\u6210\u76843D\u5185\u5bb9\u7684\u989c\u8272\u3001\u98ce\u683c\u548c\u5149\u7167\u8fdb\u884c\u7f16\u8f91\uff0c\u5e76\u4e14\u80fd\u591f\u63d0\u9ad8\u7f16\u8f91\u540e3D\u8d44\u4ea7\u7684\u591a\u89c6\u56fe\u4e00\u81f4\u6027\u548c\u7f51\u683c\u8d28\u91cf\u3002"}}
{"id": "2508.07723", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07723", "abs": "https://arxiv.org/abs/2508.07723", "authors": ["Ting Xiang", "Changjian Chen", "Zhuo Tang", "Qifeng Zhang", "Fei Lyu", "Li Yang", "Jiapeng Zhang", "Kenli Li"], "title": "Enhancing Small-Scale Dataset Expansion with Triplet-Connection-based Sample Re-Weighting", "comment": "15 pages, 8 figures, published to ACM MM2025", "summary": "The performance of computer vision models in certain real-world applications,\nsuch as medical diagnosis, is often limited by the scarcity of available\nimages. Expanding datasets using pre-trained generative models is an effective\nsolution. However, due to the uncontrollable generation process and the\nambiguity of natural language, noisy images may be generated. Re-weighting is\nan effective way to address this issue by assigning low weights to such noisy\nimages. We first theoretically analyze three types of supervision for the\ngenerated images. Based on the theoretical analysis, we develop TriReWeight, a\ntriplet-connection-based sample re-weighting method to enhance generative data\naugmentation. Theoretically, TriReWeight can be integrated with any generative\ndata augmentation methods and never downgrade their performance. Moreover, its\ngeneralization approaches the optimal in the order $O(\\sqrt{d\\ln (n)/n})$. Our\nexperiments validate the correctness of the theoretical analysis and\ndemonstrate that our method outperforms the existing SOTA methods by $7.9\\%$ on\naverage over six natural image datasets and by $3.4\\%$ on average over three\nmedical datasets. We also experimentally validate that our method can enhance\nthe performance of different generative data augmentation methods.", "AI": {"tldr": "Generative models expand datasets but can create noisy images. TriReWeight re-weights these noisy images, improving performance in computer vision tasks, especially with limited data. It outperforms existing methods and works with various generative techniques.", "motivation": "The scarcity of images in real-world applications like medical diagnosis limits the performance of computer vision models. Generative models can expand datasets, but may produce noisy images due to uncontrollable generation processes and ambiguous natural language.", "method": "TriReWeight, a triplet-connection-based sample re-weighting method, theoretically analyzes three types of supervision for generated images and assigns low weights to noisy images to enhance generative data augmentation.", "result": "TriReWeight outperforms existing state-of-the-art methods by 7.9% on average across six natural image datasets and by 3.4% on average across three medical datasets. It can also enhance the performance of different generative data augmentation methods.", "conclusion": "Re-weighting methods like TriReWeight can improve generative data augmentation by assigning lower weights to noisy images, leading to better performance in computer vision tasks, especially in data-scarce domains like medical imaging. TriReWeight demonstrates superior performance over existing methods and can be integrated with various generative approaches."}}
{"id": "2508.07747", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07747", "abs": "https://arxiv.org/abs/2508.07747", "authors": ["Junhyuk So", "Juncheol Shin", "Hyunho Kook", "Eunhyeok Park"], "title": "Grouped Speculative Decoding for Autoregressive Image Generation", "comment": "Accepted to the ICCV 2025", "summary": "Recently, autoregressive (AR) image models have demonstrated remarkable\ngenerative capabilities, positioning themselves as a compelling alternative to\ndiffusion models. However, their sequential nature leads to long inference\ntimes, limiting their practical scalability. In this work, we introduce Grouped\nSpeculative Decoding (GSD), a novel, training-free acceleration method for AR\nimage models. While recent studies have explored Speculative Decoding (SD) as a\nmeans to speed up AR image generation, existing approaches either provide only\nmodest acceleration or require additional training. Our in-depth analysis\nreveals a fundamental difference between language and image tokens: image\ntokens exhibit inherent redundancy and diversity, meaning multiple tokens can\nconvey valid semantics. However, traditional SD methods are designed to accept\nonly a single most-likely token, which fails to leverage this difference,\nleading to excessive false-negative rejections. To address this, we propose a\nnew SD strategy that evaluates clusters of visually valid tokens rather than\nrelying on a single target token. Additionally, we observe that static\nclustering based on embedding distance is ineffective, which motivates our\ndynamic GSD approach. Extensive experiments show that GSD accelerates AR image\nmodels by an average of 3.7x while preserving image quality-all without\nrequiring any additional training. The source code is available at\nhttps://github.com/junhyukso/GSD", "AI": {"tldr": "This paper introduces Grouped Speculative Decoding (GSD), a training-free method to speed up autoregressive (AR) image models. Unlike previous methods, GSD considers groups of valid image tokens instead of just one, addressing the unique properties of image data and improving acceleration. Experiments show GSD achieves an average 3.7x speedup without compromising image quality or requiring extra training.", "motivation": "Traditional Speculative Decoding (SD) methods for AR image models have limitations, either providing modest acceleration or requiring additional training. Existing methods fail to leverage the inherent redundancy and diversity of image tokens, leading to excessive false-negative rejections because they rely on a single most-likely token. This work aims to address these limitations by proposing a new SD strategy.", "method": "GSD is a novel, training-free acceleration method for AR image models that evaluates clusters of visually valid tokens rather than relying on a single target token. It dynamically clusters tokens based on embedding distance to address the ineffectiveness of static clustering.", "result": "GSD accelerates AR image models by an average of 3.7x while preserving image quality. This was demonstrated through extensive experiments.", "conclusion": "GSD can accelerate AR image models by an average of 3.7x while preserving image quality, without requiring additional training."}}
{"id": "2508.07755", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07755", "abs": "https://arxiv.org/abs/2508.07755", "authors": ["Minseo Kim", "Minchan Kwon", "Dongyeun Lee", "Yunho Jeon", "Junmo Kim"], "title": "Comparison Reveals Commonality: Customized Image Generation through Contrastive Inversion", "comment": "Accepted at CVPR 2025 workshop (AI4CC)", "summary": "The recent demand for customized image generation raises a need for\ntechniques that effectively extract the common concept from small sets of\nimages. Existing methods typically rely on additional guidance, such as text\nprompts or spatial masks, to capture the common target concept. Unfortunately,\nrelying on manually provided guidance can lead to incomplete separation of\nauxiliary features, which degrades generation quality.In this paper, we propose\nContrastive Inversion, a novel approach that identifies the common concept by\ncomparing the input images without relying on additional information. We train\nthe target token along with the image-wise auxiliary text tokens via\ncontrastive learning, which extracts the well-disentangled true semantics of\nthe target. Then we apply disentangled cross-attention fine-tuning to improve\nconcept fidelity without overfitting. Experimental results and analysis\ndemonstrate that our method achieves a balanced, high-level performance in both\nconcept representation and editing, outperforming existing techniques.", "AI": {"tldr": "A new method called Contrastive Inversion extracts common concepts from images using contrastive learning and cross-attention fine-tuning, improving image generation quality without manual guidance.", "motivation": "To address the need for techniques that effectively extract common concepts from small sets of images for customized image generation, as existing methods that rely on manual guidance can lead to incomplete separation of auxiliary features and degrade generation quality.", "method": "Contrastive learning is used to train the target token and image-wise auxiliary text tokens. Disentangled cross-attention fine-tuning is then applied to improve concept fidelity without overfitting.", "result": "Experimental results demonstrate that the proposed method achieves balanced, high-level performance in both concept representation and editing, outperforming existing techniques.", "conclusion": "Contrastive Inversion"}}
{"id": "2508.07759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07759", "abs": "https://arxiv.org/abs/2508.07759", "authors": ["Haoran Wang", "Zekun Li", "Jian Zhang", "Lei Qi", "Yinghuan Shi"], "title": "Correspondence as Video: Test-Time Adaption on SAM2 for Reference Segmentation in the Wild", "comment": null, "summary": "Large vision models like the Segment Anything Model (SAM) exhibit significant\nlimitations when applied to downstream tasks in the wild. Consequently,\nreference segmentation, which leverages reference images and their\ncorresponding masks to impart novel knowledge to the model, emerges as a\npromising new direction for adapting vision models. However, existing reference\nsegmentation approaches predominantly rely on meta-learning, which still\nnecessitates an extensive meta-training process and brings massive data and\ncomputational cost. In this study, we propose a novel approach by representing\nthe inherent correspondence between reference-target image pairs as a pseudo\nvideo. This perspective allows the latest version of SAM, known as SAM2, which\nis equipped with interactive video object segmentation (iVOS) capabilities, to\nbe adapted to downstream tasks in a lightweight manner. We term this approach\nCorrespondence As Video for SAM (CAV-SAM). CAV-SAM comprises two key modules:\nthe Diffusion-Based Semantic Transition (DBST) module employs a diffusion model\nto construct a semantic transformation sequence, while the Test-Time Geometric\nAlignment (TTGA) module aligns the geometric changes within this sequence\nthrough test-time fine-tuning. We evaluated CAVSAM on widely-used datasets,\nachieving segmentation performance improvements exceeding 5% over SOTA methods.\nImplementation is provided in the supplementary materials.", "AI": {"tldr": "\u63d0\u51faCAV-SAM\uff0c\u4e00\u79cd\u65b0\u9896\u7684\u53c2\u8003\u5206\u5272\u65b9\u6cd5\uff0c\u5c06\u53c2\u8003-\u76ee\u6807\u56fe\u50cf\u5bf9\u8868\u793a\u4e3a\u4f2a\u89c6\u9891\uff0c\u5229\u7528SAM2\u7684iVOS\u80fd\u529b\uff0c\u901a\u8fc7DBST\u548cTTGA\u6a21\u5757\u5b9e\u73b0\u8f7b\u91cf\u7ea7\u9002\u5e94\uff0c\u6027\u80fd\u8d85\u8d8aSOTA\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u53c2\u8003\u5206\u5272\u65b9\u6cd5\u4f9d\u8d56\u5143\u5b66\u4e60\u5e26\u6765\u7684\u6d77\u91cf\u6570\u636e\u548c\u8ba1\u7b97\u6210\u672c\u7684\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u8868\u793a\u65b9\u6cd5\uff0c\u5c06\u53c2\u8003-\u76ee\u6807\u56fe\u50cf\u5bf9\u7684\u5185\u5728\u5bf9\u5e94\u5173\u7cfb\u8868\u793a\u4e3a\u4f2a\u89c6\u9891\u3002", "method": "CAV-SAM\u5305\u542b\u4e24\u4e2a\u5173\u952e\u6a21\u5757\uff1a\u57fa\u4e8e\u6269\u6563\u7684\u8bed\u4e49\u8f6c\u6362\uff08DBST\uff09\u6a21\u5757\uff0c\u5b83\u4f7f\u7528\u6269\u6563\u6a21\u578b\u6765\u6784\u5efa\u8bed\u4e49\u8f6c\u6362\u5e8f\u5217\uff1b\u4ee5\u53ca\u6d4b\u8bd5\u65f6\u51e0\u4f55\u5bf9\u9f50\uff08TTGA\uff09\u6a21\u5757\uff0c\u5b83\u901a\u8fc7\u6d4b\u8bd5\u65f6\u5fae\u8c03\u6765\u5bf9\u9f50\u6b64\u5e8f\u5217\u4e2d\u7684\u51e0\u4f55\u53d8\u5316\u3002", "result": "CAV-SAM\u5728\u5e38\u7528\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u663e\u793a\uff0c\u5176\u5206\u5272\u6027\u80fd\u6bd4SOTA\u65b9\u6cd5\u63d0\u9ad8\u4e865%\u4ee5\u4e0a\u3002", "conclusion": "CAV-SAM\u901a\u8fc7\u5c06\u53c2\u8003\u56fe\u50cf\u548c\u76ee\u6807\u56fe\u50cf\u4e4b\u95f4\u7684\u5bf9\u5e94\u5173\u7cfb\u8868\u793a\u4e3a\u4f2a\u89c6\u9891\uff0c\u5e76\u5229\u7528SAM2\u7684iVOS\u80fd\u529b\uff0c\u5b9e\u73b0\u4e86\u5bf9\u4e0b\u6e38\u4efb\u52a1\u7684\u8f7b\u91cf\u7ea7\u9002\u5e94\uff0c\u5728\u5206\u5272\u6027\u80fd\u4e0a\u8d85\u8d8a\u4e86SOTA\u65b9\u6cd55%\u4ee5\u4e0a\u3002"}}
{"id": "2508.07766", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07766", "abs": "https://arxiv.org/abs/2508.07766", "authors": ["Jinke Li", "Jiarui Yu", "Chenxing Wei", "Hande Dong", "Qiang Lin", "Liangjing Yang", "Zhicai Wang", "Yanbin Hao"], "title": "UniSVG: A Unified Dataset for Vector Graphic Understanding and Generation with Multimodal Large Language Models", "comment": "Accepted at ACM MM 2025 Dataset Track", "summary": "Unlike bitmap images, scalable vector graphics (SVG) maintain quality when\nscaled, frequently employed in computer vision and artistic design in the\nrepresentation of SVG code. In this era of proliferating AI-powered systems,\nenabling AI to understand and generate SVG has become increasingly urgent.\nHowever, AI-driven SVG understanding and generation (U&G) remain significant\nchallenges. SVG code, equivalent to a set of curves and lines controlled by\nfloating-point parameters, demands high precision in SVG U&G. Besides, SVG\ngeneration operates under diverse conditional constraints, including textual\nprompts and visual references, which requires powerful multi-modal processing\nfor condition-to-SVG transformation. Recently, the rapid growth of Multi-modal\nLarge Language Models (MLLMs) have demonstrated capabilities to process\nmulti-modal inputs and generate complex vector controlling parameters,\nsuggesting the potential to address SVG U&G tasks within a unified model. To\nunlock MLLM's capabilities in the SVG area, we propose an SVG-centric dataset\ncalled UniSVG, comprising 525k data items, tailored for MLLM training and\nevaluation. To our best knowledge, it is the first comprehensive dataset\ndesigned for unified SVG generation (from textual prompts and images) and SVG\nunderstanding (color, category, usage, etc.). As expected, learning on the\nproposed dataset boosts open-source MLLMs' performance on various SVG U&G\ntasks, surpassing SOTA close-source MLLMs like GPT-4V. We release dataset,\nbenchmark, weights, codes and experiment details on\nhttps://ryanlijinke.github.io/.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86UniSVG\u6570\u636e\u96c6\uff0c\u4e00\u4e2a\u5305\u542b525K\u4e2a\u6570\u636e\u9879\u7684\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u8bad\u7ec3\u548c\u8bc4\u4f30\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728SVG\uff08\u53ef\u7f29\u653e\u77e2\u91cf\u56fe\u5f62\uff09\u7406\u89e3\u4e0e\u751f\u6210\u65b9\u9762\u7684\u80fd\u529b\u3002UniSVG\u6570\u636e\u96c6\u80fd\u591f\u63d0\u5347MLLMs\u5728SVG\u76f8\u5173\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u5728\u4e0eGPT-4V\u7b49\u95ed\u6e90\u6a21\u578b\u7684\u6bd4\u8f83\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "AI\u9a71\u52a8\u7684SVG\u7406\u89e3\u4e0e\u751f\u6210\uff08U&G\uff09\u662f\u4e00\u4e2a\u91cd\u5927\u6311\u6218\uff0c\u56e0\u4e3aSVG\u4ee3\u7801\u9700\u8981\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u4e14\u751f\u6210\u8fc7\u7a0b\u9700\u8981\u5904\u7406\u591a\u79cd\u6761\u4ef6\u7ea6\u675f\uff08\u5982\u6587\u672c\u63d0\u793a\u548c\u89c6\u89c9\u53c2\u8003\uff09\uff0c\u8fd9\u9700\u8981\u5f3a\u5927\u7684\u591a\u6a21\u6001\u5904\u7406\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u540d\u4e3aUniSVG\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b525K\u4e2a\u6570\u636e\u9879\uff0c\u7528\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u8bad\u7ec3\u548c\u8bc4\u4f30\uff0c\u4ee5\u89e3\u51b3SVG\u4ee3\u7801\u7684\u7406\u89e3\u4e0e\u751f\u6210\u95ee\u9898\u3002", "result": "UniSVG\u662f\u9996\u4e2a\u4e3a\u7edf\u4e00SVG\u751f\u6210\uff08\u4ece\u6587\u672c\u63d0\u793a\u548c\u56fe\u50cf\uff09\u548cSVG\u7406\u89e3\uff08\u989c\u8272\u3001\u7c7b\u522b\u3001\u7528\u6cd5\u7b49\uff09\u8bbe\u8ba1\u7684\u7efc\u5408\u6027\u6570\u636e\u96c6\u3002", "conclusion": "\u901a\u8fc7UniSVG\u6570\u636e\u96c6\u7684\u8bad\u7ec3\uff0c\u5f00\u6e90\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u5404\u79cdSVG\u7406\u89e3\u4e0e\u751f\u6210\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u5f97\u5230\u63d0\u5347\uff0c\u8d85\u8d8a\u4e86\u50cfGPT-4V\u8fd9\u6837\u7684\u95ed\u6e90\u6a21\u578b\u3002"}}
{"id": "2508.07769", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07769", "abs": "https://arxiv.org/abs/2508.07769", "authors": ["Xiaoyan Liu", "Kangrui Li", "Jiaxin Liu"], "title": "Dream4D: Lifting Camera-Controlled I2V towards Spatiotemporally Consistent 4D Generation", "comment": "Project Page: https://wanderer7-sk.github.io/Dream4D.github.io/", "summary": "The synthesis of spatiotemporally coherent 4D content presents fundamental\nchallenges in computer vision, requiring simultaneous modeling of high-fidelity\nspatial representations and physically plausible temporal dynamics. Current\napproaches often struggle to maintain view consistency while handling complex\nscene dynamics, particularly in large-scale environments with multiple\ninteracting elements. This work introduces Dream4D, a novel framework that\nbridges this gap through a synergy of controllable video generation and neural\n4D reconstruction. Our approach seamlessly combines a two-stage architecture:\nit first predicts optimal camera trajectories from a single image using\nfew-shot learning, then generates geometrically consistent multi-view sequences\nvia a specialized pose-conditioned diffusion process, which are finally\nconverted into a persistent 4D representation. This framework is the first to\nleverage both rich temporal priors from video diffusion models and geometric\nawareness of the reconstruction models, which significantly facilitates 4D\ngeneration and shows higher quality (e.g., mPSNR, mSSIM) over existing methods.", "AI": {"tldr": "Dream4D\u662f\u4e00\u4e2a\u65b0\u7684\u6846\u67b6\uff0c\u5229\u7528\u5c11\u6837\u672c\u5b66\u4e60\u548c\u59ff\u6001\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u65f6\u7a7a\u4e00\u81f4\u76844D\u5185\u5bb9\u3002", "motivation": "\u5f53\u524d\u8ba1\u7b97\u673a\u89c6\u89c9\u9886\u57df\u5728\u65f6\u7a7a\u4e00\u81f4\u76844D\u5185\u5bb9\u5408\u6210\u65b9\u9762\u9762\u4e34\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5927\u89c4\u6a21\u3001\u591a\u5143\u7d20\u4ea4\u4e92\u573a\u666f\u4e0b\uff0c\u9700\u8981\u540c\u65f6\u5efa\u6a21\u9ad8\u4fdd\u771f\u7a7a\u95f4\u8868\u793a\u548c\u7269\u7406\u4e0a\u5408\u7406\u7684\u65f6\u5e8f\u52a8\u6001\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u56fe\u4e00\u81f4\u6027\u3002", "method": "Dream4D\u6846\u67b6\u91c7\u7528\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a\u9996\u5148\u5229\u7528\u5c11\u6837\u672c\u5b66\u4e60\u4ece\u5355\u5f20\u56fe\u50cf\u9884\u6d4b\u6700\u4f18\u76f8\u673a\u8f68\u8ff9\uff0c\u7136\u540e\u901a\u8fc7\u4e13\u95e8\u7684\u59ff\u6001\u6761\u4ef6\u6269\u6563\u8fc7\u7a0b\u751f\u6210\u51e0\u4f55\u4e00\u81f4\u7684\u591a\u89c6\u56fe\u5e8f\u5217\uff0c\u6700\u540e\u5c06\u5e8f\u5217\u8f6c\u6362\u4e3a\u6301\u4e45\u76844D\u8868\u793a\u3002", "result": "Dream4D\u6846\u67b6\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u76844D\u5185\u5bb9\uff0c\u5e76\u5728mPSNR\u548cmSSIM\u7b49\u6307\u6807\u4e0a\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "conclusion": "Dream4D\u6846\u67b6\u9996\u6b21\u7ed3\u5408\u4e86\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e30\u5bcc\u65f6\u95f4\u5148\u9a8c\u548c\u91cd\u5efa\u6a21\u578b\u7684\u51e0\u4f55\u611f\u77e5\u80fd\u529b\uff0c\u663e\u8457\u4fc3\u8fdb\u4e864D\u5185\u5bb9\u7684\u751f\u6210\uff0c\u5e76\u5728\u591a\u4e2a\u6307\u6807\uff08\u5982mPSNR\u3001mSSIM\uff09\u4e0a\u5c55\u73b0\u51fa\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u9ad8\u8d28\u91cf\u3002"}}
{"id": "2508.07771", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07771", "abs": "https://arxiv.org/abs/2508.07771", "authors": ["Lei Wang", "Shiming Chen", "Guo-Sen Xie", "Ziming Hong", "Chaojian Yu", "Qinmu Peng", "Xinge You"], "title": "Prototype-Guided Curriculum Learning for Zero-Shot Learning", "comment": "12 pages, 7 figures", "summary": "In Zero-Shot Learning (ZSL), embedding-based methods enable knowledge\ntransfer from seen to unseen classes by learning a visual-semantic mapping from\nseen-class images to class-level semantic prototypes (e.g., attributes).\nHowever, these semantic prototypes are manually defined and may introduce noisy\nsupervision for two main reasons: (i) instance-level mismatch: variations in\nperspective, occlusion, and annotation bias will cause discrepancies between\nindividual sample and the class-level semantic prototypes; and (ii) class-level\nimprecision: the manually defined semantic prototypes may not accurately\nreflect the true semantics of the class. Consequently, the visual-semantic\nmapping will be misled, reducing the effectiveness of knowledge transfer to\nunseen classes. In this work, we propose a prototype-guided curriculum learning\nframework (dubbed as CLZSL), which mitigates instance-level mismatches through\na Prototype-Guided Curriculum Learning (PCL) module and addresses class-level\nimprecision via a Prototype Update (PUP) module. Specifically, the PCL module\nprioritizes samples with high cosine similarity between their visual mappings\nand the class-level semantic prototypes, and progressively advances to\nless-aligned samples, thereby reducing the interference of instance-level\nmismatches to achieve accurate visual-semantic mapping. Besides, the PUP module\ndynamically updates the class-level semantic prototypes by leveraging the\nvisual mappings learned from instances, thereby reducing class-level\nimprecision and further improving the visual-semantic mapping. Experiments were\nconducted on standard benchmark datasets-AWA2, SUN, and CUB-to verify the\neffectiveness of our method.", "AI": {"tldr": "CLZSL\u6846\u67b6\u901a\u8fc7PCL\u548cPUP\u6a21\u5757\u89e3\u51b3ZSL\u4e2d\u8bed\u4e49\u539f\u578b\u7684\u95ee\u9898\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u8fc1\u79fb\u6548\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u5d4c\u5165\u7684\u65b9\u6cd5\u5728\u96f6\u6837\u672c\u5b66\u4e60\uff08ZSL\uff09\u4e2d\uff0c\u5c06\u77e5\u8bc6\u4ece\u5df2\u77e5\u7c7b\u8fc1\u79fb\u5230\u672a\u77e5\u7c7b\uff0c\u4f46\u624b\u52a8\u5b9a\u4e49\u7684\u8bed\u4e49\u539f\u578b\u5b58\u5728\u5b9e\u4f8b\u7ea7\u522b\u4e0d\u5339\u914d\uff08\u6837\u672c\u4e0e\u539f\u578b\u4e0d\u7b26\uff09\u548c\u7c7b\u522b\u7ea7\u522b\u4e0d\u7cbe\u786e\uff08\u539f\u578b\u4e0d\u80fd\u51c6\u786e\u53cd\u6620\u7c7b\u522b\u771f\u5b9e\u8bed\u4e49\uff09\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u89c6\u89c9-\u8bed\u4e49\u6620\u5c04\u88ab\u8bef\u5bfc\uff0c\u964d\u4f4e\u4e86\u77e5\u8bc6\u8fc1\u79fb\u7684\u6709\u6548\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u539f\u578b\u6307\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\u6846\u67b6\uff08CLZSL\uff09\uff0c\u5305\u62ec\u539f\u578b\u6307\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\uff08PCL\uff09\u6a21\u5757\u548c\u539f\u578b\u66f4\u65b0\uff08PUP\uff09\u6a21\u5757\u3002PCL\u6a21\u5757\u901a\u8fc7\u4f18\u5148\u5904\u7406\u89c6\u89c9\u6620\u5c04\u4e0e\u7c7b\u522b\u539f\u578b\u5177\u6709\u9ad8\u4f59\u5f26\u76f8\u4f3c\u5ea6\u7684\u6837\u672c\uff0c\u5e76\u9010\u6b65\u5f15\u5165\u5bf9\u9f50\u5ea6\u8f83\u4f4e\u7684\u6837\u672c\uff0c\u6765\u51cf\u5c11\u5b9e\u4f8b\u7ea7\u522b\u4e0d\u5339\u914d\u7684\u5e72\u6270\u3002PUP\u6a21\u5757\u901a\u8fc7\u5229\u7528\u4ece\u5b9e\u4f8b\u5b66\u4e60\u5230\u7684\u89c6\u89c9\u6620\u5c04\uff0c\u52a8\u6001\u66f4\u65b0\u7c7b\u522b\u539f\u578b\uff0c\u4ee5\u51cf\u5c11\u7c7b\u522b\u7ea7\u522b\u7684\u4e0d\u7cbe\u786e\u6027\u3002", "result": "CLZSL\u6846\u67b6\u901a\u8fc7PCL\u548cPUP\u6a21\u5757\uff0c\u51cf\u5c11\u4e86\u5b9e\u4f8b\u7ea7\u522b\u4e0d\u5339\u914d\u548c\u7c7b\u522b\u7ea7\u522b\u4e0d\u7cbe\u786e\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u51c6\u786e\u7684\u89c6\u89c9-\u8bed\u4e49\u6620\u5c04\uff0c\u5e76\u5728AWA2\u3001SUN\u548cCUB\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u826f\u597d\u7684\u5b9e\u9a8c\u6548\u679c\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684CLZSL\u6846\u67b6\u901a\u8fc7\u539f\u578b\u6307\u5bfc\u7684\u8bfe\u7a0b\u5b66\u4e60\uff08PCL\uff09\u6a21\u5757\u548c\u539f\u578b\u66f4\u65b0\uff08PUP\uff09\u6a21\u5757\uff0c\u6709\u6548\u7f13\u89e3\u4e86\u5b9e\u4f8b\u7ea7\u522b\u4e0d\u5339\u914d\u548c\u7c7b\u522b\u7ea7\u522b\u4e0d\u7cbe\u786e\u7684\u95ee\u9898\uff0c\u63d0\u9ad8\u4e86\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u89c6\u89c9-\u8bed\u4e49\u6620\u5c04\u7684\u51c6\u786e\u6027\uff0c\u5e76\u5728AWA2\u3001SUN\u548cCUB\u7b49\u6807\u51c6\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002"}}
{"id": "2508.07775", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07775", "abs": "https://arxiv.org/abs/2508.07775", "authors": ["Lennart Bastian", "Mohammad Rashed", "Nassir Navab", "Tolga Birdal"], "title": "Forecasting Continuous Non-Conservative Dynamical Systems in SO(3)", "comment": "ICCV 2025 Oral", "summary": "Modeling the rotation of moving objects is a fundamental task in computer\nvision, yet $SO(3)$ extrapolation still presents numerous challenges: (1)\nunknown quantities such as the moment of inertia complicate dynamics, (2) the\npresence of external forces and torques can lead to non-conservative\nkinematics, and (3) estimating evolving state trajectories under sparse, noisy\nobservations requires robustness. We propose modeling trajectories of noisy\npose estimates on the manifold of 3D rotations in a physically and\ngeometrically meaningful way by leveraging Neural Controlled Differential\nEquations guided with $SO(3)$ Savitzky-Golay paths. Existing extrapolation\nmethods often rely on energy conservation or constant velocity assumptions,\nlimiting their applicability in real-world scenarios involving non-conservative\nforces. In contrast, our approach is agnostic to energy and momentum\nconservation while being robust to input noise, making it applicable to\ncomplex, non-inertial systems. Our approach is easily integrated as a module in\nexisting pipelines and generalizes well to trajectories with unknown physical\nparameters. By learning to approximate object dynamics from noisy states during\ntraining, our model attains robust extrapolation capabilities in simulation and\nvarious real-world settings. Code is available at\nhttps://github.com/bastianlb/forecasting-rotational-dynamics", "AI": {"tldr": "\u4e00\u4e2a\u5229\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\u548cSO(3) Savitzky-Golay\u8def\u5f84\u6765\u5916\u63a8\u4e09\u7ef4\u65cb\u8f6c\u7684\u65b0\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u5f88\u597d\u5730\u5904\u7406\u566a\u58f0\u548c\u975e\u4fdd\u5b88\u529b\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u7684\u975e\u4fdd\u5b88\u529b\u3001\u672a\u77e5\u7684\u7269\u7406\u53c2\u6570\u4ee5\u53ca\u5bf9\u566a\u58f0\u89c2\u6d4b\u503c\u7684\u9c81\u68d2\u6027\u65b9\u9762\u5b58\u5728\u6311\u6218\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u4e09\u7ef4\u65cb\u8f6c\u6d41\u5f62\u4e0a\u5bf9\u566a\u58f0\u4f4d\u59ff\u4f30\u8ba1\u8f68\u8ff9\u8fdb\u884c\u5efa\u6a21\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u795e\u7ecf\u63a7\u5236\u5fae\u5206\u65b9\u7a0b\uff0c\u5e76\u7ed3\u5408SO(3) Savitzky-Golay\u8def\u5f84\u8fdb\u884c\u5f15\u5bfc\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6a21\u62df\u548c\u73b0\u5b9e\u4e16\u754c\u8bbe\u7f6e\u4e2d\u90fd\u5177\u6709\u5f3a\u5927\u7684\u5916\u63a8\u80fd\u529b\uff0c\u5e76\u4e14\u53ef\u4ee5\u5f88\u597d\u5730\u6cdb\u5316\u5230\u5177\u6709\u672a\u77e5\u7269\u7406\u53c2\u6570\u7684\u8f68\u8ff9\u3002 ", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u5f88\u597d\u5730\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u5b58\u5728\u7684\u672a\u77e5\u7684\u7269\u7406\u53c2\u6570\uff0c\u5e76\u4e14\u5728\u6a21\u62df\u548c\u5404\u79cd\u73b0\u5b9e\u573a\u666f\u4e2d\u90fd\u8868\u73b0\u51fa\u4e86\u5f3a\u5927\u7684\u5916\u63a8\u80fd\u529b\u3002"}}
{"id": "2508.07782", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07782", "abs": "https://arxiv.org/abs/2508.07782", "authors": ["Saihui Hou", "Chenye Wang", "Wenpeng Lang", "Zhengxiang Lan", "Yongzhen Huang"], "title": "GaitSnippet: Gait Recognition Beyond Unordered Sets and Ordered Sequences", "comment": "13 pages, 5 figures", "summary": "Recent advancements in gait recognition have significantly enhanced\nperformance by treating silhouettes as either an unordered set or an ordered\nsequence. However, both set-based and sequence-based approaches exhibit notable\nlimitations. Specifically, set-based methods tend to overlook short-range\ntemporal context for individual frames, while sequence-based methods struggle\nto capture long-range temporal dependencies effectively. To address these\nchallenges, we draw inspiration from human identification and propose a new\nperspective that conceptualizes human gait as a composition of individualized\nactions. Each action is represented by a series of frames, randomly selected\nfrom a continuous segment of the sequence, which we term a snippet.\nFundamentally, the collection of snippets for a given sequence enables the\nincorporation of multi-scale temporal context, facilitating more comprehensive\ngait feature learning. Moreover, we introduce a non-trivial solution for\nsnippet-based gait recognition, focusing on Snippet Sampling and Snippet\nModeling as key components. Extensive experiments on four widely-used gait\ndatasets validate the effectiveness of our proposed approach and, more\nimportantly, highlight the potential of gait snippets. For instance, our method\nachieves the rank-1 accuracy of 77.5% on Gait3D and 81.7% on GREW using a 2D\nconvolution-based backbone.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\uff0c\u5c06\u6b65\u6001\u89c6\u4e3a\u7531\u968f\u673a\u5e27\u7ec4\u6210\u7684\u201c\u6b65\u6001\u7247\u6bb5\u201d\u7684\u7ec4\u5408\uff0c\u4ee5\u6355\u6349\u591a\u5c3a\u5ea6\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u96c6\u5408\u548c\u57fa\u4e8e\u5e8f\u5217\u7684\u6b65\u6001\u8bc6\u522b\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u57fa\u4e8e\u96c6\u5408\u7684\u65b9\u6cd5\u5ffd\u7565\u4e86\u5e27\u7684\u77ed\u671f\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u800c\u57fa\u4e8e\u5e8f\u5217\u7684\u65b9\u6cd5\u96be\u4ee5\u6709\u6548\u6355\u6349\u957f\u671f\u65f6\u95f4\u4f9d\u8d56\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6b65\u6001\u8bc6\u522b\u89c6\u89d2\uff0c\u5c06\u6b65\u6001\u89c6\u4e3a\u4e2a\u4f53\u5316\u52a8\u4f5c\u7684\u7ec4\u5408\uff0c\u5e76\u5f15\u5165\u4e86\u6b65\u6001\u7247\u6bb5\uff08snippet\uff09\u7684\u6982\u5ff5\u3002\u901a\u8fc7\u968f\u673a\u9009\u53d6\u8fde\u7eed\u7247\u6bb5\u4e2d\u7684\u5e27\u6765\u6784\u6210\u7247\u6bb5\uff0c\u4ece\u800c\u7ed3\u5408\u591a\u5c3a\u5ea6\u65f6\u95f4\u4e0a\u4e0b\u6587\uff0c\u4fc3\u8fdb\u66f4\u5168\u9762\u7684\u6b65\u6001\u7279\u5f81\u5b66\u4e60\u3002\u5177\u4f53\u5b9e\u73b0\u4e0a\uff0c\u91cd\u70b9\u5173\u6ce8\u4e86\u7247\u6bb5\u91c7\u6837\u548c\u7247\u6bb5\u5efa\u6a21\u3002", "result": "\u5728\u56db\u4e2a\u5e7f\u6cdb\u4f7f\u7528\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u9a8c\u8bc1\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\uff08\u57fa\u4e8e\u6b65\u6001\u7247\u6bb5\uff09\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u5728Gait3D\u548cGREW\u6570\u636e\u96c6\u4e0a\u5206\u522b\u53d6\u5f97\u4e8677.5%\u548c81.7%\u7684\u79e9\u4e00\u51c6\u786e\u7387\uff0c\u8bc1\u660e\u4e86\u6b65\u6001\u7247\u6bb5\u7684\u6709\u6548\u6027\u548c\u6f5c\u529b\u3002"}}
{"id": "2508.07788", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07788", "abs": "https://arxiv.org/abs/2508.07788", "authors": ["Runze Wang", "Zeli Chen", "Zhiyun Song", "Wei Fang", "Jiajin Zhang", "Danyang Tu", "Yuxing Tang", "Minfeng Xu", "Xianghua Ye", "Le Lu", "Dakai Jin"], "title": "Anatomy-Aware Low-Dose CT Denoising via Pretrained Vision Models and Semantic-Guided Contrastive Learning", "comment": null, "summary": "To reduce radiation exposure and improve the diagnostic efficacy of low-dose\ncomputed tomography (LDCT), numerous deep learning-based denoising methods have\nbeen developed to mitigate noise and artifacts. However, most of these\napproaches ignore the anatomical semantics of human tissues, which may\npotentially result in suboptimal denoising outcomes. To address this problem,\nwe propose ALDEN, an anatomy-aware LDCT denoising method that integrates\nsemantic features of pretrained vision models (PVMs) with adversarial and\ncontrastive learning. Specifically, we introduce an anatomy-aware discriminator\nthat dynamically fuses hierarchical semantic features from reference\nnormal-dose CT (NDCT) via cross-attention mechanisms, enabling tissue-specific\nrealism evaluation in the discriminator. In addition, we propose a\nsemantic-guided contrastive learning module that enforces anatomical\nconsistency by contrasting PVM-derived features from LDCT, denoised CT and\nNDCT, preserving tissue-specific patterns through positive pairs and\nsuppressing artifacts via dual negative pairs. Extensive experiments conducted\non two LDCT denoising datasets reveal that ALDEN achieves the state-of-the-art\nperformance, offering superior anatomy preservation and substantially reducing\nover-smoothing issue of previous work. Further validation on a downstream\nmulti-organ segmentation task (encompassing 117 anatomical structures) affirms\nthe model's ability to maintain anatomical awareness.", "AI": {"tldr": "ALDEN\u662f\u4e00\u79cd\u89e3\u5256\u7ed3\u6784\u611f\u77e5\u7684\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\uff0c\u5b83\u7ed3\u5408\u4e86\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\u7684\u8bed\u4e49\u7279\u5f81\u3001\u5bf9\u6297\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u4ee5\u63d0\u9ad8\u53bb\u566a\u6548\u679c\u5e76\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u5b8c\u6574\u6027\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cALDEN\u5728\u53bb\u566a\u548c\u4e0b\u6e38\u5206\u5272\u4efb\u52a1\u4e0a\u5747\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u591a\u6570\u73b0\u6709\u7684\u6df1\u5ea6\u5b66\u4e60\u53bb\u566a\u65b9\u6cd5\u5ffd\u7565\u4e86\u4eba\u4f53\u7ec4\u7ec7\u7684\u89e3\u5256\u8bed\u4e49\u4fe1\u606f\uff0c\u8fd9\u53ef\u80fd\u5bfc\u81f4\u6b21\u4f18\u7684\u53bb\u566a\u7ed3\u679c\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aALDEN\u7684\u89e3\u5256\u7ed3\u6784\u611f\u77e5\u7684\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\u3002", "method": "ALDEN\u662f\u4e00\u79cd\u89e3\u5256\u7ed3\u6784\u611f\u77e5\u7684\u4f4e\u5242\u91cfCT\u53bb\u566a\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u96c6\u6210\u9884\u8bad\u7ec3\u89c6\u89c9\u6a21\u578b\uff08PVM\uff09\u7684\u8bed\u4e49\u7279\u5f81\uff0c\u5e76\u7ed3\u5408\u5bf9\u6297\u5b66\u4e60\u548c\u5bf9\u6bd4\u5b66\u4e60\u6765\u5b9e\u73b0\u3002\u5177\u4f53\u800c\u8a00\uff0c\u8be5\u65b9\u6cd5\u5f15\u5165\u4e86\u4e00\u4e2a\u89e3\u5256\u7ed3\u6784\u611f\u77e5\u7684\u5224\u522b\u5668\uff0c\u901a\u8fc7\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u52a8\u6001\u878d\u5408\u6765\u81ea\u53c2\u8003\u6b63\u5e38\u5242\u91cfCT\uff08NDCT\uff09\u7684\u5c42\u6b21\u5316\u8bed\u4e49\u7279\u5f81\uff0c\u4ece\u800c\u5728\u5224\u522b\u5668\u4e2d\u5b9e\u73b0\u7279\u5b9a\u7ec4\u7ec7\u7684\u771f\u5b9e\u611f\u8bc4\u4f30\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8bed\u4e49\u5f15\u5bfc\u7684\u5bf9\u6bd4\u5b66\u4e60\u6a21\u5757\uff0c\u901a\u8fc7\u5bf9\u6bd4\u6765\u81eaLDCT\u3001\u53bb\u566aCT\u548cNDCT\u7684PVM\u7279\u5f81\uff0c\u5f3a\u5236\u6267\u884c\u89e3\u5256\u7ed3\u6784\u7684\u4e00\u81f4\u6027\uff0c\u5e76\u901a\u8fc7\u6b63\u6837\u672c\u5bf9\u4fdd\u7559\u7279\u5b9a\u7ec4\u7ec7\u7684\u6a21\u5f0f\uff0c\u5e76\u901a\u8fc7\u53cc\u8d1f\u6837\u672c\u5bf9\u6291\u5236\u4f2a\u5f71\u3002", "result": "ALDEN\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u80fd\u529b\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u8fc7\u5ea6\u5e73\u6ed1\u7684\u95ee\u9898\u3002", "conclusion": "ALDEN\u5728\u4e24\u4e2a\u4f4e\u5242\u91cfCT\u53bb\u566a\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cALDEN\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u4f9b\u4e86\u5353\u8d8a\u7684\u89e3\u5256\u7ed3\u6784\u4fdd\u6301\u80fd\u529b\uff0c\u5e76\u663e\u8457\u51cf\u5c11\u4e86\u5148\u524d\u5de5\u4f5c\u4e2d\u8fc7\u5ea6\u5e73\u6ed1\u7684\u95ee\u9898\u3002\u5728\u4e0b\u6e38\u591a\u5668\u5b98\u5206\u5272\u4efb\u52a1\uff08\u5305\u542b117\u4e2a\u89e3\u5256\u7ed3\u6784\uff09\u4e0a\u7684\u8fdb\u4e00\u6b65\u9a8c\u8bc1\uff0c\u8bc1\u5b9e\u4e86\u8be5\u6a21\u578b\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u611f\u77e5\u7684\u80fd\u529b\u3002"}}
{"id": "2508.07795", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07795", "abs": "https://arxiv.org/abs/2508.07795", "authors": ["Hongrui Zheng", "Yuezun Li", "Liejun Wang", "Yunfeng Diao", "Zhiqing Guo"], "title": "Boosting Active Defense Persistence: A Two-Stage Defense Framework Combining Interruption and Poisoning Against Deepfake", "comment": null, "summary": "Active defense strategies have been developed to counter the threat of\ndeepfake technology. However, a primary challenge is their lack of persistence,\nas their effectiveness is often short-lived. Attackers can bypass these\ndefenses by simply collecting protected samples and retraining their models.\nThis means that static defenses inevitably fail when attackers retrain their\nmodels, which severely limits practical use. We argue that an effective defense\nnot only distorts forged content but also blocks the model's ability to adapt,\nwhich occurs when attackers retrain their models on protected images. To\nachieve this, we propose an innovative Two-Stage Defense Framework (TSDF).\nBenefiting from the intensity separation mechanism designed in this paper, the\nframework uses dual-function adversarial perturbations to perform two roles.\nFirst, it can directly distort the forged results. Second, it acts as a\npoisoning vehicle that disrupts the data preparation process essential for an\nattacker's retraining pipeline. By poisoning the data source, TSDF aims to\nprevent the attacker's model from adapting to the defensive perturbations, thus\nensuring the defense remains effective long-term. Comprehensive experiments\nshow that the performance of traditional interruption methods degrades sharply\nwhen it is subjected to adversarial retraining. However, our framework shows a\nstrong dual defense capability, which can improve the persistence of active\ndefense. Our code will be available at https://github.com/vpsg-research/TSDF.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e24\u9636\u6bb5\u9632\u5fa1\u6846\u67b6\uff08TSDF\uff09\uff0c\u901a\u8fc7\u626d\u66f2\u4f2a\u9020\u5185\u5bb9\u548c\u6bd2\u5316\u6570\u636e\u6e90\u6765\u9632\u6b62\u653b\u51fb\u8005\u6a21\u578b\u9002\u5e94\uff0c\u4ece\u800c\u5b9e\u73b0\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u6301\u4e45\u7684\u6df1\u5ea6\u4f2a\u9020\u9632\u5fa1\u3002", "motivation": "\u73b0\u6709\u7684\u4e3b\u52a8\u9632\u5fa1\u7b56\u7565\u5728\u5e94\u5bf9\u6df1\u5ea6\u4f2a\u9020\u6280\u672f\u65b9\u9762\u7f3a\u4e4f\u6301\u4e45\u6027\uff0c\u5bb9\u6613\u88ab\u653b\u51fb\u8005\u901a\u8fc7\u6536\u96c6\u53d7\u4fdd\u62a4\u6837\u672c\u5e76\u91cd\u65b0\u8bad\u7ec3\u6a21\u578b\u6765\u7ed5\u8fc7\u3002\u8fd9\u79cd\u9759\u6001\u9632\u5fa1\u5728\u9762\u5bf9\u6a21\u578b\u518d\u8bad\u7ec3\u65f6\u4f1a\u5931\u6548\uff0c\u4e25\u91cd\u9650\u5236\u4e86\u5176\u5b9e\u9645\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u521b\u65b0\u7684\u4e24\u9636\u6bb5\u9632\u5fa1\u6846\u67b6\uff08TSDF\uff09\uff0c\u5229\u7528\u4e86\u5f3a\u5ea6\u5206\u79bb\u673a\u5236\u548c\u53cc\u529f\u80fd\u5bf9\u6297\u6027\u6270\u52a8\u3002\u8be5\u6270\u52a8\u80fd\u591f\u76f4\u63a5\u626d\u66f2\u4f2a\u9020\u5185\u5bb9\uff0c\u5e76\u4f5c\u4e3a\u4e00\u79cd\u6bd2\u5316\u8f7d\u4f53\uff0c\u7834\u574f\u653b\u51fb\u8005\u518d\u8bad\u7ec3\u6d41\u7a0b\u6240\u5fc5\u9700\u7684\u6570\u636e\u51c6\u5907\u8fc7\u7a0b\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u4e0e\u4f20\u7edf\u4e2d\u65ad\u65b9\u6cd5\u5728\u5bf9\u6297\u6027\u518d\u8bad\u7ec3\u4e0b\u7684\u6027\u80fd\u6025\u5267\u4e0b\u964d\u4e0d\u540c\uff0cTSDF\u5c55\u73b0\u4e86\u5f3a\u5927\u7684\u53cc\u91cd\u9632\u5fa1\u80fd\u529b\uff0c\u63d0\u9ad8\u4e86\u4e3b\u52a8\u9632\u5fa1\u7684\u6301\u4e45\u6027\u3002", "conclusion": "\u8be5\u9632\u5fa1\u6846\u67b6\u901a\u8fc7\u5728\u6570\u636e\u6e90\u4e2d\u5f15\u5165\u6bd2\u6027\uff0c\u65e8\u5728\u963b\u6b62\u653b\u51fb\u8005\u7684\u6a21\u578b\u9002\u5e94\u9632\u5fa1\u6027\u6270\u52a8\uff0c\u4ece\u800c\u786e\u4fdd\u9632\u5fa1\u7684\u957f\u671f\u6709\u6548\u6027\u3002"}}
{"id": "2508.07797", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07797", "abs": "https://arxiv.org/abs/2508.07797", "authors": ["Xiaoqi Zhao", "Peiqian Cao", "Lihe Zhang", "Zonglei Feng", "Hanqi Liu", "Jiaming Zuo", "Youwei Pang", "Weisi Lin", "Georges El Fakhri", "Huchuan Lu", "Xiaofeng Liu"], "title": "Power Battery Detection", "comment": "Under submission to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (T-PAMI)", "summary": "Power batteries are essential components in electric vehicles, where internal\nstructural defects can pose serious safety risks. We conduct a comprehensive\nstudy on a new task, power battery detection (PBD), which aims to localize the\ndense endpoints of cathode and anode plates from industrial X-ray images for\nquality inspection. Manual inspection is inefficient and error-prone, while\ntraditional vision algorithms struggle with densely packed plates, low\ncontrast, scale variation, and imaging artifacts. To address this issue and\ndrive more attention into this meaningful task, we present PBD5K, the first\nlarge-scale benchmark for this task, consisting of 5,000 X-ray images from nine\nbattery types with fine-grained annotations and eight types of real-world\nvisual interference. To support scalable and consistent labeling, we develop an\nintelligent annotation pipeline that combines image filtering, model-assisted\npre-labeling, cross-verification, and layered quality evaluation. We formulate\nPBD as a point-level segmentation problem and propose MDCNeXt, a model designed\nto extract and integrate multi-dimensional structure clues including point,\nline, and count information from the plate itself. To improve discrimination\nbetween plates and suppress visual interference, MDCNeXt incorporates two state\nspace modules. The first is a prompt-filtered module that learns contrastive\nrelationships guided by task-specific prompts. The second is a density-aware\nreordering module that refines segmentation in regions with high plate density.\nIn addition, we propose a distance-adaptive mask generation strategy to provide\nrobust supervision under varying spatial distributions of anode and cathode\npositions. The source code and datasets will be publicly available at\n\\href{https://github.com/Xiaoqi-Zhao-DLUT/X-ray-PBD}{PBD5K}.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7528\u4e8e\u52a8\u529b\u7535\u6c60X\u5c04\u7ebf\u56fe\u50cf\u7f3a\u9677\u68c0\u6d4b\u7684PBD5K\u6570\u636e\u96c6\u548cMDCNeXt\u6a21\u578b\u3002MDCNeXt\u901a\u8fc7\u591a\u7ef4\u5ea6\u7ed3\u6784\u7ebf\u7d22\u63d0\u53d6\u548c\u4e13\u95e8\u8bbe\u8ba1\u7684\u6a21\u5757\uff0c\u63d0\u9ad8\u4e86\u68c0\u6d4b\u7cbe\u5ea6\u548c\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u5904\u7406\u5bc6\u96c6\u6781\u8033\u548c\u89c6\u89c9\u5e72\u6270\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "motivation": "\u7535\u52a8\u6c7d\u8f66\u7684\u52a8\u529b\u7535\u6c60\u5185\u90e8\u7ed3\u6784\u7f3a\u9677\u53ef\u80fd\u5f15\u53d1\u4e25\u91cd\u7684\u5b89\u5168\u95ee\u9898\u3002\u624b\u52a8\u68c0\u6d4b\u6548\u7387\u4f4e\u4e0b\u4e14\u6613\u51fa\u9519\uff0c\u800c\u4f20\u7edf\u7684\u89c6\u89c9\u7b97\u6cd5\u96be\u4ee5\u5904\u7406\u5bc6\u96c6\u6392\u5217\u7684\u6781\u8033\u3001\u4f4e\u5bf9\u6bd4\u5ea6\u3001\u5c3a\u5ea6\u53d8\u5316\u548c\u6210\u50cf\u4f2a\u5f71\u7b49\u95ee\u9898\u3002\u56e0\u6b64\uff0c\u6709\u5fc5\u8981\u7814\u7a76\u65b0\u7684\u81ea\u52a8\u5316\u68c0\u6d4b\u65b9\u6cd5\u6765\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\u3002", "method": "\u7814\u7a76\u63d0\u51fa\u4e86PBD5K\u6570\u636e\u96c6\u548cMDCNeXt\u6a21\u578b\u3002PBD5K\u6570\u636e\u96c6\u5305\u542b5000\u5f20\u52a8\u529b\u7535\u6c60X\u5c04\u7ebf\u56fe\u50cf\uff0c\u5e76\u8fdb\u884c\u4e86\u7ec6\u7c92\u5ea6\u6807\u6ce8\u548c\u5e72\u6270\u9879\u6807\u6ce8\u3002MDCNeXt\u6a21\u578b\u5c06PBD\u4efb\u52a1\u5efa\u6a21\u4e3a\u70b9\u7ea7\u5206\u5272\u95ee\u9898\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5305\u542b\u70b9\u3001\u7ebf\u3001\u8ba1\u6570\u4fe1\u606f\u63d0\u53d6\u7684\u591a\u7ef4\u5ea6\u7ed3\u6784\u7ebf\u7d22\u6574\u5408\u65b9\u6cd5\uff0c\u5e76\u5f15\u5165\u4e86\u63d0\u793a\u8fc7\u6ee4\u6a21\u5757\u548c\u5bc6\u5ea6\u611f\u77e5\u91cd\u6392\u5e8f\u6a21\u5757\u6765\u63d0\u9ad8\u6a21\u578b\u6027\u80fd\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8ddd\u79bb\u81ea\u9002\u5e94\u63a9\u7801\u751f\u6210\u7b56\u7565\u4f5c\u4e3a\u6a21\u578b\u7684\u76d1\u7763\u3002", "result": "MDCNeXt\u5728PBD5K\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u5176\u4ed6\u5bf9\u6bd4\u65b9\u6cd5\u7684\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5730\u4eceX\u5c04\u7ebf\u56fe\u50cf\u4e2d\u63d0\u53d6\u548c\u6574\u5408\u591a\u7ef4\u5ea6\u7ed3\u6784\u7ebf\u7d22\uff0c\u5e76\u80fd\u5904\u7406\u5bc6\u96c6\u6392\u5217\u7684\u6781\u8033\u548c\u89c6\u89c9\u5e72\u6270\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86PBD5K\u57fa\u51c6\u548cMDCNeXt\u6a21\u578b\uff0c\u7528\u4e8e\u89e3\u51b3\u52a8\u529b\u7535\u6c60X\u5c04\u7ebf\u56fe\u50cf\u4e2d\u7684\u7ed3\u6784\u7f3a\u9677\u68c0\u6d4b\u95ee\u9898\u3002PBD5K\u662f\u9996\u4e2a\u5927\u89c4\u6a21\u7684\u8be5\u4efb\u52a1\u57fa\u51c6\uff0c\u5305\u542b5000\u5f20X\u5c04\u7ebf\u56fe\u50cf\u548c\u7ec6\u7c92\u5ea6\u6807\u6ce8\u3002MDCNeXt\u6a21\u578b\u901a\u8fc7\u63d0\u53d6\u548c\u6574\u5408\u70b9\u3001\u7ebf\u3001\u8ba1\u6570\u7b49\u591a\u7ef4\u5ea6\u7ed3\u6784\u7ebf\u7d22\uff0c\u5e76\u7ed3\u5408\u4e86\u7528\u4e8e\u5b66\u4e60\u5bf9\u6bd4\u5173\u7cfb\u7684\u63d0\u793a\u8fc7\u6ee4\u6a21\u5757\u548c\u7528\u4e8e\u5904\u7406\u9ad8\u5bc6\u5ea6\u533a\u57df\u7684\u5bc6\u5ea6\u611f\u77e5\u91cd\u6392\u5e8f\u6a21\u5757\uff0c\u4ee5\u63d0\u9ad8\u5bf9\u7f3a\u9677\u7684\u68c0\u6d4b\u80fd\u529b\u5e76\u6291\u5236\u89c6\u89c9\u5e72\u6270\u3002\u7814\u7a76\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u8ddd\u79bb\u81ea\u9002\u5e94\u63a9\u7801\u751f\u6210\u7b56\u7565\uff0c\u4e3a\u6a21\u578b\u63d0\u4f9b\u9c81\u68d2\u7684\u76d1\u7763\u3002"}}
{"id": "2508.07803", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07803", "abs": "https://arxiv.org/abs/2508.07803", "authors": ["Yushen Xu", "Xiaosong Li", "Zhenyu Kuang", "Xiaoqi Cheng", "Haishu Tan", "Huafeng Li"], "title": "MambaTrans: Multimodal Fusion Image Translation via Large Language Model Priors for Downstream Visual Tasks", "comment": null, "summary": "The goal of multimodal image fusion is to integrate complementary information\nfrom infrared and visible images, generating multimodal fused images for\ndownstream tasks. Existing downstream pre-training models are typically trained\non visible images. However, the significant pixel distribution differences\nbetween visible and multimodal fusion images can degrade downstream task\nperformance, sometimes even below that of using only visible images. This paper\nexplores adapting multimodal fused images with significant modality differences\nto object detection and semantic segmentation models trained on visible images.\nTo address this, we propose MambaTrans, a novel multimodal fusion image\nmodality translator. MambaTrans uses descriptions from a multimodal large\nlanguage model and masks from semantic segmentation models as input. Its core\ncomponent, the Multi-Model State Space Block, combines mask-image-text\ncross-attention and a 3D-Selective Scan Module, enhancing pure visual\ncapabilities. By leveraging object detection prior knowledge, MambaTrans\nminimizes detection loss during training and captures long-term dependencies\namong text, masks, and images. This enables favorable results in pre-trained\nmodels without adjusting their parameters. Experiments on public datasets show\nthat MambaTrans effectively improves multimodal image performance in downstream\ntasks.", "AI": {"tldr": "MambaTrans translates multimodal fused images for downstream tasks by using LLM descriptions and segmentation masks, improving performance without retraining.", "motivation": "Existing downstream pre-training models are typically trained on visible images. However, the significant pixel distribution differences between visible and multimodal fusion images can degrade downstream task performance, sometimes even below that of using only visible images. This paper explores adapting multimodal fused images with significant modality differences to object detection and semantic segmentation models trained on visible images.", "method": "MambaTrans, a novel multimodal fusion image modality translator, uses descriptions from a multimodal large language model and masks from semantic segmentation models as input. Its core component, the Multi-Model State Space Block, combines mask-image-text cross-attention and a 3D-Selective Scan Module, enhancing pure visual capabilities. By leveraging object detection prior knowledge, MambaTrans minimizes detection loss during training and captures long-term dependencies among text, masks, and images.", "result": "Experiments on public datasets show that MambaTrans effectively improves multimodal image performance in downstream tasks.", "conclusion": "MambaTrans effectively improves multimodal image performance in downstream tasks, enabling favorable results in pre-trained models without adjusting their parameters."}}
{"id": "2508.07804", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07804", "abs": "https://arxiv.org/abs/2508.07804", "authors": ["Bao Li", "Xiaomei Zhang", "Miao Xu", "Zhaoxin Fan", "Xiangyu Zhu", "Zhen Lei"], "title": "Pose-RFT: Enhancing MLLMs for 3D Pose Generation via Hybrid Action Reinforcement Fine-Tuning", "comment": null, "summary": "Generating 3D human poses from multimodal inputs such as images or text\nrequires models to capture both rich spatial and semantic correspondences.\nWhile pose-specific multimodal large language models (MLLMs) have shown promise\nin this task, they are typically trained with supervised objectives such as\nSMPL parameter regression or token-level prediction, which struggle to model\nthe inherent ambiguity and achieve task-specific alignment required for\naccurate 3D pose generation. To address these limitations, we propose Pose-RFT,\na reinforcement fine-tuning framework tailored for 3D human pose generation in\nMLLMs. We formulate the task as a hybrid action reinforcement learning problem\nthat jointly optimizes discrete language prediction and continuous pose\ngeneration. To this end, we introduce HyGRPO, a hybrid reinforcement learning\nalgorithm that performs group-wise reward normalization over sampled responses\nto guide joint optimization of discrete and continuous actions. Pose-RFT\nfurther incorporates task-specific reward functions to guide optimization\ntowards spatial alignment in image-to-pose generation and semantic consistency\nin text-to-pose generation. Extensive experiments on multiple pose generation\nbenchmarks demonstrate that Pose-RFT significantly improves performance over\nexisting pose-specific MLLMs, validating the effectiveness of hybrid action\nreinforcement fine-tuning for 3D pose generation.", "AI": {"tldr": "Pose-RFT\u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u548c\u6df7\u5408\u52a8\u4f5c\u4f18\u5316\uff0c\u63d0\u5347\u4e86\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u751f\u62103D\u4eba\u4f53\u59ff\u6001\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u57fa\u4e8e\u76d1\u7763\u5b66\u4e60\u7684\u59ff\u6001\u7279\u5b9a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u57283D\u4eba\u4f53\u59ff\u6001\u751f\u6210\u4efb\u52a1\u4e2d\uff0c\u7531\u4e8e\u96be\u4ee5\u5904\u7406\u56fa\u6709\u7684\u6a21\u7cca\u6027\u548c\u5b9e\u73b0\u4efb\u52a1\u7279\u5b9a\u7684\u5bf9\u9f50\uff0c\u6548\u679c\u53d7\u5230\u9650\u5236\u3002", "method": "Pose-RFT\u6846\u67b6\uff0c\u91c7\u7528\u6df7\u5408\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\uff0c\u5f15\u5165HyGRPO\u7b97\u6cd5\u8fdb\u884c\u8054\u5408\u4f18\u5316\uff0c\u5e76\u7ed3\u5408\u7a7a\u95f4\u5bf9\u9f50\u548c\u8bed\u4e49\u4e00\u81f4\u6027\u7684\u4efb\u52a1\u7279\u5b9a\u5956\u52b1\u51fd\u6570\u3002", "result": "Pose-RFT\u5728\u591a\u4e2a3D\u4eba\u4f53\u59ff\u6001\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u76f8\u6bd4\u4e8e\u73b0\u6709\u7684\u59ff\u6001\u7279\u5b9aMLLMs\uff0c\u6027\u80fd\u5f97\u5230\u4e86\u663e\u8457\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u6df7\u5408\u52a8\u4f5c\u5f3a\u5316\u5fae\u8c03\u57283D\u59ff\u6001\u751f\u6210\u65b9\u9762\u7684\u6709\u6548\u6027\u3002", "conclusion": "Pose-RFT\u662f\u4e00\u4e2a\u4e3a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u91cf\u8eab\u5b9a\u5236\u7684\u3001\u7528\u4e8e3D\u4eba\u4f53\u59ff\u6001\u751f\u6210\u4efb\u52a1\u7684\u5f3a\u5316\u5fae\u8c03\u6846\u67b6\u3002\u901a\u8fc7\u5c06\u4efb\u52a1\u8868\u8ff0\u4e3a\u6df7\u5408\u52a8\u4f5c\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u5f15\u5165HyGRPO\u7b97\u6cd5\u6765\u8054\u5408\u4f18\u5316\u79bb\u6563\u8bed\u8a00\u9884\u6d4b\u548c\u8fde\u7eed\u59ff\u6001\u751f\u6210\uff0c\u4ee5\u53ca\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u7684\u5956\u52b1\u51fd\u6570\uff0cPose-RFT\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u6a21\u578b\u3002"}}
{"id": "2508.07811", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07811", "abs": "https://arxiv.org/abs/2508.07811", "authors": ["Sicheng Gao", "Nancy Mehta", "Zongwei Wu", "Radu Timofte"], "title": "DiTVR: Zero-Shot Diffusion Transformer for Video Restoration", "comment": "7 pages, 6 figures", "summary": "Video restoration aims to reconstruct high quality video sequences from low\nquality inputs, addressing tasks such as super resolution, denoising, and\ndeblurring. Traditional regression based methods often produce unrealistic\ndetails and require extensive paired datasets, while recent generative\ndiffusion models face challenges in ensuring temporal consistency. We introduce\nDiTVR, a zero shot video restoration framework that couples a diffusion\ntransformer with trajectory aware attention and a wavelet guided, flow\nconsistent sampler. Unlike prior 3D convolutional or frame wise diffusion\napproaches, our attention mechanism aligns tokens along optical flow\ntrajectories, with particular emphasis on vital layers that exhibit the highest\nsensitivity to temporal dynamics. A spatiotemporal neighbour cache dynamically\nselects relevant tokens based on motion correspondences across frames. The flow\nguided sampler injects data consistency only into low-frequency bands,\npreserving high frequency priors while accelerating convergence. DiTVR\nestablishes a new zero shot state of the art on video restoration benchmarks,\ndemonstrating superior temporal consistency and detail preservation while\nremaining robust to flow noise and occlusions.", "AI": {"tldr": "DiTVR\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u89c6\u9891\u6062\u590d\u6846\u67b6\uff0c\u901a\u8fc7\u8f68\u8ff9\u611f\u77e5\u6ce8\u610f\u529b\u548c\u6d41\u4e00\u81f4\u91c7\u6837\u5668\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u89c6\u9891\u6062\u590d\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u3002", "motivation": "\u4f20\u7edf\u7684\u57fa\u4e8e\u56de\u5f52\u7684\u65b9\u6cd5\u901a\u5e38\u4f1a\u4ea7\u751f\u4e0d\u5207\u5b9e\u9645\u7684\u7ec6\u8282\uff0c\u5e76\u4e14\u9700\u8981\u5927\u91cf\u7684\u914d\u5bf9\u6570\u636e\u96c6\uff0c\u800c\u6700\u8fd1\u7684\u751f\u6210\u6269\u6563\u6a21\u578b\u5728\u4fdd\u8bc1\u65f6\u95f4\u4e00\u81f4\u6027\u65b9\u9762\u9762\u4e34\u6311\u6218\u3002", "method": "DiTVR\u662f\u4e00\u4e2a\u96f6\u6837\u672c\u89c6\u9891\u6062\u590d\u6846\u67b6\uff0c\u5b83\u5c06\u6269\u6563\u53d8\u6362\u5668\u4e0e\u8f68\u8ff9\u611f\u77e5\u6ce8\u610f\u529b\u548c\u5c0f\u6ce2\u5f15\u5bfc\u3001\u6d41\u4e00\u81f4\u91c7\u6837\u5668\u76f8\u7ed3\u5408\u3002\u8be5\u6ce8\u610f\u529b\u673a\u5236\u5c06\u4ee4\u724c\u6cbf\u7740\u5149\u6d41\u8f68\u8ff9\u5bf9\u9f50\uff0c\u7279\u522b\u5f3a\u8c03\u5bf9\u65f6\u95f4\u52a8\u6001\u6700\u654f\u611f\u7684\u5173\u952e\u5c42\u3002\u65f6\u7a7a\u90bb\u57df\u7f13\u5b58\u6839\u636e\u8de8\u5e27\u7684\u8fd0\u52a8\u5bf9\u5e94\u5173\u7cfb\u52a8\u6001\u5730\u9009\u62e9\u76f8\u5173\u7684\u4ee4\u724c\u3002\u6d41\u5f15\u5bfc\u91c7\u6837\u5668\u4ec5\u5c06\u6570\u636e\u4e00\u81f4\u6027\u6ce8\u5165\u4f4e\u9891\u5e26\uff0c\u4fdd\u7559\u9ad8\u9891\u5148\u9a8c\uff0c\u540c\u65f6\u52a0\u901f\u6536\u655b\u3002", "result": "DiTVR\u5728\u89c6\u9891\u6062\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u96f6\u6837\u672c\u72b6\u6001\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\uff0c\u540c\u65f6\u4fdd\u6301\u5bf9\u6d41\u566a\u58f0\u548c\u906e\u6321\u7684\u9c81\u68d2\u6027\u3002", "conclusion": "DiTVR\u5728\u89c6\u9891\u6062\u590d\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5efa\u7acb\u4e86\u65b0\u7684\u96f6\u6837\u672c\u72b6\u6001\uff0c\u5728\u4fdd\u6301\u5bf9\u6d41\u566a\u58f0\u548c\u906e\u6321\u7684\u9c81\u68d2\u6027\u7684\u540c\u65f6\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u548c\u7ec6\u8282\u4fdd\u7559\u80fd\u529b\u3002"}}
{"id": "2508.07812", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07812", "abs": "https://arxiv.org/abs/2508.07812", "authors": ["Jingze Gai", "Changchun Li"], "title": "Semi-supervised Multiscale Matching for SAR-Optical Image", "comment": "15 pages, 9 figures", "summary": "Driven by the complementary nature of optical and synthetic aperture radar\n(SAR) images, SAR-optical image matching has garnered significant interest.\nMost existing SAR-optical image matching methods aim to capture effective\nmatching features by employing the supervision of pixel-level matched\ncorrespondences within SAR-optical image pairs, which, however, suffers from\ntime-consuming and complex manual annotation, making it difficult to collect\nsufficient labeled SAR-optical image pairs. To handle this, we design a\nsemi-supervised SAR-optical image matching pipeline that leverages both scarce\nlabeled and abundant unlabeled image pairs and propose a semi-supervised\nmultiscale matching for SAR-optical image matching (S2M2-SAR). Specifically, we\npseudo-label those unlabeled SAR-optical image pairs with pseudo ground-truth\nsimilarity heatmaps by combining both deep and shallow level matching results,\nand train the matching model by employing labeled and pseudo-labeled similarity\nheatmaps. In addition, we introduce a cross-modal feature enhancement module\ntrained using a cross-modality mutual independence loss, which requires no\nground-truth labels. This unsupervised objective promotes the separation of\nmodality-shared and modality-specific features by encouraging statistical\nindependence between them, enabling effective feature disentanglement across\noptical and SAR modalities. To evaluate the effectiveness of S2M2-SAR, we\ncompare it with existing competitors on benchmark datasets. Experimental\nresults demonstrate that S2M2-SAR not only surpasses existing semi-supervised\nmethods but also achieves performance competitive with fully supervised SOTA\nmethods, demonstrating its efficiency and practical potential.", "AI": {"tldr": "\u7531\u4e8e\u73b0\u6709SAR-\u5149\u5b66\u56fe\u50cf\u5339\u914d\u65b9\u6cd5\u9700\u8981\u8017\u8d39\u5927\u91cf\u4eba\u529b\u8fdb\u884c\u6807\u6ce8\uff0c\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aS2M2-SAR\u7684\u534a\u76d1\u7763\u5b66\u4e60\u65b9\u6cd5\uff0c\u5229\u7528\u5c11\u91cf\u6807\u8bb0\u6570\u636e\u548c\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u8fdb\u884c\u8bad\u7ec3\uff0c\u5e76\u901a\u8fc7\u4f2a\u6807\u7b7e\u548c\u8de8\u6a21\u6001\u7279\u5f81\u89e3\u8026\u6280\u672f\uff0c\u5728\u4fdd\u8bc1\u5339\u914d\u7cbe\u5ea6\u7684\u540c\u65f6\uff0c\u5927\u5927\u964d\u4f4e\u4e86\u5bf9\u6807\u8bb0\u6570\u636e\u7684\u4f9d\u8d56\u3002", "motivation": "\u73b0\u6709\u5927\u591a\u6570SAR-\u5149\u5b66\u56fe\u50cf\u5339\u914d\u65b9\u6cd5\u4f9d\u8d56\u4e8e\u50cf\u7d20\u7ea7\u5339\u914d\u7684\u76d1\u7763\uff0c\u8fd9\u9700\u8981\u8017\u65f6\u4e14\u590d\u6742\u7684\u4eba\u5de5\u6807\u6ce8\uff0c\u96be\u4ee5\u6536\u96c6\u8db3\u591f\u7684\u6807\u8bb0\u6570\u636e\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u591f\u5229\u7528\u5927\u91cf\u672a\u6807\u8bb0\u6570\u636e\u7684\u65b9\u6cd5\u3002", "method": "\u672c\u7814\u7a76\u8bbe\u8ba1\u4e86\u4e00\u4e2a\u534a\u76d1\u7763\u7684SAR-\u5149\u5b66\u56fe\u50cf\u5339\u914d\u6d41\u7a0b\uff0c\u5229\u7528\u5c11\u91cf\u7684\u6807\u8bb0\u6570\u636e\u548c\u5927\u91cf\u7684\u672a\u6807\u8bb0\u6570\u636e\u3002\u5177\u4f53\u65b9\u6cd5\u5305\u62ec\uff1a1. \u63d0\u51fa\u4e86\u4e00\u79cd\u534a\u76d1\u7763\u591a\u5c3a\u5ea6\u5339\u914d\uff08S2M2-SAR\uff09\u65b9\u6cd5\u30022. \u901a\u8fc7\u7ed3\u5408\u6df1\u5ea6\u548c\u6d45\u5c42\u5339\u914d\u7ed3\u679c\uff0c\u4e3a\u672a\u6807\u8bb0\u7684SAR-\u5149\u5b66\u56fe\u50cf\u5bf9\u751f\u6210\u4f2a\u6807\u7b7e\u7684\u76f8\u4f3c\u6027\u70ed\u56fe\u30023. \u5229\u7528\u6807\u8bb0\u7684\u548c\u4f2a\u6807\u8bb0\u7684\u76f8\u4f3c\u6027\u70ed\u56fe\u6765\u8bad\u7ec3\u5339\u914d\u6a21\u578b\u30024. \u5f15\u5165\u4e86\u4e00\u4e2a\u8de8\u6a21\u6001\u7279\u5f81\u589e\u5f3a\u6a21\u5757\uff0c\u5e76\u4f7f\u7528\u65e0\u9700\u5730\u9762\u771f\u5b9e\u6807\u7b7e\u7684\u8de8\u6a21\u6001\u4e92\u72ec\u7acb\u635f\u5931\u8fdb\u884c\u8bad\u7ec3\uff0c\u4ee5\u4fc3\u8fdb\u6a21\u6001\u5171\u4eab\u548c\u6a21\u6001\u7279\u6709\u7279\u5f81\u7684\u5206\u79bb\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cS2M2-SAR\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u5168\u76d1\u7763\u7684SOTA\u65b9\u6cd5\u76f8\u5ab2\u7f8e\u3002", "conclusion": "S2M2-SAR\u7684\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u7684\u534a\u76d1\u7763\u65b9\u6cd5\uff0c\u5e76\u80fd\u4e0e\u5168\u76d1\u7763\u7684SOTA\u65b9\u6cd5\u76f8\u5ab2\u7f8e\uff0c\u8bc1\u660e\u4e86\u5176\u6548\u7387\u548c\u5b9e\u9645\u6f5c\u529b\u3002"}}
{"id": "2508.07818", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07818", "abs": "https://arxiv.org/abs/2508.07818", "authors": ["Chenyue Song", "Chen Hui", "Haiqi Zhu", "Feng Jiang", "Yachun Mi", "Wei Zhang", "Shaohui Liu"], "title": "Segmenting and Understanding: Region-aware Semantic Attention for Fine-grained Image Quality Assessment with Large Language Models", "comment": null, "summary": "No-reference image quality assessment (NR-IQA) aims to simulate the process\nof perceiving image quality aligned with subjective human perception. However,\nexisting NR-IQA methods either focus on global representations that leads to\nlimited insights into the semantically salient regions or employ a uniform\nweighting for region features that weakens the sensitivity to local quality\nvariations. In this paper, we propose a fine-grained image quality assessment\nmodel, named RSFIQA, which integrates region-level distortion information to\nperceive multi-dimensional quality discrepancies. To enhance regional quality\nawareness, we first utilize the Segment Anything Model (SAM) to dynamically\npartition the input image into non-overlapping semantic regions. For each\nregion, we teach a powerful Multi-modal Large Language Model (MLLM) to extract\ndescriptive content and perceive multi-dimensional distortions, enabling a\ncomprehensive understanding of both local semantics and quality degradations.\nTo effectively leverage this information, we introduce Region-Aware Semantic\nAttention (RSA) mechanism, which generates a global attention map by\naggregating fine-grained representations from local regions. In addition,\nRSFIQA is backbone-agnostic and can be seamlessly integrated into various deep\nneural network architectures. Extensive experiments demonstrate the robustness\nand effectiveness of the proposed method, which achieves competitive quality\nprediction performance across multiple benchmark datasets.", "AI": {"tldr": "RSFIQA\u901a\u8fc7\u4f7f\u7528SAM\u5206\u5272\u56fe\u50cf\u5e76\u5229\u7528MLLM\u7406\u89e3\u533a\u57df\u5185\u5bb9\u548c\u8d28\u91cf\u9000\u5316\uff0c\u7ed3\u5408RSA\u673a\u5236\u751f\u6210\u5168\u5c40\u6ce8\u610f\u529b\u56fe\uff0c\u5b9e\u73b0\u4e86\u66f4\u7cbe\u786e\u7684\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u3002", "motivation": "\u73b0\u6709NR-IQA\u65b9\u6cd5\u8981\u4e48\u4fa7\u91cd\u4e8e\u5168\u5c40\u8868\u793a\uff0c\u5bfc\u81f4\u5bf9\u8bed\u4e49\u663e\u8457\u533a\u57df\u7684\u6d1e\u5bdf\u6709\u9650\uff0c\u8981\u4e48\u91c7\u7528\u7edf\u4e00\u7684\u533a\u57df\u7279\u5f81\u52a0\u6743\uff0c\u524a\u5f31\u4e86\u5bf9\u5c40\u90e8\u8d28\u91cf\u53d8\u5316\u7684\u654f\u611f\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aRSFIQA\u7684\u7ec6\u7c92\u5ea6\u56fe\u50cf\u8d28\u91cf\u8bc4\u4f30\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u6574\u5408\u4e86\u533a\u57df\u7ea7\u5931\u771f\u4fe1\u606f\u3002\u9996\u5148\u5229\u7528SAM\u52a8\u6001\u5206\u5272\u56fe\u50cf\u4e3a\u975e\u91cd\u53e0\u8bed\u4e49\u533a\u57df\uff0c\u7136\u540e\u5229\u7528MLLM\u63d0\u53d6\u533a\u57df\u5185\u5bb9\u548c\u611f\u77e5\u591a\u7ef4\u5ea6\u5931\u771f\uff0c\u6700\u540e\u5f15\u5165\u533a\u57df\u611f\u77e5\u8bed\u4e49\u6ce8\u610f\u529b\uff08RSA\uff09\u673a\u5236\u805a\u5408\u533a\u57df\u8868\u793a\u4ee5\u751f\u6210\u5168\u5c40\u6ce8\u610f\u529b\u56fe\u3002RSFIQA\u652f\u6301\u9aa8\u5e72\u7f51\u7edc\u65e0\u5173\u6027\u3002", "result": "RSFIQA\u6a21\u578b\u901a\u8fc7\u6574\u5408\u533a\u57df\u7ea7\u5931\u771f\u4fe1\u606f\uff0c\u80fd\u591f\u611f\u77e5\u591a\u7ef4\u5ea6\u8d28\u91cf\u5dee\u5f02\uff0c\u4ece\u800c\u63d0\u9ad8\u4e86\u5bf9\u5c40\u90e8\u8d28\u91cf\u53d8\u5316\u7684\u654f\u611f\u6027\u3002", "conclusion": "RSFIQA\u6a21\u578b\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u8d28\u91cf\u9884\u6d4b\u6027\u80fd\uff0c\u8bc1\u660e\u4e86\u5176\u9c81\u68d2\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2508.07819", "categories": ["cs.CV", "cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07819", "abs": "https://arxiv.org/abs/2508.07819", "authors": ["Ke Ma", "Jun Long", "Hongxiao Fei", "Liujie Hua", "Yueyi Luo"], "title": "Architectural Co-Design for Zero-Shot Anomaly Detection: Decoupling Representation and Dynamically Fusing Features in CLIP", "comment": "4 pages, 1 reference, 3 figures, icassp 2026", "summary": "Pre-trained Vision-Language Models (VLMs) face a significant adaptation gap\nwhen applied to Zero-Shot Anomaly Detection (ZSAD), stemming from their lack of\nlocal inductive biases for dense prediction and their reliance on inflexible\nfeature fusion paradigms. We address these limitations through an Architectural\nCo-Design framework that jointly refines feature representation and cross-modal\nfusion. Our method integrates a parameter-efficient Convolutional Low-Rank\nAdaptation (Conv-LoRA) adapter to inject local inductive biases for\nfine-grained representation, and introduces a Dynamic Fusion Gateway (DFG) that\nleverages visual context to adaptively modulate text prompts, enabling a\npowerful bidirectional fusion. Extensive experiments on diverse industrial and\nmedical benchmarks demonstrate superior accuracy and robustness, validating\nthat this synergistic co-design is critical for robustly adapting foundation\nmodels to dense perception tasks.", "AI": {"tldr": "\u901a\u8fc7Conv-LoRA\u548cDFG\u7684\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\uff0c\u6539\u8fdb\u4e86\u9884\u8bad\u7ec3VLM\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u9884\u8bad\u7ec3\u7684\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u5e94\u7528\u4e8e\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\uff08ZSAD\uff09\u65f6\u5b58\u5728\u663e\u8457\u7684\u9002\u5e94\u6027\u5dee\u8ddd\uff0c\u539f\u56e0\u5728\u4e8e\u5176\u7f3a\u4e4f\u7528\u4e8e\u5bc6\u96c6\u9884\u6d4b\u7684\u5c40\u90e8\u5f52\u7eb3\u504f\u7f6e\u4ee5\u53ca\u5bf9\u4e0d\u7075\u6d3b\u7684\u7279\u5f81\u878d\u5408\u7684\u4f9d\u8d56\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u6846\u67b6\uff0c\u901a\u8fc7\u53c2\u6570\u9ad8\u6548\u7684\u5377\u79ef\u4f4e\u79e9\u81ea\u9002\u5e94\uff08Conv-LoRA\uff09\u6ce8\u5165\u5c40\u90e8\u5f52\u7eb3\u504f\u7f6e\u4ee5\u8fdb\u884c\u7ec6\u7c92\u5ea6\u8868\u793a\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u878d\u5408\u7f51\u5173\uff08DFG\uff09\u81ea\u9002\u5e94\u8c03\u5236\u6587\u672c\u63d0\u793a\uff0c\u5b9e\u73b0\u5f3a\u5927\u7684\u53cc\u5411\u878d\u5408\u3002", "result": "\u5728\u5de5\u4e1a\u548c\u533b\u5b66\u57fa\u51c6\u4e0a\u8fdb\u884c\u4e86\u5e7f\u6cdb\u7684\u5b9e\u9a8c\uff0c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u3002", "conclusion": "\u67b6\u6784\u534f\u540c\u8bbe\u8ba1\u80fd\u6709\u6548\u89e3\u51b3\u9884\u8bad\u7ec3\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u5728\u96f6\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u6ce8\u5165\u5c40\u90e8\u5f52\u7eb3\u504f\u7f6e\u548c\u52a8\u6001\u591a\u6a21\u6001\u878d\u5408\uff0c\u63d0\u5347\u4e86\u6a21\u578b\u7684\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\uff0c\u5bf9\u4e8e\u5c06\u57fa\u7840\u6a21\u578b\u5e94\u7528\u4e8e\u5bc6\u96c6\u611f\u77e5\u4efb\u52a1\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2508.07833", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07833", "abs": "https://arxiv.org/abs/2508.07833", "authors": ["Animesh Jain", "Alexandros Stergiou"], "title": "MIMIC: Multimodal Inversion for Model Interpretation and Conceptualization", "comment": "Project page: https://anaekin.github.io/MIMIC", "summary": "Vision Language Models (VLMs) encode multimodal inputs over large, complex,\nand difficult-to-interpret architectures, which limit transparency and trust.\nWe propose a Multimodal Inversion for Model Interpretation and\nConceptualization (MIMIC) framework to visualize the internal representations\nof VLMs by synthesizing visual concepts corresponding to internal encodings.\nMIMIC uses a joint VLM-based inversion and a feature alignment objective to\naccount for VLM's autoregressive processing. It additionally includes a triplet\nof regularizers for spatial alignment, natural image smoothness, and semantic\nrealism. We quantitatively and qualitatively evaluate MIMIC by inverting visual\nconcepts over a range of varying-length free-form VLM output texts. Reported\nresults include both standard visual quality metrics as well as semantic\ntext-based metrics. To the best of our knowledge, this is the first model\ninversion approach addressing visual interpretations of VLM concepts.", "AI": {"tldr": "MIMIC visualizes VLM internal representations by synthesizing visual concepts from encodings, improving transparency and trust.", "motivation": "To address the limited transparency and trust in Vision Language Models (VLMs) due to their complex architectures, by visualizing internal representations.", "method": "MIMIC uses a joint VLM-based inversion and a feature alignment objective, with regularizers for spatial alignment, natural image smoothness, and semantic realism.", "result": "MIMIC was quantitatively and qualitatively evaluated by inverting visual concepts over a range of varying-length free-form VLM output texts, using standard visual quality and semantic text-based metrics.", "conclusion": "MIMIC is the first model inversion approach to address visual interpretations of VLM concepts, providing a way to visualize internal representations and synthesize corresponding visual concepts."}}
{"id": "2508.07835", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07835", "abs": "https://arxiv.org/abs/2508.07835", "authors": ["Jingna Qiu", "Nishanth Jain", "Jonas Ammeling", "Marc Aubreville", "Katharina Breininger"], "title": "Effortless Vision-Language Model Specialization in Histopathology without Annotation", "comment": null, "summary": "Recent advances in Vision-Language Models (VLMs) in histopathology, such as\nCONCH and QuiltNet, have demonstrated impressive zero-shot classification\ncapabilities across various tasks. However, their general-purpose design may\nlead to suboptimal performance in specific downstream applications. While\nsupervised fine-tuning methods address this issue, they require manually\nlabeled samples for adaptation. This paper investigates annotation-free\nadaptation of VLMs through continued pretraining on domain- and task-relevant\nimage-caption pairs extracted from existing databases. Our experiments on two\nVLMs, CONCH and QuiltNet, across three downstream tasks reveal that these pairs\nsubstantially enhance both zero-shot and few-shot performance. Notably, with\nlarger training sizes, continued pretraining matches the performance of\nfew-shot methods while eliminating manual labeling. Its effectiveness,\ntask-agnostic design, and annotation-free workflow make it a promising pathway\nfor adapting VLMs to new histopathology tasks. Code is available at\nhttps://github.com/DeepMicroscopy/Annotation-free-VLM-specialization.", "AI": {"tldr": "\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u901a\u8fc7\u7ee7\u7eed\u9884\u8bad\u7ec3\u6765\u4f18\u5316\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u6027\u80fd\u5ab2\u7f8e\u6709\u76d1\u7763\u65b9\u6cd5\u3002", "motivation": "\u73b0\u6709\u7684\u901a\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u5728\u7ec4\u7ec7\u75c5\u7406\u5b66\u7279\u5b9a\u4e0b\u6e38\u4efb\u52a1\u4e2d\u53ef\u80fd\u8868\u73b0\u4e0d\u4f73\uff0c\u800c\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\u9700\u8981\u624b\u52a8\u6807\u6ce8\u7684\u6837\u672c\u3002\u672c\u6587\u65e8\u5728\u63a2\u7d22\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u9002\u5e94\u8fd9\u4e9b\u6a21\u578b\u7684\u65b9\u6cd5\u3002", "method": "\u901a\u8fc7\u5728\u9886\u57df\u76f8\u5173\u548c\u4efb\u52a1\u76f8\u5173\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\u4e0a\u7ee7\u7eed\u9884\u8bad\u7ec3\uff0c\u5bf9\u73b0\u6709\u7684\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08\u5982CONCH\u548cQuiltNet\uff09\u8fdb\u884c\u65e0\u9700\u6807\u6ce8\u7684\u9002\u5e94\u3002", "result": "\u5728\u4e24\u4e2aVLM\uff08CONCH\u548cQuiltNet\uff09\u548c\u4e09\u4e2a\u4e0b\u6e38\u4efb\u52a1\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u7ee7\u7eed\u9884\u8bad\u7ec3\u80fd\u591f\u663e\u8457\u63d0\u5347\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u6027\u80fd\u3002\u5728\u8f83\u5927\u7684\u8bad\u7ec3\u89c4\u6a21\u4e0b\uff0c\u8be5\u65b9\u6cd5\u6027\u80fd\u53ef\u5ab2\u7f8e\u5c11\u6837\u672c\u5fae\u8c03\u65b9\u6cd5\uff0c\u4e14\u65e0\u9700\u624b\u52a8\u6807\u6ce8\u3002", "conclusion": "\u6587\u7ae0\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u6807\u6ce8\u5373\u53ef\u5bf9\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLM\uff09\u8fdb\u884c\u9002\u5e94\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u5728\u76f8\u5173\u6570\u636e\u96c6\u7684\u56fe\u50cf-\u6807\u9898\u5bf9\u4e0a\u7ee7\u7eed\u9884\u8bad\u7ec3\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u663e\u8457\u63d0\u5347\u6a21\u578b\u5728\u96f6\u6837\u672c\u548c\u5c11\u6837\u672c\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u5e76\u4e14\u5728\u8bad\u7ec3\u89c4\u6a21\u8f83\u5927\u65f6\uff0c\u5176\u6027\u80fd\u53ef\u5ab2\u7f8e\u6709\u76d1\u7763\u5fae\u8c03\u65b9\u6cd5\uff0c\u540c\u65f6\u65e0\u9700\u4eba\u5de5\u6807\u6ce8\uff0c\u5177\u6709\u9ad8\u6548\u3001\u4efb\u52a1\u65e0\u5173\u548c\u65e0\u9700\u6807\u6ce8\u7684\u4f18\u70b9\uff0c\u4e3aVLM\u5728\u65b0\u7684\u7ec4\u7ec7\u75c5\u7406\u5b66\u4efb\u52a1\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u9014\u5f84\u3002"}}
{"id": "2508.07838", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07838", "abs": "https://arxiv.org/abs/2508.07838", "authors": ["Qi Xiang", "Kunsong Shi", "Zhigui Lin", "Lei He"], "title": "CBDES MoE: Hierarchically Decoupled Mixture-of-Experts for Functional Modules in Autonomous Driving", "comment": null, "summary": "Bird's Eye View (BEV) perception systems based on multi-sensor feature fusion\nhave become a fundamental cornerstone for end-to-end autonomous driving.\nHowever, existing multi-modal BEV methods commonly suffer from limited input\nadaptability, constrained modeling capacity, and suboptimal generalization. To\naddress these challenges, we propose a hierarchically decoupled\nMixture-of-Experts architecture at the functional module level, termed\nComputing Brain DEvelopment System Mixture-of-Experts (CBDES MoE). CBDES MoE\nintegrates multiple structurally heterogeneous expert networks with a\nlightweight Self-Attention Router (SAR) gating mechanism, enabling dynamic\nexpert path selection and sparse, input-aware efficient inference. To the best\nof our knowledge, this is the first modular Mixture-of-Experts framework\nconstructed at the functional module granularity within the autonomous driving\ndomain. Extensive evaluations on the real-world nuScenes dataset demonstrate\nthat CBDES MoE consistently outperforms fixed single-expert baselines in 3D\nobject detection. Compared to the strongest single-expert model, CBDES MoE\nachieves a 1.6-point increase in mAP and a 4.1-point improvement in NDS,\ndemonstrating the effectiveness and practical advantages of the proposed\napproach.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684 CBDES MoE \u67b6\u6784\uff0c\u7528\u4e8e\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u4e2d BEV \u611f\u77e5\u7cfb\u7edf\u7684\u5c40\u9650\u6027\uff0c\u901a\u8fc7\u5206\u5c42\u89e3\u8026\u7684\u4e13\u5bb6\u6df7\u5408\u65b9\u6cd5\u548c\u81ea\u6ce8\u610f\u529b\u8def\u7531\u5668\uff0c\u63d0\u9ad8\u4e86\u6027\u80fd\u548c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u7684\u591a\u6a21\u6001 BEV \u65b9\u6cd5\u901a\u5e38\u5b58\u5728\u8f93\u5165\u9002\u5e94\u6027\u6709\u9650\u3001\u5efa\u6a21\u80fd\u529b\u53d7\u9650\u548c\u6cdb\u5316\u80fd\u529b\u4e0d\u4f73\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u5c42\u89e3\u8026\u7684\u4e13\u5bb6\u6df7\u5408\uff08Mixture-of-Experts\uff09\u67b6\u6784\uff0c\u79f0\u4e3a CBDES MoE\uff0c\u5728\u529f\u80fd\u6a21\u5757\u7ea7\u522b\u3002CBDES MoE \u96c6\u6210\u4e86\u591a\u4e2a\u7ed3\u6784\u4e0a\u5f02\u6784\u7684\u4e13\u5bb6\u7f51\u7edc\uff0c\u5e76\u91c7\u7528\u8f7b\u91cf\u7ea7\u81ea\u6ce8\u610f\u529b\u8def\u7531\u5668\uff08SAR\uff09\u95e8\u63a7\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u52a8\u6001\u4e13\u5bb6\u8def\u5f84\u9009\u62e9\u4ee5\u53ca\u7a00\u758f\u3001\u611f\u77e5\u8f93\u5165\u7684\u6709\u6548\u63a8\u7406\u3002", "result": "CBDES MoE \u5728 3D \u5bf9\u8c61\u68c0\u6d4b\u65b9\u9762\u6301\u7eed\u4f18\u4e8e\u56fa\u5b9a\u7684\u5355\u4e00\u4e13\u5bb6\u57fa\u7ebf\u3002\u4e0e\u6700\u5f3a\u7684\u5355\u4e00\u4e13\u5bb6\u6a21\u578b\u76f8\u6bd4\uff0cmAP \u63d0\u9ad8\u4e86 1.6 \u4e2a\u767e\u5206\u70b9\uff0cNDS \u63d0\u9ad8\u4e86 4.1 \u4e2a\u767e\u5206\u70b9\u3002", "conclusion": "CBDES MoE \u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u6301\u7eed\u4f18\u4e8e\u56fa\u5b9a\u7684\u5355\u4e00\u4e13\u5bb6\u57fa\u7ebf\uff0c\u5728 3D \u5bf9\u8c61\u68c0\u6d4b\u65b9\u9762\uff0c\u4e0e\u6700\u5f3a\u7684\u5355\u4e00\u4e13\u5bb6\u6a21\u578b\u76f8\u6bd4\uff0cmAP \u63d0\u9ad8\u4e86 1.6 \u4e2a\u767e\u5206\u70b9\uff0cNDS \u63d0\u9ad8\u4e86 4.1 \u4e2a\u767e\u5206\u70b9\uff0c\u8bc1\u660e\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u5b9e\u9645\u4f18\u52bf\u3002"}}
{"id": "2508.07847", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07847", "abs": "https://arxiv.org/abs/2508.07847", "authors": ["Shunya Nagashima", "Komei Sugiura"], "title": "Deep Space Weather Model: Long-Range Solar Flare Prediction from Multi-Wavelength Images", "comment": "ICCV 2025", "summary": "Accurate, reliable solar flare prediction is crucial for mitigating potential\ndisruptions to critical infrastructure, while predicting solar flares remains a\nsignificant challenge. Existing methods based on heuristic physical features\noften lack representation learning from solar images. On the other hand,\nend-to-end learning approaches struggle to model long-range temporal\ndependencies in solar images. In this study, we propose Deep Space Weather\nModel (Deep SWM), which is based on multiple deep state space models for\nhandling both ten-channel solar images and long-range spatio-temporal\ndependencies. Deep SWM also features a sparse masked autoencoder, a novel\npretraining strategy that employs a two-phase masking approach to preserve\ncrucial regions such as sunspots while compressing spatial information.\nFurthermore, we built FlareBench, a new public benchmark for solar flare\nprediction covering a full 11-year solar activity cycle, to validate our\nmethod. Our method outperformed baseline methods and even human expert\nperformance on standard metrics in terms of performance and reliability. The\nproject page can be found at https://keio-smilab25.github.io/DeepSWM.", "AI": {"tldr": "Deep SWM, a new model using deep state space models and a masked autoencoder, accurately predicts solar flares by analyzing solar images and their temporal dependencies. It outperforms existing methods and human experts, validated on the FlareBench benchmark.", "motivation": "Accurate solar flare prediction is crucial for mitigating disruptions to critical infrastructure, but existing methods face challenges in representation learning from solar images and modeling long-range temporal dependencies.", "method": "The study proposes Deep Space Weather Model (Deep SWM), which utilizes multiple deep state space models to process ten-channel solar images and capture long-range spatio-temporal dependencies. It also incorporates a sparse masked autoencoder with a two-phase masking strategy for pretraining, which preserves critical regions like sunspots while compressing spatial information. FlareBench, a new public benchmark covering a full 11-year solar activity cycle, was created to validate the method.", "result": "The proposed method, Deep SWM, demonstrated superior performance and reliability compared to baseline methods and human experts on the FlareBench benchmark.", "conclusion": "Deep SWM outperformed baseline methods and even human expert performance on standard metrics in terms of performance and reliability."}}
{"id": "2508.07850", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07850", "abs": "https://arxiv.org/abs/2508.07850", "authors": ["Noriko Nitta", "Rei Miyata", "Naoto Oishi"], "title": "Morphological Analysis of Semiconductor Microstructures using Skeleton Graphs", "comment": "CV4MS: Computer Vision for Materials Science, Workshop in conjunction\n  with the IEEE/CVF ICCV 2025", "summary": "In this paper, electron microscopy images of microstructures formed on Ge\nsurfaces by ion beam irradiation were processed to extract topological features\nas skeleton graphs, which were then embedded using a graph convolutional\nnetwork. The resulting embeddings were analyzed using principal component\nanalysis, and cluster separability in the resulting PCA space was evaluated\nusing the Davies-Bouldin index. The results indicate that variations in\nirradiation angle have a more significant impact on the morphological\nproperties of Ge surfaces than variations in irradiation fluence.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07851", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07851", "abs": "https://arxiv.org/abs/2508.07851", "authors": ["Konrad Reuter", "Suresh Guttikonda", "Sarah Latus", "Lennart Maack", "Christian Betz", "Tobias Maurer", "Alexander Schlaefer"], "title": "Tracking Any Point Methods for Markerless 3D Tissue Tracking in Endoscopic Stereo Images", "comment": "Accecpted to CURAC conference 2025", "summary": "Minimally invasive surgery presents challenges such as dynamic tissue motion\nand a limited field of view. Accurate tissue tracking has the potential to\nsupport surgical guidance, improve safety by helping avoid damage to sensitive\nstructures, and enable context-aware robotic assistance during complex\nprocedures. In this work, we propose a novel method for markerless 3D tissue\ntracking by leveraging 2D Tracking Any Point (TAP) networks. Our method\ncombines two CoTracker models, one for temporal tracking and one for stereo\nmatching, to estimate 3D motion from stereo endoscopic images. We evaluate the\nsystem using a clinical laparoscopic setup and a robotic arm simulating tissue\nmotion, with experiments conducted on a synthetic 3D-printed phantom and a\nchicken tissue phantom. Tracking on the chicken tissue phantom yielded more\nreliable results, with Euclidean distance errors as low as 1.1 mm at a velocity\nof 10 mm/s. These findings highlight the potential of TAP-based models for\naccurate, markerless 3D tracking in challenging surgical scenarios.", "AI": {"tldr": "Markerless 3D tissue tracking for minimally invasive surgery using CoTracker models shows promising results, especially on biological tissue, with potential for improved surgical guidance and safety.", "motivation": "Accurate tissue tracking is crucial in minimally invasive surgery to address challenges like dynamic tissue motion and limited field of view, offering support for surgical guidance, improving safety by avoiding damage to sensitive structures, and enabling context-aware robotic assistance.", "method": "A novel method for markerless 3D tissue tracking is proposed by combining two CoTracker models: one for temporal tracking and one for stereo matching, to estimate 3D motion from stereo endoscopic images.", "result": "The system was evaluated on synthetic 3D-printed and chicken tissue phantoms using a clinical laparoscopic setup and a robotic arm. Tracking on the chicken tissue phantom was more reliable, achieving Euclidean distance errors as low as 1.1 mm at a velocity of 10 mm/s.", "conclusion": "The proposed method, leveraging two CoTracker models for temporal tracking and stereo matching, shows potential for accurate, markerless 3D tissue tracking in challenging surgical scenarios, with reliable results on chicken tissue phantoms yielding low Euclidean distance errors."}}
{"id": "2508.07863", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07863", "abs": "https://arxiv.org/abs/2508.07863", "authors": ["Bin Cao", "Sipeng Zheng", "Ye Wang", "Lujie Xia", "Qianshan Wei", "Qin Jin", "Jing Liu", "Zongqing Lu"], "title": "Being-M0.5: A Real-Time Controllable Vision-Language-Motion Model", "comment": "16 pages", "summary": "Human motion generation has emerged as a critical technology with\ntransformative potential for real-world applications. However, existing\nvision-language-motion models (VLMMs) face significant limitations that hinder\ntheir practical deployment. We identify controllability as a main bottleneck,\nmanifesting in five key aspects: inadequate response to diverse human commands,\nlimited pose initialization capabilities, poor performance on long-term\nsequences, insufficient handling of unseen scenarios, and lack of fine-grained\ncontrol over individual body parts. To overcome these limitations, we present\nBeing-M0.5, the first real-time, controllable VLMM that achieves\nstate-of-the-art performance across multiple motion generation tasks. Our\napproach is built upon HuMo100M, the largest and most comprehensive human\nmotion dataset to date, comprising over 5 million self-collected motion\nsequences, 100 million multi-task instructional instances, and detailed\npart-level annotations that address a critical gap in existing datasets. We\nintroduce a novel part-aware residual quantization technique for motion\ntokenization that enables precise, granular control over individual body parts\nduring generation. Extensive experimental validation demonstrates Being-M0.5's\nsuperior performance across diverse motion benchmarks, while comprehensive\nefficiency analysis confirms its real-time capabilities. Our contributions\ninclude design insights and detailed computational analysis to guide future\ndevelopment of practical motion generators. We believe that HuMo100M and\nBeing-M0.5 represent significant advances that will accelerate the adoption of\nmotion generation technologies in real-world applications. The project page is\navailable at https://beingbeyond.github.io/Being-M0.5.", "AI": {"tldr": "Being-M0.5\u662f\u4e00\u4e2a\u5b9e\u65f6\u3001\u53ef\u63a7\u7684\u52a8\u4f5c\u751f\u6210\u6a21\u578b\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u90e8\u4ef6\u611f\u77e5\u6280\u672f\u548c\u5927\u89c4\u6a21\u6570\u636e\u96c6\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u6a21\u578b\u5728\u53ef\u63a7\u6027\u65b9\u9762\u7684\u4e94\u5927\u6311\u6218\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLMM\uff09\u5728\u53ef\u63a7\u6027\u65b9\u9762\u5b58\u5728\u663e\u8457\u5c40\u9650\uff0c\u5305\u62ec\u5bf9\u591a\u6837\u5316\u6307\u4ee4\u54cd\u5e94\u4e0d\u8db3\u3001\u59ff\u6001\u521d\u59cb\u5316\u80fd\u529b\u6709\u9650\u3001\u957f\u671f\u5e8f\u5217\u8868\u73b0\u4e0d\u4f73\u3001\u5bf9\u672a\u89c1\u573a\u666f\u5904\u7406\u80fd\u529b\u4e0d\u8db3\u4ee5\u53ca\u7f3a\u4e4f\u5bf9\u5355\u4e2a\u8eab\u4f53\u90e8\u4f4d\u7684\u7cbe\u7ec6\u63a7\u5236\u3002\u8fd9\u4e9b\u95ee\u9898\u963b\u788d\u4e86VLMM\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u90e8\u7f72\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aBeing-M0.5\u7684\u65b0\u578b\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLMM\uff09\uff0c\u8be5\u6a21\u578b\u5177\u6709\u5b9e\u65f6\u6027\u548c\u53ef\u63a7\u6027\u3002\u6838\u5fc3\u662f\u91c7\u7528\u4e86\u65b0\u9896\u7684\u90e8\u4ef6\u611f\u77e5\u6b8b\u5dee\u91cf\u5316\u6280\u672f\u8fdb\u884c\u52a8\u4f5c\u5206\u8bcd\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5355\u4e2a\u8eab\u4f53\u90e8\u4f4d\u7684\u7cbe\u786e\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u8be5\u6a21\u578b\u57fa\u4e8eHuMo100M\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u6d77\u91cf\u7684\u52a8\u4f5c\u5e8f\u5217\u3001\u591a\u4efb\u52a1\u6307\u4ee4\u5b9e\u4f8b\u548c\u90e8\u4ef6\u7ea7\u6807\u6ce8\u3002", "result": "Being-M0.5\u5728\u591a\u4e2a\u52a8\u4f5c\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5c55\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u8bc1\u5b9e\u4e86\u5176\u5b9e\u65f6\u80fd\u529b\u3002\u901a\u8fc7\u90e8\u4ef6\u611f\u77e5\u6b8b\u5dee\u91cf\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u52a8\u4f5c\u751f\u6210\u8fc7\u7a0b\u4e2d\u5355\u4e2a\u8eab\u4f53\u90e8\u4f4d\u7684\u7cbe\u786e\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002", "conclusion": "Being-M0.5\u662f\u9996\u4e2a\u5b9e\u65f6\u3001\u53ef\u63a7\u7684\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\uff08VLMM\uff09\uff0c\u5728\u591a\u4e2a\u52a8\u4f5c\u751f\u6210\u4efb\u52a1\u4e0a\u5747\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002HuMo100M\u662f\u8fc4\u4eca\u4e3a\u6b62\u6700\u5927\u3001\u6700\u5168\u9762\u7684\u6570\u636e\u96c6\uff0c\u5305\u542b\u8d85\u8fc7500\u4e07\u4e2a\u52a8\u4f5c\u5e8f\u5217\u30011000\u4e07\u4e2a\u591a\u4efb\u52a1\u6307\u4ee4\u5b9e\u4f8b\u548c\u8be6\u7ec6\u7684\u90e8\u4ef6\u7ea7\u6807\u6ce8\u3002\u901a\u8fc7\u65b0\u9896\u7684\u90e8\u4ef6\u611f\u77e5\u6b8b\u5dee\u91cf\u5316\u6280\u672f\uff0c\u5b9e\u73b0\u4e86\u5bf9\u751f\u6210\u8fc7\u7a0b\u4e2d\u5355\u4e2a\u8eab\u4f53\u90e8\u4f4d\u7684\u7cbe\u786e\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u3002\u5b9e\u9a8c\u9a8c\u8bc1\u548c\u6548\u7387\u5206\u6790\u5747\u8bc1\u5b9e\u4e86Being-M0.5\u7684\u4f18\u8d8a\u6027\u80fd\u548c\u5b9e\u65f6\u80fd\u529b\u3002\u8be5\u7814\u7a76\u4e3a\u672a\u6765\u5f00\u53d1\u5b9e\u7528\u7684\u52a8\u4f5c\u751f\u6210\u5668\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u89c1\u89e3\u548c\u8be6\u7ec6\u7684\u8ba1\u7b97\u5206\u6790\u3002"}}
{"id": "2508.07871", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07871", "abs": "https://arxiv.org/abs/2508.07871", "authors": ["Yanshu Li", "Jianjiang Yang", "Zhennan Shen", "Ligong Han", "Haoyan Xu", "Ruixiang Tang"], "title": "CATP: Contextually Adaptive Token Pruning for Efficient and Enhanced Multimodal In-Context Learning", "comment": "13 pages, 12 figures, 6 tables", "summary": "Modern large vision-language models (LVLMs) convert each input image into a\nlarge set of tokens, far outnumbering the text tokens. Although this improves\nvisual perception, it introduces severe image token redundancy. Because image\ntokens carry sparse information, many add little to reasoning, yet greatly\nincrease inference cost. The emerging image token pruning methods tackle this\nissue by identifying the most important tokens and discarding the rest. These\nmethods can raise efficiency with only modest performance loss. However, most\nof them only consider single-image tasks and overlook multimodal in-context\nlearning (ICL), where redundancy is greater and efficiency is more critical.\nRedundant tokens weaken the advantage of multimodal ICL for rapid domain\nadaptation and cause unstable performance. Applying existing pruning methods in\nthis setting leads to large accuracy drops, exposing a clear gap and the need\nfor new techniques. Thus, we propose Contextually Adaptive Token Pruning\n(CATP), a training-free pruning method targeted at multimodal ICL. CATP\nconsists of two stages that perform progressive pruning to fully account for\nthe complex cross-modal interactions in the input sequence. After removing\n77.8\\% of the image tokens, CATP produces an average performance gain of 0.6\\%\nover the vanilla model on four LVLMs and eight benchmarks, exceeding all\nbaselines remarkably. Meanwhile, it effectively improves efficiency by\nachieving an average reduction of 10.78\\% in inference latency. CATP enhances\nthe practical value of multimodal ICL and lays the groundwork for future\nprogress in interleaved image-text scenarios.", "AI": {"tldr": "CATP\u662f\u4e00\u79cd\u7528\u4e8e\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u7684\u56fe\u50cf\u4ee4\u724c\u526a\u679d\u65b9\u6cd5\uff0c\u80fd\u663e\u8457\u63d0\u9ad8\u6548\u7387\u5e76\u4fdd\u6301\u6216\u63d0\u5347\u6027\u80fd\u3002", "motivation": "\u73b0\u4ee3\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08LVLMs\uff09\u867d\u7136\u63d0\u5347\u4e86\u89c6\u89c9\u611f\u77e5\u80fd\u529b\uff0c\u4f46\u4e5f\u5f15\u5165\u4e86\u56fe\u50cf\u4ee4\u724c\u5197\u4f59\uff0c\u589e\u52a0\u4e86\u63a8\u7406\u6210\u672c\uff0c\u5c24\u5176\u5728\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\uff08ICL\uff09\u573a\u666f\u4e0b\uff0c\u8fd9\u79cd\u5197\u4f59\u4f1a\u524a\u5f31\u6a21\u578b\u4f18\u52bf\u5e76\u5bfc\u81f4\u6027\u80fd\u4e0d\u7a33\u5b9a\u3002\u73b0\u6709\u7684\u526a\u679d\u65b9\u6cd5\u5728\u591a\u6a21\u6001ICL\u573a\u666f\u4e0b\u6548\u679c\u4e0d\u4f73\uff0c\u5b58\u5728\u6027\u80fd\u5927\u5e45\u4e0b\u964d\u7684\u95ee\u9898\uff0c\u56e0\u6b64\u9700\u8981\u65b0\u7684\u6280\u672f\u6765\u89e3\u51b3\u3002", "method": "CATP\u662f\u4e00\u79cd\u8bad\u7ec3\u65e0\u5173\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u4e24\u4e2a\u9636\u6bb5\u7684\u6e10\u8fdb\u5f0f\u526a\u679d\u6765\u5904\u7406\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u56fe\u50cf\u4ee4\u724c\u5197\u4f59\u95ee\u9898\uff0c\u5145\u5206\u8003\u8651\u4e86\u8de8\u6a21\u6001\u7684\u590d\u6742\u4ea4\u4e92\u3002", "result": "CATP\u5728\u56db\u4e2aLVLMs\u548c\u516b\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u5728\u53bb\u966477.8%\u7684\u56fe\u50cf\u4ee4\u724c\u540e\uff0c\u5e73\u5747\u6027\u80fd\u6bd4\u6807\u51c6\u6a21\u578b\u63d0\u5347\u4e860.6%\uff0c\u4f18\u4e8e\u6240\u6709\u57fa\u7ebf\u65b9\u6cd5\u3002\u540c\u65f6\uff0c\u63a8\u7406\u5ef6\u8fdf\u5e73\u5747\u964d\u4f4e\u4e8610.78%\uff0c\u6709\u6548\u63d0\u9ad8\u4e86\u6548\u7387\u3002", "conclusion": "CATP\u65b9\u6cd5\u901a\u8fc7\u6e10\u8fdb\u5f0f\u526a\u679d\u6709\u6548\u89e3\u51b3\u4e86\u591a\u6a21\u6001\u4e0a\u4e0b\u6587\u5b66\u4e60\u4e2d\u7684\u56fe\u50cf\u4ee4\u724c\u5197\u4f59\u95ee\u9898\uff0c\u5728\u63d0\u9ad8\u6548\u7387\u7684\u540c\u65f6\u4fdd\u6301\u751a\u81f3\u7565\u5fae\u63d0\u5347\u4e86\u6027\u80fd\uff0c\u5e73\u5747\u63a8\u7406\u5ef6\u8fdf\u964d\u4f4e\u4e8610.78%\uff0c\u5e73\u5747\u6027\u80fd\u63d0\u5347\u4e860.6%\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u7684\u4ef7\u503c\u3002"}}
{"id": "2508.07875", "categories": ["eess.IV", "cs.AI", "cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07875", "abs": "https://arxiv.org/abs/2508.07875", "authors": ["Shuo Han", "Ahmed Karam Eldaly", "Solomon Sunday Oyelere"], "title": "Towards Human-AI Collaboration System for the Detection of Invasive Ductal Carcinoma in Histopathology Images", "comment": null, "summary": "Invasive ductal carcinoma (IDC) is the most prevalent form of breast cancer,\nand early, accurate diagnosis is critical to improving patient survival rates\nby guiding treatment decisions. Combining medical expertise with artificial\nintelligence (AI) holds significant promise for enhancing the precision and\nefficiency of IDC detection. In this work, we propose a human-in-the-loop\n(HITL) deep learning system designed to detect IDC in histopathology images.\nThe system begins with an initial diagnosis provided by a high-performance\nEfficientNetV2S model, offering feedback from AI to the human expert. Medical\nprofessionals then review the AI-generated results, correct any misclassified\nimages, and integrate the revised labels into the training dataset, forming a\nfeedback loop from the human back to the AI. This iterative process refines the\nmodel's performance over time. The EfficientNetV2S model itself achieves\nstate-of-the-art performance compared to existing methods in the literature,\nwith an overall accuracy of 93.65\\%. Incorporating the human-in-the-loop system\nfurther improves the model's accuracy using four experimental groups with\nmisclassified images. These results demonstrate the potential of this\ncollaborative approach to enhance AI performance in diagnostic systems. This\nwork contributes to advancing automated, efficient, and highly accurate methods\nfor IDC detection through human-AI collaboration, offering a promising\ndirection for future AI-assisted medical diagnostics.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u201c\u4eba\u5728\u56de\u8def\u201d\u7684\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7ed3\u5408AI\uff08EfficientNetV2S\uff09\u548c\u4eba\u7c7b\u4e13\u5bb6\u7684\u53cd\u9988\uff0c\u7528\u4e8e\u63d0\u9ad8\u4e73\u817a\u764c\uff08IDC\uff09\u68c0\u6d4b\u7684\u51c6\u786e\u6027\uff0c\u6700\u7ec8\u6a21\u578b\u51c6\u786e\u7387\u8fbe93.65%\u3002", "motivation": "\u6d78\u6da6\u6027\u5bfc\u7ba1\u764c\uff08IDC\uff09\u662f\u4e73\u817a\u764c\u4e2d\u6700\u5e38\u89c1\u7684\u7c7b\u578b\uff0c\u65e9\u671f\u51c6\u786e\u8bca\u65ad\u5bf9\u63d0\u9ad8\u60a3\u8005\u751f\u5b58\u7387\u81f3\u5173\u91cd\u8981\u3002\u7ed3\u5408\u533b\u5b66\u4e13\u4e1a\u77e5\u8bc6\u548c\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6709\u671b\u63d0\u9ad8IDC\u68c0\u6d4b\u7684\u7cbe\u786e\u5ea6\u548c\u6548\u7387\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u5305\u542bEfficientNetV2S\u6a21\u578b\u548c\u4eba\u7c7b\u4e13\u5bb6\u53cd\u9988\u7684\u201c\u4eba\u5728\u56de\u8def\u201d\uff08HITL\uff09\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\u3002\u7cfb\u7edf\u9996\u5148\u7531AI\u8fdb\u884c\u521d\u6b65\u8bca\u65ad\uff0c\u7136\u540e\u7531\u533b\u5b66\u4e13\u5bb6\u5ba1\u6838\u5e76\u4fee\u6b63\u9519\u8bef\u5206\u7c7b\u7684\u56fe\u50cf\uff0c\u5c06\u4fee\u6b63\u540e\u7684\u6807\u7b7e\u53cd\u9988\u7ed9AI\u8fdb\u884c\u6a21\u578b\u8fed\u4ee3\u4f18\u5316\u3002", "result": "EfficientNetV2S\u6a21\u578b\u5728\u4e0e\u73b0\u6709\u65b9\u6cd5\u6bd4\u8f83\u4e2d\u53d6\u5f97\u4e8693.65%\u7684\u51c6\u786e\u7387\u3002\u901a\u8fc7HITL\u7cfb\u7edf\uff0c\u5229\u7528\u5305\u542b\u9519\u8bef\u5206\u7c7b\u56fe\u50cf\u7684\u56db\u4e2a\u5b9e\u9a8c\u7ec4\u8fdb\u884c\u8fdb\u4e00\u6b65\u4f18\u5316\uff0c\u6a21\u578b\u6027\u80fd\u5f97\u5230\u63d0\u5347\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4eba\u7c7b\u4e13\u5bb6\u53cd\u9988\u7684\u4eba\u5de5\u667a\u80fd\uff08AI\uff09\u6df1\u5ea6\u5b66\u4e60\u7cfb\u7edf\uff0c\u7528\u4e8e\u68c0\u6d4b\u6d78\u6da6\u6027\u5bfc\u7ba1\u764c\uff08IDC\uff09\u3002\u8be5\u7cfb\u7edf\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\uff0c\u80fd\u591f\u63d0\u9ad8AI\u5728\u533b\u5b66\u8bca\u65ad\u4e2d\u7684\u6027\u80fd\uff0c\u4e3a\u672a\u6765AI\u8f85\u52a9\u533b\u5b66\u8bca\u65ad\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2508.07877", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07877", "abs": "https://arxiv.org/abs/2508.07877", "authors": ["WonJun Moon", "Hyun Seok Seong", "Jae-Pil Heo"], "title": "Selective Contrastive Learning for Weakly Supervised Affordance Grounding", "comment": "Accepted to ICCV 2025", "summary": "Facilitating an entity's interaction with objects requires accurately\nidentifying parts that afford specific actions. Weakly supervised affordance\ngrounding (WSAG) seeks to imitate human learning from third-person\ndemonstrations, where humans intuitively grasp functional parts without needing\npixel-level annotations. To achieve this, grounding is typically learned using\na shared classifier across images from different perspectives, along with\ndistillation strategies incorporating part discovery process. However, since\naffordance-relevant parts are not always easily distinguishable, models\nprimarily rely on classification, often focusing on common class-specific\npatterns that are unrelated to affordance. To address this limitation, we move\nbeyond isolated part-level learning by introducing selective prototypical and\npixel contrastive objectives that adaptively learn affordance-relevant cues at\nboth the part and object levels, depending on the granularity of the available\ninformation. Initially, we find the action-associated objects in both\negocentric (object-focused) and exocentric (third-person example) images by\nleveraging CLIP. Then, by cross-referencing the discovered objects of\ncomplementary views, we excavate the precise part-level affordance clues in\neach perspective. By consistently learning to distinguish affordance-relevant\nregions from affordance-irrelevant background context, our approach effectively\nshifts activation from irrelevant areas toward meaningful affordance cues.\nExperimental results demonstrate the effectiveness of our method. Codes are\navailable at github.com/hynnsk/SelectiveCL.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u9009\u62e9\u6027\u5bf9\u6bd4\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u96f6\u4ef6\u548c\u7269\u4f53\u7ea7\u522b\u7684\u7ebf\u7d22\uff0c\u5e76\u5229\u7528\u4e92\u8865\u89c6\u56fe\u6765\u6316\u6398\u529f\u80fd\u7ebf\u7d22\uff0c\u4ece\u800c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u8fc7\u5ea6\u4f9d\u8d56\u5206\u7c7b\u548c\u5173\u6ce8\u65e0\u5173\u6a21\u5f0f\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u4e3a\u4e86\u514b\u670d\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u5206\u7c7b\uff0c\u5173\u6ce8\u4e0e\u529f\u80fd\u65e0\u5173\u7684\u5e38\u89c1\u7279\u5b9a\u7c7b\u522b\u6a21\u5f0f\u7684\u9650\u5236\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u8d85\u8d8a\u5b64\u7acb\u96f6\u4ef6\u7ea7\u522b\u5b66\u4e60\u7684\u65b9\u6cd5\u3002", "method": "\u5229\u7528CLIP\u627e\u5230\u7b2c\u4e00\u4eba\u79f0\uff08\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\uff09\u548c\u7b2c\u4e09\u4eba\u79f0\uff08\u7269\u4f53\u793a\u4f8b\uff09\u56fe\u50cf\u4e2d\u7684\u52a8\u4f5c\u76f8\u5173\u7269\u4f53\uff0c\u7136\u540e\u901a\u8fc7\u4ea4\u53c9\u5f15\u7528\u4e92\u8865\u89c6\u56fe\u7684\u5df2\u53d1\u73b0\u7269\u4f53\uff0c\u6316\u6398\u6bcf\u4e2a\u89c6\u89d2\u4e2d\u7cbe\u786e\u7684\u96f6\u4ef6\u7ea7\u529f\u80fd\u7ebf\u7d22\uff0c\u5e76\u5b66\u4e60\u533a\u5206\u4e0e\u529f\u80fd\u76f8\u5173\u7684\u533a\u57df\u548c\u4e0e\u529f\u80fd\u65e0\u5173\u7684\u80cc\u666f\u4e0a\u4e0b\u6587\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u9009\u62e9\u6027\u539f\u578b\u548c\u50cf\u7d20\u5bf9\u6bd4\u76ee\u6807\uff0c\u5728\u96f6\u4ef6\u548c\u5bf9\u8c61\u7ea7\u522b\u81ea\u9002\u5e94\u5730\u5b66\u4e60\u4e0e\u529f\u80fd\u76f8\u5173\u7684\u7ebf\u7d22\uff0c\u4ece\u800c\u6709\u6548\u5730\u5c06\u6fc0\u6d3b\u4ece\u4e0d\u76f8\u5173\u7684\u533a\u57df\u8f6c\u79fb\u5230\u6709\u610f\u4e49\u7684\u529f\u80fd\u7ebf\u7d22\u4e0a\uff0c\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.07878", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07878", "abs": "https://arxiv.org/abs/2508.07878", "authors": ["Hanting Wang", "Shengpeng Ji", "Shulei Wang", "Hai Huang", "Xiao Jin", "Qifei Zhang", "Tao Jin"], "title": "TAP: Parameter-efficient Task-Aware Prompting for Adverse Weather Removal", "comment": null, "summary": "Image restoration under adverse weather conditions has been extensively\nexplored, leading to numerous high-performance methods. In particular, recent\nadvances in All-in-One approaches have shown impressive results by training on\nmulti-task image restoration datasets. However, most of these methods rely on\ndedicated network modules or parameters for each specific degradation type,\nresulting in a significant parameter overhead. Moreover, the relatedness across\ndifferent restoration tasks is often overlooked. In light of these issues, we\npropose a parameter-efficient All-in-One image restoration framework that\nleverages task-aware enhanced prompts to tackle various adverse weather\ndegradations.Specifically, we adopt a two-stage training paradigm consisting of\na pretraining phase and a prompt-tuning phase to mitigate parameter conflicts\nacross tasks. We first employ supervised learning to acquire general\nrestoration knowledge, and then adapt the model to handle specific degradation\nvia trainable soft prompts. Crucially, we enhance these task-specific prompts\nin a task-aware manner. We apply low-rank decomposition to these prompts to\ncapture both task-general and task-specific characteristics, and impose\ncontrastive constraints to better align them with the actual inter-task\nrelatedness. These enhanced prompts not only improve the parameter efficiency\nof the restoration model but also enable more accurate task modeling, as\nevidenced by t-SNE analysis. Experimental results on different restoration\ntasks demonstrate that the proposed method achieves superior performance with\nonly 2.75M parameters.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684AIO\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff0c\u5229\u7528\u4efb\u52a1\u611f\u77e5\u589e\u5f3a\u63d0\u793a\u6765\u5904\u7406\u5404\u79cd\u6076\u52a3\u5929\u6c14\u9000\u5316\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u548c\u4f4e\u79e9\u5206\u89e3\u589e\u5f3a\u63d0\u793a\uff0c\u4ee5\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u548c\u4efb\u52a1\u5efa\u6a21\u7684\u51c6\u786e\u6027\uff0c\u4ec5\u75282.75M\u53c2\u6570\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684AIO\u56fe\u50cf\u6062\u590d\u65b9\u6cd5\u901a\u5e38\u9700\u8981\u4e3a\u6bcf\u79cd\u7279\u5b9a\u7684\u9000\u5316\u7c7b\u578b\u8bbe\u8ba1\u4e13\u95e8\u7684\u7f51\u7edc\u6a21\u5757\u6216\u53c2\u6570\uff0c\u5bfc\u81f4\u53c2\u6570\u5f00\u9500\u5927\uff0c\u5e76\u4e14\u5ffd\u89c6\u4e86\u4e0d\u540c\u6062\u590d\u4efb\u52a1\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u53c2\u6570\u9ad8\u6548\u7684AIO\u56fe\u50cf\u6062\u590d\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u4efb\u52a1\u611f\u77e5\u589e\u5f3a\u63d0\u793a\u6765\u5904\u7406\u5404\u79cd\u6076\u52a3\u5929\u6c14\u9000\u5316\u3002\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u8303\u5f0f\uff0c\u5305\u62ec\u9884\u8bad\u7ec3\u9636\u6bb5\u548c\u63d0\u793a\u8c03\u6574\u9636\u6bb5\uff0c\u4ee5\u51cf\u8f7b\u4efb\u52a1\u95f4\u7684\u53c2\u6570\u51b2\u7a81\u3002\u9996\u5148\uff0c\u4f7f\u7528\u76d1\u7763\u5b66\u4e60\u83b7\u53d6\u901a\u7528\u6062\u590d\u77e5\u8bc6\uff0c\u7136\u540e\u901a\u8fc7\u53ef\u8bad\u7ec3\u7684\u8f6f\u63d0\u793a\u5c06\u6a21\u578b\u9002\u5e94\u4e8e\u5904\u7406\u7279\u5b9a\u7684\u9000\u5316\u3002\u81f3\u5173\u91cd\u8981\u7684\u662f\uff0c\u4ee5\u4efb\u52a1\u611f\u77e5\u7684\u65b9\u5f0f\u589e\u5f3a\u8fd9\u4e9b\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684\u63d0\u793a\u3002\u5bf9\u8fd9\u4e9b\u63d0\u793a\u5e94\u7528\u4f4e\u79e9\u5206\u89e3\u4ee5\u6355\u6349\u4efb\u52a1\u901a\u7528\u548c\u4efb\u52a1\u7279\u5b9a\u7684\u7279\u5f81\uff0c\u5e76\u65bd\u52a0\u5bf9\u6bd4\u7ea6\u675f\u4ee5\u66f4\u597d\u5730\u4f7f\u5176\u4e0e\u5b9e\u9645\u7684\u8de8\u4efb\u52a1\u76f8\u5173\u6027\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4ec5\u4f7f\u75282.75M\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u4e0d\u540c\u7684\u6062\u590d\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u901a\u8fc7\u4ec5\u4f7f\u75282.75M\u53c2\u6570\u5728\u4e0d\u540c\u6062\u590d\u4efb\u52a1\u4e0a\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u901a\u8fc7t-SNE\u5206\u6790\u8bc1\u660e\u4e86\u589e\u5f3a\u7684\u63d0\u793a\u80fd\u591f\u63d0\u9ad8\u53c2\u6570\u6548\u7387\u548c\u51c6\u786e\u7684\u4efb\u52a1\u5efa\u6a21\u3002"}}
{"id": "2508.07897", "categories": ["cs.CV", "cs.AI", "I.3.3"], "pdf": "https://arxiv.org/pdf/2508.07897", "abs": "https://arxiv.org/abs/2508.07897", "authors": ["Tianle Zeng", "Junlei Hu", "Gerardo Loza Galindo", "Sharib Ali", "Duygu Sarikaya", "Pietro Valdastri", "Dominic Jones"], "title": "NeeCo: Image Synthesis of Novel Instrument States Based on Dynamic and Deformable 3D Gaussian Reconstruction", "comment": "13 pages, 9 figures", "summary": "Computer vision-based technologies significantly enhance surgical automation\nby advancing tool tracking, detection, and localization. However, Current\ndata-driven approaches are data-voracious, requiring large, high-quality\nlabeled image datasets, which limits their application in surgical data\nscience. Our Work introduces a novel dynamic Gaussian Splatting technique to\naddress the data scarcity in surgical image datasets. We propose a dynamic\nGaussian model to represent dynamic surgical scenes, enabling the rendering of\nsurgical instruments from unseen viewpoints and deformations with real tissue\nbackgrounds. We utilize a dynamic training adjustment strategy to address\nchallenges posed by poorly calibrated camera poses from real-world scenarios.\nAdditionally, we propose a method based on dynamic Gaussians for automatically\ngenerating annotations for our synthetic data. For evaluation, we constructed a\nnew dataset featuring seven scenes with 14,000 frames of tool and camera motion\nand tool jaw articulation, with a background of an ex-vivo porcine model. Using\nthis dataset, we synthetically replicate the scene deformation from the ground\ntruth data, allowing direct comparisons of synthetic image quality.\nExperimental results illustrate that our method generates photo-realistic\nlabeled image datasets with the highest values in Peak-Signal-to-Noise Ratio\n(29.87). We further evaluate the performance of medical-specific neural\nnetworks trained on real and synthetic images using an unseen real-world image\ndataset. Our results show that the performance of models trained on synthetic\nimages generated by the proposed method outperforms those trained with\nstate-of-the-art standard data augmentation by 10%, leading to an overall\nimprovement in model performances by nearly 15%.", "AI": {"tldr": "\u901a\u8fc7\u65b0\u9896\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u624b\u672f\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u751f\u6210\u7684\u5408\u6210\u6570\u636e\u5728\u63d0\u9ad8\u4e0b\u6e38\u4efb\u52a1\u6027\u80fd\u65b9\u9762\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6570\u636e\u9a71\u52a8\u7684\u8ba1\u7b97\u673a\u89c6\u89c9\u6280\u672f\u5728\u624b\u672f\u81ea\u52a8\u5316\u4e2d\u867d\u7136\u6f5c\u529b\u5de8\u5927\uff0c\u4f46\u53d7\u9650\u4e8e\u9700\u8981\u5927\u91cf\u9ad8\u8d28\u91cf\u6807\u8bb0\u56fe\u50cf\u6570\u636e\u96c6\uff0c\u8fd9\u9650\u5236\u4e86\u5176\u5728\u624b\u672f\u6570\u636e\u79d1\u5b66\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\uff0c\u901a\u8fc7\u52a8\u6001\u9ad8\u65af\u6a21\u578b\u8868\u793a\u624b\u672f\u573a\u666f\uff0c\u5e76\u7ed3\u5408\u52a8\u6001\u8bad\u7ec3\u8c03\u6574\u7b56\u7565\u6765\u5904\u7406\u73b0\u5b9e\u4e16\u754c\u4e2d\u76f8\u673a\u59ff\u6001\u6821\u51c6\u4e0d\u4f73\u7684\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u52a8\u6001\u9ad8\u65af\u7684\u65b9\u6cd5\u6765\u81ea\u52a8\u751f\u6210\u5408\u6210\u6570\u636e\u7684\u6ce8\u91ca\u3002", "result": "\u751f\u6210\u7684\u6570\u636e\u96c6\u5728\u5cf0\u503c\u4fe1\u566a\u6bd4\uff08PSNR\uff09\u65b9\u9762\u8fbe\u5230\u4e8629.87\uff0c\u4e3a\u540c\u7c7b\u6700\u4f73\u3002\u4f7f\u7528\u8be5\u5408\u6210\u6570\u636e\u8bad\u7ec3\u7684\u533b\u5b66\u795e\u7ecf\u7f51\u7edc\u6027\u80fd\u6bd4\u4f7f\u7528\u6700\u5148\u8fdb\u6807\u51c6\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u8bad\u7ec3\u7684\u6a21\u578b\u63d0\u9ad8\u4e8610%\uff0c\u6574\u4f53\u6a21\u578b\u6027\u80fd\u63d0\u9ad8\u4e86\u8fd115%\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u7684\u52a8\u6001\u9ad8\u65af\u6cfc\u6e85\u6280\u672f\u80fd\u591f\u751f\u6210\u7167\u7247\u7ea7\u7684\u3001\u5e26\u6709\u6ce8\u91ca\u7684\u5408\u6210\u56fe\u50cf\uff0c\u5e76\u4e14\u5728\u4e0b\u6e38\u533b\u5b66\u56fe\u50cf\u5206\u6790\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5b9e\u73b0\u4e86\u8fd115%\u7684\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2508.07901", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07901", "abs": "https://arxiv.org/abs/2508.07901", "authors": ["Bowen Xue", "Qixin Yan", "Wenjing Wang", "Hao Liu", "Chen Li"], "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video Generation", "comment": null, "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just $\\sim$1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.", "AI": {"tldr": "Stand-In\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u4eba\u8138\u89c6\u9891\uff0c\u53c2\u6570\u5c11\uff0c\u517c\u5bb9\u6027\u597d\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5728\u751f\u6210\u9ad8\u4fdd\u771f\u4eba\u8138\u89c6\u9891\u65f6\u5b58\u5728\u8bad\u7ec3\u53c2\u6570\u8fc7\u591a\u548c\u4e0e\u5176\u4ed6AIGC\u5de5\u5177\u517c\u5bb9\u6027\u5dee\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aStand-In\u7684\u8f7b\u91cf\u7ea7\u3001\u5373\u63d2\u5373\u7528\u6846\u67b6\uff0c\u901a\u8fc7\u5728\u9884\u8bad\u7ec3\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u4e2d\u5f15\u5165\u6761\u4ef6\u56fe\u50cf\u5206\u652f\uff0c\u5e76\u5229\u7528\u53d7\u9650\u81ea\u6ce8\u610f\u529b\u673a\u5236\u548c\u6761\u4ef6\u4f4d\u7f6e\u6620\u5c04\u6765\u5b9e\u73b0\u8eab\u4efd\u63a7\u5236\uff0c\u4ec5\u9700\u7ea62000\u5bf9\u6570\u636e\u5373\u53ef\u5feb\u901f\u5b66\u4e60\u3002", "result": "Stand-In\u6846\u67b6\u5728\u89c6\u9891\u8d28\u91cf\u548c\u8eab\u4efd\u4fdd\u7559\u65b9\u9762\u53d6\u5f97\u4e86\u4f18\u4e8e\u5168\u53c2\u6570\u8bad\u7ec3\u65b9\u6cd5\u7684\u6210\u679c\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u8f7b\u91cf\u5316\u548c\u826f\u597d\u7684\u517c\u5bb9\u6027\u3002", "conclusion": "Stand-In\u6846\u67b6\u80fd\u591f\u4ee5\u6781\u5c11\u7684\u989d\u5916\u53c2\u6570\uff08\u7ea61%\uff09\u5b9e\u73b0\u9ad8\u8d28\u91cf\u548c\u9ad8\u4fdd\u771f\u5ea6\u7684\u8eab\u4efd\u4fe1\u606f\u4fdd\u7559\uff0c\u5e76\u4e14\u80fd\u591f\u4e0e\u5176\u4ed6AIGC\u5de5\u5177\u65e0\u7f1d\u96c6\u6210\uff0c\u5e94\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2508.07903", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07903", "abs": "https://arxiv.org/abs/2508.07903", "authors": ["Johanna P. M\u00fcller", "Anika Knupfer", "Pedro Bl\u00f6ss", "Edoardo Berardi Vittur", "Bernhard Kainz", "Jana Hutter"], "title": "Diffusing the Blind Spot: Uterine MRI Synthesis with Diffusion Models", "comment": "Accepted at MICCAI CAPI 2025", "summary": "Despite significant progress in generative modelling, existing diffusion\nmodels often struggle to produce anatomically precise female pelvic images,\nlimiting their application in gynaecological imaging, where data scarcity and\npatient privacy concerns are critical. To overcome these barriers, we introduce\na novel diffusion-based framework for uterine MRI synthesis, integrating both\nunconditional and conditioned Denoising Diffusion Probabilistic Models (DDPMs)\nand Latent Diffusion Models (LDMs) in 2D and 3D. Our approach generates\nanatomically coherent, high fidelity synthetic images that closely mimic real\nscans and provide valuable resources for training robust diagnostic models. We\nevaluate generative quality using advanced perceptual and distributional\nmetrics, benchmarking against standard reconstruction methods, and demonstrate\nsubstantial gains in diagnostic accuracy on a key classification task. A\nblinded expert evaluation further validates the clinical realism of our\nsynthetic images. We release our models with privacy safeguards and a\ncomprehensive synthetic uterine MRI dataset to support reproducible research\nand advance equitable AI in gynaecology.", "AI": {"tldr": "\u8fd9\u9879\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6269\u6563\u6a21\u578b\u6846\u67b6\uff0c\u7528\u4e8e\u751f\u6210\u9ad8\u4fdd\u771f\u7684\u5b50\u5babMRI\u56fe\u50cf\uff0c\u89e3\u51b3\u4e86\u5987\u79d1\u6210\u50cf\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u548c\u9690\u79c1\u95ee\u9898\uff0c\u5e76\u5728\u8bca\u65ad\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6210\u679c\u3002", "motivation": "\u73b0\u6709\u7684\u6269\u6563\u6a21\u578b\u5728\u751f\u6210\u89e3\u5256\u5b66\u4e0a\u7cbe\u786e\u7684\u5973\u6027\u76c6\u8154\u56fe\u50cf\u65b9\u9762\u5b58\u5728\u56f0\u96be\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u5987\u79d1\u6210\u50cf\u4e2d\u7684\u5e94\u7528\uff0c\u800c\u6570\u636e\u7a00\u758f\u548c\u60a3\u8005\u9690\u79c1\u95ee\u9898\u5728\u5176\u4e2d\u81f3\u5173\u91cd\u8981\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u57fa\u4e8e\u6269\u6563\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u5408\u6210\u5b50\u5babMRI\uff0c\u6574\u5408\u4e86\u65e0\u6761\u4ef6\u548c\u6709\u6761\u4ef6\u7684\u53bb\u566a\u6269\u6563\u6982\u7387\u6a21\u578b\uff08DDPM\uff09\u548c\u6f5c\u5728\u6269\u6563\u6a21\u578b\uff08LDM\uff09\uff0c\u5e76\u652f\u63012D\u548c3D\u3002", "result": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u751f\u6210\u4e86\u89e3\u5256\u5b66\u4e0a\u4e00\u81f4\u3001\u9ad8\u4fdd\u771f\u7684\u5408\u6210\u56fe\u50cf\uff0c\u8fd9\u4e9b\u56fe\u50cf\u5728\u4e34\u5e8a\u4e0a\u4e0e\u771f\u5b9e\u626b\u63cf\u975e\u5e38\u76f8\u4f3c\uff0c\u5e76\u4e14\u5728\u5173\u952e\u7684\u5206\u7c7b\u4efb\u52a1\u4e2d\u663e\u793a\u51fa\u8bca\u65ad\u7cbe\u5ea6\u7684\u663e\u8457\u63d0\u9ad8\u3002\u6b64\u5916\uff0c\u4e13\u5bb6\u76f2\u8bc4\u8fdb\u4e00\u6b65\u9a8c\u8bc1\u4e86\u6211\u4eec\u5408\u6210\u56fe\u50cf\u7684\u4e34\u5e8a\u73b0\u5b9e\u6027\u3002", "conclusion": "\u6211\u4eec\u53d1\u5e03\u4e86\u6a21\u578b\uff0c\u5e76\u9644\u5e26\u4e86\u9690\u79c1\u4fdd\u62a4\u63aa\u65bd\u548c\u4e00\u4e2a\u5168\u9762\u7684\u5408\u6210\u5b50\u5babMRI\u6570\u636e\u96c6\uff0c\u4ee5\u652f\u6301\u53ef\u91cd\u590d\u7684\u7814\u7a76\u5e76\u4fc3\u8fdb\u5987\u79d1\u9886\u57dfAI\u7684\u516c\u5e73\u6027\u3002"}}
{"id": "2508.07904", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07904", "abs": "https://arxiv.org/abs/2508.07904", "authors": ["Marco Peer", "Anna Scius-Bertrand", "Andreas Fischer"], "title": "CTC Transcription Alignment of the Bullinger Letters: Automatic Improvement of Annotation Quality", "comment": "10 pages, 2 pages supplementary material. Accepted for\n  VisionDocs@ICCV2025", "summary": "Handwritten text recognition for historical documents remains challenging due\nto handwriting variability, degraded sources, and limited layout-aware\nannotations. In this work, we address annotation errors - particularly\nhyphenation issues - in the Bullinger correspondence, a large 16th-century\nletter collection. We introduce a self-training method based on a CTC alignment\nalgorithm that matches full transcriptions to text line images using dynamic\nprogramming and model output probabilities trained with the CTC loss. Our\napproach improves performance (e.g., by 1.1 percentage points CER with PyLaia)\nand increases alignment accuracy. Interestingly, we find that weaker models\nyield more accurate alignments, enabling an iterative training strategy. We\nrelease a new manually corrected subset of 100 pages from the Bullinger\ndataset, along with our code and benchmarks. Our approach can be applied\niteratively to further improve the CER as well as the alignment quality for\ntext recognition pipelines. Code and data are available via\nhttps://github.com/andreas-fischer-unifr/nntp.", "AI": {"tldr": "\u9488\u5bf9\u5386\u53f2\u6587\u732e\u7684\u624b\u5199\u6587\u672c\u8bc6\u522b\uff08HTR\uff09\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u65ad\u5b57\u7b49\u6807\u6ce8\u9519\u8bef\u65b9\u9762\uff0c\u6211\u4eec\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCTC\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\u3002\u8be5\u65b9\u6cd5\u901a\u8fc7\u5c06\u5b8c\u6574\u8f6c\u5f55\u4e0e\u6587\u672c\u884c\u56fe\u50cf\u8fdb\u884c\u5339\u914d\u6765\u63d0\u9ad8\u8bc6\u522b\u6027\u80fd\u548c\u5bf9\u9f50\u51c6\u786e\u6027\uff0c\u5e76\u53d1\u73b0\u8f83\u5f31\u7684\u6a21\u578b\u6709\u5229\u4e8e\u63d0\u9ad8\u5bf9\u9f50\u7cbe\u5ea6\uff0c\u4ece\u800c\u652f\u6301\u8fed\u4ee3\u4f18\u5316\u3002\u6211\u4eec\u8fd8\u53d1\u5e03\u4e86\u4e00\u4e2a\u4fee\u6b63\u540e\u7684\u6570\u636e\u96c6\u5b50\u96c6\u3001\u4ee3\u7801\u548c\u57fa\u51c6\u6d4b\u8bd5\u3002", "motivation": "\u624b\u5199\u6587\u672c\u8bc6\u522b\uff08HTR\uff09\u5bf9\u4e8e\u5386\u53f2\u6587\u732e\u4ecd\u7136\u662f\u4e00\u4e2a\u6311\u6218\uff0c\u4e3b\u8981\u662f\u7531\u4e8e\u5b57\u8ff9\u7684\u591a\u6837\u6027\u3001\u6587\u732e\u7684\u9000\u5316\u4ee5\u53ca\u5bf9\u611f\u77e5\u5e03\u5c40\u7684\u6807\u6ce8\u4e0d\u8db3\u3002\u5728\u8fd9\u9879\u5de5\u4f5c\u4e2d\uff0c\u6211\u4eec\u7740\u91cd\u89e3\u51b3\u4e86Bullinger\u901a\u4fe1\uff08\u4e00\u4e2a16\u4e16\u7eaa\u7684\u5927\u578b\u4e66\u4fe1\u96c6\u5408\uff09\u4e2d\u7684\u6807\u6ce8\u9519\u8bef\uff0c\u7279\u522b\u662f\u65ad\u5b57\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eCTC\u5bf9\u9f50\u7b97\u6cd5\u7684\u81ea\u8bad\u7ec3\u65b9\u6cd5\uff0c\u8be5\u7b97\u6cd5\u4f7f\u7528\u52a8\u6001\u89c4\u5212\u548c\u5728CTC\u635f\u5931\u4e0b\u8bad\u7ec3\u7684\u6a21\u578b\u8f93\u51fa\u6765\u5339\u914d\u5b8c\u6574\u8f6c\u5f55\u4e0e\u6587\u672c\u884c\u56fe\u50cf\u3002", "result": "\u8be5\u65b9\u6cd5\u5728PyLaia\u4e0a\u4ee51.1\u4e2a\u767e\u5206\u70b9\u7684CER\u8fdb\u884c\u6539\u8fdb\uff0c\u5e76\u63d0\u9ad8\u4e86\u5bf9\u9f50\u51c6\u786e\u6027\u3002\u7814\u7a76\u53d1\u73b0\uff0c\u8f83\u5f31\u7684\u6a21\u578b\u80fd\u4ea7\u751f\u66f4\u51c6\u786e\u7684\u5bf9\u9f50\uff0c\u4ece\u800c\u652f\u6301\u8fed\u4ee3\u8bad\u7ec3\u7b56\u7565\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u8fed\u4ee3\u5730\u5e94\u7528\u4e8e\u8fdb\u4e00\u6b65\u63d0\u9ad8\u6587\u672c\u8bc6\u522b\u6d41\u6c34\u7ebf\u7684\u8bcd\u9519\u8bef\u7387\uff08CER\uff09\u548c\u5bf9\u9f50\u8d28\u91cf\u3002"}}
{"id": "2508.07905", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07905", "abs": "https://arxiv.org/abs/2508.07905", "authors": ["Yongtao Ge", "Kangyang Xie", "Guangkai Xu", "Mingyu Liu", "Li Ke", "Longtao Huang", "Hui Xue", "Hao Chen", "Chunhua Shen"], "title": "Generative Video Matting", "comment": null, "summary": "Video matting has traditionally been limited by the lack of high-quality\nground-truth data. Most existing video matting datasets provide only\nhuman-annotated imperfect alpha and foreground annotations, which must be\ncomposited to background images or videos during the training stage. Thus, the\ngeneralization capability of previous methods in real-world scenarios is\ntypically poor. In this work, we propose to solve the problem from two\nperspectives. First, we emphasize the importance of large-scale pre-training by\npursuing diverse synthetic and pseudo-labeled segmentation datasets. We also\ndevelop a scalable synthetic data generation pipeline that can render diverse\nhuman bodies and fine-grained hairs, yielding around 200 video clips with a\n3-second duration for fine-tuning. Second, we introduce a novel video matting\napproach that can effectively leverage the rich priors from pre-trained video\ndiffusion models. This architecture offers two key advantages. First, strong\npriors play a critical role in bridging the domain gap between synthetic and\nreal-world scenes. Second, unlike most existing methods that process video\nmatting frame-by-frame and use an independent decoder to aggregate temporal\ninformation, our model is inherently designed for video, ensuring strong\ntemporal consistency. We provide a comprehensive quantitative evaluation across\nthree benchmark datasets, demonstrating our approach's superior performance,\nand present comprehensive qualitative results in diverse real-world scenes,\nillustrating the strong generalization capability of our method. The code is\navailable at https://github.com/aim-uofa/GVM.", "AI": {"tldr": "\u7531\u4e8e\u7f3a\u4e4f\u9ad8\u8d28\u91cf\u7684\u57fa\u7840\u6570\u636e\uff0c\u89c6\u9891\u62a0\u50cf\u7684\u6548\u679c\u53d7\u5230\u9650\u5236\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u4e2a\u65b0\u9896\u7684\u89c6\u9891\u62a0\u50cf\u65b9\u6cd5\uff0c\u5e76\u7ed3\u5408\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u548c\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e30\u5bcc\u5148\u9a8c\u77e5\u8bc6\u3002\u8be5\u65b9\u6cd5\u5728\u5904\u7406\u89c6\u9891\u65f6\u5177\u6709\u5185\u5728\u7684\u65f6\u95f4\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u80fd\u591f\u6709\u6548\u5730\u7f29\u5c0f\u5408\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u3002\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u7684\u6848\u4f8b\u7814\u7a76\u5747\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5177\u6709\u4f18\u8d8a\u7684\u6027\u80fd\u548c\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u89c6\u9891\u62a0\u50cf\u4f20\u7edf\u4e0a\u53d7\u5230\u9ad8\u8d28\u91cf\u57fa\u7840\u6570\u636e\u7684\u9650\u5236\u3002\u73b0\u6709\u7684\u5927\u591a\u6570\u89c6\u9891\u62a0\u50cf\u6570\u636e\u96c6\u4ec5\u63d0\u4f9b\u4eba\u7c7b\u6ce8\u91ca\u7684\u4e0d\u5b8c\u7f8e\u963f\u5c14\u6cd5\u548c\u524d\u666f\u6ce8\u91ca\uff0c\u8fd9\u4e9b\u6ce8\u91ca\u5728\u8bad\u7ec3\u9636\u6bb5\u5fc5\u987b\u4e0e\u80cc\u666f\u56fe\u50cf\u6216\u89c6\u9891\u8fdb\u884c\u5408\u6210\u3002\u56e0\u6b64\uff0c\u4ee5\u524d\u7684\u65b9\u6cd5\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u6cdb\u5316\u80fd\u529b\u901a\u5e38\u5f88\u5dee\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u89c6\u9891\u62a0\u50cf\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u591f\u6709\u6548\u5229\u7528\u9884\u8bad\u7ec3\u89c6\u9891\u6269\u6563\u6a21\u578b\u7684\u4e30\u5bcc\u5148\u9a8c\u77e5\u8bc6\u3002\u8be5\u6a21\u578b\u67b6\u6784\u5177\u6709\u4e24\u4e2a\u4e3b\u8981\u4f18\u70b9\uff1a1. \u5f3a\u5927\u7684\u5148\u9a8c\u77e5\u8bc6\u5728\u7f29\u5c0f\u5408\u6210\u4e0e\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e4b\u95f4\u7684\u57df\u5dee\u8ddd\u65b9\u9762\u53d1\u6325\u7740\u5173\u952e\u4f5c\u7528\u30022. \u4e0e\u5927\u591a\u6570\u73b0\u6709\u65b9\u6cd5\u9010\u5e27\u5904\u7406\u89c6\u9891\u62a0\u50cf\u5e76\u4f7f\u7528\u72ec\u7acb\u89e3\u7801\u5668\u805a\u5408\u65f6\u95f4\u4fe1\u606f\u4e0d\u540c\uff0c\u8be5\u6a21\u578b\u5728\u8bbe\u8ba1\u4e0a\u5929\u7136\u652f\u6301\u89c6\u9891\u5904\u7406\uff0c\u786e\u4fdd\u4e86\u5f3a\u5927\u7684\u65f6\u95f4\u4e00\u81f4\u6027\u3002\u6b64\u5916\uff0c\u7814\u7a76\u4eba\u5458\u8fd8\u5f3a\u8c03\u4e86\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u7684\u91cd\u8981\u6027\uff0c\u65b9\u6cd5\u5305\u62ec\u5229\u7528\u591a\u6837\u5316\u7684\u5408\u6210\u6570\u636e\u548c\u4f2a\u6807\u7b7e\u5206\u5272\u6570\u636e\u96c6\uff0c\u5e76\u5f00\u53d1\u4e86\u4e00\u4e2a\u53ef\u6269\u5c55\u7684\u5408\u6210\u6570\u636e\u751f\u6210\u6d41\u7a0b\uff0c\u80fd\u591f\u6e32\u67d3\u591a\u6837\u5316\u7684\u4eba\u7269\u8eab\u4f53\u548c\u7cbe\u7ec6\u7684\u5934\u53d1\uff0c\u751f\u6210\u5927\u7ea6200\u4e2a\u65f6\u957f\u4e3a3\u79d2\u7684\u89c6\u9891\u7247\u6bb5\u7528\u4e8e\u5fae\u8c03\u3002", "result": "\u7814\u7a76\u4eba\u5458\u63d0\u4f9b\u4e86\u4e00\u4e2a\u5168\u9762\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5c55\u793a\u4e86\u6240\u63d0\u51fa\u65b9\u6cd5\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u7684\u4f18\u8d8a\u6027\u80fd\uff0c\u5e76\u5728\u5404\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5168\u9762\u7684\u5b9a\u6027\u7ed3\u679c\uff0c\u8bf4\u660e\u4e86\u8be5\u65b9\u6cd5\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002\u63d0\u4f9b\u7684\u4ee3\u7801\u53ef\u5728https://github.com/aim-uofa/GVM\u627e\u5230\u3002", "conclusion": "\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5168\u9762\u7684\u5b9a\u91cf\u8bc4\u4f30\uff0c\u5e76\u5728\u5404\u79cd\u771f\u5b9e\u573a\u666f\u4e2d\u5c55\u793a\u4e86\u5168\u9762\u7684\u5b9a\u6027\u7ed3\u679c\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u771f\u5b9e\u4e16\u754c\u573a\u666f\u4e2d\u7684\u5f3a\u5927\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2508.07908", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07908", "abs": "https://arxiv.org/abs/2508.07908", "authors": ["Xudong Cai", "Shuo Wang", "Peng Wang", "Yongcai Wang", "Zhaoxin Fan", "Wanting Li", "Tianbao Zhang", "Jianrong Tao", "Yeying Jin", "Deying Li"], "title": "Mem4D: Decoupling Static and Dynamic Memory for Dynamic Scene Reconstruction", "comment": null, "summary": "Reconstructing dense geometry for dynamic scenes from a monocular video is a\ncritical yet challenging task. Recent memory-based methods enable efficient\nonline reconstruction, but they fundamentally suffer from a Memory Demand\nDilemma: The memory representation faces an inherent conflict between the\nlong-term stability required for static structures and the rapid, high-fidelity\ndetail retention needed for dynamic motion. This conflict forces existing\nmethods into a compromise, leading to either geometric drift in static\nstructures or blurred, inaccurate reconstructions of dynamic objects. To\naddress this dilemma, we propose Mem4D, a novel framework that decouples the\nmodeling of static geometry and dynamic motion. Guided by this insight, we\ndesign a dual-memory architecture: 1) The Transient Dynamics Memory (TDM)\nfocuses on capturing high-frequency motion details from recent frames, enabling\naccurate and fine-grained modeling of dynamic content; 2) The Persistent\nStructure Memory (PSM) compresses and preserves long-term spatial information,\nensuring global consistency and drift-free reconstruction for static elements.\nBy alternating queries to these specialized memories, Mem4D simultaneously\nmaintains static geometry with global consistency and reconstructs dynamic\nelements with high fidelity. Experiments on challenging benchmarks demonstrate\nthat our method achieves state-of-the-art or competitive performance while\nmaintaining high efficiency. Codes will be publicly available.", "AI": {"tldr": "Mem4D \u901a\u8fc7\u53cc\u8bb0\u5fc6\uff08TDM \u548c PSM\uff09\u89e3\u8026\u9759\u6001\u548c\u52a8\u6001\u4fe1\u606f\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u5355\u76ee\u91cd\u5efa\u7684\u5185\u5b58\u56f0\u5883\uff0c\u5b9e\u73b0\u4e86\u9ad8\u7cbe\u5ea6\u548c\u9ad8\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u52a8\u6001\u573a\u666f\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u4e2d\uff0c\u5185\u5b58\u8868\u793a\u5728\u957f\u671f\u7a33\u5b9a\u6027\uff08\u9759\u6001\u7ed3\u6784\uff09\u548c\u9ad8\u4fdd\u771f\u7ec6\u8282\u4fdd\u7559\uff08\u52a8\u6001\u8fd0\u52a8\uff09\u4e4b\u95f4\u5b58\u5728\u7684\u56fa\u6709\u7684\u51b2\u7a81\uff0c\u8fd9\u79cd\u51b2\u7a81\u5bfc\u81f4\u73b0\u6709\u65b9\u6cd5\u5728\u9759\u6001\u7ed3\u6784\u51e0\u4f55\u6f02\u79fb\u6216\u52a8\u6001\u7269\u4f53\u6a21\u7cca\u4e0d\u51c6\u786e\u4e4b\u95f4\u8fdb\u884c\u59a5\u534f\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Mem4D \u7684\u65b0\u9896\u6846\u67b6\uff0c\u91c7\u7528\u53cc\u8bb0\u5fc6\u67b6\u6784\uff1a1) \u77ac\u6001\u52a8\u6001\u8bb0\u5fc6 (TDM) \u6355\u6349\u6700\u8fd1\u5e27\u7684\u9ad8\u9891\u8fd0\u52a8\u7ec6\u8282\uff1b2) \u6301\u4e45\u7ed3\u6784\u8bb0\u5fc6 (PSM) \u538b\u7f29\u5e76\u4fdd\u7559\u957f\u671f\u7a7a\u95f4\u4fe1\u606f\u3002\u901a\u8fc7\u4ea4\u66ff\u67e5\u8be2\u8fd9\u4e9b\u4e13\u7528\u8bb0\u5fc6\uff0c\u540c\u65f6\u4fdd\u6301\u9759\u6001\u51e0\u4f55\u7684\u5168\u5c40\u4e00\u81f4\u6027\u548c\u52a8\u6001\u5143\u7d20\u7684\u4fdd\u771f\u5ea6\u3002", "result": "\u5728\u5177\u6709\u6311\u6218\u6027\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u7ed3\u679c\u8868\u660e\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6216\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002", "conclusion": "Mem4D \u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u9896\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u89e3\u8026\u9759\u6001\u51e0\u4f55\u548c\u52a8\u6001\u8fd0\u52a8\u7684\u5efa\u6a21\uff0c\u89e3\u51b3\u4e86\u52a8\u6001\u573a\u666f\u5355\u76ee\u89c6\u9891\u91cd\u5efa\u4e2d\u7684\u5185\u5b58\u9700\u6c42\u56f0\u5883\u3002\u8be5\u6846\u67b6\u91c7\u7528\u53cc\u8bb0\u5fc6\u67b6\u6784\uff1a\u77ac\u6001\u52a8\u6001\u8bb0\u5fc6\uff08TDM\uff09\u7528\u4e8e\u6355\u6349\u9ad8\u9891\u8fd0\u52a8\u7ec6\u8282\uff0c\u6301\u4e45\u7ed3\u6784\u8bb0\u5fc6\uff08PSM\uff09\u7528\u4e8e\u538b\u7f29\u548c\u4fdd\u7559\u957f\u671f\u7a7a\u95f4\u4fe1\u606f\uff0c\u4ee5\u786e\u4fdd\u5168\u5c40\u4e00\u81f4\u6027\u548c\u65e0\u6f02\u79fb\u91cd\u5efa\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0cMem4D \u5728\u4fdd\u6301\u9ad8\u6548\u7387\u7684\u540c\u65f6\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u6216\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\u3002"}}
{"id": "2508.07918", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07918", "abs": "https://arxiv.org/abs/2508.07918", "authors": ["Xing Zi", "Jinghao Xiao", "Yunxiao Shi", "Xian Tao", "Jun Li", "Ali Braytee", "Mukesh Prasad"], "title": "RSVLM-QA: A Benchmark Dataset for Remote Sensing Vision Language Model-based Question Answering", "comment": "This paper has been accepted to the proceedings of the 33rd ACM\n  International Multimedia Conference (ACM Multimedia 2025)", "summary": "Visual Question Answering (VQA) in remote sensing (RS) is pivotal for\ninterpreting Earth observation data. However, existing RS VQA datasets are\nconstrained by limitations in annotation richness, question diversity, and the\nassessment of specific reasoning capabilities. This paper introduces RSVLM-QA\ndataset, a new large-scale, content-rich VQA dataset for the RS domain.\nRSVLM-QA is constructed by integrating data from several prominent RS\nsegmentation and detection datasets: WHU, LoveDA, INRIA, and iSAID. We employ\nan innovative dual-track annotation generation pipeline. Firstly, we leverage\nLarge Language Models (LLMs), specifically GPT-4.1, with meticulously designed\nprompts to automatically generate a suite of detailed annotations including\nimage captions, spatial relations, and semantic tags, alongside complex\ncaption-based VQA pairs. Secondly, to address the challenging task of object\ncounting in RS imagery, we have developed a specialized automated process that\nextracts object counts directly from the original segmentation data; GPT-4.1\nthen formulates natural language answers from these counts, which are paired\nwith preset question templates to create counting QA pairs. RSVLM-QA comprises\n13,820 images and 162,373 VQA pairs, featuring extensive annotations and\ndiverse question types. We provide a detailed statistical analysis of the\ndataset and a comparison with existing RS VQA benchmarks, highlighting the\nsuperior depth and breadth of RSVLM-QA's annotations. Furthermore, we conduct\nbenchmark experiments on Six mainstream Vision Language Models (VLMs),\ndemonstrating that RSVLM-QA effectively evaluates and challenges the\nunderstanding and reasoning abilities of current VLMs in the RS domain. We\nbelieve RSVLM-QA will serve as a pivotal resource for the RS VQA and VLM\nresearch communities, poised to catalyze advancements in the field.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86RSVLM-QA\uff0c\u4e00\u4e2a\u5927\u89c4\u6a21\u3001\u6807\u6ce8\u4e30\u5bcc\u7684\u9065\u611f\u89c6\u89c9\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u65e8\u5728\u514b\u670d\u73b0\u6709\u6570\u636e\u96c6\u7684\u5c40\u9650\u6027\uff0c\u5e76\u901a\u8fc7\u5305\u542b13,820\u5f20\u56fe\u50cf\u548c162,373\u4e2a\u95ee\u7b54\u5bf9\uff0c\u6709\u6548\u8bc4\u4f30\u548c\u63a8\u52a8\u9065\u611f\u9886\u57df\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u53d1\u5c55\u3002", "motivation": "\u73b0\u6709\u9065\u611f\u9886\u57df\u89c6\u89c9\u95ee\u7b54\uff08RS VQA\uff09\u6570\u636e\u96c6\u5728\u6807\u6ce8\u4e30\u5bcc\u5ea6\u3001\u95ee\u9898\u591a\u6837\u6027\u548c\u7279\u5b9a\u63a8\u7406\u80fd\u529b\u8bc4\u4f30\u65b9\u9762\u5b58\u5728\u5c40\u9650\u6027\u3002\u672c\u7814\u7a76\u65e8\u5728\u521b\u5efa\u4e00\u4e2a\u66f4\u5927\u89c4\u6a21\u3001\u6807\u6ce8\u66f4\u4e30\u5bcc\u3001\u95ee\u9898\u66f4\u591a\u6837\u5316\u7684RS VQA\u6570\u636e\u96c6\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86RSVLM-QA\u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u901a\u8fc7\u6574\u5408\u591a\u4e2a\u9065\u611f\u6570\u636e\u96c6\u5e76\u91c7\u7528\u521b\u65b0\u7684\u53cc\u8f68\u9053\u6807\u6ce8\u751f\u6210\u6d41\u7a0b\u6784\u5efa\uff0c\u7279\u522b\u662f\u5229\u7528GPT-4.1\u81ea\u52a8\u751f\u6210\u8be6\u7ec6\u6807\u6ce8\u548c\u95ee\u7b54\u5bf9\uff0c\u5e76\u9488\u5bf9\u9065\u611f\u56fe\u50cf\u4e2d\u7684\u7269\u4f53\u8ba1\u6570\u95ee\u9898\u5f00\u53d1\u4e86\u4e13\u95e8\u7684\u81ea\u52a8\u5316\u6d41\u7a0b\u3002", "result": "RSVLM-QA\u6570\u636e\u96c6\u5305\u542b13,820\u5f20\u56fe\u50cf\u548c162,373\u4e2a\u95ee\u7b54\u5bf9\uff0c\u5177\u6709\u4e30\u5bcc\u7684\u6807\u6ce8\u548c\u591a\u6837\u7684\u63d0\u95ee\u7c7b\u578b\u3002\u901a\u8fc7\u5728\u516d\u4e2a\u4e3b\u6d41\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u4e0a\u7684\u57fa\u51c6\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6570\u636e\u96c6\u80fd\u6709\u6548\u8bc4\u4f30\u548c\u6311\u6218\u5f53\u524dVLMs\u5728\u9065\u611f\u9886\u57df\u7684\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "RSVLM-QA\u6570\u636e\u96c6\u4e3a\u9065\u611f\u9886\u57df\u89c6\u89c9\u95ee\u7b54\u7814\u7a76\u63d0\u4f9b\u4e86\u4e30\u5bcc\u7684\u6570\u636e\u8d44\u6e90\u548c\u6709\u6548\u7684\u8bc4\u4f30\u57fa\u51c6\uff0c\u6709\u671b\u63a8\u52a8\u8be5\u9886\u57df\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.07923", "categories": ["cs.CV", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.07923", "abs": "https://arxiv.org/abs/2508.07923", "authors": ["Jakub Binda", "Valentina Paneta", "Vasileios Eleftheriadis", "Hongkyou Chung", "Panagiotis Papadimitroulas", "Neo Christopher Chung"], "title": "Safeguarding Generative AI Applications in Preclinical Imaging through Hybrid Anomaly Detection", "comment": null, "summary": "Generative AI holds great potentials to automate and enhance data synthesis\nin nuclear medicine. However, the high-stakes nature of biomedical imaging\nnecessitates robust mechanisms to detect and manage unexpected or erroneous\nmodel behavior. We introduce development and implementation of a hybrid anomaly\ndetection framework to safeguard GenAI models in BIOEMTECH's eyes(TM) systems.\nTwo applications are demonstrated: Pose2Xray, which generates synthetic X-rays\nfrom photographic mouse images, and DosimetrEYE, which estimates 3D radiation\ndose maps from 2D SPECT/CT scans. In both cases, our outlier detection (OD)\nenhances reliability, reduces manual oversight, and supports real-time quality\ncontrol. This approach strengthens the industrial viability of GenAI in\npreclinical settings by increasing robustness, scalability, and regulatory\ncompliance.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u63d0\u9ad8\u6838\u533b\u5b66\u9886\u57df\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u6a21\u578b\u7684\u53ef\u9760\u6027\uff0c\u901a\u8fc7\u5728 X \u5c04\u7ebf\u751f\u6210\u548c\u8f90\u5c04\u5242\u91cf\u56fe\u4f30\u8ba1\u7b49\u5e94\u7528\u4e2d\u68c0\u6d4b\u5f02\u5e38\u6765\u589e\u5f3a\u8d28\u91cf\u63a7\u5236\u548c\u5de5\u4e1a\u53ef\u884c\u6027\u3002", "motivation": "\u5728\u6838\u533b\u5b66\u9886\u57df\uff0c\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u5728\u81ea\u52a8\u5316\u548c\u589e\u5f3a\u6570\u636e\u5408\u6210\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u751f\u7269\u533b\u5b66\u6210\u50cf\u7684\u9ad8\u98ce\u9669\u6027\u9700\u8981\u5f3a\u5927\u7684\u673a\u5236\u6765\u68c0\u6d4b\u548c\u7ba1\u7406\u6a21\u578b\u884c\u4e3a\u7684\u5f02\u5e38\uff0c\u4ee5\u786e\u4fdd\u53ef\u9760\u6027\u548c\u5b89\u5168\u6027\u3002", "method": "\u672c\u7814\u7a76\u5f15\u5165\u5e76\u5b9e\u73b0\u4e86\u4e00\u79cd\u6df7\u5408\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u5e76\u5728\u4e24\u4e2a\u5e94\u7528\uff08Pose2Xray \u548c DosimetrEYE\uff09\u4e2d\u8fdb\u884c\u4e86\u6f14\u793a\uff0c\u4ee5\u68c0\u6d4b\u548c\u7ba1\u7406 GenAI \u6a21\u578b\u7684\u610f\u5916\u6216\u9519\u8bef\u884c\u4e3a\u3002", "result": "\u901a\u8fc7\u5728 Pose2Xray \u548c DosimetrEYE \u5e94\u7528\u4e2d\u96c6\u6210\u5f02\u5e38\u68c0\u6d4b\uff08OD\uff09\uff0c\u672c\u7814\u7a76\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u63d0\u9ad8\u53ef\u9760\u6027\u3001\u51cf\u5c11\u4eba\u5de5\u76d1\u7763\u5e76\u652f\u6301\u5b9e\u65f6\u8d28\u91cf\u63a7\u5236\u3002", "conclusion": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u6df7\u5408\u5f02\u5e38\u68c0\u6d4b\u6846\u67b6\uff0c\u7528\u4e8e\u4fdd\u62a4\u6838\u533b\u5b66\u9886\u57df\u751f\u6210\u5f0f\u4eba\u5de5\u667a\u80fd\uff08GenAI\uff09\u6a21\u578b\u7684\u7a33\u5065\u6027\u3001\u53ef\u6269\u5c55\u6027\u548c\u5408\u89c4\u6027\uff0c\u4ece\u800c\u589e\u5f3a\u5176\u5728\u4e34\u5e8a\u524d\u73af\u5883\u4e2d\u7684\u5de5\u4e1a\u53ef\u884c\u6027\u3002"}}
{"id": "2508.07925", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07925", "abs": "https://arxiv.org/abs/2508.07925", "authors": ["Jin-Seop Lee", "SungJoon Lee", "Jaehan Ahn", "YunSeok Choi", "Jee-Hyong Lee"], "title": "TAG: A Simple Yet Effective Temporal-Aware Approach for Zero-Shot Video Temporal Grounding", "comment": null, "summary": "Video Temporal Grounding (VTG) aims to extract relevant video segments based\non a given natural language query. Recently, zero-shot VTG methods have gained\nattention by leveraging pretrained vision-language models (VLMs) to localize\ntarget moments without additional training. However, existing approaches suffer\nfrom semantic fragmentation, where temporally continuous frames sharing the\nsame semantics are split across multiple segments. When segments are\nfragmented, it becomes difficult to predict an accurate target moment that\naligns with the text query. Also, they rely on skewed similarity distributions\nfor localization, making it difficult to select the optimal segment.\nFurthermore, they heavily depend on the use of LLMs which require expensive\ninferences. To address these limitations, we propose a \\textit{TAG}, a simple\nyet effective Temporal-Aware approach for zero-shot video temporal Grounding,\nwhich incorporates temporal pooling, temporal coherence clustering, and\nsimilarity adjustment. Our proposed method effectively captures the temporal\ncontext of videos and addresses distorted similarity distributions without\ntraining. Our approach achieves state-of-the-art results on Charades-STA and\nActivityNet Captions benchmark datasets without rely on LLMs. Our code is\navailable at https://github.com/Nuetee/TAG", "AI": {"tldr": "TAG is a new approach for zero-shot video temporal grounding that improves accuracy and efficiency by incorporating temporal awareness and avoiding LLMs.", "motivation": "Existing zero-shot VTG methods suffer from semantic fragmentation, skewed similarity distributions, and heavy reliance on expensive LLM inferences. This work proposes TAG to address these limitations.", "method": "TAG incorporates temporal pooling, temporal coherence clustering, and similarity adjustment to address semantic fragmentation and skewed similarity distributions in zero-shot video temporal grounding.", "result": "TAG achieves state-of-the-art results on Charades-STA and ActivityNet Captions benchmark datasets.", "conclusion": "TAG effectively captures temporal context and addresses distorted similarity distributions without training, achieving state-of-the-art results on Charades-STA and ActivityNet Captions benchmark datasets without relying on LLMs."}}
{"id": "2508.07960", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07960", "abs": "https://arxiv.org/abs/2508.07960", "authors": ["Ajnas Muhammed", "Iurri Medvedev", "Nuno Gon\u00e7alves"], "title": "VOIDFace: A Privacy-Preserving Multi-Network Face Recognition With Enhanced Security", "comment": "Accepted at IEEE International Joint Conference on Biometrics (IJCB)\n  2025", "summary": "Advancement of machine learning techniques, combined with the availability of\nlarge-scale datasets, has significantly improved the accuracy and efficiency of\nfacial recognition. Modern facial recognition systems are trained using large\nface datasets collected from diverse individuals or public repositories.\nHowever, for training, these datasets are often replicated and stored in\nmultiple workstations, resulting in data replication, which complicates\ndatabase management and oversight. Currently, once a user submits their face\nfor dataset preparation, they lose control over how their data is used, raising\nsignificant privacy and ethical concerns. This paper introduces VOIDFace, a\nnovel framework for facial recognition systems that addresses two major issues.\nFirst, it eliminates the need of data replication and improves data control to\nsecurely store training face data by using visual secret sharing. Second, it\nproposes a patch-based multi-training network that uses this novel training\ndata storage mechanism to develop a robust, privacy-preserving facial\nrecognition system. By integrating these advancements, VOIDFace aims to improve\nthe privacy, security, and efficiency of facial recognition training, while\nensuring greater control over sensitive personal face data. VOIDFace also\nenables users to exercise their Right-To-Be-Forgotten property to control their\npersonal data. Experimental evaluations on the VGGFace2 dataset show that\nVOIDFace provides Right-To-Be-Forgotten, improved data control, security, and\nprivacy while maintaining competitive facial recognition performance. Code is\navailable at: https://github.com/ajnasmuhammed89/VOIDFace", "AI": {"tldr": "VOIDFace is a novel framework for facial recognition that enhances privacy, security, and efficiency by using visual secret sharing to eliminate data replication and improve user control, while a patch-based multi-training network ensures robust recognition. It also enables users to exercise their Right-To-Be-Forgotten.", "motivation": "Current facial recognition systems face issues with data replication, complicating database management, and lack of user control over personal data, raising privacy and ethical concerns. VOIDFace aims to address these issues.", "method": "VOIDFace uses visual secret sharing to securely store training face data, eliminating data replication and improving data control. It also proposes a patch-based multi-training network for privacy-preserving facial recognition.", "result": "Experimental evaluations on the VGGFace2 dataset show that VOIDFace provides Right-To-Be-Forgotten, improved data control, security, and privacy while maintaining competitive facial recognition performance.", "conclusion": "VOIDFace provides Right-To-Be-Forgotten, improved data control, security, and privacy while maintaining competitive facial recognition performance."}}
{"id": "2508.07968", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07968", "abs": "https://arxiv.org/abs/2508.07968", "authors": ["Tony Danjun Wang", "Christian Heiliger", "Nassir Navab", "Lennart Bastian"], "title": "TrackOR: Towards Personalized Intelligent Operating Rooms Through Robust Tracking", "comment": "Full Research Paper, presented at MICCAI'25 Workshop on Collaborative\n  Intelligence and Autonomy in Image-guided Surgery", "summary": "Providing intelligent support to surgical teams is a key frontier in\nautomated surgical scene understanding, with the long-term goal of improving\npatient outcomes. Developing personalized intelligence for all staff members\nrequires maintaining a consistent state of who is located where for long\nsurgical procedures, which still poses numerous computational challenges. We\npropose TrackOR, a framework for tackling long-term multi-person tracking and\nre-identification in the operating room. TrackOR uses 3D geometric signatures\nto achieve state-of-the-art online tracking performance (+11% Association\nAccuracy over the strongest baseline), while also enabling an effective offline\nrecovery process to create analysis-ready trajectories. Our work shows that by\nleveraging 3D geometric information, persistent identity tracking becomes\nattainable, enabling a critical shift towards the more granular, staff-centric\nanalyses required for personalized intelligent systems in the operating room.\nThis new capability opens up various applications, including our proposed\ntemporal pathway imprints that translate raw tracking data into actionable\ninsights for improving team efficiency and safety and ultimately providing\npersonalized support.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.07981", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.07981", "abs": "https://arxiv.org/abs/2508.07981", "authors": ["Fangyuan Mao", "Aiming Hao", "Jintao Chen", "Dongxia Liu", "Xiaokun Feng", "Jiashu Zhu", "Meiqi Wu", "Chubin Chen", "Jiahong Wu", "Xiangxiang Chu"], "title": "Omni-Effects: Unified and Spatially-Controllable Visual Effects Generation", "comment": null, "summary": "Visual effects (VFX) are essential visual enhancements fundamental to modern\ncinematic production. Although video generation models offer cost-efficient\nsolutions for VFX production, current methods are constrained by per-effect\nLoRA training, which limits generation to single effects. This fundamental\nlimitation impedes applications that require spatially controllable composite\neffects, i.e., the concurrent generation of multiple effects at designated\nlocations. However, integrating diverse effects into a unified framework faces\nmajor challenges: interference from effect variations and spatial\nuncontrollability during multi-VFX joint training. To tackle these challenges,\nwe propose Omni-Effects, a first unified framework capable of generating\nprompt-guided effects and spatially controllable composite effects. The core of\nour framework comprises two key innovations: (1) LoRA-based Mixture of Experts\n(LoRA-MoE), which employs a group of expert LoRAs, integrating diverse effects\nwithin a unified model while effectively mitigating cross-task interference.\n(2) Spatial-Aware Prompt (SAP) incorporates spatial mask information into the\ntext token, enabling precise spatial control. Furthermore, we introduce an\nIndependent-Information Flow (IIF) module integrated within the SAP, isolating\nthe control signals corresponding to individual effects to prevent any unwanted\nblending. To facilitate this research, we construct a comprehensive VFX dataset\nOmni-VFX via a novel data collection pipeline combining image editing and\nFirst-Last Frame-to-Video (FLF2V) synthesis, and introduce a dedicated VFX\nevaluation framework for validating model performance. Extensive experiments\ndemonstrate that Omni-Effects achieves precise spatial control and diverse\neffect generation, enabling users to specify both the category and location of\ndesired effects.", "AI": {"tldr": "Omni-Effects \u662f\u4e00\u4e2a\u7edf\u4e00\u7684 VFX \u6846\u67b6\uff0c\u53ef\u5b9e\u73b0\u591a\u6548\u679c\u751f\u6210\u548c\u7a7a\u95f4\u63a7\u5236\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u5f53\u524d\u7684\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728 VFX \u5236\u4f5c\u65b9\u9762\u53d7\u5230\u6bcf\u4e2a\u6548\u679c LoRA \u8bad\u7ec3\u7684\u9650\u5236\uff0c\u8fd9\u4ec5\u5c06\u751f\u6210\u9650\u5236\u4e3a\u5355\u4e00\u6548\u679c\u3002\u8fd9\u79cd\u6839\u672c\u6027\u7684\u9650\u5236\u963b\u788d\u4e86\u9700\u8981\u7a7a\u95f4\u53ef\u63a7\u7684\u590d\u5408\u6548\u679c\u7684\u5e94\u7528\uff0c\u5373\u5728\u6307\u5b9a\u4f4d\u7f6e\u540c\u65f6\u751f\u6210\u591a\u79cd\u6548\u679c\u3002\u7136\u800c\uff0c\u5c06\u5404\u79cd\u6548\u679c\u96c6\u6210\u5230\u7edf\u4e00\u7684\u6846\u67b6\u4e2d\u9762\u4e34\u7740\u4e3b\u8981\u7684\u6311\u6218\uff1a\u6548\u679c\u53d8\u5316\u9020\u6210\u7684\u5e72\u6270\u548c\u591a VFX \u8054\u5408\u8bad\u7ec3\u671f\u95f4\u7684\u7a7a\u95f4\u4e0d\u53ef\u63a7\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Omni-Effects \u7684\u7edf\u4e00\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u521b\u65b0\uff1a1. \u57fa\u4e8e LoRA \u7684\u4e13\u5bb6\u6df7\u5408\uff08LoRA-MoE\uff09\uff0c\u5b83\u91c7\u7528\u4e00\u7ec4\u4e13\u5bb6 LoRA\uff0c\u5728\u7edf\u4e00\u7684\u6a21\u578b\u4e2d\u96c6\u6210\u5404\u79cd\u6548\u679c\uff0c\u540c\u65f6\u6709\u6548\u51cf\u8f7b\u8de8\u4efb\u52a1\u5e72\u6270\u3002 2. \u7a7a\u95f4\u611f\u77e5\u63d0\u793a\uff08SAP\uff09\uff0c\u5b83\u5c06\u7a7a\u95f4\u63a9\u7801\u4fe1\u606f\u96c6\u6210\u5230\u6587\u672c\u4ee4\u724c\u4e2d\uff0c\u4ece\u800c\u5b9e\u73b0\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\u3002\u6b64\u5916\uff0c\u8fd8\u5f15\u5165\u4e86\u4e00\u4e2a\u96c6\u6210\u5728 SAP \u5185\u7684\u72ec\u7acb\u4fe1\u606f\u6d41\uff08IIF\uff09\u6a21\u5757\uff0c\u4ee5\u9694\u79bb\u5bf9\u5e94\u4e8e\u5355\u4e2a\u6548\u679c\u7684\u63a7\u5236\u4fe1\u53f7\uff0c\u4ece\u800c\u9632\u6b62\u4efb\u4f55\u4e0d\u671f\u671b\u7684\u6df7\u5408\u3002\u4e3a\u4e86\u4fbf\u4e8e\u6b64\u7814\u7a76\uff0c\u901a\u8fc7\u7ed3\u5408\u56fe\u50cf\u7f16\u8f91\u548c\u7b2c\u4e00-\u6700\u540e\u4e00\u5e27\u5230\u89c6\u9891\uff08FLF2V\uff09\u5408\u6210\u7684\u65b0\u9896\u6570\u636e\u6536\u96c6\u7ba1\u9053\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u5168\u9762\u7684 VFX \u6570\u636e\u96c6 Omni-VFX\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u4e13\u95e8\u7684 VFX \u8bc4\u4f30\u6846\u67b6\u6765\u9a8c\u8bc1\u6a21\u578b\u6027\u80fd\u3002", "result": "Omni-Effects \u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\u548c\u591a\u6837\u5316\u7684\u6548\u679c\u751f\u6210\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u6307\u5b9a\u6240\u9700\u6548\u679c\u7684\u7c7b\u522b\u548c\u4f4d\u7f6e\u3002", "conclusion": "Omni-Effects \u5b9e\u73b0\u4e86\u7cbe\u786e\u7684\u7a7a\u95f4\u63a7\u5236\u548c\u591a\u6837\u5316\u7684\u6548\u679c\u751f\u6210\uff0c\u4f7f\u7528\u6237\u80fd\u591f\u6307\u5b9a\u6240\u9700\u6548\u679c\u7684\u7c7b\u522b\u548c\u4f4d\u7f6e\u3002"}}
{"id": "2508.07989", "categories": ["cs.CV", "cs.HC"], "pdf": "https://arxiv.org/pdf/2508.07989", "abs": "https://arxiv.org/abs/2508.07989", "authors": ["Xiantao Zhang"], "title": "The Escalator Problem: Identifying Implicit Motion Blindness in AI for Accessibility", "comment": "9 pages, 3 figures, 2 tables. Accepted at CV4A11y, ICCV 2025", "summary": "Multimodal Large Language Models (MLLMs) hold immense promise as assistive\ntechnologies for the blind and visually impaired (BVI) community. However, we\nidentify a critical failure mode that undermines their trustworthiness in\nreal-world applications. We introduce the Escalator Problem -- the inability of\nstate-of-the-art models to perceive an escalator's direction of travel -- as a\ncanonical example of a deeper limitation we term Implicit Motion Blindness.\nThis blindness stems from the dominant frame-sampling paradigm in video\nunderstanding, which, by treating videos as discrete sequences of static\nimages, fundamentally struggles to perceive continuous, low-signal motion. As a\nposition paper, our contribution is not a new model but rather to: (I) formally\narticulate this blind spot, (II) analyze its implications for user trust, and\n(III) issue a call to action. We advocate for a paradigm shift from purely\nsemantic recognition towards robust physical perception and urge the\ndevelopment of new, human-centered benchmarks that prioritize safety,\nreliability, and the genuine needs of users in dynamic environments.", "AI": {"tldr": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u611f\u77e5\u8fde\u7eed\u8fd0\u52a8\u65b9\u9762\u5b58\u5728\u95ee\u9898\uff08\u4f8b\u5982\u65e0\u6cd5\u8bc6\u522b\u81ea\u52a8\u6276\u68af\u7684\u884c\u9a76\u65b9\u5411\uff09\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u4f5c\u4e3a\u8f85\u52a9\u6280\u672f\u7684\u53ef\u9760\u6027\u3002\u6587\u7ae0\u547c\u5401\u884c\u4e1a\u5f00\u53d1\u66f4\u6ce8\u91cd\u7269\u7406\u611f\u77e5\u548c\u7528\u6237\u5b89\u5168\u7684\u65b0\u65b9\u6cd5\u548c\u57fa\u51c6\u3002", "motivation": "\u5f53\u524d\u7684\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLMs\uff09\u5728\u8f85\u52a9\u76f2\u4eba\u548c\u89c6\u969c\u4eba\u58eb\uff08BVI\uff09\u65b9\u9762\u5b58\u5728\u5173\u952e\u7684\u4e0d\u53ef\u4fe1\u95ee\u9898\uff0c\u5c24\u5176\u662f\u5728\u611f\u77e5\u8fd0\u52a8\u65b9\u9762\uff0c\u8fd9\u9650\u5236\u4e86\u5b83\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u4e2d\u7684\u6f5c\u529b\u3002", "method": "\u672c\u6587\u63d0\u51fa\u201c\u81ea\u52a8\u6276\u68af\u95ee\u9898\u201d\u4f5c\u4e3a\u201c\u9690\u6027\u8fd0\u52a8\u76f2\u533a\u201d\u7684\u5178\u578b\u4f8b\u5b50\uff0c\u5206\u6790\u4e86\u5f53\u524d\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e27\u91c7\u6837\u8303\u5f0f\uff08\u5c06\u89c6\u9891\u89c6\u4e3a\u79bb\u6563\u7684\u9759\u6001\u56fe\u50cf\u5e8f\u5217\uff09\u662f\u5bfc\u81f4\u8be5\u95ee\u9898\u7684\u539f\u56e0\uff0c\u5e76\u4e3b\u5f20\u9700\u8981\u4ece\u8bed\u4e49\u8bc6\u522b\u8f6c\u5411\u7269\u7406\u611f\u77e5\u3002", "result": "\u672c\u6587\u8bc6\u522b\u5e76\u5b9a\u4e49\u4e86MLLMs\u4e2d\u7684\u201c\u9690\u6027\u8fd0\u52a8\u76f2\u533a\u201d\uff0c\u5e76\u4ee5\u201c\u81ea\u52a8\u6276\u68af\u95ee\u9898\u201d\u4e3a\u4f8b\uff0c\u8bf4\u660e\u4e86\u5f53\u524d\u6a21\u578b\u5728\u611f\u77e5\u8fde\u7eed\u8fd0\u52a8\u65b9\u9762\u7684\u5c40\u9650\u6027\u3002", "conclusion": "MLLMs\u5728\u8f85\u52a9\u76f2\u4eba\u548c\u89c6\u969c\u4eba\u58eb\u65b9\u9762\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5b58\u5728\u201c\u81ea\u52a8\u6276\u68af\u95ee\u9898\u201d\u7b49\u5173\u952e\u6545\u969c\u6a21\u5f0f\uff0c\u8fd9\u6e90\u4e8e\u5f53\u524d\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u5e27\u91c7\u6837\u8303\u5f0f\uff0c\u8be5\u8303\u5f0f\u96be\u4ee5\u611f\u77e5\u8fde\u7eed\u3001\u4f4e\u4fe1\u53f7\u7684\u8fd0\u52a8\u3002\u672c\u6587\u65e8\u5728\u6b63\u5f0f\u9610\u8ff0\u8fd9\u4e00\u201c\u9690\u6027\u8fd0\u52a8\u76f2\u533a\u201d\uff0c\u5206\u6790\u5176\u5bf9\u7528\u6237\u4fe1\u4efb\u7684\u5f71\u54cd\uff0c\u5e76\u547c\u5401\u884c\u4e1a\u8303\u5f0f\u8f6c\u53d8\uff0c\u4ece\u7eaf\u7cb9\u7684\u8bed\u4e49\u8bc6\u522b\u8f6c\u5411\u9c81\u68d2\u7684\u7269\u7406\u611f\u77e5\uff0c\u5e76\u5f00\u53d1\u4ee5\u4eba\u4e3a\u4e2d\u5fc3\u7684\u3001\u4f18\u5148\u8003\u8651\u5b89\u5168\u6027\u548c\u53ef\u9760\u6027\u4ee5\u53ca\u7528\u6237\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u771f\u5b9e\u9700\u6c42\u7684\u65b0\u57fa\u51c6\u3002"}}
{"id": "2508.07996", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.07996", "abs": "https://arxiv.org/abs/2508.07996", "authors": ["Thinesh Thiyakesan Ponbagavathi", "Chengzheng Yang", "Alina Roitberg"], "title": "Prompt-Guided Relational Reasoning for Social Behavior Understanding with Vision Foundation Models", "comment": null, "summary": "Group Activity Detection (GAD) involves recognizing social groups and their\ncollective behaviors in videos. Vision Foundation Models (VFMs), like DinoV2,\noffer excellent features, but are pretrained primarily on object-centric data\nand remain underexplored for modeling group dynamics. While they are a\npromising alternative to highly task-specific GAD architectures that require\nfull fine-tuning, our initial investigation reveals that simply swapping CNN\nbackbones used in these methods with VFMs brings little gain, underscoring the\nneed for structured, group-aware reasoning on top.\n  We introduce Prompt-driven Group Activity Detection (ProGraD) -- a method\nthat bridges this gap through 1) learnable group prompts to guide the VFM\nattention toward social configurations, and 2) a lightweight two-layer\nGroupContext Transformer that infers actor-group associations and collective\nbehavior. We evaluate our approach on two recent GAD benchmarks: Cafe, which\nfeatures multiple concurrent social groups, and Social-CAD, which focuses on\nsingle-group interactions. While we surpass state-of-the-art in both settings,\nour method is especially effective in complex multi-group scenarios, where we\nyield a gain of 6.5\\% (Group mAP\\@1.0) and 8.2\\% (Group mAP\\@0.5) using only\n10M trainable parameters. Furthermore, our experiments reveal that ProGraD\nproduces interpretable attention maps, offering insights into actor-group\nreasoning. Code and models will be released.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ProGraD \u7684\u65b0\u65b9\u6cd5\uff0c\u7528\u4e8e Group Activity Detection (GAD)\uff0c\u8be5\u65b9\u6cd5\u5229\u7528\u89c6\u89c9\u57fa\u7840\u6a21\u578b (VFM) \u548c\u521b\u65b0\u7684\u63d0\u793a\u673a\u5236\u6765\u7406\u89e3\u7fa4\u4f53\u52a8\u6001\u3002ProGraD \u901a\u8fc7\u53ef\u5b66\u4e60\u7684\u7ec4\u63d0\u793a\u548c GroupContext Transformer \u6765\u63d0\u9ad8 GAD \u7684\u6027\u80fd\uff0c\u5c24\u5176\u662f\u5728\u5904\u7406\u590d\u6742\u7684\u591a\u5c0f\u7ec4\u573a\u666f\u65f6\u3002\u8be5\u65b9\u6cd5\u5728 Cafe \u548c Social-CAD \u6570\u636e\u96c6\u4e0a\u5747\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6210\u679c\uff0c\u5e76\u80fd\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u56fe\u3002", "motivation": "\u89c6\u89c9\u57fa\u7840\u6a21\u578b (VFM) \u5728\u52a8\u4f5c\u8bc6\u522b\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u4e3b\u8981\u9488\u5bf9\u4ee5\u7269\u4f53\u4e3a\u4e2d\u5fc3\u7684\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\uff0c\u5e76\u4e14\u5728\u5bf9\u7fa4\u4f53\u52a8\u6001\u5efa\u6a21\u65b9\u9762\u7684\u5e94\u7528\u4ecd\u6709\u5f85\u63a2\u7d22\u3002\u867d\u7136 VFM \u662f\u89e3\u51b3\u9700\u8981\u5b8c\u5168\u5fae\u8c03\u7684\u3001\u9ad8\u5ea6\u7279\u5b9a\u4e8e\u4efb\u52a1\u7684 GAD \u67b6\u6784\u7684\u6709\u5e0c\u671b\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4f46\u7b80\u5355\u7684 VFM \u66ff\u6362 VFM \u5e26\u6765\u7684\u6536\u76ca\u751a\u5fae\uff0c\u8fd9\u8868\u660e\u9700\u8981\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u7ed3\u6784\u5316\u3001\u9762\u5411\u7fa4\u4f53\u7684\u63a8\u7406\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Prompt \u9a71\u52a8\u7684 Group Activity Detection (ProGraD) \u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u901a\u8fc7\u4ee5\u4e0b\u4e24\u4e2a\u90e8\u5206\u6765\u5f25\u5408\u8fd9\u4e00\u5dee\u8ddd\uff1a1) \u53ef\u5b66\u4e60\u7684\u7ec4\u63d0\u793a\uff0c\u7528\u4e8e\u5f15\u5bfc\u89c6\u89c9\u57fa\u7840\u6a21\u578b (VFM) \u7684\u6ce8\u610f\u529b\u5173\u6ce8\u4e8e\u793e\u4f1a\u914d\u7f6e\uff1b2) \u4e00\u4e2a\u8f7b\u91cf\u7ea7\u7684\u4e24\u5c42 GroupContext Transformer\uff0c\u7528\u4e8e\u63a8\u65ad\u6f14\u5458-\u7fa4\u4f53\u5173\u8054\u548c\u96c6\u4f53\u884c\u4e3a\u3002", "result": "\u5728 Cafe \u548c Social-CAD \u4e24\u4e2a GAD \u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5747\u8d85\u8d8a\u4e86\u6700\u5148\u8fdb\u7684\u6280\u672f\u3002\u5728 Cafe \u6570\u636e\u96c6\u4e0a\uff0c\u8be5\u65b9\u6cd5\u5728\u591a\u5c0f\u7ec4\u573a\u666f\u4e0b\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0cGroup mAP@1.0 \u63d0\u5347 6.5%\uff0cGroup mAP@0.5 \u63d0\u5347 8.2%\uff0c\u4ec5\u4f7f\u7528\u4e86 1000 \u4e07\u53ef\u8bad\u7ec3\u53c2\u6570\u3002", "conclusion": "\u5728\u590d\u6742\u7684\u3001\u591a\u5c0f\u7ec4\u7684\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u5c24\u5176\u6709\u6548\uff0c\u53ef\u5e26\u6765\u663e\u8457\u7684\u6027\u80fd\u63d0\u5347\uff08Group mAP@1.0 \u63d0\u5347 6.5%\uff0cGroup mAP@0.5 \u63d0\u5347 8.2%\uff09\uff0c\u4e14\u4ec5\u4f7f\u7528 1000 \u4e07\u53ef\u8bad\u7ec3\u53c2\u6570\u3002\u6b64\u5916\uff0c\u5b9e\u9a8c\u8868\u660e ProGraD \u80fd\u591f\u751f\u6210\u53ef\u89e3\u91ca\u7684\u6ce8\u610f\u529b\u56fe\uff0c\u4e3a\u7406\u89e3\u6f14\u5458-\u7fa4\u4f53\u63a8\u7406\u63d0\u4f9b\u89c1\u89e3\u3002"}}
{"id": "2508.08004", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08004", "abs": "https://arxiv.org/abs/2508.08004", "authors": ["Anqi Xiao", "Weichen Yu", "Hongyuan Yu"], "title": "Sample-aware RandAugment: Search-free Automatic Data Augmentation for Effective Image Recognition", "comment": "International Journal of Computer Vision, 2025", "summary": "Automatic data augmentation (AutoDA) plays an important role in enhancing the\ngeneralization of neural networks. However, mainstream AutoDA methods often\nencounter two challenges: either the search process is excessively\ntime-consuming, hindering practical application, or the performance is\nsuboptimal due to insufficient policy adaptation during training. To address\nthese issues, we propose Sample-aware RandAugment (SRA), an asymmetric,\nsearch-free AutoDA method that dynamically adjusts augmentation policies while\nmaintaining straightforward implementation. SRA incorporates a heuristic\nscoring module that evaluates the complexity of the original training data,\nenabling the application of tailored augmentations for each sample.\nAdditionally, an asymmetric augmentation strategy is employed to maximize the\npotential of this scoring module. In multiple experimental settings, SRA\nnarrows the performance gap between search-based and search-free AutoDA\nmethods, achieving a state-of-the-art Top-1 accuracy of 78.31\\% on ImageNet\nwith ResNet-50. Notably, SRA demonstrates good compatibility with existing\naugmentation pipelines and solid generalization across new tasks, without\nrequiring hyperparameter tuning. The pretrained models leveraging SRA also\nenhance recognition in downstream object detection tasks. SRA represents a\npromising step towards simpler, more effective, and practical AutoDA designs\napplicable to a variety of future tasks. Our code is available at\n\\href{https://github.com/ainieli/Sample-awareRandAugment}{https://github.com/ainieli/Sample-awareRandAugment", "AI": {"tldr": "SRA \u662f\u4e00\u79cd\u65b0\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5b83\u901a\u8fc7\u52a8\u6001\u8c03\u6574\u589e\u5f3a\u7b56\u7565\u6765\u63d0\u9ad8\u6027\u80fd\uff0c\u540c\u65f6\u4fdd\u6301\u7b80\u5355\u548c\u9ad8\u6548\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u4e3b\u6d41\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u641c\u7d22\u8fc7\u7a0b\u8017\u65f6\u8fc7\u957f\u6216\u7b56\u7565\u9002\u5e94\u4e0d\u8db3\u5bfc\u81f4\u6027\u80fd\u4e0d\u4f73\u7684\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86 SRA\u3002", "method": "SRA \u91c7\u7528\u4e86\u4e00\u79cd\u4e0d\u5bf9\u79f0\u7684\u3001\u65e0\u641c\u7d22\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u5728\u4fdd\u6301\u7b80\u5355\u5b9e\u73b0\u7684\u540c\u65f6\u52a8\u6001\u8c03\u6574\u589e\u5f3a\u7b56\u7565\u3002SRA \u5305\u62ec\u4e00\u4e2a\u542f\u53d1\u5f0f\u8bc4\u5206\u6a21\u5757\uff0c\u7528\u4e8e\u8bc4\u4f30\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u7684\u590d\u6742\u6027\uff0c\u4ece\u800c\u4e3a\u6bcf\u4e2a\u6837\u672c\u5e94\u7528\u91cf\u8eab\u5b9a\u5236\u7684\u589e\u5f3a\u3002\u6b64\u5916\uff0c\u8fd8\u91c7\u7528\u4e0d\u5bf9\u79f0\u589e\u5f3a\u7b56\u7565\u6765\u6700\u5927\u5316\u6b64\u8bc4\u5206\u6a21\u5757\u7684\u6f5c\u529b\u3002", "result": "SRA \u7f29\u5c0f\u4e86\u57fa\u4e8e\u641c\u7d22\u548c\u65e0\u641c\u7d22\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\u4e4b\u95f4\u7684\u6027\u80fd\u5dee\u8ddd\uff0c\u5728 ImageNet \u4e0a\u4f7f\u7528 ResNet-50 \u8fbe\u5230\u4e86 78.31% \u7684\u51c6\u786e\u7387\u3002SRA \u4e0e\u73b0\u6709\u7684\u589e\u5f3a\u6d41\u7a0b\u517c\u5bb9\uff0c\u5e76\u4e14\u5728\u65b0\u4efb\u52a1\u4e0a\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u6027\uff0c\u65e0\u9700\u8c03\u6574\u8d85\u53c2\u6570\u3002\u4f7f\u7528 SRA \u9884\u8bad\u7ec3\u7684\u6a21\u578b\u8fd8\u589e\u5f3a\u4e86\u4e0b\u6e38\u5bf9\u8c61\u68c0\u6d4b\u4efb\u52a1\u4e2d\u7684\u8bc6\u522b\u80fd\u529b\u3002", "conclusion": "SRA \u662f\u4e00\u79cd\u6709\u524d\u666f\u7684\u81ea\u52a8\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\uff0c\u5b83\u66f4\u7b80\u5355\u3001\u66f4\u6709\u6548\u3001\u66f4\u5b9e\u7528\uff0c\u5e76\u9002\u7528\u4e8e\u5404\u79cd\u672a\u6765\u7684\u4efb\u52a1\u3002"}}
{"id": "2508.08028", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08028", "abs": "https://arxiv.org/abs/2508.08028", "authors": ["Tony Danjun Wang", "Tobias Czempiel", "Nassir Navab", "Lennart Bastian"], "title": "Mitigating Biases in Surgical Operating Rooms with Geometry", "comment": "Extended Abstract, presented at the MICCAI'25 workshop on\n  Collaborative Intelligence and Autonomy in Image-guided Surgery", "summary": "Deep neural networks are prone to learning spurious correlations, exploiting\ndataset-specific artifacts rather than meaningful features for prediction. In\nsurgical operating rooms (OR), these manifest through the standardization of\nsmocks and gowns that obscure robust identifying landmarks, introducing model\nbias for tasks related to modeling OR personnel. Through gradient-based\nsaliency analysis on two public OR datasets, we reveal that CNN models succumb\nto such shortcuts, fixating on incidental visual cues such as footwear beneath\nsurgical gowns, distinctive eyewear, or other role-specific identifiers.\nAvoiding such biases is essential for the next generation of intelligent\nassistance systems in the OR, which should accurately recognize personalized\nworkflow traits, such as surgical skill level or coordination with other staff\nmembers. We address this problem by encoding personnel as 3D point cloud\nsequences, disentangling identity-relevant shape and motion patterns from\nappearance-based confounders. Our experiments demonstrate that while RGB and\ngeometric methods achieve comparable performance on datasets with apparent\nsimulation artifacts, RGB models suffer a 12% accuracy drop in realistic\nclinical settings with decreased visual diversity due to standardizations. This\nperformance gap confirms that geometric representations capture more meaningful\nbiometric features, providing an avenue to developing robust methods of\nmodeling humans in the OR.", "AI": {"tldr": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5728 OR \u4eba\u5458\u8bc6\u522b\u4e2d\u5b58\u5728\u504f\u5dee\uff0c\u5bb9\u6613\u53d7\u5230\u6807\u51c6\u5316\u670d\u88c5\u7684\u5f71\u54cd\u3002\u672c\u7814\u7a76\u63d0\u51fa\u4f7f\u7528 3D \u70b9\u4e91\u5e8f\u5217\u6765\u7f16\u7801\u4eba\u5458\uff0c\u4ee5\u533a\u5206\u8eab\u4efd\u76f8\u5173\u7684\u7279\u5f81\u548c\u5916\u89c2\u4f2a\u5f71\u3002\u5b9e\u9a8c\u8bc1\u660e\uff0c\u51e0\u4f55\u8868\u793a\u6bd4 RGB \u8868\u793a\u66f4\u80fd\u6355\u6349\u6709\u610f\u4e49\u7684\u751f\u7269\u8bc6\u522b\u7279\u5f81\uff0c\u5728\u771f\u5b9e\u4e34\u5e8a\u73af\u5883\u4e2d\u8868\u73b0\u66f4\u4f18\u3002", "motivation": "\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5bb9\u6613\u5b66\u4e60\u5230\u865a\u5047\u76f8\u5173\u6027\uff0c\u5e76\u5229\u7528\u7279\u5b9a\u4e8e\u6570\u636e\u96c6\u7684\u4f2a\u5f71\uff0c\u800c\u4e0d\u662f\u6709\u610f\u4e49\u7684\u7279\u5f81\u8fdb\u884c\u9884\u6d4b\u3002\u5728 OR \u4e2d\uff0c\u624b\u672f\u670d\u548c\u7f69\u886b\u7684\u6807\u51c6\u5316\u63a9\u76d6\u4e86\u9c81\u68d2\u7684\u8bc6\u522b\u6807\u5fd7\uff0c\u5bfc\u81f4 OR \u4eba\u5458\u5efa\u6a21\u4efb\u52a1\u51fa\u73b0\u6a21\u578b\u504f\u5dee\u3002", "method": "\u672c\u7814\u7a76\u901a\u8fc7\u5c06\u4eba\u5458\u7f16\u7801\u4e3a 3D \u70b9\u4e91\u5e8f\u5217\uff0c\u5c06\u4e0e\u8eab\u4efd\u76f8\u5173\u7684\u5f62\u72b6\u548c\u8fd0\u52a8\u6a21\u5f0f\u4e0e\u57fa\u4e8e\u5916\u89c2\u7684\u6df7\u6dc6\u56e0\u7d20\u5206\u79bb\u5f00\u6765\uff0c\u6765\u89e3\u51b3\u504f\u5dee\u95ee\u9898\u3002", "result": "\u51e0\u4f55\u8868\u793a\u76f8\u6bd4 RGB \u8868\u793a\u66f4\u5177\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u771f\u5b9e\u7684\u4e34\u5e8a\u73af\u5883\u4e2d\u3002", "conclusion": "\u51e0\u4f55\u8868\u793a\u80fd\u591f\u6355\u6349\u5230\u66f4\u6709\u610f\u4e49\u7684\u751f\u7269\u8bc6\u522b\u7279\u5f81\uff0c\u4e3a\u5f00\u53d1\u6709\u6548\u7684 OR \u4eba\u5458\u5efa\u6a21\u65b9\u6cd5\u63d0\u4f9b\u4e86\u9014\u5f84\u3002RGB \u6a21\u578b\u5728\u4e34\u5e8a\u73af\u5883\u4e2d\u51c6\u786e\u7387\u4e0b\u964d 12%\uff0c\u800c\u51e0\u4f55\u65b9\u6cd5\u5728\u5b58\u5728\u6a21\u62df\u4f2a\u5f71\u7684\u6570\u636e\u96c6\u4e0a\u8868\u73b0\u76f8\u5f53\u3002"}}
{"id": "2508.08038", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08038", "abs": "https://arxiv.org/abs/2508.08038", "authors": ["Huawei Sun", "Zixu Wang", "Hao Feng", "Julius Ott", "Lorenzo Servadei", "Robert Wille"], "title": "TRIDE: A Text-assisted Radar-Image weather-aware fusion network for Depth Estimation", "comment": "Accepted by TMLR (2025.08)", "summary": "Depth estimation, essential for autonomous driving, seeks to interpret the 3D\nenvironment surrounding vehicles. The development of radar sensors, known for\ntheir cost-efficiency and robustness, has spurred interest in radar-camera\nfusion-based solutions. However, existing algorithms fuse features from these\nmodalities without accounting for weather conditions, despite radars being\nknown to be more robust than cameras under adverse weather. Additionally, while\nVision-Language models have seen rapid advancement, utilizing language\ndescriptions alongside other modalities for depth estimation remains an open\nchallenge. This paper first introduces a text-generation strategy along with\nfeature extraction and fusion techniques that can assist monocular depth\nestimation pipelines, leading to improved accuracy across different algorithms\non the KITTI dataset. Building on this, we propose TRIDE, a radar-camera fusion\nalgorithm that enhances text feature extraction by incorporating radar point\ninformation. To address the impact of weather on sensor performance, we\nintroduce a weather-aware fusion block that adaptively adjusts radar weighting\nbased on current weather conditions. Our method, benchmarked on the nuScenes\ndataset, demonstrates performance gains over the state-of-the-art, achieving a\n12.87% improvement in MAE and a 9.08% improvement in RMSE. Code:\nhttps://github.com/harborsarah/TRIDE", "AI": {"tldr": "TRIDE \u662f\u4e00\u79cd\u7528\u4e8e\u81ea\u52a8\u9a7e\u9a76\u7684\u96f7\u8fbe-\u6444\u50cf\u5934\u878d\u5408\u6df1\u5ea6\u4f30\u8ba1\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u6587\u672c\u4fe1\u606f\u548c\u5929\u6c14\u611f\u77e5\u6280\u672f\u6765\u63d0\u9ad8\u7cbe\u5ea6\uff0c\u5c24\u5176\u662f\u5728\u6076\u52a3\u5929\u6c14\u4e0b\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u81ea\u52a8\u9a7e\u9a76\u4e2d\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u662f\u5728\u6076\u52a3\u5929\u6c14\u6761\u4ef6\u4e0b\uff0c\u5e76\u63a2\u7d22\u5229\u7528\u89c6\u89c9-\u8bed\u8a00\u6a21\u578b\u8fdb\u884c\u6df1\u5ea6\u4f30\u8ba1\u3002", "method": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u6587\u672c\u751f\u6210\u7b56\u7565\u3001\u7279\u5f81\u63d0\u53d6\u548c\u878d\u5408\u6280\u672f\u7684\u65b9\u6cd5\uff0c\u4ee5\u8f85\u52a9\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0cTRIDE \u7b97\u6cd5\u901a\u8fc7\u6574\u5408\u96f7\u8fbe\u70b9\u4fe1\u606f\u589e\u5f3a\u6587\u672c\u7279\u5f81\u63d0\u53d6\uff0c\u5e76\u63d0\u51fa\u5929\u6c14\u611f\u77e5\u878d\u5408\u6a21\u5757\uff0c\u6839\u636e\u5929\u6c14\u6761\u4ef6\u81ea\u9002\u5e94\u8c03\u6574\u96f7\u8fbe\u6743\u91cd\u3002", "result": "\u5728 KITTI \u6570\u636e\u96c6\u4e0a\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u4e0d\u540c\u7b97\u6cd5\u4e0a\u5747\u63d0\u9ad8\u4e86\u51c6\u786e\u6027\u3002\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\uff0cTRIDE \u7b97\u6cd5\u76f8\u6bd4\u73b0\u6709\u6280\u672f\u5728 MAE\uff08\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff09\u4e0a\u63d0\u9ad8\u4e86 12.87%\uff0c\u5728 RMSE\uff08\u5747\u65b9\u6839\u8bef\u5dee\uff09\u4e0a\u63d0\u9ad8\u4e86 9.08%\u3002", "conclusion": "TRIDE \u7b97\u6cd5\u901a\u8fc7\u878d\u5408\u96f7\u8fbe\u548c\u6444\u50cf\u5934\u6570\u636e\uff0c\u5e76\u5f15\u5165\u4e86\u5929\u6c14\u611f\u77e5\u878d\u5408\u6a21\u5757\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u7684\u51c6\u786e\u6027\uff0c\u5728 nuScenes \u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u6280\u672f\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08048", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08048", "abs": "https://arxiv.org/abs/2508.08048", "authors": ["Peng Dai", "Feitong Tan", "Qiangeng Xu", "Yihua Huang", "David Futschik", "Ruofei Du", "Sean Fanello", "Yinda Zhang", "Xiaojuan Qi"], "title": "S^2VG: 3D Stereoscopic and Spatial Video Generation via Denoising Frame Matrix", "comment": "immsersive video generation", "summary": "While video generation models excel at producing high-quality monocular\nvideos, generating 3D stereoscopic and spatial videos for immersive\napplications remains an underexplored challenge. We present a pose-free and\ntraining-free method that leverages an off-the-shelf monocular video generation\nmodel to produce immersive 3D videos. Our approach first warps the generated\nmonocular video into pre-defined camera viewpoints using estimated depth\ninformation, then applies a novel \\textit{frame matrix} inpainting framework.\nThis framework utilizes the original video generation model to synthesize\nmissing content across different viewpoints and timestamps, ensuring spatial\nand temporal consistency without requiring additional model fine-tuning.\nMoreover, we develop a \\dualupdate~scheme that further improves the quality of\nvideo inpainting by alleviating the negative effects propagated from\ndisoccluded areas in the latent space. The resulting multi-view videos are then\nadapted into stereoscopic pairs or optimized into 4D Gaussians for spatial\nvideo synthesis. We validate the efficacy of our proposed method by conducting\nexperiments on videos from various generative models, such as Sora, Lumiere,\nWALT, and Zeroscope. The experiments demonstrate that our method has a\nsignificant improvement over previous methods. Project page at:\nhttps://daipengwa.github.io/S-2VG_ProjectPage/", "AI": {"tldr": "\u4e00\u79cd\u65e0\u9700\u59ff\u6001\u4f30\u8ba1\u548c\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u53ef\u4ee5\u5229\u7528\u5355\u89c6\u56fe\u89c6\u9891\u751f\u6210\u6a21\u578b\u6765\u751f\u62103D\u7acb\u4f53\u548c\u7a7a\u95f4\u89c6\u9891\u3002", "motivation": "\u4e3a\u63a2\u7d22\u751f\u62103D\u7acb\u4f53\u548c\u7a7a\u95f4\u89c6\u9891\u4ee5\u6ee1\u8db3\u6c89\u6d78\u5f0f\u5e94\u7528\u9700\u6c42\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65e0\u9700\u59ff\u6001\u4f30\u8ba1\u548c\u8bad\u7ec3\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u9996\u5148\u5229\u7528\u4f30\u8ba1\u7684\u6df1\u5ea6\u4fe1\u606f\u5c06\u751f\u6210\u7684\u5355\u89c6\u56fe\u89c6\u9891\u626d\u66f2\u5230\u9884\u5b9a\u4e49\u7684\u6444\u50cf\u673a\u89c6\u70b9\uff0c\u7136\u540e\u5e94\u7528\u65b0\u9896\u7684\u5e27\u77e9\u9635\u4fee\u590d\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5229\u7528\u539f\u59cb\u89c6\u9891\u751f\u6210\u6a21\u578b\u6765\u5408\u6210\u4e0d\u540c\u89c6\u70b9\u548c\u65f6\u95f4\u6233\u4e4b\u95f4\u7684\u7f3a\u5931\u5185\u5bb9\uff0c\u5e76\u91c7\u7528dualupdate\u65b9\u6848\u6765\u63d0\u9ad8\u89c6\u9891\u4fee\u590d\u8d28\u91cf\u3002", "result": "\u751f\u6210\u76843D\u89c6\u9891\u53ef\u4ee5\u5728\u7acb\u4f53\u5bf9\u62164D\u9ad8\u65af\u4e2d\u8fdb\u884c\u9002\u914d\uff0c\u4ee5\u8fdb\u884c\u7a7a\u95f4\u89c6\u9891\u5408\u6210\uff0c\u5e76\u4e14\u5728Sora\u3001Lumiere\u3001WALT\u548cZeroscope\u7b49\u751f\u6210\u6a21\u578b\u4e0a\u8fdb\u884c\u4e86\u9a8c\u8bc1\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u6709\u663e\u8457\u63d0\u5347\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u751f\u6210\u76843D\u89c6\u9891\u5728\u7a7a\u95f4\u548c\u65f6\u95f4\u4e0a\u5177\u6709\u4e00\u81f4\u6027\uff0c\u5e76\u4e14\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
{"id": "2508.08058", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08058", "abs": "https://arxiv.org/abs/2508.08058", "authors": ["Ziad Al-Haj Hemidi", "Eytan Kats", "Mattias P. Heinrich"], "title": "PrIINeR: Towards Prior-Informed Implicit Neural Representations for Accelerated MRI", "comment": "Submitted to the British Machine Vision Conference (BMVC) 2025\n  (Before peer review version)", "summary": "Accelerating Magnetic Resonance Imaging (MRI) reduces scan time but often\ndegrades image quality. While Implicit Neural Representations (INRs) show\npromise for MRI reconstruction, they struggle at high acceleration factors due\nto weak prior constraints, leading to structural loss and aliasing artefacts.\nTo address this, we propose PrIINeR, an INR-based MRI reconstruction method\nthat integrates prior knowledge from pre-trained deep learning models into the\nINR framework. By combining population-level knowledge with instance-based\noptimization and enforcing dual data consistency, PrIINeR aligns both with the\nacquired k-space data and the prior-informed reconstruction. Evaluated on the\nNYU fastMRI dataset, our method not only outperforms state-of-the-art INR-based\napproaches but also improves upon several learning-based state-of-the-art\nmethods, significantly improving structural preservation and fidelity while\neffectively removing aliasing artefacts.PrIINeR bridges deep learning and\nINR-based techniques, offering a more reliable solution for high-quality,\naccelerated MRI reconstruction. The code is publicly available on\nhttps://github.com/multimodallearning/PrIINeR.", "AI": {"tldr": "PrIINeR \u662f\u4e00\u79cd\u65b0\u7684MRI\u91cd\u5efa\u65b9\u6cd5\uff0c\u5b83\u5229\u7528\u9884\u8bad\u7ec3\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u6765\u63d0\u9ad8\u52a0\u901fMRI\u56fe\u50cf\u7684\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u9ad8\u901f\u7387\u52a0\u901f\u65f6\uff0c\u80fd\u591f\u6709\u6548\u51cf\u5c11\u4f2a\u5f71\u5e76\u4fdd\u6301\u7ed3\u6784\u7ec6\u8282\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u5728\u9ad8\u901f\u7387\u52a0\u901f\u4e0b\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u5728MRI\u91cd\u5efa\u4e2d\u5b58\u5728\u7684\u7ed3\u6784\u4e22\u5931\u548c\u6df7\u53e0\u4f2a\u5f71\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86PrIINeR\u3002", "method": "PrIINeR \u901a\u8fc7\u6574\u5408\u9884\u8bad\u7ec3\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u7684\u5148\u9a8c\u77e5\u8bc6\u5230INR\u6846\u67b6\u4e2d\uff0c\u5e76\u5f3a\u5236\u6267\u884c\u53cc\u91cd\u6570\u636e\u4e00\u81f4\u6027\uff0c\u4ece\u800c\u540c\u65f6\u4e0e\u91c7\u96c6\u7684k\u7a7a\u95f4\u6570\u636e\u548c\u5148\u9a8c\u9a71\u52a8\u7684\u91cd\u5efa\u4fdd\u6301\u4e00\u81f4\u3002", "result": "\u5728NYU fastMRI\u6570\u636e\u96c6\u4e0a\u7684\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0cPrIINeR \u4e0d\u4ec5\u4f18\u4e8e\u76ee\u524d\u6700\u5148\u8fdb\u7684\u57fa\u4e8eINR\u7684\u65b9\u6cd5\uff0c\u800c\u4e14\u5728\u7ed3\u6784\u4fdd\u6301\u548c\u4fdd\u771f\u5ea6\u65b9\u9762\u4e5f\u4f18\u4e8e\u4e00\u4e9b\u6700\u5148\u8fdb\u7684\u5b66\u4e60\u65b9\u6cd5\uff0c\u540c\u65f6\u6709\u6548\u6d88\u9664\u4e86\u6df7\u53e0\u4f2a\u5f71\u3002", "conclusion": "PrIINeR \u662f\u4e00\u79cd\u7ed3\u5408\u4e86\u6df1\u5ea6\u5b66\u4e60\u548c\u9690\u5f0f\u795e\u7ecf\u8868\u793a\uff08INR\uff09\u7684MRI\u91cd\u5efa\u65b9\u6cd5\uff0c\u80fd\u591f\u4e3a\u52a0\u901fMRI\u63d0\u4f9b\u66f4\u53ef\u9760\u7684\u9ad8\u8d28\u91cf\u91cd\u5efa\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2508.08069", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08069", "abs": "https://arxiv.org/abs/2508.08069", "authors": ["Xiaoxiao Cui", "Yiran Li", "Kai He", "Shanzhi Jiang", "Mengli Xue", "Wentao Li", "Junhong Leng", "Zhi Liu", "Lizhen Cui", "Shuo Li"], "title": "Information Bottleneck-based Causal Attention for Multi-label Medical Image Recognition", "comment": "Early accepted by MICCAI 2025", "summary": "Multi-label classification (MLC) of medical images aims to identify multiple\ndiseases and holds significant clinical potential. A critical step is to learn\nclass-specific features for accurate diagnosis and improved interpretability\neffectively. However, current works focus primarily on causal attention to\nlearn class-specific features, yet they struggle to interpret the true cause\ndue to the inadvertent attention to class-irrelevant features. To address this\nchallenge, we propose a new structural causal model (SCM) that treats\nclass-specific attention as a mixture of causal, spurious, and noisy factors,\nand a novel Information Bottleneck-based Causal Attention (IBCA) that is\ncapable of learning the discriminative class-specific attention for MLC of\nmedical images. Specifically, we propose learning Gaussian mixture multi-label\nspatial attention to filter out class-irrelevant information and capture each\nclass-specific attention pattern. Then a contrastive enhancement-based causal\nintervention is proposed to gradually mitigate the spurious attention and\nreduce noise information by aligning multi-head attention with the Gaussian\nmixture multi-label spatial. Quantitative and ablation results on Endo and\nMuReD show that IBCA outperforms all methods. Compared to the second-best\nresults for each metric, IBCA achieves improvements of 6.35\\% in CR, 7.72\\% in\nOR, and 5.02\\% in mAP for MuReD, 1.47\\% in CR, and 1.65\\% in CF1, and 1.42\\% in\nmAP for Endo.", "AI": {"tldr": "This paper introduces IBCA, a new method for multi-label classification of medical images that effectively learns class-specific attention by addressing irrelevant features, leading to improved diagnostic accuracy and interpretability. IBCA significantly outperforms existing methods on benchmark datasets.", "motivation": "Current methods for multi-label classification of medical images struggle to interpret the true cause of disease identification due to inadvertent attention to class-irrelevant features. This paper addresses this challenge by proposing a new structural causal model (SCM) and an IBCA method to learn discriminative class-specific attention.", "method": "A novel Information Bottleneck-based Causal Attention (IBCA) is proposed, which includes learning Gaussian mixture multi-label spatial attention to filter irrelevant information and capture class-specific attention patterns, and a contrastive enhancement-based causal intervention to mitigate spurious attention and reduce noise by aligning multi-head attention with the Gaussian mixture multi-label spatial attention.", "result": "IBCA achieved improvements of 6.35% in CR, 7.72% in OR, and 5.02% in mAP for MuReD, and 1.47% in CR, 1.65% in CF1, and 1.42% in mAP for Endo, outperforming all other methods.", "conclusion": "Proposed IBCA outperforms existing methods in multi-label classification of medical images, achieving significant improvements in CR, OR, and mAP on the MuReD dataset, and CR, CF1, and mAP on the Endo dataset."}}
{"id": "2508.08082", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08082", "abs": "https://arxiv.org/abs/2508.08082", "authors": ["Zizheng Guo", "Bochao Zou", "Junbao Zhuo", "Huimin Ma"], "title": "ME-TST+: Micro-expression Analysis via Temporal State Transition with ROI Relationship Awareness", "comment": null, "summary": "Micro-expressions (MEs) are regarded as important indicators of an\nindividual's intrinsic emotions, preferences, and tendencies. ME analysis\nrequires spotting of ME intervals within long video sequences and recognition\nof their corresponding emotional categories. Previous deep learning approaches\ncommonly employ sliding-window classification networks. However, the use of\nfixed window lengths and hard classification presents notable limitations in\npractice. Furthermore, these methods typically treat ME spotting and\nrecognition as two separate tasks, overlooking the essential relationship\nbetween them. To address these challenges, this paper proposes two state space\nmodel-based architectures, namely ME-TST and ME-TST+, which utilize temporal\nstate transition mechanisms to replace conventional window-level classification\nwith video-level regression. This enables a more precise characterization of\nthe temporal dynamics of MEs and supports the modeling of MEs with varying\ndurations. In ME-TST+, we further introduce multi-granularity ROI modeling and\nthe slowfast Mamba framework to alleviate information loss associated with\ntreating ME analysis as a time-series task. Additionally, we propose a synergy\nstrategy for spotting and recognition at both the feature and result levels,\nleveraging their intrinsic connection to enhance overall analysis performance.\nExtensive experiments demonstrate that the proposed methods achieve\nstate-of-the-art performance. The codes are available at\nhttps://github.com/zizheng-guo/ME-TST.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51faME-TST\u548cME-TST+\u4e24\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u89c6\u9891\u7ea7\u56de\u5f52\u548c\u534f\u540c\u7b56\u7565\u6539\u8fdb\u4e86\u5fae\u8868\u60c5\u7684\u5b9a\u4f4d\u548c\u8bc6\u522b\uff0c\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u5728\u5fae\u8868\u60c5\u5206\u6790\u4e2d\u901a\u5e38\u91c7\u7528\u56fa\u5b9a\u7a97\u53e3\u957f\u5ea6\u548c\u786c\u5206\u7c7b\u7684\u6ed1\u52a8\u7a97\u53e3\u5206\u7c7b\u7f51\u7edc\uff0c\u8fd9\u5728\u5b9e\u8df5\u4e2d\u5b58\u5728\u5c40\u9650\u6027\u3002\u6b64\u5916\uff0c\u8fd9\u4e9b\u65b9\u6cd5\u5c06\u5fae\u8868\u60c5\u7684\u5b9a\u4f4d\u548c\u8bc6\u522b\u89c6\u4e3a\u4e24\u4e2a\u72ec\u7acb\u4efb\u52a1\uff0c\u5ffd\u7565\u4e86\u5b83\u4eec\u4e4b\u95f4\u7684\u5185\u5728\u8054\u7cfb\u3002\u672c\u7814\u7a76\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "\u672c\u7814\u7a76\u63d0\u51fa\u4e24\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08SSM\uff09\u7684\u67b6\u6784\uff0cME-TST\u548cME-TST+\u3002ME-TST\u5229\u7528\u65f6\u6001\u72b6\u6001\u8f6c\u6362\u673a\u5236\uff0c\u5c06\u4f20\u7edf\u7684\u6ed1\u52a8\u7a97\u53e3\u5206\u7c7b\u65b9\u6cd5\u66ff\u6362\u4e3a\u89c6\u9891\u7ea7\u56de\u5f52\uff0c\u4ee5\u66f4\u7cbe\u786e\u5730\u6355\u6349\u5fae\u8868\u60c5\u7684\u65f6\u95f4\u52a8\u6001\u5e76\u5904\u7406\u4e0d\u540c\u6301\u7eed\u65f6\u95f4\u7684\u5fae\u8868\u60c5\u3002ME-TST+\u5728\u6b64\u57fa\u7840\u4e0a\u5f15\u5165\u4e86\u591a\u7c92\u5ea6\u533a\u57df\uff08ROI\uff09\u5efa\u6a21\u548c\u6162\u5feb\u9a6c\u6797\u5df4\uff08slowfast Mamba\uff09\u6846\u67b6\uff0c\u4ee5\u51cf\u5c11\u4fe1\u606f\u4e22\u5931\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7279\u5f81\u548c\u7ed3\u679c\u5c42\u9762\u8fdb\u884c\u5fae\u8868\u60c5\u8bc6\u522b\u548c\u5b9a\u4f4d\u7684\u534f\u540c\u7b56\u7565\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684ME-TST\u548cME-TST+\u65b9\u6cd5\u5728\u5fae\u8868\u60c5\u5206\u6790\u4efb\u52a1\u4e0a\u8fbe\u5230\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684ME-TST\u548cME-TST+\u65b9\u6cd5\u901a\u8fc7\u5229\u7528\u65f6\u6001\u72b6\u6001\u8f6c\u6362\u673a\u5236\uff0c\u7528\u89c6\u9891\u7ea7\u56de\u5f52\u53d6\u4ee3\u4f20\u7edf\u7684\u7a97\u53e3\u7ea7\u5206\u7c7b\uff0c\u80fd\u591f\u66f4\u7cbe\u786e\u5730\u8868\u5f81\u5fae\u8868\u60c5\u7684\u65f6\u95f4\u52a8\u6001\uff0c\u5e76\u652f\u6301\u5bf9\u4e0d\u540c\u6301\u7eed\u65f6\u95f4\u7684\u5fae\u8868\u60c5\u8fdb\u884c\u5efa\u6a21\u3002\u901a\u8fc7\u5f15\u5165\u591a\u7c92\u5ea6\u533a\u57df\u5efa\u6a21\u548c\u6162\u5feb\u9a6c\u6797\u5df4\u6846\u67b6\uff0cME-TST+\u8fdb\u4e00\u6b65\u7f13\u89e3\u4e86\u5728\u5c06\u5fae\u8868\u60c5\u5206\u6790\u89c6\u4e3a\u65f6\u95f4\u5e8f\u5217\u4efb\u52a1\u65f6\u53ef\u80fd\u51fa\u73b0\u7684\u4fe1\u606f\u4e22\u5931\u95ee\u9898\u3002\u6b64\u5916\uff0c\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u5728\u7279\u5f81\u548c\u7ed3\u679c\u5c42\u9762\u4e0a\u8fdb\u884c\u8bc6\u522b\u548c\u8bc6\u522b\u7684\u534f\u540c\u7b56\u7565\uff0c\u4ee5\u5229\u7528\u5b83\u4eec\u56fa\u6709\u7684\u8054\u7cfb\u6765\u63d0\u5347\u6574\u4f53\u5206\u6790\u6027\u80fd\u3002\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08093", "categories": ["cs.CV", "cs.LG", "cs.MM", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08093", "abs": "https://arxiv.org/abs/2508.08093", "authors": ["Md Rezwanul Haque", "Md. Milon Islam", "S M Taslim Uddin Raju", "Hamdi Altaheri", "Lobna Nassar", "Fakhri Karray"], "title": "MDD-Net: Multimodal Depression Detection through Mutual Transformer", "comment": "Accepted for the 2025 IEEE International Conference on Systems, Man,\n  and Cybernetics (SMC), Vienna, Austria", "summary": "Depression is a major mental health condition that severely impacts the\nemotional and physical well-being of individuals. The simple nature of data\ncollection from social media platforms has attracted significant interest in\nproperly utilizing this information for mental health research. A Multimodal\nDepression Detection Network (MDD-Net), utilizing acoustic and visual data\nobtained from social media networks, is proposed in this work where mutual\ntransformers are exploited to efficiently extract and fuse multimodal features\nfor efficient depression detection. The MDD-Net consists of four core modules:\nan acoustic feature extraction module for retrieving relevant acoustic\nattributes, a visual feature extraction module for extracting significant\nhigh-level patterns, a mutual transformer for computing the correlations among\nthe generated features and fusing these features from multiple modalities, and\na detection layer for detecting depression using the fused feature\nrepresentations. The extensive experiments are performed using the multimodal\nD-Vlog dataset, and the findings reveal that the developed multimodal\ndepression detection network surpasses the state-of-the-art by up to 17.37% for\nF1-Score, demonstrating the greater performance of the proposed system. The\nsource code is accessible at\nhttps://github.com/rezwanh001/Multimodal-Depression-Detection.", "AI": {"tldr": "A new network (MDD-Net) uses sound and images from social media to detect depression, outperforming existing methods.", "motivation": "The simple nature of data collection from social media platforms has attracted significant interest in utilizing this information for mental health research, specifically for depression detection.", "method": "A Multimodal Depression Detection Network (MDD-Net) is proposed, utilizing acoustic and visual data from social media networks. It employs mutual transformers to extract and fuse multimodal features through four core modules: acoustic feature extraction, visual feature extraction, mutual transformer, and detection layer.", "result": "The developed MDD-Net surpasses the state-of-the-art by up to 17.37% for F1-Score on the multimodal D-Vlog dataset.", "conclusion": "MDD-Net surpasses state-of-the-art by up to 17.37% for F1-Score."}}
{"id": "2508.08094", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08094", "abs": "https://arxiv.org/abs/2508.08094", "authors": ["Jiakai Lin", "Jinchang Zhang", "Ge Jin", "Wenzhan Song", "Tianming Liu", "Guoyu Lu"], "title": "3D Plant Root Skeleton Detection and Extraction", "comment": null, "summary": "Plant roots typically exhibit a highly complex and dense architecture,\nincorporating numerous slender lateral roots and branches, which significantly\nhinders the precise capture and modeling of the entire root system.\nAdditionally, roots often lack sufficient texture and color information, making\nit difficult to identify and track root traits using visual methods. Previous\nresearch on roots has been largely confined to 2D studies; however, exploring\nthe 3D architecture of roots is crucial in botany. Since roots grow in real 3D\nspace, 3D phenotypic information is more critical for studying genetic traits\nand their impact on root development. We have introduced a 3D root skeleton\nextraction method that efficiently derives the 3D architecture of plant roots\nfrom a few images. This method includes the detection and matching of lateral\nroots, triangulation to extract the skeletal structure of lateral roots, and\nthe integration of lateral and primary roots. We developed a highly complex\nroot dataset and tested our method on it. The extracted 3D root skeletons\nshowed considerable similarity to the ground truth, validating the\neffectiveness of the model. This method can play a significant role in\nautomated breeding robots. Through precise 3D root structure analysis, breeding\nrobots can better identify plant phenotypic traits, especially root structure\nand growth patterns, helping practitioners select seeds with superior root\nsystems. This automated approach not only improves breeding efficiency but also\nreduces manual intervention, making the breeding process more intelligent and\nefficient, thus advancing modern agriculture.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4ece\u5c11\u91cf\u56fe\u50cf\u4e2d\u9ad8\u6548\u63d0\u53d6\u690d\u7269\u6839\u7cfb\u4e09\u7ef4\u7ed3\u6784\u7684\u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u80fd\u7cbe\u786e\u6355\u6349\u6839\u7cfb\u9aa8\u67b6\uff0c\u4e3a\u80b2\u79cd\u673a\u5668\u4eba\u63d0\u4f9b\u652f\u6301\uff0c\u4ee5\u63d0\u9ad8\u80b2\u79cd\u6548\u7387\u3002", "motivation": "\u690d\u7269\u6839\u7cfb\u7684\u4e09\u7ef4\u7ed3\u6784\u590d\u6742\u4e14\u96be\u4ee5\u7cbe\u786e\u6355\u6349\uff0c\u4e8c\u7ef4\u7814\u7a76\u65e0\u6cd5\u6ee1\u8db3\u5bf9\u9057\u4f20\u6027\u72b6\u548c\u6839\u7cfb\u53d1\u80b2\u5f71\u54cd\u7684\u7814\u7a76\u9700\u6c42\uff0c\u56e0\u6b64\u9700\u8981\u4e09\u7ef4\u5206\u6790\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u4e09\u7ef4\u6839\u7cfb\u9aa8\u67b6\u63d0\u53d6\u65b9\u6cd5\uff0c\u5305\u62ec\u4fa7\u6839\u68c0\u6d4b\u4e0e\u5339\u914d\u3001\u4e09\u89d2\u6d4b\u91cf\u63d0\u53d6\u4fa7\u6839\u9aa8\u67b6\u7ed3\u6784\u4ee5\u53ca\u4fa7\u6839\u4e0e\u4e3b\u6839\u7684\u6574\u5408\u3002", "result": "\u63d0\u53d6\u7684\u4e09\u7ef4\u6839\u7cfb\u9aa8\u67b6\u4e0e\u771f\u5b9e\u60c5\u51b5\u9ad8\u5ea6\u76f8\u4f3c\uff0c\u9a8c\u8bc1\u4e86\u8be5\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u53ef\u4ee5\u6709\u6548\u63d0\u53d6\u690d\u7269\u6839\u7cfb\u7684\u4e09\u7ef4\u7ed3\u6784\uff0c\u5e76\u4e3a\u80b2\u79cd\u673a\u5668\u4eba\u63d0\u4f9b\u652f\u6301\uff0c\u4ee5\u63d0\u9ad8\u80b2\u79cd\u6548\u7387\u548c\u667a\u80fd\u5316\u6c34\u5e73\u3002"}}
{"id": "2508.08098", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08098", "abs": "https://arxiv.org/abs/2508.08098", "authors": ["Junzhe Xu", "Yuyang Yin", "Xi Chen"], "title": "TBAC-UniImage: Unified Understanding and Generation by Ladder-Side Diffusion Tuning", "comment": null, "summary": "This paper introduces TBAC-UniImage, a novel unified model for multimodal\nunderstanding and generation. We achieve this by deeply integrating a\npre-trained Diffusion Model, acting as a generative ladder, with a Multimodal\nLarge Language Model (MLLM). Previous diffusion-based unified models face two\nprimary limitations. One approach uses only the MLLM's final hidden state as\nthe generative condition. This creates a shallow connection, as the generator\nis isolated from the rich, hierarchical representations within the MLLM's\nintermediate layers. The other approach, pretraining a unified generative\narchitecture from scratch, is computationally expensive and prohibitive for\nmany researchers. To overcome these issues, our work explores a new paradigm.\nInstead of relying on a single output, we use representations from multiple,\ndiverse layers of the MLLM as generative conditions for the diffusion model.\nThis method treats the pre-trained generator as a ladder, receiving guidance\nfrom various depths of the MLLM's understanding process. Consequently,\nTBAC-UniImage achieves a much deeper and more fine-grained unification of\nunderstanding and generation.", "AI": {"tldr": "TBAC-UniImage\u901a\u8fc7\u5229\u7528MLLM\u591a\u5c42\u8868\u793a\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\uff0c\u5b9e\u73b0\u4e86\u66f4\u6df1\u5c42\u6b21\u7684\u591a\u6a21\u6001\u7406\u89e3\u4e0e\u751f\u6210\u7edf\u4e00\uff0c\u514b\u670d\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u514b\u670d\u4e86\u73b0\u6709\u7edf\u4e00\u6a21\u578b\u4ec5\u4f7f\u7528MLLM\u6700\u7ec8\u9690\u85cf\u72b6\u6001\u4f5c\u4e3a\u751f\u6210\u6761\u4ef6\uff08\u8fde\u63a5\u6d45\uff09\u6216\u4ece\u5934\u5f00\u59cb\u9884\u8bad\u7ec3\u7edf\u4e00\u751f\u6210\u67b6\u6784\uff08\u8ba1\u7b97\u6210\u672c\u9ad8\uff09\u7684\u9650\u5236\u3002", "method": " TBAC-UniImage\u5c06\u9884\u8bad\u7ec3\u7684\u6269\u6563\u6a21\u578b\u4e0e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\uff08MLLM\uff09\u6df1\u5ea6\u96c6\u6210\uff0c\u5229\u7528MLLM\u7684\u4e2d\u95f4\u5c42\u8868\u793a\u6765\u6307\u5bfc\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u8fc7\u7a0b\u3002", "result": "TBAC-UniImage\u5b9e\u73b0\u4e86\u66f4\u6df1\u5165\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u7406\u89e3\u548c\u751f\u6210\u7edf\u4e00\u3002", "conclusion": "TBAC-UniImage\u901a\u8fc7\u4f7f\u7528MLLM\u7684\u591a\u4e2a\u4e0d\u540c\u5c42\u7684\u8868\u793a\u4f5c\u4e3a\u6269\u6563\u6a21\u578b\u7684\u751f\u6210\u6761\u4ef6\uff0c\u5b9e\u73b0\u4e86\u66f4\u6df1\u5165\u3001\u66f4\u7ec6\u7c92\u5ea6\u7684\u7406\u89e3\u548c\u751f\u6210\u7edf\u4e00\u3002"}}
{"id": "2508.08107", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08107", "abs": "https://arxiv.org/abs/2508.08107", "authors": ["Danfeng Hong", "Chenyu Li", "Naoto Yokoya", "Bing Zhang", "Xiuping Jia", "Antonio Plaza", "Paolo Gamba", "Jon Atli Benediktsson", "Jocelyn Chanussot"], "title": "Hyperspectral Imaging", "comment": null, "summary": "Hyperspectral imaging (HSI) is an advanced sensing modality that\nsimultaneously captures spatial and spectral information, enabling\nnon-invasive, label-free analysis of material, chemical, and biological\nproperties. This Primer presents a comprehensive overview of HSI, from the\nunderlying physical principles and sensor architectures to key steps in data\nacquisition, calibration, and correction. We summarize common data structures\nand highlight classical and modern analysis methods, including dimensionality\nreduction, classification, spectral unmixing, and AI-driven techniques such as\ndeep learning. Representative applications across Earth observation, precision\nagriculture, biomedicine, industrial inspection, cultural heritage, and\nsecurity are also discussed, emphasizing HSI's ability to uncover sub-visual\nfeatures for advanced monitoring, diagnostics, and decision-making. Persistent\nchallenges, such as hardware trade-offs, acquisition variability, and the\ncomplexity of high-dimensional data, are examined alongside emerging solutions,\nincluding computational imaging, physics-informed modeling, cross-modal fusion,\nand self-supervised learning. Best practices for dataset sharing,\nreproducibility, and metadata documentation are further highlighted to support\ntransparency and reuse. Looking ahead, we explore future directions toward\nscalable, real-time, and embedded HSI systems, driven by sensor\nminiaturization, self-supervised learning, and foundation models. As HSI\nevolves into a general-purpose, cross-disciplinary platform, it holds promise\nfor transformative applications in science, technology, and society.", "AI": {"tldr": "\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u662f\u4e00\u79cd\u5148\u8fdb\u7684\u4f20\u611f\u6280\u672f\uff0c\u53ef\u540c\u65f6\u6355\u83b7\u7a7a\u95f4\u548c\u5149\u8c31\u4fe1\u606f\uff0c\u7528\u4e8e\u7269\u8d28\u3001\u5316\u5b66\u548c\u751f\u7269\u7279\u6027\u7684\u65e0\u635f\u5206\u6790\u3002\u672c\u6587\u5168\u9762\u6982\u8ff0\u4e86HSI\u7684\u539f\u7406\u3001\u6570\u636e\u91c7\u96c6\u3001\u5206\u6790\u65b9\u6cd5\uff08\u5305\u62ecAI\u9a71\u52a8\u6280\u672f\uff09\u548c\u8de8\u5b66\u79d1\u5e94\u7528\uff0c\u540c\u65f6\u8ba8\u8bba\u4e86\u73b0\u6709\u6311\u6218\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\uff0c\u65e8\u5728\u63a8\u52a8HSI\u6210\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u3001\u8de8\u5b66\u79d1\u7684\u5e73\u53f0\uff0c\u4ee5\u5b9e\u73b0\u79d1\u5b66\u3001\u6280\u672f\u548c\u793e\u4f1a\u9886\u57df\u7684\u53d8\u9769\u6027\u5e94\u7528\u3002", "motivation": "\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u80fd\u591f\u540c\u65f6\u6355\u83b7\u7a7a\u95f4\u548c\u5149\u8c31\u4fe1\u606f\uff0c\u5b9e\u73b0\u5bf9\u7269\u8d28\u3001\u5316\u5b66\u548c\u751f\u7269\u7279\u6027\u7684\u65e0\u635f\u3001\u65e0\u6807\u8bb0\u5206\u6790\uff0c\u5e76\u80fd\u63ed\u793a\u4eba\u773c\u65e0\u6cd5\u8bc6\u522b\u7684\u4e9a\u89c6\u89c9\u7279\u5f81\uff0c\u4ece\u800c\u652f\u6301\u5148\u8fdb\u7684\u76d1\u6d4b\u3001\u8bca\u65ad\u548c\u51b3\u7b56\u3002", "method": "\u672c\u6587\u5168\u9762\u6982\u8ff0\u4e86\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u7684\u539f\u7406\u3001\u4f20\u611f\u5668\u67b6\u6784\u3001\u6570\u636e\u91c7\u96c6\u3001\u6821\u51c6\u3001\u6821\u6b63\u3001\u6570\u636e\u7ed3\u6784\u3001\u964d\u7ef4\u3001\u5206\u7c7b\u3001\u5149\u8c31\u5206\u89e3\u3001\u6df1\u5ea6\u5b66\u4e60\u7b49\u5206\u6790\u65b9\u6cd5\uff0c\u5e76\u63a2\u8ba8\u4e86\u5730\u7403\u89c2\u6d4b\u3001\u7cbe\u51c6\u519c\u4e1a\u3001\u751f\u7269\u533b\u5b66\u3001\u5de5\u4e1a\u68c0\u6d4b\u3001\u6587\u5316\u9057\u4ea7\u548c\u5b89\u5168\u7b49\u9886\u57df\u7684\u5e94\u7528\u3002", "result": "\u672c\u6587\u8ba8\u8bba\u4e86\u786c\u4ef6\u6743\u8861\u3001\u91c7\u96c6\u53d8\u5f02\u6027\u548c\u9ad8\u7ef4\u6570\u636e\u590d\u6742\u6027\u7b49\u6311\u6218\uff0c\u4ee5\u53ca\u8ba1\u7b97\u6210\u50cf\u3001\u7269\u7406\u4fe1\u606f\u5efa\u6a21\u3001\u8de8\u6a21\u6001\u878d\u5408\u548c\u81ea\u76d1\u7763\u5b66\u4e60\u7b49\u65b0\u5174\u89e3\u51b3\u65b9\u6848\uff0c\u5e76\u5f3a\u8c03\u4e86\u6570\u636e\u96c6\u5171\u4eab\u3001\u53ef\u91cd\u590d\u6027\u548c\u5143\u6570\u636e\u6587\u6863\u7684\u6700\u4f73\u5b9e\u8df5\u3002", "conclusion": "\u9ad8\u5149\u8c31\u6210\u50cf\uff08HSI\uff09\u6b63\u53d1\u5c55\u6210\u4e3a\u4e00\u4e2a\u901a\u7528\u7684\u3001\u8de8\u5b66\u79d1\u7684\u5e73\u53f0\uff0c\u6709\u671b\u5728\u79d1\u5b66\u3001\u6280\u672f\u548c\u793e\u4f1a\u9886\u57df\u5e26\u6765\u53d8\u9769\u6027\u7684\u5e94\u7528\u3002"}}
{"id": "2508.08117", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08117", "abs": "https://arxiv.org/abs/2508.08117", "authors": ["Xudong Han", "Pengcheng Fang", "Yueying Tian", "Jianhui Yu", "Xiaohao Cai", "Daniel Roggen", "Philip Birch"], "title": "GRASPTrack: Geometry-Reasoned Association via Segmentation and Projection for Multi-Object Tracking", "comment": null, "summary": "Multi-object tracking (MOT) in monocular videos is fundamentally challenged\nby occlusions and depth ambiguity, issues that conventional\ntracking-by-detection (TBD) methods struggle to resolve owing to a lack of\ngeometric awareness. To address these limitations, we introduce GRASPTrack, a\nnovel depth-aware MOT framework that integrates monocular depth estimation and\ninstance segmentation into a standard TBD pipeline to generate high-fidelity 3D\npoint clouds from 2D detections, thereby enabling explicit 3D geometric\nreasoning. These 3D point clouds are then voxelized to enable a precise and\nrobust Voxel-Based 3D Intersection-over-Union (IoU) for spatial association. To\nfurther enhance tracking robustness, our approach incorporates Depth-aware\nAdaptive Noise Compensation, which dynamically adjusts the Kalman filter\nprocess noise based on occlusion severity for more reliable state estimation.\nAdditionally, we propose a Depth-enhanced Observation-Centric Momentum, which\nextends the motion direction consistency from the image plane into 3D space to\nimprove motion-based association cues, particularly for objects with complex\ntrajectories. Extensive experiments on the MOT17, MOT20, and DanceTrack\nbenchmarks demonstrate that our method achieves competitive performance,\nsignificantly improving tracking robustness in complex scenes with frequent\nocclusions and intricate motion patterns.", "AI": {"tldr": "GRASPTrack\u901a\u8fc7\u6574\u5408\u6df1\u5ea6\u4f30\u8ba1\u548c3D\u51e0\u4f55\u63a8\u7406\u6765\u89e3\u51b3\u5355\u76eeMOT\u4e2d\u7684\u906e\u6321\u548c\u6df1\u5ea6\u6b67\u4e49\u95ee\u9898\uff0c\u5728\u590d\u6742\u573a\u666f\u4e0b\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u6027\u80fd\u3002", "motivation": "\u591a\u76ee\u6807\u8ddf\u8e2a\uff08MOT\uff09\u5728\u5355\u76ee\u89c6\u9891\u4e2d\u9762\u4e34\u7740\u906e\u6321\u548c\u6df1\u5ea6\u6b67\u4e49\u7684\u6839\u672c\u6027\u6311\u6218\uff0c\u800c\u4f20\u7edf\u7684TBD\u65b9\u6cd5\u7531\u4e8e\u7f3a\u4e4f\u51e0\u4f55\u611f\u77e5\u800c\u96be\u4ee5\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u3002", "method": "GRASPTrack\u662f\u4e00\u4e2a\u65b0\u9896\u7684\u3001\u6ce8\u91cd\u6df1\u5ea6\u7684\u591a\u76ee\u6807\u8ddf\u8e2a\u6846\u67b6\uff0c\u5b83\u5c06\u5355\u76ee\u6df1\u5ea6\u4f30\u8ba1\u548c\u5b9e\u4f8b\u5206\u5272\u6574\u5408\u5230\u6807\u51c6\u7684TBD\uff08\u8ddf\u8e2a-\u901a\u8fc7-\u68c0\u6d4b\uff09\u6d41\u7a0b\u4e2d\uff0c\u4ee5\u751f\u6210\u9ad8\u4fdd\u771f3D\u70b9\u4e91\uff0c\u4ece\u800c\u5b9e\u73b0\u663e\u5f0f\u76843D\u51e0\u4f55\u63a8\u7406\u3002\u8fd9\u4e9b3D\u70b9\u4e91\u88ab\u4f53\u7d20\u5316\uff0c\u4ee5\u5b9e\u73b0\u7cbe\u786e\u4e14\u9c81\u68d2\u7684\u57fa\u4e8e\u4f53\u7d20\u76843D\u4ea4\u5e76\u6bd4\uff08IoU\uff09\u8fdb\u884c\u7a7a\u95f4\u5173\u8054\u3002\u6b64\u5916\uff0c\u8be5\u65b9\u6cd5\u8fd8\u7ed3\u5408\u4e86\u6df1\u5ea6\u611f\u77e5\u81ea\u9002\u5e94\u566a\u58f0\u8865\u507f\uff0c\u8be5\u8865\u507f\u6839\u636e\u906e\u6321\u4e25\u91cd\u7a0b\u5ea6\u52a8\u6001\u8c03\u6574\u5361\u5c14\u66fc\u6ee4\u6ce2\u5668\u8fc7\u7a0b\u566a\u58f0\uff0c\u4ee5\u5b9e\u73b0\u66f4\u53ef\u9760\u7684\u72b6\u6001\u4f30\u8ba1\u3002\u8fd8\u63d0\u51fa\u4e86\u4e00\u79cd\u6df1\u5ea6\u589e\u5f3a\u7684\u4ee5\u89c2\u6d4b\u4e3a\u4e2d\u5fc3\u7684\u52a8\u91cf\uff0c\u5c06\u56fe\u50cf\u5e73\u9762\u7684\u8fd0\u52a8\u65b9\u5411\u4e00\u81f4\u6027\u6269\u5c55\u52303D\u7a7a\u95f4\uff0c\u4ee5\u6539\u8fdb\u57fa\u4e8e\u8fd0\u52a8\u7684\u5173\u8054\u7ebf\u7d22\uff0c\u7279\u522b\u662f\u5bf9\u4e8e\u5177\u6709\u590d\u6742\u8f68\u8ff9\u7684\u5bf9\u8c61\u3002", "result": "GRASPTrack\u901a\u8fc7\u751f\u6210\u9ad8\u4fdd\u771f3D\u70b9\u4e91\uff0c\u5b9e\u73b0\u4e86\u663e\u5f0f\u76843D\u51e0\u4f55\u63a8\u7406\uff0c\u5e76\u5229\u7528\u4f53\u7d20\u5316\u5b9e\u73b0\u7cbe\u786e\u4e14\u9c81\u68d2\u7684\u57fa\u4e8e\u4f53\u7d20\u76843D IoU\u8fdb\u884c\u7a7a\u95f4\u5173\u8054\u3002\u901a\u8fc7\u6df1\u5ea6\u611f\u77e5\u81ea\u9002\u5e94\u566a\u58f0\u8865\u507f\u548c\u6df1\u5ea6\u589e\u5f3a\u7684\u4ee5\u89c2\u6d4b\u4e3a\u4e2d\u5fc3\u7684\u52a8\u91cf\uff0c\u8fdb\u4e00\u6b65\u63d0\u9ad8\u4e86\u8ddf\u8e2a\u9c81\u68d2\u6027\uff0c\u5c24\u5176\u662f\u5728\u906e\u6321\u548c\u590d\u6742\u8fd0\u52a8\u573a\u666f\u4e0b\u3002", "conclusion": "GRASPTrack\u5728MOT17\u3001MOT20\u548cDanceTrack\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6709\u7ade\u4e89\u529b\u7684\u6027\u80fd\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5728\u9891\u7e41\u906e\u6321\u548c\u590d\u6742\u8fd0\u52a8\u6a21\u5f0f\u4e0b\u7684\u8ddf\u8e2a\u9c81\u68d2\u6027\u3002"}}
{"id": "2508.08165", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2508.08165", "abs": "https://arxiv.org/abs/2508.08165", "authors": ["Yan Wang", "Da-Wei Zhou", "Han-Jia Ye"], "title": "Integrating Task-Specific and Universal Adapters for Pre-Trained Model-based Class-Incremental Learning", "comment": "Accepted to ICCV 2025. Code is available at:\n  https://github.com/LAMDA-CL/ICCV2025-TUNA", "summary": "Class-Incremental Learning (CIL) requires a learning system to continually\nlearn new classes without forgetting. Existing pre-trained model-based CIL\nmethods often freeze the pre-trained network and adapt to incremental tasks\nusing additional lightweight modules such as adapters. However, incorrect\nmodule selection during inference hurts performance, and task-specific modules\noften overlook shared general knowledge, leading to errors on distinguishing\nbetween similar classes across tasks. To address the aforementioned challenges,\nwe propose integrating Task-Specific and Universal Adapters (TUNA) in this\npaper. Specifically, we train task-specific adapters to capture the most\ncrucial features relevant to their respective tasks and introduce an\nentropy-based selection mechanism to choose the most suitable adapter.\nFurthermore, we leverage an adapter fusion strategy to construct a universal\nadapter, which encodes the most discriminative features shared across tasks. We\ncombine task-specific and universal adapter predictions to harness both\nspecialized and general knowledge during inference. Extensive experiments on\nvarious benchmark datasets demonstrate the state-of-the-art performance of our\napproach. Code is available at: https://github.com/LAMDA-CL/ICCV2025-TUNA", "AI": {"tldr": "TUNA \u662f\u4e00\u79cd\u65b0\u7684 CIL \u65b9\u6cd5\uff0c\u901a\u8fc7\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u548c\u901a\u7528\u9002\u914d\u5668\u6765\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u7684\u9884\u8bad\u7ec3\u6a21\u578b CIL \u65b9\u6cd5\u5728\u63a8\u7406\u65f6\u9009\u62e9\u9519\u8bef\u7684\u6a21\u5757\u4f1a\u635f\u5bb3\u6027\u80fd\uff0c\u5e76\u4e14\u4efb\u52a1\u7279\u5b9a\u6a21\u5757\u5e38\u5e38\u5ffd\u7565\u5171\u4eab\u7684\u901a\u7528\u77e5\u8bc6\uff0c\u5bfc\u81f4\u533a\u5206\u76f8\u4f3c\u7c7b\u522b\u65f6\u51fa\u9519\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u6311\u6218\uff0c\u63d0\u51fa TUNA\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u548c\u901a\u7528\u9002\u914d\u5668\uff08TUNA\uff09\u7684\u65b9\u6cd5\uff0c\u5305\u62ec\u8bad\u7ec3\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u4ee5\u6355\u6349\u5173\u952e\u4efb\u52a1\u7279\u5f81\uff0c\u5f15\u5165\u57fa\u4e8e\u71b5\u7684\u9009\u62e9\u673a\u5236\u6765\u9009\u62e9\u6700\u5408\u9002\u7684\u9002\u914d\u5668\uff0c\u5e76\u901a\u8fc7\u9002\u914d\u5668\u878d\u5408\u7b56\u7565\u6784\u5efa\u4e00\u4e2a\u7f16\u7801\u8de8\u4efb\u52a1\u5171\u4eab\u5224\u522b\u6027\u7279\u5f81\u7684\u901a\u7528\u9002\u914d\u5668\u3002", "result": "TUNA \u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "TUNA\u7ed3\u5408\u4e86\u4efb\u52a1\u7279\u5b9a\u9002\u914d\u5668\u548c\u901a\u7528\u9002\u914d\u5668\u7684\u9884\u6d4b\uff0c\u4ee5\u5229\u7528\u4e13\u95e8\u5316\u548c\u901a\u7528\u77e5\u8bc6\u8fdb\u884c\u63a8\u7406\uff0c\u5e76\u5728\u5404\u79cd\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5c55\u793a\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08123", "categories": ["eess.IV", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08123", "abs": "https://arxiv.org/abs/2508.08123", "authors": ["Lingjing Chen", "Chengxiu Zhang", "Yinqiao Yi", "Yida Wang", "Yang Song", "Xu Yan", "Shengfang Xu", "Dalin Zhu", "Mengqiu Cao", "Yan Zhou", "Chenglong Wang", "Guang Yang"], "title": "A Physics-Driven Neural Network with Parameter Embedding for Generating Quantitative MR Maps from Weighted Images", "comment": null, "summary": "We propose a deep learning-based approach that integrates MRI sequence\nparameters to improve the accuracy and generalizability of quantitative image\nsynthesis from clinical weighted MRI. Our physics-driven neural network embeds\nMRI sequence parameters -- repetition time (TR), echo time (TE), and inversion\ntime (TI) -- directly into the model via parameter embedding, enabling the\nnetwork to learn the underlying physical principles of MRI signal formation.\nThe model takes conventional T1-weighted, T2-weighted, and T2-FLAIR images as\ninput and synthesizes T1, T2, and proton density (PD) quantitative maps.\nTrained on healthy brain MR images, it was evaluated on both internal and\nexternal test datasets. The proposed method achieved high performance with PSNR\nvalues exceeding 34 dB and SSIM values above 0.92 for all synthesized parameter\nmaps. It outperformed conventional deep learning models in accuracy and\nrobustness, including data with previously unseen brain structures and lesions.\nNotably, our model accurately synthesized quantitative maps for these unseen\npathological regions, highlighting its superior generalization capability.\nIncorporating MRI sequence parameters via parameter embedding allows the neural\nnetwork to better learn the physical characteristics of MR signals,\nsignificantly enhancing the performance and reliability of quantitative MRI\nsynthesis. This method shows great potential for accelerating qMRI and\nimproving its clinical utility.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u6574\u5408MRI\u5e8f\u5217\u53c2\u6570\uff08TR\u3001TE\u3001TI\uff09\u5230\u6a21\u578b\u4e2d\uff0c\u5229\u7528\u53c2\u6570\u5d4c\u5165\u6765\u63d0\u9ad8\u5b9a\u91cfMRI\u56fe\u50cf\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u65b9\u6cd5\u5728\u5404\u79cd\u6570\u636e\u96c6\u4e0a\u5747\u8868\u73b0\u51fa\u8272\uff0c\u4f18\u4e8e\u4f20\u7edf\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5c24\u5176\u5728\u5904\u7406\u672a\u89c1\u8fc7\u7684\u8111\u90e8\u7ed3\u6784\u548c\u75c5\u7076\u65f6\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\uff0c\u6709\u671b\u52a0\u901fqMRI\u5e76\u63d0\u5347\u5176\u4e34\u5e8a\u5e94\u7528\u4ef7\u503c\u3002", "motivation": "\u4e3a\u4e86\u63d0\u9ad8\u4ece\u4e34\u5e8a\u52a0\u6743MRI\u8fdb\u884c\u5b9a\u91cf\u56fe\u50cf\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\uff0c\u901a\u8fc7\u53c2\u6570\u5d4c\u5165\u5c06MRI\u5e8f\u5217\u53c2\u6570\uff08\u91cd\u590d\u65f6\u95f4TR\u3001\u56de\u6ce2\u65f6\u95f4TE\u548c\u53cd\u8f6c\u65f6\u95f4TI\uff09\u6574\u5408\u5230\u6a21\u578b\u4e2d\uff0c\u4ee5\u63d0\u9ad8\u4ece\u4e34\u5e8a\u52a0\u6743MRI\u8fdb\u884c\u5b9a\u91cf\u56fe\u50cf\u5408\u6210\u7684\u51c6\u786e\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002\u8be5\u6a21\u578b\u4ee5\u5e38\u89c4T1\u52a0\u6743\u3001T2\u52a0\u6743\u548cT2-FLAIR\u56fe\u50cf\u4f5c\u4e3a\u8f93\u5165\uff0c\u5408\u6210T1\u3001T2\u548c\u8d28\u5b50\u5bc6\u5ea6\uff08PD\uff09\u5b9a\u91cf\u56fe\u3002", "result": "\u5728\u5065\u5eb7\u8111\u90e8MRI\u56fe\u50cf\u4e0a\u8bad\u7ec3\u7684\u6a21\u578b\uff0c\u5728\u5185\u90e8\u548c\u5916\u90e8\u6d4b\u8bd5\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bc4\u4f30\u3002\u6240\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6240\u6709\u5408\u6210\u7684\u53c2\u6570\u56fe\u4e0a\u90fd\u53d6\u5f97\u4e86\u5f88\u9ad8\u7684\u6027\u80fd\uff0cPSNR\u503c\u8d85\u8fc734 dB\uff0cSSIM\u503c\u8d85\u8fc70.92\u3002\u4e0e\u4f20\u7edf\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u76f8\u6bd4\uff0c\u8be5\u65b9\u6cd5\u5728\u51c6\u786e\u6027\u548c\u9c81\u68d2\u6027\u65b9\u9762\u8868\u73b0\u66f4\u4f18\uff0c\u5305\u62ec\u5bf9\u5148\u524d\u672a\u89c1\u8fc7\u7684\u8111\u90e8\u7ed3\u6784\u548c\u75c5\u7076\u6570\u636e\u3002\u503c\u5f97\u6ce8\u610f\u7684\u662f\uff0c\u8be5\u6a21\u578b\u80fd\u591f\u51c6\u786e\u5730\u5408\u6210\u8fd9\u4e9b\u672a\u89c1\u7684\u75c5\u7406\u533a\u57df\u7684\u5b9a\u91cf\u56fe\uff0c\u51f8\u663e\u4e86\u5176\u5353\u8d8a\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u901a\u8fc7\u53c2\u6570\u5d4c\u5165\u6574\u5408MRI\u5e8f\u5217\u53c2\u6570\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u66f4\u597d\u5730\u5b66\u4e60MR\u4fe1\u53f7\u7684\u7269\u7406\u7279\u6027\uff0c\u663e\u8457\u63d0\u9ad8\u4e86\u5b9a\u91cfMRI\u5408\u6210\u7684\u6027\u80fd\u548c\u53ef\u9760\u6027\u3002\u8be5\u65b9\u6cd5\u5728\u52a0\u901fqMRI\u548c\u63d0\u9ad8\u5176\u4e34\u5e8a\u5e94\u7528\u6027\u65b9\u9762\u663e\u793a\u51fa\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.08134", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08134", "abs": "https://arxiv.org/abs/2508.08134", "authors": ["Zeqian Long", "Mingzhe Zheng", "Kunyu Feng", "Xinhua Zhang", "Hongyu Liu", "Harry Yang", "Linfeng Zhang", "Qifeng Chen", "Yue Ma"], "title": "Follow-Your-Shape: Shape-Aware Image Editing via Trajectory-Guided Region Control", "comment": "Project webpage is available at https://follow-your-shape.github.io/", "summary": "While recent flow-based image editing models demonstrate general-purpose\ncapabilities across diverse tasks, they often struggle to specialize in\nchallenging scenarios -- particularly those involving large-scale shape\ntransformations. When performing such structural edits, these methods either\nfail to achieve the intended shape change or inadvertently alter non-target\nregions, resulting in degraded background quality. We propose\nFollow-Your-Shape, a training-free and mask-free framework that supports\nprecise and controllable editing of object shapes while strictly preserving\nnon-target content. Motivated by the divergence between inversion and editing\ntrajectories, we compute a Trajectory Divergence Map (TDM) by comparing\ntoken-wise velocity differences between the inversion and denoising paths. The\nTDM enables precise localization of editable regions and guides a Scheduled KV\nInjection mechanism that ensures stable and faithful editing. To facilitate a\nrigorous evaluation, we introduce ReShapeBench, a new benchmark comprising 120\nnew images and enriched prompt pairs specifically curated for shape-aware\nediting. Experiments demonstrate that our method achieves superior editability\nand visual fidelity, particularly in tasks requiring large-scale shape\nreplacement.", "AI": {"tldr": "Follow-Your-Shape \u6846\u67b6\u901a\u8fc7\u8f68\u8ff9\u53d1\u6563\u56fe\uff08TDM\uff09\u548c\u9884\u5b9a\u7684 KV \u6ce8\u5165\u673a\u5236\uff0c\u5b9e\u73b0\u4e86\u7cbe\u786e\u3001\u53ef\u63a7\u7684\u7269\u4f53\u5f62\u72b6\u7f16\u8f91\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u975e\u76ee\u6807\u5185\u5bb9\u7684\u5b8c\u6574\u6027\uff0c\u5e76\u5728 ReShapeBench \u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u4f18\u8d8a\u7684\u6027\u80fd\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u57fa\u4e8e\u6d41\u7684\u56fe\u50cf\u7f16\u8f91\u6a21\u578b\u5728\u5904\u7406\u5927\u5c3a\u5ea6\u5f62\u72b6\u53d8\u6362\u7b49\u5177\u6709\u6311\u6218\u6027\u7684\u573a\u666f\u65f6\uff0c\u96be\u4ee5\u5b9e\u73b0\u9884\u671f\u5f62\u72b6\u6539\u53d8\u6216\u65e0\u610f\u4e2d\u6539\u53d8\u975e\u76ee\u6807\u533a\u57df\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a Follow-Your-Shape \u7684\u8bad\u7ec3\u548c\u63a9\u6a21\u65e0\u5173\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u901a\u8fc7\u8ba1\u7b97\u8f68\u8ff9\u53d1\u6563\u56fe\uff08TDM\uff09\u6765\u7cbe\u786e\u5730\u5b9a\u4f4d\u53ef\u7f16\u8f91\u533a\u57df\uff0c\u5e76\u6307\u5bfc\u9884\u5b9a\u7684 KV \u6ce8\u5165\u673a\u5236\u4ee5\u5b9e\u73b0\u7a33\u5b9a\u548c\u5fe0\u5b9e\u7684\u7f16\u8f91\u3002TDM \u901a\u8fc7\u6bd4\u8f83\u53cd\u8f6c\u548c\u53bb\u566a\u8def\u5f84\u4e4b\u95f4\u7684 token \u7ea7\u901f\u5ea6\u5dee\u5f02\u6765\u8ba1\u7b97\u3002", "result": "\u8be5\u65b9\u6cd5\u5728\u9700\u8981\u5927\u5c3a\u5ea6\u5f62\u72b6\u66ff\u6362\u7684\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u53ef\u7f16\u8f91\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002", "conclusion": "Follow-Your-Shape \u6846\u67b6\u5728\u9700\u8981\u5927\u5c3a\u5ea6\u5f62\u72b6\u66ff\u6362\u7684\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u5353\u8d8a\u7684\u53ef\u7f16\u8f91\u6027\u548c\u89c6\u89c9\u4fdd\u771f\u5ea6\u3002"}}
{"id": "2508.08136", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08136", "abs": "https://arxiv.org/abs/2508.08136", "authors": ["Yitong Yang", "Yinglin Wang", "Changshuo Wang", "Huajie Wang", "Shuting He"], "title": "FantasyStyle: Controllable Stylized Distillation for 3D Gaussian Splatting", "comment": null, "summary": "The success of 3DGS in generative and editing applications has sparked\ngrowing interest in 3DGS-based style transfer. However, current methods still\nface two major challenges: (1) multi-view inconsistency often leads to style\nconflicts, resulting in appearance smoothing and distortion; and (2) heavy\nreliance on VGG features, which struggle to disentangle style and content from\nstyle images, often causing content leakage and excessive stylization. To\ntackle these issues, we introduce \\textbf{FantasyStyle}, a 3DGS-based style\ntransfer framework, and the first to rely entirely on diffusion model\ndistillation. It comprises two key components: (1) \\textbf{Multi-View Frequency\nConsistency}. We enhance cross-view consistency by applying a 3D filter to\nmulti-view noisy latent, selectively reducing low-frequency components to\nmitigate stylized prior conflicts. (2) \\textbf{Controllable Stylized\nDistillation}. To suppress content leakage from style images, we introduce\nnegative guidance to exclude undesired content. In addition, we identify the\nlimitations of Score Distillation Sampling and Delta Denoising Score in 3D\nstyle transfer and remove the reconstruction term accordingly. Building on\nthese insights, we propose a controllable stylized distillation that leverages\nnegative guidance to more effectively optimize the 3D Gaussians. Extensive\nexperiments demonstrate that our method consistently outperforms\nstate-of-the-art approaches, achieving higher stylization quality and visual\nrealism across various scenes and styles.", "AI": {"tldr": "FantasyStyle\u5229\u7528\u6269\u6563\u6a21\u578b\u84b8\u998f\u548c\u591a\u89c6\u56fe\u9891\u7387\u4e00\u81f4\u6027\u3001\u53ef\u63a7\u98ce\u683c\u5316\u84b8\u998f\u6765\u6539\u8fdb3DGS\u98ce\u683c\u8fc1\u79fb\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u75db\u70b9\uff0c\u6548\u679c\u4f18\u4e8eSOTA\u3002", "motivation": "\u89e3\u51b3\u73b0\u67093DGS-based style transfer\u65b9\u6cd5\u9762\u4e34\u7684\u591a\u89c6\u56fe\u4e0d\u4e00\u81f4\uff08\u5bfc\u81f4\u5916\u89c2\u5e73\u6ed1\u548c\u5931\u771f\uff09\u548c\u8fc7\u5ea6\u4f9d\u8d56VGG\u7279\u5f81\uff08\u5bfc\u81f4\u5185\u5bb9\u6cc4\u9732\u548c\u8fc7\u5ea6\u98ce\u683c\u5316\uff09\u7684\u6311\u6218\u3002", "method": "FantasyStyle\u6846\u67b6\uff0c\u91c7\u7528\u591a\u89c6\u56fe\u9891\u7387\u4e00\u81f4\u6027\uff08\u901a\u8fc73D\u6ee4\u6ce2\u51cf\u5c11\u4f4e\u9891\u6210\u5206\uff09\u548c\u53ef\u63a7\u98ce\u683c\u5316\u84b8\u998f\uff08\u5f15\u5165\u8d1f\u9762\u5f15\u5bfc\u6291\u5236\u5185\u5bb9\u6cc4\u9732\uff09\uff0c\u5b8c\u5168\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u84b8\u998f\uff0c\u5e76\u79fb\u9664\u4e86\u91cd\u6784\u9879\u3002", "result": "\u5728\u5404\u79cd\u573a\u666f\u548c\u98ce\u683c\u7684\u5b9e\u9a8c\u4e2d\uff0cFantasyStyle\u7684\u98ce\u683c\u5316\u8d28\u91cf\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u6301\u7eed\u4f18\u4e8e\u6700\u5148\u8fdb\u7684\u65b9\u6cd5\u3002", "conclusion": "FantasyStyle\u57283DGS-based style transfer\u65b9\u9762\u53d6\u5f97\u4e86\u6210\u529f\uff0c\u76f8\u6bd4\u73b0\u6709\u65b9\u6cd5\u5728\u98ce\u683c\u5316\u8d28\u91cf\u548c\u89c6\u89c9\u771f\u5b9e\u611f\u65b9\u9762\u8868\u73b0\u66f4\u4f18\u3002"}}
{"id": "2508.08141", "categories": ["cs.CV", "cs.SD", "eess.AS"], "pdf": "https://arxiv.org/pdf/2508.08141", "abs": "https://arxiv.org/abs/2508.08141", "authors": ["Nicholas Klein", "Hemlata Tak", "James Fullwood", "Krishna Regmi", "Leonidas Spinoulas", "Ganesh Sivaraman", "Tianxiang Chen", "Elie Khoury"], "title": "Pindrop it! Audio and Visual Deepfake Countermeasures for Robust Detection and Fine Grained-Localization", "comment": null, "summary": "The field of visual and audio generation is burgeoning with new\nstate-of-the-art methods. This rapid proliferation of new techniques\nunderscores the need for robust solutions for detecting synthetic content in\nvideos. In particular, when fine-grained alterations via localized\nmanipulations are performed in visual, audio, or both domains, these subtle\nmodifications add challenges to the detection algorithms. This paper presents\nsolutions for the problems of deepfake video classification and localization.\nThe methods were submitted to the ACM 1M Deepfakes Detection Challenge,\nachieving the best performance in the temporal localization task and a top four\nranking in the classification task for the TestA split of the evaluation\ndataset.", "AI": {"tldr": "This paper tackles the challenge of detecting deepfakes, especially those with subtle, localized manipulations, by proposing new classification and localization methods that performed exceptionally well in a major challenge.", "motivation": "The rapid proliferation of new techniques in visual and audio generation underscores the need for robust solutions for detecting synthetic content in videos, especially when fine-grained alterations are performed.", "method": "The paper presents methods for deepfake video classification and localization.", "result": "Our methods achieved the best performance in temporal localization and a top four ranking in classification for the ACM 1M Deepfakes Detection Challenge.", "conclusion": "We present solutions for deepfake video classification and localization that achieved top performance in the ACM 1M Deepfakes Detection Challenge."}}
{"id": "2508.08170", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08170", "abs": "https://arxiv.org/abs/2508.08170", "authors": ["Chaojun Ni", "Guosheng Zhao", "Xiaofeng Wang", "Zheng Zhu", "Wenkang Qin", "Xinze Chen", "Guanghong Jia", "Guan Huang", "Wenjun Mei"], "title": "ReconDreamer-RL: Enhancing Reinforcement Learning via Diffusion-based Scene Reconstruction", "comment": null, "summary": "Reinforcement learning for training end-to-end autonomous driving models in\nclosed-loop simulations is gaining growing attention. However, most simulation\nenvironments differ significantly from real-world conditions, creating a\nsubstantial simulation-to-reality (sim2real) gap. To bridge this gap, some\napproaches utilize scene reconstruction techniques to create photorealistic\nenvironments as a simulator. While this improves realistic sensor simulation,\nthese methods are inherently constrained by the distribution of the training\ndata, making it difficult to render high-quality sensor data for novel\ntrajectories or corner case scenarios. Therefore, we propose ReconDreamer-RL, a\nframework designed to integrate video diffusion priors into scene\nreconstruction to aid reinforcement learning, thereby enhancing end-to-end\nautonomous driving training. Specifically, in ReconDreamer-RL, we introduce\nReconSimulator, which combines the video diffusion prior for appearance\nmodeling and incorporates a kinematic model for physical modeling, thereby\nreconstructing driving scenarios from real-world data. This narrows the\nsim2real gap for closed-loop evaluation and reinforcement learning. To cover\nmore corner-case scenarios, we introduce the Dynamic Adversary Agent (DAA),\nwhich adjusts the trajectories of surrounding vehicles relative to the ego\nvehicle, autonomously generating corner-case traffic scenarios (e.g., cut-in).\nFinally, the Cousin Trajectory Generator (CTG) is proposed to address the issue\nof training data distribution, which is often biased toward simple\nstraight-line movements. Experiments show that ReconDreamer-RL improves\nend-to-end autonomous driving training, outperforming imitation learning\nmethods with a 5x reduction in the Collision Ratio.", "AI": {"tldr": "ReconDreamer-RL \u6846\u67b6\u901a\u8fc7\u7ed3\u5408\u89c6\u9891\u6269\u6563\u6a21\u578b\u548c\u573a\u666f\u91cd\u5efa\uff0c\u5e76\u5f15\u5165 DAA \u548c CTG \u6765\u89e3\u51b3\u81ea\u52a8\u9a7e\u9a76\u6a21\u62df\u4e2d\u7684 sim2real \u5dee\u8ddd\u548c\u6570\u636e\u504f\u5dee\u95ee\u9898\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u4e2d\u5b9e\u73b0\u4e86 5 \u500d\u7684\u78b0\u649e\u7387\u964d\u4f4e\u3002", "motivation": "\u4e3a\u4e86\u5f25\u5408\u6a21\u62df\u5230\u73b0\u5b9e (sim2real) \u7684\u5dee\u8ddd\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u573a\u666f\u91cd\u5efa\u65b9\u6cd5\u5728\u751f\u6210\u65b0\u9896\u8f68\u8ff9\u6216\u8f6c\u89d2\u6848\u4f8b\u573a\u666f\u7684\u9ad8\u8d28\u91cf\u4f20\u611f\u5668\u6570\u636e\u65b9\u9762\u5b58\u5728\u7684\u5c40\u9650\u6027\uff0c\u672c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u96c6\u6210\u89c6\u9891\u6269\u6563\u5148\u9a8c\u6765\u6539\u8fdb\u7528\u4e8e\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u8bad\u7ec3\u7684\u6a21\u62df\u5668\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ReconDreamer-RL \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u5148\u9a8c\u4e0e\u573a\u666f\u91cd\u5efa\u76f8\u7ed3\u5408\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u5bf9\u6297\u6027\u4ee3\u7406 (DAA) \u548c\u8868\u4eb2\u8f68\u8ff9\u751f\u6210\u5668 (CTG)\u3002ReconSimulator \u7ed3\u5408\u4e86\u7528\u4e8e\u5916\u89c2\u5efa\u6a21\u7684\u89c6\u9891\u6269\u6563\u5148\u9a8c\u548c\u7528\u4e8e\u7269\u7406\u5efa\u6a21\u7684\u8fd0\u52a8\u5b66\u6a21\u578b\uff0c\u4ee5\u4ece\u771f\u5b9e\u4e16\u754c\u6570\u636e\u4e2d\u91cd\u5efa\u9a7e\u9a76\u573a\u666f\u3002DAA \u8c03\u6574\u4e86\u76f8\u5bf9\u4e8e\u81ea\u8f66\u5468\u56f4\u8f66\u8f86\u7684\u8f68\u8ff9\uff0c\u4ee5\u751f\u6210\u8f6c\u89d2\u6848\u4f8b\u4ea4\u901a\u573a\u666f\u3002CTG \u7528\u4e8e\u89e3\u51b3\u8bad\u7ec3\u6570\u636e\u5206\u5e03\u901a\u5e38\u504f\u5411\u7b80\u5355\u76f4\u7ebf\u8fd0\u52a8\u7684\u95ee\u9898\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cReconDreamer-RL \u6539\u8fdb\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u8bad\u7ec3\uff0c\u5176\u6027\u80fd\u4f18\u4e8e\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\uff0c\u78b0\u649e\u7387\u964d\u4f4e\u4e86 5 \u500d\u3002", "conclusion": "ReconDreamer-RL \u901a\u8fc7\u5c06\u89c6\u9891\u6269\u6563\u6a21\u578b\u5148\u9a8c\u4e0e\u573a\u666f\u91cd\u5efa\u76f8\u7ed3\u5408\uff0c\u5e76\u5f15\u5165\u52a8\u6001\u5bf9\u6297\u6027\u4ee3\u7406 (DAA) \u548c\u8868\u4eb2\u8f68\u8ff9\u751f\u6210\u5668 (CTG) \u6765\u5f25\u5408\u6a21\u62df\u5230\u73b0\u5b9e\u7684\u5dee\u8ddd\uff0c\u4ece\u800c\u6539\u8fdb\u4e86\u7aef\u5230\u7aef\u81ea\u52a8\u9a7e\u9a76\u6a21\u578b\u7684\u8bad\u7ec3\u3002\u8be5\u6846\u67b6\u5728\u78b0\u649e\u7387\u65b9\u9762\u6bd4\u4ec5\u6a21\u4eff\u5b66\u4e60\u65b9\u6cd5\u6709 5 \u500d\u7684\u6539\u5584\u3002"}}
{"id": "2508.08173", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.08173", "abs": "https://arxiv.org/abs/2508.08173", "authors": ["Chongke Bi", "Xin Gao", "Jiangkang Deng", "Guan"], "title": "CD-TVD: Contrastive Diffusion for 3D Super-Resolution with Scarce High-Resolution Time-Varying Data", "comment": "Time-varying data visualization, deep learning, super-resolution,\n  diffusion model", "summary": "Large-scale scientific simulations require significant resources to generate\nhigh-resolution time-varying data (TVD). While super-resolution is an efficient\npost-processing strategy to reduce costs, existing methods rely on a large\namount of HR training data, limiting their applicability to diverse simulation\nscenarios. To address this constraint, we proposed CD-TVD, a novel framework\nthat combines contrastive learning and an improved diffusion-based\nsuper-resolution model to achieve accurate 3D super-resolution from limited\ntime-step high-resolution data. During pre-training on historical simulation\ndata, the contrastive encoder and diffusion superresolution modules learn\ndegradation patterns and detailed features of high-resolution and\nlow-resolution samples. In the training phase, the improved diffusion model\nwith a local attention mechanism is fine-tuned using only one newly generated\nhigh-resolution timestep, leveraging the degradation knowledge learned by the\nencoder. This design minimizes the reliance on large-scale high-resolution\ndatasets while maintaining the capability to recover fine-grained details.\nExperimental results on fluid and atmospheric simulation datasets confirm that\nCD-TVD delivers accurate and resource-efficient 3D super-resolution, marking a\nsignificant advancement in data augmentation for large-scale scientific\nsimulations. The code is available at\nhttps://github.com/Xin-Gao-private/CD-TVD.", "AI": {"tldr": "CD-TVD\u662f\u4e00\u4e2a\u521b\u65b0\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u6269\u6563\u6a21\u578b\uff0c\u5229\u7528\u5c11\u91cf\u9ad8\u5206\u8fa8\u7387\u6570\u636e\u5b9e\u73b0\u79d1\u5b66\u6a21\u62df\u6570\u636e\u76843D\u8d85\u5206\u8fa8\u7387\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u5bf9\u5927\u91cf\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\u7684\u4f9d\u8d56\u6027\u3002", "motivation": "\u4e3a\u4e86\u89e3\u51b3\u73b0\u6709\u8d85\u5206\u8fa8\u7387\u65b9\u6cd5\u4f9d\u8d56\u5927\u91cf\u9ad8\u5206\u8fa8\u7387\u8bad\u7ec3\u6570\u636e\u3001\u9650\u5236\u5176\u5728\u591a\u6837\u5316\u6a21\u62df\u573a\u666f\u4e2d\u5e94\u7528\u7684\u95ee\u9898\uff0c\u63d0\u51faCD-TVD\u6846\u67b6\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u5bf9\u6bd4\u5b66\u4e60\u548c\u6539\u8fdb\u7684\u57fa\u4e8e\u6269\u6563\u7684\u8d85\u5206\u8fa8\u7387\u6a21\u578b\uff08CD-TVD\uff09\u7684\u6846\u67b6\u3002\u8be5\u6846\u67b6\u5728\u9884\u8bad\u7ec3\u9636\u6bb5\u5229\u7528\u5386\u53f2\u6a21\u62df\u6570\u636e\uff0c\u8ba9\u5bf9\u6bd4\u7f16\u7801\u5668\u548c\u6269\u6563\u8d85\u5206\u8fa8\u7387\u6a21\u5757\u5b66\u4e60\u9ad8\u5206\u8fa8\u7387\u548c\u4f4e\u5206\u8fa8\u7387\u6837\u672c\u7684\u9000\u5316\u6a21\u5f0f\u548c\u8be6\u7ec6\u7279\u5f81\u3002\u5728\u8bad\u7ec3\u9636\u6bb5\uff0c\u901a\u8fc7\u5f15\u5165\u5c40\u90e8\u6ce8\u610f\u529b\u673a\u5236\u7684\u6539\u8fdb\u6269\u6563\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528\u4e00\u4e2a\u65b0\u751f\u6210\u7684\u9ad8\u5206\u8fa8\u7387\u65f6\u95f4\u6b65\u957f\u8fdb\u884c\u5fae\u8c03\uff0c\u4ece\u800c\u5229\u7528\u7f16\u7801\u5668\u5b66\u4e60\u5230\u7684\u9000\u5316\u77e5\u8bc6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cCD-TVD\u5728\u6d41\u4f53\u548c\u5927\u6c14\u6a21\u62df\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u51c6\u786e\u4e14\u8d44\u6e90\u9ad8\u6548\u76843D\u8d85\u5206\u8fa8\u7387\u3002", "conclusion": "CD-TVD\u6846\u67b6\u80fd\u591f\u5bf93D\u6570\u636e\u8fdb\u884c\u7cbe\u786e\u4e14\u8d44\u6e90\u9ad8\u6548\u7684\u8d85\u5206\u8fa8\u7387\u5904\u7406\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5927\u89c4\u6a21\u79d1\u5b66\u6a21\u62df\u4e2d\u7684\u6570\u636e\u589e\u5f3a\u80fd\u529b\u3002"}}
{"id": "2508.08177", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08177", "abs": "https://arxiv.org/abs/2508.08177", "authors": ["Zhonghao Yan", "Muxi Diao", "Yuxuan Yang", "Jiayuan Xu", "Kaizhou Zhang", "Ruoyan Jing", "Lele Yang", "Yanxi Liu", "Kongming Liang", "Zhanyu Ma"], "title": "MedReasoner: Reinforcement Learning Drives Reasoning Grounding from Clinical Thought to Pixel-Level Precision", "comment": "37 pages", "summary": "Accurately grounding regions of interest (ROIs) is critical for diagnosis and\ntreatment planning in medical imaging. While multimodal large language models\n(MLLMs) combine visual perception with natural language, current\nmedical-grounding pipelines still rely on supervised fine-tuning with explicit\nspatial hints, making them ill-equipped to handle the implicit queries common\nin clinical practice. This work makes three core contributions. We first define\nUnified Medical Reasoning Grounding (UMRG), a novel vision-language task that\ndemands clinical reasoning and pixel-level grounding. Second, we release\nU-MRG-14K, a dataset of 14K samples featuring pixel-level masks alongside\nimplicit clinical queries and reasoning traces, spanning 10 modalities, 15\nsuper-categories, and 108 specific categories. Finally, we introduce\nMedReasoner, a modular framework that distinctly separates reasoning from\nsegmentation: an MLLM reasoner is optimized with reinforcement learning, while\na frozen segmentation expert converts spatial prompts into masks, with\nalignment achieved through format and accuracy rewards. MedReasoner achieves\nstate-of-the-art performance on U-MRG-14K and demonstrates strong\ngeneralization to unseen clinical queries, underscoring the significant promise\nof reinforcement learning for interpretable medical grounding.", "AI": {"tldr": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86UMRG\u4efb\u52a1\u548cU-MRG-14K\u6570\u636e\u96c6\uff0c\u5e76\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u540d\u4e3aMedReasoner\u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5229\u7528\u5f3a\u5316\u5b66\u4e60\u548cMLLM\u6765\u6539\u8fdb\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u9690\u5f0f\u67e5\u8be2\u533a\u57df\u5b9a\u4f4d\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u5f71\u50cf\u4e2d\u7684\u533a\u57df\u5b9a\u4f4d\uff08ROI\uff09\u4f9d\u8d56\u4e8e\u6709\u76d1\u7763\u7684\u5fae\u8c03\u548c\u660e\u786e\u7684\u7a7a\u95f4\u63d0\u793a\uff0c\u8fd9\u4f7f\u5f97\u5b83\u4eec\u96be\u4ee5\u5904\u7406\u4e34\u5e8a\u5b9e\u8df5\u4e2d\u5e38\u89c1\u7684\u9690\u5f0f\u67e5\u8be2\u3002\u56e0\u6b64\uff0c\u9700\u8981\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\u6765\u5904\u7406\u9690\u5f0f\u67e5\u8be2\u5e76\u8fdb\u884c\u7cbe\u786e\u7684\u533a\u57df\u5b9a\u4f4d\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aMedReasoner\u7684\u6a21\u5757\u5316\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u63a8\u7406\u4e0e\u5206\u5272\u4efb\u52a1\u5206\u79bb\u5f00\u6765\u3002\u63a8\u7406\u90e8\u5206\u7531\u4e00\u4e2aMLLM\uff08\u5927\u578b\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\uff09\u8d1f\u8d23\uff0c\u5e76\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u8fdb\u884c\u4f18\u5316\u3002\u5206\u5272\u90e8\u5206\u5219\u4f9d\u8d56\u4e8e\u4e00\u4e2a\u51bb\u7ed3\u7684\u5206\u5272\u4e13\u5bb6\uff0c\u8be5\u4e13\u5bb6\u5c06\u7a7a\u95f4\u63d0\u793a\u8f6c\u6362\u4e3a\u63a9\u7801\u3002\u901a\u8fc7\u683c\u5f0f\u548c\u51c6\u786e\u6027\u5956\u52b1\u6765\u5b9e\u73b0\u63a8\u7406\u548c\u5206\u5272\u7684\u5bf9\u9f50\u3002", "result": "MedReasoner\u5728U-MRG-14K\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u5728\u5904\u7406\u672a\u77e5\u7684\u4e34\u5e8a\u67e5\u8be2\u65f6\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u8be5\u7814\u7a76\u63d0\u51fa\u4e86\u7edf\u4e00\u533b\u5b66\u63a8\u7406\u57fa\u7840\uff08UMRG\uff09\u8fd9\u4e00\u65b0\u89c6\u89c9-\u8bed\u8a00\u4efb\u52a1\uff0c\u5e76\u53d1\u5e03\u4e86\u5305\u542b14K\u4e2a\u6837\u672c\u3001\u652f\u630110\u79cd\u6a21\u6001\u300115\u4e2a\u8d85\u7c7b\u548c108\u4e2a\u5177\u4f53\u7c7b\u522b\u7684U-MRG-14K\u6570\u636e\u96c6\u3002\u6b64\u5916\uff0c\u8be5\u7814\u7a76\u8fd8\u4ecb\u7ecd\u4e86MedReasoner\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5c06\u63a8\u7406\u4e0e\u5206\u5272\u5206\u5f00\uff0c\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u4f18\u5316MLLM\u63a8\u7406\u5668\uff0c\u5e76\u51bb\u7ed3\u5206\u5272\u4e13\u5bb6\u4ee5\u751f\u6210\u63a9\u7801\u3002MedReasoner\u5728U-MRG-14K\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u5bf9\u672a\u89c1\u8fc7\u7684\u4e34\u5e8a\u67e5\u8be2\u7684\u826f\u597d\u6cdb\u5316\u80fd\u529b\uff0c\u8868\u660e\u5f3a\u5316\u5b66\u4e60\u5728\u53ef\u89e3\u91ca\u533b\u5b66\u57fa\u7840\u65b9\u9762\u5177\u6709\u5de8\u5927\u6f5c\u529b\u3002"}}
{"id": "2508.08178", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08178", "abs": "https://arxiv.org/abs/2508.08178", "authors": ["Ozhan Suat", "Bedirhan Uguz", "Batuhan Karagoz", "Muhammed Can Keles", "Emre Akbas"], "title": "3D Human Mesh Estimation from Single View RGBD", "comment": null, "summary": "Despite significant progress in 3D human mesh estimation from RGB images;\nRGBD cameras, offering additional depth data, remain underutilized. In this\npaper, we present a method for accurate 3D human mesh estimation from a single\nRGBD view, leveraging the affordability and widespread adoption of RGBD cameras\nfor real-world applications. A fully supervised approach for this problem,\nrequires a dataset with RGBD image and 3D mesh label pairs. However, collecting\nsuch a dataset is costly and challenging, hence, existing datasets are small,\nand limited in pose and shape diversity. To overcome this data scarcity, we\nleverage existing Motion Capture (MoCap) datasets. We first obtain complete 3D\nmeshes from the body models found in MoCap datasets, and create partial,\nsingle-view versions of them by projection to a virtual camera. This simulates\nthe depth data provided by an RGBD camera from a single viewpoint. Then, we\ntrain a masked autoencoder to complete the partial, single-view mesh. During\ninference, our method, which we name as M$^3$ for ``Masked Mesh Modeling'',\nmatches the depth values coming from the sensor to vertices of a template human\nmesh, which creates a partial, single-view mesh. We effectively recover parts\nof the 3D human body mesh model that are not visible, resulting in a full body\nmesh. M$^3$ achieves 16.8 mm and 22.0 mm per-vertex-error (PVE) on the SURREAL\nand CAPE datasets, respectively; outperforming existing methods that use\nfull-body point clouds as input. We obtain a competitive 70.9 PVE on the BEHAVE\ndataset, outperforming a recently published RGB based method by 18.4 mm,\nhighlighting the usefulness of depth data. Code will be released.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aM$^3$\uff08Masked Mesh Modeling\uff09\u7684\u65b9\u6cd5\uff0c\u5229\u7528RGBD\u76f8\u673a\u8fdb\u884c3D\u4eba\u4f53\u7f51\u683c\u4f30\u8ba1\u3002\u901a\u8fc7\u4f7f\u7528\u63a9\u7801\u81ea\u7f16\u7801\u5668\u548c\u8fd0\u52a8\u6355\u6349\u6570\u636e\u6765\u514b\u670d\u6570\u636e\u7a00\u7f3a\u95ee\u9898\uff0cM$^3$\u80fd\u591f\u4ece\u5355\u89c6\u89d2RGBD\u6570\u636e\u4e2d\u6062\u590d\u5b8c\u6574\u76843D\u4eba\u4f53\u7f51\u683c\uff0c\u5e76\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "motivation": "\u5c3d\u7ba1\u5728\u4eceRGB\u56fe\u50cf\u4f30\u8ba13D\u4eba\u4f53\u7f51\u683c\u65b9\u9762\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46RGBD\u76f8\u673a\u63d0\u4f9b\u7684\u989d\u5916\u6df1\u5ea6\u6570\u636e\u4ecd\u672a\u88ab\u5145\u5206\u5229\u7528\u3002\u672c\u6587\u65e8\u5728\u5229\u7528RGBD\u76f8\u673a\u8fdb\u884c\u51c6\u786e\u76843D\u4eba\u4f53\u7f51\u683c\u4f30\u8ba1\uff0c\u4ee5\u6ee1\u8db3\u73b0\u5b9e\u4e16\u754c\u5e94\u7528\u7684\u9700\u6c42\u3002\u7136\u800c\uff0c\u5168\u76d1\u7763\u65b9\u6cd5\u9700\u8981\u6210\u672c\u9ad8\u6602\u4e14\u96be\u4ee5\u6536\u96c6\u7684RGBD\u56fe\u50cf\u548c3D\u7f51\u683c\u6807\u7b7e\u5bf9\u6570\u636e\u96c6\u3002\u4e3a\u4e86\u89e3\u51b3\u6570\u636e\u7a00\u758f\u6027\u95ee\u9898\uff0c\u672c\u7814\u7a76\u5229\u7528\u73b0\u6709\u7684\u8fd0\u52a8\u6355\u6349\uff08MoCap\uff09\u6570\u636e\u96c6\u3002", "method": "M$^3$\uff08Masked Mesh Modeling\uff09\u9996\u5148\u4ece\u8fd0\u52a8\u6355\u6349\uff08MoCap\uff09\u6570\u636e\u96c6\u4e2d\u63d0\u53d6\u5b8c\u6574\u76843D\u7f51\u683c\uff0c\u5e76\u901a\u8fc7\u6295\u5f71\u5230\u865a\u62df\u6444\u50cf\u673a\u6765\u521b\u5efa\u90e8\u5206\u3001\u5355\u89c6\u89d2\u7248\u672c\uff0c\u6a21\u62dfRGBD\u76f8\u673a\u7684\u6df1\u5ea6\u6570\u636e\u3002\u7136\u540e\uff0c\u8bad\u7ec3\u4e00\u4e2a\u63a9\u7801\u81ea\u7f16\u7801\u5668\u6765\u8865\u5168\u8fd9\u4e9b\u90e8\u5206\u7f51\u683c\u3002\u5728\u63a8\u7406\u65f6\uff0c\u8be5\u65b9\u6cd5\u5c06\u4f20\u611f\u5668\u63d0\u4f9b\u7684\u6df1\u5ea6\u503c\u4e0e\u6a21\u677f\u4eba\u4f53\u7f51\u683c\u7684\u9876\u70b9\u8fdb\u884c\u5339\u914d\uff0c\u521b\u5efa\u90e8\u5206\u7f51\u683c\uff0c\u5e76\u6062\u590d\u4e0d\u53ef\u89c1\u90e8\u5206\uff0c\u4ece\u800c\u751f\u6210\u5b8c\u6574\u7684\u8eab\u4f53\u7f51\u683c\u3002", "result": "M$^3$\u5728SURREAL\u548cCAPE\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8616.8\u6beb\u7c73\u548c22.0\u6beb\u7c73\u7684\u6bcf\u9876\u70b9\u8bef\u5dee\uff08PVE\uff09\uff0c\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002\u5728BEHAVE\u6570\u636e\u96c6\u4e0a\uff0cM$^3$\u83b7\u5f97\u4e8670.9\u6beb\u7c73\u7684PVE\uff0c\u4f18\u4e8e\u6700\u8fd1\u53d1\u5e03\u7684\u57fa\u4e8eRGB\u7684\u65b9\u6cd518.4\u6beb\u7c73\u3002", "conclusion": "M$^3$\u5728SURREAL\u548cCAPE\u6570\u636e\u96c6\u4e0a\u5206\u522b\u5b9e\u73b0\u4e8616.8\u6beb\u7c73\u548c22.0\u6beb\u7c73\u7684\u6bcf\u9876\u70b9\u8bef\u5dee\uff08PVE\uff09\uff0c\u4f18\u4e8e\u4f7f\u7528\u5168\u8eab\u70b9\u4e91\u4f5c\u4e3a\u8f93\u5165\u7684\u73b0\u6709\u65b9\u6cd5\u3002\u5728BEHAVE\u6570\u636e\u96c6\u4e0a\uff0cM$^3$\u83b7\u5f97\u4e8670.9\u6beb\u7c73\u7684PVE\uff0c\u4f18\u4e8e\u6700\u8fd1\u53d1\u5e03\u7684\u57fa\u4e8eRGB\u7684\u65b9\u6cd518.4\u6beb\u7c73\uff0c\u8bc1\u660e\u4e86\u6df1\u5ea6\u6570\u636e\u7684\u6709\u6548\u6027\u3002"}}
{"id": "2508.08179", "categories": ["cs.CV", "cs.MM"], "pdf": "https://arxiv.org/pdf/2508.08179", "abs": "https://arxiv.org/abs/2508.08179", "authors": ["Sihan Zhao", "Zixuan Wang", "Tianyu Luan", "Jia Jia", "Wentao Zhu", "Jiebo Luo", "Junsong Yuan", "Nan Xi"], "title": "PP-Motion: Physical-Perceptual Fidelity Evaluation for Human Motion Generation", "comment": "Accepted by ACM Multimedia 2025", "summary": "Human motion generation has found widespread applications in AR/VR, film,\nsports, and medical rehabilitation, offering a cost-effective alternative to\ntraditional motion capture systems. However, evaluating the fidelity of such\ngenerated motions is a crucial, multifaceted task. Although previous approaches\nhave attempted at motion fidelity evaluation using human perception or physical\nconstraints, there remains an inherent gap between human-perceived fidelity and\nphysical feasibility. Moreover, the subjective and coarse binary labeling of\nhuman perception further undermines the development of a robust data-driven\nmetric. We address these issues by introducing a physical labeling method. This\nmethod evaluates motion fidelity by calculating the minimum modifications\nneeded for a motion to align with physical laws. With this approach, we are\nable to produce fine-grained, continuous physical alignment annotations that\nserve as objective ground truth. With these annotations, we propose PP-Motion,\na novel data-driven metric to evaluate both physical and perceptual fidelity of\nhuman motion. To effectively capture underlying physical priors, we employ\nPearson's correlation loss for the training of our metric. Additionally, by\nincorporating a human-based perceptual fidelity loss, our metric can capture\nfidelity that simultaneously considers both human perception and physical\nalignment. Experimental results demonstrate that our metric, PP-Motion, not\nonly aligns with physical laws but also aligns better with human perception of\nmotion fidelity than previous work.", "AI": {"tldr": "\u672c\u7814\u7a76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u52a8\u4f5c\u4fdd\u771f\u5ea6\u8bc4\u4f30\u65b9\u6cd5 PP-Motion\uff0c\u5b83\u7ed3\u5408\u4e86\u7269\u7406\u6807\u6ce8\u548c\u6570\u636e\u9a71\u52a8\u6307\u6807\uff0c\u80fd\u591f\u540c\u65f6\u8003\u8651\u7269\u7406\u89c4\u5f8b\u548c\u4eba\u7c7b\u611f\u77e5\uff0c\u89e3\u51b3\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u4e0d\u8db3\u3002", "motivation": "\u5148\u524d\u7684\u4eba\u7c7b\u52a8\u4f5c\u751f\u6210\u8bc4\u4f30\u65b9\u6cd5\u5728\u4eba\u7c7b\u611f\u77e5\u4fdd\u771f\u5ea6\u548c\u7269\u7406\u53ef\u884c\u6027\u4e4b\u95f4\u5b58\u5728\u5dee\u8ddd\uff0c\u5e76\u4e14\u4eba\u7c7b\u611f\u77e5\u7684\u6807\u7b7e\u5177\u6709\u4e3b\u89c2\u6027\u548c\u7c97\u7cd9\u6027\uff0c\u963b\u788d\u4e86\u6570\u636e\u9a71\u52a8\u6307\u6807\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7269\u7406\u6807\u6ce8\u65b9\u6cd5\uff0c\u901a\u8fc7\u8ba1\u7b97\u8fd0\u52a8\u4e0e\u7269\u7406\u5b9a\u5f8b\u5bf9\u9f50\u6240\u9700\u7684\u6700\u5c0f\u4fee\u6539\u6765\u8bc4\u4f30\u8fd0\u52a8\u4fdd\u771f\u5ea6\u3002\u5229\u7528\u6b64\u65b9\u6cd5\u751f\u6210\u7684\u6807\u6ce8\u662f\u7ec6\u7c92\u5ea6\u7684\u3001\u8fde\u7eed\u7684\u7269\u7406\u5bf9\u9f50\u6ce8\u91ca\uff0c\u53ef\u4f5c\u4e3a\u5ba2\u89c2\u4f9d\u636e\u3002\u5728\u6b64\u57fa\u7840\u4e0a\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a PP-Motion \u7684\u65b0\u9896\u7684\u6570\u636e\u9a71\u52a8\u6307\u6807\uff0c\u7528\u4e8e\u8bc4\u4f30\u4eba\u7c7b\u52a8\u4f5c\u7684\u7269\u7406\u548c\u611f\u77e5\u4fdd\u771f\u5ea6\u3002PP-Motion \u5728\u8bad\u7ec3\u4e2d\u91c7\u7528\u4e86\u76ae\u5c14\u900a\u76f8\u5173\u635f\u5931\u6765\u6355\u6349\u6f5c\u5728\u7684\u7269\u7406\u5148\u9a8c\uff0c\u5e76\u901a\u8fc7\u5f15\u5165\u57fa\u4e8e\u4eba\u7c7b\u611f\u77e5\u7684\u4fdd\u771f\u5ea6\u635f\u5931\u6765\u540c\u65f6\u8003\u8651\u4eba\u7c7b\u611f\u77e5\u548c\u7269\u7406\u5bf9\u9f50\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0cPP-Motion \u7b97\u6cd5\u4e0d\u4ec5\u7b26\u5408\u7269\u7406\u5b9a\u5f8b\uff0c\u800c\u4e14\u6bd4\u4ee5\u5f80\u7684\u5de5\u4f5c\u66f4\u80fd\u4e0e\u4eba\u7c7b\u5bf9\u52a8\u4f5c\u4fdd\u771f\u5ea6\u7684\u611f\u77e5\u76f8\u5339\u914d\u3002", "conclusion": "PP-Motion \u7b97\u6cd5\u4e0d\u4ec5\u7b26\u5408\u7269\u7406\u5b9a\u5f8b\uff0c\u800c\u4e14\u6bd4\u4ee5\u5f80\u7684\u5de5\u4f5c\u66f4\u80fd\u4e0e\u4eba\u7c7b\u5bf9\u52a8\u4f5c\u4fdd\u771f\u5ea6\u7684\u611f\u77e5\u76f8\u5339\u914d\u3002"}}
{"id": "2508.08180", "categories": ["eess.IV", "cs.AI", "cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08180", "abs": "https://arxiv.org/abs/2508.08180", "authors": ["Luca Zedda", "Andrea Loddo", "Cecilia Di Ruberto", "Carsten Marr"], "title": "RedDino: A foundation model for red blood cell analysis", "comment": null, "summary": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc", "AI": {"tldr": "RedDino\u662f\u4e00\u4e2a\u9488\u5bf9\u7ea2\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u7684\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\uff0c\u5728\u7ea2\u7ec6\u80de\u5f62\u72b6\u5206\u7c7b\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u5e76\u5177\u6709\u826f\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "motivation": "\u65e8\u5728\u4e3a\u7ea2\u7ec6\u80de\u5206\u6790\u63d0\u4f9b\u5168\u9762\u7684AI\u89e3\u51b3\u65b9\u6848\uff0c\u4ee5\u5e94\u5bf9\u76ee\u524d\u76f8\u5173AI\u65b9\u6848\u7684\u7f3a\u4e4f\u3002", "method": "RedDino\u662f\u4e00\u4e2a\u5229\u7528DINOv2\u81ea\u76d1\u7763\u5b66\u4e60\u6846\u67b6\u5e76\u9488\u5bf9\u7ea2\u7ec6\u80de\u56fe\u50cf\u5206\u6790\u8fdb\u884c\u4f18\u5316\u7684\u6a21\u578b\uff0c\u5728125\u4e07\u5f20\u7ea2\u7ec6\u80de\u56fe\u50cf\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u8bad\u7ec3\u3002", "result": "RedDino\u5728\u7ea2\u7ec6\u80de\u5f62\u72b6\u5206\u7c7b\u4efb\u52a1\u4e0a\u8d85\u8d8a\u4e86\u73b0\u6709\u7684\u6700\u5148\u8fdb\u6a21\u578b\uff0c\u5e76\u5c55\u73b0\u51fa\u5f3a\u5927\u7684\u7279\u5f81\u8868\u793a\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "RedDino\u901a\u8fc7\u6355\u6349\u7ec6\u5fae\u7684\u5f62\u6001\u7279\u5f81\uff0c\u89e3\u51b3\u4e86\u8ba1\u7b97\u8840\u6db2\u5b66\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u63a8\u52a8\u4e86\u53ef\u9760\u8bca\u65ad\u5de5\u5177\u7684\u53d1\u5c55\u3002"}}
{"id": "2508.08183", "categories": ["cs.CV", "eess.IV"], "pdf": "https://arxiv.org/pdf/2508.08183", "abs": "https://arxiv.org/abs/2508.08183", "authors": ["Hongkun Jin", "Hongcheng Jiang", "Zejun Zhang", "Yuan Zhang", "Jia Fu", "Tingfeng Li", "Kai Luo"], "title": "THAT: Token-wise High-frequency Augmentation Transformer for Hyperspectral Pansharpening", "comment": "Accepted to 2025 IEEE International Conference on Systems, Man, and\n  Cybernetics (SMC)", "summary": "Transformer-based methods have demonstrated strong potential in hyperspectral\npansharpening by modeling long-range dependencies. However, their effectiveness\nis often limited by redundant token representations and a lack of multi-scale\nfeature modeling. Hyperspectral images exhibit intrinsic spectral priors (e.g.,\nabundance sparsity) and spatial priors (e.g., non-local similarity), which are\ncritical for accurate reconstruction. From a spectral-spatial perspective,\nVision Transformers (ViTs) face two major limitations: they struggle to\npreserve high-frequency components--such as material edges and texture\ntransitions--and suffer from attention dispersion across redundant tokens.\nThese issues stem from the global self-attention mechanism, which tends to\ndilute high-frequency signals and overlook localized details. To address these\nchallenges, we propose the Token-wise High-frequency Augmentation Transformer\n(THAT), a novel framework designed to enhance hyperspectral pansharpening\nthrough improved high-frequency feature representation and token selection.\nSpecifically, THAT introduces: (1) Pivotal Token Selective Attention (PTSA) to\nprioritize informative tokens and suppress redundancy; (2) a Multi-level\nVariance-aware Feed-forward Network (MVFN) to enhance high-frequency detail\nlearning. Experiments on standard benchmarks show that THAT achieves\nstate-of-the-art performance with improved reconstruction quality and\nefficiency. The source code is available at https://github.com/kailuo93/THAT.", "AI": {"tldr": "THAT\u6846\u67b6\u901a\u8fc7PTSA\u9009\u62e9\u6027\u6ce8\u610f\u529b\u548cMVFN\u591a\u7ea7\u65b9\u5dee\u611f\u77e5\u7f51\u7edc\uff0c\u89e3\u51b3\u4e86ViT\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4e2d\u5b58\u5728\u7684\u5197\u4f59token\u548c\u9ad8\u9891\u7ec6\u8282\u5efa\u6a21\u4e0d\u8db3\u7684\u95ee\u9898\uff0c\u8fbe\u5230\u4e86SOTA\u6027\u80fd\u3002", "motivation": "Transformer\u5728\u9ad8\u94c1\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4e2d\u867d\u7136\u80fd\u6a21\u62df\u957f\u8ddd\u79bb\u4f9d\u8d56\uff0c\u4f46\u5b58\u5728\u5197\u4f59 token \u8868\u793a\u548c\u7f3a\u4e4f\u591a\u5c3a\u5ea6\u7279\u5f81\u5efa\u6a21\u7684\u95ee\u9898\u3002ViT\u5728\u5149\u8c31-\u7a7a\u95f4\u65b9\u9762\u5b58\u5728\u4fdd\u7559\u9ad8\u9891\u5206\u91cf\uff08\u5982\u8fb9\u7f18\u548c\u7eb9\u7406\uff09\u7684\u4e0d\u8db3\uff0c\u4ee5\u53ca\u6ce8\u610f\u529b\u5206\u6563\u4e8e\u5197\u4f59 token \u7684\u95ee\u9898\u3002\u8fd9\u4e9b\u95ee\u9898\u6e90\u4e8e\u5168\u5c40\u81ea\u6ce8\u610f\u529b\u673a\u5236\u4f1a\u7a00\u91ca\u9ad8\u9891\u4fe1\u53f7\u5e76\u5ffd\u7565\u5c40\u90e8\u7ec6\u8282\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aTHAT\uff08Token-wise High-frequency Augmentation Transformer\uff09\u7684\u65b0\u578b\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5305\u542b\u4e24\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u67a2\u8f74 token \u9009\u62e9\u6027\u6ce8\u610f\u529b\uff08PTSA\uff09\uff0c\u7528\u4e8e\u4f18\u5148\u9009\u62e9\u4fe1\u606f token \u5e76\u51cf\u5c11\u5197\u4f59\uff1b2\uff09\u591a\u7ea7\u65b9\u5dee\u611f\u77e5\u524d\u9988\u7f51\u7edc\uff08MVFN\uff09\uff0c\u7528\u4e8e\u589e\u5f3a\u9ad8\u9891\u7ec6\u8282\u7684\u5b66\u4e60\u3002", "result": "THAT\u6846\u67b6\u5728\u6807\u51c6\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u4e86\u5b9e\u9a8c\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002", "conclusion": "THAT\u6846\u67b6\u901a\u8fc7\u4f18\u5148\u8003\u8651\u4fe1\u606f\u6027 token \u5e76\u6291\u5236\u5197\u4f59\uff08PTSA\uff09\uff0c\u4ee5\u53ca\u589e\u5f3a\u9ad8\u9891\u7ec6\u8282\u5b66\u4e60\uff08MVFN\uff09\uff0c\u5728\u9ad8\u5149\u8c31\u56fe\u50cf\u878d\u5408\u4efb\u52a1\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u63d0\u9ad8\u4e86\u91cd\u5efa\u8d28\u91cf\u548c\u6548\u7387\u3002"}}
{"id": "2508.08186", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08186", "abs": "https://arxiv.org/abs/2508.08186", "authors": ["Md Meftahul Ferdaus", "Mahdi Abdelguerfi", "Elias Ioup", "Steven Sloan", "Kendall N. Niles", "Ken Pathak"], "title": "KARMA: Efficient Structural Defect Segmentation via Kolmogorov-Arnold Representation Learning", "comment": "submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "summary": "Semantic segmentation of structural defects in civil infrastructure remains\nchallenging due to variable defect appearances, harsh imaging conditions, and\nsignificant class imbalance. Current deep learning methods, despite their\neffectiveness, typically require millions of parameters, rendering them\nimpractical for real-time inspection systems. We introduce KARMA\n(Kolmogorov-Arnold Representation Mapping Architecture), a highly efficient\nsemantic segmentation framework that models complex defect patterns through\ncompositions of one-dimensional functions rather than conventional\nconvolutions. KARMA features three technical innovations: (1) a\nparameter-efficient Tiny Kolmogorov-Arnold Network (TiKAN) module leveraging\nlow-rank factorization for KAN-based feature transformation; (2) an optimized\nfeature pyramid structure with separable convolutions for multi-scale defect\nanalysis; and (3) a static-dynamic prototype mechanism that enhances feature\nrepresentation for imbalanced classes. Extensive experiments on benchmark\ninfrastructure inspection datasets demonstrate that KARMA achieves competitive\nor superior mean IoU performance compared to state-of-the-art approaches, while\nusing significantly fewer parameters (0.959M vs. 31.04M, a 97% reduction).\nOperating at 0.264 GFLOPS, KARMA maintains inference speeds suitable for\nreal-time deployment, enabling practical automated infrastructure inspection\nsystems without compromising accuracy. The source code can be accessed at the\nfollowing URL: https://github.com/faeyelab/karma.", "AI": {"tldr": "KARMA\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u4f7f\u7528\u4e00\u7ef4\u51fd\u6570\u548cTiKAN\u6a21\u5757\uff0c\u5927\u5927\u51cf\u5c11\u4e86\u53c2\u6570\u91cf\uff0897%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u9ad8\u7cbe\u5ea6\uff0c\u5e76\u80fd\u5728\u5b9e\u65f6\u5e94\u7528\u4e2d\u8fd0\u884c\uff0c\u975e\u5e38\u9002\u5408\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u3002", "motivation": " \u0938\u093f\u0935\u093f\u0932 \u092c\u0941\u0928\u093f\u092f\u093e\u0926\u0940 \u0922\u093e\u0902\u091a\u0947 \u092e\u0947\u0902 \u0938\u0902\u0930\u091a\u0928\u093e\u0924\u094d\u092e\u0915 \u0926\u094b\u0937\u094b\u0902 \u0915\u0947 \u0938\u093f\u092e\u0947\u0902\u091f\u093f\u0915 \u0938\u0947\u0917\u092e\u0947\u0902\u091f\u0947\u0936\u0928, \u091c\u094b \u0915\u093f \u0926\u094b\u0937\u094b\u0902 \u0915\u0940 \u0935\u093f\u0935\u093f\u0927 \u0926\u093f\u0916\u093e\u0935\u091f, \u0915\u0920\u094b\u0930 \u0907\u092e\u0947\u091c\u093f\u0902\u0917 \u0938\u094d\u0925\u093f\u0924\u093f\u092f\u093e\u0902 \u0914\u0930 \u092e\u0939\u0924\u094d\u0935\u092a\u0942\u0930\u094d\u0923 \u0935\u0930\u094d\u0917 \u0905\u0938\u0902\u0924\u0941\u0932\u0928 \u0915\u0947 \u0915\u093e\u0930\u0923 \u091a\u0941\u0928\u094c\u0924\u0940\u092a\u0942\u0930\u094d\u0923 \u0930\u0939\u0924\u093e \u0939\u0948\u0964 \u0935\u0930\u094d\u0924\u092e\u093e\u0928 \u0917\u0939\u0928 \u0936\u093f\u0915\u094d\u0937\u0923 \u0935\u093f\u0927\u093f\u092f\u094b\u0902, \u0909\u0928\u0915\u0940 \u092a\u094d\u0930\u092d\u093e\u0935\u0936\u0940\u0932\u0924\u093e \u0915\u0947 \u092c\u093e\u0935\u091c\u0942\u0926, \u0906\u092e\u0924\u094c\u0930 \u092a\u0930 \u0932\u093e\u0916\u094b\u0902 \u092a\u0948\u0930\u093e\u092e\u0940\u091f\u0930 \u0915\u0940 \u0906\u0935\u0936\u094d\u092f\u0915\u0924\u093e \u0939\u094b\u0924\u0940 \u0939\u0948, \u091c\u094b \u0909\u0928\u094d\u0939\u0947\u0902 \u0935\u093e\u0938\u094d\u0924\u0935\u093f\u0915 \u0938\u092e\u092f \u0928\u093f\u0930\u0940\u0915\u094d\u0937\u0923 \u092a\u094d\u0930\u0923\u093e\u0932\u093f\u092f\u094b\u0902 \u0915\u0947 \u0932\u093f\u090f \u0905\u0935\u094d\u092f\u093e\u0935\u0939\u093e\u0930\u093f\u0915 \u092c\u0928\u093e\u0924\u0940 \u0939\u0948\u0964", "method": "KARMA\uff08Kolmogorov-Arnold Representation Mapping Architecture\uff09\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u8bed\u4e49\u5206\u5272\u6846\u67b6\uff0c\u5b83\u901a\u8fc7\u7ec4\u5408\u4e00\u7ef4\u51fd\u6570\u800c\u975e\u4f20\u7edf\u7684\u5377\u79ef\u6765\u6a21\u62df\u590d\u6742\u7684\u7f3a\u9677\u6a21\u5f0f\u3002KARMA\u5305\u542b\u4e09\u4e2a\u6280\u672f\u521b\u65b0\uff1a1\uff09\u4e00\u4e2a\u53c2\u6570\u9ad8\u6548\u7684Tiny Kolmogorov-Arnold Network (TiKAN) \u6a21\u5757\uff0c\u5229\u7528\u4f4e\u79e9\u5206\u89e3\u8fdb\u884c\u57fa\u4e8eKAN\u7684\u7279\u5f81\u8f6c\u6362\uff1b2\uff09\u4e00\u4e2a\u4f18\u5316\u7684\u7279\u5f81\u91d1\u5b57\u5854\u7ed3\u6784\uff0c\u91c7\u7528\u53ef\u5206\u79bb\u5377\u79ef\u8fdb\u884c\u591a\u5c3a\u5ea6\u7f3a\u9677\u5206\u6790\uff1b3\uff09\u4e00\u4e2a\u9759\u6001-\u52a8\u6001\u539f\u578b\u673a\u5236\uff0c\u589e\u5f3a\u4e86\u5bf9\u4e0d\u5e73\u8861\u7c7b\u522b\u7684\u7279\u5f81\u8868\u793a\u3002", "result": "KARMA\u4f7f\u75280.959M\u53c2\u6570\uff0c\u6bd4\u4f20\u7edf\u65b9\u6cd5\uff0831.04M\uff09\u51cf\u5c1197%\uff0c\u5b9e\u73b0\u4e86\u6bcf\u79d238\u5e27\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u540c\u65f6\u4fdd\u6301\u4e86\u4e0e\u6700\u5148\u8fdb\u65b9\u6cd5\u76f8\u5f53\u7684mIoU\u6027\u80fd\uff0c\u5728\u67d0\u4e9b\u60c5\u51b5\u4e0b\u751a\u81f3\u66f4\u597d\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u5b9e\u65f6\u57fa\u7840\u8bbe\u65bd\u68c0\u67e5\u65b9\u9762\u7684\u53ef\u884c\u6027\u3002", "conclusion": "KARMA\u5728\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u5177\u6709\u7ade\u4e89\u529b\u7684\u6216\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u5e73\u5747\u4ea4\u5e76\u6bd4\uff08mIoU\uff09\u6027\u80fd\uff0c\u540c\u65f6\u53c2\u6570\u91cf\u663e\u8457\u51cf\u5c11\uff080.959M vs 31.04M\uff0c\u51cf\u5c11\u4e8697%\uff09\uff0c\u5e76\u4e14\u8fd0\u884c\u901f\u5ea6\uff080.264 GFLOPS\uff09\u9002\u5408\u5b9e\u65f6\u90e8\u7f72\uff0c\u4ece\u800c\u5728\u4e0d\u635f\u5bb3\u7cbe\u5ea6\u7684\u524d\u63d0\u4e0b\uff0c\u5b9e\u73b0\u4e86\u5b9e\u7528\u7684\u81ea\u52a8\u5316\u57fa\u7840\u8bbe\u65bd\u68c0\u6d4b\u7cfb\u7edf\u3002"}}
{"id": "2508.08189", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08189", "abs": "https://arxiv.org/abs/2508.08189", "authors": ["Weijia Wu", "Chen Gao", "Joya Chen", "Kevin Qinghong Lin", "Qingwei Meng", "Yiming Zhang", "Yuke Qiu", "Hong Zhou", "Mike Zheng Shou"], "title": "Reinforcement Learning in Vision: A Survey", "comment": "22 pages", "summary": "Recent advances at the intersection of reinforcement learning (RL) and visual\nintelligence have enabled agents that not only perceive complex visual scenes\nbut also reason, generate, and act within them. This survey offers a critical\nand up-to-date synthesis of the field. We first formalize visual RL problems\nand trace the evolution of policy-optimization strategies from RLHF to\nverifiable reward paradigms, and from Proximal Policy Optimization to Group\nRelative Policy Optimization. We then organize more than 200 representative\nworks into four thematic pillars: multi-modal large language models, visual\ngeneration, unified model frameworks, and vision-language-action models. For\neach pillar we examine algorithmic design, reward engineering, benchmark\nprogress, and we distill trends such as curriculum-driven training,\npreference-aligned diffusion, and unified reward modeling. Finally, we review\nevaluation protocols spanning set-level fidelity, sample-level preference, and\nstate-level stability, and we identify open challenges that include sample\nefficiency, generalization, and safe deployment. Our goal is to provide\nresearchers and practitioners with a coherent map of the rapidly expanding\nlandscape of visual RL and to highlight promising directions for future\ninquiry. Resources are available at:\nhttps://github.com/weijiawu/Awesome-Visual-Reinforcement-Learning.", "AI": {"tldr": "\u672c\u6587\u5bf9\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff08Visual RL\uff09\u9886\u57df\u8fdb\u884c\u4e86\u5168\u9762\u7684\u56de\u987e\u548c\u5206\u6790\uff0c\u68b3\u7406\u4e86\u8be5\u9886\u57df\u7684\u6f14\u8fdb\u3001\u5173\u952e\u6280\u672f\u548c\u672a\u6765\u53d1\u5c55\u65b9\u5411\u3002", "motivation": "\u672c\u6587\u65e8\u5728\u6279\u5224\u6027\u5730\u3001\u53ca\u65f6\u5730\u7efc\u5408\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff08Visual RL\uff09\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u8be5\u9886\u57df\u878d\u5408\u4e86\u5f3a\u5316\u5b66\u4e60\uff08RL\uff09\u548c\u89c6\u89c9\u667a\u80fd\u3002", "method": "\u672c\u6587\u9996\u5148\u5f62\u5f0f\u5316\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\uff0c\u5e76\u8ffd\u6eaf\u4e86\u4eceRLHF\u5230\u53ef\u9a8c\u8bc1\u5956\u52b1\u8303\u5f0f\uff0c\u4ee5\u53ca\u4eceProximal Policy Optimization\u5230Group Relative Policy Optimization\u7684\u7b56\u7565\u4f18\u5316\u7b56\u7565\u7684\u6f14\u53d8\u3002\u7136\u540e\uff0c\u6587\u7ae0\u5c06200\u591a\u9879\u4ee3\u8868\u6027\u5de5\u4f5c\u7ec4\u7ec7\u6210\u56db\u4e2a\u4e3b\u9898\u652f\u67f1\uff1a\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u3001\u89c6\u89c9\u751f\u6210\u3001\u7edf\u4e00\u6a21\u578b\u6846\u67b6\u548c\u89c6\u89c9-\u8bed\u8a00-\u52a8\u4f5c\u6a21\u578b\u3002\u9488\u5bf9\u6bcf\u4e2a\u652f\u67f1\uff0c\u7814\u7a76\u4e86\u7b97\u6cd5\u8bbe\u8ba1\u3001\u5956\u52b1\u5de5\u7a0b\u3001\u57fa\u51c6\u6d4b\u8bd5\u8fdb\u5c55\uff0c\u5e76\u63d0\u70bc\u4e86\u8bfe\u7a0b\u9a71\u52a8\u8bad\u7ec3\u3001\u504f\u597d\u5bf9\u9f50\u6269\u6563\u548c\u7edf\u4e00\u5956\u52b1\u5efa\u6a21\u7b49\u8d8b\u52bf\u3002\u6700\u540e\uff0c\u56de\u987e\u4e86\u8de8\u8d8a\u96c6\u5408\u7ea7\u4fdd\u771f\u5ea6\u3001\u6837\u672c\u7ea7\u504f\u597d\u548c\u72b6\u6001\u7ea7\u7a33\u5b9a\u6027\u7684\u8bc4\u4f30\u534f\u8bae\uff0c\u5e76\u786e\u5b9a\u4e86\u5305\u62ec\u6837\u672c\u6548\u7387\u3001\u6cdb\u5316\u548c\u5b89\u5168\u90e8\u7f72\u5728\u5185\u7684\u5f00\u653e\u6027\u6311\u6218\u3002", "result": "\u672c\u6587\u5168\u9762\u6982\u8ff0\u4e86\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\u9886\u57df\u7684\u6700\u65b0\u8fdb\u5c55\uff0c\u6db5\u76d6\u4e86\u4ece\u7b56\u7565\u4f18\u5316\u5230\u5177\u4f53\u6a21\u578b\u6846\u67b6\u7684\u5404\u4e2a\u65b9\u9762\uff0c\u5e76\u5bf9\u8bc4\u4f30\u534f\u8bae\u548c\u672a\u6765\u6311\u6218\u8fdb\u884c\u4e86\u8ba8\u8bba\u3002", "conclusion": "\u672c\u7bc7\u8bba\u6587\u65e8\u5728\u4e3a\u7814\u7a76\u4eba\u5458\u548c\u5b9e\u8df5\u8005\u63d0\u4f9b\u4e00\u5e45\u5173\u4e8e\u89c6\u89c9\u5f3a\u5316\u5b66\u4e60\uff08Visual RL\uff09\u5feb\u901f\u6269\u5c55\u9886\u57df\u7684\u8fde\u8d2f\u5730\u56fe\uff0c\u5e76\u7a81\u51fa\u672a\u6765\u6709\u5e0c\u671b\u7684\u7814\u7a76\u65b9\u5411\u3002"}}
{"id": "2508.08199", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08199", "abs": "https://arxiv.org/abs/2508.08199", "authors": ["Peiqi He", "Zhenhao Zhang", "Yixiang Zhang", "Xiongjun Zhao", "Shaoliang Peng"], "title": "Spatial-ORMLLM: Improve Spatial Relation Understanding in the Operating Room with Multimodal Large Language Model", "comment": null, "summary": "Precise spatial modeling in the operating room (OR) is foundational to many\nclinical tasks, supporting intraoperative awareness, hazard avoidance, and\nsurgical decision-making. While existing approaches leverage large-scale\nmultimodal datasets for latent-space alignment to implicitly learn spatial\nrelationships, they overlook the 3D capabilities of MLLMs. However, this\napproach raises two issues: (1) Operating rooms typically lack multiple video\nand audio sensors, making multimodal 3D data difficult to obtain; (2) Training\nsolely on readily available 2D data fails to capture fine-grained details in\ncomplex scenes. To address this gap, we introduce Spatial-ORMLLM, the first\nlarge vision-language model for 3D spatial reasoning in operating rooms using\nonly RGB modality to infer volumetric and semantic cues, enabling downstream\nmedical tasks with detailed and holistic spatial context. Spatial-ORMLLM\nincorporates a Spatial-Enhanced Feature Fusion Block, which integrates 2D\nmodality inputs with rich 3D spatial knowledge extracted by the estimation\nalgorithm and then feeds the combined features into the visual tower. By\nemploying a unified end-to-end MLLM framework, it combines powerful spatial\nfeatures with textual features to deliver robust 3D scene reasoning without any\nadditional expert annotations or sensor inputs. Experiments on multiple\nbenchmark clinical datasets demonstrate that Spatial-ORMLLM achieves\nstate-of-the-art performance and generalizes robustly to previously unseen\nsurgical scenarios and downstream tasks.", "AI": {"tldr": "Spatial-ORMLLM\u662f\u7b2c\u4e00\u4e2a\u7528\u4e8e\u624b\u672f\u5ba43D\u7a7a\u95f4\u63a8\u7406\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff0c\u4ec5\u4f7f\u7528RGB\u6a21\u6001\u5373\u53ef\u63a8\u65ad\u4f53\u79ef\u548c\u8bed\u4e49\u7ebf\u7d22\uff0c\u4ece\u800c\u4e3a\u4e0b\u6e38\u533b\u7597\u4efb\u52a1\u63d0\u4f9b\u8be6\u7ec6\u548c\u5168\u9762\u7684\u7a7a\u95f4\u80cc\u666f\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u4e3b\u8981\u4f9d\u8d56\u591a\u6a21\u60013D\u6570\u636e\u8fdb\u884c\u6f5c\u5728\u7a7a\u95f4\u5bf9\u9f50\uff0c\u4f46\u5ffd\u7565\u4e86MLLM\u76843D\u80fd\u529b\u3002\u7136\u800c\uff0c\u591a\u6a21\u60013D\u6570\u636e\u7684\u83b7\u53d6\u5b58\u5728\u56f0\u96be\uff0c\u5e76\u4e14\u4ec5\u57282D\u6570\u636e\u4e0a\u8bad\u7ec3\u65e0\u6cd5\u6355\u6349\u590d\u6742\u573a\u666f\u4e2d\u7684\u7ec6\u8282\u3002\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u4e86Spatial-ORMLLM\u3002", "method": "Spatial-ORMLLM\u662f\u4e00\u4e2a\u5229\u7528RGB\u6a21\u6001\u63a8\u65ad\u4f53\u79ef\u548c\u8bed\u4e49\u7ebf\u7d22\u76843D\u7a7a\u95f4\u63a8\u7406\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u3002\u5b83\u5305\u542b\u4e00\u4e2a\u7a7a\u95f4\u589e\u5f3a\u7279\u5f81\u878d\u5408\u5757\uff0c\u5c062D\u6a21\u6001\u8f93\u5165\u4e0e\u4ece\u4f30\u8ba1\u7b97\u6cd5\u4e2d\u63d0\u53d6\u7684\u4e30\u5bcc3D\u7a7a\u95f4\u77e5\u8bc6\u76f8\u7ed3\u5408\uff0c\u7136\u540e\u5c06\u7ec4\u5408\u540e\u7684\u7279\u5f81\u8f93\u5165\u89c6\u89c9\u5854\u3002\u901a\u8fc7\u91c7\u7528\u7edf\u4e00\u7684\u7aef\u5230\u7aefMLLM\u6846\u67b6\uff0c\u5b83\u5c06\u5f3a\u5927\u7684\u7a7a\u95f4\u7279\u5f81\u4e0e\u6587\u672c\u7279\u5f81\u76f8\u7ed3\u5408\uff0c\u65e0\u9700\u4efb\u4f55\u989d\u5916\u7684\u4e13\u5bb6\u6ce8\u91ca\u6216\u4f20\u611f\u5668\u8f93\u5165\u5373\u53ef\u5b9e\u73b0\u5f3a\u5927\u76843D\u573a\u666f\u63a8\u7406\u3002", "result": "Spatial-ORMLLM\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u80fd\u591f\u9c81\u68d2\u5730\u6cdb\u5316\u5230\u4ee5\u524d\u672a\u77e5\u7684 surgical \u573a\u666f\u548c\u4e0b\u6e38\u4efb\u52a1\u3002", "conclusion": "Spatial-ORMLLM\u5728\u591a\u4e2a\u4e34\u5e8a\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u80fd\u5f88\u597d\u5730\u6cdb\u5316\u5230\u4ee5\u524d\u672a\u89c1\u8fc7\u7684\u624b\u672f\u573a\u666f\u548c\u4e0b\u6e38\u4efb\u52a1\u3002"}}
{"id": "2508.08219", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08219", "abs": "https://arxiv.org/abs/2508.08219", "authors": ["Wentao Sun", "Quanyun Wu", "Hanqing Xu", "Kyle Gao", "Zhengsen Xu", "Yiping Chen", "Dedong Zhang", "Lingfei Ma", "John S. Zelek", "Jonathan Li"], "title": "SAGOnline: Segment Any Gaussians Online", "comment": "19 pages, 10 figures", "summary": "3D Gaussian Splatting (3DGS) has emerged as a powerful paradigm for explicit\n3D scene representation, yet achieving efficient and consistent 3D segmentation\nremains challenging. Current methods suffer from prohibitive computational\ncosts, limited 3D spatial reasoning, and an inability to track multiple objects\nsimultaneously. We present Segment Any Gaussians Online (SAGOnline), a\nlightweight and zero-shot framework for real-time 3D segmentation in Gaussian\nscenes that addresses these limitations through two key innovations: (1) a\ndecoupled strategy that integrates video foundation models (e.g., SAM2) for\nview-consistent 2D mask propagation across synthesized views; and (2) a\nGPU-accelerated 3D mask generation and Gaussian-level instance labeling\nalgorithm that assigns unique identifiers to 3D primitives, enabling lossless\nmulti-object tracking and segmentation across views. SAGOnline achieves\nstate-of-the-art performance on NVOS (92.7% mIoU) and Spin-NeRF (95.2% mIoU)\nbenchmarks, outperforming Feature3DGS, OmniSeg3D-gs, and SA3D by 15--1500 times\nin inference speed (27 ms/frame). Qualitative results demonstrate robust\nmulti-object segmentation and tracking in complex scenes. Our contributions\ninclude: (i) a lightweight and zero-shot framework for 3D segmentation in\nGaussian scenes, (ii) explicit labeling of Gaussian primitives enabling\nsimultaneous segmentation and tracking, and (iii) the effective adaptation of\n2D video foundation models to the 3D domain. This work allows real-time\nrendering and 3D scene understanding, paving the way for practical AR/VR and\nrobotic applications.", "AI": {"tldr": "SAGOnline\u662f\u4e00\u4e2a\u8f7b\u91cf\u7ea7\u3001\u96f6\u6837\u672c\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u57283D\u9ad8\u65af\u573a\u666f\u4e2d\u8fdb\u884c\u5b9e\u65f6\u5206\u5272\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\uff0c\u901a\u8fc7\u7ed3\u54082D\u89c6\u9891\u57fa\u7840\u6a21\u578b\u548cGPU\u52a0\u901f\u76843D\u63a9\u6a21\u751f\u6210\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u548c\u9ad8\u63a8\u7406\u901f\u5ea6\u3002", "motivation": "\u73b0\u67093D\u5206\u6bb5\u65b9\u6cd5\u5728\u8ba1\u7b97\u6210\u672c\u30013D\u7a7a\u95f4\u63a8\u7406\u548c\u591a\u76ee\u6807\u8ddf\u8e2a\u65b9\u9762\u5b58\u5728\u6311\u6218\uff0cSAGOnline\u65e8\u5728\u89e3\u51b3\u8fd9\u4e9b\u9650\u5236\uff0c\u5b9e\u73b0\u9ad8\u6548\u3001\u4e00\u81f4\u76843D\u5206\u6bb5\u3002", "method": "SAGOnline\u6846\u67b6\u901a\u8fc7\u89e3\u8026\u7b56\u7565\u6574\u5408\u4e86\u89c6\u9891\u57fa\u7840\u6a21\u578b\uff08\u5982SAM2\uff09\u4ee5\u5b9e\u73b0\u8de8\u89c6\u56fe\u76842D\u63a9\u6a21\u4f20\u64ad\uff0c\u5e76\u5229\u7528GPU\u52a0\u901f\u76843D\u63a9\u6a21\u751f\u6210\u548c\u9ad8\u65af\u7ea7\u522b\u5b9e\u4f8b\u6807\u8bb0\u7b97\u6cd5\u6765\u4e3a3D\u56fe\u5143\u5206\u914d\u552f\u4e00\u6807\u8bc6\u7b26\uff0c\u4ece\u800c\u5b9e\u73b0\u4e86\u8de8\u89c6\u56fe\u7684\u65e0\u635f\u591a\u76ee\u6807\u8ddf\u8e2a\u548c\u5206\u5272\u3002", "result": "SAGOnline\u5728NVOS\u548cSpin-NeRF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0cmIoU\u5206\u522b\u4e3a92.7%\u548c95.2%\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4Feature3Dgs\u3001OmniSeg3D-gs\u548cSA3D\u5feb15-1500\u500d\uff0827\u6beb\u79d2/\u5e27\uff09\u3002", "conclusion": "SAGOnline\u6846\u67b6\u80fd\u591f\u5bf9\u9ad8\u65af\u573a\u666f\u8fdb\u884c\u5b9e\u65f63D\u5206\u5272\uff0c\u5e76\u5b9e\u73b0\u4e86\u5bf9\u591a\u4e2a\u7269\u4f53\u7684\u5206\u5272\u548c\u8ddf\u8e2a\uff0c\u5728NVOS\u548cSpin-NeRF\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5e76\u4e14\u63a8\u7406\u901f\u5ea6\u6bd4\u73b0\u6709\u65b9\u6cd5\u5feb15\u52301500\u500d\u3002\u8be5\u6846\u67b6\u8fd8\u901a\u8fc7\u660e\u786e\u6807\u8bb0\u9ad8\u65af\u56fe\u5143\u6709\u6548\u5730\u5b9e\u73b0\u4e862D\u89c6\u9891\u57fa\u7840\u6a21\u578b\u54113D\u9886\u57df\u7684\u9002\u914d\uff0c\u4e3aAR/VR\u548c\u673a\u5668\u4eba\u5e94\u7528\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2508.08220", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08220", "abs": "https://arxiv.org/abs/2508.08220", "authors": ["Wenyi Mo", "Ying Ba", "Tianyu Zhang", "Yalong Bai", "Biye Li"], "title": "Learning User Preferences for Image Generation Model", "comment": null, "summary": "User preference prediction requires a comprehensive and accurate\nunderstanding of individual tastes. This includes both surface-level\nattributes, such as color and style, and deeper content-related aspects, such\nas themes and composition. However, existing methods typically rely on general\nhuman preferences or assume static user profiles, often neglecting individual\nvariability and the dynamic, multifaceted nature of personal taste. To address\nthese limitations, we propose an approach built upon Multimodal Large Language\nModels, introducing contrastive preference loss and preference tokens to learn\npersonalized user preferences from historical interactions. The contrastive\npreference loss is designed to effectively distinguish between user ''likes''\nand ''dislikes'', while the learnable preference tokens capture shared interest\nrepresentations among existing users, enabling the model to activate\ngroup-specific preferences and enhance consistency across similar users.\nExtensive experiments demonstrate our model outperforms other methods in\npreference prediction accuracy, effectively identifying users with similar\naesthetic inclinations and providing more precise guidance for generating\nimages that align with individual tastes. The project page is\n\\texttt{https://learn-user-pref.github.io/}.", "AI": {"tldr": "\u4e00\u4e2a\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u5bf9\u6bd4\u504f\u597d\u635f\u5931\u548c\u504f\u597d\u6807\u8bb0\u6765\u5b66\u4e60\u4e2a\u6027\u5316\u7528\u6237\u504f\u597d\uff0c\u63d0\u9ad8\u4e86\u504f\u597d\u9884\u6d4b\u7684\u51c6\u786e\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u4e8e\u4e00\u822c\u4eba\u7c7b\u504f\u597d\u6216\u5047\u8bbe\u9759\u6001\u7528\u6237\u914d\u7f6e\u6587\u4ef6\uff0c\u5ffd\u7565\u4e86\u4e2a\u4f53\u5dee\u5f02\u548c\u4e2a\u4eba\u54c1\u5473\u52a8\u6001\u3001\u591a\u65b9\u9762\u7684\u6027\u8d28\u3002", "method": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u7684\u65b9\u200b\u200b\u6cd5\uff0c\u5f15\u5165\u5bf9\u6bd4\u504f\u597d\u635f\u5931\u548c\u504f\u597d\u6807\u8bb0\u6765\u5b66\u4e60\u4e2a\u6027\u5316\u7528\u6237\u504f\u597d\u3002", "result": "\u5b9e\u9a8c\u8bc1\u660e\u8be5\u6a21\u578b\u5728\u504f\u597d\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6a21\u578b\u5728\u504f\u597d\u9884\u6d4b\u51c6\u786e\u6027\u65b9\u9762\u4f18\u4e8e\u5176\u4ed6\u65b9\u6cd5\uff0c\u80fd\u6709\u6548\u8bc6\u522b\u5177\u6709\u76f8\u4f3c\u5ba1\u7f8e\u503e\u5411\u7684\u7528\u6237\uff0c\u5e76\u4e3a\u751f\u6210\u7b26\u5408\u4e2a\u4eba\u53e3\u5473\u7684\u56fe\u50cf\u63d0\u4f9b\u66f4\u7cbe\u786e\u7684\u6307\u5bfc\u3002"}}
{"id": "2508.08227", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08227", "abs": "https://arxiv.org/abs/2508.08227", "authors": ["Zhiqiang Wu", "Zhaomang Sun", "Tong Zhou", "Bingtao Fu", "Ji Cong", "Yitong Dong", "Huaqi Zhang", "Xuan Tang", "Mingsong Chen", "Xian Wei"], "title": "OMGSR: You Only Need One Mid-timestep Guidance for Real-World Image Super-Resolution", "comment": null, "summary": "Denoising Diffusion Probabilistic Models (DDPM) and Flow Matching (FM)\ngenerative models show promising potential for one-step Real-World Image\nSuper-Resolution (Real-ISR). Recent one-step Real-ISR models typically inject a\nLow-Quality (LQ) image latent distribution at the initial timestep. However, a\nfundamental gap exists between the LQ image latent distribution and the\nGaussian noisy latent distribution, limiting the effective utilization of\ngenerative priors. We observe that the noisy latent distribution at DDPM/FM\nmid-timesteps aligns more closely with the LQ image latent distribution. Based\non this insight, we present One Mid-timestep Guidance Real-ISR (OMGSR), a\nuniversal framework applicable to DDPM/FM-based generative models. OMGSR\ninjects the LQ image latent distribution at a pre-computed mid-timestep,\nincorporating the proposed Latent Distribution Refinement loss to alleviate the\nlatent distribution gap. We also design the Overlap-Chunked LPIPS/GAN loss to\neliminate checkerboard artifacts in image generation. Within this framework, we\ninstantiate OMGSR for DDPM/FM-based generative models with two variants:\nOMGSR-S (SD-Turbo) and OMGSR-F (FLUX.1-dev). Experimental results demonstrate\nthat OMGSR-S/F achieves balanced/excellent performance across quantitative and\nqualitative metrics at 512-resolution. Notably, OMGSR-F establishes\noverwhelming dominance in all reference metrics. We further train a\n1k-resolution OMGSR-F to match the default resolution of FLUX.1-dev, which\nyields excellent results, especially in the details of the image generation. We\nalso generate 2k-resolution images by the 1k-resolution OMGSR-F using our\ntwo-stage Tiled VAE & Diffusion.", "AI": {"tldr": "OMGSR\u901a\u8fc7\u5728\u4e2d\u95f4\u65f6\u95f4\u6b65\u6ce8\u5165LQ\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\u5e76\u4f18\u5316\u6f5c\u5728\u5206\u5e03\u5dee\u5f02\uff0c\u6539\u8fdb\u4e86DDPM/FM\u5728Real-ISR\u4efb\u52a1\u4e0a\u7684\u6027\u80fd\uff0cOMGSR-F\u8868\u73b0\u5c24\u4e3a\u7a81\u51fa\u3002", "motivation": "\u73b0\u6709\u7684\u5355\u6b65\u771f\u5b9e\u4e16\u754c\u56fe\u50cf\u8d85\u5206\u8fa8\u7387\uff08Real-ISR\uff09\u6a21\u578b\u901a\u5e38\u5728\u521d\u59cb\u65f6\u95f4\u6b65\u6ce8\u5165\u4f4e\u8d28\u91cf\uff08LQ\uff09\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\uff0c\u4f46LQ\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\u4e0e\u9ad8\u65af\u566a\u58f0\u6f5c\u5728\u5206\u5e03\u4e4b\u95f4\u5b58\u5728\u6839\u672c\u6027\u5dee\u8ddd\uff0c\u9650\u5236\u4e86\u751f\u6210\u5148\u9a8c\u7684\u6709\u6548\u5229\u7528\u3002\u7814\u7a76\u8005\u53d1\u73b0DDPM/FM\u4e2d\u95f4\u65f6\u95f4\u6b65\u7684\u566a\u58f0\u6f5c\u5728\u5206\u5e03\u4e0eLQ\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\u66f4\u63a5\u8fd1\uff0c\u4ee5\u6b64\u4e3a\u51fa\u53d1\u70b9\u8fdb\u884c\u7814\u7a76\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aOMGSR\uff08One Mid-timestep Guidance Real-ISR\uff09\u7684\u901a\u7528\u6846\u67b6\uff0c\u9002\u7528\u4e8eDDPM/FM\u751f\u6210\u6a21\u578b\u3002\u8be5\u6846\u67b6\u5728\u9884\u5148\u8ba1\u7b97\u7684\u4e2d\u95f4\u65f6\u95f4\u6b65\u6ce8\u5165\u4f4e\u8d28\u91cf\uff08LQ\uff09\u56fe\u50cf\u6f5c\u5728\u5206\u5e03\uff0c\u5e76\u63d0\u51fa\u6f5c\u5728\u5206\u5e03\u7ec6\u5316\u635f\u5931\u6765\u5f25\u5408\u6f5c\u5728\u5206\u5e03\u7684\u5dee\u5f02\u3002\u540c\u65f6\u8bbe\u8ba1\u4e86\u91cd\u53e0\u5206\u5757LPIPS/GAN\u635f\u5931\u6765\u6d88\u9664\u56fe\u50cf\u751f\u6210\u7684\u68cb\u76d8\u683c\u4f2a\u5f71\u3002OMGSR-S\uff08SD-Turbo\uff09\u548cOMGSR-F\uff08FLUX.1-dev\uff09\u662f\u8be5\u6846\u67b6\u7684\u4e24\u4e2a\u5177\u4f53\u5b9e\u73b0\u3002", "result": "OMGSR-S/F\u5728512\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e86\u5e73\u8861/\u5353\u8d8a\u7684\u6027\u80fd\u3002OMGSR-F\u5728\u6240\u6709\u53c2\u8003\u6307\u6807\u4e0a\u5747\u53d6\u5f97\u4e86\u538b\u5012\u6027\u4f18\u52bf\u30021k\u5206\u8fa8\u7387\u7684OMGSR-F\u5728\u56fe\u50cf\u7ec6\u8282\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u5e76\u4e14\u80fd\u591f\u901a\u8fc7\u4e24\u9636\u6bb5Tiled VAE & Diffusion\u6280\u672f\u751f\u62102k\u5206\u8fa8\u7387\u7684\u56fe\u50cf\u3002", "conclusion": "OMGSR-S/F\u5728512\u5206\u8fa8\u7387\u4e0b\u5b9e\u73b0\u4e86\u5b9a\u91cf\u548c\u5b9a\u6027\u6307\u6807\u7684\u5e73\u8861/\u5353\u8d8a\u6027\u80fd\uff0cOMGSR-F\u5728\u6240\u6709\u53c2\u8003\u6307\u6807\u4e0a\u5747\u5360\u7edd\u5bf9\u4f18\u52bf\u30021k\u5206\u8fa8\u7387\u7684OMGSR-F\u5728\u56fe\u50cf\u7ec6\u8282\u751f\u6210\u65b9\u9762\u8868\u73b0\u5c24\u4e3a\u51fa\u8272\uff0c\u5e76\u80fd\u901a\u8fc7\u4e24\u9636\u6bb5Tiled VAE & Diffusion\u751f\u62102k\u5206\u8fa8\u7387\u56fe\u50cf\u3002"}}
{"id": "2508.08244", "categories": ["cs.CV", "cs.AI"], "pdf": "https://arxiv.org/pdf/2508.08244", "abs": "https://arxiv.org/abs/2508.08244", "authors": ["Jingwen He", "Hongbo Liu", "Jiajun Li", "Ziqi Huang", "Yu Qiao", "Wanli Ouyang", "Ziwei Liu"], "title": "Cut2Next: Generating Next Shot via In-Context Tuning", "comment": null, "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.", "AI": {"tldr": "Error", "motivation": "Error", "method": "Error", "result": "Error", "conclusion": "Error"}}
{"id": "2508.08248", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08248", "abs": "https://arxiv.org/abs/2508.08248", "authors": ["Shuyuan Tu", "Yueming Pan", "Yinming Huang", "Xintong Han", "Zhen Xing", "Qi Dai", "Chong Luo", "Zuxuan Wu", "Yu-Gang Jiang"], "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation", "comment": null, "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.", "AI": {"tldr": "StableAvatar\u901a\u8fc7\u521b\u65b0\u7684\u65f6\u95f4\u6b65\u611f\u77e5\u97f3\u9891\u9002\u914d\u5668\u3001\u97f3\u9891\u539f\u751f\u6307\u5bfc\u673a\u5236\u548c\u52a8\u6001\u52a0\u6743\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\uff0c\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65e0\u9650\u957f\u5ea6\u7684\u97f3\u9891\u9a71\u52a8\u5316\u8eab\u89c6\u9891\u751f\u6210\u3002", "motivation": "\u5f53\u524d\u97f3\u9891\u9a71\u52a8\u7684\u5316\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u5408\u6210\u957f\u89c6\u9891\u65f6\uff0c\u9762\u4e34\u81ea\u7136\u97f3\u9891\u540c\u6b65\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u65b9\u9762\u7684\u6311\u6218\u3002\u73b0\u6709\u6a21\u578b\u4e3b\u8981\u4f9d\u8d56\u7b2c\u4e09\u65b9\u97f3\u9891\u63d0\u53d6\u5668\uff0c\u5e76\u5c06\u97f3\u9891\u5d4c\u5165\u76f4\u63a5\u6ce8\u5165\u6269\u6563\u6a21\u578b\uff0c\u5bfc\u81f4\u6f5c\u5728\u5206\u5e03\u8bef\u5dee\u7d2f\u79ef\u548c\u540e\u7eed\u7247\u6bb5\u7684\u6f5c\u5728\u5206\u5e03\u6f02\u79fb\u3002", "method": "\u672c\u6587\u63d0\u51fa\u4e86StableAvatar\uff0c\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u9891\u6269\u6563Transformer\uff0c\u65e0\u9700\u540e\u671f\u5904\u7406\u5373\u53ef\u751f\u6210\u65e0\u9650\u957f\u5ea6\u7684\u9ad8\u8d28\u91cf\u89c6\u9891\u3002\u8be5\u6a21\u578b\u6574\u5408\u4e86\u5b9a\u5236\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u6a21\u5757\uff0c\u901a\u8fc7\u65f6\u95f4\u6b65\u611f\u77e5\u97f3\u9891\u9002\u914d\u5668\u6765\u9632\u6b62\u8bef\u5dee\u7d2f\u79ef\uff0c\u5229\u7528\u97f3\u9891\u539f\u751f\u6307\u5bfc\u673a\u5236\u589e\u5f3a\u97f3\u9891\u540c\u6b65\u6027\uff0c\u5e76\u901a\u8fc7\u52a8\u6001\u52a0\u6743\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\u878d\u5408\u65f6\u57df\u4e0a\u7684\u6f5c\u5728\u8868\u793a\uff0c\u4ee5\u63d0\u9ad8\u65e0\u9650\u957f\u5ea6\u89c6\u9891\u7684\u5e73\u6ed1\u5ea6\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u5728\u5b9a\u6027\u548c\u5b9a\u91cf\u4e0a\u5747\u8bc1\u660e\u4e86StableAvatar\u7684\u6709\u6548\u6027\uff0c\u80fd\u591f\u751f\u6210\u9ad8\u8d28\u91cf\u3001\u8eab\u4efd\u4e00\u81f4\u4e14\u4e0e\u97f3\u9891\u540c\u6b65\u7684\u65e0\u9650\u957f\u5ea6\u89c6\u9891\u3002", "conclusion": "StableAvatar\u901a\u8fc7\u5176\u521b\u65b0\u7684\u65f6\u95f4\u6b65\u611f\u77e5\u97f3\u9891\u9002\u914d\u5668\u3001\u97f3\u9891\u539f\u751f\u6307\u5bfc\u673a\u5236\u548c\u52a8\u6001\u52a0\u6743\u6ed1\u52a8\u7a97\u53e3\u7b56\u7565\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u73b0\u6709\u97f3\u9891\u9a71\u52a8\u7684\u5316\u8eab\u89c6\u9891\u751f\u6210\u6a21\u578b\u5728\u957f\u89c6\u9891\u5408\u6210\u4e2d\u7684\u540c\u6b65\u6027\u548c\u8eab\u4efd\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u3001\u65e0\u9650\u957f\u5ea6\u7684\u89c6\u9891\u751f\u6210\u3002"}}
{"id": "2508.08252", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08252", "abs": "https://arxiv.org/abs/2508.08252", "authors": ["Shuting He", "Guangquan Jie", "Changshuo Wang", "Yun Zhou", "Shuming Hu", "Guanbin Li", "Henghui Ding"], "title": "ReferSplat: Referring Segmentation in 3D Gaussian Splatting", "comment": "ICML 2025 Oral, Code: https://github.com/heshuting555/ReferSplat", "summary": "We introduce Referring 3D Gaussian Splatting Segmentation (R3DGS), a new task\nthat aims to segment target objects in a 3D Gaussian scene based on natural\nlanguage descriptions, which often contain spatial relationships or object\nattributes. This task requires the model to identify newly described objects\nthat may be occluded or not directly visible in a novel view, posing a\nsignificant challenge for 3D multi-modal understanding. Developing this\ncapability is crucial for advancing embodied AI. To support research in this\narea, we construct the first R3DGS dataset, Ref-LERF. Our analysis reveals that\n3D multi-modal understanding and spatial relationship modeling are key\nchallenges for R3DGS. To address these challenges, we propose ReferSplat, a\nframework that explicitly models 3D Gaussian points with natural language\nexpressions in a spatially aware paradigm. ReferSplat achieves state-of-the-art\nperformance on both the newly proposed R3DGS task and 3D open-vocabulary\nsegmentation benchmarks. Dataset and code are available at\nhttps://github.com/heshuting555/ReferSplat.", "AI": {"tldr": "\u63d0\u51fa R3DGS \u4efb\u52a1\u548c Ref-LERF \u6570\u636e\u96c6\uff0c\u4ee5\u53ca ReferSplat \u6846\u67b6\uff0c\u7528\u4e8e\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5206\u5272 3D \u9ad8\u65af\u573a\u666f\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u5e76\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "motivation": "\u5f15\u5165\u4e86 Referring 3D Gaussian Splatting Segmentation (R3DGS) \u4efb\u52a1\uff0c\u65e8\u5728\u6839\u636e\u81ea\u7136\u8bed\u8a00\u63cf\u8ff0\u5206\u5272 3D \u9ad8\u65af\u573a\u666f\u4e2d\u7684\u76ee\u6807\u5bf9\u8c61\uff0c\u4ee5\u63a8\u52a8\u5177\u8eab AI \u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a ReferSplat \u7684\u6846\u67b6\uff0c\u8be5\u6846\u67b6\u5728\u7a7a\u95f4\u611f\u77e5\u8303\u5f0f\u4e2d\u663e\u5f0f\u5730\u7528\u81ea\u7136\u8bed\u8a00\u8868\u8fbe\u5f0f\u6765\u6a21\u62df 3D \u9ad8\u65af\u70b9\u3002", "result": "\u6240\u63d0\u51fa\u7684 R3DGS \u4efb\u52a1\u548c Ref-LERF \u6570\u636e\u96c6\uff0c\u4ee5\u53ca ReferSplat \u6846\u67b6\u5728 R3DGS \u4efb\u52a1\u548c 3D \u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u4e0a\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002", "conclusion": "ReferSplat \u6846\u67b6\u5728 R3DGS \u4efb\u52a1\u548c 3D \u5f00\u653e\u8bcd\u6c47\u5206\u5272\u57fa\u51c6\u4e0a\u90fd\u53d6\u5f97\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\u3002"}}
{"id": "2508.08254", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2508.08254", "abs": "https://arxiv.org/abs/2508.08254", "authors": ["Emily Yue-Ting Jia", "Jiageng Mao", "Zhiyuan Gao", "Yajie Zhao", "Yue Wang"], "title": "Learning an Implicit Physics Model for Image-based Fluid Simulation", "comment": "Accepted at ICCV 2025", "summary": "Humans possess an exceptional ability to imagine 4D scenes, encompassing both\nmotion and 3D geometry, from a single still image. This ability is rooted in\nour accumulated observations of similar scenes and an intuitive understanding\nof physics. In this paper, we aim to replicate this capacity in neural\nnetworks, specifically focusing on natural fluid imagery. Existing methods for\nthis task typically employ simplistic 2D motion estimators to animate the\nimage, leading to motion predictions that often defy physical principles,\nresulting in unrealistic animations. Our approach introduces a novel method for\ngenerating 4D scenes with physics-consistent animation from a single image. We\npropose the use of a physics-informed neural network that predicts motion for\neach surface point, guided by a loss term derived from fundamental physical\nprinciples, including the Navier-Stokes equations. To capture appearance, we\npredict feature-based 3D Gaussians from the input image and its estimated\ndepth, which are then animated using the predicted motions and rendered from\nany desired camera perspective. Experimental results highlight the\neffectiveness of our method in producing physically plausible animations,\nshowcasing significant performance improvements over existing methods. Our\nproject page is https://physfluid.github.io/ .", "AI": {"tldr": "\u4e00\u79cd\u65b0\u7684\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\uff0c\u4f7f\u7528\u7269\u7406\u539f\u7406\uff08\u5982\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\uff09\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5177\u6709\u7269\u7406\u4e00\u81f4\u8fd0\u52a8\u76844D\u6d41\u4f53\u573a\u666f\uff0c\u5e76\u4f7f\u75283D\u9ad8\u65af\u6765\u6355\u6349\u5916\u89c2\u3002", "motivation": "\u65e8\u5728\u5c06\u795e\u7ecf\u7f51\u7edc\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u62104D\u573a\u666f\uff08\u5305\u62ec\u8fd0\u52a8\u548c3D\u51e0\u4f55\uff09\u7684\u80fd\u529b\uff0c\u7279\u522b\u662f\u9488\u5bf9\u81ea\u7136\u6d41\u4f53\u56fe\u50cf\uff0c\u5e76\u89e3\u51b3\u73b0\u6709\u65b9\u6cd5\u5728\u8fd0\u52a8\u9884\u6d4b\u4e2d\u4e0d\u7b26\u5408\u7269\u7406\u539f\u7406\u7684\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\u4ece\u8f93\u5165\u7684\u56fe\u50cf\u53ca\u5176\u4f30\u8ba1\u7684\u6df1\u5ea6\u4e2d\u9884\u6d4b\u57fa\u4e8e\u7279\u5f81\u76843D\u9ad8\u65af\uff0c\u5e76\u4f7f\u7528\u9884\u6d4b\u7684\u8fd0\u52a8\u8fdb\u884c\u52a8\u753b\u5904\u7406\uff0c\u4ece\u4efb\u4f55\u671f\u671b\u7684\u76f8\u673a\u89c6\u89d2\u8fdb\u884c\u6e32\u67d3\u3002\u8be5\u65b9\u6cd5\u4f7f\u7528\u7eb3\u7ef4-\u65af\u6258\u514b\u65af\u65b9\u7a0b\u7b49\u57fa\u672c\u7269\u7406\u539f\u7406\u63a8\u5bfc\u51fa\u7684\u635f\u5931\u9879\u6765\u6307\u5bfc\u8fd0\u52a8\u9884\u6d4b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8bc1\u660e\u4e86\u8be5\u65b9\u6cd5\u5728\u751f\u6210\u7269\u7406\u4e0a\u5408\u7406\u52a8\u753b\u65b9\u9762\u7684\u6709\u6548\u6027\uff0c\u4e0e\u73b0\u6709\u65b9\u6cd5\u76f8\u6bd4\uff0c\u6027\u80fd\u6709\u4e86\u663e\u8457\u63d0\u9ad8\u3002", "conclusion": "\u672c\u6587\u63d0\u51fa\u7684\u7269\u7406\u5f15\u5bfc\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u80fd\u591f\u4ece\u5355\u5f20\u56fe\u50cf\u751f\u6210\u5177\u6709\u7269\u7406\u4e00\u81f4\u6027\u52a8\u753b\u76844D\u573a\u666f\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002"}}
