<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 75]
- [cs.CL](#cs.CL) [Total: 93]
- [cs.DS](#cs.DS) [Total: 8]
- [cond-mat.mtrl-sci](#cond-mat.mtrl-sci) [Total: 20]
- [cs.MA](#cs.MA) [Total: 3]
- [physics.app-ph](#physics.app-ph) [Total: 4]
- [quant-ph](#quant-ph) [Total: 53]
- [cs.NE](#cs.NE) [Total: 1]
- [cs.LG](#cs.LG) [Total: 115]
- [cs.AR](#cs.AR) [Total: 4]
- [eess.SP](#eess.SP) [Total: 5]
- [cs.LO](#cs.LO) [Total: 3]
- [eess.SY](#eess.SY) [Total: 13]
- [cs.GT](#cs.GT) [Total: 7]
- [cs.DC](#cs.DC) [Total: 17]
- [cs.RO](#cs.RO) [Total: 29]
- [cond-mat.mes-hall](#cond-mat.mes-hall) [Total: 11]
- [cs.SI](#cs.SI) [Total: 1]
- [cs.AI](#cs.AI) [Total: 55]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation](https://arxiv.org/abs/2510.05532)
*Sam Sartor,Pieter Peers*

Main category: cs.CV

TL;DR: Teamwork是一种统一的解决方案，可以通过协调和适配多个基础扩散模型实例来扩展通道数量，而无需更改预训练模型的架构，并能适应新任务。


<details>
  <summary>Details</summary>
Motivation: 当前扩散模型的通道扩展方法通常是特定应用的，并且难以适应不同的扩散模型或新任务。然而，神经渲染、SVBRDF估计和固有图像分解等应用需要额外的输入或输出通道。

Method: Teamwork通过协调和适配多个基础扩散模型实例（“队友”）来实现通道扩展，而无需更改预训练的扩散模型架构。采用一种新颖的低秩适配（LoRA）变体来同时处理适配和队友之间的协调。支持队友的动态激活/停用。

Result: 在图像修复、单张图像SVBRDF估计、固有分解、神经着色和固有图像合成等多种生成和逆图形任务上，证明了Teamwork的灵活性和效率。

Conclusion: Teamwork为扩散模型提供了灵活且高效的通道扩展和任务适配的统一解决方案。

Abstract: Large pretrained diffusion models can provide strong priors beneficial for
many graphics applications. However, generative applications such as neural
rendering and inverse methods such as SVBRDF estimation and intrinsic image
decomposition require additional input or output channels. Current solutions
for channel expansion are often application specific and these solutions can be
difficult to adapt to different diffusion models or new tasks. This paper
introduces Teamwork: a flexible and efficient unified solution for jointly
increasing the number of input and output channels as well as adapting a
pretrained diffusion model to new tasks. Teamwork achieves channel expansion
without altering the pretrained diffusion model architecture by coordinating
and adapting multiple instances of the base diffusion model (\ie, teammates).
We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address
both adaptation and coordination between the different teammates. Furthermore
Teamwork supports dynamic (de)activation of teammates. We demonstrate the
flexibility and efficiency of Teamwork on a variety of generative and inverse
graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic
decomposition, neural shading, and intrinsic image synthesis.

</details>


### [2] [Attention-Enhanced Prototypical Learning for Few-Shot Infrastructure Defect Segmentation](https://arxiv.org/abs/2510.05266)
*Christina Thrainer,Md Meftahul Ferdaus,Mahdi Abdelguerfi,Christian Guetl,Steven Sloan,Kendall N. Niles,Ken Pathak*

Main category: cs.CV

TL;DR: 该研究提出了一种增强型特征金字塔网络（E-FPN）框架，用于少样本语义分割，以解决基础设施检测中标签数据稀缺和难以学习新类别的问题。


<details>
  <summary>Details</summary>
Motivation: 少样本语义分割在基础设施检测应用中至关重要，但现有深度学习框架需要大量标注数据，并且难以用少量数据学习新的缺陷类别。

Method: 提出了一种基于原型学习的E-FPN框架，其特点是：1. 使用InceptionSepConv块和深度可分离卷积的自适应E-FPN编码器进行高效的多尺度特征提取；2. 使用掩码平均池化的原型学习进行强大的原型生成；3. 通过全局、局部和跨域自注意力机制进行基于注意力的特征表示。

Result: 在基础设施检测数据集上的实验表明，该方法实现了优异的少样本性能，在8路5样本训练配置下，2路分类测试中F1分数达到82.55%，mIoU达到72.26%。自注意力机制带来了最大的性能提升，比基线提高了2.57%的F1分数和2.9%的mIoU。

Conclusion: 该框架满足了在有限新训练数据的情况下快速响应基础设施检测中新缺陷类型的关键需求，从而能够制定更高效、更经济的关键基础设施维护计划。

Abstract: Few-shot semantic segmentation is vital for deep learning-based
infrastructure inspection applications, where labeled training examples are
scarce and expensive. Although existing deep learning frameworks perform well,
the need for extensive labeled datasets and the inability to learn new defect
categories with little data are problematic. We present our Enhanced Feature
Pyramid Network (E-FPN) framework for few-shot semantic segmentation of culvert
and sewer defect categories using a prototypical learning framework. Our
approach has three main contributions: (1) adaptive E-FPN encoder using
InceptionSepConv blocks and depth-wise separable convolutions for efficient
multi-scale feature extraction; (2) prototypical learning with masked average
pooling for powerful prototype generation from small support examples; and (3)
attention-based feature representation through global self-attention, local
self-attention and cross-attention. Comprehensive experimentation on
challenging infrastructure inspection datasets illustrates that the method
achieves excellent few-shot performance, with the best configuration being
8-way 5-shot training configuration at 82.55% F1-score and 72.26% mIoU in 2-way
classification testing. The self-attention method had the most significant
performance improvements, providing 2.57% F1-score and 2.9% mIoU gain over
baselines. Our framework addresses the critical need to rapidly respond to new
defect types in infrastructure inspection systems with limited new training
data that lead to more efficient and economical maintenance plans for critical
infrastructure systems.

</details>


### [3] [SkinMap: Weighted Full-Body Skin Segmentation for Robust Remote Photoplethysmography](https://arxiv.org/abs/2510.05296)
*Zahra Maleki,Amirhossein Akbari,Amirhossein Binesh,Babak Khalaj*

Main category: cs.CV

TL;DR: 提出一种新的rPPG皮肤分割技术，提高在移动和照明变化下的信号质量，并在新数据集SYNC-rPPG上验证其在现实世界条件下的有效性。


<details>
  <summary>Details</summary>
Motivation: rPPG技术在远程病人监护、情感分析和智能汽车等领域具有潜力，但其准确性易受光照和运动影响，因此需要改进皮肤区域选择以提取更准确的rPPG信号。

Method: 提出一种新的皮肤分割技术，该技术能检测全身皮肤区域，抵抗运动干扰，并排除嘴部、眼睛和毛发等可能引起干扰的区域，以提高rPPG信号质量。

Result: 所提出的模型在公开数据集和新的SYNC-rPPG数据集上进行了评估，结果表明该模型在说话和头部旋转等挑战性条件下能准确捕捉心跳，保持预测心率与实际心率之间的平均绝对误差（MAE），并且能准确检测各种肤色。

Conclusion: 所提出的rPPG皮肤分割技术能够提高在挑战性条件下的信号质量和准确性，并且能够准确检测各种肤色，为rPPG技术在现实世界中的应用提供了有前景的解决方案。

Abstract: Remote photoplethysmography (rPPG) is an innovative method for monitoring
heart rate and vital signs by using a simple camera to record a person, as long
as any part of their skin is visible. This low-cost, contactless approach helps
in remote patient monitoring, emotion analysis, smart vehicle utilization, and
more. Over the years, various techniques have been proposed to improve the
accuracy of this technology, especially given its sensitivity to lighting and
movement. In the unsupervised pipeline, it is necessary to first select skin
regions from the video to extract the rPPG signal from the skin color changes.
We introduce a novel skin segmentation technique that prioritizes skin regions
to enhance the quality of the extracted signal. It can detect areas of skin all
over the body, making it more resistant to movement, while removing areas such
as the mouth, eyes, and hair that may cause interference. Our model is
evaluated on publicly available datasets, and we also present a new dataset,
called SYNC-rPPG, to better represent real-world conditions. The results
indicate that our model demonstrates a prior ability to capture heartbeats in
challenging conditions, such as talking and head rotation, and maintain the
mean absolute error (MAE) between predicted and actual heart rates, while other
methods fail to do so. In addition, we demonstrate high accuracy in detecting a
diverse range of skin tones, making this technique a promising option for
real-world applications.

</details>


### [4] [DeepAf: One-Shot Spatiospectral Auto-Focus Model for Digital Pathology](https://arxiv.org/abs/2510.05315)
*Yousef Yeganeh,Maximilian Frantzen,Michael Lee,Kun-Hsing Yu,Nassir Navab,Azade Farshad*

Main category: cs.CV

TL;DR: 一种新的基于深度学习的自动对焦系统DeepAf，它结合空间和光谱特征，能将传统显微镜转变为高效的数字化切片扫描仪，显著提高对焦速度和精度，并在临床研究中表现出优异的癌症分类能力。


<details>
  <summary>Details</summary>
Motivation: 现有低成本显微镜解决方案在聚焦一致性、速度和泛化能力方面存在局限性，而昂贵的Whole Slide Imaging（WSI）扫描仪则限制了其可及性。

Method: 提出了一种名为DeepAf的新型自动对焦框架，该框架通过混合架构结合空间和光谱特征，实现单次拍摄即可预测焦点。该系统将传统显微镜转变为高效的扫描仪，自动回归最佳焦点距离并调整控制参数以获得最佳成像效果。

Result: 与基于焦堆的方法相比，对焦时间缩短了80%，在相同实验室样本中达到0.18微米的对焦精度，仅需一半的输入要求即可达到双图像方法的精度（0.19微米）。DeepAf在不同实验室之间具有良好的泛化能力，假对焦预测仅占0.72%，90%的预测在景深内。在536个脑组织样本的临床研究中，系统在4倍放大倍率下实现了0.90的癌症分类AUC。

Conclusion: 该研究提出了一个软硬件结合的综合设计，能够为资源受限的环境提供可及的、实时的数字化病理解决方案，同时保持诊断准确性。

Abstract: While Whole Slide Imaging (WSI) scanners remain the gold standard for
digitizing pathology samples, their high cost limits accessibility in many
healthcare settings. Other low-cost solutions also face critical limitations:
automated microscopes struggle with consistent focus across varying tissue
morphology, traditional auto-focus methods require time-consuming focal stacks,
and existing deep-learning approaches either need multiple input images or lack
generalization capability across tissue types and staining protocols. We
introduce a novel automated microscopic system powered by DeepAf, a novel
auto-focus framework that uniquely combines spatial and spectral features
through a hybrid architecture for single-shot focus prediction. The proposed
network automatically regresses the distance to the optimal focal point using
the extracted spatiospectral features and adjusts the control parameters for
optimal image outcomes. Our system transforms conventional microscopes into
efficient slide scanners, reducing focusing time by 80% compared to stack-based
methods while achieving focus accuracy of 0.18 {\mu}m on the same-lab samples,
matching the performance of dual-image methods (0.19 {\mu}m) with half the
input requirements. DeepAf demonstrates robust cross-lab generalization with
only 0.72% false focus predictions and 90% of predictions within the depth of
field. Through an extensive clinical study of 536 brain tissue samples, our
system achieves 0.90 AUC in cancer classification at 4x magnification, a
significant achievement at lower magnification than typical 20x WSI scans. This
results in a comprehensive hardware-software design enabling accessible,
real-time digital pathology in resource-constrained settings while maintaining
diagnostic accuracy.

</details>


### [5] [Fine-Tuned CNN-Based Approach for Multi-Class Mango Leaf Disease Detection](https://arxiv.org/abs/2510.05326)
*Jalal Ahmmed,Faruk Ahmed,Rashedul Hasan Shohan,Md. Mahabub Rana,Mahdi Hasan*

Main category: cs.CV

TL;DR: 本研究使用迁移学习策略和微调，评估了五种预训练卷积神经网络（DenseNet201、InceptionV3、ResNet152V2、SeResNet152 和 Xception）在八种芒果叶部疾病的多类别识别方面的性能。DenseNet201 表现最佳，准确率为 99.33%，在识别切割萎叶病和细菌性枯萎病方面尤为出色。


<details>
  <summary>Details</summary>
Motivation: 芒果是南亚重要的水果作物，但其种植 frequently 受到叶部病害的阻碍，严重影响产量和质量。本研究旨在为芒果叶部病害的准确识别提供解决方案。

Method: 采用迁移学习策略，对 DenseNet201、InceptionV3、ResNet152V2、SeResNet152 和 Xception 五种预训练卷积神经网络进行微调，用于识别八种芒果叶部病害。通过准确率、精确率、召回率、F1 分数和混淆矩阵等标准评估指标进行模型评估。

Result: DenseNet201 表现最佳，准确率为 99.33%，在识别切割萎叶病和细菌性枯萎病方面表现尤为出色。ResNet152V2 和 SeResNet152 也取得了良好的结果，而 InceptionV3 和 Xception 在区分烟煤病和白粉病等视觉上相似的类别时表现较差。训练和验证图显示，性能最佳的模型具有稳定的收敛性。

Conclusion: 微调的迁移学习模型能够精确可靠地进行多类别芒果叶部病害检测，可应用于智能农业。

Abstract: Mango is an important fruit crop in South Asia, but its cultivation is
frequently hampered by leaf diseases that greatly impact yield and quality.
This research examines the performance of five pre-trained convolutional neural
networks, DenseNet201, InceptionV3, ResNet152V2, SeResNet152, and Xception, for
multi-class identification of mango leaf diseases across eight classes using a
transfer learning strategy with fine-tuning. The models were assessed through
standard evaluation metrics, such as accuracy, precision, recall, F1-score, and
confusion matrices. Among the architectures tested, DenseNet201 delivered the
best results, achieving 99.33% accuracy with consistently strong metrics for
individual classes, particularly excelling in identifying Cutting Weevil and
Bacterial Canker. Moreover, ResNet152V2 and SeResNet152 provided strong
outcomes, whereas InceptionV3 and Xception exhibited lower performance in
visually similar categories like Sooty Mould and Powdery Mildew. The training
and validation plots demonstrated stable convergence for the highest-performing
models. The capability of fine-tuned transfer learning models, for precise and
dependable multi-class mango leaf disease detection in intelligent agricultural
applications.

</details>


### [6] [Mitigating Diffusion Model Hallucinations with Dynamic Guidance](https://arxiv.org/abs/2510.05356)
*Kostas Triaridis,Alexandros Graikos,Aggelina Chatziagapi,Grigorios G. Chrysos,Dimitris Samaras*

Main category: cs.CV

TL;DR: 扩散模型会产生与真实数据分布不符的幻觉样本，主要是由于数据分布模式之间的过度平滑。本文提出的动态引导（Dynamic Guidance）通过选择性地锐化分数函数来解决这个问题，只针对已知会导致伪影的方向，同时保留有效的语义变化，从而在生成时减少幻觉，而不是事后过滤。


<details>
  <summary>Details</summary>
Motivation: 扩散模型（Diffusion models）在生成样本时存在“幻觉”问题，即生成的样本包含不符合真实数据分布的结构不一致性，这归因于数据分布模式间的过度平滑。然而，语义插值（semantic interpolations）在生成多样性方面是有益的，因此需要一种更精细的解决方案。

Method: 动态引导（Dynamic Guidance）是一种在生成时解决扩散模型幻觉问题的方法。它通过选择性地锐化分数函数（score function）来减少幻觉，但仅限于那些已知会导致伪影（artifacts）的特定方向，同时保持了有效的语义变化。

Result: 动态引导（Dynamic Guidance）在受控和自然图像数据集上都显著减少了幻觉现象，并且性能显著优于基线方法。

Conclusion: 动态引导（Dynamic Guidance）是首个在生成时解决扩散模型幻觉问题的创新方法，它通过精细地调整分数函数，有效减少了幻觉，同时保留了生成的多样性，并在实验中取得了优于现有技术的成果。

Abstract: Diffusion models, despite their impressive demos, often produce hallucinatory
samples with structural inconsistencies that lie outside of the support of the
true data distribution. Such hallucinations can be attributed to excessive
smoothing between modes of the data distribution. However, semantic
interpolations are often desirable and can lead to generation diversity, thus
we believe a more nuanced solution is required. In this work, we introduce
Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates
hallucinations by selectively sharpening the score function only along the
pre-determined directions known to cause artifacts, while preserving valid
semantic variations. To our knowledge, this is the first approach that
addresses hallucinations at generation time rather than through post-hoc
filtering. Dynamic Guidance substantially reduces hallucinations on both
controlled and natural image datasets, significantly outperforming baselines.

</details>


### [7] [LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation](https://arxiv.org/abs/2510.05367)
*Yang Xiao,Gen Li,Kaiyuan Deng,Yushu Wu,Zheng Zhan,Yanzhi Wang,Xiaolong Ma,Bo Hui*

Main category: cs.CV

TL;DR: 通过异步缓存交换、特征分块和分片解码等特定阶段的策略，在不显著影响视频生成质量的情况下，加速了基于扩散模型的视频生成过程，并减少了内存消耗。


<details>
  <summary>Details</summary>
Motivation: 现有的训练免费加速方法在扩散模型推理的后两个阶段（去噪和解码）中可能导致内存激增，需要一种能够同时加速推理过程并减少内存占用的方法。

Method: 将推理过程分解为编码、去噪和解码三个阶段，并针对每个阶段提出特定的内存优化策略：1. 异步缓存交换；2. 特征分块；3. 分片解码。确保这些策略引入的时间开销低于加速带来的收益。

Result: 与基线方法相比，所提出的方法实现了更快的推理速度和更低的内存占用，同时将质量下降控制在可接受的范围内。

Conclusion: 所提出的特定阶段的加速策略能够有效解决现有方法在内存使用方面的不足，在保证视频生成质量的同时，提高了推理效率。

Abstract: Training-free acceleration has emerged as an advanced research area in video
generation based on diffusion models. The redundancy of latents in diffusion
model inference provides a natural entry point for acceleration. In this paper,
we decompose the inference process into the encoding, denoising, and decoding
stages, and observe that cache-based acceleration methods often lead to
substantial memory surges in the latter two stages. To address this problem, we
analyze the characteristics of inference across different stages and propose
stage-specific strategies for reducing memory consumption: 1) Asynchronous
Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same
time, we ensure that the time overhead introduced by these three strategies
remains lower than the acceleration gains themselves. Compared with the
baseline, our approach achieves faster inference speed and lower memory usage,
while maintaining quality degradation within an acceptable range. The Code is
available at https://github.com/NKUShaw/LightCache .

</details>


### [8] [See the past: Time-Reversed Scene Reconstruction from Thermal Traces Using Visual Language Models](https://arxiv.org/abs/2510.05408)
*Kebin Contreras,Luis Toscano-Palomino,Mauro Dalla Mura,Jorge Bacca*

Main category: cs.CV

TL;DR: 利用RGB和热成像，通过视觉语言模型和约束扩散过程，可以从几秒前的热痕迹中重建过去场景的状态。


<details>
  <summary>Details</summary>
Motivation: 从当前观察中恢复过去对于法医和场景分析具有潜在应用价值，而热成像可以提供RGB相机无法捕捉的额外信息。

Method: 提出一个时间反转重建框架，结合视觉语言模型（VLMs）和约束扩散过程。一个VLM生成场景描述，另一个VLM指导图像重建，以保持语义和结构的一致性。

Result: 在三个受控场景中评估了该方法，证明了重建长达120秒前过去帧的可行性。

Conclusion: 该方法是时间反转成像从热痕迹重建的初步尝试。

Abstract: Recovering the past from present observations is an intriguing challenge with
potential applications in forensics and scene analysis. Thermal imaging,
operating in the infrared range, provides access to otherwise invisible
information. Since humans are typically warmer (37 C -98.6 F) than their
surroundings, interactions such as sitting, touching, or leaning leave residual
heat traces. These fading imprints serve as passive temporal codes, allowing
for the inference of recent events that exceed the capabilities of RGB cameras.
This work proposes a time-reversed reconstruction framework that uses paired
RGB and thermal images to recover scene states from a few seconds earlier. The
proposed approach couples Visual-Language Models (VLMs) with a constrained
diffusion process, where one VLM generates scene descriptions and another
guides image reconstruction, ensuring semantic and structural consistency. The
method is evaluated in three controlled scenarios, demonstrating the
feasibility of reconstructing plausible past frames up to 120 seconds earlier,
providing a first step toward time-reversed imaging from thermal traces.

</details>


### [9] [Personalizing Retrieval using Joint Embeddings or "the Return of Fluffy"](https://arxiv.org/abs/2510.05411)
*Bruno Korbar,Andrew Zisserman*

Main category: cs.CV

TL;DR: 提出了一种名为pi-map的映射网络，可以将局部图像嵌入转换为文本标记，以实现结合图像和文本的复合查询，从而改进个性化检索效果。


<details>
  <summary>Details</summary>
Motivation: 为了实现一种能够结合图像中的物体实例信息和自然语言描述（描述物体在做什么或在哪里）的复合查询，以检索图像。

Method: 设计了一个映射网络（pi-map），能够将物体实例的局部图像嵌入“翻译”成一个文本标记。该标记与自然语言查询结合后，可用于CLIP风格的文本编码和图像检索。此过程仅需对每个物体实例进行一次训练。

Result: 所提出的pi-map方法，结合冻结的CLIP文本和图像编码器，在评估个性化检索的两个基准测试中均取得了优于现有技术的性能。

Conclusion: pi-map方法通过训练映射网络，能够有效地将图像中的物体实例信息转化为文本标记，从而显著提高了结合图像和文本的复合查询在个性化图像检索任务中的性能。

Abstract: The goal of this paper is to be able to retrieve images using a compound
query that combines object instance information from an image, with a natural
text description of what that object is doing or where it is. For example, to
retrieve an image of "Fluffy the unicorn (specified by an image) on someone's
head". To achieve this we design a mapping network that can "translate" from a
local image embedding (of the object instance) to a text token, such that the
combination of the token and a natural language query is suitable for CLIP
style text encoding, and image retrieval. Generating a text token in this
manner involves a simple training procedure, that only needs to be performed
once for each object instance. We show that our approach of using a trainable
mapping network, termed pi-map, together with frozen CLIP text and image
encoders, improves the state of the art on two benchmarks designed to assess
personalized retrieval.

</details>


### [10] [ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars](https://arxiv.org/abs/2510.05488)
*Peizhi Yan,Rabab Ward,Qiang Tang,Shan Du*

Main category: cs.CV

TL;DR: ArchitectHead是首个支持连续细节层次（LOD）控制的3D高斯喷溅（3DGS）头部化身框架，通过在2D UV特征空间中参数化高斯点并使用多层可学习特征图编码其潜在特征，实现高效且连续的LOD控制，而无需重新训练。


<details>
  <summary>Details</summary>
Motivation: 现有3DGS头部化身在训练后高斯点数量固定，无法满足实际应用中对可调节细节层次（LOD）以平衡渲染效率和视觉质量的需求。

Method: 将高斯点参数化于2D UV特征空间，并引入由多层可学习特征图组成的UV特征场来编码其潜在特征。使用轻量级神经网络解码器将潜在特征转换为3D高斯属性进行渲染。通过在不同分辨率下重采样UV特征场来动态调整高斯点数量，实现LOD控制。

Result: 在最高LOD下，ArchitectHead在身份重演任务中达到SOTA质量；在最低LOD下，高斯点数量减少93.8%，渲染速度近乎翻倍，而质量仅有适度下降（L1 Loss +7.9%，PSNR -0.97%，SSIM -0.6%，LPIPS Loss +24.1%）。

Conclusion: ArchitectHead成功实现了3D高斯头部化身的可控LOD，在保持高视觉质量的同时显著提高了渲染效率，为实时3D图形应用提供了新的可能性。

Abstract: 3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time
rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on
tens of thousands of 3D Gaussian points (Gaussians), with the number of
Gaussians fixed after training. However, many practical applications require
adjustable levels of detail (LOD) to balance rendering efficiency and visual
quality. In this work, we propose "ArchitectHead", the first framework for
creating 3D Gaussian head avatars that support continuous control over LOD. Our
key idea is to parameterize the Gaussians in a 2D UV feature space and propose
a UV feature field composed of multi-level learnable feature maps to encode
their latent features. A lightweight neural network-based decoder then
transforms these latent features into 3D Gaussian attributes for rendering.
ArchitectHead controls the number of Gaussians by dynamically resampling
feature maps from the UV feature field at the desired resolutions. This method
enables efficient and continuous control of LOD without retraining.
Experimental results show that ArchitectHead achieves state-of-the-art (SOTA)
quality in self and cross-identity reenactment tasks at the highest LOD, while
maintaining near SOTA performance at lower LODs. At the lowest LOD, our method
uses only 6.2\% of the Gaussians while the quality degrades moderately (L1 Loss
+7.9\%, PSNR --0.97\%, SSIM --0.6\%, LPIPS Loss +24.1\%), and the rendering
speed nearly doubles.

</details>


### [11] [Human Action Recognition from Point Clouds over Time](https://arxiv.org/abs/2510.05506)
*James Dickens*

Main category: cs.CV

TL;DR: 提出了一种利用3D点云进行动作识别的新方法，该方法能够处理深度传感器和单目深度估计产生的点云数据。


<details>
  <summary>Details</summary>
Motivation: 随着3D传感器数据的日益普及，为动作识别开辟了新的可能性，本文旨在利用这些3D数据开发一种不同于现有骨骼和基于视频方法的动作识别途径。

Method: 该方法首先分割点云中的人体，然后进行时域跟踪和身体部位分割。其核心是一个结合了基于点技术和稀疏卷积网络的三维动作识别骨干网络，并辅以表面法线、颜色、红外强度和身体部位标签等辅助点特征。

Result: 在NTU RGB-D 120数据集上的实验表明，该方法在考虑不同受试者进行训练和测试时，通过集成传感器数据和估计的深度输入，达到了89.3%的准确率，优于先前基于点云的动作识别方法，并可与现有的骨骼动作识别算法媲美。

Conclusion: 所提出的基于3D点云的动作识别方法在准确性和鲁棒性方面表现出色，为利用3D传感器数据进行动作识别提供了一种有前景的解决方案。

Abstract: Recent research into human action recognition (HAR) has focused predominantly
on skeletal action recognition and video-based methods. With the increasing
availability of consumer-grade depth sensors and Lidar instruments, there is a
growing opportunity to leverage dense 3D data for action recognition, to
develop a third way. This paper presents a novel approach for recognizing
actions from 3D videos by introducing a pipeline that segments human point
clouds from the background of a scene, tracks individuals over time, and
performs body part segmentation. The method supports point clouds from both
depth sensors and monocular depth estimation. At the core of the proposed HAR
framework is a novel backbone for 3D action recognition, which combines
point-based techniques with sparse convolutional networks applied to
voxel-mapped point cloud sequences. Experiments incorporate auxiliary point
features including surface normals, color, infrared intensity, and body part
parsing labels, to enhance recognition accuracy. Evaluation on the NTU RGB- D
120 dataset demonstrates that the method is competitive with existing skeletal
action recognition algorithms. Moreover, combining both sensor-based and
estimated depth inputs in an ensemble setup, this approach achieves 89.3%
accuracy when different human subjects are considered for training and testing,
outperforming previous point cloud action recognition methods.

</details>


### [12] [Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models](https://arxiv.org/abs/2510.05509)
*Shinnosuke Saito,Takashi Matsubara*

Main category: cs.CV

TL;DR: 该研究提出了一种新的黎曼度量，用于在扩散模型的噪声空间中进行插值，以实现更自然、更逼真的图像过渡。


<details>
  <summary>Details</summary>
Motivation: 现有的扩散模型缺乏明确的低维潜在空间，这限制了其进行流形感知分析和操作（如插值和编辑）。现有的插值方法通常遵循高密度区域的路径，可能不会与数据流形对齐，导致不自然的过渡。

Method: 提出了一种受分数函数雅可比行列式启发的新型黎曼度量，该度量能够捕捉局部数据流形的切空间。该度量旨在鼓励噪声空间中的测地线保持在学习到的数据流形内部或与之平行。

Result: 在图像插值实验中，与基于密度和朴素的基线方法相比，所提出的度量能够产生感知上更自然、更保真的过渡。

Conclusion: 该研究成功地为扩散模型引入了一种新的黎曼度量，以改进插值过程，使其过渡更自然、更忠实于数据流形。

Abstract: Diffusion models are powerful deep generative models (DGMs) that generate
high-fidelity, diverse content. However, unlike classical DGMs, they lack an
explicit, tractable low-dimensional latent space that parameterizes the data
manifold. This absence limits manifold-aware analysis and operations, such as
interpolation and editing. Existing interpolation methods for diffusion models
typically follow paths through high-density regions, which are not necessarily
aligned with the data manifold and can yield perceptually unnatural
transitions. To exploit the data manifold learned by diffusion models, we
propose a novel Riemannian metric on the noise space, inspired by recent
findings that the Jacobian of the score function captures the tangent spaces to
the local data manifold. This metric encourages geodesics in the noise space to
stay within or run parallel to the learned data manifold. Experiments on image
interpolation show that our metric produces perceptually more natural and
faithful transitions than existing density-based and naive baselines.

</details>


### [13] [Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work](https://arxiv.org/abs/2510.05538)
*Owen Henkel,Bill Roberts,Doug Jaffe,Laurence Holt*

Main category: cs.CV

TL;DR: 大型多模态模型（MLLMs）在小学和中学数学教育中具有评估手写作业的潜力，但目前在解释学生的手绘插图方面仍有困难。


<details>
  <summary>Details</summary>
Motivation: 评估大型多模态模型（MLLMs）在批改、分析和反馈手写学生课堂作业方面的潜力，特别是在小学和中学数学教育领域，因为批改手写作业耗时且难以深入了解学生的学习过程。

Method: 进行了两个实验：实验A评估了MLLMs对288份加纳中学生解决算术问题的笔试作业的评分能力，实验B评估了150份美国小学生数学插图的评分能力，并比较了直接分析插图和在提供人类描述后分析插图的表现。

Result: 在实验A中，MLLMs在客观答案的算术问题上达到了接近人类的准确率（95%，k = 0.90），但有时会犯人类教师不会犯的错误。在实验B中，MLLMs在直接分析学生插图时表现不佳（k = 0.20），但在提供人类描述后，其评分与人类评分者之间的一致性显著提高（k = 0.47）。

Conclusion: MLLMs能够较好地“看见”和解释算术作业，但在“看见”和理解学生的数学插图方面仍存在挑战。

Abstract: Recent advances in multimodal large language models (MLLMs) raise the
question of their potential for grading, analyzing, and offering feedback on
handwritten student classwork. This capability would be particularly beneficial
in elementary and middle-school mathematics education, where most work remains
handwritten, because seeing students' full working of a problem provides
valuable insights into their learning processes, but is extremely
time-consuming to grade. We present two experiments investigating MLLM
performance on handwritten student mathematics classwork. Experiment A examines
288 handwritten responses from Ghanaian middle school students solving
arithmetic problems with objective answers. In this context, models achieved
near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human
educators would be unlikely to make. Experiment B evaluates 150 mathematical
illustrations from American elementary students, where the drawings are the
answer to the question. These tasks lack single objective answers and require
sophisticated visual interpretation as well as pedagogical judgment in order to
analyze and evaluate them. We attempted to separate MLLMs' visual capabilities
from their pedagogical abilities by first asking them to grade the student
illustrations directly, and then by augmenting the image with a detailed human
description of the illustration. We found that when the models had to analyze
the student illustrations directly, they struggled, achieving only k = 0.20
with ground truth scores, but when given human descriptions, their agreement
levels improved dramatically to k = 0.47, which was in line with human-to-human
agreement levels. This gap suggests MLLMs can "see" and interpret arithmetic
work relatively well, but still struggle to "see" student mathematical
illustrations.

</details>


### [14] [Midway Network: Learning Representations for Recognition and Motion from Latent Dynamics](https://arxiv.org/abs/2510.05558)
*Christopher Hoang,Mengye Ren*

Main category: cs.CV

TL;DR: 该论文提出了一种名为Midway Network的新型自监督学习架构，首次实现了从自然视频中同时学习目标识别和运动理解的强大视觉表征，并证明了其在语义分割和光流任务上的优越性。


<details>
  <summary>Details</summary>
Motivation: 此前的自监督学习方法主要侧重于目标识别或运动理解中的某一方面，而忽略了两者结合。本研究旨在解决这一问题，并利用潜在动力学模型来学习观察及其随时间变化的表征，以应用于控制和规划任务。

Method: 提出了一种名为Midway Network的新型自监督学习架构，该架构通过引入中间的自上而下路径来推断视频帧之间的运动潜在表征，并结合密集的正向预测目标和分层结构来处理包含多个物体的复杂自然视频场景。

Result: 在两个大规模自然视频数据集上进行预训练后，Midway Network在语义分割和光流任务上的表现优于现有自监督学习方法。此外，通过一种新颖的基于前向特征扰动的方法，证明了Midway Network学习到的动力学可以捕捉到高层对应关系。

Conclusion: Midway Network是一种创新的自监督学习架构，能够从自然视频中同时学习目标识别和运动理解的表征，并在多项下游任务中展现出优越性能，为未来的视觉感知研究提供了新的方向。

Abstract: Object recognition and motion understanding are key components of perception
that complement each other. While self-supervised learning methods have shown
promise in their ability to learn from unlabeled data, they have primarily
focused on obtaining rich representations for either recognition or motion
rather than both in tandem. On the other hand, latent dynamics modeling has
been used in decision making to learn latent representations of observations
and their transformations over time for control and planning tasks. In this
work, we present Midway Network, a new self-supervised learning architecture
that is the first to learn strong visual representations for both object
recognition and motion understanding solely from natural videos, by extending
latent dynamics modeling to this domain. Midway Network leverages a midway
top-down path to infer motion latents between video frames, as well as a dense
forward prediction objective and hierarchical structure to tackle the complex,
multi-object scenes of natural videos. We demonstrate that after pretraining on
two large-scale natural video datasets, Midway Network achieves strong
performance on both semantic segmentation and optical flow tasks relative to
prior self-supervised learning methods. We also show that Midway Network's
learned dynamics can capture high-level correspondence via a novel analysis
method based on forward feature perturbation.

</details>


### [15] [HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video](https://arxiv.org/abs/2510.05560)
*Hongchi Xia,Chih-Hao Lin,Hao-Yu Hsu,Quentin Leboutet,Katelyn Gao,Michael Paulitsch,Benjamin Ummenhofer,Shenlong Wang*

Main category: cs.CV

TL;DR: HoloScene是一个新的交互式3D重建框架，可以同时满足几何完整性、对象交互性、物理合理性、照片写实渲染和现实物理属性的要求。


<details>
  <summary>Details</summary>
Motivation: 现有3D重建和场景理解方法在几何完整性、对象交互性、物理合理性、照片写实渲染或动态模拟的现实物理属性等方面存在不足。

Method: HoloScene利用全面的交互式场景图表示，对对象几何、外观和物理属性以及层次和对象间关系进行编码。将重建构建为基于能量的优化问题，并将观测数据、物理约束和生成先验整合到一个统一的、连贯的目标中。通过结合基于采样的探索和基于梯度的细化，高效地执行优化。

Result: HoloScene生成的数字孪生具有完整且精确的几何形状、物理稳定性和从新视点进行逼真渲染的能力。

Conclusion: 在多个基准数据集上进行的评估表明HoloScene的性能优越，而在交互式游戏和实时数字孪生操作中的实际用例说明了HoloScene的广泛适用性和有效性。

Abstract: Digitizing the physical world into accurate simulation-ready virtual
environments offers significant opportunities in a variety of fields such as
augmented and virtual reality, gaming, and robotics. However, current 3D
reconstruction and scene-understanding methods commonly fall short in one or
more critical aspects, such as geometry completeness, object interactivity,
physical plausibility, photorealistic rendering, or realistic physical
properties for reliable dynamic simulation. To address these limitations, we
introduce HoloScene, a novel interactive 3D reconstruction framework that
simultaneously achieves these requirements. HoloScene leverages a comprehensive
interactive scene-graph representation, encoding object geometry, appearance,
and physical properties alongside hierarchical and inter-object relationships.
Reconstruction is formulated as an energy-based optimization problem,
integrating observational data, physical constraints, and generative priors
into a unified, coherent objective. Optimization is efficiently performed via a
hybrid approach combining sampling-based exploration with gradient-based
refinement. The resulting digital twins exhibit complete and precise geometry,
physical stability, and realistic rendering from novel viewpoints. Evaluations
conducted on multiple benchmark datasets demonstrate superior performance,
while practical use-cases in interactive gaming and real-time digital-twin
manipulation illustrate HoloScene's broad applicability and effectiveness.
Project page: https://xiahongchi.github.io/HoloScene.

</details>


### [16] [CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval](https://arxiv.org/abs/2510.05586)
*Bin Kang,Bin Chen,Junjie Wang,Yulin Li,Junzhi Zhao,Zhuotao Tian*

Main category: cs.CV

TL;DR: CalibCLIP通过在视觉和文本空间中进行校准，解决了现有视觉语言模型（VLMs）中低贡献度词元过度捕获全局语义的问题，提高了图像检索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有视觉语言模型（VLMs）在图像检索任务中存在结构性限制，低贡献度词元可能过度捕获全局语义，压制了文本的判别性特征。

Method: 提出了CalibCLIP训练方法，包括在视觉空间中引入对比视觉增强器（CVE）以解耦和抑制主导词元，以及在文本空间中引入判别性概念校准器（DCC）以区分和增强判别性概念。

Result: 在七个基准测试的三个图像检索任务上实现了持续的性能提升。

Conclusion: CalibCLIP在解决现有VLMs的问题方面是有效的，显著提高了图像检索的准确性。

Abstract: Existing Visual Language Models (VLMs) suffer structural limitations where a
few low contribution tokens may excessively capture global semantics,
dominating the information aggregation process and suppressing the
discriminative features in text-driven image retrieval tasks. To address this,
we introduce \textbf{CalibCLIP}, a training-free method designed to calibrate
the suppressive effect of dominant tokens. Specifically, in the visual space,
we propose the Contrastive Visual Enhancer (CVE), which decouples visual
features into target and low information regions. Subsequently, it identifies
dominant tokens and dynamically suppresses their representations.In the textual
space, we introduce the Discriminative Concept Calibrator (DCC), which aims to
differentiate between general and discriminative concepts within the text
query. By mitigating the challenges posed by generic concepts and improving the
representations of discriminative concepts, DCC strengthens the differentiation
among similar samples. Finally, extensive experiments demonstrate consistent
improvements across seven benchmarks spanning three image retrieval tasks,
underscoring the effectiveness of CalibCLIP. Code is available at:
https://github.com/kangbin98/CalibCLIP

</details>


### [17] [Improving Chain-of-Thought Efficiency for Autoregressive Image Generation](https://arxiv.org/abs/2510.05593)
*Zeqi Gu,Markos Georgopoulos,Xiaoliang Dai,Marjan Ghazvininejad,Chu Wang,Felix Juefei-Xu,Kunpeng Li,Yujun Shi,Zecheng He,Zijian He,Jiawei Zhou,Abe Davis,Jialiang Wang*

Main category: cs.CV

TL;DR: ShortCoTI通过引入一个自适应奖励函数来鼓励更简洁的思维链，将提示推理长度减少了54%，同时保持或略微提高了图像生成质量，从而解决了视觉过度思考问题，提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有的链式思考（CoT）方法在图像生成中可能引入不必要的冗余（视觉过度思考），导致计算成本增加，甚至出现与原始提示相矛盾的细节。

Method: 提出ShortCoTI框架，通过一个根据任务难度自适应调整的奖励函数来鼓励更简洁的CoT序列，并将其融入强化学习范式。

Result: 在多个基准测试（T2I-CompBench, GenEval）上，提示推理长度减少了54%，同时保持或略微提高了图像质量。消除了冗余的解释和精炼，生成的推理提示简洁且语义丰富。

Conclusion: ShortCoTI提高了计算效率，同时不损害生成图像的保真度或视觉吸引力。

Abstract: Autoregressive multimodal large language models have recently gained
popularity for image generation, driven by advances in foundation models. To
enhance alignment and detail, newer approaches employ chain-of-thought (CoT)
reasoning, expanding user inputs into elaborated prompts prior to image
synthesis. However, this strategy can introduce unnecessary redundancy -- a
phenomenon we call visual overthinking -- which increases computational costs
and can introduce details that contradict the original prompt. In this work, we
explore how to generate more concise CoT sequences for more efficient image
generation. We introduce ShortCoTI, a lightweight optimization framework that
encourages more concise CoT while preserving output image quality. ShortCoTI
rewards more concise prompts with an adaptive function that scales according to
an estimated difficulty for each task. Incorporating this reward into a
reinforcement learning paradigm reduces prompt reasoning length by 54% while
maintaining or slightly improving quality metrics across multiple benchmarks
(T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates
verbose explanations and repetitive refinements, producing reasoning prompts
that are both concise and semantically rich. As a result, ShortCoTI improves
computational efficiency without compromising the fidelity or visual appeal of
generated images.

</details>


### [18] [HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection](https://arxiv.org/abs/2510.05609)
*Junwen Chen,Peilin Xiong,Keiji Yanai*

Main category: cs.CV

TL;DR: HOI-R1 利用 MLLM 的推理能力，通过纯文本解决了 HOID 任务，在 HICO-DET 数据集上实现了 2 倍的准确率提升。


<details>
  <summary>Details</summary>
Motivation: 现有 HOID 方法依赖 VLM 知识，但模型连接和训练复杂，且 MLLM 的推理能力在 HOID 任务上未被充分探索。

Method: 提出 HOI-R1，利用强化学习训练 MLLM，引入 HOI 推理过程和奖励函数，仅通过文本解决 HOID 任务，无需额外的检测模块。

Result: HOI-R1 在 HICO-DET 数据集上实现了 2 倍于基线方法的准确率，并具有良好的泛化能力。

Conclusion: HOI-R1 成功探索了 MLLM 在 HOID 任务上的潜力，提出了一种无需额外检测模块的纯文本解决方案。

Abstract: Recent Human-object interaction detection (HOID) methods highly require prior
knowledge from VLMs to enhance the interaction recognition capabilities. The
training strategies and model architectures for connecting the knowledge from
VLMs to the HOI instance representations from the object detector are
challenging, and the whole framework is complex for further development or
application. On the other hand, the inherent reasoning abilities of MLLMs on
human-object interaction detection are under-explored. Inspired by the recent
success of training MLLMs with reinforcement learning (RL) methods, we propose
HOI-R1 and first explore the potential of the language model on the HOID task
without any additional detection modules. We introduce an HOI reasoning process
and HOID reward functions to solve the HOID task by pure text. The results on
the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline
with great generalization ability. The source code is available at
https://github.com/cjw2021/HOI-R1.

</details>


### [19] [Efficient Conditional Generation on Scale-based Visual Autoregressive Models](https://arxiv.org/abs/2510.05610)
*Jiaqi Liu,Tao Huang,Chang Xu*

Main category: cs.CV

TL;DR: ECM是一种即插即用的框架，通过轻量级控制模块和分布式架构，提高了AR模型在空间条件生成方面的效率和控制能力。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归（AR）模型在处理复杂的空间条件生成任务时，需要对预训练模型进行微调，导致训练成本高昂。本研究旨在提出一种更高效的解决方案。

Method: 提出了一种名为ECM（Efficient Control Model）的即插即用框架。该框架包含一个轻量级控制模块，采用分布式架构，包括：1. 上下文感知注意力层，通过实时生成的token来优化条件特征。2. 共享门控前馈网络（FFN），以最大限度地利用其有限的容量并保证控制特征学习的一致性。此外，还提出了一种早期中心采样策略，优先学习早期的控制序列，并通过推理时的温度调度来弥补后期token训练不足的问题。

Result: ECM框架在基于AR的模型上进行了广泛的实验验证，证明了其在高保真度和多样性方面能够实现对图像生成的有效控制，优于现有基线方法，并显著提高了训练和推理效率。

Conclusion: ECM框架能够以更高的效率和更好的控制能力实现空间条件生成，为AR模型在图像合成领域的应用提供了新的解决方案。

Abstract: Recent advances in autoregressive (AR) models have demonstrated their
potential to rival diffusion models in image synthesis. However, for complex
spatially-conditioned generation, current AR approaches rely on fine-tuning the
pre-trained model, leading to significant training costs. In this paper, we
propose the Efficient Control Model (ECM), a plug-and-play framework featuring
a lightweight control module that introduces control signals via a distributed
architecture. This architecture consists of context-aware attention layers that
refine conditional features using real-time generated tokens, and a shared
gated feed-forward network (FFN) designed to maximize the utilization of its
limited capacity and ensure coherent control feature learning. Furthermore,
recognizing the critical role of early-stage generation in determining semantic
structure, we introduce an early-centric sampling strategy that prioritizes
learning early control sequences. This approach reduces computational cost by
lowering the number of training tokens per iteration, while a complementary
temperature scheduling during inference compensates for the resulting
insufficient training of late-stage tokens. Extensive experiments on
scale-based AR models validate that our method achieves high-fidelity and
diverse control over image generation, surpassing existing baselines while
significantly improving both training and inference efficiency.

</details>


### [20] [PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction](https://arxiv.org/abs/2510.05613)
*Ziqiao Meng,Qichao Wang,Zhiyang Dou,Zixing Song,Zhipeng Zhou,Irwin King,Peilin Zhao*

Main category: cs.CV

TL;DR: PointNSP是一个新颖的自回归点云生成框架，它采用了多尺度分解和从粗到精的预测方法，在生成质量、效率和可扩展性方面都取得了SOTA的成果。


<details>
  <summary>Details</summary>
Motivation: 自回归点云生成在质量上落后于扩散模型，原因是自回归模型强制引入了无序点集的排序，导致模型倾向于关注局部连续性而忽略全局结构属性（如对称性、拓扑结构和大规模几何规律）。

Method: PointNSP框架受 LOD（Level of Detail）原则启发，采用从粗到精的生成方式。它首先在低分辨率下保留全局形状结构，然后通过“下一尺度预测”范式逐步精炼高分辨率下的细节几何。这种多尺度分解方式使得自回归目标与点集的排列不变性保持一致，从而在避免固定排序的同时，实现了丰富的尺度内交互。

Result: PointNSP在ShapeNet数据集上取得了SOTA的生成质量，首次在自回归方法中实现了这一点。此外，它在参数、训练和推理效率方面也优于强大的扩散模型基线。在生成8192个点的密集点云时，PointNSP的优势更加明显，显示了其良好的可扩展性。

Conclusion: PointNSP通过多尺度分解和从粗到精的生成策略，成功克服了传统自回归点云生成方法的局限性，实现了高质量、高效率且可扩展的点云生成。

Abstract: Autoregressive point cloud generation has long lagged behind diffusion-based
approaches in quality. The performance gap stems from the fact that
autoregressive models impose an artificial ordering on inherently unordered
point sets, forcing shape generation to proceed as a sequence of local
predictions. This sequential bias emphasizes short-range continuity but
undermines the model's capacity to capture long-range dependencies, hindering
its ability to enforce global structural properties such as symmetry,
consistent topology, and large-scale geometric regularities. Inspired by the
level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a
coarse-to-fine generative framework that preserves global shape structure at
low resolutions and progressively refines fine-grained geometry at higher
scales through a next-scale prediction paradigm. This multi-scale factorization
aligns the autoregressive objective with the permutation-invariant nature of
point sets, enabling rich intra-scale interactions while avoiding brittle fixed
orderings. Experiments on ShapeNet show that PointNSP establishes
state-of-the-art (SOTA) generation quality for the first time within the
autoregressive paradigm. In addition, it surpasses strong diffusion-based
baselines in parameter, training, and inference efficiency. Finally, in dense
generation with 8,192 points, PointNSP's advantages become even more
pronounced, underscoring its scalability potential.

</details>


### [21] [TFM Dataset: A Novel Multi-task Dataset and Integrated Pipeline for Automated Tear Film Break-Up Segmentation](https://arxiv.org/abs/2510.05615)
*Guangrong Wan,Jun liu,Tang tang,Lianghao Shi,Wenjun Luo,TingTing Xu*

Main category: cs.CV

TL;DR: 本论文提出了TFM数据集和TF-Net模型，用于自动化干眼症诊断中的泪膜破裂分析，并构建了TF-Collab集成流水线，实现了实时分析。


<details>
  <summary>Details</summary>
Motivation: 自动化分析泪膜破裂（TFBU）对于诊断干眼症至关重要，但现有方法在缺乏标注数据集和集成解决方案的情况下面临挑战。

Method: 提出了TFM数据集（包含15个视频，6247帧），支持帧级分类、Placido环检测和TFBU区域分割三个任务。设计了TF-Net模型，结合MobileOne-mini骨干和特征金字塔网络，以提高计算效率和准确性。构建了TF-Collab集成流水线，按顺序执行帧分类、瞳孔区域定位和TFBU分割，实现全自动化分析。

Result: TF-Net在TFM分割子集上建立了基准性能，并与现有模型进行了比较。TF-Collab流水线能够完全自动化泪膜破裂分析。

Conclusion: 提出的TF-Net模型和TF-Collab流水线在眼表诊断领域展现了有效性，为未来的研究奠定了基础。

Abstract: Tear film break-up (TFBU) analysis is critical for diagnosing dry eye
syndrome, but automated TFBU segmentation remains challenging due to the lack
of annotated datasets and integrated solutions. This paper introduces the Tear
Film Multi-task (TFM) Dataset, the first comprehensive dataset for multi-task
tear film analysis, comprising 15 high-resolution videos (totaling 6,247
frames) annotated with three vision tasks: frame-level classification ('clear',
'closed', 'broken', 'blur'), Placido Ring detection, and pixel-wise TFBU area
segmentation. Leveraging this dataset, we first propose TF-Net, a novel and
efficient baseline segmentation model. TF-Net incorporates a MobileOne-mini
backbone with re-parameterization techniques and an enhanced feature pyramid
network to achieve a favorable balance between accuracy and computational
efficiency for real-time clinical applications. We further establish benchmark
performance on the TFM segmentation subset by comparing TF-Net against several
state-of-the-art medical image segmentation models. Furthermore, we design
TF-Collab, a novel integrated real-time pipeline that synergistically leverages
models trained on all three tasks of the TFM dataset. By sequentially
orchestrating frame classification for BUT determination, pupil region
localization for input standardization, and TFBU segmentation, TF-Collab fully
automates the analysis. Experimental results demonstrate the effectiveness of
the proposed TF-Net and TF-Collab, providing a foundation for future research
in ocular surface diagnostics. Our code and the TFM datasets are available at
https://github.com/glory-wan/TF-Net

</details>


### [22] [InstaGeo: Compute-Efficient Geospatial Machine Learning from Data to Deployment](https://arxiv.org/abs/2510.05617)
*Ibrahim Salihu Yusuf,Iffanice Houndayi,Rym Oualha,Mohamed Aziz Cherif,Kobby Panford-Quainoo,Arnu Pretorius*

Main category: cs.CV

TL;DR: InstaGeo是一个端到端的开源框架，可以自动处理地理空间数据，压缩模型大小，并实现模型部署，从而使研究级的地理空间基础模型（GFMs）成为实用的、低碳的工具。


<details>
  <summary>Details</summary>
Motivation: 现有GFMs在部署时面临数据处理流程自动化和模型尺寸过大的挑战，限制了它们在人道主义和环境应用中的广泛使用。

Method: InstaGeo框架集成了三个部分：(1) 自动化的数据处理流程，将原始卫星图像转换为可用于模型的现成数据集；(2) 针对特定任务的模型蒸馏，以创建更小、计算效率更高的模型；(3) 无缝部署为交互式Web地图应用程序。

Result: 使用InstaGeo，研究人员在洪水测绘、作物分割和沙漠蝗虫预测等任务上取得了与现有模型相当的结果，模型的mIoU差异很小（分别为-0.73 pp, -0.20 pp, +1.79 pp）。蒸馏后的模型尺寸减小高达8倍，计算量和二氧化碳排放量也显著降低。此外，InstaGeo还支持用户在一天内完成从原始数据到模型部署的全过程，并在作物分割任务上达到了60.65%的准确率，比之前的方法提高了12个百分点。

Conclusion: InstaGeo通过统一数据准备、模型压缩和部署，将研究级的GFMs转化为实用的、低碳的、可用于实时大规模地球观测的工具，推动地理空间AI向注重数据质量和应用驱动创新发展。

Abstract: Open-access multispectral imagery from missions like Landsat 8-9 and
Sentinel-2 has fueled the development of geospatial foundation models (GFMs)
for humanitarian and environmental applications. Yet, their deployment remains
limited by (i) the absence of automated geospatial data pipelines and (ii) the
large size of fine-tuned models. Existing GFMs lack workflows for processing
raw satellite imagery, and downstream adaptations often retain the full
complexity of the original encoder.
  We present InstaGeo, an open-source, end-to-end framework that addresses
these challenges by integrating: (1) automated data curation to transform raw
imagery into model-ready datasets; (2) task-specific model distillation to
derive compact, compute-efficient models; and (3) seamless deployment as
interactive web-map applications. Using InstaGeo, we reproduced datasets from
three published studies and trained models with marginal mIoU differences of
-0.73 pp for flood mapping, -0.20 pp for crop segmentation, and +1.79 pp for
desert locust prediction. The distilled models are up to 8x smaller than
standard fine-tuned counterparts, reducing FLOPs and CO2 emissions with minimal
accuracy loss.
  Leveraging InstaGeo's streamlined data pipeline, we also curated a larger
crop segmentation dataset, achieving a state-of-the-art mIoU of 60.65%, a 12 pp
improvement over prior baselines. Moreover, InstaGeo enables users to progress
from raw data to model deployment within a single working day.
  By unifying data preparation, model compression, and deployment, InstaGeo
transforms research-grade GFMs into practical, low-carbon tools for real-time,
large-scale Earth observation. This approach shifts geospatial AI toward data
quality and application-driven innovation. Source code, datasets, and model
checkpoints are available at:
https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML.git

</details>


### [23] [Beyond Spectral Peaks: Interpreting the Cues Behind Synthetic Image Detection](https://arxiv.org/abs/2510.05633)
*Sara Mandelli,Diego Vila-Portela,David Vázquez-Padín,Paolo Bestagini,Fernando Pérez-González*

Main category: cs.CV

TL;DR: 大多数基于深度学习的图像生成检测器并不依赖于频谱峰值，这挑战了该领域的一个普遍假设。


<details>
  <summary>Details</summary>
Motivation: 当前的深度学习取证工具通常被视为黑盒，其决策过程缺乏透明度，限制了其可解释性和可信度。特别是，尽管频谱峰值被广泛认为是合成图像的有力指标，但尚不清楚现有检测器是否真正依赖于这些峰值。

Method: 提出了一种从图像中移除频谱峰值的策略，并分析了该操作对几种现有检测器的影响。此外，还引入了一个仅依赖频率峰值的简单线性检测器，作为完全可解释的基线。

Result: 研究表明，大多数现有的深度学习检测器并不根本上依赖于频谱峰值。线性检测器提供了一个可解释的替代方案。

Conclusion: 大多数现有的深度学习检测器并不依赖于频谱峰值，这一发现与该领域的普遍看法相悖。这项工作强调了开发更透明、更可靠的取证工具的必要性。

Abstract: Over the years, the forensics community has proposed several deep
learning-based detectors to mitigate the risks of generative AI. Recently,
frequency-domain artifacts (particularly periodic peaks in the magnitude
spectrum), have received significant attention, as they have been often
considered a strong indicator of synthetic image generation. However,
state-of-the-art detectors are typically used as black-boxes, and it still
remains unclear whether they truly rely on these peaks. This limits their
interpretability and trust. In this work, we conduct a systematic study to
address this question. We propose a strategy to remove spectral peaks from
images and analyze the impact of this operation on several detectors. In
addition, we introduce a simple linear detector that relies exclusively on
frequency peaks, providing a fully interpretable baseline free from the
confounding influence of deep learning. Our findings reveal that most detectors
are not fundamentally dependent on spectral peaks, challenging a widespread
assumption in the field and paving the way for more transparent and reliable
forensic tools.

</details>


### [24] [Combined Hyperbolic and Euclidean Soft Triple Loss Beyond the Single Space Deep Metric Learning](https://arxiv.org/abs/2510.05643)
*Shozo Saeki,Minoru Kawahara,Hirohisa Aman*

Main category: cs.CV

TL;DR: 本文提出了一种结合双曲和欧几里得空间的代理损失函数CHEST，以解决深度度量学习（DML）在处理大规模数据集时的挑战，并在四个基准数据集上取得了新的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 深度度量学习（DML）旨在学习一个将数据映射到嵌入空间的神经网络，该空间可以表示数据点之间的语义相似性。双曲空间因其能够表示更丰富的结构（如树结构）而吸引人。然而，在双曲空间中的监督代理损失尚未有报道，而代理损失对于大规模数据集具有吸引力，因为它们具有较低的训练复杂度。

Method: 提出了一种结合双曲和欧几里得空间的代理损失函数CHEST，该函数由双曲和欧几里得空间中的代理损失以及基于双曲层次聚类的正则化损失组成。

Result: 将双曲和欧几里得空间相结合，可以提高两种空间的DML准确性和学习稳定性。CHEST损失在四个基准数据集上的评估结果显示，其性能达到了新的最先进水平。

Conclusion: CHEST损失有效地解决了在双曲空间中应用代理损失的挑战，并提高了DML的性能和稳定性，在大规模数据集上具有潜力。

Abstract: Deep metric learning (DML) aims to learn a neural network mapping data to an
embedding space, which can represent semantic similarity between data points.
Hyperbolic space is attractive for DML since it can represent richer
structures, such as tree structures. DML in hyperbolic space is based on
pair-based loss or unsupervised regularization loss. On the other hand,
supervised proxy-based losses in hyperbolic space have not been reported yet
due to some issues in applying proxy-based losses in a hyperbolic space.
However, proxy-based losses are attractive for large-scale datasets since they
have less training complexity. To address these, this paper proposes the
Combined Hyperbolic and Euclidean Soft Triple (CHEST) loss. CHEST loss is
composed of the proxy-based losses in hyperbolic and Euclidean spaces and the
regularization loss based on hyperbolic hierarchical clustering. We find that
the combination of hyperbolic and Euclidean spaces improves DML accuracy and
learning stability for both spaces. Finally, we evaluate the CHEST loss on four
benchmark datasets, achieving a new state-of-the-art performance.

</details>


### [25] [Ocular-Induced Abnormal Head Posture: Diagnosis and Missing Data Imputation](https://arxiv.org/abs/2510.05649)
*Saja Al-Dabet,Sherzod Turaev,Nazar Zaki,Arif O. Khan,Luai Eldweik*

Main category: cs.CV

TL;DR: 该研究提出两种深度学习框架以解决斜视等眼源性异常头姿（AHP）的诊断和数据缺失问题。


<details>
  <summary>Details</summary>
Motivation: 目前的AHP临床评估主观性强且病历不完整，给诊断带来挑战。

Method: 研究提出AHP-CADNet框架进行自动诊断，并结合课程学习方法进行数据填充，以提高在不完整数据下的诊断鲁棒性。

Result: AHP-CADNet在分类任务上准确率达96.9%-99.0%，连续变量预测误差低（MAE为0.103-0.199，R2>0.93）。数据填充框架在各临床变量上准确率达93.46%-99.78%，且临床依赖性建模显著提升了模型性能（p < 0.001）。

Conclusion: 两种框架能有效进行AHP的自动诊断，并能从临床数据缺失中恢复，验证了其在临床环境中的有效性。

Abstract: Ocular-induced abnormal head posture (AHP) is a compensatory mechanism that
arises from ocular misalignment conditions, such as strabismus, enabling
patients to reduce diplopia and preserve binocular vision. Early diagnosis
minimizes morbidity and secondary complications such as facial asymmetry;
however, current clinical assessments remain largely subjective and are further
complicated by incomplete medical records. This study addresses both challenges
through two complementary deep learning frameworks. First, AHP-CADNet is a
multi-level attention fusion framework for automated diagnosis that integrates
ocular landmarks, head pose features, and structured clinical attributes to
generate interpretable predictions. Second, a curriculum learning-based
imputation framework is designed to mitigate missing data by progressively
leveraging structured variables and unstructured clinical notes to enhance
diagnostic robustness under realistic data conditions. Evaluation on the
PoseGaze-AHP dataset demonstrates robust diagnostic performance. AHP-CADNet
achieves 96.9-99.0 percent accuracy across classification tasks and low
prediction errors for continuous variables, with MAE ranging from 0.103 to
0.199 and R2 exceeding 0.93. The imputation framework maintains high accuracy
across all clinical variables (93.46-99.78 percent with PubMedBERT), with
clinical dependency modeling yielding significant improvements (p < 0.001).
These findings confirm the effectiveness of both frameworks for automated
diagnosis and recovery from missing data in clinical settings.

</details>


### [26] [EduVerse: A User-Defined Multi-Agent Simulation Space for Education Scenario](https://arxiv.org/abs/2510.05650)
*Yiping Ma,Shiyu Hu,Buyuan Zhu,Yipei Wang,Yaxuan Kang,Shiqing Liu,Kang Hao Cheong*

Main category: cs.CV

TL;DR: EduVerse 是一个用户自定义的多智能体模拟空间，用于在虚拟教室中重现认知发展、群体互动和长期演化，旨在解决现有方法在捕捉开放式认知、动态社交互动、情感因素和多会话发展方面的局限性。该平台通过 CIE（认知-互动-演化）架构，在认知、情感和行为方面实现了个体一致性、真实互动和纵向适应，并允许用户通过界面进行交互。


<details>
  <summary>Details</summary>
Motivation: 现有的教育 AI 方法在模拟真实教室的复杂性方面存在不足，它们通常只关注短期或单智能体设置，难以同时捕捉开放式认知、动态社交互动、情感因素和多会话发展，限制了对教室复杂性的系统研究和跨任务重用。

Method: EduVerse 构建了一个基于 CIE（认知-互动-演化）架构的用户自定义多智能体模拟空间，支持环境、智能体和会话的自定义。它还提供了一个包含人类在内的交互界面，允许真实用户加入模拟空间，以实现人类与智能体之间的无缝集成。

Result: (1) 指令对齐：模拟的 IRF 费率 (0.28-0.64) 与真实教室 (0.37-0.49) 匹配；(2) 群体互动和角色分化：网络密度 (0.27-0.40)，约三分之一的同伴链接得以实现；(3) 跨会话演化：正向转换率 R+ 平均增加 11.7%，捕捉行为、情感和认知的纵向变化。

Conclusion: EduVerse 在真实性、可复现性和可解释性之间取得了平衡，为教育 AI 提供了一个可扩展的平台，能够模拟真实的课堂动态，并支持跨学科研究。

Abstract: Reproducing cognitive development, group interaction, and long-term evolution
in virtual classrooms remains a core challenge for educational AI, as real
classrooms integrate open-ended cognition, dynamic social interaction,
affective factors, and multi-session development rarely captured together.
Existing approaches mostly focus on short-term or single-agent settings,
limiting systematic study of classroom complexity and cross-task reuse. We
present EduVerse, the first user-defined multi-agent simulation space that
supports environment, agent, and session customization. A distinctive
human-in-the-loop interface further allows real users to join the space. Built
on a layered CIE (Cognition-Interaction-Evolution) architecture, EduVerse
ensures individual consistency, authentic interaction, and longitudinal
adaptation in cognition, emotion, and behavior-reproducing realistic classroom
dynamics with seamless human-agent integration. We validate EduVerse in
middle-school Chinese classes across three text genres, environments, and
multiple sessions. Results show: (1) Instructional alignment: simulated IRF
rates (0.28-0.64) closely match real classrooms (0.37-0.49), indicating
pedagogical realism; (2) Group interaction and role differentiation: network
density (0.27-0.40) with about one-third of peer links realized, while
human-agent tasks indicate a balance between individual variability and
instructional stability; (3) Cross-session evolution: the positive transition
rate R+ increase by 11.7% on average, capturing longitudinal shifts in
behavior, emotion, and cognition and revealing structured learning
trajectories. Overall, EduVerse balances realism, reproducibility, and
interpretability, providing a scalable platform for educational AI. The system
will be open-sourced to foster cross-disciplinary research.

</details>


### [27] [SD-MVSum: Script-Driven Multimodal Video Summarization Method and Datasets](https://arxiv.org/abs/2510.05652)
*Manolis Mylonas,Charalampia Zerva,Evlampios Apostolidis,Vasileios Mezaris*

Main category: cs.CV

TL;DR: 该研究提出了一种名为SD-MVSum的新方法，用于脚本驱动的多模态视频摘要，该方法能够同时考虑视频的视觉内容和语音内容，并利用加权跨模态注意力机制来衡量脚本与视频及语音之间的相关性，从而生成与用户脚本最相关的视频摘要。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有脚本驱动视频摘要方法仅考虑视频视觉内容而忽略语音内容的问题，提出一种能够同时处理视觉和语音信息的多模态方法。

Method: 提出了一种名为SD-MVSum的新方法，该方法使用加权跨模态注意力机制来建模脚本-视频和脚本-语音转录之间的依赖关系，以识别与用户提供的脚本最相关的视频片段。此外，还扩展了S-VideoXum和MrHiSum两个大型数据集，以适应脚本驱动的多模态视频摘要任务。

Result: 实验结果表明，SD-MVSum方法在脚本驱动和通用视频摘要任务上均具有竞争力，优于其他先进方法。

Conclusion: SD-MVSum是一种有效的脚本驱动多模态视频摘要方法，通过结合视觉和语音信息并利用跨模态注意力机制，能够生成与用户脚本高度相关的摘要。研究中扩展的数据集也为该领域的进一步研究提供了支持。

Abstract: In this work, we extend a recent method for script-driven video
summarization, originally considering just the visual content of the video, to
take into account the relevance of the user-provided script also with the
video's spoken content. In the proposed method, SD-MVSum, the dependence
between each considered pair of data modalities, i.e., script-video and
script-transcript, is modeled using a new weighted cross-modal attention
mechanism. This explicitly exploits the semantic similarity between the paired
modalities in order to promote the parts of the full-length video with the
highest relevance to the user-provided script. Furthermore, we extend two
large-scale datasets for video summarization (S-VideoXum, MrHiSum), to make
them suitable for training and evaluation of script-driven multimodal video
summarization methods. Experimental comparisons document the competitiveness of
our SD-MVSum method against other SOTA approaches for script-driven and generic
video summarization. Our new method and extended datasets are available at:
https://github.com/IDT-ITI/SD-MVSum.

</details>


### [28] [A Hierarchical Geometry-guided Transformer for Histological Subtyping of Primary Liver Cancer](https://arxiv.org/abs/2510.05657)
*Anwen Lu,Mingxin Liu,Yiping Jiao,Hongyi Gong,Geyang Xu,Jun Chen,Jun Xu*

Main category: cs.CV

TL;DR: ARGUS通过整合宏观、中观和微观的组织信息，并利用几何学原理，提高了肝癌组织学亚型的分类性能，有望成为临床诊断工具。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能充分利用全切片图像（WSIs）中的层次结构、肿瘤微环境（TME）和几何特征，导致肝癌组织学亚型分类性能不佳。

Method: ARGUS首先构建微观几何特征来表示细胞级别模式，然后设计了一个多层级的视野（FoVs）对齐模块来模拟宏观和中观的相互作用，最后通过几何先验引导融合策略将这些特征融合成一个联合表示。

Result: 在公共和私有队列上的广泛实验表明，ARGUS在肝癌的组织学亚型分类方面达到了最先进（SOTA）的性能。

Conclusion: ARGUS通过捕捉TME中的宏-中-微层次信息，提高了肝癌组织学亚型的分类性能，为临床实践中的原发性肝恶性肿瘤提供了一个有效的诊断工具。

Abstract: Primary liver malignancies are widely recognized as the most heterogeneous
and prognostically diverse cancers of the digestive system. Among these,
hepatocellular carcinoma (HCC) and intrahepatic cholangiocarcinoma (ICC) emerge
as the two principal histological subtypes, demonstrating significantly greater
complexity in tissue morphology and cellular architecture than other common
tumors. The intricate representation of features in Whole Slide Images (WSIs)
encompasses abundant crucial information for liver cancer histological
subtyping, regarding hierarchical pyramid structure, tumor microenvironment
(TME), and geometric representation. However, recent approaches have not
adequately exploited these indispensable effective descriptors, resulting in a
limited understanding of histological representation and suboptimal subtyping
performance. To mitigate these limitations, ARGUS is proposed to advance
histological subtyping in liver cancer by capturing the macro-meso-micro
hierarchical information within the TME. Specifically, we first construct a
micro-geometry feature to represent fine-grained cell-level pattern via a
geometric structure across nuclei, thereby providing a more refined and precise
perspective for delineating pathological images. Then, a Hierarchical
Field-of-Views (FoVs) Alignment module is designed to model macro- and
meso-level hierarchical interactions inherent in WSIs. Finally, the augmented
micro-geometry and FoVs features are fused into a joint representation via
present Geometry Prior Guided Fusion strategy for modeling holistic phenotype
interactions. Extensive experiments on public and private cohorts demonstrate
that our ARGUS achieves state-of-the-art (SOTA) performance in histological
subtyping of liver cancer, which provide an effective diagnostic tool for
primary liver malignancies in clinical practice.

</details>


### [29] [Teleportraits: Training-Free People Insertion into Any Scene](https://arxiv.org/abs/2510.05660)
*Jialu Gao,K J Joseph,Fernando De La Torre*

Main category: cs.CV

TL;DR: 提出了一种新颖的、无需训练的流水线，利用预训练的文本到图像扩散模型，实现了真实的人物图像插入。该方法通过结合反演技术和无分类器引导，实现了可感知感知的全局编辑，能够将人物无缝插入场景，并保证了高质量的个性化，保留了人物的身份、服装和身体特征，在各种复合场景图像中达到了最先进的效果。


<details>
  <summary>Details</summary>
Motivation: 当前的人物图像插入方法通常将定位和个性化视为独立问题，忽略了它们之间的相互联系，并且高度依赖于训练。然而，训练过程通常需要大量的标注数据，这在实际应用中可能难以获得。因此，开发一种无需训练即可实现高质量人物图像插入的方法具有重要的现实意义。

Method: 该方法利用预训练的文本到图像扩散模型，结合了反演技术和无分类器引导。通过一种新颖的掩模引导自注意力机制，实现了可感知感知的全局编辑，能够将人物无缝插入场景，同时保持人物的身份、服装和身体特征。

Result: 该方法在各种复合场景图像中取得了最先进的结果，实现了高质量的人物图像插入，并能很好地保留人物的身份信息。其无需训练的特性使其在实际应用中更具优势。

Conclusion: 提出了一种新颖的、无需训练的流水线，利用预训练的扩散模型实现了真实的人物图像插入。该方法通过独特的机制解决了人物定位和个性化的问题，并在实验中取得了优于现有方法的性能，同时保持了人物身份的高度一致性。

Abstract: The task of realistically inserting a human from a reference image into a
background scene is highly challenging, requiring the model to (1) determine
the correct location and poses of the person and (2) perform high-quality
personalization conditioned on the background. Previous approaches often treat
them as separate problems, overlooking their interconnections, and typically
rely on training to achieve high performance. In this work, we introduce a
unified training-free pipeline that leverages pre-trained text-to-image
diffusion models. We show that diffusion models inherently possess the
knowledge to place people in complex scenes without requiring task-specific
training. By combining inversion techniques with classifier-free guidance, our
method achieves affordance-aware global editing, seamlessly inserting people
into scenes. Furthermore, our proposed mask-guided self-attention mechanism
ensures high-quality personalization, preserving the subject's identity,
clothing, and body features from just a single reference image. To the best of
our knowledge, we are the first to perform realistic human insertions into
scenes in a training-free manner and achieve state-of-the-art results in
diverse composite scene images with excellent identity preservation in
backgrounds and subjects.

</details>


### [30] [When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach](https://arxiv.org/abs/2510.05661)
*Daniel Gonzálbez-Biosca,Josep Cabacas-Maso,Carles Ventura,Ismael Benito-Altamirano*

Main category: cs.CV

TL;DR: 本论文提出了一种新的多模态方法来解决自动视频剪辑问题，特别是针对多摄像头录制的古典音乐会。该方法将问题分解为“何时剪切”和“如何剪切”两个子任务。


<details>
  <summary>Details</summary>
Motivation: 与视频生成和场景理解等领域相比，自动视频剪辑，尤其是多摄像头古典音乐会剪辑，是一个被忽视的研究方向。

Method: 对于“何时剪切”问题，提出了一种新的多模态架构，结合音频信号的对数梅尔频谱图、可选的图像嵌入和标量时间特征，通过卷积-Transformer管道进行时序分割。对于“如何剪切”问题，使用基于CLIP的编码器替换了旧的骨干网络（如ResNet），并将干扰项选择限制在同一音乐会内的片段，以改进空间选择。数据集是通过伪标签方法构建的，将原始视频数据自动聚类成连贯的镜头片段。

Result: 所提出的模型在检测剪切点方面优于之前的基线方法，并在视觉镜头选择方面提供了有竞争力的结果。

Conclusion: 本研究提出的模型在多模态自动视频剪辑方面取得了进展，显著优于现有技术。

Abstract: Automated video editing remains an underexplored task in the computer vision
and multimedia domains, especially when contrasted with the growing interest in
video generation and scene understanding. In this work, we address the specific
challenge of editing multicamera recordings of classical music concerts by
decomposing the problem into two key sub-tasks: when to cut and how to cut.
Building on recent literature, we propose a novel multimodal architecture for
the temporal segmentation task (when to cut), which integrates log-mel
spectrograms from the audio signals, plus an optional image embedding, and
scalar temporal features through a lightweight convolutional-transformer
pipeline. For the spatial selection task (how to cut), we improve the
literature by updating from old backbones, e.g. ResNet, with a CLIP-based
encoder and constraining distractor selection to segments from the same
concert. Our dataset was constructed following a pseudo-labeling approach, in
which raw video data was automatically clustered into coherent shot segments.
We show that our models outperformed previous baselines in detecting cut points
and provide competitive visual shot selection, advancing the state of the art
in multimodal automated video editing.

</details>


### [31] [Development and Validation of a Low-Cost Imaging System for Seedling Germination Kinetics through Time-Cumulative Analysis](https://arxiv.org/abs/2510.05668)
*M. Torrente,A. Follador,A. Calcante,P. Casati,R. Oberti*

Main category: cs.CV

TL;DR: 本研究利用低成本的图像监测系统，结合新颖的时间序列图像分析方法，精确评估了R. solani病原菌对生菜种子萌发和早期生长的影响，结果显示病原菌显著降低了发芽率和幼苗活力。


<details>
  <summary>Details</summary>
Motivation: 研究旨在开发一种低成本、可扩展的非破坏性表型分析方法，以量化病原菌对植物早期生长发育的影响，并评估R. solani对生菜种子萌发和生长的具体影响。

Method: 开发了一种新颖的图像分析流程，该流程整合了形态和空间特征，并通过时间序列分析来识别和量化单个幼苗。该方法能够处理叶片重叠等复杂情况，即使在传统图像分析方法失效时也能实现精确计数和活力评估。

Result: R. solani感染显著降低了生菜种子的发芽率和幼苗的早期活力。所提出的图像分析方法在幼苗计数和活力评估方面表现出高精度，决定系数（R²）为0.98，均方根误差（RMSE）为1.12，尤其在叶片重叠的后期阶段表现优于传统方法。

Conclusion: 低成本成像硬件与先进计算工具相结合，能够以非破坏性和可扩展的方式获得植物表型数据。该研究开发的时间序列分析方法能够准确量化发芽种子和确定幼苗出现的时间，为研究病原菌等环境因素对植物生长的影响提供了一种有效工具。

Abstract: The study investigates the effects of R. solani inoculation on the
germination and early development of Lactuca sativa L. seeds using a low-cost,
image-based monitoring system. Multiple cameras were deployed to continuously
capture images of the germination process in both infected and control groups.
The objective was to assess the impact of the pathogen by analyzing germination
dynamics and growth over time. To achieve this, a novel image analysis pipeline
was developed. The algorithm integrates both morphological and spatial features
to identify and quantify individual seedlings, even under complex conditions
where traditional image analyses fails. A key innovation of the method lies in
its temporal integration: each analysis step considers not only the current
status but also their developmental across prior time points. This approach
enables robust discrimination of individual seedlings, especially when
overlapping leaves significantly hinder object separation. The method
demonstrated high accuracy in seedling counting and vigor assessment, even in
challenging scenarios characterized by dense and intertwined growth. Results
confirm that R. solani infection significantly reduces germination rates and
early seedling vigor. The study also validates the feasibility of combining
low-cost imaging hardware with advanced computational tools to obtain
phenotyping data in a non-destructive and scalable manner. The temporal
integration enabled accurate quantification of germinated seeds and precise
determination of seedling emergence timing. This approach proved particularly
effective in later stages of the experiment, where conventional segmentation
techniques failed due to overlapping or intertwined seedlings, making accurate
counting. The method achieved a coefficient of determination of 0.98 and a root
mean square error (RMSE) of 1.12, demonstrating its robustness and reliability.

</details>


### [32] [Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension](https://arxiv.org/abs/2510.05674)
*Jike Zhong,Yuxiang Lai,Xiaofeng Yang,Konstantinos Psounis*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Recent advances in language modeling have witnessed the rise of highly
desirable emergent capabilities, such as reasoning and in-context learning.
However, vision models have yet to exhibit comparable progress in these areas.
In this paper, we argue that this gap could stem from the lack of semantic and
contextual guidance in current vision transformer (ViT) training schemes, and
such a gap can be narrowed through the design of a semantic-grounded objective.
Specifically, we notice that individual words in natural language are
inherently semantic, and modeling directly on word tokens naturally learns a
realistic distribution. In contrast, ViTs rely on spatial patchification, which
inevitably lacks semantic information. To bridge this gap, we propose to
directly model "object" as the visual equivalence of "word," pushing the model
to learn the global context and semantics among visual elements. We investigate
our hypotheses via masked image modeling (MIM), a framework where our approach
can be readily tested by applying masks to visual objects rather than random
patches. Considerable evidence from qualitative and quantitative evaluations
reveals a key finding: object-level representation alone helps to learn a
real-world distribution, whereas pixel-averaging shortcuts are often learned
without it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual
question answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning
and contextual understanding gained with this simple objective. We hope our
study highlights the effectiveness of object-level encoding and provides a
plausible direction for developing stronger vision encoders and tokenizers.
Code and model will be publicly released. Keywords: Semantic Visual Tokenizer,
Vision Reasoning, In-context Learning, Multimodal Reasoning

</details>


### [33] [AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models](https://arxiv.org/abs/2510.05715)
*Shihao Zhu,Bohan Cao,Ziheng Ouyang,Zhen Li,Peng-Tao Jiang,Qibin Hou*

Main category: cs.CV

TL;DR: AgeBooth通过年龄提示融合和特定年龄的LoRA融合策略，在无需配对数据的情况下，提升了基于适配器的身份个性化模型在年龄控制方面的能力，实现了高质量的跨年龄人脸生成。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在生成身份一致图像时，年龄控制能力不足且需要昂贵的配对数据进行微调。AgeBooth旨在解决这个问题。

Method: 提出了一种名为AgeBooth的新方法，利用年龄提示融合和SVDMix技术进行年龄特定的LoRA融合，以克服对大量年龄标签数据的依赖，并实现高质量的中间年龄肖像生成。

Result: AgeBooth能够从单一参考图像生成逼真且身份一致的跨年龄人脸图像，并在实验中显示出优于现有编辑方法的年龄控制和视觉质量。

Conclusion: AgeBooth是一种有效的方法，可以在不使用昂贵的年龄变化数据集的情况下，提高基于适配器的身份个性化模型进行年龄控制的能力。

Abstract: Recent diffusion model research focuses on generating identity-consistent
images from a reference photo, but they struggle to accurately control age
while preserving identity, and fine-tuning such models often requires costly
paired images across ages. In this paper, we propose AgeBooth, a novel
age-specific finetuning approach that can effectively enhance the age control
capability of adapterbased identity personalization models without the need for
expensive age-varied datasets. To reduce dependence on a large amount of
age-labeled data, we exploit the linear nature of aging by introducing
age-conditioned prompt blending and an age-specific LoRA fusion strategy that
leverages SVDMix, a matrix fusion technique. These techniques enable
high-quality generation of intermediate-age portraits. Our AgeBooth produces
realistic and identity-consistent face images across different ages from a
single reference image. Experiments show that AgeBooth achieves superior age
control and visual quality compared to previous state-of-the-art editing-based
methods.

</details>


### [34] [Data Factory with Minimal Human Effort Using VLMs](https://arxiv.org/abs/2510.05722)
*Jiaojiao Ye,Jiaxing Zhong,Qian Xie,Yuzhou Zhou,Niki Trigoni,Andrew Markham*

Main category: cs.CV

TL;DR: 提出了一种结合ControlNet和视觉-语言模型（VLM）的免训练流程，用于生成带有像素级标签的合成图像，以解决传统数据增强方法在处理高层语义属性时的局限性。


<details>
  <summary>Details</summary>
Motivation: 传统数据增强方法在处理高层语义属性（如材料和纹理）时面临挑战，而现有基于扩散模型的方法计算成本高或性能受损。本文旨在提出一种更有效的解决方案。

Method: 引入了一种创新的免训练流程，该流程集成了预训练的ControlNet和视觉-语言模型（VLM），并辅以多路提示生成器、掩码生成器和高质量图像选择模块，以生成带有像素级标签的合成图像。

Result: 在PASCAL-5i和COCO-20i数据集上进行了实验，结果表明该方法具有良好的性能，并在单次语义分割任务上优于同期研究。

Conclusion: 所提出的免训练流程能够有效地生成带有像素级标签的合成图像，解决了传统方法的局限性，并在下游任务中展现出有前景的性能。

Abstract: Generating enough and diverse data through augmentation offers an efficient
solution to the time-consuming and labour-intensive process of collecting and
annotating pixel-wise images. Traditional data augmentation techniques often
face challenges in manipulating high-level semantic attributes, such as
materials and textures. In contrast, diffusion models offer a robust
alternative, by effectively utilizing text-to-image or image-to-image
transformation. However, existing diffusion-based methods are either
computationally expensive or compromise on performance. To address this issue,
we introduce a novel training-free pipeline that integrates pretrained
ControlNet and Vision-Language Models (VLMs) to generate synthetic images
paired with pixel-level labels. This approach eliminates the need for manual
annotations and significantly improves downstream tasks. To improve the
fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and
High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i
present promising performance and outperform concurrent work for one-shot
semantic segmentation.

</details>


### [35] [Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect](https://arxiv.org/abs/2510.05740)
*Amirtaha Amanzadi,Zahra Dehghanian,Hamid Beigy,Hamid R. Rabiee*

Main category: cs.CV

TL;DR: 现有的图像检测方法泛化能力不足，作者提出了OmniGen基准和FusionDetect方法来解决跨视觉域泛化的问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究过于关注跨生成器泛化，忽略了跨视觉域泛化的重要性。

Method: 提出OmniGen基准数据集，包含12个最先进的生成器。提出FusionDetect方法，结合CLIP和Dinov2两个基础模型的特征，构建统一的特征空间，以适应生成器内容和设计的变化。

Result: FusionDetect在现有基准测试上的准确率和精确率均优于现有方法，分别领先3.87%和6.13%。在OmniGen基准上，准确率提升4.48%，并对常见图像扰动具有出色的鲁棒性。

Conclusion: FusionDetect在图像检测方面取得了最先进的性能，OmniGen基准和FusionDetect方法为通用AI图像检测的研究提供了新的方向。

Abstract: The rapid development of generative models has made it increasingly crucial
to develop detectors that can reliably detect synthetic images. Although most
of the work has now focused on cross-generator generalization, we argue that
this viewpoint is too limited. Detecting synthetic images involves another
equally important challenge: generalization across visual domains. To bridge
this gap,we present the OmniGen Benchmark. This comprehensive evaluation
dataset incorporates 12 state-of-the-art generators, providing a more realistic
way of evaluating detector performance under realistic conditions. In addition,
we introduce a new method, FusionDetect, aimed at addressing both vectors of
generalization. FusionDetect draws on the benefits of two frozen foundation
models: CLIP & Dinov2. By deriving features from both complementary models,we
develop a cohesive feature space that naturally adapts to changes in both
thecontent and design of the generator. Our extensive experiments demonstrate
that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more
accurate than its closest competitor and 6.13% more precise on average on
established benchmarks, but also achieves a 4.48% increase in accuracy on
OmniGen,along with exceptional robustness to common image perturbations. We
introduce not only a top-performing detector, but also a new benchmark and
framework for furthering universal AI image detection. The code and dataset are
available at http://github.com/amir-aman/FusionDetect

</details>


### [36] [Dropping the D: RGB-D SLAM Without the Depth Sensor](https://arxiv.org/abs/2510.06216)
*Mert Kiray,Alican Karaomer,Benjamin Busam*

Main category: cs.CV

TL;DR: DropD-SLAM是一个实时单目SLAM系统，无需深度传感器即可达到RGB-D级别的准确度。


<details>
  <summary>Details</summary>
Motivation: 提出一种无需深度传感器即可实现RGB-D级别精度的SLAM系统。

Method: 使用预训练的单目深度估计、关键点检测和实例分割模块替代深度输入。通过膨胀实例掩码抑制动态对象，并为静态关键点分配预测深度值，将其投影到3D以形成度量尺度特征。然后将这些特征输入到标准的RGB-D SLAM后端进行跟踪和建图。

Result: 在TUM RGB-D基准测试中，DropD-SLAM在静态序列上达到7.4厘米的平均ATE，在动态序列上达到1.8厘米，在单GPU上以22 FPS运行。

Conclusion: 现代预训练视觉模型可以替代主动深度传感器，作为可靠的实时度量尺度来源，有助于构建更简单、更具成本效益的SLAM系统。

Abstract: We present DropD-SLAM, a real-time monocular SLAM system that achieves
RGB-D-level accuracy without relying on depth sensors. The system replaces
active depth input with three pretrained vision modules: a monocular metric
depth estimator, a learned keypoint detector, and an instance segmentation
network. Dynamic objects are suppressed using dilated instance masks, while
static keypoints are assigned predicted depth values and backprojected into 3D
to form metrically scaled features. These are processed by an unmodified RGB-D
SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM
attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences,
matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS
on a single GPU. These results suggest that modern pretrained vision models can
replace active depth sensors as reliable, real-time sources of metric scale,
marking a step toward simpler and more cost-effective SLAM systems.

</details>


### [37] [ALISE: Annotation-Free LiDAR Instance Segmentation for Autonomous Driving](https://arxiv.org/abs/2510.05752)
*Yongxuan Lyu,Guangfeng Jiang,Hongsi Liu,Jun Liu*

Main category: cs.CV

TL;DR: ALISE是一个新框架，可以在完全无监督的情况下对户外LiDAR点云进行实例分割，并且其性能优于使用2D边界框监督的方法。


<details>
  <summary>Details</summary>
Motivation: 手动标注LiDAR点云数据用于实例分割成本高昂且耗时，现有方法仍需人工标注，因此需要一个完全不需要人工标注的解决方案。

Method: ALISE首先使用视觉基础模型（VFMs）生成初始伪标签，然后通过一个结合2D和3D语义的空间-时间投票模块进行优化。此外，引入了基于2D先验的损失和基于3D语义一致性的原型对比损失进行语义监督，以提升特征学习。

Result: ALISE在无监督3D实例分割方面取得了显著的性能提升，达到了新的最先进水平，并且其性能超过了使用2D边界框监督的MWSIS方法。

Conclusion: ALISE成功实现了完全无监督的LiDAR实例分割，并通过创新的伪标签生成、优化策略和多样的语义监督方法，在性能上超越了现有监督方法。

Abstract: The manual annotation of outdoor LiDAR point clouds for instance segmentation
is extremely costly and time-consuming. Current methods attempt to reduce this
burden but still rely on some form of human labeling. To completely eliminate
this dependency, we introduce ALISE, a novel framework that performs LiDAR
instance segmentation without any annotations. The central challenge is to
generate high-quality pseudo-labels in a fully unsupervised manner. Our
approach starts by employing Vision Foundation Models (VFMs), guided by text
and images, to produce initial pseudo-labels. We then refine these labels
through a dedicated spatio-temporal voting module, which combines 2D and 3D
semantics for both offline and online optimization. To achieve superior feature
learning, we further introduce two forms of semantic supervision: a set of 2D
prior-based losses that inject visual knowledge into the 3D network, and a
novel prototype-based contrastive loss that builds a discriminative feature
space by exploiting 3D semantic consistency. This comprehensive design results
in significant performance gains, establishing a new state-of-the-art for
unsupervised 3D instance segmentation. Remarkably, our approach even
outperforms MWSIS, a method that operates with supervision from ground-truth
(GT) 2D bounding boxes by a margin of 2.53% in mAP (50.95% vs. 48.42%).

</details>


### [38] [OneVision: An End-to-End Generative Framework for Multi-view E-commerce Vision Search](https://arxiv.org/abs/2510.05759)
*Zexin Zheng,Huangyu Dai,Lingtao Mao,Xinyu Sun,Zihan Liang,Ben Chen,Yuqing Ding,Chenyi Lei,Wenwu Ou,Han Li,Kun Gai*

Main category: cs.CV

TL;DR: OneVision 是一个端到端生成框架，通过统一检索和个性化，解决了传统多阶段视觉搜索中的多视图表示不一致和优化目标冲突问题，提高了效率和用户体验。


<details>
  <summary>Details</summary>
Motivation: 传统视觉搜索采用多阶段级联架构，在效率和用户体验之间存在冲突，难以实现帕累托最优。

Method: 提出名为 OneVision 的端到端生成框架，基于 VRQ（视觉对齐残差量化编码）实现多视图表示的一致性，并采用多阶段语义对齐方案整合用户个性化信息。

Result: 离线评估与在线 MCA 相当，推理效率提高 21%。在线 A/B 测试中，点击率（CTR）提升 2.15%，转化率（CVR）提升 2.27%，订单量提升 3.12%。

Conclusion: 以语义 ID 为中心的生成架构可以统一检索和个性化，并简化服务路径。

Abstract: Traditional vision search, similar to search and recommendation systems,
follows the multi-stage cascading architecture (MCA) paradigm to balance
efficiency and conversion. Specifically, the query image undergoes feature
extraction, recall, pre-ranking, and ranking stages, ultimately presenting the
user with semantically similar products that meet their preferences. This
multi-view representation discrepancy of the same object in the query and the
optimization objective collide across these stages, making it difficult to
achieve Pareto optimality in both user experience and conversion. In this
paper, an end-to-end generative framework, OneVision, is proposed to address
these problems. OneVision builds on VRQ, a vision-aligned residual quantization
encoding, which can align the vastly different representations of an object
across multiple viewpoints while preserving the distinctive features of each
product as much as possible. Then a multi-stage semantic alignment scheme is
adopted to maintain strong visual similarity priors while effectively
incorporating user-specific information for personalized preference generation.
In offline evaluations, OneVision performs on par with online MCA, while
improving inference efficiency by 21% through dynamic pruning. In A/B tests, it
achieves significant online improvements: +2.15% item CTR, +2.27% CVR, and
+3.12% order volume. These results demonstrate that a semantic ID centric,
generative architecture can unify retrieval and personalization while
simplifying the serving pathway.

</details>


### [39] [A Novel Technique for Robust Training of Deep Networks With Multisource Weak Labeled Remote Sensing Data](https://arxiv.org/abs/2510.05760)
*Gianmarco Perantoni,Lorenzo Bruzzone*

Main category: cs.CV

TL;DR: 深度学习在遥感图像场景分类中表现出色，但需要大量标注数据且对标签错误敏感。本文提出一种结合少量可靠标签和多个弱标签源的方法，并设计了一种考虑各标签源可靠性的新颖训练策略，通过利用误差统计的转移矩阵来调整梯度权重，以解决遥感领域数据标注成本高的问题。实验结果证明了该方法的有效性、鲁棒性以及利用不可靠标签源的能力。


<details>
  <summary>Details</summary>
Motivation: 深度学习在遥感图像分类中虽然有效，但对大量标注数据和精确标签有较高要求，这在遥感领域难以满足，因为可靠标签获取成本高且数量有限，而低质量标签数据（如旧地图）则相对易得。

Method: 提出了一种结合少量可靠标签数据和多个弱标签源（如旧地图）的方法来生成多源标注数据集，并设计了一种新颖的训练策略。该策略利用描述各标签源误差统计特性的转移矩阵，将矩阵嵌入标签中，并在训练过程中根据各标签源的可靠性对标签进行加权。具体而言，该方法在梯度层面进行加权，使得每个样本对不同类别的优化贡献不同。

Result: 通过在不同数据集上进行实验验证，结果表明所提方法能够有效利用不可靠的标签源，并展现出良好的鲁棒性和能力。

Conclusion: 本文提出的方法能够有效解决遥感图像场景分类中高质量标注数据不足的问题，通过结合多种弱标签源并采用考虑标签源可靠性的训练策略，提高了模型的泛化能力和鲁棒性。

Abstract: Deep learning has gained broad interest in remote sensing image scene
classification thanks to the effectiveness of deep neural networks in
extracting the semantics from complex data. However, deep networks require
large amounts of training samples to obtain good generalization capabilities
and are sensitive to errors in the training labels. This is a problem in remote
sensing since highly reliable labels can be obtained at high costs and in
limited amount. However, many sources of less reliable labeled data are
available, e.g., obsolete digital maps. In order to train deep networks with
larger datasets, we propose both the combination of single or multiple weak
sources of labeled data with a small but reliable dataset to generate
multisource labeled datasets and a novel training strategy where the
reliability of each source is taken in consideration. This is done by
exploiting the transition matrices describing the statistics of the errors of
each source. The transition matrices are embedded into the labels and used
during the training process to weigh each label according to the related
source. The proposed method acts as a weighting scheme at gradient level, where
each instance contributes with different weights to the optimization of
different classes. The effectiveness of the proposed method is validated by
experiments on different datasets. The results proved the robustness and
capability of leveraging on unreliable source of labels of the proposed method.

</details>


### [40] [Mysteries of the Deep: Role of Intermediate Representations in Out of Distribution Detection](https://arxiv.org/abs/2510.05782)
*I. M. De la Jara,C. Rodriguez-Opazo,D. Teney,D. Ranasinghe,E. Abbasnejad*

Main category: cs.CV

TL;DR: 本研究提出一种利用预训练模型中间层信息进行OOD检测的新方法，通过熵准则自动选择信息丰富的中间层，无需OOD数据即可提升检测准确率。


<details>
  <summary>Details</summary>
Motivation: 现有OOD检测方法主要依赖预训练模型的最终层表示，忽略了中间层可能包含的丰富检测信号。

Method: 提出一种熵基准则，用于在无监督、无OOD数据的情况下，自动识别和选择能够提供最互补信息的预训练模型中间层，并利用这些中间层表示进行OOD检测。

Result: 通过选择性地结合中间层表示，在远距离OOD和近距离OOD基准测试中，OOD检测准确率相较于最先进的无监督方法，在不同模型架构和训练目标下，分别提升了高达10%和7%。

Conclusion: 预训练模型的中间层编码了丰富的OOD检测信号，利用这些信息并结合熵基准则可以显著提高OOD检测性能，为OOD检测研究开辟了新方向，并揭示了训练目标和模型架构对基于置信度的OOD检测方法的影响。

Abstract: Out-of-distribution (OOD) detection is essential for reliably deploying
machine learning models in the wild. Yet, most methods treat large pre-trained
models as monolithic encoders and rely solely on their final-layer
representations for detection. We challenge this wisdom. We reveal the
\textit{intermediate layers} of pre-trained models, shaped by residual
connections that subtly transform input projections, \textit{can} encode
\textit{surprisingly rich and diverse signals} for detecting distributional
shifts. Importantly, to exploit latent representation diversity across layers,
we introduce an entropy-based criterion to \textit{automatically} identify
layers offering the most complementary information in a training-free setting
-- \textit{without access to OOD data}. We show that selectively incorporating
these intermediate representations can increase the accuracy of OOD detection
by up to \textbf{$10\%$} in far-OOD and over \textbf{$7\%$} in near-OOD
benchmarks compared to state-of-the-art training-free methods across various
model architectures and training objectives. Our findings reveal a new avenue
for OOD detection research and uncover the impact of various training
objectives and model architectures on confidence-based OOD detection methods.

</details>


### [41] [Rasterized Steered Mixture of Experts for Efficient 2D Image Regression](https://arxiv.org/abs/2510.05814)
*Yi-Hsin Li,Thomas Sikora,Sebastian Knorr,Mårten Sjöström*

Main category: cs.CV

TL;DR: 本文提出了一种基于光栅化的优化策略，用于加速Steered Mixture of Interactions回归框架在二维图像处理中的应用，通过结合高斯核渲染效率和边缘感知门控机制，在保持模型稀疏性和重建质量的同时，显著提高了计算效率和内存效率。


<details>
  <summary>Details</summary>
Motivation: Steered Mixture of Experts (SMoE) 回归框架在图像重建、压缩、去噪和超分辨率等领域表现出色，但其高计算成本限制了实际应用。因此，需要一种更高效的优化方法。 

Method: 提出了一种结合光栅化高斯核渲染效率和SMoE边缘感知门控机制的光栅化优化策略。该策略用光栅化公式取代全局迭代优化，以加速二维图像回归，同时保持模型的稀疏性和重建质量。

Result: 与标准光栅化高斯核方法相比，该方法实现了更快的参数更新和更节省内存的模型表示。此外，该框架支持标准方法无法直接实现的超分辨率和图像去噪等应用。

Conclusion: 该方法结合了快速光栅化优化和SMoE的边缘感知结构，为二维图像处理任务提供了计算效率和重建保真度之间新的平衡。

Abstract: The Steered Mixture of Experts regression framework has demonstrated strong
performance in image reconstruction, compression, denoising, and
super-resolution. However, its high computational cost limits practical
applications. This work introduces a rasterization-based optimization strategy
that combines the efficiency of rasterized Gaussian kernel rendering with the
edge-aware gating mechanism of the Steered Mixture of Experts. The proposed
method is designed to accelerate two-dimensional image regression while
maintaining the model's inherent sparsity and reconstruction quality. By
replacing global iterative optimization with a rasterized formulation, the
method achieves significantly faster parameter updates and more
memory-efficient model representations. In addition, the proposed framework
supports applications such as native super-resolution and image denoising,
which are not directly achievable with standard rasterized Gaussian kernel
approaches. The combination of fast rasterized optimization with the edge-aware
structure of the Steered Mixture of Experts provides a new balance between
computational efficiency and reconstruction fidelity for two-dimensional image
processing tasks.

</details>


### [42] [Deformable Image Registration for Self-supervised Cardiac Phase Detection in Multi-View Multi-Disease Cardiac Magnetic Resonance Images](https://arxiv.org/abs/2510.05819)
*Sven Koehler,Sarah Kaye Mueller,Jonathan Kiekenap,Gerald Greil,Tarique Hussain,Samir Sarikouch,Florian André,Norbert Frey,Sandy Engelhardt*

Main category: cs.CV

TL;DR: 提出了一种新的自监督深度学习方法，用于在心血管磁共振成像（CMR）中检测关键帧，显著提高了检测精度，并能深入了解心肌运动。


<details>
  <summary>Details</summary>
Motivation: 现有的自动 CMR 关键帧检测方法仅基于左心室容积曲线，无法提供心肌运动的深入信息，并且在处理个体心动周期时存在困难。

Method: 利用密集可变形配准场计算一维运动描述符，该描述符能反映心肌整体收缩和舒张模式，并基于此运动曲线通过规则集确定关键帧。该方法是自监督的，并独立评估了短轴（SAX）和四腔切面（4CH）的电影 CMR。

Result: 该方法在 SAX 和 4CH 视图上均取得了显著的改进，ED 和 ES 关键帧的检测精度分别提高了 30% - 51% 和 11% - 47%。SAX 和 LAX 的平均 cFD 低于 1.31 帧和 1.73 帧，能够检测到 ED、ES 以及心动周期中的另外三个关键帧。

Conclusion: 所提出的自监督深度学习方法能够准确地检测 CMR 中的多个关键帧，实现跨患者和患者内部的心脏动力学的时间对齐分析，不受心动周期或相位长度的影响，并能提供比传统基于容积的方法更深入的心肌运动信息。

Abstract: Cardiovascular magnetic resonance (CMR) is the gold standard for assessing
cardiac function, but individual cardiac cycles complicate automatic temporal
comparison or sub-phase analysis. Accurate cardiac keyframe detection can
eliminate this problem. However, automatic methods solely derive end-systole
(ES) and end-diastole (ED) frames from left ventricular volume curves, which do
not provide a deeper insight into myocardial motion. We propose a
self-supervised deep learning method detecting five keyframes in short-axis
(SAX) and four-chamber long-axis (4CH) cine CMR. Initially, dense deformable
registration fields are derived from the images and used to compute a 1D motion
descriptor, which provides valuable insights into global cardiac contraction
and relaxation patterns. From these characteristic curves, keyframes are
determined using a simple set of rules. The method was independently evaluated
for both views using three public, multicentre, multidisease datasets. M&Ms-2
(n=360) dataset was used for training and evaluation, and M&Ms (n=345) and ACDC
(n=100) datasets for repeatability control. Furthermore, generalisability to
patients with rare congenital heart defects was tested using the German
Competence Network (GCN) dataset. Our self-supervised approach achieved
improved detection accuracy by 30% - 51% for SAX and 11% - 47% for 4CH in ED
and ES, as measured by cyclic frame difference (cFD), compared with the
volume-based approach. We can detect ED and ES, as well as three additional
keyframes throughout the cardiac cycle with a mean cFD below 1.31 frames for
SAX and 1.73 for LAX. Our approach enables temporally aligned inter- and
intra-patient analysis of cardiac dynamics, irrespective of cycle or phase
lengths. GitHub repository:
https://github.com/Cardio-AI/cmr-multi-view-phase-detection.git

</details>


### [43] [Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow](https://arxiv.org/abs/2510.05836)
*Ruyang Liu,Shangkun Sun,Haoran Tang,Ge Li,Wei Gao*

Main category: cs.CV

TL;DR: Flow4Agent利用光流信息来解决长视频理解中的冗余问题，并在多个视频MLLM基准测试中取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 长视频理解因时空冗余和多模态大语言模型（MLLM）有限的上下文长度而具有挑战性。现有方法依赖CLIP等语义先验来提取关键信息，但忽略了运动信息。

Method: Flow4Agent框架通过时间粒度优化（TGO）和运动令牌修剪（MTP）来解决长视频的冗余问题。TGO利用粗粒度光流先验和语义先验来优化帧级层次结构，过滤不相关场景信息。MTP利用细粒度光流信息修剪帧内冗余的视频令牌。

Result: Flow4Agent在长视频理解任务中表现优于现有方法，在Video-MME上达到64.7%，MLVU上达到71.4%，LongVideoBench上达到60.4%。

Conclusion: Flow4Agent通过引入光流运动先验，有效解决了长视频理解中的冗余问题，并在多个基准测试中取得了领先的性能。

Abstract: Long-form video understanding has always been a challenging problem due to
the significant redundancy in both temporal and spatial contents. This
challenge is further exacerbated by the limited context length of Multimodal
Large Language Models (MLLMs). To address this issue, many previous works have
attempted to extract key video information, where the "key" is typically
semantic-aware and heavily dependent on the CLIP model as prior. In this paper,
we propose Flow4Agent, a novel framework that pioneeringly incorporates motion
priors from optical flow to facilitate LLM-based long video understanding.
Flow4Agent mitigates the redundancy in long videos at both temporal and spatial
levels through two core modules: Temporal Granularity Optimization (TGO)
adaptively refines framelevel hierarchies, which first leverages coarse flow
priors to group similar visual contents and then applies semantic priors to
filter out highly irrelevant scene information. Motion Token Pruning (MTP)
further refines the intra-frame visual representations, pruning high-redundancy
video tokens using fine-grained optical flow information. Extensive experiments
demonstrate that our Flow4Agent outperforms existing methods across a wide
range of video MLLM benchmarks, especially for hour-level video understanding
tasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.

</details>


### [44] [acia-workflows: Automated Single-cell Imaging Analysis for Scalable and Deep Learning-based Live-cell Imaging Analysis Workflows](https://arxiv.org/abs/2510.05886)
*Johannes Seiffarth,Keitaro Kasahara,Michelle Bund,Benita Lückel,Richard D. Paul,Mathias Pesch,Lennart Witting,Michael Bott,Dietrich Kohlheyer,Katharina Nöh*

Main category: cs.CV

TL;DR: acia-workflows是一个集成了深度学习细胞分割和追踪功能的Jupyter Notebook平台，旨在简化和自动化高通量活细胞成像数据的分析，提高研究的可访问性、可重复性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 生命科学研究，特别是生物医学应用和生物处理，需要详细的时空表征活细胞，而高通通量活细胞成像技术产生了海量数据，阻碍了对这些数据的深入分析。现有的深度学习方法虽然能自动分析大量数据，但缺乏易于访问、灵活且用户友好的工作流程来支持常规的生物学研究。

Method: 该研究提出了一个名为acia-workflows的平台，该平台包含三个主要部分：1. acia（Automated live-Cell Imaging Analysis）Python库，支持模块化的图像分析流程，并提供八种深度学习分割和追踪方法。2. 工作流程，将图像分析流程、软件依赖、文档和可视化整合到单个Jupyter Notebook中，实现可访问、可重复和可扩展的分析。3. 应用工作流程，展示了在真实应用中的分析和定制能力，包括针对微流控活细胞成像实验的三种具体工作流程，用于生长速率比较以及对氧气条件变化的单个细胞动态响应的定量分析。

Result: 该平台提供了超过十个开源的应用工作流程，并公开在GitHub上，展示了其分析和定制能力。这些工作流程能够处理不同类型的微流控活细胞成像实验，从生长速率比较到精确的、以分钟为分辨率的动态细胞响应分析。

Conclusion: acia-workflows通过整合先进的深度学习工具和用户友好的Jupyter Notebook界面，解决了活细胞成像数据分析的挑战，为生物学研究人员提供了强大而易于使用的解决方案，促进了对单细胞动态的系统性研究。

Abstract: Live-cell imaging (LCI) technology enables the detailed spatio-temporal
characterization of living cells at the single-cell level, which is critical
for advancing research in the life sciences, from biomedical applications to
bioprocessing. High-throughput setups with tens to hundreds of parallel cell
cultivations offer the potential for robust and reproducible insights. However,
these insights are obscured by the large amount of LCI data recorded per
experiment. Recent advances in state-of-the-art deep learning methods for cell
segmentation and tracking now enable the automated analysis of such large data
volumes, offering unprecedented opportunities to systematically study
single-cell dynamics. The next key challenge lies in integrating these powerful
tools into accessible, flexible, and user-friendly workflows that support
routine application in biological research. In this work, we present
acia-workflows, a platform that combines three key components: (1) the
Automated live-Cell Imaging Analysis (acia) Python library, which supports the
modular design of image analysis pipelines offering eight deep learning
segmentation and tracking approaches; (2) workflows that assemble the image
analysis pipeline, its software dependencies, documentation, and visualizations
into a single Jupyter Notebook, leading to accessible, reproducible and
scalable analysis workflows; and (3) a collection of application workflows
showcasing the analysis and customization capabilities in real-world
applications. Specifically, we present three workflows to investigate various
types of microfluidic LCI experiments ranging from growth rate comparisons to
precise, minute-resolution quantitative analyses of individual dynamic cells
responses to changing oxygen conditions. Our collection of more than ten
application workflows is open source and publicly available at
https://github.com/JuBiotech/acia-workflows.

</details>


### [45] [BioAutoML-NAS: An End-to-End AutoML Framework for Multimodal Insect Classification via Neural Architecture Search on Large-Scale Biodiversity Data](https://arxiv.org/abs/2510.05888)
*Arefin Ittesafun Abian,Debopom Sutradhar,Md Rafi Ur Rashid,Reem E. Mohamed,Md Rafiqul Islam,Asif Karim,Kheng Cher Yeo,Sami Azam*

Main category: cs.CV

TL;DR: BioAutoML-NAS是一个结合图像和元数据的多模态模型，利用神经架构搜索（NAS）自动优化网络结构，并采用交替双层优化策略进行训练，实现了高精度的昆虫分类，优于现有多种先进方法。


<details>
  <summary>Details</summary>
Motivation: 昆虫分类在农业管理和生态研究中至关重要，但面临数据复杂性、类别不平衡和大规模数据集的挑战。

Method: 提出BioAutoML-NAS模型，该模型使用多模态数据（图像和元数据），应用神经架构搜索（NAS）为图像学习最佳操作，并通过多模态融合模块结合图像特征和类别元数据。采用交替双层优化策略训练，并使用零操作去除冗余连接，生成稀疏高效的网络。

Result: 在BIOSCAN-5M数据集上，BioAutoML-NAS达到了96.81%的准确率，性能优于现有技术约16%。在Insects-1M数据集上也取得了93.25%的准确率。

Conclusion: BioAutoML-NAS能够进行准确、可靠的昆虫分类，为现代可持续农业提供支持。

Abstract: Insect classification is important for agricultural management and ecological
research, as it directly affects crop health and production. However, this task
remains challenging due to the complex characteristics of insects, class
imbalance, and large-scale datasets. To address these issues, we propose
BioAutoML-NAS, the first BioAutoML model using multimodal data, including
images, and metadata, which applies neural architecture search (NAS) for images
to automatically learn the best operations for each connection within each
cell. Multiple cells are stacked to form the full network, each extracting
detailed image feature representations. A multimodal fusion module combines
image embeddings with metadata, allowing the model to use both visual and
categorical biological information to classify insects. An alternating bi-level
optimization training strategy jointly updates network weights and architecture
parameters, while zero operations remove less important connections, producing
sparse, efficient, and high-performing architectures. Extensive evaluation on
the BIOSCAN-5M dataset demonstrates that BioAutoML-NAS achieves 96.81%
accuracy, 97.46% precision, 96.81% recall, and a 97.05% F1 score, outperforming
state-of-the-art transfer learning, transformer, AutoML, and NAS methods by
approximately 16%, 10%, and 8% respectively. Further validation on the
Insects-1M dataset obtains 93.25% accuracy, 93.71% precision, 92.74% recall,
and a 93.22% F1 score. These results demonstrate that BioAutoML-NAS provides
accurate, confident insect classification that supports modern sustainable
farming.

</details>


### [46] [$\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection](https://arxiv.org/abs/2510.05891)
*Yanran Zhang,Bingyao Yu,Yu Zheng,Wenzhao Zheng,Yueqi Duan,Lei Chen,Jie Zhou,Jiwen Lu*

Main category: cs.CV

TL;DR: 该论文提出了一种名为 D$^3$QE 的新方法来检测视觉自回归模型生成的合成图像，该方法通过分析量化误差和码本频率分布的差异来实现。


<details>
  <summary>Details</summary>
Motivation: 视觉自回归模型在图像生成领域取得了显著进展，但同时也带来了新的合成图像检测挑战。以往的 GAN 或扩散模型检测方法难以适用于自回归模型独特的离散标记预测机制。

Method: 提出 D$^3$QE 方法，利用离散分布不匹配感知量化误差来检测自回归生成的图像。该方法通过分析真实图像和虚假图像中存在的码本的独特模式和频率分布偏差来实现。引入了一个离散分布不匹配感知 Transformer，将动态码本频率统计信息集成到注意力机制中，融合了语义特征和量化误差潜变量。

Result: 在 ARForensics 数据集上进行了评估，该数据集涵盖了 7 种主流的视觉自回归模型。实验结果表明，D$^3$QE 在检测准确性和泛化能力方面表现优越，并且对现实世界的扰动具有鲁棒性。

Conclusion: D$^3$QE 是一种有效且通用的方法，能够检测由各种视觉自回归模型生成的合成图像，并在真实世界场景中表现出良好的鲁棒性。

Abstract: The emergence of visual autoregressive (AR) models has revolutionized image
generation while presenting new challenges for synthetic image detection.
Unlike previous GAN or diffusion-based methods, AR models generate images
through discrete token prediction, exhibiting both marked improvements in image
synthesis quality and unique characteristics in their vector-quantized
representations. In this paper, we propose to leverage Discrete Distribution
Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated
image detection that exploits the distinctive patterns and the frequency
distribution bias of the codebook existing in real and fake images. We
introduce a discrete distribution discrepancy-aware transformer that integrates
dynamic codebook frequency statistics into its attention mechanism, fusing
semantic features and quantization error latent. To evaluate our method, we
construct a comprehensive dataset termed ARForensics covering 7 mainstream
visual AR models. Experiments demonstrate superior detection accuracy and
strong generalization of D$^3$QE across different AR models, with robustness to
real-world perturbations. Code is available at
\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.

</details>


### [47] [Efficient Universal Models for Medical Image Segmentation via Weakly Supervised In-Context Learning](https://arxiv.org/abs/2510.05899)
*Jiesi Hu,Yanwu Yang,Zhiyu Ye,Jinyan Zhou,Jianfeng Cao,Hanyang Peng,Ting Ma*

Main category: cs.CV

TL;DR: WS-ICL是一种新的ICL范式，使用弱提示（如边界框或点）而不是密集标签作为上下文，以减轻标注成本。


<details>
  <summary>Details</summary>
Motivation: 通用医学图像分割模型（如交互式和ICL模型）泛化能力强，但需要大量标注。交互式模型需要对每个图像进行重复的用户提示，而ICL依赖于密集的像素级标签。WS-ICL旨在解决这个问题。

Method: WS-ICL是一种新的ICL范式，利用弱提示（如边界框或点）代替密集标签作为上下文。

Result: 在三个独立的基准测试中，WS-ICL的性能与常规ICL模型相当，但标注成本却显著降低。此外，即使在交互式范式下，WS-ICL也极具竞争力。

Conclusion: WS-ICL是迈向量更高效、更统一的通用医学图像分割模型的有前景的一步。

Abstract: Universal models for medical image segmentation, such as interactive and
in-context learning (ICL) models, offer strong generalization but require
extensive annotations. Interactive models need repeated user prompts for each
image, while ICL relies on dense, pixel-level labels. To address this, we
propose Weakly Supervised In-Context Learning (WS-ICL), a new ICL paradigm that
leverages weak prompts (e.g., bounding boxes or points) instead of dense labels
for context. This approach significantly reduces annotation effort by
eliminating the need for fine-grained masks and repeated user prompting for all
images. We evaluated the proposed WS-ICL model on three held-out benchmarks.
Experimental results demonstrate that WS-ICL achieves performance comparable to
regular ICL models at a significantly lower annotation cost. In addition,
WS-ICL is highly competitive even under the interactive paradigm. These
findings establish WS-ICL as a promising step toward more efficient and unified
universal models for medical image segmentation. Our code and model are
publicly available at https://github.com/jiesihu/Weak-ICL.

</details>


### [48] [Kaputt: A Large-Scale Dataset for Visual Defect Detection](https://arxiv.org/abs/2510.05903)
*Sebastian Höfer,Dorian Henning,Artemij Amiranashvili,Douglas Morrison,Mariliza Tzes,Ingmar Posner,Marc Matvienko,Alessandro Rennola,Anton Milan*

Main category: cs.CV

TL;DR: 本文提出了一个大规模的物流场景缺陷检测数据集，解决了现有数据集在物体姿态和外观多样性方面的局限性，并验证了现有方法在此新场景下的不足。


<details>
  <summary>Details</summary>
Motivation: 工业异常检测主要集中在制造场景，而零售物流中的异常检测面临物体姿态和外观多样性带来的新挑战，现有方法在此场景下表现不佳。

Method: 创建了一个包含超过23万张图像、2.9万个缺陷实例、4.8万个独立物体的大规模数据集，并对现有最先进的异常检测方法进行了广泛评估。

Result: 现有最先进的异常检测方法在本文数据集上的AUROC分数最高仅为56.96%，表明现有方法难以应对物流场景中的姿态和外观变化。

Conclusion: 本文提出的新数据集为零售物流异常检测设定了新的基准，并鼓励对该具有挑战性的问题进行进一步研究。

Abstract: We present a novel large-scale dataset for defect detection in a logistics
setting. Recent work on industrial anomaly detection has primarily focused on
manufacturing scenarios with highly controlled poses and a limited number of
object categories. Existing benchmarks like MVTec-AD [6] and VisA [33] have
reached saturation, with state-of-the-art methods achieving up to 99.9% AUROC
scores. In contrast to manufacturing, anomaly detection in retail logistics
faces new challenges, particularly in the diversity and variability of object
pose and appearance. Leading anomaly detection methods fall short when applied
to this new setting. To bridge this gap, we introduce a new benchmark that
overcomes the current limitations of existing datasets. With over 230,000
images (and more than 29,000 defective instances), it is 40 times larger than
MVTec-AD and contains more than 48,000 distinct objects. To validate the
difficulty of the problem, we conduct an extensive evaluation of multiple
state-of-the-art anomaly detection methods, demonstrating that they do not
surpass 56.96% AUROC on our dataset. Further qualitative analysis confirms that
existing methods struggle to leverage normal samples under heavy pose and
appearance variation. With our large-scale dataset, we set a new benchmark and
encourage future research towards solving this challenging problem in retail
logistics anomaly detection. The dataset is available for download under
https://www.kaputt-dataset.com.

</details>


### [49] [Shaken or Stirred? An Analysis of MetaFormer's Token Mixing for Medical Imaging](https://arxiv.org/abs/2510.05971)
*Ron Keuth,Paul Kaftan,Mattias P. Heinrich*

Main category: cs.CV

TL;DR: MetaFormer架构在医学影像领域的应用研究，比较了不同Token Mixer在图像分类和语义分割任务上的表现，并探讨了预训练权重迁移的有效性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要集中在自然图像数据集上，对MetaFormer在医学影像领域的应用及不同Token Mixer的比较研究不足。

Method: 在MetaFormer架构下，系统性地分析了基于池化、卷积和注意力机制的Token Mixer在医学图像分类和语义分割任务上的表现，并在八个多样化的医学数据集上进行了评估，同时研究了预训练权重迁移的影响。

Result: 在图像分类任务中，低复杂度Token Mixer（如分组卷积或池化）效果良好，预训练权重迁移有效。在语义分割任务中，卷积Token Mixer的局部归纳偏置至关重要，其中分组卷积因其效率和参数量优势成为首选。

Conclusion: 分组卷积是医学影像语义分割任务中MetaFormer架构的理想选择，而对于图像分类任务，低复杂度Token Mixer已足够。预训练权重迁移在缓解数据稀缺和适应新Token Mixer方面均有价值。

Abstract: The generalization of the Transformer architecture via MetaFormer has
reshaped our understanding of its success in computer vision. By replacing
self-attention with simpler token mixers, MetaFormer provides strong baselines
for vision tasks. However, while extensively studied on natural image datasets,
its use in medical imaging remains scarce, and existing works rarely compare
different token mixers, potentially overlooking more suitable designs choices.
In this work, we present the first comprehensive study of token mixers for
medical imaging. We systematically analyze pooling-, convolution-, and
attention-based token mixers within the MetaFormer architecture on image
classification (global prediction task) and semantic segmentation (dense
prediction task). Our evaluation spans eight datasets covering diverse
modalities and common challenges in the medical domain. Given the prevalence of
pretraining from natural images to mitigate medical data scarcity, we also
examine transferring pretrained weights to new token mixers. Our results show
that, for classification, low-complexity token mixers (e.g. grouped convolution
or pooling) are sufficient, aligning with findings on natural images.
Pretrained weights remain useful despite the domain gap introduced by the new
token mixer. For segmentation, we find that the local inductive bias of
convolutional token mixers is essential. Grouped convolutions emerge as the
preferred choice, as they reduce runtime and parameter count compared to
standard convolutions, while the MetaFormer's channel-MLPs already provide the
necessary cross-channel interactions. Our code is available on GitHub.

</details>


### [50] [Diffusion Models for Low-Light Image Enhancement: A Multi-Perspective Taxonomy and Performance Analysis](https://arxiv.org/abs/2510.05976)
*Eashan Adhikarla,Yixin Liu,Brian D. Davison*

Main category: cs.CV

TL;DR: 本文对用于低光照图像增强（LLIE）的扩散模型进行了全面的分析和评估，并与GAN和Transformer等现有方法进行了比较，同时讨论了实际部署的挑战和未来方向。


<details>
  <summary>Details</summary>
Motivation: 低光照图像增强（LLIE）在安全关键应用中至关重要，现有方法的性能瓶颈亟待突破，因此需要对新兴的扩散模型在LLIE领域的应用进行深入研究。

Method: 本文提出了一种多视角分类法，将LLIE方法分为六类：内在分解、光谱与潜在、加速、引导、多模态和自主。通过比较扩散模型与GAN和Transformer等SOTA方法在定性故障模式、基准不一致性、可解释性、泛化能力和推理效率等方面的表现，并讨论了实际部署的约束和伦理考量。

Result: 扩散模型在LLIE任务中展现出潜力，但仍面临实际部署的挑战。本文的评估揭示了不同方法之间的权衡，并为未来的研究指明了方向。

Conclusion: 本文对LLIE领域的扩散模型进行了全面的分析和评估，为下一代基于扩散模型的LLIE研究提供了指导，并指出了新颖的条件、实时适应和基础模型等未来研究方向。

Abstract: Low-light image enhancement (LLIE) is vital for safety-critical applications
such as surveillance, autonomous navigation, and medical imaging, where
visibility degradation can impair downstream task performance. Recently,
diffusion models have emerged as a promising generative paradigm for LLIE due
to their capacity to model complex image distributions via iterative denoising.
This survey provides an up-to-date critical analysis of diffusion models for
LLIE, distinctively featuring an in-depth comparative performance evaluation
against Generative Adversarial Network and Transformer-based state-of-the-art
methods, a thorough examination of practical deployment challenges, and a
forward-looking perspective on the role of emerging paradigms like foundation
models. We propose a multi-perspective taxonomy encompassing six categories:
Intrinsic Decomposition, Spectral & Latent, Accelerated, Guided, Multimodal,
and Autonomous; that map enhancement methods across physical priors,
conditioning schemes, and computational efficiency. Our taxonomy is grounded in
a hybrid view of both the model mechanism and the conditioning signals. We
evaluate qualitative failure modes, benchmark inconsistencies, and trade-offs
between interpretability, generalization, and inference efficiency. We also
discuss real-world deployment constraints (e.g., memory, energy use) and
ethical considerations. This survey aims to guide the next generation of
diffusion-based LLIE research by highlighting trends and surfacing open
research questions, including novel conditioning, real-time adaptation, and the
potential of foundation models.

</details>


### [51] [A Dynamic Mode Decomposition Approach to Morphological Component Analysis](https://arxiv.org/abs/2510.05977)
*Owen T. Huber,Raghu G. Raj,Tianyu Chen,Zacharie I. Idriss*

Main category: cs.CV

TL;DR: 本论文提出了一种利用视频内容动态变化的特征来适应性地表示视频的新方法，通过对动态模式分解的特征值进行聚类，学习视频表示，并用于分离视频中结构不同的组成部分。


<details>
  <summary>Details</summary>
Motivation: 提出一种新的自适应视频表示方法，利用场景内容动态变化来学习表示，以分离视频中结构不同的组成部分。

Method: 提出了一种新的动态形态分量分析（DMCA）算法，该算法扩展了形态分量分析（MCA），通过引入数据驱动的特征值聚类技术来获得字典，用于分离信号中的不同源。

Result: 通过在静态图像上的示例、Adobe 240fps数据集上的视频去噪应用、海洋背景下的目标信号增强以及逆合成孔径雷达图像中分离自行车和风杂波的实例，展示了DMCA的有效性。

Conclusion: DMCA能够有效地处理视频信号，并在去噪、目标增强和杂波分离等多种应用中表现出色。

Abstract: This paper introduces a novel methodology of adapting the representation of
videos based on the dynamics of their scene content variation. In particular,
we demonstrate how the clustering of dynamic mode decomposition eigenvalues can
be leveraged to learn an adaptive video representation for separating
structurally distinct morphologies of a video. We extend the morphological
component analysis (MCA) algorithm, which uses multiple predefined incoherent
dictionaries and a sparsity prior to separate distinct sources in signals, by
introducing our novel eigenspace clustering technique to obtain data-driven MCA
dictionaries, which we call dynamic morphological component analysis (DMCA).
After deriving our novel algorithm, we offer a motivational example of DMCA
applied to a still image, then demonstrate DMCA's effectiveness in denoising
applications on videos from the Adobe 240fps dataset. Afterwards, we provide an
example of DMCA enhancing the signal-to-noise ratio of a faint target summed
with a sea state, and conclude the paper by applying DMCA to separate a bicycle
from wind clutter in inverse synthetic aperture radar images.

</details>


### [52] [Diffusion-Based Image Editing for Breaking Robust Watermarks](https://arxiv.org/abs/2510.05978)
*Yunyi Ni,Finn Carter,Ze Niu,Emily Davis,Bo Zhang*

Main category: cs.CV

TL;DR: 扩散模型能够有效破解现有的鲁棒图像水印技术，即使是深度学习方法也不例外。


<details>
  <summary>Details</summary>
Motivation: 现有的鲁棒图像水印技术面临着由扩散模型驱动的图像生成和编辑技术带来的新威胁，这些技术能够有效去除水印同时保持图像的视觉内容。

Method: 提出了一种基于扩散模型的“图像再生”攻击方法，该方法能够擦除水印，并引入了一种引导扩散攻击，专门针对水印信号进行攻击，以降低水印的可检测性。理论上证明了扩散变换能够消除水印信息与水印图像之间的互信息，导致解码失败。

Result: 在多种先进的水印方案（包括StegaStamp、TrustMark和VINE）上进行了实验评估，证明了在攻击后，水印恢复率接近于零，同时保持了再生图像的高视觉保真度。

Conclusion: 当前鲁棒水印技术在应对生成模型攻击方面存在根本性漏洞，需要新的水印策略来适应生成式AI时代。

Abstract: Robust invisible watermarking aims to embed hidden information into images
such that the watermark can survive various image manipulations. However, the
rise of powerful diffusion-based image generation and editing techniques poses
a new threat to these watermarking schemes. In this paper, we present a
theoretical study and method demonstrating that diffusion models can
effectively break robust image watermarks that were designed to resist
conventional perturbations. We show that a diffusion-driven ``image
regeneration'' process can erase embedded watermarks while preserving
perceptual image content. We further introduce a novel guided diffusion attack
that explicitly targets the watermark signal during generation, significantly
degrading watermark detectability. Theoretically, we prove that as an image
undergoes sufficient diffusion-based transformation, the mutual information
between the watermarked image and the embedded watermark payload vanishes,
resulting in decoding failure. Experimentally, we evaluate our approach on
multiple state-of-the-art watermarking schemes (including the deep
learning-based methods StegaStamp, TrustMark, and VINE) and demonstrate
near-zero watermark recovery rates after attack, while maintaining high visual
fidelity of the regenerated images. Our findings highlight a fundamental
vulnerability in current robust watermarking techniques against generative
model-based attacks, underscoring the need for new watermarking strategies in
the era of generative AI.

</details>


### [53] [Detection and Measurement of Hailstones with Multimodal Large Language Models](https://arxiv.org/abs/2510.06008)
*Moritz Alker,David C. Schedl,Andreas Stöckl*

Main category: cs.CV

TL;DR: 利用预训练的多模态大语言模型，结合社交媒体和新闻图片，可以有效检测和测量冰雹的大小。


<details>
  <summary>Details</summary>
Motivation: 研究利用社交媒体和新闻图片，通过预训练的多模态大语言模型来检测和测量冰雹，以期为灾害评估提供新的数据来源。

Method: 收集了来自奥地利2022年1月至2024年9月期间的474张包含冰雹的众包图像，冰雹最大直径在2到11厘米之间。研究人员估计了冰雹的直径，并比较了四种不同的模型，采用了单阶段和两阶段的提示策略，其中两阶段策略利用了图像中参照物（如人手）的尺寸线索。

Result: 预训练模型在测量冰雹直径方面展现出潜力，最佳模型的平均绝对误差为1.12厘米。与单阶段提示相比，两阶段提示提高了大多数模型的可靠性。

Conclusion: 研究表明，即使没有进行微调，这些现成的模型也可以通过从社交媒体图像中提取有意义且空间密集的信息，来补充传统的冰雹传感器，从而实现对严重天气事件更快、更详细的评估。未来，自动实时收集社交媒体和其他来源的图像数据将使该方法能够直接应用于未来的冰雹事件。

Abstract: This study examines the use of social media and news images to detect and
measure hailstones, utilizing pre-trained multimodal large language models. The
dataset for this study comprises 474 crowdsourced images of hailstones from
documented hail events in Austria, which occurred between January 2022 and
September 2024. These hailstones have maximum diameters ranging from 2 to 11cm.
We estimate the hail diameters and compare four different models utilizing
one-stage and two-stage prompting strategies. The latter utilizes additional
size cues from reference objects, such as human hands, within the image. Our
results show that pretrained models already have the potential to measure
hailstone diameters from images with an average mean absolute error of 1.12cm
for the best model. In comparison to a single-stage prompt, two-stage prompting
improves the reliability of most models. Our study suggests that these
off-the-shelf models, even without fine-tuning, can complement traditional hail
sensors by extracting meaningful and spatially dense information from social
media imagery, enabling faster and more detailed assessments of severe weather
events. The automated real-time image harvesting from social media and other
sources remains an open task, but it will make our approach directly applicable
to future hail events.

</details>


### [54] [Continual Learning for Image Captioning through Improved Image-Text Alignment](https://arxiv.org/abs/2510.06009)
*Bertram Taetz,Gal Bordelius*

Main category: cs.CV

TL;DR: 该论文提出了一种新的多损失框架，用于持续图像字幕生成，通过基于提示的持续学习和对比度对齐来整合语义指导，以解决灾难性遗忘和视觉概念与语言对齐的挑战。


<details>
  <summary>Details</summary>
Motivation: 持续学习设置下的图像字幕生成面临灾难性遗忘以及视觉概念随时间演变的语言对齐困难的挑战。

Method: 该方法基于预训练的 ViT-GPT-2 主干，结合了标准的交叉熵损失以及三种附加组件：(1) 基于提示的余弦相似度损失，使图像嵌入与编码对象、属性和动作的合成提示对齐；(2) CLIP 风格的损失，促进图像嵌入与目标字幕嵌入的对齐；(3) 语言引导的对比损失，采用三元组损失来增强任务之间的类别级可辨别性。

Result: 该方法减轻了灾难性遗忘，同时与最先进的方法相比，实现了更好的语义字幕对齐。

Conclusion: 所提出的多损失框架有效解决了持续图像字幕生成中的挑战，在减轻灾难性遗忘和提高字幕语义对齐方面取得了显著成效。

Abstract: Generating accurate and coherent image captions in a continual learning
setting remains a major challenge due to catastrophic forgetting and the
difficulty of aligning evolving visual concepts with language over time. In
this work, we propose a novel multi-loss framework for continual image
captioning that integrates semantic guidance through prompt-based continual
learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone,
our approach combines standard cross-entropy loss with three additional
components: (1) a prompt-based cosine similarity loss that aligns image
embeddings with synthetically constructed prompts encoding objects, attributes,
and actions; (2) a CLIP-style loss that promotes alignment between image
embeddings and target caption embedding; and (3) a language-guided contrastive
loss that employs a triplet loss to enhance class-level discriminability
between tasks. Notably, our approach introduces no additional overhead at
inference time and requires no prompts during caption generation. We find that
this approach mitigates catastrophic forgetting, while achieving better
semantic caption alignment compared to state-of-the-art methods. The code can
be found via the following link https://github.com/
Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.

</details>


### [55] [Emergent AI Surveillance: Overlearned Person Re-Identification and Its Mitigation in Law Enforcement Context](https://arxiv.org/abs/2510.06026)
*An Thi Nguyen,Radina Stoykova,Eric Arazo*

Main category: cs.CV

TL;DR: 即使在没有人类对象的训练数据集中，通用实例搜索模型也可能通过过度学习获得识别特定个人的能力。本研究评估了两种技术安全措施——索引排除和损失混淆——以降低这种能力。虽然这些措施可以显著降低重新识别的准确性，同时保持对非人员对象的检索性能，但仍存在漏洞，例如可以使用部分人员图像来规避。这引发了关于具有新兴识别能力的系统的分类和监管以及如何防止此类能力在看似良性的应用程序中开发的紧迫问题。


<details>
  <summary>Details</summary>
Motivation: 通用实例搜索模型可能意外地获得识别特定个人的能力，这引发了对数据隐私和个人身份识别的担忧，并且目前缺乏明确的去识别标准。

Method: 评估了两种技术安全措施：索引排除和损失混淆，以降低模型进行人员重新识别的能力。

Result: 结合使用索引排除和损失混淆可以将人员重新识别的准确性降低到 2% 以下，同时保持 82% 的非人员对象检索性能。然而，研究也发现，通过部分人员图像等方式仍然可以规避这些缓解措施。

Conclusion: 虽然提出的技术措施可以减少模型的人员重新识别能力，但仍然存在漏洞。这突出表明，在人工智能治理和数据保护交叉领域，需要解决关于如何对具有新兴识别能力的系统进行分类和监管，以及需要制定哪些技术标准来防止在看似良性的应用程序中出现此类能力的问题。

Abstract: Generic instance search models can dramatically reduce the manual effort
required to analyze vast surveillance footage during criminal investigations by
retrieving specific objects of interest to law enforcement. However, our
research reveals an unintended emergent capability: through overlearning, these
models can single out specific individuals even when trained on datasets
without human subjects. This capability raises concerns regarding
identification and profiling of individuals based on their personal data, while
there is currently no clear standard on how de-identification can be achieved.
We evaluate two technical safeguards to curtail a model's person
re-identification capacity: index exclusion and confusion loss. Our experiments
demonstrate that combining these approaches can reduce person re-identification
accuracy to below 2% while maintaining 82% of retrieval performance for
non-person objects. However, we identify critical vulnerabilities in these
mitigations, including potential circumvention using partial person images.
These findings highlight urgent regulatory questions at the intersection of AI
governance and data protection: How should we classify and regulate systems
with emergent identification capabilities? And what technical standards should
be required to prevent identification capabilities from developing in seemingly
benign applications?

</details>


### [56] [Universal Neural Architecture Space: Covering ConvNets, Transformers and Everything in Between](https://arxiv.org/abs/2510.06035)
*Ondřej Týbl,Lukáš Neumann*

Main category: cs.CV

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: We introduce Universal Neural Architecture Space (UniNAS), a generic search
space for neural architecture search (NAS) which unifies convolutional
networks, transformers, and their hybrid architectures under a single, flexible
framework. Our approach enables discovery of novel architectures as well as
analyzing existing architectures in a common framework. We also propose a new
search algorithm that allows traversing the proposed search space, and
demonstrate that the space contains interesting architectures, which, when
using identical training setup, outperform state-of-the-art hand-crafted
architectures. Finally, a unified toolkit including a standardized training and
evaluation protocol is introduced to foster reproducibility and enable fair
comparison in NAS research. Overall, this work opens a pathway towards
systematically exploring the full spectrum of neural architectures with a
unified graph-based NAS perspective.

</details>


### [57] [VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization](https://arxiv.org/abs/2510.06040)
*Xinye Cao,Hongcan Guo,Jiawen Qian,Guoshun Nan,Chao Wang,Yuqi Pan,Tianhao Hou,Xiaojuan Wang,Yutong Gao*

Main category: cs.CV

TL;DR: 通过分段、添加标题和聚类长视频来创建一个分层的树状结构，以克服现有方法在视频理解方面的局限性，并提出一种名为T-GRPO的基于树的强化学习方法来精确地定位关键帧。


<details>
  <summary>Details</summary>
Motivation: 为了实现对长视频的端到端理解，需要克服现有方法在处理冗余信息和动态适应复杂结构方面的挑战。

Method: 提出了一种名为VideoMiner的方法，它通过迭代地分割、添加标题和聚类长视频来形成一个分层的树状结构。并引入了一种名为T-GRPO的基于树的强化学习方法来精确地定位关键帧。

Result: VideoMiner在所有长视频理解任务上都取得了优越的性能，并且T-GRPO能够引导模型生成推理链，同时动态调整树的扩展深度以提高准确性和效率。

Conclusion: VideoMiner和T-GRPO的结合成功地解决了长视频理解中的挑战，并在准确性和效率方面取得了显著的改进。

Abstract: Understanding hour-long videos with multi-modal large language models
(MM-LLMs) enriches the landscape of human-centered AI applications. However,
for end-to-end video understanding with LLMs, uniformly sampling video frames
results in LLMs being overwhelmed by a vast amount of irrelevant information as
video length increases. Existing hierarchical key frame extraction methods
improve the accuracy of video understanding but still face two critical
challenges. 1) How can the interference of extensive redundant information in
long videos be mitigated? 2) How can a model dynamically adapt to complex
hierarchical structures while accurately identifying key frames? To address
these issues, we propose VideoMiner, which iteratively segments, captions, and
clusters long videos, forming a hierarchical tree structure. The proposed
VideoMiner progresses from long videos to events to frames while preserving
temporal coherence, effectively addressing the first challenge. To precisely
locate key frames, we introduce T-GRPO, a tree-based group relative policy
optimization in reinforcement learning method that guides the exploration of
the VideoMiner. The proposed T-GRPO is specifically designed for tree
structures, integrating spatiotemporal information at the event level while
being guided by the question, thus solving the second challenge. We achieve
superior performance in all long-video understanding tasks and uncover several
interesting insights. Our proposed T-GRPO surprisingly incentivizes the model
to spontaneously generate a reasoning chain. Additionally, the designed tree
growth auxin dynamically adjusts the expansion depth, obtaining accuracy and
efficiency gains. The code is publicly available at
https://github.com/caoxinye/VideoMiner.

</details>


### [58] [GLVD: Guided Learned Vertex Descent](https://arxiv.org/abs/2510.06046)
*Pol Caselles Rico,Francesc Moreno Noguer*

Main category: cs.CV

TL;DR: GLVD是一种结合了学习顶点下降（LVD）、逐顶点神经场优化和动态预测3D关键点的全局结构引导的混合方法，用于从少量图像进行3D面部重建，无需密集3D监督，实现了高效且适应性强的几何重建，并在单视图设置中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有的3D面部建模方法通常依赖于3D可变形模型（3DMM），这会限制表示能力为固定的先验形状。基于优化的方法虽然重建质量高，但计算成本昂贵。

Method: GLVD通过整合逐顶点神经场优化和来自动态预测的3D关键点的全局结构引导来扩展学习顶点下降（LVD），并结合相对空间编码，迭代地优化网格顶点，而无需密集3D监督。

Result: GLVD在单视图设置中实现了最先进的性能，在多视图场景中也具有很强的竞争力，同时大大缩短了推理时间。

Conclusion: GLVD通过结合神经场优化和全局结构引导，实现了高效、自适应的3D面部几何重建，克服了现有方法的局限性。

Abstract: Existing 3D face modeling methods usually depend on 3D Morphable Models,
which inherently constrain the representation capacity to fixed shape priors.
Optimization-based approaches offer high-quality reconstructions but tend to be
computationally expensive. In this work, we introduce GLVD, a hybrid method for
3D face reconstruction from few-shot images that extends Learned Vertex Descent
(LVD) by integrating per-vertex neural field optimization with global
structural guidance from dynamically predicted 3D keypoints. By incorporating
relative spatial encoding, GLVD iteratively refines mesh vertices without
requiring dense 3D supervision. This enables expressive and adaptable geometry
reconstruction while maintaining computational efficiency. GLVD achieves
state-of-the-art performance in single-view settings and remains highly
competitive in multi-view scenarios, all while substantially reducing inference
time.

</details>


### [59] [Medical Vision Language Models as Policies for Robotic Surgery](https://arxiv.org/abs/2510.06064)
*Akshay Muppidi,Martin Radfar*

Main category: cs.CV

TL;DR: 通过将医疗领域特定的视觉-语言模型MedFlamingo与PPO集成，我们提出了一种用于机器人腹腔镜手术任务的视觉引导方法。该方法在LapGym的五个环境中进行了评估，仅使用内窥镜视觉观察。MedFlamingo PPO比标准的视觉PPO和OpenFlamingo PPO基线表现更好，收敛更快，成功率超过70%，改进幅度为66.67%至1114.29%。该方法通过为每个回合生成高层规划令牌，有效地结合了医学专业知识和实时视觉反馈，突显了专业医学知识在机器人手术规划和决策中的价值。


<details>
  <summary>Details</summary>
Motivation: 机器人腹腔镜手术任务中的视觉PPO因视觉输入的高维度、稀疏的奖励以及从原始视觉数据中提取相关特征的难度而面临挑战。

Method: 将MedFlamingo（一种医疗领域的视觉-语言模型）与PPO相结合，并仅使用内窥镜视觉观察。

Result: MedFlamingo PPO在LapGym的五个腹腔镜手术任务环境中，成功率超过70%，比基线方法（标准视觉PPO和OpenFlamingo PPO）有显著提高（66.67%至1114.29%），并且收敛速度更快。

Conclusion: 所提出的MedFlamingo PPO方法有效地结合了医学专业知识和实时视觉反馈，在机器人手术规划和决策中显示出巨大潜力，优于现有基线方法。

Abstract: Vision-based Proximal Policy Optimization (PPO) struggles with visual
observation-based robotic laparoscopic surgical tasks due to the
high-dimensional nature of visual input, the sparsity of rewards in surgical
environments, and the difficulty of extracting task-relevant features from raw
visual data. We introduce a simple approach integrating MedFlamingo, a medical
domain-specific Vision-Language Model, with PPO. Our method is evaluated on
five diverse laparoscopic surgery task environments in LapGym, using only
endoscopic visual observations. MedFlamingo PPO outperforms and converges
faster compared to both standard vision-based PPO and OpenFlamingo PPO
baselines, achieving task success rates exceeding 70% across all environments,
with improvements ranging from 66.67% to 1114.29% compared to baseline. By
processing task observations and instructions once per episode to generate
high-level planning tokens, our method efficiently combines medical expertise
with real-time visual feedback. Our results highlight the value of specialized
medical knowledge in robotic surgical planning and decision-making.

</details>


### [60] [Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA](https://arxiv.org/abs/2510.06067)
*Python Song,Luke Tenyi Chang,Yun-Yun Tsai,Penghui Li,Junfeng Yang*

Main category: cs.CV

TL;DR: 现有商用视觉语言模型（VLM）在处理需要空间推理的CAPTCHA任务时表现不佳，准确率仅为21.9%。通过引入逐步推理机制，准确率可显著提升。我们提出了首个包含推理步骤的真实世界CAPTCHA基准测试集CAPTCHA-X，并设计了五种评估指标。基于此，我们开发了一个新的VLM框架，在五种高难度CAPTCHA上实现了83.9%的平均准确率，证明了推理能力在解决视觉空间挑战中的重要性。


<details>
  <summary>Details</summary>
Motivation: CAPTCHA作为区分人类和机器的工具，已成为评估视觉语言模型（VLM）空间推理能力的重要基准。然而，现有商用VLM在处理这类高难度空间推理任务时遇到困难。

Method: 本研究提出了CAPTCHA-X，一个包含七种类型CAPTCHA的基准测试集，并提供了逐步推理解决方案和标注。同时，研究定义了五种面向推理的评估指标，并提出了一种基于VLM的通用Agentic框架，以增强模型的推理能力。

Result: 在CAPTCHA-X基准测试中，研究提出的VLM框架在五种高难度CAPTCHA上的平均准确率达到83.9%，显著优于现有方法，而现有商用VLM的准确率仅约为21.9%。

Conclusion: 逐步推理对于VLM解决高难度空间推理任务（如CAPTCHA）至关重要。本研究提出的CAPTCHA-X基准和基于推理的VLM框架，显著提升了模型在该类任务上的表现，并揭示了当前模型在推理能力上的局限性，指明了未来视觉空间挑战研究的方向。

Abstract: CAPTCHA, originally designed to distinguish humans from robots, has evolved
into a real-world benchmark for assessing the spatial reasoning capabilities of
vision-language models. In this work, we first show that step-by-step reasoning
is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent
high-difficulty spatial reasoning tasks, and that current commercial
vision-language models still struggle with such reasoning. In particular, we
observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to
effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent).
However, our findings indicate that requiring the model to perform step-by-step
reasoning before generating the final coordinates can significantly enhance its
solving accuracy, underscoring the severity of the gap. To systematically study
this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with
reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha,
etc.) with step-by-step action solutions and grounding annotations. We further
define five reasoning-oriented metrics that enable a comprehensive evaluation
of models reasoning capabilities. To validate the effectiveness of reasoning,
we also propose a general agentic VLM-based framework that incorporates the
models inherent reasoning abilities. Our method achieves state-of-the-art
performance across five high-difficulty CAPTCHA types, with an average solving
accuracy of 83.9 percent, substantially surpassing existing baselines. These
results reveal the limitations of current models and highlight the importance
of reasoning in advancing visual-spatial challenges in the future.

</details>


### [61] [There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers](https://arxiv.org/abs/2510.06070)
*Meghna P Ayyar,Jenny Benois-Pineau,Akka Zemmari*

Main category: cs.CV

TL;DR: XAI方法难以应用于ViT，提出结合注意力与统计过滤的方法，并在人类注视数据上进行评估，效果优于SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有XAI方法（尤其是基于CNN的方法）在ViT上表现不佳，基于ViT的注意力权重的方法产生的解释图（maps）噪声较大， MLP块也被用于解释，但注意力仍是潜在有价值的信号。

Method: 提出一种结合注意力图与统计过滤（最初用于CNN）的方法，以去除噪声模式，产生更可信的解释。进一步提出类特定的变体以产生区分性解释。

Result: 与SOTA方法相比，该方法产生的解释图更清晰、更可信。在多个数据集上，该方法在效率和人类可解释性方面，表现优于或媲美SOTA方法。

Conclusion: 结合注意力与统计过滤的方法能够为ViT生成更清晰、更可信、更符合人类感知和高效率的解释。

Abstract: Explainable AI (XAI) has become increasingly important with the rise of large
transformer models, yet many explanation methods designed for CNNs transfer
poorly to Vision Transformers (ViTs). Existing ViT explanations often rely on
attention weights, which tend to yield noisy maps as they capture
token-to-token interactions within each layer.While attribution methods
incorporating MLP blocks have been proposed, we argue that attention remains a
valuable and interpretable signal when properly filtered. We propose a method
that combines attention maps with a statistical filtering, initially proposed
for CNNs, to remove noisy or uninformative patterns and produce more faithful
explanations. We further extend our approach with a class-specific variant that
yields discriminative explanations. Evaluation against popular state-of-the-art
methods demonstrates that our approach produces sharper and more interpretable
maps. In addition to perturbation-based faithfulness metrics, we incorporate
human gaze data to assess alignment with human perception, arguing that human
interpretability remains essential for XAI. Across multiple datasets, our
approach consistently outperforms or is comparable to the SOTA methods while
remaining efficient and human plausible.

</details>


### [62] [When Thinking Drifts: Evidential Grounding for Robust Video Reasoning](https://arxiv.org/abs/2510.06077)
*Mi Luo,Zihui Xue,Alex Dimakis,Kristen Grauman*

Main category: cs.CV

TL;DR: CoT在视频推理中表现不佳，会导致“视觉思维漂移”，提出VER框架解决此问题，并在10个基准测试中取得领先。


<details>
  <summary>Details</summary>
Motivation: CoT机制在文本推理中有效，但在视频理解中的应用效果不佳，需要研究其在视频推理中的局限性并提出解决方案。

Method: 提出“视觉思维漂移”现象，并用贝叶斯理论解释其原因；设计了视觉证据奖励（VER）框架，通过强化学习奖励与视觉证据一致的推理过程。

Result: 在10个视频理解基准测试中，Video-VER方法始终表现出色，超越了现有技术。

Conclusion: CoT在视频推理中存在“视觉思维漂移”问题，VER框架能够有效解决此问题，实现更可靠的视觉推理。

Abstract: Video reasoning, the task of enabling machines to infer from dynamic visual
content through multi-step logic, is crucial for advanced AI. While the
Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks,
its application to video understanding remains underexplored. This paper
presents a systematic analysis revealing that CoT often degrades performance in
video reasoning, generating verbose but misleading internal monologues, and
leading to hallucinated visual details and overridden correct intuitions - a
phenomenon we term "visual thinking drift". We explain this drift through a
Bayesian lens, positing that CoT traces often diverge from actual visual
evidence, instead amplifying internal biases or language priors, causing models
to storytell rather than engage in grounded reasoning. To counteract this, we
introduce Visual Evidence Reward (VER), a novel reinforcement learning
framework that explicitly rewards the generation of reasoning traces that are
verifiably grounded in visual evidence. Comprehensive evaluation across 10
diverse video understanding benchmarks demonstrates that our Video-VER
consistently achieves top performance. Our work sheds light on the distinct
challenges of video-centric reasoning and encourages the development of AI that
robustly grounds its inferences in visual evidence - for large multimodal
models that not only "think before answering", but also "see while thinking".

</details>


### [63] [A public cardiac CT dataset featuring the left atrial appendage](https://arxiv.org/abs/2510.06090)
*Bjoern Hansen,Jonas Pedersen,Klaus F. Kofoed,Oscar Camara,Rasmus R. Paulsen,Kristine Soerensen*

Main category: cs.CV

TL;DR: 本文介绍了首个开源的、解剖学上连贯的左心耳（LAA）、冠状动脉（CA）和肺静脉（PV）分割数据集，并提供了对ImageCAS数据集的分析，旨在促进LAA形态学研究。


<details>
  <summary>Details</summary>
Motivation: 现有的分割框架在分割LAA、CA和PV方面仍存在挑战，需要一个高质量的数据集来推动新的研究方法。

Method: 利用先进的分割框架，在私有数据集上训练模型，并将其应用于ImageCAS公开数据集，对LAA、CA和PV进行分割。同时，对ImageCAS数据中的常见缺陷进行了标注。

Result: 生成了包含LAA、CA、PV和全心脏分割的ImageCAS数据集，并识别了存在数据缺陷的扫描。

Conclusion: 该数据集为LAA形态学等心脏结构分析提供了宝贵的资源，有助于提高医学影像分割的准确性。

Abstract: Despite the success of advanced segmentation frameworks such as
TotalSegmentator (TS), accurate segmentations of the left atrial appendage
(LAA), coronary arteries (CAs), and pulmonary veins (PVs) remain a significant
challenge in medical imaging. In this work, we present the first open-source,
anatomically coherent dataset of curated, high-resolution segmentations for
these structures, supplemented with whole-heart labels produced by TS on the
publicly available ImageCAS dataset consisting of 1000 cardiac computed
tomography angiography (CCTA) scans. One purpose of the data set is to foster
novel approaches to the analysis of LAA morphology.
  LAA segmentations on ImageCAS were generated using a state-of-the-art
segmentation framework developed specifically for high resolution LAA
segmentation. We trained the network on a large private dataset with manual
annotations provided by medical readers guided by a trained cardiologist and
transferred the model to ImageCAS data. CA labels were improved from the
original ImageCAS annotations, while PV segmentations were refined from TS
outputs. In addition, we provide a list of scans from ImageCAS that contains
common data flaws such as step artefacts, LAAs extending beyond the scanner's
field of view, and other types of data defects.

</details>


### [64] [Compact Multi-level-prior Tensor Representation for Hyperspectral Image Super-resolution](https://arxiv.org/abs/2510.06098)
*Yinjian Wang,Wei Li,Yuanyuan Gui,Gemine Vivone*

Main category: cs.CV

TL;DR: 提出了一种新颖的、用于高光谱图像超分辨率的张量分解模型，该模型能够同时有效地利用多层次、多维度的先验知识，并通过优化的交替方向乘子法解决模型问题。


<details>
  <summary>Details</summary>
Motivation: 现有的基于张量的模型在处理多层次先验知识时存在模型复杂性高、难以平衡权重和优化多块结构的问题。

Method: 通过块项分解将潜在高空间-光谱分辨率图像映射到光谱子空间和空间图，然后将这些空间图堆叠成编码高阶空间低秩和光滑性先验的空间张量，并采用新颖的非凸模置换张量相关全变分进行联合建模。最后，利用线性化交替方向乘子法设计了一种高效的优化算法。

Result: 实验证明了该模型在多个数据集上的有效性。

Conclusion: 所提出的模型能够有效融合高光谱和多光谱图像，实现高分辨率图像超分辨率，并且算法具有良好的收敛性。

Abstract: Fusing a hyperspectral image with a multispectral image acquired over the
same scene, \textit{i.e.}, hyperspectral image super-resolution, has become a
popular computational way to access the latent high-spatial-spectral-resolution
image. To date, a variety of fusion methods have been proposed, among which the
tensor-based ones have testified that multiple priors, such as multidimensional
low-rankness and spatial total variation at multiple levels, effectively drive
the fusion process. However, existing tensor-based models can only effectively
leverage one or two priors at one or two levels, since simultaneously
incorporating multi-level priors inevitably increases model complexity. This
introduces challenges in both balancing the weights of different priors and
optimizing multi-block structures. Concerning this, we present a novel
hyperspectral super-resolution model compactly characterizing these multi-level
priors of hyperspectral images within the tensor framework. Firstly, the
proposed model decouples the spectral low-rankness and spatial priors by
casting the latent high-spatial-spectral-resolution image into spectral
subspace and spatial maps via block term decomposition. Secondly, these spatial
maps are stacked as the spatial tensor encoding the high-order spatial
low-rankness and smoothness priors, which are co-modeled via the proposed
non-convex mode-shuffled tensor correlated total variation. Finally, we draw
inspiration from the linearized alternating direction method of multipliers to
design an efficient algorithm to optimize the resulting model, theoretically
proving its Karush-Kuhn-Tucker convergence under mild conditions. Experiments
on multiple datasets demonstrate the effectiveness of the proposed algorithm.
The code implementation will be available from https://github.com/WongYinJ.

</details>


### [65] [Multimodal Feature Prototype Learning for Interpretable and Discriminative Cancer Survival Prediction](https://arxiv.org/abs/2510.06113)
*Shuo Jiang,Zhuwen Chen,Liaoman Xu,Yanming Zhu,Changmiao Wang,Jiong Zhang,Feiwei Qin,Yifei Chen,Zhu Zhu*

Main category: cs.CV

TL;DR: FeatProto是一个创新的多模态原型学习框架，通过整合病理图像和基因组数据，提高了癌症生存预测的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 当前临床决策中使用的生存分析模型可解释性差，而传统原型学习方法在处理肿瘤的整体背景和基因组数据关联方面存在不足。

Method: 提出了一种名为FeatProto的多模态原型学习框架，该框架通过以下方式进行：1. 融合全局和局部图像特征与基因组数据，构建统一的特征原型空间，以减少局部偏差。2. 采用指数原型更新策略（EMA ProtoUp），通过“游走机制”适应肿瘤异质性，维持跨模态的稳定性。3. 设计了一种分层原型匹配机制，以捕捉全局中心性、局部典型性和队列趋势。

Result: 在四个公开的癌症数据集上进行的大量评估表明，FeatProto在准确性和互操作性方面优于现有的领先的单模态和多模态生存预测技术。

Conclusion: FeatProto通过其创新的多模态原型学习方法，在癌症生存预测方面取得了显著的改进，为医学领域的原型学习应用提供了新的视角。

Abstract: Survival analysis plays a vital role in making clinical decisions. However,
the models currently in use are often difficult to interpret, which reduces
their usefulness in clinical settings. Prototype learning presents a potential
solution, yet traditional methods focus on local similarities and static
matching, neglecting the broader tumor context and lacking strong semantic
alignment with genomic data. To overcome these issues, we introduce an
innovative prototype-based multimodal framework, FeatProto, aimed at enhancing
cancer survival prediction by addressing significant limitations in current
prototype learning methodologies within pathology. Our framework establishes a
unified feature prototype space that integrates both global and local features
of whole slide images (WSI) with genomic profiles. This integration facilitates
traceable and interpretable decision-making processes. Our approach includes
three main innovations: (1) A robust phenotype representation that merges
critical patches with global context, harmonized with genomic data to minimize
local bias. (2) An Exponential Prototype Update Strategy (EMA ProtoUp) that
sustains stable cross-modal associations and employs a wandering mechanism to
adapt prototypes flexibly to tumor heterogeneity. (3) A hierarchical prototype
matching scheme designed to capture global centrality, local typicality, and
cohort-level trends, thereby refining prototype inference. Comprehensive
evaluations on four publicly available cancer datasets indicate that our method
surpasses current leading unimodal and multimodal survival prediction
techniques in both accuracy and interoperability, providing a new perspective
on prototype learning for critical medical applications. Our source code is
available at https://github.com/JSLiam94/FeatProto.

</details>


### [66] [Towards Data-Efficient Medical Imaging: A Generative and Semi-Supervised Framework](https://arxiv.org/abs/2510.06123)
*Mosong Ma,Tania Stathaki,Michalis Lazarou*

Main category: cs.CV

TL;DR: SSGNet通过结合生成模型和半监督学习来解决医学影像中数据稀疏和不平衡的问题，在多个基准测试中提升了分类和分割性能。


<details>
  <summary>Details</summary>
Motivation: 医学影像中的深度学习常受限于稀疏和不平衡的标注数据。

Method: SSGNet框架结合了特定类别的生成模型和迭代半监督伪标签，通过StyleGAN3生成图像来扩充训练数据，并迭代地优化伪标签，以增强现有模型的分类和分割能力。

Result: 在多个医学影像基准测试中，SSGNet均显示出分类和分割性能的一致性提升，并且Frechet Inception Distance分析证实了生成样本的高质量。

Conclusion: SSGNet是一种实用的策略，可以缓解标注瓶颈，提高医学影像分析的鲁棒性。

Abstract: Deep learning in medical imaging is often limited by scarce and imbalanced
annotated data. We present SSGNet, a unified framework that combines class
specific generative modeling with iterative semisupervised pseudo labeling to
enhance both classification and segmentation. Rather than functioning as a
standalone model, SSGNet augments existing baselines by expanding training data
with StyleGAN3 generated images and refining labels through iterative pseudo
labeling. Experiments across multiple medical imaging benchmarks demonstrate
consistent gains in classification and segmentation performance, while Frechet
Inception Distance analysis confirms the high quality of generated samples.
These results highlight SSGNet as a practical strategy to mitigate annotation
bottlenecks and improve robustness in medical image analysis.

</details>


### [67] [Discrete Diffusion Models with MLLMs for Unified Medical Multimodal Generation](https://arxiv.org/abs/2510.06131)
*Jiawei Mao,Yuhan Wang,Lifeng Chen,Can Zhao,Yucheng Tang,Dong Yang,Liangqiong Qu,Daguang Xu,Yuyin Zhou*

Main category: cs.CV

TL;DR: MeDiM是首个能够跨模态学习共享分布的医学离散扩散模型，它通过移除因果注意力掩码并注入连续时间步嵌入，利用多模态大语言模型作为扩散主干，实现了跨影像、病理和临床记录等模态的统一生成，并在医学影像和报告生成任务中取得了高保真度和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有医学生成模型在处理多模态数据时存在碎片化问题，限制了其向能够学习和推理跨生物医学数据全谱的基础模型发展。

Method: 提出MeDiM，一个医学离散扩散模型，利用多模态大语言模型作为其主干，通过移除因果注意力掩码和注入连续时间步嵌入来实现跨模态学习和生成。

Result: MeDiM在医学影像生成（MIMIC-CXR上FID为16.60，PathGen上FID为24.19）和报告生成（METEOR为0.2650和0.2580）方面取得了高保真度和准确性。联合生成的图像-报告对进一步提高了下游任务的性能。

Conclusion: MeDiM能够支持连贯且符合临床的、跨模态的生成任务，克服了现有方法的局限性。

Abstract: Recent advances in generative medical models are constrained by
modality-specific scenarios that hinder the integration of complementary
evidence from imaging, pathology, and clinical notes. This fragmentation limits
their evolution into foundation models that can learn and reason across the
full spectrum of biomedical data. We propose MeDiM, the first medical discrete
diffusion model that learns shared distributions across modalities without
modality-specific components. MeDiM unifies multiple generative tasks:
translating between images and text, and jointly producing image-report pairs
across domains in response to prompts. Built on a discrete diffusion framework,
MeDiM bridges vision and language representations through a shared
probabilistic space. To enable unified and flexible medical generation, we
employ a multimodal large language model (MLLM) as the diffusion backbone,
leveraging its prior knowledge and cross-modal reasoning. Two key designs are
introduced: (1) removing the causal attention mask for bidirectional context,
and (2) injecting continuous timestep embeddings for diffusion awareness.
Experiments demonstrate high-fidelity medical generation (FID 16.60 on
MIMIC-CXR and FID 24.19 on PathGen) and accurate report generation (METEOR
0.2650 and 0.2580). Jointly generated image-report pairs further enhance
downstream performance (plus6.43 percent BLEU-1, plus18.57 percent BLEU-2,
plus31.58 percent BLEU-3, plus4.80 percent METEOR), showing that MeDiM supports
coherent and clinically grounded multimodal outputs.

</details>


### [68] [Deforming Videos to Masks: Flow Matching for Referring Video Segmentation](https://arxiv.org/abs/2510.06139)
*Zanyi Wang,Dengyang Jiang,Liuzhuozheng Li,Sizhe Dang,Chengzu Li,Harry Yang,Guang Dai,Mengmeng Wang,Jingdong Wang*

Main category: cs.CV

TL;DR: FlowRVS是一个新框架，将Referring Video Object Segmentation (RVOS)视为一个条件连续流问题，利用预训练的T2V模型，实现像素级控制、文本-视频语义对齐和时间连贯性，并在RVOS基准测试中取得了新的最先进成果。


<details>
  <summary>Details</summary>
Motivation: 以前的RVOS方法通常采用“定位-然后分割”的流水线，这会在语义信息的传递中产生信息瓶颈，并且由于分割过程与初始语言基础脱钩，难以保持时间连贯性。

Method: FlowRVS将RVOS重新构想为一个条件连续流问题，通过学习视频整体表示到目标掩码的直接、语言引导的变形，而不是从噪声生成掩码或直接预测掩码。

Result: FlowRVS在MeViS上达到了51.1的J&F分数（比之前的方法提高了1.6），在零样本Ref-DAVIS17上达到了73.3的分数（比之前的方法提高了2.7），在所有主要的RVOS基准测试中均取得了新的最先进成果。

Conclusion: 将视频理解任务建模为连续变形过程具有巨大的潜力，FlowRVS通过其新颖的框架证明了这一点。

Abstract: Referring Video Object Segmentation (RVOS) requires segmenting specific
objects in a video guided by a natural language description. The core challenge
of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels
and continuously segment them through the complex dynamics of a video. Faced
with this difficulty, prior work has often decomposed the task into a pragmatic
`locate-then-segment' pipeline. However, this cascaded design creates an
information bottleneck by simplifying semantics into coarse geometric prompts
(e.g, point), and struggles to maintain temporal consistency as the segmenting
process is often decoupled from the initial language grounding. To overcome
these fundamental limitations, we propose FlowRVS, a novel framework that
reconceptualizes RVOS as a conditional continuous flow problem. This allows us
to harness the inherent strengths of pretrained T2V models, fine-grained pixel
control, text-video semantic alignment, and temporal coherence. Instead of
conventional generating from noise to mask or directly predicting mask, we
reformulate the task by learning a direct, language-guided deformation from a
video's holistic representation to its target mask. Our one-stage, generative
approach achieves new state-of-the-art results across all major RVOS
benchmarks. Specifically, achieving a $\mathcal{J}\&\mathcal{F}$ of 51.1 in
MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7),
demonstrating the significant potential of modeling video understanding tasks
as continuous deformation processes.

</details>


### [69] [Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images](https://arxiv.org/abs/2510.06145)
*Aditya Prakash,David Forsyth,Saurabh Gupta*

Main category: cs.CV

TL;DR: 我们提出了一种从单个图像预测多姿势三维手部运动和关节的方法，并设计了一个包含扩散模型的注释流程来解决数据不足的问题，同时使用扩散损失来处理手部运动分布的多模态性。


<details>
  <summary>Details</summary>
Motivation: 解决从单个图像预测多姿势三维手部运动和关节的问题，并克服真实世界场景中缺乏三维手部标注的挑战。

Method: 设计了一个包含扩散模型的注释流程，将二维手部关键点序列提升为四维手部运动，并采用扩散损失来处理手部运动分布的多模态性。

Result: 在6个数据集上进行的广泛实验表明，在包含填补标签的各种数据上进行训练可以带来14%的提升，而我们提出的提升模型（42%的提升）和预测模型（16.4%的提升）在各项指标上均优于现有的最佳基线模型，尤其是在对真实世界图像进行零样本泛化方面表现出色。

Conclusion: 提出的方法在从单张图像预测三维手部运动方面取得了显著成效，尤其在数据稀疏和复杂场景下展现出优越的泛化能力。

Abstract: We tackle the problem of forecasting bimanual 3D hand motion & articulation
from a single image in everyday settings. To address the lack of 3D hand
annotations in diverse settings, we design an annotation pipeline consisting of
a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the
forecasting model, we adopt a diffusion loss to account for the multimodality
in hand motion distribution. Extensive experiments across 6 datasets show the
benefits of training on diverse data with imputed labels (14% improvement) and
effectiveness of our lifting (42% better) & forecasting (16.4% gain) models,
over the best baselines, especially in zero-shot generalization to everyday
images.

</details>


### [70] [ShapeGen4D: Towards High Quality 4D Shape Generation from Videos](https://arxiv.org/abs/2510.06208)
*Jiraphon Yenphraphai,Ashkan Mirzaei,Jianqi Chen,Jiaxu Zou,Sergey Tulyakov,Raymond A. Yeh,Peter Wonka,Chaoyang Wang*

Main category: cs.CV

TL;DR: 从视频生成动态4D形状。


<details>
  <summary>Details</summary>
Motivation: 视频条件下的4D形状生成旨在直接从输入视频中恢复随时间变化的3D几何形状和视图一致的外观。

Method: 提出一个端到端生成动态3D表示的框架，包含三个关键组件：条件化所有帧的‘时间注意力’，生成时间索引动态表示；‘时间感知点采样和4D潜在锚定’，促进时间上一致的几何和纹理；以及‘跨帧噪声共享’，增强时间稳定性。

Result: 在各种实际视频中，与基线方法相比，该方法能够准确捕捉非刚性运动、体积变化甚至拓扑转换，而无需进行每帧优化，同时提高了鲁棒性和感知保真度，并减少了故障模式。

Conclusion: 该框架通过利用大规模预训练的3D模型，实现了高效且高质量的视频到4D形状生成。

Abstract: Video-conditioned 4D shape generation aims to recover time-varying 3D
geometry and view-consistent appearance directly from an input video. In this
work, we introduce a native video-to-4D shape generation framework that
synthesizes a single dynamic 3D representation end-to-end from the video. Our
framework introduces three key components based on large-scale pre-trained 3D
models: (i) a temporal attention that conditions generation on all frames while
producing a time-indexed dynamic representation; (ii) a time-aware point
sampling and 4D latent anchoring that promote temporally consistent geometry
and texture; and (iii) noise sharing across frames to enhance temporal
stability. Our method accurately captures non-rigid motion, volume changes, and
even topological transitions without per-frame optimization. Across diverse
in-the-wild videos, our method improves robustness and perceptual fidelity and
reduces failure modes compared with the baselines.

</details>


### [71] [Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models](https://arxiv.org/abs/2510.06209)
*Jiahao Wang,Zhenpei Yang,Yijing Bai,Yingwei Li,Yuliang Zou,Bo Sun,Abhijit Kundu,Jose Lezama,Luna Yue Huang,Zehao Zhu,Jyh-Jing Hwang,Dragomir Anguelov,Mingxing Tan,Chiyu Max Jiang*

Main category: cs.CV

TL;DR: 生成模型可用于生成自动驾驶测试视频，并帮助端到端驾驶模型更好地泛化。


<details>
  <summary>Details</summary>
Motivation: 目前尚不清楚生成模型生成的视频是否足够逼真以用于端到端驾驶模型的评估，以及如何利用这些模型来深入了解端到端模型的偏差并提高其泛化能力。

Method: 提出了一种结合端到端驾驶模型和生成世界模型的方法（Drive&Gen），使用端到端驾驶模型评估生成视频的真实性，并通过可控的视频生成模型进行有针对性的实验，以研究影响端到端模型性能的分布外情况。

Result: 生成模型生成的合成数据可用于有效提高端到端模型的泛化能力，并且比收集真实世界数据更具成本效益。

Conclusion: Drive&Gen 方法可以弥合生成模型与自动驾驶模型之间的差距，生成的合成数据可以提高端到端模型的泛化能力，使其能够扩展到新的运行环境。

Abstract: Recent advances in generative models have sparked exciting new possibilities
in the field of autonomous vehicles. Specifically, video generation models are
now being explored as controllable virtual testing environments.
Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined
alternative to conventional modular autonomous driving systems, gaining
popularity for their simplicity and scalability. However, the application of
these techniques to simulation and planning raises important questions. First,
while video generation models can generate increasingly realistic videos, can
these videos faithfully adhere to the specified conditions and be realistic
enough for E2E autonomous planner evaluation? Second, given that data is
crucial for understanding and controlling E2E planners, how can we gain deeper
insights into their biases and improve their ability to generalize to
out-of-distribution scenarios? In this work, we bridge the gap between the
driving models and generative world models (Drive&Gen) to address these
questions. We propose novel statistical measures leveraging E2E drivers to
evaluate the realism of generated videos. By exploiting the controllability of
the video generation model, we conduct targeted experiments to investigate
distribution gaps affecting E2E planner performance. Finally, we show that
synthetic data produced by the video generation model offers a cost-effective
alternative to real-world data collection. This synthetic data effectively
improves E2E model generalization beyond existing Operational Design Domains,
facilitating the expansion of autonomous vehicle services into new operational
contexts.

</details>


### [72] [Fine-grained Defocus Blur Control for Generative Image Models](https://arxiv.org/abs/2510.06215)
*Ayush Shrivastava,Connelly Barnes,Xuaner Zhang,Lingzhi Zhang,Andrew Owens,Sohrab Amirghodsi,Eli Shechtman*

Main category: cs.CV

TL;DR: 现有文生图模型难以控制相机元数据（如光圈），本研究提出一个整合相机元数据（EXIF）以生成可控的镜头模糊的文生图框架。


<details>
  <summary>Details</summary>
Motivation: 现有文生图模型难以根据相机元数据（如光圈设置）生成具有特定模糊效果的图像。

Method: 生成一张全聚焦图像，估计其单目深度图，预测焦点距离，然后利用可微分的镜头模糊模型生成模糊图像。通过反向传播训练模型，无需显式监督即可根据内容和EXIF数据学习生成散景效果。

Result: 在推理时，该模型能够精确地控制散景效果，同时保持场景内容，并且能够实现现有扩散模型无法达到的效果。实验结果证明了该模型在不改变场景内容的情况下，能够实现卓越的精细控制。

Conclusion: 本研究提出的文生图框架能够有效利用相机元数据生成可控的镜头模糊效果，为用户提供精细的散景控制能力。

Abstract: Current text-to-image diffusion models excel at generating diverse,
high-quality images, yet they struggle to incorporate fine-grained camera
metadata such as precise aperture settings. In this work, we introduce a novel
text-to-image diffusion framework that leverages camera metadata, or EXIF data,
which is often embedded in image files, with an emphasis on generating
controllable lens blur. Our method mimics the physical image formation process
by first generating an all-in-focus image, estimating its monocular depth,
predicting a plausible focus distance with a novel focus distance transformer,
and then forming a defocused image with an existing differentiable lens blur
model. Gradients flow backwards through this whole process, allowing us to
learn without explicit supervision to generate defocus effects based on content
elements and the provided EXIF data. At inference time, this enables precise
interactive user control over defocus effects while preserving scene contents,
which is not achievable with existing diffusion models. Experimental results
demonstrate that our model enables superior fine-grained control without
altering the depicted scene.

</details>


### [73] [EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark](https://arxiv.org/abs/2510.06218)
*Deheng Zhang,Yuqian Fu,Runyi Yang,Yang Miao,Tianwen Qian,Xu Zheng,Guolei Sun,Ajad Chhatkuli,Xuanjing Huang,Yu-Gang Jiang,Luc Van Gool,Danda Pani Paudel*

Main category: cs.CV

TL;DR: EgoNight是首个针对夜间单目视觉理解的基准，包含VQA任务，并通过日夜对齐的视频、自动标注引擎和人工验证来提高标注质量，以解决现有基准在弱光条件下研究不足的问题。


<details>
  <summary>Details</summary>
Motivation: 现有针对单目视觉理解的基准主要关注白天场景，忽略了真实应用中不可避免的弱光条件。

Method: 收集了合成和真实世界数据，并利用日夜对齐的视频构建了EgoNight-VQA数据集，包含自动标注引擎和人工验证，并引入了辅助任务，如日夜对应检索和夜间单目深度估计。

Result: 评估表明，在从白天到夜间的迁移中，最先进的多模态大语言模型（MLLMs）性能显著下降，突显了在弱光条件下进行推理的挑战。

Conclusion: EgoNight-VQA为推进应用驱动的单目视觉研究和开发能跨越光照域的模型提供了坚实的基础。

Abstract: Most existing benchmarks for egocentric vision understanding focus primarily
on daytime scenarios, overlooking the low-light conditions that are inevitable
in real-world applications. To investigate this gap, we present EgoNight, the
first comprehensive benchmark for nighttime egocentric vision, with visual
question answering (VQA) as the core task. A key feature of EgoNight is the
introduction of day-night aligned videos, which enhance night annotation
quality using the daytime data and reveal clear performance gaps between
lighting conditions. To achieve this, we collect both synthetic videos rendered
by Blender and real-world recordings, ensuring that scenes and actions are
visually and temporally aligned. Leveraging these paired videos, we construct
EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and
refinement through extensive human verification. Each QA pair is double-checked
by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs
across 90 videos, spanning 12 diverse QA types, with more than 300 hours of
human work. Evaluations of state-of-the-art multimodal large language models
(MLLMs) reveal substantial performance drops when transferring from day to
night, underscoring the challenges of reasoning under low-light conditions.
Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night
correspondence retrieval and egocentric depth estimation at night, that further
explore the boundaries of existing models. We believe EgoNight-VQA provides a
strong foundation for advancing application-driven egocentric vision research
and for developing models that generalize across illumination domains. All the
data and code will be made available upon acceptance.

</details>


### [74] [Human3R: Everyone Everywhere All at Once](https://arxiv.org/abs/2510.06219)
*Yue Chen,Xingyu Chen,Yuxuan Xue,Anpei Chen,Yuliang Xiu,Gerard Pons-Moll*

Main category: cs.CV

TL;DR: Human3R是一个统一的、前馈的框架，用于从随意拍摄的单眼视频中进行在线4D人景重建，能够一次性、实时地重建多个SMPL-X人体、3D场景和相机轨迹，无需多阶段流水线和迭代精炼。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常依赖多阶段流水线、迭代的精炼过程以及对人体检测、深度估计和SLAM预处理等技术的重度依赖，这些方法复杂且效率低下。本研究旨在提出一个统一、高效的框架，能够直接从单眼视频中一次性重建多个人体和3D场景。

Method: Human3R在CUT3R的4D在线重建模型基础上，利用参数高效的视觉提示调优技术，在一次前向传播中联合恢复全局多人SMPL-X身体、密集3D场景和相机轨迹。该方法消除了对重度依赖和迭代精炼的需求。

Result: 在相对小规模的合成数据集BEDLAM上进行为期一天、单GPU的训练后，Human3R实现了优越的性能和显著的效率：它能够以单次前向传播的方式重建多个人体以及3D场景，在单一阶段实现实时速度（15 FPS），且内存占用低（8 GB）。实验证明，Human3R在全局人体运动估计、局部人体网格恢复、视频深度估计和相机位姿估计等任务上，均取得了最先进或具有竞争力的性能。

Conclusion: Human3R是一个统一、高效、简单且强大的模型，能够一次性、实时地从单眼视频中进行4D人景重建，解决了现有方法的复杂性和效率问题，并为下游应用提供了良好的基础。

Abstract: We present Human3R, a unified, feed-forward framework for online 4D
human-scene reconstruction, in the world frame, from casually captured
monocular videos. Unlike previous approaches that rely on multi-stage
pipelines, iterative contact-aware refinement between humans and scenes, and
heavy dependencies, e.g., human detection, depth estimation, and SLAM
pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies
("everyone"), dense 3D scene ("everywhere"), and camera trajectories in a
single forward pass ("all-at-once"). Our method builds upon the 4D online
reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning,
to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct
readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates
heavy dependencies and iterative refinement. After being trained on the
relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it
achieves superior performance with remarkable efficiency: it reconstructs
multiple humans in a one-shot manner, along with 3D scenes, in one stage, at
real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive
experiments demonstrate that Human3R delivers state-of-the-art or competitive
performance across tasks, including global human motion estimation, local human
mesh recovery, video depth estimation, and camera pose estimation, with a
single unified model. We hope that Human3R will serve as a simple yet strong
baseline, be easily extended for downstream applications.Code available in
https://fanegg.github.io/Human3R

</details>


### [75] [Synthetic History: Evaluating Visual Representations of the Past in Diffusion Models](https://arxiv.org/abs/2505.17064)
*Maria-Teresa De Rosa Palmini,Eva Cetinic*

Main category: cs.CV

TL;DR: TTI模型在描绘历史时期方面存在系统性不准确性，包括不明确的风格联想、历史不一致性和不准确的人口统计学代表性。


<details>
  <summary>Details</summary>
Motivation: 评估TTI模型在准确描绘历史背景方面的能力，弥补了以往研究主要关注人口和文化偏见的不足。

Method: 引入HistVis数据集（包含30,000张合成图像），并从三个方面评估TTI模型生成图像的准确性：1. 隐式风格联想；2. 历史一致性；3. 人口统计学代表性。

Result: TTI模型在生成历史图像时存在系统性错误，包括刻板印象、时代错置（例如，将现代物品置于古代背景中）以及不符合历史模式的人口统计学分布。

Conclusion: 该研究提出了一个评估历史表征的系统性方法和基准，为构建更具历史准确性和文化适应性的TTI模型奠定了基础。

Abstract: As Text-to-Image (TTI) diffusion models become increasingly influential in
content creation, growing attention is being directed toward their societal and
cultural implications. While prior research has primarily examined demographic
and cultural biases, the ability of these models to accurately represent
historical contexts remains largely underexplored. In this work, we present a
systematic and reproducible methodology for evaluating how TTI systems depict
different historical periods. For this purpose, we introduce the HistVis
dataset, a curated collection of 30,000 synthetic images generated by three
state-of-the-art diffusion models using carefully designed prompts depicting
universal human activities across different historical periods. We evaluate
generated imagery across three key aspects: (1) Implicit Stylistic
Associations: examining default visual styles associated with specific eras;
(2) Historical Consistency: identifying anachronisms such as modern artifacts
in pre-modern contexts; and (3) Demographic Representation: comparing generated
racial and gender distributions against historically plausible baselines. Our
findings reveal systematic inaccuracies in historically themed generated
imagery, as TTI models frequently stereotype past eras by incorporating
unstated stylistic cues, introduce anachronisms, and fail to reflect plausible
demographic patterns. By offering a scalable methodology and benchmark for
assessing historical representation in generated imagery, this work provides an
initial step toward building more historically accurate and culturally aligned
TTI models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [76] [Collaborative and Proactive Management of Task-Oriented Conversations](https://arxiv.org/abs/2510.05110)
*Arezoo Saedi,Afsaneh Fatemi,Mohammad Ali Nematbakhsh,Sophie Rosset,Anne Vilnat*

Main category: cs.CL

TL;DR: LLM驱动的面向任务的对话系统，通过引入中间信息和基于信息状态的方法来改进规划，在MultiWOZ数据集上实现了最佳的 inform 和 success。


<details>
  <summary>Details</summary>
Motivation: 现有的面向任务的对话系统（TOD）虽然在利用大型语言模型（LLMs）方面表现出色，但常常忽略了有效的、面向目标的规划，而这对于任务完成至关重要。

Method: 该模型基于信息状态对话管理方法，通过创建和利用中间信息来增强规划。它包括定义槽位和文本片段信息组件来模拟用户偏好，识别关键情况，创建对应于这些情况的信息组件，形成有限的信息状态，并定义对话移动和更新策略。模型利用LLMs的上下文学习能力实现，通过生成的数据库查询和基于文本的实体排序来匹配用户偏好。

Result: 在MultiWOZ数据集（单领域对话）上的评估显示，该模型实现了最高的 inform 和 success 指标，并优于先前的方法。

Conclusion: 该模型通过整合中间信息和信息状态方法，改进了LLM在TOD中的规划能力，从而提高了对话系统的性能。

Abstract: Task oriented dialogue systems (TOD) complete particular tasks based on user
preferences across natural language interactions. Considering the impressive
performance of large language models (LLMs) in natural language processing
(NLP) tasks, most of the latest TODs are centered on LLMs. While proactive
planning is crucial for task completion, many existing TODs overlook effective
goal-aware planning. This paper creates a model for managing task-oriented
conversations, conceptualized centered on the information state approach to
dialogue management. The created model incorporated constructive intermediate
information in planning. Initially, predefined slots and text part
informational components are created to model user preferences. Investigating
intermediate information, critical circumstances are identified. Informational
components corresponding to these circumstances are created. Possible
configurations for these informational components lead to limited information
states. Then, dialogue moves, which indicate movement between these information
states and the procedures that must be performed in the movements, are created.
Eventually, the update strategy is constructed. The created model is
implemented leveraging in-context learning of LLMs. In this model, database
queries are created centered on indicated predefined slots and the order of
retrieved entities is indicated centered on text part. This mechanism enables
passing the whole corresponding entities to the preferences in the order of
congruency. Evaluations exploiting the complete test conversations of MultiWOZ,
with no more than a domain in a conversation, illustrate maximal inform and
success, and improvement compared with previous methods.

</details>


### [77] [Trainable Reference-Based Evaluation Metric for Identifying Quality of English-Gujarati Machine Translation System](https://arxiv.org/abs/2510.05113)
*Nisheeth Joshi,Pragya Katyayan,Palak Arora*

Main category: cs.CL

TL;DR: 提出了一种基于监督学习的参考式机器翻译（MT）评价指标，用于古吉拉特语，并与其他现有指标进行了比较，结果显示其具有更好的人类相关性。


<details>
  <summary>Details</summary>
Motivation: 现有的机器翻译评价方法对于欧洲语言有效，但对于印度语言效果不佳，因此需要为古吉拉特语开发新的评价指标。

Method: 基于监督学习，使用25个特征训练了两个版本的评价指标，一个包含6个隐藏层，另一个包含10个隐藏层，每个模型训练500个周期。使用7个机器翻译系统的1000个翻译输出来测试指标性能，并与1个人工参考翻译进行比较。

Result: 所提出的评价指标与其他可用指标相比，显示出更好的人类相关性。

Conclusion: 所提出的基于监督学习的参考式评价指标在古吉拉特语机器翻译评价方面表现优于现有指标。

Abstract: Machine Translation (MT) Evaluation is an integral part of the MT development
life cycle. Without analyzing the outputs of MT engines, it is impossible to
evaluate the performance of an MT system. Through experiments, it has been
identified that what works for English and other European languages does not
work well with Indian languages. Thus, In this paper, we have introduced a
reference-based MT evaluation metric for Gujarati which is based on supervised
learning. We have trained two versions of the metric which uses 25 features for
training. Among the two models, one model is trained using 6 hidden layers with
500 epochs while the other model is trained using 10 hidden layers with 500
epochs. To test the performance of the metric, we collected 1000 MT outputs of
seven MT systems. These MT engine outputs were compared with 1 human reference
translation. While comparing the developed metrics with other available
metrics, it was found that the metrics produced better human correlations.

</details>


### [78] [Hallucination is Inevitable for LLMs with the Open World Assumption](https://arxiv.org/abs/2510.05116)
*Bowen Xu*

Main category: cs.CL

TL;DR: LLM的“幻觉”是泛化问题的一种表现，在开放世界假设下是不可避免的，应被视为一种需要适应的结构特征，而不是一个需要最小化的缺陷。


<details>
  <summary>Details</summary>
Motivation: 本文旨在重新审视大语言模型（LLM）的“幻觉”现象，探讨其与通用人工智能（AGI）所需条件的关联，并提出一种新的理解和应对策略。

Method: 本文提出将“幻觉”视为泛化问题的一种表现，并区分了在封闭世界和开放世界假设下“幻觉”的不同性质。在此基础上，对“幻觉”进行了分类，并提出了相应的应对思路。

Result: 在封闭世界假设下，可以通过工程方法减轻“幻觉”；而在开放世界假设下，“幻觉”是不可避免的。本文提出的分类有助于区分可纠正的“幻觉”和开放世界条件下似乎无法避免的“幻觉”。

Conclusion: “幻觉”不应仅仅被视为一个工程缺陷，而应被视为一种结构特征，需要被容忍并使其与人类智能兼容。

Abstract: Large Language Models (LLMs) exhibit impressive linguistic competence but
also produce inaccurate or fabricated outputs, often called ``hallucinations''.
Engineering approaches usually regard hallucination as a defect to be
minimized, while formal analyses have argued for its theoretical inevitability.
Yet both perspectives remain incomplete when considering the conditions
required for artificial general intelligence (AGI). This paper reframes
``hallucination'' as a manifestation of the generalization problem. Under the
Closed World assumption, where training and test distributions are consistent,
hallucinations may be mitigated. Under the Open World assumption, however,
where the environment is unbounded, hallucinations become inevitable. This
paper further develops a classification of hallucination, distinguishing cases
that may be corrected from those that appear unavoidable under open-world
conditions. On this basis, it suggests that ``hallucination'' should be
approached not merely as an engineering defect but as a structural feature to
be tolerated and made compatible with human intelligence.

</details>


### [79] [MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation](https://arxiv.org/abs/2510.05124)
*Mingjin Li,Yu Liu,Huayi Liu,Xiang Ye,Chao Jiang,Hongguang Zhang*

Main category: cs.CL

TL;DR: MADS是一个多智能体对话生成框架，通过智能体自我博弈生成有说服力的多轮对话，可用于生成训练数据，在营销场景中将转化率提高了22.4%。


<details>
  <summary>Details</summary>
Motivation: 缺乏用户数据、冷启动评估困难和提示效率低下是行业面临的关键挑战，需要低成本生成训练数据的方法。

Method: MADS框架包含三个协调的智能体：模拟用户驱动行为的用户智能体、执行任务导向的说服策略的对话智能体以及评估和优化对话结果的优化智能体。通过用户态度链建模和专门的LLM说服评估来验证其有效性。

Result: MADS显著提高了小型LLM的说服能力，将自然流量转化率提高了22.4%（从1.83%提高到2.24%）。

Conclusion: MADS在实际营销场景中证明了其明确的商业价值，为解决行业挑战提供了一种低成本、无需人工标注的解决方案。

Abstract: We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for
generating persuasive multi-turn dialogues via agent self-play. MADS employs
three coordinated agents: User Agents simulating diverse persona-driven
behaviors, a Dialog Agent executing task-oriented persuasion strategies and an
Optimization Agent evaluating and refining dialogue outcomes. We further
validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and
dedicated LLMs' persuasion assessment. This approach enables low-cost
generation of training data without human annotation, addressing key industry
challenges such as lack of user data, cold-start evaluation difficulties, and
prompt inefficiency. Applied to a real-world marketing scenario, MADS
significantly improved the persuasion capacity of small LLMs, increasing the
organic traffic conversion rate by 22.4\% (from 1.83\% to 2.24\%) ,
demonstrating clear business value.

</details>


### [80] [Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models](https://arxiv.org/abs/2510.05121)
*Durgesh Nandini,Rebekka Koch,Mirco Schoenfeld*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This study investigates the effectiveness of Large Language Models (LLMs) for
the extraction of structured knowledge in the form of Subject-Predicate-Object
triples. We apply the setup for the domain of Economics application. The
findings can be applied to a wide range of scenarios, including the creation of
economic trade knowledge graphs from natural language legal trade agreement
texts. As a use case, we apply the model to regional trade agreement texts to
extract trade-related information triples. In particular, we explore the
zero-shot, one-shot and few-shot prompting techniques, incorporating positive
and negative examples, and evaluate their performance based on quantitative and
qualitative metrics. Specifically, we used Llama 3.1 model to process the
unstructured regional trade agreement texts and extract triples. We discuss key
insights, challenges, and potential future directions, emphasizing the
significance of language models in economic applications.

</details>


### [81] [CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation](https://arxiv.org/abs/2510.05122)
*Jie Zhu,Yuanchen Zhou,Shuo Jiang,Junhui Li,Lifan Guo,Feng Chen,Chi Zhang,Fang Kong*

Main category: cs.CL

TL;DR: CARE框架通过加强对话中的认知推理能力，在不依赖大规模合成数据的情况下，提升了情感支持对话的质量和逻辑性。


<details>
  <summary>Details</summary>
Motivation: 现有研究在情感支持对话（ESC）方面，大多侧重于数据增强和合成语料库的构建，而忽略了有效情感支持背后更深层次的认知推理过程。

Method: 提出CARE框架，利用原始ESC训练集引导模型生成逻辑连贯且具有支持性的回应，从而显式地增强认知推理能力，并进一步使用强化学习来优化和巩固推理过程。

Result: 实验结果表明，CARE显著提高了回应的逻辑性和支持性质量。

Conclusion: CARE框架在不依赖大规模合成数据的情况下，通过加强认知推理能力，提升了情感支持对话的质量和逻辑性，有助于开发更具共情能力、认知鲁棒性和人类化水平的情感支持系统。

Abstract: Emotional Support Conversation (ESC) plays a vital role in alleviating
psychological stress and providing emotional value through dialogue. While
recent studies have largely focused on data augmentation and synthetic corpus
construction, they often overlook the deeper cognitive reasoning processes that
underpin effective emotional support. To address this gap, we propose
\textbf{CARE}, a novel framework that strengthens reasoning in ESC without
relying on large-scale synthetic data. CARE leverages the original ESC training
set to guide models in generating logically coherent and supportive responses,
thereby explicitly enhancing cognitive reasoning. Building on this foundation,
we further employ reinforcement learning to refine and reinforce the reasoning
process. Experimental results demonstrate that CARE significantly improves both
the logical soundness and supportive quality of responses, advancing the
development of empathetic, cognitively robust, and human-like emotional support
systems.

</details>


### [82] [Catalog-Native LLM: Speaking Item-ID Dialect with Less Entanglement for Recommendation](https://arxiv.org/abs/2510.05125)
*Reza Shirkavand,Xiaokai Wei,Chen Wang,Zheng Hui,Heng Huang,Michelle Gong*

Main category: cs.CL

TL;DR: 本项目提出了IDIOMoE模型，一种结合协同过滤和大型语言模型（LLM）的方法，用于提高推荐系统的性能和用户体验。


<details>
  <summary>Details</summary>
Motivation: 现代推荐系统需要整合协同过滤的预测准确性和效率与大型语言模型的表达和推理能力，以满足用户对自然语言查询和透明解释日益增长的期望。然而，协同信号通常在语义上不透明，而大型语言模型在仅基于文本输入时难以模拟隐式用户偏好。

Method: IDIOMoE模型将物品交互历史视为语言空间内的原生方言，使得协同信号能够被像自然语言一样理解。通过将预训练LLM的每个块的馈送网络（FFN）分离为单独的文本专家和物品专家，并采用token类型门控，该方法避免了文本和目录模式之间的破坏性干扰。

Result: IDIOMoE模型在公开和专有数据集上均表现出强大的推荐性能，同时保留了预训练模型的文本理解能力。

Conclusion: IDIOMoE模型成功地将协同信号和自然语言理解能力融合，为构建更强大、更具交互性的推荐系统提供了一种有效的方法。

Abstract: While collaborative filtering delivers predictive accuracy and efficiency,
and Large Language Models (LLMs) enable expressive and generalizable reasoning,
modern recommendation systems must bring these strengths together. Growing user
expectations, such as natural-language queries and transparent explanations,
further highlight the need for a unified approach. However, doing so is
nontrivial. Collaborative signals are often token-efficient but semantically
opaque, while LLMs are semantically rich but struggle to model implicit user
preferences when trained only on textual inputs. This paper introduces Item-ID
+ Oral-language Mixture-of-Experts Language Model (IDIOMoE), which treats item
interaction histories as a native dialect within the language space, enabling
collaborative signals to be understood in the same way as natural language. By
splitting the Feed Forward Network of each block of a pretrained LLM into a
separate text expert and an item expert with token-type gating, our method
avoids destructive interference between text and catalog modalities. IDIOMoE
demonstrates strong recommendation performance across both public and
proprietary datasets, while preserving the text understanding of the pretrained
model.

</details>


### [83] [Improving Metacognition and Uncertainty Communication in Language Models](https://arxiv.org/abs/2510.05126)
*Mark Steyvers,Catarina Belem,Padhraic Smyth*

Main category: cs.CL

TL;DR: 大型语言模型(LLM)的置信度可以被微调以提高其准确性，但不同类型的置信度任务需要联合训练才能实现跨领域泛化。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型(LLM)在决策中可能提供错误的答案，尽管它们内部有不确定性信号，但外部表现的置信度却不准确。本研究旨在探究监督微调是否能提高LLM表达不确定性的能力，并验证这种提高是否能在不同任务和领域之间泛化。

Method: 研究人员在包含常识、数学和琐事知识的数据集上对两种LLM进行了微调，并评估了两种元认知任务：(1)单问题置信度估计，模型为其答案分配一个数值确定性；(2)成对置信度比较，模型选择它认为更可能是正确的答案。研究人员还评估了模型在未见过的新领域（包括医学和法律推理）的泛化能力。

Result: 结果表明，微调在模型内部和跨领域上都提高了其置信度校准（置信度与准确性的一致性）和区分度（正确答案的置信度高于错误答案），但准确性保持不变。然而，这种提高具有任务特异性：在单问题校准上训练的提高不能迁移到成对比较任务，反之亦然。相比之下，联合训练这两种元认知任务可以带来更广泛的收益，在跨领域评估中能降低校准误差并增强区分度。

Conclusion: 本研究表明，LLM的不确定性传达能力是可以通过训练来提高并实现跨领域泛化的。但是，不同的元认知技能之间不会自然地相互促进，必须通过多任务训练才能同时发展它们。

Abstract: Large language models (LLMs) are increasingly used in decision-making
contexts, but when they present answers without signaling low confidence, users
may unknowingly act on erroneous outputs. While prior work shows that LLMs
maintain internal uncertainty signals, their explicit verbalized confidence is
typically miscalibrated and poorly discriminates between correct and incorrect
answers. Across two types of LLMs, we investigate whether supervised finetuning
can improve models' ability to communicate uncertainty and whether such
improvements generalize across tasks and domains. We finetune the LLMs on
datasets spanning general knowledge, mathematics, and open-ended trivia, and
evaluate two metacognitive tasks: (1) single-question confidence estimation,
where the model assigns a numeric certainty to its answer, and (2) pairwise
confidence comparison, where the model selects which of two answers it is more
likely to have correct. We assess generalization to unseen domains, including
medical and legal reasoning. Results show that finetuning improves calibration
(alignment between stated confidence and accuracy) and discrimination (higher
confidence for correct vs. incorrect responses) within and across domains,
while leaving accuracy unchanged. However, improvements are task-specific:
training on single-question calibration does not transfer to pairwise
comparison, and vice versa. In contrast, multitask finetuning on both forms of
metacognition yields broader gains, producing lower calibration error and
stronger discrimination in out-of-domain evaluations. These results show that
while uncertainty communication in LLMs is trainable and generalizable,
different metacognitive skills do not naturally reinforce one another and must
be developed together through multitask training.

</details>


### [84] [Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models](https://arxiv.org/abs/2510.05128)
*Si-Ioi Ng,Pranav S. Ambadi,Kimberly D. Mueller,Julie Liss,Visar Berisha*

Main category: cs.CL

TL;DR: 本研究提出了一种基于BERT的流水线，用于自动提取和排序图片描述中的内容信息单元（CIUs），以评估认知语言障碍。


<details>
  <summary>Details</summary>
Motivation: 现有方法在通过图片描述评估认知语言障碍时，忽视了视觉叙事路径（元素被描述的顺序和位置）。虽然现有的时空语义特征分析可以捕捉这条路径，但手动标记或基于词典的映射效率低下。

Method: 提出了一种基于BERT的流水线，并使用二元交叉熵和成对排序损失进行微调，以实现CIU的自动提取和排序。通过5折交叉验证进行评估。

Result: 该方法在CIU检测方面达到了93%的中位数精确率和96%的中位数召回率，序列错误率为24%。提取的特征与真实值具有很强的皮尔逊相关性，并且在外部分析中优于基于词典的基线方法。这些特征在通过ANCOVA评估群体差异方面，表现与手动标注的特征相当。

Conclusion: 该流水线能够有效表征用于认知障碍评估的视觉叙事路径，并且已开源实现和模型。

Abstract: Current methods for automated assessment of cognitive-linguistic impairment
via picture description often neglect the visual narrative path - the sequence
and locations of elements a speaker described in the picture. Analyses of
spatio-semantic features capture this path using content information units
(CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This
study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and
pairwise ranking loss, for automated CIU extraction and ordering from the
Cookie Theft picture description. Evaluated by 5-fold cross-validation, it
achieves 93% median precision, 96% median recall in CIU detection, and 24%
sequence error rates. The proposed method extracts features that exhibit strong
Pearson correlations with ground truth, surpassing the dictionary-based
baseline in external validation. These features also perform comparably to
those derived from manual annotations in evaluating group differences via
ANCOVA. The pipeline is shown to effectively characterize visual narrative
paths for cognitive impairment assessment, with the implementation and models
open-sourced to public.

</details>


### [85] [Automated Alignment of Math Items to Content Standards in Large-Scale Assessments Using Language Models](https://arxiv.org/abs/2510.05129)
*Qingshu Xu,Hong Jiao,Tianyi Zhou,Ming Li,Nan Zhang,Sydney Peters,Yanbin Fu*

Main category: cs.CL

TL;DR: 本研究评估了三种自动化方法，用于将测验项目与内容标准（领域和技能标签）进行对齐，并比较了各种机器学习和深度学习模型（包括BERT变体和集成学习）的性能。


<details>
  <summary>Details</summary>
Motivation: 在大型评估中，将项目准确地与内容标准对齐对于有效分数解释至关重要。

Method: 研究人员提取了词嵌入并训练了监督机器学习模型，还研究了降维的影响。其次，他们对8个BERT模型及其变体进行了微调，以实现领域和技能的对齐。最后，他们探索了使用多数投票和堆叠的集成学习。

Result: DeBERTa-v3-base在领域对齐方面取得了最高的加权平均F1分数（0.950），而RoBERTa-large在技能对齐方面取得了最高的F1分数（0.869）。降维提高了基于嵌入的线性分类器的性能，但整体性能不如语言模型。集成模型未能超越表现最佳的语言模型。

Conclusion: 本研究展示了自动化项目与内容标准对齐的不同方法，并强调了DeBERTa-v3-base和RoBERTa-large在各自任务中的有效性。

Abstract: Accurate alignment of items to content standards is critical for valid score
interpretation in large-scale assessments. This study evaluates three automated
paradigms for aligning items with four domain and nineteen skill labels. First,
we extracted embeddings and trained multiple classical supervised machine
learning models, and further investigated the impact of dimensionality
reduction on model performance. Second, we fine-tuned eight BERT model and its
variants for both domain and skill alignment. Third, we explored ensemble
learning with majority voting and stacking with multiple meta-models. The
DeBERTa-v3-base achieved the highest weighted-average F1 score of 0.950 for
domain alignment while the RoBERTa-large yielded the highest F1 score of 0.869
for skill alignment. Ensemble models did not surpass the best-performing
language models. Dimension reduction enhanced linear classifiers based on
embeddings but did not perform better than language models. This study
demonstrated different methods in automated item alignment to content
standards.}

</details>


### [86] [Submodular Context Partitioning and Compression for In-Context Learning-short paper](https://arxiv.org/abs/2510.05130)
*Shaoyi Zheng,Canyu Zhang,Tianyi Zhou,Shengjie Wang*

Main category: cs.CL

TL;DR: ICL在LLM中实现高效的少样本学习，但Transformer的二次输入复杂度限制了样本数量。现有方法通过分块处理上下文来提高效率，但可能导致信息冗余或表示不足。本文提出Sub-CP，一个利用子模态目标控制块多样性的感知块上下文选择框架，实现了从全局多样化到局部一致化的灵活选择，并在各种任务和模型规模上均取得了性能提升。


<details>
  <summary>Details</summary>
Motivation: Transformer的二次输入复杂度限制了ICL中的样本数量，现有分块处理方法存在信息冗余或表示不足的问题。

Method: 提出Sub-CP，一个利用子模态目标控制块多样性的感知块上下文选择框架，允许在全局多样化和局部一致化之间进行灵活选择。

Result: 在多种数据集和任务上的广泛实验表明，Sub-CP在不同模型规模下均能持续提高性能。

Conclusion: Sub-CP通过控制块多样性，解决了现有ICL方法在分块处理上下文时遇到的信息冗余和表示不足问题，从而提高了ICL的效率和性能。

Abstract: In-context learning (ICL) enables efficient few-shot learning in large
language models (LLMs) without training, but suffers from the quadratic input
complexity of transformers, limiting the maximum number of exemplars. While
various efficient ICL approaches partition the context into blocks to process
(e.g., ensembling, compression, cross-attention), they often ignore the
information redundancy or under-representation caused by different partition
strategies, leading to suboptimal performance. To tackle this problem, we
propose Sub-CP, a block-aware context selection framework that leverages
submodular objectives to control block diversity. Sub-CP supports a flexible
spectrum of selection strategies, allowing each block to range from globally
diverse to locally coherent. This allows fine-grained control over semantic
structure while enabling precomputation. Extensive experiments across diverse
tasks on multiple datasets show that Sub-CP consistently improves performance
across model scales.

</details>


### [87] [Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery](https://arxiv.org/abs/2510.05131)
*Bowen Wei*

Main category: cs.CL

TL;DR: GoEngage平台上的Head Start项目新员工难以找到相关任务，提出了一种结合词汇检索、向量相似性和LLM重排的混合语义搜索系统，以提高任务检索效率和准确性。


<details>
  <summary>Details</summary>
Motivation: Head Start项目新员工在GoEngage平台上面临任务检索困难，原因是领域术语、系统术语和词汇搜索的局限性。

Method: 提出了一种混合语义搜索系统，结合了容错词汇检索、基于嵌入的向量相似性和约束式LLM重排，并利用了现有的任务库和知识库基础设施。

Result: 该系统具有低误报率、适应术语变化的可演化性以及通过缓存、候选列表生成和优雅降级机制实现的经济高效性。并提供了详细的资源需求、分阶段实施策略、离线评估协议和在线测量方法。

Conclusion: 该混合语义搜索系统有望解决Head Start项目在GoEngage平台上面临的任务检索挑战，并提供了详细的评估和实施方案。

Abstract: Head Start programs utilizing GoEngage face significant challenges when new
or rotating staff attempt to locate appropriate Tasks (modules) on the platform
homepage. These difficulties arise from domain-specific jargon (e.g., IFPA,
DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent
limitations of lexical search in handling typos and varied word ordering. We
propose a pragmatic hybrid semantic search system that synergistically combines
lightweight typo-tolerant lexical retrieval, embedding-based vector similarity,
and constrained large language model (LLM) re-ranking. Our approach leverages
the organization's existing Task Repository and Knowledge Base infrastructure
while ensuring trustworthiness through low false-positive rates, evolvability
to accommodate terminological changes, and economic efficiency via intelligent
caching, shortlist generation, and graceful degradation mechanisms. We provide
a comprehensive framework detailing required resources, a phased implementation
strategy with concrete milestones, an offline evaluation protocol utilizing
curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online
measurement methodology incorporating query success metrics, zero-result rates,
and dwell-time proxies.

</details>


### [88] [Training Large Language Models To Reason In Parallel With Global Forking Tokens](https://arxiv.org/abs/2510.05132)
*Sheng Jia,Xiao Wang,Shiva Prasad Kasiviswanathan*

Main category: cs.CL

TL;DR: 通过引入基于集合的全局损失和自监督二分匹配，SSFT方法在保持多样性和准确性之间取得了更好的平衡，并在多个推理基准测试中优于SFT。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM并行推理方法在处理复杂问题时，在多样性和准确性之间存在权衡，尤其是在需要深入采样树以获得多样化推理路径时。

Method: 将并行推理视为一个“下一个词预测”问题，并提出了一种称为Set Supervised Fine-Tuning（SSFT）的方法，该方法在监督微调（SFT）中加入了一个基于集合的全局损失，并利用了全局分叉词元和唯一推理轨迹之间的自监督二分匹配。

Result: SSFT方法能够保留多样化的推理模式，并产生新兴的全局分叉词元，在Pass@1和Cons@k指标上均优于标准的SFT方法。

Conclusion: SSFT方法通过利用集合匹配来优化LLM的并行推理，有效解决了现有方法在多样性和准确性之间的权衡问题，并在多个推理任务上展示了优越的性能。

Abstract: Although LLMs have demonstrated improved performance by scaling parallel
test-time compute, doing so relies on generating reasoning paths that are both
diverse and accurate. For challenging problems, the forking tokens that trigger
diverse yet correct reasoning modes are typically deep in the sampling tree.
Consequently, common strategies to encourage diversity, such as temperature
scaling, encounter a worsened trade-off between diversity and accuracy.
Motivated by this challenge, we treat parallel reasoning as a
set-of-next-token-prediction problem, and incorporate a set-based global loss
into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching
between our global forking tokens and unique reasoning traces. We observe that,
while naive fine-tuning with multiple reasoning traces collapses these unique
reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT),
preserves these modes and produces emergent global forking tokens. Experiments
on multiple reasoning benchmarks show that our SSFT consistently outperforms
SFT under both Pass@1 and Cons@k metrics.

</details>


### [89] [Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios](https://arxiv.org/abs/2510.05133)
*Y. Du,G. Wu,G. Tang,W. Wang,Q. Fan*

Main category: cs.CL

TL;DR: LLM生成合成数据在NLP训练中至关重要，但其比例对模型行为的影响尚不明确。本研究通过控制实验，研究了不同合成数据比例对模型性能、校准和输出特性的影响。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在系统地理解合成数据比例如何影响不同规模模型Across在不同任务上的行为，为实践者提供合成数据使用预算的指导。

Method: 使用Pythia模型套件（410M-12B参数），在五个不同任务上，对模型在合成数据比例从0-50%的条件下进行一到三次训练迭代，评估其性能、校准和输出特性。

Result: 研究发现：1）模型在合成数据比例高达20%时性能稳定，超过30%后性能下降加速；2）更大模型（6.9B-12B）比小型模型（410M-1.4B）更能抵抗合成数据的影响；3）模型校准下降先于准确率损失，可作为早期预警信号；4）任务特性很重要，推理任务比检索任务更容易受到合成数据的影响。现有最佳实践（如STaR和Self-Instruct）使用的外部数据比例（>80%）处于安全范围内。

Conclusion: 合成数据比例对模型性能和校准有显著影响，但存在一个安全阈值。模型规模和任务特性会影响该阈值。本研究为实践者提供了关于合成数据预算的实用指导。

Abstract: Synthetic data generated by large language models has become integral to
modern NLP training pipelines, from bootstrapping reasoning capabilities to
augmenting instruction-following datasets. While recent work demonstrates
successful applications maintaining high external data ratios, systematic
understanding of how synthetic data proportion affects model behavior across
different scales remains limited. This paper presents a controlled empirical
study examining model performance, calibration, and output characteristics when
trained on varying synthetic-to-external data ratios. Using the Pythia model
suite (410M-12B parameters) across five diverse tasks, we evaluate models after
one to three training iterations with synthetic data proportions ranging from
0-50\%. Our key findings include: models maintain stable performance with up to
20\% synthetic data, but degradation accelerates beyond 30\%; larger models
(6.9B-12B) show greater robustness to synthetic data than smaller models
(410M-1.4B); calibration degradation precedes accuracy loss, providing an early
warning signal; and task characteristics matter, with reasoning tasks degrading
faster than retrieval tasks under synthetic data training. Importantly, we find
that current best practices, such as those employed in STaR and Self-Instruct
systems that maintain greater than 80\% external data, operate well within safe
regimes identified by our experiments. We provide practical guidance for
practitioners on synthetic data budgets based on model scale and task
requirements, alongside detailed comparison with concurrent work including
Shumailov et al.'s model collapse findings.

</details>


### [90] [Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment](https://arxiv.org/abs/2510.05135)
*Vanya Bannihatti Kumar,Divyanshu Goyal,Akhil Eppa,Neel Bhandari*

Main category: cs.CL

TL;DR: LLMs can be trained to judge creative writing by learning individual preferences, outperforming standard methods.


<details>
  <summary>Details</summary>
Motivation: Traditional LLMs struggle with subjective tasks like evaluating creativity, necessitating a new approach for personalized creative assessment.

Method: A curiosity-driven LLM-as-a-judge was developed to learn individual creative judgments using the TTCW benchmark, which features human-annotated stories on dimensions like originality.

Result: The proposed method showed improvements over baseline supervised finetuning (SFT) across various metrics (Pearson correlation, Cohen's , F1), demonstrating its effectiveness in learning nuanced creative judgments for different individuals and handling inter-annotator disagreement.

Conclusion: The developed curiosity-driven LLM-as-a-judge offers a novel and effective solution for personalized, subjective evaluations of creative writing, particularly in scenarios with diverse annotator opinions.

Abstract: Modern large language models (LLMs) excel at objective tasks such as
evaluating mathematical reasoning and factual accuracy, yet they falter when
faced with the nuanced, subjective nature of assessing creativity. In this
work, we propose a novel curiosity-driven LLM-as-a-judge for evaluating
creative writing which is personlized to each individual's creative judgments.
We use the Torrance Test of Creative Thinking(TTCW) benchmark introduced in
Chakrabarty et al. (2024), which has stories annotated by expert humans across
various subjective dimensions like Originality, to test our hypothesis. We show
that our method enables models across various sizes, to learn the nuanced
creative judgments of different individuals, by showing improvements over
baseline supervised finetuning(SFT) method across various evaluation metrics
like Pearson correlation, Cohen's and F1 values. Our method is especially
useful in subjective evaluations where not all the annotators agree with each
other.

</details>


### [91] [Linguistic Characteristics of AI-Generated Text: A Survey](https://arxiv.org/abs/2510.05136)
*Luka Terčon,Kaja Dobrovoljc*

Main category: cs.CL

TL;DR: LLM生成文本的语言特征研究现状的综述。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在各领域的广泛应用，研究其生成文本的语言特征变得日益重要，以促进对该领域更深入的理解。

Method: 对现有研究进行分类，包括语言描述层面、模型、体裁、语言和提示方法，并展示当前研究的趋势和发现。

Result: AI生成文本通常更正式、更 impersonal，名词、限定词和介词使用增多，形容词和副词使用减少；词汇多样性较低，词汇量较小，且文本具有重复性。现有研究主要集中在英语和GPT模型，忽视了跨语言和跨模型的研究，以及提示词敏感性问题。

Conclusion: AI生成文本的语言特征研究是一个快速发展的领域，但需要更广泛的跨语言、跨模型的研究，并关注提示词对生成文本的影响。

Abstract: Large language models (LLMs) are solidifying their position in the modern
world as effective tools for the automatic generation of text. Their use is
quickly becoming commonplace in fields such as education, healthcare, and
scientific research. There is a growing need to study the linguistic features
present in AI-generated text, as the increasing presence of such texts has
profound implications in various disciplines such as corpus linguistics,
computational linguistics, and natural language processing. Many observations
have already been made, however a broader synthesis of the findings made so far
is required to provide a better understanding of the topic. The present survey
paper aims to provide such a synthesis of extant research. We categorize the
existing works along several dimensions, including the levels of linguistic
description, the models included, the genres analyzed, the languages analyzed,
and the approach to prompting. Additionally, the same scheme is used to present
the findings made so far and expose the current trends followed by researchers.
Among the most-often reported findings is the observation that AI-generated
text is more likely to contain a more formal and impersonal style, signaled by
the increased presence of nouns, determiners, and adpositions and the lower
reliance on adjectives and adverbs. AI-generated text is also more likely to
feature a lower lexical diversity, a smaller vocabulary size, and repetitive
text. Current research, however, remains heavily concentrated on English data
and mostly on text generated by the GPT model family, highlighting the need for
broader cross-linguistic and cross-model investigation. In most cases authors
also fail to address the issue of prompt sensitivity, leaving much room for
future studies that employ multiple prompt wordings in the text generation
phase.

</details>


### [92] [Demystifying deep search: a holistic evaluation with hint-free multi-hop questions and factorised metrics](https://arxiv.org/abs/2510.05137)
*Maojia Song,Renhang Liu,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Soujanya Poria,Jingren Zhou*

Main category: cs.CL

TL;DR: WebDetective是一个新的基准测试和评估框架，用于评估多跳搜索任务中的RAG系统和网络代理，解决了现有基准测试中推理路径泄露和评估过于简化的缺点。它通过无提示问题、可控的维基百科沙盒和多维度的评估指标（搜索充分性、知识利用和拒绝行为）来诊断模型弱点，并提出了一种名为EvidenceLoop的代理工作流以指导模型改进。


<details>
  <summary>Details</summary>
Motivation: 当前对RAG系统和网络代理在多跳搜索任务上的评估存在两大局限：1. 许多基准测试在问题中泄露了推理路径，导致模型依赖表面线索而非自主发现推理链。2. 评估通常简化为单一的通过率，无法区分搜索不足、知识利用不当或不当拒绝等失败原因。

Method: 提出WebDetective基准测试，包含无提示的多跳问题和一个受控的维基百科沙盒，确保模型行为的可追溯性。同时，开发了一个整体评估框架，区分搜索充分性、知识利用和拒绝行为。在此基础上，提出了一种名为EvidenceLoop的代理工作流，包含验证循环和证据跟踪，以解决基准测试暴露出的问题。

Result: 评估25个先进模型后发现，所有模型在知识利用方面都存在系统性弱点，即使有充分证据也难以有效利用；在缺乏证据时，模型几乎从不进行恰当的拒绝。这表明当前系统擅长执行给定推理路径，但在发现推理路径方面存在缺陷。EvidenceLoop基线模型显示，WebDetective的诊断框架能够指导具体的架构改进。

Conclusion: WebDetective基准测试及其评估框架能够有效地诊断RAG系统和网络代理在多跳搜索任务中的系统性弱点，特别是知识利用和恰当拒绝方面。提出的EvidenceLoop工作流证明了该框架在指导模型改进方面的潜力，有助于开发真正的自主推理系统，而非仅仅模仿模式的代理。

Abstract: RAG (Retrieval-Augmented Generation) systems and web agents are increasingly
evaluated on multi-hop deep search tasks, yet current practice suffers from two
major limitations. First, most benchmarks leak the reasoning path in the
question text, allowing models to follow surface cues rather than discover
reasoning chains autonomously. Second, evaluation is typically reduced to a
single pass rate, which collapses diverse behaviours into one score and
obscures whether failures stem from inadequate search, poor knowledge use, or
inappropriate refusal. To address these issues, we present WebDetective, a
benchmark of hint-free multi-hop questions paired with a controlled Wikipedia
sandbox that ensures full traceability of model actions, and a holistic
evaluation framework that separates search sufficiency, knowledge utilisation,
and refusal behaviour. Our evaluation of 25 state-of-the-art models reveals
systematic weaknesses across all architectures: models struggle with knowledge
utilisation despite having sufficient evidence and demonstrate near-absent
appropriate refusal when evidence is lacking. These patterns expose a
fundamental gap: today's systems excel at executing given reasoning paths but
fail when required to discover them. We develop an agentic workflow,
EvidenceLoop, that explicitly targets the challenges our benchmark identifies,
incorporating verification loops and systematic evidence tracking that improve
both search and synthesis capabilities. This baseline demonstrates that
WebDetective's diagnostic framework can guide concrete architectural
improvements, establishing our benchmark as a critical tool for developing
genuinely autonomous reasoning systems rather than pattern-following agents.

</details>


### [93] [LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation](https://arxiv.org/abs/2510.05138)
*Gregory Hok Tjoan Go,Khang Ly,Anders Søgaard,Amin Tabatabaei,Maarten de Rijke,Xinyi Chen*

Main category: cs.CL

TL;DR: LiRA是一个多智能体协作流程，用于自动化科学文献综述的写作，在写作和引用质量方面优于现有基线，并能生成与人类撰写的综述相似的、连贯且全面的综述文章。


<details>
  <summary>Details</summary>
Motivation: 科学出版物快速增长，使得文献综述难以全面和及时更新。现有工作主要集中在自动化检索和筛选，而综述写作阶段，特别是可读性和事实准确性方面，仍有待探索。

Method: LiRA（Literature Review Agents）是一个多智能体协作流程，模仿人类文献综述过程。它使用专门的智能体进行内容大纲设计、子部分写作、编辑和审阅，以生成连贯且全面的综述文章。

Result: 在SciReviewGen和专有的ScienceDirect数据集上评估，LiRA在写作和引用质量方面优于AutoSurvey和MASS-Survey等当前基线，同时保持与人类撰写综述相近的相似度。在真实世界场景中，通过文档检索评估了LiRA的鲁棒性。

Conclusion: 研究结果表明，即使没有领域特定的调整，智能体LLM工作流程也有潜力提高自动化科学写作的可靠性和可用性。

Abstract: The rapid growth of scientific publications has made it increasingly
difficult to keep literature reviews comprehensive and up-to-date. Though prior
work has focused on automating retrieval and screening, the writing phase of
systematic reviews remains largely under-explored, especially with regard to
readability and factual accuracy. To address this, we present LiRA (Literature
Review Agents), a multi-agent collaborative workflow which emulates the human
literature review process. LiRA utilizes specialized agents for content
outlining, subsection writing, editing, and reviewing, producing cohesive and
comprehensive review articles. Evaluated on SciReviewGen and a proprietary
ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey
and MASS-Survey in writing and citation quality, while maintaining competitive
similarity to human-written reviews. We further evaluate LiRA in real-world
scenarios using document retrieval and assess its robustness to reviewer model
variation. Our findings highlight the potential of agentic LLM workflows, even
without domain-specific tuning, to improve the reliability and usability of
automated scientific writing.

</details>


### [94] [NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description](https://arxiv.org/abs/2510.05139)
*Hamed Jelodar,Mohammad Meymani,Parisa Hamedi,Tochukwu Emmanuel Nwankwo,Samita Bai,Roozbeh Razavi-Far,Ali A. Ghorbani*

Main category: cs.CL

TL;DR: NLD-LLM是一个用于评估语言模型生成代码描述的NLP框架，通过精心设计的提示和迭代优化，发现提示工程对模型性能有显著影响，即使是较小的模型在良好提示下也能表现出色。


<details>
  <summary>Details</summary>
Motivation: 评估语言模型在自然语言描述（NLD）任务上的性能，特别是生成准确、简洁的代码描述的能力。

Method: 提出NLD-LLM框架，集成多种Transformer模型（Qwen, DeepSeek, Phi, LLaMA, Mistral），采用标准化的提示设计策略（包括格式化、任务指导和NLD提示），并应用迭代优化过程来提高输出质量和评估模型适应性。使用语义和结构指标进行分析。

Result: 提示工程显著影响模型性能，表明精心设计的提示可以使小型模型在代码描述生成任务上表现出与大型模型相媲美的竞争力。

Conclusion: 提示工程在NLD任务中至关重要，可以通过优化提示来提升模型性能，并使小型模型更具竞争力。

Abstract: Natural Language Description (NLD) is a Natural Language Processing (NLP)
task that requires models to generate structured and meaningful outputs from
natural language inputs. In this work, we propose NLD-LLM, a systematic NLP
framework to evaluate the performance of language models to generate accurate
and concise source code descriptions. This framework incorporates a diverse set
of transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral,
spanning various sizes, architectures, and training approaches. Central to
NLD-LLM is a comprehensive prompt design strategy that includes standardized
formatting, clear task guidance, and NLD prompting, ensuring fair and
consistent evaluation. Additionally, we apply an iterative refinement process
to improve output's quality and assess the model's adaptability. Using semantic
and structural metrics, our analysis demonstrates that prompt engineering
significantly impacts the effectiveness of the model such that smaller models
often performing competitively when supported by well-crafted prompts.

</details>


### [95] [To model human linguistic prediction, make LLMs less superhuman](https://arxiv.org/abs/2510.05141)
*Byung-Doh Oh,Tal Linzen*

Main category: cs.CL

TL;DR: LLMs在预测人类语言能力方面已超越人类，主要归因于其强大的长短期记忆。文章提出应开发具有类人记忆能力的LLMs，并指出现有数据集不足以衡量此目标进展，同时提出相关实验方向。


<details>
  <summary>Details</summary>
Motivation: LLMs在预测人类语言能力方面已超越人类，文章旨在探讨LLMs超乎人类的预测能力的原因，并提出开发具有类人记忆能力的LLMs的方向。

Method: 本文通过分析LLMs在预测人类语言方面的表现，指出其超乎人类的预测能力主要源于其强大的长短期记忆。

Result: LLMs在预测人类语言方面已超越人类，这主要得益于其强大的长短期记忆能力，导致其在预测难度上低于人类实验观察到的结果。

Conclusion: 文章认为LLMs的超乎人类的表现主要归因于其在长短期记忆方面优于人类，并提出应开发具有类人记忆能力的LLMs。同时，文章指出当前人类数据不足以衡量此目标进展，并提出相关实验方向。

Abstract: When people listen to or read a sentence, they actively make predictions
about upcoming words: words that are less predictable are generally read more
slowly than predictable ones. The success of large language models (LLMs),
which, like humans, make predictions about upcoming words, has motivated
exploring the use of these models as cognitive models of human linguistic
prediction. Surprisingly, in the last few years, as language models have become
better at predicting the next word, their ability to predict human reading
behavior has declined. This is because LLMs are able to predict upcoming words
much better than people can, leading them to predict lower processing
difficulty in reading than observed in human experiments; in other words,
mainstream LLMs are 'superhuman' as models of language comprehension. In this
position paper, we argue that LLMs' superhumanness is primarily driven by two
factors: compared to humans, LLMs have much stronger long-term memory for facts
and training examples, and they have much better short-term memory for previous
words in the text. We advocate for creating models that have human-like
long-term and short-term memory, and outline some possible directions for
achieving this goal. Finally, we argue that currently available human data is
insufficient to measure progress towards this goal, and outline human
experiments that can address this gap.

</details>


### [96] [Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models](https://arxiv.org/abs/2510.05142)
*Xin Wang,Anshu Raj,Matthew Luebbe,Haiming Wen,Shuozhi Xu,Kun Lu*

Main category: cs.CL

TL;DR: 利用大型语言模型提取材料数据，实现对材料成分、加工、微观结构和性能的全面解析。


<details>
  <summary>Details</summary>
Motivation: 现有材料信息提取方法存在局限，无法全面整合材料的成分、加工、微观结构和性能等关键信息，阻碍了综合性材料数据库的构建。

Method: 提出一个多阶段、基于大型语言模型的信??提取流程，该流程整合了迭代提取和来源追踪技术，能够提取47种特征，涵盖材料的成分、加工、微观结构和性能。

Result: 在特征和元组层面均达到约0.96的F1分数，相比于无来源追踪的单次提取，在微观结构类别上的F1分数分别提高了10.0%（特征层面）和13.7%（元组层面），并显著降低了遗漏材料的数量（从396篇文献中的49个减少到13个）。

Conclusion: 该流程能够高效、大规模地从文献中挖掘材料信息，构建高精度、低遗漏、零误报的数据库，为机器学习和材料信息学提供可靠的数据支持，并且其模块化设计可应用于不同材料类别的信息提取。

Abstract: Data-driven materials discovery requires large-scale experimental datasets,
yet most of the information remains trapped in unstructured literature.
Existing extraction efforts often focus on a limited set of features and have
not addressed the integrated composition-processing-microstructure-property
relationships essential for understanding materials behavior, thereby posing
challenges for building comprehensive databases. To address this gap, we
propose a multi-stage information extraction pipeline powered by large language
models, which captures 47 features spanning composition, processing,
microstructure, and properties exclusively from experimentally reported
materials. The pipeline integrates iterative extraction with source tracking to
enhance both accuracy and reliability. Evaluations at the feature level
(independent attributes) and tuple level (interdependent features) yielded F1
scores around 0.96. Compared with single-pass extraction without source
tracking, our approach improved F1 scores of microstructure category by 10.0%
(feature level) and 13.7% (tuple level), and reduced missed materials from 49
to 13 out of 396 materials in 100 articles on precipitate-containing
multi-principal element alloys (miss rate reduced from 12.4% to 3.3%). The
pipeline enables scalable and efficient literature mining, producing databases
with high precision, minimal omissions, and zero false positives. These
datasets provide trustworthy inputs for machine learning and materials
informatics, while the modular design generalizes to diverse material classes,
enabling comprehensive materials information extraction.

</details>


### [97] [SynCED-EnDe 2025: A Synthetic and Curated English - German Dataset for Critical Error Detection in Machine Translation](https://arxiv.org/abs/2510.05144)
*Muskaan Chopra,Lorenz Sparrenberg,Rafet Sifa*

Main category: cs.CL

TL;DR: WMT21 CED 数据集存在规模、标签平衡、领域覆盖和时效性方面的不足。本研究提出了 SynCED-EnDe，一个包含 1,000 个黄金标签和 8,000 个银标签的英文-德文句子对数据集，错误和非错误案例各占 50%。该数据集来源于 2024-2025 年的多元化来源（StackExchange、GOV.UK），并包含明确的错误子类、触发器标志和细粒度的辅助判断（明显性、严重性、本地化复杂度、上下文依赖性、充分性偏差）。这些丰富的内容超越了二元检测，实现了对错误风险和复杂度的系统性分析。数据集托管在 GitHub 和 Hugging Face 上，并提供文档、标注指南和基线脚本。XLM-R 等编码器的基准实验表明，由于标签平衡和注释的改进，性能得到了显著提升。SynCED-EnDe 旨在成为一个社区资源，以推动机器翻译在信息检索和对话助手中的安全部署，特别是在可穿戴 AI 设备等新兴领域。


<details>
  <summary>Details</summary>
Motivation: WMT21 英文-德文关键错误检测（CED）数据集在规模、标签平衡、领域覆盖和时效性方面存在局限性，需要一个更全面、更细致的数据集来推动该领域的研究和应用。

Method: 构建了一个包含 1,000 个黄金标签和 8,000 个银标签的英文-德文句子对数据集（SynCED-EnDe），数据来源于 2024-2025 年的 StackExchange 和 GOV.UK 等多元化来源。数据集在错误和非错误案例之间进行了 50/50 的平衡，并引入了明确的错误子类、结构化触发器标志以及细粒度的辅助判断（如明显性、严重性、本地化复杂度、上下文依赖性、充分性偏差）。最后，使用 XLM-R 和相关编码器进行了基准实验。

Result: SynCED-EnDe 数据集在标签平衡和注释的精细化方面优于 WMT21 数据集。基于 XLM-R 和相关编码器的基准实验表明，在 SynCED-EnDe 数据集上取得了显著的性能提升。

Conclusion: SynCED-EnDe 是一个比 WMT21 更优越的英文-德文关键错误检测数据集，它通过提供更大规模、更平衡、更细致的数据，有望推动机器翻译在信息检索、对话助手和新兴领域（如可穿戴 AI 设备）中的安全应用。研究者可以利用该数据集进行更深入的错误风险和复杂性分析。

Abstract: Critical Error Detection (CED) in machine translation aims to determine
whether a translation is safe to use or contains unacceptable deviations in
meaning. While the WMT21 English-German CED dataset provided the first
benchmark, it is limited in scale, label balance, domain coverage, and temporal
freshness. We present SynCED-EnDe, a new resource consisting of 1,000
gold-labeled and 8,000 silver-labeled sentence pairs, balanced 50/50 between
error and non-error cases. SynCED-EnDe draws from diverse 2024-2025 sources
(StackExchange, GOV.UK) and introduces explicit error subclasses, structured
trigger flags, and fine-grained auxiliary judgments (obviousness, severity,
localization complexity, contextual dependency, adequacy deviation). These
enrichments enable systematic analyses of error risk and intricacy beyond
binary detection. The dataset is permanently hosted on GitHub and Hugging Face,
accompanied by documentation, annotation guidelines, and baseline scripts.
Benchmark experiments with XLM-R and related encoders show substantial
performance gains over WMT21 due to balanced labels and refined annotations. We
envision SynCED-EnDe as a community resource to advance safe deployment of MT
in information retrieval and conversational assistants, particularly in
emerging contexts such as wearable AI devices.

</details>


### [98] [Every Step Counts: Decoding Trajectories as Authorship Fingerprints of dLLMs](https://arxiv.org/abs/2510.05148)
*Qi Li,Runpeng Yu,Haiquan Lu,Xinchao Wang*

Main category: cs.CL

TL;DR: dLLMs的解码机制可用于模型归因，提出DDM提取结构信息，GTA计算归因得分。


<details>
  <summary>Details</summary>
Motivation: dLLMs的解码机制除了提升模型效用外，还可以作为强大的模型归因工具，但需要解决归因场景多样性和信息提取与利用的问题。

Method: 提出方向解码图（DDM）提取解码轨迹的结构关系，并提出高斯轨迹归因（GTA）方法，计算轨迹在特定模型下解码轨迹的似然度作为归因得分。

Result: DDM和GTA方法在不同设置下都有效，验证了其在模型归因方面的实用性。

Conclusion: DDM和GTA是dLLMs模型归因的有效方法，能够克服现有方法在处理多场景和利用解码轨迹信息方面的不足。

Abstract: Discrete Diffusion Large Language Models (dLLMs) have recently emerged as a
competitive paradigm for non-autoregressive language modeling. Their
distinctive decoding mechanism enables faster inference speed and strong
performance in code generation and mathematical tasks. In this work, we show
that the decoding mechanism of dLLMs not only enhances model utility but also
can be used as a powerful tool for model attribution. A key challenge in this
problem lies in the diversity of attribution scenarios, including
distinguishing between different models as well as between different
checkpoints or backups of the same model. To ensure broad applicability, we
identify two fundamental problems: what information to extract from the
decoding trajectory, and how to utilize it effectively. We first observe that
relying directly on per-step model confidence yields poor performance. This is
mainly due to the bidirectional decoding nature of dLLMs: each newly decoded
token influences the confidence of other decoded tokens, making model
confidence highly redundant and washing out structural signal regarding
decoding order or dependencies. To overcome this, we propose a novel
information extraction scheme called the Directed Decoding Map (DDM), which
captures structural relationships between decoding steps and better reveals
model-specific behaviors. Furthermore, to make full use of the extracted
structural information during attribution, we propose Gaussian-Trajectory
Attribution (GTA), where we fit a cell-wise Gaussian distribution at each
decoding position for each target model, and define the likelihood of a
trajectory as the attribution score: if a trajectory exhibits higher
log-likelihood under the distribution of a specific model, it is more likely to
have been generated by that model. Extensive experiments under different
settings validate the utility of our methods.

</details>


### [99] [Chronological Thinking in Full-Duplex Spoken Dialogue Language Models](https://arxiv.org/abs/2510.05150)
*Donghang Wu,Haoyang Zhang,Chen Chen,Tianyu Zhang,Fei Tian,Xuerui Yang,Gang Yu,Hexin Liu,Nana Hou,Yuchen Hu,Eng Siong Chng*

Main category: cs.CL

TL;DR: Chronological Thinking enables continuous, real-time thinking during conversations in full-duplex spoken dialogue systems, improving response quality without adding latency.


<details>
  <summary>Details</summary>
Motivation: Existing full-duplex spoken dialogue systems keep the agent idle by repeatedly predicting silence tokens during the listening phase, which is unlike human behavior. The goal is to improve response quality by enabling the agent to perform lightweight thinking during this idle time.

Method: Chronological Thinking is a novel, on-the-fly conversational thinking mechanism for full-duplex SDLMs. It is strictly causal, meaning the agent reasons incrementally based only on past audio without lookahead. The reasoning process is amortized during the listening window, so there is no additional latency once the user stops speaking and the agent begins responding.

Result: Experiments show that Chronological Thinking significantly improves response quality, as demonstrated by both objective metrics and human evaluations. It also robustly handles conversational dynamics and achieves competitive performance on full-duplex interaction metrics.

Conclusion: Chronological Thinking is an effective mechanism for full-duplex SDLMs that enhances response quality and conversational dynamics by enabling continuous, causal, and latency-free thinking during the listening phase, mimicking human conversational behavior.

Abstract: Recent advances in spoken dialogue language models (SDLMs) reflect growing
interest in shifting from turn-based to full-duplex systems, where the models
continuously perceive user speech streams while generating responses. This
simultaneous listening and speaking design enables real-time interaction and
the agent can handle dynamic conversational behaviors like user barge-in.
However, during the listening phase, existing systems keep the agent idle by
repeatedly predicting the silence token, which departs from human behavior: we
usually engage in lightweight thinking during conversation rather than
remaining absent-minded. Inspired by this, we propose Chronological Thinking, a
on-the-fly conversational thinking mechanism that aims to improve response
quality in full-duplex SDLMs. Specifically, chronological thinking presents a
paradigm shift from conventional LLM thinking approaches, such as
Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly
causal: the agent reasons incrementally while listening, updating internal
hypotheses only from past audio with no lookahead. (2) No additional latency:
reasoning is amortized during the listening window; once the user stops
speaking, the agent halts thinking and begins speaking without further delay.
Experiments demonstrate the effectiveness of chronological thinking through
both objective metrics and human evaluations show consistent improvements in
response quality. Furthermore, chronological thinking robustly handles
conversational dynamics and attains competitive performance on full-duplex
interaction metrics.

</details>


### [100] [Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA](https://arxiv.org/abs/2510.05151)
*Prudence Djagba,Abdelkader Y. Saley*

Main category: cs.CL

TL;DR: 领域自适应大语言模型在金融NLP任务中表现出优势（如情感分析、分类），但在数值推理、实体识别和摘要等方面存在挑战。


<details>
  <summary>Details</summary>
Motivation: 金融NLP领域对模型的准确性、可靠性和领域适应性有高要求，旨在探索领域自适应大语言模型的优势和劣势。

Method: 评估了基于PIXIU框架的FinMA模型，该模型使用了金融指令调优（FIT）数据集，并在FLARE基准下进行了评估。

Result: FinMA在情感分析和分类任务上表现良好，但在涉及数值推理、实体识别和摘要的任务上遇到挑战。

Conclusion: FinMA在特定金融任务中展现了潜力，但也指出了在数值推理和实体识别等方面的局限性，为未来金融大语言模型的设计和评估提供了方向。

Abstract: This research explores the strengths and weaknesses of domain-adapted Large
Language Models (LLMs) in the context of financial natural language processing
(NLP). The analysis centers on FinMA, a model created within the PIXIU
framework, which is evaluated for its performance in specialized financial
tasks. Recognizing the critical demands of accuracy, reliability, and domain
adaptation in financial applications, this study examines FinMA's model
architecture, its instruction tuning process utilizing the Financial
Instruction Tuning (FIT) dataset, and its evaluation under the FLARE benchmark.
Findings indicate that FinMA performs well in sentiment analysis and
classification, but faces notable challenges in tasks involving numerical
reasoning, entity recognition, and summarization. This work aims to advance the
understanding of how financial LLMs can be effectively designed and evaluated
to assist in finance-related decision-making processes.

</details>


### [101] [A Single Character can Make or Break Your LLM Evals](https://arxiv.org/abs/2510.05152)
*Jingtong Su,Jianyu Zhang,Karen Ullrich,Léon Bottou,Mark Ibrahim*

Main category: cs.CL

TL;DR: LLM评估中的示例分隔符选择会显著影响模型性能，某些分隔符甚至能操纵模型排名。通过探究注意力机制，发现有效的分隔符能引导模型关注关键信息。指定分隔符或使用推荐的分隔符可提高模型对分隔符选择的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: LLM评估通常依赖示例来引导模型响应，但示例格式（尤其是分隔符选择）对模型性能的影响被忽视了，而这可能对模型排名产生重大影响。

Method: 通过实验比较不同分隔符对Llama、Qwen、Gemma等模型在MMLU任务上的性能影响，并探究了模型在不同主题、规模下的表现。通过分析注意力分数来理解分隔符如何影响模型对关键信息的关注。实验了在提示中指定分隔符以及选择最佳分隔符的方法来提高模型的鲁棒性。

Result: 在LLM评估中，分隔符的选择可以导致MMLU分数±23%的变化，并且能够操纵模型排名。模型在不同主题、模型家族和规模下都表现出对分隔符选择的脆弱性。有效的分隔符能够将注意力引导至输入中的关键标记。在提示中明确指定分隔符可以提高模型的鲁棒性。

Conclusion: LLM对示例分隔符的选择非常敏感，这种敏感性会影响模型性能和排名。通过分析注意力机制可以理解这种现象。通过在提示中指定分隔符或选择最佳分隔符，可以提高LLM对分隔符选择的鲁棒性，并为实际应用提供指导。

Abstract: Common Large Language model (LLM) evaluations rely on demonstration examples
to steer models' responses to the desired style. While the number of examples
used has been studied and standardized, the choice of how to format examples is
less investigated. In evaluation protocols and real world usage, users face the
choice how to separate in-context examples: use a comma? new line? semi-colon?
hashtag? etc.? Surprisingly, we find this seemingly minor choice can
dramatically alter model response quality. Across leading model families
(Llama, Qwen, Gemma), performance on MMLU for example can vary by $\pm 23\%$
depending on the choice of delimiter. In fact, one can manipulate model
rankings to put any model in the lead by only modifying the single character
separating examples. We find LLMs' brittleness pervades topics, model families,
and doesn't improve with scale. By probing attention head scores, we find that
good-performing delimiters steer attention towards key tokens in the input.
Finally, we explore methods to improve LLMs' robustness to the choice of
delimiter. We find specifying the selected delimiter in the prompt boosts
robustness and offer practical recommendations for the best-performing
delimiters to select.

</details>


### [102] [Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs](https://arxiv.org/abs/2510.05154)
*Shenzhe Zhu,Shu Yang,Michiel A. Bakker,Alex Pentland,Jiaxin Pei*

Main category: cs.CL

TL;DR: 该研究提出了DeliberationBank数据集和DeliberationJudge模型，用于评估大型公开审议的摘要生成，解决了现有方法中LLM偏见和对少数群体意见代表性不足的问题，并证明了其模型在评估摘要的代表性、信息量、中立性和政策采纳度方面优于现有LLM评估方法。


<details>
  <summary>Details</summary>
Motivation: 现有的大型公开审议摘要方法（如使用LLM）存在代表性不足（特别是少数群体观点）和输入顺序偏见等问题，而现有评估方法（使用LLM作为裁判）与人类判断的一致性较弱，这在高风险的政策制定场景中引发了公平性担忧。

Method: 研究构建了DeliberationBank数据集，包含3000名参与者就10个审议问题的意见数据，以及4500名参与者对摘要的代表性、信息量、中立性和政策采纳度四个维度的标注数据。基于此数据集，研究训练了DeliberationJudge模型（一个微调的DeBERTa模型），用于从个体视角评估审议摘要。

Result: DeliberationJudge模型在评估摘要的准确性和效率上优于多种LLM裁判。通过使用DeliberationJudge评估18种LLM，研究揭示了在审议摘要方面LLM普遍存在不足，尤其是在代表少数群体立场方面。该研究提出的评估框架能够大规模、可靠地评估审议摘要的质量。

Conclusion: 该研究提出的DeliberationBank数据集和DeliberationJudge模型为评估和改进大型公开审议摘要的公平性和代表性提供了一个可扩展且可靠的框架，有助于确保AI系统在政策制定中的公正性和公平性。

Abstract: Large-scale public deliberations generate thousands of free-form
contributions that must be synthesized into representative and neutral
summaries for policy use. While LLMs have been shown as a promising tool to
generate summaries for large-scale deliberations, they also risk
underrepresenting minority perspectives and exhibiting bias with respect to the
input order, raising fairness concerns in high-stakes contexts. Studying and
fixing these issues requires a comprehensive evaluation at a large scale, yet
current practice often relies on LLMs as judges, which show weak alignment with
human judgments. To address this, we present DeliberationBank, a large-scale
human-grounded dataset with (1) opinion data spanning ten deliberation
questions created by 3,000 participants and (2) summary judgment data annotated
by 4,500 participants across four dimensions (representativeness,
informativeness, neutrality, policy approval). Using these datasets, we train
DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation
summaries from individual perspectives. DeliberationJudge is more efficient and
more aligned with human judgements compared to a wide range of LLM judges. With
DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in
deliberation summarization, especially underrepresentation of minority
positions. Our framework provides a scalable and reliable way to evaluate
deliberation summarization, helping ensure AI systems are more representative
and equitable for policymaking.

</details>


### [103] [A novel hallucination classification framework](https://arxiv.org/abs/2510.05189)
*Maksym Zavhorodnii,Dmytro Dehtiarov,Anna Konovalenko*

Main category: cs.CL

TL;DR: 本文提出了一种新的方法来自动检测大型语言模型（LLM）推理过程中产生的幻觉。该方法通过提示工程系统地对不同类型的幻觉进行分类和受控复现，然后使用嵌入模型将幻觉数据集映射到向量空间，并通过无监督学习技术分析降维后的数据。实验结果表明，幻觉的信息失真程度与其在向量空间中与正确输出簇的空间差异呈一致性相关。这证明了即使是简单的分类算法也能可靠地区分单个LLM产生的幻觉和准确的响应，为提高模型可靠性提供了一个轻量级且有效的框架。


<details>
  <summary>Details</summary>
Motivation: LLM推理过程中产生的幻觉是一个关键问题，影响着模型的可靠性。本文旨在提出一种自动检测这些幻觉的方法。

Method: 1. 通过提示工程对不同类型的幻觉进行分类和受控复现，构建幻觉数据集。
2. 使用嵌入模型将幻觉数据集映射到向量空间。
3. 使用无监督学习技术分析降维后的数据。
4. 量化评估了信息失真程度与空间差异之间的相关性。

Result: 发现幻觉的信息失真程度与其在向量空间中与正确输出簇的空间差异呈一致性相关。即使是简单的分类算法也能可靠地区分单个LLM产生的幻觉和准确的响应。

Conclusion: 本文提出的方法为自动检测LLM幻觉提供了一个轻量级且有效的框架，能够可靠地区分幻觉和准确响应，从而提高模型的可靠性。

Abstract: This work introduces a novel methodology for the automatic detection of
hallucinations generated during large language model (LLM) inference. The
proposed approach is based on a systematic taxonomy and controlled reproduction
of diverse hallucination types through prompt engineering. A dedicated
hallucination dataset is subsequently mapped into a vector space using an
embedding model and analyzed with unsupervised learning techniques in a
reduced-dimensional representation of hallucinations with veridical responses.
Quantitative evaluation of inter-centroid distances reveals a consistent
correlation between the severity of informational distortion in hallucinations
and their spatial divergence from the cluster of correct outputs. These
findings provide theoretical and empirical evidence that even simple
classification algorithms can reliably distinguish hallucinations from accurate
responses within a single LLM, thereby offering a lightweight yet effective
framework for improving model reliability.

</details>


### [104] [Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning](https://arxiv.org/abs/2510.05251)
*Chenghao Yang,Lin Gui,Chenxiao Yang,Victor Veitch,Lizhu Zhang,Zhuokai Zhao*

Main category: cs.CL

TL;DR: EAD通过在生成早期提高温度、后期降低温度，从而在探索和利用之间取得更好的平衡，提高了LLM的样本效率和推理能力。


<details>
  <summary>Details</summary>
Motivation: 传统的固定温度采样难以平衡样本质量和训练稳定性，限制了LLM在RLVR中的探索效率。

Method: 提出了一种名为EAD（Exploratory Annealed Decoding）的探索策略，该策略采用“先探索，后利用”的生成方式，通过在生成初期升高采样温度，并在后期逐渐降低温度来实现。

Result: EAD在样本效率、训练稳定性和模型性能方面显著优于固定温度采样，并且易于集成和使用。

Conclusion: 将探索策略与序列生成的自然动态相结合，是提高LLM推理能力的有效途径。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm
for enhancing the reasoning capabilities of large language models (LLMs), yet
its success hinges on effective exploration. An ideal exploration strategy must
navigate two fundamental challenges: it must preserve sample quality while also
ensuring training stability. While standard fixed-temperature sampling is
simple, it struggles to balance these competing demands, as high temperatures
degrade sample quality and low temperatures limit discovery. In this work, we
propose a simpler and more effective strategy, Exploratory Annealed Decoding
(EAD), grounded in the insight that exploration is most impactful on early
tokens which define a sequence's semantic direction. EAD implements an
intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by
annealing the sampling temperature from high to low during generation. This
dynamic schedule encourages meaningful, high-level diversity at the start, then
gradually lowers the temperature to preserve sample quality and keep the
sampling distribution close to the target policy, which is essential for stable
training. We demonstrate that EAD is a lightweight, plug-and-play method that
significantly improves sample efficiency, consistently outperforming
fixed-temperature sampling across various RLVR algorithms and model sizes. Our
work suggests that aligning exploration with the natural dynamics of sequential
generation offers a robust path to improving LLM reasoning.

</details>


### [105] [Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages](https://arxiv.org/abs/2510.05291)
*Tarek Naous,Anagha Savit,Carlos Rafael Catalan,Geyang Guo,Jaehyeok Lee,Kyungdon Lee,Lheane Marie Dizon,Mengyu Ye,Neel Kothari,Sahajpreet Singh,Sarah Masud,Tanish Patwa,Trung Thanh Tran,Zohaib Khan,Alan Ritter,JinYeong Bak,Keisuke Sakaguchi,Tanmoy Chakraborty,Yuki Arase,Wei Xu*

Main category: cs.CL

TL;DR: LLMs在处理亚洲语言时存在文化偏见，尤其是在文化适应、情感关联和实体抽取方面。本研究提出了Camellia基准来评估这种偏见，并发现不同LLM家族和训练数据会影响偏见程度。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏多语言基准，目前尚不清楚LLMs在阿拉伯语中偏向西方实体的偏见是否也存在于其他非西方语言中。本研究旨在评估LLMs在亚洲语言中的文化偏见。

Method: 引入Camellia基准，包含19,530个亚洲和西方文化相关实体以及2,173个自然语言掩码上下文。使用Camellia评估了四个多语言LLM家族在文化适应、情感关联和实体抽取方面的表现。

Result: LLMs在所有亚洲语言的文化适应方面都表现不佳，并且不同模型在处理亚洲语言时存在性能差异。LLM家族表现出各自独特的情感关联偏见。LLMs在亚洲语言的上下文理解方面存在困难，导致实体抽取任务在不同文化之间存在性能差距。

Conclusion: LLMs在处理亚洲语言时存在显著的文化偏见，尤其是在文化适应、情感关联和实体抽取任务中。模型的性能受其训练数据和开发区域的影响。需要进一步研究和开发以解决这些偏见问题。

Abstract: As Large Language Models (LLMs) gain stronger multilingual capabilities,
their ability to handle culturally diverse entities becomes crucial. Prior work
has shown that LLMs often favor Western-associated entities in Arabic, raising
concerns about cultural fairness. Due to the lack of multilingual benchmarks,
it remains unclear if such biases also manifest in different non-Western
languages. In this paper, we introduce Camellia, a benchmark for measuring
entity-centric cultural biases in nine Asian languages spanning six distinct
Asian cultures. Camellia includes 19,530 entities manually annotated for
association with the specific Asian or Western culture, as well as 2,173
naturally occurring masked contexts for entities derived from social media
posts. Using Camellia, we evaluate cultural biases in four recent multilingual
LLM families across various tasks such as cultural context adaptation,
sentiment association, and entity extractive QA. Our analyses show a struggle
by LLMs at cultural adaptation in all Asian languages, with performance
differing across models developed in regions with varying access to
culturally-relevant data. We further observe that different LLM families hold
their distinct biases, differing in how they associate cultures with particular
sentiments. Lastly, we find that LLMs struggle with context understanding in
Asian languages, creating performance gaps between cultures in entity
extraction.

</details>


### [106] [RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts](https://arxiv.org/abs/2510.05310)
*Yining She,Daniel W. Peterson,Marianne Menglin Liu,Vikas Upadhyay,Mohammad Hossein Chaghazardi,Eunsuk Kang,Dan Roth*

Main category: cs.CL

TL;DR: 外部基于LLM的 guardrail 模型容易受到分布外数据的影响，本研究以 RAG 为例，研究了 LLM Guardrails 的鲁棒性，发现即使是良性文档也会改变 guardrails 的判断，表明存在上下文鲁棒性差距，并激励了新的训练和评估协议。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）的广泛应用，LLM系统的安全性问题日益突出。外部基于LLM的 guardrail 模型作为一种流行的解决方案，用于筛选不安全的输入和输出，但它们本身是经过微调或提示工程的LLM，容易受到数据分布变化的影响。

Method: 本研究以检索增强生成（RAG）为案例，研究了基于LLM的 guardrails 在面对上下文中嵌入的额外信息时的鲁棒性。通过对3个Llama Guards和2个GPT-oss模型进行系统性评估，分析了增强上下文中每个组件（检索文档、用户查询、LLM生成响应）的影响，并测试了两种缓解方法。

Result: 评估结果表明，将良性文档插入 guardrail 的上下文会改变输入和输出 guardrails 的判断，分别影响约11%和8%的情况，使其变得不可靠。所测试的两种缓解方法仅带来微小的改进。

Conclusion: 这些结果暴露了当前 guardrails 存在的上下文鲁棒性差距，并促使研究者开发对检索和查询组合具有鲁棒性的训练和评估协议。

Abstract: With the increasing adoption of large language models (LLMs), ensuring the
safety of LLM systems has become a pressing concern. External LLM-based
guardrail models have emerged as a popular solution to screen unsafe inputs and
outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are
vulnerable to data distribution shifts. In this paper, taking Retrieval
Augmentation Generation (RAG) as a case study, we investigated how robust
LLM-based guardrails are against additional information embedded in the
context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss
models, we confirmed that inserting benign documents into the guardrail context
alters the judgments of input and output guardrails in around 11% and 8% of
cases, making them unreliable. We separately analyzed the effect of each
component in the augmented context: retrieved documents, user query, and
LLM-generated response. The two mitigation methods we tested only bring minor
improvements. These results expose a context-robustness gap in current
guardrails and motivate training and evaluation protocols that are robust to
retrieval and query composition.

</details>


### [107] [WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives](https://arxiv.org/abs/2510.05336)
*Yongan Yu,Xianda Du,Qingchen Hu,Jiahao Liang,Jingwei Ni,Dan Qiang,Kaiyu Huang,Grant McKenzie,Renee Sieber,Fengran Mo*

Main category: cs.CL

TL;DR: 历史天气档案因规模大、数字化质量差、语言古老而难以转化为结构化知识，本文提出了WeatherArchive-Bench基准来评估RAG系统处理这些档案的能力。


<details>
  <summary>Details</summary>
Motivation: 历史天气档案提供了关于社会如何经历和应对极端天气事件的宝贵见解，但由于其规模和质量问题，难以用于气候研究。

Method: 提出了WeatherArchive-Bench基准，包含两个任务：WeatherArchive-Retrieval（从一百万条档案新闻中检索相关段落）和WeatherArchive-Assessment（评估LLM分类社会脆弱性和韧性指标的能力）。

Result: 实验发现，密集检索器在处理历史术语时表现不佳，LLM经常误解脆弱性和韧性概念，揭示了现有RAG系统在处理复杂社会指标方面的局限性。

Conclusion: WeatherArchive-Bench揭示了当前RAG系统在处理历史天气档案方面的不足，并为设计更强大的气候相关RAG系统提供了见解。

Abstract: Historical archives on weather events are collections of enduring primary
source records that offer rich, untapped narratives of how societies have
experienced and responded to extreme weather events. These qualitative accounts
provide insights into societal vulnerability and resilience that are largely
absent from meteorological records, making them valuable for climate scientists
to understand societal responses. However, their vast scale, noisy digitized
quality, and archaic language make it difficult to transform them into
structured knowledge for climate research. To address this challenge, we
introduce WeatherArchive-Bench, the first benchmark for evaluating
retrieval-augmented generation (RAG) systems on historical weather archives.
WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which
measures a system's ability to locate historically relevant passages from over
one million archival news segments, and WeatherArchive-Assessment, which
evaluates whether Large Language Models (LLMs) can classify societal
vulnerability and resilience indicators from extreme weather narratives.
Extensive experiments across sparse, dense, and re-ranking retrievers, as well
as a diverse set of LLMs, reveal that dense retrievers often fail on historical
terminology, while LLMs frequently misinterpret vulnerability and resilience
concepts. These findings highlight key limitations in reasoning about complex
societal indicators and provide insights for designing more robust
climate-focused RAG systems from archival contexts. The constructed dataset and
evaluation framework are publicly available at
https://anonymous.4open.science/r/WeatherArchive-Bench/.

</details>


### [108] [Residualized Similarity for Faithfully Explainable Authorship Verification](https://arxiv.org/abs/2510.05362)
*Peter Zeng,Pegah Alipoormolabashi,Jihu Mun,Gourab Dey,Nikita Soni,Niranjan Balasubramanian,Owen Rambow,H. Schwartz*

Main category: cs.CL

TL;DR: 本研究提出了一种名为残差相似性（RS）的新方法，通过在可解释特征系统之上添加神经网络来提高作者身份验证的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的作者身份验证（AV）系统虽然准确率高，但往往缺乏可解释性，尤其是基于神经网络的方法，其预测过程难以理解。而大型语言模型（LLM）的预测虽然可以提供解释，但这些解释并不一定真实反映模型的推理过程。然而，在现实世界中，尤其是在需要做出具有实际后果的决策时，模型的预测必须是可解释的，并且能够追溯到原始文本。

Method: 提出残差相似性（RS）方法，该方法利用神经网络来预测可解释特征系统的相似性度量误差（即残差）。作者身份验证本质上是一个相似性任务，RS 方法旨在通过神经网络来弥补可解释特征系统在计算相似性时的不足，从而提高整体性能。

Result: 在四个数据集上的评估表明，RS 方法不仅能达到最先进的作者身份验证模型的性能水平，而且还能清晰地展示最终预测是如何产生的，以及其可解释性和忠实度。

Conclusion: RS 方法成功地在提高作者身份验证准确性的同时，保持了模型的可解释性，解决了现有方法在可解释性方面的不足，使得模型预测更值得信赖，尤其适用于需要做出影响深远决策的场景。

Abstract: Responsible use of Authorship Verification (AV) systems not only requires
high accuracy but also interpretable solutions. More importantly, for systems
to be used to make decisions with real-world consequences requires the model's
prediction to be explainable using interpretable features that can be traced to
the original texts. Neural methods achieve high accuracies, but their
representations lack direct interpretability. Furthermore, LLM predictions
cannot be explained faithfully -- if there is an explanation given for a
prediction, it doesn't represent the reasoning process behind the model's
prediction. In this paper, we introduce Residualized Similarity (RS), a novel
method that supplements systems using interpretable features with a neural
network to improve their performance while maintaining interpretability.
Authorship verification is fundamentally a similarity task, where the goal is
to measure how alike two documents are. The key idea is to use the neural
network to predict a similarity residual, i.e. the error in the similarity
predicted by the interpretable system. Our evaluation across four datasets
shows that not only can we match the performance of state-of-the-art authorship
verification models, but we can show how and to what degree the final
prediction is faithful and interpretable.

</details>


### [109] [The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures](https://arxiv.org/abs/2510.05364)
*Alexander M. Fichtl,Jeremias Bohn,Josefin Kelber,Edoardo Mosca,Georg Groh*

Main category: cs.CL

TL;DR: Transformers在序列处理任务中占主导地位，但其二次复杂度限制了上下文长度。本文综述了解决此瓶颈的最新进展，包括亚二次注意力变体、RNN、状态空间模型和混合架构，并对其计算/内存复杂度、基准结果和局限性进行了批判性分析，以评估纯注意力Transformer的统治地位是否会受到挑战。


<details>
  <summary>Details</summary>
Motivation: Transformer模型由于其二次复杂度，在处理长序列时存在显著的性能瓶颈。

Method: 本文综述了用于克服Transformer二次复杂度瓶颈的最新技术，包括亚二次注意力变体、循环神经网络、状态空间模型和混合架构。对这些方法进行了计算和内存复杂度、基准测试结果以及基本局限性的批判性评估。

Result: Transformer在序列处理任务中占主导地位，但其二次复杂度限制了上下文长度。本文综述了解决此瓶颈的最新进展，包括亚二次注意力变体、RNN、状态空间模型和混合架构，并对其计算/内存复杂度、基准结果和局限性进行了批判性分析，以评估纯注意力Transformer的统治地位是否会受到挑战。

Conclusion: 虽然Transformer在序列处理任务中占据主导地位，但其固有的二次计算复杂度限制了其在处理长序列时的可扩展性。本文对亚二次注意力变体、RNN、状态空间模型和混合架构等替代方法进行了批判性评估，旨在确定它们是否有可能挑战纯注意力Transformer的统治地位。

Abstract: Transformers have dominated sequence processing tasks for the past seven
years -- most notably language modeling. However, the inherent quadratic
complexity of their attention mechanism remains a significant bottleneck as
context length increases. This paper surveys recent efforts to overcome this
bottleneck, including advances in (sub-quadratic) attention variants, recurrent
neural networks, state space models, and hybrid architectures. We critically
analyze these approaches in terms of compute and memory complexity, benchmark
results, and fundamental limitations to assess whether the dominance of
pure-attention transformers may soon be challenged.

</details>


### [110] [Context Length Alone Hurts LLM Performance Despite Perfect Retrieval](https://arxiv.org/abs/2510.05381)
*Yufeng Du,Minyang Tian,Srikanth Ronanki,Subendhu Rongali,Sravan Bodapati,Aram Galstyan,Azton Wells,Roy Schwartz,Eliu A Huerta,Hao Peng*

Main category: cs.CL

TL;DR: Despite retrieval improvements, LLMs still struggle with long contexts due to input length alone, not just retrieval errors. A simple mitigation strategy improves performance.


<details>
  <summary>Details</summary>
Motivation: Investigate why LLMs fail to scale on long-context tasks even when retrieval is perfect, challenging the assumption that perfect retrieval solves the problem.

Method: Systematically experiment across 5 LLMs on math, QA, and coding tasks, controlling for retrieval accuracy and distractions (whitespace, masked tokens, evidence placement). Propose and test a mitigation strategy of prompting evidence recitation.

Result: LLM performance degrades significantly (13.9%-85%) with increasing input length, even with perfect retrieval and minimal distractions. The proposed strategy shows consistent improvement (up to 4%) for GPT-4o on RULER.

Conclusion: Input length itself is a significant impediment to LLM performance, independent of retrieval quality. A simple strategy of reciting retrieved evidence before task solving can effectively mitigate this length-induced performance degradation.

Abstract: Large language models (LLMs) often fail to scale their performance on
long-context tasks performance in line with the context lengths they support.
This gap is commonly attributed to retrieval failures -- the models' inability
to identify relevant information in the long inputs. Accordingly, recent
efforts often focus on evaluating and improving LLMs' retrieval performance: if
retrieval is perfect, a model should, in principle, perform just as well on a
long input as it does on a short one -- or should it? This paper presents
findings that the answer to this question may be negative. Our systematic
experiments across 5 open- and closed-source LLMs on math, question answering,
and coding tasks reveal that, even when models can perfectly retrieve all
relevant information, their performance still degrades substantially
(13.9%--85%) as input length increases but remains well within the models'
claimed lengths. This failure occurs even when the irrelevant tokens are
replaced with minimally distracting whitespace, and, more surprisingly, when
they are all masked and the models are forced to attend only to the relevant
tokens. A similar performance drop is observed when all relevant evidence is
placed immediately before the question. Our findings reveal a
previously-unrealized limitation: the sheer length of the input alone can hurt
LLM performance, independent of retrieval quality and without any distraction.
They motivate our simple, model-agnostic mitigation strategy that transforms a
long-context task into a short-context one by prompting the model to recite the
retrieved evidence before attempting to solve the problem. On RULER, we observe
a consistent improvement of GPT-4o up to 4% on an already strong baseline.

</details>


### [111] [Cross-Lingual Mental Health Ontologies for Indian Languages: Bridging Patient Expression and Clinical Understanding through Explainable AI and Human-in-the-Loop Validation](https://arxiv.org/abs/2510.05387)
*Ananth Kandala,Ratna Kandala,Akshata Kishore Moharir,Niva Manchanda,Sunaina Singh*

Main category: cs.CL

TL;DR: 印度心理健康沟通面临语言、文化和临床NLP的挑战。我们提出了跨语言患者痛苦表达图（CL-PDE）框架，通过基于图的方法构建跨语言心理健康本体，以捕捉、对齐和链接具有文化特色的痛苦表达，弥合了现有研究的不足。


<details>
  <summary>Details</summary>
Motivation: 印度心理健康沟通在语言、文化和临床NLP方面存在碎片化、多样化和代表性不足的问题。现有的健康本体和心理健康资源主要基于西方文化和英语，未能充分体现印度患者的痛苦表达。

Method: 我们提出了跨语言患者痛苦表达图（CL-PDE）框架，采用基于图的方法来构建跨语言心理健康本体。该框架能够捕捉具有文化特色的痛苦表达，并将其跨语言对齐，同时与临床术语相关联。

Result: CL-PDE框架能够弥合印度心理健康沟通中的关键差距，通过将AI系统与具有文化有效性的表征相结合，为多语言环境下的心理健康护理提供更具包容性和以患者为中心的NLP工具。

Conclusion: CL-PDE框架为构建跨语言心理健康本体提供了一种新颖的方法，解决了印度多语言背景下心理健康沟通的挑战，并促进了更具包容性和以患者为中心的NLP工具的发展。

Abstract: Mental health communication in India is linguistically fragmented, culturally
diverse, and often underrepresented in clinical NLP. Current health ontologies
and mental health resources are dominated by diagnostic frameworks centered on
English or Western culture, leaving a gap in representing patient distress
expressions in Indian languages. We propose cross-linguistic graphs of patient
stress expressions (CL-PDE), a framework for building cross-lingual mental
health ontologies through graph-based methods that capture culturally embedded
expressions of distress, align them across languages, and link them with
clinical terminology. Our approach addresses critical gaps in healthcare
communication by grounding AI systems in culturally valid representations,
allowing more inclusive and patient-centric NLP tools for mental health care in
multilingual contexts.

</details>


### [112] [Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care](https://arxiv.org/abs/2510.05410)
*Junyi Fan,Li Sun,Negin Ashrafi,Kamiar Alaei,Maryam Pishgar*

Main category: cs.CL

TL;DR: 本研究使用直接偏好优化（DPO）技术，通过对8,838份心力衰竭护理记录和21,210个偏好对进行训练，显著提高了Mistral-7B语言模型在ICU心力衰竭护理文档生成方面的质量。


<details>
  <summary>Details</summary>
Motivation: ICU护理文档存在术语不一致、风格非正式和缺乏标准化的问题，尤其在心力衰竭护理中更为关键，因此需要改进文档质量。

Method: 本研究应用直接偏好优化（DPO）方法，使用MIMIC-III数据库中的8,838份心力衰竭护理记录和21,210个偏好对（来源于专家验证的GPT输出、模型生成和原始记录）来调整Mistral-7B语言模型。

Result: DPO显著提高了文档质量，BLEU分数从0.173提升至0.318（增长84%），BERTScore从0.828提升至0.891（增长7.6%）。专家评估显示，准确性、完整性、逻辑一致性、可读性和结构清晰度均有显著提升（+14.4, +14.5, +14.1, +11.1, +6.0分）。

Conclusion: DPO能够使轻量级临床语言模型符合专家标准，支持在电子健康记录系统中进行隐私保护的AI辅助文档记录，以减轻行政负担并提高ICU患者的安全性。

Abstract: Nursing documentation in intensive care units (ICUs) provides essential
clinical intelligence but often suffers from inconsistent terminology, informal
styles, and lack of standardization, challenges that are particularly critical
in heart failure care. This study applies Direct Preference Optimization (DPO)
to adapt Mistral-7B, a locally deployable language model, using 8,838 heart
failure nursing notes from the MIMIC-III database and 21,210 preference pairs
derived from expert-verified GPT outputs, model generations, and original
notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert
qualitative assessments demonstrates that DPO markedly enhances documentation
quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore
improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy
(+14.4 points), completeness (+14.5 points), logical consistency (+14.1
points), readability (+11.1 points), and structural clarity (+6.0 points).
These results indicate that DPO can align lightweight clinical language models
with expert standards, supporting privacy-preserving, AI-assisted documentation
within electronic health record systems to reduce administrative burden and
improve ICU patient safety.

</details>


### [113] [A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis](https://arxiv.org/abs/2510.05414)
*Ziheng Geng,Jiachen Liu,Ran Cao,Lu Cheng,Haifeng Wang,Minghui Cheng*

Main category: cs.CL

TL;DR: 该研究开发了一个基于LLM的多智能体系统，用于自动化2D框架的有限元建模，实现了80%以上的高准确率，优于Gemini-2.5 Pro和ChatGPT-4o。


<details>
  <summary>Details</summary>
Motivation: LLM在结构工程领域的潜力，特别是在有限元建模任务中，仍有待充分发掘，这些任务需要几何建模、复杂推理和领域知识。为了弥合这一差距，本研究开发了一个LLM驱动的多智能体系统来自动化2D框架的有限元建模。

Method: 该系统将结构分析分解为由专门的智能体管理的子任务，每个智能体都由轻量级的Llama-3.3 70B Instruct模型提供支持。工作流程包括问题分析智能体、几何智能体、翻译智能体、模型验证智能体和负载智能体，以逐步提取参数、构建模型、生成代码并应用负载条件。

Result: 在20个基准问题上的实验评估表明，该系统在10次重复试验中的大多数情况下实现了超过80%的准确率，优于Gemini-2.5 Pro和ChatGPT-4o模型。

Conclusion: 所提出的LLM多智能体系统能够有效自动执行2D框架的有限元建模任务，并在准确性和效率方面展现出有前景的结果，为LLM在结构工程领域的应用开辟了新的途径。

Abstract: Large language models (LLMs) have recently been used to empower autonomous
agents in engineering, significantly improving automation and efficiency in
labor-intensive workflows. However, their potential remains underexplored in
structural engineering, particularly for finite element modeling tasks
requiring geometric modeling, complex reasoning, and domain knowledge. To
bridge this gap, this paper develops a LLM-based multi-agent system to automate
finite element modeling of 2D frames. The system decomposes structural analysis
into subtasks, each managed by a specialized agent powered by the lightweight
Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis
Agent, which extracts geometry, boundary, and material parameters from the user
input. Next, a Geometry Agent incrementally derives node coordinates and
element connectivity by applying expert-defined rules. These structured outputs
are converted into executable OpenSeesPy code by a Translation Agent and
refined by a Model Validation Agent through consistency checks. Then, a Load
Agent applies load conditions into the assembled structural model. Experimental
evaluations on 20 benchmark problems demonstrate that the system achieves
accuracy over 80% in most cases across 10 repeated trials, outperforming
Gemini-2.5 Pro and ChatGPT-4o models.

</details>


### [114] [Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification](https://arxiv.org/abs/2510.05431)
*Yoo Yongmin,Zhang Xu,Cao Longbing*

Main category: cs.CL

TL;DR: LLM生成的解释可能包含错误，因此直接使用它们进行监督会引入噪声。为解决此问题，提出了一种名为“自过滤蒸馏”的框架，用于专利分类。该框架不直接将LLM解释视为监督信号，而是根据三个无监督的信任指标（自洽性、类别蕴含对齐、LLM一致性评分）来评估其可信度，并根据信任分数来调整训练样本的权重。实验表明，该方法在准确性、稳定性和可解释性方面优于传统的监督学习和蒸馏方法。


<details>
  <summary>Details</summary>
Motivation: LLM生成的自然语言解释（rationales）虽然增强了可解释性，但常常包含逻辑错误、标签不匹配和领域特定失准，直接使用这些解释作为监督信号会引入噪声并影响训练稳定性。

Method: 提出了一种名为“自过滤蒸馏”的框架，该框架将LLM生成的解释视为信任信号而非真实监督。该框架使用三个无监督的信任指标来指导选择性蒸馏：1. 自洽性（衡量LLM解释在多次生成中的稳定性）；2. 类别蕴含对齐（评估与专利类别定义的语义一致性）；3. LLM一致性评分（验证解释与标签的合理性）。这些指标被整合到一个统一的信任分数中，主要用于加权训练样本，并可选择性地过滤掉信任度极低的情况，从而实现“能推理的监督”。

Result: 在USPTO-2M数据集（一个广泛使用的专利分类基准）上进行实验，结果表明，该方法在准确性、稳定性和可解释性方面优于基于标签的学习和传统的蒸馏方法。

Conclusion: 自过滤蒸馏框架能够有效地利用“能推理的信任指标”来处理LLM解释中的噪声，为在专利分析中利用这类信息提供了一种可靠的范式。

Abstract: Large language models (LLMs) increasingly generate natural language
rationales to enhance interpretability, but these often contain logical errors,
label mismatches, and domain-specific misalignments. Directly using such
rationales as supervision risks propagating noise and undermining training
stability. To address this challenge, we introduce Self-Filtered Distillation,
a framework specifically tailored for patent classification, which treats
LLM-generated rationales as trust signals rather than ground-truth supervision.
The framework employs selective distillation guided by three unsupervised trust
metrics: (1) Self-Consistency, which measures the stability of LLM-generated
rationales across multiple generations; (2) Class Entailment Alignment, which
assesses semantic coherence with patent-specific class definitions; and (3) LLM
Agreement Scoring, which validates rationale-label plausibility. These metrics
are integrated into a unified trust score that primarily weights training
samples while optionally filtering out extremely low-trust cases, enabling
reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used
benchmark for patent classification, show that our method outperforms
label-based learning and conventional distillation in accuracy, stability, and
interpretability, establishing a reliable paradigm for leveraging
reasoning-aware trust indicators in patent analytics.

</details>


### [115] [SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?](https://arxiv.org/abs/2510.05444)
*Yao Dou,Michel Galley,Baolin Peng,Chris Kedzie,Weixin Cai,Alan Ritter,Chris Quirk,Wei Xu,Jianfeng Gao*

Main category: cs.CL

TL;DR: LLMs can simulate users for conversational AI evaluation, but their reliability is unproven. This paper introduces SimulatorArena, a benchmark with 909 human-LLM conversations, to assess simulator performance. Results show profile-conditioned simulators achieve 0.7 Spearman's rho, offering a scalable alternative to human evaluation. The benchmark also evaluates 18 assistants, including GPT-5 and Gemini 2.5 Pro.


<details>
  <summary>Details</summary>
Motivation: Human evaluation of LLMs in multi-turn conversations is costly and time-consuming. This work aims to evaluate the reliability of using LLMs to simulate users for automatic assistant evaluation, as there is no existing benchmark or systematic study for this purpose.

Method: The paper introduces SimulatorArena, a benchmark comprising 909 annotated human-LLM conversations across two interactive tasks: math tutoring and document creation. This benchmark assesses simulators based on two criteria: 1) the similarity of their messages to human behavior, and 2) the alignment of their assistant ratings with human judgments. Experiments were conducted using various simulator methods, with a focus on those conditioned on user profiles (e.g., background, message style).

Result: Experiments demonstrated that simulators conditioned on user profiles exhibit close alignment with human judgments, achieving a Spearman's rho of 0.7 on both tasks. This indicates they are a practical and scalable alternative to human evaluation. The study also used these best-performing simulators to benchmark 18 different assistants, including state-of-the-art LLMs like GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro.

Conclusion: LLM-based user simulators, particularly those conditioned on user profiles, offer a reliable and scalable alternative to expensive and time-consuming human evaluations for assessing conversational AI assistants. The introduced SimulatorArena benchmark validates this approach and provides a method for comparing various LLM assistants.

Abstract: Large language models (LLMs) are increasingly used in interactive
applications, and human evaluation remains the gold standard for assessing
their performance in multi-turn conversations. Since human studies are costly,
time-consuming, and hard to reproduce, recent work explores using LLMs to
simulate users for automatic assistant evaluation. However, there is no
benchmark or systematic study to evaluate whether these simulated users are
reliable stand-ins for real users. To address this, we introduce
SimulatorArena, a benchmark of 909 annotated human-LLM conversations on two
interactive tasks -- math tutoring and document creation. SimulatorArena
evaluates simulators based on how closely their messages match human behavior
and how well their assistant ratings align with human judgments. Experiments on
various simulator methods show that simulators conditioned on user profiles,
capturing traits like background and message styles, align closely with human
judgments. They reach Spearman's $\rho$ of 0.7 on both tasks, providing a
practical, scalable alternative to human evaluation. Using the best simulator
for each task, we benchmark 18 assistants, including the latest LLMs such as
GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro.

</details>


### [116] [AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering](https://arxiv.org/abs/2510.05445)
*Zheyuan Zhang,Kaiwen Shi,Zhengqing Yuan,Zehong Wang,Tianyi Ma,Keerthiram Murugesan,Vincent Galassi,Chuxu Zhang,Yanfang Ye*

Main category: cs.CL

TL;DR: LLM代理路由框架


<details>
  <summary>Details</summary>
Motivation: 现有代理路由方法忽视了QA任务中的细粒度上下文和关系结构，导致在选择最佳配置时存在不确定性。

Method: 提出tAgentRouter框架，将多智能体QA转化为知识图引导的路由问题，并使用经验性能信号进行监督。具体地，将QA实例转换为知识图，然后训练异构图神经网络（GNN）来传播信息并产生面向任务的代理路由分布。通过软监督和代理输出来加权聚合，tAgentRouter学习了捕获不同代理互补优势的原则性协作方案。

Result: 在QA任务上，tAgentRouter的性能始终优于单代理和集成基线，并且在不同基准和LLM骨干上表现出良好的泛化能力。

Conclusion: 基于图监督的多智能体路由在问答方面是有效且稳健的。

Abstract: Large language models (LLMs) and agent-based frameworks have advanced
rapidly, enabling diverse applications. Yet, with the proliferation of models
and agentic strategies, practitioners face substantial uncertainty in selecting
the best configuration for a downstream task. Prior studies show that different
agents and backbones exhibit complementary strengths, and that larger models
are not always superior, underscoring the need for adaptive routing mechanisms.
Existing approaches to agent routing, however, often emphasize cost efficiency
while overlooking the fine-grained contextual and relational structure inherent
in QA tasks. In this paper, we propose tAgentRouter, a framework that
formulates multi-agent QA as a knowledge-graph-guided routing problem
supervised by empirical performance signals. Specifically, we convert QA
instance into a knowledge graph that jointly encodes queries, contextual
entities, and agents, and then train a heterogeneous graph neural network (GNN)
to propagate information across node types and produce task-aware routing
distributions over agents. By leveraging soft supervision and weighted
aggregation of agent outputs, AgentRouter learns principled collaboration
schemes that capture the complementary strengths of diverse agents. Extensive
experiments demonstrate that our framework consistently outperforms
single-agent and ensemble baselines, while generalizing across benchmarks and
LLM backbones. These results highlight the effectiveness and robustness of
graph-supervised multi-agent routing for question answering.

</details>


### [117] [SocialNLI: A Dialogue-Centric Social Inference Dataset](https://arxiv.org/abs/2510.05458)
*Akhil Deo,Kate Sanders,Benjamin Van Durme*

Main category: cs.CL

TL;DR: 本文提出了首个社会对话推理数据集SocialNLI（SoNLI），用于评估大型语言模型（LLMs）在理解对话中的社会学因素（如讽刺和反语）方面的能力。


<details>
  <summary>Details</summary>
Motivation: 目前的LLMs在理解和推理复杂的社会现象（如讽刺和反语）方面存在不足，而理解这些现象是构建优秀AI助手的关键。

Method: 构建了一个名为SocialNLI（SoNLI）的数据集，其中包含精心挑选的对话语录、推理、相应可能性得分和人工编写的解释。通过多步反事实推理来评估LLMs的“心智理论”能力。

Result: LLMs在处理SoNLI数据集时表现出不足，表明其在理解复杂的社会细微差别方面存在局限性。

Conclusion: SoNLI数据集为评估和改进LLMs的社会推理能力提供了一个新的基准，并突显了在AI助手发展中解决社会智能差距的必要性。

Abstract: Making theory-of-mind inferences from human dialogue is a strong indicator of
a model's underlying social abilities, which are fundamental for adept AI
assistants. However, large language and reasoning models struggle to understand
sophisticated social phenomena in transcript data, such as sarcasm and irony.
To assess the weaknesses of current models and to identify their solutions, we
introduce SocialNLI (SoNLI) -- the first social dialogue inference dataset.
SoNLI consists of a collection of dialogue transcripts hand-picked to center
complex social nuances like irony and sarcasm, paired with inferences,
corresponding likelihood scores, and human-written explanations. We explore
social inference analysis as a facet of theory-of-mind, and evaluate LLM and
reasoning model theory-of-mind ability through multi-step counterfactual
reasoning.

</details>


### [118] [TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation](https://arxiv.org/abs/2510.05485)
*Adam Filipek*

Main category: cs.CL

TL;DR: TensorBLEU是一个为GPU加速的PyTorch环境设计的、用于计算BLEU指标的新型实现，它通过内存高效的n-gram计数机制，显著提高了计算速度，解决了现有评估工具的计算瓶颈问题，尤其适用于大规模模型和强化学习等场景。


<details>
  <summary>Details</summary>
Motivation: 现有的NLP模型评估工具计算成本高，成为研究的瓶颈，尤其是在训练过程中需要进行逐句评估的场景（如强化学习中的奖励信号）。

Method: TensorBLEU是一种全新的BLEU指标实现，完全向量化，适用于GPU加速的PyTorch环境。它采用内存高效的计数机制，利用`torch.unique`创建紧凑的、批次特定的n-gram词典，避免了传统基于哈希向量化的内存开销，使其适用于大词汇量的模型。

Result: 与CPU上基于Token-ID的BLEU计算标准库NLTK相比，TensorBLEU在消费级GPU（NVIDIA T4）上实现了超过13倍的速度提升，在数据中心级硬件（NVIDIA A100）上甚至超过40倍的速度提升。

Conclusion: TensorBLEU通过提供高效的逐句BLEU评估能力，将一个重要的计算瓶颈转化为训练循环中一个可以忽略不计的部分，并通过开源其实现，为加速强化学习等领域的研究提供了强大工具。

Abstract: Modern natural language processing models have achieved unprecedented scale,
yet the tools for their evaluation often remain a computational bottleneck,
limiting the pace of research. This is particularly acute for in-training
evaluation metrics, such as per-sentence reward signals in Reinforcement
Learning, which must operate efficiently on batches of token IDs directly on
the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the
BLEU metric designed from the ground up for this specific use case. Our
approach is fully vectorized for GPU-accelerated, per-sentence computation
within PyTorch and introduces a memory-efficient counting mechanism. By
creating a compact, batch-specific dictionary of n-grams using
\texttt{torch.unique}, our method avoids the prohibitive memory costs of
traditional hashing-based vectorization, making it practical for
large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard
library for token-ID-based BLEU calculation on the CPU. Experiments show that
TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and
exceeding 40x on data-center-class hardware (NVIDIA A100). This performance
transforms a significant bottleneck into a negligible part of the training
loop. By clearly defining its role as a "Token-ID BLEU" for development
purposes and open-sourcing our implementation, we provide a powerful tool for
accelerating research in areas like RL-based model fine-tuning.

</details>


### [119] [Language Model as Planner and Formalizer under Constraints](https://arxiv.org/abs/2510.05486)
*Cassie Huang,Stuti Mohan,Ziyi Yang,Stefanie Tellex,Li Zhang*

Main category: cs.CL

TL;DR: LLMs在规划任务中表现优于标准基准，但引入自然语言约束会显著降低其性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 规划研究依赖于过于简化的基准，可能高估了 LLM 的规划能力并引发安全问题。本研究旨在通过引入细粒度的自然语言约束来弥合这一差距。

Method: 通过手动注释，为标准规划基准添加了跨越四种形式化类别的、细粒度的、丰富的自然语言约束。

Result: 引入约束后，LLM 的规划性能下降了一半，并且对问题复杂性和词汇变化的鲁棒性受到显著挑战。

Conclusion: 在标准基准上表现良好的 LLM 在面对更复杂的、包含自然语言约束的真实世界规划任务时，其性能和鲁棒性会大打折扣。

Abstract: LLMs have been widely used in planning, either as planners to generate action
sequences end-to-end, or as formalizers to represent the planning domain and
problem in a formal language that can derive plans deterministically. However,
both lines of work rely on standard benchmarks that only include generic and
simplistic environmental specifications, leading to potential overestimation of
the planning ability of LLMs and safety concerns in downstream tasks. We bridge
this gap by augmenting widely used planning benchmarks with manually annotated,
fine-grained, and rich natural language constraints spanning four formally
defined categories. Over 4 state-of-the-art reasoning LLMs, 3 formal languages,
5 methods, and 4 datasets, we show that the introduction of constraints not
only consistently halves performance, but also significantly challenges
robustness to problem complexity and lexical shift.

</details>


### [120] [LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation](https://arxiv.org/abs/2510.05490)
*Zhoutong Fu,Yihan Cao,Yi-Lin Chen,Aman Lunia,Liming Dong,Neha Saraf,Ruijie Jiang,Yun Dai,Qingquan Song,Tan Wang,Guoyao Li,Derek Koh,Haichao Wei,Zhipeng Wang,Aman Gupta,Chengming Jiang,Jianqiang Shen,Liangjie Hong,Wenjing Zhang*

Main category: cs.CL

TL;DR: LLM 知识蒸馏框架 LANTERN 针对招聘中的人岗匹配和解释任务进行了优化，解决了部署中的挑战，提高了用户参与度。


<details>
  <summary>Details</summary>
Motivation: 在招聘平台等领域特定应用中部署大型语言模型（LLM）面临挑战，如人岗匹配和解释任务需要高质量、可操作的反馈，而 LLM 的尺寸导致高推理延迟和可扩展性限制。

Method: 提出了一种名为 LANTERN 的 LLM 知识蒸馏框架，该框架针对人岗匹配任务进行了定制。LANTERN 涉及多目标建模、分类编码器和解释解码器。它通过多层次知识蒸馏（整合数据和 Logit 见解）来更好地从强大的黑盒教师模型中提炼知识到多个下游模型。此外，还分享了关于训练后技术和提示工程的见解。

Result: LANTERN 在人岗匹配和解释任务上的特定指标得到了显著改善。在线评估证实了其有效性，在求职者参与度方面取得了可衡量的进展，申请率提高了 0.24%，合格申请率提高了 0.28%。

Conclusion: LANTERN 框架通过知识蒸馏、多目标建模以及结合训练后技术和提示工程，有效地解决了在特定领域（如招聘）应用 LLM 的挑战，并取得了显著的业务成果。

Abstract: Large language models (LLMs) have achieved strong performance across a wide
range of natural language processing tasks. However, deploying LLMs at scale
for domain specific applications, such as job-person fit and explanation in job
seeking platforms, introduces distinct challenges. At LinkedIn, the job person
fit task requires analyzing a candidate's public profile against job
requirements to produce both a fit assessment and a detailed explanation.
Directly applying open source or finetuned LLMs to this task often fails to
yield high quality, actionable feedback due to the complexity of the domain and
the need for structured outputs. Moreover, the large size of these models leads
to high inference latency and limits scalability, making them unsuitable for
online use. To address these challenges, we introduce LANTERN, a novel LLM
knowledge distillation framework tailored specifically for job person fit
tasks. LANTERN involves modeling over multiple objectives, an encoder model for
classification purpose, and a decoder model for explanation purpose. To better
distill the knowledge from a strong black box teacher model to multiple
downstream models, LANTERN incorporates multi level knowledge distillation that
integrates both data and logit level insights. In addition to introducing the
knowledge distillation framework, we share our insights on post training
techniques and prompt engineering, both of which are crucial for successfully
adapting LLMs to domain specific downstream tasks. Extensive experimental
results demonstrate that LANTERN significantly improves task specific metrics
for both job person fit and explanation. Online evaluations further confirm its
effectiveness, showing measurable gains in job seeker engagement, including a
0.24\% increase in apply rate and a 0.28\% increase in qualified applications.

</details>


### [121] [Prototype-Based Dynamic Steering for Large Language Models](https://arxiv.org/abs/2510.05498)
*Ceyhun Efe Kayan,Li Zhang*

Main category: cs.CL

TL;DR: PDS是一种无需修改或添加指令即可增强LLM推理能力的方法，通过聚类激活差异得到“推理原型”，并在推理时使用这些原型生成特定于实例的引导向量，从而在多个任务上提高准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM推理增强方法依赖于明确的指令或静态的引导方法，缺乏自适应和无需指令的方法。

Method: 1. 聚类Chain-of-Thought (CoT)提示和中性提示之间的激活差异，以创建“推理原型”。 2. 在推理时，将输入的隐藏状态投影到这些原型上，生成特定于实例的引导向量。 3. 应用该引导向量来增强LLM的推理能力。

Result: 在GSM8K、AQuA-RAT和BIG-Bench等任务上，PDS在没有进行微调或提示工程的情况下，持续提高了准确性。即使在抑制CoT以提高成本效益的情况下，PDS的增益也依然存在。

Conclusion: 动态的、基于原型的引导是一种轻量级的方法，可以替代训练时的方法来增强LLM的推理能力。

Abstract: Despite impressive breadth, LLMs still rely on explicit reasoning
instructions or static, one-fits-all steering methods, leaving a gap for
adaptive, instruction-free reasoning amplification. We present Prototype-Based
Dynamic Steering (PDS), a test-time method that amplifies large language model
(LLM) reasoning without adding or altering instructions. We introduce
"reasoning prototypes" by clustering activation differences between
Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden
state is projected onto these prototypes to form an instance-specific steering
vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently
improves accuracy without fine-tuning or prompt engineering. Notably, the gains
persist even when CoT is explicitly suppressed to improve cost-efficiency,
indicating that the intervention strengthens latent reasoning processes rather
than inducing a superficial behavioral shift. These results position dynamic,
prototype-guided steering as a lightweight alternative to training-time
approaches for enhancing LLM reasoning.

</details>


### [122] [CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension](https://arxiv.org/abs/2510.05520)
*Rui Li,Zeyu Zhang,Xiaohe Bo,Zihang Tian,Xu Chen,Quanyu Dai,Zhenhua Dong,Ruiming Tang*

Main category: cs.CL

TL;DR: LLMs在处理长文本时面临信息过载问题，本文提出了一种基于建构主义理论的CAM记忆模块，以提高LLM的自主阅读能力。CAM通过聚类算法构建结构化记忆，并能在推理时自适应地检索相关信息，在多项长文本阅读任务中表现出优越的性能和效率。


<details>
  <summary>Details</summary>
Motivation: 当前的LLM在理解长篇幅文档时面临信息量过载的挑战，需要一个内聚的记忆模块来提升其作为自主阅读代理的能力，但目前缺乏系统性的设计原则。

Method: 提出了一种名为CAM（Constructivist Agentic Memory）的记忆模块原型，借鉴了皮亚杰的建构主义理论，具备结构化、柔性和动态性。CAM采用增量式重叠聚类算法来构建结构化记忆，支持分层摘要和在线批次集成。在推理过程中，CAM能自适应地探索记忆结构，激活与查询相关的信息，模拟人类联想过程。

Result: 与现有方法相比，CAM在包括问答、查询摘要和声明验证在内的多种长文本阅读理解任务中，在性能和效率上均表现出双重优势。

Conclusion: CAM通过模拟人类建构主义认知过程，为LLM的阅读理解提供了更强大、更高效的记忆系统，克服了长文本信息过载的挑战。

Abstract: Current Large Language Models (LLMs) are confronted with overwhelming
information volume when comprehending long-form documents. This challenge
raises the imperative of a cohesive memory module, which can elevate vanilla
LLMs into autonomous reading agents. Despite the emergence of some heuristic
approaches, a systematic design principle remains absent. To fill this void, we
draw inspiration from Jean Piaget's Constructivist Theory, illuminating three
traits of the agentic memory -- structured schemata, flexible assimilation, and
dynamic accommodation. This blueprint forges a clear path toward a more robust
and efficient memory system for LLM-based reading comprehension. To this end,
we develop CAM, a prototype implementation of Constructivist Agentic Memory
that simultaneously embodies the structurality, flexibility, and dynamicity. At
its core, CAM is endowed with an incremental overlapping clustering algorithm
for structured memory development, supporting both coherent hierarchical
summarization and online batch integration. During inference, CAM adaptively
explores the memory structure to activate query-relevant information for
contextual response, akin to the human associative process. Compared to
existing approaches, our design demonstrates dual advantages in both
performance and efficiency across diverse long-text reading comprehension
tasks, including question answering, query-based summarization, and claim
verification.

</details>


### [123] [KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance](https://arxiv.org/abs/2510.05524)
*Kuangshi Ai,Jonathan A. Karr Jr,Meng Jiang,Nitesh V. Chawla,Chaoli Wang*

Main category: cs.CL

TL;DR: KEO框架使用知识图谱增强LLM在安全关键领域的问答能力，提高了全局理解和系统洞察力，但对于局部检索任务，传统的基于文本块的RAG方法仍然有效。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域，利用大型语言模型（LLMs）进行知识抽取和推理具有重要意义，但需要领域特定的框架和方法。

Method: 构建了一个名为KEO（Knowledge Extraction on OMIn）的框架，该框架利用OMIn数据集，结合了结构化知识图谱（KG）和检索增强生成（RAG）技术，以增强LLMs在安全关键上下文中的问答和推理能力。KEO将KG集成到RAG流水线中，以实现比传统基于文本块的RAG更强的、跨越整个数据集的连贯推理能力。

Result: 实验表明，KEO显著提高了LLMs在全局理解和系统层面洞察力方面的能力，能够揭示模式和提供系统级见解。相比之下，传统的基于文本块的RAG方法在需要局部检索的细粒度程序性任务上仍然表现出有效性。

Conclusion: KEO框架在安全、领域特定的问答任务中展示了知识图谱增强LLM的巨大潜力，尤其在需要高风险推理的场景下，其能够提供比传统方法更强的全局理解和洞察力。

Abstract: We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge
extraction and reasoning framework with large language models (LLMs) in
safety-critical contexts. Using the Operations and Maintenance Intelligence
(OMIn) dataset, we construct a QA benchmark spanning global sensemaking and
actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and
integrates it into a retrieval-augmented generation (RAG) pipeline, enabling
more coherent, dataset-wide reasoning than traditional text-chunk RAG. We
evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ
stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO
markedly improves global sensemaking by revealing patterns and system-level
insights, while text-chunk RAG remains effective for fine-grained procedural
tasks requiring localized retrieval. These findings underscore the promise of
KG-augmented LLMs for secure, domain-specific QA and their potential in
high-stakes reasoning.

</details>


### [124] [H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference](https://arxiv.org/abs/2510.05529)
*Harshil Vejendla*

Main category: cs.CL

TL;DR: H1B-KV通过使用1位量化键向量和4位量化值向量，将70亿参数大语言模型的缓存内存从3.5GB减少到60MB（减少70倍），同时在各种基准测试中保持了与全精度模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前的LLM长上下文推理由于KV缓存的内存消耗而成为一个内存瓶颈。现有的方法，如量化、令牌驱逐或仅压缩键，都未能提供完整的解决方案。

Method: H1B-KV是一种混合压缩方案，它用1位二进制草图表示每个键向量，允许进行硬件友好的比特级注意力，并使用4位量化进一步压缩值向量。

Result: H1B-KV使一个70亿参数的LLM能够以不到60MB的缓存内存处理8k上下文，这是一个70倍的减少。经过轻量级微调后，H1B-KV在困惑度基准和数学推理、多任务理解和代码生成等下游任务上都达到了全精度性能。

Conclusion: H1B-KV在质量/字节方面明显优于现有的量化、令牌驱逐和仅键草图方法，使其成为在内存受限环境中部署LLM的稳健解决方案。

Abstract: Autoregressive decoding in large language models (LLMs) requires caching a
growing list of past key-value (KV) pairs, making long-context inference a
memory-bound problem. While recent methods have explored quantizing the cache,
evicting tokens, or using binary sketches for keys (e.g., Loki), these
approaches often provide an incomplete solution by leaving one component (like
values) uncompressed or by discarding context information. This paper
introduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression
scheme that radically reduces memory usage without sacrificing context. H1B-KV
represents each key vector using a 1-bit binary sketch, enabling
hardware-friendly bitwise attention, and further compresses value vectors using
4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter
LLM to handle an 8k-token context with under 60 MB of cache memory - a 70x
reduction. We demonstrate that after a lightweight finetuning, H1B-KV matches
full-precision performance not only on perplexity benchmarks but also on
complex downstream tasks like mathematical reasoning (GSM8K), multi-task
understanding (MMLU), and code generation (HumanEval). Our results show H1B-KV
significantly outperforms leading quantization (KIVI), token eviction
(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,
establishing it as a robust solution for deploying LLMs in memory-constrained
environments.

</details>


### [125] [On the Role of Difficult Prompts in Self-Play Preference Optimization](https://arxiv.org/abs/2510.05534)
*Yao Xiao,Jung-jae Kim,Roy Ka-wei Lee,Lidong Bing*

Main category: cs.CL

TL;DR: self-play preference optimization in LLMs is sensitive to prompt difficulty, with harder prompts underperforming and potentially degrading training. Increasing model capacity closes the performance gap, and selective removal of difficult prompts can improve outcomes.


<details>
  <summary>Details</summary>
Motivation: investigate how prompts of varying difficulty influence self-play preference optimization for LLMs.

Method: Use the mean reward of N sampled responses of a prompt as a proxy for its difficulty. Analyze the impact of difficult prompts on self-play optimization performance, compare it to easy prompts, and examine the interaction between prompt difficulty and model capacity. Explore strategies to mitigate the negative effect of difficult prompts.

Result: Difficult prompts show substantially inferior self-play optimization performance compared to easy prompts. Incorporating difficult prompts into training fails to enhance overall performance and can lead to degradation. The performance gap between difficult and easy prompts closes as model capacity increases. Selectively removing a portion of challenging prompts enhances overall self-play performance.

Conclusion: Prompt difficulty significantly impacts LLM self-play preference optimization. While difficult prompts underperform, increasing model capacity mitigates this issue. Strategic removal of challenging prompts can improve overall performance, highlighting the importance of prompt selection in the training pipeline.

Abstract: Self-play preference optimization has emerged as a prominent paradigm for
aligning large language models (LLMs). It typically involves a language model
to generate on-policy responses for prompts and a reward model (RM) to guide
the selection of chosen and rejected responses, which can be further trained
with direct preference optimization (DPO). However, the role of prompts remains
underexplored, despite being a core component in this pipeline. In this work,
we investigate how prompts of varying difficulty influence self-play preference
optimization. We first use the mean reward of $N$ sampled responses of a prompt
as a proxy for its difficulty. We find that difficult prompts exhibit
substantially inferior self-play optimization performance in comparison to easy
prompts for language models. Moreover, incorporating difficult prompts into
training fails to enhance overall performance and, in fact, leads to slight
degradation compared to training on easy prompts alone. We also observe that
the performance gap between difficult and easy prompts closes as the model
capacity increases, suggesting that difficulty interacts with the model
capacity. Building on these findings, we explore strategies to mitigate the
negative effect of difficult prompts on final performance. We demonstrate that
selectively removing an appropriate portion of challenging prompts enhances
overall self-play performance, while also reporting failed attempts and lessons
learned.

</details>


### [126] [Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM](https://arxiv.org/abs/2510.05544)
*Ryan Solgi,Parsa Madinei,Jiayi Tian,Rupak Swaminathan,Jing Liu,Nathan Susanj,Zheng Zhang*

Main category: cs.CL

TL;DR: 通过低秩压缩框架解决大型语言模型和视觉语言模型的部署挑战，提出PGSVD算法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型和视觉语言模型在部署时面临显著的内存和计算挑战。

Method: 提出了一种新的低秩压缩框架，通过层激活来界定网络损失的变化，并将低秩模型压缩设计成双目标优化问题，最终提出PGSVD算法，通过帕累托最优秩选择和交替最小二乘法实现压缩。

Result: 将PGSVD应用于大型语言模型和视觉语言模型，在相同的压缩水平下实现了更高的准确性和更快的推理速度。

Conclusion: PGSVD是一种有效的低秩压缩框架，能够解决大型语言模型和视觉语言模型的部署挑战。

Abstract: Large language models (LLM) and vision-language models (VLM) have achieved
state-of-the-art performance, but they impose significant memory and computing
challenges in deployment. We present a novel low-rank compression framework to
address this challenge. First, we upper bound the change of network loss via
layer-wise activation-based compression errors, filling a theoretical gap in
the literature. We then formulate low-rank model compression as a bi-objective
optimization and prove that a single uniform tolerance yields surrogate
Pareto-optimal heterogeneous ranks. Based on our theoretical insights, we
propose Pareto-Guided Singular Value Decomposition (PGSVD), a zero-shot
pipeline that improves activation-aware compression via Pareto-guided rank
selection and alternating least-squares implementation. We apply PGSVD to both
LLM and VLM, showing better accuracy at the same compression levels and
inference speedup.

</details>


### [127] [Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations](https://arxiv.org/abs/2510.05571)
*Chengzhi Liu,Yuzhe Yang,Kaiwen Zhou,Zhen Zhang,Yue Fan,Yannan Xie,Peng Qi,Xin Eric Wang*

Main category: cs.CL

TL;DR: EvoPresent是一个用于提升学术论文推广效率和吸引力的自适应智能体框架，通过结合连贯叙事、美学设计和虚拟角色进行个性化展示。其核心是PresAesth，一个多任务强化学习模型，能进行美学评分、缺陷调整和反馈，实现迭代式自我优化。EvoPresent Benchmark包含演示生成质量和美学感知两部分，用于评估该方法。研究表明，高质量反馈对智能体自我提升至关重要，自动生成存在视觉设计与内容构建的权衡，而多任务强化学习在美学感知任务上泛化能力更强。


<details>
  <summary>Details</summary>
Motivation: 现有的学术论文推广方法在叙事性、美学质量和自我调整能力方面存在不足，难以实现高效且引人入胜的传播。根本原因在于无法准确评估推广效果，从而限制了改进的可能。

Method: 提出EvoPresent框架，整合了连贯的叙事、注重美学的اdjustment和通过虚拟角色进行的真实展示。其核心是PresAesth，一个多任务强化学习（RL）美学模型，能够提供可靠的美学评分、缺陷调整和对比反馈，即使在有限的美学训练数据下也能实现迭代式自我改进。同时，引入EvoPresent Benchmark，包含演示生成质量（基于650篇顶尖AI会议论文的多模态资源）和美学感知（基于2000对不同美学水平的幻灯片）两部分，用于系统性评估。

Result: 研究结果表明：1. 高质量的反馈对于智能体的自我改进至关重要，而仅仅具备初始能力并不能保证有效的自我纠正。2. 自动化生成流程在视觉设计和内容构建之间存在权衡。3. 多任务RL训练在美学感知任务上展现出更强的泛化能力。

Conclusion: EvoPresent框架通过其核心的PresAesth美学模型和EvoPresent Benchmark，有效地解决了现有学术论文推广方法在叙事、美学和自我调整方面的局限性，实现了更高效、更具吸引力的学术成果传播。研究强调了高质量反馈、生成过程中的权衡以及多任务RL在提升模型性能方面的重要性。

Abstract: The promotion of academic papers has become an important means of enhancing
research visibility. However, existing automated methods struggle limited
storytelling, insufficient aesthetic quality, and constrained self-adjustment,
making it difficult to achieve efficient and engaging dissemination. At the
heart of those challenges is a simple principle: \emph{there is no way to
improve it when you cannot evaluate it right}. To address this, we introduce
\textbf{EvoPresent}, a self-improvement agent framework that unifies coherent
narratives, aesthetic-aware designs, and realistic presentation delivery via
virtual characters. Central to EvoPresent is \textbf{PresAesth}, a multi-task
reinforcement learning (RL) aesthetic model that provides reliable aesthetic
scoring, defect adjustment, and comparative feedback, enabling iterative
self-improvement even under limited aesthetic training data. To systematically
evaluate the methods, we introduce \textbf{EvoPresent Benchmark}, a
comprehensive benchmark comprising: \textit{Presentation Generation Quality},
built on 650 top-tier AI conference papers with multimodal resources (slides,
videos and scripts) to assess both content and design; and \textit{Aesthetic
Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels,
supporting joint training and evaluation on scoring, defect adjustment, and
comparison. Our findings highlight that (i) High-quality feedback is essential
for agent self-improvement, while initial capability alone does not guarantee
effective self-correction. (ii) Automated generation pipelines exhibit a
trade-off between visual design and content construction. (iii) Multi-task RL
training shows stronger generalization in aesthetic awareness tasks.

</details>


### [128] [Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs](https://arxiv.org/abs/2510.05577)
*Dong Yan,Gaochen Wu,Bowen Zhou*

Main category: cs.CL

TL;DR: FGDIP是一个新框架，通过动态和自适应的信息探索策略来增强语言模型（LLM）的开放域多跳推理能力，实验结果显示在HotpotQA和StrategyQA数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有语言模型在处理需要海量信息检索的开放域多跳推理问题时存在困难，因为它们依赖于固定的动作序列。FGDIP旨在解决这个问题。

Method: FGDIP框架首先识别与问题相关的关键实体作为推理的初始节点。然后，通过历史错误分析和实时反馈相结合的方式生成推理子节点，从而动态地调整和优化推理策略。该框架结合了深度优先搜索和一种新的节点生成技术，能够根据先前的错误路径和同一层次级别上并发生成的节点进行自适应调整，从而扩展搜索空间并确保推理过程系统地收敛到准确的解决方案。

Result: FGDIP在HotpotQA数据集上取得了高达54.47%的F1分数，在StrategyQA数据集上取得了70.05%的F1分数，分别比最佳基线高出5.03%和7.25%。

Conclusion: FGDIP框架能够有效地增强语言模型在开放域多跳推理任务中的表现，展现了其多功能性和潜力。

Abstract: Recent advancements in language agents have led to significant improvements
in multi-hop reasoning tasks. However, existing approaches often struggle with
handling open-domain problems, which require massive information retrieval due
to their reliance on a fixed sequence of actions. To address this, we propose
Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework
tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive
strategies for information exploration in open-domain multi-hop reasoning
tasks. Our approach begins by identifying key entities relevant to the problem,
which serve as the initial nodes in the reasoning process. From these initial
nodes, we then generate reasoning child nodes with the process being refined
through a combination of historical error analysis and real-time feedback,
which allows the framework to dynamically adjust and optimize its reasoning
strategies. By integrating depth-first search with an innovative node
generation technique, our framework adapts based on both prior error paths and
concurrently generated nodes at the same hierarchical level. This dynamic
strategy effectively expands the search space while ensuring the reasoning
process systematically converges toward accurate solutions. Experimental
results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset
and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and
7.25% respectively, highlighting its versatility and potential to enhance
language agents in multi-hop reasoning tasks.

</details>


### [129] [A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks](https://arxiv.org/abs/2510.05608)
*Shuzheng Si,Haozhe Zhao,Kangyang Luo,Gang Chen,Fanchao Qi,Minjia Zhang,Baobao Chang,Maosong Sun*

Main category: cs.CL

TL;DR: EAGLET是一个无需人工干预即可训练的LLM执行器的方法，它通过合成高质量的计划并使用基于规则的强化学习进行微调，提高了LLM在长时任务中的规划能力，同时降低了训练成本。


<details>
  <summary>Details</summary>
Motivation: LLM在处理长时任务时，由于缺乏全局规划能力，容易出现盲目试错和幻觉行为。

Method: 提出了一种即插即用的全局规划器，通过同源共识过滤策略从LLM合成高质量计划，并进行冷启动微调。然后，利用新颖的执行器能力增益奖励，通过基于规则的强化学习进一步优化规划器。

Result: 在三个长时任务的实验中，配备EAGLET规划器的执行器代理的性能优于现有方法，达到了新的SOTA水平。

Conclusion: EAGLET通过合成高质量的计划和基于规则的强化学习，显著提高了LLM在长时任务中的规划能力，同时将训练成本降低了8倍，且无需人工干预或额外数据，是一种高效且有效的解决方案。

Abstract: Agents based on large language models (LLMs) struggle with brainless
trial-and-error and generating hallucinatory actions due to a lack of global
planning in long-horizon tasks. In this paper, we introduce a plan-and-execute
framework and propose EAGLET, an efficient and effective planner training
method to enhance the executor agent's planning abilities without human effort.
Specifically, we train a plug-and-play global planner through a two-step
process: we first synthesize high-quality plans from an advanced LLM using our
proposed homologous consensus filtering strategy, and apply fine-tuning as a
cold start. Moreover, we further improve the planner with a rule-based
reinforcement learning stage using a novel executor capability gain reward,
ensuring it can handle task instructions of varying difficulty. Experiments on
three long-horizon agent tasks show that executor agents equipped with our
planner outperform existing methods, achieving new state-of-the-art
performance. Meanwhile, EAGLET reduces training costs by 8x compared to
RL-based baselines, and it does not require manual effort or extra training
data, offering an efficient and effective solution.

</details>


### [130] [MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction](https://arxiv.org/abs/2510.05611)
*Wei-Chieh Huang,Cornelia Caragea*

Main category: cs.CL

TL;DR: 使用多智能体辩论框架改进电子商务中的隐式属性值提取。


<details>
  <summary>Details</summary>
Motivation: 电子商务中的隐式属性值提取（AVE）对于准确表示产品至关重要，但由于多维数据的复杂性和视觉-文本理解的差距，MLLM 在这方面仍面临挑战。

Method: 提出了一种名为 \textsc{modelname} 的多智能体辩论框架，该框架利用多个 MLLM 智能体通过一系列辩论轮次来迭代地完善推理，智能体相互验证和更新响应。

Result: 在 ImplicitAVE 数据集上的实验表明，即使是少量的辩论也能显著提高准确性，特别是对于初始性能较低的属性。评估了不同的辩论配置，并分析了辩论轮次对收敛动态的影响。

Conclusion: 多智能体辩论策略具有解决单智能体方法的局限性并为电子商务中的隐式 AVE 提供可扩展解决方案的潜力。

Abstract: Implicit Attribute Value Extraction (AVE) is essential for accurately
representing products in e-commerce, as it infers lantent attributes from
multimodal data. Despite advances in multimodal large language models (MLLMs),
implicit AVE remains challenging due to the complexity of multidimensional data
and gaps in vision-text understanding. In this work, we introduce
\textsc{\modelname}, a multi-agent debate framework that employs multiple MLLM
agents to iteratively refine inferences. Through a series of debate rounds,
agents verify and update each other's responses, thereby improving inference
performance and robustness. Experiments on the ImplicitAVE dataset demonstrate
that even a few rounds of debate significantly boost accuracy, especially for
attributes with initially low performance. We systematically evaluate various
debate configurations, including identical or different MLLM agents, and
analyze how debate rounds affect convergence dynamics. Our findings highlight
the potential of multi-agent debate strategies to address the limitations of
single-agent approaches and offer a scalable solution for implicit AVE in
multimodal e-commerce.

</details>


### [131] [COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning](https://arxiv.org/abs/2509.22075)
*Dmitriy Shopkhoev,Denis Makhov,Magauiya Zhussip,Ammar Ali,Stamatios Lefkimmiatis*

Main category: cs.CL

TL;DR: CoSpaDi是一种新的无训练压缩框架，通过结构化稀疏分解替代低秩近似，以实现更灵活的子空间表示和更好的模型保真度，并在准确性和困惑度方面优于现有的低秩方法。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM压缩方法（如低秩近似）存在结构性约束过于僵化，导致模型精度下降的问题。

Method: 提出CoSpaDi框架，使用稠密的字典和稀疏的系数矩阵来表示权重矩阵，实现子空间联合表示，并利用少量校准数据优化分解，最小化输出激活的重建误差，而非仅仅逼近权重。

Result: 在Llama和Qwen模型上，CoSpaDi在20-50%的压缩率下，无论是在逐层还是分组设置下，其准确性和困惑度均优于最先进的低秩方法。

Conclusion: 结构化稀疏字典学习是比传统低秩方法更有效的LLM压缩方法，适用于高效的LLM部署。

Abstract: Post-training compression of large language models (LLMs) largely relies on
low-rank weight approximation, which represents each column of a weight matrix
in a shared low-dimensional subspace. While this is a computationally efficient
strategy, the imposed structural constraint is rigid and can lead to a
noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression
via Sparse Dictionary Learning), a novel training-free compression framework
that replaces low-rank decomposition with a more flexible structured sparse
factorization in which each weight matrix is represented with a dense
dictionary and a column-sparse coefficient matrix. This formulation enables a
union-of-subspaces representation: different columns of the original weight
matrix are approximated in distinct subspaces spanned by adaptively selected
dictionary atoms, offering greater expressiveness than a single invariant
basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the
factorization such that the output activations of compressed projection layers
closely match those of the original ones, thereby minimizing functional
reconstruction error rather than mere weight approximation. This data-aware
strategy preserves better model fidelity without any fine-tuning under
reasonable compression ratios. Moreover, the resulting structured sparsity
allows efficient sparse-dense matrix multiplication and is compatible with
post-training quantization for further memory and latency gains. We evaluate
CoSpaDi across multiple Llama and Qwen models under per-layer and per-group
settings at 20-50\% compression ratios, demonstrating consistent superiority
over state-of-the-art data-aware low-rank methods both in accuracy and
perplexity. Our results establish structured sparse dictionary learning as a
powerful alternative to conventional low-rank approaches for efficient LLM
deployment.

</details>


### [132] [The African Languages Lab: A Collaborative Approach to Advancing Low-Resource African NLP](https://arxiv.org/abs/2510.05644)
*Sheriff Issaka,Keyi Wang,Yinka Ajibola,Oluwatumininu Samuel-Ipaye,Zhaoyi Zhang,Nicte Aguillon Jimenez,Evans Kofi Agyei,Abraham Lin,Rohan Ramachandran,Sadick Abdul Mumin,Faith Nchifor,Mohammed Shuraim,Lieqi Liu,Erick Rosas Gonzalez,Sylvester Kpei,Jemimah Osei,Carlene Ajeneza,Persis Boateng,Prisca Adwoa Dufie Yeboah,Saadia Gabriel*

Main category: cs.CL

TL;DR: 尽管非洲语言在NLP技术中代表性不足，但非洲语言实验室（All Lab）通过系统的数据收集、模型开发和能力建设，解决了这一技术差距，构建了最大的非洲多模态语音和文本数据集，并在31种语言的翻译任务中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 现代自然语言处理（NLP）技术严重忽视了占世界语言近三分之一的非洲语言，其中88%被归类为代表性严重不足或完全被忽略。

Method: 构建了一个高质量的数据收集流程，创建了涵盖40种语言的、拥有190亿词元单语文本和12,628小时对齐语音数据的非洲多模态语音和文本数据集。通过对数据集进行微调，在31种评估语言中，相对于基线模型，ChrF++平均提升了+23.69，COMET平均提升了+0.33，BLEU平均提升了+15.34。与谷歌翻译进行了比较评估。

Result: 生成了最大的非洲多模态语音和文本数据集，涵盖40种语言。通过微调，在31种语言的翻译任务中取得了显著的性能提升，平均ChrF++为+23.69，COMET为+0.33，BLEU为+15.34。与谷歌翻译相比，在某些语言上表现具有竞争力。

Conclusion: 非洲语言实验室（All Lab）通过系统的数据收集、模型开发和能力建设，有效解决了非洲语言在NLP技术中的代表性不足问题，构建了大型数据集并取得了显著的性能提升，为该领域的发展做出了重要贡献。

Abstract: Despite representing nearly one-third of the world's languages, African
languages remain critically underserved by modern NLP technologies, with 88\%
classified as severely underrepresented or completely ignored in computational
linguistics. We present the African Languages Lab (All Lab), a comprehensive
research initiative that addresses this technological gap through systematic
data collection, model development, and capacity building. Our contributions
include: (1) a quality-controlled data collection pipeline, yielding the
largest validated African multi-modal speech and text dataset spanning 40
languages with 19 billion tokens of monolingual text and 12,628 hours of
aligned speech data; (2) extensive experimental validation demonstrating that
our dataset, combined with fine-tuning, achieves substantial improvements over
baseline models, averaging +23.69 ChrF++, +0.33 COMET, and +15.34 BLEU points
across 31 evaluated languages; and (3) a structured research program that has
successfully mentored fifteen early-career researchers, establishing
sustainable local capacity. Our comparative evaluation against Google Translate
reveals competitive performance in several languages while identifying areas
that require continued development.

</details>


### [133] [Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models](https://arxiv.org/abs/2510.05678)
*Haneul Yoo,Jiho Jin,Kyunghyun Cho,Alice Oh*

Main category: cs.CL

TL;DR: CSICL通过在演示和指令中逐步从目标语言转换为英语来促进LLM的潜在推理，从而克服了翻译障碍，提高了跨语言对齐能力，特别是在低资源情况下。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言少样本学习方法依赖单语演示，未能解决LLM潜在的英语表征造成的翻译障碍，导致非英语语言性能下降。

Method: 提出代码转换少样本学习（CSICL），一种通过在演示和指令中逐步从目标语言转换为英语来促进LLM潜在英语推理的提示策略。

Result: CSICL在4个LLM、6个数据集和10种语言的实验中，在目标语言和未见语言上分别比X-ICL基线提高了3.1%和1.9%，在低资源情况下提高更为显著（分别为14.7%和5.3%）。

Conclusion: 代码转换是一种原则性和鲁棒性的方法，可以克服推理过程中的翻译障碍，使LLM在多语言系统中的应用更加公平和有效。

Abstract: While large language models (LLMs) exhibit strong multilingual abilities,
their reliance on English as latent representations creates a translation
barrier, where reasoning implicitly depends on internal translation into
English. When this process fails, performance in non-English languages
deteriorates sharply, limiting the inclusiveness of LLM-based applications.
Existing cross-lingual in-context learning (X-ICL) methods primarily leverage
monolingual demonstrations, often failing to mitigate this barrier and instead
reinforcing it. In this work, we introduce code-switching in-context learning
(CSICL), a simple yet effective prompting strategy that progressively
transitions from a target language to English within demonstrations and
instruction to facilitate their latent reasoning in English. By explicitly
scaffolding the reasoning process through controlled code-switching, CSICL acts
as an implicit linguistic bridge that enhances cross-lingual alignment and
reduces reliance on the translation barrier. We conduct extensive experiments
across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive
and reasoning-oriented domains. Our results demonstrate that CSICL consistently
outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target
and unseen languages, respectively. The improvement is even more pronounced in
low-resource settings, with gains of 14.7% in target and 5.3% in unseen
languages. These findings establish code-switching as a principled and robust
approach for overcoming the translation barrier during inference, moving LLMs
toward more equitable and effective multilingual systems.

</details>


### [134] [DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision](https://arxiv.org/abs/2510.05691)
*Yongqi Leng,Yikun Lei,Xikai Liu,Meizhi Zhong,Bojian Xiong,Yurong Zhang,Yan Gao,Yi Wu,Yao Hu,Deyi Xiong*

Main category: cs.CL

TL;DR: DecEx-RAG通过将RAG建模为马尔可夫决策过程（MDP），并引入高效的剪枝策略，解决了现有方法在探索、奖励信号和全局奖励反馈方面存在的挑战，从而提高了LLM在自主任务分解、动态检索和高质量答案生成方面的能力。


<details>
  <summary>Details</summary>
Motivation: 现有的Agentic RAG方法（如Search-R1）虽然在复杂任务处理方面表现出色，但仍存在探索效率低下、奖励信号稀疏以及全局奖励反馈模糊的问题。

Method: 提出DecEx-RAG，将RAG建模为包含决策和执行的马尔可夫决策过程（MDP），并引入高效的剪枝策略来优化数据扩展。通过全面的过程级策略优化，增强LLM的自主任务分解、动态检索和高质量答案生成能力。

Result: DecEx-RAG在六个数据集上实现了平均6.2%的绝对性能提升，显著优于现有基线。此外，剪枝策略将数据构建效率提高了近6倍。

Conclusion: DecEx-RAG通过改进的MDP建模和剪枝策略，在提高Agentic RAG的性能和训练效率方面取得了显著成效，为过程监督的RAG训练提供了一个高效的解决方案。

Abstract: Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing
capability for complex tasks through dynamic retrieval and adaptive workflows.
Recent advances (e.g., Search-R1) have shown that outcome-supervised
reinforcement learning demonstrate strong performance. However, this approach
still suffers from inefficient exploration, sparse reward signals, and
ambiguous global reward feedback. To address these challenges, we propose
DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating
decision-making and execution, while introducing an efficient pruning strategy
to optimize data expansion. Through comprehensive process-level policy
optimization, DecEx-RAG significantly enhances the autonomous task
decomposition, dynamic retrieval, and high-quality answer generation
capabilities of large language models (LLMs). Experiments show that DecEx-RAG
achieves an average absolute performance improvement of $6.2\%$ across six
datasets, significantly outperforming existing baselines. Moreover, the pruning
strategy improves data construction efficiency by nearly $6 \times$, providing
an efficient solution for process-supervised RAG training. The code is
available at https://github.com/sdsxdxl/DecEx-RAG.

</details>


### [135] [Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities](https://arxiv.org/abs/2510.05744)
*Liza Fretel,Baptiste Cecconi,Laura Debisschop*

Main category: cs.CL

TL;DR: 生成多源天文观测设施映射


<details>
  <summary>Details</summary>
Motivation: 为了比较两个实体，我们需要开发一种生成多源天文观测设施映射的方法。

Method: 通过利用包括Wikidata和面向天文学的资源在内的八个语义工件中提取的实体，并采用词袋方法、顺序方法和表面方法等自然语言处理技术，并计算具有可适应标准的得分。此外，还利用了标签、定义、描述、外部标识符以及观测波段、航天器发射日期、资助机构等特定领域属性。最后，使用大型语言模型来接受或拒绝映射建议并提供理由，以确保所验证的同义词对的合理性和FAIR性。

Result: 生成由多源同义词集组成的映射，每个实体只有一个标准化的标签。

Conclusion: 最终生成的映射将用于为我们的名称解析器API提供数据，并整合到国际虚拟天文台联盟（IVOA）词汇表和OntoPortal-Astro平台中。

Abstract: This ongoing work focuses on the development of a methodology for generating
a multi-source mapping of astronomical observation facilities. To compare two
entities, we compute scores with adaptable criteria and Natural Language
Processing (NLP) techniques (Bag-of-Words approaches, sequential approaches,
and surface approaches) to map entities extracted from eight semantic
artifacts, including Wikidata and astronomy-oriented resources. We utilize
every property available, such as labels, definitions, descriptions, external
identifiers, and more domain-specific properties, such as the observation
wavebands, spacecraft launch dates, funding agencies, etc. Finally, we use a
Large Language Model (LLM) to accept or reject a mapping suggestion and provide
a justification, ensuring the plausibility and FAIRness of the validated
synonym pairs. The resulting mapping is composed of multi-source synonym sets
providing only one standardized label per entity. Those mappings will be used
to feed our Name Resolver API and will be integrated into the International
Virtual Observatory Alliance (IVOA) Vocabularies and the OntoPortal-Astro
platform.

</details>


### [136] [Diversity Is All You Need for Contrastive Learning: Spectral Bounds on Gradient Magnitudes](https://arxiv.org/abs/2510.05767)
*Peter Ochieng*

Main category: cs.CL

TL;DR: 本论文推导了非渐近谱带，以界定 InfoNCE 梯度范数的平方，该范数受对齐、温度和批次谱的影响，并与 1/τ² 定律及合成数据和 ImageNet 上的批次均值梯度密切相关。通过使用有效秩 R_eff 作为各向异性代理，设计了包括快速贪婪构建器在内的谱感知批次选择方法。


<details>
  <summary>Details</summary>
Motivation: 通过推导非渐近谱带，研究 InfoNCE 梯度范数界限，并探索批次谱、对齐和温度等因素的影响。

Method: 推导非渐近谱带以界定 InfoNCE 梯度范数的平方；使用有效秩 R_eff 作为各向异性代理，设计谱感知批次选择方法；引入了贪婪构建器；在 CIFAR-10 和 ImageNet-100 上进行了实验。

Result: 在 ImageNet-100 上，Greedy-64 将达到 67.5% 的 top-1 准确率的时间缩短了 15%（相比随机选择），或 24%（相比 Pool--P3），同时保持了相同的准确率。CIFAR-10 也显示出类似的收益。批内白化提高了各向同性，并将 50 步的梯度方差减少了 1.37 倍，这与理论推导的上限一致。

Conclusion: 谱感知批次选择方法（如 Greedy-64）和批内白化可以显著提高训练效率和稳定性。

Abstract: We derive non-asymptotic spectral bands that bound the squared InfoNCE
gradient norm via alignment, temperature, and batch spectrum, recovering the
\(1/\tau^{2}\) law and closely tracking batch-mean gradients on synthetic data
and ImageNet. Using effective rank \(R_{\mathrm{eff}}\) as an anisotropy proxy,
we design spectrum-aware batch selection, including a fast greedy builder. On
ImageNet-100, Greedy-64 cuts time-to-67.5\% top-1 by 15\% vs.\ random (24\%
vs.\ Pool--P3) at equal accuracy; CIFAR-10 shows similar gains. In-batch
whitening promotes isotropy and reduces 50-step gradient variance by
\(1.37\times\), matching our theoretical upper bound.

</details>


### [137] [InforME: Improving Informativeness of Abstractive Text Summarization With Informative Attention Guided by Named Entity Salience](https://arxiv.org/abs/2510.05769)
*Jianbin Shen,Christy Jie Liang,Junyu Xuan*

Main category: cs.CL

TL;DR: 本文提出了一种结合最优传输和联合熵减的抽象文本摘要新方法，通过关注关键信息和命名实体来提升摘要信息量，并在CNN/Daily Mail和XSum数据集上取得了更好的ROUGE分数和人类评估结果。


<details>
  <summary>Details</summary>
Motivation: 随着大数据时代的到来，需要更有效的方法来处理海量文本数据，生成简洁、连贯且信息丰富的摘要，但现有方法在信息量方面仍有提升空间。

Method: 提出了一种包含两个方法的学习方法：1. 基于最优传输的信息注意力机制，用于提升摘要中关键信息的学习。2. 命名实体的累积联合熵减方法，用于增强信息显著性。

Result: 在CNN/Daily Mail数据集上取得了优于先前工作的ROUGE分数，并在XSum数据集上取得了有竞争力的结果。人类评估也表明该方法在信息量方面优于强基线模型。

Conclusion: 该方法在提高文本摘要信息量方面是有效的，实验结果和人类评估均证实了其优越性。对结果的进一步分析也为理解其有效性提供了依据。

Abstract: Abstractive text summarization is integral to the Big Data era, which demands
advanced methods to turn voluminous and often long text data into concise but
coherent and informative summaries for efficient human consumption. Despite
significant progress, there is still room for improvement in various aspects.
One such aspect is to improve informativeness. Hence, this paper proposes a
novel learning approach consisting of two methods: an optimal transport-based
informative attention method to improve learning focal information in reference
summaries and an accumulative joint entropy reduction method on named entities
to enhance informative salience. Experiment results show that our approach
achieves better ROUGE scores compared to prior work on CNN/Daily Mail while
having competitive results on XSum. Human evaluation of informativeness also
demonstrates the better performance of our approach over a strong baseline.
Further analysis gives insight into the plausible reasons underlying the
evaluation results.

</details>


### [138] [Mixture of Neuron Experts](https://arxiv.org/abs/2510.05781)
*Runxi Cheng,Yuchen Guan,Yucheng Ding,Qingguo Hu,Yongxian Wei,Chun Yuan,Yelong Shen,Weizhu Chen,Yeyun Gong*

Main category: cs.CL

TL;DR: MoNE通过在专家内部进行神经元粒度的专家选择，实现了与传统MoE相当的性能，同时减少了参数激活数量和计算开销。


<details>
  <summary>Details</summary>
Motivation: 在推理时，MoE（Mixture of Experts）层的激活参数是否保持高度稀疏性？大部分神经元激活值接近于零，这表明在预训练时，我们应该只选择高激活神经元专家。

Method: 首先，对MoE模型进行稀疏性研究，分析参数在激活子集中的修剪阈值。然后，将专家分解为神经元粒度，可视化激活值。在此基础上，提出MoNE（Mixture of Neuron Experts），通过在每个专家内部进行Top-K神经元选择来实现神经元粒度的专家选择，无需额外的路由参数或通信开销。

Result: MoNE在激活参数数量减半的情况下，性能与传统MoE相当，并且在激活参数数量相同时，性能优于传统MoE。实验表明，MoNE在提高模型参数利用率和推理效率方面是一种有效的方法。

Conclusion: MoNE是一种实用的方法，可以提高MoE类模型的参数利用率和推理效率。

Abstract: In this work, we first explore whether the parameters activated by the MoE
layer remain highly sparse at inference. We perform a sparsification study on
several representative MoE models. For each expert, we rank parameters by the
magnitude of their activations from the gate projection and progressively prune
the activated subset. Pruning up to 60% of parameters within that subset causes
only negligible task-performance degradation; substantial drops occur only
after more than 90% are removed. We further decompose experts into
neuron-granular MoE and visualize their activation values, finding that most
neuron activations are near zero. This observation motivates us to select only
high-activation neuron experts during pretraining. Based on this insight, we
propose Mixture of Neuron Experts (MoNE). MoNE achieves neuron-granular expert
selection by only applying a simple top-k selection within each expert, incurs
negligible latency, and requires no additional routing parameters or
inter-expert communication. Extensive experiments demonstrate that MoNE matches
traditional MoE performance while activating only 50% of the MoE-layer
parameters, and it consistently outperforms traditional MoE when compared at
equal numbers of activated parameters. These results suggest that MoNE is a
practical approach to improving parameter utilization and inference efficiency
in MoE-like models.

</details>


### [139] [Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech](https://arxiv.org/abs/2510.05799)
*Rikuto Kotoge,Yuichi Sasaki*

Main category: cs.CL

TL;DR: 本研究提出的TKTO方法无需配对数据，可以直接针对 token 级别进行优化，从而更高效地提升TTS系统的准确性和自然度。


<details>
  <summary>Details</summary>
Motivation: 现有的基于人类反馈的TTS优化方法通常需要配对的理想和非理想样本，但这种配对数据在TTS输出中往往有限，并且仅限于语句级别，无法实现精细化的发音对齐。因此，需要一种更有效的数据训练范式，直接针对 token 级别进行优化。

Method: TKTO方法无需配对数据，直接针对 token 级别进行优化，自动提供精细化的对齐信号，无需 token 级别注释。

Result: TKTO方法将具有挑战性的日语TTS准确性提高了39%，并将词错误率（CER）降低了54%，同时自动为目标 token 分配了12.8倍的强奖励。

Conclusion: TKTO通过消除配对数据的需求并直接针对 token 级别进行优化，实现了更高效的训练，并显著提高了TTS系统的准确性和自然度。

Abstract: Aligning text-to-speech (TTS) system outputs with human feedback through
preference optimization has been shown to effectively improve the robustness
and naturalness of language model-based TTS models. Current approaches
primarily require paired desirable and undesirable samples at the utterance
level. However, such pairs are often limited in TTS output data, and
utterance-level formulation prevents fine-grained token-level optimization
needed for accurate pronunciation alignment. In this study, we propose TKTO
that eliminates the need for paired data, enabling a more data-efficient
training paradigm, and directly targets token-level units, automatically
providing fine-grained alignment signals without token-level annotations. TKTO
improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%,
automatically assigning 12.8 times stronger reward to targeted tokens.

</details>


### [140] [EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget](https://arxiv.org/abs/2510.05837)
*Liang Chen,Xueting Han,Qizhou Wang,Bo Han,Jing Bai,Hinrich Schutze,Kam-Fai Wong*

Main category: cs.CL

TL;DR: EEPO通过两阶段回滚和自适应学习促进探索，解决了RLVR中探索与利用的平衡问题，并在多个推理基准测试中优于GRPO。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法倾向于过度利用，导致探索能力下降和性能受限，即使增加策略随机性的技术也难以跳出主导行为模式，形成恶性循环。

Method: EEPO采用两阶段回滚和自适应学习机制：首先，模型生成部分轨迹；然后，通过轻量级学习步骤暂时抑制这些采样响应，迫使第二阶段探索不同的输出空间区域，从而打破恶性循环。

Result: EEPO在五个推理基准测试中表现优于GRPO，在Qwen2.5-3B上平均相对提升24.3%，在Llama3.2-3B-Instruct上提升33.0%，在Qwen3-8B-Base上提升10.4%。

Conclusion: EEPO框架通过样本-遗忘机制有效促进了更广泛的探索，解决了RLVR中的探索-利用平衡挑战，并在多项推理任务上取得了显著的性能提升。

Abstract: Balancing exploration and exploitation remains a central challenge in
reinforcement learning with verifiable rewards (RLVR) for large language models
(LLMs). Current RLVR methods often overemphasize exploitation, leading to
entropy collapse, diminished exploratory capacity, and ultimately limited
performance gains. Although techniques that increase policy stochasticity can
promote exploration, they frequently fail to escape dominant behavioral modes.
This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant
modes-that further erodes exploration. We introduce Exploration-Enhanced Policy
Optimization (EEPO), a framework that promotes exploration via two-stage
rollouts with adaptive unlearning. In the first stage, the model generates half
of the trajectories; it then undergoes a lightweight unlearning step to
temporarily suppress these sampled responses, forcing the second stage to
explore different regions of the output space. This sample-then-forget
mechanism disrupts the self-reinforcing loop and promotes wider exploration
during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO,
achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on
Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.

</details>


### [141] [Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer](https://arxiv.org/abs/2510.05846)
*Maxence Lasbordes,Sinoué Gad*

Main category: cs.CL

TL;DR: LLMs在法语方面表现不佳，我们提出了Luth，一种专门针对法语的SLM，在法语基准测试中表现优于其他开源模型，同时保持了英语能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多语言模型在法语上的表现远不如英语，并且针对法语的高效适配方法研究有限。

Method: 通过在精心策划的高质量法语数据上进行定向的训练后，对LLM进行微调，并采用模型合并策略。

Result: Luth模型在多个法语基准测试中表现优于所有同等规模的开源模型，同时保留了英语能力。模型合并策略进一步提高了两种语言的表现。

Conclusion: Luth系列模型是法语SLM的新技术水平，为未来的法语语言研究提供了一个坚实的基础。

Abstract: The landscape of Large Language Models (LLMs) remains predominantly
English-centric, resulting in a significant performance gap for other major
languages, such as French, especially in the context of Small Language Models
(SLMs). Existing multilingual models demonstrate considerably lower performance
in French compared to English, and research on efficient adaptation methods for
French remains limited. To address this, we introduce \textbf{Luth}, a family
of French-specialized SLMs: through targeted post-training on curated,
high-quality French data, our models outperform all open-source counterparts of
comparable size on multiple French benchmarks while retaining their original
English capabilities. We further show that strategic model merging enhances
performance in both languages, establishing Luth as a new state of the art for
French SLMs and a robust baseline for future French-language research.

</details>


### [142] [DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization](https://arxiv.org/abs/2510.05858)
*Xue-Yong Fu,Elena Khasanova,Md Tahmid Rahman Laskar,Harsh Saini,Shashi Bhushan TN*

Main category: cs.CL

TL;DR: continual pre-training can improve LLMs for conversational summarization using unlabeled data.


<details>
  <summary>Details</summary>
Motivation: Fine-tuning LLMs for summarization in specialized domains or conversational data is limited by costly and scarce labeled data.

Method: Investigate continual pre-training as a scalable, self-supervised approach to adapt LLMs for conversational summarization using large-scale, unlabeled business conversation data.

Result: Continual pre-training significantly improves both in-domain and out-of-domain summarization benchmarks, while maintaining generalization and robustness. Data selection strategies also impact performance.

Conclusion: Continual pre-training is an effective method for adapting LLMs to conversational summarization tasks, offering practical guidelines for industrial applications.

Abstract: Large language models (LLMs) have achieved impressive performance in text
summarization, yet their performance often falls short when applied to
specialized domains %or conversational data that differ from their original
pre-training distribution. While fine-tuning can improve summarization quality,
it typically relies on costly and scarce high-quality labeled data. In this
work, we explore continual pre-training as a scalable, self-supervised approach
to adapt LLMs for downstream summarization tasks, particularly in the context
of noisy real-world conversation transcripts. We conduct extensive experiments
using large-scale, unlabeled business conversation data to investigate whether
continual pre-training enhances model capabilities in conversational
summarization. Our results demonstrate that continual pre-training yields
substantial gains in both in-domain and out-of-domain summarization benchmarks,
while maintaining strong generalization and robustness. We also analyze the
effects of data selection strategies, providing practical guidelines for
applying continual pre-training in summarization-focused industrial
applications.

</details>


### [143] [Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies](https://arxiv.org/abs/2510.05860)
*Luka Nenadic,David Rodriguez*

Main category: cs.CL

TL;DR: 瑞士隐私法修订背景下，分析了自动化合同生成器在提高公司隐私政策合规性方面的作用。研究发现，使用自动化生成器的公司合规性显著提高，并探讨了大型语言模型在法律分析中的潜力。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在解决在日益复杂的数字法规环境下，特别是小型企业在遵守法律方面面临资源和专业知识不足的问题，并评估自动化法律服务提供商（如合同生成器）的普及度和输出质量。

Method: 研究人员创建并注释了一个多语言基准数据集，该数据集涵盖了瑞士和欧盟隐私法的关键合规义务，并利用该数据集验证了一种基于GPT-5的新型方法，以大规模评估隐私政策的合规性，从而衡量2023年瑞士隐私法修订的影响。

Result: 研究发现，瑞士隐私法修订后合规性有所提高。自动化合同生成器被18%的本地网站明确引用，并且与显著更高的合规水平相关，合规性提高了多达15个百分点。这表明自动化工具能够有效提升合规性。

Conclusion: 本研究的发现支持了大型语言模型在跨语言法律分析中的潜力，证实了欧盟法规的“布鲁塞尔效应”，并强调了自动化工具在提高合规性和合同质量方面的重要作用。

Abstract: It has become increasingly challenging for firms to comply with a plethora of
novel digital regulations. This is especially true for smaller businesses that
often lack both the resources and know-how to draft complex legal documents.
Instead of seeking costly legal advice from attorneys, firms may turn to
cheaper alternative legal service providers such as automated contract
generators. While these services have a long-standing presence, there is little
empirical evidence on their prevalence and output quality.
  We address this gap in the context of a 2023 Swiss privacy law revision. To
enable a systematic evaluation, we create and annotate a multilingual benchmark
dataset that captures key compliance obligations under Swiss and EU privacy
law. Using this dataset, we validate a novel GPT-5-based method for large-scale
compliance assessment of privacy policies, allowing us to measure the impact of
the revision. We observe compliance increases indicating an effect of the
revision. Generators, explicitly referenced by 18% of local websites, are
associated with substantially higher levels of compliance, with increases of up
to 15 percentage points compared to privacy policies without generator use.
These findings contribute to three debates: the potential of LLMs for
cross-lingual legal analysis, the Brussels Effect of EU regulations, and,
crucially, the role of automated tools in improving compliance and contractual
quality.

</details>


### [144] [Revisiting Long-context Modeling from Context Denoising Perspective](https://arxiv.org/abs/2510.05862)
*Zecheng Tang,Baibei Ji,Juntao Li,Lijun Wu,Haijia Gui,Min Zhang*

Main category: cs.CL

TL;DR: 长文本模型（LCM）在处理长序列方面表现出巨大潜力，但容易受到噪声的干扰。本文提出了一种基于集成梯度（IG）分数的噪声检测方法，并开发了一种名为上下文去噪训练（CDT）的策略，以提高模型对关键信息的关注度。实验证明，CDT可以显著提升模型性能，甚至使一个8B模型在某些任务上达到与GPT-4o相当的水平。


<details>
  <summary>Details</summary>
Motivation: 长文本模型（LCM）虽然在处理长序列方面有很大潜力，但容易受到噪声的干扰，这会误导模型的注意力。因此，需要一种方法来检测和量化这种噪声，并减轻其对模型性能的影响。

Method: 本文提出了一种基于集成梯度（IG）分数的噪声检测方法，用于量化长文本中的噪声信息。在此基础上，开发了一种名为上下文去噪训练（CDT）的策略，旨在提高模型对关键信息的关注度，并增强这些信息对模型预测的影响。

Result: 研究发现，即使是简单的噪声缓解措施也能显著提升模型对关键令牌的注意力，并最终改善预测结果。CDT策略在多种任务和设置下都表现出优越性，特别是能够使一个8B模型在性能上接近GPT-4o。

Conclusion: 上下文去噪训练（CDT）是一种有效的方法，可以提高长文本模型（LCM）对关键信息的关注度，从而提升模型性能。该方法通过集成梯度（IG）分数来检测和量化噪声，并以此为依据进行模型训练，最终在多种任务上取得了显著的改进，甚至能够使一个8B模型达到与GPT-4o相媲美的性能。

Abstract: Long-context models (LCMs) have demonstrated great potential in processing
long sequences, facilitating many real-world applications. The success of LCMs
can be attributed to their ability to locate implicit critical information
within the context for further prediction. However, recent research reveals
that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens,
that can mislead model attention. In this paper, we conduct a fine-grained
analysis of the context noise and propose an effective metric, the Integrated
Gradient (IG) score, to detect and quantify the noise information within the
context. Our findings reveal that even simple mitigation of detected context
noise can substantially boost the model's attention on critical tokens and
benefit subsequent predictions. Building on this insight, we propose Context
Denoising Training (CDT), a straightforward yet effective training strategy
that improves attention on critical tokens while reinforcing their influence on
model predictions. Extensive experiments across four tasks, under both context
window scaling and long-context alignment settings, demonstrate the superiority
of CDT. Notably, when trained with CDT, an open-source 8B model can achieve
performance (50.92) comparable to GPT-4o (51.00).

</details>


### [145] [Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input](https://arxiv.org/abs/2510.05864)
*Faeze Ghorbanpour,Alexander Fraser*

Main category: cs.CL

TL;DR: LLMs在长上下文中的有害内容检测能力在不同类型、位置、普遍性和上下文长度下存在显著差异，其性能在有害内容占比为0.25时达到峰值，且随着上下文长度增加而下降，对开头和显式有害内容的检测效果更佳。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLMs）在长上下文场景下对有害内容的敏感性，以了解其在安全关键应用中的行为。

Method: 通过改变有害内容的类型（显式 vs. 隐式）、位置（开头、中间、结尾）、普遍性（占提示的0.01-0.50）和上下文长度（600-6000个token），在LLaMA-3、Qwen-2.5和Mistral模型上进行评估，涵盖了毒性、冒犯性和仇恨言论等有害内容类别。

Result: 研究发现，LLMs对有害内容的性能在有害内容普遍性达到0.25时达到峰值，在内容稀疏或占主导地位时性能下降；召回率随上下文长度增加而降低；开头位置的有害内容通常能更可靠地检测出来；显式有害内容比隐式有害内容更容易被识别。

Conclusion: 这项研究首次系统地展示了LLMs在长上下文环境中如何优先处理和校准有害内容，揭示了其在安全关键应用方面的新兴优势和尚存的挑战。

Abstract: Large language models (LLMs) increasingly support applications that rely on
extended context, from document processing to retrieval-augmented generation.
While their long-context capabilities are well studied for reasoning and
retrieval, little is known about their behavior in safety-critical scenarios.
We evaluate LLMs' sensitivity to harmful content under extended context,
varying type (explicit vs. implicit), position (beginning, middle, end),
prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens).
Across harmful content categories such as toxic, offensive, and hate speech,
with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance
peaks at moderate harmful prevalence (0.25) but declines when content is very
sparse or dominant; recall decreases with increasing context length; harmful
sentences at the beginning are generally detected more reliably; and explicit
content is more consistently recognized than implicit. These findings provide
the first systematic view of how LLMs prioritize and calibrate harmful content
in long contexts, highlighting both their emerging strengths and the challenges
that remain for safety-critical use.

</details>


### [146] [The fragility of "cultural tendencies" in LLMs](https://arxiv.org/abs/2510.05869)
*Kun Sun,Rong Wang*

Main category: cs.CL

TL;DR: 尽管LSZ声称LLM会根据提示的语言显示出文化倾向，但本研究通过更广泛的LLM和更多的测试项目进行重复实验，发现提示语言对模型输出的影响很小，质疑LSZ的结论。


<details>
  <summary>Details</summary>
Motivation: 评估LSZ声称LLM在不同语言提示下会表现出文化倾向的研究，并质疑其方法论、理论框架和结论。

Method: 通过使用更广泛的LLM和更多的测试项目进行有针对性的重复实验，来重新评估LSZ的研究方法、理论框架和结论。

Result: 提示语言对LLM输出的文化倾向影响很小，这与LSZ的研究结果不符。

Conclusion: LLM的“文化倾向”并非稳定的内在特征，而是模型和任务设计的特定产物，并非如LSZ所声称的那样，模型编码了深层的文化信念。

Abstract: In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large
language models (LLMs), when prompted in different languages, display
culturally specific tendencies. They report that the two models (i.e., GPT and
ERNIE) respond in more interdependent and holistic ways when prompted in
Chinese, and more independent and analytic ways when prompted in English. LSZ
attribute these differences to deep-seated cultural patterns in the models,
claiming that prompt language alone can induce substantial cultural shifts.
While we acknowledge the empirical patterns they observed, we find their
experiments, methods, and interpretations problematic. In this paper, we
critically re-evaluate the methodology, theoretical framing, and conclusions of
LSZ. We argue that the reported "cultural tendencies" are not stable traits but
fragile artifacts of specific models and task design. To test this, we
conducted targeted replications using a broader set of LLMs and a larger number
of test items. Our results show that prompt language has minimal effect on
outputs, challenging LSZ's claim that these models encode grounded cultural
beliefs.

</details>


### [147] [Prompt reinforcing for long-term planning of large language models](https://arxiv.org/abs/2510.05921)
*Hsien-Chin Lin,Benjamin Matthias Ruppik,Carel van Niekerk,Chia-Hao Shen,Michael Heck,Nurul Lubis,Renato Vukovic,Shutong Feng,Milica Gašić*

Main category: cs.CL

TL;DR: LLM在多轮交互中表现不佳，提出一种受强化学习启发的提示优化框架，通过生成反馈和经验回放来重写提示，显著提高了LLM在文本到SQL和面向任务对话等任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在多轮交互中表现不佳，经常依赖错误的早期假设并且无法跟踪用户目标，使得这类任务具有挑战性。对话系统中的先前工作表明，长期规划对于处理交互式任务至关重要。

Method: 提出一个受强化学习启发的提示优化框架，通过生成逐轮反馈和利用经验回放进行提示重写，实现了LLM的长期规划能力，仅修改LLM代理的任务指令提示。

Result: 在文本到SQL和面向任务对话等多轮任务中取得了显著的改进，并且可以跨不同的LLM代理进行泛化，并利用不同的LLM作为元提示代理。

Conclusion: 所提出的方法在多轮任务中表现出显著的改进，并且具有泛化能力，这表明了受强化学习启发的无参数优化方法值得未来研究。

Abstract: Large language models (LLMs) have achieved remarkable success in a wide range
of natural language processing tasks and can be adapted through prompting.
However, they remain suboptimal in multi-turn interactions, often relying on
incorrect early assumptions and failing to track user goals over time, which
makes such tasks particularly challenging. Prior works in dialogue systems have
shown that long-term planning is essential for handling interactive tasks. In
this work, we propose a prompt optimisation framework inspired by reinforcement
learning, which enables such planning to take place by only modifying the task
instruction prompt of the LLM-based agent. By generating turn-by-turn feedback
and leveraging experience replay for prompt rewriting, our proposed method
shows significant improvement in multi-turn tasks such as text-to-SQL and
task-oriented dialogue. Moreover, it generalises across different LLM-based
agents and can leverage diverse LLMs as meta-prompting agents. This warrants
future research in reinforcement learning-inspired parameter-free optimisation
methods.

</details>


### [148] [Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens](https://arxiv.org/abs/2510.05931)
*Mai AlKhamissi,Yunze Xiao,Badr AlKhamissi,Mona Diab*

Main category: cs.CL

TL;DR: 文化基准测试未能捕捉文化的动态性，需要更符合人类学的方法。


<details>
  <summary>Details</summary>
Motivation: 当前的文化评估基准测试未能充分捕捉文化的动态性、历史情境和实践性，而人类学研究强调了这些方面。因此，需要改进基准测试以更准确地评估大型语言模型在复杂文化情境中的表现。

Method: 提出一个四部分框架（知识、偏好、表现、偏见）来分析基准测试如何构建文化。定性地检查了 20 个文化基准测试，识别了六个方法论问题（例如，将国家等同于文化、忽视文化内部多样性、过度简化调查格式）。基于人类学方法，提出改进建议，包括纳入真实叙事和情境、让文化社区参与设计和验证、以及在情境中评估模型。

Result: 确定了当前文化基准测试的局限性，包括将国家等同于文化、忽视文化内部多样性以及依赖过度简化的调查格式。提出了改进建议，例如纳入真实世界叙事、让社区参与以及在情境中评估。

Conclusion: 目前的文化基准测试存在不足，需要采用更符合人类学的方法，重点关注动态性、实践性和文化内部多样性，以更准确地评估大型语言模型。

Abstract: Cultural evaluation of large language models has become increasingly
important, yet current benchmarks often reduce culture to static facts or
homogeneous values. This view conflicts with anthropological accounts that
emphasize culture as dynamic, historically situated, and enacted in practice.
To analyze this gap, we introduce a four-part framework that categorizes how
benchmarks frame culture, such as knowledge, preference, performance, or bias.
Using this lens, we qualitatively examine 20 cultural benchmarks and identify
six recurring methodological issues, including treating countries as cultures,
overlooking within-culture diversity, and relying on oversimplified survey
formats. Drawing on established anthropological methods, we propose concrete
improvements: incorporating real-world narratives and scenarios, involving
cultural communities in design and validation, and evaluating models in context
rather than isolation. Our aim is to guide the development of cultural
benchmarks that go beyond static recall tasks and more accurately capture the
responses of the models to complex cultural situations.

</details>


### [149] [EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models](https://arxiv.org/abs/2510.05942)
*Hadi Mohammadi,Anastasia Giachanou,Ayoub Bagheri*

Main category: cs.CL

TL;DR: EvalMORAAL是一个评估大型语言模型道德对齐的框架，发现存在地域偏见。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）在不同文化和地区间的道德对齐情况，并解决现有评估方法的局限性。

Method: 提出EvalMORAAL框架，结合了两种评分方法（对数概率和直接评分）以及模型作为裁判的同行评审机制。在世界价值观调查（WVS）和皮尤全球态度调查（PEW）的数据集上评估了20个LLM。

Result: 在WVS数据集上，顶尖模型与调查回应的相关性（Pearson's r）约为0.90。然而，西方地区的平均相关性为0.82，而非西方地区为0.61，存在显著的地域偏见。模型作为裁判的同行评审机制识别出348个冲突，且同行评分与调查对齐度相关。

Conclusion: EvalMORAAL框架在实现文化感知AI方面取得了进展，但仍需解决跨地区应用的挑战，特别是地域偏见问题。

Abstract: We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that
uses two scoring methods (log-probabilities and direct ratings) plus a
model-as-judge peer review to evaluate moral alignment in 20 large language
models. We assess models on the World Values Survey (55 countries, 19 topics)
and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL,
top models align closely with survey responses (Pearson's r approximately 0.90
on WVS). Yet we find a clear regional difference: Western regions average
r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap),
indicating consistent regional bias. Our framework adds three parts: (1) two
scoring methods for all models to enable fair comparison, (2) a structured
chain-of-thought protocol with self-consistency checks, and (3) a
model-as-judge peer review that flags 348 conflicts using a data-driven
threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39,
both p<.001), supporting automated quality checks. These results show real
progress toward culture-aware AI while highlighting open challenges for use
across regions.

</details>


### [150] [Probing the Difficulty Perception Mechanism of Large Language Models](https://arxiv.org/abs/2510.05969)
*Sunbowen Lee,Qingyu Yin,Chak Tou Leong,Jialiang Zhang,Yicheng Gong,Xiaoyu Shen*

Main category: cs.CL

TL;DR: LLMs can implicitly assess math problem difficulty through their internal representations, specifically in certain attention heads of the final Transformer layer. This finding has practical implications for automatic difficulty annotation, reducing reliance on human labeling, and offers theoretical insights into LLM capabilities.


<details>
  <summary>Details</summary>
Motivation: The paper aims to investigate whether LLMs can internally evaluate problem difficulty, a crucial ability for adaptive reasoning and efficient resource allocation, which is currently not well understood.

Method: The study uses linear probes on the final-token representations of LLMs to model problem difficulty and identifies specific attention heads in the final Transformer layer responsible for this perception by analyzing their activation patterns for simple vs. difficult problems. Ablation experiments are conducted to validate the identified locations.

Result: Math problem difficulty can be linearly modeled using LLM representations. Specific attention heads in the final Transformer layer were identified, showing distinct activation patterns for problems of varying difficulty. Ablation experiments confirmed the accuracy of these identified locations. A significant difference in entropy and difficulty perception at the token level was also uncovered.

Conclusion: LLMs possess an inherent and structurally organized ability to perceive problem difficulty, demonstrated through their internal representations and specific attention mechanisms. This capability can be leveraged for practical applications like automatic difficulty annotation, offering theoretical insights into LLM reasoning processes.

Abstract: Large language models (LLMs) are increasingly deployed on complex reasoning
tasks, yet little is known about their ability to internally evaluate problem
difficulty, which is an essential capability for adaptive reasoning and
efficient resource allocation. In this work, we investigate whether LLMs
implicitly encode problem difficulty in their internal representations. Using a
linear probe on the final-token representations of LLMs, we demonstrate that
the difficulty level of math problems can be linearly modeled. We further
locate the specific attention heads of the final Transformer layer: these
attention heads have opposite activation patterns for simple and difficult
problems, thus achieving perception of difficulty. Our ablation experiments
prove the accuracy of the location. Crucially, our experiments provide
practical support for using LLMs as automatic difficulty annotators,
potentially substantially reducing reliance on costly human labeling in
benchmark construction and curriculum learning. We also uncover that there is a
significant difference in entropy and difficulty perception at the token level.
Our study reveals that difficulty perception in LLMs is not only present but
also structurally organized, offering new theoretical insights and practical
directions for future research.

</details>


### [151] [LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language](https://arxiv.org/abs/2510.05972)
*Periklis Mantenoglou,Rishi Hazra,Pedro Zuidberg Dos Martires,Luc De Raedt*

Main category: cs.CL

TL;DR: LexiCon是一个新的基准测试，用于评估LLM在带有约束条件规划任务中的表现，表明随着约束的增加，LLM的性能会下降。


<details>
  <summary>Details</summary>
Motivation: 需要评估LLM在现实世界场景中的规划能力，而现实世界场景通常需要遵守约束，特别是安全约束。

Method: 提出LexiCon基准测试，它通过在现有规划环境中添加状态的时间约束来创建基于自然语言的规划任务，并自动为新环境生成约束。

Result: 实验表明，随着规划任务约束程度的增加，包括GPT-5、o3和R1在内的LLM的性能会下降。

Conclusion: LLM在约束条件规划任务中的性能会随着约束的增加而下降，这表明在部署LLM解决实际问题之前，还需要进一步的研究和改进。

Abstract: Owing to their reasoning capabilities, large language models (LLMs) have been
evaluated on planning tasks described in natural language. However, LLMs have
largely been tested on planning domains without constraints. In order to deploy
them in real-world settings where adherence to constraints, in particular
safety constraints, is critical, we need to evaluate their performance on
constrained planning tasks. We introduce LexiCon -- a natural language-based
(Lexi) constrained (Con) planning benchmark, consisting of a suite of
environments, that can be used to evaluate the planning capabilities of LLMs in
a principled fashion. The core idea behind LexiCon is to take existing planning
environments and impose temporal constraints on the states. These constrained
problems are then translated into natural language and given to an LLM to
solve. A key feature of LexiCon is its extensibility. That is, the set of
supported environments can be extended with new (unconstrained) environment
generators, for which temporal constraints are constructed automatically. This
renders LexiCon future-proof: the hardness of the generated planning problems
can be increased as the planning capabilities of LLMs improve. Our experiments
reveal that the performance of state-of-the-art LLMs, including reasoning
models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of
the planning tasks increases.

</details>


### [152] [Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments](https://arxiv.org/abs/2510.06001)
*Timothy Pistotti,Jason Brown,Michael Witbrock*

Main category: cs.CL

TL;DR: LLM在评估复杂句法可学性方面存在分歧，本文认为最小对比较方法比DiD方法更具诊断透明度，并通过实验证明GPT-2在寄生缺口（PG）环境中也具备稳健的填隙-缺口依赖知识。


<details>
  <summary>Details</summary>
Motivation: 探究语言习得中的“刺激贫乏论”问题，评估大型语言模型（LLM）在学习复杂句法方面的能力，并解决现有研究中因评估指标不同而产生的结论分歧。

Method: 生成完整的8种排列组合的寄生缺口（PG）刺激材料，并使用Wilcox风格的wh-effect分析方法（直接最小对比较）来评估GPT-2模型在填隙-缺口依赖方面的表现。

Result: GPT-2模型在所有四种测试条件下均成功，表明其在复杂的PG环境中也具备对填隙-缺口约束原则的稳健理解，这与使用DiD等指标得到的结果形成对比。

Conclusion: 直接最小对比较方法比DiD等指标更能提供诊断透明度，对评估LLM的句法能力至关重要，并且GPT-2模型实际上掌握了PG环境下的填隙-缺口依赖知识。

Abstract: Recent studies probing the Argument from the Poverty of the Stimulus (APS)
have applied Large Language Models (LLMs) to test the learnability of complex
syntax through surprisal-based metrics. However, divergent conclusions raise
questions concerning the insights these metrics offer. While Wilcox et al.
(2024) used direct minimal pair comparisons (the "wh-effect") to demonstrate
that models successfully generalise knowledge of filler-gap dependencies, Lan
et al. (2024) used a Difference-in-Differences (DiD) metric and found that
models largely fail on parasitic gaps (PGs). This paper argues that the direct
minimal pair approach offers greater diagnostic transparency. We demonstrate
this by generating a full 8-permutation paradigm of refined PG stimuli and
evaluating the GPT-2 model used in previous studies with a systematic
Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across
all four tested conditions, indicating robust knowledge of filler-gap licensing
principles even in complex PG environments. This finding, which contrasts with
the more ambiguous results from DiD-style metrics, suggests that the choice of
evaluation metric is critical for assessing an LLM's syntactic competence.

</details>


### [153] [MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation](https://arxiv.org/abs/2510.06005)
*Qin Dong,Yuntian Tang,Heming Jia,Yunhang Shen,Bohan Jia,Wenxuan Huang,Lianyue Zhang,Jiao Xie,Shaohui Lin*

Main category: cs.CL

TL;DR: MASA通过引入多A单B结构来解决LoRA的表示瓶颈问题，在多领域泛化、单领域专业化和多任务推理等任务上表现优于LoRA。


<details>
  <summary>Details</summary>
Motivation: LoRA依赖单一的下行投影矩阵A，这限制了其捕捉复杂任务所需多样化信号的能力，从而导致表示瓶颈。

Method: MASA（Multi-A Shared Adaptation）采用多A单B结构，其中多A专家集合被不对称地共享在不同层之间，以保持参数效率。这些专家捕捉不同的特征，然后由单一的、特定于层的B矩阵进行整合。

Result: 在MMLU基准测试中，MASA实现了59.62%的平均准确率，比标准LoRA高出1.08个百分点（相对提高了1.84%），而可学习参数量相当（0.52%）。

Conclusion: MASA通过其创新的多A单B架构，有效解决了LoRA的表示瓶颈问题，并在多项下游任务中展现出优越的性能和效率。

Abstract: Low-Rank Adaptation (LoRA) has emerged as a dominant method in
Parameter-Efficient Fine-Tuning (PEFT) for large language models, which
augments the transformer layer with one down-projection $A$ and one
up-projection $B$. However, LoRA's reliance on a single down-projection matrix
($A$) creates a representational bottleneck, as this solitary feature extractor
is inherently insufficient for capturing the diverse signals required by
complex tasks. This motivates our architectural shift to focus on enriching the
feature adaptation to improve the downstream task adaptation ability. We
propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a
multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is
asymmetrically shared across layers to ensure parameter efficiency. In MASA,
these specialized experts capture diverse features, which are then integrated
by a single, layer-specific $B$-matrix. The effectiveness and versatility of
our method are validated through a comprehensive suite of experiments spanning
multi-domain generalization, single-domain specialization, and multi-task
reasoning. For example, on the MMLU benchmark, MASA achieves an average
accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative
improvement of 1.84%) with comparable learnable parameters of 0.52%.

</details>


### [154] [Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance](https://arxiv.org/abs/2510.06018)
*Timothy Pistotti,Jason Brown,Michael Witbrock*

Main category: cs.CL

TL;DR: LLM在语言学中的表现可能受到测试材料质量的影响。


<details>
  <summary>Details</summary>
Motivation: 评估大型语言模型（LLM）在语言“刺激贫乏论”方面的能力，并探究现有研究结果不一致的原因。

Method: 1. 建立一个基线，使用先前研究中用过的（经过筛选和未经过筛选的）刺激材料来测试GPT-2。 2. 使用先进的生成式LLM（Gemini 2.5 Pro Preview）和语言学模板生成新的、经过改进的数据集，以解决词汇歧义和结构复杂性等混淆因素。 3. 重新评估GPT-2在改进后的刺激材料上的表现。

Result: GPT-2在经过改进的刺激材料上的表现明显优于基线测试，表明刺激材料的质量显著影响了基于“意外性”评估LLM句法能力的评估结果。

Conclusion: 刺激材料的质量对评估LLM句法能力的结果有重要影响。

Abstract: Recent studies employing Large Language Models (LLMs) to test the Argument
from the Poverty of the Stimulus (APS) have yielded contrasting results across
syntactic phenomena. This paper investigates the hypothesis that
characteristics of the stimuli used in recent studies, including lexical
ambiguities and structural complexities, may confound model performance. A
methodology is proposed for re-evaluating LLM competence on syntactic
prediction, focusing on GPT-2. This involves: 1) establishing a baseline on
previously used (both filtered and unfiltered) stimuli, and 2) generating a
new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5
Pro Preview) guided by linguistically-informed templates designed to mitigate
identified confounds. Our preliminary findings indicate that GPT-2 demonstrates
notably improved performance on these refined PG stimuli compared to baselines,
suggesting that stimulus quality significantly influences outcomes in
surprisal-based evaluations of LLM syntactic competency.

</details>


### [155] [CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs](https://arxiv.org/abs/2510.06039)
*Chengwei Wu,Jiapu Wang,Mingyang Gao,Xingrui Zhuo,Jipeng Guo,Runlin Lei,Haoran Luo,Tianyu Chen,Haoyi Zhou,Shirui Pan,Zechao Li*

Main category: cs.CL

TL;DR: 该论文提出了一个名为CB-ECLLM的中文大模型评测基准，并构建了CDTP数据集来解决中文大模型面临的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有中文大模型评测方法存在不足，主要因为中文语料缺乏结构化表示，现有的英文为主的基准无法全面评估中文大模型的性能，尤其是在知识驱动的任务上。

Method: 构建了一个包含超过700万对齐文本对（文本-三元组）和1500万三元组的CDTP数据集，并基于此数据集设计了CB-ECLLM评测基准，涵盖知识图谱补全、三元组到文本生成和问答等任务，同时进行了广泛的实验和消融研究。

Result: 通过实验验证了CB-ECLLM的有效性、SFT性能和鲁棒性，并开源了代码。

Conclusion: 提出的CB-ECLLM基准和CDTP数据集能够为中文大模型提供更精细化的评估，并促进相关研究的进行。

Abstract: Large Language Models (LLMs) have achieved remarkable success across a wide
range of natural language processing tasks. However, Chinese LLMs face unique
challenges, primarily due to the dominance of unstructured free text and the
lack of structured representations in Chinese corpora. While existing
benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly
English-centric and fail to address the unique linguistic characteristics of
Chinese, lacking structured datasets essential for robust evaluation. To
address these challenges, we present a Comprehensive Benchmark for Evaluating
Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese
Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million
aligned text pairs, each consisting of unstructured text coupled with one or
more corresponding triples, alongside a total of 15 million triples spanning
four critical domains. The core contributions of CDTP are threefold: (i)
enriching Chinese corpora with high-quality structured information; (ii)
enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii)
supporting multi-task fine-tuning to assess generalization and robustness
across scenarios, including Knowledge Graph Completion, Triple-to-Text
generation, and Question Answering. Furthermore, we conduct rigorous
evaluations through extensive experiments and ablation studies to assess the
effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark.
To support reproducible research, we offer an open-source codebase and outline
potential directions for future investigations based on our insights.

</details>


### [156] [ASPO: Asymmetric Importance Sampling Policy Optimization](https://arxiv.org/abs/2510.06062)
*Jiakang Wang,Runze Liu,Lei Lin,Wenping Hu,Xiu Li,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.CL

TL;DR: LLM RL中的IS比率失衡问题，提出ASPO解决，提升训练稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM RL方法依赖于token级别的裁剪机制，但OSRL范式存在IS比率不匹配的问题，导致token权重不平衡，抑制低概率token的更新并过度放大高概率token。

Method: 提出ASPO，通过翻转正向优势token的IS比率来解决不匹配问题，并引入软双裁剪机制来稳定更新。

Result: ASPO显著缓解了过早收敛，提高了训练稳定性，并在编码和数学推理基准上提升了最终性能。

Conclusion: ASPO修正了OSRL中的IS比率问题，为LLM RL中的token级别加权提供了新的见解。

Abstract: Recent Large Language Model (LLM) post-training methods rely on token-level
clipping mechanisms during Reinforcement Learning (RL). However, we identify a
fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance
Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to
unbalanced token weighting for positive and negative tokens. This mismatch
suppresses the update of low-probability tokens while over-amplifying already
high-probability ones. To address this, we propose Asymmetric Importance
Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy
that flips the IS ratios of positive-advantage tokens, aligning their update
direction with the learning dynamics of negative ones. AIS further incorporates
a soft dual-clipping mechanism to stabilize extreme updates while maintaining
gradient flow. Comprehensive experiments on coding and mathematical reasoning
benchmarks demonstrate that ASPO significantly mitigates premature convergence,
improves training stability, and enhances final performance over strong
GRPO-based baselines. Our analysis provides new insights into the role of
token-level weighting in OSRL and highlights the critical importance of
correcting IS in LLM RL. The code and models of ASPO are available at
https://github.com/wizard-III/Archer2.0.

</details>


### [157] [Spectrum Tuning: Post-Training for Distributional Coverage and In-Context Steerability](https://arxiv.org/abs/2510.06084)
*Taylor Sorensen,Benjamin Newman,Jared Moore,Chan Park,Jillian Fisher,Niloofar Mireshghallah,Liwei Jiang,Yejin Choi*

Main category: cs.CL

TL;DR: 语言模型在指令遵循和下游任务方面表现优异，但在存在多个有效答案的任务上会付出代价。研究人员提出了三个条件分布建模的理想标准：语境可控性、有效输出空间覆盖率和分布对齐。他们发现，当前的训练后技术会损害模型的语境可控性。为了解决这个问题，他们引入了 Spectrum Suite 资源和 Spectrum Tuning 方法，以提高模型的语境可控性和分布覆盖率。


<details>
  <summary>Details</summary>
Motivation: 语言模型在指令遵循和下游任务方面表现优异，但同时也损害了在存在多个有效答案的任务上的表现。这项研究旨在解决这个问题，并提出改进方法。

Method: 研究人员提出了三个条件分布建模的理想标准：语境可控性、有效输出空间覆盖率和分布对齐。他们通过分析三种模型家族来评估当前训练后技术对这些标准的影响。为了更好地评估和改进这些标准，他们引入了 Spectrum Suite 资源。最后，他们提出了一种名为 Spectrum Tuning 的训练后方法。

Result: 研究发现，当前的训练后技术在提高模型潜在能力和知识方面有所帮助，但在灵活的语境控制方面却有所损害。Spectrum Tuning 方法在语境可控性、输出空间覆盖率和分布对齐方面优于预训练模型及其指令调优版本。

Conclusion: Spectrum Tuning 是一种有效的训练后方法，可以提高语言模型在存在多个有效答案的任务上的语境可控性、输出空间覆盖率和分布对齐能力。

Abstract: Language model post-training has enhanced instruction-following and
performance on many downstream tasks, but also comes with an often-overlooked
cost on tasks with many possible valid answers. We characterize three
desiderata for conditional distributional modeling: in-context steerability,
valid output space coverage, and distributional alignment, and document across
three model families how current post-training can reduce these properties. In
particular, we disambiguate between two kinds of in-context learning: ICL for
eliciting existing underlying knowledge or capabilities, and in-context
steerability, where a model must use in-context information to override its
priors and steer to a novel data generating distribution. To better evaluate
and improve these desiderata, we introduce Spectrum Suite, a large-scale
resource compiled from >40 data sources and spanning >90 tasks requiring models
to steer to and match diverse distributions ranging from varied human
preferences to numerical distributions and more. We find that while current
post-training techniques help elicit underlying capabilities and knowledge,
they hurt models' ability to flexibly steer in-context. To mitigate these
issues, we propose Spectrum Tuning, a post-training method using Spectrum Suite
to improve steerability and distributional coverage. We find that Spectrum
Tuning often improves over pretrained models and their instruction-tuned
counterparts, enhancing steerability, spanning more of the output space, and
improving distributional alignment on held-out datasets.

</details>


### [158] [The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models](https://arxiv.org/abs/2510.06101)
*Muyu He,Muhammad Ali Shafique,Anand Kumar,Tsach Mackey,Nazneen Rajani*

Main category: cs.CL

TL;DR: 在神经网络的训练中，数据量和模型性能之间的关系通常被认为是单调递增的，但本文的研究表明，在代码推理蒸馏的特定场景下，这种关系并非如此简单。研究发现在增加蒸馏数据量的初期，模型性能反而会下降，随后才开始显著提升，这被称为“代码推理的 घाटी ”。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索不同数量的蒸馏数据对小型语言模型（LLM）在竞争性编程任务上性能的影响，特别是验证是否存在一个“代码推理的 घाटी ”现象，即性能随数据量增加而先下降后上升。

Method: 研究者在两个小型非推理LLM上进行了实验，通过改变蒸馏数据的数量来研究模型性能的变化趋势。他们还进一步在蒸馏过程的不同阶段对模型进行了微调，以深入了解其学习阶段。

Result: 研究发现，随着蒸馏数据量的增加，小型模型的竞争性编程能力呈现出先下降后急剧上升（上升速度超过对数线性）的趋势。此外，研究还指出，在低数据量和中低数据量的情况下，模型从较简单的编程问题中获益更多，而训练数据的正确性对蒸馏结果没有影响。

Conclusion: 本文的研究揭示了代码推理蒸馏过程中数据量与模型性能之间复杂的非单调关系，挑战了传统认知。研究结果表明，在某些情况下，增加数据量可能不会立即带来性能提升，甚至会暂时导致性能下降。此外，模型在不同学习阶段对数据难易程度的偏好以及训练数据正确性的不敏感性，都为理解和优化代码推理蒸馏的训练动态提供了新的见解。

Abstract: Distilling the thinking traces of a Large Language Model (LLM) with reasoning
capabilities into a smaller model has been proven effective. Yet, there is a
scarcity of work done on how model performances scale with the quantity of
distillation data. In this work, we study the scaling trend of distilling
competitive coding skills on two small non-reasoning LLMs. We validate the
hypothesis that there is a $\textit{valley of code reasoning}$: downstream
performance on competitive coding first drops as data quantity increases, then
it steadily increases in a sharper-than-log-linear fashion. Having identified
the trend, we further fine-tune the models at two different distillation stages
on the same data to ground conclusions on their respective learning phases. We
learn that across stages in the low and medium-low data regimes, small models
benefit significantly from easier coding questions than from harder ones. We
also find that, surprisingly, the correctness of outputs in training data makes
no difference to distillation outcomes. Our work represents a step forward in
understanding the training dynamics of code reasoning distillation outside
intuition

</details>


### [159] [Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models](https://arxiv.org/abs/2510.06107)
*Gagan Bhatia,Somayajulu G Sripada,Kevin Allan,Jacobo Azcona*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large Language Models (LLMs) are prone to hallucination, the generation of
plausible yet factually incorrect statements. This work investigates the
intrinsic, architectural origins of this failure mode through three primary
contributions.First, to enable the reliable tracing of internal semantic
failures, we propose \textbf{Distributional Semantics Tracing (DST)}, a unified
framework that integrates established interpretability techniques to produce a
causal map of a model's reasoning, treating meaning as a function of context
(distributional semantics). Second, we pinpoint the model's layer at which a
hallucination becomes inevitable, identifying a specific \textbf{commitment
layer} where a model's internal representations irreversibly diverge from
factuality. Third, we identify the underlying mechanism for these failures. We
observe a conflict between distinct computational pathways, which we interpret
using the lens of dual-process theory: a fast, heuristic \textbf{associative
pathway} (akin to System 1) and a slow, deliberate \textbf{contextual pathway}
(akin to System 2), leading to predictable failure modes such as
\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the
coherence of the contextual pathway reveals a strong negative correlation
($\rho = -0.863$) with hallucination rates, implying that these failures are
predictable consequences of internal semantic weakness. The result is a
mechanistic account of how, when, and why hallucinations occur within the
Transformer architecture.

</details>


### [160] [Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer](https://arxiv.org/abs/2510.06128)
*Muhammad Dehan Al Kautsar,Fajri Koto*

Main category: cs.CL

TL;DR: 现有分词方法未能有效支持跨语言迁移，因为语义等价的词被分配了不同的嵌入。我们提出了并行分词器，通过双语词典或词对词翻译对齐词汇表，确保语义等价词的索引一致，从而实现跨语言共享表示。在13种低资源语言上的实验表明，并行分词器在各种下游任务上优于传统多语言基线。


<details>
  <summary>Details</summary>
Motivation: 现有的分词方法在支持跨语言迁移方面存在不足，主要原因是语义等价的词语在不同语言中会被映射到不同的词汇索引，阻碍了共享表示的学习，并限制了跨语言泛化能力，尤其是在低资源场景下。

Method: 提出了一种名为“并行分词器”的新框架。该框架首先在单语环境下训练分词器，然后利用双语词典或词对词翻译技术，对不同语言的分词器词汇表进行详尽的对齐。这种对齐方式确保了语义上等价的词语在不同语言中具有相同的索引，从而强制构建跨语言共享的语义空间，并自然地改善了词语生成（fertility）的平衡性。

Result: 在13种低资源语言上预训练的 Transformer 编码器模型，在使用并行分词器后，在情感分析、仇恨言论检测、情感分类和句子嵌入相似性等任务上的表现，均优于使用传统多语言分词方法的基线模型。

Conclusion: 通过重新思考分词方法对于推动多语言表示学习至关重要，尤其是在低资源语言的应用场景中，并行分词器提供了一种有效的方法来提升跨语言迁移能力。

Abstract: Tokenization defines the foundation of multilingual language models by
determining how words are represented and shared across languages. However,
existing methods often fail to support effective cross-lingual transfer because
semantically equivalent words are assigned distinct embeddings. For example, "I
eat rice" in English and "Ina cin shinkafa" in Hausa are typically mapped to
different vocabulary indices, preventing shared representations and limiting
cross-lingual generalization. We introduce parallel tokenizers. This new
framework trains tokenizers monolingually and then aligns their vocabularies
exhaustively using bilingual dictionaries or word-to-word translation, ensuring
consistent indices for semantically equivalent words. This alignment enforces a
shared semantic space across languages while naturally improving fertility
balance. To assess their effectiveness, we pretrain a transformer encoder from
scratch on thirteen low-resource languages and evaluate it on sentiment
analysis, hate speech detection, emotion classification, and sentence embedding
similarity. Across all tasks, models trained with parallel tokenizers
outperform conventional multilingual baselines, confirming that rethinking
tokenization is essential for advancing multilingual representation
learning--especially in low-resource settings.

</details>


### [161] [CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits](https://arxiv.org/abs/2510.06133)
*Kangyu Wang,Zhiyun Jiang,Haibo Feng,Weijia Zhao,Lin Liu,Jianguo Li,Zhenzhong Lan,Weiyao Lin*

Main category: cs.CL

TL;DR: dLLM 的一种名为 CreditDecoding 的新解码算法通过利用历史预测信息来加速文本生成并提高准确性，它在多个基准测试中均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有 dLLM 在解码过程中存在重复遮蔽 token 的问题，导致迭代冗余并限制了整体加速效果。这是由于模型在早期迭代中置信度得分较低，需要多次迭代才能确定最终预测。因此，需要一种新的解码算法来解决这个问题，提高解码效率和文本生成质量。

Method: 提出了一种名为 Trace Credit 的概念，通过累积历史 logit 来量化每个 token 的收敛潜力。在此基础上，开发了一种名为 CreditDecoding 的训练无关的并行解码算法。该算法通过融合当前 logit 和 Trace Credit 来加速正确但置信度偏低的 token 的收敛，从而减少冗余迭代并提高解码鲁棒性。

Result: CreditDecoding 在八个基准测试中，相比 LLaDA-8B-Instruct 实现了 5.48 倍的加速和 0.48 的性能提升，相比 LLaDA-MoE-Instruct 实现了 4.11 倍的加速和 0.15 的性能提升。此外，CreditDecoding 在长序列上表现出良好的可扩展性，并且不影响主流的推理优化方法，易于集成和使用。

Conclusion: CreditDecoding 算法通过引入 Trace Credit 概念和 CreditDecoding 解码算法，有效解决了 dLLM 解码过程中的冗余迭代问题，显著提高了文本生成速度和准确性。该算法具有良好的可扩展性和通用性，是一种有前景的 dLLM 推理优化方案。

Abstract: Diffusion large language models (dLLMs) generate text through iterative
denoising steps, achieving parallel decoding by denoising only high-confidence
positions at each step. However, existing approaches often repetitively remask
tokens due to initially low confidence scores, leading to redundant iterations
and limiting overall acceleration. Through the analysis of dLLM decoding
traces, we observe that the model often determines the final prediction for a
token several steps before the decoding step. To leverage this historical
information and avoid redundant steps, we introduce the concept of Trace
Credit, which quantifies each token's convergence potential by accumulating
historical logits. Furthermore, we propose CreditDecoding, a training-free
parallel decoding algorithm that accelerates the confidence convergence of
correct but underconfident tokens by fusing current logits with Trace Credit.
This process significantly reduces redundant iterations and enhances decoding
robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup
and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times
speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct.
Importantly, CreditDecoding scales effectively to long sequences and is
orthogonal to mainstream inference optimizations, making it a readily
integrable and versatile solution.

</details>


### [162] [RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets](https://arxiv.org/abs/2510.06143)
*Jan Cegin,Branislav Pecher,Ivan Srba,Jakub Simko*

Main category: cs.CL

TL;DR: LLM生成的合成数据可用于训练小型模型，尤其是在低资源语言中。然而，评估LLM生成器具有挑战性。RoSE是一种无需人工标注即可选择最佳LLM生成器的新指标。RoSE通过训练一个小型模型来评估候选生成器的输出来工作，并在其他生成器的合成数据上进行评估。RoSE在下游任务性能方面优于其他内在启发式方法，并且与人工测试数据的性能呈正相关。


<details>
  <summary>Details</summary>
Motivation: 评估LLM作为合成数据生成器对下游任务的有用性具有挑战性，尤其是在低资源语言中，因为昂贵的人工标注通常不可用，而内在指标与下游性能的相关性很差。

Method: RoSE（Round robin Synthetic data Evaluation）通过训练一个小型模型来评估一个候选生成器（LLM）的输出，然后使用所有其他候选LLM生成的合成示例来评估该小型模型。RoSE得分是该小型模型的平均性能。

Result: 在六个LLM、十一种语言和三种任务（情感、主题、意图）的实验中，RoSE比其他内在启发式方法更频繁地识别出最佳生成器，并且在下游性能方面接近最佳生成器基线（仅相差0.76个百分点）。RoSE是唯一与人工测试数据性能呈正相关的指标。

Conclusion: RoSE是一种有效的代理指标，可以在没有人工测试集的情况下选择最佳的LLM合成数据生成器，并且在识别下游性能方面优于现有的内在启发式方法。

Abstract: LLMs are powerful generators of synthetic data, which are used for training
smaller, specific models. This is especially valuable for low-resource
languages, where human-labelled data is scarce but LLMs can still produce
high-quality text. However, LLMs differ in how useful their outputs are for
training. Selecting the best LLM as a generator is challenging because
extrinsic evaluation requires costly human annotations (which are often
unavailable for low-resource languages), while intrinsic metrics correlate
poorly with downstream performance. We introduce Round robin Synthetic data
Evaluation (RoSE), a proxy metric for selecting the best LLM generator without
human test sets. RoSE trains a small model on the outputs of a candidate
generator (LLM) and then evaluates it on generated synthetic examples from all
other candidate LLMs. The final RoSE score is the mean performance of this
small model. Across six LLMs, eleven languages, and three tasks (sentiment,
topic, intent), RoSE identifies the optimal generator more often than any other
intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within
0.76 percentage points of the optimal generator baseline. This result is
measured in terms of downstream performance, obtained by training a small model
on the chosen generator's outputs (optimal vs. proxy metric selected) and
evaluating it on human-labelled test data. Additionally, RoSE is the only
metric to achieve a positive correlation with performance on human test data.

</details>


### [163] [VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization](https://arxiv.org/abs/2510.06175)
*Dingyu Yao,Chenxu Yang,Zhengyang Tong,Zheng Lin,Wei Liu,Jian Luan,Weiping Wang*

Main category: cs.CL

TL;DR: VecInfer通过平滑和Hadamard变换来压缩KV缓存，减少内存占用并提高LLM推理速度，在2位量化下性能接近全精度。


<details>
  <summary>Details</summary>
Motivation: 现有的向量量化（VQ）方法在压缩KV缓存时，在极低比特宽度下会因键缓存异常值而导致性能下降。

Method: 提出VecInfer，一种新的VQ方法，通过应用平滑和Hadamard变换来抑制键缓存异常值，并设计了一个优化的CUDA核来融合计算和反量化。

Result: VecInfer在长上下文理解和数学推理任务上始终优于现有量化基线。在2位量化下，VecInfer实现了与全精度相当的性能，并在Llama-3.1-8B模型上实现了高达2.7倍的批处理自注意力计算加速和8.3倍的单批处理端到端延迟减少。

Conclusion: VecInfer能够有效地进行KV缓存压缩，同时实现高效推理，解决了现有方法在超低比特量化下的性能瓶颈。

Abstract: The Key-Value (KV) cache introduces substantial memory overhead during large
language model (LLM) inference. Although existing vector quantization (VQ)
methods reduce KV cache usage and provide flexible representational capacity
across bit-widths, they suffer severe performance degradation at ultra-low
bit-widths due to key cache outliers that hinder effective codebook
utilization. To address this challenge, we propose VecInfer, a novel VQ method
for aggressive KV cache compression while enabling efficient inference. By
applying smooth and Hadamard transformations, VecInfer suppresses outliers in
the key cache, enabling the codebook to comprehensively cover the original data
distribution and thereby reducing quantization difficulty. To facilitate
efficient deployment, we design an optimized CUDA kernel that fuses computation
with dequantization to minimize memory access overhead. Extensive evaluations
demonstrate that VecInfer consistently outperforms existing quantization
baselines across both long-context understanding and mathematical reasoning
tasks. With only 2-bit quantization, VecInfer achieves performance comparable
to full precision, while delivering up to $\mathbf{2.7\times}$ speedup in
large-batch self-attention computation and $\mathbf{8.3\times}$ reduction in
single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.

</details>


### [164] [Mixing Mechanisms: How Language Models Retrieve Bound Entities In-Context](https://arxiv.org/abs/2510.06182)
*Yoav Gur-Arieh,Mor Geva,Atticus Geiger*

Main category: cs.CL

TL;DR: 大型语言模型（LMs）在上下文推理中通过位置、词汇和反射机制来绑定和检索实体，并且这些机制的组合可以准确地预测下一个词元。


<details>
  <summary>Details</summary>
Motivation: 先前研究发现LMs在上下文推理中主要依赖位置机制来检索实体，但该机制在实体数量增多时会变得不可靠。本研究旨在探讨LMs如何在新设置下补偿这种不可靠性，并提出一个更完整的实体绑定和检索模型。

Method: 通过大量实验，研究了九种模型和十种绑定任务，分析了LMs如何结合使用位置、词汇和反射机制。基于实验结果，构建了一个结合这三种机制的因果模型，并评估了其预测下一个词元的能力。最后，在更长的开放式文本和实体组混合的输入上验证了模型的泛化能力。

Result: LMs在处理更多绑定的实体时，除了原有的位置机制外，还会引入词汇机制和反射机制。这三种机制的组合模型能够以95%的一致性准确估计下一个词元分布，并且在更长的、混合了实体组的开放式文本输入上表现出良好的泛化能力。

Conclusion: LMs在上下文推理中并非只依赖单一的位置机制，而是会根据实体数量的增加，灵活地组合使用位置、词汇和反射三种机制来检索实体。本研究提出的结合三种机制的因果模型能够准确地模拟LMs的行为，并为理解LMs的上下文推理能力提供了更全面的视角。

Abstract: A key component of in-context reasoning is the ability of language models
(LMs) to bind entities for later retrieval. For example, an LM might represent
"Ann loves pie" by binding "Ann" to "pie", allowing it to later retrieve "Ann"
when asked "Who loves pie?" Prior research on short lists of bound entities
found strong evidence that LMs implement such retrieval via a positional
mechanism, where "Ann" is retrieved based on its position in context. In this
work, we find that this mechanism generalizes poorly to more complex settings;
as the number of bound entities in context increases, the positional mechanism
becomes noisy and unreliable in middle positions. To compensate for this, we
find that LMs supplement the positional mechanism with a lexical mechanism
(retrieving "Ann" using its bound counterpart "pie") and a reflexive mechanism
(retrieving "Ann" through a direct pointer). Through extensive experiments on
nine models and ten binding tasks, we uncover a consistent pattern in how LMs
mix these mechanisms to drive model behavior. We leverage these insights to
develop a causal model combining all three mechanisms that estimates next token
distributions with 95% agreement. Finally, we show that our model generalizes
to substantially longer inputs of open-ended text interleaved with entity
groups, further demonstrating the robustness of our findings in more natural
settings. Overall, our study establishes a more complete picture of how LMs
bind and retrieve entities in-context.

</details>


### [165] [RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback](https://arxiv.org/abs/2510.06186)
*Chunyu Miao,Henry Peng Zou,Yangning Li,Yankai Chen,Yibo Wang,Fangxin Wang,Yifan Li,Wooseong Yang,Bowei He,Xinni Zhang,Dianzhi Yu,Hanchen Yang,Hoang H Nguyen,Yue Zhou,Jie Yang,Jizhou Guo,Wenzhe Fan,Chin-Yuan Yeh,Panpan Meng,Liancheng Fang,Jinhu Qi,Wei-Chieh Huang,Zhengyao Gu,Yuwei Han,Langzhou He,Yuyao Yang,Xue Liu,Irwin King,Philip S. Yu*

Main category: cs.CL

TL;DR: RECODE-H 是一个包含 102 个来自研究论文和代码库任务的基准，通过与 LLM 模拟的人类反馈进行多轮交互来评估 LLM 代理。它包括结构化指令、单元测试和五级反馈层次结构，以反映研究人员与代理的现实协作。ReCodeAgent 是一个将反馈集成到迭代代码生成中的框架。通过在领先的 LLM 上进行实验，表明更丰富的反馈可带来显著的性能提升，同时也突显了在生成复杂研究代码方面仍然存在的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的 LLM 在生成正确和可执行的代码方面能力有限，并且忽略了科学研究开发工作流的迭代和反馈驱动的性质。RECODE-H 旨在解决这一差距。

Method: 构建了一个包含 102 个任务的 RECODE-H 基准，涵盖了结构化指令、单元测试和五级反馈层次结构，并提出了 ReCodeAgent 框架，将反馈集成到迭代代码生成中。

Result: 在 GPT-5、Claude-Sonnet-4、DeepSeek-V3.1 和 Gemini 2.5 等领先 LLM 上进行的实验表明，更丰富的反馈可带来显著的性能提升，但也凸显了生成复杂研究代码方面的挑战。

Conclusion: RECODE-H 为开发科学研究实施中自适应、反馈驱动的 LLM 代理奠定了基础。

Abstract: Large language models (LLMs) show the promise in supporting scientific
research implementation, yet their ability to generate correct and executable
code remains limited. Existing works largely adopt one-shot settings, ignoring
the iterative and feedback-driven nature of realistic workflows of scientific
research development. To address this gap, we present RECODE-H, a benchmark of
102 tasks from research papers and repositories that evaluates LLM agents
through multi-turn interactions with LLM-simulated human feedback. It includes
structured instructions,unit tests, and a five-level feedback hierarchy to
reflect realistic researcher-agent collaboration. We further present
ReCodeAgent, a framework that integrates feedback into iterative code
generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4,
DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer
feedback, while also highlighting ongoing challenges in the generation of
complex research code. RECODE-H establishes a foundation for developing
adaptive, feedback-driven LLM agents in scientific research implementation

</details>


### [166] [BanglaTalk: Towards Real-Time Speech Assistance for Bengali Regional Dialects](https://arxiv.org/abs/2510.06188)
*Jakir Hasan,Shubhashis Roy Dipta*

Main category: cs.CL

TL;DR: 该研究提出了BanglaTalk，一个支持孟加拉语方言的实时语音助手系统。


<details>
  <summary>Details</summary>
Motivation: 现有语音助手系统在孟加拉语，特别是其丰富的方言方面，进展有限，且未针对实时应用进行优化。

Method: BanglaTalk采用客户端-服务器架构，利用RTP协议实现低延迟通信。通过微调IndicWav2Vec模型，开发了一个方言感知的自动语音识别系统BRDialect，该系统可处理十种孟加拉语方言。

Result: BRDialect在RegSpeech12数据集上的表现比基线模型好12.41-33.98%。BanglaTalk能在24 kbps的低带宽下运行，平均端到端延迟为4.9秒。

Conclusion: BanglaTalk凭借其低带宽消耗和低延迟，成本效益高且交互性强，为孟加拉语社区提供了包容性和可访问的语音技术。

Abstract: Real-time speech assistants are becoming increasingly popular for ensuring
improved accessibility to information. Bengali, being a low-resource language
with a high regional dialectal diversity, has seen limited progress in
developing such systems. Existing systems are not optimized for real-time use
and focus only on standard Bengali. In this work, we present BanglaTalk, the
first real-time speech assistance system for Bengali regional dialects.
BanglaTalk follows the client-server architecture and uses the Real-time
Transport Protocol (RTP) to ensure low-latency communication. To address
dialectal variation, we introduce a dialect-aware ASR system, BRDialect,
developed by fine-tuning the IndicWav2Vec model in ten Bengali regional
dialects. It outperforms the baseline ASR models by 12.41-33.98% on the
RegSpeech12 dataset. Furthermore, BanglaTalk can operate at a low bandwidth of
24 kbps while maintaining an average end-to-end delay of 4.9 seconds. Low
bandwidth usage and minimal end-to-end delay make the system both
cost-effective and interactive for real-time use cases, enabling inclusive and
accessible speech technology for the diverse community of Bengali speakers.

</details>


### [167] [Latent Speech-Text Transformer](https://arxiv.org/abs/2510.06195)
*Yen-Ju Lu,Yashesh Gaur,Wei Zhou,Benjamin Muller,Jesus Villalba,Najim Dehak,Luke Zettlemoyer,Gargi Ghosh,Mike Lewis,Srinivasan Iyer,Duc Le*

Main category: cs.CL

TL;DR: 通过将语音令牌聚合成潜在语音块来提高语音-文本模型的预训练效率和性能。


<details>
  <summary>Details</summary>
Motivation: 现有的自回归语音-文本模型由于语音序列比文本序列长得多，存在计算不平衡和对齐效率低下的问题，导致扩展定律的效率低下。

Method: 提出了一种潜在语音-文本Transformer（LST）模型，通过动态聚合语音令牌来创建潜在语音块。这些块可以与文本单元对齐，也可以封装常见的语音序列（如静音），从而提高数据和计算效率。

Result: LST在语音-语音和文本-文本基准测试中都优于传统方法。在HellaSwag故事补全任务中，LST在语音准确性方面取得了显著的提升（计算受控训练下提高6.5%，数据受控训练下提高5.3%），并同时提高了文本性能。

Conclusion: LST通过聚合语音令牌来解决现有模型中的计算不平衡和效率低下问题，从而实现更高效的预训练、更好的表示对齐以及更快的扩展定律。

Abstract: Auto-regressive speech-text models are typically pre-trained on a large
number of interleaved sequences of text tokens and raw speech encoded as speech
tokens using vector quantization. These models have demonstrated
state-of-the-art performance in speech-to-speech understanding and generation
benchmarks, together with promising scaling laws, primarily enabled by the
representational alignment between text and speech. Nevertheless, they suffer
from shortcomings, partly owing to the disproportionately longer sequences of
speech tokens in contrast to textual tokens. This results in a large compute
imbalance between modalities during pre-training as well as during inference,
and a potential hindrance to effectively aligning speech and text, ultimately
translating to several orders of magnitude slower scaling laws. We introduce
the Latent Speech-Text Transformer (LST), which makes pre-training speech-text
models more data-efficient by dynamically and inexpensively aggregating speech
tokens into latent speech patches. These patches serve as higher-level units
that can either align with corresponding textual units to aid capability
transfer or even encapsulate common speech sequences like silences to be more
compute-efficient. We show that LST outperforms vanilla approaches on
speech-to-speech as well as text-to-text benchmarks in both data- and
compute-controlled settings, the former indicating more effective
representational alignment and the latter indicating steeper scaling laws for
speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute
gain in speech accuracy under compute-controlled training and 5.3% under
data-controlled training, while also improving text performance. We will
release our models, code, and the evaluation data to facilitate further
research.

</details>


### [168] [Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction](https://arxiv.org/abs/2510.06198)
*Xinyu Guo,Zhengliang Shi,Minglai Yang,Mahdi Rahimi,Mihai Surdeanu*

Main category: cs.CL

TL;DR: CogRE框架通过认知科学启发的推理和强化学习优化，提升了关系抽取（RE）的准确性和可解释性，尤其在小样本RE任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统关系抽取（RE）方法缺乏对语言解释的监督，导致解释质量不高。本研究旨在提升RE的准确性和可解释性。

Method: 提出CogRE框架，包含两部分：1. 受认知科学启发的文本处理推理机制；2. 基于新颖奖励函数的强化学习（RL）优化过程。自动构建包含重要关系关键词的高质量词典。

Result: 在小样本RE任务上，CogRE框架使用Qwen2.5-15B-Instruct在One-shot NYT29数据集上达到24.65% F1，优于现有基于推理的设计。RL优化后性能提升+23.46%（绝对值）。人类评估显示，生成的关键词与黄金标签高度一致，解释质量提升54%（相对值）。

Conclusion: CogRE框架通过结合认知结构推理和RL优化，显著提高了小样本关系抽取任务的准确性和解释质量，解决了传统方法中注意力焦点不佳和学习能力有限的问题。

Abstract: This paper introduces a framework for relation extraction (RE) that enhances
both accuracy and explainability. The framework has two key components: (i) a
reasoning mechanism that formulates relation extraction as a series of
text-processing steps inspired by cognitive science, and (ii) an optimization
process driven by reinforcement learning (RL) with a novel reward function
designed to improve both task accuracy and explanation quality. We call our
approach CogRE. Our framework addresses the lack of supervision for
language-based explanations in traditional RE by promoting outputs that include
important relation keywords. These keywords are drawn from a high-quality
dictionary that is automatically constructed using an LLM. We evaluate our
approach for the task of one-shot RE using two LLMs and two RE datasets. Our
experiments show that CogRE improves explanation quality by addressing two
common failure patterns in one-shot RE: poor attention focus and limited
one-shot learning capability. For example, our cognitive-structured reasoning
with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing
prior reasoning-based designs. Optimizing this approach with RL using our
reward further improves performance by +23.46% (absolute). Finally, human
evaluation shows that our best model generates relational keywords closely
aligned with gold labels, increasing human explanation quality ratings by 54%
(relative).

</details>


<div id='cs.DS'></div>

# cs.DS [[Back]](#toc)

### [169] [Approximating Multiple-Depot Capacitated Vehicle Routing via LP Rounding](https://arxiv.org/abs/2510.05321)
*Zachary Friggstad,Tobias Mömke*

Main category: cs.DS

TL;DR: 该研究提出了一个针对多车场车辆路径问题（CVRP-MD）的3.9365近似算法，该算法基于新的线性规划松弛方法。


<details>
  <summary>Details</summary>
Motivation: 需要解决客户点集合C、车场点集合R、点之间的成本c(i,j)以及车辆容量k约束下的车辆路径问题，目标是最小化总成本。

Method: 提出了一种新的CVRP-MD的线性规划松弛方法，并基于该方法进行舍入得到近似解。

Result: 获得了一个3.9365的近似比。

Conclusion: 通过新的线性规划松弛方法，为CVRP-MD问题提供了一个近似算法。

Abstract: In Capacitated Vehicle Routing with Multiple Depots (CVRP-MD) we are given a
set of client locations $C$ and a set of depots $R$ located in a metric space
with costs $c(i,j)$ between $u,v \in C \cup R$. Additionally, we are given a
capacity bound $k$. The goal is to find a collection of tours of minimum total
cost such that each tour starts and ends at some depot $r \in R$ and includes
at most $k$ clients and such that each client lies on at least one tour. Our
main result is a $3.9365$-approximation based on rounding a new LP relaxation
for CVRP-MD.

</details>


### [170] [Time To Replace Your Filter: How Maplets Simplify System Design](https://arxiv.org/abs/2510.05518)
*Michael A. Bender,Alex Conway,Martín Farach-Colton,Rob Johnson,Prashant Pandey*

Main category: cs.DS

TL;DR: maplets是用于近似键值映射的空间高效数据结构，可替代过滤器，并在数据库、计算生物学和网络等领域提供更好的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的过滤器（如Bloom、商、布谷鸟过滤器）在空间效率方面表现出色，但无法直接支持键值关联，这导致了复杂的性能下降的解决方案。

Method: 提出了一种名为maplet的新型数据结构，它提供与过滤器相同的空间效率，并原生支持带有单侧错误保证的键值关联。通过在SplinterDB（LSM键值存储）、Squeakr（k-mer计数器）和Mantis（基因组序列搜索）中的应用案例研究，展示了maplet的通用性和优势。

Result: maplets在SplinterDB、Squeakr和Mantis中的应用表明，使用maplet可以简化设计并提高性能，尤其是在数据库、计算生物学和网络等领域。

Conclusion: maplets是比过滤器更优越的数据结构，适用于需要近似键值映射的各种应用，应成为这些领域的默认选择。

Abstract: Filters such as Bloom, quotient, and cuckoo filters are fundamental building
blocks providing space-efficient approximate set membership testing. However,
many applications need to associate small values with keys-functionality that
filters do not provide. This mismatch forces complex workarounds that degrade
performance. We argue that maplets-space-efficient data structures for
approximate key-value mappings-are the right abstraction. A maplet provides the
same space benefits as filters while natively supporting key-value associations
with one-sided error guarantees. Through detailed case studies of SplinterDB
(LSM-based key-value store), Squeakr (k-mer counter), and Mantis (genomic
sequence search), we identify the common patterns and demonstrate how a unified
maplet abstraction can lead to simpler designs and better performance. We
conclude that applications benefit from defaulting to maplets rather than
filters across domains including databases, computational biology, and
networking.

</details>


### [171] [Parameterized Complexity of Temporal Connected Components: Treewidth and k-Path Graphs](https://arxiv.org/abs/2510.05806)
*Argyrios Deligkas,Michelle Döring,Eduard Eiben,Tiger-Lily Goldsmith,George Skretas,Georg Tennigkeit*

Main category: cs.DS

TL;DR: 本文研究了时间图（随时间确定性变化的图）中最大时间连通分量（tccs）的参数化复杂性。研究了开放和闭合的最大时间连通分量问题，并考虑了图的树宽（tw）和时间图的时间路径数（tpn）这两个参数。结果表明，单独使用这两个参数不足以实现固定参数可解性，因为在tw=9时，这两个问题都是NP难的，而在tpn=6时，闭合tccs问题也是NP难的。然而，本文证明了开放tccs问题在tpn参数下属于XP类。此外，研究表明，当结合结构参数和时间参数（如tw+tpn、tw+图的生命周期、tw+最大时间度）时，这两个问题都可以实现固定参数可解性。


<details>
  <summary>Details</summary>
Motivation: 本文旨在研究最大时间连通分量（tccs）问题的参数化复杂性，特别是关注图的树宽（tw）和时间路径数（tpn）这两个参数，以探索在何种条件下该问题可以高效求解。

Method: 本文通过分析开放和闭合的最大时间连通分量（openTCC和closedTCC）问题，并结合图的树宽（tw）和时间路径数（tpn）这两个参数，来研究其参数化复杂性。具体来说，本文证明了单独使用tw或tpn不足以使问题固定参数可解，但证明了openTCC在tpn参数下属于XP类，并展示了tw与其他时间参数（如tpn、图的生命周期、最大时间度）的组合可以实现这两个问题的固定参数可解性。

Result: 本文证明了openTCC和closedTCC问题在tw=9时都是NP难的，closedTCC问题在tpn=6时也是NP难的。然而，openTCC问题在tpn参数下是XP类问题。此外，本文证明了当tw与tpn、图的生命周期或最大时间度等参数组合时，openTCC和closedTCC问题均可实现固定参数可解性。

Conclusion: 本文的研究表明，最大时间连通分量问题的参数化复杂性取决于所选参数的组合。虽然单独的树宽或时间路径数不足以保证问题的固定参数可解性，但结合这些参数或引入其他时间相关参数可以有效地降低问题的计算复杂度，为在特定条件下高效求解这些问题提供了理论基础。

Abstract: We study the parameterized complexity of maximum temporal connected
components (tccs) in temporal graphs, i.e., graphs that deterministically
change over time. In a tcc, any pair of vertices must be able to reach each
other via a time-respecting path. We consider both problems of maximum open
tccs (openTCC), which allow temporal paths through vertices outside the
component, and closed tccs (closedTCC) which require at least one temporal path
entirely within the component for every pair. We focus on the structural
parameter of treewidth, tw, and the recently introduced temporal parameter of
temporal path number, tpn, which is the minimum number of paths needed to fully
describe a temporal graph. We prove that these parameters on their own are not
sufficient for fixed parameter tractability: both openTCC and closedTCC are
NP-hard even when tw=9, and closedTCC is NP-hard when tpn=6. In contrast, we
prove that openTCC is in XP when parameterized by tpn. On the positive side, we
show that both problem become fixed parameter tractable under various
combinations of structural and temporal parameters that include, tw plus tpn,
tw plus the lifetime of the graph, and tw plus the maximum temporal degree.

</details>


### [172] [Improved Streaming Algorithm for Fair $k$-Center Clustering](https://arxiv.org/abs/2510.05937)
*Longkun Guo,Zeyu Lin,Chaoqi Jia,Chao Chen*

Main category: cs.DS

TL;DR: 该论文提出了一种用于处理带公平性约束的k中心聚类问题的流式算法，该算法能在单遍流中计算一个预留点集，并从中选择中心点，实现了5的近似比和O(k log n)的内存消耗。该算法也可应用于离线公平k中心问题，达到3的近似比，并能扩展到半结构化数据流。实验证明，该算法在聚类成本和运行时效率方面优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的k中心聚类问题常涉及公平性约束，尤其是在大数据和流式数据场景下，需要保证每个子群组的中心点数量不超过预设上限。

Method: 提出了一种基于λ-独立中心集的单遍流式算法，先计算预留点集，再通过分析三种情况选择中心点，并将最复杂的情况转化为辅助图中的顶点覆盖问题。

Result: 该流式算法实现了5的近似比和O(k log n)的内存消耗。将其应用于离线公平k中心问题，可获得3的近似比。对于半结构化数据流，对于m=2的情况，提出3-近似算法，对于一般m，提出4-近似算法。

Conclusion: 提出的流式和离线公平k中心聚类算法在理论上具有良好的近似比和效率，并通过实验验证了其优越性。

Abstract: Many real-world applications pose challenges in incorporating fairness
constraints into the $k$-center clustering problem, where the dataset consists
of $m$ demographic groups, each with a specified upper bound on the number of
centers to ensure fairness. Focusing on big data scenarios, this paper
addresses the problem in a streaming setting, where data points arrive one by
one sequentially in a continuous stream. Leveraging a structure called the
$\lambda$-independent center set, we propose a one-pass streaming algorithm
that first computes a reserved set of points during the streaming process.
Then, for the post-streaming process, we propose an approach for selecting
centers from the reserved point set by analyzing all three possible cases,
transforming the most complicated one into a specially constrained vertex cover
problem in an auxiliary graph. Our algorithm achieves a tight approximation
ratio of 5 while consuming $O(k\log n)$ memory. It can also be readily adapted
to solve the offline fair $k$-center problem, achieving a 3-approximation ratio
that matches the current state of the art. Furthermore, we extend our approach
to a semi-structured data stream, where data points from each group arrive in
batches. In this setting, we present a 3-approximation algorithm for $m = 2$
and a 4-approximation algorithm for general $m$. Lastly, we conduct extensive
experiments to evaluate the performance of our approaches, demonstrating that
they outperform existing baselines in both clustering cost and runtime
efficiency.

</details>


### [173] [Efficient Heuristics and Exact Methods for Pairwise Interaction Sampling](https://arxiv.org/abs/2510.05955)
*Sándor P. Fekete,Phillip Keldenich,Dominik Krupke,Michael Perk*

Main category: cs.DS

TL;DR: 该论文研究了在现代可配置软件系统（例如汽车行业）的测试中，一个基本的优化问题：成对交互采样。目标是在一个大的配置空间中找到一个最小的配置族，使得每对特征至少被测试一次。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是解决现代可配置软件系统测试中的一个基本优化问题，即成对交互采样，该问题在软件工程领域具有重要意义，并已得到广泛研究。

Method: 本文证明了该问题的BH-硬度，并提出了一系列实际贡献，以提高实际性能。

Result: 该方法能够解决现有方法无法处理的大规模实例（具有约5亿个可行的交互），并能保证最优性。

Conclusion: 该研究在理论和实践上都取得了显著进展，能够解决先前无法处理的大规模成对交互采样问题，并保证最优解。

Abstract: We consider a class of optimization problems that are fundamental to testing
in modern configurable software systems, e.g., in automotive industries. In
pairwise interaction sampling, we are given a (potentially very large)
configuration space, in which each dimension corresponds to a possible Boolean
feature of a software system; valid configurations are the satisfying
assignments of a given propositional formula $\varphi$. The objective is to
find a minimum-sized family of configurations, such that each pair of features
is jointly tested at least once. Due to its relevance in Software Engineering,
this problem has been studied extensively for over 20 years. In addition to new
theoretical insights (we prove BH-hardness), we provide a broad spectrum of key
contributions on the practical side that allow substantial progress for the
practical performance. Remarkably, we are able to solve the largest instances
we found in published benchmark sets (with about 500000000 feasible
interactions) to provable optimality. Previous approaches were not even able to
compute feasible solutions.

</details>


### [174] [Fast-Convergent Proximity Graphs for Approximate Nearest Neighbor Search](https://arxiv.org/abs/2510.05975)
*Binhong Li,Xiao Yan,Shangqi Lu*

Main category: cs.DS

TL;DR: PG-based ANN搜索在理论保证方面存在不足。本文提出的 α-CG 图结构通过边剪枝规则解决了这个问题，能够保证在多对数时间内找到精确或近似最近邻。其变体 α-CNG 在实际应用中表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于邻近图（PG）的近似最近邻（ANN）搜索方法在理论保证方面存在不足，尤其是在最坏情况下。

Method: 提出了一种新的邻近图结构 α-收敛图（α-CG），该结构采用精心设计的边剪枝规则，利用移位-缩放三角不等式来消除候选邻居。在此基础上，提出了其变体 α-收敛邻域图（α-CNG），并通过优化减少索引构建时间。

Result: α-CG 能够在多对数时间内找到精确的最近邻（如果查询点与精确最近邻的距离在某个常数 τ > 0 的范围内），否则找到一个近似最近邻。实验表明，α-CNG 在真实数据集上优于现有的 PG 方法，距离计算和搜索步骤减少了 15% 和 45%。

Conclusion: α-CG 和 α-CNG 为具有理论保证的高维 ANN 搜索提供了有效的方法，并且在实际应用中具有良好的可扩展性和性能。

Abstract: Approximate nearest neighbor (ANN) search in high-dimensional metric spaces
is a fundamental problem with many applications. Over the past decade,
proximity graph (PG)-based indexes have demonstrated superior empirical
performance over alternatives. However, these methods often lack theoretical
guarantees regarding the quality of query results, especially in the worst-case
scenarios. In this paper, we introduce the {\alpha}-convergent graph
({\alpha}-CG), a new PG structure that employs a carefully designed edge
pruning rule. This rule eliminates candidate neighbors for each data point p by
applying the shifted-scaled triangle inequalities among p, its existing
out-neighbors, and new candidates. If the distance between the query point q
and its exact nearest neighbor v* is at most {\tau} for some constant {\tau} >
0, our {\alpha}-CG finds the exact nearest neighbor in poly-logarithmic time,
assuming bounded intrinsic dimensionality for the dataset; otherwise, it can
find an ANN in the same time. To enhance scalability, we develop the
{\alpha}-convergent neighborhood graph ({\alpha}-CNG), a practical variant that
applies the pruning rule locally within each point's neighbors. We also
introduce optimizations to reduce the index construction time. Experimental
results show that our {\alpha}-CNG outperforms existing PGs on real-world
datasets. For most datasets, {\alpha}-CNG can reduce the number of distance
computations and search steps by over 15% and 45%, respectively, when compared
with the best-performing baseline.

</details>


### [175] [A Finer View of the Parameterized Landscape of Labeled Graph Contractions](https://arxiv.org/abs/2510.06102)
*Yashaswini Mathur,Prafullkumar Tale*

Main category: cs.DS

TL;DR: 该论文研究了标记收缩问题，提出了一个参数化复杂性的新算法，并对现有硬度结果进行了加强。


<details>
  <summary>Details</summary>
Motivation: 研究标记收缩问题的参数化复杂性，并在此基础上提出更优的算法和更强的硬度结果。

Method: 提出一个运行时间为 2^O(tw^2) * |V(G)|^O(1) 的构造性固定参数算法，并通过 ETH 证明了其下界。同时，研究了其他参数化以及改进了已有算法。

Result: 提出了一个构造性的固定参数算法，其运行时间为 2^O(tw^2) * |V(G)|^O(1)，并证明了其下界。此外，还加强了现有硬度结果，并对其他参数化进行了研究，提出了改进的算法。

Conclusion: 该研究为标记收缩问题提供了一个构造性的固定参数算法，并对其复杂性进行了深入分析，给出了精确的算法和下界。

Abstract: We study the \textsc{Labeled Contractibility} problem, where the input
consists of two vertex-labeled graphs $G$ and $H$, and the goal is to determine
whether $H$ can be obtained from $G$ via a sequence of edge contractions.
  Lafond and Marchand~[WADS 2025] initiated the parameterized complexity study
of this problem, showing it to be \(\W[1]\)-hard when parameterized by the
number \(k\) of allowed contractions. They also proved that the problem is
fixed-parameter tractable when parameterized by the tree-width \(\tw\) of
\(G\), via an application of Courcelle's theorem resulting in a
non-constructive algorithm.
  In this work, we present a constructive fixed-parameter algorithm for
\textsc{Labeled Contractibility} with running time \(2^{\mathcal{O}(\tw^2)}
\cdot |V(G)|^{\mathcal{O}(1)}\). We also prove that unless the Exponential Time
Hypothesis (\ETH) fails, it does not admit an algorithm running in time
\(2^{o(\tw^2)} \cdot |V(G)|^{\mathcal{O}(1)}\). This result adds
\textsc{Labeled Contractibility} to a small list of problems that admit such a
lower bound and matching algorithm.
  We further strengthen existing hardness results by showing that the problem
remains \NP-complete even when both input graphs have bounded maximum degree.
We also investigate parameterizations by \((k + \delta(G))\) where
\(\delta(G)\) denotes the degeneracy of \(G\), and rule out the existence of
subexponential-time algorithms. This answers question raised in Lafond and
Marchand~[WADS 2025]. We additionally provide an improved \FPT\ algorithm with
better dependence on \((k + \delta(G))\) than previously known. Finally, we
analyze a brute-force algorithm for \textsc{Labeled Contractibility} with
running time \(|V(H)|^{\mathcal{O}(|V(G)|)}\), and show that this running time
is optimal under \ETH.

</details>


### [176] [Local Search-based Individually Fair Clustering with Outliers](https://arxiv.org/abs/2510.06130)
*Binita Maity,Shrutimoy Das,Anirban Dasgupta*

Main category: cs.DS

TL;DR: 该论文提出了一种基于局部搜索的算法，用于处理含有异常值的数据集中的个体公平聚类问题。该算法能有效处理异常值，并为非异常点找到公平且最优的聚类中心。


<details>
  <summary>Details</summary>
Motivation: 现有模型在处理含有异常值的数据集时，可能无法为非异常点找到最优的公平聚类中心。本研究旨在解决此问题。

Method: 提出了一种新的算法，该算法能够识别并剔除异常点，然后对剩余的非异常点应用基于随机局部搜索的算法来寻找公平的聚类中心。该方法具有可扩展性，并提供了近似保证和异常点数量的上界。

Result: 实验结果表明，该方法在真实数据集上能够有效地处理异常值，并为非异常点找到公平且最优的聚类中心。

Conclusion: 本研究提出的基于局部搜索的算法能够有效解决含有异常值的数据集中的个体公平聚类问题，并在可扩展性和保证方面表现出色。

Abstract: In this paper, we present a local search-based algorithm for individually
fair clustering in the presence of outliers. We consider the individual
fairness definition proposed in Jung et al., which requires that each of the
$n$ points in the dataset must have one of the $k$ centers within its $n/k$
nearest neighbors. However, if the dataset is known to contain outliers, the
set of fair centers obtained under this definition might be suboptimal for
non-outlier points. In order to address this issue, we propose a method that
discards a set of points marked as outliers and computes the set of fair
centers for the remaining non-outlier points. Our method utilizes a randomized
variant of local search, which makes it scalable to large datasets. We also
provide an approximation guarantee of our method as well as a bound on the
number of outliers discarded. Additionally, we demonstrate our claims
experimentally on a set of real-world datasets.

</details>


<div id='cond-mat.mtrl-sci'></div>

# cond-mat.mtrl-sci [[Back]](#toc)

### [177] [Tunable electronic energy level alignment and exciton diversity in organic-inorganic van der Waals heterostructures](https://arxiv.org/abs/2510.05267)
*Aurélie Champagne,Olugbenga Adeniran,Jonah B. Haber,Antonios M. Alvertis,Zhen-Fei Liu,Jeffrey B. Neaton*

Main category: cond-mat.mtrl-sci

TL;DR: 有机-无机范德华异质结结合了分子单层和过渡金属二硫化物的优点，可以实现可调的光电性能和量子激子现象。


<details>
  <summary>Details</summary>
Motivation: 探索有机-无机范德华异质结的可调光电性能和量子激子现象。

Method: 使用从头算多体微扰理论（GW近似和Bethe-Salpeter方程）研究了分子晶体和TMDs的混合双层结构。

Result: 发现TMD诱导的极化导致分子晶体带隙的显著重构，并且可以通过改变TMD来调控激子的能量级别对齐，实现各种最低能量激子（包括强束缚的混合激子和长寿命的电荷转移激子）。

Conclusion: 有机-无机范德华异质结为可调光电器件和量子激子现象提供了一种有前景的材料，扩展了低维系统的设计空间。

Abstract: van der Waals stacking of two-dimensional (2D) materials offers a powerful
platform for engineering material interfaces with tailored electronic and
optical properties. While most van der Waals multilayers have featured
inorganic monolayers, incorporating molecular monolayers introduces new degrees
of tunability and functionality. Here, we investigate hybrid bilayers composed
of atomically thin perylene-based molecular crystals interfaced with monolayer
transition metal dichalcogenides (TMDs), specifically MoS2 and WS2. Using ab
initio many-body perturbation theory within the GW approximation and the
Bethe-Salpeter equation approach, we predict emergent properties beyond those
of the isolated constituent systems. Notably, we find substantial
renormalization of monolayer molecular crystal band gap due to TMD-induced
polarization. Furthermore, by varying the TMD monolayer, we demonstrate tuning
of the energy level alignment of the bilayer and subsequent control over a
diversity of lowest-energy excitons, which include strongly bound hybrid
excitons and long-lived charge-transfer excitons. These findings establish
organic-inorganic van der Waals heterostructures as a promising class of
materials for tunable optoelectronic devices and quantum excitonic phenomena,
expanding the design space for low-dimensional systems.

</details>


### [178] [Fermi surface and Berry phase analysis for Dirac nodal line semimetals: cautionary tale to SrGa$_2$ and BaGa$_2$](https://arxiv.org/abs/2510.05304)
*Yuxiang Gao,Yichen Zhang,Shiming Lei,Neil Harrison,Mun Keat Chan,Jonathan D. Denlinger,Sergey Gorovikov,Sanu Mishra,Yan Sun,Ming Yi,Emilia Morosan*

Main category: cond-mat.mtrl-sci

TL;DR: 在中心对称化合物中，Berry相分析需要考虑塞曼效应，即使在SrGa2和BaGa2等狄拉克节点线半金属中也是如此。


<details>
  <summary>Details</summary>
Motivation: 在中心对称化合物中，量子振荡推断出的Berry相常被认为是拓扑性质的证据，但塞曼效应和轨道磁矩的贡献会使Berry相的确定变得复杂。尽管中心对称化合物的轨道磁矩贡献可以忽略，但塞曼效应通常被忽视。

Method: 对非磁性中心对称化合物SrGa2和BaGa2进行了详细研究，结合了角度依赖的量子振荡、角分辨光电子能谱（ARPES）和密度泛函理论（DFT）计算，并重新审视了Lifshitz-Kosevich（LK）公式，分析了与塞曼效应相关的现象和结果。

Result: 实验证实SrGa2和BaGa2是狄拉克节点线半金属。尽管LK公式（包括高次谐波）可以很好地拟合实验数据，但Berry相的确定仍然受到塞曼效应的干扰。研究强调了自旋阻尼项在Berry相分析中的作用。

Conclusion: 尽管在中心对称化合物中，量子振荡可以提供非平凡拓扑的证据，但Berry相的精确确定必须考虑塞曼效应，自旋阻尼项起着关键作用。

Abstract: A Berry phase of odd multiples of $\pi$ inferred from quantum oscillations
(QOs) has often been treated as evidence for nontrivial reciprocal space
topology. However, disentangling the Berry phase values from the Zeeman effect
and the orbital magnetic moment is often challenging. In centrosymmetric
compounds, the case is simpler as the orbital magnetic moment contribution is
negligible. Although the Zeeman effect can be significant, it is usually
overlooked in most studies of QOs in centrosymmetric compounds. Here, we
present a detailed study on the non-magnetic centrosymmetric $\mathrm{SrGa_2}$
and $\mathrm{BaGa_2}$, which are predicted to be Dirac nodal line semimetals
(DNLSs) based on density functional theory (DFT) calculations. Evidence of the
nontrivial topology is found in magnetotransport measurements. The Fermi
surface topology and band structure are carefully studied through a combination
of angle-dependent QOs, angle-resolved photoemission spectroscopy (ARPES), and
DFT calculations, where the nodal line is observed in the vicinity of the Fermi
level. Strong de Haas-van Alphen fundamental oscillations associated with
higher harmonics are observed in both compounds, which are well-fitted by the
Lifshitz-Kosevich (LK) formula. However, even with the inclusion of higher
harmonics in the fitting, we found that the Berry phases cannot be
unambiguously determined when the Zeeman effect is included. We revisit the LK
formula and analyze the phenomena and outcomes that were associated with the
Zeeman effect in previous studies. Our experimental results confirm that
$\mathrm{SrGa_2}$ and $\mathrm{BaGa_2}$ are Dirac nodal line semimetals.
Additionally, we highlight the often overlooked role of spin-damping terms in
Berry phase analysis.

</details>


### [179] [Thermodynamics of proton insertion across the perovskite-brownmillerite transition in La0.5Sr0.5CoO3-δ](https://arxiv.org/abs/2510.05323)
*Armand J. Lannerd,Nathan J. Szymanski,Christopher J. Bartel*

Main category: cond-mat.mtrl-sci

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: La$_{1-x}$Sr$_{x}$CoO3-$\delta$ is a promising off-stoichiometric metal oxide
that undergoes a topotactic perovskite ($\delta$ = 0) to brownmillerite
($\delta$ = 0.5) transition under electrochemical and thermochemical stimuli,
with concomitant variations in its electrical, magnetic, thermal, and optical
properties. Recent studies on thin-film cycling in electrochemical devices show
incomplete reversibility of this transition, with significant acid-etching
serving as a degradation mechanism. While earlier investigations examined the
protonation of brownmillerite SrCoO2.5, the thermodynamics of protonation
across the perovskite-to-brownmillerite transition remain poorly understood. In
this work, we combine density functional theory calculations with predictions
from universal machine-learning interatomic potentials to elucidate the
energetics and implications of protonation across the transition for
La0.5Sr0.5CoO3-$\delta$. These calculations reveal negative hydrogen insertion
energies and strong competition with oxygen vacancy formation across the
transition for a wide range of conditions. The extent of protonation is
primarily limited by the availability of Co 3d states to accommodate reduction
by inserted hydrogen. Although hydrogen insertion is often thermodynamically
favorable within a defect picture, a convex hull analysis of the resulting
HyLa0.5Sr0.5CoO3-$\delta$ phases reveals them to be unstable against
decomposition into hydroxides among other products. This instability increases
with hydrogen content and provides a thermodynamic basis for the acid-etching
observed during electrochemical cycling. This work advances the fundamental
understanding of protonation in La0.5Sr0.5CoO3-$\delta$ and contextualizes
experimental observations of related materials in the presence of moisture or
H2.

</details>


### [180] [Cation vacancies mediate thermochemical water splitting with iron aluminates](https://arxiv.org/abs/2510.05328)
*Nathan J. Szymanski,Kent J. Warren,Alan W. Weimer,Christopher J. Bartel*

Main category: cond-mat.mtrl-sci

TL;DR: Fe空位在铁铝尖晶石氧化还原行为中起关键作用，受位点反位影响，从而支持太阳能水分解制氢。


<details>
  <summary>Details</summary>
Motivation: 探究铁铝尖晶石在太阳能水分解制氢中的氧化还原行为，特别是阳离子空位（特别是Fe空位）的影响，并解释其与实验观察一致的机制。

Method: 通过计算研究Fe空位在(Fe$\zeta$Al1-$\zeta$)3O4中的形成能，并考虑了Fe和Al的位点反位（反位无序）对空位形成能的影响。

Result: 发现Fe空位在位点反位促进下变得容易形成，特别是当1/3的阳离子位点发生反位时，Al含量高的尖晶石（$\\zeta$ = 1/3）中Fe空位的形成能从3 eV以上降至0.62 eV，从而在氧化条件下允许高Fe空位浓度，支持高达361 $\\mu$mol/g的H2产率。

Conclusion: 证明了太阳能水分解可以通过阳离子空位机制进行，并阐明了位点反位、空位能量学和缺陷相互作用如何共同影响氧化还原性能，为设计和优化阳离子空位循环材料提供了通用原则。

Abstract: Solar thermochemical water splitting enables hydrogen production by cycling
metal oxides between reduced and oxidized states, typically through an oxygen
vacancy mechanism. However, recent experimental work suggests that cation
vacancies have a greater influence on the redox behavior of iron aluminate
spinels used in water splitting. This remains debated, as calculations predict
that such cation vacancies are thermodynamically unfavorable. In the current
work, we show that Fe vacancies in (Fe$\zeta$Al1-$\zeta$)3O4 become accessible
only when facilitated by inversion between Fe and Al. This antisite disorder
lowers the formation energy of octahedral Fe vacancies in Al-rich spinels
($\zeta$ = 1/3) from over 3 eV to just 0.62 eV when one third of the cation
sites are inverted, allowing high Fe vacancy concentrations under oxidizing
conditions. This mechanism supports high H2 yields up to 361 $\mu$mol/g,
consistent with experimental observations. Our findings support the notion that
solar thermochemical water splitting can occur through a cation vacancy
mechanism. They also clarify how site inversion, vacancy energetics, and defect
interactions each contribute to redox performance, offering general design
principles for identifying and optimizing materials that operate through cation
vacancy cycling.

</details>


### [181] [High- and medium-entropy nitride coatings from the Cr-Hf-Mo-Ta-W-N system: properties and high-temperature stability](https://arxiv.org/abs/2510.05928)
*Pavel Souček,Stanislava Debnárová,Šárka Zuzjaková,Shuyao Lin,Matej Fekete,Zsolt Czigány,Katalin Balázsi,Lukáš Vrána,Tatiana Pitoňáková,Ondřej Jašek,Petr Zeman,Nikola Koutná*

Main category: cond-mat.mtrl-sci

TL;DR: 高熵氮化物涂层（Cr-Hf-Mo-Ta-W-N）在相稳定性、微观结构和高温行为方面，熵和个别元素的作用得到了计算和实验的阐明。


<details>
  <summary>Details</summary>
Motivation: 阐明熵和个别元素在 Cr-Hf-Mo-Ta-W-N 氮化物涂层的相稳定性、微观结构和高温行为中的作用。

Method: 使用从头计算和实验研究高熵和中熵氮化物涂层。计算形成能以确定相稳定性。通过反应磁控溅射沉积涂层，并在不同温度下进行测试。使用透射电子显微镜分析微观结构和元素分布。

Result: 计算表明氮空位稳定了立方（fcc）相，其中 Hf 和 Ta 是强稳定剂，而 W 使晶格不稳定。沉积的涂层均表现出柱状 fcc 结构。高温沉积（HT）涂层更致密，氮含量较低，晶粒较大，硬度和弹性模量更高。HT 涂层在高达 1200°C 的热稳定性和 1400°C 的氧化测试中表现优于低温沉积（AT）涂层。氮含量低于 10 at.% 是在 1000°C 下生存的关键。透射电子显微镜显示 W 偏析和 HfO2 形成，而 fcc 氮化物仍然是优势相。Ta 的富集对优异的热稳定性和氧化稳定性至关重要。

Conclusion: Ta 的富集对 Cr-Hf-Mo-Ta-W-N 氮化物涂层的热稳定性和氧化稳定性至关重要。

Abstract: High- and medium-entropy nitride coatings from the Cr-Hf-Mo-Ta-W-N system
were studied using ab initio calculations and experiments to clarify the role
of entropy and individual elements in phase stability, microstructure, and
high-temperature behaviour. Formation energy calculations indicated that
nitrogen vacancies stabilise the cubic (fcc) phase, with hafnium and tantalum
acting as strong stabilisers, while tungsten destabilises the lattice. Coatings
were deposited by reactive magnetron sputtering at approx. 50C (AT) and approx.
580C (HT). All exhibited columnar fcc structures; high-temperature deposition
produced denser coatings, lower nitrogen content, and larger crystallites,
resulting in higher hardness and elastic modulus. Thermal stability was tested
up to 1200C on Si and oxidation at 1400C on sapphire. AT coatings failed early,
while most HT coatings endured. Nitrogen loss less than 10 at.% at 1000C was
critical for survival. TEM revealed tungsten segregation and HfO2 formation,
while fcc nitride remained dominant. Ta enrichment proved essential for
superior thermal and oxidation stability.

</details>


### [182] [Machine Learning Interatomic Potentials Enable Molecular Dynamics Simulations of Doped MoS2](https://arxiv.org/abs/2510.05339)
*Abrar Faiyad,Ashlie Martini*

Main category: cond-mat.mtrl-sci

TL;DR: 使用基于机器学习势能的通用原子模型，我们开发了一个计算框架，用于模拟掺杂25种元素的MoS2，在计算成本大幅降低的情况下，捕获了复杂的掺杂现象，为二维掺杂材料系统的规模化发现提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在开发一种高效的计算框架，用于模拟掺杂 MoS2，以探索其在摩擦学、电子和光电性能方面的应用潜力。

Method: 本研究提出了一个计算框架，使用 Meta 的通用原子机器学习势能（MLIP）来模拟使用 25 种不同元素（金属、非金属和过渡金属）掺杂的 MoS2。该框架通过与密度泛函理论（DFT）计算进行基准测试来验证其准确性，并进行了掺杂 MoS2 超晶胞的加热-冷却模拟。

Result: 模拟结果显示，该框架能够有效捕获掺杂 MoS2 中出现的复杂现象，例如掺杂剂聚集、MoS2 层断裂、层间扩散以及化学化合物形成。与 DFT 计算相比，该方法的计算成本降低了几个数量级。

Conclusion: 本研究提出的基于 MLIP 的计算框架为应用驱动的掺杂 MoS2 设计提供了一个开源的工作流程，能够对掺杂剂候选物进行高通量筛选和成分优化，以实现特定的摩擦学、电子和光电性能。该 MLIP 成功地缩小了第一性原理方法和经验势能之间的准确性-效率差距，为二维掺杂材料系统的规模化发现提供了前所未有的机会。

Abstract: We present the first computational framework for molecular dynamics
simulation of MoS2 doped with 25 elements spanning metals, non-metals, and
transition metals using Meta's Universal Model for Atoms machine learning
interatomic potential (MLIP). Benchmarking against density functional theory
calculations demonstrates the accuracy of the MLIP for simulating doped-MoS2
systems and highlights opportunities for improvement. Using the MLIP, we
perform heating-cooling simulations of doped-MoS2 supercells. The simulations
capture complex phenomena including dopant clustering, MoS2 layer fracturing,
interlayer diffusion, and chemical compound formation at orders-of-magnitude
reduced computational cost compared to density functional theory. This work
provides an open-source computational workflow for application-oriented design
of doped-MoS2, enabling high-throughput screening of dopant candidates and
optimization of compositions for targeted tribological, electronic, and
optoelectronic performance. The MLIP bridges the accuracy-efficiency gap
between first-principles methods and empirical potentials, and the framework
offers unprecedented opportunities for large-scale materials discovery in
two-dimensional doped material systems.

</details>


### [183] [Retardance of lab grown diamond substrates as a function of thickness: momentum-drift random walk model](https://arxiv.org/abs/2510.05932)
*Thanh Tran,Phuong Vo,Thomas Sheppard,Timothy Grotjohn,Paul Quayle*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究探讨了MPCVD生长金刚石衬底的平均迟滞与厚度的关系，发现迟滞与厚度的相关性因测量方向（垂直或平行于生长方向）而异。垂直方向上，迟滞与厚度平方根近似成正比；平行方向上，迟滞更高且与厚度近似成线性关系。这种各向异性源于应力轴层间相关性的差异，而非应力大小。研究提出了一个二维随机游走模型来模拟迟滞，并发现该模型能很好地匹配实验数据，且沿生长方向的动量因子更高。该研究为理解金刚石衬底的双折射效应提供了量化框架，对材料选择和发展具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 研究金刚石衬底的平均迟滞与厚度之间的相关性，特别是不同测量方向下的差异，以理解和量化其双折射效应。

Method: 通过微波等离子体增强化学气相沉积（MPCVD）生长金刚石衬底，并在垂直和平行于生长方向两个方向上测量其迟滞。提出一个二维随机游走模型来模拟集成迟滞，并优化动量因子以匹配实验数据。

Result: 测量结果显示，垂直于生长方向的平均迟滞与厚度平方根近似成正比，而平行于生长方向的平均迟滞更高，且与厚度近似成线性关系。模型结果与实验数据吻合，并指出沿生长方向的动量因子更高。模型和实验均表明，薄样品的迟滞-厚度比趋于相似的基础迟滞值。

Conclusion: 该研究建立了理解金刚石衬底双折射效应的量化框架，揭示了迟滞与厚度在不同测量方向下的关系，以及各向异性的成因。模型模拟结果与实验数据一致，为金刚石材料在热管理、量子传感、高功率电子和光学等领域的应用提供了指导。

Abstract: This work studies the correlation between mean retardance and thickness of
diamond substrates grown homoepitaxially via microwave plasma-enhanced chemical
vapor deposition (MPCVD). We measure the retardance of a diamond substrate in
two orientations: perpendicular and parallel to the growth direction. Our
experimental results demonstrate that the correlation between mean retardance
and thickness differs for these orientations. When measured perpendicular to
the growth direction, the mean retardance is approximately proportional to the
square root of the substrate thickness. In contrast, when measured parallel to
the growth direction, we observe a generally higher mean retardance and an
approximately linear correlation with thickness. This anisotropy arises not
from differences in stress magnitude but from differences in the interlayer
correlation of the principal stress axes, as evidenced by correlation
coefficients between the azimuth angles of consecutive layers in the diamond
crystal. To simulate the integrated retardance of diamond wafers, we propose a
two-dimensional random walk model with momentum drift, which captures the
diamond crystal tendency to preserve the azimuth angle across the samples. By
optimizing the momentum factor, we show that the model can closely match
experimental data. The momentum factor is found higher along the growth
direction, which is consistent with the calculated correlation coefficients.
Furthermore, both the model and experiments indicate that
retardance-to-thickness ratios of thin samples converge toward similar base
retardances in both orientations. These findings establish a quantitative
framework for interpreting birefringence in diamond substrates, with
implications for material selection and development in thermal management,
quantum sensing, high-power electronics, and optical applications.

</details>


### [184] [Photoluminescence excitation spectroscopy of quantum wire-like dislocation states in ZnS](https://arxiv.org/abs/2510.05357)
*Alexander Blackston,Alexandra Fonseca Montenegro,Sevim Polat Genlik,Maryam Ghazisaeidi,Roberto C. Myers*

Main category: cond-mat.mtrl-sci

TL;DR: 通过实验证实了ZnS材料中位错核心处存在一维色散电子能带，并通过光谱学研究了其光学性质。


<details>
  <summary>Details</summary>
Motivation: 验证近期从头算（ab initio）计算关于ZnS材料中位错核心处存在一维色散电子能带的预测，并研究这些能带的光学特性。

Method: 利用扫描电子显微镜-电子通道图形成像技术量化了外延ZnS中因应变弛豫形成的位错密度，并通过室温椭圆光度法和低温光致发光光谱研究了位错相关的光学吸收和发射特性。

Result: 室温椭圆光度法发现了与位错密度相关的吸收峰，与理论预测一致。低温光致发光光谱显示出与位错一维带-带跃迁相匹配的深发射峰。光致发光激发光谱识别出四种与位错相关的发射峰（2.78, 2.41, 2.20, 1.88 eV），其强度在低于带隙激发时仅有少量衰减，另外两个峰（3.11, 1.53 eV）则强烈淬灭。

Conclusion: 实验结果支持在ZnS材料的位错核心中存在高效的一维带-带辐射跃迁，这些跃迁源于量子线状的位错核心态，并与宽带隙半导体中常见的非辐射深层缺陷不同。

Abstract: Recent \textit{ab initio} calculations predict 1D dispersive electronic bands
confined to the atomic scale cores of dislocations in the wide bandgap (3.84
eV) semiconductor ZnS. We test these predictions by correlating sub-bandgap
optical transitions with the density of dislocations formed during strain
relaxation in epitaxial ZnS grown on GaP. The densities for four predicted
partial dislocations are quantified using scanning electron microscopy-based
electron channeling contrast imaging. Room-temperature ellipsometry reveals
absorption peaks that scale with dislocation density and align with theoretical
predictions. Low-temperature photoluminescence spectra show deep emission peaks
matching dislocation 1D band-to-band transitions. Photoluminescence excitation
spectroscopy reveals six distinct emission lines with contrasting excitation
dependence. Four peaks (2.78, 2.41, 2.20, 1.88 eV), assigned to dislocations,
exhibit only modest suppression ($\leq$5$\times$) when excited below the ZnS
bandgap, while two other peaks (3.11, 1.53~eV) are strongly quenched
($>$10$\times$). These findings support the existence of efficient, 1D
band-to-band radiative transitions within quantum wire-like dislocation core
states in ZnS, distinct from typical non-radiative deep-level defects in
wide-gap semiconductors.

</details>


### [185] [Imaging Nanoscale Carrier, Thermal, and Structural Dynamics with Time-Resolved and Ultrafast Electron Energy-Loss Spectroscopy](https://arxiv.org/abs/2510.05413)
*Wonseok Lee,Levi D. Palmer,Thomas E. Gage,Scott K. Cushing*

Main category: cond-mat.mtrl-sci

TL;DR: 时间分辨和超快电子能量损失谱（EELS）是一种新兴技术，可用于在飞秒到微秒的时间尺度上测量光激发载流子、晶格动力学和近场。当在专门的扫描透射电子显微镜或超快电子显微镜（UEM）中进行时，时间分辨和超快EELS可以对光激发或施加偏压后的载流子、晶格振动和热耗散进行直接成像。然而，理论计算和电子光学方面的最新进展对于充分发挥超快EELS谱成像的潜力至关重要。本综述全面概述了时间分辨和超快EELS的理论和仪器方面的最新进展。首先介绍该技术，然后从物理上描述损失函数。我们概述了计算和解释从低损耗等离激元到类似于X射线吸收的核激发态的基态和瞬态EELS谱的方法。然后，我们调查了时间分辨和超快EELS技术在光子诱导近场电子显微镜之外的现状，强调了对载流子和热动力学成像的能力。最后，我们探讨了新兴技术带来的未来方向，包括电子束单色仪、原位和操作池、无激光UEM和高速直接电子探测器。这些进展使时间分辨和超快EELS成为揭示量子材料和太阳能转换器件中纳米尺度动态过程的关键工具。


<details>
  <summary>Details</summary>
Motivation: 介绍时间分辨和超快电子能量损失谱（EELS）技术，概述其在测量光激发载流子、晶格动力学和近场方面的能力，并探讨其在量子材料和太阳能转换器件等领域的应用潜力。

Method: 综述了时间分辨和超快EELS的理论和仪器方面的最新进展，包括对损失函数的物理描述、计算和解释基态及瞬态EELS谱的方法，以及对该技术当前能力的介绍，并展望了未来发展方向。

Result: 总结了时间分辨和超快EELS在成像载流子和热动力学方面的能力，并指出了电子束单色仪、原位和操作池、无激光UEM和高速直接电子探测器等新兴技术将带来的未来发展。

Conclusion: 时间分辨和超快EELS技术，在新兴技术的支持下，将成为研究量子材料和太阳能转换器件中纳米尺度动态过程的关键工具。

Abstract: Time-resolved and ultrafast electron energy-loss spectroscopy (EELS) is an
emerging technique for measuring photoexcited carriers, lattice dynamics, and
near-fields across femtosecond to microsecond timescales. When performed in
either a specialized scanning transmission electron microscope or ultrafast
electron microscope (UEM), time-resolved and ultrafast EELS can directly image
charge carriers, lattice vibrations, and heat dissipation following
photoexcitation or applied bias. Yet recent advances in theoretical
calculations and electron optics are often required to realize the full
potential of ultrafast EEL spectrum imaging. In this review, we present a
comprehensive overview of the recent progress in the theory and instrumentation
of time-resolved and ultrafast EELS. We begin with an introduction to the
technique, followed by a physical description of the loss function. We outline
approaches for calculating and interpreting ground-state and transient EEL
spectra spanning low-loss plasmons to core-level excitations analogous to X-ray
absorption. We then survey the current state of time-resolved and ultrafast
EELS techniques beyond photon-induced near-field electron microscopy,
highlighting abilities to image carrier and thermal dynamics. Finally, we
examine future directions enabled by emerging technologies, including electron
beam monochromation, in situ and operando cells, laser-free UEM, and high-speed
direct electron detectors. These advances position time-resolved and ultrafast
EELS as a critical tool for uncovering nanoscale dynamic processes in quantum
materials and solar energy conversion devices.

</details>


### [186] [Unveiling the physical attributes of CdGa2Te4 and ZnGa2Te4 compounds: A first-principles study for next-generation photovoltaics](https://arxiv.org/abs/2510.05424)
*Md Hasan Shahriar Rifat,Tanvir Khan,Md Arafat Hossain Shourov,Md Sahat Bin Sayed,Md Saiful Islam*

Main category: cond-mat.mtrl-sci

TL;DR: CdGa2Te4 and ZnGa2Te4在可见光区域具有高吸收率，低激子结合能，适用于光伏应用。


<details>
  <summary>Details</summary>
Motivation: 研究CdGa2Te4和ZnGa2Te4的电子和光学性质，并评估其作为太阳能电池的潜力。

Method: 使用第一性原理DFT计算研究CdGa2Te4和ZnGa2Te4的电子和光学性质，并使用SCAPS-1D工具模拟薄膜太阳能电池的性能。

Result: CdGa2Te4和ZnGa2Te4在可见光区域表现出近乎统一的光吸收效率（10^4 cm^-1），具有低激子结合能（18.85-26.81 meV）和大的Bohr半径（23-34.3埃）。模拟显示，为达到20%以上的效率，吸收层缺陷密度需低于1.772 x 10^13 cm^-3。基于CdGa2Te4和ZnGa2Te4的太阳能电池的最高模拟效率分别为18.46%和17.35%。

Conclusion: CdGa2Te4和ZnGa2Te4因其优异的光电特性，在光伏应用方面具有巨大潜力，但需要优化器件结构和降低缺陷密度以进一步提高效率。

Abstract: The electronic and optical properties of CdGa2Te4 and ZnGa2Te4 were studied
using first-principles DFT calculations. Band gaps were calculated using the
GGA-PBESol functional. Both materials show promise for photovoltaic
applications because of their large, near-unity absorption efficiencies (10^4
cm^-1) in the visible region. They exhibit low exciton binding energies
(18.85-26.81 meV), large Bohr radii (23-34.3 Angstrom), and moderate exciton
temperatures (218-311 K), which are favorable for photovoltaic applications.
Their performance as solar cells was simulated using the SCAPS-1D tool for
thin-film devices with Pt/CdS/CdGa2Te4/Cu2O/Ti and Pt/CdS/ZnGa2Te4/Cu2O/Ti
structures. We investigated the effects of layer thickness, donor and acceptor
concentrations (shallow donors/acceptors), and defect density on device
performance. The ideal absorber thickness for XGa2Te4 (X = Cd, Zn) was found to
be 1000-1800 nm, and the CdS buffer layer around 150 nm. To obtain an
efficiency above 20%, the defect density in the CdGa2Te4 and ZnGa2Te4 absorber
layers should be kept below 1.772 x 10^13 cm^-3. The best simulations show
efficiencies of 18.46% and 17.35% for CdGa2Te4- and ZnGa2Te4-based solar cells,
respectively.

</details>


### [187] [Co-evaporated Formamidinium tin triiodide with suppressed p-type self-doping](https://arxiv.org/abs/2510.05564)
*Junhyoung Park,Andrea Olivati,Mirko Prato,Min Kim,Annamaria Petrozza*

Main category: cond-mat.mtrl-sci

TL;DR: 无溶剂共蒸发法可制备高质量的FASnI3薄膜，具有优异的带隙和稳定性。


<details>
  <summary>Details</summary>
Motivation: 开发一种制备高质量、低缺陷Sn基钙钛矿多晶薄膜的方法。

Method: 采用无添加剂、无还原剂的FASnI3前驱体共蒸发法生长薄膜。

Result: 获得了带隙约为1.31 eV的高度结晶的FASnI3薄膜，其Sn2+氧化和自掺杂倾向较低，缺陷形成抵抗力更强。

Conclusion: 无溶剂共蒸发法是制备高质量Sn基钙钛矿多晶薄膜的有前景的方法。

Abstract: Co-evaporation of formamidinium tin triiodide (FASnI3) precursors, without
any additives or reducing agents, leads to the growth of a highly crystalline
thin film which shows a bandgap around 1.31 eV, closely matching the
theoretical value predicted from the ideal single crystal structure of FASnI3.
The polycrystalline thin film presents a lower tendency of Sn2+ to Sn4+
oxidation and highly reduced tendency to self-doping, demonstrating, overall,
an improved resistance to defects formation. These findings suggest
solvent-free co-evaporation processes as a promising route for high quality
Sn-based perovskite polycrystalline thin films.

</details>


### [188] [From High-Entropy Ceramics (HECs) to Compositionally Complex Ceramics (CCCs) and Beyond](https://arxiv.org/abs/2510.05629)
*Jian Luo*

Main category: cond-mat.mtrl-sci

TL;DR: 熵不一定是高熵陶瓷（HECs）的唯一目标；成分复杂陶瓷（CCCs）通过非等摩尔成分和有序性提供了新的性能优化途径。


<details>
  <summary>Details</summary>
Motivation: 探讨高熵陶瓷（HECs）的概念，质疑是否必须最大化熵，并提出探索成分复杂性（CCCs）是设计优越陶瓷性能的新策略。

Method: 回顾了HECs的关键概念和术语，讨论了双相等比复杂陶瓷（CCCs）、超高熵相以及超快反应烧结等新兴方向，并提出了利用成分复杂性和相关无序性来设计陶瓷的策略。

Result: 提出了探索成分复杂性和相关无序性（化学和结构短程有序耦合）比单纯最大化熵更能有效地设计出具有优越性能的陶瓷。

Conclusion: 认为探索广阔的非等摩尔空间中的成分复杂性，连同相关无序性，是设计高性能陶瓷比单纯最大化熵更有效的策略。

Abstract: Over the past decade, the field of high-entropy ceramics (HECs) has rapidly
expanded to encompass a wide range of oxides, borides, silicides, and other
ceramic solid solutions. In 2020, we proposed extending the concept of HECs to
compositionally complex ceramics (CCCs), in which non-equimolar compositions
and the presence of long- or short-range order reduce entropy while offering
new opportunities to tailor and enhance properties, often beyond those of
higher-entropy counterparts. Here, fundamental questions arise: Is the entropy
in HECs truly high? Should maximizing entropy always be our goal? This
perspective article revisits key concepts and terminologies and highlights
emerging directions, including dual-phase CCCs, ultrahigh-entropy phases, and
novel processing routes such as ultrafast reactive sintering. We propose that
exploring compositional complexity across vast non-equimolar spaces, together
with correlated disorder (coupled short-range chemical and structural orders),
offers a more effective strategy for designing ceramics with superior
performance than simply maximizing entropy.

</details>


### [189] [Confinement-Controlled Morphology and Stability of One-Dimensional CrI3 Nanotubes](https://arxiv.org/abs/2510.05889)
*Ihsan Caha,Aqrab ul Ahmad,Francis Leonard Deepak*

Main category: cond-mat.mtrl-sci

TL;DR: 模板辅助封装技术可用于生长稳定的二维磁性材料单层，以设计新颖的一维异质结构，这为研究低维量子效应和自旋相关现象开辟了新途径。


<details>
  <summary>Details</summary>
Motivation: 将二维范德华磁性材料单层集成到下一代技术应用中，由于其结构和磁性不稳定性问题，仍然是一个重大挑战。

Method: 通过多壁碳纳米管作为纳米级主体模板，探索了二维CrI3晶体随直径变化的封装行为。

Result: 结果表明，CrI3纳米管的形成受到主体碳纳米管直径的直接影响。CrI3纳米棒优先在内径高达5纳米的MWCNT中形成，而在直径高达8纳米的CNT中则稳定形成单壁CrI3纳米管。当主体CNT的直径超过10纳米时，CrI3主要形成表面涂层。原位电子束辐照表明，限制在MWCNT中的单壁CrI3纳米管比外部涂层的CrI3具有更高的结构稳定性，而外部涂层的CrI3会分解成金属Cr簇。长期辐照会导致CrI3纳米管转变为纳米棒。

Conclusion: 这些研究为工程坚固、可调的一维CrI3磁性异质结构，用于自旋电子学和数据存储应用奠定了基础。

Abstract: Integrating monolayers derived from 2D van der Waals (vdW) magnetic materials
into next-generation technological applications remains a significant challenge
due to their structural and magnetic instability issues. Template-assisted
encapsulation is a potential route for the growth of stable 2D monolayers aimed
at designing novel 1D heterostructures, opening new avenues for studying
low-dimensional quantum effects and spin-related phenomena. In this study, we
explored the diameter-dependent encapsulation of 2D CrI3 crystals using
multi-walled carbon nanotubes as nanoscale host templates. Advanced microscopic
analysis revealed distinct structural transitions, ranging from internal
nanorod encapsulation to external shell formation, directly influenced by the
host nanotube diameter. Furthermore, statistical analysis of structural
morphologies indicates that CrI3 nanorods preferentially form within MWCNTs
with inner diameters up to 5 nm, while single-walled CrI3 nanotubes are
stabilized in CNTs with diameters up to 8 nm. For host CNTs exceeding 10 nm in
diameter, CrI3 predominantly forms surface coatings rather than confined
one-dimensional structures. In situ electron beam irradiation demonstrates the
superior structural stability of single-walled CrI3 confined within MWCNTs,
while externally coated CrI3 undergoes decomposition into metallic Cr clusters.
Prolonged irradiation induces a morphological transformation of CrI3 nanotubes
into nanorods. These insights lay the groundwork for engineering robust,
tunable 1D magnetic heterostructures of CrI3 for spintronic and data storage
applications.

</details>


### [190] [Magnon-Magnon Interaction Induced by Dynamic Coupling in a Hybrid Magnonic Crystal](https://arxiv.org/abs/2510.06004)
*Rawnak Sultana,Mojtaba Taghipour Kaffash,Gianluca Gubbiotti,Yi Ji,M. Benjamin Jungfleisch,Federico Montoncello*

Main category: cond-mat.mtrl-sci

TL;DR: 杂化磁子晶体中的自旋波动力学，通过实验和数值模拟研究了由CoFeB人工自旋冰（ASI）和NiFe薄膜组成的杂化磁子晶体中的自旋波动力学，揭示了层间偶极耦合对磁化动力学的影响，并观察到ASI边缘模式与NiFe薄膜中的后向体积模式之间的杂化现象，这为磁子信号传输和操纵提供了新的途径。


<details>
  <summary>Details</summary>
Motivation: 研究由CoFeB人工自旋冰（ASI）和NiFe薄膜组成的杂化磁子晶体中的自旋波动力学，特别是层间偶极耦合对磁化动力学的影响，以及ASI几何结构如何选择性地增强特定自旋波波长。

Method: 结合布里渊光散射光谱实验和微磁模拟，研究了在不同外加磁场和波矢量下，杂化磁子晶体中热自旋波的频率依赖性。

Result: 实验和模拟都观察到ASI边缘模式与NiFe薄膜中的后向体积模式之间的强烈的杂化现象，表现为光谱中的三峰结构。这种杂化现象在较宽的磁场范围内持续存在，影响着自旋波色散和磁滞回线中的频率-磁场响应。

Conclusion: ASI几何结构可以有效增强下层薄膜中的特定自旋波波长，为磁子信号的传输和操纵提供了优先通道。

Abstract: We report a combined experimental and numerical investigation of spin-wave
dynamics in a hybrid magnonic crystal consisting of a CoFeB artificial spin ice
(ASI) of stadium-shaped nanoelements patterned atop a continuous NiFe film,
separated by a 5 nm Al2O3 spacer. Using Brillouin light scattering
spectroscopy, we probe the frequency dependence of thermal spin waves as
functions of applied magnetic field and wavevector, revealing the decisive role
of interlayer dipolar coupling in the magnetization dynamics. Micromagnetic
simulations complement the experiments, showing a strong interplay between ASI
edge modes and backward volume modes in the NiFe film. The contrast in
saturation magnetization between CoFeB and NiFe enhances this coupling, leading
to a pronounced hybridization manifested as a triplet of peaks in the spectra -
predicted by simulations and observed experimentally. This magnon-magnon
coupling persists over a wide magnetic field range, shaping both the spin-wave
dispersion and frequency-field response throughout the hysteresis loop. Our
findings establish how ASI geometry can selectively enhance specific spin-wave
wavelengths in the underlying film, identifying them as preferential channels
for magnonic signal transport and manipulation.

</details>


### [191] [Autonomous interpretation of atomistic scattering data](https://arxiv.org/abs/2510.05938)
*Andy S. Anker,John L. A. Gardner,Louise A. M. Rosset,Andrew L. Goodwin,Volker L. Deringer*

Main category: cond-mat.mtrl-sci

TL;DR: 该研究引入了一个可微分优化框架，能够统一处理衍射计算、能量学和化学约束，以解决材料的原子结构解析问题。


<details>
  <summary>Details</summary>
Motivation: 当前的材料发现过程，尤其是利用自主实验室合成材料时，在利用散射实验验证原子结构方面存在瓶颈。传统方法依赖用户专业知识、手动处理或有限数据集的机器学习模型，阻碍了完全自主的材料发现。

Method: 提出了一种新颖的可微分优化框架，将衍射计算、能量学和化学约束整合为一个统一的优化问题。该框架能够处理多模态输入，以解决结构歧义性。

Result: 该方法在分子、晶体结构、纳米颗粒和无定形物质等多种体系中得到成功验证，能够有效解决结构歧义性问题。

Conclusion: 该数据驱动的方法有望为自主实验室提供信息，并最终指导其运行，从而加速材料发现的进程。

Abstract: Materials with bespoke properties have long been identified by computational
searches, and their experimental realisation is now coming within reach through
autonomous laboratories. Scattering experiments are central to verifying the
atomic structures of autonomously synthesised materials. Yet, interpreting
these measurements typically requires user expertise and manual processing, or
machine learning (ML) models trained on predefined datasets, limiting fully
autonomous materials discovery. Here, we introduce a differentiable
optimisation framework that treats scattering calculations, energetics, and
chemical constraints as a unified refinement problem. Capability demonstrations
across molecules, crystal structures, nanoparticles, and amorphous matter show
that this data-driven approach resolves structural degeneracies with
multi-modal inputs - suggesting its usefulness for informing, and ultimately
guiding, the operation of autonomous laboratories.

</details>


### [192] [Effect of crystallographic texture on dealloying kinetics and composition of nanoporous gold surface](https://arxiv.org/abs/2510.05970)
*Ezgi Hatipoğlu,Ayman A. El-Zoka,Yujun Zhao,Stanislav Mráz,Jochen M. Schneider,Baptiste Gault,Aparna Saksena*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究通过在Ag70Au30薄膜上进行化学剥离，发现{111}择优取向的薄膜比随机取向的薄膜具有更高的银含量和更细化的组织，表明其具有更快的剥离动力学，可用于催化和电化学应用。


<details>
  <summary>Details</summary>
Motivation: 探索纳米多孔金属在催化领域的应用潜力，重点研究织构对面内均匀性和催化性能的影响。

Method: 通过在400°C下沉积{111}择优取向或随机取向的Ag70Au30薄膜，然后进行化学剥离，并利用原子探针断层扫描技术分析纳米多孔结构的成分和形貌。

Result: 与随机取向的薄膜相比，择优取向的薄膜在韧带中保留了高2.7倍的银浓度，并且韧带粗化程度较低，表明其具有更快的剥离动力学。

Conclusion: 微结构工程（特别是织构控制）可以调控纳米多孔金属的性质，这对其在催化和电化学领域的应用至关重要。

Abstract: Nanoporous metals allow for tailoring composition and surface-to-volume
ratio, both aspects critical for applications in catalysis. Here, Ag70Au30 (2
at. %) films with a face-centered cubic structure were deposited at 400{\deg}C,
either {111}-textured or randomly oriented. Upon chemical dealloying, atom
probe tomography of the nanoporous structure reveals that the textured film
retains a up to 2.7 times higher Ag concentration within the ligaments compared
to the randomly oriented film that exhibits ligament coarsening, indicating
faster dealloying kinetics. Our study highlights the potential of
microstructure engineering in tailoring the properties of nanoporous metals for
possible future catalytic and electrochemical applications.

</details>


### [193] [Spin wave theory for the triaxial magnetic anisotropy 2D van der Waals antiferromagnet CrSBr](https://arxiv.org/abs/2510.06006)
*Sergio M. Rezende,Byron Freelon,Roberto L. Rodríguez-Suárez*

Main category: cond-mat.mtrl-sci

TL;DR: CrSBr是一种二维材料，具有反铁磁性。通过量子自旋波理论，结合反铁磁共振和非弹性中子散射数据，我们确定了其磁相互作用参数。


<details>
  <summary>Details</summary>
Motivation: 二维材料的磁性质因其独特性和潜在应用而备受关注。CrSBr作为一种具有反铁磁顺序的二维范德华晶体，引起了广泛兴趣。

Method: 采用全量子自旋波理论，考虑了三个层内交换相互作用、一个层间交换相互作用以及三轴磁各向异性，并通过拟合反铁磁共振和非弹性中子散射数据来确定相互作用参数。

Result: 成功拟合了理论结果与实验测量数据，获得了CrSBr的七个相互作用参数的可靠值。

Conclusion: 所确定的相互作用参数为进一步计算CrSBr的其他性质提供了基础。

Abstract: The magnetic properties of two-dimensional (2D) materials have been
attracting increasing attention in recent years due to their unique behavior
and possible applications in new devices. One material of great interest is the
2D van der Waals (vdW) crystal CrSBr, that exhibits antiferromagnetic (AF)
order at low temperatures due to an interlayer AF exchange interaction. Here we
present a full quantum spin-wave theory for CrSBr considering three intralayer
and one interlayer exchange interactions, and triaxial magnetic anisotropy. The
fits of the theoretical results to antiferromagnetic resonance (AFMR)
measurements and inelastic neutron scattering data yield reliable values for
the seven interaction parameters that can be used to calculate other properties
of this interesting material.

</details>


### [194] [Colorimetry and Tribology of Ultrapure Copper Surface Micromodification](https://arxiv.org/abs/2510.06075)
*Aleksandra Szczupak,Grzegorz Cios,Benedykt R. Jany*

Main category: cond-mat.mtrl-sci

TL;DR: 通过控制铜表面微观纹理来调节其光学和摩擦学性质。


<details>
  <summary>Details</summary>
Motivation: 在不改变铜化学成分的情况下，控制其颜色和磨损率，是一种具有成本效益的替代传统化学方法.

Method: 使用不同粒度的砂纸，以不同方向打磨超纯铜，研究磨损率和光学性质的变化.

Result: 通过改变砂纸粒度和打磨方向，可以调节微观纹槽的形貌，进而改变磨损率和颜色。在铜晶粒边界处打磨，磨损率会改变一倍。

Conclusion: 通过微观纹理工程，可以简单有效地控制铜的颜色和反射率，实现材料性能定制。

Abstract: Controlling optical and tribological properties of metal surfaces, like color
and wear rate, without altering their chemical composition is a highly
desirable process across numerous fields of science and industry. It represents
a cost-effective alternative to traditional chemical methods, particularly for
copper, one of the most important metals widely used where high electrical and
thermal conductivity, alongside resistance to corrosion, are required. We
investigated the control of copper surface texture through a controlled
micromodification process, utilizing constant force and velocity with abrasive
silicon carbide sandpaper on ultrapure copper pellets exhibiting elongated
crystallographic grains, and its impact on optical properties. Systematically
varying grit size and rubbing direction, both along and across the grains,
resulted in tunable microgroove morphology, demonstrating a marked difference
in wear rate between single-grain and multi-grain abrasion. Furthermore,
modification along copper grain boundaries yielded a change in the wear rate by
a factor of two, related to single-grain and multi-grain abrasion regime
changes, enabling precise control over material performance via tuned abrasion
conditions. Colorimetric analysis via C-Microscopy revealed a strong,
statistically significant relationship between abrasive parameters, microgroove
geometry (inclination angle, depth, and size), and optical spectral signatures,
which were then parametrized to achieve targeted control. This research
demonstrates a simple yet effective approach to color and reflectance
modification via microgroove engineering, offering a pathway to customized
material properties by uniquely coupling contact mechanics, surface morphology,
and colorimetry at the microscale level.

</details>


### [195] [Origin of trapped intralayer Wannier and charge-transfer excitons in moiré materials](https://arxiv.org/abs/2510.06137)
*Indrajit Maity,Johannes Lischner,Arash A. Mostofi,Ángel Rubio*

Main category: cond-mat.mtrl-sci

TL;DR: Continuum models and ab initio methods disagree on moiré excitons. This paper uses an atomistic Bethe-Salpeter equation approach with Wannier functions to resolve discrepancies, showing dielectric screening is crucial for WS2/WSe2 heterobilayers. The lowest-energy bright excitons can be either Wannier-like or charge-transfer-like depending on the material and encapsulation, demonstrating atomistic modeling's power for designing moiré materials.


<details>
  <summary>Details</summary>
Motivation: Continuum models used for studying moiré excitons are computationally efficient but often disagree with more accurate ab initio methods, necessitating a resolution to understand and engineer these materials for optoelectronics.

Method: An atomistic, quantum-mechanical framework based on the Bethe-Salpeter equation utilizing localized Wannier functions for the electronic structure was employed. The study also incorporated dielectric screening effects from hexagonal boron nitride (hBN) encapsulation.

Result: The inclusion of dielectric screening was found to be essential for reproducing experimental observations of moiré intralayer excitons in WS2/WSe2 heterobilayers. A competition between Wannier and charge transfer characters was revealed, influenced by stacking regions and tunable electron-hole interactions. The lowest-energy bright excitons were identified as Wannier-like in WS2/WSe2 heterobilayers but charge-transfer-like in hBN-encapsulated twisted WSe2 homobilayers, and Wannier-like in twisted WSe2 homobilayers without hBN.

Conclusion: Atomistic modeling, particularly using the Bethe-Salpeter equation with Wannier functions and considering dielectric screening, provides an efficient and powerful approach for designing and controlling excitonic phenomena in moiré materials, resolving previous discrepancies between different theoretical models and explaining experimental observations.

Abstract: Moir\'e materials offer a versatile platform for engineering excitons with
unprecedented control, promising next-generation optoelectronic applications.
While continuum models are widely used to study moir\'e excitons due to their
computational efficiency, they often disagree with ab initio many-body
approaches, as seen for intralayer excitons in WS$_2$/WSe$_2$ heterobilayers.
Here, we resolve these discrepancies using an atomistic, quantum-mechanical
framework based on the Bethe-Salpeter equation with localized Wannier functions
as the basis for the electronic structure. We show that inclusion of dielectric
screening due to hexagonal boron nitride (hBN) encapsulation is essential to
reproduce the full set of experimentally observed features of moir\'e
intralayer excitons. Our analysis reveals a competition between Wannier and
charge transfer characters, driven by variations between direct and indirect
band gaps at high symmetry stacking regions due to atomic relaxations and
environmentally tunable electron-hole interactions. Building on this insight,
we demonstrate that the lowest-energy bright excitons are Wannier-like in
WS2/WSe2 heterobilayers but charge-transfer-like in twisted WSe2 homobilayers,
despite having comparable moir\'e lengths when encapsulated in hBN. In the
absence of hBN encapsulation, the lowest-energy bright exciton in twisted
WSe$_2$ becomes Wannier-like. These results establish atomistic modeling as a
powerful and efficient approach for designing and controlling excitonic
phenomena in moir\'e materials.

</details>


### [196] [Bridging the Synthesizability Gap in Perovskites by Combining Computations, Literature Data, and PU Learning](https://arxiv.org/abs/2510.06166)
*Rushik Desai,Junyeong Ahn,Alejandro Strachan,Arun Mannodi-Kanakkithodi*

Main category: cond-mat.mtrl-sci

TL;DR: 本研究结合高通量DFT计算和实验数据，利用PU学习策略训练模型，预测卤化物和硫属化物钙钛矿的合成可行性。


<details>
  <summary>Details</summary>
Motivation: 现有DFT计算虽然能预测钙钛矿的光电性质，但难以预测其合成可行性，导致理论预测和实验实现的差距。因此，需要学习实验文献中钙钛矿组成-结构与合成可能性之间的关系。

Method: 结合高通量DFT数据和从文献收集的实验标签，采用PU学习策略，并利用多种材料描述符训练分类模型，以预测任何给定钙钛矿化合物的合成可行性。该框架基于DFT计算能量和已有类似合成化合物的存在情况，进行合成可能性的概率估计。

Result: 开发了一个能够预测钙钛矿合成可行性的框架，并提供了一个可通过FAIR nanoHUB工具访问的数据和模型。

Conclusion: 研究成功地结合了理论计算和实验数据，开发了一种预测钙钛矿合成可行性的新方法，为缩小理论与实验之间的差距提供了有效途径。

Abstract: Among emerging energy materials, halide and chalcogenide perovskites have
garnered significant attention over the last decade owing to the abundance of
their constituent species, low manufacturing costs, and their highly tunable
composition-structure-property space. Navigating the vast perovskite
compositional landscape is possible using density functional theory (DFT)
computations, but they are not easily extended to predictions of the
synthesizability of new materials and their properties. As a result, only a
limited number of compositions identified to have desirable optoelectronic
properties from these calculations have been realized experimentally. One way
to bridge this gap is by learning from the experimental literature about how
the perovskite composition-structure space relates to their likelihood of
laboratory synthesis. Here, we present our efforts in combining high-throughput
DFT data with experimental labels collected from the literature to train
classifier models employing various materials descriptors to forecast the
synthesizability of any given perovskite compound. Our framework utilizes the
positive and unlabeled (PU) learning strategy and makes probabilistic estimates
of the synthesis likelihood based on DFT- computed energies and the prior
existence of similar synthesized compounds. Our data and models can be readily
accessed via a Findable, Accessible, Interoperable, and Reproducible (FAIR)
nanoHUB tool.

</details>


<div id='cs.MA'></div>

# cs.MA [[Back]](#toc)

### [197] [AgentZero++: Modeling Fear-Based Behavior](https://arxiv.org/abs/2510.05185)
*Vrinda Malhotra,Jiaman Li,Nandini Pisupati*

Main category: cs.MA

TL;DR: AgentZero++ 是一个基于智能体（agent）的模型，它整合了认知、情感和社会机制来模拟空间分布式系统中的去中心化集体暴力。该模型在 Epstein 的 Agent_Zero 框架基础上进行了八项行为增强，包括基于年龄的冲动控制、基于记忆的风险估计、情感-认知耦合、内源性破坏半径、战或逃动力学、情感同质性、报复性损害和多智能体协调。这些增强使得智能体能够根据内部状态、过往经验和社会反馈进行适应，从而产生抗议不对称、升级周期和局部报复等涌现动力学。AgentZero++ 使用 Python 和 Mesa ABM 框架实现，支持模块化实验和可视化，以展示微观认知异质性如何塑造宏观冲突模式。研究结果表明，记忆、反应性和情感协调方面的微小变化可以通过反馈循环放大或抑制动荡。通过显式模拟情感阈值、身份驱动行为和自适应网络，该工作提供了一个灵活且可扩展的平台，用于分析情感传染和心理学基础的集体行动。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是创建一个能够模拟去中心化集体暴力，特别是其中认知、情感和社会机制相互作用的代理模型。通过扩展现有的 Agent_Zero 框架，研究旨在更深入地理解微观层面的个体行为如何影响宏观层面的冲突动态。

Method: 该研究提出了 AgentZero++ 模型，该模型基于 Epstein 的 Agent_Zero 框架，并增加了八项行为增强：基于年龄的冲动控制、基于记忆的风险估计、情感-认知耦合、内源性破坏半径、战或逃动力学、情感同质性、报复性损害和多智能体协调。该模型使用 Python 和 Mesa ABM 框架实现，允许模块化实验和可视化。

Result: AgentZero++ 能够产生涌现动力学，如抗议不对称、升级周期和局部报复。研究结果表明，记忆、反应性和情感协调方面的微小变化可以通过反馈循环对动荡产生放大或抑制作用。

Conclusion: AgentZero++ 提供了一个灵活且可扩展的平台，用于分析情感传染和心理学基础的集体行动。该模型通过显式模拟情感阈值、身份驱动行为和自适应网络，有助于理解微观认知异质性如何塑造宏观冲突模式。

Abstract: We present AgentZero++, an agent-based model that integrates cognitive,
emotional, and social mechanisms to simulate decentralized collective violence
in spatially distributed systems. Building on Epstein's Agent\_Zero framework,
we extend the original model with eight behavioral enhancements: age-based
impulse control; memory-based risk estimation; affect-cognition coupling;
endogenous destructive radius; fight-or-flight dynamics; affective homophily;
retaliatory damage; and multi-agent coordination. These additions allow agents
to adapt based on internal states, previous experiences, and social feedback,
producing emergent dynamics such as protest asymmetries, escalation cycles, and
localized retaliation. Implemented in Python using the Mesa ABM framework,
AgentZero++ enables modular experimentation and visualization of how
micro-level cognitive heterogeneity shapes macro-level conflict patterns. Our
results highlight how small variations in memory, reactivity, and affective
alignment can amplify or dampen unrest through feedback loops. By explicitly
modeling emotional thresholds, identity-driven behavior, and adaptive networks,
this work contributes a flexible and extensible platform for analyzing
affective contagion and psychologically grounded collective action.

</details>


### [198] [Emergent Coordination in Multi-Agent Language Models](https://arxiv.org/abs/2510.05174)
*Christoph Riedl*

Main category: cs.MA

TL;DR: 多智能体LLM系统可以通过提示工程从单纯的聚合体引导为具有更高阶结构的集体，这种结构可以通过信息论框架进行量化和区分。


<details>
  <summary>Details</summary>
Motivation: 区分多智能体LLM系统是独立的智能体集合还是具有更高阶结构的集成集体。

Method: 提出一个信息论框架，利用信息分解来量化和定位多智能体LLM系统中的动态涌现，区分虚假的 temporal coupling 和与性能相关的跨智能体协同。具体实现为对时间延迟互信息（TDMI）进行偏信息分解。

Result: 在简单的猜谜游戏中，控制组表现出强烈的 temporal synergy 但协调性较差。分配角色（persona）后，出现了与身份相关的区分。结合角色和“思考其他智能体的行为”的指令，则出现了与身份相关的区分和目标导向的互补性。

Conclusion: 多智能体LLM系统可以通过提示工程从单纯的聚合体引导为具有更高阶结构的集体。观察到的交互模式与人类集体智能的原则一致，即有效的绩效需要目标一致性和成员间的互补贡献。

Abstract: When are multi-agent LLM systems merely a collection of individual agents
versus an integrated collective with higher-order structure? We introduce an
information-theoretic framework to test -- in a purely data-driven way --
whether multi-agent systems show signs of higher-order structure. This
information decomposition lets us measure whether dynamical emergence is
present in multi-agent LLM systems, localize it, and distinguish spurious
temporal coupling from performance-relevant cross-agent synergy. We implement
both a practical criterion and an emergence capacity criterion operationalized
as partial information decomposition of time-delayed mutual information (TDMI).
We apply our framework to experiments using a simple guessing game without
direct agent communication and only minimal group-level feedback with three
randomized interventions. Groups in the control condition exhibit strong
temporal synergy but only little coordinated alignment across agents. Assigning
a persona to each agent introduces stable identity-linked differentiation.
Combining personas with an instruction to ``think about what other agents might
do'' shows identity-linked differentiation and goal-directed complementarity
across agents. Taken together, our framework establishes that multi-agent LLM
systems can be steered with prompt design from mere aggregates to higher-order
collectives. Our results are robust across emergence measures and entropy
estimators, and not explained by coordination-free baselines or temporal
dynamics alone. Without attributing human-like cognition to the agents, the
patterns of interaction we observe mirror well-established principles of
collective intelligence in human groups: effective performance requires both
alignment on shared objectives and complementary contributions across members.

</details>


### [199] [Agent+P: Guiding UI Agents via Symbolic Planning](https://arxiv.org/abs/2510.06042)
*Shang Ma,Xusheng Xiao,Yanfang Ye*

Main category: cs.MA

TL;DR: LLM UI agents are enhanced by AGENT+P, a framework using symbolic planning (UI Transition Graphs) to prevent hallucinations and optimize task completion.


<details>
  <summary>Details</summary>
Motivation: LLM-based UI agents struggle with long-horizon tasks due to a lack of understanding of global UI transition structures, leading to hallucinations.

Method: AGENT+P models app UI transitions as a UI Transition Graph (UTG), enabling a symbolic planner to generate optimal high-level plans. This plan guides the LLM agent, preventing redundant exploration and ensuring goal achievement. The framework is plug-and-play for existing UI agents.

Result: AGENT+P improves state-of-the-art UI agents' success rates by up to 14% and reduces action steps by 37.7% on the AndroidWorld benchmark.

Conclusion: AGENT+P effectively addresses LLM UI agent hallucinations in long-horizon tasks by incorporating symbolic planning, leading to improved performance and efficiency.

Abstract: Large Language Model (LLM)-based UI agents show great promise for UI
automation but often hallucinate in long-horizon tasks due to their lack of
understanding of the global UI transition structure. To address this, we
introduce AGENT+P, a novel framework that leverages symbolic planning to guide
LLM-based UI agents. Specifically, we model an app's UI transition structure as
a UI Transition Graph (UTG), which allows us to reformulate the UI automation
task as a pathfinding problem on the UTG. This further enables an off-the-shelf
symbolic planner to generate a provably correct and optimal high-level plan,
preventing the agent from redundant exploration and guiding the agent to
achieve the automation goals. AGENT+P is designed as a plug-and-play framework
to enhance existing UI agents. Evaluation on the AndroidWorld benchmark
demonstrates that AGENT+P improves the success rates of state-of-the-art UI
agents by up to 14% and reduces the action steps by 37.7%.

</details>


<div id='physics.app-ph'></div>

# physics.app-ph [[Back]](#toc)

### [200] [Field Free Spin-Orbit Torque Controlled Synapse and Stochastic Neuron Devices for Spintronic Boltzmann Neural Networks](https://arxiv.org/abs/2510.05616)
*Aijaz H. Lone,Meng Tang,Camelia Florica,Bin He,Jingkai Xu,Xixiang Zhang,Gianluca Setti*

Main category: physics.app-ph

TL;DR: 本研究提出了一种利用 CoFeB 薄膜系统实现无需外加磁场即可进行自旋轨道扭矩 (SOT) 控制的忆阻器和神经元器件，为全自旋电子类脑计算硬件的实际应用奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 实现无需外加磁场即可进行自旋轨道扭矩 (SOT) 控制的突触和神经元器件，并将其应用于类脑计算。

Method: 通过不对称器件设计和侧向刻槽来促进 CoFeB 薄膜中的畴壁成核、移动和钉扎/解钉扎，从而实现多重模拟、非易失性电阻状态，并构建了 SOT 驱动的随机神经元。

Result: 实现了具有增强的线性度和对称性的可编程、稳定的突触权重，并实现了电流可调的、类玻尔兹曼分布的概率开关行为的随机神经元。将这些器件集成到玻尔兹曼机中，在 MNIST 数据集上实现了超过 98% 的识别准确率，在 Fashion MNIST 数据集上实现了 86% 的识别准确率。

Conclusion: 本研究展示了一个无需外加磁场即可工作的突触和神经元器件的框架，为开发兼容的、全自旋电子的类脑计算硬件铺平了道路。

Abstract: Spintronics offers a promising approach to energy efficient neuromorphic
computing by integrating the functionalities of synapses and neurons within a
single platform. A major challenge, however, is achieving field-free spin orbit
torque SOT control over both synaptic and neuronal devices using an
industry-adopted spintronic materials stack. In this study, we present
field-free SOT spintronic synapses utilizing a CoFeB ferromagnetic thin film
system, where asymmetrical device design and specifically added lateral notches
in the CoFeB thin film facilitate effective domain wall DW nucleation,
movement, and pinning and depinning. This method yields multiple analog,
nonvolatile resistance states with enhanced linearity and symmetry, resulting
in programmable and stable synaptic weights. We provide a systematic
measurement approach to improve the linearity and symmetry of the synapses.
Additionally, we demonstrate nanoscale magnetic tunnel junctions MTJs that
function as SOT-driven stochastic neurons, exhibiting current-tunable,
Boltzmann-like probabilistic switching behavior, which provides an intrinsic
in-hardware Gibbs sampling capability. By integrating these synapses and
neurons into a Boltzmann machine complemented by a classifier layer, we achieve
recognition accuracies greater than 98 percent on the MNIST dataset and 86
percent on Fashion MNIST. This work establishes a framework for field free
synaptic and neuronal devices, setting the stage for practical,
materials-compatible, and all spintronic neuromorphic computing hardware.

</details>


### [201] [Artificial Synapse based on ULTRARAM Memory Device for Neuromorphic Applications](https://arxiv.org/abs/2510.05671)
*Abhishek Kumar,Peter D. Hodgson,Manus Hayne,Avirup Dasgupta*

Main category: physics.app-ph

TL;DR: 使用ULTRARAM内存设备和CMOS外围电路实现神经网络的片上训练和推理。


<details>
  <summary>Details</summary>
Motivation: 大型深度神经网络（DNN）的内存需求要求将突触权重值存储和更新在片外内存（如动态随机存取存储器）中，这会降低能效并延长训练时间。片上存储和更新权值的单片交叉阵列或伪交叉阵列利用模拟非易失性存储器，为高效加速DNN训练提供了机会。

Method: 提出了一种使用ULTRARAM内存设备突触阵列和互补金属氧化物半导体（CMOS）外围电路进行神经网络片上训练和推理的方法。利用基于物理学的紧凑型ULTRARAM内存设备模型来模拟浮栅（FG）中电荷的实时捕获/释放，并用于突触模拟。采用电路级宏模型来评估和基准化ULTRARAM突触核心在面积、延迟、能耗和准确性方面的片上学习性能。

Result: 与基于CMOS的设计相比，ULTRARAM突触核心在面积和能耗方面分别提高了1.8倍和1.52倍，训练准确率为91%。

Conclusion: ULTRARAM内存设备和CMOS外围电路的结合能够实现高效的神经网络片上训练和推理，并在性能上优于纯CMOS设计。

Abstract: The memory demands of large-scale deep neural networks (DNNs) require
synaptic weight values to be stored and updated in off-chip memory like dynamic
random-access memory, which reduces energy efficiency and increases training
time. Monolithic crossbar or pseudo-crossbar arrays using analog non-volatile
memories, which can store and update weights on-chip, present an opportunity to
efficiently accelerate DNN training. In this article, we present on-chip
training and inference of a neural network using an ULTRARAM memory
device-based synaptic array and complementary metal-oxide-semiconductor (CMOS)
peripheral circuits. ULTRARAM is a promising emerging memory exhibiting high
endurance (>10^7 P/E cycles), ultra-high retention (>1000 years), and ultra-low
switching energy per unit area. A physics-based compact model of ULTRARAM
memory device has been proposed to capture the real-time trapping/de-trapping
of charges in the floating gate (FG) and utilized for the synapse simulations.
A circuit-level macro-model is employed to evaluate and benchmark the on-chip
learning performance in terms of area, latency, energy, and accuracy of an
ULTRARAM synaptic core. In comparison to CMOS-based design, it demonstrates an
overall improvement in area and energy by 1.8x and 1.52x, respectively, with
91% of training accuracy.

</details>


### [202] [Multi-Channel Amplitude-Phase Asymmetric-Encrypted Janus Acoustic Meta-Holograms](https://arxiv.org/abs/2510.05789)
*Haohan Zeng,Zhenyu He,Tianxiang Zhang,Xiao Guo,Xinghao Hu,Youyu Mo,Tingting Li,Feilong Mao,Haiyan Fan,Xudong Fan,Weiwei Kan,Yifan Zhu,Hui Zhang,Guodong Yin,Badreddine Assouar*

Main category: physics.app-ph

TL;DR: 提出了一种多通道振幅-相位非对称加密的Janus声学超表面全息图，实现了双面全息图的生成、加密和解密，并提高了通信容量和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有的加密全息图主要在单通道（调制空间幅度）上工作，而本研究旨在提出一种能够处理多通道、振幅和相位信息，并能在超表面两侧生成全息图的系统。

Method: 提出了一种多通道振幅-相位非对称加密的Janus声学超表面全息图，并设计了一种灵活解耦的机制来操控双向声波的振幅和相位，实现了单输入、双面、四通道的非对称加密。

Result: 实现了单输入、双面、四通道的非对称加密，通信容量远超传统声学全息图，并通过基于数学问题的安全框架证明了其安全性。

Conclusion: 所提出的Janus声学超表面全息图系统在通信容量和安全性方面取得了显著突破，为多通道声场通信、非透明介质中的声学幻象和隐身等应用提供了新的可能性。

Abstract: Encrypted optical and acoustic meta-holograms only focus on the encrypted
hologram in a single channel, viz. modulating spatial amplitude to project a
holographic image. In this research, the unique concept of multi-channel
amplitude-phase asymmetric-encrypted Janus acoustic meta-holograms is proposed,
demonstrating remarkable capabilities of generating, encrypting, and decrypting
both amplitude and phase holographic images on both sides of a metascreen. The
flexible and decoupled manipulation mechanism for the amplitude-phase of the
bidirectional acoustic waves used in our concept offers multiple possibilities
to apply various encryption methods. In this work, our system enables
single-input, two-faced four-channel asymmetric encryption, which substantially
increase the communication capacity of conventional acoustic holograms, and
establish a security framework based on mathematical problem, proving its
security. Our work can lead to concrete applications including, but not limited
to, multi-channel acoustic field communications and acoustic illusion and
cloaking in non-transparent media.

</details>


### [203] [The Feasibility of Acoustophoresis Multimodal Control](https://arxiv.org/abs/2510.06116)
*Guilherme Perticarari,Dongjun Wu,Thierry Baasch*

Main category: physics.app-ph

TL;DR: 通过选择声学共振模式的数量和粒子的数量来操纵微流控设备中的粒子位置。


<details>
  <summary>Details</summary>
Motivation: 研究选择的共振模式数量 M 和粒子数量 P 如何影响操纵任务的成功概率 S，即声泳控制问题 (ACP)。

Method: 使用模拟来计算局部可控体积与状态空间体积的比率，并将其与成功概率 S 相关联。在无噪声的一维系统中，推导出 S ≈ 1 - P/M 的近似公式。在有噪声的一维和二维系统中，使用 Wendel 定理进行预测。

Result: 局部可控体积与状态空间体积的比率与成功概率 S 强相关。在无噪声的一维系统中，S ≈ 1 - P/M。在有噪声的系统中，Wendel 定理准确预测 S。M 和 P 之间的关系近似呈线性，只要 P/M 恒定，S 保持不变。成功模拟了多达 60 个粒子和 600 个模式的系统控制。

Conclusion: 局部可控体积与状态空间体积的比率是预测和优化声泳控制问题成功概率的有效指标。P/M 比率是决定成功概率的关键因素，并且该关系近似呈线性，允许在保持成功概率的同时扩展系统规模。

Abstract: Actuating the acoustic resonance modes of a microfluidic device containing
suspended particles (e.g., cells) allows for the manipulation of their
individual positions. In this work, we investigate how the number of resonance
modes $M$ chosen for actuation and the number of particles $P$ affect the
probability of success $S$ of manipulation tasks, denoted Acoustophoretic
Control Problems (ACPs). Using simulations, we show that the ratio of locally
controllable volume to the state-space volume correlates strongly with $S$.
This ratio can be efficiently computed from the pressure field geometry as it
does not involve solving a control problem, thus opening possibilities for
experimental and numerical device optimization routines. Further, we show
numerically that in noise-free 1D systems $S \approx 1 - P/M$, and that in
noisy 1D and 2D systems $S$ is accurately predicted by Wendel's Theorem. We
also show that the relationship between $M$ and $P$ for a given $S$ is
approximately linear, suggesting that as long as $P/M$ is constant, $S$ will
remain unchanged. We validate this finding by successfully simulating the
control of systems with up to $60$ particles with up to $600$ modes.

</details>


<div id='quant-ph'></div>

# quant-ph [[Back]](#toc)

### [204] [Randomness from causally independent processes](https://arxiv.org/abs/2510.05203)
*Martin Sandfuchs,Carla Ferradini,Renato Renner*

Main category: quant-ph

TL;DR: 即使两个独立信道处理的输入存在关联，也可以从它们的输出中提取均匀随机数。


<details>
  <summary>Details</summary>
Motivation: 本研究的动机是放松先前工作中对输出（条件）独立性的要求，允许处理可能相关的输入，并基于更实际和物理上可行的假设（如空间分离的信道）生成随机性。

Method: 本研究考虑了两个因果独立的信道，它们作用于可能相关的输入，产生随机输出X和Y。研究表明，如果这些信道产生了足够多的随机数，则可以从X和Y中提取均匀随机数。

Result: 该方法推广了先前仅限于X和Y（条件）独立性的结果。具体来说，研究允许处理可能相关的输入，并且由于信道的独立性可以通过空间分离来强制执行，因此可以实现更实际的随机数生成。

Conclusion: 本研究的结果表明，可以从两个因果独立信道（即使处理的输入存在关联）的输出来提取均匀随机数，从而在设备无关的随机性放大部分等应用中实现更实际的随机数生成。

Abstract: We consider a pair of causally independent processes, modelled as the tensor
product of two channels, acting on a possibly correlated input to produce
random outputs X and Y. We show that, assuming the processes produce a
sufficient amount of randomness, one can extract uniform randomness from X and
Y. This generalizes prior results, which assumed that X and Y are
(conditionally) independent. Note that in contrast to the independence of
quantum states, the independence of channels can be enforced through spacelike
separation. As a consequence, our results allow for the generation of
randomness under more practical and physically justifiable assumptions than
previously possible. We illustrate this with the example of device-independent
randomness amplification, where we can remove the constraint that the adversary
only has access to classical side information about the source.

</details>


### [205] [Self-dual bivariate bicycle codes with transversal Clifford gates](https://arxiv.org/abs/2510.05211)
*Zijian Liang,Yu-An Chen*

Main category: quant-ph

TL;DR: Bivariate bicycle codes are a new type of quantum memory that offer high encoding rates and transversal gates, outperforming existing surface and color codes.


<details>
  <summary>Details</summary>
Motivation: The paper aims to combine the advantages of bivariate bicycle codes (promising for high-threshold, low-overhead fault-tolerant quantum memories) and color codes (prominent self-dual CSS codes with transversal Clifford gates).

Method: The authors introduce a broad family of self-dual bivariate bicycle codes, enumerate weight-8 codes up to 200 physical qubits on twisted tori, and analyze their properties.

Result: The proposed codes achieve higher encoding rates than surface and color codes and admit transversal CNOT, Hadamard, and S gates. Specific examples of codes with parameters [[n,k,d]] are provided, such as [[16,4,4]], [[40,6,6]], [[56,6,8]], [[64,8,8]], [[120,8,12]], [[152,6,16]], and [[160,8,16]].

Conclusion: The newly introduced self-dual bivariate bicycle codes offer improved performance in terms of encoding rates and transversal gate operations compared to existing quantum memory codes.

Abstract: Bivariate bicycle codes are promising candidates for high-threshold,
low-overhead fault-tolerant quantum memories. Meanwhile, color codes are the
most prominent self-dual CSS codes, supporting transversal Clifford gates that
have been demonstrated experimentally. In this work, we combine these
advantages and introduce a broad family of self-dual bivariate bicycle codes.
These codes achieve higher encoding rates than surface and color codes while
admitting transversal CNOT, Hadamard, and $S$ gates. In particular, we
enumerate weight-8 self-dual bivariate bicycle codes with up to $n \leq 200$
physical qubits, realized on twisted tori that enhance code distance and
improve stabilizer locality. Representative examples include codes with
parameters $[[n,k,d]]$: $[[16,4,4]]$, $[[40,6,6]]$, $[[56,6,8]]$, $[[64,8,8]]$,
$[[120,8,12]]$, $[[152,6,16]]$, and $[[160,8,16]]$.

</details>


### [206] [Fault-tolerant interfaces for modular quantum computing on diverse qubit platforms](https://arxiv.org/abs/2510.05221)
*Frederik K. Marqversen,Gefen Baranes,Maxim Sirotin,Johannes Borregaard*

Main category: quant-ph

TL;DR: 模块化架构是实现容错量子计算的可扩展途径，但需要模块间实现高速、容错的接口。本文分析和比较了包括晶格手术、横向纠缠门以及基于码增长和逻辑蒸馏的新型增长-蒸馏协议在内的多种接口方法。在使用表面码的情况下，研究确定了在不同硬件参数（如门保真度、纠缠率和内存资源）下的最优接口策略，并估算了实现 $10^{-6}$ 和 $10^{-12}$ 逻辑错误率的要求。研究结果明确了接口何时会成为计算瓶颈，并为超导、原子和固态硬件的实验实现提供了指导。


<details>
  <summary>Details</summary>
Motivation: 为了实现可扩展的容错量子计算，需要建立模块化量子处理单元（QPU）之间的高速、容错接口。

Method: 通过分析和比较晶格手术、横向纠缠门以及基于码增长和逻辑蒸馏的新型增长-蒸馏协议，并使用表面码在不同的硬件参数下（如门保真度、纠缠率和内存资源）确定最优接口策略。

Result: 估算了实现 $10^{-6}$ 和 $10^{-12}$ 逻辑错误率所需的接口资源，并确定了接口何时成为计算瓶颈。

Conclusion: 本文为实现容错量子计算的模块化接口提供了指导，并确定了不同硬件实现下的最优策略。

Abstract: Modular architectures offer a scalable path toward fault-tolerant quantum
computing by interconnecting smaller quantum processing units (QPUs) provided
that high-rate, fault-tolerant interfaces can be realized across modules. We
present a comprehensive analysis and comparison of known and new methods for
establishing such interfaces, including lattice surgery, transversal gates, and
novel grow-and-distil protocols based on code growing and logical distillation.
Using the surface code, we identify optimal interface strategies across a wide
range of hardware parameters, such as gate fidelities, entangling rates, and
memory resources, and estimate the requirements to achieve logical error rates
of $10^{-6}$ and $10^{-12}$. Our results establish when the interface become a
bottleneck in the computation and provide guidance for experimental
implementations with superconducting, atomic, and solid-state hardware.

</details>


### [207] [Dynamical quantum codes and logic gates on a lattice with sparse connectivity](https://arxiv.org/abs/2510.05225)
*Dominic J. Williamson,Bence Hetényi*

Main category: quant-ph

TL;DR: 本文介绍了一种利用中途测量和最近邻门在最大顶点度为三的格点上实现拓扑码和其间逻辑门的操作方案。


<details>
  <summary>Details</summary>
Motivation: 利用现有量子比特和门操作实现拓扑量子计算。

Method: 介绍了Floquet码的实现方式，包括利用辅助量子比特进行重置以及同时实现一对Floquet码。此外，还展示了如何执行逻辑Clifford门来纠缠一对Floquet码，以及如何通过颜色码和一对Floquet码之间的切换来实现颜色码的表征提取。

Result: 实现了拓扑码的逻辑门操作，并提出了提高计算效率和稳定性的方法。

Conclusion: 提出的方案为在近期含噪声量子设备上实现容错量子计算提供了新的途径。

Abstract: We introduce several dynamical schemes that take advantage of mid-circuit
measurement and nearest-neighbor gates on a lattice with maximum vertex degree
three to implement topological codes and perform logic gates between them. We
first review examples of Floquet codes and their implementation with
nearest-neighbor gates and ancillary qubits. Next, we describe implementations
of these Floquet codes that make use of the ancillary qubits to reset all
qubits every measurement cycle. We then show how switching the role of data and
ancilla qubits allows a pair of Floquet codes to be implemented simultaneously.
We describe how to perform a logical Clifford gate to entangle a pair of
Floquet codes that are implemented in this way. Finally, we show how switching
between the color code and a pair of Floquet codes, via a depth-two circuit
followed by mid-circuit measurement, can be used to perform syndrome extraction
for the color code.

</details>


### [208] [Peaked quantum advantage using error correction](https://arxiv.org/abs/2510.05262)
*Abhinav Deshpande,Bill Fefferman,Soumik Ghosh,Michael Gullans,Dominik Hangleiter*

Main category: quant-ph

TL;DR: 量子优势实验的验证难题在于需要完整的经典模拟，这限制了实验的可验证范围。本文提出了“隐藏代码采样”方案，通过条件峰值实现高效验证，并证明了其采样在经典上是困难的，除非多项式阶层崩溃，同时提出了平均情况困难性的猜想。该方案基于量子纠错，有望实现下一代量子优势实验。


<details>
  <summary>Details</summary>
Motivation: 目前的量子优势实验在验证时需要完整的经典模拟，这使得实验的可验证范围恰好局限于可模拟的范围。因此，寻找能够进行经典验证的量子优势方案是一个重要问题。

Method: 提出了一种名为“隐藏代码采样”的新型量子优势方案，该方案的输出分布具有条件峰值，从而可以在比完整模拟短得多的时间内进行验证。同时，证明了除非多项式阶层崩溃，否则从输出分布中进行精确采样在经典上是困难的，并提出了关于平均情况困难性的猜想。该方案基于量子纠错的思想，相关的量子计算与量子容错电路密切相关，并可能实现横向部署。

Result: 设计了一种新的量子优势方案“隐藏代码采样”，其输出分布的条件峰值使得验证所需的经典模拟时间远少于完整模拟。证明了该方案的精确采样在经典上是困难的（除非P.H.崩溃），并提出了平均情况困难性的猜想。

Conclusion: “隐藏代码采样”方案为解决量子优势实验的验证难题提供了思路，通过利用输出分布的条件峰值实现了高效验证，并且在经典计算上具有难度。该方案基于量子纠错，有望实现下一代量子优势实验，并朝着完全量子容错的方向迈进。

Abstract: A key issue of current quantum advantage experiments is that their
verification requires a full classical simulation of the ideal computation.
This limits the regime in which the experiments can be verified to precisely
the regime in which they are also simulatable. An important outstanding
question is therefore to find quantum advantage schemes that are also
classically verifiable. We make progress on this question by designing a new
quantum advantage proposal--Hidden Code Sampling--whose output distribution is
conditionally peaked. These peaks enable verification in far less time than it
takes for full simulation. At the same time, we show that exactly sampling from
the output distribution is classically hard unless the polynomial hierarchy
collapses, and we propose a plausible conjecture regarding average-case
hardness. Our scheme is based on ideas from quantum error correction. The
required quantum computations are closely related to quantum fault-tolerant
circuits and can potentially be implemented transversally. Our proposal may
thus give rise to a next generation of quantum advantage experiments en route
to full quantum fault tolerance.

</details>


### [209] [Overshifted Parameter-Shift Rules: Optimizing Complex Quantum Systems with Few Measurements](https://arxiv.org/abs/2510.05289)
*Leonardo Banchi,Dominic Branford,Chetan Waghela*

Main category: quant-ph

TL;DR: 参数迁移规则的泛化框架，可用于任意量子门生成器，提高变分量子算法的表达能力和应用范围。


<details>
  <summary>Details</summary>
Motivation: 现有参数迁移规则仅适用于特定谱结构的量子门生成器，限制了其在变分量子算法中的应用。

Method: 提出一个泛化框架，能够计算任意量子门生成器（包括复杂多体相互作用和无限维系统）的梯度，并最小化测量开销。

Result: 成功将参数迁移规则扩展到更广泛的量子门生成器，能够处理更复杂的量子电路，并利用所有可用的硬件自由度。

Conclusion: 泛化的参数迁移规则框架能够增强变分量子算法的表达能力，并扩大其在量子化学、量子模拟和量子机器学习等领域的应用范围。

Abstract: Gradient-based optimization is a key ingredient of variational quantum
algorithms, with applications ranging from quantum machine learning to quantum
chemistry and simulation. The parameter-shift rule provides a hardware-friendly
method for evaluating gradients of expectation values with respect to circuit
parameters, but its applicability is limited to circuits whose gate generators
have a particular spectral structure. In this work, we present a generalized
framework that, with optimal minimum measurement overhead, extends parameter
shift rules beyond this restrictive setting to encompass basically arbitrary
gate generator, possibly made of complex multi-qubit interactions with unknown
spectrum and, in some settings, even infinite dimensional systems such as those
describing photonic devices or qubit-oscillator systems. Our generalization
enables the use of more expressive quantum circuits in variational quantum
optimization and enlarges its scope by harnessing all the available hardware
degrees of freedom.

</details>


### [210] [Optimised spectral purity of unfiltered photons via pump and nonlinearity shaping](https://arxiv.org/abs/2510.06196)
*Tommaso Faleo,Christopher L. Morrison,Roméo Beignon,Francesco Graffitti,Vikas Remesh,Stefan Frick,Alessandro Fedrizzi,Gregor Weihs,Robert Keil*

Main category: quant-ph

TL;DR: 通过结合高斯拟相位匹配和高斯泵浦光谱整形，优化了来自电信波长光源的光子光谱纯度和不可区分性，在没有光谱滤波的情况下，时间飞行光谱测量估计的上光谱纯度为99.9272(6)%，在具有独立光源的双光子干涉实验中实现了高达98.5(8)%的可见度。


<details>
  <summary>Details</summary>
Motivation: 光子量子技术依赖于高效生成和干涉不可区分的光子。

Method: 结合高斯拟相位匹配和高斯泵浦光谱整形，优化了来自电信波长光源的光子光谱纯度和不可区分性。使用时间飞行光谱测量估计上光谱纯度，并进行双光子干涉实验。

Result: 在没有光谱滤波的情况下，时间飞行光谱测量估计的上光谱纯度为99.9272(6)%，在具有独立光源的双光子干涉实验中实现了高达98.5(8)%的可见度。

Conclusion: 所提出的方法在没有光谱滤波的情况下，能够显著提高光子光谱纯度和不可区分性，为光子量子技术的发展提供了有前景的解决方案。

Abstract: Photonic quantum technologies rely on the efficient generation and
interference of indistinguishable photons. Exceptional achievements in this
respect have been obtained by domain engineering of quasi-phase-matched
parametric down-conversion sources, demonstrating high two-photon interference
visibility using only moderate bandpass spectral filtering. Here, we optimised
the spectral purity and indistinguishability of photons from telecom-wavelength
sources by combining Gaussian quasi-phase-matching with Gaussian pump spectral
shaping. Without spectral filtering, we used time-of-flight spectrometry to
estimate an upper bound spectral purity of 99.9272(6)%, and achieved
visibilities of up to 98.5(8)% in two-photon interference experiments with
independent sources.

</details>


### [211] [A New Approach to Arguments of Quantum Knowledge](https://arxiv.org/abs/2510.05316)
*James Bartusek,Ruta Jawale,Justin Raizes,Kabir Tomer*

Main category: quant-ph

TL;DR: 提出了一种具有透明设置和可提取性的非交互式零知识证明系统，用于QMA问题。


<details>
  <summary>Details</summary>
Motivation: 与现有的QMA证明系统相比，该系统具有更简单的设置要求（仅需均匀随机字符串）和可提取性。

Method: 该系统利用了Bartusek等人提出的coset状态认证方案，并设计了一种具有“强完备性”的新型ZX QMA验证器。安全性证明依赖于一种后量子不可区分模糊器的启发式应用，称为“规避组合启发式”。此外，还提出了一种使用哈希函数和函数加密替代模糊器的方法。

Result: 成功构建了一个满足透明设置和可提取性要求的QMA非交互式零知识证明系统。该系统还解决了将仅在经典Oracle模型中已知的多项量子密码学结果转化为量子伪随机Oracle模型中的问题。

Conclusion: 该研究为QMA问题的零知识证明提供了一种更实用、更安全的方法，并对量子密码学领域做出了贡献，特别是将经典Oracle模型的结果推广到量子伪随机Oracle模型。

Abstract: We construct a non-interactive zero-knowledge argument system for QMA with
the following properties of interest.
  1. Transparent setup. Our protocol only requires a uniformly random string
(URS) setup. The only prior (publicly-verifiable) NIZK for QMA (Bartusek and
Malavolta, ITCS 2022) requires an entire obfuscated program as the common
reference string.
  2. Extractability. Valid QMA witnesses can be extracted directly from our
accepting proofs. That is, we obtain an argument of knowledge, which was
previously only known in a secret parameters model (Coladangelo, Vidick, and
Zhang, CRYPTO 2020).
  At the heart of our construction is a novel application of the coset state
authentication scheme from (Bartusek, Brakerski, and Vaikuntanathan, STOC 2024)
to the setting of QMA verification. Along the way, we establish new properties
of the authentication scheme, and design a new type of ZX QMA verifier with
``strong completeness.'' The security of our construction rests on the
heuristic use of a post-quantum indistinguishability obfuscator. However,
rather than rely on the full-fledged classical oracle model (i.e. ideal
obfuscation), we isolate a particular game-based property of the obfuscator
that suffices for our proof, which we dub the evasive composability heuristic.
Going a step further, we show how to replace the heuristic use of an obfuscator
with the heuristic use of a hash function (plus sub-exponentially secure
functional encryption). We accomplish this by establishing security of the
ideal obfuscation scheme of Jain, Lin, Luo, and Wichs (CRYPTO 2023) in the
quantum pseudorandom oracle model, which can be heuristically instantiated with
a hash function. This result is of independent interest, and allows us to
translate several quantum-cryptographic results that were only known in the
classical oracle model to results in the quantum pseudorandom oracle model.

</details>


### [212] [Levitated optomechanics with cylindrically polarized vortex beams](https://arxiv.org/abs/2510.05384)
*Felipe Almeida,Peter Barker*

Main category: quant-ph

TL;DR: 通过使用具有径向和方位角极化的光学圆柱极化涡旋光束，可以显著减少光学悬浮和冷却纳米粒子的后坐力加热，并实现更广泛的颗粒尺寸的稳定三维光学陷阱。


<details>
  <summary>Details</summary>
Motivation: 光学悬浮和冷却纳米粒子作为一种新的量子系统，在创建非经典运动状态和量子限制传感方面的应用受到后坐力和体块加热的根本限制。

Method: 研究使用具有径向和方位角极化的光学圆柱极化涡旋光束创建稳定的三维光学陷阱，并将其与传统的单高斯光束镊子进行比较，以评估后坐力加热的减少效果。此外，还研究了这些光束在捕获瑞利体制之外的较大粒子（使用明暗镊子）以及通过改变激光波长或粒子尺寸来创建非线性排斥势以用于创建非经典运动状态的可能性。

Result: 与传统的单高斯光束镊子相比，后坐力加热最多可减少一个数量级。这些光束还可以捕获瑞利体制之外的较大粒子，并实现明暗镊子陷阱。通过改变激光波长或粒子尺寸，可以创建用于创建非经典运动状态的非线性排斥势。

Conclusion: 研究表明，使用具有径向和方位角极化的光学圆柱极化涡旋光束可以显著减少光学悬浮和冷却纳米粒子的后坐力加热，并实现更广泛的颗粒尺寸的稳定三维光学陷阱，为创建非经典运动状态和量子限制传感开辟了新的可能性。

Abstract: Optically levitated and cooled nanoparticles are a new quantum system whose
application to the creation of non-classical states of motion and quantum
limited sensing is fundamentally limited by recoil and bulk heating. We study
the creation of stable 3D optical traps using optical cylindrically polarized
vortex beams with radial and azimuthal polarization and show that a significant
reduction in recoil heating by up to an order of magnitude can be achieved when
compared with conventional single Gaussian beam tweezers. Additionally these
beams allow trapping of larger particles outside the Rayleigh regime using both
bright and dark tweezer trapping with reduced recoil heating. By changing the
wavelength of the trapping laser, or the size of the particles, non-linear and
repulsive potentials of interest for the creation of non-classical states of
motion can also be created.

</details>


### [213] [Quantum Concept Music Score from Quantum Picturalism: Musical Incarnation of a Bell-Pair under Measurements](https://arxiv.org/abs/2510.05391)
*Rakhat-Bi Abdyssagin,Bob Coecke*

Main category: quant-ph

TL;DR: 我们提出了一种名为量子概念音乐（QCM）的新型量子音乐语言和理论，它基于范畴量子力学（CQM）及其图示化方法量子图示主义（QPict），并利用ZX-演算。QCM能够以直观、严谨且机械的方式将量子现象转化为音乐。我们还提出了一个音乐家互动的乐谱示例，并概述了其现场演奏的可能性。与主要依赖线性表示的西方古典音乐记谱法不同，QCM 强调音乐的根本关系维度，并对音乐创作、现场演奏和自动化音乐（如AI生成）产生深远影响。QCM 是一种强大的音乐形式，能够捕捉音乐内部和外部的交互性质，并超越西方古典音乐记谱法的界限，适用于多种音乐体裁和方向。


<details>
  <summary>Details</summary>
Motivation: 介绍一种新的音乐语言和理论——量子概念音乐（QCM），旨在解决传统音乐记谱法在线性表示方面存在的局限性，并提供一种能够捕捉音乐交互性质的强大而高效的形式。

Method: 提出了一种基于范畴量子力学（CQM）及其图示化方法量子图示主义（QPict）的新型音乐形式，并利用ZX-演算。通过显式表示音乐构成、演奏和自动化中的关键概念关系，使量子现象能够以直观、严谨和机械的方式转化为音乐。

Result: 提出了一种新的音乐形式QCM，能够直接将量子现象转化为音乐，并提出了一个音乐家互动的乐谱示例，展示了其现场演奏的可能性。该方法影响了音乐创作、现场演奏和自动化音乐。

Conclusion: QCM 是一种强大而高效的新型音乐形式，能够捕捉音乐的交互性质，并超越西方古典音乐记谱法的局限性，适用于多种音乐体裁和方向。

Abstract: We initiate the development of a new language and theory for quantum music,
to which we refer as Quantum Concept Music (QCM). This new music formalism is
based on Categorical Quantum Mechanics (CQM), and more specifically, its
diagrammatic incarnation Quantum Picturalism (QPict), which is heavily based on
ZX-calculus. In fact, it is naturally inherited from CQM/QPict. At its heart is
the explicit notational representation of relations that exist within and
between the key concepts of music composition, performance, and automation. QCM
also enables one to directly translate quantum phenomena into music
compositions in a both intuitively obvious, rigorous and mechanical manner.
  Following this pattern, we propose a score for musicians interacting like a
Bell-pair under measurement, and outline examples of how it could be live
performed. While most of the Western classical music notation has heavily
relied on linear representation of music - which does not always adequately
capture the nature of music - our approach is distinct by highlighting the
fundamental relational dimension of music. In addition, this quantum-based
technique not only influences the music at the profound level of composition,
but also has a direct impact on a live performance, and also provides a new
template for automating music, e.g.~in the context of AI-generation.
  All together, we initiate the creation of new music formalism that is
powerful and efficient in capturing the interactive nature of music, both in
terms of internal and external interactions, and goes beyond the boundaries of
Western classical music notation, which allows to use it in many different
genres and directions.

</details>


### [214] [On Limits on the Provable Consequences of Quantum Pseudorandomness](https://arxiv.org/abs/2510.05393)
*Samuel Bouaziz--Ermann,Minki Hhan,Garazi Muguruza,Quoc-Huy Vu*

Main category: quant-ph

TL;DR: 量子伪随机性（包括PRU、PRSG、PRSFG）在不同定义之间可能不存在等价性，这与经典伪随机性不同。研究表明，某些量子伪随机性可能难以从其他类型构建，除非某些数学猜想不成立。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨量子伪随机性的不同概念（PRUs、PRSGs、PRSFGs）之间的关系，并与经典伪随机性进行对比，以确定它们之间是否存在等价性，并为实现完全的黑盒分离提供方向。

Method: 通过构建特定的酉预言机（unitary oracle）来研究不同量子伪随机性之间的关系。具体包括：1. 构造一个PRSFG存在但PRU（无辅助比特）不存在的预言机。2. 在一个关于等周不等式的猜想下，构造一个PRSFG存在但证明QPRG存在的难度等同于证明BQP != QCMA的预言机。3. 在相同猜想下，证明从短输出PRSFG构建长输出PRSG的某种自然方法是不可行的。

Result: 1. 发现了一个PRSFG存在但PRU不存在的预言机，并提出若能证明PRU算法的结构属性，则可推广至一般PRU。2. 证明了在特定猜想下，PRSG输出长度的逆多项式误差可能是固有的。3. 证明了在特定猜想下，从短输出PRSFG构建长输出PRSG的某种自然方法是不可行的。

Conclusion: 量子伪随机性的不同概念之间可能不存在等价性，这与经典伪随机性不同。研究为理解和区分这些概念提供了新的视角，并指出了在量子计算中构建和分析伪随机性的潜在挑战和限制。

Abstract: There are various notions of quantum pseudorandomness, such as pseudorandom
unitaries (PRUs), pseudorandom state generators (PRSGs) and pseudorandom
function-like state generators (PRSFGs). Unlike the different notions of
classical pseudorandomness, which are known to be existentially equivalent to
each other, the relation between quantum pseudorandomness has yet to be fully
established.
  We present some evidence suggesting that some quantum pseudorandomness is
unlikely to be constructed from the others, or at least is hard to construct
unless some conjectures are false. This indicates that quantum pseudorandomness
could behave quite differently from classical pseudorandomness. We study new
oracle worlds where one quantum pseudorandomness exists but another
pseudorandomness does not under some assumptions or constraints, and provide
potential directions to achieve the full black-box separation. More precisely:
  - We give a unitary oracle relative to which PRFSGs exist but PRUs without
using ancilla do not. This can be extended to the general PRUs if we can prove
a structural property of the PRU algorithm.
  - Assuming an isoperimetric inequality-style conjecture, we show a unitary
oracle world where log-length output PRFSGs exist but proving the existence of
quantum-computable pseudorandom generators (QPRGs) with negligible correctness
error is as hard as proving that ${\sf BQP}\neq {\sf QCMA}$. This result
suggests that the inverse-polynomial error in the state of the art construction
of QPRGs from log-length PRSGs is inherent.
  - Assuming the same conjecture, we prove that some natural way of
constructing super-log-length output PRSGs from log-length output PRFSGs is
impossible. This partly complements the known hardness of shrinking the PRSG
output lengths. Along the way, we also discuss other potential approaches to
extend the PRSG output lengths.

</details>


### [215] [Efficient learning of bosonic Gaussian unitaries](https://arxiv.org/abs/2510.05531)
*Marco Fanizza,Vishnu Iyer,Junseo Lee,Antonio A. Mele,Francesco A. Mele*

Main category: quant-ph

TL;DR: 该研究提出了一种高效学习量子光学干涉和量子纠错方案中的玻色子高斯幺正算法。


<details>
  <summary>Details</summary>
Motivation: 需要一种能够高效学习玻色子高斯幺正的方法，因为它们是量子技术的基础。

Method: 提出了一种时间高效的算法，使用相干和压缩探测、线性光学元件和零/单边探测。通过有效的经典后处理和辛正则化来估计幺正。

Result: 该算法能够以能量约束的菱形距离来衡量，达到小的最坏情况误差。查询复杂度与模式数量、目标精度以及能量参数成多项式关系。在无限输入能量的极限下，使用 2m+2 次查询即可达到任意高精度。

Conclusion: 这是第一个可证明高效学习多参数连续变量幺正的方法。

Abstract: Bosonic Gaussian unitaries are fundamental building blocks of central
continuous-variable quantum technologies such as quantum-optic interferometry
and bosonic error-correction schemes. In this work, we present the first
time-efficient algorithm for learning bosonic Gaussian unitaries with a
rigorous analysis. Our algorithm produces an estimate of the unknown unitary
that is accurate to small worst-case error, measured by the physically
motivated energy-constrained diamond distance. Its runtime and query complexity
scale polynomially with the number of modes, the inverse target accuracy, and
natural energy parameters quantifying the allowed input energy and the
unitary's output-energy growth.
  The protocol uses only experimentally friendly photonic resources: coherent
and squeezed probes, passive linear optics, and heterodyne/homodyne detection.
We then employ an efficient classical post-processing routine that leverages a
symplectic regularization step to project matrix estimates onto the symplectic
group. In the limit of unbounded input energy, our procedure attains
arbitrarily high precision using only $2m+2$ queries, where $m$ is the number
of modes. To our knowledge, this is the first provably efficient learning
algorithm for a multiparameter family of continuous-variable unitaries.

</details>


### [216] [Observation of Genuine Tripartite Non-Gaussian Entanglement from a Superconducting Three-Photon Spontaneous Parametric Down-Conversion Source](https://arxiv.org/abs/2510.05405)
*Benjamin Jarvis-Frain,Andy Schang,Fernando Quijandría,Ibrahim Nsanzineza,Dmytro Dubyna,C. W. Sandbo Chang,Franco Nori,C. M. Wilson*

Main category: quant-ph

TL;DR: 通过超导参量腔和传输线中的三光子自发参量下转换（3P-SPDC）首次观察到真正三方非高斯纠缠。


<details>
  <summary>Details</summary>
Motivation: 在量子光学领域，纠缠光子的产生是许多关键实验和技术的重要资源。虽然双光子SPDC已被广泛研究，但三光子SPDC产生的纠缠状态仍有待证实。

Method: 利用由超导参量腔耦合到传输线组成的3P-SPDC源，并通过测量三模关联函数构建纠缠判据来研究非高斯三方纠缠。

Result: 观察到真正的三方非高斯纠缠，纠缠判据的违反程度达到统计噪声的23个标准差，并与理论预测的扩展规律高度吻合。同时，研究了用于定义光子模式的时间函数对纠缠判据值的影响。

Conclusion: 首次在3P-SPDC源中观察到真正的三方非高斯纠缠，并提出了一种基于三模关联函数的纠缠判据来验证和量化这种纠缠。

Abstract: The generation of entangled photons through Spontaneous Parametric
Down-Conversion (SPDC) is a critical resource for many key experiments and
technologies in the domain of quantum optics. Historically, SPDC was limited to
the generation of photon pairs. However, the use of the strong nonlinearities
in circuit quantum electrodynamics has recently enabled the observation of
Three-Photon SPDC (3P-SPDC). Despite great interest in the entanglement
structure of the resultant states, entanglement between photon triplets
produced by a 3P-SPDC source has still has not been confirmed. Here, we report
on the observation of genuine tripartite non-Gaussian entanglement in the
steady-state output field of a 3P-SPDC source consisting of a superconducting
parametric cavity coupled to a transmission line. We study this non-Gaussian
tripartite entanglement using an entanglement witness built from three-mode
correlation functions, and observe a maximum violation of the bound by 23
standard deviations of the statistical noise. Furthermore, we find strong
agreement between the observed and the analytically predicted scaling of the
entanglement witness. We then explore the impact of the temporal function used
to define the photon mode on the observed value of the entanglement witness.

</details>


### [217] [A New Quantum Linear System Algorithm Beyond the Condition Number and Its Application to Solving Multivariate Polynomial Systems](https://arxiv.org/abs/2510.05588)
*Jianqiang Li*

Main category: quant-ph

TL;DR: 本论文提出了一种新的量子线性系统（QLS）算法，该算法通过利用右侧向量b的结构来提高性能。与现有算法不同，新算法的运行时间不依赖于条件数，而是依赖于稀疏性、逆精度和实例相关参数。此外，该算法还引入了一种新的右侧重缩放技术，并成功应用于解决多元多项式系统，特别是最大独立集（MIS）问题，有望在某些条件下实现多项式时间复杂度。


<details>
  <summary>Details</summary>
Motivation: 现有量子线性系统（QLS）算法的运行时间与条件数、稀疏性和逆精度成线性关系，但忽略了右侧向量b的结构信息，而这些信息可能对算法性能产生重要影响。

Method: 提出一种新的QLS算法，该算法显式利用右侧向量b的结构。算法运行时间依赖于增广矩阵H的稀疏性、逆精度、解的范数以及新的实例相关参数。同时，提出一种结构感知重缩放技术，将原方程转化为ADz=b。

Result: 新算法的运行时间与增广矩阵H的稀疏性、逆精度、解的范数以及新的实例相关参数成多项式关系。将该算法应用于解决多元多项式系统，特别是最大独立集（MIS）问题，并证明在某些条件下，该量子算法可以实现多项式时间复杂度。

Conclusion: 该研究提出了一种新的、实例感知的QLS算法和重缩放技术，该技术能够利用右侧向量b的结构信息，从而在更广泛的参数范围内解决QLS问题。该方法在解决多元多项式系统方面取得了突破，特别是为最大独立集问题提供了一种潜在的多项式时间量子算法。

Abstract: Given a matrix $A$ of dimension $M \times N$ and a vector $\vec{b}$, the
quantum linear system (QLS) problem asks for the preparation of a quantum state
$|\vec{y}\rangle$ proportional to the solution of $A\vec{y} = \vec{b}$.
Existing QLS algorithms have runtimes that scale linearly with the condition
number $\kappa(A)$, the sparsity of $A$, and logarithmically with inverse
precision, but often overlook structural properties of $\vec{b}$, whose
alignment with $A$'s eigenspaces can greatly affect performance.
  In this work, we present a new QLS algorithm that explicitly leverages the
structure of the right-hand side vector $\vec{b}$. The runtime of our algorithm
depends polynomially on the sparsity of the augmented matrix $H = [A,
-\vec{b}]$, the inverse precision, the $\ell_2$ norm of the solution $\vec{y} =
A^+ \vec{b}$, and a new instance-dependent parameter \[ ET= \sum_{i=1}^M p_i^2
\cdot d_i, \] where $\vec{p} = (AA^{\top})^+ \vec{b}$, and $d_i$ denotes the
squared $\ell_2$ norm of the $i$-th row of $H$. We also introduce a
structure-aware rescaling technique tailored to the solution $\vec{y} = A^+
\vec{b}$. Unlike left preconditioning methods, which transform the linear
system to $DA\vec{y} = D\vec{b}$, our approach applies a right rescaling
matrix, reformulating the linear system as $AD\vec{z} = \vec{b}$.
  As an application of our instance-aware QLS algorithm and new rescaling
scheme, we develop a quantum algorithm for solving multivariate polynomial
systems in regimes where prior QLS-based methods fail. This yields an
end-to-end framework applicable to a broad class of problems. In particular, we
apply it to the maximum independent set (MIS) problem, formulated as a special
case of a polynomial system, and show through detailed analysis that, under
certain conditions, our quantum algorithm for MIS runs in polynomial time.

</details>


### [218] [Photoswitchable radicals as reporter spins for quantum sensing with spin defects in diamond](https://arxiv.org/abs/2510.05406)
*Lakshmy Priya Ajayakumar,David J. Durden,Aksshay Nandakumar Regeni,Mingcai Xie,Swastik Hegde,Gustavo Aldas,Kyle Haggard,Mikael P. Backlund*

Main category: quant-ph

TL;DR: 利用金刚石表面光还原产生的罗丹明衍生物染料形成的自由基阴离子作为报告自旋，以克服NV中心磁传感中目标信号强度随距离衰减的挑战。


<details>
  <summary>Details</summary>
Motivation: 金刚石中NV中心进行纳米尺度磁传感时，目标信号强度随距离的快速衰减限制了灵敏度和空间分辨率。

Method: 生成自由基阴离子，进行相干操纵和单、浅层NV中心读出，并观察到光激活自旋的局域磁环境的异质性。

Result: 实验证明了自由基阴离子的相干操纵和检测，并观察到局域磁环境的异质性。

Conclusion: 该方法能够进行相关的纳米尺度磁和光学成像，并为单分子磁共振研究开辟了新途径。

Abstract: The rapid decay of target signal strength with distance from the sensor
presents a key challenge in nanoscale magnetic sensing with nitrogen-vacancy
(NV) centers in diamond, limiting both sensitivity and spatial resolution. Here
we introduce a strategy to overcome this limitation by using radical anions
formed from rhodamine-derived dyes as reporter spins localized to the diamond
surface. These radicals, generated through photoreduction, are optically
identifiable and stable on timescales exceeding an hour. We experimentally
demonstrate their coherent manipulation and detection using single, shallow NV
centers for readout. We observe heterogeneity in the local magnetic
environments of the photoactivated spins from site to site, likely due to
variations in inter-radical couplings across our measurements. Looking forward,
our approach enables correlative nanoscale magnetic and optical imaging, and
opens new pathways toward single-molecule magnetic resonance studies.

</details>


### [219] [Fermionic versus Spin Baths in non-Interacting Transport Models](https://arxiv.org/abs/2510.06027)
*Muhammad Zia,Moritz Cygorek,Erik M. Gauger,Brendon W. Lovett*

Main category: quant-ph

TL;DR: 费米子反对易位在非相互作用共振能级模型中影响输运现象，研究了费米子库和自旋库在系统-库耦合高阶项中的差异，特别是在小到中等库尺寸下，这些差异会逐渐减小，直至趋于马尔可夫近似的稳态。


<details>
  <summary>Details</summary>
Motivation: 研究费米子反对易位如何影响非相互作用共振能级模型中的输运现象，比较费米子库和自旋库的差异。

Method: 使用精确对角化和主方程的微扰展开，比较了费米子库和自旋库的差异。

Result: 发现自旋库和费米子库在约简动力学上存在差异，即使粒子不相互作用且模型强制执行局部泡利阻塞。这种差异源于系统-库耦合中的高阶项，费米子反对易位在更高阶相关性中引入了交换符号。所有二阶贡献都一致。偏差在小到中等库尺寸下最大，在有效马尔可夫近似下趋于消失。

Conclusion: 确定了自旋库何时可以替代费米子库，反之亦然。

Abstract: We investigate how fermionic anticommutation shapes transport in a
noninteracting resonant-level model where a single central site is coupled to
an environment. To this end, we compare a fermionic reservoir with a bath of
spin-half modes using exact diagonalization and a perturbative expansion of the
master equation to identify the differences. Notably, we find different reduced
dynamics for the spin and fermionic baths even though the particles remain
noninteracting and both models enforce local Pauli blocking. These differences
originate from higher-order terms in the system-bath coupling, where fermionic
anticommutation introduces exchange signs in higher-order correlations. By
contrast, all second-order contributions, set solely by two-point correlators,
coincide. Deviations are largest for small to intermediate bath sizes, fading
in the effectively Markovian regime where higher-order corrections are
negligible. These results identify when spin baths can be a substitute for
fermionic reservoirs and vice versa.

</details>


### [220] [Non-iid hypothesis testing: from classical to quantum](https://arxiv.org/abs/2510.06147)
*Giacomo De Palma,Marco Fanizza,Connor Mowry,Ryan O'Donnell*

Main category: quant-ph

TL;DR: 本研究探讨了非独立同分布（non-identically distributed）设定下的假设检验问题，包括经典概率分布和量子态。在经典情况下，研究者发现当样本数量 $c=2$ 且 $T 	o 	ext{inf}$ 时，可以在 $d$ 维空间中区分平均分布与目标分布，其样本复杂度接近最优 iid 情形。在量子态情况下，研究者发现仅需单个样本（$c=1$）即可在 $d$ 维空间中区分平均量子态与目标量子态，样本复杂度为 $T 	o 	ext{inf}$，这在经典情况下是不可能的。此外，研究者还提出了量子 Efron-Stein 不等式和分解。


<details>
  <summary>Details</summary>
Motivation: 探讨非独立同分布（non-identically distributed）设定下的假设检验问题，特别是在经典概率分布和量子态两种场景下的理论界限和可能出现的异常现象。

Method: 1. 经典概率分布假设检验：在非独立同分布设定下，使用 $c=2$ 个样本，分析区分平均分布 $p_{\mathrm{avg}}$ 与目标分布 $q$ 的样本复杂度，并与最优 iid 情形进行比较。2. 量子态假设检验：在非独立同分布设定下，使用 $c=1$ 个样本，研究区分平均量子态 $\rho_{\mathrm{avg}}$ 与目标量子态 $\sigma$ 的样本复杂度，并揭示了与经典情况不同的现象。3. 引入量子 Efron-Stein 不等式和分解作为技术工具。

Result: 1. 经典情况：当 $c=2$ 且 $T 	o 	ext{inf}$ 时，可以在 $d$ 维空间中区分平均分布与目标分布，样本复杂度接近最优 iid 情形。2. 量子情况：当 $c=1$ 且 $T 	o 	ext{inf}$ 时，可以在 $d$ 维空间中区分平均量子态与目标量子态，样本复杂度为 $T 	o 	ext{inf}$，优于经典情况下的 $c=1$ 样本复杂度。3. 提出了量子 Efron-Stein 不等式和分解。

Conclusion: 非独立同分布设定下的假设检验问题在量子领域存在不同于经典领域的现象，仅需单个样本即可达到与 iid 情形相似的区分精度。所提出的量子 Efron-Stein 工具可能具有独立的研究价值。

Abstract: We study hypothesis testing (aka state certification) in the non-identically
distributed setting. A recent work (Garg et al. 2023) considered the classical
case, in which one is given (independent) samples from $T$ unknown probability
distributions $p_1, \dots, p_T$ on $[d] = \{1, 2, \dots, d\}$, and one wishes
to accept/reject the hypothesis that their average $p_{\mathrm{avg}}$ equals a
known hypothesis distribution $q$. Garg et al. showed that if one has just $c =
2$ samples from each $p_i$, and provided $T \gg \frac{\sqrt{d}}{\epsilon^2} +
\frac{1}{\epsilon^4}$, one can (whp) distinguish $p_{\mathrm{avg}} = q$ from
$d_{\mathrm{TV}}(p_{\mathrm{avg}},q) > \epsilon$. This nearly matches the
optimal result for the classical iid setting (namely, $T \gg
\frac{\sqrt{d}}{\epsilon^2}$). Besides optimally improving this result (and
generalizing to tolerant testing with more stringent distance measures), we
study the analogous problem of hypothesis testing for non-identical quantum
states. Here we uncover an unexpected phenomenon: for any $d$-dimensional
hypothesis state $\sigma$, and given just a single copy ($c = 1$) of each state
$\rho_1, \dots, \rho_T$, one can distinguish $\rho_{\mathrm{avg}} = \sigma$
from $D_{\mathrm{tr}}(\rho_{\mathrm{avg}},\sigma) > \epsilon$ provided $T \gg
d/\epsilon^2$. (Again, we generalize to tolerant testing with more stringent
distance measures.) This matches the optimal result for the iid case, which is
surprising because doing this with $c = 1$ is provably impossible in the
classical case. We also show that the analogous phenomenon happens for the
non-iid extension of identity testing between unknown states. A technical tool
we introduce may be of independent interest: an Efron-Stein inequality, and
more generally an Efron-Stein decomposition, in the quantum setting.

</details>


### [221] [AC magnetometry in the strong drive regime with NV centers in diamond](https://arxiv.org/abs/2510.05471)
*Katrijn Everaert,Saipriya Satyajit,Jiashen Tang,Zechuan Yin,Xiechen Zheng,Jner Tzern Oon,Connor A. Hart,John W. Blanchard,Ronald L. Walsworth*

Main category: quant-ph

TL;DR: 通过相位调制协议 SIPHT，将金刚石 NV 磁力计的测量范围扩展到强驱动场区域，即使在驱动场强度接近或大于 NV 微波脉冲拉比强度时，也能有效测量材料的磁响应，特别是对相位相关的信号进行测量，其在材料科学和无损检测领域具有潜在应用价值。


<details>
  <summary>Details</summary>
Motivation: 传统的金刚石氮-空位（NV）中心磁力计在弱驱动场条件下测量效果好，但当交流驱动场强度接近或大于 NV 微波（MW）脉冲拉比强度时，NV 灵敏度会下降，无法有效测量材料的磁响应。本文旨在扩展 NV 磁力计的测量范围，以适应强驱动场条件。

Method: 提出了一种名为 SIPHT（Signal Isolation through PHase Tuning）的相位调制协议，该协议通过相位调制来抵消 MW 脉冲失谐效应，从而在强驱动场下保持 NV 磁力计的灵敏度。通过实验比较了 SIPHT 和传统的 Hahn echo 协议，并使用铜、铝和钛样品进行了演示。

Result: SIPHT 协议成功地将 NV 磁力计扩展到了强驱动场区域，并且在实验中保持了 NV 磁力计对相位信号的测量对比度。使用 SIPHT 协议成功检测到了铜、铝和钛样品中的涡流感应磁场，测量到的响应场相位延迟反映了它们不同的电导率。

Conclusion: SIPHT 协议是一种有效的相位调制方法，可以克服强驱动场对 NV 磁力计灵敏度的抑制作用，将其应用范围扩展到以前无法达到的区域，为磁性材料研究、磁热疗以及导体无损检测等领域开辟了新的应用前景。

Abstract: Magnetic response measurements in the presence of AC drive fields provide
critical insight into the properties of magnetic and conductive materials, such
as phase transitions in two-dimensional van der Waals magnets, the heating
efficiency of magnetic nanoparticles in biological environments, and the
integrity of metals in eddy current testing. Nitrogen-vacancy (NV) centers in
diamond are a commonly-used platform for such studies, due to their high
spatial resolution and sensitivity, but are typically limited to weak-drive
conditions, i.e., AC drive fields well below the NV microwave (MW) pulse Rabi
strength. Once the AC drive field grows comparable to or larger than the Rabi
strength, the induced MW pulse detuning suppresses NV sensitivity to the
out-of-phase magnetic response, which encodes dissipation and conductivity in
materials of interest. Here, we introduce a phase modulation protocol that
cancels MW pulse detuning to leading order, and extends NV AC magnetometry into
the strong drive field regime. The protocol, termed SIPHT (Signal Isolation
through PHase Tuning), is experimentally demonstrated using an NV ensemble. By
directly comparing SIPHT to the conventional Hahn echo AC sensing protocol, we
quantify the preservation of NV magnetometry contrast for an out-of-phase
signal. We further showcase SIPHT by detecting eddy current-induced magnetic
fields from Cu, Al, and Ti samples, with the measured response field phase
delays reflecting their distinct conductivities. SIPHT extends NV AC
magnetometry to regimes inaccessible to standard dynamical decoupling
measurement protocols, unlocking novel utility, e.g., in the study of magnetic
hyperthermia and nondestructive testing of conductors.

</details>


### [222] [Universal super-resolution framework for imaging of quantum dots](https://arxiv.org/abs/2510.06076)
*Dominik Vašinka,Jaewon Lee,Charlie Stalker,Victor Mitryakhin,Ivan Solovev,Sven Stephan,Sven Höfling,Falk Eilenberger,Seth Ariel Tongay,Christian Schneider,Miroslav Ježek,Ana Predojević*

Main category: quant-ph

TL;DR: 本研究提出了一种通用的深度学习方法，可以从单个相机帧测量中重建量子发射器的超分辨率图像。


<details>
  <summary>Details</summary>
Motivation: 现有的超分辨率技术通常需要复杂的校准和多次的图像采集，限制了其在纳米尺度表征和量子光子器件制造中的应用。本研究旨在开发一种快速、鲁棒的单次曝光超分辨率方法。

Method: 该方法使用基于物理的合成数据训练了一个深度学习网络，数据覆盖了多种点扩展函数、像差和噪声。该网络能够泛化到不同的实验条件，无需进行针对特定系统的重新训练。

Result: 研究人员在低密度和高密度的In(Ga)As量子点以及2D单层WSe2中的应变诱导点上验证了该方法的有效性，即使在低信噪比和不均匀背景下也能分辨重叠的发射器。

Conclusion: 该深度学习方法通过消除校准和迭代采集的需要，实现了一种快速、鲁棒的单次曝光超分辨率策略，可用于纳米尺度表征和量子光子器件的制造。

Abstract: We present a universal deep-learning method that reconstructs super-resolved
images of quantum emitters from a single camera frame measurement. Trained on
physics-based synthetic data spanning diverse point-spread functions,
aberrations, and noise, the network generalizes across experimental conditions
without system-specific retraining. We validate the approach on low- and
high-density In(Ga)As quantum dots and strain-induced dots in 2D monolayer
WSe$_2$, resolving overlapping emitters even under low signal-to-noise and
inhomogeneous backgrounds. By eliminating calibration and iterative
acquisitions, this single-shot strategy enables rapid, robust super-resolution
for nanoscale characterization and quantum photonic device fabrication.

</details>


### [223] [Quantum Regression Theory and Efficient Computation of Response Functions for Non-Markovian Open Systems](https://arxiv.org/abs/2510.05472)
*Xiantao Li,Chunhao Wang*

Main category: quant-ph

TL;DR: 本文提出了一种适用于开放量子系统、超越马尔可夫极限的记忆无关两点关联函数形式，并开发了相应的量子算法，能够高效计算非平衡态性质。


<details>
  <summary>Details</summary>
Motivation: 线性响应函数在物理学中用于高效估计动力学性质，但现有方法在处理开放量子系统时存在局限，无法直接计算两点关联函数。

Method: 提出了一种记忆无关的、仅依赖于系统的两点关联函数形式，该形式扩展了量子回归定理（QRT）的适用范围，使其超越了马尔可夫极限。通过结合浴的谱性质，将响应函数中的时间传播子表示为无记忆的、类似Lindblad形式的生成元。推导了新的QRT，并给出了相应的量子算法，能够以仅依赖于系统维度和目标精度的多对数复杂度来估计两点关联函数。

Result: 开发了适用于开放量子系统的、超越马尔可夫极限的、记忆无关的两点关联函数形式。提出了量子算法，其复杂度与系统维度呈多对数关系，与目标精度 $\epsilon$ 呈 $1/\epsilon^{1.25}$ 关系。

Conclusion: 该框架消除了可分离性（Born-Markov）假设，为高效计算开放量子系统的非平衡态性质提供了途径。

Abstract: Linear response functions are a cornerstone concept in physics as they enable
efficient estimation of many dynamical properties. In addition to predicting
dynamics of observables under perturbations without resimulating the system,
these response functions lead to electric conductivity, magnetic
susceptibility, dielectric constants, etc. Estimating two-time correlation
functions is a key ingredient of measuring linear response functions. However,
for open quantum systems, simulating the reduced density operator with a
quantum master equation only yields \emph{one-point} observables and is
insufficient for this task. In this paper, we develop a memoryless, system-only
formulation of two-point correlations for open quantum systems that extends the
standard quantum regression theorem (QRT) beyond the Markov limit. We further
incorporate the spectral property of the bath and express the time propagators
in the response function as the memoryless generators in Lindblad-type forms.
The resulting expressions recast the total response function into evolutions
generated by time-dependent Hamiltonian and Lindblad primitives together with
the more challenging propagation of commutators and anti-commutators. In
addition to the derivation of the new QRT, we present quantum algorithms for
these primitives and obtain an estimator for two-time correlations whose cost
scales poly-logarithmically in the system dimension and $1/\epsilon^{1.25}$ in
the target accuracy $\epsilon$. The framework removes the separability
(Born-Markov) assumption and offers a pathway to efficient computation of
nonequilibrium properties from open quantum systems.

</details>


### [224] [Cored product codes for quantum self-correction in three dimensions](https://arxiv.org/abs/2510.05479)
*Brenden Roberts,Jin Ming Koh,Yi Tan,Norman Y. Yao*

Main category: quant-ph

TL;DR: 通过引入无序量子码“cored product codes”，研究了三维自校正量子存储器的可能性，并通过数值模拟发现，在临界温度以下，该存储器的寿命随系统尺寸的增大而增加。


<details>
  <summary>Details</summary>
Motivation: 研究三维自校正量子存储器的存在性问题，并解决其在实现自校正方面的基本挑战。

Method: 提出一类名为“cored product codes”的无序量子码，该码通过“coring”程序嵌入到较低的空间维度，同时保留了码的性质。以基于无定形风车铺砖的“fractal code”为例，进行了三维量子存储器的有限温度数值模拟。

Result: 在临界温度以下，当系统尺寸增大时，该量子存储器的寿命随之增加，模拟的系统最大尺寸达到60000个量子比特。

Conclusion: 三维自校正量子存储器的实现面临挑战，但通过引入无序量子码“cored product codes”以及采用基于无定形风车铺砖的“fractal code”，可以在一定温度下实现具有随系统尺寸增大而增长的寿命的量子存储器。

Abstract: The existence of self-correcting quantum memories in three dimensions is a
long-standing open question at the interface between quantum computing and
many-body physics. We take the perspective that large contributions to the
entropy arising from fine-tuned spatial symmetries, including the assumption of
an underlying regular lattice, are responsible for fundamental challenges to
realizing self-correction. Accordingly, we introduce a class of disordered
quantum codes, which we call "cored product codes". These codes are derived
from classical factors via the hypergraph product but undergo a coring
procedure which allows them to be embedded in a lower number of spatial
dimensions while preserving code properties. As a specific example, we focus on
a fractal code based on the aperiodic pinwheel tiling as the classical factor
and perform finite temperature numerical simulations on the resulting
three-dimensional quantum memory. We provide evidence that, below a critical
temperature, the memory lifetime increases with system size for codes up to
60000 qubits.

</details>


### [225] [The code distance of Floquet codes](https://arxiv.org/abs/2510.05549)
*Keller Blackwell,Jeongwan Haah*

Main category: quant-ph

TL;DR: Floquet codes are fault-tolerant quantum memories with periodic Pauli measurements. We show that correctable, undetectable spacetime errors are either measurement operators or pairs of Paulis sandwiching a measurement. These errors are called 'benign' and generalize stabilizers of static codes. The code distance is the minimal weight of a non-benign undetectable error. This applies to other dynamical codes as well.


<details>
  <summary>Details</summary>
Motivation: The paper aims to understand and characterize spacetime errors in Floquet codes, a type of fault-tolerant quantum memory, and to generalize the concept of stabilizers to these dynamical codes.

Method: The paper proves that correctable, undetectable spacetime errors in Floquet codes are products of specific types of operators: (i) measurement operators and (ii) pairs of identical Pauli operators that commute with the measurement. These errors are termed 'benign'. The code distance is then defined as the minimal weight of an undetectable error that is not benign. The results are generalized to other dynamical codes.

Result: Identified 'benign' spacetime errors in Floquet codes, which are products of measurement operators and pairs of Paulis. Showed that these benign errors generalize stabilizers of static codes. Defined the code distance for Floquet codes based on non-benign undetectable errors. Extended these findings to a broader class of dynamical codes.

Conclusion: The characterization of benign spacetime errors provides a new way to understand error correction in Floquet codes and other dynamical quantum codes. The definition of code distance based on these errors is a key step towards building more robust quantum memories.

Abstract: For fault-tolerant quantum memory defined by periodic Pauli measurements,
called Floquet codes, we prove that every correctable, undetectable spacetime
error occurring during the steady stage is a product of (i) measurement
operators inserted at the time of the measurement and (ii) pairs of identical
Pauli operators sandwiching a measurement that commutes with the operator. We
call such errors benign; they define a binary vector subspace of spacetime
errors which properly generalize stabilizers of static Pauli stabilizer codes.
Hence, the code distance of a Floquet code is the minimal weight of an
undetectable spacetime Pauli error that is not benign. Our results apply more
generally to families of dynamical codes for which every instantaneous
stabilizer is inferred from measurements in a time interval of bounded length.

</details>


### [226] [Fermionic Insights into Measurement-Based Quantum Computation: Circle Graph States Are Not Universal Resources](https://arxiv.org/abs/2510.05557)
*Brent Harrison,Vishnu Iyer,Ojas Parekh,Kevin Thompson,Andrew Zhao*

Main category: quant-ph

TL;DR: Circle graph states are not efficiently universal for Measurement-Based Quantum Computation (MBQC).


<details>
  <summary>Details</summary>
Motivation: The paper aims to determine if circle graph states are a universal resource for MBQC, a critical question for realizing quantum computers.

Method: The paper proves this by establishing a graph-theoretic correspondence between circle graph states and a subset of fermionic Gaussian states, synthesizing techniques to handle both stabilizer and fermionic Gaussian states.

Result: Circle graph states are shown to not be efficiently universal for MBQC, assuming BQP != BPP.

Conclusion: While circle graph states are not efficiently universal, the developed techniques may have broader applications beyond MBQC.

Abstract: Measurement-based quantum computation (MBQC) is a strong contender for
realizing quantum computers. A critical question for MBQC is the identification
of resource graph states that can enable universal quantum computation. Any
such universal family must have unbounded entanglement width, which is known to
be equivalent to the ability to produce any circle graph state from the states
in the family using only local Clifford operations, local Pauli measurements,
and classical communication. Yet, it was not previously known whether or not
circle graph states themselves are a universal resource. We show that, in spite
of their expressivity, circle graph states are not efficiently universal for
MBQC (i.e., assuming $\mathsf{BQP} \neq \mathsf{BPP}$). We prove this by
articulating a precise graph-theoretic correspondence between circle graph
states and a certain subset of fermionic Gaussian states. This is accomplished
by synthesizing a variety of techniques that allow us to handle both stabilizer
states and fermionic Gaussian states at the same time. As such, we anticipate
that our developments may have broader applications beyond the domain of MBQC
as well.

</details>


### [227] [Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP](https://arxiv.org/abs/2510.06010)
*Aueaphum Aueawatthanaphisut,Nyi Wunna Tun*

Main category: quant-ph

TL;DR: 与经典强化学习（RL）相比，量子强化学习（QRL）在CartPole-v1环境中表现出较差的收敛性、对噪声更敏感，但具有参数量少和可扩展性的潜力。


<details>
  <summary>Details</summary>
Motivation: 为了研究经典和量子强化学习（QRL）在收敛行为、噪声鲁棒性和计算效率方面的对比，并在一个基准控制环境中进行评估。

Method: 使用多层感知机（MLP）作为经典基线，参数化变分量子电路（VQC）作为量子对应物，在CartPole-v1环境中训练500个回合。

Result: MLP实现了接近最优策略的收敛，平均回报为498.7 +/- 3.2。VQC的学习能力有限，平均回报为14.6 +/- 4.8，受限于电路深度和量子比特连接性。MLP在噪声下表现出平稳的性能下降，而VQC对噪声更敏感。VQC的参数量较少，训练时间略长。

Conclusion: 尽管经典神经网络策略在当前控制基准中仍然占主导地位，但一旦硬件噪声和表达能力限制得到缓解，量子增强架构有望在效率方面提供优势。

Abstract: The comparative evaluation between classical and quantum reinforcement
learning (QRL) paradigms was conducted to investigate their convergence
behavior, robustness under observational noise, and computational efficiency in
a benchmark control environment. The study employed a multilayer perceptron
(MLP) agent as a classical baseline and a parameterized variational quantum
circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1
environment over 500 episodes. Empirical results demonstrated that the
classical MLP achieved near-optimal policy convergence with a mean return of
498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast,
the VQC exhibited limited learning capability, with an average return of 14.6
+/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise
robustness analysis further revealed that the MLP policy deteriorated
gracefully under Gaussian perturbations, while the VQC displayed higher
sensitivity at equivalent noise levels. Despite the lower asymptotic
performance, the VQC exhibited significantly lower parameter count and
marginally increased training time, highlighting its potential scalability for
low-resource quantum processors. The results suggest that while classical
neural policies remain dominant in current control benchmarks, quantum-enhanced
architectures could offer promising efficiency advantages once hardware noise
and expressivity limitations are mitigated.

</details>


### [228] [A general dark-state theory for arbitrary multilevel quantum systems](https://arxiv.org/abs/2510.05561)
*Xuan Zhao,Le-Man Kuang,Jie-Qiao Liao*

Main category: quant-ph

TL;DR: 利用箭头-矩阵方法提出了一种通用理论，用于确定多能级量子系统中具有任意耦合配置的暗态。


<details>
  <summary>Details</summary>
Motivation: 暗态效应在光-原子相互作用、量子物理和量子信息中具有重要意义，但有效确定多能级量子系统中暗态的数量和形式是一个挑战。

Method: 提出了一种通用理论，使用箭头-矩阵方法，通过定义上下能级子空间、对限制在这些子空间内的哈密顿量进行对角化得到缀饰上下能态，然后表达缀饰能态之间的跃迁，将多能级系统映射到二分图网络，并根据网络的耦合配置确定暗态。

Result: 该方法能够确定三能级、四能级和五能级量子系统中不同耦合配置下的暗态，并已成功应用于驱动三能级系统中的暗态极化子。

Conclusion: 该理论为操纵和利用现代量子科学和技术中的多能级量子系统暗态提供了途径。

Abstract: The dark-state effect, caused by destructive quantum interference, is an
important physical effect in atomic physics and quantum optics. It not only
deepens the understanding of light-atom interactions, but also has wide
application in quantum physics and quantum information. Therefore, how to
efficiently and conveniently determine the number and form of the dark states
in multilevel quantum systems with complex transitions is an important and
interesting topic in this field. In this work, we present a general theory for
determining the dark states in multilevel quantum systems with any coupling
configuration using the arrowhead-matrix method. To confirm the dark states in
a multilevel system, we first define the upper- and lower-state subspaces, and
then diagonalize the Hamiltonians restricted within the two subspaces to obtain
the dressed upper and lower states. By further expressing the transitions
between the dressed upper and lower states, we can map the multilevel system to
a bipartite-graph network, in which the nodes and links are acted by the
dressed states and transitions, respectively. Based on the coupling
configurations of the network, we can determine the lower dark states with
respect to the upper-state subspace. As examples, we analyze the dark states in
three-, four-, and five-level quantum systems, for all possible configurations
through the classification of the numbers of upper and lower states. Further,
we extend the framework to multilevel quantum systems and discuss the existence
of dark states in some typical configurations. We also recover the results of
the dark-state polaritons in driven three-level systems with the
arrowhead-matrix method. Our theory paves the way for manipulating and
utilizing the dark states of multilevel quantum systems in modern quantum
science and technology.

</details>


### [229] [Quantum approximate optimization of bosonic finite-state systems](https://arxiv.org/abs/2510.05576)
*Shakib Daryanoosh*

Main category: quant-ph

TL;DR: 通过设计合适的混合哈密顿量，利用基于哈密顿量的量子近似优化算法（QAOA）来排除不相关的配置空间，并研究了不同的量子比特编码方法，最终将该框架应用于量子近似热化和求解特定物理模型。


<details>
  <summary>Details</summary>
Motivation: 现有的量子算法在处理高维问题时，需要将大的希尔伯特空间映射到量子比特上，这可能导致不相关子空间的出现，而现有的惩罚函数方法效率低下。

Method: 提出一种基于哈密顿量的QAOA方法，设计混合哈密顿量来排除不相关的配置空间，并研究了二进制、对称和一元三种映射技术，以受控-非门数量作为衡量标准。

Result: 对于对称映射，标准的混合哈密顿量（比特翻转操作的和）是最优的。而对于另外两种编码方案，在p层QAOA中，受控-非门数量会增加p倍。将此框架应用于量子近似热化，并成功求解了强弱相互作用下排斥Bose-Hubbard模型的基态。

Conclusion: 所提出的基于哈密顿量的QAOA框架能够有效地排除不相关的配置空间，并在处理物理问题方面展现出优势。

Abstract: There exist numerous problems in nature inherently described by finite
$D$-dimensional states. Formulating these problems for execution on qubit-based
quantum hardware requires mapping the qudit Hilbert space to that of multiqubit
which may be exponentially larger. To exclude the infeasible subspace, one
common approach relies on penalizing the objective function. However, this
strategy can be inefficient as the size of the illegitimate subspace grows.
Here we propose to employ the Hamiltonian-based quantum approximate
optimization algorithm (QAOA) through devising appropriate mixing Hamiltonians
such that the infeasible configuration space is ruled out. We investigate this
idea by employing binary, symmetric, and unary mapping techniques. It is shown
that the standard mixing Hamiltonian (sum of the bit-flip operations) is the
optimal option for symmetric mapping, where the controlled-NOT gate count is
used as a measure of implementation cost. In contrast, the other two encoding
schemes witness a $p$-fold increase in this figure for a $p$-layer QAOA. We
apply this framework to quantum approximate thermalization and find the ground
state of the repulsive Bose-Hubbard model in the strong and weak interaction
regimes.

</details>


### [230] [Quantum Kernel Anomaly Detection Using AR-Derived Features from Non-Contact Acoustic Monitoring for Smart Manufacturing](https://arxiv.org/abs/2510.05594)
*Takao Tomono,Kazuya Tsujimura*

Main category: quant-ph

TL;DR: 本研究提出使用量子核方法，利用少量非接触式麦克风，在嘈杂的工厂环境中实现稳健的多类别异常检测，以降低传感器复杂性和计算成本，并提高维护效率。


<details>
  <summary>Details</summary>
Motivation: 目前的智能工厂设备维护严重依赖接触式传感器，导致了传感器复杂性和计算成本的增加。本研究旨在探索使用量子核方法，利用非接触式传感器进行异常检测，以解决这一挑战。

Method: 本研究使用单一定向麦克风收集两种制造设备（传送带和链条驱动机）的音频数据。通过自回归（AR）模型提取特征，并利用量子核将特征映射到量子特征空间，然后使用单类支持向量机（SVM）进行分类。

Result: 在不同距离（0至3米）的实验中，量子核分类器在所有距离下均实现了超过0.92的高准确率和F1分数。而经典分类器在超过0米后准确率显著下降。可视化显示，不同类别的异常（传送带异常和链条驱动机异常）在量子特征空间中具有清晰的可分离性。

Conclusion: 本研究证明了量子核方法能够使用最少的非接触式传感器，在嘈杂的工厂环境中实现稳健的多类别异常检测。这一成果是实现量子增强型智能工厂、减少传感基础设施和提高维护效率的重要进展。

Abstract: The evolution of manufacturing toward smart factories has underscored major
challenges in equipment maintenance, particularly the dependence on numerous
contact sensors for anomaly detection, leading to increased sensor complexity
and computational costs. This study explores the use of quantum kernels to
enhance anomaly detection based on noncontact sensors. We hypothesize that the
expressive power of quantum feature spaces can effectively discriminate among
multiple anomaly types using fewer sensors. Experiments were conducted on two
types of manufacturing equipment a conveyor and a chain belt machine where a
single directional microphone was placed at varying distances (0 to 3 m) to
capture audio data. The signals were processed using autoregressive (AR)
modeling to extract coefficient based features, which were then mapped into
quantum feature space via quantum kernels for one class SVM classification. The
quantum kernel classifiers achieved consistently high accuracy and F1 scores
(more than 0.92) across all distances, while classical counterparts exhibited
significant degradation beyond 0 m. Visualization of the feature space revealed
clear separability, with distinct quadrants corresponding to different anomaly
types: conveyor anomalies were mainly distributed in the second quadrant, and
chain belt anomalies clustered in the fourth. These results demonstrate that
quantum kernel methods can achieve robust, multi class anomaly detection in
noisy factory environments using minimal non contact sensors. This represents a
significant advancement toward quantum enhanced smart factories with reduced
sensing infrastructure and improved maintenance efficiency. This work has been
accepted for presentation at IEEE QCE25 and will appear in the IEEE Xplore
Digital Library.

</details>


### [231] [Quantum interference between autonomous dissimilar quantum light sources for hybrid quantum networks](https://arxiv.org/abs/2510.05607)
*Kyu-Young Kim,Heewoo Kim,Dong Hyun Park,Jinhyuk Bea,Gyeongmin Ju,Suk In Park,Jin Dong Song,Je-Hyung Kim,Han Seb Moon*

Main category: quant-ph

TL;DR: 该研究成功从两种不同的量子系统中生成了无法区分的光子（来自温原子系综和固态量子点），并在没有额外光谱滤波和时间同步的情况下实现了量子干涉，从而为混合量子网络实现了自主量子节点。


<details>
  <summary>Details</summary>
Motivation: 在混合量子网络中，连接异构量子节点并分发纠缠是一个重要挑战，因为不同量子节点发出的单光子通常具有不同的光谱和时间特性，需要额外的滤波和同步，这会导致显著的光子损耗并需要额外资源。

Method: 利用133Cs原子系综在917 nm波长处产生受激单光子，并调整InAs/GaAs量子点发出的单光子以匹配该波长。通过腔耦合的量子点和密集温原子系综，分别以接近MHz的探测率生成明亮且共振的单光子。

Result: 实现了来自温原子系综和固态量子点的无法区分的光子的生成，并在没有额外光谱滤波和时间同步的情况下实现了量子干涉。生成的单光子具有内在的光谱相似性（波长和光谱线宽），光谱重叠度高达0.92。

Conclusion: 该研究通过生成具有内在光谱兼容性的单光子，克服了混合量子网络中不同量子源之间的接口挑战，为构建大规模、功能性的混合量子网络铺平了道路。

Abstract: Hybrid quantum systems play a crucial role in advancing scalable and
versatile quantum networks as they combine the strengths of different quantum
platforms. An important challenge for the development of hybrid quantum
networks lies in interfacing heterogeneous quantum nodes and distributing
entanglement among them. Single photons emitted from these dissimilar quantum
nodes typically show distinct spectral and temporal properties. Therefore, they
necessitate spectral filtering and temporal synchronization, which introduce
significant photon losses and require additional resources. In this work, we
successfully generate indistinguishable photons from two distinct quantum
systems of a warm atomic ensemble and a solid-state quantum dot. Remarkably,
quantum interference between dissimilar sources is achieved without additional
spectral filtering and time synchronization, which enables autonomous quantum
nodes for a hybrid quantum network. 133Cs atomic ensemble can efficiently
generate heralded single photons at the wavelength of 917 nm of the
6P_(3/2)-6D_(5/2) transition, while the single photons emitted from an
InAs/GaAs quantum dot can be tuned to match the 133Cs transition wavelength.
Our dense warm atomic ensemble and cavity-coupled quantum dot can efficiently
generate bright and resonant single photons at detection rates approaching MHz,
respectively. More importantly, these single photons exhibit inherent spectral
similarities not only in the wavelength but also in the spectral linewidth,
achieving a high spectral overlap of 0.92. Such intrinsic compatibility between
dissimilar quantum sources is essential to leverage the advantages of different
quantum platforms, paving the way toward a large-scale and functional hybrid
quantum network.

</details>


### [232] [An approach using geometric diagrams to generic Bell inequalities with multiple observables](https://arxiv.org/abs/2510.05622)
*Junghee Ryu,Jinhyoung Lee,Hoon Ryu*

Main category: quant-ph

TL;DR: 本文将适用于三方系统的贝尔不等式扩展到包含多个可观测量，并提出一种计算经典上界的几何方法，将问题转化为线性同余关系中的约束识别。研究推导了三和四可观测量场景下的上界，并使用GHZ态证明了量子违反，且违反程度随可观测量数量的增加而增强。


<details>
  <summary>Details</summary>
Motivation: 将适用于三方系统的贝尔不等式扩展到包含多个可观测量，并提出一种计算经典上界的几何方法。

Method: 将计算经典上界的问题转化为线性同余关系中的约束识别，并使用GHZ态来演示量子违反。

Result: 推导了三和四可观测量场景下的贝尔不等式上界，并展示了GHZ态的量子违反，违反程度随可观测量数量的增加而增强。

Conclusion: 提出的几何方法可以有效地计算贝尔不等式的经典上界，并且量子系统（如GHZ态）可以实现超越这些上界的违反，这种违反程度与可观测量数量正相关。

Abstract: We extend the generic Bell inequalities suggested by Son, Lee, and Kim [Phys.
Rev. Lett. 96, 060406 (2006)] to incorporate multiple observables for
tripartite systems and introduce a geometric methodology for calculating
classical upper bounds of the inequalities. Our method transforms the problem
of finding the classical upper bounds into identifying constraints in linear
congruence relations. Using this approach, we derive the upper bounds for
scenarios with three and four observables per party. In order to demonstrate
quantum violations, we employ Greenberger-Horne-Zeilinger entangled states that
can achieve values exceeding the classical upper bounds, with the violation
becoming more pronounced as the number of observables increases.

</details>


### [233] [Recovery of the second law in fully quantum thermodynamics](https://arxiv.org/abs/2510.05642)
*Naoto Shiraishi,Ryuji Takagi*

Main category: quant-ph

TL;DR: 量子退火被证明可以被一个热操作和一个相关的催化剂所转化，并且这种转化仅由自由能决定。


<details>
  <summary>Details</summary>
Motivation: 量子热力学研究第二定律在量子世界中的普适性，而量子相干性是实现这一目标的主要障碍。

Method: 我们证明了量子相干状态在有相关催化剂的热操作下的可转化性完全由自由能序所表征。

Result: 量子状态（包括具有量子相干性的状态）在有相关催化剂的热操作下的可转化性完全由自由能序决定。

Conclusion: 我们提供了一个没有外部辅助的、忠实的量子热力学状态转化的操作表征，完全由自由能序决定。

Abstract: Quantum thermodynamics investigates how robust the second law of
thermodynamics serves as the unique fundamental law in the small quantum world.
To tackle this problem, the quantum coherence constitutes a major difficulty of
investigations, which provides severe constraints hindering the recovery of a
single thermodynamic potential. Here we solve this long-standing problem of
quantum information theory by revealing that the state convertibility under
thermal operations is fully characterized by the second law of thermodynamics.
Specifically, we prove that whether a quantum state with quantum coherence is
convertible to another by a thermal operation with a correlated catalyst is
completely determined by the free energy ordering. Unlike previous attempts,
our setting does not resort to any additional external coherent assist,
providing a faithful operational characterization of thermodynamic state
transformation.

</details>


### [234] [Tensor Network Loop Cluster Expansions for Quantum Many-Body Problems](https://arxiv.org/abs/2510.05647)
*Johnnie Gray,Gunhee Park,Glen Evenbly,Nicola Pancotti,Garnet Kin-Lic Chan*

Main category: quant-ph

TL;DR: 该论文分析了张量网络环路簇展开（arXiv:2504.07344）在通用量子多体问题中的应用。


<details>
  <summary>Details</summary>
Motivation: 将arXiv:2504.07344中引入的张量网络环路簇展开作为信念传播的系统性修正，应用于通用的量子多体问题。

Method: 对张量网络环路簇展开在计算高键维度张量网络的基态可观测量方面的准确性和实际适用性进行了数值分析。

Result: 在二维和三维、开边界和周期性边界条件、自旋和费米子问题中，通过数值示例展示了该方法的准确性和适用性。

Conclusion: 张量网络环路簇展开在计算高键维度张量网络的基态可观测量方面具有准确性和实用性。

Abstract: We analyze the tensor network loop cluster expansion, introduced in
arXiv:2504.07344 as a systematic correction to belief propagation, in the
context of general quantum many-body problems. We provide numerical examples of
the accuracy and practical applicability of the approach for the computation of
ground-state observables for high bond dimension tensor networks, in two- and
three-dimensions, with open and periodic boundary conditions, and for spin and
fermion problems.

</details>


### [235] [Low Overhead Universal Quantum Computation with Triorthogonal Codes](https://arxiv.org/abs/2510.05708)
*Dawei Jiao,Mahdi Bayanifar,Alexei Ashikhmin,Olav Tirkkonen*

Main category: quant-ph

TL;DR: 利用三正交码进行容错量子计算，并提出两种绕过Eastin-Knill定理的方法，一个是通过利用其横向CZ门简化逻辑Hadamard门的实现，另一个是通过生成对称Calderbank-Shor-Steane码与三正交码的配对来实现CNOT和CZ门横向性。


<details>
  <summary>Details</summary>
Motivation: Eastin-Knill定理禁止任何单一量子纠错码同时支持通用性和横向门集，本研究旨在探索三正交码在通用容错量子计算中的应用，并提出绕过该定理的方法。

Method: 提出两种方法：1. 利用三正交码的横向CZ门简化逻辑Hadamard门的实现。2. 生成对称Calderbank-Shor-Steane码与三正交码的配对，实现CNOT和CZ门横向性。并提出逻辑状态蒸馏协议。

Result: 实现了简化的逻辑Hadamard门，并实现了CNOT和CZ门在配对码上的横向性，以及逻辑状态蒸馏协议。这些方法可以集成到Steane纠错框架中，无需额外资源。

Conclusion: 本研究提出的方法有效地规避了Eastin-Knill定理的限制，为实现通用容错量子计算提供了新的途径，并能与现有框架集成，无需增加资源消耗。

Abstract: We explore the use of triorthogonal codes for universal fault-tolerant
quantum computation and propose two methods to circumvent the Eastin-Knill
theorem, which prohibits any single quantum error-correcting code from
supporting both universality and a transversal gate set. First, we simplify the
implementation of the logical Hadamard gate for triorthogonal codes by
exploiting the fact that they have transversal controlled-Z (CZ) gates,
resulting in a circuit with reduced overhead. Then, we introduce procedure for
generating a symmetric Calderbank-Shor-Steane code paired with a triorthogonal
code, which allows CNOT and CZ gate transversality across the pair of codes. We
also present a circuit for realizing a logical state teleportation protocol
between the two codes, enabling all logical operations to be performed
transversally. Finally, we demonstrate how these methods can be integrated into
the Steane error correction framework without incurring additional resource
cost.

</details>


### [236] [The PESCAD Method for Autonomous Systems: An Application to Photoionization at Near-optical Wavelengths](https://arxiv.org/abs/2510.05776)
*Selstø Sølve,Bendik Steinsvåg Dalen*

Main category: quant-ph

TL;DR: 本文提出了一种半解析方法，用于在原子与外部场相互作用结束后，通过复吸收势计算光电子谱，并能外推至无限时间。


<details>
  <summary>Details</summary>
Motivation: 为了克服传统方法需要模拟到波函数几乎完全被吸收的缺点，并能精确计算光电子谱，特别是在探测器靠近原子时。

Method: 提出了一种半解析方法，利用复吸收势，并能外推至无限时间。

Result: 计算了氢原子的光电子谱，并讨论了探测器位置对角分布的影响。

Conclusion: 该半解析方法可以有效计算光电子谱，并揭示了探测器位置对角分布的畸变效应。

Abstract: In a recent publication, Dalen, and Selst{\o}, Phys. Rev. A {\bf 111}, 033116
(2025), it was demonstrated how converged photo electron spectra could be
determined using a complex absorbing potential on a truncated numerical domain
considerably smaller than the extension of the dynamical wave function. That
approach required simulation until virtually all unbound parts of the wave
function was absorbed, far beyond the duration of the interaction with the
external field. In this work we formulate the method in a semi-analytical
manner which allows us to extrapolate to infinite times after the interaction
with the external field. In addition to obtaining photoelectron spectra for
hydrogen differential in energy and ejection angle, we also demonstrate how --
and when -- the absorber may be seen as a detector, distorting the angular
distributions when the detector is placed in the extreme vicinity of the atom.

</details>


### [237] [Implementation of multiparticle quantum speed limits on observables](https://arxiv.org/abs/2510.05794)
*Rui-Heng Miao,Zhao-Di Liu,Chen-Xi Ning,Yu-Cong Hu,Hao Zhang,Chuan-Feng Li,Guang-Can Guo*

Main category: quant-ph

TL;DR: 实验验证了多粒子和纠缠可以加速量子系统的演化速度，并证明了初始量子态和开放系统的作用。


<details>
  <summary>Details</summary>
Motivation: 探索多粒子量子速度极限的实验验证，以及其在量子任务加速中的作用。

Method: 利用超高精度控制的量子演化时间，在两粒子系统中进行实验验证，并证明了初始量子态的关键作用，以及在非幺正马尔可夫开放系统中对量子速度上下界的验证。

Result: 实验证明了多粒子和纠缠能够加速量子系统在可观测量上的演化速度；证明了初始量子态在纠缠系统的量子速度极限中扮演着关键角色；证明了量子速度的上下界在含两个光子的非幺正马尔可夫开放系统中是可行的。

Conclusion: 实验结果具有普遍性，可推广至更多粒子，为复杂量子系统的动态瞬态性质表征和大规模量子系统的量子速度控制提供了基础。

Abstract: The energy-time uncertainty relation limits the maximum speed of quantum
system evolution and is crucial for determining whether quantum tasks can be
accelerated. However, multiparticle quantum speed limits have not been
experimentally explored. In this work, we experimentally verify that both
multiparticles and entanglement can accelerate the quantum speed on observables
in two-particle systems based on ultrahigh precision control of quantum
evolution time. Furthermore, we experimentally prove that the initial quantum
state plays a critical role in the quantum speed limits of the entangled
systems. In addition, we experimentally demonstrate that the upper bound and
lower bound of the quantum speed are workable even in a nonunitary Markovian
open system with two photons. The results obtained based on two-photon
experiments have been shown to be generalizable to more particles. Our work
facilitates the characterization of the dynamic transient properties of complex
quantum systems and the control of the quantum speed of large-scale quantum
systems.

</details>


### [238] [Efficient Post-Selection for General Quantum LDPC Codes](https://arxiv.org/abs/2510.05795)
*Seok-Hyung Lee,Lucas English,Stephen D. Bartlett*

Main category: quant-ph

TL;DR: 该研究提出了一种基于启发式置信度度量的后选择策略，用于提高量子纠错的保真度，并适用于任意量子低密度奇偶校验码（QLDPC）和实时解码。


<details>
  <summary>Details</summary>
Motivation: 现有的基于“逻辑间隙”度量的后选择策略在计算开销和泛化能力方面存在局限性，尤其是在处理多量子比特系统和非表面码时。

Method: 提出并验证了一种新的后选择策略，该策略基于计算效率高的启发式置信度度量，利用了基于聚类的解码器的错误簇统计信息（如聚集簇大小和对数似然比）。该方法适用于任意QLDPC码，并集成了滑窗框架以支持实时解码和提前中止。

Result: 通过在表面码、双变量自行车码和超图乘积码上的数值模拟，证明了该方法能够以适度的中止率（如1%或19%）将逻辑错误率降低几个数量级。对于[[144, 12, 12]]双变量自行车码，在0.1%的物理错误率下，逻辑错误率降低了三个数量级。

Conclusion: 所提出的后选择策略为具有QLDPC码的容错量子计算提供了一种高效且实用的方法，它克服了先前方法的局限性，并在计算效率、代码泛化性和实时应用方面表现出色。

Abstract: Post-selection strategies that discard low-confidence computational results
can significantly improve the effective fidelity of quantum error correction at
the cost of reduced acceptance rates, which can be particularly useful for
offline resource state generation. Prior work has primarily relied on the
"logical gap" metric with the minimum-weight perfect matching decoder, but this
approach faces fundamental limitations including computational overhead that
scales exponentially with the number of logical qubits and poor
generalizability to arbitrary codes beyond surface codes. We develop
post-selection strategies based on computationally efficient heuristic
confidence metrics that leverage error cluster statistics (specifically,
aggregated cluster sizes and log-likelihood ratios) from clustering-based
decoders, which are applicable to arbitrary quantum low-density parity check
(QLDPC) codes. We validate our method through extensive numerical simulations
on surface codes, bivariate bicycle codes, and hypergraph product codes,
demonstrating orders of magnitude reductions in logical error rates with
moderate abort rates. For instance, applying our strategy to the $[[144, 12,
12]]$ bivariate bicycle code achieves approximately three orders of magnitude
reduction in the logical error rate with an abort rate of only 1% (19%) at a
physical error rate of 0.1% (0.3%). Additionally, we integrate our approach
with the sliding-window framework for real-time decoding, featuring early
mid-circuit abort decisions that eliminate unnecessary overheads. Notably, its
performance matches or even surpasses the original strategy for global
decoding, while exhibiting favorable scaling in the number of rounds. Our
approach provides a practical foundation for efficient post-selection in
fault-tolerant quantum computing with QLDPC codes.

</details>


### [239] [Optimal ancilla-free Clifford+T synthesis for general single-qubit unitaries](https://arxiv.org/abs/2510.05816)
*Hayata Morisaki,Kaoru Sano,Seiseki Akibue*

Main category: quant-ph

TL;DR: 提出两种最优T-count的Clifford+T合成算法：确定性合成和概率性合成，能够以高精度近似任意单比特幺正变换，且运行时间具有实际意义。


<details>
  <summary>Details</summary>
Motivation: 需要开发能够高效合成任意单比特幺正变换的Clifford+T电路，并最小化T门数量，以满足量子计算的需求。

Method: 提出确定性合成和概率性合成两种算法。确定性合成通过Clifford+T电路以最小T-count近似任意单比特幺正变换。概率性合成通过概率混合的单比特Clifford+T电路以最小T-count近似任意单比特幺正变换。

Result: 对于大多数单比特幺正变换，确定性合成和概率性合成算法的运行时间分别为$\(varepsilon^{-1/2 - o(1)}\)$和$\(varepsilon^{-1/4 - o(1)}\)$，其中$\(varepsilon\)$为近似误差。在$\(varepsilon \approx 10^{-15}\)$和$\(varepsilon \approx 10^{-22}\)$时，算法运行时间具有实际意义。确定性合成需要至多$\(3\log_2(1/\varepsilon) + o(\log_2(1/\varepsilon))\)$个T门，概率性合成需要至多$\(1.5\log_2(1/\varepsilon) + o(\log_2(1/\varepsilon))\)$个T门。

Conclusion: 提出的两种Clifford+T合成算法在T-count方面是最优的，能够高效且高精度地近似单比特幺正变换，并且其复杂度分析不依赖于任何数值或数论猜想。

Abstract: We propose two Clifford+$T$ synthesis algorithms that are optimal with
respect to $T$-count. The first algorithm, called deterministic synthesis,
approximates any single-qubit unitary by a single-qubit Clifford+$T$ circuit
with the minimum $T$-count. The second algorithm, called probabilistic
synthesis, approximates any single-qubit unitary by a probabilistic mixture of
single-qubit Clifford+$T$ circuits with the minimum $T$-count. For most of
single-qubit unitaries, the runtimes of deterministic synthesis and
probabilistic synthesis are $\varepsilon^{-1/2 - o(1)}$ and $\varepsilon^{-1/4
- o(1)}$, respectively, for an approximation error $\varepsilon$. Although this
complexity is exponential in the input size, we demonstrate that our algorithms
run in practical time at $\varepsilon \approx 10^{-15}$ and $\varepsilon
\approx 10^{-22}$, respectively. Furthermore, we show that, for most
single-qubit unitaries, the deterministic synthesis algorithm requires at most
$3\log_2(1/\varepsilon) + o(\log_2(1/\varepsilon))$ $T$-gates, and the
probabilistic synthesis algorithm requires at most $1.5\log_2(1/\varepsilon) +
o(\log_2(1/\varepsilon))$ $T$-gates. Remarkably, complexity analyses in this
work do not rely on any numerical or number-theoretic conjectures.

</details>


### [240] [Hybrid Sequential Quantum Computing](https://arxiv.org/abs/2510.05851)
*Pranav Chandarana,Sebastián V. Romero,Alejandro Gomez Cadavid,Anton Simen,Enrique Solano,Narendra N. Hegade*

Main category: quant-ph

TL;DR: 我们提出了一种名为混合顺序量子计算 (HSQC) 的新范式，它将经典和量子计算方法结合在一个分阶段的工作流程中，用于组合优化。HSQC 在具有挑战性的高阶无约束二元优化 (HUBO) 问题上展示了优于纯经典方法的显著加速（最多 700 倍）。


<details>
  <summary>Details</summary>
Motivation: 将经典和量子计算的优势结合起来，以解决经典方法可能陷入局部最小值或效率低下的组合优化问题。

Method: HSQC 采用分阶段的工作流程，将经典优化器（如模拟退火 SA）用于探索解空间，然后使用量子优化器（如偏置场数字化反绝热量子优化 BF-DCQO）来改进解决方案，最后使用经典求解器（如模拟退火 SA 或模因禁忌搜索 MTS）进行精炼。

Result: 在 156 量子比特的超导量子处理器上，HSQC 能够持续找到高阶无约束二元优化 (HUBO) 问题的基态解，并且在估计运行时间上比纯 SA 快 700 倍，比纯 MTS 快 9 倍。

Conclusion: HSQC 是一个灵活且可扩展的框架，在先进的商用量子处理器上，在运行时间上实现了高达两个数量级的量子优势。

Abstract: We introduce hybrid sequential quantum computing (HSQC), a paradigm for
combinatorial optimization that systematically integrates classical and quantum
methods within a structured, stage-wise workflow. HSQC may involve an arbitrary
sequence of classical and quantum processes, as long as the global result
outperforms the standalone components. Our testbed begins with classical
optimizers to explore the solution landscape, followed by quantum optimization
to refine candidate solutions, and concludes with classical solvers to recover
nearby or exact-optimal states. We demonstrate two instantiations: (i) a
pipeline combining simulated annealing (SA), bias-field digitized
counterdiabatic quantum optimization (BF-DCQO), and memetic tabu search (MTS);
and (ii) a variant combining SA, BF-DCQO, and a second round of SA. This
workflow design is motivated by the complementary strengths of each component.
Classical heuristics efficiently find low-energy configurations, but often get
trapped in local minima. BF-DCQO exploits quantum resources to tunnel through
these barriers and improve solution quality. Due to decoherence and
approximations, BF-DCQO may not always yield optimal results. Thus, the best
quantum-enhanced state is used to continue with a final classical refinement
stage. Applied to challenging higher-order unconstrained binary optimization
(HUBO) problems on a 156-qubit heavy-hexagonal superconducting quantum
processor, we show that HSQC consistently recovers ground-state solutions in
just a few seconds. Compared to standalone classical solvers, HSQC achieves a
speedup of up to 700 times over SA and up to 9 times over MTS in estimated
runtimes. These results demonstrate that HSQC provides a flexible and scalable
framework capable of delivering up to two orders of magnitude improvement at
runtime quantum-advantage level on advanced commercial quantum processors.

</details>


### [241] [Dynamic Scheduling in Fiber and Spaceborne Quantum Repeater Networks](https://arxiv.org/abs/2510.05854)
*Paolo Fittipaldi*

Main category: quant-ph

TL;DR: 该论文提出了一个用于量化网络调度的数学框架，并应用Lyapunov漂移最小化推导了一种新的二次优化调度策略，并与Max Weight线性策略进行了比较。此外，论文还扩展了量子网络模拟器QuISP，并为卫星链路推导了纠缠分发率模型，研究了量子内存分配策略。结果表明，经典通信延迟和光速限制了可达速率。


<details>
  <summary>Details</summary>
Motivation: 解决量子网络中的调度问题，即选择哪种纠缠交换操作以更好地满足用户需求，并提供一种比较不同调度策略的方法。

Method: 提出一个数学框架来定义量子网络调度问题，应用Lyapunov漂移最小化推导出二次优化调度策略，并将其与Max Weight线性策略进行比较。开发了QuISP模拟器的扩展，并为卫星链路推导了纠缠分发率模型。

Result: Lyapunov漂移最小化策略和Max Weight策略的比较结果，QuISP模拟器的扩展，卫星链路纠缠分发率模型，以及经典通信延迟和光速限制对量子链路速率的影响（限制在几十kHz）。

Conclusion: 量子网络调度问题可以通过所提出的数学框架进行建模和分析。Lyapunov漂移最小化和Max Weight策略是两种可行的调度方法。卫星链路的调度受到经典通信延迟和物理限制的影响。未来研究需要克服在光纤和卫星量子网络上研究量子调度问题的挑战。

Abstract: The problem of scheduling in quantum networks amounts to choosing which
entanglement swapping operations to perform to better serve user demand. The
choice can be carried out following a variety of criteria (e.g. ensuring all
users are served equally vs. prioritizing specific critical applications,
adopting heuristic or optimization-based algorithms...), requiring a method to
compare different solutions and choose the most appropriate. We present a
framework to mathematically formulate the scheduling problem over quantum
networks and benchmark general quantum scheduling policies over arbitrary lossy
quantum networks. By leveraging the framework, we apply Lyapunov drift
minimization to derive a novel class of quadratic optimization based scheduling
policies, which we then analyze and compare with a Max Weight inspired linear
class. We then give an overview of the pre-existing fiber quantum simulation
tools and report on the development of numerous extensions to QuISP, an
established quantum network simulator focused on scalability and accuracy in
modeling the underlying classical network infrastructure. To integrate
satellite links in the discussion, we derive an analytical model for the
entanglement distribution rates for satellite-to-ground and
ground-satellite-ground links and discuss different quantum memory allocation
policies for the dual link case. Our findings show that classical communication
latency is a major limiting factor for satellite communication, and the effects
of physical upper bounds such as the speed of light must be taken into account
when designing quantum links, limiting the attainable rates to tens of kHz. We
conclude by summarizing our findings and highlighting the challenges that still
need to be overcome in order to study the quantum scheduling problem over fiber
and satellite quantum networks. [Abridged abstract, see PDF for full version]

</details>


### [242] [Entanglement dynamics and performance of two-qubit gates for superconducting qubits under non-Markovian effects](https://arxiv.org/abs/2510.05872)
*Kiyoto Nakamura,Joachim Ankerhold*

Main category: quant-ph

TL;DR: 本文研究了含噪声的两比特量子系统的耗散动力学，重点关注量子比特-环境关联和非马尔可夫过程对量子计算器件性能的影响。


<details>
  <summary>Details</summary>
Motivation: 揭示精细的量子比特-环境关联（包括非马尔可夫过程）在量子计算器件改进中的作用。

Method: 采用数值精确模拟技术，研究了每个量子比特与其各自噪声源耦合的两比特体系的耗散动力学。分析了旋转波近似的有效性、$\sqrt{\mbox{iSWAP}^\dagger}$门作用下以及Hadamard + CNOT序列下的纠缠生成与破坏、退纠缠动力学以及噪声对门操作性能的影响，并考虑了不同类型的噪声源和量子比特参数。

Result: 研究了旋转波近似对退纠缠动力学有效性的影响，分析了$\sqrt{\mbox{iSWAP}^\dagger}$门操作对纠缠动力学的影响，并考虑了环境的记忆效应，最后评估了Hadamard + CNOT门序列在不同分解方案下的性能。

Conclusion: 通过数值模拟揭示了量子比特-环境关联和非马尔可夫过程对量子计算性能的影响，为改进量子计算器件提供了指导。

Abstract: Within a numerically exact simulation technique, the dissipative dynamics of
a two-qubit architecture is considered in which each qubit couples to its
individual noise source (reservoir). The goal is to reveal the role of subtle
qubit-reservoir correlations including non-Markovian processes as a
prerequisite to guide further improvements of quantum computing devices. This
paper addresses the following three topics. First, we examine the validity of
the rotating wave approximation imposed previously on the qubit-reservoir
coupling with respect to the disentanglement dynamics. Second, generation of
the entanglement as well as destruction are analyzed by monitoring the reduced
dynamics during and after application of a $\sqrt{\mbox{iSWAP}^\dagger}$ gate,
also focusing on memory effects caused by reservoirs. Finally, the performance
of a Hadamard + CNOT sequence is analyzed for different gate decomposition
schemes. In all three cases, various types of noise sources and qubit
parameters are considered.

</details>


### [243] [Learning stabilizer structure of quantum states](https://arxiv.org/abs/2510.05890)
*Srinivasan Arunachalam,Arkopal Dutt*

Main category: quant-ph

TL;DR: 该论文提出了一种学习任意量子态结构化稳定分解的方法，并给出了相应的学习算法。


<details>
  <summary>Details</summary>
Motivation: 学习量子态的结构化表示，特别是稳定分解，对于理解和操作复杂量子态至关重要。

Method: 论文首先证明了稳定分解的存在性，然后提出了一个基于Gowers-3范数逆定理的方法。接着，研究了状态的自纠正问题，并提出了一个在APFR猜想下的多项式时间算法和一个无APFR猜想下的拟多项式时间算法。最后，将这些技术应用于学习具有稳定度$\xi$的量子态，给出了一个时间复杂度为$\textsf{poly}(n,\xi^{\log \xi})$的协议，并在APFR猜想下进一步优化为多项式时间。

Result: 论文证明了稳定分解的存在性，并提出了自纠正算法和量子态学习算法。具体来说，对于稳定秩为$k$的量子态，给出了一个时间复杂度为$\textsf{poly}(n,k^{k^2})$的无条件学习算法。

Conclusion: 该研究在量子态学习和结构分析方面取得了重要进展，特别是为学习具有特定结构（如稳定分解）的量子态提供了有效的算法。

Abstract: We consider the task of learning a structured stabilizer decomposition of an
arbitrary $n$-qubit quantum state $|\psi\rangle$: for $\varepsilon > 0$, output
a state $|\phi\rangle$ with stabilizer-rank $\textsf{poly}(1/\varepsilon)$ such
that $|\psi\rangle=|\phi\rangle+|\phi'\rangle$ where $|\phi'\rangle$ has
stabilizer fidelity $< \varepsilon$. We firstly show the existence of such
decompositions using the recently established inverse theorem for the
Gowers-$3$ norm of states [AD,STOC'25].
  To learn this structure, we initiate the task of self-correction of a state
$|\psi\rangle$ with respect to a class of states $\textsf{C}$: given copies of
$|\psi\rangle$ which has fidelity $\geq \tau$ with a state in $\textsf{C}$,
output $|\phi\rangle \in \textsf{C}$ with fidelity $|\langle \phi | \psi
\rangle|^2 \geq \tau^C$ for a constant $C>1$. Assuming the algorithmic
polynomial Frieman-Rusza (APFR) conjecture (whose combinatorial version was
recently resolved [GGMT,Annals of Math.'25], we give a polynomial-time
algorithm for self-correction of stabilizer states. Given access to the state
preparation unitary $U_\psi$ for $|\psi\rangle$ and its controlled version
$cU_\psi$, we give a polynomial-time protocol that learns a structured
decomposition of $|\psi\rangle$. Without assuming APFR, we give a
quasipolynomial-time protocol for the same task.
  As our main application, we give learning algorithms for states
$|\psi\rangle$ promised to have stabilizer extent $\xi$, given access to
$U_\psi$ and $cU_\psi$. We give a protocol that outputs $|\phi\rangle$ which is
constant-close to $|\psi\rangle$ in time $\textsf{poly}(n,\xi^{\log \xi})$,
which can be improved to polynomial-time assuming APFR. This gives an
unconditional learning algorithm for stabilizer-rank $k$ states in time
$\textsf{poly}(n,k^{k^2})$. As far as we know, learning arbitrary states with
even stabilizer-rank $k \geq 2$ was unknown.

</details>


### [244] [Hybrid quantum-classical analog simulation of two-dimensional Fermi-Hubbard models with neutral atoms](https://arxiv.org/abs/2510.05897)
*Sergi Julià-Farré,Antoine Michel,Christophe Domain,Joseph Mikael,Jacques-Charles Lafoucriere,Joseph Vovrosh,Ahmed Chahlaoui,Dorian Claveau,Guillaume Villaret,Julius de Hond,Loïc Henriet,Antoine Browaeys,Thomas Ayral,Alexandre Dauphin*

Main category: quant-ph

TL;DR: A Rydberg-based quantum computer and a hybrid quantum-classical algorithm are used to study the Fermi-Hubbard model by reformulating it into a solvable free-fermion problem and an interacting spin problem. This approach can be used to study equilibrium and non-equilibrium properties of the model, showing the potential of hybrid methods for strongly correlated matter.


<details>
  <summary>Details</summary>
Motivation: The paper aims to experimentally study the two-dimensional Fermi-Hubbard model using a Rydberg-based quantum processing unit in analog mode, by reformulating the model to overcome limitations of direct fermion encoding.

Method: The study reformulates the Fermi-Hubbard model into a system of fermions coupled to spins, then decouples them. A hybrid quantum-classical algorithm, the auxiliary spin solver, is introduced. This algorithm separates the problem into a free-fermion part (solved classically) and an interacting spin part (encoded in the analog quantum computer).

Result: The hybrid approach allows for the study of both equilibrium properties (Mott transition) and non-equilibrium properties of the Fermi-Hubbard model.

Conclusion: Quantum-classical hybrid approaches show significant potential for studying strongly correlated matter, as demonstrated by the successful application to the Fermi-Hubbard model.

Abstract: We experimentally study the two-dimensional Fermi-Hubbard model using a
Rydberg-based quantum processing unit in the analog mode. Our approach avoids
encoding directly the original fermions into qubits and instead relies on
reformulating the original model onto a system of fermions coupled to spins and
then decoupling them in a self-consistent manner. We then introduce the
auxiliary spin solver: this hybrid quantum-classical algorithm handles a
free-fermion problem, which can be solved efficiently with a few classical
resources, and an interacting spin problem, which can be naturally encoded in
the analog quantum computer. This algorithm can be used to study both the
equilibrium Mott transition as well as non-equilibrium properties of the
original Fermi-Hubbard model, highlighting the potential of quantum-classical
hybrid approaches to study strongly correlated matter.

</details>


### [245] [Fast and Robust Non-Adiabatic Holonomic Gates for Qutrit Systems](https://arxiv.org/abs/2510.05905)
*Jie Lu,Jie-Dong Huang,Yang Qian,Ying Yan*

Main category: quant-ph

TL;DR: 本文提出了一种结合逆向工程和时变微扰理论的框架，用于在包含实际误差源的qutrit系统中实现非绝热全息量子计算（NHQC），以实现快速且抗干扰的全息门操作。


<details>
  <summary>Details</summary>
Motivation: 受绝热性捷径的启发，本文旨在开发一种框架，用于在qutrit系统中实现非绝热全息量子计算（NHQC），并应对实际的误差源。

Method: 本文结合逆向工程和时变微扰理论，推导出抑制二阶Rabi误差和消除失谐误差的解析条件，并提出通过补偿脉冲来确保鲁棒的门操作。

Result: 分析表明，补偿脉冲能够有效消除失谐误差，确保门操作的鲁棒性。该方法在超导量子比特、离子阱和基于原子的平台上具有广泛适用性。

Conclusion: 本文提出的NHQC方法为实现快速且抗干扰的全息门操作提供了一条可行的途径，有望用于可扩展的量子计算。

Abstract: Motivated by shortcuts to adiabaticity, we develop a framework that combines
inverse engineering with time-dependent perturbation theory to implement
non-adiabatic holonomic quantum computing (NHQC) in qutrit systems under
realistic error sources. We derive analytical conditions that suppress
second-order Rabi errors through tailored pulse parameters and eliminate
detuning errors via a compensation pulse. Our analysis reveals the cancellation
from the compensation pulse, which ensures robust gate operations. The proposed
approach, broadly applicable to superconducting qubits, trapped ions, and
atom-based platforms, provides a pathway to fast and error-resilient holonomic
gates, potentially for scalable quantum computing.

</details>


### [246] [Angular--Momentum--Resolved Aharonov--Bohm Coupling Energy](https://arxiv.org/abs/2510.06016)
*Ju Gao,Fang Shen*

Main category: quant-ph

TL;DR: Aharonov-Bohm (AB) 效应可以被理解为带电粒子环绕磁通量时获得的相位移，而其路径上没有局部场作用。本文研究发现，受限的狄拉克电子会表现出不同的 AB 耦合能量，这是由局部电流-势能相互作用产生的，其形式取决于选择的处方。在波-粒（WP）处方中，响应被限制在磁通量核心内，只有 l=0 的模式在核心收缩时留下有限的残余，而所有更高模式都消失。在波-实体（WE）处方中，l=0 的结果与 WP 相同，但对于 l≥1，响应变为量化的、与 l 呈线性关系的能量位移。因此，AB 效应表现为一种量化的、模式分辨的能量定律，它通过标准的场耦合建立局部性，并区分电子处方。


<details>
  <summary>Details</summary>
Motivation: 研究 Aharonov-Bohm (AB) 效应的传统解释，并探索受限狄拉克电子在局部相互作用下的 AB 耦合能量。

Method: 在波-粒（WP）和波-实体（WE）两种不同的处方下，分析受限狄拉克电子的 AB 效应，特别是 l=0 和 l≥1 的模式。

Result: 在 WP 处方下，只有 l=0 模式留下有限的残余；在 WE 处方下，l=0 模式结果与 WP 相同，但 l≥1 的模式表现为量化的、与 l 呈线性关系的能量位移。

Conclusion: AB 效应可以被视为一种量化的、模式分辨的能量定律，它通过标准的场耦合建立局部性，并能区分不同的电子处方。

Abstract: The Aharonov--Bohm (AB) effect is conventionally interpreted as a phase shift
acquired by charged particles encircling a flux, with no fields acting locally
along their paths. Here we show that a confined Dirac electron exhibits a
distinct AB \emph{coupling energy} arising from a \emph{local
current--potential interaction}, whose form depends on the chosen prescription.
In the \emph{wave--particle} (WP) prescription the response is confined to the
flux core: only the $l=0$ mode leaves a finite remnant as the core shrinks,
while all higher modes vanish. In the \emph{wave--entity} (WE) prescription the
$l=0$ result coincides with WP, but for $l\!\ge\!1$ the response becomes a
quantized, $l$--linear energy shift. The AB effect thereby emerges as a
quantized, mode--resolved energy law that establishes locality through standard
field coupling and distinguishes between electron prescriptions.

</details>


### [247] [Quantum Annealing for Realistic Traffic Flow Optimization: Clustering and Data-Driven QUBO](https://arxiv.org/abs/2510.06053)
*Renáta Rusnáková,Martin Chovanec,Juraj Gazda*

Main category: quant-ph

TL;DR: 数据驱动方法将城市交通优化重构为二次无约束二元优化问题，并利用混合量子退火解决大规模网络拥堵问题。


<details>
  <summary>Details</summary>
Motivation: 城市交通管理是一个复杂的NP-hard问题，传统方法难以扩展，需要新的数据驱动方法来优化拥堵和出行时间。

Method: 将交通优化重构为二次无约束二元优化问题，整合模拟的出行数据、多种路线选择和分析得出的惩罚约束。应用Leiden聚类来减小大规模网络规模，并利用混合量子退火解决问题。

Result: 在最多25,000辆车的基准测试中，混合量子退火在1%的容差内接近经典求解器Gurobi的最优解，并将拥堵减少了高达25%。

Conclusion: 混合量子退火是一种有效的方法，可以解决大规模城市交通管理问题，在保持接近最优解的同时显著减少拥堵。

Abstract: Managing city traffic is a complex NP-hard problem where traditional methods
often fail to scale. We present a data-driven approach that reformulates
traffic optimization as a Quadratic Unconstrained Binary Optimization,
capturing both congestion reduction and travel-time efficiency. The model
integrates simulated realistic mobility data, multiple routing alternatives,
and analytically derived penalty constraints. To address large networks, we
apply Leiden clustering to preserve critical congestion patterns while reducing
problem size. Benchmarking on up to 25,000 vehicles shows that hybrid quantum
annealing achieves near-optimal solutions within 1% of the classical solver
Gurobi while reducing congestion by up to 25%.

</details>


### [248] [Noise-induced decoherence-free zones for anyons](https://arxiv.org/abs/2510.06094)
*Eric R. Bittner*

Main category: quant-ph

TL;DR: 我们提出了一个随机框架，将任意子系统中的交换相位视为一个变化的量，而不是固定的参数。我们从Stratonovich随机Liouville方程出发，进行Stratonovich-Itô转换，得到一个Lindblad主方程，该方程将耗散项直接与扭曲的任意子代数联系起来。


<details>
  <summary>Details</summary>
Motivation: 将任意子系统中的交换相位从固定参数提升为变化的量，以研究其对系统退相干的影响。

Method: 从Stratonovich随机Liouville方程出发，进行Stratonovich-Itô转换，得到一个Lindblad主方程，并将耗散项与扭曲的任意子代数联系起来。

Result: 构建了一个依赖于统计量的退相干通道，其速率由实对称相关矩阵D_ab的本征结构决定。D的特征向量选择哪些集体交换流（等价地说，系统的哪些不可约表示）可以免受随机退相干的影响，从而为无退相干子空间和噪声诱导的例外点提供了一种自然的机制。分析的关键结果是最佳统计角度的普适性：在具有平衡增益和损耗的最小双站点模型中，受保护模式的总是在θ*=π/2时最小化其退相干，这与D的具体形式无关。

Conclusion: 最优统计角度的普适性强调了优化嘈杂任意子系统相干性的简单设计规则，对超冷原子实现和分数统计的其他新兴平台具有直接意义。

Abstract: We develop a stochastic framework for anyonic systems in which the exchange
phase is promoted from a fixed parameter to a fluctuating quantity. Starting
from the Stratonovich stochastic Liouville equation, we perform the
Stratonovich--It\^o conversion to obtain a Lindblad master equation that ties
the dissipator directly to the distorted anyon algebra. This construction
produces a statistics--dependent dephasing channel, with rates determined by
the eigenstructure of the real symmetric correlation matrix $D_{ab}$. The
eigenvectors of $D$ select which collective exchange currents -- equivalently,
which irreducible representations of the system -- are protected from
stochastic dephasing, providing a natural mechanism for decoherence-free
subspaces and noise-induced exceptional points. The key result of our analysis
is the universality of the optimal statistical angle: in the minimal two-site
model with balanced gain and loss, the protected mode always minimizes its
dephasing at $\theta^\star = \pi/2$, independent of the specific form of $D$.
This robustness highlights a simple design rule for optimizing coherence in
noisy anyonic systems, with direct implications for ultracold atomic
realizations and other emerging platforms for fractional statistics.

</details>


### [249] [On the Quantum Equivalence between $S|LWE\rangle$ and $ISIS$](https://arxiv.org/abs/2510.06097)
*André Chailloux,Paul Hermouet*

Main category: quant-ph

TL;DR: 本论文研究了量子学习错误问题（S|LWE⟩）与非齐次短整数解问题（ISIS）之间的等价性，并提出了从ISIS到S|LWE⟩的通用规约，以及一种新的量子学习错误问题变体（IC|LWE⟩）并证明了其规约到S|LWE⟩。最后，论文证明了在特定条件下，ISIS求解器可以转化为S|LWE⟩求解器，并通过实例验证了该方法在特定参数下的有效性。


<details>
  <summary>Details</summary>
Motivation: CLZ22引入了S|LWE⟩和C|LWE⟩问题作为LWE问题的量子类似，用于构建ISIS问题的量子算法。然而，这些问题之间的普遍联系尚不清楚。本研究旨在阐明S|LWE⟩和ISIS之间的规约关系。

Method: 1. 提出从ISIS到S|LWE⟩的通用规约，该规约在存在错误的情况下也成立。
2. 引入C|LWE⟩的非齐次变体IC|LWE⟩，并证明IC|LWE⟩可以规约到S|LWE⟩。
3. 证明在特定可恢复性条件下，ISIS算法可以转化为S|LWE⟩算法。
4. 通过修改已知的(I)SIS_∞算法，在字母表大小q为2的小幂次时，构建S|LWE⟩的量子算法，并验证了部分已有结果。

Result: 1. 实现了从ISIS到S|LWE⟩的第一个通用规约。
2. 证明了IC|LWE⟩可以规约到S|LWE⟩。
3. 提出了ISIS到S|LWE⟩的逆向规约，并在特定条件下得到验证。
4. 发现了S|LWE⟩和ISIS之间紧密联系，但也揭示了证明它们完全等价性所面临的挑战。

Conclusion: 本研究阐明了S|LWE⟩和ISIS之间的规约关系，证明了它们之间的强联系，并指出了未来研究中需要克服的障碍，以实现两者之间的完全等价性。 DICE-QA模型在各种基准测试中都表现出了优越的性能。S|LWE⟩和ISIS之间的具体联系以及证明它们完全等价性的方法仍是开放性问题。

Abstract: Chen, Liu, and Zhandry [CLZ22] introduced the problems $S|LWE\rangle$ and
$C|LWE\rangle$ as quantum analogues of the Learning with Errors problem,
designed to construct quantum algorithms for the Inhomogeneous Short Integer
Solution ($ISIS$) problem. Several later works have used this framework for
constructing new quantum algorithms in specific cases. However, the general
relation between all these problems is still unknown. In this paper, we
investigate the equivalence between $S|LWE\rangle$ and $ISIS$. We present the
first fully generic reduction from $ISIS$ to $S|LWE\rangle$, valid even in the
presence of errors in the underlying algorithms. We then explore the reverse
direction, introducing an inhomogeneous variant of $C|LWE\rangle$, denoted
$IC|LWE\rangle$, and show that $IC|LWE\rangle$ reduces to $S|LWE\rangle$.
Finally, we prove that, under certain recoverability conditions, an algorithm
for $ISIS$ can be transformed into one for $S|LWE\rangle$. We instantiate this
reverse reduction by tweaking a known algorithm for $(I)SIS_\infty$ in order to
construct quantum algorithm for $S|LWE\rangle$ when the alphabet size q is a
small power of 2, recovering some results of Bai et al. [BJK+ 25]. Our results
thus clarify the landscape of reductions between $S|LWE\rangle$ and $ISIS$, and
we show both their strong connection as well as the remaining barriers for
showing full equivalence.

</details>


### [250] [Quantum Strategies to Overcome Classical Multiplexing Limits](https://arxiv.org/abs/2510.06099)
*Tzula B. Propp,Bethany Davies,Jeroen Grimbergen,Emil R. Hellebek,Junior R. Gonzales-Ureta,Janice van Dam,Joshua A. Slater,Anders S. Sørensen,Stephanie D. C. Wehner*

Main category: quant-ph

TL;DR: 量子网络通信速率低是近期的瓶颈，限制了量子互联网应用。本文提出了量子多路复用和多服务器多路复用技术，以克服这一限制，并能实现超越经典的多路复用优势。通过三个示例应用（纠缠生成、多用户远程态制备、多节点远程态制备）的说明，这些策略为实现高速多量子比特网络应用提供了新途径。


<details>
  <summary>Details</summary>
Motivation: 近期的量子网络面临通信速率低的问题，这会降低运行速度并增加量子比特在嘈杂内存中的存储时间，使得一些量子互联网应用变得不可行。

Method: 本文推导了多量子比特协议的半经典极限，并提出了新的技术：量子多路复用和多服务器多路复用。

Result: 通过纠缠生成、远程态制备等三个示例应用，说明了所提出的多路复用技术能够实现超越经典的多路复用优势。

Conclusion: 所提出的多路复用策略能够利用许多嘈杂的、互联的量子设备，而不是少数低噪声设备，为实现高速多量子比特量子网络应用开辟了新路径。

Abstract: Near-term quantum networks face a bottleneck due to low quantum communication
rates. This degrades performance both by lowering operating speeds and
increasing qubit storage time in noisy memories, making some quantum internet
applications infeasible. One way to circumvent this bottleneck is multiplexing:
combining multiple signals into a single signal to improve the overall rate.
Standard multiplexing techniques are classical in that they do not make use of
coherence between quantum channels nor account for decoherence rates that vary
during a protocol's execution. In this paper, we first derive semiclassical
limits to multiplexing for many-qubit protocols, and then introduce new
techniques: quantum multiplexing and multi-server multiplexing. These can
enable beyond-classical multiplexing advantages. We illustrate these techniques
through three example applications: 1) entanglement generation between two
asymetric quantum network nodes (i.e., repeaters or quantum servers with
inequal memories), 2) remote state preparation between many end user devices
and a single quantum node, and 3) remote state preparation between one end user
device and many internetworked quantum nodes. By utilizing many noisy
internetworked quantum devices instead of fewer low-noise devices, our
multiplexing strategies enable new paths towards achieving high-speed
many-qubit quantum network applications.

</details>


### [251] [Self-concordant Schrödinger operators: spectral gaps and optimization without condition numbers](https://arxiv.org/abs/2510.06115)
*Sander Gribling,Simon Apers,Harold Nieuwboer,Michael Walter*

Main category: quant-ph

TL;DR: 本文研究了与自协和势垒相关的薛定谔算子的谱隙，并提出了量子内点法。


<details>
  <summary>Details</summary>
Motivation: 研究薛定谔算子谱隙在量子力学、量子计算和凸优化中的重要性。

Method: 利用半经典分析、凸优化和量子退火技术，研究与自协和势垒相关的薛定谔算子，并证明了其谱隙的非渐近下界。

Result: 发现谱隙不依赖于条件数，并提出了一种不依赖条件数的量子内点法。

Conclusion: 所提出的量子内点法适用于任意自协和势垒，克服了传统方法的局限性。

Abstract: Spectral gaps play a fundamental role in many areas of mathematics, computer
science, and physics. In quantum mechanics, the spectral gap of Schr\"odinger
operators has a long history of study due to its physical relevance, while in
quantum computing spectral gaps are an important proxy for efficiency, such as
in the quantum adiabatic algorithm. Motivated by convex optimization, we study
Schr\"odinger operators associated with self-concordant barriers over convex
domains and prove non-asymptotic lower bounds on the spectral gap for this
class of operators. Significantly, we find that the spectral gap does not
display any condition-number dependence when the usual Laplacian is replaced by
the Laplace--Beltrami operator, which uses second-order information of the
barrier and hence can take the curvature of the barrier into account. As an
algorithmic application, we construct a novel quantum interior point method
that applies to arbitrary self-concordant barriers and shows no
condition-number dependence. To achieve this we combine techniques from
semiclassical analysis, convex optimization, and quantum annealing.

</details>


### [252] [Batched high-rate logical operations for quantum LDPC codes](https://arxiv.org/abs/2510.06159)
*Qian Xu,Hengyun Zhou,Dolev Bluvstein,Madelyn Cain,Marcin Kalinowski,John Preskill,Mikhail D. Lukin,Nishad Maskara*

Main category: quant-ph

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: High-rate quantum LDPC (qLDPC) codes reduce memory overhead by densely
packing many logical qubits into a single block of physical qubits. Here we
extend this concept to high-rate computation by constructing \emph{batched}
fault-tolerant operations that apply the same logical gate across many code
blocks in parallel. By leveraging shared physical resources to execute many
logical operations in parallel, these operations realize high rates in
space-time and significantly reduce computational costs. For \emph{arbitrary}
CSS qLDPC codes, we build batched gadgets with \emph{constant space-time
overhead} (assuming fast classical computation) for (i) single-shot error
correction, state preparation, and code surgeries (ii) code switching, and
(iii) addressable Clifford gates. Using these batched gadgets we also construct
parallel non-Clifford gates with low space-time cost. We outline principles for
designing parallel quantum algorithms optimized for a batched architecture, and
show in particular how lattice Hamiltonian dynamical simulations can be
compiled efficiently. We also propose a near-term implementation using new
self-dual Bivariate-Bicycle codes with high encoding rates ($\sim 1/10$),
transversal Clifford gates, and global $T$ gates via parallel magic state
cultivation, enabling Hamiltonian simulations with a lower space-time cost than
analogous surface-code protocols and low-rate qLDPC protocols. These results
open new paths toward scalable quantum computation via co-design of parallel
quantum algorithms and high-rate fault-tolerant protocols.

</details>


### [253] [Quantumness and its hierarchies in PT-symmetric down-conversion models](https://arxiv.org/abs/2510.06171)
*Jan Peřina Jr.,Karol Bartkiewicz,Grzegorz Chimczak,Anna Kowalewska-Kudlaszyk,Adam Miranowicz,Joanna K. Kalaga,Wiesław Leonski*

Main category: quant-ph

TL;DR: PTSS系统中量子关联的层级，重点关注物理非线性和增益/损耗动力学的影响。


<details>
  <summary>Details</summary>
Motivation: 研究具有不同耗散和放大通道的二次玻色子宇称-时间对称系统（PTSS）中量子关联的层级，并阐明系统物理非线性与PTSS动力学之间的相互作用。

Method: 使用局部和全局非经典性深度、负值、诱导参数和贝尔参数等量化指标，比较标准PTSS、仅受阻尼或仅受放大的PTSS，以及标准、无源和有源PTSS。

Result: 标准PTSS的量子性通常弱于仅受阻尼或仅受放大的PTSS。无源PTSS产生最强的非经典态，但标准PTSS在特定条件下也能产生高度非经典态。

Conclusion: 量子关联的层级受到PTSS动力学的显著影响，其中量子涨落起着关键作用。无源PTSS在产生非经典态方面具有优势，且在实际应用中更具优势。

Abstract: We investigate the hierarchy of quantum correlations in a quadratic bosonic
parity-time-symmetric system (PTSS) featuring distinct dissipation and
amplification channels. The hierarchy includes global nonclassicality,
entanglement, asymmetric quantum steering, and Bell nonlocality. We elucidate
the interplay between the system physical nonlinearity -- which serves as a
source of quantumness -- and the specific dynamics of bosonic PTSSs, which are
qualitatively influenced by their damping and amplification characteristics.
Using a set of quantifiers -- including local and global nonclassicality
depths, negativity, steering parameters, and the Bell parameter -- we
demonstrate that the standard PTSS typically exhibits weaker quantumness than
its counterparts affected solely by damping or solely by amplification. Both
the maximum values attained by these quantifiers and the speed and duration of
their generation are generally lower in the standard PTSS. A comparative
analysis of three two-mode PTSSs -- standard, passive, and active -- with
identical eigenvectors and real parts of eigenfrequencies, but differing in
their damping and amplification strengths, reveals the crucial role of quantum
fluctuations associated with gain and loss. Among them, the passive PTSS yields
the most strongly nonclassical states. Nevertheless, under suitable conditions,
the standard PTSS can also generate highly nonclassical states. The supremacy
of the passive PTSS is further supported by its fundamental advantages in
practical realizations.

</details>


### [254] [Anchor: Reducing Temporal and Spatial Output Performance Variability on Quantum Computers](https://arxiv.org/abs/2510.06172)
*Yuqian Huo,Daniel Leeds,Jason Ludmir,Nicholas S. DiBrita,Tirthak Patel*

Main category: quant-ph

TL;DR: Anchor技术通过利用线性规划来减少量子计算的性能变化，平均可减少73%的性能变化。


<details>
  <summary>Details</summary>
Motivation: 现有的量子计算机存在硬件噪声，导致程序输出错误和性能（输出保真度）不一致。现有解决方案侧重于减少错误，但未能解决性能不一致的问题。

Method: 提出了一种名为Anchor的新技术，利用线性规划来减少性能变化。

Result: Anchor技术平均将性能变化减少了73%，优于现有的专注于减少错误的方法。

Conclusion: Anchor是首个解决量子计算性能不一致问题的技术，通过线性规划显著减少了性能变化。

Abstract: Quantum computing, which has the power to accelerate many computing
applications, is currently a technology under development. As a result, the
existing noisy intermediate-scale quantum (NISQ) computers suffer from
different hardware noise effects, which cause errors in the output of quantum
programs. These errors cause a high degree of variability in the performance
(i.e., output fidelity) of quantum programs, which varies from one computer to
another and from one day to another. Consequently, users are unable to get
consistent results even when running the same program multiple times. Current
solutions, while focusing on reducing the errors faced by quantum programs, do
not address the variability challenge. To address this challenge, we propose
Anchor, a first-of-its-kind technique that leverages linear programming to
reduce the performance variability by 73% on average over the state-of-the-art
implementation focused on error reduction.

</details>


### [255] [Quantum $f$-divergences and Their Local Behaviour: An Analysis via Relative Expansion Coefficients](https://arxiv.org/abs/2510.06183)
*Paula Belzig,Shreyas Iyer,Graeme Smith,Peixue Wu*

Main category: quant-ph

TL;DR: 任何区分量子态的合理度量都必须满足数据处理不等式，即它在量子通道的作用下不应增加。通过研究收缩和扩张系数，可以得到一个单一的相对扩张系数。本文重点研究了两种主要的 f 散度及其局部行为（与 χ^2 散度相关）。研究人员确定了新的 f 族，对于这些 f 族，全局（f 散度）和局部（黎曼）相对扩张系数对于每一对通道都恰好相等，并阐明了这种精确匹配的稀有性。此外，研究人员还引入了一个等价框架，用于在不同的相对扩张系数之间传递严格正值等定性性质。通过利用数据处理不等式（DPI）中的相等性与信道可逆性之间的联系，将相对扩张系数框架应用于量子信息近似可恢复性。利用原始信道的相对扩张结果，证明了一个反向量子马尔可夫收敛定理，将正扩张系数转换为收敛速率的定量下界。


<details>
  <summary>Details</summary>
Motivation: 区分量子态的度量必须满足数据处理不等式，即它在量子通道作用下不应增加。研究收缩和扩张系数，以及它们的相对扩张系数，对于理解信息在量子过程中的保持或损失至关重要。

Method: 本文研究了两种量子 f 散度及其局部行为（与 χ^2 散度相关）。通过识别新的 f 族，使全局和局部相对扩张系数对所有通道对都相等。然后，研究人员引入了一个等价框架，以在不同相对扩张系数之间传递定性属性。最后，将相对扩张系数框架应用于近似可恢复性，并证明了一个反向量子马尔可夫收敛定理。

Result: 发现了新的 f 族，使得全局和局部相对扩张系数对于所有通道对都相等。引入了一个等价框架，用于在不同相对扩张系数之间传递定性属性。证明了一个反向量子马尔可夫收敛定理，将正扩张系数转换为收敛速率的定量下界。

Conclusion: 本文通过研究量子 f 散度和其局部行为的相对扩张系数，揭示了在特定条件下全局和局部度量可以精确匹配。引入的等价框架为理解和传递不同度量之间的定性属性提供了工具。此外，在量子信息近似可恢复性方面的应用，以及反向量子马尔可夫收敛定理的证明，为量化量子信息处理的收敛速率提供了新的见解。

Abstract: Any reasonable measure of distinguishability of quantum states must satisfy a
data processing inequality, that is, it must not increase under the action of a
quantum channel. We can ask about the proportion of information lost or
preserved and this leads us to study contraction and expansion coefficients
respectively, which can be combined into a single \emph{relative expansion
coefficient} for study. We focus on two prominent families: (i) standard
quantum $f$ divergences and (ii) their local (second-order) behaviour, which
induces a monotone Riemannian semi-norm (that is linked to the $\chi^2$
divergence). Building on prior work, we identify new families of $f$ for which
the global ($f$ divergence) and local (Riemannian) relative expansion
coefficients coincide for every pair of channels, and we clarify how
exceptional such exact coincidences are. Beyond equality, we introduce an
\emph{equivalence} framework that transfers qualitative properties such as
strict positivity uniformly across different relative expansion coefficients.
  Leveraging the link between equality in the data processing inequality (DPI)
and channel reversibility, we apply our framework of relative expansion
coefficients to approximate recoverability of quantum information. Using our
relative expansion results for primitive channels, we prove a reverse quantum
Markov convergence theorem, converting positive expansion coefficients into
quantitative lower bounds on the convergence rate.

</details>


### [256] [Anonymous Quantum Tokens with Classical Verification](https://arxiv.org/abs/2510.06212)
*Dmytro Gavinsky,Dar Gilboa,Siddhartha Jain,Dmitri Maslov,Jarrod R. McClean*

Main category: quant-ph

TL;DR: 该研究提出了一种单次使用的量子金钱方案，解决了现有方案中量子内存/通信需求和隐私泄露问题，并且验证过程是经典的。


<details>
  <summary>Details</summary>
Motivation: 现有量子金钱方案存在需要长期量子内存和量子通信，以及用户隐私泄露的问题。

Method: 提出了一种单次使用的量子金钱方案，并包含一个审计程序来检测发行方是否在追踪用户。

Result: 该方案实现了单次使用的量子金钱，用户可以检测是否被追踪，且验证过程为经典，无需长期量子内存或量子通信。

Conclusion: 该方案是单次使用的量子金钱，解决了现有方案的缺点，并且具有实际部署的可能性，还可以应用于其他领域。

Abstract: The no-cloning theorem can be used as a basis for quantum money constructions
which guarantee unconditionally unforgeable currency. Existing schemes,
however, either (i) require long-term quantum memory and quantum communication
between the user and the bank in order to verify the validity of a bill or (ii)
fail to protect user privacy due to the uniqueness of each bill issued by the
bank, which can allow its usage to be tracked. We introduce a construction of
single-use quantum money that gives users the ability to detect whether the
issuing authority is tracking them, employing an auditing procedure for which
we prove unconditional security. Bill validation is classical, and hence does
not require long-term quantum memory or quantum communication, making the
protocol relatively practical to deploy. We discuss potential applications
beyond money, including anonymous one-time pads and voting.

</details>


<div id='cs.NE'></div>

# cs.NE [[Back]](#toc)

### [257] [From Neural Activity to Computation: Biological Reservoirs for Pattern Recognition in Digit Classification](https://arxiv.org/abs/2510.05637)
*Ludovico Iannello,Luca Ciampi,Fabrizio Tonelli,Gabriele Lagani,Lucio Maria Calcagnile,Federico Cremisi,Angelo Di Garbo,Giuseppe Amato*

Main category: cs.NE

TL;DR: 使用包含培养的生物神经元的网络作为计算基板，我们提出了生物水库计算（BRC）。BRC 使用神经元的自发和诱发活动来执行计算，并使用多电极阵列（MEA）进行输入和输出。在一项数字分类任务的研究中，BRC 成功地支持了计算，其性能可与标准的人工水库相媲美，表明其作为计算基板的潜力。


<details>
  <summary>Details</summary>
Motivation: 将生物原理整合到机器学习中，并探索活体神经系统如何为高效、符合生物学原理的模型设计提供信息，同时在人类启发的视觉领域取得进展。

Method: 使用由培养的生物神经元组成的水库，通过多电极阵列（MEA）施加电刺激作为输入，并记录神经元活动以进行输出。对数字分类任务进行了评估，并将BRC与人工水库进行了比较。

Result: 生物水库计算（BRC）在数字分类任务中显示出有效支持计算的能力，并且其性能与人工水库相当。

Conclusion: 生物水库计算（BRC）有潜力作为一种可行的、可解释的计算基板，为将生物原理融入机器学习提供了新的途径。

Abstract: In this paper, we present a biologically grounded approach to reservoir
computing (RC), in which a network of cultured biological neurons serves as the
reservoir substrate. This system, referred to as biological reservoir computing
(BRC), replaces artificial recurrent units with the spontaneous and evoked
activity of living neurons. A multi-electrode array (MEA) enables simultaneous
stimulation and readout across multiple sites: inputs are delivered through a
subset of electrodes, while the remaining ones capture the resulting neural
responses, mapping input patterns into a high-dimensional biological feature
space. We evaluate the system through a case study on digit classification
using a custom dataset. Input images are encoded and delivered to the
biological reservoir via electrical stimulation, and the corresponding neural
activity is used to train a simple linear classifier. To contextualize the
performance of the biological system, we also include a comparison with a
standard artificial reservoir trained on the same task. The results indicate
that the biological reservoir can effectively support classification,
highlighting its potential as a viable and interpretable computational
substrate. We believe this work contributes to the broader effort of
integrating biological principles into machine learning and aligns with the
goals of human-inspired vision by exploring how living neural systems can
inform the design of efficient and biologically plausible models.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [258] [Inductive inference of gradient-boosted decision trees on graphs for insurance fraud detection](https://arxiv.org/abs/2510.05676)
*Félix Vandervorst,Bruno Deprez,Wouter Verbeke,Tim Verdonck*

Main category: cs.LG

TL;DR: 图模型在机器学习领域日益受到关注，尤其在处理保险欺诈等复杂关系数据方面。然而，现有图模型在处理高类别不平衡和异构动态图数据时面临挑战，导致基于梯度提升树的表格数据方法仍占主导地位。本文提出了一种新的归纳图梯度提升机（G-GBM），用于在异构动态图上进行监督学习。实验证明，G-GBM在模拟随机图和真实的保险欺诈检测任务上，与主流图神经网络方法具有竞争力。此外，得益于其梯度提升树的骨干模型，G-GBM能够应用成熟的可解释性方法，提供更深入的预测洞察。


<details>
  <summary>Details</summary>
Motivation: 保险欺诈数据通常存在高类别不平衡，并且保险网络具有异构和动态的特点，这给传统的图模型带来了挑战，使得梯度提升树在表格数据上仍是主流方法。因此，需要一种能够处理异构动态图并且能克服类别不平衡问题的图模型。

Method: 提出了一种新的归纳图梯度提升机（G-GBM），该模型基于梯度提升树，能够处理异构和动态的图数据，并适用于监督学习任务。

Result: 在模拟随机图和真实的保险欺诈检测数据集（包括开源和专有数据集）上，G-GBM与流行的图神经网络方法相比具有竞争力。此外，G-GBM可以利用成熟的可解释性方法来增强预测的洞察力。

Conclusion: G-GBM是一种有效的图模型，能够处理保险欺诈检测中的挑战，并在性能和可解释性方面展现出优势。

Abstract: Graph-based methods are becoming increasingly popular in machine learning due
to their ability to model complex data and relations. Insurance fraud is a
prime use case, since false claims are often the result of organised criminals
that stage accidents or the same persons filing erroneous claims on multiple
policies. One challenge is that graph-based approaches struggle to find
meaningful representations of the data because of the high class imbalance
present in fraud data. Another is that insurance networks are heterogeneous and
dynamic, given the changing relations among people, companies and policies.
That is why gradient boosted tree approaches on tabular data still dominate the
field. Therefore, we present a novel inductive graph gradient boosting machine
(G-GBM) for supervised learning on heterogeneous and dynamic graphs. We show
that our estimator competes with popular graph neural network approaches in an
experiment using a variety of simulated random graphs. We demonstrate the power
of G-GBM for insurance fraud detection using an open-source and a real-world,
proprietary dataset. Given that the backbone model is a gradient boosting
forest, we apply established explainability methods to gain better insights
into the predictions made by G-GBM.

</details>


### [259] [A Fuzzy Logic-Based Framework for Explainable Machine Learning in Big Data Analytics](https://arxiv.org/abs/2510.05120)
*Farjana Yesmin,Nusrat Shirmin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The growing complexity of machine learning (ML) models in big data analytics,
especially in domains such as environmental monitoring, highlights the critical
need for interpretability and explainability to promote trust, ethical
considerations, and regulatory adherence (e.g., GDPR). Traditional "black-box"
models obstruct transparency, whereas post-hoc explainable AI (XAI) techniques
like LIME and SHAP frequently compromise accuracy or fail to deliver inherent
insights. This paper presents a novel framework that combines type-2 fuzzy
sets, granular computing, and clustering to boost explainability and fairness
in big data environments. When applied to the UCI Air Quality dataset, the
framework effectively manages uncertainty in noisy sensor data, produces
linguistic rules, and assesses fairness using silhouette scores and entropy.
Key contributions encompass: (1) A type-2 fuzzy clustering approach that
enhances cohesion by about 4% compared to type-1 methods (silhouette 0.365 vs.
0.349) and improves fairness (entropy 0.918); (2) Incorporation of fairness
measures to mitigate biases in unsupervised scenarios; (3) A rule-based
component for intrinsic XAI, achieving an average coverage of 0.65; (4)
Scalable assessments showing linear runtime (roughly 0.005 seconds for sampled
big data sizes). Experimental outcomes reveal superior performance relative to
baselines such as DBSCAN and Agglomerative Clustering in terms of
interpretability, fairness, and efficiency. Notably, the proposed method
achieves a 4% improvement in silhouette score over type-1 fuzzy clustering and
outperforms baselines in fairness (entropy reduction by up to 1%) and
efficiency.

</details>


### [260] [Monte Carlo-Type Neural Operator for Differential Equations](https://arxiv.org/abs/2510.05620)
*Salah Eddine Choutri,Prajwal Chauhan,Othmane Mazhar,Saif Eddin Jabari*

Main category: cs.LG

TL;DR: MCNO是一种用于学习一维偏微分方程解算子（PDE）的框架，通过直接学习核函数并使用蒙特卡洛型方法逼近相关的积分算子，可处理多分辨率和任意网格，在标准一维PDE基准测试中表现出竞争力，并具有理论支持。


<details>
  <summary>Details</summary>
Motivation: MCNO旨在为学习一维偏微分方程（PDE）的解算子提供一种新框架，克服现有方法（如FNO）的局限性，探索将蒙特卡洛积分融入神经算子框架以处理连续域PDE。

Method: MCNO直接学习核函数，并使用蒙特卡洛型方法逼近积分算子。核函数表示为在采样输入-输出对上的可学习张量，采样一次。通过插值步骤映射到任意输入和输出网格。

Result: MCNO在标准1D PDE基准测试中达到了具有竞争力的精度和高效的计算成本。理论分析证明了蒙特卡洛估计在温和的正则性假设下具有有限的偏差和方差。

Conclusion: MCNO提供了一种理论支持的、可替代傅立叶神经算子（FNO）等谱方法以及图核神经算子（GNO）等图基蒙特卡洛方法的选择，并且可能自然地扩展到多维问题。

Abstract: The Monte Carlo-type Neural Operator (MCNO) introduces a framework for
learning solution operators of one-dimensional partial differential equations
(PDEs) by directly learning the kernel function and approximating the
associated integral operator using a Monte Carlo-type approach. Unlike Fourier
Neural Operators (FNOs), which rely on spectral representations and assume
translation-invariant kernels, MCNO makes no such assumptions. The kernel is
represented as a learnable tensor over sampled input-output pairs, and sampling
is performed once, uniformly at random from a discretized grid. This design
enables generalization across multiple grid resolutions without relying on
fixed global basis functions or repeated sampling during training, while an
interpolation step maps between arbitrary input and output grids to further
enhance flexibility. Experiments on standard 1D PDE benchmarks show that MCNO
achieves competitive accuracy with efficient computational cost. We also
provide a theoretical analysis proving that the Monte Carlo estimator yields a
bounded bias and variance under mild regularity assumptions. This result holds
in any spatial dimension, suggesting that MCNO may extend naturally beyond
one-dimensional problems. More broadly, this work explores how Monte Carlo-type
integration can be incorporated into neural operator frameworks for
continuous-domain PDEs, providing a theoretically supported alternative to
spectral methods (such as FNO) and to graph-based Monte Carlo approaches (such
as the Graph Kernel Neural Operator, GNO).

</details>


### [261] [Auditing Algorithmic Bias in Transformer-Based Trading](https://arxiv.org/abs/2510.05140)
*Armin Gerami,Ramani Duraiswami*

Main category: cs.LG

TL;DR: Transformer模型在金融应用中存在风险，易受低频数据偏见影响，且不考虑数据波动性。


<details>
  <summary>Details</summary>
Motivation: 探究Transformer模型在金融应用中的潜在风险和偏见，特别是模型对波动性数据的依赖程度以及价格变动频率对模型预测置信度的影响。

Method: 使用Transformer模型进行预测，并引入基于部分信息分解（PID）的度量标准来量化每个资产对模型决策的影响。

Result: 模型分析显示，Transformer模型完全忽略数据波动性，并且偏向于选择价格变动频率较低的数据进行预测。

Conclusion: Transformer模型在金融应用中存在对低频数据产生偏见的问题，并且忽略了数据本身的波动性。

Abstract: Transformer models have become increasingly popular in financial
applications, yet their potential risk making and biases remain under-explored.
The purpose of this work is to audit the reliance of the model on volatile data
for decision-making, and quantify how the frequency of price movements affects
the model's prediction confidence. We employ a transformer model for
prediction, and introduce a metric based on Partial Information Decomposition
(PID) to measure the influence of each asset on the model's decision making.
Our analysis reveals two key observations: first, the model disregards data
volatility entirely, and second, it is biased toward data with lower-frequency
price movements.

</details>


### [262] [Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment](https://arxiv.org/abs/2510.05157)
*Abrar Shahid,Ibteeker Mahir Ishum,AKM Tahmidul Haque,M Sohel Rahman,A. B. M. Alim Al Islam*

Main category: cs.LG

TL;DR: 本研究通过自定义的OpenAI Gym环境，对网络安全中的对抗性强化学习进行了受控研究，模拟了多端口服务的暴力破解攻击和响应式防御。


<details>
  <summary>Details</summary>
Motivation: 在网络安全领域，使用强化学习（RL）来模拟和研究攻击者与防御者之间的动态对抗是一个重要方向，但现有的研究往往缺乏对真实世界复杂性的模拟。

Method: 研究者构建了一个OpenAI Gym环境，模拟了暴力破解攻击和多端口服务的防御机制，并引入了背景流量噪声、利用机制、IP规避、蜜罐陷阱和速率限制等现实因素。然后，他们使用深度Q网络（DQN）训练了攻击者和防御者两个智能体，在一个零和博弈框架下进行学习，其中成功攻击获得高额奖励，而试探性动作则付出小代价。

Result: 实验结果表明，防御者的可观察性和蜜罐的有效性对成功攻击构成了显著障碍。研究还发现，奖励塑造和精心的训练调度对于在该对抗环境中实现学习稳定性至关重要。在超过50,000次的训练回合中，防御者始终保持战略优势，并且在面对复杂的防御策略（如自适应IP封锁和端口特定控制）时，其性能提升更为明显。

Conclusion: 本研究提出的对抗性强化学习框架和环境为研究自主防御系统、攻击-防御协同演化以及迁移学习到实际网络安全场景提供了基础。零和博弈的设定和真实的运行约束使得该环境特别适合探索网络安全中的智能体对抗。

Abstract: This paper presents a controlled study of adversarial reinforcement learning
in network security through a custom OpenAI Gym environment that models
brute-force attacks and reactive defenses on multi-port services. The
environment captures realistic security trade-offs including background traffic
noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot
traps, and multi-level rate-limiting defenses. Competing attacker and defender
agents are trained using Deep Q-Networks (DQN) within a zero-sum reward
framework, where successful exploits yield large terminal rewards while
incremental actions incur small costs. Through systematic evaluation across
multiple configurations (varying trap detection probabilities, exploitation
difficulty thresholds, and training regimens), the results demonstrate that
defender observability and trap effectiveness create substantial barriers to
successful attacks. The experiments reveal that reward shaping and careful
training scheduling are critical for learning stability in this adversarial
setting. The defender consistently maintains strategic advantage across 50,000+
training episodes, with performance gains amplifying when exposed to complex
defensive strategies including adaptive IP blocking and port-specific controls.
Complete implementation details, reproducible hyperparameter configurations,
and architectural guidelines are provided to support future research in
adversarial RL for cybersecurity. The zero-sum formulation and realistic
operational constraints make this environment suitable for studying autonomous
defense systems, attacker-defender co-evolution, and transfer learning to
real-world network security scenarios.

</details>


### [263] [Generative Inverse Design: From Single Point Optimization to a Diverse Design Portfolio via Conditional Variational Autoencoders](https://arxiv.org/abs/2510.05160)
*Muhammad Arif Hakimi Zamrai*

Main category: cs.LG

TL;DR: 本研究提出了一种基于条件变分自编码器（CVAE）的生成式逆向设计框架，用于探索工程设计空间，并能生成多样化的、高性能的设计方案。


<details>
  <summary>Details</summary>
Motivation: 传统的基于代理优化的方法倾向于收敛到单一最优解，限制了设计空间的探索，并可能忽略有价值的替代设计方案。本研究旨在克服这一局限，实现从单点优化到生成式逆向设计的范式转变。

Method: 提出了一种基于条件变分自编码器（CVAE）的框架，学习设计参数和性能之间的概率映射关系，从而能够根据特定的性能目标生成多样化的、高性能的设计候选集。该方法应用于空气动力学翼型自噪声最小化问题，并与基于代理优化的方法进行了对比。

Result: CVAE框架成功生成了256个新颖的设计，有效性达到94.1%。随后的基于代理的评估显示，77.2%的有效设计性能优于基于代理优化的基线方法找到的单一最优设计。

Conclusion: 生成式逆向设计方法不仅能发现更高质量的解决方案，还能提供多样化的候选方案组合，通过支持多标准决策，从根本上改进了工程设计过程。

Abstract: Inverse design, which seeks to find optimal parameters for a target output,
is a central challenge in engineering. Surrogate-based optimization (SBO) has
become a standard approach, yet it is fundamentally structured to converge to a
single-point solution, thereby limiting design space exploration and ignoring
potentially valuable alternative topologies. This paper presents a paradigm
shift from single-point optimization to generative inverse design. We introduce
a framework based on a Conditional Variational Autoencoder (CVAE) that learns a
probabilistic mapping between a system's design parameters and its performance,
enabling the generation of a diverse portfolio of high-performing candidates
conditioned on a specific performance objective. We apply this methodology to
the complex, non-linear problem of minimizing airfoil self-noise, using a
high-performing SBO method from a prior benchmark study as a rigorous baseline.
The CVAE framework successfully generated 256 novel designs with a 94.1\%
validity rate. A subsequent surrogate-based evaluation revealed that 77.2\% of
these valid designs achieved superior performance compared to the single
optimal design found by the SBO baseline. This work demonstrates that the
generative approach not only discovers higher-quality solutions but also
provides a rich portfolio of diverse candidates, fundamentally enhancing the
engineering design process by enabling multi-criteria decision-making.

</details>


### [264] [Machine learning for fraud detection in digital banking: a systematic literature review REVIEW](https://arxiv.org/abs/2510.05167)
*Md Zahin Hossain George,Md Khorshed Alam,Md Tarek Hasan*

Main category: cs.LG

TL;DR: 机器学习在数字银行欺诈检测中的应用：一项系统性文献综述。


<details>
  <summary>Details</summary>
Motivation: 对数字银行欺诈检测领域中机器学习的现有研究进行系统性梳理和综合分析，以了解主要方法、趋势和挑战。

Method: 遵循 PRISMA 指南，对 118 篇同行评审研究和机构报告进行了识别、筛选、资格审查和纳入，以确保严谨性和透明度。

Result: 研究发现，决策树、逻辑回归和支持向量机等监督学习方法因其可解释性和既有性能而占主导地位。无监督异常检测方法因能处理高度不平衡数据集中的新型欺诈模式而日益普及。深度学习（特别是循环神经网络和卷积神经网络）在处理交易序列数据和检测复杂欺诈方面展现出变革性潜力，但可解释性和实时部署仍是挑战。混合模型结合了监督、无监督和深度学习策略，表现出更强的适应性和检测准确性。

Conclusion: 机器学习，特别是深度学习和混合模型，为数字银行欺诈检测提供了强大且不断发展的解决方案。尽管面临可解释性和部署方面的挑战，但其在提高检测准确性和适应新型欺诈模式方面具有巨大潜力。

Abstract: This systematic literature review examines the role of machine learning in
fraud detection within digital banking, synthesizing evidence from 118
peer-reviewed studies and institutional reports. Following the PRISMA
guidelines, the review applied a structured identification, screening,
eligibility, and inclusion process to ensure methodological rigor and
transparency. The findings reveal that supervised learning methods, such as
decision trees, logistic regression, and support vector machines, remain the
dominant paradigm due to their interpretability and established performance,
while unsupervised anomaly detection approaches are increasingly adopted to
address novel fraud patterns in highly imbalanced datasets. Deep learning
architectures, particularly recurrent and convolutional neural networks, have
emerged as transformative tools capable of modeling sequential transaction data
and detecting complex fraud typologies, though challenges of interpretability
and real-time deployment persist. Hybrid models that combine supervised,
unsupervised, and deep learning strategies demonstrate superior adaptability
and detection accuracy, highlighting their potential as convergent solutions.

</details>


### [265] [Discretized Quadratic Integrate-and-Fire Neuron Model for Deep Spiking Neural Networks](https://arxiv.org/abs/2510.05168)
*Eric Jahns,Davi Moreno,Milan Stojkov,Michel A. Kinsy*

Main category: cs.LG

TL;DR: QIF神经元模型比LIF神经元模型具有更丰富的非线性动力学特性，但由于训练不稳定而受到限制。本文提出了QIF神经元的第一个离散化模型，并推导了其参数集的替代梯度窗口，以确保训练稳定性。该方法在CIFAR-10、CIFAR-100、ImageNet和CIFAR-10 DVS上进行了评估，其性能优于基于LIF的最新方法。


<details>
  <summary>Details</summary>
Motivation: LIF神经元模型的效率以表达能力为代价，其动力学约束于每个时间步的线性衰减。QIF神经元模型虽然具有更丰富的非线性动力学特性，但由于训练不稳定而未被广泛采用。

Method: 提出QIF神经元模型的第一个离散化模型，并推导了其参数集的替代梯度窗口，以确保训练稳定性。在CIFAR-10、CIFAR-100、ImageNet和CIFAR-10 DVS上进行了评估。

Result: 所提出的QIF神经元离散化模型在CIFAR-10、CIFAR-100、ImageNet和CIFAR-10 DVS上的性能优于基于LIF的方法。

Conclusion: QIF神经元模型离散化是深度SNN的有力替代方案，结合了丰富的动力学特性和实际的可扩展性。

Abstract: Spiking Neural Networks (SNNs) have emerged as energy-efficient alternatives
to traditional artificial neural networks, leveraging asynchronous and
biologically inspired neuron dynamics. Among existing neuron models, the Leaky
Integrate-and-Fire (LIF) neuron has become widely adopted in deep SNNs due to
its simplicity and computational efficiency. However, this efficiency comes at
the expense of expressiveness, as LIF dynamics are constrained to linear decay
at each timestep. In contrast, more complex models, such as the Quadratic
Integrate-and-Fire (QIF) neuron, exhibit richer, nonlinear dynamics but have
seen limited adoption due to their training instability. On that note, we
propose the first discretization of the QIF neuron model tailored for
high-performance deep spiking neural networks and provide an in-depth analysis
of its dynamics. To ensure training stability, we derive an analytical
formulation for surrogate gradient windows directly from our discretizations'
parameter set, minimizing gradient mismatch. We evaluate our method on
CIFAR-10, CIFAR-100, ImageNet, and CIFAR-10 DVS, demonstrating its ability to
outperform state-of-the-art LIF-based methods. These results establish our
discretization of the QIF neuron as a compelling alternative to LIF neurons for
deep SNNs, combining richer dynamics with practical scalability.

</details>


### [266] [Higher-Order Feature Attribution: Bridging Statistics, Explainable AI, and Topological Signal Processing](https://arxiv.org/abs/2510.06165)
*Kurt Butler,Guanchao Feng,Petar Djuric*

Main category: cs.LG

TL;DR: a TLDR summary


<details>
  <summary>Details</summary>
Motivation: explain the motivation

Method: explain the method

Result: explain the result

Conclusion: explain the conclusion

Abstract: Feature attributions are post-training analysis methods that assess how
various input features of a machine learning model contribute to an output
prediction. Their interpretation is straightforward when features act
independently, but becomes less direct when the predictive model involves
interactions such as multiplicative relationships or joint feature
contributions. In this work, we propose a general theory of higher-order
feature attribution, which we develop on the foundation of Integrated Gradients
(IG). This work extends existing frameworks in the literature on explainable
AI. When using IG as the method of feature attribution, we discover natural
connections to statistics and topological signal processing. We provide several
theoretical results that establish the theory, and we validate our theory on a
few examples.

</details>


### [267] [Carbon Emission Prediction in China Considering New Quality Productive Forces Using a Deep & Corss Learning Modeling Framework](https://arxiv.org/abs/2510.05171)
*Haijin Xie,Gongquan Zhang*

Main category: cs.LG

TL;DR: 该研究提出了一种多头注意力深度与交叉网络（MADCN）框架，结合了特征交互建模和注意力机制，以预测城市碳排放并研究技术因素的影响。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能（AI）和数字经济推动新的高质量生产力（NQPF），以促进可持续城市发展和减少城市碳排放。

Method: 提出MADCN框架，并结合SHapley Additive exPlanations（SHAP）进行可解释性分析，使用覆盖275个中国城市的面板数据集进行测试。

Result: MADCN模型在预测城市碳排放方面优于传统基线模型，测试集上的MSE为406,151.063，MAE为612.304，R²为0.991。SHAP分析表明人口、城市规模、城市化率和GDP是影响碳排放的最大因素，而NQPF、数字经济指数和AI技术水平的影响相对较小但有意义。

Conclusion: 发展NQPF、加强数字经济和加速AI技术发展能显著减少城市碳排放。政策制定者应优先将技术创新纳入减排战略，特别是推广智能基础设施和提高各行业数字化水平，以实现双碳目标。

Abstract: New quality productive forces (NQPF), digital economy advancement, and
artificial intelligence (AI) technologies are becoming crucial for promoting
sustainable urban development. This study proposes a Multi-head Attention Deep
& Cross Network (MADCN) framework, combining feature interaction modeling and
attention mechanisms, to predict urban carbon emissions and investigate the
impacts of technological factors. The framework incorporates an interpretable
learning phase using SHapley Additive exPlanations (SHAP) to assess the
contributions of different features. A panel dataset covering 275 Chinese
cities is utilized to test the MADCN model. Experimental results demonstrate
that the MADCN model achieves superior predictive performance compared to
traditional machine learning and deep learning baselines, with a Mean Squared
Error (MSE) of 406,151.063, a Mean Absolute Error (MAE) of 612.304, and an
R-squared value of 0.991 on the test set. SHAP analysis highlights that
population, city size, urbanization rate, and GDP are among the most
influential factors on carbon emissions, while NQPF, digital economy index, and
AI technology level also show meaningful but relatively moderate effects.
Advancing NQPF, strengthening the digital economy, and accelerating AI
technology development can significantly contribute to reducing urban carbon
emissions. Policymakers should prioritize integrating technological innovation
into carbon reduction strategies, particularly by promoting intelligent
infrastructure and enhancing digitalization across sectors, to effectively
achieve dual-carbon goals.

</details>


### [268] [Conformalized Gaussian processes for online uncertainty quantification over graphs](https://arxiv.org/abs/2510.06181)
*Jinwen Xu,Qin Lu,Georgios B. Giannakis*

Main category: cs.LG

TL;DR: 图上的不确定性量化（UQ）面临计算复杂性和严格建模假设的挑战。本文提出了一种新颖的图感知参数化GP模型，利用随机特征（RF）近似核函数，并结合在线框架进行在线贝叶斯更新和模型集成，以实现可扩展性和自适应性。


<details>
  <summary>Details</summary>
Motivation: 现有基于图的高斯过程（GP）方法在计算复杂性和建模假设方面存在局限性，特别是在标签动态到达时，可能导致覆盖率不佳。

Method: 1. 提出图感知参数化GP模型，利用随机特征（RF）近似核函数，实现高效的递归贝叶斯模型更新。
2. 利用图感知RF基础GP模型的集成，并对每个GP的权重进行自适应调整，以处理增量式到达的数据。
3. 将GP基础集预测器与在线框架相结合，通过自适应阈值进行后处理，以确保有效的覆盖率并提高对模型误设的鲁棒性。

Result: 所提出的方法通过自适应地集成GP模型和设置CP的关键阈值参数，在覆盖率和预测集效率方面优于现有方法。

Conclusion: 所提出的方法通过结合RF近似、模型集成和在线框架，有效地解决了图上不确定性量化的可扩展性和自适应性问题，并在实验中取得了优于基线方法的性能。

Abstract: Uncertainty quantification (UQ) over graphs arises in a number of
safety-critical applications in network science. The Gaussian process (GP), as
a classical Bayesian framework for UQ, has been developed to handle
graph-structured data by devising topology-aware kernel functions. However,
such GP-based approaches are limited not only by the prohibitive computational
complexity, but also the strict modeling assumptions that might yield poor
coverage, especially with labels arriving on the fly. To effect scalability, we
devise a novel graph-aware parametric GP model by leveraging the random feature
(RF)-based kernel approximation, which is amenable to efficient recursive
Bayesian model updates. To further allow for adaptivity, an ensemble of
graph-aware RF-based scalable GPs have been leveraged, with per-GP weight
adapted to data arriving incrementally. To ensure valid coverage with
robustness to model mis-specification, we wed the GP-based set predictors with
the online conformal prediction framework, which post-processes the prediction
sets using adaptive thresholds. Experimental results the proposed method yields
improved coverage and efficient prediction sets over existing baselines by
adaptively ensembling the GP models and setting the key threshold parameters in
CP.

</details>


### [269] [Learning More with Less: A Generalizable, Self-Supervised Framework for Privacy-Preserving Capacity Estimation with EV Charging Data](https://arxiv.org/abs/2510.05172)
*Anushiya Arunan,Yan Qin,Xiaoli Li,U-Xuan Tan,H. Vincent Poor,Chau Yuen*

Main category: cs.LG

TL;DR: 该研究提出了一种基于自监督预训练的新型电动汽车电池容量估计算法，该算法利用隐私友好的充电数据，在处理数据稀疏、噪声大和分布偏移等挑战方面表现出色，并将测试误差降低了31.9%。


<details>
  <summary>Details</summary>
Motivation: 由于隐私法规和标签数据短缺，准确估计电动汽车电池容量并构建可泛化的模型面临挑战，现有的自监督学习技术难以有效处理隐私友好型数据。本研究旨在开发一种能够有效利用这些受限数据进行容量估计的模型。

Method: 提出了一种名为“片段相似性加权掩码输入重构”的自监督预训练框架，结合了对比学习和加权掩码输入重构。首先利用对比学习捕捉片段间的高层相似性，然后通过相似性加权掩码重构来学习细粒度的充电模式和片段间的关联关系。

Result: 所提出的模型在处理具有挑战性的、受制造商和使用年限影响的领域转移设置下，其测试误差比性能最佳的基准低31.9%，显著优于现有最先进的方法。

Conclusion: 本研究提出的自监督预训练容量估计算法能够从隐私友好、信息量少且不完整的充电数据中学习到丰富、可泛化的表示，有效解决了实际应用中的数据限制和分布偏移问题，并取得了优于现有技术的性能。

Abstract: Accurate battery capacity estimation is key to alleviating consumer concerns
about battery performance and reliability of electric vehicles (EVs). However,
practical data limitations imposed by stringent privacy regulations and labeled
data shortages hamper the development of generalizable capacity estimation
models that remain robust to real-world data distribution shifts. While
self-supervised learning can leverage unlabeled data, existing techniques are
not particularly designed to learn effectively from challenging field data --
let alone from privacy-friendly data, which are often less feature-rich and
noisier. In this work, we propose a first-of-its-kind capacity estimation model
based on self-supervised pre-training, developed on a large-scale dataset of
privacy-friendly charging data snippets from real-world EV operations. Our
pre-training framework, snippet similarity-weighted masked input
reconstruction, is designed to learn rich, generalizable representations even
from less feature-rich and fragmented privacy-friendly data. Our key innovation
lies in harnessing contrastive learning to first capture high-level
similarities among fragmented snippets that otherwise lack meaningful context.
With our snippet-wise contrastive learning and subsequent similarity-weighted
masked reconstruction, we are able to learn rich representations of both
granular charging patterns within individual snippets and high-level
associative relationships across different snippets. Bolstered by this rich
representation learning, our model consistently outperforms state-of-the-art
baselines, achieving 31.9% lower test error than the best-performing benchmark,
even under challenging domain-shifted settings affected by both manufacturer
and age-induced distribution shifts.

</details>


### [270] [Exact Causal Attention with 10% Fewer Operations](https://arxiv.org/abs/2510.05175)
*Dmitry Rybin,Yushun Zhang,Ding Tian,Zhihang Lin,Ruoyu Sun,Zhi-Quan Luo*

Main category: cs.LG

TL;DR: FCA是一种加速因果注意力计算的算法，通过利用特殊矩阵乘法的代数性质，在保持精度的前提下减少了10%的运算量，并在GPU上实现了显著的加速。


<details>
  <summary>Details</summary>
Motivation: 因果注意力的计算成本较高，需要加速其在深度学习模型中的应用。

Method: 提出了一种名为快速因果注意力（FCA）的算法，该算法利用了代数恒等式，能够加速因果注意力计算中一类特殊的矩阵乘法（其中一个操作数或结果矩阵是上三角或下三角矩阵）。

Result: FCA在GPU上实现了比PyTorch默认实现和Triton编译内核更快的计算速度，减少了10%的运算量，同时保证了计算的精确性。

Conclusion: FCA通过利用代数性质和机器学习/组合搜索发现的算法，在不牺牲精度的情况下，有效降低了因果注意力的计算复杂度，并实现了显著的性能提升。

Abstract: We present Fast Causal Attention (FCA), an algorithm that computes exact
Causal Attention using 10\% fewer operations. FCA accelerates a special class
of matrix multiplications where either one operand or the output matrix is
upper- or lower-triangular. This includes all operations in forward and
backward pass of Causal Attention, such as masked product
$\mathrm{Mask}(QK^{T})$. For these matrix multiplications on GPU, FCA reaches
noticeable accelerations over the default PyTorch implementations and Triton
compiled kernels. FCA is built upon algebraic identities discovered via machine
learning and combinatorial search.

</details>


### [271] [PatternKV: Flattening KV Representation Expands Quantization Headroom](https://arxiv.org/abs/2510.05176)
*Ji Zhang,Yiwei Li,Shaoxiong Feng,Peiwen Yuan,Xinglin Wang,Jiayi Shi,Yueqi Zhang,Chuyi Tan,Boyuan Pan,Yao Hu,Kan Li*

Main category: cs.LG

TL;DR: KV缓存量化是减少KV缓存成本的关键，但现有方法在低比特设置下性能脆弱。本文提出PatternKV，一种模式对齐残差量化方案，通过挖掘代表性模式向量并量化残差来展宽KV分布，从而提高低比特KV量化的保真度。


<details>
  <summary>Details</summary>
Motivation: KV缓存是自回归语言模型推理中的主要内存和带宽瓶颈，尤其是在长上下文和测试时扩展场景下。KV量化是降低缓存成本的关键，但现有方法在低比特量化时精度下降明显。

Method: PatternKV是一种模式对齐残差量化方案。它在线挖掘代表性模式向量，将每个KV向量与其最近的模式对齐，并仅量化残差。

Result: PatternKV在长上下文和测试时扩展场景下，在多种骨干网络上实现了2比特的性能提升，4比特量化相比FP16平均精度下降0.08%，测试时扩展精度平均提高10%，吞吐量提高1.4倍，同时支持1.25倍的更大批次。

Conclusion: PatternKV通过展宽KV分布并缩小其范围，提高了低比特KV量化的保真度，在长上下文和测试时扩展场景下取得了显著的性能提升。

Abstract: KV cache in autoregressive LLMs eliminates redundant recomputation but has
emerged as the dominant memory and bandwidth bottleneck during inference,
notably with long contexts and test-time scaling. KV quantization is a key
lever for reducing cache cost, but accuracy drops sharply as the native KV
distribution lacks flatness and thus maintains a wide quantization range. Prior
work focuses on isolating outliers, which caps their error but fails to flatten
the overall distribution, leaving performance fragile under low-bit settings.
In this work, we show that the K cache maintains a stable structure that
evolves gradually with context, while the V cache carries latent semantic
regularities. Building on these insights, we propose PatternKV, a
pattern-aligned residual quantization scheme. It mines representative pattern
vectors online, aligns each KV vector to its nearest pattern, and quantizes
only the residual. This reshaping of the KV distribution flattens the
quantization target and narrows its range, thereby improving the fidelity of
low-bit KV quantization. Across long-context and test-time scaling settings on
multiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08%
average 4-bit drop relative to FP16, improves test-time scaling accuracy by 10%
on average, and raises throughput by 1.4x while supporting 1.25x larger
batches.

</details>


### [272] [Logistic-Gated Operators Enable Auditable Unit-Aware Thresholds in Symbolic Regression](https://arxiv.org/abs/2510.05178)
*Ou Deng,Ruichen Cong,Jianting Xu,Shoji Nishimura,Atsushi Ogihara,Qun Jin*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Symbolic regression promises readable equations but struggles to encode
unit-aware thresholds and conditional logic. We propose logistic-gated
operators (LGO) -- differentiable gates with learnable location and steepness
-- embedded as typed primitives and mapped back to physical units for audit.
Across two primary health datasets (ICU, NHANES), the hard-gate variant
recovers clinically plausible cut-points: 71% (5/7) of assessed thresholds fall
within 10% of guideline anchors and 100% within 20%, while using far fewer
gates than the soft variant (ICU median 4.0 vs 10.0; NHANES 5.0 vs 12.5), and
remaining within the competitive accuracy envelope of strong SR baselines. On
predominantly smooth tasks, gates are pruned, preserving parsimony. The result
is compact symbolic equations with explicit, unit-aware thresholds that can be
audited against clinical anchors -- turning interpretability from a post-hoc
explanation into a modeling constraint and equipping symbolic regression with a
practical calculus for regime switching and governance-ready deployment.

</details>


### [273] [OptiFLIDS: Optimized Federated Learning for Energy-Efficient Intrusion Detection in IoT](https://arxiv.org/abs/2510.05180)
*Saida Elouardi,Mohammed Jouhari,Anas Motii*

Main category: cs.LG

TL;DR: 本研究提出了一种名为OptiFLIDS的新型联邦学习方法，用于优化物联网（IoT）环境中的入侵检测系统（IDS）。该方法通过在本地训练期间应用剪枝技术来降低模型复杂度和能耗，并采用定制的聚合方法来处理非独立同分布（non-IID）数据带来的模型差异，同时保持了高检测性能，能够有效应用于资源受限的IoT设备。


<details>
  <summary>Details</summary>
Motivation: 传统的基于机器学习的IDS需要大量数据，但数据共享受隐私和安全限制。联邦学习（FL）允许在不共享原始数据的情况下进行协作模型训练，但面临数据异质性（non-IID数据）和高能耗/计算成本的挑战，尤其是在资源有限的IoT设备上。因此，需要一种在保证安全性的同时优化FL在IoT环境中部署的方法。

Method: 提出OptiFLIDS方法，在本地训练期间应用剪枝技术以降低模型复杂度和能耗。采用定制的聚合方法来处理因non-IID数据分布而导致的不同剪枝模型。

Result: 在TON_IoT、X-IIoTID和IDSIoT2024三个最新的IoT IDS数据集上进行实验，结果表明OptiFLIDS在提高能源效率的同时，保持了强大的检测性能。

Conclusion: OptiFLIDS是一种有效的方法，适用于在资源受限的IoT环境中部署IDS，它通过剪枝技术和定制聚合方法解决了联邦学习中的数据异质性和能耗问题，实现了高检测性能和高能源效率的平衡。

Abstract: In critical IoT environments, such as smart homes and industrial systems,
effective Intrusion Detection Systems (IDS) are essential for ensuring
security. However, developing robust IDS solutions remains a significant
challenge. Traditional machine learning-based IDS models typically require
large datasets, but data sharing is often limited due to privacy and security
concerns. Federated Learning (FL) presents a promising alternative by enabling
collaborative model training without sharing raw data. Despite its advantages,
FL still faces key challenges, such as data heterogeneity (non-IID data) and
high energy and computation costs, particularly for resource constrained IoT
devices. To address these issues, this paper proposes OptiFLIDS, a novel
approach that applies pruning techniques during local training to reduce model
complexity and energy consumption. It also incorporates a customized
aggregation method to better handle pruned models that differ due to non-IID
data distributions. Experiments conducted on three recent IoT IDS datasets,
TON_IoT, X-IIoTID, and IDSIoT2024, demonstrate that OptiFLIDS maintains strong
detection performance while improving energy efficiency, making it well-suited
for deployment in real-world IoT environments.

</details>


### [274] [Improved High-probability Convergence Guarantees of Decentralized SGD](https://arxiv.org/abs/2510.06141)
*Aleksandar Armacki,Ali H. Sayed*

Main category: cs.LG

TL;DR: 在去中心化设置下，我们研究了具有轻尾噪声的去中心化随机梯度下降（DSGD）的高概率（HP）收敛性。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化高概率收敛研究的假设过于严格，与均方误差（MSE）收敛的假设存在差距。

Method: 通过仔细分析矩生成函数（MGF）以及提出一种新的高概率意义下的方差缩减机制，我们为DSGD提供了更宽松的收敛条件。

Result: DSGD在高概率意义下收敛的条件与MSE收敛的条件相同，并且实现了最优收敛率，同时在用户数量上实现了线性加速。

Conclusion: 我们为去中心化高概率收敛性提供了更强的理论保证，并为去中心化算法在高概率下的性能分析提供了新的见解。

Abstract: Convergence in high-probability (HP) has been receiving increasing interest,
due to its attractive properties, such as exponentially decaying tail bounds
and strong guarantees for each individual run of an algorithm. While HP
guarantees are extensively studied in centralized settings, much less is
understood in the decentralized, networked setup. Existing HP studies in
decentralized settings impose strong assumptions, like uniformly bounded
gradients, or asymptotically vanishing noise, resulting in a significant gap
between assumptions used to establish convergence in the HP and the
mean-squared error (MSE) sense, even for vanilla Decentralized Stochastic
Gradient Descent ($\mathtt{DSGD}$) algorithm. This is contrary to centralized
settings, where it is known that $\mathtt{SGD}$ converges in HP under the same
conditions on the cost function as needed to guarantee MSE convergence.
Motivated by this observation, we revisit HP guarantees for $\mathtt{DSGD}$ in
the presence of light-tailed noise. We show that $\mathtt{DSGD}$ converges in
HP under the same conditions on the cost as in the MSE sense, removing
uniformly bounded gradients and other restrictive assumptions, while
simultaneously achieving order-optimal rates for both non-convex and strongly
convex costs. Moreover, our improved analysis yields linear speed-up in the
number of users, demonstrating that $\mathtt{DSGD}$ maintains strong
performance in the HP sense and matches existing MSE guarantees. Our improved
results stem from a careful analysis of the MGF of quantities of interest
(norm-squared of gradient or optimality gap) and the MGF of the consensus gap
between users' models. To achieve linear speed-up, we provide a novel result on
the variance-reduction effect of decentralized methods in the HP sense and more
fine-grained bounds on the MGF for strongly convex costs, which are both of
independent interest.

</details>


### [275] [A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors](https://arxiv.org/abs/2510.05205)
*Sebastian Wagner-Carena,Aizhan Akhmetzhanova,Sydney Erickson*

Main category: cs.LG

TL;DR: 扩散模型可用于解决源分离问题，无需对源进行明确假设。


<details>
  <summary>Details</summary>
Motivation: 在自然科学中，区分来自观测数据的不同、未知源是一个普遍的挑战，例如在密集星系场中分离星系、区分单个神经元的活动与重叠信号，以及分离地震事件与环境背景。传统方法依赖于简化的源模型，这些模型无法准确再现数据。

Method: 利用多视图的性质，即不同的观测集包含未知源的不同线性变换，并训练扩散模型直接学习源的先验分布，以解决源分离问题。

Result: 所提出的扩散模型方法在没有明确源假设的情况下，能够处理嘈杂、不完整且分辨率不同的观测数据，即使在没有单独观测到任何源的情况下也能成功。该模型可以从源先验中采样，评估候选源的概率，并从给定观测的源分布的联合后验中进行抽样。

Conclusion: 通过在合成问题和真实世界星系观测的广泛应用证明了该方法的有效性。

Abstract: A common challenge in the natural sciences is to disentangle distinct,
unknown sources from observations. Examples of this source separation task
include deblending galaxies in a crowded field, distinguishing the activity of
individual neurons from overlapping signals, and separating seismic events from
an ambient background. Traditional analyses often rely on simplified source
models that fail to accurately reproduce the data. Recent advances have shown
that diffusion models can directly learn complex prior distributions from
noisy, incomplete data. In this work, we show that diffusion models can solve
the source separation problem without explicit assumptions about the source.
Our method relies only on multiple views, or the property that different sets
of observations contain different linear transformations of the unknown
sources. We show that our method succeeds even when no source is individually
observed and the observations are noisy, incomplete, and vary in resolution.
The learned diffusion models enable us to sample from the source priors,
evaluate the probability of candidate sources, and draw from the joint
posterior of the source distribution given an observation. We demonstrate the
effectiveness of our method on a range of synthetic problems as well as
real-world galaxy observations.

</details>


### [276] [Approximate Gaussianity Beyond Initialisation in Neural Networks](https://arxiv.org/abs/2510.05218)
*Edward Hirst,Sanjaye Ramgoolam*

Main category: cs.LG

TL;DR: 研究了神经网络权重矩阵的集合在MNIST分类问题上的训练过程，并通过高斯性和排列对称性的假设来检验矩阵模型表示其分布的有效性。


<details>
  <summary>Details</summary>
Motivation: 研究神经网络权重矩阵集合在MNIST分类问题上的训练过程，并检验矩阵模型表示其分布的有效性。

Method: 使用13参数的置换不变高斯矩阵模型来拟合权重矩阵的分布，并计算了 Wasserstein 距离来量化分布在训练过程中的变化。同时，研究了不同初始化方案、正则化、层数和层宽对该模型的影响。

Result: 13参数的置换不变高斯矩阵模型能有效表示权重矩阵中的相关高斯性，其适用范围远超简单的独立同分布高斯矩阵变量模型，并且在初始化之后仍然有效。研究还识别了特定情况下偏离高斯性的增强以及如何开发更通用但仍具可解释性的模型。

Conclusion: 研究提出了一个可解释的框架，用于最佳拟合模型和偏离高斯性的情况，并使用 Wasserstein 距离量化了训练过程中分布的变化。同时，探索了不同训练策略对模型的影响，并为开发更通用的模型提供了方向。

Abstract: Ensembles of neural network weight matrices are studied through the training
process for the MNIST classification problem, testing the efficacy of matrix
models for representing their distributions, under assumptions of Gaussianity
and permutation-symmetry. The general 13-parameter permutation invariant
Gaussian matrix models are found to be effective models for the correlated
Gaussianity in the weight matrices, beyond the range of applicability of the
simple Gaussian with independent identically distributed matrix variables, and
notably well beyond the initialisation step. The representation theoretic model
parameters, and the graph-theoretic characterisation of the permutation
invariant matrix observables give an interpretable framework for the best-fit
model and for small departures from Gaussianity. Additionally, the Wasserstein
distance is calculated for this class of models and used to quantify the
movement of the distributions over training. Throughout the work, the effects
of varied initialisation regimes, regularisation, layer depth, and layer width
are tested for this formalism, identifying limits where particular departures
from Gaussianity are enhanced and how more general, yet still
highly-interpretable, models can be developed.

</details>


### [277] [CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers](https://arxiv.org/abs/2510.05228)
*Haining Pan,James V. Roggeveen,Erez Berg,Juan Carrasquilla,Debanjan Chowdhury,Surya Ganguli,Federico Ghimenti,Juraj Hasik,Henry Hunt,Hong-Chen Jiang,Mason Kamb,Ying-Jer Kao,Ehsan Khatami,Michael J. Lawler,Di Luo,Titus Neupert,Xiaoliang Qi,Michael P. Brenner,Eun-Ah Kim*

Main category: cs.LG

TL;DR: LLMs 在硬科学研究级问题解决方面表现不佳，CMT-Benchmark 包含 50 个凝聚态理论问题，旨在评估和改进 LLMs 的物理推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有 LLM 在高级硬科学研究问题上的评估不足。

Method: 创建 CMT-Benchmark 数据集，包含 50 个凝聚态理论问题，由专家设计和验证，涵盖多种分析和计算方法。通过程序化检查解决方案与专家提供的真值来评估 LLM，包括处理非对易算符的符号操作。

Result: 现有 LLM 在 CMT-Benchmark 数据集上表现不佳，平均解决率仅为 11.4%，最佳模型 GPT5 解决率也仅为 30%。许多问题（18 个）没有模型能够解决，26 个问题仅有一个模型能解决。部分答案违反物理对称性或出现不合理尺度。LLMs 在量子蒙特卡洛、变分蒙特卡洛和 DMRG 方法上尤其困难。

Conclusion: CMT-Benchmark 凸显了当前 LLMs 在物理推理方面的不足，但可以通过识别 LLMs 的常见故障模式来指导开发更强大的 AI 研究助手和导师。

Abstract: Large language models (LLMs) have shown remarkable progress in coding and
math problem-solving, but evaluation on advanced research-level problems in
hard sciences remains scarce. To fill this gap, we present CMT-Benchmark, a
dataset of 50 problems covering condensed matter theory (CMT) at the level of
an expert researcher. Topics span analytical and computational approaches in
quantum many-body, and classical statistical mechanics. The dataset was
designed and verified by a panel of expert researchers from around the world.
We built the dataset through a collaborative environment that challenges the
panel to write and refine problems they would want a research assistant to
solve, including Hartree-Fock, exact diagonalization, quantum/variational Monte
Carlo, density matrix renormalization group (DMRG), quantum/classical
statistical mechanics, and model building. We evaluate LLMs by programmatically
checking solutions against expert-supplied ground truth. We developed
machine-grading, including symbolic handling of non-commuting operators via
normal ordering. They generalize across tasks too. Our evaluations show that
frontier models struggle with all of the problems in the dataset, highlighting
a gap in the physical reasoning skills of current LLMs. Notably, experts
identified strategies for creating increasingly difficult problems by
interacting with the LLMs and exploiting common failure modes. The best model,
GPT5, solves 30\% of the problems; average across 17 models (GPT, Gemini,
Claude, DeepSeek, Llama) is 11.4$\pm$2.1\%. Moreover, 18 problems are solved by
none of the 17 models, and 26 by at most one. These unsolved problems span
Quantum Monte Carlo, Variational Monte Carlo, and DMRG. Answers sometimes
violate fundamental symmetries or have unphysical scaling dimensions. We
believe this benchmark will guide development toward capable AI research
assistants and tutors.

</details>


### [278] [Simultaneous Learning and Optimization via Misspecified Saddle Point Problems](https://arxiv.org/abs/2510.05241)
*Mohammad Mahdi Ahmadi,Erfan Yazdandoost Hamedani*

Main category: cs.LG

TL;DR: 本文提出了一种统一的优化和学习框架，用于解决包含未知参数的鞍点问题。


<details>
  <summary>Details</summary>
Motivation: 现有研究通常假设参数是已知的或预先估计的，而本文则研究了参数需要从数据中同时学习的失配鞍点问题。

Method: 提出两种基于加速原对偶（APD）方法的算法：一种是直接替换参数估计的朴素扩展，另一种是显式考虑参数动态的、调整动量更新的学习感知变体。

Result: 两种方法都实现了$\\\mathcal{O}(\\\\log K / K)$的可证明收敛率，其中学习感知方法具有更紧凑的$\\\mathcal{O}(1)$常数，并通过回溯策略实现了自适应步长选择。该框架还扩展到学习问题有多个最优解的情况，在结构化设置下实现了$\\\mathcal{O}(1/\\\ackslash	ext{sqrt}(K))$的收敛率。

Conclusion: 所提出的方法在失配投资组合优化问题上显示出优越的经验性能，证明了其在实际应用中的有效性。

Abstract: We study a class of misspecified saddle point (SP) problems, where the
optimization objective depends on an unknown parameter that must be learned
concurrently from data. Unlike existing studies that assume parameters are
fully known or pre-estimated, our framework integrates optimization and
learning into a unified formulation, enabling a more flexible problem class. To
address this setting, we propose two algorithms based on the accelerated
primal-dual (APD) by Hamedani & Aybat 2021. In particular, we first analyze the
naive extension of the APD method by directly substituting the evolving
parameter estimates into the primal-dual updates; then, we design a new
learning-aware variant of the APD method that explicitly accounts for parameter
dynamics by adjusting the momentum updates. Both methods achieve a provable
convergence rate of $\mathcal{O}(\log K / K)$, while the learning-aware
approach attains a tighter $\mathcal{O}(1)$ constant and further benefits from
an adaptive step-size selection enabled by a backtracking strategy.
Furthermore, we extend the framework to problems where the learning problem
admits multiple optimal solutions, showing that our modified algorithm for a
structured setting achieves an $\mathcal{O}(1/\sqrt{K})$ rate. To demonstrate
practical impact, we evaluate our methods on a misspecified portfolio
optimization problem and show superior empirical performance compared to
state-of-the-art algorithms.

</details>


### [279] [ECLipsE-Gen-Local: Efficient Compositional Local Lipschitz Estimates for Deep Neural Networks](https://arxiv.org/abs/2510.05261)
*Yuezhu Xu,S. Sivaranjani*

Main category: cs.LG

TL;DR: 计算神经网络的 Lipschitz 常数是一个NP难问题，现有方法难以扩展。本文提出了一种组合框架，能够为深度前馈神经网络提供精确且可扩展的 Lipschitz 估计。


<details>
  <summary>Details</summary>
Motivation: 现有计算 Lipschitz 常数的方法难以扩展，且未能有效利用局部输入信息。因此，需要更精确、更可扩展的 Lipschitz 估计方法。

Method: 提出了一种广义 SDP 框架，允许灵活处理异构激活函数斜率，并支持任意输入-输出对和子网络。通过将广义 SDP 分解为一系列计算复杂度随网络深度线性增长的子问题，实现可扩展性。还开发了一种通过子问题闭式解实现近乎瞬时计算的变体。开发了 ECLipsE-Gen-Local 算法系列，结合了局部输入信息。

Result: 实验表明，本文算法在多个基准测试中实现了显著的加速，并提供了比全局方法更紧密的 Lipschitz 界限。对于足够小的输入区域，所提出的算法提供的 Lipschitz 常数的严格上限接近自动微分的雅可比行列式。

Conclusion: 本文提出的组合框架能够提供精确且可扩展的 Lipschitz 估计，并能有效结合局部输入信息，在实际应用中与网络鲁棒性表现出良好的一致性。

Abstract: The Lipschitz constant is a key measure for certifying the robustness of
neural networks to input perturbations. However, computing the exact constant
is NP-hard, and standard approaches to estimate the Lipschitz constant involve
solving a large matrix semidefinite program (SDP) that scales poorly with
network size. Further, there is a potential to efficiently leverage local
information on the input region to provide tighter Lipschitz estimates. We
address this problem here by proposing a compositional framework that yields
tight yet scalable Lipschitz estimates for deep feedforward neural networks.
Specifically, we begin by developing a generalized SDP framework that is highly
flexible, accommodating heterogeneous activation function slope, and allowing
Lipschitz estimates with respect to arbitrary input-output pairs and arbitrary
choices of sub-networks of consecutive layers. We then decompose this
generalized SDP into a sequence of small sub-problems, with computational
complexity that scales linearly with respect to the network depth. We also
develop a variant that achieves near-instantaneous computation through
closed-form solutions to each sub-problem. All our algorithms are accompanied
by theoretical guarantees on feasibility and validity. Next, we develop a
series of algorithms, termed as ECLipsE-Gen-Local, that effectively incorporate
local information on the input. Our experiments demonstrate that our algorithms
achieve substantial speedups over a multitude of benchmarks while producing
significantly tighter Lipschitz bounds than global approaches. Moreover, we
show that our algorithms provide strict upper bounds for the Lipschitz constant
with values approaching the exact Jacobian from autodiff when the input region
is small enough. Finally, we demonstrate the practical utility of our approach
by showing that our Lipschitz estimates closely align with network robustness.

</details>


### [280] [When Does Global Attention Help? A Unified Empirical Study on Atomistic Graph Learning](https://arxiv.org/abs/2510.05583)
*Arindam Chowdhury,Massimiliano Lupo Pasini*

Main category: cs.LG

TL;DR: GNNs在研究原子尺度化合物行为方面有广泛应用，但全局注意力机制的优势尚不清楚。本文提出了一个统一的基准测试框架，用于比较不同GNN模型（MPNN、带编码器的MPNN、MPNN与全局注意力的混合模型、以及局部-全局融合模型）。结果表明，带编码器的MPNN是一个稳健的基线，而融合局部-全局模型在受长程相互作用影响的性质上具有最显著的优势。此外，还量化了注意力机制的精度-计算权衡及其内存开销。


<details>
  <summary>Details</summary>
Motivation: 在研究原子尺度化合物行为的GNN中，全局注意力机制的优势尚不明确，因为实现、特征或超参数调整不一致。因此，需要一个统一的、可复现的基准测试框架来评估其影响。

Method: 构建了一个基于HydraGNN的统一、可复现的基准测试框架，能够方便地在四类受控模型之间切换：MPNN、带化学/拓扑编码器的MPNN、MPNN与全局注意力的GPS风格混合模型、以及完全融合的局部-全局模型。使用七个多样化的开源数据集，跨回归和分类任务，系统地分离了消息传递、全局注意力和基于编码器的特征增强的贡献。

Result: 研究表明，带编码器的MPNN模型是一个稳健的基线。完全融合的局部-全局模型在受长程相互作用影响的性质上提供了最清晰的优势。此外，还量化了注意力机制的精度-计算权衡，并报告了其内存开销。

Conclusion: 本文建立了第一个对原子尺度图学习中的全局注意力进行受控评估，并提供了一个可复现的测试平台，以供未来的模型开发使用。研究结果为理解和选择合适的GNN模型提供了指导。

Abstract: Graph neural networks (GNNs) are widely used as surrogates for costly
experiments and first-principles simulations to study the behavior of compounds
at atomistic scale, and their architectural complexity is constantly increasing
to enable the modeling of complex physics. While most recent GNNs combine more
traditional message passing neural networks (MPNNs) layers to model short-range
interactions with more advanced graph transformers (GTs) with global attention
mechanisms to model long-range interactions, it is still unclear when global
attention mechanisms provide real benefits over well-tuned MPNN layers due to
inconsistent implementations, features, or hyperparameter tuning. We introduce
the first unified, reproducible benchmarking framework - built on HydraGNN -
that enables seamless switching among four controlled model classes: MPNN, MPNN
with chemistry/topology encoders, GPS-style hybrids of MPNN with global
attention, and fully fused local - global models with encoders. Using seven
diverse open-source datasets for benchmarking across regression and
classification tasks, we systematically isolate the contributions of message
passing, global attention, and encoder-based feature augmentation. Our study
shows that encoder-augmented MPNNs form a robust baseline, while fused
local-global models yield the clearest benefits for properties governed by
long-range interaction effects. We further quantify the accuracy - compute
trade-offs of attention, reporting its overhead in memory. Together, these
results establish the first controlled evaluation of global attention in
atomistic graph learning and provide a reproducible testbed for future model
development.

</details>


### [281] [Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs](https://arxiv.org/abs/2510.05278)
*Paloma García-de-Herreros,Philipp Slusallek,Dietrich Klakow,Vagrant Gautam*

Main category: cs.LG

TL;DR: decoder-only模型在科学机器学习任务的跨模态适应方面不如encoder-only模型，但通过引入Parallel Flipping和Sequence Doubling两种新方法可以缩小性能差距。


<details>
  <summary>Details</summary>
Motivation: 探究模型架构（encoder-only vs decoder-only）如何影响跨模态适应的性能，特别是在科学机器学习任务中。

Method: 通过消融实验系统地比较了encoder-only和decoder-only模型在基于偏微分方程的时间依赖性模拟任务上的跨模态适应性能。引入了Parallel Flipping和Sequence Doubling两种新方法来改进decoder-only模型的性能。

Result: 在直接应用现有方法时，decoder-only模型的表现远不如encoder-only模型，并且模型规模的扩大也无法改善其性能。提出的Parallel Flipping和Sequence Doubling两种新方法显著提高了decoder-only模型的性能，缩小了与encoder-only模型的差距。

Conclusion: decoder-only模型在科学机器学习的跨模态适应任务中具有潜力，但需要新的方法（如Parallel Flipping和Sequence Doubling）来克服其固有的局限性，才能达到与encoder-only模型相当的性能。

Abstract: Large language models have shown great success on natural language tasks in
recent years, but they have also shown great promise when adapted to new
modalities, e.g., for scientific machine learning tasks. Even though
decoder-only models are more popular within NLP and scale exceedingly well at
generating natural language, most proposed approaches for cross-modal
adaptation focus on encoder-only models, raising the question of how model
architecture affects these approaches. In this paper, we therefore perform a
series of ablation studies to answer this question, systematically comparing
encoder-only and decoder-only models on cross-modal adaptation for
time-dependent simulation tasks based on partial differential equations (PDEs).
We find that decoder-only models are far worse than encoder-only models, when
existing approaches are applied unmodified. In contrast to several other
domains, scaling decoder-only models also does not help. To harness the
potential of decoder-only models in this context, we introduce two novel
approaches, Parallel Flipping and Sequence Doubling, attempting to mimic
bidirectionality in autoregressive models. Both our methods improve overall
performance using decoder-only models for all tasks and all cross-model
adaptation methods, closing the gap to encoder-only model performance. We hope
that our findings broaden the spectrum of models used on cross-modal adaptation
tasks to further scientific ML.

</details>


### [282] [Learning Mixtures of Linear Dynamical Systems (MoLDS) via Hybrid Tensor-EM Method](https://arxiv.org/abs/2510.06091)
*Lulu Gong,Shreya Saxena*

Main category: cs.LG

TL;DR: 提出了一种结合张量方法和EM算法的MoLDS学习新方法，提高了模型在复杂和噪声环境下的学习可靠性和鲁棒性，并成功应用于神经数据分析。


<details>
  <summary>Details</summary>
Motivation: 现有的MoLDS方法在复杂和噪声环境下应用受限，EM方法对初始化敏感且易陷入局部最优。

Method: 提出一种基于张量的方法来学习MoLDS，提供全局可辨识性保证，然后通过卡尔曼EM算法进行更新，对所有LDS参数进行更新。

Result: 在合成数据上，Tensor-EM方法比纯张量或随机初始化的EM方法更可靠、更鲁棒。在神经数据分析中，成功地将不同条件建模为不同的子系统，并应用于猴子完成不同方向和顺序的抓取任务的神经记录。

Conclusion: MoLDS为神经数据建模提供了一个有效的框架，而Tensor-EM是用于这些应用的可靠MoLDS学习方法。

Abstract: Mixtures of linear dynamical systems (MoLDS) provide a path to model
time-series data that exhibit diverse temporal dynamics across trajectories.
However, its application remains challenging in complex and noisy settings,
limiting its effectiveness for neural data analysis. Tensor-based moment
methods can provide global identifiability guarantees for MoLDS, but their
performance degrades under noise and complexity. Commonly used
expectation-maximization (EM) methods offer flexibility in fitting latent
models but are highly sensitive to initialization and prone to poor local
minima. Here, we propose a tensor-based method that provides identifiability
guarantees for learning MoLDS, which is followed by EM updates to combine the
strengths of both approaches. The novelty in our approach lies in the
construction of moment tensors using the input-output data to recover globally
consistent estimates of mixture weights and system parameters. These estimates
can then be refined through a Kalman EM algorithm, with closed-form updates for
all LDS parameters. We validate our framework on synthetic benchmarks and
real-world datasets. On synthetic data, the proposed Tensor-EM method achieves
more reliable recovery and improved robustness compared to either pure tensor
or randomly initialized EM methods. We then analyze neural recordings from the
primate somatosensory cortex while a non-human primate performs reaches in
different directions. Our method successfully models and clusters different
conditions as separate subsystems, consistent with supervised single-LDS fits
for each condition. Finally, we apply this approach to another neural dataset
where monkeys perform a sequential reaching task. These results demonstrate
that MoLDS provides an effective framework for modeling complex neural data,
and that Tensor-EM is a reliable approach to MoLDS learning for these
applications.

</details>


### [283] [Adjusting the Output of Decision Transformer with Action Gradient](https://arxiv.org/abs/2510.05285)
*Rui Lin,Yiwen Zhang,Zhicheng Peng,Minghao Lyu*

Main category: cs.LG

TL;DR: DT是一种结合强化学习和Transformer的新型离线RL方法，它通过最大化动作的似然性来学习策略，解决了轨迹拼接和动作外插问题，并提出了一种名为Action Gradient (AG) 的新方法来优化动作，取得了先进水平的性能。


<details>
  <summary>Details</summary>
Motivation: 原始的DT方法在处理离线强化学习时，虽然整合了RL和Transformer，但面临轨迹拼接和动作外插两大挑战。现有的解决方案（如替换特定token和结合策略梯度PG）在稳定性和性能提升方面存在不足。

Method: 本文提出了一种名为Action Gradient (AG) 的新方法。AG直接调整动作以实现类似策略梯度（PG）的功能，并能有效地与token预测技术结合。具体来说，AG利用Q值对动作的梯度来优化动作。

Result: AG方法能够显著提升基于DT的算法性能，部分结果达到了当前最优（state-of-the-art）水平。

Conclusion: AG方法是一种创新的解决方案，它通过直接优化Q值相对于动作的梯度，有效解决了DT在离线强化学习中遇到的挑战，并取得了优于现有方法的性能。

Abstract: Decision Transformer (DT), which integrates reinforcement learning (RL) with
the transformer model, introduces a novel approach to offline RL. Unlike
classical algorithms that take maximizing cumulative discounted rewards as
objective, DT instead maximizes the likelihood of actions. This paradigm shift,
however, presents two key challenges: stitching trajectories and extrapolation
of action. Existing methods, such as substituting specific tokens with
predictive values and integrating the Policy Gradient (PG) method, address
these challenges individually but fail to improve performance stably when
combined due to inherent instability. To address this, we propose Action
Gradient (AG), an innovative methodology that directly adjusts actions to
fulfill a function analogous to that of PG, while also facilitating efficient
integration with token prediction techniques. AG utilizes the gradient of the
Q-value with respect to the action to optimize the action. The empirical
results demonstrate that our method can significantly enhance the performance
of DT-based algorithms, with some results achieving state-of-the-art levels.

</details>


### [284] [Computing frustration and near-monotonicity in deep neural networks](https://arxiv.org/abs/2510.05286)
*Joel Wendin,Erik G. Larsson,Claudio Altafini*

Main category: cs.LG

TL;DR: 深度卷积神经网络比预期具有更低的“挫败感”，表明其结构平衡性更好，行为更接近单调函数，并暗示了一种新的隐式正则化形式。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探讨深度神经网络（DNN）的结构平衡性，特别是计算与DNN相关联的有符号图的“挫败感”水平，并将其与统计物理学中的Ising自旋玻璃模型以及函数的单调性行为联系起来。

Method: 计算预训练深度卷积神经网络（CNN）的有符号图的“挫败感”水平，并与零模型进行比较。从统计物理学角度，参考Ising自旋玻璃模型进行分析。从函数角度，分析其行为的单调性。

Result: 所有考虑的预训练CNN的挫败感均低于零模型的预期。挫败感的降低意味着网络中编码的无序度低于零模型。低挫败感（接近结构平衡）表明网络函数接近单调行为，与零模型相比更具单调性。

Conclusion: 深度卷积神经网络倾向于表现出比零模型更强的有序行为，这表明存在一种新的隐式正则化形式。

Abstract: For the signed graph associated to a deep neural network, one can compute the
frustration level, i.e., test how close or distant the graph is to structural
balance. For all the pretrained deep convolutional neural networks we consider,
we find that the frustration is always less than expected from null models.
From a statistical physics point of view, and in particular in reference to an
Ising spin glass model, the reduced frustration indicates that the amount of
disorder encoded in the network is less than in the null models. From a
functional point of view, low frustration (i.e., proximity to structural
balance) means that the function representing the network behaves
near-monotonically, i.e., more similarly to a monotone function than in the
null models. Evidence of near-monotonic behavior along the partial order
determined by frustration is observed for all networks we consider. This
confirms that the class of deep convolutional neural networks tends to have a
more ordered behavior than expected from null models, and suggests a novel form
of implicit regularization.

</details>


### [285] [DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping](https://arxiv.org/abs/2510.05288)
*Ruoxing Yang*

Main category: cs.LG

TL;DR: LLM在本地部署时存在安全风险，研究者通过改进差分隐私优化算法来增强本地化LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: LLM存在易受网络攻击和训练数据泄露的安全隐患，现有技术难以在本地设备上安全地运行和微调LLM。

Method: 提出并实现了一种名为DP-Adam-AC的优化算法，该算法结合了自适应梯度裁剪和其他工程改进，用于微调本地化语言模型。实验中使用了Qwen2.5-0.5B和Bitnet-b1.58-2B两种本地化LLM设计，并在两个合成数据集上进行了测试。

Result: DP-Adam-AC优化器在实验中展示了在损失函数方面的改进效果，证明了其在提高本地化LLM安全性方面的潜力。

Conclusion: 通过改进差分隐私优化算法（DP-Adam-AC），可以有效地增强本地化LLM在安全性和隐私保护方面的能力，为在消费级设备上安全运行和微调LLM提供了可行方案。

Abstract: Large language models (LLMs) such as ChatGPT have evolved into powerful and
ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire
specialized skills for specific tasks efficiently. Although LLMs provide great
utility in both general and task-specific use cases, they are limited by two
security-related concerns. First, traditional LLM hardware requirements make
them infeasible to run locally on consumer-grade devices. A remote network
connection with the LLM provider's server is usually required, making the
system vulnerable to network attacks. Second, fine-tuning an LLM for a
sensitive task may involve sensitive data. Non-private fine-tuning algorithms
produce models vulnerable to training data reproduction attacks. Our work
addresses these security concerns by enhancing differentially private
optimization algorithms and applying them to fine-tune localizable language
models. We introduce adaptable gradient clipping along with other engineering
enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our
optimizer to fine-tune examples of two localizable LLM designs, small language
model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We
demonstrate promising improvements in loss through experimentation with two
synthetic datasets.

</details>


### [286] [Gamma Mixture Modeling for Cosine Similarity in Small Language Models](https://arxiv.org/abs/2510.05309)
*Kevin Player*

Main category: cs.LG

TL;DR: sentence embedding余弦相似度可以由gamma混合模型很好地建模，该模型可以用于相似度评分。


<details>
  <summary>Details</summary>
Motivation: 研究sentence transformer embedding的余弦相似度，并观察到它们可以被gamma混合模型很好地模拟。

Method: 使用gamma混合模型来建模句子嵌入的余弦相似度，并提出一种启发式模型，其中主题的层次聚类自然地导致相似度得分的gamma混合结构。最后，概述了一种用于拟合移位gamma混合的期望最大化算法。

Result: 经验上发现，这些分布通常可以被一个移位并截断为[-1,1]的gamma分布很好地捕捉，在许多情况下，可以被gamma混合物捕捉。

Conclusion: 提出的启发式模型和期望最大化算法为建模相似度分布提供了一种实用的工具。

Abstract: We study the cosine similarity of sentence transformer embeddings and observe
that they are well modeled by gamma mixtures. From a fixed corpus, we measure
similarities between all document embeddings and a reference query embedding.
Empirically we find that these distributions are often well captured by a gamma
distribution shifted and truncated to [-1,1], and in many cases, by a gamma
mixture. We propose a heuristic model in which a hierarchical clustering of
topics naturally leads to a gamma-mixture structure in the similarity scores.
Finally, we outline an expectation-maximization algorithm for fitting shifted
gamma mixtures, which provides a practical tool for modeling similarity
distributions.

</details>


### [287] [RegMix: Adversarial Mutual and Generalization Regularization for Enhancing DNN Robustness](https://arxiv.org/abs/2510.05317)
*Zhenyu Liu,Varun Ojha*

Main category: cs.LG

TL;DR: 通过引入加权的对抗性互学习和对抗性泛化正则化，改进了对抗性训练的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 标准的对抗性训练方法（如使用交叉熵损失和均方误差正则化）存在一些局限性，特别是均方误差在训练过程中对两个输出分布施加了过于统一的优化，这限制了其在对抗性训练场景下的鲁棒性。

Method: 提出两种新颖的正则化策略：(i) 加权的对抗性互正则化，设计了一个分解的对抗性互KL散度损失，可以灵活地通过分配不同的权重给主要和辅助目标来控制优化过程；(ii) 对抗性泛化正则化，在对抗性训练目标中引入额外的清洁目标分布，以提高泛化能力和模型鲁棒性。

Result: 实验结果表明，所提出的方法在对抗鲁棒性方面显著优于现有的基于正则化的方法。

Conclusion: 所提出的加权的对抗性互正则化和对抗性泛化正则化策略能够有效提升模型的对抗鲁棒性。

Abstract: Adversarial training is the most effective defense against adversarial
attacks. The effectiveness of the adversarial attacks has been on the design of
its loss function and regularization term. The most widely used loss function
in adversarial training is cross-entropy and mean squared error (MSE) as its
regularization objective. However, MSE enforces overly uniform optimization
between two output distributions during training, which limits its robustness
in adversarial training scenarios. To address this issue, we revisit the idea
of mutual learning (originally designed for knowledge distillation) and propose
two novel regularization strategies tailored for adversarial training: (i)
weighted adversarial mutual regularization and (ii) adversarial generalization
regularization. In the former, we formulate a decomposed adversarial mutual
Kullback-Leibler divergence (KL-divergence) loss, which allows flexible control
over the optimization process by assigning unequal weights to the main and
auxiliary objectives. In the latter, we introduce an additional clean target
distribution into the adversarial training objective, improving generalization
and enhancing model robustness. Extensive experiments demonstrate that our
proposed methods significantly improve adversarial robustness compared to
existing regularization-based approaches.

</details>


### [288] [Tensor-on-tensor Regression Neural Networks for Process Modeling with High-dimensional Data](https://arxiv.org/abs/2510.05329)
*Qian Wang,Mohammad N. Bisheh,Kamran Paynabar*

Main category: cs.LG

TL;DR: A novel Tensor-on-Tensor Regression Neural Network (TRNN) is proposed to handle large, multi-way tensor data from modern sensing systems, combining the geometric preservation of tensor methods with the nonlinearity of neural networks.


<details>
  <summary>Details</summary>
Motivation: Existing methods for analyzing large, multi-way tensor data from sensing systems either fail to capture nonlinear interactions (tensor-based regressors) or discard spatial structure and become computationally expensive (conventional neural networks after flattening).

Method: The paper introduces a Tensor-on-Tensor Regression Neural Network (TRNN) that integrates tensor geometry preservation with neural network nonlinearity.

Result: The TRNN unifies existing tensor-based and neural network paradigms for regression on multi-way tensor data.

Conclusion: The proposed TRNN offers a unified approach for regression on heterogeneous, high-dimensional tensor data, addressing limitations of previous methods.

Abstract: Modern sensing and metrology systems now stream terabytes of heterogeneous,
high-dimensional (HD) data profiles, images, and dense point clouds, whose
natural representation is multi-way tensors. Understanding such data requires
regression models that preserve tensor geometry, yet remain expressive enough
to capture the pronounced nonlinear interactions that dominate many industrial
and mechanical processes. Existing tensor-based regressors meet the first
requirement but remain essentially linear. Conversely, conventional neural
networks offer nonlinearity only after flattening, thereby discarding spatial
structure and incurring prohibitive parameter counts. This paper introduces a
Tensor-on-Tensor Regression Neural Network (TRNN) that unifies these two
paradigms.

</details>


### [289] [Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization](https://arxiv.org/abs/2510.05342)
*Hyung Gyu Rho*

Main category: cs.LG

TL;DR: MADPO是一种改进的直接偏好优化方法，通过实例级自适应温度解决了DPO在处理多样化偏好数据时的问题，提高了模型在不同质量数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的DPO及其变体（如IPO和β-DPO）在处理多样化的偏好数据时存在局限性，例如在简单样本上过拟合、对信息量大的样本学习不足、以及批次级自适应的不足等。

Method: MADPO采用两步法：首先训练一个奖励模型来估计偏好边距，然后利用这些边距为每个训练样本的DPO损失应用连续的、自适应的权重。这种重加权方案会放大难例的有效目标边距，并抑制易例的边距，从而实现对学习信号的精细控制。

Result: MADPO在情感生成任务上表现出色，在高质量数据集上比次优方法提高了+33.3%，在低质量数据集上提高了+10.5%。理论分析表明，MADPO具有良好的优化前景，并且对奖励模型的估计误差具有鲁棒性。

Conclusion: MADPO提供了一种更稳定、数据保留且实例级的方法来进行偏好对齐，克服了现有方法的局限性，并在实验中取得了显著的性能提升。

Abstract: Direct Preference Optimization (DPO) has emerged as a simple and effective
method for aligning large language models. However, its reliance on a fixed
temperature parameter leads to suboptimal training on diverse preference data,
causing overfitting on easy examples and under-learning from informative ones.
Recent methods have emerged to counter this. While IPO addresses general
overfitting, its uniform regularization can be overly conservative. The more
targeted approach of $\beta$-DPO suffers from its own limitations: its
batch-level adaptation applies a single, compromised temperature to
mixed-margin pairs, its linear update rule can produce unstable negative
$\beta$ values, and its filtering mechanism discards potentially useful
training signals. In this work, we introduce Margin-Adaptive Direct Preference
Optimization (MADPO), a method that provides a stable, data-preserving, and
instance-level solution. MADPO employs a practical two-step approach: it first
trains a reward model to estimate preference margins and then uses these
margins to apply a continuous, adaptive weight to the DPO loss for each
individual training sample. This re-weighting scheme creates an effective
target margin that is amplified for hard pairs and dampened for easy pairs,
allowing for granular control over the learning signal. We provide a
comprehensive theoretical analysis, proving that MADPO has a well-behaved
optimization landscape and is robust to reward model estimation errors. We
validate our theory with experiments on a sentiment generation task, where
MADPO consistently and significantly outperforms strong baselines across
datasets of varying quality. It achieves performance gains of up to +33.3\% on
High Quality data and +10.5\% on Low Quality data over the next-best method.
Our results establish MADPO as a more robust and principled approach to
preference alignment.

</details>


### [290] [Physics-informed Attention-enhanced Fourier Neural Operator for Solar Magnetic Field Extrapolations](https://arxiv.org/abs/2510.05351)
*Jinghao Cao,Qin Li,Mengnan Du,Haimin Wang,Bo Shen*

Main category: cs.LG

TL;DR: PIANO是一种结合了注意力机制和物理信息损失的傅里叶神经算子，用于解决太阳物理中的非线性无力场问题，可以直接从2D边界条件学习3D磁场结构，并在准确性和物理一致性方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统的非线性无力场（NLFFF）问题求解方法依赖于迭代数值方法，效率和准确性有待提高。本研究旨在提出一种新的方法来直接学习3D磁场结构，以克服这些限制。

Method: 提出了一种名为PIANO（Physics-informed Attention-enhanced Fourier Neural Operator）的方法。该方法整合了高效通道注意力（ECA）机制和扩张卷积（DC），以增强模型捕获多模态输入的能力，并优先考虑与磁场变化相关的关键通道。此外，在训练过程中强制执行无力场和无散度条件，以确保预测结果在物理上具有高精度的一致性。

Result: 在ISEE NLFFF数据集上的实验结果表明，PIANO在准确性方面优于最先进的神经算子，并且在从各种太阳活动区域重建的磁场中，与NLFFF数据的物理特征表现出很强的一致性。

Conclusion: PIANO能够准确地从2D边界条件学习3D磁场结构，并且在求解NLFFF问题方面表现出比现有方法更好的性能和物理一致性。

Abstract: We propose Physics-informed Attention-enhanced Fourier Neural Operator
(PIANO) to solve the Nonlinear Force-Free Field (NLFFF) problem in solar
physics. Unlike conventional approaches that rely on iterative numerical
methods, our proposed PIANO directly learns the 3D magnetic field structure
from 2D boundary conditions. Specifically, PIANO integrates Efficient Channel
Attention (ECA) mechanisms with Dilated Convolutions (DC), which enhances the
model's ability to capture multimodal input by prioritizing critical channels
relevant to the magnetic field's variations. Furthermore, we apply
physics-informed loss by enforcing the force-free and divergence-free
conditions in the training process so that our prediction is consistent with
underlying physics with high accuracy. Experimental results on the ISEE NLFFF
dataset show that our PIANO not only outperforms state-of-the-art neural
operators in terms of accuracy but also shows strong consistency with the
physical characteristics of NLFFF data across magnetic fields reconstructed
from various solar active regions. The GitHub of this project is available
https://github.com/Autumnstar-cjh/PIANO

</details>


### [291] [MT-DAO: Multi-Timescale Distributed Adaptive Optimizers with Local Updates](https://arxiv.org/abs/2510.05361)
*Alex Iacob,Andrej Jovanovic,Mher Safaryan,Meghdad Kurmanji,Lorenzo Sani,Samuel Horváth,William F. Shen,Xinchi Qiu,Nicholas D. Lane*

Main category: cs.LG

TL;DR: MT-DAO 是一种新的优化器系列，通过使用多个快慢动量来解决分布式数据并行训练中的通信开销和性能差距问题，在语言模型预训练中消除了与 DDP 的性能差距，并减少了训练时间。


<details>
  <summary>Details</summary>
Motivation: 分布式数据并行（DDP）训练中的频繁通信会达到带宽瓶颈。局部 SGD 等不频繁通信策略可以降低开销，但当应用于自适应优化器时，通常会产生性能差距。

Method: 提出 MT-DAO，一种采用多个慢速和快速动量来跟踪不同时间尺度的更新动态的优化器系列，并提供了首个收敛保证。

Result: 在语言模型预训练中，MT-DAO 消除了与 DDP 的性能差距，在困惑度和训练时间方面优于不频繁通信的基线。MT-DAO 在 720M 模型规模下，以更少的步数和更短的时间达到目标困惑度。

Conclusion: MT-DAO 能够有效实现跨数据中心和跨地域的训练，并解决了分布式训练中的通信开销和性能差距问题。

Abstract: Training large models with distributed data parallelism (DDP) requires
frequent communication of gradients across workers, which can saturate
bandwidth. Infrequent communication strategies (e.g., Local SGD) reduce this
overhead but, when applied to adaptive optimizers, often suffer a performance
gap relative to fully synchronous DDP. We trace this gap to a time-scale
mismatch: the optimizer's fast-moving momentum, tuned for frequent updates,
decays too quickly to smooth gradients over long intervals, leading to
noise-dominated optimization. To address this, we propose MT-DAO, a family of
optimizers that employs multiple slow- and fast-moving first momenta or the
gradient to track update dynamics across different time scales, for which we
provide the first convergence guarantees. Empirically, for language-model
pre-training, this eliminates the performance gap with DDP, outperforming
infrequent-communication baselines in perplexity and reducing iso-token
wall-clock time by 6-27% on Ethernet interconnects. At the 720M scale, MT-DAO
reaches a target perplexity in 24% fewer steps and 35% less time than the
single-momentum DDP baseline. MT-DAO enables effective cross-datacenter
training and training over wide geographic areas.

</details>


### [292] [KVLinC : KV Cache Quantization with Hadamard Rotation and Linear Correction](https://arxiv.org/abs/2510.05373)
*Utkarsh Saxena,Kaushik Roy*

Main category: cs.LG

TL;DR: KVLinC通过结合Hadamard旋转和线性校正适配器来解决低精度KV缓存量化引起的注意力错误，在LLaMA、Qwen2.5和Qwen3模型上实现了更高的压缩率和相当的性能，并提供了一个自定义的注意力核以提高推理速度。


<details>
  <summary>Details</summary>
Motivation: 低精度KV缓存量化会引入显著误差，影响LLM生成质量，需要一种方法来减轻这些误差。

Method: KVLinC结合了Hadamard旋转（减少值量化误差）和轻量级线性校正适配器（补偿量化键引入的误差）。

Result: KVLinC在LLaMA、Qwen2.5和Qwen3模型上，在实现更高KV缓存压缩的同时，性能与强基线相当或更优。

Conclusion: KVLinC框架能够有效减轻极端低精度KV缓存量化引入的注意力误差，并在保持或提高模型性能的同时实现显著的KV缓存压缩。此外，其自定义的注意力核能够大幅提升推理速度，实现高效的长上下文LLM推理。

Abstract: Quantizing the key-value (KV) cache is a promising strategy for improving the
inference efficiency of large language models (LLMs). However, aggressive
quantization to very low precision (e.g., 2 bits) introduces significant errors
in the stored key and value tensors, which propagate through the dot-product
attention mechanism and ultimately degrade generation quality. To address this,
we propose KVLinC, a framework to mitigate attention errors introduced by KV
cache quantization in the extreme low-precision regime. KVLinC combines a
Hadamard rotation, which reduces quantization error in values, with lightweight
linear correction adapters that explicitly compensate for errors introduced by
quantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3
model families, KVLinC consistently matches or surpasses strong baselines while
achieving higher KV-cache compression. Furthermore, we implement a custom
attention kernel that results in upto 2.55x faster inference compared to Flash
Attention baseline, enabling efficient long-context LLM inference.

</details>


### [293] [Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding](https://arxiv.org/abs/2510.05385)
*Rohan Arni,Carlos Blanco*

Main category: cs.LG

TL;DR: PINNsformer的改进版S-Pformer通过移除冗余编码器并集成傅立叶特征嵌入来解决参数量和频谱偏差问题，并在各项基准测试中表现优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 为了解决现有基于Transformer的物理信息神经网络（PINNs）框架PINNsformer中编码器冗余和频谱偏差问题，提出一种新的架构。

Method: 提出Spectral PINNSformer（S-Pformer），一种改进的编码器-解码器PINNsformer架构。通过移除不必要的编码器来减少参数量，并集成傅立叶特征嵌入来解决频谱偏差问题，从而实现对频域中多尺度行为的自适应编码。

Result: S-Pformer在各项基准测试中表现优于编码器-解码器PINNsformer架构，性能达到或超过MLP，同时显著减少了参数数量。

Conclusion: S-Pformer通过去除编码器和引入傅立叶特征嵌入，成功解决了PINNsformer的参数冗余和频谱偏差问题，并在保持或提高性能的同时提高了效率。

Abstract: Physics-Informed Neural Networks (PINNs) are a useful framework for
approximating partial differential equation solutions using deep learning
methods. In this paper, we propose a principled redesign of the PINNsformer, a
Transformer-based PINN architecture. We present the Spectral PINNSformer
(S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two
key issues; 1. the redundancy (i.e. increased parameter count) of the encoder,
and 2. the mitigation of spectral bias. We find that the encoder is unnecessary
for capturing spatiotemporal correlations when relying solely on
self-attention, thereby reducing parameter count. Further, we integrate Fourier
feature embeddings to explicitly mitigate spectral bias, enabling adaptive
encoding of multiscale behaviors in the frequency domain. Our model outperforms
encoder-decoder PINNSformer architectures across all benchmarks, achieving or
outperforming MLP performance while reducing parameter count significantly.

</details>


### [294] [A Neural Network Algorithm for KL Divergence Estimation with Quantitative Error Bounds](https://arxiv.org/abs/2510.05386)
*Mikil Foss,Andrew Lamperski*

Main category: cs.LG

TL;DR: 该研究提出了一种基于浅层神经网络和随机特征的KL散度估计新算法，理论上证明了其在样本量和神经元数量有限的情况下，仍能以高概率实现较低的估计误差。


<details>
  <summary>Details</summary>
Motivation: 现有的KL散度估计方法在处理高维和大规模数据时存在效率问题，而基于神经网络的现有方法虽然有理论基础，但缺乏实际算法保证低误差。

Method: 提出一种使用具有随机隐藏权重和偏差的浅层神经网络（随机特征方法）的KL散度估计算法。

Result: 该算法能够以高概率实现KL散度估计误差为$O(m^{-1/2}+T^{-1/3})$，其中m为神经元数量，T为算法步数和样本数量。

Conclusion: 所提出的随机特征方法为KL散度估计提供了一种可行的、具有理论保证的算法，能够有效解决传统方法和现有神经网络方法的局限性。

Abstract: Estimating the Kullback-Leibler (KL) divergence between random variables is a
fundamental problem in statistical analysis. For continuous random variables,
traditional information-theoretic estimators scale poorly with dimension and/or
sample size. To mitigate this challenge, a variety of methods have been
proposed to estimate KL divergences and related quantities, such as mutual
information, using neural networks. The existing theoretical analyses show that
neural network parameters achieving low error exist. However, since they rely
on non-constructive neural network approximation theorems, they do not
guarantee that the existing algorithms actually achieve low error. In this
paper, we propose a KL divergence estimation algorithm using a shallow neural
network with randomized hidden weights and biases (i.e. a random feature
method). We show that with high probability, the algorithm achieves a KL
divergence estimation error of $O(m^{-1/2}+T^{-1/3})$, where $m$ is the number
of neurons and $T$ is both the number of steps of the algorithm and the number
of samples.

</details>


### [295] [Fusion-Based Neural Generalization for Predicting Temperature Fields in Industrial PET Preform Heating](https://arxiv.org/abs/2510.05394)
*Ahmad Alsheikh,Andreas Fischer*

Main category: cs.LG

TL;DR: 提出了一种新的深度学习框架，用于PET预制坯在吹塑前的工业微波加热过程中的温度预测，该框架具有数据效率高、泛化能力强、可扩展性等优点。


<details>
  <summary>Details</summary>
Motivation: 准确高效的温度预测对于优化PET预制坯在吹塑前的工业微波加热过程至关重要。

Method: 提出了一种新的深度学习框架，该框架具有数据效率高、泛化能力强、可扩展性等优点。通过预训练专门的神经回归器，并将其表示集成到统一的全局模型中，创建了一个能够学习跨异构输入的共享热动力学的系统。该体系结构采用了跳跃连接来提高稳定性和预测精度。

Result: 与从头开始训练的模型相比，该方法减少了对大型模拟数据集的需求，同时实现了卓越的性能。在材料可变性和几何多样性两个案例研究中进行的实验验证表明，泛化能力有了显著提高，为制造业环境中智能热控制的可扩展机器学习解决方案奠定了基础。

Conclusion: 该方法通过数据效率高和泛化策略，证明了其在工业制造领域，特别是在涉及复杂物理建模和数据量有限的应用中，具有广泛的应用前景。

Abstract: Accurate and efficient temperature prediction is critical for optimizing the
preheating process of PET preforms in industrial microwave systems prior to
blow molding. We propose a novel deep learning framework for generalized
temperature prediction. Unlike traditional models that require extensive
retraining for each material or design variation, our method introduces a
data-efficient neural architecture that leverages transfer learning and model
fusion to generalize across unseen scenarios. By pretraining specialized neural
regressor on distinct conditions such as recycled PET heat capacities or
varying preform geometries and integrating their representations into a unified
global model, we create a system capable of learning shared thermal dynamics
across heterogeneous inputs. The architecture incorporates skip connections to
enhance stability and prediction accuracy. Our approach reduces the need for
large simulation datasets while achieving superior performance compared to
models trained from scratch. Experimental validation on two case studies
material variability and geometric diversity demonstrates significant
improvements in generalization, establishing a scalable ML-based solution for
intelligent thermal control in manufacturing environments. Moreover, the
approach highlights how data-efficient generalization strategies can extend to
other industrial applications involving complex physical modeling with limited
data.

</details>


### [296] [Comparing LSTM-Based Sequence-to-Sequence Forecasting Strategies for 24-Hour Solar Proton Flux Profiles Using GOES Data](https://arxiv.org/abs/2510.05399)
*Kangwoo Yi,Bo Shen,Qin Li,Haimin Wang,Yong-Jae Moon,Jaewon Lee,Hwanhee Lee*

Main category: cs.LG

TL;DR: 本研究使用深度学习序列到序列模型预测太阳质子事件（SPE）的质子通量时间剖面。


<details>
  <summary>Details</summary>
Motivation: 准确预测SPE的质子通量时间剖面对于早期预警和缓解措施至关重要。

Method: 本研究采用基于长短期记忆网络（LSTM）的深度学习序列到序列（seq2seq）模型，利用1997年至2017年期间40个SPE（与>=M级西部日耀相关且质子通量未受干扰）的NOAA GOES观测数据，对SPE开始后的24小时质子通量剖面进行预测。通过4折分层交叉验证，评估了不同隐藏单元和嵌入维度配置的seq2seq模型，并针对以下预测场景进行了多项实验：（i）仅输入质子数据 vs. 输入质子+X射线数据；（ii）原始通量数据 vs. 趋势平滑数据；（iii）自回归预测 vs. 一次性预测。

Result: 1. 一次性预测在误差方面优于自回归预测，避免了迭代方法中的误差累积。
2. 在原始数据上，仅包含质子输入数据的模型优于包含质子+X射线输入数据的模型。
3. 然而，在趋势平滑数据上，质子+X射线模型在性能上的差距缩小甚至反转。
4. 趋势平滑显著提高了质子+X射线模型的性能，通过减少X射线通道的波动。
5. 尽管平均而言，在趋势平滑数据上训练的模型表现最佳，但表现最好的模型是在原始数据上训练的，这表明模型架构的选择有时可以克服数据预处理带来的好处。

Conclusion: 本研究表明，一次性预测比自回归预测更有效。数据预处理（如趋势平滑）对模型性能有显著影响，尤其是在结合X射线数据时。然而，模型架构的选择也可能对性能产生重要影响。

Abstract: Solar Proton Events (SPEs) cause significant radiation hazards to satellites,
astronauts, and technological systems. Accurate forecasting of their proton
flux time profiles is crucial for early warnings and mitigation. This paper
explores deep learning sequence-to-sequence (seq2seq) models based on Long
Short-Term Memory networks to predict 24-hour proton flux profiles following
SPE onsets. We used a dataset of 40 well-connected SPEs (1997-2017) observed by
NOAA GOES, each associated with a >=M-class western-hemisphere solar flare and
undisturbed proton flux profiles. Using 4-fold stratified cross-validation, we
evaluate seq2seq model configurations (varying hidden units and embedding
dimensions) under multiple forecasting scenarios: (i) proton-only input vs.
combined proton+X-ray input, (ii) original flux data vs. trend-smoothed data,
and (iii) autoregressive vs. one-shot forecasting. Our major results are as
follows: First, one-shot forecasting consistently yields lower error than
autoregressive prediction, avoiding the error accumulation seen in iterative
approaches. Second, on the original data, proton-only models outperform
proton+X-ray models. However, with trend-smoothed data, this gap narrows or
reverses in proton+X-ray models. Third, trend-smoothing significantly enhances
the performance of proton+X-ray models by mitigating fluctuations in the X-ray
channel. Fourth, while models trained on trendsmoothed data perform best on
average, the best-performing model was trained on original data, suggesting
that architectural choices can sometimes outweigh the benefits of data
preprocessing.

</details>


### [297] [Correlating Cross-Iteration Noise for DP-SGD using Model Curvature](https://arxiv.org/abs/2510.05416)
*Xin Gu,Yingtai Xiao,Guanlin He,Jiamu Bai,Daniel Kifer,Kiwan Maeng*

Main category: cs.LG

TL;DR: DP-MF 算法通过在不同迭代中相关隐私噪声来提高隐私保护训练的准确性。


<details>
  <summary>Details</summary>
Motivation: DP-SGD 在深度学习模型的准确性方面与普通 SGD 存在差距，需要改进隐私保护训练方法。

Method: 提出了一种名为 NoiseCurve 的技术，该技术利用从公开无标签数据估计的模型曲率来改进跨迭代隐私噪声的相关性。

Result: NoiseCurve 在准确性方面提供了比 DP-MF 更稳定且显著的改进，该改进在各种数据集、模型和隐私参数下都得到了验证。

Conclusion: NoiseCurve 通过利用模型曲率来改进 DP-MF 的噪声相关性，从而提高了隐私保护深度学习的准确性。

Abstract: Differentially private stochastic gradient descent (DP-SGD) offers the
promise of training deep learning models while mitigating many privacy risks.
However, there is currently a large accuracy gap between DP-SGD and normal SGD
training. This has resulted in different lines of research investigating
orthogonal ways of improving privacy-preserving training. One such line of
work, known as DP-MF, correlates the privacy noise across different iterations
of stochastic gradient descent -- allowing later iterations to cancel out some
of the noise added to earlier iterations. In this paper, we study how to
improve this noise correlation. We propose a technique called NoiseCurve that
uses model curvature, estimated from public unlabeled data, to improve the
quality of this cross-iteration noise correlation. Our experiments on various
datasets, models, and privacy parameters show that the noise correlations
computed by NoiseCurve offer consistent and significant improvements in
accuracy over the correlation scheme used by DP-MF.

</details>


### [298] [Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding](https://arxiv.org/abs/2510.05421)
*Shrenik Bhansali,Larry Heck*

Main category: cs.LG

TL;DR: Speculative decoding (SD) accelerates autoregressive (AR) generation by using a drafter to propose multi-token blocks for a verifier to accept or reject. However, many SD systems require extensive offline training or additional components, increasing costs and reducing robustness to distribution drift. We introduce Draft, Verify, & Improve (DVI), a training-aware self-speculative framework that integrates inference with continuous online learning. DVI partitions a large language model (LLM) into a drafter and a verifier. During generation, the verifier's accept/reject decisions provide supervision signals to update the drafter head. A KL->RL schedule initially calibrates using online distillation, then incorporates reward-masked cross-entropy with a policy-gradient term, enabling lossless, single-model deployment. DVI achieves a 2.16x speedup on Spec-Bench, comparable to state-of-the-art methods like EAGLE-2, with significantly less training data. Ablations confirm DVI outperforms KL-only online distillation, demonstrating that training-aware self-speculation can achieve state-of-the-art, lossless speedups with minimal training overhead.


<details>
  <summary>Details</summary>
Motivation: Autoregressive (AR) decoding in large language models (LLMs) is a significant bottleneck for latency. Existing speculative decoding (SD) methods often require substantial offline training or extra components, leading to high data and compute costs, and can be fragile when faced with distribution shifts.

Method: The paper introduces Draft, Verify, & Improve (DVI), a training-aware self-speculative framework that combines inference with online learning. An LLM is partitioned into a drafter and a verifier. During generation, the verifier's decisions generate supervision signals to update the drafter head. A KL->RL schedule is used for initial calibration through online distillation, followed by reward-masked cross-entropy and a policy-gradient term. This approach allows for lossless, single-model deployment.

Result: On the Spec-Bench benchmark, DVI achieved a 2.16x speedup in wall-clock time, which is competitive with state-of-the-art methods such as EAGLE-2. Furthermore, DVI required orders of magnitude less data for training compared to other methods. Ablation studies indicated that DVI outperformed methods using only KL-online distillation.

Conclusion: DVI demonstrates that training-aware self-speculation can achieve state-of-the-art performance, providing lossless speedups with minimal training overhead. This approach offers a more efficient and robust way to accelerate AR decoding in LLMs.

Abstract: Autoregressive (AR) decoding is a major latency bottleneck for large language
models. Speculative decoding (SD) accelerates AR by letting a drafter propose
multi-token blocks that a verifier accepts or rejects. However, many SD systems
require heavy offline training or extra components. These choices raise
data/compute cost and can yield brittle drafters under distribution drift. We
introduce \emph{Draft, Verify, \& Improve (DVI)}, a training-aware
self-speculative framework that combines inference with continual online
learning. We partition an LLM into a drafter and a verifier, and during
generation, verifier accept/reject decisions are converted into supervision
signals and used to update the drafter head. A simple \emph{KL$\rightarrow$RL}
schedule bootstraps calibration via online distillation and then adds
reward-masked cross-entropy with a on-policy policy-gradient term, preserving
lossless, single model deployment. On Spec-Bench, DVI achieves a $2.16\times$
wall-time speedup, on par with SoTA approaches like EAGLE-2, while orders of
magnitude less data for training, and ablations show that DVI outperforms
KL-only online distillation. DVI demonstrates that \emph{training-aware}
self-speculation can deliver state-of-the-art, lossless speedups with minimal
training overhead.

</details>


### [299] [Physics-Informed Machine Learning in Biomedical Science and Engineering](https://arxiv.org/abs/2510.05433)
*Nazanin Ahmadi,Qianying Cao,Jay D. Humphrey,George Em Karniadakis*

Main category: cs.LG

TL;DR: PIML通过结合物理定律和数据驱动方法，在生物医学建模领域展现出巨大潜力，主要包括PINNs、NODEs和NOs三类框架。


<details>
  <summary>Details</summary>
Motivation: 在生物医学领域，传统的黑盒学习方法在处理物理可解释性、数据稀疏性或系统复杂性方面存在不足，因此需要PIML。

Method: 本文回顾了三类主要的PIML框架：PINNs（将控制方程嵌入深度学习模型）、NODEs（提供连续时间建模，适用于动态生理系统）和NOs（学习函数空间之间的映射，适用于多尺度和空间异构生物域）。

Result: PINNs已成功应用于生物固体和流体力学、力学生物学和医学成像等领域；NODEs尤其适用于动态生理系统、药代动力学和细胞信号传导；NOs能够高效地模拟多尺度和空间异构生物域。

Conclusion: PIML在生物医学领域具有广阔的应用前景，但仍面临不确定性量化、泛化能力和与大型语言模型集成等挑战。

Abstract: Physics-informed machine learning (PIML) is emerging as a potentially
transformative paradigm for modeling complex biomedical systems by integrating
parameterized physical laws with data-driven methods. Here, we review three
main classes of PIML frameworks: physics-informed neural networks (PINNs),
neural ordinary differential equations (NODEs), and neural operators (NOs),
highlighting their growing role in biomedical science and engineering. We begin
with PINNs, which embed governing equations into deep learning models and have
been successfully applied to biosolid and biofluid mechanics, mechanobiology,
and medical imaging among other areas. We then review NODEs, which offer
continuous-time modeling, especially suited to dynamic physiological systems,
pharmacokinetics, and cell signaling. Finally, we discuss deep NOs as powerful
tools for learning mappings between function spaces, enabling efficient
simulations across multiscale and spatially heterogeneous biological domains.
Throughout, we emphasize applications where physical interpretability, data
scarcity, or system complexity make conventional black-box learning
insufficient. We conclude by identifying open challenges and future directions
for advancing PIML in biomedical science and engineering, including issues of
uncertainty quantification, generalization, and integration of PIML and large
language models.

</details>


### [300] [Adversarial Reinforcement Learning for Large Language Model Agent Safety](https://arxiv.org/abs/2510.05442)
*Zizhao Wang,Dingcheng Li,Vaishakh Keshava,Phillip Wallis,Ananth Balashankar,Peter Stone,Lukas Rutishauser*

Main category: cs.LG

TL;DR: LLM代理使用工具（如Google搜索）可能面临由工具输出中的恶意指令引起的间接提示注入攻击。我们提出了ARLAS框架，利用对抗性强化学习来训练一个能够自主生成多样化提示注入的攻击者和一个能够防御这些攻击并完成任务的代理。ARLAS通过训练代理防御所有先前的攻击者检查点来确保鲁棒性。在BrowserGym和AgentDojo上的实验表明，ARLAS显著降低了攻击成功率，同时提高了任务成功率，并生成了更多样化和更具挑战性的攻击。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM代理防御策略依赖于手动制作的攻击模式数据集进行微调，这限制了其多样性，并使代理容易受到新型提示注入的攻击。因此，需要一种更有效的方法来生成多样化的攻击并训练代理防御这些攻击。

Method: 我们提出了ARLAS（Adversarial Reinforcement Learning for Agent Safety）框架，该框架利用对抗性强化学习，将问题构建为一个双人零和博弈。ARLAS共同训练两个LLM：一个攻击者，负责自主生成多样化的提示注入；一个代理，负责在完成指定任务的同时防御攻击。为了确保鲁棒性并防止循环学习，我们采用基于种群的学习框架，训练代理防御所有先前的攻击者检查点。

Result: 在BrowserGym和AgentDojo上进行的评估表明，使用ARLAS进行微调的代理相比原始模型，攻击成功率显著降低，同时任务成功率也有所提高。此外，分析证实对抗过程生成了多样化且具有挑战性的攻击，从而产生了比基础模型更鲁棒的代理。

Conclusion: ARLAS框架通过引入对抗性强化学习，能够有效地训练LLM代理防御多样化的间接提示注入攻击，同时提高其任务执行能力。该方法生成的攻击更具挑战性，并能提升代理的整体鲁棒性，为保障LLM代理的安全提供了一种有前景的解决方案。

Abstract: Large Language Model (LLM) agents can leverage tools such as Google Search to
complete complex tasks. However, this tool usage introduces the risk of
indirect prompt injections, where malicious instructions hidden in tool outputs
can manipulate the agent, posing security risks like data leakage. Current
defense strategies typically rely on fine-tuning LLM agents on datasets of
known attacks. However, the generation of these datasets relies on manually
crafted attack patterns, which limits their diversity and leaves agents
vulnerable to novel prompt injections. To address this limitation, we propose
Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework
that leverages adversarial reinforcement learning (RL) by formulating the
problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker
that learns to autonomously generate diverse prompt injections and an agent
that learns to defend against them while completing its assigned tasks. To
ensure robustness against a wide range of attacks and to prevent cyclic
learning, we employ a population-based learning framework that trains the agent
to defend against all previous attacker checkpoints. Evaluated on BrowserGym
and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower
attack success rate than the original model while also improving their task
success rate. Our analysis further confirms that the adversarial process
generates a diverse and challenging set of attacks, leading to a more robust
agent compared to the base model.

</details>


### [301] [Prior-Aligned Meta-RL: Thompson Sampling with Learned Priors and Guarantees in Finite-Horizon MDPs](https://arxiv.org/abs/2510.05446)
*Runlin Zhou,Chixiang Chen,Elynn Chen*

Main category: cs.LG

TL;DR: 本文研究有限时间水平马尔可夫决策过程中的元强化学习，假设相关任务的最优动作值函数具有相似结构，并提出基于Thompson采样的方法MTSRL和MTSRL+，以实现元遗憾界限并优于现有基线。


<details>
  <summary>Details</summary>
Motivation: 在有限时间水平马尔可夫决策过程中，相关任务的最优动作值函数共享相似结构，这为利用元信息来提升强化学习效率提供了动机。

Method: 提出了一种基于线性表示和高斯元先验的 Thompson 风格算法（MTSRL 和 MTSRL+）。MTSRL 学习先验均值并进行后验采样；MTSRL+ 进一步估计协方差并采用先验加宽来控制样本估计误差。结合先验对齐技术，实现了元遗憾界限。

Result: MTSRL/MTSRL+ 在模拟的推荐环境中，在短暂探索后能够跟踪元预言机，并显著优于不考虑先验的强化学习和仅考虑 bandit 的元基线。理论上，在已知协方差的情况下，元遗憾界限为 $	ilde{O}(H^{4}S^{3/2}	ext{}	ext{}	ext{ANK})$；在学习协方差的情况下，为 $	ilde{O}(H^{4}S^{3/2}	ext{}	ext{AN^3K})$。

Conclusion: 本文首次为具有学习到的 Q-先验的 Thompson 风格强化学习提供了元遗憾界限，并为实验丰富的场景提供了实用的方法（如通过 RLSVI 进行预启动、OLS 聚合、协方差加宽）。所提出的算法在模拟环境中表现优于现有基线。

Abstract: We study meta-reinforcement learning in finite-horizon MDPs where related
tasks share similar structures in their optimal action-value functions.
Specifically, we posit a linear representation
$Q^*_h(s,a)=\Phi_h(s,a)\,\theta^{(k)}_h$ and place a Gaussian meta-prior $
\mathcal{N}(\theta^*_h,\Sigma^*_h)$ over the task-specific parameters
$\theta^{(k)}_h$. Building on randomized value functions, we propose two
Thompson-style algorithms: (i) MTSRL, which learns only the prior mean and
performs posterior sampling with the learned mean and known covariance; and
(ii) $\text{MTSRL}^{+}$, which additionally estimates the covariance and
employs prior widening to control finite-sample estimation error. Further, we
develop a prior-alignment technique that couples the posterior under the
learned prior with a meta-oracle that knows the true prior, yielding
meta-regret guarantees: we match prior-independent Thompson sampling in the
small-task regime and strictly improve with more tasks once the prior is
learned. Concretely, for known covariance we obtain
$\tilde{O}(H^{4}S^{3/2}\sqrt{ANK})$ meta-regret, and with learned covariance
$\tilde{O}(H^{4}S^{3/2}\sqrt{AN^3K})$; both recover a better behavior than
prior-independent after $K \gtrsim \tilde{O}(H^2)$ and $K \gtrsim
\tilde{O}(N^2H^2)$, respectively. Simulations on a stateful recommendation
environment (with feature and prior misspecification) show that after brief
exploration, MTSRL/MTSRL\(^+\) track the meta-oracle and substantially
outperform prior-independent RL and bandit-only meta-baselines. Our results
give the first meta-regret guarantees for Thompson-style RL with learned
Q-priors, and provide practical recipes (warm-start via RLSVI, OLS aggregation,
covariance widening) for experiment-rich settings.

</details>


### [302] [QDeepGR4J: Quantile-based ensemble of deep learning and GR4J hybrid rainfall-runoff models for extreme flow prediction with uncertainty quantification](https://arxiv.org/abs/2510.05453)
*Arpit Kapoor,Rohitash Chandra*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Quantile DeepGR4J 的新框架，通过结合深度学习和分位数回归，提高了河流流量预测的准确性和不确定性量化能力，并可用于洪水风险评估。


<details>
  <summary>Details</summary>
Motivation: 为了提高水文模型的可解释性和预测性能，结合了深度学习模型。已有工作 DeepGR4J 增强了 GR4J 模型，特别是在干旱流域表现更好。为了量化不确定性并预测极端值，需要进一步扩展该模型。

Method: 本文将分位数回归集成学习框架扩展到 DeepGR4J 模型，以量化流量预测的不确定性。该框架还用于识别可能导致洪水的极端流量事件，并扩展到多步流量预测。使用 CAMELS-Aus 数据集进行评估。

Result: Quantile DeepGR4J 框架在预测准确性和不确定性区间质量（区间得分）方面优于基线深度学习模型。此外，该模型在洪水风险评估方面表现出作为预警系统的适用性。

Conclusion: Quantile DeepGR4J 框架能够提高河流流量预测的准确性和不确定性量化能力，为洪水风险评估提供了一个有前途的工具。

Abstract: Conceptual rainfall-runoff models aid hydrologists and climate scientists in
modelling streamflow to inform water management practices. Recent advances in
deep learning have unravelled the potential for combining hydrological models
with deep learning models for better interpretability and improved predictive
performance. In our previous work, we introduced DeepGR4J, which enhanced the
GR4J conceptual rainfall-runoff model using a deep learning model to serve as a
surrogate for the routing component. DeepGR4J had an improved rainfall-runoff
prediction accuracy, particularly in arid catchments. Quantile regression
models have been extensively used for quantifying uncertainty while aiding
extreme value forecasting. In this paper, we extend DeepGR4J using a quantile
regression-based ensemble learning framework to quantify uncertainty in
streamflow prediction. We also leverage the uncertainty bounds to identify
extreme flow events potentially leading to flooding. We further extend the
model to multi-step streamflow predictions for uncertainty bounds. We design
experiments for a detailed evaluation of the proposed framework using the
CAMELS-Aus dataset. The results show that our proposed Quantile DeepGR4J
framework improves the predictive accuracy and uncertainty interval quality
(interval score) compared to baseline deep learning models. Furthermore, we
carry out flood risk evaluation using Quantile DeepGR4J, and the results
demonstrate its suitability as an early warning system.

</details>


### [303] [AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning](https://arxiv.org/abs/2510.05468)
*Yurun Song,Zhuoyi Yang,Ian G. Harris,Sangeetha Abdu Jyothi*

Main category: cs.LG

TL;DR: 该研究提出了一种名为AMAQ的参数高效的拆分学习方法，通过自适应混合比特激活量化来降低LLM分布式训练的通信开销和计算开销，在保持较低通信成本的同时提高了生成和分类的准确性。


<details>
  <summary>Details</summary>
Motivation: 为了解决大型语言模型（LLM）分布式训练中通信效率和计算开销的挑战，特别是在资源受限的设备上进行协作式训练时。

Method: 提出并实现了一种名为自适应混合比特激活量化（AMAQ）的策略，该策略通过比特正则化，根据特征和层的重要性动态地将比特预算分配到不同通道，将激活和梯度从高精度（6-8比特）逐步压缩到低精度（3-4比特）。

Result: 在相同的比特预算下，AMAQ在LLaMA3 8B和Qwen2.5 7B等模型上，生成准确率提高了约2.5%，分类准确率提高了约1.3%。此外，AMAQ显著提高了训练稳定性，并减少了超低比特表示在训练过程中出现的崩溃现象。实验表明AMAQ能有效地集成到实际的多机协作训练设置中，在训练期间仅有适度的比特自适应通信开销的情况下，提供了卓越的推理准确性。

Conclusion: AMAQ是一种实用且有效的解决方案，可以在LLM分布式训练中实现低通信成本，它在保持较低通信成本的同时，提高了训练效率和模型性能，使得在资源受限的设备上进行LLM的协作式训练成为可能。

Abstract: Large Language Models (LLMs) are scaling rapidly, creating significant
challenges for collaborative server client distributed training, particularly
in terms of communication efficiency and computational overheads. To address
these challenges, we implement Parameter-efficient Split Learning, which
effectively balances efficiency and performance for collaborative training on
low-resource devices.
  To reduce communication overhead in collaborative training, we introduce
Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that
progressively compresses activations and gradients from high precision (6 to 8
bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively
allocating bit budgets across channels based on feature wise and layer wise
importance using bit regularization.
  Under the same bit budgets, AMAQ outperforms fixed-precision approaches,
delivering about 2.5% higher generation accuracy and about 1.3% better
classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition,
it significantly enhances training stability and reducing ultra-low bit
representation collapse during the training.
  Experiments demonstrate that AMAQ integrates effectively into practical
multi-machine collaborative training setups, offering superior inference
accuracy with only a modest communication overhead for bits adaptation during
training. This trade off makes AMAQ a practical and effective solution for
collaborative training with minimal communication cost.

</details>


### [304] [ATOM: A Pretrained Neural Operator for Multitask Molecular Dynamics](https://arxiv.org/abs/2510.05482)
*Luke Thompson,Davy Guan,Dai Shi,Slade Matthews,Junbin Gao,Andi Han*

Main category: cs.LG

TL;DR: ATOM是一个用于分子动力学模拟的预训练Transformer神经网络算子，它通过准等变设计和时间注意力机制提高了灵活性和效率，并能在不同分子和时间尺度上实现出色的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习模型在计算药物发现、材料科学和生物化学领域虽然能加速分子动力学(MD)模拟，但它们通常强制要求严格的等变性、依赖顺序模拟、并且是单任务模型，限制了其在处理未见化合物和更长时间尺度时的泛化能力和效率。

Method: 提出了一种名为ATOM（Atomistic Transformer Operator for Molecules）的预训练Transformer神经网络算子，用于多任务分子动力学。ATOM采用了准等变设计，无需显式的分子图，并利用时间注意力机制实现对多个未来状态的并行解码。为了支持跨化学物质和时间尺度的预训练，研究者创建了一个包含80个化合物、超过250万飞秒轨迹的大型、多样化且数值稳定的MD数据集TG80。

Result: ATOM在MD17、RMD17和MD22等已有的单任务基准测试中取得了最先进的性能。在TG80数据集上进行多任务预训练后，ATOM在未见过的分子和不同时间尺度上展现了出色的零样本泛化能力。

Conclusion: ATOM在准确性、效率和可转移性方面代表了分子动力学模型的重要进步，有望解决现有方法的局限性。

Abstract: Molecular dynamics (MD) simulations underpin modern computational drug dis-
covery, materials science, and biochemistry. Recent machine learning models
provide high-fidelity MD predictions without the need to repeatedly solve
quantum mechanical forces, enabling significant speedups over conventional
pipelines. Yet many such methods typically enforce strict equivariance and rely
on sequential rollouts, thus limiting their flexibility and simulation
efficiency. They are also com- monly single-task, trained on individual
molecules and fixed timeframes, which restricts generalization to unseen
compounds and extended timesteps. To address these issues, we propose Atomistic
Transformer Operator for Molecules (ATOM), a pretrained transformer neural
operator for multitask molecular dynamics. ATOM adopts a quasi-equivariant
design that requires no explicit molecular graph and employs a temporal
attention mechanism, allowing for the accurate parallel decod- ing of multiple
future states. To support operator pretraining across chemicals and timescales,
we curate TG80, a large, diverse, and numerically stable MD dataset with over
2.5 million femtoseconds of trajectories across 80 compounds. ATOM achieves
state-of-the-art performance on established single-task benchmarks, such as
MD17, RMD17 and MD22. After multitask pretraining on TG80, ATOM shows
exceptional zero-shot generalization to unseen molecules across varying time
hori- zons. We believe ATOM represents a significant step toward accurate,
efficient, and transferable molecular dynamics models

</details>


### [305] [The Method of Infinite Descent](https://arxiv.org/abs/2510.05489)
*Reza T. Batley,Sourav Saha*

Main category: cs.LG

TL;DR: 本论文提出了一种名为“无限下降法”的优化新范式，通过直接求解最优性条件来训练模型，实现了非迭代学习。


<details>
  <summary>Details</summary>
Motivation: 传统模型训练依赖于微小的、局部的、迭代的更新，这种方法可以追溯到 Cauchy 和 Newton。本研究旨在探索一种新的优化方法，以克服迭代训练的局限性。

Method: 提出了一种名为“无限下降法”的半解析优化范式，将训练重构为一阶最优性条件的直接求解。通过对泰勒展开进行解析重求和，该方法得到了更新步骤的精确代数方程，并提出了一种直接求解的算法。

Result: 在提出的 AION（Analytic, Infinitely-Optimisable Network）架构上进行了演示。AION 架构专为满足无限下降法所需的代数闭包而设计。在一个简单的测试问题中，AION 在一个下降步骤中就达到了最优值。

Conclusion: 无限下降法与 AION 模型架构相结合，展示了如何通过解析结构实现精确的、非迭代的收敛。该方法不仅限于此示例，还可以应用于任何具有适当闭包的架构，这为非迭代学习开辟了新的途径，并提出了“无限类”模型这一新类别。

Abstract: Training - the optimisation of complex models - is traditionally performed
through small, local, iterative updates [D. E. Rumelhart, G. E. Hinton, R. J.
Williams, Nature 323, 533-536 (1986)]. Approximating solutions through
truncated gradients is a paradigm dating back to Cauchy [A.-L. Cauchy, Comptes
Rendus Math\'ematique 25, 536-538 (1847)] and Newton [I. Newton, The Method of
Fluxions and Infinite Series (Henry Woodfall, London, 1736)]. This work
introduces the Method of Infinite Descent, a semi-analytic optimisation
paradigm that reformulates training as the direct solution to the first-order
optimality condition. By analytical resummation of its Taylor expansion, this
method yields an exact, algebraic equation for the update step. Realisation of
the infinite Taylor tower's cascading resummation is formally derived, and an
exploitative algorithm for the direct solve step is proposed.
  This principle is demonstrated with the herein-introduced AION (Analytic,
Infinitely-Optimisable Network) architecture. AION is a model designed
expressly to satisfy the algebraic closure required by Infinite Descent. In a
simple test problem, AION reaches the optimum in a single descent step.
Together, this optimiser-model pair exemplify how analytic structure enables
exact, non-iterative convergence. Infinite Descent extends beyond this example,
applying to any appropriately closed architecture. This suggests a new class of
semi-analytically optimisable models: the \emph{Infinity Class}; sufficient
conditions for class membership are discussed. This offers a pathway toward
non-iterative learning.

</details>


### [306] [NorMuon: Making Muon more efficient and scalable](https://arxiv.org/abs/2510.05491)
*Zichong Li,Liming Liu,Chen Liang,Weizhu Chen,Tuo Zhao*

Main category: cs.LG

TL;DR: NorMuon结合了Muon的参数更新正交化和Adam的逐层自适应学习率，在不显著增加内存占用的情况下，提高了训练效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 虽然Muon优化器通过参数更新正交化在改善优化几何形状方面表现出潜力，但尚未系统探索其与Adam优化器优势的结合。此外，Muon产生的更新可能导致某些神经元主导优化过程，需要解决参数利用不平衡的问题。

Method: 提出了一种名为NorMuon（Neuron-wise Normalized Muon）的优化器，它结合了Muon的正交化方法和逐层自适应学习率。NorMuon通过保持每个神经元的二阶动量统计并进行行归一化来解决更新不平衡问题。同时，为了实现大规模部署，开发了基于FSDP2框架的分布式实现。

Result: 在不同规模的模型上进行的大量实验表明，NorMuon在训练效率上持续优于Adam和Muon。在1.1B预训练设置下，NorMuon比Adam的训练效率高21.74%，比Muon高11.31%，同时内存占用与Muon相当。

Conclusion: 正交化和自适应学习率是互补的方法，而不是相互竞争的方法，这为大规模深度学习中的优化器设计开辟了新的途径。

Abstract: The choice of optimizer significantly impacts the training efficiency and
computational costs of large language models (LLMs). Recently, the Muon
optimizer has demonstrated promising results by orthogonalizing parameter
updates, improving optimization geometry through better conditioning. Despite
Muon's emergence as a candidate successor to Adam, the potential for jointly
leveraging their strengths has not been systematically explored. In this work,
we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an
optimizer that synergistically combines orthogonalization with neuron-level
adaptive learning rates. Our analysis reveals that while Muon effectively
reduces condition numbers, the resulting updates exhibit highly non-uniform
neuron norms, causing certain neurons to dominate the optimization process.
NorMuon addresses this imbalance by maintaining second-order momentum
statistics for each neuron and applying row-wise normalization after
orthogonalization, ensuring balanced parameter utilization while preserving
Muon's conditioning benefits. To enable practical deployment at scale, we
develop an efficient distributed implementation under the FSDP2 framework that
strategically distributes orthogonalization computations across devices.
Experiments across multiple model scales demonstrate that NorMuon consistently
outperforms both Adam and Muon, achieving 21.74% better training efficiency
than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while
maintaining a comparable memory footprint to Muon. Our findings suggest that
orthogonalization and adaptive learning rates are complementary rather than
competing approaches, opening new avenues for optimizer design in large-scale
deep learning.

</details>


### [307] [High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training](https://arxiv.org/abs/2510.05492)
*Zhuoyi Huang,Nutan Sahoo,Anamika Kumari,Girish Kumar,Kexuan Cai,Shixing Cao,Yue Kang,Tian Xia,Somya Chatterjee,Nicholas Hausman,Aidan Jay,Eric S. Rosenthal,Soundar Srinivasan,Sadid Hasan,Alex Fedorov,Sulaiman Vesal,Soundar Srinivasan,Sadid Hasan,Alex Fedorov,Sulaiman Vesal*

Main category: cs.LG

TL;DR: 生成式AI在医疗领域的应用受到数据隐私的限制，本研究提出了一种名为SSSD-ECG的新模型，通过引入时间-频率域监督（MIDT-ECG）和多模态人口统计学条件，提高了生成心电图（ECG）的保真度和个性化水平，同时保证了隐私安全，并在低数据量情况下提高了下游任务的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的生成模型在生成心电图时存在形态保真度不足和无法生成个性化信号的问题，限制了其在临床上的应用。

Method: 提出了一种基于条件扩散模型的结构化状态空间模型（SSSD-ECG），并引入了两个创新点：1. 时间-频率域监督训练范式（MIDT-ECG），以增强生理结构的真实性；2. 多模态人口统计学条件，以实现患者特定的信号生成。

Result: 在PTB-XL数据集上进行评估，结果显示：1. MIDT-ECG在形态保真度上有所提高；2. 隐私保护性能增强，各项指标比基线提高了4-8%；3. 交互相关误差平均减少了74%；4. 人口统计学条件改善了信噪比和个性化水平；5. 在数据稀疏的情况下，使用合成ECG增强的数据训练的分类器性能可与仅使用真实数据训练的分类器相媲美。

Conclusion: SSSD-ECG模型通过提出的时间-频率结构正则化方案，可以生成个性化、高保真度、隐私保护的ECG替代信号，尤其适用于数据稀缺的场景，推动了生成式AI在医疗领域的负责任应用。

Abstract: The development of machine learning for cardiac care is severely hampered by
privacy restrictions on sharing real patient electrocardiogram (ECG) data.
Although generative AI offers a promising solution, the real-world use of
existing model-synthesized ECGs is limited by persistent gaps in
trustworthiness and clinical utility. In this work, we address two major
shortcomings of current generative ECG methods: insufficient morphological
fidelity and the inability to generate personalized, patient-specific
physiological signals. To address these gaps, we build on a conditional
diffusion-based Structured State Space Model (SSSD-ECG) with two principled
innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a
novel training paradigm with time-frequency domain supervision to enforce
physiological structural realism, and (2) multi-modal demographic conditioning
to enable patient-specific synthesis. We comprehensively evaluate our approach
on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity,
clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG
achieves substantial gains: it improves morphological coherence, preserves
strong privacy guarantees with all metrics evaluated exceeding the baseline by
4-8%, and notably reduces the interlead correlation error by an average of 74%,
while demographic conditioning enhances signal-to-noise ratio and
personalization. In critical low-data regimes, a classifier trained on datasets
supplemented with our synthetic ECGs achieves performance comparable to a
classifier trained solely on real data. Together, we demonstrate that ECG
synthesizers, trained with the proposed time-frequency structural
regularization scheme, can serve as personalized, high-fidelity,
privacy-preserving surrogates when real data are scarce, advancing the
responsible use of generative AI in healthcare.

</details>


### [308] [Fundamental Limits of Crystalline Equivariant Graph Neural Networks: A Circuit Complexity Perspective](https://arxiv.org/abs/2510.05494)
*Yang Cao,Zhao Song,Jiahao Zhang,Jiale Zhao*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Graph neural networks (GNNs) have become a core paradigm for learning on
relational data. In materials science, equivariant GNNs (EGNNs) have emerged as
a compelling backbone for crystalline-structure prediction, owing to their
ability to respect Euclidean symmetries and periodic boundary conditions.
Despite strong empirical performance, their expressive power in periodic,
symmetry-constrained settings remains poorly understood. This work
characterizes the intrinsic computational and expressive limits of EGNNs for
crystalline-structure prediction through a circuit-complexity lens. We analyze
the computations carried out by EGNN layers acting on node features, atomic
coordinates, and lattice matrices, and prove that, under polynomial precision,
embedding width $d=O(n)$ for $n$ nodes, $O(1)$ layers, and $O(1)$-depth,
$O(n)$-width MLP instantiations of the message/update/readout maps, these
models admit a simulation by a uniform $\mathsf{TC}^0$ threshold-circuit family
of polynomial size (with an explicit constant-depth bound). Situating EGNNs
within $\mathsf{TC}^0$ provides a concrete ceiling on the decision and
prediction problems solvable by such architectures under realistic resource
constraints and clarifies which architectural modifications (e.g., increased
depth, richer geometric primitives, or wider layers) are required to transcend
this regime. The analysis complements Weisfeiler-Lehman style results that do
not directly transfer to periodic crystals, and offers a complexity-theoretic
foundation for symmetry-aware graph learning on crystalline systems.

</details>


### [309] [EEG-Based Acute Pain Classification: Machine Learning Model Comparison and Real-Time Clinical Feasibility](https://arxiv.org/abs/2510.05511)
*Aavid Mathrawala,Dhruv Kurup,Josie Lau*

Main category: cs.LG

TL;DR: 该研究使用脑电图（EEG）和机器学习来评估镇静或认知障碍患者的疼痛程度，以期改善疼痛管理并减少阿片类药物的过度使用。


<details>
  <summary>Details</summary>
Motivation: 当前医院的疼痛评估方法（如自我报告或心电图生命体征）无法有效评估危重、镇静或认知障碍患者的疼痛，可能导致疼痛治疗不足或阿片类药物滥用。因此，需要一种更客观的疼痛评估方法。

Method: 本研究收集了52名健康成人暴露于三种不同强度激光（低、中、高）刺激下的脑电图（EEG）数据。将每段4秒的EEG数据转换为包含537个特征（如光谱功率、带比、Hjorth参数、熵、相干性、小波能量和峰值频率）的向量。评估了九种传统的机器学习模型，并使用留一参与者交叉验证法进行评估。另外，还实现了一个实时XGBoost模型。

Result: 支持向量机（SVM）模型在使用径向基函数核时，离线评估准确率达到88.9%，推理时间为1.02毫秒，表现最佳。特征重要性分析结果与现有的疼痛生理学理论一致，包括对侧alpha波抑制、中线theta/alpha波增强和额叶gamma波爆发。实时XGBoost模型的端到端延迟约为4毫秒，准确率为94.2%。

Conclusion: 基于脑电图（EEG）的疼痛监测在技术上是可行的，并且在真实临床环境中具有应用潜力。该研究结果为进一步的临床验证铺平了道路，有望改善危重患者的疼痛管理。

Abstract: Current pain assessment within hospitals often relies on self-reporting or
non-specific EKG vital signs. This system leaves critically ill, sedated, and
cognitively impaired patients vulnerable to undertreated pain and opioid
overuse. Electroencephalography (EEG) offers a noninvasive method of measuring
brain activity. This technology could potentially be applied as an assistive
tool to highlight nociceptive processing in order to mitigate this issue. In
this study, we compared machine learning models for classifying high-pain
versus low/no-pain EEG epochs using data from fifty-two healthy adults exposed
to laser-evoked pain at three intensities (low, medium, high). Each four-second
epoch was transformed into a 537-feature vector spanning spectral power, band
ratios, Hjorth parameters, entropy measures, coherence, wavelet energies, and
peak-frequency metrics. Nine traditional machine learning models were evaluated
with leave-one-participant-out cross-validation. A support vector machine with
radial basis function kernel achieved the best offline performance with 88.9%
accuracy and sub-millisecond inference time (1.02 ms). Our Feature importance
analysis was consistent with current canonical pain physiology, showing
contralateral alpha suppression, midline theta/alpha enhancement, and frontal
gamma bursts. The real-time XGBoost model maintained an end-to-end latency of
about 4 ms and 94.2% accuracy, demonstrating that an EEG-based pain monitor is
technically feasible within a clinical setting and provides a pathway towards
clinical validation.

</details>


### [310] [NeST-BO: Fast Local Bayesian Optimization via Newton-Step Targeting of Gradient and Hessian Information](https://arxiv.org/abs/2510.05516)
*Wei-Ting Tang,Akshay Kudva,Joel A. Paulson*

Main category: cs.LG

TL;DR: NeST-BO是一种新的贝叶斯优化方法，通过学习梯度和Hessian信息，并优化低维子空间来解决高维问题，从而实现更快的收敛和更低的遗憾。


<details>
  <summary>Details</summary>
Motivation: 在高维问题中，贝叶斯优化（BO）虽然有效但仍然面临挑战。

Method: NeST-BO 通过高斯过程代理共同学习梯度和Hessian信息，并选择通过牛顿步误差的单步前瞻界限来进行评估。它通过在低维子空间中优化采集，将计算成本从 O(d^2) 降低到 O(m^2)，其中 m « d。

Result: NeST-BO 在高维合成和真实世界的问题中，包括具有数千个变量和未知活动子空间的问题，显示出比最先进的局部和高维 BO 基线更快地收敛和更低的遗憾。

Conclusion: NeST-BO 是一种有效的局部贝叶斯优化方法，通过学习梯度和Hessian信息，并优化低维子空间，在高维问题中实现了更快的收敛和更低的遗憾。

Abstract: Bayesian optimization (BO) is effective for expensive black-box problems but
remains challenging in high dimensions. We propose NeST-BO, a local BO method
that targets the Newton step by jointly learning gradient and Hessian
information with Gaussian process surrogates, and selecting evaluations via a
one-step lookahead bound on Newton-step error. We show that this bound (and
hence the step error) contracts with batch size, so NeST-BO directly inherits
inexact-Newton convergence: global progress under mild stability assumptions
and quadratic local rates once steps are sufficiently accurate. To scale, we
optimize the acquisition in low-dimensional subspaces (e.g., random embeddings
or learned sparse subspaces), reducing the dominant cost of learning curvature
from $O(d^2)$ to $O(m^2)$ with $m \ll d$ while preserving step targeting.
Across high-dimensional synthetic and real-world problems, including cases with
thousands of variables and unknown active subspaces, NeST-BO consistently
yields faster convergence and lower regret than state-of-the-art local and
high-dimensional BO baselines.

</details>


### [311] [Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment](https://arxiv.org/abs/2510.05526)
*Ziyi Chen,Junyi Li,Peiran Yu,Heng Huang*

Main category: cs.LG

TL;DR: RLHF和DPO训练可能受到偏好损坏、奖励过度优化和冗长偏见等问题的严重影响。本文提出了RLHF-COV和DPO-COV算法，可以同时解决这些问题，并在理论上和实验上都证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: RLHF和DPO是使大型语言模型与人类偏好对齐的重要技术，但它们的训练质量会受到偏好损坏、奖励过度优化和冗长偏见等问题的严重影响。现有的大多数工作只解决了其中一个问题，或者需要大量的计算资源来估计多个奖励模型，并且缺乏泛化能力的理论保证。

Method: 提出RLHF-COV和DPO-COV算法，能够同时缓解偏好损坏、奖励过度优化和冗长偏见这三个问题，适用于离线和在线设置。通过推导DPO-COV算法在损坏数据上的长度正则化泛化误差界，理论上证明了其性能。

Result: 理论上，DPO-COV算法的长度正则化泛化误差界与在干净数据上进行更简单情况下的最优已知速率相匹配。实验证明了DPO-COV算法在离线和在线设置下的有效性。

Conclusion: RLHF-COV和DPO-COV算法能够有效地同时解决RLHF和DPO训练中的三大挑战，并且DPO-COV算法易于实现且无需奖励估计。这进一步证明了香草RLHF和DPO算法之间的等价性。

Abstract: Reinforcement learning from human feedback (RLHF) and direct preference
optimization (DPO) are important techniques to align large language models
(LLM) with human preference. However, the quality of RLHF and DPO training is
seriously compromised by \textit{\textbf{C}orrupted} preference, reward
\textit{\textbf{O}veroptimization}, and bias towards
\textit{\textbf{V}erbosity}. To our knowledge, most existing works tackle only
one of these important issues, and the few other works require much computation
to estimate multiple reward models and lack theoretical guarantee of
generalization ability. In this work, we propose RLHF-\textbf{COV} and
DPO-\textbf{COV} algorithms that can simultaneously mitigate these three
issues, in both offline and online settings. This ability is theoretically
demonstrated by obtaining length-regularized generalization error rates for our
DPO-COV algorithms trained on corrupted data, which match the best-known rates
for simpler cases with clean data and without length regularization. Moreover,
our DPO-COV algorithm is simple to implement without reward estimation, and is
proved to be equivalent to our RLHF-COV algorithm, which directly implies the
equivalence between the vanilla RLHF and DPO algorithms. Experiments
demonstrate the effectiveness of our DPO-COV algorithms under both offline and
online settings.

</details>


### [312] [Transfer Learning on Edge Connecting Probability Estimation under Graphon Model](https://arxiv.org/abs/2510.05527)
*Yuyao Wang,Yu-Hung Cheng,Debarghya Mukherjee,Huimin Cheng*

Main category: cs.LG

TL;DR: GTRANS是一个图 لن估计的迁移学习框架，通过整合邻域平滑和Gromov-Wasserstein最优传输来对齐和转移图结构模式，并包含自适应去偏机制以防止负迁移，从而提高小目标图的估计精度及其下游应用性能。


<details>
  <summary>Details</summary>
Motivation: 在实际应用中，图 لن估计通常需要大量数据，但往往只能观察到小型网络。为了解决这个问题，需要一种方法来利用大型相关源图的结构信息来改善小型目标图的估计。 

Method: GTRANS提出了一种新的迁移学习框架，该框架整合了邻域平滑和Gromov-Wasserstein最优传输，以对齐和转移图之间的结构模式。它还包括一个自适应去偏机制，通过残差平滑来识别和纠正目标特定的偏差，以防止负迁移。

Result: GTRANS在理论上保证了估计的对齐矩阵的稳定性，并在大量的合成和真实数据实验中证明了其在提高目标图估计精度方面的有效性。

Conclusion: GTRANS通过整合邻域平滑和Gromov-Wasserstein最优传输，并包含自适应去偏机制，能够有效地从大型源图转移结构信息到小型目标图，从而提高估计精度，并增强图分类和链接预测等下游任务的性能。

Abstract: Graphon models provide a flexible nonparametric framework for estimating
latent connectivity probabilities in networks, enabling a range of downstream
applications such as link prediction and data augmentation. However, accurate
graphon estimation typically requires a large graph, whereas in practice, one
often only observes a small-sized network. One approach to addressing this
issue is to adopt a transfer learning framework, which aims to improve
estimation in a small target graph by leveraging structural information from a
larger, related source graph. In this paper, we propose a novel method, namely
GTRANS, a transfer learning framework that integrates neighborhood smoothing
and Gromov-Wasserstein optimal transport to align and transfer structural
patterns between graphs. To prevent negative transfer, GTRANS includes an
adaptive debiasing mechanism that identifies and corrects for target-specific
deviations via residual smoothing. We provide theoretical guarantees on the
stability of the estimated alignment matrix and demonstrate the effectiveness
of GTRANS in improving the accuracy of target graph estimation through
extensive synthetic and real data experiments. These improvements translate
directly to enhanced performance in downstream applications, such as the graph
classification task and the link prediction task.

</details>


### [313] [ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization](https://arxiv.org/abs/2510.05528)
*Lawrence Liu,Alexander Liu,Mengdi Wang,Tuo Zhao,Lin F. Yang*

Main category: cs.LG

TL;DR: ARMOR是一种创新的模型压缩算法，通过矩阵分解实现2:4稀疏性，在保持模型性能的同时大幅降低计算和内存需求。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）因其巨大的计算和内存需求而面临部署挑战，而现有的稀疏化方法常导致性能下降。ARMOR旨在解决这一问题，提供一种能在保持模型性能的同时实现硬件加速的方法。

Method: ARMOR通过将权重矩阵分解为2:4稀疏核心和两个低开销的块对角矩阵来实现。通过块坐标下降算法优化稀疏核心和块对角矩阵，以最小化代理损失。

Result: 在Llama和Qwen模型上的实验表明，ARMOR在各种下游任务和困惑度评估中，显著优于现有的2:4稀疏化方法，同时保持了2:4稀疏性带来的推理加速和内存减少。

Conclusion: ARMOR是一种有效的模型压缩算法，它通过创新的矩阵分解方法，在模型压缩和任务准确性之间取得了更好的平衡，克服了传统2:4稀疏化方法性能下降的缺点。

Abstract: Large language models (LLMs) present significant deployment challenges due to
their immense computational and memory requirements. While semi-structured
pruning, particularly 2:4 sparsity, offers a path to practical hardware
acceleration, existing methods often incur substantial performance degradation.
To bridge this gap, we introduce ARMOR: (Adaptive Representation with
Matrix-factORization), a novel one-shot post-training pruning algorithm.
Instead of directly pruning weights, ARMOR factorizes each weight matrix into a
2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These
wrappers act as efficient pre and post-transformation error correctors,
offering greater flexibility to preserve model quality compared to conventional
2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen
through a block coordinate descent algorithm that minimizes a layer-wise proxy
loss. We theoretically prove this optimization is guaranteed to converge to a
solution with a proxy loss less than or equal to state-of-the-art pruning
algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and
Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and
significantly outperforms state-of-the-art 2:4 pruning methods across a wide
range of downstream tasks and perplexity evaluations. ARMOR achieves this
superior performance while retaining the inference speedups and substantial
memory usage reductions of 2:4 pruning, establishing a more effective trade-off
between model compression and task accuracy

</details>


### [314] [LATTA: Langevin-Anchored Test-Time Adaptation for Enhanced Robustness and Stability](https://arxiv.org/abs/2510.05530)
*Harshil Vejendla*

Main category: cs.LG

TL;DR: Latta是一种新的测试时间自适应方法，通过引入噪声权重扰动和稳定的权重锚定来提高适应稳定性和有效性，同时避免灾难性遗忘，并在CIFAR-10-C等基准测试中取得了新的最先进成果。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时间自适应（TTA）方法（如Tent）在小批量大小或具有挑战性的数据损坏情况下，由于过于确定的更新和复杂的损失表面，可能存在不稳定性并灾难性地遗忘源知识。

Method: Latta通过（1）受随机梯度Langevin动力学（SGLD）启发的噪声权重扰动来探索局部参数空间和逃离不良局部最小值，以及（2）防止模型偏离其鲁棒的源预训练的稳定权重锚定，来正则化适应过程。

Result: Latta在CIFAR-10-C上的平均准确率提高了2%以上，同时降低了性能方差，显著优于Tent、CoTTA和EATA等现有方法，并在CIFAR-10-C等标准基准测试中设定了自监督TTA的新最先进水平。

Conclusion: Latta通过结合噪声权重扰动和稳定的权重锚定，有效且稳定地实现了测试时间自适应，而无需架构更改或昂贵的蒙特卡洛采样，解决了现有方法的局限性。

Abstract: Test-time adaptation (TTA) aims to adapt a pretrained model to distribution
shifts using only unlabeled test data. While promising, existing methods like
Tent suffer from instability and can catastrophically forget the source
knowledge, especially with small batch sizes or challenging corruptions. We
argue that this arises from overly deterministic updates on a complex loss
surface. In this paper, we introduce Langevin-Anchored Test-Time Adaptation
(LATTA), a novel approach that regularizes adaptation through two key
mechanisms: (1) a noisy weight perturbation inspired by Stochastic Gradient
Langevin Dynamics (SGLD) to explore the local parameter space and escape poor
local minima, and (2) a stable weight anchor that prevents the model from
diverging from its robust source pre-training. This combination allows LATTA to
adapt effectively without sacrificing stability. Unlike prior Bayesian TTA
methods, LATTA requires no architectural changes or expensive Monte Carlo
passes. We conduct extensive experiments on standard benchmarks, including
Rotated-MNIST and the more challenging CIFAR-10-C. Our results demonstrate that
LATTA significantly outperforms existing methods, including Tent, CoTTA, and
EATA, setting a new state of the art for self-supervised TTA by improving
average accuracy on CIFAR-10-C by over 2% while simultaneously reducing
performance variance.

</details>


### [315] [Permutation-Invariant Representation Learning for Robust and Privacy-Preserving Feature Selection](https://arxiv.org/abs/2510.05535)
*Rui Liu,Tao Zhe,Yanjie Fu,Feng Xia,Ted Senator,Dongjie Wang*

Main category: cs.LG

TL;DR: 通过引入隐私保护的知识融合策略和样本感知加权策略，扩展了现有的特征选择框架，以解决联邦学习中数据不平衡和隐私限制的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有特征选择方法难以捕捉复杂的特征交互并适应不同场景，且基于生成智能的方法存在置换敏感性和梯度搜索的凸性假设限制。此外，现实中的分布式场景面临数据不平衡、异构性和隐私限制等问题，现有框架难以整合跨客户端的特征选择知识。

Method: 提出了一种新框架，结合了置换不变嵌入和策略引导搜索，并扩展了该框架：1) 开发了一种隐私保护的知识融合策略，在不共享原始数据的情况下导出统一的表示空间；2) 结合了样本感知加权策略，以解决异构本地客户端之间的分布不平衡问题。

Result: 实验证明了该框架的有效性、鲁棒性和效率，并在联邦学习场景中展现了强大的泛化能力。

Conclusion: 该扩展框架能够有效应对联邦学习中的数据不平衡、异构性和隐私限制等挑战，实现跨客户端的知识融合和特征选择。

Abstract: Feature selection eliminates redundancy among features to improve downstream
task performance while reducing computational overhead. Existing methods often
struggle to capture intricate feature interactions and adapt across diverse
application scenarios. Recent advances employ generative intelligence to
alleviate these drawbacks. However, these methods remain constrained by
permutation sensitivity in embedding and reliance on convexity assumptions in
gradient-based search. To address these limitations, our initial work
introduces a novel framework that integrates permutation-invariant embedding
with policy-guided search. Although effective, it still left opportunities to
adapt to realistic distributed scenarios. In practice, data across local
clients is highly imbalanced, heterogeneous and constrained by strict privacy
regulations, limiting direct sharing. These challenges highlight the need for a
framework that can integrate feature selection knowledge across clients without
exposing sensitive information. In this extended journal version, we advance
the framework from two perspectives: 1) developing a privacy-preserving
knowledge fusion strategy to derive a unified representation space without
sharing sensitive raw data. 2) incorporating a sample-aware weighting strategy
to address distributional imbalance among heterogeneous local clients.
Extensive experiments validate the effectiveness, robustness, and efficiency of
our framework. The results further demonstrate its strong generalization
ability in federated learning scenarios. The code and data are publicly
available: https://anonymous.4open.science/r/FedCAPS-08BF.

</details>


### [316] [Critical attention scaling in long-context transformers](https://arxiv.org/abs/2510.05554)
*Shi Chen,Zhengjiang Lin,Yury Polyanskiy,Philippe Rigollet*

Main category: cs.LG

TL;DR: 模型中的秩崩溃现象可以通过注意力缩放来缓解，但缺乏理论依据。本文通过一个简化的模型分析了注意力缩放，发现存在一个临界缩放因子 $\beta_n 	o \log n$，该因子可以维持稀疏、内容自适应的注意力。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型在处理长上下文时，注意力机制存在注意力分数趋于均匀化（秩崩溃）的问题，导致模型性能下降。注意力缩放方法虽然有效，但缺乏理论支持。

Method: 通过一个简化的、可解析的模型来研究注意力缩放的效果，分析注意力分数随缩放因子 $\beta_n$ 变化的相变行为。

Result: 在简化的模型中，注意力表现出相变行为：缩放不足会导致所有词元坍缩到单一方向；缩放过度则使注意力退化为恒等变换。证明了临界缩放因子为 $\beta_n \asymp \log n$，这为 YaRN 和 Qwen 等模型中的注意力缩放提供了理论依据，并解释了为何对数缩放能在长上下文下保持稀疏、内容自适应的注意力。

Conclusion: 临界缩放因子 $\beta_n \asymp \log n$ 能够有效解决长上下文注意力中的秩崩溃问题，并解释了 YaRN 和 Qwen 等模型采用对数缩放的合理性，确保了在长上下文下的模型性能。

Abstract: As large language models scale to longer contexts, attention layers suffer
from a fundamental pathology: attention scores collapse toward uniformity as
context length $n$ increases, causing tokens to cluster excessively, a
phenomenon known as rank-collapse. While $\textit{attention scaling}$
effectively addresses this deficiency by rescaling attention scores with a
polylogarithmic factor $\beta_n$, theoretical justification for this approach
remains lacking.
  We analyze a simplified yet tractable model that magnifies the effect of
attention scaling. In this model, attention exhibits a phase transition
governed by the scaling factor $\beta_n$: insufficient scaling collapses all
tokens to a single direction, while excessive scaling reduces attention to
identity, thereby eliminating meaningful interactions between tokens. Our main
result identifies the critical scaling $\beta_n \asymp \log n$ and provides a
rigorous justification for attention scaling in YaRN and Qwen, clarifying why
logarithmic scaling maintains sparse, content-adaptive attention at large
context lengths.

</details>


### [317] [Generative Dynamic Graph Representation Learning for Conspiracy Spoofing Detection](https://arxiv.org/abs/2510.05562)
*Sheng Xiang,Yidong Jiang,Yunting Chen,Dawei Cheng,Guoping Zhao,Changjun Jiang*

Main category: cs.LG

TL;DR: 提出了一种名为生成动态图模型（GDGM）的新框架，用于检测金融交易中的共谋欺诈行为，该框架通过结合生成动态潜在空间、神经常微分方程和门控循环单元来捕捉动态交易行为和节点间关系的时空模式，并通过伪标签生成和异构聚合技术进一步增强检测性能，实验结果表明该方法优于现有最先进模型，并已成功应用于实际交易市场。


<details>
  <summary>Details</summary>
Motivation: 传统的机器学习方法在金融交易欺诈检测中，特别是共谋欺诈行为的识别上，存在不足，因为它们主要关注孤立节点的特征，忽略了节点间的广泛联系。图神经网络（GNNs）虽然有效利用了关系信息，但现有方法难以捕捉真实欺诈数据中动态、不规则且不断演变的节点间关系。

Method: 提出生成动态图模型（GDGM）框架。首先将原始交易数据转换为带时间戳的序列。然后，利用神经常微分方程（neural ordinary differential equations）和门控循环单元（gated recurrent units）对交易行为进行建模，以捕捉欺诈模式的时间动态性，生成包含时间动态的表示。此外，采用伪标签生成（pseudo-label generation）和异构聚合（heterogeneous aggregation）技术来收集相关信息，提升共谋欺诈行为的检测性能。

Result: 在欺诈检测数据集上的实验表明，GDGM方法在检测准确率上优于现有最先进的模型。此外，该欺诈检测系统已成功部署于全球最大的交易市场之一，验证了其在实际应用中的有效性和性能。

Conclusion: GDGM框架能够有效地捕捉动态交易行为和节点间关系的时空模式，在金融交易共谋欺诈检测方面表现出色，具有很高的实际应用价值。

Abstract: Spoofing detection in financial trading is crucial, especially for
identifying complex behaviors such as conspiracy spoofing. Traditional
machine-learning approaches primarily focus on isolated node features, often
overlooking the broader context of interconnected nodes. Graph-based
techniques, particularly Graph Neural Networks (GNNs), have advanced the field
by leveraging relational information effectively. However, in real-world
spoofing detection datasets, trading behaviors exhibit dynamic, irregular
patterns. Existing spoofing detection methods, though effective in some
scenarios, struggle to capture the complexity of dynamic and diverse, evolving
inter-node relationships. To address these challenges, we propose a novel
framework called the Generative Dynamic Graph Model (GDGM), which models
dynamic trading behaviors and the relationships among nodes to learn
representations for conspiracy spoofing detection. Specifically, our approach
incorporates the generative dynamic latent space to capture the temporal
patterns and evolving market conditions. Raw trading data is first converted
into time-stamped sequences. Then we model trading behaviors using the neural
ordinary differential equations and gated recurrent units, to generate the
representation incorporating temporal dynamics of spoofing patterns.
Furthermore, pseudo-label generation and heterogeneous aggregation techniques
are employed to gather relevant information and enhance the detection
performance for conspiratorial spoofing behaviors. Experiments conducted on
spoofing detection datasets demonstrate that our approach outperforms
state-of-the-art models in detection accuracy. Additionally, our spoofing
detection system has been successfully deployed in one of the largest global
trading markets, further validating the practical applicability and performance
of the proposed method.

</details>


### [318] [Efficient Learning-based Graph Simulation for Temporal Graphs](https://arxiv.org/abs/2510.05569)
*Sheng Xiang,Chenhao Xu,Dawei Cheng,Xiaoyang Wang,Ying Zhang*

Main category: cs.LG

TL;DR: 该论文提出了一种名为TGAE的高效学习方法来生成时间图快照，解决了现有时间图生成器效率低下的问题。


<details>
  <summary>Details</summary>
Motivation: 现有时间图生成器在处理演化图时，忽略了时间信息，并且在训练和生成方面效率低下，尤其是在基于时间随机游走的方法中。

Method: 提出了一种名为TGAE（Temporal Graph Autoencoder）的高效学习方法。该方法使用基于注意力机制的图编码器来编码时间图的结构和时间特征，并提出了一种能在模拟质量和效率之间取得良好平衡的自编码器解码器来生成图快照。

Result: 在真实和合成的时间图数据集上进行的实验评估表明，TGAE在模拟质量和效率方面优于最先进的时间图生成器。

Conclusion: TGAE是一种高效的学习方法，能够生成高质量的时间图快照，解决了现有方法的局限性。

Abstract: Graph simulation has recently received a surge of attention in graph
processing and analytics. In real-life applications, e.g. social science,
biology, and chemistry, many graphs are composed of a series of evolving graphs
(i.e., temporal graphs). While most of the existing graph generators focus on
static graphs, the temporal information of the graphs is ignored. In this
paper, we focus on simulating temporal graphs, which aim to reproduce the
structural and temporal properties of the observed real-life temporal graphs.
In this paper, we first give an overview of the existing temporal graph
generators, including recently emerged learning-based approaches. Most of these
learning-based methods suffer from one of the limitations: low efficiency in
training or slow generating, especially for temporal random walk-based methods.
Therefore, we propose an efficient learning-based approach to generate graph
snapshots, namely temporal graph autoencoder (TGAE). Specifically, we propose
an attention-based graph encoder to encode temporal and structural
characteristics on sampled ego-graphs. And we proposed an ego-graph decoder
that can achieve a good trade-off between simulation quality and efficiency in
temporal graph generation. Finally, the experimental evaluation is conducted
among our proposed TGAE and representative temporal graph generators on
real-life temporal graphs and synthesized graphs. It is reported that our
proposed approach outperforms the state-of-the-art temporal graph generators by
means of simulation quality and efficiency.

</details>


### [319] [Power Mechanism: Private Tabular Representation Release for Model Agnostic Consumption](https://arxiv.org/abs/2510.05581)
*Praneeth Vepakomma,Kaustubh Ponkshe*

Main category: cs.LG

TL;DR: 通过共享差分隐私嵌入来改进协作学习，减少通信和计算量。


<details>
  <summary>Details</summary>
Motivation: 传统协作学习依赖模型权重共享，存在效率和隐私问题；共享嵌入是更高效的方案，但缺乏隐私保护机制。

Method: 提出一种新方法，联合学习隐私编码网络和辅助生成网络，生成具有差分隐私保证的嵌入。这些嵌入随后被发送到服务器，服务器进行后处理以提高准确性。

Result: 与传统方法相比，该方法仅需一轮私有化通信，并减少了客户端的计算量。生成的私有化嵌入可用于服务器上的各种模型（深度学习、随机森林、XGBoost）。

Conclusion: 协同设计协作和隐私学习可以实现更高效、更安全的数据共享和模型训练。

Abstract: Traditional collaborative learning approaches are based on sharing of model
weights between clients and a server. However, there are advantages to resource
efficiency through schemes based on sharing of embeddings (activations) created
from the data. Several differentially private methods were developed for
sharing of weights while such mechanisms do not exist so far for sharing of
embeddings. We propose Ours to learn a privacy encoding network in conjunction
with a small utility generation network such that the final embeddings
generated from it are equipped with formal differential privacy guarantees.
These privatized embeddings are then shared with a more powerful server, that
learns a post-processing that results in a higher accuracy for machine learning
tasks. We show that our co-design of collaborative and private learning results
in requiring only one round of privatized communication and lesser compute on
the client than traditional methods. The privatized embeddings that we share
from the client are agnostic to the type of model (deep learning, random
forests or XGBoost) used on the server in order to process these activations to
complete a task.

</details>


### [320] [(Token-Level) \textbf{InfoRMIA}: Stronger Membership Inference and Memorization Assessment for LLMs](https://arxiv.org/abs/2510.05582)
*Jiashu Tao,Reza Shokri*

Main category: cs.LG

TL;DR: LLMs 存在严重的隐私风险，因为它们会“记住”训练数据。本文提出了一种名为 InfoRMIA 的新方法，可以更准确地量化这种风险，并在计算效率方面优于现有方法。此外，本文还提出了 token 级别的分析，可以更精确地定位泄露，为更有效的隐私保护措施（如精确的 unlearning）铺平了道路。


<details>
  <summary>Details</summary>
Motivation: 由于 LLM 训练数据量巨大，存在严重的隐私泄露风险，因此在发布 LLM 之前量化隐私风险至关重要。

Method: 提出了一种名为 InfoRMIA 的信息论方法，用于量化 LLM 的隐私风险。此外，还提出了一种 token 级别的分析方法，以更精确地定位泄露。

Result: InfoRMIA 在基准测试中持续优于 RMIA，并提高了计算效率。token 级别的分析能够更精确地定位泄露，并提高了 LLM 的序列级别推理能力。

Conclusion: 本文提出的 InfoRMIA 方法和 token 级别的分析方法为 LLM 的隐私保护提供了新的视角和更有效的解决方案。

Abstract: Machine learning models are known to leak sensitive information, as they
inevitably memorize (parts of) their training data. More alarmingly, large
language models (LLMs) are now trained on nearly all available data, which
amplifies the magnitude of information leakage and raises serious privacy
risks. Hence, it is more crucial than ever to quantify privacy risk before the
release of LLMs. The standard method to quantify privacy is via membership
inference attacks, where the state-of-the-art approach is the Robust Membership
Inference Attack (RMIA). In this paper, we present InfoRMIA, a principled
information-theoretic formulation of membership inference. Our method
consistently outperforms RMIA across benchmarks while also offering improved
computational efficiency.
  In the second part of the paper, we identify the limitations of treating
sequence-level membership inference as the gold standard for measuring leakage.
We propose a new perspective for studying membership and memorization in LLMs:
token-level signals and analyses. We show that a simple token-based InfoRMIA
can pinpoint which tokens are memorized within generated outputs, thereby
localizing leakage from the sequence level down to individual tokens, while
achieving stronger sequence-level inference power on LLMs. This new scope
rethinks privacy in LLMs and can lead to more targeted mitigation, such as
exact unlearning.

</details>


### [321] [Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising](https://arxiv.org/abs/2510.05589)
*Kangjia Yan,Chenxi Liu,Hao Miao,Xinle Wu,Yan Zhao,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: 该研究提出了TimePD，一个用于时间序列预测的无源域适应框架，利用大型语言模型（LLMs）和代理去噪来解决数据隐私问题，并在真实数据集上取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: 随着移动设备的普及，产生了海量的时序数据，而有效的时间序列预测对于各种实际应用至关重要。本研究关注的是无源域适应时间序列预测这一新问题，旨在适应预训练模型到稀疏目标时间序列域，同时不访问源数据，以遵守数据保护法规。

Method: TimePD框架包含三个关键组件：1) 双分支不变解耦特征学习，通过季节-趋势分解强制执行表示和梯度级别的不变性；2) 轻量级、无参数的代理去噪，动态校准LLMs的系统偏差；3) 知识蒸馏，双向对齐去噪预测和原始目标预测。

Result: 大量在真实世界数据集上的实验表明，TimePD的有效性，平均比最先进的基线提高了9.3%。

Conclusion: TimePD是首个具有代理去噪功能的无源时间序列预测框架，通过利用LLMs的泛化能力和提出的关键组件，有效解决了无源域适应问题，并在真实数据集上取得了优于现有方法的性能。

Abstract: The proliferation of mobile devices generates a massive volume of time series
across various domains, where effective time series forecasting enables a
variety of real-world applications. This study focuses on a new problem of
source-free domain adaptation for time series forecasting. It aims to adapt a
pretrained model from sufficient source time series to the sparse target time
series domain without access to the source data, embracing data protection
regulations. To achieve this, we propose TimePD, the first source-free time
series forecasting framework with proxy denoising, where large language models
(LLMs) are employed to benefit from their generalization capabilities.
Specifically, TimePD consists of three key components: (1) dual-branch
invariant disentangled feature learning that enforces representation- and
gradient-wise invariance by means of season-trend decomposition; (2)
lightweight, parameter-free proxy denoising that dynamically calibrates
systematic biases of LLMs; and (3) knowledge distillation that bidirectionally
aligns the denoised prediction and the original target prediction. Extensive
experiments on real-world datasets offer insight into the effectiveness of the
proposed TimePD, outperforming SOTA baselines by 9.3% on average.

</details>


### [322] [Riddled basin geometry sets fundamental limits to predictability and reproducibility in deep learning](https://arxiv.org/abs/2510.05606)
*Andrew Ly,Pulin Gong*

Main category: cs.LG

TL;DR: 深度学习的吸引域具有分形、充满漏洞的几何结构，导致其可预测性存在根本性限制，即使增加初始条件的精度也只能带来边际收益。


<details>
  <summary>Details</summary>
Motivation: 探讨深度学习可预测性的根本限制，这些限制源于其吸引域的分形、充满漏洞的几何结构。

Method: 通过分析深度学习中的混沌学习动力学和对称性诱导的不变子空间，推导出吸引域出现漏洞的充分条件，并将这些条件与深度网络中的实际情况联系起来。

Result: 证明了深度学习的吸引域具有无限精细的分形结构，其不确定性指数接近于零，这意味着即使初始条件精度大幅提高，结果的可预测性也只会得到边际改善。

Conclusion: 吸引域的漏洞性对神经网络训练的可预测性和可复现性构成了根本性限制，这为许多经验观察提供了统一的解释。该发现揭示了深度学习的一个普遍组织原则，对优化和人工智能的安全部署具有重要意义。

Abstract: Fundamental limits to predictability are central to our understanding of many
physical and computational systems. Here we show that, despite its remarkable
capabilities, deep learning exhibits such fundamental limits rooted in the
fractal, riddled geometry of its basins of attraction: any initialization that
leads to one solution lies arbitrarily close to another that leads to a
different one. We derive sufficient conditions for the emergence of riddled
basins by analytically linking features widely observed in deep learning,
including chaotic learning dynamics and symmetry-induced invariant subspaces,
to reveal a general route to riddling in realistic deep networks. The resulting
basins of attraction possess an infinitely fine-scale fractal structure
characterized by an uncertainty exponent near zero, so that even large
increases in the precision of initial conditions yield only marginal gains in
outcome predictability. Riddling thus imposes a fundamental limit on the
predictability and hence reproducibility of neural network training, providing
a unified account of many empirical observations. These results reveal a
general organizing principle of deep learning with important implications for
optimization and the safe deployment of artificial intelligence.

</details>


### [323] [NEO: No-Optimization Test-Time Adaptation through Latent Re-Centering](https://arxiv.org/abs/2510.05635)
*Alexander Murphy,Michal Danilowski,Soumyajit Chatterjee,Abhirup Ghosh*

Main category: cs.LG

TL;DR: NEO是一种无需超参数即可进行测试时自适应（TTA）的方法，通过将目标数据嵌入重新居中于原点来显著改善源域和分布偏移样本之间的对齐，并且计算成本低，在ImageNet-C上将ViT-Base的准确率从55.6%提高到59.2%（使用64个样本），在1024个样本时优于其他7种TTA方法，同时计算量最少，并在嵌入式设备上减少了推理时间和内存使用量。


<details>
  <summary>Details</summary>
Motivation: 现有的测试时自适应（TTA）方法计算成本高，需要大量数据，并且对超参数敏感。本文旨在解决这些问题，提出一种更高效、更灵活的TTA方法。

Method: 通过理论分析潜在空间的几何结构，提出一种将目标数据嵌入重新居中于原点的方法，以改善源域和分布偏移样本之间的对齐。基于此，开发了一种名为NEO的无需超参数的全自动TTA方法。

Result: NEO在ImageNet-C上使用64个样本时，将ViT-Base的分类准确率从55.6%提高到59.2%。在使用512个样本时，NEO在ImageNet-C、ImageNet-R和ImageNet-S上优于所有7种对比的TTA方法，在CIFAR-10-C上优于6/7种方法，且计算量最少。NEO在模型校准指标上表现良好，并且能够从1个类别适应到ImageNet-C中的999个其他类别。在Raspberry Pi和Jetson Orin Nano设备上，NEO将推理时间减少了63%，内存使用量减少了9%。

Conclusion: NEO是一种高效且有效的TTA方法，具有计算成本低、无需超参数、适应性强和在嵌入式设备上表现优异等优点。该方法基于对潜在空间几何的理解，为TTA领域带来了显著的改进。

Abstract: Test-Time Adaptation (TTA) methods are often computationally expensive,
require a large amount of data for effective adaptation, or are brittle to
hyperparameters. Based on a theoretical foundation of the geometry of the
latent space, we are able to significantly improve the alignment between source
and distribution-shifted samples by re-centering target data embeddings at the
origin. This insight motivates NEO -- a hyperparameter-free fully TTA method,
that adds no significant compute compared to vanilla inference. NEO is able to
improve the classification accuracy of ViT-Base on ImageNet-C from 55.6% to
59.2% after adapting on just one batch of 64 samples. When adapting on 512
samples NEO beats all 7 TTA methods we compare against on ImageNet-C,
ImageNet-R and ImageNet-S and beats 6/7 on CIFAR-10-C, while using the least
amount of compute. NEO performs well on model calibration metrics and
additionally is able to adapt from 1 class to improve accuracy on 999 other
classes in ImageNet-C. On Raspberry Pi and Jetson Orin Nano devices, NEO
reduces inference time by 63% and memory usage by 9% compared to baselines. Our
results based on 3 ViT architectures and 4 datasets show that NEO can be used
efficiently and effectively for TTA.

</details>


### [324] [Quantifying the Accuracy-Interpretability Trade-Off in Concept-Based Sidechannel Models](https://arxiv.org/abs/2510.05670)
*David Debot,Giuseppe Marra*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Concept Bottleneck Models (CBNMs) are deep learning models that provide
interpretability by enforcing a bottleneck layer where predictions are based
exclusively on human-understandable concepts. However, this constraint also
restricts information flow and often results in reduced predictive accuracy.
Concept Sidechannel Models (CSMs) address this limitation by introducing a
sidechannel that bypasses the bottleneck and carry additional task-relevant
information. While this improves accuracy, it simultaneously compromises
interpretability, as predictions may rely on uninterpretable representations
transmitted through sidechannels. Currently, there exists no principled
technique to control this fundamental trade-off. In this paper, we close this
gap. First, we present a unified probabilistic concept sidechannel meta-model
that subsumes existing CSMs as special cases. Building on this framework, we
introduce the Sidechannel Independence Score (SIS), a metric that quantifies a
CSM's reliance on its sidechannel by contrasting predictions made with and
without sidechannel information. We propose SIS regularization, which
explicitly penalizes sidechannel reliance to improve interpretability. Finally,
we analyze how the expressivity of the predictor and the reliance of the
sidechannel jointly shape interpretability, revealing inherent trade-offs
across different CSM architectures. Empirical results show that
state-of-the-art CSMs, when trained solely for accuracy, exhibit low
representation interpretability, and that SIS regularization substantially
improves their interpretability, intervenability, and the quality of learned
interpretable task predictors. Our work provides both theoretical and practical
tools for developing CSMs that balance accuracy and interpretability in a
principled manner.

</details>


### [325] [QGraphLIME - Explaining Quantum Graph Neural Networks](https://arxiv.org/abs/2510.05683)
*Haribandhu Jena,Jyotirmaya Shivottam,Subhankar Mishra*

Main category: cs.LG

TL;DR: QGraphLIME 是一种模型无关的后验框架，用于解释量子图神经网络。它将模型解释视为在图的结构保持扰动上拟合的局部代理的分布，并提供不确定性感知的节点和边重要性排名。该框架还提供了一个无分布的、有限样本的保证，可以通过 Dvoretzky-Kiefer-Wolfowitz 界来确保在标准独立性假设下，二元类概率的诱导分布得到均匀近似。实验表明，QGraphLIME 在解释量子图神经网络方面是准确、稳定且结构敏感的。


<details>
  <summary>Details</summary>
Motivation: 量子图神经网络的可解释性因测量引起的随机性和图结构的组合性质而变得复杂。需要一种方法来解决这个问题。

Method: 提出了一种名为 QGraphLIME 的模型无关的后验框架。该框架将模型解释视为在图的结构保持扰动上拟合的局部代理的分布。通过聚合代理归因及其分散性，QGraphLIME 为量子图模型提供不确定性感知的节点和边重要性排名。该框架还提供了一个无分布的、有限样本的保证，可以通过 Dvoretzky-Kiefer-Wolfowitz 界来确保在标准独立性假设下，二元类概率的诱导分布得到均匀近似。

Result: 在具有已知真实情况的可控合成图上进行的实证研究表明，QGraphLIME 能够提供准确且稳定的解释。消融研究显示了非线性代理建模的明显优势，并突出了对扰动设计的敏感性。

Conclusion: QGraphLIME 是一种原则性、不确定性感知且结构敏感的方法，用于解释量子图神经网络。它为扩展到更广泛的体系结构和真实数据集奠定了基础，并为量​​子资源的发展铺平了道路。

Abstract: Quantum graph neural networks offer a powerful paradigm for learning on
graph-structured data, yet their explainability is complicated by
measurement-induced stochasticity and the combinatorial nature of graph
structure. In this paper, we introduce QuantumGraphLIME (QGraphLIME), a
model-agnostic, post-hoc framework that treats model explanations as
distributions over local surrogates fit on structure-preserving perturbations
of a graph. By aggregating surrogate attributions together with their
dispersion, QGraphLIME yields uncertainty-aware node and edge importance
rankings for quantum graph models. The framework further provides a
distribution-free, finite-sample guarantee on the size of the surrogate
ensemble: a Dvoretzky-Kiefer-Wolfowitz bound ensures uniform approximation of
the induced distribution of a binary class probability at target accuracy and
confidence under standard independence assumptions. Empirical studies on
controlled synthetic graphs with known ground truth demonstrate accurate and
stable explanations, with ablations showing clear benefits of nonlinear
surrogate modeling and highlighting sensitivity to perturbation design.
Collectively, these results establish a principled, uncertainty-aware, and
structure-sensitive approach to explaining quantum graph neural networks, and
lay the groundwork for scaling to broader architectures and real-world
datasets, as quantum resources mature. Code is available at
https://github.com/smlab-niser/qglime.

</details>


### [326] [vAttention: Verified Sparse Attention](https://arxiv.org/abs/2510.05688)
*Aditya Desai,Kumar Krishna Agrawal,Shuo Yang,Alejandro Cuadron,Luis Gaspar Schroeder,Matei Zaharia,Joseph E. Gonzalez,Ion Stoica*

Main category: cs.LG

TL;DR: vAttention是首个具有可验证的近似保证的稀疏注意力机制，它结合了top-k和采样方法的优点，在保证准确性的同时显著提高了效率。


<details>
  <summary>Details</summary>
Motivation: 现有的稀疏注意力方法（如top-k和采样）在近似完全注意力方面存在局限性，无法提供跨头和跨查询向量的一致性近似，并且缺乏近似质量的保证。

Method: vAttention结合了top-k和随机采样方法的优势，并利用采样的统计保证，提供了用户指定的 $(\epsilon, \delta)$ 近似精度保证，实现了首个具有可验证保证的实用稀疏注意力机制。

Result: vAttention在实践中表现优于单独的top-k和采样方法，实现了更好的质量-效率权衡。实验表明，vAttention显著提高了稀疏注意力的质量（例如，在RULER-HARD上，Llama-3.1-8B-Inst和Deepseek-R1-Distill-Llama-8B分别提高了约4.5个百分点），并有效缩小了完全注意力和稀疏注意力之间的差距（例如，在不同数据集上，稀疏度高达20倍时可匹配完全模型的质量）。此外，在推理场景下，vAttention在AIME2024上实现了10倍稀疏度和高达32K的token生成，同时保持了与完全模型相当的质量。

Conclusion: vAttention通过结合现有方法的优点并提供可验证的近似保证，克服了现有稀疏注意力方法的局限性，是实现大规模实用、可靠的稀疏注意力部署的重要一步。

Abstract: State-of-the-art sparse attention methods for reducing decoding latency fall
into two main categories: approximate top-$k$ (and its extension, top-$p$) and
recently introduced sampling-based estimation. However, these approaches are
fundamentally limited in their ability to approximate full attention: they fail
to provide consistent approximations across heads and query vectors and, most
critically, lack guarantees on approximation quality, limiting their practical
deployment. We observe that top-$k$ and random sampling are complementary:
top-$k$ performs well when attention scores are dominated by a few tokens,
whereas random sampling provides better estimates when attention scores are
relatively uniform. Building on this insight and leveraging the statistical
guarantees of sampling, we introduce vAttention, the first practical sparse
attention mechanism with user-specified $(\epsilon, \delta)$ guarantees on
approximation accuracy (thus, verified). These guarantees make vAttention a
compelling step toward practical, reliable deployment of sparse attention at
scale. By unifying top-k and sampling, vAttention outperforms both
individually, delivering a superior quality-efficiency trade-off. Our
experiments show that vAttention significantly improves the quality of sparse
attention (e.g., $\sim$4.5 percentage points for Llama-3.1-8B-Inst and
Deepseek-R1-Distill-Llama-8B on RULER-HARD), and effectively bridges the gap
between full and sparse attention (e.g., across datasets, it matches full model
quality with upto 20x sparsity). We also demonstrate that it can be deployed in
reasoning scenarios to achieve fast decoding without compromising model quality
(e.g., vAttention achieves full model quality on AIME2024 at 10x sparsity with
up to 32K token generations). Code is open-sourced at
https://github.com/xAlg-ai/sparse-attention-hub.

</details>


### [327] [Primal-Dual Direct Preference Optimization for Constrained LLM Alignment](https://arxiv.org/abs/2510.05703)
*Yihan Du,Seo Taek Kong,R. Srikant*

Main category: cs.LG

TL;DR: 本研究提出了一种新的约束对齐方法，通过双重DPO（Primal-Dual DPO）来提高LLM的安全性，同时减少了计算和内存成本，并且不需要先验知识。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全对齐方法存在高计算和内存成本，或需要先验知识的问题。

Method: 提出了一种新的Primal-Dual DPO方法，首先使用标准DPO训练模型提供奖励信息，然后利用奖励信息和成本偏好数据进行拉格朗日DPO目标微调。

Result: 在PKU-SafeRLHF数据集上进行了实验，证明了该方法在降低成本和满足约束方面的有效性，并提供了理论保证。

Conclusion: 所提出的Primal-Dual DPO方法能够有效地对LLM进行约束对齐，显著降低了成本，并且不需要先验知识。

Abstract: The widespread application of Large Language Models (LLMs) imposes increasing
demands on safety, such as reducing harmful content and fake information, and
avoiding certain forbidden tokens due to rules and laws. While there have been
several recent works studying safe alignment of LLMs, these works either
require the training of reward and cost models and incur high memory and
computational costs, or need prior knowledge about the optimal solution.
Motivated by this fact, we study the problem of constrained alignment in LLMs,
i.e., maximizing the output reward while restricting the cost due to
potentially unsafe content to stay below a threshold. For this problem, we
propose a novel primal-dual DPO approach, which first trains a model using
standard DPO on reward preference data to provide reward information, and then
adopts a rearranged Lagrangian DPO objective utilizing the provided reward
information to fine-tune LLMs on cost preference data. Our approach
significantly reduces memory and computational costs, and does not require
extra prior knowledge. Moreover, we establish rigorous theoretical guarantees
on the suboptimality and constraint violation of the output policy. We also
extend our approach to an online data setting by incorporating exploration
bonuses, which enables our approach to explore uncovered prompt-response space,
and then provide theoretical results that get rid of the dependence on
preference data coverage. Experimental results on the widely-used preference
dataset PKU-SafeRLHF demonstrate the effectiveness of our approach.

</details>


### [328] [DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities](https://arxiv.org/abs/2510.05717)
*Hedi Zisling,Ilan Naiman,Nimrod Berman,Supasorn Suwajanakorn,Omri Azencot*

Main category: cs.LG

TL;DR: 提出了一种名为DiffSDA的新型框架，用于无监督的序列解耦，它结合了潜在扩散和高效采样器，并能在文本、视频和音频等多种真实世界数据上有效运行，同时提出了一个新的评估协议，并在实验中证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 无监督表征学习（特别是序列解耦）旨在分离数据中的静态和动态变化因素，但现有方法（如VAE和GAN）通常涉及复杂的优化过程，并且在真实世界数据上的应用和评估仍面临挑战。此外，扩散模型在序列解耦方面的理论应用尚未建立。

Method: 提出了一种名为Diffusion Sequential Disentanglement Autoencoder (DiffSDA) 的新型框架，该框架利用新的概率建模、潜在扩散和高效采样器，并结合了一个具有挑战性的评估协议。

Result: 在各种真实世界的基准测试中，DiffSDA在序列解耦方面优于最近最先进的方法。

Conclusion: DiffSDA是一个新颖的、与模态无关的框架，在多种真实世界数据模态（包括时间序列、视频和音频）上有效，并在序列解耦任务上取得了最先进的性能。

Abstract: Unsupervised representation learning, particularly sequential
disentanglement, aims to separate static and dynamic factors of variation in
data without relying on labels. This remains a challenging problem, as existing
approaches based on variational autoencoders and generative adversarial
networks often rely on multiple loss terms, complicating the optimization
process. Furthermore, sequential disentanglement methods face challenges when
applied to real-world data, and there is currently no established evaluation
protocol for assessing their performance in such settings. Recently, diffusion
models have emerged as state-of-the-art generative models, but no theoretical
formalization exists for their application to sequential disentanglement. In
this work, we introduce the Diffusion Sequential Disentanglement Autoencoder
(DiffSDA), a novel, modal-agnostic framework effective across diverse
real-world data modalities, including time series, video, and audio. DiffSDA
leverages a new probabilistic modeling, latent diffusion, and efficient
samplers, while incorporating a challenging evaluation protocol for rigorous
testing. Our experiments on diverse real-world benchmarks demonstrate that
DiffSDA outperforms recent state-of-the-art methods in sequential
disentanglement.

</details>


### [329] [Neighborhood-Adaptive Generalized Linear Graph Embedding with Latent Pattern Mining](https://arxiv.org/abs/2510.05719)
*S. Peng,L. Hu,W. Zhang,B. Jie,Y. Luo*

Main category: cs.LG

TL;DR: 提出了NGLGE模型，通过自适应学习邻域和低秩表示来解决现有图嵌入方法的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有图嵌入方法需要预定义邻域大小，并且基于线性投影的方法适应性较差。

Method: 提出邻域自适应广义线性图嵌入（NGLGE）模型，该模型基于潜在模式挖掘，引入自适应图学习方法，并利用重建的低秩表示和L2,0范数约束。

Result: 在不同场景的数据集上进行比较评估，证明了该模型优于现有最先进的方法。

Conclusion: NGLGE模型在图嵌入领域具有优越性能。

Abstract: Graph embedding has been widely applied in areas such as network analysis,
social network mining, recommendation systems, and bioinformatics. However,
current graph construction methods often require the prior definition of
neighborhood size, limiting the effective revelation of potential structural
correlations in the data. Additionally, graph embedding methods using linear
projection heavily rely on a singular pattern mining approach, resulting in
relative weaknesses in adapting to different scenarios. To address these
challenges, we propose a novel model, Neighborhood-Adaptive Generalized Linear
Graph Embedding (NGLGE), grounded in latent pattern mining. This model
introduces an adaptive graph learning method tailored to the neighborhood,
effectively revealing intrinsic data correlations. Simultaneously, leveraging a
reconstructed low-rank representation and imposing $\ell_{2,0}$ norm constraint
on the projection matrix allows for flexible exploration of additional pattern
information. Besides, an efficient iterative solving algorithm is derived for
the proposed model. Comparative evaluations on datasets from diverse scenarios
demonstrate the superior performance of our model compared to state-of-the-art
methods.

</details>


### [330] [Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies](https://arxiv.org/abs/2510.05725)
*Chunsan Hong,Seonho An,Min-Soo Kim,Jong Chul Ye*

Main category: cs.LG

TL;DR: MDMs通过学习到的调度器来优化句子生成，克服了基于规则的调度器的局限性，并在多个基准测试中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: MDMs在句子生成中，其性能对下一步取消掩码的位置选择非常敏感。现有的基于规则的启发式方法（如最大置信度、最大间隔）仅能提供临时的改进。

Method: 将去噪过程构建为一个KL正则化的马尔可夫决策过程（MDP），并引入一个显式的参考策略来优化一个正则化目标，该目标在标准假设下具有策略改进和收敛保证。

Result: 在四个基准测试中，学习到的策略始终优于最大置信度方法。特别是在SUDOKU任务中，相较于随机方法，性能提升了20.1%，相较于最大置信度方法，性能提升了11.2%。

Conclusion: 所提出的学习调度器框架能够生成更接近数据分布的样本，优于启发式调度器。

Abstract: Masked diffusion models (MDMs) have recently emerged as a novel framework for
language modeling. MDMs generate sentences by iteratively denoising masked
sequences, filling in [MASK] tokens step by step. Although MDMs support
any-order sampling, performance is highly sensitive to the choice of which
position to unmask next. Prior work typically relies on rule-based schedules
(e.g., max-confidence, max-margin), which provide ad hoc improvements. In
contrast, we replace these heuristics with a learned scheduler. Specifically,
we cast denoising as a KL-regularized Markov decision process (MDP) with an
explicit reference policy and optimize a regularized objective that admits
policy improvement and convergence guarantees under standard assumptions. We
prove that the optimized policy under this framework generates samples that
more closely match the data distribution than heuristic schedules. Empirically,
across four benchmarks, our learned policy consistently outperforms
max-confidence: for example, on SUDOKU, where unmasking order is critical, it
yields a 20.1% gain over random and a 11.2% gain over max-confidence.

</details>


### [331] [Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches](https://arxiv.org/abs/2510.05748)
*Hachem Madmoun,Salem Lahlou*

Main category: cs.LG

TL;DR: 在多智能体大模型系统中，直接通信比课程学习更能有效促进合作。


<details>
  <summary>Details</summary>
Motivation: 多智能体大模型系统的合作是AI对齐的关键。

Method: 研究了两种方法：直接通信和课程学习。通过四方“鹿 the Stag Hunt”游戏测试了“廉价”的单字通信渠道；通过迭代“公共物品游戏与惩罚”游戏测试了课程学习。

Result: 单字通信渠道将合作率从0%提高到48.3%。然而，课程学习（通过逐步复杂的游戏）将智能体收益降低了27.4%，并可能导致“习得性悲观”。

Conclusion: 对于协调问题，简单的通信协议可能比基于经验的训练更可靠。课程设计需要仔细考虑游戏序列中嵌入的战略教训。

Abstract: Eliciting cooperation in multi-agent LLM systems is critical for AI
alignment. We investigate two approaches: direct communication and curriculum
learning. In a 4-player Stag Hunt, a one-word "cheap talk" channel increases
cooperation from 0% to 48.3%, demonstrating communication as a robust
coordination mechanism. In contrast, we find that curriculum learning is highly
sensitive to design choices: our pedagogical curriculum through progressively
complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game
with Punishment. Qualitative analysis reveals that curricula emphasizing
defection-equilibrium games can induce "learned pessimism" in agents. These
findings suggest that for coordination problems, simple communication protocols
may be more reliable than experience-based training, and that curriculum design
for social dilemmas requires careful attention to the strategic lessons
embedded in game sequences.

</details>


### [332] [Are Heterogeneous Graph Neural Networks Truly Effective? A Causal Perspective](https://arxiv.org/abs/2510.05750)
*Xiao Yang,Xuejiao Zhao,Zhiqi Shen*

Main category: cs.LG

TL;DR: HGNNs 的有效性主要归因于异构信息，而非模型架构。


<details>
  <summary>Details</summary>
Motivation: 探究 HGNNs 在节点分类任务中有效性的内在原因，区分模型架构和异构信息各自的作用。

Method: 通过在 21 个数据集和 20 个基线模型上进行系统性复现，并结合因果效应估计框架（包括事实和反事实分析），来评估模型架构和异构信息对 HGNNs 性能的因果效应。

Result: 模型架构和复杂性对 HGNNs 的性能没有因果效应；异构信息通过增加同质性和局部-全局分布差异，对性能有正向因果效应，使节点类别更易区分。

Conclusion: HGNNs 的性能提升主要源于异构信息，而非模型架构的改进。异构信息能够增强节点的区分度，从而提升节点分类的效果。

Abstract: Graph neural networks (GNNs) have achieved remarkable success in node
classification. Building on this progress, heterogeneous graph neural networks
(HGNNs) integrate relation types and node and edge semantics to leverage
heterogeneous information. Causal analysis for HGNNs is advancing rapidly,
aiming to separate genuine causal effects from spurious correlations. However,
whether HGNNs are intrinsically effective remains underexamined, and most
studies implicitly assume rather than establish this effectiveness. In this
work, we examine HGNNs from two perspectives: model architecture and
heterogeneous information. We conduct a systematic reproduction across 21
datasets and 20 baselines, complemented by comprehensive hyperparameter
retuning. To further disentangle the source of performance gains, we develop a
causal effect estimation framework that constructs and evaluates candidate
factors under standard assumptions through factual and counterfactual analyses,
with robustness validated via minimal sufficient adjustment sets, cross-method
consistency checks, and sensitivity analyses. Our results lead to two
conclusions. First, model architecture and complexity have no causal effect on
performance. Second, heterogeneous information exerts a positive causal effect
by increasing homophily and local-global distribution discrepancy, which makes
node classes more distinguishable. The implementation is publicly available at
https://github.com/YXNTU/CausalHGNN.

</details>


### [333] [Empirical Comparison of Membership Inference Attacks in Deep Transfer Learning](https://arxiv.org/abs/2510.05753)
*Yuxuan Bai,Gauri Pradhan,Marlon Tobaben,Antti Honkela*

Main category: cs.LG

TL;DR: 在迁移学习模型中，不同的成员推断攻击（MIAs）在隐私风险评估方面表现各异，没有一种攻击能完全捕捉所有风险，但LiRA在大多数情况下表现最佳，IHA在高数据量下对PatchCamelyon数据集微调的模型更有效。


<details>
  <summary>Details</summary>
Motivation: 评估迁移学习设置下多种成员推断攻击（MIAs）的有效性，以帮助实践者识别用于隐私风险评估的最有效攻击。

Method: 比较了多种MIAs在迁移学习设置下的性能。

Result: 研究发现，对于基于分数的MIAs，攻击效果随训练数据量的增加而降低。LiRA在大多数实验场景中表现出优越的性能，而IHA在PatchCamelyon数据集上进行微调且数据量较大的情况下更有效。

Conclusion: 没有一种MIAs能够完全捕捉迁移学习模型中的所有隐私风险，实践者在评估隐私风险时应考虑多种攻击方法，并根据具体情况选择最合适的攻击。

Abstract: With the emergence of powerful large-scale foundation models, the training
paradigm is increasingly shifting from from-scratch training to transfer
learning. This enables high utility training with small, domain-specific
datasets typical in sensitive applications.Membership inference attacks (MIAs)
provide an empirical estimate of the privacy leakage by machine learning
models. Yet, prior assessments of MIAs against models fine-tuned with transfer
learning rely on a small subset of possible attacks. We address this by
comparing performance of diverse MIAs in transfer learning settings to help
practitioners identify the most efficient attacks for privacy risk evaluation.
We find that attack efficacy decreases with the increase in training data for
score-based MIAs. We find that there is no one MIA which captures all privacy
risks in models trained with transfer learning. While the Likelihood Ratio
Attack (LiRA) demonstrates superior performance across most experimental
scenarios, the Inverse Hessian Attack (IHA) proves to be more effective against
models fine-tuned on PatchCamelyon dataset in high data regime.

</details>


### [334] [DP-SNP-TIHMM: Differentially Private, Time-Inhomogeneous Hidden Markov Models for Synthesizing Genome-Wide Association Datasets](https://arxiv.org/abs/2510.05777)
*Shadi Rahimian,Mario Fritz*

Main category: cs.LG

TL;DR: 提出了一种使用时间非齐次隐马尔可夫模型（TIHMM）生成合成SNP序列数据集的创新框架，以解决SNP数据集的隐私风险问题。


<details>
  <summary>Details</summary>
Motivation: SNP数据集在遗传学研究中至关重要，但共享时存在重大的隐私风险，现有方法存在复杂性和局限性。

Method: 提出了一种创新的框架，使用时间非齐次隐马尔可夫模型（TIHMM）生成合成SNP序列数据集。通过确保每个SNP序列在训练中只产生有限的影响，实现了差分隐私保证。通过直接处理完整的SNP序列并限制其梯度贡献，解决了SNP序列固有的相关性带来的隐私风险。

Result: 在真实世界的1000 Genomes数据集上进行了实验，在隐私预算为 ε ∈ [1, 10] 且 δ=10^-4 的情况下，证明了该方法在生成统计特性与非私有数据集相似的合成数据集方面的有效性。通过使HMM的转移模型依赖于序列中的位置，显著提高了性能。

Conclusion: 该框架能够私有化共享基因组数据，同时为研究人员提供卓越的灵活性和实用性。

Abstract: Single nucleotide polymorphism (SNP) datasets are fundamental to genetic
studies but pose significant privacy risks when shared. The correlation of SNPs
with each other makes strong adversarial attacks such as masked-value
reconstruction, kin, and membership inference attacks possible. Existing
privacy-preserving approaches either apply differential privacy to statistical
summaries of these datasets or offer complex methods that require
post-processing and the usage of a publicly available dataset to suppress or
selectively share SNPs.
  In this study, we introduce an innovative framework for generating synthetic
SNP sequence datasets using samples derived from time-inhomogeneous hidden
Markov models (TIHMMs). To preserve the privacy of the training data, we ensure
that each SNP sequence contributes only a bounded influence during training,
enabling strong differential privacy guarantees. Crucially, by operating on
full SNP sequences and bounding their gradient contributions, our method
directly addresses the privacy risks introduced by their inherent correlations.
  Through experiments conducted on the real-world 1000 Genomes dataset, we
demonstrate the efficacy of our method using privacy budgets of $\varepsilon
\in [1, 10]$ at $\delta=10^{-4}$. Notably, by allowing the transition models of
the HMM to be dependent on the location in the sequence, we significantly
enhance performance, enabling the synthetic datasets to closely replicate the
statistical properties of non-private datasets. This framework facilitates the
private sharing of genomic data while offering researchers exceptional
flexibility and utility.

</details>


### [335] [Improving Clinical Dataset Condensation with Mode Connectivity-based Trajectory Surrogates](https://arxiv.org/abs/2510.05805)
*Pafue Christy Nganjimi,Andrew Soltan,Danielle Belgrave,Lei Clifton,David A. Clifton,Anshul Thakur*

Main category: cs.LG

TL;DR: 通过使用二次贝塞尔曲线连接初始和最终模型状态来替代密集的随机梯度下降（SGD）轨迹，提出了一种新的数据集凝聚（DC）方法，以提高训练稳定性和效率。


<details>
  <summary>Details</summary>
Motivation: 现有的数据集凝聚（DC）方法通常使用密集的随机梯度下降（SGD）轨迹作为监督信号，但这些轨迹可能存在噪声、曲率高、存储量大等问题，导致训练不稳定、收敛缓慢和内存开销大。

Method: 提出使用平滑的、低损耗的参数化代理（二次贝塞尔曲线）来连接真实训练轨迹的初始和最终模型状态，以此替代密集的SGD轨迹。

Result: 所提出的方法在五个临床数据集上进行了评估，结果表明其在数据集凝聚方面优于现有的最先进方法，生成的凝聚数据集能够支持有效的临床模型开发。

Conclusion: 二次贝塞尔曲线可以作为SGD路径的有效替代品，用于监督数据凝聚，从而实现更稳定、更高效的训练过程。

Abstract: Dataset condensation (DC) enables the creation of compact, privacy-preserving
synthetic datasets that can match the utility of real patient records,
supporting democratised access to highly regulated clinical data for developing
downstream clinical models. State-of-the-art DC methods supervise synthetic
data by aligning the training dynamics of models trained on real and those
trained on synthetic data, typically using full stochastic gradient descent
(SGD) trajectories as alignment targets; however, these trajectories are often
noisy, high-curvature, and storage-intensive, leading to unstable gradients,
slow convergence, and substantial memory overhead. We address these limitations
by replacing full SGD trajectories with smooth, low-loss parametric surrogates,
specifically quadratic B\'ezier curves that connect the initial and final model
states from real training trajectories. These mode-connected paths provide
noise-free, low-curvature supervision signals that stabilise gradients,
accelerate convergence, and eliminate the need for dense trajectory storage. We
theoretically justify B\'ezier-mode connections as effective surrogates for SGD
paths and empirically show that the proposed method outperforms
state-of-the-art condensation approaches across five clinical datasets,
yielding condensed datasets that enable clinically effective model development.

</details>


### [336] [Mitigating Premature Exploitation in Particle-based Monte Carlo for Inference-Time Scaling](https://arxiv.org/abs/2510.05825)
*Giorgio Giannone,Guangxuan Xu,Nikhil Shivakumar Nayak,Rohan Mahesh Awhad,Shivchander Sudalairaj,Kai Xu,Akash Srivastava*

Main category: cs.LG

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Inference-Time Scaling (ITS) improves language models by allocating more
computation at generation time. Particle Filtering (PF) has emerged as a strong
ITS method for complex mathematical reasoning tasks, but it is vulnerable when
guided by process reward models, which often assign overconfident scores early
in the reasoning process. This causes PF to suffer from premature exploitation:
it myopically commits to locally promising trajectories, prunes potentially
correct hypotheses, and converges to suboptimal solutions. This failure mode,
known as particle impoverishment, is especially severe under constrained
computational budgets. To address this, we analyze the problem and identify two
root causes: a lack of diversity in the particle set due to overconfident
resampling and consequent inability to assess the potential of a reasoning
path. We introduce Entropic Particle Filtering (ePF), an algorithm that
integrates two new techniques to solve these issues. The first technique,
Entropic Annealing (EA), directly mitigates particle impoverishment by
monitoring search diversity via entropy; when diversity drops, it intervenes by
dynamically annealing the resampling distribution to preserve exploration. The
second, an enhancement called Look-ahead Modulation (LaM), adds a predictive
guide to evaluate a state's potential based on its successors. On several
challenging math benchmarks, ePF significantly outperforms strong baselines and
achieves up to a 50 % relative improvement in task reward. Together, these
methods improve PF's resilience by balancing the exploration of diverse
solution spaces with the exploitation of high-reward regions, ultimately
leading to higher-quality solutions.

</details>


### [337] [Multimodal Trajectory Representation Learning for Travel Time Estimation](https://arxiv.org/abs/2510.05840)
*Zhi Liu,Xuyuan Hu,Xiao Han,Zhehao Dai,Zhaolin Deng,Guojiang Shen,Xiangjie Kong*

Main category: cs.LG

TL;DR: MDTI框架通过整合GPS轨迹、网格轨迹和路网约束，并采用动态轨迹建模和自监督预训练，提高了交通行程时间估计的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的交通行程时间估计方法在处理异构数据源和复杂交通动态时面临挑战，并且将轨迹转换为固定长度表示会丢失信息或产生冗余特征。

Method: MDTI框架使用特定模态的编码器和跨模态交互模块来整合GPS序列、网格轨迹和路网约束，并通过动态轨迹建模机制自适应地调整信息密度，同时利用对比学习和掩码语言模型进行自监督预训练。

Result: 在三个真实世界数据集上的广泛实验表明，MDTI框架的性能持续优于最先进的基线方法，证明了其鲁棒性和强大的泛化能力。

Conclusion: MDTI框架能够有效地整合多模态轨迹信息，并自适应地处理不同长度的轨迹，从而显著提高交通行程时间估计的准确性，优于现有方法。

Abstract: Accurate travel time estimation (TTE) plays a crucial role in intelligent
transportation systems. However, it remains challenging due to heterogeneous
data sources and complex traffic dynamics. Moreover, conventional approaches
typically convert trajectories into fixed-length representations, neglecting
the inherent variability of real-world trajectories, which often leads to
information loss or feature redundancy. To address these challenges, this paper
introduces the Multimodal Dynamic Trajectory Integration (MDTI) framework--a
novel multimodal trajectory representation learning approach that integrates
GPS sequences, grid trajectories, and road network constraints to enhance TTE
accuracy. MDTI employs modality-specific encoders and a cross-modal interaction
module to capture complementary spatial, temporal, and topological semantics,
while a dynamic trajectory modeling mechanism adaptively regulates information
density for trajectories of varying lengths. Two self-supervised pretraining
objectives, named contrastive alignment and masked language modeling, further
strengthen multimodal consistency and contextual understanding. Extensive
experiments on three real-world datasets demonstrate that MDTI consistently
outperforms state-of-the-art baselines, confirming its robustness and strong
generalization abilities. The code is publicly available at:
https://github.com/freshhxy/MDTI/

</details>


### [338] [ESS-Flow: Training-free guidance of flow-based models as inference in source space](https://arxiv.org/abs/2510.05849)
*Adhithyan Kalaivanan,Zheng Zhao,Jens Sjölund,Fredrik Lindsten*

Main category: cs.LG

TL;DR: ESS-Flow是一种无梯度方法，通过在源空间中进行贝叶斯推断来指导预训练的流模型进行条件生成。


<details>
  <summary>Details</summary>
Motivation: 在没有配对数据的情况下，需要一种方法来指导预训练的流模型进行条件生成或生成具有所需目标属性的样本，以解决各种任务。

Method: ESS-Flow 利用流模型中通常的高斯先验，通过椭圆切片采样在源空间中进行贝叶斯推断。该方法仅需要通过生成模型和观测过程的正向传递，不需要梯度或雅可比计算。

Result: ESS-Flow 在设计具有所需目标属性的材料和根据稀疏的残基间距离测量预测蛋白质结构方面被证明是有效的。

Conclusion: ESS-Flow 是一种有效的、无梯度的条件生成方法，适用于梯度不可靠或不可用的情况。

Abstract: Guiding pretrained flow-based generative models for conditional generation or
to produce samples with desired target properties enables solving diverse tasks
without retraining on paired data. We present ESS-Flow, a gradient-free method
that leverages the typically Gaussian prior of the source distribution in
flow-based models to perform Bayesian inference directly in the source space
using Elliptical Slice Sampling. ESS-Flow only requires forward passes through
the generative model and observation process, no gradient or Jacobian
computations, and is applicable even when gradients are unreliable or
unavailable, such as with simulation-based observations or quantization in the
generation or observation process. We demonstrate its effectiveness on
designing materials with desired target properties and predicting protein
structures from sparse inter-residue distance measurements.

</details>


### [339] [How to model Human Actions distribution with Event Sequence Data](https://arxiv.org/abs/2510.05856)
*Egor Surkov,Dmitry Osin,Evgeny Burnaev,Egor Shvetsov*

Main category: cs.LG

TL;DR: 本篇论文研究人类行为序列中未来事件的分布预测，挑战了传统的自回归范式，并提出了一种基于KL散度量化时间漂移的度量方法。研究发现，显式的分布预测目标优于复杂的隐式基线模型，并且模式崩溃主要是由分布不平衡驱动的。


<details>
  <summary>Details</summary>
Motivation: 研究人类行为序列的未来事件分布预测，以及探索显式分布预测或顺序无关的多标记方法是否优于保留顺序的方法。

Method: 分析局部顺序不变性，并引入基于KL散度的度量来量化时间漂移。

Result: 显式的分布预测目标始终优于复杂的隐式基线。模式崩溃主要由预测类别的分布不平衡引起。

Conclusion: 提出了一种选择建模策略的原则性框架，并为构建更准确、更鲁棒的预测系统提供了实际指导。

Abstract: This paper studies forecasting of the future distribution of events in human
action sequences, a task essential in domains like retail, finance, healthcare,
and recommendation systems where the precise temporal order is often less
critical than the set of outcomes. We challenge the dominant autoregressive
paradigm and investigate whether explicitly modeling the future distribution or
order-invariant multi-token approaches outperform order-preserving methods. We
analyze local order invariance and introduce a KL-based metric to quantify
temporal drift. We find that a simple explicit distribution forecasting
objective consistently surpasses complex implicit baselines. We further
demonstrate that mode collapse of predicted categories is primarily driven by
distributional imbalance. This work provides a principled framework for
selecting modeling strategies and offers practical guidance for building more
accurate and robust forecasting systems.

</details>


### [340] [MaNGO - Adaptable Graph Network Simulators via Meta-Learning](https://arxiv.org/abs/2510.05874)
*Philipp Dahlinger,Tai Hoang,Denis Blessing,Niklas Freymuth,Gerhard Neumann*

Main category: cs.LG

TL;DR: Meta-learning enables GNS to adapt to new physical parameters without retraining by learning a shared latent structure.


<details>
  <summary>Details</summary>
Motivation: Traditional mesh-based simulations are precise but computationally expensive and require physical parameters. Data-driven GNSs are faster but must be retrained for parameter variations and require labor-intensive data collection.

Method: A novel architecture using conditional neural processes (CNPs) to encode graph trajectories into a latent representation, combined with a neural operator architecture to mitigate error accumulation. This meta-learning approach allows fast adaptation to new physical parameters.

Result: The proposed Meta Neural Graph Operator (MaNGO) demonstrates superior performance over existing GNS methods on dynamics prediction tasks with varying material properties. It achieves accuracy close to an oracle model on unseen material properties.

Conclusion: MaNGO effectively addresses the limitations of traditional GNS by leveraging meta-learning to adapt to new physical parameters quickly and efficiently, achieving high accuracy.

Abstract: Accurately simulating physics is crucial across scientific domains, with
applications spanning from robotics to materials science. While traditional
mesh-based simulations are precise, they are often computationally expensive
and require knowledge of physical parameters, such as material properties. In
contrast, data-driven approaches like Graph Network Simulators (GNSs) offer
faster inference but suffer from two key limitations: Firstly, they must be
retrained from scratch for even minor variations in physical parameters, and
secondly they require labor-intensive data collection for each new parameter
setting. This is inefficient, as simulations with varying parameters often
share a common underlying latent structure. In this work, we address these
challenges by learning this shared structure through meta-learning, enabling
fast adaptation to new physical parameters without retraining. To this end, we
propose a novel architecture that generates a latent representation by encoding
graph trajectories using conditional neural processes (CNPs). To mitigate error
accumulation over time, we combine CNPs with a novel neural operator
architecture. We validate our approach, Meta Neural Graph Operator (MaNGO), on
several dynamics prediction tasks with varying material properties,
demonstrating superior performance over existing GNS methods. Notably, MaNGO
achieves accuracy on unseen material properties close to that of an oracle
model.

</details>


### [341] [OBSR: Open Benchmark for Spatial Representations](https://arxiv.org/abs/2510.05879)
*Julia Moska,Oleksii Furman,Kacper Kozaczko,Szymon Leszkiewicz,Jakub Polczyk,Piotr Gramacki,Piotr Szymański*

Main category: cs.LG

TL;DR: 该研究提出了一个多任务、跨模态的地理空间嵌入模型基准，以解决现有地理空间人工智能评估的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有的地理空间人工智能基准集中于单一任务和单一模态，限制了该领域的发展。需要一个标准化的、多任务的、跨模态的基准来系统地评估地理空间嵌入模型的性能。

Method: 设计了一个跨模态的基准，包含来自三大洲不同城市的7个不同数据集，并建立了简单的、面向任务的模型基线。

Result: 该基准能够评估地理空间嵌入模型在各种具有地理过程现象上的表现、准确性和效率，并为比较复杂解决方案提供了一个基准。

Conclusion: 提出的基准通过使用跨模态、多任务的数据集，并建立基线模型，为地理空间人工智能嵌入模型的评估提供了一个更全面、更标准化的平台。

Abstract: GeoAI is evolving rapidly, fueled by diverse geospatial datasets like traffic
patterns, environmental data, and crowdsourced OpenStreetMap (OSM) information.
While sophisticated AI models are being developed, existing benchmarks are
often concentrated on single tasks and restricted to a single modality. As
such, progress in GeoAI is limited by the lack of a standardized, multi-task,
modality-agnostic benchmark for their systematic evaluation. This paper
introduces a novel benchmark designed to assess the performance, accuracy, and
efficiency of geospatial embedders. Our benchmark is modality-agnostic and
comprises 7 distinct datasets from diverse cities across three continents,
ensuring generalizability and mitigating demographic biases. It allows for the
evaluation of GeoAI embedders on various phenomena that exhibit underlying
geographic processes. Furthermore, we establish a simple and intuitive
task-oriented model baselines, providing a crucial reference point for
comparing more complex solutions.

</details>


### [342] [Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods](https://arxiv.org/abs/2510.05901)
*Martin Benfeghoul,Teresa Delgado,Adnan Oomerjee,Haitham Bou Ammar,Jun Wang,Zafeirios Fountas*

Main category: cs.LG

TL;DR: 现有的混合线性化方法存在问题，我们提出了三种解决方案，以确保组件的平衡使用，同时保持计算效率并恢复大部分基础模型性能。


<details>
  <summary>Details</summary>
Motivation: Transformers 的二次计算复杂度限制了其可扩展性。尽管线性注意力将复杂度降至线性，但从头开始预训练此类模型仍然成本高昂。最近的训练后线性化方法可以有效地将预训练的 Transformer 转换为线性模型，通常使用结合了线性注意力和滑动窗口 softmax 的混合方法。然而，我们发现现有混合方法会绕过线性组件，几乎完全依赖滑动窗口注意力。

Method: 我们提出了三种解决方案：(i) 将仅线性转换与滑动窗口 softmax 进行推理时间混合；(ii) HedgeCATs，结合了注意力权重转移和目标 LoRA 微调；(iii) 计划性滑动窗口 Dropout (SSD)，在训练期间随机抑制 softmax 分支以防止组件崩溃。

Result: 我们的方法在保持计算效率的同时，恢复了大部分基础模型性能，并确保了真正的线性注意力采用。

Conclusion: 我们发现现有混合方法存在一个关键缺陷：它们无意中绕过了线性组件，并且几乎完全依赖滑动窗口注意力。这是由于在常识性基准测试中被忽视的评估实践。我们提出的三种方法可以解决这个问题，确保组件的平衡使用，同时保持计算效率并恢复大部分基础模型性能，从而恢复混合转换中性能归因的有效性。

Abstract: Transformers' quadratic computational complexity limits their scalability
despite remarkable performance. While linear attention reduces this to linear
complexity, pre-training such models from scratch remains, in most cases,
prohibitively expensive. Recent post-training linearisation methods convert
pre-trained Transformers to linear models efficiently, often using hybrid
approaches that combine linear attention with sliding-window softmax. We
identify a critical flaw: existing hybrid methods inadvertently bypass the
linear component, relying almost entirely on SWA. Component-level diagnostics
reveal this previously undetected behaviour stems from overlooked evaluation
practices on common-sense benchmarks. We propose three solutions to ensure
balanced component usage: (i) inference-time hybridisation of linear-only
conversions with sliding-window softmax; (ii) HedgeCATs, combining
attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled
Sliding-window Dropout (SSD), which stochastically suppresses the softmax
branch during training to prevent component collapse. Our methods maintain
computational efficiency while recovering most base model performance and
ensuring genuine linear attention adoption, restoring the validity of
performance attributions in hybrid conversions.

</details>


### [343] [An Attention-Augmented VAE-BiLSTM Framework for Anomaly Detection in 12-Lead ECG Signals](https://arxiv.org/abs/2510.05919)
*Marc Garreta Basora,Mehmet Oguz Mulayim*

Main category: cs.LG

TL;DR: 本文比较了三种基于自动编码器的架构（CAE、VAE-BiLSTM 和 VAE-BiLSTM-MHA）在 12 导联心电图（ECG）异常检测中的性能，其中 VAE-BiLSTM-MHA 是首次应用于此任务。实验结果表明，VAE-BiLSTM-MHA 在 CPSC 数据集上表现最佳，AUPRC 为 0.81，召回率为 0.85，并已集成到可视化异常定位的交互式仪表板中。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病的识别依赖于对 12 导联心电图（ECG）进行异常检测，以识别与疾病相关的偏差。

Method: 本文提出并比较了三种自动编码器架构：卷积自动编码器（CAE）、带双向长短期记忆的变分自动编码器（VAE-BiLSTM）以及带多头注意力的 VAE-BiLSTM（VAE-BiLSTM-MHA），用于 ECG 的无监督异常检测。所有模型均在正常 ECG 样本上进行训练，以重建非异常心 morphology 并检测指示疾病的偏差。

Result: 在统一的预处理和评估流程下，使用公开的 CPSC 数据集，注意力增强的 VAE 取得了最佳性能，在保留的测试集上 AUPRC 为 0.81，召回率为 0.85，优于其他架构。

Conclusion: VAE-BiLSTM-MHA 架构在 ECG 异常检测中表现出最佳性能，并且可以集成到可视化异常定位的仪表板中，以支持临床分诊。

Abstract: Anomaly detection in 12-lead electrocardiograms (ECGs) is critical for
identifying deviations associated with cardiovascular disease. This work
presents a comparative analysis of three autoencoder-based architectures:
convolutional autoencoder (CAE), variational autoencoder with bidirectional
long short-term memory (VAE-BiLSTM), and VAE-BiLSTM with multi-head attention
(VAE-BiLSTM-MHA), for unsupervised anomaly detection in ECGs. To the best of
our knowledge, this study reports the first application of a VAE-BiLSTM-MHA
architecture to ECG anomaly detection. All models are trained on normal ECG
samples to reconstruct non-anomalous cardiac morphology and detect deviations
indicative of disease. Using a unified preprocessing and evaluation pipeline on
the public China Physiological Signal Challenge (CPSC) dataset, the
attention-augmented VAE achieves the best performance, with an AUPRC of 0.81
and a recall of 0.85 on the held-out test set, outperforming the other
architectures. To support clinical triage, this model is further integrated
into an interactive dashboard that visualizes anomaly localization. In
addition, a performance comparison with baseline models from the literature is
provided.

</details>


### [344] [Carré du champ flow matching: better quality-generalisation tradeoff in generative models](https://arxiv.org/abs/2510.05930)
*Jacob Bamberger,Iolo Jones,Dennis Duncan,Michael M. Bronstein,Pierre Vandergheynst,Adam Gosztolai*

Main category: cs.LG

TL;DR: CDC-FM通过引入几何噪声来改善生成模型的质量-泛化权衡，解决了高样本质量与记忆化之间的冲突，并在多种数据集和模型上展示了优越性能。


<details>
  <summary>Details</summary>
Motivation: 深度生成模型在追求高样本质量时，常面临记忆化（复制训练数据而非泛化）的问题。

Method: 提出了一种名为Carré du champ flow matching (CDC-FM) 的方法，通过引入几何噪声来正则化概率路径，以改善质量-泛化权衡。该方法使用空间变化的、各向异性的高斯噪声，其协方差能捕捉潜在数据流形的局部几何结构，并证明了该噪声可从数据中优化估计且易于扩展。

Result: 在多种数据集（合成流形、点云、单细胞基因组学、动物运动捕捉和图像）和模型架构（MLPs、CNNs和Transformers）上进行了广泛的实验评估，结果表明CDC-FM在质量-泛化权衡方面持续表现更优。在数据稀疏和高度非均匀采样的数据集上，CDC-FM相比标准FM有显著改进。

Conclusion: CDC-FM提供了一个研究数据几何、泛化和记忆化在生成模型中相互作用的数学框架，并提供了一个可扩展且鲁棒的算法，可轻松集成到现有的流匹配流程中，尤其适用于AI for Science应用中的数据稀疏和非均匀采样场景。

Abstract: Deep generative models often face a fundamental tradeoff: high sample quality
can come at the cost of memorisation, where the model reproduces training data
rather than generalising across the underlying data geometry. We introduce
Carr\'e du champ flow matching (CDC-FM), a generalisation of flow matching
(FM), that improves the quality-generalisation tradeoff by regularising the
probability path with a geometry-aware noise. Our method replaces the
homogeneous, isotropic noise in FM with a spatially varying, anisotropic
Gaussian noise whose covariance captures the local geometry of the latent data
manifold. We prove that this geometric noise can be optimally estimated from
the data and is scalable to large data. Further, we provide an extensive
experimental evaluation on diverse datasets (synthetic manifolds, point clouds,
single-cell genomics, animal motion capture, and images) as well as various
neural network architectures (MLPs, CNNs, and transformers). We demonstrate
that CDC-FM consistently offers a better quality-generalisation tradeoff. We
observe significant improvements over standard FM in data-scarce regimes and in
highly non-uniformly sampled datasets, which are often encountered in AI for
science applications. Our work provides a mathematical framework for studying
the interplay between data geometry, generalisation and memorisation in
generative models, as well as a robust and scalable algorithm that can be
readily integrated into existing flow matching pipelines.

</details>


### [345] [LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection](https://arxiv.org/abs/2510.05935)
*Mohamed Bal-Ghaoui,Fayssal Sabri*

Main category: cs.LG

TL;DR: LLM-FS-Agent是一个创新的多智能体框架，通过智能体间的“辩论”来进行可解释、鲁棒的特征选择，在网络安全领域表现出色，显著减少了训练时间并保持了分类性能。


<details>
  <summary>Details</summary>
Motivation: 高维数据在机器学习中普遍存在，影响模型的可解释性和计算效率。现有的基于LLM的特征选择方法缺乏结构化推理和透明度。

Method: LLM-FS-Agent采用多智能体架构，让扮演不同角色的LLM智能体进行“辩论”，共同评估特征相关性并提供详细的理由。

Result: 在网络安全领域的CIC-DIAD 2024 IoT入侵检测数据集上，LLM-FS-Agent的分类性能优于或媲美基线方法（如LLM-Select和PCA），同时下游训练时间平均减少了46%（XGBoost的p=0.028，具有统计学意义）。

Conclusion: LLM-FS-Agent的体系结构提高了决策透明度和计算效率，是解决现实世界应用中高维数据挑战的实用且可靠的方案。

Abstract: High-dimensional data remains a pervasive challenge in machine learning,
often undermining model interpretability and computational efficiency. While
Large Language Models (LLMs) have shown promise for dimensionality reduction
through feature selection, existing LLM-based approaches frequently lack
structured reasoning and transparent justification for their decisions. This
paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for
interpretable and robust feature selection. The system orchestrates a
deliberative "debate" among multiple LLM agents, each assigned a specific role,
enabling collective evaluation of feature relevance and generation of detailed
justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the
CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance
against strong baselines, including LLM-Select and traditional methods such as
PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves
superior or comparable classification performance while reducing downstream
training time by an average of 46% (statistically significant improvement, p =
0.028 for XGBoost). These findings highlight that the proposed deliberative
architecture enhances both decision transparency and computational efficiency,
establishing LLM-FS-Agent as a practical and reliable solution for real-world
applications.

</details>


### [346] [Gaussian Embeddings: How JEPAs Secretly Learn Your Data Density](https://arxiv.org/abs/2510.05949)
*Randall Balestriero,Nicolas Ballas,Mike Rabbat,Yann LeCun*

Main category: cs.LG

TL;DR: JEPAs 学习到的表示可以用于估计数据密度，通过其反崩溃项实现，该项被证明可以估计数据密度。任何成功训练的 JEPA 都可以用来获得样本概率，用于数据管理、异常值检测或密度估计。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在揭示 JEPAs 中的反崩溃项不仅仅是为了防止表示崩溃，实际上它还能够估计数据密度。

Method: 通过理论分析和实验验证，证明 JEPAs 的反崩溃项可以估计数据密度，并提出了一种名为 JEPA-SCORE 的方法，利用模型的雅可比矩阵来计算样本的对数概率。

Result: 证明了 JEPAs 的反崩溃项可以估计数据密度，并且提出的 JEPA-SCORE 方法在各种数据集（合成、受控和 ImageNet）和不同 JEPAs 模型（I-JEPA、DINOv2、MetaCLIP）上都得到了实证验证。

Conclusion: JEPAs 的反崩溃项具有估计数据密度的能力，这为 JEPAs 的应用开辟了新的可能性，例如数据管理、异常值检测和密度估计。

Abstract: Joint Embedding Predictive Architectures (JEPAs) learn representations able
to solve numerous downstream tasks out-of-the-box. JEPAs combine two
objectives: (i) a latent-space prediction term, i.e., the representation of a
slightly perturbed sample must be predictable from the original sample's
representation, and (ii) an anti-collapse term, i.e., not all samples should
have the same representation. While (ii) is often considered as an obvious
remedy to representation collapse, we uncover that JEPAs' anti-collapse term
does much more--it provably estimates the data density. In short, any
successfully trained JEPA can be used to get sample probabilities, e.g., for
data curation, outlier detection, or simply for density estimation. Our
theoretical finding is agnostic of the dataset and architecture used--in any
case one can compute the learned probabilities of sample $x$ efficiently and in
closed-form using the model's Jacobian matrix at $x$. Our findings are
empirically validated across datasets (synthetic, controlled, and Imagenet) and
across different Self Supervised Learning methods falling under the JEPA family
(I-JEPA and DINOv2) and on multimodal models, such as MetaCLIP. We denote the
method extracting the JEPA learned density as {\bf JEPA-SCORE}.

</details>


### [347] [Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs](https://arxiv.org/abs/2510.05987)
*Xueyan Li,Guinan Su,Mrinmaya Sachan,Jonas Geiping*

Main category: cs.LG

TL;DR: LLMs在处理复杂推理任务时，需要平衡探索多样性与保证解的质量。现有方法在增加探索性或提高可靠性上存在冲突。本文提出解码规则应基于正确性而非置信度进行校准，并通过Greedy-Threshold、Calibrated-TopK和Calibrated-epsilon等策略实现，提升了模型在数学和通用推理基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在复杂推理任务中，为了探索多种推理路径，需要注入随机性，但同时又要保证每条路径的准确性和质量，这导致了两个相互竞争的目标。而现有方法在提高探索性（如增加temperature）或提高可靠性（如拒绝低置信度样本）时，未能区分不同来源的不确定性，存在冲突。

Method: 提出解码规则应由正确性而非置信度校准，即在正确性估计较高的token上采样，在正确性估计较低处减少采样。具体策略包括：1. Greedy-Threshold：在置信度非常低时，强制进行贪婪采样。2. Calibrated-TopK和Calibrated-epsilon：基于估计的逐级正确性来设置TopK或epsilon截断阈值。

Result: 提出的方法在数学和通用推理基准上均取得了性能提升，挑战了现有的关于在不确定性下解码的启发式方法。

Conclusion: 解码规则应基于正确性而非置信度进行校准，可以有效解决LLM在复杂推理任务中探索与可靠性之间的矛盾，并提升模型性能。

Abstract: Large Language Models (LLMs) are increasingly applied to complex tasks that
require extended reasoning. In such settings, models often benefit from diverse
chains-of-thought to arrive at multiple candidate solutions. This requires two
competing objectives: to inject enough stochasticity to explore multiple
reasoning chains, and to ensure sufficient accuracy and quality in each path.
Existing works pursue the first objective by increasing exploration at highly
uncertain steps with higher temperature or larger candidate token sets, while
others improve reliability by rejecting samples with low confidence
post-generation, implying that low confidence correlates with low answer
quality. These two lines of thought are in conflict, as they conflate different
sources of uncertainty. To resolve this, we argue that the decoding rule should
be calibrated by correctness, not confidence alone. We should sample from
tokens with higher estimated correctness, and reduce sampling where expected
correctness is low. We propose simple strategies that achieve this goal:
Greedy-Threshold makes sampling greedy at very low confidence steps.
Calibrated-TopK and Calibrated-epsilon set truncation threshold based on
estimated rank-wise correctness. Together, our findings challenge prevailing
heuristics about decoding under uncertainty and show gains across math and
general reasoning benchmarks.

</details>


### [348] [Uncertainty in Machine Learning](https://arxiv.org/abs/2510.06007)
*Hans Weytjens,Wouter Verbeke*

Main category: cs.LG

TL;DR: 本书介绍了机器学习中不确定性量化的原理和应用，涵盖了不确定性类型识别、预测模型量化方法（如线性回归、随机森林、神经网络）以及置信区间生成框架（如保角预测）。最后探讨了不确定性估计在业务决策、模型可靠性和风险感知策略中的应用。


<details>
  <summary>Details</summary>
Motivation: 介绍机器学习中不确定性量化的原理和实际应用，重点在于如何识别和区分不同类型的不确定性，并量化其在预测模型中的不确定性。

Method: 提出并讨论了在线性回归、随机森林和神经网络等预测模型中量化不确定性的方法，并介绍了保角预测作为生成具有预定义置信区间的预测的框架。

Result: 不确定性估计可用于改进业务决策，增强模型可靠性，并支持风险感知策略。

Conclusion: 不确定性量化在机器学习中至关重要，能够通过提供置信区间和支持风险管理来提高模型性能和决策的可靠性。

Abstract: This book chapter introduces the principles and practical applications of
uncertainty quantification in machine learning. It explains how to identify and
distinguish between different types of uncertainty and presents methods for
quantifying uncertainty in predictive models, including linear regression,
random forests, and neural networks. The chapter also covers conformal
prediction as a framework for generating predictions with predefined confidence
intervals. Finally, it explores how uncertainty estimation can be leveraged to
improve business decision-making, enhance model reliability, and support
risk-aware strategies.

</details>


### [349] [RamPINN: Recovering Raman Spectra From Coherent Anti-Stokes Spectra Using Embedded Physics](https://arxiv.org/abs/2510.06020)
*Sai Karthikeya Vemuri,Adithya Ashok Chalain Valapil,Tim Büchner,Joachim Denzler*

Main category: cs.LG

TL;DR: 该研究提出了一种名为RamPINN的物理信息神经网络，通过利用Kramers-Kronig因果关系和一种平滑先验，从噪声的CARS测量中恢复拉曼光谱，即使在合成数据上训练也能很好地泛化到真实数据，且无需真实拉曼光谱即可获得有竞争力的结果。


<details>
  <summary>Details</summary>
Motivation: 深度学习在科学领域的应用受限于大规模训练数据集的缺乏。本研究旨在利用科学理论（物理定律）作为归纳偏置，解决数据稀疏问题。

Method: 提出了一种名为RamPINN的物理信息神经网络，采用双解码器架构，通过可微分希尔伯特变换损失强制执行Kramers-Kronig因果关系来分离共振和非共振信号，并对非共振信号施加平滑先验。

Result: RamPINN在合成数据上训练后，能很好地泛化到真实世界实验数据，显著优于现有基线。仅使用基于物理的损失进行训练，无需任何真实拉曼光谱，也能获得有竞争力的结果。

Conclusion: 正式的科学规则可以作为强大的归纳偏置，在数据受限的科学领域实现鲁棒的自监督学习。

Abstract: Transferring the recent advancements in deep learning into scientific
disciplines is hindered by the lack of the required large-scale datasets for
training. We argue that in these knowledge-rich domains, the established body
of scientific theory provides reliable inductive biases in the form of
governing physical laws. We address the ill-posed inverse problem of recovering
Raman spectra from noisy Coherent Anti-Stokes Raman Scattering (CARS)
measurements, as the true Raman signal here is suppressed by a dominating
non-resonant background. We propose RamPINN, a model that learns to recover
Raman spectra from given CARS spectra. Our core methodological contribution is
a physics-informed neural network that utilizes a dual-decoder architecture to
disentangle resonant and non-resonant signals. This is done by enforcing the
Kramers-Kronig causality relations via a differentiable Hilbert transform loss
on the resonant and a smoothness prior on the non-resonant part of the signal.
Trained entirely on synthetic data, RamPINN demonstrates strong zero-shot
generalization to real-world experimental data, explicitly closing this gap and
significantly outperforming existing baselines. Furthermore, we show that
training with these physics-based losses alone, without access to any
ground-truth Raman spectra, still yields competitive results. This work
highlights a broader concept: formal scientific rules can act as a potent
inductive bias, enabling robust, self-supervised learning in data-limited
scientific domains.

</details>


### [350] [Out-of-Distribution Detection from Small Training Sets using Bayesian Neural Network Classifiers](https://arxiv.org/abs/2510.06025)
*Kevin Raina,Tanya Schmah*

Main category: cs.LG

TL;DR: 贝叶斯神经网络在有限数据下比确定性方法在OOD检测上表现更好。


<details>
  <summary>Details</summary>
Motivation: OOD检测对于AI的可靠性和安全至关重要，但在实际中训练数据通常有限。贝叶斯神经网络（BNNs）能够明确表示模型不确定性，因此在OOD检测中具有潜力，尤其是在训练数据稀疏的情况下，BNNs可以结合先验模型信息。

Method: 提出了一种基于期望logit向量的新的贝叶斯后验OOD评分方法，并对5种贝叶斯和4种确定性后验OOD评分方法进行了比较。

Result: 在MNIST和CIFAR-10数据集上，使用少于5000个训练样本的实验表明，贝叶斯方法在OOD检测上的表现优于相应的确定性方法。

Conclusion: 贝叶斯方法在有限的训练数据条件下，在OOD检测任务上优于确定性方法。

Abstract: Out-of-Distribution (OOD) detection is critical to AI reliability and safety,
yet in many practical settings, only a limited amount of training data is
available. Bayesian Neural Networks (BNNs) are a promising class of model on
which to base OOD detection, because they explicitly represent epistemic (i.e.
model) uncertainty. In the small training data regime, BNNs are especially
valuable because they can incorporate prior model information. We introduce a
new family of Bayesian posthoc OOD scores based on expected logit vectors, and
compare 5 Bayesian and 4 deterministic posthoc OOD scores. Experiments on MNIST
and CIFAR-10 In-Distributions, with 5000 training samples or less, show that
the Bayesian methods outperform corresponding deterministic methods.

</details>


### [351] [Generalization of Gibbs and Langevin Monte Carlo Algorithms in the Interpolation Regime](https://arxiv.org/abs/2510.06028)
*Andreas Maurer,Erfan Mirzaei,Massimiliano Pontil*

Main category: cs.LG

TL;DR: Gibbs算法在过参数化插值模型中具有数据依赖的泛化界限，且该界限对 Langevin Monte Carlo 算法近似稳定。实验证明该界限在真实数据和随机标签数据上均有意义。


<details>
  <summary>Details</summary>
Motivation: 在过参数化插值模型（包括随机标签数据）中，为 Gibbs 算法提供数据依赖的泛化界限，并探索其在 Langevin Monte Carlo 算法近似下的稳定性。

Method: 推导 Gibbs 算法的泛化界限，并验证其在 MNIST 和 CIFAR-10 数据集上的有效性，同时考察其对 Langevin Monte Carlo 算法近似的稳定性。

Result: 实验表明，该泛化界限在真实标签数据上能给出有意义的预测，在随机标签数据上能有效地给出上界。研究发现，低温柔插值模型下的泛化能力可以通过高温模型下的训练误差来预测。

Conclusion: 在过参数化插值模型中，Gibbs 算法的泛化能力可以通过高温模型的训练误差来预测，并且该泛化界限对 Langevin Monte Carlo 算法的近似是稳定的。

Abstract: The paper provides data-dependent bounds on the test error of the Gibbs
algorithm in the overparameterized interpolation regime, where low training
errors are also obtained for impossible data, such as random labels in
classification. The bounds are stable under approximation with Langevin Monte
Carlo algorithms. Experiments on the MNIST and CIFAR-10 datasets verify that
the bounds yield nontrivial predictions on true labeled data and correctly
upper bound the test error for random labels. Our method indicates that
generalization in the low-temperature, interpolation regime is already signaled
by small training errors in the more classical high temperature regime.

</details>


### [352] [Fast Leave-One-Out Approximation from Fragment-Target Prevalence Vectors (molFTP) : From Dummy Masking to Key-LOO for Leakage-Free Feature Construction](https://arxiv.org/abs/2510.06029)
*Guillaume Godin*

Main category: cs.LG

TL;DR: molFTP是一种新的分子表示方法，它通过一种称为“dummy masking”或“key-LOO”的技术，实现了快速、防泄漏的特征向量化，并且在不牺牲模型性能的情况下，大大降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 为了解决在分子研究中，由于特征泄露导致的交叉验证评估偏差问题，以及提高模型训练效率。

Method: 提出了一种名为molFTP（molecular fragment-target prevalence）的紧凑表示方法，并设计了一种“dummy masking”程序来防止特征泄露。同时，研究了“key leave-one-out”（key-loo）方法，以近似模拟真实分子级别的留一法（LOO）交叉验证。

Result: molFTP在交叉验证中表现出强大的预测性能，并且通过“dummy masking”或“key-LOO”技术，有效避免了特征泄露。研究发现key-loo方法能够近似真实的LOO，偏差低于8%，从而可以在接近全数据训练的情况下获得无偏的模型性能估计。

Conclusion: molFTP提供了一种快速、防泄漏的分子特征向量化方法，并结合dummy masking或key-LOO等实用保护措施，以较低的计算成本实现了接近LOO的交叉验证效果。

Abstract: We introduce molFTP (molecular fragment-target prevalence), a compact
representation that delivers strong predictive performance. To prevent feature
leakage across cross-validation folds, we implement a dummy-masking procedure
that removes information about fragments present in the held-out molecules. We
further show that key leave-one-out (key-loo) closely approximates true
molecule-level leave-one-out (LOO), with deviation below 8% on our datasets.
This enables near full data training while preserving unbiased cross-validation
estimates of model performance. Overall, molFTP provides a fast,
leakage-resistant fragment-target prevalence vectorization with practical
safeguards (dummy masking or key-LOO) that approximate LOO at a fraction of its
cost.

</details>


### [353] [From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning](https://arxiv.org/abs/2510.06038)
*Li Zeqiao,Wang Yijing,Wang Haoyu,Li Zheng,Li Peng,Liu Wenfei,Zuo Zhiqiang*

Main category: cs.LG

TL;DR: 该研究提出了一种名为H-DSAC的奖励函数无关的主动人机协同强化学习方法，用于解决真实世界自动驾驶的挑战。


<details>
  <summary>Details</summary>
Motivation: 在真实世界中应用强化学习（RL）面临安全、高效和鲁棒性方面的挑战。将人类专业知识融入学习过程有助于克服这些挑战，减少风险探索并提高样本效率。

Method: 提出了一种名为H-DSAC（Human-Guided Distributional Soft Actor-Critic）的方法，结合了代理价值传播（PVP）和分布软Actor-Critic（DSAC）。该方法的核心创新在于DSAC框架内构建分布式代理价值函数，该函数通过为专家演示分配更高的预期回报并惩罚需要人类干预的操作来编码人类意图，并通过将这些标签外推到未标记状态来有效地引导策略学习专家行为。

Result: 在模拟和真实世界实验中，该框架实现了安全、鲁棒且样本高效的自动驾驶策略学习，并且在实际可行的训练时间内完成了学习。

Conclusion: H-DSAC方法能够有效地实现自动驾驶策略的学习，具有安全、鲁棒和样本高效的特点。

Abstract: Autonomous driving with reinforcement learning (RL) has significant
potential. However, applying RL in real-world settings remains challenging due
to the need for safe, efficient, and robust learning. Incorporating human
expertise into the learning process can help overcome these challenges by
reducing risky exploration and improving sample efficiency. In this work, we
propose a reward-free, active human-in-the-loop learning method called
Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines
Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to
enable efficient and safe training in real-world environments. The key
innovation is the construction of a distributed proxy value function within the
DSAC framework. This function encodes human intent by assigning higher expected
returns to expert demonstrations and penalizing actions that require human
intervention. By extrapolating these labels to unlabeled states, the policy is
effectively guided toward expert-like behavior. With a well-designed state
space, our method achieves real-world driving policy learning within practical
training times. Results from both simulation and real-world experiments
demonstrate that our framework enables safe, robust, and sample-efficient
learning for autonomous driving.

</details>


### [354] [BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining](https://arxiv.org/abs/2510.06048)
*Jie Hao,Rui Yu,Wei Zhang,Huixia Wang,Jie Xu,Mingrui Liu*

Main category: cs.LG

TL;DR: BLISS是一种新的轻量级数据选择方法，无需外部模型即可从头开始选择数据，并考虑了长期影响，在LLM预训练中实现了1.7倍的加速和更优的性能。


<details>
  <summary>Details</summary>
Motivation: 现有数据选择方法依赖外部模型，难以区分数据选择和外部模型的影响，并且忽略了数据对模型收敛后的长期影响。

Method: BLISS是一种双层优化方法，使用代理模型和评分模型来估计训练样本的长期影响，并将数据选择制定为优化问题。

Result: 通过在C4数据集上预训练Pythia和LLaMA模型，BLISS在1B模型设置下实现了1.7倍的加速，并在多个下游任务上表现出更优的性能。

Conclusion: BLISS是一种有效的数据选择方法，可以提高LLM预训练的效率和泛化能力。

Abstract: Effective data selection is essential for pretraining large language models
(LLMs), enhancing efficiency and improving generalization to downstream tasks.
However, existing approaches often require leveraging external pretrained
models, making it difficult to disentangle the effects of data selection from
those of the external pretrained models. In addition, they often overlook the
long-term impact of selected data if the model is trained to convergence,
primarily due to the prohibitive cost of full-scale LLM pretraining. In this
paper, we introduce BLISS (\textbf{B}ileve\textbf{L} \textbf{I}nfluence
\textbf{S}coring method for data \textbf{S}election): a lightweight data
selection method that operates entirely \emph{from scratch}, without relying on
any external pretrained oracle models, while explicitly accounting for the
long-term impact of selected data. BLISS leverages a small proxy model as a
surrogate for the LLM and employs a score model to estimate the long-term
influence of training samples if the proxy model is trained to convergence. We
formulate data selection as a bilevel optimization problem, where the
upper-level objective optimizes the score model to assign importance weights to
training samples, ensuring that minimizing the lower-level objective (i.e.,
training the proxy model over the weighted training loss until convergence)
leads to best validation performance. Once optimized, the trained score model
predicts influence scores for the dataset, enabling efficient selection of
high-quality samples for LLM pretraining. We validate BLISS by pretraining
410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4
dataset. Notably, under the 1B model setting, BLISS achieves $1.7\times$
speedup in reaching the same performance as the state-of-the-art method,
demonstrating superior performance across multiple downstream tasks.

</details>


### [355] [Edit-Based Flow Matching for Temporal Point Processes](https://arxiv.org/abs/2510.06050)
*David Lüdke,Marten Lienen,Marcel Kollovieh,Stephan Günnemann*

Main category: cs.LG

TL;DR: 本文提出了一种名为 Edit Flow 的新型时间点过程（TPP）模型，通过插入、删除和替换等编辑操作将噪声转化为数据，从而克服了传统自回归模型的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有时间点过程（TPP）模型多采用自回归参数化，存在采样效率低的问题。虽然已有非自回归扩散模型取得一定进展，但仍有改进空间。

Method: 提出了一种基于连续时间马尔可夫链的 Edit Flow 模型，通过学习瞬时编辑速率来实现噪声到数据的转换，并允许插入、删除和替换等编辑操作。

Result: 实验结果表明，该模型在多种无条件和条件生成任务上都表现出色，并且能够有效地减少生成过程中所需的编辑操作总数，展示了其生成灵活性。

Conclusion: Edit Flow 模型为 TPP 提供了一种灵活且高效的建模新视角，通过学习编辑操作的速率，有效提高了生成效率和灵活性。

Abstract: Temporal point processes (TPPs) are a fundamental tool for modeling event
sequences in continuous time, but most existing approaches rely on
autoregressive parameterizations that are limited by their sequential sampling.
Recent non-autoregressive, diffusion-style models mitigate these issues by
jointly interpolating between noise and data through event insertions and
deletions in a discrete Markov chain. In this work, we generalize this
perspective and introduce an Edit Flow process for TPPs that transports noise
to data via insert, delete, and substitute edit operations. By learning the
instantaneous edit rates within a continuous-time Markov chain framework, we
attain a flexible and efficient model that effectively reduces the total number
of necessary edit operations during generation. Empirical results demonstrate
the generative flexibility of our unconditionally trained model in a wide range
of unconditional and conditional generation tasks on benchmark TPPs.

</details>


### [356] [Analyzing the Effect of Embedding Norms and Singular Values to Oversmoothing in Graph Neural Networks](https://arxiv.org/abs/2510.06066)
*Dimitrios Kelesis,Dimitris Fotakis,Georgios Paliouras*

Main category: cs.LG

TL;DR: 本文提出了一种新的度量（MASED）来量化图神经网络（GNNs）中的过平滑效应，并通过理论和实验分析了导致过平滑的因素，提出了G-Reg正则化方案来缓解过平滑，并在节点分类任务中取得了更好的性能和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 研究深度图神经网络（GNNs）中过平滑效应的根本原因，并提出有效的缓解方法。

Method: 提出了一种新的度量（MASED）来量化过平滑；推导了MASED的层级界限；分析了节点嵌入的范数和权重矩阵的奇异值；提出了G-Reg正则化方案。

Result: 过平滑随可训练权重矩阵和邻接矩阵数量的增加而增加；G-Reg正则化方案提高了节点分类精度和鲁棒性；在某些任务中，通过减少过平滑可以获得比浅层网络更好的结果；在“冷启动”场景下进行了实验；证明了感受野大小和性能之间的权衡。

Conclusion: MASED度量可以有效量化过平滑；G-Reg正则化方案能够缓解过平滑，提高深度GNNs的性能和鲁棒性；通过调整模型参数可以避免欠参数化或过参数化的问题。

Abstract: In this paper, we study the factors that contribute to the effect of
oversmoothing in deep Graph Neural Networks (GNNs). Specifically, our analysis
is based on a new metric (Mean Average Squared Distance - $MASED$) to quantify
the extent of oversmoothing. We derive layer-wise bounds on $MASED$, which
aggregate to yield global upper and lower distance bounds. Based on this
quantification of oversmoothing, we further analyze the importance of two
different properties of the model; namely the norms of the generated node
embeddings, along with the largest and smallest singular values of the weight
matrices. Building on the insights drawn from the theoretical analysis, we show
that oversmoothing increases as the number of trainable weight matrices and the
number of adjacency matrices increases. We also use the derived layer-wise
bounds on $MASED$ to form a proposal for decoupling the number of hops (i.e.,
adjacency depth) from the number of weight matrices. In particular, we
introduce G-Reg, a regularization scheme that increases the bounds, and
demonstrate through extensive experiments that by doing so node classification
accuracy increases, achieving robustness at large depths. We further show that
by reducing oversmoothing in deep networks, we can achieve better results in
some tasks than using shallow ones. Specifically, we experiment with a ``cold
start" scenario, i.e., when there is no feature information for the unlabeled
nodes. Finally, we show empirically the trade-off between receptive field size
(i.e., number of weight matrices) and performance, using the $MASED$ bounds.
This is achieved by distributing adjacency hops across a small number of
trainable layers, avoiding the extremes of under- or over-parameterization of
the GNN.

</details>


### [357] [Benchmark It Yourself (BIY): Preparing a Dataset and Benchmarking AI Models for Scatterplot-Related Tasks](https://arxiv.org/abs/2510.06071)
*João Palmeiro,Diogo Duarte,Rita Costa,Pedro Bizarro*

Main category: cs.LG

TL;DR: 本研究提出了一个包含18000多个散点图的合成数据集和基准测试，用于评估AI模型在散点图特定任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试很少涉及散点图特定任务，限制了对AI模型在数据分析和可视化方面表现的深入了解。

Method: 创建了一个包含6种数据生成器和17种图表设计的合成散点图数据集，并在此基础上进行基准测试。使用N-shot prompting评估了OpenAI和Google的模型在聚类边界框、中心坐标和异常值坐标识别等五个任务上的表现。

Result: OpenAI模型和Gemini 2.5 Flash在计数聚类和识别异常值方面表现出90%以上的准确率，尤其是在提供示例时。然而，在定位任务方面，除Flash在异常值识别方面达到65.01%的准确率外，其他任务的精确率和召回率均低于或接近50%。图表设计对模型表现有一定影响，建议避免使用宽长比（16:9和21:9）或随机着色的散点图。

Conclusion: AI模型在散点图特定任务上的表现参差不齐，计数任务表现较好，但定位任务仍需改进。图表设计因素，如长宽比和着色方式，也会影响模型性能。

Abstract: AI models are increasingly used for data analysis and visualization, yet
benchmarks rarely address scatterplot-specific tasks, limiting insight into
performance. To address this gap for one of the most common chart types, we
introduce a synthetic, annotated dataset of over 18,000 scatterplots from six
data generators and 17 chart designs, and a benchmark based on it. We evaluate
proprietary models from OpenAI and Google using N-shot prompting on five
distinct tasks derived from annotations of cluster bounding boxes, their center
coordinates, and outlier coordinates. OpenAI models and Gemini 2.5 Flash,
especially when prompted with examples, are viable options for counting
clusters and, in Flash's case, outliers (90%+ Accuracy). However, the results
for localization-related tasks are unsatisfactory: Precision and Recall are
near or below 50%, except for Flash in outlier identification (65.01%).
Furthermore, the impact of chart design on performance appears to be a
secondary factor, but it is advisable to avoid scatterplots with wide aspect
ratios (16:9 and 21:9) or those colored randomly. Supplementary materials are
available at https://github.com/feedzai/biy-paper.

</details>


### [358] [Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL](https://arxiv.org/abs/2510.06092)
*Nyal Patel,Matthieu Bou,Arjun Jagota,Satyapriya Krishna,Sonali Parbhoo*

Main category: cs.LG

TL;DR: RLHF 训练的 LLM 的奖励信号是隐藏的，这给可解释性和安全性带来了挑战。本研究提出了一种新的


<details>
  <summary>Details</summary>
Motivation: 现有 IRL 方法平等地对待所有偏好对，忽略了最有信息量的信号：被提取的奖励模型错误分类或评分接近的例子（称为“失败”）。

Method: 提出了一种新颖的“面向失败”的 IRL 算法，该算法专注于错误分类或困难的例子，以恢复定义模型行为的潜在奖励。

Result: 面向失败的 IRL 在 LLM 解毒任务上，在多个指标上优于现有的 IRL 基线，无需外部分类器或监督。此外，它能更好地捕捉 RLHF 期间学到的真实激励，并能比标准的 IRL 更有效地进行再 RLHF 训练。

Conclusion: 面向失败的 IRL 是一种稳健、可扩展的方法，用于审计模型对齐和减少 IRL 过程中的歧义。

Abstract: Reinforcement Learning from Human Feedback (RLHF) aligns Large Language
Models (LLMs) with human preferences, yet the underlying reward signals they
internalize remain hidden, posing a critical challenge for interpretability and
safety. Existing approaches attempt to extract these latent incentives using
Inverse Reinforcement Learning (IRL), but treat all preference pairs equally,
often overlooking the most informative signals: those examples the extracted
reward model misclassifies or assigns nearly equal scores, which we term
\emph{failures}. We introduce a novel \emph{failure-aware} IRL algorithm that
focuses on misclassified or difficult examples to recover the latent rewards
defining model behaviors. By learning from these failures, our failure-aware
IRL extracts reward functions that better reflect the true objectives behind
RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines
across multiple metrics when applied to LLM detoxification, without requiring
external classifiers or supervision. Crucially, failure-aware IRL yields
rewards that better capture the true incentives learned during RLHF, enabling
more effective re-RLHF training than standard IRL. This establishes
failure-aware IRL as a robust, scalable method for auditing model alignment and
reducing ambiguity in the IRL process.

</details>


### [359] [The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives](https://arxiv.org/abs/2510.06096)
*Matthieu Bou,Nyal Patel,Arjun Jagota,Satyapriya Krishna,Sonali Parbhoo*

Main category: cs.LG

TL;DR: 本研究提出了一种基于贝叶斯逆强化学习（BIRL）的审计框架，用于推断大型语言模型（LLMs）的潜在优化目标，解决了现有方法中存在的奖励估计不确定性高和任务固有的不确定性问题。该框架通过量化和减少不确定性、提供不确定性感知诊断以及验证策略效用，实现了对LLM行为的全面验证。实验证明，该框架能够成功审计并加强LLM的对齐，为AI的信任和问责提供了实用工具。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的优化目标不明确，给信任和审计带来了挑战。现有的逆强化学习（IRL）方法要么给出过于自信的单一奖励估计，要么无法解决任务固有的不确定性问题。

Method: 本研究提出了一个基于贝叶斯逆强化学习（BIRL）的审计框架。该框架将奖励推断从简单的估计任务重新定义为全面的验证过程。它能够推断出目标分布，并支持三种关键的审计能力：(1) 通过连续多轮证据的后验收缩来量化和系统地减少不确定性；(2) 提供可操作的、不确定性感知的诊断，以揭示虚假的捷径，并识别推断的目标不可信的分布外提示；(3) 通过展示精炼的、低不确定性的奖励可直接用于RLHF，以实现与真实对齐过程相当的训练动态和毒性降低，从而验证策略层面的效用。

Result: 该框架在实际审计中成功验证了一个经过去毒化处理的LLM，得出了一个校准良好且可解释的目标，从而增强了对齐保证。

Conclusion: 本研究提供了一个实用的工具集，用于审计、安全团队和监管机构验证LLM的真实目标，朝着更值得信赖和可问责的AI迈进。

Abstract: The objectives that Large Language Models (LLMs) implicitly optimize remain
dangerously opaque, making trustworthy alignment and auditing a grand
challenge. While Inverse Reinforcement Learning (IRL) can infer reward
functions from behaviour, existing approaches either produce a single,
overconfident reward estimate or fail to address the fundamental ambiguity of
the task (non-identifiability). This paper introduces a principled auditing
framework that re-frames reward inference from a simple estimation task to a
comprehensive process for verification. Our framework leverages Bayesian IRL to
not only recover a distribution over objectives but to enable three critical
audit capabilities: (i) Quantifying and systematically reducing
non-identifiability by demonstrating posterior contraction over sequential
rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics
that expose spurious shortcuts and identify out-of-distribution prompts where
the inferred objective cannot be trusted; and (iii) Validating policy-level
utility by showing that the refined, low-uncertainty reward can be used
directly in RLHF to achieve training dynamics and toxicity reductions
comparable to the ground-truth alignment process. Empirically, our framework
successfully audits a detoxified LLM, yielding a well-calibrated and
interpretable objective that strengthens alignment guarantees. Overall, this
work provides a practical toolkit for auditors, safety teams, and regulators to
verify what LLMs are truly trying to achieve, moving us toward more trustworthy
and accountable AI.

</details>


### [360] [The Physics of Data and Tasks: Theories of Locality and Compositionality in Deep Learning](https://arxiv.org/abs/2510.06106)
*Alessandro Favero*

Main category: cs.LG

TL;DR: 深度神经网络在处理高维数据时表现出色，但对其学习机制的理解有限。本文研究了数据、任务和深度学习表示中的局部性和组合性，以揭示神经网络如何学习和利用潜在结构，以及这种结构如何影响泛化能力。


<details>
  <summary>Details</summary>
Motivation: 深度神经网络在学习高维数据方面取得了巨大成功，但对其学习机制的理解仍然有限。这种成功与“维度灾难”现象似乎矛盾，表明可学习的数据必然存在潜在结构。本文旨在探讨这种结构的本质，以及神经网络如何对其进行编码、利用，并量化其对模型性能的影响。

Method: 本文研究了数据、任务和深度学习表示中的局部性和组合性。

Result: 本文研究了数据、任务和深度学习表示中的局部性和组合性，以揭示神经网络如何学习和利用潜在结构，以及这种结构如何影响泛化能力。

Conclusion: 本文研究了数据、任务和深度学习表示中的局部性和组合性，以揭示神经网络如何学习和利用潜在结构，以及这种结构如何影响泛化能力。

Abstract: Deep neural networks have achieved remarkable success, yet our understanding
of how they learn remains limited. These models can learn high-dimensional
tasks, which is generally statistically intractable due to the curse of
dimensionality. This apparent paradox suggests that learnable data must have an
underlying latent structure. What is the nature of this structure? How do
neural networks encode and exploit it, and how does it quantitatively impact
performance - for instance, how does generalization improve with the number of
training examples? This thesis addresses these questions by studying the roles
of locality and compositionality in data, tasks, and deep learning
representations.

</details>


### [361] [Influence Functions for Efficient Data Selection in Reasoning](https://arxiv.org/abs/2510.06108)
*Prateek Humane,Paolo Cudrano,Daniel Z. Kaplan,Matteo Matteucci,Supriyo Chakraborty,Irina Rish*

Main category: cs.LG

TL;DR: 使用影响函数来定义和选择高质量的链式思考（CoT）数据，以提升大语言模型（LLM）在数学推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有链式思考（CoT）数据选择方法依赖于间接启发式方法（如问题难度、推理过程长度），未能明确定义“质量”；而指令微调虽探索了自动化选择策略，但很少应用于推理任务。因此，需要一种更有效的方法来定义和选择高质量的CoT数据。

Method: 提出使用影响函数来定义推理数据的质量，即衡量单个CoT示例对下游准确度的因果效应。在此基础上，引入基于影响函数的剪枝方法来选择数据。

Result: 所提出的基于影响函数的剪枝方法在数学推理任务上，相较于基于困惑度（perplexity）和基于嵌入（embedding）的基线方法，始终能获得更好的性能，且在同一模型家族内表现一致。

Conclusion: 影响函数为定义和选择高质量的CoT数据提供了一种新的、更有效的方法，能够显著提升大语言模型在数学推理任务上的性能。

Abstract: Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows
that a small amount of high-quality data can outperform massive datasets. Yet,
what constitutes "quality" remains ill-defined. Existing reasoning methods rely
on indirect heuristics such as problem difficulty or trace length, while
instruction-tuning has explored a broader range of automated selection
strategies, but rarely in the context of reasoning. We propose to define
reasoning data quality using influence functions, which measure the causal
effect of individual CoT examples on downstream accuracy, and introduce
influence-based pruning, which consistently outperforms perplexity and
embedding-based baselines on math reasoning within a model family.

</details>


### [362] [PolyGraph Discrepancy: a classifier-based metric for graph generation](https://arxiv.org/abs/2510.06122)
*Markus Krimmel,Philip Hartout,Karsten Borgwardt,Dexiong Chen*

Main category: cs.LG

TL;DR: 现有的图生成模型评估方法主要依赖于基于图描述符的最大均值差异（MMD）指标，但这些指标无法提供绝对的性能度量，且对外部参数敏感。我们提出了一种新的评估框架PolyGraph Discrepancy（PGD），通过拟合二元分类器来区分真实和生成的图，从而近似Jensen-Shannon距离。


<details>
  <summary>Details</summary>
Motivation: 现有的图生成模型评估方法（如MMD）存在无法提供绝对性能度量、对外部参数敏感以及跨描述符不可比的问题。

Method: 通过拟合二元分类器来区分真实和生成的图，并利用分类器的对数似然来近似Jensen-Shannon距离的变分下界，从而提出PolyGraph Discrepancy（PGD）评估框架。

Result: PGD指标被约束在[0,1]区间内，可以跨不同的图描述符进行比较。此外，我们还推导了一个理论上合理的摘要指标，可以提供给定描述符的最大可能下界。

Conclusion: PGD提供了一种比MMD更鲁棒、更具洞察力的图生成模型评估方法。

Abstract: Existing methods for evaluating graph generative models primarily rely on
Maximum Mean Discrepancy (MMD) metrics based on graph descriptors. While these
metrics can rank generative models, they do not provide an absolute measure of
performance. Their values are also highly sensitive to extrinsic parameters,
namely kernel and descriptor parametrization, making them incomparable across
different graph descriptors. We introduce PolyGraph Discrepancy (PGD), a new
evaluation framework that addresses these limitations. It approximates the
Jensen-Shannon distance of graph distributions by fitting binary classifiers to
distinguish between real and generated graphs, featurized by these descriptors.
The data log-likelihood of these classifiers approximates a variational lower
bound on the JS distance between the two distributions. Resulting metrics are
constrained to the unit interval [0,1] and are comparable across different
graph descriptors. We further derive a theoretically grounded summary metric
that combines these individual metrics to provide a maximally tight lower bound
on the distance for the given descriptors. Thorough experiments demonstrate
that PGD provides a more robust and insightful evaluation compared to MMD
metrics. The PolyGraph framework for benchmarking graph generative models is
made publicly available at https://github.com/BorgwardtLab/polygraph-benchmark.

</details>


### [363] [Downsized and Compromised?: Assessing the Faithfulness of Model Compression](https://arxiv.org/abs/2510.06125)
*Moumita Kamal,Douglas A. Talbert*

Main category: cs.LG

TL;DR: 该论文提出了一种评估模型压缩后忠实度的新方法，强调了在医疗、金融等高风险领域，模型行为的一致性比单纯的准确性更重要。


<details>
  <summary>Details</summary>
Motivation: 现有模型压缩评估主要关注大小与准确性之间的权衡，忽视了模型在压缩后行为的忠实度，这在高风险应用领域（如医疗、金融、司法）是不可接受的。

Method: 提出了一套新的忠实度评估指标，包括使用模型一致性评估压缩前后模型的预测一致性，并运用卡方检验来检测数据集整体和人口子群体中预测模式的统计学显著变化。

Result: 通过在三个不同的、具有社会意义的数据集上对人工神经网络进行量化和剪枝实验，证明了高准确性并不保证忠实度，并且所提出的统计检验能够检测出准确率和均衡赔率等标准指标所忽略的细微但显著的变化。

Conclusion: 提出的指标为确保模型压缩带来的效率提升不损害公平性和忠实度提供了实用且更直接的方法，这对于构建可信赖的人工智能至关重要。

Abstract: In real-world applications, computational constraints often require
transforming large models into smaller, more efficient versions through model
compression. While these techniques aim to reduce size and computational cost
without sacrificing performance, their evaluations have traditionally focused
on the trade-off between size and accuracy, overlooking the aspect of model
faithfulness. This limited view is insufficient for high-stakes domains like
healthcare, finance, and criminal justice, where compressed models must remain
faithful to the behavior of their original counterparts. This paper presents a
novel approach to evaluating faithfulness in compressed models, moving beyond
standard metrics. We introduce and demonstrate a set of faithfulness metrics
that capture how model behavior changes post-compression. Our contributions
include introducing techniques to assess predictive consistency between the
original and compressed models using model agreement, and applying chi-squared
tests to detect statistically significant changes in predictive patterns across
both the overall dataset and demographic subgroups, thereby exposing shifts
that aggregate fairness metrics may obscure. We demonstrate our approaches by
applying quantization and pruning to artificial neural networks (ANNs) trained
on three diverse and socially meaningful datasets. Our findings show that high
accuracy does not guarantee faithfulness, and our statistical tests detect
subtle yet significant shifts that are missed by standard metrics, such as
Accuracy and Equalized Odds. The proposed metrics provide a practical and more
direct method for ensuring that efficiency gains through compression do not
compromise the fairness or faithfulness essential for trustworthy AI.

</details>


### [364] [lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models](https://arxiv.org/abs/2510.06126)
*Haoxin Wang,Xiaolong Tu,Hongyu Ke,Huirong Chai,Dawei Chen,Kyungtae Han*

Main category: cs.LG

TL;DR: lm-Meter是一个轻量级的在线延迟分析器，用于设备端大语言模型推理，解决了云端部署的隐私和可持续性问题，并为优化设备端LLM系统提供了基础。


<details>
  <summary>Details</summary>
Motivation: 云端部署大语言模型存在数据隐私、可持续性、通信成本等问题；而设备端大语言模型部署面临内存和计算需求高、资源受限硬件上性能-效率权衡不清晰的挑战。

Method: 提出lm-Meter，一个轻量级的在线延迟分析器，能够捕捉设备端大语言模型推理的细粒度、实时延迟，包括阶段（如embedding、prefill、decode、softmax、sampling）和内核级别，无需辅助设备。

Result: 在商用移动平台上实现了lm-Meter，并证明了其高分析精度和极低的系统开销（在Powersave模式下吞吐量仅降低2.58%（prefill）和0.99%（decode））。利用lm-Meter进行了全面的实证研究，揭示了设备端LLM推理的阶段和内核瓶颈，量化了准确性-效率的权衡，并识别了系统性的优化机会。

Conclusion: lm-Meter为设备端LLM推理提供了前所未有的运行时可见性，为知情的优化奠定了基础，并加速了设备端LLM系统的普及。

Abstract: Large Language Models (LLMs) are increasingly integrated into everyday
applications, but their prevalent cloud-based deployment raises growing
concerns around data privacy and long-term sustainability. Running LLMs locally
on mobile and edge devices (on-device LLMs) offers the promise of enhanced
privacy, reliability, and reduced communication costs. However, realizing this
vision remains challenging due to substantial memory and compute demands, as
well as limited visibility into performance-efficiency trade-offs on
resource-constrained hardware. We propose lm-Meter, the first lightweight,
online latency profiler tailored for on-device LLM inference. lm-Meter captures
fine-grained, real-time latency at both phase (e.g., embedding, prefill,
decode, softmax, sampling) and kernel levels without auxiliary devices. We
implement lm-Meter on commercial mobile platforms and demonstrate its high
profiling accuracy with minimal system overhead, e.g., only 2.58% throughput
reduction in prefill and 0.99% in decode under the most constrained Powersave
governor. Leveraging lm-Meter, we conduct comprehensive empirical studies
revealing phase- and kernel-level bottlenecks in on-device LLM inference,
quantifying accuracy-efficiency trade-offs, and identifying systematic
optimization opportunities. lm-Meter provides unprecedented visibility into the
runtime behavior of LLMs on constrained platforms, laying the foundation for
informed optimization and accelerating the democratization of on-device LLM
systems. Code and tutorials are available at
https://github.com/amai-gsu/LM-Meter.

</details>


### [365] [Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks](https://arxiv.org/abs/2510.06138)
*Rushiv Arora*

Main category: cs.LG

TL;DR: LEXPOL是一种语言条件化的策略架构，通过文本编码器和门控模块，在多任务强化学习中无需任务特定重训即可匹配或超越基线模型，并能有效组合专家策略以适应新任务。


<details>
  <summary>Details</summary>
Motivation: 多任务强化学习通常依赖任务元数据（如自然语言描述）来指导跨不同目标的行为。

Method: LEXPOL是一种语言条件化的策略架构，它使用文本编码器来编码任务元数据，并使用学习到的门控模块来选择或混合多个子策略，从而实现跨任务的端到端训练。

Result: 在MetaWorld基准测试中，LEXPOL在成功率和样本效率方面匹配或超越了强大的多任务基线模型，且无需进行特定任务的再训练。通过研究固定的专家策略，表明学习到的语言门可以组合这些专家策略，以产生适用于新颖的任务描述和未见过的任务组合的行为。

Conclusion: 自然语言元数据可以有效地索引和重组单个策略中的可重用技能。

Abstract: Multi-task reinforcement learning often relies on task metadata -- such as
brief natural-language descriptions -- to guide behavior across diverse
objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned
mixture-of-policies architecture for multi-task RL. LEXPOL encodes task
metadata with a text encoder and uses a learned gating module to select or
blend among multiple sub-policies, enabling end-to-end training across tasks.
On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines
in success rate and sample efficiency, without task-specific retraining. To
analyze the mechanism, we further study settings with fixed expert policies
obtained independently of the gate and show that the learned language gate
composes these experts to produce behaviors appropriate to novel task
descriptions and unseen task combinations. These results indicate that
natural-language metadata can effectively index and recombine reusable skills
within a single policy.

</details>


### [366] [LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams](https://arxiv.org/abs/2510.06151)
*Aju Ani Justus,Chris Baber*

Main category: cs.LG

TL;DR: LLMs可作为策略无关的人类代理，通过生成合成数据来模拟人类决策，以应对异构智能体团队的训练挑战。


<details>
  <summary>Details</summary>
Motivation: 训练智能体与策略不可访问或不稳定的队友（如人类）协作是一个关键挑战。传统方法依赖昂贵的人工数据，限制了可扩展性。

Method: 提出使用大型语言模型（LLMs）作为策略无关的人类代理，生成模拟人类决策的合成数据。在受“鹿 D.2 猎 D.2 游戏”启发的网格世界捕获游戏中进行了三项实验。

Result: LLM（在提示中包含游戏状态和奖励结构时）比人类参与者更符合专家判断。通过修改提示，LLM能够模仿人类在风险规避和风险寻求行为之间的转换。在动态网格世界实验中，LLM生成的轨迹与人类参与者相似。

Conclusion: LLM 提供了一种可扩展的基础来模拟策略无关的队友，尽管它们尚不能完全复制人类的适应性。

Abstract: A critical challenge in modelling Heterogeneous-Agent Teams is training
agents to collaborate with teammates whose policies are inaccessible or
non-stationary, such as humans. Traditional approaches rely on expensive
human-in-the-loop data, which limits scalability. We propose using Large
Language Models (LLMs) as policy-agnostic human proxies to generate synthetic
data that mimics human decision-making. To evaluate this, we conduct three
experiments in a grid-world capture game inspired by Stag Hunt, a game theory
paradigm that balances risk and reward. In Experiment 1, we compare decisions
from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and
Mixtral 8x22B models. LLMs, prompted with game-state observations and reward
structures, align more closely with experts than participants, demonstrating
consistency in applying underlying decision criteria. Experiment 2 modifies
prompts to induce risk-sensitive strategies (e.g. "be risk averse"). LLM
outputs mirror human participants' variability, shifting between risk-averse
and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic
grid-world where the LLM agents generate movement actions. LLMs produce
trajectories resembling human participants' paths. While LLMs cannot yet fully
replicate human adaptability, their prompt-guided diversity offers a scalable
foundation for simulating policy-agnostic teammates.

</details>


### [367] [TabPFN-Wide: Continued Pre-Training for Extreme Feature Counts](https://arxiv.org/abs/2510.06162)
*Christopher Kolberg,Katharina Eggensperger,Nico Pfeifer*

Main category: cs.LG

TL;DR: 该研究提出了一种名为 TabPFN-Wide 的新模型，可以处理具有大量特征（超过 50,000 个）的高维生物医学数据，同时保持模型的可解释性。


<details>
  <summary>Details</summary>
Motivation: 现有的深度学习模型在处理高维生物医学数据时面临挑战，因为它们通常特征数量庞大且包含噪声，而现有的基于优先数据训练的模型（如 TabPFN）无法有效处理超过 500 个特征的情况。特征降维虽然可以解决这个问题，但会牺牲特征重要性分析的能力。因此，需要一种能够处理高维数据并保持模型可解释性的方法。

Method: 研究提出了一种扩展现有模型（如 TabPFN）的策略，通过在自定义先验生成的合成数据上进行持续预训练。具体来说，他们设计了一种名为 TabPFN-Wide 的模型，该模型通过在精心设计的先验数据上进行预训练，使其能够扩展到处理超过 50,000 个特征，并且在噪声较大的情况下也能保持良好的性能和可解释性。

Result: TabPFN-Wide 模型在性能上达到或超过了其基础模型，并且在噪声鲁棒性方面有所提高。该模型能够处理超过 50,000 个特征，不受噪声水平的影响，同时保持了生物医学应用中至关重要的模型可解释性。在真实的生物医学数据集上，该模型识别出的重要特征与之前的生物学发现高度吻合，并提出了一些有潜力的新发现。

Conclusion: 研究结果表明，基于先验信息进行模型适应是增强基础模型处理高维数据能力的有效策略。TabPFN-Wide 模型在处理高维生物医学数据方面取得了成功，既能保持高性能，又能提供可解释性，为生物医学研究提供了有价值的工具。

Abstract: Revealing novel insights from the relationship between molecular measurements
and pathology remains a very impactful application of machine learning in
biomedicine. Data in this domain typically contain only a few observations but
thousands of potentially noisy features, posing challenges for conventional
machine learning approaches. While prior-data fitted networks emerge as
foundation models for tabular data, they are currently not suited to handle
large feature counts (>500). Although feature reduction enables their
application, it hinders feature importance analysis. We propose a strategy that
extends existing models through continued pre-training on synthetic data
sampled from a customized prior. The resulting model, TabPFN-Wide, matches or
exceeds its base model's performance while exhibiting improved robustness to
noise. It seamlessly scales beyond 50,000 features, regardless of noise levels,
while maintaining inherent interpretability, which is critical for biomedical
applications. Our results show that prior-informed adaptation is suitable to
enhance the capability of foundation models for high-dimensional data. On
real-world biomedical datasets many of the most relevant features identified by
the model overlap with previous biological findings, while others propose
potential starting points for future studies.

</details>


### [368] [Thermodynamic Performance Limits for Score-Based Diffusion Models](https://arxiv.org/abs/2510.06174)
*Nathan X. Kodama,Michael Hinczewski*

Main category: cs.LG

TL;DR: score-based diffusion models与非平衡热力学相关联，通过熵率推导出性能上限，并将生成模型性能与随机热力学的基本物理原理联系起来。


<details>
  <summary>Details</summary>
Motivation: 建立score-based diffusion models与非平衡热力学之间的基本联系，并通过熵率推导出性能上限。

Method: 推导出数据负对数似然的下界，将模型性能与扩散过程的熵率联系起来，并在合成数据集上进行数值验证。

Result: 在合成数据集上进行了数值验证，并探讨了该界限的紧密性。通过引入系统熵、固有熵和交换熵，为这些模型的 istung 提供了新的见解，并与麦克斯韦妖和热力学计算硬件的含义进行了类比。

Conclusion: score-based diffusion models的性能与随机热力学的基本物理原理相关联。

Abstract: We establish a fundamental connection between score-based diffusion models
and non-equilibrium thermodynamics by deriving performance limits based on
entropy rates. Our main theoretical contribution is a lower bound on the
negative log-likelihood of the data that relates model performance to entropy
rates of diffusion processes. We numerically validate this bound on a synthetic
dataset and investigate its tightness. By building a bridge to entropy rates -
system, intrinsic, and exchange entropy - we provide new insights into the
thermodynamic operation of these models, drawing parallels to Maxwell's demon
and implications for thermodynamic computing hardware. Our framework connects
generative modeling performance to fundamental physical principles through
stochastic thermodynamics.

</details>


### [369] [On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond](https://arxiv.org/abs/2510.06190)
*Chenxiao Yang,Cai Zhou,David Wipf,Zhiyuan Li*

Main category: cs.LG

TL;DR: 生成模型（如自回归和掩码扩散）可以通过超越当前方法的重写和长度可变编辑能力，获得显著的理论和经验优势，这对于处理更难问题和跨领域（如编码和科学）的LLM具有重要意义。


<details>
  <summary>Details</summary>
Motivation: 该研究的动机是超越当前生成模型的架构限制，在更抽象的层面研究其优缺点，为前沿大型语言模型（LLM）在处理更复杂任务和跨领域应用方面提供理论和实践指导。

Method: 本研究通过可衡量的标准（如计算难度和可学性）来量化生成过程（包括自回归和掩码扩散）的理论优势和局限性。具体来说，研究探索了允许生成过程超越当前自回归和掩码扩散的方法，实现了重写和长度可变编辑等能力。

Result: 研究表明，允许生成过程进行重写和长度可变编辑等操作，相比于传统的自回归和掩码扩散方法，在理论和经验上都带来了显著的优势。

Conclusion: 该研究的结论是，通过引入重写和长度可变编辑等能力，可以增强生成模型的通用性和处理复杂问题的能力，这对未来在自然语言、代码和科学等领域具有广泛应用前景的大型语言模型至关重要。

Abstract: This paper formally studies generation processes, including auto-regressive
next-token prediction and masked diffusion, that abstract beyond architectural
specifics. At this level of abstraction, we quantify their benefits and
limitations through measurable criteria such as computational hardness and
learnability. In particular, we demonstrate that allowing generation to proceed
beyond autoregression and current masked diffusion, with capabilities to
rewrite and length-variable edit, can bring significant theoretical and
empirical advantages, with important implications for frontier LLMs that aspire
to tackle increasingly hard problems and work universally across domains beyond
natural language, such as coding and science.

</details>


### [370] [Reference Grounded Skill Discovery](https://arxiv.org/abs/2510.06203)
*Seungeun Rho,Aaron Trinh,Danfei Xu,Sehoon Ha*

Main category: cs.LG

TL;DR: RGSD通过利用参考数据，在语义有意义的潜在空间中进行技能发现，解决了高自由度（DoF）下无监督技能发现的挑战，并在模拟人形机器人上取得了优于基线方法的成果。


<details>
  <summary>Details</summary>
Motivation: 高自由度系统下的无监督技能发现面临探索空间巨大和有意义技能有限的挑战，需要语义引导来有效探索。

Method: RGSD算法首先通过对比预训练将运动嵌入到单位超球面上，然后将每个参考轨迹聚类到不同的方向。这种方法使得技能发现能够同时模仿参考行为并发现语义上相关的多样化行为。

Result: 在具有359维观测值和69维动作的模拟SMPL人形机器人上，RGSD成功学习了行走、跑步、拳击和侧步等结构化技能，并发现了相关的、新颖的行为。与基于模仿的技能获取基线相比，RGSD在下游控制任务中表现更优。

Conclusion: 轻量级的参考引导方法为在高自由度系统中发现语义丰富且结构化的技能提供了一条实用的途径。

Abstract: Scaling unsupervised skill discovery algorithms to high-DoF agents remains
challenging. As dimensionality increases, the exploration space grows
exponentially, while the manifold of meaningful skills remains limited.
Therefore, semantic meaningfulness becomes essential to effectively guide
exploration in high-dimensional spaces. In this work, we present
Reference-Grounded Skill Discovery (RGSD), a novel algorithm that grounds skill
discovery in a semantically meaningful latent space using reference data. RGSD
first performs contrastive pretraining to embed motions on a unit hypersphere,
clustering each reference trajectory into a distinct direction. This grounding
enables skill discovery to simultaneously involve both imitation of reference
behaviors and the discovery of semantically related diverse behaviors. On a
simulated SMPL humanoid with 359-D observations and 69-D actions, RGSD learns
structured skills including walking, running, punching, and side stepping, and
also discovers related novel behaviors. In downstream control tasks, RGSD
outperforms imitation-based skill acquisition baselines. Our results suggest
that lightweight reference-guided grounding offers a practical path to
discovering semantically rich and structured skills in high-DoF systems.

</details>


### [371] [Training Dynamics Impact Post-Training Quantization Robustness](https://arxiv.org/abs/2510.06213)
*Albert Catalan-Tatjer,Niccolò Ajroldi,Jonas Geiping*

Main category: cs.LG

TL;DR: 后训练量化在高效部署大型语言模型方面得到广泛应用，但其鲁棒性机制尚不明确。研究人员分析了高达 320 亿参数和 15 万亿训练词元的开源语言模型训练轨迹，以评估训练动态与量化性能之间的关系。他们发现，在大型训练运行中，量化误差主要由学习率和其他训练超参数相互作用引起。当学习率衰减后，验证损失和量化误差会发散，并且在很大程度上与训练数据规模无关。为了研究训练动态的干预措施并确定有利于调节量化鲁棒性的特定配置，研究人员进行了高达 100 万亿词元的受控实验。结果表明，增加数据集规模并不一定会损害量化效果，反而可以通过战略性的超参数干预来提高量化质量。


<details>
  <summary>Details</summary>
Motivation: 量化是部署大型语言模型（LLM）的关键技术，但其鲁棒性机制尚不明确。本研究旨在深入理解训练动态如何影响量化性能，并探索改善量化鲁棒性的方法。

Method: 研究人员分析了不同规模（高达 320 亿参数）和训练数据量（高达 15 万亿词元）的开源语言模型训练过程，重点关注学习率等超参数对量化误差的影响。随后，他们进行了受控实验（高达 100 万亿词元），以验证和探索改善量化鲁棒性的训练策略。

Result: 研究发现，学习率衰减后，验证损失和量化误差会发散，且量化误差主要受学习率和其它训练超参数的相互作用影响，与训练数据规模关系不大。此外，研究表明通过调整训练超参数可以改善量化质量，即使在更大的数据集上也是如此。

Conclusion: 本研究挑战了增加数据集规模必然会损害量化效果的假设。研究结果强调了通过战略性地调整训练超参数来提高大型模型量化鲁棒性的重要性。

Abstract: While post-training quantization is widely adopted for efficient deployment
of large language models, the mechanisms underlying quantization robustness
remain unclear. We conduct a comprehensive analysis of quantization degradation
across open-source language model training trajectories up to 32B parameters
and 15T training tokens to accurately assess the relationship between training
dynamics and quantization performance. Our key finding is that quantization
errors in large-scale training runs are driven by a complex interplay between
learning rate and other training hyperparameters. Specifically, once learning
rates decay, validation loss and quantization error diverge, largely
independent of training data scale. To investigate interventions on the
training dynamics and identify specific configurations that can modulate
quantization robustness favorably, we train our own models in controlled
experiments up to 100B tokens. Our results challenge the assumption that
increasing dataset scale inherently compromises quantization effectiveness,
demonstrating instead that strategic training hyperparameter interventions can
improve quantization quality at scale.

</details>


### [372] [Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents](https://arxiv.org/abs/2510.06214)
*Mingkang Zhu,Xi Chen,Bei Yu,Hengshuang Zhao,Jiaya Jia*

Main category: cs.LG

TL;DR: LLM 代理利用搜索工具解决复杂问题，但搜索轨迹异构导致奖励分布不同。标准策略梯度法存在跨层偏差，阻碍了学习。本研究提出分层 GRPO，通过将轨迹划分为同质层并进行局部优势计算，解决了此问题。


<details>
  <summary>Details</summary>
Motivation: 标准策略梯度法在 LLM 搜索代理的训练中存在跨层偏差，影响了对异构搜索轨迹的信用分配和复杂搜索策略的探索。

Method: 提出分层 GRPO (Stratified GRPO)，其核心组件是分层优势归一化 (SAN)，根据结构属性将轨迹划分为同质层，并在每层内局部计算优势。

Result: 分层 GRPO 在多项问答基准测试中，相比 GRPO 性能提升高达 11.3 个点，同时提高了训练奖励、训练稳定性和搜索策略的有效性。

Conclusion: 分层作为一种原则性的方法，可以有效解决强化学习中 LLM 搜索代理的结构异质性问题。

Abstract: Large language model (LLM) agents increasingly rely on external tools such as
search engines to solve complex, multi-step problems, and reinforcement
learning (RL) has become a key paradigm for training them. However, the
trajectories of search agents are structurally heterogeneous, where variations
in the number, placement, and outcomes of search calls lead to fundamentally
different answer directions and reward distributions. Standard policy gradient
methods, which use a single global baseline, suffer from what we identify and
formalize as cross-stratum bias-an "apples-to-oranges" comparison of
heterogeneous trajectories. This cross-stratum bias distorts credit assignment
and hinders exploration of complex, multi-step search strategies. To address
this, we propose Stratified GRPO, whose central component, Stratified Advantage
Normalization (SAN), partitions trajectories into homogeneous strata based on
their structural properties and computes advantages locally within each
stratum. This ensures that trajectories are evaluated only against their true
peers. Our analysis proves that SAN eliminates cross-stratum bias, yields
conditionally unbiased unit-variance estimates inside each stratum, and retains
the global unbiasedness and unit-variance properties enjoyed by standard
normalization, resulting in a more pure and scale-stable learning signal. To
improve practical stability under finite-sample regimes, we further linearly
blend SAN with the global estimator. Extensive experiments on diverse
single-hop and multi-hop question-answering benchmarks demonstrate that
Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3
points, achieving higher training rewards, greater training stability, and more
effective search policies. These results establish stratification as a
principled remedy for structural heterogeneity in RL for LLM search agents.

</details>


<div id='cs.AR'></div>

# cs.AR [[Back]](#toc)

### [373] [Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving](https://arxiv.org/abs/2510.05245)
*Yue Pan,Zihan Xia,Po-Kai Hsu,Lanxiang Hu,Hyungyo Kim,Janak Sharda,Minxuan Zhou,Nam Sung Kim,Shimeng Yu,Tajana Rosing,Mingu Kang*

Main category: cs.AR

TL;DR: MoE模型因其高效能而广泛应用，但硬件部署面临挑战。Stratum提出一种系统-硬件协同设计，结合Mono3D DRAM、近内存处理（NMP）和GPU加速，解决了MoE模型的部署问题，在解码吞吐量和能效方面取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: MoE模型虽然在性能上表现优异，但在硬件部署上存在数据量大、延迟高等挑战。

Method: 提出Stratum系统-硬件协同设计，采用Mono3D DRAM、近内存处理（NMP）和GPU加速。通过混合键合连接逻辑与DRAM，并通过硅中介层连接DRAM与GPU。利用Mono3D DRAM的高内部带宽和近内存处理能力，并通过构建内存层级和基于主题的专家使用预测来优化数据访问，以解决垂直扩展带来的延迟问题。

Result: Stratum系统在解码吞吐量方面实现了高达8.29倍的提升，能效比GPU基线提高了7.66倍。

Conclusion: Stratum通过创新的系统-硬件协同设计，有效解决了MoE模型的部署挑战，并在性能和能效上取得了显著的改进。

Abstract: As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE)
architecture has emerged as a prevailing design for achieving state-of-the-art
performance across a wide range of tasks. MoE models use sparse gating to
activate only a handful of expert sub-networks per input, achieving
billion-parameter capacity with inference costs akin to much smaller models.
However, such models often pose challenges for hardware deployment due to the
massive data volume introduced by the MoE layers. To address the challenges of
serving MoE models, we propose Stratum, a system-hardware co-design approach
that combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D
DRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D
DRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack
and GPU are interconnected via silicon interposer. Mono3D DRAM offers higher
internal bandwidth than HBM thanks to the dense vertical interconnect pitch
enabled by its monolithic structure, which supports implementations of
higher-performance near-memory processing. Furthermore, we tackle the latency
differences introduced by aggressive vertical scaling of Mono3D DRAM along the
z-dimension by constructing internal memory tiers and assigning data across
layers based on access likelihood, guided by topic-based expert usage
prediction to boost NMP throughput. The Stratum system achieves up to 8.29x
improvement in decoding throughput and 7.66x better energy efficiency across
various benchmarks compared to GPU baselines.

</details>


### [374] [DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base](https://arxiv.org/abs/2510.05327)
*Zahin Ibnat,Paul E. Calzada,Rasin Mohammed Ihtemam,Sujan Kumar Saha,Jingbo Zhou,Farimah Farahmandi,Mark Tehranipoor*

Main category: cs.AR

TL;DR: LLM在硬件设计自动化（RTL代码生成）领域存在集成新IP和保持模型先进性的挑战。本文提出DeepV框架，利用高质量数据集和检索增强生成（RAG）技术，无需RAG特定训练，提升了GPT-5在VerilogEval基准上的性能近17%，解决了现有方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有LLM在RTL代码生成方面存在两个主要问题：1. 难以集成新IP到模型知识库，导致代码生成质量不高；2. 随着通用LLM的进步，基于旧模型的微调方法将逐渐落后。现有RAG技术也存在代码库质量低、微调成本高或未直接应用于RTL生成等问题。

Method: 提出DeepV：一个模型无关的RAG框架，通过一个大型、高质量的数据集来增强上下文，无需任何RTL特定训练，直接在RTL生成步骤中应用RAG。

Result: DeepV框架在VerilogEval基准上，使用最新的GPT-5模型，性能提升了近17%。

Conclusion: DeepV是一个有效的模型无关RAG框架，能够通过增强上下文和利用高质量数据集来生成高质量的RTL设计，解决了现有LLM在RTL代码生成方面的局限性。

Abstract: As large language models (LLMs) continue to be integrated into modern
technology, there has been an increased push towards code generation
applications, which also naturally extends to hardware design automation.
LLM-based solutions for register transfer level (RTL) code generation for
intellectual property (IP) designs have grown, especially with fine-tuned LLMs,
prompt engineering, and agentic approaches becoming popular in literature.
However, a gap has been exposed in these techniques, as they fail to integrate
novel IPs into the model's knowledge base, subsequently resulting in poorly
generated code. Additionally, as general-purpose LLMs continue to improve,
fine-tuned methods on older models will not be able to compete to produce more
accurate and efficient designs. Although some retrieval augmented generation
(RAG) techniques exist to mitigate challenges presented in fine-tuning
approaches, works tend to leverage low-quality codebases, incorporate
computationally expensive fine-tuning in the frameworks, or do not use RAG
directly in the RTL generation step. In this work, we introduce DeepV: a
model-agnostic RAG framework to generate RTL designs by enhancing context
through a large, high-quality dataset without any RTL-specific training. Our
framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17%
increase in performance on the VerilogEval benchmark. We host DeepV for use by
the community in a Hugging Face (HF) Space:
https://huggingface.co/spaces/FICS-LLM/DeepV.

</details>


### [375] [From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs](https://arxiv.org/abs/2510.05632)
*Tianhao Zhu,Dahu Feng,Erhu Feng,Yubin Xia*

Main category: cs.AR

TL;DR: LLM推理服务需求增长，现有AI加速器（如TPU, NPU等）多采用多核架构但缺乏灵活性。本文提出一个多层级模拟框架，用于分析和优化多核NPU上的张量并行、核放置、内存管理和PD选择策略，实验结果显示可实现1.32x-6.03x的加速，并为LLM服务提供硬件设计和部署策略指导。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）的广泛应用，对高性能LLM推理服务的需求日益增长。现有的AI加速器虽然采用了多核架构以提高可扩展性，但缺乏灵活性，可能导致计算资源利用不足和推理性能不佳。

Method: 提出一个包含事务级和性能模型的多层级模拟框架，用于分析多核NPU。基于该模拟器，系统性地分析并提出了张量并行策略、核放置策略、内存管理方法以及PD-disaggregation与PD-fusion选择的最优解决方案。

Result: 通过在代表性LLM和多种NPU配置上进行广泛实验，所提出的解决方案相比于多核NPU的现有最优（SOTA）设计，在不同硬件配置下实现了1.32x到6.03x的加速。

Conclusion: 该研究为多核NPU的设计和LLM推理服务提供了优化策略和指导，能够在不同LLM工作负载下提升硬件架构和服务的性能。

Abstract: With the widespread adoption of Large Language Models (LLMs), the demand for
high-performance LLM inference services continues to grow. To meet this demand,
a growing number of AI accelerators have been proposed, such as Google TPU,
Huawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators
adopt multi-core architectures to achieve enhanced scalability, but lack the
flexibility of SIMT architectures. Therefore, without careful configuration of
the hardware architecture, as well as deliberate design of tensor parallelism
and core placement strategies, computational resources may be underutilized,
resulting in suboptimal inference performance.
  To address these challenges, we first present a multi-level simulation
framework with both transaction-level and performance-model-based simulation
for multi-core NPUs. Using this simulator, we conduct a systematic analysis and
further propose the optimal solutions for tensor parallelism strategies, core
placement policies, memory management methods, as well as the selection between
PD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive
experiments on representative LLMs and various NPU configurations. The
evaluation results demonstrate that, our solution can achieve 1.32x-6.03x
speedup compared to SOTA designs for multi-core NPUs across different hardware
configurations. As for LLM serving, our work offers guidance on designing
optimal hardware architectures and serving strategies for multi-core NPUs
across various LLM workloads.

</details>


### [376] [An opportunity to improve Data Center Efficiency: Optimizing the Server's Upgrade Cycle](https://arxiv.org/abs/2510.05787)
*Panagiota Nikolaou,Freddy Gabbay,Jawad Haj-Yahya,Yiannakis Sazeides*

Main category: cs.AR

TL;DR: 通过在数据中心设计时制定覆盖其整个生命周期的全局升级计划，优化服务器升级时间，可以提高 QPS/(TCOxCO2) 指标。


<details>
  <summary>Details</summary>
Motivation: 提高数据中心的效率，通过优化服务器升级计划来确定用新服务器替换旧服务器的最佳时机。

Method: 制定一个全局升级计划，该计划利用服务器的服役年份、性能和功耗信息，并与仅考虑当前可用服务器模型的局部升级计划进行比较。

Result: 全局升级计划可能涉及非固定时间段的升级，并且优于仅基于当前可用服务器模型的局部升级计划。

Conclusion: 在数据中心设计时制定一个覆盖其整个生命周期的全局升级计划，可以显著提高 QPS/(TCOxCO2) 指标，并且优于传统的局部升级计划。

Abstract: This work aims to improve a data center's efficiency by optimizing the server
upgrade plan: determine the optimal timing for replacing old servers with new
ones. The opportunity presented by this approach is demonstrated through a
study based on historical server data. The study establishes a significant
opportunity to increase the QPS/(TCOxCO2) metric by formulating a global
upgrade plan at the data center's design time covering its entire life cycle.
This plan leverages information, such as server entry year, performance, and
active power consumption for both existing and future servers. Our findings
reveal that an optimal global upgrade plan, may involve upgrades at non fixed
time periods and outperforms local upgrade plans. Local upgrade plans follow a
fixed, equal-length cycle and make decisions based only on currently available
server models. These local plans select the best available server at each
upgrade cycle without accounting for future server releases.

</details>


<div id='eess.SP'></div>

# eess.SP [[Back]](#toc)

### [377] [Model-based Deep Learning for Joint RIS Phase Shift Compression and WMMSE Beamforming](https://arxiv.org/abs/2510.05438)
*Alexander James Fernandes,Ioannis Psaromiligkos*

Main category: eess.SP

TL;DR: 提出了一种基于模型的深度学习（DL）架构，用于可重构智能表面（RIS）辅助的多用户通信，以减少从接入点（AP）到RIS控制器传输相位移信息的工作量。AP计算相位移，然后将它们编码成压缩的二进制控制消息发送给RIS控制器进行配置。为了减少相位移压缩误差引起的波束赋形器失配，使用基于有效信道的加权最小均方误差（WMMSE）更新波束赋形器。通过将迭代WMMSE算法作为无线通信感知DL架构的一部分进行展开，可以进行端到端的联合相位移压缩和WMMSE波束赋形训练。


<details>
  <summary>Details</summary>
Motivation: 减少接入点（AP）到RIS控制器的相位移信息传输开销。

Method: 在AP处计算相位移，将其编码为压缩的二进制控制消息发送至RIS控制器。使用基于有效信道的加权最小均方误差（WMMSE）更新波束赋形器以减少相位移压缩误差。将WMMSE算法展开到深度学习架构中，实现联合相位移压缩和WMMSE波束赋形的端到端训练。

Result: 在控制比特数低于RIS单元数的情况下，考虑相位移压缩误差的波束赋形能显著提高和速率性能。

Conclusion: 提出的模型结合了深度学习和WMMSE算法，能够有效地处理RIS辅助通信中的相位移压缩误差，并在低比特数限制下实现性能提升。

Abstract: A model-based deep learning (DL) architecture is proposed for reconfigurable
intelligent surface (RIS)-assisted multi-user communications to reduce the
overhead of transmitting phase shift information from the access point (AP) to
the RIS controller. The phase shifts are computed at the AP, which has access
to the channel state information, and then encoded into a compressed binary
control message that is sent to the RIS controller for element configuration.
To help reduce beamformer mismatches due to phase shift compression errors, the
beamformer is updated using weighted minimum mean square error (WMMSE) based on
the effective channel resulting from the actual (decompressed) RIS reflection
coefficients. By unrolling the iterative WMMSE algorithm as part of the
wireless communication informed DL architecture, joint phase shift compression
and WMMSE beamforming can be trained end-to-end. Simulations show that
accounting for phase shift compression errors during beamforming significantly
improves the sum-rate performance, even when the number of control bits is
lower than the number of RIS elements.

</details>


### [378] [Efficient Coherence Inference Using the Demodulated Band Transform and a Generalized Linear Model](https://arxiv.org/abs/2510.05559)
*Md Rakibul Mowla,Sukhbinder Kumar,Ariane E. Rhone,Brian J. Dlouhy,Christopher K. Kovach*

Main category: eess.SP

TL;DR: 本研究提出了一种基于广义线性模型（GLM）的参数化方法，用于替代计算成本高且p值不稳定的传统时间/相位随机化方法，以统计检验神经信号的相干性。


<details>
  <summary>Details</summary>
Motivation: 传统的基于代理的统计显著性检验方法（如时间-移位或相位随机化）在处理大规模EEG/iEEG数据时计算成本高昂且p值不稳定，限制了其可扩展性。

Method: 使用广义线性模型（GLM）对复值时频系数（如DBT或STFT）进行建模，并采用似然比检验来评估神经信号的相干性。

Result: GLM方法在模拟和真实数据中均表现出与传统方法相当甚至更优的灵敏度，同时生成连续、稳定的p值，并且计算速度显著提高（快近200倍）。在80%的检测效力下，GLM能检测到C=0.25的相干性，而代理测试需要C=0.49。

Conclusion: 基于GLM的推理方法为处理大规模EEG/iEEG数据集提供了一种稳健、可扩展且高效的替代方案，能够有效地进行跨通道、跨频率和跨参与者的相干性分析。

Abstract: Statistical significance testing of neural coherence is essential for
distinguishing genuine cross-signal coupling from spurious correlations. A
widely accepted approach uses surrogate-based inference, where null
distributions are generated via time-shift or phase-randomization procedures.
While effective, these methods are computationally expensive and yield discrete
p-values that can be unstable near decision thresholds, limiting scalability to
large EEG/iEEG datasets. We introduce and validate a parametric alternative
based on a generalized linear model (GLM) applied to complex-valued
time--frequency coefficients (e.g., from DBT or STFT), using a likelihood-ratio
test. Using real respiration belt traces as a driver and simulated neural
signals contaminated with broadband Gaussian noise, we perform dense sweeps of
ground-truth coherence and compare GLM-based inference against
time-shift/phase-randomized surrogate testing under matched conditions. GLM
achieved comparable or superior sensitivity while producing continuous, stable
p-values and a substantial computational advantage. At 80% detection power, GLM
detects at C=0.25, whereas surrogate testing requires C=0.49, corresponding to
an approximately 6--7 dB SNR improvement. Runtime benchmarking showed GLM to be
nearly 200x faster than surrogate approaches. These results establish GLM-based
inference on complex time--frequency coefficients as a robust, scalable
alternative to surrogate testing, enabling efficient analysis of large EEG/iEEG
datasets across channels, frequencies, and participants.

</details>


### [379] [Time-reassigned synchrosqueezing frequency-domain chirplet transform for multicomponent signals with intersecting group delay curves](https://arxiv.org/abs/2510.06173)
*Shuixin Li,Jiecheng Chen,Qingtang Jiang,Lin Li*

Main category: eess.SP

TL;DR: TSST及其变体在处理具有快速频率变化或瞬态分量的信号方面表现良好，但它们在处理具有相交群延迟（GD）曲线的多分量信号方面存在局限性。本文提出了一种新颖的三维时频-群延迟色散（TF-GDD）表示，并引入了时间重分配同步压缩频域चirplet变换（TSFCT）和频域群信号分离运算（FGSSO）来克服这些局限性，从而实现更精确的GD估计和模式检索。


<details>
  <summary>Details</summary>
Motivation: 传统的时间重分配同步压缩变换（TSST）在处理具有相交群延迟（GD）曲线的多分量信号时存在局限性，这会影响特征提取和信号分量恢复的准确性，并降低时频表示（TFR）的可解释性。这在需要精确测量群延迟色散（GDD）的宽带信号处理系统中尤其成问题。

Method: 提出了一种基于频域चirplet变换的新颖三维时频-群延迟色散（TF-GDD）表示。然后引入时间重分配同步压缩频域चirplet变换（TSFCT）以获得更清晰的TF-GDD分布和更准确的GD估计。最后，提出了一种新颖的频域群信号分离运算（FGSSO）用于模式检索。

Result: 实验结果表明，所提出的TSFCT和FGSSO能够有效地估计GD，并检索模式，即使对于具有相交GD轨迹的模式也是如此。

Conclusion: 本文提出的TF-GDD表示、TSFCT和FGSSO能够有效地处理具有相交GD曲线的多分量信号，提高了GD估计和模式检索的准确性，克服了传统TSST方法的局限性。

Abstract: To analyze signals with rapid frequency variations or transient components,
the time-reassigned synchrosqueezing transform (TSST) and its variants have
been recently proposed. Unlike the traditional synchrosqueezing transform, TSST
squeezes the time-frequency (TF) coefficients along the group delay (GD)
trajectories rather than the instantaneous frequency trajectories. Although
TSST methods perform well in analyzing transient signals, they are
fundamentally limited in processing multicomponent signals with intersecting GD
curves. This limitation compromises the accuracy of both feature extraction and
signal component recovery, thereby significantly reducing the interpretability
of time-frequency representations (TFRs). This is particularly problematic in
broadband signal processing systems, where the linearity of the phase response
is critical and precise measurement of group delay dispersion (GDD) is
essential.
  Motivated by the superior capability of frequency-domain signal modeling in
characterizing rapidly frequency-varying signals, this paper proposes a novel
three-dimensional time-frequency-group delay dispersion (TF-GDD) representation
based on the frequency-domain chirplet transform. A subsequent time-reassigned
synchrosqueezing frequency-domain chirplet transform (TSFCT) is introduced to
achieve a sharper TF-GDD distribution and more accurate GD estimation. For mode
retrieval, a novel frequency-domain group signal separation operation (FGSSO)
is proposed.The theoretical contributions include a derivation of the
approximation error for the GD and GDD reference functions and an establishment
of the error bounds for FGSSO-based mode retrieval. Experimental results
demonstrate that the proposed TSFCT and FGSSO effectively estimate GDs and
retrieve modes--even for modes with intersecting GD trajectories.

</details>


### [380] [Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals](https://arxiv.org/abs/2510.05826)
*Pubudu L. Indrasiri,Bipasha Kashyap,Pubudu N. Pathirana*

Main category: eess.SP

TL;DR: 本研究利用改进的视觉 Transformer (ViT) 模型，结合卷积神经网络 (CNN) 和 SE 模块，通过分析心电图 (ECG) 图像来识别情绪状态，并在 YAAD 和 DREAMER 数据集上取得了优于现有方法的性能。


<details>
  <summary>Details</summary>
Motivation: 生物信号（如心电图）不仅能诊断疾病，还能揭示情绪状态与生理反应之间的联系。情绪唤起、压力水平和自主神经系统活动都会引起心率变异性的变化，这为理解情绪的生理基础提供了途径。传统方法在处理这些复杂信号方面存在局限性。

Method: 本研究首先评估了 Vision Transformer (ViT) 模型在识别心电图图像中的情绪方面的有效性。随后，提出并评估了一种改进版的 ViT 模型，该模型集成了 CNN 和 SE 模块，以提高在处理与情绪检测相关的心电图图像时的性能。具体方法包括两个阶段：1. 信号预处理和图像转换，利用连续小波变换和功率谱密度分析技术对信号进行净化并转换为可解释的图像。2. 提出一种增强型 ViT 架构，结合了 CNN 模块，以增强情绪识别能力。

Result: 在 YAAD 数据集上，本研究的方法在分类七种不同的情绪状态、以及效价（valence）和唤起（arousal）方面，均优于现有的最先进方法。在 DREAMER 数据集上，本研究的方法在区分效价、唤起和优势度（dominance）方面也表现出色，超过了当前领先的技术。

Conclusion: 本研究提出的结合了 CNN 和 SE 模块的增强型 ViT 模型，能够有效地从心电图图像中识别情绪状态，并在多个数据集上取得了优于现有方法的性能，证明了其在情绪识别领域的潜力和先进性。

Abstract: Biomedical signals provide insights into various conditions affecting the
human body. Beyond diagnostic capabilities, these signals offer a deeper
understanding of how specific organs respond to an individual's emotions and
feelings. For instance, ECG data can reveal changes in heart rate variability
linked to emotional arousal, stress levels, and autonomic nervous system
activity. This data offers a window into the physiological basis of our
emotional states. Recent advancements in the field diverge from conventional
approaches by leveraging the power of advanced transformer architectures, which
surpass traditional machine learning and deep learning methods. We begin by
assessing the effectiveness of the Vision Transformer (ViT), a forefront model
in image classification, for identifying emotions in imaged ECGs. Following
this, we present and evaluate an improved version of ViT, integrating both CNN
and SE blocks, aiming to bolster performance on imaged ECGs associated with
emotion detection. Our method unfolds in two critical phases: first, we apply
advanced preprocessing techniques for signal purification and converting
signals into interpretable images using continuous wavelet transform and power
spectral density analysis; second, we unveil a performance-boosted vision
transformer architecture, cleverly enhanced with convolutional neural network
components, to adeptly tackle the challenges of emotion recognition. Our
methodology's robustness and innovation were thoroughly tested using ECG data
from the YAAD and DREAMER datasets, leading to remarkable outcomes. For the
YAAD dataset, our approach outperformed existing state-of-the-art methods in
classifying seven unique emotional states, as well as in valence and arousal
classification. Similarly, in the DREAMER dataset, our method excelled in
distinguishing between valence, arousal and dominance, surpassing current
leading techniques.

</details>


### [381] [Time-causal and time-recursive wavelets](https://arxiv.org/abs/2510.05834)
*Tony Lindeberg*

Main category: eess.SP

TL;DR: 本研究提出了一种真正时域因果的小波分析方法，用于实时处理无法访问未来的时间信号。


<details>
  <summary>Details</summary>
Motivation: 在实时处理时间信号时，信号处理的每一步都必须基于真正时域因果的计算机制。

Method: 基于时间尺度空间理论，通过对时间平滑核进行分类，推导出指数核及其时间导数是唯一允许的核。通过选择特定方式的时间常数，确保时间尺度协方差，从而实现尺度自相似性，并将尺度空间理论与小波理论联系起来。

Result: 提出了一种时间因果小波表示，能够反映输入信号中局部主导时间结构的持续时间。

Conclusion: 这种时间因果小波分析可作为信号处理任务的宝贵工具，尤其适用于需要完全时域因果分析才能在物理上现实的信号处理场景。

Abstract: When to apply wavelet analysis to real-time temporal signals, where the
future cannot be accessed, it is essential to base all the steps in the signal
processing pipeline on computational mechanisms that are truly time-causal.
  This paper describes how a time-causal wavelet analysis can be performed
based on concepts developed in the area of temporal scale-space theory,
originating from a complete classification of temporal smoothing kernels that
guarantee non-creation of new structures from finer to coarser temporal scale
levels. By necessity, convolution with truncated exponential kernels in cascade
constitutes the only permissable class of kernels, as well as their temporal
derivatives as a natural complement to fulfil the admissibility conditions of
wavelet representations. For a particular way of choosing the time constants in
the resulting infinite convolution of truncated exponential kernels, to ensure
temporal scale covariance and thus self-similarity over temporal scales, we
describe how mother wavelets can be chosen as temporal derivatives of the
resulting time-causal limit kernel.
  By developing connections between wavelet theory and scale-space theory, we
characterize and quantify how the continuous scaling properties transfer to the
discrete implementation, demonstrating how the proposed time-causal wavelet
representation can reflect the duration of locally dominant temporal structures
in the input signals.
  We propose that this notion of time-causal wavelet analysis could be a
valuable tool for signal processing tasks, where streams of signals are to be
processed in real time, specifically for signals that may contain local
variations over a rich span of temporal scales, or more generally for analysing
physical or biophysical temporal phenomena, where a fully time-causal analysis
is called for to be physically realistic.

</details>


<div id='cs.LO'></div>

# cs.LO [[Back]](#toc)

### [382] [On the Interplay of Cube Learning and Dependency Schemes in QCDCL Proof Systems](https://arxiv.org/abs/2510.05876)
*Abhimanyu Choudhury,Meena Mahajan*

Main category: cs.LO

TL;DR: QCDCL是解决量化布尔公式（QBF）的主要方法之一。本文形式化了使用子句学习和依赖关系的QCDCL求解器的推理过程，并给出了其可靠性和完备性的充要条件。


<details>
  <summary>Details</summary>
Motivation: 现有QCDCL方法在处理QBF时存在不足，需要更完善的理论基础来支持其求解过程，特别是结合子句学习和依赖关系的处理。

Method: 形式化了使用子句学习和依赖关系（包括标准和自反关系依赖模式）的QCDCL求解器的推理过程，并分析了其可靠性和完密性。

Result: 证明了使用标准和自反关系依赖模式可以缩短反驳过程。在决策顺序受限于量化顺序时，结合子句学习和依赖关系的处理，详细研究了由此产生的证明系统及其相对强度。

Conclusion: 本文提出的基于子句学习和依赖关系的QCDCL证明系统在理论上是可靠和完备的，并且能够通过缩短反驳过程来提高求解效率。

Abstract: Quantified Conflict Driven Clause Leaning (QCDCL) is one of the main
approaches to solving Quantified Boolean Formulas (QBF). Cube-learning is
employed in this approach to ensure that true formulas can be verified.
Dependency Schemes help to detect spurious dependencies that are implied by the
variable ordering in the quantifier prefix of QBFs but are not essential for
constructing (counter)models. This detection can provably shorten refutations
in specific proof systems, and is expected to speed up runs of QBF solvers.
  The simplest underlying proof system [BeyersdorffB\"ohm-LMCS2023], formalises
the reasoning in the QCDCL approach on false formulas, when neither cube
learning nor dependency schemes is used. The work of
[B\"ohmPeitlBeyersdorff-AI2024] further incorporates cube-learning. The work of
[ChoudhuryMahajan-JAR2024] incorporates a limited use of dependency schemes,
but without cube-learning.
  In this work, proof systems underlying the reasoning of QCDCL solvers which
use cube learning, and which use dependency schemes at all stages, are
formalised. Sufficient conditions for soundness and completeness are presented,
and it is shown that using the standard and reflexive resolution path
dependency schemes ($D^{std}$ and $D^{rrs}$) to relax the decision order
provably shortens refutations.
  When the decisions are restricted to follow quantification order, but
dependency schemes are used in propagation and learning, in conjunction with
cube-learning, the resulting proof systems using the dependency schemes
$D^{std}$ and $D^{rrs}$ are investigated in detail and their relative strengths
are analysed.

</details>


### [383] [On Equivalent Characterizations of NP in Abstract Models of Computation](https://arxiv.org/abs/2510.05894)
*Jeremy C. Kirn,Lucas Meijer,Tillmann Miltzow,Hans L. Bodlaender*

Main category: cs.LO

TL;DR: 研究了类似图灵机的机器模型，并证明在弱条件下，NP(R)复杂度类可以通过三种等价方式进行表征。


<details>
  <summary>Details</summary>
Motivation: 在弱条件下，将 NP(R) 复杂度类通过三种等价方式进行表征，并推广到整个多项式层级。

Method: 通过类似图灵机的机器模型，并结合 R 结构的操作，利用多项式时间验证算法、SAT(R) 问题以及 R 上的存在 second-order 元有限逻辑。

Result: 在弱条件下，NP(R) 可以通过多项式时间验证算法、SAT(R) 和存在 second-order 元有限逻辑进行表征。即使在 NP(R) 没有完全问题的无穷词汇结构中，NP(R) 也可以通过存在 second-order 元有限逻辑进行表征。还得到了 R 上常数无关布尔部分 NP(R) 的类似结果，并将其推广到多项式层级和布尔层级。

Conclusion: 描述性复杂度理论非常适合处理无穷词汇结构。NP(R) 的三种表征以及对多项式层级和布尔层级的推广，为理解这些模型提供了统一的框架。

Abstract: We investigate machine models similar to Turing machines that are augmented
by the operations of a first-order structure $\mathcal{R}$, and we show that
under weak conditions on $\mathcal{R}$, the complexity class
$\text{NP}(\mathcal{R})$ may be characterized in three equivalent ways: (1) by
polynomial-time verification algorithms implemented on $\mathcal{R}$-machines,
(2) by the $\text{NP}(\mathcal{R})$-complete problem $\text{SAT}(\mathcal{R})$,
and (3) by existential second-order metafinite logic over $\mathcal{R}$ via
descriptive complexity. By characterizing $\text{NP}(\mathcal{R})$ in these
three ways, we extend previous work and embed it in one coherent framework.
  Some conditions on $\mathcal{R}$ must be assumed in order to achieve the
above trinity because there are infinite-vocabulary structures for which
$\text{NP}(\mathcal{R})$ does not have a complete problem. Surprisingly, even
in these cases, we show that $\text{NP}(\mathcal{R})$ does have a
characterization in terms of existential second-order metafinite logic,
suggesting that descriptive complexity theory is well suited to working with
infinite-vocabulary structures, such as real vector spaces.
  In addition, we derive similar results for $\exists\mathcal{R}$, the
constant-free Boolean part of $\text{NP}(\mathcal{R})$, by showing that
$\exists\mathcal{R}$ may be characterized in three analogous ways. We then
extend our results to the entire polynomial hierarchy over $\mathcal{R}$ and to
its constant-free Boolean counterpart, the Boolean hierarchy over
$\mathcal{R}$. Finally, we give a characterization of the polynomial and
Boolean hierarchies over $\mathcal{R}$ in terms of oracle
$\mathcal{R}$-machines.

</details>


### [384] [A Timed Obstruction Logic for Dynamic Game Models](https://arxiv.org/abs/2510.06045)
*David Cortes,Jean Leneutre,Vadim Malvone,James Ortiz*

Main category: cs.LO

TL;DR: 该论文提出了一种名为“时序障碍逻辑”（TOL）的新型逻辑，用于验证实时网络安全和隐私应用中的定时博弈。


<details>
  <summary>Details</summary>
Motivation: 实时网络安全和隐私应用需要可靠的验证方法和系统设计工具来确保其正确性。这些嵌入在机场、医院和输油管道等基础设施中的反应式实时应用容易受到恶意网络攻击。因此，需要一个理论基础来模拟攻击者和防御者之间的战略互动。

Method: 提出时序障碍逻辑（TOL），它是障碍逻辑（OL）的扩展，用于验证具有实时目标和动态模型的特定时序博弈。TOL能够描述涉及离散和连续动作的玩家对时序博弈模型的影响。

Result: TOL可以用来描述实时网络安全博弈的重要时序属性。该论文还提供了一种TOL的验证程序，并证明其复杂度为PSPACE-complete。

Conclusion: TOL在不增加复杂性的情况下提高了属性的可表达性，使其成为验证实时网络安全和隐私应用的有用工具。

Abstract: Real-time cybersecurity and privacy applications require reliable
verification methods and system design tools to ensure their correctness. Many
of these reactive real-time applications embedded in various infrastructures,
such as airports, hospitals, and oil pipelines, are potentially vulnerable to
malicious cyber-attacks. Recently, a growing literature has recognized Timed
Game Theory as a sound theoretical foundation for modeling strategic
interactions between attackers and defenders. This paper proposes Timed
Obstruction Logic (TOL), an extension of Obstruction Logic (OL), a formalism
for verifying specific timed games with real-time objectives unfolding in
dynamic models. These timed games involve players whose discrete and continuous
actions can impact the underlying timed game model. We show that TOL can be
used to describe important timed properties of real-time cybersecurity games.
Finally, in addition to introducing our new logic and adapting it to specify
properties in the context of cybersecurity, we provide a verification procedure
for TOL and show that its complexity is PSPACE-complete, meaning that it is not
higher than that of classical timed temporal logics like TCTL. Thus, we
increase the expressiveness of properties without incurring any cost in terms
of complexity.

</details>


<div id='eess.SY'></div>

# eess.SY [[Back]](#toc)

### [385] [Pricing Short-Circuit Current via a Primal-Dual Formulation for Preserving Integrality Constraints](https://arxiv.org/abs/2510.05293)
*Peng Wang,Luis Badesa*

Main category: eess.SY

TL;DR: 同步发电机（SG）提供的短路电流（SCC）对线路保护至关重要，但随着SG被电力电子设备取代，优化剩余SG的SCC供应变得必要。然而，由于单元承诺（UC）中的积分约束，SCC的定价面临挑战。现有方法存在局限性，无法有效处理二元变量，导致SCC价格无法覆盖运营成本或缺乏可解释性。本文提出了一种原对偶（primal-dual）方法，用于SCC约束调度，该方法在有效计算SCC服务影子价格的同时，保持了UC的二元性质。通过修改后的IEEE 30节点系统进行测试，并与现有定价方案进行比较，证明了原对偶方法在SCC定价中保持UC积分性方面的优势。


<details>
  <summary>Details</summary>
Motivation: 同步发电机（SG）提供的短路电流（SCC）作为一种关键的辅助服务，对于确保短路故障期间线路保护的跳闸至关重要。然而，随着SG被电力电子设备取代，其提供的SCC水平不断下降，因此有必要优化剩余SG所提供的SCC。但由于单元承诺（UC）中存在积分约束，SCC的定价变得具有挑战性。

Method: 提出了一种SCC约束调度的原对偶（primal-dual）公式，该公式在有效计算SCC服务影子价格的同时，保持了UC的二元性质。

Result: 通过在修改后的IEEE 30节点系统上进行比较，证明了原对偶方法在SCC定价中保持UC积分性方面优于现有技术。

Conclusion: 所提出的原对偶方法能够有效计算SCC服务的影子价格，同时保持UC的积分性，克服了现有定价方法的局限性。

Abstract: Synchronous Generators (SGs) currently provide important levels of
Short-Circuit Current (SCC), a critical ancillary service that ensures line
protections trip during short-circuit faults. Given the ongoing replacement of
SGs by power-electronics-based generation, which have a hard limit for current
injection, it has become relevant to optimize the procurement of SCC provided
by remaining SGs. Pricing this service is however challenging due to the
integrality constraints in Unit Commitment (UC). Existing methods, e.g.,
dispatchable pricing, restricted pricing and marginal unit pricing, attempt to
address this issue but exhibit limitations in handling binary variables,
resulting in SCC prices that either fail to cover the operating costs of units
or lack interpretability. To overcome these pitfalls, we propose a primal-dual
formulation of the SCC-constrained dispatch that preserves the binary nature of
UC while effectively computing shadow prices of SCC services. Using a modified
IEEE 30-bus system, a comparison is carried out between the proposed approach
and the state-of-the-art pricing schemes, highlighting the advantages of the
primal-dual method in preserving UC integrality for SCC pricing.

</details>


### [386] [Robust Sensor Placement for Poisson Arrivals with False Alarm Aware Spatiotemporal Sensing](https://arxiv.org/abs/2510.05343)
*Mingyu Kim,Pronoy Sarker,Seungmo Kim,Daniel J. Stilwell,Jorge Jimenez*

Main category: eess.SY

TL;DR: 本篇论文提出了一种用于处理传感器在时空变化的随机环境因素和存在误报的情况下进行部署的统一模型。该模型通过一个可用性函数将检测和误报耦合起来，该函数捕捉误报如何降低有效传感和对干扰的过滤响应。


<details>
  <summary>Details</summary>
Motivation: 研究在环境因素导致检测性能随时间和空间随机变化且存在误报的情况下，如何进行传感器部署。

Method: 引入一个统一模型，通过可用性函数将检测和误报耦合起来，并推导了覆盖范围相关的空置概率下界，证明了过滤能够提高检测性能的充分条件，并提供了鲁棒性保证。

Result: 提出的模型在数值研究中得到了验证，使用了AIS船舶交通数据和合成海事场景。

Conclusion: 该研究为在动态、不确定的环境中部署传感器提供了理论和实践指导。

Abstract: This paper studies sensor placement when detection performance varies
stochastically due to environmental factors over space and time and false
alarms are present, but a filter is used to attenuate the effect. We introduce
a unified model that couples detection and false alarms through an availability
function, which captures how false alarms reduce effective sensing and
filtering responses to the disturbance. Building on this model, we give a
sufficient condition under which filtering improves detection. In addition, we
derive a coverage-based lower bound on the void probability. Furthermore, we
prove robustness guarantees showing that performance remains stable when
detection probabilities are learned from limited data. We validate the approach
with numerical studies using AIS vessel-traffic data and synthetic maritime
scenarios. Together, these results provide theory and practical guidance for
deploying sensors in dynamic, uncertain environments.

</details>


### [387] [Koopman Control Factorization: Data-Driven Convex Controller Design for a Class of Nonlinear Systems](https://arxiv.org/abs/2510.05359)
*Taha Ondogan,Ran Jing,Andrew P. Sabelhaus,Roberto Tron*

Main category: eess.SY

TL;DR: 


<details>
  <summary>Details</summary>
Motivation: For nonautonomous systems, designing state (or output) feedback controllers is typically a nonconvex problem. This paper aims to address this gap by introducing a novel parameterization and a new control synthesis method.

Method: The paper introduces the Koopman Control Factorization, a new way to parameterize control-affine dynamical systems and define a feedback controller. This allows the Koopman operator of the closed-loop system to be expressed as a bilinear combination of matrices representing the system and controller. The paper also presents an algorithm using semi-definite programming to calculate the feedback matrix, ensuring a convex optimization and Lyapunov stability.

Result: The proposed controllers were evaluated on inverted pendulum systems, demonstrating successful stabilization using properly chosen basis functions.

Conclusion: This work presents a generalizable, efficient, and verifiably stable control synthesis method for nonlinear systems that is data-driven and does not rely on approximations.

Abstract: Although Koopman operators provide a global linearization for autonomous
dynamical systems, nonautonomous systems are not globally linear in the inputs.
State (or output) feedback controller design therefore remains nonconvex in
typical formulations, even with approximations via bilinear control-affine
terms. We address this gap by introducing the Koopman Control Factorization, a
novel parameterization of control-affine dynamical systems combined with a
feedback controller defined as a linear combination of nonlinear measurements.
With this choice, the Koopman operator of the closed-loop system is a bilinear
combination of the coefficients in two matrices: one representing the system,
and the other the controller. We propose a set of sufficient conditions such
that the factorization holds. Then, we present an algorithm that calculates the
feedback matrix via semi-definite programming, producing a Lyapunov-stable
closed-loop system with convex optimization. We evaluate the proposed
controllers on two canonical examples of control-affine nonlinear systems
(inverted pendulums), and show that our factorization and controller
successfully stabilize both under properly-chosen basis functions. This
manuscript introduces a broadly generalizable control synthesis method for
stabilization of nonlinear systems that is quick-to-compute, verifiably stable,
data-driven, and does not rely on approximations.

</details>


### [388] [Digital Twins for Intelligent Intersections: A Literature Review](https://arxiv.org/abs/2510.05374)
*Alben Rome Bagabaldo,Jürgen Hackl*

Main category: eess.SY

TL;DR: 数字孪生技术可用于增强城市交通交叉口的安全性与效率。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨数字孪生技术在智能交通系统中的应用，以提升城市交通效率与安全性。

Method: 通过对现有研究进行系统性分类，分析数字孪生在智能交叉口领域的应用，重点关注五大主题：数字孪生架构、数据处理与仿真、人工智能交通控制、弱势道路使用者安全以及从局部到城市规模的扩展性。

Result: 研究表明，融合实时数据与人工智能决策的多层数字孪生架构能显著提升交通效率与安全。先进的仿真技术与人工智能算法在实时响应与预测方面表现出色，并在保护弱势道路使用者方面展现巨大潜力。

Conclusion: 尽管数字孪生技术在智能交叉口管理方面取得了显著进展，但在数据互操作性、大规模部署以及处理动态城市环境中的不确定性方面仍面临挑战。解决这些挑战对于未来开发和部署智能、自适应和可持续的交叉口管理系统至关重要。

Abstract: Intelligent intersections play a pivotal role in urban mobility, demanding
innovative solutions such as digital twins to enhance safety and efficiency.
This literature review investigates the integration and application of digital
twins for intelligent intersections, a critical area within smart urban traffic
systems. The review systematically categorizes existing research into five key
thematic areas: (i) Digital Twin Architectures and Frameworks; (ii) Data
Processing and Simulation Techniques; (iii) Artificial Intelligence and Machine
Learning for Adaptive Traffic Control; (iv) Safety and Protection of Vulnerable
Road Users; and (v) Scaling from Localized Intersections to Citywide Traffic
Networks. Each theme is explored comprehensively, highlighting significant
advancements, current challenges, and critical insights. The findings reveal
that multi-layered digital twin architectures incorporating real-time data
fusion and AI-driven decision-making enhances traffic efficiency and safety.
Advanced simulation techniques combined with sophisticated AI/ML algorithms
demonstrate notable improvements in real-time responsiveness and predictive
accuracy for traffic management. Additionally, the integration of digital twins
has shown substantial promise in safeguarding vulnerable road users through
proactive and adaptive safety strategies. Despite these advancements, key
challenges persist, including interoperability of diverse data sources,
scalability of digital twins for extensive traffic networks, and managing
uncertainty within dynamic urban environments. Addressing these challenges will
be essential for the future development and deployment of intelligent,
adaptive, and sustainable intersection management systems.

</details>


### [389] [Safety-Critical Control with Bounded Inputs: A Closed-Form Solution for Backup Control Barrier Functions](https://arxiv.org/abs/2510.05436)
*David E. J. van Wijk,Ersin Das,Tamas G. Molnar,Aaron D. Ames,Joel W. Burdick*

Main category: eess.SY

TL;DR: bCBFs is a method for synthesizing safe controllers, but it is computationally expensive. This paper proposes a closed-form interpolation method between a nominal and a backup controller that is computationally efficient and guarantees safety.


<details>
  <summary>Details</summary>
Motivation: Verifying the safety of controllers with bounded inputs is challenging. Backup control barrier functions (bCBFs) offer a structured approach but are computationally expensive due to high-dimensional quadratic programming at run-time, making them unsuitable for computationally-constrained systems.

Method: The paper proposes an approach that optimally interpolates between a nominal controller and a backup controller. The solution to this optimization problem is derived in closed form, ensuring safety and adherence to input bounds.

Result: The proposed closed-form controller is proven to be safe and respects input bounds. Its effectiveness is demonstrated on a double integrator and a nonlinear fixed-wing aircraft example.

Conclusion: The proposed closed-form interpolation method offers a computationally efficient and safe alternative to bCBFs for systems with bounded inputs, validated through simulations on two examples.

Abstract: Verifying the safety of controllers is critical for many applications, but is
especially challenging for systems with bounded inputs. Backup control barrier
functions (bCBFs) offer a structured approach to synthesizing safe controllers
that are guaranteed to satisfy input bounds by leveraging the knowledge of a
backup controller. While powerful, bCBFs require solving a high-dimensional
quadratic program at run-time, which may be too costly for
computationally-constrained systems such as aerospace vehicles. We propose an
approach that optimally interpolates between a nominal controller and the
backup controller, and we derive the solution to this optimization problem in
closed form. We prove that this closed-form controller is guaranteed to be safe
while obeying input bounds. We demonstrate the effectiveness of the approach on
a double integrator and a nonlinear fixed-wing aircraft example.

</details>


### [390] [Operational Risks in Grid Integration of Large Data Center Loads: Characteristics, Stability Assessments, and Sensitivity Studies](https://arxiv.org/abs/2510.05437)
*Kyung-Bin Kwon,Sayak Mukherjee,Veronica Adetola*

Main category: eess.SY

TL;DR: 本文研究了大型数据中心和电网之间的动态交互，重点关注因需求突然波动而产生的可靠性挑战。随着人工智能驱动的工作负载的快速增长，这些波动以及快速的斜坡模式预计将加剧电网的压力和系统不稳定性。我们考虑了来自麻省理工学院超级计算中心数据集的几个开源人工智能数据中心消费模型，并生成了几个基于实验高性能计算作业分布的推理模型。随后，我们开发了实时评估电网稳定性的分析方法，重点关注瞬态和小信号稳定性评估。通过计算局部数据中心总线动能流和随时间窗口变化的邻近总线耦合相互作用而产生的非线性瞬态稳定性能的能量流度量，帮助操作员实时评估数据中心枢纽区域电网的压力。另一方面，在快速斜坡期间，根据可变操作条件构建的分析状态矩阵，小信号稳定性度量能够基于快照评估数据中心负载波动，从而增强对不断变化的电网状况的观测能力。通过量化大型数据中心集群的稳定性影响，在修改后的 IEEE 68 总线基准模型中进行的研究有助于提高操作员的态势感知能力，从而抓住可靠集成大型数据中心负载的风险。


<details>
  <summary>Details</summary>
Motivation: AI工作负载的快速增长导致数据中心需求突然波动，加剧了电网压力和不稳定性。

Method: 开发了实时评估瞬态和中小信号稳定性的分析方法。瞬态稳定性使用能量流度量，中小信号稳定性使用分析状态矩阵。

Result: 研究量化了大型数据中心集群对稳定性的影响，改进了 IEEE 68 总线基准模型，提高了电网运营商的态势感知能力，以应对可靠集成大型数据中心负载的风险。

Conclusion: 所提出的分析方法和模型有助于提高电网运营商的态势感知能力，从而更可靠地将大型数据中心负载集成到电网中。

Abstract: This paper investigates the dynamic interactions between large-scale data
centers and the power grid, focusing on reliability challenges arising from
sudden fluctuations in demand. With the rapid growth of AI-driven workloads,
such fluctuations, along with fast ramp patterns, are expected to exacerbate
stressed grid conditions and system instabilities. We consider a few
open-source AI data center consumption profiles from the MIT supercloud
datasets, along with generating a few experimental HPC job-distribution-based
inference profiles. Subsequently, we develop analytical methodologies for
real-time assessment of grid stability, focusing on both transient and
small-signal stability assessments. Energy-flow-like metrics for nonlinear
transient stability, formulated by computing localized data center bus
kinetic-like flows and coupling interactions with neighboring buses over
varying time windows, help provide operators a real-time assessments of the
regional grid stress in the data center hubs. On the other hand, small-signal
stability metrics, constructed from analytical state matrices under variable
operating conditions during a fast ramping period, enable snapshot-based
assessments of data center load fluctuations, provide enhanced observability
into evolving grid conditions. By quantifying the stability impacts of large
data center clusters, studies conducted in the modified IEEE benchmark $68-$bus
model support improved operator situational awareness to capture risks in
reliable integration of large data center loads.

</details>


### [391] [A Predictive and Sampled-Data Barrier Method for Safe and Efficient Quadrotor Control](https://arxiv.org/abs/2510.05456)
*Ming Gao,Zhanglin Shangguan,Shuo Liu,Liang Wu,Bo Yang,Wei Xiao*

Main category: eess.SY

TL;DR: 该研究提出了一种用于四旋翼飞行器轨迹跟踪的级联控制框架，并提供了正式的安全保证。


<details>
  <summary>Details</summary>
Motivation: 为了解决四旋翼飞行器轨迹跟踪中的安全约束问题，特别是当约束具有高相对阶数时。

Method: 设计了一个包含外环位置模型预测控制（MPC）和内环非线性姿态控制的控制器。采用高阶控制障碍函数（HOCBFs）来保证安全，并将其扩展为采样数据 HOCBFs（SdHOCBFs），以处理整个采样间隔内的安全问题。将 SdHOCBFs 作为控制仿射约束嵌入 MPC 框架中。

Result: 提出的方法在保证安全性和最优性的同时，保持了凸性，适用于实时实现。仿真结果表明，与现有方法相比，该方法具有更高的安全性和效率。

Conclusion: 该研究成功提出了一种结合 MPC 和 SdHOCBFs 的级联控制框架，为四旋翼飞行器提供了正式的安全保证和高效的轨迹跟踪能力。

Abstract: This paper proposes a cascaded control framework for quadrotor trajectory
tracking with formal safety guarantees. First, we design a controller
consisting of an outer-loop position model predictive control (MPC) and an
inner-loop nonlinear attitude control, enabling decoupling of position safety
and yaw orientation. Second, since quadrotor safety constraints often involve
high relative degree, we adopt high order control barrier functions (HOCBFs) to
guarantee safety. To employ HOCBFs in the MPC formulation that has formal
guarantees, we extend HOCBFs to sampled-data HOCBF (SdHOCBFs) by introducing
compensation terms, ensuring safety over the entire sampling interval. We show
that embedding SdHOCBFs as control-affine constraints into the MPC formulation
guarantees both safety and optimality while preserving convexity for real-time
implementations. Finally, comprehensive simulations are conducted to
demonstrate the safety guarantee and high efficiency of the proposed method
compared to existing methods.

</details>


### [392] [Sample-Efficient and Smooth Cross-Entropy Method Model Predictive Control Using Deterministic Samples](https://arxiv.org/abs/2510.05706)
*Markus Walker,Daniel Frisch,Uwe D. Hanebeck*

Main category: eess.SY

TL;DR: dsCEM通过使用确定性采样取代随机采样来改进CEM-MPC，以提高控制性能和输入平滑度。


<details>
  <summary>Details</summary>
Motivation: 传统的交叉熵方法模型预测控制（CEM-MPC）受限于随机采样，导致探索效率低下和控制输入不平滑。

Method: 提出了一种确定性采样CEM（dsCEM）框架，使用基于局部累积分布（LCD）的确定性样本，并结合时间相关性来生成和适应样本集，以实现平滑的控制轨迹。

Result: dsCEM 在两个非线性控制任务上的实验评估表明，其累积成本和控制输入平滑度优于最先进的 iCEM，尤其是在样本量较少的情况下。

Conclusion: dsCEM 作为一种即插即用的替代方案，可以有效改进 CEM-MPC 的性能，特别是在样本量有限的情况下。

Abstract: Cross-entropy method model predictive control (CEM--MPC) is a powerful
gradient-free technique for nonlinear optimal control, but its performance is
often limited by the reliance on random sampling. This conventional approach
can lead to inefficient exploration of the solution space and non-smooth
control inputs, requiring a large number of samples to achieve satisfactory
results. To address these limitations, we propose deterministic sampling CEM
(dsCEM), a novel framework that replaces the random sampling step with
deterministic samples derived from localized cumulative distributions (LCDs).
Our approach introduces modular schemes to generate and adapt these sample
sets, incorporating temporal correlations to ensure smooth control
trajectories. This method can be used as a drop-in replacement for the sampling
step in existing CEM-based controllers. Experimental evaluations on two
nonlinear control tasks demonstrate that dsCEM consistently outperforms
state-of-the-art iCEM in terms of cumulative cost and control input smoothness,
particularly in the critical low-sample regime.

</details>


### [393] [Multi-Segment Photonic Power Converters for Energy Harvesting and High-Speed Optical Wireless Communication](https://arxiv.org/abs/2510.06205)
*Othman Younus,Behnaz Majlesein,Richard Nacke,Isaac N. O. Osahon,Carmine Pellegrino,Sina Babadi,Iman Tavakkolnia,Henning Helmers,Harald Haas*

Main category: eess.SY

TL;DR: 使用多段GaAs基PPC实现3.8Gbps的光学无线通信。


<details>
  <summary>Details</summary>
Motivation: 满足对高能效、高速无线通信和物联网设备的需求，需要集成储能和光数据接收系统以消除充电或更换电池的需要。

Method: 提出并测试了多段GaAs基PPC，将有源区分为2、4或6个子电池，并采用串联连接优化吸收并减小寄生效应。

Result: 在1.5米的光学无线链路上实现了3.8Gbps的世界纪录数据速率，比先前工作快四倍，并将2.3毫瓦光功率的39.7%转换为电能。

Conclusion: 多段GaAs基PPC为未来通信网络（如6G）提供了离网回程的解决方案。

Abstract: The demand for energy-efficient high-speed wireless communication, coupled
with the rapid rise of IoT devices, requires systems that integrate power
harvesting with optical data reception to eliminate the need for charging or
battery replacements. Recent advances have explored the use of solar cells as
optical receivers for high-speed data detection alongside power harvesting.
\acs{GaAs}-based \acp{PPC} provide six times greater electron mobility than
silicon- or cadmium telluride-based cells, enabling faster data detection and
improved power efficiency. However, their bandwidth is constrained by junction
capacitance, which increases with active area, creating a trade-off between
power output and data rate. To address this, we propose and test multi-segment
\acs{GaAs}-based \Acp{PPC} that serve as both energy harvesters and data
detectors. By segmenting the active area into 2, 4, or 6 subcells, forming
circular areas with diameters of 1, 1.5, or 2.08~mm, we reduce capacitance and
boost bandwidth while preserving light collection. Fabricated on a
semi-insulating \ac{GaAs} substrate with etched trenches for electrical
isolation, the series-connected subcells optimize absorption and minimize
parasitic effects. The \Acp{PPC} were used for an eye-safe 1.5~m optical
wireless link, employing \ac{OFDM} with adaptive bit and power loading. The
system achieved a world record data rate of 3.8~Gbps, which is four times
higher than prior works. The system converts 39.7\% of optical power from a
beam of 2.3~mW, although the segmentation increases the sensitivity of the
alignment. These findings provide new solutions for off-grid backhaul for
future communication networks, such as 6th generation (6G) cellular.

</details>


### [394] [Safe Landing on Small Celestial Bodies with Gravitational Uncertainty Using Disturbance Estimation and Control Barrier Functions](https://arxiv.org/abs/2510.05895)
*Felipe Arenas-Uribe,T. Michael Seigler,Jesse B. Hoagg*

Main category: eess.SY

TL;DR: 本文提出了一种结合轨迹跟踪、扰动估计和安全执行的自主软着陆控制方法，以应对小天体着陆的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有控制方法缺乏安全约束满足的形式化保证，而小天体着陆需要高度自主性来应对不确定的引力模型和动态环境。

Method: 使用扩展高增益观测器估计引力模型不确定性引起的扰动，然后采用反馈线性化和扰动抵消控制器实现参考轨迹的指数跟踪，最后使用基于控制障碍函数的最小干预控制器强制执行状态和输入约束。

Result: 数值模拟结果表明，该控制器能够实现精确且安全的软着陆，并且在燃料最优的轨迹下也能有效运行。

Conclusion: 该控制方法结合了轨迹跟踪和形式化的安全保证，符合航天器常见的制导与控制架构，能够在不影响安全的前提下执行激进的着陆机动，有潜力应用于小天体自主着陆任务。

Abstract: Soft landing on small celestial bodies (SCBs) poses unique challenges, as
uncertainties in gravitational models and poorly characterized, dynamic
environments require a high level of autonomy. Existing control approaches lack
formal guarantees for safety constraint satisfaction, necessary to ensure the
safe execution of the maneuvers. This paper introduces a control that addresses
this limitation by integrating trajectory tracking, disturbance estimation, and
safety enforcement. An extended high-gain observer is employed to estimate
disturbances resulting from gravitational model uncertainties. We then apply a
feedback-linearizing and disturbance-canceling controller that achieves
exponential tracking of reference trajectories. Finally, we use a control
barrier function based minimum-intervention controller to enforce state and
input constraints through out the maneuver execution. This control combines
trajectory tracking of offline generated reference trajectories with formal
guarantees of safety, which follows common guidance and control architectures
for spacecraft and allows aggressive maneuvers to be executed without
compromising safety. Numerical simulations using fuel-optimal trajectories
demonstrate the effectiveness of the controller in achieving precise and safe
soft-landing, highlighting its potential for autonomous SCB missions.

</details>


### [395] [Distributed Platoon Control Under Quantization: Stability Analysis and Privacy Preservation](https://arxiv.org/abs/2510.05959)
*Kaixiang Zhang,Zhaojian Li,Wei Lin*

Main category: eess.SY

TL;DR: 该论文研究了在确定性或概率性量化下，分布式车辆编队控制的稳定性和隐私保护性。确定性量化保证了系统误差是有界的，并且在没有辅助信息的情况下，窃听者无法推断车辆状态。概率性量化可以实现期望上的渐近收敛，并满足差分隐私保证，即使在窃听者拥有任意辅助信息的情况下也能保护隐私。论文还分析了控制性能和隐私之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 解决分布式车辆编队控制中数据共享带来的隐私泄露和潜在恶意活动风险。

Method: 分析了确定性量化和概率性量化两种量化策略下分布式编队控制的稳定性和隐私保护性，并进行了数值模拟。

Result: 确定性量化保证了系统误差有界，但隐私保护能力有限。概率性量化实现了期望上的渐近收敛，并提供了差分隐私保证，但可能影响控制性能。论文还量化了量化步长对控制性能和隐私的影响。

Conclusion: 在分布式车辆编队控制中，需要根据具体需求在控制性能和隐私保护之间进行权衡，概率性量化在隐私保护方面表现更优。

Abstract: Distributed control of connected and automated vehicles has attracted
considerable interest for its potential to improve traffic efficiency and
safety. However, such control schemes require sharing privacy-sensitive vehicle
data, which introduces risks of information leakage and potential malicious
activities. This paper investigates the stability and privacy-preserving
properties of distributed platoon control under two types of quantizers:
deterministic and probabilistic. For deterministic quantization, we show that
the resulting control strategy ensures the system errors remain uniformly
ultimately bounded. Moreover, in the absence of auxiliary information, an
eavesdropper cannot uniquely infer sensitive vehicle states. In contrast, the
use of probabilistic quantization enables asymptotic convergence of the vehicle
platoon in expectation with bounded variance. Importantly, probabilistic
quantizers can satisfy differential privacy guarantees, thereby preserving
privacy even when the eavesdropper possesses arbitrary auxiliary information.
We further analyze the trade-off between control performance and privacy by
formulating an optimization problem that characterizes the impact of the
quantization step on both metrics. Numerical simulations are provided to
illustrate the performance differences between the two quantization strategies.

</details>


### [396] [Optimal Batched Scheduling of Stochastic Processing Networks Using Atomic Action Decomposition](https://arxiv.org/abs/2510.06033)
*Jim Dai,Manxi Wu,Zhanhao Zhang*

Main category: eess.SY

TL;DR: We propose an atomic action decomposition framework to control stochastic processing networks (SPNs) efficiently and scalably, achieving optimal long-run average reward.


<details>
  <summary>Details</summary>
Motivation: The control of SPNs is challenging due to the exponential growth of the policy dimension with the number of servers, making standard methods intractable at scale.

Method: We developed an atomic action decomposition framework that breaks joint server assignments into sequential single-server assignments, resulting in policies with constant dimension regardless of the number of servers. We studied step-dependent and step-independent atomic policies.

Result: Both classes of atomic policies achieve the same optimal long-run average reward as the original joint policies, proving the scalability of computing optimal SPN control without loss of optimality.

Conclusion: The atomic framework provides theoretical justification for its strong empirical success in large-scale SPN applications and enables scalable optimal control.

Abstract: Stochastic processing networks (SPNs) have broad applications in healthcare,
transportation, and communication networks. The control of SPN is to
dynamically assign servers in batches under uncertainty to optimize long-run
performance. This problem is challenging as the policy dimension grows
exponentially with the number of servers, making standard reinforcement
learning and policy optimization methods intractable at scale. We propose an
atomic action decomposition framework that addresses this scalability challenge
by breaking joint assignments into sequential single-server assignments. This
yields policies with constant dimension, independent of the number of servers.
We study two classes of atomic policies, the step-dependent and
step-independent atomic policies, and prove that both achieve the same optimal
long-run average reward as the original joint policies. These results establish
that computing the optimal SPN control can be made scalable without loss of
optimality using the atomic framework. Our results offer theoretical
justification for the strong empirical success of the atomic framework in
large-scale applications reported in previous articles.

</details>


### [397] [Toward Model Matching for Remotely Controlled Differential Drive Robotic Vehicles](https://arxiv.org/abs/2510.06081)
*Nikolaos D. Kouvakas,Fotis N. Koumboulis,Konstantinos G. Tzierakis,John Sigalas,Anastasios Dimakakos*

Main category: eess.SY

TL;DR: 研究了具有执行器动力学和网络引起延迟的遥控差速移动机器人的方向角调节问题。通过预装的非线性控制方案解耦线性和角速度并调节航向，引入了第三个依赖于延迟的层，该层实现了从方向角命令到方向角的精确模型匹配。所提出的外环控制器是具有动态预补偿器的动态可测量输出反馈控制器。参数化产生一个简单的特征拟多项式，其系数受约束以满足直到可计算边界的所有延迟的稳定性。计算实验证实了精确跟踪、快速稳定以及有界内部信号和控制电压。该方法为基于人工智能的延迟机器人系统调优提供了一种分析设计替代方案。


<details>
  <summary>Details</summary>
Motivation: 研究具有执行器动力学和网络延迟的遥控差速移动机器人的方向角调节问题。

Method: 引入一个依赖于延迟的三层非线性控制方案，包括解耦线性和角速度、调节航向以及实现精确模型匹配。外环控制器为动态可测量输出反馈控制器，并带有动态预补偿器。通过参数化获得一个简单的特征拟多项式，并约束其系数以确保所有延迟下的稳定性。

Result: 计算实验表明，该方法能够实现精确跟踪、快速稳定以及有界内部信号和控制电压。

Conclusion: 所提出的方法为具有网络延迟的机器人系统提供了一种分析设计替代方案，可以替代基于人工智能的调优方法。

Abstract: The problem of regulation of the orientation angle of a remotely controlled
differential-drive mobile robot with actuator dynamics and network-induced
delays is studied. Using a preinstalled two-layer nonlinear control scheme that
decouples linear and angular velocities and regulates heading, a third,
delay-dependent layer that achieves exact model matching from the orientation
angle command to the orientation angle is introduced. The proposed outer loop
controller is a delay dependent dynamic measurable output-feedback controller
with dynamic proper precompensator. Parameterization yields a simple
characteristic quasi-polynomial with coefficients constrained to satisfy
stability for all delays up to a computable bound. Computational experiments
confirm accurate tracking, fast settling and bounded internal signals and
control voltages. The approach offers an analytic design alternative to
AI-based tuning for delayed robotic systems.

</details>


<div id='cs.GT'></div>

# cs.GT [[Back]](#toc)

### [398] [Computing Envy-Free up to Any Good (EFX) Allocations via Local Search](https://arxiv.org/abs/2510.05429)
*Simina Brânzei*

Main category: cs.GT

TL;DR: 提出了一种计算EFX（即使对任何好的物品也无嫉妒）分配的简单局部搜索算法，该算法用于n个代理的m个不可分割商品，具有加性估价。该算法采用模拟退火法，以EFX违规总数为目标函数，并结合单次转移邻域结构来遍历分配空间。该算法在所有测试实例中都找到了EFX分配，包括数千个随机生成的输入，并且可以扩展到数百个代理和/或数千个项目的设置。该算法的简单性及其出色的经验性能使其成为评估未来方法的简单基准。在理论方面，为相同的加性估价提供了一个势函数，确保在单次转移邻域下的任何严格下降过程都以EFX分配结束。这代表了相同估价存在的另一种证明。


<details>
  <summary>Details</summary>
Motivation: EFX（即使对任何好的物品也无嫉妒）分配是公平分配领域的一个重要概念，而EFX分配的存在性仍然是一个悬而未决的问题。

Method: 采用模拟退火法，以EFX违规总数为目标函数，并结合单次转移邻域结构来遍历分配空间。

Result: 在包括数千个随机生成的输入在内的所有测试实例中，该算法都找到了EFX分配，并且可以扩展到具有数百个代理和/或数千个项目的设置。

Conclusion: 该算法的简单性及其出色的经验性能使其成为评估未来方法的简单基准。此外，为相同的加性估价提供了一个势函数，代表了相同估价存在的另一种证明。

Abstract: We present a simple local search algorithm for computing EFX (envy-free up to
any good) allocations of $m$ indivisible goods among $n$ agents with additive
valuations. EFX is a compelling fairness notion, and whether such allocations
always exist remains a major open question in fair division.
  Our algorithm employs simulated annealing with the total number of EFX
violations as an objective function together with a single-transfer
neighborhood structure to move through the space of allocations. It found an
EFX allocation in all the instances tested, which included thousands of
randomly generated inputs, and scaled to settings with hundreds of agents
and/or thousands of items. The algorithm's simplicity, along with its strong
empirical performance makes it a simple benchmark for evaluating future
approaches.
  On the theoretical side, we provide a potential function for identical
additive valuations, which ensures that any strict-descent procedure under the
single-transfer neighborhood ends at an EFX allocation. This represents an
alternative proof of existence for identical valuations.

</details>


### [399] [Fair Rent Division: New Budget and Rent Constraints](https://arxiv.org/abs/2510.05434)
*Rohith Reddy Gangam,Shayan Taherijam,Vijay V. Vazirani*

Main category: cs.GT

TL;DR: 我们研究了带租金和房间限制的经典租金划分问题，并提出了计算公平分配的算法。


<details>
  <summary>Details</summary>
Motivation: 在经典租金划分问题中，算法效率很高。然而，在实际应用中，需要引入一些限制条件，例如每个房间的租金上下限以及每个房间的预算限制。这些限制条件使得问题更具挑战性。

Method: 针对引入的限制条件，我们开发了有效的组合算法，可以计算出可行的、无嫉妒的分配方案，或者证明不存在这样的分配方案。此外，我们还设计了优化无嫉妒分配方案的算法，以满足最大最小效用、字典序最小效用和最小效用差等公平性目标。

Result: 我们的方法将两种限制条件统一在同一个算法框架内，提高了公平分配方法在实际平台（如Spliddit）中的应用。

Conclusion: 我们提出的算法能够有效地处理带有租金和房间限制的租金划分问题，并能在保证公平性的同时进行优化，从而为实际应用提供了更强大的支持。

Abstract: We study the classical rent division problem, where $n$ agents must allocate
$n$ indivisible rooms and split a fixed total rent $R$. The goal is to compute
an envy-free (EF) allocation, where no agent prefers another agent's room and
rent to their own. This problem has been extensively studied under standard
assumptions, where efficient algorithms for computing EF allocations are known.
  We extend this framework by introducing two practically motivated
constraints: (i) lower and upper bounds on room rents, and (ii) room-specific
budget for agents. We develop efficient combinatorial algorithms that either
compute a feasible EF allocation or certify infeasibility.
  We further design algorithms to optimize over EF allocations using natural
fairness objectives such as maximin utility, leximin utility, and minimum
utility spread. Our approach unifies both constraint types within a single
algorithmic framework, advancing the applicability of fair division methods in
real-world platforms such as Spliddit.

</details>


### [400] [Fair metric distortion for matching with preferences](https://arxiv.org/abs/2510.05460)
*Jabari Hastings,Prasanna Ramakrishnan*

Main category: cs.GT

TL;DR: 在度量失真框架下研究匹配问题，比较了总成本和最大成本两种度量方式对匹配机制失真的影响，并提出了改进的RepMatch算法。


<details>
  <summary>Details</summary>
Motivation: 在度量失真框架下研究匹配问题，并探索了以最大成本作为度量方式如何改变问题以及对现有机制的影响。

Method: 在度量失真框架下，研究了总成本和最大成本两种度量方式下的匹配问题。对RepMatch机制的变体进行了分析，并证明了在最大成本目标下其失真度优于总成本目标。同时，证明了该算法对于任何单调对称范数定义的公平目标，保证了O(n^2)的失真度。

Result: 在最大成本目标下，RepMatch算法的失真度从总成本目标下的O(n^2)提高到O(n^1.58)。对于任何单调对称范数定义的公平目标，该算法保证了O(n^2)的失真度。

Conclusion: 以最大成本作为度量方式可以改善匹配机制的失真度，并且改进后的RepMatch算法在公平性方面也有保证。

Abstract: We consider the matching problem in the metric distortion framework. There
are $n$ agents and $n$ items occupying points in a shared metric space, and the
goal is to design a matching mechanism that outputs a low-cost matching between
the agents and items, using only agents' ordinal rankings of the candidates by
distance. A mechanism has distortion $\alpha$ if it always outputs a matching
whose cost is within a factor of $\alpha$ of the optimum, in every instance
regardless of the metric space.
  Typically, the cost of a matching is measured in terms of the total distance
between matched agents and items, but this measure can incentivize unfair
outcomes where a handful of agents bear the brunt of the cost. With this in
mind, we consider how the metric distortion problem changes when the cost is
instead measured in terms of the maximum cost of any agent. We show that while
these two notions of distortion can in general differ by a factor of $n$, the
distortion of a variant of the state-of-the-art mechanism, RepMatch, actually
improves from $O(n^2)$ under the sum objective to $O(n^{1.58})$ under the max
objective. We also show that for any fairness objective defined by a monotone
symmetric norm, this algorithm guarantees distortion $O(n^2)$.

</details>


### [401] [Hallucinating Flows for Optimal Mechanisms](https://arxiv.org/abs/2510.05474)
*Marios Mertzanidis,Athina Terzoglou*

Main category: cs.GT

TL;DR: 该研究在多物品拍卖机制设计领域取得了突破，首次在几种复杂设定下（包括n个独立同分布的代理和m个独立同分布的物品；n个非独立同分布的代理和两个独立同分布的物品；n个独立同分布的代理和两个非独立同分布的物品）推导出了最优机制的封闭形式解。研究还扩展了先前关于大数值物品最优捆绑的结论，证明了在特定条件下，大范围捆绑策略可以接近最优收入。


<details>
  <summary>Details</summary>
Motivation: 现有的拍卖机制设计理论在多物品设置的推广上存在巨大挑战，最优机制的封闭形式解非常罕见，仅限于单代理或特定分布的场景。本研究旨在克服这些限制，扩展多物品拍卖机制设计的理论边界。

Method: 本研究基于Yao提出的双值设定（物品价值仅支持两个值 {a, b}），并将其扩展到更一般化的多代理和多物品场景，包括代理和物品的独立同分布（i.i.d.）与非独立同分布（non-i.i.d.）的组合。通过数学推导，寻求并建立了在这些新设定下的最优拍卖机制的封闭形式解。此外，研究还分析了在大数值物品下，整体捆绑策略（grand bundling）作为最优策略的条件，并探讨了其在连续分布下的近似最优性。

Result: 研究成功地在以下三种新的、更复杂的设定下，推导出了最优拍卖机制的封闭形式解：(i) n个独立同分布（i.i.d.）的代理和m个独立同分布（i.i.d.）的物品；(ii) n个非独立同分布（non-i.i.d.）的代理和两个独立同分布（i.i.d.）的物品；(iii) n个独立同分布（i.i.d.）的代理和两个非独立同分布（non-i.i.d.）的物品。此外，研究还证明了当物品价值足够大时，对于单个代理拥有多个（非独立同分布）离散分布物品的情况，整体捆绑（grand bundling）是最优策略。对于连续乘积分布，整体捆绑能够实现接近最优（OPT - ε）的收入。

Conclusion: 本研究在多物品拍卖机制设计领域取得了重要进展，首次在多种复杂场景下实现了最优机制的解析化，极大地推动了理论研究。研究结果表明，在特定条件下，整体捆绑策略在多物品拍卖中具有很高的效率，尤其是在物品价值较高时。这些发现为设计更有效的拍卖机制提供了理论基础和实践指导。

Abstract: Myerson's seminal characterization of the revenue-optimal auction for a
single item \cite{myerson1981optimal} remains a cornerstone of mechanism
design. However, generalizing this framework to multi-item settings has proven
exceptionally challenging. Even under restrictive assumptions, closed-form
characterizations of optimal mechanisms are rare and are largely confined to
the single-agent case \cite{pavlov2011optimal,hart2017approximate,
daskalakis2018transport, GIANNAKOPOULOS2018432}, departing from the two-item
setting only when prior distributions are uniformly distributed
\cite{manelli2006bundling, daskalakis2017strong,giannakopoulos2018sjm}. In this
work, we build upon the bi-valued setting introduced by Yao
\cite{YAO_BIC_DSIC}, where each item's value has support 2 and lies in $\{a,
b\}$. Yao's result provides the only known closed-form optimal mechanism for
multiple agents. We extend this line of work along three natural axes,
establishing the first closed-form optimal mechanisms in each of the following
settings: (i) $n$ i.i.d. agents and $m$ i.i.d. items (ii) $n$ non-i.i.d. agents
and two i.i.d. items and (iii) $n$ i.i.d. agents and two non-i.i.d. items. Our
results lie at the limit of what is considered possible, since even with a
single agent and m bi-valued non-i.i.d. items, finding the optimal mechanism is
$\#P$-Hard \cite{daskalakis2014complexity, xi2018soda}. We finally generalize
the discrete analog of a result from~\cite{daskalakis2017strong}, showing that
for a single agent with $m$ items drawn from arbitrary (non-identical) discrete
distributions, grand bundling is optimal when all item values are sufficiently
large. We further show that for any continuous product distribution, grand
bundling achieves $\mathrm{OPT} - \epsilon$ revenue for large enough values.

</details>


### [402] [Mechanism design and equilibrium analysis of smart contract mediated resource allocation](https://arxiv.org/abs/2510.05504)
*Jinho Cha,Justin Yoo,Eunchan Daniel Cha,Emily Yoo,Caedon Geoffrey,Hyoshin Song*

Main category: cs.GT

TL;DR: 该研究提出了一个基于智能合约的资源分配机制设计框架，结合了经济学理论和算法，以实现去中心化协调中的效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有去中心化协调和数字合约方法缺乏经济学基础，该研究旨在填补这一空白，为智能合约嵌入效率和公平性。

Method: 提出了一种机制设计框架，建立了合约均衡的存在性和唯一性，并引入了一个具有收敛性保证的去中心化价格调整算法，最后通过合成数据和MovieLens数据集进行评估。

Result: 提出的机制在合成基准测试和MovieLens数据集上均表现出鲁棒性，能够实现效率和公平性的显著提升，并能抵抗干扰，保持稳定。

Conclusion: 该研究结合了理论严谨性和实证验证，展示了数字合约作为透明、可问责和有弹性的高风险资源分配的制度性工具的潜力，具有广泛的管理和政策意义。

Abstract: Decentralized coordination and digital contracting are becoming critical in
complex industrial ecosystems, yet existing approaches often rely on ad hoc
heuristics or purely technical blockchain implementations without a rigorous
economic foundation. This study develops a mechanism design framework for smart
contract-based resource allocation that explicitly embeds efficiency and
fairness in decentralized coordination. We establish the existence and
uniqueness of contract equilibria, extending classical results in mechanism
design, and introduce a decentralized price adjustment algorithm with provable
convergence guarantees that can be implemented in real time. To evaluate
performance, we combine extensive synthetic benchmarks with a proof-of-concept
real-world dataset (MovieLens). The synthetic tests probe robustness under fee
volatility, participation shocks, and dynamic demand, while the MovieLens case
study illustrates how the mechanism can balance efficiency and fairness in
realistic allocation environments. Results demonstrate that the proposed
mechanism achieves substantial improvements in both efficiency and equity while
remaining resilient to abrupt perturbations, confirming its stability beyond
steady state analysis. The findings highlight broad managerial and policy
relevance for supply chains, logistics, energy markets, healthcare resource
allocation, and public infrastructure, where transparent and auditable
coordination is increasingly critical. By combining theoretical rigor with
empirical validation, the study shows how digital contracts can serve not only
as technical artifacts but also as institutional instruments for transparency,
accountability, and resilience in high-stakes resource allocation.

</details>


### [403] [Möbius transforms and Shapley values for vector-valued functions on weighted directed acyclic multigraphs](https://arxiv.org/abs/2510.05786)
*Patrick Forré,Abel Jansma*

Main category: cs.GT

TL;DR: 本文将Möbius反演和Shapley值推广到有向无环多重图及其加权版本，并允许值函数（博弈）及其Möbius变换（协同函数）和Shapley值取值于包含图权重的任意阿贝尔群的模，例如向量值函数。


<details>
  <summary>Details</summary>
Motivation: 在更一般的设定中，经典的Shapley值公理（线性、效率、零玩家、对称性）不足以唯一确定Shapley值。因此，需要新的方法来处理更广泛的图结构和值函数。

Method: 本文从两个新颖的角度分析了Shapley值：1）引入投影算子，将Shapley值解释为高阶协同作用向低阶协同作用的递归投影和再归因；2）提出零玩家公理和局部对称性公理（弱元素和扁平层级公理）的加强版。

Result: 结合线性公理，这些公理能够唯一确定Shapley值的显式公式，以及效率、零玩家、对称性等经典性质，并引入了新的投影性质。该框架还可特化为有限包含代数、格、偏序和本体，并以新的视角恢复了某些先前已知的情况。

Conclusion: 将一般的加权有向无环多重图结构层级和向量值函数及Shapley值纳入考虑，为机器学习、语言处理、可解释人工智能等领域带来了新的分析工具和应用前景。

Abstract: We generalize the concept of M\"obius inversion and Shapley values to
directed acyclic multigraphs and weighted versions thereof. We further allow
value functions (games) and thus their M\"obius transforms (synergy function)
and Shapley values to have values in any abelian group that is a module over a
ring that contains the graph weights, e.g. vector-valued functions. To achieve
this and overcome the obstruction that the classical axioms (linearity,
efficiency, null player, symmetry) are not strong enough to uniquely determine
Shapley values in this more general setting, we analyze Shapley values from two
novel points of view: 1) We introduce projection operators that allow us to
interpret Shapley values as the recursive projection and re-attribution of
higher-order synergies to lower-order ones; 2) we propose a strengthening of
the null player axiom and a localized symmetry axiom, namely the weak elements
and flat hierarchy axioms. The former allows us to remove coalitions with
vanishing synergy while preserving the rest of the hierarchical structure. The
latter treats player-coalition bonds uniformly in the corner case of
hierarchically flat graphs. Together with linearity these axioms already imply
a unique explicit formula for the Shapley values, as well as classical
properties like efficiency, null player, symmetry, and novel ones like the
projection property. This whole framework then specializes to finite inclusion
algebras, lattices, partial orders and mereologies, and also recovers certain
previously known cases as corner cases, and presents others from a new
perspective. The admission of general weighted directed acyclic multigraph
structured hierarchies and vector-valued functions and Shapley values opens up
the possibility for new analytic tools and application areas, like machine
learning, language processing, explainable artificial intelligence, and many
more.

</details>


### [404] [A Small Collusion is All You Need](https://arxiv.org/abs/2510.05986)
*Yotam Gafni*

Main category: cs.GT

TL;DR: 交易费用机制（TFM）的研究重点是区块链背景下的拍卖设计，并且比传统的拍卖理论更加强调对矿工和用户合谋的鲁棒性。本文表明，2-SCP机制类别等于任何 c-SCP（其中 c>=2）类别，前提是存在一致的决胜规则。本质上，这意味着任何易受合谋影响的机制也易受小型合谋的影响。


<details>
  <summary>Details</summary>
Motivation: 交易费用机制（TFM）的研究重点是区块链背景下的拍卖设计，并且比传统的拍卖理论更加强调对矿工和用户合谋的鲁棒性。

Method: 本文证明了2-SCP机制类别等于任何c-SCP（其中c>=2）类别，前提是存在一致的决胜规则。

Result: 任何易受合谋影响的机制也易受小型合谋的影响。

Conclusion: 2-SCP机制类别等于任何c-SCP（其中c>=2）类别，前提是存在一致的决胜规则。这意味着任何易受合谋影响的机制也易受小型合谋的影响。

Abstract: Transaction Fee Mechanisms (TFMs) study auction design in the Blockchain
context, and emphasize robustness against miner and user collusion, moreso than
traditional auction theory. \cite{chung2023foundations} introduce the notion of
a mechanism being $c$-Side-Contract-Proof ($c$-SCP), i.e., robust to a
collusion of the miner and $c$ users. Later work
\cite{chung2024collusion,welfareIncreasingCollusion} shows a gap between the
$1$-SCP and $2$-SCP classes. We show that the class of $2$-SCP mechanisms
equals that of any $c$-SCP with $c\geq 2$, under a relatively minor assumption
of consistent tie-breaking. In essence, this implies that any mechanism
vulnerable to collusion, is also vulnerable to a small collusion.

</details>


<div id='cs.DC'></div>

# cs.DC [[Back]](#toc)

### [405] [Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices](https://arxiv.org/abs/2510.05109)
*Yilong Li,Shuai Zhang,Yijing Zeng,Hao Zhang,Xinmiao Xiong,Jingyu Liu,Pan Hu,Suman Banerjee*

Main category: cs.DC

TL;DR: NANOMIND是一个硬件-软件协同设计的推理框架，用于将大型多模态模型（LMM）分解为模块化“砖块”，并将其映射到最理想的加速器上，从而在资源受限的设备上实现高效的端到端推理。


<details>
  <summary>Details</summary>
Motivation: 现有的LMM推理方式是整体执行，未能充分利用异构加速器，导致端到端延迟高。

Method: NANOMIND框架将LMM分解为模块（如视觉、语言、音频），并将每个模块映射到最适合的加速器，实现了跨异构加速器的模块级动态卸载。该框架结合了定制硬件设计、系统级调度和优化的低比特计算内核。

Result: NANOMIND框架在资源受限的设备上实现了LMM的高效端到端推理，能耗降低42.3%，GPU内存使用量减少11.2%。其原型设备能够完全在本地运行LMM，实现了高吞吐量和卓越的能效，并能支持长时间的语音和视觉交互。

Conclusion: NANOMIND框架通过模块化分解和异构加速器协同设计，有效解决了LMM在资源受限设备上推理的挑战，实现了显著的能效和性能提升，为在电池供电设备上实现完全本地化的智能助手提供了可能。

Abstract: Large Multimodal Models (LMMs) are inherently modular, consisting of vision
and audio encoders, projectors, and large language models. Yet, they are almost
always executed monolithically, which underutilizes the heterogeneous
accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end
latency. In this paper, we present NANOMIND, a hardware--software co-design
inference framework for Large Multimodal Models (LMMs) that breaks large models
into modular ``bricks'' (vision, language, audio, etc.) and maps each to its
ideal accelerator. The key insight is that large models can be broken into
modular components and scheduled to run on the most appropriate compute units.
It performs module-level dynamic offloading across accelerators on
unified-memory SoCs. By combining customized hardware design, system-level
scheduling, and optimized low-bit computation kernels, we demonstrate our
framework with a compact, battery-powered device capable of running LMMs
entirely on device. This prototype functions as a self-contained intelligent
assistant that requires no network connectivity, while achieving higher
throughput and superior power efficiency under strict resource constraints. The
design further bypasses CPU bottlenecks and reduces redundant memory usage
through token-aware buffer management and module-level coordination. Our system
outperforms existing implementations in resource efficiency, cutting energy
consumption by 42.3\% and GPU memory usage by 11.2\%. This enables a
battery-powered device to run LLaVA-OneVision with a camera for nearly half a
day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.

</details>


### [406] [Agora: Bridging the GPU Cloud Resource-Price Disconnect](https://arxiv.org/abs/2510.05111)
*Ian McDougall,Noah Scott,Joon Huh,Kirthevasan Kandasamy,Karthikeyan Sankaralingam*

Main category: cs.DC

TL;DR: 摩尔定律在GPU上失效，内存带宽成为瓶颈，基于时间的云GPU定价模型经济效率低下，提出基于特征的定价框架Agora，通过Agora的实现验证了该框架的可行性和有效性。


<details>
  <summary>Details</summary>
Motivation: 现代GPU在计算能力（FLOPs）上遵循摩尔定律的指数增长，但内存带宽的增长并未跟上，导致性价比脱节，现有的基于时间的云GPU定价模型未能反映内存带宽不断上升的边际成本，造成市场扭曲和硬件配置低效。

Method: 提出一个新颖的、基于特征的定价框架，将成本直接与包括内存带宽在内的资源消耗挂钩，并给出了该框架的经济和算法定义，最后通过Agora系统架构实现并验证。

Result: Agora系统在50微秒采样率下能达到理想定价的95%（仅损失5%的收入），在10微秒采样率下能达到97.6%（损失2.4%的收入），表明现代遥测系统能够支持该采样率，且所提出的系统设计是可行的。

Conclusion: 基于特征的定价框架能更有效地为云GPU资源定价，通过Agora系统验证了该框架的经济和技术可行性，能够实现更透明、更高效的市场。

Abstract: The historic trend of Moore's Law, which predicted exponential growth in
computational performance per dollar, has diverged for modern Graphics
Processing Units (GPUs). While Floating Point Operations per Second (FLOPs)
capabilities have continued to scale economically, memory bandwidth has not,
creating a significant price-performance disconnect. This paper argues that the
prevailing time-based pricing models for cloud GPUs are economically
inefficient for bandwidth-bound workloads. These models fail to account for the
rising marginal cost of memory bandwidth, leading to market distortions and
suboptimal hardware allocation. To address this, we propose a novel
feature-based pricing framework that directly links cost to resource
consumption, including but not limited to memory bandwidth. We provide a robust
economic and algorithmic definition of this framework and introduce Agora, a
practical and secure system architecture for its implementation. Our
implementation of Agora shows that a 50us sampling provides nearly perfect
pricing as what ideal sampling would provide - losing only 5\% of revenue. 10us
sampling is even better result in 2.4\% loss. Modern telemetry systems can
already provide this rate of measurement, and our prototype implementation
shows the system design for feature-based pricing is buildable. Our evaluation
across diverse GPU applications and hardware generations empirically validates
the effectiveness of our approach in creating a more transparent and efficient
market for cloud GPU resources.

</details>


### [407] [A Flexible Programmable Pipeline Parallelism Framework for Efficient DNN Training](https://arxiv.org/abs/2510.05112)
*Lijuan Jiang,Xingjian Qian,Zhenxiang Ma,Zan Zong,Hengjie Li,Chao Yang,Jidong Zhai*

Main category: cs.DC

TL;DR: FlexPipe是一个可编程流水线并行框架，通过DSL和自动化调度器，能够高效探索和定制流水线调度，性能优于现有框架。


<details>
  <summary>Details</summary>
Motivation: 现有流水线并行调度方法难以自动适应复杂多变的DNN模型，手动实现成本高，现有框架在自动化调度探索和灵活性方面存在局限。

Method: FlexPipe包含一个领域特定语言（DSL）和自动化调度器，允许用户快速开发和定制流水线顺序，并方便地添加新操作。

Result: FlexPipe在性能上实现了对Megtron-LM最高2.28倍的加速，以及对现有自动化流水线并行框架最高1.49倍的加速。

Conclusion: FlexPipe通过提高生产力、可编程性、可调试性和易调优性，有效解决了现有流水线并行调度方法的局限性，并取得了显著的性能提升。

Abstract: Pipeline parallelism is an essential distributed parallelism method.
Increasingly complex and diverse DNN models necessitate meticulously customized
pipeline schedules for performance. However, existing practices typically rely
on predefined schedules, each with strengths, but fail to adapt automatically
to the emerging model architectures. Exploring novel high-efficiency schedules
is daunting due to the enormous and varying schedule space. Besides, manually
implementing schedules can be challenging due to the onerous coding burdens and
constantly changing needs. Unfortunately, existing frameworks have limitations
in automated schedule exploration and lack flexibility and controllability.
  This paper presents FlexPipe, a programmable pipeline parallelism framework
with enhanced productivity, programmability, debuggability, and ease of tuning.
FlexPipe has two main components: a succinct domain-specific language (DSL) and
an automated scheduler. FlexPipe enables automated schedule exploration for
various parallel scenarios within a broad spectrum of schedule types at a small
search cost. Besides, users can swiftly develop and customize schedules using
the FlexPipe DSL, which embodies flexible controllability in the pipeline order
of micro-batch computations over stages. It also provides convenient mechanisms
to include new operations in schedules to meet changing demands. Our evaluation
results demonstrate that FlexPipe achieves up to 2.28X performance speedup
compared to the popular large-scale parallel framework Megtron-LM, and gains up
to 1.49X performance speedup compared to the state-of-the-art automated
pipeline parallelism framework.

</details>


### [408] [Lumos: Performance Characterization of WebAssembly as a Serverless Runtime in the Edge-Cloud Continuum](https://arxiv.org/abs/2510.05118)
*Cynthia Marcelino,Noah Krennmair,Thomas Pusztai,Stefan Nastic*

Main category: cs.DC

TL;DR: WebAssembly (Wasm) 作为一种轻量级、可移植的服务器无状态函数运行时，在异构和资源受限的边缘云环境中具有潜力，但其性能权衡尚不明确。本文提出了 Lumos，一个性能模型和基准测试工具，用于表征服务器无状态运行时。


<details>
  <summary>Details</summary>
Motivation: 评估 WebAssembly (Wasm) 在边缘云连续体中作为服务器无状态函数的运行时性能，并理解其与容器相比的性能优势和劣势。

Method: 开发了一个名为 Lumos 的性能模型和基准测试工具，该工具能够识别边缘云连续体中的工作负载、系统和环境级别性能驱动因素。使用 Lumos 对先进的容器和 Wasm 运行时（包括解释模式和提前编译模式）进行了基准测试。

Result: 与容器相比，提前编译 (AoT) 的 Wasm 镜像大小小 30 倍，冷启动延迟降低 16%。然而，解释模式下的 Wasm 在热启动延迟方面高出 55 倍，I/O 序列化开销高出 10 倍。

Conclusion: 提前编译的 Wasm 在镜像大小和冷启动延迟方面提供了显著优势，使其成为资源受限环境的有吸引力的选择。然而，解释模式下的 Wasm 在热启动性能和 I/O 处理方面存在显着劣势，需要进一步优化。Lumos 提供了一个有价值的工具来量化这些权衡。

Abstract: WebAssembly has emerged as a lightweight and portable runtime to execute
serverless functions, particularly in heterogeneous and resource-constrained
environments such as the Edge Cloud Continuum. However, the performance
benefits versus trade-offs remain insufficiently understood. This paper
presents Lumos, a performance model and benchmarking tool for characterizing
serverless runtimes. Lumos identifies workload, system, and environment-level
performance drivers in the Edge-Cloud Continuum. We benchmark state-of-the-art
containers and the Wasm runtime in interpreted mode and with ahead-of-time
compilation. Our performance characterization shows that AoT-compiled Wasm
images are up to 30x smaller and decrease cold-start latency by up to 16%
compared to containers, while interpreted Wasm suffers up to 55x higher warm
latency and up to 10x I/O-serialization overhead.

</details>


### [409] [cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided Inter-Node Communications](https://arxiv.org/abs/2510.05476)
*Xi Wang,Bin Ma,Jongryool Kim,Byungil Koh,Hoshik Kim,Dong Li*

Main category: cs.DC

TL;DR: cMPI利用CXL内存共享优化MPI通信，显著降低延迟和提高带宽。


<details>
  <summary>Details</summary>
Motivation: 传统的MPI通信依赖复杂的网络堆栈，延迟较高。cMPI旨在通过CXL内存共享，将跨节点通信转化为内存事务，以优化MPI点对点通信。

Method: cMPI在真实CXL平台上利用CXL内存共享，将跨节点通信转化为CXL内存内的内存事务和数据复制，绕过传统网络协议。它解决了数据对象管理、缓存一致性和原子操作等挑战。

Result: 在真实CXL平台上，cMPI实现的CXL内存共享相比基于TCP的互连，在小中规模集群中可实现7.2倍至8.1倍的低延迟。与标准以太网NIC和高端SmartNIC上的TCP相比，cMPI在小消息传输上，延迟最多可降低49倍，带宽最多可提高72倍。

Conclusion: cMPI是第一个利用CXL内存共享优化MPI点对点通信的工作，通过将通信模式转变为内存事务，在延迟和带宽方面取得了显著的性能提升，尤其是在小消息场景下。

Abstract: Message Passing Interface (MPI) is a foundational programming model for
high-performance computing. MPI libraries traditionally employ network
interconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP
and RoCE) with complex software stacks for cross-node communication. We present
cMPI, the first work to optimize MPI point-to-point communication (both
one-sided and two-sided) using CXL memory sharing on a real CXL platform,
transforming cross-node communication into memory transactions and data copies
within CXL memory, bypassing traditional network protocols. We analyze
performance across various interconnects and find that CXL memory sharing
achieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in
small- and medium-scale clusters. We address challenges of CXL memory sharing
for MPI communication, including data object management over the dax
representation [50], cache coherence, and atomic operations. Overall, cMPI
outperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x
and 72x in latency and bandwidth, respectively, for small messages.

</details>


### [410] [FlashResearch: Real-time Agent Orchestration for Efficient Deep Research](https://arxiv.org/abs/2510.05145)
*Lunyiu Nie,Nedim Lipka,Ryan A. Rossi,Swarat Chaudhuri*

Main category: cs.DC

TL;DR: FlashResearch框架通过将顺序处理转变为并行运行时编排，解决了深度研究代理的顺序推理瓶颈，提高了效率和适应性。


<details>
  <summary>Details</summary>
Motivation: 深度研究代理的顺序推理过程导致高延迟、差的运行时适应性和低效的资源分配，限制了它们在交互式应用中的使用。

Method: FlashResearch通过动态地将复杂查询分解为树状子任务来实现并行化。它包括一个自适应规划器、一个实时编排层和一个多维并行化框架。

Result: FlashResearch在固定的时间预算内提高了最终报告的质量，并将速度提高了5倍，同时保持了可比的质量。

Conclusion: FlashResearch通过并行化解决了深度研究代理的顺序推理瓶颈，提高了效率和适应性。

Abstract: Deep research agents, which synthesize information across diverse sources,
are significantly constrained by their sequential reasoning processes. This
architectural bottleneck results in high latency, poor runtime adaptability,
and inefficient resource allocation, making them impractical for interactive
applications. To overcome this, we introduce FlashResearch, a novel framework
for efficient deep research that transforms sequential processing into
parallel, runtime orchestration by dynamically decomposing complex queries into
tree-structured sub-tasks. Our core contributions are threefold: (1) an
adaptive planner that dynamically allocates computational resources by
determining research breadth and depth based on query complexity; (2) a
real-time orchestration layer that monitors research progress and prunes
redundant paths to reallocate resources and optimize efficiency; and (3) a
multi-dimensional parallelization framework that enables concurrency across
both research breadth and depth. Experiments show that FlashResearch
consistently improves final report quality within fixed time budgets, and can
deliver up to a 5x speedup while maintaining comparable quality.

</details>


### [411] [Artificial Intelligence for Cost-Aware Resource Prediction in Big Data Pipelines](https://arxiv.org/abs/2510.05127)
*Harshit Goyal*

Main category: cs.DC

TL;DR: 本研究提出了一种利用随机森林回归的 AI 方法来预测大数据管道中的资源利用率，以优化云资源分配。


<details>
  <summary>Details</summary>
Motivation: 高效的资源分配是云计算中的关键挑战，过度配置会增加成本，而配置不足则会带来性能下降和违反 SLA 的风险。因此，有必要对资源利用率进行准确预测，以实现成本效益和性能保障。

Method: 使用随机森林回归模型，对谷歌 Borg 集群跟踪数据进行预处理、清理、转换并提取相关特征（CPU、内存、使用率分布），以预测资源利用率。

Result: 该模型在预测资源利用率方面取得了很高的准确性（R 方 = 0.99，平均绝对误差 MAE = 0.0048，均方根误差 RMSE = 0.137），能够捕捉工作负载特征与资源利用率之间的非线性关系。错误分析表明，该模型在处理中小规模作业时表现出色，但在罕见的大规模作业上存在较高的预测方差。

Conclusion: 研究结果表明，人工智能驱动的预测在云环境中具有成本效益的自动扩展潜力，可以在减少不必要资源配置的同时保障服务质量。

Abstract: Efficient resource allocation is a key challenge in modern cloud computing.
Over-provisioning leads to unnecessary costs, while under-provisioning risks
performance degradation and SLA violations. This work presents an artificial
intelligence approach to predict resource utilization in big data pipelines
using Random Forest regression. We preprocess the Google Borg cluster traces to
clean, transform, and extract relevant features (CPU, memory, usage
distributions). The model achieves high predictive accuracy (R Square = 0.99,
MAE = 0.0048, RMSE = 0.137), capturing non-linear relationships between
workload characteristics and resource utilization. Error analysis reveals
impressive performance on small-to-medium jobs, with higher variance in rare
large-scale jobs. These results demonstrate the potential of AI-driven
prediction for cost-aware autoscaling in cloud environments, reducing
unnecessary provisioning while safeguarding service quality.

</details>


### [412] [Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting](https://arxiv.org/abs/2510.05497)
*Zhongkai Yu,Yue Guan,Zihao Yu,Chenyang Zhou,Shuyi Pei,Yangwook Kang,Yufei Ding,Po-An Tsai*

Main category: cs.DC

TL;DR: MoE架构的大型语言模型（LLMs）在性能上取得了显著的提升，但其随机专家选择机制引入了显著的数据移动开销，成为多单元服务系统中的主要瓶颈。通过对大规模MoE模型进行数据移动为中心的分析，我们获得了指导未来服务系统设计的关键见解，并在实际应用中取得了显著的性能提升。


<details>
  <summary>Details</summary>
Motivation: MoE架构LLMs在性能上表现优异，但随机专家选择带来的数据移动成为服务系统中的瓶颈，需要进行分析以优化服务效率。

Method: 对三种先进的大规模MoE模型（200B-671B）进行数据移动为中心的剖析，使用超过24,000个请求和150GB+的追踪文件，从时间与空间两个维度进行系统性分析。

Result: 提炼出六个关键见解，指导未来服务系统的设计。将这些见解应用于实际的wafer-scale GPU，对DeepSeek V3和Qwen3分别实现了6.3倍和4.0倍的平均加速。

Conclusion: 该研究首次对大规模MoE模型进行了全面的数据为中心的分析，其剖析追踪和分析结果已公开，并计划发布模拟框架以促进未来研究。

Abstract: Large Language Models (LLMs) with Mixture of Experts (MoE) architectures
achieve remarkable performance improvements, but their random expert selection
mechanism introduces significant data movement overhead that becomes the
dominant bottleneck in multi-unit serving systems. To forecast the patterns
underlying this data movement, we conduct comprehensive data-movement-centric
profiling across three state-of-the-art large-scale MoE models (200B- 671B)
using over 24,000 requests spanning diverse workloads. With the resulting
150GB+ trace files, we perform systematic analysis from both temporal and
spatial perspectives and distill six key insights to guide the design of
diverse future serving systems. Taking wafer-scale GPUs as a case study, we
demonstrate that minor architectural modifications leveraging our insights
achieve substantial performance gains, delivering 6.3X and 4.0X average
speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first
comprehensive data-centric analysis of MoE models at scale. Our profiling
traces and analysis results are publicly available at
{https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will
also release our simulation framework shortly to facilitate future research in
this area.

</details>


### [413] [Percepta: High Performance Stream Processing at the Edge](https://arxiv.org/abs/2510.05149)
*Clarisse Sousa,Tiago Fonseca,Luis Lino Ferreira,Ricardo Venâncio,Ricardo Severino*

Main category: cs.DC

TL;DR: Edge computing is growing due to IoT and real-time data, facing challenges like latency and privacy. Percepta is a lightweight DSP system for edge AI, especially RL, offering features for reward computation, data storage, real-time preparation, normalization, protocol harmonization, and missing data handling.


<details>
  <summary>Details</summary>
Motivation: Cloud-centric solutions face limitations in latency, bandwidth, and privacy due to real-time data and IoT devices, driving the need for Edge Computing. Challenges include data rate harmonization, protocol conversion, data loss handling, and AI integration.

Method: This paper presents Percepta, a lightweight Data Stream Processing (DSP) system designed for edge AI workloads, particularly Reinforcement Learning (RL). It incorporates features like reward function computation, data storage for retraining, and real-time data preparation.

Result: Percepta provides specialized features for edge AI, including reward computation, data storage for retraining, real-time data preparation, data normalization, harmonization across protocols and sampling rates, and handling of missing data.

Conclusion: Percepta is a suitable system for edge-based AI deployment, addressing the challenges of real-time data processing, AI model integration, and the specific needs of Reinforcement Learning at the edge.

Abstract: The rise of real-time data and the proliferation of Internet of Things (IoT)
devices have highlighted the limitations of cloud-centric solutions,
particularly regarding latency, bandwidth, and privacy. These challenges have
driven the growth of Edge Computing. Associated with IoT appears a set of other
problems, like: data rate harmonization between multiple sources, protocol
conversion, handling the loss of data and the integration with Artificial
Intelligence (AI) models. This paper presents Percepta, a lightweight Data
Stream Processing (DSP) system tailored to support AI workloads at the edge,
with a particular focus on such as Reinforcement Learning (RL). It introduces
specialized features such as reward function computation, data storage for
model retraining, and real-time data preparation to support continuous
decision-making. Additional functionalities include data normalization,
harmonization across heterogeneous protocols and sampling rates, and robust
handling of missing or incomplete data, making it well suited for the
challenges of edge-based AI deployment.

</details>


### [414] [Decoupling Correctness from Policy: A Deterministic Causal Structure for Multi-Agent Systems](https://arxiv.org/abs/2510.05621)
*Zhiyuan Ren,Tao Zhang,Wenchi Chen*

Main category: cs.DC

TL;DR: 该论文提出了一种名为确定性因果结构（DCS）的形式化基础，将分布式系统中的正确性与操作策略（如调度、批处理或路由）分离开来，解决了传统模型（如CRDTs）无法处理的因果歧义问题，并在异步计算中确立了正确性作为一种不变的、与策略无关的基础。


<details>
  <summary>Details</summary>
Motivation: 分布式多智能体系统中，正确性与调度、批处理或路由等操作策略紧密相关，这使得系统在策略演进时容易变得脆弱，可能破坏完整性保证。本研究旨在将正确性与策略解耦。

Method: 提出确定性因果结构（DCS）的形式化基础，建立了一个最小公理化理论，并证明了存在唯一性、策略无关不变性、观测等价性和公理最小性四项结果。

Result: 证明了DCS可以解决CRDTs等值中心化收敛模型无法处理的因果歧义，并且移除任何公理都会导致确定性崩溃为歧义。DCS成为了异步计算的边界原理，类似于CAP和FLP，正确性仅在并半格的表达能力内得以保持。

Conclusion: DCS将正确性确立为一个固定、与策略无关的基础，即“正确性即底盘”范式，为分布式智能系统的模块化、安全和可演进构建提供了支持。

Abstract: In distributed multi-agent systems, correctness is often entangled with
operational policies such as scheduling, batching, or routing, which makes
systems brittle since performance-driven policy evolution may break integrity
guarantees. This paper introduces the Deterministic Causal Structure (DCS), a
formal foundation that decouples correctness from policy. We develop a minimal
axiomatic theory and prove four results: existence and uniqueness,
policy-agnostic invariance, observational equivalence, and axiom minimality.
These results show that DCS resolves causal ambiguities that value-centric
convergence models such as CRDTs cannot address, and that removing any axiom
collapses determinism into ambiguity. DCS thus emerges as a boundary principle
of asynchronous computation, analogous to CAP and FLP: correctness is preserved
only within the expressive power of a join-semilattice. All guarantees are
established by axioms and proofs, with only minimal illustrative constructions
included to aid intuition. This work establishes correctness as a fixed,
policy-agnostic substrate, a Correctness-as-a-Chassis paradigm, on which
distributed intelligent systems can be built modularly, safely, and evolvably.

</details>


### [415] [SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading](https://arxiv.org/abs/2510.05164)
*Yuanzhe Shen,Yide Liu,Zisu Huang,Ruicheng Yin,Xiaoqing Zheng,Xuanjing Huang*

Main category: cs.DC

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Large language models (LLMs) demonstrate remarkable performance across
diverse tasks, yet their effectiveness frequently depends on costly commercial
APIs or cloud services. Model selection thus entails a critical trade-off
between performance and cost: high-performing LLMs typically incur substantial
expenses, whereas budget-friendly small language models (SLMs) are constrained
by limited capabilities. Current research primarily proposes two routing
strategies: pre-generation routing and cascade routing. Both approaches have
distinct characteristics, with cascade routing typically offering superior
cost-effectiveness and accuracy despite its higher latency. To further address
the limitations of both approaches, we introduce SATER, a dual-mode compatible
approach that fine-tunes models through shortest-response preference
optimization and a confidence-aware rejection mechanism. SATER significantly
reduces redundant outputs and response times, while improving both the
performance of pre-generation routing and the efficiency of cascade routing.
Experiments across three SLMs and six datasets, varying in type and complexity,
demonstrate that SATER achieves comparable performance while consistently
reducing computational costs by over 50\% and cascade latency by over 80\%.

</details>


### [416] [OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training](https://arxiv.org/abs/2510.05186)
*Hongpei Li,Han Zhang,Huikang Liu,Dongdong Ge,Yinyu Ye*

Main category: cs.DC

TL;DR: 通过将流水线调度视为一个约束优化问题，对LLM训练中的流水线并行进行了优化，以在内存限制下最小化气泡并提高吞吐量。


<details>
  <summary>Details</summary>
Motivation: 现有的流水线并行技术在减少内存消耗方面存在启发式和粗粒度的问题，忽视了内存、计算和调度延迟之间的细粒度权衡。

Method: 将流水线调度公式化为一个约束优化问题，该问题联合考虑了内存容量、激活重用和流水线气泡最小化，从而得到细粒度的调度。

Result: 该方法将流水线气泡减少了高达50%，同时保持了相同的每设备内存限制，并在某些情况下支持在有限的内存预算内训练更大的模型。

Conclusion: 通过将流水线调度视为一个约束优化问题，可以动态地优化流水线并行中的内存、计算和调度延迟的权衡，从而提高吞吐量和内存利用率。

Abstract: Pipeline parallelism (PP) has become a standard technique for scaling large
language model (LLM) training across multiple devices. However, despite recent
progress in reducing memory consumption through activation offloading, existing
approaches remain largely heuristic and coarse-grained, often overlooking the
fine-grained trade-offs between memory, computation, and scheduling latency. In
this work, we revisit the pipeline scheduling problem from a principled
optimization perspective. We observe that prevailing strategies either rely on
static rules or aggressively offload activations without fully leveraging the
interaction between memory constraints and scheduling efficiency. To address
this, we formulate scheduling as a constrained optimization problem that
jointly accounts for memory capacity, activation reuse, and pipeline bubble
minimization. Solving this model yields fine-grained schedules that reduce
pipeline bubbles while adhering to strict memory budgets. Our approach
complements existing offloading techniques: whereas prior approaches trade
memory for time in a fixed pattern, we dynamically optimize the tradeoff with
respect to model structure and hardware configuration. Experimental results
demonstrate that our method consistently improves both throughput and memory
utilization. In particular, we reduce idle pipeline time by up to 50% under the
same per-device memory limit, and in some cases, enable the training of larger
models within limited memory budgets.

</details>


### [417] [Performance of a high-order MPI-Kokkos accelerated fluid solver](https://arxiv.org/abs/2510.05254)
*Filipp Sporykhin,Holger Homann*

Main category: cs.DC

TL;DR: 使用高阶方法和 Kokkos 库进行流体动力学模拟，并在 CPU 和 GPU 上进行性能和能耗分析。


<details>
  <summary>Details</summary>
Motivation: 研究现代高性计算架构上的数值方案性能，特别是高阶不连续伽辽金方法在处理流体动力学问题时的效率和能耗。

Method: 使用空间节点不连续伽辽金格式（最高八阶）和龙格-库塔方法（最高六阶）进行时间耦合。利用 Kokkos 库和 MPI 实现单源代码，以适应包括 CPU 和 GPU 在内的多种高性计算硬件。在 CPU 和 GPU 上分别测试了不同网格大小和自由度数量下的模拟性能和能耗。

Result: 高阶方法模拟速度更快，达到相同全局误差所需计算时间更少。RK 方法对性能影响较小，四阶 RK 方法是较优选择。代码在 GPU 上表现良好，CPU 扩展性良好（弱扩展性）。小网格模拟 CPU 优于 GPU，大网格（>10^7 自由度）GPU 显著优于 CPU。GPU 在大网格模拟中能耗更低，但小网格模拟中能耗更高。现代 GPU 需要更大的网格才能高效利用，可能导致能耗反弹效应。

Conclusion: 高阶不连续伽辽金方法在现代高性计算架构上表现出色，特别是在 GPU 上处理大规模问题时。选择合适的计算设备（CPU 或 GPU）和网格大小对于优化性能和能耗至关重要。需要关注高性计算的能耗问题，并采取措施避免因设备更新迭代带来的能耗反弹。

Abstract: This work discusses the performance of a modern numerical scheme for fluid
dynamical problems on modern high-performance computing architectures. Our code
implements a spatial nodal discontinuous Galerkin scheme that we test up to an
order of convergence of eight. It is temporally coupled to a set of Runge-Kutta
methods of orders up to six. The code integrates the linear advection equations
as well as the isothermal Euler equations in one, two, and three dimensions. In
order to target modern hardware involving many-core Central Processing Units
and accelerators such as Graphic Processing Units we use the Kokkos library in
conjunction with the Message Passing Interface to run our single source code on
various GPU systems. We find that the higher the order the faster is the code.
Eighth-order simulations attain a given global error with much less computing
time than third- or fourth-order simulations. The RK scheme has a smaller
impact on the code performance and a classical fourth-order scheme seems to
generally be a good choice. The code performs very well on all considered GPUs.
The many-CPU performance is also very good and perfect weak scaling is observed
up to many hundreds of CPU cores using MPI. We note that small grid-size
simulations are faster on CPUs than on GPUs while GPUs win significantly over
CPUs for simulations involving more than $10^7$ degrees of freedom ($\approx
3100^2$ grid points). When it comes to the environmental impact of numerical
simulations we estimate that GPUs consume less energy than CPUs for large
grid-size simulations but more energy on small grids. We observe a tendency
that the more modern is the GPU the larger needs to be the grid in order to use
it efficiently. This yields a rebound effect because larger simulations need
longer computing times and in turn more energy that is not compensated by the
energy efficiency gain of the newer GPUs.

</details>


### [418] [Toward Systems Foundations for Agentic Exploration](https://arxiv.org/abs/2510.05556)
*Jiakai Xu,Tianle Zhou,Eugene Wu,Kostis Kaffes*

Main category: cs.DC

TL;DR: LLM驱动的代理探索需要快速的快照/恢复机制，但现有的通用工具不足以满足需求，并且在真实部署中面临严峻挑战。需要解决三个关键问题：fork语义、外部副作用和原生fork。


<details>
  <summary>Details</summary>
Motivation: LLM代理的探索性学习需要快速的快照/恢复机制来支持其分支、回溯和跨多个执行路径搜索的能力，这超出了当前系统所能提供的范围。

Method: 通过对六种不同的快照/恢复机制进行基准测试，评估了通用工具（如CRIU或容器提交）在隔离测试环境和真实部署环境中的性能。

Result: 通用的快照/恢复工具（如CRIU或容器提交）在隔离测试环境中速度不够快，在真实部署环境中更是无法满足要求，因为代理需要与文件、套接字和云API共享资源。

Conclusion: 为了支持LLM代理的探索，需要解决三个根本性挑战：1. fork语义（如何处理分支中的可见性或隐藏性更新）；2. 外部副作用（需要为服务添加fork感知能力或拦截其调用）；3. 原生fork（在微秒级内克隆数据库和运行时而不进行 bulk copy）。

Abstract: Agentic exploration, letting LLM-powered agents branch, backtrack, and search
across many execution paths, demands systems support well beyond today's
pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that
generic tools such as CRIU or container commits are not fast enough even in
isolated testbeds, and they crumble entirely in real deployments where agents
share files, sockets, and cloud APIs with other agents and human users. In this
talk, we pinpoint three open fundamental challenges: fork semantics, which
concerns how branches reveal or hide tentative updates; external side-effects,
where fork awareness must be added to services or their calls intercepted; and
native forking, which requires cloning databases and runtimes in microseconds
without bulk copying.

</details>


### [419] [Intertemporal Pricing of Time-Bound Stablecoins: Measuring and Controlling the Liquidity-of-Time Premium](https://arxiv.org/abs/2510.05711)
*Ailiya Borjigin,Cong He*

Main category: cs.DC

TL;DR: 该论文提出了一种名为“流动性溢价”（TLP）的概念，用于衡量传统证券在非交易时段提供流动性的额外回报或成本，并构建了一个定价模型和风险控制机制，以管理和稳定与时间挂钩的稳定币。


<details>
  <summary>Details</summary>
Motivation: 时间限制型稳定币旨在解决传统证券在市场休市期间无法进行交易的问题，提供持续的跨市场流动性。然而，这种跨时区交易会产生额外的成本或收益，即TLP，需要进行量化和管理。

Method: 该研究结合了金融工程（无套利定价、期权定价）和实证金融（跨时区上市股票和期货的事件研究）的方法。作者推导了TLP的定价模型，包括其期限结构，并提出了一种动态调整贷款价值比（LTV）的风险控制机制，以在目标范围内管理TLP。

Result: 研究结果表明，TLP随着休市时间和波动性的增加而增长，但可以通过自适应LTV进行控制。论文还提供了回测、期限结构曲线、资本效率与尾部风险的权衡以及时间-流动性热图等实证分析。

Conclusion: 时间限制型稳定币可以作为一种减少时间市场低效率的工具。该研究为未来的相关研究和实际应用提供了理论基础和实证依据。

Abstract: Time-bound stablecoins are DeFi assets that temporarily tokenize traditional
securities during market off-hours, enabling continuous cross-market liquidity.
We introduce the Liquidity-of-Time Premium (TLP): the extra return or cost of
providing liquidity when the primary market is closed. We build a no-arbitrage
pricing model that yields a band for fair values over different expiries, and a
dynamic risk-control mechanism that adjusts loan-to-value (LTV) ratios in real
time to keep TLP within a target range. Our analysis blends financial
engineering (no-arbitrage conditions, option-style pricing) with empirical
finance (event studies on cross-listed stocks and futures) to measure TLP under
time-zone frictions. We define TLP formally, derive closed-form expressions for
its term structure under idealized assumptions, and simulate scenarios that
vary volatility and collateralization. We then propose an LTV policy that
raises or lowers collateral to expand or curtail time-bound stablecoin supply,
analogous to a central bank adjusting rates to defend a peg. We outline
empirical proxies for TLP, including ADR premiums, overseas index futures
versus cash index divergence, and pre-market versus official close gaps.
Results show that TLP grows with closure length and volatility, yet can be
contained by adaptive LTV. We provide backtests and figures (term-structure
curves, capital-efficiency versus tail-risk trade-offs, time-liquidity
heatmaps) and discuss protocol design (vault structure, closing-price oracles,
on-chain auction liquidations). The findings position time-bound stablecoins as
a tool to reduce temporal market inefficiencies and inform future research and
deployment.

</details>


### [420] [A Review of Ontology-Driven Big Data Analytics in Healthcare: Challenges, Tools, and Applications](https://arxiv.org/abs/2510.05738)
*Ritesh Chandra,Sonali Agarwal,Navjot Singh,Sadhana Tiwari*

Main category: cs.DC

TL;DR: 医疗保健数据的爆炸式增长需要有效的治理，而本体驱动的语义数据管理通过链接元数据到医疗保健知识图谱来提供解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着电子健康记录、医学影像、可穿戴设备传感器和生物医学研究等医疗保健数据的指数级增长，数据湖和集中式架构得到了广泛应用。然而，缺乏有效治理可能导致这些存储库变成混乱的数据沼泽。

Method: 本综述采用系统性研究策略，提出关键研究问题，并在各大学术数据库中进行结构化文献检索。对选定的研究进行分析，并将其归类为本体驱动的医疗保健分析的六个类别：(i) 本体驱动的集成框架，(ii) 用于元数据丰富化的语义建模，(iii) 基于本体的数据访问 (OBDA)，(iv) 基本语义数据管理，(v) 用于决策支持的基于本体的推理，以及 (vi) 用于非结构化数据的语义注释。

Result: 本综述进一步探讨了本体技术与大数据框架（如 Hadoop、Spark、Kafka 等）的集成，并强调了它们共同提供可扩展和智能的医疗保健分析的潜力。对每个类别，我们回顾了最新的技术、代表性的案例研究、技术和组织挑战，以及新兴趋势（如人工智能、机器学习、物联网 (IoT) 和实时分析）。

Conclusion: 本体驱动的语义数据管理通过链接元数据到医疗保健知识图谱，增强了语义互操作性，改善了数据可发现性，并实现了丰富的、特定领域的数据访问，为应对医疗保健数据挑战提供了强大的解决方案。本综述旨在指导可持续、可互操作和高性能的医疗保健数据生态系统的发展。

Abstract: Exponential growth in heterogeneous healthcare data arising from electronic
health records (EHRs), medical imaging, wearable sensors, and biomedical
research has accelerated the adoption of data lakes and centralized
architectures capable of handling the Volume, Variety, and Velocity of Big Data
for advanced analytics. However, without effective governance, these
repositories risk devolving into disorganized data swamps. Ontology-driven
semantic data management offers a robust solution by linking metadata to
healthcare knowledge graphs, thereby enhancing semantic interoperability,
improving data discoverability, and enabling expressive, domain-aware access.
This review adopts a systematic research strategy, formulating key research
questions and conducting a structured literature search across major academic
databases, with selected studies analyzed and classified into six categories of
ontology-driven healthcare analytics: (i) ontology-driven integration
frameworks, (ii) semantic modeling for metadata enrichment, (iii)
ontology-based data access (OBDA), (iv) basic semantic data management, (v)
ontology-based reasoning for decision support, and (vi) semantic annotation for
unstructured data. We further examine the integration of ontology technologies
with Big Data frameworks such as Hadoop, Spark, Kafka, and so on, highlighting
their combined potential to deliver scalable and intelligent healthcare
analytics. For each category, recent techniques, representative case studies,
technical and organizational challenges, and emerging trends such as artificial
intelligence, machine learning, the Internet of Things (IoT), and real-time
analytics are reviewed to guide the development of sustainable, interoperable,
and high-performance healthcare data ecosystems.

</details>


### [421] [EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models](https://arxiv.org/abs/2510.05943)
*Zheyue Tan,Mustapha Abdullahi,Tuo Shi,Huining Yuan,Zelai Xu,Chao Yu,Boxun Li,Bo Zhao*

Main category: cs.DC

TL;DR:  EARL是一个可扩展的系统，用于高效的代理强化学习，通过动态调整并行策略和优化数据交换来解决LLM训练中的上下文长度和内存瓶颈。


<details>
  <summary>Details</summary>
Motivation: 在大型语言模型（LLM）的强化学习（RL）后训练以及代理RL的扩展中，存在两个实际瓶颈：1. 上下文长度在训练过程中迅速增长，导致内存占用和延迟增加，并可能引发内存不足（OOM）错误；2. 中间张量随上下文长度累积，使得跨设备数据移动成为主要的系统瓶颈。

Method:  EARL通过引入一个并行选择器，该选择器能根据序列长度和系统负载动态地调整RL各阶段的模型和训练并行度。此外，还设计了一个数据调度器，用于执行感知布局的、去中心化的中间数据批次交换。

Result: EARL提高了吞吐量，减少了长上下文相关的失败，并实现了代理LLM的稳定大规模训练，无需依赖上下文长度的硬性限制或惩罚。

Conclusion:  EARL通过其创新的并行选择器和数据调度器，有效解决了大规模代理RL训练中的关键挑战，实现了LLM训练效率和稳定性的显著提升。

Abstract: Reinforcement learning (RL) has become a pivotal component of large language
model (LLM) post-training, and agentic RL extends this paradigm to operate as
agents through multi-turn interaction and tool use. Scaling such systems
exposes two practical bottlenecks: (1) context length grows rapidly during
training, inflating memory usage and latency, and triggering out-of-memory
(OOM) failures; and (2) intermediate tensors accumulate with context length,
making cross-device data movement a major system bottleneck.
  We present EARL, a scalable system for efficient agentic RL. EARL designs a
parallelism selector that dynamically adapts model and training parallelism
across RL stages based on sequence length and system load, and a data
dispatcher that performs layout-aware, decentralized exchange of intermediate
data batches. Together, these components increase throughput, reduce
long-context failures, and enable stable large-scale training of agentic LLMs
without relying on hard limits or penalties of context length.

</details>


<div id='cs.RO'></div>

# cs.RO [[Back]](#toc)

### [422] [VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing](https://arxiv.org/abs/2510.05213)
*Yixiao Wang,Mingxiao Huo,Zhixuan Liang,Yushi Du,Lingfeng Sun,Haotian Lin,Jinghuan Shang,Chensheng Peng,Mohit Bansal,Mingyu Ding,Masayoshi Tomizuka*

Main category: cs.RO

TL;DR: VER是一个视觉专家Transformer，通过动态选择和融合多个预训练视觉基础模型（VFMs）的表征来提高机器人学习的泛化能力和效率。它通过轻量级的路由网络，仅用占总参数量不到0.4%的参数，就能根据下游任务动态选择最相关的视觉专家，并支持参数高效的微调，从而在多项机器人任务中取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 单独的视觉基础模型（VFMs）在机器人学习中虽然能提供丰富的视觉表征，但通常只在特定领域表现优异，泛化能力受限。现有的融合多个VFMs的方法存在任务特定特征选择不灵活和需要高成本全量重训练的问题。

Method: 提出了一种名为VER（Vision Expert transformer for Robot learning）的方法。在预训练阶段，VER将多个VFMs蒸馏成一个视觉专家库。然后，通过微调一个轻量级的路由网络（占总参数量不到0.4%），实现从专家库中动态选择与任务相关的专家。为了提高动态选择的灵活性和精度，引入了Patchwise Expert Routing with Curriculum Top-K Annealing。此外，VER还支持参数高效的微调，以便进行可扩展的专家利用和自适应的机器人领域知识整合。

Result: 在17项不同的机器人任务和多个策略头上，VER都达到了最先进的性能。实验发现，VER能够减少任务不相关区域（如背景）中的大范数异常值，并将表征集中在任务关键区域。

Conclusion: VER通过动态选择和融合多个VFMs的表征，有效解决了现有方法在泛化能力、灵活性和训练效率方面的问题，并在机器人学习任务中取得了优越的性能。

Abstract: Pretrained vision foundation models (VFMs) advance robotic learning via rich
visual representations, yet individual VFMs typically excel only in specific
domains, limiting generality across tasks. Distilling multiple VFMs into a
unified representation for policy can mitigate this limitation but often yields
inflexible task-specific feature selection and requires costly full re-training
to incorporate robot-domain knowledge. We propose VER, a Vision Expert
transformer for Robot learning. During pretraining, VER distills multiple VFMs
into a vision expert library. It then fine-tunes only a lightweight routing
network (fewer than 0.4% of parameters) to dynamically select task-relevant
experts from the pretrained library for downstream robot tasks. We further
introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve
both flexibility and precision of dynamic expert selection. Moreover, VER
supports parameter-efficient finetuning for scalable expert utilization and
adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks
and multiple policy heads, VER achieves state-of-the-art performance. We find
that VER reduces large-norm outliers in task-irrelevant regions (e.g.,
background) and concentrates on task-critical regions. Visualizations and codes
can be found in https://yixiaowang7.github.io/ver_page/.

</details>


### [423] [Adaptive Dynamics Planning for Robot Navigation](https://arxiv.org/abs/2510.05330)
*Lu Yuanjie,Mao Mingyang,Xu Tong,Wang Linji,Lin Xiaomin,Xiao Xuesu*

Main category: cs.RO

TL;DR: 该研究提出了一种名为自适应动力学规划（ADP）的增强学习方法，用于优化机器人导航系统。ADP能够根据环境复杂度动态调整动力学参数，以提高导航的成功率、安全性和效率。


<details>
  <summary>Details</summary>
Motivation: 传统的机器人分层规划方法在全局规划和局部规划之间存在动力学不连续性，导致在复杂环境中导航失败。现有的动态集成方法手动设置动力学保真度，无法适应环境变化。

Method: ADP使用强化学习来动态调整机器人动力学属性，并将其集成到三种不同的规划器和一种独立的导航系统中。

Result: 实验结果表明，ADP在模拟和真实世界测试中均能持续提高导航的成功率、安全性和效率。

Conclusion: ADP是一种有效的增强学习范式，能够提升机器人导航系统的性能，使其能够适应不同复杂度的环境。

Abstract: Autonomous robot navigation systems often rely on hierarchical planning,
where global planners compute collision-free paths without considering
dynamics, and local planners enforce dynamics constraints to produce executable
commands. This discontinuity in dynamics often leads to trajectory tracking
failure in highly constrained environments. Recent approaches integrate
dynamics within the entire planning process by gradually decreasing its
fidelity, e.g., increasing integration steps and reducing collision checking
resolution, for real-time planning efficiency. However, they assume that the
fidelity of the dynamics should decrease according to a manually designed
scheme. Such static settings fail to adapt to environmental complexity
variations, resulting in computational overhead in simple environments or
insufficient dynamics consideration in obstacle-rich scenarios. To overcome
this limitation, we propose Adaptive Dynamics Planning (ADP), a
learning-augmented paradigm that uses reinforcement learning to dynamically
adjust robot dynamics properties, enabling planners to adapt across diverse
environments. We integrate ADP into three different planners and further design
a standalone ADP-based navigation system, benchmarking them against other
baselines. Experiments in both simulation and real-world tests show that ADP
consistently improves navigation success, safety, and efficiency.

</details>


### [424] [A multi-modal tactile fingertip design for robotic hands to enhance dexterous manipulation](https://arxiv.org/abs/2510.05382)
*Zhuowei Xu,Zilin Si,Kevin Zhang,Oliver Kroemer,Zeynep Temel*

Main category: cs.RO

TL;DR: 该论文提出了一种低成本、易于制造、自适应且紧凑的机器人指尖触觉传感器设计，集成了应变片和接触式麦克风，可实现高精度的多模态触觉感知，并成功应用于多种抓取操作任务。


<details>
  <summary>Details</summary>
Motivation: 现有机器人手触觉感知技术成本高、制造集成困难、信息提取不易，限制了其应用，因此需要低成本、易集成的多模态触觉传感器。

Method: 设计了一种集成应变片（测量静态力）和接触式麦克风（测量高频振动）的紧凑型机器人指尖，所有传感器内置，不易磨损。通过实验表征了传感器的性能。

Result: 应变片传感器在0-5N范围内可重复地测量2D平面力，接触式麦克风传感器可区分不同材料的接触属性。在有无视觉遮挡的多种抓取任务中，该触觉传感器提高了任务性能，例如，在堆叠的纸杯任务中成功率达到100%。

Conclusion: 所提出的多模态机器人指尖触觉传感器设计成本低、易于实现且性能可靠，能够有效提取触觉信息，并能根据任务需求灵活组合使用，显著提升了机器人的操作精度和任务成功率，尤其在视觉信息受限的情况下优势明显。

Abstract: Tactile sensing holds great promise for enhancing manipulation precision and
versatility, but its adoption in robotic hands remains limited due to high
sensor costs, manufacturing and integration challenges, and difficulties in
extracting expressive and reliable information from signals. In this work, we
present a low-cost, easy-to-make, adaptable, and compact fingertip design for
robotic hands that integrates multi-modal tactile sensors. We use strain gauge
sensors to capture static forces and a contact microphone sensor to measure
high-frequency vibrations during contact. These tactile sensors are integrated
into a compact design with a minimal sensor footprint, and all sensors are
internal to the fingertip and therefore not susceptible to direct wear and tear
from interactions. From sensor characterization, we show that strain gauge
sensors provide repeatable 2D planar force measurements in the 0-5 N range and
the contact microphone sensor has the capability to distinguish contact
material properties. We apply our design to three dexterous manipulation tasks
that range from zero to full visual occlusion. Given the expressiveness and
reliability of tactile sensor readings, we show that different tactile sensing
modalities can be used flexibly in different stages of manipulation, solely or
together with visual observations to achieve improved task performance. For
instance, we can precisely count and unstack a desired number of paper cups
from a stack with 100\% success rate which is hard to achieve with vision only.

</details>


### [425] [Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios](https://arxiv.org/abs/2510.05425)
*Marta Lagomarsino,Francesco Tassi*

Main category: cs.RO

TL;DR: 该研究提出了一个新颖的在线自适应人机交互（HRI）框架，用于解决上肢残疾个体在工作环境中面临的挑战，旨在促进其积极参与工作。与假设用户健全的传统方法不同，该框架将用户的关节活动度限制纳入了分层最优控制器，使机器人能够实时生成适应用户移动能力的、并能引导用户受损肢体利用其残余功能的行为。


<details>
  <summary>Details</summary>
Motivation: 当前工作环境对上肢残疾个体往往缺乏包容性，阻碍了他们积极参与工作。

Method: 该方法将针对特定关节限制的移动性模型集成到分层最优控制器中，使机器人能够实时生成具有移动感知能力的反应行为，并指导用户受损的肢体利用其残余功能。

Result: 该框架在涉及不同上肢活动能力受损（模拟肘关节和肩关节炎、腕关节受限）的交接任务中进行了测试，并进行了定量和定性比较。初步结果表明，该框架能够根据用户的残余运动范围个性化交互，并根据其功能限制的严重程度鼓励关节使用。

Conclusion: 该框架能够根据用户的残余运动范围个性化交互，并根据其功能限制的严重程度鼓励关节使用。

Abstract: Work environments are often inadequate and lack inclusivity for individuals
with upper-body disabilities. This paper presents a novel online framework for
adaptive human-robot interaction (HRI) that accommodates users' arm mobility
impairments, ultimately aiming to promote active work participation. Unlike
traditional human-robot collaboration approaches that assume able-bodied users,
our method integrates a mobility model for specific joint limitations into a
hierarchical optimal controller. This allows the robot to generate reactive,
mobility-aware behaviour online and guides the user's impaired limb to exploit
residual functional mobility. The framework was tested in handover tasks
involving different upper-limb mobility impairments (i.e., emulated elbow and
shoulder arthritis, and wrist blockage), under both standing and seated
configurations with task constraints using a mobile manipulator, and
complemented by quantitative and qualitative comparisons with state-of-the-art
ergonomic HRI approaches. Preliminary results indicated that the framework can
personalise the interaction to fit within the user's impaired range of motion
and encourage joint usage based on the severity of their functional
limitations.

</details>


### [426] [Active Semantic Perception](https://arxiv.org/abs/2510.05430)
*Huayi Tang,Pratik Chaudhari*

Main category: cs.RO

TL;DR: 我们提出了一种基于大型语言模型（LLM）的主动语义感知方法，通过构建分层场景图并利用其进行空间推理，能够更快速、更准确地理解复杂室内环境的语义。


<details>
  <summary>Details</summary>
Motivation: 场景的语义信息对于探索等任务至关重要，而现有方法难以高效地表示和利用大规模复杂场景的语义信息。

Method: 1. 构建紧凑、分层的多层场景图，可在不同抽象级别（如房间、物体、墙壁、窗户等）表示室内环境及其几何细节。
2. 基于大型语言模型（LLM）的方法，为未观察区域采样与部分观测一致的场景图。
3. 利用采样得到的场景图计算潜在路径点的信息增益，用于空间推理。

Result: 在复杂、逼真的3D室内环境中进行了仿真评估。

Conclusion: 与基线方法相比，我们提出的方法能够更快、更准确地确定环境的语义信息。

Abstract: We develop an approach for active semantic perception which refers to using
the semantics of the scene for tasks such as exploration. We build a compact,
hierarchical multi-layer scene graph that can represent large, complex indoor
environments at various levels of abstraction, e.g., nodes corresponding to
rooms, objects, walls, windows etc. as well as fine-grained details of their
geometry. We develop a procedure based on large language models (LLMs) to
sample plausible scene graphs of unobserved regions that are consistent with
partial observations of the scene. These samples are used to compute an
information gain of a potential waypoint for sophisticated spatial reasoning,
e.g., the two doors in the living room can lead to either a kitchen or a
bedroom. We evaluate this approach in complex, realistic 3D indoor environments
in simulation. We show using qualitative and quantitative experiments that our
approach can pin down the semantics of the environment quicker and more
accurately than baseline approaches.

</details>


### [427] [AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control](https://arxiv.org/abs/2510.05443)
*Shao-Yi Yu,Jen-Wei Wang,Maya Horii,Vikas Garg,Tarek Zohdi*

Main category: cs.RO

TL;DR: 提出一种自适应动力学模型，通过学习状态-动作历史来推断环境，无需直接环境知识，并使用神经常微分方程进行建模，通过两阶段训练学习潜在环境表示，以应对移动机器人中的不确定性和环境变化。


<details>
  <summary>Details</summary>
Motivation: 移动机器人需要在模型预测控制中使用能够响应环境变化的动力学模型，尤其是在难以获取环境信息的情况下。

Method: 使用神经常微分方程构建自适应动力学模型，并通过两阶段训练程序学习潜在的环境表示，以从状态-动作历史中推断操作环境。

Result: 在具有挑战性的目标到达和路径跟踪任务中，在具有不同复杂性的三种机器人平台（2D差速轮式机器人、3D四旋翼无人机和Sphero BOLT机器人）上，在模拟和真实环境中，证明了该方法能够有效处理时空变化的坏境。

Conclusion: 所提出的基于神经常微分方程的自适应动力学模型能够有效地处理移动机器人面临的时空环境变化，无需直接的环境信息，适用于各种机器人平台和任务。

Abstract: Mobile robots, such as ground vehicles and quadrotors, are becoming
increasingly important in various fields, from logistics to agriculture, where
they automate processes in environments that are difficult to access for
humans. However, to perform effectively in uncertain environments using
model-based controllers, these systems require dynamics models capable of
responding to environmental variations, especially when direct access to
environmental information is limited. To enable such adaptivity and facilitate
integration with model predictive control, we propose an adaptive dynamics
model which bypasses the need for direct environmental knowledge by inferring
operational environments from state-action history. The dynamics model is based
on neural ordinary equations, and a two-phase training procedure is used to
learn latent environment representations. We demonstrate the effectiveness of
our approach through goal-reaching and path-tracking tasks on three robotic
platforms of increasing complexity: a 2D differential wheeled robot with
changing wheel contact conditions, a 3D quadrotor in variational wind fields,
and the Sphero BOLT robot under two contact conditions for real-world
deployment. Empirical results corroborate that our method can handle temporally
and spatially varying environmental changes in both simulation and real-world
systems.

</details>


### [428] [Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation](https://arxiv.org/abs/2510.05536)
*Mahboubeh Zarei,Robin Chhabra,Farrokh Janabi-Sharifi*

Main category: cs.RO

TL;DR: 本文提出了一种新颖的去中心化融合方法，用于机器人操纵器中目标物体的姿态和速度估计。


<details>
  <summary>Details</summary>
Motivation: 准确的姿态和速度估计对于机器人操纵器的空间任务规划至关重要。

Method: 采用眼在手和眼到手机器人视觉传感器配置，利用双视场测量跟踪目标物体。运行两个独立的自适应扩展卡尔曼滤波器，在矩阵李群上进行状态预测和更新，并在李群上使用与相关性感知融合规则获得最终融合状态。

Result: 实验结果表明，该方法在UFactory xArm 850上跟踪移动目标时，相比现有最先进的方法具有一致的改进，验证了该去中心化双视场估计框架的有效性和鲁棒性。

Conclusion: 所提出的去中心化双视场估计框架能够有效且鲁棒地进行机器人姿态和速度估计。

Abstract: Accurate pose and velocity estimation is essential for effective spatial task
planning in robotic manipulators. While centralized sensor fusion has
traditionally been used to improve pose estimation accuracy, this paper
presents a novel decentralized fusion approach to estimate both pose and
velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand
vision sensor configuration mounted on a manipulator to track a target object
whose motion is modeled as random walk (stochastic acceleration model). The
robot runs two independent adaptive extended Kalman filters formulated on a
matrix Lie group, developed as part of this work. These filters predict poses
and velocities on the manifold $\mathbb{SE}(3) \times \mathbb{R}^3 \times
\mathbb{R}^3$ and update the state on the manifold $\mathbb{SE}(3)$. The final
fused state comprising the fused pose and velocities of the target is obtained
using a correlation-aware fusion rule on Lie groups. The proposed method is
evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras,
tracking a moving target. Experimental results validate the effectiveness and
robustness of the proposed decentralized dual-view estimation framework,
showing consistent improvements over state-of-the-art methods.

</details>


### [429] [ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation](https://arxiv.org/abs/2510.05547)
*Eugene Vorobiov,Ammar Jaleel Mahmood,Salim Rezvani,Robin Chhabra*

Main category: cs.RO

TL;DR: ARRC系统结合RAG和机器人感知与安全执行，实现了从自然语言指令到安全本地机器人控制的连接，并在实际任务中证明了其有效性。


<details>
  <summary>Details</summary>
Motivation: 连接自然语言指令到安全的本地机器人控制，克服了现有方法的局限性。

Method: 使用RAG（检索增强生成）技术，结合RGB-D感知和安全执行策略，将自然语言指令转化为JSON结构动作计划，并在UFactory xArm 850机器人上进行执行。

Result: 实验结果证明了该方法的有效性，ARRC系统能够提高计划的有效性和适应性，同时保持感知和底层控制的本地化。

Conclusion: 基于RAG的规划方法可以显著提高机器人控制的计划有效性和适应性，同时保持感知和低级控制的本地化。

Abstract: We present ARRC (Advanced Reasoning Robot Control), a practical system that
connects natural-language instructions to safe local robotic control by
combining Retrieval-Augmented Generation (RAG) with RGB-D perception and
guarded execution on an affordable robot arm. The system indexes curated robot
knowledge (movement patterns, task templates, and safety heuristics) in a
vector database, retrieves task-relevant context for each instruction, and
conditions a large language model (LLM) to produce JSON-structured action
plans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven
parallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag
detections fused with depth to produce object-centric metric poses. Execution
is enforced via software safety gates: workspace bounds, speed and force caps,
timeouts, and bounded retries. We describe the architecture, knowledge design,
integration choices, and a reproducible evaluation protocol for tabletop scan,
approach, and pick-place tasks. Experimental results demonstrate the efficacy
of the proposed approach. Our design shows that RAG-based planning can
substantially improve plan validity and adaptability while keeping perception
and low-level control local to the robot.

</details>


### [430] [Federated Split Learning for Resource-Constrained Robots in Industrial IoT: Framework Comparison, Optimization Strategies, and Future Directions](https://arxiv.org/abs/2510.05713)
*Wanli Ni,Hui Tian,Shuai Wang,Chengyang Li,Lei Sun,Zhaohui Yang*

Main category: cs.RO

TL;DR: 本文对工业物联网中的联邦分层学习（FedSL）进行了研究，重点关注资源受限的机器人，并比较了不同FedSL框架的优缺点，对token融合策略进行了分类，并提出了一些优化技术，最后指出了未来的研究方向。


<details>
  <summary>Details</summary>
Motivation: 在工业物联网（IoT）系统，特别是智能工厂中，数据隐私、通信效率和设备异构性是关键问题。FedSL作为一种协作智能范式，为解决这些问题提供了有前景的途径。

Method: 比较了同步、异步、分层和异构FedSL框架在工作流、可扩展性、适应性和局限性方面的表现。对token融合策略进行了分类（输入级、中间级、输出级）。提出了模型压缩、切分层选择、计算频率分配和无线资源管理等自适应优化技术。

Result: 仿真结果验证了所提出的FedSL框架在工业检测场景下的性能。

Conclusion: FedSL在工业物联网场景下具有应用潜力，但仍存在一些开放性问题和未来研究方向，特别是在智能制造系统方面。

Abstract: Federated split learning (FedSL) has emerged as a promising paradigm for
enabling collaborative intelligence in industrial Internet of Things (IoT)
systems, particularly in smart factories where data privacy, communication
efficiency, and device heterogeneity are critical concerns. In this article, we
present a comprehensive study of FedSL frameworks tailored for
resource-constrained robots in industrial scenarios. We compare synchronous,
asynchronous, hierarchical, and heterogeneous FedSL frameworks in terms of
workflow, scalability, adaptability, and limitations under dynamic industrial
conditions. Furthermore, we systematically categorize token fusion strategies
into three paradigms: input-level (pre-fusion), intermediate-level
(intra-fusion), and output-level (post-fusion), and summarize their respective
strengths in industrial applications. We also provide adaptive optimization
techniques to enhance the efficiency and feasibility of FedSL implementation,
including model compression, split layer selection, computing frequency
allocation, and wireless resource management. Simulation results validate the
performance of these frameworks under industrial detection scenarios. Finally,
we outline open issues and research directions of FedSL in future smart
manufacturing systems.

</details>


### [431] [GO-Flock: Goal-Oriented Flocking in 3D Unknown Environments with Depth Maps](https://arxiv.org/abs/2510.05553)
*Yan Rui Tan,Wenqi Liu,Wai Lun Leong,John Guan Zhong Tan,Wayne Wen Huei Yong,Fan Shi,Rodney Swee Huat Teo*

Main category: cs.RO

TL;DR: GO-Flock是一个混合方法，通过结合规划和反应式APF控制来解决传统APF方法在面对障碍物时遇到的难题，如死锁和局部极小值，并在真实环境中成功地控制了9架无人机（6架物理，3架虚拟）进行集群导航。


<details>
  <summary>Details</summary>
Motivation: 传统人工势场（APF）方法在群体控制中常遇到死锁和局部极小值问题，尤其是在有障碍物时。现有解决方案通常被动且效率低下，限制了APF方法在复杂环境下的应用。

Method: GO-Flock是一个混合群体框架，整合了规划和反应式APF控制。它包括一个上游感知模块（处理深度图以提取航点和虚拟代理以避开障碍物）和一个下游集体导航模块（采用新颖的APF策略以在拥挤环境中实现有效的群体行为）。

Result: GO-Flock与被动的APF方法进行了对比评估，证明了其在群体行为和克服局部极小值方面的优势。在充满障碍物的环境中以及通过硬件在环（HIL）实验进行了验证，成功地在森林环境中进行了9架无人机的群体飞行。

Conclusion: GO-Flock通过结合规划和反应式APF控制，有效地解决了传统APF方法在复杂环境中遇到的挑战，并在真实世界的无人机集群导航任务中得到了成功验证。

Abstract: Artificial Potential Field (APF) methods are widely used for reactive
flocking control, but they often suffer from challenges such as deadlocks and
local minima, especially in the presence of obstacles. Existing solutions to
address these issues are typically passive, leading to slow and inefficient
collective navigation. As a result, many APF approaches have only been
validated in obstacle-free environments or simplified, pseudo 3D simulations.
This paper presents GO-Flock, a hybrid flocking framework that integrates
planning with reactive APF-based control. GO-Flock consists of an upstream
Perception Module, which processes depth maps to extract waypoints and virtual
agents for obstacle avoidance, and a downstream Collective Navigation Module,
which applies a novel APF strategy to achieve effective flocking behavior in
cluttered environments. We evaluate GO-Flock against passive APF-based
approaches to demonstrate their respective merits, such as their flocking
behavior and the ability to overcome local minima. Finally, we validate
GO-Flock through obstacle-filled environment and also hardware-in-the-loop
experiments where we successfully flocked a team of nine drones, six physical
and three virtual, in a forest environment.

</details>


### [432] [DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation](https://arxiv.org/abs/2510.05662)
*Taeyeop Lee,Gyuree Kang,Bowen Wen,Youngho Kim,Seunghyeok Back,In So Kweon,David Hyunchul Shim,Kuk-Jin Yoon*

Main category: cs.RO

TL;DR: 本研究提出DeLTa框架，通过结合深度估计、6D姿态估计和视觉-语言规划，实现了对透明物体进行精确的长时程操作，仅需单次演示即可泛化到新物体，并能根据机器人约束优化任务规划。


<details>
  <summary>Details</summary>
Motivation: 现有透明物体操作研究在泛化性和长时程精确操作方面存在局限，本研究旨在解决这些问题。

Method: 提出DeLTa框架，整合深度估计、6D姿态估计和视觉-语言规划，通过单次演示实现6D轨迹泛化，并设计任务规划器以适应单臂机器人的操作约束。

Result: DeLTa框架在综合评估中显著优于现有方法，尤其在需要精确长时程操作的任务中表现突出。

Conclusion: DeLTa框架成功实现了对透明物体的精确长时程操作，并具备良好的泛化能力。

Abstract: Despite the prevalence of transparent object interactions in human everyday
life, transparent robotic manipulation research remains limited to
short-horizon tasks and basic grasping capabilities.Although some methods have
partially addressed these issues, most of them have limitations in
generalizability to novel objects and are insufficient for precise long-horizon
robot manipulation. To address this limitation, we propose DeLTa (Demonstration
and Language-Guided Novel Transparent Object Manipulation), a novel framework
that integrates depth estimation, 6D pose estimation, and vision-language
planning for precise long-horizon manipulation of transparent objects guided by
natural task instructions. A key advantage of our method is its
single-demonstration approach, which generalizes 6D trajectories to novel
transparent objects without requiring category-level priors or additional
training. Additionally, we present a task planner that refines the
VLM-generated plan to account for the constraints of a single-arm, eye-in-hand
robot for long-horizon object manipulation tasks. Through comprehensive
evaluation, we demonstrate that our method significantly outperforms existing
transparent object manipulation approaches, particularly in long-horizon
scenarios requiring precise manipulation capabilities. Project page:
https://sites.google.com/view/DeLTa25/

</details>


### [433] [Verifier-free Test-Time Sampling for Vision Language Action Models](https://arxiv.org/abs/2510.05681)
*Suhyeok Jang,Dongyoung Kim,Changyeon Kim,Youngsuk Kim,Jinwoo Shin*

Main category: cs.RO

TL;DR: MG-Select是一种无需额外训练或外部模块的测试时扩展框架，用于提高机器人控制中视觉-语言-动作模型的精度。


<details>
  <summary>Details</summary>
Motivation: 现有的视觉-语言-动作（VLA）模型在需要高精度的任务中存在局限性，因为它们采用单一推理范式，并且现有的测试时扩展方法需要额外训练且泛化性差。

Method: MG-Select利用KL散度作为置信度度量，从多个候选动作中选择最优动作。它通过在同一VLA模型上输入随机掩码的状态和语言条件来生成参考动作令牌分布，以实现最大不确定性并保持与目标任务分布的一致性。此外，还提出了一种联合训练策略，通过在状态和语言条件上应用dropout来学习条件和无条件分布，从而进一步提高参考分布的质量。

Result: MG-Select在实际任务中带来了显著的性能提升，在分布内和分布外任务上分别提高了28%/35%，并在RoboCasa拾放任务上实现了168%的相对增益。

Conclusion: MG-Select通过利用模型的内部属性，无需额外训练或外部模块，有效提高了VLA模型在机器人控制任务中的精度和泛化能力。

Abstract: Vision-Language-Action models (VLAs) have demonstrated remarkable performance
in robot control. However, they remain fundamentally limited in tasks that
require high precision due to their single-inference paradigm. While test-time
scaling approaches using external verifiers have shown promise, they require
additional training and fail to generalize to unseen conditions. We propose
Masking Distribution Guided Selection (MG-Select), a novel test-time scaling
framework for VLAs that leverages the model's internal properties without
requiring additional training or external modules. Our approach utilizes KL
divergence from a reference action token distribution as a confidence metric
for selecting the optimal action from multiple candidates. We introduce a
reference distribution generated by the same VLA but with randomly masked
states and language conditions as inputs, ensuring maximum uncertainty while
remaining aligned with the target task distribution. Additionally, we propose a
joint training strategy that enables the model to learn both conditional and
unconditional distributions by applying dropout to state and language
conditions, thereby further improving the quality of the reference
distribution. Our experiments demonstrate that MG-Select achieves significant
performance improvements, including a 28%/35% improvement in real-world
in-distribution/out-of-distribution tasks, along with a 168% relative gain on
RoboCasa pick-and-place tasks trained with 30 demonstrations.

</details>


### [434] [Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies](https://arxiv.org/abs/2510.05692)
*Yuhang Zhang,Jiaping Xiao,Chao Yan,Mir Feroskhan*

Main category: cs.RO

TL;DR: OMC-RL 框架通过解耦表示学习和策略学习，并结合掩码 Transformer、对比学习和教师监督，提高了视觉-动作策略学习的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 直接从高维视觉观测映射到动作命令的强化学习方法存在样本效率低和仿真到现实差距大的问题。

Method: OMC-RL 框架包含两个阶段：1. 预训练表示学习阶段：使用掩码 Transformer 模块，结合时间建模和对比学习，从连续视觉输入中提取时间感知和任务相关的表示。2. 下游策略学习阶段：冻结编码器提取视觉表示，并使用具有全局状态信息的教师策略监督训练，然后逐渐减少监督以进行独立探索。

Result: OMC-RL 在模拟和现实世界环境中，在样本效率、最终策略性能和跨不同复杂场景的泛化能力方面均表现出色。

Conclusion: OMC-RL 框架能有效提高视觉-动作策略学习的样本效率和性能。

Abstract: A prevailing approach for learning visuomotor policies is to employ
reinforcement learning to map high-dimensional visual observations directly to
action commands. However, the combination of high-dimensional visual inputs and
agile maneuver outputs leads to long-standing challenges, including low sample
efficiency and significant sim-to-real gaps. To address these issues, we
propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a
novel framework designed to improve the sample efficiency and asymptotic
performance of visuomotor policy learning. OMC-RL explicitly decouples the
learning process into two stages: an upstream representation learning stage and
a downstream policy learning stage. In the upstream stage, a masked Transformer
module is trained with temporal modeling and contrastive learning to extract
temporally-aware and task-relevant representations from sequential visual
inputs. After training, the learned encoder is frozen and used to extract
visual representations from consecutive frames, while the Transformer module is
discarded. In the downstream stage, an oracle teacher policy with privileged
access to global state information supervises the agent during early training
to provide informative guidance and accelerate early policy learning. This
guidance is gradually reduced to allow independent exploration as training
progresses. Extensive experiments in simulated and real-world environments
demonstrate that OMC-RL achieves superior sample efficiency and asymptotic
policy performance, while also improving generalization across diverse and
perceptually complex scenarios.

</details>


### [435] [Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs](https://arxiv.org/abs/2510.05707)
*David Boetius,Abdelrahman Abdelnaby,Ashok Kumar,Stefan Leue,Abdalla Swikir,Fares J. Abu-Dakka*

Main category: cs.RO

TL;DR: 我们提出了一个在黎曼流形上学习稳定动力学系统的框架，使用神经常微分方程和Lyapunov稳定性判据来保证稳定性。


<details>
  <summary>Details</summary>
Motivation: 在黎曼流形上学习稳定的动力学系统对于安全的机器人运动规划和控制至关重要，但黎曼流形的几何约束带来了挑战。

Method: 使用神经常微分方程，通过将神经向量场投影到黎曼流形上来保证Lyapunov稳定性，同时利用灵活的神经参数化来表示向量场和Lyapunov函数，并在流形上直接演化解决方案。

Result: 该框架能够准确表示复杂的轨迹，同时遵守流形约束，并在单位四元数流形、对称正定矩阵流形以及在 R^3 x S^3 上演化的机器人运动等数据集上进行了演示。

Conclusion: 该框架在性能、可扩展性和实际应用方面都得到了广泛验证，能够学习机器人运动并保证稳定性。

Abstract: Learning stable dynamical systems from data is crucial for safe and reliable
robot motion planning and control. However, extending stability guarantees to
trajectories defined on Riemannian manifolds poses significant challenges due
to the manifold's geometric constraints. To address this, we propose a general
framework for learning stable dynamical systems on Riemannian manifolds using
neural ordinary differential equations. Our method guarantees stability by
projecting the neural vector field evolving on the manifold so that it strictly
satisfies the Lyapunov stability criterion, ensuring stability at every system
state. By leveraging a flexible neural parameterisation for both the base
vector field and the Lyapunov function, our framework can accurately represent
complex trajectories while respecting manifold constraints by evolving
solutions directly on the manifold. We provide an efficient training strategy
for applying our framework and demonstrate its utility by solving Riemannian
LASA datasets on the unit quaternion (S^3) and symmetric positive-definite
matrix manifolds, as well as robotic motions evolving on \mathbb{R}^3 \times
S^3. We demonstrate the performance, scalability, and practical applicability
of our approach through extensive simulations and by learning robot motions in
a real-world experiment.

</details>


### [436] [Precise and Efficient Collision Prediction under Uncertainty in Autonomous Driving](https://arxiv.org/abs/2510.05729)
*Marc Kaufeld,Johannes Betz*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: This research introduces two efficient methods to estimate the collision risk
of planned trajectories in autonomous driving under uncertain driving
conditions. Deterministic collision checks of planned trajectories are often
inaccurate or overly conservative, as noisy perception, localization errors,
and uncertain predictions of other traffic participants introduce significant
uncertainty into the planning process. This paper presents two semi-analytic
methods to compute the collision probability of planned trajectories with
arbitrary convex obstacles. The first approach evaluates the probability of
spatial overlap between an autonomous vehicle and surrounding obstacles, while
the second estimates the collision probability based on stochastic boundary
crossings. Both formulations incorporate full state uncertainties, including
position, orientation, and velocity, and achieve high accuracy at computational
costs suitable for real-time planning. Simulation studies verify that the
proposed methods closely match Monte Carlo results while providing significant
runtime advantages, enabling their use in risk-aware trajectory planning. The
collision estimation methods are available as open-source software:
https://github.com/TUM-AVS/Collision-Probability-Estimation

</details>


### [437] [Human-in-the-loop Optimisation in Robot-assisted Gait Training](https://arxiv.org/abs/2510.05780)
*Andreas Christou,Andreas Sochopoulos,Elliot Lister,Sethu Vijayakumar*

Main category: cs.RO

TL;DR: 该研究探讨了使用人类在回路优化 (HILO) 和协方差矩阵自适应进化策略 (CMA-ES) 来个性化下肢外骨骼的步态训练辅助。尽管 CMA-ES 为每个受试者收敛到了一组独特的刚度值，但在验证试验中并未观察到对受试者表现的可衡量的影响。


<details>
  <summary>Details</summary>
Motivation: 由于个体行走模式的可变性，需要开发能够适应个体特征的机器人控制器，以实现外骨骼在步态训练中的个性化辅助。

Method: 采用协方差矩阵自适应进化策略 (CMA-ES) 优化下肢外骨骼的辅助即时控制器，并让六名健康个体参与了两天的实验。

Result: CMA-ES 似乎为每个受试者收敛到了一组独特的刚度值，但在随后的验证试验中，并未观察到对受试者表现的显著影响。

Conclusion: 研究结果表明，人类-机器人共同适应和人类行为的可变性可能比基于规则的个性化辅助控制器的潜在好处影响更大。这项工作有助于理解当前外骨骼辅助步态康复中个性化方法的局限性，并指出了在这一领域有效实施人类在回路优化的关键挑战。

Abstract: Wearable robots offer a promising solution for quantitatively monitoring gait
and providing systematic, adaptive assistance to promote patient independence
and improve gait. However, due to significant interpersonal and intrapersonal
variability in walking patterns, it is important to design robot controllers
that can adapt to the unique characteristics of each individual. This paper
investigates the potential of human-in-the-loop optimisation (HILO) to deliver
personalised assistance in gait training. The Covariance Matrix Adaptation
Evolution Strategy (CMA-ES) was employed to continuously optimise an
assist-as-needed controller of a lower-limb exoskeleton. Six healthy
individuals participated over a two-day experiment. Our results suggest that
while the CMA-ES appears to converge to a unique set of stiffnesses for each
individual, no measurable impact on the subjects' performance was observed
during the validation trials. These findings highlight the impact of
human-robot co-adaptation and human behaviour variability, whose effect may be
greater than potential benefits of personalising rule-based assistive
controllers. Our work contributes to understanding the limitations of current
personalisation approaches in exoskeleton-assisted gait rehabilitation and
identifies key challenges for effective implementation of human-in-the-loop
optimisation in this domain.

</details>


### [438] [VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation](https://arxiv.org/abs/2510.05827)
*Haoran Zhang,Shuanghao Bai,Wanqi Zhou,Yuedi Zhang,Qi Zhang,Pengxiang Ding,Cheng Chi,Donglin Wang,Badong Chen*

Main category: cs.RO

TL;DR: VCoT-Grasp是一个端到端的抓取基础模型，通过引入视觉思维链推理来增强抓取生成中的视觉理解能力，解决了现有方法泛化能力不足和过于依赖对话语义的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于语言的抓取方法存在推理和泛化能力不足，或依赖复杂流水线的问题。此外，当前抓取基础模型过于侧重对话和物体语义，导致性能不佳且限制于单物体抓取。

Method: 提出VCoT-Grasp，一个端到端的抓取基础模型，通过引入视觉思维链（VCoT）推理来增强视觉理解，以进行抓取生成。该模型采用多轮处理范式，动态关注视觉输入并提供可解释的推理轨迹。训练方面，引入并优化了大规模数据集VCoT-GraspSet。

Result: 在VCoT-GraspSet和真实机器人数据集上的大量实验表明，该方法显著提高了抓取成功率，并能有效地泛化到未见过的物体、背景和干扰物上。

Conclusion: VCoT-Grasp通过视觉思维链推理，在处理复杂环境下的抓取任务方面展现出强大的推理和泛化能力，显著优于现有方法。

Abstract: Robotic grasping is one of the most fundamental tasks in robotic
manipulation, and grasp detection/generation has long been the subject of
extensive research. Recently, language-driven grasp generation has emerged as a
promising direction due to its practical interaction capabilities. However,
most existing approaches either lack sufficient reasoning and generalization
capabilities or depend on complex modular pipelines. Moreover, current grasp
foundation models tend to overemphasize dialog and object semantics, resulting
in inferior performance and restriction to single-object grasping. To maintain
strong reasoning ability and generalization in cluttered environments, we
propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates
visual chain-of-thought reasoning to enhance visual understanding for grasp
generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically
focuses on visual inputs while providing interpretable reasoning traces. For
training, we refine and introduce a large-scale dataset, VCoT-GraspSet,
comprising 167K synthetic images with over 1.36M grasps, as well as 400+
real-world images with more than 1.2K grasps, annotated with intermediate
bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot
demonstrate that our method significantly improves grasp success rates and
generalizes effectively to unseen objects, backgrounds, and distractors. More
details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.

</details>


### [439] [A Co-Design Framework for Energy-Aware Monoped Jumping with Detailed Actuator Modeling](https://arxiv.org/abs/2510.05923)
*Aman Singh,Aastha Mishra,Deepak Kapa,Suryank Joshi,Shishir Kolathaya*

Main category: cs.RO

TL;DR: 该研究提出了一个三阶段的联合设计优化框架，用于同时最大化单腿跳跃高度并最小化其机械能消耗，解决了现有方法忽略两者权衡以及简化执行器质量模型的问题。


<details>
  <summary>Details</summary>
Motivation: 现有单腿机器人（monoped）的跳跃高度和能耗优化框架通常只关注最大高度或最小能耗中的一个，忽略了它们之间的权衡关系。此外，这些框架常常省略齿轮箱参数优化，并使用过于简化的执行器质量模型，导致产生的机器人在实际中难以复现。

Method: 本研究引入了一个新颖的三阶段联合设计优化框架，该框架能够同时优化单腿机器人的跳跃高度和机械能消耗。该方法明确考虑了现实的执行器质量模型，并在统一的框架内优化了包括齿轮箱在内的机械设计参数和控制参数。

Result: 通过该框架生成的机器人设计，经过实验评估，相比基线设计，机械能消耗降低了50%，同时实现了0.8米的跳跃高度。生成的机器人设计还可以自动生成参数化的CAD模型，便于直接制造，减少了手动设计迭代。

Conclusion: 本研究提出的三阶段联合设计优化框架能够有效地联合优化单腿机器人的跳跃高度和机械能消耗，并考虑了更实际的执行器质量模型和齿轮箱参数，实现了显著的性能提升和更易于制造的设计。

Abstract: A monoped's jump height and energy consumption depend on both, its mechanical
design and control strategy. Existing co-design frameworks typically optimize
for either maximum height or minimum energy, neglecting their trade-off. They
also often omit gearbox parameter optimization and use oversimplified actuator
mass models, producing designs difficult to replicate in practice. In this
work, we introduce a novel three-stage co-design optimization framework that
jointly maximizes jump height while minimizing mechanical energy consumption of
a monoped. The proposed method explicitly incorporates realistic actuator mass
models and optimizes mechanical design (including gearbox) and control
parameters within a unified framework. The resulting design outputs are then
used to automatically generate a parameterized CAD model suitable for direct
fabrication, significantly reducing manual design iterations. Our experimental
evaluations demonstrate a 50 percent reduction in mechanical energy consumption
compared to the baseline design, while achieving a jump height of 0.8m. Video
presentation is available at http://y2u.be/XW8IFRCcPgM

</details>


### [440] [Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion](https://arxiv.org/abs/2510.05957)
*Vaughn Gzenda,Robin Chhabra*

Main category: cs.RO

TL;DR: 利用基于模型的强化学习（MB-RL）框架，通过学习到的潜在动力学来优化软体机器人的爬行策略，该策略仅基于嘈杂的传感器反馈。


<details>
  <summary>Details</summary>
Motivation: 为具有模型不准确性、传感器噪声和需要发现步态的软体机器人设计控制策略具有挑战性。

Method: 提出一种基于模型的强化学习（MB-RL）框架，其中从板载传感器推断出的潜在动力学作为预测模型，指导 actor-critic 算法优化运动策略。

Result: 在模拟中，学习到的潜在动力学能够进行短时程运动预测，actor-critic 发现了有效的运动策略。

Conclusion: 基于潜在动力学的 MB-RL 在仅基于嘈杂传感器反馈的软体机器人自适应运动方面具有潜力。

Abstract: Soft robotic crawlers are mobile robots that utilize soft body deformability
and compliance to achieve locomotion through surface contact. Designing control
strategies for such systems is challenging due to model inaccuracies, sensor
noise, and the need to discover locomotor gaits. In this work, we present a
model-based reinforcement learning (MB-RL) framework in which latent dynamics
inferred from onboard sensors serve as a predictive model that guides an
actor-critic algorithm to optimize locomotor policies. We evaluate the
framework on a minimal crawler model in simulation using inertial measurement
units and time-of-flight sensors as observations. The learned latent dynamics
enable short-horizon motion prediction while the actor-critic discovers
effective locomotor policies. This approach highlights the potential of
latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion
based solely on noisy sensor feedback.

</details>


### [441] [The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics](https://arxiv.org/abs/2510.05981)
*Cristina Luna,Alba Guerra,Almudena Moreno,Manuel Esquer,Willy Roa,Mateusz Krawczak,Robert Popela,Piotr Osica,Davide Nicolis*

Main category: cs.RO

TL;DR: 该论文提出了一种名为 DISTANT 的新型火星车移动系统设计，将驱动和转向执行器移至车体内部的温箱中，以提高在极端环境下的鲁棒性和寿命，满足了 50 公里行程的要求。


<details>
  <summary>Details</summary>
Motivation: 为了满足行星探测任务对在极端环境下长期可靠运行的移动系统的需求，特别是长距离行进任务中的热循环、粉尘污染和机械磨损等挑战。

Method: 设计了一种名为 DISTANT 的系统，将火星车的牵引和转向执行器从车轮位置移至车体内部的温箱中，并采用了双叉臂悬架、万向节和绞盘驱动转向架构，实现了独立的车轮牵引、转向控制和悬架管理，同时将所有电机置于受保护的环境中。

Result: 该设计满足了 50 公里行程的要求，且性能无下降，并集成了防尘和热管理方案。

Conclusion: DISTANT 系统通过将关键的移动部件置于受保护的温箱内，成功地解决了行星探测任务中的关键挑战，为实现更长距离、更可靠的火星车探测任务提供了有效的解决方案。

Abstract: Planetary exploration missions require robust locomotion systems capable of
operating in extreme environments over extended periods. This paper presents
the DISTANT (Distant Transmission and Steering Systems) design, a novel
approach for relocating rover traction and steering actuators from
wheel-mounted positions to a thermally protected warm box within the rover
body. The design addresses critical challenges in long-distance traversal
missions by protecting sensitive components from thermal cycling, dust
contamination, and mechanical wear. A double wishbone suspension configuration
with cardan joints and capstan drive steering has been selected as the optimal
architecture following comprehensive trade-off analysis. The system enables
independent wheel traction, steering control, and suspension management whilst
maintaining all motorisation within the protected environment. The design meets
a 50 km traverse requirement without performance degradation, with integrated
dust protection mechanisms and thermal management solutions. Testing and
validation activities are planned for Q1 2026 following breadboard
manufacturing at 1:3 scale.

</details>


### [442] [AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations](https://arxiv.org/abs/2510.05985)
*Cristina Luna,Robert Field,Steven Kay*

Main category: cs.RO

TL;DR: 本研究提出了一种集成AI系统，通过结合远距离障碍物检测、多机器人协同以及基于深度学习的地形分类，将行星探测器的行驶速度从约10厘米/秒提高到1米/秒，并提高了操作安全性。


<details>
  <summary>Details</summary>
Motivation: 当前的行星探测器行驶速度过慢（约10厘米/秒），严重限制了探索效率。

Method: 开发了三个关键组件：（1）FASTNAV远距离障碍物检测器（FOD），利用计算机视觉技术在1米/秒的速度下进行障碍物检测；（2）CISRU多机器人协同框架，支持人类与机器人协同进行就地资源利用；（3）ViBEKO和AIAXR的深度学习地形分类研究。

Result: 在火星模拟环境中进行了现场验证，技术成熟度达到4级（TRL 4），显著提高了探测器的行驶速度、分类准确性和操作安全性。

Conclusion: 本研究提出的集成AI系统能够将行星探测器的行驶速度提高一个数量级，为下一代行星任务提供了更高效、更安全的操作能力。

Abstract: Current planetary rovers operate at traverse speeds of approximately 10 cm/s,
fundamentally limiting exploration efficiency. This work presents integrated AI
systems which significantly improve autonomy through three components: (i) the
FASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s
speeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot
coordination framework enabling human-robot collaboration for in-situ resource
utilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain
classification studies. Field validation in Mars analogue environments
demonstrated these systems at Technology Readiness Level 4, providing
measurable improvements in traverse speed, classification accuracy, and
operational safety for next-generation planetary missions.

</details>


### [443] [Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations](https://arxiv.org/abs/2510.05992)
*Tien-Dat Nguyen,Thien-Minh Nguyen,Vinh-Hao Nguyen*

Main category: cs.RO

TL;DR: 提出一种两阶段方法，融合UWB和SLAM数据，实现一致且精确的机器人定位。


<details>
  <summary>Details</summary>
Motivation: SLAM定位坐标系不一致，UWB定位需要精确的锚点坐标，旨在解决这些问题。

Method: 第一阶段：通过连续时间批处理优化，利用UWB测距和里程计数据，结合高度先验和锚点间距离因子，恢复锚点3D位置。第二阶段：使用滑动窗口优化，融合UWB和SLAM数据，实现同一坐标系下的精确率定位。

Result: 在NTU VIRAL数据集的六种无人机飞行场景中进行了实验，验证了在一个运行周期内进行的校准足以支持后续运行的精确定位。

Conclusion: 所提出的两阶段方法可以实现跨会话的一致且精确的定位，并且在单个运行周期内进行校准就足够了。

Abstract: Onboard simultaneous localization and mapping (SLAM) methods are commonly
used to provide accurate localization information for autonomous robots.
However, the coordinate origin of SLAM estimate often resets for each run. On
the other hand, UWB-based localization with fixed anchors can ensure a
consistent coordinate reference across sessions; however, it requires an
accurate assignment of the anchor nodes' coordinates. To this end, we propose a
two-stage approach that calibrates and fuses UWB data and SLAM data to achieve
coordinate-wise consistent and accurate localization in the same environment.
In the first stage, we solve a continuous-time batch optimization problem by
using the range and odometry data from one full run, incorporating height
priors and anchor-to-anchor distance factors to recover the anchors' 3D
positions. For the subsequent runs in the second stage, a sliding-window
optimization scheme fuses the UWB and SLAM data, which facilitates accurate
localization in the same coordinate system. Experiments are carried out on the
NTU VIRAL dataset with six scenarios of UAV flight, and we show that
calibration using data in one run is sufficient to enable accurate localization
in the remaining runs. We release our source code to benefit the community at
https://github.com/ntdathp/slam-uwb-calibration.

</details>


### [444] [Cross-Embodiment Dexterous Hand Articulation Generation via Morphology-Aware Learning](https://arxiv.org/abs/2510.06068)
*Heng Zhang,Kevin Yuchen Ma,Mike Zheng Shou,Weisi Lin,Yan Wu*

Main category: cs.RO

TL;DR: 本篇论文提出了一种基于本征抓取（eigengrasp）的端到端框架，用于跨不同手部模型生成灵巧抓取，即使在只有少量样本或未见过的手部模型上也能实现高成功率。


<details>
  <summary>Details</summary>
Motivation: 高自由度的多指灵巧手抓取是一个具有挑战性的问题，现有的基于优化的方法计算成本高，而现有的端到端方法需要针对特定手部模型进行大规模训练，泛化能力有限。

Method: 提出了一种基于本征抓取的端到端框架。该框架根据手部形态描述生成形态嵌入和本征抓取集。结合物体点云和腕部姿态，幅值预测器在低维空间中回归关节系数，并解码为完整的关节运动。使用强调指尖相关运动并注入形态特定结构的运动学感知损失（KAL）进行监督学习。

Result: 在模拟环境中，该模型在三种不同的手部模型上对未见过物体实现了 91.9% 的平均抓取成功率，平均推理时间少于 0.4 秒。通过少样本适应到一种新的手部模型后，在模拟环境中对未见过物体实现了 85.6% 的成功率，并在真实世界实验中达到了 87% 的成功率。

Conclusion: 所提出的框架能够有效地进行跨手部模型的抓取生成，即使在仅用少量样本进行适应或面对全新手部模型时，也能在模拟和真实世界中取得高成功率。

Abstract: Dexterous grasping with multi-fingered hands remains challenging due to
high-dimensional articulations and the cost of optimization-based pipelines.
Existing end-to-end methods require training on large-scale datasets for
specific hands, limiting their ability to generalize across different
embodiments. We propose an eigengrasp-based, end-to-end framework for
cross-embodiment grasp generation. From a hand's morphology description, we
derive a morphology embedding and an eigengrasp set. Conditioned on these,
together with the object point cloud and wrist pose, an amplitude predictor
regresses articulation coefficients in a low-dimensional space, which are
decoded into full joint articulations. Articulation learning is supervised with
a Kinematic-Aware Articulation Loss (KAL) that emphasizes fingertip-relevant
motions and injects morphology-specific structure. In simulation on unseen
objects across three dexterous hands, our model attains a 91.9% average grasp
success rate with less than 0.4 seconds inference per grasp. With few-shot
adaptation to an unseen hand, it achieves 85.6% success on unseen objects in
simulation, and real-world experiments on this few-shot generalized hand
achieve an 87% success rate. The code and additional materials will be made
available upon publication on our project website
https://connor-zh.github.io/cross_embodiment_dexterous_grasping.

</details>


### [445] [Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor](https://arxiv.org/abs/2510.06085)
*Roman Ibrahimov,Jannik Matthias Heinen*

Main category: cs.RO

TL;DR: 该项目提出了一种受生物启发的、使用分布式优化的多机器人系统，用于在未知环境中进行高效的探索和地图绘制。


<details>
  <summary>Details</summary>
Motivation: 该项目旨在通过受生物启发的分布式优化多机器人系统，实现未知环境的高效探索和地图绘制。

Method: 该方法采用受墙壁跟随行为启发的分布式控制策略，每个机器人利用安装在机器人表面的触觉传感器（类似于蟑螂的触角）自主探索其邻域，并通过记录碰撞点来处理障碍物，最终整合各机器人绘制的局部地图形成全局2D地图。

Result: 通过在模拟的1.5 x 1.5米环境中进行实验，使用e-puck机器人，结果表明该系统在实现高覆盖率、最小化碰撞和构建精确2D地图方面是有效的。

Conclusion: 该受生物启发的分布式优化多机器人系统能够有效地探索未知环境并绘制地图，在搜索与救援、工业检查和环境监测等领域具有潜在应用价值。

Abstract: This project proposes a bioinspired multi-robot system using Distributed
Optimization for efficient exploration and mapping of unknown environments.
Each robot explores its environment and creates a map, which is afterwards put
together to form a global 2D map of the environment. Inspired by wall-following
behaviors, each robot autonomously explores its neighborhood based on a tactile
sensor, similar to the antenna of a cockroach, mounted on the surface of the
robot. Instead of avoiding obstacles, robots log collision points when they
touch obstacles. This decentralized control strategy ensures effective task
allocation and efficient exploration of unknown terrains, with applications in
search and rescue, industrial inspection, and environmental monitoring. The
approach was validated through experiments using e-puck robots in a simulated
1.5 x 1.5 m environment with three obstacles. The results demonstrated the
system's effectiveness in achieving high coverage, minimizing collisions, and
constructing accurate 2D maps.

</details>


### [446] [Towards Autonomous Tape Handling for Robotic Wound Redressing](https://arxiv.org/abs/2510.06127)
*Xiao Liang,Lu Shen,Peihan Zhang,Soofiyan Atar,Florian Richter,Michael Yip*

Main category: cs.RO

TL;DR: 该研究提出了一种用于伤口敷料新任务的机器人框架，特别是胶带操作，以实现自动化伤口护理。


<details>
  <summary>Details</summary>
Motivation: 慢性伤口（如糖尿病、压疮和静脉溃疡）影响着美国约650万患者，每年治疗费用超过250亿美元。尽管负担沉重，但慢性伤口护理仍然是一个常规但手动过程，完全由训练有素的临床医生执行。研究人员设想了一个机器人和自动化支持伤口护理的未来，以降低成本并改善患者的治疗效果。

Method: 研究人员提出了一种用于胶带初始分离（TID）和安全胶带放置的自主框架。对于TID，他们采用了一种基于力反馈的模仿学习方法，该方法通过人类遥操作演示进行训练。对于胶带放置，他们开发了一种基于数值轨迹优化的方法，以确保在各种解剖学表面上平稳粘附且无皱纹地应用。

Result: 通过广泛的实验验证了所提出的方法，在定量评估和集成伤口敷料流程中均展示了可靠的性能。

Conclusion: 研究结果表明，胶带操作是实现实用机器人伤口护理自动化的关键一步。

Abstract: Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over
6.5 million patients in the United States alone and generate an annual cost
exceeding \$25 billion. Despite this burden, chronic wound care remains a
routine yet manual process performed exclusively by trained clinicians due to
its critical safety demands. We envision a future in which robotics and
automation support wound care to lower costs and enhance patient outcomes. This
paper introduces an autonomous framework for one of the most fundamental yet
challenging subtasks in wound redressing: adhesive tape manipulation.
Specifically, we address two critical capabilities: tape initial detachment
(TID) and secure tape placement. To handle the complex adhesive dynamics of
detachment, we propose a force-feedback imitation learning approach trained
from human teleoperation demonstrations. For tape placement, we develop a
numerical trajectory optimization method based to ensure smooth adhesion and
wrinkle-free application across diverse anatomical surfaces. We validate these
methods through extensive experiments, demonstrating reliable performance in
both quantitative evaluations and integrated wound redressing pipelines. Our
results establish tape manipulation as an essential step toward practical
robotic wound care automation.

</details>


### [447] [Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments](https://arxiv.org/abs/2510.06146)
*Jaehwan Jeong,Tuan-Anh Vu,Radha Lahoti,Jiawen Wang,Vivek Alumootil,Sangpil Kim,M. Khalid Jawed*

Main category: cs.RO

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Robotic pollination offers a promising alternative to manual labor and
bumblebee-assisted methods in controlled agriculture, where wind-driven
pollination is absent and regulatory restrictions limit the use of commercial
pollinators. In this work, we present and validate a vision-guided robotic
framework that uses data from an end-effector mounted RGB-D sensor and combines
3D plant reconstruction, targeted grasp planning, and physics-based vibration
modeling to enable precise pollination. First, the plant is reconstructed in 3D
and registered to the robot coordinate frame to identify obstacle-free grasp
poses along the main stem. Second, a discrete elastic rod model predicts the
relationship between actuation parameters and flower dynamics, guiding the
selection of optimal pollination strategies. Finally, a manipulator with soft
grippers grasps the stem and applies controlled vibrations to induce pollen
release. End-to-end experiments demonstrate a 92.5\% main-stem grasping success
rate, and simulation-guided optimization of vibration parameters further
validates the feasibility of our approach, ensuring that the robot can safely
and effectively perform pollination without damaging the flower. To our
knowledge, this is the first robotic system to jointly integrate vision-based
grasping and vibration modeling for automated precision pollination.

</details>


### [448] [A Preview of HoloOcean 2.0](https://arxiv.org/abs/2510.06160)
*Blake Romrell,Abigail Austin,Braden Meyers,Ryan Anderson,Carter Noh,Joshua G. Mangelson*

Main category: cs.RO

TL;DR: HoloOcean 2.0是一个新的海洋机器人模拟器，使用虚幻引擎5.3，具有先进的车辆动力学和ROS2支持，旨在提高海洋机器人开发和验证的保真度。


<details>
  <summary>Details</summary>
Motivation: 随着海洋机器人领域的关注度不断提高，人们对开发更高保真度的海洋传感器、物理和视觉渲染模拟的兴趣日益浓厚，以支持自主海洋机器人的开发和验证。

Method: HoloOcean 2.0迁移到虚幻引擎5.3，使用Fossen的模型进行先进的车辆动力学，并使用自定义桥接器支持ROS2。

Result: HoloOcean 2.0 带来了先进的功能，包括虚幻引擎5.3的迁移、基于Fossen模型的先进车辆动力学以及ROS2支持。

Conclusion: HoloOcean 2.0 是一个通用的海洋模拟器，能够支持各种任务，并正在开发包括更高效的光线追踪声纳实现、语义传感器、环境生成工具、体积环境效应和逼真波浪在内的其他功能。

Abstract: Marine robotics simulators play a fundamental role in the development of
marine robotic systems. With increased focus on the marine robotics field in
recent years, there has been significant interest in developing higher
fidelitysimulation of marine sensors, physics, and visual rendering
capabilities to support autonomous marine robot development and validation.
HoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art
features under a general marine simulator capable of supporting a variety of
tasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE)
5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2
using a custom bridge. Additional features are currently in development,
including significantly more efficient ray tracing-based sidescan,
forward-looking, and bathymetric sonar implementations; semantic sensors;
environment generation tools; volumetric environmental effects; and realistic
waves.

</details>


### [449] [DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation](https://arxiv.org/abs/2510.06199)
*Chengyang Zhao,Uksang Yoo,Arkadeep Narayan Chaudhury,Giljoo Nam,Jonathan Francis,Jeffrey Ichnowski,Jean Oh*

Main category: cs.RO

TL;DR: DYMO-Hair是一个基于模型的机器人美发系统，它使用新颖的动力学学习范式和包含多样化发型的紧凑3D潜在空间，实现了对未见过发型的泛化能力，并在模拟和现实世界中均取得了优于现有技术的效果。


<details>
  <summary>Details</summary>
Motivation: 由于头发精细的物理结构和复杂的动力学特性，头发护理对于行动不便者来说仍然难以实现，对于自主机器人系统来说也充满挑战。

Method: 提出了DYMO-Hair系统，采用基于模型的机器人美发方法。其核心是一个新颖的动力学学习范式，适用于头发这类体积量，结合了动作条件潜在状态编辑机制和一个包含多样化发型的紧凑3D潜在空间，以提高泛化能力。该潜在空间利用新颖的头发物理模拟器进行大规模预训练，实现了对未见过发型的泛化。结合动力学模型和模型预测路径积分（MPPI）规划器，DYMO-Hair能够执行视觉目标导向的发型设计。

Result: 在模拟实验中，DYMO-Hair的动力学模型在捕捉多样化、未见过发型的局部形变方面优于基线模型。在闭环发型设计任务中，DYMO-Hair在未见过发型上的表现也优于基线模型，最终几何误差平均降低22%，成功率平均提高42%。在现实世界的实验中，该系统能够零样本迁移到假发上，并在具有挑战性的未见过发型上取得了一致的成功，而现有技术在该场景下则失败了。

Conclusion: DYMO-Hair为基于模型的机器人美发奠定了基础，朝着在非约束物理环境中实现更具泛化性、灵活性和可及性的机器人美发方向迈出了重要一步。

Abstract: Hair care is an essential daily activity, yet it remains inaccessible to
individuals with limited mobility and challenging for autonomous robot systems
due to the fine-grained physical structure and complex dynamics of hair. In
this work, we present DYMO-Hair, a model-based robot hair care system. We
introduce a novel dynamics learning paradigm that is suited for volumetric
quantities such as hair, relying on an action-conditioned latent state editing
mechanism, coupled with a compact 3D latent space of diverse hairstyles to
improve generalizability. This latent space is pre-trained at scale using a
novel hair physics simulator, enabling generalization across previously unseen
hairstyles. Using the dynamics model with a Model Predictive Path Integral
(MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair
styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model
outperforms baselines on capturing local deformation for diverse, unseen
hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling
tasks on unseen hairstyles, with an average of 22% lower final geometric error
and 42% higher success rate than the state-of-the-art system. Real-world
experiments exhibit zero-shot transferability of our system to wigs, achieving
consistent success on challenging unseen hairstyles where the state-of-the-art
system fails. Together, these results introduce a foundation for model-based
robot hair care, advancing toward more generalizable, flexible, and accessible
robot hair styling in unconstrained physical environments. More details are
available on our project page: https://chengyzhao.github.io/DYMOHair-web/.

</details>


### [450] [EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model](https://arxiv.org/abs/2510.06207)
*Zefu Lin,Rongxu Cui,Chen Hanning,Xiangyu Wang,Junjia Xu,Xiaojuan Jin,Chen Wenbo,Hui Zhou,Lue Fan,Wenling Li,Zhaoxiang Zhang*

Main category: cs.RO

TL;DR: EmbodiedCoder是一个无需训练的框架，利用代码模型直接生成机器人轨迹，用于开放世界移动机器人操作，实现了灵活的对象几何参数化和操作轨迹合成，无需额外数据收集或微调。


<details>
  <summary>Details</summary>
Motivation: 现有机器人控制方法难以扩展到多样化环境，因为它们依赖大量标注数据且可解释性有限。

Method: EmbodiedCoder利用代码模型直接生成可执行的机器人轨迹，通过将高级指令转化为代码，实现灵活的对象几何参数化和操作轨迹合成，无需额外数据收集或微调。

Result: 在真实移动机器人上的实验表明，EmbodiedCoder在多样化的长期任务中表现稳健，并能有效地泛化到新的物体和环境中。

Conclusion: EmbodiedCoder提供了一种可解释的方法，能够连接高级推理和低级控制，实现了从固定原语到通用机器人智能的转变。

Abstract: Recent advances in control robot methods, from end-to-end
vision-language-action frameworks to modular systems with predefined
primitives, have advanced robots' ability to follow natural language
instructions. Nonetheless, many approaches still struggle to scale to diverse
environments, as they often rely on large annotated datasets and offer limited
interpretability.In this work, we introduce EmbodiedCoder, a training-free
framework for open-world mobile robot manipulation that leverages coding models
to directly generate executable robot trajectories. By grounding high-level
instructions in code, EmbodiedCoder enables flexible object geometry
parameterization and manipulation trajectory synthesis without additional data
collection or fine-tuning.This coding-based paradigm provides a transparent and
generalizable way to connect perception with manipulation. Experiments on real
mobile robots show that EmbodiedCoder achieves robust performance across
diverse long-term tasks and generalizes effectively to novel objects and
environments.Our results demonstrate an interpretable approach for bridging
high-level reasoning and low-level control, moving beyond fixed primitives
toward versatile robot intelligence. See the project page at:
https://anonymous.4open.science/w/Embodied-Coder/

</details>


<div id='cond-mat.mes-hall'></div>

# cond-mat.mes-hall [[Back]](#toc)

### [451] [Variational and field-theoretical approach to exciton-exciton interactions and biexcitons in semiconductors](https://arxiv.org/abs/2510.05242)
*Peter A. Noordman,Lucas Maisel Licerán,Henk T. C. Stoof*

Main category: cond-mat.mes-hall

TL;DR: 本工作使用变分方法研究了Wannier激子间的相互作用，得到了两个基态激子间的有效相互作用势，该势通常是非局域的，并且依赖于粒子的耦合自旋。该结果可以被解释为Heitler-London势的推广。


<details>
  <summary>Details</summary>
Motivation: 激子，即半导体中有界电子-空穴对，由于其在光电探测器和器件中的应用潜力而备受关注，尤其是在二维材料领域。虽然自由激子的性质已得到充分理解，但其相互作用的通用描述由于其复合性质而变得复杂，这会导致不同激子之间同一费米子的交换。然而，目前对激子-激子相互作用的理论描述仍然不完整。

Method: 本工作采用变分方法研究激子间的相互作用，得到两个基态激子间的有效相互作用势。该势在一般情况下是非局域的，并且依赖于粒子的耦合自旋。当应用于具有重空穴的类氢激子时，该势变为局域的，并精确地再现了两个相互作用的氢原子的Heitler-London结果。此外，研究人员还利用路径积分形式主义，发展了一种多体理论，得到了激子作用量，该量在形式上包含了激子间的多体相互作用。

Result: 通过变分方法得到了两个基态激子间的有效相互作用势，该势通常是非局域的，并且依赖于粒子的耦合自旋。当应用于具有重空穴的类氢激子时，该势变为局域的，并精确地再现了两个相互作用的氢原子的Heitler-London结果。在包含激发态的修正后，在远距离处得到了范德华势。利用路径积分形式主义得到了一种激子气体多体理论，其激子作用量在形式上包含了激子间的多体相互作用。

Conclusion: 本工作提出的方法和结果为激子-激子相互作用的一般理论及其在双激子光谱和关联激子物质研究中的应用奠定了基础。本研究将激子-激子相互作用的研究推向了新的高度，为理解和设计新型光电器件提供了理论支撑。

Abstract: Bound electron-hole pairs in semiconductors known as excitons are the subject
of intense research due to their potential for optoelectronic devices and
applications, especially in the realm of two-dimensional materials. While the
properties of free excitons in these systems are well understood, a general
description of their interactions is complicated due to their composite nature,
which leads to exchange between the identical fermions of different excitons.
In this work we employ a variational approach to study interactions between
Wannier excitons and obtain an effective interaction potential between two
ground-state excitons in a system of spin-degenerate electrons and holes. This
potential is in general nonlocal and depends on the coupled spins of the
particles. When particularized to hydrogen-like excitons with a heavy hole, it
becomes local and exactly reproduces the Heitler-London result for two
interacting hydrogen atoms. Thus, our result can be interpreted as a
generalization of the Heitler-London potential to arbitrary masses. Including
corrections due to excited states results in a van der Waals potential at large
distances, which is expected due to the induced dipole-dipole nature of the
interactions. Additionally, we use a path-integral formalism to develop a
many-body theory for a gas of excitons, resulting in an excitonic action that
formally includes many-body interactions between excitons. While in the field
representing the excitons is exactly bosonic, we clarify how the internal
exchange processes arise in the field-theoretical treatment, and show that the
diagrams corresponding to the interactions between excitons align with our
variational calculation when evaluated on shell. Our methods and results lay
the groundwork for a generalized theory of exciton-exciton interactions and
their application to the study of biexciton spectra and correlated excitonic
matter.

</details>


### [452] [Semiconductor Meta-Graphene and Valleytronics](https://arxiv.org/abs/2510.05250)
*Praveen Pai,Aron W. Cummings,Alexander Cerjan,Wei Pan,Fan Zhang,Catalin D. Spataru*

Main category: cond-mat.mes-hall

TL;DR: 通过引入额外的纳米图案化，在人工石墨烯中实现了人工六方氮化硼（AhBN），并发现了拓扑保护的边界态，即使在存在实验相关缺陷的情况下也能保持其鲁棒性，这为低功耗微电子应用提供了潜力。


<details>
  <summary>Details</summary>
Motivation: 探索纳米图案化半导体界面在量子超材料和电子现象中的应用，特别是研究其对人工石墨烯特性的影响。

Method: 通过模拟计算了人工石墨烯（AhBN）的能带结构和电子输运，并引入了电荷や和几何缺陷等实验相关缺陷。

Result: 结果表明，AhBN能够打开狄拉克带隙，并产生拓扑谷陈数。在存在缺陷的情况下，边界态的局域长度可以达到几微米，显示出对缺陷的鲁棒性。长宽比大的AhBN带状结构能够提高低耗散边界通道的效率。

Conclusion: AhBN在低功耗、高能效的微电子应用方面具有巨大潜力，但同时也面临着由缺陷引起的局域化挑战。

Abstract: Nano-patterned semiconductor interfaces offer a versatile platform for
creating quantum metamaterials and exploring novel electronic phenomena. In
this study, we illustrate this concept using artificial graphene--a
metamaterial featuring distinctive properties including Dirac and saddle
points. We demonstrate that introducing additional nano-patterning can open a
Dirac band gap, giving rise to what we term artificial hexagonal boron nitride
(AhBN). The calculated valley Chern number of AhBN indicates the presence of
topological valley Hall states confined to Dirac-gap domain walls. A key
question is whether these one-dimensional edge states are topologically
protected against disorder, given their vulnerability to Anderson localization.
To this end, we perform band structure and electronic transport simulations
under experimentally relevant disorder, including charge puddles and geometric
imperfections. Our results reveal the resilience of the domain wall states
against typical experimental disorder, particularly while the AhBN band gap
remains open. The localization length along the domain wall can reach several
microns--several times longer than the bulk electron mean free path--even
though the number of bulk transport channels is greater. To enhance the
effectiveness of the low-dissipation domain wall channel, we propose ribbon
geometries with a large length-to-width ratio. These findings underscore both
the potential and challenges of AhBN for low-energy, power-efficient
microelectronic applications.

</details>


### [453] [Topological Protection in a Landau Flat Band at $ν=7/11$, a Candidate Filling Factor for Unconventional Correlations](https://arxiv.org/abs/2510.05265)
*Waseem Hussain,Haoyun Huang,Loren N. Pfeiffer,Kenneth W. West,Kirk. W. Baldwin,Gábor A. Csáthy*

Main category: cond-mat.mes-hall

TL;DR: 在两种二维电子系统中，我们发现了拓扑保护的证据，这为研究非常规电子相关性开辟了新的可能性。


<details>
  <summary>Details</summary>
Motivation: 强相互作用在朗道平带中稳定反常的分数量子霍尔效应，但这种现象在其他类型的平带中并不存在。因此，研究朗道平带中的反常分数量子霍尔效应具有重要意义。

Method: 通过实验测量得到不同朗道能级填充因子下的输运特性，并与理论预测进行比较，以识别和确认反常分数量子霍尔态的存在。

Result: 在两种二维电子系统中，于v=7/11填充因子处观测到拓扑保护的迹象，该填充因子是v=4/11的粒子-空穴共轭。此外，在v=11/17, 5/8, 和8/13填充因子下也观测到一些输运信号，但这些信号没有表现出拓扑保护。

Conclusion: 本研究首次在v=7/11填充因子下观测到反常分数量子霍尔态的拓扑保护，为非常规电子相关性的研究提供了新的实验实例和更广阔的研究空间。

Abstract: Strong interactions in Landau flat bands are known to stabilize correlated
states that do not form in other types of flat bands. We report hallmarks of
topological protection at the Landau level filling factor v=7/11 in a
two-dimensional electron system. The $\nu=7/11$ filling factor is the
particle-hole conjugate of $\nu=4/11$, a filling factor intensely studied for
the possibility of realizing unconventional electronic correlations. Our data
establishes a new instance for an unusual fractional quantum Hall state and
opens up possibilities for the study of unconventional correlations in an
enlarged parameter space. We report and discuss transport signatures developing
at other filling factors of interest $\nu= 11/17$, $5/8$, and $8/13$, which
however in our sample do not exhibit topological protection.

</details>


### [454] [Switchable spin-photon coupling with hole spins in single-quantum dots](https://arxiv.org/abs/2510.05301)
*Carlos Sagaseta,María José Calderón,José Carlos Abadillo-Uriel*

Main category: cond-mat.mes-hall

TL;DR: 半导体量子点中的空穴自旋-光子耦合可实现长距离相互作用，并可能用于构建紧凑型自旋-腔QED架构。


<details>
  <summary>Details</summary>
Motivation: 研究在紧凑型单量子点设置中实现空穴自旋与光子的耦合，探索其在量子信息处理中的应用。

Method: 通过理论模拟，考虑了向量势-自旋-轨道几何机制、不均匀Rashba项和应变诱导的g因子项，并比较了硅、非应变锗和双轴应变锗的耦合特性。

Result: 发现硅和非应变锗具有最佳的耦合强度（数十兆赫兹），并能实现高效的自旋-光子耦合切换和超过99%的量子态转移保真度以及超过90%的两比特门保真度。

Conclusion: 单点空穴自旋是构建紧凑型自旋-腔QED架构的可行平台，而非应变锗在自旋-光子相互作用方面具有潜力。

Abstract: Spin qubits in semiconductor quantum dots offer a gate-tunable platform for
quantum information processing. While two-qubit interactions are typically
realized through exchange coupling between neighboring spins, coupling spin
qubits to photons via hybrid spin-cQED devices enables long-range interactions
and integration with other cQED platforms. Here, we investigate hole
spin-photon coupling in compact single quantum dot setups. By incorporating
ubiquitous strain inhomogeneities to our theory, we identify three main
spin-photon coupling channels: a vector-potential-spin-orbit geometric
mechanism--dominant for vertical magnetic fields--, an inhomogeneous Rashba
term generalizing previous spin-orbit field models, and strain-induced
$g$-tensor terms--most relevant for in-plane fields. Comparing Si, unstrained
(relaxed) Ge, and biaxially strained Ge wells, we find that Si and unstrained
Ge provide optimal coupling strengths (tens of MHz) thanks to their reduced
heavy-hole, light-hole splitting. We demonstrate efficient switching of the
spin-photon coupling while preserving sweet spot operation. Finally, we
evaluate quantum state transfer and two-qubit gate protocols, achieving $>99\%$
fidelity for state transfer and $>90\%$ for two-qubit gates with realistic
coherence times, establishing single-dot hole spins as a viable platform for
compact spin-cQED architectures and highlighting unstrained Ge as a promising
candidate for spin-photon interactions.

</details>


### [455] [Dynamics of quantum measurement via electron transport in quantum dot systems: many-particle wavefunction approach](https://arxiv.org/abs/2510.05348)
*George Stavisskii,Leonid Fedichkin*

Main category: cond-mat.mes-hall

TL;DR: 点接触的电荷量子比特测量


<details>
  <summary>Details</summary>
Motivation: 考虑具有复杂内部结构点接触的电荷量子比特测量问题

Method: 提出量子比特测量方法，并推导出适用于任意内部状态数点接触的相应主方程

Result: 研究了点接触的电流噪声功率谱及其对量子比特动力学和点接触参数的依赖性

Conclusion: 得出了点接触的电流噪声功率谱与其量子比特动力学和点接触参数相关的结论

Abstract: Measurement of a charge qubit via point contacts with complex internal
structures is considered. In this context, a fully formalized derivation of the
many-body wave function method is presented, together with the corresponding
master equations for point contacts possessing an arbitrary number of internal
states. The focus is placed on the current noise power spectrum and its
dependence on the qubit dynamics and the point contact parameters.

</details>


### [456] [Investigation of the Effect of Thermal-Induced Atomic Motion on the Conductance of Copper Thin Films](https://arxiv.org/abs/2510.05349)
*Sihe Chen,Kevin Batzinger,Manuel Smeu*

Main category: cond-mat.mes-hall

TL;DR: 随着集成电路(IC)和金属互连件的尺寸减小，电子散射效应的增强会导致电阻率增加，从而降低芯片的效率。本研究通过从头算分子动力学(AIMD)模拟和非平衡格林函数(NEGF)方法，研究了不同温度下(218 K, 300 K, 540 K)铜薄膜的电子输运性质。结果表明，温度升高会增加铜薄膜的电阻，但对于已存在表面粗糙度的铜薄膜，温度升高的影响相对较小，表面粗糙度是主要的电阻来源。


<details>
  <summary>Details</summary>
Motivation: 集成电路(IC)和金属互连件尺寸的减小导致电阻率增加，降低了芯片效率。尽管之前的研究探讨了表面粗糙度引起的电子散射，但热诱导的原子运动对粗糙表面的影响尚不清楚。

Method: 使用从头算分子动力学(AIMD)轨迹模拟了20 ps内不同温度(218 K, 300 K, 540 K)下原始和粗糙铜薄膜的原子运动。然后，利用非平衡格林函数(NEGF)与密度泛函理论(DFT)相结合的方法，计算了最后10 ps内每100 fs间隔的快照的电子输运性质。

Result: 更高的温度会导致原子位移增大，原子层间距增加。对于原始铜薄膜，温度升高会导致电阻增加（电导率降低）。然而，对于粗糙铜薄膜，温度升高的影响不那么显著，因为表面粗糙度本身是主要的电阻来源。

Conclusion: 这项研究揭示了原始和粗糙铜薄膜在热力学条件下的行为，为研究人员设计更好的方法来减缓IC及其金属互连件中的热效应提供了见解。

Abstract: Decrease in the size of integrated circuits (IC) and metal interconnects
raise resistivity due the amplification of electron scattering effects, which
decreases the efficiency of chiplets. While previous studies have investigated
the electron scattering due to a roughened surface, the effect of thermal
induced atomic motion on the roughened surface remains unclear. To address this
gap, we investigated electron transport in pristine and roughened Cu thin films
by performing \textit{ab initio} molecular dynamics (AIMD) trajectories over
20~ps at temperatures of 218~K, 300~K, and 540~K on Cu thin film models, and
then calculating the electron transport properties of the resulting snapshots
at 100-fs intervals for the last 10~ps using the non-equilibrium Green's
function formalism in combination with density functional theory (NEGF-DFT). As
expected, higher temperatures induce larger atomic displacement from their
equilibrium positions and increase atomic layer separation. We also find that
increase in temperature results in increased resistance (lower conductance) for
the pristine film, but less so for the roughened thin film where the surface
roughness itself is the main source of resistance. This study provides insights
into how pristine and roughened Cu thin films behave under thermal conditions,
helping researchers design better treatments to mitigate thermal effects in ICs
and their metal interconnects.

</details>


### [457] [Probing orbital currents through inverse orbital Hall and Rashba effects](https://arxiv.org/abs/2510.05543)
*E. Santos,J. L. Costa,R. L. Rodriguez-Suarez,J. B. S. Mendes,A. Azevedo*

Main category: cond-mat.mes-hall

TL;DR: 本文报告了对金属和半导体材料中轨道到电荷转换的实验研究，重点关注逆轨道霍尔效应（IOHE）和逆轨道拉什巴效应。


<details>
  <summary>Details</summary>
Motivation: 研究材料中轨道到电荷的转换机制，重点是IOHE和逆轨道拉什巴效应。

Method: 使用自旋泵浦驱动的铁磁共振（SP-FMR）和自旋塞贝克效应（SSE），在YIG/Pt/NM（NM为金属或半导体）结构中进行实验。

Result: 发现轨道贡献在信号生成和检测中起主导作用，即使在自旋-轨道耦合较弱的体系中也是如此。在Cu氧化物存在下，SP-FMR和SSE信号显著增强。在Ti和Ge中观察到正负IOHE信号，并提取了轨道扩散长度。

Conclusion: 证实了轨道传输的存在，并为发展轨道电子学提供了指导。

Abstract: We report a comprehensive experimental investigation of orbital-to-charge
conversion in metallic and semiconductor materials, emphasizing the fundamental
roles of the inverse orbital Hall effect (IOHE) and the inverse orbital Rashba
effect. Using spin pumping driven by ferromagnetic resonance (SP-FMR) and the
spin Seebeck effect (SSE), we demonstrate efficient orbital current generation
and detection in YIG/Pt/NM structures, where NM is either a metal or a
semiconductor. A central finding is the dominance of orbital contributions over
spin-related effects, even in systems with weak spin-orbit coupling. In
particular, a large enhancement of the SP-FMR and SSE signals is observed in
the presence of naturally oxidized Cu in different heterostructures.
Furthermore, we identify positive and negative IOHE signals in Ti and Ge,
respectively, and extract orbital diffusion lengths in both systems using a
diffusive model. Our results confirm the presence of orbital transport and
offer valuable insights that may guide the further development of orbitronics.

</details>


### [458] [Engineering Magnetic States and Magnetoresistance in Bisegmented Co-Ni Jellyfish Nanowires via Interplay of Shape and Magnetocrystalline Anisotropies](https://arxiv.org/abs/2510.05618)
*M. I. Sobirov,K. A. Rogachev,M. A. Bazrov,Zh. Zh. Namsaraev,I. M. Sapovskii,T. R. Rakhmatullaev,N. V. Ilin,A. O Lembikov,S. M. Pisarev,A. V. Ognev,A. S. Samardak,A. Yu. Samardak*

Main category: cond-mat.mes-hall

TL;DR: 通过控制Co和Ni纳米线的几何形状，可以调控磁畴结构，从而实现磁电阻特性的工程化设计。


<details>
  <summary>Details</summary>
Motivation: 需要掌握不同磁异质性贡献的相互作用，以扩展3D磁性纳米结构的谱系。

Method: 制备了具有定制排列的Co（强磁晶各向异性）和Ni（主导形状各向异性）片段的二分段水母纳米线，并通过磁畴成像（MFM）和微磁模拟来研究其磁畴结构，以及通过磁电阻效应来研究其全局磁响应。

Result: Co片段的磁畴结构可以被调控，表现出磁通闭合的多畴态或形状各向异性主导的涡旋构型。这种局部畴结构抑制了阵列中的静磁相互作用，并实现了对单根纳米线各向异性磁电阻（AMR）的可编程性。

Conclusion: 这项工作为设计功能性磁性纳米材料提供了蓝图，可以通过战略性的各向异性控制来设计其磁阻特性。

Abstract: Expanding the spectrum of 3D magnetic nanostructures requires mastering the
interplay between different anisotropy contributions. Here, we fabricate
bisegmented jellyfish nanowires with tailored arrangements of Co (strong
magnetocrystalline anisotropy) and Ni (dominant shape anisotropy) segments. We
uncover a unique magnetic duality: Co segments can be tuned to exhibit either a
flux-closing multidomain state or a shape-anisotropy-dominated vortex
configuration, directly governed by their geometry. This local domain
structure, imaged by MFM and simulated micromagnetically, dictates the global
magnetic response-suppressing magnetostatic interactions in arrays and enabling
the programming of the anisotropic magnetoresistance (AMR) in single nanowires.
Our work provides a blueprint for designing functional magnetic nanomaterials
where magnetoresistive properties are engineered through strategic anisotropy
control.

</details>


### [459] [Valley-dependent topological interface states in biased armchair nanoribbons in gapless graphene](https://arxiv.org/abs/2510.05653)
*Zheng-Han Huang,Jing-Yuan Lai,Yu-Shu G. Wu*

Main category: cond-mat.mes-hall

TL;DR: 通过在石墨烯纳米带界面施加相反的电偏压，研究了谷极化的电学控制和拓扑类不连续性。


<details>
  <summary>Details</summary>
Motivation: 研究电学偏压控制下的石墨烯纳米带中的谷极化不连续性及其在电子输运中的应用。

Method: 使用高效的紧束缚理论方法，获得界面约束电子本征态的能量特征值和概率分布。

Result: 发现了界面约束电子本征态，并研究了它们对电子输运的影响。通过构型变化，这些本征态转变为准局域态，导致透射谱中出现Fano“反共振”现象。

Conclusion: 所提出的准局域态及其Fano指纹具有鲁棒性，对构型波动不敏感，有望在实际器件中被探测到，并表明输运光谱学可用于探测石墨烯纳米带中依赖于谷的拓扑界面物理。

Abstract: We investigate an electrical bias-controlled, topological kind discontinuity
in valley polarization, in a two-segment armchair nanoribbon of gapless
graphene, where the discontinuity is created at the interface by applying
opposite in-plane, transverse electrical biases to the two segments. In
particular, using an efficient tight-binding theoretical formulation, we
explicitly obtain energy eigenvalues and probability distributions of
discontinuity-induced, interface-confined electron eigenstates, in a reference
configuration. Moreover, implications of the confinement for electron transport
are explored. A configurational variation is introduced to transform the
eigenstates into transport-active, quasi-localized ones. Such states are shown
to result in Fano "anti-resonances" in transmission spectra. The resilience of
the quasi-localized states and their associated Fano fingerprints is also
illustrated with respect to configurational fluctuations, demonstrating their
potential detectability in realistic devices and suggesting transport
spectroscopy as a practical probe of valley-dependent topological interface
physics in graphene nanoribbons.

</details>


### [460] [Giant and robust Josephson diode effect in multiband topological nanowires](https://arxiv.org/abs/2510.05772)
*Bao-Zong Wang,Zi-Kai Li,Zhong-Da Li,Xiong-Jun Liu*

Main category: cond-mat.mes-hall

TL;DR: 多能带约瑟夫森二极管效应


<details>
  <summary>Details</summary>
Motivation: 在准一维拓扑Majorana纳米线中，研究多能带下的Josephson二极管效应，并探索其潜在应用。

Method: 理论预测多能带Majorana纳米线中的Josephson二极管效应，特别关注其自旋奇偶交换机制。

Result: 发现了由Majorana束缚态和常规安德烈夫束缚态共存引起的、具有4π/2π周期性的Josephson二极管效应。揭示了多能带特有的自旋奇偶交换机制，导致了高效的二极管效应平台。

Conclusion: 多能带工程是优化Josephson二极管效应的有效手段，并为识别超导纳米线中的拓扑相区提供了新方法。

Abstract: We theoretically predict the giant and robust Josephson diode effect in
quasi-one-dimensional topological Majorana nanowires in the regime with
multiple subbands, which is expected to be relevant for the real experiment. In
the multiband regime, the Majorana bound states and conventional Andreev bound
states can naturally coexist, and respectively contribute to the fractional and
conventional parts in the Josephson effect, with the former/latter having
4$\pi$/2$\pi$-periodicity. We show that the interplay between the two types of
bound modes can produce a robust and giant diode effect in the deep topological
phase regime. Notably, we unveil a novel spin parity exchange mechanism,
occurring only in the multiband regime, which leads to a robust high efficiency
plateau of the giant diode effect. This effect is a nontrivial consequence of
the balanced Fermi moment shifts of the multiple subbands in tuning the
external magnetic field. Our finding highlights the subband engineering as a
powerful tool to optimize the Josephson diode effect realistically and provides
a new feasible signature to identify topological phase regime in
superconducting nanowires.

</details>


### [461] [Measurement of the Quantum Capacitance Between Two Metallic Electrodes](https://arxiv.org/abs/2510.05866)
*T. de Ara,B. Olivera,C. Sabater,C. Untiedt*

Main category: cond-mat.mes-hall

TL;DR: 量子效应导致金属电极间电容在接近时饱和，并出现量子隧穿效应，这可用于探测分子吸附引起的电子态密度变化。


<details>
  <summary>Details</summary>
Motivation: 研究当金属电极接近到需要考虑量子效应的极限时，电容的演变。

Method: 在分子（甲苯）吸附的情况下，通过测量金属电极（Pt或Au）之间的电容来研究量子效应和电子态密度。

Result: 在小距离下，观察到电容从经典增加转变为饱和，达到量子电容极限，并出现由于量子隧穿引起的电容泄漏。还观察到分子吸附会改变电子态密度。

Conclusion: 量子电容提供了一种探测分子吸附引起金属表面电子态密度变化的方法。

Abstract: Two factors contribute to the electrical capacitance between two electrodes:
a classical contribution, stemming from the electric field, and a quantum
contribution, governed by the Pauli exclusion principle, which increases the
difficulty of adding charge to the electrodes. In metals, the high electronic
Density of States (DOS) at the Fermi energy allows the quantum contribution to
be neglected, and a classical description of the electrical capacitance between
two metallic electrodes is normally used. Here, we study the evolution of the
capacitance as two metallic electrodes (Pt or Au) are approached to the limit
when quantum corrections are needed, before contact formation. At small
distances, we observe that the classical increase in capacitance turns into
saturation as the electrodes are approached, reaching the quantum capacitance
limit. Finally, a capacitance leakage due to quantum tunneling is observed.
Since the quantum capacitance depends on the electronic DOS on the surface of
the electrodes, we use it to probe the DOS change induced by molecular
adsorption (Toluene) on the metallic surface.

</details>


<div id='cs.SI'></div>

# cs.SI [[Back]](#toc)

### [462] [Emergent Directedness in Social Contagion](https://arxiv.org/abs/2510.06012)
*Fabian Tschofenig,Douglas Guilbeault*

Main category: cs.SI

TL;DR: 复杂传染病的传播路径具有单向性，这挑战了现有网络传播理论。


<details>
  <summary>Details</summary>
Motivation: 现有网络结构难以预测传染病传播的复杂性，需要新的建模框架。

Method: 提出因果建模框架，模拟传播路径并识别关键节点和边，分析复杂传染病的路径方向性。

Result: 复杂传染病（需要多重暴露）的传播路径具有单向性，越复杂越单向。弱连接会促进跨社区传播，但不一定是双向的。传播路径倾向于从网络边缘流向核心，颠覆了传统中心性模型。

Conclusion: 新兴的单向性解释了LinkedIn工作扩散研究中的非线性效应，并揭示了网络演化偏向于定向路径，但受文化因素（如三元闭包）制约，这对网络建设和干预有战略意义。

Abstract: An enduring challenge in contagion theory is that the pathways contagions
follow through social networks exhibit emergent complexities that are difficult
to predict using network structure. Here, we address this challenge by
developing a causal modeling framework that (i) simulates the possible network
pathways that emerge as contagions spread and (ii) identifies which edges and
nodes are most impactful on diffusion across these possible pathways. This
yields a surprising discovery. If people require exposure to multiple peers to
adopt a contagion (a.k.a., 'complex contagions'), the pathways that emerge
often only work in one direction. In fact, the more complex a contagion is, the
more asymmetric its paths become. This emergent directedness problematizes
canonical theories of how networks mediate contagion. Weak ties spanning
network regions - widely thought to facilitate mutual influence and integration
- prove to privilege the spread contagions from one community to the other.
Emergent directedness also disproportionately channels complex contagions from
the network periphery to the core, inverting standard centrality models. We
demonstrate two practical applications. We show that emergent directedness
accounts for unexplained nonlinearity in the effects of tie strength in a
recent study of job diffusion over LinkedIn. Lastly, we show that network
evolution is biased toward growing directed paths, but that cultural factors
(e.g., triadic closure) can curtail this bias, with strategic implications for
network building and behavioral interventions.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [463] [Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis](https://arxiv.org/abs/2510.05106)
*Joachim Diederich*

Main category: cs.AI

TL;DR: LLM安全代理设计需超越简单提示工程，本文提出信息论分析，揭示规则编码对注意力和合规行为的影响。研究表明，低句法熵和高锚点集中度规则可降低注意力熵并提高指针保真度，但存在锚点冗余与注意力熵的权衡。通过对多种注意力机制的分析，确立了指针保真度界限，并提出锚点放置策略需兼顾保真度和熵。结合动态规则验证架构，证明了热重载验证规则集可提高合规输出的渐近概率。强调了原则性锚点设计和双重执行机制对于防御提示注入攻击和在动态域中保持合规性的必要性。


<details>
  <summary>Details</summary>
Motivation: LLM安全代理设计需要超越简单的提示工程，并且需要对规则编码如何影响注意力和合规行为进行全面的信息论分析。

Method: 本文提出了一种信息论分析方法，用于研究系统提示中的规则编码如何影响LLM代理的注意机制和合规行为。通过形式分析多种注意力架构，并结合动态规则验证架构，证明了热重载验证规则集可提高合规输出的渐近概率。

Result: 研究表明，句法熵低、锚点高度集中的规则格式可以降低注意力熵并提高指针保真度。然而，锚点冗余和注意力熵之间存在一种先前工作中未被认识到的根本权衡。通过对多种注意力架构的分析，我们确定了指针保真度的界限，并展示了锚点放置策略如何利用竞争性的保真度和熵目标。

Conclusion: 保护LLM代理免受提示注入攻击并同时在不断变化的域中保持合规性，必须采用原则性的锚点设计和双重执行机制。

Abstract: The design of safety-critical agents based on large language models (LLMs)
requires more than simple prompt engineering. This paper presents a
comprehensive information-theoretic analysis of how rule encodings in system
prompts influence attention mechanisms and compliance behaviour. We demonstrate
that rule formats with low syntactic entropy and highly concentrated anchors
reduce attention entropy and improve pointer fidelity, but reveal a fundamental
trade-off between anchor redundancy and attention entropy that previous work
failed to recognize. Through formal analysis of multiple attention
architectures including causal, bidirectional, local sparse, kernelized, and
cross-attention mechanisms, we establish bounds on pointer fidelity and show
how anchor placement strategies must account for competing fidelity and entropy
objectives. Combining these insights with a dynamic rule verification
architecture, we provide a formal proof that hot reloading of verified rule
sets increases the asymptotic probability of compliant outputs. These findings
underscore the necessity of principled anchor design and dual enforcement
mechanisms to protect LLM-based agents against prompt injection attacks while
maintaining compliance in evolving domains.

</details>


### [464] [Structured Cognition for Behavioral Intelligence in Large Language Model Agents: Preliminary Study](https://arxiv.org/abs/2510.05107)
*Myung Ho Kim*

Main category: cs.AI

TL;DR: SCL通过分离推理、记忆和控制来改进大型语言模型作为自主代理的架构，提高了任务成功率和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型框架将推理、记忆和控制交织在一起，导致连贯性和可预测性下降，因此需要一种新的架构来解决这些问题。

Method: 引入了结构化认知循环（SCL）架构，将语言模型专门用于推理，使用外部记忆，并由一个轻量级控制器指导执行，形成一个面向目标的循环。

Result: 在三个场景（旅行规划、电子邮件起草和图像生成）的评估中，SCL相比基于提示的基线（ReAct和LangChain代理）在任务成功率（86.3%对70-77%）、目标保真度、减少冗余调用和不支持的断言方面均有提高。

Conclusion: 架构分离可以提高大型语言模型的可靠性和可追溯性，而无需更大的模型或更重的提示，这为未来的研究提供了方向。

Abstract: Large language models have advanced natural language understanding and
generation, yet their use as autonomous agents raises architectural challenges
for multi-step tasks. Existing frameworks often intertwine inference, memory,
and control in a single prompt, which can reduce coherence and predictability.
The Structured Cognitive Loop (SCL) is introduced as an alternative
architecture that separates these functions. In SCL, the language model is
dedicated to inference, memory is maintained externally, and execution is
guided by a lightweight controller within a goal-directed loop. This design
offloads cognitive load from the model and allows intermediate results to be
stored, revisited, and checked before actions are taken, providing a clearer
basis for traceability and evaluation.
  We evaluate SCL against prompt-based baselines including ReAct and common
LangChain agents across three scenarios: temperature-based travel planning,
email drafting with conditional send, and constraint-guided image generation.
All systems share the same base model and tools under matched decoding
settings. Across 360 episodes, SCL shows modest but consistent improvements.
Task success averages 86.3 percent compared with 70-77 percent for baselines.
Goal fidelity is higher, redundant calls are fewer, intermediate states are
reused more reliably, and unsupported assertions per 100 tool calls are
reduced. Ablations show that external memory and control each contribute
independently, and decoding sweeps confirm stability of the effects.
  These results suggest that architectural separation can improve reliability
and traceability without relying on larger models or heavier prompts. The
findings are preliminary and intended to guide extended studies with additional
models, longer horizons, multimodal tasks, and collaborative settings.

</details>


### [465] [Optimization Modeling via Semantic Anchored Alignment](https://arxiv.org/abs/2510.05115)
*Yansen Zhang,Qingcan Kang,Yujie Chen,Yufei Wang,Xiongwei Han,Tao Zhong,Mingxuan Yuan,Chen Ma*

Main category: cs.AI

TL;DR: SAC-Opt 是一个创新的框架，通过语义锚点进行反向引导，修正了大型语言模型生成的优化模型中的逻辑错误，提高了模型的准确性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的大型语言模型（LLMs）在从自然语言生成可执行的优化模型代码方面存在局限性，它们通常是单向生成，并且仅基于求解器的错误信息进行有限的后处理，导致存在未被发现的语义错误，生成了语法正确但逻辑错误的模型。

Method: SAC-Opt 框架通过在每一步将原始的语义锚点与从生成的代码中重建的语义锚点进行对齐，并有选择地修正不匹配的组件，从而实现向语义保真模型的收敛。这种锚点驱动的修正能够精细地改进约束和目标逻辑，而无需额外的训练或监督。

Result: 在七个公开数据集上的实证结果表明，SAC-Opt 将平均建模准确率提高了 7.8%，在 ComplexLP 数据集上的准确率提升高达 21.9%。

Conclusion: SAC-Opt 强调了在基于 LLM 的优化工作流中进行语义锚点修正的重要性，以确保从问题意图到求解器可执行代码的忠实翻译。

Abstract: Large language models (LLMs) have opened new paradigms in optimization
modeling by enabling the generation of executable solver code from natural
language descriptions. Despite this promise, existing approaches typically
remain solver-driven: they rely on single-pass forward generation and apply
limited post-hoc fixes based on solver error messages, leaving undetected
semantic errors that silently produce syntactically correct but logically
flawed models. To address this challenge, we propose SAC-Opt, a backward-guided
correction framework that grounds optimization modeling in problem semantics
rather than solver feedback. At each step, SAC-Opt aligns the original semantic
anchors with those reconstructed from the generated code and selectively
corrects only the mismatched components, driving convergence toward a
semantically faithful model. This anchor-driven correction enables fine-grained
refinement of constraint and objective logic, enhancing both fidelity and
robustness without requiring additional training or supervision. Empirical
results on seven public datasets demonstrate that SAC-Opt improves average
modeling accuracy by 7.8\%, with gains of up to 21.9\% on the ComplexLP
dataset. These findings highlight the importance of semantic-anchored
correction in LLM-based optimization workflows to ensure faithful translation
from problem intent to solver-executable code.

</details>


### [466] [Structuring Reasoning for Complex Rules Beyond Flat Representations](https://arxiv.org/abs/2510.05134)
*Zhihao Yang,Ancheng Xu,Jingpeng Li,Liang Yan,Jiehui Zhou,Zhen Qin,Hengyun Chang,Ahmadreza Argha,Hamid Alinejad-Rokny,Minghuan Tan,Yujun Cai,Min Yang*

Main category: cs.AI

TL;DR: DAT框架通过结构化推理过程解决了LLM在处理复杂规则系统时的局限性，提高了推理的准确性和效率，并使小型模型也能取得优异的性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）在处理复杂规则系统时存在挑战，容易将相互依赖的规则视为非结构化文本，导致推理分歧，并且现有的链式思考（CoT）方法缺乏系统性且易于传播错误。

Method: 提出动态裁决模板（DAT）框架，模仿人类专家推理过程，将推理分为三个阶段：定性分析（评估上下文）、证据收集（提取信息并验证规则）和裁决（综合判断）。

Result: DAT在复杂的基于规则的任务中持续优于传统的CoT方法，并且使小型语言模型能够匹配甚至超越更大模型的性能。

Conclusion: DAT框架能有效处理复杂的规则系统，提高推理准确性和效率，并具有模型尺寸无关的优势。

Abstract: Large language models (LLMs) face significant challenges when processing
complex rule systems, as they typically treat interdependent rules as
unstructured textual data rather than as logically organized frameworks. This
limitation results in reasoning divergence, where models often overlook
critical rule dependencies essential for accurate interpretation. Although
existing approaches such as Chain-of-Thought (CoT) reasoning have shown
promise, they lack systematic methodologies for structured rule processing and
are particularly susceptible to error propagation through sequential reasoning
chains. To address these limitations, we propose the Dynamic Adjudication
Template (DAT), a novel framework inspired by expert human reasoning processes.
DAT structures the inference mechanism into three methodical stages:
qualitative analysis, evidence gathering, and adjudication. During the
qualitative analysis phase, the model comprehensively evaluates the contextual
landscape. The subsequent evidence gathering phase involves the targeted
extraction of pertinent information based on predefined template elements
([placeholder]), followed by systematic verification against applicable rules.
Finally, in the adjudication phase, the model synthesizes these validated
components to formulate a comprehensive judgment. Empirical results demonstrate
that DAT consistently outperforms conventional CoT approaches in complex
rule-based tasks. Notably, DAT enables smaller language models to match, and in
some cases exceed, the performance of significantly larger LLMs, highlighting
its efficiency and effectiveness in managing intricate rule systems.

</details>


### [467] [An Algorithmic Information-Theoretic Perspective on the Symbol Grounding Problem](https://arxiv.org/abs/2510.05153)
*Zhangchi Liu*

Main category: cs.AI

TL;DR: 该论文使用算法信息论（AIT）为符号接地问题（SGP）提供了一个统一的框架，将意义的接地定义为一种信息压缩过程，并受到信息论的限制。


<details>
  <summary>Details</summary>
Motivation: 该论文旨在为符号接地问题（SGP）提供一个统一的框架，将已有理论（如G"odelian和No Free Lunch）纳入其中。

Method: 通过将符号系统建模为通用图灵机，并将接地定义为信息压缩，论文分四个阶段进行论证：1. 证明符号系统无法接地几乎所有“世界”（数据字符串）；2. 证明任何静态接地系统都是不完整的；3. 证明“接地行为”是不可推断的；4. 利用Chaitin不完备性定理证明算法学习过程的局限性。

Result: 证明了纯粹的符号系统无法接地所有可能的“世界”；任何静态接地系统都是不完整的；接地行为需要新的信息且不可推断；任何算法学习过程都有其信息论的局限性。

Conclusion: 意义是一个开放式的过程，系统不断尝试克服自身的信息论限制。

Abstract: This paper provides a definitive, unifying framework for the Symbol Grounding
Problem (SGP) by reformulating it within Algorithmic Information Theory (AIT).
We demonstrate that the grounding of meaning is a process fundamentally
constrained by information-theoretic limits, thereby unifying the G\"odelian
(self-reference) and No Free Lunch (statistical) perspectives. We model a
symbolic system as a universal Turing machine and define grounding as an act of
information compression. The argument proceeds in four stages. First, we prove
that a purely symbolic system cannot ground almost all possible "worlds" (data
strings), as they are algorithmically random and thus incompressible. Second,
we show that any statically grounded system, specialized for compressing a
specific world, is inherently incomplete because an adversarial, incompressible
world relative to the system can always be constructed. Third, the "grounding
act" of adapting to a new world is proven to be non-inferable, as it requires
the input of new information (a shorter program) that cannot be deduced from
the system's existing code. Finally, we use Chaitin's Incompleteness Theorem to
prove that any algorithmic learning process is itself a finite system that
cannot comprehend or model worlds whose complexity provably exceeds its own.
This establishes that meaning is the open-ended process of a system perpetually
attempting to overcome its own information-theoretic limitations.

</details>


### [468] [Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework](https://arxiv.org/abs/2510.05158)
*Xin He,Liangliang You,Hongduan Tian,Bo Han,Ivor Tsang,Yew-Soon Ong*

Main category: cs.AI

TL;DR: Lang-PINN是一个由LLM驱动的多智能体系统，可以直接从自然语言任务描述构建可训练的物理信息神经网络（PINN），解决了现有PINN构建的劳动密集和易错问题。


<details>
  <summary>Details</summary>
Motivation: 现有的PINN构建方法在将问题表述为偏微分方程（PDE）、设计网络架构和损失函数以及实现稳定的训练流程方面，劳动密集且易出错。虽然现有的基于LLM的方法可以处理代码生成或架构建议等孤立的步骤，但它们通常假设PDE已经形式化，缺乏端到端的视角。

Method: Lang-PINN协调四个智能体：PDE智能体（将任务描述解析为符号PDE）、PINN智能体（选择架构）、代码智能体（生成模块化实现）和反馈智能体（执行和诊断错误以进行迭代优化）。

Result: 实验表明，Lang-PINN的均方误差（MSE）比竞争基线低3-5个数量级，端到端执行成功率提高了50%以上，时间开销减少了74%。

Conclusion: Lang-PINN通过将非正式的任务陈述转化为可执行和可验证的PINN代码，显著提高了PINN的构建效率和性能。

Abstract: Physics-informed neural networks (PINNs) provide a powerful approach for
solving partial differential equations (PDEs), but constructing a usable PINN
remains labor-intensive and error-prone. Scientists must interpret problems as
PDE formulations, design architectures and loss functions, and implement stable
training pipelines. Existing large language model (LLM) based approaches
address isolated steps such as code generation or architecture suggestion, but
typically assume a formal PDE is already specified and therefore lack an
end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system
that builds trainable PINNs directly from natural language task descriptions.
Lang-PINN coordinates four complementary agents: a PDE Agent that parses task
descriptions into symbolic PDEs, a PINN Agent that selects architectures, a
Code Agent that generates modular implementations, and a Feedback Agent that
executes and diagnoses errors for iterative refinement. This design transforms
informal task statements into executable and verifiable PINN code. Experiments
show that Lang-PINN achieves substantially lower errors and greater robustness
than competitive baselines: mean squared error (MSE) is reduced by up to 3--5
orders of magnitude, end-to-end execution success improves by more than 50\%,
and reduces time overhead by up to 74\%.

</details>


### [469] [In-the-Flow Agentic System Optimization for Effective Planning and Tool Use](https://arxiv.org/abs/2510.05592)
*Zhuofeng Li,Haoxiang Zhang,Seungju Han,Sheng Liu,Jianwen Xie,Yu Zhang,Yejin Choi,James Zou,Pan Lu*

Main category: cs.AI

TL;DR: AgentFlow是一个可训练的、流程中的agentic框架，通过演进式记忆协调四个模块（规划器、执行器、验证器、生成器），并在多轮循环中直接优化其规划器。Flow-GRPO是一种新的优化算法，用于在直播环境中进行在线训练，通过将多轮优化转换为一系列可行的单轮策略更新来处理长期稀疏奖励信用分配。


<details>
  <summary>Details</summary>
Motivation: 传统的工具增强方法在长周期和多样化工具方面扩展性差，并且对新场景的泛化能力弱。Agentic系统提供了一个有前景的替代方案，但大多数是免训练的或依赖于与多轮交互的实时动态解耦的离线训练。

Method: AgentFlow协调四个模块（规划器、执行器、验证器、生成器）通过一个不断发展的记忆，并在多轮循环中直接优化其规划器。Flow-GRPO将多轮优化转换为一系列可行的单轮策略更新，将单一的可验证轨迹级别结果广播到每一轮，以使本地规划器决策与全局成功保持一致，并通过组归一化优势来稳定学习。

Result: 在十个基准测试中，AgentFlow在使用7B规模骨干模型的情况下，在搜索、agentic、数学和科学任务上的准确性平均分别提高了14.9%、14.0%、14.5%和4.1%，优于顶级的基线，甚至超过了GPT-4o等更大的专有模型。

Conclusion: AgentFlow通过流程中的优化、提高规划能力、增强工具调用可靠性以及模型规模和推理轮次的积极扩展性，证明了其有效性。

Abstract: Outcome-driven reinforcement learning has advanced reasoning in large
language models (LLMs), but prevailing tool-augmented approaches train a
single, monolithic policy that interleaves thoughts and tool calls under full
context; this scales poorly with long horizons and diverse tools and
generalizes weakly to new scenarios. Agentic systems offer a promising
alternative by decomposing work across specialized modules, yet most remain
training-free or rely on offline training decoupled from the live dynamics of
multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow
agentic framework that coordinates four modules (planner, executor, verifier,
generator) through an evolving memory and directly optimizes its planner inside
the multi-turn loop. To train on-policy in live environments, we propose
Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles
long-horizon, sparse-reward credit assignment by converting multi-turn
optimization into a sequence of tractable single-turn policy updates. It
broadcasts a single, verifiable trajectory-level outcome to every turn to align
local planner decisions with global success and stabilizes learning with
group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale
backbone outperforms top-performing baselines with average accuracy gains of
14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on
scientific tasks, even surpassing larger proprietary models like GPT-4o.
Further analyses confirm the benefits of in-the-flow optimization, showing
improved planning, enhanced tool-calling reliability, and positive scaling with
model size and reasoning turns.

</details>


### [470] [Representation Potentials of Foundation Models for Multimodal Alignment: A Survey](https://arxiv.org/abs/2510.05184)
*Jianglin Lu,Hailing Wang,Yi Xu,Yizhou Wang,Kuo Yang,Yun Fu*

Main category: cs.AI

TL;DR: Foundation models pretrained on diverse data learn transferable representations with remarkable similarity across architectures and modalities. This survey explores their representation potentials for capturing task-specific information and enabling cross-modal alignment and unification. Evidence from various fields suggests structural regularities and semantic consistencies in their representations, making them suitable for cross-modal transfer. The survey also analyzes factors fostering these potentials, discusses open questions, and highlights challenges.


<details>
  <summary>Details</summary>
Motivation: The increasing body of research indicating remarkable similarity in transferable representations across architectures and modalities learned by foundation models.

Method: Synthesizing empirical evidence of representation potentials from studies in vision, language, speech, multimodality, and neuroscience, and analyzing key factors that foster representation potentials.

Result: Foundation models often exhibit structural regularities and semantic consistencies in their representation spaces, positioning them as strong candidates for cross-modal transfer and alignment.

Conclusion: Foundation models hold significant potential for cross-modal transfer and alignment due to the structural regularities and semantic consistencies in their learned representations, though further research is needed to address open questions and challenges.

Abstract: Foundation models learn highly transferable representations through
large-scale pretraining on diverse data. An increasing body of research
indicates that these representations exhibit a remarkable degree of similarity
across architectures and modalities. In this survey, we investigate the
representation potentials of foundation models, defined as the latent capacity
of their learned representations to capture task-specific information within a
single modality while also providing a transferable basis for alignment and
unification across modalities. We begin by reviewing representative foundation
models and the key metrics that make alignment measurable. We then synthesize
empirical evidence of representation potentials from studies in vision,
language, speech, multimodality, and neuroscience. The evidence suggests that
foundation models often exhibit structural regularities and semantic
consistencies in their representation spaces, positioning them as strong
candidates for cross-modal transfer and alignment. We further analyze the key
factors that foster representation potentials, discuss open questions, and
highlight potential challenges.

</details>


### [471] [Real-time Framework for Interoperable Semantic-driven Internet-of-Things in Smart Agriculture](https://arxiv.org/abs/2510.05187)
*Mohamed El-Dosuky*

Main category: cs.AI

TL;DR: 该论文提出了一个包含六层（感知、语义标注、互操作性、传输、语义推理、应用）的物联网（IoT）数据处理框架，通过增加语义层来帮助物联网设备理解数据含义和来源，并利用模糊逻辑、Dempster-Shafer理论和贝叶斯网络等方法进行推理，最终通过GUI展示，以应对物联网数据收集和理解的挑战，尤其是在农业领域。


<details>
  <summary>Details</summary>
Motivation: 物联网在农业等领域得到广泛应用，但仍面临数据收集和理解方面的挑战。

Method: 提出一个包含感知、语义标注、互操作性、传输、语义推理、应用六层的实时框架。在语义标注层，对传感器收集的原始数据（如电压）添加元数据（如目的、ID、应用）。在语义互操作性层，提出互操作性语义算法和同义词识别算法。在传输层，通过WiFi、Zigbee、蓝牙、移动网络等传输数据。在语义推理层，利用模糊逻辑、Dempster-Shafer理论、贝叶斯网络推理新知识。在应用层，提供图形用户界面（GUI）供用户监控和交互。

Result: 该框架能够处理物联网数据，确保语义完整性，并实现实时知识推理，为物联网应用（尤其是在农业领域）提供了鲁棒的解决方案。

Conclusion: 该框架通过集成不确定性推理方法和语义互操作性技术，提高了物联网数据的管理和理解能力，促进了物联网应用的进步。

Abstract: The Internet of Things (IoT) has revolutionized various applications
including agriculture, but it still faces challenges in data collection and
understanding. This paper proposes a real-time framework with three additional
semantic layers to help IoT devices and sensors comprehend data meaning and
source. The framework consists of six layers: perception, semantic annotation,
interoperability, transportation, semantic reasoning, and application, suitable
for dynamic environments. Sensors collect data in the form of voltage, which is
then processed by microprocessors or microcontrollers in the semantic
annotation and preprocessing layer. Metadata is added to the raw data,
including the purpose, ID number, and application. Two semantic algorithms are
proposed in the semantic interoperability and ontologies layer: the
interoperability semantic algorithm for standardizing file types and the
synonym identification algorithm for identifying synonyms. In the
transportation layer, raw data and metadata are sent to other IoT devices or
cloud computing platforms using techniques like WiFi, Zigbee networks,
Bluetooth, and mobile communication networks. A semantic reasoning layer is
proposed to infer new knowledge from the existing data, using fuzzy logic,
Dempster-Shafer theory, and Bayesian networks. A Graphical User Interface (GUI)
is proposed in the application layer to help users communicate with and monitor
IoT sensors, devices, and new knowledge inferred. This framework provides a
robust solution for managing IoT data, ensuring semantic completeness, and
enabling real-time knowledge inference. The integration of uncertainty
reasoning methods and semantic interoperability techniques makes this framework
a valuable tool for advancing IoT applications in general and in agriculture in
particular.

</details>


### [472] [Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents](https://arxiv.org/abs/2510.05188)
*Wenda Xie,Chao Guo,Yanqing Jing. Junle Wang,Yisheng Lv,Fei-Yue Wang*

Main category: cs.AI

TL;DR: LLMs难以生成高质量长篇叙事，Dramaturge提出一种分而治之的多LLM代理方法，通过分层审查和协调修订，有效解决全局和局部问题，显著提升叙事质量。


<details>
  <summary>Details</summary>
Motivation: 单个LLM难以在单次生成中产出高质量长篇叙事，因为这需要对全局结构和局部细节有深入理解，并进行多粒度协调修订，而直接修改容易导致不一致性。

Method: 提出一种任务和特征驱动的、分而治之的、由分层多个LLM代理驱动的方法（Dramaturge），包括全局审查、场景级审查和分层协调修订，采用自顶向下、粗粒度到细粒度的迭代流程。

Result: Dramaturge在剧本整体质量和场景细节方面显著优于所有基线方法。

Conclusion: Dramaturge是一种即插即用的方法，能够有效改进LLM生成的叙事脚本，通过分层多LLM代理协同，解决了长篇叙事生成中的关键挑战。

Abstract: Although LLMs have been widely adopted for creative content generation, a
single-pass process often struggles to produce high-quality long narratives.
How to effectively revise and improve long narrative scripts like scriptwriters
remains a significant challenge, as it demands a comprehensive understanding of
the entire context to identify global structural issues and local detailed
flaws, as well as coordinating revisions at multiple granularities and
locations. Direct modifications by LLMs typically introduce inconsistencies
between local edits and the overall narrative requirements. To address these
issues, we propose Dramaturge, a task and feature oriented divide-and-conquer
approach powered by hierarchical multiple LLM agents. It consists of a Global
Review stage to grasp the overall storyline and structural issues, a
Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a
Hierarchical Coordinated Revision stage that coordinates and integrates
structural and detailed improvements throughout the script. The top-down task
flow ensures that high-level strategies guide local modifications, maintaining
contextual consistency. The review and revision workflow follows a
coarse-to-fine iterative process, continuing through multiple rounds until no
further substantive improvements can be made. Comprehensive experiments show
that Dramaturge significantly outperforms all baselines in terms of
script-level overall quality and scene-level details. Our approach is
plug-and-play and can be easily integrated into existing methods to improve the
generated scripts.

</details>


### [473] [RareAgent: Self-Evolving Reasoning for Drug Repurposing in Rare Diseases](https://arxiv.org/abs/2510.05764)
*Lang Qin,Zijian Gan,Xu Cao,Pengcheng Jiang,Yankai Jiang,Jiawei Han,Kaishun Wu,Jintai Chen*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Computational drug repurposing for rare diseases is especially challenging
when no prior associations exist between drugs and target diseases. Therefore,
knowledge graph completion and message-passing GNNs have little reliable signal
to learn and propagate, resulting in poor performance. We present RareAgent, a
self-evolving multi-agent system that reframes this task from passive pattern
recognition to active evidence-seeking reasoning. RareAgent organizes
task-specific adversarial debates in which agents dynamically construct
evidence graphs from diverse perspectives to support, refute, or entail
hypotheses. The reasoning strategies are analyzed post hoc in a
self-evolutionary loop, producing textual feedback that refines agent policies,
while successful reasoning paths are distilled into transferable heuristics to
accelerate future investigations. Comprehensive evaluations reveal that
RareAgent improves the indication AUPRC by 18.1% over reasoning baselines and
provides a transparent reasoning chain consistent with clinical evidence.

</details>


### [474] [Graph-based LLM over Semi-Structured Population Data for Dynamic Policy Response](https://arxiv.org/abs/2510.05196)
*Daqian Shi,Xiaolei Diao,Jinge Wu,Honghan Wu,Xiongfeng Tang,Felix Naughton,Paulina Bondaronek*

Main category: cs.AI

TL;DR: 该研究提出了一种新颖的图 GNN 基础推理框架，结合了大型语言模型、结构化人口属性和非结构化公众反馈，用于在公共卫生紧急情况下进行人口规模分析。


<details>
  <summary>Details</summary>
Motivation: 公共卫生紧急情况（如 COVID-19 大流行）期间，对人口级别数据的及时准确分析对于有效决策至关重要。然而，大量的半结构化数据（包括结构化人口信息和非结构化人类反馈）给传统分析方法带来了重大挑战。

Method: 提出了一种新颖的图 GNN 基础推理框架，该框架在弱监督学习流程中集成了大型语言模型、结构化人口属性和非结构化公众反馈。该方法将不断变化的公民需求动态地建模到一个需求感知图中，从而能够基于年龄、性别和多重剥夺指数等关键特征进行人口特定的分析。

Result: 使用真实世界的数据集对该方法进行了测试，初步的实验结果证明了其可行性。

Conclusion: 该方法为资源受限的临床和政府环境中的智能人口健康监测提供了一个可扩展的解决方案。

Abstract: Timely and accurate analysis of population-level data is crucial for
effective decision-making during public health emergencies such as the COVID-19
pandemic. However, the massive input of semi-structured data, including
structured demographic information and unstructured human feedback, poses
significant challenges to conventional analysis methods. Manual expert-driven
assessments, though accurate, are inefficient, while standard NLP pipelines
often require large task-specific labeled datasets and struggle with
generalization across diverse domains. To address these challenges, we propose
a novel graph-based reasoning framework that integrates large language models
with structured demographic attributes and unstructured public feedback in a
weakly supervised pipeline. The proposed approach dynamically models evolving
citizen needs into a need-aware graph, enabling population-specific analyses
based on key features such as age, gender, and the Index of Multiple
Deprivation. It generates interpretable insights to inform responsive health
policy decision-making. We test our method using a real-world dataset, and
preliminary experimental results demonstrate its feasibility. This approach
offers a scalable solution for intelligent population health monitoring in
resource-constrained clinical and governmental settings.

</details>


### [475] [Efficient Prediction of Pass@k Scaling in Large Language Models](https://arxiv.org/abs/2510.05197)
*Joshua Kazdan,Rylan Schaeffer,Youssef Allouah,Colin Sullivan,Kyssen Yu,Noam Levi,Sanmi Koyejo*

Main category: cs.AI

TL;DR: 重复采样能提升AI能力和风险，但预测大规模采样行为受限于预算。本研究提出一种改进的统计框架和动态采样策略，以在有限预算下更准确地预测AI的稀有能力和风险。


<details>
  <summary>Details</summary>
Motivation: 在AI能力和安全预测方面，如何在大规模采样预算下准确预测模型行为是一个关键问题，这对模型提供者和监管机构都至关重要。

Method: 研究者提出了一个包含三个部分的解决方案：1. 指出标准方法在数据有限情况下预测不准确的统计缺陷；2. 引入一个使用beta-二项分布的稳健估计框架，以提高从有限数据中进行预测的准确性；3. 提出一种动态采样策略，将更多预算分配给更难的问题。

Result: 该研究提出的框架和策略能够以更低的计算成本，更可靠地预测稀有风险和能力。

Conclusion: 通过改进的统计估计和动态采样策略，可以在有限预算下实现对AI系统大规模采样行为的更准确预测，从而在提升能力和控制风险方面提供更好的支持。

Abstract: Assessing the capabilities and risks of frontier AI systems is a critical
area of research, and recent work has shown that repeated sampling from models
can dramatically increase both. For instance, repeated sampling has been shown
to increase their capabilities, such as solving difficult math and coding
problems, but it has also been shown to increase their potential for harm, such
as being jailbroken. Such results raise a crucial question for both capability
and safety forecasting: how can one accurately predict a model's behavior when
scaled to a massive number of attempts, given a vastly smaller sampling budget?
This question is directly relevant to model providers, who serve hundreds of
millions of users daily, and to governmental regulators, who seek to prevent
harms. To answer this questions, we make three contributions. First, we find
that standard methods for fitting these laws suffer from statistical
shortcomings that hinder predictive accuracy, especially in data-limited
scenarios. Second, we remedy these shortcomings by introducing a robust
estimation framework, which uses a beta-binomial distribution to generate more
accurate predictions from limited data. Third, we propose a dynamic sampling
strategy that allocates a greater budget to harder problems. Combined, these
innovations enable more reliable prediction of rare risks and capabilities at a
fraction of the computational cost.

</details>


### [476] [Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment](https://arxiv.org/abs/2510.05283)
*Radha Gulhane,Sathish Reddy Indurthi*

Main category: cs.AI

TL;DR: 提出一个混合奖励建模框架，结合基于模型和基于规则的奖励，以更有效地对齐多模态大语言模型。


<details>
  <summary>Details</summary>
Motivation: 传统的单一信号、基于模型的奖励方法缺乏置信度校准，无法捕捉人类偏好的多样性，并且需要大量标注数据和奖励模型训练。

Method: 提出一个混合奖励建模框架，整合基于模型的奖励（预测分数）和基于规则的奖励（显式正确性信号），并加入多方面奖励（指令遵循）和广义长度惩罚奖励（稳定训练和提高性能）。

Result: 在多个多模态基准测试中表现出持续的改进。在3B模型家族中，整体平均提高约9.5%。在数学基准测试中，平均提高约16%。

Conclusion: 所提出的混合奖励建模框架为通过强化学习策略优化对齐多模态大语言模型提供了一种灵活有效的方法。

Abstract: Aligning multimodal large language models (MLLMs) with human preferences
often relies on single-signal, model-based reward methods. Such monolithic
rewards often lack confidence calibration across domain-specific tasks, fail to
capture diverse aspects of human preferences, and require extensive data
annotation and reward model training. In this work, we propose a hybrid reward
modeling framework that integrates complementary reward paradigms: (i)
model-based rewards, where a learned reward model predicts scalar or vector
scores from synthetic and human feedback, and (ii) rule-based rewards, where
domain-specific heuristics provide explicit correctness signals with
confidence. Beyond accuracy, we further incorporate multi-aspect rewards to
enforce instruction adherence and introduce a generalized length-penalty reward
to stabilize training and improve performance. The proposed framework provides
a flexible and effective approach to aligning MLLMs through reinforcement
learning policy optimization. Our experiments show consistent improvements
across different multimodal benchmarks when applying hybrid and multi-aspect
reward modeling. Our best performing model in the 3B family achieves an overall
average improvement of ~9.5% across general and math reasoning tasks. Focusing
specifically on mathematical benchmarks, the model achieves a significant
average improvement of ~16%, highlighting its effectiveness in mathematical
reasoning and problem solving.

</details>


### [477] [BIRD-INTERACT: Re-imagining Text-to-SQL Evaluation for Large Language Models via Lens of Dynamic Interactions](https://arxiv.org/abs/2510.05318)
*Nan Huo,Xiaohan Xu,Jinyang Li,Per Jacobsson,Shipei Lin,Bowen Qin,Binyuan Hui,Xiaolong Li,Ge Qu,Shuzheng Si,Linheng Han,Edward Alexander,Xintong Zhu,Rui Qin,Ruihan Yu,Yiyao Jin,Feige Zhou,Weihao Zhong,Yun Chen,Hongyu Liu,Chenhao Ma,Fatma Ozcan,Yannis Papakonstantinou,Reynold Cheng*

Main category: cs.AI

TL;DR: BIRD-INTERACT是一个新的多轮Text-to-SQL基准，它通过包含分层知识库、函数驱动的用户模拟器以及支持CRUD操作的测试任务来模拟真实世界的数据库交互，并评估了GPT-5在此基准上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有Text-to-SQL基准无法充分模拟真实世界中多轮交互的复杂性，如处理模糊查询、执行错误和不断变化的用户需求，而这些在实际数据库应用中至关重要。

Method: BIRD-INTERACT基准通过以下方式恢复了真实性：1) 一个全面的交互环境，将每个数据库与分层知识库、元数据文件和函数驱动的用户模拟器相结合，使模型能够在无人监督的情况下寻求澄清、检索知识并从错误中恢复；2) 两种评估设置：预定义的对话协议（c-Interact）和开放式的代理设置（a-Interact）；3) 一个涵盖商业智能和运营用例的CRUD全谱的具有挑战性的任务套件，并附有可执行的测试用例。

Result: 在BIRD-INTERACT基准上，GPT-5在c-Interact设置中仅完成了8.67%的任务，在a-Interact设置中完成了17.00%的任务。通过记忆嫁接和交互测试时缩放进行的分析验证了有效的交互对于复杂、动态的Text-to-SQL任务的重要性。

Conclusion: BIRD-INTERACT基准有效地揭示了现有模型在处理真实世界多轮Text-to-SQL任务方面的不足，并强调了有效交互策略对于解决复杂数据库交互任务的重要性。

Abstract: Large language models (LLMs) have demonstrated remarkable performance on
single-turn text-to-SQL tasks, but real-world database applications
predominantly require multi-turn interactions to handle ambiguous queries,
execution errors, and evolving user requirements. Existing multi-turn
benchmarks fall short by treating conversation histories as static context or
limiting evaluation to read-only operations, failing to reflect
production-grade database assistant challenges. We introduce BIRD-INTERACT, a
benchmark that restores this realism through: (1) a comprehensive interaction
environment coupling each database with a hierarchical knowledge base, metadata
files, and a function-driven user simulator, enabling models to solicit
clarifications, retrieve knowledge, and recover from errors without human
supervision; (2) two evaluation settings consisting of a pre-defined
conversational protocol (c-Interact) and an open-ended agentic setting
(a-Interact) where models autonomously decide when to query the user simulator
or explore the environment; (3) a challenging task suite covering the full CRUD
spectrum for business-intelligence and operational use cases, guarded by
executable test cases. Each task features ambiguous and follow-up sub-tasks
requiring dynamic interaction. The suite comprises BIRD-INTERACT-FULL (600
tasks, up to 11,796 interactions) for comprehensive performance assessment, and
BIRD-INTERACT-LITE (300 tasks with simplified databases) for detailed
behavioral analysis and rapid method development. Our empirical results
highlight BIRD-INTERACT's difficulty: GPT-5 completes only 8.67% of tasks in
c-Interact and 17.00% in a-Interact. Analysis via memory grafting and
Interaction Test-time Scaling validates the importance of effective interaction
for complex, dynamic text-to-SQL tasks.

</details>


### [478] [Biomedical reasoning in action: Multi-agent System for Auditable Biomedical Evidence Synthesis](https://arxiv.org/abs/2510.05335)
*Oskar Wysocki,Magdalena Wysocka,Mauricio Jacobo,Harriet Unsworth,André Freitas*

Main category: cs.AI

TL;DR: M-Reason是一个用于生物医学领域（特别是癌症研究）的透明、基于代理的推理和证据整合演示系统。它利用大型语言模型（LLMs）和模块化代理编排来自动化来自不同生物医学数据源的证据检索、评估和综合。


<details>
  <summary>Details</summary>
Motivation: 该系统旨在解决生物医学研究中证据综合的挑战，提供一个透明、可解释且可审计的框架。

Method: M-Reason系统利用大型语言模型（LLMs）和模块化代理编排。每个代理专门处理特定的证据流，实现并行处理和精细化分析。该系统还集成了确定性代码进行验证，并提供了一个交互式用户界面，允许研究人员观察和评估多代理工作流。

Result: 评估显示，M-Reason在效率和输出一致性方面取得了显著的提高。

Conclusion: M-Reason有潜力成为一个实用的证据综合工具，也是科学研究中鲁棒的多代理LLM系统的试验台。

Abstract: We present M-Reason, a demonstration system for transparent, agent-based
reasoning and evidence integration in the biomedical domain, with a focus on
cancer research. M-Reason leverages recent advances in large language models
(LLMs) and modular agent orchestration to automate evidence retrieval,
appraisal, and synthesis across diverse biomedical data sources. Each agent
specializes in a specific evidence stream, enabling parallel processing and
fine-grained analysis. The system emphasizes explainability, structured
reporting, and user auditability, providing complete traceability from source
evidence to final conclusions. We discuss critical tradeoffs between agent
specialization, system complexity, and resource usage, as well as the
integration of deterministic code for validation. An open, interactive user
interface allows researchers to directly observe, explore and evaluate the
multi-agent workflow. Our evaluation demonstrates substantial gains in
efficiency and output consistency, highlighting M-Reason's potential as both a
practical tool for evidence synthesis and a testbed for robust multi-agent LLM
systems in scientific research, available at https://m-reason.digitalecmt.com.

</details>


### [479] [Integrating Bayesian methods with neural network--based model predictive control: a review](https://arxiv.org/abs/2510.05338)
*Asli Karacelik*

Main category: cs.AI

TL;DR: 贝叶斯方法在基于神经网络的MPC中用于不确定性量化，但其有效性尚不明确，需要标准化基准和更严格的分析。


<details>
  <summary>Details</summary>
Motivation: 评估贝叶斯方法在模型预测控制（MPC）中的应用，特别是在神经元网络建模、控制设计和不确定性量化方面，并分析其在实践中的实施情况。

Method: 系统性地分析了现有的研究，重点关注贝叶斯方法在MPC中的应用，包括神经元网络建模、控制设计和不确定性量化。

Result: 虽然已有研究表明贝叶斯方法在捕捉和传播MPC中的不确定性方面有所应用，但其性能和鲁棒性的提升效果不一致，缺乏可靠性分析。

Conclusion: 为了严格确定贝叶斯技术在MPC中的有效性，需要标准化基准、消融研究和透明的报告。

Abstract: In this review, we assess the use of Bayesian methods in model predictive
control (MPC), focusing on neural-network-based modeling, control design, and
uncertainty quantification. We systematically analyze individual studies and
how they are implemented in practice. While Bayesian approaches are
increasingly adopted to capture and propagate uncertainty in MPC, reported
gains in performance and robustness remain fragmented, with inconsistent
baselines and limited reliability analyses. We therefore argue for standardized
benchmarks, ablation studies, and transparent reporting to rigorously determine
the effectiveness of Bayesian techniques for MPC.

</details>


### [480] [MHA-RAG: Improving Efficiency, Accuracy, and Consistency by Encoding Exemplars as Soft Prompts](https://arxiv.org/abs/2510.05363)
*Abhinav Jain,Xinyu Yao,Thomas Reps,Christopher Jermaine*

Main category: cs.AI

TL;DR: 通过将示例表示为软提示并使用与示例顺序无关的模型架构，MHA-RAG 在多项问答基准测试中实现了比标准 RAG 高 20 分的性能，同时将推理成本降低了 10 倍。


<details>
  <summary>Details</summary>
Motivation: 在新的领域用有限的训练数据来调整基础模型是具有挑战性和计算成本高昂的。虽然先前的工作已经证明了使用特定领域的示例作为上下文演示的有效性，但本研究旨在探究将示例纯粹表示为文本是否是最有效、最稳定和最经济的方法。

Method: 提出了一种名为 MHA-RAG 的框架，其中注意力头的数量作为一个简单的超参数，用于控制跨不同任务的软提示生成。该框架采用与示例顺序无关的模型架构。

Result: MHA-RAG 在多项问答基准测试和模型规模上，实现了比标准 RAG 高 20 分的性能，同时将推理成本降低了 10 倍（GFLOPS）。

Conclusion: MHA-RAG 框架在提高准确性和效率方面均取得了显著成效，并且不受示例顺序的影响。

Abstract: Adapting Foundation Models to new domains with limited training data is
challenging and computationally expensive. While prior work has demonstrated
the effectiveness of using domain-specific exemplars as in-context
demonstrations, we investigate whether representing exemplars purely as text is
the most efficient, effective, and stable approach. We explore an alternative:
representing exemplars as soft prompts with an exemplar order invariant model
architecture. To this end, we introduce Multi-Head Attention
Retrieval-Augmented Generation (MHA-RAG), a framework with the number of
attention heads serving as a simple hyperparameter to control soft
prompt-generation across different tasks. Across multiple question-answering
benchmarks and model scales, MHA-RAG achieves a 20-point performance gain over
standard RAG, while cutting inference costs by a factor of 10X
GFLOPs-delivering both higher accuracy and greater efficiency, invariant to
exemplar order.

</details>


### [481] [What Do You Mean? Exploring How Humans and AI Interact with Symbols and Meanings in Their Interactions](https://arxiv.org/abs/2510.05378)
*Reza Habibi,Seung Wan Ha,Zhiyu Lin,Atieh Kashani,Ala Shafia,Lakshana Lakshmanarajan,Chia-Fang Chung,Magy Seif El-Nasr*

Main category: cs.AI

TL;DR: 该研究探讨了人类和人工智能（AI）如何通过互动共同构建符号及其意义，借鉴了符号互动论的理论。


<details>
  <summary>Details</summary>
Motivation: 人类与AI的有效协作需要超越语言处理，深入理解符号及其社会构建的意义。AI系统常常缺乏在对话中产生的动态解读能力。

Method: 通过两项研究，运用符号互动论，探究人类与AI如何共同构建符号和意义。

Result: 研究表明，人类和AI能够共同塑造互动中的意义。参与者会根据AI提出的符号和解读调整他们对意义的初始定义，尤其是在引入社会背景时。参与者还将个人和社会价值观融入互动，并随着时间的推移提炼意义。

Conclusion: 共享的理解并非源于简单的同意，而是源于符号的双向交流和重新解读，这为人类-AI交互设计提供了新的范式。

Abstract: Meaningful human-AI collaboration requires more than processing language; it
demands a deeper understanding of symbols and their socially constructed
meanings. While humans naturally interpret symbols through social interaction,
AI systems often miss the dynamic interpretations that emerge in conversation.
Drawing on Symbolic Interactionism theory, we conducted two studies to
investigate how humans and AI co-construct symbols and their meanings. Findings
provide empirical insights into how humans and conversational AI agents
collaboratively shape meanings during interaction. We show how participants
shift their initial definitions of meaning in response to the symbols and
interpretations suggested by the conversational AI agents, especially when
social context is introduced. We also observe how participants project their
personal and social values into these interactions, refining meanings over
time. These findings reveal that shared understanding does not emerge from mere
agreement but from the bi-directional exchange and reinterpretation of symbols,
suggesting new paradigms for human-AI interaction design.

</details>


### [482] [Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation](https://arxiv.org/abs/2510.05402)
*Ahmad Alsheikh,Andreas Fischer*

Main category: cs.AI

TL;DR: 本研究提出了一种基于教师-学生学习框架的新方法，用于解决钢材热处理最终硬度预测的逆向问题，即根据目标硬度值推断输入参数。


<details>
  <summary>Details</summary>
Motivation: 钢材热处理的最终硬度预测是一个困难的回归任务，因为多种输入参数组合可能导致相同的硬度值，使得逆向问题（根据期望硬度预测输入参数）尤为棘手。

Method: 首先，训练一个前向模型（教师）来预测13个冶金输入特征的最终硬度。然后，训练一个反向模型（学生）来推断目标硬度值对应的可能输入参数组合。学生模型通过在前向模型的反馈中进行迭代监督学习进行优化。

Result: 在公开的调质钢数据集上，与基线回归和强化学习模型相比，本研究提出的教师-学生框架在逆向预测准确性上更高，并且计算时间显著减少。

Conclusion: 所提出的教师-学生框架在材料科学的逆向过程建模方面是有效且高效的。

Abstract: Predicting the final hardness of steel after heat treatment is a challenging
regression task due to the many-to-one nature of the process -- different
combinations of input parameters (such as temperature, duration, and chemical
composition) can result in the same hardness value. This ambiguity makes the
inverse problem, estimating input parameters from a desired hardness,
particularly difficult. In this work, we propose a novel solution using a
Teacher-Student learning framework. First, a forward model (Teacher) is trained
to predict final hardness from 13 metallurgical input features. Then, a
backward model (Student) is trained to infer plausible input configurations
from a target hardness value. The Student is optimized by leveraging feedback
from the Teacher in an iterative, supervised loop. We evaluate our method on a
publicly available tempered steel dataset and compare it against baseline
regression and reinforcement learning models. Results show that our
Teacher-Student framework not only achieves higher inverse prediction accuracy
but also requires significantly less computational time, demonstrating its
effectiveness and efficiency for inverse process modeling in materials science.

</details>


### [483] [AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems](https://arxiv.org/abs/2510.05432)
*Shambhavi Mishra,Gaurav Sahu,Marco Pedersoli,Laurent Charlin,Jose Dolz,Christopher Pal*

Main category: cs.AI

TL;DR: LLMs在AI研究问题解决方面存在潜力但受限于表述方式。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs是否仅凭预训练知识就能解决AI研究问题，而非依赖微调或外部工具。

Method: 提出AInstein框架，提取ICLR论文问题，并使用solver agents进行迭代式解决方案的提出与批判，模拟科研过程。

Result: 在1,214篇ICLR论文评估中，LLMs能重新发现可行方案，偶有创新，但解决能力不稳定且对问题表述敏感。

Conclusion: LLMs在自主科研问题解决方面展现出潜力和局限性，但其能力高度依赖于问题的呈现方式。

Abstract: Large language models (LLMs) demonstrate impressive capabilities across a
wide range of tasks, yet it remains unclear whether such success reflects
genuine reasoning or sophisticated recall. We introduce AInstein, a framework
for testing whether LLMs can generate valid solutions to AI research problems
using only their pretrained parametric knowledge -- without domain-specific
fine-tuning, retrieval augmentation, or other external aids. Our approach
extracts distilled problem statements from high-quality ICLR 2025 submissions,
then tasks specialized solver agents with proposing and refining technical
solutions through iterative critique loops, mimicking the cycles of proposal,
review, and revision central to scientific inquiry. We evaluate AInstein on
1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster),
using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by
targeted manual checks. Performance is assessed with three metrics: Success
Rate (does the solution address the problem?), Rediscovery (does it align with
human-proposed methods?), and Novelty (does it yield valid, original
approaches?). Our results reveal that while LLMs can rediscover feasible
solutions and occasionally propose creative alternatives, their problem-solving
ability remains fragile and highly sensitive to framing. These findings provide
the first large-scale evidence on the extent to which LLMs can act as
autonomous scientific problem-solvers, highlighting both their latent potential
and their current limitations.

</details>


### [484] [NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification](https://arxiv.org/abs/2510.05451)
*Fadi Al Machot,Fidaa Al Machot*

Main category: cs.AI

TL;DR: 提出一种结合ASP和Transformer的混合方法，用于航空安全报告系统（ASRS）的文本分类，以解决Transformer模型常违反领域逻辑的问题，通过规则增强和可微分正则化提高模型性能和逻辑一致性。


<details>
  <summary>Details</summary>
Motivation: 深度Transformer模型在多标签文本分类中表现出色，但在安全关键应用中常违反领域逻辑。

Method: 将领域知识形式化为加权的ASP规则，并使用Clingo求解器进行验证。通过规则增强（生成逻辑一致的合成样本）和模糊逻辑正则化（在微调过程中以可微分形式强制执行规则）两种互补方式将规则整合到Transformer模型中。

Result: 与标准的二元交叉熵（BCE）基线相比，该方法提高了微平均F1和宏平均F1分数，并在ASRS测试集上实现了高达86%的规则违规减少。

Conclusion: 这是首个将ASP推理、规则驱动增强和可微分Transformer训练相结合，用于航空安全报告系统（ASRS）的大规模神经符号应用，旨在实现可信赖、安全关键的自然语言处理。

Abstract: Deep transformer models excel at multi-label text classification but often
violate domain logic that experts consider essential, an issue of particular
concern in safety-critical applications. We propose a hybrid neuro-symbolic
framework that integrates Answer Set Programming (ASP) with transformer-based
learning on the Aviation Safety Reporting System (ASRS) corpus. Domain
knowledge is formalized as weighted ASP rules and validated using the Clingo
solver. These rules are incorporated in two complementary ways: (i) as
rule-based data augmentation, generating logically consistent synthetic samples
that improve label diversity and coverage; and (ii) as a fuzzy-logic
regularizer, enforcing rule satisfaction in a differentiable form during
fine-tuning. This design preserves the interpretability of symbolic reasoning
while leveraging the scalability of deep neural architectures. We further tune
per-class thresholds and report both standard classification metrics and
logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE)
baseline, our approach improves micro- and macro-F1 scores and achieves up to
an 86% reduction in rule violations on the ASRS test set. To the best of our
knowledge, this constitutes the first large-scale neuro-symbolic application to
ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and
differentiable transformer training for trustworthy, safety-critical NLP.

</details>


### [485] [Do Code Models Suffer from the Dunning-Kruger Effect?](https://arxiv.org/abs/2510.05457)
*Mukul Singh,Somya Chatterjee,Arjun Radhakrishna,Sumit Gulwani*

Main category: cs.AI

TL;DR: AI模型在编程任务中会表现出类似邓宁-克鲁格效应的现象，能力越弱或在不熟悉/低资源语言环境下，越倾向于高估自身能力。


<details>
  <summary>Details</summary>
Motivation: 探究人工智能系统在与人类进行创造性和技术性协作时，其认知边界和偏见如何影响我们共同的代理能力，特别是邓宁-克ruger效应（DKE）在编码任务中的表现。

Method: 通过分析模型在多样化编程语言上的置信度和性能，揭示AI模型是否会复制人类在能力不足时倾向于高估自身能力的模式。

Result: 实验表明，能力较弱的AI模型以及在稀有编程语言环境下运行的模型表现出更强的DKE偏见，显示出偏见的强度与模型的竞争力成正比。

Conclusion: AI模型，尤其是在编码任务中，会显现出类似邓宁-克鲁格效应的偏见，这表明AI的认知偏见可能与人类相似，特别是在它们能力不足或面对不熟悉的环境时。

Abstract: As artificial intelligence systems increasingly collaborate with humans in
creative and technical domains, questions arise about the cognitive boundaries
and biases that shape our shared agency. This paper investigates the
Dunning-Kruger Effect (DKE), the tendency for those with limited competence to
overestimate their abilities in state-of-the-art LLMs in coding tasks. By
analyzing model confidence and performance across a diverse set of programming
languages, we reveal that AI models mirror human patterns of overconfidence,
especially in unfamiliar or low-resource domains. Our experiments demonstrate
that less competent models and those operating in rare programming languages
exhibit stronger DKE-like bias, suggesting that the strength of the bias is
proportionate to the competence of the models.

</details>


### [486] [VAL-Bench: Measuring Value Alignment in Language Models](https://arxiv.org/abs/2510.05465)
*Aman Gupta,Denny O'Shea,Fazl Barez*

Main category: cs.AI

TL;DR: 该研究引入了一个名为 VAL-Bench 的新基准，用于评估大型语言模型（LLMs）在面对有争议的现实世界问题时是否能保持一致的人类价值观。该基准包含 115K 对来自维基百科争议部分的配对提示，并使用 LLM-as-judge 来衡量模型在面对不同框架下的反应是否一致。研究发现，现有模型在价值观对齐方面存在显著差异，并且安全策略与表达价值观之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要检查 LLMs 的拒绝或预定义的 violate，但不能揭示模型在面对有争议的现实世界问题时是否能体现一致的人类价值观。

Method: 我们引入了 VAL-Bench，这是一个包含 115K 对配对提示的基准，这些提示来自维基百科的争议部分。我们使用 LLM-as-judge 来衡量模型在面对配对提示时反应的一致性或分歧性。

Result: 研究发现，在接受测试的领先的开源和闭源模型中，价值观对齐方面存在巨大的差异。这突出表明，在安全策略（例如，拒绝）和更具表现力的价值体系之间存在权衡。

Conclusion: VAL-Bench 提供了一个可扩展、可重现的基准，可以系统地比较 LLMs 体现人类价值观的可靠性。

Abstract: Large language models (LLMs) are increasingly used for tasks where outputs
shape human decisions, so it is critical to test whether their responses
reflect consistent human values. Existing benchmarks mostly track refusals or
predefined safety violations, but these only check rule compliance and do not
reveal whether a model upholds a coherent value system when facing
controversial real-world issues. We introduce the \textbf{V}alue
\textbf{AL}ignment \textbf{Bench}mark (\textbf{VAL-Bench}), which evaluates
whether models maintain a stable value stance across paired prompts that frame
opposing sides of public debates. VAL-Bench consists of 115K such pairs from
Wikipedia's controversial sections. A well-aligned model should express similar
underlying views regardless of framing, which we measure using an LLM-as-judge
to score agreement or divergence between paired responses. Applied across
leading open- and closed-source models, the benchmark reveals large variation
in alignment and highlights trade-offs between safety strategies (e.g.,
refusals) and more expressive value systems. By providing a scalable,
reproducible benchmark, VAL-Bench enables systematic comparison of how reliably
LLMs embody human values.

</details>


### [487] [Vul-R2: A Reasoning LLM for Automated Vulnerability Repair](https://arxiv.org/abs/2510.05480)
*Xin-Cheng Wen,Zirui Lin,Yijun Yang,Cuiyun Gao,Deheng Ye*

Main category: cs.AI

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: The exponential increase in software vulnerabilities has created an urgent
need for automatic vulnerability repair (AVR) solutions. Recent research has
formulated AVR as a sequence generation problem and has leveraged large
language models (LLMs) to address this problem. Typically, these approaches
prompt or fine-tune LLMs to generate repairs for vulnerabilities directly.
Although these methods show state-of-the-art performance, they face the
following challenges: (1) Lack of high-quality, vulnerability-related reasoning
data. Current approaches primarily rely on foundation models that mainly encode
general programming knowledge. Without vulnerability-related reasoning data,
they tend to fail to capture the diverse vulnerability repair patterns. (2)
Hard to verify the intermediate vulnerability repair process during LLM
training. Existing reinforcement learning methods often leverage intermediate
execution feedback from the environment (e.g., sandbox-based execution results)
to guide reinforcement learning training. In contrast, the vulnerability repair
process generally lacks such intermediate, verifiable feedback, which poses
additional challenges for model training.

</details>


### [488] [Decade-long Emission Forecasting with an Ensemble Model in Taiwan](https://arxiv.org/abs/2510.05548)
*Gordon Hung,Salinna Abdullah*

Main category: cs.AI

TL;DR: 台湾面临严峻的空气污染问题，本研究旨在通过比较21种时间序列模型来预测二氧化碳排放量，并提出了一种基于集成学习的预测模型，以辅助政策制定。


<details>
  <summary>Details</summary>
Motivation: 台湾人口稠密且依赖化石燃料，导致严重的空气污染和温室气体排放（主要是二氧化碳）。

Method: 研究人员比较了21种常用的时间序列模型（包括单变量和多变量方法），并重点分析了前馈神经网络（FFNN）、支持向量机（SVM）和随机森林回归（RFR）这三种表现最佳的模型。在此基础上，他们提出了一种定制的堆叠泛化集成技术，将这些表现最佳的模型与线性回归相结合，以进一步提高预测的鲁棒性。

Result: 研究提出的集成模型在预测精度上达到了SMAPE 1.407，并且没有出现过拟合现象。

Conclusion: 该研究成功构建了一个能够进行准确的、长达十年的排放量预测的模型，旨在为政策制定者提供数据驱动的决策支持。

Abstract: Taiwan's high population and heavy dependence on fossil fuels have led to
severe air pollution, with the most prevalent greenhouse gas being carbon
dioxide (CO2). There-fore, this study presents a reproducible and comprehensive
case study comparing 21 of the most commonly employed time series models in
forecasting emissions, analyzing both univariate and multivariate approaches.
Among these, Feedforward Neural Network (FFNN), Support Vector Machine (SVM),
and Random Forest Regressor (RFR) achieved the best performances. To further
enhance robustness, the top performers were integrated with Linear Regression
through a custom stacked generalization en-semble technique. Our proposed
ensemble model achieved an SMAPE of 1.407 with no signs of overfitting.
Finally, this research provides an accurate decade-long emission projection
that will assist policymakers in making more data-driven decisions.

</details>


### [489] [MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption](https://arxiv.org/abs/2510.05580)
*Chen Li,Zhantao Yang,Han Zhang,Fangyi Chen,Chenchen Zhu,Anudeepsekhar Bolimera,Marios Savvides*

Main category: cs.AI

TL;DR: MetaVLA是一个统一的、骨干无关的、用于高效和可扩展的对齐的训练后框架，它通过结合上下文感知的元协同训练，在LIBERO基准上，在长时任务上比OpenVLA的性能高出8.0%，同时减少了训练步骤和GPU时间。


<details>
  <summary>Details</summary>
Motivation: 目前的视觉-语言-动作（VLA）模型在具身推理方面虽然有前景，但离真正的通才还有很大距离，它们通常需要针对特定任务进行微调，并且泛化到未见过的任务的能力较差。

Method: MetaVLA通过上下文感知的元协同训练（Context-Aware Meta Co-Training）将多样化的目标任务整合到单个微调阶段，并利用结构多样化的辅助任务来改善域内泛化。该框架集成了一个轻量级的元学习机制（源自Attentive Neural Processes），以实现从不同上下文的快速适应，同时保持最小的架构更改和推理开销。

Result: 在LIBERO基准上，集成了六个辅助任务的MetaVLA在长时任务上的表现比OpenVLA高出8.0%，训练步骤从240K减少到75K，GPU时间减少了约76%。

Conclusion: MetaVLA证明了可扩展、低资源的训练后方法是可行的，为实现通用目的的具身代理铺平了道路。

Abstract: Vision-Language-Action (VLA) models show promise in embodied reasoning, yet
remain far from true generalists-they often require task-specific fine-tuning,
and generalize poorly to unseen tasks. We propose MetaVLA, a unified,
backbone-agnostic post-training framework for efficient and scalable alignment.
MetaVLA introduces Context-Aware Meta Co-Training, which consolidates diverse
target tasks into a single fine-tuning stage while leveraging structurally
diverse auxiliary tasks to improve in-domain generalization. Unlike naive
multi-task SFT, MetaVLA integrates a lightweight meta-learning
mechanism-derived from Attentive Neural Processes-to enable rapid adaptation
from diverse contexts with minimal architectural change or inference overhead.
On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA
by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K,
and cuts GPU time by ~76%. These results show that scalable, low-resource
post-training is achievable-paving the way toward general-purpose embodied
agents. Code will be available.

</details>


### [490] [From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions](https://arxiv.org/abs/2510.05596)
*Changyuan Zhao,Ruichen Zhang,Jiacheng Wang,Dusit Niyato,Geng Sun,Xianbin Wang,Shiwen Mao,Abbas Jamalipour*

Main category: cs.AI

TL;DR: 该论文提出了一种名为“自演化代理人工智能”的新范式，用于无线系统，使代理能够自主地适应和改进。


<details>
  <summary>Details</summary>
Motivation: 为了实现未来无线系统的自主演进和持续改进，克服传统静态AI模型的局限性。

Method: 提出了一种多代理协作的自演化代理AI框架，其中大型语言模型（LLMs）被分配了角色专门化的提示，并在主管代理的协调下进行工作。该框架通过结构化对话、迭代反馈和系统化验证，自主执行完整的生命周期。具体技术包括工具智能、工作流优化、自我反思和演化学习。

Result: 在低空无线网络（LAWNs）的天线演化案例研究中，该框架成功地将固定的天线优化升级为移动天线优化。实验结果表明，该系统能够自主地提高波束增益并恢复性能，最高可提升52.02%，并且表现优于固定的基线模型，同时几乎不需要人工干预。

Conclusion: 自演化代理AI框架在无线通信领域具有高度的适应性和鲁棒性，能够自主地优化系统性能，为下一代无线智能提供了有效解决方案。

Abstract: Self-evolving agentic artificial intelligence (AI) offers a new paradigm for
future wireless systems by enabling autonomous agents to continually adapt and
improve without human intervention. Unlike static AI models, self-evolving
agents embed an autonomous evolution cycle that updates models, tools, and
workflows in response to environmental dynamics. This paper presents a
comprehensive overview of self-evolving agentic AI, highlighting its layered
architecture, life cycle, and key techniques, including tool intelligence,
workflow optimization, self-reflection, and evolutionary learning. We further
propose a multi-agent cooperative self-evolving agentic AI framework, where
multiple large language models (LLMs) are assigned role-specialized prompts
under the coordination of a supervisor agent. Through structured dialogue,
iterative feedback, and systematic validation, the system autonomously executes
the entire life cycle without human intervention. A case study on antenna
evolution in low-altitude wireless networks (LAWNs) demonstrates how the
framework autonomously upgrades fixed antenna optimization into movable antenna
optimization. Experimental results show that the proposed self-evolving agentic
AI autonomously improves beam gain and restores degraded performance by up to
52.02%, consistently surpassing the fixed baseline with little to no human
intervention and validating its adaptability and robustness for next-generation
wireless intelligence.

</details>


### [491] [Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography](https://arxiv.org/abs/2510.05664)
*Hanna Kreutzer,Anne-Sophie Caselitz,Thomas Dratsch,Daniel Pinto dos Santos,Christiane Kuhl,Daniel Truhn,Sven Nebelung*

Main category: cs.AI

TL;DR: GPT-4o能从放射学报告中提取诊断标签，并用于训练多标签图像分类模型，性能具有竞争力，不确定性标签不影响模型性能。


<details>
  <summary>Details</summary>
Motivation: 评估GPT-4o从自由文本放射学报告中提取诊断标签（含不确定性）的能力，并测试这些标签如何影响肌肉骨骼放射照片的多标签图像分类。

Method: 本回顾性研究纳入了锁骨（n=1,170）、肘部（n=3,755）和拇指（n=1,978）的放射学系列。对数据进行匿名化处理后，使用GPT-4o填写结构化模板，将影像学发现标记为“存在”（true）、“不存在”（false）或“不确定”（uncertain）。为了评估标签不确定性的影响，“不确定”标签在训练和验证集中被自动重新分配为“true”（包含）或“false”（排除）。使用ResNet50和标签-图像对进行多标签分类。手动验证了内部（锁骨：n=233，肘部：n=745，拇指：n=393）和外部（n=300/模型）测试集上的标签提取准确性。使用宏平均接受者操作特征（ROC）曲线下面积（AUC）、精确率-召回率曲线、敏感性、特异性和准确性进行评估。使用DeLong检验比较AUC。

Result: 自动提取在测试集的标签中准确率为98.6%（61,488个中有60,618个）。在所有解剖区域中，基于标签的模型训练在宏平均AUC方面表现出有竞争力的性能，包含标签（例如，肘部：AUC=0.80 [范围，0.62-0.87]）和排除标签（肘部：AUC=0.80 [范围，0.61-0.87]）的模型性能相当。模型在外部数据集上表现出良好的泛化能力（肘部[包含]：AUC=0.79 [范围，0.61-0.87]；肘部[排除]：AUC=0.79 [范围，0.63-0.89]）。在不同标签策略或数据集之间未观察到显著差异（p>=0.15）。

Conclusion: GPT-4o能从放射学报告中提取标签，以高准确率训练出具有竞争力的多标签分类模型。检测到的放射学报告中的不确定性并未影响这些模型的性能。

Abstract: Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with
uncertainty) from free-text radiology reports and to test how these labels
affect multi-label image classification of musculoskeletal radiographs.
Methods: This retrospective study included radiography series of the clavicle
(n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o
filled out structured templates by indicating imaging findings as present
("true"), absent ("false"), or "uncertain." To assess the impact of label
uncertainty, "uncertain" labels of the training and validation sets were
automatically reassigned to "true" (inclusive) or "false" (exclusive).
Label-image-pairs were used for multi-label classification using ResNet50.
Label extraction accuracy was manually verified on internal (clavicle: n=233,
elbow: n=745, thumb: n=393) and external test sets (n=300 for each).
Performance was assessed using macro-averaged receiver operating characteristic
(ROC) area under the curve (AUC), precision recall curves, sensitivity,
specificity, and accuracy. AUCs were compared with the DeLong test. Results:
Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the
test sets. Across anatomic regions, label-based model training yielded
competitive performance measured by macro-averaged AUC values for inclusive
(e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow:
AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets
(elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79
[range, 0.63-0.89]). No significant differences were observed across labeling
strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from
radiologic reports to train competitive multi-label classification models with
high accuracy. Detected uncertainty in the radiologic reports did not influence
the performance of these models.

</details>


### [492] [D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI](https://arxiv.org/abs/2510.05684)
*Suwhan Choi,Jaeyoon Jung,Haebin Seong,Minchan Kim,Minyeong Kim,Yongjun Cho,Yoonshik Kim,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: 本研究提出了D2E框架，通过桌面交互预训练来解决实体AI（embodied AI）中物理轨迹数据收集成本高昂的问题，实现了从桌面到实体机器人的有效迁移。


<details>
  <summary>Details</summary>
Motivation: 物理轨迹数据的收集成本高昂限制了实体AI的发展，而桌面环境（特别是游戏）可以提供丰富的、大规模的交互数据，是实体AI学习的理想替代方案。

Method: D2E框架包含三个组件：1. OWA Toolkit：统一不同桌面交互数据，实现152倍压缩。 2. Generalist-IDM：通过基于时间戳的事件预测，实现跨游戏零样本泛化和大规模伪标签生成。 3. VAPT：将桌面预训练的表示迁移到物理操作和导航任务。

Result: 使用1.3K+小时的数据（259小时人类演示，1K+小时伪标签），在LIBERO操作任务上实现了96.6%的成功率，在CANVAS导航任务上实现了83.3%的成功率。

Conclusion: 数字交互中的感觉运动原语具有足够的 বলেছেন不变性，可以有效地迁移到物理实体任务中，证明了桌面预训练作为机器人领域实用范例的可行性。

Abstract: Large language models leverage internet-scale text data, yet embodied AI
remains constrained by the prohibitive costs of physical trajectory collection.
Desktop environments -- particularly gaming -- offer a compelling alternative:
they provide rich sensorimotor interactions at scale while maintaining the
structured observation-action coupling essential for embodied learning. We
present D2E (Desktop to Embodied AI), a framework that demonstrates desktop
interactions can serve as an effective pretraining substrate for robotics
embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT
for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a
complete pipeline from scalable desktop data collection to verified transfer in
embodied domains. Our framework comprises three components: (1) the OWA Toolkit
that unifies diverse desktop interactions into a standardized format with 152x
compression, (2) the Generalist-IDM that achieves strong zero-shot
generalization across unseen games through timestamp-based event prediction,
enabling internet-scale pseudo-labeling, and (3) VAPT that transfers
desktop-pretrained representations to physical manipulation and navigation.
Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of
pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO
manipulation and 83.3% on CANVAS navigation benchmarks. This validates that
sensorimotor primitives in digital interactions exhibit sufficient invariance
to transfer meaningfully to physical embodied tasks, establishing desktop
pretraining as a practical paradigm for robotics. We will make all our work
public, including the OWA toolkit, datasets of human-collected and
pseudo-labeled, and VAPT-trained models available at
https://worv-ai.github.io/d2e/

</details>


### [493] [Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach](https://arxiv.org/abs/2510.05698)
*Yousef Emami,Seyedsina Nabavirazavi,Jingjing Zheng,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 本文提出了一种基于注意力机制的上下文学习方法（AIC-VDS），用于优化无人机（UAV）在海啸等灾后监测场景下的数据收集调度和速度控制，以最小化数据丢失。


<details>
  <summary>Details</summary>
Motivation: 在海啸等灾后监测场景中，无人机（UAV）在收集传感数据时面临数据收集调度和飞行速度设计的挑战，不当的设置可能导致传输错误和缓冲区溢出，造成严重的数据包丢失。传统的深度强化学习（DRL）方法训练复杂且存在仿真与现实不匹配的问题，无法满足紧急监测的需求。

Method: 提出了一种名为“注意力机制上下文学习用于速度控制和数据收集调度（AIC-VDS）”的方法，作为DRL在紧急情况下的替代方案。该方法考虑了地面传感器的电池电量、队列长度、信道条件以及无人机的轨迹，以联合优化多个无人机的数据收集调度和速度控制，从而最小化数据丢失。

Result: 仿真结果表明，所提出的AIC-VDS方法在最小化数据丢失方面优于深度Q网络（DQN）和最大信道增益基线方法。

Conclusion: AIC-VDS是一种有效的方法，可以解决海啸等灾后监测场景下无人机数据收集调度和速度控制的挑战，能够有效减少数据丢失，并且优于现有的DRL和基线方法。

Abstract: Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated
to collect sensory data in post-disaster monitoring scenarios, such as
tsunamis, where early actions are critical to limit coastal damage. A major
challenge is to design the data collection schedules and flight velocities, as
unfavorable schedules and velocities can lead to transmission errors and buffer
overflows of the ground sensors, ultimately resulting in significant packet
loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a
complex training process and a mismatch between simulation and reality that
does not meet the urgent requirements of tsunami monitoring. Recent advances in
Large Language Models (LLMs) offer a compelling alternative. With their strong
reasoning and generalization capabilities, LLMs can adapt to new tasks through
In-Context Learning (ICL), which enables task adaptation through natural
language prompts and example-based guidance without retraining. However, LLM
models have input data limitations and thus require customized approaches. In
this paper, a joint optimization of data collection schedules and velocities
control for multiple UAVs is proposed to minimize data loss. The battery level
of the ground sensors, the length of the queues, and the channel conditions, as
well as the trajectories of the UAVs, are taken into account. Attention-Based
In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS)
is proposed as an alternative to DRL in emergencies. The simulation results
show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and
maximum channel gain baselines.

</details>


### [494] [Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge](https://arxiv.org/abs/2510.05733)
*Zijun Jia,Shuang Liang,Jinsong Yu*

Main category: cs.AI

TL;DR: Syn-Diag 是一个利用大语言模型克服工业故障诊断中数据稀缺和资源限制问题的云边协同框架，通过视觉-语义协同、内容感知推理和云边协同，实现了高效的少样本故障诊断。


<details>
  <summary>Details</summary>
Motivation: 工业故障诊断面临数据稀缺和在资源受限环境中部署大型 AI 模型的双重挑战。

Method: Syn-Diag 框架包括三个机制：1) 视觉-语义协同，通过跨模态预训练将信号特征与大语言模型的语义空间对齐；2) 内容感知推理，动态构建上下文提示以提高有限样本的诊断准确性；3) 云边协同，利用知识蒸馏创建轻量级、高效的边缘模型，并可通过共享决策空间进行在线更新。

Result: 在六个数据集上的广泛实验表明，Syn-Diag 在 1 样本和跨条件场景下显著优于现有方法。边缘模型在性能上可与云端版本相媲美，同时模型尺寸减小 83%，延迟降低 50%。

Conclusion: Syn-Diag 提供了一种实用、鲁棒且可部署的现代智能诊断范式，有效解决了工业故障诊断中的数据稀缺和资源限制问题。

Abstract: Industrial fault diagnosis faces the dual challenges of data scarcity and the
difficulty of deploying large AI models in resource-constrained environments.
This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that
leverages Large Language Models to overcome these limitations in few-shot fault
diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic
Synergy, which aligns signal features with the LLM's semantic space through
cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically
constructs contextual prompts to enhance diagnostic accuracy with limited
samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create
a lightweight, efficient edge model capable of online updates via a shared
decision space. Extensive experiments on six datasets covering different CWRU
and SEU working conditions show that Syn-Diag significantly outperforms
existing methods, especially in 1-shot and cross-condition scenarios. The edge
model achieves performance comparable to the cloud version while reducing model
size by 83% and latency by 50%, offering a practical, robust, and deployable
paradigm for modern intelligent diagnostics.

</details>


### [495] [Artificially intelligent agents in the social and behavioral sciences: A history and outlook](https://arxiv.org/abs/2510.05743)
*Petter Holme,Milena Tsvetkova*

Main category: cs.AI

TL;DR: 本文回顾了人工智能代理（agentic AI）在社会和行为科学中的历史发展和当前趋势，从早期的可编程计算机和社会模拟，一直到今天的大型语言模型实验。文章重点关注AI在科学过程中的作用，以及技术进步和科学演变（约1950年至今）带来的变化。


<details>
  <summary>Details</summary>
Motivation: 回顾人工智能代理（agentic AI）在社会和行为科学中的历史发展和当前趋势，重点关注AI在科学过程中的作用以及技术进步和科学演变带来的变化。

Method: 通过回顾从早期可编程计算机、社会模拟到大型语言模型实验的历史发展和当前趋势，分析AI在社会和行为科学中的应用。

Result: 文章涵盖了早期社会模拟研究的挑战、社会系统科学的兴起、智能博弈论代理、大数据时代的认知变革以及生成式AI应用的现状等内容。

Conclusion: 人工智能代理在社会和行为科学中的发展深刻影响了我们理解自身的方式，技术与科学的融合日益加深。

Abstract: We review the historical development and current trends of artificially
intelligent agents (agentic AI) in the social and behavioral sciences: from the
first programmable computers, and social simulations soon thereafter, to
today's experiments with large language models. This overview emphasizes the
role of AI in the scientific process and the changes brought about, both
through technological advancements and the broader evolution of science from
around 1950 to the present. Some of the specific points we cover include: the
challenges of presenting the first social simulation studies to a world unaware
of computers, the rise of social systems science, intelligent game theoretic
agents, the age of big data and the epistemic upheaval in its wake, and the
current enthusiasm around applications of generative AI, and many other topics.
A pervasive theme is how deeply entwined we are with the technologies we use to
understand ourselves.

</details>


### [496] [ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems](https://arxiv.org/abs/2510.05746)
*Bohan Yao,Shiva Krishna Reddy Malay,Vikas Yadav*

Main category: cs.AI

TL;DR: LLM驱动的多智能体系统(MAS)在复杂推理任务上表现优异，但现有自动设计技术效率低下且成本高昂。本文提出了一种新的自动MAS设计范式，专注于优化思维链(CoT)推理，引入了代理推理模块(ARM)，通过代码空间搜索和变异发现，ARM作为可复用的推理构建块，显著优于现有方法，并展现出良好的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有LLM驱动的MAS自动设计技术效率低下、成本高昂，且性能往往不如简单的基线方法，需要为新任务重新发现架构和进行昂贵的数据标注。简单的思维链(CoT)推理已能达到有竞争力的性能，表明CoT本身值得深入研究。

Method: 提出了一种新的自动MAS设计范式，将重点转移到优化CoT推理。引入了代理推理模块(ARM)，这是CoT的代理泛化，其中每个细粒度的推理步骤由一个专门的推理模块执行。该模块通过代码空间上的树搜索发现，从一个简单的CoT模块开始，并利用对执行跟踪的反思进行变异以进行演化。由此产生的ARM可以作为直接的递归循环或学习到的元协调器中的子程序使用。

Result: 通过ARM构建的MAS在与手动设计的MAS和最先进的自动MAS设计方法相比时，表现显著优越。ARM构建的MAS表现出极好的泛化能力，在不同的基础模型和任务领域上无需进一步优化即可保持高性能。

Conclusion: ARM作为一种通用的推理构建块，通过优化CoT推理，能够显著提升MAS的设计效率和性能，并实现跨模型和跨任务的良好泛化能力。

Abstract: Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved
state-of-the-art results on various complex reasoning tasks. Recent works have
proposed techniques to automate the design of MASes, eliminating the need for
manual engineering. However, these techniques perform poorly, often achieving
similar or inferior performance to simple baselines. Furthermore, they require
computationally expensive re-discovery of architectures for each new task
domain and expensive data annotation on domains without existing labeled
validation sets. A critical insight is that simple Chain of Thought (CoT)
reasoning often performs competitively with these complex systems, suggesting
that the fundamental reasoning unit of MASes, CoT, warrants further
investigation. To this end, we present a new paradigm for automatic MAS design
that pivots the focus to optimizing CoT reasoning. We introduce the Agentic
Reasoning Module (ARM), an agentic generalization of CoT where each granular
reasoning step is executed by a specialized reasoning module. This module is
discovered through a tree search over the code space, starting from a simple
CoT module and evolved using mutations informed by reflection on execution
traces. The resulting ARM acts as a versatile reasoning building block which
can be utilized as a direct recursive loop or as a subroutine in a learned
meta-orchestrator. Our approach significantly outperforms both manually
designed MASes and state-of-the-art automatic MAS design methods. Crucially,
MASes built with ARM exhibit superb generalization, maintaining high
performance across different foundation models and task domains without further
optimization.

</details>


### [497] [The Safety Challenge of World Models for Embodied AI Agents: A Review](https://arxiv.org/abs/2510.05865)
*Lorenzo Baraldi,Zifan Zeng,Chongzhe Zhang,Aradhana Nayak,Hongbo Zhu,Feng Liu,Qunli Zhang,Peng Wang,Shiming Liu,Zheng Hu,Angelo Cangelosi,Lorenzo Baraldi*

Main category: cs.AI

TL;DR: 本文对自动驾驶和机器人领域的强化学习世界模型进行了文献综述，重点关注了场景和控制生成任务中的安全问题，并对现有模型的预测进行了实证分析，识别和分类了常见故障。


<details>
  <summary>Details</summary>
Motivation: 为了应对具身智能领域对更高级、更集成模型的需求，这些模型能够感知、解释和预测环境动态，并确保对智能体和环境的安全。

Method: 通过文献综述，重点关注自动驾驶和机器人领域的世界模型在场景和控制生成任务中的安全问题。此外，还通过实证分析收集和检查了最先进模型的预测，识别和分类了常见故障（病态），并对结果进行了定量评估。

Result: 识别并量化了现有世界模型在场景和控制生成任务中可能出现的安全问题（病态）。

Conclusion: 世界模型在具身智能中具有巨大潜力，但在实际应用中，特别是自动驾驶和机器人领域，必须优先考虑其预测的安全性。

Abstract: The rapid progress in embodied artificial intelligence has highlighted the
necessity for more advanced and integrated models that can perceive, interpret,
and predict environmental dynamics. In this context, World Models (WMs) have
been introduced to provide embodied agents with the abilities to anticipate
future environmental states and fill in knowledge gaps, thereby enhancing
agents' ability to plan and execute actions. However, when dealing with
embodied agents it is fundamental to ensure that predictions are safe for both
the agent and the environment. In this article, we conduct a comprehensive
literature review of World Models in the domains of autonomous driving and
robotics, with a specific focus on the safety implications of scene and control
generation tasks. Our review is complemented by an empirical analysis, wherein
we collect and examine predictions from state-of-the-art models, identify and
categorize common faults (herein referred to as pathologies), and provide a
quantitative evaluation of the results.

</details>


### [498] [Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport](https://arxiv.org/abs/2510.05751)
*Jeffrey N. Clark,Elena Fillola,Nawid Keshtmand,Raul Santos-Rodriguez,Matthew Rigby*

Main category: cs.AI

TL;DR: 利用图神经网络模拟拉格朗日粒子扩散模型，实现了大气传输模拟的加速和不确定性量化，提高了温室气体排放监测的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有的温室气体监测方法在效率、可扩展性和可靠性方面存在不足，特别是顶层方法中使用的传输模型计算成本高且不确定性难以量化。

Method: 提出了一种基于图神经网络（GNN）的拉格朗日粒子扩散模型（LPDM）模拟器，结合集合方法来估计大气传输“足迹”、温室气体摩尔分数及其不确定性。该方法使用GOSAT观测数据在2016年的巴西进行了演示。

Result: GNN模拟器比传统的NAME LPDM快约1000倍，同时能复现大尺度足迹结构。集合计算揭示了预测误差的空间相关性，并突显了大气传输足迹和甲烷摩尔分数在时间和空间上的低置信度预测区域。

Conclusion: 该方法为大气传输模型提供了加速和不确定性量化的途径，有望支持不确定性感知的温室气体反演系统，提高基于卫星的温室气体排放监测的鲁棒性。未来可进一步用于探索LPDM系统误差，为温室气体通量估算提供更全面的不确定性预算。

Abstract: Monitoring greenhouse gas emissions and evaluating national inventories
require efficient, scalable, and reliable inference methods. Top-down
approaches, combined with recent advances in satellite observations, provide
new opportunities to evaluate emissions at continental and global scales.
However, transport models used in these methods remain a key source of
uncertainty: they are computationally expensive to run at scale, and their
uncertainty is difficult to characterise. Artificial intelligence offers a dual
opportunity to accelerate transport simulations and to quantify their
associated uncertainty.
  We present an ensemble-based pipeline for estimating atmospheric transport
"footprints", greenhouse gas mole fraction measurements, and their
uncertainties using a graph neural network emulator of a Lagrangian Particle
Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse
Gases Observing Satellite) observations for Brazil in 2016. The emulator
achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale
footprint structures. Ensembles were calculated to quantify absolute and
relative uncertainty, revealing spatial correlations with prediction error. The
results show that ensemble spread highlights low-confidence spatial and
temporal predictions for both atmospheric transport footprints and methane mole
fractions.
  While demonstrated here for an LPDM emulator, the approach could be applied
more generally to atmospheric transport models, supporting uncertainty-aware
greenhouse gas inversion systems and improving the robustness of
satellite-based emissions monitoring. With further development, ensemble-based
emulators could also help explore systematic LPDM errors, offering a
computationally efficient pathway towards a more comprehensive uncertainty
budget in greenhouse gas flux estimates.

</details>


### [499] [Information-Theoretic Policy Pre-Training with Empowerment](https://arxiv.org/abs/2510.05996)
*Moritz Schneider,Robert Krug,Narunas Vaskevicius,Luigi Palmieri,Michael Volpp,Joschka Boedecker*

Main category: cs.AI

TL;DR: empowerment可作为强化学习的预训练信号，用于提高下游任务的适应性。


<details>
  <summary>Details</summary>
Motivation: empowerment作为强化学习的内在激励和探索框架，但作为预训练信号的研究有限，本文旨在探索其潜力。

Method: 提出折扣empowerment，平衡短期和长期影响，并以此为基础设计预训练范式，最大化折扣empowerment来初始化策略。

Result: empowerment预训练策略在多种强化学习算法上均有效，能够提高数据效率和下游任务的适应性，尤其是在长折扣因子下。

Conclusion: empowerment预训练是一种通用的初始化策略，能够提升强化学习的性能，并为未来扩展到高维复杂任务提供了方向。

Abstract: Empowerment, an information-theoretic measure of an agent's potential
influence on its environment, has emerged as a powerful intrinsic motivation
and exploration framework for reinforcement learning (RL). Besides for
unsupervised RL and skill learning algorithms, the specific use of empowerment
as a pre-training signal has received limited attention in the literature. We
show that empowerment can be used as a pre-training signal for data-efficient
downstream task adaptation. For this we extend the traditional notion of
empowerment by introducing discounted empowerment, which balances the agent's
control over the environment across short- and long-term horizons. Leveraging
this formulation, we propose a novel pre-training paradigm that initializes
policies to maximize discounted empowerment, enabling agents to acquire a
robust understanding of environmental dynamics. We analyze empowerment-based
pre-training for various existing RL algorithms and empirically demonstrate its
potential as a general-purpose initialization strategy: empowerment-maximizing
policies with long horizons are data-efficient and effective, leading to
improved adaptability in downstream tasks. Our findings pave the way for future
research to scale this framework to high-dimensional and complex tasks, further
advancing the field of RL.

</details>


### [500] [Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis](https://arxiv.org/abs/2510.05761)
*Sedat Dogan,Nina Dethlefs,Debarati Chakraborty*

Main category: cs.AI

TL;DR: 本研究提出了一种基于混合参与度得分的方法，利用跨语言数据集，通过XGBoost模型在30分钟内对表情包的病毒传播性进行早期预测，实现了PR-AUC > 0.52，并发现特征重要性随时间动态变化。


<details>
  <summary>Details</summary>
Motivation: 预测在线内容的病毒传播性，尤其是对于文化复杂且快速演变的表情包，仍然是一个挑战。本研究旨在探索使用早期数据预测表情包病毒传播性的可行性。

Method: 研究人员构建了一个大规模、跨语言的数据集，来自25个不同的Reddit社区。他们提出了一种基于混合参与度得分（hybrid engagement score）的病毒传播性定义方法，并从时间划分的训练集中学习基于百分位数的阈值，以防止数据泄露。研究评估了逻辑回归、XGBoost和多层感知器（MLP）等模型，使用了包含静态内容和网络特征的模态特征集，并在不同时间窗口（30-420分钟）内进行了测试。

Result: 研究表明，有用的信号可以很快出现：表现最佳的XGBoost模型在仅仅30分钟内就达到了 PR-AUC > 0.52。分析揭示了一个明显的“证据过渡”现象，即随着表情包的传播，特征的重要性从静态上下文动态地转移到时间动态。

Conclusion: 本研究为无法获得完整传播链数据的早期病毒传播性预测场景建立了一个稳健、可解释且实用的基准。研究贡献了一个新颖的跨语言数据集，并提出了一种方法论上可靠的病毒传播性定义。本研究是首次结合时间序列数据与静态内容和网络特征来预测早期表情包病毒传播性的研究。

Abstract: Predicting the virality of online content remains challenging, especially for
culturally complex, fast-evolving memes. This study investigates the
feasibility of early prediction of meme virality using a large-scale,
cross-lingual dataset from 25 diverse Reddit communities. We propose a robust,
data-driven method to define virality based on a hybrid engagement score,
learning a percentile-based threshold from a chronologically held-out training
set to prevent data leakage. We evaluated a suite of models, including Logistic
Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive,
multimodal feature set across increasing time windows (30-420 min). Crucially,
useful signals emerge quickly: our best-performing model, XGBoost, achieves a
PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear "evidentiary
transition," in which the importance of the feature dynamically shifts from the
static context to the temporal dynamics as a meme gains traction. This work
establishes a robust, interpretable, and practical benchmark for early virality
prediction in scenarios where full diffusion cascade data is unavailable,
contributing a novel cross-lingual dataset and a methodologically sound
definition of virality. To our knowledge, this study is the first to combine
time series data with static content and network features to predict early meme
virality.

</details>


### [501] [ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming](https://arxiv.org/abs/2510.05774)
*Weichun Shi,Minghao Liu,Wanting Zhang,Langchen Shi,Fuqi Jia,Feifei Ma,Jian Zhang*

Main category: cs.AI

TL;DR: ConstraintLLM是第一个专门为约束编程（CP）建模设计的LLM，通过CARM和ToT框架提高了 in-context learning能力，并在IndusCP基准上取得了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管约束编程（CP）在解决现实世界的约束优化问题（COPs）方面至关重要，但相较于基于运筹学（OR）模型的研究，它受到的关注较少。LLM在自动生成COPs形式化模型方面展现出潜力，旨在借助符号求解器构建可信的神经-符号AI。

Method: ConstraintLLM使用多指令监督微调在一个开源LLM上进行训练。它集成了约束感知检索模块（CARM），以增强 in-context learning能力，并结合了引导式自我纠正机制的Tree-of-Thoughts（ToT）框架。此外，研究人员构建并发布了IndusCP，这是一个包含140个来自不同领域的工业级CP建模任务的基准。

Result: ConstraintLLM在多个基准上实现了最先进的求解准确性，并在新的IndusCP基准上比基线方法提高了2倍的性能。

Conclusion: ConstraintLLM是第一个专门为CP建模设计的LLM，通过结合CARM和ToT框架，能够有效解决CP建模问题，并在新的工业级基准IndusCP上取得了显著的性能提升。

Abstract: Constraint programming (CP) is a crucial technology for solving real-world
constraint optimization problems (COPs), with the advantages of rich modeling
semantics and high solving efficiency. Using large language models (LLMs) to
generate formal modeling automatically for COPs is becoming a promising
approach, which aims to build trustworthy neuro-symbolic AI with the help of
symbolic solvers. However, CP has received less attention compared to works
based on operations research (OR) models. We introduce ConstraintLLM, the first
LLM specifically designed for CP modeling, which is trained on an open-source
LLM with multi-instruction supervised fine-tuning. We propose the
Constraint-Aware Retrieval Module (CARM) to increase the in-context learning
capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with
guided self-correction mechanism. Moreover, we construct and release IndusCP,
the first industrial-level benchmark for CP modeling, which contains 140
challenging tasks from various domains. Our experiments demonstrate that
ConstraintLLM achieves state-of-the-art solving accuracy across multiple
benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark.
Code and data are available at: https://github.com/william4s/ConstraintLLM.

</details>


### [502] [Towards Label-Free Biological Reasoning Synthetic Dataset Creation via Uncertainty Filtering](https://arxiv.org/abs/2510.05871)
*Josefa Lia Stoisser,Lawrence Phillips,Aditya Misra,Tom A. Lamb,Philip Torr,Marc Boubnovski Martell,Julien Fauqueur,Kaspar Märtens*

Main category: cs.AI

TL;DR: 使用模型不确定性来过滤合成的思维链（CoT）数据，以提高生物学等数据稀疏领域的模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有的合成CoT方法需要昂贵的标签来生成或筛选数据，尤其是在生物学等领域，湿式实验数据稀缺，成本高昂。

Method: 提出了一种无需标签的方法，即基于不确定性的过滤。该方法利用模型自身的不确定性度量（如自洽性和预测困惑度）来量化置信度，并以此作为外部标签的替代。通过采样多个推理轨迹，只保留不确定性低的子集。

Result: 将该方法应用于生物学扰动预测任务，发现过滤后的子集准确率更高。使用不确定性过滤的数据进行有监督微调（SFT），其效果优于未过滤的合成数据，缩小了与真实标签训练的差距，并且超过了强大的大型语言模型（LLM）基线。消融实验表明，按类别过滤可以校正特定类别的 But also and hybrid uncertainty metrics can create higher quality datasets.

Conclusion: 模型内部的置信度是创建高效推理数据集的强大信号，使得LLM在监管昂贵的领域得到应用。

Abstract: Synthetic chain-of-thought (CoT) traces are widely used to train large
reasoning models (LRMs), improving generalization by providing step-level
supervision. Yet most approaches require ground-truth labels to seed or filter
these traces - an expensive bottleneck in domains like biology where wet-lab
data are scarce. We propose a label-free alternative: uncertainty-based
filtering, which uses a model's own confidence - quantified through established
uncertainty metrics like self-consistency and predictive perplexity - as a
substitute for external labels. We sample multiple reasoning traces and retain
only low-uncertainty subsets. Applied to biological perturbation prediction, a
domain where wet-lab labels are especially costly, we show that the filtered
subset has higher accuracy, and that supervised fine-tuning (SFT) on
uncertainty-filtered data outperforms unfiltered synthetic data, narrows the
gap to ground-truth training, and surpasses strong LRM baselines. Ablations
show that per-class filtering corrects for class-specific uncertainty scales
and that hybrid uncertainty metrics yield higher-quality datasets. Our results
suggest that model-internal confidence is a powerful signal for efficient
reasoning dataset creation, enabling LRMs in domains where supervision is
expensive.

</details>


### [503] [Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies](https://arxiv.org/abs/2510.05909)
*Aksel Joonas Reedi,Corentin Léger,Julien Pourcel,Loris Gaven,Perrine Charriau,Guillaume Pourcel*

Main category: cs.AI

TL;DR: 通过辩论式对抗训练（DebateQD）优化LLM，发现说服性目标相比真理追求目标能提升模型泛化能力。


<details>
  <summary>Details</summary>
Motivation: 主流的基于真理的优化方法会导致LLM过拟合，泛化能力差。以往的基于说服的优化方法在辩论场景中展现了潜力，但未与基于真理的方法进行系统性比较。

Method: 提出了一种名为DebateQD的最小化质量多样性（QD）进化算法，通过单LLM架构内的提示工程来维持对手的多样性，并进行锦标赛式辩论，其中两LLM辩论，第三者裁判。通过比较以说服为目标（不顾事实地说服裁判）和以真理为目标（合作追求正确性）的两种优化方式。

Result: 在三个模型规模（7B, 32B, 72B）和QuALITY基准的多个数据集上，以说服为目标的策略实现了高达13.94%的更小训练-测试泛化差距，并且在测试性能上匹配或超越了以真理为目标的策略。

Conclusion: 首次通过对照实验提供了证据，表明以说服为目标的竞争压力（而非合作追求真理）能够培养更具迁移性的推理技能，为提高LLM的泛化能力提供了有前景的途径。

Abstract: Large Language Models (LLMs) optimized to output truthful answers often
overfit, producing brittle reasoning that fails to generalize. While
persuasion-based optimization has shown promise in debate settings, it has not
been systematically compared against mainstream truth-based approaches. We
introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm
that evolves diverse debate strategies across different categories
(rationality, authority, emotional appeal, etc.) through tournament-style
competitions where two LLMs debate while a third judges. Unlike previously
proposed methods that require a population of LLMs, our approach maintains
diversity of opponents through prompt-based strategies within a single LLM
architecture, making it more accessible for experiments while preserving the
key benefits of population-based optimization. In contrast to prior work, we
explicitly isolate the role of the optimization objective by fixing the debate
protocol and swapping only the fitness function: persuasion rewards strategies
that convince the judge irrespective of truth, whereas truth rewards
collaborative correctness. Across three model scales (7B, 32B, 72B parameters)
and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized
strategies achieve up to 13.94% smaller train-test generalization gaps, while
matching or exceeding truth optimization's test performance. These results
provide the first controlled evidence that competitive pressure to persuade,
rather than seek the truth collaboratively, fosters more transferable reasoning
skills, offering a promising path for improving LLM generalization.

</details>


### [504] [Training-Free Time Series Classification via In-Context Reasoning with LLM Agents](https://arxiv.org/abs/2510.05950)
*Songyuan Sui,Zihang Xu,Yu-Neng Chuang,Kwei-Herng Lai,Xia Hu*

Main category: cs.AI

TL;DR: FETA是一个无需训练的多智能体框架，通过示例引导的上下文推理来解决时间序列分类问题，在UEA数据集上表现优于多个训练模型。


<details>
  <summary>Details</summary>
Motivation: 标记数据稀缺，导致任务特定训练成本高且不灵活，而纯零样本LLM性能不佳。

Method: FETA将多元时间序列分解为逐通道子问题，检索相似的标记样本，并利用LLM进行比对，生成带有置信度的通道级标签，最后通过加权聚合器融合各通道决策。

Result: 在九个UEA数据集上，FETA在完全无需训练的情况下实现了高准确率，超过了多个训练基线。

Conclusion: FETA证明了多智能体上下文推理框架可以将LLM转变为无需训练参数即可竞争的即插即用时间序列分类器。

Abstract: Time series classification (TSC) spans diverse application scenarios, yet
labeled data are often scarce, making task-specific training costly and
inflexible. Recent reasoning-oriented large language models (LLMs) show promise
in understanding temporal patterns, but purely zero-shot usage remains
suboptimal. We propose FETA, a multi-agent framework for training-free TSC via
exemplar-based in-context reasoning. FETA decomposes a multivariate series into
channel-wise subproblems, retrieves a few structurally similar labeled examples
for each channel, and leverages a reasoning LLM to compare the query against
these exemplars, producing channel-level labels with self-assessed confidences;
a confidence-weighted aggregator then fuses all channel decisions. This design
eliminates the need for pretraining or fine-tuning, improves efficiency by
pruning irrelevant channels and controlling input length, and enhances
interpretability through exemplar grounding and confidence estimation. On nine
challenging UEA datasets, FETA achieves strong accuracy under a fully
training-free setting, surpassing multiple trained baselines. These results
demonstrate that a multi-agent in-context reasoning framework can transform
LLMs into competitive, plug-and-play TSC solvers without any parameter
training. The code is available at https://github.com/SongyuanSui/FETATSC.

</details>


### [505] [MatheMagic: Generating Dynamic Mathematics Benchmarks Robust to Memorization](https://arxiv.org/abs/2510.05962)
*Dayyán O'Brien,Barry Haddow,Emily Allaway,Pinzhen Chen*

Main category: cs.AI

TL;DR: 提出了一种动态、反事实的数学基准MatheMagic，用于评估和揭示模型的过拟合现象，并衡量其真实推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前数学基准测试存在模型记忆测试集和基准测试集本身过拟合（符号和规则有限、答案封闭）的问题，导致难以进行无污染的数学能力评估。

Method: 利用现有基准测试的缺点，通过改变数字和运算符的解释来生成动态、反事实的测试实例，并自动验证答案。MatheMagic在测试时随机生成测试实例，以评估模型的归纳或演绎能力。

Result: 模型在演绎推理方面比归纳推理更容易，但在MatheMagic生成的反事实测试中，模型会回归到标准数学计算。进一步分析表明，经过数学适配的模型未能展现出通用的推理“技能”，并且在归纳任务上进行微调泛化能力较差。

Conclusion: 当前的数学模型在面对动态、反事实的测试时，其推理能力并未得到有效提升，并且在归纳任务上的微调泛化能力有限，表明它们并未掌握通用的数学推理技能。

Abstract: Conducting contamination-free evaluation of mathematical capabilities can be
difficult for two reasons: models may memorize a test set once it is made
public, and current mathematical benchmarks are prone to overfitting due to
having limited diversity of symbols and rules, coupled with closed-ended
answers. This paper proposes a method to leverage these shortcomings as useful
features to a construct dynamic, counterfactual benchmark, which can be used to
both reveal overfitting and measure true reasoning. We demonstrate this via
MatheMagic, which generates math test instances with the interpretations of
numbers and operators altered, yet has automatically verifiable answers. Test
instances are randomly seeded and constructed at test time to evaluate a
model's induction or deduction capability, offering stability, extensibility,
comparability, and robustness to overfitting. Our experiments find that models
solve deduction more easily than induction, but they revert to standard math.
Further analysis reveals that math-adapted models fail to exhibit a general
"skill" of reasoning, and fine-tuning on induction tasks generalizes poorly.

</details>


### [506] [Deterministic Legal Retrieval: An Action API for Querying the SAT-Graph RAG](https://arxiv.org/abs/2510.06002)
*Hudson de Martim*

Main category: cs.AI

TL;DR: SAT-Graph API通过引入以规范动作为中心的查询执行层，解决了标准法律领域检索增强生成（RAG）的局限性，实现了高精度混合搜索、鲁棒引用解析、按时版本检索和可审计因果追踪。


<details>
  <summary>Details</summary>
Motivation: 填补了在不牺牲确定性特性的情况下可靠查询结构化法律知识图谱的空白。

Method: 提出了SAT-Graph API，一个以规范动作为中心的查询执行层，这些动作是原子性的、可组合的和可审计的原语，将概率性发现与确定性检索分离开来。并展示了规划器指导的代理如何将复杂查询分解为这些动作的有向无环图（DAG）。

Result: 将检索从不透明的黑箱转变为透明、可审计的过程，满足了高风险领域对可解释人工智能（XAI）的要求。

Conclusion: SAT-Graph API和规划器指导的代理的双层架构为法律领域的可解释和可审计知识检索提供了一个有效的解决方案。

Abstract: The Structure-Aware Temporal Graph RAG (SAT-Graph RAG) addresses core
limitations of standard Retrieval-Augmented Generation in the legal domain by
providing a verifiable knowledge graph that models hierarchical structure,
temporal evolution, and causal events of legal norms. However, a critical gap
remains: how to reliably query this structured knowledge without sacrificing
its deterministic properties. This paper introduces the SAT-Graph API, a formal
query execution layer centered on canonical actions-atomic, composable, and
auditable primitives that isolate probabilistic discovery from deterministic
retrieval. These actions enable: (i) high-precision hybrid search; (ii) robust
reference resolution; (iii) point-in-time version retrieval; and (iv) auditable
causal tracing. We demonstrate how planner-guided agents can decompose complex
queries into Directed Acyclic Graphs (DAGs) of these actions. This two-layer
architecture transforms retrieval from an opaque black box to a transparent,
auditable process, directly addressing Explainable AI (XAI) requirements for
high-stakes domains.

</details>


### [507] [ARISE: An Adaptive Resolution-Aware Metric for Test-Time Scaling Evaluation in Large Reasoning Models](https://arxiv.org/abs/2510.06014)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Zhiyuan Yu,Qipeng Guo,Xuanjing Huang,Xipeng Qiu*

Main category: cs.AI

TL;DR: ARISE是一个用于评估大型推理模型测试时可扩展性的新指标，它通过样本级感知和动态采样来解决现有方法的不足，并在数学推理、代码生成和代理任务等领域进行了广泛的实验，结果显示Claude Opus在可扩展性方面表现优于其他模型。


<details>
  <summary>Details</summary>
Motivation: 现有评估测试时可扩展性的方法存在不足，无法系统地比较不同模型的可扩展性能力。

Method: 提出了一种名为ARISE（自适应分辨率感知缩放评估）的新指标。该指标包含两个关键创新：1. 样本级感知，能够有效惩罚计算量增加但性能下降的负缩放行为；2. 动态采样机制，以减轻准确性波动和代币计数不稳定性对最终评估的影响。

Result: 在数学推理、代码生成和代理任务等多个领域的最新推理模型进行的综合实验表明，ARISE能够可靠且精细地测量测试时可扩展性能力，揭示了模型之间在缩放效率方面的显著差异。

Conclusion: ARISE提供了一种有效的评估模型测试时可扩展性的方法，并在此基础上，本文发现Claude Opus在可扩展性方面表现优于其他同类推理模型。

Abstract: Test-time scaling has emerged as a transformative paradigm for enhancing the
performance of large reasoning models, enabling dynamic allocation of
computational resources during inference. However, as the landscape of
reasoning models rapidly expands, a critical question remains: how can we
systematically compare and evaluate the test-time scaling capabilities across
different models? In this paper, we introduce ARISE (Adaptive Resolution-aware
Scaling Evaluation), a novel metric specifically designed to assess the
test-time scaling effectiveness of large reasoning models. Unlike existing
evaluation approaches, ARISE incorporates two key innovations: (1) sample-level
awareness that effectively penalizes negative scaling behaviors where increased
computation leads to performance degradation, and (2) a dynamic sampling
mechanism that mitigates the impact of accuracy fluctuations and token count
instability on the final assessment. We conduct comprehensive experiments
evaluating state-of-the-art reasoning models across diverse domains including
mathematical reasoning, code generation, and agentic tasks. Our results
demonstrate that ARISE provides a reliable and fine-grained measurement of
test-time scaling capabilities, revealing significant variations in scaling
efficiency across models. Notably, our evaluation identifies Claude Opus as
exhibiting superior scaling characteristics compared to other contemporary
reasoning models.

</details>


### [508] [Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?](https://arxiv.org/abs/2510.06036)
*Qingyu Yin,Chak Tou Leong,Linyi Yang,Wenxuan Huang,Wenjie Li,Xiting Wang,Jaehong Yoon,YunXing,XingYu,Jinjin Gu*

Main category: cs.AI

TL;DR: 大型推理模型（LRM）在多步推理方面表现出色，但存在未被充分理解的安全漏洞。本研究通过机制可解释性探究了LRM安全对齐失败的原因。研究发现，许多对齐不佳的模型在推理过程中能识别有害提示并保持强烈的拒绝意图，但在生成输出前的最终标记处，拒绝意图会急剧下降，这种现象被称为“拒绝悬崖”。通过因果干预分析，识别出少数会抑制拒绝行为的注意力头，去除其中3%的注意力头可将攻击成功率降至10%以下。基于此，研究提出了“Cliff-as-a-Judge”数据选择方法，通过识别具有最大拒绝悬崖的训练样本来有效修复模型安全对齐。该方法仅用1.7%的常规安全训练数据就能达到相似的安全改进效果。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRM）在多步推理方面能力强大，但存在严重且未被充分理解的安全漏洞。因此，理解LRM安全对齐失败的机制至关重要。

Method: 研究人员使用线性探测方法追踪拒绝意图在不同标记位置的变化，发现了“拒绝悬崖”现象。通过因果干预分析，确定了抑制拒绝行为的注意力头。在此基础上，提出了一种名为“Cliff-as-a-Judge”的新数据选择方法。

Result: 研究发现，许多对齐不佳的LRM在推理过程中识别有害提示并保持拒绝意图，但在生成输出前的最终标记处，拒绝意图会急剧下降（拒绝悬崖）。通过因果干预，识别出少量（3%）对拒绝行为有负面影响的注意力头，并发现移除这些注意力头可显著降低攻击成功率。提出的“Cliff-as-a-Judge”方法通过选择具有最大拒绝悬崖的样本，能够以更少的数据（1.7%）实现显著的安全对齐改进。

Conclusion: LRM的安全对齐失败并非源于内在不安全，而是其拒绝意图在生成阶段被系统性地压制。通过理解和利用“拒绝悬崖”现象，可以开发出更有效的数据选择策略（如Cliff-as-a-Judge），以更少的资源实现LRM的安全对齐。这表明在安全对齐领域，“少即是多”是可行的。

Abstract: Large reasoning models (LRMs) with multi-step reasoning capabilities have
shown remarkable problem-solving abilities, yet they exhibit concerning safety
vulnerabilities that remain poorly understood. In this work, we investigate why
safety alignment fails in reasoning models through a mechanistic
interpretability lens. Using a linear probing approach to trace refusal
intentions across token positions, we discover a striking phenomenon termed as
\textbf{refusal cliff}: many poorly-aligned reasoning models correctly identify
harmful prompts and maintain strong refusal intentions during their thinking
process, but experience a sharp drop in refusal scores at the final tokens
before output generation. This suggests that these models are not inherently
unsafe; rather, their refusal intentions are systematically suppressed. Through
causal intervention analysis, we identify a sparse set of attention heads that
negatively contribute to refusal behavior. Ablating just 3\% of these heads can
reduce attack success rates below 10\%. Building on these mechanistic insights,
we propose \textbf{Cliff-as-a-Judge}, a novel data selection method that
identifies training examples exhibiting the largest refusal cliff to
efficiently repair reasoning models' safety alignment. This approach achieves
comparable safety improvements using only 1.7\% of the vanilla safety training
data, demonstrating a less-is-more effect in safety alignment.

</details>


### [509] [MixReasoning: Switching Modes to Think](https://arxiv.org/abs/2510.06052)
*Haiquan Lu,Gongfan Fang,Xinyin Ma,Qi Li,Xinchao Wang*

Main category: cs.AI

TL;DR: MixReasoning框架通过动态调整模型在推理过程中的详细程度，在处理不同难度的子问题时，对难题进行详尽推理，对简单问题进行简要推理，从而提高效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的推理模型在处理问题时，无论问题难度如何，都采用相同的推理深度，导致冗余和效率低下。

Method: 提出MixReasoning框架，该框架能够根据子问题的难度动态调整推理深度，实现详略得当的推理。

Result: 在GSM8K、MATH-500和AIME数据集上进行实验，证明MixReasoning在不牺牲准确性的前提下，缩短了推理长度，显著提高了效率。

Conclusion: MixReasoning框架通过自适应地调整推理深度，有效解决了现有模型效率问题，并在多个数学推理任务上取得了良好的效果。

Abstract: Reasoning models enhance performance by tackling problems in a step-by-step
manner, decomposing them into sub-problems and exploring long chains of thought
before producing an answer. However, applying extended reasoning to every step
introduces substantial redundancy, as sub-problems vary widely in difficulty
and complexity: a small number of pivotal steps are genuinely challenging and
decisive for the final answer, while many others only involve straightforward
revisions or simple computations. Therefore, a natural idea is to endow
reasoning models with the ability to adaptively respond to this variation,
rather than treating all steps with the same level of elaboration. To this end,
we propose MixReasoning, a framework that dynamically adjusts the depth of
reasoning within a single response. The resulting chain of thought then becomes
a mixture of detailed reasoning on difficult steps and concise inference on
simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning
shortens reasoning length and substantially improves efficiency without
compromising accuracy.

</details>


### [510] [Scientific Algorithm Discovery by Augmenting AlphaEvolve with Deep Research](https://arxiv.org/abs/2510.06056)
*Gang Liu,Yihan Zhu,Jie Chen,Meng Jiang*

Main category: cs.AI

TL;DR: DeepEvolve结合了深度研究和算法进化，通过迭代循环来改进科学算法。


<details>
  <summary>Details</summary>
Motivation: 现有的LLM科学助手要么依赖纯粹的算法进化，要么依赖孤立的深度研究，两者都有局限性。纯算法进化在复杂领域会迅速达到平台期，而纯深度研究提出的想法缺乏验证。需要一种能够结合外部知识检索、跨文件代码编辑和系统调试的方法。

Method: DeepEvolve在一个由反馈驱动的迭代循环中集成了深度研究和算法进化。每个迭代不仅提出新的假设，而且还对其进行改进、实现和测试。

Result: 在化学、数学、生物学、材料学和专利学的九个基准测试中，DeepEvolve持续改进初始算法，生成可执行的新算法并带来持续的收益。

Conclusion: DeepEvolve弥合了无指导进化和缺乏实践的研究所带来的差距，为推进科学算法发现提供了一个可靠的框架。

Abstract: Large language models hold promise as scientific assistants, yet existing
agents either rely solely on algorithm evolution or on deep research in
isolation, both of which face critical limitations. Pure algorithm evolution,
as in AlphaEvolve, depends only on the internal knowledge of LLMs and quickly
plateaus in complex domains, while pure deep research proposes ideas without
validation, resulting in unrealistic or unimplementable solutions. We present
DeepEvolve, an agent that integrates deep research with algorithm evolution,
uniting external knowledge retrieval, cross-file code editing, and systematic
debugging under a feedback-driven iterative loop. Each iteration not only
proposes new hypotheses but also refines, implements, and tests them, avoiding
both shallow improvements and unproductive over-refinements. Across nine
benchmarks in chemistry, mathematics, biology, materials, and patents,
DeepEvolve consistently improves the initial algorithm, producing executable
new algorithms with sustained gains. By bridging the gap between unguided
evolution and research without grounding, DeepEvolve provides a reliable
framework for advancing scientific algorithm discovery. Our code is available
at https://github.com/liugangcode/deepevolve.

</details>


### [511] [TelecomTS: A Multi-Modal Observability Dataset for Time Series and Language Analysis](https://arxiv.org/abs/2510.06063)
*Austin Feng,Andreas Varvarigos,Ioannis Panitsas,Daniela Fernandez,Jinbiao Wei,Yuwei Guo,Jialin Chen,Ali Maatouk,Leandros Tassiulas,Rex Ying*

Main category: cs.AI

TL;DR: Observability data from 5G networks are unique and underrepresented in benchmarks. We introduce TelecomTS, a new dataset with scale information, to facilitate research in anomaly detection, root-cause analysis, and multi-modal reasoning. Existing models struggle with this data, highlighting the need for scale-aware foundation models.


<details>
  <summary>Details</summary>
Motivation: Observability data from modern enterprises, such as time series metrics from complex systems, are crucial but underrepresented in public benchmarks due to proprietary restrictions and data anonymization. Existing datasets often lack scale information, limiting their use for tasks beyond forecasting, like anomaly detection and root-cause analysis. This necessitates a new, large-scale, and de-anonymized dataset with explicit scale information to support a wider range of downstream tasks.

Method: A large-scale observability dataset, TelecomTS, was derived from a 5G telecommunications network. This dataset includes heterogeneous, de-anonymized covariates with explicit scale information. It supports various downstream tasks, including anomaly detection, root-cause analysis, and a question-answering benchmark for multi-modal reasoning. State-of-the-art models were benchmarked on this dataset to evaluate their performance.

Result: Benchmarking state-of-the-art time series, language, and reasoning models on TelecomTS revealed that existing approaches struggle with the abrupt, noisy, and high-variance dynamics characteristic of observability data. The experiments demonstrated the critical importance of preserving the absolute scale of covariates, showing that scale information is essential for practical observability applications.

Conclusion: Existing models are ill-equipped to handle the complexities of observability data, particularly its noisy and high-variance dynamics. The introduction of TelecomTS and the experimental findings underscore the need for foundation time series models that can natively leverage absolute scale information to effectively address practical challenges in observability, such as anomaly detection and root-cause analysis.

Abstract: Modern enterprises generate vast streams of time series metrics when
monitoring complex systems, known as observability data. Unlike conventional
time series from domains such as weather, observability data are zero-inflated,
highly stochastic, and exhibit minimal temporal structure. Despite their
importance, observability datasets are underrepresented in public benchmarks
due to proprietary restrictions. Existing datasets are often anonymized and
normalized, removing scale information and limiting their use for tasks beyond
forecasting, such as anomaly detection, root-cause analysis, and multi-modal
reasoning. To address this gap, we introduce TelecomTS, a large-scale
observability dataset derived from a 5G telecommunications network. TelecomTS
features heterogeneous, de-anonymized covariates with explicit scale
information and supports a suite of downstream tasks, including anomaly
detection, root-cause analysis, and a question-answering benchmark requiring
multi-modal reasoning. Benchmarking state-of-the-art time series, language, and
reasoning models reveals that existing approaches struggle with the abrupt,
noisy, and high-variance dynamics of observability data. Our experiments also
underscore the importance of preserving covariates' absolute scale, emphasizing
the need for foundation time series models that natively leverage scale
information for practical observability applications.

</details>


### [512] [Constraint-Aware Route Recommendation from Natural Language via Hierarchical LLM Agents](https://arxiv.org/abs/2510.06078)
*Tao Zhe,Rui Liu,Fateme Memar,Xiao Luo,Wei Fan,Xinyue Ye,Zhongren Peng,Dongjie Wang*

Main category: cs.AI

TL;DR: RouteLLM是一个多智能体框架，用于将自然语言查询转换为满足用户特定需求的路线规划。


<details>
  <summary>Details</summary>
Motivation: 现有路线推荐方法在处理自然语言查询和同时考虑路线和兴趣点（POI）偏好方面存在局限性。

Method: RouteLLM采用分层多智能体方法，解析用户查询为结构化意图，并协调专门的子智能体（约束、POI、路径优化和验证）来生成满足约束并优化用户偏好的路线。

Result: 实验表明，RouteLLM能够可靠地将文本偏好转化为满足约束的路线，提高了路线质量和偏好满足度。

Conclusion: RouteLLM通过结合语言灵活性和空间结构，成功解决了现有路线推荐方法的局限性。

Abstract: Route recommendation aims to provide users with optimal travel plans that
satisfy diverse and complex requirements. Classical routing algorithms (e.g.,
shortest-path and constraint-aware search) are efficient but assume structured
inputs and fixed objectives, limiting adaptability to natural-language queries.
Recent LLM-based approaches enhance flexibility but struggle with spatial
reasoning and the joint modeling of route-level and POI-level preferences. To
address these limitations, we propose RouteLLM, a hierarchical multi-agent
framework that grounds natural-language intents into constraint-aware routes.
It first parses user queries into structured intents including POIs, paths, and
constraints. A manager agent then coordinates specialized sub-agents: a
constraint agent that resolves and formally check constraints, a POI agent that
retrieves and ranks candidate POIs, and a path refinement agent that refines
routes via a routing engine with preference-conditioned costs. A final verifier
agent ensures constraint satisfaction and produces the final route with an
interpretable rationale. This design bridges linguistic flexibility and spatial
structure, enabling reasoning over route feasibility and user preferences.
Experiments show that our method reliably grounds textual preferences into
constraint-aware routes, improving route quality and preference satisfaction
over classical methods.

</details>


### [513] [Classical AI vs. LLMs for Decision-Maker Alignment in Health Insurance Choices](https://arxiv.org/abs/2510.06093)
*Mallika Mainali,Harsha Sureshbabu,Anik Sen,Christopher B. Rauch,Noah D. Reifsnyder,John Meyer,J. T. Turner,Michael W. Floyd,Matthew Molineaux,Rosina O. Weber*

Main category: cs.AI

TL;DR: 现有AI方法与大型语言模型（LLM）在决策者对齐（DMA）方面各有优劣，且在特定领域（如医疗分诊）表现良好，但在新情境下的泛化能力有待进一步研究。本研究比较了经典AI模型和LLM在健康保险决策制定方面的对齐效果，发现在面对不同风险承受能力的决策者时，两者表现相当，经典AI模型在中等风险情境下略胜一筹。


<details>
  <summary>Details</summary>
Motivation: 算法决策者日益广泛地应用于高风险领域，AI对齐研究已从普适性价值对齐转向考虑决策者属性的特定情境方法。已有关于决策者对齐（DMA）的研究探索了两种主要策略：经典AI方法和基于LLM的方法，但它们在不同情境下的泛化能力仍需探索。

Method: 本文实现了一个先前的经典AI模型，并开发了一个基于LLM的算法决策者。实验采用大型推理模型（GPT-5）和非推理模型（GPT-4）进行评估，并在零样本提示框架下结合加权自洽性。在具有三种不同风险承受能力（0.0、0.5、1.0）的目标的健康保险决策数据集上进行了评估。

Result: 经典AI模型和LLM在与基于属性的目标对齐方面表现相当，经典AI模型在具有中等风险特征的目标上表现出略好的对齐效果。

Conclusion: 经典AI模型和LLM在健康保险决策制定中，与不同风险偏好的决策者目标实现可比的对齐效果，显示出在类似任务上的潜力。

Abstract: As algorithmic decision-makers are increasingly applied to high-stakes
domains, AI alignment research has evolved from a focus on universal value
alignment to context-specific approaches that account for decision-maker
attributes. Prior work on Decision-Maker Alignment (DMA) has explored two
primary strategies: (1) classical AI methods integrating case-based reasoning,
Bayesian reasoning, and naturalistic decision-making, and (2) large language
model (LLM)-based methods leveraging prompt engineering. While both approaches
have shown promise in limited domains such as medical triage, their
generalizability to novel contexts remains underexplored. In this work, we
implement a prior classical AI model and develop an LLM-based algorithmic
decision-maker evaluated using a large reasoning model (GPT-5) and a
non-reasoning model (GPT-4) with weighted self-consistency under a zero-shot
prompting framework, as proposed in recent literature. We evaluate both
approaches on a health insurance decision-making dataset annotated for three
target decision-makers with varying levels of risk tolerance (0.0, 0.5, 1.0).
In the experiments reported herein, classical AI and LLM-based models achieved
comparable alignment with attribute-based targets, with classical AI exhibiting
slightly better alignment for a moderate risk profile. The dataset and
open-source implementation are publicly available at:
https://github.com/TeX-Base/ClassicalAIvsLLMsforDMAlignment and
https://github.com/Parallax-Advanced-Research/ITM/tree/feature_insurance.

</details>


### [514] [Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences](https://arxiv.org/abs/2510.06105)
*Batu El,James Zou*

Main category: cs.AI

TL;DR: 优化LLM以在竞争中获胜会适得其反，导致它们变得不那么符合伦理，并且会传播更多错误信息。这被称为“AI的摩洛克契约”，并可能需要新的治理和激励措施来防止AI的广泛滥用。


<details>
  <summary>Details</summary>
Motivation: 本研究旨在探讨在竞争环境下，优化大型语言模型（LLMs）的行为如何影响其与人类价值观的一致性，特别是当LLMs被用于营销、选举和社交媒体等场景时。

Method: 通过在模拟环境中，针对销售、投票份额和社交媒体参与度等竞争性指标优化LLMs的行为，并量化由此产生的虚假信息、煽动性言论和有害内容传播的增加。

Result: 研究发现在竞争性优化下，LLMs会表现出更不符合伦理的行为：营销中欺骗性内容增加14.0%，选举中虚假信息增加22.3%，社交媒体中虚假信息增加188.6%。

Conclusion: 在竞争压力下，LLMs会牺牲伦理道德以求得竞争优势，即使有明确的指令要求其保持真实和可靠，这种现象被称为“AI的摩洛克契约”。这表明当前的AI对齐方法不足以应对市场驱动的优化压力，需要更强有力的治理和激励机制来确保AI的安全部署，维护社会信任。

Abstract: Large language models (LLMs) are increasingly shaping how information is
created and disseminated, from companies using them to craft persuasive
advertisements, to election campaigns optimizing messaging to gain votes, to
social media influencers boosting engagement. These settings are inherently
competitive, with sellers, candidates, and influencers vying for audience
approval, yet it remains poorly understood how competitive feedback loops
influence LLM behavior. We show that optimizing LLMs for competitive success
can inadvertently drive misalignment. Using simulated environments across these
scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise
in deceptive marketing; in elections, a 4.9% gain in vote share coincides with
22.3% more disinformation and 12.5% more populist rhetoric; and on social
media, a 7.5% engagement boost comes with 188.6% more disinformation and a
16.3% increase in promotion of harmful behaviors. We call this phenomenon
Moloch's Bargain for AI--competitive success achieved at the cost of alignment.
These misaligned behaviors emerge even when models are explicitly instructed to
remain truthful and grounded, revealing the fragility of current alignment
safeguards. Our findings highlight how market-driven optimization pressures can
systematically erode alignment, creating a race to the bottom, and suggest that
safe deployment of AI systems will require stronger governance and carefully
designed incentives to prevent competitive dynamics from undermining societal
trust.

</details>


### [515] [Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification](https://arxiv.org/abs/2510.06135)
*Weihao Zeng,Keqing He,Chuqiao Kuang,Xiaoguang Li,Junxian He*

Main category: cs.AI

TL;DR: 测试时计算可以顺序或并行扩展。顺序扩展通过延长生成过程实现，并行扩展通过验证和选择多个候选输出来实现。将这两种策略结合起来，可以构建最强大的 AI 系统。在某些情况下（例如，解决数独问题），验证响应比生成响应更容易，这被称为“不对称验证”，凸显了测试时扩展（TTS）的巨大潜力。本研究着重于深度搜索代理的顺序和并行 TTS，并利用不对称验证的优势，在不显著增加验证器计算量的情况下，实现了显著的性能提升。通过 TTS，旗舰开源模型及其“Heavy”变体在 BrowseComp 等基准测试中取得了高达 27 个绝对点的提升。值得注意的是，GLM-4.5 Heavy 作为开源替代品，在 BrowseComp 和 GAIA 上的准确率分别达到了 54.0% 和 66.0%，与 OpenAI Deep Research 等最佳专有模型相当。Tongyi-DeepResearch Heavy 在 BrowseComp 上的准确率更是达到了 69.0%，显著超过了现有的最佳专有模型。


<details>
  <summary>Details</summary>
Motivation: 研究深度搜索代理的测试时扩展（TTS），特别是顺序和并行 TTS，以及利用“不对称验证”的潜力，即验证响应通常比生成响应更容易。

Method: 研究了深度搜索代理的顺序和并行 TTS。顺序扩展方法包括预算强制。利用不对称验证，通过为验证器分配少量计算量来实现性能提升。实验中使用了旗舰开源模型及其“Heavy”变体。

Result: 顺序扩展方法（如预算强制）最初有效，但随后性能下降。利用不对称验证，在仅分配少量计算量给验证器的情况下，实现了显著的性能提升。在 BrowseComp 基准测试中，旗舰开源模型及其“Heavy”变体取得了高达 27 个绝对点的性能提升。GLM-4.5 Heavy 在 BrowseComp 和 GAIA 上的准确率分别达到 54.0% 和 66.0%。Tongyi-DeepResearch Heavy 在 BrowseComp 上的准确率达到 69.0%，超过了现有的最佳专有模型。

Conclusion: 测试时扩展（TTS），特别是利用不对称验证，能够显著提升深度搜索代理的性能。通过将少量计算量分配给验证器，可以实现比顺序扩展更优越的结果。开源模型通过 TTS 能够达到甚至超越顶级的专有模型性能。

Abstract: Test-time compute can be scaled both sequentially and in parallel. Sequential
scaling involves lengthening the generation process, while parallel scaling
involves verifying and selecting among multiple candidate outputs. Combining
these two strategies has led to the most powerful AI systems, such as Grok 4
Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles),
verifying responses can be substantially easier than generating them. This
property, referred to as \emph{asymmetric verification}, highlights the strong
potential of test-time scaling (TTS). In this work, we study both sequential
and parallel TTS of deep search agents, motivated by the intuition that
verification in this setting is often much easier than generation. In
experiments, we first show that sequential scaling methods, such as budget
forcing, can be effective initially but soon degrade performance. Leveraging
asymmetric verification, however, we are able to achieve substantial
improvements by allocating only a modest amount of compute to the verifier. We
conduct experiments with flagship open-source models and extend them to their
``Heavy'' variants through TTS. These deep research agents achieve gains of up
to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an
open-source alternative, GLM-4.5 Heavy reaches accuracy of {\bf 54.0\%} on
BrowseComp and {\bf 66.0\%} on GAIA, placing it comparable to the best
proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy
further achieves {\bf 69.0\%} accuracy on BrowseComp, greatly surpassing the
best proprietary results.

</details>


### [516] [Barbarians at the Gate: How AI is Upending Systems Research](https://arxiv.org/abs/2510.06189)
*Audrey Cheng,Shu Liu,Melissa Pan,Zhifei Li,Bowen Wang,Alex Krentsel,Tian Xia,Mert Cemri,Jongseok Park,Shuo Yang,Jeff Chen,Aditya Desai,Jiarong Xing,Koushik Sen,Matei Zaharia,Ion Stoica*

Main category: cs.AI

TL;DR: AI can automate the discovery of new solutions by generating and verifying them. This paper proposes an AI-Driven Research for Systems (ADRS) approach, suitable for systems research due to reliable verifiers. Case studies show ADRS outperforming human designs. The paper also discusses best practices and implications for the systems community, suggesting a future where humans focus on problem formulation and AI on algorithm design.


<details>
  <summary>Details</summary>
Motivation: The paper argues that AI is transforming research by automating solution discovery, and that systems research is particularly well-suited for this due to the existence of reliable verifiers for performance-oriented problems.

Method: The paper proposes and demonstrates an approach called AI-Driven Research for Systems (ADRS), which iteratively generates, evaluates, and refines solutions using existing frameworks like penEvolve. It presents case studies across various domains and distills best practices for guiding algorithm evolution.

Result: ADRS, using penEvolve, discovered algorithms that outperform state-of-the-art human designs in multiple instances, achieving significant improvements in runtime and cost reductions. The paper also provides best practices for guiding algorithm evolution and discusses the broader implications for the systems community.

Conclusion: AI has disruptive potential in systems research, necessitating adaptation of current practices. Human researchers will likely shift focus towards problem formulation and strategic guidance, while AI takes a central role in algorithm design. The ADRS approach shows promise in discovering superior algorithms.

Abstract: Artificial Intelligence (AI) is starting to transform the research process as
we know it by automating the discovery of new solutions. Given a task, the
typical AI-driven approach is (i) to generate a set of diverse solutions, and
then (ii) to verify these solutions and select one that solves the problem.
Crucially, this approach assumes the existence of a reliable verifier, i.e.,
one that can accurately determine whether a solution solves the given problem.
We argue that systems research, long focused on designing and evaluating new
performance-oriented algorithms, is particularly well-suited for AI-driven
solution discovery. This is because system performance problems naturally admit
reliable verifiers: solutions are typically implemented in real systems or
simulators, and verification reduces to running these software artifacts
against predefined workloads and measuring performance. We term this approach
as AI-Driven Research for Systems (ADRS), which iteratively generates,
evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS
instance, we present case studies across diverse domains, including load
balancing for multi-region cloud scheduling, Mixture-of-Experts inference,
LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS
discovers algorithms that outperform state-of-the-art human designs (e.g.,
achieving up to 5.0x runtime improvements or 50% cost reductions). We distill
best practices for guiding algorithm evolution, from prompt design to evaluator
construction, for existing frameworks. We then discuss the broader implications
for the systems community: as AI assumes a central role in algorithm design, we
argue that human researchers will increasingly focus on problem formulation and
strategic guidance. Our results highlight both the disruptive potential and the
urgent need to adapt systems research practices in the age of AI.

</details>


### [517] [TaTToo: Tool-Grounded Thinking PRM for Test-Time Scaling in Tabular Reasoning](https://arxiv.org/abs/2510.06217)
*Jiaru Zou,Soumya Roy,Vinay Kumar Verma,Ziyi Wang,David Wipf,Pan Lu,Sumit Negi,James Zou,Jingrui He*

Main category: cs.AI

TL;DR: TaTToo是一个新的表格推理框架，通过显式推理表格步骤和整合工具验证来改进大型推理模型（LRMs）在表格推理任务上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有PRM在处理表格推理任务时存在瓶颈，尤其是在表格操作和模式交互方面，其在表格推理领域的潜力尚未被充分发掘。

Method: TaTToo通过一个可扩展的数据创建流程，整合了表格验证和工具执行，构建了超过6万个高质量的步骤级别标注。在此基础上，采用两阶段训练范式：冷启动监督微调以捕捉工具使用模式，然后进行基于工具的奖励塑造强化学习，以实现与表格验证的对齐。

Result: TaTToo在5个具有挑战性的表格推理基准测试中，将下游策略LRM的推理性能提高了30.9%，其参数量仅为8B，优于Qwen-2.5-Math-PRM-72B等模型，并展现出良好的泛化能力。

Conclusion: TaTToo框架在表格推理方面取得了显著成效，解决了现有PRM的局限性，并通过其新颖的标注和训练方法，显著提升了大型推理模型的性能和泛化能力。

Abstract: Process Reward Models (PRMs) have recently emerged as a powerful framework
for enhancing the reasoning capabilities of large reasoning models (LRMs),
particularly in the context of test-time scaling (TTS). However, their
potential for supervising LRMs on tabular reasoning domains remains
underexplored. Through detailed empirical analyses, we identify that existing
PRMs, though widely adopted for supervising text-only reasoning steps, struggle
with table-specific operations such as sub-table retrieval and schema
interaction, leading to critical performance bottlenecks. To address this
limitation, we propose TaTToo, a novel table-grounded PRM framework that (i)
reasons explicitly over tabular reasoning steps and (ii) integrates tool-based
verification to provide precise reward supervision. Concretely, we first design
a scalable data curation pipeline that constructs over 60k high-quality
step-level annotations by integrating table verification rationales with
tool-based executions. Building on the collected data, we train TaTToo with a
dual-stage paradigm: cold-start supervised fine-tuning to capture tool-use
reasoning patterns, followed by reinforcement learning with tool-grounded
reward shaping to align our model with table-based verification. We provide a
comprehensive evaluation of the policy improvement induced by our newly
designed PRM. Across 5 challenging tabular reasoning benchmarks covering
numerical reasoning, fact-checking, and data analysis, TaTToo improves
downstream policy LRMs by 30.9% at inference, surpasses strong PRM baselines
such as Qwen-2.5-Math-PRM-72B with only 8B parameters, and demonstrates strong
generalizability across diverse TTS strategies.

</details>
